question_id,title,body,tags
2572657,How to find $E(Y|X)$,"I have the joint PDF of $X$ and $Y$ as follows: $$f_{X, Y}(x,y) =
\begin{cases}
\frac{1}{2} e^{-\frac{x}{y}-y}, & x \ge 0, y>0 \\[2ex]
0, & \text{otherwise}
\end{cases}$$ I calculated $E(X|Y) = y$ but I can't manage calculating $E(Y|X)$ because im not sure as to how to start it. In the process of calculating $E(X|Y)$ I also calculated $$f_{X|Y}(x|y) = \frac{1}{y} e^{-\frac{x}{y}}, x \ge 0$$ and $$f_Y(y) = e^{-y} , y>0$$ thanks","['statistics', 'probability']"
2572670,Prove that there exists a Lebesgue Measurable subset $B$ of $A$ such that $\lambda(B)=\frac{1}{2}$.,Let $A$ be a measurable subset of $\Bbb R$ and $\lambda(A)=1$ where $\lambda $ denotes the Lebesgue Measure on $\Bbb R$ . Prove that there exists a Lebesgue Measurable subset $B$ of $A$ such that $\lambda(B)=\frac{1}{2}$ . I have completed sections on Measure Theory from G.D.Baraa upto Lebesgue Measure but I am not getting any hints on how to solve this problem.Please give me some hints. Also please give me some names of books where I can find these sort of questions and try to solve them .,"['real-analysis', 'lebesgue-measure', 'measure-theory']"
2572692,Coin Toss Game - Flip Until Failure,"The Problem You start off with $N$ coins. All coins are fair and land heads with probability $p_f=0.5$ , except one weighted coin which lands heads with a weight $p_w$ . When the game starts perform the following steps: Flip each coin in play If no coin flips heads the game ends. Otherwise remove all coins that flipped heads from the game. Flip all the remaining coins. Repeat until no coin flips heads or all coins are removed from the game. Assuming $n\leq N$ coins have been removed from the game, what is the probability that the weighted coin was removed? i.e. $$P(\text{ Weighted Coin Removed from Play } | \text{ } n \text{ Coins Removed }) = \text{???}$$ My Approach Initially my thought was that the game itself isn't really relevant and I can just look at a single trial of the game. We just need to look at the probability of flipping $n$ heads (where one of which is the weighted coin) over the probability of flipping $n$ heads. For example, let's set $N=3$ , $n=2$ , $p_f=0.5$ , and $p_w=0.1$ . Then the probability of flipping two heads where one of which is the weighted coin is $$2p_wp_f(1-p_f) = 2(0.1)(0.5)(1-0.5) = 0.05.$$ And the probability of flipping two heads where one of which ISN'T the weighted coin is $$(1-p_w)p_f^2 = (0.9)(0.5)^2 = 0.225.$$ So I would think the probability of having already removed the weighted coin once two coins are removed is $$\frac{0.05}{0.05+0.225}\approx 0.182$$ But I wrote a sim that says it should be closer $0.166$ , and I'm sure there's something wrong with my approach needs to take into account the game. Not really sure what I'm doing wrong, but I'm pretty sure I need to take into account the possibility of multiple turns of the game somehow.","['combinatorics', 'probability']"
2572696,"Neukirch, Proposition 2.11","I am reading Neukirch's Algebraic Number Theory , and I do not understand the following step in the proof of Proposition 2.11. The proof can be found in this imgur album , but only the very end confuses me. The step that confuses me is on one of the last lines. He writes M as a product of two $n' \times n'$ matrices, each of whose entries is an $n \times n$ matrix. He then says, ""By changing indices the second matrix may be transformed to look like the first one,"" and writes,
$$\det(M)^2 = \det(Q)^{2n'} \det((\sigma_{\ell}^{'}\omega_j^{'}))^{2n}.$$ I understand the $\det(Q)^{2n'}$ term, but I do not understand where the $\det((\sigma_{\ell}^{'}\omega_j^{'}))^{2n}$ comes from or what he means by ""changing indices"". It seems to me that because the second matrix is $n' \times n'$ as well, you would except an exponent of $2n'$, rather than $2n$. But I'm not sure how he's calculating it.","['matrices', 'abstract-algebra', 'algebraic-number-theory', 'number-theory', 'linear-algebra']"
2572780,Is it sufficient to define a measure on half-open intervals?,"Suppose I want to define measure $\mu$ on the Borel sigma-algebra of $[0,1]$. Is it sufficient to define $\mu$ on the half-open intervals $[a,b)$ where $0\leq a<b\leq 1$? The reason I think this is so is because the value of $\mu$ on other intervals can be computed by the properties of a measure. For example: $$[a,b] = \bigcap_{n>0} \big[a,b+{1\over n}\big)$$ so: $$\mu([a,b]) = \lim_{n\to\infty} \mu\bigg(\big[a,b+{1\over n}\big)\bigg)$$ and: $$(a,b) = \bigcup_{n>0} \big[a+{1\over n},b\big)$$ so: $$\mu((a,b)) = \lim_{n\to\infty} \mu\bigg(\big[a+{1\over n},b\big)\bigg)$$ Is this true?",['measure-theory']
2572802,Looking for an alternative elegant solution to this Combinatorial Problem.....,"We were asked to find the number of five digit numbers $N=d_1d_2d_3d_4d_5$, where $d_i$ is the $i$th digit of the number and $d_1 < d_2 < d_3 < d_4 < d_5 $. The solution was trivial as for a given selection of five random distinct digits, there is only one way to arrange them in strict increasing order. Now to raise the difficulty level of the question, our teacher changed all the ""less than"" signs into ""less than or equal to"" signs, thus yielding the new constraint $d_1 \leq d_2 \leq d_3 \leq d_4 \leq d_5$. I proceeded as follows ..... Let $x_i = d_{i+1} - d_i$. So,  $$ x_1 + x_2 + x_3 + x_4 = d_5 - d_1,$$ or $$d_1 + x_1 + x_2 + x_3 + x_4 = d_5 \leq 9.$$ But $d_1 \geq 1, x_i \geq 0$ for all $i $ belonging to the set ${1,2,3,4} $. So, the problem reduces to find the number of non negative integral solutions of the inequality $$d_1 + x_1 + x _2 + x_3 + x_4 \leq 8,\quad      
 (*)$$ which comes out to be $^{13} C_ {5}$. But looking at the simplicity of the answer, I am bound to think that there must exist some elegant approach to this problem. In Combinatorics, every one thinks differently, which is why I ask this question. I just want to explore the different ways one can approach a problem. Please note that I am just a beginner in Combinatorics. I am familiar with Permutations and Combinations, but I do not know a single thing in Graph Theory and all those ""advanced"" concepts. So please answer accordingly.","['combinations', 'alternative-proof', 'integers', 'permutations', 'combinatorics']"
2572813,"Smooth map on a ""non-open"" subset","Let $A\subset\mathbb{R}^n$ be a subset (not necessarily open) and $f:A\to\mathbb{R}$ be a map. As far as I have read, there are two definitions of the smoothness of $f$. [Definition 1]
The map $f$ is smooth if and only if there exists an open set $U\subset\mathbb{R}^n$ and a smooth map $F:U\to\mathbb{R}$ such that 
$A\subset U$ and $f=F$ on $A\cap U$. [Definition 2]
The map $f$ is smooth if and only if for every $x\in\mathbb{R}^n$ there exists an open neighborhood $U$ of $x$ and a smooth map $F:U\to\mathbb{R}$ such that $f=F$ on $A\cap U$. Maybe Definition 1 is more usual than Definition 2.
Definition 2 is seen in Lee's ""Introduction to smooth manifolds."" Now, I have a question.
Are these two definitions equivalent? The implication [Definition 1]$\Rightarrow$[Definition 2] is clear.
But the converse does not seem to be obvious.
Is there any counterexample?","['smooth-manifolds', 'differential-geometry', 'calculus']"
2572902,CLT type convergence with Lyapunov condition violated,"For $i=1,2,\dots$ let $X_i$ be a random variable, independent of all $X_j$ with $i\neq j$, taking the values $\pm 1$ with probability $\frac{1}{2i^2}$ each and the value $0$ with probability $\frac{i^2 -1}{i^2}$. Then the variance of $X_i$ is $\sigma_i^2 = \frac{1}{i^2}$. Let $s_n^2 = \sum_{i=1}^n \sigma_i^2$. I am trying to understand the convergence of the sequence
$$
Y_n = \frac{1}{s_n} \sum_{i=1}^n X_i
$$ Question: Does it converge in distribution and what is the limit? The first thing that comes to mind is the central limit theorem. We can try the Lyapunov condition which (in this case) asks if for any $\delta>0$
$$
\lim_{n\to\infty} \frac{1}{s_n^{2+\delta}} \sum_{i=1}^n \mathbb{E} (|X_i|^{2+\delta}) = 0
$$
Since $|X_i| = |X_i|^{2+\delta}$ we find that the sum is equal to $s_n^2$ and since $\sum_{i=1}^n \frac{1}{i^2}$ converges as $n\to \infty$, the limit is nonzero and the Lyapunov condition does not yield the central limit theorem.","['probability-theory', 'convergence-divergence', 'central-limit-theorem']"
2572994,Prove $\tan$ function is a bijection,"let $f : (-\frac{\pi}{2},\frac{\pi}{2}) \to \mathbb{R}$ be a function so $f(x)=\tan(x)$. I am trying to prove that $f$ is bijective. 
Firstly I want to show that $f$ is injection. Consider $x,y\in (-\frac{\pi}{2},\frac{\pi}{2})$ so $\tan(x)=\tan(y)$. we need to prove that $x=y$ but how? If I want to prove that $f$ is surjective, then I want to show that for all $y\in \mathbb{R}$ there is $x\in (\frac{-\pi}{2},\frac{\pi}{2})$ so $f(x)=y$ but here I also hove some trouble to prove. Any hints?","['trigonometry', 'functions']"
2573023,An infinite chain of finite set is countably infinite,"Let $Z$ be an infinite set of finite sets. For all $X,Y\in Z$ we have $X\subseteq Y$ or $Y\subseteq X$. I am trying to prove that $Z\sim\mathbb{N}$ where $\sim$ represents equivalence of sets. I think I need to think of a function $f$ from $Z$ to $\mathbb{N}$ that is a bijection and that will prove this theorem, but I can't think of one. Hints?",['elementary-set-theory']
2573029,$\lim_{x\to 0} \left(\frac{1}{1-\cos x}-\frac{2}{x^2}\right)$,Find the limits : $$\lim_{x\to 0} \left(\frac{1}{1-\cos x}-\frac{2}{x^2}\right)$$ My Try : $$\lim_{x\to 0} \left(\frac{1}{1-\cos x}-\frac{2}{x^2}\right)=\lim_{x\to 0}\frac{x^2-2(1-\cos x)}{x^2(1-\cos x)}$$ Now what do I do ?,"['limits-without-lhopital', 'trigonometry', 'calculus', 'limits']"
2573051,"MLE for gaussian, finding $\mu$ and $\sigma^2$","""Assume that a dataset $x_1,\ldots, x_N$ consisting of $N$ points was sampled from a Gaussian distribution, i.e., $X_i \sim N(\mu; \sigma^2)$ for some unknown $- \infty < \mu < \infty$ and unknown $0 < \sigma^2 < \infty$. Also, assume that the $X_i$ are independent and identically distributed (iid). Find the maximum likelihood estimate of the Gaussian mean $\mu$ and variance $\sigma^2$
(and show that the critical point obtained is, at least, a local maximum)"" -exercise $2.8$, A first course in machine learning, second edition. I'm currently trying to solve the exercise above, however it's proving hard for me, nad i would love some help / a reference solution to the exercise. So first i'll define the log-likelihood as:
$$\log L = -\frac{N}{2}\log2\pi-N \log \sigma - \frac {1}{2 \sigma^2}\sum_{n=1}^N(t_n - w^Tx_n$$ I then find the derivative and set it equal to $0$ so: $$\frac{\log L} w = \frac{1}{\sigma^2}\sum_{n=1}^N x_nt_n - x_n x_n^T w=0$$ which can then be rewritten as: $$\frac{\log L} w = \frac 1 {\sigma^2}(X^T t-X^t Xw)=0,$$ and i can then solve for $w$ and get:
$$w = (X^TX)^{-1} X^T t$$ After this point i get stuck, and unsure of how to find MLE for $\mu$ and $\sigma^2$, aswell as how to show that the critical point is at least a local maximum.","['maximum-likelihood', 'statistics', 'log-likelihood']"
2573102,Show that $\int_{0}^{4e}x^2{\ln{x}\ln{(4e-x)}\over \sqrt{x(4e-x)}}dx=-e^2\pi(\pi-1)(\pi+1)$,"Show that
  $$\int_{0}^{4e}x^2{\ln{x}\ln{(4e-x)}\over \sqrt{x(4e-x)}}\mathrm dx=-e^2\pi(\pi-1)(\pi+1)\tag1$$ My work. Let $x=4e\sin^2{t}$ then $\mathrm dx=8e\sin{t}\cos{t}=4e\sin{(2t)}\mathrm dt$. After a long simplification we got 
$$32e^2\int\sin^4{t}\ln(4e\sin^2{t})\ln(4e\cos^2{t})\mathrm dt\tag2$$
$(2)$ probably a long IBP.","['integration', 'definite-integrals']"
2573137,"Change of basis - Definition, matrix and relation to diagonalization","Change of basis and diagonalization is a hassle for anyone new to the world of linear algebra; anyway for me. I was thinking I could post my full interpretation and then you guys could correct me if I'm wrong in fill in the gaps if I've missed anything etc. Maybe it also could work as future reference for others. Current questions: First, are my interpretations below correct? Anything that's vague or indefinite? I'm especially unsure about the use of diagonalization for system of equations. Does it really have the same equal solution or is it dependent on the ""new"" basis or something like that? What are the further uses of diagonalization? How would one go about to find the change of basis matrix from $\mathcal A$ to $\mathcal B$? From e.g. base $\mathcal C$ to the standard basis it's simple, the transformation matrix consists of the basis vectors of $\mathcal C$ (although expressed in the standardbasis I guess, but it's simpler to grasp). Must one ""go through"" the standard basis if I want to go from $\mathcal A$ to $\mathcal B$? Change of basis Let $\mathcal A = \{\overrightarrow{a_1},\cdots,\overrightarrow{a_n}\}$ and $\mathcal B = \{\overrightarrow{b_1},\cdots,\overrightarrow{b_n}\}$ be two bases in $\mathbb R^n$.
Then, the change of basis matrix from $\mathcal A$ to $\mathcal B$ is the matrix $T_{\mathcal A \rightarrow \mathcal B}$ which columns is the $\mathcal A$-basis vectors expressed in $\mathcal B$:
$$T_{\mathcal A \rightarrow \mathcal B} = \begin{bmatrix}[\overrightarrow{a_1}]_\mathcal B&\cdots&[\overrightarrow{a_n}]_\mathcal B\end{bmatrix}$$
Then, for all vectors $\overrightarrow{x}$ in $\mathbb R^n$ the following applies:
$$T_{\mathcal A \rightarrow \mathcal B}[\overrightarrow{x}]_\mathcal A=[\overrightarrow{x}]_\mathcal B$$ I.e. the change of basis matrix T that transforms a vector $\overrightarrow{x}$ from $\mathcal A$ to $\mathcal B$ is obtained by inserting $\mathcal A$'s base vectors expressed in $\mathcal B$ as columns. ""The old base is expressed in the new"". Notes $T_{\mathcal A \rightarrow \mathcal B} = (T_{\mathcal B \rightarrow \mathcal A})^{-1}$ (invers) $T_{\mathcal A \rightarrow \mathcal C} = T_{\mathcal B \rightarrow \mathcal C}T_{\mathcal A \rightarrow \mathcal B}$ (multiple basis changes, note the order) The above is in some sense also true for a linear transformation,
since the transformation matrix have the images of its basis as
columns. Though, a linear transformation can be from $\mathbb R^n$ to
$\mathbb R^k$ and is not always invertible. Diagonalization A square matrix $A$ can be decomposed to special form $A=SDS^{-1}$ (or the more commonly written and equal $D=S^{-1}AS$), where $S$ is the matrix consisting of eigenvectors to $A$ and $D$ is the diagonal matrix with its corresponding eigenvalues. To do this one must find the eigenvalues and eigenvectors and if there are as many linear independent eigenvectors as the basis of the space which $A$ exists in, diagonalization is possible. In other words, to diagonlize a matrix $A$ one must find a new basis which consists of its eigenvectors. So, diagonalization is a way of expressing the same linear transformation but in another basis, which is easier to understand and visualize since they are mapped on themselves by a scalar of their corresponding eigenvalues. Use in system of equations An initial equation $AX=Y$ can then be written as: $$SDS^{-1}X=Y$$ By multiplying both sides with $S^{-1}$ we get: $$DS^{-1}X=S^{-1}Y$$ Since the same linear transformation $S^{-1}$ is applied to both $X$ and $Y$ the initial equation is equivalent to the transformed one: $$DX'=Y'$$
Where $X'=S^{-1}X$ and $Y'=S^{-1}Y$ So, to solve for $X$ one must first obtain $X'$ from $DX'=Y'$, which becomes easy since $D$ is diagonal. $X$ is then obtained from $X=SX'$ Therefore diagonalization is a way solving a system of equations.","['systems-of-equations', 'matrices', 'change-of-basis', 'diagonalization', 'linear-algebra']"
2573186,Difference between statistics and stochastic? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Can somebody explain me the difference between statistics and stochastic? I know that stochastic calculates probabilities but isn't statistics the same?","['stochastic-analysis', 'statistics']"
2573198,"Why use Bordered Hessian than ""simple"" Hessian as second derivative test?","Why use Bordered Hessian than ""simple"" Hessian as second derivative test in a multi constrained optimization problem?
The critical points are found from the Lagrangian so they follow the constraints.
All you have to do is to check the convexity/concavity with Hessian in these critical points and tell whether it is local minimum or maximum.
I find it much easier in calculations and to remember the rules of convexity/concavity, than these of bordered Hessian...","['multivariable-calculus', 'hessian-matrix', 'constraints', 'optimization']"
2573202,Prove $\mathrm{exp}(\mathrm{Tr}(A)) = \det(\mathrm{exp}(A))$ via maximal tori,"This is an exercise in Kris Tapp's little AMS booklet Matrix Groups for Undergraduates . I have reviewed the proof given earlier in the text but that proof relies on properties of exp and other aspects of commuting matrices, the definition of the derivative and other simpler observations.  Ah! Commuting matrices must be one  key here but I am afraid I haven't made enough of  the connections. My feeling is that conjugation will be helpful as well, since all maximal tori are conjugates of each other.
I believe I have all the parts of the jigsaw puzzle but have yet to fit the appropriate pieces. Maximal Tori is the name of the Chapter (#9) and they are, hence, a NEW topic, which again points toward their properties to be the glue to  hold the pieces together. Any suggestions?","['eigenvalues-eigenvectors', 'matrices', 'determinant', 'trace', 'linear-algebra']"
2573245,Let$A$ be a $3\times3$ real symmetric matrix such that $A^6=I$ . Then $A^2=I$,Let $A$ be a $3\times3$ real symmetric matrix such $ A^6=I$ . Then $A^2=I$ . How can I prove this statement is true or false? As it is given $A$ is symmetric so $A=A^T$ . Also $ A^6=I$ . But the main problem is that I can't operate $A^{-1}$ on both sides whether it is invertible or not. Can any help me what should I do?,"['matrices', 'symmetric-matrices', 'linear-algebra']"
2573289,A continuous surjection is proper if and only if pre-images of compact sets are compact,"Dugundji's Definition: A map $f:X\to Y$ between topological spaces is called perfect (or proper ), if it is a closed continuous surjection such that each fiber $f^{-1}(\{y\})$, $y\in Y$, is compact. Show that a continuous surjection $f:X\to Y$ between topological spaces is proper if, and only if, $f^{-1}(K)\subset X$ is compact, whenever $K\subset Y$ is compact. My try: $(\Rightarrow)$ We need just to show that $K\subset Y$ compact $\Rightarrow f^{-1}(K)\subset X$ compact. Let $K\subset Y$ be a compact set and let $\scr U$ be an open convering of $f^{-1}(K)$. Then $\scr F$$=\{F_U\,:\, U\in \scr U\}$, $F_U:=X-U$, is a family of closed sets. Since $f$ is closed, $\scr V$$=\{Y-f(F_U)\,:\,U\in \scr U\}$ is a family of open subsets of $Y$. But... what then? $(\Leftarrow)$ It is obvious that each fiber $f^{-1}(\{y\})$ is compact, since every $\{y\}\subset Y$ is compact. Let $F\subset X$ be closed. How can I show that $f(F)\subset Y$ is closed? (in order to show that $f:X\to Y$ is closed)","['closed-map', 'general-topology', 'compactness']"
2573292,Does the number a: a=cos(a) (fixed point for cosine = 0.73908513321516...) have any other interesting properties,Can it be expressed in terms that do not directly or indirectly require the cosine calculation? Is there a way to compute it without iteration? I presume it is irrational...,"['fixed-point-theorems', 'numerical-methods', 'trigonometry']"
2573294,Bounded $L^2$ expectation implies convergence in $L^2.$,"In probability at least four different types of convergence are considered: $a)$ Almost sure convergence $b)$ Convergence in probability $c)$ Convergence in $L^p$ $d)$ Convergence in distribution It is known (see e.g. the book of Karr: ""Probability"", Springer) that $$a)\,\Rightarrow \, b)\,\Rightarrow \, d)$$ and that $$c)\,\Rightarrow \, b).$$
All the other implications are false and it is possible to find counterexamples to them here: Convergence types in probability theory : Counterexamples Moreover, convergence in $L^p$ implies convergence in $L^q$ if $p>q$ and the space is finite. Now, let ${x_n}$ be a family of bounded random variables defined over a given probability space that converge a.s. to a bounded random variable $x.$ The following statements are true or false? 1) If $\mathbb E(x_n^2)\leq \mathbb E(x^2)$ then $x_n$ converges to $x$ in $L^2.$ 2) If $\mathbb E(x_n^2)=1$ $\forall n$ then $lim_{n\to \infty}\mathbb E(x_n)=\mathbb E(x).$ 3) If $\mathbb E(x_n^2)=1$ and $\mathbb E(x_n^3)=c\in \mathbb R\,$ $\,\forall n$ then $x_n$ converges to $x$ in $L^2.$","['real-analysis', 'probability', 'probability-theory']"
2573300,Find the length of a diagonal in an inscribed (non-regular) hexagon,"I stumbled upon an old problem on the site Gifted Mathematics . In the inscribed hexagon $ABCDEF$ of area $54$, we know that AB=CD=EF=5, BC=DE=2, AF=11. It is required to find the value of BE. Applying Bramaguptha's formula to trapezoids ABEF and BCDE (and feeding it to  Wolfram Alpha...), it is possible to find the answer, but the formulas are very ugly. Is there a smarter way to do it?","['recreational-mathematics', 'geometry']"
2573376,Second directional derivative and Hessian matrix,"I am reading the following from the book Deep Learning , and I have the following questions. I don't quite understand second directional derivatives. The first directional derivative of a function $f:\mathbb{R}^m\to\mathbb{R}$ in the direction $u$ represents the slope of $f$ in the direction $u$. So what does the second directional derivative along the direction $u$ represent? In the above paragraph, I understood that $d^THd$, the second directional derivative of $f$ in the direction $d$ ($||d||_2=1$), is given by the corresponding eigenvalue when $d$ is an eigenvector of $H$, because if $d$ is an eigenvector of $H$ then $d^THd=d^T\lambda_d d=\lambda_d d^Td=\lambda_d$. However, I don't understand the statement ""For other directions of $d$, the directional second derivative is a weighted average of all the eigenvalues, with weights between $0$ and $1$""::--Since $H$ is real symmetric, $H$ has $m$ independent orthogonal eigenvectors, which form a basis for $\mathbb{R}^m$. Thus, if $d$ is not an eigenvector, then $d=c_1x_1+\cdots +c_mx_m$ for some scalars $c_i$s and eigenvectors $x_i$s. Thus, $$d^THd=d^TH(c_1x_1+\cdots +c_mx_m)\\=d^T(c_1\lambda_1x_1+\cdots +c_m\lambda_mx_m)\\=c_1^2||x_1||^2\lambda_1 +\cdots +c_m^2||x_m||^2\lambda_m$$, which is ofcourse the weighted average of all the eigenvalues of $H$. But I don't understand why the weights lie between $0$ and $1$ as given. In fact, there is no reason to believe that the weights $c_i^2||x_i||^2$ to be in the range $(0,1)$. Also, I don't understand the statement ""The maximum eigenvalue determines the maximum second derivative, and the minimum eigenvalue determines the minimum second derivative"". Can you explain this?","['multivariable-calculus', 'hessian-matrix', 'eigenvalues-eigenvectors', 'linear-algebra']"
2573402,Book recomendation for Elliptic-curve cryptography,"I have already taken a course on Cryptography, The course focused mainly on the  public-key cryptography based on the algebraic structure of elliptic curves over finite fields. Now, I would like to deepen this topic. I've been searching for a book in this line, but haven't found many good recomendations. I would like to find a book that not only explains the theoretical aspect of the Elliptic Curve, but also by means of exercice and examples put into practice the theoretical content. Any recommendations on what books or what material may be helpful?
Thanks!","['elliptic-curves', 'abstract-algebra', 'cryptography', 'book-recommendation']"
2573436,How to compute $\sum^k_{i=0}{k\choose i}\frac{\prod_{j=0}^{i-1}(x+jm)\prod_{j=0}^{k-i-1}(y+jm)}{\prod_{j=0}^{k-1}(x+y+jm)}$?,"This basic course on probability and statistics is the first course where I feel like a total idiot... especially since I've already forgotten much from the basic courses at discrete mathematics and analysis. Sigh... The task from a former test: A HDD contains $x+y$ infected programs; $x$ are infected by malware $X$ and $y$ are infected by malware $Y$. The user runs random programs. Each time a program infected by malware $X$ is run $m$ uninfected programs become infected by malware $X$; same for malware $Y$. Compute the probability that when the $k$th infected program is run it is infected by malware $X$. Let $X_1=1$  iff the first infected program is infected by $X$, similary let's define $Y_1$, $X_2$, etc. We have: $\operatorname{P}(X_1)=\frac{x}{x+y}$ $\operatorname{P}(Y_1)=\frac{y}{x+y}$ $\operatorname{P}(X_2)=\operatorname{P}(X_1)\operatorname{P}(X_2|X_1)+\operatorname{P}(Y_1)\operatorname{P}(X_2|Y_1)=\frac{x}{x+y}\frac{x+m}{x+m+y}+\frac{y}{x+y}\frac{x}{x+y+m}$ $P(Y_2)=\operatorname{P}(X_1)\operatorname{P}(Y_2|X_1)+\operatorname{P}(Y_1)\operatorname{P}(Y_2|Y_1)=\frac{x}{x+y}\frac{y}{x+m+y}+\frac{y}{x+y}\frac{y+m}{x+y+m}$ $\operatorname{P}(X_3)=\frac{x}{x+y}\frac{x+m}{x+m+y}\frac{x+2m}{x+2m+y}+\frac{y}{x+y}\frac{x}{x+y+m}\frac{x+m}{x+m+y+m}+\frac{x}{x+y}\frac{y}{x+m+y}\frac{x+m}{x+m+y+m}+\frac{y}{y+m}\frac{y+m}{x+y+m}+\frac{x}{x+y+2m}$ More generally: $\operatorname{P}(X_k)=\sum^k_{i=0}{k\choose i}\frac{\prod_{j=0}^{i-1}(x+jm)\prod_{j=0}^{k-i-1}(y+jm)}{\prod_{j=0}^{k-1}(x+y+jm)}$ This is madness for me. That's not even binomial theorem, albeit it's a little bit similar. Am I doing something wrong? Or how am I supposed to get a closed formula from this sum?","['binomial-coefficients', 'summation', 'probability', 'discrete-mathematics']"
2573438,Name of a degenerate foliation,"Last summer I went to a talk where the speaker had a specific name for a certain kind of degenerated foliation on a manifold. Sadly, I forgot that name. Question : what is that name ? (Sorry if my question is vague) e.g. If you look at the level sets of $f(x,y)= x^2-y^2$ on $\mathbb R^2$ you get that sort of degenerated foliation.",['differential-geometry']
2573439,Proving that irreducible components of Zariski closed subsets are minimal prime ideals,"So the question is to prove that if $X = Z(\mathfrak{U})$ is a Zariski-closed subset in $A^n$, then $Y = Z(\mathfrak{P})$ is an irreducible component of $X$ if and only if $\mathfrak{P}$ is a minimal prime ideal containing $\mathfrak{U}$ in $\mathbb{k}[x_1, \ldots, x_n]$ I get that since $A^n$ is Noetherian, there is a finite decomposition into $X = X_1 \cup X_2\ \cup \cdots \ X_r$, with each $X_i = Z(\mathfrak{Q}_i)$ for some prime $\mathfrak{Q}_i$ in $\mathbb{k}[x_1, \ldots, x_n]$, with (I think) $\mathfrak{U} = \mathfrak{Q}_1 \cap \cdots \cap \mathfrak{Q}_r$. So my job is to show that if $\mathfrak{P}$ is a minimal prime ideal of this kind then $\mathfrak{P} = \mathfrak{Q}_i$ for some $1 \leq I \leq r$, and that if $Y = X_i$ then $\mathfrak{P}$ is a minimal prime ideal &c &c. And then I'm stuck. I mean, intuitively I can see why it has to be so - if $\mathfrak{P}$ isn't minimal then there's something in $Z(\mathfrak{P})$ which isn't in $Z(\mathfrak{U})$ - or conversely if there's no $\mathfrak{Q}_i$ such that $\mathfrak{P}$ = $\mathfrak{Q}_i$ then you can't have $Y = X_i$,  but I can't see a way to do it formally.","['algebraic-geometry', 'dimension-theory-algebra']"
2573501,Finding the center of mass with varying density,"Given a triangle $\mathrm{A}\left(2,0\right),\ \mathrm{B}\left(1,3\right),\
\mathrm{C}\left(5,2\right)\ \mbox{with}\ \rho\left(x,y\right) = x$; I need to find it's centre of mass ?. I know I need to integrate the density formula over the region, but I don't understand how to get the limits for the integrals to calculate the area. Do I need to find formulas for the lines and somehow use those ?.","['multivariable-calculus', 'calculus']"
2573542,Image of simply connected open set is simply connected?,"I feel the following assertion does not hold but I did not come up with an counter-example so far. Suppose $f:X\to Y$ is continuous. Furthermore, suppose $X$ is simply connected. So $f_\star:\pi(X,x_0)\to\pi(Y,f(x_0))$. The question is whether $\exists U\ni f(x_0)$ which is simply connected where $U$ is some set(I hope). I do not see any reason to hope $\pi(Y,f(x_0))=1$ here. Furthermore $f(X)\to Y$ is continuous map from subspace topology. So I truly want $f:X\to f(X)$ inducing $f_\star:\pi(X,x_0)\to\pi(f(X),f(x_0))$. There is no hope for homeomorphism. So I still cannot conclude triviality of $\pi(f(X),f(x_0))$. Q1: Is my expectation correction? Q2: What is the counter-example or what are essential ingredient to construct such a counter-example? It looks I need to somehow make sure $Z\to \pi(f(X),f(x_0))$ injection minimal.","['algebraic-topology', 'general-topology', 'continuity']"
2573557,Continuity of coordinate functions,"Let $X$ and $Y$ be topological spaces, $X\times{Y}$ with the product topology, and $f$ a function from $X\times{Y}$ to some other topological space. I know that the fact that the functions $f(x,.)$ and $f(.,y)$ are continuous for all $x\in{X}$ and $y\in{Y}$ doesn't imply that $f$ is continuous, but I just can't find a counter-example.","['continuity', 'general-topology', 'functions']"
2573584,For which $x \in \mathbb{R}$ does the following series converge?,"$$\sum_{n=1}^\infty \frac{3x^n}{2+x^{4n}}$$ I think the solution is $|x| \neq 1$
But I don't really know how to prove it rigorously. Thanks in advance :)","['convergence-divergence', 'divergent-series', 'sequences-and-series', 'analysis']"
2573598,Why is $\cos(45°) = \frac{\sqrt{2}}{2} \simeq 0.7071$ rather than $0.5$?,"I'm trying to wrap my head around trigonometry.
Working in degrees we get: $$\cos(0°) = 1$$ $$\cos(90°) = 0$$ Half way between $0$ and $90$ degrees we get $45$ degrees, so it seems logical to me that $\cos(45°)$ would give $0.5$, but instead we get $\frac{\sqrt{2}}{2} \simeq 0.7071$, why is that?",['trigonometry']
2573601,"On polar normal coordinates, Jacobi field and parallel transport.","Setup. Let $(M,g)$ be a Riemannian manifold of dimension $2$, let $m\in M$ and let $U$ be an open neighborhood of $0$ in $T_mM$ such that $\exp_m\colon U\rightarrow M$ is an embedding and let $h:={\exp_m}^*g$. According to Gauss' lemma, in polar coordinates $[r,\theta]$ on $U\setminus\{0\}$, one has the following decomposition: $$h=\mathrm{d}r^2+f(r,\theta)^2\mathrm{d}\theta^2.$$ Let $u$ and $v$ be two vectors of $T_mM$ which are orthogonal for $h_0$ and assume that polar coordinates have been chosen such that the angle associated to $u$ is $0$. Question. Let $c\colon r\mapsto ru$, let $r\mapsto V(r)$ be the parallel transport of $v$ along $c$ and let $\cdot'$ be the connection induced on $c^*TM$ by $\nabla$ the Levi-Civita connection of $M$, then:
  $$J(r):=f(r,0)V(r)$$
  is the Jacobi field along $c$ with initial values $J(0)=0$ and $J'(0)=v$. My attempts. I have to prove that $J$ satisfies the following second order linear differential equation:
$$J''-R(\dot{c},J)\dot{c}=0.$$
The first term is easy to compute, as $V$ is parallel along $c$, one has:
$$J''(r)=\frac{\partial^2f}{\partial r^2}(r,0)V(r).$$
However, I am having a hard time with the curvature term, I am aware that by definition, one has:
$$R(\dot{c},J)\dot{c}=\nabla_{\dot{c}}\nabla_J\dot{c}-\underbrace{\nabla_{J}\nabla_{\dot{c}}{\dot{c}}}_{=0}-\nabla_{[\dot{c},J]}\dot{c}$$
and the middle term is vanishing since $c$ is a geodesic path and here I am stuck. An other strategy I have in mind is finding a smooth map $\ell\colon I\times]-\varepsilon,\varepsilon[\rightarrow M$ such that:
$$J(r)=\frac{\partial\ell}{\partial s}(r,0)$$
and for all $s$, $t\mapsto\ell(t,s)$ is a geodesic of $M$. I have not dig in depth this approach yet. Another idea could be to prove that one has $J(r)=T_{ru}\exp_m(v)$, but it does not seem too doable. Any enlightenment will be greatly appreciated.","['vector-fields', 'riemannian-geometry', 'differential-geometry']"
2573639,"All possible solutions for $x_1+x_2+x_3=25$ with conditions for $x_1, x_2$ and $x_3$ ranges","Find all possible solutions for $x_1+x_2+x_3=25$. Numbers have to fulfill the following conditions: $0 \leq x_1 \leq 5,  2 < x_2 \leq 10$ and $5 \leq x_3 \leq 15$. Solutions $(a,b,c)$ and $(b,a,c)$ are considered different. $x_1, x_2$ and $x_3$ are natural numbers. I've tried it this way: I put $x_1$ as fixed $(0)$ then I put $x_2$ as fixed $(3)$ now I have $25-3=22$ which means that no matter what I put as $x_3$ I cannot get $22$. So, I see that $x_2$ can be min $(22-15-0=7)$ therefore I see that when $x_1=0, x_2$ can be min $7$ so the number of possible combinations for $x_1=0$ is $4$. Going up by one from $x_1 = 0 .. 5$, I can also increase number of $x_2$ that fits so all together there are $4+5+6+7+8+9= 39$ combinations. I've done the same for the $x_2$ and $x_3$: Fix one of them and calculate the number of possibilities, but apparently the number that I get is incorrect. I do not have the correct number to check whether I'm just making a numerical mistake or whether my logic is flawed. Any help is appreciated. EDIT: $x_1, x_2$ and $x_3$ are natural numbers.",['discrete-mathematics']
2573646,Turning sum to integral representation,"I'd like to turn this sum: 
\begin{align}\sum_{n=0}^{\infty} \frac{x^{n+1}}{3^{n+1}(n+1)} \end{align}
into an integral $\displaystyle \int_{a}^{b} g(x) \space dx$. There seems to be many methods to either change or approximate sums as integrals. So I've become confused which approach would work. In Is it possible to write a sum as an integral to solve it? robjohn used $\int_0^\infty e^{-nt}\,\mathrm{d}t=\frac1n$ which looks similar to a Laplace Transforms. I can't see how he gets rid of the n's so I'm not able to apply it here otherwise it seems promising.  But looking elsewhere there are also approximations methods such as: Turning infinite sum into integral which even more obscure at least to me. How do I convert this sum to an integral?","['calculus', 'integration', 'riemann-sum', 'summation', 'sequences-and-series']"
2573697,Is there a known closed form sequence for algebraic number one-to-one with the natural numbers?,"We have a closed form recursive sequence for the rational numbers, where they appear only once. It is called the Calkin-Wilf sequence. Namely, $q_{i+1}=\frac{1}{2⌊q_i⌋-q_i+1},q_1=1$. Has someone found out an analog for algebraic numbers? I couldn't find any when searching but would be very surprised if one wasn't discovered already or serious public attempts were made to find one.","['algebraic-number-theory', 'sequences-and-series']"
2573754,find the limit of sequence $\ln(n) ^{1/n}$,"I'm not sure if I can use the squeeze theorem??? I did the following, not sure if it's correct $$1^{1/n} < \ln(n)^{1/n} < n^{1/n}$$ for n greater than or equal to 3 and as  $$\lim_{n\to\infty} 1^{1/n} = 1\qquad \text{and}\qquad
\lim_{n\to\infty} n^{1/n} = 1\qquad \text{(it's a fact)}$$
 then $$\lim_{n\to\infty} \ln(n)^{1/n} = 1$$ ??????","['real-analysis', 'sequences-and-series', 'proof-verification', 'limits']"
2573771,How to find the area of the triangle which intersects a line in a rectangle?,"The problem is as follows: $ABCD$ is a rectangle, $EA=5\,cm$ , $BE=3EA=\frac{AD}{2}$. Find the area of the triangle $BNC$. What I tried so far is pictured below, as from Pitagorean theorem I reached to $EC=\, 15\sqrt{5}$: but my problem lies on how to find the sides of $EN$ and $NC$ Is there anything that I'm missing? In this case help which would include a reworked diagram with letters and labels and not segment line notation be much appreciated.",['geometry']
2573797,Confusion about Green's functions for inhomogeneous boundary conditions but no forcing.,"I have recently seen Green's functions for the first time but for some reason can't seem to get my head around them. I have been told we can use them to solve differential equations up to an integral, however I am having difficulties seeing why they should work (probably from my inexperience). For example if we are asked to solve $\mathcal{L}(u(x)) = f(x)$ on say $[0,1]$ with $u(0) = u(1) = 1$ where $\mathcal{L}$ is any second order Sturm-Liouville operator (say take $\mathcal{L} = \frac{d^2}{dx^2}$) then with Green's functions we would have $G(x,\xi)$ satisfies $\mathcal{L}(G) = \delta(x-\xi)$ and our solution for $u$ is $u(x) = \int\limits_0^1 G(x,\xi) f(\xi)\ d\xi$. Now consider the case where $f = 0$ everywhere. This means that in our integral we have $f(\xi) = 0$ everywhere so we get $u(x) = 0$ everywhere as $G$ has to be bounded (at least that is what I think it has to based on all the examples I have seen so far) which is wrong. I will be glad if someone could point out to me where I am making my mistake. Thank you in advance.","['greens-function', 'dirac-delta', 'sturm-liouville', 'ordinary-differential-equations']"
2573829,How can we define the notion of multiplication without numbers? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 6 years ago . Improve this question Say I have a two lines: ""_______"" ""_________________"" Addition is clear: ""________________________"" Subtraction is clear: ""__________"" But multiplication and division seem to require numbers. What's going on?","['number-theory', 'arithmetic']"
2573842,No Immediate Successor? No Largest Element?,"I am reading through the proof of the following theorem in Munkres' topology: Every well-ordered set $X$ is normal in the order topology. Here is a troubling quote from Munkres' We assert that every interval of the form $(x,y]$ is open in $X$: If $X$ has a largest element and $y$ is that element, then $(x,y]$ is just a basis element about $y$. If $y$ is not the largest element of $X$, then $(x,y]$ equals the open set $(x,y')$, where $y'$ is the immediate successor of $y$. First, what if $y$ isn't the largest element; will $(x,y]$ still be open? Secondly, what if $y$ has no immediate successor? I don't quite follow Munkres' reasoning.","['general-topology', 'proof-explanation']"
2573877,Expected value of exponential of maximum of iid exponential random variables,"Suppose that we have $X_1,\ldots,X_n$ iid random variables with distribution each  $\mathcal{Exp}(1)$. Also we have a random variable $M_n=\max(X_1,X_2,\ldots,X_n)$ (where $n$ is not a random variable) with CDF $F_{M_n}=(1-e^{-x})^n,$ $x\geq 0$. And we are asked to calculate $\mathbb{E}(e^{M_n})$. So I imagine that because we know the distribution of $M_n$ we will just must to calculate $$\mathbb{E}(e^{M_n})=\int_0^\infty e^{\max(X_1,\ldots,X_n)}F_{M_n} \, dx$$ but what's the way that we have to handle  $e^{\max(X_1,\ldots,X_n)}$.","['probability-theory', 'expectation']"
2573889,"Find from first principle, the derivative of","Find from first principle, the derivative of
$$f(x)=\dfrac {ax+b}{\sqrt {x}}.$$ My Attempt:
$$f(x)=\dfrac {ax+b}{\sqrt {x}}$$
$$f(x+\Delta x)=\dfrac {a(x+\Delta x)+b}{\sqrt {x+\Delta x}}$$
where $\Delta x$ is a small increment in $x$.
From first principle,
$$f'(x)=\lim_{\Delta x\to 0} \dfrac {f(x+\Delta x)-f(x)}{\Delta x}$$
$$=\frac {\left(\frac {ax+a\Delta x+b}{\sqrt {x+\Delta x} - \frac {ax+b}{\sqrt {x}}}\right)}{\Delta x}$$","['derivatives', 'calculus']"
2573957,Derive Barycentric coordinate distance formula,"please pardon the poor formatting. (I'll work on learning it in time; I just started this account to see help with this question.) I've recently started learning about affine geometry and Barycentric coordinates, and I have a question regarding the distance formula for Barycentric coordinates. The Wikipedia page on Barycentric coordinate system gives two versions of this formula, and while I have no trouble proving the first, (first I took the dot product of the displacement vector $PQ$ while setting $A$ to the origin, much as the author of the Mathematical Gazette, cited by Wikipedia, did; I also proved it by setting the origin to the circumcenter of triangle $ABC$. Also, I followed another citation in said article which should have lead to an answer-but alas, that article stated the result without even a ""proof is obvious."") my ""proof"" of the second relies on some (very simple) algebraic manipulation which lacks geometric intuition/motivation. Yes, it works, but there should be a better argument. (Both forms are written below.) Essentially, my question is this: can anyone help me prove the second form, but without first proving the first form? (Presumably, such a proof would provide the geometric intuition I'm looking for.) I've been such on this for days and it's starting to get to me-I've tried many different approaches. Setting:
Triangle $ABC$ is positively oriented; $P, Q$ are vectors in the plane of $ABC$, with $P, Q$ having normalized/homogeneous Barycentric coordinates $P= [p_1, p_2, p_3], Q= [q_1,q_2,q_3].$ Thus, displacement vector $PQ= [q_1-p_1,q_2-p_2,q_3-p_3]=[x,y,z],$ with $x+y+z=0.$ Form $1$: (no problems here)
$\textrm{dist}(P,Q)^2 = -yza^2-xzb^2-xyc^2.$ Form $2$: (subject of my question-and yes, I'm familiar with the polarization identity and its relation to the coefficients below-also familiar with the circumcenter's Barycentric coordinates and the similarity to those coefficients but I'm not sure how to relate the two in a proof.) $\textrm{dist}(P,Q)^2 = \frac12\{(b^2+c^2-a^2)x^2 + (a^2+c^2-b^2)y^2 + (b^2+a^2-c^2)z^2\}.$ Thanks for any help/guidance-it's much appreciated. This one has me stumped.","['euclidean-geometry', 'barycentric-coordinates', 'geometry']"
2573974,Putting $m$ distinct balls to $n$ identical boxes with each box has at least $2$ balls,"If we were to put $m$ distinct balls to $n$ identical boxes with no empty boxes, there would be $S(m,n)$ ways to do it where $S(m,n)$ is the Stirling number of the second kind. But when it comes to the case where we should put the balls with each box has at least $2$ balls, I am stuck here because this is not an ordinary surjection anymore. What I tried so far was trying to examine the problem case by case. Let $\sigma(m,n)$ be the number of ways of putting $m$ distinct balls to $n$ identical boxes with each box has at least $2$ balls. For the case $m < 2n$, we have $0$ way to do this, obviously. For the case $m = 2n$, I think the number of ways $$\sigma(m,n) = \sigma(2n,n) = \frac{1}{n!} \prod_{\scriptstyle i = 0\atop\scriptstyle i \ even}^{2n-2} \binom{2n-i}{2}$$ because we are partioning $2n$ as $2+2+...+2$ where there are $n$ many $2$'s and since the boxes are identical, we are dividing it by $n!$ in order to avoid overcounting. It is also obvious that for $m \ge 2$ and $n = 1$, $\sigma(m,n) = \sigma(m,1) = 1$. But for other cases, I cannot find any way to find a closed form expression or a recurrence relation for $\sigma(m,n)$ because I think I need to consider both number of ways of partioning $m$ into $n$ classes and number of ways of seperating balls for each partioning in order to find a closed form expression. And in order to find a recurrence relation, I have to proceed as deriving a recurrence relation for Stirling numbers of the second kind but here there are not two cases as in the Stirling numbers $\bigg(S(m,n) = S(m-1,n-1) + nS(m-1,n)$, here the two cases are ""when we put $m-1$ balls into $n-1$ boxes"" and ""when we put $m-1$ balls into $n$ boxes"" $\bigg)$. If I could find a way to divide the problem into some disjoint cases, I would have found a recurrence relation but no luck there so any suggestion will be appreciated.","['stirling-numbers', 'combinatorics', 'balls-in-bins', 'discrete-mathematics']"
2573976,The limit points of an open interval (open set),"I am not sure I understand why for the open set $(a,b)$, the limit points are $[a,b]$. Why are $a,b$ now included as limit points? Is this because we can somehow find a sequence in the open interval converging to these points?","['general-topology', 'real-analysis']"
2574010,"Let $f : \mathbb{R}^2\to \mathbb{R}^2$ be defined by the equation $f(r,\theta) = (r \cos \theta, r \sin \theta)$.","Let $f : \mathbb{R}^2\to \mathbb{R}^2$ be defined by the equation $f(r,\theta) = (r \cos \theta, r \sin \theta)$. It is called the polar coordinate transformation. (a) Calculate $Df$ and $\text{det} Df$. (b) Sketch the image under $f$ of the set $S = [1 , 2] \times [0, \pi]$. [Hint: Find the images under $f$ of the line segments that bound $S$ .] I have calculated that $Df(r,θ)$ is the matrix $$\begin{bmatrix} \cos\theta &-r \sin\theta \\ \sin\theta & r\cos\theta \end{bmatrix}$$ so its determinant is $r$. I do not know how I can solve (b), could someone help me please? Thank you very much. Edit: I think that in (b), the image is this:","['real-analysis', 'calculus', 'multivariable-calculus', 'analysis', 'vector-analysis']"
2574046,Diagonalizability of $A \in M_{n}(\Bbb{R})$ (Upper traingular matrix) with all diagonal entries 1 and $A \neq I$?,"If $A \in M_{n}(\Bbb{R})$ is an Upper triangular matrix with diagonal entries $1$ such that  $A \neq I$, then what can we say about the diagonalizability of $A$ ? I know that if the matrix has distinct eigenvalues or the set of eigenvectors are linearly independent then the matrix is diagonalizable. And that the eigenvalues of the triangular matrices are given by the diagonal elements like here and here , but they work nocely if we had distinct elements on the main diagonal. But in my case I have same value 1 on the main diagonal, how can I approach about the diagonalizability of the matrix?","['matrices', 'diagonalization', 'linear-algebra']"
2574083,"Presentation $\langle x,y,z\mid xyx^{-1}y^{-2},yzy^{-1}z^{-2},zxz^{-1}x^{-2}\rangle$ of group equal to trivial group","Problem: Show that the group given by the presentation $$\langle x,y,z  \mid xyx^{-1}y^{-2}\, , \, yzy^{-1}z^{-2}\, , \, zxz^{-1}x^{-2} \rangle $$ is equivalent to the trivial group. I have tried all sorts of manners to try to show that the relations given by the presentation above imply that $x=y=z=e$. However, I am stuck and would appreciate any hints as to how I should move forward.","['algebraic-topology', 'abstract-algebra', 'group-theory', 'group-presentation']"
2574128,Prove that these perpendicular distances are in G.P.,"Let $BC$ be the chord of contact of the tangents from a point $A$ to the circle $x^2+y^2=1$. $P$ is any point on the arc $BC$. Let $PX, PY$ and $PZ$ be the lengths of the perpendiculars from P on the line $AB,  BC$ and $CA $ respectively then prove that $PX,  PY$,  and $PZ$ are $G. P.$ My approach : I considered point $A$ to be $(\alpha, \beta)$. Using this I got the equation of common chord as $$\alpha x+\beta y-1=0$$. Now I considered point P to be $(\gamma,  \delta)$.  For this circle the pair of tangents can be given as $$(x^2+y^2-1)({\alpha}^2+{\beta}^2-1)=(\alpha x +\beta y -1)^2$$ . Simplifying this we get $$(x-\alpha) ^2 + (y-\beta)^2=(y\alpha-x\beta)^2$$. Now I can get the perpendicular distances to the pair of tangents and the common chord from their equations but I am stuck in finding the perpendicular distance in the case of pair of tangents because I am not able to factorise it into pair of two straight lines nor do I know any formula to find the perpendicular distances without factorising the expression. Any other method to solve this problem is also appreciated.","['circles', 'euclidean-geometry', 'trigonometry', 'geometry']"
2574132,Exercise books in linear algebra,"I'm going to find some Exercise books in linear algebra  and I was wondering if there are some exercise books (that is, books with solved problems and exercises) 
The books I'm searching for should be: full of hard , non-obvious , non-common , and thought-provoking problems; rich of complete , step by step , rigorous , and enlightening solutions; Thanks in advance","['reference-request', 'book-recommendation', 'linear-algebra']"
2574133,How do these authors get to this result?,"I am not sure if this question satisfies the guidelines, but here goes. I'm reading a paper (text is below) from a financial mathematics journal, and the authors make a calculation step that I don't get. They reach the conclusion that $$E_i(\theta)=\frac{\alpha y + \beta x_i}{\alpha +\beta}$$ Whereas I would guess that the result should be either $$E_i(\theta)=y$$ or $$E_i(\theta):=E_i(\theta|x_i)=E_i(x_i)-E_i(\epsilon_i)=x_i$$ In the second case I'm interpreting the expectation as conditional expectation, and this seems to me the most sensible interpretation. However the author's result is completely different as you can see, and I have no idea how they get to that result . In fact, if this weren't published in a peer reviewed journal, I would assume they made a very weird mistake. But since it's in a peer reviewed journal, I must miss something. Here is the entire text: Here is some context: Here is the part my question is about: How do they get to this result? How should I interpret the setup of their problem so that the conclusion makes sense?","['conditional-expectation', 'finance', 'probability']"
2574158,Every analytic hypersurface can be written as a locally finite union of its irreducible components,"Let $M$ be a complex manifold and $V\subseteq M$ be an analytic hypersurface i.e. $V$ is locally a zero set of some holomorphic function which can be treated as a defining function of $V$. Want to prove that for each $x\in V$ one can find an open set $U_{x}$ of $M$ containing $x$ s.t. $V\cap U_{x}=\cup_{i=1}^{k}V_{i}\cap U_{x}$, where $V_{i}\cap U_{x}$ is an irreducible component of $V\cap U_{x}$ in $U_{x}$ for each $i=1\ldots k$. Now, pick an $x\in M$. For any subset $A$ of $M$ define the germ of $A$ at $x$ as the image of $A$ under the map $\mathcal{P}(M)\mapsto \mathcal{P}(M)$/~, where for $X,Y\subseteq M$, $X$~$Y$ if there exists an open set $U$ in $M$ containing $x$ s.t. $X\cap U=Y\cap U$. I'll denote it by $(A,x)$. $(A,x)$ will be called an analytic germ if A is an analytic set in some neighbourhood of $x$ (Analytic sets are locally zero set of finitely many holomorphic functions). Lastly, an analytic germ $(A,x)$ is irreducible if $(A,x)=(A_{1},x)\cup(A_{2},x)$ implies $(A,x)=(A_{i},x)$, $i=1$ or $2$, where $(A_{i},x)$ is an analytic germ for each $i=1,2$. In the book ""Complex Analytic and Differential Geometry"" by Jean-Pierre Demailly it has been shown that every analytic germ has a finite decomposition into irreducible analytic germs (P-92, Theorem 4.7). I suspect that this can be used to prove the above. However, I am stuck in the following: Fix $x\in V$. Then we have $(V,x)=\cup_{i=1}^{k}(V_{i},x)$, where $(V_{i},x)$ is irreducible for each $i=1,\ldots k$. The equality here implies that there exists an open set $U_{x}$ in $M$ containing $x$ s.t. $V\cap U_{x}=\cup_{i=1}^{k}V_{i}\cap U_{x}$. But how irreducibility of $(V_{i},x)$ guarantees an open neighbourhood $W_{x}$ of $x$ s.t. $V_{i}$ is irreducible in $W_{x} ?$ Any help is appreciated.","['several-complex-variables', 'complex-geometry', 'algebraic-geometry']"
2574172,Is numerical approximation the only option when derivative cannot be expressed explicitly as an expression?,My problem is the following: Are there any differentiable functions on $\Bbb R$ for which we don't know or can't find an explicit expression for the derivative? So is approximating the derivative numerically the only choice?,"['derivatives', 'real-analysis', 'calculus']"
2575647,Derivation of $\cos^2z+\sin^2z=1$,$$\sin^2z+\cos^2z=1\tag a \space \forall z \in \mathbb{C}$$ $(a)\div\sin^2z$ $$1+\cot^2z={\csc^2{z}}\tag b$$ $(a)\div\cos^2z$ $$\tan^2z+1=\sec^2z\tag c$$ $(b)$ and $(c)$ are derived from $(a)$. But where was $(a)$ originally derived from?,['trigonometry']
2575683,Are constructed mathematical syntaxes discouraged by the mathematical community?,"I get a little bit dyslexic/dyscalculia-like, when I try to interpret the more complicated arrangements of numbers [and symbols], used to describe set-theory and calculus. So I've taken to writing psuedocode, and drowning my statements in unnecessary parentheses [to help me nullify the need to remember an ""order of operations""]. 
I'm even working on a new personal standard math-markup-language, to help myself grasp an ever expanding range of topics. But that begs the question; assuming I find something that works for me and does not fail/contradict itself: ""Will I universally be expected [by the academic community of mathematicians] to translate my work into the pre-existing ""standard notation"" for it to be taken seriously; or is the academic field willing to accept different standards of mathematical expression, so long as they are fully realized [syntactically-standardized notations]? If a new way of describing the same maths/math crops up [alongside the tradional way of describing those maths]: does it have a fighting chance? Or is it something that is universally frowned upon and considered disreputable [in an academic setting]?","['number-theory', 'elementary-number-theory']"
2575738,Showing a twice differentiable function with positive hessian is convex.,"Let $f:\mathbb{R}^n\to\mathbb{R}$ be twice differentiable . If $f_{xx}( .)\geq 0$ for any $x\in\mathbb{R}^n$, then $f$ is convex. I think they mean $f_{xx}(.)$ is a positive matrix. As, $f$ is twice differentiable, we can see for any $x, y\in \mathbb{R}^n$, there exist $\theta_1, \theta_2\in (0,1)$, satisfying 
$$f(y)=f(x)+f_x(x).(y-x)+\theta_1(y-x)^Tf_{xx}(x+\theta_1\theta_2(y-x)).(y-x).$$ Hence $f(y)-f(x)\geq f_x(x).(y-x).$ Using the fact $f_x(x).(y-x)=\lim_{\lambda\to1}\frac{f(\lambda x+(1-\lambda)y)-f(x)}{1-\lambda}$, I get
$$f(y)-f(x)\geq \lim_{\lambda\to1}\frac{f(\lambda x+(1-\lambda)y)-f(x)}{1-\lambda}.$$ Now if I don't consider the $\lim_{\lambda\to1}$, then by some calculation, I can get $$\lambda f(x)-(1-\lambda)f(y)\geq f(\lambda x+(1-\lambda)y).$$ However, I'm not sure if this is a valid thought. Thank you.","['hessian-matrix', 'convex-analysis', 'analysis', 'proof-verification']"
2575794,Attempting to find an unified treatment of the separation axioms,"Disclaimer This question has been once edited to incorporate the suggestion of the answers given below but that post too contained gaps and so I have decided to rollback the post to its second version. Background Motivated by this answer , I have been trying to find an answer to the question that I have asked here . This post is an attempt to answer the question. For the definition of the spaces with which I have dealt see this page . Note that even though I write the conditions of being a $T_i$-space symbollicaly, don't take the expressions to be a formula of some formal language. Just read them as ""convenient abbreviation"" of the intended English language sentence. Definition. Let $(X,\tau)$ be a topological space. Then for each $A\subseteq X$ we define, $$\mathcal{N}_A:=\{U:A\subseteq U\land U\in \tau\}$$and $$\mathcal{C}_A:=\{V:A\subseteq V\land X\setminus V\in\tau\land (\exists U\in\mathcal{N}_A)(A\subseteq U\subseteq V)\}$$If $A=\{x\}$ then instead of $\mathcal{N}_{\{x\}}$ and $\mathcal{C}_{\{x\}}$ we will simply write $\mathcal{N}_{x}$ and $\mathcal{C}_{x}$. $T_0$-Space Theorem 1. A topological space $(X,\tau)$ is $T_0$ iff $$\forall x\forall y((\{x\}\not\subseteq \{y\}\land \{y\}\not\subseteq \{x\})\iff(\mathcal{N}_x\not\subseteq\mathcal{N}_y\lor \mathcal{N}_y\not\subseteq\mathcal{N}_x))$$ Proof. Let $x\ne y$ and let $X$ be $T_0$. Then since $X$ is $T_0$, by the definition of a $T_0$-space, either there exists open set $U\in \mathcal{N}_x$ such that $y\not\in U$ or there exists an open set $V\in \mathcal{N}_y$ such that $x\not\in V$. In the first case (since the condition $y\not\in U$ is equivalent to saying that $U\not\in\mathcal{N}_y$) we conclude that there exists $U\in \mathcal{N}_x$ such that $U\not\in \mathcal{N}_y$. Hence $\mathcal{N}_x\ne\mathcal{N}_y$, implying that $\mathcal{N}_x\ne\mathcal{N}_y\lor \mathcal{N}_y\ne\mathcal{N}_x$. The second case can be dealt similarly. The proof of the converse is trivial and hence skipped. $T_1$-Space Theorem 2. A topological space $(X,\tau)$ is $T_1$ iff $$\forall x\forall y((\{x\}\not\subseteq \{y\}\land \{y\}\not\subseteq \{x\})\iff(\mathcal{N}_x\not\subseteq\mathcal{N}_y\land \mathcal{N}_y\not\subseteq\mathcal{N}_x))$$ Proof. Let $x\ne y$ and let $X$ be $T_1$. Then since $X$ is $T_1$ by the definition of a $T_1$-space, there exists open set $U\in \mathcal{N}_x$ such that $y\not\in U$ and there exists an open set $V\in \mathcal{N}_y$ such that $x\not\in V$. Just like the proof of the previous theorem, we are then forced to conclude that there exists $U\in \mathcal{N}_x$ such that $U\not\in \mathcal{N}_y$ and there exists an open set $V\in \mathcal{N}_y$ such that $V\not\in \mathcal{N}_x$. Consequently $\mathcal{N}_x\not \subseteq \mathcal{N}_y\land \mathcal{N}_y\not\subseteq\mathcal{N}_x$. The proof of the converse is trivial and hence skipped. $T_2$-Space Theorem 3. A topological space $(X,\tau)$ is $T_2$ iff $$\forall x\forall y((\{x\}\not\subseteq \{y\}\land \{y\}\not\subseteq \{x\})\iff(\mathcal{C}_x\not\subseteq\mathcal{C}_y\land \mathcal{C}_y\not\subseteq\mathcal{C}_x))$$ Proof. Suppose $x,y\in X$ such that $x\ne y$ and $X$ is $T_2$. Since $X$ is $T_2$, there exists open sets $U, V$ such that, $x\in U$ $y\in V$ $U\subseteq X\setminus V$ The last condition implies that $\overline{U}\subseteq X\setminus V$. But observe that $\overline{U}\not\in \mathcal{C}_y$ (because $y\not\in X\setminus V$) but $\overline{U}\in \mathcal{C}_x$. Consequently it follows that $\mathcal{C}_x\not\subseteq \mathcal{C}_y$. In a similar manner we will be able to prove that $\mathcal{C}_y\not\subseteq \mathcal{C}_x$ and this would imply in turn that $\mathcal{C}_x\not\subseteq\mathcal{C}_y\land \mathcal{C}_y\not\subseteq\mathcal{C}_x$ and we are done. The proof of the converse is trivial and hence skipped. $T_3$-Space Theorem 4. A topological space $(X,\tau)$ is $T_3$ iff (here $A$ denotes a closed set) $$\forall x\forall A((\{x\}\not\subseteq A\land A\not\subseteq \{x\})\iff(\mathcal{C}_x\not\subseteq\mathcal{C}_A\land \mathcal{C}_A\not\subseteq\mathcal{C}_x))$$ Proof. Suppose $x\in X$ and $A\subseteq X$ such that $x\not\in A$ and $X$ is $T_3$. Since $X$ is $T_3$, there exists open sets $U, V$ such that, $x\in U$ $A\subseteq V$ $U\subseteq X\setminus V$ The last condition implies that $\overline{U}\subseteq X\setminus V$. But observe that $\overline{U}\not\in \mathcal{C}_A$ (because $A\not\subseteq X\setminus V$) but $\overline{U}\in \mathcal{C}_x$. Consequently it follows that $\mathcal{C}_x\not\subseteq \mathcal{C}_A$. To show that $\mathcal{C}_A\not\subseteq\mathcal{C}_x$ observe that $\overline{V}\subseteq X\setminus U$. But observe that $\overline{V}\not\in \mathcal{C}_x$ (because $x\not\in X\setminus U$) but $\overline{V}\in \mathcal{C}_A$. Consequently it follows that $\mathcal{C}_A\not\subseteq \mathcal{C}_x$. This implies in turn that $\mathcal{C}_x\not\subseteq\mathcal{C}_A\land \mathcal{C}_A\not\subseteq\mathcal{C}_x$ and we are done. To prove the converse observe that if $z\in A\cap B$ and $V\in \mathcal{C}_A$ then as $A\subseteq V$ and $x\in A$ so $x\in V$ and hence it follows immediately that $V\in \mathcal{C}_x$. Consequently it follows that $\mathcal{C}_A\subseteq \mathcal{C}_x$ and we are done. $T_4$-Space Theorem 5. A topological space $(X,\tau)$ is $T_4$ iff (here $A,B$ denotes closed sets) $$\forall B\forall A((B\not\subseteq A\land A\not\subseteq B)\iff(\mathcal{C}_B\not\subseteq\mathcal{C}_A\land \mathcal{C}_A\not\subseteq\mathcal{C}_B))$$ Proof. Suppose $A,B\subseteq X$ such that $A\cap B=\emptyset$ and $X$ is $T_4$. Since $X$ is $T_4$, there exists open sets $U, V$ such that, $B\subseteq U$ $A\subseteq V$ $U\subseteq X\setminus V$ The last condition implies that $\overline{U}\subseteq X\setminus V$. But observe that $\overline{U}\not\in \mathcal{C}_A$ (because $A\not\subseteq X\setminus V$) but $\overline{U}\in \mathcal{C}_B$. Consequently it follows that $\mathcal{C}_B\not\subseteq \mathcal{C}_A$. To show that $\mathcal{C}_A\not\subseteq\mathcal{C}_B$ observe that $\overline{V}\subseteq X\setminus U$. But observe that $\overline{V}\not\in \mathcal{C}_B$ (because $B\not\subseteq X\setminus U$) but $\overline{V}\in \mathcal{C}_A$. Consequently it follows that $\mathcal{C}_A\not\subseteq \mathcal{C}_B$. This implies in turn that $\mathcal{C}_B\not\subseteq\mathcal{C}_A\land \mathcal{C}_A\not\subseteq\mathcal{C}_B$ and we are done. To prove the converse without loss of generality let us assume that $B\subseteq A$. Then observe that if $B\subseteq A$ and $V\in \mathcal{C}_A$ then as $A\subseteq V$ and $B\subseteq A$ so $B\subseteq V$ and hence it follows immediately that $V\in \mathcal{C}_B$. Consequently it follows that $\mathcal{C}_A\subseteq \mathcal{C}_B$ and we are done. $T_5$-Space (I am indebted to Daniel Wainfleet for this theorem, see his remark below) Theorem 6. A topological space $(X,\tau)$ is $T_5$ iff $$\forall B\forall A((B\not\subseteq A\land A\not\subseteq B)\iff(\mathcal{N}_B\not\subseteq\mathcal{N}_A\land \mathcal{N}_A\not\subseteq\mathcal{N}_B))$$ Proof. Suppose $A,B\subseteq X$ and $X$ is $T_5$. Since $X$ is $T_5$, we have, $B\subseteq X\setminus\overline{A}$ $A\subseteq X\setminus\overline{B}$ Now observe that $X\setminus \overline{A}\in \mathcal{N}_B$ and $X\setminus \overline{B}\in \mathcal{N}_A$. However, $X\setminus \overline{A}\not\in \mathcal{N}_A$ because $A\not\subseteq X\setminus \overline{A}$ (unless $A=\emptyset$, which clearly is not the case always. Consequently it follows that $\mathcal{N}_B\not\subseteq \mathcal{N}_A$. Similarly, one can prove that $\mathcal{N}_A\not\subseteq \mathcal{N}_B$ To prove the converse without loss of generality let us assume that $B\subseteq A$. Then observe that if $B\subseteq A$ and $V\in \mathcal{N}_A$ then as $A\subseteq V$ and $B\subseteq A$ so $B\subseteq V$ and hence it follows immediately that $V\in \mathcal{N}_B$. Consequently it follows that $\mathcal{N}_A\subseteq \mathcal{N}_B$ and we are done. Questions Are the proofs of my theorems correct? Is it possible to formulate equivalent conditions for other spaces mentioned here that I haven't covered only in terms of $\mathcal{N}$'s and $\mathcal{C}$'s?","['general-topology', 'separation-axioms']"
2575795,Does uniform convergence of curves imply convergence of integrals?,"Let $\gamma_n, \gamma: [0, 1] \to \mathbb C$ be (continuous) piecewise $C^1$ curves with $\gamma_n \to \gamma$ uniformly for $n \to \infty$ and $f: \mathbb C \to \mathbb C$ continuous. Does this already imply
$$
\int_{\gamma_n} f(z) \,\textrm{d}z \to \int_{\gamma} f(z) \,\textrm{d}z
$$
for $n \to \infty$? Cauchy's integral theorem states this is true for holomorphic $f$, as then $\int_{\tilde \gamma} f(z) \,\textrm{d}z = 0$ for $\tilde \gamma = \gamma \oplus [\gamma(1), \gamma_n(1)] \oplus \gamma_n^- \oplus [\gamma_n(0), \gamma(0)]$. However I wonder whether this is true for any continuous $f$. I guess “problematic“ curves could be something like
$$
\gamma_n(t) = t + i \cdot \frac1n \left[ \sin(n^2 \pi t) \right], \quad t \in [0, 1],
$$
as then $\|\gamma_n'\|_\infty$ is unbounded.","['complex-analysis', 'integration']"
2575807,Prove the following for the given expression,"Consider the equation $$\frac{\pi ^e}{(x-e)} + \frac{e^{\pi}}{(x-\pi)} + \frac{\pi^{\pi}+e^e}{(x-e-\pi)}=0$$
Prove that 1) This equation has two real roots in $(\pi-e, \pi +e) $ 2) This equation has one real root in $(e,\pi)$ and other root in $(\pi, e+\pi)$ My approach : Let $$f(x)=\frac{\pi ^e}{(x-e)} + \frac{e^{\pi}}{(x-\pi)} + \frac{\pi^{\pi}+e^e}{(x-e-\pi)}$$. Then I tried to find the sign of $f(\pi-e)f(\pi +e)$ so as to check for its sign to prove the first part. But this gives me a very tedious equation from which determining the sign is difficult. Moreover $f(\pi +e)$ creates a 0 in the denominator in third term.Any help would be greatly appreciated. Edit 1: Can anyone prove this using the properties and fundamentals just of polynomials. I know that maths is meant to learn what you don't understand but at current I want to know if this question can be solved using only the fundamentals of algebra and the polynomials without use of calculus","['exponential-function', 'polynomials', 'functions', 'quadratics']"
2575829,How can I count the number of square on this sphere?,"I have a sphere has equation $$(x-2)^2+(y-4)^2+(z-6)^2 = 225.$$ Now I want to count all squares on this sphere with the following condition:
All vertices of four point are 12 different integeral numbers, for example, the square:
$A(-12, -1, 4)$, $B(-9, 14, 8)$, $C(0, 9, 20)$ and $D(-3, -6, 16)$. How many squares satifying the above conditions? I tried. From the equation $$(x-2)^2+(y-4)^2+(z-6)^2 = 225,$$ we have
$$(x-2)^2 \leqslant 225 \Leftrightarrow -13 \leqslant x \leqslant 17,$$
$$(y-2)^2 \leqslant 225 \Leftrightarrow -11 \leqslant y \leqslant 19,$$
$$(z-6)^2 \leqslant 225 \Leftrightarrow -9 \leqslant z \leqslant 21.$$
Now, I can't solve my problem.","['combinatorial-geometry', 'geometry']"
2575850,Constrained optimization using calculus of variations (entropy maximization),"Please note that I don't know (almost) anything about the calculus of variations, and I'm familiar with analysis only at an undergraduate level (i.e., Baby-Rudin level). Let $[a, b] \subset \mathbb{R}$, and $p: [a,b] \to \mathbb{R}_{\geq 0}$ be such that $\int_{\mathbb{R}}p = 1$. Let
$$H(p) = -\int_{\mathbb{R}}p\log p\text{.}$$
We define $p(x)\log[p(x)] = 0$ whenever $p(x) = 0$.
Consider the problem $$\max H(p) \text{ subject to }\int_{\mathbb{R}}p = 1\text{.}$$ (This is the uncountable-set analogue of a previous question I asked .) According to this textbook I have, this is solved using the calculus of variations. Every time I read something about the calculus of variations, the Euler-Lagrange equations always pop up. However, this is slightly different from most of the examples I've seen online, in that there is a constraint applied. I know that the solution is apparently (and this is provided in the textbook) $$p(x) = \dfrac{1}{b-a}\mathbf{1}_{[a, b]}(x)$$
where $\mathbf{1}_{A}(x) = 1$ for $x \in A$, and $0$ otherwise. But I'm not sure at all how to show this.","['entropy', 'machine-learning', 'calculus-of-variations', 'probability']"
2575852,Find the limit of $\lim_{x\to0}{\frac{\ln(1+e^x)-\ln2}{x}}$ without L'Hospital's rule,"I have to find: $$\lim_{x\to0}{\frac{\ln(1+e^x)-\ln2}{x}}$$
and I want to calculate it without using L'Hospital's rule. With L'Hospital's I know that it gives $1/2$.
Any ideas?","['limits-without-lhopital', 'indeterminate-forms', 'calculus', 'limits']"
2575882,Realizing a map between first homology groups of surfaces by a continuous map,"Suppose $S$ is a closed, oriented surface of genus $g$ and $\chi$ is some element of $H^1(S; \mathbb{C})$, which I prefer to think of as a map $\chi : H_1(S) \to \mathbb{C}$.  Now suppose this $\chi$ is such that its image is a lattice $\Lambda$ in $\mathbb{C}$.  Then for each homology class $\gamma$ you could interpret $\chi(\gamma)$ as a closed loop on the complex torus $\mathbb{T}^2 = \mathbb{C} / \Lambda$, and looking at the homology class of that loop you could think of $\chi$ as a map $\chi : H_1(S) \to H_1(\mathbb{T}^2)$.  Something I'm reading now makes a remark that this map between homology groups must be induced by a map $f : S \to \mathbb{T}^2$, and I don't see why this needs to be the case.  In general maps between homology groups don't need to be induced by maps between the spaces, so is there something special here that promises the existence of such an inducing map in this case?","['algebraic-topology', 'complex-analysis', 'riemann-surfaces', 'low-dimensional-topology']"
2575887,Weak topology on unit ball,"I have the following problem: $X \in \mathbb{K}$ linear space and $F,f_1,\ldots,f_n : X \rightarrow \mathbb{K}$ linear with $$ \bigcap_{i=1}^n N (f_i) \subset N (F)$$ Show that $F$ is a linear combination of $f_i$ . Maybe it works with induction but I have no clue how to start. This should help me to solve then why $B^\circ _1 (0) = (x \in X : ||x||_x <1)$ (unit ball) is empty for weak topology on $X$ with $(X,||.||_x)$ being an infinite dimensional normed space. Can someone help me to show that? especially with the linear combination?","['functional-analysis', 'general-topology']"
2575888,When do colonizers start marrying cousins?,"Suppose a group of humans, with $n$ males and $n$ females, colonizes another planet.  Suppose they all pair up in the usual way, and each couple has two children--one boy and one girl.  Then the process repeats. Question : How many generations can pass before somebody has to marry their cousin? EDIT : To clarify: Each member of a generation marries someone of the same generation. We assume that the original colonists are all unrelated. Nobody marries their sibling, though if you broaden the definition of ""cousin"" to include ""anybody who shares a common ancestor"" then you don't need to make this assumption.  (So in light of the above point, the question can be restated ""How many generations can pass before someone marries a relative."")",['combinatorics']
2575967,Proving whether the series $\sum_{n=1}^\infty \frac{(-1)^n}{n-(-1)^n}$ converges.,"I've updated my proof to be complete now, edited for proof-verification! We know that for the partial sums with even an uneven terms, the following holds: $S_{2N}=\sum_{n=1}^{2N} \frac{(-1)^n}{n-(-1)^n} = -\frac{1}{2} + \frac{1}{1} -\frac{1}{4} +\frac{1}{3} -\frac{1}{6} +\frac{1}{5}-\dots+\frac{1}{2N-1}$
$= \frac{1}{2\times1} +\frac{1}{4\times3} + \frac{1}{6\times5}+\dots+\frac{1}{2N(2N-1)} = \sum_{n=1}^{2N} \frac{1}{2n(2n-1)}$ We may rewrite the series in pairs as we know  it will have an even amount of terms. $S_{2N+1} = \sum_{n=1}^{2N+1} \frac{(-1)^n}{n-(-1)^n} = -\frac{1}{2} + \frac{1}{1} -\frac{1}{4} +\frac{1}{3}-\dots+\frac{1}{2N-1} -\frac{1}{2N+2}$
$=\frac{1}{2\times1} +\frac{1}{4\times3} + \frac{1}{6\times5}+\dots+\frac{1}{2N(2N-1)} - \frac{1}{2N+2} = \sum_{n=1}^{2N} \frac{1}{2n(2n-1)}-\frac{1}{2N+2}$ As $n\in\mathbb{N}$, we know that $n\geq1$ so: $n\geq1 \iff 3n\geq3 \iff 3n^2\geq3n \iff 3n^2-3n\geq0 \iff 4n^2-2n \geq n^2+n$ So: $2n(2n-1)\geq n(n+1) \iff \frac{1}{2n(2n-1)}\leq \frac{1}{n(n+1)}$ for all $n\geq1$. As the series of the latter sequence converges, we can conclude, by the comparison, test that the series $\sum_{n=1}^{2N} \frac{1}{2n(2n-1)}$ converges. Suppose it converges to $s$, then we know $^{\lim S_{2N}}_{N\to\infty} = s$ and thus $\lim_{N\to\infty}[S_{2N+1}] = s - (\lim_{N\to\infty}[\frac{1}{2N+2}]) = s-0 = s.$ As the partial sums ending with even and uneven terms both converge to the same limit, the series $\sum_{n=1}^\infty \frac{(-1)^n}{n-(-1)^n}$  converges. $\tag*{$\Box$}$","['real-analysis', 'convergence-divergence', 'sequences-and-series', 'proof-verification']"
2575974,Venn diagram with rectangles: How many among $\binom{n}{k}$ regions created by intersections of exactly $k$ rectangles can be represented?,"This question is related to the question I asked a few time ago, see Venn diagram with rectangles for $n>3$ . Within this question I wanted to know whether it is possible to draw a Venn diagram with rectangles for $n>3$ under the restriction that the rectangles are having the same width and height as well as being axis-aligned (not rotated). By his Bound 2 Alex Ravsky pointed out that this is not possible for $n>5$. However, Barry Cipra then showed that it is not possible for $n\ge4$. Now I want to go a little bit deeper into the number of regions of a Venn diagram. Therefore I also consider axis-aligned rectangles with the same width and height. The rectangles are not rotated. Preliminaries A Venn diagram cannot be drawn with rectangles for $n\ge4$ Interpreting a Venn diagram geometrically, it forms $2^n$ regions, which is $2^n=\sum_{k=0}^n\binom{n}{k}$ The second preliminary implies, that a Venn diagram has $\binom{n}{k}$ regions where exactly $k$ rectangles intersect (i.e. overlap). Using the first preliminary, this formula is geometrically valid for $n<4$. As a Venn diagram with rectangles can't be drawn for $n\ge4$, at least how many regions can be formed using $n$ rectangles (is there an upper bound?)? The maximum possible number of regions that can be formed using $n$ rectangles is $2+n(n-1)$. What is the maximum possible number of regions where exactly $k$ rectangles intersect? For the first question I made the following considerations: Let $a_n$ define the maximum number of regions which can be formed using $n$ rectangles. Imagine $n-1$ already aligned rectangles that produce $a_{n-1}$ regions. Now arrange the $n$-th rectangle in a way so that it intersects each of the $n-1$ rectangles in two distinct points (this maximizes the number of regions). This produces two new regions for every intersection of the $n-1$ already aligned rectangles. Thus, $a_n=a_{n-1}+2(n-1)$ with $a_1=2$. Using the iteration method for recursion yields $a_n=n^2-n+2$. Note that $a_n=2^n$ for $n\le 3$ and $a_n<2^n$ for $n>3$.","['combinatorics', 'combinatorial-geometry', 'rectangles', 'discrete-mathematics']"
2575983,May a smooth bijection have a non continuous inverse?,"Suppose that $f$ is an injective function between open subsets (i.e. with range and domain both open sets) of $\mathbb{R}$. Is it possible that $f\in C^{\infty}$, but $f^{-1}\notin C^0$?","['derivatives', 'functions', 'general-topology', 'differential-geometry', 'analysis']"
2575997,What is the conformal map of a rectangle into a band?,"What is the conformal map from the rectangle to the ""band"" (space between two parallel lines) in the complex plane? By the Riemann mapping theorem , such a map exists. We can define a rectangle as $\{a+bi: a \in (-1,1),b\in(-w,w)\}$ for some $w \in \mathbb R_+$, and the ""band"" as $\{a+bi: a \in (-1,1), b \in (-\infty,\infty)\}$. Note in particular that the function should map the rectangles vertical axis to the bands vertical axis (i.e., it should map $\{0\} \times (-w,w)$ to $\{0\} \times (-\infty,\infty)$). (The reason I want such a map is that I want to transform the band model of Circle Limit III into a desktop wallpaper.) EDIT: I found this article claiming (on page 4) that you can map from a rectangle to a disk of the form $$w = \frac{1 + i\sqrt k sn(\frac 1 \alpha (z + ib))}{1 + \sqrt k sn(\frac 1 \alpha (z + ib))}$$ where $z$ is a point the rectangle $[-a,a] \times [0,2b]$ and $w$ is a point in the unit circle ($\alpha = \frac K a$, $K$ is a quarter of the real period of $sn$, and $k$ is parameter drawn from $[0,1]$). It is trivial then to map the disk into the band. The problem is that I looked up the $sn$ function, and it has two arguments, but in the equation above, only one argument is being given. EDIT: $sn$ does not preserve symmetry.","['special-functions', 'complex-analysis', 'conformal-geometry']"
2576004,Confusion of the definition of a completely regular polygon $P$,"I am a student studying an algebra & geometry module with an exam at the end of January. I have noticed that there are two alternate definitions of a completely regular polygon $P$ : A topological space $X$ such that for every closed subset $C$ of $X$ and every point $x \in X\setminus C$ , there is a continuous function $f:X\to[0,1]$ such that $f(x)=0$ and $f(C)=\{1\}$ . A topological space $X$ is said to be completely regular space, if every closed set $A$ in $X$ and a point $x\in X$ , $x\notin A$ , then there exist a continuous function $f:X\to[0,1]$ , such that $f(x)=0$ and $f(A)=\{1\}$ . Now my question is, why are these two definitions equivalent?",['algebraic-geometry']
2576053,Kernel of functional on dual space is weak-* closed iff it is evaluation functional,"Let $X$ be a Banach space and $f \in X^{**}$. Show that $\ker f$ is weak-* closed if and only if $f = i_x$ for some $x \in X$, where $i_x\in X^{**}$ is the evaluation functional $i_x(g)\equiv g(x)$ ($g\in X^*$). I’ve seen this topic , but in this problem the author considers the case with a set of kernels. Can I reduce my problem to this one?","['functional-analysis', 'general-topology', 'banach-spaces']"
2576077,How to find the closed form formula for $\hat{\beta}$ while using ordinary least squares estimation?,"According to Wikipedia's article on Linear Regression : Given a data set $\{y_i,x_{i1},\ldots,x_{ip}\}_{i=1}^{i=n}$ of $n$
  statistical units, a regression model assumes that the relationship
  between the dependent variable $y_i$ and the $p-\text{vector}$ or
  regressors $x_i$ is linear. This relationship is modelled through a
  disturbance term or error variable $\varepsilon_i$-an unobserved random
  variable that adds random noise to the to the linear relationship
  between the dependent variables and regressors. The model takes the
  form
  $y_i=\beta_0(1)+\beta_1x_{i1}+\cdots+\beta_px_{ip}+\epsilon_i=x_i^T \beta + \varepsilon_i$ These equations can be written in vector form as $$y=\mathbf{X\beta+\epsilon}$$ For the Ordinary Least Square estimation they say that the closed form expression for the estimated value of the unknown parameter $\beta$ is $$\hat{\mathbf{\beta}}=(\mathbf{X^{T}X})^{-1}\mathbf{X}^{T}y$$ I'm not sure how they get this formula for $\hat{\beta}$. It would be very nice if someone can explain me the derivation.","['regression', 'statistics', 'linear-algebra', 'calculus']"
2576106,Does $Var(X^2) \geq (VarX)^2$ hold?,"It is well known that $E(X^2) \geq (EX)^2$, but I was wondering if there is a similar result for variances, e.g. is $Var(X^2) \geq (VarX)^2$? I was doing some research and came up with that inequality, but I can’t prove it. I’ve done simulations in R for several known distributions (e.g. bernoulli, binomial, poisson, normal, gamma, t, exponential, all for few parameters) and they seem to show that it really does hold, but I’m not sure whether it holds generally. If this doesn’t hold, is there any other result that somehow links $Var(X^2)$ and $(VarX)^2$? EDIT: So, as H. H. Rugh showed in his answer, it can't hold generally, but it often does. I'm still interested in a reference to a different result that gives a certain link between the two expressions, or perhaps some sufficient conditions for the inequality, etc.",['probability']
2576216,Matrix equation in characteristic 2,"Let $n\geq 3$ be an odd integer and $K$ be a field with characteristic $2$. Let $A,B\in M_n(K)$ s.t. $A^2+B^2=I_n$; is it true that $AB+BA$ is a singular matrix? Remark. i) It is not difficult to see that the result is false when $n$ is even. ii) The proposition is true for $n=3$; but I do not know the answer if $n=5,7,\cdots$. EDIT. answer to @ i707107 . i) user1551 gave a counterexample for even $n$: $A=\begin{pmatrix}0&1\\1&0\end{pmatrix},B=\begin{pmatrix}0&1\\0&0\end{pmatrix}$. ii) To obtain the reported result for $n=3$, I did a PC computation using Grobner basis theory (the basis contains $234$ elements). The case $n=5$ has too large complexity. Remark. You are right; the problem is equivalent to show that $A+B$ admits $1$ as eigenvalue.","['matrices', 'matrix-equations', 'linear-algebra']"
2576232,Is this a valid method of solving $3^x + 2^x = 35$?,"I want to know if this is a valid method of solving this equation: $3^x + 2^x = 35$ $3^x+2^x = (7)(5)$ $3^x+2^x = (3+2^2)(2+3)$ $3^x+2^x = 3^2 + 2^3 + 18$ $3^x+2^x = 9 + 18 + 2^3 $ $3^x+2^x = 27 + 2^3 $ $3^x+2^x = 3^3 + 2^3 $ And now comes my problem. Is it correct to say that this last equation implies $x=3$? I don't think I can just compare terms just like it was a polynomial... If this is not correct, is there a way of solving this equation algebraically?","['algebra-precalculus', 'exponential-function']"
2576243,Prove that $\sum_{n\mathop=1}^\infty \frac 1 {n^{1+i}}$ is divergent,I am struggling with this analysis exercise: Prove that $\displaystyle \sum_{n\mathop=1}^\infty \dfrac 1 {n^{1+i}}$ is divergent but the limit $\displaystyle \lim_{t \to 1^+} \sum_{n\mathop=1}^\infty \frac 1 {n^{t+i}}$ exists. I know the first sum is not absolutely convergent because $\sum \frac 1 n$ diverges by the integral test. How how do I prove it is divergent? (i.e. not conditionally convergent). Would appreciate hints for the 2nd part too,"['convergence-divergence', 'sequences-and-series', 'analysis']"
2576248,Calculating $\sum\limits_{n=1}^{\infty}\frac{(-1)^n \sin(n)}{n}$,"I'm trying to evaluate the following sum: $$\displaystyle\sum\limits_{n=1}^{\infty}\frac{(-1)^n \sin(n)}{n}$$ But I'm having trouble getting anywhere. Wolphram Alpha indicated some stragety that uses complex logarithms in order to find the answer, which seems to be $-0.5$. Anyone know a good strategy here?","['infinity', 'sequences-and-series']"
2576251,"If $y' + p(x) y = q(x)$, $q \to 0$, and $p(x) \geq a > 0$, then $y \to 0$","Given the first order equation $$ y' + p(x)y=q(x) $$ where $p,q: \mathbb{R} \longrightarrow \mathbb{R} $ are continuous functions such that $p(x) \geq \alpha > 0, \forall x \in \mathbb{R}$ and $q(x) \longrightarrow 0$ if $x \longrightarrow +\infty$. Show that any solution of equation fulfills that $$ \lim_{x \longrightarrow +\infty} y(x) = 0$$","['derivatives', 'integrating-factor', 'ordinary-differential-equations', 'calculus']"
2576253,On the weak equivalence axiom for non-CW spaces,"A cohomology theory $h^*$ satisfies the weak equivalence axiom (wea) if for every weak homotopy equivalence $f : X \to Y$ there is a (degreewise) isomorphism $h^*(Y)\to h^*(X)$ induced by $f$. There is an exercise in Strom's book that asks to prove that ordinary, represented reduced cohomology $X\mapsto [X, K(G,n)]$ does not satisfy the wea. The counterexample is the following: Let $\mathbb N$ be the discrete set $\{0,1,2,...\}$ and $L$ be the set $\{0\}\cup \{1/n\mid n\ge 1\}\subset \mathbb R$ with the subspace topology. Then there is a map $\mathbb N\to L$ which is a weak homotopy equivalence, but it does not induce isomorphisms in reduced cohomology. My attempt to a proof follows: I want to dot all the i's of this exercise. Any map $\mathbb N\to L$ will be continuous, because $\mathbb N$ is discrete; I think $\varphi : 0\mapsto 0, n\mapsto 1/n$ does the job; to see this let's note that The connected component of $0$ in $L$ is the singleton $\{0\}$. Proof. Easy: connected components form a partition of a space, and every other point $1/n$ has the singleton $\{1/n\}$ as connected self-component. $\square$ Let $X$ be connected; then
  $$ [X,L]\cong \hom(X,L)\cong L$$ in the sense that two maps are homotopic iff they are equal, and there are only constant maps. Proof. If $f : X\to L$ does not assume the value $0$, then it factors through a discrete subspace of $L$, and the result follows; if there is $x_0$ such that $f(x_0)=0$, then $f(X)$ must be connected and contain $0$, so it must be $\{0\}$. So there are only constant maps: now if $f,g$ are homotopic through $H : X\times [0,1]\to L$, then $f=g$ because $X\times [0,1]$ is connected, and then $H$ must be constant. $\square$ Corollary of 1+2 is that each map $[S^n,\mathbb N]\to [S^n,L]$ is the zero map between homotopy groups, and a bijection on $\pi_0$'s, so the first part of the claim is proved. Now, the map induced on $H^0$'s is not an isomorphism of abelian groups. Proof. Let $f$ be a continuous map $L\to K(G,0)$. Then $f(0)=\lim_{n\to \infty} f(1/n)$, so that $f$ must be eventually constant (its codomain is $G^\delta$, $G$ with the discrete topology). On the other hand, $[\mathbb N, G^\delta]$ is a countable product of copies of $G$, so (for example) the jumping sequence $(0,g,0,g...)$ for $g\neq 0$ does not lie in the image of $\varphi^*$. $\square$ As in every fairy-tale there's a moral in this story: This mess happens because $L$ is not a CW complex (CW's have the discrete topology on 0-skeleta, but $0\in L$ is a limit point). In fact, if the spaces are CW's, then  this pathology disappears: cohomology detects $n$-homotopy equivalences for simply connected spaces. Now for my question(s): 0. A question on definitions. $L$ is not a CW. But it has the weak homotopy type of a CW complex; does this make $L$ ""nice"" in some sense? 1. A metamathematical question. Is there something else to learn from this counterexample? Who's the wisest between homotopy groups and reduced cohomology, or in other words: are $\pi_n$'s unable to see some information that instead $\tilde{H}^n$ does see, and that I should take care of, or rather $\tilde{H}^n$'s are too picky? 2. A more concrete question. Are there cohomology groups of $\mathbb N$ and $L$ different in higher degrees? Better said: how do you (dis)prove that $H^n(L,G)$ is zero for $n\ge 1$? Again, what worries me is that $L$ is not a CW complex, but it does have the weak homotopy type of a CW complex. When I first saw this exercise I was surprised: I bet that the chain map of the singular complexes $C^*(\mathbb N, G)$ and $C^*(L, G)$ is a quasi-isomorphism. So that's why I want to be so picky: I want to see if this guess is false, or if all that matters to build this counterexample lives in degree zero.","['algebraic-topology', 'general-topology', 'homotopy-theory', 'homology-cohomology']"
2576294,Finite Probability - Discrete Math,"Background Information: I am studying Discrete Math regarding finite probability. I understand If |S| = n, a ∈ S, and A ⊆ S, then Pr({a}) = Pr(a) = 1/n (the probability that ""a"" occurs). Thus, Pr(A) = |A|/ n ""the probability that ""A"" occurs"". - Original Question: 100 tickets, numbered 1,2,3,…, 100, are sold to 100 different people for a drawing. Four different prizes are awarded, including a grand prize (a trip to Tahiti). Find the probability that ticket 47 wins a prize while ticket 73 does not. My thinking: We know that n = 100, so the sample space S = n. Considering that A is the number of events, then A = 1 because ticket 47 is considered only one ticket from the original 100 tickets, and ticket 73 is still A = 1 because there is only one ticket 73 from 100 tickets. Therefore, both 47 and 73 tickets have the same probability of happening, so n = 100, then S = {1,2, ..., 100}, then Pr(A) = |A|/n = 1/100. Am I right? If not, could you please clarify the question for me?","['probability', 'discrete-mathematics']"
2576370,Number of hand of cards with exactly 3 aces (Unsure if answer is correct),"Problem A pokerhand is 5 card subset, which is picked from 52 cards in total. Four of all cards are aces. Now how many there are such pokerhands that contain exactly 3 aces and 2 cards that can be anything. Also what is probability for obtaining exactly Attempt to solve Now we can pick 3 aces from total of 4 aces and pick any two cards that are not aces.
$$ (\text{number of possible aces})(\text{number of possible not aces}) $$
$$ {{4}\choose{3}} {{48}\choose{2}}=54155$$ Now the probability would be simply: $$ \frac{\text{number of hands with aces}}{\text{number of all hands}} $$
$$ \frac{{{4}\choose{3}} {{48}\choose{2}}}{{{52}\choose{5}}}\approx 1.736079047\cdot 10^{-4}$$
We get very small probability of $\approx 0.017\%$ Combinatorics isn't strong point of mine so if someone could point out possible flaws that there are with my approach that would be highly appreciated. Also if the approach seems correct please comment that this seems correct.","['combinatorics', 'probability', 'proof-verification']"
2576408,"Lyapunov function and an open disk inside the basin of $(0,0)$","a)Find a strict Lyapunov function for the equilibrium point $(0,0)$ of $$x'=-2x-y^2$$ $$y'=-y-x^2$$. b)Find $\delta>0$ as large as possible so that the open disk of radius $\delta$ and center $(0,0)$ is contained in the basin of $(0,0)$ Solution a) is done. Consider the Lyapunov function $L(x,y)=x^2+y^2$. $L'(x,y)=-2x^2(2+y)-2y^2(1+x)$ is strictly negative when x and y are near from zero. Switching into polar coordinates: $L'=-2r^3(1+cos^2\theta)[\frac{1}{r}+\frac{cos\theta sin\theta(cos\theta+sin\theta)}{1+cos^2\theta}]$ Now since $\color{blue}{\frac{cos\theta sin\theta(cos\theta+sin\theta)}{1+cos^2\theta}>-\frac{1}{2},}$ as long as $r<2$ the quantity of the brackets is positive. Thus $L'<0$ is in the open disk of radius 2 with center (0,0). Moreover, there are $\color{blue}{no}$ solutions on which $L$ is constant except for the equilibrium at $(0, 0). $ This implies, by the Lasalle
invariance principle, that the circle of radius 2, centered at the origin is contained in the basin of
attraction. To say no solutions on which L is constant means that there are no other equilibrium points , i.e the only point is $(0,0)$? I don't understand the blue part of the solution and also the italic text. Any kind of help is greatly appreciated.😊","['basins-of-attraction', 'dynamical-systems', 'lyapunov-functions', 'stability-in-odes', 'ordinary-differential-equations']"
2576433,"For second-order ODE, why do we only assign state variables for $\dot q, q$?","Suppose we are given an ODE of a system, for example, a mass spring damper: $$m\ddot q + \dot q + q = u$$ We may think of $q$ as position, $\dot q$ as the velocity, $\ddot q$ as the acceleration. Why is it that we only assign states to $q$ and $\dot q$, i.e., letting $x_1 = q$ and $x_2 = \dot q$, instead of letting letting $x_1 = q$ and $x_2 = \dot q$ and $x_3 = \ddot q$? Is it not that the acceleration is a ""state"" of the system? If not, then why should the other two variables be the state? I'm watching a video on Youtube where they attempt to assign state variables to a swinging pendulum, and the video says, in which I paraphrase: ""since kinetic energy and potential energy depends on position and velocity, therefore it makes sense to assign these as state variables."" I'm looking for a more precise way of stating the above quote.","['control-theory', 'physics', 'ordinary-differential-equations', 'mathematical-modeling']"
2576440,Proof that biconditional implication is an equivalence relation,"The definition of an equivalence relation is that of a binary relation between distinct elements of a set. Partial orders have a similar definition. Seeing as this could include things like the set of all people, as well as standard sets of numbers, could an equivalence relation, and by extension, a partial order, be constructed over the set of Boolean statements (such as 'It will rain tomorrow') which have true or false values? I have conjectured and attempted to prove that biconditional implication $\leftrightarrow$ is an equivalence relation. Proof: Let $p, q, r$ be propositions with value true or false. Claim 1: $\leftrightarrow$ is reflexive. If $p$ is a proposition with value true or false, $\neg p \lor p$ is a tautology, but by the law of material implication $\neg p \lor p \equiv p \to p$. So $p \to p$ together with itself gives $p \leftrightarrow p$ and $\leftrightarrow$ is reflexive. Claim 2: $\leftrightarrow$ is symmetric. Suppose $p \leftrightarrow q$. Then by definition, $p \to q \land q \to p$. The commutative law gives $q \to p \land p \to q \equiv q \leftrightarrow p$ and hence $\leftrightarrow$ is symmetric. Claim 3: $\leftrightarrow$ is transitive. Suppose $p \leftrightarrow q \land q \leftrightarrow r$. Using definitions, the associative & commutative laws and transitivity of $\to$ we have $$(p \to q \land q \to p) \land (q \to r \land r \to q)$$
$$\equiv (p \to q \land q \to r) \land (r \to q \land q \to p)$$
$$\equiv (p \to r) \land (r \to p)$$
$$\equiv p \leftrightarrow r$$ and so $\leftrightarrow$ is transitive. By proof of claims 1, 2 and 3, $\leftrightarrow$ is an equivalence relation.","['propositional-calculus', 'logic', 'proof-verification', 'discrete-mathematics']"
2576475,Kodaira dimension and Canonical Ring,"I have the following definition of the Kodaira dimension of a smooth variety $X$ : $$k(X):= \begin{cases} \max \dim \phi_{|nK_{X}|}(X) & \exists n:|nk|\neq \emptyset for \\ -\infty & \text{otherwise} \end{cases}$$ Where $\phi_{|nK_{X}|}$ is the map associated to the linear system $|nK_{X}|$ for $K_{X}$ the canonical divisor. I also have the canonical ring defined as the graded $\mathbb{C}$ algebra $$R(X):= \bigoplus_{n\geq 0} H^{0}(X,nK_{X})$$ I would like to show that $\dim R(X)-1=k(X)$ when $k(X) \geq 0$ . My idea was to let $Q(D)=Frac{H^{0}(X,D)} \subseteq \mathbb{C}(X)$ for any divisor $D$ . It's clear that $Q(D)=Q(D')$ whenever $D,D'$ are effective and linearly equivalent. Then choosing $D_{n} \in |nK_{X}|$ for every possible $n$ we can take $Q=\bigcup_{n \geq 0} Q(D_{n})$ . We can identify $R(X)$ with the subring $$\bigoplus_{n\geq 0} L(nK_{X})t^{n}\subseteq \mathbb{C}(V)(t)$$ . If we fix an $n$ we can take $f$ s.t $D=nK_{x} +(f)$ . Then for any $m$ and any $g \in H^{0}(X, mK_{X})$ we have $D'=(g^{n})+nmK_{X} \sim mD$ and in fact $D'=mD+(g^{n}f^{-m})$ but $D'$ is effective so we have $g^{n}f^{-m}\in Q(mD)\subseteq Q$ . Thus $gt^{m}$ is algebraic over $Q(ft^{n})$ , so we must have $Frac(R(X))$ algebraic over $Q(ft^{n})$ . In particular then $\dim (R(X))= Trdeg(Frac(R(X))=Trdeg(Q)+1$ .  We must have that Q is algebraic over $Q(D_{N})$ for some $N$ as $\mathbb{C}(X)$ has finite transcendence degree, thus $\dim R(X)= Trdeg(Q(D_{N})+1$ . I am pretty sure but not 100% certain that $Q(D_{n}) \cong \mathbb{C}( \phi_{|D_{n}|}(X))$ So we have $\dim R(X)-1=Trdeg(Q(D_{n}))=\dim \phi_{|D_{n}|}(X)+1$ , which is very close to what I wanted to show, but sadly not quite right. Could someone point out where I went wrong and/or tell me if this is salvageable. A good reference for a different proof would also be welcome. I only actually need the result for surfaces if that has any impact at all. Thanks.","['krull-dimension', 'complex-geometry', 'algebraic-geometry']"
2576479,Is there always at least one prime in these closed intervals?,"I know that it is known that for every integer $n>1$ there is always a prime $p$ such that $n<p<2n$ ( Bertrand´s postulate ). Also, I guess that it is still not known whether there is at least one prime between $n^2$ and $(n+1)^2$, for every positive integer $n$ ( Legendre´s conjecture ). What about this: Is there always at least one prime in the closed interval $[2^n,2^n+n]$ for every positive integer $n$? I just checked for the first few $n$ by heart and found no counterexample although sometimes primes are at the endpoints of these closed intervals. Maybe first counterexample, if it exists, is not far away, but somebody will, I hope, check that.","['number-theory', 'prime-numbers', 'elementary-number-theory']"
2576538,Form of particular solution to inhomogeneous differential equation,"I want to solve the initial value problem
$$x''(t)+3x'(t)+2x(t)=\frac{1}{1+e^t}, \quad x(0)=2\ln2,~x'(0)=1-3\ln 2$$ considering the homogeneous couterpart
$$x''(t)+3x'(t)+2x(t)=0$$
with the characteristic polynomial $$ \chi(\lambda)=\lambda^2+3\lambda+2=(\lambda+1)(\lambda+2)$$
the general solution to the homogeneous equation is of the form
$$x(t)=c_1e^{-1t}+c_2e^{-2t}$$ using my initial conditions
$$x(0)=c_1+c_2=2\ln 2, \quad x'(0)=-c_1-2c_2=1-3\ln 2$$
I have $$c_1=1+\ln2,~c_2=-1+\ln 2,\quad x(t)=(1+\ln 2)e^{-t}+(-1+\ln 2)e^{-2t}$$ Solution using @Karn Watcharasupat's method :
$u := x+x', \quad u'=x'+x''$ the equation becomes a first order linear differential equation, with $\mu$ integration factor such that $\mu 2=\mu'\Rightarrow \mu  = e^{2t}$
$$u'+2u=\frac{1}{1+e^t}$$
$$(\mu u)'=\mu \frac{1}{1+e^t}\iff \frac{d\left(e^{2t}u\right)}{dt}=\frac{e^{2t}}{1+e^t}$$
$$ e^{2t}u=\int \frac{e^{2t}}{1+e^t}dt=\int \frac{v-1}{v} dv=\int 1 dv - \int \frac{1}{v}dv=v-\ln |v|=1+e^t-\ln|1+e^t|+C_1$$ for $v:=1+e^t$.
And then $$u=\frac{e^t-\ln|1+e^t|+C_2}{e^{2t}}=e^{-t}-e^{-2 t}\ln|1+e^t|+e^{-2t}C_2$$
Plugging back in $$x+x'=C_2e^{-2t}+e^{-t}-e^{-2 t}\ln(1+e^t)$$ which is similarly a first order ODE with $\mu=\mu'\Rightarrow \mu=e^t$
$$\left(x\mu\right)'=\mu\cdot \left(C_2e^{-2t}+e^{-t}-e^{-2 t}\ln(1+e^t)\right)=e^t\left(C_2e^{-2t}+e^{-t}-e^{-2 t}\ln(1+e^t)\right)$$ $$\frac{d\left(xe^t\right)}{dt}=\left(C_2e^{-t}+1-e^{- t}\ln(1+e^t)\right)$$
\begin{align}xe^t&=\int \left(C_2e^{-t}+1-e^{- t}\ln(1+e^t)\right) dt\\&=\int C_2e^{-t} dt+\int 1 dt -\int e^{- t}\ln(1+e^t) dt\\&=-C_2e^{-t}+t-\left[-e^{-t}\ln \left(e^t+1\right)+t-\ln \left|e^t+1\right|+C\right]\\&=-C_2e^{-t}+t+e^{-t}\ln \left(e^t+1\right)-t+\ln \left(e^t+1\right)+C\\&=C_2e^{-t}+e^{-t}\ln \left(e^t+1\right)+\ln \left(e^t+1\right)+C\end{align}
And finally
$$x=\frac{C_2e^{-t}+e^{-t}\ln \left(e^t+1\right)+\ln \left(e^t+1\right)+C}{e^t}=C_2e^{-2t}+e^{-2t}\ln \left(e^t+1\right)+e^{-t}\ln \left(e^t+1\right)+Ce^{-t}$$
$$x(t)=C_1e^{-t}+C_2e^{-2t}+e^{-t}\ln \left(e^t+1\right)+e^{-2t}\ln \left(e^t+1\right)$$
$$x'(t)=-C_1e^{-t}-2C_2e^{-2t}-e^{-t}\ln \left(e^t+1\right)+\frac{1}{e^t+1}-2e^{-2t}\ln \left(e^t+1\right)+\frac{e^{-t}}{e^t+1}$$
Finding $C_1,~C_2$
$$x(0)=C_1+C_2+2\ln(2)=2\ln(2)\Rightarrow C_1=-C_2$$
$$x'(0)=-C_1-2C_2-\ln 2+\frac{1}{2}-2\ln 2+\frac{1}{2}=1-3\ln 2 \Rightarrow C_1=-2C_2$$
So $$C_1=C_2=0$$ and the solution to the IVP is
$$x(t)=e^{-t}\ln \left(e^t+1\right)+e^{-2t}\ln \left(e^t+1\right)$$",['ordinary-differential-equations']
2576554,Prime numbers yield from Pythagoras triples,"Pythagoras theorem $$a^2+b^2=c^2$$ we got $$P_{prime}(a,b)={a^4+b^4+(a+b)^4\over a^2+b^2+(a+b)^2}$$ Where $(a,b,c)$ are Pythagoras theorem triples, this function $P_{prime}(a,b)$ always produce a prime number for the values of a and b. Examples: $P_{prime}(3,4)=37$, $P_{prime}(5,12)=229$, $P_{prime}(68,285)=105229$ and so on... I have checked a lot of values, it seem to be prime so far. My question is: Does the function $P_{prime}(a,b)$ always produce prime numbers?","['polynomials', 'factoring', 'number-theory', 'prime-numbers', 'pythagorean-triples']"
2576594,"Is a continuous function $ f\colon(0,\infty)\to R$, such that $f(x)\leq f(nx)$ increasing?","My question is related to: LeL $f: (0, \infty)\to R$ be continuous and $f(x)\leq f(nx)$ prove $\lim\limits_{x\to\infty} f(x)$ exists and $f\colon(0,\infty)\to \mathbb R$ be continuous ; $f(x)\le f(nx) , \forall n \in \mathbb N , \forall x >0$ , then $\lim_{x\to \infty} f(x)$ exists? Let $f\colon (0, \infty)\to R$ be  continuous such that  $f(x)\leq f(nx)$ for all positive $x$ and natural $n$. It was proved that the limit (finite or infinite) in the infinity exists.
Do we know if such a function must be (weakly) increasing?
I believe that there might be counterexamples.","['calculus', 'limits']"
2576633,For which complex numbers $\alpha$ and $\beta$ is it true that $\alpha^n+\beta^n$ is always an integer?,"Possibly a very straightforward question, but: Question. For which complex numbers $\alpha$ and $\beta$ is it true that $\alpha^n+\beta^n$ is always an integer for all $n=1,2,3\ldots$ ? For example, $$\alpha = \frac{1+i\sqrt{7}}{2}, \beta = \frac{1-i\sqrt{7}}{2}$$ have this relationship. A couple of remarks. Firstly, a way of finding such $\alpha$ and $\beta$ pairs show's up in Silverman's book ""The Arithmetic of Elliptic Curves."" In particular: Secondly, something similar seems to occur in connection with the Fibonacci numbers . Following this line of thought, perhaps a better question would be: for which complex numbers $\alpha$ and $\beta$ does there exist a complex number $k$ such that $$\frac{\alpha^n+\beta^n}{k}$$ is always an integer?","['fibonacci-numbers', 'algebraic-geometry', 'elementary-number-theory']"
2576675,Did Gauss know Jacobi's four squares theorem?,"This is a question that I have already asked on HSM stackexchange, and I decided to ask it again here because it is more mathematical than historic (to make a conclusion in this question one needs more mathematical then historical understanding). In p. 283-285 of volume 2 of Dickson's “history of the theory of numbers” appear several formulas of striking similarity: some of them are stated by Gauss (p.283) and some are stated by Jacobi (p.285); they are actually the same and only the notation differs ( $x$ in Gauss's formula and $q$ in Jacobi's formula). Gauss's formulas are the following identities on the 4th power of the theta function: $(\sum_{-\infty}^\infty q^{{n^2}})^4 = (\sum_{-\infty}^\infty (-1)^n q^{{n^2}})^4 + (\sum_{-\infty}^\infty q^{{(2n - 1)^2/4}})^4 = 1 + 8\sum_{1\le m} \frac {{mq^m}}{{1 - (-1)^{m + 1}q^m}} = 1 + \sum_{1 \le m}\hat \sigma (k)q^k$ The point is that the last equality means that the coefficients of the $k$ th power in the right side of the last equallity must be equal to $r_4(k)$ (number of representations of $k$ as sum of $4$ squares), and an additonal interpretation (by certain manipulations) of the right side of the equallity gives the result of Jacobi: $r_4(k) = 8\sigma(k)$ or $24\sigma(k)$ , depends if k is odd or even. In the same passage from Gauss's nachlass (Werke, volume 3, p. 444-445, passage [9])  in which he writes down Jacobi's identity, and just before this identity, Gauss also writes down $\mathbb{log}(\vartheta_3^4(x))$ as: $$\mathbb{log}((1+2x+2x^4+2x^9+\cdots)^4) = 8\cdot (\frac{x}{1+x}+\frac{x^3}{3(1+x^3)}+\frac{x^5}{5(1+x^5)}+\cdots)$$ , (actually he writes down the series for $\frac{1}{2}\mathbb{log}(\vartheta_3(x))$ , but it is equivalent to what I wrote). Immediately after writing down several identities on the fourth powers of Jacobi theta functions $p,q,r$ , Gauss proceeds and writes a differential equation satisfied by new variables $t,u$ (defined by : $t = \frac{1}{p^2}, u  = \frac{1}{q^2}$ )  and their first, second and third derivatives (of $t,u$ ). Since I'm unfamiliar with the theory of modular forms, I'm unable to see how Gauss arrived at this identity for $\mathbb{log}(\vartheta_3^4(x))$ , nor I'm able to see how one can find the series developement of $\vartheta_3^4(x)$ from that of $\mathbb{log}(\vartheta_3^4(x))$ . But maybe some of the mathematicians here who are familiar with modular forms can see the connection. Update (July 23, 2022) The identity for $\mathbb{log}(\vartheta_3(x))$ can be derived on the basis of Jacobi triple product indentity; it is essensially an expansion of the logarithm into a linear combination of Lambert series by transforming the logarithm of the infinite product form of the theta function (which is a special case of Jacobi triple product) into an infinite sum of logarithms. A detailed derivation of it can be found in this post . But I believe the key to uncover the infinite series for $\vartheta^4_3(x)$ from that of $\mathbb{log}(\vartheta^4_3(x))$ is the differential equation Gauss writes at the end of passage 9 - he defines $t = \frac{1}{p^2}, u = \frac{1}{q^2}$ where $p = \vartheta_3(x), q = \vartheta_4(x)$ and then writes down several relations, and one of them is: $$\frac{u}{t}-\frac{t}{u}=2x(tu'-ut')$$ Rewriting it in terms of $p,q,r$ , one gets: $$\frac{p^2}{q^2}-\frac{q^2}{p^2}=2x(tu'-ut')\implies \frac{p^4-q^4}{p^2q^2}=2x(tu'-ut')\implies p^4-q^4 = 2p^2q^2x(tu'-ut') \implies r^4 = \frac{2x}{tu}(tu'-ut')\implies r^4 = 2x(\frac{u'}{u}-\frac{t'}{t})\implies r^4 = 2x(\mathbb{log}'(u) - \mathbb{log}'(t))\implies r^4 = x\mathbb{log'}(\frac{u^2}{t^2}) = x\mathbb{log}'(\frac{p^4}{q^4})$$ I dont know how to prove this differential equation, but this development shows that it connects the logarithm of ratio of theta functions with the fourth power of another theta function ( $r$ ), so this might be the original method Gauss used to arrive at the series for $\vartheta^4_3(x)$ . In addition, since $q(x) = p(-x)$ , the right side of the last equation, which is $$x(\mathbb{log}'(p(x)^4)-\mathbb{log}'(q(x)^4)) = x(\mathbb{log}'(p(x)^4)-\mathbb{log}'(p(-x)^4))$$ , can be calculated on the basis of the series expansion for $\mathbb{log}(p(x))$ . This produces Jacobi's identity for the generating function of $r^4(x)$ . However, I still dont know how to derive the series for $p^4,q^4$ , nor I am able to prove the differential equation stated by Gauss.","['number-theory', 'theta-functions']"
2576688,How to get the Ito's formula almost surely from proving that $df(B_t)$ converges in probability to $f'(B_t)dB_t + 1/2 f''(B_t)dt$.,"I am looking at the proof of the Ito's formula given in Rene Schilling's Brownian Motion. Theorem. Let $B_t$ be a one dimensional Brownian motion and let $f:\mathbb{R}\to \mathbb{R}$ be a $C^2$-function. Then we have for all $t \ge 0$ almost surely 
$$f(B_t)-f(B_0) = \int_0^t f'(B_s)dB_s + \frac{1}{2}\int_0^t f''(B_s)ds.$$ The proof proceeds as first assuming that the support of $f$ is compact, and then showing that for any partition $t_0=0<t_1<\cdots <t_N=t$ we have the decomposition 
$$f(B_t)-f(B_0) = \sum_{l=1}^N f'(B_{t_{l-1}})(B_{t_l}-B_{t_{l-1}})+\frac{1}{2} \sum_{l=1}^N f''(\xi_l)(B_{t_l}-B_{t_{l-1}})^2 =:J_1 + J_2$$ where $\xi_l=\xi_l(\omega)$ is an intermediate point between $B_{t_{l-1}}$ and $B_{t_l}$. The proof is complete by showing that $J_1 \to \int_0^t f'(B_s)dB_s$ in probability as $|\Pi|\to 0$ and $J_2\to \frac{1}{2}\int_0^t f''(B_s)ds$ in probability as $|\Pi|\to 0$. Finally, we use cutoff functions in the general case and using the above identity for $t\wedge \tau(l)$, where $\tau(l):=\inf \{s>0: |B_s|\ge l\}$, use $\lim_{l\to \infty} \tau(l)=\infty $ almost surely to complete. Question: In the theorem, how do we get the identity almost surely? The proof actually shows that the LHS in the Ito's formula converges in probability to the RHS. How does this imply that the identity holds a.s.? I.e. the proof shows that $P-\lim_{|\Pi|\to 0} (f(B_t)-f(B_0))=\int_0^t f'(B_s)dB_s + \frac{1}{2}\int_0^t f''(B_s)ds$, but how does this mean that $f(B_t)-f(B_0)= \int_0^t f'(B_s)dB_s + \frac{1}{2}\int_0^t f''(B_s)ds$?","['stochastic-processes', 'real-analysis', 'probability-theory', 'brownian-motion', 'stochastic-calculus']"
2576705,Interesting function with geo,"For which pairs $(m,n)$ does there exist an injective function $f:\mathbb{ℝ}^2→\mathbb{ℝ}^2$ under which the image of every regular polygon with $m$ sides is a regular polygon with $n$ sides? That is probaly a realy hard problem. I love solving functions, but I haven’t solved functions in geometry. I don’t have any idea. Please help!","['functions', 'geometry']"
2576724,Dimension of $End(V)$ with $V$ countable dimension irreducible module over a complex algebra,"Let $A$ be a $\mathbb{C}$-algebra and $V$ be an irreducible $A$-module with countable dimension. What is the dimension of $End(V)$ as $A$-module? Note that every endomorphism must be injective and surjective, because of the irreducibility of $V$. My claim (or at least my hope) is that $End(V)$ has countable dimension, but I can't see a proof of this fact.","['abstract-algebra', 'division-algebras', 'cardinals', 'linear-algebra']"
2576751,Express $f_n(x)=\cos{(n\arccos{x})}$ as a polynomial.,"I had an interesting problem on an exam a few days ago in elementary calculus. It reads: Show that for $n\geq 2,$ the function $f_n(x)=\cos{(n\arccos{x})}, \ x\in[-1,1]$ is a polynomial of degree $n$ and determine the coefficient for $x^n$. I was not able to work this problem and after the exam I looked this up online and it seems as if this is related to the Chebyshev polynomials. But we have never covered these kinds of polynomials in this course. Is there other ways to do this? What I tried to do in the exam was to compute $f_n(x)$ for $n=1,2...$ and see if I can find a pattern and formulate an induction hypothesis and then prove it. I got that $$f_1(x)=\cos{(1\cdot \arccos{x})}=x,\\f_2(x)=\cos{(2\cdot \arccos{x})}=1-2\cos^2{(\arccos{x})}=1-2x^2\\
f_3(x)=\cos{(3\cdot \arccos{x})}=4\cos^3{(\arccos{x})}-3\cos{(\arccos{x})}=4x^3-3x$$ As you can see it quickly becomes ugly and there is no pattern to be seen. So, to formulate a hypothesis for even numbers $n=2k$ was not hard, but for odd numbers, $n=2k+1$ i could not do it. Is this start a good one or is this totally wrong? Would this method work If I was a bit better at math? any other tips/tricks that only uses elementary calculus? We are not allowed to use expansions in this course.",['calculus']
2576759,"Is $\operatorname{Hom}_R(R,G)\cong G$?","In this question ( How to show that for any abelian group $G$, $\text{Hom}(\mathbb{Z},G)$ is isomorphic to $G$ ), it is shown that $\operatorname{Hom}_\mathbb{Z}(\mathbb{Z},G)\cong G$, where $G$ is an abelian group (i.e. $\mathbb{Z}$-module). When we generalize to $R$-modules, can we still say $$\operatorname{Hom}_R(R,G)\cong G ?$$ (isomorphic as $R$-modules) ($R$ is a commutative ring with 1, $G$ is an $R$-module). I tried using the same isomorphism $\phi: \operatorname{Hom}_R(R,G)\to G$ defined by $f\mapsto f(1)$. It seems to work out; $\phi$ is an $R$-module homomorphism. It is injective since if $f(1)=0$, then $f(r)=rf(1)=0$ for all $r\in R$, so $f$ is the zero homomorphism. It is surjective since any for any $g\in g$, we can define a $f\in \operatorname{Hom}_R(R,G)$ such that $f(1)=g$, and then $f(r)=rf(1)=rg$ for any $r\in R$. Is the above reasoning correct? Thanks.","['abstract-algebra', 'modules']"
2576766,Rotation invariants for higher degree homogeneous polynomials (like Tr$(P^m)$ for degree 2)?,"Treating rotation in $\mathbb{R}^n$ as $x\to Ox$ for orthogonal $O^T O=O O^T=1$, we can easily get complete sets of independent rotation invariants for degree 1 and 2 homogeneous polynomials : Degree 1: for $p(x)=\sum_i P_i x_i$, rotation: $P_i\to \sum_a P_a O_{ai}$ has single invariant: $\sum_i P_i^2$:
$$\sum_i \left(\sum_a P_a O_{ai}\right)^2 = \sum_{ia\alpha} P_a O_{ai} P_\alpha O_{\alpha i} = \sum_{a\alpha} P_a P_\alpha \delta_{a\alpha} = \sum_a P_a^2$$ Degree 2: $p(x)=\sum_{ij} P_{ij} x_i x_j$ has $n$ independent rotation invariants: eigenspectrum of (symmetric) $P=P^T$, or equivalently $\{0,\ldots,n-1\}$ coefficients of characteristic polynomial $\det(P-\lambda I)$, or equivalently $\textrm{Tr}(P^m)$ for $m\in \{1,\ldots,n\}$. Let's check the last one: $$\textrm{Tr}((O^T P O)^m)=\textrm{Tr}(O^T P^m O)=\sum_{iab} O_{ai}(P^m)_{ab}O_{bi}=\sum_{ab}(P^m)_{ab} \delta_{ab}=\textrm{Tr}(P^m).$$ For degree 3 such rotation would analogously mean: $$P_{ijk}\to\sum_{abc} P_{abc} O_{ai} O_{bj} O_{ck}$$ How to construct rotation invariants for degree 3 and higher? How many independent invariants should we expect? My motivation is testing graph isomorphism problem, which after looking at eigenspaces of adjacency matrix becomes question if two sets of points differ only by rotation, and theses sets can be described using homogeneous polynomials ( stack ) - efficient testing for degree 3 or 4 should be sufficient. Update 1: In analogy to degree 1, here is one for degree 3: $\sum_{ijk} P_{ijk}^2$ (and the same for higher):
$$\sum_{ijk}\left(\sum_{abc} P_{abc} O_{ai} O_{bj} O_{ck}\right)^2=
\sum_{ijkabc\alpha\beta\gamma} P_{abc} O_{ai} O_{bj} O_{ck} 
P_{\alpha\beta\gamma} O_{\alpha i} O_{\beta j} O_{\gamma k}=\sum_{abc}P_{abc}^2$$ Update 2: In analogy to Tr$(P^2)=\sum_{ij} P_{ij} P_{ji}$ for degree 2, for degree 3 we can get invariant $\sum_{ijk} P_{ijk} P_{jki}$: $$\sum_{ijk}\left(\sum_{abc} P_{abc} O_{ai} O_{bj} O_{ck}\right)
\left(\sum_{\alpha\beta\gamma} P_{\beta\gamma\alpha} O_{\beta j} O_{\gamma k} O_{\alpha i}\right)
=\sum_{abc} P_{abc} P_{bca} $$
As every $ijk$ summation leads to one Kronecker delta $\delta$. Analogously $\sum_{ijk} P_{ijk} P_{kij}$. Update 3 (22.12.2017): We can analogously go with lager sets of variables, turning pairs of $O$ into Kronecker deltas: every variable should appear in exactly two terms. For example $\sum_{abcdef} P_{abc} P_{ade} P_{bcf} P_{def} $ is rotation invariant this way. We get nice combinatorics of indexes (like in free probability) - leading to final questions: For degree 2 we would take Tr$(P^{m})$ for $m=1\ldots n$, the next one: Tr$(P^{n+1})$ would be dependent - how to generally choose the largest independent set of rotation invariants? Looking at degree 2, I suspect what is crucial is determining its size (kind of size of eigenbasis) - then ""many""(?) sets of invariants of this size should be right (""determine eigenbasis""). Are they really all rotation invariants for homogeneous polynomials? (we know it is true for degree 1 and 2) If all required invariants agree, can we effectively determine the rotation? (I suspect the set of possible rotations might be nasty (?) ) Update 4: Diagrammatic representation of some first rotation invariants for degree 1,2,3,4: Degree of polynomial is also degree of vertex. Edge corresponds to summed index. Vertices and edges are unlabeled. Disconnected graphs gives invariants being products over its components. Update 5: We can analogously get rotation invariants for general polynomials: 
$$p(x)=p+\sum_i p_i x_i +\sum_{ij} p_{ij} x_i x_j + \ldots $$
Beside the homogeneous terms like in the above diagram, there are also additional mixing terms corresponding to graphs with vertices of varying degree, starting with $\sum_{ab} p_a p_{ab} p_b$. It is much better than what is offered by standard e.g. rotationally invariant spherical harmonics , which give only 1 rotation invariant per degree (angular momentum $l$), and don't allow to test mixing between them. Here we probably get a complete set of rotation invariants - such that their agreement ensures that polynomials differ only by rotation. How to choose such complete sets of invariants - completely defining polynomial of given degree modulo rotation? For degree 2: $p(x)=x^T A x + b^Tx+ c$ describes e.g. paraboloid. Rotation invariants are: $c$, $\sum_i b_i^2$, Tr$(A^k)$ for $k=1\ldots n$. Adding  $\sum_{ij} b_i (A^k)_{ij} b_j$ for $k=1...n-1$ invariants should completely define this paraboloid modulo rotation. Update 6: For a complete set of invariants (determining polynomial modulo rotation), there is needed an automatic procedure to calculate a lot of them. A simple way to do it is to build e.g. matrix like $M_{ab}=\sum_{cd} p_{acd} p_{cdb}$ and then construct Tr$(M^k)$ invariants for $k=1...n$. This was for ""-<>-"" graph with two external edges. Analogously we could construct ""-<=|=|...|=>-"" ladder-like graphs, constructed by expanding it step by step, and in each step getting $n$ invariants from closing such graph. The question is if such construction can be systematized to get a complete set of invariants? For degree $d$ there should be ${n+d-1 \choose d}-n(n-1)/2$ of them, plus $n(n-1)/2$ mixed invariants describing relative rotation to the lower degree terms. Update: I decided to write separate paper about these invariants, especially from perspective of machine learning applications (they are much stronger than standard approaches): https://arxiv.org/pdf/1801.01058","['polynomials', 'rotations', 'matrices', 'multilinear-algebra', 'invariance']"
2576768,A subset of a ring R closed under addition and multiplication by elements of R that is not an ideal of R,"A subset $I$ of a ring $R$ is an ideal if and only if $I$ is closed under subtraction and multiplication by elements of $R$. If $R$ has unity, this is equivalent to $I$ being closed under addition and multiplication by elements of $R$. Can anyone think of an example showing these conditions are not equivalent in general, i.e. that there is a ring $R$ (without unity) and a subset $I\subseteq R$ such that $I$ is closed under addition and multiplication by elements of $R$, but $I$ is not an ideal of $R$?","['abstract-algebra', 'ring-theory', 'ideals']"
2576776,Find all functions $f: \mathbb{R} \rightarrow \mathbb{R}$ such that $f(xy - 1) + f(x)f(y) = 2xy - 1$,"Using induction, I proved that $f(x) = x$ and $f(x) = -x^2$ work, but only for rational numbers. How can I prove them for all real numbers?","['contest-math', 'linear-algebra', 'functional-equations']"
