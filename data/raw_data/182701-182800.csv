question_id,title,body,tags
3336738,Statistics Question MME and MLE,"I have been attempting this question for a while and whenever I get to the standard error question of part (a) I keep getting 0. Ok so ai) $$\ E(X)= 3-2\theta $$ $$\ Var(X) = 2\theta -2\theta^2 $$ for aii) I found my MME by equating first theoretical moment (mu) to the 1st sample moment. First moment is $$ E(X) = 3-2\theta$$ and sample moment is $ 1/n\sum_{i=1}^n X_i =M_k $ ,where n=20 . This can be referred to as X.bar by equating these two I get an estimator of Theta, Theta.hat $$ Theta.hat = (3-2X.bar)/2$$ To get an estimate I plug in Xbar of 1.75 into here and get estimate of $$ \theta=0.625$$ Now to find Standard error I first need to find variance $$Var(theta.hat)= E(theta.hat^2)- E(theta.hat)^2$$ However now as I substitute in my estimator into the equation I find they just cancel, giving me a variance of 0",['statistics']
3336742,How to compute $\sum_{n=1}^\infty\frac{H_n^2}{n^32^n}$?,Can we evaluate $\displaystyle\sum_{n=1}^\infty\frac{H_n^2}{n^32^n}$ ? where $H_n=\sum_{k=1}^n\frac1n$ is the harmonic number. A related integral is $\displaystyle\int_0^1\frac{\ln^2(1-x)\operatorname{Li}_2\left(\frac x2\right)}{x}dx$ . where $\operatorname{Li}_2(x)=\sum_{n=1}^\infty\frac{x^n}{n^2}$ is the dilogarithmic function. Here is how the integral and the sum are related: From here we have $$\int_0^1x^{n-1}\ln^2(1-x)\ dx=\frac{H_n^2+H_n^{(2)}}{n}$$ Divide both sides by $n^22^n$ then sum up we get $$\sum_{n=1}^\infty \frac{H_n^2+H_n^{(2)}}{n^32^n}=\int_0^1\frac{\ln^2(1-x)}{x}\sum_{n=1}^\infty \frac{x^n}{n^22^n}dx=\int_0^1\frac{\ln^2(1-x)\operatorname{Li}_2(x/2)}{x}dx$$,"['integration', 'harmonic-numbers', 'polylogarithm', 'closed-form', 'sequences-and-series']"
3336746,How to prove that a set is equivalent to $\mathbb{Q}$?,"My goal is to prove that the set: AB = $\{ ab \mid a \in A, b \in B \} = \mathbb{Q}$ where $A=\{ a \in \mathbb{Q} \mid a < 1 \}$ and $B=\{ b \in \mathbb{Q} \mid b<2 \}$ .
I understand that $AB=\mathbb{Q} \iff (AB \subseteq \mathbb{Q}) \land (\mathbb{Q} \subseteq AB)$ but I'm unsure how to prove $(AB \subseteq \mathbb{Q}) \land (\mathbb{Q} \subseteq AB)$ with rigour.","['elementary-set-theory', 'real-analysis']"
3336771,Impossible hyperbolic integral,"I'm quite good with integral calculation, very rarely happens that I'm not able to solve an indefinite integral. I've tried for 7 days to solve this monster but in the end, I surrendered to this beast. I'm sure that it has a solution because it was a challenge from my calculus professor. Unfortunately, he didn't give the solution of this integral. It has been unsolved for years, since 7 days ago when I found it in my old notebook. Can some expert help me? $$\int { \frac { \sinh { x }  }{ \sqrt { 2\cosh { x+2 }  }  }\frac { 1+\sqrt { \frac { \sqrt { \cosh { x } +1 } +\sqrt { 2 }  }{ \sqrt { \cosh { x } +1 } +2\sqrt { 2 }  }  }  }{ 1-\sqrt [ 3 ]{ \frac { \sqrt { \cosh { x } +1 } +\sqrt { 2 }  }{ \sqrt { \cosh { x } +1 } +2\sqrt { 2 }  }  }  }  } dx$$","['integration', 'indefinite-integrals', 'calculus', 'hyperbolic-functions']"
3336777,Theoretical link between Two Solutions and the Existence and Uniqueness Theorem,"I am new to Differential Equations, and am one step away from the solution to what was a very challenging exercise. Any help would be amazing. Find two different solutions to the initial value problem $$xy'-2x^2 \sqrt{|y|}=4y~,~~~~~~~~~~~~~~y(1)=0$$ and explain how the existence of two different solutions to this problem abides by the Existence and Uniqueness Theorem. After a lot of work I found the solutions to this problem, namely $|y|^{\frac{1}{2}}=x^2\ln|x|$ , which renders two $x$ solutions for each $y$ . However, I am not sure how to link this back theoretically to the Existence and Uniqueness Theorem. I have two theories, but I am not sure regarding either of them: ${}$ $1.~$ the derivative of $xy'-2x^2 \sqrt{|y|}=4y$ , that is to say $y'=2x \sqrt{|y|}+4\frac{y}{x}$ , is $~\frac{xy}{|y|\sqrt{|y|}}+\frac{4}{x}$ , and this is not defined at (1,0). I am not sure if that is a valid way of justifying why the Existence and Uniqueness Theorem does not apply. ${}$ $2.~$ To show that there is no $L$ , such that $f(x,y)-f(x,0) \leq L(y-0)$ . This leads me to $2x \sqrt{|y|}+4\frac{y}{x}\leq L(y)$ . Again, I am not sure if this is valid, and - if so - how to take it to the next step. Any insight would be immensely appreciated. Thank you! ${}$ PS:",['ordinary-differential-equations']
3336781,Find the limit of $\arctan(y/x)$,"I need to calculate the limit of function $\arctan(y/x)$ for the arguments $(x,y)\to(0,1)$ . Earlier I thought it's a bit easy but I just cannot get any answer, Instinctively it may look like limit is $\pi/2$ but one could also argue about two paths yielding different limits. Kindly help!! Thanks & regards","['multivariable-calculus', 'limits', 'calculus', 'inverse-function']"
3336788,$\mathrm{rank}(M)=\mathrm{rank}(M^2)$ whenever $M$ is skew-symmetric,"On p.231 of Linear Algebra by Greub, it is stated that a real skew-symmetric matrix has the same rank as its square, i.e., $\mathrm{rank}(M)=\mathrm{rank}(M^2)$ whenever $M$ is real skew-symmetric. I tried to use the fact that skew-symmetric matrix is normal and some geometric properties of normal matrices, but cannot proceed. Any help is appreciated.","['matrices', 'matrix-rank', 'linear-algebra']"
3336817,Derive the EGF of Stirling number of the second kind with the OGF-EGF conversion formula,"It is well known that, the EGF(Exponential Generating Function) of Stirling number of the second kind is $$
\hat G_{k}(x)=\sum_{n=k}^{\infty} S(n, k) \frac{x^{n}}{n !}=\frac{1}{k !}\left(e^{x}-1\right)^{k}
$$ And the OGF(Ordinary Generating Function) of Stirling number of the second kind is $$
G_k(x) = \sum_{n\ge k} S(n,k) x^n =
\frac{x^k}{(1-x)(1-2x)\cdots(1-kx)}
$$ The OGF-EGF conversion formula is given here Generating function transformation , that is $$
\hat F(z)={\frac {1}{2\pi }}\int _{-\pi }^{\pi }F\left(ze^{-\imath \vartheta }\right)e^{e^{\imath \vartheta }}d\vartheta 
$$ where, $\hat F(z)$ means the EGF and $F(z)$ means the OGF. However, I need to calculate $$
\int_{-\pi }^{\pi } \frac{e^{e^{i t}} \left(-e^{-i t} z\right)^{-k} \left(e^{-i t} z\right)^k}{\left(1-\frac{e^{i t}}{z}\right){}_k} \, dt, \text{where } (x)_k \text{ the Pochhammer symbol.}
$$ which is  really difficult for me. So my problem is to calculate the integral above .","['special-functions', 'complex-analysis', 'combinatorics', 'generating-functions', 'sequences-and-series']"
3336847,75th percentile of $e^x$,"Let $f(x) = e^x$ on the interval $[0,2]$ and $0$ everywhere else. Calculate the 75th percentile of $f(x)$ . Attempt So normally I would take the integral from $\int_{- \infty }^{x} f(x)dx$ , equal it to $0.75$ and solve for $x$ . But $f(x)$ only exists between $0$ and $2$ , so I take the integral from $0$ to $x$ and equal it to $0.75$ . Solve for $x$ gives $$e^x - e^0 = 0.75$$ or $$x = \ln(1.75) = 0.560 ~ .$$ However my book says it's $4.48$ , I did some calculations and found that's equal to $e^{1.75}$ . So am I wrong? How could the percentile lie outside the interval?",['statistics']
3336873,$L^2$ dense in $L^1$ under certain conditions?,"I'm currently trying to understand a proof which approximates an $L^1$ -function on a probability space with $L^2$ -functions. In this context I'm wondering if for a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ it holds that $L^2(\mathbb{P})$ is dense in $L^1(\mathbb{P})$ . Or how could one otherwise justify such an approximation? Thanks in advance!","['lp-spaces', 'probability-theory']"
3336908,"Consider $a,b\in \Bbb Z_n$ such that $o(a)=r,o(b)=s$. If $s\mid r$ then $b\in \langle a\rangle $. [duplicate]","This question already has answers here : Subgroups of a cyclic group and their order. (5 answers) Closed 4 years ago . Is the following statement true ? If yes prove it and if no then give a counterexample? Consider the finite cyclic group $\Bbb Z_n$ . Consider $a,b\in \Bbb Z_n$ such that $o(a)=r,o(b)=s$ .
If $s\mid r$ then $b\in \langle a\rangle $ . Initially I thought the statement is correct.
But the problem is I am unable to prove it. How to prove it?","['group-theory', 'abstract-algebra', 'finite-groups']"
3336941,partition that maximizes lcm,"Let $n$ be a natural number. How can I find a partition of $n$ such that its least common multiple is maximized? In other words, how can I find $a_1, a_2, \cdots a_m \in \mathbb{N}$ such that $$a_1 + a_2 + \cdots + a_m = n \\\text{and} \\ \text{lcm}(a_1, a_2, \cdots ,a_m) \text{ is largest}$$ ? If not possible to find it, is the largest lcm value bounded by some functions of n? I tried to find it's upper bound by multiplying all numbers from each partition, and using a little calculus, able to drive the upper bound $e^{n/e}$ . But it seems a bit too loose. Is there a better bound for lcm?","['integer-partitions', 'number-theory', 'gcd-and-lcm']"
3336956,"Evaluate $\int_{-\infty }^{\infty } \left(\cos \left(\sqrt{x^2-1}\right)-\cos \left(\sqrt{x^2+1}\right)\right) \, dx$","From Gradshteyn & Ryzhik $3.692.6$ we know that $$\int_{-\infty }^{\infty } \left(\cos \left(\sqrt{x^2-1}\right)-\cos \left(\sqrt{x^2+1}\right)\right) \ dx=\pi  (J_1(1)+I_1(1))$$ How can we establish it? Thanks for helping. Update: I found a proof. One may start from the well-known formula $$\int_{-\infty }^{\infty } \frac{\sin \left(p \sqrt{a^2+x^2}\right)}{\sqrt{a^2+x^2}} \, dx=\pi  J_0(a p)$$ Let $a=i a$ , subtract it from the original result, differntiate with respect to $p$ yields: $$\int_{-\infty }^{\infty } \left(\cos \left(p \sqrt{x^2-a^2}\right)-\cos \left(p \sqrt{a^2+x^2}\right)\right) \, dx=\pi  a (J_1(a p)+I_1(a p))$$ Now letting $p=a=1$ completes the proof. This also verified @skbmoore's generating function identity. Furthermore, using the same technique in my answer of this post , a formula revealing beautiful symmetry is found: $$\int_{-\infty }^{\infty } \left(\cos \left(p \sqrt{x^2-a^2}\right)-\cos \left(p \sqrt{a^2+x^2}\right)\right) \, dx=\sum _{n=-\infty }^{\infty } \left(\cos \left(p \sqrt{n^2-a^2}\right)-\cos \left(p \sqrt{a^2+n^2}\right)\right)=\pi  a (J_1(a p)+I_1(a p))$$","['integration', 'improper-integrals', 'definite-integrals', 'special-functions', 'bessel-functions']"
3336960,Summation of fractional parts of $x/n$,"Let us consider the sums $$\displaystyle T_1(x)=\sum_{n\geq a\sqrt{x}}^{b \sqrt{x}} \left\{ \frac{x}{n} \right\} \\T_2(x)=\sum_{n\geq a\sqrt{x}}^{b \sqrt{x}} \left\{ \frac{x}{2n} \right\}
  $$ where $x$ is a positive integer, $0\leq a<b\leq 1$ , $n$ runs over all integers in the interval $[a \sqrt{x}, b\sqrt{x}]$ , and $\{ \}$ indicates the fractional part. Because of the equidistribution of such fractional parts, the plots of $T_1(x)$ and $T_2(x)$ versus $x$ show oscillations around the line $\frac{b-a}{2} \sqrt{x}$ . By experimental calculations, I noted that the average value of the difference between $T_1(x)$ (or $T_2(x)$ ) and $\frac{b-a}{2} \, \sqrt{x}$ , calculated over all positive integers $x \leq N$ , is $O(1)$ . In particular, we have $$\lim_{N \rightarrow \infty} \frac{1}{N} \sum_{x=1}^{N} \left( T_1(x)- \frac{b-a}{2} \sqrt{x} \right)= K_1(a,b)\\
\lim_{N \rightarrow \infty} \frac{1}{N} \sum_{x=1}^{N} \left( T_2(x)- \frac{b-a}{2} \sqrt{x} \right)= K_2(a,b)$$ where $K_1(a,b)$ and $K_2(a,b)$ are constant terms that depend on $a$ and $b$ , indicating that the distributions of such differences are biased. The problem can be simplified by considering the case where $b=1$ (once solved this case, the general case with $b<1$ can be solved by difference). In this case, the biases are determined only by $a$ . For example, analyzing the distributions of differences over all integers $N$ up to $2500$ , setting $a=0.5$ we have $K_1(0.5,1)\approx -0.370...$ and $K_2(0.5,1)\approx -0.169...$ , with rather slow convergence rates. I wonder how these terms are generated and whether they could be exactly calculated. Again based on experimental results, it seems that $K_1$ and $K_2$ have a logarithmic relation with $a$ , with values near to $\frac{1}{2} \log(a)$ and $\frac{1}{4} \log(a)$ , respectively. This question is somewhat linked to this other one, for which a nice answer was previously given. EDIT (after Mathworker21's answer): A rough numerical computation until $N=10000$ for the case $a=0.5$ , $b=1$ seems to confirm the above estimates obtained with $N$ up to $2500$ . Here is the plot of the differences $T_1(x)- \frac{b-a}{2} \sqrt{x}$ versus $x$ , followed by that of the average value of these differences calculated over the first $N$ integers. As show in this second plot, $K_1(0.5,1)$ seems to converge to $\approx -0.37$ . The dotted black line in the first graph is the best fitting line, whose intercept is compatible with such value. Based on a visual assessment, large departures from this value (as suggested by the provided answer) for higher $N$ seem unlikely: Similar considerations can be made for $K_2(0.5,1)$ , which seems to converge to $\approx -0.17$ :","['number-theory', 'summation', 'asymptotics']"
3337070,Symmetric Rank-1 Decomposition for Density Matrices,"Let $(H,\langle\cdot,\cdot\rangle)$ be an $n$ -dimensional complex Hilbert space. For concreteness, you can just take $H=\mathbb{C}^n$ with standard inner product. Note that we will use the physicist's convention of the inner product being complex-linear in the second entry. Consider the symmetric $k$ -fold tensor product product of $H$ with itself, which we will denote by $H^{\otimes_s^k}$ . Similarly, consider the symmetric $k$ -fold tensor product of the dual of $H$ with itself, which we will denote by $H^{*,\otimes_s^k}$ . We are interested in elements of the tensor product $H^{\otimes _s^k} \otimes H^{*,\otimes_s^k}$ . Of course using the Riesz representation theorem, we can identify $H^*$ with $H$ using the inner product and so $H^{\otimes _s^k} \otimes H^{*,\otimes_s^k}$ is the complex span of elements of the form $$|f\rangle\langle g|, \qquad f,g\in H^{\otimes_s^k}, \tag{1}$$ where we have used Dirac's bra-ket notation. We are not interested in the whole space $H^{\otimes _s^k} \otimes H^{*,\otimes_s^k}$ , but only those elements which are self-adjoint , by for an element of the form (1) means $f=g$ . My question is the following: Question. Is it possible to write every self-adjoint element of $H^{\otimes_s^k} \otimes H^{*,\otimes_s^k}$ into a linear combination $$\sum_{j=1}^N a_j |f_j^{\otimes k}\rangle\langle f_j^{\otimes_k}|, \tag{2}$$ where $N\in\mathbb{N}$ , $a_j\in\mathbb{C}$ , and $f_j\in H$ ? I know that it is possible to write every element $f\in H^{\otimes_s^k}$ as $$f=\sum_{j=1}^N a_j f_j^{\otimes k},$$ and consequently every self-adjoint of element of $H^{\otimes_s^k}\otimes H^{*,\otimes_s^k}$ can be written as $$\sum_{j=1}^N a_j(|f_j^{\otimes k}\rangle\langle g_j^{\otimes k}| + |g_j^{\otimes k}\rangle\langle f_j^{\otimes k}|).$$ However, I do not know how to prove such a symmetric rank-1 type decomposition like (2).","['tensors', 'abstract-algebra', 'linear-algebra', 'tensor-products', 'commutative-algebra']"
3337101,"If $R \subseteq A \times B$ is a surjective relation, as is $S \subseteq B \times A$, are they bijective relations?","Suppose two sets $A$ and $B$ , two relations $R \subseteq A \times B$ and $S  \subseteq B \times A$ . If both of them are surjective, then both of them are bijective relations. Is this correct? Can we disprove this with the following example: $A =  \{ 1, 2 \}$ $B = \{3\}$ $R = A \times B =  \{  (1, 3) , (2,3) \} $ $S = B \times A =  \{  (3,1), (3,2)    \}$ Both of $R,S$ are certainly surjective but $R$ isn't injective, isn't it?","['elementary-set-theory', 'relations', 'solution-verification', 'discrete-mathematics']"
3337102,Proving norm closure of a set of functions in $L^1$,"I'm working at optimal control and I came across this situation. We have a set of functions that consists of all the measurable functions with values in $U \subset \mathbb{R}^m$ a.e., where $U$ is compact. Consider a function $$F:\mathbb{R} \times \mathbb{R}^m \to \mathbb{R}^n$$ $$(t,u) \mapsto F(t,u)$$ such that $F$ is continuous in $u$ for each $t$ and such that $F$ is measurable in $t$ for each $u$ . Clearly the set of functions defined on a finite interval $I$ $$K = \{u \mid u(t) \in U \; a.e. \; \mathrm{in} \; I \}$$ is closed in $\Vert .\Vert_1$ . Here, as usual, by $\Vert .\Vert_p$ of a function $f : \mathbb{R} \to \mathbb{R}^n$ we mean $$\Vert f\Vert_p^p = \int_I \sum_{i=1}^n \mid f^i(t) \mid^p dt$$ where $f^i$ are the coordinate functions of $f$ . $\textbf{The problem is}$ Prove that the set of functions $$H = \{v \mid v(t) = F(t,u(t)) \; u \in K \}$$ is closed too. We have also the hypotesis that $$\Vert F(t,u) \Vert \leq m(t)$$ with $m$ a function that is integrable on the finite intervals of $\mathbb{R}$ . I tried, but I can't prove it. Does someone have any ideas? Thanks to everyone who wants to help me! P.S. Note that if $F$ is injective in $u$ a.e. then the statement is clearly true, and note also that if $m=1$ the statement is clearly true either. $\textbf{Edit}$ Maybe I found something. Let $u_n$ be the sequence of functions as above. Then all these functions are in $L^2(I)$ , thanks to the fact that $U$ is compact. The sequence is also bounded, clearly. $L^2$ is reflexive, so we can use the following well known theorem of functional analysis $\textbf{Theorem}$ Let $E$ be a reflexive normed space and $x_n$ a bounded sequence. Then there exists a sub-sequence converging weakly. In our case, we can then extract a subsequence $u_{n_k}$ converging weakly to something, and so it must be pointwise convergent to some function in K, thanks to the closure of $U$ . (Clearly this doesn't work as weakly convergence doesn't implie pointwise convergence.) $\textbf{Edit}$ If $u$ was a function with real values (not in $\mathbb{R}^m$ ) we could just take $\limsup u_n(t)$ . Is there something similar we can do in general here? $\textbf{Edit}$ Maybe we could define $f(s) = \lim_{n \to \infty} F(s,u_n(s))$ and then consider the multivalued function $$G(t) = \left\{u \in U \mid f(t) = F(t,u) \right\}$$ Now if we can prove that there exists at least one measurable selection of $G(t)$ we could take that selection to prove the closure of $H$ ! Now $G(t)$ is closed valued and non empty, so by a known theorem there exists a measurable selection if $G$ is measurable... so can someone prove that $G$ is measurable? $\textbf{Definition}$ A set valued map $G: \Omega \to \mathbb{R}^n$ is measurable if for every closed set $C \subset \mathbb{R}^n$ the set $$\left\{t \in \Omega \mid G(t) \cap C \neq \emptyset \right\}$$ is measurable.","['optimal-control', 'functional-analysis', 'analysis', 'real-analysis']"
3337119,Five people are tossing a coin ten times. What is the probability that at least 1 person gets heads 10 times?,"I know that the probability that one person getting heads all ten times is 1/(2^10) or 1/1024. I also know the calculation when using ""at least one"" is P(At least one) = 1 - P(None). The probability of one person getting no heads is the same: 1/1024. The part that is confusing is the number of people.  Is it P(At least one) = 1 - (1/1024)^5? If there were 100 people tossing a coin, the probability that at least one person gets all ten heads is P(At least one) = 1 - (1/1024)^100? Is this correct?",['probability']
3337130,Derivative of $e^x$ (i),"This will be a very basic question, but it will help my thought process to have someone answer it. $e$ can be defined as the number such that $$\frac{de^x}{dx}=e^x\;\;\;\;\;(1)$$ Taking $x=2$ , this implies that $$\frac{de^2}{dx}=e^2\;\;\;\;\;(2)$$ The power rule gives us that $$\frac{de^2}{dx}=2e\;\;\;\;\;(3)$$ Therefore from $(2)$ and $(3)$ we get $$e^2=2e\;\;\;\;\;(4)$$ But a calculator tells me that $(4)$ is false. Where have I gone wrong?","['calculus', 'derivatives', 'exponential-function']"
3337154,"Does $\lim_{\epsilon \to 0^+} \frac{\int_0^\epsilon f}{\epsilon^{1/q}} = 0$ hold for all $f\in L^p[0,1]$.","Fix $1 < p < \infty$ and choose $f\in L^p[0,1]$ .
Let $q = \frac{p}{p-1}$ denote the Hölder conjugate of $p$ . I am trying to prove the following statement: $$\lim_{\epsilon \to 0^+} \frac{\int_0^\epsilon f}{\epsilon^{1/q}} = 0$$ $$\rule{10cm}{1pt}$$ My attempt:
Hoping to apply L'Hôpital's Rule, I took derivatives of top and bottom $$\frac{q\cdot F'(\epsilon)}{\epsilon^{-1/p}}$$ where $F'(\epsilon) = \frac{d}{d\epsilon}\int_0^\epsilon f$ .
I know that $F'(\epsilon) = f$ a.e. on $[0,1]$ and I also know that if $|f|$ grows at least as fast as $\epsilon^{-1/p}$ as $\epsilon \to 0^+$ , then $f \notin L^p[0,1]$ . But I'm having trouble using those facts to prove something like $$\limsup_{\epsilon \to 0^+}\frac{|F'(\epsilon)|}{\epsilon^{-1/p}} = 0$$ For example, is it possible that $|F'(\epsilon)| \geq \epsilon^{-1/p}$ on a small enough set that $F' \in L^p[0,1]$ ?","['limits', 'real-analysis']"
3337172,"Alternate proof of the isomorphism $T_{(p,q)}(X \times Y) \cong T_p X \times T_q Y$","As in the title, with $X$ and $Y$ smooth manifolds. I am filling in the details of a proof here , and I have a question about identifications of tangent vectors/spaces. Fix $(p,q)\in X\times Y$ . Let $i:x\mapsto (x,q)$ and $\pi:(x,y)\mapsto x$ . Then, $L=d(i\circ \pi)$ is a projection, so $\text{ker}(L)\oplus \text{im}(L)=T_{(p,q)}(X\times Y).$ Assume first that $X$ and $Y$ are Euclidean spaces, and take a curve $\gamma:(-\epsilon,\epsilon)\to X\times Y$ with $\gamma (0)=(p,q)$ . Now, $\gamma (t)=(\gamma_1(t),\gamma_2(t))$ with $\gamma_1(0)=p$ . Then, $d(\iota_X\circ \pi_X)_{(p,q)}\gamma'(0) = \frac{d}{dt}|_{t=0}(\gamma_1(t),q) = (\gamma_1'(0),0)$ so the image of $L$ is isomorphic to $T_pX.$ Simlilarly, since $di$ is injective, $\text{ker}L=\text{ker}\pi$ so for $\gamma$ as above we have $(d\pi)_{(p,q)}\gamma'(0)=\frac{d}{dt}|_{t=0}\pi\circ\gamma(t)=\gamma_1'(0).$ If this is to be equal to $0$ then $T_pX=0$ and so $\text{ker}L\cong T_qY.$ Now, suppose that $X$ and $Y$ are arbitrary manifolds, so that now the tangent vector is defined by $d(\iota_X\circ \pi_X)_{(p,q)}\gamma'(0)(f) = \frac{d}{dt}|_{t=0}f(\gamma_1(t),q)$ for smooth $f:X\times Y\to \mathbb R.$ In the above link to the sketch of a proof, the author uses the first blockquote equation for the arbitrary manifolds $X$ and $Y$ , (the second is mine) but isn't there a problem with this? He/she seems to be using the same symbol for different objects and/or making some identification. Namely, the equation that should be used is that in the third blockquote, because if not, then the $\gamma'(0)$ on the LHS is a tangent vector in $T_{(p,q)}(X\times Y$ ), whereas the RHS is not a tangent vector. The derivative does not even make sense unless we are working in some coordinate chart, but even in this case, it is not clear to me what identification(s) we are using. I am not claiming that the result is false, only that there is some explanation that I am missing here. In fact, I have a general problem with this type of machinery one uses in differential geometry (I am self-studying Lee). I understand how the various definitions of the tangent space are equivalent, but I have trouble understanding equations in which there are implicit identifications made.","['proof-verification', 'smooth-manifolds', 'alternative-proof', 'differential-topology', 'differential-geometry']"
3337189,Rubik's cube problem - Combinatorics required [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question There are $4.3\times 10^{19}$ combinations possible with a standard $3\times 3\times 3$ Rubick's cube. My question is, if you would take 2 cubes and scramble them randomly, what are the odds of perfectly matching a single side on both cubes?","['statistics', 'combinatorics']"
3337248,Internal direct sum of infinitely many subspaces,"I understand what the (external) direct product of an infinite number of vector spaces. I understand the internal direct sum of a finite number of vector spaces. I think I understand what an external direct sum of vector spaces is. My question is, Does it make sense to talk about an internal direct sum of infinitely many subspaces of a vector space? If so, how is this defined?","['definition', 'direct-sum', 'linear-algebra', 'vector-spaces']"
3337261,Is $\lim\limits_{n\to\infty}\Bigl(\frac{2+\sin n}{3}\Bigr)^n =0 $; $n \in \mathbb{N}$?,$$\lim_{n\to\infty}\Bigl(\frac{2+\sin n}{3}\Bigr)^n $$ where $n$ is a natural number. The problem is if the numerator is less than 3. I think it is because $\sin  n$ is never $1$ otherwise $\pi$ would be rational. But i am not sure if the limit exists because $\sin$ oscillates. How to solve this ?,"['limits', 'sequences-and-series', 'real-analysis']"
3337299,"Among midpoint method, Heun's method and Ralston method, which method of solving ODE performs better in which case and why?","Midpoint method, Heun's method and Ralston method- all are 2nd order Runge-Kutta methods. I know how these methods work. Are there some specific functions for which one of these methods performs better than the other two? If yes, what are those functions? And why do we see the difference in performance?","['numerical-methods', 'ordinary-differential-equations']"
3337302,"Is $\{\sin^n{(n)}:n\in\mathbb{N}\}$ dense in $[-1,1]$?","I know that it's true that the set $\{\sin{(n)}:n\in\mathbb{N}\}$ is dense in $[-1,1]$ but is the set $\{\sin^n{(n)}:n\in\mathbb{N}\}$ also? I would assume it is but I'm unsure of how to prove this because the $n$ th power can change the sign of the terms and reduces their absolute value. On a similar note, is it true that $$\limsup_{n\to\infty}\sin^n{(n)}=1$$ $$\liminf_{n\to\infty}\sin^n{(n)}=-1$$ where $n\in\mathbb{N}$ ? Both of these results would follow if the associated set is dense in $[-1,1]$ but if the initial statement is false then is it possible to seperately prove the limits above? I have no university level education so I don't think I would be able to provide context such as my own working etc. but I'm quite interested in seeing a proof of the above results as I cannot find them elsewhere.","['limits', 'trigonometry', 'supremum-and-infimum', 'sequences-and-series']"
3337332,Entire function whose square or composition with itself is a polynomial,"Let $f$ be an entire function whose square $f^2$ is a polynomial. Then is $f$ a polynomial as well? I think due to the Great Picard Theorem, since $f$ cannot assume any complex value infinitely many times, it is forced to be a polynomial. Is my argument correct? Also if $f$ is an entire function such that $f(f(z))$ is a polynomial, the same reasoning with the Great Picard Theorem shows that $f$ is again forced to be a polynomial. Would anyone please give me a comment on my argument?",['complex-analysis']
3337358,Does a false statement always have a counterexample?,"It feels like a dumb question, but is it always the case that a false statement has a counterexample? Is there a proof of it (or of the contrapositive, if it has no counterexample it must be true)?","['logic', 'discrete-mathematics']"
3337414,Is the main purpose of the dot product to find the angle between 2 vectors?,"From what I have read , there is no trigonometric way to find the angle between the hypotenuse and the adjacent side of a triangle unless that triangle is a right triangle. Or alternatively said, it is trigonometrically impossible to find an angle between 2 sides of an oblique triangle, if all you know is the length of those 2 adjacent sides. But using the dot product, we are able to find the length required to make the adjacent side perpendicular to the opposite side, where that opposite side intersects the hypotenuse. To clarify what I mean I attached an illustration: click me Then we can use trigonometry to find the angle between the hypotenuse and the adjacent side because now this triangle is a right triangle. My questions is: I've been trying to understand the importance of the dot product geometrically. However, I'm recently wondering if the real importance of the dot product is not what it geometrically represents but what it can be used to find. So is the real importance/purpose of the dot product is that we can use it to ""convert"" 2 vectors into 2 vectors that form a right triangle so we can find the angle between them knowing only the magnitude of those two vectors?","['calculus', 'vectors', 'trigonometry']"
3337431,"The derivatives of a real polynomial function $f(x,y)$ are divisible by a certain polynomial of $x$ and $y$. What can we know about $f\mkern1mu$?","Given $f(x,y)$ , a two real variable polynomial such that $\frac{\partial f(x,y)}{\partial x}$ and $\frac{\partial f(x,y)}{\partial y}$ are divisible by $x^2+y^2-1$ . Prove that there exist a polynomial $g(x,y)$ and a constant $c$ such that: $$f(x,y)= g(x,y){(x^2+y^2-1)}^2+c$$ That is the problem I'm currently trying to solve. I first tried to write de derivatives in the form $P(x,y)*
({x^2+y^2-1})$ with $P(x,y)$ being a polynomial. Then tried to write both $P$ and ${x^2+y^2-1}$ as derivatives of a function (one at a time) to perform integrarion by parts, but it led me nowhere. Any ideas in how to solve it? If it's possible to just give me a hint in how to start and not the whole solution I'd appreciate it very much","['multivariable-calculus', 'polynomials', 'real-analysis']"
3337432,In how many ways we can distribute 7 distinct balls among 3 students such that everyone gets at least 2 balls?,"So in order to solve this question, I first found all the number of ways in which every student gets at least one ball, i.e. $$3^7-3C2 \times (2^7-2) - 3C1=1806$$ and then I subtracted it with number of ways where exactly one ball is given to one or two students, i.e. $$1806-(3\times7C1(6C2+6C3+6C4))-(3\times7C1\times6C1)=1806-1050-126
=630 ways$$ So can anyone tell is this approach is right? and if you are still not able to understand my answer, please ignore it and solve the question. Thanks in advance","['permutations', 'combinations', 'combinatorics', 'contest-math']"
3337447,Show that $2\cos z = e^{iz} + e^{-iz}$. Do I need absolute convergence?,"I'm reading Conway's ""Functions of one complex variable"". Define $$e^z =\sum_{n=0}^\infty \frac{z^n}{n!}$$ $$\cos z =\sum_{n=0}^\infty \frac{(-1)^nz^{2n}}{(2n)!}$$ It is already shown that these converge for all $z \in \mathbb{C}$ . I'm asked to show that $$e^{iz} + e^{-iz} = 2 \cos z$$ I have two questions: (1) Is my attempt correct?
(2) Did I need absolute convergence anywhere? (The author claims that this should be necessary) $$e^{iz} + z^{-iz} = \sum_{n=0}^\infty \frac{(iz)^n}{n!} + \sum_{n=0}^\infty \frac{(-iz)^n}{n!} \quad (definition)$$ $$= \sum_{k=0}^3 \left(\sum_{n=0}^\infty \frac{(iz)^{4n+k}}{(4n+k)!} + \sum_{n=0}^\infty \frac{(-iz)^{4n+k}}{(4n+k)!}\right) \quad (**, see \ lemma  \ down \ with \ k =4)$$ $$= 2 \left( \sum_{n=0}^\infty \frac{z^{4n}}{(4n)!}- \sum_{n=0}^\infty \frac{z^{4n+2}}{(4n+2)!} \right) \quad (simplifying \ sums)$$ $$= 2 \sum_{n=0}^\infty \left(\frac{z^{4n}}{(4n)!}- \frac{z^{4n+2}}{(4n+2)!} \right) \quad (linearity)$$ $$= 2 \sum_{n=0}^\infty \frac{(-1)^n z^{2n}}{(2n)!} = 2\cos z \quad (** \ with \ k =2, definition)$$ Note that I used the following lemma (**): $$k \geq 2, \sum_{n=0}^\infty a_n \mathrm{\ converges}\implies \sum_{n=0}^\infty a_n = \sum_{n=0}^\infty (a_{kn} + \dots a_{kn+(k-1)})$$ This can be proven by observing that the right series has partial sums that are a subsequence of the partial sums of the left series.","['complex-analysis', 'power-series', 'absolute-convergence', 'sequences-and-series']"
3337455,second derivative of a function equals the function squared,"Can someone solve the following differential equation for me please?
The second derivative of a function equals the function squared. Find $y(x)$ if $$
\frac{d^2 y}{dx^2} = y^2
$$","['differential', 'ordinary-differential-equations']"
3337457,$\nabla u=0~ a.e.\Rightarrow u=\text{constant}~ a.e. $ on Riemannian manifold,"Let $M$ be a Riemannian manifold and $\Omega\subset M$ be a connected open set. Is it true that for $u\in W^{1, 2}(\Omega)$ $$\nabla u=0~ a.e.\Rightarrow u=\text{constant}~ a.e.?$$ I know that this true if $M=\mathbb{R}^{n}$ by mollification, but does this also hold in this general setting?
Thanks in advance!","['riemannian-geometry', 'analysis', 'partial-differential-equations']"
3337460,Optimize the Game - Make Fair Game with Biased Coins,"The traditional question states that given an unfair coin, with 0.7 probability of heads, how would you make a fair game? The answer is only consider two outcomes. If two trials yields $HT$ then the player wins, else if $TH$ the player loses. Now the interesting part is the follow-up question. If now my head probability is 0.99, how should one go about optimizing making a fair game out of this coin? ""Optimizing"" means to come up with a fair game with minimal amount of coin tosses. The original approach is inefficient, because most of the outcome would be $HH$ , with probability $0.99^2$ ). The probability of $HT$ or $TH$ would be relatively small.","['statistics', 'probability']"
3337472,Axiom of Choice as a tool for proving measurability,"We know that non measurable sets in $\mathbb{R}$ exist, relative to the Lebesgue measure, but for finding one we need the Axiom of Choice. So I was thinking, maybe every time we need to prove measurability of a set, we could say that we didn't need the Axiom of Choice for constructing that set? For example, take two measurable functions $f,g:\mathbb{R} \to \mathbb{R}$ . We know that $$\left\{x \in \mathbb{R} \mid f(x) =g(x) \right\}$$ is measurable. Could we prove it just saying that we didn't use the Axiom of Choice to build that set? Thanks for your answers. Edit To be more precise, my question is : if we make operations starting from some sets that are measurable or measurable functions, then we can conclude measurability just noticing that we haven't used the axiom of choice? (If we actually didn't)","['axiom-of-choice', 'measure-theory', 'lebesgue-measure']"
3337510,"How can I represent the (haskell, python etc.) zip function in math notation?","It is similar to cartesian product of sets $\{a,b\} \times \{c,d\} = \{(a,c), (a,d), (b,c), (b,d)\}$ , but applied to (ordered) tuples and without the permutations that don't match, i.e. $\langle a,b\rangle zip \langle c,d\rangle = \langle(a,c), (b,d)\rangle$ .",['elementary-set-theory']
3337546,What does it mean for a function to be spherically symmetric?,"I have searched but I couldn't find much. I was asked to show if a function $f=f(x,y,z)$ , say, is spherically symmetric.
Does this mean if we rotate $x,y,z$ axises then the function will stay the same? Like, if $f=x^2+y^2+z^2$ then this is symmetric spherically? but if $f=x+y+z$ then this is not spherically symmetric? (Just two easy examples I thought of) Any help is appreciated! Bit more context : This was a wave equation I got when doing quantum mechanics, $\psi(x,y,z)=Ae^{-\frac{m\omega (x^2+y^2+z^2)}{2\hbar}}$ , which I can see that this is just depending on $|\vec r|$ really but I am struggling to explain in greater detail as I don't seem to have an exact definition. (A is just a constant)","['functions', 'terminology']"
3337562,Understanding the Poincaré cone condition and its intuition (in probabilistic solutions to PDEs),"So, context. Theorem. Let $D\subseteq\mathbb R^d$ be bounded and satisfy the Poincaré cone condition and let $\varphi:\partial D\to\mathbb R$ be $C^0$ boundary conditions. Let $(B_t)_{t\geq0}$ be a standard Brownian motion and $\tau:=\inf\{t\geq0:B_t\in\partial D\}$ . Then a $C^2$ solution to the PDE $$\Delta u=0$$ in the interior of $D$ is $$u(x)=\mathbb E\left[\varphi(x+B_\tau)\right].$$ And we have $u\vert_{\partial D}=\varphi$ . This is a pretty well-known theorem. I'm trying to learn it right now.
I'm hitting a stumbling block with regards to the Poincaré cone condition. First, I don't understand its statement in certain forms. One such statement I don't understand is from a set of lecture notes from Cambridge (by the legendary Dexter Chua) is Definition. We say a domain $D$ satisfies the Poincaré cone condition if for any $x\in\partial D$ , there is an open cone $C$ based at $X$ [ sic ] such that $$C\cap D\cap B(x,\delta)=\varnothing$$ for some $\delta\geq0$ . What is an ""open"" cone? Is it just any cone that contains $x$ minus its boundary? Do we require that $x\notin D$ or $x\notin C$ , otherwise it trivially fails for all $x$ ? Where is this requirement coming from? I have confusions with other (seemingly isomorphic) definitions of the Poincaré cone condition but I won't belabor the point here since I'm sure solving my confusion with this one will solve my confusion with others. My next question is: What is the geometric intuition of this condition, in the context of the theorem above? I literally have no idea what this is giving me except a little bit of space in the proof of the earlier stated theorem. I'm not sure why this would be necessary for the proof, i.e. why is the proof necessarily impossible without this condition. Nor can I ""see"" what this property implies of my domain in the ""visually intuitive"" settings of 2- or 3-space.","['stochastic-processes', 'probability-theory', 'stochastic-calculus', 'partial-differential-equations']"
3337568,Second order ODE with variable coefficients.,"Consider the following ODE: \begin{equation}
(\cos x)y''-2(\sin x)y'-(\cos x)y=e^x
\end{equation} The above equation is a second order linear ODE. However, I noticed that it doesn't have constant coefficients, so I cannot ""guess"" the solution is of the form $e^{\lambda x}$ . Im very confused by this, since I have never solved a 2nd order ODE with variable coefficients. The first part of the question says: (a) Show that the ODE is of the form: \begin{equation}
\frac{d^2}{dx^2}(f(x)y)=e^x
\end{equation} by finding the function f. (b) Hence, find the general soluion of he differential equation. For part (a) I have have no idea where to begin, but I tink that If i knew how to complete part (a) I would be able to find the general solution since I hae done things like that many times. Any help (part a especially) would be much appreciated. Thanks in advance.",['ordinary-differential-equations']
3337570,The $k$th derivative of $\sin^n x$ as $n \ \sin^{n - k} x$ times a polynomial in $\cos x$,"It seems the $k$ th derivative of the function $x \mapsto \sin^n x$ can be expressed as $n \ \sin^{n - k} x$ times a polynomial with only even or only odd powers of $\cos x$ (depending on the parity of $k$ ), with polynomials in $n$ as coefficients, for natural $k$ and integer $n$ and complex $x$ . The highest term is $n^{k-1} \cos^k x$ ; the ranks of the other polynomial-in- $n$ coefficients for $\cos^{k-2j} x$ seem to be (maximally) $(k-1-j)$ . $$
\frac{\mathrm{d}^0}{\mathrm{d}x^0} \sin^n x = n \ \sin^{n - 0} x \ \left(n^{-1} \cos^0 x\right)\text{ for }n \neq 0
$$ $$
\frac{\mathrm{d}^1}{\mathrm{d}x^1} \sin^n x = n \ \sin^{n - 1} x \ \left(n^0 \cos x\right)
$$ $$
\frac{\mathrm{d}^2}{\mathrm{d}x^2} \sin^n x = n \ \sin^{n - 2} x \ \left(n^1 \cos^2 x - 1\right)
$$ $$
\frac{\mathrm{d}^3}{\mathrm{d}x^3} \sin^n x = n \ \sin^{n - 3} x \ \left(n^2 \cos^3 x + (-3 n + 2) \cos x\right)
$$ $$
\frac{\mathrm{d}^4}{\mathrm{d}x^4} \sin^n x = n \ \sin^{n - 4} x \ \left(n^3 \cos^4 x + (-6 n^2 + 8 n - 4) \cos^2 x + (3 n - 2)\right)
$$ $$
\frac{\mathrm{d}^5}{\mathrm{d}x^5} \sin^n x = n \ \sin^{n - 5} x
$$ $$
\left(n^4 \cos^5 x + (-10 n^3 + 20 n^2 - 20 n + 8) \cos^3 x + (15 n^2 - 30 n + 16) \cos x\right)
$$ $$
\frac{\mathrm{d}^6}{\mathrm{d}x^6} \sin^n x = n \ \sin^{n - 6} x
$$ $$\left(n^5 \cos^6 x + (-15 n^4 + 40 n^3 - 60 n^2 + 48 n - 16) \cos^4 x + (45 n^3 - 150 n^2 + 196 n - 88) \cos^2 x + (-15 n^2 + 30 n - 16)\right)
$$ Is there a general rule to determine the polynomial-in- $n$ coefficients? Edit: The OEIS sequence A133341 seems to determine the second polynomial-in- $n$ coefficients (when one alternates the signs, and discards the italic values): 1 , 2 , 1, 3 , 3, 2, 4 , 6, 8, 4, 5 , 10, 20, 20, 8, 6 , 15, 40, 60, 48, 16, 7 , 21, 70, 140, 168, 112, 32, 8 , 28, 112, 280, 448, 448, 256, 64, 9 , 36, 168, 504, 1008, 1344, 1152, 576, 128, 10 , 45, 240, 840, 2016, 3360, 3840, 2880, 1280, 256, …",['derivatives']
3337577,What kinds of functions preserve inequality?,"Given real numbers $a, b, c$ such that $$a < b < c$$ if $f$ is a function that satisfies $$f(a) < f(b) < f(c)$$ for all $a, b, c$ , then what can be said about $f$ ? I thought that such an $f$ would have to be monotonic . But I also thought that $f(x) = x^2$ is monotonic (edit: it's definitely not monotonic) , and it doesn't satisfy the above inequality for all $a, b, c$ (for example, $a = -3$ , $b = -2$ , and $c = -1$ ). Could someone list all of the general requirements for a function that preserves inequalities?",['algebra-precalculus']
3337599,Sylvester's Criterion for tensors,"We know from linear algebra that if $A = (a_{ij})_{i,j=1}^n$ is a symmetric matrix, then $A$ is positive-definite if and only if all principal minors of $A$ are positive. If I say that a symmetric (covariant) $k$ -tensor $(T_{i_1\cdots i_k})_{i_1,\ldots, i_k=1}^n$ is positive-definite if for any non-zero vector $v = (v^1,\ldots, v^n)$ , we have that $$\sum T_{i_1\cdots i_k}v^{i_1}\cdots v^{i_k} > 0,$$ is there a Sylvester-like criterion for that? I imagine so, but it should be ugly. I know no references, though.","['tensors', 'linear-algebra', 'positive-definite']"
3337623,"If $\lim_{x\to 1} \frac{f(x)-6}{x-1} = 10$, then what is $\lim_{x\to 1} f(x)$?","I was given this question: If $$\lim_{x\to 1} \frac{f(x)-6}{x-1} = 10$$ then what is $$\lim_{x\to 1} f(x)$$ I am assuming that I will need to use limit laws in reverse, but this doesn't seem to work as the limit of $x-1$ as $x$ approaches $1$ is $0$ , which gives an undefined answer.",['limits']
3337634,"Approximating $\pi$ and $\ln 2$ with $I_k=\int_0^\infty \left(\text{sech}x\tanh\tfrac12x\right)^k\,dx$ for integer $k$","Consider the following integral: $$I_k=\int_0^\infty \left(\text{sech}x\tanh\tfrac12x\right)^k\,dx$$ where $k\in\Bbb N$ . If we evaluate $I_1$ , $I_2$ , $I_3$ , etc. we get the following pattern: $I_1=\log(2)$ $I_2=-3+\pi$ $I_3=\frac 72-5\log(2)$ $I_4=22-7\pi$ $I_5=-\frac{341}{12}+41\log(2)$ $I_6=-\frac{968}{5}+\frac{493}{8}\pi$ ... From this data we can see that: $$\begin{align}
\pi&=3+I_2=\frac{22}{7}-\frac17 I_4=\frac{7744}{2465}+\frac{8}{493}I_6 \\[4pt]
\log(2)&=0+I_1=\frac{7}{10}-\frac15I_3=\frac{341}{492}+I_5
\end{align}$$ And because $I_k$ decreases very rapidly( $I_{10}$ is in the order of $1e6$ ) we can set $I_k\approx 0$ for high $k$ and we get rational approximations of both, $\pi$ and $\log(2)$ , for $I_{2k}$ and $I_{2k+1}$ , respectively, that apparently go on forever. I see some equations that somehow ""encode"" the information of a given number, but how is it that this integral has the information of both $\pi$ and $\log(2)$ , apparently unrelated numbers? Thanks.","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
3337651,Derivative of generalized hypergeometric function,"Good evening Say we are working with a hypergeometric $3F3(a,b,c; d,e,f; z)$ function. I know that $\frac{d}{dz}3F3(a,b,c; d,e,f; z)=\frac{abc}{def}3F3(a+1,b+1,c+1; d+1,e+1,f+1; z)$ . There are numerous references for why this is so. For my specific parameters, $3F3(1,1,1; 2,2,2;z)$ Mathematica gives a different solution. The result I get is: $$ \frac{d}{dz}3F3(1,1,1; 2,2,2; z)=2F2(1,1; 2,2; z)-3F3(1,1,1; 2,2,2; z)$$ Can anyone explain why this would be the case, or provide a reference to this statement? I have been through Gradstein and Ryzhik and a few other books, but none of them give an identity that will result in a decrease in the number of hypergeometric parameters!","['derivatives', 'hypergeometric-function']"
3337671,Algebraic (?) proof that Ricci form is closed,"Let $(M,\omega, J, g)$ be a Kähler manifold. The Ricci form of $M$ is defined as $\rho(X,Y)={\rm Ric}(JX,Y)$ . I wanted to give a possibly coordinate-free proof that ${\rm d}\rho=0$ . From the condition $\nabla J=0$ we have that $${\rm d}\rho(X,Y,Z) = (\nabla_X{\rm Ric})(JY,Z) +(\nabla_Y{\rm Ric})(JZ,X)+(\nabla_Z{\rm Ric})(JX,Y).$$ I'm guessing that there is a smart way of using the second Bianchi identity to get the result from the above, but I can't see how to deal with terms of the form $\nabla_{JX}$ . Also the fact that the second Bianchi identity is true for all connections makes me think that it might not be powerful enough. Help?","['kahler-manifolds', 'complex-geometry', 'riemannian-geometry', 'differential-geometry']"
3337701,"Expectation of $\min(a \bmod p,2a\bmod p,....,ka \bmod p)$","Given $k,p \in \mathbb N$ , what's $$
\mathbb E(\min(a \bmod p,2a\bmod p,...,ka \bmod p))
$$ where $a$ is a random integer in $[0,p)$ ? According to the simulation, I found the answer is roughly $\dfrac{p}{2k}$ (or imprecisely speaking, $\Theta(p/k)$ ). Update: we can prove that it is $>p/2k$ when $p$ is prime. In fact I want to know the asymptotic expansion of $$
{k \to \infty }, {p\to \infty} \mathbb E(a)
$$ and it means $k<p$ but $k=\Theta(p)$ (Thanks to @mathworker21's comment) New discovery: when $k \ll p$ , the expectation can be asymptotically given by the integral: $$
p \times \int _{0}^ 1 \min(\{x\},\{2x\},...,\{kx\}) \mathrm dx
$$ but when $k = \Theta (p)$ , the error will be large (even when $p \to \infty $ )","['modular-arithmetic', 'number-theory', 'analytic-number-theory', 'expected-value', 'combinatorics']"
3337771,How to prove this statement for metric subspaces?,"How to prove the following statement about an intersection of two open sets in a metric subspace? If $(Y,d) \subset (X,d)$ are metric space and its subspace relatively, then $$\forall G_1, G_2 \subseteq (Y,d) : G_1 \cap G_2 = \varnothing$$ $$\exists H_1, H_2 \subseteq (X,d) : H_1 \supseteq G_1, H_2 \supseteq G_2, H_1 \cap H_2 = \varnothing$$ It is supposed that metric spaces topic in the course I read are introduced before any other topological terms, so I assume that proof must be simple. Thank you in advance!","['calculus', 'general-topology', 'metric-spaces']"
3337810,Limit and Integration are interchangeable,"Let $f_n:[0,1]\to\mathbb{R}$ be defined by $f_n(x)=\frac{n+x^3\cos x}{ne^x+x^5\sin x}$ , $n\geq 1$ . Find $\lim_{n\to\infty}\int_0^1f_n(x)dx$ . Approach : Here I found the limit function $f(x)=e^{-x}$ . So, if I can interchange the limit and integration, then easy to find solution. But we can do that only when the convergence to $f$ is uniform. I am not getting the idea to show this convergence is uniform.","['sequence-of-function', 'limits', 'real-analysis']"
3337821,properties of ordinal numbers,"An ordinal number is a set $\xi$ such that: $\zeta\in\xi\Rightarrow \zeta\subset\xi$ $\zeta,\eta\in\xi\Rightarrow \zeta=\eta$ or $\zeta\in\eta$ or $\eta\in\zeta$ $\emptyset\not=A\subset \xi\Rightarrow \exists\zeta\in A:\zeta\cap A=\emptyset$ I want to prove 1) If $\xi$ is an ordinal number then $\xi\cup\{\xi\}$ is an ordinal number. 2)If $A\not=\emptyset$ is a set of ordinal numbers then $\cup A=\{\zeta:\zeta\in\xi\text{ for some } \xi\in A\}$ is an ordinal number. I need some help with the third dot in both of them. Thanks in advance! EDIT:
1) is solved. Any ideas for 2) ?",['elementary-set-theory']
3337882,Topology of the link of a rational hypersurface singularity,"Let $V\subseteq \mathbb{C}^{n+1}$ be an affine algebraic hypersurface, i.e. a zero set of a polynomial in $n+1$ complex variables. Let $S$ be a small sphere around the origin $0\in \mathbb{C}^{n+1}$ and let $K= S\cap V$ be the link of $0\in V$ . Then $K$ is a compact manifold of (real) dimension $2n-1$ . A well-known result of Milnor asserts that $\pi_{i}(K)$ is trivial for $i\leq n-2$ . In particular, $H_{i}(K;\mathbb{Z})=0$ for $i\not\in \{0,n-1,n,2n-1\}$ . 
I would like to ask what is known about $H_{n-1}(K)$ ? Under what conditions is it true that $K$ is a $\mathbb{Q}$ -homology sphere? Does the topology of $K$ somehow reflect the fact that the singularity is canonical, terminal etc? I am mostly interested in the following question: Is it true that the link of a rational singularity is a $\mathbb{Q}$ -homology sphere? I am aware of the fact that algebraic geometers prefer to study the topology of the dual complex of the resolution instead of the topology of the link. Is it because the latter does not capture the algebraic properties of the singularity? I would be very grateful for any examples illustrating this issue.","['algebraic-geometry', 'homology-cohomology', 'singularity-theory']"
3337887,Uniqueness of minimal $\infty$-norm polynomial,"From this proof it is clear to me that Chebyshev polynomial $\frac{1}{2^{n-1}} T_n(x)$ is minimum $\infty$ -norm in $[-1,1]$ among the monic polynomials of degree $n$ . How to prove the uniqueness (if true) of such minimizing polynomial? I found this question , but the answer is not completely related. I have no idea to start.","['chebyshev-polynomials', 'measure-theory', 'polynomials']"
3337917,Evaluating the following integral: $\int_{0}^{\infty }\frac{x\cos(ax)}{e^{bx}-1}\ dx$,How can we compute this integral for all $\operatorname{Re}(a)>0$ and $\operatorname{Re}(b)>0$ ? $$\int_{0}^{\infty }\frac{x\cos(ax)}{e^{bx}-1}\ dx$$ Is there a way to compute it using methods from real analysis?,"['integration', 'calculus', 'improper-integrals']"
3337938,Can we rearrange primes to make this ratio converge to any real?,"Let $p_n$ be the $n$ -th prime and let $q_1, q_2, \ldots, q_n$ be any rearrangement of the first $n$ primes. Using the rearrangement inequality and the solution to this problem , we can prove that $$
1 \le \lim_{n \to \infty}\frac{p_1 + 2p_2 + 3p_3 + \cdots + np_n}{q_1 + 2q_2 + 3q_3 + \cdots + nq_n} \le 2
$$ I was wondering if we can make the above ratio arbitrarily close to any real number in the interval $(1,2)$ by choosing a suitable rearrangement of the first $n$ primes i.e. Question : Let $\alpha, 1 \le \alpha \le 2$ be any real. Does there always  exist a rearrangement such that, $$ \lim_{n \to \infty}\frac{p_1 + 2p_2 + 3p_3 + \cdots +
 np_n}{q_1 + 2q_2 + 3q_3 + \cdots + nq_n} = \alpha$$","['number-theory', 'real-analysis', 'limits', 'convergence-divergence', 'prime-numbers']"
3338003,"Why do we require differential forms to be smooth, rather than $C^2$?","I feel like this is probably a naive question with an obvious answer, but I haven't been able to think of one. So I'll ask away. In every treatment I've seen, differential forms are required to be smooth. $0$ -forms are smooth functions, $1$ -forms are smooth covector fields, etc. I'm puzzled by this requirement, because the theory of differential forms doesn't seem to require us taking derivatives of order $>2$ . Clearly the $C^2$ condition is necessary for the identity $d^2=0$ . But once we have this identity it seems to do away with all higher-order phenomena, and that's the end of the story. Despite this, every treatment I've ever seen requires smoothness. What am I missing?","['differential-forms', 'multivariable-calculus', 'vector-analysis', 'differential-geometry']"
3338046,Computing 95% confidence interval,"The question: The gas in bubbles within amber should be a sample of the atmosphere at the time the amber was formed. Measurements on specimens of amber 75 million years ago give these percents of nitrogen: 63.4, 65.0, 64.4, 63.3, 54.8, 64.5, 60.8, 49.1, 51.0. Construct a 95% confidence interval for the true average % nitrogen in the atmosphere at this time. My attempt: I used the following formula for confidence intervals from my textbook: ""A 100(1- $\alpha$ )% confidence interval for $\mu$ is $(\bar{X} - t_{n-1, 1-\frac{\alpha}{2}} \frac{S}{\sqrt{n}}, \bar{X} + t_{n-1, 1-\frac{\alpha}{2}} \frac{S}{\sqrt{n}})$ ."" I know $\bar{X}$ is 59.6, and $n$ is 9. But there are 2 things I'm having trouble with: Firstly, in the answers they say that $S$ is 6.25 when I am calculating it to be 5.89. Secondly, I'm having trouble finding $t_{n-1, 1-\frac{\alpha}{2}}$ . According to the solutions, it is 2.31. But I'm not sure where they got that number from, or how to find $t_{n-1, 1-\frac{\alpha}{2}}$ in general? I thought it was supposed to be 1.96 but that's incorrect. Any help is appreciated.","['statistics', 'confidence-interval']"
3338091,Rewriting Sigma Notation (Index),"We had our quiz in Pre-calculus earlier and we argued about the index in writing sigma notation from a given expression. $$1-2+3-4+5-6+\cdots -10$$ We were instructed to write the expression in sigma notation. I had my answer with $$
\sum_{n = 2}^{11}(-1)^n(n-1)
$$ I was told that my answer was incorrect. So I want to ask if is it necessary for the index to be equal to 1?","['algebra-precalculus', 'summation']"
3338153,Divergence of a sequence.,"In an example in the book, Thomas Calculus 14e: Q: Show that sequence $\{(-1)^{n+1}\}$ diverges? A: They choose $\varepsilon$ to be $1/2$ and thus, $|L-1|<1/2$ for $+1$ and $|L+1|<1/2$ if value converges to $+1$ , where $L$ is the value to which the sequence converges. On solving both equations, we get no common solution, thus no such limit $L$ exists and thus the sequence diverges. My question: If we choose $\varepsilon$ to be $3$ or any number greater than it, then the above inequalities give a solution, so does it mean the sequence converges?? Also later it states, a sequence can diverge without diverging to $\pm \infty$ , how is that relevant and possible?","['sequence-of-function', 'calculus', 'convergence-divergence', 'sequences-and-series']"
3338154,Fundamental question on test for normal distribution,"I have a fundamental question on statistical tests, particularly tests for normal distribution. As I understand statistical tests in general, they have a null hypothesis $H_0$ (e.g. the samples were drawn from a normal distribution) and an alternative hypothesis $H_1$ (e.g. the samples were not drawn from a normal distribution). If the test is significant ( $p < p_\alpha$ ) one can reject $H_0$ and assume that $H_1$ is true. However, if the test is not significant, one can not automatically assume that $H_0$ is true. Now, all tests for normal distribution that I read about have a $H_0$ that the samples were drawn from a normal distribution. Hence, the only thing you can do with these tests is to assume that the samples were not drawn from a normal distribution if the test is significant. You can't assume that the samples are drawn from a normal distribution if the test is not significant. But that's what everybody seems to be doing. Is there anything fundamentally wrong with my understanding of statistical tests? How can I ""prove"" that a given sample was drawn from a normal distribution?","['statistics', 'hypothesis-testing']"
3338155,What is the dimension of the span over $\mathbb{F}$ of permutation matrices on $n$ elements if $char(\mathbb{F})$ divides $n$?,"I'm trying to generalize Birkhoff's theorem and prove that over any field $\mathbb{F}$ any matrix whose sum of rows and columns is equal to some constant, is a linear combination of permutation matrices. For fields where the characteristic $p$ does not divide $n$ , we can use a dimensional argument: compare the dimension over $\mathbb{F}$ of the span of all permutation matrices on $n$ points with the algebra $Mat_{n-1}(\mathbb{F})\oplus \mathbb{F}$ . These algebras are isomorphic as it is sufficient for $p$ to not be a factor of $n$ so that it the representation decomposes into the direct sum of the trivial and the permutation representation. This does not work otherwise. Any hints?","['permutation-matrices', 'representation-theory', 'matrices', 'linear-algebra', 'combinatorics']"
3338156,Group action on pullback sheaf.,"I want to prove the following fact: If $G$ is a finite group scheme acting freely by $\mu$ on an abelian variety $X$ and $\pi \colon X \rightarrow X/G$ is the quotient map then for any coherent sheaf $\mathcal{F}$ on $X/G$ then there is a lift of the $G$ -action to the pullback $\pi^{*}\mathcal{F}$ . I have read Mumford's proof in his book Abelian Varieties but I am struggling to prove the details he omits. Let $\mathcal{G} := \pi^{*}\mathcal{F}$ . There are natural isomorphisms $\lambda_{1}:p_{X}^{*}\mathcal{G} \rightarrow (\pi \circ p_{X})^{*}\mathcal{F}$ and $\lambda_{2}:\mu^{*}\mathcal{G} \rightarrow (\pi \circ \mu)^{*}\mathcal{F}$ . Using the fact that $\pi \circ p_{X} = \pi \circ \mu$ because $\pi$ is $G$ -invariant, we can define an isomorphism $p_{X}^{*}\mathcal{G} \rightarrow \mu^{*}\mathcal{G}$ as $\lambda_{2}^{-1} \circ \lambda_{1}$ . Now Mumford says to use the functorial properties of pullbacks to prove that this is in fact a lift of the $G$ action. This amounts to showing that a diagram is commutative. I am struggling with this because we are not working up to isomorphism, we are working with commutative diagrams where all the morphisms are isomorphisms. It seems like an important example of descent so I thought I should try and work through the details; is this a bad idea? If not then is there a simple way of showing that this is in fact the lift of $\mu$ ?","['group-actions', 'algebraic-geometry', 'abelian-varieties', 'descent']"
3338183,"Rational $a$ and $b$ in $(0,\frac12)$ such that $\cos(a\pi)=\cos^2(b\pi)$","I’m interested in pairs of rational numbers $a, b$ in the interval $(0,\frac12)$ such that $$\cos(a\pi) = \cos^2(b\pi)$$ Certainly $a=\frac13$ , $b=\frac14$ is a solution. I suspect that this is the only solution – as a sanity check, I verified this numerically for denominators less than 200 – but I can’t currently see how to prove it. I have the feeling there’s a simple proof that I’m not quite seeing, maybe involving expressing the cosines in terms of roots of unity? They’re all algebraic numbers, of course.",['trigonometry']
3338212,Discriminant Analysis: the polynomial kernel of Support Vector Machine,"The following 【Quiz】 and 【Official Answer】are the rough translation (with minor modification) of Quiz No.03-1 of the exam of the ""2018's semi-first grade of Japan Statistical Society Certificate (JSSC)"" You can 
 find the original of this quiz from this URL (But written in Japanese). 【My Question】 How can we derive Equation 2 from Equation 1 ?  What is the relationship between $(1 + \textbf{x}^{T} \textbf{x}')^{4}$ and $(x-3)(x-1)(x+1)(x+3)$ . As a characteristic of data on Fig.1, I recognize the following. All Data near x = -4 are  positive cases All Data near x = -2 are  negative case All Data near x = 0 are  positive cases All Data near x = 2 are  negative case Therefore, I understand that, if there is a function such that positive near x = -4, negative near x = -2 , positive near x = 0, and negative near x = 2 , it can be correctly identified. 
And I can understand that the following Equation 2 satisfies the above requirement. However, I cannot imagine the relationship between $(1 + \textbf{x}^{T} \textbf{x}')^{4}$ and $(x-3)(x-1)(x+1)(x+3)$ . I feel, a formula like $ (1+(x,y)\binom{x'}{y'})^4$ will never be a formula like $(x-a)(x-b)(x-c)(x-d)$ 【Quiz】 Two types of data, positive-case (+1) and negative-case (-1), are shown in Fig.1. Fig.1 In order to determine whether each data in FIG.1 are a positive case or a negative case, Support Vector Machines using the polynomial kernel of Equation 1 is performed. Equation 1: $\ \ k(\textbf{x}, \textbf{x}') =  (1 + \textbf{x}^{T} \textbf{x}')^{p} $ Here, $\textbf{x},\textbf{x}' \in {R}^2$ , ${\textbf{x}}^{T} $ is the 
  transpose vector of ${\textbf{x}} $ , and p  shall be a positive integer. At this time, what is the smallest p that can correctly distinguish all data? Explain why. 【Official Answer】 By using a fourth-order polynomial kernel, the following function of Equation 2 is configured. Equation 2: $\ \ f(x) = sign\{ (x-3)(x-1)(x+1)(x+3)\} $ This function completely discriminates the data. On the other hand, in the polynomial kernel of the third-order or less, since the change of the sign are three times or less, the data in FIG.1 cannot be discriminated. P.S. I'm not very good at English, so I'm sorry if I have some impolite or unclear expressions.","['machine-learning', 'statistics', 'polynomials', 'discriminant']"
3338227,Directional derivatives and unit vector,"I have an exercise in my textbook saying the following: Let $f : R^2 → R$ be defined by $z(x, y) = x^3 − 2x^2y + xy^2 + 1$ . Find the directional derivative at $(1, 2)$ along the direction towards $(4, 6)$ . While I understand the process and the evaluation process, I don't understand my teachers approach to finding the unit vector. According to the solution it is this: We have $x = (1, 2), y = (3, 4)$ because the direction is given by the difference of the two points. Yet, using the normal approach, I would get $u=\frac{(4,6)}{\sqrt{4^2+6^2}} = (\frac{2}{\sqrt{13}},\frac{3}{\sqrt{13}})$ Meaning his solution is $5$ , while mine is $\frac{4\sqrt{13}}{13}$ Who is right and if he is, why is my approach wrong and how does his' work?","['derivatives', 'vectors']"
3338248,Elements and Subsets,"This is my first discrete math course so this questions might seem simple but I would like clarification. Say I have a set $A=\{1,2,3\}$ and a set $D=\{1,2,3\}$ . Is it true that set $A$ is both an element and a subset of set $D$ ? Now say $D=\{\{1,2,3\}\}$ . Is it true that set $A$ is no longer an element of $D$ because $D$ is a set which contains a set that contains $1,2,3$ ? A third case. Say I have set $D=\{x,16,\{1,2,3\}\}$ . Is set $A$ an element and proper subset of $D$ ?","['elementary-set-theory', 'discrete-mathematics']"
3338309,Is my interpretation of three dimensional improper integral correct?,"In Physics/Electrostatics textbook, I am in a situation where we have to find the electric field at a point inside the volume charge distribution. In Cartesian coordinates, we can't do it the usual way because of the integrand singularity. So we use the three dimensional improper integral. $$\mathbf{E}=\lim\limits_{\epsilon\to 0} \int_{V'-\delta_{\epsilon}}
\rho'\ \dfrac{\mathbf{r}-\mathbf{r'}}{|\mathbf{r}-\mathbf{r'}|^3} dV' \tag1$$ where: $\mathbf{r'}=(x',y',z')$ is coordinates of source points $\mathbf{r}=(x,y,z)$ is coordinates of field points $V'$ is the volume occupied by the charge $\delta_{\epsilon}$ is an arbitrary volume contained in $V'$ around the singular point $\mathbf{r}=\mathbf{r'}$ with $\epsilon$ being its greatest chord. $\rho'$ is the charge density and is continuous throughout the volume $V'-\delta_{\epsilon}$ While taking the limit the shape of $\delta_{\epsilon}$ is kept unaltered From equation $(1)$ , we can get the $x$ -component of $\mathbf{E}$ : $$E_x=\lim\limits_{\epsilon\to 0} \int_{V'-\delta_{\epsilon}}
\rho'\ \dfrac{x-x'}{|\mathbf{r}-\mathbf{r'}|^3} dV' \tag2$$ I view the steps of solving $E_x$ as follows: Make a $\delta_{\epsilon}$ cavity (with $\epsilon=a$ ) contained in $V'$ around the singular point $\mathbf{r}=\mathbf{r'}$ . Then take the Riemann integral over $V'-\delta_{\epsilon}$ Find the function which relates ""Riemann integral over $V'-\delta_{\epsilon}$ "" and "" $\epsilon$ "" over the interval $(0,a]$ . For the sake of clarity we can also make a graph of "" $\epsilon$ "" ( $x$ -axis) and ""Riemann integral over $V'-\delta_{\epsilon}$ "" ( $y$ -axis) over the interval $(0,a]$ Find $l$ $\ni$ $\forall \varepsilon > 0, \exists \delta \ni \text{when} |x-0|<\delta, |y-l|< \varepsilon$ Thus $l$ is the solution for $E_x$ in equation $(2)$ I think this is the correct interpretation of equation $(2)$ (as I confirmed with one of my teachers) I know steps $(1)$ and $(3)$ can be done. Please explain a way to execute step $(2)$ .","['improper-integrals', 'physics', 'multivariable-calculus', 'limits', 'riemann-integration']"
3338310,How to evaluate integral: $ \int_{0}^{1} \operatorname{sgn}(\sin(\ln(x))) \ dx $,"I need help with integral below.I tried simple substitution $\ln(x)=y$ but I don't know how to deal with fact that $y$ belong to $[-\infty,0]$ .I would be grateful for any hints. $$ \int_{0}^{1} \operatorname{sgn}(\sin(\ln(x))) \ dx =\int_{-\infty}^{0} e^{y}\operatorname{sgn}(\sin(y)) \ dy.$$","['integration', 'definite-integrals']"
3338320,Showing $P \land (P \lor Q) = P$ using only laws of propositions.,"How can I show $P \land (P \lor Q) = P$ using only laws of propositions? I see how $P \land (P \lor Q) = P$ simplifies to just $P$ using a truth table, but I can't see how to get it using propositional calculus. The distributive law just changes it to $P \lor (P \land Q)$ , leaving me essentially in the same position.","['logic', 'discrete-mathematics']"
3338348,Equivalence of definitions of tame ramification,"There are three different characterisations of a tamely ramified extension $L/K$ of a local field $K$ and I don't understand why they are equivalent. $\bullet$ $p \nmid e$ where $p = \rm{char}(k)$ , $k$ is the residue field of $K$ and $e$ is the ramification index. $\bullet$ $G_1 = \{1\}$ where $G_1$ is the ramification group of $L/K$ . $\bullet$ $L/K$ has conductor $1$ , where the conductor is the least integer $\frak f$ such that $1+\frak{m}^f \subseteq \rm{Ker}(\phi_{L/K})$ . $\frak m$ is the unique maximal ideal of $K$ and $\phi_{L/K}: K^\times \longrightarrow \rm{Gal}(L/K)$ is the local Artin map. The equivalence of the first two definitions has already been discussed on stackexchange but as far as I know, no proof has been provided. Hints are very welcome too. Thank you.","['algebraic-number-theory', 'number-theory', 'galois-theory', 'local-field', 'class-field-theory']"
3338369,Transforming differential equations into autonomous first-order systems,"Problem 1.7 in G.Teschl ODE and Dynamical Systems asks me to transform the following differential equation into autonomous first-order system: $\ddot x = t\sin(\dot x) +x$ Transforming the ODE to a system is in this case easy, but whats the usual technique to transform it to an AUTONOMOUS system? Thanks so much <3","['calculus', 'ordinary-differential-equations']"
3338385,Dimension via Sobolev spaces?,"For a non-empty compact subset $K\subset \mathbb{R}^d$ and $s\in\mathbb{R}$ denote $H^s_K$ the set of Sobolev-functions of order $s$ which are supported in $K$ . Define $$\delta(K)= \inf \{\epsilon\ge0: H^{-\epsilon}_K \neq 0 \} \in [ 0, d/2].$$ When $K$ has positive ( $d$ -dimensional) Lebesgue measure, then obviously $\delta(K)=0$ . Further, if $K$ is a subset of an $m$ -dimensional submanifold of $\mathbb{R}^d$ and has positive ( $m$ -dimensional) Lebesgue measure, then $\delta(K)=(d-m)/2$ . This suggests to view the quantity $$
\dim_S K := d - 2 \delta(K)\in [ 0, d]
$$ as a dimension of some sort. Question: Has this notion been studied anywhere, in particular is it known whether $\dim_S$ coincides with some other notion of dimension, e.g. the Hausdorff dimension, especially when taking non-integer values?","['measure-theory', 'distribution-theory', 'fractional-sobolev-spaces', 'sobolev-spaces', 'functional-analysis']"
3338415,How can you show by hand that $ e^{-1/e} < \ln(2) $?,"By chance I noticed that $e^{-1/e}$ and $\ln(2)$ both have decimal representation $0.69\!\ldots$ , and it got me wondering how one could possibly determine which was larger without using a calculator. For example, how might Euler have determined that these numbers are different?","['real-numbers', 'sequences-and-series', 'decimal-expansion', 'inequality', 'exponential-function']"
3338464,"The right derivative, $f'_{+}$, of a convex function $f$ is continuous $\iff$ $f$ is differentiable.","Let be $f$ a convex function defined on an open set. We know from theory that $f'_{+},f'_{-}$ both exist not decreasing. I want to prove or in case it is false to refute this: The right derivative, $f'_{+}$ , of a convex function $f$ is continuous $\iff$ $f$ is differentiable. I'm stuck with $[\Rightarrow]$ . But also for $[\Leftarrow]$ I don't have a formal proof. Any help,hint or solution would be appreciated. (on both arrows)","['analysis', 'real-analysis', 'calculus', 'derivatives', 'convex-analysis']"
3338466,"Showing $3^iq-2^{i-p}\neq2^pq-1$, with $p:=\lceil i\,\log_23\rceil$, $q:=\left\{\frac{2^{i}\,3^{2^{p-i-2}}}{2^p3^i}\right\}$, $i>6$","For this inequality $$3^i\cdot\left\{\frac{2^i\cdot3^{2^{\lceil i\cdot \log_23\rceil-i-2}}}{2^{\lceil i\cdot \log_23\rceil}3^i}\right\}-\frac{2^i}{2^{\lceil i\cdot \log_23\rceil}}\leqslant 2^{\lceil i\cdot \log_23\rceil}\cdot\left\{\frac{2^i\cdot3^{2^{\lceil i\cdot \log_23\rceil-i-2}}}{2^{\lceil i\cdot \log_23\rceil}3^i}\right\}-1$$ where $\{x\}$ is the fractional part of $x$ , and $i>6$ (to have integers on both sides), Show that, for any $i$ , this cannot be equal. Note that $3^i\{x\}<2^{\lceil i\cdot \log_23\rceil}\{x\}$ (the fractional part is the same on both sides). Also note that for the exponents in the fractional part when $i>6$ , we have $i<2^{\lceil i\cdot \log_23\rceil-i-2}$ and obviously $i<{\lceil i\cdot \log_23\rceil}$ EDIT (27/01/22) this can also be written this way (no limitation on $i$ ): $$\begin{array}{|c|}\hline (3^i\cdot(3^{-i}\mod 2^{\lceil i\cdot \log_2\frac{3}{2}\rceil})-1)\cdot 2^{-\lceil i\cdot \log_2\frac{3}{2}\rceil}\leqslant 2^i\cdot(3^{-i}\mod 2^{\lceil i\cdot \log_2\frac{3}{2}\rceil})-1\\\hline\end{array}$$ $Background$ Any positive odd integer can be written as $$n_0=a\cdot 2^i-1$$ It is well known that in a Collatz sequence, $n_0$ goes straight to $a\cdot 3^i-1$ in exactly $i$ steps of the combined Collatz function $T(n)=\frac{3n+1}{2}$ Now this number being even, it needs to be shaved of a few factor $2$ . I am looking at $n_j=\frac{a\cdot 3^i-1}{2^{\lceil i\cdot \log_2\frac{3}{2}\rceil}}$ , the first $n_j$ for which $\frac{3^i}{2^j}=\frac{3^i}{2^{\lceil i\cdot \log_23\rceil}}<1$ ( $n_j$ can be odd or even) Now in this particular scenario, it can be shown that $$n_j \le n_0$$ which is the simplified version of the equation on top, but I want to show that $$n_j < n_0$$ e.g. for $i=7$ : $n_0=3\cdot 2^7-1=383$ Goes up to $6560=3\cdot 3^7-1$ and is shaved down to $n_j=205$ $$\{383,575,863,1295,1943,2915,4373,6560,3280,1640,820,410,205\}$$ Reminder: $3^i=2^{i\cdot \log_23}$ EDIT:
Another way would be to show that $n_j$ or $n_0\neq\frac{3^i-2^i}{2^{\lceil i\cdot \log_23\rceil}-3^i}$ Here is a list of $n_0$ values for each $i>=1$ (they can be found using formula above for $i>6$ ): $$\{1,3,23,15,95,575,383,255,5631,...\}$$ and as said in a comment they are found every $2^{\lceil i\cdot \log_23\rceil}$ which means that numbers concerned by this post are: $$\{4k+1, 16k+3, 32k+23, 128k+15, 256k+95, 1024k+575, 4096k+383... \}$$ PARI/GP nj(i)=3^i*frac((2^i*3^(2^(ceil(i*log(3)/log(2))-i-2)))
      /(2^ceil(i*log(3)/log(2))*3^i))-2^i/2^ceil(i*log(3)/log(2))

  n0(i)=2^ceil(i*log(3)/log(2))*
         frac((2^i*3^(2^(ceil(i*log(3)/log(2))-i-2)))
          /(2^ceil(i*log(3)/log(2))*3^i))-1 Proposals to improve the PARI/GP-routines (thanks to Gottfried ) \\ define global constants instead of calling recomputations of costly functions
    beta=log(3)/log(2)    \\ of course with sufficient internal precision, I use 200 by default

    \\ then your functions become

   { nj(i)=3^i*frac((2^i*3^(2^(ceil(i*beta)-i-2)))
          /(2^ceil(i*beta)*3^i))-2^i/2^ceil(i*beta)   }
    
   { n0(i)=2^ceil(i*beta)*
             frac((2^i*3^(2^(ceil(i*beta)-i-2)))
              /(2^ceil(i*beta)*3^i))-1    } I think it is a good habit, to use (and reserve) one-letter variable like i,j,k always for indices of loops, vectors etc. I take $N$ for the (N)umber-of-odd-steps { nj(N)=3^N*frac((2^N*3^(2^(ceil(N*beta)-N-2)))
          /(2^ceil(N*beta)*3^N))-2^N/2^ceil(N*beta)   }
    
   { n0(N)=2^ceil(N*beta)*
             frac((2^N*3^(2^(ceil(N*beta)-N-2)))
              /(2^ceil(N*beta)*3^N))-1   } Now a new constant internal to your function which captures the repeated ""ceil()""-expression. I use ""S"" for it, the number of even steps, or (S)um of exponents of 2. This is depending on the argument of your function and
must be computed inside your function as a local constant: { nj(N)=my(S=ceil(N*beta));
          3^N*frac((2^N*3^(2^(S-N-2)))/(2^S*3^N))-2^N/2^S   }
    
    { n0(N)=my(S=ceil(N*beta));
          2^S*frac((2^N*3^(2^(S-N-2)))/(2^S*3^N))-1    } Next, another local constant with letter $B$, because I've to use often the difference $S-N=B$ and cancel $2^S/2^N = 2^B$ { nj(N)=my(S=ceil(N*beta),B=S-N);
          3^N    * frac(3^(2^(B-2)-N) /2^B )  -  1/2^B  }
    
    { n0(N)=my(S=ceil(N*beta),B=S-N);
          2^S    * frac(3^(2^(B-2)-N) /2^B )   - 1  } Finally two more improvements are possible. One is the avoiding of intermediate gigantic numbers like $3^{2^B}$ by putting cancellations into the exponents, and second, the common expression of the frac() into an own function - to make sure the reader sees immediately, that it is the same term in both functions: beta = log(3)/log(2)
    fr(N,B)=  frac(2^(  beta*(2^(B-2)-N)  - B  ))

    nj(N)  =  my(S=ceil(N*beta),B=S-N);    3^N * fr(N,B) -  1/2^B
    
    n0(N)  =  my(S=ceil(N*beta),B=S-N);    2^S * fr(N,B) -  1 . EDIT As we discussed in a linked post, this can also be written this way $$(3^N(3^{-N} \% 2^B)-1)2^{-B}\leq 2^N(3^{-N} \% 2^B)-1$$","['collatz-conjecture', 'number-theory', 'inequality', 'elementary-number-theory']"
3338492,Can the zero set of an irreducible polynomial contain a non-empty Zariski open subset?,"Let $k$ be an algebraically closed field and $f \in k[x_1,...,x_n]$ be an irreducible polynomial. Is it possible that $Z(f)$ , the zero set of $f$ , contains a non-empty Zariski open subset of $\mathbb A^n(k)$ ? (By $\mathbb A^n(k)$ I  don't mean $\mathrm{Spec}(k[x_1,...,x_n])$ , rather only the closed points.) I know that it is impossible for $n=1$ . Also when $k=\mathbb C$ , I know it is impossible for all $n$ , in fact then the zero set can't even contain any non-empty Euclidean open set. But I'm not sure what happens in other cases.","['zariski-topology', 'algebraic-geometry', 'commutative-algebra']"
3338497,Solving differential equation with vector magnitude.,I am trying to solve a system of three coupled differential equations. I managed to simplify them using a matrix. $$  \newcommand{\myMatrix}[1]{\bm{\mathit{#1}}} \frac{d\vec{x}}{dt}=\pmb{A}\left| \vec{x} \right|\vec{x}-\vec{d} $$ Where $\pmb{A}$ is a constant $3\times3$ matrix and $\vec{d}$ is a constant vector. I know there are ways to solve this if it weren't for the vector magnitude. Does anybody have any idea how to solve it with the vector magnitude?,"['calculus', 'linear-algebra', 'ordinary-differential-equations']"
3338556,When every point of a topological dynamical system is recurrent.,"Definitions A topological dynamical system is a pair $(X, T)$ where $X$ is a compact metric space and let $T:X\to X$ is a continuous map. By the forward-orbit of a point $x$ in $X$ we mean the set $O_x=\{T^nx:\ n=1, 2, 3, ...\}$ . We say that a point $x$ in $X$ is generic if $\bar O_x$ (closure of $O_x$ ) is $X$ . We say that $x$ is recurrent if $$x\in \bigcap_{n\geq 1}\overline{\{T^ix:\ i\geq n\}}$$ (Thanks to @JohnB for correcting the definition of a recurrent point). We say that the dynamical system $(X, T)$ is minimal if the only non-empty $T$ -invariant closed subset of $X$ is $X$ . Question. It is easy to show that if $(X, T)$ is a minimal system then every point of $X$ is generic and hence every point is recurrent. Suppose $(X, T)$ is such that every point is recurrent. This is not enough to ensure minimality. For one can consider the closed disc $D^2$ with an irrational rotation. So I am wondering if the following is true: Let $(X, T)$ be a topological dynamical system such that every point of $X$ is recurrent and that there is at least one generic point. Does this imply that the system is minimal? What if one assumes $T$ is a homeomorphism and not just a continuous map?","['general-topology', 'dynamical-systems']"
3338560,Hypergeometric function at $z=1$,"There is a nice formula for the value of the hypergeometric function ${}_2 F_1(a,b,c,z)$ at $z=1$ when $\Re{(c)}>\Re(b+a)$ given for example at https://en.wikipedia.org/wiki/Hypergeometric_function Is there some formula for what happens when $\Re{(c)}\leq\Re(b+a)$ . Presumably, the  function diverges but is there a known asymptotic behavior as $z\rightarrow 1^-$ ?","['special-functions', 'ordinary-differential-equations', 'asymptotics', 'real-analysis', 'hypergeometric-function']"
3338636,Power series differentiable at endpoints?,"The usual theorem is something like this (please correct me if this is wrong): Let $R>0$ ( $R$ can be $+\infty$ ). Suppose $c_0+c_1x+c_2x^2+c_3x^3+\dots$ converges on $(-R,R)$ . Define $f:(-R,R)\rightarrow \mathbb R$ by $f(x)=c_0+c_1x+c_2x^2+c_3x^3+\dots$ . Then $f$ is differentiable on $(-R,R)$ , with $f'(x)=c_1+2c_2x+3c_3x^2+\dots$ . Here's my question: Now assume $R\neq+\infty$ . In the above theorem, replace each instance of $(-R,R)$ with (A) $(-R,R]$ ; (B) $[-R,R)$ ; or (C) $[-R,R]$ . to obtain new Theorems A, B, and C. Is each of these new Theorems true? If not, how can we strengthen the assumptions so that each new Theorem is true? (Added note: $R$ need not be the radius of convergence.)","['power-series', 'calculus', 'derivatives', 'real-analysis']"
3338654,What is a martingale measure? And in particular what is a $L^2(P)$ valued sigma-finite measure?,"My text book defines a Martingale measure in the following way: Let $(\Omega, \mathscr F, \{\mathscr F_t\}_{t \ge 0}, P)$ be a filtered probability space. A Processes $\{M_t(A)\}_{t \ge 0, \; A\in \mathscr B(\mathbb R^n) }$ is a martingale measure with respect to the filtration $\{\mathscr F_t\}_{t\ge 0}$ if: 1: $M_0(A)=0$ for any $A \in \mathscr B(\mathbb R^n)$ 2: If $t\ge 0 $ then $M_t$ is a sigma finite $L^2(P)$ valued signed measure 3: For all $A \in \mathscr B(\mathbb R^n)$ , $\{M_t(A)\}_{t \ge 0}$ is a $0$ -mean martingale with respect to the filtration $\mathscr F$ . I have a question about the meaning of (2). How should I think of this? For fixed $t \ge 0$ , is $M_t(A)$ the measure of $A$ for any $A \in \mathscr B( \mathbb R^n)$ ? Or is $E\big[\big(M_t(A)\big)^2\big]$ the measure of $A$ ? Also what woud it mean to be a sigma finite $L^2(P)$ valued measure? Does that mean that we can find a collection of countable sets, $\{B_n\}$ , such that these sets cover $\mathbb R^n$ and that $E\big[\big(M_t(B_n)\big)^2\big] < \infty $ ?","['stochastic-processes', 'martingales', 'probability-theory', 'probability']"
3338687,Nevanlinna - Herglotz - Pick - R functions : boundary values on the real axis,"Nevanlinna/Herglotz functions are analytic functions defined on the upper half complex plane and have non-negative imaginary part there. A remarkable theorem asserts that every such function has a unique integral representation of the form $$f(z) = a + bz + \int_{\mathbb{R}} \left( \frac{1}{\lambda-z} -\frac{\lambda}{\lambda^2+1} \right) d\mu(\lambda),$$ where $\mu$ is a positive Borel measure satisfying $\int (\lambda^2+1)^{-1} d\mu(\lambda) < \infty$ , $a \in \mathbb{R}$ and $b \geq 0$ . The measure $\mu$ is recovered from $f$ by the Stieltjes inversion formula $$\mu(( \lambda_1,\lambda_2]) = \lim \limits_{\delta \downarrow 0} \lim \limits_{\epsilon \downarrow 0} \frac{1}{\pi} \int_{\lambda_1+\delta} ^{\lambda_2+\delta} \text{Im} \ (f(\lambda+i\epsilon)) d\lambda.$$ I am interested in understanding the relationship between the support of the measure $\mu$ and the restriction of $f$ to the real line. To this end, I define the Domain of $f(x)$ to be $$\text{Dom}(f) := \{ x \in \mathbb{R} : f(x) \ \text{is defined there and} \ f(x) \ \text{is a finite real number} \}.$$ My question to you : Is it true in general that for all Nevanlinna functions, $\text{Dom} (f) \cup \text{supp} \ \mu = \mathbb{R}$ and $\text{Dom} (f) \cap \text{supp} \ \mu = \emptyset$ ? Intuitively, from the Stieltjes formula $ \text{Dom} (f) \cap \text{supp} \ \mu = \emptyset$ makes sense...but I'd like to hear from the experts. Thanks ! ---------------------------------------------------------------------- Some examples of Nevanlinna functions : 1) $f(z) = z$ .
Then $\text{Dom}(f) = \mathbb{R}$ , $\mu = 0$ and $\text{supp} \ \mu = \emptyset$ . 2) $f(z) = -z^{-1}$ . Then $\text{Dom}(f) = \mathbb{R}\setminus \{0\}$ , $\mu = 1_{\{0\}}$ and $\text{supp} \ \mu = \{0\}$ . 3) $f(z) = \log(z)$ . Then $\text{Dom}(f) = (0,+\infty)$ , assuming the standard branch cut, $\mu = 1_{(-\infty,0]}$ and $\text{supp} \ \mu = (-\infty,0]$ . 4) $f(z) = \tan(z)$ . Then, denoting $\Sigma := \{ (n+1/2)\pi : n \in \mathbb{Z}\}$ , we have $\text{Dom}(f) = \mathbb{R} \setminus \Sigma $ , $\mu = 1_{\Sigma}$ and $\text{supp} \ \mu = \Sigma$ .","['complex-analysis', 'functions', 'analytic-continuation']"
3338738,Minimum and maximum sum of squares given constraints,"Say that we know that $$\sum_{i=1}^n x_i = x_1+x_2+...+x_n = 1$$ for some positive integer $n$ , with $x_1 \le x_2 \le x_3 \le ... \le x_n$ . The values of $x_1$ and $x_n$ are also known. How can the minimum and maximum values of $$\sum_{i=1}^n x_i^2$$ be found? My attempt: I found the minimum value by setting all the $x_i$ other than $x_1$ and $x_n$ equal to each other. This means that $(n-2)x_i + x_1 + x_n = 1 \rightarrow x_i = \frac{1-x_1-x_n}{n-2}$ . Therefore, $$\sum_{i=1}^n x_i^2 = \frac{(1-x_1-x_n)^2}{n-2}+x_1^2+x_n^2$$ However, I do not know how to find the maximum. The hard part is that $x_1 \le x_i \le x_n$ must be satisfied.","['maxima-minima', 'multivariable-calculus', 'cauchy-schwarz-inequality', 'optimization', 'karamata-inequality']"
3338751,Are there any functions that are differentiable but not continuously-differentiable?,"Let $U$ be an open set on ${\mathbb R}^{n}$ (but $U$ is not an empty set), $\textbf{p}\in{U}$ , and $f:U\to \mathbb R$ is continuously-differentiable on $U$ . Then, it is known that, ""the function $f$ can be differentiable for all $\textbf{q}\in U$ ."" (See Spivac ) And I know that, there is a function $f$ such that it is differentiable at $\textbf{p}$ but, for any $r> 0$ , $f$ is not differentiable  (and continuously differentiable) on $U_{\textbf{p}} (r)$ .
Here, $U_{\textbf{p}} (r)$ is an open ball of radius $r$ centered on $\textbf{p}$ . For example, if $f:{\mathbb R}^{2}\to \mathbb R$ is defined as follows, $f$ is differentiable at $\textbf{0}$ ,, but is not differentiable (and not continuous) at any other point. Here $\mathbb Q$ is the set of all rational numbers, and $U_{\textbf{p}} (r)$ is an open ball of radius $r$ centered on $\textbf{p}$ . $f(x,y):=\left\{
\begin{array}{rr}
0, &  (x,y)\in \mathbb Q^{2} \\
x^2 + y^2, &  (x,y)\notin \mathbb Q^{2} \\
\end{array}
\right.$ Therefore, there is at least one function that does not have a continuously-differentiable region, even if it can be differentiable at one point. 
But I cannot imagine whether are there any functions that are differentiable on $U$ but not continuously-differentiable . My question Let $U$ be an open set of $\mathbb R^n$ (but is not an empty set), and $\  \textbf{p}\in U $ . Then, are there any functions $f:U\to \mathbb R$ such that, $f$ is differentiable on $U$ , but for any $r> 0$ , $f$ is not continuously-differentiable on $U_{\textbf{p}} (r)$ ? If so, give an example. If not, please explain why. Here, $U_{\textbf{p}} (r)$ is an open ball of radius $r$ centered on $\textbf{p}$ . Here, the definitions of differentiable and continuously differentiable are as follows. Def1 (Differentiable at $\textbf{p}$ ) Let $U$ be an open set (but not empty set) of ${\mathbb R}^{n}$ , $\textbf {p} \in \mathbb R^n$ , and $f$ is a function whose domain is $U$ .
  At this time, $f$ is differentiable at $\textbf{p}$ iff the following is satisfied. ${\exists} A:{\mathbb R}^{n}\to \mathbb R$ : a linear map such that $${\lim}_{\textbf{x}\to\textbf{p}}\frac{|f(\textbf{x}) - A(\textbf{x}-\textbf{p}) - f(\textbf{p})|}{|\textbf{x}-\textbf{p}|} = 0$$ Def2 (Differentiable on $\textbf{U}$ ) Let $U$ be an open set (but not empty sets) of ${\mathbb R}^{n}$ , and $f$ is a function which domain is $U$ . At this time, $f$ is differentiable at $U$ iff ""for all $\textbf{q}\in{\mathbb R}^{n}$ , $f$ is differentiable at $\textbf{q}$ "". Def3 (Continuously-differentiable on $U$ ) Let $U$ be an open set (but is not empty sets) of ${\mathbb R}^{n}$ , and $f$ is a function which domain is $U$ .
  At this time, $f:U\to \mathbb R$ is continuously-differentiable on $U$ iff $f$ is  partially differentiable for all direction, ${x}_{1},
   {x}_{2}, ..., {x}_{n}$ (that mean, we can define $\frac{\partial f}{\partial{x}_{1}}, \cdots\frac{\partial f} {\partial{x}_{n}} $ on $U$ ).  and, $\frac{\partial f} {\partial{x}_{1}}, \cdots\frac{\partial f}
   {\partial{x}_{n}} $ are continuous on $U$ . P.S. I'm not very good at English, so I'm sorry if I have some impolite or unclear expressions. Post-hoc Note: 【Verification of the function taught by Thomas Shelby】 The following are the confirmation that the following function $f$ meets my requirement (Is it correct as proof?): $f(x,y)=\begin{cases}(x^2+y^2)\sin\left(\frac1 {\sqrt{x^2+y^2}}\right),&(x,y)\neq 0\\0,&(x,y)=0\end{cases}.$ My proof: ${\lim}_{\|\textbf{x}\|\to 0} \frac{f(\textbf{x}) - f(\textbf{0})}{\|\textbf{x}\|}=
{\lim}_{\|\textbf{x}\|\to 0} \frac{{\|\textbf{x}\|}^{2}\sin(1/ \|\textbf{x}\| - 0)}{\|\textbf{x}\|}=
$ ${\lim}_{\|\textbf{x}\|\to 0} \|\textbf{x}\|\sin(1/ \|\textbf{x}\|)
= 0$ Therefore, the $f$ is differntiable at $(0,0)$ and $Jf(0,0)=(0,0)$ . On the other hand, for $\textbf{x}\neq\textbf{0}$ , Let $g$ and $h$ be $g(x,y):=\sqrt{{x}^2 + {y}^2}\ $ and $\ h(t):={t}^{2}\sin(1/t)$ (for $t\neq 0$ ) respectively, then $$\frac{d\sqrt{t}}{dt} = \frac{1}{2\sqrt{t}} $$ and, $$(J\|\textbf{x}\|^2)(x,y) = (2x,2y) ,$$ Therefore, $$(Jg)(x,y) = \left(\frac{x}{\|\textbf{x}\|} , \frac{y}{\|\textbf{x}\|}\right)\quad (\textrm{for all $\textbf{x}\neq\textbf{0}\ $}),$$ and $$\ \frac{d\sin(1/t)}{dt} = -\frac{\cos(1/t)}{t^2}\ \ \ (\textrm{at $t\neq 0$}).$$ Therefore, $$\frac{dh}{dt} ={t}^{2}\frac{d\sin(1/t)}{dt} + 2t\sin(1/t) = -\cos(1/t) + 2t\sin(1/t).$$ Therefore, at $\textbf{x}\neq \textbf{0}$ , $$Jf(x,y) = \left(\left.\frac{dh}{dt}\right|_{t=||\textbf{x}||}\right)(Jg)(x,y) = (-\cos(1/||\textbf{x}||) + 2t\sin(1/||\textbf{x}||))\left(\frac{x}{||\textbf{x}||} , \frac{y}{||\textbf{x}||}\right).$$ Therefore, $$\frac{\partial f}{\partial x} =  -\frac{x\cos(1/||\textbf{x}||)}{||\textbf{x}||} + 2x\sin(1/||\textbf{x}||),\,\textbf{x}\neq\textbf{0}$$ and $$\frac{\partial f}{\partial y} =  -\frac{y\cos(1/||\textbf{x}||)}{||\textbf{x}||} + 2y\sin(1/||\textbf{x}||),\,\textbf{x}\neq\textbf{0}.$$ However, both $\dfrac{x\cos(1/||\textbf{x}||)}{||\textbf{x}||} $ and $\dfrac{y\cos(1/||\textbf{x}||)}{||\textbf{x}||} $ do not get converted at $(0,0)$ . So, both $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ are not continuous at $(0,0)$ . ■","['calculus', 'derivatives', 'real-analysis']"
3338811,Limit of Lebesgue integrals,"Let $a>0$ and $f,g:[0,+\infty) \to \Bbb{R}$ where $f$ is a Lebesgue integrable function and $g$ has the property: $$|\frac{g(t)}{t}| \leq a, \forall t \geq 1$$ .
  Prove that $\lim_{t \to +\infty}\frac{1}{t}\int_1^tf(x)g(x)dx = 0$ Here is my proof: $$|\frac{1}{t}\int_1^tf(x)g(x)dx| \leq \frac{1}{t}\int_1^t|f(x)||g(x)|dx$$ $$=\frac{1}{t}\int_1^{\sqrt{t}}|f(x)||g(x)|dx+\frac{1}{t}\int_{\sqrt{t}}^t|f(x)||g(x)|dx$$ $$\leq \frac{a||f||_1}{\sqrt{t}}+a\int_{\sqrt{t}}^t|f(x)|dx \to 0$$ as $t \to +\infty$ because $\int_{\sqrt{t}}^t|f(x)|dx \to 0$ from integrability of $f$ . Is this proof correct,or i am missing something? Thank you in advance.","['measure-theory', 'proof-verification', 'lebesgue-integral', 'real-analysis']"
3338875,Strong and Weak Continuity in $L^p$ Spaces,"Set-Up Let $\Omega \subseteq \mathbb{R}^n$ be a domain, $T>0$ . We find $u_m(x,t) : \Omega \times [0,T] \rightarrow \mathbb{R}^n$ which converge weakly to some $u \in L^{\infty}(0,T; L_{\sigma}^2)$ . Here, $L_{\sigma}^2 = \{ f \in L(\Omega)^n \ | \ \text{div}f = 0 \}$ We know that $||u_m||$ is uniform bounded, and $u_m(\cdot , t) := u(t)$ is continuous with respect to $t$ for all $m$ . We also have that $u$ is weakly continuous in $L_{\sigma}^2$ with respect to $t$ . That is, $(u(t) , f) : \mathbb{R} \rightarrow \mathbb{R}$ is continuous, for all $f \in L^{2}_{\sigma}$ , where $(\cdot , \cdot)$ is the inner-product on $L^2$ . (See this previous question for a full explanation). Claim Our claim is that the inner-product $(u(t),u_m(t))$ is also continuous with respect to $t$ , for all $m \in \mathbb{N}$ . That is, the integral $\int_{\Omega} u(t)u_m(t) \text{d}x$ is continuous with respect to $t$ . I believe that the above claim should hold, by the weak continuity of $u(t)$ , and strong continuity of $u_m(t)$ , but I cannot find an obvious way to show this... Does anyone know even how I might start? Thank you.","['integration', 'inner-products', 'real-analysis', 'lp-spaces', 'functional-analysis']"
3338906,Finding polynomial functions that satisfy a given condition,"Find all polynomial functions $P(x)$ which satisfy $P(x)^2=P(P(x))$ . Attempt at reaching answer: Let $d$ be the degree of $p(x) $ . We have $d+d=d*d \implies d=0 \vee d=2$ . If $d=0$ we have that $p(x)$ is constant, and this is indeed a solution. Otherwise, we have $d=2 \implies p(x)=ax^2+bx+c$ . Plugging this into the original equation, we get $(ax^2+bx+c)^2=a(ax^2+bx+c)^2+b(ax^2+bx+c)+c$ . Checking the coefficient of $x^4$ gives trivially $a=1$ . Rewriting and simplifying gives $bx^2+b^2x+bc+c=0 \forall x \implies b=0 $ , $ c=0$ . Hence our solutions are $p(x) = x $ and $p(x)= c$ , with $c \in \mathbb{R} $ . Apparently, the first function (p(x)=x) is correct, but the second one isn't because it's not a polynomial function and it doesn't work for $x^2+1$ ... What would the correct answer then be?","['functions', 'polynomials']"
3338910,Formula for calculating the odds per user of winning in a raffle each player can win once,"Am trying to write a program which gives each user in a raffle contest their odds so far at winning. The rules of the game are simple: A predefined number of tickets are sold for example 1000 each user can at most buy 50 tickets and a user can only win once in the game after that all his tickets become invalid or pulled out and a new draw begins giving the other users a better chance of winning. The game will have 10 winners only. With the data collected from the software I would then know how many tickets were sold, the users that bought them and the amount of tickets that each user has. Now I just need a formula or pseudocode that would give each user their statistic probability of winning based on the data acquired, so that it be can used before and after each draw in the game to show each user their odds so far. I have looked at similar questions asked here, but no one seems to want to address the part that if a person wins the rest of their tickets become invalid. Am not good with probability or understand those fancy notations , so I don't understand is such a thing possible to calculate per user. Thanks for the help Update Testing my understanding of joriki second method:
lets say 10 tickets were sold to 4 users each bought A: 1, B: 2, C: 4, D: 3 
and there will be 3 prizes given to users.
I calculated the total probability of being drawn for each user to be A = $\frac{1}{10} + \frac{2}{10}*\frac{1}{8}*\frac{1}{6} + \frac{4}{10}*\frac{1}{6}*\frac{1}{2} + \frac{3}{10}*\frac{1}{7}*\frac{1}{4}$ = 0.1482 B = $\frac{1}{10}*\frac{2}{9}*\frac{2}{8} + \frac{2}{10} + \frac{4}{10}*\frac{2}{6}*\frac{2}{2} + \frac{3}{10}*\frac{1}{7}*\frac{1}{4}$ = 0.3817 C= $\frac{1}{10}*\frac{4}{9}*\frac{4}{8} + \frac{2}{10}*\frac{4}{8}*\frac{4}{6} + \frac{4}{10} + \frac{3}{10}*\frac{4}{7}*\frac{4}{4}$ = 0.6603 D= $\frac{1}{10}*\frac{3}{9}*\frac{3}{8} + \frac{2}{10}*\frac{3}{8}*\frac{3}{6} + \frac{4}{10}*\frac{3}{6}*\frac{3}{2} + \frac{3}{10}$ = 0.6500 Their total sum is 1.8403 and not 3 ? also is this considered the total probability of being drawn for the 3 draws or just for the first round of the game with the tickets becoming invalid","['statistics', 'probability']"
3338913,Can $a^4+1$ be a Carmichael number?,"Prove or disprove : There is no positive integer $\ a\ $ , such that $\ a^4+1\ $ is a Carmichael number. Since $\ 3\ $ is not a weak Fermat-psedudoprime of $\ a^4+1\ $ upto at least $\ a=5\cdot 10^7\ $ (which I tested with pari/gp using the strict primality test, for ""ispseudoprime"" , which is a very reliable test, I arrived at $\ a=3\cdot 10^8\ $ ) , a Carmichael number is not possible until this limit. I also tried to use Korselt's criterion which in this case menas that every prime factor $\ q\ $ of $\ a^4+1\ $ must satisfy $\ q-1\mid a^4\ $ , but this lead to nowhere.","['number-theory', 'carmichael-numbers', 'elementary-number-theory']"
3338937,Proof that $(1+\frac{1}{n})^n$ can't converge to a rational number,"One of my colleagues challenged me (and his students) with the following: ""Assume you don't know that $\lim_{n\to +\infty}(1+\frac{1}{n})^n=e$ . Prove the sequence $u_n=(1+\frac{1}{n})^n$ converges to a real non rational value."" I know how to do part of this. Actually, the proof of $u_n$ convergence is quite familiar (using the monotonic and bounded real sequences theorem). However, I can't prove the non rationality of the limit. Considering that $\lim_{n\to +\infty}(1+\frac{1}{n})^n=\ell$ , I tried to assume that $\ell$ can be written by $\frac{p}{q}, p,q\in \mathbb N$ with $p$ and $q$ being co-prime, but I reached a point I can't proceed. Here's my calculations: Let $u_n:\mathbb N\to \mathbb R, n\mapsto (1+\frac{1}{n})^n$ . STEP 1: Prove $u_n$ is monotonic If $u_n=(1+\frac{1}{n})^n$ then $u_{n+1}=(1+\frac{1}{n-1})^{n+1}$ . So $\frac{u_{n+1}}{u_n}=\frac{\left(1+\frac{1}{n+1}\right)^{n+1}}{\left(1+\frac{1}{n}\right)^n}=\left(1-\frac{1}{(n+1)^2}\right)^n\left(1+\frac{1}{n+1}\right)$ . By Bernoulli's inequality, as $-\frac{1}{(n+1)^2}>-1$ , $\left(1-\frac{1}{(n+1)^2}\right)^n\geq 1-\frac{n}{(n+1)^2}$ . So $\left(1-\frac{1}{(n+1)^2}\right)^n\left(1+\frac{1}{n+1}\right)\geq \left(1-\frac{n}{(n+1)^2}\right) \left(1+\frac{1}{n+1}\right)=1+\frac{1}{(n+1)^3}$ . As $1+\frac{1}{(n+1)^3}>1$ , $\frac{u_{n+1}}{u_n}>1$ , so $u_n$ is monotonically increasing. QED STEP 2: Prove $u_n$ is bounded As $u_n$ is monotonically increasing, by Step 1, $u_1=2$ is a lower bound for the set of $u_n$ terms. On the other side, as $n(n-1)...(n-k+1)\leq n^3$ and $\frac{1}{k\,!}\leq \frac{1}{2^{k-1}}$ , for all $k\in \mathbb N$ , then as $u_n = \sum_{k=0}^n \frac{\binom{n}{k}}{n^k}=1+\sum_{k=1}^n \frac{n(n-1)...(n-k+1)}{k\,!\,n^k}$ , $u_n\leq 1+\sum_{k=1}^n \frac{1}{2^{k-1}}=3-(\frac{1}{2})^{n-1}<3$ . So $3$ is an upper bound for the set of $u_n$ terms. Once bounded from below and bounded from above, the sequence $u_n$ is bounded. QED STEP 3: Prove $u_n$ is convergent Once monotonic (Step 1) and bounded (Step 2), by the monotonic and bounded real sequences theorem, the sequence $u_n$ is convergent, i.e. $\exists\,\ell\in\mathbb R:\lim_{n\to +\infty}(1+\frac{1}{n})^n=\ell$ .
By Step 1 and Step 2, we also conclude that $2<\ell<3$ . QED STEP 4: Prove $\ell$ can't be rational As we saw in Step 2, $u_n =\sum_{k=0}^n \frac{n(n-1)...(n-k+1)}{k\,!\,n^k}$ . Then $\ell=\lim_{n\to +\infty}\sum_{k=0}^n \frac{n(n-1)...(n-k+1)}{k\,!\,n^k}$ . As $\lim_{n\to +\infty}\sum_{k=0}^n \frac{n(n-1)...(n-k+1)}{n^k}=1$ , then $\ell=\sum_{n=0}^\infty \frac{1}{n\,!}$ (*). Let's now suppose $\sum_{n=0}^\infty \frac{1}{n\,!}$ can be written by $\frac{p}{q}, p,q\in \mathbb N$ with $p$ and $q$ being co-prime... (and I can't proceed from here). (*) Please note that I obviously know that $e=\sum_{n=0}^\infty \frac{1}{n\,!}$ . However, the challenge asks to assume we don't know that $\lim_{n\to +\infty}(1+\frac{1}{n})^n=e$ . Also, I just wrote all the steps, because it could be useful to complete the proof. I did some research and one suggestion I saw somewhere is to assume, for some $x\in\mathbb R$ , let $x=q\,!\left(\frac{p}{q}-\sum_{n=0}^q \frac{1}{n\,!}\right)$ . Then $x=p(q-1)\,!-\sum_{n=0}^q q(q-1)...(q-n+1)$ . What's the next step? Is this the right approach? Thanks","['convergence-divergence', 'rational-numbers', 'sequences-and-series']"
3338952,How to evaluate integral: $ \int_{0}^{\infty} e^{-x}\left|\sin{x}\right| \ dx $,I try to evaluate integral below.I solved indefinite integral but after evaluating limit I get wrong result.I don't know where can be problem.Maybe I just use the wrong method? $$ \int_{0}^{\infty} e^{-x}\left|\sin{x}\right| \ dx= $$ $$=  \left[ -\frac{1}{2}e^{-x}\operatorname{sgn}\left(\sin{x}\right)\left(\sin{x}+\cos{x}\right)\right]_0^\infty $$,"['integration', 'improper-integrals', 'definite-integrals']"
3338957,A prime number is not a quadratic residue modulo some prime without quadratic reciprocity,"In Cox's book ""Primes of form $x^2 + ny^2$ "", I stumbled upon a lemma $
\newcommand{\Z}{\mathbb{Z}}
$ Lemma 1.14: If $D \equiv 0,1 \pmod{4}$ is a nonzero integer, then there is a unique homomorphism $\chi:(\Z/D\Z)^* \longrightarrow \{\pm 1\}$ such that $\chi([p]) = (D/p)$ for odd primes $p$ not dividing $D$ . Furthermore, $\chi([-1]) = \operatorname{sign}(D)$ . One can prove this using quadratic reciprocity. But later on in one of the exercises, Cox suggests to prove quadratic reciprocity using this lemma - Problem 1.13 - we assume Lemma 1.14 holds for all nonzero $D\equiv 0,1 \mod4 $ and using this assumption we prove quadratic reciprocity. He gives a hint, for two primes $p,q$ , use $D=q^*=q(-1)^\frac{q-1}{2}$ . Then $\chi = (q^*/\cdot)$ is one homomorphism, and $(\cdot/q)$ is another homomorphism from $(\Z/q\Z)^*$ to $\{\pm 1\}$ . Since $(\Z/q\Z)^*$ is cyclic, there are only two homomorphisms from $(\Z/q\Z)^*$ to $\{\pm 1\}$ . One of them is the trivial homomorphism, and the other one is the Legendre symbol, which is non-trivial. If they were equal then $\chi([p]) = (q^*/p)=(p/q)$ which proves quadratic reciprocity. The only thing left to finish the proof would be to show that $\chi$ is not trivial. One way to do it is by showing that $\pm q$ is not a square modulo at least one prime coprime to $q$ , but the only way I know to do that is either by using quadratic reciprocity or with an overkill using Chebotaryev. Is there a simpler method to prove For every odd prime $q$ there exists an odd prime $p$ such that $(q^*/p) = -1$ . or just The unique homomorphism $\chi:(\Z/D\Z)^* \longrightarrow \{\pm 1\}$ that satisfies $\chi([p]) = (D/p)$ is not trivial when $D = q^*$ .","['group-homomorphism', 'legendre-symbol', 'number-theory', 'quadratic-residues', 'quadratic-reciprocity']"
3338965,Matrix exponential bound,"I am looking to find analogues for products of matrices of the scalar inequalities $$|1+x|\leq e^x,\qquad \big|\frac{1}{1+x}\big|\leq e^{-x+x^2},$$ which hold for $|x|\leq 1/2$ . Take $n,d\geq 1$ , $A_1,\ldots,A_n\in\mathbb{R}^{d\times d}$ such that $\|A_i\|\leq 1/2$ , where $\|\cdot\|$ stands for the operator norm.
Are the following inequalities true? $$
\big\|\prod_{i=1}^n(I+A_i)\big\|\leq \big\|\exp\big(\sum_{i=1}^nA_i\big)\big\|
$$ $$
\big\|\prod_{i=1}^n(I+A_i)^{-1}\big\|\leq\big\|\exp\big(\sum_{i=1}^n-A_i+A_i^2\big)\big\|
$$","['matrices', 'matrix-exponential', 'linear-algebra', 'upper-lower-bounds']"
3339010,"Understanding ""perimeter"" invariant","During 6.042, the students are sitting in an $n$ × $n$ grid. A sudden outbreak of beaver flu
(a rare variant of bird flu that lasts forever; symptoms include yearning for problem sets
and craving for ice cream study sessions) causes some students to get infected. Here is an
example where $n = 6$ and infected students are marked ×. Example. Now the infection begins to spread every minute (in discrete time-steps). Two students are
considered adjacent if they share an edge (i.e., front, back, left or right, but NOT diagonal);
thus, each student is adjacent to $2, 3,$ or $4$ others. A student is infected in the next time step
if either: the student was previously infected (since beaver flu lasts forever), or the student is adjacent to at least two already-infected students. In the example, the infection spreads as shown below. Example. In this example, over the next few time-steps, all the students in class become infected. Theorem. If fewer than $n$ students in class are initially infected, the whole class will never
be completely infected. Prove this theorem. Hint: When one wants to understand how a system such as the above “evolves” over time,
it is usually a good strategy to (1) identify an appropriate property of the system at the
initial stage, and (2) prove, by induction on the number of time-steps, that the property is
preserved at every time-step. So look for a property (of the set of infected students) that
remains invariant as time proceeds. Source: MIT OCW Mathematics for Computer Science, Problem Set 2, Problem 3. Unfortunately, I could not solve this problem and had to look up the solution. But even after looking at the solution, the whole thing doesn't make complete sense to me. I think I might be misunderstanding what perimeter means (Define the perimeter of an infected set of students to be the number of edges with infection on exactly one side). I currently have two problems in understanding the solution - When I calculate the perimeter of the infected students in the starting state, I get 30, when I should be getting n*4 = 6*4 = 24. Here's how. The solution also states that for every infected student added, the edges subtracted from the perimeter of the previous state is neutralized by the edges of the new infected student. What about this case? Also, as a more general question, how does one go about identifying the invariant property for such questions?","['invariance', 'induction', 'discrete-mathematics']"
3339016,How to get the following summation of the series $\sum\limits_{n=0}^{\infty}\frac{1}{n!(n^4+n^2+1)}$,"I am trying to find the sum $$\sum\limits_{n=0}^{\infty}\frac{1}{n!(n^4+n^2+1)}$$ I had factorized the sum as $$\frac{1}{2n(n!)}\left(\frac{1}{n(n-1)+1}-\frac{1}{n(n+1)+1}\right)$$ From this step, how to proceed?",['sequences-and-series']
3339019,Show this is a branch for $z^{ab}$.,"Let $f: G \to \mathbb{C}$ and $g: G \to \mathbb{C}$ branches of $z^a$ resp. $z^b$ . Suppose that $g(G) \subseteq G$ and $f(G) \subseteq G$ . Show that $$f \circ g, g \circ f$$ are branches of $z^{ab}$ . Attempt: Let $h: G \to \mathbb{C}$ be a branch of the logarithm such that $f(z) = \exp (a h(z))$ and $g(z) = \exp(b h(z))$ for all $z \in G$ . Thus $$\forall z \in G: \exp (h(z)) = z$$ I then have to show that $\exp(ab h(z)) = f \circ g = g \circ f$ . $$f \circ g (z) = f (g(z)) = f(\exp(bh(z)) = \exp(ah(\exp(bh(z))))$$ and I'm not sure to proceed. Of course, one wants to use that $$h(\exp(bh(z)) = bh(z)$$ but I'm not sure why this is true. Any help wil be appreciated!","['complex-analysis', 'exponentiation', 'branch-cuts', 'logarithms']"
3339029,Questions about a proof of the Gauss-Bonnet theorem,"$\newcommand{\dz}{{\rm d}z}$ $\newcommand{\dbz}{{\rm d}\bar z}$ I am reading this version of the Gauss-Bonnet theorem (in the book Compact Riemann Surfaces by Jurgen Jost), which is regarding compact Riemann surfaces without boundary equipped with a conformal metric: Theorem (Gauss-Bonnet) Let $\Sigma$ be a compact Riemann surface without boundary, of genus $p$ , with a conformal Riemannian metric given in local coordinates by $\rho^2(z)\dz\dbz$ . Let the curvature be $K_\rho$ . Then $$\int_\Sigma K_\rho\rho^2(z)\frac{i}{2}\dz\wedge\dbz=2\pi(2-2p)$$ Before giving the proof I should note that the cases in which the curvature is constant have been proved, so we can make use of them, namely, these cases: The unit sphere $\Sigma=S^2,\ K\equiv 1$ (Induced Eulidean metric) The torus $\Sigma=\mathbb C/M,\ K\equiv 0$ (Induced Eucliean metric, and $M$ is a rank-2 lattice) $\Sigma=H/\Gamma,\ K\equiv -1$ (Hyperbolic metric given by $\frac{1}{y^2}\dz\dbz$ , and $\Gamma$ is a discrete group of isometries, acting without fixed points) The proof in the book goes like this: proof. By the uniformization theorem we first assume $\Sigma$ is diffeomorphic to $S^2$ or $\mathbb C/M$ or $H/\Gamma$ . We put another metric $\lambda^2(z)\dz\dbz$ of constant curvature $K$ . Now the quotient $\rho^2(z)/\lambda^2(z)$ is invariant under coordinate transformations, i.e. behaves like a function. We compute now $$\int K\lambda^2\frac{i}{2}\dz\wedge\dbz-\int K_\rho\rho^2\frac{i}{2}\dz\wedge\dbz\\
=-4\int\frac{\partial^2}{\partial z\partial\bar z}\log\lambda\frac{i}{2}\dz\wedge\dbz+4\int\frac{\partial^2}{\partial z\partial\bar z}\log\rho\frac{i}{2}\dz\wedge\dbz\\
=4\int\frac{\partial^2}{\partial z\partial\bar z}\log\frac{\rho}{\lambda}\frac{i}{2}\dz\wedge\dbz$$ which vanishes by Gauss' Divergence Theorem. The theorem then follows from the cases with constant curvatures. Thoughts and Questions: (1) How can we ensure the existence of a metric with constant curvature? Can I say that the diffeomorphism $f:\Sigma\to S^2$ (or $\mathbb C/M$ or $H/\Gamma$ , by the uniformization theorem) equips $S^2$ with a metric induced by that on $\Sigma$ , and the curvature integrals on them are the same, so as to focus on $S^2$ instead of $\Sigma$ ? (2) I understand why $\rho^2(z)/\lambda^2(z)$ is invariant under a coordinate transformation. But it doesn't seem to be useful in the following proof. What does it do? (3) Why does the last integral vanish? The Gauss divergence theorem turns an integral over a domain to its boundary. However, $\Sigma$ is a surface without a boundary, so I am not sure how the divergence theorem applies here. Does this have something to do with $\rho^2/\lambda^2$ being invariant under coordinate changes?","['riemann-surfaces', 'riemannian-geometry', 'differential-geometry']"
3339097,What is $\lim_{n \to \infty} 2^n \tan\left(\frac{a}{2^n}\right)$?,"$$\lim_{n\to \infty}{2^n\cdot \tan\left(\frac{a}{2^n}\right)}$$ Experimenting with Wolfram Alpha, I came to suspect the limit is ${a}$ . Any help on this matter? I couldn't find it with with half-angle formulas and the series expansion is not very productive either. A similar  result is used (without proof) in the answer of Rohan Shindes question:  Evaluate $$\lim_{n\to \infty} \sum_{r=1}^n \frac {1}{2^r}\tan \left(\frac {1}{2^r}\right)$$ yet eventhough I invested some effort, I was unable to find the solution. Any help would be appreciated.","['limits', 'trigonometry', 'analysis', 'real-analysis']"
3339113,Bounding tails of sum of not identical random variables,"Let $\{X_i\}_{i=1}^n$ be a set of $n$ statistically independent random variables such that $\Pr(X_i=c_1)=\alpha/i$ , and $\Pr(X_i=c_2/i)=1-\alpha/i$ , and the constants $c_1<0,c_2>0$ , and $\alpha>0$ , are such that the expectation of each random variable $\mathbb{E}(X_i) \leq -\beta/i$ , for some $\beta>0$ . I want to upper bound the following probability $$
\Pr\left(\sum_{i=1}^nX_i>C\right),
$$ for some value of $C>0$ . I am also interested in upper bounding $$
\Pr\left(\max_{1\leq j\leq n}\sum_{i=1}^jX_i>C\right).
$$ Note that $\sum_{i=1}^n\mathbb{E}(X_i) = -\beta\sum_{i=1}^n1/i$ , and so it diverges to $-\infty$ logarithmically in $n$ . Since the random variables are highly non-identical I am not sure how to derive meaningful upper bound on these probabilities. I'm interested in any non-trivial (i.e., strictly less than $1$ ). I tried to use Hoeffding's inequality but it seems that it is not the ""right tool"" to use in this scenario.","['concentration-of-measure', 'probability-theory', 'probability']"
3339142,Proof that all $n\times n$ matrices that are nilpotent of order $n$ are similar.,"Can someone give a proof that all $n\times n$ matrices that are nilpotent of order $n$ are similar? A matrix $A$ is called nilpotent if there exists some positive integer $k$ such that $A^k$ is the $0$ -matrix. The order of nilpotency of a matrix is the smallest $k$ with this property. That is, the question is about matrices such that $A^n=0$ yet $A^{n-1} \neq 0$ . I am new in linear algebra and know up to linear transformation and isomorphisms and matrix representation of transformations. The same question is answered in this site using Jordan Matrix and other things which I do not know. So can anyone give a proof using elementary things which I know.",['linear-algebra']
