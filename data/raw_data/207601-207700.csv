question_id,title,body,tags
4154440,"$A \in GL_n (\mathbb C)$ and $N$ nilpotent matrix, A and N commute. Show there exists B such as $B^{2} = A + N$","Let $n \in \mathbb N^{*}$ , $A \in GL_n (\mathbb C)$ and $N \in M_n (\mathbb C)$ with $N$ Nilpotent such as $AN=NA$ . Prove that there exists $B \in M_n (\mathbb C) $ such as $B^{2} = A + N$ Since $A$ and $N$ are triangularisable  and are commuting matrices, $A$ and $N$ are simultaneously triangularisable. There exists $P \in GL_n (\mathbb C)$ such as $A=P\left(
    \begin{array}{ccccc}
    \lambda_1                                    \\
      & \ddots             &   & \huge*\\
      &               & \ddots                \\
      & \huge0 &   & \lambda_n 
    \end{array}
    \right)P^{-1}\:\:\:$ with $(\lambda_1,\dots,\lambda_n)$ the eigenvalues of $A$ . $A$ is invertible thus $(\lambda_1,\dots,\lambda_n) \ne(0,\dots,0)$ and $N=P\left(
    \begin{array}{ccccc}
    0                                    \\
      & \ddots             &   & \huge{*'}\\
      &               & \ddots                \\
      & \huge0 &   & 0
    \end{array}
    \right)P^{-1}$ Let's set $T=\left(
    \begin{array}{ccccc}
    \lambda_1                                    \\
      & \ddots             &   & \huge*\\
      &               & \ddots                \\
      & \huge0 &   & \lambda_n 
    \end{array}
    \right)+\left(
    \begin{array}{ccccc}
    0                                    \\
      & \ddots             &   & \huge{*'}\\
      &               & \ddots                \\
      & \huge0 &   & 0
    \end{array}
    \right)=\left(
    \begin{array}{ccccc}
    \lambda_1                                    \\
      & \ddots             &   & \huge*''\\
      &               & \ddots                \\
      & \huge0 &   & \lambda_n 
    \end{array}
    \right)$ Let's define $ L \in M_n (\mathbb C)$ by $\forall (i,j) \in [\![1;n]\!]^{2}, L_{i,j}=\left\{
    \begin{array}{lll}
        0 & \mbox{if } i>j \\
        \alpha_i & \mbox{if } i=j \\
        \frac{1}{\alpha_i+\alpha_j} \times(T_i,j - \sum_{k=i+1}^{j-1} L_{i,k} L_{k,j}) &\mbox{else}
    \end{array}
\right.$ , with $(\alpha_1,\dots,\alpha_n) \:\:$ square roots of $(\lambda_1,\dots,\lambda_n)$ . The diagonal of L is well defined because for all $i$ , $\alpha_i \ne 0$ since $\lambda_i \ne 0$ . The upper part of L is defined step by step, the diagonal then the first subdiagonal, then the second subdiagonal, and so on; (for $j-i$ going from $0$ to $n-1$ , with $(i,j) \in [\![1;n]\!]^{2}$ and $j\geq i$ ). $L$ is an upper triangular matrix so $L^{2}$ is also an upper triangular matrix.
Let $(i,j) \in [\![1;n]\!]^{2}$ and $j> i$ $L^{2}_{i,i}= \alpha_i^{2} = \lambda_i =T_{i,i}\\
L^{2}_{i,j}= \sum_{k=0}^{n}L_{i,k}L_{k,j}=\sum_{k=i}^{j}L_{i,k}L_{k,j}= L_{i,i}L_{i,j}+ \sum_{k=i+1}^{j-1}L_{i,k}L_{k,j} +  L_{i,j}L_{j,j} = L_{i,j} ( \alpha_i+\alpha_j) + \sum_{k=i+1}^{j-1}L_{i,k}L_{k,j}=\frac{1}{\alpha_i+\alpha_j} \times(T_i,j - \sum_{k=i+1}^{j-1} L_{i,k} L_{k,j}) \times ( \alpha_i+\alpha_j) + \sum_{k=i+1}^{j-1}L_{i,k}L_{k,j}=T_{i,j}$ ie $L^{2}=T$ let's pose $B=PLP^{-1}$ we have $ B^{2}= PL^{2}P^{-1}= PTP^{-1}= A+N$ Several questions: Is the definition of $L$ correct? Is there another method, which does not use the matrix L ?","['matrices', 'nilpotence', 'linear-algebra']"
4154454,Sequence of random variables and limits,"Let $X_1, X_2, \dots : \Omega \to \mathbb R$ be a sequence of random variables. I want to show that there exists a sequence $\{A_n\}$ , where $A_n > 0$ for all $n$ , such that: $$ P(\{ \omega \in \Omega: \lim_{n \to \infty}\frac{X_n(\omega)}{A_n}=0\}) =1.$$ For this I've been told that I should use $P(\{ |X_n|>\frac{A_n}{n} \})\leq \frac{1}{2^n}$ . My first question is: how to prove this? I have an intuition of why it is true, but don't know how to write it down. Then, I know that because of the Borel Cantelli lemma I can write $$
P(\{ \frac{|X_n|}{A_n} > \frac{1}{n}\ \quad \text{i.o.}\})=0
$$ which leads that there is some measurable set in which $P(\{ \frac{|X_n|}{A_n} \leq \frac{1}{n}\ \})=1$ . My second question is how to guarantee that this happens for some $n > N$ to take the limit and conclude the proof.","['measure-theory', 'real-analysis']"
4154460,Directly Computing Posterior Distribution for Gaussian Likelihood and Prior,"my first question so apologies in advance. I'm currently studying posterior distributions and for practice sake I'm trying to directly compute the posterior distribution for the mean of a normal random variable (unit variance) with standard normal prior. I'm trying to validate if this is correct (note that this is for observing ONE new data point). The posterior distribution is defined as $$p(\theta\mid x)=\frac{p(x\mid\theta)p(\theta)}{\int_\theta p(x\mid\theta) p(\theta) \, d\theta}$$ and I am getting $$p(\theta\mid x)=\frac{1}{\sqrt{\pi}}e^{-\frac{1}{4}x^2+x\theta-\theta^2}$$ I did a couple sanity checks, $$\int_{-\infty}^\infty p(\theta\mid X=0)\,d\theta=\int_{-\infty}^\infty \frac{1}{\sqrt{\pi}} e^{-\theta^2} \, d\theta=1$$ $$\int_{-\infty}^\infty p(\theta\mid X=2) \, d\theta=\int_{-\infty}^\infty \frac{1}{\sqrt{\pi}} e^{-1+2\theta-\theta^2} \, d\theta=1$$ Is this the correct analytical solution? My derivation is as follows: $$\text{Let } X \sim N(\theta,1)$$ $$\text{Let } \theta \sim N(0,1)$$ The likelihood (dependent on theta): $$p(x\mid\theta)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x-\theta)^2}$$ The prior: $$p(\theta)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\theta^2}$$ The joint density: $$p(x\mid\theta)p(\theta)=\frac{1}{2\pi}e^{-\frac{1}{2}(x-\theta)^2}e^{-\frac{1}{2}\theta^2}$$ $$=\frac{1}{2\pi}e^{-\frac{1}{2}x^2+x\theta-\frac{1}{2}\theta^2}e^{-\frac{1}{2}\theta^2}$$ $$=\frac{1}{2\pi}e^{-\frac{1}{2}x^2+x\theta-\theta^2}$$ sanity check : $$\int_{-\infty}^\infty \int_{-\infty}^\infty \frac{1}{2\pi}e^{-\frac{1}{2} x^2 + x\theta-\theta^2} \, dx \, d\theta=1$$ The marginal: $$p(x)=\int_\theta p(x\mid\theta)p(\theta) \, d\theta$$ $$=\frac{1}{2\pi}e^{-\frac{1}{2}x^2} \int_{-\infty}^\infty e^{-\theta^2+\theta x} \, d\theta$$ $$=\frac{1}{2\pi}e^{-\frac{1}{2}x^2}\Biggl[\sqrt{\pi}e^{\frac{1}{4}x^2}\Biggl] \text{by Gaussian integral}$$ $$=\frac{1}{2\sqrt{\pi}}e^{-\frac{1}{4}x^2}$$ sanity check: $$\int_{-\infty}^\infty \frac{1}{2\sqrt{\pi}}e^{-\frac{1}{4}x^2} \, dx=1$$ The posterior: $$p(\theta\mid x)=\frac{p(x\mid\theta)p(\theta)}{\int_\theta p(x\mid \theta) p(\theta) \, d\theta} = \frac{\frac{1}{2\pi}e^{-\frac{1}{2}x^2+x\theta-\theta^2}}{\frac{1}{2\sqrt{\pi}}e^{-\frac{1}{4}x^2}}$$ $$=\frac{\sqrt{\pi}}{\pi}e^{-\frac{1}{2}x^2+x\theta-\theta^2+\frac{1}{4}x^2}$$ $$=\frac{1}{\sqrt{\pi}}e^{-\frac{1}{4}x^2+x\theta-\theta^2}$$","['statistics', 'bayesian', 'probability']"
4154499,Issue in my proof that the limit of circles through three points on a curve is the osculating circle,"I'm going through some differential geometry exercises (from Kristopher Tapp's Differential Geometry of Curves and Surfaces ) I worked on a while ago, and realised I missed a detail in Part (2) of the following: Let $\boldsymbol\gamma: I \to \mathbb R^n$ be a unit-speed curve. Let $t_0 \in I$ and $\kappa(t_0) \neq 0$ . For sufficiently small $h > 0$ , prove that the three points $\boldsymbol\gamma(t_0 - h), \boldsymbol\gamma(t_0)$ , and $\boldsymbol\gamma(t_0 + h)$ are not collinear, so there is a unique plane $P_h$ containing them and a unique circle $C_h$ containing them. Precisely formulate and prove the following: (1) As $h \to 0$ , $P_h$ converges to the osculating plane (translated to $\boldsymbol\gamma(t_0)$ ). (2) As $h \to 0$ , $C_h$ converges to the osculating circle (translated to $\boldsymbol\epsilon(t_0)$ ). HINT: For (1), use the Taylor approximation formulas from this section. For (2), for fixed $h$ , let $\mathbf p(h)$ denote the center of $C_h$ , and define $$f(s) = \lvert\boldsymbol\gamma(t_0 + s) - \mathbf p(h)\rvert^2.$$ Since $f(-h) = f(0) = f(h)$ , the mean value theorem says that there exist $\delta_1 \in (-h, 0)$ and $\delta_2 \in (0, h)$ with $f'(\delta_1) = f'(\delta_2) = 0$ , and then that there exists $\epsilon \in (\delta_1, \delta_2)$ with $f''(\epsilon) = 0$ , which becomes \begin{align}
0 = f'(\delta_i)  &= 2 \langle\boldsymbol\gamma'(t_0 + \delta_i), \boldsymbol\gamma(t_0 + \delta_i) - \mathbf p(h)\rangle \textit{ (for $i \in \{1, 2\}$),}\\
0 = f''(\epsilon) &= 2 \langle\boldsymbol\gamma''(t_0 + \epsilon), \boldsymbol\gamma(t_0 + \epsilon) - \mathbf p(h)\rangle + 2 \lvert\boldsymbol\gamma'(t_0 + \epsilon)\rvert^2.
\end{align} Now consider the limit as $h \to 0$ . In the exercise, $\kappa$ is the curvature function and $\boldsymbol\epsilon$ the center of the osculating circle. Further, the text treats all curves as smooth. I interpreted the statement as saying that (in addition to the statement in Part (1)) $\lim_{h \to 0^+}\mathbf p(h) = \boldsymbol\epsilon(t_0)$ . What I actually showed, however, was that if $\lim_{h \to 0^+}\mathbf p(h) = \mathbf L$ , then $\mathbf L = \boldsymbol\epsilon(t_0)$ : We define $$f(s) = \lvert\boldsymbol\gamma(t_0 + s) - \mathbf p(h)\rvert^2.$$ Since $f$ is smooth and $f(-h) = f(0) = f(h)$ , the mean value theorem says that there exist $\delta_1 \in (-h, 0)$ and $\delta_2 \in (0, h)$ with $f'(\delta_1) = f'(\delta_2) = 0$ , and then that there exists $\epsilon \in (\delta_1, \delta_2)$ with $f''(\epsilon) = 0$ , which becomes \begin{align}
\langle\boldsymbol\gamma'(t_0 + \delta_i), \boldsymbol\gamma(t_0 + \delta_i) - \mathbf p(h)\rangle &= 0\\
\langle\boldsymbol\gamma''(t_0 + \epsilon), \boldsymbol\gamma(t_0 + \epsilon) - \mathbf p(h)\rangle &= -1.
\end{align} Now, as $h \to 0$ , $\delta_i, \epsilon \to 0$ , and so if $\lim_{h \to 0^+}\mathbf p(h) = \mathbf L$ , \begin{align}
\langle\boldsymbol\gamma'(t_0), \boldsymbol\gamma(t_0) - \mathbf L\rangle  &= 0\\
\langle\boldsymbol\gamma''(t_0), \boldsymbol\gamma(t_0) - \mathbf L\rangle &= -1.
\end{align} This tells us that $\mathbf L - \boldsymbol\gamma(t_0) \perp \boldsymbol\gamma'(t_0)$ . Further, since $\mathbf p(h)$ lies in $P_h$ , $\mathbf L$ lies in the translated osculating plane, and so $\mathbf L - \boldsymbol\gamma(t_0)$ lies in the osculating plane. Thus, $\mathbf L - \boldsymbol\gamma(t_0) \parallel \boldsymbol\gamma''(t_0)$ . We can then conclude that $\mathbf L - \boldsymbol\gamma(t_0)$ points in the direction of $\boldsymbol\gamma''(t_0)$ and that $\lvert\mathbf L - \boldsymbol\gamma(t_0)\rvert = \frac{1} {\kappa(t_0)}$ , i.e. $\mathbf L - \boldsymbol\gamma(t_0) = \frac{1} {\kappa(t_0)} \mathfrak n$ (where $\mathfrak n$ is the unit normal vector). Thus $\mathbf L = \boldsymbol\epsilon(t_0)$ . However, this doesn't rule out the possibility that $\mathbf p(h)$ doesn't have a limit as $h \to 0$ . I thought to try and finish the proof I should show that $\mathbf p$ is smooth with bounded derivative, and hence uniformly continuous. In doing so, however, I found $\mathbf p(h) - \boldsymbol\gamma(t_0)$ in terms of $\mathbf u_1(h) := \boldsymbol\gamma(t_0 + h) - \boldsymbol\gamma(t_0)$ and $\mathbf u_2(h) := \boldsymbol\gamma(t_0 - h) - \boldsymbol\gamma(t_0)$ , and a direct calculation of the limit seems to suggest that the limit is $\boldsymbol\gamma(t_0)$ instead. To find $\mathbf p(h) - \boldsymbol\gamma(t_0)$ , I set $\mathbf p(h) - \boldsymbol\gamma(t_0) = \alpha(h) \mathbf u_1(h) + \beta(h) \mathbf u_2(h)$ . I then used the condition that $\langle\mathbf p(h) - \boldsymbol\gamma(t_0 + h), \mathbf p(h) - \boldsymbol\gamma(t_0 + h)\rangle = \langle\mathbf p(h) - \boldsymbol\gamma(t_0), \mathbf p(h) - \boldsymbol\gamma(t_0)\rangle = \langle\mathbf p(h) - \boldsymbol\gamma(t_0 - h), \mathbf p(h) - \boldsymbol\gamma(t_0 - h)\rangle$ along with the facts that $\mathbf p(h) - \boldsymbol\gamma(t_0 + h) = (\alpha(h) - 1) \mathbf u_1(h) + \beta(h) \mathbf u_2(h)$ and $\mathbf p(h) - \boldsymbol\gamma(t_0 - h) = \alpha(h) \mathbf u_1(h) + (\beta(h) - 1) \mathbf u_2(h)$ to solve for $\alpha(h), \beta(h)$ . As a result, I found $$\mathbf p(h) - \boldsymbol\gamma(t_0) = \frac{\lvert\mathbf u_2(h)\rvert^2 \langle\mathbf u_1(h), \mathbf u_1(h) - \mathbf u_2(h)\rangle \mathbf u_1(h) + \lvert\mathbf u_1(h)\rvert^2 \langle\mathbf u_2(h) - \mathbf u_1(h), \mathbf u_2(h)\rangle \mathbf u_2(h)}{2 (\lvert\mathbf u_1(h)\rvert^2 \lvert\mathbf u_2(h)\rvert^2 - \langle\mathbf u_1(h), \mathbf u_2(h)\rangle^2)}.$$ But, since $\lim_{h \to 0^+}\mathbf u_1(h) / h = \boldsymbol\gamma'(t_0)$ and $\lim_{h \to 0^+}\mathbf u_2(h) / h = -\boldsymbol\gamma'(t_0)$ , this should imply that $$\lim_{h \to 0^+}(\mathbf p(h) - \boldsymbol\gamma(t_0)) = \lim_{h \to 0^+}\frac{\left\lvert\frac{\mathbf u_2(h)}{h}\right\rvert^2 \left\langle\frac{\mathbf u_1(h)}{h}, \mathbf u_1(h) - \mathbf u_2(h)\right\rangle \frac{\mathbf u_1(h)}{h} + \left\lvert\frac{\mathbf u_1(h)}{h}\right\rvert^2 \left\langle\mathbf u_2(h) - \mathbf u_1(h), \frac{\mathbf u_2(h)}{h}\right\rangle \frac{\mathbf u_2(h)}{h}}{2 \left(\left\lvert\frac{\mathbf u_1(h)}{h}\right\rvert^2 \left\lvert\frac{\mathbf u_2(h)}{h}\right\rvert^2 - \left\langle\frac{\mathbf u_1(h)}{h}, \frac{\mathbf u_2(h)}{h}\right\rangle^2\right)} = \mathbf0.$$ Am I going wrong somewhere?","['curves', 'osculating-circle', 'curvature', 'solution-verification', 'differential-geometry']"
4154521,Holonomy and Infinitesimal Rotation,"I'm relatively new to the concept of holonomy so I'm currently trying to understand holonomy in the context of specific examples, i.e., Hopf fibration. Let $\hat{n}(t) \in \mathbb{S}^2$ denote a curve and consider the (loosely defined) infinitesimal transition from $t\mapsto t+\delta t$ , so that $\hat{n}\mapsto \hat{n}+\delta \hat{n}$ . We can then consider a horizontal lift, which I denote as $\psi(t) \in \mathbb{C}^2$ with $|\psi| =1$ (I shall regard $\mathbb{S}^3$ and the unit sphere in $\mathbb{C}^2$ as equivalent spaces), and consider the infinitesimal holonomy $\psi \mapsto \psi +\delta\psi$ . Based on this article , the change in $\psi$ can be expressed as a change in phase, plus a rotation, i.e., $$
\psi +\delta \psi=e^{i\delta A} U^{\delta t}\psi 
$$ Where $iA$ is the conventional connection 1-form so that $i\delta A =\int_t^{t+\delta t} iA$ and $U(t)\in SU(2)$ is defined by $$
U(t)=\exp(-iH(t)\cdot\tau), \quad H(t)=\frac{1}{2}(\hat{n}\times \delta\hat{n})
$$ And $\tau$ is the vector of Pauli matrices so that $\psi \mapsto U^{\delta t}\psi$ corresponds to an infinitesimal rotation which maps $\hat{n} \mapsto \hat{n}+\delta\hat{n}$ when projected onto $\mathbb{S}^2$ . Question . If you work through all the algebra, the above formulation indeed seems to works. However, I'm confused on how one comes up with this and how this can be formulated rigorously.
It seems to be related to how the holonomy is related to the path-ordered integral of the connection, but I'm a little fuzzy on this concept and I was hoping someone could clarify and possibly provided references. EDIT . Just as a review, the Hopf fibration from the unit sphere in $\mathbb{C}^2$ onto $\mathbb{S}^2$ can be described by the map given as follows: let $\psi\in \mathbb{C}^2$ with $|\psi|=1$ , then the projection operator $P_\psi = |\psi\rangle \langle \psi|$ (physics notation) onto the span of $\psi$ is given by $$
P_\psi=\frac{1}{2}(\tau_0+\hat{n}\cdot \tau)
$$ (Where $\tau_0$ is the $2\times 2$ identity operator) for some unique $\hat{n}\in \mathbb{S}^2$ . Hence, $\psi\mapsto \hat{n}$ is the corresponding fibration.","['holonomy', 'differential-geometry']"
4154526,Is this proof valid? Intersection of complements equals complement of union,"If $X \subset S\  $ , $Y \subset S , \text{ then } eX \cap eY=e(X \cup Y)$ (where $e$ is ""complement"") Proof: if $  x\in S\text{ and } X,Y\subset S,\text{and if } x \not\in X ,\ x \not\in Y \text{ then }x \in eX,eY  \text{ and  } x \in eX\cap eY $ If $x\not\in X,Y \text{ then } x\not\in X\cup Y \text{ hence }x \in e(X\cup Y) $ Side Question: is this notation ok, $\ x\in eX,eY\ $ ? Does that mean x is in $eX$ and also in $eY$ ? Thank you!","['elementary-set-theory', 'solution-verification']"
4154551,What is the Group of Bijections of the circle,"Ok, this problem is driving me crazy because the problem says: In the group of bijections of the circle, build a succesion of subgroups $G_i$ such that $G_i$ is a subgroup of $G_{i+1}$ , and $|G_i|=i!$ . I understand it like this: The circle has infinite points, if I have one, the bijection is basically a rotation of $0°$ because it ends up in the same point, if I have $2$ points in the circle, then I have $2!$ permutations that is, $2$ rotations of the circle, if I have $3$ elements with $3!=6$ possible permutations $= 6$ rotations, if I have $4$ elements then there are $4!=24$ permutations or rotations. That is how I understand it but I don't know how could those rotations be, I mean graphically I can't think of a way to express those permutations. I want to understand these first examples to extend it to any $G_i$ group. Thanks in advance for your support.","['group-theory', 'abstract-algebra']"
4154557,Showing that a bundle homomorphism is a smooth isomorphism.,"This is Lee's problem $10-11$ in his Introduction to Smooth Manifolds . If $\pi:E\to M$ and $\pi':E'\to M$ are vector bundles over the smooth manifold $M$ and if $F:E\to E'$ is a bijective smooth bundle homomorphism, then $F$ is a smooth bundle isomorphism. Here is my attempt. My question is at the very end. We need to show that $F^{-1}$ is smooth. So, let $p\in E.$ There are open sets $U,U'\subseteq M$ such that $p\in U, F(p)\in U'$ and such that there are local trivializations $\Psi:\pi^{-1}(U)\to U\times \mathbb R^n\ $ and $\Psi':\pi'^{-1}(U')\to U'\times \mathbb R^n.$ Define $f=\Psi'\circ F\circ \Psi^{-1}$ on a sufficiently small open set containing $\Psi (p)$ so that the diagram from which the following data follow makes sense: $\tag 1 \pi_U\circ \Psi=\pi$ $\tag 2 \pi_{U'}\circ \Psi'=\pi'$ $\tag 3 \pi'\circ F=\pi$ Now, take $x\times v$ in this open set. Using $(1),(2),(3),$ we can show that $f$ sends $x\times v$ to $x\times w$ for some $w\in \mathbb R^n.$ Since $\Psi, \Psi'$ are diffeomorphisms and and $F$ is bijective, $f$ is bijective. We also have that $f\big |_{ \{x\}\times \mathbb R^n}$ is a vector space isomorphism. This means that there is an invertible linear transformation $\tau_x:\mathbb R^n\to \mathbb R^n$ such that $f(x,v)=(x,\tau_x(v)).$ Then, $f^{-1}(x,v)=(x,\tau_x^{-1}(v)).$ To finish, just note that $f^{-1}=\Psi\circ F^{-1}\circ \Psi'^{-1}$ and  that the composition $(x,v)\mapsto (x,\tau_x(v))\mapsto (x,\tau_x^{-1}(v))$ is smooth. Inversion is smooth but it is not clear to me why the first one is. Edit: the first map is smooth just because it is just $f=\Psi'\circ F\circ \Psi^{-1},$ which is smooth.","['manifolds', 'smooth-manifolds', 'differential-geometry']"
4154562,This inequality holds on Hilbert Spaces?,"I am trying to solve a probelm and if I conclude that the following inequality holds I finish it. Consider $H$ an Hilbert space over $\mathbb{R}$ . Let $\alpha_1, ..., \alpha_n$ be real numbers such that $\alpha_i\geq 0$ , for all $i=1,...,n$ and $\sum_{i=1}^n\alpha_i=1$ . Let $x_1, ...,x_n \in H$ and define $$x:=\sum_{i=1}^n\alpha_ix_i$$ My question is: the following inequality holds for all $i,j$ ? $$\lVert x-x_i\rVert ^2+\lVert x-x_j\rVert ^2 \leq \lVert x_i-x_j\rVert ^2$$ I can easily see that it holds when $\{x_1, ..., x_n\}$ is orthogonal, but I'm not sure in the general case. I would appreciate any hint. Thanks!","['hilbert-spaces', 'functional-analysis', 'analysis', 'real-analysis']"
4154584,What is the length of side AB of the triangle?,"Point X is 3m from A, 4m from B and 5m from C. Point X is inside the triangle formed by ABC. If AB = BC and angle B is right angle. Find the length of side AB. I have come up with the following figure and aware that this is an isosceles right triangle, unfortunately i'm not sure what to do next.","['trigonometry', 'geometry', 'plane-geometry']"
4154597,Can time be measured in degrees?,"As far as I know, minutes and seconds (units of time) are definitely somehow related to the angular conversions we study in high school math (1 degree = 60 minutes, so on and so forth). So why don't we measure time in degrees (or even radians)? Are these minutes in a clock even related to these conversions taught in trigonometry class? And, how are earth's latitudes measured in degrees, I mean latitudes are straight lines and I don't see any angle forming. I'm literally at sixes and sevens right now, please provide me with some insight on this.","['trigonometry', 'circles', 'geometry', 'unit-of-measure']"
4154608,Spivak Chapter 7 Question 13: Clarification,"In question 13 b of chapter 7 of Spivak, I'm being asked to prove that if a function satisfies the conclusion of the intermediate value theorem, and if that function takes on each value only once, it is continuous. I don't see why this is true, however. Let $f(x) = x$ in the interval $[1, 2)$ , and 2 when x = 3. Then f(x) satisfies the conclusion of the intermediate value theorem on [1, 3], but isn't continuous. Could someone explain to me why this isn't a counterexample for the given statement, or if I've misinterpreted the question? Could you also avoid giving hints on how to solve this problem? I would still like to attempt solving this question. Thanks so much in advance!","['calculus', 'analysis']"
4154668,Determinant as the volume of a box in n-dimensions,"Below is a proof found in Gilbert Strang's book and here Why is the determinant the volume of a parallelepiped in any dimensions? that the determinant equals the volume of a box : To find the volume of a box whose edges are given by a  set of vectors $\{v_{1},\ldots,v_{n}\}$ we apply Gram-Schmidt orthogonalization to $\{v_{1},\ldots,v_{n}\}$ , so that \begin{eqnarray*}
v_{1} & = & v_{1}\\
v_{2} & = & c_{12}v_{1}+v_{2}^{\perp}\\
v_{3} & = & c_{13}v_{1}+c_{23}v_{2}+v_{3}^{\perp}\\
 & \vdots
\end{eqnarray*} where $v_{2}^{\perp}$ is orthogonal to $v_{1}$ ; and $v_{3}^{\perp}$ is orthogonal to $span\left\{ v_{1},v_{2}\right\} $ , etc.
Since determinant is multilinear, anti-symmetric, then \begin{eqnarray*}
\det\left(v_{1},v_{2},v_{3},\ldots,v_{n}\right) & = & \det\left(v_{1},c_{12}v_{1}+v_{2}^{\perp},c_{13}v_{1}+c_{23}v_{2}+v_{3}^{\perp},\ldots\right)\\
 & = & \det\left(v_{1},v_{2}^{\perp},v_{3}^{\perp},\ldots,v_{n}^{\perp}\right)\\
 & =?& \mbox{signed volume}\left(v_{1},\ldots,v_{n}\right)
\end{eqnarray*} Question : This only proves that the determinant of the original edges equals the volume of the new created box. That is we don't show $\det(v_1,...,v_n)$ = signed volume $(v_1,...v_n)$ .
What we do prove is $\det(v_1,...,v_n)$ = Volume of  the box with orthogonal edges.","['matrices', 'determinant', 'linear-algebra', 'volume']"
4154715,Question about Bezout's theorem between varieties and schemes.,"Exercise 18.6.K in Vakil's Foundations of Algebraic geometry is: Let $X$ be a projective scheme of dimension $\geq 1$ over a field $k$ , with a fixed closed immersion $i : X \rightarrow \mathbb{P}^n_k$ . Let $H= V(f)$ be a hypersurface not containing any associated points of $X$ . Then, $\deg(H \cap X) = \deg(H) \cdot \deg(X)$ Theorem 1.7.7 in Hartshorne's Algebraic Geometry is : Let $Y$ be a variety of dimension $\geq 1$ in $\mathbb{P}^n_k$ . Let $H$ be a hypersurface not containing $Y$ . Let $Z_1, \dots, Z_s$ be the irreducible components of $Y \cap H$ , corresponding to prime ideals $p_1, \dots, p_s$ . Define $i(Y, H ; Z_j)$ to be the length of $k[x_0, \dots, x_n] / (I(Y) + I(H))$ at $p_j$ . Then, $\sum_{j=1}^s i(Y, H ; Z_j) \cdot \deg(Z_j) = \deg(Y) \cdot \deg(H)$ . I want to use theorem 1.7.7 to solve exercise 18.6.K. The difficulty that I'm having is that in chapter 1 of Hartshorne, only integral varieties over algebraically closed fields are considered. But in Vakil, $X$ can be reduce or have multiple components. My attempt at solving this problem is: if I take $X = \operatorname{Proj}(k[x_0, \dots, x_n] / I)$ , I can then take a primary decomposition of $I = \cap Q_i$ to consider each individual irreducible component separately, therefore I reduce to where $X$ is irreducible. But then I'm not sure how to relate the degree of $k[x_0, \dots, x_n] / I$ with $k[x_0, \dots, x_n] / Q_i$ . Furthermore, $Q_i$ is just primary , and not prime. So I can't apply theorem 1.7.7 directly. I also don't know how relate the degree of $k[x_0, \dots, x_n] / Q_i$ with its length over $p_i$ .","['algebraic-geometry', 'intersection-theory']"
4154786,The translation in french of «Layer Cake Representation»,"I would like to know if there exists a generic way of referring to this identity $$\int_X |f(x)| \mathrm{d} \mu(x) = \int_0^\infty \mu\{x \in X : |f(x)| > t \} \mathrm{d} t$$ in french. In english, it is the layer cake representation. The same question arose on this site for the German translation : translate layer cake representation in German and the question was closed ; I apologize if it is not on-topic. I didn't managed to find to wikipedia page in french : see [EN] https://en.wikipedia.org/wiki/Layer_cake_representation .","['measure-theory', 'translation-request', 'analysis', 'mathematical-french']"
4154790,"If a different factorial function would've been defined, what would the graph of this function look like?","Let's define a new, different factorial function, such that $\frac{a}{b}!$ returns $\frac{a!}{b!}$ . How would the graph of the function look like? For example, $\frac{1}{2}!$ does not return $\frac{\sqrt{\pi}}{2}$ , instead $\frac{1!}{2!}$ (or just $\frac{1}{2}$ ).","['gamma-function', 'functions', 'factorial', 'graphing-functions']"
4154812,Finding the probability of winning lottery in four different conditions,"There is a lottery game , it consists of $6$ block and each blocks contains numbers from $1$ to $90$ .When you play the lottery , you should select one number from each of these $6$ blocks. $\color{red}{a-)}$ What is the probability of winning lottery if winner numbers are different from each other (i.e repetition is not allowed) and the order of numbers matters? (for example if winner lottery is $5-12-45-85-3-2$ respectively, you cannot take money if your numbers are $12-5-45-85-3-2$ ,respectively.) $\color{red}{b-)}$ What is the probability of winning lottery if winner numbers can repeat and the order of numbers are matter ? $\color{red}{c-)}$ What is the probability of winning lottery if winner numbers are different from each other (i.e repetition is not allowed) and the order of numbers does not matter ? (For example , if the seleced numbers are $3-4-56-78-46-33$ , then the tickets have any permutation of these numbers will be winner such that $3-4-56-78-46-33$ , $56-78-46-3-4-33$ , $4-78-33-46-56-3$ ,etc  are winners) $\color{red}{d-)}$ What is the probability of winning lottery if winner numbers can repeat and the order of numbers does not matter ? (For example , if the winner numbers are $2-19-2-89-89-23$ , then any permutation of them can take prize such as $2-19-2-89-89-23$ ,or $19-2-2-23-89-89$ ,or $89-2-23-89-2-19$ ) MY ATTEMPT: $\color{blue}{a-)}$ $\frac{1}{P(90,6)}$ $\color{blue}{b-)}$ $\frac{1}{90^{6}}$ $\color{blue}{c-)}$ $6! \times \frac{1}{P(90,6)}$ $\color{blue}{d-)}$ I thought that i should make use of exponential function such as $(e^x)^6 $ and find the coefficient of $\frac{x^6}{6!}$ , so the answer is $\frac{6^6}{90^6} $ or should i use $(e^x)^{90} $ and find the coefficient of $\frac{x^6}{6!}$ , so the answer is $\frac{90^6}{90^6}=1 $ Is my solutions correct ? If not , can you help to correct them ? EDITED NOTE = selection probability of each number is equal","['solution-verification', 'combinatorics', 'lotteries', 'discrete-mathematics', 'probability']"
4154851,Differential equation non homogeneous [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Pls help me with this $$2y''–y'–3y= 5e^{3x/2}$$ I've solved the $yc$ part but the particular solution is given me zero
I also try to use $y=Axe^{3x/2}$ but I keep getting $x$ in my answer please help",['ordinary-differential-equations']
4154852,An elementary (and soft) question on strict open subschemes of the affine plane.,"Question: A scheme $(X, \mathcal{O}_X)$ is a pair where $X$ is a topological space and $\mathcal{O}_X$ is a sheaf of rings on $X$ , with the property that there is an open cover $U_i$ of $X$ with $U_i \cong Spec(A_i)$ an affine scheme. It is easy to construct an example of a topological space $X_t$ and a strict open subspace $U_t \subsetneq X_t$ with an abstract homeomorhism $\phi_t: U_t \cong X_t$ . Does a similar property hold for a scheme of finite type over a field? Example: Let $k$ be an algebraically closed field and let $S:=\mathbb{A}^2_k$ be the affine plane and let $U \subsetneq S$ be a strict open subscheme. Can you give an elementary example of such $U$ where there is an abstract isomorphism $\phi: U \cong S$ of schemes? What about a scheme $X$ of finite type over a field $k$ ? As a consequence: If $U_t$ is the topological space of $U$ it follows there is a strict subset $U_t \subsetneq S_t$ with an abstract homeomorphism $\phi_t:U_t  \cong S_t$ . Note: Two schemes (or ringed spaces) $(X, \mathcal{O}_X), (Y, \mathcal{O}_Y)$ are ""isomorphic"" iff there are maps $f:(X, \mathcal{O}_X) \rightarrow (Y, \mathcal{O}_Y), g:(Y, \mathcal{O}_Y)\rightarrow  (X, \mathcal{O}_X)$ with $f\circ g =Id,g \circ f=Id$ . In particular the maps $f,g$ induce homeomorphisms of topological spaces $X \cong Y$ . If a strict  open subscheme $U \subsetneq V$ is abstractly isomorphic to $V$ you end up with a topological space $U_t \subsetneq V_t$ realizable as a strict subspace of $V_t$ , but where there is an abstract isomorphism $U_t \cong V_t$ . This stuation is ""paradoxical"" - the topological space $V_t$ would then be homeomorphic to a strict subspace of itself. Example: For the affine line $\mathbb{A}^1_k$ with $k$ an algebraically closed field, it follows there is no abstract isomorphism $\phi: U \cong \mathbb{A}^1_k$ for any strict open subscheme $U \subsetneq \mathbb{A}^1_k$ : Let $k$ be an algebraically closed field and let $A:=k[x]$ be the polynomial ring in one variable $x$ over $k$ . Let $$f(x):=(x-u_1)^{l_1}\cdots (x-u_m)^{l_m}$$ with $u_i \neq u_j\in k$ . Let $X:=Spec(A)$ and $U:=D(f) \subsetneq X$ be a strict open subscheme of $X$ and assume $X \cong D(f)$ is an abstract isomorphism of schemes over $k$ , induced by a map of $k$ -algebras $\tilde{\phi}: A \cong A_f$ . The map $\tilde{\phi}$ induce a map of $k$ -algebras $$\phi:k[x] \rightarrow k[x]$$ with $\phi(x)=a(x)$ and $\phi(f(x)):=f(a(x))\in k^*$ a unit. It follows $\phi(a(x)):=a_0\in k^*$ is a unit with $a_0\neq u_i$ for all $i$ . Hence the map $\phi$ factors as follows $$ k[x] \rightarrow k[x]/(x-a_0) \rightarrow k \subseteq k[x].$$ Hence the induced map $k[x]\rightarrow A_f \rightarrow A$ is the ""constant map"": The induced map of affine schemes $$ D(f) \rightarrow X$$ identifies $D(f) \cong \{(x-a_0)\} \subseteq X$ as a topological space with one point which contradicts the assumption that $D(f) \subseteq X$ is an open subscheme. Hence for the affine line there are no such strict open subschemes. Can you construct such a strict open subscheme $U \subsetneq \mathbb{A}^2_k$ for the affine plane? What about a scheme $X$ of finite type over a field $k$ ? Note: I post this question to set theorists and algebraic geometers - I want response from both groups of people.","['algebraic-geometry', 'abstract-algebra', 'ringed-spaces', 'elementary-set-theory', 'commutative-algebra']"
4154927,2nd order ODE $y''+2(\tan t)y'-y=0$,"Let $I=(-\pi/2, \pi/2)$ Solve $y''+2(\tan t)y'-y=0$ with the initial conditions $y(0)=2, y'(0)=1$ So far I've tried two methods. The first method is what we saw in class: if you have an evident solution $\varphi$ , then we can find the solution $\phi(t)=z(t)\varphi(t)$ , where $z$ is a function such that $z'\varphi^2 = e^{-\int_{t_0}^t a(s)ds} $ . In this case $a(s)=2\tan(s)$ . But what could $\varphi$ be? I can't seem to see any ""evident"" solutions. I've tried $\sin t$ and $\cos t$ and I'm out of ideas because nothing seems to work. The second method I've tried is transforming this into a first order ODE system. So we would have $$x'=-2(\tan t )x+y$$ $$y'=x$$ But every time I try solving it using eigenvalues, the solution I get doesn't work.","['ordinary-differential-equations', 'real-analysis']"
4154954,Absolute value in definite integral. My answer differs from wolfram alphas. Why?,"I'm in highschool and just started integration. I found the following problem in an old question paper and I find it very challenging. $$\int_0^1\sqrt{x^2-2x+1} dx$$ so I simplified it algebraically to $$\int_0^1\sqrt{(x-1)^2} dx$$ which of course is $$\int_0^1|x-1| dx$$ as the absolute value is a linear function over $x \in [0,\infty)$ so I proceed to evaluate it as $x^2/2-x$ for upper limit $1$ and lower limit $0$ which is $((1)^2/2 -1)-(0-0)$ and equals $-\frac12$ , but according to wolfram alpha it is $\frac12$ . link to wolframalpha's computation Please explain at beginner level.","['integration', 'calculus', 'absolute-value']"
4154961,What are the conditions for a system of equations to have a solution in some abelian group extension?,"Given an abelian group $G$ and a system of equations $$a_1 = n^1_1 y_1 + ... + n^1_r y_r$$ $$...$$ $$a_s = n^s_1 y_1 + ... + n^s_r y_r$$ for $a_i \in G$ and $n^i_j \in \mathbb{Z}_{\geq 0}$ , under what conditions does there exist a group containing $G$ as a subgroup in which this system is solvable? In particular, can the conditions be stated in the first-order logic of group theory? For example, a system $$a_1 = y_1$$ $$a_2 = y_1$$ is solvable in an extension if and only if $a_1 = a_2$ , so its conditions can be stated in first-order logic.
Furthermore, any single equation $a = n_1 y_1 + ... + n_r y_r$ with at least one $n_i \neq 0$ can be solved in some extension of any group, since in $H = (G \times \mathbb{Z}) / \langle (a, -n_i)\rangle $ we have $y_i = (0, 1)$ as a solution, with $G$ embedding into it by $\alpha: G \rightarrow H: g \mapsto (g, 0)$ , as described by this answer","['systems-of-equations', 'first-order-logic', 'model-theory', 'abstract-algebra', 'abelian-groups']"
4155051,Are there surfaces with radial curvature but no radial symmetry?,"Let $S$ be a surface, and let $g$ be a smooth Riemannian metric on $S$ . Let $p \in S$ , and set $r(x)=d_g(p,x)$ . Suppose that the curvature of $S$ at a point $q \in S$ depends only on $r(q)$ , at least for points sufficiently close to $p$ . Does there always exist a coordinate $\theta$ defined on some punctured ball $U=B_r(p) \setminus\{p\}$ such that on $U$ , $g$ is given by $$g=dr^2+q^2(r)d\theta^2. \tag{1}$$ I am quite certain we can always ""complete"" $r$ to an orthogonal coordinate system $(r,\theta)$ , i.e. find $\theta$ such that $$g=dr^2+q^2(r,\theta)d\theta^2. \tag{2}$$ A computation gives $$
K(r,\theta)=-\frac{q_{rr}}{q},
$$ which should be independent of $\theta$ . ( $K$ is the curvature of $S$ ). Finding a coordinate $\tilde \theta$ and $\tilde q(r)$ such that $$g=dr^2+\tilde q^2(r)d\tilde \theta^2, $$ is equivalent to solving $$
q(r,\theta)d\theta=\tilde q(r)d\tilde \theta.
$$ Taking the exterior derivative, we get $$
q_r dr \wedge d\theta=\frac{\tilde q_r q}{\tilde q}dr \wedge d\theta,
$$ so $$
q_r=\frac{\tilde q_r q}{\tilde q}=F(r)q, \tag{3}
$$ where $F(r):=\frac{\tilde q_r}{\tilde q}$ . Differentiating this, we get $$
q_{rr}=F'(r)q+F(r)q_r,
$$ so finally $$
-K(r)=\frac{q_{rr}}{q}=F'(r)+F^2(r). \tag{4}
$$ So, to construct such $\tilde q$ , we first need to find a solution $F$ to the Riccati type equation $(4)$ , and then check whether equation $(3)$ holds. This seems to be only the starting point...","['surfaces', 'riemannian-geometry', 'curvature', 'symmetry', 'differential-geometry']"
4155062,The number of positive integral values taken by $f(x)=(x^3-11^3)^{1/3}$ over natural numbers $x$,"For $f:{\Bbb{N}}\to \mathbb R$ , $f(x)=(x^3-11^3)^{1/3}$ , what is the number of positive integral values $f(x)$ can take? The first thing that came into my mind was just simply trial and error but I soon realised that it wasn't going to come handy here.   I thought maybe factoring $$(x^3-11^3)= (x-11)(x^2 + 11x + 121)$$ might give some leads to this but I just couldn't figure something out from this. How do I approach this? I believe there might be some kind of specific theorem or rule that helps to solve these kind of problems that I don't know of, because I just can't figure out how to start in the first place.","['number-theory', 'functions']"
4155080,"Step question, locus of points where angle of elevation to tops of flagpoles is always the same","The smooth and level parade ground of the First Ruritanian Infantry Divison is ornamented by two tall vertical flagpole of heights $h_1$ and $h_2$ a distance d apart. As part of an initiative test a soldier has to march in a such a way that he keeps the angles of elevation of the tops of the two flagpoles equal to one another. Show that if the two flag poles are of different heights he will march in a circle. To celebrate the King's birthday a third flagpole is added. Soldiers are then assigned to each of the three different pairs of flagpoles and are told to march in such a way that they always keep the tops of their two assigned flagpoles at equal angles of elevation to one another. Show that, if the three flagpoles have different heights $h_1, h_2$ and $h_3$ and the circles in which the soldiers march have centres at $(x_{ij},y_{ij})$ (for the flagpoles of height $h_i$ and $h_j$ ) relative to Cartesian coordinates fixed in the parade ground, then $x_{ij}$ satisfy $$h_3^2(h_1^2-h_2^2)x_{12}+h_1^2(h_2^2-h_3^2)x_{23}+h_2^2(h_3^2-h_1^2)x_{31}=0.$$ Deduce that the three centres lie in a straight line. The question is much shorter than it looks. The first part is easy, by drawing a quick diagram you get that, with coordinates relative to one of the flagpoles and both of flags on the $y$ -axis, $$(h_1^2-h_2^2)x^2+(h_1^2-h_2^2)y^2-2h_1^2dx+h_1^2d^2=0$$ as the locus of the soldiers. For the second part I'm pretty sure you just have to use this equation three times. Completing the square gives the centre of the circle having coordinates $\left(\frac{dh_1^2}{h_1^2-h_2^2},0\right)$ i.e. a distance $\frac{d_{12}h_1^2}{h_1^2-h_2^2}$ along the line connecting pole 1 and 2 where $d_{12}$ is the distance between them(and analogous equations can be written down the other pairs of poles). However, this isn't really $x_{12}$ due to our choice of axes, we can only say, in terms of vectors $$(x_{12},y_{12})=X_1+\frac{dh_1^2}{h_1^2-h_2^2}{(X_2-X_1)}$$ where $X_1$ is the absolute position vector of the first flag pole, $X_2$ of the second. Writing down the equations of this form for the other circles gives a nasty system of vector equations, which you could probably solve to eliminate $X_1,X_2,X_3$ , but have no hope of getting rid of $d_{12}$ et al. and is no way the question is supposed to be approached. There is a ""model"" answer here if this has some more details that may be useful to you, but it is totally unintelligible to me. I would like to know how to proceed in generalising the first result as is required, to get the required answer. The question is question 7 here should it be needed. EDIT:
I have solved the question in the manner set out above(thanks to a comment below), I will let the bounty go to the most upvoted answer(of which all so far have been wonderful) in a couple of days","['contest-math', 'analytic-geometry', 'locus', 'geometry', 'algebra-precalculus']"
4155092,How to find the general term of the following series?,"Let $\alpha_1$ , $\alpha_2$ , $\alpha_3$ ...... $\alpha_n$ form a real sequence in the following manner $\tan(\alpha_{n+1})=\tan(\alpha_{n})+\sqrt{1+\tan^2(\alpha_n)}$ . Then find the genreal term $\alpha_n$ if $\alpha_1=60^{\circ}$ Following are my steps: $$\tan(\alpha_{n+1})=\tan(\alpha_{n})+\sqrt{1+\tan^2(\alpha_n)}$$ $$\implies \tan(\alpha_{n+1})=\tan(\alpha_{n})+\sqrt{\sec^2(\alpha_n)}$$ $$ \tan(\alpha_{n+1})=\tan(\alpha_{n})+\sec(\alpha_n)$$ $$ \tan(\alpha_{n+1})- \tan(\alpha_{n})=\frac{1}{\cos(\alpha_n)}$$ $$ \frac{\sin(\alpha_{n+1})}{\cos(\alpha_{n+1})}-\frac{\sin(\alpha_{n})}{\cos(\alpha_{n})}=\frac{1}{\cos(\alpha_n)}$$ $$\implies \frac{\sin(\alpha_{n+1}).\cos(\alpha_n)-\sin(\alpha_{n}).\cos(\alpha_{n+1})}{\cos(\alpha_{n+1}).\cos(\alpha_n)}=\frac{1}{\cos(\alpha_n)}$$ $$\frac{\sin(\alpha_{n+1}-\alpha_n)}{\cos(\alpha_{n+1})}=1$$ $$\sin(\alpha_{n+1}-\alpha_n)=\sin(90^{\circ}-\alpha_{n+1})$$ $$ \alpha_{n+1}-\alpha_n=\frac{\pi}{2}-\alpha_{n+1}\implies 2\alpha_{n+1}=\alpha_n+90^{\circ}$$ Now from the above obtained relation I can conclude that the angle $\alpha$ is forming a series  similar to Geometric Progression, but because of that $90^{\circ}$ present along with it, I am unable to proceed any further. Please help The ans is $\alpha_n=90^{\circ}-\frac{30^{\circ}}{2^{n-1}}$","['trigonometry', 'sequences-and-series']"
4155098,If $T:\mathbb R^k\to \mathbb R^k$ is linear and $\dim\operatorname{range}T < k$ then $m(\operatorname{range}T) = 0$,"If $T:\mathbb R^k\to \mathbb R^k$ is linear and the range of $T$ is a subspace of lower dimension, i.e. $\dim\operatorname{range}T < k$ then prove that $m(\operatorname{range}T) = 0$ , where $m$ is the Lebesgue measure on $\mathbb R^k$ . Rudin uses this in his proof of Theorem 2.20(e) of Real and Complex Analysis . To prove this, I have two ideas in mind, but I haven't been able to complete any one of them. Following is my work, which I request you to help me complete (or suggest alternatives): Attempt $1$ (Induction): I am trying to do induction on $k$ . The base case $k=1$ is trivial since the only subspace of $\mathbb R$ of lower dimension is $\{0\}$ and $m(\{0\}) = 0$ . Assume the statement holds for all $k < n$ . To complete the induction step, we must prove it for $k = n$ . Assume $\dim\operatorname{range}T < n$ for some linear $T:\mathbb R^n\to\mathbb R^n$ . What do I do next? Attempt $2$ (Direct Proof): I feel we can take arbitrarily thin sets (in the sense that their measure is small) which enclose the subspace $\operatorname{range}T$ . By monotonicity of the Lebesgue measure, we should be able to arrange $m(\operatorname{range}T) < \epsilon$ for every $\epsilon > 0$ , implying $m(\operatorname{range}T) = 0$ . Some vague thoughts are about using cosets - which are essentially parallely-shifted copies of the subspace in consideration. Attempt $3$ (Inner regularity of $m$ ): Let $Y = \operatorname{range} T$ . It suffices to show that $m(K) = 0$ for every compact $K\subset Y$ . Translation invariance of the Lebesgue measure has already been established (but not rotation yet, don't use that). Suppose $K\subset Y$ is compact. I want to cover $K$ by $\epsilon$ -balls of full dimension, and use a monotonicity argument to deduce $m(K) = 0$ . This seems hard to do without information on the exact form of $K$ (or $Y$ ). Thank you!","['measure-theory', 'lebesgue-measure', 'linear-algebra', 'real-analysis']"
4155130,Intuition for limit not existing,"Consider the function: For $x \neq 1$ , $f(x) = 2(x-1)\cos(\frac{1}{x-1}) + \sin(\frac{1}{x-1})$ ,
and for $x = 1$ , $f(x) = 0$ . Why does the limit of $f(x)$ as $x \to 1$ not exist intuitively speaking? How can one tell?","['limits', 'intuition', 'real-analysis']"
4155188,How to show the total number of real solutions of the equation $x^4-2x^2+4=n$ for each set of real values of $n$ using differentiation?,"How to show the total number of real solutions of the equation $x^4-2x^2+4=n$ for each set of real values of $n$ using differentiation? Trying to solve this, I converted the given equation as the following function $f(x) = x^4-2x^2+4-n$ Then I solve the derivative which will help me determine the critical numbers for x. $f'(x) = 4x^3-4x = 4x(x^2-1)$ Hence, the critical numbers
will be $x = 0, \pm 1$ This means that I can identify the relative extremums. The relative maximum will be $(0, 4-n)$ The relative minimums are $(\pm1, 3-n)$ But I don't know how can I use those relative extremums to show the following which I found using a graphing tool. $n < 3$ implies no solution $n = 3$ implies $2$ solutions $3 < n < 4$ implies $4$ solutions $n = 4$ implies $3$ solutions $n > 4$ implies $2$ solutions How can I use, the local extremums or other concepts under differentiation to find the above results? If you have strategies other than derivatives, please share them here. Please feel free to share your ideas, comments, and suggestions about this matter. Thank you in advance!","['solution-verification', 'derivatives']"
4155191,Computing the total variation for a multivariable function,"I am trying to write an example computation with multivariable total variation to include in my functional analysis notes using the following definition from Wikipedia : Let $\Omega$ be an open subset of $\mathbb{R}^n$ . Given a function $f$ belonging to $L^1(\Omega)$ , the total variation of $f$ in $\Omega$ is defined as $$V(f,\Omega):=\sup \left \{ \int_\Omega f(x) \text{div} \phi(x) dx : \phi \in C_c^1(\Omega, \mathbb{R}^n), \|\phi\|_{L^\infty(\Omega)} \leq 1\right\}$$ $C_c^1(\Omega, \mathbb{R}^n)$ is the set of continuously differentiable vector functions of compact support contained in $\Omega$ , $\|\cdot\|_{L^\infty(\Omega)}$ is the essential supremum norm, and $\text{div}$ is the divergence operator. I would like to use this formula directly and demonstrate the process of taking the supremum. I understand that if $f$ is $C^1$ on $\overline \Omega$ , then the formula for total variation is simplified to the computation of $$
V(f, \Omega) = \int_\Omega |\nabla f(x)| dx,
$$ which I am not trying to use here. So, the problem is computing the total variation of $f: \mathbb{R}^2 \to \mathbb{R}$ , $$
f(x,y) =\frac{xy}{x^2 + y^2}
$$ on $\Omega$ , where $\Omega$ is the open unit disk in $\mathbb{R}^2$ . So, $\Omega = \{x : x \in \mathbb{R}^2 \text{ and }\|x\| < 1\}.$ Here is a visual of this situation: My Attempted Solution . $f$ is a classic example of a function discontinuous at 0, so $f \notin C^1(\overline \Omega)$ . We first show that $f \in L^1(\Omega)$ . Recall $f \in L^1(\Omega) \iff \int_\Omega |f| < \infty$ . So, $$\begin{align} \int_{\Omega} |f| &= \iint_D |f(x,y)| dA \\ &= \int_{-1}^1 \int_{-\sqrt{1 - x^2}}^{\sqrt{1 - x^2}} \left \lvert \frac{xy}{x^2 + y^2}\right \rvert dy dx \\ &= 1 < \infty \qquad \text{(C.A.S)}\end{align}.$$ Hence, $f \in L^1(\Omega)$ . It remains to compute $$V(f,\Omega):=\sup \left \{ \int_\Omega f(x,y) \text{div} \phi(x,y) dx : \phi \in C_c^1(\Omega, \mathbb{R}^2), \|\phi\|_{L^\infty(\Omega)} \leq 1\right\}$$ Before taking the supremum over $\phi \in C_c^1(\Omega, \mathbb{R}^2)$ , we attempt the following simplification, $$\begin{align*}
				\int_\Omega f(x,y) \text{div} \phi(x,y) dx &= \iint_D f(x,y) \left(\nabla \cdot \left(\phi_x, \phi_y\right)\right) dA \\
				&= \iint_D f(x,y) \left(\frac{\partial \phi_x}{\partial x} + \frac{\partial \phi_y}{\partial y}\right) dA \\
				&= \iint_D f(x,y) \frac{\partial \phi_x}{\partial x} dA + \iint_D f(x,y) \frac{\partial \phi_y}{\partial y} dA
			\end{align*} $$ This is where I am stuck in terms of working with $\phi(x,y)$ . It boils down to two main questions: Given what we know about $\phi$ , can the above be simplified any further to an expression that does not depend on $\phi$ ? (So that we don't have to take a supremum) Otherwise, If we do have to take a supremum over the vector fields $\phi$ , how would one go about this? I understand in the single variable case of total variation, one is simply constructing a family of partitions on an interval that enable you to take the supremum over all partitions. But here, how would I go about constructing a family of vector fields that enable me to take the supremum required?","['total-variation', 'functional-analysis']"
4155201,Binomial theorem and sum from $k = 0$ to $N/2$,"While calculating a partition function in physics, I stumbled across a weird sum, namely: $$Z=\sum_{k=0}^{N/2}{N \choose 2k}b^{N-k}e^{ak}$$ Wolfram says that it's equal to: $$Z = \frac{1}{2}\left((b-\sqrt{b}e^{a/2})^N+(b+\sqrt{b}e^{a/2})^N\right)=\frac{(x-y)^N+(x+y)^N}{2}$$ But I dont understand how we can link it to the binomial theorem since the coefficient is: ${N \choose 2k}$ and not ${N/2 \choose 2k}$ and then prove the formula.","['summation', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'binomial-theorem']"
4155232,"Is the space $\text{Map}(S^1,S^1)$ of continuous maps on $S^1$ compact? (Compact-open topology)","Is the space $\text{Map}(S^1,S^1)$ of continuous maps $S^1\to S^1$ compact? Here $\text{Map}(S^1,S^1)$ has the compact-open topology. I'm not too savvy with the compact-open topology so I'm not sure where to really begin. I also tried searching this question and couldn't find it on here. I know that $\text{Map}(S^1,S^1)$ has a prebasis (subbasis) given by all $$
S(K,U):=\{f:S^1\to S^1\mid f(K)\subseteq U\}
$$ where $K\subseteq S^1$ is compact and $U\subseteq S^1$ is open. But I don't know how to relate this to the compactness of $\text{Map}(S^1,S^1)$ . Any hints?","['general-topology', 'compactness']"
4155296,$d\omega \wedge \omega=0$ implies $d(f\omega)=0$,"I would like to prove the following ( without the Frobenius Theorem ): On $\mathbb{R}^n$ , if $\omega$ is nowhere vanishing $1$ -form such that $d\omega\wedge\omega=0$ then there exists (at least locally) $f$ a positive function such that $d(f\omega)=0$ . In fact, I am looking for an easier proof of the Frobenius theorem for distribution of hyperplanes. Indeed the proof I know are inductive, we start with distribution of lines, then inductively we arrive to distribution of hyperplanes. My statement implies the Frobenius theorem for hyperplanes, so I wonder if we can prove it with as less as technology as possible? Thanks for your help Added: It is easy to prove that there exists a 1-form $\theta$ such that $d\omega =\theta \wedge \omega$ , then $d(f\omega)=(df+f\theta)\wedge \omega$ , so the problem can be rephrased as, does there exisst $f$ and $g$ such that $df+f\theta =g \omega$ ? I stuck here.",['differential-geometry']
4155308,Quick Question: using expectation as a scalar,"I'd like to ask a question about possible rules of expectation when we can and can't use them ""as if"" they were scalars. Given Random Variables: $X_i \underset{iid}{\sim} z$ forming a random sample of z where z is some distribution and two given functions f and g. lets define $\mathbb{E}_{g}[f(X)]=\int_{-\infty}^{\infty} f(X)g(X)~dx$ as the weighted expectation of f wrt to g. From my understanding this is a number... it has a value and is not a ""random variable"" (even though i know constants are technically random variables with constant value)... so if we were to calculate for example: $$Var[h(X)] = \sum\limits_{1}^{n}Var_{g}[\mathbb{E}_{g}[f(X_i)]w(X_i)]$$ In this instance, would it be correct to consider the expectation as if it's acting as a scalar (since it's value is constant for each defined observation) and so we could rewrite this as $$\sum\limits_{1}^{n}\mathbb{E}_{g}[f(X_i)]Var_{g}[w(X_i)]$$ where the $Var_g$ is defined analogously to $\mathbb{E}_{g}$ Thanks.","['statistics', 'monte-carlo', 'probability']"
4155317,Is there any deep relation between lens space and number theory?,"It writes on Wiki that ""...(lens space) were the first known examples of 3-manifolds which were not determined by their homology and fundamental group alone."" And actually we have known the complete classfication of the lens space $L(p,q)$ w.r.s to the prime p, and it is obtained by the different operation of surgery. My question is that since the classification rely highly on the prime $p$ and a kind of reverseble operation due to the primeness of $p$ , is there any deep relation between lens space and number theory except for the technical 3-manifold operation? Or I wonder how a number theoriest will treat the lens space?","['number-theory', 'low-dimensional-topology', 'algebraic-topology']"
4155332,Is it possible for a set to contain an element that does not have the defining property?,"Let $A:=\{x \ | \ \Phi(x)\}$ be a set and $a,b$ be objects such that $a=b$ and $b$ satisfies $\Phi(b)$ but $a$ does not satisfy $\Phi(a)$ . Is it true that $a \in A$ ? A more concrete example would be $$B:=\{e^{2\pi ix} \ | \ x \in [0,1)\}$$ and $a:=1,b:=e^{2\pi i}$ (or any other integer multiple of $2\pi ix$ ). It then holds that $a \in B$ and thus by $b=a$ it should also hold that $b \in B$ . However, is this conflicting with the concept of the defining property? I assume it is not because the set contains all the objects that have the value of any expression $e^{2 \pi i x}$ with $x \in [0,1)$ , thus also containing $b$ , however I am not sure since this might be some philosophical or set theoretic question that I am not familiar with and also touches on the question ""what equality really is"". Any comment is greatly appreciated.","['elementary-set-theory', 'logic']"
4155337,Show the intersection of $A$ and $B$ and the relative complement of $B$ in $A$ are disjoint and their union is $A$,"I am trying to solve this problem: Show the sets $A\cap B$ and $A\setminus B$ are disjoint and their union is $A$ . So, I can intuitively understand why the first statement is true; The intersection of $A$ and $B$ contains elements that are in both $A$ and $B$ { $x$ : $x \in A$ and $x\in B$ } - The Venn Diagram of such a set would have only the overlapping portion of $A$ and $B$ shaded in. The Relative complement of $B$ in A contains the elements of $A$ that are not elements of $B$ { $x \in A$ : $x \notin B$ } - The Venn diagram of such a set would have only the $A$ circle (excluding the overlapping portion) shaded in. So the intersection of these sets would be the set of elements that are (in both $A$ and $B$ ) and (in $A$ but not $B$ ) which means if $x \in$ $A \cap B$ then $x \notin A \setminus B$ necessarily since $A \setminus B$ contains no elements of the set B by definition - inspecting the corresponding Venn diagrams we see that they have no shaded portions in common. My question for this portion is: how would I go about proving this statement using set theory notation? Would I start by assuming that $x \in A \cap B$ then go to show that this assumption leads to $x \notin A \setminus B$ ? Which would then mean the intersection was $\emptyset$ ? For the second part of the question I, again, have an intuitive understanding of why this is true: For one, inspecting the Venn diagrams we can see that combining the two diagrams leads to the entire circle for $A$ being shaded in which is interpreted as the set $A$ . Symbolically: $(A \cap B)$ $\cup$ ( $A \setminus B$ ) = { $x$ : [ $x \in A$ and $x \in B$ ] or [ $x \in A$ : $x \notin B$ ]} which allows us to select points from either set and thus reconstruct the set $A$ by combining the points only in A with the points in both $A$ and $B$ . Similar to the first point above, I'm not sure how to formally prove this statement with notation. I tried negating both statements as well to see if that led to anything but it did not help to illuminate a way forward. Any help is greatly appreciated!","['proof-explanation', 'proof-writing', 'solution-verification', 'discrete-mathematics', 'elementary-set-theory']"
4155362,Is it possible to turn this geometric demonstration of the area of a circle into a rigorous proof?,"In this New York Times article, Steven Strogatz offers the following argument for why the area of a circle is $\pi r^2$ . Suppose you divide the circle into an even number of pizza slices of equal arc length, and wedge them together in such a way that half of the slices have an arc at the bottom, and half of the slices have an arc at the top: Then, the base of the shape created has length $\pi r$ , and its height is $r$ . As the number of slices tends to infinity, the limiting case is that of a rectangle: Hence, the area of the circle is $\pi r^2$ . Although this argument is very geometrically appealing, it also seems fairly difficult to make rigorous. I suppose the most challenging part is showing that the base of the shape really does become arbitrarily flat, and its height becomes arbitrarily vertical, if that makes sense. How might we convert this intuitive argument into a rigorous proof?","['calculus', 'circles', 'geometry', 'real-analysis']"
4155365,Simple differential-difference equation [duplicate],"This question already has answers here : When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? (5 answers) Closed 3 years ago . I was doing some exam practice when I almost surely went the wrong way in my method and ended up with the following equation: $$f'(x) = f(x) - f(x-1)$$ Although not what I was intended to find, I'm still curious to know if there are methods to solve this. We can spot the obvious solution $f(x) = Ax+B$ , but I could not find a way to prove this was the only solution, or find any other solutions. Any insight would be helpful. For context I am a first-year undergrad.","['delay-differential-equations', 'derivatives', 'recurrence-relations']"
4155431,Set of points at which $f$ is continuous is a Borel set,"I have proved the following statement(s) and I would like to know if my proof is correct and/or/how it could be improved, thank you. Suppose $f:\mathbb{R}\to\mathbb{R}$ is a function. (a) For $k\in\mathbb{Z^+}$ , let $G_k=\{a\in\mathbb{R}:\text{there exists }\delta>0\text{ such that }|f(b)-f(c)|<\frac{1}{k}\text{ for all }b,c\in (a-\delta,a+\delta)\}$ . Prove that $G_k$ is an open set for each $k\in\mathbb{Z^+}$ ; (b) Prove that the set of points at which $f$ is continuous equals $\bigcap_{k=1}^{\infty}G_k$ (c) Conclude that the set of points at which $f$ is continuous is a Borel set. (a) Let $k\in\mathbb{Z^+}$ and $\alpha\in G_k$ : then there exists $\delta>0$ such that $|f(b)-f(c)|<\frac{1}{k}$ for all $b,c\in (\alpha-\delta,\alpha+\delta)$ . Now, let $\delta_{\alpha}:=\frac{\delta}{4}$ and take $\beta\in B_{\delta_{\alpha}}(\alpha):=(\alpha-\delta_{\alpha},\alpha+\delta_{\alpha})\subset (\alpha-\delta,\alpha+\delta)$ : if we let $\delta_{\beta}:=\frac{\delta_{\alpha}}{4}$ and take $b,c\in B_{\delta_{\beta}}(\beta):=(\beta-\delta_{\beta},\beta+\delta_\beta)\subset B_{\delta_{\alpha}}(\alpha)$ we have that $|f(b)-f(c)|<\frac{1}{k}$ thus $B_{\delta_\alpha}(\alpha)\subset G_k$ ie we have found an open ball centered in $\alpha$ contained in $G_k$ which is thus open, as desired. (b) Let $x_0\in\mathbb{R}$ be a point where $f$ is continuous and $k\in\mathbb{Z^+}$ : then by definition of continuous function for every $\varepsilon>0$ there exists $\delta_{\varepsilon}>0$ such that $|f(x)-f(x_0)|<\varepsilon$ for all $x\in (x_0-\delta_{\varepsilon},x_0+\delta_{\varepsilon})$ so in particular (since $\frac{1}{k}>0\ \forall k\in\mathbb{Z^+}$ ) for every $k\in\mathbb{Z^+}$ there exists $\delta_{k}>0$ such that $|f(x)-f(x_0)|<\frac{1}{2k}$ for all $x\in (x_0-\delta_{k},x_0+\delta_{k})$ and if we take $b,c\in(x_0-\delta_{k},x_0+\delta_k)$ we have that $|f(b)-f(a)|=|f(b)-f(x_0)+f(x_0)-f(a)|\leq |f(b)-f(x_0)|+|f(x_0)-f(a)|<\frac{1}{2k}+\frac{1}{2k}=\frac{1}{k}$ so $x_0\in G_k$ for every $k\in\mathbb{Z^+}$ thus $x\in\bigcap_{k=1}^{\infty}G_k$ . Now let $x_0\in\bigcap_{k=1}^{\infty} G_k$ and $\varepsilon>0$ : then $x\in G_k$ with $k>\frac{1}{2\varepsilon}$ so there exists $\delta_k$ such that $|f(b)-f(c)|<\frac{1}{2k}<\varepsilon$ for all $b,c\in (x_0-\delta_k,x_0+\delta_k)$ so it is also $|f(b)-f(x_0)|=|f(b)-f(c)+f(c)-f(x_0)|\leq |f(b)-f(c)|+|f(c)-f(x_0)|<\frac{1}{2k}+\frac{1}{2k}=\frac{1}{k}<\varepsilon$ for all $b\in (x_0-\delta_k,x_0+\delta_k)$ so $f$ is continuous at $x_0$ . Thus $\{x\in\mathbb{R}:f\text{ is continuous at }x\}=\bigcap_{k=1}^{\infty}G_k$ . (c) Since from point (b) we know that $\{x\in\mathbb{R}:f\text{ is continuous at }x\}=\bigcap_{k=1}^{\infty}G_k$ and from point (a) that the $G_k$ s are open sets in $\mathbb{R}$ hence Borel sets, since $\sigma$ -algebras like the set $\mathcal{B}$ of Borel sets are closed under intersections, we have that $\bigcap_{k=1}^{\infty} G_k$ is a Borel set hence $\{x\in\mathbb{R}:f\text{ is continuous at }x\}$ is a Borel set too.","['borel-sets', 'measure-theory', 'solution-verification', 'real-analysis']"
4155443,Binomial coefficients identity with cases,"Let $i,j,k$ be non-negative integers such that $i$ is even, $j \leq \frac{i}{2}$ , and $k < i$ . I would like to show the following identity: $$\binom{i-j+k}{k} - \sum_{\ell = 0}^{\lfloor\frac{k}{2}\rfloor} \frac{i-2j+k}{\frac{i}{2}-j+k-\ell} \binom{\frac{i}{2}+\ell}{2\ell} \binom{\frac{i}{2}-j+k-\ell}{k-2\ell} = 
\begin{cases} 0 & \text{if }j < k \\ (-1)^{k+1}\binom{j}{k} & \text{if } j \geq k \end{cases}.$$ We can change the summation into $$\sum_{\ell=0}^{\lfloor\frac{k}{2}\rfloor}\binom{\frac{i}{2} + \ell}{2\ell}\left(\binom{\frac{i}{2}-j+k-\ell}{k-2\ell} + \binom{\frac{i}{2}-j+k-\ell-1}{k-2\ell} \right)$$ but I don't know of any identities that could help here. I've also tried induction on $k$ , but couldn't see a good way for the induction hypothesis to be used. I would appreciate any ideas!","['binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4155444,Bounded operator mapping onto a subspace,"Let $X$ and $Y$ be Banach space and $W\subseteq Y$ be a subspace. Let $T_1:X\to Y$ be bounded and linear such that $T_1(X)=W$ and $T_2:Y\to Y$ be bounded. Does there exist a Banach space $Z$ and a surjective bounded operator $T:Z\to\{y\in Y: T_2y\in W\}$ ? My attempt: If $T_2$ is bijective, then I know the answer is yes. I couldn't answer the general case. I considered example of shift and projection operators on $l^\infty$ and could always find a bounded operator but I couldn't generalize it. Edit: I think I can even handle the case when range of $T_2$ is closed, but I still can't handle the general case.","['operator-theory', 'functional-analysis']"
4155453,Limit of the series $\sum_{k=1}^\infty \frac{n}{n^2+k^2}.$,"I am trying to evaluate $$\lim_{n\to \infty} \sum_{k=1}^\infty \frac{n}{n^2+k^2}.$$ Now I am aware that clearly $$\lim_{n\to \infty} \sum_{k=1}^n \frac{n}{n^2+k^2} = \int_0^1 \frac{1}{1+x^2}dx = \tan^{-1}(1) = \frac{\pi}{4},$$ but I do not know what to do if each sum is already sent to infinity. Im taking a limit of limits. I suppose I could rewrite my limit as $$\lim_{n\to \infty} \lim_{m\to \infty} \sum_{k=1}^m \frac{n}{n^2+k^2}?$$ But I am unaware if this is helpful at all. Any hints would be appreciated. Obviously, Wolfram calculates this as $\frac{\pi}{2}$ but I am unaware of the steps and logic to get there.","['calculus', 'sequences-and-series', 'summation', 'real-analysis']"
4155460,What kind of mathematical spiral does the volute of an Ionic capital approximate?,"Ionic is one of the Classical Orders of architecture. A capital is the top portion of a column. The Ionic capital looks like this: The spirals on either side are the volutes. Various architects have given in-depth descriptions of how to draw these spirals, including Vignola in 1562 and Gibbs in 1753. In essence, their method looks like this: Starting with a vertical line AB divided into 8 parts. Take 7 of these parts to create a horizontal line AC in order to describe a grid: The eye is a circle whose diameter corresponds to one of these parts, and is placed as indicated above. A square rotated 45° is inscribed within the eye. It is bisected across the midpoints of its edges, and these bisectors are further divided into six equal parts: Here is how you draw the spiral: You start with point 1 as the center of the circle, and follow it up to the top of the grid to get the initial radius. You draw this circle until it intersects the line 2. Then you move the center to point 2, and draw until you hit line 3. Then you move the center to line 3 and draw until you hit line 4. This is repeated until your spiral intersects with the circle of the eye. If that description wasn't sufficiently helpful, here is a video demonstrating (a slightly different version of) this technique. My question is the following:
Is this method of drawing a spiral meant to approximate a specific kind of mathematical spiral? I know it's not Archimedean, but there are a bunch of other types of spiral. Which one of them is closest to this?",['geometry']
4155537,What is the number of possible simple directed graphs of $n$ elements without 2-cycles and self-loops?,"Let $A$ be the adjacency matrix of a graph with $n$ vertices. Let $a_{ij}$ denote the entry in the $i$ -th row and $j$ -th column. For a given $n$ , how can we compute the number of possible networks, $f(n)$ , such that There are no self-loops: $a_{ii}=0$ There are no 2-cycles: $a_{ij}=1\implies a_{ji}=0$ there is at most one edge between two vertices: $a_{ij}\in\{0,1\}$ To get some intuition, I wrote a script that generates all such networks for $n=3,4,5$ and got $27,729,59049$ possible networks respectively. I did some reverse engineering and got that $$f(n)=3^{\frac{n(n-1)}{2}}$$ However, I don't understand why is this the case. It is possible, of course, that the function I deduced is wrong and that it only works for those three integers. Can someone shed some light on whether this formula is correct and why is this the case?","['network', 'graph-theory', 'adjacency-matrix', 'combinatorics', 'discrete-mathematics']"
4155543,Why is the topology on the Sorgenfrey line not second countable?,"For context, let me clarify some things. Our set is $\mathbb{R}$ . The topology on $\mathbb{R}$ is the topology generated by the arbitrary unions of closed-open intervals $[a,b)$ with $a,b \in \mathbb{R}$ . I've been told that this is a classic example of a topological space that is separable but not second countable. What is it exactly that makes this space not second countable? Also, just so I can garner a better understanding, how could we change $[a,b)$ so that it becomes second countable? Is it the closed bracket that's causing the problem or the open bracket? It's hard to tell. I think it's the closed bracket because if we had two open brackets, that would just be the natural topology on $\mathbb{R}$ which is second countable. So it must be the closed bracket that throws us off. I'm just not understanding which part of the lower limit topology violates the definition of second countable.","['sorgenfrey-line', 'general-topology', 'second-countable']"
4155558,"Constructing a ""Lebesgue-Stieltjes"" measure in $\mathbb R^d$","Given $x, y \in \mathbb R^d$ , write $x \leq y$ iff $x_i \leq y_i$ for all $i$ . We call a function $F : \mathbb R^d \to \mathbb R$ monotone increasing if $F(b) \geq F(a)$ whenever $b \geq a$ and right-continuous if $F(x_n) \to F(x)$ whenever $x_n \to x$ in $\mathbb R^d$ and $x_n \geq x_{n+1}$ for all $n$ . Also, say that a monotone increasing, right-continuous function $F$ vanishes at $-\infty$ if $$
F(-\infty) := \lim_{x_1 \to -\infty} \cdots \lim_{x_d \to -\infty} F(x) = 0.
$$ Also, we use the following notation: $$
(a,b] := \prod_{i=1}^d (a_i, b_i] \quad \textrm{and} \quad (-\infty, b] := \prod_{i=1}^d (-\infty, b_i] \quad \textrm{for } a = (a_1, \ldots, a_d), \: b=(b_1, \ldots, b_d) \in \mathbb R^d.
$$ My question: Suppose $F : \mathbb R^d \to \mathbb R$ is monotone increasing, right-continuous, and vanishes at $-\infty$ . Let $\mathcal A_0 = \left\{(-\infty, b] : b \in \mathbb R^d\right\}$ , and define the set function $\mu : \mathcal A_0 \to [0,\infty)$ by $\mu((-\infty, b]) = F(b)$ . Under what conditions does $\mu$ extend to a measure on $\mathcal B(\mathbb R^d)$ ? The obvious answer would involve the measure extension theorem: Let $\mathcal A$ be a semiring and let $\mu : \mathcal A \to [0,\infty]$ be an additive, $\sigma$ -subadditive and $\sigma$ -finite set function with $\mu(\emptyset) = 0$ . Then there is a unique $\sigma$ -finite measure $\tilde\mu : \sigma(A) \to [0,\infty]$ such that $\tilde\mu(A) = \mu(A)$ for all $A \in \mathcal A$ . The collection $\mathcal A_0$ isn't a semiring, but the collection $\mathcal A := \left\{(a,b] : a, b \in \mathbb R^d\right\}$ is. In dimension $d=1$ , this is the idea behind Lebesgue-Stieltjes measures. In higher dimensions, it's easy to inductively extend $\mu$ to $\mathcal A$ so that $\mu : \mathcal A \to \mathbb R$ is additive. Obviously $\mu$ is $\sigma$ -finite on $\mathcal A$ , and $\mu(\emptyset) = 0$ . But the remaining properties are giving me trouble. In general, $\mu$ constructed from a monotone increasing, right-continuous $F$ need not even be positive. Take $F : \mathbb R^2 \to \mathbb R$ so that $F(x,y) = 0$ if $x<0$ or $y<0$ , and $F(x,y) = \arctan(x+y)$ otherwise. Then $F$ is right-continuous and monotone increasing, but if we let $\mu((-\infty, b]) = F(b_1, b_2)$ , then the additive extension of $\mu$ to $\mathcal A \subset \mathcal B(\mathbb R^2)$ isn't positive: \begin{align*}
\mu\big((0,1] \times (0,1]\big) &= \mu\big((0,1] \times (-\infty, 1]\big) - \mu\big((0,1] \times (-\infty, 0]\big) \\
&= \mu\big((-\infty, 1] \times (-\infty, 1]\big) - \mu\big((-\infty, 0] \times (-\infty, 1]\big) - \mu\big((-\infty, 1] \times (-\infty, 0]\big) + \mu\big((-\infty, 0] \times (-\infty, 0]\big) \\
&= \arctan(2) - 2\arctan(1) < 0.
\end{align*} As for $\sigma$ -additivity, I only know how to show this if $\mu((a,b])$ is monotone increasing in $b$ and monotone decreasing in $a$ (which is certainly true for Lebesgue-Stieltjes measures in dimension 1, where $\mu((a,b]) = F(b) - F(a)$ ). This is similarly difficult to show in higher dimensions. One possible direction: Recently I came across the following result: Let $(F_n)_{n \geq 1}$ be a uniformly bounded sequence of right-continuous, monotone increasing functions $F_n : \mathbb R^d \to \mathbb R$ . Then there is a right-continuous, monotone increasing, bounded function $F : \mathbb R^d \to \mathbb R$ and a subsequence $(F_{n_k})_{k \geq 1}$ for which $\displaystyle \lim_{k \to \infty} F_{n_k}(x) = F(x)$ for all $x$ where $F$ is continuous. This theorem implies in particular that if $(\mu_n)_{n \geq 1}$ is a family of sub-probability measures on $\mathbb R^d$ , then the functions $F_n(x) = \mu_n((-\infty, x])$ admit a subsequence $(F_{n_k})$ and a monotone increasing, right-continuous bounded function $F$ for which $F_{n_k}(x) \to F(x)$ if $F$ is continuous at $x$ . Since $\mu_n$ are measures and $F$ is monotone increasing, it follows that $F$ is nonnegative. And, if $(a,b] \subset \mathbb R^d$ is a half-open rectangle whose vertices are points of continuity for $F$ , the inductive definition of $\mu$ on $\mathcal A$ implies that $\mu((a,b]) = \displaystyle \lim_{k \to \infty} \mu_{n_k}((a,b]) \geq 0$ . But showing that $\mu$ is positive in general on $A$ requires again showing that $\mu((a,b])$ is increasing in $b$ and decreasing in $a$ , which I'm having trouble proving. Does anyone know of such a construction of a measure in general on $\mathbb R^d$ ? What are peoples' thoughts?","['reference-request', 'measure-theory', 'probability-theory', 'real-analysis']"
4155564,"Disproving the statement ""If $f(z)$ is not an entire function, then $g(z) = f^2(z)$ is not entire.”","If $f(z)$ is not an entire function, then $g(z) = (f(z))^2$ cannot be an entire function.” From this statement why would saying ""Let $f(z) = \sqrt z$ , assuming that it is a branch that takes $−1$ to $i$ . Then it is not analytic on the branch cut, but $g(z) = (f(z))^2 = z$ is obviously an entire function.""
Not be a proper example to disprove this statement/ be a incorrect counterexample? I understand that for $\sqrt z$ to be a incorrect counterexample means it is not analytic on the branch -1 to I, therefore not entire. However I thought $\sqrt z$ would be analytic throughout the whole branch cut from -1 to i. Since on the complex plane, $\lim_{z \to -1} \sqrt z$ exists as it would approach $i$ in that case, and in the other case $\lim_{z \to i} \sqrt z$ it approaches $\sqrt i$ meaning that at least on that branch cut in the complex plane it is continuous everywhere, therefore the partial derivatives exist everywhere on the branch cut meaning it is analytic, but this assumption seems to be incorrect. What am I missing or not understanding here. Precisely, I don't understand why $\sqrt z$ would not be a proper counterexample to the statement.","['complex-analysis', 'entire-functions', 'analytic-functions']"
4155583,"Is there a formal name for equations and inequalities containing parameters (known variables, coefficients)?","Is there a formal name for equations and inequalities containing parameters (known variables, coefficients)? Is it just ""equations with parameters""? ""general equations""? Here are some examples: $$\sqrt{4x - 3} \cdot ln(5x - a) = \sqrt{4x - 3} \cdot ln(6x + a) $$ $$ (5^{2x + 1} - 25^x - 20)(\sqrt{ax - 6} - \sqrt{a - 2x}) = 0 $$ $$ \dfrac{log_ax}{x^2 + (a - 4)x + 4 - 2a} \le 0$$ I want to learn how to solve them, but my problem I cannot find resources in English on how to do it. I'm not a native English speaker, so I suspect this is because I'm translating it wrong. The literal translation from Russian would be ""equations with a parameter"", and usually such problems are stated in the following way: ""find all the solutions of the equation depending on the value of $a$ "" or ""find all the values of $a$ for which the equation has two distinct roots"". For example, when I search for YouTube videos on how to solve such equations in Russian with the search query ""уравнения с параметром"" (""equations with a parameter""), I get tons of relevant videos of different sorts, but when I do it in English with the same search query ""equations with a parameter"", I get almost nothing relevant. It makes me think that I'm doing something wrong. Here's what I've found so far on Wikipedia: Equation Usually, the unknowns are denoted by letters at the end of the alphabet, x, y, z, w, ..., 2 while coefficients (parameters) are denoted by letters at the beginning, a, b, c, d, ... . For example, the general quadratic equation is usually written $ax^2 + bx + c = 0$ . The process of finding the solutions, or, in case of parameters, expressing the unknowns in terms of the parameters, is called solving the equation. Parameter Mathematical functions have one or more arguments that are designated in the definition by variables. A function definition can also contain parameters, but unlike variables, parameters are not listed among the arguments that the function takes. When parameters are present, the definition actually defines a whole family of functions, one for every valid set of values of the parameters. For instance, one could define a general quadratic function by declaring $f(x) = ax^2 + bx + c$ . But it doesn't really answer my question.","['algebra-precalculus', 'terminology']"
4155603,"Does naming the integers in $(\mathbb{R};+,\times)$ change the ""definable topologies"" significantly?","This is motivated by this recent question of Gregory Nisbet : For a structure $\mathcal{A}=(A;...)$ and a set $X\subseteq A$ , let $\tau_X^\mathcal{A}$ be the topology on $A$ generated by sets definable in $\mathcal{A}$ with parameters from $X$ . For example, $\tau^\mathcal{A}_A$ is always the discrete topology, although $\tau_\emptyset^\mathcal{A}$ need not be. Now say that an expansion $\mathcal{B}$ of $\mathcal{A}$ is restrained iff for each $X\subseteq A$ there is some $Y\subseteq A$ with $\tau^\mathcal{B}_X=\tau^\mathcal{A}_Y$ . Basically, a restrained expansion doesn't provide us with any new topological power except perhaps some ""parameter juggling."" I'm trying to get a sense of what the restrained expansions of a ""tame"" structure can look like, and the following seems a good starting point: Is $\mathcal{R}_\mathbb{Z}=(\mathbb{R};+,\times,\mathbb{Z})$ a restrained expansion of $\mathcal{R}=(\mathbb{R};+,\times)$ ? After some thought, my guess is that the answer is yes (so in particular a restrained expansion of an o-minimal structure need not be o-minimal). However, I don't immediately see how to prove this, the general obstacle being that $X$ is usually not definable in the expansion of $\mathcal{A}$ by constants naming the elements of $X$ . In fact it's not immediately clear to me whether $\mathcal{R}$ has any non-o-minimal restrained expansions.","['general-topology', 'logic', 'model-theory']"
4155612,Are these two definitions of positive measure equivalent?,"Let's consider the two following definitions of a positive measure : D1 : Let $\mu$ be an application from a $\sigma$ -algebra $T$ to $\overline{\mathbb{R}^+}$ . $\mu$ is called a (positive) measure if and only if : 1) $\mu(\emptyset)=0$ 2)for all countable collections $(A_n)_{n\in\mathbb{N}}$ of pairwise disjoint sets of T , $$\mu(\bigcup_{n \in \mathbb{N}}A_n)=\sum_{n \in \mathbb{N}}\mu(A_n)$$ D2 : Let $\mu$ be an application from a $\sigma$ -algebra $T$ to $\overline{\mathbb{R}^+}$ . $\mu$ is called a (positive) measure if and only if : $\mu(\emptyset)=0$ 2) $\mu$ is additive , which means for all disjoint pairs $A,B$ of T , $\mu(A\cup B)=\mu(A)+\mu(B)$ for all sequence $(A_n)_{n \in \mathbb{N}}$ of $T$ such that $A_{n+1}\subset{A_n} ,\forall n \in \mathbb{N}$ and $\mu(A_0)<\infty$ , we have $$\mu(\bigcap_{n \in \mathbb{N}} A_n)= \lim_{n \to \infty} \mu(A_n)$$ EDIT:
I add a 4th condition :
4)for all sequence $(A_n)_{n \in \mathbb{N}}$ of $T$ such that $A_{n}\subset A_{n+1} ,\forall n \in \mathbb{N}$ , we have $$\mu(\bigcup_{n \in \mathbb{N}} A_n)= \lim_{n \to \infty} \mu(A_n)$$ I think it can show that $(D_1)\implies(D_2)$ , but does $(D_2)\implies (D_1)$ holds ? Is there a proof by equivalence ?",['measure-theory']
4155664,Is the way I computed this triple integral correct?,"I had to compute the following triple integral: Let $$D = \{(x, y, z) \in \mathbb{R}^3 \ | \ 1 \leq x^2 + y^2 + z^2 \leq 4 \ | \ x, y, z \geq 0\}$$ Compute $$\int_D x \ dx dy dz$$ I chose to integrate over x as my outer integral, then y and then z as my inner integral. I deduced the bounds for each variable as such: $$0 \leq x \leq 2$$ $$\sqrt{1-x^2} \leq y \leq \sqrt{4-x^2}$$ $$\sqrt{1-x^2-y^2} \leq z \leq \sqrt{4-x^2-y^2}$$ Then I split up the integral: $$\int_0^2 \int_\sqrt{1-x^2}^\sqrt{4-x^2} \int_\sqrt{1-x^2-y^2}^\sqrt{4-x^2-y^2} x \ dz \ dy \ dx$$ Is my approach correct? Can I now just normally calculate the integrals from the inside to the outside? Edit: So I tried to do it with spherical coordinates. However, I get $0$ as a result which cannot be. In the end I got to this triple integral: $$\int_0^{2\pi} \int_0^\pi \int_1^2 r \ \sin{\phi} \ \cos{\theta} \ r^2 \sin{\phi} \ dr \ d\phi \ d\theta$$ Is this correct?","['integration', 'multivariable-calculus', 'real-analysis']"
4155725,"The additive group $(\Bbb Z[X], +)$ is not isomorphic to the additive group of rationals $(\mathbb{Q},+)$","I want to show that $(\Bbb Z[X], +)$ is not isomorphic to the additive group of rationals $(\mathbb{Q},+)$ . I basically want to know if my method is correct. My steps Let $\phi : (\Bbb Z[X], +) \to (\mathbb{Q},+)$ be an isomorphism then if $\phi(1)=c$ , and $\phi(x)=r$ .
Then we have $\phi(a_0 + a_1 x +\cdots + a_nx^n) = a_0 \phi(1)+ a_1\phi(1)\phi(x)+a_2 \phi(1)\phi(x)^2 + \cdots + a_n \phi(1) \phi(x)^n$ So we can see that $\mathbb{Q}$ is being generated by linear combination of $\phi(1)=c$ and $\phi(x)=r$ , $i.e.$ $\mathbb{Q}$ is finitely generated, a contradiction to the fact that $\Bbb Q$ is not finitely generated by rationals. So is my proof correct?","['group-theory', 'abstract-algebra']"
4155744,"Problem $2.17$, Rudin's RCA (Dictionary Order Topology)","Problem $2.17$ : Define the distance between the points $(x_1,y_1)$ and $(x_2,y_2)$ in the plane to be $$|y_1-y_2| \quad \text{if }x_1 = x_2, \quad\quad 1+|y_1 - y_2|\quad \text{if } x_1\ne x_2$$ Show that this is indeed a metric, and that the resulting metric space $X$ is locally compact. If $f\in C_c(X)$ , let $x_1,\ldots,x_n$ be those values of $x$ for which $f(x,y)\ne 0$ for at least one $y$ (there are only finitely many such $x$ !), and define $$\Lambda f = \sum_{j=1}^n \int_{-\infty}^\infty f(x_j,y)\ dy$$ Let $\mu$ be the measure associated with this $\Lambda$ by Theorem $2.14$ . If $E$ is the $x$ -axis, show that $\mu(E) = \infty$ although $\mu(K) = 0$ for every compact $K\subset E$ . Theorem $2.14$ above refers to the Riesz representation theorem, which relates $\mu$ with the linear functional $\Lambda$ . I found a solution to the above problem here , which I need some help with understanding. I've reproduced it to the extent necessary below. Hereafter, the distance defined above (between points in $\mathbb R^2$ ) is represented by $d$ , and it is indeed a metric. It was also proved that $(X,\tau)$ is a LCHS (locally compact Hausdorff space), by identifying it as $(X,\tau) = (\mathbb R, \tau_1) \times (\mathbb R,\tau_2)$ where $\tau_1$ is the discrete topology on $\mathbb R$ , and $\tau_2$ is the usual one. If $d_1$ and $d_2$ are the metrics corresponding to these topologies, it is easy to see that the product metric $d = d_1 + d_2$ . Now, the real problem begins. If $f\in C_c(X)$ , why is it that there are only finitely many $x$ for which $f(x,y)\ne 0$ for at least one $y$ ? The link I've attached says: If $K$ is compact in $X$ , the first projection $\text{pr}_1(K)$ is compact in $(\mathbb R, τ_1)$ . Hence
it is a finite set. Therefore $K$ is a finite union $$\{x_1\} × K_1 ∪ · · · ∪ \{x_n\} × K_n$$ where each $K_i$ , $i = 1, 2, . . . , n$ , is a compact set in $(\mathbb R, τ_2)$ . Here's my reasoning (please confirm if it is correct): Take $K = \text{supp}(f) = \overline{\{(x,y): f(x,y)\ne 0\}}$ . Since $f\in C_c(X)$ , $K$ is compact, and its projection $\text{pr}_1(K)$ is also compact. In the discrete topology, sets are compact iff they are finite, so $\text{pr}_1(K)$ is finite. Since projection maps preserve inclusion, we have $\text{pr}_1(\{(x,y): f(x,y)\ne 0\}) \subset \text{pr}_1(K)$ is finite. Now, $\text{pr}_1(\{(x,y): f(x,y)\ne 0\}) = \{x: \exists y, f(x,y)\ne 0\}$ which is finite (as the claim requires). In the very next paragraph, they mention that the support of $f\in C_c(X)$ is contained in $\{x_1,x_2,\ldots,x_n\}\times\mathbb R$ . How do we use this to deduce that $\Lambda$ is a positive linear functional on $C_c(X)$ ? How do we get $\mu(\{x\} \times K) = m(K)$ ? Let $V$ be an open set containing $\mathbb R × \{0\}$ . Then for $x ∈ \mathbb R$ , $(x, 0) ∈ V$ , so
that there exists an $ε_x > 0$ with $\{x\} × [−ε_x, ε_x] ⊂ V$ . This implies that there must be an $n$ with uncountably many $ε_x \ge 1/n$ . (Otherwise, $ε_x \ge 1/n$ for at most countably many $x$ , contradicting the fact that $\mathbb R$ is uncountable.) Let $K_x = \{x\} \times [-ε_x/2,ε_x/2]$ for $ε_x \ge 1/n$ . For $K = \bigcup_{j=1}^m K_{x_j}$ , we have $\mu(K) \ge m/n$ . Hence, if $V ⊃ \mathbb R × \{0\}$ is open, then $\mu(V ) ≥ \sup_{m∈\mathbb N} m/n= ∞$ . This implies $\mu(\mathbb R × \{0\}) = ∞$ . How do we get $\{x\} × [−ε_x, ε_x] ⊂ V$ ? I know the idea is that there exists some $\epsilon_x$ such that the open ball of this radius centered at $x$ is in $V$ , but why do open balls in this topology look like this? Also, I didn't really understand what was done after this to show $\mu(\mathbb R × \{0\}) = ∞$ , and I'd appreciate if someone could explain in detail. Thank you!","['proof-explanation', 'measure-theory', 'riesz-representation-theorem', 'real-analysis']"
4155793,"Harvard Stat 110 Strategic Practice 2, Fall 2011 - Inclusion Exclusion - Problem 1.1","Harvard Stat 110 Strategic Practice 2, Fall 2011 - Inclusion Exclusion - Problem 1.1 For a group of $7$ people, find the probability that all $4$ seasons (winter, spring, summer, fall) occur at least once each among their birthdays, assuming that all seasons are equally likely. I tried to solve it using another method (which came into my mind at that moment) but I must be doing a mistake. Maybe someone could help me finding the error and also explaining WHY I make the error, so I can avoid it in the future. I tried to apply the naive definition. So, I have $7$ people and $4$ seasons to choose. 1st person can have $4$ picks, the 2nd $4$ picks, etc etc. All are independent, so I have a total of $4^7$ possibilities. The number of favourable outcomes are when from the $7$ people, $4$ have each Spring, Summer, Fall, Winter and the other $3$ might get any choice. From $7$ people, I chose $4$ to fill Spring, Summer, Fall, Winter and the other $3$ can have whatever choice. So, result should be $$\binom{7}{4} \cdot \frac{4^3}{4^7}$$ which yields $0.546$ which is clearly different from the practice answer of $0.513$ . Could somebody, please, point out what I am doing wrong? Thank you!","['inclusion-exclusion', 'combinatorics', 'probability']"
4155802,Maximise $(\cos x\ln (x^{\frac1x})-\frac{\cos x\log_{10}x}{\ln(\ln x)})^{\cos x}$,"Maximise $$\large \left(\cos x\ln (x^{\frac1x})-\frac{\cos x\log_{10}x}{\ln(\ln x)}\right)^{\cos x}$$ I wonder why Mathematica says infinity as answer, but on desmos graph it is clear that it is less than 2 $\color{blue}{\text{Even WolframAlpha gave correct answer}}\color{red}{\text{ local maximum value as 1.461}}$ My try: I used the command Maximise with and without constraints. I tried both using constaint as $\ln x >1$ But nothing correct come  up. Next I also tried FindMaximum it didn't show anything either, but WolframAlpha command gave local maximum correctly. I wonder is there any analytical way to find $\color{green}{\text{Local Maximum Value ?}}$","['wolfram-alpha', 'maxima-minima', 'calculus', 'optimization', 'derivatives']"
4155858,Can any coordinate function be completed into an orthogonal coordinates system?,"Let $U$ be an open neighbuorhood $U \subset \mathbb{R}^2$ , and let $g$ be a smooth Riemannian metric on $U$ . Let $x$ be a smooth function on $U$ , with nonvanishing derivative $dx \neq 0$ . Let $p \in U$ . Does there always exist a smooth function $y$ on a neighbourhood of $p$ , such that the coordinates $x,y$ are orthogonal w.r.t the metric $g$ ? i.e. $g=f(x,y)dx^2+h(x,y)dy^2$ , for some functions $f,h$ . Equivalently: $g(\partial_x,\partial_y)=0$ . Note that I am not asking for Isothermal coordinates, so $f,h$ can be different.","['riemannian-geometry', 'coordinate-systems', 'smooth-functions', 'smooth-manifolds', 'differential-geometry']"
4155864,Does there exist an unfair coin such that the probability of an even number of heads in $n$ flips is $\frac{1}{2}$?,"Question: Does there exist an unfair coin and an $n$ such that the probability of an even number of heads in $n$ flips is $\frac{1}{2}$ ? First attempt Let $p$ be the probability of our coin flipping heads. If we try $n=2 $ then we must have: $p^2+(1-p)^2 = \frac{1}{2}$ and the only root to this is $p = \frac{1}{2}$ so no bias coin works in only $2$ flips. Moving on to $n=3$ we must have: $p^3 + 3p(1-p^2) = \frac{1}{2} $ and again the only root is $p = \frac{1}{2}$ I then proceeded to check these up to $n=6$ and found that the only root was $p=\frac{1}{2}$ . This suggested to me that the answer was no but I wasnt able to go much further with this method for a general $n$ . I could have proceeded by proving $(p-\frac{1}{2})^n \propto $ ""the polynomials we were getting - 0.5 "" but this is cumbersome. Second attempt I then noticed that if we found such a $p$ and an $n$ then it would also be true for $n+1$ as the probability of an even number of heads in $n+1$ tosses = $\frac{p}{2}+\frac{1-p}{2} = \frac{1}{2} $ (conditioning on the first $n$ tosses ) I then wanted to do this in reverse i.e showing if its true for $n$ its true for $n-1$ Define $P_n^p$ as the probability of getting an even number of heads from a coin having bias $p$ in $n$ flips. Then we have $P_n^p = (1-p)\cdot P_{n-1}^p +  p \cdot (1-P_{n-1}^p)$ and if $P_{n}^p = \frac{1}{2}$ then $\frac{1}{2} = P_{n-1}^p -2pP_{n-1}^p +p   $ and so $\frac{1}{2} - p = P_{n-1}^p (1-2p)$ and then $P_{n-1}^p = \frac{1}{2}$ And so if it is true for $n$ it is true for $n-1$ but we know that for $n=2$ no bias coins work and so inductively there do not exist any coins with bias that work! (as true for $n$ implies $n-1 $ , implies $n-2$ , implies $n-3$ ,.... implies $2$ But we already showed $n=2$ is false. Something more streamline? I cannot help but think that due to the symmetry of this problem there must surely exist such a simple and elegant way of showing it is not true! Notice if $p$ works then so does $1-p$ .   Can someone find a very simple way of solving this problem ? :)","['statistics', 'puzzle', 'probability']"
4155873,Proof of Brezis Theorem 9.17.,"Theorem 9.17 of Brezis, ""Functional Analysis, Sobolev Spaces, and Partial Differential Equations,"" states the following claim. Suppose that $\Omega$ is of class $C^1$ . Let $u \in W^{1,p}(\Omega) \cap C(\overline{\Omega})$ with $1 \leq p < \infty$ .
Then the following properties equivalent: (i) $u = 0$ on $\Gamma = \partial \Omega$ .
(ii) $u \in W^{1,p}_0 (\Omega)$ . I need the proof of (i) $\Rightarrow$ (ii).
The Brezis' book just prove this when $\textrm{supp} \, u$ is a compact subset of $\Omega$ .
And, the Brezis' book says ""in the general case in which $\textrm{supp} \, u$ is not bounded,
consider the sequence $(\zeta_n u)$ (where $(\zeta_n)$ is a sequence of cut-off functions)."" However, I cannot prove the general case. I want to show that $\|\nabla u - \nabla u_n\|_{L^p} \to 0$ , but if I try to do it, $\nabla \zeta_n$ appear and I cannot handle it.","['analysis', 'real-analysis', 'calculus', 'functional-analysis', 'partial-differential-equations']"
4155874,Applications of the Generalized Stokes Theorem?,"It states that for a $k$ -dimensional manifold (with boundary) $M$ and $k-1$ -differential form $f$ we have $$\int_{\partial M}f = \int_M df.$$ Most of textbooks spend significant effort to prove the general version and then derive the classical low-dimensional version, which are known for their applications in classical physics. However, I was curious whether the generalized version has some interesting applications, including in mathematics, except proving Brouwer-fixed point theorem.","['stokes-theorem', 'manifolds-with-boundary', 'analysis']"
4155931,Topology of $\;\mathbb{R}^3$ without origin and antipodal points identified.,"Setting: Let $\sim \subseteq {\Bbb R}^3 \times {\Bbb R}^3$ be the relation identifying antipodal points, i.e. $\vec{x} \sim \vec{y}$ iff $\vec{x} = \vec{y}$ or $\vec{x} = - \vec{y}$ . Question: What is the quotient topology of $({\Bbb R}^3 \setminus \{ \vec{0} \})/\sim$ like? Background: I understand that ${\Bbb S}^2 / \sim$ is the real projective plane (with ${\Bbb S}^2 \subseteq {\Bbb R}^3$ having the subspace topology) and I understand how I get the real projective plane from ${\Bbb R}^3\setminus \{ \vec{0} \}$ by identifying $\vec{x}$ with $\lambda \cdot \vec{x}$ for non-zero $\lambda$ , and I understand how to get the ${\Bbb S}^2$ by identifying $\vec{x}$ with $\lambda\cdot \vec{x}$ for positive $\lambda$ . My goal is to fill in all parts of the below diagram and I am missing the ?? part. I also would be interested what we can say about the diagonals (which cannot be drawn in MathJax afaik, the main diagonal direction is obvious, but can we say something about the other diagonal). $\require{AMScd}$ \begin{CD}
{\Bbb R}^3\setminus\{ \vec{0} \} @>a>> ??\\
@V b V V e,f @VV  c V\\
{\Bbb S}^2 @>>d> P{\Bbb R}^2
\end{CD}","['general-topology', 'quotient-spaces']"
4155940,Find explicit form of following: $a_n=3a_{n-1}+3^{n-1}$,"I wanted to find the explicit form of a recurrence relation , but i  stuck in nonhomogenous part. Find explicit form of following: $a_n=3a_{n-1}+3^{n-1}$ where $a_0=1 , a_1 =4,a_2=15$ My attempt: For homogeneous part , it is obvious that $c_13^n$ For non-homogenouspart = $3C3^n=9C3^{n-1}+3^n \rightarrow 9C3^n=9C3^{n}+3 \times3^n$ , so there it not solution. However , answer is $n3^{n-1} + 3^n$ . What am i missing ?","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
4155958,Definition of heavy tails and moment generating functions,"In this video (exact time already selected in the link) the connection between so-called 'heavy tails' and an infinite moment generating function is explained as follows: The benchmark to break into 'heavy' and 'light'-tailed distributions is the exponential. The survival function of an exponential distribution is $\bar F_{\text{exp}}(x) = \Pr(X>x)=e^{-\lambda x}.$ Considering non-negative rv, a distribution is heavy-tailed if: $$\lim \sup_{x\to\infty} \frac{\bar F(x)}{\bar F_{\text{exp}}(x)}= \lim \sup_{x\to\infty} \frac{\bar F(x)}{e^{-\lambda x}}= \lim \sup_{x\to\infty} \bar F(x)\;\color{red}{e^{\lambda x}}=\infty$$ which is equivalent to saying that the MGF is infinite for all $\lambda>0$ . The presenter points to the part in red (if I understood it correctly), which certainly would blow up with a positive $\lambda,$ but he never connects that part of this expression above with the definition of the MGF, i.e. $\mathbb E(e^{\lambda X}).$ What is the connection between both expressions? The answer is ""promised"" in the Wikipedia entry for heavy-tailed distributions by actually defining heavy tails as those distributions with infinity MGF: \begin{align}
M_X(t)=\mathbb E\left[e^{t X}\right]&=\int_{-\infty}^{\infty}e^{tx}f_X(x)dx\\
&=\int_{-\infty}^{\infty}e^{tx}dF_X(x)=\infty
\end{align} and claiming that this implies that \begin{align}
\lim_{x\to\infty}e^{tX}\bar F(x)=\lim_{x\to\infty}e^{tX}\Pr(X>x)=\infty
\end{align} This implication is not proven, and it is my question.","['statistics', 'probability-distributions', 'laplace-transform', 'moment-generating-functions', 'probability-theory']"
4155988,Doubt while finding the range of $f(x)=\cos^2x+4\sec^2x$ using the AM-GM inequality,"I was asked to find range of $$f(x)=\cos^2x+4\sec^2x$$ I converted the given expression to- $$f(x)=\cos^2x+\frac{4}{\cos^2x}$$ and then applied AM-GM inequality but when I checked whether  equality will hold or not I found that for equality $\cos x=\pm\sqrt{2}$ which is not possible, therefore I can't find range of expression from here. But when I repeated the same procedure by writing $f(x)=4\sec^2x+\frac{1}{\sec^2x}$ . I found that inequality condition is also satisfied and I can easily write range. I want to know why I can't get range from my first try?","['trigonometry', 'inequality']"
4156024,Smallest non-4-space-filling polytesseract?,"As a follow-up to Smallest non-space-filling polycube? , what's the smallest polychoron produced by fusing tesseracts 3-face to 3-face which does not fill 4-space?","['polyhedra', 'geometry', 'tiling']"
4156074,"calculate $\int_0^\infty \int_0^\infty e^{-xy} \sin(x) \,dx\, dy$","I want to prove $\int_0^\infty \sin(x) / x \,dx = \frac{\pi}{2}$ using $$\int_0^\infty \int_0^\infty e^{-xy} \sin(x) \,dx\, dy= \int_0^\infty \int_0^\infty e^{-xy} \sin(x) \,dy\, dx $$ assuming I've proved the integral converges so I can switch integral.
but my trouble is how to calculate $$\tag1\int_0^\infty \int_0^\infty e^{-xy} \sin(x) \,dx\, dy$$ I just wrote it in the below form $$\int_0^\infty \int_0^\infty e^{-xy}\sin x\,dx\,dy = \int_0^\infty \operatorname{Im} \int_0^\infty e^{-xy + ix}\,dx\,dy = \int_0^\infty \operatorname{Im} \frac{1}{y -i}\,dy\\ =\int_0^\infty  \frac{1}{y^2 +1 }\,dy = \frac{\pi}2.
$$ but I'm also looking for another way to calculate this integral $(1)$ , any help is appreciated , thanks!","['integration', 'calculus']"
4156129,Are $\mathbb{R}$ and $\mathbb{Q}$ the only subfields of $\mathbb{C}$ with natural structure as ordered fields?,"We know that $\mathbb{R}$ and $\mathbb{Q}$ have a unique structure as ordered fields with the usual order, and that $\mathbb{C}$ cannot be realised as an ordered field. Various non-trivial subfields of $\mathbb{R}$ , such as $\mathbb{Q}(\sqrt 2)$ , have multiple orderings, but I can't yet completely rule out the possibility of there being other subfields of $\mathbb{C}$ with unique order structure. Clearly any such subfield cannot contain $bi$ for any $b\in\mathbb{R}^\times$ . How can I determine whether there are any other possibilities than $\mathbb{R}$ and $\mathbb{Q}$ ?","['ordered-rings', 'field-theory', 'order-theory', 'abstract-algebra', 'ordered-fields']"
4156144,Proof Verification - Hungerford I.8.3,"$\textbf{Hungerford I $\S$8, problem 3.}$ Let $G$ be an (additive) abelian group with subgroups $H$ and $K$ . Show that $G \cong H \oplus K$ iff there are homomorphisms $\pi_1:G\to H$ , $\pi_2:G\to K$ , $\iota_1: H\to G$ , and $\iota_2:K\to G$ such that $$\pi_1\iota_1 = 1_H,\ \pi_2\iota_2 = 1_K,\ \pi_1\iota_2 = 0 = \pi_2\iota_1$$ and $\iota_1\pi_1(x) + \iota_2\pi_2(x) = x$ for all $x \in G$ . My question is if the abelian assumption is needed at all. Here is a purposed proof of the $\Longleftarrow$ direction that does not use the fact that $G$ is abelian whatsoever: Consider $f :G \to H \oplus K$ with $f(g) := (\pi_1(g), \pi_2(g))$ for all $g\in G$ . Then $f$ is a homomorphism; $f$ is surjective since if $(h,k)\in H \oplus K$ , then by taking $g = \iota_1(h)\iota_2(k)$ , our assumption gives that $f(g) = (h,k)$ ; and $f$ is injective since if $g,g'\in G$ and $f(g) = f(g')$ , then $\pi_1(g) = \pi_1(g')$ and $\pi_2(g) = \pi_2(g')$ which (again by our assumption) yields that $$g = \iota_1(\pi_1(g)) \cdot \iota_2(\pi_2(g)) = \iota_1(\pi_1(g'))\cdot \iota_2(\pi_2(g')) = g'.$$ The other direction is somewhat obvious and I don't think uses the fact that $G$ is abelian at all either: Let $\phi: G\to H \oplus K$ be an isomorphism and for $i = 1,2$ let $\pi_i := p_i\phi$ and $\iota_i:=\phi^{-1}j_i$ , where $p_i$ and $j_i$ are canonical projections/inclusions. Yes, there is another post about this question but focuses on generalizing this problem in a completely different way and thus does not answer my question.","['group-theory', 'abstract-algebra', 'solution-verification']"
4156151,Elementary Union of Sets Question,"I have a ton of questions about elementary set theory.  Here's one that stumped me just now. The task is to give an example of sets A and B for which $\cup A = \cup B$ but $A\neq B$ . Here's my guess: A = {a,{b,c}} B = {{a,b},c} ∪A = {a,b,c} = ∪B Is this true, or, on the other hand, is ∪A = {a,{b,c}}, requiring some other solution? Thank you.  All of my set theory practice exercises are from the book Elements of Set Theory by Herbert B. Enderton","['elementary-set-theory', 'solution-verification']"
4156163,Magicians Problem [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question Consider a competition for magicians. The point of the competition is to prove to the general audience that there is no such thing as magical abilities at all, so the organizers hope that nobody wins simply due to luck. The procedure of the competition is as follows. A fair coin is tossed 11 times and the magician tries to guess the results of these tosses. The organizers of the competition test null hypothesis that claims that probability to guess is 1/2 against an alternative that this probability is larger than 1/2. If null hypothesis is rejected on 5% significance level, then the organizers will claim that the magician has magical abilities and pay him his winning money. Assume that 50 magicians participate in the competition and try to prove that they have magical abilities. All magicians are tested independently. What is the probability that at least one magician wins (and therefore the organizers have to conclude that people with magical abilities exist). Enter answer with 2 digits after decimal point. Ans: W : no. of magicians out of 50 who wins W ~ Binom(n=50, p = 0.5) P(W >= 1) = 1 - P(W=0) 0.92 but this not correct answer `","['statistics', 'probability']"
4156173,How many fixed polyominoes does it take to force an aperiodic tiling of the plane?,"A longstanding open problem asks whether there is a single connected tile that only tiles the plane aperiodically. As far as I know, this is still open even in the case of polyominoes: we do not know if there is a single polyomino that forces an aperiodic tiling of the plane. I am curious about the case where we treat the polyominoes as fixed , i.e., when we consider rotated/reflected copies of a tile as distinct, and only permit translations. In this situation, we do know that more than one tile is needed; as shown in this paper , a single fixed polyomino tiles the plane periodically if it forms any tessellation at all. (In fact, we can ensure the tiling is isohedral!) Conversely, there exist finite sets of fixed polyominoes which tile the plane, but not in a periodic manner. If we use Wang tiles to generate polyominoes, we can obtain a solution with $11$ tiles (which is minimal), but there are smaller solutions. For instance, consider Matthew Cook's set of three polyominoes which force an aperiodic tiling (with rotations and reflections allowed): We observe that the tiling only makes use of $4$ orientations each of two of the tiles, and a single orientation of the third tile, so only $9$ fixed polyominoes are needed in total. The answer is therefore somewhere between $2$ and $9$ inclusive; I am interested in learning of any improvements to either the upper or lower bounds, or pointers to discussion of this problem in the literature. One potential avenue for a smaller upper bound is the aperiodic set of polyominoes described at this Wolfram MathWorld page as being announced by Roger Penrose in 1994, but it gives no further details, and I have been unable to track down a reference.","['tessellations', 'geometry', 'reference-request', 'polyomino', 'tiling']"
4156196,A natural choice of 'maximal ordered subfield' of a field?,"For any field $K$ of characteristic zero, its prime subfield $Q(K)$ has a natural ordering inherited from $\mathbb{Q}$ . I am interested in finding out to what extent (if at all) this natural ordering can be extended to larger subfields of $K$ , in a natural way. (Of course, many constraints on $K$ may be required for this to be natural). My motivating example is $\mathbb{C}$ , and I want to generalise the role that $\mathbb{R}$ plays as the 'natural' choice of ordered subfield of $\mathbb{C}$ . One can check that $\mathbb{R}$ has a unique structure as an ordered field and that $\mathbb{C}$ cannot be realised as an ordered field. Since there are no fields strictly between $\mathbb{R}$ and $\mathbb{C}$ , we have that $\mathbb{R}$ is maximal as an ordered subfield of $\mathbb{C}$ . Note that the ordering of proper subfields of $\mathbb{R}$ is not necessarily unique. There are certainly maximal ordered subfields for any $K$ (Zorn's Lemma), but no guarantee of uniqueness. Any ideas?","['field-theory', 'order-theory', 'abstract-algebra', 'ordered-fields']"
4156258,Let $f : \mathbb{R} \to \mathbb{R}$ be measurable and let $Z = {\{x : f'(x)=0}\}$. Prove that $λ(f(Z)) = 0$.,"The following is an exercise from Bruckner's Real Analysis: Let $f : \mathbb{R} \to \mathbb{R}$ be measurable and let $Z = {\{x : f'(x)=0}\}$ . Prove  that $λ(f(Z)) = 0$ . For the case $f$ being nondecreasing / nonincreasing, we can defined $g=f^{-1}$ and then $g'$ and then use the following theorem from the book : Let $f$ be nondecreasing / nonincreasing / of bounded variation on $[a,b]$ . Then $f$ has a finite derivative almost everywhere. Is it possible to ""reduce"" evaluation of any measurable $f$ to a nondecreasing one or otherwise how the claim can be proved for any measurable $f$ ?","['lebesgue-measure', 'derivatives', 'measurable-functions']"
4156262,Proving existence of inverse function using the rule of differentiating inverse functions.,"I am working from a textbook that gives us that The derivative of the inverse function is $$ \frac{dx}{dy} = \large{\frac{1}{\frac{dy}{dx}}}.\qquad (*)$$ $$$$ Now I am stuck on part $(a)$ of the following question: A function is defined by $\ f(x) =x^3 + 3x + 2.$ $(a)$ By considering $f'(x),\ $ prove that $\ f(x)\ $ has an inverse function. $(b)\ $ Find the gradient of the graph of $y=f^{-1}(x)$ at the point where $x=2.$ $$$$ For $(b),\ f(x) = 2\ \iff x=0\ $ . Therefore, the gradient of the graph of $y=f^{-1}(x)$ at the point where $\ x=2\ $ is equal to $\ \large{\frac{1}{f'(0)}} = \frac{1}{3}.$ But I'm simply a bit confused unsure on what it wants for part $(a)$ ... The derivative of the inverse function is $\ \large{\frac{1}{3}\cdot \frac{1}{x^2+1} },\ $ but this does not say that, for example, the derivative of the inverse function $\ y=f^{-1}(x)\ $ at the point $\ x=3\ $ is $\ \large{\frac{1}{3}\cdot \frac{1}{3^2+1} },\ $ which it isn't. This means I find $(*)$ confusing as to what it is saying exactly, and like I say, I'm confused about this and not sure how to tackle part $(a)$ . Please can someone clarify. Thanks.","['inverse-function', 'derivatives']"
4156263,Minmax value of sums of three consecutive terms in cyclic permutation.,"Consider a permutation $(x_i)_{i=1,2,\dots,12}$ of the numbers $\{1,2,\dots,n\}$ . We
want to think of these as a cyclic permutation one way of doing this is to extend $x_i$ for every $i \in \mathbb{Z}$ as expected. For example $x_0 = x_n$ and so on. Now let us define $a_i = x_{i-1} + x_i + x_{i+1}$ , and let us ask the following question
¿If we set the permutation $x$ at ""random"", what is the lowest possible value of $\max\{a_i : i=1,2\dots,n\}$ we could get? Let us call this minmax value $t(n)$ . Note that $\sum_{i=1}^n a_i = 3\sum_{i=1}^n = 3\frac{n(n+1)}{2}$ , and thus by a simple pigenhole
argument we know there must be some $i$ such that $a_i \geq \lceil \frac{1}{n}3\frac{n(n+1)}{2}\rceil = \lceil\frac{3}{2}(n+1)\rceil$ . This gives the following lower bound: $$\bigg\lceil\frac{3}{2}(n+1)\bigg\rceil \leq t(n)$$ By computing some values we see that $t(n)$ does frequently exceed the lower bound, but not always. Some values are given below in the form of $(n,t(n) - \lceil\frac{3}{2}(n+1)\rceil)$ : $$(3,0),(4,1),(5,1),(6,0),(7,2),(8,1),(9,1),(10,1),(11,2),(12,1)$$ Does anyone know a closed formula of $t(n)$ ? Or how it relates to this lower bound
asymptotically, that is, does this limit exist: $\lim_{n\to\infty} t(n) - \lceil\frac{3}{2}(n+1)\rceil$ ? if not, does it go to infinity?, etc. Anything among these lines really. Thanks.","['permutations', 'combinatorics']"
4156288,References on piecewise linear functions,I've been looking for references on piecewise linear functions in analysis books but I can't find many. Some treat the subject a bit passing and not general. Do you know of any references that might be useful? Thanks in advance.,"['analysis', 'real-analysis']"
4156292,How to derive $SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\hat{\sigma}\bigg[\frac{1}{n}+\frac{(x_0-\bar{x})^2}{(n-1)s^2_x}\bigg]^\frac{1}{2}$,"I am trying to derive the following formula given by the lecture notes $$SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\hat{\sigma}\bigg[\frac{1}{n}+\frac{(x_0-\bar{x})^2}{(n-1)s^2_x}\bigg]^\frac{1}{2}$$ My attempt $$SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\big[SE^2(\hat{\beta_0})+x_0^2SE^2(\hat{\beta_1})\big]^\frac{1}{2}\\
SE^2(\hat{\beta_0})=\hat{\sigma}^2\bigg[\frac{1}{n}+\frac{\bar{x}^2}{(n-1)s^2_x}\bigg]\\
x^2_0SE^2(\hat{\beta_1})=\hat{\sigma}^2\bigg[\frac{x^2_0 }{(n-1)s^2_x}\bigg]\\
\text{Plug in}\\
SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\hat{\sigma}\bigg[\frac{1}{n}+\frac{\bar{x}^2}{(n-1)s^2_x}+\frac{x^2_0}{(n-1)s^2_x}\bigg]^{\frac{1}{2}}=\hat{\sigma}\bigg[\frac{1}{n}+\frac{\color{red}{x^2_0+\bar{x}^2}}{(n-1)s^2_x}\bigg]^{\frac{1}{2}}
$$ In the correct answer, the numerator should be $(x_0-\bar{x})^2$ , I don't see how can I obtained that. Any comment are more than welcome and appreciated it for that.","['regression', 'statistical-inference', 'statistics', 'standard-error']"
4156298,The ratio of finitely based magmas to all magmas,"Let $n$ be a positive integer. By $S_n$ , I denote the set of positive integers from $1$ to $n$ . By $F_n$ , I denote the cardinality of the set of magmas on $S_n$ which are finitely based, that is, which have a finite generating set of identities. I conjecture that the ratio of $F_n$ to the cardinality of the set of all magmas on $S_n$ tends to $0$ as $n$ tends to infinity. Is this true, and if not, what is the ratio? And has anyone wrote a paper on this topic? Non-OP edit: Here's the precise definition of ""finitely based"" (since there's some potential confusion around what ""generating set of identities"" means - under one interpretation it trivially includes all finite algebras) : An algebra $A$ is finitely based iff its equational theory can be axiomatized by finitely many equations (where ""equation"" is meant in the sense of universal algebra). Equivalently, iff there is a finite set of equations $F$ such that the variety $Mod(F)$ of algebras satisfying each equation in $F$ is exactly the variety generated by $A$ . There do indeed exist non-finitely-based finite algebras (including a three-element magma ), and in fact the general problem of determining whether a finite algebra is finitely based is extremely complicated - see e.g. here .","['universal-algebra', 'magma', 'combinatorics']"
4156317,Sheaf Axioms and Limits - Intuition,"The question is based on the following problem from Vakil's notes in Algebraic Geometry: 2.2.C. The identity and gluability axioms (of sheaves) may be interpreted as saying that $\mathcal{F}(\cup_i U_i)$ is a certain limit. What is that limit? The precise solution / proof / answer to this question isn't as interesting to me as the intuitive approach we take to get there, because while thinking about this problem, I've realized that I am quite confused about the intuition behind limits and colimits. Before going into my thoughts, here are some stack exchange questions and links I found in looking into ths: In (relatively) simple words: What is an inverse limit? Inverse vs direct limits intuition for limits Limits wiki page (I have looked at more answers, but these seemed the most relevant to what I want to ask) Here are two trains of thought I've had for each (these will be quite hand-wavey. As I said, I'm not looking at the proof just yet. I'm more interested in the intuition): For viewing the axioms as limits: The motivating example of limits in my head are p-adic numbers. Ravi brings them up as his primary example as well (1.4.1 - 1.4.B). It seems like they key idea here is that the limit is some 'global' object that we demand that it behaves well locally. Based on the stack exchange posts linked above, I got the sense of something similar - inverse limits are 'zoomed out' objects, and we construct them from their local projections, just like p-adic numbers. Now I could argue that this is the spirit of the two axioms, one is defining a global object (the unique global section you get after gluing) and making sure it plays well locally. For viewing the axioms as colimits: The motivating example of colimits is fractions, according to Ravi. I don't have a very good sense for this, even though I can formally work out proofs of exercises such as 1.4.C, which asks to prove that $\mathbb{Q} = \varinjlim \frac{1}{n}\mathbb{Z}$ . I don't see any obvious way (even within the stack exchange answers) to view the gluing axioms in this sense. But I do intuitively feel comfortable with viewing colimits as disjoint unions modulo the relevant relations (one could ask me why this makes sense intuitively and not the fraction view, and one would not get a satisfying answer since I don't intuitively see their connection). With the disjoint union view, viewing colimits as unions makes intuitive sense, and this seems to be exactly the spirit of the gluing axioms - where we are literally gluing ('union-ing') the different sections to get a bigger section. Both interpretations seem reasonable to me and so my question are 1) are these intuitive views accurate/correct or are there any glaring issues with these views? 2) (The more important question) How would you argue for the correct answer and against the incorrect answer? Where do the above views work and where do they fail in context of the correct and incorrect answers? And/or how would you approach approaching the problem intuitively. I.e. what are you looking for when you are trying to intuit whether something is a limit or colimit. (Note, I am not looking for a proof unless the proof provides an insight to the second question above) Thanks a lot!","['algebraic-geometry', 'limits-colimits', 'category-theory', 'sheaf-theory']"
4156337,Prove $\frac{|a|-|b|}{1-|ab|}\leq \frac{|a+b|}{|1+ab|}\leq \frac{|a|+|b|}{1+|ab|}$,"Prove $\frac{|a|-|b|}{1-|ab|}\leq \frac{|a+b|}{|1+ab|}\leq \frac{|a|+|b|}{1+|ab|}$ where $|a|,|b|<1$ . My idea: Take $f:\mathbb{D}\rightarrow \mathbb{D}$ holomorphic with $f(-a)=0$ and the conformal map $h:\mathbb{D}\rightarrow\mathbb{D}$ with $$h(z)=\frac{z-a}{1-az}.$$ Then the inverse of $h$ is $$h^{-1}(z)=\frac{a+z}{1+az}.$$ Then the holomorphic mapping $$g(z)=f\circ h(z)$$ satisfies $g(0)=0$ . Then by Scharwz lemma $$|f\circ h(z)|\leq |z|.$$ Which implies $$|f(z)|\leq \left| \frac{a+z}{1+az}\right|.$$ If I am able to prove that I can choose $f(b)=\frac{|a|-|b|}{1-|ab|}$ then I get the first Inequality and I can get the other in a similar manner.","['complex-analysis', 'inequality']"
4156353,Does squeeze theorem apply for almost sure convergence,"Suppose we are given that $X'_n \leq Z_n \leq X_n$ for random variables $X'_n,Z_n, X_n$ . If we are told that $X_n,X'_n$ converges almost surely to some random variable $Y$ , can we conclude that $Z_n \stackrel{a.s}{\to}Y$ ? The closest variant of this question that I have seen is Squeeze theorem for convergence in distribution .
I've considered $\{\omega: \lim_{n \to \infty}Z_n(\omega)=Y(\omega)\}$ , and realized that this set is a superset of the intersection of $\{X_n(\omega)=Y\}$ and $\{X'_n(\omega)=Y\}$ (due to the inequality), but that doesn't allow us to conclude that $Pr(\{\omega: \lim_{n \to \infty}Z_n(\omega)=Y(\omega)\})=1$ unfortunately.","['almost-everywhere', 'pointwise-convergence', 'probability']"
4156381,How to prove a block matrix is positive definite?,"Suppose there is a Hermitian matrix $W\in\mathbb{C}^{mn\times mn}$ with size $mn\times mn$ .
Denote $W_{ij}$ as the submatrix with size $n\times n$ in the following position: \begin{equation}
W=\begin{bmatrix}
W_{11}& W_{12} &\cdots &W_{1m}\\
W_{21} &W_{22} &\cdots &W_{2m}\\
\vdots &\vdots &\ddots &\vdots \\
W_{m1}& W_{m2}& \cdots &W_{mm}\\
\end{bmatrix}
\end{equation} $W_{ij}$ is known to be Hermitian and positive semi-definite. What kind of condition can guarantee that $W$ is positive definite? Do we have some results like if $W_{ii}\succ \sum_{j\neq i}W_{ij} $ for each $i$ , then $W$ is positive definite?","['matrices', 'matrix-analysis', 'positive-definite', 'block-matrices']"
4156382,A polynomial $ f $ in three variables with real coefficients and any line $l$ in $ \mathbb R^3 $,"Prove that for every polynomial $ f $ in three variables with real coefficients and any line $\ell$ in $ \mathbb R^3 $ either $ \ell \subset Z_f $ or $ |\ell \cap Z_f|\leq \deg f $ , where $$ Z_f :=\{(x,y,z)\in \mathbb R^3: f(x,y,z)=0 \}.$$ Here is the case for a polynomial $f$ in two variables and line in $\mathbb R^2$ : http://diposit.ub.edu/dspace/bitstream/2445/159040/2/159040.pdf ( Page 16 , Lemma : 2.2.6) which is well understood. But I am stuck on how to solve the case with $3$ variables. Any hint or help is appreciated. Thanks!","['multivariable-calculus', 'abstract-algebra', 'polynomials', 'real-analysis']"
4156392,Functions of finite subsets of a set,"I found an old exam and this question appeared:
Let $X$ be a set and $X^{fin}$ denote the set of all finite subsets of $X$ . We say for $n\in\mathbb{N}$ that $X$ is $n$ -good if there exists a function $f:X^{fin}\rightarrow X^{fin}$ such that for all subsets $A$ of cardinality $n$ there exists $B\subsetneq A$ such that $A\subseteq f(B)$ . Show that $X$ is $n+2$ -good iff $|X|\leq \aleph_n$ . Edit: This problem makes since contrary to what I initially thought. For the case where $n=0$ we have that if $\vert X \vert\leq \aleph_0$ then if it is finite we can simply set the function to be the constant function $f(A)=X$ as Erick Wofsey pointed out or you could just send the empty set to $X$ and leave everything constant. In the case where $\vert X \vert=\aleph_0$ we can wlog asume $X=\omega$ and let $f$ send $\{a\}\mapsto a+1=\{0,1,\dots,a\}$ This funciton has the desired property. Edit 2: I believe I have a solution for the $(\leftarrow)$ implication: We proceed by induction on $n$ and we already shown the base case. Wlog we can assume that $X=\omega_{n+1}$ . For each $\alpha\in\omega_{n+1}$ we have it's one of it's initial segments so it's cardinality is $\aleph_n$ so there is a $n$ -good function $f_\alpha:\alpha^{fin}\rightarrow\alpha^{fin}$ by the induction hypothesis. We set $\tilde{f}_\alpha$ to be defined on all the finite sets of $\alpha+1$ that contain $\alpha$ to itself by $A\cup\{\alpha\}\mapsto f_\alpha(A)\cup\{\alpha\}$ . Let $f=\bigcup_{\alpha\in\omega_{n+1}}f_\alpha$ . This is the $n+1$ -good function we were looking for.",['elementary-set-theory']
4156405,Question about everywhere differentiable functions,"Suppose function $f$ is defined on $(0,1)$ , and differentiable everywhere. If for almost every $x$ , we have $|f'(x)|=1$ , can we conclude that $f$ is a linear function, that is for all $x$ , $f'(x)\equiv 1$ or $f'(x)\equiv -1$ ? I think this is related to the intermediate value property of derivatives. If ""for almost every"" is modified as ""for every"", then it's obviously true. All I can get is that, the points where derivative is continuous forms a dense open set. Then I don't know how to progress.","['derivatives', 'real-analysis']"
4156433,"If $x=ye^y$, find $\frac{dx}{dy}$, and use it to find $\frac{dy}{dx}$","If $x=ye^y$ , then explicit differentiation to find $\frac{dx}{dy}$ , and the implicit differentiation to find $\frac{dy}{dx}$ yield consistent results. Explain if the above line is True or False. (Ans: True) But why? When we can clearly see using wolframalpha , explicit differentiation is $\dfrac{dx}{dy} = e^y \times (1 + y)$ . Implicit differentiation using wolframalpha , is $y'(x) = \dfrac{dy}{dx} = \dfrac{1}{e^y (y+1)}$ They seem clearly different, and not consistent result because they are reciprocal(?) of each other?","['calculus', 'implicit-differentiation', 'derivatives', 'algebra-precalculus']"
4156482,Can every continuous function be continuously ''transformed'' into a differentiable function?,"Can every continuous function $f(x)$ from $\mathbb{R}\to \mathbb{R}$ be continuously ""transformed"" into a differentiable function? More precisely is  there always  a continuous (non constant) $g(x)$ such that $g(f(x))$ is differentiable? This seems to hold for simple functions, for instance the function $f(x)=|x|$ can be transformed into a differentiable function by the function $g(x)=x^2$ . If $g(x)$ is additionally required to be increasing everywhere and
differentiable then the answer seems to be no by the inverse
function theorem, because of the existence of continuous nowhere
differentiable functions.","['examples-counterexamples', 'real-analysis']"
4156547,Finding a Lyapunov function and proving the stability,"I'm trying to find a Lyapunov function for $(0, 0)$ in the system \begin{cases}
   x' = 2x - 2y - (2x - y)^3\\
    y' = 4x - 2y + (x - y)^3
 \end{cases} I thought that the following one would appropriate $$V = 2x^2 - 2xy + y^2 = x^2 + (x - y)^2 > 0$$ $$V(0, 0) = 0$$ but is it true that $$V' = (2x + 2x - 2y)(2x - 2y - (2x - y)^3) - 2(x - y)(4x - 2y + (x - y)^3) = -34x^4 - 4y^4 - 60x^2y^2 + 24xy(3x^2 + y^2) \le 0  ?$$ Maybe it is better to find Chetaev function and use other theorem... I would be thankful for any help!","['stability-theory', 'ordinary-differential-equations', 'lyapunov-functions']"
4156598,Injective functions between sets,"Given the statement: The number of injective functions $f:\{1,2,3,4\} \to \{1,2,3,4,5\}$ such that $\{1,2,3\} \subseteq f[\{1,2,3,4\}]$ equals to the number of The number of injective functions $f:\{1,2,3,4\} \to \{1,2,3,4,5\}$ such that $\{1,2\} \not\subseteq f[\{1,2,3,4\}]$ My question is: The statement is true or false? MY APPROACH: First, I calculated the number of injective
functions such that $\{1,2,3\} \subseteq f[\{1,2,3,4\}]$ and I got $3! \cdot 5=30$ Second, I calculated the number of all functions
available from $A$ to $B$ : $f:\{1,2,3,4\} \to \{1,2,3,4,5\}$ ,
means: $5^4=625$ and then reduced the number of injective functions that do exist $\{1,2\} \subseteq f[\{1,2,3,4\}]: 2 \cdot 5= 50$ and then
I got: $625-50 \neq 30$ But I think I have a mistake in my method any help?","['functions', 'combinatorics', 'discrete-mathematics']"
4156655,"$E[\exp(t \Delta_{N})]\le\exp(t^2(b-a)^2/8)$ and $E[\Delta_{N}\lvert F_{N-1}]=0$, why is $E[\exp(t \Delta_{N})\lvert F_{N-1}]\le\exp(t^2(b-a)^2/8)$","Let $\Delta_{N}$ be bounded by $[a,b]$ , where $$\Delta_{N}:=E[f(X_{1},...,X_{M})\lvert F_{N}]-E[f(X_{1},...,X_{M})\lvert F_{N-1}]$$ with $F_{N}:=\sigma(X_{1},...,X_{N})$ When proving the bounded differences lemma I came across the following statement with very little explanation: Since $$E[\exp(t \Delta_{N})]\leq \exp(t^2(b-a)^2/8)$$ and $$E[\Delta_{N}\lvert F_{N-1}]=0 \; (*)\; \; ,$$ we obtain $$E[\exp(t \Delta_{N})\lvert F_{N-1}]\leq \exp(t^2(b-a)^2/8)$$ I am aware that line $(*)$ is merely a consequence of Hoeffding's Inequality, but I do not understand how can conclude the inequality with respect to the sigma algebra $F_{N-1}$ . Any ideas?","['measure-theory', 'real-analysis', 'expected-value', 'inequality', 'probability-theory']"
4156695,"Given a randomly generated binary matrix with fixed row and column weights, what is the probability that columns have ones at different rows?","Setup: Given $a,b\in\mathbb{N}$ , and $b\geq a$ such that $b/a\in\mathbb{N}$ , I generate (i.e., uniformly sample amongst all possible matrices) a random matrix $\mathbf{A}\in\{0,1\}^{a,b}$ , where $a$ is the number of rows and $b$ is the number of columns, such that each column of $\mathbf{A}$ contains exactly one entry 1 (i.e., weight of one), and each row of $\mathbf{A}$ contains exactly $b/a$ entries of 1 (i.e., weight of $b/a$ ). This implies that any individual column is uniformly distributed among all length- $a$ columns of weight one (in total there are only $a$ such columns), and any individual row is uniformly distributed among all length- $b$ columns of weight $b/a$ . Question: Assuming $x<a$ and $x<b$ , looking at just $x$ specific columns (e.g., the first $x$ columns), what is the probability that all $x$ columns have a one in a different row? Attempt: I think of counting the number of such matrices. More specifically I count the number of ways to choose the rows of the matrix sequentially. This gives \begin{align}
\underbrace{{b-x\choose b/a-1}\dots{b-x-(x-1)(b/a-1)\choose b/a-1}}
_{\text{#ways to choose rows with entry 1 in the $x$ columns}}
\underbrace{{b-x(b/a)\choose b/a}\dots{b-(a-1)b/a\choose b/a}}
_{\text{#ways to choose remaining rows}}
\end{align} and the number of ways to choose the matrix without the imposed restrictions is \begin{align}
{b\choose b/a}\dots{b-(a-1)b/a\choose b/a}.
\end{align} Dividing the final terms of the two equations give us our probability. Did I under/over-count the number of possibilities? Do I also need to include the number of rows each of the $x$ columns can have their one entry in? This would result in a further multiplication of $a(a-1)\dots(a-x+1)$ .","['combinatorics', 'probability']"
4156711,"How do we explain the ""Bad Luck"" in probability?","Say, in the game of Pokemon Go, the probability of catching a shiny Pokemon is $\frac{1}{450} \approx 0.22222\%$ . Some players consider that ""lucky"". And by the Law of Large Numbers, we can say, if we catch 450 Pokemon per day, then over the long term, we get about 1 shiny Pokemon per day (assuming all Pokemon has a chance of being shiny, for simplicity, because in reality not all Pokemon has its shiny form released yet). by probability, we can also say that, the probability of getting one or more shiny Pokemon per day is $$ 1 - \left(\frac{449}{450}\right) ^ {450} \approx 0.6325 = 63.25 \% $$ So how do we explain the difference between the ""long term"" $100\%$ vs the short term $63.25\%$ ?  How do we explain this ""lucky everyday"" vs ""not as lucky everyday at $63.25\%$ ? Where did that remaining $36.75\%$ go? P.S. I think I know the answer and how the math relates to the philosophy of everyday life... but I will put the answer here 3 - 7 days later.",['probability']
4156725,Product of roots of unity.,"Let $x_n$ be the $n$ roots of unity. Does there exist a closed expression for $$ F_n:=\prod_{i=1}^n(1+x_i+x^{n-1}_i)?$$ Interestingly, if $n=1$ , then $F_n=3$ and if $n=2$ then $F_2=-3$ and if $n=3$ then $F_3=0.$ So maybe it just oscillates between these numbers, but how could one prove something like this?","['complex-analysis', 'elementary-number-theory', 'roots-of-unity', 'analysis']"
4156742,"If $\frac{1}{n} \sum_{i=1}^n X_i$ converges a.s. to a finite constant, then $E|X_n|<+\infty$.","The complete question is: Let $\{X_n\}$ be an I.I.D. random variable sequence. If $\frac{1}{n}\sum_{i=1}^n X_i$ converges to a finite constant, then $X_n$ is integrable. It seems like the inverse problem of Strong Law of Large Numbers. But it really gets me stuck. Note: I have seen some similar problem at here: Convergence of the empirical average of iid random variables in probability implies integrability , where it assumes that $X_i$ is non-negative. Try: Thanks to Murthy, I have discovered something new: We can write $X_i$ as $X_i = X_i^+ - X_i^-$ , where $X_i^+ = X_i\cdot 1_{X_i>0}$ and $X_i^- = X_i\cdot 1_{X_i<0}$ , then $$E|X_i|<+\infty \iff E|X_i^+|<+\infty , E|X_i^-|<+\infty$$ We assume that $E|X_n| = \infty$ , then $E|X_n^+| = \infty$ or $E|X_n^-| = \infty$ or they both become infinite. For first two cases, we can use the method THAT LINK mentioned. But it can not be used in the third case.",['probability-theory']
4156743,Is the diagonal still measurable in this product measure?,"Is the diagonal $\Delta=\{(x,x)|x
\in[0,1]\}$ still measurable in $\mathcal{B}([0,1])\times\mathcal{F}$ where $\mathcal{F}$ is the $\sigma$ -algebra formed by all sets of Lebesgue measure $0$ or $1$ in $[0,1]$ ? This problem comes to me when I want to prove that the indicator of $A$ is measurable in this question: https://mathoverflow.net/questions/176622/progressively-measurable-vs-adapted","['measure-theory', 'lebesgue-measure']"
4156782,Change of Variables and Integration Domain,"This is a purely theoretic problem which doesn't involve any specific calculation. Suppose we have two coordinate systems for $\mathbb R^2$ and they are $xy$ -system and $uv$ -system respectively. Suppose we have $x=g(u,v)$ and $y=h(u,v)$ and we know both $g$ and $h$ are continuously differentiable, i.e. $C^1$ . Suppose the mapping $T(u,v):=(g(u,v),h(u,v))=(x,y)$ is injective. Suppose we have a small rectangle $S$ in the $uv$ -plane. The general idea to prove the formula for change of variables is approximating the image of $S$ under $T$ in the $xy$ -plane by a parallelogram in the $xy$ -plane. People use the image of the boundary of $S$ under $T$ to calculate the area of the parallelogram I quoted. By doing this, people use the fact that $T$ preserves boundaries, i.e. $T(\partial S)=\partial [T(S)].$ My question is , how can we assure that $T(\partial S)=\partial [T(S)]$ ? If the equality is not true, then how do we approximate the parallelogram in the $xy$ -plane? I know the equality is true if $T$ is a homeomorphism, but under our assumption, $T$ may not have a continuous inverse(though $T^{-1}$ is always defined on $T(S)$ due to our injective condition). Edit: I think I missed an important condition that the Jacobian determinant $\frac{\partial (x,y)}{\partial (u,v)}$ is nonzero in the interior of the integral domain in the $uv$ -plane. If we have this condition, then we can use the Inverse Function Theorem to guarantee the existence of local homeomorphism, and then our operation is valid.(Am I right?) Second Edit: In my calculus book, the formula for change of variables doesn't require the Jacobian determinant is nonzero. I am stuck here again. Third Edit: Intuitively I feel like the Jacobian must be nonzero and so I am trying to prove it now. Suppose the Jacobian is zero on a set with positive measure, which is denoted by $E$ . The key equation in the proof is that $$dx\,dy= \frac{\partial (x,y)}{\partial (u,v)} du\,dv.$$ On $E$ , we can have some $du,dv$ s.t. $du>0$ and $dv>0$ . Since the Jacobian is zero on $E$ , we have $dx\,dy=0$ . So a rectangle with area of $du\,dv(>0)$ is mapped to a set with zero measure by an injective continuous function $T$ , where $T$ is defined as above. Intuitively I feel like this statement is wrong, but I can not prove it. Any help with deriving a contradiction will be appreciate.","['analysis', 'multivariable-calculus', 'calculus', 'change-of-variable', 'general-topology']"
4156799,An induction problem,"Recently I started to learn some induction problems, and I'm stuck with this one: prove that, for each $n$ $\in \mathbb N$ , $(n-1)^2$ divides $n^n - n^2 + n - 1$ . There are no tips or answers in the textbook on how to solve this problem, so I'm little bit stuck and frustrated. Here's what I got: Basis: $n = 1$ , $(1-1)^2 = 0$ , and for $n = 1$ we have $1^1 - 1^2 + 1 - 1 = 0$ , so for $n = 1$ it's true. Induction Hypothesis: for each $n$ $\in \mathbb N$ , $(n-1)^2$ divides $n^n - n^2 + n - 1$ . Induction Step:  this is where I'm stuck. I know that I must show that for $((n+1)-1)^2$ = $n^2$ , the number $({n^2})^{n^2} - n^2 + n - 1$ is equal to zero. So, is there anyone who can help me, or give any advice? Thank you in advance.","['induction', 'discrete-mathematics']"
4156914,Differential equation: $2x^2y'=y^2(2xy'-y)$,"Solve the differential equation: $$2x^2y'=y^2(2xy'-y)$$ I tried to convert it to the form of a total differential equation. $$\begin{array}{lrl}
&2x^2y'&=y^2(2xy'-y)\\
\Leftrightarrow&y'(2x^2-2xy^2)+y^3&=0\\
\Leftrightarrow&y^3dx+(2x^2-2xy^2)dy&=0
\end{array}$$ Here, I tried my best to find the integrating factor, but can't. Of course, it has no form $\mu (x)$ or $\mu (y)$ . I also tried to set $\dfrac{y}{x}=u^\alpha$ . I wanted a perfect "" $\alpha$ "" in order to have a ""beautiful"" form. But, it isn't successful.","['systems-of-equations', 'ordinary-differential-equations', 'real-analysis']"
4156990,How to prove this function isn't differentiable at origin [duplicate],"This question already has an answer here : $f(x,y) = \frac{x^3y}{x^4 + y^2}$ is not differentiable at $(0,0)$. (1 answer) Closed 3 years ago . [Exercise 5.18 - Pugh] Show that the function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ defined by $$ f(x,y)= \begin{cases} \frac{x^3y}{x^4+y^2} \ ; (x,y)\neq 0 \\\quad 0\quad\ ;(x,y)=0 \end{cases}$$ has $\nabla_{(0,0)}f(u)=0$ for all $u$ but is not differentiable at $(0,0)$ . My attempt Isn't hard to show the first part (that all directional derivatives exist) but I cannot show that this function is differentiable. My ideia is standard, I need to find a path such that $\lim\limits_{\gamma\;\mapsto (0,0)} f(\gamma)$ doens't exist. In order to achieve this I tried $y=mx, y=mx^2, y=mx^3$ but no one gave me a clear result. At best, I was able to show that $$\lim\limits_{(x,y)\mapsto(0,0)}\frac{f(x,y=x^2)}{f(x,y=-x^2)}=-1 $$ But I don't think that's enough. Is that enough ? If yes, why? If not, is there a better method to solve this ?","['real-analysis', 'multivariable-calculus', 'calculus', 'partial-derivative', 'limits']"
4156997,Bounded Sobolev sequence together with convergence in Lebesgue space implies convergence in intermediate Sobolev spaces,"I am trying to understand the proof of a theorem, but I just can't seem to wrap my head around this one step: We have a sequence of functions $(f_i)_{i \in \mathbb{N}}$ that are bounded in the Sobolev space $W^{k,p}$ , where $k \in \mathbb{N}$ , $p \geq 2$ , i.e. $\|f_i\|_{W^{k,p}} \leq C$ for some constant $C$ and all $i$ . We also know that $f_i$ converges to some $g$ in $L^p(\mathbb{R})$ and this $g$ is also an element of $W^{k,p}$ . The proof now just claims, that using standard properties of Sobolev spaces, this bound together with the convergence in $L^p$ implies the convergence $f_i \to g$ in all intermediate spaces $W^{k',p}$ , where $k' \in (0,k)$ . I am rather new to Sobolev spaces, so I am quite confused as to what these standard properties might be. I tried to read up in the book on Sobolev spaces by Adams, but I could not really find anything helpful. To be honest, I have yet to understand what norms these intermediate spaces use. I also tried to first understand the statement asuming $k'$ is an integer, because in this case I am more familiar with the spaces, but I was also unsuccessful. Do you have ideas on how to approach this task? Any help is appreciated","['fractional-sobolev-spaces', 'sobolev-spaces', 'functional-analysis']"
