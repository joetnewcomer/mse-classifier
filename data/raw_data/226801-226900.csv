question_id,title,body,tags
4685500,"Weird $\sin(\ln (x))$, $\cos(\ln (x))$ pattern","I just came across these four integrals, evaluated them, and noticed they show this weird alternate pattern, I think its cool but I have no idea why they do. Anyone knows an intuitive reason for this? $$\hspace{1cm} \int_0^\infty \frac{\cos(\ln(x))}{1+x}\mathrm{d}x=0 \hspace{2cm} \int_0^\infty \frac{\sin(\ln(x))}{1+x}\mathrm{d}x= \frac{\pi}{\sinh(\pi)}$$ $$\int_0^\infty \frac{\cos(\ln(x))}{(1+x)^2}\mathrm{d}x= \frac{\pi}{\sinh(\pi)} \hspace{1cm} \int_0^\infty \frac{\sin(\ln(x))}{(1+x)^2}\mathrm{d}x=0 $$","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'trigonometric-integrals']"
4685537,"if $\displaystyle\lim_{z\to\infty} f(z)=0$ then f is bounded, for a holomorphic function $f$","I would like to ask you for some help, I have a doubt that I have not been able to solve yet... let $f:\mathbb{C}\rightarrow\mathbb{C}$ a holomorphic function.
if $\displaystyle\lim_{z\to\infty} f(z)=0$ then $f$ is bounded I don't know yet if this is true, however this is what I have been trying to solve it. Suppose that $f$ is not bounded, then \begin{align*}
\forall r>0 : \exists z_r\in\mathbb{C} : f(z_r)\not\in B(0,r) 
\end{align*} iff \begin{align*}
\forall r>0  : r\leq|f(z_r)|
\end{align*} then \begin{align*}
\infty=\lim_{r\rightarrow\infty}r\leq\lim_{r\rightarrow\infty}|f(z_r)|
\end{align*} This would be complete provided that $|z_r|\rightarrow\infty$ However, I believe that what I have written is not necessarily true. If someone could give me some guidance it would be great.","['complex-analysis', 'complex-integration']"
4685567,Integrating monomials over $\mathbb{S}^{n-1}$ - Folland,"This is an exercise from Folland: Suppose $f(x)=\prod_{j=1}^n x_j^{\alpha_j}$ where all $\alpha_j$ are even. Then show that $$\int_{\mathbb{S}^{n-1}} f d \sigma= \frac{2 \prod_{j=1}^{n}\Gamma(\beta_j)}{\Gamma\left(\sum_{j=1}^n\beta_j\right)} \quad \text{where }\beta_j=\frac{\alpha_j+1}{2}$$ . As hint Folland suggests to calculate $\displaystyle \int_{\mathbb{R}^n} e^{-|x|^2}f(x)\mathrm{d}x$ . Attempt: By Tonelli's we have $$\int_{\mathbb{R}^n} e^{-|x|^2}f(x)\mathrm{d}x=\prod_{j=1}^{n}\int_{-\infty}^{\infty}e^{-y^2}y^{\alpha_j} \mathrm{d}y=\prod_{j=1}^{n}\Gamma(\beta_j)$$ I think I am supposed to change to polar coordinates now $(r=|x|, \, x'=\frac{x}{|x|})$ . Since $f(rx')=r^{\sum \alpha_j}f(x')$ , we get: $$\prod_{j=1}^{n}\Gamma(\beta_j)=\int_{\mathbb{R}^n} e^{-|x|^2}f(x)\mathrm{d}x=\int_0^{\infty}e^{-r^2}r^{(\sum \alpha_j)+n-1}\mathrm{d}r \int_{\mathbb{S}^{n-1}} f \mathrm{d}\sigma$$ from which the result follows. Is this correct?","['integration', 'real-analysis', 'multivariable-calculus', 'solution-verification', 'gaussian-integral']"
4685580,A curious identity for powers of the generating function of the Catalan numbers.,For a power series $\displaystyle f(x)=\sum_{n\geq 0}a_n x^n$ let $\displaystyle[f(x)]_r=\sum_{n\geq r} a_n x^n.$ Let $\displaystyle c(x)=\frac{1-\sqrt{1-4x}}{2x}$ be the generating function of the Catalan numbers which satisfies $c(x)=1+xc(x)^2.$ Computations suggest that $$\sum_{j=r}^{2m}\left(\binom{2m}{j}-\binom{2m}{2r-j-1}\right)x^j c(x)^{2j+1-2r}=\left[c(x)^{2m+1-2r}\right]_r.$$ For $r=0$ this reduces to the trivial identity $c(x)^{2m+1}=c(x)\left(1+x c(x)^2\right)^{2m}.$ Any idea how to prove this?,"['catalan-numbers', 'combinatorics', 'generating-functions']"
4685582,Why are the derivatives of complementary angles negative?,"I’m taking Calculus in highschool and my teacher told us a special trick she learnt was that the derivatives of all the trigonometric functions starting with “c” (such as cosine, cotangent, cosecant) give you a negative value. She claimed there was no mathematical reasoning behind this but that does not make sense. Is there a proof or reason as to why this is true?","['algebra-precalculus', 'calculus', 'geometry']"
4685583,"How does $H^{1,1}(X,\mathbb Z)$ look like in $H^{1,1}(X,\mathbb R)$?","Let $X$ be a compact Kahler manifold. Let $c_1:Pic(X)\rightarrow H^2(X,\mathbb Z)$ and $NS(X) = Im(c_1)$ and $H^{1,1}(X,\mathbb Z) = Im(H^2(X,\mathbb Z)\rightarrow H^2(X,\mathbb C))\cap H^{1,1}(X)$ . My question is: Intuitively, how does $H^{1,1}(X,\mathbb Z)$ look like in $H^{1,1}(X,\mathbb R)$ ? Is it a complete lattice? In Demailly's 'Analytic Methods of Algebraic Geometry', he defines the algebraic class of Kahler form as $\mathcal K_{NS} = \mathcal K\cap NS_\mathbb R(X)$ when $X$ is projective. But Lefschetz (1,1)-theorem tells us $Pic(X)\rightarrow H^{1,1}(X,\mathbb Z)$ is surjective. So, why don't we have $\mathcal K\subseteq NS_{\mathbb R}(X)$ ?","['complex-geometry', 'algebraic-geometry', 'differential-geometry']"
4685604,Sum of a set of numbers squared is larger than $N$ times the mean squared [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I require the proof for the following $$\sum_{i = 1}^N x_i^2 \ge N \mu ^2 $$ where $x_i \in \mathbb R$ and $$\mu = \frac{1}{N} \sum_{i = 1}^N x_i$$ I can visually see how this is true (I imagine rectangles and squares), and would like to know if there's a common name for the result. If not, what's an easy way to show a proof for this? Thanks!",['statistics']
4685688,"$f$ is convex if and only if $f =\sup\{\varphi:\mathbb{R}^M\to \mathbb{R}: \varphi \le f, \varphi \text { affine} \}$","Let $f : \mathbb{R}^M\to \mathbb{R} $ $f$ is convex if and only if $f =\sup\{\varphi:\mathbb{R}^M\to \mathbb{R}: \varphi \le f, \varphi  \text { affine} \}$ I am having trouble proving ( $\implies$ ) To prove it I have to follow the following hints: 1)Fix $x \in \mathbb{R}^M$ , and $t \in \mathbb{R}$ with $t < f (x)$ We claim that there exists an affine function $\varphi : \mathbb{R}^M \to \mathbb{R}$ with $\varphi \le f$ such that $\varphi(x) > t$ ; For this: 1a) Consider the point $P := (x, t)$ . Prove that there exists a point $Q$ which minimizes the distance from $P$ to the graph of $f$ 1b)Define the function $\varphi : \mathbb{R}^M \to \mathbb{R}$ as the affine function whose graph is the tangent plane to $\partial  B(P, |P − Q|)$ and passing by $Q$ 1c) Prove that $\varphi \le f$ Assume by contradiction that there exists a point $y \in \mathbb{R}^M$ such that $f (y) < \varphi(y)$ . Let $A := (y, f (y))$ . Prove that there exists a point $E$ on the graph of $f$ which is inside $B(P, |P − Q|)$ . contradicting the choice of $Q$ My try: I have worked out some of my ideas for each part , but I am missing pieces to complete it 1a) This one I almost have it.  Define the graph of $f$ as: $Graph (f)=\{(x,f(x)):x \in dom f\}$ and the epigraph: $Ep (f)=\{(x,y):x \in dom f , y>f(x)\}$ , which is convex because $f$ is convex. Ep(f) is also a closed set, and non-empty. Since we are in a Hilbert space and these are the hypotheses of Hilbert's projection theorem, I can conclude that there exists a point $Q\in Ep(f)$ such that the distance from $P$ to the set $Ep(f)$ is minimized. Now, I am missing the fact that this point should be in the graph, not in the epigraph Ep(f), how do I conclude that? It is kind of intuitive that points in the interior are further, but how would I formalize it? Maybe Hilbert's theorem is not the way to go then? 1b) The equation of $\varphi$ as described in the hint should be $\varphi (y)= a \cdot y+a_0$ , with $a \in \mathbb{R}^M$ and $a_0 \in \mathbb{R}$ . I am not sure if this way of writting it is convenient, because it is not clear by just looking at the equation that it passes by $Q$ and that the normal vector is $Q-P$ .  Maybe  a vector form is better: $n \cdot (R-Q)=(Q-P) \cdot (R-Q)=0, $ for all $R\in \mathbb{R}^{M+1}$ . The problem is that this equation is implicit, I don't see the function as in the cartesian form 1c) Proceding as in the hint:
Assume by contradiction that there exists a point $y \in \mathbb{R}^M$ such that $f (y) < \varphi(y)$ . Let $A := (y, f (y))$ . Let $Q=(y_q,f(y_q))$ and let $E=(y,\varphi(y))$ (Which I claim is the point I am looking for)
So I need to show that this point is inside $B(P, |P − Q|)$ , namely that $|E-P|<|P-Q| $ . I have tried to use the triangle inequality in many ways but I don't find what I need, for instance $|P-E|\le |P-Q|+|Q-A|+|A-E|=|P-Q|+|Q-A|+|f(y)-\varphi(y)|$ In this way $f(y)-\varphi(y)$ shows up, which should be non zero because of the hypothesis, but I don't know how to continue","['general-topology', 'convex-analysis', 'analysis']"
4685734,Math competition question about ways to spell BANANA in a square,"This is a math competition question I did. Essentially, starting at B, a move consists of moving to a non-diagonal adjacent square and noting the letter you land on (with the exception of the starting letter B). How many ways can you spell BANANA? So I've attempted this problem multiple times (to an incorrect answer) and have eventually resorted to the most primitive method, which was just listing every possible combination which does give me the right answer of 84. What I did was I labelled each square with a number from 1 to 9 and wrote down all the possible 'number combinations' assuming the first move is a rightwards move and then multiplied it by 2 because of the symmetry of the puzzle. While this is not wrong, could anyone suggest possibly a better/more elegant solution? I'm not really sure what tags to add to this so please give some suggestions and I'll edit this post later. There is a solution provided in Dutch but I'm just curious about other ways as well. Here is the English translation. If the first N of BANANA is in the blue box as well as the second N, then there are two possibilities for the second and the third A, so $2+2 = 4$ possibilities for BANANA. For blue-green there are likewise $2 \cdot 4 = 8$ possibilities. For blue-white there are $1 \cdot 2 = 2$ possibilities. In total there are therefore $4 + 8 + 2 = 14$ possibilities if the first N is in the blue box. If the first N is in the yellow box, there are also $14$ possibilities in the same way. For green-green we find $2 \cdot 4 \cdot 4 = 32$ possibilities. For green-blue we find $2 \cdot 2 \cdot 2 = 8$ possibilities, likewise for green-yellow and for green-white. So if the first N is in the green box, there are $32+ 8 + 8 + 8 = 56$ possibilities. In total you can make BANANA in $14 + 14 + 56 = 84$ ways.","['contest-math', 'puzzle', 'combinatorics', 'dynamic-programming']"
4685847,Is it always okay to substitute $a^2$ in place of $D^2$ in the inverse differential operator of cosh ax?,"In class, we were told that it was okay to make the following substitution while trying to solve a particular integral of a nonhomogeneous linear ODE, if $f(D)$ contains only even powers of $D$ , and $f(a) \neq 0$ $$\frac{1}{f(D)}\cosh ax = \frac{1}{f(a)}\cosh ax$$ But in some examples, we only replaced $D^2$ with $a^2$ , while keeping odd powers. For example, the general solution of the following equation $$\left(D^2-2D-3\right)y=2\cosh 3x$$ is $$y=c_1e^{3x}+c_2e^{-x}+\frac{xe^{3x}}{4}+\frac{1}{12}e^{-3x}$$ But our instructor replaced $D^2$ with $3^2$ while he was solving for the particular solution, which seems to give an inaccurate result $$y_p=\frac{1}{\left(D^2-2D-3\right)}2\cosh 3x = \frac{-1}{2}\frac{1}{\left(D-3\right)}\left(e^{3x}+e^{-3x}\right) = \frac{-xe^{3x}}{2}+\frac{e^{-3x}}{12}$$ So, my questions is, when is it really okay to make this substitution without getting an erroneous result? and does the same go for replacing $D^2$ with $-a^2$ if the image of the linear differential operator is $\sin ax$ or $\cos ax$ ?","['differential-operators', 'linear-algebra', 'ordinary-differential-equations']"
4685903,Surface area over a scalar field,"Let S be the sphere of radius $r$ centered at the origin. Define $f:S\to\Bbb R$ as $f(x)=\frac{1}{||x -x_0||}$ , where $x_0=(a,b,c)\in\Bbb R^3\setminus S$ . Compute $\int_S fdS$ (Scalar surface integral). My solution is to first parametrize $S$ by $\sigma:D:=[0,\pi]\times[0,2\pi]\to\Bbb R^3$ where $\sigma(\varphi, \theta) = (r\cos\theta\sin\varphi, r\sin\theta\sin\varphi, r\cos\varphi)$ . Hence, after some computation, $$\int_S fdS = \int_D \frac{r^2\sin\varphi}{\sqrt{(r\cos\theta\sin\varphi-x_0)^2 + (r\sin\theta\sin\varphi-y_0)^2 + (r\cos\varphi-z_0)^2}}d\varphi d\theta = \int_D \frac{r^2\sin\varphi \;d\varphi\;d\theta}{ \sqrt {r^2 - 2r\cos\theta \sin\varphi x_0 - 2r\sin\theta\sin\varphi y_0 -2r\cos\varphi z_0 + x_0^2 +y_0^2 + z_0^2} }$$ But from there I don't know how to compute it. In fact, after researching more about this problem, I found that it has already been solved here: https://math.stackexchange.com/a/3225231/1174522 But I don't understand how $$\sqrt{r^2+\lVert x_0\rVert^2-2r\,\lVert x_0\rVert\cos\varphi} = \sqrt{(r\cos\theta\sin\varphi-x_0)^2 + (r\sin\theta\sin\varphi-y_0)^2 + (r\cos\varphi-z_0)^2}$$ and then how he solves it.","['integration', 'proof-explanation', 'multivariable-calculus']"
4685911,Derivative-ish of $f^2(x)$,"The problem is as follows: $f$ is differentiable at $x$ . Show that $\lim_{h\rightarrow 0}\frac{f^2(x+3h)-f^2(x-h)}{h}$ exists and find its value. Note that $f^2(a)$ just means $[f(a)]^2$ . Well, basis calculus tells me the answer should be $2f'(x)f(x)$ . Here's my attempt: using the fact that $a^2-b^2=(a-b)(a+b)$ , I get that the fraction turns into $\frac{f(x+3h)-f(x-h)}{h}\cdot (f(x+3h)+f(x-h))$ . I know since $f$ is differentiable at $x$ , it's continuous in a neighborhood around $x$ , and so the right product just turns into $2f(x)$ as $h\rightarrow 0$ . The left product is basically $f'(x)$ , but I'm worried abound $f$ being evaluated at $x+3h$ and $x-h$ , rather than strictly $x+h$ and $x$ . How do I deal with this? Or can I just say that the left product is $f'(x)$ has $h\rightarrow 0$ ? Thanks.",['derivatives']
4685921,Consider strings of length 9 that satisfy the following conditions :,"Consider strings of length 9 that satisfy the following conditions: (i) The string contains four digits chosen from the set ${2, 3, 4, 5, 6, 7}$ . (ii) The string contains all five letters $A$ , $B$ , $C$ , $D$ , $E$ , such that the first occurrence of letters (i.e. the leftmost letter) must be $A$ , and the last occurrence of letters (i.e. the rightmost letter) must be $E$ . For example, $BCDAE2345$ doesn't satisfy the conditions since $A$ isn't the leftmost letter. Solution: Choose 4 digits: $\binom{6}{4}$ Choose location for 5 letters: $\binom{9}{5}$ Choose relative arrangement of $B,C,D$ : $P(3,3)$ Choose relative arrangement of digits: $P(4,4)$ Thus, the answer would be: $\binom{6}{4} \binom{9}{5} \cdot P(3,3) \cdot P(4,4) = 272160$ What I don't understand: To find the number of ways to put 4 digits in the string, why didn't we first find all the ways to pick 4 elements of the set which is $\binom{6}{4}$ , then count the ways to put these 4 digits in the 9 letter string, so $\binom{9}{5}$ . Then we count all the locations to put 5 letters in the string. so $C(9,5)$ . Then we permute around the 4 numbers to find all arrangements. so $P(4,4)$ . Then we fix A in the left and E and the right, so we need to permute 3 letters in the 3 possible places. so $P(3,3)$ which clearly gives me the wrong number of ways and I don't understand why. Honestly, I don't understand their solution very much and I don't understand why choosing location for 5 letters in a string is a combination. like how is that the same thing as calculating the number of 5 letter subset from a 9 element set. Please can someone give me a good intuition for why my solution doesn't work and theirs d","['permutations', 'combinations', 'proof-explanation', 'combinatorics', 'discrete-mathematics']"
4686046,Abelian group is free iff projective,"It is well-known that an abelian group is free iff it is projective, which can be easily extended to modules over PID. The standard proof requires the use of Zorn's Lemma/Well-ordering principle. Are there ways of proving this without axiom of choice or anything equivalent? Any hints or proof outlines will be appreciated. Any tools from introductory homological algebra and commutative algebra can be freely used. Thanks in advance.","['homological-algebra', 'modules', 'abstract-algebra', 'abelian-groups', 'commutative-algebra']"
4686055,Does a solution for this integral even exists? $I=\int_0^\frac{\pi}{2}\ln(a+\sin(x))dx$,"Some time ago I computed $\displaystyle \int_0^\frac{\pi}{2} \ln(1+\sin(x))dx$ and $\displaystyle \int_0^\frac{\pi}{2} \ln(1-\sin(x))dx$ , which suggested to tackle the more general case: $$I=\int_0^\frac{\pi}{2}\ln(a+\sin(x))dx$$ with $a>0$ . I wasn't able to solve it. I approached it whith Feynman technique: consider $I$ as $I(a)$ , and so $\frac{dI}{da}=\displaystyle \int_0^\frac{\pi}{2}\dfrac{1}{a+\sin(x)}dx$ , which evaluates with standard calculus to $\frac{2}{\sqrt{a^2-1}}\arctan{\sqrt{\frac{a-1}{a+1}}}$ . Unfortunately seems like this result cannot be integrated with respect to $a$ , so I got stuck. Anyone knows a solution? Notes: Clearly $I=\displaystyle \int_0^\frac{\pi}{2} \ln(a+\sin(x))dx=\int_0^\frac{\pi}{2} \ln(a+\cos(x))dx$ . Here are the results I got for the first two integrals if it can help: $$\int_0^\frac{\pi}{2} \ln(1+\sin(x))dx=2G-\frac{\pi}{2}\ln(2)\hspace{1cm}\int_0^\frac{\pi}{2} \ln(1-\sin(x))dx=-2G-\frac{\pi}{2}\ln(2)$$ where G is Catalan constant.","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'trigonometric-integrals']"
4686092,Gradient of $C \mapsto\frac{1}{2}\left\lVert CA - BC \right\rVert_F^2$,"Given the matrices $A \in \mathbb{R}^{n \times n}$ and $B \in \mathbb{R}^{m \times m}$ , let the scalar field $f : \mathbb{R}^{m \times n} \to \mathbb{R}$ be defined by $$ f(C) := \frac{1}{2}\left\lVert CA - BC \right\rVert_F^2 $$ What is the gradient $\nabla f$ ? I am trying to differentiate this function w.r.t. to $C$ but I cannot find a way to manipulate the expression that would enable me to do so. I've also tried a definition of derivative adapted in this case but I don't endup with something useful at first glance. I endup with a linear map $df(C)$ defined by the expression $$
df(C)E = \text{trace} \left\{ (CA -BC)^T (EA - BE)\right\} = \left\langle CA -BC,EA-BE\right\rangle
$$ which then leads to me to $$
df(C) = \left\langle AA^TC^T - AC^TB^T - A^TC^TB + C^TB^TB, \cdot \right\rangle
$$ Is this expression correct?","['matrices', 'scalar-fields', 'matrix-calculus', 'derivatives']"
4686104,Parametrizing unstable extensions of line bundles,"Suppose $L$ and $M$ are holomorphic line bundles over the same compact Riemann surface $X$ . I want to study which extensions of $M$ by $L$ , i.e., vector bundles $E$ in a short exact sequence of the form $$
0 \rightarrow L \rightarrow E \rightarrow M \rightarrow 0
$$ are unstable, in the sense of slope stability. I want extensions $E$ of degree zero (for now, but I don't think the degree is the most important thing here). For example, assuming that $M$ has degree 1 and $L$ has degree -1, I was able to see that all non-trivial extensions (that are not the direct sum $L\oplus M$ ) are semistable, by doing some small calculations with degrees and bundles of homomorphisms. So the only unstable extension is the trivial one. I am no longer able to do it if $L$ has degree -2 and $M$ has degree 2. The extensions are given by the cohomology group $H^1(M^{-1}L)$ . The idea would be to get the unstable extensions as the kernel of some homomorphism from $H^1(M^{-1}L)$ to another cohomology group, probably provided by some short exact sequence of bundles giving a long exact sequence in cohomology. Sadly I am not yet used to this enough to see what to do, but it seems that, in the case of line bundles and rank 2 extensions, the more basic stuff should be enough. Any suggestions? If you have some book suggestions, I would greatly appreciate that too.","['riemann-surfaces', 'complex-geometry', 'vector-bundles', 'algebraic-geometry', 'line-bundles']"
4686125,Is there a simple way to evaluate $\int_0^1 \arctan x \ln \left(\frac{1-x}{1+x}\right) d x?$,"After trying many methods but fail, I try the following substitution. Letting $t=\frac{1-x}{1+x}$ preserves  the interval and transforms the integral into $$I=\int_0^1 \arctan x \ln \left(\frac{1-x}{1+x}\right) d x = 2 \int_0^1 \arctan \left(\frac{1-t}{1+t}\right) \frac{\ln t}{(1+t)^2} d t$$ Noting that $$\tan \left(\frac{\pi}{4}-\arctan t\right)=\frac{1-t}{1+t} 
\Leftrightarrow \frac{\pi}{4}-\arctan t=\arctan \left(\frac{1-t}{1+t}\right)
$$ $$
I=\frac{\pi}{2} \underbrace{\int_0^1 \frac{\ln t}{(1+t)^2} d t }_K-2 \underbrace{\int_0^1\frac{\arctan t}{(1+t)^2} \ln t d t}_L
$$ Using integration by parts, we have $$
\begin{aligned}
\int \frac{\ln t}{(1+t)^2} d t & =-\int \ln t d\left(\frac{1}{1+t}\right)=-\frac{\ln t}{1+t}+\int \frac{d t}{t(1+t)} \\
& =-\frac{\ln t}{1+t}+\ln t-\ln (1+t)=\frac{t \ln t}{1+t}-\ln (1+t) \cdots (*) \\\Rightarrow \quad K&=-\ln 2
\end{aligned}
$$ Using (*), we can tackle the integral $L$ by integration by parts again, $$
\begin{aligned}
L= &  \underbrace{ {\left[\arctan t\left(\frac{t \ln t}{1+t}-\ln (1+t)\right)\right]_0^1 }}_{=-\frac{\pi}{4}\ln 2  }  -  \underbrace{ \int_0^1 \frac{t \ln t}{\left(1+t^2\right)(1+t)} d t}_{M} +  \underbrace{ \int_0^1 \frac{\ln (1+t)}{1+t^2} d t}_{N} 
\end{aligned}
$$ Using power series and  partial fractions, $$ \begin{aligned} M&=-\frac{1}{2} \int_0^1 \frac{\ln t}{1+t} d t+\frac{1}{2} \int_0^1 \frac{t \ln t}{1+t^2} d t+\frac{1}{2} \int_0^1 \frac{\ln t}{1+t^2} dt \\&=-\frac{1}{2} \sum_{k=0}^{\infty}(-1)^k \int_0^1 t^k \ln t d t+\frac{1}{2} \sum_{k=0}^{\infty}(-1)^k \int_0^1 t^{2k +1} \ln tdt +\frac{1}{2} \sum_{k=0}^{\infty}(-1)^k \int_0^1 t^{2 k} \ln t d t\\&= -\frac{1}{2} \sum_{k=0}^{\infty} \frac{(-1)^k}{(k+1)^2}+\frac{1}{2} \sum_{k=0}^{\infty} \frac{(-1)^k}{(2 k+2)^2}+\frac{1}{2} \sum_{k=0}^{\infty} \frac{(-1)^k}{(2 k+1)^2}\\&= -\frac{3}{8}\left(1-\frac{1}{2}\right) \zeta(2)+\frac{1}{2} G\\&=-\frac{\pi^2}{32}+\frac{G}{2}\end{aligned} $$ Letting $t=\tan \theta$ transforms $$
N=\int_0^{\frac{\pi}{4}} \ln (1+\tan \theta) d \theta \stackrel{x\mapsto\frac{\pi}{4}-x}{=} \int_0^{\frac{\pi}{4}} \ln \left(\frac{2}{1+\tan \theta}\right) d \theta= \frac{\pi}{8} \ln 2
$$ Grouping them together yields $$I=-\frac{\pi}{4} \ln 2+\frac{\pi^2}{16}-G$$ Is  there a simple way to evaluate $\int_0^1 \arctan x \ln \left(\frac{1-x}{1+x}\right) d x?$ Your comments and alternatives are highly appreciated.","['improper-integrals', 'definite-integrals', 'calculus', 'trigonometry', 'catalans-constant']"
4686147,Equivalent definitions of hyperelliptic Riemann surfaces,"Let $X$ be a connected Riemann surface. I’m trying to understand the equivalence about these two definitions (I’m still new to this subject): $X$ is hyperelliptic iff there is a holomorphic degree $2$ map $p:X \rightarrow \mathbb{CP}^1$ . $X$ is hyperelliptic iff there is a degree $2$ divisor $D$ such that $\dim(L(D)) \geq 2$ (here $L$ denotes the Riemann-Roch space). I think I can see that $2)$ implies $1)$ : if $\dim(L(D)) \geq 2$ , then there must be a function $f \in \dim(L(D))$ such that either $f$ has two single poles or $f$ has a double pole at some point $p\in X$ . In either case, $\deg(\tilde{f})=\sum_{p\in \tilde{f}^{-1}({\infty})}mult_p(\tilde{f})=2$ , where $\tilde{f}:X \rightarrow  \mathbb{CP}^1$ is the holomorphic function obtained by “extending” the meromorphic function $f:X \rightarrow \mathbb{C}$ to the point at infinity. (Is this correct?) However, I don’t see how to prove that $1)$ implies $2)$ . Do I need to find a divisor $D$ that satisfies the conditions? Any help would be appreciated.","['riemann-surfaces', 'divisors-algebraic-geometry', 'algebraic-geometry']"
4686154,Nonhomogeneous 2nd Order Differential Equation with variable coefficients,"I'm having trouble with solving this nonhomogeneous 2nd ODE $$x^{2}\frac{\mathrm{d}^{2}y }{\mathrm{d} x^{2}}+x\frac{\mathrm{d}y }{\mathrm{d} x}-y=x$$ In order to get the general solution I rewrote the equation like this $$x^{2}\frac{\mathrm{d}^{2}y }{\mathrm{d} x^{2}}+x\frac{\mathrm{d}y }{\mathrm{d} x}-y=0$$ Then I assumed that $y=x^{\alpha}$ , where $\alpha$ is a constant. When I plugged this into the 2nd equation I got $$\alpha (\alpha -1)x^{2}x^{\alpha-2}+\alpha x^{\alpha-1}x-x^{\alpha}=0$$ The general solution was $$y=C_{1}x+C_{2}x^{-1}$$ and then I got confused as to what to do next. Any suggestions?",['ordinary-differential-equations']
4686207,Almost exact formula for sum of Euler Phi function $\sum_{k=1}^n \varphi(k)$,"Solving a problem, I needed a formula for $\Phi(n)=\sum_{k=1}^n \varphi(k)$ , where $\varphi(k)$ is Euler's notorious totient function, that counts the number of numbers between $1$ and $k$ that are coprime with $k$ . I looked it up online and found that the best we were able to get until now is $$\Phi(n)=\sum_{k=1}^n \varphi(k)=\frac{3}{\pi^2}n^2+\mathcal{O}\left(n(\log{}n)^{\frac{2}{3}}(\log\log{}n)^{\frac{4}{3}}\right)$$ which becomes, when calculating: $$\Phi(n)\simeq\frac{3}{\pi^2}n^2$$ and the error is quite small. Howewer I needed an exact formula, and so, with some combinatorics and probability, I came up with the following: $$\Phi(n)=\sum_{k=1}^n \varphi(k)\simeq\left({n\choose 2}+1\right)\prod_{p\ prime}\left(1-\frac{1}{n^2}{\left\lfloor{\frac{n}{p}}\right\rfloor}^2\right)$$ The proof is simple but quite long, you can find it here: https://www.overleaf.com/read/xxjmgmcwqfgb . For simplicity I will call the product function $L(n) \triangleq\left({n\choose 2}+1\right)\prod_{p\hspace{1pt}prime}\left(1-\frac{1}{n^2}{\left\lfloor{\frac{n}{p}}\right\rfloor}^2\right)$ from now on. It is interesting to notice that the product taken over all prime is the same as the product taken only over the primes s.t. $p<n$ , and this makes the calculation a lot easier. Unfortunately (and I have no idea why), $L(n)$ still doesn't compute the exact value of $\Phi(n)$ , but the error it provides is a lot smaller than the one of $\frac{3}{\pi^2}n^2$ , here are some examples: $$ \Phi(2064)=1\,295\,506\hspace{2cm}\Phi(2064)-L(2064)=58.0496 \hspace{2cm}\Phi(2064)-\frac{3}{\pi^2}2064^2=592.093 $$ the error with $L$ is $ 0.0044$ %, the error with $\frac{3}{\pi^2}n^2$ is $0.0457$ %. $$ \Phi(10463)=33\,282\,718\hspace{2cm}\Phi(10463)-L(10463)=-8.35606 \hspace{2cm}\Phi(10463)-\frac{3}{\pi^2}10463^2=6500.06 $$ the error with $L$ is $ -2.51063 \cdot 10^{-5}$ %, the error with $\frac{3}{\pi^2}n^2$ is $0.0195298%$ %. There are even values where $\Phi(n)-L(n)\ll 1$ , for example if $n=502$ we have: $$ \Phi(502)=76\,698\hspace{2cm}\Phi(502)-L(502)=0.0215688 \hspace{2cm}\Phi(502)-\frac{3}{\pi^2}502^2=97.9693 $$ where the error for $L$ is $2.81217 \cdot 10^{-5} $ % I have no idea how to interpret these results. If someone knows why the formula fails I'd be delighted to hear. EDIT: As @Peter Košinár pointed out, the correct formula should be $$\sum_{k=1}^N \varphi(k)\simeq\ \frac{1}{2}\left(1+N^2\prod_{p\ prime}\left(1-\frac{1}{N^2}{\left\lfloor{\frac{N}{p}}\right\rfloor}^2\right)\right)$$","['number-theory', 'summation', 'totient-function', 'arithmetic-functions']"
4686216,"Show that the differential equation $\frac{dy}{dx}=\sqrt y,y=0$ has non-unique solution.","Show that the differential equation $\frac{dy}{dx}=\sqrt y,y(0)=0$ has non-unique solution. To solve this problem I used Lipschitz theorem. My solution goes like this: Let $f$ be continuous in an unbounded domain $$D:a\leq x\leq b,-\infty\leq y\leq \infty.$$ Let $f$ satisfy a Lipschitz condition in $D$ .  That is, assume $\exists k\in\Bbb R$ such that $|f(x,y_1)-f(x,y_2)|\leq k|y_1-y_2|$ for all $(x,y_1),(x,y_2)\in D.$ Then the differential equation $\frac{dy}{dx}=f(x,y)$ and $y(x_0)=y_0$ ( where $(x_0,y_0)\in D$ ) has a unique solution in $[a,b].$ ( Lipschitz Theorem) Now, here, we have $f(x,y)=\sqrt y$ and $\frac{\partial f}{\partial y}=\frac {1}{2\sqrt y}.$ We see that $f$ is continuous everywhere, but $\frac{\partial f}{\partial y}=\frac {1}{2\sqrt y},$ is not continuous at $y=0$ and also not bounded at $y=0$ and so, Lipschitz condition is not satisfied( since $f$ is said to satisfy a Lipschitz condition with respect to $y$ in a domain $D$ iff $\frac{\partial f}{\partial y}$ is bounded). Thus, we can conclude that  the given differential equation $\frac{dy}{dx}=\sqrt y$ and $y(0)=0$ has no unique solution. But I am confused as I studied Picard's Theorem as well which goes like this: The differential equation $\frac{dy}{dx}=f(x,y)$ and $y(x_0)=y_0$ has a unique solution on $|x-x_0|\leq h$ , if both $f$ and $\frac{\partial f}{\partial y}$ are continuous in the domain $D=\{(x,y):|x-x_0|\leq h,|y-y_0|\leq k,h,k>0\}$ If we use this theorem to conclude the above assertion in the question is true, then the reasoning would go like this: Here, $\frac{\partial f}{\partial y}$ is not continuous in the domain $D$ if $(0,y)\in D.$ Thus, Picard's theorem won't be satisfied if $(0,y) \in D.$ But according to Picard's theorem, we have: the differential equation $\frac{dy}{dx}=\sqrt y$ and $y(0)=0$ has a unique solution on $|x|\leq h(,h>0)$ , if both $f$ and $\frac{\partial f}{\partial y}$ are continuous in the domain $D=\{(x,y):|x|\leq h,|y|\leq k,h,k>0\}.$ But, $\frac{\partial f}{\partial y}$ is not continuous in $D$ as $(0,y)\in D.$ So, the differential equation $\frac{dy}{dx}=\sqrt y,y(0)=0$ has non-unique solution. But I am confused whether both of my solutions (one using Lipschitz theorem and the other one using Picard's theorem) is valid or not? I think, Lipschitz theorem is more appropriate to solve these sort of problems as Picard's theorem, gurantees a unique solution in a particular or (better say,) a bounded domain but the Lipschitz theorem gurantees the existence of a unique solution, in an unbounded domain( Is it ?) But, in this case what I did was that: I showed both the Lipschitz theorem and the Picard's theorem are not satisfied for the set of equations : $\frac{dy}{dx}=\sqrt y,y(0)=0.$ So, in general, if conditions of  Lipschitz's theorem  are not satisfied, then can we directly conclude that a set of equations, say: $\frac{dy}{dx}=f(x,y)$ and $y(x_0)=y_0$ (where $(x_0,y_0)$ is in the domain of $f$ ) do not have a unique solution in the domain of $f,$ just as the thing was with this case? Then, the usage of Picard's theorem further is unnecessary(here), isn't it?","['solution-verification', 'ordinary-differential-equations']"
4686249,How to integrate $\int \frac{1}{x^6-1}dx$,How to integrate $$\int \frac{1}{x^6-1}dx$$ I have done it by using $$x^6-1=(x-1)(x+1)(x^2+x+1)(x^2-x+1)$$ and then applying partial fraction decomposition but this makes for a very long process. Is there a better way to integrate this function?,"['integration', 'indefinite-integrals']"
4686272,"Can we bound $\sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle}$ by $\sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle}\hat g(a_i\omega)$?","Let $x_1,\ldots,x_k\in[0,1)^d$ , $a_1,\ldots,a_d>0$ and $$f(\omega):=\sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle}\hat g(a_i\omega)\;\;\;\text{for }\omega\in\mathbb R^d,$$ where $g(x)=e^{-(x/\sigma)^2}$ for some $\sigma>0$ . Suppose $|f(\omega)|<\varepsilon$ for all $\omega\in\mathbb R^d$ with $0<\|\omega\|<\delta$ . Can we show that $\left|\sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle}\right|$ is also bounded by something multiplied by $\varepsilon$ ? This is clearly true when all $a_i$ are equal to the same value, since we can then pull the $\hat\varphi$ out.","['fourier-transform', 'inequality', 'fourier-analysis', 'functional-analysis']"
4686281,"If $a^2-a-1=0$ where $a\gt0$, then what does $a^6$ equal? (Olympiad question)","$\color{white}{\require{cancel}{3}}$ So I was looking on Youtube for math equations that I thought that I could probably solve when I came across this video by the channel Maths and many more . The question in this video was $$\text{If }a^2-a-1=0\text{, then what is the value of }a^6\text{, where }a\gt0$$ which I wanted to try and solve on my own. Here are the steps that I took to solve the equation: $$\text{Put it into the quadratic formula since it is unfactorable with rational numbers}$$ $$\frac{1\pm\sqrt{1-4(1)(-1)}}{2}$$ $$=\frac{1\pm\sqrt{5}}{2}$$ however, since the value of $a$ is greater than $0$ , it is actually $$a=\frac{1+\sqrt{5}}{2}$$ $$\text{Then, all you have to do is square it}$$ $$\left(\frac{1+\sqrt{5}}{2}\right)^2$$ $$\implies\frac{1}{4}(1+\sqrt{5})^2\text{, and simplifying that gets}$$ $$\frac{1}{4}(6+2\sqrt{5})$$ $$\iff\frac{6+2\sqrt{5}}{4}$$ $$\iff1.5+\frac{\sqrt{5}}{2}=a^2$$ $$\implies(1.5+\frac{\sqrt{5}}{2})^2=a^4$$ $\color{white}{.}$ $1.5$ $\frac{\sqrt{5}}{2}$ $1.5$ $2.25$ $\frac{3\sqrt{5}}{4}$ $\frac{\sqrt{5}}{2}$ $\frac{3\sqrt{5}}{4}$ $1.25$ $$2.25+2\left(\frac{3\sqrt{5}}{4}\right)+1.25$$ $$\iff3.5+\frac{3\sqrt{5}}{2}=a^4$$ $\color{white}{.}$ $3.5$ $\frac{3\sqrt{5}}{2}$ $1.5$ $5.25$ $2.25\sqrt{5}$ $\frac{\sqrt{5}}{2}$ $1.75\sqrt{5}$ $3.75$ $$5.25+(2.25+1.75)\sqrt{5}+3.75=a^6$$ $$\implies\therefore9+4\sqrt{5}=a^6$$ $$\text{And,when we plug it into Wolfram Alpha:}$$ $$\text{We get the same thing! :)}$$ My question Is my solution correct, and if not, what could I do to attain the correct solution or what could I do to attain it more easily? To clarify Sorry if this seems like a trivial/short question If you want to edit this question to improve it, I am so sorry about the compressed formatting, it's just easier for me to type it like this for it to be faster. Sorry if the tags aren't correct, they most likely are but still. Sorry if I used any math functions incorrectly.","['golden-ratio', 'algebra-precalculus', 'solution-verification', 'quadratics']"
4686282,Best strategy to maximize outcome in Binomial Distribution.,"The question is the following: Given a multiple choice test with 12 ""yes"" or ""no"" questions, for which you need 8 correct answers to pass. You answer randomly, but know that 6 out of the twelve questions have answer ""yes"" (although you don't know which ones), what is the best strategy to maximize the likelyhood of passing? My approach feels unnecessarily complicated and might be wrong, I feel like I am missing something obvious. Results: I get $22.7\%$ chance of passing for $j = 10$ . I get $27.27\%$ for $j=8$ . I get $\approx 12\%$ for $j=7$ . I get $\approx 28.35 \%$ for $j=6$ My take went as follows: I want to find how many ""yes"" you should put at random in the test, in order to maximize the odds of getting $8$ answers correct. Here is my approach: The upper Line is the answer of the candidate ( $1$ for yes, $0$ for no) and the lower Line is the solution to the test: $$ \omega =  \begin{pmatrix} \omega_1 & \omega_2 & \cdots & \omega_6 & \omega_7 & \cdots &\omega_{12} \\ T_1 & T_2 & \cdots & T_6 & T_7 & \cdots & T_{12} \end{pmatrix}$$ Where $\omega_i$ is answer of a candidate to the $i$ -th question and $T_i$ the Solution to said question. If $\omega_i = T_i$ , then the answer is correct. Given a test, we can rearrange the order of the questions such that $\omega$ is equivalent to some $$[\omega] =  \begin{pmatrix} \omega_{\pi(1)} & \omega_{\pi(2)} & \cdots & \omega_{\pi(3)} & \omega_{\pi(7)} & \cdots &\omega_{\pi(12)} \\ 1 & 1 & \cdots & 1 & 0 & \cdots & 0 \end{pmatrix}$$ Where $\pi$ is some permutation of the columns. Now, let $1 \leq j \leq 12$ be the number of ""yes"" answers, we have $\binom{12}{j}$ possible ways to answer the test. For $j=12$ , we always get $6$ correct answers, but never $8$ . For $j=11$ , we can get a maximum of $7$ correct answers. For $j=10$ , things get more interesting and my attempt is too long to write down, but I reason as follows: All elements are equivalent to one of the following possible scenarios: \begin{align*}
         r_1 &= \begin{pmatrix} 1 & 1 &  \cdots & 1 & 0 & 0 & 1 & \cdots & 1 & 1 \\
                                1 & 1 & \cdots  & 1 & 0 & 0 & 0 & \cdots  & 0 & 0 \end{pmatrix}
       & r_2& = \begin{pmatrix} 0 & 1 & 1 & \cdots & 1 & 0 & 1 & \cdots & 1 \\
                                1 & 1 & 1 & \cdots & 1 & 0 & 0 & \cdots & 0 \end{pmatrix} \\
          r_3 &= \begin{pmatrix} 0 & 0 & 1 & \cdots & 1 & 1 & \cdots & 1 \\
                                 1 & 1 & 1 & \cdots & 1 & 0 & \cdots & 0 \end{pmatrix} & & 
     \end{align*} And by counting the number of possible permutations of each distinguishalbe column of each representant, we get the number of possibilities tied to each number of correct answer. Only $[r_1]$ can get us to pass and we get a $22.7\%$ chance of passing for $j = 10$ . I use a similar process for the rest. Here is my concrete computation in the case $j=6$ , where $S(r_i)$ is the number of correct answers associated to a certain respresentant and the numbers below for example $\begin{pmatrix} 1 \\ 0 \end{pmatrix}$ correspond to the number of columns of this form that any element equivalent to $r_i$ has:","['statistics', 'probability']"
4686355,"Let $F:[0,1] \rightarrow \mathbb{R} $ be continuous. Suppose $F$ is differentiable on (0,1), $F(0) = 0$ and $F(x)>0$ for all $x \in (0,1)$.","Let $F:[0,1] \rightarrow \mathbb{R} $ be continuous. Suppose $F$ is differentiable on (0,1), $F(0) = 0$ and $F(x)>0$ for all $x \in (0,1)$ . Prove that there exists $r,s \in (0,1)$ such that $r + s = 1$ and $8\frac{F'(r)}{F(r)} = 5\frac{F'(s)}{F(s)}$ It seems that this question requires the mean-value theorem. I considered a function G(x) = F(x)F(1-x) and proved the case $\frac{F'(r)}{F(r)} = \frac{F'(s)}{F(s)}$ , but for the case involving two different constants. I have no clue. Really thanks for any help or advice.","['analysis', 'real-analysis', 'calculus', 'mean-value-theorem', 'derivatives']"
4686383,A Riemann surface $X$ is hyperelliptic iff there is a holomorphic involution of $X$ that has exactly $2g(X)+2$ fixed points,"I’d like to show that a Riemann surface $X$ is hyperelliptic iff there is a holomorphic involution $\tau : X \rightarrow X$ such that $\tau$ has exactly $2g+2$ fixed points, where $g$ is the genus of $X$ . (I’m working with the following definition of a hyperelliptic Riemann surface: $X$ is hyperelliptic iff there is a holomorphic degree $2$ map $p:X \rightarrow \mathbb{CP}^1$ ). My thoughts so far (please tell me if this is the correct idea): Assuming $X$ is hyperelliptic, I was thinking of defining $\tau$ such that $p^{-1}(z)=\{x,\tau(x)\}$ for all $z \in \mathbb{CP}^1$ . That way, $\tau$ would be an involution of $X$ that would fix the ramification points of $p$ . By using Riemann-Hurwitz, I found $p$ would need to have $2g+2$ ramification points, so that seems to work. I’m not sure how to show that $\tau$ would have to be holomorphic, though. For the other direction, I don’t really know how to proceed… any hint would be helpful.","['riemann-surfaces', 'algebraic-geometry']"
4686418,"Show that the function $f(x, y) = x^2 + y^2$ is convex","I have to show that the function $f(x, y) = x^2 + y^2$ is convex so I am thinking to demonstrate that the graph of the function lies above any of its tangent planes.But I want to prove it with linear algebra perspective. To do this, i used the definition of a convex function in terms of its Hessian matrix. The Hessian matrix of a function with two variables is a $2 \times 2$ matrix that contains its second partial derivatives. If the Hessian matrix is positive semidefinite then the function is convex.Am I correct ? To find the Hessian matrix of $f(x, y) = x^2 + y^2$ , i need to compute its second partial derivatives: \begin{align*}
\frac{\partial^2 f}{\partial x^2} &= \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}\right) \\
&= \frac{\partial}{\partial x}\left(2x\right) \\
&= 2
\end{align*} \begin{align*}
\frac{\partial^2 f}{\partial y^2} &= \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}\right) \\
&= \frac{\partial}{\partial y}\left(2y\right) \\
&= 2
\end{align*} \begin{align*}
\frac{\partial^2 f}{\partial x\partial y} &= \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}\right) \\
&= \frac{\partial}{\partial x}\left(0\right) \\
&= 0
\end{align*} \begin{align*}
\frac{\partial^2 f}{\partial y\partial x} &= \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right) \\
&= \frac{\partial}{\partial y}\left(0\right) \\
&= 0
\end{align*} Therefore, the Hessian matrix of $$f(x, y) = x^2 + y^2$$ is: $$H = \begin{pmatrix}
2 & 0 \\
0 & 2\\
\end{pmatrix}$$ To show that the Hessian matrix of $f(x, y) = x^2 + y^2$ is positive semidefinite, i need to demonstrate that for any vector $x = [u, v]^T$ , the quadratic form $x^T H x$ is non-negative: \begin{align*}
x^T H x &= \begin{bmatrix} u & v \end{bmatrix}^T \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} \begin{bmatrix} u \ v \end{bmatrix} \\
&= \begin{bmatrix} 2u & 2v \end{bmatrix} \begin{bmatrix} u \ v \end{bmatrix} \\
&= 2u^2 + 2v^2 \\
&= 2(x_1^2 + x_2^2)
\end{align*} Since $x^T H x$ is equal to 2 times the sum of squares of $x$ 's components, it is non-negative for any choice of $x$ . Therefore, the Hessian matrix of $f(x, y)$ is positive semidefinite and therefore the function is convex.Am I correct ?","['multivariable-calculus', 'functions', 'solution-verification', 'convex-analysis']"
4686421,Can optional stopping hold for $\mathcal{L}^1$ bounded martingales?,"As part of a larger problem I am trying to figure out if it possible for a $\mathcal{L}^1$ -bounded martingale $(M_n)_{n\geqslant 1}, M_0=0$ to satisfy the following: For some almost surely finite stopping time $\tau$ , we have $\mathbb{E}[M_\tau]>0$ . I believe it is not possible since bounded in $\mathcal{L}^1$ implies convergence to some integrable $M_\infty$ , and so for an a.s. finite stopping time $\tau$ we can write $$
\mathbb{E}[M_\tau]=\mathbb{E}[M_\tau \mathbb{1}_{\{\tau < \infty\}}]+ \mathbb{E}[M_\infty]
$$ We know the second term on the RHS is $0$ since it equals the expectation of $M_0$ , and the first term on the RHS will be the expectation of some random variable in the sequence which is again that of $M_0$ . Is this correct? I am concerned that it is not true since I cannot seem to find a variant of optional stopping that would be satisfied for almost surely finite stopping times and boundedness in $\mathcal{L}^1$ . Perhaps this is a case to show that the key deduction of optional stopping is not an if statement? EDIT: As mentioned in the comment, I was mistaken in thinking that the martingale property holds the limit, which it doesn't in this case.","['martingales', 'stopping-times', 'probability-theory']"
4686422,How would I have guessed to rearrange and use the fact that any number squared is positive in this induction proof in Abbott's Understanding Analysis?,"In Abbott's Analysis, there's a question here: Exercise 2.4.5 (Calculating Square Roots). Let $x_1=2$ , and define $$
> x_{n+1}=\frac{1}{2}\left(x_n+\frac{2}{x_n}\right) $$ (a) Show that $x_n^2$ is always greater than or equal to 2 , and then use this to
prove that $x_n-x_{n+1} \geq 0$ . Conclude that $\lim x_n=\sqrt{2}$ . And (part) of the answer here: Exercise 2.4.5. (a) We first observe that a simple induction argument
shows that $x_n$ is positive for all $n$ . We can also write $$
> x_{n+1}^2-2=\left(\frac{1}{2}\left(x_n+\frac{2}{x_n}\right)\right)^2-2=\frac{x_n^2}{4}+\frac{1}{x_n^2}-1=\left(\frac{x_n}{2}-\frac{1}{x_n}\right)^2
> \geq 0 $$ as any number squared is positive. This shows that $x_n^2
> \geq 2$ for all choices of $n$ . ... more stuff When I was solving it, I kept trying to prove it by just saying saying okay $x_1^2 >=2$ so if we assume $x_n^2 >=2$ then to prove that implies $x_{n+1}^2 >=2$ we need to first expand out $x_{n+1}^2$ squared like so: $$
x_{n+1}^2=\frac{1}{4}\left(x_n+\frac{2}{x_n}\right)^2=\frac{x_n^2}{4}+\frac{1}{x_n^2}+1$$ and then start plugging in our $x_{n+1}^2 >=2$ so then $$x_{n+1}^2= \frac{x_n^2}{4}+\frac{1}{x_n^2}+1>=\frac{1}{2}+\frac{1}{x_n^2}+1 >= \frac{3}{2}+\frac{1}{x_n^2}$$ so then I'd look at it and think okay now all we need is to prove that $\frac{1}{x_n^2} >= \frac{1}{2}$ and then we will have $x_{n+1}^2 >= 2$ like we want. So then I'd search for something in the form $x_n^2 < 2$ But of course this can't be true, since $x_1^2$ is already = 4 which is not < 2 And I know that doesn't disprove anything. That of course $x_{n+1}^2$ could still be more than 2 because I used the >= replacement too haphazardly/early after all, but this is the path my brain goes down and I want to figure out how to think the way I need to instead D:","['algebra-precalculus', 'proof-writing', 'soft-question', 'analysis']"
4686443,Injectivity of map on sheaves $f^\sharp : \mathcal{O}_X \to f_*\mathcal{O}_Y$. Hartshorne $2.18$.,"Let $\varphi : A \to B$ be a homomorphism of rings, and let $f: Y = \operatorname{Spec} B \to X = \operatorname{Spec} A$ be the induced morphism of affine schemes. Show that $\varphi$ is injective if and only if the map of sheaves $f^\sharp : \mathcal{O}_X \to f_* \mathcal{O}_Y$ is injective. Been having trouble with this for a while now. It is a problem from the book of Hartshorne. If we start with assuming $\varphi$ to be injective, then in order to show that the map on sheaves is injective it is satisfactory to prove that the map on stalks $$f^\sharp_\mathfrak{p} : \mathcal{O}_{\operatorname{Spec}A, f(\mathfrak{p})} \to \mathcal{O}_{\operatorname{Spec}B, \mathfrak{p}} $$ is injective for $\mathfrak{p} \in \operatorname{Spec} B$ . Now few remarks here. I've noticed that $\mathcal{O}_{\operatorname{Spec}A, f(\mathfrak{p})} = A_{f(\mathfrak{p})}$ and that $\mathcal{O}_{\operatorname{Spec}B, \mathfrak{p}}  = B_\mathfrak{p}$ . We also have that $f(\mathfrak{p})=\varphi^{-1}(\mathfrak{p})$ so in essence we are trying to show that $$A_{\varphi^{-1}(\mathfrak{p})} \to B_\mathfrak{p}$$ is injective given that $\varphi$ is. The issue comes here since apparently this is not true. Here is a link to a thread here which shows that there is a counterexample. It's likely that I'm doing something wrong here, but I cannot figure out what so any help is welcome.",['algebraic-geometry']
4686455,Reference request to the theory and examples of the spectrum of the Laplace-Beltrami operator on different compact manifolds,"In virtually all books, lecture notes, and articles I've encountered, it is taken as a fact that the Laplace-Beltrami operator $\Delta$ has a discrete spectrum on compact Riemannian manifolds. Some texts attempt to reference supporting material, but the cited resources are often so extensive that a beginner would need to read hundreds of pages to verify whether the reference indeed supports the claim. Consequently, I am seeking helpful guides that can walk me through the necessary theory of eigenfunctions and eigenvalues of the Laplace operator: i) with simple examples in a Euclidean setting (no need to go beyond $\mathbb{R}^2$ ), and ii) on Riemannian manifolds (involving the Laplace-Beltrami operator). Essentially, I am looking for a text which covers discussions such as 1.) What is spectrum for Laplacian in $\mathbb{R}^n$? and 2.) Spectrum of the Laplace operator with Neumann condition on intervals by introducing/referring to necessary theory and shows some examples of computing the said spectrum with some example boundary-initial value problems. Additionally, I would like to comment that the post: What is spectrum for Laplacian in $\mathbb{R}^n$? offers useful insights into what I am interested in: understanding when and why the spectrum of $\Delta$ is what it is, and how we interpret $\Delta$ as an operator. However, I would prefer to refer to established sources, and the comments in the post do not delve into the depth I desire. Edit: Rosenberg's book, The Laplacian on Riemannian manifold , seems to have some components which I am seeking: The book has a strong start with the basic examples, but then jumps, understandably, to heavier machinery with Hodge theory. I have no need for Hodge theory as of now. I am satisfied with a example rich text which takes the time to look at the eigenvalue-function problem with different domains and different boundary conditions.","['reference-request', 'laplacian', 'functional-analysis', 'partial-differential-equations', 'spectral-theory']"
4686471,Naïve definition of a measure on a fractal,"Let $K\subset \mathbb R^2$ be a compact fractal of Hausdorff dimension $1<d<2$ . I want to define a natural measure on $K$ . One option would be to use the so-called Hausdorff measure $\mathcal H^d$ . Where, for every $A\subset K$ , $$ \mathcal H^d(A) = \lim_{\delta\to 0} \inf \left\{\sum _{i=1}^{\infty }(\operatorname {diam} U_{i})^{d}:\bigcup _{i=1}^{\infty }U_{i}\supseteq S,\ \operatorname {diam} U_{i}<\delta \right\}.$$ Alternatively, given $ε >0$ , one could define $K_\varepsilon =\{z\in\mathbb R^2, \operatorname{dist}(z, K) \leq \varepsilon\}$ , and define $$\mu_\varepsilon(\mathrm d x) = \frac{\mathbf{1}_{K_\varepsilon} (x)}{\lvert K_\varepsilon\rvert} \mathrm{d}x,  $$ where $\mathbf{1}_{K_\varepsilon} $ is the indicator function of $K_\varepsilon$ and $\lvert K_\varepsilon\rvert$ is the Lebesgue measure of $K_\varepsilon$ . Question: Is there any relation between the weak ${}^*$ accumulation points of $\{\mu_\varepsilon\}_{\varepsilon >0}$ (as $\varepsilon \to 0)$ and the Hausdorff measure $\mathcal H^d$ ? I could not find any book/paper that addresses this question. I am particularly interested in the case where $K$ is a Julia set $J_c = \partial\{z\in\mathbb C; p_c^n(z) \not \to \infty\ \ \text{as }n\to\infty\},$ where $p_c(z) = z^2 +c$ , for some $c$ in the Mandelbrot set (it is ok to assume $c$ hyperbolic).","['measure-theory', 'hausdorff-measure', 'dimension-theory-analysis', 'fractals']"
4686473,Concavity of a function - issue,"I've a trouble in an economic application. Let me put the question in the simplest way by avoiding any economic content. Consider the following function $g(x)$ , with $g'(x)>0$ and $g''(x)<0$ , where $g: R_{+} \mapsto R_{+}$ . Then, with $x^\ast$ being the unique fixpoint of $g(.)$ , why $\frac{g'(x^\ast)x^\ast}{g(x^\ast)}$ defines the degree of concavity of $g(x)$ ?","['functions', 'derivatives']"
4686487,"Solving $\frac{\partial}{\partial t} f =hf+ h \int \mathrm {d} i\, h f$","I'm looking for the solution of partial differential equation $$\frac{\partial}{\partial t} f(i,t) =\left(a f(i,t) + b\int_0^\infty \mathrm {d} i\, h(i) f(i,t)\right)h(i)$$ Where $$f(i, 0)=1, \int_0^\infty\mathrm {d} i\, h(i)=1$$ Interesting special case: $h(i)=(i+1)^{-2}$ , $a=-2$ , $b=1$ , want to know $g(t)=\int_0^\infty \mathrm{d}i\ h(i)f(i,t)$ . Background: $g(t)$ gives effect on environment of bird population at time $t$ under continuous version of bird population dynamics model described here","['integral-equations', 'laplace-transform', 'ordinary-differential-equations', 'integro-differential-equations']"
4686489,Convergence of series $\sum_n|b_n g(nx+a_n)|$ in a set of positive measure implies convergence of $\sum_n|b_n|$,"This problem is in a set of old past qualifying tests I am using for practice. Suppose $g$ is bounded measurable function of period $T$ with $m(x:|f(x)|>0)>0$ . Let $(a_n)$ and $(b_n)$ numerical sequences and suppose that $G(x)=\sum_n|b_n g(nx+a_n)|$ converges for all points $x$ in a set of positive (Lebesgue) measure. Show that $\sum_nb_n$ converges absolutely. I define $E=\{x\in\mathbb{R}: G(x)<\infty\}$ and $E_k=\{x\in \mathbb{R}: G(x)\leq k\}$ . Then $E=\bigcup^\infty_{k=1}E_k$ which implies that there is $k'$ such $m(E_{k'})>0$ . From this I obtain that \begin{align*}
\int_{E_{k'}}G(x)\,dx=\sum_n|b_n|\int_{E_{k'}}|g(nx+a_n)|\,dx<k'm(E_{k'})
\end{align*} This is as far I can go. I will appreciate any hint.","['integration', 'lebesgue-integral', 'analysis', 'real-analysis']"
4686536,Show that the polynomial $P(x):=x^4-6x+6$ has no real roots .,"Show that the polynomial $$P(x):=x^4-6x+6$$ has no real roots. We need to solve this problem without using calculus. This is a problem from my son's olympiad textbook.
Since the degree of the polynomial is $4$ , our task seems difficult.  I tried to factor the polynomial.  For example, I was expecting an expression like $(x^2+1)(x^2+x+1)$ .  But unfortunately it didn't. Rational root theorem obviously fails. Because, real roots don't exist. I tried to rewrite the polynomial $$x^4-6x+6=(x^2+ax+b)(x^2+cx+d)$$ So, it sufficies to show that $a^2-4b<0$ and $c^2-4d<0$ . But, I am not sure, is it good track or not. Thanks for advance .","['contest-math', 'algebra-precalculus', 'quartics']"
4686566,"If $(t_n u_n-t_m u_m)(u_n-u_m) \le 0$ for all $m, n \in \mathbb N$, then $(u_n)$ is convergent","I'm trying to solve below exercise, i.e., Let $(u_n) \subset \mathbb R$ and $(t_n) \subset \mathbb R_{>0}$ such that $(t_n)$ is non-decreasing. Assume that $$
(t_n u_n-t_m u_m)(u_n-u_m) \le 0
\quad \forall m, n \in \mathbb N.
$$ Then $(u_n)$ is convergent. Could you have a check on my below attempt? Is there a more direct approach? Thank you so much for your elaboration! We have $$
\begin{aligned}
& (t_{n+1} u_{n+1}-t_n u_n)(u_{n+1}-u_n) \le 0 \\
\iff & (u_{n+1}-u_n)^2 \le \frac{t_{n} - t_{n+1}}{t_{n+1}} (u_{n+1}-u_n) u_n.
\end{aligned}
$$ If $t_{n} - t_{n+1}=0$ then $u_{n+1} = u_n$ . Let $t_{n} - t_{n+1}<0$ . Because $(t_n)$ is non-decreasing, $\frac{|t_{n} - t_{n+1}|}{t_{n+1}} \le 1$ and thus $|u_{n+1}-u_n| \le  |u_n|$ . We have three cases, i.e., If $u_n=0$ then $u_{n+1}=0$ . If $u_n>0$ then $u_{n+1} \le u_n$ . On the other hand, $|u_{n+1}-u_n| \le  |u_n|$ . So $0 \le u_{n+1} \le u_n$ . If $u_n<0$ then $u_{n+1} \ge u_n$ . On the other hand, $|u_{n+1}-u_n| \le  |u_n|$ . So $u_n \le u_{n+1} \le 0$ . It follows that if $u_0=0$ , then $u_n=0$ for all $n$ . if $u_0>0$ , then $(u_n) \subset \mathbb R_{>0}$ is decreasing. if $u_0<0$ , then $(u_n) \subset \mathbb R_{<0}$ is increasing. Thus $(u_n)$ is convergent.","['alternative-proof', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4686599,"Why are the improper integrals $\int_{-\infty}^{\infty}x\,\mathrm{d}x$ and $\lim\limits_{n\to\infty} \int_{-n}^{n}x\,\mathrm{d}x$ different?","Why are the improper integrals $$\int_{-\infty}^{\infty}x\,\mathrm{d}x \qquad\text{and}\qquad \lim_{n\to\infty} \int_{-n}^{n}x\,\mathrm{d}x$$ different? This is question #11.4 from the 2015 ANPEC Exam. It's originally presented with an equality and the official answer is ""False"". There is no further context on the solution, so I'd like some help with the intuition behind this conclusion. The full exam can be accessed here.","['integration', 'improper-integrals', 'calculus', 'infinity', 'limits']"
4686692,"Prove that if $S$ is a partition of $A$ and $E_S$ is the corresponding equivalence, then $A/E_S = S$.","Studying with the book Introduction to Set Theory by Karel Hrbacek, I encountered this proposition: If $S$ is a partition of $A$ and $E_S$ is the corresponding equivalence, then $A/E_S = S$ . Where $$E_S=\{\left \langle a,b \right \rangle \in A \times A : (\exists C \in S)(a,b \in C)\}$$ $$A/E_S = \{[a]_{E_S} : a \in A \}  $$ I tried to do the proof, but would like to know if the first part is correctly done. Also, regarding the second part, I wonder if it is possible to conclude $X=C$ . I have thought of something different like: ... by reflexivity we know that $a \space E_S \space a$ iff $a \in X$ , so it must follow that $X \in S$ . But I have doubts whether or not this is a correct reasoning. Any help is appreciated. Proof . Assume $S$ is a partition of $A$ and $E_S$ is its corresponding equivalence. ( $\subseteq$ ). Let $X \in S$ . Then $X$ is a non-empty subset of $A$ , so $a \in X$ for some $a \in A$ . Hence $a \space E_S \space a$ , so it follows that $a \in [a]_{E_S}$ , which means $X \subseteq [a]_{E_S}$ . Therefore $X$ is an equivalence class modulo $E_S$ . Consequently $X \in A/E_S$ . ( $\supseteq $ ). Let $X \in A/E_S$ . Then $X$ is an equivalence class modulo $E_S$ . Hence $X=[a]_{E_S}$ for some $a \in A$ , by reflexivity we know that $a \space E_S \space a$ , so $a \in X$ . But also $a \space E_S \space a$ , implies there exists some $C \in S$ for which $a \in C$ holds. ...","['elementary-set-theory', 'solution-verification']"
4686699,"If $(\gcd(a,b), \gcd(b,c), \gcd(c,a))$ form a triangle, what is the probability that $(a,b,c)$ also form a triangle?","Let $A_N$ be set of triplets $(\gcd(a,b), \gcd(b,c), \gcd(c,a))$ , $1\le a \le b \le c \le N$ such that $(\gcd(a,b), \gcd(b,c), \gcd(c,a))$ are the sides of a triangle. Let $B_N$ be the set of triplets $(a,b,c)$ , $1\le a \le b \le c \le N$ , $(\gcd(a,b), \gcd(b,c), \gcd(c,a)) \in A_N$ such that $(a,b,c)$ are the sides a triangle. Let $A(N)$ and $B(N)$ be the number of elements in $A_N$ and $B_N$ respectively. Is the following conjecture true? $$
\lim_{n \to \infty} \frac{A(N)}{B(N)} = \frac{1}{2}
$$ Experimental evidence :
Out the the first $1,012,270,546$ triplets where $(\gcd(a,b), \gcd(b,c), \gcd(c,a))$ formed a triangle, there were $505,222,520$ triplets of $(a,b,c)$ which also formed a triangle, giving a ratio of $0.499098$ . Increasing the range for the first $24,292,129,662$ triplets $(\gcd(a,b), \gcd(b,c), \gcd(c,a))$ increased the ratio only slightly to $0.499697$ .Thus experimental data indicates that the ratio is possibly $\displaystyle \frac{1}{2}$ . Can this be proved? Note : The converse is different i.e. if $(a,b,c)$ form a triangle then the probability that $(\gcd(a,b), \gcd(b,c), \gcd(c,a))$ also form a triangle is about $0.345$ (not sure if thi converges to a well known constant). Code import math as mt
a = 1
t = tg = ttg = 0

while True:
    b = 1
    while b <= a:
        c = 1
        while c <= b:
            ab = mt.gcd(a,b)
            bc = mt.gcd(b,c)
            ca = mt.gcd(c,a)
                
            if ab+bc>ca and bc+ca>ab and ca+ab>bc:
                tg = tg+1
                if a+b>c and b+c > a and c+a>b:
                    t = t + 1
            c = c + 1
        b = b + 1
    
    print(a,tg,t,t/tg)
    
    a = a + 1","['divisibility', 'number-theory', 'elementary-number-theory', 'triangles', 'probability']"
4686733,"Determining the type of conic from the parameterization $(t^2+t+1,t^2-t+1)$","In the question the parametric coordinate of a conic is given by $(t^2+t+1,t^2-t+1)$ In questions which are similar to this I would just go on by writing ""X=x coordinate expression"" and similarly ""Y = y coordinate expression"" and then get value of ""t"" in terms of x and y after which i would equate both the values of ""t"" to get the locus expression from where determining conic type is easy. But in this expression the parametric is in quadratic i am unable to figure out how to proceed. Thanks in advance","['algebra-precalculus', 'conic-sections']"
4686761,Calculate $\int _0^1\frac{\arcsin ^2\left(x\right)\ln \left(x\right)\ln \left(1-x\right)}{x}\:\mathrm{d}x$,"this integral got posted on a mathematics group by a friend $$I=\int _0^1\frac{\arcsin ^2\left(x\right)\ln \left(x\right)\ln \left(1-x\right)}{x}\:\mathrm{d}x$$ I tried seeing what I'd get from integrating by parts which got me $$I=\frac{1}{2}\int _0^1\frac{\ln ^2\left(x\right)\arcsin ^2\left(x\right)}{1-x}\:\mathrm{d}x-\int _0^1\frac{\ln ^2\left(x\right)\arcsin \left(x\right)\ln \left(1-x\right)}{\sqrt{1-x^2}}\:\mathrm{d}x$$ and these seem quite hard to evaluate, I also tried expanding the integral in series but got nowhere. I also tried using expressing $I$ the following way $$I=\frac{1}{2}\int _0^1\frac{\arcsin ^2\left(x\right)\ln ^2\left(x\right)}{x}\:\mathrm{d}x+\int _0^1\frac{\arcsin ^2\left(x\right)\ln ^2\left(1-x\right)}{x}\:\mathrm{d}x$$ $$-\frac{1}{2}\int _0^1\frac{\arcsin ^2\left(x\right)\ln ^2\left(\frac{x}{1-x}\right)}{x}\:\mathrm{d}x$$ and the first $2$ integrals are probably doable but I can't think of a way to approach the last one. Is it possible to evaluate $I$ or the third integral on the last equation in a fairly simple way? Edit:
By Dilogarithm functional equations the following are also related $$\int _0^1\frac{\arcsin ^2\left(x\right)\operatorname{Li}_2\left(x\right)}{x}\:\mathrm{d}x,\:\int _0^1\frac{\arcsin ^2\left(x\right)\operatorname{Li}_2\left(1-x\right)}{x}\:\mathrm{d}x$$ and therefore some very rough hamornic series are involved.","['definite-integrals', 'real-analysis', 'harmonic-numbers', 'calculus', 'polylogarithm']"
4686772,Monochromatic equilateral triangles in a 2-colored circle,"Problem. Suppose every point of a circle (with a fixed radius) has been colored either red or blue. Does there exist an equilateral triangle whose 3 vertices are on the circle and share the same color? In more brief terms, does there exist a monochromatic inscribed equilateral triangle inside a $2$ -colored circle? Context. When the conclusion is weakened so that we ask for a monochromatic isosceles triangle instead of an equilateral triangle, the problem is much easier to solve: we can take a regular pentagon inscribed inside the circle. By the pigeonhole principle, three of the five vertices are colored same: those three vertices clearly form an isosceles triangle. Thank you for your time!","['problem-solving', 'circles', 'geometry', 'coloring']"
4686781,"Question on Permutation and Combinations. 6 people are to be accommodated in 4 hotel rooms, provided each room has at least 1 person and Max 2.","Complete Question:
In a hotel, four rooms are available. Six persons are to be accommodated in these four rooms in such a way  that each of these rooms contains at least one person and at most two persons. Then the number of all  possible ways in which this can be done is? I am not getting answer using my method, kindly tell me where am I going wrong:
Number of ways of selecting 4 people out of 6= 15.
Now arranging those 4 people in 4 rooms= 4!=24
Now remaining 2 people.
To select any 2 random rooms out of 4= 6ways
Now arranging the 2 people in 2 rooms is 6ways * 2= 12.
So final answer according to me: 15 * 24 * 12= 4320.
But the correct answer given in the solution is 1080. Where am I going wrong please explain.","['permutations', 'combinations', 'combinatorics']"
4686794,Prove that $\frac{a}{2a+b^2}+\frac{b}{2b+c^2}+\frac{c}{2c+a^2}\le1$ where $ab+bc+ca=3$,"Let $a,b,c$ be positive real numbers such that $ab+bc+ca=3$ . Prove that $$\frac{a}{2a+b^2}+\frac{b}{2b+c^2}+\frac{c}{2c+a^2}\le1$$ Source from: AwesomeMath summer program 2022, Algebra 3.5, test2. I once tried to make it homogeneous via $\sqrt{3}=\sqrt{ab+bc+ca}$ but this only makes it much more complicated
Then I made some progress using Chebyshev's inequality as following: Switching it into greater or equal by $$\frac{a}{2a+b^2}=\frac{1}{2}-\frac{b^2}{2(2a+b^2)}$$ And thus $$\sum_{cyc}\frac{b^2}{2a+b^2}=\sum_{cyc}(a+b+c)(\frac{b}{a+b+c}\cdot\frac{b}{2a+b^2})\ge1\dots\boxed{0}$$ Then I use weighted Chebyshev's Inequality, seeing $\frac{a}{a+b+c},\frac{b}{a+b+c},\frac{c}{a+b+c}$ as weights $$(\sum_{cyc}\frac{b}{a+b+c})(\sum_{cyc}\frac{b}{a+b+c}\cdot\frac{1}{2a+b^2}\cdot b)\ge(\sum_{cyc}\frac{b}{a+b+c}\cdot\frac{1}{2a+b^2})(\sum_{cyc}\frac{b}{a+b+c}\cdot b)\dots\boxed{1}$$ And by Chebyshev's inequality $$3(\sum_{cyc}\frac{b}{a+b+c}\cdot\frac{1}{2a+b^2})\ge(\sum_{cyc}\frac{b}{a+b+c})(\sum_{cyc}\frac{1}{2a+b^2})=\sum_{cyc}\frac{1}{2a+b^2}\dots\boxed{2}$$ Combing $\boxed{1}$ and $\boxed{2}$ $$\sum_{cyc}\frac{b^2}{2a+b^2}\ge\frac{a^2+b^2+c^2}{a+b+c}\cdot\frac{1}{3}\sum_{cyc}\frac{1}{2a+b^2}$$ In order to reach $\boxed{0}$ , the only thing left to be proved is that $$(a^2+b^2+c^2)(\frac{1}{2a+b^2}+\frac{1}{2b+c^2}+\frac{1}{2c+a^2})\ge3$$ Here is where I stuck, though I have no idea how to apply the criteria $ab+bc+ca=3$ for further process.","['algebra-precalculus', 'inequality']"
4686840,"If $x_n \to x$ and $y_n \to y$ weakly. Then $\langle x, y\rangle = \lim_n \langle x_n, y_n\rangle$","I'm trying to show below property, i.e., Let $(H, \langle \cdot, \cdot\rangle)$ be a real Hilbert space. Let $x,y, x_n, y_n\in H$ such that $x_n \xrightarrow{n \to \infty} x$ and $y_n \xrightarrow{n \to \infty} y$ in the weak topology $\sigma(H, H^*)$ . Then $\langle x, y\rangle = \lim_n \langle x_n, y_n\rangle$ . Could you have a check on my below attempt? Is there another way that does not use below lemma? Thank you so much for your help! Let $\sigma (H, H^*) \otimes \sigma (H, H^*)$ be the product topology of $\sigma (H, H^*)$ and itself. Lemma $\sigma (H, H^*) \otimes \sigma (H, H^*)= \sigma (H^2, (H^2)^*)$ . By above Lemma , $(x_n, y_n) \xrightarrow{n \to \infty} (x, y)$ in $\sigma (H^2, (H^2)^*)$ . Clearly, the map $(u, v) \mapsto \langle u, v \rangle$ belongs to $(H^2)^*$ . The claim then follows.","['weak-convergence', 'hilbert-spaces', 'alternative-proof', 'functional-analysis', 'weak-topology']"
4686851,"Assume $\langle t_n u_n-t_m u_m, u_n-u_m \rangle \le 0$ for all $m, n \in \mathbb N$. How to show that $(u_n)$ is convergent in $\mathbb R^d$?","Let $\langle \cdot, \cdot\rangle$ be the canonical inner product on $\mathbb R^d$ . I'm trying to solve below exercise, i.e., Let $(u_n) \subset \mathbb R^d$ and $(t_n) \subset \mathbb R_{>0}$ such that $(t_n)$ is non-decreasing. Assume that $$
\langle t_n u_n-t_m u_m, u_n-u_m \rangle \le 0
\quad \forall m, n \in \mathbb N.
$$ Then $(u_n)$ is convergent in $\mathbb R^d$ . I'm able to solve the exercise in case $d=1$ . My proof relies on the ordering of $\mathbb R$ . Could you elaborate on how to tackle the case $d > 1$ ?","['convergence-divergence', 'multivariable-calculus', 'sequences-and-series', 'real-analysis']"
4686871,Find sum of series $\sum_{n=1}^{\infty}\frac{\cos(nx)}{2n - 1}$,"Given series $$\sum_{n=1}^{\infty}\frac{\cos(nx)}{2n-1}$$ how can we find the sum of given series? Update: This task is from calculus workbook we got in class, so I can't give a valid source where the task came from.
I've tried to change $$\cos$$ to $$z^n$$ so I would have a power series and it would be possible to create a geometric progression using it, and then I would find the real part of it. Unfortunately, I can't implement this idea because after that I don't know how to get rid of complex numbers","['complex-analysis', 'summation', 'complex-numbers', 'sequences-and-series']"
4686909,Why is $E[X_{A \text{ or } B}]$ equal to half the harmonic mean of $E[X_A]$ and $E[X_B]$?,"[In the title I was attempting to reference a relationship similar to that of two resistors in parallel, i.e. $\dfrac{1}{R} = \dfrac{1}{R_1} + \dfrac{1}{R_2}$ ; if there's a name for such a relationship please let me know and I'll edit the title.] This question arose from an answer I recently wrote to a dice-rolling question; I'll add a link to it at the bottom but here's a a quick summary: The original question asked what the expected number of rolls of a fair 6-sided die would be until one gets either a $1$ or two consecutive $6$ 's; it also noted that the expected numbers of rolls for each of those two events considered separately are $6$ and $42$ respectively. As it turned out, the answer to that question was $\dfrac{21}{4}$ , which struck me as being somehow related to the two previously-mentioned expected values, and pretty quickly a surprising (to me, anyway) connection popped up. Let $X_A$ be the number of rolls taken until a $1$ appears, $X_B$ the number of rolls until two consecutive $6$ 's appear, and $X_{A \text{ or } B}$ the number of rolls until either a $1$ or two consecutive $6$ 's appear. Then it turned out that $$\frac{1}{\mathbb{E}[X_{A \text{ or } B}]}=\frac{1}{\mathbb{E}[X_A]}+ \frac{1}{\mathbb{E}[X_B]}$$ Trying to make sense of this, the first thing that came to mind was that if somehow these reciprocals of expected values could be thought of as probabilities, then the equation would essentially be $\Pr(A \text{ or } B)$ for mutually exclusive events, which these particular $A$ and $B$ are. The only such relationship I know of is for geometric probability distributions, where we have $\mathbb{E}[X] = \dfrac{1}{p}$ ; the probability distribution of the number of rolls until a $1$ appears is of course geometric, but I can't see how either of the other two are. Despite that, I tried to make sense of $\frac{1}{42}$ as a probability related to rolling two consecutive $6$ 's and couldn't; I also checked whether there was some property of expected values I was unfamiliar with that would explain this but came up empty. So is there an explanation for this relationship between these expected values? [Here's the link to the original question .]","['expected-value', 'probability']"
4686978,On The Construction of Parabolas,"I am not aware of any other geometry other than Euclidean. Correction: I am aware of other geometries, however I am not aware of their axioms, theorems, or applications. I only know that they disregard the 5th, parallel postulate; that is all. I have, however, considered the construction of the parabola using only a rule and compass (Euclidean postulates). I soon found it quite the cumbersome activity. Is seems impossible to construct a parabola, for reasoning coordinate geometrically: a parabola is a figure that has the property that it's $y$ coordinate continually increases quadratically while it's $x$ coordinate increases linearly. Such a property doesn't seem constructible using a rule (which could construct loci with both coordinates increasing linearly) and a compass (which could construct loci with both coordinates increasing quadratically). Does this mean that the parabola doesn't exist in a Euclidean plane? Same goes for the hyperbola, or other loci where one coordinate increases at the rate $= n$ and the other coordinate at the rate $= m ≠ n$ . Thank you in advance.","['euclidean-geometry', 'analytic-geometry', 'geometry']"
4686988,A special class of integrals: $\int_0^\infty \frac{f(x)}{1+x^2}dx$,"Consider the integral $$I=\int_0^\infty \frac{f(x)}{1+x^2}\mathrm{d}x$$ now make a change of variable to $x=\dfrac{1}{t}$ , and so $\mathrm{d}x=-\dfrac{1}{t^2}\mathrm{d}t$ .
The integral becomes $$I=\int_0^\infty \frac{f\left(\frac{1}{t}\right)}{1+t^2}\mathrm{d}t$$ and so $$I=\frac{1}{2}\int_0^\infty \frac{f(x)+f\left(\frac{1}{x}\right)}{1+x^2}\mathrm{d}x$$ Hence the question: what are the functions $f(x)$ such that the quantity $L(f)=f(x)+f\left(\frac{1}{x}\right)$ has a ""nice"" value that makes the integral easy? For example, some uninteresting cases are: $L(\ln(x))=0$ that provides $\displaystyle\int_0^\infty \dfrac{\ln(x)}{1+x^2}\mathrm{d}x=0$ $L(\arctan(x))=\frac{\pi}{2}$ that provides $\displaystyle\int_0^\infty \dfrac{\arctan(x)}{1+x^2}\mathrm{d}x=\dfrac{\pi^2}{8}$ $L(\sin(\ln(x)))=0$ then $\displaystyle\int_0^\infty \dfrac{\sin(\ln(x))}{1+x^2}\mathrm{d}x=0$ and in general every time $g(x)$ is odd and the integral still converges, then $\displaystyle\int_0^\infty \dfrac{g(\ln(x))}{1+x^2}\mathrm{d}x=0$ I wonder if there are some other special or lesser known functions that have the charateric of having a nice $L(f)$ . Tell me if you come up with something.","['integration', 'calculus']"
4686994,"In square $c^2=a^2+2b^2$, Find the angle $\alpha$",In square in the image $c^2=a^2+2b^2$ . Find $\alpha$ . I tried to solve it using law of cosines. m is the side of the square: $m^2=b^2+a^2-2abcos(\alpha)$ And if the angle of intersection of c and b is $\beta$ : $m^2=b^2+c^2-2bccos(\beta)$ And as the other angle is $360-(\alpha+\beta)$ and $cos(360-(\alpha+\beta))=cos(\alpha+\beta)$ and diagonal of square is $m\sqrt{2}$ : $2m^2=c^2+a^2-2accos(\alpha+\beta)$ If we subtract $m^2=b^2+c^2-2bccos(\beta)$ $m^2=b^2+a^2-2abcos(\alpha)$ We get: $0=c^2-a^2+2abcos(\alpha)-2bccos(\beta)$ as $c^2-a^2=2b^2$ we get $2bccos(\beta) -2abcos(\alpha)=2b^2$ $\to$ $b=ccos(\beta)-acos(\alpha)$ Then we equalize RHS of two below and simplify: $2m^2=2b^2+2a^2-4abcos(\alpha)$ $2m^2=c^2+a^2-2accos(\alpha+\beta)$ We get: $2bcos(\alpha)= ccos(\alpha+\beta)$ $\to$ $b=\frac{ccos(\alpha+\beta)}{2cos(\alpha)}$ And if we equalize RHS of two below and simplify: $b=ccos(\beta)-acos(\alpha)$ $b=\frac{ ccos(\alpha+\beta)}{2cos(\alpha)}$ We get: $2ccos(\alpha)cos(\beta)-2acos^2(\alpha)=ccos(\alpha)cos(\beta)- csin(\alpha)sin(\beta)$ $\to$ $ccos(\alpha-\beta)= 2acos^2(\alpha)$ But then I can't go any further.Can you finish my way or can you suggest maybe easier and better solution?,"['angle', 'problem-solving', 'geometry']"
4687001,Solution of inhomogenous system with fundamental matrix.,"Could anyone show me how to calculate the general solution to $$x' = \begin{pmatrix} 3 & -1 \\ 4 & -1 \end{pmatrix}x+\begin{pmatrix} 1 \\ t \end{pmatrix}$$ where $x_1(0) = 1$ and $x_2(0) = 0$ ? I get the fundamental matrix $$e^{tA} =  e^t+(A-I_2)te^{t}$$ And then I want to use a theorem that says that the general solution will be on the form $$x(t) = F(t)c+F(t) \int_{t_0}^{t} F^{-1}(\tau)b(\tau)d\tau$$ where $F(t)$ is the fundamental matrix, and c is a column-matrix dependent on the initial value given. In this case, this would be: $$x(t) = e^{tA}\begin{pmatrix} 1 \\ 0 \end{pmatrix}+e^{tA} \int_{0}^{t} e^{-\tau A}\begin{pmatrix} 1 \\ \tau \end{pmatrix} d\tau$$ But then it feels like the computations get nasty, when you substitute $$e^t+(A-I_2)te^t$$ for $e^{tA}$ . Is there anyway to simplify this?","['differential', 'ordinary-differential-equations']"
4687063,Example of an inner product space with no orthonormal basis,"Let $X$ be an infinite-dimensional vector space with an inner product $(\cdot, \cdot)$ . A system of non-zero vectors $B = \{ x_\alpha \}$ from $X$ is called orthonormal if $$
(x_\alpha, x_\beta) = 
\begin{cases}
0, & \alpha \neq \beta,\\
1, & \alpha = \beta.
\end{cases}
$$ The orthonormal system $B = \{ x_\alpha \}$ is called an orthonormal basis for $X$ if the subspace of $X$ generated by finite linear combinations of elements of $B$ is dense in $X$ . It is well-known that every separable inner product space has an orthonormal basis. What is some good example of non-separable inner product space with no orthonormal basis? I am interested in any concrete example. P.S.: The definition of orthogonal basis given includes uncountable cases as well.","['hilbert-spaces', 'inner-products', 'orthonormal', 'functional-analysis']"
4687083,Application of Banach-Alaoglu theorem to extract convergent subsequence of currents,"While reading about currents I came across the following lemma in Lectures on Geometric Measure Theory by Leon Simon on page 135: Lemma. If $\left\{T_j\right\}_{j\in\mathbb{N}}$ is a sequence of currents in $\mathcal{D}_k(U)$ such that $$\sup_{j\in\mathbb{N}}\mathbb{M}_W(T_j)<\infty$$ for all $W\subset\kern-2px\subset U$ , then there is a subsequence $\{T_{j_i}\}_{i\in\mathbb{N}}$ and a $T\in\mathcal{D}_k(U)$ such that $T_{j_i}\rightharpoonup T$ in $U$ as $i\to\infty$ . For some context, the space $\mathcal{D}_k(U)$ is the space of $k$ -dimensional currents on the open set $U\subseteq\mathbb{R}^n$ , i.e. the (topological) dual space of the space of smooth compactly supported differential $k$ -forms on $U$ , $\mathcal{D}^k(U)$ . Furthermore, for a current $T\in\mathcal{D}_k(U)$ and a set $W\subset\kern-2px\subset U$ , we have that $$\mathbb{M}_W(T)=\sup\left\{T(\omega) : \omega\in\mathcal{D}^k(U),\,\lVert\omega\rVert\leq1,\,\operatorname{supp}\omega\subseteq W\right\}$$ is the mass of $T$ in $W$ . My concern is with how the above theorem is actually proven. In particular, right after defining the notion of weak convergence of currents, i.e. that $T_j\rightharpoonup T$ in $U$ as $j\to\infty$ iff $\lim_{j\to\infty}T_j(\omega)=T(\omega)$ for all $\omega\in\mathcal{D}^k(U)$ , it is quickly mentioned that mass is lower semi-continuous with respect to weak convergence (which I can easily verify), and right after that the lemma is given, with the only ""proof"" being that it's an application of the Banach-Alaoglu theorem on the Banach spaces $$\mathcal{M}_k(W)=\left\{T\in\mathcal{D}_k(W):\mathbb{M}_W(T)<\infty\right\}$$ for $W\subset\kern-2px\subset U$ . Being very inexperienced with functional analysis, I am unable to really make any substantial progress on actually verifying this lemma. While it might be trivial for someone who is more familiar with applications of the Banach-Alaoglu theorem, the only thing I've thought to is to, for all $W\subset\kern-2px\subset U$ , let $\alpha_W>0$ be such that $$\mathbb{M}_W(T_j)\leq\alpha_W$$ for all $j\in\mathbb{N}$ , then define a new sequence $\{R^W_j\}_{j\in\mathbb{N}}$ by setting $R^W_j=\frac{T_j}{\alpha_W}$ , so that $$\mathbb{M}_W(R^W_j)\leq1.$$ If I understand Banach-Alaoglu correctly (which I might not), then I should be able to extract a convergent subsequence $\{R^W_{j_i}\}_{i\in\mathbb{N}}$ converging weakly in $W$ to some $R^W$ , from which I should be able to deduce that $T_{j_i}\rightharpoonup \alpha_W R^W$ in $W$ as $i\to\infty$ . The problem here is that each such subsequence depends on the choice of $W$ , and the subsequences only converge weakly on the respective $W$ , which is not what the lemma entails. Perhaps what I have concluded (assuming it is correct) will imply the lemma, however I doubt it. I have also tried looking for other resources containing the same lemma, however each one I was able to find simply stated that it is a consequence of the Banach-Alaoglu theorem without elaborating further.","['banach-spaces', 'geometric-measure-theory', 'functional-analysis', 'weak-convergence']"
4687104,Why are Wigner matrices the appropriate representation for rotations in physics?,"Question. Why are the Wigner-D-matrices the appropriate representation for rotation of operators with respect to space-coordinates in physics? Background. A physics paper brought up this question in my university group. Suppose for each $(x_1,\cdots,x_m) \in \mathbb{R}^{3m}$ (hence $x_i \in \mathbb{R}^3$ for every $i \in \{1,2,\cdots,m\}$ ), a matrix $H(x_1,\cdots,x_m)$ is studied that is equivariant with respect to $\text{SO}(3)$ in the following sense. If $q$ is a 3d rotation, then $$
H(qx_1,\cdots,qx_m) = \rho(q^*)H(x_1,\cdots,x_m) \rho(q).
$$ The authors say that the representation $\rho$ is a Wigner matrix. In physics terminology, $H$ is a subblock of the hamiltonian and $(x_1,\cdots,x_m)$ are the coordinates of atom nuclei (but I don't expect specialist knowledge of atomic physics is necessary to understand the question). I have not found a 'Lie-theory'-explanation of why the Wigner-D-matrices appear in contexts such as these in physics. There is an audience for an answer of this type, because someone in my group said that in physics literature, the topic is often tackled by that the reader is given a parameterization of the Wigner matrices and told to calculate, so to speak. I feel the same way, and therefore an explanation from a graduate-level (or higher) in mathematics would be helpful. Why is it the appropriate representation, and not something like the 'defining representation of SO(3)', for example (the one given by the inclusion)?","['smooth-manifolds', 'physics', 'lie-groups', 'mathematical-physics', 'differential-geometry']"
4687126,When does this wrong method of solving integrals give the right answer?!,"Basically my question is about $\int_a^c f(x)dx$ such that there is a 'singularity' (*) at $x=b$ and $a<b<c$ Assume that $\displaystyle\int f dx= F$ , my question is when is $\displaystyle\int_a^c f(x)dx = F(c)-F(a)$ For some examples, the general way to deal with stuff like this is to integrate from $a$ to $b- \epsilon$ and $b+ \epsilon$ to $c$ and sum them and take the limit as $\epsilon \to 0$ . And we need to do this otherwise we may get some absurd things, for a famous example, if we just do $F(c)-F(a)$ we get $\displaystyle\int_{-1}^1 \left(\dfrac1x\right)^2 = -2$ which is clearly false. So I thought that we would always have to do the limit thing whenever integrating around singularity, but then I did the following question: $$\int_0^{\infty} \dfrac1{x^3-1}$$ It is quite easy to find the antiderivative of this using partial fractions, (page from ""inside interesting integrals, really cool book btw) So then as there is a singularity at $1$ I did the routine and long $\epsilon$ calculations to get the final answer of $\dfrac{\pi}{\sqrt{27}}$ but then I was quite surprised to find out that this was indeed equal to $\displaystyle\lim_{x \to \infty}F(x)-F(0)$ . This is what has inspired this question,cos i would really like it if there was some way to know when I have to do these boring calculations to get the correct answer and when I do not. Finally, tldr; my question is when do we need to do the limit $\epsilon$ thing when integrating over a singularity (*)singularity meaning:(the function is not defined, more specifically, for this question, assume $f$ tends to $\pm \infty$ as $x \to b$ )","['integration', 'limits', 'calculus']"
4687134,Methods for finding and guessing closed forms of infinite series,"I want to prove $\displaystyle\sum_{k \ge 0} \Big(\frac{1}{3k+1} - \frac{1}{3k+2}\Big) = \frac{\pi}{\sqrt{27}}$ The reason for this question is I was doing the integral $\displaystyle\int_0^{\infty} \frac{1}{1-x^3}$ and I proved that this was equal to $\displaystyle\sum_{k \ge 0} \Big(\frac{1}{3k+1} - \frac{1}{3k+2}\Big)$ by proving it was equal to $\displaystyle\int_0^1 \frac{1-x}{1-x^3}$ and then using the infinite series for $\dfrac{1}{1-x}$ and swapping the order of integration and summation to get that answer. I tried for some while to write this in closed form but to no avail. So then I checked that the answer was mysteriously $\dfrac{\pi}{\sqrt{27}}$ . So my question mainly is that given the summation answer, how would you guess and prove it was equal to $\dfrac{\pi}{\sqrt{27}}$ given that you do not have access to calculators (I would appreciate if you could also point me towards some more general methods/resources to do this kind of stuff also, because i often times calculate integrals as an infinite sum but I can not find the closed form answer...) Thank you for your help","['power-series', 'calculus', 'summation', 'sequences-and-series']"
4687161,Fourier series of a particular elliptic function,"There is an established result that the Fourier expansion of a particular ratio of Jacobi elliptic theta functions: $$\frac{\theta_1(x+y)\theta_1'(0)}{\theta_1(x)\theta_1(y)} = \cot(x)+\cot(y)+4\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}q^{2mn}\sin(2my+2nx)$$ where $|\Im(x)| <|\Im(\pi\tau)|$ and $|\Im(y)| <|\Im(\pi\tau)|$ c.f. Whittaker and Watson 5th edition page 515 Example 21.13. One can evaluate the integrals in $x$ and $y$ for the Fourier coefficient by considering contour integrals about the parallelogram in the complex plane with vertices at $\left\{\pm \dfrac{\pi}{2}, \pm\dfrac{\pi}{2}+\pi\tau\right\}$ and arrive at the result by computing the sum. I am interested in evaluating the Fourier expansion for the following ratio of Jacobi elliptic theta functions: $$\frac{\theta_4(x+y)\theta_1'(0)}{\theta_4(x)\theta_4(y)}=\sum_{m,n}e^{2inx+2imy}A_{m,n}$$ where $$A_{m,n} = \frac{1}{\pi^2}\int_{-\pi/2}^{\pi/2}\mathrm{d}ye^{-2imy}\int_{-\pi/2}^{\pi/2}\mathrm{d}x e^{-2inx } \frac{\theta_4(x+y)\theta_1'(0)}{\theta_4(x)\theta_4(y)}$$ I can evaluate the integral with respect to $x$ by considering the parallelogram contour as before, giving $$A_{m,n} = \frac{2i}{\pi}\int_{-\pi/2}^{\pi/2}\mathrm{d}ye^{-2imy} \frac{q^{-n}e^{-iy}}{1-q^{-2n}e^{-2iy}} \frac{\theta_1(y)}{\theta_4(y)}$$ However, if I try to compute this integral using the same method I instead generate the following recurrence relation: $$ A_{m,n} - q^{-2m}A_{m,n+1} = -4\frac{\theta_4(0)}{\theta_1'(0)} \frac{q^{-m}q^{-(n+\frac{1}{2})}}{1-q^{-(2n+1)}}$$ This seems simple, but I do not have an explicit boundary condition to solve this difference relation. I can see that the original function is symmetric under interchange of $x\leftrightarrow y$ and $x\rightarrow -x, y\rightarrow-y$ simultaneously, which may imply $A_{m,n}=A_{n,m}$ and $A_{m,n}=A_{-m,-n}$ respectively, but I can't seem to reconcile this with the difference equation. Any help would be greatly appreciated!","['complex-analysis', 'fourier-series', 'elliptic-functions', 'recurrence-relations']"
4687186,E[XY] where X and Y are the **sign functions** of standard normal distributions,"The following question is from the book: ""150 Most Frequently Asked Questions on Quant Interviews"" By Stefanica, Radoicic, and Wang. Let $X$ and $Y$ be standard normal variables with joint normal distribution with correlation $\rho$ . Find the expectation $$ \mathbb E [\text{sgn}(X )\text{sgn}(Y )]$$ where $\text{sgn}(·)$ is the sign function given by $\text{sgn}(x) = 1$ , if $x > 0$ , $\text{sgn}(x) = −1$ , if $x < 0$ , and $\text{sgn}(0) = 0$ . My Approach Now, my first instinct was to write out the formula for the correlation in terms of expectations and see where to go from there. So for readability, using $X = \text{sgn}(X)$ and $Y = \text{sgn}(Y)$ : $$ \rho_{XY} = \dfrac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y} = \dfrac{\mathbb E[XY] - \mathbb E[X]\mathbb E[Y]}{\sigma_X\sigma_Y} $$ from there, I was able to intuit that the means of $X$ and $Y$ are zero. Nothing really changes if you take the sign of a standard normal distribution instead of the value right? They're both symmetric around zero? So I simplified the above fraction to: $$ \rho_{XY} = \dfrac{\mathbb E[XY]}{\sigma_X\sigma_Y} $$ So next was to find the standard deviations, the formula for which is: $$ \sigma_X =  \sqrt{\frac{1}{n} \sum_i^n (x_i - \bar{X})^2} $$ And if we know $\bar{X} = 0$ , then the variance's for both is 1? Which leads me to the solution: $$ \mathbb E[XY] = \rho_{XY}$$ Which when I checked the book's solution, is utterly wrong. There is a deterministic solution... Why Am I Wrong? This is really what I'm trying to reach. This is practice for interviews, so I don't care about knowing the answer, I want to learn how to tackle problems such as these. Here's where I think I went wrong: the expectation is not zero? I think the correlation twists the expectation here. I think my answer only works when they are independent, which doesn't help... But I thought I did the math right? This is where I'm lost. Sanity check for why I'm wrong: If $\rho=-1$ , then $\mathbb E[XY] = -1$ If $\rho=1$ , then $\mathbb E[XY] = 1$ The solution confirms this... The Book's Solution The book has a three-page long derivation which simplifies to $\frac{\pi}{2}$ that I honestly don't understand. It starts off with assuming $\rho=1$ or $\rho=-1$ and reaching the conclusion that $E \in [-1,1]$ , which is what I have above. So at least I have that part correct... It then goes on to break down the expectation by the 2x2=4 possible combinations of values for $X,Y$ , then uses the fact that all four spaces sum up to 1 to boil down the problem into a simple formula, dependent on $\mathbb{P}[X > 0, Y> 0]$ (1.27): I don't think I would've done this, but OK. So far so good. It then assumes an identity/transformation that I've never seen before, with notation I've never seen before either (1.28): Which then uses polar coordinates and change of variables to solve the integral: At which point, I'm completely lost. This seems like a hard problem right? Definitely not one I'd see in an interview. My question is: is there a simpler way to understand how to tackle this problem? Seems like it's way too complicated. If not, is there a more visual way to understand this? I know that two standard normal RV's can be visualized in a 2D grid so that their distributions are symmetric around the origin. Not sure how that translates to when they're correlated or their sign version is The last few equations seem to imply some level of circular intuition behind the solution.. so can anyone help provide some clarity? My problem really is once I saw the identity that I didn't know I completely lost track of the problem in my head, and don't think I could've recreated the solution after that... Thanks","['normal-distribution', 'bivariate-distributions', 'correlation', 'expected-value', 'probability-theory']"
4687226,Involution on $2\times 2$ matrices,"Show that the map on $2\times 2$ matrices \begin{eqnarray} 
\left( \begin{matrix} a & b\\ c & d \end{matrix} \right)\overset{\Phi}{\mapsto} \left( \begin{matrix} a & b\\ c & d \end{matrix} \right)\cdot \left( \begin{matrix} 2 (a c - b^2) & a d - b c\\ a d - b c & 2(b d - c^2) \end{matrix} \right)^{-1} \cdot \left( \begin{matrix} 0 & 1\\ 1 & 0 \end{matrix} \right)
\end{eqnarray} is an involution, that is $\Phi\circ \Phi=\operatorname{Id}$ Notes: For convenience, one could treat $a$ , $b$ , $c$ , $d$ as independent variables, with coefficients in $\mathbb{C}$ . If we consider the form $F(x,y) =a x^3 + 3 b x^2 y + 3 c x y^2 + d$ , then the Hessian determinant is a quadratic form with matrix coefficients (up to a constant( $\left( \begin{matrix} 2 (a c - b^2) & a d - b c\\ a d - b c & 2(b d - c^2) \end{matrix} \right)$ . Its determinant is up to a constant the discriminant of the cubic form $F$ . See also the matrix $\left(\begin{matrix} a& b &c \\
b&c&d\end{matrix}\right)$ and its $2\times 2$ minors. If instead of the inverse on the second factor we consider the adjugate, then we don't have to worry about denominators. Then the composition will be a constant times the matrix. One could consider an alternate map with the third factor on RHS given by $\left( \begin{matrix} 0 & -1\\ 1 & 0 \end{matrix} \right)$ . This is connected to the covariant $t$ for cubics, and  an involution on triplets ( the roots of the cubic).  Note that the square of the map is in this case $-\operatorname{Id}$ . One can show that we have an involution by direct calculation ( RHS could be multiplied by an arbitrary fixed constant).
It would be nice to have a proof involving some shortcuts. Maybe everything is just something basic. Any feedback would be appreciated!","['cubics', 'schur-complement', 'roots-of-cubics', 'matrices', 'invariant-theory']"
4687228,Nonaffines must base change to nonaffines,"Let $F \subseteq E$ be fields, and let $X$ be an affine scheme of finite type over $E$.  There is a general problem of finding affine schemes $X_0$ over $F$ such that $X \cong_E X_0 \otimes_F E$.  For example, if $E/F$ is Galois, then the $F$-isomorphism classes of such $X_0$ are parameterized by the cohomology set $H^1(\textrm{Gal}(E/F), \textrm{Aut}_E(X))$. My question is, is it possible for there to exist a nonaffine scheme $X_0$ over $F$ such that $X \cong X_0 \times_F E$?  In general, I don't expect nonaffine schemes to become affine after base change, but I don't know. Attempt : Use Serre's affine criterion.  If $\mathcal F$ is any quasicoherent sheaf of $\mathcal O_{X_0}$-modules, then $H^1(X_0, \mathcal F) \otimes_F E = H^1(X, \pi_X^{\ast}(\mathcal F))  = 0$, where $\pi_X: X \rightarrow X_0$ is the projection.  Hence $H^1(X_0,\mathcal F) = 0$.  The problem is I don't know that $X_0$ is Noetherian, or separated.",['algebraic-geometry']
4687241,Factor $\frac{x^4 - 9x^2 + 4x + 12}{x - 2}$ using difference of cubes formula,"This is  a problem I found from Lara Alcock's How to Think About Analysis (page 161) and I've been stumped for a while. So far I have tried to factor by grouping like so: $\displaystyle\frac{x^4 - 9x^2 + 4x + 12}{x - 2}$ First considering the numerator: $x^4 - 9x^2 + 4x + 12$ $= x^2(x^2 - 3^2) + 4(x + 3)$ $= x^2(x-3)(x+3) + 4(x+3)$ I've also tried rewriting the numerator to get a difference of cubes I could factor but haven't had much success. I would also love any recommendations for books/resources to practice more algebra problems like this. This is my first post, any pointers would be much appreciated!","['algebra-precalculus', 'factoring']"
4687246,Prove $ \frac{x_1-x_2}{x_n+x_1} + \frac{x_2-x_3}{x_1+x_2}+\cdots+ \frac{x_n-x_1}{x_{n-1} +x_n}\le 0$ s.t. $x_1+\cdots+x_n=1$,"Is it true that: $$
f_n(x_1,\ldots,x_n)=\frac{x_1-x_2}{x_n+x_1} + \frac{x_2-x_3}{x_1+x_2}+\cdots+ \frac{x_n-x_1}{x_{n-1} +x_n}\le 0
$$ for $x_1,\ldots,x_n>0$ , such that $x_1+\cdots+x_n=1$ ? It is a cyclic inequality that I came up with while proving similar ones, although it may well be already solved/proposed somewhere else. Equality is attained at $x_1=\cdots=x_n=1/n$ . The first approach I’ve tried is to represent the inequality equivalently as $$
n\le \frac{x_1 +x_3}{x_1+x_2} + \frac{x_2+x_4}{x_2+x_3}+\cdots+\frac{x_n+x_2}{x_n+x_1},
$$ which is trivially true for $n=3$ by AM-GM, but not so for $n>3$ , since the terms don’t cancel. I’ve also tried the following approach: let $x_1$ be the largest wlog, then we can examine the function $$
g(t)=f_n(x_1-t, x_2 +t,\ldots,x_n) = \frac{x_1-x_2-2t}{x_n+x_1-t} + \frac{x_2+t-x_3}{x_1+x_2}+ \frac{x_3-x_4}{x_2+t+x_3} +\cdots+ \frac{x_n-x_1+t}{x_{n-1} +x_n}
$$ and see if it is increasing for small enough $t$ . Then we can proceed by generating sequences that approach the average while increasing the value of $f$ . For example $$
g(t)-g(0)=\frac{t}{x_1+x_2}+\frac{t}{x_n+x_{n-1}}-t\frac{x_1+x_2+2x_n}{(x_1+x_n-t)(x_1+x_n)}-t\frac{x_3-x_4}{(x_2+x_3+t)(x_2+x_3)},
$$ but not sure if this is nonnegative. EDIT : I've made some progress towards the $n=5$ case. Let us assume wlog that $x_1$ is the largest throughout the proof. We also know that the $n=4$ case is true (see comments). Thus $$
f_5(x_1,\ldots,x_5)=f_4(x_2,\ldots,x_5)+\frac{x_2-x_1}{x_4+x_5}+\frac{x_1-x_2}{x_5+x_1}+\frac{x_2-x_3}{x_1+x_2}-\frac{x_2-x_3}{x_5+x_1}\leq\\\leq
\frac{x_2-x_1}{x_4+x_5}+\frac{x_1-x_2}{x_5+x_1}+\frac{x_2-x_3}{x_1+x_2}-\frac{x_2-x_3}{x_5+x_1}=\\=\frac{(x_1-x_2)(x_4-x_1)}{(x_1+x_5)(x_4+x_5)}+\frac{(x_2-x_3)(x_5-x_1)}{(x_1+x_2)(x_2+x_5)}.\tag{1}\label{ineq:1}
$$ By successively cyclically permuting the sequence we also derive the inequalities: \begin{align}
f_5(x_1,\ldots,x_5) &\leq \frac{(x_2-x_3)(x_5-x_2)}{(x_2+x_1)(x_5+x_1)}+\frac{(x_3-x_4)(x_1-x_2)}{(x_2+x_3)(x_3+x_1)}\label{ineq:2}\tag{2}, \\
f_5(x_1,\ldots,x_5) &\leq \frac{(x_3-x_4)(x_1-x_3)}{(x_3+x_2)(x_1+x_2)}+\frac{(x_4-x_5)(x_2-x_3)}{(x_3+x_4)(x_4+x_2)}\label{ineq:3}\tag{3}, \\
f_5(x_1,\ldots,x_5) &\leq \frac{(x_4-x_5)(x_2-x_4)}{(x_4+x_3)(x_2+x_3)}+\frac{(x_5-x_1)(x_3-x_4)}{(x_4+x_5)(x_5+x_3)}\label{ineq:4}\tag{4}, \\
f_5(x_1,\ldots,x_5) &\leq \frac{(x_5-x_1)(x_3-x_5)}{(x_5+x_4)(x_3+x_4)}+\frac{(x_1-x_2)(x_4-x_5)}{(x_5+x_1)(x_1+x_4)}\label{ineq:5}\tag{5}.
\end{align} Now we consider several cases. Case 1 : If $x_2\geq x_3$ then $\eqref{ineq:1}$ is $\leq 0$ . Case 3 : If $x_2 < x_3 \leq x_4 \leq x_5$ then $\eqref{ineq:2}$ is $\leq 0$ . Case 2 : If $x_2 < x_3 \leq x_4$ and $x_4 > x_5$ then $\eqref{ineq:3}$ is $\leq 0$ . Case 4 : If $x_2 < x_3$ , $x_3 > x_4, x_4 \leq x_5$ and $x_3 \geq x_5$ then $\eqref{ineq:5}$ is $\leq 0$ . Case 5 : If $x_2 < x_3$ , $x_3 > x_4, x_4 \leq x_5$ and $x_2 \geq x_4$ then $\eqref{ineq:4}$ is $\leq 0$ . Case 6 : If $x_2 < x_3$ , $x_3 > x_4, x_4 > x_5$ and $x_2 \leq x_4$ then $\eqref{ineq:4}$ is $\leq 0$ . There are two more cases to consider. Case 7 : If $x_3>x_2>x_4 > x_5$ then it seems to be the case that $$
f_5(x_1,\ldots,x_5) \leq f_5(x_1,x_2,x_4,x_3,x_5).
$$ via simulation. If proven true, then we just need to apply case 1 to it and we're done. Case 8 : Since case 4 and 5 are true independently, we need consider only $x_5>x_3>x_4>x_2$ as a subcase. However I haven't the slightest clue how to approach this. EDIT 2 According to WolframAlpha , it seems that the individual fractions are concave functions of $\mathbf{x}=(x_1,\ldots,x_n)$ . Does that mean that $f$ is also concave, therefore we can bound by $$
f_n(\mathbf{x}) \leq f_n(\mathbf{1}/n) + \nabla f_n(\mathbf{1}/n)^T(\mathbf{x} - \mathbf{1}/n) = 0?
$$","['optimization', 'inequality', 'lagrange-multiplier', 'real-analysis']"
4687247,How can one evaluate this monster $\int_0^\infty \int_0^1 \frac{\ln(1+t^x)}{(1+t)(1+x^2)}\ dt\ dx$,"When I encountered this integral I couldn't imagine it having such an elegant closed form, and yet here it is: $$\int_0^\infty \int_0^1 \frac{\ln(1+t^x)}{(1+t)(1+x^2)}\ \mathrm{d}t\ \mathrm{d}x=\frac{\pi}{4}\ln^2(2)$$ Or at least this is what I conjectured looking at the numerical value of the integral. Wolfram wasn't able to catch the closed form, and neither was I. My attempts in solving the integral included exchanging the order of integration, ending up with $$\int_0^1 \frac{1}{1+t} \int_0^\infty \frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x\ dt$$ but this doesn't look simpler. However, the inner integral can be reduced to $$\int_0^\infty \frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x= $$ $$=\int_0^1\frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x\ +\int_1^\infty\frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x=$$ $$=\int_0^1\frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x\ +\int_0^1\frac{\ln(1+t^{\frac{1}{x}})}{1+x^2}\ \mathrm{d}x$$ and here I tried to use Taylor series, but couldn't make progress. Suggestions would be awesome.","['integration', 'calculus', 'definite-integrals']"
4687259,How to evaluate $\displaystyle\int_{-\infty}^{\infty}e^{-\frac{x^{2}}{2}}\ln\left(1+e^{x}\right)\mathrm{d}x\textbf{ efficiently}$?,"I am struggling in evaluating the follow integral: $$\int_{-\infty}^{\infty}e^{-\frac{x^{2}}{2}}\ln\left(1+e^{x}\right)\mathrm{d}x\overset{\text{Wolfram}}{\approx} 2.02049$$ $\color{red}{\text{Despite asking Wolfram to give me 10 digits, it always gives me this result}}$ My process $$\int_{-\infty}^{\infty}e^{-\frac{x^{2}}{2}}\ln\left(1+e^{x}\right)\mathrm{d}x=1+2\int_{0}^{\infty}e^{-\frac{x^{2}}{2}}\ln\left(1+e^{-x}\right)\mathrm{d}x$$ $$\displaystyle\ln(1+x)=-\sum_{n=1}^{\infty}\frac{(-1)^n}{n}x^n\qquad \text{for }|x|<1$$ $$\begin{align}\int_{-\infty}^{\infty}e^{-\frac{x^{2}}{2}}\ln\left(1+e^{x}\right)\mathrm{d}x=&1-2\sum_{n=1}^{\infty}\frac{\left(-1\right)^{n}}{n}\int_{0}^{\infty}e^{-\frac{x^{2}}{2}}e^{-nx}\mathrm{d}x\\
=&1-2\sum_{n=1}^{\infty}\frac{\left(-1\right)^{n}}{n}\sqrt{\frac{\pi}{2}}e^{\frac{n^{2}}{2}}\text{erfc}\left(\frac{n}{\sqrt{2}}\right)\\
=&1-\sqrt{2\pi}\sum_{n=1}^{\infty}\frac{\left(-1\right)^{n}}{n}e^{\frac{n^{2}}{2}}\text{erfc}\left(\frac{n}{\sqrt{2}}\right)\end{align}$$ This solution is correct, but has an huge defect: it is extremely inefficient to calculate When I calculate the series for the first $n$ terms I have to take $e^{\frac{n^2}{2}}$ into account $n=10$ : $e^{\frac{n^2}{2}}\approx 5.18\cdot 10^{21}$ and the series is $\approx \color{blue}{2.0}115$ $n=20$ : $e^{\frac{n^2}{2}}\approx 7.22\cdot 10^{86}$ and the series is $\approx \color{blue}{2.0}181$ $n=30$ : $e^{\frac{n^2}{2}}\approx 2.70\cdot 10^{195}$ and the series is $\approx \color{blue}{2.0}194$ $n=50$ : $e^{\frac{n^2}{2}}\approx \color{red}{7.38\cdot 10^{542}}$ and the series is $\approx \color{blue}{2.020}0$ $n=110$ : $e^{\frac{n^2}{2}}\approx \color{red}{3.03\cdot 10^{2627}}$ and the series is $\approx \color{blue}{2.0204}$ In reality these $n$ are even higher because the series is alternated, with $n+1$ they lose a correct figure and with $n+2$ they recover it. So to calculate at least $3$ correct decimal digits we need to calculate a number that not even Matlab and Desmos are able to handle (the first Desmos and Matlab overflow number is $2^{1024}\approx 1.79\cdot 10^{308}$ ) Question Anyone knows some tricks to solve it in some other way? Hint 1 The Taylor series of $\ln(1+e^{-x})$ cannot be used, since it has convergence radius $\pi$ : $$\ln\left(1+e^{-x}\right)=\ln(2)-\frac{x}{2}-\frac{1}{4}\sum_{n=1}^{\infty}\frac{E_{2n-1}(0)}{n(2n-1)!}x^{2n}\qquad |x|<\pi$$ Therefore used in the integral for $|x|>\pi$ we have that the integral diverges. $E_{\nu}(z)$ is the Euler polynomials. Hint 2 (It's not useful but it's an interesting thing in my opinion) $$\text{erfc}(z)e^{z^2}\propto \frac{1}{\sqrt{\pi}z}\sum_{n=0}^{\infty}\left(-1\right)^{n}\frac{\left(2n\right)!}{n!}\left(2z\right)^{-2n}$$ Hint 3 I tried even doing the Gaussian series: $$e^{-\frac{x^2}{2}}=\sum_{n=0}^{\infty}\frac{\left(-1\right)^{n}x^{2n}}{2^{n}n!}\qquad \text{for }x\in\mathbb{R}$$ $$\begin{align}\int_{-\infty}^{\infty}e^{-\frac{x^{2}}{2}}\ln\left(1+e^{x}\right)dx=&1+2\int_{0}^{\infty}e^{-\frac{x^{2}}{2}}\ln\left(1+e^{-x}\right)dx\\
=&1+2\int_{0}^{\infty}\sum_{n=0}^{\infty}\frac{\left(-1\right)^{n}x^{2n}}{2^{n}n!}\ln\left(1+e^{-x}\right)dx\\
=&1+2\sum_{n=0}^{\infty}\frac{\left(-1\right)^{n}}{2^{n}n!}\int_{0}^{\infty}x^{2n}\ln\left(1+e^{-x}\right)dx\\
\end{align}$$ $$\color{green}{\int_{0}^{\infty}x^{2n}\ln\left(1+e^{-x}\right)dx=\left(1-2^{-2n-1}\right)\zeta\left(2+2n\right)\left(2n\right)!}$$ $$1+2\sum_{n=0}^{\infty}\frac{\left(-1\right)^{n}}{2^{n}n!}\left(1-2^{-2n-1}\right)\zeta\left(2+2n\right)\left(2n\right)!$$ There must be a mistake here because you end up with a $\color{red}{\text{diverget series}}$ Hint 4 I would like to try the Gauss–Hermite quadrature formula but it seems quite complicated to implement (I guess the result comes, but I don't know how quickly)","['integration', 'improper-integrals', 'definite-integrals', 'normal-distribution', 'gaussian-integral']"
4687268,Quadratic and quartic subfields of $\mathbb{Q}(\zeta_{2023})$,"Let $\zeta_{2023}=e^{2\pi i /2023}$ be a primitive $2023$ th root of unity. Describe : Quadratic subfields of $\mathbb{Q}(\zeta_{2023})$ . Quartic subfields $E$ of $\mathbb{Q}(\zeta_{2023})$ such that $\mathcal{Gal}(E/\mathbb{Q})\cong \mathbb{Z}/4\mathbb{Z}$ . Since $2023=7\cdot 17^2$ , we have $\mathbb{Q}(\zeta_{2023})=\mathbb{Q}(\zeta_{7})\cdot\mathbb{Q}(\zeta_{17^2})$ , and then $$\begin{align}
\mathcal{Gal}(\mathbb{Q}(\zeta_{2023})/\mathbb{Q})&\cong\mathcal{Gal}(\mathbb{Q}(\zeta_{7})/\mathbb{Q})\times\mathcal{Gal}(\mathbb{Q}(\zeta_{17^2})/\mathbb{Q})\cong(\mathbb{Z}/7\mathbb{Z})^*\times(\mathbb{Z}/17^2\mathbb{Z})^* \\
&\cong\mathbb{Z}/6\mathbb{Z}\times\mathbb{Z}/17\cdot16\mathbb{Z}
\end{align}
$$ Furthermore $(\mathbb{Z}/7\mathbb{Z})^*=\langle [3]_7 \rangle$ and $(\mathbb{Z}/17^2\mathbb{Z})^*=\langle [3]_{17^2} \rangle$ , thus $\mathcal{Gal}(\mathbb{Q}(\zeta_{7})/\mathbb{Q})=\langle\sigma\rangle$ with $\sigma(\zeta_7)=\zeta_{7}^3$ , and $\mathcal{Gal}(\mathbb{Q}(\zeta_{17^2})/\mathbb{Q})=\langle\tau\rangle$ with $\tau(\zeta_{17^2})=\zeta_{17^2}^3$ . Finally, by extending $\sigma$ and $\tau$ to automorphisms of $\mathbb{Q}(\zeta_{2023})$ , we get $$
\mathcal{Gal}(\mathbb{Q}(\zeta_{2023})/\mathbb{Q})=\langle\sigma\rangle\times\langle\tau\rangle
$$ The only subgroups of index $2$ in $\mathcal{Gal}(\mathbb{Q}(\zeta_{2023})/\mathbb{Q})$ are $$
\begin{align} 
H&:=\langle\sigma^2\rangle\times\langle\tau\rangle\cong\mathbb{Z/3Z}\times\mathbb{Z/17\cdot 16Z} \\
L&:=\langle\sigma^4\rangle\times\langle\tau\rangle\cong\mathbb{Z/3Z}\times\mathbb{Z/17\cdot 16Z} \\
K&:=\langle\sigma\rangle\times\langle\tau^2\rangle\cong\mathbb{Z/6Z}\times\mathbb{Z/17\cdot 8Z}  
\end{align}
$$ Thus, the only quadratic subfield are $\mathbb{Q}(\zeta_{2023})^H$ , $\mathbb{Q}(\zeta_{2023})^L$ and $\mathbb{Q}(\zeta_{2023})^K$ . Unfortunately, I couldn't find an explicit description of these subfields through generators. Again, with some groups theory arguments I could find the desired subgroups of $\mathcal{Gal}(\mathbb{Q}(\zeta_{2023})/\mathbb{Q})$ , but I couldn't get an explicit description of the corresponding subfields.","['field-theory', 'galois-theory', 'abstract-algebra']"
4687306,Derivative as a matrix: $\mathbf{D}=\dfrac{\mathrm{d}}{\mathrm{d}x}$,"I have a strange question, it is possible to consider the derivative as a matrix? (Both are linear transformation technically). I thought about this example, since $1, x,x^2,...,x^n$ can be thought of as the basis of a vector space, I can consider a polynomial as a vector of its coefficients: Let $p_n(x)=a_0+a_1 x+a_2 x^2+...+a_n x^n$ Its derivative is $p'_n(x)=\dfrac{\mathrm{d}}{\mathrm{d}x}p_n(x)=a_1+2a_2 x+...+n a_n x^{n-1}$ (and so far nothing new) But if I consider $p_n(x)$ as a vector of his coefficients: $(a_0,a_1,...,a_n)$ and I want to create a matrix that transforms $p_n(x)$ into $p'_n(x)$ I have: $$\begin{pmatrix}0&1&0&\cdots&0\\
0&0&2&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&0\\
0&0&0&\cdots&n\\
0&0&0&\cdots&0
\end{pmatrix}\begin{pmatrix}a_0\\ a_1\\ a_2\\ \vdots\\ a_n\end{pmatrix}=\begin{pmatrix}a_1\\ 2a_2\\\vdots\\ n a_n\\ 0\\ \end{pmatrix}$$ So technically $\mathbf{D}=\begin{pmatrix}0&1&0&\cdots&0\\
0&0&2&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&0\\
0&0&0&\cdots&n\\
0&0&0&\cdots&0
\end{pmatrix}$ represents the derivative. After this I tried to do some operations on it, and it came out that: $\mathbf{D}^n$ represents $\dfrac{\mathrm{d}^n}{\mathrm{d}x^n}$ $\det(\mathbf{D})=0$ , this means that $\mathbf{D}^{-1}$ is not defined (I interpreted this fact as the fact that the inverse operation of the derivative is the integral, which in general is not unique since there are infinitely many that vary for arbitrary constants, EVEN IF it is possible to calculate the pseudoinverse and it gives the integral) $\text{trace}(\mathbf{D})=0$ (I don't know how to interpret that), idem for $\mathbf{D}^{\top}$ In general, now I was working on a polynomial, so the dimension of the matrix is $n\times n$ with rank $n-1$ , but for a general function I suppose it is $""\infty\times\infty""$ (I don't know if it can be defined the rank for an infinite matrix) $\exp(\mathbf{D})$ gives the upper Pascal Matrix $\mathbf{D}=\text{diag}(1,1,2,...,n!)^{-1}\begin{pmatrix}0&1&0&\cdots&0\\
0&0&1&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&0\\
0&0&0&\cdots&1\\
0&0&0&\cdots&0
\end{pmatrix}\text{diag}(1,1,2,...,n!)$ Are these things correct? I'm curious to see if it also has applications for partial derivatives or fractional calculus. (I tried to search on the internet ""derivative as a matrix"" but the main result was the Jacobian, so tell me if this has a name)","['infinite-matrices', 'matrices', 'linear-algebra', 'polynomials', 'derivatives']"
4687341,Prove that in a triangle : $\cot A + \cot B + \cot C \ge \sqrt{3}$,"$\textbf{Question :}$ Prove that in a triangle $$\cot A + \cot B + \cot C \ge \sqrt{3}$$ $\textbf{My Attempt :}$ For a acute triangle I can say that all angles are less than $\frac{\pi}{2}$ and as in $[0,\frac{\pi}{2}]$ the $\cot x$ function is concave upwards we can say through Jensen's Inequality that $$\frac{\sum \cot A}{3} \ge \cot\left(\frac{\sum A}{3}\right) \implies \sum \cot A \ge \sqrt{3}$$ And now for the proof in a obtuse angled triangle, where two angles are less than $\frac{\pi}{2}$ and one is more than $\frac{\pi}{2}$ . We can take each angle as an $x$ -coordinate and plot the corresponding $y$ -coordinate which is nothing but $(x , \cot x)$ we get three points. Now if we form a triangle with these three points, how to show that the centroid of this triangle still stays above the graph of $y=\cot x$ . In the case of acute triangle it was trivial to show that but in obtuse triangle one point will have a negative $y$ -coordinate. So what to do in this case?","['analytic-geometry', 'geometry', 'calculus', 'trigonometry', 'algebra-precalculus']"
4687366,"Show $ \lim_{x \to 0}\left[\int_0^{1} by+a(1-y)^x\,dy \right]^\frac{1}{x} = \frac{1}{e}\left[\frac{b^b}{a^a}\right]^\frac{1}{b-a}$ for $a < b$","How do we prove; $$L =  \lim_{x \to 0}\left[\int_0^{1} by+a(1-y)^x\,dy \right]^\frac{1}{x} = \frac{1}{e}\left[\frac{b^b}{a^a}\right]^\frac{1}{b-a}, a < b$$ This question is from a test. Let $\displaystyle V = \int_0^{1} by+a(1-y)^x\,dy$ then I used the substitution $by+a(1-y) = t$ to derive $V = \left[\dfrac{b^{x+1}-a^{x+1}}{(x+1)(b-a)}\right]$ , from which, $$L = \lim_{x \to 0}\left[\frac{b^{x+1}-a^{x+1}}{(x+1)(b-a)}\right]^\frac{1}{x}$$ Could anyone suggest how this limit can be evaluated (taking log on both sides did not get me the answer)","['limits', 'calculus']"
4687379,Stabilizers of an arbitrary element of $\mathbb Z^2$,"Let $G = SL(2,\mathbb Z)$ acting on $\mathbb Z^2$ by the product $Av$ , I'm trying to define $\operatorname{Stab}(v)$ for all $v\in \mathbb Z^2$ . $Av = v$ , then $1$ is an eigenvalue associated with the eigenvector $v$ . For all $A\in \operatorname{Stab}(v)$ , the characteristic polynomial is $(x-1)^2$ , then the minimal polynomial $m_A$ is $(x-1)$ or $(x-1)^2$ for Hamilton–Cayley Theorem.
If $m_A = (x-1)$ , then $A = I_2$ and $I_2 \in \operatorname{Stab}(v)$ .
If $m_A = (x-1)^2$ , then $A$ is not diagonalizable, then the Jordan normal form of A is \begin{equation}
\begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix}
\end{equation} I don't know if this is a easy way, but I can't find the solution of this exercise.","['representation-theory', 'integer-lattices', 'linear-algebra', 'group-theory', 'group-actions']"
4687467,Group structure on compact Riemann surface,"I am currently reading chapter 3 of Arithmetic of Elliptic Curves by Silverman. We know that elliptic curves can be viewed as a Riemann surface of genus 1, and there is a well-known group structure on it. I wonder if we can do the same for arbitrary Riemann surfaces of genus 1. The notes of Terence Tao for 246C ( Notes 1, Exercise 43 ) suggests that this can be done by showing for $R$ a point on $X$ , and $P$ , $Q$ points on $X$ , we have some points $P+Q$ such that $$(P)+(Q)-(P+Q)-(R)$$ is a principal divisor. I am not sure how to show this using Riemann-Roch, and how can we use this to get the desired group structure. Thanks in advance.","['complex-analysis', 'complex-geometry', 'algebraic-geometry', 'riemann-surfaces']"
4687499,"For finding limit of $ \displaystyle\lim_{x\to0} \dfrac{e^{1/x} - 1}{e^{1/x} +1}$ , why L'Hospital rule is not working?","So for evaluating limit of $\displaystyle\lim_{x\to0} \frac{e^{1/x} - 1}{e^{1/x} +1 } $ I used De l'Hospital rule, as conditions are being satisfied, a) $f(x),g(x)\to \infty $ b) both are differential And upon using L'Hospital rule I get $\displaystyle\lim_{x\to0} \frac{e^{1/x} (-1/x^2)}{e^{1/x} (-1/x^2)} $ , cancelling denominator with numerator i get 1. But evaluating left hand and right hand limit separately shows that this function does not have limiting value at $x\to 0$ I don't understand what I am doing wrong ?",['limits']
4687511,Finding the general solution of a linear differential equation when there is just y' given,"The formulation of the problem is as following: $$(\cos x) * y' = \cos x + 2\sin x$$ Since this is a linear differential of the first order, and I'm looking for a general solution, the aim is to find $P(x)$ and $Q(x)$ . Since there is just $y'$ , I thought $P(x)$ would be zero. However, the official solution of the equation is $$y=(1/(\cos x)^2)(C + x/2 + \sin 2x/4)
$$ In no way can I get this solution if $P(x)$ is $0$ . But what is it then? I tried getting $y$ through integration of the left side by $dx$ , and then plugging in $x$ derived from that equation into the original linear differential equation, but that is not giving me the requested solution either. I would appreciate any help provided.",['ordinary-differential-equations']
4687515,$A$ is a square matrix of order $2$ with real entries and ${\rm Tr}(A)+|A|=2$. Show that $|A^2+|A|\cdot A+{\rm Tr}(A)I_2|\geq 4$,$A$ is a square matrix of order $2$ with real entries and ${\rm Tr}(A)+|A|=2$ . Show that $$|A^2+|A|\cdot A+{\rm Tr}(A)I_2|\geq 4$$ My Attempt I could observe that ${\rm Tr}(A)+|A|=2\Rightarrow 1+a+d+ad-bc=3 \Rightarrow|A+I|=3$ By Cayley-Hamilton theorem for a $2\times 2$ matrix we have $A^2-{\rm Tr}(A)\cdot A+|A|\cdot I_2=0$ . $A^2={\rm Tr}(A)\cdot A-|A|\cdot I_2$ So $A^2+|A|\cdot A+{\rm Tr}(A)I_2={\rm Tr}(A)\cdot A-|A|\cdot I_2+|A|\cdot A+{\rm Tr}(A)I_2$ $=({\rm Tr}(A)+|A|)A+({\rm Tr}(A)-|A|)I_2$ $=2A+({\rm Tr}(A)-|A|)I_2=2A+2(1-|A|)I_2$ $|A^2+|A|\cdot A+{\rm Tr}(A)I_2|=4|A+(1-|A|)I_2|$ Not able to proceed from here. I am wondering whether Cayley Hamilton was the right approach or not.,"['determinant', 'trace', 'cayley-hamilton', 'matrices', 'linear-algebra']"
4687541,How to calculate the hemisphere using triple integrals?,"How to solve this $\displaystyle\iiint _K (y + x^2) \mathrm{d}x\mathrm{d}y\mathrm{d}z$ where $K$ is the hemisphere $x^2 + y^2 + z^2 \leq 4$ , $z \geq 0$ . This is what I tried so far: I used variable substitution $$\iiint_K (r \sin(\phi)\sin(\theta) + r^2 \sin^2(\phi)\cos^2(\theta)) r^2 \sin(\phi) \mathrm{d}r\mathrm{d}\phi\mathrm{d}\theta$$ Here is $K$ given by the new variables: $ 0 \leq r \leq 2$ and $ 0 \leq \theta \leq 2\pi$ and $0 \leq \phi \leq \pi$ The problem is I have no idea how to integrate this.","['multivariable-calculus', 'spherical-coordinates']"
4687567,How to solve the integral in solving this differential equation? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question $$ y' = -x^2y + \tanh(x+3) $$ $$ a(x) = -x^2 $$ $$ A(x) = -x^3/3 $$ $$ y = e^{-x^3/3} \int{e^{x^3/3} \tanh(x+3)\mathrm{d}x} $$ I can't solve this integral. I tried to write $\tanh(x+3)$ as exponential but couldn't solve it anyway.","['integration', 'ordinary-differential-equations']"
4687571,Wheel of Fortune design,"In the case of a generalized Wheel of Fortune, the Expected Value is simply the dot-product of a Probabilities vector with a corresponding Values vector: $$E(W)=\sum p_i v_i$$ By way of example, a three-slice wheel with two slices having a $25\%$ chance of ""hitting"" and the third slice having a $50\%$ chance of hitting, using values for the three slices respectively of 1, 4, 10 would yield : $$E(W) = (.25\cdot 1) + (.25\cdot 4) + (.50\cdot 10) = 6.25$$ If the goal were to have the same $ E(W) $ but make the probabilities for each slice as close in value as possible, one could define a simple error function acknowledging that perfectly equal slices would each have $\frac{1}{3}$ probability of hitting. $$Err(p) = \sum ( p_i - \overline{p} )^2$$ This error could never be zero unless the $E(W)$ value were changed to 5, which is not desirable. $$E(W) = \left(\frac{1}{3}\cdot 1\right ) + \left(\frac{1}{3}\cdot 4 \right) + \left(\frac{1}{3}\cdot 10\right) = 5$$ An alternate error function may be considered without regard to the mean probability: $$Err(p) = \sum p_i^2$$ These $p_i$ values are probabilities: $$\sum p_i = 1$$ $$0 < p_i < 1$$ Question: Given $E(W)$ and each value on the Wheel of Fortune $(v_1 \ldots v_n)$ as a priori constant, is there a well known way to establish $(p_1 \ldots p_n)$ such that either error function $Err(p)$ is minimized? Its safe to assume the error function is continuous. I don't think simplex can work because of the constraints about probabilities summing to 1.","['probability-distributions', 'problem-solving', 'probability-theory', 'probability']"
4687632,"Is the set of “Pythagorean” square roots dense in $\left[0, \infty\right)$? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I was thinking about proofs of the fact that square roots exist and ended up wondering if
the set of numbers of the form $\sqrt{\dfrac{a^2}{b^2}+ \dfrac{c^2}{d^2}}$ with $a, b, c, d \in\Bbb N$ is a dense subset of $\left[0, \infty\right)$ . I have no idea how to go about determining this.","['pythagorean-triples', 'geometry']"
4687671,Perfect square of the form 12...23..34..45..56..67..78..89,"I am searching form perfect squares of the form : $$1\underbrace{2\dots2}_a\underbrace{3\dots3}_b\underbrace{4\dots4}_c\underbrace{5\dots5}_d\underbrace{6\dots6}_e\underbrace{7\dots7}_f\underbrace{8\dots8}_g9,$$ where $(a,b,c,d,e,f)\in (\mathbb{N^*})^6$ , and $g\in\mathbb{N}$ . With a systematic search using python and gmpy I could not find a single number with $a+b+c+d+e+f+g<50$ digits. Is there an argument that prevent such perfect squares from existing? What if we allow numbers of the form :   12...23..34..45..56..67..78..8912...23..34..45..56..67..78..89?","['number-theory', 'square-numbers', 'elementary-number-theory']"
4687695,Why is the degree of a linear holomorphism $[z] \mapsto [cz]$ from the complex torus $X=\mathbb{C}/\Lambda$ to itself equal to |c|^2?,"Why is the degree of a linear holomorphism $f:X\rightarrow X$ such that $[z] \mapsto [cz]$ from the complex torus $X=\mathbb{C}/\Lambda$ to itself equal to |c|^2? (here we assume that $\Lambda=\mathbb{Z}+\tau \mathbb{Z}$ with $\operatorname{Im}(\tau)>0$ , $c\Lambda \subseteq \Lambda$ , and $[z]$ designs the equivalence class of $z$ in $X$ ). This is mentioned on this answer , but I haven’t been able to find the reason by myself. I’ve tried to compute it by using $\deg(f)=\sum_{p\in f^{-1}({z})}\operatorname{mult}_p(f)$ where $f([z])=[cz]$ and hence $f^{-1}([z])=[c^{-1}z]$ , but I’m stuck at trying to find $\operatorname{mult}_p(f)$ for the points in $f^{-1}([z])$ here. Edit: I already understand why $\operatorname{mult}_p(f)=1$ for all $p\in f^{-1}([z])$ but I don’t see why should $|f^{-1}([z])|=|c|^2$ .","['complex-analysis', 'riemann-surfaces']"
4687709,An $\ell^qL^p$ inequality: $\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{\ell^2L^4}\leq \|{\varphi}\|_{L^2}$.,"An $\ell^qL^p$ inequality: $\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{\ell^2L^4}\leq \|{\varphi}\|_{L^2}$ . The dimensions I care about are $d=2$ and $d=3$ . We define $\ell^qL^p$ in the following way. For $\alpha\in \mathbb Z^d$ , let $\square_\alpha$ be the unit cube in $\mathbb R^d$ with centre at $\alpha$ and let $\chi_\alpha$ be its characteristic function. Define $\ell^q(L^p)$ to be the set of functions for which $$\|{f}\|_{\ell^qL^p}:=\left({\sum_\alpha\|{f\chi_\alpha}\|^q_{L^p}}\right)^{\frac{1}{q}}=\left({\sum_\alpha\left({\int_{\square_\alpha}|{f(x)}|^p\mathrm{d}x}\right)^{\frac{p}{q}}}\right)^{\frac{1}{q}}$$ is finite. The question says to use the Hölder inequality and the Sobolev inequality. Idea of proof: Using $L^p$ interpolation, $\frac{1}{4}=\frac{\frac{1}{2}}{2}+\frac{\frac{1}{2}}{\infty}$ , so \begin{align*}\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{\ell^2L^4}= \left({\sum_\alpha\|{\nabla\langle\nabla\rangle^{-2}\varphi\chi_\alpha}\|^2_{L^4}}\right)^{\frac{1}{2}}&\leq\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^\infty}^{\frac{1}{2}}\left({\sum_\alpha\|{\nabla\langle\nabla\rangle^{-2}\varphi\chi_\alpha}\|^2_{L^2}}\right)^{\frac{1}{2}}\\
&= \|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^\infty}^{\frac{1}{2}}\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^2}^{\frac{1}{2}}.\end{align*} Now $\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^2}\leq\|\varphi\|_{L^2}$ which is easily seen on the Fourier side. I'm not sure how to prove $\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^\infty}$ is bounded by $\|\varphi\|_{L^2}$ (I'm not even sure if this is true).","['analysis', 'linear-algebra', 'functional-analysis', 'partial-differential-equations', 'quantum-mechanics']"
4687718,"Prove that if $t:=\lim_n t_n >0$, then $(u_n)$ converges","I'm trying to solve below exercise, i.e., Let $(H, \langle \cdot, \cdot\rangle)$ be a real Hilbert space and $|\cdot|$ its induced norm. Let $(u_n) \subset H$ and $(t_n) \subset \mathbb R_{>0}$ such that $(t_n)$ is non-increasing. Assume that $$
\langle t_n u_n-t_m u_m, u_n-u_m \rangle \le 0
\quad \forall m, n \in \mathbb N.
$$ Prove that either $|u_n| \to \infty$ or $(u_n)$ converges. Prove that if $t:=\lim_n t_n >0$ , then $(u_n)$ converges. Could you verify my attempt on (1.)? Could you elaborate on how to solve (2.)? We have $$
\begin{aligned}
\langle t_{n+1} u_{n+1}-t_n u_n, u_{n+1}-u_n \rangle &\le 0 \\
\iff  |u_{n+1}-u_n|^2 &\le \frac{t_n - t_{n+1}}{t_{n}} \langle u_{n+1}, u_{n+1}-u_{n} \rangle \\
\implies  |u_{n+1}-u_n|^2 &\le \langle u_{n+1}, u_{n+1}-u_{n} \rangle \qquad (1) \\
\iff |u_{n+1}|^2 - 2 \langle u_n, u_{n+1} \rangle +  |u_n|^2 &\le |u_{n+1}|^2 - \langle u_n, u_{n+1} \rangle \\
\iff |u_{n}|^2 &\le \langle u_n, u_{n+1} \rangle \qquad (2)\\
\implies |u_n| &\le |u_{n+1}|.
\end{aligned}
$$ So $(|u_n|)_n$ is non-decreasing. Assume that $|u_n| \not\to \infty$ . Then $(|u_n|)_n$ is bounded and convergent in $\mathbb R$ . From $(1)$ and $(2)$ , we have $$
\begin{aligned}
|u_{n+1}-u_n|^2 &\le \langle u_{n+1}, u_{n+1}-u_{n} \rangle \\
\iff |u_{n+1}-u_n|^2 &\le  |u_{n+1}|^2 - \langle u_n, u_{n+1} \rangle \\
\implies |u_{n+1}-u_n|^2 &\le  |u_{n+1}|^2 - |u_n|^2. \\
\end{aligned}
$$ Because $(|u_n|^2)_n$ is a Cauchy sequence, so is $(u_n)_n$ .","['hilbert-spaces', 'convergence-divergence', 'functional-analysis', 'sequences-and-series']"
4687721,Find the sum of the real roots of the equation $6x^4+9x^3-15x^2+9x+6=0$,Find the sum of the real roots of the equation $$6x^4+9x^3-15x^2+9x+6=0$$ We can solve the equation by dividing both sides by $x^2\ne0$ . Then we'll get $$6\left(x^2+\dfrac{1}{x^2}\right)+9\left(x+\dfrac{1}{x}\right)-15=0$$ which by putting $t=x+\dfrac{1}{x}$ can be written as $$6(t^2-2)+9t-15=0\iff 6t^2+9t-27=0$$ We can find the roots from here. This just seems to me like a lot of work to simply find the sum. The sum we're searching for is $-3$ : . See this I would like to ask if there is a faster approach? Does needing only the sum of the real roots make things more complicated for finding a faster approach $?$,"['algebra-precalculus', 'quartics', 'polynomials']"
4687735,Can you graph $y^{4}x^{3}+x^{4}-y=0$ without a computer?,"I'm trying to get an intuitive sense of algebraic varieties from their equations. Some varieties are more easily imagined without a computer since either $y$ or $x$ can be isolated, producing functions, and these can be more easily graphed. But what about for varieties like $y^{4}x^{3}+x^{4}-y=0$ ? Is there any way to think of its shape without a computer? One approach would be to use the quartic formulas to isolate $y$ , but this is time consuming. Are there better approaches, if not to obtain a perfect graph, at least to obtain an estimation?",['algebraic-geometry']
4687743,Laurent series of the inverse of a perturbed singular matrix,"Let $M(\epsilon)$ be the square matrix $M(\epsilon) = A + \epsilon B$ where $A,B$ are square matrices. Throughout this post, suppose that $M(\epsilon)$ is invertible for arbitrarily small but positive $\epsilon$ , but $M(0) = A$ is not invertible (note that my question becomes very easy when $A$ is invertible by just using the Neumann series of the inverse). I'm interested in finding the limiting behavior of the inverse $M(\epsilon)^{-1}$ as $\epsilon\to 0^+$ . Specifically, I'd like to expand it as $$M(\epsilon)^{-1} = \sum_{n=-\infty}^\infty D_n \epsilon^n,$$ where $D_n$ are some matrices. How can I find the matrices $D_n$ ? Ultimately, I am interested in the limiting behavior of the solution $x$ to the equation $M(\epsilon)x = c$ , where $x,c$ are vectors. Specifically, suppose that $x(\epsilon)$ is the solution to $M(\epsilon)x(\epsilon) = c$ . I'm interested in knowing $$\lim_{\epsilon\to 0^+} \frac{x(\epsilon)}{\lVert x(\epsilon) \rVert_{\infty}} = \lim_{\epsilon\to 0^+} \frac{M(\epsilon)^{-1}c}{\lVert M(\epsilon)^{-1}c \rVert_{\infty}}.$$ If $M(\epsilon)^{-1} = \sum_{n=s}^\infty D_n \epsilon^n$ for some finite $s\in\mathbb Z$ , then I think it follows that $\lim_{\epsilon\to 0^+} \frac{x(\epsilon)}{\lVert x(\epsilon) \rVert_{\infty}} = \frac{D_s c}{\lVert D_s c \rVert_\infty}$ . Hence, I want to determine $s$ and $D_s$ .","['numerical-linear-algebra', 'limits', 'linear-algebra', 'laurent-series']"
4687760,Source and/or detailed proof for $\exp(\lambda (A + B)) = \exp(\lambda A)\exp(\lambda B)$ for commuting elements in a Banach algebra,"Every time I need to revisit the proof that $\exp(\lambda (A + B)) = \exp(\lambda A)\exp(\lambda B), \lambda \in\mathbb{K}$ for commuting elements in a Banach algebra, I find myself struggling to remember the key details. I can recall certain aspects, such as why the series converges in norm (which justifies the whole equality), but I have difficulty recalling the exact justification for other details. For instance, I can't seem to remember why we can ""just permute"" the order of the double series $\displaystyle \sum_{n=0}^{\infty}\sum_{k=0}^{n}\frac{(\lambda A)^k}{k!}\frac{(\lambda B)^{n-k}}{(n-k)!}$ . As dull and uninteresting it may sound, I am looking for a proof which is as explicit as possible without being ""too tedious"". That is, I would like to see a treatment of this proof with indicator functions and the such to make life easier. Additionally, I currently can't recall any written sources that discuss this theorem. Therefore I am asking either for a reference to a book discussing the prior claim in detail (including why it makes sense to permute the two series; let the convergence theorems from measure theory be allowed if that makes it any easier) or a proof of the said claim, in detail. Any help would be greatly appreciated. Thank you in advance!","['banach-spaces', 'banach-algebras', 'reference-request', 'functional-analysis', 'exponential-function']"
4687773,Finding the density of a function of a distribution,"I came across a question that defined the R.V $X$ to be uniformly distributed on the interval $[-2,1]$ and we are asked to find the density of $U=X^2$ . Since the function being applied to X is not injective, I opted for the method of distribution functions, thus: $$P(U\le u)$$ $$= P(X^2 \le u)$$ $$=P(-\sqrt{u} \le X \le \sqrt{u}) $$ It is clear by substitution that the bounds for $U$ are from $[0,4]$ So all that is left is to solve for: $$ \ \int_{-\sqrt{u}}^{\sqrt{u}} \frac{1}{3} \ dx \ $$ $$ = 2\frac{\sqrt{u}}{3} $$ This gives me my distribution function but after deriving to obtain the density: $$ f(u) = \frac{1}{3\sqrt{u}} $$ This does not give a valid density for the bounds $[0,4]$ . I assume my error lies in the initial bounds for the derivation of the distribution function but after a little messing around I still cannot pinpoint where the mistake lies. And clues would be greatly appreciated!","['statistics', 'probability-distributions', 'probability']"
4687828,Bias be larger than variance in ERM,"Given a convex set $S\subset \mathbb{R}^n$ and some $\theta\in S$ , consider the observation $y=\theta+\epsilon$ where $\epsilon\sim \mathcal{N}(0,I)$ , the ERM estimator is $$\hat{\theta}=\arg \min_{x\in S}\|x-y\|_2.$$ Consider the usual bias-variance decomposition $$\mathbb{E}\|\hat\theta-\theta\|^2=\|\mathbb{E}\hat\theta-\theta\|^2+var(\hat\theta)=bias^2+variance.$$ Does there exists some $S$ and $\theta$ , such that the bias term is much larger than the variance term? For example, for 1D linear regression on $x_1,...,x_n\in\mathbb{R}$ , the $S$ here is $S=\{(y_1,...,y_n):y_i=kx_i+b, k,b\in\mathbb{R}\}$ is a hyperplane in $\mathbb{R}^n$ , so the estimator is unbiased.","['statistical-inference', 'statistics', 'estimation']"
4687837,"Does $\text{cov}(\lVert X-X' \rVert, \lVert X-X'' \rVert) \geq 0$ hold for i.i.d. random vectors?","Let $X$ , $X'$ , and $X''$ be i.i.d. random vectors taking values in $\mathbb{R}^N$ . Is it true that $$\text{cov}(\lVert X-X' \rVert, \lVert X-X'' \rVert) \geq 0?$$ My numerical simulations suggest that it is, but I have so far only been able to show the following: $\text{cov}(\lVert X \rVert, \lVert X \rVert) = \text{var} (\lVert X \rVert) \geq 0$ $\text{cov}(\lVert -X' \rVert, \lVert -X'' \rVert) = \text{cov}(\lVert X' \rVert, \lVert X'' \rVert) = 0$ $\text{cov}(X-X',X-X'') = \text{var}(X) \geq 0$ for $X, X', X''$ with dimension $1$","['statistics', 'covariance', 'inequality', 'probability', 'random-variables']"
4687840,Evaluated (maybe) $\int_0^\infty \int_0^1 \frac{\mathrm{Li}_2(t^x) \ln(1-t)} {t(1+x^2)}\ dt\ dx\ = -\frac{\pi^5}{144}$,"I might have proven that $$\int_0^\infty \int_0^1 \frac{\mathrm{Li}_2(t^x) \ln(1-t)} {t(1+x^2)}\ dt\ dx\ = -\frac{\pi^5}{144}$$ If this turns out to be right I would be astonished by the fact that a monster dilogarithmic integral like that has such a nice closed form. The problem is I am not able to check the if the result is correct, since the most powerful tool I have, WolframAlpha, can't compute the value of the integral. So if anybody can use some stronger math software like Maple or Mathematica to check if the result is correct I would be delighted. Thank you.","['integration', 'calculus', 'definite-integrals']"
4687877,Is it impossible to inscribe any given regular polygon with number of sides n greater than 3 to a parabola?,"So, I was doing some exercises and one of them was about a regular triangle inscribed in a parabola given by, naturally: $f(x)=ax^2+bx+c$ , with $a > 0$ and $\Delta>0$ . This question later got me thinking of how I could possibly solve it if, instead of a triangle, I had a square, let's say, or a regular pentagon. I went to Geogebra, started plotting some stuff and realized it seems to be impossible to inscribe any regular polygon with number of sides n > 3 to a given parabola of the mentioned form. I looked it up on the internet, but couldn't find anything satisfactory. How can I prove that it actually is impossible? I mean, if it is so.","['analytic-geometry', 'conic-sections', 'geometry']"
4687934,An algorithm that always reach the number one.,"Using the function below $$ f(n)=\begin{cases} 
      n/2, & n \text{ even} \\
      n+\lceil\sqrt{n}\rceil, & n \text{ odd} 
   \end{cases}
$$ for an integer $0<n\leq10^4$ it will eventually reach $1$ . Has this function been studied before? Does it always work for every $n \in \mathbb{N}$ ? Note that using the floor function instead of the ceil function will work as well. (Why?)",['discrete-mathematics']
4687938,How to find the exact value of $\sum_{n=1}^{\infty} \frac{\sin \left(\frac{n \pi}{4}\right)}{n^2 \cdot 2^{\frac{n}{2}}} $?,"Once I met the identity $$
\boxed{S_0=\sum_{n=1}^{\infty} \frac{\sin \left(\frac{n \pi}{4}\right)}{2^{\frac{n}{2}}}=1},
$$ I first  tried to prove it by $e^{xi}=\cos x+i\sin x$ . $$
\begin{aligned}
\sum_{n=1}^{\infty} \frac{\sin \left(\frac{n \pi}{4}\right)}{2^{\frac{n}{2}}} & =\Im\left[\sum_{n=1}^{\infty}\left(\frac{e^{\frac{\pi}{4} i}}{\sqrt{2}}\right)^n\right]\\& =\Im\left(\frac{e^{\frac{\pi}{4} i} / \sqrt{2}}{1-e^{\frac{\pi}{4} i} / \sqrt{2}}\right)\\&=\Im\left(\frac{1+i}{1-i}\right)\\&=1
\end{aligned}
$$ and consequently $C_0=\sum_{n=1}^{\infty} \frac{\cos \left(\frac{n \pi}{4}\right)}{ 2^{\frac{n}{2}}}=0 $ . Similarly, I try further with $$
S_1=\sum_{n=1}^{\infty} \frac{\sin \left(\frac{n \pi}{4}\right)}{n \cdot 2^{\frac{n}{2}}}= \begin{equation}
\Im\left[\sum_{n=1}^{\infty} \frac{\left(e^{\frac{\pi}{4} i} / \sqrt{2}\right)^n}{n}\right]
\end{equation}
$$ Inspired by the post , I used the Taylor expansion of $\ln(1-t)$ for $|t|<1$ . \begin{equation}
-\ln (1-t)=\sum_{n=1}^{\infty} \frac{t^n}{n}
\end{equation} $$
\begin{aligned}\sum_{i=1}^{\infty} \frac{\left(e^{\frac{\pi}{4} i}/ \sqrt{2}\right)^n}{n} & =-\ln \left(1-\frac{e^{\frac{\pi}{4} i}}{\sqrt{2}}\right) \\
& =\frac{1}{2}\ln 2+   \frac{\pi}{4} i
\end{aligned}
$$ So we got $$\boxed{S_1=\sum_{n=1}^{\infty} \frac{\sin \left(\frac{n \pi}{4}\right)}{n \cdot 2^{\frac{n}{2}}}=\frac{\pi}{4} } $$ and consequently $C_1=\sum_{n=1}^{\infty} \frac{\cos \left(\frac{n \pi}{4}\right)}{n \cdot 2^{\frac{n}{2}}}=\frac{1}{2}\ln 2  $ . Both results are so nice that I then kept going with $S_2$ using the dilogarithm function $\operatorname{Li}_2$ $$\boxed{\begin{aligned}S_2&=\sum_{n=1}^{\infty} \frac{\sin \left(\frac{n \pi}{4}\right)}{n^2 \cdot 2^{\frac{n}{2}}}= \Im\left[\sum_{n=1}^{\infty} \frac{\left(e^{\frac{\pi}{4} i} / \sqrt{2}\right)^n}{n^2}\right]\\& = \Im\left[\sum_{n=1}^{\infty} \frac{1}{n^2}\left(\frac{1+i}{2}\right)^n\right] =\Im \operatorname{Li}_2\left(\frac{1+i}{2}\right)\end{aligned} }$$ and consequently $C_2= \sum_{n=1}^{\infty} \frac{\cos \left(\frac{n \pi}{4}\right)}{n^2 \cdot 2^{\frac{n}{2}}}=\Re \operatorname{Li}_2\left(\frac{1+i}{2}\right)  $ . My question : How to find the exact value of $S_2$ or $\operatorname{Li}_2\left(\frac{1+i}{2}\right) $ ? Your comments and solutions are highly appreciated.","['integration', 'calculus', 'polylogarithm', 'sequences-and-series', 'complex-numbers']"
4687974,Significance of Roots of Unity,"A strong precalculus course (think AMC, JEE) will teach complex numbers, specifically $n^{\text{th}}$ roots of unity with their properties: They lie equally spaced on the unit circle A power of an $n^{\text{th}}$ root is another $n^{\text{th}}$ root Their sum What applications do roots of unity have in theoretical or applied math?","['complex-analysis', 'roots-of-unity', 'algebra-precalculus', 'soft-question', 'complex-numbers']"
4687993,Equivalence of the definitions of exactness and mixing,"Let $f:X \to X$ be a continuous map, where $X$ is a compact metric space. We say that $f$ is expanding if there are constants $\lambda >1$ and $\delta_0 > 0$ such that, for all $x, y\in X$ , $d(f(x), f(y)) \ge \lambda d(x, y)$ whenever $d(x, y) \le \delta_0$ . Reading this article and searching the internet, it seems that for an expanding map the definitions of exactness and mixing are equivalent. (Mixing) if for every pair of open and not-empty sets $U, V \subset X$ , there exists an $n_0 \in \mathbb{N}$ such that $f^n(U) \cap V \neq \emptyset$ for all $n \ge n_0$ . (Exact or Locally Eventually Onto) if for every open and not-empty $U \subset X$ , there exists $n_0\in\mathbb{N}$ such that $f^{n_0}(U) = X$ . I found an exercise that asks to prove that for an expanding map one has $1\Rightarrow 2$ , but I couldn't complete the exercise. The case $2\Rightarrow 1$ seems trivial. Because if exists $n_0$ , $f^{n_0}(U)=X$ , for any $V\subset X$ the intersection will not be empty. In this case it was not necessary for the map to be expanding. So the problem for me is at $1\Rightarrow 2$ . Any reference for the proof? Or a help to conclude... For clarification, $f^n(x)=f\circ f \circ \cdots \circ f(x)$ an iterative map. Here we are using the topological definition of mixing and exactness. The exercise is from the book ""Foundations of Ergodic Theory"" by Viana and Oliveira, where an extra hypothesis is needed in the definition of expanding: for every $x\in X$ the image of the ball $B(x,\delta_0)$ contains a neighborhood of the closure of $B(f(x),\delta_0)$ . Discussion on mathoverflow .","['dynamical-systems', 'ergodic-theory', 'analysis', 'general-topology', 'mixing']"
4688081,"Bounded first and second derivative on [-1,1]","Let f be a function which is twice differentiable on $\mathbb R$ . Suppose that there exist $\alpha, \beta>0$ such that $|f(x)|\le \alpha$ and $|f''(x)|\le \beta$ for any $x\in [-1,1]$ . Prove that $|f'(x)|\le \alpha+\beta$ for any $x\in [-1,1]$ . By applying the MVT on $f'$ I got $|{f'(x)-f'(0)\over x-0}|\le \beta $ . Hence $|f'(x)-f'(0)|\le B|x|$ . It follows that for any $x\in[-1,1]$ , $f'(0)-\beta \le f'(x)\le f'(0)+\beta$ . However, I do not really know how to use the condition $|f(x)|\le \alpha$ , and cannot figure out the proof. Any help is appreciated","['derivatives', 'analysis']"
4688142,Partial product martingale and Kakutani's theorem without independence,"Let $S_0, \ldots, S_n$ be positive random variables with $S_0 = 1$ and of the form \begin{equation} \tag{1}
S_n = S_0 \prod_{i=1}^{n} (1+X_i).
\end{equation} The standard assumption (for example in the Kakutani's theorem) is that if the $(X_i)$ are independent and centered, then $(S_n)$ is a martingale. But this is just a sufficient condition. More precisely, we have the following necessary and sufficient condition: $(S_n)$ is a martingale if and only if $\prod_{i=1}^{n} (1+X_i)$ is integrable and \begin{equation} \tag{2}
\mathbb{E}[X_n \, \vert \, \sigma(S_0, \ldots, S_{n-1}) ] = 0.
\end{equation} My feeling is that there are examples where $(2)$ is satisfied and $(S_n)$ is a martingale, but the $X_n$ are not independent. Indeed, a constant conditional expectation does not imply that the random variable has to be independent of the $\sigma$ -algebra. However, in this specific situation with positive random variables $S_n$ and the product form $(1)$ , I fail to construct an example. Are there any good examples of dependent sequences $X_n$ having this property? I guess we need dependent but probably uncorrelated $X_n$ such that the $S_n$ are still positive and a martingale.","['independence', 'conditional-expectation', 'martingales', 'probability-theory', 'probability']"
