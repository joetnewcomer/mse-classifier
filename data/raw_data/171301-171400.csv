question_id,title,body,tags
3035936,"If X has beta property, do $(x_i)_i$ points are LUR?","Let $X$ be a Banach space, $S_X$ its unit sphere and $B_X$ its unit ball. The space $X$ is said to has $\beta$ property if there exists a system $\{(x_i,f_i):i \in I\} \subset S_X \times S_{X^*}$ and $0 \leq \rho <1$ such that (i) $f_i(x_i)=1$ , for all $i \in I$ (ii) $|f_i(x_j)| \leq \rho$ , for all $i \neq j$ (iii) $||x|| = \sup\{|f_i(x)|:i \in I\}$ , for all $x \in X$ A point $x \in S_X$ is called locally uniformly convex (LUR) if for every $\epsilon>0$ there exists $\delta(\epsilon)>0$ such that $y \in S_X, \cfrac{||x+y||}{2}>1-\delta(\epsilon) \Rightarrow ||x-y||<\epsilon $ I'm trying to prove that if $X$ has $\beta$ property and $\dim X >1$ , such points $(x_i)_{i}$ cannot be LUR points. My first attempt at a solution was try to define a sequence $(x_n)$ in $S_X$ such that $||x_n+x_i||$ approaches to $2$ and $||x_n-x_i||$ does not approach to $0$ . It is easy to see that if $x \in S_X \cap \ker{f_i}$ , we have that $||x-x_i|| \geq 1$ , then maybe it is a good idea to define such sequence in $S_X \cap \ker{f_i}$ . Is it a good approach? Any other hints?","['banach-spaces', 'functional-analysis']"
3035942,Fibers Under a Covering Map are Discrete Subspaces of the Domain,"In Munkres' topology book, the following claim is made: If $p : E \to B$ is a covering map, then for every $b \in B$ , $p^{-1}(b)$ is a discrete subspace of $E$ . Here's my attempt at a proof: Given $b \in B$ , there's an open set $U \ni b$ and disjoint open sets $\{V_i\}_{i \in I}$ in $X$ such that $p^{-1}(U) = \bigcup_{i \in I} V_i$ and such that $p \big|_{V_i} : V_i \to U$ is a homeomorphism. Hence, if $x \in p^{-1}(b) \subseteq p^{-1}(U)$ , then $x \in V_i$ for some $i \in I$ . If $y \in p^{-1}(b) \cap V_i$ , then $p(y) = b$ and $y \in V_i$ . But $p \big|_{V_i}$ is, in particular, injective so $p \big|_{V_i}(x) = b = p \big|_{V_i}(y)$ implies $x=y$ . This shows $p^{-1}(b) \cap V_i = \{x\}$ , proving that $\{x\}$ is open in $p^{-1}(b)$ . Does this seem right?","['general-topology', 'algebraic-topology', 'covering-spaces']"
3035968,Interpretation of Symmetric Normalised Graph Adjacency Matrix?,"I'm trying to follow a blog post about Graph Convolutional Neural Networks. To set up some notation, the above blog post denotes a graph $\mathcal{G}$ , it's adjacency matrix $A$ , and the degree matrix $D$ . A section of that blog post then says: I understand how an adjacency matrix can be row-normalised with $A_{row} = D^{-1}A$ , or column normalised with $A_{col} = AD^{-1}$ . My question: is there some intuitive interpretation of a symmetrically normalized adjacency matrix $A_{sym} = D^{-1/2}AD^{-1/2}$ ?","['graph-theory', 'adjacency-matrix', 'matrices', 'algebraic-graph-theory', 'symmetric-matrices']"
3035983,How can I find integers which satisfy $\frac{150+n}{15+n}=m$?,"Here are some facts about myself: In $2017$ I was $15$ years old. Canada, my country, was $150$ years old. When will be the next time that my country's age will be a multiple of mine? I've toned this down to a function. With $n$ being the number of years before this will happen and $m$ being any integer, $$\frac{150+n}{15+n}=m$$ How would you find $n$ ?","['algebra-precalculus', 'integers', 'divisibility', 'recreational-mathematics']"
3035985,Function composition - getting different outputs for the same input.,"Given: $$ f(x) = 3x-1 $$ $$ g(x) = x^3+2 $$ If you evaluate $g(f(1))$ by doing $f(1)$ first and inputting its value into $g(1)$ , you get: $$ f(1) = 3(1) - 1 = 2 $$ $$ g(f(1)) = 2^3+2 = 10 $$ But if you try to substitute $x$ with $f(x)$ in $g(x)$ , i.e: $$ f(x) = 3x-1 $$ $$ g(x) = x^3+2 $$ $$ g(f(x)) = (3x-1)^3+2 $$ If you evaluate the last expression, you get: $$ g(f(x)) = 27x^3-1+2 $$ i.e $$ g(f(x)) = 27x^3+1 $$ $g(f(1))$ now gives a different value (28) instead of 10. Did I make a mistake with the evaluation of the parenthesis or is there a rule here that the parenthesis shouldn't be evaluated in certain circumstances? Thanks.",['functions']
3035993,Proof verification: Strong Law of Large Numbers Under Fourth Moment Control,"I'm trying to solve the following exercise. Show that if $(X_n)_{n =1}^\infty$ is a sequence of independent real-valued random variables with mean zero and uniformly bounded fourth moment, $n^{-1}\sum_{j=1}^n X_j \to 0$ almost surely. (Hint: Think about a fourth moment tail bound.) Here's what I did. 
Without loss, by rescaling $X_i$ , we may assume that $\mathbf{E} X_i^4 \leq 1$ . Note that this means $\mathbf{E} X_i^2 = \|X_i\|_2^2 \leq \|X_i\|_4^2 = (\mathbf{E} X_i^4)^{1/2} \leq 1$ .  Put $S_n = \sum_{j=1}^n X_j$ , observe that $\mathbf{E}S_n^4 \leq n + (n^2 - n) = n^2$ , simply because the other cross terms vanish due to independence. Following the hint, notice that by Markov's inequality, $$
\mathbf{P}(|n^{-1}S_n|\geq n^{-1/8}) = \mathbf{P}(S_n^4 \geq n^{3.5}) \leq n^{-3.5} \mathbf{E}S_n^4 \leq n^{-1.5}.
$$ The upshot of this is that with $A_n = \{|n^{-1}S_n| \geq n^{-1/8}\}$ , $\mathbf{P}(A_n)$ is summable, and hence $\mathbf{P}(A_n~\mathrm{i.o.}) = 0$ by Borel-Cantelli Lemma 1. If $\omega$ is such that $|n^{-1}S_n(\omega)| \geq n^{-1/8}$ for finitely many terms, then $\limsup_n |n^{-1}S_n(\omega)| = 0$ , and hence $n^{-1}S_n(\omega) \to 0$ . But the set of such $\omega$ is almost sure (indeed, the complement of $A_n~\mathrm{i.o.}$ ), proving the claim.","['measure-theory', 'proof-verification', 'probability-theory']"
3035999,solutions to $\int_{-\infty}^\infty \frac{1}{x^n+1}dx$ for even $n$,"I was playing around with glasser's master theorem and integrals of the form $$\int_{-\infty}^\infty \frac{1}{x^n+1}dx$$ I observed that for positive, even values of n, the solution to the integral followed the form $$\frac{\pi}{\frac{n}{2}\sin(\frac{\pi}{n})}$$ , the obvious example being at n = 2, the integral equals $\pi$ . I've yet to prove this pattern, but I recognized it after some fiddling around. Is this integral part of some larger concept that is already documented? For example, the way that $\int_0^\infty \frac{x^{p-1}}{e^x-1}dx = \zeta(p)\Gamma(p), p>1$ , I can't help but think about why the general solution for even values has a sine function in it.","['integration', 'improper-integrals', 'definite-integrals', 'infinity', 'rational-functions']"
3036004,"Algebra Mess, don't know how to proceed","I have kind of an algebra problem. The original question is a Bilinear transformation for analogue to digital filters.
(This is not a homework question) In my lecture notes, he goes to the answer like 1 step, I'm trying to work it out but I'm getting stuck at one place and I don't know how to proceed to get the same way he has it.",['algebra-precalculus']
3036024,Two different results with residue calculus for the integral $\int_{0}^{2\pi}\frac{\cos(3x)}{5-4\cos(x)}dx$. What went wrong?,"So I have to evaluate the following integral : $$\int_{0}^{2\pi}\frac{\cos(3x)}{5-4\cos(x)}dx$$ So I solve as usual with the residue theorem and by using $\cos(3x)= Re(e^{3ix})$ , $\cos(x)=\frac{e^{ix}+e^{-ix}}{2}$ and $z=e^{ix}$ , but then I have some troubles evaluating the residues. I get as poles 2 and $\frac{1}{2}$ where only $\frac{1}{2}$ is in our domain (circle of radius 1). When I calculate the residue using $\frac{g(z)}{f'(z)}$ , I get $\frac{\frac{1}{2^3}}{5-2}=\frac{1}{24}$ (I put the i outside, that's not the problem here) but when I use $\lim_{z \to \frac{1}{2}} (z-\frac{1}{2})\frac{z^3}{-(z-2)(z-\frac{1}{2})}$ , I get $\frac{1}{12}$ so that when I multiply by $2 \pi $ (again the i is not the problem here), I get $\frac{\pi}{6}$ . WolframAlpha gets $\frac{\pi}{12}=2\pi *\frac{1}{24}$ so my second way of finding the residue ( $\frac{1}{12}$ --> $\frac{\pi}{6}$ ) seems to be wrong. My question is, what did I do wrong ?","['integration', 'complex-analysis', 'residue-calculus']"
3036126,Does the power method converge?,"I want to check if the power method converges for the matrix $A$ and the vector $\vec{v} $ where $$A=\begin{pmatrix}\lambda & 1 \\ 0 & -\lambda\end{pmatrix} \text{ and } 
\vec{v} =\begin{pmatrix}1 \\ 1\end{pmatrix}$$ Do we have to apply some steps of the power method and see if the results converge to the absolute maximum eigenvalue of the matrix A or is there a criterion for convergence?","['matrices', 'numerical-methods', 'approximation', 'eigenvalues-eigenvectors']"
3036130,Gap in spiral sequence,"OEIS sequence A272573 describes a sequence generated in the following way: Start a spiral of numbers on a hexagonal tiling, with the initial hexagon as a(1) = 1. a(n) is the smallest positive integer not equal to or previously adjacent to its neighbors. The sequence begins 1,2,3,4,5,6,7,4,6,8,... . If you look at the scatterplot, there appears to be a very sparse region. Question I'm curious if there is some heuristic which explains why this gap appears. (In particular, the heuristic should also account for the fact that such a gap does not appear in A260643 , the analogous sequence on a square grid.)","['oeis', 'sequences-and-series']"
3036134,Formally proving that the image of $f(E)$ is of measure zero when $E$ is a null set and f is $C^1$,"So $f:U \rightarrow R^n$ is a continuously differential function, and $U \subset R^n$ is an open set. When $E$ a set of measure zero, I need to prove that $f(E)$ is also of measure zero. I know that this question was asked here before, but I still can't formalize my proof because I am missing something. I know that since $E$ is of measure zero then $E \subset \bigcup_{i}Q_i$ where $Q_i$ are intervals and $\sum_{i}|Q_i| < \epsilon$ for $\epsilon > 0$ . I also know that since $U$ is an open set there exist intervals $R_i$ that satisfy $U = \bigcup_{j}R_j$ . Also I know that if I look at $E  \bigcap ([-k,k] $ x ... x $[-k,k])$ then $f$ is lipshitz on this domain, let's call it $f_k$ , and $f(E) = \bigcup_{k}E(f_k)$ . But still I am missing here something that will prove that $f(E)$ is covered by intervals with measure zero each.","['multivariable-calculus', 'calculus', 'measure-theory']"
3036139,Proof verification: Tightness of a family of random variables converging in distribution.,"I'm trying to solve the following exercise: Suppose that $X_n \to X$ in distribution. Show that $(X_n)_{n \geq 1}$ is a tight family. First, just to recall the definition for those possibly unfamiliar, such a family of random variables (assumed to be defined on the same probability space, and real-valued) is tight iff for every $\epsilon > 0$ , there is $K$ such that $\sup_n \mathbf{P}(|X_n| \geq K) \leq \epsilon$ . Here's the argument I had in mind. First let $1 > \epsilon > 0$ be given and notice that we can find a $K_0$ such that $F_X(K_0) \geq 1 - \epsilon/2$ , since $F_X(t) \to 1$ as $t \to +\infty$ . Without loss, we can also assume that $K_0$ is a continuity point of $F_X$ (simply because $F_X$ is nondecreasing, and there are countably many discontinuities of $F_X$ , and so replace $K_0$ by some element of $[K_0, K_0 + 1]$ , if necessary). Notice that $$
\mathbf{P}(|X_n| \geq K_0) = 1 - F_{X_n}(K_0) + F_X(K_0) - F_X(K_0) \leq (1 - F_X(K_0)) + |F_{X_n}(K_0) - F_X(K_0)|.
$$ By choice of $K_0$ , $1 - F_X(K_0) \leq \epsilon/2$ . 
Hence since $X_n \to X$ in distribution and $K_0$ is a continuity point of $F_X$ , there is $N$ large such that $\sup_{n \geq N} \mathbf{P}(|X_n| \geq K_0) \leq \epsilon$ . For $n < N$ , put $K_n$ such that $\mathbf{P}(|X_n| \geq K_n) \leq \epsilon$ (as before, such $K_n$ necessarily exists). Now taking $K= \max\{K_0, K_1, \dots, K_N\}$ , we have $\sup_{n} \mathbf{P}(|X_n| \geq K) \leq \epsilon$ , as needed.","['proof-verification', 'probability-theory', 'weak-convergence', 'random-variables']"
3036169,A curiosity: how do we prove $\mathbb{R}$ is closed under addition and multiplication?,"So I tried looking around for this question, but I didn't find much of anything - mostly unrelated-but-similarly-worded stuff. So either I suck at Googling or whatever but I'll get to the point. So far in my coursework, it seems like we've mostly taken for granted that $(\mathbb{R},+,\cdot)$ is a field. I'm not doubting that much, that would seem silly. However, my question is: how would one prove this? In particular, how would one prove that $(\mathbb{R},+)$ and $(\mathbb{R}\setminus \{0\}, \cdot)$ are closed under their respective operations? I understand the definition of closure, but to say ""a real number plus/times a real number is a real number"" seems oddly circular since, without demonstrating that, it essentially invokes the assumptions we're trying to prove. Obviously, there's something ""more"" to the definition of ""real number"" that would make proving this possible. Though I'm not sure what property would be used for this. One thought I dwelled on for a while was instead looking at what the real numbers are not . For example, they are numbers lacking those ""imaginary"" components you see in their higher-dimensional generalizations - the complex numbers ( $i$ ), quaternions ( $i,j,k$ ), and so on. But that didn't seem quite ""right"" to me? Like I'm not sure if it's really wrong, it just irked me in some way. Like it's simple enough to say ""a real number is any complex number with a zero imaginary component,"" take two real numbers, show their imaginary parts sum/multiply to zero, and thus we have a real number. Maybe it's just a personal issue? Like I said - I'm not saying it's inherently wrong (it might be, though, I don't know - if it is, I would like to know why). Maybe it's just the whole idea of ""defining a number by what's it's not "" that bugs me. Like I said, I'm not really sure, and I think I'm rambling/unclear enough as it is, so I'll get straight to the point. In short, how does one properly demonstrate, if not in the above way, $$a,b \in \mathbb{R} \Rightarrow (a+b)\in \mathbb{R}$$ $$a,b\in \mathbb{R \setminus \{0\}} \Rightarrow (a\cdot b) \in \mathbb{R \setminus \{0\}}$$ (And again, I'm not at all doubting that these are true. I'm just curious as to how one would demonstrate these facts in the most appropriate manner since I don't believe it's come up in my coursework and I've been curious on how one would prove it for a few days now.)","['real-numbers', 'real-analysis', 'field-theory', 'group-theory', 'abelian-groups']"
3036192,Percolation Textbook Recommendation,"I was wondering if someone could recommend a Percolation textbook for undergraduates.  I have looked at Percolation by Grimmett and it seems quite dense.  I was looking for a book that I could self-study from with a decent amount of explanation.  As far as background, I have taken a graduate course in measure theory and the undergraduate analysis sequence. Thanks.","['measure-theory', 'probability-theory', 'analysis', 'statistical-mechanics', 'percolation']"
3036195,Group cohomology of the natural action of automorphism group on a finitely generated abelian group,"It's well known that we can classify finitely generated abelian groups. Let $M$ be a finitely generated abelian group, in principle we can decide the group structure of $G=Aut(M)$ from $M$ . What about the group cohomology? In other words, what are the group structures of $H^i(G,M)$ ( $i \geq 0$ )? More modestly, what are the orders of $H^i(G,M)$ ? If we can't do things precisely, is there any bound?","['homological-algebra', 'group-theory', 'abstract-algebra', 'group-cohomology']"
3036328,Question on word combinations with exclusivity,"""How many 4 letter words on the alphabet {a,b,c} in which 'a' occurs exactly twice are there?"" My answer is incorrect as I answered 3*3*2*2 4 letter words. However, this doesn't necessarily remove 'a' from the letter choices. What would be the correct way to solve this?","['permutations', 'combinations', 'combinatorics-on-words', 'combinatorics', 'discrete-mathematics']"
3036340,"Show that there exist $[a,b]\subset [0,1]$, such that $\int_{a}^{b}f(x)dx$ = $\int_{a}^{b}g(x)dx$ = $\frac{1}{2}$","Let $f(x)$ and $g(x)$ be two continuous functions on $[0,1]$ and $$\int_{0}^{1}f(x) dx= \int_{0}^{1}g(x)dx = 1$$ Show that there exist $[a,b]\subset [0,1]$ , such that $$\int_{a}^{b}f(x) dx= \int_{a}^{b}g(x)dx = \frac{1}{2} $$ The question can be solved by considering the fundamental group of $S^1$ , now I am wondering if we can solve it by real-analysis. Here is the topological solution: Assume that for any $[a,b]\subset[0,1]$ , we always have $$(\int_{a}^{b}f(x) dx\neq \frac{1}{2}) \quad \vee \quad(\int_{a}^{b}g(x)dx \neq \frac{1}{2}) $$ Consider mapping $$\phi:D\rightarrow\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\},\,\,(x,y)\mapsto(\int_{y}^{x}f(t) dt,\int_{y}^{x}g(t) dt)$$ where $D=\{(x,y)|0\leq x\leq y\leq 1\}$ . Let $a$ be path from $(0,0)$ to $(0,1) $ in $D$ and $b$ be path from $(0,1)$ to $(1,1) $ in $D$ , then $ab$ is a path from $(0,0)$ to $(1,1) $ in $D$ and $$\phi\circ (ab):[0,1]\rightarrow\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\} $$ Notice that $$(\phi\circ (ab))(0)=(\phi\circ (ab))(1)=(0,0)$$ so $\phi\circ (ab)$ is a loop based on $(0,0)$ in $\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\}$ . For any $t\in [0,\frac{1}{2}]$ , it's not hard to get that $$(\phi\circ (ab))(t+\frac{1}{2})=(1,1)-(\phi\circ (ab))(t)$$ is equivalent to $$(\phi\circ (ab))(t+\frac{1}{2})-(\frac{1}{2},\frac{1}{2})=-((\phi\circ (ab))(t)-(\frac{1}{2},\frac{1}{2}))$$ Define retraction $$r:\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\}\rightarrow S^1,\quad (x,y)\rightarrow\ \frac{(x,y)-(\frac{1}{2},\frac{1}{2})}{||(x,y)-(\frac{1}{2},\frac{1}{2})||}$$ Then $r\circ \phi\circ (ab)$ is a loop on $S^1$ , such that $$(r\circ \phi\circ (ab))(t+\frac{1}{2})=-(r\circ \phi\circ (ab))(t),\quad\forall t\in [0,\frac{1}{2}]$$ So $<r\circ \phi\circ (ab)>$ is not trivial in $\pi_1(S^1)$ . However, $ab$ is path homotopic to $c$ in $D$ , where $c$ is the path between $(0,0)$ and $(1,1)$ in $D$ . In this case, $r\circ \phi\circ (ab)$ is a point-path in $S^1$ by $$(\phi\circ c)(t)=\phi(t,t)\equiv (0,0)$$ which leads to contradiction.","['algebraic-topology', 'real-analysis']"
3036346,a Function to Push Numbers Away From a Central Number,"I'm looking for a function that takes an array of values between a..b (like 0..1) and a central point a < c < b (like 0.5) and a factor (like 2, 3, 4, etc) and pushes all the array's values away from the central point. For example if we have [0.0, 0.25, 0.48, 0.56, 0.87, 0.98] as input to the function, I'm expecting to get something like [0.0, 0.23, 0.41, 0.65, 0.895, 0.9848]. The factor number should be able to control how much the numbers are stretched away from the central point. There are some implementation details like how much should the factor affect the stretching that I leave up to you. I just want a pushing function to do something as mentioned above. If there is a name for these kinds of functions, I'd be glad if you could point me towards that direction. Thanks...","['functions', 'percentages']"
3036351,Sufficient statistics for $p$ for a random sample from $\text{Ber}(p)$ distribution,"Let $X_1,X_2, X_3$ be a random sample from Bernoulli distribution $B(p)$ . Which of the following is sufficient statistic for $p$ ? $(A)\ \ X_{1}^{2}+X_{2}^{2}+X_{3}^{2}$ $(B) \ \ X_1+2X_{2}+X_{3}$ $(C)\ \ 2X_1-X_{2}-X_{3}$ $(D) \ \ X_1+X_{2}$ $(E)\ \ 3X_1+2X_{2}-4X_{3}$ We know $T=\sum_i^3 X_i$ is sufficient statistic for $p$ via Neymann Factorization theorem. The reasoning for different options. $(A)$ It's not a one-one function of sufficient statistic $T$ (If it was $T^2$ , was it sufficient statistic then? My answer is yes because it is one to one function of sufficient statistic our random variable is positive am I right?). $(B)X_1+X_{2}+X_{3}+X_{2}$ It contains original statistic therefor it is suffient statistic for $p$ . $(D)$ It doesn't include $X_3$ So it's not sufficient statistic for $p$ All other options include subtraction in it so I ruled out all of them. I think my reasoning is not very good I lack some intuition behind finding sufficient statistic. Correct me here, please.","['statistical-inference', 'statistics', 'binomial-distribution', 'estimation']"
3036362,Exact value of Hausdorff measure of two dimensional Cantor set,"Let $\mathcal{C}$ denote the classical Cantor set, then it is well-known that $\mathcal{C}$ has Hausdorff dimension $\alpha = \ln 2 /\ln 3$ , and its $\alpha$ -dimensional Hausdorff measure is $\mathcal{H}_\alpha(\mathcal{C}) = 1$ . For $\mathcal{C}\times \mathcal{C}\subset \mathbb{R}^2$ , it is fairly easy to show its Hausdorff dimension is $2\alpha$ , and $0< \mathcal{H}_{2\alpha}(\mathcal{C}\times \mathcal{C}) <\infty$ . I am interested in knowing the exact value of $\mathcal{H}_{2\alpha}(\mathcal{C}\times \mathcal{C})$ . The proof for 1D case $\mathcal{H}_\alpha(\mathcal{C}) = 1$ cannot carry directly into higher dimension. This answer claims the measure is $1$ and refers to Falcon's book The Geometry of Fractal Sets . However, after really looking into the book, it does not say anything about Hausdorff measure (only Hausdorff dimension) for any non-1D set (at least at the position quoted). Nonetheless, it might be possible that the Hausdorff measure does not admit a simple closed-form, as expected for many fractals in $\mathbb{R}^n$ with $n>1$ . Any idea/reference is much appreciated.","['measure-theory', 'hausdorff-measure', 'geometric-measure-theory', 'real-analysis', 'fractals']"
3036399,Does $\int_0^\infty\frac{\ln(1+x)}{x(1+x^n)}dx$ have a general form?,"Does $$I_n=\int_0^\infty\frac{\ln(1+x)}{x(1+x^n)}dx$$ have a general form? I tried to evaluate some small $n$ s. For $n=1$ , $I_1$ is obviously $\frac16\pi^2$ . For $n=2$ , see here . $I_2=\frac5{48}\pi^2$ . For $n=3$ , I put it in Mathematica and get $$\small{\frac{1}{108} \left(9 \left(4 \left(\text{Li}_2\left(\frac{\sqrt[6]{-1}}{\sqrt{3}}\right)+\text{Li}_2\left(-\frac{(-1)^{5/6}}{\sqrt{3}}\right)\right)+\log ^2(3)\right)+5 \pi ^2\right)}$$ Use the result $$\Re\operatorname{Li}_2\left(\frac{1+ti}2\right)=\frac1{12}\pi^2-\frac12\arctan^2t-\frac18\ln^2\frac{1+t^2}4,$$ I'm able to show $I_3=\frac5{54}\pi^2$ . For $n=4$ , I numerically found $I_4=\frac{17}{192}\pi^2$ . I'm not able to find the general form with $n\in \mathbb{Z}^+$ .","['integration', 'calculus', 'definite-integrals']"
3036431,Does $\pi$ contain every real?,"One can generate decimal expansions between $0$ and $1$ by taking with the $m$ th digit of $\pi$ , and then taking every $n$ th digit. $\pi = 3.141592653589793238462643383279502884197169399...$ for instance, for $m=3$ , $n=5$ , this will be 3.1 4 1592 6 5358 9 7932 3 8462 6 4338 3 2795 0 2884 1 9716 9 399... giving $0.469363019...$ I can't see why this shouldn't be able to create every real number. But if this creates any real number given a pair if natural numbers $m$ and $n$ , it would mean that the cardinality of the continuum is equal to that of the natural numbers. So I guess this method can't create any real. But why?",['elementary-set-theory']
3036489,Proof for this binomial coefficient's equation,"For $k, l \in \mathbb N$ $$\sum_{i=0}^k\sum_{j=0}^l\binom{i+j}i=\binom{k+l+2}{k+1}-1$$ How can I prove this? I thought some ideas with Pascal's triangle, counting paths on the grid and simple deformation of the formula. It can be checked here (wolframalpha) . If the proof is difficult, please let me know the main idea. Sorry for my poor English. Thank you. EDIT:
I got the great and short proof using Hockey-stick identity by Anubhab Ghosal, but because of this form, I could also get the Robert Z's specialized answer .
Then I don't think it is fully duplicate.","['summation', 'binomial-coefficients', 'combinatorics']"
3036510,Evaluate the limit of the sequence: $\lim_{n_\to\infty}\frac{\sqrt{(n-1)!}}{(1+\sqrt{1})\cdot(1+\sqrt{2})\cdot (1+\sqrt{3})\cdots (1+\sqrt{n})}$,Evaluate the limit of the sequence: $$\lim_{n\to\infty}\frac{\sqrt{(n-1)!}}{(1+\sqrt{1})\cdot(1+\sqrt{2})\cdot (1+\sqrt{3})\cdots (1+\sqrt{n})}$$ My try: Stolz-cesaro: The limit of the sequence is $\frac{\infty}{\infty}$ $$\lim_{n\to\infty}\frac{a_n}{b_n}=\lim_{n\to\infty}\frac{a_{n+1}-a_n}{b_{n+1}-b_n}$$ For our sequence: $\lim_{n\to\infty}\frac{\sqrt{(n-1)!}}{(1+\sqrt{1})\cdot(1+\sqrt{2})\cdot (1+\sqrt{3})\cdots (1+\sqrt{n})}=\lim_{n\to\infty}\frac{\sqrt{n!}-\sqrt{(n-1)!}}{(1+\sqrt{1})\cdot(1+\sqrt{2})\cdot (1+\sqrt{3})\cdots (1+\sqrt{n})\cdot(1+\sqrt{n+1})-(1+\sqrt{1})\cdot(1+\sqrt{2})\cdot (1+\sqrt{3})\cdots (1+\sqrt{n})}=\lim_{n\to\infty}\frac{\sqrt{(n-1)!}\cdot(\sqrt{n-1})}{\left((1+\sqrt{1})\cdot(1+\sqrt{2})\cdot (1+\sqrt{3})\cdots (1+\sqrt{n})\right)\cdot(\sqrt{n}+1)}$ Which got me nowhere.,"['limits', 'calculus', 'sequences-and-series']"
3036554,Limits of a multiple integral function,"Problem Let $f(x)\in L^{1}(\mathbb{R}^N)\cap L^{\infty}(\mathbb{R}^N)$ and $S_t=\left\{x\in\mathbb{R}^N: |x_1|\le t\right\}$ with $t>0$ . Let $\phi(t)$ the integral function $$\phi(t)=\int_{\mathbb{R}^N\setminus S_t}|f(x)|^2dx$$ Find $$\lim_{t\to 0^{+}}\phi(t) \ \ \ \mbox{and} \ \ \ \lim_{t\to +\infty}\phi(t)$$ My approach Since $f(x)\in L^{1}(\mathbb{R}^{N})\cap L^{\infty}(\mathbb{R}^N)$ then $f(x)\in L^{2}(\mathbb{R}^{N})$ . Infact, by definition of integral $$\phi(t)=\int_{\mathbb{R}^N\setminus S_t}|f(x)|^2dx=\int_{\mathbb{R}^{N}}\chi_{\mathbb{R}^N\setminus S_{t}}(x)|f(x)|^2dx\le\\ \\ \le \int_{\mathbb{R}^{N}}|f(x)|^2dx\le \|f\|_{\infty}\int_{\mathbb{R}^{N}}|f(x)|dx=\|f\|_{\infty}\|f\|_{1}<+\infty$$ This means that $\phi(t)$ is a well posed and bounded function: $$0\le \phi(t)\le \|f\|_{\infty}\|f\|_{1}\ \ \ \forall t\in (0,+\infty)$$ Note that for $0<t_1<t_2$ , $S_{t_1}\subset S_{t_2} \implies\mathbb{R}^N\setminus S_{t_1}\supset\mathbb{R}^N\setminus S_{t_2}$ so: $$\phi(t_1)=\int_{\mathbb{R}^{N}\setminus S_{t_1}}|f(x)|^2dx\ge \int_{\mathbb{R}^{N}\setminus S_{t_2}}|f(x)|^2dx=\phi(t_2)$$ Hence $\phi(t)$ is a bounded decreasing function, so $\lim_{t\to 0^{+}}\phi(t)$ and $\lim_{t\to +\infty}\phi(t)$ exist and are finite. Now, I would like to use the sequential criterion for a limit of a function: $$\lim_{t\to 0^{+}}\phi(t)=\lim_{n\to +\infty}\phi\left(\frac{1}{n}\right)=\lim_{n\to +\infty}\int_{\mathbb{R}^{N}\setminus S_{\frac{1}{n}}}|f(x)|^2dx=\\ \\ \\ =\lim_{n\to +\infty}\int_{\mathbb{R}^{N}}\chi_{\mathbb{R}^{N}\setminus S_{\frac{1}{n}}}(x)|f(x)|^2dx$$ Since $\chi_{\mathbb{R}^{N}\setminus S_{\frac{1}{n}}}(x)\le 1$ the sequence $$g_{n}(x):=\chi_{\mathbb{R}^{N}\setminus S_{\frac{1}{n}}}(x)|f(x)|^2\le |f(x)|^2 \ \ \forall x\in\mathbb{R}^{N}$$ I can use the dominated convergence theorem $$\lim_{n\to +\infty}\int_{\mathbb{R}^{N}}\chi_{\mathbb{R}^{N}\setminus S_{\frac{1}{n}}}(x)|f(x)|^2dx=\int_{\mathbb{R}^{N}}\lim_{n\to+\infty}\chi_{\mathbb{R}^{N}\setminus S_{\frac{1}{n}}}(x)|f(x)|^2dx=\int_{\mathbb{R}^{N}}|f(x)|^2dx=\|f\|_{2}^2$$ For the limit $\lim_{t\to +\infty}\phi(t)$ I use the sequence $t_{n}=n$ so $$\lim_{t\to +\infty}\phi(t)=\lim_{n\to +\infty}\int_{\mathbb{R}^N}\chi_{\mathbb{R}^{N}\setminus S_{n}}(x)|f(x)|^2dx=\int_{\mathbb{R}^N}\overbrace{\lim_{n\to +\infty}\chi_{\mathbb{R}^{N}\setminus S_{n}}(x)}^{=0}|f(x)|^2dx=0$$ Is this approach ok? Thanks.","['integration', 'measure-theory', 'real-analysis', 'lp-spaces', 'functional-analysis']"
3036557,Where is my mistake? Calculating surface integral/Stoke's theorem,"Let $F(x,y,z)= \begin{pmatrix} -y \\ 2x\\z \end{pmatrix}$ be a vector field and $A$ a hemisphere with $x^2+y^2+z^2=9 $ , $ z>0 $ with a circular edge at the $x,y $ - level with the unit normal vector $n$ showing outwards. I want to determine $ \int_A ( \nabla \times F) n\; do $ 1) as a surface integral 2) with Stoke's theorem. 1) I used the parametrization $ \Phi(\phi, \theta) =\begin{pmatrix} R\sin \theta \cos\phi \\ R\sin\theta \sin \phi\\R\cos \theta \end{pmatrix}$ with $ 0 \leq \phi \leq 2 \pi $ and $ 0\leq \theta\leq \frac{\pi}{2}$ for the unit normal: $ \frac{ \delta \Phi }{ \phi} \times \frac{ \delta \Phi}{ \theta} = \begin{pmatrix} R^2\sin^2 \theta \cos \phi \\ R^2\sin^2 \theta \sin \phi \\R^2\sin \theta \cos\theta \end{pmatrix}$ so integrate $\int_0^{ \frac{ \pi}{2}} \int_0^{2 \pi} \begin{pmatrix} -Rsin\theta \cos \phi \\ 2R\sin \theta \sin \phi \\R \cos\theta \end{pmatrix}\begin{pmatrix} R^2\sin^2 \theta \cos \phi \\ R^2\sin^2 \theta \sin \phi \\R^2\sin \theta \cos\theta \end{pmatrix} d\phi d\theta = 2 \pi R^3 $ 2) for Stoke's theorem I use as parametrization $ \Phi ( \phi) = \begin{pmatrix} -\sin \phi \\ \cos \phi \\ 0 \end{pmatrix} $ because $z=0 .$ Then I get to calculate following: $\int_0^{2 \pi} \begin{pmatrix} -r \sin \phi \\ 2r\cos \phi \\ -r^2 \end{pmatrix}\begin{pmatrix} -\sin \phi \\ \cos \phi \\ 0 \end{pmatrix} d \phi =\int_0^{2 \pi} r \sin^2 \phi+ 2r \cos^2 \phi d \phi = 3\pi r, $ and with $r=3$ follows $ 9 \pi .$ So, they are not equal. 
I dont see my mistake. Could not find one in the calculations, so there must be one in the process?
I have lost perspective, I appreciate any help a loot !!","['integration', 'surface-integrals', 'multivariable-calculus', 'multiple-integral', 'stokes-theorem']"
3036609,"How pathological can the boundary of an open, simply connected subset of $\mathbb{C}$ be?","In my complex analysis class we’re currently covering the Riemann Mapping Theorem, and as is well-known there are no conditions imposed on the actual shape of the region $\Omega$ , except that it must be open, simply connected, and not all of $\mathbb{C}$ . So I was wondering how pathological such a region can really be, and particularly its boundary $\partial \Omega$ . For example, can the boundary be a non-nullset? No-where differentiable? Can it be that the interior of the closure of $\Omega$ is not the same as $\Omega$ itself? And so on. This question may also be extended to $\mathbb{R}^n$ . However, I feel like “simply connected” is no longer the right analogue here, because $\mathbb{R}^3$ with a disjoint set of points removed is simply connected, which is not true of $\mathbb{R}^2$ . I think to generalize, the question would have to be “How pathological can the boundary of an open, connected subset $\Omega \subset \mathbb{R}^n$ be, given that $\mathbb{R}^n \cup \{\infty\} \setminus \Omega$ is also connected?”.","['complex-analysis', 'examples-counterexamples', 'analysis']"
3036667,Identify a truth-teller among a group of truth-tellers and (honest) liars.,"This question is inspired by this thread .  In that thread, a liar may both tells lies and truths.  However, in my version, liars always lie . Main Question. A group of people consists of $m$ truth-tellers (who always are truthful) and $n$ liars (who always lie) where $m$ and $n$ are positive integers.  In the group, everybody knows whether another from the group is a truth-teller or a liar.  You do not have this information whatsoever, and cannot discern the distinction between a truth-teller and a liar, but you know the values of $m$ and $n$ . The aim is to identify a truth-teller within the group.  You can only ask a person $A$ about another person $B$ whether $B$ is a liar.  If $N(m,n)$ is the smallest possible number of questions you need to guarantee that the job can be accomplished, then determine the value of $N(m,n)$ for each pair $(m,n)\in\mathbb{Z}_{>0}\times\mathbb{Z}_{>0}$ . Known Results: If $m=n$ , then $N(m,n)$ does not exist. If $m\neq n$ , then Mike Earnest showed that $$N(m,n)\leq \max\big\{n,2\,\min\{m,n\}\big\}\,.$$ If $m<n$ , then Todor Markov gave an improvement: $$N(m,n)\leq \max\big\{n-1,2\,\min\{m,n\}\big\}\,.$$ If $m>n$ , then the user fedja found that $$N(m,n)\leq 2n-1\,.$$ For all $m>1$ , $N(m,1)=1$ . I know that $N(2,3)=3$ , $N(2,4)=4$ , and $N(2,m)=m-1$ for $m\geq 5$ . The user fedja and I discovered that $N(3,2)=2$ and $N(m,2)=3$ for all $m\geq 4$ . The user fedja found that $N(m,3)=4$ for all sufficiently large $m$ , and $N(m,4)=7$ for all sufficiently large $m$ . However, if you want to solve the following generalized version of the question in the linked thread here, then you are very welcome.  In other words, I would also love to see the answer to this auxiliary question.  (It is also good if you put your answer to the question below in the thread above.) Auxiliary Question. A group of people consists of $m$ truth-tellers (who always are truthful) and $n$ drunkards (who may tell both truths or lies) where $m$ and $n$ are positive integers.  In the group, everybody knows whether another from the group is a truth-teller or a drunkard.  You do not have this information whatsoever, and cannot discern the distinction between a truth-teller and a drunkard, but you know the values of $m$ and $n$ . The aim is to identify a truth-teller within the group.  You can only ask a person $A$ about another person $B$ whether $B$ is a drunkard.  If $M(m,n)$ is the smallest possible number of questions you need to guarantee that the job can be accomplished, then determine the value of $M(m,n)$ for each pair $(m,n)\in\mathbb{Z}_{>0}\times\mathbb{Z}_{>0}$ .","['puzzle', 'logic', 'extremal-combinatorics', 'combinatorics', 'combinatorial-game-theory']"
3036708,Which 3-manifolds can be cubulated?,"I am trying to get a picture of what is currently known about cubulability of 3-manifolds, though cannot seem to find a good overview.  I am personally most interested in compact 3-manifolds with boundary embedded in $\mathbb{R}^3$ , but would be happy to hear any answers to this question.  If I had to name one concrete question, it would be: Question: Can you cubulate every compact 3-manifold $M \subset \mathbb{R}^3$ ?  If not, which ones can you cubulate?  What are some specific examples of non-cubulable $M$ ? I am aware of some scattered results, e.g., all hyperbolic 3-manifolds are cubulable (discussed in Sections 4.5, 4.6 of this paper ) there is some discussion about cubulability of Kähler groups/Kähler manifolds here there is a characterization in terms of the boundary here Apart from the result about hyperbolic 3-manifolds, I find it quite hard to connect largely algebraic results like these back to a more concrete geometric/topological picture.","['geometric-topology', 'geometric-group-theory', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
3036715,Sum of All Combinations of Products of Two Matrices,"Suppose that $\mathbf A$ and $\mathbf B$ are square, diagonalizable matrices. Consider the following infinite sum of all combinations of these two matrices: \begin{align} \mathbf S = \mathbf I &+\mathbf A + \mathbf B +\\
&+\mathbf A^2 + \mathbf A\mathbf B + \mathbf B\mathbf A + \mathbf B^2+ \\
&+\mathbf A^3 + \mathbf A^2\mathbf B + \mathbf A\mathbf B\mathbf A+ \mathbf A\mathbf B^2 + \mathbf B\mathbf A^2+ \mathbf B\mathbf A\mathbf B+ \mathbf B^2\mathbf A + \mathbf B^3+\\
&+\cdots
\end{align} Let $\mathbf A=\mathbf Q_A\mathbf \Lambda_A\mathbf Q_A^{-1}$ , and $\mathbf B=\mathbf Q_B\mathbf \Lambda_B\mathbf Q_B^{-1}$ be the eigendecompositions of $\mathbf A$ and $\mathbf B$ . If $\mathbf B=\mathbf 0$ , then the sum would be equal to $$\mathbf S = \sum_{k=0}^\infty \mathbf A^k=\sum_{k=0}^\infty(\mathbf Q_A\mathbf \Lambda_A\mathbf Q_A^{-1})^k=\mathbf Q_A \left(\sum_{k=0}^\infty\mathbf\Lambda_A^k\right)\mathbf Q_A^{-1}.$$ Is there a general way to express $\mathbf S$ as a function of $\mathbf Q_A$ , $\mathbf Q_B$ , and sums that involve $\mathbf \Lambda_A$ and $\mathbf \Lambda_B$ ?","['matrices', 'summation', 'linear-algebra', 'combinatorics']"
3036762,Losing solutions in $\cos{x}+\cos{2x}=0$,"I'm trying to solve $\cos x=-\cos2x$ , $-\pi<x\leq\pi$ , WITHOUT USING the double angle formulae, here's my current working: $$\begin{align*}\cos x&=-\cos2x\\
\cos (x\pm2k\pi)&=\cos (2x\pm(2k+1)\pi)\\
\implies x\pm2k\pi&=2x\pm(2k+1)\pi\\
x&=2x\pm\pi\\
x&=\pm\pi\\
\therefore x&=\pi\text{  , in the given range of $x$}\end{align*}$$ Graphically it's obvious I've missed two other solutions, $x=\pm\frac{\pi}{3}$ , however I do not know why because I have not, to my knowledge, carried out any illegal operations such as dividing by zero. Could someone point out my mistake please and give an edited solution? Thanks!","['algebra-precalculus', 'proof-verification', 'trigonometry']"
3036786,Examples of some Pointwise Convergent Sequences of Functions,"I have recently come across pointwise/uniformly convergent sequences of functions, and I am hoping if someone could give some examples of certain sequences of functions so that I could understand the concept better. Thanks! • Pointwise convergent sequences that do / do not preserve continuity, • Pointwise convergent sequences that do / do not preserve
integrals. It seems that $f_n(x) = x^n$ is an example that do not preserve continuity?","['pointwise-convergence', 'real-analysis']"
3036812,Average distance between point in a disc and line segment,"What is the average distance between a (randomly chosen) point in a disc of radius r and a line segment of length $a < 2r$ whose midpoint is at the center of the disc? [""Distance"" here being the shortest distance to any point on the line segment.]",['geometry']
3036831,Analyzing isolated singularities using $\lim_{z\to z_0}(z-z_0)f(z)$,"so I currently read about isolated singularities and residue. If we have a complex function $f(z)$ with a simple pole at $z_0$ we can use $$\operatorname{Res}(f;z_0)=\lim_{z\to z_0}(z-z_0)f(z) \tag{1}$$ to find its residue for $z_0$ . Now, I read that we can also use $(1)$ to analyze another isolated singularity $z_1$ . Basically I read that we can say the following: $(1)=c, c$ finite: we have a simple pole $(1)=\infty$ : Pole of higher order $(1)=0$ : $f(z)$ is analytic at $z_1$ Now, all three cases make sense to me, if we assume, that $z_1$ is a pole. Sadly, it wasn't clearly stated so I started to think about ""What if $z_1$ is a removable or essential singularity? Can we still use $(1)$ to gather information and if so, what kind of information?"" From how I understand $(1)$ and how it is derive and if one thinks about the properties of the Laurent-Series of removable and essential isolated singularities I'd say we can't use $(1)$ to really get more information about $z_1$ if we assume it to be any kind of isolated singularity.","['complex-analysis', 'limits', 'singularity']"
3036852,Why do we automatically assume that when we divide a polynomial by a second degree polynomial the remainder is linear?,"Given question: If a polynomial leaves a remainder of $5$ when divided by $x − 3$ and a remainder of $−7$ when divided by $x + 1$ ,
  what is the remainder when the polynomial is divided by $x^2 − 2x − 3$ ? Solution: We observe that when we divide by a second degree polynomial the remainder will generally be linear. Thus
  the division statement becomes $p(x) = (x^2 − 2x − 3)q(x) + ax + b $ Can someone please explain at a PRE-CALCULUS level? Thanks","['algebra-precalculus', 'quadratics', 'polynomials']"
3036855,Measure-theoretic definition of conditional probability,"I am trying to reconcile the formal definition of conditional probability via measure theory (as in this question's answer and the Wikipedia definition given here ) with the more elementary definitions of conditional probability given in early probability courses. By elementary definitions, I mean the following: for two continuous real-valued r.v. $X, Y$ which have densities $f_X, f_Y$ (with respect to the Lebesgue measure), we define the conditional density of $X$ given $Y$ to be $$f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)} \text{ when } f_Y(y) > 0$$ where $f_{X,Y}$ is the joint density of $X,Y$ (with respect to the product measure on $\mathbb{R}^2$ ). We have similar equations in the mixture (discrete-continuous) and discrete-discrete cases, as given on Wikipedia here . Question : How can we derive these ""definitions"" from the more general notions of conditional probability distributions defined via measure theory? I am also trying to make sense of things like $f_{X,Y|Z}(x,y|z)$ , i.e. the conditional density/distribution of $X,Y$ given $Z$ (which could be any combination of discrete and continuous r.v.'s or even taking values in a general space). It seems cumbersome to have to define all these cases in the elementary way. I hope answering my question will also clarify this. Thank you.","['conditional-probability', 'measure-theory', 'probability-theory']"
3036917,Find $\lim_{n\to\infty} \cos(\frac{\pi}{4}) \cos(\frac{\pi}{8})\ldots \cos(\frac{\pi}{2^n}) $,"I already know that $$ a_n = \cos\left(\frac{\pi}{2^{n+1}}\right) = \overbrace{\frac{\sqrt{2+\sqrt{2+\ldots + \sqrt{2}}}}{2}}^{n\text{ roots}}$$ Also I know that $$\lim_{n\to\infty}  2\cos\left(\frac{\pi}{2^n}\right) = 2
\text{ and if } a_n \xrightarrow {n\to\infty} a \text{ then } \sqrt[n]{a_1 a_2 \ldots a_n} \xrightarrow{n\to\infty} a $$ With that method I only got indeterminate form $$ \lim_{n\to\infty} \cos\left(\frac{\pi}{4}\right) \cos\left(\frac{\pi}{8}\right)\ldots \cos\left(\frac{\pi}{2^n}\right) = \Big(\frac{\sqrt[n]{a_1 a_2 \ldots a_n}}{2}\Big)^n = 1^\infty  $$ Anyone knows a working solution?","['trigonometry', 'real-analysis']"
3036959,rational points on the quadrifolium $(x^2 + y^2)^3 = (x^2 - y^2)^2$,"I have been reading the Wikipedia page on the Quadrifolium there are two of them: \begin{eqnarray*}
r &=& \sin 2\theta \\ 
(x^2 + y^2)^3 &=& 4 x^2 y^2
\end{eqnarray*} and it's $45^\circ$ -rotated version, which is also a rose curve: \begin{eqnarray*}
r &=& \cos 2\theta\\
(x^2 + y^2)^3 &=& (x^2 - y^2)^2 \\
\end{eqnarray*} These equations also have a $(x,y)$ -parameterization, which can be found with some algebra. \begin{eqnarray*}
x &=& \cos k \theta \cos \theta = \tfrac{1}{4}\big[ e^{i(k+1)\theta} + e^{i(k-1)\theta} + e^{-i(k-1)\theta} + e^{-i(k+1)\theta} \big] \\
y &=& \cos k \theta \sin \theta
\,= \tfrac{1}{4}\big[ e^{i(k+1)\theta} - e^{i(k-1)\theta} + e^{-i(k-1)\theta} - e^{-i(k+1)\theta} \big] \\
\end{eqnarray*} And $\cos k (\theta + \phi)$ for a rotated version.  Here $\theta = 0, \frac{\pi}{4}$ and $k = 2$ . Here is the picture of the quadrifolium, a kind of rose curve , and when it's rotated. The reason I am going to look at this question again, is because I wanted to study the rational paramterization. Let $X$ be our curve, with a singular point at the origin $(0,0)$ . We're trying to describe $X(\mathbb{Q})$ . The curve on the left right has no rational points. Naively, if I divide one equation by the other, I get an expression for the tangent function.  In fact, $\frac{x}{y} = \tan \theta$ for all values of $\theta \in [0, 2\pi]$ just as if it were a circle.  Let's define a map: $$ \big[ (x,y) = (\cos \theta, \sin \theta) \big] \mapsto 
\Big[ ( (2x^2 - 1 )x, (1 - 2y^2) y) = ( (2 \cos^2 \theta - 1) \cos \theta , (1 - 2\sin^2 \theta)\sin \theta ) \Big] $$ And we have that $LHS \in \mathbb{Q}$ if $RHS \in \mathbb{Q}$ .  Is this sufficient?  Is this map birational, is this enough for a paramerization? Here is even another strategy, since we have Fourier series (even just trigonometric series because there are only 4 terms). $$ \left[ \begin{array}{c} x \\ y \end{array}\right] =
\left[ \begin{array}{c} 
e^{3i\theta} + e^{i\theta} + e^{-i\theta} + e^{-3i\theta} \\
e^{3i\theta} - e^{i\theta} + e^{-i\theta} - e^{-3i\theta}\end{array} \right]
= \left[ \begin{array}{rrrr} 1 & 1 & 1 & 1 \\
1 & -1 & 1 & -1 \end{array} \right]
\left[ \begin{array}{c} e^{3i\theta} \\ e^{i\theta} \\
e^{-i\theta} \\ e^{-3i\theta} \end{array} \right]  $$ Or we could even try a different approach using the triple-angle formulas of trigonometry: \begin{eqnarray*}
\cos 3\theta &=& 4 \cos^3 \theta - 3 \cos \theta \\
\sin 3\theta &=& 3 \sin \theta - 4 \sin^3 \theta \\
\tan 3\theta &=& \frac{3 \tan \theta - \tan^3 \theta}{1 - 3 \tan^2 \theta}
\end{eqnarray*} Now these are reading as a degree-map from the circle to another algebraic variety (or scheme).  In that case, why does the angle $3\theta$ appear in a 4-fold symmetric curve? These singularities could be studied by Netwton's method (e.g. Puiseux series or resolution of singularities ).  Unfortunatley I could never get the jargon straight about the exceptional divisor and so forth. E.g. the strategy here to rationalize the lemniscate seems to amount to computing a blow-up of the curve.  At this point I am going to consult an algebraic geometry textbook. Example What is the "" derivative "" or tangent space at the origin $(0,0)$ ?","['analytic-geometry', 'algebraic-geometry', 'intersection-theory', 'coherent-sheaves']"
3036992,Integral of $\int\frac{\sin x}{\sin x+\cos x}dx$,The questions defines $$I=\int\frac{\sin x}{\sin x +\cos x}dx\;\;J=\int\frac{\cos x}{\sin x +\cos x}dx$$ It asked me to find $I+J$ and $J-I$ which I have done and I will show below but now I need to find the integral shown below and I'm unsure on what to do. $$\int\frac{\sin x}{\sin x+\cos x}dx$$ I have found that: $$I+J = x+c$$ $$J-I=\ln{|\cos x +\sin x|} +c$$ But now i'm unsure on how to find just $I$,"['integration', 'trigonometric-integrals']"
3037045,If T is a Consistent estimator of $\theta$ then $T^3$ is consistent estimator of $\theta^3$,If T is a Consistent estimator of $\theta$ then $T^3$ is a consistent estimator of $\theta^3$ . Can anyone tell me in a single line what's the logic behind this? I am familiar with invariance property of MLE where $T$ is MLE of $\theta$ and $T^2$ will be MLE of $\theta ^2$ and we know that all MLE's are consistent. If there is some other logic please do tell me I might ve skipped that portion. I want to study that.,"['statistical-inference', 'statistics']"
3037052,Determining probability of a rainy day,"I have the following problem: If today is a sunny day, a probability that it will rain tomorrow is $0.2$ . If today is a rainy day, a probability that it will be sunny tomorrow is $0.4$ . I need to find the probability that if it's rainy on the third of May, it will also rain on the third of June. My initial idea was to write a program that will create the binary tree with all possible combinations and then I just traverse through all of them and sum the probabilities accordingly, but unfortunately, I have to do this by hand, so any help is very welcome.","['probability-theory', 'probability']"
3037055,Embedding a compact manifold in $\mathbb{R}^N$,"I have an attempt at solving the following problem. This is not so much a question asking for a solution in general, but more on how to complete my own. Let $M^n$ be a compact smooth manifold. Show that there exists an embedding of $M$ into $\mathbb{R}^N$ for some $N$ . (Recall that an embedding of smooth manifolds is a topological embedding such that each differential is injective.) My attempt: Since $M$ is compact, we can cover it by finitely many coordinate neighbourhoods $U_i$ for $i=1,\dotsc,k$ , where $\varphi_i : U_i \to \mathbb{R}^n$ are the corresponding charts. Choose a subordinate (smooth) partition of unity $\psi_i : M \to \mathbb{R}$ . Then the functions $$ f_i := \psi_i \cdot \varphi_i$$ are smooth, where we interpret $\varphi$ as being zero outside of the support of $\psi$ . Now define $$ F : M \to \mathbb{R}^{n\cdot k + k} : x\mapsto (f_1(x),\dotsc,f_k(x),\psi_1(x),\dotsc,\psi_k(x)).$$ My hope was that $F$ is an embedding of smooth manifolds. For this, we verify: Injectivity . This is why the $\psi$ 's were stuck at the end of the map. If all the $\psi(x)$ 's are the same, then all the $\varphi(x)$ 's are the same, but these are local diffeomorphisms, in particular bijections. Smoothness . Trivial. Topological embedding . Immediate since $M$ is compact and $F$ is injective and continuous. Injective differentials . This is my issue. Are all the differentials injective for $F$ as defined above? Or would we need more assumptions on the covering or of the partition of unity for this to work (or for this to work more easily)?","['compact-manifolds', 'smooth-manifolds', 'differential-geometry']"
3037056,Evaluate $\lim_{x \to 0^+ } x^{x^{x}} - x^x$,Evaluate $$\lim_{x \to 0^+ } x^{x^{x}} - x^x$$ This is a solved example in my text book but i do not think that the solution is quite correct. They have essentially used the fact $\lim_{x\to0^+}x^x$ is 1 and used that to write the term to be evaluated as $$0^1 - 1$$ which gives an answer of -1 The graph indeed gives the limit at $0^+$ as -1. BUT We could have used $\lim_{x\to0^+}x^x$ to evaluate $\lim_{x \to 0^+ } x^{x^{x}} - x^x$ as $$1^0 - 1$$ which does not give the correct answer. Is the book's method correct?,['limits']
3037110,Do random experiments actually exist?,"I am studying probability and in most of the books that i have read they mention that for an experiment to be random-- (1)there should be more than 1 possible outcome--(2)even when the experiment is repeated under similar conditions the outcomes must not be predictable.The second point really confuses me...if we take a coin toss for example and we repeat the experiment under totally similar conditions   of gravity,air resistance,apply the same force,etc won't we be able to predict the exact outcome every time and as long as the conditions remain same my intuition tells me that i will get the same outcome every time....so do random experiments exist?and if they do what can be considered as a random experiment?","['probability-theory', 'probability']"
3037134,"Conditional expectation, what is my mistake","From SOA sample #238: In a large population of patients, $.20$ have early stage cancer, $.10$ have advanced stage
  cancer, and the other $.70$ do not have cancer. Six patients from this population are
  randomly selected.
  Calculate the expected number of selected patients with advanced stage cancer, given that
  at least one of the selected patients has early stage cancer. What is wrong with my solution? $${1\cdot{5\choose 1}\cdot.1^1\cdot.9^4+2\cdot{5\choose 2}\cdot.1^2\cdot.9^3+3\cdot{5\choose 3}\cdot.1^3\cdot.9^2+4\cdot{5\choose 4}\cdot.1^4\cdot.9^1+5\cdot{5\choose 5}\cdot.1^5\cdot.9^0}\over{1-.8^6}$$ where the numerator is assuming there are only $5$ spots for a patient to have advanced stage cancer, since at least once has early stage cancer, and the denominator is the probability that at least one has early stage cancer. EDIT: It has been made clear to me from David Diaz's answer that at least part of my mistake was trying to apply methods that can only be used in the hyper-geometric distribution to the binomial distribution. That is, I was trying to say, let there be one person with early stage cancer and consider him independently, and consider the other five independently where they can be either early, advanced, or no cancer. That would work if the question was hyper-geometric, for example, if the question was ""If there are $N$ patients $.20$ have early stage cancer, $.10$ have advanced stage
cancer, and the other $.70$ do not have cancer. Six patients from this population are
randomly selected etc..."".  Then I would be able to do the following $${1\cdot {.2N \choose 1}{{.1N}\choose 1 }{{.2N-1+.7N}\choose 4 } + 2\cdot {.2N \choose 1}{{.1N}\choose 2 }{{.2N-1+.7N}\choose 3 }+ 3\cdot {.2N \choose 1}{{.1N}\choose 3 }{{.2N-1+.7N}\choose 2 }+ 4\cdot {.2N \choose 1}{{.1N}\choose 4 }{{.2N-1+.7N}\choose 1 }+ 5\cdot {.2N \choose 1}{{.1N}\choose 5 }{{.2N-1+.7N}\choose 0 }\over {.2N \choose 1}{{.2N-1+.8N}\choose 5}}$$ However, the binomial distribution is fundamentally different, and every trial that selects an early stage patient must be accounted for.","['statistics', 'probability']"
3037145,"Showing $f(x) = \frac{1}{\sqrt x}$ is Lebesgue integrable on $(0,1]$","While reviewing for an exam recently, I came across this question which gave me pause. Explain why $f(x) = \frac{1}{\sqrt x}$ is Lebesgue integrable over $(0,1]$ . It is clear that $f$ is a decreasing, non-negative function. So, it is called Lebesgue integrable over $(0,1]$ if: $\int_{(0,1]} f \text{dm} < \infty$ And: $\int_{(0,1]} f \text{dm} = \sup\{ \int_{(0,1]} s \text{dm}: s \text{ simple}, s(x) \leq f(x) \forall x \in (0,1]\}$ The problem I’m having is that a lot of the typical useful theorems (MCT, DCT) are about non-decreasing functions. Broadly speaking, I understand that the “problem” with this function is that $f(0)$ is undefined and as $x \to 0$ , $f(x)$ gets very big. I guess that the reason this function is integrable over $(0,1]$ is that the most “problematic” point ( $x = 0$ ) is removed. How do I go about showing this more rigorously? Which (common) theorems should I use?","['integration', 'lebesgue-integral', 'analysis', 'real-analysis']"
3037176,Function that halves the angle of a complex point,How would a function mapping a complex point $z=re^{i\theta}$ to $re^{i\frac{\theta}{2}}$ be correctly written?,['functions']
3037193,"If two functions are equal almost everywhere, the first is continuous a.e., is the second?","If $f = g$ a.e. in $E \in \mathfrak{M}$ (the Lebesgue measurable sets)
  and $f$ is continuous a.e. in $E$ , is $g$ continuous a.e. in $E$ ? I think this is true. My “proof”: Let us denote $D_1 = \{ x \in E: f(x) \text{ discontinuous}\}$ , $m(D_1) = 0$ and $D_2 = \{ x \in E: f(x) \neq g(x)\}$ , $m(D_2) = 0$ . Define $D_3 = \{ x \in E: g(x) \text{ discontinuous}\}$ . If $f$ is identically $g$ , then it is clear that the result follows as $D_3 = D_1$ . Otherwise, we have $D_3 \subseteq D_1 \cup D_2$ , and so $m^*(D_3) \leq m^*(D_1 \cup D_2) \leq m^*(D_1) + m^*(D_2) = 0$ . So, $m(D_3) = 0$ and hence $g$ is continuous almost everywhere. Does this proof work? Thanks! Edit: for clarity, $m$ denotes the Lebesgue measure and $m^*$ the Lebesgue outer measure.","['measure-theory', 'analysis', 'real-analysis']"
3037224,Need help computing the double integral $\int_{0}^{\infty} \int_{0}^{\infty} \frac{f(x + y)}{x + y} \mathop{dy} \mathop{dx}$,"Need help computing the double integral $\int_{0}^{\infty} \int_{0}^{\infty} \frac{f(x + y)}{x + y} \mathop{dy} \mathop{dx}.$ I know that $\int_{0}^{\infty} f(u) \mathop{du}$ equals $1$ . The entire integral should come out to be $1$ . I am new to multivariable calculus and would appreciate some help. Making the substitution $u = x + y$ and $v = y$ , we get $\mathop{du} = \mathop{dx} + \mathop{dy}$ and $\mathop{dv} = \mathop{dy}$ . So, $$\int_{0}^{\infty} \int_{0}^{\infty} \frac{f(u)}{u} \mathop{dy} \mathop{dx}$$ I don't know what to change the $dy$ and $dx$ to since $du$ has both $dx$ and $dy$ in it. Also, for the Jacobian, I know that it will equal $1$ , but I don't know what I'm taking partial derivatives of in the determinant. Can someone please help me with this integral?","['integration', 'improper-integrals', 'definite-integrals', 'multivariable-calculus', 'substitution']"
3037247,Series convergence without sigma notation,"Consider the following series: $$\frac{1}{1} + \frac{10}{2} + \frac{100}{3} - \frac{37}{4} - \frac{37}{5} - \frac{37}{6} + \frac{1}{7} + \frac{10}{8} + \frac{100}{9} - \frac{37}{10} - \frac{37}{11} - \frac{37}{12} + \dots$$ This series seems to converge but I am not able to prove it. It seems impossible to write thing whole thing into summation notation. I tried to group by modulo 6, but then for example sum of $\frac{1}{6n+1}$ diverges already so that doesn't seem right. Also tried to change the 37s into powers of 10 to maybe use the comparison test but also failed.
Any suggestions?",['sequences-and-series']
3037254,How to prove that $ | \Phi_u \times \Phi_v | = \sqrt{ \det g} $?,Let be $ \Phi $ a parametrization of a surface $ \in \mathbb{R}^3 $ and $g$ the metric tensor. As the title says...how to show that How to prove that $ | \Phi_u \times \Phi_v | = \sqrt{\det g} $ ? I started like this: $$| \Phi_u \times \Phi_v | = \left| \begin{pmatrix} \frac{\delta \phi_1}{ \delta u} \\ \frac{\delta \phi_2}{ \delta u} \\ \frac{\delta \phi_2}{ \delta u} \end{pmatrix} \times \begin{pmatrix} \frac{\delta \phi_1}{ \delta v} \\ \frac{\delta \phi_2}{ \delta v} \\ \frac{\delta \phi_2}{ \delta v} \end{pmatrix} \right| = \left| \begin{pmatrix} \frac{\delta \phi_2}{ \delta u} \frac{\delta \phi_3}{ \delta v}- \frac{\delta \phi_3}{ \delta u} \frac{\delta \phi_2}{ \delta v}\\ \frac{\delta \phi_1}{ \delta u} \frac{\delta \phi_3}{ \delta v}- \frac{\delta \phi_3}{ \delta u} \frac{\delta \phi_1}{ \delta v}\\ \frac{\delta \phi_1}{ \delta u} \frac{\delta \phi_2}{ \delta v}- \frac{\delta \phi_2}{ \delta u} \frac{\delta \phi_1}{ \delta v}\end{pmatrix} \right|$$ I am not sure how to proceed..If I continue I am coming to a dead end..hope you can help me out a bit :) !,"['integration', 'multivariable-calculus', 'parametrization']"
3037268,Proving that functions send ultrafilter basis to ultrafilter basis,"I'm currently revisiting a proof of Tychonoff's theorem via ultrafilters. The definitions we were working with are as follows, $\mathcal{B}$ is a basis for a filter $\mathcal{F}$ on a set $X$ if $\mathcal{F} = \{A : B \subset A, \text{for some $B \in \mathcal{B}$}\}$ . an ultrafilter on a set $X$ is a filter $\mathcal{F}$ that is a maximal element of the set of filters on $X$ , ordered by inclusion. As an intermediate step, a lemma was left for the reader to prove: Lemma . Let $f : X \to Y$ be a function. Then, if $\mathcal{B}$ is a basis for a filter on $X$ , then $f(\mathcal{B})$ is a basis for a filter on $Y$ if $\mathcal{B}$ is a basis for a filter on $Y$ , then $f^{-1}(\mathcal{B})$ is a basis for a filter on $X$ , provided that $f^{-1}(B) \neq \emptyset$ for all $B \in \mathcal{B}$ . if $\mathcal{B}$ is a basis for an ultrafilter on $X$ , then $f(\mathcal{B})$ is a basis for an ultrafilter on $Y$ I have managed to prove the first two stamentents, but I am struggling with the last one. Certainly $f(\mathcal{B})$ is a basis for a filter on $Y$ , but why is this an ultrafilter? I've attempted to assume the contrary and take a finer filter than the one generated by $f(\mathcal{B})$ , in order to take preimages and contradict that $\mathcal{B}$ generates an ultrafilter, but I haven't been able to complete the proof.","['filters', 'general-topology']"
3037291,Snub cube's angles,"I am trying to build a snub cube . I have made $6$ squares and $32$ equilateral triangles (out of perler beads if you're curious).
I am trying to figure out the angles at which I adjoin the squares to the triangles, and the triangles to other triangles. I have found a few formulas, but I think I am a bit overwhelmed by the vocabulary used and do not understand what the listed variables are. H. Rajpoot says ""There is a general expression of the solid angle subtended by the snub cube at any of its $24$ vertices is given by the general expression \begin{align}\Omega&=2\sin^{-1}\left(\frac{(1-\sqrt{1-K^2})-\sqrt{2K^2-1}}{K^2\sqrt{2}}\right)+8\sin^{-1}\left(\frac{(1-\sqrt{1-K^2})-\sqrt{4K^2-1}}{2K^2\sqrt{3}}\right)\\&\approx 3.589629551 \space sr,\end{align} where $K\approx 0.928191378""$ . and Felix Marin says that the formula to find the angles is $$
\cos\left(\vphantom{\Large A}\angle{\rm ABC}\right)
=
{\left(\vec{A} - \vec{B}\right)\cdot\left(\vec{C} - \vec{B}\right)
 \over
 \left\vert\vec{A} - \vec{B}\right\vert\;\left\vert\vec{C} - \vec{B}\right\vert}
$$ where $A$ , $B$ , and $C$ are are vectors $A:[x_1,y_1,z_1]$ , $B:[x_2,y_2,z_2]$ , and $C:[x_3,y_3,z_3]$ . I suppose, I am completely overwhelmed. 
I have a sight feeling that finding the 'subtended angle' is not the same as the angle I am trying to find. Is that true?
What is $s$ ? $r$ ? 
Why are $A$ , $B$ , & $C$ vectors and how do I know which vectors to use? I saw online, here that the the coordinates for the vertices of a snub cube are all the even permutations of $(±1, ±1/t, ±t)$ with an even number of plus signs, along with all the odd permutations with an odd number of plus signs, where $t ≈ 1.83929$ is the tribonacci constant. Are these the values I am supposed to use to find the vectors to use the second equation?
Is there an easier way to do this? I fell like I have way over-complicated this. edit: okay, I found this website that says the square-triangle angle is $142$ degrees, $59$ minutes and the triangle-triangle angle is $153$ degrees, $14$ minutes. Would still be stoked to know how on earth to figure this out on my own. thanks!","['euclidean-geometry', 'vectors', 'geometry', 'solid-geometry', 'trigonometry']"
3037304,Including random variables in differential equations,"I am having trouble finding information regarding the inclusion of random variables in differential equations. For example I have a simple model describing the growth of some population $X$ over time: $$
\dot{X} = X (r - \alpha X)
$$ In this deterministic form we can easily solve to get the equilibrial and explicit solutions with regards to time My question comes when I want to express random variations in parameter $\alpha$ , allowing it be represented as a random variable. This means that $X$ also becomes a random variable. Can we derive equations to describe the change in this distribution and its moments over time? Note that here i am not taking about a time varying stochastic process (like brownian walks) but rather static variation in the parameters. Additionally can we include these moments in the differential equation above so that they depend on the moments of the distribution? $$
\dot{X} = X (r - \alpha X - E[X])
$$","['calculus', 'ordinary-differential-equations', 'mathematical-modeling', 'probability']"
3037317,Show that no asymmetric graph $G$ exists with $1 < \big|V(G)\big| \leq 5.$,"Show that no asymmetric graph $G$ exists with $$1 < \big|V(G)\big| \leq 5\,.$$ I tried listing all the possibilities for $\big|V(G)\big| \leq 5$ to prove this statement. I did all for $2$ and $3$ , and then it’s getting complicated. Is there any elegant, simple solution to show all $G$ of five vertices or less must have some automorphism other than identity mapping?","['automorphism-group', 'graph-theory', 'algebraic-graph-theory', 'combinatorics', 'discrete-mathematics']"
3037351,Use the spherical coordinates to compute the integral $\int\limits_{B} z^2 dx dy dz$ where B is defined by $1\leq x^2 + y^2 + z^2 \leq 4$,", however the answer I got to is different than the answer sheet. The answer sheet says that it should be $\frac{62}{15}$ Am I making some mistake or is the answer sheet incorrect?","['multivariable-calculus', 'calculus', 'spherical-coordinates', 'change-of-variable', 'substitution']"
3037356,Half of Vandermonde's Identity,"We know Vandermonde's Identity, which states $\sum_{k=0}^{r}{m \choose k}{n \choose r-k}={m+n \choose r}$ . Does anyone know what happens if we walk bigger steps with k? Say we skip all the odd ks, is something like $\sum_{k=0}^{r/2}{m \choose 2k}{n \choose r-2k}=\frac{1}{2} {m+n \choose r}$ or at least $\sum_{k=0}^{r/2}{m \choose 2k}{n \choose r-2k}=\Theta \left( \frac{1}{2} {m+n \choose r}\right)$ true? Maybe someone here has even some general insight on other step widths? Thank you!","['binomial-coefficients', 'combinatorics', 'analysis']"
3037371,Find the highest and lowest possible value of function $f(x)={3x^2+6x+6\over x^2+4x+5}$,"Find the highest and lowest value of the next function without the use of derivatives $$f(x)={3x^2+6x+6\over x^2+4x+5}$$ Okay so my teacher gave me this problem, and told me to strictly solve it without using derivatives . Obviously with using derivatives it's pretty easy, just find where $f'(x_0)=0$ , and check it's second derivative in $x_0$ to see if it's a maximum or minimum. So I was thinking about this: Since this function is written as a fraction, the lowest possible value should be when the numerator value is the lowest, while the denominator is the highest possible value. BUT , again to find the highest and lowest value of both I could use derivatives again, which again isn't the point. I can see that both the numerator and denominator of this fraction is always positive for every $x$ , so maybe there's a hint that I can't see there? Maybe I could use limits too, but I'm not sure how.",['functions']
3037396,Understanding Lie algebra of matrix Lie group,"In my lecture, we gave a very sloppy (physics people ...) proof of the fact that the Lie algebra $\mathfrak{g}$ of a matrix Lie group $G$ is a subspace of $\text{Mat}_n(\mathbb{F})$ .
I am not satisfied and I would love to understand this fact in the language of differential geometry that I am learning right now. What I know : The Lie algebra $\mathfrak{g}$ of a matrix Lie group $G$ is the set of all left-invariant vector fields on $G$ . This set is isomorphic to the tangent space at the identity of the Lie group $G$ . The elements $v$ of the tangent space at the identity are according to my definition of the tangent space real-valued functions $$v:F(G)\to \mathbb{R}$$ where $F(M)$ is the set of all smooth-real valued functions on the manifold $G$ . The tangent vectors are $\mathbb{R}$ linear and fulfil a Leibniz rule and the tangent space at $p$ , namely $T_pG$ is simply the set of all these tangent vectors. What I want to know Given the above definitions, how can I formally understand that the members of the tangent space at the identity of a matrix Lie group are matrices?","['tangent-spaces', 'lie-algebras', 'lie-groups', 'differential-geometry']"
3037450,Why are differential equations satisfied/defined only on open domains?,"I am going through FEM lectures by University of Michigan, and in one of the lectures, the professor writes an ODE: $$\frac{d^2u}{dx^2}+f(x)= 0, \ \ \ x \in (0,L)$$ Why are these differential equations only satisfied in open intervals, not in closed domains, when boundary conditions are specified?
For example, one of the set of boundary conditions could be $$u(0) = 0 \ \ and \ \ \frac{du}{dx}(x=L) = 0$$","['finite-element-method', 'ordinary-differential-equations']"
3037465,Evaluating a function with a negative number,"Given the function $h(x)=3 x^2 + 5$ , evaluate $h(-4)$ . My friend's tutor says its $h(-4)=149$ but isn't it $h(-4)=53$",['functions']
3037476,Separating Hyperplanes and Hahn-Banach,"Let $f:X \rightarrow \mathbb{R}$ be convex, $x \in X$ . Assume further that $f$ is continuous and finite in $x.$ Then it says that by Hahn-Banach there is $x^* \in X^*$ with \begin{equation}
\langle y - x, x^* \rangle_{X \times X^*} + f(x) \leq f(y) \ \ \ \forall y \in X.
\end{equation} I'm trying to understand why this is true. As far as I know, by Hahn-Banach we can separate the convex set given by the points above the graph of $f$ from the point $(x,f(x))$ by some hyperplane. More generally, for $A,B \subset X$ , $A\cap B= \emptyset$ convex sets with $A$ open there is $x^* \in X^*$ and $\gamma \in \mathbb{R}$ such that \begin{equation}
\langle a,x^* \rangle _{X \times X^*} \leq \gamma \leq \langle b,x^* \rangle _{X \times X^*} \ \ \ \forall a\in A,\ b \in B. \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  (1)  
\end{equation} Intuitively, the above estimate makes sense. The desired hyperplane is an affine function below the graph of $f$ which intersects with the graph in the point $(x,f(x))$ . But I would like to understand how this follows from the formal statement $(1)$ in the Hahn-Banach Theorem. Thank you for any help.","['convex-analysis', 'functional-analysis']"
3037511,joint PDF of max and min of $n$ iid standard uniform random variables,"Let $U_1, ... U_n$ be iid standard uniform variables.
Let $X = max(U_i)$ and $Y = min(U_i)$ .
The goal is to compute the joint PDF of $X, Y$ ! I have already computed the PDFs of $X$ and $Y$ separately.
But I am not sure if that is so useful because $X, Y$ are not independent (by definition $X \geq Y$ ) so it is not correct to just multiply the marginal PDFs of $X$ and $Y$ . I thought about starting from the CDF and taking advantage of the fact that all of the $U_i$ 's are independent. So something like: $P(X \leq x, Y \leq y) = P(U_1, ...U_n \leq x, \text{at least one $U_i$ is less than }y)$ and you can already see the problem here -- I don't know how to express the relation for $Y$ in terms of all the $U_i$ 's like that. when computing $Y$ 's CDF, I did $1 - P(U_1 \geq y, ... U_n \geq y)$ and was able to take advantage of the independence of all the $U_i$ 's there. But I'm not sure how to translate that to the joint PDF of $X, Y$ .","['probability-distributions', 'uniform-distribution', 'probability-theory', 'probability']"
3037532,Stuck on proof using Cauchy's integral formula,"I posted my attempted proof to this question here but I realized that I was wrong in taking the limit, and that the proof did not make sense. So I am still stuck on this problem let $f: \Omega \rightarrow \mathbb{C}$ be analytic and $z_0 \in \mathbb{C}$ . Define $$g(z) = \begin{cases} 
      \frac{f(z)-f(z_0)}{z- z_0} & z \not = z_0 \\
      f'(z_0) & z = z_0 
   \end{cases}$$ now pick $\varepsilon$ small enough so that $\overline{D(z_0, \varepsilon)} \subset \Omega$ Show that whenever $z \in D(z_0, \varepsilon)$ $$\frac{g(z) - g(z_0)}{z-z_0} = \frac{1}{2\pi i}\int_{\partial D(z_0, \varepsilon)}\frac{f(\zeta)}{(\zeta-z)(\zeta-z_0)^2}d\zeta$$ So is Cauchy's integral formula still the right way to go? I end up getting that $$\frac{g(z) - g(z_0)}{z-z_0} = \frac{f(z) - f(z_0)}{z-z_0} - \frac{1}{z-z_0}\int_{\partial D(z_0, \varepsilon)} \frac{f(\zeta)}{(\zeta-z_0)^2}d\zeta$$ and I am not sure how to proceed from here","['complex-analysis', 'contour-integration', 'derivatives', 'cauchy-integral-formula']"
3037533,Coordinate ring of a scheme in functorial algebraic geometry,"I will preface this by saying that I am new to algebraic geometry, but I am somewhat experienced with category theory. I'm just reading the introduction to Milne's notes ""Basic Theory of Affine Group Schemes"". He uses the functorial point of view here, so I am viewing an affine scheme over $K$ as a representable functor $X: K\mathsf{Alg} \to \mathsf{Sets}$ , and a scheme is likewise defined as a functor satisfying appropriate gluing properties. We can think of some general functor as a generalized scheme. In section I.3 he has a subsection titled ""The canonical coordinate ring of an affine group"" but I noticed that his construction seems to define a canonical ""coordinate ring"" for every sort of ""generalized scheme"", not just affine group schemes. Indeed, if $X: K\mathsf{Alg} \to \mathsf{Sets}$ is a functor then $\mathrm{Nat}(X, \mathbb{A}^1_K)$ is a $K$ -algebra (with operations defined pointwise), since the affine line over $K$ is the forgetful functor $\mathbb{A}^1_K: K\mathsf{Alg} \to \mathsf{Sets}$ . So we have a functor $\mathsf{Sets}^{K\mathsf{Alg}} \to K\mathsf{Alg}$ defined by $X \mapsto \mathrm{Nat}(X, \mathbb{A}^1_K)$ . Moreover, we have an obvious natural transformation $\alpha: X \to \mathrm{Spec_K}(\mathrm{Nat}(X, \mathbb{A}^1_K))$ ,
where $\mathrm{Spec}_K$ here is just the contravariant Yoneda embedding (since I am thinking of affine schemes as functors rather than ringed spaces). This natural transformation has components $\alpha_A: X(A) \to \mathrm{Hom}(\mathrm{Nat}(X, \mathbb{A}^1_K), A)$ given by $x \mapsto (f \mapsto f_A(x))$ . My question is: Is it reasonable to call $\mathrm{Nat}(X, \mathbb{A}^1_K), A)$ the coordinate ring for any ""generalize scheme"" given by a functor $X: K\mathsf{Alg} \to \mathsf{Sets}$ ? If not, what should we call this? Is the functor $\mathsf{Sets}^{K\mathsf{Alg}} \to K\mathsf{Alg}$ mapping $X$ to $\mathrm{Nat}(X, \mathbb{A}^1_K)$ adjoint (on the left or right) to $\mathrm{Spec}_K: K\mathsf{Alg}^{\mathrm{opp}} \to \mathsf{Sets}^{K\mathsf{Alg}}$ ? My guess is that it is the left adjoint to $\mathrm{Spec}_K$ . Is there a name and interpretation for this natural transformation $\alpha_A: X(A) \to \mathrm{Hom}(\mathrm{Nat}(X, \mathbb{A}^1_K), A)$ ? I can see that $X$ is an affine scheme over $K$ if and only if this is an isomorphism. But what if $X$ is not affine? How do we interpret this?","['algebraic-groups', 'category-theory', 'ring-theory', 'algebraic-geometry', 'schemes']"
3037566,Real world applications of Riemann surfaces of holomorphic functions [closed],Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question The maximal analytic continuation of a holomorphic function is an example of Riemann surfaces. What is it  used for? Please edit the question to limit it to a specific problem with enough detail to identify an adequate answer. I want to know the applications related to the real world  rather than pure mathematics.,"['riemann-surfaces', 'analytic-continuation', 'applications', 'analysis', 'complex-analysis']"
3037574,"Showing that any square matrix in $\mathbb{R}^{n \times n}$ has a ""square root""","Prove that there exists $\delta > 0$ so that for all square matrices $A\in \mathbb{R}^{n\times n}$ with $\|A-I\| < \delta$ (where $I$ denotes the identity matrix) there exists $B\in \mathbb{R}^{n\times n}$ so that $B^2=A$ . My attempt so far: $$A-I= \begin{bmatrix}
a_{11}-1 & a_{12} & ... & a_{1n} \\
a_{21} & a_{22}-1 & ... & a_{2n} \\
\vdots \\
a_{n1} & a_{n2} & ... & a_{nn}-1 \\
\end{bmatrix} $$ Taking $x=(1,0,...,0)\in \mathbb{R}^n$ , we have that $$\|A-I\|_{op}=\underset{\|x\|=1}{\sup}\|(A-I)x\|= \sqrt{(a_{11}-1)^2+...+a_{n1}^2}<\delta$$ Intuitively, this seems to suggest that for each basis vector of $\mathbb{R}^n$ means that $A-I$ can be made close to the zero matrix, and hence, close to being a diagonal matrix. But I am having trouble going from here. Anyone have any hints?","['matrices', 'linear-algebra', 'real-analysis']"
3037643,spaghetti hoops combinatorics variation,"You may have heard about the classic spaghetti hoops combinatorics problem, which has been stated like this: ""You have N pieces of rope in a bucket. You reach in and grab one end-piece, then reach in and grab another end-piece, and tie those two together. What is the expected value of the number of loops in the bucket?"" The solution is straight forward and involves defining two events: event that you close the loop, vs. event that you just extend the path you have without forming a loop. My question is what happens when we generalize this a bit so that the objects no longer need to have 2 endpoints? For simplicty lets say the objects all still have the same number $E$ of endpoints. But some interesting things happen: if $E$ is odd, the objects can no longer close up on themselves, e.g. for the $E$ =2 case a loop could form to connect a single object to itself, but now that cannot happen. When $E$ =even, now this can happen again. So specifically, let's define a ""tangle"" as the generalization of a loop, i.e. a ""tangle"" is a group of connected objects, so each object in the tangle is connected by at least 1 edge to at least one other object in that tangle. Let's call the ""size"" of the tangle the number of objects that are in the tangle. My question is: given $N$ objects, for number of endpoints per object $E\geq3$ , what is the expected number of ""tangles""? Bonus points if you can give the expected number of tangles of a given size $S$ ?","['puzzle', 'recursion', 'combinatorics', 'recreational-mathematics', 'probability']"
3037664,"Proving that $f(z;\sigma)=\sum_{k\in\Bbb Z}\frac{1}{\sqrt{2\pi}\, \sigma}{\rm e}^{-\frac{(z-k)^2}{2\sigma^2}}$ converges to $1$ as $\sigma\to\infty$","I have an application where I get following function as a result: $$f(z;\sigma) = \sum_{k \in \mathbb{Z}} \frac{1}{\sqrt{2 \pi} \, \sigma} \textrm{e}^{-\frac{(z - k)^2}{2 {\sigma}^{2}}}$$ It appears that $$\lim_{\sigma \rightarrow \infty} f(z;\sigma) = 1$$ but I can't currently find a way to prove this. Is this property of the sum true, and if it is, why? Any references would be greatly appreciated.","['exponential-function', 'sequences-and-series']"
3037671,Weak star compactness and the sphere,"If $X$ is a dual space, the ball $B_X$ is weak star compact. Is the sphere generally weak star compact? I think the answer is no because in $\ell^2$ , the sequence $(e_n)_n$ converges weakly to zero, and so no subsequence converges weakly to an element with norm one. The context for the question is the following statement from Wikipedia: Let $X$ be a $C^*$ algebra and with unit. The set of states (continuous linear functionals $f$ on $X$ such that $||f||=1$ and $f(x)\geq 0$ when $x\geq 0$ ) is a weakly compact set.","['operator-theory', 'functional-analysis']"
3037699,Perpendicular bisector in a different metric,"Consider two points $a, b \in \mathbb{R}^2$ . Then from elementary geometry, the set of points that are equidistant from both $a$ and $b$ is precisely the perpendicular bisector of the line segment $ab$ . Now suppose we put some other metric on $\mathbb{R}^2$ that generates the standard topology. Then what can we say about the set of points that are equidistant from $a$ and $b$ ? Intuitively, it seems to me that this set should be ""one-dimensional"" and cannot contain an open set. Is this true?","['general-topology', 'geometry']"
3037709,How much can we rearrange a series?,"There's a well-known result that if $\sum a_n$ is conditionally convergent, then for any real $c$ there exists a permutation $\pi:\mathbb{N} \to \mathbb{N}$ such that $\sum a_{\pi(n)} = c$ . A consequence of this is that you cannot get away with rearranging infinite sums, in general. However , if the rearrangement is sufficiently tame, then we can get away this. For instance, if we rearrange only finitely many terms, we can get away with this without the value of the sum changing. Indeed, there is a stronger result that if we have a permutation $\pi$ and a uniform constant $M$ such that $|\pi(n) - n| < M$ for all positive integers $n$ , then $\sum a_n = \sum a_{\pi(n)}$ . This is not too hard to prove. My question is, essentially, can we get a ""maximally strong"" result of this sort? Specifically I ask the following question: Characterize permutations $\pi:\mathbb{N} \to \mathbb{N}$ with the following property: For any convergent infinite sum $\sum a_n$ , we have that $\sum a_n = \sum a_{\pi(n)}$ . Notice the order of quantifiers here. It's conceivable that the result I described in the second paragraph is the best we can do, in the sense that if $\sup_n |\pi(n) - n| = \infty$ , there exists a conditionally convergent $a_n$ with $\sum a_n \neq \sum a_{\pi(n)}$ . Perhaps we can do better, though. One guess is slightly changing the hypotheses as follows: there exists a uniform $M$ such that $|\pi(n) - n| < M$ for all $n \in \mathbb{N} \setminus S$ , where $S$ is a subset of $\mathbb{N}$ with $0$ asymptotic density .","['sequences-and-series', 'real-analysis']"
3037721,general solution of $\frac{dy}{dx}+P(x)y=0$,"Consider the homogeneous first-order linear differential equation(DE), $$\frac{dy}{dx}+P(x)y=0,$$ where $P(x)$ is continuous on an interval $I=(a,b)$ . I'm trying to convince myself that its general solution on $I$ is $ce^{-\int P(x)dx}.$ My arguments are given below.  I'll appreciate it if someone can confirm or refute it, or provide a simpler/better proof. First of all, let's divide the solutions of the DE into two types.  Type 1: $y(x)\ne 0$ for any $x\in I.$ And we call the rest of the solutions Type 2. For Type 1 solutions , $y\ne 0$ , so the DE is equivalent to: $$\frac{1}{y}\frac{dy}{dx}=-P(x),$$ which can be solved by $$\int \frac{1}{y}dy=-\int P(x)dx.$$ So the general Type 1 solution is $ce^{-\int P(x)dx}, c\ne 0.$ (Note that indeed $y\ne 0$ for any $x\in I.$ ) Regarding Type 2 solutions , first note that the constant solution $y=0$ is clearly a Type 2 solution.  Furthermore, we show it's the only Type 2 solution, by an argument similar to the one in why can't a non-constant solution of an autonomous DE intersect an equilibrium solution? : Suppose $y(x)$ is a Type 2 solution with $y(\alpha)=0$ for some $\alpha\in I.$ Then by the existence and uniqueness theorem, $y(x)=0$ on $(\alpha-h, \alpha+h)\subset I$ for some $h>0.$ Let $\beta=\sup\{t: t\le b, y(t)=0, x\in (\alpha-h, t)\}$ .  Now suppose $\beta<b$ .  Then $y(\beta) \ne 0$ , by definition and the existence and uniqueness theorem.  But this means $y$ is discontinuous at $\beta$ , contradicting the fact that $y$ has to be continuous. So we must have $\beta=b,$ and $y(x)=0$ for $x\in (\alpha-h, b)$ .  By the same token, we can show $y(x)=0$ for $x\in (a, \alpha+h)$ too. Is this argument correct?  Is there a simpler proof?  Thanks a lot!","['proof-verification', 'ordinary-differential-equations']"
3037724,Find the number of permutations of the word 'ESTATE' if all vowels must be adjacent,"First separating the word estate into vowels and non vowels gives $EAE$ for the vowels and $S,T,T$ for the non-vowels. I interpreted this as a group of 4 where there's two T's resulting in $\frac{4!}{2!}$ but since the vowels can also be permuted and there's two E's this results in
#permutations = $\frac{4!}{2!}\cdot\frac{3!}{2!}=3!3!=36$ . The correct answer is $180$ so I'm off by a factor of 5 although I have no clue where it comes from. If someone could point out the flaw in my reasoning that would be great.","['permutations', 'combinatorics']"
3037741,Find all positive integer $m$ such $2^{m}+1\mid5^m-1$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 5 years ago . Improve this question Find all positive integer $m$ such $$2^{m}+1\mid5^m-1\,.$$ It seem there no solution, I think it might be necessary to use quadratic reciprocity  knowledge to solve this problem. Let $M=2^m+1$ , so we have $$5^{m}\equiv 1\pmod{M}. $$ If $m$ is odd, then we have $$\left(5^{\frac{m+1}{2}}\right)^2=5\pmod {M}\Longrightarrow \left(\dfrac{5}{M}\right)=1,$$ so $$\left(\dfrac{M}{5}\right)(-1)^{\frac{(5-1)(M-1)}{4}}=\left(\dfrac{M}{5}\right)(-1)^M=-\left(\dfrac{M}{5}\right).$$","['modular-arithmetic', 'divisibility', 'number-theory', 'elementary-number-theory', 'diophantine-equations']"
3037764,For what $n$ is $W_n$ finite?,"Suppose, $W_n$ is the set of all words formed by letters ' $a$ ' and ' $b$ ', that do not contain $n$ same consecutive nonempty subwords (that means that for any nonempty word $u$ , the word $u^n$ is not a subword of words from $W_n$ ) For example "" $bababab$ "" is not in $W_3$ , as it contains three consecutive "" $ba$ "" subwords, but it obviously is in $W_4$ .
For what $n$ is $W_n$ finite? It is easy to see, that $W_n \subset W_{n+1}$ and thus the sequence of cardinals $\{|W_n|\}_{n=1}^{\infty}$ is monotonously non-decreasing. 
Thus either $W_n$ is finite for all $n$ , or it is infinite for all $n$ , or there exists $n_0$ , such that $W_n$ is finite for all $n < n_0$ and infinite for all $n \geq n_0$ . One can also see, that $W_2 = \{a, b, ab, ba, aba, bab\}$ is finite. One can prove that just by looking at all 16 words of length 4 and seeing that none of them lies in $W_2$ . However, $W_{665}$ is already infinite. Suppose $G$ is an infinite $2$ -generated group of exponent $665$ (such group exists according to Adyan-Novikov theorem). Then any element of it can be expressed as a multiple product of those two generators (which can be written as a word formed by letters ' $a$ ' and ' $b$ ' (that denote the first and the second generator respectively). Due to the group having exponent $665$ , any such word can be ""reduced"" to a word from $W_{665}$ . One can see that two elements that can be written as the same word are equal. And in $G$ there is infinitely many pairwise not equal elements. Thus $W_{665}$ is infinite by pigeonhole principle. So we can say that there exists such $n_0$ , that $W_n$ is finite for all $n < n_0$ and infinite for all $n \geq n_0$ . And that aforementioned $n_0$ satisfies the inequality $2 < n_0 \leq 665$ . However I failed to determine anything else about that number. Any help will be appreciated.","['combinatorics-on-words', 'combinatorics', 'group-theory', 'formal-languages', 'dynamical-systems']"
3037770,"Right cone, you are at A and need to complete a revolution before reaching the bottom B. What is shortest distance AB?",You are on a mountain that is a right cone shape. You are trying to get to B and you are somewhere up the mountain A such that you lie on the line OB. The line AB must do one full revolution of the mountain and is the shortest distance. What is the general formula for finding any AB distance? How much of that distance is uphill/downhill? When uncurled the cone looks like the shape below with A and B demonstrated. With A being a distance $x$ along the line OB. I appreciate that the straight line is the shortest distance but how do I work that distance out? I am not sure where to begin with uphill/downhill part.,"['trigonometry', 'geometry']"
3037780,An unpleasant measure theory/functional analysis problem,"I am currently taking a functional analysis course, and at the moment every student on the course is stumped by a specific question. We're looking at the bounded linear map $$
\varphi_n:L^1\left([0,1]\right)\to \mathbb{R},\quad \varphi_n(f) = \int^1_0 f(x)(n\sin(n^2 x)) dx,\quad\|\varphi_n\|=n.
$$ We are attempting to prove the existence of an $f\in L^1$ with respect to the Lebesgue measure, so that $\lim_{n\rightarrow \infty} |\varphi_n(f)|\rightarrow \infty$ , and from an earlier assignment we know that this function cannot be continuously differentiable. However, nobody has been able to make even the slightest progress. I, and all of us would appreciate a hint or a sketch of proof!","['measure-theory', 'functional-analysis']"
3037799,Integration over convex polytope,"I have a small problem (as stated below). Moreover, I am new to this site (asking questions that is) and not a mathematician by trade. I tried to be as precise as possible in asking the question. If anything is unclear, please let me know and I will try to make my point more clear. The problem is the following: I have a convex polytope $A$ which is split by a half space into the two convex polytopes $A_L$ and $A_R$ . Moreover, I have a continuous function $p(x)$ and a probability density function $f(X)$ , where $X$ is an $i.i.d.$ random vector (with mutually independent elements) and I know that $$
\frac{\int_Ap(x)f(x)dx}{\int_Af(x)dx}=\frac{\int_{A_L}p(x)f(x)dx}{\int_{A_L}f(x)dx}=\frac{\int_{A_R}p(x)f(x)dx}{\int_{A_R}f(x)dx}
$$ holds for all possible splits of $A$ into $A_L$ and $A_R$ . I want to show that if the above equality holds for all possible splits of $A$ into $A_L$ and $A_R$ , then $p(x)$ must be constant on $A$ . I can show the result in the case of a scalar r.v. $X$ . However, I struggle to show the result for the general case where $X$ is an $i.i.d.$ random vector (with mutually independent elements). However, I also realize that the problem itself is independent of the fact that $f(x)$ is a pdf. Therefore, I was thinking that there might be a general mathematical result which can be used to show the claim. I very much appreciate any pointers towards a way to prove the above statement.","['integration', 'probability', 'real-analysis']"
3037846,Proof: A tangent space of the manifold of SPD matrices is the set of symmetric matrices,"The set of SPD matrices, $\mathbb{P}_n := \{X \in \mathbb{R}^{n \times n} | X=X^T, X \succ 0 \}   $ , forms a differentiable manifold. Claim: The tangent space at a point $A$ , $T_A\mathbb{P}_n$ , is the space of symmetric matrices. Info: I have seen this mentioned in a paper ( https://www.ncbi.nlm.nih.gov/pubmed/27845666 ), but (with my limited understanding of differential geometry) can't see how this occurs (although it may actualy be obvious to someone with a more rigorous maths education). I have also checked out the book they referenced when making this claim ( http://www.cmat.edu.uy/~lessa/tesis/Positive%20Definite%20Matrices.pdf ) in order to understand it. Sadly the section that I think they are referencing (Start of chapter 6) is far too formal for me. For completeness I will write out what is written there, and if it is the proof I am looking for then an explanation of some of the jumps in the steps would answer my question. The space $\mathbb{M}_n$ is a Hilbert space with inner product $ \langle A,B \rangle = tr A^*B $ and the associated norm $\vert \vert A \vert \vert _2 = (tr A^*A)^{\frac{1}{2}}$ The set of Hermitian matrices constitutes a real vector space $\mathbb{H}_n$ in $\mathbb{M}_n$ . The subset $\mathbb{P}_n$ consisting of strictly positive matrices is an open subset in $\mathbb{H}_n$ . Hence it is a differentiable manifold. The tangent space to $\mathbb{P}_n$ at any of its points A is the space $T_A\mathbb{P}_n = \{A\} \times \mathbb{H}_n$ If this is indeed the proof that I seek then these are the steps that I don't understand: I know that open subsets of manifolds are manifolds, and I am also presuming that ""stricly positive"" means positive definite. But how do I know that this manifold is differentiable, ie has differentiable transistion maps? This is my main confusion - $\mathbb{H}_n$ is the set of symmetric matrices, but why is the tangent space at any point defined like this? I understand the tangent space to be the collection of velocity vectors through that point. Why can I represent the gradients of all the geodesics that pass through a point like this? EDIT - UPDATE: Having been browsing the stack I also came across tangent space clarification where someone is asking why tangent spaces aren't just defined in the same way (ish) as they were in 4) - does anyone know where this definition/way of writing has come from?","['proof-explanation', 'proof-writing', 'manifolds', 'positive-definite', 'differential-geometry']"
3037882,Solving independent linear equations,"\begin{align}
&{-}2y+2z-1=0 \tag{4} \\[4px]
&{-}2x+4y-2z-2=0 \tag{5} \\[4px]
&\phantom{-2}x-y+3/2=0 \tag{6}
\end{align} Equation (6) is the sum of (4) and (5). There are only two independent equations.
  Putting $z=0$ in (5) and (6) and solving for x and y, we have \begin{align}
x&=-2 \\[4px]
y&=-1/2
\end{align} equation (6) is the sum of (4) and (5): OK, I see it There are only two independent equations: I didn't get; what does this sentence mean? Putting $z=0$ in (5) and (6): why putting z=0 in equation (5) and (6)? Please help",['linear-algebra']
3037892,Is it possibile to obtain the sum of the following series without using hypergeometric functions?,"I know that the following series: $$
\sum_{n=1}^{+\infty}\frac{ (n!)^2}{(2n)!}
$$ converges. If I plug it in Wolphram Alpha, I can see that its sum is $$
\frac{1}{27} \left(9 + 2 \sqrt{3} \pi\right).
$$ Is it possibile to obtain it without the use of the hypergeometric functions?","['factorial', 'sequences-and-series']"
3037918,Finding the cube roots of two complex numbers knowing the product of these cube roots,"Let $c_E$ , $c_F$ , $c_G$ and $c_H$ be known real numbers. Let $x_1$ , $w_1$ , $x_2$ and $w_2$ be unknown real numbers. When solving a cubic equation, one could be led to solve that kind of system: \begin{equation}
\begin{split}
(x_1+\text{i}w_1)+(x_2+\text{i}w_2)&=8(c_G+\text{i}c_H)\\
\sqrt[3]{x_1+\text{i}w_1}\sqrt[3]{x_2+\text{i}w_2}&=4(c_E+\text{i}c_F)\\
\end{split}
\end{equation} Raising the second equation to the cube will allow to find $x_1$ , $w_1$ , $x_2$ and $w_2$ by solving a quadratic equation. However it seems to also result in a loss of information about the relation between the arguments of $\sqrt[3]{x_1+\text{i}w_1}$ and $\sqrt[3]{x_2+\text{i}w_2}$ , which appear in the expressions of the cubic's roots. This problem has been underlined at Cubic formula gives the wrong result (triple checked) . 1) If $(c_E,c_F)=(0,0)$ then at least one of the complex numbers $(x_j+\text{i}w_j)$ is $0$ , for $j\in\{1,2\}$ . Then there is no constraint on the argument of the other one. Is it correct? 2) If $(c_E,c_F)\neq(0,0)$ then I guess there is a degree of freedom allowing to choose the argument of one complex number, and then the other is determined by the second equation, isn'it? A way to take this into account has been provided at Why not write the solutions of a cubic this way? , but what if one wants to have no cube root in the denominator? 2.1) ATTEMPT 1: I simply rewrote the second equation as: \begin{equation}
\sqrt[3]{x_2+\text{i}w_2}=\frac{4(c_E+\text{i}c_F)\left(\sqrt[3]{x_1+\text{i}w_1}\right)^2}{x_1+\text{i}w_1}
\end{equation} It seems to always work. However, is there a more elegant way to write the square of the cube root ? (I know that $\frac{4(c_E+\text{i}c_F)\sqrt[3]{(x_1+\text{i}w_1)^2}}{x_1+\text{i}w_1}$ will always work for real numbers by choosing the real cube root, but it won't always work for other choices or unreal numbers). 2.2) ATTEMPT 2: $\text{atan2}(x_1,w_1)$ , $\text{atan2}(x_2,w_2)$ and $\text{atan2}(c_E,c_F)$ are defined. Then, for each $j\in\{1,2\}$ and $k_j\in\{-1,0,1\}$ , I can write: \begin{equation}
\sqrt[3]{x_j+\text{i}w_j}=\sqrt[6]{x_j^2+w_j^2}\exp\left(\frac{\text{i}}{3}\text{atan2}(x_j,w_j)+\frac{2k_j\pi\text{i}}{3}\right)
\end{equation} For any real integer $k_{EF}$ , I can also write: \begin{equation}
c_E+\text{i}c_F=\sqrt{c_E^2+c_F^2}\exp\left(\text{i}\ \text{atan2}(c_E,c_F)+2k_{EF}\pi\text{i}\right)
\end{equation} I included these equations in the second equation of the initial system: \begin{equation}
\begin{split}
&\sqrt[6]{x_1^2+w_1^2}\sqrt[6]{x_2^2+w_2^2}\exp\left(\frac{\text{i}}{3}\text{atan2}(x_1,w_1)+\frac{\text{i}}{3}\text{atan2}(x_2,w_2)+\frac{2(k_1+k_2)\pi\text{i}}{3}\right)\\&=4\sqrt{c_E^2+c_F^2}\exp\left(\text{i}\ \text{atan2}(c_E,c_F)+2k_{EF}\pi\text{i}\right)
\end{split}
\end{equation} As far as I know, both norms in this last equation are necessarily equal, thus both arguments are equal as well. Then I can write the argument of $(x_2+\text{i}w_2)$ as: \begin{equation}
\begin{split}
&\frac{\text{i}}{3}\text{atan2}(x_2,w_2)+\frac{2k_2\pi\text{i}}{3}=\frac{\text{i}}{3}(3\ \text{atan2}(c_E,c_F)-\text{atan2}(x_1,w_1))+\frac{2(3k_{EF}-k_1)\pi\text{i}}{3}\\
&=\frac{\text{i}}{3}\text{atan2}(c_E(c_E^2-3c_F^2)x_1+c_F(3c_E^2-c_F^2)w_1,c_F(3c_E^2-c_F^2)x_1-c_E(c_E^2-3c_F^2)w_1)+\frac{2(3k_{EF}-k_1)\pi\text{i}}{3}\\
\end{split}
\end{equation} The value of $k_{EF}$ doesn't seem to change anything and can always be chosen as 0, right? The right-hand-side of the first line seems to always provide the good argument. However the right-hand-side of the second line, obtained by applying the formula for the sum of atan2, seems to have again resulted in a loss of information. What's wrong with it? 2.3) AESTHETICS Whathever the method is, it always seems to favour one complex number over the other. Is there a more ""neutral"" way to get an always working expression? Thanks in advance for your answers.","['cubics', 'polynomials', 'radicals', 'algebra-precalculus', 'complex-numbers']"
3037919,"Evaluating $ \int \frac1{5S_{8} - 9S_{10} + 7S_{12} - 2S_{14}} {\rm d}x$, where $S_n = \sin^n(x) + \cos^n(x)$","Prove that $$ \int \frac1{5S_{8} - 9S_{10} + 7S_{12} - 2S_{14}} {\rm d}x = 2x - \arctan \left( \frac{\tan2x}{2 + \tan^22x} \right) + C$$ where $S_n = \sin^n(x) + \cos^n(x)$ . Even differentiating the right doesn't end up with anything close to that monster integrand. $$ \frac{{\rm d}}{{\rm d}x} \left( 2x - \arctan \left( \frac{\tan2x}{2 + \tan^22x} \right) + C \right)
\\ = 2 + \frac{(\tan^22x+1)(2\tan^22x-4)}{\tan^42x+5\tan^22x+4}
\\ = \frac{ 4 } {\sin^42x+ 5\sin^22x\cos^22x + 4\cos^42x}
\\ = \frac1{\sin^8x + \sin^2x\cos^6x +\cos^2x\sin^6x + \cos^8x} $$","['integration', 'indefinite-integrals', 'trigonometry', 'trigonometric-integrals']"
3037922,Sum of freely chosen subset of $n$-tuple is divisible by $n$.,"I am struggling with the following task: Be $n \in N$ and $(a_1, a_2, \ldots, a_n) \in \mathbb{Z}^n$ . Prove that there is always $i, j \in \underline{n}$ , with $i≤j$ , so that $\sum_{k=i}^j a_k$ is divisible by n. My first idea was to use a distinction of cases. In the first case a multiple of n is an element of $\mathbb{Z}^n$ so you could select the sum to be just this element so it would obviously be divisible. In the second case, all elements of the tuple are the same, so that adding them all up will result in a multiple of $n$ once more, making it divisible. I am lost however on what to do in the third case, where the two above are not true. I would truly appreciate your input.","['elementary-number-theory', 'divisibility', 'discrete-mathematics']"
3037929,Establish bound for a probability using moment generating function,"I have the following question Let $X_{1}$ , $X_{2}$ , ..., $X_{n}$ be independent and identically
  distributed random variables with moment generating function $M_{X}(t)$ , for -h < t < h, h > 0, $S_{n}$ = $\sum_{i=1}^{n}X_i$ , and $\bar{X}$ = $S_{n}/n$ . Ultimately I need to establish the following: Use the fact that $M_{X}(0)=1$ and $dM_{X}(t)/dt$ $\vert_{t=0} =
> \mathbb{E}(X)$ to show that if $\mathbb{E}(X)<0$ then there is a $0>c<1$ with $P(S_{n}>a) \leq c^{n}$ . Establish a similar bound for $P(S_{n} \leq a)$ . I have previously shown that $P(S_{n}>a) \leq e^{-at}[M_{X}(t)]^{n}$ for $0<t<h$ , $a\geq0$ and $P(S_{n}\leq a) \leq e^{-at}[M_{X}(t)]^{n}$ for $ -h<t<0 $ As follows: $P(S_{n} > a) = \int_a^{\infty} dF(s) \leq \int_a^{\infty} e^{-at} e^{st} dF(s) \leq \int_{-\infty}^{\infty} e^{-at} e^{st} dF(s) = e^{-at} \mathbb{E} (e^{t \sum_{i=1}^{n}X_i}) = e^{-at}[M_{X}(t)]^{n} $ And similarly: $P(S_{n} \leq a) = \int_{-\infty}^{a} dF(s) \leq \int_{-\infty}^{a} e^{-at} e^{st} dF(s) \leq \int_{-\infty}^{\infty} e^{-at} e^{st} dF(s) = e^{-at} \mathbb{E} (e^{t \sum_{i=1}^{n}X_i}) = e^{-at}[M_{X}(t)]^{n} $ Now, I have really no clue on how to establish a bound at $c_{n}$ using the fact that $M_{X}(t) = 0 $ and $dM_{X}(t)/dt$ $\vert_{t=0} = \mathbb{E}(X)$ to show that if $\mathbb{E}(X)<0$ then there is a $0>c<1$ with $P(S_{n}>a) \leq c^{n}$ . I have no attempt to show but any ideas would be very welcome. Thanks in advance","['moment-generating-functions', 'statistics', 'random-variables']"
3038017,proof that $\frac{m^a}{2}+\frac{m}{2}-1$ is not a prime number,"I'm having trouble proving the following: If $m$ is an even number, $m\ge4$ and $a$ is an integer, $a\ge2$ then $$\frac{m^a}{2}+\frac{m}{2}-1$$ is not a prime number. Usually, in this type of exercises, someone has to show that the number can be written as a product of more than two expressions but whatever I've tried doesn't work...Thank you!","['number-theory', 'elementary-number-theory', 'prime-numbers']"
3038078,Find existence of limit,"Show that $\left(\frac{1}{|x|} +\frac{1}{|y |}\right)$ tends to infinity as $(x,y)$ tends to $(0,0)$ .
I have used $x=r\cos \alpha$ and $y=r\sin \alpha$ , $r>0$ .But I got stuck as $\left(\frac{1}{|\cos \alpha|} +\frac{1}{ |r\sin \alpha|}\right) \left(\frac{1}{r}\right)$ lies between $\frac{1}{r}$ and infinity.","['limits', 'calculus']"
3038130,Modelling of a flow network with a positional constraint,"While I do realise pasting an exercise question in here is not exactly perfect form, I am desperate and I wil try anyway. So here it goes: Consider a set of n mobile computing clients in a town with k mobile base stations. Every client needs to be connected to one exactly one base station. Each of the n clients as well as each of the k base stations is speciﬁed by its (x,y) coordinates in the plane. Each client needs to be connected to exactly one base station. For a given parameter r, a client can only be connected to a base station that is within distance r from the client. At the same time, no base station can handle more than L clients being assigned to it. Design a polynomial-time algorithm for the following problem: Given the positions of the n clients and k base stations, as well as the range and load parameters r and L, respectively, decide whether all clients can be connected to the base stations, subject to the range and load conditions as described above. Argue that your algorithm is correct. I understand that I have to use Max-Flow theory somehow, and I am guessing the flow network would be a bipartite graph with a constraint on the degrees of one of the two partitions to be <= L. But how do I model a positional variable like r into it? Any help would be greatly appreciated.","['graph-theory', 'network-flow', 'bipartite-graphs', 'discrete-mathematics']"
3038148,How can I write a function for $c$ in terms of $a$ and $b$? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question For $a,b\neq 0$ , let $c$ be $1$ if $a$ and $b$ both are positive. $-1$ if any one of $a$ or $b$ is negative or  both are negative. How can I write a function for $c$ in terms of $a$ and $b$ ?","['algebra-precalculus', 'functions']"
3038160,Why is it Euler's 'Totient' Function?,"The function $\phi(n)$ calculates the number of positive integers $k \leqslant n \space , \gcd(k,n)=1$ . It was found by mathematician Leonhard Euler . In 1879, mathematician J.J.Sylvester coined the term 'totient' function. What is the meaning of the word 'totient' in the context? Why was the name coined for the function? I have received replies that 'tot' refers to 'that many, so many' in Latin. What about the suffix 'ient'? It can be seen in words such as 'quotient' etc. Finally isn't there any reference to 'relatively prime numbers' ?","['number-theory', 'math-history', 'terminology']"
3038211,prove a polynomial has at least 2n-1 distinct real roots,"If $P(x)$ is a real polynomial has $n$ distinct real roots in $(1,+\infty)$ . Set: $$Q(x)=(x^2+1)P(x)P'(x)+x(P^2(x)+P'^2(x))$$ How to prove $Q(x)=0$ has at least $2n-1$ distinct real roots? I think the idea is finding closed intervals that the boundary points have different signs. I think the only special points are the roots of $P(x)$ or $P'(x)$ . But if $a$ is such a point, we have $Q(a)>0$ . So i'm not sure this can work out... Can anyone show me some hints? Thanks!","['linear-algebra', 'polynomials']"
3038253,Simple error propagation calculation,"I have two elevation models that I subtract from each other, to find out how the topography changed over time. For the elevation models I have given the RMSE error of the horizontal and vertical uncertainty/error. I want to calculate the error of the resulting elevation model, that represents the differences between the two models. I know that I have to calculate a error propagation, but I am very unsure how to do it. The method does not need to be very exact or complex, I need a simple method to state that the resulting elevation model has an error of round about xy. The first model has a resolution of 0.05 m with an error of 0.03 m horizontally and 0.05 m vertically. The second model has a resolution of 1.0 m and an error of 0.15 m horizontally and 0.10 m vertically. So I would say, I have four variables: Variable 1: 0.05 m +/- 0.03 m Variable 2: 0.05 m +/- 0.05 m Variable 3: 1.00 m +/- 0.15 m Variable 4: 1.00 m +/- 0.10 m Is this correct? How can I further calculate the error? EDIT:
I am not sure about the output variable but I think I would have two output variables. Variable 1: output resulting from Variable 1 and Variable 3. Maybe something like XX m +/- XX m or a percentage value. Variable 2: output resulting from Variable 2 and Variable 4","['statistics', 'error-propagation']"
3038261,What's the direct sum of infinite sheaves?,"In the Lemma 5.1.3 of Liu Qing's book on algebraic geometry, he uses $O_{X}^{(I)}$ which the direct sum of $O_{X}$ indexed by $I$ . What's the global section of this sheaf? Is it $\bigoplus O_{X}(X)$ ? But I think this is impossible since when $I$ is infinite we can suppose $X=\bigsqcup U_{i}$ and glue $1_{U_{i}}$ together to get a global section which can't be an element in the direct sum . Or we just get a presheaf from $O_{X}^{(I)}$ and we consider its associate sheaf instead? If so, is there a big difference between direct sum of infinite sheaves and product of infinite sheaves?","['algebraic-geometry', 'coherent-sheaves']"
3038278,The surjective image of a proper scheme is proper,"Proper is supposed to generalize compactness so something like this should be true. Let $X$ be a proper $S$ -scheme and let $f:X\rightarrow Y$ be a surjective morphism of $S$ -schemes. Is it true that $Y$ also a proper $S$ -scheme? I think that I can prove that $g:Y \rightarrow S$ is universally closed. In fact, given a $S$ -scheme $Z$ we have that the composition $$f_Z:X\times_S Z\rightarrow Y\times_S Z$$ is surjective as this property is invariant under base change. So if $W$ is a closed subset of $Y\times_S Z$ we have that $$g_Z(W)=(g_Z\circ f_Z)(f_Z^{-1}(W))$$ is closed as $g_Z\circ f_Z=X\times_S Z\rightarrow Z$ is a base chage of $X\rightarrow S$ . Also, as is show in here , Y should be separated. ( Edit: This was not true as KReiser example shows ) Now, what about finite type? Remark: Because of this it is enough to prove locally of finite type. Edit: Corrected the proof of universally closed.","['algebraic-geometry', 'schemes']"
3038296,"Let $A$ and $B$ be matrixes. If $BA=B$ and $Rank\space A = Rank\space B$, prove $A^2=A$","Let $A$ and $B$ be matrixes. If $BA=B$ and $Rank\space A = Rank\space B$ , prove $A^2=A$ Ok, so I can see that: $ABA=AB$ $AABA=AAB$ $AABAA=AABA$ $A^2BA^2=A^2BA$ but I don't know how to keep following. Any hint? Also how would I apply the Rank thing? EDIT: I messed up ABA implies B is regular which I don't know.","['matrices', 'linear-algebra']"
3038312,Why don't we always consider complete measure spaces?,"Let $(\Omega ,\mathcal F,\mathbb P)$ a probability space and let $X=(X_t)$ and $Y=(Y_t)$ two stochastic processes. I know  for example that $X$ and $Y$ are indistinguishable if there is a set $N$ of measure $0$ s.t. for all $\omega \notin N$ we have $X_t=Y_t$ for all $t$ , but we can't write $\mathbb P\{\forall t, \ X_t=Y_t\}$ since $\{\forall t, X_t=Y_t\}$ may be not $\mathcal F-$ measurable. The thing is if $Y$ is a copy of $X$ and $(\Omega ,\mathcal F,\mathbb P)$ is complete, then $\{\forall t,X_t=Y_t\}$ is $\mathcal F-$ measurable. I also know that each measure space can be completed by adding sets of measure. Questions : So, why don't we always work with complete measure space (since they can be always completed), and avoid for example the problem of the measurability of $\{\forall t, X_t=Y_t\}$ if $Y$ is a copy of $X$ (or many other measurability problem) ? In what working in a non complete measure space can be interesting, or at least more interesting than to work with it's completion ? (since a non complete measure space can always be completed). Do you have an example where it's worth to work with the uncompleted measure space rather than with the completed space ?",['probability']
3038349,Example of a ring with no minimal prime ideal,"I am a math student, in the course of abstract algebra we have shown that in a unitary commutative ring every ideal I possesses at least one minimal prime ideal. I am trying to find an example of ideal contents in a non-commutative (not unitary) ring that does not possess a minimal prime ideal.","['abstract-algebra', 'maximal-and-prime-ideals', 'ideals']"
3038449,How to prove an expectation inequality?,"Let $X_1,X_2,\ldots$ be mutually independent random variables, and for each $X_i$ , there is $P(X_i=1)=p$ and $P(X_i=0)=1-p$ . Let $Y_1,Y_2,\ldots$ be binary random variables defined by \begin{equation}
\begin{split}
Y_1&=X_1,\\
Y_t&=\begin{cases}
1,&\mbox{if $\frac{1}{t}\sum^t_{j=1}X_j\ge p+(1-2p)\frac{1}{t-1}\sum^{t-1}_{i=1}Y_i$}\\
0,&\mbox{otherwise}
\end{cases}.
\end{split}
\end{equation} We'd like to prove that \begin{equation}
E(Y_t)=Pr(Y_t=1)\le p, \ \ \ \forall\, t=1,2,\ldots
\end{equation} Could you please tell me how to prove it? Thanks a lot.","['expected-value', 'combinatorics', 'discrete-mathematics', 'probability-theory', 'probability']"
