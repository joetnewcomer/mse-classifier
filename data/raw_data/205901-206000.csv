question_id,title,body,tags
4101324,Computation of binomial summation [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I am trying to compute the summation $$
\sum^{n}_{i=0, \,i\text{ even}} i \binom{n}{i}
$$ but got stuck. Is there any possible hint?","['summation', 'binomial-coefficients', 'combinatorics']"
4101330,"Is $A^c \cap B^c$ finite if $A,B$ are disjoint finite subsets of an uncountable set? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If $\Omega$ is an uncountable set, and $A$ and $B$ are two finite sets such that $A, B \subset \Omega$ and $A \cap B = \varnothing$ . Is $A^c \cap B^c$ finite? I think it is (for example, $\Omega = \mathbb R$ , $A = \{1\}$ , $B = \{2\}$ ) but don't know how to prove it.",['elementary-set-theory']
4101350,A non-linear coupled second order ODE for a geodesic,"Let $g = (dx)^2 + e^x (dy)^2$ be a Riemannian metric on $\mathbb{R}^2$ . I want to find the equation of the geodesic. To do this, I know that I can solve a system of coupled non-linear second order ODEs. After calculating the Christoffel symbols, I have arrived at the following system for $\gamma(t)=(\gamma^1(t),\gamma^2(t)) =(u(t),v(t))$ : $$ u''-\frac{1}{2}e^u(v')^2 =0 $$ $$ v''+u'v'=0 $$ I'm not very familiar with ODEs beyond the basic techniques that one might learn in a first course. I started by guessing $u=-k\log t$ for some $k>0$ , but this turned out to not work. Can someone help me solve this system? 🥺 This is not for homework. I wanted to do an example to help me better understand parallel transport, but it ended up being too complicated. Thanks. EDIT: Intuitively, I feel like a geodesic from $(0,0)$ to $(1,1)$ will travel in a path that sticks closely to the $y$ -axis for most of its time, because $e^x$ makes $g$ ""lengthier"" the bigger the value of $x$ . This led me to guess a solution that is like $\log t$ , but I don't know if this intuition is correct. Per AlexanderJ93's solution, I have graphed the parameterization on Desmos for $a=b=c=1$ and got a curve that is roughly what I expected in the previous edit.","['nonlinear-system', 'geodesic', 'ordinary-differential-equations', 'differential-geometry']"
4101379,Proving an integral doesn't converge,"I have derived a differential equation for the density of a gas due to gravity as a function of distance to the center of mass. However, in order to prove some properties about it, I need to know if a certain integral converges or diverges to infinity. The latter seems to be the case. In this sense, given that $$\rho'' = \dfrac{4\pi G}{RT}\rho^2 + \dfrac{(\rho')^2}{\rho}-\dfrac{2T+zT'}{zT}\rho'$$ (Where $\rho$ and $T$ are a function of $z$ )
How do I prove that $$\int^{\infty}_{0}x^2\rho(x)\mathrm{d}x$$ does not converge? For simplicity, one may consider $T(z)$ as being constant","['integration', 'definite-integrals', 'ordinary-differential-equations', 'calculus', 'convergence-divergence']"
4101401,Prove that $\prod_{i\geq 1}\frac{1}{1-xy^{2i-1}} = \sum_{n\geq 0} \frac{(xy)^{n}}{\prod_{i=1}^{n}\left( 1-y^{2i} \right)}.$,"Prove that $$\prod_{i\geq 1}\frac{1}{1-xy^{2i-1}} = \sum_{n\geq 0} \frac{(xy)^{n}}{\prod_{i=1}^{n}\left( 1-y^{2i} \right)}.$$ Here I am trying the following \begin{align*}
\prod_{i\geq 1}\frac{1}{1-xy^{2i-1}} &= \prod_{i\geq 1} \left( \sum_{n \geq 0} x^{n}y^{n(2i-1)} \right)\\ 
&= \sum_{m\geq 0} \sum_{n \geq 0} p(2m-1,n)x^{n}y^{2m-1}\\ 
&= \sum_{n\geq 0}\left( \sum_{m \geq 0} p(2m-1,n)y^{2m-1}\right)x^{n} \\ 
&= \sum_{n\geq 0} \frac{y^{n}}{\prod_{i=1}^{n}\left( 1-y^{2i} \right)}x^{n}\\ 
\end{align*} However, I am not sure if this is totally correct.","['integer-partitions', 'discrete-mathematics', 'generating-functions']"
4101406,Distribution of minimum of Uniform products [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I've been stuck on this question for a while. { $U_{i}$ iid, Uniform(0,1) $V_{n} = \prod_{1}^{n} U_i$ , n = 1, 2,... $N = \min \{k: V_k < 0.1\} $ Find the distribution of $N$ I know that this is supposed to be related somehow to a poisson distribution based off of other things that I have learnt, but that was with the summation of uniform distributions, not the product. I honestly just don't know where to begin. Any help would be very much appreciated!","['statistics', 'uniform-distribution', 'poisson-distribution', 'probability-distributions', 'probability']"
4101445,Question about Divergence formula derivation posted a while ago,"As a new user, I am not allowed to comment on someone else's answer to a question. My only choice was to ask a new question about an old answer to a question. User @Kcronix mentioned, in this question , that the divergence of an arbitrary vector field can be derived as the net flux through the boundary of a region $R$ : $$\text{net flux}=\oint_{\partial \text{Rect}} \vec{V}\cdot \hat{n}\;\mathbb{d}s=\int_{R} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{L} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{T} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{B} \vec{V}\cdot \hat{n}\;\mathbb{d}s$$ I have tried to define the same situation in a notebook and I figured that going counter-clockwise through the sides of the rectangle would be a good idea. So it would be simply rearranged in my case: $$\text{net flux}=\oint_{\partial \text{Rect}} \vec{V}\cdot \hat{n}\;\mathbb{d}s=\int_{B} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{R} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{T} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{L} \vec{V}\cdot \hat{n}\;\mathbb{d}s$$ Thus, I defined my top flux as: $$\int_{T} \vec{V}\cdot \hat{n}\;\mathbb{d}s=\int_{x+\Delta x}^{x} \left(M(X,y+\Delta y)\hat 
 i+N(X,y+\Delta y)\hat j\right)\cdot \hat j\;\mathbb{d}X=\int_{x+\Delta x}^{x}N(X,y+\Delta y)\;\mathbb{d}X$$ Please, notice how the integral goes from $x+\Delta x$ to $x$ , instead of going from $x$ to $x+\Delta x$ . This also happens to occur with the left side of the rectangle, going from $y+\Delta y$ to $y$ , instead of $y$ to $y+\Delta y$ . This is my main question with this derivation. I know I can switch the boundaries of the integral by bringing a negative sign outside of it (from the Fundamental Theorem of Calculus), but still, I would get: $$\text{net flux}=\int_{y}^{y+\Delta y}M(x+\Delta x,Y)\;\mathbb{d}Y+\int_{y}^{y+\Delta y}M(x,Y)\;\mathbb{d}Y\\-\int_{x}^{x+\Delta x}N(X,y+\Delta y)\;\mathbb{d}X-\int_{x}^{x+\Delta x}N(X,y)\;\mathbb{d}X\tag{1}$$ and I do not think I can reduce to the partial derivative definition with the above. Is there anything I am missing?","['divergence-theorem', 'divergence-operator', 'multivariable-calculus', 'partial-derivative', 'line-integrals']"
4101451,When to simply plug in infinity when evaluating limits to infinity.,"Please don't destroy me as I am teaching myself calculus. Anyway, my question pertains to this problem: The problem with solution. This is the solution to the limit of $x-$ $\sqrt{x^2-7}$ as $x$ approaches - $\infty$ . My question is, when is it okay to just plug in - $\infty$ like they did in the solution? Also, when I did the problem, I got an answer of 0. Here is my step by step solution: My solution Feel free to critique it and give me and tips, tricks, and advice. Thank you! Edit: I made an error. The denominator in my solution should be a 0 as -1 +1 = 0.My final result based on the work I did should be 0/0.  Nevertheless, my answer is still incorrect.","['limits', 'calculus', 'soft-question', 'infinity']"
4101491,Hard! Integrate $\int_0^{\frac{\pi}{2}}\frac{x\ln|\cot (x-\frac{\pi}{4})|}{\sin^2x}{d}x=\frac{\pi^{2}}{4}+\frac{\pi}{2}\ln2$,"I am struggling with this integral, and I have tried to draw several contours but none did work. So proving: $$\int_0^{\frac{\pi}{2}}\frac{x}{\sin^2x}\ln\left|\cot\left(x-\frac{\pi}{4}\right)\right|\mathrm{d}x=\frac{\pi^{2}}{4}+\frac{\pi}{2}\ln2$$ So what I did is that I substitute $ t = \cot x$ , which leads to the following integral: $$ \int_{0}^{\infty} \frac{\arctan{t}}{t^2} \cdot \ln\left|\frac{1+t}{1-t}\right| \mathrm{d}t$$ However the absolute value of the logarithm bothers me a lot. I did try to split it out and obtain an equivalent integral: $$ 
 \int_{-\infty}^{\infty} \frac{\arctan{t}}{t^2} \cdot \ln\left| 1+t\right|\mathrm{d}t$$ It seems to me intuitively that one can construct a contour for this function but none of what I created works. The answer for this integral is considerably simple for me since there are various complicated integrals. Solutions with the application of real analysis are also welcomed. Thanks in advance!","['integration', 'contour-integration', 'residue-calculus']"
4101500,What/Where is the logical error in the second solution?,"Problem Statement Seven women and nine men are on the faculty in the mathematics department at a school. How many ways are there to select a committee of five members if at least one woman must be on the committee? Solution I My first thoughts to solve this problem were to find out how many possible combinations of committees could be formed from the given population. I then subtracted from this the total number of committees that could be formed of all men. ${16 \choose 5} - {9 \choose 5} = 4242$ On most platforms, this seems to be the consensus on what is the correct answer. Solution II Out of curiosity, I tried to solve the problem from a different angle. Assume we choose an arbitrary woman from the population of women faculty members as the first committee member. Now there are 6 women and 9 men in the remaining population. Now that at least one woman is guaranteed to be on the committee, the total number of committees that can be formed is the number of possible combinations of the remaining four spots, which is ${15 \choose 4} = 1365$ Obviously, this approach reveals a significantly smaller number of possible combinations, so my question is what went wrong?","['combinations', 'combinatorics', 'discrete-mathematics']"
4101562,"Derivative, slope or, the tangent of a graph with a shape which has corner like tips as in the letter V [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Consider the graph of a function in the shape of  the letter ‘V’ , how would we be finding the derivative, slope or, the tangent of the function at the value of the function that corresponds to the tip of ‘V’ ?","['graph-theory', 'tangent-line', 'derivatives', 'slope']"
4101568,$\forall K>0: \ P[\inf\{t>0:|B_t|>K\sqrt{t}\}=0]=1$ means that a.s. $B$ isn't Hölder-$\tfrac{1}{2}$ continuous at $t$?,"Suppose that for every $K > 0$ we have that $$
\mathrm{P} \left [\inf \left\{t >0: \left| B_t \right|> K \sqrt{t} \right \} = 0 \right] = 1,
$$ where $B$ is a Brownian Motion. I would like to motivate - from an explicit definition of Hölder- $\tfrac{1}{2}$ continuity - that this means that almost surely $B$ is not Hölder- $\tfrac{1}{2}$ continuous at $t$ . Perhaps it is possible to show that $B$ isn't Hölder- $\tfrac{1}{2}$ continuous ""from the right"". This would be sufficient for the claim to be true and perhaps easier than my attempt at a solution below, since it would be easier to apply the property $B_t - B_s \sim B_{t-s}$ ? Here comes my long winded attempt to a solution: Given any $t$ we have that the path $B_s(\omega)$ is $\tfrac{1}{2}$ -Hölder continuous at $t$ if there exists a $K>0$ and $\epsilon>0$ such that for $$
|t-s| < \epsilon,
$$ it is the case that $$
\left | B_t(\omega) - B_s(\omega) \right | \le K \sqrt{|t - s|}.
$$ We will try to use the fact that $$
B_t - B_s \sim B_{t-s}.
$$ If we want to calculate the probability that $B$ is $\tfrac{1}{2}$ -Hölder continuous at $t$ , we have to consider that the $\epsilon$ in the above requirement may differ for different paths (the $K$ may of course also vary). My idea was to start with an explicit expression for the event that $B_t$ is Hölder- $\tfrac{1}{2}$ continuous at $t$ , and then show that the given probability means that the complement of this event has probability one. Thus the event that $B$ is $\tfrac{1}{2}$ -Hölder continuous at $t$ may be written as, $$
\bigcup_{K=1}^\infty \bigcup_{n=1}^\infty\left \{\left | B_t - B_s \right | \le K \sqrt{|t - s|}:0< |t-s|<\frac{1}{n}  \right \}.
$$ Now, $$
\left \{\left | B_t - B_s \right | \le K \sqrt{|t - s|}:0< |t-s|<\frac{1}{n}  \right \}
$$ is distributed as (Not $100 \%$ sure about  this) $$
\left \{\left | B_{|t-s|}\right | \big/ \sqrt{|t - s|} \le K :0< |t-s|<\frac{1}{n}  \right \},
$$ and thus we may consider the event where $B$ is $\tfrac{1}{2}$ -Hölder continuous at $t$ as \begin{multline*}
\bigcup_{K=1}^\infty \bigcup_{n=1}^\infty\left \{\left | B_{|t-s|}\right | \big/ \sqrt{|t - s|} \le K :0< |t-s|<\frac{1}{n}  \right \} \\
= \bigcup_{K=1}^\infty \bigcup_{n=1}^\infty \left \{ \sup_{0 < |t-s| < \frac{1}{n} }\left |B_{|t-s|}\right | \big/ \sqrt{|t-s|} \le K  \right \} \\
=\bigcup_{K=1}^\infty \bigcup_{n=1}^\infty \left \{ \sup_{0 < t < \frac{1}{n} }\left |B_{t}\right | \big/ \sqrt{t} \le K  \right \}.
\end{multline*} If the above is correct, one may consider (a set that is distributed as) a subset of the the event that $B$ isn't $\tfrac{1}{2}$ -Hölder continuous at $t$ . We have that $$
\left (\bigcup_{n=1}^\infty \left \{  \sup_{0<t \le \frac{1}{n} }\left |B_{t}\right | \big/ \sqrt{t} \le K \right \}\right ) ^C = \left \{\inf \left\{t >0: \left| B_t \right|> K \sqrt{t} \right \} = 0 \right \}. 
$$ Now, $$
\left \{\inf \left\{t >0:  B_t > K \sqrt{t} \right \} = 0 \right \} \subset \left \{\inf \left\{t >0: \left| B_t \right|> K \sqrt{t} \right \} = 0 \right \},
$$ and so the given condition is sufficient for the desired conclusion as we have that a subset of the set where $B$ isn't Hölder- $\tfrac{1}{2}$ continuous has probability one. Most grateful for any help provided!","['stochastic-processes', 'brownian-motion', 'probability-theory']"
4101618,Statistical advantages of complex-valued random variables?,"I am interested in random complex numbers and am trying to understand: why to use complex random variables in statistics? Complex numbers can equally be viewed as a vector in $\mathbb{R}^2$ , but with a defined multiplication (or $2\times 2$ matrices). This thread addresses how to do linear regression with complex data. There are also several discussions around the value of complex numbers in Quantum Mechanics, where you also encounter probabilistic behaviour. I am aware complex-valued data arise in areas such as signal processing through complex numbers as a transform of real information. Alternatively, complex numbers can be constructed to represent a pair of real values and provide compact notation in other areas such as economics. Statistical operations are then run upon such data obeying the complex algebra. A common discussed benefit is compactness of notation, but do we also have calculation or computational benefits by taking advantage of $\mathbb{C}$ and its multiplication? To ask another way, once you have complex-valued data, are there specific statistical (e.g. distribution, modelling, estimation) benefits by continuing to treat it in the complex field, rather than using real bivariate statistical methods? (Note: This question was originally posted on CV , however I am hoping to broaden the audience it reaches given that it originates as a consequence of other fields' use of complex numbers.)","['statistics', 'complex-numbers', 'random-variables']"
4101683,A differential equation with a Leibniz's formula,"I found this exercise and I don't know how to solve the first question. Solve this differential equation : $$\sum_{k=0}^{n} \binom{n}{k}y^{(k)}=0$$ Deduce that for all $j<n$ : $$\sum_{k=0}^{n} \binom{n}{k} \binom{k}{j}(-1)^k=0$$ For the first part, I don't know how to start, it looks like a Leibniz formula for the product of functions $y$ and a function with all its derivatives equal to one .... which is strange. I also thought about the superposition principle, but I don't know if it's a good idea because it would impliy using differential equations of different orders. So,if you could give me some tips ^^
Thanks in adavance","['derivatives', 'ordinary-differential-equations']"
4101732,"Solving ODE with initial condition, one step wrong","I have a step wrong while solving my IVP but I cannot find what. I will post my detailed solution in the hope someone sees where it goes awry:
The IVP: $$t^2 \frac{dy}{dt}−t=1+y+ty,y(1)=4.$$ I start by moving all function of y on the RHS: $$\displaystyle$$ $$\displaystyle t^2\frac{dy}{dt}-y-ty=1 +t$$ $$t^2\frac{dy}{dt}-(1+t)y=1 +t$$ It is not in standard form so I continue by divinding by $t^2$ : $$\frac{dy}{dt}-\frac{(1+t)}{t^2}y=\frac{1 +t}{t^2}$$ With $h(t) = -\frac{(1+t)}{t^2}$ I can use integrating factor $e^{\int h(t)}$ = $e^{H(t)}$ . With $H(t) = \frac{1}{t} - ln(t)$ , I get : $$e^{\frac{1}{t} - ln(t)}\frac{dy}{dt}- e^{\frac{1}{t} - ln(t)}\frac{(1+t)}{t^2}y=\frac{1 +t}{t^2}e^{\frac{1}{t} - ln(t)}$$ Which can be rewritten as: $$\frac{d}{dt}(e^{\frac{1}{t} - ln(t)} y) = \frac{1 +t}{t^2}e^{\frac{1}{t} - ln(t)}$$ $$\displaystyle e^{\frac{1}{t} - ln(t)} y = \int{ \frac{1 +t}{t^2}e^{\frac{1}{t} - ln(t)}} $$ $$\displaystyle e^{\frac{1}{t} - ln(t)} y =  -e^{\frac{1}{t} - ln(t)} + C $$ 4. by multiplying with $e^-H(t)= e^{-\left(\frac{1}{t} - ln(t)\right)}$ we get: $$ y(1) = 4 =  -1 + Ce^{-\left(\frac{1}{t} - ln(t)\right)} \\
C = 5e$$ The initial value condition gives that C: $$y(1) = 4 =  -1 + Ce^{-\left(\frac{1}{t} - ln(t)\right)} \\
C = 5e$$ $$y(t) = -1 + 5 e e^{-\left(\frac{1}{t} - ln(t)\right)}  \\ 
y(t) = -1 + 5 e^{-\left(\frac{1}{t} - ln(t) - 1\right)}$$ But this appears to be wrong. Is there some step I am doing wrong? EDIT: fixed -1 in exponent as per the comments. EDIT 2: trying to plug in in the original equation Simplifying we get: $$y(t) = -1 + 5 t e^{1-\frac{1}{t}}\\
y'(t) =  5  e^{1-\frac{1}{t}} + 5t \frac{1}{t^2}e^{1-\frac{1}{t}} \\
y'(t) =  5  e^{1-\frac{1}{t}} + 5 \frac{1}{t}e^{1-\frac{1}{t}}$$ $$t^2 \frac{dy}{dt}−t=1+y+ty \\
t^2 \left(5  e^{1-\frac{1}{t}} + 5t \frac{1}{t}e^{1-\frac{1}{t}} \right) -t = 1 -1 + 5 t e^{1-\frac{1}{t}}  -t + 5 t^2 e^{1-\frac{1}{t}} $$ Seems everything is disappearing, but I still get wrong on the automatic assessment.",['ordinary-differential-equations']
4101763,Proving the Borell-Cantelli Lemma by martingale convergence theorem,"I got stuck on an exercise (Exercise 5.2.1) from the book:
Ergodic Theory: with view towards Number Theory By Manfred Leopold Einsiedler and Thomas Ward. In the book they presented the following theorem. Theorem: Let $(\Omega,\mathcal{A},\mu)$ be a prob. space and $(\mathcal{F}_n,n\geq 1)$ an increasing sequence of $\sigma-$ algebras, $\mathcal{F}=\sigma(\cup\mathcal{F}_n)$ . If $f\in L^1(\mathcal{A},\mu)$ , then: $$\lim_{n\rightarrow \infty}\mathbb{E}[f|\mathcal{F}_n]=\mathbb{E}[f|\mathcal{F}],$$ where the convergence is almost sure and in $L^1$ . And then the exercise asked for a proof of the following version of the Borell-Cantelli Lemma:
Let $(\Omega,\mathcal{A},\mu)$ be a prob. space and $(A_n)_{n\geq 1}$ a sequence of independent measurable sets. Then if $\sum_{n=1}^{\infty}\mu(A_n)=\infty$ it holds that $$\mu(\bigcap_{N=1}^{\infty}\bigcup_{n=N}^{\infty}A_n)=1.$$ I already achieved to apply the martingale convergence theorem to show that for $A:=\bigcap_{N=1}^{\infty}\bigcup_{n=N}^{\infty}A_n$ , we  get $\mu(A)=\mathcal{X}(A)$ so especially $\mu(A)\in\{0,1\}$ . But my question is now how I can prove that it actually holds $\mu(A)=1$ using the condition $\sum_{n=1}^{\infty}\mu(A_n)=\infty$ . Thanks for the help. Edit: Okay I think the following should work.
Just assume $\mu(A)=0$ then we get $0=\lim_{N\rightarrow\infty}\mu(\cup_{n\geq N}A_n)=\lim_{N\rightarrow\infty}\sum_{n=N}^{\infty}\mu(A_n)$ and this contradicts the assumption $\sum_{n=1}^{\infty}\mu(A_n)=\infty$ . Is that correct?","['martingales', 'ergodic-theory', 'probability-theory']"
4101898,A non-standard ordinary differential equation: $\frac{d\left(xf\left(x\right)\right)}{dx}f(x)=\text{const}$,How can I solve this ODE? $$\frac{d(xf(x))}{dx}f(x)=\text{const}.$$ It is clear that $f\left(x\right)=\text{const}$ is a solution but is it the only one? I don't need a formal proof just a good argument. A formal proof is also good though.,"['calculus', 'ordinary-differential-equations']"
4101974,What does this matrix operator norm mean?,"I am trying to understand what this matrix operator norm means and what it does to matrix $A$ . $$ {{\left\| A \right\|}_{1,\,\infty }} := {\max_{{{\left\| x \right\|}_{\infty }}=1}}{{\left\| Ax \right\|}_{1}} $$ Can somebody help with the explanation and maybe an example?","['matrices', 'matrix-norms']"
4102009,How do I find the length attained by a bullet when fired by a rifle?,"The problem is as follows: The horizontal range of a bullet fired by a rifle from a certain height above sea level is given by the minimum value of the function presented below: $$f(x)=2\sec\left(\pi x-\frac{\pi}{4}\right)+1$$ in kilometers when, $-\frac{1}{12}\leq x \leq \frac{5}{12}$ With the given information, find the length attained by the bullet. The alternatives given in my workbook are as follows: $\begin{array}{ll}
1.&\textrm{4 km}\\
2.&\textrm{1 km}\\
3.&\textrm{3 km}\\
4.&\textrm{2 km}\\
\end{array}$ What I attempted to do in order to solve this problem was to use the given domain to reconstruct the function and hence having the range, and with that minimum value I can get what it is being asked. In other words the horizontal range of that bullet. Since the domain is in this: $-\frac{1}{12}\leq x \leq \frac{5}{12}$ Then: $-\frac{\pi}{12}\leq \pi x \leq \frac{5\pi}{12}$ $-\frac{\pi}{12}\leq \pi x \leq \frac{5\pi}{12}$ $-\frac{\pi}{12}-\frac{\pi}{4}\leq \pi x - \frac{\pi}{4} \leq \frac{5\pi}{12}-\frac{\pi}{4}$ $-\frac{4\pi}{12}\leq \pi x - \frac{\pi}{4} \leq \frac{2\pi}{12}$ $-\frac{\pi}{3}\leq \pi x - \frac{\pi}{4} \leq \frac{\pi}{6}$ Now here's where it comes the source of controversy. I'm assuming that I can ""take the secant function"" to that interval in order to get the range. Which would yield. $\sec\left(-\frac{\pi}{3}\right)\leq \sec \left(\pi x - \frac{\pi}{4}\right) \leq \sec \left(\frac{\pi}{6}\right)$ Since the negative sign is absorved by the secant function this will generate: $2 \leq \sec \left(\pi x - \frac{\pi}{4}\right) \leq \frac{2}{\sqrt{3}}$ $4 \leq 2\sec \left(\pi x - \frac{\pi}{4}\right) \leq 2\frac{2}{\sqrt{3}}$ $4 +1  \leq 2\sec \left(\pi x - \frac{\pi}{4}\right)+1 \leq \frac{2 \cdot 2}{\sqrt{3}}+1$ The finally: $5  \leq 2\sec \left(\pi x - \frac{\pi}{4}\right)+1 \leq \frac{4}{\sqrt{3}}+1$ But rewritting this doing the rationalization it would be: $5 \leq f(x) \geq \frac{4\sqrt{3}+3}{3}$ Now: $\frac{4\sqrt{3}+3}{3} \approx 3.3094$ But looking at the orientation of the signs it doesn't make sense. How it can possibly be that this result is greater than $4$ ? What went wrong here?. Can someone help me here?. Or is it just that I did not got the right picture?. I can spot that $4\,km$ appears in one of the choices But I have no idea if whether I got to the right answer or some critical concept I missunderstood here. Thus I require help to settle down this contradiction. Help please?. I'd also like to know that since I got this problem in my precalculus workbook I'd like that the answer would follow the similar approach given here and not use derivatives.","['maxima-minima', 'algebra-precalculus', 'functions', 'trigonometry']"
4102159,Bounds on $\sum\limits_{k=1}^n \frac{\sin(k)}{k}$,"$\sum\limits_{k=1}^n \frac{\sin(k)}{k}$ converges as $n$ increases , to a limit of $\frac12(\pi-1) \approx 1.0708$ Empirically, it seems to be bounded by about $\frac12(\pi-1)  \pm \frac{1.043}{n}$ , as shown in the chart below. What is the precise value of this $1.043$ term? Added: I think the three answers from Oliver Diaz, Gary, and Gabriel Romon, plus the comments, have demonstrated that the bounds are $$\frac12(\pi-1)  \pm \frac1{2\sin(1/2)}\frac{1}{n}$$ not only asymptotically but also as tight actual bounds; $\frac1{2\sin(1/2)}\approx 1.0429148214667441$ created in R with n <- 1:11000
plot(n, cumsum(sin(n)/n), ylim=c(1.07,1.0715), xlim=c(1,10000), pch=46)
abline(h=(pi-1)/2, col=""red"")
curve((pi-1)/2 + 1.043/x, from=1, to=max(n), add=TRUE, col=""red"")
curve((pi-1)/2 - 1.043/x, from=1, to=max(n), add=TRUE, col=""red"")","['upper-lower-bounds', 'analysis', 'sequences-and-series']"
4102164,Fitting a tetrahedron through the smallest hole,"I'm designing a child's toy consisting of a closed box with a hole on top; a unit tetrahedron must fit through this hole. What is the smallest possible area of the hole? Currently my hole is an isosceles triangle with base $1$ and height $\frac{\sqrt{2}}{2}$ (the distance between the midpoints of two opposite edges), which gives area $\frac{\sqrt{2}}{4}$ . With the correct orientation, the tetrahedron is dropped straight through the hole. Is it possible to do better considering rotations of the tetrahedra during insertion?","['geometry', '3d']"
4102167,Common notation of differentials,I have a doubt which confuses me a lot. If f(x) is a function in x then does f’(2x) mean $d f(2x)/d 2x $ or $d f(2x)/ d x $ as when we integrate it the result is f(2x)/2 and the same for $ f’(x^2)$ . Is it $d f(x^2)/d x^2 $ or $d f(x^2)/d x $ and what do we get on integrating it? Sorry for the very petty doubt but it really confuses me. Any help would be greatly appreciated ! Thanks in advance!,['derivatives']
4102186,How does one prove the following inequality?,"I am currently studying Calculus and I have stumbled upon an inequality, which is crucial to one of the proofs in the “Limits of functions”, namely the following limit ( $ m \in \mathbb{N} $ ): $$ \lim_{x \to 0} \dfrac{\sqrt[m]{1+x} - 1}{x} = \dfrac{1}{m} $$ And the inequality I’m talking about is ( $|x| > 1$ ): $$ 1 - |x| < \sqrt[m]{1+x}  < 1 + |x| $$ The textbook says, that the limit immediately follows from the latter expression, but I can’t see how that is even possible, since we put $ |x| > 1 $ but in the limit we head $x$ towards 0. How should I deal with this?","['limits', 'calculus', 'functions']"
4102239,Importance of stability in optimal control,"This is a quite general question, so even answers highlighting particular papers or books will be very helpful. Given an optimal control problem of the form $$\min_{u\in\mathbb{R}^k} Loss(x,u) \;\;s.t. \;\; \dot{x}(t) = f(t,x,u),$$ when is the stability under small perturbation in the initial data of the plant equation $\dot{x}(t) = f(t,x,u)$ relevant? In these cases, why is it relevant? For simplicity let us consider a Bolza problem, so $Loss(x,u) = f_u(\Phi^T(x_0))$ , where $\Phi^T$ is the flow of the ODE at time $T>0$ . I am quite new to control problems, so I know the importance of stability for dynamical systems in general and even the various available notions, but I don't get why for control problems it should be important.","['optimal-control', 'control-theory', 'ordinary-differential-equations', 'dynamical-systems']"
4102246,Applications of Riemann Series Theorem,"I recently came across Riemann Series Theorem. The theorem seems to be quite general and powerful, making strong statements on the limsup and liminf of rearrangements of conditionally convergent series (specifically that the limsup and liminf can take any arbitrary value). Consequently, I would imagine that it has lots of applications to interesting problems, in proving theorems etc. A cursory search of this site revealed very few such answers and my book does not list any interesting problems. So, I am looking for problems/theorems which can be easily answered/proved using Riemann series theorem.","['big-list', 'sequences-and-series', 'examples-counterexamples', 'real-analysis']"
4102249,Existence of a weak solution to an SDE,"Let $A\subset\mathbb{R}^n$ be a compact set. Suppose I have an SDE in $\mathbb{R}^n$ of the form $$d X_t = b(X_t)dt + \sigma(X_t)dW_t$$ with $X_0=x_0\in A$ , where $b$ and $\sigma$ are continuous but not necessarily bounded. I am having a hard time formulating the existence of a solution taking values in $A$ .  Intuitively, I would want to stop the process $X$ at $\tau = \inf\left\{t\geq 0 : X_t\notin A\right\}$ . Then, $b$ and $\sigma$ are bounded over $A$ so Theorem 5.4.22 in Karatzas and Shreve gives us existence. Is it rigorous to proceed in this way? If $X$ doesn't necessarily exist, how can we define $\tau$ ?","['stochastic-processes', 'stochastic-differential-equations', 'stopping-times', 'probability-theory', 'stochastic-calculus']"
4102256,Computing Derivatives of Functions $\mathbb{R} \to \mathbb{C}$,"In every complex analysis book I have come across (mainly Ahlfors and Stein/Shakarchi), the calculation $\frac{d}{dt} e^{it} = ie^{it}$ is taken for granted. But I want to look at this more carefully, because it doesn't seem immediately obvious to me. The function $f(t) = e^{it}$ is a function $\mathbb{R} \to \mathbb{C}$ , and for any function $\mathbb{R} \to \mathbb{C}$ we define the derivative by dealing with the real and imaginary parts separately: $\frac{d}{dt}(u(t) + iv(t)) := u'(t) + iv'(t)$ . So working from the definition, I would compute $\frac{d}{dt}e^{it}$ by writing $e^{it} = \cos(t) + i\sin(t)$ and differentiating each component. This does give us $ie^{it}$ , but what the authors of these books seem to suggest without proof is that there is a type of chain rule that can be applied to make the calculation simpler. This chain rule would have to apply to compositions $\mathbb{R} \to \mathbb{C} \to \mathbb{C}$ , but the only chain rules I have seen explicitly described are for compositions of differentiable functions $\mathbb{R} \to \mathbb{R} \to \mathbb{R}$ and for holomorphic functions $\mathbb{C} \to \mathbb{C} \to \mathbb{C}$ . In other places in the books, a chain rule for compositions $\mathbb{R} \to \mathbb{R} \to \mathbb{C}$ was used, but the proof of this one is trivial in light of the definition of derivatives of functions $\mathbb{R} \to \mathbb{C}$ and the ordinary real chain rule. But the case $\mathbb{R} \to \mathbb{C} \to \mathbb{C}$ is different from all of these, and I'm surprised none of the authors have mentioned it, as it doesn't seem to follow immediately from either the $\mathbb{R} \to \mathbb{R} \to \mathbb{R}$ or $\mathbb{C} \to \mathbb{C} \to \mathbb{C}$ chain rule. Does such a chain rule actually exist? And if so, how do we prove it?","['complex-analysis', 'chain-rule']"
4102313,On an integral related to the prime number theorem,"I am trying to prove the prime number theorem by showing $$
J(x)=\sum_{n\le x}{\Lambda(n)\over\log n}\sim{x\over\log x}
$$ To begin with, I use $$
\sigma\ge1-{A\over\log^9|t|}
$$ as the zero-free region for $\zeta(\sigma+it)$ , so applying Perron's formula gives $$
J(x)={1\over2\pi i}\oint_\Gamma{x^s\over s}\log\zeta(s)\mathrm ds+\mathcal O\left(xe^{-c\log^{1/10}x}\right)
$$ where $\Gamma$ is any positively oriented contour that encloses only $s=1$ . Since $\zeta(s)\sim 1/(s-1)$ when $s$ is near one, it suffices to evaluate $$
{1\over2\pi i}\oint_\Gamma{x^s\over s}\log{1\over s-1}\mathrm ds
$$ However, this is where I become stuck. Could anybody suggest a contour to work this out?","['complex-analysis', 'residue-calculus', 'analytic-number-theory']"
4102332,MGF of standard normal raised to a power,"Let $Z \sim \mathcal N(0,1)$ be a standard normal variable. What is the moment generating function $M_{Z^d}(t)$ for the variable $Z^d$ where $d$ is a positive integer? For $d = 1$ , we have $M_Z(t) = e^{t^2/2}$ , and for $d = 2$ , thinking of the variable as Chi-Squared with $1$ degree of freedom, we have that $M_{Z^2}(t) = \frac{1}{\sqrt{1 - 2t}}$ . Is computation of $M_{Z^d}(t)$ tractable for general $d$ ?","['moment-generating-functions', 'probability-theory']"
4102395,How to compute $\lim\limits_{x\to 0}\dfrac{e^x-1}{x}$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How does one compute $\lim\limits_{x\to 0}\dfrac{e^x-1}{x}$ without using l'Hôpital's rule or knowledge about the derivative of $e^x$ ? $e^x$ denotes the exponential function with base $e$ (Euler's constant).","['limits', 'exponential-function']"
4102482,Equality of two-variable expectations on product $\sigma$ algebra,"Setting Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $(S, \mathcal{S})$ and $\left(S^{\prime}, \mathcal{S}^{\prime}\right)$ be two measurable spaces. Consider two independent r.v. $X: \Omega \rightarrow S$ and $Y: \Omega \rightarrow S^{\prime}$ . a) Let $f: S \times S^{\prime} \rightarrow[0, \infty)$ be a measurable with respect to the product $\sigma$ -algebra $\mathcal{S} \otimes \mathcal{S}^{\prime} .$ Define $$ g: x \in S \mapsto \mathbb{E}[f(x, Y)] \in[0, \infty] $$ I want to show that $\mathbb{E}[g(X)]=\mathbb{E}[f(X, Y)]$ . Questions My first question is whether $g$ is at all well-defined: why are we allowed to ""freeze"" one variable here? Then, I also cannot convince myself that it $g$ is at all measurable I feel it follows from the Monotone Class Theorem and the fact but cannot formalize it. Ultimately, any hints about proving the statement would be appreciated -- I do not know where to even start.","['measure-theory', 'measurable-functions', 'general-topology', 'probability-theory', 'probability']"
4102561,$A_i \cap A_j$ has $i + j$ elements [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have the next problem: Find sets $A_1,...,A_n$ , such that $|A_i\cap A_j|=i+j$ for $i,j \leq n$ with $i \neq j$ I've tried to solve the problem but I just can't imagine the way to do it. I know the solution by an induction construction in the infinite case, but the finite case is not clear to me. I don't want just say something like ""this works in the infinite case, so in the finite would do it too"".","['elementary-set-theory', 'combinatorics']"
4102627,Combining terms in a conditionally convergent series,"I am aware that one is unable to rearrange terms in a conditionally convergent series. But, take a conditionally convergent series, say $$\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n}$$ and group terms with a stride of two to produce $$\sum_{n=1}^{\infty} \frac{1}{2n-1} - \frac{1}{2n}.$$ With this particular series, even if I take the absolute value of the terms, it converges to the same value. Does this imply the reformatted series is absolutely convergent, and that I can validly rearrange its terms arbitrarily?","['conditional-convergence', 'convergence-divergence', 'absolute-convergence', 'sequences-and-series']"
4102636,"Sidon set in $\mathbb F_q\times\mathbb F_q$, where $q$ is the power of a odd prime","I can't prove that the set $A=\{(r(x), s(x))\mid x\in\mathbb F_q\}$ is a Sidon set in $\mathbb F_q\times \mathbb F_q$ , where $r(x)$ and $s(x)$ are two polynomials of degree $2$ or less in $\mathbb F_q[X]$ linearly independent. Anyone can helpme with some hints I would appreciete a lot. Thanks. Just to remember a Sidon set is set where all the sums between two elements are different, i.e., for all $a,b,c,d$ elements in a Sidon set, then $a+b\neq c+d$","['additive-combinatorics', 'combinatorics', 'discrete-mathematics']"
4102701,Erroneous proof that derivative of function is always continuous,"Suppose that $f:\mathbb{R}\to \mathbb{R}$ is differentiable on the entire real line. (It is not difficult to give an example when $f'(x)$ is not continuous.) Find the flaw in the following ""proof"" that $f'(x)$ is continuous. Proof: Let $x_0$ be an arbitrary point on $\mathbb{R}$ and $f'(x_0)$ the derivative of $f$ at the point $x_0$ . By definition of the derivative and Lagrange's(aka MVT) theorem $$f'(x_0)=\lim \limits_{x\to x_0}\dfrac{f(x)-f(x_0)}{x-x_0}=\lim \limits_{x\to x_0}f'(\xi)=\lim \limits_{\xi \to x_0}f'(\xi), \qquad \qquad (*)$$ where $\xi$ is a point between $x_0$ and $x$ and therefore tends to $x_0$ as $x\to x_0$ . I have some difficulties to point out to the mistake in this ""proof"". But here is I was thinking. The first equality in $(*)$ is just the definition of the derivative. The second equality follows from MVT. I do not think that the third equality is true. So basically we need to show the following: If $\lim \limits_{x\to x_0}f'(\xi)=f'(x_0)$ then $\lim \limits_{\xi\to
 x_0}f'(\xi)=f'(x_0)$ I was trying to show it via $\varepsilon-\delta$ formalist and the fact that for any $x\in \mathbb{R}$ so that $x\neq x_0$ the $\xi$ is in between $x$ and $x_0$ but I failed. Can anyone help me please? Am I right that the third equality in $(*)$ is not valid. And where is the flaw in this ""proof""? EDIT: Suppose that $\lim \limits_{x\to x_0}f'(g(x))=f'(x_0)$ , where $g:\mathbb{R}\to \mathbb{R}$ defined by $$g(x)=
\begin{cases}
\xi_x, & \text{if } x\neq x_0, \\
x_0, & \text{if } x=x_0,
\end{cases}$$ and $g(x)$ has Intermediate Value property (every value between $g(x)$ and $x_0$ is taken somewhere between $x$ and $x_0$ by $g$ ). Then $$\lim \limits_{y\to x_0}f'(y)=f'(x_0).$$ Remark: I am trying to prove it in a rigorous way via $\varepsilon-\delta$ formalism but I have some issues. My approach: Let $\varepsilon>0$ be given then $\exists \delta>0$ such that $\forall x\in \mathbb{R} (0<|x-x_0|<\delta \Rightarrow |f'(g(x))-f'(x_0)|<\varepsilon)$ . I have no idea how to invoke the Intermediate value property to obtain the desired result.","['fake-proofs', 'real-analysis']"
4102736,Why do we use the unit circle to solve for sin and cos,"I know that in a unit circle where the radius is always one, sin is equal to y and cos is equal to x. But why do we use these values even when the radius or the hypothenuse of the triangle isn't equal to one. We also say that sin(π/2) = 1, but this only works if the hypothenuse of the triangle is one. What if it isn't. Can someone clear this confusion. P.S. I have just started studying trigonometry.",['trigonometry']
4102749,Joint distribution of two brownian bridges,"Consider $X_t=B_t-tB_1$ for $0\leq t\leq 1$ , where $B_t$ represents the standard Brownian motion. I derived that $E(X_t)=0$ and $\operatorname{Cov}(X_t,X_s)=\min(t,s)-st$ . Now, I am asked the joint distribution of $X_t$ and $X_s$ for some fixed $s,t\in[0,1]$ . My first thought was that the joint distribution is the bivariate normal with mean $\begin{pmatrix} 0 \\ 0 \end{pmatrix}$ and covariance matrix $\Sigma = \begin{pmatrix}  t(1-t) & \min(t,s)-st \\ \min(t,s)-st & s(1-s) \end{pmatrix}$ . However, I do not know how to justify this choice. Is it the bivariate normal?","['normal-distribution', 'bivariate-distributions', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
4102776,Inverse image as a function,"I'm trying to understand the below paragraph from Aluffi's ""Chapter 0"" textbook. The standard notation for the inverse of a bijection $f$ is $f^{-1}$ . This symbol is also used for functions that are not bijections, but in a slightly different context: if $f: A \to B$ is any function and $T \subseteq B$ is a subset of $B$ , then $f^{-1} (T)$ denotes the subset of $A$ of `all elements that map to $T$ '; that is, $$f^{-1} (T) = \{a \in A \mid f(A) \in T\}.$$ In what sense, though, is the inverse image a function? It surely isn't a function on elements $B \to A$ since $f$ map not be injective. $T$ is a subset of $B$ and $f^{-1} (T)$ a subset of $A$ , so can we say that $f^{-1}$ is a function from $\mathcal{P}(B)$ to $\mathcal{P}(A)$ ? I assume we have to require that both $A$ and $B$ are nonempty in this case.","['elementary-set-theory', 'functions']"
4102806,Do only many-one functions cross its horizontal asymptote?,"Informal definitions: many-one, rational function and asymptote Many-one : For a function $f: A \to B$ , If two o more elements of A, let's say, $a_{1}$ and $a_{2}$ have the same image in $B$ , the function is referred to as a many-one function, also called many-to-one or non-injective since if a function is not one-one (also one-to-one), then it is many-one. Rational function : Because functions can be expressed as a ratio between two polynomials , we can refer to them as rational functions, which is defined only when the denominator is non-zero. We also call them rational expressions Asymptote : The line a cruve aproaches to as it heads towards infinity. Note that the distance between the line and the curve tends to zero as they head towards positive or negative infinity. Finding the horizontal asymptote of a rational function For the function $h(x) = \frac{2x-3}{x+2}$ , let $h(x)$ be equal to $y$ , we then isolate x , thus we get $x = \frac{2y+3}{2-y}$ , where $y \neq$ 2. Therefore, its horizontal asymptote is 2. By using this method, it may sometimes seem to result that you have two horizontal asymptotes, but graphically, there's only one. A simpler way would be, since the degree of both polynomials are the same, we take the ratio of the leading coefficients as it is, in this case, 2. In a similar manner, the horizontal asymptote of $g(x) = \frac{x-2}{x^{3} - 1}$ is zero since the degree of the denominator is greater than the numerator degree. If you want to use limits, then we just evaluate the limit of the function as it goes to infinity. How to tell if a function is many-one Using the definition given for many-one, a function $f: A \to B$ is many-one if we consider two elements of $A$ , for instance, $a_{1}$ and $a_{2}$ such that setting $f(a_{1})$ and $f(a_{2})$ equal to each other will yield $a_{1}$ $\neq$ $a_{2}$ , otherwise the function is one-one . Thus, $h(x)$ is many-one, whereas $g(x)$ is not. You can also verify this through the horizontal line test , or do the computations. When does a function cross the horizontal asymptote? By definition, an asymptote is approached by the function as $x$ goes to infinity. If we were to consider finite values of x, not values of x that go to infinity, it may cross . By this I mean , $f(x)$ can be equal to the value the horizontal asymptote graphically represents for values of $x$ that do not approach infinity, or this may not happen as well. Because of this posibility, I ask this question, when does a function ""cross"" the horizontal asymptote? Abramson, J. (2020, March 25). Rational functions . Mathematics LibreTexts https://math.libretexts.org/Courses/Borough_of_Manhattan_Community_College/MAT_206.5/04%3A_Polynomial_and_Rational_Functions/4.08%3A_Rational_Functions As you can see, the curve crosses twice the horizontal asymptote. That is, zero is part of the range of the function, $f(x)$ can be equal to zero even though the horizontal asymptote is located at $y = 0$ , which is expected. After all, the definition of an asymptote does only consider values of $x$ that go to infinity. Note that, for other functions, it crosses once. Rational functions: ratios of polynomials . (n.d.). Xaktly. Retrieved April 14, 2020, from https://xaktly.com/MathRationalFunctions.html As I said before, it may also not cross. For instance, consider $h(x)$ . The horizontal asymptote of $h(x)$ is located at $y = 2$ , and since 2 is not part of the range of $h(x)$ , it never crosses. For the below, the function is $\sqrt{\frac{x-1}{x+1}}$ , the horizontal asymptote is 1 (not 1 and -1) Solution For a rational function to cross its horizontal asymptote, the value the horizontal asymptote represents must be part of the range . Thus, if the horizontal asymptote is equal to, let's say, $c$ , then $c$ $\in$ $range$ . Consequently, $h(x)$ does not cross it horizontal asymptote because the range does not include 2. What kind of rational functions cross the horizontal asymptote? If you go back to the examples given, those who cross its horizontal asymptote are many-one functions and those who don't are one-one , which leads to my question. Remark: I'm not considering rational trigonometric functions.",['algebra-precalculus']
4102812,How can one simplify $\sum_{n=1}^8\frac{\sin10n^\circ}{\cos5^\circ\cos10^\circ\cos20^\circ}$?,"I have trouble solving the following question: Consider the following expression: $$\sum_{n=1}^8\frac{\sin10n^\circ}{\cos5^\circ\cos10^\circ\cos20^\circ}$$ The value of the above expression can be fully simplified into the form $a\sqrt b$ . What is $a+b$ ? I know that $$\sin\alpha+\sin\beta=2\sin\left(\frac{\alpha+\beta}2\right)\cos\left(\frac{\alpha-\beta}2\right)$$ but I'm not too sure what pairs of $\sin$ I should apply it on, or if this is even the right approach to this problem. Could someone give me hints for solving this problem(Am I on the right track)?","['algebra-precalculus', 'trigonometry']"
4102830,Abstract idea behind the method of characteristics to solve first-order PDEs,"I'm trying to understand the method of characteristics for a general first-order PDE $$F(\nabla u(x),u(x),x)=0\;\;\;\text{for all }x\in\Omega,\tag1$$ where $F:\mathbb R^d\times\mathbb R\times\overline\Omega$ is sufficiently regular and $\Omega\subseteq\mathbb R^d$ is open. I've tried to understand the motivation/derivation presented in chapter 3.2 of Evans Partial Differential Equations , but it's hard for me to understand the argumentation (and notation). Here's what I understand (but I'm unsure whether it is the correct approach to obtain the ""characteristic equations""): Assume $F\in C^1(\mathbb R^d\times\mathbb R\times\Omega)$ and $u\in C^2(\Omega)$ . Let $$E(x):=(\nabla u(x),u(x),x)\;\;\;\text{for }x\in\Omega$$ and $G:=F\circ E$ . Then $^1$ $G\in C^1(\Omega)$ and $$\nabla G(x)=\nabla^2u(x)\nabla_1F(E(x))+\partial_2F(E(x))\nabla u(x)+\nabla_3F(E(x))\tag2$$ for all $x\in\Omega$ . Now let $M:=\{G=0\}$ and $x_0\in M$ . Assuming that $G$ is a submersion at $x_0$ , the tangent space of $M$ at $x_0$ is fiven by $$T_{x_0}\:M:=\mathcal N({\rm D}G(x_0))\tag3.$$ Let $\gamma:I\to M$ be a $C^1$ -curve on $M$ through $x_0$ for some neighborhood $I$ of $0\in\mathbb R$ . To ease the following, let \begin{align}z&:=u\circ\gamma;\\p&:=\nabla u\circ\gamma;\\ E_\gamma&:=E\circ\gamma;\\ G_\gamma&:=G\circ\gamma=F\circ E_\gamma=F(p,z,\gamma).\end{align} Note that \begin{align}z'&=\langle\gamma',p\rangle;\\ p'&=\nabla^2u(\gamma)(\gamma')\tag4\end{align} Since $\gamma(I)\subseteq M$ , we've got $G_\gamma=0$ and hence \begin{equation}\begin{split}0=G_\gamma'=\langle\gamma',\nabla G(\gamma)\rangle&=\langle\gamma',\nabla^2u(\gamma)\nabla_1F(E_\gamma)+\partial_2F(E_\gamma)p+\nabla_3F(E_\gamma)\\&=\langle p',\nabla_1F(E_\gamma)\rangle+z'\partial_2F(E_\gamma)+\langle\gamma',\nabla_3F(E_\gamma)\rangle\end{split}\tag5\end{equation} by $(2)$ and $(4)$ . But this is the point where I got stuck. Instead of $(5)$ , Evans is deriving $$0=\nabla^2u(\gamma)\nabla_1F(E_\gamma)+\partial_2F(E_\gamma)p+\nabla_3F(E_\gamma)\tag{5'},$$ which can ""formally"" be obtained by inserting $x=\gamma$ into $(2)$ (hence ignoring that $\gamma$ is a function and hence ignoring that the chain rule should be applied). Then he is assuming that $$\gamma'=\nabla_1F(E_\gamma)\tag6$$ , from which we easily obtain the ""characteristic equations"" which Evans presents: \begin{align}p'&=-\partial_2F(E_\gamma)p-\nabla_3F(E_\gamma);\\ z'&=\langle p,\nabla_1F(E_\gamma)\rangle;\\\gamma'&=\nabla_1F(E_\gamma).\tag7\end{align} However, while it's clearly finite to assume $(6)$ and look what we can derive from this, I have no idea how $(5)$ is justified or why it is at least sensible to assume that it holds. And, most importantly , aren't we able to derive a sensible set of ""characteristic equations"" from $(5)$ , which - in contrast to $(5')$ - is an equation which can rigorously be derived (as I have shown) as long as we assume the stated regularity assumptions. $^1$ $\nabla_iF$ denotes the gradient of the map $F$ in the $i$ th argument and I write $\partial_2F$ for the derivative in the scalar second argument of $F$ .","['book-recommendation', 'reference-request', 'partial-differential-equations', 'characteristics', 'differential-geometry']"
4102880,Are there any $f(x)$ whose $\xi_n$ of the Lagrange's remainder does not converge to $0$?,"Consider the Maclaurin's series of an analytic function $f$ with Lagrange's remainder. Define $\xi_n$ as $$f(1)=\sum_{k=0}^{n-1}\frac{f^{(k)}(0)}{k!}+\frac{f^{(n)}(\xi_n)}{n!}$$ where $0<\xi_n<1$ , and if there are several that satisfy this, choose the one closest to $0$ (or the infimum if there are infinitely many). Is there an example of $f$ such that $\xi_n$ doesn't converge to $0$ ? I'm not sure why, but all the ones I tried seemed to go to zero. Edit: $\frac{f^{(n)}(\xi_n)}{f^{(n)}(0)}=1+\frac{1}{n+1}\frac{f^{(n+1)}(\xi_{n+1})}{f^{(n)}(0)}$ , so, if $\frac{f^{(n+1)}(\xi_{n+1})}{f^{(n)}(0)}=o(n)$ then $f^{(n)}(\xi_n)\to f^{(n)}(0)$ . This implies that $\xi_n$ converges to $0$ for some functions such that $e^{ax}, \sin ax$ . From the results of numerical methods for other functions, I suppose that the same is true for all analytic functions.","['limits', 'taylor-expansion', 'real-analysis']"
4102888,Is it possible to get all possible sums with the same probability if I throw two unfair dice together?,"I throw 2 unfair dice, suppose that $p_i$ is the probability that the first die can give an $i$ if I throw it, for $i =1,2,3,..6$ and $q_i$ the probability that the second die can give an $i$ .
If I throw the dice together, is it possible to get all possible sums $2,3,4,...12$ with the same probability? Here's what I've tried so far, the probability that I get a $2$ if I throw both dice is $p_1q_1$ , the probability that I get $3$ is $p_1q_2+p_2q_1$ , and generally the probability that I get $n$ is $$\sum_{i+j=n} p_iq_j$$ where $i=1,2,...6$ , $j=1,2,...6$ . So now in order for all possible sums to appear with the same probability, it must be true that $$p_1q_1=p_1q_2+p_2q_1$$ $$p_1q_2+p_2q_1=p_1q_3+p_2q_2+p_3q_1$$ $$........$$ has a solution, this is where I am stuck I can't find a way to prove that the system above has a solution, can you help?","['dice', 'probability']"
4103066,"If the roots of unity lie on a circle, do arbitrary polynomial roots also lie on some kind of characteristic curve?","I've been learning about roots of unity and how they manifest on the complex plane. I understand that if you take $z^{n}=1$ , then the values of $z$ that satisfy this equation happen to lie on the unit circle with equal angles $\frac{2\pi}{n}$ between them. I tried a couple examples on Wolfram Alpha, here's z^5 = 1 and z^12 = 1 . But I was wondering if a similar strategy could be used for arbitrary complex polynomials, so I decided to tweak the inputs to have more terms. For example, here's z^5 + z^3 = 1 and z^5 - z^3 + z = 1 . The complex plane representations generated by Wolfram Alpha seem to suggest that the solutions might lie on an ellipse instead of a pure circle, or maybe some other characteristic curve. Is this the case?","['complex-analysis', 'soft-question', 'polynomials', 'complex-numbers']"
4103149,PDF of dependent variables,"How to calculate the pdf (and the Expectation) of $f(X, Y)$ when X and Y are continuous random variables and Y depends on X? For example: $$ X∼\operatorname{Uniform}(0,1) \\ Y∼\operatorname{Uniform}(X, 1) \\ f(X, Y) = (Y-X) ^ 2$$","['probability-distributions', 'probability-theory']"
4103166,Proving the existence of the Euler line using methods from Coordinate Geometry.,"I saw a video by Salman Khan, in which he gave a proof of existence of the Euler Line . He proved that the circumcenter , orthocenter and centroid of a triangle are collinear , and used normal geometry to do this. My background is in coordinate geometry, and I want to prove that the Euler line exists, using only methods from coordinate geometry, such as the distance between points formula, the section formulae (internal division), the perpendicular bisector formula , centroid formula and so on. For this, I need to know the coordinates of the circumcenter and the orthocenter , since the coordinates for the centroid are very easy to find and are given by the centroid formula . So, if I consider a triangle $ABC$ with $A \equiv (x_1, y_1)$ , $B \equiv (x_2, y_2)$ and $C \equiv (x_3, y_3)$ coordinates then by using the centroid formula I can find out that the x-coordinate (abscissa) and the y-coordinate (ordinate) of the centroid are following repectively: $$\left(\frac{x_1+x_2+x_3}{3}, \frac{y_1+y_2+y_3}{3}\right) $$ Let's denote the centroid of the triangle $D$ . Now, using perpendicular bisector formula, with some easy steps I got that the coordinates of the circumcenter of the triangle are the followings: $$\left(\frac{-\frac{1}{2}(x_2^{2}+y_2^{2}-x_3^{2}-y_3^{2})(y_1-y_2)+\frac{1}{2}(x_1^{2}+y_1^{2}-x_2^{2}-y_2^{2})(y_2-y_3)}{(x_1-x_2)(y_2-y_3)-(x_2-x_3)(y_1-y_2)}, \frac{-\frac{1}{2}(x_1^{2}+y_1^{2}-x_2^{2}-y_2^{2})(x_2-x_3)+\frac{1}{2}(x_2^{2}+y_2^{2}-x_3^{2}-y_3^{2})(x_1-x_2)}{(x_1-x_2)(y_2-y_3)-(x_2-x_3)(y_1-y_2)}\right) $$ Let's denote the circumcenter of the triangle $E$ . Then finding the equations of the altitudes of triangle $ABC$ , I found out again with some easy steps, that the coordinates of the orthocenter of the triangle are the followings: $$\left(\frac{(y_2-y_1)\{y_1(y_3-y_2)+x_1(x_3-x_2)\}-(y_3-y_2)\{y_3(y_2-y_1)+x_3(x_2-x_1)\}}{(x_2-x_3)(y_1-y_2)-(x_1-x_2)(y_2-y_3)}, \frac{(x_3-x_2)\{y_3(y_2-y_1)+x_3(x_2-x_1)\}-(x_2-x_1)\{y_1(y_3-y_2)+x_1(x_3-x_2)\}}{(x_2-x_3)(y_1-y_2)-(x_1-x_2)(y_2-y_3)}\right)$$ Let's denote the orthocenter of the triangle $F$ . As you can see, The circumcenter and orthocenter has very difficult coordinates in formation which I tried but can not be simplified any further. As you've already seen, to find from coordinate geometry, I need to solve simultaneous equations to find the intersection of two lines whose equations I got from using the vertices of the triangles, and this leads to a this very complicated Cartesian form of coordinates. As part of my textbook, I need to prove that the orthocenter, centroid and circumcenter is collinear and if I denote those points respectively as $F, D, E$ then prove that the ratio of the line segment $FD$ to $DE$ is $2$ to $1$ . That is $$\frac{FD}{DE}=\frac{2}{1}$$ This is basically the gist of the Euler's Line Theorem . I know that the circumcenter of a triangle is the orthocenter of the medial triangle, but this is not helpful for me, maybe you can see why. One online resource for "" What is the formula for circumcentre? "" if that helps but sadly no one gives an explicit formula using only the coordinates of the vertices. Help: Can someone provide a proof of the fact that the circumcenter, centriod and orthocenter are collinear ? And using my coordinates of the Circumcenter, Orthocenter and Centroid using section formula can you prove that $FD : DE = 2:1$ from there? I have been struggling with this, so I would be grateful if someone
help me work it out. Links: https://www.khanacademy.org/math/geometry-home/triangle-properties/triangle-property-review/v/euler-s-line-proof https://www.quora.com/What-is-the-formula-for-circumcentre","['coordinate-systems', 'analytic-geometry', 'geometry']"
4103171,How could I approach $\lim_{n\to\infty}\left(\frac{1+\cos\left(\frac{1}{2^{n}}\right)}{2}\right)^n$?,"So, about the following limit: $$\lim_{n\to\infty} \left( \frac{1+\cos(\frac{1}{2^{n}})}{2} \right)^n$$ I tried several things to evaluate it, namely looking at it as $\cos(\frac{1}{2^{n+1}})^{2n}$ instead or as $\exp(2n \cdot \ln({\cos(\frac{1}{2^{n+1}})})$ and then trying to show that the limit of $n\cdot\ln({\cos(\frac{1}{2^{n+1}})})$ is $0$ (for example using L'Hopital's rule), but I haven't been very successful (though it is possible I gave up too early). I'm just not sure how to approach this cosine-exponential combo. I believe the limit is $1$ , so things like the root test weren't very helpful in this case, either. I'd really appreciate a direction/hint, a full solution, or anything inbetween.","['limits', 'trigonometry', 'exponential-function', 'sequences-and-series']"
4103216,Get $f(x)=u_x\frac{x}{u}$ from ODE for $u$,"Consider the Cauchy-Euler ODE \begin{align*}
\frac{1}{2}x^2u_{xx}+xu_x-u=0.
\end{align*} Guessing $u(x)=Cx^n$ gives \begin{align*}
\frac{1}{2}n(n-1)Cx^n+nCx^n-Cx^n=0,
\end{align*} which we can solve to get \begin{align*}
n_1&=-2,\\
n_2&=1.
\end{align*} Given initial conditions and boundary behavior, we can pin down a unique solution. I'm really interested in the function $$f(x)=u_x\frac{x}{u}.$$ Given the solution for $u$ , we can compute $f$ to be $f(x)=n$ , which is constant! Q: I wonder whether I can find $f$ , without solving the ODE first? Set $f=u_x\frac{x}{u}$ . Then, $u_x=\frac{fu}{x}$ and $u_{xx}=\frac{f_xu}{x}+\frac{fu_x}{x}-\frac{fu}{x^2}$ . The ODE then turns into \begin{align*}
\frac{1}{2}\left(xf_xu+xfu_x-fu\right)+fu-u=0.
\end{align*} Dividing this ODE by $u$ gives \begin{align*}
\frac{1}{2}\left(xf_x+f^2-f\right)+f-1=0.
\end{align*} If I assume $f_x=0$ , then I get \begin{align*}
\frac{1}{2}f^2+\frac{1}{2}f-1=0,
\end{align*} which is a normal quadratic equation with solutions \begin{align*}
f_1&=-2,\\
f_2&=1.
\end{align*} These are precisely the solutions I expected from the previous calculations. However, I had to assume $f_x=0$ . I only knew this because I already knew the solution. Why doesn't the second approach work? Is there a way to compute $f(x)$ without first solving the ODE for $u(x)$ ?","['derivatives', 'ordinary-differential-equations', 'real-analysis']"
4103294,Is there a closed form expression for the eigenvectors of a 2x2 matrix?,"Is there a closed form expression for the eigenvalues/eigenvectors of an arbitrary 2x2 matrix $  \begin{bmatrix}
    a & b \\
    c & d
  \end{bmatrix}
$ ? Wolfram|Alpha tries to provide an expression, but it seems wrong since it produces undefined results when $c = 0$ , but all matrices have at least one (possibly complex) eigenvector. Is there an actual closed form expression for the eigenvectors and eigenvalues of a matrix?","['wolfram-alpha', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'closed-form']"
4103332,"Can we define probability measures $\text P_x$ on $C([0,\infty))$ such that the coordinate process is a Brownian motion started at $x$?","Let $(W_t)_{t\ge0}$ be a continuous Brownian motion on a probability space $(\Omega,\mathcal A,\operatorname P)$ and $$\pi_t:C([0,\infty))\to\mathbb R\;,\;\;\;x\mapsto x(t)$$ for $t\ge0$ . Equip $C([0,\infty))$ with the topology of compact convergence. Then $$\mathcal B(C([0,\infty))=\sigma\left(\pi_t,t\ge0\right)\tag1$$ and hence $W:\Omega\to C([0,\infty))$ is $\left(\mathcal A,\mathcal B(\tau)\right)$ -measurable. So, the process $(\pi_t)_{t\ge0}$ is a Brownian motion on $(C[0,\infty),\mathcal B(C[0,\infty)),\operatorname P_0)$ , where $\operatorname P_0:=\operatorname P\circ\:W^{-1}$ . Now it is tempting to consider for every $x\in\mathbb R$ a separate continuous Brownian motion started in $x$ and obtain probability measure $\operatorname P_x$ on $(C[0,\infty),\mathcal B(C[0,\infty))$ in this way. Question : Are we able to do this such that the map $x\mapsto\operatorname P_x(B)$ is Borel measurable for all $B\in\mathcal B(C([0,\infty))$ ?","['measure-theory', 'stochastic-processes', 'solution-verification', 'brownian-motion', 'probability-theory']"
4103360,About $\cos(\sqrt{-x})$,"By Euler's Formula $e^{ix}=\cos{x}+i\sin{x}$ we can deduce that: $\cos{\sqrt{-x}}=\cosh {\sqrt{x}}$ My question is the following true: $\cos{\sqrt{-x}}=\begin {cases}  
\cos{\sqrt{-x}} &  ,x \text{ is negative real number} \\ 
\cosh {\sqrt{x}} &  ,x \text{ is positive real number}
\end{cases}$ and it is differentiable and continuous at zero. If this is true...is it useful? Read the following to know my level in mathematics: I am second year student of mathematics I know calculus 1+2+3 ,ODES,logic and writing proofs . at my current semester I am studying Abstract Algebra 01 ,Elementary Number Theory ,Introduction to Real Analysis ,PDES 01 and Linear Algebra 01. This question comes to my mind because I love mathematics and I am curious about this idea about $\cos{\sqrt{-x}}$ whether it is true or false ,whether it is useful or useless.","['calculus', 'analysis']"
4103375,"""Asymptotic"" functionals on $C^k(\mathbb{R})$","Let $C^k(\mathbb{R})$ denote the vector space of $k$ -times continuously differentiable functions $\mathbb{R}\to\mathbb{R}$ (with $k\in\mathbb{N}\cup\{0,\infty\}$ ), and $C^k_c(\mathbb{R})\subset C^k(\mathbb{R})$ denote the subspace of compactly supported functions. Let $D^k(\mathbb{R})$ denote the algebraic dual of $C^k(\mathbb{R})$ , i.e. the set of linear maps $C^k(\mathbb{R})\to\mathbb{R}$ . $D^k(\mathbb{R})$ contains familiar functionals such as $f\mapsto f(x)$ and $f\mapsto\int\psi f$ , but it also contains less familiar objects. In particuar, let $A\subset D^k(\mathbb{R})$ denote the subspace of functionals which vanish on $C_c^k(\mathbb{R})$ , or equivalently the kernel of $\iota^*$ , where $\iota:C^k_c(\mathbb{R})\to C^k(\mathbb{R})$ is the inclusion map. It's straightforward enough to show that $A$ is nonempty on algebraic grounds, but nonetheless difficult to describe the set analytically. My question, then, takes one of two forms: Is there an explicit example of a nonzero element $\lambda\in A$ , ideally defined in such a way that one could in principle compute $\lambda(f)$ for a simply defined (e.g. polynomial) $f$ ? Alternately, is there a reason (set-theoretic or otherwise) that $A$ does not contain any ""easily-constructed"" functionals?","['function-spaces', 'linear-algebra', 'functional-analysis', 'set-theory']"
4103405,Prove $\int_{0}^{\pi}\frac{u}{1-\cos u}\ln\frac{1+\sin u}{1-\sin u}{d}u=\left(\pi+2\ln2\right)\pi$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How to prove $$\displaystyle\int_{0}^{\pi}\frac{u}{1-\cos u}\ln\left(\frac{1+\sin u}{1-\sin u}\right)\mathrm{d}u=\left(\pi+2\ln2\right)\pi\,\,?$$ I tried to apply the Feynman method to get the improper integral, but I failed. Now I am stuck in this improper integral. This is not a duplicate problem, I am looking for a proof without using complex analysis.","['integration', 'calculus', 'improper-integrals', 'closed-form']"
4103411,Functions agree in first $n$ derivatives and differ in the $n+1$-th derivative,"Let $f:(a,b)\to \mathbb{R}$ and $g:(a,b)\to \mathbb{R}$ such that there exists $x_0\in (a,b)$ verifying $f(x_0)=g(x_0)$ and $$\frac{d^k}{dx^k}f(x_0)=\frac{d^k}{dx^k}g(x_0)$$ for $k=1,\ldots ,n$ and $$\frac{d^{n+1}}{dx^{n+1}}f(x_0)\neq \frac{d^{n+1}}{dx^{n+1}}g(x_0).$$ Prove that: If $n$ is odd then there exists $\delta >0$ such that $f(x)-g(x)$ does not change sign for $x\in (x_0-\delta ,x_0+\delta )$ . If $n$ is even then for every $\delta >0$ there exists $x_1,x_2\in (x_0-\delta ,x_0+\delta )$ such that $f(x_1)-g(x_1)<0<f(x_2)-g(x_2)$ . It is true for $f(x)=x^m$ and $g(x)=x^{m+1}$ with $x_0=0$ , so I have the feeling this should be true for every function verifying the hypothesis, but I don't know how to prove it in general.","['derivatives', 'real-analysis']"
4103415,Rewrite/simplify a trigonometric expression (electrical AC),"I'm trying to rewrite the trig expression: Problem $\frac{1}{2}\left ( \cos( \theta -\gamma) +\cos (2\omega t+\theta +\gamma ) \right )=....\text{should become....}=\frac{1}{2}\left [ \cos( \theta -\gamma)(1+ \cos (2\omega t+2\theta))   + \sin( \theta -\gamma)\sin (2\omega t+2\theta) \right ]$ My attempt to solve it: If I assume $\alpha=\omega t+\theta  \quad and \quad \beta=\omega t +\gamma$ then $\alpha -\beta=\theta -\gamma \text{  and  } \alpha +\beta=2\omega t+\theta +\gamma  $ So I can use the trig forumla: $\cos (\alpha +\beta)=\cos (\alpha)\cos (\beta)-\sin(\alpha)\sin(\beta)$ From the first row: $ \frac{1}{2}\left ( \cos( \alpha -\beta) +\cos (\alpha +\beta) \right )=\frac{1}{2}\left ( \cos( \alpha -\beta) + \cos (\alpha)\cos (\beta)-\sin(\alpha)\sin(\beta) \right )$ But I get stuck here, I am supposed to get the $\sin(2\alpha)$ somehow?","['trigonometry', 'calculus', 'algebra-precalculus']"
4103435,What is the easiest way to determine the list of all strings up to length N accepted by DFA or regular expression?,"For example I have the next simple regular expression: (11|0)+ It is clear that size of the set of strings that match with this regular expression is infinite: 011,110,11011, 11011011... The size of a set of all binary strings with length less then or equal 8 is N=2^8+2^7... How to determine the size of subset of the strings that match with provided regular expression? How to get all these strings? What is the general algorithm for any regular expression / DFA to determine such subset?","['automata', 'regular-expressions', 'discrete-mathematics', 'regular-language']"
4103469,Is it possible to define a coordinate system with 3 angles and no radii?,"In Cartesian coordinates, a point or vector is defined with an x distance or magnitude, a y distance or magnitude, and in 3 dimensions, a z distance or magnitude. In 2 dimensional polar coordinates, a point or vector is defined by a radius and an angle. In 3 dimensions, we have cylindrical coordinates and spherical coordinates. In cylindrical coordinates, we have a vertical distance for a point or vector, and an angle and radius for the ""circle"" in the cylinder. In spherical coordinates, we have two angles and one radius. But is it possible for a coordinate system to be defined with three angles and no radius?","['coordinate-systems', 'geometry']"
4103475,Good resource for differential equation solution approximation in python?,"I'm a business operations guy. I've come to really enjoy reading up on the math, stats, and coding that comes with the models/tools that we use. I specify this so that you have a practical frame of reference: I'm not a physics undergrad student who needs to solve ODEs and/or PDEs by hand for class. Rather, I'm interested in (A) Numerical approximations via python to ODEs/PDEs and (B) examples of the sort of problems they can solve. What originally perked my interest was usage of Hamiltonian Monte Carlo based samplers, which use differential equations to inform a dynamic path across probability distributions. (HMC is implemented in PyMC3, so I don't need to implement this myself.) Anyway, I did further reading and learned that differential equations are also used heavily in ecology and economics. The potential applications seem limitless and so I'd like to get some exposure to A & B (above.) Any recommendations on books and or courses would be appreciated!","['python', 'ordinary-differential-equations', 'reference-request', 'partial-differential-equations', 'numerical-methods']"
4103558,General form of Hunt lemma,"Consider on a filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_n)_n,P),$ a sequence of random variable $(X_n)_n$ converging a.s to $X.$ $(Y_n)_n$ is a sequence of random variable in $L^1$ , converging a.s to $Y \in L^1,$ such that $|X_n| \leq Y_n$ and $E[Y_n|\mathcal{F}_{\infty}]$ converges a.s to $E[Y|\mathcal{F}_{\infty}],$ where $\mathcal{F}_{\infty}=\sigma(\bigcup_{n \in \mathbb{N}}\mathcal{F}_n).$ Prove or disprove the following: $E[X_n|\mathcal{F}_n]$ converges a.s to $E[X|\mathcal{F}_{\infty}].$ $|E[X_n|\mathcal{F}_n]-E[X|\mathcal{F}_{\infty}]| \leq E[|X_n-X||\mathcal{F}_n]+|E[X|\mathcal{F}_n]-E[X|\mathcal{F}_{\infty}]|, E[X|\mathcal{F}_n]$ converges a.s to $E[X|\mathcal{F}_{\infty}],$ it remains to prove that $E[|X_n-X||\mathcal{F}_n]$ converges a.s to $0,$ it seems it's a version of the dominated convergence theorem. Any suggestions ? Remark: If $E[Y_n|\mathcal{F}_{n}]$ converges a.s to $E[Y|\mathcal{F}_{\infty}],$ where $\mathcal{F}_{\infty}=\sigma(\bigcup_{n \in \mathbb{N}}\mathcal{F}_n),$ then $E[X_n|\mathcal{F}_n]$ converges a.s to $E[X|\mathcal{F}_{\infty}].$ $|E[X_n|\mathcal{F}_n]-E[X|\mathcal{F}_{\infty}]| \leq E[|X_n-X||\mathcal{F}_n]+|E[X|\mathcal{F}_n]-E[X|\mathcal{F}_{\infty}]|.$ $E[X|\mathcal{F}_n]$ converges a.s to $E[X|\mathcal{F}_{\infty}].$ We can prove that $E[|X_p-X||\mathcal{F}_p]$ converges a.s to $0$ by writing for all $r,k \in \mathbb{N}^*,p \geq k,$ $$E[|X_p-X||\mathcal{F}_p] \leq \frac{1}{r}+E[(Y+Y_p)1_{\{|X_p-X|>\frac{1}{r}\}}|\mathcal{F}_p] \leq\frac{1}{r}+E[Y1_{\bigcup_{q \geq k}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_p]+E[Y_p1_{\bigcup_{q \geq k}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_p]$$ So $$E[|X_p-X||\mathcal{F}_p] \leq\frac{1}{r}+E[Y1_{\bigcup_{q \geq k}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_p]+E[Y_p1_{\bigcup_{q \geq k}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_p]-E[\min(r,Y_p)1_{\bigcup_{q \geq k}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_p]+E[\min(r,Y_p)1_{\bigcup_{q \geq k}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_p]$$ Hence, $$E[|X_p-X||\mathcal{F}_p] \leq\frac{1}{r}+E[Y1_{\bigcup_{q \geq k}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_p]+E[Y_p|\mathcal{F}_p]-E[\min(r,Y_p)|\mathcal{F}_p]+E[\min(r,Y_p)1_{\bigcup_{q \geq k}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_p]$$ Take $p \to \infty:$ $$\limsup_p E[|X_p-X||\mathcal{F}_p] \leq \frac{1}{r}+E[Y1_{\bigcup_{q \geq k}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_\infty]+E[Y|\mathcal{F}_{\infty}]-E[\min(Y,r)|\mathcal{F}_{\infty}]+E[\min(r,Y)1_{\bigcup_{q \geq k}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_\infty],$$ Taking $k \to \infty $ and noticing that $P(\limsup_q\{|X_q-X|>\frac{1}{r}\})=0$ we obtain: $$\limsup_p E[|X_p-X||\mathcal{F}_p] \leq \frac{1}{r}+E[Y1_{\limsup_{q}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_\infty]+E[Y|\mathcal{F}_{\infty}]-E[\min(Y,r)|\mathcal{F}_{\infty}]+E[\min(r,Y)1_{\limsup_{q}\{|X_q-X|>\frac{1}{r}\}}|\mathcal{F}_\infty] \leq \frac{1}{r}+E[Y|\mathcal{F}_{\infty}]-E[\min(Y,r)|\mathcal{F}_{\infty}]$$ To conclude by letting $r \to \infty.$ Additional facts: If $W_k$ is an increasing sequence of positive random variable, $W=\sup_k W_k$ then $$\lim_k E[W_k|\mathcal{F}_k]=E[W|\mathcal{F}_{\infty}]$$ since $E[W_k|\mathcal{F}_k] \leq E[W|\mathcal{F}_k]$ so that $\limsup_k E[W_k|\mathcal{F}_k] \leq E[W|\mathcal{F}_{\infty}]$ $(E[W|\mathcal{F}_k]$ is a positive supermartingale converging to $E[W|\mathcal{F}_{\infty}]).$ Also $$\forall q,k, E[\min(W_k,q)|\mathcal{F}_k] \leq E[W_k|\mathcal{F}_k]$$ so for all $q$ $$E[\min(W,q)|\mathcal{F}_{\infty}] \leq \liminf_k E[W_k|\mathcal{F}_k]$$ then we let $q \to \infty$ From the previous point we can prove the following version of Fatou lemma: $$E[\liminf_kW_k|\mathcal{F}_{\infty}] \leq \liminf_k E[W_k|\mathcal{F}_k]$$ which holds for any sequence $(W_k)_k$ of positive random variable. The above exercise can be proved using the previous point $($ take $W_k=|X|+Y_k-|X_k-X|)$ More generally if $(U_k),(V_k),$ are sequence in $L^1$ , converging a.s to $U \in L^1$ and $V \in L^1$ respectively, $(W_k)_k$ is such that $U_k \leq W_k \leq V_k,$ $W_k$ converges a.s to $W,$ $E[U_k|\mathcal{F}_k],E[V_k|\mathcal{F}_k]$ converges a.s to $E[U|\mathcal{F}_{\infty}]$ and $E[V|\mathcal{F}_{\infty}]$ respectively then $E[W_k|\mathcal{F}_k]$ converges a.s to $E[W|\mathcal{F}_{\infty}].$ All the results remain true if $\mathcal{F}_k$ are decreasing All the results remain true if we have convergence in probability instead of a.s convergence (by taking subsequences)","['measure-theory', 'stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
4103562,Why do we care about Cauchy principal value?,"Question is in the title, basically. I don't understand the motivation behind assigning the Cauchy principal value to otherwise divergent integrals. I'm more comfortable with things like Abel summation that assign values to divergent series, because in my (limited) experience, all the reasonable ways you can do this lead to the same values. But Cauchy p.v. isn't compatible with substitutions, which are normally one of the fundamental tools for evaluating integrals. So why do we care about Cauchy principal value?","['complex-analysis', 'cauchy-principal-value']"
4103564,"On the equivalence relation $(a,b) \sim (c,d)\iff a+b=c+d$","Setup Let $A=\{a_1<a_2<\cdots<a_p\}$ and $B=\{b_1<b_2<\cdots<b_q\}$ be two finite sets of real numbers. Define an equivalence relation $\sim_{A,B}$ on $R_{p,q}=\{1,\dots,p\}\times\{1,\dots,q\}$ by setting $$(i,j)\sim_{A,B}(k,l)\iff a_i+b_j=a_k+b_l$$ and further define a total order on the set of equivalence classes $R_{p,q}/\sim_{A,B}$ by setting, for any equivalence classes $c,d$ and representatives $(i,j)\in c$ and $(k,l)\in d$ $$c<_{A,B}d\iff a_i+b_j<a_k+b_l$$ This question is about characterizing such equivalence relations $\sim_{A,B}$ and total orders $<_{A,B}$ . An equivalence relation $\sim$ on $R_{p,q}$ for which there exist $A,B$ with $\sim\;=\;\sim_{A,B}$ is called realizable . If $\sim$ is realizable, a total order $<$ on $R_{p,q}/\sim$ is called realizable if we can find $A,B$ with $\sim\;=\;\sim_{A,B}$ and $<\;=\;<_{A,B}$ . In both cases we say that the pair $(A,B)$ realizes $\sim$ (resp. $(\sim,<)$ .) Problem 1. Characterize realizable equivalence relations on $R_{p,q}$ . Problem 2. Suppose $\sim$ is realizable, can one find $\mathcal{A},\mathcal{B}\subset\Bbb{N}$ with $\sim\;=\;\sim_{\mathcal{A}, \mathcal{B}}$ ? Problem 3. Characterize realizable orders on $R_{p,q}/\sim$ (where $\sim$ is realizable.) Have these questions been considered / solved somewhere? Motivation As suggested in the comments I should give some context. The question arose while solving Problem 3.5 in David Eisenbud's Commutative Algebra with a View Toward Algebraic Geometry . In this problem one considers a graded ring $R=\bigoplus_{\gamma\in\Gamma} R_\gamma$ and a graded module $M=\bigoplus_{\lambda\in\Lambda}M_\lambda$ where $(\Gamma,+,0,<)$ is a totally ordered abelian monoid acting freely and compatibly on the totally ordered set $(\Lambda,<)$ . When working out the solution to this exercise I found myself drawing rectangles $R_{p,q}=\{1,\dots,p\}\times\{1,\dots,q\}$ and writing $\gamma_i+\lambda_j$ at position $(i,j)$ , where I had picked some subsets $\{\gamma_1<\gamma_2<\cdots<\gamma_p\}\subset\Gamma$ and $\{\lambda_1<\lambda_2<\cdots<\lambda_q\}\subset\Lambda$ . I was trying to figure out when $R_{\gamma_i}\cdot M_{\lambda_j}$ and $R_{\gamma_k}\cdot M_{\lambda_l}$ lie in the same $M_{\lambda}$ i.e. what and one say about the equivalence relation on $R_{p,q}$ defined by $(i,j)\sim(k,l)\iff\gamma_i+\lambda_j=\gamma_k+\lambda_l$ ? For concreteness I considered $\Gamma=\Lambda=\Bbb{N}$ or $\Gamma=\Lambda=\Bbb{R}_+$ but the problem makes sense in the broader context described by Eisenbud. I realized that there were some nice pictures to be drawn and that there was a not so obvious combinatorial problem of independent interest which is Problem 1 . Problem 2 arose when comparing the cases $\Bbb{N}$ and $\Bbb{R}_+$ : do we recover the same equivalence relations? Or would some not be realizable using integers? Problem 3 arose when drawing not only the blobs representing equivalence classes but the total order between them. What constraints must such orders satisfy? Necessary conditions and proposed characterizations While thinking about these problems I made some progress and was able to identify some necessary conditions for realizability of $\sim$ and $<$ . I wouldn't call any of these ""conjectures"" per se ... Is there an acceptable phrase for ""half-assed guesses""? Using terminology introduced below I wonder Observation and Guess 1. Any realizable equivalence relation $\sim$ on $R_{p,q}$ satisfies Inclination , Non Crossing and both Antidiagonal Propagation conditions. Conversely, are these conditions enough to characterize realizability of $\sim$ ? I'm not sure these conditions alone will do the job ... but they may. It seems in that for $p=2$ these conditions are sufficient (with an algorithm for finding explicit sets $A,B$ of rational numbers realizing $\sim$ .) Using terminology introduced below, there are some simple necessary Visibility conditions satisfies by realizable orders. There are also some less obvious but still simple Horizontal and Vertical Coherence conditions that such orders must satisfy. Observation and Guess 2. Any realizable order satisfies Visibility and both Horizontal and Vertical Coherence conditions. Conversely, are these enough to characterize realizability of $<$ ? Examples Here are some simple examples representing equivalence classes of $\sim_{A,B}$ and the order relation $<_{A,B}$ . In the pictures below we represent equivalence classes as ""blobs"" and signify $c<_{A,B}d$ by drawing an arrow from one equivalence class to another. Example 1. Let $A=\{0<1<2<\cdots<p-1\}$ and $B=\{0<p<2p<\cdots<(q-1)p\}$ . The equivalence classes of $\sim_{A,B}$ are singletons (this is uniqueness of euclidean division). Example 2. Let $A=\{0<1<2<\cdots<p-1\}$ and $B=\{0<1<2<\cdots<q-1\}$ . The equivalence classes of $\sim_{A,B}$ are antidiagonals. The answer to Problem 2 may be positive. Something along the lines of ""the condition is open and as long as one moves in tandem the things that are correlated (i.e. related by $\sim_{A,B}$ ) one can change coordinates to rational ones"" ought to ""prove"" the assertion. Maybe results from real algebraic geometry and semi-rational sets of degree 1 could also answer the question, but I know nothing about this material. Once one has rational $A$ and $B$ one can multiply everything by some integer and get the desired integer $\mathcal{A}$ and $\mathcal{B}$ . There may be a connection with configuration spaces. There certainly is a connection with geometry (existence of rational points on a rational algebraic set) and hyperplane arrangements but there, too, I'm not quite sure how to make it precise. Notation Given an equivalence class $c$ for $\sim_{A,B}$ we define its sum to be $a_i+b_j$ for any $(i,j)\in c$ . Given $(i,j)\in R_{p,q}$ we define the vertical set $V_i=\{(i,a)\mid a=1,\dots,q\}$ and the horizontal set $H_j=\{(a,j)\mid a=1,\dots,p\}$ . Given an equivalence class $c$ we define two subsets of the rectangle: $c^+$ (the green shaded region in  the picture below) is the set of all $(i,j)$ such that there exists some $(a,b)\in c$ with $(a,b)\neq(i,j)$ and $a\leq i$ and $b\leq j$ and $c^-$ (the orange shaded region in  the picture below) is the set of all $(i,j)$ such that there exists some $(a,b)\in c$ with $(a,b)\neq(i,j)$ and $a\geq i$ and $b\geq j$ .
In the picture below the blue dots represent an equivalence class, the green region is $c^+$ and the orange region is $c^-$ . Inclination and Non Crossing (necessary conditions for realizability of $\sim$ ) The following are simple necessary conditions on an equivalence relation $\sim$ on $R_{p,q}$ for it to be of the form $\sim_{A,B}$ : Inclination. Two equivalent elements may not have the same first or second coordinate, i.e. any intersection $c\cap V_i$ and $c\cap H_j$ contains at most one element. Non Crossing. Given distinct equivalence classes $c,c'$ , the equivalence class $c'$ may not intersect both $c^+$ and $c^-$ . The necessity of both conditions is clear (the sum of an equivalence class meeting $c^+$ (resp. $c^-$ ) is greater (resp. smaller) than that of $c$ ). Actually the first condition is redundant since it is contained in the second. These conditions alone, however, don't characterize equivalence classes of the form $\sim_{A,B}$ , see the counter example below. Constraints and counter example Some configurations of equivalence classes have a constraining effect on the rest of the equivalence classes. Consider for example the case where we have an antidiagonal and subantidiagonal equivalence class (say with $p=q=n+1\geq 3$ ) i.e. $(1,n)\sim_{A,B}(2,n-1)\sim_{A,B}\cdots\sim_{A,B}(n,1)$ and $(1,n-1)\sim_{A,B}(2,n-2)\sim_{A,B}\cdots\sim_{A,B}(n-1,1)$ . Then $(i,j)\sim_{A,B}(k,l)\iff i+j=k+l$ . This implies that some equivalence relations are not of the form $\sim_{A,B}$ even though they satisfy the Inclination and Non Crossing conditions. For example the one depicted below: These can't represent the equivalence classes of a relation $\sim_{A,B}$ : the presence of the $3$ element equivalence classes would impose the relations $(1,3)\sim_{A,B}(2,2)$ and $(2,7)\sim_{A,B}(3,6)$ (both represented in blue on the right). We thus get a third necessary condition: Suppose $A'=\{a_1',\dots,a_r'\}\subset A$ and $B'=\{b_1',\dots,b_r'\}\subset B$ for $r\geq 3$ satisfy either that $(1,r-1)\sim_{A',B'}(2,r-2)\sim_{A',B'}\cdots\sim_{A',B'}(r-1,1)$ and $(1,r-2)\sim_{A',B'}(2,r-3)\sim_{A',B'}\cdots\sim_{A',B'}(r-2,1)$ or the analoguous condition with the antidiagonal above the principal antidiaongal. Then for all $1\leq i,j,k,l\leq r$ , $(i,j)\sim_{A',B'}(k,l)\iff i+j=k+l$ . It turns out we can improve on this condition. Antidiagonal Propagation (necessary condition for realizability of $\sim$ ) There are two versions of antidiagonal propagation: the one described below where we consider a main antidiagonal and one below it, but one should add the other possibility too, where the smaller antidiagonal is above the larger one. Antidiagonal Propagation. Suppose $A=\{a_1<a_2<\cdots<a_n\}$ , $B=\{b_1<b_2<\cdots<b_n\}$ with $n=mT+r$ with $m+1\geq 3$ , $T\geq 1$ , $\newcommand{\T}{[\![1,T]\!]}r\in\T$ . Suppose $$\newcommand{\n}{[\![1,n]\!]}
\forall i,j\in\n,
\quad
\begin{cases}
i+j=n+1:& a_i+b_j=C\\
i+j=n+1-T:& a_i+b_j=D
\end{cases}$$ Then $$\newcommand{\n}{[\![1,n]\!]}
\forall i,j,k,l\in\n,
\quad
\left\{
\begin{array}{l}
i\equiv k\mod T,\\
j\equiv l\,\mod T,\\
\text{and }i+j=k+l
\end{array}
\right\}
\implies (i,j)\sim_{A,B}(k,l)$$ In other words if $(i,j)$ and $(k,l)$ lie on the same antidiagonal and are a multiple of $T$ positions apart then they are $\sim_{A,B}$ -equivalent. Proof. The condition implies that, setting $\Delta=C-D$ , $$
\left\{
\begin{array}{lll}
a_{1+kT}=a_1+k\Delta, & b_{1+kT}=b_1+k\Delta & ~\text{for }k=0,1,\dots,m-1,m\\
a_{2+kT}=a_2+k\Delta, & b_{2+kT}=b_2+k\Delta & ~\text{for }k=0,1,\dots,m-1,m\\
\qquad\vdots&\qquad\vdots&\qquad\vdots\\
a_{r+kT}=a_r+k\Delta, & b_{r+kT}=b_r+k\Delta & ~\text{for }k=0,1,\dots,m-1,m\\[2mm]\hline
a_{r+1+kT}=a_{r+1}+k\Delta, & b_{r+1+kT}=b_{r+1}+k\Delta & ~\text{for }k=0,1,\dots,m-1\\
\qquad\vdots&\qquad\vdots&\qquad\vdots\\
a_{T+kT}=a_T+k\Delta, & b_{T+kT}=b_T+k\Delta & ~\text{for }k=0,1,\dots,m-1
\end{array}
\right.
$$ and thus if $i\equiv k\mod T$ , say $i=IT+\tau$ , $k=KT+\tau$ , $j\equiv l\mod T$ , say $j=JT+\sigma$ , $l=LT+\sigma$ , and $i+j=k+l$ i.e. $I+J=K+L$ , then $$
a_i+b_j
=
a_\tau+b_\sigma+(I+J)\Delta
=
a_\tau+b_\sigma+(K+L)\Delta
=
a_k+b_l
$$ Here is an illustration. You have two large equivalence classes: You focus on the lines and columns that contain these equivalence classes and chuck the others out for now. Then there are some automatic equivalences depicted below. Equivalence classes are always subsets of antidiagonals, i.e. sets of the form $\{(i,j)\mid i+j=\mathrm{cst}\}$ and are color coded there. For extra clarity I've also added some markings to some of the equivalence classes. Funnily these look like designs straight out of the 1960ies and 1970ies. Visibility (necessary condition for realizability of $<$ ) There is an obvious necessary condition for a total order on $R_{p,q}/\sim$ , where $\sim$ is realizable, to be realizable. Suppose $c,d$ are equivalence classes. We say that $d$ is visibly greater than $c$ if there exists some $i$ with $V_i$ intersecting both $c$ and $d$ and such that if $(i,a)\in c$ and $(i,b)\in d$ then $a<b$ , or if there is some $j$ with $H_j$ intersecting both $c$ and $d$ and if $(a,j)\in c$ and $(b,j)\in d$ then $a<b$ . We allow ourselves to say that $c$ is visibly greater than $d$ if there is a chain $c=c_0,c_1,\dots,c_n=d$ with $c_i$ visibly greater than $c_{i-1}$ (i.e. we consider the transitive closure of the previously defined relation). Visibility. If $c$ is visibly greater than $d$ then $c<d$ . Visibility is a necessary property satisfied by realizable $<$ but it is not enough. Counter Example for realizability of $<$ There's more to this, yet. Consider the following discrete equivalence relation (i.e. equivalence classes are singletons) and total order $<$ . This total order satisfies all previously proposed conditions but can't arise as a $\sim_{A,B}$ and $<_{A,B}$ . Here's a simpler counter example showing that the individual $3\times 3$ orders are realized. Horizontal and Vertical Coherence (necessary conditions for realizability of $<$ ) The previous example shows that Visibility is not enough to characterize realizable orders. A problem arose where traversing too many horizontal gaps when following $<$ along equivalence classes led to inconsistencies in the values of those gaps. There are some horizontal and vertical consistency conditions that a realizable $<$ must satisfy. Let's start with Horizontal Consistency conditions. Take $(i,j)$ with $1<i$ . The sum of the classes containing $(i-1,j)$ and $(i,j)$ differ by $(\Delta a)_i=a_i-a_{i-1}$ . If we follow the $<$ -path from the $\sim$ -class containing $(i-1,j)$ to that containing $(i,j)$ we get $$(\Delta a)_i=\text{sum of positive terms, some $(\Delta a)_{k}$, some $(\Delta b)_l$}$$ from which we extract a condition $$H_{ij}:(\Delta a)_i>\text{sum of some $(\Delta a)_k$ and some $(\Delta b)_l$}$$ where the $(\Delta a)_k$ and $(\Delta b)_l$ to take into account are found using visibility relations. Similarly there are Vertical Consistency conditions of the same ilk $$V_{ij}:(\Delta b)_j>\text{sum of some $(\Delta a)_k$ and some $(\Delta b)_l$}$$ where again, the $(\Delta a)_k$ and $(\Delta b)_l$ that appear on the right hand side are deduced from visibility relations along the $<$ -path of equivalence classes linking the $\sim$ equivalence class containing $(i,j-1)$ to the $\sim$ equivalence class containing $(i,j)$ Beginnings of a positive solution for $p=2$ I believe the answer is positive to both questions at least when $p=2$ . WLOG we can take $a_1=0$ , $a_2=1$ and $b_0=0$ . There seems to exist an explicit algorithm to find a rational solution to the problem. I've illustrated it below. The necessary and sufficient condition on $\sim$ in the case $p=2$ seems to be that the lines associated to $2$ -element equivalence classes don't intersect. The algorithm is simple but I haven't conceptualized it really: You start with $b_1=0$ , the sum in the the lower left corner $(1,1)$ is thus $0$ since $a_1=0$ You put $1$ in the lower right corner $(2,1)$ , the sum is $1$ since $a_2=1$ If the lower right corner is part of a $2$ element equivalence class you put are forced to attribute $1$ to the other member $(1,r)$ . if $r>2$ you set $b_r=1$ . This is the case in the example below so you set $b_3=1$ . Then you are forced to put a $2$ in position $(2,r)$ since $a_2=1$ . You keep on going until you don't fall into a 2 element equivalence class. Then you have to insert values for $b_2,\dots,b_{r-1}$ . You pick them uniformly apart between $b_1=0$ and $b_r=1$ , that is $b_k=\frac{k-1}r$ . Then you are forced to inscribe $b_k+1$ on the right hand side. If the right hand side is part of a $2$ element equivalence class you are forced to set the same value for some new $b_s$ etc ... I hope the picture below (and the color coded rounds) are more explicit than the half-baked algorithm above. I don't know if this will work for $p\geq 3$ . A connection with topology $\newcommand{\O}{\mathcal{O}}\newcommand{\R}{\Bbb{R}}$ Fix $\sim$ an equivalence relation on $R_{p,q}$ and $<$ a total order on $R_{p,q}/\sim$ . Define $$\O_{p,q}[\sim,<]=\{(A,B)\in\O_p\times\O_q\text{ inducing $\sim$ and }<\}$$ where $\O_n=\{(x_1,\dots,x_n)\in\R^n\mid x_1<x_2<\cdots<x_n\}$ . Then $\O_{p,q}[\sim,<]$ is nonempty iff $(\sim,<)$ is realizable. When it is nonempty it is a convex cell. The $\O_{p,q}[\sim,<]$ partition $\O_p\times\O_q\simeq \R^{p+q}$ . It would be interesting to understand the poset structure associated to containment of closures of these. It seems natural to expect: the poset structure is closely related to refinements of $(\sim,<)$ ; maximal elements correspond to the discrete equivalence relation with the compatible total orders; $(\sim_{A,B},<_{A,B})$ for $A=\{1<2<\cdots<p\}$ and $B=\{1<2<\cdots<q\}$ is minimal but there may be other minimal elements. Let us put $\newcommand{\real}{\mathfrak{R}}\real_{p,q}$ the set of realizable pairs $(\sim,<)$ . We define an order relation on $\real_{p,q}$ by setting $$(\sim,<)\prec(\sim',<')\iff\text{there is an onto map of posets }
(R_{p,q}/\sim',<')\to(R_{p,q}/\sim,<)$$ Lemma. Let $(\sim,<),(\sim',<')\in\real_{p,q}$ be realizable. The following are equivalent: $\overline{\O_{p,q}[\sim,<]}\subset\overline{\O_{p,q}[\sim',<']}$ $\O_{p,q}[\sim,<]\subset\overline{\O_{p,q}[\sim',<']}$ $\O_{p,q}[\sim,<]\cap\overline{\O_{p,q}[\sim',<']}\neq\emptyset$ $(\sim,<)\prec(\sim',<')$ Sketch of Proof. $1$ and $2$ are always equivalent and clearly imply $3$ . If we let $(A_n',B_n')\in\O_{p,q}[\sim',<']$ tend to $(A,B)\in\O_{p,q}[\sim,<]$ then what can happen is fusion of $<'$ -intervals of $\sim'$ -equivalence relations and $<$ and $<'$ have to be compatible which amounts to the existence of an onto homomorphism. This will prove $3\implies 4$ . To finish the proof we show that $4\implies 2$ . It is enough to notice that if $(\sim,<)\prec(\sim',<')$ and $(A,B)\in\O_{p,q}[\sim,<]$ and $(A',B')\in\O_{p,q}[\sim',<']$ then for all $t\in[0,1)$ , $$\qquad\qquad\qquad(tA+(1-t)A', tB+(1-t)B')\in\O_{p,q}[\sim',<'].\qquad\qquad\qquad\square$$ This has desirable homotopical consequences: one can use these cells to compute homotopy colimits of subcategories of the poset $(\real_{p,q,\prec})$ if one ever wanted to do so.","['configuration-space', 'equivalence-relations', 'additive-combinatorics', 'combinatorics', 'discrete-mathematics']"
4103611,Finding the maximum of the entries of matrix $A B^T$ without computing $A B^T$,"Given tall matrices $A, B \in \mathbb{R}^{n \times k}$ , where $n \gg k$ , let $C := AB^T$ . I want to calculate $$ \max C = \max AB^T$$ where $\max: \mathbb{R}^{n\times n} \rightarrow \mathbb{R}$ returns the maximal entry in a matrix. Is there a way to calculate the max element without explicitly calculating $C$ ? This is beneficial in case $n$ is large. Background This problem arises when solving LASSO for large scale data. Basically you choose an active set (i.e., parameters that are not zero and then you set the others to zero) which is relatively small to the number of parameters you want to optimize over. You optimize over the active set using whatever algorithm and then you check the optimality condition for the zero coefficients, which is defined as the maximal element in the multiplication of the residual matrix with the data matrix is less than the regularization parameter. Multiplication the residual (i.e., $A$ ) with the data (i.e., $B$ ) is the computational bottleneck.","['matrices', 'optimization', 'discrete-optimization', 'algorithms']"
4103612,Lefschetz Hyperplane Theorem's original proof,"I'm trying to understand the main ideas used in the original proof
by Lefschetz of his Hyperplane theorem. Here it is sketched shortly
(source: Here )
and I want to fill the gaps: Let $X$ be an $n$ -dimensional complex projective algebraic variety closed embedded in $\mathbb{CP}^n$ and let $Y$ be a hyperplane section of $X$ such that $U = X ∖ Y$ is smooth. Lefschetz used his idea of a Lefschetz pencil to prove the theorem.
Rather than considering the hyperplane section $Y$ alone, he put it into a
family of hyperplane sections $Y_t$ , where $Y = Y_0$ . Because a generic
hyperplane section is smooth, all but a finite number of $Y_t$ are smooth
varieties. After removing these points from the $t$ -plane and making an
additional finite number of slits, the resulting family of hyperplane sections
is topologically trivial. That is, it is a product of a generic $Y_t$ with
an open subset of the $t$ -plane. $X$ , therefore, can be understood if
one understands how hyperplane sections are identified across the slits
and at the singular points. Away from the singular points, the identification
can be described inductively. At the singular points, the Morse lemma
implies that there is a choice of coordinate system for $X$ of a particularly
simple form. This coordinate system can be used to prove the theorem directly. The original source is 'L'Analysis situs et la géométrie algébrique'
but I can't get along with that text. So I would be happy if someone
could help me to clear up the understanding problems. That's, what I understand so far: We take any arbitrary pencil $\{Y_t \}_{t \in \mathbb{CP}^1}$ of hyperplanes in $X$ with $Y_0=Y$ (that is what we really do is
we take a family $\{H_t \}_{t \in \mathbb{CP}^1}$ of hyperplanes in $\mathbb{CP}^n$ such that for every member $H_t$ the intersection $X \cap H_t$ is a $n-1$ -dimensional variety with is for almost all $t$ smooth and we
set $Y_t:=  X \cap H_t$ . Then we consider the canonical map $\{Y_t \}_{t \in \mathbb{CP}^1} \to \mathbb{CP}^1$ ,
remove all $t_i$ which have singular fibers (only finitely many) and make some
marvelous cuts $S$ in the projective line $\mathbb{CP}^1$ making the the
restriction $\{Y_t \}_{t \in \mathbb{CP}^1 ∖ S} \to \mathbb{CP}^1 ∖ S$ trivial. Although it isn't explicitely explained I think that it is reasonable to assume
that the cuts a done in that way that $\mathbb{CP}^1 ∖ S$ becomes simply connected, so the fibration becomes trivial.
Is that true? Are the cuts in the original proof indeed done in same spirit like in
the original construction of Riemann surfaces, to make the base simply connected? So we consider now $\{Y_t \}_{t \in \mathbb{CP}^1 ∖ S}= Y_0 \times (\mathbb{CP}^1 ∖ S)$ . Next the
identifications along the slits are concretely performed. The text refers
to an inductive argument. How is then the induction step done? Next Morse theory tells us how $\{Y_t \}_{t \in \mathbb{CP}^1}$ looks like
in singular points, where we take $\{Y_t \}_{t \in \mathbb{CP}^1} \to \mathbb{CP}^1$ as Morse function, right? Well, but finally I not understand how this procedure helps to
reconstruct $X$ (or say better it's homology groups) from these
of $Y$ resp $\{Y_t \}_{t \in \mathbb{CP}^1}$ . What was the advantage in this
construction to work with Lefschetz pencil and why does it contribure a progress in the
computation of the map $H_k(Y) \to H_k(X)$ ?","['complex-geometry', 'fiber-bundles', 'fibration', 'algebraic-geometry', 'algebraic-topology']"
4103684,Sum of an infinite series $\sum _{k=3}^{\infty }\:\frac{1}{k\left(k^4-5k^2+4\right)^2}$,"I want to compute this series, $$\sum _{k=3}^{\infty }\:\frac{1}{k\left(k^4-5k^2+4\right)^2}.$$ I don't know how to contiune after factoring, which yields $\frac{1}{k(k-1)^2(k+1)^2(k-2)^2(k+2)^2}.$ I know that $\frac{1}{k(k-1)}$ telescopes. However, I can't apply this here, as the product of the sum is not equal to the sum of the product. I only want hints for now.","['algebra-precalculus', 'sequences-and-series']"
4103776,What is the proper convention regarding the order of operations of a fractional exponent and/or the simplification of it?,"Specifically, consider the example $\sqrt[4]{x^2}$ . The answer of course would be $\sqrt{|x|}$ since the x is squared first. However if converted to the exponential fraction of $x^{2/4}$ , you lose the information of which came first, so you could end up with the wrong answer in this case. In fact, one might simplify it to $x^{1/2}$ which would simply be $\sqrt{x}$ which is NOT equal to $\sqrt{|x|}$ . Is there a proper convention for dealing with this? Or is the takeaway that you should be careful when simplifying a function into a fractional exponent? The reason this is important to me is because I'm tutoring a student taking college algebra and I want to be 100% correct in my explanation of this sort of problem. He encounters many problems where he is asked to find the domain of functions and if he's ever given this type of scenario I need to know what to tell him.","['exponentiation', 'algebra-precalculus', 'radicals']"
4103818,"Why define $\bigcup_{n\in I} A_n=\emptyset,\bigcap_{n\in I} A_n=\Omega$ when $I$ is the empty set [duplicate]","This question already has answers here : Empty intersection and empty union (7 answers) $\bigcup \emptyset$ is defined but $\bigcap \emptyset$ is not. Why? (4 answers) intersection of the empty set and vacuous truth (3 answers) Closed 3 years ago . In the textbook I used on Probability and measure theory , a definition catches my eye: If $I=\emptyset$ , we define $\bigcup_{n\in I} A_n=\emptyset,\bigcap_{n\in I} A_n=\Omega$ I didn't draw attention to this definition initially, but this definition puzzles me a lot now. I don't get what it means to map an empty index set to a collection of a subset of $\Omega$ . I also don't understand why the author would bother talking about the scenario where the index set is empty.","['elementary-set-theory', 'definition']"
4103885,Generalizations of Poisson's Equation,"I'm currently reading a book in Multivariable Calculus, and there is a section on Applications of Calculus to Physics - Poisson's Equation. It states the following: We have the 3D version: Let $$u(x) = \iiint_{\Bbb{R}^3}\frac{p(x+y)}{|y|}d^3y$$ , where $p$ is a $C^2$ function on $\mathbb{R}^3$ , that vanished outside a bounded set. Then $u$ is of class $C^2$ and $\nabla^2u = -4\pi p$ . The 2D analog: Let $u(x) = \int p(x+y)\log{|y|}d^2y$ , where $p$ is a $C^2$ function on $\mathbb{R}^2$ , that vanished outside a bounded set. Then $u$ is of class $C^2$ and $\nabla^2u = 2\pi p$ . I was wondering how we could generalize this to 4 dimensions - or even n dimensions (if its possible). My conjecture for the 4D version is: Let $$u(x) = \iiiint_{\Bbb{R}^4}\frac{p(x+y)}{|y|^2}d^4y$$ , where $p$ is a $C^2$ function on $\mathbb{R}^4$ , that vanished outside a bounded set. Then $u$ is of class $C^2$ and $\nabla^2u = -4\pi^2 p$ . Is this conjecture correct? If not, what would the correct analog be, and if so, how could we prove it?","['poissons-equation', 'multivariable-calculus', 'real-analysis']"
4103939,Find the general solution for $t^2y'' -2y = t^2 -1$,"The question already gives $y_1=t^2$ and $y_2=t^{-1}$ as solutions to the homogeneous equation. So the homogeneous equation is: $y_h = c_1t^2 + c_2t^{-1}$ So we need to find the particular solution, we have a polynomial as the RHS of the EDO, so our $y_p$ will be $y_p = At^2 + Bt + C$ $\Rightarrow y_p'= 2At + B$ $\Rightarrow y_p''= 2A$ We substitute our findings in the EDO: $t^2(2A) - 2(At^2+Bt+C) = t^2 - 1$ $-2Bt - 2C = t^2 - 1$ $\Rightarrow -2B = 0, -2C = -1$ $\Rightarrow B = 0, C = 1/2, A = ???$ The part of the LHS that was multiplying A cancelled itself, so we can't determine A. What should I do in this situation?","['calculus', 'ordinary-differential-equations']"
4103944,Changing of variable in a distribution defined by a convolution,"Let $f$ , $g$ , and $h$ denote probability distributions such that $$f(x) = \int_0^x g(x')h(x-x')~dx'$$ where $x$ only takes non-negative values. Now, suppose we want to perform a change of variable from $x$ to $y=q(x)$ in the above equation. My question is: how does the convolution equation change when we perform the change of variables. In other words, after changing the variable to $y$ , suppose the distributions corresponding to $f(x)$ , $g(x)$ , and $h(x)$ are $F(y)$ , $G(y)$ , and $H(y)$ , then can we write the equation satisfied by $F$ , $G$ , and $H$ ? Take for example $y=\log x$ . Here is my attempt:
We can see that $F(y) = e^yf(e^y)$ where $F(y)$ denotes the distribution of $y$ obtained by performing a change of variable $x$ to $y$ in $f$ . Similarly, we define $y' = \log x'$ , and write $G(y') = e^{y'}g(e^{y'})$ . Now we come to the the part where I am facing some difficulty: dealing with $h(x-x')$ . Approach: Define $z = x-x'$ , and $w = \log z$ . Now, $h(z) = H(w)e^{-w}$ , and substituting $w = \ln(e^y - e^{y'})$ allows us to write $$h(x-x') = \frac{H(\ln(e^y - e^{y'}))}{e^y-e^{y'}}$$ This would mean, our original convolution can now be written as: $$e^{-y}F(y) = \int_{-\infty}^{e^y} e^{-y'}G(y')\cdot\frac{H(\ln(e^y - e^{y'}))}{e^y-e^{y'}}~ e^{y'} dy'$$ which gives $$F(y) = \int_{-\infty}^{e^y} e^{y}~G(y')\cdot \frac{H(\ln(e^y - e^{y'}))}{e^y-e^{y'}}~ dy'.$$ Here, I have literally written each term in the first equation in terms of the corresponding distribution of $y$ . Does this seem correct? If not, can someone point me in the right direction?","['change-of-variable', 'convolution', 'probability']"
4103953,Why care about homotopy equivalence up to $\vee S^2$ or $\# (S^2 \times S^2)$?,"I have read that in the study of 4-manifolds (specifically, their classification up to homotopy) that a popular/useful notion of equivalence is that up to a wedge of 2-spheres or connected sum of $S^2\times S^2$ . That is, when for two smooth 4-manifolds $M,N$ , we have for some $r,s$ $$M\vee_r S^2 \cong N \vee_sS^2 \,~ \text{ or } ~\, M\#_r(S^2 \times S^2) \cong N\#_s(S^2 \times S^2).$$ My questions: why is this important? What are some of the useful results supposing this notion of equivalence? Are these notions related, and do they generalize in some way? I know of, for instance, the following fact which follows from Donaldson's theorem: Smooth, simply-connected 4-manifolds are homotopy equivalent iff their connected sums with $S^2 \times S^2$ are homotopy equivalent.","['cobordism', 'manifolds', 'general-topology', 'differential-topology', 'algebraic-topology']"
4104000,how prove this $\sum_{x=1}^{p-1}\left(\frac{1+x^4}{p}\right)\equiv 4\pmod 8$,"Today, I read  a book saying it is easy to show this result: Let $k$ be postive integer,and prime number $p=8k+1$ , show that $$\sum_{x=1}^{p-1}\left(\dfrac{1+x^4}{p}\right)\equiv 4\pmod 8$$ where $\left(\dfrac{\cdot}{p}\right)$ is  Legendre symbol mod $p$ I know that $\sum_{y\in\mathbb{F}_p} \left(\dfrac{y}{p} \right)=0$ , but how  do show this? This is from a competition question. It's easy to prove the following conclusion,At present, middle school only needs to know elementary number theory, but algebraic number theory and analytic number theory do not need to master, because I am a middle school teacher","['contest-math', 'number-theory']"
4104032,The role of non-negativity in Fatou's lemma,"Fatou's lemma requires that $f_{n}$ be non-negative. I know that there exist counterexamples such as $f_{n} = \mathbb {-1}_{[n,n+1]}$ which show that the non-negativity condition is critical. However, I don't know what role this constraint comes into play in the proof of this lemma. Where is it needed? Here is the proof: Let $g_{k}:=inf_{n \geq k} f_{n}$ , then $\lim_{n} inf f_{n} = \uparrow \lim g_{k}$ . For $n \geq k$ , $\begin{align}
f_{n} \geq g_{k} \Longrightarrow \int f_{n} \, d\mu \geq \int g_{k} \, d\mu \Longrightarrow \inf_{n \geq k} \int f_{n} \, d\mu \geq \int g_{k} \, d\mu
\end{align}
$ . Therefore, $
\begin{align}
\int \lim 
\inf f_{n} \, d\mu = \uparrow \lim_{k} \int g_{k} \, d\mu \leq \uparrow \lim_{k} \inf_{n \geq k} \int f_{n} \, d\mu = \lim_{n} \inf \int f_{n} \, d\mu
\end{align}
$ , where in the first equation, we use Monotone Convergence Theorem.","['measure-theory', 'probability-theory', 'probability']"
4104036,Why do the partial sums of the Maclaurin series expansion of $\sin$ approximate it better than their hyperbolic counterparts approximate $\sinh$?,"While exploring Taylor series with numerical and graphing tools, I noticed a very peculiar and interesting fact: the partial sums of the Maclaurin series expansion of $\sin(x)$ approximate it better than their hyperbolic counterparts approximate $\sinh(x)$ . To be specific, I noticed that for every $x\in\mathbb{R}$ and $k\in\mathbb{N}$ , the following inequality seems (haven't proven it) to hold $$\left|\sin(x)-\sum_{n=0}^{k}(-1)^{n}\frac{x^{2n+1}}{(2n+1)!}\right|\leq\left|\sinh(x)-\sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!}\right|$$ For all $x\neq 0$ , the inequality is strict, showing that $\sum_{n=0}^{k}(-1)^{n}\frac{x^{2n+1}}{(2n+1)!}$ does indeed approximate $\sin(x)$ better than $\sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!}$ approximates $\sinh(x)$ . But why do they provide better approximations? The remainders $$\sin(x)-\sum_{n=0}^{k}(-1)^n\frac{x^{2n+1}}{(2n+1)!},\text{ }\sinh(x)-\sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!}$$ obviously can't be equal everywhere, but given the similarity between the partial sums for each function, it's unclear to me why the $\sin$ remainder should be bounded by an inequality as ""uniform"" as the one above. Why isn't it the other way around, with $\sinh$ 's remainder being bounded by $\sin$ 's? Why is the inequality true for all $x\in\mathbb{R}$ , and not just some disjoint intervals? Looking at the partial sums for each series, it can be seen that the only difference is the $(-1)^n$ factor for the $\sin$ 's series, which doesn't seem like the kind of thing that can make the difference between $$\left|\sin(x)-\sum_{n=0}^{k}(-1)^{n}\frac{x^{2n+1}}{(2n+1)!}\right|\leq\left|\sinh(x)-\sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!}\right|$$ and some other inequality relating the two remainders. Can someone give an intuitive explanation for why this happens?","['calculus', 'taylor-expansion', 'sequences-and-series']"
4104070,"Are there nonisomorphic, finitely generated groups that surject onto each other? [duplicate]","This question already has answers here : Groups $G,H$ with surjective homomorphisms that are quotient of each other (3 answers) Closed 3 years ago . Are there nonisomorphic, finitely generated groups that surject (homomorphically) onto each other? For example of the 'dual' problem, the free group on $2$ generators $F_2$ , and the free group on $3$ generators $F_3$ inject into each other; but there is no surjection from $F_2 \to F_3$ (intuitively, if there was was, since $F_3$ surjects onto $\mathbb{Z}^3$ , there would therefore be a surjection $F_2 \to \mathbb{Z}^3$ , but the image of such a map could only be a $2$ -dimensional sublattice of $\mathbb{Z}^3$ , so we have a contradiction)",['group-theory']
4104077,a mean value theorem question [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question if $f(x)$ can be differentiated to any degree for $x\in (0,+\infty)$ and $f'(x)>0,f''(x)<0$ ，if $0<a<b$ ，acordding to mean value theorem we have $\displaystyle\exists \xi\in(a,b),st.\frac{f(b)-f(a)}{b-a}=f'(\xi)$ ,
prove: $\displaystyle \xi<\frac{a+b}{2}$ And I think that's pretty obvious by looking at the graph of the function,but i don't know how to prove it in math words.","['functions', 'analysis', 'real-analysis']"
4104090,Generalized Hardy-Ramanujan Sum [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I am looking for a proof of the following fabulous identity by Hardy and Ramanujan $$\sum_{n = 1}^{\infty}\dfrac{1}{n^{2q - 1}}\left(a^{2q - 2}\coth\dfrac{n\pi b}{a} + (-1)^qb^{2q - 2}\coth\dfrac{n\pi a}{b}\right) \\ = \dfrac{2}{\pi ab}\sum_{k = 0}^{q}(-1)^{k - 1}\zeta(2k)\zeta(2q - 2k)a^{2q - 2k}b^{2k}$$ I'm most interested in a proof making use of complex analysis, Mellin transforms, Infinite series. However any approach is most welcomed. Thanks.","['complex-analysis', 'alternative-proof', 'sequences-and-series']"
4104104,$k(k^n - (k-1)^n) = k^{n+1}-(k-1)^{n+1}-(k-1)^n$,"In Feller's Intro to Probability Theory Vol. 1. there is a step I don't know how the author proceded. You have the full source here In first place we have (you can ignore the $N^n$ ): $$N^n p_k = k^n - (k-1)^n $$ Now we have: $$E(X) = \sum_{k = 1}^Nkp_k$$ $$ = N^{-n}\sum_{k = 1}^N \{k (k^n - (k-1)^n)\} = N^{-n}\sum_{k = 1}^N \{k^{n+1}-(k-1)^{n+1}-(k-1)^n\}$$ Which is followed by the question: $$k(k-1)^n \overset{?}{=} (k-1)^{n+1}+(k-1)^n$$ This algebra is the argument for an approximation he later does on the same page number, however I don't believe the last steps he made are correct.","['discrete-mathematics', 'polynomials', 'probability', 'factoring']"
4104109,Derivative of squared frobenius norm of hadamard product of outer product of vector with itself and matrix w.r.t. vector,"I know the title is a mouth full, and there have been many similar (and probably more complicated) questions/answers on this site, but I'm stuck on this specific problem. I am working with the following function: $$f(x) = \frac{1}{2}||xx^{T} \circ Y||_{2}^{2}$$ where $x \in \mathbb{R}^{n}$ , $Y \in \mathbb{R}^{n \times n}$ and $\circ$ denotes the hadamard (element wise) product. I would like to compute the following gradient: $$\frac{\partial f(x)}{\partial x} = \frac{\partial }{\partial x} \frac{1}{2}||xx^{T} \circ Y||_{2}^{2}$$ The furthest I've gotten is w.r.t. $xx^{T}$ , following this : $$\frac{\partial f(x)}{\partial xx^{T}} = xx^{T} \circ Y$$ but I do not know how to actually compute this w.r.t. $x$ . I am pretty inexperienced with this kind of thing, so any input would be really helpful. Thank you!","['matrices', 'hadamard-product', 'matrix-calculus', 'derivatives']"
4104213,Basis Extension Theorem,"I know that using basis extension theorem we can extend a set of LI vector to the basis of vector space, but how we actually do it? I have only theoretically used this theorem in proof, how to actually extend a given set of LI vectors of a vector space to its basis? Thanks.","['linear-algebra', 'vector-spaces']"
4104217,What means integrating against the heat kernel,"I got this advice for calculating a partial differential equation with drift, that said I should integrate the approximation of the previous time step against the heat kernel. Now I wonder, what does it mean to integrate a function against the heat kernel, as I originally thought integrating $f$ against $g$ means $\int f dg$ which does not make sense to me in this case.","['integration', 'heat-equation', 'partial-differential-equations']"
4104220,Multiplier Operator Norm Estimate,"I'm using Muscalu and Schlag's textbook to study harmonic analysis by myself, where they give the following exercise about multipliers' estimate: Consider a sequence of complex numbers $\{m_{n}\}_{n \in \mathbb{Z}}$ that satisfies: $$\sum_{n \in \mathbb{Z}}|m_{n}-m_{n-1}| \leq B, \ \lim_{n \rightarrow -\infty}m_{n} = 0$$ where $B > 0$ . Now let's define a multiplier operator $T$ : $$Tf(x)= \sum_{n \in \mathbb{Z}}m_{n}\hat{f}(n)e^{2\pi inx}$$ Then for any trigonometric polynomial $f$ and any $p \in (1,\infty)$ , there exists some constant $C_p > 0$ , such that: $$||Tf||_{L^p} \leq C_{p}B||f||_{L^p}$$ Any ideas on this? What I have tried is to try generalizing the claim to functions in $L^p$ . However, it seems that I can't find a proof or a counter-example for the general case....","['measure-theory', 'fourier-analysis', 'harmonic-analysis', 'normed-spaces', 'inequality']"
4104284,What is $e^{\frac d{dx}}$?,"At the end of this video , 3blue1brown suggests it is possible to take $e^{\frac d{dx}}$ . So, what does $e^{\frac d{dx}}$ equal?","['exponential-function', 'popular-math', 'derivatives', 'functional-analysis']"
4104328,Ambiguity when finding an Inverse trigonometric derivative [duplicate],"This question already has an answer here : What is wrong with this substitution? (1 answer) Closed 3 years ago . The question I was solving was to find the derivative of $y = \arcsin(2x\sqrt{1-x^2}), -\frac{1}{\sqrt{2} } <x< \frac{1}{\sqrt{2}}$ Substituting $x = \cos\theta \Rightarrow \theta = \arccos (x)$ , I got $y = \arcsin(\sin2\theta) = 2\theta$ Differentiating, I got $\frac{-2}{\sqrt{1-x^2}}$ However, substituting $x=\sin\theta$ , simplifying and then differentiating, I got $y' = \frac{2}{\sqrt{1-x^2}}$ , which is the negative of what I'd gotten earlier. Also, this is the correct answer according to my textbook. Both substitutions lead to $2x\sqrt{1-x^2}$ turning into $\sin2\theta$ , so why is the substitution $x = \sin\theta $ more valid than the substitution $x = \cos\theta$ ?","['calculus', 'derivatives', 'trigonometry', 'substitution']"
4104385,What is the outward normal vector at the end points of the one-dimensional line?,"In the one-dimensional space, suppose the interval $[a, b]$ exists on the real line with boundary points $a$ and $b$ . What is the outward normal vector at the point $a$ and at the point $b$ ? The answer is $-1$ for $a$ and $1$ for $b$ . But I cannot understand why so. Usually, the outward normal vector is perpendicular to the line.
But in this case, aren't they tangent to the line itself? I can't digest what's ""normal"" in the $1$ -D case. I can kind of get that the interval ends at $a$ and at $b$ and so the outward direction is to $-1$ and to $1$ , respectively...but is there a mathematical way to express this in the $1$ -D case?","['integration', 'orthonormal', 'geometry']"
4104459,Boundaries of a definite Integral,"I'm not a math major, and I've found myself in need of urgent help dealing with the following integral, pls. $$ \int_{-2}^1 \frac{1}{x^2} dx $$ It looks simple at first except the anti derivative becomes undefined at $x=0$ . So how do you deal with integrals like this? Also is there any book that deals w integrals like this?","['integration', 'definite-integrals', 'calculus', 'indefinite-integrals', 'algebra-precalculus']"
4104488,"Is max the only binary function that is idempotent, commutative, associative and satisfies $f(x,y)\geq x$?","Is $f(x,y)=\max(x,y)$ with $x,y\in\mathbb{Z}$ the only binary operation on the set of integers that satisfies following properties? $f(x,f(y,z))=f(f(x,y),z)$ (associativity) $f(x,y)=f(y,x)$ (commutativity) $f(x,x)=x$ (idempotency) $f(x,y)\geq x$ If not, which other properties are needed to get the max function as the only solution?",['functions']
4104530,Are two homeomorphic hypersurfaces of the same smooth manifold also diffeomorphic?,"Let $M$ be a smooth (connected, without boundary) manifold and $N_1$ , $N_2$ be two smooth (connected, without boundary) hypersurfaces of $M$ . Suppose $N_1$ and $N_2$ are homeomorphic. Can $N_1$ and $N_2$ be non-diffeomorphic? I am currently working on a problem where I have shown that two smooth, compact hypersurfaces $N_1$ and $N_2$ of the same manifold $M$ are both homeomorphic (even $C^{\alpha}$ -homeomorphic for some $\alpha \in (0,1)$ ) to the same manifold $N$ . However, I would like to use some differential properties of both $N_1$ and $N_2$ and it would be suitable that they have the same differential structure. I know there exists many examples of non-diffeomorphic manifolds which are homeomorphic, such as exotic spheres. However, I don't know if one can realize two different differentiable spheres as hypersurfaces of the same smooth manifold.","['submanifold', 'differential-geometry']"
4104599,When does $(x+1)^s -x^s = x^{-s}$ have a solution?,"Let $0\lt s \lt 1$ . I want to find the range of $s$ for which the following equation has a solution for $x\ge 0$ . $$(x+1)^s -x^s = x^{-s} $$ After looking at the graphs of the LHS and RHS and Wolfram, I believe that if $s$ is greater than some unknown value, there is always one solution. For example, WolframAlpha gives no solutions for $s=0.5$ but one for $s=0.6$ . How can I obtain this transition point? Does its closed-form even exist? I might add an idea. At that special value of $s$ , the two curves must touch each other, giving the system $$(x+1)^s -x^s = x^{-s} \\ (x+1)^{s-1} -x^{s-1} = -x^{-s-1} $$ Maybe there’s a way to solve for $s$ .","['calculus', 'functions', 'roots']"
4104624,Find number of sequences of 7 letters such that every letter is equal to previous or next,"Let $a_n$ be such sequence of $7$ letters, such that every letter is equal to previous or next letter in that sequence. My idea is to find recursive formula for $a_n$ and then to write generating function for $a_n$ . From that I can find general formula for $a_n$ .
I have two cases: – If $a_{n-1}$ ends with two same letters, then I can put any letter at $n$ -th position. – If $a_{n-1}$ ends with two different letters, then at $n$ -th position I can put only one letter (same as at $(n-1)$ -th position). So I have recursive formula: $a_n = 7 \cdot (\text{number of sequences $a_{n-1}$ such that two last letters are the same}) + (\text{number of sequences $a_{n-1}$ such that two last letters are different} )$ I don't where to go from here. Thanks in advance for any clues.","['combinatorics', 'discrete-mathematics', 'recursion']"
4104672,"A (geometric?) limit distribution for the ""delicacy width"" of prime numbers?","[Revised to ask only the second of my two previous questions. Revised again to include the approximate Geometric distribution, and to add another reference.] A recent Quanta article refers to this paper by Filaseta & Southwick, which is behind a paywall. (EDIT: Presumably, Southwick's doctoral dissertation has similar content with no paywall.) The paper has the following Abstract (italics are mine): Abstract : We show that a positive proportion of the primes have the property that if any one of its digits in base $10$ , including its infinitely many leading $0$ digits, is replaced by a different digit, then the resulting number is composite . Primes having the italicized property are called widely digitally delicate in the article, which mentions that no example of such a prime has yet been found , in spite of there being infinitely many of them (indeed, a positive proportion). I take the Abstract to mean that $$P_\infty:=\lim_{n\to\infty}{\pi^*(n)\over\pi(n)}>0 $$ where $\pi()$ is the usual prime counting function and $\pi^*(n)$ counts the number of primes less than or equal to $n$ and satisfying the italicized property. Now, suppose we define the delicacy width (which I'll just call the ""width"") of a prime as follows: Definition : The width of a prime $p$ whose decimal expansion is $\ldots d_2 d_1 d_0$ ( including infinitely many leading $0$ s ) is the length of the longest suffix in $\ldots d_2 d_1 d_0$ with the property that changing any single digit in the suffix changes the expansion to that of a composite number. This defines the width as a function $W:PRIMES\to\mathbb{N}\cup\{\infty\},$ with $W(p)=\infty$ when the longest such suffix is the entire expansion of $p$ ; otherwise, $W(p)$ is finite and equal to the number of digits to the right of digit $r$ , where $r$ is the rightmost digit that can be altered to change the prime into different noncomposite number. Examples: All primes less than $97$ have width $0$ , because in each case the very rightmost digit (having no digits to its right) can be altered to change the prime into another noncomposite; e.g., $89$ can be changed into the noncomposite $83.$ On the other hand, $97$ has width= $1$ because the $9$ (having one digit to its right) is the rightmost digit that can be altered (to $0,1,3,4,$ or $6$ ) to change the prime into a different noncomposite -- altering the $7$ cannot accomplish this. Question : Since the quoted Abstract implies that a positive proportion ( $P_\infty$ ) of the primes have infinite width, does it follow that the remaining primes have widths $(0,1,2,\ldots)$ occurring in respective proportions $(P_0, P_1, \ldots)$ that sum to $1-P_\infty?$ Can it be concluded that all of these proportions are positive? In other words, does there exist a limit distribution $\{P_w\}_{w\in\mathbb{N}\cup\{\infty\}}$ with $(P_0+P_1+P_2+\ldots)+P_\infty=1,$ where $$P_k:=\lim_{n\to\infty}{\pi_k(n)\over\pi(n)}>0 \quad\text {for all $k\in\mathbb{N}\cup\{\infty\}$}$$ and $\pi_k(n)$ counts the number of primes that are less than or equal to $n$ and have width equal to $k$ ? Here are plots I've computed of the distribution of widths among the first $10^n$ primes, for $n=6,7,8,9(\text{blue})$ -- there seems to plenty of ""converging"" yet to occur, if convergence is indeed what happens: The greatest widths occurring in those respective cases are $14, 16, 24,$ and $31,$ although the plots show only up to width= $8$ . (Elsewhere, the widest prime I've found is $142243533671$ , with width= $59$ . As mentioned above, no one has yet found a prime of the kind described in the Abstract, i.e. a prime of infinite width.) To get some inkling of what happens for much larger primes and yet keep the computing time feasible, I also looked at the width distribution among the first $10^6$ primes after each number of form $10^n,$ for $n=8,10,12,...,30(\text{blue})$ -- the greatest width to occur overall was $53$ , but I plot only up to width= $15$ : Approximate Geometric Distribution Examining these plots numerically, it appears that among sufficiently large primes, the distribution of positive integer widths (i.e. $0<W<\infty$ ) is approximately geometric . That is, the width distributions appear to be approximately as follows (taking $P_0, P_1,$ and $P_\infty$ as parameters): $$P_k =\begin{cases}
P_0  &\text{if }k=0\\
P_1\,a^{k-1} &\text{if }1\le k\lt\infty\\
P_\infty &\text{if }k=\infty 
\end{cases}$$ where $a=1-{{P_1\over 1-P_0-P_\infty}}$ in order to make $P_0+(P_1+P_2+\ldots)+P_\infty=1.$ Here's a comparison of the empirical distribution for $n=30$ (blue) from the preceding picture, and a geometric distribution (red) fitted with $P_0=0.09833$ and $P_1=0.2354$ to match the empirical values, and taking $P_\infty=10^{-9}$ just as some small number (known to be positive even though no instance of infinite width has been found among more than $10^9$ primes). At the scale of the plot, the two distributions visually overlap: NB : The width of a prime can be seen as the outcome of a sequence of ""trials"" on the successive digits $d_0,d_1,d_2,\ldots$ , each trial asking ""Can this digit be changed so as to change the expansion into that of another noncomposite number?"" -- if the answer is ""yes"", then the trial is a ""success"", otherwise it's a ""failure"". The width is then just the number of failures before an eventual success (if there is never a success, then the width is infinite). Empirically, it appears that only trials after $d_0$ (i.e. only the trials on $d_1,d_2,\ldots$ ) can be modelled as ""Bernoulli trials"", and so give rise to a geometric distribution only for $0\lt W\lt\infty.$","['number-theory', 'probability-distributions', 'asymptotics', 'limits', 'prime-numbers']"
4104709,"Find $x$ such that $(ax)^{bx}>c$, where $a,b,c,x>0$","Let $a>0$ , $b>0$ , and $c>0$ . Let $$x_0\triangleq \inf\{x>0:{\rm for~all~} \bar x>x, (a\bar x)^{b\bar x}>c\}.$$ What is a good estimate (least-conservative estimate) for $x_0?$","['logarithms', 'calculus', 'algebra-precalculus', 'supremum-and-infimum', 'exponential-function']"
4104716,Find the slope of the line such that the area of triangle formed inside the circle is maximum,"Here is the question. It states: A circle of radius 1 unit touches positive x  -axis and positive y  -axis at A  and B  respectively. A  variable line passing through origin intersects the circle in two points D  and E . If the area of the triangle DEB is maximum when the slope of the line is $m$ , then the value of $\frac{1}{m^2}$ is When I tried plotting its graph it came out to be something like this: Now When I thought, I Felt like the triangle with maximum area should be an equilateral triangle.
So from the center of the circle I joined a line to B which makes 90 with the x axis. This line should also bisect the angle of the equilateral triangle so formed. Hence If I assume point D to be the point intersecting the circle on left. Then the line DB should have a slope $\tan(120) = -\sqrt{3}$ . This line shall make and angle 60 with the line DE, so I calculated that and the slope comes out to be $m = 0$ However the answer to $\frac{1}{m^2}$ is $3$ and I am not able to understand how. Any hint/suggestions are appreciated. Thanks for your time","['analytic-geometry', 'circles', 'geometry', 'slope', 'optimization']"
4104761,Wasserstein Metric: Question About Empirical Computation,"I am trying to estimate the Wasserstein distance between two empirical distributions for which I only have two sets of data which effectively create two histograms. I am reading here that the computation for this 1-dimensional case where we want to compute $W_p(P,Q)$ when $p=1$ then the formula is: $$W_1(P,Q) = \sum_{i=1}^n |X_i - Y_i|$$ where we have $n$ data points and $X_i$ and $Y_i$ come from the data sets representing $P$ and $Q$ respectively. My question here is: isn't the order of the $X_i$ and $Y_i$ important? I am envisioning two Gaussians with the same distribution, $P=Q \implies W_1(P,Q)=0$ but the data is the same but order is reversed: $X_i = Y_{(n+1)-i}$ . This empirical estimate will be non-zero. I am just asking here because there is no mention of the order of the data but it does seem important. I am trying to compute some of these metrics and I'm wondering if I should first sort the data somehow.","['probability-distributions', 'probability']"
4104779,A very confusing statistics problem from a high school math student,"I have just made an account to ask this question, since nobody has been able to clear up my confusion as of yet. I am a high school student so if I am using incorrect terminology or notation I do apologize for any confusion that may occur. Say that one were to roll two fair, 6 sided dice, such that each number 1-6 has an equal chance of appearing. After the first roll, the sum of the presenting faces is recorded on a table  as 'N', the 1st term in a sequence. To continue the sequence, the dice are rolled N amount of times. There are two possibilities during this 'rolling period:' If during this rolling period a sum of N appears before the final roll, the remaining rolls are discarded and the number N is recorded as the next term in the sequence. If during this rolling period a sum of N does not appear at any time, the sum of the faces on the Nth roll are recorded as the next term in the sequence. I have two questions here about the resulting sequence from this procedure. 1: If the sequence is continued to the 10th term, what is the probability that the sequence consists of all 7s? 2: Now assume that N was determined to be 7, and is not included in the resulting sequence. The rest of the procedure is unchanged. If the sequence is continued infinitely, and each term is placed into a bar graph recording the prevalence of each possible sum, would the graph present a standard bellcurve? I came up with this situation and both questions while sitting in a Adv. Algebra 2/Trig class. I have solved the first question (I think), however I am both unable to answer the second question and very curious about the answer. If anybody wants to take a crack I'd be interested in seeing how this situation is approached. If anything is unclear or confusing let me know so that I can clarify.","['statistics', 'dice', 'probability']"
4104788,Definition of expectation for non-real-valued random variables,"I have checked many sources for the definition of expectation for non-real-valued random variables. I am interested on the conditions of the image space that guarantee the existence of an expectation operator. For example, I have seen definitions of expectation for complex-valued and real-vector-valued random variables, and these work ""component by component"". Question. I am wondering if there is a general definition of expected value that asks for some conditions about the image space of the random variables and captures all image spaces for which expected values make sense. So far I have only concluded that the image space must necessarily support addition and scalar multiplication to be able to define expectation for random variables with finite set of possible outcomes, i.e. $\sum_{i=1}^k P(X=x_i) x_i$ . Some other conditions that come to my mind are being a measure space, a metric space and being complete, but I am not sure how are they necessary and if they are sufficient.","['expected-value', 'vector-spaces', 'probability-theory']"
4104873,Convergence in distribution and convergence of expectations,"Let $(X_n)$ a sequence of complex valued random variables and $X_n \to X$ in distribution. Portemanteau lemma stipulates that for any $f$ continuous and bounded, $\mathbf{E}\left[f(X_n)\right] \to \mathbf{E}\left[f(X)\right]$ , but this doesn't include $\mathbf{E}\left[X_n\right] \to \mathbf{E}\left[X\right]$ , which is not always true. Now if we assume that $(X_n)$ are uniformly bounded by some constant $M$ , does it work? It seems that by taking $f: x \in \mathbb{C} \mapsto x\mathbf{1}_{|x| \leq M} + Msign(x)\mathbf{1}_{|x| \gt M}$ , then $f$ is bounded and continuous and $\mathbf{E}\left[f(X_n)\right] = \mathbf{E}\left[X_n\right]$ , hence the result. Is this proof somehow flawed? I am reading a proof which arrives at the same conclusion ( $\mathbf{E}\left[X_n\right] \to \mathbf{E}\left[X\right])$ with the same boundedness assumption and using the dominated convergence theorem, but it isn't clear to me how the DCT is used with convergence in distribution.","['measure-theory', 'lebesgue-measure', 'probability-distributions', 'probability-theory', 'probability']"
4104985,Calculating method of moments estimators for exponential random variables,"I'm trying to find the method of moment estimators for $\sigma$ and $\tau$ . I have the i.i.d. exponential random variables $X_1, \dots, X_n$ with the density functions $$f(x; \sigma, \tau)=
\begin{cases}
 \dfrac{1}{\sigma} e^{-(x - \tau)/\sigma} &\text{if}\, x\geq \tau\\
      0 &\text{otherwise}
\end{cases}$$ I know that $E(X)=\sigma + \tau$ and $E(X^2)=2\sigma^2+2\sigma \tau + \tau^2$ . Then, for method of moments, we set $\frac{1}{n}\sum_{i=1}^{n}X_i=\hat{\sigma}+\hat{\tau}$ and $\frac{1}{n}\sum_{i=1}^nX_i^2=2\hat{\sigma}^2+2\hat{\sigma}\hat{\tau}+\hat{\tau}^2$ . But I don't understand how to proceed from here to get the estimators for $\sigma$ and $\tau$ , and I don't really understand how $E(X)=\sigma + \tau$ and $E(X^2)=2\sigma^2+2\sigma \tau + \tau^2$ were calculated. For $E(X)=\sigma + \tau$ , I tried $$E(X) = \int_\tau^\infty \dfrac{1}{\sigma} e^{-(x - \tau)/\sigma} \ dx,$$ but this results in $1$ , so I don't think it's correct. So how do proceed from here to get the estimators for $\sigma$ and $\tau$ ? And how were $E(X)$ and $E(X^2)$ calculated?","['statistics', 'exponential-distribution']"
4105032,Do almost commuting matrices almost commute with the square root?,"If $A$ and $B$ are $n\times n$ self-adjoint matrices with $B$ positive semidefinite and $\|B\|_F\leq1$ such that $$\|AB-BA\|_F\leq \epsilon,$$ where $\|A\|^2_F:=tr(A^*A)$ is the Frobenius (or Hilbert-Schmidt) norm. I'd like to know if there is a constant $C>0$ such that $\|A\sqrt{B}-\sqrt{B}A\|_F<C\epsilon$ is independent of $n$ . Using the solution to this question one can use the fact that there is a polynomial of degree at most $n$ such that $p(B)=\sqrt{B}$ and it follows that there is a constant $K>0$ such that for any polynomial $p(B)$ we see that $$\|Ap(B)-p(B)A\|_F\leq Kn\epsilon,$$ just using the submutliplicativity of $\|\cdot\|_F$ and the triangle inequality. However, in the case where $p(B)=\sqrt{B}$ , I would like to know if there is a bound that depends on $\epsilon$ but not $n$ . Edit: in the comments of the linked question, the author of the answer has noted that the degree of the polynomial is at least the number of distinct eigenvalues of $B$ , which makes sense if you are using the polynomial that interpolates the eigenvalues.","['matrices', 'normed-spaces']"
4105048,Derivative of symmetric matrix with respect to its elements?,"Let's say we have a symmetric matrix $g$ whose elements are $g_{ij}$ . I would like to write down the answer for the derivative of $g_{ij}$ with respect to $g_{kl}$ . Since $g$ is symmetric I would expect that $$\frac{\partial g_{ij}}{\partial g_{ij}}=\frac{\partial g_{ji}}{\partial g_{ij}}=1$$ And all other derivatives vanish. I would like to be able to write this as a result in terms of Kronecker deltas for general indices but I am getting stuck. My first guess would be to say $$\frac{\partial g_{ij}}{\partial g_{kl}}=\delta^{k}_{i}\delta^{l}_j$$ But this does not respect the symmetry of the matrix elements, so one might guess to symmetrize: $$\frac{\partial g_{ij}}{\partial g_{kl}}=\frac{1}{2}\Big(\delta^{k}_{i}\delta^{l}_j+\delta^{l}_{i}\delta^{k}_j\Big)$$ But now we have a problem because the nonzero off-diagonal derivatives equal $1/2$ and the diagonal derivatives equal $1$ . Is there anyway to resolve this? Edit : Going off of the comments so far, I tried a different form which looks wrong, but it's the only form I can think of which respects the way in which these indices transform under an e.g. orthogonal transformation: $$\frac{\partial g_{ij}}{\partial g_{kl}}=\frac{1}{2}\Big(\delta^{k}_{i}\delta^{l}_j+\delta^{l}_{i}\delta^{k}_j-\frac{3}{n}g_{ij}g^{kl}\Big)$$ Where $g^{kl}$ are the elements of $g^{-1}$ , assuming it exists, and $n$ is the dimension of the matrix. This is obtained by assuming that the right hand side is a projection matrix.","['matrices', 'matrix-calculus', 'derivatives']"
4105146,Prove that the lines parallel to the angle bisectors through the midpoints of sides of a triangle are concurrent at the Spieker center,"I've been learning about triangle centers, and I have been able to prove that most of them exist. However, this one that I've come across has stumped a bit, and I haven't been able to find any proofs or hints towards how to prove this specific point always exists. I believe the point is called the Spieker center. The problem prompt goes a little like this. Given a triangle $ABC$ , construct lines parallel to the interior angle bisectors of angles $A$ , $B$ , and $C$ , such that they pass through the midpoints of sides $BC$ , $CA$ , and $AB$ respectively. Prove that these lines are concurrent. Any ideas on how I should approach this proof?","['euclidean-geometry', 'triangles', 'geometry']"
