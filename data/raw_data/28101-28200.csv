question_id,title,body,tags
260930,Possible typo in proof of Bukovský-Hechler,"In the proof of the following theorem: Theorem 29 (Bukovský-Hechler): Let $\kappa, \lambda$ be infinite cardinals such that $\mathrm{cf}(\kappa) \le \lambda$ and
  $\mathrm{cf}(\kappa) < \kappa$. Denote $\displaystyle \sum_{\alpha <
 \kappa} |\alpha|^\lambda = \mu$. (a) If there exists $\alpha_0 < \kappa$ such that $|\alpha|^\lambda =
 |\alpha_0|^\lambda$ for all $\alpha$ satisfying $\alpha_0 \le \alpha <
 \kappa$, then $\kappa^\lambda = \mu$. (b) If for each $\alpha < \kappa$ there exists $\beta$ such that
  $\alpha < \beta < \kappa$ and $\alpha^\lambda < \beta^\lambda$, then
  $\kappa^\lambda = \mu^{\mathrm{cf}(\mu)}$. there appears to be a typo in the proof of part (b). Can you confirm this? The proof is the following: I don't quite believe that $H$ maps into ${^{\mathrm{cf}(\kappa)}}F$. Assume $\gamma_\xi$ is somewhere between $\alpha < \gamma_\xi < \kappa$. Then $f(\beta)$ could also be somewhere above $\alpha$ so that $g \notin F$. I am not sure what am missing but I think the definition of $H$ should depend on $\alpha$. The fix I propose is the following: $$ g(\beta) = \begin{cases} f(\beta) & \gamma_\xi < \alpha \\ 0 & \text{otherwise}\end{cases}$$ Thanks for your help.","['cardinals', 'elementary-set-theory']"
260948,Is this true for $3$ by $2$ matrices?,"Suppose that $M$ is a $3$ by $2$ matrix in which every $2$ by $2$
submatrix is invertible. Is it true that $M$ always has a $2$ by
$2$ submatrix $M_{1}$ such that $\left\Vert M_{1}\right\Vert ^{2}\left\Vert M_{1}^{-1}\right\Vert ^{2}\lambda_{2}\left(MM^{T}\right)\ge\left\Vert M\right\Vert ^{2}$
where $\left\Vert \cdot\right\Vert $ is the norm of a matrix (refer to here for the definition) and
$\lambda_{2}\left(\cdot\right)$ is the second largest eigenvalue
of a $3$ by $3$ symmetric matrix? Thanks","['linear-algebra', 'real-analysis', 'analysis']"
260968,Derivative of a parallel translation inside a metric,"Let $M$ be a riemannian manifold with metric $g$ and a connection
   $\nabla$ on $M$.  Let $X,Y$ two vector fields along a curve $\gamma$
   on $M$. Let $$\tau_{t,s}:T_{\gamma(s)}M\to T_{\gamma(t)}M$$ the
   parallel translation along $\gamma$. We consider the fonction
   $$F(t)=g(\gamma(t))(\tau_{t,s} X(\gamma(s)), \tau_{t,s}
 Y(\gamma(s))).$$ Then
   $$F'(s)=g(\gamma(t))\left(\frac{d}{dt}\bigg|_{t=s}\tau_{t,s}
 X(\gamma(s)),
 \tau_{t,s}Y(\gamma(s))\right)+g(\gamma(t))\left(\frac{d}{dt}\bigg|_{t=s}\tau_{t,s}
 Y(\gamma(s)), \tau_{t,s}X(\gamma(s))\right)+\dot\gamma\left[g(X,Y)\right](\gamma(s)) \ (\star).$$ I do not understand why this identity holds. Here is what I have tried so far: Let $$a(t)=\tau_{t,s} X(\gamma(s))=a_i(t)\frac{\partial}{\partial x_i}\bigg|_{\gamma(t)}\text{ and }b(t)=\tau_{t,s} Y(\gamma(s))=b_j(t)\frac{\partial}{\partial x_i}\bigg|_{\gamma(t)},$$
so $F(t)=g(\gamma(t))(a(t),b(t))$. If we let
$$g_{ij}(t)=g(\gamma(t))\left(\frac{\partial}{\partial x_i}\bigg|_{\gamma(t)}, \frac{\partial}{\partial x_j}\bigg|_{\gamma(t)}\right),$$
we find that $F(t)=a_i(t)b_j(t)g_{ij}(t)$, thus the derivative of $F$ is
$$F'(s)=a_i'(s)b_j(s)g_{ij}(s)+a_i(s)b_j'(s)g_{ij}(s)+a_i(s)b_j(s)g'_{ij}(s).$$
The first two terms give the first two terms of $(\star)$, but what about the last one ? We have that
$$\dot\gamma\left[g(X,Y)\right](\gamma(s))=\frac{d}{dt}\bigg|_{t=s} g(\gamma(t))(X(\gamma(t)),Y(\gamma(t))),$$
right ? This does not seem equal to $a_i(s)b_j(s)g'_{ij}(s)$... ?","['riemannian-geometry', 'derivatives']"
260969,Find the number of homomorphisms between cyclic groups.,"In each of the following examples determine the number of homomorphisms between the given groups: $(a)$ from $\mathbb{Z}$ to $\mathbb{Z}_{10}$ ; $(b)$ from $\mathbb{Z}_{10}$ to $\mathbb{Z}_{10}$ ; $(c)$ from $\mathbb{Z}_{8}$ to $\mathbb{Z}_{10}$ . Could anyone just give me hints for the problem? Well, let $f:\mathbb{Z}\rightarrow \mathbb{Z}_{10}$ be homo, then $f(1)=[n]$ for any $[n]\in \mathbb{Z}_{10}$ will give a homomorphism hence there are $10$ for (a)?","['cyclic-groups', 'finite-groups', 'group-theory', 'abstract-algebra']"
260970,Exercise: Application of The Uniform Boundedness Principle,"I'm working on this exercise (not homework) and I would gladly welcome some hints for how to solve it! Exercise: Let $1 \leq p,q \leq \infty$ be conjugate exponents. Let $a=(a_1,a_2,...)$ be a sequence such that $\sum_1^\infty a_n x_n$ converges for all $x=(x_1,x_2,...) \in l^p$. Prove that $a \in l^q$. My idea is like this: It's sufficient to show that $a \in l^1$ since $l^1 \subset l^2 \subset... $ I define a family of operators $\{T_n \}_{n=1}^\infty$ by $T_n(x) = \sum_{k=1}^n a_k x_k$. It is clear that each $T_n$ is linear, bounded and that $\sup_{n}|T_n(x)| < \infty$ so by the Uniform Boundedness principle we get $\sup_n \| T_n \| < \infty$. Is this a good approach? If so, I'd be grateful for some guidance on how to proceed to get to the conclusion: $$\sum_1^\infty |a_k| < \infty$$ Otherwise, steer me in a better direction :) Thanks in advance",['functional-analysis']
260973,"$h\left(\frac{m}{2^n}\right)=0$ $\forall m\in\mathbb{Z},n\in\mathbb{N}$ implies $h(x)=0$ $\forall x\in\mathbb{R}$ if $h$ is continuous.","Let $h:\mathbb{R}\to\mathbb{R}$ be a function that is continuous on $\mathbb{R}$ and has the property that $$h\left(\frac{m}{2^n}\right)=0,\quad\forall m\in\mathbb{Z},n\in\mathbb{N}.$$ How can we show that this implies that $h(x)=0$, $\forall x\in\mathbb{R}$? The way I thought about this problem is to show that the set $$ S:=\left\{\frac{m}{2^n}:\forall m\in\mathbb{Z},n\in\mathbb{N}\right\}$$ is dense in $\mathbb{R}$. It then follows that for any $c\in\mathbb{R}$ there exists a sequence $(x_n)$ that converges to $c$ such that all the terms are of the form $p/2^q$. Thus, the sequence $(h(x_n))$ converges to $0$ and so by the sequential criterion for continuity we must have $h(c)=0$. Is there a simpler approach? Proving that $S$ is dense in $\mathbb{R}$ seems overly complicated for this problem. Or, can we deduce it from the density of $\mathbb{Q}$ in $\mathbb{R}$?","['continuity', 'real-analysis']"
260982,How to determine phase image,"I need to sketch the phase image belonging to the following vector field (I'm sorry, I don't know the exact terms in English, so I have just freely translated them - thanks for sharing the correct terms, if you want to :)) $$F(x,y) = \begin{pmatrix} \frac{1}{2x}\\ yx^2\\ \end{pmatrix}  $$ In order to draft the phase image, I have to know the flow $\phi(t, x, y)$ (again, freely translated) of the vector field. Thus, I solved the differential equations $\frac{d}{dt}x(t) = \frac{1}{2x}$ and $\frac{d}{dt} y(t) = yx^2$, which led me to the following result (using initial valus $y(0) = y_0$ and $x(0) = x_0$: $$\phi(t,x,y) = \begin{pmatrix} \sqrt{t + x^2}\\ ye^{tx^2}\end{pmatrix}$$ My question is: How I am supposed to draft the phase image of this vector flow (it's dependending of three variables, after all) and is it the correct solutiona for the vector flow anyway? Thanks for your help!","['multivariable-calculus', 'ordinary-differential-equations']"
260991,Functions with discontinuous derivative at the endpoints of an open interval,"(a) Does there exist a function $f$ defined on the open interval $(a,b)$ such that $f'(b^-)$ exists, and $\lim_{x\to b-}f'(x)\neq f'(b^-)$, or (b) where $f'(b^-)$ exists and $\lim_{x\to b-}f'(x)$ does not exist? Since $f(b)$ is undefined, define $$f'(b^-)=\lim_{h\to0+}\frac{f(b-h)-f(b-2h)}h.$$ Are there any difficulties with this definition as compared to the standard definition of the one-sided derivative? Just reading my analysis textbook and thought this would make an interesting problem. Related: $f(x)=x^2\sin\frac1x$ has a derivative which is defined at $0$ (equal to $0$), but $\lim_{x\to 0}f'(x)$ does not exist. (c) Is it always true that if the limit exists, it is equal to $f'(0)$? Even more curiously, $\limsup_{x\to0}f'(x)+\liminf_{x\to0}f'(x)=2f'(0)$ for this function. (d) Is this always the case, when the quantity on the left side of the equality is defined?","['derivatives', 'real-analysis']"
261002,Tail bound for hypergeometric distribution,I am looking for a reference (book) for the tail bound for the Hypergeometric distribution. I know there is a nice paper by Skala (2009) but its unpublished. I am looking for a book which would be a nice reference. Thank you for your help.,"['probability-theory', 'distribution-tails', 'reference-request', 'hypergeometric-function']"
261011,Using product and chain rule to find derivative.,"Find the derivative of 
$$y =(1+x^2)^4 (2-x^3)^5$$ 
To solve this I used the product rule and the chain rule. $$u = (1+x^2)^4$$
$$u' = 4 (1+x^2)^3(2x)$$ $$v= (2-x^3)^5$$
$$v' = 5(2-x^3)^4(3x^2)$$ $$uv'+vu'$$ $$((1+x^2)^4)(5(2-x^3)^4(3x^2)) + ((2-x^3)^5 )(4 (1+x^2)^3(2x))$$ The answer I got is: $$(15x^2)(1-x^2)^4(2-x^3)^4 + 8x(2-x^3)^5(1+x^2)^3$$. Why is the answer $$8x(x^2 +1)^3(2-x^3)^5-15x^2(x^2)(X^2+1)^4(2-x^3)4$$? 
How did the $15x^2$ become negative?","['calculus', 'derivatives']"
261014,Proof for: $(a+b)^{p} \equiv a^p + b^p \pmod p$,"a , b are integers. p is prime. I want to prove: $(a+b)^{p} \equiv a^p + b^p \pmod p$ I know about Fermat's little theorem, but I still can't get it I know this is valid: $(a+b)^{p} \equiv a+b \pmod p$ but from there I don't know what to do. Also I thought about $(a+b)^{p} = \sum_{k=0}^{p}\binom{p}{k}a^{k}b^{p-k}=\binom{p}{0}b^{p}+\sum_{k=1}^{p-1} \binom{p}{k} a^{k}b^{p-k}+\binom{p}{p}a^{p}=b^{p}+\sum_{k=1}^{p-1}\binom{p}{k}a^{k}b^{p-k}+a^{p}$ Any ideas? Thanks!","['prime-numbers', 'elementary-number-theory', 'number-theory']"
261023,"'Simple"" Chernoff bounds","Reading an academic paper I've observed the next claim: A simple Chernoff argument will now show that if an event has a constant probability at every step of occurring and there's independence between the steps then after $O(c*\log n)$ steps the event will happen $O(\log n)$ times with probability greater than $1-1/n^{O(c)}$. Trying to follow this statement I'm trying to find the matching variant of Chernoff inequality and derive using the matching formula  the probability as described in the quote. The model I've suggested is $c*\log n$ i.i.d. random Bernoulli variables, each has similar chance generate an event  $Prob(event)=p$. The expectation of the sum of variables is $p*c*\log n$. I need to find matching Chernoff inequality that shows that probability of getting at least $\log n$ events within $c*\log n$ variables (steps) is at least $1-1/n^{O(c)}$. I'm totally lost in deriving the expression $1-1/n^{O(c)}$ from the Chernoff inequalities I've found. Thanks in advance.","['distribution-tails', 'inequality', 'probability']"
261049,Effective model for calculating momentum or growth rate for a time series,"I have a series of numbers tracking the performance of an entity on any given day.  It's nothing but a simple integer for each date.  For example, here's a series for Entity ""X"" 11/1/2012 - 11/30/2012 - 3000 (one month summary)
12/1/2012 - 123
12/2/2012 - 129
12/3/2012 - 131
12/4/2012 - 112
...
12/31/2012 - 147 Ultimately I'm trying to output some form of momentum indicator for this entity for a day, week, and month.  My initial attempts were as follows: I could simply calculate the slope between two points.  For daily, I would compare day 1 versus day 2 and take the slope.  For weekly, the average count for week 1 and compare to average count for week 2, etc. Use a theoretical formula for detecting whether or not the count is ""breaking out"" of a local or global maxima (up) or minima (down) and assign a value based on the distance from the local or global maxima/minima.  Unfortunately I'm not sure how one would approach calculating this. An additional formula that determines whether or not the trend created in #2 is actually sustained. My question is this: in a very simple time series, is there a relatively straight-forward approach to coming up with a momentum indicator?  Obviously there are flaws with a simple equation but the more simple the better.  I can improve the equation over time. One assumption: There are no other variables to be taken into consideration aside from this number.  All other things are constant.","['statistics', 'regression', 'algorithms']"
261058,Is the limsup of a sequence of measures also a measure?,"Given a sequence $(\mu_{n})_{n\in\mathbb{N}}$ of $\sigma$-finite measures on the measurable space $(Ω,\Sigma)$, is the $\limsup_{n}\mu_{n}$ also a $\sigma$-finite measure? Clearly, $\limsup_{n}\mu_{n}(A)\geq 0$ for all $A\in \Sigma$, and that $\limsup_{n}\mu_{n}(\varnothing)=0$. But I am not sure if $\sigma$-additivity holds. I have read about the Vitali-Hahn-Saks theorem which shows that the limit of a sequence of measures is also a measure.
Can we adapt the proof of Vitali-Hahn-Saks' theorem or is there a counterexample?","['measure-theory', 'real-analysis']"
261061,A question on convergence of series,"Suppose $(z_i)$ is a sequence of complex numbers such that $|z_i|\to 0$ strictly decreasing. If $(a_i)$ is a sequence of complex numbers that has the property that for any $n\in\mathbb{N}$ $$
 \sum_{i}a_iz_i^{n}=0  
$$ 
does this imply that $a_i=0$ for any $i$? Edit: For the ""finite dimensional"" case, when we have $n$ distinct $(z_i)$, then $(a_i)$ must be $0$. This amounts to solving a homogeneous system of $n$ equations with $n$ unknowns, which only has the trivial solution in the case of distinct $(z_i)$. I am really curious what happens in the infinite dimensional case. My intuition tells me the same must be true, but I don't have a proof for it. Edit 2: Very interesting, looking were this question originated, the fact that all $a_i=0$ when $(a_i)\in l_1$ is ""expected"". I was hoping to get a counterexample otherwise. However, if a non-trivial sequence $a_i$ exists (at least for some sequences $z_i$), I would ""expect"" to be able to choose it in $l_2$. Looking at Davide and Julien answers below, it seems $(a_i)\in l_1$ is an essential assumption in their argument.","['convergence-divergence', 'linear-algebra', 'real-analysis', 'analysis', 'complex-analysis']"
261066,"Why for a compact metric probability space, any Borel subset can be approximated by compact set?","Let $X$ be a compact metric space with a Probability Borel measure $\mu$.
Let $C$ be any Borel subset of $X$.
Then for any small positive number $a$, we can find compact set $K$ such that $K$ is subset of C and $\mu(C\setminus K)<a$. Why is it so?","['measure-theory', 'functional-analysis', 'descriptive-set-theory']"
261069,Two Steps away from the Hamilton Cycle,"Assume an at least $2$-vertex connected, cubic, bipartite, planar graph $G$ that contains a Hamilton cycle (HC) $abcdefg\dots yx\dots za$ (in fact $G$ would then have at least four HCs, see here ; it is $yx$ due to the picture I've drawn below and an assumption disproven here ). My question is: How can one deviate from a given Hamilton cycle in such a way, that one introduces exactly two errors, resp. does introducing two errors rely on certain subgraphs? By errors I mean, that two vertices are met twice, while two others are not met at all. EDIT: To be a little more specific: The Hamilton cycle and the deviation from it with two errors both start and end at the same vertex and are of the same length. We don't add or remove edges. With the notation above the HC would be e.g. $adef\color{red}{ef}\dots yx \dots za$, so the $\color{grey}{cd}$-edge is missing.
I found three possibilities that are PAINTed below. Let the edges of the HC be coloured in $\color{green}{green}$ and edges that introduce errors in $\color{red}{red}$. $\color{black}{Black}$ edges don't contribute to the case under investigation. Figure 1: We can introduce two errors if we go directly from $a$ to $d$, thereby miss $bc$, and then: use backtracking along the HC $ad\dots ef\color{red}{ef}\dots yx$ $\scriptstyle \text{(this can happen at any edge along the HC)}$ use backtracking aside the HC $ad\dots e\color{red}{xe}f\dots yx$ $\scriptstyle \text{(this also can happen everywhere on the HC)}$ or make a detour at a second square $ad\dots e\color{red}{xy}f\dots yx$ $\scriptstyle \text{(only here we need a 2nd square)}$. EDIT For Figure 1)ii. I found another version without backtracking: Let the part of the HC be $\dots abcd\dots ef \dots yx\dots$, so that there is another cycle part $C_{fy}$ joining $f$ and $y$. Then two errors can be introduced as follows: make the usual shortcut at a square, missing the $bc$-edge go from $e$ to $x$ walk along $C_{fy}$ in the opposite direction again go from $e$ to $x$, which is traversed twice You may think of backtracking as a $2$-cycle. This variant may also happen everywhere. Figure 2: ""$\color{blue}4$-$\color{goldenrod}2$-hexagon"": Again the original HC $\underbrace{abcd}_{\color{blue}4} \dots \underbrace{yx}_{\color{goldenrod}2}$ is depicted in green. Two errors can be introduced by $a\color{red}{xy}d\dots yx$. My question rephrased: $\hskip1.3in$ Are these subgraphs the only ones to introduce two errors? Some remarks: In Figure 1 the vertices $d$ and $e$ don't need to be adjacent as indicated by the dashed line. In both figures, I think (not sure) that I could have also chosen $yx$ at the end. I tried to use the ""$2$-$2$-$2$-hexagon"" (where Hamilton runs through like $ab...cd...yx$, so without the $bc$-, $dy$- and $xa$-edges) without success and sorry for the PAINT work (feel free to improve it!)... .. and I'm pretty sure that the answer is no, see below... EDIT: Some more remarks concerning a general solution I also thought about a more general approach by using $n$th powers of adjacency (sub)matrices, which corresponds to doing $n$ steps on the subgraphs. It's even possible to exclude backtracking as you can read here . But since the the set of adjacency (sub)matrices, i.e. the (sub)graphs sould be the result of such an approach, I admit that for now, I don't know how to work this out. How does the HC come into play then? Thanks for your help and special thanks to Brian Rushton,...","['graph-theory', 'combinatorics']"
261073,Finding probability $P(X<Y)$,How can I find this probability $P(X<Y)$ ? knowing that X and Y are independent random variables.,"['statistics', 'probability', 'random-variables', 'probability-theory']"
261074,True Definition of the Real Numbers,"I've found lots of resources that say this is a real number if it's not rational, but what is a real number, really?  I mean what is the definition of a real number?  If nothing else, anyone know of a resource where I could find out myself? Thanks!","['real-analysis', 'definition']"
261081,Proving an Entire Function is a Polynomial,"I had this question on last semesters qualifying exam in complex analysis, and I've attempted it several times since to little result. Let $f$ be an entire function with $|f(z)|\geq 1$ for all $|z|\geq 1$.  Prove that $f$ is a polynomial. I was trying to use something about $f$ being uniformly convergent to a power series, but I can't get it to go anywhere.",['complex-analysis']
261096,Are correlated univariate normals equivalent to a bivariate normal?,"If I know that $A \sim \mathcal N(0,\sigma^2_A)$ and $B \sim \mathcal N(0,\sigma^2_B)$ and that $A$ and $B$ have a correlation coefficient of $\rho$, does this mean that it must be that case that $\begin{bmatrix} A \\ B \end{bmatrix} \sim  \mathcal N\left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \sigma^2_A & \rho \sigma_A \sigma_B \\ \rho \sigma_A \sigma_B & \sigma_B^2    \end{bmatrix} \right) \>?$ Or is that just one possibility? And what about for other distributions?","['statistics', 'probability-distributions', 'probability']"
261100,Optimization of the area of a cross inscribed in a circle,"I've really been scratching my head over this optimization problem. ""Consider a symmetric cross inscribed in a circle of radius $r$."" The length from the center of the cross to the middle of one of its arms is $x$. Also, the angle between two line segments drawn from the cross's center to the vertices of one of its arms has a measure of $\theta$. Here's a diagram: There are three parts to the problem: ""(a) Write the area $A$ of the cross as a function of $x$ and find the value of $x$ that maximizes the area. (b) Write the area $A$ of the cross as a function of $\theta$ and find the value of $\theta$ that maximizes the area. (c) Show that the critical numbers of parts (a) and (b) yield the same maximum area. What is that area?"" So, let me show you what I've done so far. For part (a), I decided to break the cross into two middle rectangles and two side rectangles. I saw that a middle rectangle (from the center to the top) would have an area of $$x \cdot 2 \sqrt{r^2 - x^2}$$ using the Pythagorean theorem. I worked out that a side rectangle (the remaining area on the right, adjacent to the middle rectangles) would have an area of $$2 \sqrt{r^2-x^2} \cdot \left( x - \sqrt{r^2 - x^2} \right) .$$ So, the area of the cross is $$A = 2 \bigg( x \cdot 2 \sqrt{r^2 - x^2} + 2 \sqrt{r^2 - x^2} \cdot \Big( x - \sqrt{r^2 - x^2} \Big) \bigg) = 8x \sqrt{r^2 - x^2} - 4r^2 + 4x^2 .$$ If my math is right there (fingers crossed), then I'll take the first derivative to locate a maximum. $$A^\prime = 8 \sqrt{r^2 - x^2} + 8x \left( 1 \over 2 \right) \left( r^2 - x^2 \right)^{- {1 \over 2}} \left( -2x \right) + 8x.$$ I was a little unsure about what to do at this point. I plugged the $A^\prime$ equation into my graphing calculator, substituting $1^2$ for $r^2$ (for a radius of $1$). The graph crosses the $x$-axis at $x \approx 0.85$. Substituting $2^2$ for $r^2$ (for a radius of $2$) gives me $x \approx 1.70$. From this, I concluded that $$A^\prime = 0 \; \mathbf{at} \; x \approx 0.85r.$$ Analysis of graphs of $A$ for various values of $r$ concludes that, indeed, maxima do appear at $x \approx 0.85r$. So, I have the function $A$ in terms of $x$, but I'm curious: What should my final answer be for the second part of (a)? All I have is $x \approx 0.85r$. Is that a sufficient answer? As for part (b), I really have no idea how to write $A$ in terms of $\theta$. I know that $\text{area} = {1 \over 2} b \cdot c \cdot \sin A$ for triangles, but I really need help writing the area of this cross in terms of $\theta$ . Part (c) should be easy enough once I finish (b). If you got to the end of this, I sincerely thank you for reading, and I would really appreciate an answer (and any corrections to my math). Thanks!","['optimization', 'geometry', 'calculus']"
261101,Variation of the fundamental lemma of calculus of variation,"Let
$$C^1_0[a,b]:=\{f \ C^1[a,b]|f(a)=f(b)=0\}.$$
Providing $C^1_0[a,b]$ is dense in $L^2[a,b]$, I want to prove the following statement: if for $g,h\in L^2[a,b]$, $$\int_a^b g \phi \,dx =\int_a^b h \phi \,dx$$ 
for all test functions $\phi\in C^1_0[a,b]$, then $g = h$ almost everywhere. It seems to be similar with fundamental lemma of calculus of variation, how can I extend the the result to $L^2$ functions? My guess is using density argument, along with one of convergence theorems, but I failed to construct the proof. Please help me out here.
Thank you.","['functional-analysis', 'calculus-of-variations']"
261112,Prove that random variables satisfy the inequality $E(XY)^2 \le E(X^2)E(Y^2)$?,Given Random variables $X$ and $Y$ is it true always that; $$E(XY)^2 \le  E(X^2)E(Y^2)$$ Is it easy to prove?,"['inequality', 'probability']"
261122,Help with infinite sum,"Can you guys give me a hint on evaluating $$\sum_{n=1}^\infty \frac{1}{n(n+2)(n+4)}?$$
I have tried partial fractions but the series is not telescopic (at least I cannot see it)...","['sequences-and-series', 'algebra-precalculus']"
261133,Why is the best position for LCR not the last person?,"For the uninitiated, LCR is a game in which each player starts with three ""tokens"" and rolls up to three dice (at most as many as tokens they have). Each die has three sides which indicate that nothing happens and one spot apiece for left, center, and right; left and right indicating that they pass one token in that direction and center indicating that a token goes to the ""center,"" where it is removed from the game. The game ends when only one player has any tokens. I was wondering what position would be best, so I created the following Python script to do this for me: from random import *

def LCRRound(players):
    global playerset
    playerset = [3] * players
    while not GameOver():
        for player in range(len(playerset)):
            for i in range(min(3, playerset[player])):
                Move(player)
            if GameOver():
                break
    return [playerset.index(p) for p in playerset if p!=0][0]
def L(player):
    playerset[player]-=1
    playerset[player-1]+=1
def C(player):
    playerset[player]-=1
def R(player):
    playerset[player]-=1
    playerset[(player+1)%len(playerset)]+=1
def GameOver():
    return playerset.count(0) == len(playerset)-1
def Move(player):
    tmp = randrange(6)
    if tmp==3:
        L(player)
    if tmp==4:
        C(player)
    if tmp==5:
        R(player)

for x in range(2, 11):
    wins = [0] * x
    for y in range(100000 * x):
        wins[LCRRound(x)]+=1
    print(wins) Which tests for randomly generated games with 2 to 10 players, playing 100,000 games for each player in each set (so 200,000 games for the two player tests, 300,000 for three, and so on up to 1,000,000 games for ten players). This generated the following output (and similar output other times I ran it), with the numbers being the number of times that player one (players are in order of who rolls): [76233, 123767]
[91720, 98359, 109921]
[95913, 97396, 101796, 104895]
[96340, 97629, 99926, 103080, 103025]
[96985, 96768, 98607, 101509, 103283, 102848]
[97557, 96211, 97613, 100659, 102562, 103595, 101803]
[97636, 95984, 96652, 99220, 101364, 103619, 104070, 101455]
[98000, 95559, 96338, 97589, 99600, 102966, 104767, 104061, 101120]
[97355, 95754, 95876, 97163, 99303, 101537, 103103, 104583, 103943, 101383] For two and three players, the optimal position is last. However, after that, the player who wins the most becomes the second to last and then the third to last (and would presumably continue in this motion as more players are added). This is contrary to what I expected, since I thought the optimal position would be the player immediately before, after, or opposite the starting player, but it is none of these. In face, the second player seems to be the most disfavorable rather than the person who starts. Which leads me to my question: What is the mathematical explanation for what position is the best in LCR?","['dice', 'game-theory', 'probability']"
261139,The indicator function of an open set is an increasing limit of a sequence of continuous functions,"Let $G$ be an open set on some measure space $(\Omega,\Sigma, \mu)$.
Show that the indicator function of $G$ is an increasing limit of a sequence of continuous functions. I understand that this is a ""simple text book exercise"". I am only seeking for hints. My idea (at least in 2D) is that we should construct some smooth trapezoidal-like functions, where the gradients near the endpoints of $G$ gets steeper and steeper. Am I on the right track? Also, how do I write this out mathematically?","['measure-theory', 'real-analysis']"
261141,Upper Triangular Form of a Matrix,"I am trying to find the upper triangular form of $B$ and an invertible matrix $C$ such that $B=C^{-1}AC$ where A is given by the following: $A = \pmatrix{1&1\\
          -1&3}$ The characteristic equation is $(x-2)^2$ with eigenvalue $2$ and eigenvector $\langle1, 1\rangle$. Finding an upper triangular matrix similar to $A$ means to find a vector $w$ such that $(A-2I)w = v$. In the above, is $v$ the eigenvector? Could someone explain both of the above to me and why this is true? Taking the above statements to be true we have: $\pmatrix{1&-1 \\
          1&-1}\pmatrix{w_1 \\ w2} = \pmatrix{1 \\ 1}$ $\implies w_1 - w_2 = 1$ One solution is $\langle1, 0\rangle$. From here I do not know what to do next.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
261144,Godambe estimating equation (Proof),"Let $Y_1, \ldots ,Y_n$ be iid with density $f(y;\theta)$. We assume that $\dfrac\partial{\partial\theta}\log f(y ; \theta)$ and $\dfrac{\partial^2}{\partial\theta^2}\log f(y ; \theta)$ exist for all $\theta$. Consider a class $G$ of real functions $g(Y;\theta)$ such that: 1) $E_\theta[g(Y;\theta)] = 0$. 2) $\dfrac{\partial{g(Y;\theta)}}{\partial\theta}$ exists, is negative and bounded for all $\theta$ 
and $Y$. 3) $E_\theta[g^2(Y;\theta)] < \infty,\,$ for all $\theta$. The goal is to show that the score function (derivative of the log likelihood with respect to $\theta$) is a member of $G$ that minimizes: $$ \frac{E_{\theta}[g^2(Y;\theta)]}{(E_\theta[\partial_\theta g(Y;\theta)])^2}$$ Thanks for your help.","['statistics', 'convergence-divergence', 'probability-theory']"
261145,Find an abelian infinite group such that every proper subgroup is finite,"I found this question in Arhangel'skii and Tkachenko's book Topological Groups and Related Structures . The first chapter of the book is devoted to algebraic preliminaries. The question actually reads: Give an example of an infinite abelian group all proper subgroups of which are finite. What I have done is:
Every element of this group has finite order, else we could find an infinite proper subgroup, namely the group generated by $x²$ if $x$ has infinite order. I think this can be strengthened: every element should have a prime order. Although I haven't proved this. Intuitively this group cannot be and infinite product of smaller groups, because you could take the product of the even group factors and find an infinite proper subgroup. Well, this is it, a highly non-trivial problem. Thanks in advance.","['infinite-groups', 'group-theory', 'abelian-groups']"
261157,Show $S = f^{-1}(f(S))$ for all subsets $S$ iff $f$ is injective,"Let $f: A \rightarrow B$ be a function. How can we show that for all subsets $S$ of $A$, $S \subseteq f^{-1}(f(S))$? I think this is a pretty simple problem but I'm new to this so I'm confused. Also, how can we show that $S = f^{-1}(f(S))$ for all subsets $S$ iff $f$ is injective?","['elementary-set-theory', 'functions']"
261164,Every subgroup of finite rank of $\mathbb Z^\mathbb N$ is free abelian,"I want to prove that every subgroup of finite rank of $\mathbb Z^\mathbb N$ is free abelian.
A group $G$ has finite rank $N$ if there exists a subset $ \{e_1,...,e_N\}$ of $G$ such that: $(1)$ If $a_i \in \mathbb Z$,  $\sum_{i=1}^{N} a_i e_i=0 $ implies $a_i=0$ for $i=1,2,...,N$. $(2)$ For every $g\in G$, exist $k \in \mathbb Z ,k\neq 0$ and $a_i \in \mathbb Z$, $i=1,2,...,N$ such that $kg=\sum_{i=1}^{N} a_i e_i$. So let $G_n=\{a\in \mathbb Z^\mathbb N:\exists k\in \mathbb Z, k\neq 0 $ such that $ka=\sum_{i=1}^{n} a_i e_i\}$. I was trying to find a free $\mathbb Z$-module $M$ such that $G_n$ is a submodule of $M$ and use that every submodule of a free $R$-module, where $R$ is a PID, is free. But I'm having troubles finding this module. Any idea would be appreciated, thanks.","['modules', 'group-theory']"
261165,Joint Bernoulli Distribution,"If $X$ and $Y$ are two (not necessarily independent) Bernoulli's with success probabilities $a$ and $b$ resp., how do we construct the joint dist. in terms of $a$,$b$, and $\rho$---the correlation? I can get $\mathbb{P}(X=1,Y=1)$ by manipulating the expression for $\rho$, but lost for the other three...",['probability']
261174,Definition of differential of map (in algebraic geometry),"Given two schemes $X, Y$ with a map $f: X \rightarrow Y$, one should have a map $df: T^*Y \times_Y X \rightarrow T^*X$. How is this map defined (using the language of algebraic geometry, rather than the language of manifolds)?",['algebraic-geometry']
261194,Rephrasing a Convergence Result to make use of the Borel-Cantelli Lemma,"Let $X_n$ be a sequence of non-negative iid random variables. Is it true that the condition, $$\limsup_{n\rightarrow\infty} \frac{X_n}{n} = \infty \text{ almost surely}$$ is equivalent to the condition, $$\mathbb{P} \Bigg(  \limsup_{n\rightarrow\infty} \Big\{ {\frac{X_n}{n} \geq x} \Big\} \Bigg) = 1 \ \text{ for all } \ x > 0$$ I am wondering because I would like to rephrase the initial condition so as to make use of the Borel Cantelli lemma.","['probability-theory', 'measure-theory', 'probability']"
261204,Every integer vector in $\mathbb R^n$ with integer length is part of an orthogonal basis of $\mathbb R^n$,"Suppose $\vec x$ is a (non-zero) vector with integer coordinates in $\mathbb R^n$ such that $\|\vec x\| \in \mathbb Z$. Is it true that there is an orthogonal basis of $\mathbb R^n$ containing $\vec x$, consisting of vectors with integer coordinates, all with the same length? For example: let $\vec x = \left<2,10,11\right>$, so $\|\vec x\| = 15$. Then the vectors $\left<14,-5,2\right>$ and $\left<5,10,-10\right>$ complete a basis of $\mathbb R^3$. I've checked all such integer vectors in $\mathbb R^3$ with an integer length up to 17 and found no counter examples. Moreover, these can always be arranged as a symmetric matrix, possibly changing the order (or permuting the coordinates) and changing signs ( edit: not always; see answer below). For example, the vectors above can be arranged as: $$\begin{bmatrix}14&-5&2\\-5&-10&10\\2&10&11\end{bmatrix}$$ This is easily true if $n$ is even ( edit: rather, if $n=4$), you can simply permute the entries of $\vec x$ (altering signs appropriately) to find $n-1$ other vectors orthogonal to $\vec x$. In $\mathbb R^3$, finding a second integer vector is sufficient because the cross product of the two (divided by $\|\vec x\|$) will give the third vector of such a basis. Then it is straightforward to prove for special cases (like $\vec x = \left<1,2m,2m^2 \right>$, $m\in \mathbb Z$), but I can't think of a good reason why this should be true in general. Edited , hopefully for clarity, by o.p. The original phrasing and title referred to $\mathbb Z^n$.","['integer-lattices', 'quadratic-forms', 'elementary-number-theory', 'matrices', 'linear-algebra']"
261208,Proof of the Pythagorean Theorem via $\frac{d}{dx}\sin^2 x + \frac{d}{dx}\cos^2 x = 0$,"Any one seen this proof before? $$\frac{d}{dx} \sin(x)^2=2\cos(x)\sin(x)$$
$$\frac{d}{dx} \cos(x)^2=-2\cos(x)\sin(x)$$ $$\frac{d}{dx} \sin(x)^2+\frac{d}{dx} \cos(x)^2=0$$
$$\sin(x)^2+\cos(x)^2=c$$
$$\sin(0)^2+\cos(0)^2=c$$
$$1=c,$$ $$\sin(x)^2+\cos(x)^2=1$$ Let C, A, and B be the hypotinuse, opposite, and ajacent sides of a right triangle, then
$$((C\sin(x))^2+(C\cos(x))^2=C^2$$
$$A^2+B^2=C^2$$ Is this proof valid, i.e. is the Pythagorean theorem used in defining the above trig relations?","['geometry', 'calculus', 'proof-writing']"
261225,two successive zeros,"I am trying to solve a programming problem, but i dont know how to find number of numbers that dont contain two successive zeros. Please help me. 
for deeper understanding i included part of the problem in message. Let’s consider K-based numbers, containing exactly N digits. We define a number to be valid if its K-based notation doesn’t contain two successive zeros. For example: 1010230 is a valid 7-digit number;
1000198 is not a valid number; 
0001235 is not a 7-digit number, it is a 4-digit number. Thank you!",['combinatorics']
261236,A measure theory question,"Let $x_0,x_1,\ldots$ be an infinite sequence of real numbers with $x_n \in [0,1],\,\forall n\in\mathbb{N}$. Let $\mathcal{F}_0,\mathcal{F}_1,\ldots,$ be an infinite sequence of disjoint Lebesgue measurable subsets of $[0,1]$ with $\bigcup_{n\in\mathbb{N}}\mathcal{F}_n = [0,1]$. Consider the real number $\delta = \sum_{n\in\mathbb{N}}\int_{\mathcal{F}_n} (x-x_n)^2 \mathrm{d}x$. So, what I did is the following: Since $\sum_{n\in\mathbb{N}}\int_{\mathcal{F}_n}\mathrm{d}x = 1$, there is an index, say $m$, with $\int_{\mathcal{F}_m}\mathrm{d}x >0$. Now, $\delta \geq \int_{\mathcal{F}_m} (x-x_m)^2 \mathrm{d}x$, and since the integrand is non-zero almost everywhere (except at $x_m$) we have $\delta > 0$. I guess my derivations are fine. What ""worries"" me is that I can make $\delta$ arbitrarily small but never $0$. Somehow, and I don't know why, I ""feel"" that I should be able to take $\{x_0,x_1,\ldots\}$ to be all the rational numbers in $[0,1]$, and get $\delta = 0$ with a ""nice"" choice of $\mathcal{F}_n$. Am I wrong in my calculations, and if I am right, why is this happening at an intuitive level? Is there any ""system"" (or a different kind of measure) where I would get $\delta = 0$ (with respect to that measure).",['measure-theory']
261241,Let $G$ be a group of order $56$. Then which of the following are true,Let $G$ be a group of order $56$. Then which of the following are true All $7$-sylow subgroups of $G$ are normal All $2$-Sylow Subgroups of $G$ are normal Either a $7$-Sylow subgroup or a $2$-Sylow subgroup of $G$ is normal There is a proper normal subgroup of $G$. How would I able to solve this problem and which theorem(s) would be required to solve this? Thanks for your time.,"['examples-counterexamples', 'finite-groups', 'group-theory', 'abstract-algebra']"
261244,Is there a fundamental reason that $\int_b^a = -\int_a^b$,"Is there a fundamental reason that switching the order of the limits in an integral results in the negative, i.e., $$\int_b^af(x)\,dx = -\int_a^bf(x)\,dx?$$  As far as I can tell, this is just chosen as a convention so that the rule $$\int_a^bf(x)\,dx + \int_b^cf(x)\,dx = \int_a^cf(x)\,dx$$ works out.  But I was wondering if there was some more fundamental reason for this, perhaps somehow relating to signed measures or something. Background The reason I'm asking is that we're trying to figure out how to define things like $$\sum_{n=4}^1n$$ in the computer algebra system SymPy (see this discussion ).  The natural thing, for me at least, is to define this as 0, since it represens a summation over an empty set ($\{x\,|\,4\leq x\leq 1\}$).  But it seems that some authors define this as $-\sum_{n=2}^3n$, so that the convention $\sum_{n=a}^bf(n) + \sum_{n=b + 1}^c f(n)= \sum_{n=a}^cf(n)$ works out (namely, Karr in ""Summation in Finite Terms"").  That got me thinking about integrals, and whether in that case the rule is also defined simply for convenience, or whether there is actually a fundamental motivation behind it in the definition of the integral. I know that summations are just special cases of integrals (at least in the Lebesgue sense), so learning why things work for integrals would help me understand how things should work for summations.","['lebesgue-integral', 'summation', 'integration', 'convention']"
261246,"Ideals in $C[0,1]$","Let $C[0,1]$ be the ring of continuous real-valued functions on $[0,1]$, with addition and multiplication defined pointwise. 
  For any subset $S$ of $C[0,1]$ let $Z(S)=\{f\in C[0,1]: f(x)=0 \text{ for all }x\in S\}$. Then which of the following statements are true? (a) If $Z(S)$ is an ideal in $C[0,1]$ then $S$ is closed in $[0,1]$. (b) If $Z(S)$ is a maximal ideal then $S$ has only one point. (c) If $S$ has only one point then $Z(S)$ is a maximal ideal. (a) Not necessary: if I take $S=(1/2,1/3)$ still $Z(S)$ is an ideal. (b) I know that maximal ideals in $C[0,1]$ come in this way (I don't know the proof rigorously), i.e. $C_a=\{f\in C[0,1]:f(a)=0\}$ so $S$ may be finite or countable set? So I guess $(b)$ is a true statement and for the same reason $(c)$ is also true. But I will be happy if someone can explain me a bit about (b) and (c). Thank you.","['ring-theory', 'ideals', 'abstract-algebra']"
261295,"To invert a Matrix, Condition number should be less than what?","I see that there is a matlab tag in this site, so I ask my question here and not in stackoverflow although it is also related to programming in matlab. I am going to invert a positive definite matrix in matlab and I should consider multicolinearity too. I read this in Wikipedia 's Multicollinearity : If the Condition Number is above 30, the regression is said to have significant multicollinearity. and something similar in Greene's Econometrics book (condition number must be less than 20). But there are some links that says different, like PlanetMath's Matrix Condition Number : Matrices with condition numbers much greater than one (such as around $10^5$ for a $5 \times 5$ Hilbert matrix) are said to be ill-conditioned. or Wikipedia's Condition_number : As a general rule of thumb, if the condition number $\kappa(A) = 10^k$ , then you may lose up to $k$ digits of accuracy on top of what would be lost to the numerical method due to loss of precision from arithmetic methods Which one is correct? (both? I mean something is different and I do not get it?) Update I used the answer and the comment to update my question: Consider $\mathbf{X}$ to be the matrix of observations. Ordinary Least square estimates vector is $\mathbf{b=(X'X)}^{-1}\mathbf{X'y}$ . If $\mathbf{X'X}$ is non-singular, we can not calculate this vector. The matrix $\mathbf{X'X}$ is non-singular if 2 columns of $\mathbf{X}$ are linearly dependant. It is sometimes called Perfect Multicollinearity. I think this discussion is used to conclude Multicolinearity and $\mathbf{X'X}$ inversion and as a result the condition number of $\mathbf{X'X}$ are related. It means sometimes we can invert $\mathbf{X'X}$ with acceptable precision, but it does not mean that two columns of $\mathbf{X}$ are linearly dependant. Is the last paragraph correct? Thanks.","['matrices', 'matlab', 'inverse']"
261314,Convergence in the Box Topology.,"Given the sequence in $\mathbb{R}^\omega$:
$$y_1=(1,0,0,0\ldots), y_2=(\frac{1}{2},\frac{1}{2},0,0\ldots), y_3=(\frac{1}{3},\frac{1}{3},\frac{1}{3},0,\ldots),\ldots$$
I know that it converges in the product topology because it converges pointwise to $(0,0,\ldots).$ I know that it converges in the uniform topology since it converges to $(0,0,\ldots)$ uniformly. How do I determine whether or not this sequence converges in the box topology?","['general-topology', 'convergence-divergence']"
261326,How many $N$ digits binary numbers can be formed where $0$ is not repeated,"How many $N$ digits binary numbers can be formed where $0$ is not repeated.
Note - first digit can be $0$. I am more interested on the thought process to solve such problems, and not just the answer. If anyone can cite some resources for learning how to solve such problems would be great.","['reference-request', 'combinatorics']"
261336,Intersection between a rectangle and a circle?,"I have a poor working knowledge of math. I would like to calculate collision detection between a 2D circle and a 2D rectangle for a simple game of Pong. I thought of splitting the 2D rectangle into 4 separate lines, each with its own equation, and then equating each line equation with the circle's equation to check for intersection points. A search on Math StackExchange turned up this post: solution #1 . (Also: I was wondering why the difference in approach between solution #1 and this solution #2 ? (EDIT: The difference is that solution #2 solves the intersection of a line segment with a circle while the first solution solves the intersection of a line with a circle.) Why would a person use one over the other? It looks like the first solution is simpler: solving two equations (hey I can do that), whereas the second solution involves parametric equations (which I do not understand)). I am curious for a different solution though. The solution above checks for an intersection between the equation of a line and the equation of a circle (I think). What about checking the intersection between the equation of a rectangle and a circle ? Is that the same thing as saying the intersection between a plane and a circle (is a rectangle a plane)? I thought planes were defined by 3 points ... my rectangle is defined by four. I'm not really sure what I'm saying anymore. Questions: Is either solution #1 or solution #2 valid in solving collision detection? Which solution (#1 or #2) is preferable for what reasons? How can I calculate the intersection between a 2D rectangle and a 2D circle (without splitting up the rectangle into four separate lines)? Please select the most straightforward solution as its already hard for me to follow the concept of parametric equations. Could you also please explain the solution in some amount of detail? I'm curious as to how the math works.","['geometry', 'trigonometry', 'multivariable-calculus']"
261337,How to prove these two random variables are independent?,"If $X$ and $Y$ are independent Gamma random variables with parameters $(\alpha,\lambda)$ and $(\beta,\lambda)$ respectively, how to show that $U=X+Y$ and $V=X/(X+Y)$ are independent?","['probability-distributions', 'probability']"
261357,How many rounds does it take to be 99% sure of reaching Expected Value?,"I have a feeling this might be a common question but I was unable to find the right way of asking, and I'm just a hobbyist at stats/math. Say I have a bet that costs six dollars.  If I lose I get nothing, and if I win I get my six dollars back, plus $\$7$ more.  I have a $74$% chance of winning. I intend to stop betting once I reach EV.  How many times must I make the bet before I can be $99$% sure of first reaching cumulative Expected Value?  In other words, assuming $n > 0$, how many rounds until we are $99$% sure of reaching $3.62n$ the first time? I intend to stop betting once I reach break-even.  How many times must I make the bet before I can be $99$% sure of breaking even for the series of bets the first time? This is not homework, I'm seeking a way to communicate a point on a discussion board about how making decisions according to EV is not always wise if you don't have access to many rounds of a +EV scenario.  I'm trying to figure out how to answer these questions for a variety of certainty levels (e.g. $99$%), probabilities (e.g. $74$%), and odds.",['probability']
261387,Application of the Hahn- Banach theorem,Let $E$ be a normed space and $F$ be a subspace of $E$. Show that $F$ is dense in $E$ if and only if all the linear and continuous functional on $E$ satisfying $f\vert _F=0 $ are identically zero ($f = 0$).,['functional-analysis']
261389,How to apply Borel-Cantelli Lemma?,"Assume that we are given a sequence of continuous functions $f_n(x)$ on $[0,1]$. How to show the existence of a sequence $a_n$ and a set $A$ with $\mu(A^c)=0$ so that $$ 
\lim_{ n\to \infty} \frac{f_n(x)}{a_n}=0, ~~ \forall x\in A.
$$ I choose a sequence $a_n$ such that $ \mu (\phi_n) \leq 1/2^n$ where
$$ 
\phi_n := \left\{ x: \frac{|f_n(x)|}{a_n} \geq \frac{1}{n} \right\}.
$$ Since $\sum_n \mu (\phi_n) < \infty$, using Borel-Cantelli Lemma we have $\mu(\limsup_n \phi_n)=0$. It seems okay if we say that $\limsup_n \phi_n = A^c$. How can we write it clearly in full details? Also, how can we assure the existence of $a_n$, how to construct such a sequence by means of $f_n(x)$? Thanks!","['lebesgue-integral', 'measure-theory', 'real-analysis']"
261404,On convergence of nets in a topological space,Let's consider a topological space that is not necessarily metrizable. Question: I wonder what is the motivation for defining convergence of nets in a topological space? What do we gain in working with convergence of nets rather than convergence of sequences?,"['general-topology', 'soft-question']"
261417,Solving $|x^2 - 1| - 1 = 3x - 2$ without graphing,"The book I'm working though has just showed how to find the intersection points of functions containing moduli by graphing them, visually spotting the intersection points, and then solving algebraically. I've done the exercises and am happy with that. Now it's showing another method to solve without graph sketching. I've taken a picture of the text: The first exercise is asking to solve the equation $|x^2 - 1| - 1 = 3x - 2$, so I start by adding 1 to both sides, squaring, and simplifying to get $x^4 - 11x^2 + 6x = 0$. I now have no idea what to do! Can someone give he any hints on how to approach this please?",['algebra-precalculus']
261420,Integration Techniques,"We just learnt the different types of integration techniques in school such as substitution, by parts, etc. But, these methods seem kind of laborious. Do professional mathematics and theoretical physicists use these techniques itself or do they use more faster methods ? I would like to know these methods.","['calculus', 'integration']"
261427,Exercise on derivatives from Rudin's Principles of Mathematical Analysis,"Exercise 5.26 --Rudin [Principle of Mathematical Analysis] Ex.5.26 If $f'$ exists on $[a,b]$, $f(a)=0$ and $\exists A\in\mathbb{R}\;(|f'|\le A|f|\,\text{on }[a,b])$, then $f = 0$ on $[a,b]$. Hint: Fix $x_0\in [a,b]$, let $M_0 = \sup |f|([a,x_0])$, $M_1=\sup |f'|([a,x_0])$.  Then $$|f(x)|\le M_1(x_0 -a)\le A(x_0-a)M_0$$ (for $x\in [a,x_0]$). Hence $M_0= 0$ so $f = 0$ on $[a,x_0]$ if $A(x_0 - a) < 1$. Proceed. Now I've done the above but I'm asked to assume $f(a) = y_0 > 0$. Show that $f(t)\le y_0*e^{A(t-a)}$. How do I take care of the case where $f(t) = 0$ for some $t > a$? I'm then asked to examine $\ln (f(t))$? Proof for 5.26: Assume $A>0$ (otherwise nothing to prove) and let $\beta = \sup \{c\in [a,b]: f([a,c]) = \{0\}\}$. 
Then $\beta \in [a,b],\; f([a,\beta]) = \{0\}$ since $f(a)=0$ and $f$ is continuous. we shall show $\beta = b$. If $\beta < b$, let $\gamma = \min(b,\beta+\frac{1}{A})$ and take $\beta_1\in (\beta, \gamma)$ then for $x\in [\beta,\beta_1]$ we have $|f(x)| = |f(x) - f(\beta)|\le M_1(x-\beta)\le A(\beta_1-\beta)M_0$ and $A(\beta_1-\beta) < A(\gamma-\beta)\le 1$ Where $M_0 = \sup |f|([\beta,\beta_1]),\; M_1 = \sup |f'|([\beta,\beta_1])$. Thus $M_0 \overset{(\star)}{=} 0$ but then we get
$f([a,\beta_1]) = f([a,\beta]\cup [\beta,\beta_1]) = \{0\}$ and $\beta < \beta_1 \in [a,b]$, contradicts to the def. of $\beta.\;\square$ $$(0\le M_0 \le sM_0 \wedge s<1)\implies (0\le (1-s)M_0 \le 0)\implies (M_0=0)\tag{$\star$}$$",['analysis']
261443,Question about cumulative hierarchy,"In the following let $\mathbf{V} = \bigcup_{\alpha \in \mathbf{ON}} V_\alpha$ denote the cumulative hierarchy. Let $\{\varphi_0, \dots, \varphi_n, \dots \}$ denote a list of all $ZF$ axioms. I am reading the following sentence: ""Given $n \in \omega$, the symbols ""$\mathbf{V} \models \{\varphi_0, \dots, \varphi_n\}$"" and ""$\varphi_0 \land \dots \land \varphi_n$"" stand for exactly the same thing."" (Just/Weese, p 192) I don't understand how this is true. The first expression says that $\varphi_i$ are all true in $\mathbf{V}$ given any valuation. The second expression seems to be a formula that may or may not be true but there is nothing saying that it is true in $\mathbf{V}$. Thank you for shedding light into my confusion.","['elementary-set-theory', 'model-theory']"
261471,Is this function measurable on $\mathbb{R}$. General methods?,"I'm trying to show the function $f:\mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x) =  \frac{1}{[x (\log(x))^2]}$  for $x\in (0,\frac{1}{2})$ and $0$ otherwise is measurable. Are there any general methods we can use? I understand the underlying $\sigma$-algebras of $\mathbb{R}$ are important. Would somebody mind speaking more to that? Thanks for any help.",['measure-theory']
261509,What's the Clifford algebra?,"I'm reading a book on Clifford algebra for physicists. I don't quite understand it conceptually even if I can do most algebraic manipulations. Can some-one teach me what the Clifford algebra really is? (Keep in mind that I don't know abstract algebra, nothing except some group theory.) 
Does it make sense to write the sum of a scalar and a bivector in the Clifford product? Both are very different things.","['intuition', 'abstract-algebra', 'mathematical-physics', 'linear-algebra', 'clifford-algebras']"
261517,What's the most efficient algorithm for Divisibility?,"What is the most efficient (in time complexity) algorithm known nowadays for the Divisibity Decision Problem: given two integers, say $a$ and $b$, does $a$ divide $b$? Let it be clear that what I ask for is not (necessarily) an algorithm for Remainder Calculation. I just want to know whether $a$ divides $b$ or not. Thanks and regards,
Leandro","['computational-complexity', 'divisibility', 'algorithms', 'number-theory']"
261521,Find if a point is in a circle,"I am coding a video game, but I am not so good at the math.  I am hoping for some help here: Given: $X, Y$ that is the center of the Circle $R$ that is the radius of the Circle $X_1, Y_1$ that may or may not be in the circle. The idea is that I have a tower that will shoot at a bad guy when it comes in range.  The bad guy has an $X_1, Y_1$ coordinate that will continually update. I need an equation I can run to see if the bad guy is in range.","['geometry', 'trigonometry', 'circles']"
261529,Topological Space in which every compact subset is metrizable,"Is there an (more or less) established name for the class of topological spaces in which every compact subset is metrizable? This is true for example in (LF)-spaces (inductive limits of Frechet-spaces) [see for example the article ""Metrizability of precompact subsets in (LF)-spaces"" by Cascales/ Orihuela].","['general-topology', 'terminology', 'functional-analysis']"
261530,Finding $E(|X_1|\ |\max |X_i| )$,"Suppose $X_1,\ldots,X_n$ are a random sample of $U(-\theta,\theta)$ with $\theta>0$. How can find
$$
E\left(|X_1|\ \big|\max |X_i| \right)
$$ that $1\leq i\leq n$?","['statistics', 'probability']"
261557,Intuitive explanations for Gaussian distribution function and mahalanobis distance,"I was wondering If anyone could give intuitive explanations for the multivariate Gaussian distribution function and mahalanobis distance? My professor didn't explain these in probability class, they were only defined... Where did the formula come from? Why is the Gaussian function the way it is? Is there a way to intuitively explain mahalanobis distance? Thank you for any support!!",['probability']
261564,How can every triangle have a circumcircle,"Let's take for example $\triangle ABC$ with $\angle A = \angle B = 1^o$. How can a triangle like this have a circumcircle? My confusion is with triangles like this in general, with very long sides.","['geometry', 'triangles']"
261574,A particular way of writing a polynomial.,"Notice that by  Taylor's theorem 
If the function $f : \mathbb{R}^{n} \rightarrow \mathbb{R}$  is $  k+1$ times continuously differentiable in the closed ball B, then one can derive an exact formula for the remainder in terms of (k+1)-th order partial derivatives of f in this neighborhood. Namely, \begin{align}& f( \boldsymbol{x} ) = \sum_{|\alpha|\leq k} \frac{\mathrm D^\alpha f(\boldsymbol{a})}{\alpha!} (\boldsymbol{x}-\boldsymbol{a})^\alpha + \sum_{|\beta|=k+1} R_\beta(\boldsymbol{x})(\boldsymbol{x}-\boldsymbol{a})^\beta, \\& R_\beta( \boldsymbol{x} ) = \frac{|\beta|}{\beta!} \int_0^1 (1-t)^{|\beta|-1} \mathrm D^\beta f \big(\boldsymbol{a}+t( \boldsymbol{x}-\boldsymbol{a} )\big) \, dt. \end{align} Until the degree two I Know that I can write the polynomial $  \sum_{|\alpha|\leq k} \frac{\mathrm D^\alpha f(\boldsymbol{a})}{\alpha!} (\boldsymbol{x}-\boldsymbol{a})^\alpha $ in this form
 \begin{equation}
f(0) + \mathrm Df(0) \cdot X + \frac{1}{2} X^{t} \mathrm D^{2}f(0)  X
\end{equation}
Can we write the polynomial above in a similar way for a degree grater than two?",['multivariable-calculus']
261578,Testing Convergence of $\sum \sqrt{\ln{n}\cdot e^{-\sqrt{n}}}$,"What test should i apply for testing the convergence/divergence of $$\sum_{n=1}^{\infty} \sqrt{\ln{n}\cdot e^{-\sqrt{n}}}$$ Help with hints will be appreciated.
Thanks",['sequences-and-series']
261605,Finding volume of a shape using double integral,"I'm trying to find the volume of a given shape:
$$ V= \begin{cases} \sqrt{x}+\sqrt{y}+\sqrt{z} \leq 1\\x\geq 0,\ y\geq 0,\ z\geq0\end{cases} $$
using double integral. Unfortunately I don't know how to start, namely:
$$ z = (1 - \sqrt{y} - \sqrt{x})^{2} $$
and now what should I do?
Wolfram can't even plot this function, I'm unable to imagine how it looks like... Would it be simpler with a triple integral?","['multivariable-calculus', 'integration']"
261613,The monotonicity of function $y=x/2 + x^{2}\sin(1/x)$ near $x=0$,"Is the function $y=x/2 + x^{2}\sin(1/x)$ monotonic near $0$? The derivative $f'$ obviously goes positive and negative near $0$, because $$f'(x)= \frac12 + 2x\sin(1/x) - \cos(1/x))$$ Does that mean that $f$ is not monotonic near $0$?","['derivatives', 'real-analysis']"
261617,Find the rotation axis and angle of a matrix,"$$A=\frac{1}{9}
\begin{pmatrix}
-7 & 4 & 4\\ 
4 & -1 & 8\\ 
4 & 8 & -1
\end{pmatrix}$$ How do I prove that A is a rotation ? How do I find the rotation axis and the rotation angle ?",['linear-algebra']
261622,"A homomorphism from $(\mathbb{Q},+)\rightarrow (\mathbb{Q},+)$","I claim that any group homomorphism will be of the form $f(x)=qx$ for some $q\in\mathbb{Q}$, now as $f$ is non zero so $q\neq 0$ now clearly kernel of $f$ will be $\{0\}$ only hence $f$ is injective, now let $f(1)=p\neq 0$ then for any $y\in\mathbb{Q}$ and $f(y/p)=y$ so $f$ is surjective too, hence bijective. am I right?",['group-theory']
261663,"Is $[0,1]$ a topological group?","Can one endow the unit interval $[0,1]$ with a group operation to make it a topological group under its natural Euclidean topology?","['general-topology', 'topological-groups']"
261670,seek a direct method to show $(a_k)\in l^2$,"Let $\{a_i\}, i=1, 2, \cdots$ be a nonincreasing sequence of positive numbers, and suppose $\sum_{k=1}^{\infty}\frac{a_k}{\sqrt{k}}<\infty~$, show that $\sum_{k=1}^{\infty}a_k^2<\infty.$ Could you give a direct proof by finding an inequality of the type $\sum_{k=1}^{\infty}a_k^2\leq f<\infty$, where $f$ is some expression related to the already known number $\sum_{k=1}^{\infty}\frac{a_k}{\sqrt{k}}$? Thanks in advance! Remark: I have solved this problem, but my method is indirect, I have used the uniformly bounded principal and the inequality $lim_{k\to\infty}\frac{1}{\sqrt{k}}\sum_{j=1}^ka_j=0,\forall (a_k)\in l^2$, so I really want to know how to solve the problem by finding a direct inequality.","['soft-question', 'inequality', 'real-analysis', 'analysis']"
261685,Paul Erdos's Two-Line Functional Analysis Proof,"Legends hold that once upon a time, some mathematicians were rather pleased about a 30-ish page result in functional analysis. Paul Erdos, upon learning of the problem, spent ten or so minutes thinking about the original problem, and came up with a two-line proof. I believe I read about this first in the biography The Man Who Loved Only Numbers , and it seems as though the internet maintains this legend (q.v. here , here , here ). This seems extraordinary, which leads me to some skepticism. I cannot seem to find any other reference to the actual proof or the problem. Is this simply an urban legend? Did it actually happen? What was the problem, and what was the original 30-page result?","['reference-request', 'math-history', 'functional-analysis']"
261688,Kakutani skyscraper is infinite,"Karl E. Petersen's book ""Ergodic Theory"", chapter 2, exercise 9, on page 56 Prove that for any ergodic measure preserving transformation $T:X\rightarrow X$ on a non-atomic probability space $(X, \mathcal B, \mu)$ there exists a set $A$ of positive measure for which the return time $n_A$ is unbounded. Notation:
Here $n_A$ is given by $n_A (x):= \min\{n: T^n(x)\in A \} $, so $n_A: A \rightarrow \mathbb N$ is an a.e. defined measurable map (by Poincare Recurrence) and it is the smallest integer such that the point $x\in A$ returns to $A$ under the action of $T$. By unbounded I suppose he means that the essential supremum of $n_A$ is not finite... I could really use some help here! Thanks so much","['probability-theory', 'measure-theory', 'ergodic-theory', 'dynamical-systems']"
261692,Labeled/unlabeled balls in unlabeled boxes,"I was hoping I could receive some clarification into the the four cases: Placing labeled balls in unlabeled boxes with repetition. Placing labeled balls in unlabeled boxes without repetition. Placing unlabeled balls in unlabeled boxes with repetition. Placing unlabeled balls in unlabeled boxes without repetition. I just want an intuitive understanding of the four cases.  How many ways are there to arrange, say, 4 balls into 9 boxes or 9 balls into 4 boxes in each case above, and why?","['permutations', 'balls-in-bins', 'intuition', 'combinatorics']"
261699,Average Value for an uncountable set.,Let $S$ be an uncountable set of indexed real numbers. So the same number can occur more than once in the set (although with a different index). I don't assume that there is any ordering on $S$. Is there a simple necessary and sufficient condition for $S$ to have a well-defined average? I take it that a sufficient condition for there to be a well-defined average is if all but a finitely many of the indexed numbers are identical. Is this necessary as well?,"['measure-theory', 'calculus']"
261704,Show that the direct sum of a kernel of a projection and its image create the originating vector space.,"I got the following question as my homework. Given $V$ is a vector space with $P \in \operatorname{End} V$. $P \circ P = P$ ( ""P is idempotent"" ). Show that $V = \operatorname{Ker} P \oplus \operatorname{Im} P$. One $P$ I can imagine is a projection from 3d-space to plane, that just sets some coordinates to zero. For example $\begin{pmatrix} x \\ y \\ z\end{pmatrix} \mapsto \begin{pmatrix}x \\ y \\ 0\end{pmatrix}$. Then $\operatorname{Ker} P$ would give the line $\begin{pmatrix} 0 \\ 0 \\ z\end{pmatrix}$ and $\operatorname{Im} P$ would contain all $\begin{pmatrix}x \\ y \\ 0\end{pmatrix}$. So the result of $\operatorname{Ker} P \oplus \operatorname{Im} P$ is of course $V$. But how do I prove that in a mathematical way?",['linear-algebra']
261709,How to solve integrals like Ramanujan,"If you try to prove Ramanujan-like identities involving integrals or series, there is a wide variety of techniques to approach those problems (such as contour integrals, Fourier transformation, Dirichlet series and much more). I wonder if there is a nice book that subsumes these techniques and offers a systematic approach to learn to solve hard integral identities. What would you recommend regarding this subject?","['reference-request', 'integration']"
261714,Data Type Classification in Statistics,"It is my understanding that in statistics one has 4 basic data types:
nominal, ordinal, ratio, and interval. I see cases where people refer to ""count data"" (which is a random variable whose range is the set of whole numbers, such as the number of accidents in a week or the number of passengers on a plane), which brings me to my question: is ""count data"" is really data . It seems to me that it is a statistic computed from nominal data for two reasons. First, it doesn't seem to fall into one of the four data type categories. Secondly, it is obtained not from a measurement or recording of single events but rather from an arithmetic operation (i.e. addition). So for example, the number of passengers in a plane is a statistic whch would be computed from the nominal data associated with each seat in the plane (""1"" = occupied, ""0"" = unoccupied). Matt","['statistics', 'data-analysis']"
261715,Comparison of 2 sets and the '+' operator in set theory,"I would like clarification on a set theory question I have. The question reads: Suppose $X$, $Y$ and $Z$ are sets: Does $X \times (Y +Z)=(X\times Y)+(X\times Z)$ (Where $\times$ is the cartesian product operator)? Now, the answer is already given as: $$X \times (Y+Z) = \{(x,(y,0))\mid x \in X, y \in Y\} \cup \{(x, (z,1)) \mid x \in X, z \in Z\}$$ $$(X \times Y) + (X \times Z) = \{((x,y),0) \mid x \in X, y \in Y\}\cup \{((x,z),1) \mid x \in X, z \in Z\}$$ I can see that by the rule of ordered pairs, these 2 sets are different. What I don't understand is where the union comes from and moreover the appearance of 0 & 1 in both of these sets? I believe it may be as simple as being unable to find clarification for what the '+' operator means, but any help with this is appreciated. Many thanks.","['discrete-mathematics', 'elementary-set-theory']"
261730,Applying Ergodic Theorem on fractional Brownian motion,"For a fractional Brownian motion $B_H$ consider the sequence for $p>0$
$$Y_{n,p}={1\over n}\sum\limits_{i=1}^n \left|B_H(i)-B_H(i-1)\right|^p.$$
By the Ergodic Theorem it is $$\lim\limits_{n\to\infty}Y_{n,p}=\mathbb{E}[|B_H(1)|^p] \ a.s.\text{ and in } L^1.$$
The Ergodic Theorem of Birkhoff says: Let $(\Omega,\mathcal{F},\mathbb{P},\tau)$ be a measure-preserving dynamical system, $p>0$, $X_0\in\mathcal{L}^p$ and $X_n=X_0\circ \tau^n$. If $\tau$ is ergodic, then it holds
  $${1\over n}\sum\limits_{k=0}^{n-1}X_k\overset{n\to\infty}{\longrightarrow}\mathbb{E}[X_0]\ a.s.\text{ and in }L^1.$$ My problem is that I don't know how this theorem is used on the case described above, i.e. what is $\tau$ and what is $X_n$ in this case? Another question is: Why do I have to use the ergodic theorem while by using the stationarity I have $$Y_{n,p}\sim {1\over n}\sum\limits_{i=1}^n |B_H(1)|^p=|B_H(1)|^p\ ?$$ I would be thankful for every help and explanation. Maybe I should add saying that I am trying to understand how to prove that a fractional Brownian motion is not a semi-martingale for $H\neq {1\over2}$, by applying these statements.","['probability-theory', 'stochastic-calculus', 'brownian-motion']"
261747,Dimension of the irreducible components of an affine open in $\mathbb{P}^n_k$.,"I was doing some exercises in Liu's book on Algebraic Geometry. I am currently trying to solve a problem by showing the following: Let $U \subset \mathbb{P}^n_k$, k  a field, be an affine open subset. Show that the irreducible components of $\mathbb{P}^n_k-U$ all have dimension n-1. I would appreciate any help / hint here. I have some problems understanding $\mathbb{P }^n_k$ at a deep (even semi-deep) level. I suspect that one could maybe show that the dimension of such an affine open should be of dimension n (or am I wrong here?), since we can compute the dimension of X on any open set. We should be able to write the complement as $V_+(I)$ for some homogenous ideal . However, I don't see how to get from this to that the irreducible components of the component has dimension n-1. Thank you for looking at my question and please forgive me if it's a naive one.","['algebraic-geometry', 'projective-schemes']"
261757,How to find the limit of two divided functions,"How can I find a $c$ such that $f_{2}(n) \leq c \cdot f_{3}(n)$? where $f_{2}(n) = 2n + 20$ and $f_{3}(n) = n + 1$. This was from the textbook, Algorithms (explaining something else), but I was wondering how they got the following: $$\frac{f_{2}(n)}{f_{3}(n)} = \frac{2n+20}{n+1} \leq 20$$",['functions']
261769,"Simple Power Series Expansion for Problems similar to $f = (1 + \epsilon \,x)^{1/\epsilon}$","I was flicking through a book on perturbation methods and saw a simple question asking the reader to expand the following expression for $f$ in a power series (up to the first 2 terms): $f = (1 + \epsilon \,x)^{1/\epsilon}$, where $\epsilon$ is a small parameter. I'm sure this is very simple, but I wasn't certain about the best way to approach this. A quick look at mathematica tells me the solution is $e^x - \frac{1}{2} (e^x x^2) \,\epsilon + ...$. How would I go about getting this answer - and more importantly, how would I systematically find series expansions for problems similar to this one?",['power-series']
261783,"Distribution of $(XY)^Z$ if $(X,Y,Z)$ is i.i.d. uniform on $[0,1]$","$X,Y$ and $Z$ are independent uniformly distributed on $[0,1]$ How is random variable $(XY)^Z$ distributed? I had an idea to logarithm this and use convolution integral for the sum, but I'm not sure it's possible.","['probability-theory', 'probability-distributions']"
261787,"Showing that $\nu \ll \mu$ implies $\forall \epsilon > 0$, $\exists \delta > 0$ s.t. $\mu(A) < \delta \implies \nu(A) < \epsilon$","I am stuck on what I think may be the very last line of the proof I am seeking. Let $(X, \mathcal{B})$ be a measurable space which has associated with it the finite measures $\mu$ and $\nu$ s.t. $\nu \ll \mu$.  I aim to show that $\forall \epsilon > 0$, $\exists \delta > 0$ s.t. $\forall A \in \mathcal{B}$, $$\mu(A) < \delta \implies \nu(A) < \epsilon$$ Fix $\epsilon > 0$. For all $n \in \mathbb{N}$, let $\delta_n = \frac{1}{n^2}$. For all $n \in \mathbb{N}$, let $A_n \in \mathcal{B}$ s.t. if $\exists E \in \mathcal{B}$ s.t. $\mu(E) < \delta_n$ and $\nu(E) > \epsilon$, then set $A_n = E$.  Otherwise, set $A_n = \emptyset \in \mathcal{B}$. Suppose for sake of contradiction that $|\{A_n\}| = \infty$, so that no matter the $\delta > 0$, we could find a $\delta_n = \frac{1}{n^2}  < \delta$ which has associated with it a measurable $A_n \ne \emptyset$ with $\mu(A_n) < \delta_n < \delta$ and $\nu(A_n) > \epsilon$. Now if we let $\underset{n \rightarrow \infty}{\text{limsup}}$ $A_n = S$, we have (from a prior problem) that $\mu(S) = 0$ since $\mu$ is a finite measure and $\sum_{n=1}^\infty \mu(A_n) \le \sum_{n=1}^\infty \delta_n = \sum_{n=1}^\infty \frac{1}{n^2} < \infty$.  Since $\nu \ll \mu$ we therefore have $\nu(S) = 0$ as well. Yet $\nu(S) = \nu(\bigcap_{n=1}^\infty \bigcup_{n=m}^\infty A_m) \ge \epsilon > 0$ since... and it's here where I'm stuck in the proof.",['measure-theory']
261801,How can you explain the Singular Value Decomposition to non-specialists?,"In two days, I am giving a presentation about a search engine I have been making the past summer. My research involved the use of singular value decompositions, i.e., $A = U \Sigma V^T$ . I took a high school course on Linear Algebra last year, but the course was not very thorough, and though I know how to find the SVD of a matrix, I don't know how to explain what I have in my hands after the matrix has been decomposed. To someone who has taken linear algebra, I can say that I can decompose a matrix $A$ into matrix $\Sigma$ , whose diagonal holds the singular values, and matrices $U$ and $V$ whose columns represent the left and right singular vectors of matrix $A$ . I am not sure how to explain what a singular value or what left/right singular vectors are. I can still be satisfied if there is no easy way to explain what this decomposition means, but I always prefer keeping the audience as informed as possible.","['matrix-decomposition', 'svd', 'matrices', 'linear-algebra', 'singular-values']"
261830,Computing gradient in cylindrical polar coordinates using metric?,"I am trying to understand coordinate transformations properly (having
studied some general relativity in the past). Let us consider the transformation from cartesian to cylindrical coordinates,
$x=\rho\cos\varphi$, $y=\rho\sin\varphi$, $z=z$. I know that the
line element (invariant at coord transformations) is simply defined
as 
$$
ds^{2}=g_{ij}dx^{i}dx^{j}=\tilde{g}_{ij}d\tilde{x}^{i}d\tilde{x}^{j}
$$
where the non-tilde quantities belong to one coordinates (cartesian
in my example) and the tilde ones to another (cylindrical in my example).
The cartesian coordinates are simply
$$
ds^{2}=dx^{2}+dy^{2}+dz^{2}
$$
where the metric is $g_{ij}=diag(1,1,1)$ and the inverse metric is
$g^{ij}=g_{ij}$. Simply using total differentials I can compute that
$dx^{2}=\left[d(\rho\cos\varphi)\right]^{2}$ and do the derivatives
etc. This way, I obtain
$$
ds^{2}=d\rho^{2}+\rho^{2}d\varphi^{2}+dz^{2}
$$
which means $\tilde{g}_{ij}=diag(1,\rho^{2},1)$ and $\tilde{g}^{ij}=diag(1,\frac{1}{\rho^{2}},1)$.
So far, so good. Now I was wondering how I can use the metric in order to obtain expressions
for simple things, for instance the gradient of a scalar field $\phi$.
In my thinking, it should simply be
$$
\nabla\phi=\left(\partial^{i}\phi\right)\mathbf{e}_{\left(i\right)}=\tilde{g}^{ij}\left(\partial_{i}\phi\right)\mathbf{e}_{\left(j\right)}=\tilde{g}^{\rho\rho}\left(\partial_{\rho}\phi\right)\mathbf{e}_{\rho}+\tilde{g}^{\varphi\varphi}\left(\partial_{\varphi}\phi\right)\mathbf{e}_{\varphi}+\tilde{g}^{zz}\left(\partial_{z}\phi\right)\mathbf{e}_{z}.
$$
where $\mathbf{e}_{\left(i\right)}$ represents the $i$-th unit vector,
e.g. $\mathbf{e}_{\left(2\right)}=\mathbf{e}_{\varphi}$. The result
should then be 
$$
\nabla\phi=\frac{\partial\phi}{\partial\rho}\mathbf{e}_{\rho}+\frac{1}{\rho^{2}}\frac{\partial\phi}{\partial\varphi}\mathbf{e}_{\varphi}+\frac{\partial\phi}{\partial z}\mathbf{e}_{z}
$$
which is wrong. Computing other expressions, like the Laplacian, gets things considerably
more wrong. My thinking about using the metric in this way must be
flawed. Can you help me out and rectify this?","['coordinate-systems', 'polar-coordinates', 'differential-geometry']"
261835,Probability - Balls and Buckets; variance question,"I've been working on this problem for a while and its giving me no end of trouble! The question is this: Suppose we have 2k buckets, numbered 1 through 2k. We throw x black balls and y white balls, at random, towards the buckets. A black ball has twice the chance of landing in an odd bucket as landing in an even one, and a white ball has twice the chance of landing in an even bucket as landing in an odd one. Aside from this, the chances of a ball falling into a certain bucket is evenly distributed (i.e. a black ball is just as likely to land in bucket 1 as bucket 15, etc).
I'm trying to compute $E(z)$ and $Var(z)$, where $z$ is the number of buckets that have at least one ball in them. It seems to me that it may be easier to look at the chance that bucket i is empty, as follows: Pr(bucket i is empty|bucket is odd) = $(\frac{3k-2}{3k})^x(\frac{3k-1}{3k})^y$, and Pr(bucket i is empty|bucket is even) = $(\frac{3k-2}{3k})^y(\frac{3k-1}{3k})^x$. If we let $Z_i$ equal 1 if the bin is empty and 0 otherwise, our expected value should be $((\frac{3k-2}{3k})^x(\frac{3k-1}{3k})^y)\cdot k$ $+$ $((\frac{3k-2}{3k})^y(\frac{3k-1}{3k})^x)\cdot k$. This expression seems really unwieldy though, and calculating variance and covariance with it seems an almost insurmountable task - am I going about this in an incorrect fashion?
Thanks so much for your help! Edit: Attempting to continue down this path, running into some issues? If I go along with this, I'm looking for $\sum E(z_i^2) + \sum\sum_{i\neq j}E(z_iz_j)+E(z)^2$, where the first sum is equal to $\sum E(z_i)$, since $z$ takes on values either 1 or 0. There are four cases for the covariance term in the middle: i is odd, j is odd; i is odd, j is even; i is even, j is odd; i is even, j is even. If i and j are both even or odd, there are $k^2$ possibilities for [both odd] and for [both even]. Otherwise, there are $k^2-k$ possibilities for each case. So I must calculate probabilities of four cases: 1.) Given: i is odd, j is odd Pr(no balls in i)$x$Pr(no balls in j|no balls in  i): $\large (k^2-k)*((\frac{3k-2}{3k})^x\cdot (\frac{3k-1}{3k})^y\cdot (\frac{3k-4}{3k-2})^x\cdot (\frac{3k-2}{3k-1})^y)$ The first half of the expression is just the probability that nobody gets off at floor i. The second half needs to take into account that one of the odd buckets will not have balls in it. The rest of the cases are basically the same, with minor tweaks. Assuming this is correct, do I just attempt to plug in these expressions in the variance expression above and do my best to simplify? thanks for the help!","['balls-in-bins', 'probability']"
261839,Sequence of analytic functions,"Let $G,H$ be disjoint open subsets of $\mathbb{C}$ and $f_n:G\to H$ be analytic functions. If $f_n(z)\to f(z)$ for all $z\in G$, then prove that $f$ is analytic and $f(G)\subset H$. Any help is appreciated.",['complex-analysis']
261840,"Finding the order of an element in $GL(2,\mathbb{R})$","I am working on a problem involving basic abstract algebra/group theory and am getting confused. I am following an online course by Dr. Bob found here, and am currently on assignment two . My difficulty lies with problem 1b where I am given a matrix $A=$
$\left( \begin{array}{ccc}
0 & -1 \\
1 & 0 \\ \end{array} \right)$ and asked to find its order. Now I am fairly sure that matrix multiplication is not commutative so this makes me suspect that there are either multiple answers or a convention we must adopt (which I dont think he mentioned). If I multiply on the right I get $A\cdot A = -I, A^3 = A\cdot A^2 =
\left( \begin{array}{ccc}
0 & 1 \\
-1 & 0 \\ \end{array} \right)$, and $A^4 = A\cdot A^3 = I$ so $|A| =4$. Now when I do this on the by multiplying on the left by $A$ I get the same answer, but my intuition says this is a coincidence because of the trivial chosen matrix. Is it true in general that the order of elements in $GL(2,\mathbb{R})$ is the same regardless of which side you multiply on, or are there criterion when this property holds? Finally, since I'm guessing that this is just a special case situation, which side do I multiply on when asked to find the order of an element? Thanks for the help!","['linear-algebra', 'group-theory', 'abstract-algebra']"
261850,"Commutators, and Christoffel symbols in a non holonomic basis","I have a frame that varies along a curve $\gamma$ : the frame consists in the tangent vector of the curve plus some constant non orthogonal vectors. I need to compute $\nabla_{\dot\gamma}{\dot\gamma}$. Apparently, the Christoffel symbols in that case are computed with : $$\omega^i{}_{k\ell}=\frac{1}{2}g^{im} \left( g_{mk,\ell} +
 g_{m\ell,k} - g_{k\ell,m} + c_{mk\ell}+c_{m\ell k} - c_{k\ell m} 
 \right)\,$$ where $c_{k\ell m}=g_{mp} {c_{k\ell}}^p\ $ are the commutation
  coefficients of the basis; that is, $[\mathbf{u}_k,\mathbf{u}_\ell] = c_{k\ell}{}^m \mathbf{u}_m\,\ $ ( wiki ) I have several questions regarding the commutation coefficients: I read that the commutator is defined by $[X,Y]=\nabla_X Y - \nabla_Y X$ : isn't it a catch 22? how is it possible to compute that commutator, knowing that the connection $\nabla$ is obtained by the Christoffel symbols that uses the commutator that uses the connection that uses [...] ? For one of my applications, my tangent space is made of symmetric matrices (and the manifold has a fancy metric $g$) : in that case, is the commutator only defined by $[X,Y]=X*Y-Y*X$ with $*$ the standard matrix multiplication ? For another application, my tangent space are vectors in $R^N$ and my space is Euclidean. Of course, in that case there are probably easier ways to handle that, but I'd still like to understand what would the commutator be in that case. In the definition $[\mathbf{u}_k,\mathbf{u}_\ell] = c_{k\ell}{}^m \mathbf{u}_m\,\ $ , is-it necessary to solve a linear system to get the coefficients $c_{k\ell}{}^m$ or can I just project $[\mathbf{u}_k,\mathbf{u}_\ell]$ on $\mathbf{u}_m$ using the metric $g$ (my basis is not orthogonal, so I guess the linear system is needed?). Thanks!","['riemannian-geometry', 'derivatives', 'differential-geometry']"
261857,Normal subgroup of prime index and another subgroup,"Suppose that $N$ is a normal subgroup of a finite group $G$, and $H$ is a subgroup of $G$. If $|G/N| = p$ for some prime $p$, then show that $H$ is contained in $N$ or that $NH = G$. I imagine this is related to the fact that $|NH| = |N||H|/|N \cap H|$, but this is not really helping me. I considered the fact that since $N$ is normal, we get that $NH \leq G$, and I then used Largrange, but I'm stuck, and some help would be nice.","['group-theory', 'abstract-algebra']"
261869,Compactification problem.,"How do I compactify  the curve $Q(x,y)=0$ in $\mathbb{P}^1\times\mathbb{P}^1$ where $Q$ is a polynomial  ?","['riemann-surfaces', 'algebraic-geometry', 'reference-request']"
261886,"limit of Holder norms: $\sup\limits_{x\in [a,b]} f(x) = \lim\limits_{n\rightarrow\infty} \left(\int_a^b (f(x))^n \;dx\right)^{\frac{1}{n}}$ [duplicate]","This question already has answers here : If $f(x)$ is continuous on $[a,b]$ and $M=\max \; |f(x)|$, is $M=\lim \limits_{n\to\infty} \left(\int_a^b|f(x)|^n\,\mathrm dx\right)^{1/n}$? (2 answers) Closed 6 years ago . Show that $$\sup_{x\in [a,b]} f(x) = \lim_{n\rightarrow\infty} \left(\int_a^b (f(x))^n \;dx\right)^{\frac{1}{n}}$$ for f continuous and positive on [a,b].  I can show that LHS is greater than or equal to RHS but I can't show the other direction.","['calculus', 'integration', 'real-analysis']"
261889,Expectation of Stopping Time w.r.t a Brownian Motion,"How do you take the expectation of a stopping time with respect to a Brownian motion?  The specific question is: $$
\tau = \inf\{ t \ge 0: B(t) \in \{-a, b\}\}
$$ I understand the optional stopping theorem tells us that $E[M_\tau ] = E[M_0]$ but how do I use that to find the expectation?","['probability-theory', 'stochastic-processes', 'brownian-motion']"
261936,Examples of fields of characteristic 1,"I'm a beginner in this topic, so this question seems stupid, but I think it's a doubt many beginners can have. I realized that I can't find examples of fields with characteristic 1, and as 1 is a prime number we can try to find examples of such fields. I find a little bit strange this kind of field, it seems that it can have only the zero element, then it can't exist such fields I said something wrong? Thanks","['abstract-algebra', 'field-theory']"
261946,"3-D geometry : three vertices of a ||gm ABCD is (3,-1,2), (1,2,-4) & (-1,1,2). Find the coordinate of the fourth vertex.","The question is Three vertices of a parallelogram ABCD are A(3,-1,2), B(1,2,-4) and C(-1,1,2) . Find the coordinate of the fourth vertex. To get the answer I tried the distance formula, equated AB=CD and AC=BD.","['geometry', '3d']"
261948,Inequality involving expectation,"$X,Y$ are two RV taking values in $[1,+\infty)$. Do we have following inequality? $$
E\left[\frac{Y}{X}\right]\geq\frac{E[Y]}{E[X]}
$$","['probability-theory', 'probability']"
261956,bilinear map and differentiability [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question let $f\colon \mathbb R^{2}\times\mathbb R^{2} \to \mathbb R$ be a bilinear map,i.e,linear in each variable separately.Then for $(V,W)$ in $\mathbb R^{2}\times \mathbb R^{2}$,the derivative $Df(V,W)$ evaluated on $(H,K)$ in $\mathbb R^{2}\times \mathbb R^{2}$ is given by 1.$f(V,K)+f(H,W)$ 2.$f(H,K)$ 3.$f(V,H)+f(W,K)$ 4.$f(H,V)+f(W,K)$","['bilinear-form', 'derivatives']"
261970,Counterexample sought to show that squared Hellinger distance doesnt satisfy the triangle inequality,"I read in a paper that the squared Hellinger distance between two densities $f$ and $g$ $$H^2(f,g)=\frac{1}{2}\int \left(\sqrt{f(x)}-\sqrt{g(x)}\right)^2 dx$$ is not a metric. I wonder if there is a nice counterexample showing this. Thanks in advance.","['metric-spaces', 'probability', 'functions']"
