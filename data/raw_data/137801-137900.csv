question_id,title,body,tags
2201188,"Find 2 basis $M=\left\{v_1,v_2,v_3\right\}$ and $N=\left\{w_1,w_2,w_3\right\}$ such that $T_{MN(f)}$","$A=\begin{pmatrix} 1 & 3 & -1\\ 
-1 & 0 & -2\\  1 & 1 & 1 \end{pmatrix}$ is a real matrix and $f: \mathbb{R}^3 \rightarrow \mathbb{R}^3, \text{ } f(x)= A \cdot x$ is a
  linear mapping. Find two basis $M= \left\{v_1,v_2,v_3\right\}$ and $N=
\left\{w_1,w_2,w_3\right\}$ of $\mathbb{R}^3$ such that $T_{MN}(f) =
\begin{pmatrix} 1 & 0 & 0\\  0 & 1 & 0\\  0 & 0 & 0 \end{pmatrix}$ I have absolutely no idea how to do this task, nor understand the notation $T_{MN}$. Maybe I could do it if I  would understand the notation. I hope some will explain me what it means / how this could be solved? If it matters, $Ker(f)=\left\{\begin{pmatrix}
-2z\\ 
z\\ 
z
\end{pmatrix} \mid z \in \mathbb{R}\right\}$ and $Im(f)= span\left(\left\{\begin{pmatrix}
1\\ 
-1\\ 
1
\end{pmatrix},\begin{pmatrix}
0\\ 
-3\\ 
2
\end{pmatrix}\right\}\right)$ (This is no homework!) I will set a bounty on this question of 200 rep because I really want understand it.","['matrices', 'linear-algebra', 'linear-transformations']"
2201239,Does $1 \cup 2$ has any meaning?,"Does $1 \cup 2$ has any meaning? In this answer I used something similar, but it was pointed out $1 \cup 2$ is not same as $\{1,2\}$ Is this only due to convention or is there a deeper reason for it? Added what about $a \cup b \equiv\{a,b\}$?","['notation', 'soft-question', 'elementary-set-theory']"
2201287,What is the dual of a sum of Banach function spaces,"Given two Banach function spaces, e.g. $L^1(\mathbb{R})$ and $L^2(\mathbb{R})$, define the sum $X=L^1+L^2$ as the class of functions $f=f_1+f_2$ having $f_i\in L^i$. Define a norm by
$$\|f\|_X=\inf\{\|f_1\|_{L^1}+\|f_2\|_{L^2}; f=f_1+f_2\} $$
where the infimum is taken over all decompositions $f=f_1+f_2$. Is there any (simple) way to see what the dual of $X$ is? As far as I can see it must be a subset of $L^\infty\cap L^2$. Could it be that this is actually the dual space?","['functional-analysis', 'lp-spaces', 'duality-theorems']"
2201309,How to calculate congruence modulo using Euclid's algorithm?,"How to find all $x\in\mathbb{Z}$ that satisfy this congruence modulo: $15x\equiv 5 \mod25$. So, I know that first thing I need to check is if $\gcd(a,m)$ is dividing $b$. In our case $a$ is $15$, $b$ is $5$ and $m$ is $25$, therefore $\gcd(15,25) = 5$. We see that our $b$ can be divided by $\gcd$, so we can divide whole congruence modulo with 5, and we get this: $3x\equiv 1 \mod5$. We can also write this as $3x + 5k = 1$. After Euclid's algorithm and making the table, I get this. $\begin{array}{|c|c|c|c|}
\hline
a_i&1& 0& 1& -1 \\ \hline
 b_i&0& 1& -1& 2\\ \hline
 q_i&&  & 1& 1& 2\\ \hline
 &5& 3& 2& 1& 0\\ \hline
\end{array}$ From this I am not sure how should I precede. I can confirm $3x + 5k = 1$ by putting last $a$ and $b$ instead of $k$ and $x$, but I really don't have idea what should I do next. Any suggestion would be helpful.","['congruences', 'modular-arithmetic', 'discrete-mathematics']"
2201325,$\phi \mapsto \lim_{\epsilon \to 0} \int_{\mathbb{R}^n} \frac{\phi (x)}{\left \Vert x \right \Vert-1 + i\epsilon}$ is a distribution,"I want to show that
$$\phi \mapsto \lim_{\epsilon \to 0} \int_{\mathbb{R}^n} \frac{\phi (x)}{\left \Vert x \right \Vert-1 + i\epsilon}$$
is a distribution for $x \in \mathbb{R}^n$, $\phi \in \mathcal{S}(\mathbb{R}^n)$ in the Schwartz space and $\left \Vert \cdot \right \Vert$ the euclidean norm. I think that for the one dimensional case this should follow from the theory of Cauchy principle values but I'm not sure.","['real-analysis', 'schwartz-space', 'limits', 'distribution-theory', 'functional-analysis']"
2201327,Show that $V^* \neq \{0\}$ if $V \neq \{0\}$ normed space?,Suppose $V$ be a non zero normed space( possibly infinite dimensional). Can we prove that $V^*$ the space of continuous linear functional is non zero without using Hahn Banach theorem?,"['functional-analysis', 'normed-spaces', 'linear-algebra']"
2201330,Trace of an inverse inequality $\text{Tr}(A^{-1}) \ge n^2 \text{Tr}(A)^{-1}$,"Let $A \in \mathbb{R}^n$ be a positive definite matrix. Then, it is well known that $$ \mbox{Tr} \left( A^{-1} \right) \ge n^2 \, \mbox{Tr}(A)^{-1} $$ The proof follows by using the fact that trace is and a sum of eigenvalues and using AM-GM inequality. My question: Does this inequality hold with equality iff and only if $A$ is a diagonal matrix? I know also that this inequality holds with equality iff eigenvalues of $A$ are identical.   But not sure of this implies that $A$ is a diagonal matrix.","['matrices', 'trace', 'positive-definite', 'linear-algebra']"
2201336,Which isomorphism of coordinate rings corresponds to isomorphisms of affine varieties?,"Consider the following claim: Let $X \subset k[x_1,\dots,x_n]$ and $Y\subset k[y_1,\dots,y_m]$ be algebraic sets and suppose that we have a ring isomorphism $\varphi: \mathcal{O}_Y \to \mathcal{O}_X$ . Show that the algebraic sets $X$ and $Y$ are isomorphic. Now, I know that if $\varphi$ is an isomorphism of $k$ -algebras, then $X$ and $Y$ are isomorphic. And I know that any isomorphism of $k$ -algebras is also an isomorphism of the underlying rings. Question: However, isn't it the case that not every ring homomorphism $\varphi: \mathcal{O}_Y \to \mathcal{O}_X$ is a $k$ -algebra homomorphism $\mathcal{O}_Y \to \mathcal{O}_X$ , and that only $k$ -algebra homomorphisms $ \mathcal{O}_Y \to \mathcal{O}_X$ correspond to morphisms $X \to Y$ , not arbitrary ring homomorphsims $\mathcal{O}_Y \to \mathcal{O}_X$ ? In other words, which of the following two statements are correct? Are they even mutually exclusive? Or are they equivalent? If they are equivalent, why? Two algebraic sets are isomorphic if and only if their coordinate rings are isomorphic as rings. OR Two algebraic sets are isomorphic if and only if their coordinate rings are isomorphic as $k$ -algebras. Context: My book , in sections 4.18 and 4.19, as well as this question on Math.SE implies that the first statement is true. However, section 4.8 of the same book , and every other source I have found (e.g. here or here ), implies only that the second statement is true. Moreover, I was only able to prove the second statement, not the first. Which is correct? This is a follow-up to my previous question , an attempt to show that the two statements are mutually exclusive and not equivalent. It was unanswered though, so I don't know if my attempt was successful -- for all I know, the two statements could be equivalent. If so, why? EDIT: What I showed in my notes, and was agreed to be true in the comments is that: There is a one-one correspondence between morphisms $X \to Y$ and ring homomorphisms $\mathcal{O}_Y \to \mathcal{O}_X$ . ( FALSE ) There is a one-one correspondence between morphisms $X \to Y$ and $k$ -algebra homomorphisms $\mathcal{O}_Y \to \mathcal{O}_X$ . ( TRUE )","['algebraic-geometry', 'abstract-algebra', 'reference-request', 'ring-theory', 'commutative-algebra']"
2201345,How to find the number of solutions to a system of equations with one parameter which cannot be reduced?,"I have the system of linear equations where $k$ is the parameter:
$$
\begin{cases} 
x+ky+z = 0 \\
kx+y+kz=0 \\
(k+1)x-y+z=0
\end{cases}
$$ For which the matrix is:
$$
\begin{bmatrix} 1 & k & 1 \\
k & 1 & k \\
k+1 & -1 & 1
\end{bmatrix}
$$ I managed to reduce the matrix to:
$$
\begin{bmatrix} 1 & k & 1 \\
0 & 1-k^2 & 0 \\
0 & 2+k & k
\end{bmatrix}
$$ However, I don't see how this can be reduced further. As far as I know I need the echelon form to be able to determine how many solutions exist. Should I just plug in $k=-2$ in order to reduce it further? If yes then what can be said about the number of solutions if $k \neq =2$? EDIT: I'm only beginning linear math course so I can't use any advanced techniques (like determinants etc.)","['linear-algebra', 'systems-of-equations']"
2201347,Smallest eigenvalue is supremum for...,"Suppose $A \in \mathbb{R}^{nxn}$ symmetric and positive definit. We can show that:
$$\exists \epsilon > 0 \in \mathbb{R}: \forall x \in \mathbb{R}^n: x^TAx \ge \epsilon \Vert x \Vert^2$$ My claim is that the smallest eigenvalue of $A$ is the supremum for those $\epsilon$. I'd appreciate to see a prove/disprove!","['matrices', 'proof-writing', 'positive-definite', 'linear-algebra']"
2201386,Does the limit exist when there is a removable discontinuity but the function takes a different value there?,"I know that when there is a hole the limit still exists because the limit is asking what value the function approaches, but does the limit still exist at the hole when the function takes a different value? In the graph on the right is the limit at $x=c$ equal to $L$?","['calculus', 'limits']"
2201391,What is the expected number of flips of an unfair coin until you have 2 more heads than tails?,"$p$ is the probability of heads. Note if $p \le 0.5$, the answer is infinity, so assume $p > 0.5$. What is the expected number of flips of the coin where we have 2 more heads than tails? Note you would stop flipping the coin when you first encounter the situation where you have 2 more heads than tails.","['combinatorics', 'probability']"
2201447,Computing $ \int_0^1 \frac{1 + 3x +5x^3}{\sqrt{x}}\ dx$,$$ \int_0^1 \frac{1 + 3x +5x^3}{\sqrt{x}}\ dx$$ My idea for this was to break each numerator into its own fraction as follows $$ \int_0^1 \left(\frac{1}{\sqrt{x}} + \frac{3x}{\sqrt{x}} + \frac{5x^3}{\sqrt{x}}\right)dx$$ $$ \int_0^1 (x^{-1/2} + 3x^{1/2} +5x^{5/2})\ dx $$ $$ \int_0^1 2x^{1/2} + 2x^{3/2} + \frac{10}{7}x^{7/2} $$ Not really sure where to go from there. Should I sub 1 in for the x values and let that be the answer?,"['integration', 'fractions']"
2201465,How to prove my matrix's powers?,"Being bored I played around with matrices I stumbled upon this matrix. $$T = \left[\begin{array}{ll|cc}
0&2&0&0\\
\frac{1}{2}&0&0&0\\
\hline 1&0&1&0\\
1&1&0&1
\end{array}\right]$$ What is $T^{2^k}$ for $\cases{k\in \mathbb N\\k>1}$? I have my suspicions what it should be, but how can we prove it?","['matrices', 'abstract-algebra', 'proof-writing', 'representation-theory', 'linear-algebra']"
2201496,"If $\dfrac {1}{a+b} +\dfrac {1}{b+c}=\dfrac {3}{a+b+c}$, prove that $\angle B=60^\circ $","If $\dfrac {1}{a+b}+\dfrac {1}{b+c}=\dfrac {3}{a+b+c}$, prove that $\angle B=60^\circ$. My Attempt 
$$\dfrac {1}{a+b}+\dfrac {1}{b+c}=\dfrac {3}{a+b+c}$$
$$\dfrac {a+2b+c}{(a+b)(b+c)}=\dfrac {3}{a+b+c}$$
$$a^2-ac-b^2+c^2=0$$. How to prove further?","['algebra-precalculus', 'trigonometry']"
2201592,What makes a function not defined?,I have started studying precalculus and would then start up with calculus. While studying about functions I wondered whether this function would be defined at $a$ or not. Take a look at it. $$ f(x) = \frac{(x-a)(x-b)(x-c)...(x-n)}{(x-a)} $$ Here if we will simplify it further then the term $\left( x-a\right)$ would cancel out making the function defined at $a$ but if we would leave it as such it would be undefined at that point. I asked this question because I found in some sources that the graph of such functions have an open dot at that point indicating it discontinuous at that point. But I couldn't explain it. Are the expressions before and after cancelling different or it's something else? I would be highly obliged for your help and thanks ...,"['algebra-precalculus', 'continuity', 'functions']"
2201638,Doubt about Cauchy-Lipshitz theorem use,"I'll show my doubt with the Cauchy problem $\begin{cases} y'=1+y^2  \\ y(0)=0 \end{cases}$. I know the solution is $y(t)=tg(t)$, but let's say I don't know the explicit solution. If I look at $1+y^2$, then I have all the hypothesis for the application of Cauchy-Lipshitz Theorem in $[0, 2\pi]$ for instance, because it is lipshitz there (or not?). So I have a unique solution in $[0,2\pi]$, but now if I wonder if I can prolong the solution, how can I understand that in fact I cannot prolong the solution to $2\pi$ but at best to $\pi/2$ without solving  the differential equation?
I mean, where do I commit a mistake in using the Cauchy-Lipshitz Theorem? If you have some other (not simple like mine) examples that have the same problem, I would like to see how to solve them in the right way. Thanks for the help.","['cauchy-problem', 'ordinary-differential-equations']"
2201644,Prove a finite-dim subspace of a Banach space is always topologically complemented.,"Let $X$ be a Banach space with a finite-dimensional subspace $F$, then we want to show there is a closed subspace $Y$ of $X$ such that $X = Y \oplus F$. My attempt is to first just consider algebraically complement (i.e. just in the sense of vector space rather than topological vector space) of $F$ and I try to prove the algebraic projection $\pi: X \to F$ is continuous using the closed graph theorem, but I get stuck.","['functional-analysis', 'vector-spaces']"
2201691,Exponential tail bound on a growing sum of iid random variables,"I'm trying to derive an exponential tail bound like $P(X>t)<ae^{-bt}$ on a sum of independent random variables, except where the number of items in the sum grows with $t$. I can't figure it out. More specifically, let $\{Y_i\}_{i \geq 1}$ be a sequence of iid positive integer-valued random variables. Assume that the $Y_1$ has a finite moment generating function in some neighborhood around 0. For some constants $c,c'$ I want a bound like: $$P \left( \sum_{i=1}^{[f(t)]} Y_i > t\right) < c' \cdot \exp(-ct)$$ Question : without imposing any more assumptions, is there an easy way to see how fast can I let $f(t)$ grow and still get the exponential bound on the right-hand side? Letting $m(s)$ be the mgf for the random variable $y$, a first shot using the Chernoff method starts like:
$$P \left( \sum_{i=1}^{f(t)} Y_i > t \right) < \inf_{s>0} \left[ e^{-st} m(s)^{f(t)} \right]$$ If I try to parameterise $f(t)$ linearly by $f(t) = at$ for some $a>0$, then we get:
$$P \left( \sum_{i=1}^{f(t)} Y_i > t \right) < \inf_{s>0} \left[ \frac{m(s)^a}{e^s} \right]^t$$
which makes me think that, at least with linear growth, we can't really say anything without knowing more about the mgf. Or that I'm going about this in completely the wrong way. Probably the latter.","['probability-theory', 'probability', 'concentration-of-measure']"
2201724,Show $\mathbb{E}[X] \le \mathbb{E}[Y]$,"I have a problem that I'm having trouble to identify the correct integral intervals for proof. I would like to have some help on the thought process. Problem: Let $X$ and $Y$ have densities $f$ and $g$, respectively, and 
  $f(x)  
\begin{cases}
\ge g(x),  & \text{if $x$ }\le a;\\
\le g(x), & \text{if $x$ }\ge a
\end{cases}
$ Show that $\mathbb{E}[X] \le \mathbb{E}[Y]$ Since $\mathbb{E}[X]= \displaystyle  \int_{-\infty}^{\infty} xf(x)\, dx$ I will have to identify intervals (one positive and one negative) that are able to prove $\mathbb{E}[X] \le \mathbb{E}[Y]$ for all cases. However, I feel there may be an intersection, which leads to 3 intervals total, but I'm not sure how to divide up the segments. I would like some help with laying out the proof.","['density-function', 'proof-verification', 'continuity', 'statistics', 'probability']"
2201771,"Let f be a function twice differentiable and with derivatives continuous on an interval $[a,b]$ containing $0$. Prove the following statement:","$$\lim_{x\to0}  \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \frac{f''(0)}{2}$$ I have been thinking about this for a bit of time now, but I'm not getting anything. What I have done: write $f'(x)$ as it's defined:
$$f'(x) = \lim_{h\to x}\frac{f(h) - f(x)}{h-x}$$ Now we can write: 
$$\lim_{x\to0}  \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \lim_{x\to0}  \frac{\lim_{h\to x}\frac{f(h) - f(x)}{h-x} - \frac{f(x)-f(0)}{x}}{x}  $$ I have tried lots of algebraic manipulation after this, but nothing has come out so far. Could someone steer me in the right direction? EDIT: I just opened Rudin, which has a similar question that indicates that l'Hopital's should be used. Indeed: $$\lim_{x\to0} \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \lim_{x\to0} \frac{xf'(x) - f(x)-f(0)}{x^2}$$ We can apply l'Hôpital:
$$ \lim_{x\to0} \frac{xf'(x) - f(x)-f(0)}{x^2} = \lim_{x\to0}\frac{f'(x) + xf''(x) - f'(x)}{2x} = \frac{f''(0)}{2}$$ EDIT #2 Using Taylor's theorem, as suggested in the comments.
We choose to perform the Taylor expansion of $f(0)$ at $x_0 = x $. Then:
$\exists c $ between $x$ and $0$ such that: $$f(0) = f(x) - xf'(x) + \frac{x^2f''(c)}{2} \implies \frac{f''(c)}{2} = \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x}   $$ As $x\to 0$, $c\to 0 $ and we have $$\lim_{x\to0}  \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \frac{f''(0)}{2}$$","['real-analysis', 'calculus']"
2201776,"Is this an equivalent formulation of ""surjective"" resp. ""epimorphism""?","An $A$-element $x$ of $X$, written $x\in_A X$, is a map $x:A\to X$. If $f$ is a map with domain $X$, and $x\in_A X$ is an element, we write $f(x)$ to denote the composite of $f$ and $x$. Now say that a map $f:X\to Y$ is cool if $$\forall A\quad \forall x, x'\in_A X\quad:\quad f(x) = f(x')\implies x = x'.$$ In the category of sets, a map is cool if and only if it is injective! In an arbitrary category, a map is cool if and only it is an monomorphism! Call a map $f : X\to Y$ fresh if for all $A$, and for all $y\in_A Y$, there is an $x\in_A X$ such that $f(x) = y$. It would now be quite nice if, in the category of sets, a map is fresh if and only if it is surjective. 1. Is this true? 2. Also, in an arbitrary category, is a map fresh if and only if it is an epimorphism ?","['category-theory', 'elementary-set-theory']"
2201794,"Prove $\lim_{(x,y) \to (-1,8)} xy = -8$ using only the definition.","I'm having problems trying to prove this limit using only the definition with delta and epsilon. I need to see that: $$ \forall\epsilon \  \exists \delta \ : \sqrt{(x+1)^2+(y-8)^2} < \delta \Rightarrow |xy+8|<\epsilon$$ I want to make $|xy+8|$ look like the first half. I start by replacing $t=x+1$, so $x=t-1$ and $s=y-8$, so $y=s+8$. So I get: $$ |(t-1)(s+8)+8| = |ts+8t-s-8+8| = |s(t-1)+8t| \leq |s(t-1)|+8|t|$$ Here I'm stuck, if I use that $\delta < 1$ then I can say that $t-1 > 0$ and since $|s(t-1)| = |s||t-1|$ then $|s||t-1| = |s|t-|s|$, which means I get: $$ |(t-1)(s+8)+8| \leq |s|t-|s|+8|t| \leq |s|t+8|t| $$ I think I need to bound it by $||(s,t)||$, so I can add/substract and simplify, but I can't see how to do it. Any tips you can give me?","['multivariable-calculus', 'epsilon-delta', 'proof-verification', 'limits']"
2201798,Proof involving symmetric bilinear forms and matrices,"Let $f$ $:$ $V$$\times$$V$ $\rightarrow$ $\Bbb R$ be a symmetric bilinear form. Let $A, B$ be the matrices of $f$ with respect to possibly different bases. I want to prove that there exists $P$ such that $B$ $=$ $P^T$$AP$. Please can anyone lend a hand here?","['matrices', 'change-of-basis', 'bilinear-form', 'linear-algebra']"
2201857,Representation theory and simultaneous block diagonalization of matrices with common symmetries,"Group representation theory has applications in a lot of areas and problems in mathematics, one of such an example is the finest simultaneous block diagonalization of the set of matrices with common symmetries. According to Page 7 of this paper , let $\{T(g)\}$ be an orthogonal matrix representation of a group $G$, if a set of matrices $\{A_p\}$ all share the symmetry described by $G$, that is, $T(g)^\intercal A_p T(g) = A_p, \ \forall g\in G \ \text{and} \ \forall p,$ then a simultaneous block diagonalization of $A_p$ can be obtained through the decomposition of the representation $\{T(g)\}$ into irreducible representations. In particular, if $\{P^\intercal T(g) P\}$ is the direct sum of irreducible representations with dimensions $n_i$ and multiplicities $m_i$, then each element in $\{P^\intercal A_p P\}$ has a common block structure as the direct sum of blocks of dimensions $m_i$ and multiplicities $n_i$. Can anyone point me to references/theorems in group representation theory from which this claim is based? Thanks!","['matrices', 'abstract-algebra', 'diagonalization', 'representation-theory', 'linear-algebra']"
2201871,A sequence in base 3,"Take the sequence of numbers such that no two of them average to another:
$$   0, 1, 3, 4, 9, 10, 12, 13, 27, 28, 30, 31, 36, 37, 39, 40,\ldots$$
I've discovered through experimentation that if we set the first two terms to $0$ and $1$, the rest of them can be found through the observation that if $n$ is in the sequence, so is $3n$ and $3n+1$.
I don't know how to prove it, though. I'm not even really sure where to start. How can I prove this?","['number-theory', 'elementary-number-theory']"
2201872,Frequently in Nets,"Let $(x_\alpha)_{\alpha \in I}$ be a net with elements $x_{\alpha}$ in some countable set $A$. Suppose that no element of $A$ is frequently in $(x_\alpha)_{\alpha \in I}$. Is it true that there is some infinite subset $B \subseteq A$ so that for any infinite subset $C \subseteq B$, $C$ is frequently in $(x_\alpha)_{\alpha \in I}$? That is, $\forall \alpha \in I$, $\exists \beta \gtrsim \alpha$ so that $x_\beta \in C$. I think this is true but can't find a proof. It's hard because nets are very different from sequences.","['general-topology', 'real-analysis', 'nets', 'analysis']"
2201878,Is the norm topology the same as the initial topology generated by the norm function?,"I have a question about the topology on a normed vector space. A normed vector space $(X, \| \cdot \|_X)$ naturally comes with a topology, known as the norm topology or strong topology on $X$, generated by the open balls $B (x, r) := \{y \in X : \|y-x\|_X < r\}$. Is this the same as the intial topology on $X$ generated by the norm function $\| \cdot \|_X : X \to \mathbb{R}$, i.e., the smallest topology that makes this norm function continuous? I thought yes. But then, I noticed one property of initial topology as follows: If $X$ has the initial topology generated by a family $\mathcal{F}$ of functions,then a net $\langle x_\alpha\rangle$ converges to $x \in X$ iff $\langle f(x_\alpha)\rangle$ converges to $f(x)$ for all $f \in \mathcal{F}$. This would imply that a sequence $\{ x_n :n \in \mathbb{N}\}$ converges to $x \in X$ iff $\| x_n\|$ converges to $\|x\|$, which in general I think is not true. Why am I getting this contradiction? Where did I go wrong? Thanks in advance for any help!","['general-topology', 'normed-spaces', 'real-analysis', 'functional-analysis']"
2201905,Map of Čech cohomology induced by refinement,"Let $X$ be a topological space, with open covers $\mathfrak{U} = (U_i)_{i\in I}$ and $\mathfrak{V} = (V_j)_{j\in J}$ such that $\mathfrak{V}$ is a refinement of $\mathfrak{U}$ for totally ordered sets $I$ and $J$, i.e. there exists some map of sets $\lambda: J\to I$ such that for all $j\in J$, $V_j\subseteq U_{\lambda(j)}$. Recall that we define $U_{i_0,\ldots,i_p} = U_{i_0}\cap\cdots\cap U_{i_p}$ for $i_0,\ldots,i_p\in I$ and we define, for an abelian sheaf $\mathscr{F}$ on $X$, $$\check{C\,}^p(\mathfrak{U},\mathscr{F}) = \prod_{i_0<\cdots <i_p}\mathscr{F}(U_{i_0,\ldots,i_p})$$ with the boundary maps as usual, so that taking homology gives us $\check{H\,}^i(\mathfrak{U},\mathscr{F})$, and likewise for $\mathfrak{V}$. Now, I wish to define natural maps $\lambda^i:\check{H\,}^i(\mathfrak{U},\mathscr{F})\to \check{H\,}^i(\mathfrak{V},\mathscr{F})$, and I suspect that the way to do this is by defining a map of complexes $\lambda:\check{C\,}^\bullet(\mathfrak{U},\mathscr{F})\to \check{C\,}^\bullet(\mathfrak{V},\mathscr{F})$. However, I'm having some difficulty defining the map $\lambda^p: \check{C\,}^p(\mathfrak{U},\mathscr{F})\to \check{C\,}^p(\mathfrak{V},\mathscr{F})$. If I fix some element $$\alpha = (\alpha_{i_0,\ldots,i_p})_{i_0<\cdots<i_p}\in\check{C\,}^p(\mathfrak{U},\mathscr{F})$$ then I wish to define something along the lines of $$\lambda^p(\alpha)_{j_0,\ldots,j_p} = \alpha_{\lambda(j_0),\ldots,\lambda(j_p)}$$ but this may not be defined. The fix is most likely to extend the definition of $\check{C\,}^\bullet(\mathfrak{U},\mathscr{F})$ to include nonincreasing tuples with the usual sign convention, but will this give the same homology?","['homology-cohomology', 'sheaf-cohomology', 'algebraic-geometry']"
2201918,What about this method to compute $\dfrac{dx}{dy}$ if $x=y+k$?,"I'm trying to compute $\dfrac{dx}{dy}$ if $x=y+k$. I tried this: $$
\begin{align}
x&=y+k\tag{1}\\[6pt]
dx&=d(y+k)\tag{2}\\[6pt]
\frac{dx}{dy}&=\frac{d(y+k)}{dy}\\[6pt]
&=\frac{dy}{dy}+\frac{dk}{dy}\\[6pt]
&=1+0\\[6pt]
\frac{dx}{dy}&=1
\end{align}
$$ But what about step $(2)$? Is that correct? what does that really mean? Should I just jump into the next step where I just take the derivative of both expressions? Can I take $d$ like so in all the situations? From my intuition, with $(1)$ he's saying that the value of $x$ and $y+k$ is the same, so if $x$ moves a tiny bit then $y+k$ will move that same bit, and that's where $(2)$ comes from. Does this mean that $dx$ is a little proportion of $x$, same as $y+k$'s?","['derivatives', 'calculus']"
2201927,How do I find the formula (or rules) that created a list of numbers with seemingly no pattern?,"Newbie here, and I apologize if this is the wrong forum for this type of question... I have a group of 200 or so alphanumeric codes from an unknown source. Here's an example piece of the data set: 2230-4D16-5112
2301-7D05-7062
2373-4A20-0106
3072-5E26-2033
0662-2E10-237F
1172-5E30-520B There are some ""rules"" that I have been able to discern just by looking at enough of these codes. First, these always begin with 0, 1, 2, or 3. Secondly, the only valid characters are 0-9 and A-F. Other than that, I haven't yet determined what other constraints or patterns there are. So my question is, given a set of data, what are the known ways (mathematical or otherwise) to determine the formula used to create the data without having access to anything from the original creator? Or maybe the numbers are just random? How do I know? And my other question: Is there any sort of software currently out there where you can plug into large sets of data and have it figure out patterns/rules to stuff like this? UPDATE: Interestingly enough, after looking at 177 of these sequences, I see some interesting rules developing. Only certain values can be in certain positions. Maybe someone out there can recognize this pattern and why it's this way? ( All means 0-9 and A-F) 1: 0-3
2: 0-7
3: All
4: 0-3

5: 0-7
6: All
7: 0-3
8: 0-7

9: All
10: 0-3
11: 0-7
12: All UPDATE 2: The comment by Théophile below was enough for me to look at the sequencing differently. Instead of 3 groups of 4, it's actually 4 groups of 3, and the dashes are used to obfuscate that. Maybe these simple rules are enough to consider a value as valid vs invalid in a system? I don't know for sure, but it's a starting point with the data I have. Now to look for patterns within... 1: 0-3
2: 0-7
3: All

4: 0-3
5: 0-7
6: All

7: 0-3
8: 0-7
9: All

10: 0-3
11: 0-7
12: All","['generating-functions', 'data-analysis', 'sequences-and-series', 'data-mining']"
2201948,What is the topology on a vector space generated by a family of seminorms?,"I have a question about topological vector spaces. I'm getting so confused about what we mean by saying a topology generated by a family of seminorms. At first, I thought it just means the initial topology generated by the family of seminorm functions. But then, I read about this sample, Example 2 . Let X be a set, then $\mathbb{C}^X = \{ f: X \to \mathbb{C} \}$ is a complex vector space. To each $x \in X$, consider $\| \cdot \|_x : \mathbb{C}^X \rightarrow \mathbb{R}$, ${\| f \|}_x = | f(x) |$. This is a seminorm on $\mathbb{C}^X$. The topology generated by this set of seminorms as $x$ varies over $X$ is the product topology on $\mathbb{C}$. Why is this the same topology as the product topology on $\mathbb{C}^X$? If this is true, the property of initial topology would imply $f_n$ converges to $f$ in $\mathbb{C}^X$ iff $|f_n(x)|$ converges to $| f(x)|$, for all $x\in X$. However, the property of the product topology would imply $f_n$ converges to $f$ in $\mathbb{C}^X$ iff $f_n(x)$ converges to $f(x)$, for all $x\in X$. Consequently, $f_n(x)$ converges to $f(x)$, for all $x\in X$ iff $|f_n(x)|$ converges to $| f(x)|$, for all $x\in X$, which I think is not true. Why do I get a contradiction here? Thanks in advance for any help!","['functional-analysis', 'general-topology', 'analysis', 'vector-spaces']"
2201979,Sheaf over a Locale,"Define a frame to be a lattice with arbitrary joins and where finite meets distribute over joins. Morphisms of frames are lattice morphisms distributing over arbitrary joins. Observing that the poset of open sets on a topological space is a frame, one can see that there is a contravariant functor $\Phi: {\bf Top } \rightarrow {\bf Frame}$, so that the category of frames can be thought of as a generalization of the opposite category of topological spaces. Forming the category of locales as the opposite category of the category of frames, one comes upon the question of setting up concepts for locales analogous to those for topological spaces. For instance, I have read that one can establish measure theory over locales in a way that avoids many of the pitfalls of ordinary measure theory. Measure theory over locales avoids the Banach Tarski Paradox and the need to distinguish only certain sets as measurable. All this motivates my particular question: Does there exist a notion of a Sheaf over a locale? If so, does it have any desirable properties or avoid analogous complications in topos theory? Where can I read about such objects?","['general-topology', 'sheaf-theory']"
2201995,show that $\|f_n-f\|_p \to 0 \iff \|f_n\|_p \to \|f\|_p$,"Suppose that $1 \le p \lt \infty$. If $f_n,f \in L^p$ and $f_n \to f$ a.e, then show that $\|f_n-f\|_p \to 0 \iff \|f_n\|_p \to \|f\|_p$ Hint: If $f_n,g_n,f,g \in L^1, f_n \to f \text{ and } g_n \to g \text{ a.e }. |f_n| \le g_n \text{ and } \int g_n \to \int g$, then $\int f_n \to \int f$. Proof (hint): Apply Fatou's Lemma to $g_n \pm\text{Re}({f_n})$ and $g_n\pm\text{Im}({f_n})$ respectively to get the result. Now ($\implies $) follows by Triangle inequality. For ($\impliedby )$ we have $$|f_n-f|^p \le (|f_n|+|f|)^p \to 2^p |f|^p$$
Moreover $$\left(\int\left(|f_n|+|f| \right)^p \right)^{\frac{1}{p}} \le \|f_n\|_p+\|f\|_p \to 2\|f\|_p$$
$$\implies \limsup_n\left(\int\left(|f_n|+|f| \right)^p \right)^{\frac{1}{p}} \le 2\|f\|_p$$
By Fatou's Lemma we have $$\int \liminf_n \left(|f_n|+|f|\right)^p \le \liminf_n \int \left(|f_n|+|f|\right)^p $$
$$\implies 2^p \int |f|^p \le \liminf_n \int \left(|f_n|+|f|\right)^p \le \limsup_n \int \left(|f_n|+|f|\right)^p \le 2^p \|f\|_p^p$$ This gives us that $$\lim_n \int \left(|f_n|+|f|\right)^p=\int 2^p |f|^p=2^p||f||_p^p$$
Then $h_n=|f_n-f|^p \to 0$ and $|h_n| \le g_n=(|f_n|+|f|)^p \to 2^p|f|^p=g$. Also $\int g_n \to \int g$. Thus by the hint we have $\int |f_n-f|^p\to 0$. This seems alright to me. I just wanted to confirm.
Thanks for the help!","['real-analysis', 'functional-analysis', 'lp-spaces', 'measure-theory', 'analysis']"
2202000,$\lim_{x\to 0} \frac{\sin x - x}{x^2}$ without L'Hospital or Taylor,"It is easy to see that $$\lim_{x\to 0} \frac{\sin x - x}{x^2} =0, $$but I can't figure out for the life of me how to argue without using L'Hospital or Taylor. Any ideas?","['limits-without-lhopital', 'calculus', 'limits']"
2202026,"For $\{a,b\}\subset(0,1]$ prove that $a^{b-a}+b^{a-b}+(a-b)^2\leq2$","Let $\{a,b\}\subset(0,1]$. Prove that:
  $$a^{b-a}+b^{a-b}+(a-b)^2\leq2$$ I don't see even how to begin the proof. We can rewrite our inequality in the following form
$$\frac{a^b}{a^a}+\frac{b^a}{b^b}+(a-b)^2\leq2$$ or
$$(ab)^b+(ab)^a\leq(2-(a-b)^2)a^ab^b.$$
By Jensen easy to show that  $a^ab^b\geq\left(\frac{a+b}{2}\right)^{a+b}$. Thus, it remains to prove that
$$\left(\frac{a+b}{2}\right)^{a+b}(2-(a-b)^2)\geq(ab)^b+(ab)^a$$
and I not sure that the last inequality is true. Thank you!","['real-analysis', 'inequality', 'exponential-function', 'calculus', 'contest-math']"
2202030,Find Equation of a Third Plane Which Interacts With Two Given Planes,"I have an online course asking me to find an equation of a third plane which intersects with two given planes (P1: x + y + 3z − 2 = 0 and P2: x − y + 2z = 0)
in one point , and in one line (so two different answers). We have covered nothing like this in the content provided and I can't find anything like this on the internet (including the database on mathematics stackexchange) I'm not asking for straight up answers, but could someone please show the path or show me where I am supposed to start? What I tried: 
I thought that if all the planes are perpendicular, then the planes would meet at one point, thus I found the cross product of the two normals (1,-1,3) x (1,-1,2) and found (5,1,-2) to be the normal, thus I created this into a plane P3: x + y = 0, but thought that maybe the D value (Ax + Bx + Cy + D = 0) is supposed to equal something, thus I abandoned such solution. Also for the intersection at one line, I believe I could just make the third  plane perpendicular to one of the planes and that would create a system where the planes intersect at a line. Is this right? Here's the actual question: Determine the equation of a plane, P3, that intersects the planes P1: x + y + 3z − 2 = 0 and P2: x − y + 2z = 0 in
a point;
b line. Please help! Thanks!","['plane-geometry', 'vectors', 'geometry']"
2202058,Prove Chi-squared distribution by induction,"$\forall n \in \mathbb{N}, {X_n\sim N(0,1)}$ independent.
I wanna prove the pdf of $X=\sum_{k=1}^{n}X_k^2$ by induction : Define $$m_n=\int_0^\infty t^{n/2-1}exp(-t/2)dt$$ we have
$$\forall t > 0, f_n(t)=\dfrac 1 {m_n} t^{n/2-1}exp(-t/2)$$ When $n=1$, it's easy to prove that $f_1(t)=\dfrac{exp(-\frac t 2)}{\sqrt{2\pi t}}$. Suppose the argument holds for $n$, when for $n+1$, I'm stuck at this step: $$f_{n+1}(t)=\int_0^tf_n(u)f_1(t-u)du=...=\dfrac{exp(-t/2)}{m_1\cdot m_n}t^{\frac{n-1}2}\int_0^1 \dfrac {v^{n/2-1}} {\sqrt{1-v}}dv$$ How could I simplify further this expression? Thanks a lot~","['probability', 'chi-squared']"
2202077,Find all integer values: $a^2+b^2=2017^2$,"I read something about how the possible digits of any square number is $0,1,4,5,6,9$ and that I could bash it out, but then that would take a lot of time. Is there any other way to to this problem?","['number-theory', 'pythagorean-triples', 'elementary-number-theory']"
2202087,"$f(x)=ax+b$ for some $a,b\in\mathbb{Q}$ if $f(\mathbb{Q})\subset\mathbb{Q}$ and $f(\mathbb{R-Q})\subseteq \mathbb{R}-\mathbb{Q}$","let $f$ be a polynomial function such that $f(\mathbb{Q})\subset \mathbb{Q}$ and $f(\mathbb{R}-\mathbb{Q}) \subset \mathbb{R}- \mathbb{Q}$ . Prove that $f(x)=ax+b$ for some $a,b\in\mathbb{Q}$ My approach : I started by defining $g(x)=f(x)-f(0)$ . Now $g(x)$ satisfies the properties $g(\mathbb{Q})\subset \mathbb{Q}$ ,  $g(\mathbb{R}-\mathbb{Q})\subset\mathbb{R}-\mathbb{Q}$ and $g(0)=0$ .
$g$ is a polynomial and $g(0)=0$ . $0$ is root of $g$ and hence $g(x)=x h(x)$ for some polynomial $h(x)$ . It will be enough if one can prove that $h(x)$ is constant . I'm stuck here . Any form help will be highly appreciated.","['general-topology', 'real-analysis', 'analysis']"
2202104,How to solve a differential equation including $\ddot{x}\dot{x}$ term?,"I have the following differential equation $$ m \ddot{x} \dot{x} + f \dot{x} = p $$ where $m > 0$ , $f > 0$ , $p > 0$ are constants. How to solve it analytically?",['ordinary-differential-equations']
2202124,L1 distance to L2 sphere,"This may seem like a strange question, but I have been spending quite some time trying to find the answer to the following problem: Given a Euclidean (L2) unit sphere and a point $p$, what is the minimum L1 distance of $p$ to that sphere? I have had quite some success for circles, and the formulas tend to be elegant and simple. For instance, for points whose $x$ and $y$ coordinates are both outside of the range $[-1,1]$, the distance is simply $|x|+|y|-\sqrt2$. I am wondering if there are similarly elegant formulas in 3D. However, this is where my math skills are reaching their limits. (To arrive at the 2D formulas, I minimized $|\cos\theta-x|+|\sin\theta-y|$ for $\theta$ to varying degrees of success. Depending on $x$ and $y$, the curve can become quite scary). If you are wondering what this is for: I am writing a somewhat unconventional ray tracer that is using a discrete space. The L1 distance makes things a lot easier for many things, but as you may imagine circles and spheres take some extra tinkering.","['analytic-geometry', 'normed-spaces', 'trigonometry', 'geometry']"
2202129,Residue for quotient of functions,"Let $f, g$ be holomorphic functions on a disk $\mathbb{D}(z_0,r)$ centered at $z_0$ and of radius $r>0$. Suppose $f$ has a simple zero at $z_0$. I want to find an expression for $Res(g/f, z_0)$. But I'm not sure what this expression should look like. Here's my guess: Since $f$ has a simple zero at $z_0$, $\exists h(z)$ holomorphic on $\mathbb{D}(z_0,r)$ such that $f(z)=(z-z_0)h(z)$ and $h(z_0)\ne 0$. So that we can represent $g/f = \frac{g(z)}{(z-z_0)h(z)}$, where we observe that $z_0$ is a pole of order 1 of $g/f$. This implies that we can express $$g/f=\frac{a_{-1}}{z-z_0}+\sum\limits_{n=0}^\infty a_n(z-z_0)^n$$ Hence, $$a_{-1}=\frac{g(z)}{f(z)}(z-z_0)-\sum\limits_{n=0}^\infty a_n(z-z_0)^{n+1}$$ Does this look like a correct approach? I think that this expression is too general because of the infinite series on the right-hand side. Is there a clue I'm missing? Update: Another approach might be this: $$g/f = \frac{g(z)}{(z-z_0)h(z)}=\frac{c_0+c_1(z-z_0)+\dots}{d_1(z-z_0)+\dots}=\frac{c_0}{d_1(z-z_0)+\dots}\\
+\frac{c_1(z-z_0)+\dots}{d_1(z-z_0)+\dots}=\frac{a_{-1}}{z-z_0}+\sum a_n(z-z_0)^n$$ But what next?","['laurent-series', 'complex-analysis', 'residue-calculus', 'proof-verification']"
2202138,Convergence of an alternating series : $ \sum_{n\geq 1} \frac{(-1)^n|\sin n|}{n}$,"Study the convergence of $$\displaystyle \sum_{n\geq 1} \frac{(-1)^n|\sin n|}{n}.$$ I am stuck with this series, we need probably some measure of irrationally of $\pi$, unfortunately I am unfamiliar with this. So here is my attempt : Let $f(x) = \sum \frac{|\sin{n}|}{n} x^n, |x| < 1$ It's not difficult to compute the Fourier series of $|\sin(x)|$ : $$
\displaystyle|\sin(x)|=\frac{2}{\pi}-\frac{4}{\pi}\sum_{n=1}^{+\infty}\frac{\cos(2nx)}{4n^2-1}
$$ Then Fubini's theorem ( Series Version ) works very well (because the previous series converges absolutely at $x$ fixed ) and all calculations made, we find that for all $x\in( -1,1)$: $$
\displaystyle f(x)=\frac{2}{\pi}\sum_{n=1}^{+\infty}\frac{x^n}{n}-\frac{4}{\pi}\sum_{p=1}^{+\infty}\frac{x^2-2x\cos(p)}{(4p^2-1)(x^2-2x\cos(p)+1)}
$$ However, the second sum I have not been able to show the convergence. I feel the series diverge because the following series
$$
\displaystyle\sum\frac{1}{p^2\sin^2\left(\frac{p}{2}\right)}
$$ diverge because  $0$ is an accumulation point of $\displaystyle (n\sin(n))$ sequence. Any ideas (for the original series) ?","['real-analysis', 'sequences-and-series']"
2202161,How to solve: $y'+\int y \mathrm dx = 2y$?,"The question is to solve for $y$:
$$ \frac{\mathrm dy}{\mathrm dx} + \int y\,\mathrm dx = 2y$$
How do I solve this? I differentiated the equation to get $y'' + y = 2y'$, but further solving proved hopeless. Also, by observation $e^x$ is a solution, but the answer he told me is $y=xe^x$, which is also a valid solution! I can't even see how they both can be a part of the same family! So how do I solve this equation? Since I only know how to solve 1st order differential equations, this is even harder for me. Thank you.",['ordinary-differential-equations']
2202169,Dimension of $\mathbb{R}$ over $\mathbb{Q}$,"Show that the set $\mathbb{R}$ is a vector space on the set $\mathbb{Q}$. What is dimension of of $\mathbb{R}$ over $\mathbb{Q}$? First part is easy. Both $\mathbb{R}$ and $\mathbb{Q}$ are fields and $\mathbb{R}$ contain $\mathbb{Q}$, hence proved. Assume the dimension is $n$ and the vector space has the ordered basis $B=(x_1,x_2,\dots,x_n)$ where $x_i\in \mathbb{R}$. Then for every $x\in \mathbb{R}$ we have exactly one ordered set of rational number $t_i$'s such that $x=x_1t_1+\cdots+x_nt_n$. Now we define the map $$f:\mathbb{R}\rightarrow \mathbb{Q^n}$$ $$f(x)=(t_1,\dots,t_n)\text{ where }x=x_1t_1+\cdots+x_nt_n\,.$$ I think the function $f$ is well defined injective function and by: Theorem : Let $f:X\rightarrow Y$ be an injection and $Y$ is countable, then $X$ is coutable as well. We reach a contradiction because $\mathbb{R}$ is uncountable. I'm not sure about the proof. It seems alright but this question was to be supposed a tough one. Please provide your thoughts about it. If it is wrong please point it out.","['elementary-set-theory', 'real-analysis', 'calculus', 'proof-verification']"
2202171,Is following function valid to be a copula function? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I know $C(u,v)=uv$ is a copula corresponding to the independent case. How about $C(u,v)=u^2v^2$?",['statistics']
2202172,"Why does $\lim\limits_{ (x,y) \to (0,0) } \frac{x-y}{\sqrt x - \sqrt y}$ not exist?","Why does this limit not exist? $$\lim\limits_{ (x,y) \to (0,0) } \frac{x-y}{\sqrt x - \sqrt y}$$ If you set y = 0, the limit goes to zero. If you set x = 0, the limit goes to zero.
You can also manipulate it with algebra to get zero.
However, if x=y you have zero/zero before you even evaluate the limit but is that proof enough? Thanks! From Larson Calculus 13.2 Exercise 27",['multivariable-calculus']
2202201,Hermitian Operators and the Spectral Theorem,"I understand that in a finite-dimensional vector space $V$, a diagonalizable linear operator $T: V \to V$ decomposes $V$ into a direct sum of its invariant eigenspaces, on each of which it restricts to a scalar multiple of a projection. The Spectral Theorem says that this occurs if $T$ is Hermitian, and I'm trying to piece together a geometric intuition for the proof. Here's my thinking: we know that the eigenspace $E$ of some eigenvector $v$ is $T$-invariant, but we need something extra (presumably involving the Hermiticity of $T$) to show that its orthogonal complement $E^{\perp}$ is also invariant: that is, we know that $Tv$ will stay inside $E$, but Hermiticity is required for $Tw$ (for some $w \notin E$) to stay outside of $E$. Once this is done, we can just restrict to $E^{\perp}$ and play the same game there, so that by induction we separate out $n = \dim{V}$ orthogonal directions to get our eigenbasis. So what is it about Hermiticity that ""stabilizes"" $E^{\perp}$ under $T$? I'm not looking for a rigorous proof, but rather a natural explanation of the geometrical significance of self-adjointness. More generally, why does requiring $T$ to be normal make the Spectral Theorem biconditional?","['eigenvalues-eigenvectors', 'adjoint-operators', 'invariant-subspace', 'spectral-theory', 'linear-algebra']"
2202242,"Prove $X^tX$, where $X$ is a matrix of full column rank, is positive definite?","Let $X$ be a matrix of dimension $n\times k$ where $n>k$,  $\text{rk}(X)=k$ so $X$ is of full column rank. Then how do I prove $X^tX$ is always positive definite, where $X^t$ is transpose of $X$? This is given sortta like a lemma in our lecture slides without proof but would like to have some reasoning behind this. Thank you  for your help!","['matrices', 'positive-definite', 'linear-algebra']"
2202251,"Why is this seemingly valid argument using modus tollens not valid? How is this an example of ""begging the question""?","I faced a question in the book "" Discrete Mathematics "" by Rosen. The question is this: Determine whether the argument is valid: If $n$ is a real number with $n > 3$ , then $n^2 > 9$ : Suppose that $n^2 \le 9$ . Then $n \le 3$ . I really think that this argument is valid by using modus tollens, but the answer mentioned at the end of the book is this:
"" Fallacy of begging the question "". Can someone explain this for me?
Thanks.","['logic', 'discrete-mathematics']"
2202259,Element-wise differentiation of a unit-normalized vector,"Let $f_d$ is the $d^{th}$ element of a $D$-dimensional vector $\mathbf{f}$.
SImilarly, $\hat{f_d}$ is the $d^{th}$ element of its unit normalized form: $\hat{\mathbf{f}} = \frac{\mathbf{f}}{||\mathbf{f}||}$. I wanted to compute $\frac{\partial \hat{f_d}}{\partial f_d}$ and found: \begin{equation}
\frac{\partial \hat{f_d}}{\partial f_d} = \frac{1 - \hat{f_d^2}}{||\mathbf{f}||}
\end{equation} However, this paper (in Page 4 , Eq. 9 ) provides the following form:
\begin{equation}
\frac{\partial \hat{f_d}}{\partial f_d} = \frac{1 - (\mathbf{\hat{f}}^T . \bigtriangledown \mathbf{\hat{f}}) \hat{f_d}}{||\mathbf{f}||}
\end{equation} Can anyone please help me to find what did I miss? Thanks.","['derivatives', 'normed-spaces', 'vectors']"
2202263,Derivatives on quadratic programming,"I'm trying to minimize a function using quadratic programming. The loss function to minimize is: $$L=\sum_{i}x_{i}M(a_{i}, a_{i})-\sum_{i,j}x_{i}x{j}M(a_{i},a_{j})$$ where M is a $NxN$ matrix and I have to calculate values of x. For quadratic programming y need to calculate the $H$ matrix (hessian matrix) and the $c$ coefficients vector. I get that $H$ matrix is $-2M$ and $c$ is the diagonal of $M$ => $c=diag(M)$ calculating derivatives. I have seen the solution and $H$ matrix is equal to $2M$ and $c=-diag(M)$. My question is: why the solution changes the sign of $H$ and $c$? and why it works and my solution doesn't? Thanks","['derivatives', 'quadratic-programming', 'lagrange-multiplier']"
2202279,Counter example for limit,"I have a question,we know that limit of $3x$ when $x$ approaches $2$ is $6.$ We can prove that with epsilon and delta definition. Suppose I wrongly assume the limit is $7$ then by definition for some epsilon I must be unable to find a delta. What is that epsilon? Just trying understand the notion of limit using counter example. That is for what value of epsilon this fails? $|3x-7|< \epsilon$ whenever $|x-2|< \delta.$","['epsilon-delta', 'real-analysis', 'calculus', 'limits']"
2202299,Space of quadrics,"What does one mean when they say that a certain ""subspace"" of quadrics has dimension $d$? Specifically, let $\mathbb{P}^1(\mathbb{C})\rightarrow \mathbb{P}^3(\mathbb{C})$ be given by $[X_0:X_1]\mapsto [X_0^3:X_0^2X_1: X_0X_1^2:X_1^3]=[Y_0:Y_1:Y_2:Y_3]$.  The image of this map is obviously contained in the zero locus of each of the three polynomials $Y_0Y_3-Y_1Y_2$, $Y_0Y_2-Y_1^2$, $Y_1Y_3-Y_2^2$. What does it mean that the space of quadrics that contain the image is three-dimensional? Do quadrics form a vector space or something? I can understand that the set of quadrics is closed under scalar multiplication, but what about addition? Also, if there is a notion of dimension, then there must be a notion of basis. Do the quadrics defined by the polynomials above form a basis of the space mentioned? Why or why not? And what is a basis in this case?","['polynomials', 'algebraic-geometry', 'projective-geometry', 'projective-space', 'quadrics']"
2202307,To prove that $n$ does not divide $1+(3^n)$.,$n$ is an odd integer greater than $1$. How do I prove that $n$ does not divide $1+(3^n)$. I have proved the case when $n$ is a prime by using Fermat's Theorem . Can I get some help for the general case?,"['number-theory', 'divisibility']"
2202311,Why does $\epsilon$ come first in the $\epsilon-\delta$ definition of limit? [duplicate],"This question already has answers here : What's wrong with this ""backwards"" definition of limit? (5 answers) Closed 4 years ago . As we know, $\underset{x\rightarrow c}{\lim}f(x)=L\Leftrightarrow$ for every $\epsilon>0$ there exists $\delta>0$ such that if $0<|x-c|<\delta$, then $|f(x)-L|<\epsilon$. My question is: why do we say that for every $\epsilon>0$ there exists $\delta>0$ and not vice versa , why not for every $\delta>0$ there exists $\epsilon>0$?","['epsilon-delta', 'limits']"
2202333,"Given a point in a manifold, what points of a submanifold are closest to it?","Note: I'm still relatively new to studying differential / Riemannian geometry so I'm not sure if my problem is well posed or not. Any reference materials to point me to the right direction would be greatly appreciated! Suppose we have some ambient Riemannian manifold $(M,g)$ and we have some submanifold $S$ of it. I know that up to conditions and definitions, if one specifies two points $p$ and $q$ of $M$, one can find a minimizing geodesic between them. Question : But suppose we are given only one point $p \in M \setminus S$. Can one ask which point $q \in S$ is ""closest"" to $p$? And where ""closest"" is taken to mean the smallest arc length of its geodesics? Can somebody please point me to some references on this question (assuming it is well posed)?","['reference-request', 'riemannian-geometry', 'differential-geometry']"
2202347,Prove $2\int_0^1 \frac{\ln^2(1-x)}{1+x^2}dx+\int_0^1 \frac{\ln^2(1+x)}{1+x^2}dx=\frac{3}{16}\pi\ln^22-2G\ln2+\frac{7}{64}\pi^3$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question How prove that
 $$2\int_0^1 \dfrac{(\ln(1-x))^2}{1+x^2}dx+\int_0^1 \dfrac{(\ln(1+x))^2}{1+x^2}dx=\dfrac{3}{16}\pi\left(\ln2\right)^2-2G\ln2+\dfrac{7}{64}\pi^3$$
 Where G is the Catalan's Constant.
 $$ \int_0^1 \dfrac{(\ln(1+x))^2}{1+x^2}dx=\Big[\arctan x\left(\ln\left(1+x\right)\right)^2\Big]_0^1-2\int_0^1 \dfrac{\arctan x\ln(1+x)}{1+x}dx=\dfrac{1}{4}\pi\left(\ln2\right)^2-2J$$ see users FDP about Evaluating $$\int_0^1 \frac{x \arctan x \log \left( 1-x^2\right)}{1+x^2}dx$$","['integration', 'definite-integrals', 'analysis']"
2202369,Help with this trigonometry problem,"Prove the given identity. $$\left(\sin\frac {9\pi}{70}+ \sin\frac {29\pi}{70} - \sin\frac {31\pi}{70}\right) \left(\sin\frac {\pi}{70}-\sin\frac {11\pi}{70} - \sin\frac {19\pi}{70}\right) =\frac {\sqrt {5} -4}{4}$$ Please help, I could not gather enough ideas, even on how to get to the first step. I thought of using the transformation formula (from sum to product) but did not find a fruitful result. UPDATE 1.: $\left(-\frac{1 +\sqrt{5}}{8} + \frac{1}{8} \sqrt{14\left(5 - \sqrt{5} \right)}\right)\left(-\frac{ 1+ \sqrt{5}}{8} -\frac{1}{8} \sqrt{14 \left(5 -\sqrt{5} \right)}\right) $, I noticed that Alexis had got to this point.  Can anyone explain me how did he get here, with calculations. Please help. Thanks in Advance.",['trigonometry']
2202395,Two-fold coverings $S^n \rightarrow X$,"Let $S^n \rightarrow X$ be a two-fold covering. Is it true that $X$ is homeomorphic to $\mathbb{R}\mathrm{P}^n$? If so, how does one prove it?","['general-topology', 'covering-spaces', 'projective-space']"
2202439,Gaussian measure on sequence space,"We had the following definition of a Gaussian measure on a Banach space:
A Gaussian probabilty measure $\mu$ on a Banach space $B$ is a Borel measure such that $\ell^*\mu$ is a real Gaussian probability measure on $\mathbb R$ for every linear functional $\ell: B\rightarrow \mathbb R$ . Where we used the push-forward of a measure $(f^*\mu)(A)=\mu(f^{-1}(A))$ . Now, let $\{\xi_n\}$ be a sequence of i.i.d. $N(0,1)$ random variables and let $\{a_n\}$ be a sequence of real numbers. Show that the law of $(a_0\xi_0,a_1\xi_1,...)$ determines a Gaussian measure on $\ell^2$ if and only if $\sum_{n\geq0}a_n^2<\infty$ . I guess, here $\ell^2$ is the space of square summable sequences. I don't really know how to prove this. The law of $(a_0\xi_0,a_1\xi_1,...)$ would be something like $\mathbb P(\{(c_n)_n:(a_0\xi_0(c_0),a_1\xi_1(c_1),....)\in A\})$ right? As a linear functional that appears in the definition one might take $\ell((c_n)_n)=\sum c_n^2$ . However, it must be for every linear functional. I also don't see the other direction. Can somebody help me with this? This is exercise 3.5 from Hairer's notes https://www.hairer.org/notes/SPDEs.pdf .","['banach-spaces', 'probability', 'measure-theory']"
2202486,Gaussian integral variant $\int_{-\infty}^\infty \frac{e^{-x^2}}{1+a e^{-x}} dx$,"I have been trying to compute this integral
$$\int_{-\infty}^{\infty} \frac{e^{-x^2}}{1+a e^{-x}} dx$$
quickly and to a high-degree of accuracy. I have some partial results, for example for $n \in \mathbb N$
$$ \frac{1}{\sqrt\pi} \int_{-\infty}^{\infty} \frac{e^{-x^2}}{1+e^{-(x+n/2)}}dx = \frac{1}{2}\sum_{i=0}^{2n}(-1)^{i} e^{-i(2n-i)/4},$$
and for $a \in (0,1)$ we have
$$ \frac{1}{\sqrt\pi} \int_{0}^\infty \frac{e^{-x^2}}{1+ae^{-x}}dx = \frac{1}{2}\sum_{n=0}^\infty (-1)^n a^n e^{n^2/4} \mathrm{erfc}(n/2)$$
where $\mathrm{erfc}$ is the error function complement fuction
$$ \mathrm{erfc}(x) = 1-\mathrm{erf}(x),$$
which we obtain by using the substitution
$$ \frac{1}{1+ae^{-x}} = \sum_{n=0}^\infty (-1)^n a^n e^{-nx}.$$
This $\mathrm{erfc}$ series does not converge very quickly unless $a$ is very small, and wolframalpha can compute the integral very accurately between say $-100$ and $100$ for various values of $a$, so I must be missing a trick. Edit I found an infinite series for the whole integral, but it converges slowly
$$\frac{1}{\sqrt\pi}\int_{-\infty}^\infty \frac{e^{-x^2}}{1+e^{-(x+a)}}dx = \frac{e^{-a^2}}{2}\left[\sum_{n=0}^\infty (-1)^n \mathrm{erfcx}(-a+n/2) + \sum_{n=1}^\infty (-1)^{n+1} \mathrm{erfcx}(a+n/2)\right],$$
where $\mathrm{erfcx}$ is the scaled complement error function
$$ \mathrm{erfcx}(x) = e^{x^2} \mathrm{erfcx}(x) = e^{x^2}(1-\mathrm{erf}(x)).$$","['integration', 'exponential-function']"
2202526,Four models of combinatorial proof,"Today I heard someone mention there are four combinatorial models to prove identities: tiling, flagpole, block walking and committee selection. I am familiar with the last one and up till this point I thought it was THE combinatorial method. For example it's not hard to show $\sum_{k=0}^n \binom nk^2 = \binom {2n}{n}$ by selecting committees. If it's at all possible how would one use other models to prove this identity? I can't find any info about these models in my book. All the proofs are either algebraic or by committee selection. If you(anyone) don't mind, can I get three simple examples demonstrating these models?","['combinatorics', 'discrete-mathematics']"
2202527,Arthur Engel Problem Solving Strategies infinite descent proof contradiction Ch-14 Q11,"The question goes as follows: $2n+1$ integral weights are given, where $n \geq 1$ . If we remove any of the weights, the remaining $2n$ weights can be split into two heaps of equal weights. Prove that all weights are equal. I was discussing the book with my teacher and she told me she found a contradiction to the statement. Basically, take $2n$ weights of weight $1$ (say $blue$ weights) and one weight of weight $2n-1$ (say $red$ weight). Here, if we remove one blue weight, then we can split with one red weight one side and the rest $2n-1$ blue weights on the other. If we remove the red weight, then we can split the rest blue weights into two partitions of size $n$ . This partition satisfies the given condition and all the weights are not equal here. So are we wrong here? How so? Because I find it hard to digest Arthur Engel being so wrong because in the solution section, he says this can be extended to rational and irrational weights as well.
Also, if we aren't, where is the step in Arthur Engel's proof which overlooks our case? I'll put the original solution here as well, exactly as it is given in PSS: Let $w_1,...,w_{2n+1}$ be the integral weights. Since any $2n$ of the weights balance, the sum of any $2n$ of the weights is even. This implies they are all of the same parity. If they are even, we set $w_i \leftarrow \frac{w_i}{2}$ , and if they are odd, we set $w_i \leftarrow \frac{w_i-1}{2}$ . In each case we get a new set of weights with the same balancing property. Applying this reduction repeatedly, we see that the $w_i$ are congruent mod $2^k$ for all $k$ . This implies that all $w_i$ are equal. Generalize the result to rational weights, which is easy, and to irrational weights, which is much more difficult.","['combinatorics', 'contest-math', 'linear-algebra', 'discrete-mathematics']"
2202532,Second Isomorphism Theorem for Banach Spaces,"I am looking for verification of the following: Let $X$ be a Banach space. We will show that if $M, N$ and $M+N$ are closed subspaces of $X$, then
$$\dfrac{M+N}{N} \cong \dfrac{M}{M \cap N},$$
using the map $\phi : M \to \dfrac{M+N}{N}$, where $x \mapsto x + N$. For any $x,y \in M$, we have that,
$$\phi(x+y) = (x+y) + N = (x + N) + (y + N) = \phi(x) + \phi(y)$$
Also, for any $\lambda \in \mathbb{F}$, we have,
$$\phi(\lambda x) = (\lambda x) + N = \lambda(x + N) = \lambda\phi(x)$$
Then:
\begin{align*}
\ker(\phi) &= \left\{ m \in M : \phi(m) = e_{\frac{M+N}{N}} \right\} \hspace{1cm}\text{should this be sent to 0 instead of the ""identity""?}\\
&= \left\{ m \in M : m + N = N \right\} \\
&= \left\{ m \in M : m \in N \right\} \\
&= M \cap N
\end{align*}
Then, we see that $\phi$ is a surjection because 
$$(m+n) + N = m + N \in \dfrac{M+N}{N}$$
which is $\phi(m)$. Then, we use the first isomorphism theorem, that is, for any operator $\phi: X \to Y$ between Banach spaces,
$$X / \ker(\phi) \cong \text{im} \phi \iff \text{im}\phi \space\ \text{is closed in} \space\ Y$$
So that here, $X = M$, $\ker(\phi) = M \cap N$, and $Y = \text{im}(\phi) = \dfrac{M+N}{N}$, so that the result follows.","['functional-analysis', 'banach-spaces', 'proof-verification']"
2202549,CS231N Backpropagation gradient,"I'm reading the Stanford course about Convolutional Neural Network and I don't understand how he backpropagates a 2 neural network. Actually, the thing I'm trying to understand is here: http://cs231n.github.io/neural-networks-case-study/ We know that: scores = np.dot(X, W) + b And to backpropagate he computes: dW = np.dot(X.T, dscores)
db = np.sum(dscores, axis=0, keepdims=True) So mathematically, we can write ($F$ being scores)
$$F = XW + b$$ and when he backpropagates he gets:
$${dW} = X^\intercal {dF}$$
$${db} = \begin{bmatrix}
       \sum\limits_{i=1} {dF}_{1i} \\
       \sum\limits_{i=1} {dF}_{2i} \\
       \vdots \\
       \sum\limits_{i=1} {dF}_{ni}
     \end{bmatrix}$$ I'm trying to do the math but I don't understand how to achieve those derivatives rigorously.
I hope some of you will help me to understand how it works :s","['matrices', 'neural-networks', 'vector-analysis']"
2202552,Rounding Percents: how far from 100?,"Given n positive, rational numbers $a_1, a_2...a_n$ where $\sum_{i=1}^{n} a_i = 100$, what is the tightest limit we can put on the difference of (the sum of each number rounded to the nearest integer) and (100)? In other words, what is the tightest bound, in terms of n, for: $|(\sum_{i=1}^{n} ||a_i||) - 100|$ What happens when you take the floor of each term instead? $| (\sum_{i=1}^{n} \lfloor a_i \rfloor ) - 100 |$ I stumbled across this problem while trying to round percents. You could say that the bound differs from 100 by at most n, since at most there are n numbers with decimals, each decimal can be at most arbitrarily close to 1. But is there a tighter limit? In the rounded case, you can construct numbers that all end in 0.5 for an even n and show that, since they all round in the same direction, they will differ from 100 by at most n/2. (Is that correct?) But I'm at a loss for the floor case. I'm not sure if the requirement that rational numbers add to an integer reduces the bound.","['percentages', 'discrete-mathematics']"
2202605,"Can it be shown that the Banach algebra $(L^1(\mathbb{R}^n), \ast)$ does not have a unit element?","Let $f, g \in L^1(\mathbb{R}^n)$ and define the convolution of $f$ with $g$ by
$$
(f \ast g)(x) = \int_{\mathbb{R}^n} f(y)g(x-y)dy.
$$
Then $(L^1(\mathbb{R}^n), \ast)$ is a Banach algebra. I have seen it stated in a few places that this Banach does not have a unit element. Is it possible to show this property? It seems to me that the dirac delta function could be a unit element here as it will pick out the original values of the function. E.g. $(f \ast \delta)(x) = f(x)$?","['functional-analysis', 'banach-algebras']"
2202723,Using Rolle's Theorem to prove roots.,"Show that $x^5+10x+3=0$ has exactly one real solution using Rolle's Theorem. I am referencing a closely related stack answer here . 
So I have tried letting $y=x^5+10x+3$.
Then $y'=5x^4+10$. Set $y'=0$
$$0=5x^4+10$$
$f$ is continuous on $\mathbb{R}$ and $f$ is differentiable on $\mathbb{R}$ but I need a closed interval $[a.b]$ of continuity that corresponds to an open interval $(a,b)$ of differentiation. Where $f(a)=f(b)$. I do not see any real solution such as they were able to find in the provided link. As well as I do not see how to satisfy $f(a)=f(b)$. Any help and explanation is appreciated!","['derivatives', 'rolles-theorem', 'roots', 'calculus']"
2202735,Inverse of matrix of ones + nI,"Having a vector $\mathbf{1} \in \mathbb{R}^{n}$ containing only ones, following equality should be true according to a paper I am currently reading: \begin{equation}
  \left( nI+\mathbf{1}\mathbf{1}^T \right)^{-1}= \frac{1}{n}\left( I - \frac{1}{2n} \mathbf{1}\mathbf{1}^T \right)
\end{equation} EDIT: what is the general rule for constructing an inverse of a matrix with $n$ on diagonal and $1$ elsewhere and how is this rule derived?","['matrices', 'inverse']"
2202742,Vector orthogonal to hyperplane,"Finding the vector perpendicular to the plane Why is weight vector orthogonal to decision plane in neural networks I am thinking now about hyperplanes and orthogonal vectors. My problem is following one: Definition of hyperplane:
$$H=\left\{ x\ \epsilon\ R^2 : u^Tx=v \right\}$$ Consider for instance simple case of hyperplane in $R^2$ (straight line): $$H=\left\{ x\ \epsilon\ R^2 : u^Tx=3 \right\}$$ so that $x_{1}-x_{2}=3$ so $u=\left(\begin{array}{c}1\\ -1\end{array}\right)$ take any point from that straight line : $x=\left(\begin{array}{c}4\\ 1\end{array}\right)$
$$u^Tx\neq0$$ So we conclude that points on this line (hyperplane) are not orhogonal to vector of coefficients $u$, and this is the case (they are othogonal) only if:
$$H=\left\{ x\ \epsilon\ R^2 : u^Tx=0 \right\}$$ In case of plane we have following definition of hyperplane :
$$H=\left\{ x\ \epsilon\ R^3 : u^Tx=v \right\}$$ then similarily $x$ and $u$ are orthogonal only in case $v=0$ I came across following definition which confused me a little bit: $$H=\left\{ x\ \epsilon\ R^n : u^Tx=v \right\}=\left\{ x\ \epsilon\ R^n : u^T(x-a)=0 \right\}$$ where $a$ is any point on hyperplane ( so $u^Ta-v=0$). Therefore, the hyperplane H consists of the points x for which $<u, x — a> = 0$.
  In other words, the hyperplane H consists of the points x for which the vectors u and x-a are orthogonal So this is the case such that $(x-a)\ \epsilon\ R^n$ and $v=0$, thats why $(x-a)$ is orthogonal to $u$. So why in  literature about for instance about Suppor Vector Machines, we assume that vector of coefficients $u$ is orthogonal to separating hyperplane, for cases where $v\neq0$? Shouldn't it be like if $u$ is orthogonal to some hyperplane, then $u$ is orthogonal to every vector $x$ in such hyperplane? I certainly misuse some concepts, so could you provide step by step proof of the fact that vector of coefficients $u$ is orthogonal to any hyperplane?","['multivariable-calculus', 'linear-algebra', 'optimization']"
2202795,Different method of proving $\int_{0}^{1}{ dx\over x}\ln\left({1-\sqrt{x}\over 1+\sqrt{x}}\cdot{1+\sqrt[3]{x}\over 1-\sqrt[3]{x}}\cdots\right)=-\pi^2$,"Given the integral $(1)$ $$\text{Prove that}\ \int_{0}^{1}{\mathrm dx\over x}\ln\left({1-\sqrt{x}\over 1+\sqrt{x}}\cdot{1+\sqrt[3]{x}\over 1-\sqrt[3]{x}}\cdot{1-\sqrt[5]{x}\over 1+\sqrt[5]{x}}\right)=-\pi^2\tag1$$ An attempt: $u=x^2,x^3 \text{and}\ x^5$, then $(1)$ becomes $$\int_{0}^{1}{\mathrm du\over u}\ln\left[\left({1-u\over 1+u}\right)^2\left({1+u\over 1-u}\right)^3\left({1-u\over 1+u}\right)^5\right]\tag2$$ Simplify to $$\int_{0}^{1}{\mathrm du\over u}\ln\left({1-u\over 1+u}\right)^4\tag3$$ Apply $\ln\left({1+u\over 1-u}\right)$ series, then we have $$-\sum_{n=0}^{\infty}{1\over (2n+1)}\int_{0}^{1}u^{2n}\mathrm du\tag4$$ $$-8\sum_{n=0}^{\infty}{1\over (2n+1)^2}=-\pi^2\tag5$$ Looking for another method of proving $(1)$","['integration', 'definite-integrals', 'calculus']"
2202809,functions on sets proof (injective/surjective/bijective) question,"I have to prove the following three statements: Let $A$ and $B$ be finite sets and let $f: A \rightarrow B$. A. $f$ is one-to-one. B. $f$ is onto. C. $|A|=|B|$. Prove that $(1)$ if A and B are true, then C is true, $(2)$ if A and C are true, then B is true, and $(3)$ if B and C are true, then A is true. So far I have this: (1). If $f$ is one-to-one, then for all $(x,b), (y,b) \in f$, $x=y$. If $f$ is onto, then for every $b \in B$ there is an $a \in A$ s.t. $f(a)=b$. This means that every value in the set $A$ maps to a distinct value in the set $B$. Thus, the sets have the same cardinalities, so, $|A|=|B|$. (2). If $f$ is one-to-one, then for all $(x,b), (y,b) \in f$, $x=y$. And if $|A|=|B|$ then that means that every value of A maps to a distinct value in B. So, for every $b \in B$ there is an $a \in A$ s.t. $f(a)=b$, meaning $f$ is onto. (3). If $f$ is onto, then for every $b \in B$ there is an $a \in A$ s.t. $f(a)=b$. And if $|A|=|B|$ then that means that every value of B must map to a distinct value in A. So, that means $f$ is one-to-one because there is no $(x,b),(y,b) \in f$ s.t. $x \neq y$. But I don't know if I wrote enough for them?","['elementary-set-theory', 'functions', 'discrete-mathematics']"
2202828,Subgroup of a unit group,"Let $\alpha$ be a root of $x^{3}-6x-3$, let $K=\mathbb{Q}(\alpha)$ and let $u_{1}=\frac{\alpha^{3}}{3}=2\alpha+1$, $u_{2}=\alpha+2$. Prove that $u_{1}$ and $u_{2}$ generate a subgroup of $\mathbb{Z}_{K}^{\times}$ of rank $2$. Since $K\subset\mathbb{R}$ is an extension of degree $3$ we have that $\mathbb{Z}_{K}^{\times}\cong\mathbb{Z}/2\mathbb{Z}\times\mathbb{Z}^{2}$.","['number-theory', 'algebraic-number-theory']"
2202905,Dense transfer of a set with positive Lebesgue measure: is it conull?,"I'm facing a problem in measure theory and I need to prove the following conjecture to move on.
Attention: I'm not sure the following statement is true. Let $A \subset \mathbb{R}$ be a measurable set such that $m(A)>0$ and $H$ be a countable, dense subset of $\mathbb{R}$. If $A+H=\{a+h: a \in A, h \in H\}$, prove that $m((A+H)^c)=0$. I'm totally stuck. It's easy to see that $A+H=\displaystyle{\bigcup_{h \in H} A+h}$, so it's definitely a measurable set, but that's the only progress I've been able to make. Any help would be greatly appreciated!","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2202916,"If $A$ has measure zero, then is there $x$ such that $A \cap (A-x)=\emptyset$?","If $A\subseteq \mathbb{R}^d$ has Lebesgue measure zero, is it always possible to find an $x$ such that $A \cap (A-x)=\emptyset$? This is one of those things that seems reasonable , but I'm worried about pathologies. For instance, if $A$ were a group, then the result is obvious. One might consider the group generated by $A$, but this may be too large. E.g. if $A$ is the standard Cantor set on $[0,1]$, then the subgroup generated by $A$ is all of $\mathbb{R}$.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2202925,"Sketch the vector equation:$ r(t) = (2\cos(t), 2\sin(t), 1)$","Given the parametric equations:
$$x = 2\cos(t), \quad y=2\sin(t), \quad z = 1.$$ We know that $x^2 + y^2 = 4\cos^2(t) + 4\sin^2(t) = 4(\cos^2(t) + \sin^2(t)) = 4$. In other problems, I've seen this fact tell us that the graph lies on the cylinder $x^2 + y^2 = 4$. However, in this case, $z$ is a constant. Does this mean that the 3D graph looks like a circle with radius of $2$, at $z = 1$?","['multivariable-calculus', 'graphing-functions', 'vectors', 'vector-spaces']"
2202928,Does $A^TA = \det(A) I$ imply anything significant?,I was fiddling with some $n\times n$ square matrices and I came across a matrix where the transpose of $A$ multiplied by $A$ gives me the diagonal matrix with values of determinant of $A$. That is $A^TA = \det(A)I$. Does this mean anything significant? Would love to hear your thoughts.,"['matrices', 'transpose', 'matrix-equations', 'linear-algebra']"
2202944,$\lim_{n\to\infty} \frac{n!!}{(n+1)!!}=0$ please check my proof,"for  $a_n = \frac {n!!}{(n+1)!!}$, prove that $\lim a_n = 0$ putting $a_n$ into logarithm, we get
$$\ln a_n=\sum_{k=1}^n \ln \frac k {k+1} = -\sum_{k=1}^n \ln (1+\frac1 k)$$ by taylor series, $$\ln(1+1)=\frac 1 1 - \frac 1 2 + \frac 1 3+...$$
$$\ln(1+\frac1 2)=\frac 1 2 - \frac 1 8 + \frac 1 {24}-...$$
$$...$$
$$\ln(1+\frac1 k)=\frac 1 k - \frac 1 {2k^2} + \frac 1 {3k^3}-...$$ if ""$\lim \ln a_n$"" exists, $\sum_1^\infty \ln(1+\frac 1 k)$  (absolutely) converges. so we know that any rearrangement series of $\ln(1+\frac 1 k)$ converges and has the same value. furthermore, $\ln(1+\frac 1 k) =\sum_1^\infty\frac {(-1)^{k+1}} k$ converges absolutely if $k>1$ so, by assuning $\ln a_n$ converges, expand $\ln(1+\frac 1 k)$ by taylor series for $k>1$ and rearrange column by column.
$$\sum_{k=1}^{\infty} \ln (1+\frac1 k)=(\ln2 + (\frac1 2 + \frac 1 3 + ...) - \frac 1 2 (\frac 1 4 + \frac 1 9 + ...) +\frac1 3(\frac 1 8 +\frac 1 {27} +...)+...)$$
however the second term $(\frac 1 2 + \frac 1 3 +...)$ doesn't converge while all the other terms converge. this contradiction proves that $\ln a_n$ doesn't converge. so we have $$\lim_{n\to\infty}\ln a_n=-\sum_{k=1}^{\infty} \ln (1+\frac1 k)=-\infty$$ which implies $\lim a_n =0$. Is this proof correct? Can you give me more simple proof?","['real-analysis', 'limits']"
2202948,"Is $u,v,w$ a basis of $\text{span}(u,v,w)$, and why?","Let $u = (1, 2, 0, 1)$, $v = (1, −1, 1, 0)$, $w = (2, 4, 0, 2)$, and suppose $W = \text{span}(u, v, w)$ is a subspace of $\mathbb{R}^4$.
Is $u,v,w$ a basis of $W$? Why or why not? I'm not sure if it is or not, I reduced it down to echelon form.
$$ \left[
    \begin{array}{cccc}
      1&2&0&1\\
      0&1&-1/3&1/3\\
      0&0&0&0
    \end{array}
\right] $$ But I'm not sure where to go from here.","['matrices', 'linear-algebra']"
2202953,Sum of biquadrat and square is equal to $2017^{2017 }$. How can I prove it?,"Prove that there are integers $m$ and $n$ such that $$m^4 + n^2 = 2017^{2017}$$ I've tried dividing $m^2$ or $n^2$, but i'm not getting anywhere.","['algebra-precalculus', 'contest-math', 'number-theory', 'elementary-number-theory']"
2203036,A Closed Supermartingale might not be uniformly integrable? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question A supermartingale $\{X_n,\mathcal{F}_n\}$ is closed, if there exists $Z\in L^1$ such that $X_n\geq E[Z|\mathcal{F}_n]$ for all $n$. Is there any counterexample that a closed supermartingale is not uniformly integrable? Thanks!","['stochastic-processes', 'probability-theory', 'martingales']"
2203042,The number of esquares of idempotents in the rank 2 $\mathcal{D}$-class of $M_n(\mathbb{Z}_2)$.,"Here's a variation of a question I was given during a research internship. Some Definitions: Definition 1: Let $S$ be a semigroup. For any $a, b\in S$, define Green's $\mathcal{L}$-relation by $a\mathcal{L}b$ if and only if $S^1a=S^1b$ and define Green's $\mathcal{R}$-relation by $a\mathcal{R}b$ if and only if $aS^1=bS^1$, where $S^1$ is $S$ with a one adjoined if necessary. Then Green's $\mathcal{D}$-relation is given by $a\mathcal{D}b$ if and only if $a(\mathcal{L}\circ\mathcal{R})b$ (which is equivalent to $a(\mathcal{R}\circ\mathcal{L})b$); that is, there exists a $c\in S$ such that $a\mathcal{L}c\mathcal{R}b$ Definition 2: The rank of a matrix is the number of linearly independent columns it has. The Question: Let $S=M_n(\mathbb{Z}_2)$. Let $D^{(n)}_2$ be the matrices in the rank $2$ $\mathcal{D}$-class of $S$. Find $$N_n=\left\lvert\left\{\left.\begin{array}
\, e & \mathcal{L} & f \\
\mathcal{R} & \, & \mathcal{R} \\
h & \mathcal{L} & g
\end{array}\right\vert e, f, g, h\in E\left(D^{(n)}_2\right)\right\}\right\rvert;$$ that is, find $N_n$,  the number of quadruples $(e, f, g,h)\in E\left(D_2^{(n)}\right)^4$ such that $e\mathcal{L}f\mathcal{R}g\mathcal{L}h\mathcal{R}e$. Here $E(T)$ is the set of idempotents of the semigroup $T$. Background: I did most of the cases when $n=4$ and $n=6$ using the programming language GAP . For the $6\times 6$ case, I sorted matrices of the $D^{(6)}_2$ into certain types (that I can't produce from memory as my notes are missing) then had GAP do an iterated procedure to find $N_6$.","['abstract-algebra', 'semigroups', 'combinatorics', 'idempotents', 'linear-algebra']"
2203068,"Is the derivative of $ \ln f(x) $, where $f(x)$ is linear, the same for all $f(x)$?","$\frac{d}{dx}\ln f(x)=\frac{{f(x)}'}{f(x)}$ Taking $\ f(x) $ as a linear $\ ax $ the result will always be $\frac{a}{ax} =\frac{1}{x}$ I found this confusion causing in the case of integrating, how would I know  what function to get back? $\ln$ here stands for the natural logarithm.","['derivatives', 'logarithms']"
2203081,"How do you construct a function that is continuous over $(0,1)$ whose image is the entire real line?","How do you construct a continuous function over the interval $(0,1)$ whose image is the entire real line? When I first saw this problem, I thought $\frac{1}{x(x-1)}$ might work since it is continuous on $(0,1)$, but when I graphed it, I saw that there is a minimum at $(1/2,4)$, so the image is $[4,\infty)$ and not $(-\infty,\infty)$. Apparently, one answer to this question is: $$\frac{2x-1}{x(x-1)}$$ But how is one supposed to arrive at this answer without using a  graphing calculator?","['algebra-precalculus', 'calculus']"
2203129,What is wrong with this proof that anti-commutative matrices have product zero?,"Suppose $AB-BA=0$, where $A$ and $B$ are $n\times n$ matrices. Then $AB=-BA$. Then
\begin{aligned}
AB=\frac{1}{2}(AB+AB)=\frac{1}{2}(AB-BA)=\frac{1}{2}(0)=0.
\end{aligned}
I can't seem to find the error in this argument, although I know it's wrong because I found a counterexample by google search.","['matrices', 'fake-proofs', 'linear-algebra', 'proof-verification']"
2203136,$P(z)-zQ(z)=0$ having no solution in $\mathbb{C}$,"I am currently reading ""Iteration of Rational Functions"" by Alan F. Beardon and my Question is about the beginning of $§2.6.$ which deals with Fixed points. Given a rational function $R=P/Q$ where $P,Q$ are coprime polynomials. If $z_0 \neq \infty$ is fixed by $R$, then $Q(z_0)\neq 0$ and $P(z_0)=z_0Q(z_0).$ It is now stated that the fixed points of $R$ in $\mathbb{C}$ are the solutions of $P(z)-zQ(z)=0$. After that it is noted that this need not have any solutions in $\mathbb{C}$ and the counter example given is $z \mapsto z+\frac{1}{z}$. So far so good. My question is now that I do not see how to get this counterexample with the given solution.","['abstract-algebra', 'complex-analysis']"
2203142,How would you compute the limit $\lim_{x\to\infty} \left(\frac{1}{x^x+x}\right)^{1/x}$?,"I understand that you would use a natural log to break up because it is an indeterminate form, but that is what is seemingly giving me trouble.","['derivatives', 'indeterminate-forms', 'calculus', 'limits']"
2203200,Why are we allowed to put conditions on $\epsilon$ in this limit proof?,"Let $$X_n=\begin{cases} 1/n, &\text{ with prob } 1-1/n\\ n^2, &\text{ with prob } 1/n.\end{cases}.$$ $$Y_{n}=\begin{cases} 0, &\text{ with prob } 1-1/n\\ n, &\text{ with prob } 1/n.\end{cases}.$$ We want to show that $X_n \rightarrow 0$ in probability, which means showing that $\lim_{n\rightarrow\infty}P(|X_n-0|>\epsilon)=0$ for all $\epsilon>0$. And similarly for showing $Y_n \rightarrow 0$ in probability. The solutions I've seen to these questions go as follows: Whenever $\epsilon>1/n$, we have $0\leq P(|X_n-0|>\epsilon)=P(X_n>1/n)=P(X_n=n^2)=1/n$. The result follows by letting $n\rightarrow \infty$ and using the Sandwich theorem. And for the second one ( https://youtu.be/x4q6H6lxFFE?t=594 ): Whenever $\epsilon<n$, we have $0\leq P(|Y_n-0|\geq \epsilon)=1/n$. The result follows by letting $n\rightarrow \infty$ and using the Sandwich theorem. Simple enough, but why are we allowed to put conditions on $\epsilon$? I thought $\epsilon$ was just any positive number. Also even though the examples are really similar, in one case it's $\epsilon>1/n$ and in the other it's $\epsilon<n$. Is that just because one definition allows for $> \epsilon$ and the other is $\geq\epsilon$? Edit: On second glance I'm confused as to why the second proof has $\epsilon<n$? Shouldn't it just be $\epsilon>0$ as then $P(|Y_n|>\epsilon)=P(Y_n>0)=P(Y_n=n)=1/n$?","['probability-limit-theorems', 'probability', 'limits']"
2203249,A medium-sized compactification of the natural numbers,"Let $\mathbb{N}$ be the set of natural numbers with the discrete topology.  Is there a compactification $X$ of $\mathbb{N}$, other than the Stone–Čech compactification $\beta \mathbb{N}$, in which every infinite subset of $\mathbb{N}$ has at least two limit points? In other words, does there exist a compact Hausdorff space $X$, not homeomorphic to $\beta \mathbb{N}$, having a countable dense discrete subset $A$ such that every infinite subset of $A$ has at least two limit points? Motivation: $\beta \mathbb{N}$ has the property that for every infinite $B \subset \mathbb{N}$, the closure $\bar{B}$ is again  homeomorphic to $\beta \mathbb{N}$  (see Lemma 5 here ).  In particular, $B$ has $2^\mathfrak{c}$ limit points.  Of course, $\beta \mathbb{N}$ is the ""largest possible"" compactification of $\mathbb{N}$. On the other hand, if $X$ is first countable at any point $x$ of $X \setminus \mathbb{N}$, then there is a sequence in $\mathbb{N}$ converging to $x$.  This sequence is thus an infinite set with only one limit point.  So any first countable compactification is ""too small"". I am wondering what is in between. (This came up while thinking about this answer .)","['general-topology', 'compactness']"
2203250,Quotient of solvable groups is solvable - what's wrong with this proof?,"Prove: $G$ is a solvable group and $H\trianglelefteq G$. Then $G/H$ is solvable. Definition: A group $G$ is said to be solvable if there is a normal series of $G$ such that the factors are abelian, i.e. $G=G_0\trianglerighteq G_1\trianglerighteq \ldots \trianglerighteq G_n=\{1\},$ where $G_{i}/G_{i+1}$ are abelians. What's wrong with the following proof? I searched on MSE and it seems that in order to quotient out $H$ many (such as here ) consider the tower $G_iH$, but why can't we just take the image of canonical map as below? Let $\phi: G \rightarrow G/H$ be the canonical map, then $\phi(G_i)$ forms a normal tower: $\phi(G_i)$ are groups as they are images of homomorphic maps, and given $\phi(a) \in \phi(G_i)$, 
$$ \phi(a)\phi(G_{i+1})\phi(a)^{-1} = \phi(aG_{i+1}a^{-1}) \subseteq \phi(G_{i+1}).$$
using normality of $G_{i+1}$ in $G_i$. $\phi(G_i)/ \phi(G_{i+1})$ is abelian: We have
$$ \phi(a)\phi(G_{i+1}) \phi(b) \phi(G_{i+1}) = \phi(ab) \phi(G_{i+1})$$
since $G_i/G_{i+1}$ is abelian, $abG_{i+1} = baG_{i+1}$ so, $ab= ba g'$, for some $g' \in G_{i+1}$. Hence, 
$$ \phi(ab) G_{i+1} = \phi(ba)\phi(g')\phi(G_{i+1})= \phi(ba) \phi(G_{i+1}). $$
so the group is abelian. Thus, $G/H= \phi(G) \trianglerighteq \ldots \trianglerighteq \phi(1) = H$ is an abelian tower.","['abstract-algebra', 'proof-verification', 'normal-subgroups', 'solvable-groups', 'group-theory']"
2203300,Why does $(A-I)^2=0$ implies all eigenvalues of $A$ are $1$?,"Why does $(A-I)^2=0$ implies all eigenvalues of $A$ are $1$? Here $A$ is a $n \times n$ matrix. Write down the characteristic polynomial of $A$, which is $p(A)=(t-\lambda_1)(t-\lambda_2)\dots(t-\lambda_n)$. $1$ is one of its eigenvalues, but why all $\lambda$ are $1$?","['matrices', 'linear-algebra']"
2203316,Conditional Expectation Problem?,"An insurance company supposes that the number of accidents that each of its customers will have this year is Poisson distributed, with a mean depending on the customer: the Poisson mean $\Lambda$ of a randomly chosen person has a Gamma distribution with the $\Gamma(2, 1)$-density function $f_\Lambda(\lambda) = \lambda e^{−\lambda}$, $(\lambda > 0)$. Find the expected value
of $\Lambda$ for a policyholder having $x$ accidents this year $(x = 0, 1, 2 \ldots)$? Not quite sure how to start this, I was thinking it's $\mathbb E[\Lambda|X=x]$, but I can't find any documentation on how conditional expectation is solved when the distributions aren't both discrete or both continuous.","['statistics', 'probability']"
2203320,Why don't we approach the point angularly to find the derivative of Complex functions?,"In case of Real functions, we approach the point from left and right and see if both derivatives are equal. But in case of Complex functions, the point can be approached from infinitely many directions. Then how can we check that the derivatives from all the directions are equal? I think in case of Complex functions, we should approach the point angularly. For example, to find a derivative at a point $z$, we can rotate through an angle $d\theta$ about the origin clockwise or anti-clockwise and then find the difference in the value of the function, then divide by the angle change and apply the limit $d\theta \rightarrow0$. In this case, we'll have clock-wise and anti-clockwise derivatives and we can see if they're equal. So, if the argument of $z$ is $\arg{(z)}$, then the derivative would be: $$\lim_{d\theta \rightarrow0}\frac{f(|z|(\cos{(\arg{(z)}+d\theta})+i\sin{(\arg{(z)}+d\theta})))-f(z)}{d\theta}$$ I think approaching the point angularly should be more convenient than approaching the point from all directions. So, why do we approach the point linearly even in case of Complex functions? EDIT: I've one more idea, we can even approach the point from outward and inward directions along the line joining the point with the origin. So, we have outward derivative and inward derivative. In this case the angle remains constant but the modulus changes by an infinitesimal amount. It will be: $$\lim_{dz\rightarrow 0}\frac{f((|z|+dz)(\cos{(\arg{(z)})}+i\sin{(\arg{(z)})})-f(z)}{dz}$$ So, if a Complex function is given as a function of modulus ($|z|$) and argument ($\theta$ )instead as a function of the co-ordinates, then my 'angluar' derivative is the partial derivative of the function w.r.t $\theta$ and my 'modular' derivative is the partial derivative of the function w.r.t. $|z|$. Together, both my angular and modular derivatives can get from value of the function to another. For example, let $f(|z|,\theta)$ be a complex function. Let it's partial derivative w.r.t $\theta$ be $a(|z|,\theta)$ and the partial derivative w.r.t $|z|$ be $m(|z|,\theta)$. Let $z_1$ and $z_2$ be two points. Then, $$f(z_2)=f(z_1)+\int_{\arg{(z_1)}}^{\arg{(z_2)}}a(|z_1|, \theta)d\theta+\int_{|z_1|}^{|z_2|}m({|z|, \arg{(z_2)}})d|z|$$ Also, what if the definition of the complex derivative is $$f'(z) = \lim_{w\to z} \frac{f(w)-f(z)}{w-z}$$ That doesn't make a difference. Problem is, how do we evaluate that limit? If we substitute $w=z+h$, then that's the same thing as approaching $z$ from the right along the line parallel to the the real axis passing through $z$. If we substitute $w=z+ih$, then that's the same as approaching $z$ from upward along the line parallel to the imaginary axis passing through $z$. If we substitute $w=z+h(cos45+isin45)$, then that's the same as approaching $z$ from North-East along the line passing through $z$ and inclined 45 degrees to the real axis. But how do we know that all those approaches are equal? If they are, only then the derivative exists.","['derivatives', 'complex-numbers', 'calculus', 'functions', 'complex-analysis']"
2203324,"When ring is commutative, prove that left and right modules coincide","I have the following definition of a left $R$-module for a ring $R$ and an abelian group $X$: these conditions must be satisfied: $$(\alpha+\beta)x = \alpha x + \beta x$$
$$\alpha(x+y) = \alpha x + \alpha y$$
$$\alpha(\beta x) = (\alpha\beta)x$$
$$1x = x$$ The book then asks me to define a right $R$-module and prove that, when the ring $R$ is commutative, both definitions coincide. That's what I did: Definition for the right $R$-module: $$x(\alpha+\beta) = x\alpha + x\beta$$ $$(x+y)\alpha = x\alpha + x\beta$$ $$(x\beta)\alpha = x(\beta\alpha)$$ $$x1 = x$$ I think now I need to prove that $$(\alpha+\beta)x = x(\alpha + \beta)$$
$$\alpha(x+y) = (x+y)\alpha$$
$$\alpha(\beta x) = (x\alpha)\beta$$
$$1x = x1$$ at least for the last two: $$\alpha(\beta x) = (\alpha\beta)x = (\beta\alpha)x = \beta(\alpha x)$$ but how to prove that $\alpha x = x\alpha$? And for the unit it's easy: $$1x = x = x1$$ but what about the first two?","['abstract-algebra', 'ring-theory', 'modules']"
2203355,"Null space, column space and rank with projection matrix","If I have a projection matrix L in $\mathbb {R^4}$ , I'm just wondering how L would transform vectors in the nullspace of $[L]$ and the column space. I'm also trying to figure out how these pieces of information allow me to find the rank and nullity of $[L]$ without elementary row operations. For context, here is the question: $\text{The linear transformation of $L:\mathbb {R^4}\rightarrow \mathbb {R^4}$ projects $\mathbb {R^4}$ orthogonally}$
$\text{onto the subspace $V=\text{span}\{a,b\}$, with}:$ $a=(1,1,1,1)$ $b=(4,2,1,2)$ $\text { (a) How does L transform vectors transform vectors in the null space of [L]?}$ $\text { (b) How does L transform vectors transform vectors in the Column space of [L]?}$ $\text {(c)Explain how the answers to parts (a) and (b) }$
$\text{enable you to find the rank and the nullity of [L] without row reduction.}$ I'm looking at notes here , but I'm having a hard time coming up with some reasoning. I can see that the null space and the column space are orthogonal to each other but I am not really sure how that would explain the transformation or help with part $(c)$ in any way. If someone could nudge me in the right direction that would be great! I mean I feel like null space comes into play somehow because I am doing projections and since I want the matrix to be orthogonal, the dot product has to be $0$ so I am trying to see if I can relate that somehow. The fact I am writing the vectors as columns I feel like has to do with something in the column space but I'm not entirely sure about that... For part $(c)$, I feel like I have to use the rank nullity theorem somehow but I am not sure about this...","['linear-algebra', 'linear-transformations']"
2203435,Prove that the real projective line cannot be embedded into Euclidean space,"Can the real projective line $RP^1$ be embedded into $\mathbb{R}^n$ for any $n$? At first, I thought it could because $RP^2$ can be embedded in four-dimensional space, and $RP^1$ seemed like a simpler object to deal with than $RP^2$. But after drawing diagrams and attempting to find an embedding, I'm getting less and less sure that it can be embedded... unrigorously it always seems to require that 'point at infinity' that comes from say the one-point compactification of $\mathbb{R}$, so is there a way that I could prove this one way or the other?","['general-topology', 'projective-space']"
2203463,Why does the Newman/Zagier proof of the PNT invoke complex analysis?,"I was thinking, specifically, of this paper , in which Zagier offers a proof of the PNT, inspired by a paper of Newman's, the Cliffs Notes version of which would be that, first, it's fairly easy to show that $$\left|\int_1^{\infty}\frac{\vartheta(x) - x}{x^2}dx\right| < \infty \implies \vartheta(x) \sim x$$ As well as that $$\lim_{s\to1^+}\int_1^{\infty}\frac{\vartheta(x) - x}{x^{1+s}}dx = \lim_{s\to1^+}\left(\sum\frac{((1-s)p^s - 1)\log p}{sp^s(p^s - 1)} - \frac{d}{ds}\log((s-1)\zeta(s))\right)$$ which converges.  What Zagier does from here to prove the convergence of the integral itself is that he uses a particular Tauberian theorem for a slightly rewritten integral, whose preconditions take a fair amount of complex analysis to justify. What I was wondering was, from this step, since the absolute value of the integrand will at every point be increasing as $s$ decreases, couldn't you prove its convergence more easily with the monotone convergence theorem?  Since Zagier is a world-famous mathematician and I collect academic suspensions like expensive pogs, I'm guessing the answer is ""no,"" but I'd like to know why.","['number-theory', 'real-analysis', 'proof-explanation']"
2203476,Expected sum of drawing 4 poker cards?,"You have a deck of 52 cards. You draw 4, what is the expected value of the sum of those 4 cards that you draw? Attempt: Basically, the possible sum ranges from 4 to 52. Pr{sum = 4} = Pr{ 4 ace} = 4/52*3/51 *2/50*1/49 And then, you repeat this for sum =5, 6, 7, .... , 52. Then. $E[Sum] =\sum_{i=4}^{53}i*Pr(i)$ I am wondering is there better/quicker approach besides manually finding each probability?","['probability-theory', 'probability', 'probability-distributions']"
