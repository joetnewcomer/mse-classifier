question_id,title,body,tags
2796632,Finding the Jordan canonical form of A and Choose the correct option,"Let $$ A = \begin{pmatrix}
  0&0&0&-4 \\ 1&0&0&0 \\ 0&1&0&5 \\ 0&0&1&0
 \end{pmatrix}$$ Then a Jordan canonical form of  A is Choose the correct option $a) \begin{pmatrix}
  -1&0&0&0 \\ 0&1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2
 \end{pmatrix}$ $b) \begin{pmatrix}
  -1&1&0&0 \\ 0&1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2
 \end{pmatrix}$ $c) \begin{pmatrix}
  1&1&0&0 \\ 0&1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2
 \end{pmatrix}$ $d) \begin{pmatrix}
  -1&1&0&0 \\ 0&-1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2
 \end{pmatrix}$ My attempt : I know that  Determinant   of  A = product  of eigenvalues of A,  as  option c and d  is  not correct because  Here Determinant of A = 4 that is $ \det A =  -(-4) \begin{pmatrix}1 & 0 &0\\0& 1 & 0\\ 0&0&1\end{pmatrix}$ I'm  in  confusion  about  option  a) and  b).......how  can I find the  Jordan canonical form of A ? PLiz  help  me. Any hints/solution will be appreciated. Thanks in advance","['matrices', 'jordan-normal-form', 'linear-algebra']"
2796647,$\mathbb{P}\{\lim_{n \to \infty }|Z_n - Z| = 0\} = 1 \iff \mathbb{P}\{\limsup_{n \to \infty} |Z_n - Z| = 0\} = 1$,"In a proof I read, it is claimed that for a sequence $(Z_n)_n$ of random variables: $\mathbb{P}\{\lim_{n \to \infty} |Z_n - Z| = 0\} = 1 \iff \mathbb{P}\{\limsup_{n \to \infty} |Z_n - Z| = 0\} = 1$ I can see that $\implies$ holds, but why does the other implication hold?","['probability-theory', 'probability']"
2796654,if $u=x^2$ can you say $x^2$ is a function of $u$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question If $$u=x^2$$ then obviously $u$ is a function of $x$ and $x$ is not a function of $u$. But can we say that $x^2$ is a function of $u$? Thanks",['functions']
2796662,Cardinality of the set of infinite binary sequences,"Let $B := \{ (x_n) \mid x_n \in \{0, 1\}, n \in \mathbb N \}$ then prove that $|B| = 2^{\aleph_0}$. I know that the given set $B$ is uncountable. This can be deduced by proving that any countable subset of sequences of $B$ will be a proper subset. $B$ being countable would then give a contradiction. To explicitly find out the cardinality of $B$, however, is what the problem demands. Will it be correct to say that since there are exactly $2$ choices ($0$ or $1$) for each term of any infinite binary sequence, whose cardinality is ${\aleph_0}$, so, the cardinality of $B$ is $2^{\aleph_0}$?","['cardinals', 'elementary-set-theory']"
2796667,Evaluate $\oint_C \frac{3}{z + 1 + i}dz$ along the circle $|z| = 2$,"Evaluate $$\oint_C \frac{3}{z + 1 + i}dz$$ along the circle $|z| = 2$ The solution that I saw read as follows: The integrand is not holomorphic at $z = -1-i$ and this point $(-1, -1)$ lies within $C$. Put $z-z_0 = re^{it}$ where $z_0 = -1-i$ so $dz = ire^{it}$. So$$\oint \frac{3}{z+1+i}dz = \oint \frac{3}{re^{it}}ire^{it}dt = 3i\int_0^{2\pi}dt = 6\pi i$$ This was my lecturer's solution to the above problem. I can see that the lecturer ""set"" $z+1+i = re^{it}$ but how is this even possible? Because firstly $r$ is not even defined and secondly $2 \in C$ but $2+ 1 + i$ is certainly not in $C$. For example a parametrization of the circle $C = \{z \in \mathbb{C} \ | \ |z| = a\}$ is given by the holomorphic function $f : [0, 2\pi] \to C$ defined by $f(t) = ae^{it}$, then for any point $z \in C$ we have $z= f(t) = ae^{it}$ for some $t \in [0, 2\pi]$. What my lecturer did above does not seem correct. What would a correct solution look like for this? Taking the parameterizatin $z = 2e^{it}$ does not help in this case I think because we'd then end up with $$\frac{6ie^{it}}{2e^{it}+1+i}$$ as the integrand (which we'd be integrating from $t=0$ to $t=2\pi$",['complex-analysis']
2796687,Find all parallel vector fields along a curve,"I am given a local representation of a metric $g$ on the real projective space $\mathbb{R}P^2$ $$g^{\varphi} = \frac{1}{(\rho^2+1)^2} d \rho^2 +\frac{\rho^2}{(\rho^2 + 1)} d\theta^2$$
where 
$$\varphi \colon (\rho, \theta) \mapsto [\rho \cos \theta, \rho \sin \theta, 1]$$
and the curve
$$\alpha \colon t \mapsto [r \cos t, r \sin t, 1] \in \mathbb{R}P^2$$
I am asked to find all parallel vector fields along $\alpha$, and I'm a bit lost on how to proceed. If anyone could give some directions on what I should do, I'd be grateful. Thank you in advance. EDIT So I am going to try something I am not entirely sure it works. First it seems that we need to do $\Gamma_{ij}^k \circ \alpha$ for $k = 1, 2$. For $k=1$ one gets 
$$\begin{pmatrix}
-\frac{2r\cos t}{r^2\cos^2t + 1} & 0\\
0 & -r\cos t
\end{pmatrix}$$
and for $k = 2$
$$\begin{pmatrix}
\frac{1}{r \cos t(r^2\cos^2 t + 1)} & 0 \\
0 & \frac{1}{r\cos t(r^2 \cos^2 t + 1)}
\end{pmatrix}$$
since $\rho(t) = r\cos t, \theta(t) = r\sin t$. Now I have to multiply dos two matrix above by $\alpha'(t)$ so that the I get a system of odes. But my curve $\alpha(t)$ has three coordinates (the two real and the point at the infinity) so how should I treat that?","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
2796694,How to derive an equation for terminal velocity assuming air resistance is some constant multiplied by the square of velocity?,"So for my latest physics homework question, I had to derive an equation for the terminal velocity of a ball falling in some gravitational field assuming that the air resistance force was equal to some constant c multiplied by $v^2.$ So first I started with the differntial equation: $\frac{dv}{dt}=-mg-cv^2$ Rearranging to get: $\frac{dv}{dt}=-\left(g+\frac{cv^2}{m}\right)$ From here I tried solving it and ended up with: $\frac{\sqrt{m}}{\sqrt{c}\sqrt{g}}\arctan \left(\frac{\sqrt{c}v}{\sqrt{g}\sqrt{m}}\right)+C=-t$ I rearranged this to get:
$v\left(t\right)=\left(\frac{\sqrt{g}\sqrt{m}\tan \left(\frac{\left(-C\sqrt{c}\sqrt{g}-\sqrt{c}\sqrt{g}t\right)}{\sqrt{m}}\right)}{\sqrt{c}}\right)$ In order to calculate the terminal velocity I took the limit as t approaches infinity: $\lim _{t\to \infty }\left(\frac{\sqrt{g}\sqrt{m}\tan \:\left(\frac{\left(-C\sqrt{c}\sqrt{g}-\sqrt{c}\sqrt{g}t\right)}{\sqrt{m}}\right)}{\sqrt{c}}\right)$ This reduces to:
$\frac{\sqrt{g}\sqrt{m}\tan \left(\infty \right)}{\sqrt{c}}$ The problem with this is that tan $(\infty)$ is indefinite. Where did I go wrong? Could someone please help properly solve this equation. Cheers, Gabriel.","['physics', 'ordinary-differential-equations', 'infinity']"
2796722,Why can't we write $\sin x$ as $\prod_{n=0}^{\infty}\left(x^2-n^2\pi^2\right)$?,"Why can't we write $\sin x$ as $\prod_{n=0}^{\infty}\left(x^2-n^2\pi^2\right)$ ? Since $ n\pi$ where $n \in \mathbb{N}$ are all the roots of $\sin{x}$ , then by the fundamental theorem of arithmetic it may be written as $$\sin x=\prod_{n=0}^{\infty}\left(x^2-n^2\pi^2\right)$$ But this does not actually represent $\sin{x}$ as shown by graph below Then what modifications have to be made to make both graphs look similar, Please Explain?","['roots', 'infinite-product', 'trigonometry', 'calculus']"
2796727,Is the set of all real symmetric matrices all of whose eigenvalues satisfy $|λ| \le 2 $ compact?,"Is the set of all real symmetric matrices all of whose eigenvalues satisfy
  $|λ| \le  2 $ compact? Let $S$ be the set of all real symmetric matrices all of whose eigenvalues satisfy
$|λ| \le  2 $. Define $f:M_n(\Bbb R)\to M_n(\Bbb R)$  by $f(A)=A-A^T$ which is continuous . Then $S=f^{-1}(0)$ and hence is closed. Also any $A\in S$ is diagonalisable and hence $A=PDP^T$ where $D$ is a diagonal matrix and its diagonal entries are the eigen values of $A$. But the diagonal entries of $D$ are $\le 2$ and hence bounded. How can we conclude that $S$ is bounded from here? Please help.","['general-topology', 'metric-spaces', 'linear-algebra', 'compactness']"
2796817,Why it is necessary to maximize and then minimize Lagrangian in hard margin SVM?,"I was learning about support vector machines from Andrew Ng video lectures. I figured it out. I understand why we try to minimize $\frac{1}{2} w^2$. Margin (width) of the support vector is $2/\|w\|$. We want to maximize width it means that we also want to minimize $\|w\|$ from the equation $2/\|w\|$. It is true that we also want to minimize $\frac{1}{2}\|w\|^2$. Now we use Lagrange expression here, $$L = \frac{1}{2}\|w\|^2 + \sum_i α_i(y_i(w \bullet x+b)-1).$$ To find the minimum of $\frac{1}{2}\|w\|^2$, we apply gradient; $∇L = 0$. From the $∇L = 0$ equation, we get $Σ_iα_iy_i = 0$ and $w = Σ_iαi_iy_ix_i$. Then by using $Σ_iα_iy_i = 0$ and $w = Σ_iα_iy_ix_i$ equations and putting this in the Lagrange expression we end up with $$L(w,b,α) = \sum_iα_i-\frac{1}{2}\sum_{i,j}y_iy_jα_iα_j(x_i\bullet x_j).$$ I really understand up to here, but the professor said that we want to maximize  $L$ (w.r.t $α$) and at the same time minimize it w.r.t $w$ and $b$ here, so the final Lagrangian primal problem becomes $$min_{w,b}max_α L(w, b, α)$$ $$s.t.$$ $$α_i >= 0, i=1,2 ..., m$$ why? 
I really don't understand. Why are we trying to maximize and then minimize the Lagrange expression ($L$)? Any intuitive explanation will help.","['machine-learning', 'convex-optimization', 'lagrange-multiplier', 'calculus']"
2796825,If $\left< e^{-x}\right>=1$ then $\left<x\right>\ge 0$.,"I am wanting to prove that:
$$\left< e^{-x}\right>=1\Rightarrow \left<x\right>\ge 0\tag{a}$$
which comes up in nonequilbrium physics with $x$ being the difference in entropy. Intuitively it seems correct as we have:
$$ \int dx\; e^{-x} P(x)=1\tag{b}$$
thus $P(x)$ will need to be larger for the $x \gt 0$ terms then for the $x\lt 0$ due to the $e^{-x}$ factor weighing down $x\gt 0$ terms. But how can I rigorously prove (a)?","['expectation', 'probability']"
2796826,"You roll a die until you get a $5$, what is the expected value of the minimum value rolled?","I am struggling to work out a simple way to answer this, and a rationale behind this approach using tail sum (as I do not understand): $$E\left( x\right) =\sum ^{5}_{k=1}P\left( x\geq k\right) =\dfrac {1}{6}\sum ^{5}_{k=1}\left( \sum ^{\infty }_{i=0}\left( \dfrac {k}{6}\right) ^{i}\right)  = \frac{137}{60}.$$ Does this always hold? $$E\left( x\right)=\sum ^{n}_{k=1}kP\left( x= k\right)  =\sum ^{n}_{k=1}P\left( x\geq k\right)$$ I have never seen this formula, but working through it I understand. EDIT: I can get the right answer with a long winded method, calculating each probability separately, which I believe the Tails sum speeds up: This is my long winded approach. $$E\left( X_{\min }\right) = 5P\left( x= 5\right) +\ldots +1P(x=1)$$ $$=5\left( \dfrac {1}{6}\sum ^{\infty }_{i=0}\left( \dfrac {1}{6}\right) ^{i}\right) +4\left( \dfrac {1}{6}\sum ^{\infty }_{i=1}\left( \dfrac {2}{6}\right) ^{i}- \dfrac {1}{6}\sum ^{\infty }_{i=1}\left( \dfrac {1}{6}\right) ^{i}\right)+\ldots$$ $$+1\left( \dfrac {1}{6}\sum ^{\infty }_{i=1}\left( \dfrac {5}{6}\right) ^{i}- \dfrac {1}{6}\sum ^{\infty }_{i=1}\left( \dfrac {4}{6}\right) ^{i}\right),$$ where $\dfrac {1}{6}$ represents getting a $5$ , so the first sum is all the possibilities of getting repeated $6$ s and then a $5$ , or just rolling a $5$ . The next sum is all the possibilities of getting a $4$ or a $6$ , then a $5$ , minus all the possibilities of just getting a $6$ then a $5$ . So it represents all the strings of just $4$ s or $6$ s before getting a $5$ , so all the possibilities of minimum value being a $4$ .","['algebra-precalculus', 'dice', 'probability', 'expected-value']"
2796830,Importance of Measure Theory,I know that the whole concept of Measure was introduced to develop Lebesgue integration. But I just wondered Even if we had Riemann integration already why it was necessary . Thanx and regards in advance,"['riemann-integration', 'lebesgue-measure', 'integration', 'lebesgue-integral', 'measure-theory']"
2796849,"Relations between the ""$\forall$"" quantifier and integration","I'm studying introduction to mathematical logic, we recently began predicate logic and I find similarities between the ""$\forall$"" predicate and integration: 1. They both have a dummy variable, which they ""enumerate"" over its possible values 2. The fact that $\forall x \forall y \alpha \vDash \forall y \forall x \alpha$ is similar to Fubini's theorem. Is there a formalism that relates the two?","['predicate-logic', 'integration', 'logic', 'calculus']"
2796881,stabilizer of action of a group scheme on a scheme,"Let $G$ be a group scheme over a basis $S$, and $X$ be a scheme over $S$. Let $\rho: G \times_SX \to X$ be an action of $G$ on $S$. If $T$ is an $S$-scheme and $x \in X(T)$, then the definition of the stabilizer of $x$ is given as the subgroup scheme of $G_T$ that represents the functor $T' \mapsto \{g \in G(T') \mid g \cdot x = x\}$ on $T$-schemes $T'$. I have problem understanding this definition. Should the $G(T')$ actually be a $G_T(T')$, since we're defining a subgroup scheme of $G_T$? And how does $g \in G(T')$ (or $G_T(T')$) act on $x \in X(T)$? We only know a priori that $G(T')$ acts on $X(T')$.","['abelian-varieties', 'group-schemes', 'algebraic-geometry']"
2796926,L'Hôpital's rule: number of iterations...,"I have this exercise : $$ \lim_{x\to\infty} {(\ln x)^9\over x} $$ Now, I know I must use the L'Hôpital's rule to solve this one, until I reach: somthing*$ 1/x^2 $ ,because each time I get infinity/infinity, I am allowed to use L'Hospital's rule. My question is: Is there any shortcut to solve such problems? Sorry for bad English.","['infinity', 'calculus']"
2796940,A situation with limits,"In a proof, I encountered the following situation: For every $n \in \mathbb{N}$ between $k^2 \leq n \leq (k+1)^2$, we have: $$a_k \leq b_n \le c_{k+1}$$ where $a_k, c_k \to 0$ if $k \to \infty$. They then conclude that $\lim_{n \to \infty} b_n = 0$. I can see this intuitively but can't write it out rigorously.","['real-analysis', 'limits']"
2796981,Applying the Poincaré-Bendixson Theorem to show the existence of a limit cycle,"Show that there exists a limit cycle by finding a positive invariant set
   $$y'' = -y + y'(1-3y^2-2y')^2$$ I rewrote this second order ODE as a system of two first order ODE's $y_1' = y_2$ and $y_2' = -y_1 + y_2(1-3y_1^2-2y_2)^2$ Then I rewrote the system in polar co-ordinates: $r' = r\sin^2\theta(1-2r^2\cos^2\theta-2r\sin\theta)^2$ and
$\theta' = 1 + \sin\theta(1-3r^2\cos^2\theta-2r\sin\theta)\cos\theta$ Then I used the fact that $0 \leq \sin^2\theta \leq 1$ to get $0 \leq r' \leq r(1-2r^2\cos^2\theta-2r\sin\theta)$ and then the fact that $0 \leq \cos^2\theta \leq 1$ to get $$r(1-2r\sin\theta)^2 \leq r' \leq r(1-3r^2 -2r\sin\theta)^2$$ and then I used the fact that $-1 \leq \sin\theta \leq 1$ to get $$r(1-3r^2 + 2r)^2 \leq r' \leq r(1-2r)$$ From here I'm not sure how to proceed to find this positive invariant set. The solutions to this question state that the positive invariant set is given by $$D = \left\{ r \ : \frac{1}{3} \leq r^2 \leq \frac{1}{2}\right\}$$ And I'm not sure how to arrive at that.","['ordinary-differential-equations', 'dynamical-systems']"
2797010,Find point in which 2 functions are tangent,Let $f(x)=x^6$ and $g(x)=2x^5-2x-1$. Find x such that these to functions are tangent. My attempt: $f(x)=g(x)\implies x^6-2x^5+2x+1=0$ and I know it's a polynomial and I could approximate it's roots but I have to find the exact $x$ such that they are tangent. Or I thought their tangents to the graphic in that point must be equal so $$f'(x)=g'(x)\implies 3x^5-5x^4+1=0.$$ and yet again another polynomial which does not have any nice roots. Are there any other ways or am I just supposed to guess the solutions? Or approximate the solutions and the answers?,"['derivatives', 'real-analysis', 'calculus', 'tangent-line']"
2797012,Maximum and minimum value of Determinant of $3 \times 3$ Matrix with entries $\pm 1$,"Find Maximum value of Determinant of $3 \times 3$ Matrix  with  entries $\pm 1$ My try: I considered a matrix as : $$A=\begin{bmatrix}
1 &-1  &-1 \\ 
-1 &1  &-1 \\ 
 -1&-1  &1 
\end{bmatrix}$$ we have $$Det(A)=-4$$ and maximum is $4$, but how can we show that these are max and min values? I also tried as follows: By definition Determinant of a matrix is dot product of elements of any row with corresponding Cofactors hence $$Det(A)=a_{11}C_{11}+a_{12}C_{12}+a_{13}C_{13}$$ By cauchy Scwartz Inequality we have $$a_{11}C_{11}+a_{12}C_{12}+a_{13}C_{13} \le \left(\sqrt{a_{11}^2+a_{12}^2+a_{13}^2}\right)\left(\sqrt{C_{11}^2+C_{12}^2+C_{13}^2}\right)$$ any way to proceed here?","['matrices', 'algebra-precalculus', 'linear-algebra', 'determinant']"
2797034,Existence of complex branch for real exponents,"I've recently encountered a problem regarding complex branches that made me feel like there is something fundamental about branches I do not understand: The problem Let $G$ be an open subset of $\mathbb{C}$ and $z_1,z_2,...z_m \in \mathbb C \setminus G$ be distinct points such that there are real numbers $a_1,a_2,...a_m$ satisfying:
$$\sum_{j=1}^ma_jInd_\gamma(z_j)=0$$
for every closed path $\gamma \subset G$. Show that there is a holomorphic branch of $(z-z_1)^{a_1}(z-z_2)^{a_2}...(z-z_m)^{a_m}$ in $G$. My confusion The basic idea behind the statement of the problem seems intuitively obvious to me, but I've had great difficulty expressing a solution rigurously, which makes me believe there's something fundamental I'm missing here. My approach was to try and utilize a theorem that states $logf$ has a holomorphic branch in $G$ if and only if $$\int_\gamma \frac{f'}{f}dz=0$$
for every closed path $\gamma \subset G$. However, the only sensible way I can think of to define $f$ is by $f=\exp[\sum_{j=1}^{m}a_jlog(z-z_j)]$ Obviously if the exponent is holomorphic, so is $f$, but how can it be shown that it is indeed holomorphic? Induction does not work because the separate $log(z-z_j)$ may not exist on their own, even though their weighted sum does. It is also quite simple to show that $f$ as defined in the problem satisfies the conditions of the theorem, and hence the sum of logarithms in the above exponent is indeed holomorphic.. but that presumes that $f$ exists (and is holomorphic) in the first place. Can $f$ be defined in some other way that makes this all fall into place?","['logarithms', 'complex-analysis', 'winding-number']"
2797035,How to apply probability with a deck of cards,"For this question, I'm not sure if I am doing it right. Here is what I have so far? Can anyone please help me out? Suppose we roll a fair six-sided die and then pick a number of cards from a
well-shuffled deck equal to the number showing on the die. (For example, if the die
shows $4$, then we pick $4$ cards.)
What is the probability that the number of jacks in our hand equals $2$? $P(\text{rolling a number on die}) = \frac{1}{6}$ $P(\text{number of jacks $= 2$}) = \frac{1}{6}\times{^{52}C_4}\times{^{48}C_2}$","['probability-theory', 'probability']"
2797039,Two inequalities about using Fatou Lemma,"1) Let $\{f_n\}$ be a sequence of nonnegative measurable functions of $\mathbb R$ that converges pointwise on $\mathbb R$ to $f$ integrable. Show that $$\int_{\mathbb R} f = \lim_{n\to \infty}\int_{\mathbb R}f_n  \Rightarrow \int_{E} f = \lim_{n\to \infty}\int_{E}f_n $$ for any measurable set $E$ I know that $\int_{\mathbb R} f = \int_{\mathbb R \setminus E} f + \int_{E} f$ and $\int_{\mathbb R \setminus E} f \le \liminf_{n\to {\infty}}\biggr(\int_{\mathbb R \setminus E} f_n \biggr)$ from Fatau's Lemma. I couldn't obtain $\int_{E} f = \liminf_{n\to \infty}\int_{E}f_n = \limsup_{n\to \infty}\int_{E}f_n$ and I have seen that inequality below for obtaining it but I couldn't understand. Could someone explain me please? $$\liminf_{n\to \infty}\int_{\mathbb R \setminus E}f_n = \int_{\mathbb R}f-\limsup_{n\to \infty}\int_{E}f_n$$ 2) It has been written ""since $\int_Ef_n \le \int_Ef$ (this inequality from monotonicity I have understood) thus $$\limsup\int_Ef_n \le \int_Ef$$ in proof of The Monotone Convergence Theorem in Royden's Real Analysis. I couldn't see why that inequality obtains. Thanks for any help Regards","['real-analysis', 'lebesgue-integral', 'lebesgue-measure']"
2797041,Solving improper integral involving product logarithm and exponentials,"It is possible to verify or show numerically that $\displaystyle \int^{\infty}_{-\infty}W(e^{x-e^{x}})dx=\frac{\pi^2}{12}$ $W(x)$ is the Lambert W function. This integral does not seem to have a solution in terms of elementary or standard functions. Is it possible to prove or solve it analytically, or to find a symbolic solution?","['integration', 'lambert-w', 'calculus', 'analysis']"
2797074,A map with a small bound on the ratio of Dini derivatives is injective on the unit ball in Hilbert spaces,"Let $f$ be a continuous map defined from the unit ball $U$ of a Hilbert space $E$ to another Hilbert space $F$. The Dini derivatives are defined as (for $x\in U$)
$$D^+f(x)=\limsup_{y\to x}\frac{\Vert f(y)-f(x)\Vert}{\Vert y-x\Vert}$$
and
$$D^-f(x)=\liminf_{y\to x}\frac{\Vert f(y)-f(x)\Vert}{\Vert y-x\Vert}$$ I have read that if on $U$ we have $0<m\leq D^-f(x)\leq D^+f(x)\leq M<\infty$ and $$k=M/m<\sqrt{(1+\sqrt{5})/2}$$ then one can prove that $f$ is a homeomorphism from $U$ to $f(U)$ More precisely, for $x, y\in U$ we have 
$$\Vert f(x)-f(y)\Vert\geq \mu\Vert x-y\Vert$$
where $$\mu=m\frac{(1+k\sqrt{k^2-1})}{1+\sqrt{k^2-1}}$$
Does anybody know a proof of this strange result or a reference for a proof of it (that seems to work only because of the geometry of Hilbert space, not in a more general Banach setting)?","['functional-analysis', 'hilbert-spaces']"
2797141,About the isotriviality of pencils of plane curves.,"Let $F$ and $G$ be coprime homogeneous polynomials in three variables of the same degree $d\geq 4$. Suppose that a general member of the pencil $\{F+tG=0\}\subset \mathbb{P}^2$ is smooth. Which are the tools to verify whether this pencil is isotrivial (general members of the pencil are isomorphic)? I wish to avoid a brute force computation of possible isomorphisms in ${\rm PGL}(3,\mathbb{C})$.","['algebraic-curves', 'algebraic-geometry']"
2797221,"If $c$ is the chromatic number of $X(G)$, does $G$ contain an abelian subgroup of order $c$?","Let $G$ be a finite group. Define a simple graph (called the commuting graph) $X(G)$ as follows: the vertices of $X(G)$ are the elements of $G$ and two distinct vertices $x, y$ form an edge if and only if $xy=yx$ . Question: If $c$ is the chromatic number of $X(G)$ , does $G$ contain an abelian subgroup of order $c$ ? Notice that the existence of such a subgroup is equivalent to the existence of a complete subgraph on $c$ vertices of $X(G)$ . I tried using Dirac's Theorem: Let $c$ the chromatic number of a graph $X$ . If $X$ does not contain $c$ -cliques and $$T=\{x\in V(X) | d(x)>c-1\},$$ then $\sum_{x \in T} (d(x)-c+1)\geq c-3$ .","['finite-groups', 'graph-theory', 'group-theory']"
2797249,What does $A^{-1}=A^T$ have to do with “orthogonality”?,"Whenever I read some use of the term “orthogonal”, I have been able to find some way in which it is at least metaphorically similar to the idea of two orthogonal lines in euclidean space. E.g. orthogonal random variables, etc. But I cannot see how $A^{-1}=A^T$ captures the idea of “orthogonality”. What is “orthogonal” about a matrix that satisfies this property?","['intuition', 'matrices', 'terminology', 'orthogonality', 'orthogonal-matrices']"
2797272,What does Infimum of Upper Sum and Supremum of Lower Sums mean?,"I'm trying to figure out the Darboux Integral definition as it states: 
$f$ is integrable if $inf${U($f$,$P$)} = $sup${L($f$,$P$)}, where U($f$,$P$) is the upper sum, L($f$,$P$) is the lower sum of $f$ and $P$ is a parition of $f$. I'm not understanding what the $sup$/$inf$ of the sums mean. When calculating, they are a finite value (ie. L($f$,$P$) = $\sum_{i=1}^n m_i (x_i - x_{i-1})$) Consider: $$f(x) = 2x, x \in [0,1];~ P = \{0,\frac{1}{4},\frac{1}{2},1\}.$$ Since $P$ has 4 elements, n=3, thus 3 subintervals of $[0,1]$, can you definte these 3 subintervals however you want as long as they range [0,1]? ie. $$[0,\tfrac{1}{4}] \,\cup\, [\tfrac{1}{4},\tfrac{1}{2}] \,\cup\,[\tfrac{1}{2},1] \text{ or } [0,\tfrac{1}{3}] \,\cup\, [\tfrac{1}{3},\tfrac{2}{3}] \,\cup\,[\tfrac{2}{3},1] $$ So to calculate $U(f,P)$ for the first set of subintervals: $$U(f,P) = \sum_{i=1}^n M_i (x_i - x_{i-1})$$
$$ = f(\frac{1}{4})(\frac{1}{4} - 0)\;+ f(\frac{1}{2})(\frac{1}{2} - \frac{1}{4})\;+  f(1)(1 - \frac{1}{2})$$
$$ = \frac{1}{8} + \frac{1}{4} + 1= \frac{11}{8}$$ Calculate $L(f,P)$:
$$L(f,P) = \sum_{i=1}^n m_i (x_i - x_{i-1})$$
$$ = f(0)(\frac{1}{4} - 0)\;+ f(\frac{1}{4})(\frac{1}{2} - \frac{1}{4})\;+  f(\frac{1}{2})(1 - \frac{1}{2})$$
$$ = 0 + \frac{1}{8} + \frac{1}{2}= \frac{5}{8}$$ First off, can someone confirm that my calculations for upper and lower sums are correct? Secondly, back to the main question, what is the $inf${U($f$,$P$)} and $sup${L($f$,$P$)} in this? as my $U$($f$,$P$) = $\frac{11}{8}$ and $L$($f$,$P$) = $\frac{5}{8}$. As I already know that $f$ is integrable, $U$($f$,$P$) = $L$($f$,$P$) only if $f$ is constant, but what is the set in which I'm supposed to take the $inf$ and $sup$ of? As per the Darboux Integral definition, $sup${$L(f,P)$} = $inf${$U(f,P)$} for this function. 
. If someone could clear this up for me it would be greatly appreciated","['riemann-integration', 'supremum-and-infimum', 'calculus', 'integration', 'definite-integrals']"
2797316,Issue with elementary exercise on martingales,"This is Exercise 14.4.1 in Rosenthal's A First Look at Rigorous Probability Theory , Second Edition, page 173. Let $\{Z_i\}$ be i.i.d. with $P(Z_i  =  1)  =  P(Z_i  =  -1)  =   1/2$ . Let $X_0 = 0, X_1 = Z_i$ ,  and  for $n  >  2$ , $X_n  =  X_{n-1}  +  (1 +  Z_1  +
  \cdots
 + Z_{n-1})(2Z_n - 
  1)$ .   (Intuitively,  this  corresponds  to  wagering,  at  each  time
n,
one
dollar  more than  the  number  of  previous  victories.) Prove that $X_n$ is a martingale. Here's my attempt, very standard: Letting $F_n$ be the $\sigma$ -algebra generated by $X_0,...,X_n$ $$\Bbb E (X_{n+1} \mid F_n) = \Bbb E (X_n \mid F_n) + \Bbb E (2Z_{n+1}-1 \mid F_n) + \sum_{i=1}^n \Bbb E ((2Z_{n+1}-1)Z_i\mid F_n)$$ by linearity of the conditional expectation. Now the first summand is $X_n$ , since $X_n$ is $F_n$ -measurable, the second is $-1$ , since $Z_{n+1}$ is independent from $F_n$ , and in the third I can factor out the $Z_i$ in each expectation since they're also $F_n$ -measurable. This leaves us with $$\Bbb E (X_{n+1} \mid F_n) = X_n - 1 + \Bbb E (2Z_{n+1}-1 \mid F_n) \sum_{i=1}^n Z_i$$ once again $\Bbb E (2Z_{n+1}-1 \mid F_n) = \Bbb E (2Z_{n+1}-1) = -1$ , and we're left with $$\Bbb E (X_{n+1}\mid F_n) = X_n - 1 - \sum_{i=1}^n Z_i$$ which is far from the desired result of $X_n$ . Did I make some mistake or is there an error in the text?","['probability-theory', 'conditional-expectation', 'martingales']"
2797344,(approximate) closed form for ${n \choose n/2} p^n$?,"I am looking for (approximate) closed forms for the following expression: 
$$
{n \choose n/2} p^n = \frac{n!}{(n/2)!(n/2)!} p^n,
$$
for a very small constant $p$ (say $p = 1e-5$). What if $p$ is a function of $n$? (say $ p = 1/n$ or $p = \frac{\log n}{n}$)","['combinatorics', 'discrete-mathematics']"
2797348,Commutative subalgebra of matrix-algebra,"Let $M_3$ be the $\mathbb{C}$- vector space of matrices of order 3 with complex entries. What is the dimension of subspace $V$ of $M_3$ satisfying the following both conditions? (a) $AB=BA$ if $A,B\in V$. (b) If $W$ is a subspace of $M_3$ satisfying $W\supset V$ and $W \neq V$, then there exist $A,B\in W$ such that $AB\neq BA$.","['matrices', 'linear-algebra']"
2797352,$E(Y\mid X)$ and $E(Y\mid X=x)$,"I know that from measure-theoretic probability, $E(Y\mid X)$ and $E(Y\mid X=x)$ are different in nature: the former is ""conditional on a random variable"" and the latter is ""conditional on an event"" (let's assume it is a null event here). But I am still not sure about a few things: When the two are equivalent, i.e. one implies the other? If I specify $E(Y\mid X)=X$ and $E(Y\mid X=x)=x$, are the two equations equivalent, i.e., one implies the other? When discussing, e.g., Statistical models like linear regression, we often write $E(Y\mid X)=X\beta$. In this case, are we using ""conditioning on random variable"" or ""conditioning on event""? (This question will be trivial if the answer to question 2 is yes).","['probability-theory', 'conditional-expectation']"
2797360,The four basic combinatoric formulas?,"There are 4 basic combinatoric formulas when picking $k$ elements among $n$ We have repetition is allowed or not allowed, and order matters or does not matter. When order matters and repetition is not allowed we call it a permutation. When order does not matter and repetition is not allowed we call it a combination. What are the names of the missing two and what are the formulas for each?","['permutations', 'combinatorics', 'combinations']"
2797396,Part of proving Schroder-Bernstein Thm.,"In Abbott's real analysis, I am asked to prove Schroder-Bernstein using steps that are given in the book. We have two sets $X$ and $Y$, and there are injections $f:X\rightarrow Y$and $g:Y\rightarrow X$. We are to prove that there exists a bijection $h:X\rightarrow Y$. 
The idea proposed by the author is to first define a set $A_1=X\backslash g(Y)=\lbrace x\in X | x\notin g(Y)\rbrace$, and then inductively define the sets $A_n=\lbrace x\in X | x=g(f(A_n))\rbrace$. Of course, if $A_1=\emptyset$, we are done with the proof as we have that $g(x)$ is bijective. If not, we let $A=\bigcup\limits_{n=1}^\infty A_n$ and $B=\bigcup\limits_{n=1}^{\infty}f(A_n)$, such that $f(A)=B$. Next we define sets $A'=X\backslash A$ and $B'=Y\backslash B$. I am asked to prove the following: Show that $g: Y\rightarrow X$ maps $B'$ onto $A'$. My attempt: My idea is to prove that $A'=g(B')$, i.e. show that $x\in A'\Leftrightarrow x\in g(B')$. First, letting $x\in g(B')$ we know that $x\notin A_1$ as $x\in g(Y)$. We also have that $x\notin A_i$ for all $i\geq 2$ as each $A_{n+1}=g(f(A_n))$ and $B'\notin A_n$ for all $n\in\mathbb{N}$. I'm not sure how to prove that $x\in A'\Rightarrow x\in g(B')$.","['elementary-set-theory', 'functions']"
2797448,Number of ways to flip a coin 10 times with no consecutive heads,"The problem statement is as follows: A fair coin is to be tossed $10_{}^{}$ times. Let $i/j^{}_{}$, in lowest terms, be the probability that heads never occur on consecutive tosses. Find $i+j_{}^{}$. My solution was to consider the sequence of flips as a string of either [Head then Tail] or [Tail]. Let $x$ represent the number of [Head then Tail] and $y$ represent the number of [Tail]. Then $2x$ + $y$ = $10$. Then I did casework for each value of $x$: When $x = 0$ it is bijective to the number of arrangements of $AAAAAAAAAA$, which is $1$. Then, when $x = 1$ it is bijective to the number of arrangements of $AAAAAAAAB$, which is 9 and so on... The sum of these values turns out to be $89$ and the number of ways to flip is $1024$, but that is wrong. What is wrong with my solution? Thanks!","['combinatorics', 'probability', 'discrete-mathematics']"
2797453,What is difference between constant group scheme associated with cyclic group and $\mu_n$,Let $k$ a field. Let $\operatorname{char}(k) \not\mid n$. Consider the two group schemes $\mu_n=spec(k[t]/(t^n-1))$ and $\underline{\mathbb{Z}/n\mathbb{Z}}$ the constant group scheme associated to the cyclic group ${\mathbb{Z}/n\mathbb{Z}}$. Are they isomorphic as group schemes?  Do they become isomorphic when $k$ is algebraically closed? It is false in general because $\mu_n(k)$ may not have $n$ elements. But when are they isomorphic?,"['algebraic-groups', 'group-schemes', 'algebraic-geometry', 'commutative-algebra']"
2797457,Closed-form solution of a linear time-varying system,"Consider the following three-dimensional linear time-varying system
$$\label{eq1}\tag{$\star$}
\dot{x}(t)=(A \cos(\omega_1 t) + B \cos(\omega_2 t) )x(t), \ \ x(0)\in\mathbb{R}^{3},
$$
where $\omega_1$ and $\omega_2$ are positive real numbers such that $\omega_1\ne \omega_2$ and
$$
A=\begin{bmatrix}a & -a & 0 \\ -a & a & 0 \\ 0 & 0 & 0\end{bmatrix}, \quad B=\begin{bmatrix}0 & 0 & 0 \\ 0 & -b & b \\ 0 & b & -b\end{bmatrix}
$$
with $a,b$ positive real numbers. Notice that $A$, $B$ do not commute. However, the commutator
$$
[A,B]=AB-BA = \begin{bmatrix}0 & ab & -ab \\ -ab & 0 & ab \\ ab & -ab & 0\end{bmatrix}
$$
is a skew-symmetric matrix (I don't know if this can be useful though). My questions are: Does there exist an explicit closed-form expression for the solution of \eqref{eq1}? If not, is it possible to find a bound on $\|x(t)\|$ which explicitly depends on $\omega_1$ and $\omega_2$? I've been stuck on this problem for a while now. So I would greatly appreciate any kind of comment or help. Thanks.","['dynamical-systems', 'control-theory', 'closed-form', 'ordinary-differential-equations', 'analysis']"
2797506,What numerical approach should I take with solving this system of 2nd order coupled ODEs?,"I want to solve this system numerically, but I am stuck as to how to proceed.  Do I need to transform this into a set of four 1st order equations before discretizing?  Which numerical method would be recommended here? $ \frac{d^2x}{dt^2} = \omega^2 x + 2\omega\frac{dy}{dt} $ $ \frac{d^2y}{dt^2} = \omega^2 y - 2\omega\frac{dx}{dt} $ Thanks!","['numerical-methods', 'ordinary-differential-equations']"
2797511,How to show the derivative by using the limit definition?,"Let $(\Omega, \mathcal{F}, \mu)$ be a probability space. For the sake of simplicity, let $\Omega$ be $\mathbb{R}$. In what follows, the measurability always refers to Borel measurability. Let $f \colon \mathbb{R}_+ \times \Omega \to \mathbb{R}_+$ be a function such that: (i) For each $z \in \Omega$, the function $k \mapsto f(k,z)$ is concave, increasing, and continuously differentiable,
while $z \mapsto f(k,z)$ is Borel measurable for each $k \in \mathbb{R}_+$; (ii) $\lim_{k \downarrow 0} f'(k,z) >0$ for each $z \in \Omega$.
Here and below, $f'(k,z)$ denotes the partial derivative of $f$ with respect to $k$; and (iii) $f(0,z)=0$ for all $z \in \Omega$. Let $v \colon \mathbb{R}_+ \to \mathbb{R}_+$ be a bounded, strictly concave and strictly increasing function,
and be continuously differentiable on $(0, \infty)$. Define a function $g $  by \begin{align*}
g(k) := \left( \int_{\Omega} \left[ v\left( f( k, z ) \right) \right]^\alpha \mu (\mathrm{d}z) \right)^{1/\alpha},
\qquad (0<\alpha <1).
\end{align*} Question: In fact, since $g$ is concave (it has been proved), we know that the right-hand and the left-hand derivatives of $g$ exist. I aim to show that $g$ is differentiable on $(0, \eta)$ for any fixed constant $\eta >0$, and to show the derivative of $g$ which I conjecture is
  \begin{align*}
g'(k) =  \left( \int_{\Omega} \left[ v\left( f( k, z ) \right) \right]^\alpha \mu (\mathrm{d}z) \right)^{\frac{1}{\alpha} -1 }  
\int_{\Omega} \left[ v\left( f( k, z ) \right) \right]^{\alpha -1} 
v'\left( f( k, z ) \right)
f'( k, z ) \mu( \mathrm{d} z)
\end{align*}
  for all $0 < k < \eta$. Here, $g'(k) = \dfrac{\mathrm{d}}{\mathrm{d} k} g(k)$, $v'\left( f( k, z ) \right) := \dfrac{\mathrm{d}}{\mathrm{d} f }v( f( k, z ))$,
and $f'(k, z) := \dfrac{\partial}{\partial k} f(k,z)$. My attempt: The above stated derivative of $g$ is just my conjecture and I am not sure if the right-hand derivative of $g$ is equal to its left-hand derivative, thus I wish to verify it.
  My attempt is making use of the limit definition to show the left-hand and right-hand derivatives of $g$ are the same and equal to the above stated formula. In fact, I even got stuck in finding the left-hand side and the right-hand side derivatives of $g$. 
But I thought it might suffice to show that the left-hand side derivative $g’_-(k) := \lim_{h \to 0^-} \dfrac{g(k+h)-g(k)}{h}$ is less than the conjecture formulation that stated above, and to show that the right-hand side derivative $g’_+(k) := \lim_{h \to 0^+}\dfrac{g(k+h)-g(k)}{h}$  is greater than the conjecture formulation. Then, by concavity of $g$, we have $g’_-(k) \geq g’_+(k)$ and hence, $g’_-(k)= g’_+(k)=g’(k)$ as desired. In this connection, I think the problem becomes how to establish the relation between the right-hand derivative and conjecture formula, and relation between the left-hand derivative and conjecture formula. Could anyone give me some guidance and help me out please? Thank you very much in advance!","['derivatives', 'real-analysis', 'calculus', 'integration', 'ordinary-differential-equations']"
2797532,One sided bound for Lipschitz functions for gaussian variables,"Let $(X_1, . . . , X_n)$ be a vector of i.i.d. standard Gaussian variables,
and let $f : R^n → R$ be L-Lipschitz with respect to the Euclidean norm. Then the
variable $f(X) − E[f(X)]$ is sub-Gaussian with parameter at most L, and hence $$ P\left[|f(X)-E[f(X)]| \geq t \right] \leq 2e^{\frac{t^2}{2L^2}}$$ Is there an equivalent bound for the one sided equivalent?
$$ P\left[f(X)-E[f(X)] \geq t \right]$$","['real-analysis', 'functional-analysis', 'statistics', 'probability', 'lipschitz-functions']"
2797557,Is there a formula for how the Hodge star interacts with the wedge product?,"Let $V$ be a real vector space of dimension $n$ with an inner product $g$. Let $e_1,\ldots,e_n\in V$ be an orthonormal basis with respect to $g$. On the $k$th exterior power $\bigwedge^k V$ the induced inner product is characterized by the property that the wedges $e_{i_1}\wedge\cdots\wedge e_{i_k}$ give an orthonormal basis for the induced inner product, where $i_1 < i_2 < \cdots < i_k$. In particular, the top wedge power $\bigwedge^n V\cong\mathbb{R}$ is generated by the norm 1 vector $\omega := e_1\wedge\cdots\wedge e_n$. Let $\beta\in\bigwedge^k V$ be a $k$-form. The Hodge dual (or Hodge star) of $\beta$ is by definition the $(n-k)$-form $*\beta$ satisfying:
$$\alpha\wedge(*\beta) = g(\alpha,\beta)\cdot\omega$$
for all $k$-forms $\alpha\in \bigwedge^k V$. I'd like to know if there is a ""formula"" for $*(\alpha\wedge\beta)$ in terms of $\alpha,\beta,*\alpha,*\beta$ where $\alpha,\beta$ are forms of degrees $k,\ell$ respectively, with $k+\ell < n$.","['complex-geometry', 'differential-geometry', 'linear-algebra', 'algebraic-geometry']"
2797561,Random walk length and maximum,"The following a gambling game: We have a sequence of i.i.d. random variables, $X_1, X_2, \dots,$ with $P(X_n=1)=p, P(X_n=-1)=1-p, p<1/2$. A game is played by generating a sequence of realizations of the X's. The game ends when a sequence of $k$ (a given parameter) consecutive ""losses"" is first encountered. For example if $k=4$ a possible game would be $$(1,1,-1,1,-1,-1,-1,1,-1,1,-1,-1,-1,-1)$$ All sequences of consecutive losses have length at most 3 except one which has length 4 and is at the tail. We know for sure that the game will end in a finite time. Question is now what's the distribution of the game time and what's the distribution of the maximum of the partial sums of the X's over the game duration. I've done some poking around with this using recurrence relations but didn't quite crack it. For $k=1$ the game is a series of ""wins"" until the first ""loss"". In this case $T$ (the game time) is given by the geometric distribution. Is there a generalization of that for $k>1$ ? All ideas appreciated. EDIT: Can we find the probability that a sequence of $t$ trials does not contain a $m$ losing streak ? We could take it from there","['random-walk', 'probability-theory', 'probability-distributions']"
2797579,Solving first order differential equation with power series,"So, I was told solve the equation $y' - y = x^2$ using power series. Normal methods tell me that the solution is $y = c_{0}e^{x}-x^{2}-2x-2$, and this can be verified by plugging it back in. However, I am stuck on trying to solve this with power series. We assume $y = \Sigma_{n=0}^{\infty}a_{n}x^{n}$. Thus, $y' = \Sigma_{n=0}^{\infty}a_{n+1}(n+1)x^{n}$. I plug these into the original equation. $$\Sigma_{n=0}^{\infty}a_{n+1}(n+1)x^{n}-\Sigma_{n=0}^{\infty}a_{n}x^{n}=x^{2}
\\
a_{1}x^{0}+2a_{2}x^{1}+3a_{3}x^{2}+4a_{4}x^{3}+\dots-a_{0}x^{0}-a_{1}x^{1}-a_{2}x^{2}-a_{3}x^{3}-\dots = x^{2}$$
By equating powers of $x$, I find the following relations
$$a_{1}-a_{0}=0
\\
2a_{2}-a_{1}=0
\\
3a_{3}-a_{2}=1
\\
4a_{4}-a_{3}=0
\\
\vdots
\\
na_{n}-a_{n-1}=0
$$
So, I write the coefficients as
$$
a_{1}=a_{0}\\
a_{2}=\frac{a_{1}}{2}=\frac{a_{0}}{2}\\
a_{3}=\frac{1}{3}+\frac{a_{2}}{3}=\frac{a_{0}}{6}+\frac{1}{3}\\
a_{4}=\frac{a_{0}}{24}+\frac{1}{12}\\
\vdots\\
a_{n}=\frac{a_{n}}{n!}+\frac{2}{n!}
$$
Combining these to form $y$, I get
$$y=\Sigma_{n=0}^{\infty}\frac{a_{0}}{n!}x^{n}+\Sigma_{n=3}^{\infty}\frac{2}{n!}x^{n}
$$
The first bit gives me $a_{0}e^{x}$ as expected, but I don't see how to extract $-x-2x-2$ from the second half. Is there something wrong in my approach that lead to an incorrect answer, or am I missing something in the manipulation of power series?","['ordinary-differential-equations', 'power-series']"
2797630,"Solving $\frac{\sqrt{r+1}-\sqrt{r-1}}{\sqrt{r+1}+\sqrt{r-1}}=\log_2\left(|x-2|+|x+2|\right)-\frac{11}{9}$, where $r=\frac{1+x^2}{2x}$","The problems in my sophomore workbook are marked with three colors that signify how hard is the marked problem: green for D and C, yellow for B and A and red for advanced students. All problems are also ordered from the easiest to the hardest. This problem is marked red and appears last in the section of logarithmic problems, which means that it's the hardest logarithmic problem in the whole workbook. I know what its solution is (there are solutions at the end of the workbook), but I'd like to see how do we arrive at it. I wrote the solution here, but it's hidden until it's hovered over. I don't want to spoil fun to those who want to find it by themselves. Find the value(s) of $x$ when the left-hand side is equal to the right-hand side. $$\frac{\sqrt{\dfrac{1+x^2}{2x}+1}\;-\;\sqrt{\dfrac{1+x^2}{2x}-1}}{\sqrt{\dfrac{1+x^2}{2x}+1}\;+\;\sqrt{\dfrac{1+x^2}{2x}-1}}=\log_2(|x-2|+|x+2|)-\frac{11}{9}$$ The solution is $$x_1=\frac{7}{9},x_2=\frac{9}{7}$$","['algebra-precalculus', 'logarithms', 'functions']"
2797634,Evaluating correctness of various definitions of countable sets,"I was trying to understand the definition of countable set (again!!!). Wikipedia has a very great explanation : A set $S$ is countable if there exists an $\color{red}{\text{injective}}$ function $f$ from $S$ to the natural numbers $\mathbb N$. If such an $f$ can be found that is also $\color{red}{\text{surjective}}$ (and therefore bijective), then $S$ is called countably infinite . In other words, a set is countably infinite if it has $\color{red}{\text{bijection}}$ with the $\mathbb N$. So I summarize: $S$ is countable iff $S\xrightarrow{injection}\mathbb N$ $S$ is countably infinite iff $S\xrightarrow{bijection}\mathbb N$ But then wikipedia confuses by stating following points: Theorem: Let $S$ be a set. The following statements are equivalent: $S$ is countable, i.e. there exists an injective function $f : S → \mathbb N$. Either $S$ is empty or there exists a surjective function $g : \mathbb N → S$. Either $S$ is finite or there exists a bijection $h : \mathbb N → S$. Q1. I feel 2nd statement is wrong, as it allows some element in $S$ to not to map to any element in $\mathbb N$. That is $\mathbb N \xrightarrow{surjection} S$ does not imply $S\xrightarrow{injection}\mathbb N$. Hence $S$ is not countable. Right? Q2. 3rd  statement defines countably infinite set, so its countable also. Right? Q3. Also I dont get if the extra restrictions of emptyness and finiteness in statements 2 and 3 are required. Wikipedia further says: Corollary: Let $S$ and $T$ be sets. If the function $f : S → T$ is injective and $T$ is countable then $S$ is countable. If the function $g : S → T$ is surjective and $S$ is countable then $T$ is countable. Q4. Here, too, I feel 2nd statement is incorrect for the same reason as 2nd statement in the theorem. Right? Edit I dont know if its correct to add this edit. But its the source of my confusion. So adding it anyway. All answers on this post go on explaining how sujectivity and injectivity imply each other and hence bijectivity. But does that means, whenever injective $f:X\rightarrow Y$ exists, there also  holds surjective $g:Y\rightarrow X$ (and also a bijective)? I dont feel so, as the wikipedia gives examples of injective $f:X\rightarrow Y$, for which $g:Y\rightarrow X$ is not surjective: On the same page, it gives example of surjective $g:Y\rightarrow X$, for which $f:X\rightarrow Y$ is not injective: How can I reconcile these facts with given answers? I must be missing something very basic!!!",['elementary-set-theory']
2797636,Unbiased estimator for $\theta$,"Exercise : Let $X_1, \dots, X_n$ be a random sample $(n>1)$ from the distribution with pdf $f(x) = \theta x^{-2}, \; \; 0 < \theta \leq x < \infty$ , where $\theta$ an unknown parameter. Find the Maximum Likelihood Estimator $\hat{\theta}$ of $\theta$ and determine if it's an unbiased estimator for the parameter $\theta$ . Attempt : The likelihood function is : $$L(x;\theta) = \prod_{i=1}^n \theta x^{-2} \mathbb{I}_{[\theta, + \infty)}(x_i) = \theta^n \mathbb{I}_{[\theta, + \infty)}(\min x_i)$$ and thus the MLE is : $\hat{\theta} = \min x_i$ . Now, in order to determine if it's an unbiased estimator for $\theta$ , I have to find : $$\mathbb{E} [\min x_i |\theta] $$ and determine whether it's equal to $\theta$ or not. How does one proceed with calculating $\mathbb{E} [\min x_i |\theta] $ ? Or is there another way of determining if $\hat{\theta}$ is an unbiased estimator for $\theta$ ?","['expected-value', 'probability-distributions', 'maximum-likelihood', 'statistics', 'probability']"
2797652,"What's the opposite of ""main diagonal""?","This matrix has only '1's on its main diagonal: $A = \begin{pmatrix}
  1 & 0 & 0 & 0\\
  0 & 1 & 0 & 0\\
  0 & 0 & 1 & 0\\
  0 & 0 & 0 & 1
\end{pmatrix}$ But whats the opposite of main diagonal? I want to say something like:
""There are only '0's on the NOT-main-diagonal"" Can someone help me with a word?",['matrices']
2797687,Independence and probability measures,"Let $X = (X_1, \ldots, X_n)$ be a random vector with i.i.d. components.
I am trying to understand the following expression from my probability textbook: $\displaystyle P(\mathbf{X} \in B)  = P(X_1 \in B_1, \, X_2 \in B_2, \ldots, X_n \in B_n) =$  $\displaystyle = \prod_{i=1}^n P(X_i \in B_i), \qquad \forall B = B_1 \times \ldots \times B_n \in \mathcal{B}(\mathbb{R}^n), \qquad \qquad \qquad \qquad \qquad \qquad \quad ~~~(1)$ where $B_i \in \mathcal{B}(\mathbb{R}), ~ \forall i$. I have two questions about this expression. I think that we should use the same symbol $P$ in all places of this expression because random vector $\mathbf{X}$ and random variable $X_i$ have the same domain probability space $(\Omega, \mathcal{F}, P)$ (by definition of a random vector). 1) Does this imply that event $\{\omega: X_i(\omega) \in B_i\}$ is a shorthand notation for event $\{ \omega: X_1(\omega) \in \mathbb{R} , \ldots , X_{i-1}(\omega) \in \mathbb{R} , X_i(\omega) \in B_i , X_{i+1}(\omega) \in \mathbb{R}, \ldots, X_n(\omega) \in \mathbb{R} \}$ ? If no, I don't understand which measure $P$ is used in expression $\prod_{i=1}^n P(X_i \in B_i)$. 2) Random vector $\mathbf{X}$ induces probability space $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n), \mathcal{P}_{\mathbf{X}})$ and r.v. $X_i$ induces probability space $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathrm{P}_{X})$. How to rewrite expression (1) in terms of induced measures (i.e. joint distribution $\mathcal{P}_{\mathbf{X}}$ and marginal distribution $\mathrm{P}_X$)? I got the following result but I'm not sure: $$\mathcal{P}_{\mathbf{X}}(B) = \prod_{i=1}^n \mathrm{P}_X(B_i), \qquad \forall B = B_1 \times \ldots \times B_n \in \mathcal{B}(\mathbb{R}^n)$$","['probability-theory', 'measure-theory']"
2797768,Cubic root epsilon delta proof,I'm reviewing this epsilon delta proof for the continuity of the cubic root: but I can't see why is so evident that $\sqrt[3]{x^2}+\sqrt[3]{xc}+\sqrt[3]{c^2}\ge \sqrt[3]{c^2}$. Any help please? Thanks in advance.,"['continuity', 'radicals', 'real-analysis', 'epsilon-delta']"
2797777,Solving the system $5(\sin x + \sin y) = 1$ and $5(\sin 2x + \sin 2y) = 1$,"To find the general solution $(x,y)$ satisfying the system of equations
\begin{align} 5(\sin x + \sin y) &= 1 \\ 5(\sin 2x + \sin 2y) &= 1
\end{align}
I applied $\sin C + \sin D$ rule and then divided these two equations, then I am stuck at
$$\cos\frac{x-y}{2} = 2\cos \frac{x+y}{2}\cos(x-y).$$ I do not know what to do further.","['algebra-precalculus', 'trigonometry']"
2797842,If $H$ has finite index there are finitely many distinct subgroups of form $aHa^{-1}$,"If $H$ has finite index there are finitely many distinct subgroups of form $aHa^{-1}$ . I tried the following: Let the distinct left cosets of $H$ in $G$ be $a_1H ,a_2H, \dots, a_nH$ . Then the distinct right cosets are $Ha_1^{-1}, Ha_2^{-1}, \dots, Ha_n^{-1}$ .
Consider any subgroup $aHa^{-1}$ . Now $aH=a_iH$ for some $i=1,2,\dots, n$ . Then $Ha^{-1}=Ha_i^{-1}$ . So $aHa^{-1}=a_iHa_i^{-1}$ , proving these subgroups are finite in number. In my proof I have used the following argument: $$\begin{align}aH=bH &\iff a^{-1}b\in H \\
&\iff a^{-1}(b^{-1})^{-1}\in H\\
&\iff Ha^{-1}=Hb^{-1}
\end{align}$$ Is my proof okay? P.S. I understand that all of the subgroups $a_iHa_i^{-1}$ need not be unique. For instance, the subgroup $3\Bbb Z$ has 3 distinct left/right cosets in $\Bbb Z$ .
 However the only subgroup of the form $a+3\Bbb Z -a$ is $3\Bbb Z$ .","['group-theory', 'proof-verification']"
2797879,How to estimate position from noisy angles?,"First of all: I think this problem has been studied very well (I assume in the field of surveying or navigation), but I was unable to find any information as I do not know what it is called. EDIT: As @Rahul mentioned this is called resection problem . Wikipedia points to various methods but they usually require exact measurements and assume that you have $n=3$. Suppose we know the exact position of a number $n \geq 3$ points (in the example below $n=4$) $P_i$ in the plane. (We can assume that those points are in some non degenerate configuration.) Furthermore we are at some unknown location $P_0$ looking in an unknown direction $\hat x_1$. Now we can observe the angles $\alpha_i$ of how much the direction we see each point deviates from our direction $\hat x_1$. With these observations $\alpha_1,\ldots,\alpha_n$ we'd like to determine $P_0$ (as well as the direction $\hat x_1$, but that is trivial once we know $P_0$). The problem is that the angles $\alpha_i$ are affected by noise i.e. cannot be measured exactly. (We can assume that they have an e.g. normally distributed error, or whatever is convenient. ) Is there a way to ""robustly"" estimate $P_0$? Is there a well known name for this problem?","['numerical-methods', 'trigonometry', 'geometry']"
2797891,Can one distinguish finite groups by their maps from abelian groups?,"Given a finite group $G$, is $G$ determined by its category of maps from abelian groups? Specifically, we can form the category $G_A$ of ""abelian points"" of $G$ with objects pairs $(A,\phi)$, with $\phi:A\rightarrow G$, for $A$ abelian, with morphisms $\xi:(A,\phi)\rightarrow (B,\psi)$ given by morphisms of groups $\xi:B\rightarrow A$ such that $\phi \circ \xi = \psi$. Very similar questions have been asked before, but they focused on the maps from cyclic groups, and the (small) counterexamples given for the analagous question do not have isomorphic categories of abelian points. The following are the similar questions I refer to: Is a finite group uniquely determined by the orders of its elements? If I know the order of every element in a group, do I know the group?","['finite-groups', 'category-theory', 'abelian-groups', 'group-theory']"
2797898,understanding nonhomogeneous linear differential equation formula,"For the equation $y'=a(x)y+b(x)$ we can express the function y as $$y=C(x)\exp{
\int_{x_0}^x}{a(s)ds}$$
Which means that if we find the function $C(x)$ we'll have ourselves a solution. Now, my lecture notes say two things I don't understand. First, that finding such function $C(x)$ is actually the variable substitution, and second, that the final formula for $y$ is:
$$
y=e^{\int{a(x)dx}}(C+\int{e^{-\int{a(x)dx}}b(x)dx})
$$ What does it mean that finding $C(x)$ is the variable substitution and how is the final formula for $y$ derived? Thanks in advance!",['ordinary-differential-equations']
2797930,"Can I check, whether $n$ with $\frac{n}{\phi(n)}=r$ exists?","Suppose, a rational number $r$ is given. How can I check whether there is a positive integer $n$ with $$\frac{n}{\phi(n)}=r$$ where $\phi(n)$ denotes the totient function ? In particular, I need the answer for $r=\frac{(s+2)\cdot s}{s+1}$ with positive integer $s$ to find all solutions of $$\frac{1}{x}-\frac{1}{y}=\frac{1}{\phi(x+y)}$$ with $\phi(x+y)|y$. In this case the even $s\ge 2$ can be ruled out because we have $$\frac{n}{\phi(n)}=\frac{p_1}{p_1-1}\cdots \frac{p_n}{p_n-1}$$ where $p_1,\cdots ,p_n$ are the prime divisors of $n$ ($n=1$ gives $\frac{n}{\phi(n)}=1$) because the numerator cannot be divisible by $4$. I conjecture that $s=1$ and $s=3$ are the only cases with a solution.","['number-theory', 'totient-function', 'elementary-number-theory']"
2797935,Do both versions (invariant and primary) of the Fundamental Theorem for Finitely Generated Abelian Groups hold at the same time?,"So there are the two versions of the Fundamental Theorem for Finitely Generated Abelian Groups (FTFGAG). I take the following from A First Course in Abstract Algebra by Fraleigh. The first is as follows: FTFGAG 1: Every finitely generated abelian group $G$ is isomorphic to a direct product of cyclic groups in the form
$$ G \cong \mathbb{Z}_{p_1^{r_1}} \times \cdots \times \mathbb{Z}_{p_n^{r_n}} \times \mathbb{Z} \times \cdots \times \mathbb{Z}$$
where $p_i$ are primes (not necessarily distinct) and $r_i$ are positive integers. But then we also have the second version: FTFGAG 2: Every finitely generated abelian group $G$ is isomorphic to a direct product of cyclic groups in the form
$$ G \cong \mathbb{Z}_{m_1} \times \cdots \times \mathbb{Z}_{m_r} \times \mathbb{Z} \times \cdots \times \mathbb{Z}$$
where $m_1 | m_2 | \cdots | m_r$. My question is whether these two hold at all times? So say for $\mathbb{Z}_{20}$, do we have $\mathbb{Z}_{20} \cong \mathbb{Z}_2 \times \mathbb{Z}_2 \times \mathbb{Z}_5 \cong \mathbb{Z}_4 \times \mathbb{Z}_5 \cong \mathbb{Z}_2 \times \mathbb{Z}_{10}$, even though $10$ is not a power of a prime (though of course $2|10$)? I've also seen statements of the Chinese Remainder Theorem (CRT) which say that $\mathbb{Z}_{nm} \cong \mathbb{Z}_n \times \mathbb{Z}_m$ if and only if $\gcd(n,m)=1$. Does this not contradict $\mathbb{Z}_{20} \cong \mathbb{Z}_2 \times \mathbb{Z}_{10}$ from above? Or is treating $\mathbb{Z}_n$ and $\mathbb{Z}_m$ as rings in the CRT what makes the situation different? I guess what I'm asking in a nutshell is: why don't the primary and invariant forms of the structure theorems for abelian groups (and also modules over PIDs, etc) contradict each other? Many thanks for any answers. Edit: So it seems that I didn't see this question which basically answers mine. CRT gives us a way of decomposition but FTFGAG only tells us that some decomposition is always possible. So for FTFGAG 1, $\mathbb{Z}_4 \times \mathbb{Z}_5$ suffices, for FTFGAG 2, $\mathbb{Z}_{20}$ suffices, and for the CRT $\mathbb{Z}_{20} \cong \mathbb{Z}_4 \times \mathbb{Z}_5$ works.","['modules', 'abstract-algebra', 'ring-theory', 'abelian-groups', 'group-theory']"
2797943,Degree of functions on a curve that are no zero divisors,"Let $R$ be a noetherian commutative ring of dimension one which is reduced. Moreover, $R$ is free of rank $n$ over the polynomial ring $k[x]$ where $k$ is a field. The background: $R = \mathcal{O}_X(U)$ is the affine coordinate ring where $U \subseteq X$ is an affine everywhere dense open subset of $X$, a reduced projective curve over $k$ with finite surjective morphism onto $\mathbb{P}^1_k$. Fix a basis $y_1,\ldots,y_n$ of $R$ over $k[x]$. For any $y \in R$ let $\deg(y)$ be the maximal degree of its coefficients regarding the fixed basis of $R$. Let $P_1,\ldots,P_r$ be the minimal primes of $R$ such that $\cup_i P_i \neq R$. Thus there are elements of $R$ not being zero divisors. What can we say about the upper bound on the minimum of the degrees of regular elements of $R$? What is $$\min_{y \in R}\ \{ \deg(y) \mid y \notin \cup_i P_i \}?$$ I am looking for some bounds in terms of $n$ and $g$, where $g$ is the arithmetic genus of $X$. Bounds in terms of the genera of the irreducible components belonging to the $P_i$ are also welcome. Does anyone have some helpful references or ideas how to approach this kind of problem? Remark: Originally, the question arised in terms of elements of fractional ideals $I$ of $R$, which are also free modules over $k[x]$ and do not only consist of zero divisors. Hence the question above is the case $I = R$.","['ring-theory', 'algebraic-geometry', 'commutative-algebra']"
2797956,Show that a $k$-form $\omega$ is smooth if only if it is smooth as map $\omega : M\rightarrow \Lambda ^k(M)$,Let $M$ be a smooth manifold. Consider $\Lambda ^k(M)=\bigcup_{p\in M}\Lambda ^k(T_{p}M)$ with the natural smooth structure. With this structure I showed that the $\pi :\Lambda ^k(M)\rightarrow M$ projection is smooth. Show that a $k$-form $\omega$ is smooth if only if it is smooth as map $\omega : M\rightarrow \Lambda ^k(M)$.,"['differential-forms', 'smooth-manifolds', 'differential-geometry']"
2797981,Counting zeros of parametrical functions.,"Today I was trying to do this exercise: Find for which $\lambda \in \mathbb{R}$ $$x+x^2=\arctan(\lambda x+x^2)$$ has exactly one solution. My attempt : Let's define $f(x)=x+x^2-\arctan(\lambda x + x^2)$. Finding solutions of that equation is equivalent to find zeros of this function. We note that $f(x)$ is bounded below by some constant $-M$ as its limits are both $+\infty$ as $x \to \pm \infty$ and it is a continuous function. Let's try to study its derivative:
$$f'(x)=\frac{2x^5+(4\lambda+1)x^4+(2\lambda+2\lambda^2)x^3+\lambda^2x^2+1-\lambda}{1+(\lambda x+x^2)^2}$$ so clearly its sign depends only on the numerator which is a polynomial of degree $5$ in $x$. Before going on we should notice that there are mainly 4 cases to consider: $$\lambda>1, \quad \lambda=1, \quad \lambda=0, \quad \lambda < 1 \wedge \lambda \neq 0;$$ The problem is that I don't really know how to study the sign of the derivative as it's difficult to study the numerator sign.
I tried to avoid studying the sign of the numerator by noting that if we approach the problem in another way, comparing the two functions $g(x)=x+x^2$ and $h(x)=\arctan(\lambda x+x^2)$ we notice that $h(x)$ has only one global minimum in $x= -\frac{\lambda}{2}$ and $g(0)=h(0)=0$ so clearly we should pay attention (expecially in the case $\lambda=1$) on which function is bigger than the other one. Now I'm stuck and I don't know how to proceed anymore. Any hint or help is really appreciated.","['real-analysis', 'calculus', 'functions']"
2798024,"Proving $yxy$ and $xyxyx$ generate a free subgroup of $\langle x, y \mid x^2, y^3 \rangle$.","I am trying to prove that the subgroup of $\langle x, y \mid x^2, y^3 \rangle$ generated by $yxy$ and $xyxyx$ is free. I know that I must show every reduced word can by described uniquely by these generators, but I still lack a general approach for this type of exercise. Even the brute force approach of comparing $w = w'$ and showing that the generators used must be the same does not work for me, but I also wonder if there is some more general approach using geometric group theory for these problems?","['geometric-group-theory', 'group-theory', 'free-groups']"
2798034,Description of $PSL_n$-torsors,"It is a classical result that $PGL_n$-torsors over a field $K$ correspond to central simple algebras of degree $n$ over $K$. I was wondering if there is a similar description for $PSL_n$-torsors along similar lines. What about other projectivizations of classical groups? A reference would be useful, unfortunately I could not find anything on my own.","['reference-request', 'algebraic-geometry']"
2798070,Connected 1-manifolds,"I am reading the beginning of the first volume on the introduction to Differential Geometry by Spivak. In the first part he defines manifolds and states that 'the only connected 1-manifolds are the line $\mathbb{R}$ and the circle $\mathbb{S}^1$'. (1) Is that ' only ' a direct consequence of the definition of manifold? I do not really see the answer. It is also true that, at this point of the book, Spivak uses results from Algebraic topology/Homology (without proving them) to give a wider outlook on the world of manifolds. So it might well be that the answer is not trivial. If it actually is, I would like to see why. (2) Does this fact imply that the only surfaces of revolution in $\mathbb{R}^3$ which can be obtained by connected 1-manifolds are cylinders and 2-tori?","['manifolds', 'general-topology', 'differential-geometry']"
2798076,"Let $\varphi(x) = \frac{1 - e^{-ax}}{1 + e^{-ax}}$, proof that $\varphi'(x) = \frac{a}{2}(1-\varphi^2(x))$","I am trying to find the required steps to reach that derivative, but I am not finding the right way for that. My current development has the following steps: $\varphi(x) = \dfrac{1 - e^{-ax}}{1 + e^{-ax}}$, then $\varphi'(x) = \dfrac{(0-(-ae^{-ax}))(1 + e^{-ax})-(1-e^{-ax})(0-ae^{-ax})}{(1+e^{-ax})^2}$ $\varphi'(x) = \dfrac{ae^{-ax}(1 + e^{-ax})+(1-e^{-ax})ae^{-ax}}{(1+e^{-ax})^2}$ $\varphi'(x) = \dfrac{2ae^{-ax}}{(1+e^{-ax})^2}$. Now, I am trying to find some way assuming that $$\dfrac{1 - e^{-ax}}{1 + e^{-ax}} = \tanh\left(\frac{ax}{2}\right)$$ but without success yet.","['derivatives', 'ordinary-differential-equations', 'calculus']"
2798147,Category of vector spaces and matrices,"I was brushing up on linear algebra and the following came to the mind. Consider a category whose nodes are n-dimensional vector spaces (n>0). Morphisms are matrices (transformations between vector spaces). This construction seems to be forming a category: let identities to be identity matrices. let composition to be composition of matrices. Composition is associative and identity rules of category are satisfied, so we have a category. Questions: Any reference studied this category? (any name associated with this category at all in the literature?) Is matrix addition associated with any categorical construction in this category? What do product, co-product, terminal, and initial objects mean in this category?","['matrices', 'category-theory', 'linear-algebra']"
2798162,Tricky Bendixson-Dulac question,"The Question: Consider the system $$\frac{dx}{dt}=x(2-x)-y \qquad \frac{dy}{dt} = y-kx$$ where $k>2$ is a constant. Show that there are no non-trivial closed trajectories in the quadrant $x,y≥0$ using the Bendixson-Dulac Theorem. Bendixson-Dulac Theorem: For the system $$\frac{dx}{dt} = X(x,y) \qquad \frac{dy}{dt} = Y(x,y)$$ where $X,Y \in C^1$, if there exists $\phi = \phi(x,y) \in C^1$ such that $$\frac{\partial}{\partial x} \big(\phi X \big)+\frac{\partial}{\partial y} \big (\phi Y \big) >0$$ everywhere in a simply connected region $R$, then the system has no non-trivial closed trajectories lying entirely in $R$. My Attempt: So I considered a general (continuously differentiable) function $\phi = \phi (x,y)$. Then \begin{align}
& \frac{\partial}{\partial x} \big(\phi X \big)+\frac{\partial}{\partial y} \big (\phi Y \big) \\
= & (3\phi) + \bigg(-2\phi +2\frac{\partial \phi}{\partial x}-k\frac{\partial \phi}{\partial y} \bigg) x + \bigg(-\frac{\partial \phi}{\partial x}+\frac{\partial \phi}{\partial y} \bigg) y + \bigg(-\frac{\partial \phi}{\partial x} \bigg)x^2
\end{align} Suppose we insist that each of the four functions in brackets must be everywhere positive in the quadrant $x,y≥0$. Then $\phi>0$ from the first term. $\dfrac{\partial \phi}{\partial x}<0$ from the last term. $\dfrac{\partial \phi}{\partial y}>0$ from the third term. But this would imply that everything in the second bracket is negative, so this does not work. And I am stuck. Any hints? Why does it even matter that $k>2$?","['stability-in-odes', 'ordinary-differential-equations']"
2798170,Rademacher theorem for 2nd order derivative,"The (simplest form of the) Rademacher theorem reads as follows: Any Lipschitz continuous function $f: \mathbb{R} \to \mathbb{R}$ is Lebesgue-almost everywhere differentiable. In other words: If the finite difference $\Delta_h^1[f](x) := f(x+h)-f(x)$ satisfies $$|\Delta_h^1[f](x)| \leq C |h|, \qquad x,h \in \mathbb{R} \tag{1}$$ for some absolute constant $C>0$, then $f$ is almost everywhere differentiable. Question: Is there a generalization for derivatives of second order? More precisely, if we replace the finite difference $\Delta_h^1$ by a second-order difference, for instance $$\Delta_h^2[f](x) := f(x+h)-2f(x)+f(x-h), \tag{2}$$ then does $$|\Delta_h^1[f](x)| \leq C_1|h| \qquad \quad |\Delta_h^2[f](x)| \leq C_2 |h|^2, \qquad x,h \in \mathbb{R} \tag{3}$$ imply that $f$ is almost everywhere twice differentiable? If not, then what additional information on the regularity gives the estimate $(3)$ compared to the almost everyhwere differentiability which follows from the weaker assumption $(1)$? The obvious idea would be to try to apply the Rademacher theorem twice (first to $f$ and then to its derivative $f'$), but unfortunately the estimate $$|\Delta_h^1[f'](x)| \leq C |h|$$ will, in general, only hold for Lebesgue almost every $x,h$, and therefore it is not possible to apply the Rademacher theorem directly to $f'$.","['derivatives', 'real-analysis', 'lipschitz-functions']"
2798202,$\sum\limits_{n=1}^{\infty} \frac{\tan^{-1} n}{n}$ diverges.,"I've come up with what I think are two alternate, valid ways to show that the series $\sum\limits_{n=1}^{\infty} \frac{\tan^{-1}{n}}{n}$ diverges. Hopefully someone can let me know if these hold. (1) Direct Comparison Test: For $x$ greater than about $1.557$ or so (an approximation, based on plotting), $\frac{\tan^{-1}{x}}{x} \geq \frac{1}{x}$. So, taking $N = 1$, for $n > N$, we have $\frac{\tan^{-1}{x}}{x} \geq \frac{1}{x} \geq 0$, where the harmonic series diverges. Thus, $\sum\limits_{n=1}^{\infty} \frac{\tan^{-1}{n}}{n}$ also diverges by direct comparison. (2) Limit-Comparison Test: Again take our series of comparison to be the harmonic series. We get:
\begin{align*}
\lim\limits_{n \to \infty} \frac{\frac{\tan^{-1}{n}}{n}}{\frac{1}{n}} & = \lim\limits_{n \to \infty} \tan^{-1} n \\
& = \frac{\pi}{2}
\end{align*}
Since this ratio is a finite number $\neq 0$, we can conclude that either both series converge or both diverge. Since the harmonic series diverges, $\sum\limits_{n=1}^{\infty} \frac{\tan^{-1} n}{n}$ also diverges. How do these look? Thanks in advance.","['sequences-and-series', 'proof-verification']"
2798207,How many ways are there to place $2$ identical kings on an $8\times 8$ chessboard so that the kings are not in adjacent squares?,This problem needs also to be extended to $n*m$ chessboard. I tried to think like this: First I choose a place for the first king in $64$ ways. Then I have a  choice $64-5 = 59$ squares for the second king . But this solution is not right because this is not the case if I place the first king in the sidemost layers of squares. Then I have $64-4 = 60$ squares for the other king. How can I solve this problem?,"['combinatorics', 'discrete-mathematics']"
2798231,"Generating Function for $(4,9,16,25,36,.....)$","I have a sequence $(4,9,16,25,36,...)$ it is being generated by $a_n=(n+1)^2)$ I have found the generating function for $n^2$ here: Proving the generating function of $n^2$ . I know I can shift a sequence:
$(1,4,9,16,25,....)$ to the right $(0,1,4,9,25,...)$ via $x^1*A(x)$ with $ A(x)$ a generating function. Does this also work with a left shift? Is there a definition? $n$ was a natural numbers. Maybe something else?","['generating-functions', 'combinatorics', 'discrete-mathematics']"
2798234,continuity of a multivariable function2,"I'm studying the continuity of the function
$$ f(x,y) = \left\{ 
   \begin{array}{l l}
     \frac{x^2y^2}{x^2+y^2} & \quad , \quad(x,y)\neq(0,0)\\
     0 & \quad , \quad(x,y)=(0,0)
   \end{array} \right.$$ in the point $(x,y)=(0,0)$. It's clear to me that if a function is not continuous I have to find a case of discontinuity, but perhaps it's more difficult to prove the continuity in which I can't find a fault in the behaviour of the function. In this case how can I show the limit
$ \lim_{(x,y)\rightarrow (0,0)}  f(x,y) = \lim_{(x,y)\rightarrow (0,0)} 
     \frac{x^2y^2}{x^2+y^2} =0$? It's simple to prove that on a line $y=mx$ through the origin $(0,0)$ but in the most general way?",['multivariable-calculus']
2798269,A non-zero continuous function such that summing over equally spaced values always gives zero,"A long time ago now, I wondered whether or not there exists some sequence of real numbers $(a_n)_{n \in \mathbb{N}}$ , different from the zero sequence, such that for any $m \in \mathbb{N}$ , $$
\sum_{n=1}^{\infty} \, a_{nm} = 0.
$$ To avoid any confusion: I really mean the product $nm$ in the subscript, it does not indicate an array of numbers. A friend of mine found a solution to this problem: $a_n = \mu(n)/n$ , where $\mu$ denotes the Möbius function, should work. Now, this makes it easy to construct a function $f : \mathbb{R} \to \mathbb{R}$ , different from the zero function, with the property that $$
\sum_{n=1}^{\infty} f(\alpha n) = 0,
$$ for all $\alpha \in \mathbb{R}$ , but it would of course by no means be continuous. Indeed, we may set $f(n) = a_n$ for all $n \in \mathbb{N}$ and extend by zero everywhere else. Therefore my question is: does such a continuous function exist?","['continuity', 'discrete-mathematics', 'elementary-number-theory', 'limits']"
2798281,F - measure in Clustering,"We can define the F - measure as follows: $$F_\alpha=\frac{1}{\alpha \frac{1}{P}+(1-\alpha)\frac{1}{R}} $$ Now we might be interested in choosing a good $\alpha$. In the article The truth of the F-measure the author states that one can choose the conditions: $$\beta=\frac R P, \text{ where } \frac{\partial F_{\alpha}}{\partial P} = \frac{\partial F_\alpha}{\partial R}$$ and then we obtain $\alpha=1/(\beta^2+1)$ and $$F_\beta=\frac{(1+\beta^2)PR}{\beta^2 P+R} $$ It is said that The motivation behind this condition is that at the point where the gradients of $E$ w.r.t. $P$ and $R$ are equal, the ratio of $R$ against $P$ should be a desired ratio $\beta$. I understand that the condition will guarantee that the user is willing to trade an increment in precision for an equal loss in recall. But I do not get why the equality of both partial derivatives correspond to these hypothesis. I would rather understand when one partial derivative equals the other partial derivative multiplied by minus one. Could anyone explain me why the desired condition (condition in words) correspond to this equality (condition in math terms)? EDIT: Well, we could do the following: $$\partial F=\frac{\partial F_{\alpha}}{\partial P}\partial P+\frac{\partial F_\alpha}{\partial R}\partial R.$$ And since we want for $\partial P=-\partial R$ that $\partial F=0$, we obtain easily the condition. But I have one problem with this: Since $\left. \frac{\partial F_\alpha}{\partial P} \right/ \frac{\partial F_\alpha}{\partial R}=1$, and the fact that the gradient is perpendicular to each level curve ( https://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/2.-partial-derivatives/part-b-chain-rule-gradient-and-directional-derivatives/session-36-proof/MIT18_02SC_pb_32_comb.pdf ) we would have that the level curve must have $m=-1$. Nonetheless, when I calculate the level curve for some constant $c$ I get the result, $$R(P)=\frac{c(1-\alpha ) P}{P-c\alpha},$$ which clearly is not a linear function with $m=-1$. What am I missing?","['machine-learning', 'statistics', 'clustering']"
2798283,Solving the First-Order Homogeneous ODE $y'-\frac{1}{3x}y=\frac{x}{3y}$,"I wish to solve 
$$y'-\frac{1}{3x}y=\frac{x}{3y}$$
At first, rearrange and simplify to 
$$y'=\frac{x^2+y^2}{3xy}$$
which clearly indicates that it is a homogeneous differential equation. Apply the substitution $y=ux\rightarrow y'=u'x+u$ which gives
$$u'x=\frac{1-2u^2}{3u}$$
and by integrating we get that
$$-\frac{3}{4}\ln\left| 2u^2-1\right| = \ln \left| x\right|+c$$
$$e^{-3/4\ln\left| 2u^2-1\right|} = e^{\ln \left| x\right|+c}$$ 
$$\left| 2u^2-1\right|^{-3/4}=C \left |x \right|$$
$$\left| 2u^2-1\right|=(C \left |x \right|)^{-4/3}$$
$$\left| 2u^2-1\right|=C' \left |x ^{-4/3}\right|$$
$$\left| 2u^2-1\right|=C' x ^{-4/3}$$
Now, if I choose $ 2u^2-1=C' x ^{-4/3}$ and solve for $u$ and then for $y$, I am getting the expected $y(x) = \pm\sqrt{x^2/2+C' x^{2/3}}$. But, if I choose $ 1-2u^2=C' x ^{-4/3}$, I am getting $y(x) = \pm\sqrt{x^2/2-C' x^{2/3}}$ which are not listed in the answers at least at Alpha. How to choose between the two? Note that $C'$ is clearly positive.","['homogeneous-equation', 'ordinary-differential-equations', 'absolute-value']"
2798284,Explain please the following statement from Naive Set Theory,"Here is a statement that I can't understand: Ordered triples, ordered quadruples, etc., may be defined as families whose
  index sets are unordered triples, quadruples, etc. For now, let's stick with the ordered triples. First of all I can't understand whether the index set is a set of unordered triples like $\{ \{a, b, c\}, \{d, e, f\}\}$ or it is itself an unordered triple? Also, I just can't imagine how having any of such sets as the index set can help me to build a set of ordered triples. Maybe some example will make things clear. I will be very grateful if you help. Thanks in advance.",['elementary-set-theory']
2798314,Combinatorial proof of $\sum_{k=0}^p {p \choose k}^2 {{n+2p-k}\choose {2p}} = {{n+p} \choose p}^2$,"(Valid for $n\geq p \geq 1$) Basically, I'd like to exhibit two sets, one with cardinality ${{n+p} \choose p}^2$, another one with cardinality $\sum_{k=0}^p {p \choose k}^2 {{n+2p-k}\choose {2p}}$ and find a bijection between the two. Defining $C_{k}^{n}$ as the subsets of  $\left \{ 1,...,n \right \}$ with cardinality k, it is clear that $(C_{p}^{n+p})^{2}$ has a cardinality of ${{n+p} \choose p}^2$, and that $\bigcup_{k=0}^{p}\left (  (C_{k}^{p})^2 \times C_{2p}^{n+2p-k}\right )$ has a cardinality of $\sum_{k=0}^p {p \choose k}^2 {{n+2p-k}\choose {2p}}$, but I'm unable to find a bijection between the two sets.","['combinatorics', 'summation', 'binomial-coefficients', 'combinatorial-proofs']"
2798330,Discussion of the curve $x^2+y^2+1$ over $\mathbb{R}$,"I am currently studying von Szamuelys ""Galois Groups and Fundamental Groups"" and in the chapter 4.3 he discusses the curve $x^2+y^2+1$ over $\mathbb{R}$ via the base change to $\mathbb{C}$ . Since I'm new to algebraic geometry I am having a hard time understanding a few of his statements. Before he discusses this example he defines a integral algebraic curve as the ringed space $(X=Spec(A),\mathcal{O}_X)$ where $A$ is an integral domain of an arbitrary field $k$ with trancendence degree 1. (In this case every prime ideal is maximal). And $\mathcal{O}_X$ is a sheaf of rings on $X$ (equipped with the Zarski Topology) via: $\begin{equation}
\mathcal{O}_X(U):=\bigcap_{P \in U}A_p
\end{equation}$ for any open U. The closed points of a curve are those maximal ideals that do not contain an ideal I. (I am not sure if this is a common definition, so I repeated it here) When discussing the curve $f=(x^2+y^2+1)$ he considers the $\mathbb{R}$ -curve $X=\operatorname{Spec}(\mathbb{R}[x,y]/(f))$ It is not clear to me how this works with the definition since $A$ was supposed to be an integral domain of a field, but even though $\mathbb{R}[x,y]/(f)$ is a integral domain, it is not part of $\mathbb{R}$ . Is my understanding of the definition wrong? We now conclud that the closed points of $X$ correspond to the maximal ideals of $\mathbb{R}[x,y]$ not containing $(f)$ .
Why? shouldent those be the open sets since we factor out f? For each such point we now have $\mathbb{R}[x,y]/M \simeq \mathbb{C} $ because $\mathbb{C}$ is the only nontrivial finite extension of R and X has no points over R. (this is clear) With the base change morphism $X_C \to X$ there are two closed points lying above each closed point of X, (yes) because $\mathbb{C} \otimes_{\mathbb{R}} \mathbb{C} \simeq \mathbb{C} \oplus \mathbb{C}.$ If we make Gal (C|R) act on the
tensor product via its action on the second term, then on the right-hand side the resulting action interchanges the components. (why would we even consider this?) Thanks for your answers already.","['algebraic-curves', 'algebraic-geometry']"
2798370,Area of Triangle inside a Circle in terms of angle and radius,"A circle $O$ is circumscribed around a triangle $ABC$, and its radius is $r$. The angles of the triangle are $\angle CAB = a, \angle ABC = b$ and $\angle ACB = c$. The area $\triangle ABC$ is expressed by $a, b, c$ and $r$ as: $\Large r^2 \over\Large2$$\Bigg(\sin(x)+\sin(y)+\sin(z)\Bigg)$ find $x, y$ and $z$: My approach: Firstly, to make it clear, I set $\overline {AB} = A$, $\overline {BC} = B$ and $\overline {CA} = C$. $\triangle ABC= \Large{Bh \over 2}$ where $h$ is the height $\triangle ABC = \Large{BA\sin(c) \over2}$ then, using the law of sine: $r= \Large{A\over 2 \sin(a)} = \Large{B\over 2 \sin(b)}$ $A = 2r\sin(a)$ $B = 2r\sin(b)$ replacing on the formula of area: $\triangle ABC = 2r^2\sin(a)\sin(b)\sin(c)$ But that doesn't help to answer the question. Is my approach correct, or else, what am I missing?","['circles', 'angle', 'trigonometry']"
2798390,Can definitions of Operators and Relations lead to Contradictions?,"Operators and relation definitions are important in mathematics but could some definitions be inconsistent, leading to a contradiction either syntax wise and/or semantically? If so, is there any examples of relations and operators leading to a contradiction as a result of of being improperly defined?","['logic', 'elementary-set-theory']"
2798400,"How to compute $(\mathbb{Z}\oplus\mathbb{Z}/d\mathbb{Z})/\langle(a,b+d\mathbb{Z})\rangle$?","The following problem astounded me, because I really thought it shouldn't be that hard, but somehow I can't wrap my head around it. Let $A=\mathbb{Z}\oplus\mathbb{Z}/d\mathbb{Z}$ for some $d\in\mathbb{N}$, as well as $(a,b+d\mathbb{Z})\in A$. Then what is $A/\langle(a,b+d\mathbb{Z})\rangle$? I tried to find necessary and sufficient conditions in terms of some modular equations for $(x,y+d\mathbb{Z})$ to be in $\langle(a,b+d\mathbb{Z})\rangle$, which of course would give the result by the first isomorphism theorem. One way of doing this would be to use the observation that $(x,y+d\mathbb{Z})\in\langle(a,b+d\mathbb{Z})\rangle$ if and only if $a|x$ and $ad|bx-ay$, which gives that $A/\langle(a,b+d\mathbb{Z})\rangle$ is isomorphic to the image of $A$ under $(x,y+d\mathbb{Z})\mapsto(x+a\mathbb{Z},bx-ay+ad\mathbb{Z})$, but then I'm stuck at calculating this image.","['abstract-algebra', 'quotient-group', 'abelian-groups', 'group-theory', 'elementary-number-theory']"
2798439,Variational Equation in Perturbation Theory Chapter of Arnold *Geometric Methods in ODE*,"In the preface to Chapter 4 ``Perturbation Theory"" in Arnold's book Geometric Methods in the Theory of Ordinary Differential Equations available for preview here https://www.springer.com/gp/book/9780387966496 he writes: ``If the size of the perturbation is characterized by a small parameter $\varepsilon$, then the effect of perturbations over time of order 1 leads to a change of order $\varepsilon$ of the solution.  This change can be calculated approximately by solving a variational equation along the unperturbed solution."" He then goes on to discuss the asymptotic methods necessary to discuss the validity of approximation at long times, but I am left wondering: Question: What is the ``variational equation along the unperturbed solution'' referred to here? I would guess that we are finding for finite time the next-order correction term in $\varepsilon$ to the leading order behavior (given by the solution of the unforced equation) but I am not sure about this and also not sure how this leads to a variational problem.  Any help would be greatly appreciated!!","['variational-analysis', 'ordinary-differential-equations', 'perturbation-theory']"
2798482,"Space of linear, continuous, hyperbolic functions is open, dense in the set of invertible functions","Let $(X,||\cdot||)$ be a Banach space on $\mathbb{C}$ and $\mathcal{L}(X)$ the set of linear, continuous functions from $X$ to itself. For $T\in\mathcal{L}(X)$, define the norm $||T||_{\mathcal{L}(X)}:=\sup_{v\neq 0}\frac{||T(v)||}{||v||}$ and the continuous spectrum $\sigma(T):=\{\lambda\in\mathbb{C}\mid A-\lambda I\,\text{ has no inverse}\}$. Define the sets:
  $$GL(X):=\{T\in\mathcal{L}(X)\mid T\text{ is invertible}\}$$
  $$\mathcal{H}(X):=\{T\in GL(X)\mid T\text{ is hyperbolic}\}$$
  (where hyperbolic means $|\lambda|\neq 1$ for all $\lambda\in\sigma(T)$) Prove that: (1) $GL(X)\subset\mathcal{L}(X)$ is open and (2) $\mathcal{H}(X)$ is open, dense in $GL(X)$. (1) Take $A\in GL(X)$ and the open ball $B_r(A)$, where $r:=1/||A^{-1}||$. If $B\in B_r(A)$, then $||I-BA^{-1}||\leq||A-B||\cdot||A^{-1}||<r||A^{-1}||=1$. The series $\sum_{n=0}^\infty(I-BA^{-1})^n$ is therefore convergent, so $BA^{-1}$ is invertible with $(BA^{-1})^{-1}=(I-(I-BA^{-1}))^{-1}=\sum_{n=0}^\infty(I-BA^{-1})^n$. Therefore, $B$ is invertible $\Rightarrow B_r(A)\subset GL(X)$. $_\blacksquare$ (2) We prove $GL(X)\setminus\mathcal{H}(X)$ is closed in $GL(X)$. If $\{A_n\}_{n\in\mathbb{N}}\subset GL(X)\setminus\mathcal{H}(X)$ a Cauchy sequence converging to $A\in GL(X)$, we need to prove $A\notin \mathcal{H}(X)$. By definition, for all $n$ there is $\lambda_n\in \mathbb{C}$ such that $|\lambda_n|=1$ and $A_n-\lambda_nI\notin GL(X)$. Since $\mathbb{S}^1:=\{\lambda\in\mathbb{C}\mid |\lambda|=1\}$ is compact, there is a convergent subsequence $\{\lambda_{i_n}\}_{n\in\mathbb{N}}$ converging to $\lambda\in\mathbb{S}^1$, so $\lim_{n\to\infty} A_{i_n}-\lambda_{i_n}I=$ $A-\lambda I$. Since $A_{i_n}-\lambda_{i_n} I\notin GL(X)$ (which is open), $A-\lambda I\notin GL(X)$, so $A\in GL(X)\setminus\mathcal{H}(X)$. To prove $\mathcal{H}(X)$ is dense in $GL(X)$, my idea is this: let $A\in GL(X)\setminus\mathcal{H}(X)$ and $\epsilon>0$ arbitrary, then there exist $z\in\mathbb{C}$ with $|z|<\epsilon$ such that $A+zI\in\mathcal{H}(X)$. I feel like this could work, but I don't know how to guarantee $\sigma(A+zI)\cap\mathbb{S}^1=\emptyset$.","['functional-analysis', 'banach-spaces', 'dynamical-systems', 'spectral-theory']"
2798550,Integral $\int_0^2 \frac{\arctan x}{x^2+2x+2}dx$,"I am tring to evaluate $$I=\int_0^2 \frac{\arctan x}{x^2+2x+2}dx$$ The first thing I did was to notice that $$\frac{1}{x^2+2x+2}=\frac{1}{(x+1)^2+1}=\frac{d}{dx}\arctan(x+1)$$ So I integrated by parts in order to get $$I=\arctan 2\arctan 3-\int_0^2\frac{\arctan(x+1)}{1+x^2}dx$$ I let $x=u+1$ but when I do that I get $$I=\arctan 2\arctan 3+\int_{-1}^1\frac{\arctan(u)}{1+(1+u)^2}du
   =\arctan 2\arctan 3$$ Now this is not close to the approximation given by wolfram. What have I done wrong and how to solve this?","['integration', 'definite-integrals']"
2798578,"Find sequential orthographic projections, linking three different manifolds of dimension $n=1,2,3$","Below is an image of the family of 2D curves for reference. The image is arbitrary. Still trying to formulate a concise question. I know that the geodesics for flat Euclidean Space are straight lines. But I want to take these curves and try to work backwards to determine the geometry of the manifold that results from these geodesic paths going across it. I can't really visualise the manifold because there's a lot going on. So how would I build up a representation of the manifold that matches up with these curves running across it? What would the steps be to calculate the metric of this space? Is there enough information given by the geodesics to provide a good representation of the manifold? Any other interesting questions you could ask about this graph? What topological information could I learn from these geodesics? Edit: If the projection of geodesics from a 2-manifold to the plane were realized below, the 2-manifold would look basically like a 2-sphere right? Does this give any insight as to what the 3-manifold could be? Edit 2/27/2020: Partially related: Geometry of transformed spacetimes? , Mapping modular flow to a subspace of $\Bbb R^2.$ Are the same knots produced? , Stability of a combination of two real analytic manifolds . My desmos code as a more complete version of the diagram below: https://www.desmos.com/calculator/hbiwqpyxzm . The reason that the posts are related is because the curves below are a symmetrical representations of lines of constant time in a Minkowski diagram mapped via the map $g.$ These curves are not geodesics in Minkowski space. They are hyperbolic curves mapped to the unit square. So hopefully more is clarified than last time I edited this post. Edit 3/4/2020: Further clarification: One could perform revolutions on each ""component,"" of the diagram (below), and then project the ""geodesic lines"" of some 3-manifold onto this 2-manifold to build up a representation of the 3-manifold. One would have to prove that the lines on the 3-manifold are indeed geodesics. I feel there needs to be a sort of transitive/sequential map that
connects the planar family of 2D curves to a 2-manifold (above it) and
a 3-manifold (above that) s.t. the orthographic projection from the
3-manifold yields the 2-manifold and likewise, the orthographic
projection of the 2-manifold yields the planar 2D curves. By a single ""component,"" I mean the space of functions $\ln(x)\ln(y)=s,$ for $\Re(s)>0. $ Note that it is necessary (for my purposes) to only use Euclidean transformations after performing the nonlinear map $g$ described below. First, $g$ acts to ""warp"" the geometry (needs proof), but gives a way to map this back to the foliation of hyperbolas using a simple change of coordinates. In essence I am, $1)$ warping the geometry and then $2)$ taking a union of these warped components, and next $3)$ configuring them systematically in $(0,1)^2.$ I wish to extend to the case of $(0,1)^3$ but I'm not sure how to do this and what invariants, structural conditions and other conditions I need. Once $(0,1)^3$ is complete, this question should find an answer. I think it will be helpful and important for others to understand the origin of this question, and the process I took. I first considered rectangular hyperbolas foliating the plane and then mapped the structure via the nonlinear map $g:=(x,y)\mapsto (e^x,e^y).$ Then I took the sub component in $(0,1)^2$ and performed reflections to situate $4$ components in $(0,1)^2.$ The reason for this is to build up a bounded representation of the original foliation of rectangular hyperbolas in $\Bbb R^2.$ (see image above). I would like to consider the boundary of the square to be compact in the topological sense and view the union of the components in $(0,1)^2$ as being bounded by a compact space.","['general-relativity', 'manifolds', 'general-topology', 'differential-geometry', 'manifolds-with-boundary']"
2798600,"Lorenz Attractor, its Geometric Model, and 14th Smale's problem.","I've found a post with a beautifully animated video that states the following: In 2001 mathematician Warwick Tucker proved that the paper model accurately describes the motion on the Lorenz attractor. For every trajectory on the attractor, there is a trajectory on the paper model that behaves exactly the same way ( illustration below: paper model on the left and trajectory on Lorenz Attractor on the right ). As a try to understand this fact, I've found Dr. Warwick's Ph.D. dissertation and his 2002 paper (which turned out to be a solution to 14th Smale's problem ). However, with my humble knowledge in this area, I am struggling to understand what part of these works implies the statement above? If you are familiar with this work, please explain it in simpler and intuitive terms on how one can imply the statement above. Or, please give me a hint on how to approach these works to get the above implication. Any help is appreciated. UPDATE: There is the following part of the 2002 paper that might lead to the answer of my question, although I don't know how: Problem Number 14 reads as follows:
  Is the dynamics of the ordinary differential equations of Lorenz that of the
  geometric Lorenz attractor of Williams, Guckenheimer, and Yorke? As an affirmative answer to Smale’s question, we are now ready to state the
  sole theorem of this paper: Main Theorem. For the classical parameter values, the Lorenz equations support a robust strange attractor $A$. Furthermore, the flow admits a unique SRB measure $\mu_\phi$ with $\text{supp}(\mu_\phi) = A$. How is this theorem answers Smale's question and implies the statement above about trajectories?","['chaos-theory', 'ordinary-differential-equations', 'dynamical-systems', 'geometry']"
2798709,double integral with singularity (not exactly singularity),"I'd like to solve $\iint_R \tan^{-1} \frac{y}{x}dA$, where $R=\{ (x,y) : x\ge0, y\ge 0, x^2+y^2 \le 4\}.$ I'm wondering if the following calculation is true:
$$
\begin{split}
\iint_R \tan^{-1} \frac{y}{x}dA
  &=\int_0^{\pi/2}\int_0^2
      \tan^{-1} \left(\frac{r \sin \theta}{r\cos\theta}\right)rdrd\theta\\
  &=\int_0^{\pi/2}\int_0^2 \tan^{-1}(\tan\theta)rdrd\theta\\
  &=\int_0^{\pi/2}\int_0^2\theta rdrd\theta\\
  &=\frac{\pi^2}{4}
\end{split}
$$ It might be true, but I don't think it is a rigid proof.
Apparently, $\tan^{-1} \frac{y}{x}$ is not defined for $x=0,$ but since $\lim_{x \to +0} \frac{y}{x} \to \infty$, we may extend or regard $\tan^{-1} \frac{y}{x}=\frac{\pi}{2}$ for $x=0.$ Please let me know if you have any comments for this problem. Thanks in advance!","['multivariable-calculus', 'singularity', 'calculus', 'singular-integrals']"
2798720,Random point on hyperspherical cap,"I have the cap of an n-sphere which is given by a n-dim vector for the center and by another vector which lies on the edge of the cap, both in Cartesian coordinates. The cap should be ""circular"" or rather an (n-1)-sphere, so these two vectors should be enough to fully describe it. Now I want to generate a random point which is uniformly distributed on that cap. I know that a random vector on the n-sphere can be created by drawing n numbers from a normal distribution and normalizing the resulting vector. (see http://mathworld.wolfram.com/SpherePointPicking.html (16)) And there are answers for the 3 dimensional case (see Generate a random direction within a cone ), but I am not sure how to extrapolate that to higher dimensions. Rejection sampling is not possible, since I am dealing with n around 1000, so I would have to reject LOTS of samples, especially if my cap is small.","['random', 'geometry']"
2798726,Limit of cumulative distribution functions is a cumulative distribution function,Le $X_n$ be a sequence of random variables with a cumulative distribution function $F_{X_n}$. Now of the sequence of function $F_{X_n}$ converge pointwise to a function $F$. Is $F$ itself a cumulative distribution function of some random variable?,"['density-function', 'probability-theory', 'probability-distributions', 'probability', 'random-variables']"
2798747,Doubt on grouping of terms in a series,"We know that if a series is convergent, then we can perform grouping on the series and the resulting series would still be convergent. However,for an arbitrary series, grouping may not always give the same result.
 E.g. $a_n=(-1)^{n+1}$  is the non-convergent series $1+(-1)+1+(-1)+...$ which can be grouped to $(1-1)+(1-1)+...$ which converges to 0. So my question is, in this link Establish convergence of the series: $1-\frac{1}{2}-\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}-...$ the solutions have grouped the terms in the series (before knowing whether the series converges or not) and showed it is convergent. How is that possible?",['sequences-and-series']
2798748,Proof that $\frac{9}{8} < \sum\limits_{n=1}^{\infty} \frac{1}{n^3} < \frac{5}{4}$.,"I'm trying to prove that $\frac{9}{8} < \sum\limits_{n=1}^{\infty} \frac{1}{n^3} < \frac{5}{4}$. I've seen similar proofs to this that tend to approach the proofs geometrically, using the upper and lower bounds of the remainder, the sum of the series less its $k$th partial sum, generated from the integral test. It usually involves some trick like ""excluding $k$ terms."" My professor tends to start at the second term to find a ""lower bound,"" from which the proof follows with algebraic manipulation, but seemed to suggest to me that this followed from trial and error. So, with that said, I really can't say that I understand the intuition behind finding or proving this. The first step seems to be plotting $y = \frac{1}{x^3}$ and then considering upper and lower Riemann sums (geometrically, ""boxes"" above the curve that will generate a sum greater than the area below it and boxes below the curve that will generate a sum less than the area below it). Though I believe I can draw the graph, I'm struggling with how to approach the problem, even if I have the abstract idea right, which I  quite doubt. I'd very much appreciate if someone could shed some light on this. Thanks in advance.",['sequences-and-series']
2798813,Nowhere zero holomorphic function with no logarithm,"I'm interested in nowhere zero holomorphic functions that are not the exponential of a holomorphic function, i.e. the space $\mathcal{O}^*(X)/\exp(\mathcal{O}(X))$, where $X$ is an affine curve. In particular are there any explicit examples of these kinds of functions when $X = \mathbb{C}^*$ or a punctured torus? I know that such functions must exist - from the short exact sequence 
$$0\to 2\pi i\mathbb{Z}\to\mathcal{O} \to \mathcal{O}^*\to 0$$
and the fact that $H^1(X,\mathcal{O}) = 0$ the connecting homomorphism $\mathcal{O}^*(X)\to H^1(X,\mathbb{Z})$ must be surjective - but I am not aware of an explicit construction. Edit : As pointed out in the comments this question is completely obvious when $X=\mathbb{C}^*$ as you can take $z\mapsto z^n$ to represent any class in $H^1(X,\mathbb{Z})$ (woops!). I'm still interested in the case of the punctured torus though.","['algebraic-curves', 'complex-analysis', 'riemann-surfaces', 'algebraic-geometry']"
2798819,"Prove that $\int\limits_{-\infty}^{\infty} \frac{e^{-x}}{1+e^{-2\pi x}}\,dx=\frac1{2\sin\left(\frac{1}{2}\right)}$","I need to prove that $\displaystyle\int_{-\infty}^{\infty} \dfrac{e^{-x}}{1+e^{-2\pi x}}\,dx=\dfrac{1}{2\sin\left(\frac{1}{2}\right)}$. This is an exercise from Basic Complex Analysis by Marsden and Hoffman. My attempt: First, Marsden says that we need to consider the complex function $f(z)=\dfrac{e^{-z}}{1+e^{-2\pi z}}$ and the next curve $\gamma_2=r+t\pi i$ with $t\in [0,1]$ $\gamma_3=(\pi i+r)-2tr$ with $t\in [0,1]$ $\gamma_4=(1-t)(\pi i-r)-tr$ with $t\in [0,1]$ Note that the only poles of the function $f(z)$ in the rectangle are when  $z=\dfrac{i}{2}$, $z=\dfrac{3i}{2}$ and $z=\dfrac{5i}{2}$. From a direct calculation we obtain that $\text{Res}\left(f(z),\dfrac{i}{2}\right)=\dfrac{e^{-i/2}}{2\pi}$, $\text{Res}\left(f(z),\dfrac{3i}{2}\right)=\dfrac{e^{-3i/2}}{2\pi}$ and $\text{Res}\left(f(z),\dfrac{5i}{2}\right)=\dfrac{e^{-5i/2}}{2\pi}$. After to a lot of calculations we obtain a bound for the integral over $\gamma_2$: $$\dfrac{e^{-r}}{\sqrt{(1-e^{-2\pi r})^2}}\geq \dfrac{|e^{-r-t\pi i}|}{|1+e^{-2\pi(r+t\pi i)}|}\geq 0$$Thus $$\displaystyle\int_{\gamma_2}^{}f(\gamma_2)\cdot d\gamma\leq \dfrac{e^{-r}}{\sqrt{(1-e^{-2\pi r})^2}}$$When $r\to\infty$ then $\displaystyle\int_{\gamma_2}^{}f(\gamma_2)\cdot d\gamma\to 0$ I think that is the same for $\gamma_4$ but I have troubles with $\gamma_3$. After a lot of calculations we obtain the next bound: $$\dfrac{e^{-r+2rt}}{\sqrt{1+2e^{-2\pi}\cos(-2\pi^2)+e^{-4\pi r}}}\geq \dfrac{|e^{-r+2rt-\pi i}|}{|1+e^{-2\pi(\pi i+r-2rt)}|}$$ but when $r\to\infty$ we obtain that $\dfrac{e^{-r+2rt}}{\sqrt{1+2e^{-2\pi}\cos(-2\pi^2)+e^{-4\pi r}}}\to\infty$ and I need, maybe, that this limits exists (maybe, zero). Clearly I want the integrals over the curves because if $\gamma=\gamma_1\cup\gamma_2\cup\gamma_3\cup\gamma_4$ then $\displaystyle\int_{\gamma} f(\gamma)\cdot d\gamma=\displaystyle\int_{\gamma_1} f(\gamma_1) \cdot d\gamma_1+\displaystyle\int_{\gamma_2} f(\gamma_2) \cdot d\gamma_2+\displaystyle\int_{\gamma_3} f(\gamma_3) \cdot d\gamma_3+\displaystyle\int_{\gamma_4} f(\gamma_4) \cdot d\gamma_4$ and I want to use the Residue Theorem. But, is the rectangle the correct curve? Here an screenshot of the exercise:","['residue-calculus', 'complex-integration', 'complex-analysis', 'improper-integrals', 'integration']"
2798847,Norm of difference in exponential of matrices,"I would like to prove that $\|e^A-e^B\| \leq \|A-B\|e^{max\{\|A\|,\|B\|\}}$, where $A,B \in \mathbb{R}^{n \times n}$. So far I was able to create the first difference term, but I have no idea how to incorporate the max norm.
I've read this post, where the Fréchet calculus was mentioned, but I'm still stuck. Any help would be appreciated.
Thank you in advance!","['matrices', 'normed-spaces', 'inequality', 'matrix-exponential']"
2798853,Probability of a $1000 \times 1000$ square matrix over $\mathbb{Z}_2$ having full rank,"There are only two entries, $0$ and $1$, over $\mathbb{Z}_2$. Thus, only $16$ possible $2\times2$ matrices over $\mathbb{Z}_2$, and $6$ of them have full rank: $$\begin{pmatrix}0&1\\  
1&0\end{pmatrix}  \quad
\begin{pmatrix}1&1\\  
1&0\end{pmatrix}  \quad
\begin{pmatrix}0&1\\  
1&0\end{pmatrix}  \quad
\begin{pmatrix}0&1\\  
1&1\end{pmatrix}  \quad
\begin{pmatrix}1&1\\  
0&1\end{pmatrix}  \quad
\begin{pmatrix}1&0\\  
0&1\end{pmatrix}$$ Randomly generate a $n \times n$ matrix over $\mathbb{Z}_2$ (where $n$ is big, say, $1000$). What's the probability that the matrix has full rank?","['matrices', 'matrix-rank']"
2798856,Operator norm of a self adjoint operator.,"Let $T$ be a self adjoint operator on $\Bbb C^n$. Let $\lambda_1,\lambda_2, \cdots ,\lambda_n$ be the eigen values of $T$. Then show that the operator norm of $T$ i.e. $\|T\| = \mathrm {max}_j\ |\lambda_j|$. I am trying to prove it in the following way. I have just proved that $\|T\|= \mathrm {sup} \left \{|\left < Tx,x \right > |\ :\ \|x\|=1 \right \}$. Also since $T$ is self adjoint it is normal. So by Spectral Theorem for Normal Operators we have a orthonormal basis $\{X_1,X_2, \cdots ,X_n \}$ for $\Bbb C^n$ each vector of which is an eigen vector of $T$. WLOG let us assume that $\lambda_j$ be an eigen value of $T$ corresponding to the eigen vector $X_j$, for $j=1,2, \cdots ,n$. Then $|\left < TX_j,X_j \right > | = |\left <\lambda_jX_j,X_j \right > | = |\lambda_j|$ (since $\left < X_j,X_j \right >=1$) for $j=1,2, \cdots ,n$. Since $\|X_j\|=1$ for $j=1,2, \cdots ,n$ so we have $\mathrm {max}_j\ |\lambda_j| \le \|T\|$  by the above proven result. But I find difficulty to prove the reverse inequality. For the reverse I started by taking a vector $X \in \Bbb C^n$ with $\|X\|=1$.  Then there exist scalars $c_1,c_2, \cdots ,c_n \in \Bbb C$ such that $X=c_1X_1+c_2X_2+ \cdots +c_nX_n$. Since $X_j$'s are orthonormal we have $\|X\|=\sqrt {|c_1|^2+|c_2|^2+ \cdots +|c_n|^2}$. So we have $|c_1|^2+|c_2|^2+ \cdots +|c_n|^2=1$. Again by orthonormality of $X_j$'s we have $\left <TX,X \right > = |c_1|^2\lambda_1+|c_2|^2\lambda_2+ \cdots +|c_n|^2\lambda_n \le (|c_1|^2+|c_2|^2+ \cdots + |c_n|^2)(\lambda_1+\lambda_2+ \cdots +\lambda_n)=\lambda_1+\lambda_2+\cdots +\lambda_n$. Hence by triangle inequality we have $|\left <TX,X \right >| \le |\lambda_1|+|\lambda_2|+ \cdots +|\lambda_n|$. Which clearly doesn't meet my purpose. So how do I proceed to prove the reverse inequality? Please help me in this regard. Thank you very much.","['eigenvalues-eigenvectors', 'spectral-theory', 'linear-algebra', 'operator-theory']"
2798861,Summing $\sum\frac{1}{i}$ with two constraints on $i$,"Let's fix a positive integer $n$ and consider two positive composites $a,b \le n$. We can consider the prime factorizations $a=2^{a_2}3^{a_3}\cdots p^{a_p}$ and $b=2^{b_2}3^{b_3}\cdots p^{b_p}$, where $p$ is the largest prime $\le n.$ I am trying to sum $\displaystyle\sum\frac{1}{i}$, over positive integers $i$ such that: $i$ consists of no prime factor larger than $n$ (but $i$ may be larger than $n$); that is, $i$ is of the form $i=2^{i_2}\cdots p^{i_p}$. The $i_k$'s must satisfy both of the following conditions:
$$(a_2-i_2)1_{\lbrace a_2-i_2>0\rbrace}+\cdots+(a_p-i_p)1_{\lbrace a_p-i_p>0\rbrace}\ge 2,\\(b_2-i_2)1_{\lbrace b_2-i_2>0\rbrace}+\cdots+(b_p-i_p)1_{\lbrace b_p-i_p>0\rbrace}\ge 2.$$
($1_{\lbrace>0\rbrace}$ is a characteristic function which is $1$ when the difference is positive, $0$ otherwise.) Is it possible to determine which $i$'s satisfy (i) and (ii)? If not, is there any chance of finding naive bounds for $\displaystyle\sum\frac{1}{i}$ over such $i$?","['combinatorics', 'summation', 'elementary-number-theory']"
2798882,Hypothesis testing - Critical region and confidence level,"Exercise : For the estimation of the unknown rate of votes $p$ that a political group $A$ will gather in the following elections, suppose we selected a random sample of $n=15$ voters. Suppose that you want to check the null hypothesis $H_0 : p = 0.5$ against the alternative $H_1 : p< 0.5$. Suppose that the critical region of this specific hypothesis test is $K=\{y\leq 2\}$, where $y$ is the observed number of voters who voted for the political group $A$. i) Calculate the confidence level $a$ of the above hypothesis test. ii) If the political group $A$ eventually gathers a rate of $30\%$ in the elections, calculate the probability of the type II error for the above hypothesis test. Question - Request : For our upcoming exams, we are supposed to be able to handle such problems but we weren't really introduced to any of them due to shortage of time. I kindly request some tips or a thorough explanation such as I will be able to get a grip on such problems. (This is a past exams exercise)","['probability-theory', 'probability', 'statistics', 'hypothesis-testing']"
2798918,"Show that arbitrary $A$ and $A^T$ have same eigenvalue, algebraic and geometric multiplicity","Show that an arbitrary $n \times n$ matrix $A$ and its transpose $A^T$
  have the same eigenvalues, algebraic multiplicity and geometric
  multiplicity. I'm not sure if I did it correctly and especially how to show that they have same geometric multiplicity? same eigen values Assume $A$ and $A^T$ have same eigenvalues, then they have the same chracteristic polynomial. So we need to show that $p_A(\lambda)=\det(A-\lambda I)$ is same as $p_{A^T}(\lambda)=\det(A^T-\lambda I)$. So we have $$p_{A^T}(\lambda)=\det(A^T-\lambda I) = \det(A^T-\lambda I^T) = \det\left((A-\lambda I)^T\right) = \det(A-\lambda I)=p_A(\lambda)$$ We see their characteristic polynomials are same so their eigenvalues are same as well. same algebraic multiplicity I'm not sure if this is a correct reason proof but: Because the characteristic polynomials are same, we have that the algebraic multiplicities of the eigenvalues of $A$ nd $A^T$ sre the same. same geometric multiplicity I don't know? :/","['eigenvalues-eigenvectors', 'matrices', 'transpose', 'proof-writing', 'linear-algebra']"
2798934,"Continuous $f:[0,1]\to\mathbb{R}$ is differentiable on $(0,1]$ and $\lim_{x\to 0^+} f'(x)$ is finite. Prove that f has right hand derivative at x=0 [duplicate]","This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 6 years ago . Suppose $f:[0,1]\to\mathbb{R}$ is differentiable on $(0,1]$  and $\lim_{x\to 0^+} f'(x)$ exists and it is finite. Prove that $f$ has a right hand derivative at $x=0$. What I know is that there exists some c such that 
$$\lim_{x\to 0^+}\left(\lim_{x\to y} \frac{f(x)-f(y)}{x-y}\right)=c$$ and $$\lim_{x\to a} f(x)=f(a)$$ I really don't see how I should start from here, any hints welcome.","['real-analysis', 'calculus', 'analysis']"
2798941,Limiting distribution of MLE for uniform distribution,"We consider iid random variables $X_1,X_2,\ldots,X_n\sim\mathcal{U}_{[0,\theta]}$ and are interested in the asymptotic behaviour of the corresponding MLE for $\theta$, i.e.
$$M_n:=\mathrm{max}_{i\in\{1,2,\ldots,n\}}X_i.$$
More precisely, I would like to confirm explicitly that $M_n$ converges (in some sense) to a reversed-Weibull-distributed rv in distribution and I suppose that I am just missing some computational detail/trick/idea... I thought I should try studying a standardised version of $M_n$ since that's what seems to be done in extreme value theory and more generally when studying limiting distributions. We know that
$$\mathbb{E}_{\theta}[M_n]=\frac{n}{n+1}\theta,~~\mathbb{Var}_\theta[M_n]=\frac{n}{(n+1)^2(n+2)}\theta^2,$$
so I consider the cdf of
$$Z_n=\frac{M_n-\mathbb{E}_\theta[M_n]}{\sqrt{\mathbb{Var}_\theta[M_n]}}~~\mathrm{or}~~Z_n'=\frac{M_n-\theta}{\sqrt{\mathbb{Var}_\theta[M_n]}},$$
which leads to
$$\mathbb{P}_\theta(Z_n'\leq x)=\left(1+\frac{\sqrt{n}}{(n+1)\sqrt{n+2}}x\right)^n$$
and
$$\mathbb{P}_\theta(Z_n\leq x)=\left(1-\frac{1}{n+1}\left(1-\frac{\sqrt{n}}{\sqrt{n+2}}x\right)\right)^n$$
for $x\leq 0$ and $x\leq \theta/(n+1)$ respectively.
However, I don't see how the limit with respect to $n$ takes the form
$$\exp\left(-\left(\frac{1}{1+\xi x}\right)^{1/\xi}\right)$$
for some $\xi<0$ here. (I know that $\lim_{n\rightarrow\infty}(1+x/n)^n=\exp(x)$) How can I resolve this? Is it maybe just a clever alternative for the denominator $\sqrt{\mathbb{Var}_\theta[M_n]}$?","['probability-limit-theorems', 'uniform-distribution', 'statistics', 'maximum-likelihood']"
