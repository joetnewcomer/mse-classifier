question_id,title,body,tags
4510571,Duality of an Ample invertible sheaf has no non-trivial global section?,"This is EXERCISE 7.1 of Chapter III in Hartshorne's AG: [Q] Let $X$ be an integral projective scheme of dimension $\geq1$ over a field $k$ , and
let $\mathscr{L}$ be an ample invertible sheaf on $X$ . Then $H^0(X,\mathscr{L}^{-1})=0$ . Actually if $s\in H^0(X,\mathscr{L}^{-1})\neq0$ , as $H^0(X,\mathscr{L}^{-1})=\hom(\mathscr{O}_X,\mathscr{L}^{-1})$ , $s$ can be seen as a non-trivial map $s:\mathscr{O}_X\to\mathscr{L}^{-1}$ . As $X$ integral, this map need be injective, so we get an inclusion $s:\mathscr{L}^n\to\mathscr{O}_X$ for all $n>0$ .
So induce an injection $\Gamma(X,\mathscr{L}^n)\to\Gamma(X,\mathscr{O}_X)$ ! Now as $\mathscr{L}$ be an ample invertible sheaf, $\dim_k(\Gamma(X,\mathscr{L}^n))$ should be very big and $\dim_k(\Gamma(X,\mathscr{O}_X))$ should be not so big. So we get the contradiction. (I) For $\Gamma(X,\mathscr{O}_X)$ : Here $k$ may not algebraic closed and $X$ may not geometrically reduced, so we can not get $\Gamma(X,\mathscr{O}_X)=k$ , in fact this can be very large! (see: https://stacks.math.columbia.edu/tag/0BUG ) We go through that proof, $t\in\Gamma(X,\mathscr{O}_X)=\hom(X,\mathbb{A}^1)$ . And we consider $X\to\mathbb{A}^1\to\mathbb{P}^1$ , use $X$ is proper, we can get that $X$ maps to a single closed point of $\mathbb{A}^1$ . So $\Gamma(X,\mathscr{O}_X)$ are all closed points in $\mathbb{A}^1$ ! (II) For $\Gamma(X,\mathscr{L}^n)$ : As $\mathscr{L}$ is ample, we may assume $\mathscr{L}^n$ generated by global sections and very ample which induce $i:X\to\mathbb{P}^N$ such that $\mathscr{L}^n=i^*\mathscr{O}(1)$ . But I don't know which can be used.",['algebraic-geometry']
4510635,multivariable function maximum,"$max\Bigl\{\sum_{i=1}^{n}x_{i}^{2}:\sum_{i=1}^{n}x_{i}=1,x_{i}>\lambda,i=1,...n\Bigl\}= \\
max\Bigl\{\sum_{i=1}^{n-1}x_{i}^{2}+(1-\sum_{i=1}^{n-1}x_{i})^{2} : \sum_{i=1}^{n-1}x_{i} \leq 1-\lambda,x_{i}\geq\lambda, i=1,...,n-1 \Bigl\}= \\
(n-1)\lambda^{2}+(1-(n-1)\lambda)^{2}$ Any idea how to solve this? I tried Lagrange multipliers in both braces. The first one returns critical point at 1/n while the second at $\frac{1-\lambda}{n-1}$ . edit. $(1-\frac{2}{n})\frac{1}{n-1}<\lambda<\frac{1}{n}$ $\sum_{i=1}^{n}x_{i}^{2}=\sum_{i=1}^{n-1}x_{i}^{2}+x_{n}^{2}=\sum_{i=1}^{n-1}x_{i}^{2}+(1-\sum_{i=1}^{n-1}x_{i})^{2}$",['multivariable-calculus']
4510637,Show the function $g(z)=\frac{1+z}{1-z}$ maps $\mathbb{D}$ onto $\{ z \in \mathbb{C} :\operatorname{Im}(z) >0\}$,"Show the function $g(z)=\frac{1+z}{1-z}$ maps $\mathbb{D}$ onto $\{ z \in \mathbb{C} :\operatorname{Im}(z) >0\}$ First, we know that $g$ has a pole at $z=1$ , which is on the boundary of the unit circle. So the circle is being mapped to a line. Now we need to figure out which line. Note that we have $g(1)=dne, g(-1)=0, g(i)=i, g(-i)=-i, g(0)=1$ . This is the part I'm confused about. Since we want to show that $g$ maps $\mathbb{D}$ onto $\{ z \in \mathbb{C} :\operatorname{Im}(z) >0\}$ , shouldn't $g(0)$ be somewhere in $\{ z \in \mathbb{C} :\operatorname{Im}(z) >0\}$ ? And shouldn't we have any $z$ with $|z|=1$ being mapped to the real-axis? Aside: I tried to find a function that maps $\mathbb{D}$ onto $\{ z \in \mathbb{C} :\operatorname{Im}(z) >0\}$ as an exercise on my own. I tried by using the cross-ratio method, and I got that $f(z)= \frac{(z-1)(i+1)}{(z+1)(i-1)}$ is such a function. Could someone please verify this? Thanks!","['complex-analysis', 'mobius-transformation']"
4510641,Trivial question on derivative of quadratic form of vector-valued function,"This seems like a trivial question but I am currently stuck and cannot see what I am doing wrong. So let us consider a function $f(x) : \mathbb{R}^d \rightarrow \mathbb{R}^d$ . I want to compute the derivative w.r.t. $x \in \mathbb{R}^d$ of an expression that contains a quadratic form of $f(x)$ $$I = f(x)^{\top} C f(x) . $$ Here $C$ is a $d\times d$ matrix. By taking the derivative w.r.t to the vector $x$ we have $$ \frac{\partial I}{\partial x} = 2C f(x) \cdot \nabla f(x),  $$ where $\nabla f(x)$ denotes the Jacobian of $f$ which will be a $d \times d$ matrix. Now  my problem is that the dimensions of the matrices in the last expression do not match:
We have $C: d\times d$ , $f(x): d\times 1$ , and $\nabla f(x): d \times d$ . So the last two dimensions do not add up.
What I am doing wrong? Is the correct derivative $$ \frac{\partial I}{\partial x} = \nabla f(x) 2 C f(x)  ,  $$ or $$ \frac{\partial I}{\partial x} = ( 2 C f(x) )^{\top} \cdot \nabla f(x)  $$","['matrices', 'matrix-calculus', 'linear-algebra', 'partial-derivative', 'derivatives']"
4510646,"If $g$ is continuous and $A$ is a closed set and $A\subset \mathbb{R}$, then $g^{-1}(A)$ is closed","Definition: Let $g$ be defined on all of $\mathbb{R}$ . If $A$ is a subset of $\mathbb{R}$ , define $$g^{-1}(A)=\{ x\in\mathbb{R}:~ g(x)\in A\}$$ True or false: If $g$ is continuous and $A$ is a closed set and $A\subset \mathbb{R}$ , then $g^{-1}(A)$ is closed. This problem is found from Understanding Analysis (Ex.4.4.12(d), by Abbott, Stephen). I found the answer from internet showing this statement is True. But can I use the following counter-example? $$\begin{align} g(x)&=3x-1,~x\in[1/3,~2/3]\\
\\
g(x)&=1, ~~~~~~~~~~x\in (2/3,1)\\
\\
g(x)&=0,~~~~~~~~~~x\in(0,1/3)
\end{align}$$ So $A=[0,1]$ is closed. But $g^{-1}(A)=(0,1)$ is not closed.","['analysis', 'real-analysis']"
4510653,Show that a complex function is bounded,"Let $\mathbb{D}= \{ z\in \mathbb{C}: |z|<1\}$ . For $t\in \mathbb{R}$ , let $f_t$ denote the holomorphic function on $\mathbb{D}$ defined by $f_t(z)= (\frac{1+z}{1-z})^{it}$ , $z\in \mathbb{D}$ with respect to the principal branch of the logarithm. Show that there is $C>0$ such that for all $t\in \mathbb{R}$ , we have $\sup_{z\in \mathbb{D}}|f_t(z)|<C^t$ I know that $\frac{1+z}{1-z}$ is a mobius transformation. Also, we have $f_t(z)= (\frac{1+z}{1-z})^{it} = \exp(it\frac{\log(1+z)}{\log(1-z)})) = \exp(it\log (1+z))\exp(-it\log (1-z))) = (1+z)^{it} (1-z)^{-it}$ . From here, I'm not sure how to proceed to bound such function. Thanks in advance!",['complex-analysis']
4510664,Suppose that $g$ is an even function and let $h=f \circ g$. Is $h$ always an even function?,"The answer I got from my book is positive. However, I am wondering what if we defined: $g(x)=x^2$ and $f(x)=\sqrt{x}$ Then $h(x)=f(g(x))=x$ , but $y=x$ is and odd function! so I am very confused now, could anybody help me?","['functions', 'real-analysis']"
4510680,"Find the function $f(x)$ when $f(f(x))=1-x$, for $x\in [0,1]$ [duplicate]","This question already has answers here : How to find $ f(x)$ if $f(1-f(x))=x$ for all $x$ $\in \mathbb{R}$ (2 answers) Closed 1 year ago . The function f(x) is continuous and $f(f(x))=1-x$ , for $x\in [0,1]$ then, (A) $f(\frac{1}{8})+f(\frac{7}{8})=3$ (B) $f(\frac{2}{3})+f(\frac{1}{3})=2$ (C) $f(\frac{5}{6})+f(\frac{1}{6})=1$ (D) None of These My approach is as follow $f\left( {f\left( x \right)} \right) = 1 - x \Rightarrow f'\left( {f\left( x \right)} \right) \times f'\left( x \right) =  - 1$ $f'\left( {f\left( 0 \right)} \right) \times f'\left( 0 \right) =  - 1\& f'\left( {f\left( 1 \right)} \right) \times f'\left( 1 \right) =  - 1$ $f\left( {f\left( 0 \right)} \right) = 1;f\left( {f\left( 1 \right)} \right) = 0$ Not able to procced from here",['functions']
4510703,Group action for signal,"I am working through the book Geometric Deep Learning ( https://arxiv.org/abs/2104.13478 ) and have hit the following problem (Chapter 3.1, page 14). We have a group $\mathfrak{G}$ and a set $\Omega$ for which a signal $\mathcal{X}(\Omega)$ is defined. Let us consider a group action $\mathfrak{g}.u$ . Based on it, we automatically obtain an action of $\mathfrak{G}$ on the space $\mathcal{X}(\Omega)$ : $$(\mathfrak{g}.x)(u) = x(\mathfrak{g}^{-1}u)$$ Two questions: What does automatically obtain mean here? Define? How do I verify that this is a valid group action? The best I have been able to get is: $$(\mathfrak{g}.(\mathfrak{h}.x))(u) = \mathfrak{g}.(x(\mathfrak{h}^{-1}u)) = x(\mathfrak{g}^{-1}x(\mathfrak{h}^{-1}u))$$ and $$((\mathfrak{gh}).x)(u) = x((\mathfrak{gh})^{-1}u))$$ These are not obviously equal. If it is not apparent, I am struggling a bit with the notation for group actions. This question is related to, but not the same as How group action of $G$ on set $\Omega$ acts on the vector space induced by $\Omega$, $\mathcal{X}(\Omega)$?","['symmetric-groups', 'group-theory', 'group-actions']"
4510735,Other approaches to simplify $\frac{\tan^2x-\sin^2x}{\tan 2x-2\tan x}$,"I want to simplify the trigonometric expression $\frac{\tan^2x-\sin^2x}{\tan 2x-2\tan x}$ . My approach, Here I used the abbreviation $s,c,t$ for $\sin x$ and $\cos x$ and $\tan x$ respectively, Numerator is, $$\frac{s^2}{c^2}-s^2=\frac{s^2-s^2c^2}{c^2}=\frac{s^4}{c^2}=s^2t^2.$$ And denominator is, $$\frac{2t}{1-t^2}-2t=\frac{2t^3}{1-t^2}.$$ So $$\frac{\tan^2x-\sin^2x}{\tan 2x-2\tan x}=s^2t^2\times\frac{1-t^2}{2t^3}=\sin^2x\times \frac1{\tan 2x}=\frac{1-\cos 2x}{2\tan 2x}.$$ I'm looking for alternative approaches to simplify the expression.","['algebra-precalculus', 'trigonometry']"
4510764,Rewriting equation for solving differential equation,"When going through a solution concerning a differential equation we have the following expression: $(1-\frac{x^2}{y^2})\frac{dy}{dx} + \frac{2x}{y} = 0$ and then its said to note that: $ \frac{d}{dx}(\frac{x^2}{y}+y)=\frac{2x}{y}-\frac{x^2}{y^2}\frac{dy}{dx} + \frac{dy}{dx} = (1-\frac{x^2}{y^2})\frac{dy}{dx} + \frac{2x}{y}$ so that we later can use the expression to the left, but I don't really understand how the $\frac{dy}{dx}$ is used here, could someone explain how ? Specifically what is going on with the $-\frac{x^2}{y^2}\frac{dy}{dx}+ \frac{dy}{dx}$ terms in the middle part. Thanks","['derivatives', 'ordinary-differential-equations']"
4510854,geometrical/physical interpretation of multiplication of real numbers (including negative),"In calculus we see that the derivative has a physical interpretation as speed, and a geometric interpretation as slope, and that they are helpful when thinking intuitively about that concept. But this is just an example to motivate the question I want to make about a more elementary topic: It is well know that multiplication of two positive real numbers can be interpreted as the area of a rectangle (and that it provides intuitive insight about the commutativity of multiplication). The problem is, what about negative numbers? The rectangle interpretation doesn't help in that case. Does it? So I ask: Are there geometrical or physical interpretations of multiplication of
two real numbers (which include negative numbers)? Thank you.","['physics', 'algebra-precalculus', 'geometry']"
4510930,Integration of $\frac{x^{1/3}}{1+x^2} dx$ [duplicate],"This question already has answers here : integration using residue (3 answers) Closed 1 year ago . I have problem with integration $I = \int_0^\infty \frac{x^{1/3}}{1+x^2} dx$ using residue theory. Define $\log z$ on the complex plane except the positive real line so that its imaginary part is in $(0, 2\pi)$ . Consider a counterclockwise path $C$ with a small circle around $0$ , a real line from $0$ to $\infty$ on the upper half plane, a big circle around $0$ , and a real line from $\infty$ to $0$ on the lower half plane. The integral $\int_0^\infty \frac{z^{1/3}}{1+z^2} dz$ converges to $(1-e^{2\pi i /3})I$ as on the lower real line $z^{1/3} = e^{2\pi i /3}x^{1/3}$ . Using residues I have $\frac{1}{2\pi i} \int_0^\infty \frac{z^{1/3}}{1+z^2} dz = \frac{1}{2i} (e^{\pi i /6} - e^{\pi i /2})$ . But this leads to $I = \frac{\pi i}{\sqrt 3}$ . Where did this argument go wrong? Is it not OK to apply residue theorem in this situation?",['complex-analysis']
4510938,confidence interval in terms of the empirical CDF,"Recall that the cumulative distribution function (CDF) of X is defined as: $F(x)=P(w: X(w) \leq x)$ Using the sequence $(X_i)$ estimate the CDF F(x) using the empirical CDF: $$
\bar{F_n}(x) = \frac{1}{n} \sum_{i=1}^n \chi_{(-\infty, x)} (X_i).
$$ For each fixed $x \in \mathbb {R}$ , show that $$
\mathbb {I}_{\alpha, n}(x)
 = \left\{  y \in \mathbb {R}: \bar{F_n}(x)- \frac{1}{2 \sqrt{n \alpha}} \leq y \leq \bar{F_n}(x) + \frac{1}{2 \sqrt{n \alpha}} \right\}
$$ is a $(1-\alpha) \times 100$ % confidence interval for F(x) in the sense that $P(w: F(x) \in \mathbb {I}_{\alpha, n}(x) \geq 1-\alpha)$ How to prove this statement? My attempt: I can convert the $$
\mathbb {I}_{\alpha, n}(x)
 = \left\{  y \in \mathbb {R}: \bar{F_n}(x)- \frac{1}{2 \sqrt{n \alpha}} \leq y \leq \bar{F_n}(x) + \frac{1}{2 \sqrt{n \alpha}} \right\}
$$ to $$
\mathbb {I}_{\alpha, n}(x)
 = \left\{ ny- \frac{\sqrt{n}}{2 \sqrt{ \alpha}} \leq n\bar{F_n}(x) \leq ny+ \frac{\sqrt{n}}{2 \sqrt{ \alpha}}  \right\}
$$ Then how to do next?","['statistics', 'confidence-interval']"
4510941,Is coming up heads and coming up tails independent events in a single coin toss?,"I just want to know, if a coin is tossed, the event of getting heads(A) and the event of getting tails(B) are independent to each other or not. As, P(A and B) is not equal to P(A) x P(B) , can I conclude it is not a independent event? But getting heads, how it can be dependent on getting tails? Also, getting tails, how it can be dependent on getting heads?","['probability-distributions', 'probability-theory', 'probability']"
4510955,find $\int_0^\infty \frac{|\cos (\pi x)|}{4x^2 - 1} dx$,"Find (with proof) $\displaystyle\int_0^\infty \frac{|\cos (\pi x)|}{4x^2 - 1}dx$ It's actually not even clear that the integral converges. If there were only sines/cosines in the integral, a standard technique would be to use the trigonometric identity $\sin \theta = \frac{2t}{1+t^2}$ , $\cos \theta = \frac{1-t^2}{1+t^2}$ , $t = \tan(\theta/2)$ . I know that $\frac{2}{4x^2-1} = \frac{1}{2x-1} -\frac{1}{2x+1}$ , so maybe one could plug this into the given integral to obtain an integral that's easier to evaluate? Edit: from a comment, I think it would be useful to note that $\cos (\pi x) \leq 0$ iff $(2k+1/2)\pi \leq \pi x \leq (2k + 3/2)\pi\iff(2k + 1/2)\leq x\leq 2k+3/2$ (for some integer $k$ ) and $\cos(\pi x) \ge 0$ iff $(2k - 1/2) \leq  x \leq (2k+1/2)$ (for some integer $k$ ). So we can split the integral according to these ranges. Then it might be useful to apply integration by parts. Let $a\in \mathbb{R}$ . Then $$\begin{align}\int \frac{\cos(ax)}{4x^2-1}dx &= \frac{1}2\left(\int \frac{\cos(a x)}{2x-1}dx - \int \frac{\cos(a x)}{2x+1}dx\right)\\
\\
&= \frac{1}2\left(\frac{1}2 \ln(2x-1)\cos (ax) +\frac{1}2a \int \sin(ax)\ln(2x-1)dx\right) \\
\\&-\frac{1}2\left(\frac{1}2 \ln(2x+1)\cos (ax) +\frac{1}2a \int \sin(ax)\ln(2x+1)dx\right)\end{align}$$ but I'm not sure how to simplify the result. For the bounty, I'm looking for formal proofs. In particular, I'd like to see justifications for interchanging an integral and a sum. One can freely interchange an integral and a sum if the terms are nonnegative (as $\sum \int f_n = \int \sum f_n$ for nonnegative Lebesgue measurable functions $f_n$ and the Lebesgue integral equals the Riemann integral for Riemann integrable functions). Also I'd like to see justifications for why $$\int_0^\infty \frac{\cos^2(\pi (2m+1) x)}{4x^2 - 1}dx = 0 = \int_0^\infty \frac{\sin^2(\pi (2m+1)x)}{4x^2-1} dx,$$ where $m$ is any nonnegative integer.","['integration', 'calculus', 'trigonometry', 'real-analysis']"
4510974,"If $\vec a,\vec b,\vec c$ be three vectors such that $|\vec a|=1,|\vec b|=2,|\vec c|=4$ and then find the value of $|2\vec a+3\vec b+4\vec c|$","If $\vec a,\vec b,\vec c$ be three vectors such that $\vert \vec a\vert =1,\vert \vec b\vert =2,\vert \vec c\vert=4$ and $\vec a \cdot \vec b+\vec b \cdot \vec c+\vec c \cdot\vec a=-10$ then find the value of $\vert 2\vec a+3\vec b+4\vec c \vert$ My Attempt $\vert \vec a+\vec b+\vec c \vert^2=a^2+b^2+c^2+2(\vec a \cdot \vec b+\vec b \cdot \vec c+\vec c \cdot \vec a)=1+4+16-20=1$ So, $\vert \vec a+\vec b+\vec c \vert=1$ Further by hit and trial I could see that if $\vec a=\vec i,\vec b=2\vec i,\vec c=-4\vec i$ (where $\vec i$ is unit vector along x-axis) satisfies all conditions. So, $\vert 2\vec a+3\vec b+4\vec c \vert =\vert 2\vec i+6\vec i-16\vec i\vert =8$ But can there be a better way to do this.","['algebra-precalculus', 'vectors', 'geometry', '3d']"
4510976,Is it possible for a function and its inverse to have intersections that are not on $y=x$?,"Define a function $y=f(x)$ on $\mathbb{R^{2}}$ . Can the graph of this function and that of $y=f^{-1}(x)$ have intersection(s) that cannot be represented in the form $(k,k)$ , where $k$ is a real number? How about other domains?",['functions']
4510983,Existence of smooth function with prescribed zeros and value of derivative at the origin,"Let $$
f(x) = \begin{cases}
e^{-1/x} & x > 0
\\
0 & x \leq 0.
\end{cases}
$$ Does there exist an infinitely differentiable function $h: \mathbb{R}^2 \to \mathbb{R}$ such that $$
h(x, 0) = 0, \quad h(x, f(x)) = 0, \quad x \in \mathbb{R}
$$ and ${\partial h \over \partial y}(0, 0) \neq 0$ ? I do not seem to be able to come up with an example. My attempts at the problem Taking derivatives we get the following: $$
{\partial h \over \partial x}(x, 0) = 0 \quad (x \in \mathbb{R}), \quad  {\partial h \over \partial x}(x, e^{-1 \over x}) + e^{-1 \over x}{1 \over x^2}{\partial h \over \partial y}(x, e^{-1 \over x}) = 0  \quad (x > 0),
$$ however neither equation gives information about the value of the partial derivatives of $h$ with respect to y at the origin. The function $h(x, y) = y(y - f(x))$ and other examples I have tried do not work. Using Taylor's theorem, $$
h(x, y) = \partial_yh(0, 0)y  + o(\|(x, y)\|),
$$ and solving $y = e^{-1/x}$ for $x$ , we get $$
0 = h\left ({-1 \over \ln y}, y\right) = \partial_yh(0, 0)y + o\left(\sqrt{y^2 + {1 \over (\ln y)^2}}\right).
$$ The equation remains valid if $\partial_yh(0, 0) \neq 0$ because it is true that $$
y = o\left(\sqrt{y^2 + {1 \over (\ln y)^2}}\right), \quad y \to 0.
$$ This doesn't lead to a contradiction. I've tried replacing $f(x)$ with other functions which vanish to order two at zero, but I get nothing better.","['calculus', 'derivatives', 'taylor-expansion', 'real-analysis']"
4511018,limit of $\sum_{j=1}^{n-1}a_j\frac{1}{n-j}$ as $n\to \infty$,"Suppose that $\sum_{j=1}^{\infty}a_j$ is convergent, with $a_j\geq 0$ . Is it true that $$\lim_{n\to \infty}S_n=0,\qquad S_n:=\sum_{j=1}^{n-1} \frac{a_j}{n-j}?$$ Attempt (probably wrong) with Fourier:
Define $a_j=0$ for $j\leq 0$ . $$F(x)=\sum_{n=-\infty}^\infty S_ne^{inx}=\sum_{n=1}^{\infty}a_ne^{inx}\sum_{n=1}^{\infty}\frac{1}{n}e^{inx}=\sum_{n=1}^{\infty}a_ne^{inx}(-\log (1-e^{ix})),\qquad x\in (0,2\pi) $$ Since $\begin{cases}1/n, &n>0 \\ 0, & n \leq 0\end{cases}\in L^2(\mathbb{Z})$ , then its Fourier transform $\sum_{n=1}^{\infty}\frac{1}{n}e^{inx}\in L^{2}(0,2\pi)\subset L^1(0,2\pi)$ . Since $\sum_{n=1}^{\infty}a_n<\infty$ , then $F\in L^1(0,2\pi)$ , and $\lim_{n\to \infty}S_n=0$ (Riemann-Lebesgue lemma). Anyway, even if this is somehow correct I assume this also has an elementary solution. Also, is it still true if we only require that $\sum_{j=1}^{\infty}\frac{a_j}{j}$ is convergent? It seems so from numerical experiments.","['fourier-analysis', 'analysis', 'sequences-and-series']"
4511033,Euler's derivation of Lemniscate addition theorem,"In the notes ""a Brief history of elliptic integral addition theorems"" , the author states at the very beginning of Chapter 4 that Euler found a general solution of the equation $$
\frac{dx}{\sqrt{1-x^4}}=\frac{dy}{\sqrt{1-y^4}}\tag{1}
$$ to be $$
x=\frac{c\sqrt{1-y^4}+y\sqrt{1-c^4}}{1+x^2c^2}\tag{2}
$$ I don't have a problem proving that this is a solution, but how could Euler have possibly arrived at such solution?","['elliptic-functions', 'elliptic-integrals', 'ordinary-differential-equations']"
4511034,Application of nonfamous finite groups in computer science [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question I have searched a lot about applications of finite groups in computer science. Most of the results include: Finite fields or groups of numbers coprime to $n$ which are widely used in cryptography and coding theory Permutations (symmetric group) Ring of matrices over an arbitrary field But group theory is much more enormous and broader than these groups and includes many exotic enormous groups. I wonder if there are some applications of other groups in computer science. Specifically, I would appreciate if you mention some other finite (or at least finitely generated) groups (not semigroups or monoids) with their applications. Example) for instance, optimal solving of a Rubik's cube is a computationally-intensive problem (which is called God's algorithm). Another example I want to see is something like monster group or some other finite simple groups.","['gap', 'finite-groups', 'applications', 'group-theory', 'computer-science']"
4511038,"Let $f \in C^{\infty}(\mathbb{R}^n)$ be such that $\underline{0}$ is a local minimum point for every algebraic curve, is it $0$ a local minimum point?","Let $f \in C^{\infty}(\mathbb{R}^n)$ be a real function such that $\underline{0}$ is a local minimum point for $f$ on every algebraic curve, i.e. for every algebraic curve $C$ there exists an $\epsilon > 0$ such that if $x \in B_{\epsilon}(\underline{0}) \cap C$ then $f(0) \leq f(x)$ Where $B_{\epsilon}(0) := \{ x \in \mathbb{R}^n \; : \; ||x|| < \epsilon \}$ and $C$ is the set of points that belongs to the algebraic curve (Observe that $\epsilon$ depends on $C$ ). Is it true that $\underline{0}$ is necessarily local minimum point for $f$ ?","['algebraic-curves', 'real-analysis', 'maxima-minima', 'calculus', 'differential-geometry']"
4511042,Bound for a sum of poisson probabilities,"I'd like to understand a bound that has appeared in a paper I'm reading. It has the following expression: $$\begin{align*}[...] &\leq \sum_{m \geq n }4^mP[Poisson(kt)\geq m]
\\&=e^{-kt}\sum_{m \geq n}4^m \sum_{j \geq m}\frac{(kt)^j}{j!}
\\&\leq e^{-kt}\sum_{m\geq n}e^{kt}\frac{(4kt)^m}{m!}\\
&{\color{red}\leq} e^{4kt}\frac{(4kt)^n}{\left(\frac{n}{2}\right)^{\frac{n}{2}}}
\\&{\color{blue}\leq} e^{-\frac{1}{8}n \log n}\end{align*}$$ if $n \geq (16kt)^8$ . Now, I do understand everything except the two colored inequalities. For the blue one, I believe the idea is the following: Since $n\geq(16kt)^8\implies 4kt\leq \frac{n^{\frac{1}{8}}}{4}$ and so $$e^{4kt}\frac{(4kt)^n}{\left(\frac{n}{2}\right)^{\frac{n}{2}}}\leq e^{\frac{n^{1/8}}{4}}\frac{\left(\frac{n^{1/8}}{4}\right)^n}{\left(\frac{n}{2}\right)^{n/2}} = e^{\frac{n^{1/8}}{4}} n^{n/8}n^{-n/2}4^{-n}2^{n/2} = e^{\frac{n^{1/8}}{4}}n^{-\frac{3}{8}n}2^{-\frac{3}{2}n}$$ $$=\exp\left(\frac{n^{1/8}}{4}-\frac{3}{8}n \log n -\frac{3}{2}n \log 2\right)$$ Now, for $n$ big enough $\frac{n^{\frac{1}{8}}}{4}-\frac{3}{2}n \log 2<0$ and so $\exp\left(\frac{n^{1/8}}{4}-\frac{3}{8}n \log n -\frac{3}{2}n \log 2\right)\leq \exp\left(-\frac{3}{8}n \log n\right)\leq \exp \left(-\frac{1}{8}n \log n\right)$ Now, the red one I have absolutely on idea on how to proceed.","['poisson-distribution', 'probability', 'upper-lower-bounds']"
4511105,Are metacyclic $p$-groups semidirect products?,"A group $G$ is called metacyclic if there is cyclic $N\unlhd G$ such that $G/N$ is cyclic as well. If $G$ is a metacyclic $p$ -group, I know that there is a presentation $$G\cong\langle x,y\mid\, x^{p^a}=1,\,y^{p^b}=x^{p^c},\,yxy^{-1}=x^k\rangle,$$ but is it always possible to realize $G$ as a semidirect product? Since it might make a difference: I do not care about the case $p=2$ .","['semidirect-product', 'group-theory', 'abstract-algebra', 'p-groups']"
4511112,Gradient of a higher Lie derivative of a composite function,"Although there are some formulations of the gradient of the $k$ -th Lie derivative of a function composition $g\circ h$ with respect to $\bf x$ on the vector field $\bf f$ in the literature, I am trying to prove the following pattern for any $k$ . $$
\nabla L_{\bf f}^k(g\circ h) \stackrel{?}{=}\sum_{i=0}^{k}{k \choose i}L_{\bf f}^{k-i}(g'\circ h) \nabla L_{\bf f}^ih
$$ This pattern is the second and more reasonable one I came up with after being able to write down the following equalities using brute force. \begin{align}
\frac{\partial L_\mathbf{f}(g\circ h)}{\partial x_i}=&     \frac{\partial g}{\partial h}\frac{\partial L_\mathbf{f}h}{\partial x_i}+L_\mathbf{f}(\frac{\partial g}{\partial h})\frac{\partial h}{\partial x_i}\\
\frac{\partial L_\mathbf{f}^2(g\circ h)}{\partial x_i}=&     \frac{\partial g}{\partial h}\frac{\partial L_\mathbf{f}^2h}{\partial x_i}+L_\mathbf{f}^2(\frac{\partial g}{\partial h})\frac{\partial h}{\partial x_i}+2L_\mathbf{f}(\frac{\partial g}{\partial h})\frac{\partial L_\mathbf{f}h}{\partial x_i}\\
\frac{\partial L_\mathbf{f}^3(g\circ h)}{\partial x_i}=&     \frac{\partial g}{\partial h}\frac{\partial L_\mathbf{f}^3h}{\partial x_i}+L_\mathbf{f}^3(\frac{\partial g}{\partial h})\frac{\partial h}{\partial x_i}+3L_\mathbf{f}^2(\frac{\partial g}{\partial h})\frac{\partial L_\mathbf{f}h}{\partial x_i}+3L_\mathbf{f}(\frac{\partial g}{\partial h})\frac{\partial L_\mathbf{f}^2h}{\partial x_i}&&
\end{align} I would appreciate any help to show that the conjecture above is true/false. Notation $\nabla$ is the gradient operator such that \begin{align}
    \nabla \phi = \left(
        \begin{matrix}
            \frac{\partial \phi}{\partial x_1}&
            \frac{\partial \phi}{\partial x_2}&
            \cdots&
            \frac{\partial \phi}{\partial x_n}
        \end{matrix}
        \right)^\top,\nonumber
\end{align} for some function $\phi:\mathbb{ R}\rightarrow\mathbb{ R}$ with respect to a coordinate system $(x_1, x_2, \dots, x_n)$ . The Lie derivative of a function $h:\mathbb{ R}^n\rightarrow \mathbb{ R}$ with respect to a vector field $f:\mathbb{ R}^n\rightarrow \mathbb{ R}^n$ is \begin{align}
    L_{\bf f}(h) := \nabla h \cdot f\,. \nonumber
\end{align} One can calculate the $k$ -th Lie derivative of $h$ recursively via \begin{align}
    L^k_{\bf f}(h)=L_{\bf f}L^{k-1}_{\bf f}(h)\,.\nonumber
\end{align} We use $g\circ h$ to denote the composition of the functions $h:\mathbb{ R}^n\rightarrow \mathbb{ R}$ and $g:\mathbb{ R}\rightarrow \mathbb{ R}$ . We assume that $g$ , $h$ , and the vector-valued function $f$ are at least $k$ times differentiable. We use $g'$ or $g^{(1)}$ to denote the first derivative of $g$ with respect to $h$ . The $j$ -th derivative of $g$ is $g^{(j)}$ and $g^{(0)}=g$ . The expression $g'\circ h$ has the same meaning as the expression $\partial g/\partial h$ .","['combinatorics', 'lie-algebras', 'lie-derivative', 'differential-geometry']"
4511124,Are the roots of unity the only algebraic subgroups of the multiplicative group?,"$\newcommand{\G}{\mathbb{G}}$ Let $k$ a field (or maybe more generally an arbitrary ring with connected spectrum), $\G_m$ the multiplicative group over $k$ . Are the $\mu_n = \{x^n = 1\}$ the only $k$ -subgroup schemes of $\G_m$ ? What is the best way to see this? I have difficulty mostly in the algebraically closed field case. My incomplete thoughts: Working first on $\overline{k}$ an algebraically closed field, a proper closed subgroup-scheme $H$ of $\G_m$ must be affine of dimension zero, hence its $\overline{k}$ -points are a finite discrete subgroup of $\overline{k}^\times$ and must coincide with $\mu_n$ for some $n$ (WLOG relatively prime to $p$ , the characteristic). How do I know there aren't any choices for nonreduced structure to put on these sets of points other than the ones which define $\mu_{np^k}$ ? Edit: I suppose by homogeneity that the nonreduced structure on each point must be identical to that on the identity, so we reduce to classifying subgroups of $\G_m$ supported on the origin. Perhaps this can be approached ring-theoretically? How do you descend the case above to $k_s$ , so that Galois descent gives the case of an arbitrary field? The proof above doesn't seem to translate so well, as we could have to worry about $H$ being supported on points which aren't $k_s$ -rational. But $k_s \to \overline{k}$ is a universal homeomorphism, so maybe one can sidestep the issue somehow? I don't know how one would formally patch this result to arbitrary rings or schemes, but it seems intuitively true since the classification is uniform and discrete (so it doesn't seem like a subgroup could vary in families except to be constant.)","['algebraic-groups', 'group-schemes', 'algebraic-geometry', 'group-theory', 'schemes']"
4511125,If $x^2+2(\alpha-1)x-\alpha+7=0$ has distinct negative solutions...,"Let $\displaystyle{ \alpha }$ be real such that the equation $\displaystyle{ x^2+2(\alpha-1)x-\alpha+7=0 }$ has two different real negative solutions. Then $ \ \displaystyle{ \alpha<-2 }$ ; $ \ \displaystyle{ 3<\alpha<7 }$ ; it is impossible  ; none of (a)-(c). $$$$ I have done the following : The value $\alpha$ is real and such that the equation $x^2+2(\alpha-1)x-\alpha+7=0$ has two different real negative solutions. The solutions of the equation are given from the quadratic formula \begin{align*}x_{1,2}&=\frac{-2(\alpha-1)\pm \sqrt{[2(\alpha-1)]^2-4\cdot 1\cdot (-\alpha+7)}}{2}\\ & =\frac{-2(\alpha-1)\pm \sqrt{4(\alpha^2-2\alpha-1)-4\cdot (-\alpha+7)}}{2}\\ & =-(\alpha-1)\pm \sqrt{2(\alpha^2-2\alpha-1)-2\cdot (-\alpha+7)} \\ & =-(\alpha-1)\pm \sqrt{2\alpha^2-4\alpha-2+2\alpha-14} \\ & =-(\alpha-1)\pm \sqrt{2\alpha^2-2\alpha-16}\end{align*} So that we have two different solutions the discriminant must be non-zero. So that we have two negative solutions, it must hold $-(\alpha-1)\pm \sqrt{2\alpha^2-2\alpha-16}<0$ . So that we have real solutions the expression under the square root must be non negative. The expression under the square root has the sign of the coefficient of $x^2$ , i.e. positive, outside the roots. We have that \begin{equation*}2\alpha^2-2\alpha-16=0 \Rightarrow \alpha_{1,2}=\frac{1}{2}\pm \frac{\sqrt{33}}{2}\end{equation*} So we have that the expression under the square root if $\alpha<\frac{1}{2}- \frac{\sqrt{33}}{2}$ and if $\alpha>\frac{1}{2}+ \frac{\sqrt{33}}{2}$ . Is my attempt correct so far? Now do we check if the first two intervals of $\alpha$ can hold for all these conditions? Or how do we continue? Or is there a better way to solve that exercise ?","['calculus', 'functions', 'quadratics']"
4511164,Likelihood of knowing the answer given y correct answers on multiple choice test,"I've been trying to solve the following problem but am thrown off by the ""guessing in case she doesn't know the answer"". The problem is as follows: A student takes a test with n questions of equal difficulty. For each question, the student has a
probability $\theta$ of knowing the answer and in that case she solves the question perfectly. If she does
not know the answer, she guesses and then she has a probability of 0.5 of solving it correctly. The
responses to all n questions can be considered independent. Based on this information, the following is asked: Derive analytically the likelihood function for $\theta$ given y correctly solved items. The ""likelihood given y correctly solved items"" confuses me. I've seen many examples of solving $P(\theta|Y=1)$ using Bayes' theorem (e.g. see here ). But how can this be generalized to a likelihood for $\theta$ given y correctly solved items? Do we even need Bayes' theorem or can this just be answered with a Binomial distribution corrected for the guessing given she doesn't know the answer? Furthermore, a follow-up question is asked: Assuming a beta prior with constants a and b, derive an expression for the marginal probability $p(y)$ . Simplify the expression, but leave the integral if there is no closed-form solution. This makes me believe Bayes' theorem would indeed be used for the first question.
Any help on any part of this problem is greatly appreciated.","['conditional-probability', 'binomial-distribution', 'bayes-theorem', 'maximum-likelihood', 'probability-theory']"
4511183,Prove that $S$ is a coset of some subgroup of $G$ iff $S+S-S=S.$,"I'm a student self-learning abstract-algebra but encountered this problem. The full problem is here: Suppose $S$ is a nonempty subset of an additive abelian group $G$ . Prove that $S$ is a coset of some subgroup of $G$ iff $S+S-S=\{a+b-c:a,b,c \in S\}$ is equal to $S$ . My sketch of proof is like this: ( $\Rightarrow$ ) I can prove this direction. ( $\Leftarrow$ ) But for this direction, I can't figure it out. I tried two ways, the first is that suppose $x\in G$ , want to show there exists $s \in S$ such that $s=x+h$ , for some $h \in H$ , then $x+h=a+b-c$ , but I think I cannot find the relation between the coset and the subgroup $H$ through this equation. The second approach is that since $G$ is additive abelian, then $H$ is a normal subgroup of $G$ . I can also show that $S$ is a normal subgroup of $G$ , but it seems like there isn't any relation between the coset $S$ and $H$ , either. Any discussions and help are really appreciated. Thanks!","['group-theory', 'abstract-algebra', 'abelian-groups']"
4511187,"Given that for each $n,\;x_n^n + x_n-1= 0,$ is $(x_n)_n$ convergent?","Prove that for $n\ge 2$ , the equation $x^n + x-1 = 0$ has a unique root in $[0,1]$ . If $x_n$ denotes this root, prove that $(x_n)_n$ is convergent and find its limit. The limit is $1$ . But to find the limit, I need to assume $\lim\limits_{n\to\infty} x_n^n = 0,$ which seems nontrivial (e.g. it doesn't hold for $y_n = 1-1/n$ as $\lim\limits_{n} y_n^n = 1/e,$ even though $0 \le  y_n < 1$ for all $n\ge 1$ ). The derivative of $f_n(x) = x^n + x-1$ is positive on $[0,1]$ , which along with the fact that $f_n(0)f_n(1) < 0$ implies that $f_n(x)$ has a unique root in $[0,1]$ . $x_n$ is convergent because $0 < x_n < 1\Rightarrow 0 < x_{n+1} < 1$ and $x_{n}^{n+1} + x_n - 1 < 0\Rightarrow x_n < x_{n+1}$ as $f_n'(x) > 0$ on $[0,1]$ . However, if I do not assume $\lim\limits_n x_n^n = 0$ , I'm not sure how to prove that $\lim\limits_n x_n = 1.$","['limits', 'calculus', 'sequences-and-series']"
4511248,Why outer measure of Vitali set is greater than 0 or equals to 0?,"In the following conservation (link below) https://www.researchgate.net/post/Bounds_on_the_outer_measure_of_Vitali_Set_in_0_1 There is a person who said that outer measure of the Vitali set on $[0,1]$ is greater than 0. Anyone can tells me why?","['measure-theory', 'lebesgue-measure', 'outer-measure']"
4511249,"Is there a conceptual proof that $A_n$ is simple for $n \ge 5$, or that it is the only nontrivial normal subgroup of $S_n$? [closed]","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 1 year ago . Improve this question All the proofs I’ve seen of this are very combinatorial and at least for me not very memorable. Is there a cleaner or more conceptual proof? For instance, I wondered if representation theory might yield something easier. $S_n$ is such a ubiquitous group, that I imagine that looking at an appropriate action and employing techniques from the category in which the action lives, we might find a cleaner proof.","['permutations', 'normal-subgroups', 'intuition', 'symmetric-groups', 'group-theory']"
4511250,I found a pattern in consecutive squares: $(a^2-b^2)-(b^2-c^2)$ is always $2$.,I was working on squares of numbers then found out that the difference of difference between two consecutive numbers is $2$ . Saying this with an example like $$2^2=4\qquad\qquad 3^2=9\qquad\qquad 4^2=16$$ $$9-4=5\qquad\qquad 16-9=7$$ $$7-5=2$$ $4^2-3^2-(3^2-2^2)$ is always $2$ . If we take any three consecutive three numbers and do the same thing we get $2$ . Is this proved before ?,"['algebra-precalculus', 'arithmetic']"
4511251,"Prove $\frac{x}{y^2}+\frac{y}{z^2}+\frac{z}{x^2}\ge \frac{x^2}{y}+\frac{y^2}{z}+\frac{z^2}{x}$ where $x, y, z > 0$ and $x+y+z=3$","Prove $$\frac{x}{y^2}+\frac{y}{z^2}+\frac{z}{x^2}\ge \frac{x^2}{y}+\frac{y^2}{z}+\frac{z^2}{x},$$ where $x,y,z>0$ and $x+y+z=3$ . Maybe we can show $$\frac{x}{y^2}+\frac{y}{z^2}+\frac{z}{x^2}\ge x^3+y^3+z^3,\tag1$$ then $$\frac{x}{y^2}+\frac{y}{z^2}+\frac{z}{x^2}\ge \frac{1}{2}\left(\frac{x}{y^2}+x^3+\frac{y}{z^2}+y^3+\frac{z}{x^2}+z^3\right)\ge \frac{x^2}{y}+\frac{y^2}{z}+\frac{z^2}{x}.$$ But it's also difficlut to show $(1)$ .","['contest-math', 'algebra-precalculus', 'inequality']"
4511296,The isomorphism between two $C^*$-algebras,"Let $A,B$ be two $C^*$ -algebras. If there exist two injective $*$ -homomorphisms $f:A\to B$ and $g:B\to A$ , can we conclude that $A$ is $*$ -isomorphic to $B$ ?","['c-star-algebras', 'functional-analysis', 'operator-algebras']"
4511304,How to get angles of a triangle given three slopes from each line,"I need a formula to tell whether a trigonometry corner has an obtuse angle or not, given slopes value of each line connecting every two points respectively. Assuming I have three points | X(1,3). . . . .
| . . . . Y(5,2).
| . . Z(3,1). . .
|________________ So, their slopes will be XY : -1/4
XZ : -1
YZ : 1/2 So from given slopes as variables, we'll be able to tell that point Z has an obtuse angle. The rest are acute of course. But I can't figure out how to do it. Thank you in advance.","['coordinate-systems', 'trigonometry']"
4511321,"Prove if $\sqrt{x+1}+\sqrt{y+2}+\sqrt{z+3}=\sqrt{y+1}+\sqrt{z+2}+\sqrt{x+3}=\sqrt{z+1}+\sqrt{x+2}+\sqrt{y+3}$, then $x=y=z$.","Let $x$ , $y$ , $z$ be real numbers satisfying $$
\begin{align}
&\sqrt{x+1}+\sqrt{y+2}+\sqrt{z+3}\\
=&\sqrt{y+1}+\sqrt{z+2}+\sqrt{x+3}\\
=&\sqrt{z+1}+\sqrt{x+2}+\sqrt{y+3}.
\end{align}$$ Prove that $x=y=z$ . I tried assuming $x>y>z$ , $x>y=z$ , $x<y<z$ , etc., but none of the directions work. Please help me solve this problem.","['algebra-precalculus', 'radicals']"
4511332,What's the $2$d-object that's in $1:1$ correspondence with all lines in $\Bbb R^2$?,"What's the 2d-object that's in 1:1 correspondence with all lines in $\Bbb R^2$ ? (And what's the correct term for such an object? Locus? Fundamental domain?) Writing a line as $$y=ax+b \quad\text{ with }\quad (a,b)\in \Bbb R^2\tag 1$$ each point $(a,b)$ represents a different line, but this misses all vertical lines. The line for $a\to\infty$ is $x=0$ and the same as $a\to-\infty$ . It can be included by representing lines as $$y=\tan(a)\cdot x+b \quad\text{ with }\quad (a,b)\in \left[-\tfrac\pi2,\tfrac\pi2\right]\times\Bbb R\tag 2$$ However $(\pm\frac\pi2,b)$ will all yield the same line, so that these points have to be identified with each other. And the lines $x=c$ , $c\neq0$ are still missing. For 2-dimensonal projective space $M=P\Bbb R^2$ , I think all lines are  represented by $M$ itself:  For a line $g=(g_x:g_y:g_z)$ and a point $P=(x:y:z)$ , the condition that $P\in g$ is $$g\cdot P = g_xx+g_yy+g_zz=0$$ which is symmetric in $g$ and $P$ , and therefore the space of all lines is isomorphic to the space $M$ of all points. To go from $M$ to $\Bbb R^2$ one can remove the line ""at infinity"", which is homeomorphic to $\Bbb S^1$ .  But $M\backslash \Bbb S^1$ has two connected components: One is a Möbius Band and one is a disc, and I have no idea how to interpret them in terms of unique affine lines. Addendum: Meanwhile I found a better representation of lines in the real $a$ , $b$ -plane: Let $g$ be the line $$g:ax+by=a^2+b^2-r^2\tag 3$$ for some constant $r$ , say $r=1$ . Then $a^2+b^2 > r^2$ will yield all possible lines except the ones through the origin. Lines through the origin are realized by $a^2+b^2 = r^2$ where $(a,b)$ and $(-a,-b)$ yield the same line, i.e. we have to identify opposite points of the circle of radius $r$ . Points in the circle do not represent (new) lines, so we effectively poked a hole of radius $r$ into the plane and glued opposite sides of that hole together, yielding a Möbius band. Here is a Desmos plot .  The blue dot is $(a,b)$ in the $a$ , $b$ -plane which represents the blue line and can be dragged around. The red-ish circle represents the hole poked into the plane. Actually, this can be used the other way round: All points in the circle represent lines, except the point at the origin which would represent the line at infinity.  Points in the white area do not represent (new) lines. Again, opposite points of the circle are glued together because they represent the same line through the origin. Moreover, $(3)$ generalizes nicely to higher dimensions: Each point $g\in\Bbb R^n$ with $|g| > r$ represents the unique $n-1$ -dimensional hyper-plane $$L_g: \langle g,x\rangle = |g|^2-r^2\tag 4$$ And the points with $|g|=r$ represent hyper-planes through the origin where $L_g = L_{-g}$ , so that $g$ and $-g$ have to be indentified to achieve uniqueness.  Hence the locus is $n$ -dimensional projective space with one point removed. $L_g$ has a distance of $(g^2-r^2)/|g|$ to the origin, and it is perpendicular to $g$ which means that for all $x,y\in L_g$ we have $\langle x-y,g\rangle=0$ .","['projective-space', 'geometry', 'geometric-topology', 'general-topology', 'affine-geometry']"
4511363,MGF from a conditional distribution,"I'm having a difficulty with this problem, it is not for submission. Let Y~U(0,1) be a continuous random variable, and X is another random variable such that given Y=p: $X|_{Y=p}$ ~B(n,p). I need to find the moment generating function of X. I honestly don't have much here, but I will try to add some context. It's weird to me that they ask about Y=p while being a continuous random variable, but my guess is that this will be relevant later. I have the MGF formula of the Unified distribution: $$M_x(t) = \frac{e^{tb}-e^{ta}}{t\left(b-a\right)} $$ And I thought about using that, but I still don't really see the way.
Another direction I thought of going through but couldn't advance with it is going to the definition: $$M_x(t) = \sum _x\:e^{tx}P\left(X_{|Y=p}=x\right) $$ but I couldn't really solve it. Thanks in advance for any help provided.","['conditional-probability', 'statistics']"
4511369,Show that $f(x)=\cos x-\sin x$ is bounded,"Show that $$f(x)=\cos x-\sin x$$ is bounded. We know that $$-1\le\cos x\le1\\{-1}\le\sin x\le1,$$ so I tried to subtract these two inequalities, but it seems that it doesn't work as we get: $$0\le\cos x-\sin x\le0$$ I also noticed: $f(x)=\cos x-\sin x\ge\cos x-(-1)=\cos x+2$ and $f(x)=\cos x-\sin x\ge\cos x - 1$ .","['algebra-precalculus', 'trigonometry', 'upper-lower-bounds']"
4511420,Is this a valid proof for the pythagorean theorem?,"The Pythagorean theorem states that in a right triangle, $a^2+b^2=c^2$ , where $c$ is the hypotenuse. I had this idea for a proof: Let $a$ and $b$ be orthogonal vectors in $\mathbb{R}^2$ . We need to prove that $$\lVert a\rVert^2+\lVert b\rVert^2=\lVert a+b\rVert^2$$ as $a+b$ is the hypotenuse of the right triangle with the other sides $a$ and $b$ . $$\lVert a+b\rVert^2=\langle a+b,a+b\rangle$$ (Definition of the norm) $$\langle a+b,a+b\rangle=\langle a,a+b\rangle+\langle b,a+b\rangle$$ (Inner product axioms) $$\langle a,a+b\rangle + \langle b,a+b\rangle=\langle a,a\rangle+\langle a,b\rangle+\langle b,a\rangle+\langle b,b\rangle$$ (Inner product axioms) $$\langle a,a\rangle+\langle a,b\rangle+\langle b,a\rangle+\langle b,b\rangle=\lVert a\rVert^2+2\langle a,b\rangle+\lVert b\rVert^2$$ (Definition of norm + inner product axioms) $$\lVert a\rVert^2+2\langle a,b\rangle+\lVert b\rVert^2=\lVert a\rVert^2+\lVert b\rVert^2$$ (The inner product of orthogonal vectors is $0$ ) Therefore, $$\lVert a+b\rVert^2=\lVert a\rVert^2+\lVert b\rVert^2$$ Q.E.D Is this proof valid? Thanks for reading!","['vectors', 'solution-verification', 'geometry']"
4511424,Lagrange multiplier for QCQP with $1$ equality constraint,"I want to find the maximum of $f(x,y) = x^2 - y^2$ under the constraint $\frac12 x^2 + y^2 - 1 = 0$ . I defined Lagrange function: $$ L= x^2 - y^2 + \lambda \left( \frac12 x^2 + y^2 - 1 \right) $$ Then caculated $L'_x, L'_y, L'_{\lambda}$ and got the following equations system: $$ \begin{aligned} x (2 + \lambda) &= 0 \\ 2 y (\lambda - 1) &= 0 \\ \frac12 x^2 + y^2 - 1 &= 0 \end{aligned} $$ How to solve this system so I can find the maximizer $(x,y)$ ?","['qcqp', 'lagrange-multiplier', 'multivariable-calculus', 'calculus', 'optimization']"
4511441,Generalizing a property of cones to cusps,"An $\alpha$ -cusp with slope $a$ and vertex $z$ is a function $ C^\alpha: \mathbb{R}^{n} \longrightarrow \mathbb{R} $ defined by $ C^\alpha := a| x - z |^{\alpha} + b  $ where $  a,b \in \mathbb{R} $ . For a given function $f:D \subset \mathbf{R}^n \longrightarrow \mathbb{R}$ and a real number $\alpha \in [0,1]$ we set for all $A \subset D$ \begin{equation} \label{Holder seminorm}
    [f]_{\alpha,A} : = \inf \left \{ C \in \mathbb{R} : \frac{|u(x) - u(y)|}{|x-y|^\alpha } \le C \quad \mbox{for all} \quad x \neq y \in A  \right \}.
\end{equation} I'd like to prove the following result Let $ W \subset \subset \mathbb{R}^{n} $ and $ C^{\alpha}_u$ be an $\alpha$ -cusp with vertex $ z \notin W $ and slop $ a \in \mathbb{R} $ . If $ u \in C( \overline{W}) $ satisfies $ u = C^{\alpha}_u $ on $ \partial W $ and $ [u]_{\alpha,W} = | a | $ , then $ u \equiv C^{\alpha}_u $ in $ W $ . Suppose that there exists $ y \in W $ so that $ u(y) \neq C^\alpha_u (y) $ , for instance $ u(y) > C^{\alpha}_u (y) $ . Let $L$ be the line determined by the vertex $z$ of the cusp $C^{\alpha}_u$ and $y$ . Let $y^{*  }$ and $y^{**}$ respectively  the closest and further point in $L \cap \partial W$ in relation to $y    $ . If $ a \geq 0 $ we obtain, $$ u (y) - u(y^{*}) \ > C^{\alpha}_u(y) - C^{\alpha}_u(y^{*})   = a| y - y^{*} |^{\alpha} . $$ The proof for $\alpha =1$ is the case when $C^\alpha_u$ is a cone. You can see detais em (1.4) in here https://www.ams.org/journals/bull/2004-41-04/S0273-0979-04-01035-3/S0273-0979-04-01035-3.pdf . But we do the proof below. Notice that we have used the equality in the triangle inequality at the end. For $\alpha \in (0,1)$ So I ask Is there a triangle inequalty for $d(x,y) = |x-y|^\alpha $ that reaches an equality for some $y^*,y^{**}$ similar to the aboves? Maybe instead of a Line passing through $y$ and $z$ we have to find some ""cusp"" instead a line. Given corrects $y^*,y^{**}$ in this case. If you can find a counterexemplo I appreciate too.","['solution-verification', 'analysis']"
4511445,How to understand the notation $\frac{\partial}{\partial z}$ and $\frac{\partial}{\partial \bar{z}}$ (Wirtinger derivatives)?,"The Wirtinger derivatives are defined as $$\frac{\partial}{\partial z}:=\frac{1}{2}\left(\frac{\partial}{\partial x} - i\frac{\partial}{\partial y}\right)\quad \quad\frac{\partial}{\partial \bar{z}}:=\frac{1}{2}\left(\frac{\partial}{\partial x} + i\frac{\partial}{\partial y}\right)$$ I wonder why we use the notation partial derivative $\frac{\partial}{\partial z}$ and $\frac{\partial}{\partial \bar{z}}$ to represent Wirtinger derivatives. Does this suggest Wirtinger derivatives are a kind of partial derivative with respect to $z$ or $\bar{z}$ ? I am confused by this fact: $z$ and $\bar{z}$ are not independent. Here by not independent I mean $\bar{z}$ is a function of $z$ . This leads to several following problems. Let $f$ be the function $f(z)=\bar{z}$ . As a function of $z$ , the partial derivative of $f$ with respect to $z$ should not equal to 0. However, according the definition of Wirtinger derivatives, $\frac{\partial f}{\partial z} = 0$ . Let $f:\mathbb{C} \to \mathbb{C}$ be a function. Can we write $f(z) = g(z,\bar{z})$ for some function $g:\mathbb{C}^2 \to \mathbb{C}$ and regard $\frac{\partial f}{\partial z}$ as $\frac{\partial g}{\partial z}$ , where $\frac{\partial g}{\partial z}$ means partial derivative instead of Wirtinger derivative ? If so, when we compute partial derivative of $g$ respect to $z$ , we should fix the second variable $\bar{z}$ . But fixing $\bar{z}$ also means fixing $z$ , then how does partial derivative make sence? By the definition of Wirtinger derivatives, it's easy to get $\frac{\partial \bar{z}^n}{\partial \bar{z}} = n\bar{z}^{n-1}$ , similar to partial derivative. For arbitrary $f:\mathbb{C} \to \mathbb{C}$ , can we get a right answer like this, computing Wirtinger derivative as partial derivative ? If so, how to prove it? I believe the notation $\frac{\partial}{\partial z}$ and $\frac{\partial}{\partial \bar{z}}$ suggest Wirtinger derivatives are a kind of partial derivative, but I can't solve the problems above. Could you please give me a rigorous answer? Thanks for your help.",['complex-analysis']
4511476,Find the quantity: $P \left(\bigcap_{j=1}^{n} \left \{U_{(j)} > \frac{\alpha j}{n} \right \} \right)$,"Let $U_1,...,U_n$ be i.i.d. $U(0,1)$ and $U_{(1)},...,U_{(n)}$ be their order statistics. For $n=1,2,...$ ,  find the quantity: $$ P \left(\bigcap_{j=1}^{n} \left \{U_{(j)} > \frac{\alpha j}{n} \right \} \right) $$ where $0 \le \alpha \le 1$ . My approach: We can write $$ \begin{align*}
P \left(\bigcap_{j=1}^{n} \left \{U_{(j)} > \frac{\alpha j}{n} \right \} \right) & = E \left[ I_{\bigcap_{j=1}^{n} \left \{U_{(j)} > \frac{\alpha j}{n} \right \}} \right] \\
& = E \left[ \prod_{j=1}^{n} I_{\bigcap_{j=1}^{n} \left \{U_{(j)} > \frac{\alpha j}{n} \right \}} \right] \\
& = E \left[ E \left [ \prod_{j=1}^{n} I_{\bigcap_{j=1}^{n} \left \{U_{(j)} > \frac{\alpha j}{n} \right \}} | U_{(n)} \right ] \right ] \\
& = E \left[ E \left [ \prod_{j=1}^{n} I_{\bigcap_{j=1}^{n} \left \{\frac{U_{(j)}}{U_{(n)}} > \frac{\alpha j}{nt} \right \}} | U_{(n)}=t \right ] \right ] \\
& = E \left[  \prod_{j=1}^{n} P  \left \{ \frac{U_{(j)}}{U_{(n)}} > \frac{\alpha j}{nt} \right \} \right ] \ \text{using Basu's Theorem} \\
\end{align*}
$$ I am not sure if I am doing this the right way. I used the fact that $\left \{ \frac{U_{(1)}}{U_{(n)}},...,\frac{U_{(n-1)}}{U_{(n)}} \right \}$ and $U_{(n)}$ are independent. Even if this is correct, how should I compute the probability in the last step? Is there an easier way out? Thanks.","['statistics', 'uniform-distribution', 'independence', 'order-statistics', 'probability']"
4511493,Determine which sequences are equicontinuous.,"I'd appreciate if somebody could check if my proofs below are correct and also give me some hints on the equicontinuity of $(k_n)$ (part c) ). $\mathbb{R}$ represents the real line with the standard topology. Consider the following sequences of functions in $\mathcal{C}(\mathbb{R}, \mathbb{R})$ , (the set of continuous functions $f : \mathbb{R} \to \mathbb{R}$ ): a) $f_n (x) = n + \sin(x)$ b) $h_n (x) = |x|^{1/n}$ c) $k_n (x) = n\sin(x/n)$ where $n\ge 1$ . Question Which sequences are equicontinuous / pointwise bounded? Answer a) $(f_n)$ is not pointwise bounded, since given any $x, |n + \sin(x)| \to \infty$ . For equicontinuity at a point $x$ , we need to show that given any $\epsilon > 0$ , there is an open neighborhood $U$ of $x$ such that $$ \forall n\ge 1, \forall x^* \in U: |f_n(x^*) - f_n(x)| < \epsilon $$ Since $x \mapsto \sin(x)$ is continuous, let $U$ be an open set containing $x$ such that $$x^* \in U \implies |\sin(x^*) - \sin(x)| < \epsilon$$ Since $|f_n(x^*) - f_n(x)| = |\sin(x^*) - \sin(x)|$ for all $n$ , we see that $U$ satisfies the required condition. Since $x$ was arbitrary, we conclude that the sequence $(f_n)$ is equicontinuous. b) $(h_n)$ is indeed pointwise bounded, since for any given $x$ , $|x|^{1/n} \to 1$ . I claim that it is not equicontinuous, since it is not equicontinuous at $x=0$ : Assume it is equi. cont. at $x=0$ . Pick $\epsilon = 1/2$ . Then there exists $(-\delta, \delta)$ for some $\delta > 0$ such that $$ \forall x^* \in (-\delta, \delta) \land \forall n\ge 1: |x^*|^{1/n} < 1/2$$ But this is not possible, since $|x^*|^{1/n} \to 1$ . c) $(k_n)$ is clearly not pointwise bounded. For equicontinuity, I'm a bit lost. Any tips?","['continuity', 'general-topology', 'equicontinuity', 'real-analysis']"
4511505,"If $T: E \rightarrow E$ is a bounded linear operator, then $e^T$ is as well","On page 39 of Differential Dynamical Systems (revised edition) by Meiss, the author gives the following Lemma. Lemma 2.8 : If $T$ is a bounded linear operator, then $e^T$ is as well. We are assuming (I think) that $T: E \rightarrow E$ , where $E$ is a normed vector space over $\mathbb{R}$ . I have a couple doubts about the proof of the lemma, which goes as follows: Proof : Choose an arbitrary $x \in E$ and consider the value of $e^T(x)$ . By definition (2.25) [ $e^T \equiv \sum_{k=0}^{\infty} \frac{T^k}{k!}$ ], this is a series whose terms are elements of $E$ . The norm of this series is bounded by the sum of the norms of each term. By the definition of the operator norm, for any $x$ , $$ |T(x)| \leq \|T\| |x|, $$ $$ |T^k(x)|  = |T(T^{k-1}(x))| \leq \|T\| |T^{k-1}(x)| \leq \cdots \leq \|T\|^k |x|. $$ Consequently, each of the terms in the series $e^T(x)$ can be bounded by $$ \left|\frac{T^k(x)}{k!} \right| \leq \frac{\|T\|^k}{k!} |x| = M_k. $$ The series of real numbers $$ \sum_{k=0}^{\infty} M_k = \sum_{k=0}^{\infty} \frac{\|T\|^k}{k!}|x| = e^{\|T\|}|x| $$ converges for any finite value of $\|T\|$ . By the Weierstrass M-test $|e^T(x)| \leq e^{\|T\|}|x|$ and the series for $e^T(x)$ converges uniformly in $x$ . Moreover, $\|e^T\| \leq e^{\|T\|}$ , so the exponential is a bounded operator. $\qquad \square$ My questions: How are we allowed to use the Weierstrass M-test here? Don't we require that the norm of each term in the series is bounded uniformly in $x$ , i.e. $\left|\frac{T^k(x)}{k!}\right| \leq M_k$ for all $x \in E$ ?  (I am going by the version of the Weierstrass M-Test that I stated in this previous post . Don't we also need to assume that $E$ is a Banach-space? (This was an assumption I used in the proof I linked.)","['operator-theory', 'vector-spaces', 'analysis', 'linear-algebra', 'uniform-convergence']"
4511512,Equations and truth tables,"Let's say I work on the inequality $|x-1|>x-2$ in the following way: $$
|x-1|>x-2 ⇔ x-1≥0 \quad ∧ \quad |x-1|>x-2 \quad ∨ \quad x-1<0 \quad ∧ \quad |x-1|>x-2 \\
⇔  x-1≥0 \quad ∧ \quad x-1>x-2 \quad ∨ \quad x-1<0 \quad ∧ \quad -(x-1)>x-2 \\
⇔ x-1≥0 \quad ∧ \quad -1>-2 \quad ∨ \quad x-1<0 \quad ∧ \quad -(x-1)>x-2
$$ In $x-1≥0 ∧ -1>-2$ there is the true statement $-1>-2$ . So the truth value of the whole statment $$
x-1≥0 ∧ -1>-2 
$$ only depends on $x-1≥0$ . If $x-1≥0$ is true, then the whole statment is true, if $x-1≥0$ is false then whole statment is false. So I should be able to write the following $$
x-1≥0 ∧ -1>-2 ⇔ x-1≥0 
$$ I think that I can prove that I am allowed to write $$
x-1≥0 ∧ -1>-2 ⇔ x-1≥0 
$$ with the help of a truth table in the following way:
We know that the statement $B$ is true. With this knowledge we analyse the statement $$
A∧B⇔A 
$$ with the help of a truth table: $A$ $B$ $A$ $∧$ $B$ $⇔$ $A$ 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 for ""true"", 0 for ""false"". The first two columns show all the possible truth values the statements A and B can have. As you can see in the truth table, the equivalence ⇔ has always the truth value of 1. This proves that if you have a statement B that is true, you can write $$
A∧B⇔A .
$$ So coming back to the equations: this proves that I am allowed to write $$
x-1≥0 ∧ -1>-2 ⇔ x-1≥0 .
$$ I also analysed this $$
A∨B⇔A 
$$ statement in a similar way: We know that the statement B is false. With this knowledge we analyse the statement $$
A∨B⇔A 
$$ with the help of a truth table: $A$ $B$ $A$ $∨$ $B$ $⇔$ $A$ 1 0 1 1 0 1 1 0 0 0 0 0 1 0 As you can see in the truth table, the equivalence ⇔ has always the truth value of 1. So first I want to know, up to this point, is everything correct? I have another question which is similar. Let's say I work on a set of equations and get to the following point: $$
\frac{20}{x} = \sqrt{(41-x^2 )}  ∧  y=\frac{20}{x}
$$ Now, in a side-calculation, I work on the left equation by squaring it $$
\frac{20}{x} = \sqrt{(41-x^2 )} ⟹ \frac{400}{x^2} =41-x^2
$$ Now I use this Information in my set of equations, like this: $$
\frac{20}{x} = \sqrt{(41-x^2 )}  ∧  y=\frac{20}{x} ⟹ \frac{400}{x^2} =41-x^2  ∧  y=\frac{20}{x}
$$ I want to  check if I actually can write the Implication-Arrow ""⟹"" there. I do this by using a truth table again, in the following way:
We know that $$
A⇒C
$$ is true (thats the squaring in the side-calculation). With this knowledge we analyse the statement $$
A ∧ B ⇒ C ∧ B
$$ with the help of a truth table: $A$ $B$ $A$ $∧$ $B$ $⟹$ $C$ $∧$ $B$ 0 1 0 0 1 1 ? ? 1 0 0 0 0 0 1 ? 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 I put the question marks there because if $A$ is false, we don't know what truth value $C$ has, even if $A⇒C$ is a tautology. All we know from the tautology $A⇒C$ is, that if $A$ is true, then $C$ is also true.
The truth table proves that the statement $$
A ∧ B ⇒ C ∧ B 
$$ with: $A⇒C$ is true is always true. That means it's correct to write an implication arrow here: $$
\frac{20}{x} = \sqrt{(41-x^2 )}  ∧  y=\frac{20}{x} ⟹ \frac{400}{x^2} =41-x^2  ∧  y=\frac{20}{x}
$$ Again I have analysed the same stuff with the logical OR $∨$ :
We know that $$
A⇒C
$$ is true. With this knowledge we analyse the statement $$
A ∨ B ⇒ C ∨ B
$$ with the help of a truth table: $A$ $B$ $A$ $∨$ $B$ $⟹$ $C$ $∨$ $B$ 0 1 0 1 1 1 ? 1 1 0 0 0 0 0 1 ? ? 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 The truth table proves that the statement $$
A ∨ B ⇒ C ∨ B 
$$ with: $A⇒C$ is true is always true. Are all these thoughts correct ? EDIT @ryang thank you for your answer.
Yes I do know about the quantifier ∀x. Maybe I'll write the following to lay some groundwork, so that you also know where I stand with my knowledge:
We can think of two predicates $$
A(x):(x=2 \Rightarrow  x^2=4) \ \ \ \  \mathrm{and} \ \ \ \ B(x):(x^2=4 \Rightarrow x=2 )
$$ There are real numbers that turn the predicate $A(x)$ into a true statement for eg. $A(3)$ is true, $A(2)$ is true, $A(-2)$ is true. There are real numbers that turn the predicate $B(x)$ into a true statement for eg. $B(3)$ is true, $B(2)$ is true. We can easily see that $B(-2)$ is false.
Now it turns out that the statement $$
\forall x\in \mathbb R: A(x)
$$ is a true statement and that the statement $$
\forall x\in \mathbb R: B(x)
$$ is a false statement (just by looking at the truth value of $B(-2)$ ).
The reason that we definitely know that $$
\forall x\in \mathbb R: A(x)
$$ is a true statement is because we know that $$
t_1=t_2 \Rightarrow f(t_1)=f(t_2)
$$ (with a function $f$ )
is a tautology, because that's just what functions do (input→output). So when we do our usual rearrangings of equations, we actually use functions on both sides of the equation. If the function $f$ is injective we can actually put the equivalence arrow $⇔$ there. The function used in the predicate $A(x)$ looks like this $$
f:t\mapsto t^2
$$ First I wanted to know, what do you think about the first two truth tables? Are they and the thoughts behind them  (how I set them up and how I used them to prove my point) correct? Regarding your first point, I meant it this way:
We know that $$
A⇒C
$$ is true/a tautology. With this knowledge we analyse the statement $$
A ∧ B ⇒ C ∧ B
$$ with the help of a truth table. So I don't think that the truth table you posted is actually correct or maybe I should say, it doesn't represent what I am trying to do. Because the statement $$
A⇒C
$$ should always have a truth value of 1 since it is a tautology. This should represent the step in the side-calculation where I squared the equation $$
\frac{20}{x} = \sqrt{(41-x^2 )}  ⟹ \frac{400}{x^2} =41-x^2.
$$ I was trying to prove with a truth table that I can actually write the Implication-Arrow $⟹$ here $$
\frac{20}{x} = \sqrt{(41-x^2 )}  ∧  y=\frac{20}{x} ⟹ \frac{400}{x^2} =41-x^2  ∧  y=\frac{20}{x}.
$$ That's why the third and fourth truth table look the way they do. Is it possible to set the truth tables up the way that I did to prove my point? Regarding your point:
""Furthermore, it is not generally valid to determine the truth of a predicate-logic formula by simply dropping its quantifiers and ignoring the internal structure of $Q(x)$ by pretending that it is just $Q$ ."" But are we not doing this actually pretty often? For example, let's say I work on the following set of equations $$
y=\frac{1}{6}x^2 \ \  \land \ \  x^2+y^2=16
$$ like this $$
y=\frac{1}{6}x^2 \ \  \land \ \  x^2+y^2=16 \iff 6y=x^2 \ \  \land \ \ x^2+y^2=16 \iff 6y=x^2 \ \  \land 6y+y^2=16 \\ 
\iff 6y=x^2 \ \  \land \ \ (y=2 \lor y=-8) \iff 6y=x^2 \ \land \ y=2 \ \ \lor \ \ 6y=x^2 \ \land \ y=-8   
$$ The reason I knew that this step $$
6y=x^2 \ \  \land \ \ (y=2 \lor y=-8) \iff 6y=x^2 \ \land \ y=2 \ \ \lor \ \ 6y=x^2 \ \land \ y=-8   
$$ is valid, is because earlier I looked at the truth table of $$
A \ \land \ (B \lor C) \iff A \ \land \ B \ \ \lor \ \ A \ \land \ C.  
$$ The truth table shows me that $$
A \ \land \ (B \lor C) \iff A \ \land \ B \ \ \lor \ \ A \ \land \ C.  
$$ is a tautology, so that's why I used this when I worked on my sets of equations. I hope you see my point.","['propositional-calculus', 'predicate-logic', 'logic', 'discrete-mathematics', 'inequality']"
4511556,Solving $2(y+e^x)dy + (y^2+4y e^x)dx = 0 $ and understanding integrating-factors,"We want to solve $2(y+e^x)dy + (y^2+4y e^x)dx = 0 $ which, across the spectrum is the standard format of the integrating factor technique for ODE. My book, however, covers only the integrating factors of the forms \begin{align}
\dfrac{1}{N(x,y)} \cdot \left( \dfrac{\partial M}{\partial y} - \dfrac{\partial N}{\partial x} \right)\\
\dfrac{1}{M(x,y)} \cdot \left( \dfrac{\partial N}{\partial x} - \dfrac{\partial M}{\partial y} \right)
\end{align} where of course we want the first expression to be a function of $x$ only or the second to be a function of $y$ only. In the above example however, doing these calculations yields nothing like that, and cannot be covered by my book. How can one proceed further?","['integrating-factor', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
4511557,$n$-th derivative of inverse of a function wrt the function.,"I wanted to write the taylor series of the inverse of a bijective function around a point $y_0$ with the coefficients being in terms of the $\underline{\text{ derivatives of the function itself  }}$ at the point $f(y=y_0)=x_0$ , i.e: $$f^{-1}(y)=\sum_{n\geq 0}\left.\frac{d^nf^{-1}(y)}{dy^n}\right|_{y=y_0}\frac{(y-y_0)^n}{n!}\\
=\sum_{n\geq 0}o_n\left(\frac{dy}{df^{-1}(y)},\ldots,\frac{d^ny}{d(f^{-1}(y))^n}\right)_{f^{-1}(y=y_0)=x_0}\frac{(y-y_0)^n}{n!}$$ This helps me find the coefficients of the taylor series of an inverse function directly if I know the coefficients of the taylor series itself. And thus, I can invert a function easily. Moving forward, I will use $y$ for the function and $x$ for its inverse and let $y_n$ be $\frac{d^ny}{dx^n}$ and the same for $x_n$ . We know: $$y_1=\frac{1}{x_1}$$ Differentiating both sides wrt $y$ : $$\frac{dy_1}{dy}=-\frac{x_2}{x_1^2}$$ Then, multiplying both sides by $y_1$ and using the chain rule: $$\frac{dy_1}{dy}y_1=y_2=-\frac{x_2}{x_1^3}$$ You can imagine how deducing the $n$ -th derivative this way will get easily chaotic. So, instead I decided to take the $n$ -th derivative of the equation $y_1x_1=1$ wrt $y$ then look for a pattern. Taking the first derivative this way goes as follows: $$y_1x_1=1\\
(y_1)_yx_1+y_1x_2=0\\
y_2x_1+y_1^2x_2=0$$ The first four derivatives go as follows: $$y_1x_1=1\\
y_2x_1+y_1^2x_2=0\\
y_3x_1+3y_1y_2x_2+y_1^3x_3=0\\
y_4x_1+4y_1y_3x_2+3y_2^2x_2+3y_1^2y_2x_3+y_1^4x_4=0$$ I have this expression for the pattern that is easy to prove by induction: $$\frac{d^n}{dy^n}(x_1y_1)=\sum_{o_1p_1+o_2p_2=n}\left(n-\left|o_1-o_2\right|\right)y_{o_1}^{p_1}y_{o_2}^{p_2}x_{p_1+p_2}$$ However, I don't know how to use this to deduce the $n$ -th derivative in terms of $x_k$ 's only.","['inverse-function', 'inverse', 'taylor-expansion', 'power-series', 'derivatives']"
4511560,Implicit Differentiation - $x^my^n = (x+y)^{m+n}$,"Use implicit differentiation to find $\frac{\mbox{dy}}{\mbox{dx}}$ if $$x^my^n = (x+y)^{m+n}$$ Differentiation both sides with respect to $x$ : $$mx^{m-1}y^n + x^mny^{n-1}y' =(m+n)(x+y)^{m+n-1}(1 + y')$$ $$y' = \frac{(m+n)(x+y)^{m+n-1} - mx^{m-1}y^n}{x^mny^{n-1}-(m+n)(x+y)^{m+n-1}}$$ $$y'= \frac{nxy-my^2}{nx^2-mxy}$$ after using given $x^my^n = (x+y)^{m+n}$ . The answer given to me is $\frac{y}{x}$ . So, it seems $y'$ can be simplified even more. How do we do this? Thanks","['real-analysis', 'calculus', 'implicit-differentiation', 'limits', 'derivatives']"
4511561,Smoothing issues in $\left( \int_0^1 f(x) dx \right) \left( \int_0^1 \frac{1}{f(x)} dx \right) \le \frac{(m+M)^2}{4mM}$,"Let $f: [0,1] \to [m, M]$ where $m,M\in \mathbb{R}^+$ . Prove that $$\left( \int_0^1 f(x) dx \right) \left( \int_0^1 \frac{1}{f(x)} dx \right) \le \frac{(m+M)^2}{4mM}.$$ I know a rigorous solution now: it is \begin{align*}
	\left( \int_0^1 f(x)\, \mathrm{d}x \right) \left( \int_0^1 \frac{1}{f(x)}\, \mathrm{d} x \right)
	&= \frac{M}{m} \left( \int_0^1 \frac{f(x)}{M}\, \mathrm{d}x \right) \left( \int_0^1 \frac{m}{f(x)}\, \mathrm{d} x \right)\\
	&\le \frac{M}{m}\cdot \frac14\left(\int_0^1 \frac{f(x)}{M}\, \mathrm{d}x + \int_0^1 \frac{m}{f(x)}\, \mathrm{d} x\right)^2\\
	&= \frac{M}{m}\cdot \frac14\left(\int_0^1 \left(\frac{f(x)}{M} + \frac{m}{f(x)}\right)\, \mathrm{d}x\right)^2\\
	&\le \frac{M}{m}\cdot \frac14\left(\int_0^1 \left(1 + \frac{m}{M}\right)\, \mathrm{d}x\right)^2\\
	&= \frac{(m + M)^2}{4mM}
\end{align*} where we use $\frac{u}{M} + \frac{m}{u}
\le 1 + \frac{m}{M}$ for all $u\in [m, M]$ ( Note : $1 + \frac{m}{M} - \frac{u}{M} - \frac{m}{u} = \frac{(u - m)(M - u)}{Mu}$ ). But I wonder what is wrong with the following solution (which works if I replace $\int_0^1 f(x) dx$ by $\frac{a_1+\cdots+a_n}{n}$ where $a_i\in [m,M]$ , which is a discrete version of the ineq) One can show that $g(x)=ax+b/x$ is maximized when $x\in \{m,M\}$ so we can assume $f(j)\in \{m,M\}$ . Let $h(x)=\frac{f(x)-m}{M-m}$ and $k=\int_0^1 h(x) dx$ Then $\int_0^1 f(x)dx = m + (M-m)\int_0^1 h(x)dx = m+(M-m)k$ and $\int_0^1 f(x)^{-1} dx = (1-k)/m + k/M$ and we can see their product is maximized when $k=\frac 12$ . I think it has issues but I'm not sure where.","['integration', 'analysis', 'real-analysis', 'calculus', 'inequality']"
4511562,Variational Calculus - Derivation of Lagrangian Equation,"While learning about the calculus of variations to look at the principle of least action, we arrive at a point where we want to minimise the following functional (or of similar form): $$S(y(x),y'(x),x) = \int_{x_{1}}^{x_{2}} \sqrt{1+\left(\frac{dy}{dx}\right)^2} \, dx$$ We start of by creating a 'corrected' function of the following form: $\Upsilon = y(x) + \eta(x)$ such that $\eta(x_{1}) = \eta(x_{2}) = 0.$ Whilst doing so, we say that $\eta(x)$ is continuous and differentiable . I don't understand why $\eta(x)$ needs to be differentiable. Isn't it just a function that allows us to look at all the possible paths from $x_{1}$ to $x_{2}$ ? Why do these paths need to be differentiable? Why can't these paths have corners?","['multivariable-calculus', 'calculus', 'variational-analysis']"
4511567,Interchanging random variables with the same distribution,"Let $X, W, Y, Z$ be $\mathbb{R}$ -valued random variables on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ such that $Y, Z, W$ are i.i.d. and $\sigma(X)$ is independent of $\sigma(W, Y, Z)$ . Let $f : \mathbb{R}^2 \rightarrow \mathbb{R}$ be a measurable function and $B \in \mathcal{B}(\mathbb{R})$ . Is it true that $$
\mathbb{P} (f(X, Y) \in B, f(X, Z) \in B) = \mathbb{P}(f(X, Y) \in B, f(X, W) \in B).
$$ The probability on the left-hand side depends on the joint law of $X$ , $Y$ , $Z$ , which is given by the joint law of $X$ and $(Y, Z)$ . Since $X$ is independent of $(Y, Z)$ , the joint law is given by the product of the two laws. But $(Y, Z)$ has the same law as $(Y, W)$ , and so the joint law of $X, Y, Z$ is the same as that of $X, Y, W$ . Can this be used to show the equality?","['measure-theory', 'probability-theory']"
4511595,A clarification regarding maximal linearly ordered subsets and the Hausdorff maximal principle,"I am currently beginning to read Folland after finishing chapters 1 - 6 in baby Rudin. I am a little confused about the section on partially ordered sets. Firstly, I think I know the answer to this, but if $x \leq y$ is false, then that doesn't necessarily imply $y < x$ i.e. $y < x$ isn't necessarily the negation of $x \leq y$ . Clearly, this will imply in the Real numbers, which I am used to. Also, I am a bit confused about the Hausdorff maximal principle, which is ""Every partially ordered set has a maximal linearly ordered subset"". When it's saying maximal linearly ordered subset does that mean that it has an element that is maximal to the entire set or specifically for the subset?","['order-theory', 'set-theory', 'analysis']"
4511612,Help with a limit of a certain differential equation.,"I have an ODE $y’+a(t)y=b(t)$ with initial condition $y(0)=0$ . The functions $a(t),b(t)$ are continuous and each of them satisfy $$\lim_{t\rightarrow \infty}a(t)=A>0.$$ And $$\lim_{t\rightarrow \infty}b(t)=B>0.$$ When I graph $y$ , I see that $$\lim_{t\rightarrow \infty}y(t)=B/A.$$ .m, but I don’t know how to prove it analytically. Maybe it’s simple and I am overcomplicating. If I take the limit as $t\rightarrow \infty$ , then the ODE becomes $y’+Ay=B$ and intuitively, the conclusion is clear but I’m not sure if I can do such thing. Any help is appreciated!","['limits', 'ordinary-differential-equations', 'real-analysis']"
4511623,"Exercise 10, Section 3.4 of Hoffman’s Linear Algebra","We have seen that the linear operator $T$ on $R^2$ defined by $T(x_1, x_2) = (x_1, 0)$ is represented in the standard ordered basis by the matrix $A=\begin{bmatrix} 1 & 0\\ 0 & 0\\ \end{bmatrix}$ . This operator satisfies $T^2 = T$ . Prove that if $S$ is a linear operator on $R^2$ such that $S^2=S$ , then $S=0$ , or $S=I$ , or there is an ordered basis $B$ for $R^2$ such that $[S]_B = A$ (above). There are lots of way to prove this problem. I’m trying to show, if $S\neq 0$ and $S\neq I$ , then $\exists B$ ordered basis of $R^2$ such that $[S]_B=A$ . So $\exists \alpha_1,\alpha_2\in \Bbb{R}^2$ such that $S(\alpha_1)\neq (0,0)$ and $S(\alpha_2)\neq \alpha_2$ . How to progress from here?","['matrices', 'proof-writing', 'linear-algebra', 'linear-transformations']"
4511626,What justifies interchanging partial derivatives here?,"I am following a text on turbulence by Frisch. Using the summation convention, he writes the Navier-Stokes equations as $$\partial_t v_i + v_j\partial_j v_i = - \partial_i p + \nu \partial_{jj}v_i\\
\partial_iv_i = 0.$$ where $\nu$ is a constant. He claims that taking the divergence above results in a Poisson equation. Taking the divergence results in : $$\partial_i \partial_t v_i + \partial_i v_j\partial_j v_i = -\partial_i \partial_i p + \partial_i \nu \partial_{jj}v_i.$$ Assuming we may interchange partial derivatives, we have $$\partial_t \partial_i v_i + \partial_i v_j\partial_j v_i = -\partial_i \partial_i p +  \nu \partial_{jj}\partial_iv_i.$$ By the divergence free condition the first and last terms vanish, leaving $$\partial_i v_j\partial_j v_i = -\partial_i \partial_i p \\ \implies\partial_{ij}(v_iv_j) = -\partial_{ii}p.$$ The question I have now is what justifies interchanging the partial derivatives as we did? Also as an aside, I am still grasping the summation notation. How can we think of the term $\partial_{ij}(v_iv_j)$ ? Since the indices are not repeated adjacently, is it not summed?","['real-analysis', 'partial-differential-equations', 'partial-derivative', 'derivatives', 'fluid-dynamics']"
4511641,Does derivative of antiderivative always equal the integrand function?,"For some reals $a<b$ we consider $F:(a,b)\to\mathbb{R}$ where $F(x):=\int\limits_{a}^xf(t)dt$ and assume that $F$ is continously differentiable. (We don't know if $f$ is continuous) Can we conclude that $F'(x)=f(x)$ for all $x\in (a,b)$ ? For some $x_0\in(a,b)$ we choose $$f(t)=\begin{cases}0, & t\in(a,b)\setminus\{x_0\}\\2,& t=x_0.\end{cases}$$ and $g(t):=\hat{0}$ . Due to properties of the Riemann integral $$\int\limits_a^{x_0}f(t)dt=\int\limits_a^{x_0}g(t)dt=0.$$ Then \begin{align*}
&\left|\frac{\int\limits_{a}^xf(t)~dt-\int\limits_{a}^xf(t)~dt}{x_0-x}\right|=\left|\frac{\int\limits_{x_0}^xf(t)~dt}{x_0-x}\right|=\left|\frac{\int\limits_{x_0}^xg(t)~dt}{x_0-x}\right|=0,\text{ where }x\neq x_0\\
&\implies F'(x_0)=0.
\end{align*} As $x_0$ was chosen arbitrarily $F'(x)=0$ for all $x\in(a,b)$ . Hence $F'$ is continuous but $F'\neq f$ . Is this correct?","['integration', 'riemann-integration', 'derivatives', 'real-analysis']"
4511651,"Point-set topology in Theorem 6.15 of Le Gall: Brownian Motion, Martingales, and Stochastic Calculus","I am trying to understand the proof of Theorem 6.15 in Le Gall's book Brownian Motion, Martingales, and Stochastic Calculus about the regularity of sample paths of Markov processes (that they have a càdlàg modification) which is also Markov with respect to the completed filtration. However, I am having difficulty understanding some separability statements related to point-set topology. The topological space $E$ under consideration is assumed to be metrizable, locally compact, and $\sigma$ -compact. In a step of the proof, Le Gall states that we can find a sequence of nonnegative bounded functions (let the space denoted by $C^+_0(E)$ ) which separates the points of $E_{\Delta}$ , which is the Alexandroff compactification of $E$ . He then states that the subset $\mathcal{H} = \{R_pf_n: p \geq 1, n \geq 0\}$ is a countable subset of $C^+_0(E)$ and separates the points of $E_{\Delta}$ , where $R_p$ is the resolvent operator $R_pf(x) = \int_{0}^{\infty} e^{pt} Q_t f(x) dt$ , and $(Q_t)_{t \geq 0}$ is a transitional semigroup (in this theorem a Feller semigroup). I can understand why there exists such a sequence (because $E$ is metrizable thus normal, and appeal to the Urysohn lemma), I can also understand why $\mathcal{H}$ separates the points of $E_{\Delta}$ , but I am having some trouble seeing why such a set indexed by positive real numbers $p$ is countable. My question is: why is $\mathcal H$ countable? Or am I understanding something incorrectly about the definitions? For your reference, here is a screenshot of the theorem and the part of the proof I have problems with:","['stochastic-processes', 'markov-process', 'general-topology', 'probability-theory', 'stochastic-calculus']"
4511661,"Solution Verification: An equivalence relation on finite subsets of $\mathbb{N}$, defined by having equal sums of the elements in the sets","Let $E$ be the set whose elements are the finite parts (subset) of $\mathbb{N}$ . We define the binary relation $S$ on $E$ by posing $\forall A \in E, B \in E ,ASB $ if and only if $\sum\limits_{a\in A}a =\sum\limits_{b\in B}b $ Questions: Give two examples of set couples $A$ and $B$ such as $ASB$ . Show that the binary relation $S$ is an equivalence relation. My resolution: Suppose that: $A=(1+3+2)$ and $B=(2+2+2)$ that satisfies the relation S which implies $ASB$ . 2.So as we all know, according to the definition of equivalent relation the elements of the set $E$ must satisfy the next properties such as reflexivity, symmetricity and transitivity: Reflexivity: Let $\forall a \in A$ and $A \in\mathbb{N} $ then $a_1+a_2+a_3... = a_1+a_2+a_3...$ Hence $ASA$ Symmetricity: $\forall a \in A$ , $\forall b \in B$ and $A,B \in\mathbb{N}$ also we suppose that $ASB$ which means $a_1+a_2+a_3... = b_1+b_2+b_3...$ then $ASB \Longrightarrow a_1+a_2+a_3... = b_1+b_2+b_3...$ we also know  that ""="" possesses the commutative property then we have $b_1+b_2+b_3... = a_1+a_2+a_3... \Longrightarrow BSA $ Transitivity: $\forall a \in A$ , $\forall b \in B,\forall c \in C $ and $A,B,C \in\mathbb{N}$ .Also we suppose that $ASB$ and $BSC$ which means $a_1+a_2+a_3... = b_1+b_2+b_3...$ and $b_1+b_2+b_3... = c_1+c_2+c_3...$ then we obtain: \begin{equation}
\left\{ \begin{aligned} 
  a_1+a_2+a_3... = b_1+b_2+b_3\ldots\\ 
  b_1+b_2+b_3... = c_1+c_2+c_3\ldots
\end{aligned} \right.       
\end{equation} $\Longrightarrow (a_1+a_2+a_3...)+(b_1+b_2+b_3...)=(b_1+b_2+b_3...)+(c_1+c_2+c_3...)$ $\Longrightarrow (a_1+a_2+a_3...)=(c_1+c_2+c_3...)$ $\Longrightarrow ASC$ I have written my own resolution to this problem, knowing that it contains some mistakes. For this reason I would like to get a correction. Thank you in advance!","['equivalence-relations', 'proof-writing', 'solution-verification', 'discrete-mathematics', 'elementary-set-theory']"
4511663,Homogeneity on Minkowski functional,"Let $ X $ be a vector space over a field $ \mathbb{K} = \mathbb{R}, \mathbb{C} $ and $ A \subseteq X $ convex and absorbing. Then $ \forall x, y \in X $ , $ \forall t > 0 $ $ \mu_A (x + y) \leq \mu_A (x) + \mu_A (y) $ $ \mu_A (tx) = t \mu_A (x) $ In Rudin's Functional Analysis, theorem 1.35, it states  that $ \mu_A $ is a seminorm if $ A $ is also balanced, so that one has to prove $ \forall \alpha \in \mathbb{C} $ $ \mu_A (\alpha x) = |\alpha| \mu_A (x) $ ; according to the book, it follows from 1. and 2., still I have no clue why. I suppose I may start with $ \mu_A (\alpha x) = \mu_A ((a+ib)x) \leq \mu_A (ax) + \mu_A (ibx) $ but I cannot go much further. Can anyone provide (a sketch of) a complete proof?","['proof-explanation', 'topological-vector-spaces', 'functional-analysis']"
4511676,Dot product between a vector and a matrix,"Suppose $v$ is a vector valued function. Then $\nabla v$ is a rank 2 tensor, i.e. a matrix. In the Navier-Stokes equations we see a term of the form $$(v \cdot \nabla) v.$$ At first I thought that the dot would indicate matrix multiplication, but as it's written, this does not work due to dimensions. How is one supposed to interpret/evaluate this term? Also, what does the divergence of this term look like, i.e. $\nabla\cdot\big((v \cdot \nabla) v\big)$ ? Does it follow the usual product rule?","['multivariable-calculus', 'tensors', 'linear-algebra', 'tensor-products']"
4511677,Is there a combinatorial interpretation for a sum that includes $(-1)^k$?,"There are a lot of combinatorial sums that one can prove with a combinatorial interpretation like double counting. For example \begin{align*}
\begin{array}{c}
\displaystyle{\sum_{k = 0}^n \binom{n}{k} = 2^n}\\
\text{The number of the subsets of $\{1, \ldots, n\}$}
\end{array}\\
\ \\
\hline
\end{align*} \begin{align*}
\begin{array}{c}
\displaystyle{\sum_{j = 0}^k \binom{m}{j}\binom{n}{k - j} = \binom{m + n}{k}}\\
\text{Choose $k$ balls from a bag that includes $m$ red balls and $n$ blue balls}
\end{array}\\
\ \\
\hline
\end{align*} \begin{align*}
\begin{array}{c}
\displaystyle{\sum_{k = 0}^n k\binom{n}{k}^2} = n\binom{2n-1}{n-1}\\
\text{Create a group of $n$ students with a boy as the leader from $n$ girls and $n$ boys}
\end{array}\\
\ \\
\hline
\end{align*} \begin{align*}
\begin{array}{c}
\displaystyle{\sum_\limits{k = 1}^n \frac{1}{2k - 1}\binom{2k}{k}\binom{2n - 2k}{n - k} = \binom{2n}{n}}\\
\text{The number of permutations of $n$ digits $0$ and $n$ digits $1$}
\end{array}\\
\ \\
\hline
\end{align*} And similar ones. But when there's a coefficient $(-1)^k$ or $(-1)^{n - k}$ in the sum, the problem is more complicated and there aren't many ideas for giving a solution. For examples \begin{align*}
&\text{(A)} && \sum_{k = 0}^n (-1)^{n-k}\binom{2n}{k}^2 = \binom{2n}{n}\\
&\text{(B)} && \sum_{k = 0}^n (-1)^{n - k}\binom{n}{k}\binom{mk}{n} = m^n\\
&\text{(C)} && \sum_{k = 0}^{\lfloor n/2 \rfloor} (-1)^k \binom{n - k}{k}2^{n - 2k} = n + 1
\end{align*} My Question Do you know any combinatorial interpretation for sums like $\text{(A), (B), or (C)}$ ? I know that for simpler identities like $\sum_{k = 0}^n (-1)^k\binom{n}{k} = 0$ , by considering odd and even subsets we can prove the identity, but I couldn't apply the same idea to more complicated identities. Also, I think that the Inclusion-Exclusion Principle that has an alternating sum in its formula could be used here, but how? Any idea and combinatorial approach for the general situation or especially, for $\text{(A), (B), or (C)}$ will appreciate. Note that I don't want to use other methods like generating functions, WZ pairs, or binomial expansion . Thanks.","['summation', 'combinatorial-proofs', 'inclusion-exclusion', 'binomial-coefficients', 'combinatorics']"
4511708,"If $f(1)=c$ and $f(x+1) = f(x) + f(1)$, then is $f(x + y) = f(x) + f(y)$?","Suppose that $f: \mathbb{R} \to \mathbb{R}$ and let $c \in \mathbb{R} \backslash \{0\}$ be fixed such that: $f(1) = c$ $f(x+1) = f(x) + f(1)$ , for all $x \in \mathbb{R}$ I proved the following properties: For any $m \in \mathbb{Z}$ and any $x \in \mathbb{R}$ , $$f(x + m) = f(x) + f(m) = f(x) + cm$$ $f$ is well-defined. It seems that $f$ satisfy: For all $x,y \in \mathbb{R}$ , $f(x+y) = f(x) + f(y)$ but I dont think its true, but if I can show that property, maybe $f(x) = cx$ , for all $x \in \mathbb{R}$ . Maybe using $x = \lfloor x \rfloor - \text{frac}(x)$ will work, but it got me with this property: For any $x \in \mathbb{R}$ : $$f(x) - f(\text{frac}(x)) = f(x - \text{frac}(x))$$ Any hints? Thanks. EDIT I noticed that $f$ is not injective. My bad.",['functions']
4511726,What is the maximum area of this structure made out of triangles?,"If each of the $6$ fully drawn segments are of length $1$ m, what is the maximum area of the shape to the nearest cm $^2$ ? This problem is sourced from this contest document aimed at high school students . My first thoughts were to consider the left half of the figure, divide it in $2$ right triangles and a rectangle, and let the measures of the two bottom angles of the triangles be $A$ and $B$ . the area of the shape is then $\cos A \sin A +\cos B \sin B + 2 \sin A \cos B$ . From now there are multiple possibilities, including using sine and cosine formulas for sums or even using the Cauchy Schwartz inequality, but I found no success. I'm looking for any solution even if they involve Calculus. Answer for reference is $24 142$ cm $^2$","['trigonometry', 'area']"
4511738,"Probability of winning with $\{1,\dots,9\}$ vs $\{1,\dots,8\}$","Assume A and B have access to the set $\{1,\dots,9\}$ and $\{1,\dots,8\}$ , respectively. They choose three numbers from each's set without replacement and form the largest 3-digit number accordingly. For example, 4,2,5 means 542. What's the probability for A getting a larger number than B? My attempt: $P(A\ win) = P(A\ win|A\ with\ 9)P(A\ with\ 9) + P(A\ win|A\ without\ 9)P(A\ without\ 9)=1\times 3/9 + 1/2\times6/9 = 2/3$ . I am not sure if the above works. Specially, it seems $P(A\ win|A\ without\ 9)$ is not $1/2$ given the possibility of tie. Any hint and suggestion? Thanks a lot!",['probability']
4511766,"Strange remark in a solution: Recursion, definite integral and a duplication formula?","I was doing some problems in Arthur Engel's ""Problem-Solving Strategies"" when I came across this problem in the Induction section: Find a closed formula for the sequence $a_1 = 1,$ $$a_{n+1} = \frac{1}{16}(4a_n + 1 + \sqrt{24a_n + 1}).$$ Now the problem itself is not so interesting, but what I found interesting was a comment by the author at the end of his solution. He leaves the following remark: ""The sequence $a_n$ converges to $\int_0^1 x^2dx$ . The recursion is a ""duplication formula"" for the parabola $y = x^2$ . This is the way I discovered it."" Obviously the statement that the sequence converges to the same value as the integral ( $1/3$ ) is not hard to verify. But I am at a loss as to how the recursion formula is in any way related to the parabola $y=x^2$ and what the author means by ""duplication"" formula in this case. Typically I would think of a duplication formula being an expression for $f(2x)$ in terms of $f(x)$ , which in the case of the stated parabola would simply lead to the recursion $b_{n+1} = 4b_n$ . It is not clear to me how this could be reflected in a definite integral. Any insight would be greatly appreciated as I am curious how the author connected this recursive sequence to the definite integral.",['sequences-and-series']
4511773,Evaluating $\int_0^\pi x\frac{\sin{\frac{x}{2}} - \cos{\frac{x}{2}}}{\sqrt{\sin{x}}} dx$,"How am I supposed to solve the following definite integral? $$
\mathcal{I} = \int_0^\pi x \cdot \frac{\sin{\frac{x}{2}} - \cos{\frac{x}{2}}}{\sqrt{\sin{x}}} dx
$$ This definite integral is solved if the minus sign is replaced by a plus sign , and it yields $\pi^2$ . $$
\mathcal{I} = \int_0^\pi x \cdot \frac{\sin{\frac{x}{2}} + \cos{\frac{x}{2}}}{\sqrt{\sin{x}}} dx \text{ 
— (I)} \\
\implies \mathcal{I} = \int_0^\pi (\pi - x) \cdot \frac{\cos{\frac{x}{2} + \sin{\frac{x}{2}}}}{\sqrt{\sin{x}}} dx \text{ 
— (II)}
$$ On (I) + (II), we have, $$
\mathcal{I} = \frac{\pi}{2}\int_0^\pi \frac{\sin{\frac{x}{2}} + \cos{\frac{x}{2}}}{\sqrt{\sin{x}}} dx = \frac{\pi}{2} \int_0^\pi \frac{\sin{\frac{x}{2}}+\cos{\frac{x}{2}}}{\sqrt{1 - (\sin{\frac{x}{2}-\cos{\frac{x}{2}}})^2}} dx
$$ On substitution, $$
\sin{\frac{x}{2}} - \cos{\frac{x}{2}} = u \implies \left(\sin{\frac{x}{2}} + \cos{\frac{x} {2}}\right) dx = 2 \cdot du
$$ The upper and lower limits changes to 1 and -1. Now, we have $$
\mathcal{I} = \frac{\pi}{2} \int_{-1}^1 \frac{2 \cdot du}{\sqrt{1 - u^2}} du = \pi \cdot \left[\arcsin{u}\right]_{-1}^1 = \pi^2
$$ But... The sign was not supposed to be changed. We get $(2x - \pi)$ instead of $\pi$ in the nominator when adding both integrals. It complicates the problem. Using integral-calculator.com or a scientific calculator is helpless. The answer to the original problem should be $2\pi \cdot \ln{2}$ (approx 4.35.)","['integration', 'calculus', 'definite-integrals']"
4511817,solutions in upper half plane for polynomial-argument theorem,"Given $P(z)=z^5-12z^2+14$ ,
prove that there exist 2 solutions to $P(z)=0 $ in $\{Im(z)\ge 0 \}$ I tried using the argument theorem that states $\oint_{\gamma}\frac{p'\left(z\right)}{p\left(z\right)}dz=2\pi i\left(N-P\right) $ . Obviously $P=0 $ because it is a polynomial, I need to prove that $N=2$ . I looked at $$ \begin{cases}
\gamma_{1}=t, & t\in\left[-R,R\right]\\
\gamma_{2}=Re^{it} & t\in\left[0,\pi\right]
\end{cases} $$ and $\gamma =\gamma_1 \cup \gamma_2 $ . This made the 2 integrals $$ \oint_{\gamma}\frac{p'\left(z\right)}{p\left(z\right)}dz=\int\limits _{-R}^{R}\frac{5t^{4}-24t}{t^{5}-12t^{2}+14}dt+\int\limits _{0}^{\pi}\frac{R^{4}e^{4it}-24Re^{it}}{R^{5}e^{5it}-12R^{2}e^{2it}+14}\cdot iRe^{it}dt $$ $$ \int\limits _{-R}^{R}\frac{5t^{4}-24t}{t^{5}-12t^{2}+14}dt=\ln\left|\frac{R^{5}-12R^{2}+14}{-R^{5}-12R^{2}+14}\right|\xrightarrow{R\to\infty}0 $$ The problem is evaluating $$ \int\limits _{0}^{\pi}\frac{5R^{4}e^{4it}-24Re^{it}}{R^{5}e^{5it}-12R^{2}e^{2it}+14}\cdot iRe^{it}dt\approx\int\limits _{0}^{\pi}\frac{5iR^{5}e^{5it}}{R^{5}e^{5it}}\cdot dt=5\pi i $$ .
I used a very loose approximation because I have no idea how to actually integrate that, and this gives me $N=2.5 $ , which I think is wrong,I need to get an integer.
Where did I go wrong?",['complex-analysis']
4511830,"If every orthogonal projection of a manifold $M \subset \mathbb R^3$ on a plane is a disk, then $M$ is a sphere?","Given  a 2-d manifold $M$ in $\mathbb R^3$ and a plane $\alpha$ in $\mathbb R^3$ . Assume that $M$ is smooth and has convex, simply-connected inner part. If we project $M$ orthogonally onto $\alpha$ we always get a round disk (maybe in different diameter as $\alpha$ changes), can we prove or disprove that $M$ is a sphere?","['spheres', 'submanifold', 'differential-geometry']"
4511844,"If $\frac{x}{y}+\frac{y}{x}\ge2$ and $\sum_{cyc}\frac{x}{y+z}\ge\frac32$, can we say $\sum_{cyc}\frac{w}{x+y+z}\ge\frac43$?","We know that $$\frac{x}{y}+\frac{y}{x}\ge2$$ and $$\frac{x}{y+z}+\frac{y}{x+z}+\frac{z}{y+x}\ge\frac32$$ Can we say that $$\frac{w}{x+y+z}+\frac{x}{w+y+z}+\frac{y}{x+w+z}+\frac{z}{x+y+w}\ge\frac43$$ Or in general, where there are $n$ variables can we say that their sum (in the same format as above) is no less than $\frac{n}{n-1}$$?$ All the variables used here are positive real numbers. My thoughts: I think that we might need induction here, and then we can try completing the squares to find the minimum value. Any help is greatly appreciated.","['algebra-precalculus', 'inequality']"
4511861,Taylor Expansion of a function that maps matrices to scalars,"Consider first-order taylor approximations For $f: \mathbb{R} \to \mathbb{R}$ we have $f(\tilde x) \approx f(x) + (\tilde x - x)f'(x)$ For $f: \mathbb{R}^{n} \to \mathbb{R}$ we have $f(\mathbf{\tilde{x}}) \approx f(\mathbf
{x}) + (\mathbf{\tilde{x}} - \mathbf{x})^T\nabla_xf(\mathbf{x})$ What shall be the corresponding expansion for a $f: \mathbb{R}^{n\times m} \to \mathbb{R}$ ? $f(\mathbf{\tilde{X}}) \approx f(\mathbf
{X}) + ?$","['matrices', 'calculus', 'linear-algebra', 'taylor-expansion', 'derivatives']"
4511868,gradient of KL-Divergence,"Let $X$ be a finite set and $A$ be a set of probability distributions. Then KL-Divergence between two probability distributions $P(x)$ and $Q(x)$ $\in$ A  is $$D(P(x)\vert \vert Q(x))=\sum P(x)\operatorname{log}\left(\frac{P(x)}{Q(x)}\right)$$ for all x $\in$ X. Since KL-divergence is the distance between two probability distributions, therefore for fixed $Q(x)$ we can talk about its derivative with respect to $P(x).$ Can anyone tell what will be its gradient w.r.t $P(x)$ ?","['statistics', 'probability', 'information-theory']"
4511879,"Symbol of $d^*$, the adjoint of the exterior derivative $d: \Omega^k(M) \to \Omega^{k+1}(M)$","In my Global Analysis course we are studying the symbols of differential operator. We did the example of the Laplacian $\Delta = dd^* + d^*d$ but there is something I do not really understand. Let me explain. For $d: \Omega^k(M) \to \Omega^{k+1}(M)$ it is not too hard to show that, for $\xi \in T^*M$ and $\alpha \in \Lambda^kT^*M$ we have that $$\sigma(d)(\xi)\alpha = i \xi \wedge \alpha \in \Lambda^{k+1}T^*M,$$ where $\sigma(d)$ stands for the symbol of the operator $d$ . Now we know that $$\sigma(d^*)(\xi) = \sigma(d)^*(\xi):\Lambda^{k+1}T^*M \to \Lambda^{k}T^*M$$ for $\sigma(d)^*$ the hermitian dual of $\sigma(d)$ . From there my teacher deduced that, for $\omega \in \Lambda^{k+1}T^*M$ , $$\sigma(d)^*(\xi)\omega = -i \iota_{\xi_\sharp}(\omega)$$ where $\xi_\sharp\in TM$ is the metric dual of $\xi$ , i.e. $$g(\xi_\sharp, v) = \xi(v), \quad \forall v \in TM$$ and $\iota_{\xi_\sharp}(\omega)$ is the contraction $$\iota_{\xi_\sharp}(\omega) = \omega(\xi_\sharp, \cdot, \ldots, \cdot).$$ Therefore, we can compute the symbol of $\Delta$ as \begin{align}
\sigma(\Delta)(\xi)\alpha &= \sigma(d) \circ \sigma(d^*)(\xi)\alpha + \sigma(d^*) \circ \sigma(d)(\xi)\alpha\\
&= \xi \wedge \iota_{\xi_\sharp}(\alpha) + \iota_{\xi_\sharp}(\xi \wedge \alpha)\\
&= \xi \wedge \iota_{\xi_\sharp}(\alpha) + |\xi|^2 \alpha - \xi \wedge \iota_{\xi_\sharp}(\alpha)\\
&= |\xi|^2\alpha
\end{align} so that $\sigma(\Delta)(\xi) = |\xi|^2 \text{Id}_{\Lambda^{k}T^*M}.$ Now the thing is that I really don't understand how to deduce the form of $\sigma(d)^*(\xi)$ , could one of you explain how  he got this ?","['laplacian', 'global-analysis', 'differential-geometry']"
4511908,Construct a metric that induces a given topology,"In a topology textbook there was a exercise to determine the topology induced by $$x^2:\mathbb{R}\to\mathbb{R}$$ where the target has the euclidean topology. I am the opinion that $x^2$ induced a kind of ""mirrored"" topology, meaning the open sets are all open sets of the euclidean topology that are symmetric at $0$ . However there was the bonus-question if this topology is induced by a metric. I figured out that this topology satisfy the second axiom of countability (take $B=\{U_{1/n}(x)\mid x\in\mathbb{Q}, n\in\mathbb{N}$ }, so we can take the same set like in the euclidean topology) and this ensures that this topology comes from a metric. If there are any errors I made please correct me. However, I now wondered if it would be possible not only to show that it comes from a metric, but also to explicitly give the metric that induces the topology? I tried some stuff out by unfortunately I was not able to do this, maybe anyone of you has an idea, or maybe it is not even possible? I would appreciate any answers on this","['metric-spaces', 'analysis', 'metrizability', 'functional-analysis', 'general-topology']"
4511910,Is there a positive integer $d$ in existence$?$,"Suppose there are three positive integers $a,b,c$ such that the product of
any two is one less than an integer squared, i.e., $$\begin{align}
ab+1=x^2\\
ac+1=y^2\\
bc+1=z^2
\end{align}$$ Then is it true that there exists a fourth positive integer $d$ such that its product with any
of the first three is also one less than a square, i.e. $$\begin{align}
ad+1=w^2\\
bd+1=u^2\\
cd+1=v^2
\end{align}$$ I tried factorizing the expressions but it did not prove to be of much help. Like we can say that $ab=(x+1)(x-1)$ and similarly for all expressions. Any help is greatly appreciated. EDIT A triplet satisfying $(a,b,c)$ is $(1,3,8)$ and the value of $d$ in this case is $120$ . As far as I know, this is an old and very interesting problem and many famous mathematicians had attacked this problem but I guess none of them gave a detailed proof or maybe I don't know about it.","['number-theory', 'elementary-number-theory', 'integers', 'square-numbers', 'quadratics']"
4511929,Generalisation of Bertrand's postulate: not just one but two primes,"Just a sunday's question. Bertrand's postulate states that for every $n>1$ , there is always at least one prime $p$ such that $n \lt p \lt 2n$ . Playing around with some small numbers suggests the question if there is possibly not just one but there are at least two primes in that interval. From the following numerical example we can see that the answer seems to be affirmative for $n\ge6$ \begin{array}{ccc}
n&\{n-1,2n-1\}&\text{list of primes }p\\
 1 & \{0,1\} & \{\} \\
 2 & \{1,3\} & \{3\} \\
 3 & \{2,5\} & \{5\} \\
 4 & \{3,7\} & \{5,7\} \\
 5 & \{4,9\} & \{7\} \\
 6 & \{5,11\} & \{7,11\} \\
 7 & \{6,13\} & \{11,13\} \\
 8 & \{7,15\} & \{11,13\} \\
 9 & \{8,17\} & \{11,13,17\} \\
 10 & \{9,19\} & \{11,13,17,19\} \\
\end{array} Question I would greatly appreciate some references for this most probably well known result. EDIT Here I add some plots which show the number of primes between $n+1$ and $2n-1$ over different ranges of $n$ . I have fitted the curves with the function $$b(n) = \frac{A n }{\log(1+B n)}$$ The parameters are shown in the plots. Discussion These graphs illustrate the proof of Erdös: ""for any $k$ and sufficiently large $n$ there are always at least $k$ primes between $n$ and $2n$ "", as was pointed out in a comment of @lulu and in the solution of @Thomas Preu The number $b(n)$ is not monotonous on a small scale. The shape of the fitted functions is just what was to be expected from the prime counting function $\pi(n)$ . Indeed we have exactly just $b(n) = \pi(2n) - \pi(n)$ .","['number-theory', 'prime-numbers']"
4511969,Can a sequence have infinitely many subsequences that are disjoint?,"Can a sequence have infinitely many subsequences that are disjoint? I wanted to prove the following, Let $\{x_n\}$ be a sequence of a metric space with no converging subsequences. Then for each point outside the sequence $\{x_n\}$ has an open neighborhood that has no intersection with $\{x_n\}$ . Here is how I proceeded. Let $\{ y_n \}$ be a subsequence and let $x$ be a point not in $x_n$ . Since $y_n$ is not a convergent sequence it does not converge to $x$ .
Therefore, for all $N>0$ , $\exists \epsilon>0$ such that $\{y_{n>N}\} \cap B(x,\epsilon) =
\emptyset$ .
Since metric space is $T_0$ space, for any point $y_k \in  \{y_{n}\} $ we can find $B(x,\epsilon_k)$ not containing $y_k$ . Now the set $ U_y= \cap_{k\le N} B(x,\epsilon_k) \cap B(x,\epsilon) $ is an open set(intersection of finite number of open sets) with $U_y \cap \{y_n\} =\emptyset$ . Now consider the set $\{x_n\} - \{y_n\}$ and let $\{z_n\}$ be a subsequence of $\{x_n\} - \{y_n\}$ . Then can find an open set $U_z$ such that $U_z \cap \{z_n\} =\emptyset$ . Assume $\{x_n\}$ can only have a finite number of disjoint subsequences. Then continue like that until we find finite number of points $S=\{x_n\} - \{y_n\}-\{z_n\} ...$ . Again using the $T_0$ property, for each point $s \in S$ , we can find an open ball around $x$ , $B(x,\epsilon_s)$ that does not contain the point $s$ . Therefore The set $U= \cap_{s\in S} B(x,\epsilon_s) \cap U_y \cap U_z \cap .. $ is a finite intersection of open sets that contain $x$ but does not contain $\{x_n\}$ . For the proof, I needed to use that there are only finitely many disjoint subsequences in $\{x_n\} $ which I think should not be an assumption. Any comments or thoughts are really appreciated.","['general-topology', 'real-analysis']"
4512006,Why does satisfying the countability conditions make topological spaces so nice?,"Motivation for the question: While I can appreciate the significance of Theorem 30.1 below, and also the Urysohn metrization theorem, I can't seem to understand why either of them is rooted in these countability axioms, as in what their essence is. Question: Could someone explain the essence of how having such countability axioms leads to such nice results? Relevant Definitions First countability: A space X is said to have a countable basis at $x$ if there is a countable collection $B$ of neighborhoods of $x$ such that each neighborhood of $x$ contains at least one of the elements of $B$ . A space that has a countable basis at each of its points is said to satisfy the first countability axiom. Second countability:  If a space $X$ has a countable basis for its topology, then $X$ is said to satisfy the second countability axiom. Theorem 30.1. (Chapter-4 Munkres's Topology) Let $X$ be a topological space. (a) Let $A$ be a subset of $X$ . If there is a sequence of points of $A$ converging to $x$ , then $x \in \bar{A}$ ; the converse holds if $X$ is first-countable. (b) Let $f: X \rightarrow Y$ . If $f$ is contınuous, then for every convergent sequence $x_{n} \rightarrow x$ in $X$ , the sequence $f\left(x_{n}\right)$ converges to $f(x)$ . The converse holds if $X$ is first countable. (Earlier on) Our goal is to prove the Urysohn metrization theorem, if a topological space $X$ satisfies certain countability axiom (the second) and a certain seperation axiom (the regularity axiom), then $X$ can be imbedded in a metric space and is this metrizable I am looking for something around the lines of the discussions in these questions: What should be the intuition when working with compactness? Why is a topology made up of 'open' sets? Edit: I found out that the intuition behind this question is discussed in Tai Danae Bradley's Categorical approach to topology chapter-3. So check that out if you want a resolution.","['second-countable', 'intuition', 'general-topology', 'first-countable', 'soft-question']"
4512036,A finitely presented module over a finitely presented algebra,"Let $R\rightarrow S$ be a finitely presented homomorphism of rings and $M$ a finitely presented $S$ -module. My question: if $M$ is finitely generated as an $R$ -module, then is $M$ also finitely presented over $R$ ? The special case when $M=S$ is [EGA IV1, 1.4.7]: any finite finitely presented algebra is finitely presented as a module.","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4512061,Equivalent definition of Differentiability of function of two variable. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I know the definition of Fréchet derivative of function between two normed space.and one can think it is a generalization of our usual definition of derivative of real function.But in a book i faced (equivalent) definition for real valued function of two variable.so can i have hint how can i show this definition is equivalent to Fréchet derivative :
I dont even know how to start as in definition which i faced they define two error term what can i do from Frechet definition is only 1 error term.so how to get another?",['multivariable-calculus']
4512068,Unit square integral $\int_{0}^{1} \int_{0}^{1} \left( - \frac{\ln(xy)}{1-xy} \right)^{m} dx dy $,"In an article by Guillera and Sondow, one of the unit square integral identities that is proved (on p. 9) is: $$\int_{0}^{1} \int_{0}^{1}  \frac{\left(-\ln(xy) \right)^{s}}{1-xy}  dx dy = \Gamma(s+2) \zeta(s+2),$$ which holds for $\mathfrak{R}(s) > -1 $ . Let's define $$I_{m} := \int_{0}^{1} \int_{0}^{1} \left(  \frac{-\ln(xy) }{1-xy} \right)^{m}  dx dy  $$ for $m \in \mathbb{Z}_{\geq 1} $ . Then WA finds $$I_{1} = -2 \zeta(3), \qquad I_{2} = 6 \zeta(3). $$ However, so far I haven't been able to find any closed-form evaluations for $I_{m}$ when $m \geq 3$ , neither in the literature nor with CAS software. Questions : What is $I_{m}$ for $m>2$ ? Does this family of definite integrals appear in the literature somewhere?","['integration', 'definite-integrals', 'analytic-number-theory', 'reference-request']"
4512106,Solving recurrence relation $a_n = a_{n-1} - a_{n-2}$,"I am given a sequence of determinants of matrices $M_n$ , where the matrix elements $(M_n)_{ij}$ of $M_n$ are $0$ whenever $|i-j|>1$ and $1$ whenever $|i-j| ≤ 1$ . Writing out the first five matrices, it becomes apparent that $\det(M_n) = \det(M_{n-1}) - \det(M_{n-2})$ . I want a formula for the mapping $n ↦ \det(M_n)$ , which I believe to be $$a_n = \begin{cases} 0, & n ≡ 2 \mod 6 \,\, \vee n ≡ 5 \mod 6, \\ 1, & n ≡ 0 \mod 6 \,\, \vee n ≡ 1 \mod 6, \\ -1, & n ≡ 3 \mod 6 \,\, \vee n ≡ 4 \mod 6. \end{cases}
$$ This can quite readily be seen from the first 15 or so terms. Of course, this doesn't constitute a proof , which most likely will have to be performed by induction. I just fear that I am to embark on a six-piece proof by exhaustion, which I would like to avoid if there is a (much) quicker way to do it!","['matrices', 'determinant', 'linear-algebra', 'recurrence-relations']"
4512108,Relating energy integral to the existence of a Frostman measure,"I am trying to follow the proof in Mattila's Fourier Analysis and Hausdorff Dimension (bottom of page 19) that given a measure $\mu$ compactly supported on a Borel set $A$ with $0<\mu(A)<\infty$ that has $$I_s(\mu):=\iint |x-y|^{-s}d\mu_xd\mu_y < \infty$$ we can construct a measure (also compactly supported on a Borel set $A$ , with $0<\mu(A)<\infty$ ) that satisfies the Frostman condition , that is, a measure $\nu$ with $$\nu(B(x,r))<r^s.$$ The book says that if $I_s(\mu)<\infty$ , then $\int |x-y|^{-s} d\mu_x < \infty$ $\mu$ -almost everywhere, which I follow. It also says we can find an $M \in \mathbb{R}^+$ such that $$A:=\left\{y: \int |x-y|^{-s} d \mu_x < M \right\}$$ has $\mu(A)>0$ , which also makes sense. But then it claims that $\nu(E) := \mu(E \cap A)$ has $$\nu(B(x,r))\leq 2^sMr^s,$$ and I don't understand where that comes from at all. Can someone explain this step to me?","['measure-theory', 'fourier-analysis', 'hausdorff-measure', 'geometry', 'geometric-measure-theory']"
4512114,Integral Inequality involving a binomial,"Let $\binom{t}{k}$ where $k\in \mathbb{N}$ be defined as $\frac{t(t-1)\cdots (t-k+1)}{k!}$ . Prove that $\int_n^{\infty} \binom{t-1}{n-1} e^{-t} dt \le \frac{1}{(e-1)^n}$ My progress is as follows: we can find a recursion. Let $F(n)$ be the sum in question, then $$F(n)=\int_n^{n+1} \binom{t-1}{n-1} e^{-t} dt + \int_{n+1}^{\infty} \binom{t-1}{n-1} e^{-t} dt$$ Using Pascal's identity $\binom{t-1}{n-1} = \binom{t-2}{n-1} + \binom{t-2}{n-2}$ , one may obtain the recursion that $$F(n)=\int_n^{n+1} \binom{t-1}{n-1} e^{-t} dt + \frac{F(n)+F(n-1)}{e}$$ $$\frac{e-1}{e} F(n)=\int_n^{n+1} \binom{t-1}{n-1} e^{-t} dt + \frac{F(n-1)}{e}$$ $$F(n)=\int_n^{n+1} \frac{e}{e-1}\binom{t-1}{n-1} e^{-t} dt + \frac{F(n-1)}{e-1}$$ Let $K(n)=F(n)(e-1)^n$ then $K(n)-K(n-1)=e(e-1)^{n-1} \int_n^{n+1} \binom{t-1}{n-1} e^{-t} dt$ So now it makes sense to show $\sum\limits_{n\ge 1} e(e-1)^{n-1} \int_n^{n+1} \binom{t-1}{n-1} e^{-t} dt$ converges to a constant at most 1. For convenience, we can write it as $$e\sum\limits_{n\ge 0} (e-1)^n \int_{n+1}^{n+2} \binom{t-1}{n} e^{-t} dt = \sum\limits_{n\ge 0} (e-1)^n \int_{n}^{n+1} \binom{t}{n} e^{-t} dt$$ Now I am stuck. If anyone has any ideas on a solution, please don't hesitate to share with us here!","['integration', 'inequality', 'binomial-coefficients', 'integral-inequality']"
4512142,The relation of the Bernoulli numbers to the Catalan numbers,"The Bernoulli numbers $B_n$ are the backbone of calculus, and according to B. Mazur, they ""act as a unifying force, holding together seemingly disparate fields of mathematics."" The Catalan numbers $C_n$ are the delight of the combinatorialist. The entry of these numbers is the longest in the OEIS, and according to N. Sloane, ""rightly so."" But what is the relationship between these extraordinary numbers? I found a formula that is amazingly simple and perhaps worthy of closer consideration: $$ \frac{B_n}{C_n} \, = \, \sum_{k=0}^{n} (-1)^{k} \frac{E(n, k)}{F(n,k)} $$ The $E(n,k)$ are the Eulerian numbers, GKP CM 6.35 and A173018 . The triangle $F(n, k)$ is the factorial counterpart of Leibniz's harmonic triangle. They are defined for $n \ge 0$ and $ 0 \le k \le n$ as $$ F(n,k) \, = \, \frac{ (n+1)^{ \overline{n} } }{ k! \, (n-k)! },$$ where $ n^{\overline{k} } $ denotes the rising factorial. (The Leibniz triangle is obtained by choosing the falling factorial instead). Notice: We set $B(1)=1/2$ , following Donald Knuth's new definition of the Bernoulli numbers. The sign has reversed, in Concrete Mathematics since the 34-th printing, Jan. 2022. Now the question arises, is there a simple proof of this identity? Postscript: The identity is only a particular case of $$\operatorname{\beta}_{n}(x) = \frac{1}{n+1}\, \sum_{k=0}^{n}  (-1)^{k}\frac{ \left\langle n\atop k \right\rangle}{\binom{n}{k}}\, x^{n-k} \quad(n \ge 0).$$ I dubbed these polynomials Eulberian polynomials because I don't know any other name, but I had the impression that they should have one. They provide only one of several representations of the Bernoulli numbers which go back to Julius Worpitzky's 1883 'Studien'. The paper suggested by Jean Marie in the comments provides a proof in a modern presentation. See also OEIS A342321 and A356601.","['catalan-numbers', 'combinatorics', 'bernoulli-numbers', 'sequences-and-series', 'eulerian-numbers']"
4512148,"In measure theory, what is an example of a set of not well-defined area in unit square in $\mathbb{R}^2$?","I am trying to understand the need of sigma-algebras in probability spaces. In https://stats.stackexchange.com/questions/199280/why-do-we-need-sigma-algebras-to-define-probability-spaces the following example is given Consider this example. Suppose you have a unit square in $\mathbb{R}^2$ , and you're interested in the probability of randomly
selecting a point that is a member of a specific set in the unit
square. In lots of circumstances, this can be readily answered based
on a comparison of areas of the different sets. For example, we can
draw some circles, measure their areas, and then take the probability
as the fraction of the square falling in the circle. Very simple. But what if the area of the set of interest is not well-defined? If the area is not well-defined, then we can reason to two different
but completely valid (in some sense) conclusions about what the area
is. So we could have $P(A)=1$ on the one hand and $P(A)=0$ on the
other hand, which implies $0=1$ . This breaks all of math beyond
repair. You can now prove $5<0$ and a number of other preposterous
things. Clearly this isn't too useful. What does it mean for a set $A$ to be not well-defined in this example? Also, How could we have both $P(A)=0$ and $P(A)=1$ at the same time?","['measure-theory', 'probability-theory', 'probability']"
4512180,Are there any non-constant differentiable functions $f : \mathbb R \to \mathbb R$ where each $t \in \mathbb R$ has $f(t)f(f'(t))=1$?,"Partition 1st differentiable $f : \mathbb R \to \mathbb R$ , by the number of $t \in \mathbb R$ where $f(t)f(f'(t))=1$ . My main question is Are there any non-constant functions where every $t \in \mathbb R$ satisfies the property? A few things I've noticed: I was able to generate all of the countable equivalence classes using $\text {exp, sin,}+,\circ,\cdot$ . The property is volatile with respect to adding a constant $c$ to a given function $f$ . I'm 99% sure $f$ cannot both be a finite-degree polynomial and have the property of the main question.","['calculus', 'derivatives', 'real-analysis']"
4512183,Proving a relation is a mapping,"In my abstract algebra book in the preliminary section there is a review of relations, mappings, etc. In the practice problems there are a few problems that check your understanding. Example 1: $$f(\frac{p}{q}) = \frac{3p}{3q}$$ This one is trivially a mapping because no matter what p, q you choose you will get a unique value from Q to Q. Example 2: $$f(\frac{p}{q}) = \frac{p + q}{q^2}$$ This one is not a mapping. A counter-example is $f(1/2) \neq f(2/4)$ . These both map to different values so this cannot be a mapping. But I am left kind of confused on the ""proof"" part. Showing a relation is not a mapping seems to be difficult because you have to search for a counter-example of a potentially hard problem. But proving something is a mapping seems extremely difficult. Is it possible to construct some kind of contradiction to prove it, or are you left sort of ""guessing and checking""? I am self learning at the moment so I don't really have a professor to ask this to.","['elementary-set-theory', 'abstract-algebra']"
4512206,Proving that $(e^T)^{-1} = e^{-T}$ using term-by-term multiplication of the series for $e^T e^{-T}$,"My question comes from Differential Dynamical Systems by Meiss, page 40. The author makes the following claim: If $T: E \rightarrow E$ is a bounded linear operator, then $$(e^T)^{-1} = e^{-T}.$$ The author does not prove the claim, he just writes ""(term-by-term multiplication of the series for $e^T e^{-T}$ )"" next to it. I tried to verify this using the Cauchy product formula as follows: \begin{align*}
    e^T e^{-T} &= \left(\sum_{n=0}^{\infty} \frac{T^n}{n!} \right) \left(\sum_{n=0}^{\infty} \frac{(-T)^n}{n!} \right) \\[5pt]
     &= \sum_{n=0}^{\infty} \sum_{k=0}^{n}   \frac{T^k}{k!} \frac{(-1)^{n-k}\, T^{n-k}}{(n-k)!} \\[5pt]
     &= \sum_{n=0}^{\infty} T^n \sum_{k=0}^{n}  \frac{(-1)^{n-k}}{k! (n-k)!} \ \\[5pt]
\end{align*} At this point, I don't know really know how to proceed to show that the result is the identity map. Any suggestions?","['operator-theory', 'linear-algebra', 'ordinary-differential-equations', 'analysis']"
4512213,"What is the probability that a 2D symmetric random walk will reach a point $(x,y)$ before returning to the origin?","Let $p_n(\vec{x})$ be the probability that a symmetric random walk (SRW) on $\mathbb{Z}^n$ starting at the origin reaches the point $\vec{x}\in\mathbb{Z}^n-\{0\}$ before returning to the origin. What is $p_2(\vec{x})$ ? Judging from this post , I fear the answer might be too complicated, but it's worth a shot. I've already been able to calculate $p_1(x)=\frac{1}{2|x|}$ as follows: Let $p_n(\vec{s},\vec{x})$ be the probability that a SRW on $\mathbb{Z}^n$ starting at $\vec{s}$ reaches the point $\vec{x}\in\mathbb{Z}^n-\{0\}$ before the origin. Lemma . $$p_n(\vec{s},\vec{x})+p_n(\vec{x}-\vec{s},\vec{x})=1\quad\text{for}\quad n=1,2\tag{1}$$ Proof . Firstly, a SRW for $n=1,2$ will reach each point with probability $1$ . Hence, we may assume WLOG that a SRW will either reach $0$ first, or $\vec{x}$ first. If $W$ is a walk, then $W+\vec{s}$ reaches $\vec{x}$ before $0$ iff $-W+\vec{x}-\vec{s}$ reaches $0$ before $\vec{x}$ . Hence, $p_n(\vec{s},\vec{x})=1-p_n(\vec{x}-\vec{s},\vec{x})$ . Rearranging gives the desired equality $(1)$ . \begin{multline}\shoveright\square
\end{multline} Theorem. $$p_1(x)=\frac{1}{2|x|}\tag{2}$$ Proof. Suppose $0<s<x$ . Then a SRW that reaches $x$ before $0$ will also reach $s$ before $0$ . Then, $$p_1(s)p_1(s,x)=p_1(x)\tag{3}$$ Then we may write $(1)$ as $$\frac{1}{p_1(s)}+\frac{1}{p_1(x-s)}=\frac{1}{p_1(x)}\tag{4}$$ Setting $s=1$ and using induction gives $$\frac{1}{p(x)}=\frac{x}{p_1(1)}\tag{5}$$ We can quickly check that $p_1(1)=\frac{1}{2}$ : either the first step in the walk is to $1$ , or the first step is to $-1$ , in which case the walk will have to return to the origin before reaching $1$ . Hence, $p_1(x)=\frac{1}{2x}$ . By symmetry, $p_1(-x)=p_1(x)$ , so we arrive at $(2)$ . \begin{multline}\shoveright\square
\end{multline} I've tried ways of extending this method to $n=2$ , but to no avail. The biggest problem is that there doesn't seem to be a nice analogue of $(3)$ for $n=2$ because a walk that reaches $\vec{x}$ does not necessarily pass through any other specific point. According to Wikipedia, $(2)$ can be derived from the fact that a SRW on $\mathbb{Z}^n$ is a martingale, but I know nothing of martingales so I do not know how to use this information to find a formula for $n=2$ . EDIT: I should have looked a little more closely at the post I linked. There is a link in the comments leading to this page , which shows that if we define $R_{n,m}$ to be the resistance between the origin and the point $(n,m)$ on $\mathbb{Z}^2$ where every edge has unit resistance, then $$R_{1,0} = \frac{1}{2}\\ R_{m,m}=\frac{2}{\pi}\sum_{k=1}^m\frac{1}{2k-1}\quad\quad m>0\\ 4R_{n,m} -R_{n-1,m}-R_{n+1,m}-R_{n,m-1}-R_{n,m+1}=0\quad\quad (n,m)\neq (0,0)$$ Using symmetry accross the $x$ -axis, $y$ -axis, and diagonals $y=\pm x$ , the above is enough to determine $R_{n,m}$ for every $n,m$ . Then, $$\boxed{p_2((n,m))=\frac{1}{4R_{n,m}}}$$ As a sanity check, the asymptotic from Yuval Peres's answer gives $$p_2((m,m))=\frac{\pi}{8}\left(\sum_{k=1}^m\frac{1}{2k-1}\right)^{-1}\sim \left(\frac{4}{\pi}\log(m)+\frac{4\gamma}{\pi}+\frac{8}{\pi}\log 2\right)^{-1}\quad\quad m>0$$ which is a tight approximation easily derivable from the fact that $H_{m-1}-\gamma =\psi(m) \sim \log m$ , and $\sum_{k=1}^m\frac{1}{2k-1}=H_{2m-1}-\frac{1}{2}H_{m-1}$ .","['stochastic-processes', 'martingales', 'random-walk', 'probability']"
4512218,Equivalent Cauchy sequences.,"Hi everyone I'm having a bad time with two questions in the Analysis book of Terry Tao. I finally finished one of the exercises and I'm wondering if the next reasoning is correct or maybe needs some changes: Definitions : Two sequence are equivalence $\iff$ $(\forall \varepsilon \in \mathbb{Q}^+\,) ( \, \exists N\in \mathbb{N}\,) \text{ s.t. }(\, \forall n \ge N\, (|a_n-b_n|\le \varepsilon)\,)  $ (where $\mathbb{Q}^+$ is a positive rational number). Exercise : Show that if $\langle a_n \rangle_{n=1}^{\infty}$ and $\langle b_n \rangle_{n=1}^{\infty}$ are equivalences secuences of rationals. Then  $\langle a_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence if and only if $\langle b_n \rangle_{n=1}^{\infty}$  is a Cauchy sequence. Proof: 
We suppose that $\langle a_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence, and  also we may assume that $\langle a_n \rangle_{n=1}^{\infty}$ and $\langle b_n \rangle_{n=1}^{\infty}$ are equivalent sequences; we wish to show that $\langle b_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence. Let $\, \varepsilon $ be  an arbitrary positive integer, we shall show that there is some $N \in \mathbb{N}$ such that $|b_j-b_k| \le \varepsilon$ for all $\,j,k \ge N$. Let $\gamma$ be a positive rational number such that $\gamma < \varepsilon$. Since $\langle a_n \rangle _{n = 1}^{\infty}$ is a Cauchy sequence it follows that there is some $N'\in \mathbb{N}$ such that $|a_{j'}-a_{k'} |\le \varepsilon-\gamma$ for all $j',k' \ge N'$. Now we set $|a_n-b_n|\le \frac{\gamma}{2}\,$  for all $n \ge M$. So either $M>N'$ or $\,M\le N'$ by the ordering of natural numbers. If $M>N'$ we choose $j',k'\ge M$ and then, we have that: $|b_{j'} -b_{k'}|-|a_{j'} -a_{k'}|\le|(a_{j'} -a_{k'})- (b_{j'} -b_{k'})| \le |a_{j'} -b_{j'}|+|a_{k'} -b_{k'}|\le \gamma$ And so we have that $|b_{j'} -b_{k'}|\le \gamma +|a_{j'} -a_{k'}| \le \varepsilon$, it follows that $|b_{j'} -b_{k'}|\le \varepsilon$ for all $j',k'\ge M$ as desired. On the other hand if $\,M \le N'$ we choose a $n\ge N'$ and a similar argument give us that $|b_{j'} -b_{k'}|\le \varepsilon$ for all $j',k'\ge N'$ as desired. So in either case, $|b_{j} -b_{k}|\le \varepsilon\,$ for all $\,j,k \ge N$ and hence the sequence $\langle b_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence. To conclude note that the converse may be disposed with no additional work, by applying the same argument with the roles of $\langle a_n \rangle_{n=1}^{\infty}$ and $\langle b_n \rangle_{n=1}^{\infty}$ interchanged, i.e., where $\langle b_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence. Thanks in advance as usual.","['self-learning', 'cauchy-sequences', 'proof-verification', 'real-analysis', 'calculus']"
4512227,Is there a topological point of view for Liouville's theorem about elliptic functions?,"Good evening! One of Liouville's theorems about elliptic functions states that there is no such function that has only one pole of order 1. This result is very well known and easily proven using the residue formula. My question is the following: Is there some topological reason/ intuition for this fact? For example, one can see the fact that $\int\limits_{\partial B_1(0)} f(z) d z \neq 0$ for some functions that are holomorphic in $G = \mathbb{C}\setminus\{0\}$ as a result of $G$ having non-vanishing first homology group (or plainly having one $0$ -dimensional hole). So my intuition would be that an elliptic function essentially lives on a torus (after identifying opposing sides of the fundamental lattice), whose first homology group is isomorphic to $\mathbb{Z}\times\mathbb{Z}$ ... however, I do not really see how to go from there.","['complex-analysis', 'homology-cohomology', 'de-rham-cohomology', 'algebraic-topology']"
4512264,Sample means and confidence intervals,"For a population, where its parameters are unknown, why is it that if we take samples, those samples create a distribution that resembles that of a Normal Distribution? What if the original population distribution is not normal, why don't the samples create a distribution that is similar to the population, because they should have similar features to the population?","['statistics', 'probability-distributions', 'normal-distribution', 'probability']"
4512280,Is there an example of a Ricci flat algebraic surface?,"Given a surface defined by a polynomial $f(x,y,z,...)=0$ in $\mathbb{R}^n$ are there any known to be Ricci-flat but not flat? I am trying to find a simple example with smallest possible polynomial. e.g. $x^2+y^2+z^2-1=0$ is not Ricci-flat as it is a sphere. $(x^2+y^2-1)^2+(z^2+w^2-1)^2=0$ is Ricci flat but is also just flat because it is a flat torus. I imagine some Euclidean form of a Schwarzschield wormhole type surface would work? Embedded in $\mathbb{R}^5$ (All I can find is surfaces defined in $\mathbb{CP}^n$ instead of Euclidean space. (I don't know how to convert these to surfaces in $\mathbb{R}^m$ .) I think the smallest example should be a 4-manifold thus defined by an equation $f(x,y,z,w,u)=0$ . (Since for 3-manifolds the Riemann tensor can be decomposed into the Ricci tensor). (2) Are there any polynomials shown to have zero-scalar curvature but non-zero Ricci-curvature?  I think the simplest example should be a 3-manifold defined by an equation $f(x,y,z,w)=0$ (Since for 2-manifolds the Ricci tensor is proportional to the Scalar tensor).","['algebraic-geometry', 'differential-geometry']"
4512284,Permutations of Independent Probabilities,"Problem: Say that I have a list of $n$ tasks to complete. Each of the tasks have independent probabilities $p_1, p_2, ..., p_n$ of completing that task.
There is a particular task on the list that I want to get completed. However, the list will be generated in a random order and I have to complete each task in order.
Once I complete the particular task that I have in mind, I have succeeded and the rest of the list does not matter. What is the probability that I will complete the task? My attempt at solving it: To make it easier, I assume that the task that I want to complete is task $n$ (the task with independent probability $p_n$ ). I hope I'm not messing anything up by making that assumption. I read something that basically said that the total probability of completing task $n$ would be $$P(task_n) = \frac{1}{n}\sum_{i=1}^n \left(p_n*\left(\frac{\sum_{j=1}^{n-1} p_j}{n-1}\right)^{i-1}\right)$$ But although the equation seems to work for $i=1$ and $i=2$ , it stops working after that point. I'm pretty sure that's because this equation is not taking into account that there is no repetition in the tasks. Basically what that equation is trying to do is taking each possible location for $task_n$ to show up on the list, calculating the probability that you get to that task given that it is put in that position, and then averaging all of those together. That general approach made sense to me, because there is an equal probability of that task showing up in each position. So I tried figuring out equations given each position to see if I could find the pattern and extrapolate a general equation. I didn't get very far. Case 1 : $task_n$ shows up in position 1. $$P(task_n, pos_1)=p_n$$ This is the most straightforward case. If $task_n$ shows up first on the list, then the total probability of completing it is exactly the same as the independent probability of completing that task. Case 2 : $task_n$ shows up in position 2. $$P(task_n, pos_2)=p_n \left( \frac{\sum_{j=1}^{n-1}p_j}{n-1} \right)$$ Still pretty straightforward. If $task_n$ shows up second on the list, then the total probability of completing it is equal to its individual probability times the probability of whatever shows up in position 1. Since all other tasks on the list have an equal chance of showing up, average all those probabilities together. Case 3 : $task_n$ shows up in position 3. $$P(task_n, pos_3)=p_n \left( \frac{\sum_{j=1}^{n-1}p_j\left( \sum_{k=1}^{n-1}(p_k)-p_j \right)}{(n-1)(n-2)} \right)$$ So this is where it starts getting complicated, and I wasn't able to figure out any equation for position 4 or after. At this point, I can see that I'm just multiplying the probabilities of all previous tasks, but I'm basically subtracting out the instance that the task is repeated. I know that another way to look at this is that we're finding the product of all possible ways to choose 1 out of $p_1, p_2, ..., p_{n-1}$ , then choose 2, then choose 3, ... then choose $n-1$ , and averaging that all out. I just don't know how to put that into a single formula. I feel like I'm on the right track, but at the same time the equation gets so much more complicated after this that I feel like I'm not getting anywhere. I tried looking it up because this equation must exist out there somewhere, but I couldn't figure out how to search it because if I try looking for ""permutations of probabilities"" then I all I get is basic prealgebra homework problems. Please help, this has been killing my brain for days.","['permutations', 'summation', 'probability', 'products']"
4512321,Bound on $p$-th moment from tail probability,"In Corollary 2 of this paper , the following result is given: Under some assumptions, there exists a constant $C>0$ such that for all $t \ge 1$ , with probability at least $1-e^{-t}$ , $$
\| \hat{\Sigma} - \Sigma\| \le C \|\Sigma\| \left ( \sqrt{\frac{r}{n}} \lor \frac{r}{n} \lor \sqrt{\frac{t}{n}} \lor \frac{t}{n}\right ).
$$ This implies for $p \ge 1$ , $$
E^{1/p} \|\hat{\Sigma} - \Sigma\|^p \lesssim_p \|\Sigma\| 
\left ( \sqrt{\frac{r}{n}} \lor \frac{r}{n}\right ).
$$ The notation $\lesssim_p $ here means that the left hand side is smaller than the right hand side multiplied by a constant that depends only on $p$ . My question is about how to go from the tail bound in the first line to the second. The proof in the paper states that one can do so by integrating the tail probabilities, but I am not really sure how this follows so easily for all $p$ .","['statistics', 'probability-theory', 'probability']"
4512345,Does the Wronskian of three or more linearly independent functions change its sign?,"If $y_1, y_2$ are two Linearly independent solutions of a differential equation of order $2$ then we know that if the Wronskian is not zero then it never changes its sign (using the Abel's identity ). How far is this true for a third order linear differential equation having $y_1, y_2, y_3$ as linearly independent solutions? I tried finding a solution but i dont think i have an answer. How far is this true in a general case that if $W(y_1,\dots,y_n)$ if non zero then it never changes its sign?","['determinant', 'ordinary-differential-equations', 'analysis', 'real-analysis', 'wronskian']"
