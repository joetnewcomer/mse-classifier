question_id,title,body,tags
3372390,Did S. Lang prove Kuratowski–Zorn lemma without Axiom of choice or Well-ordering theorem?,"In S. Lang' Algebra , Appendix 2, the author proved Zorn's Lemma . After a carefully reading of the proof, I failed to see either Axiom of choice or Well-ordering theorem were assumed in his proof. So did he use any equivalent forms of Zorn's Lemma to prove it at all? If not, can Zorn's Lemma be proven (as Lang did) using other axioms of set theory?","['elementary-set-theory', 'proof-verification']"
3372488,Why is variance squared rather than cubed (or any other exponent)?,"Variance is $$\dfrac{\sum_{i=1}^{n}(x_i-\bar x)^2}{n-1}$$ But why square the difference? Why not cube it, or any other exponent? Related question .","['statistics', 'variance']"
3372576,$\left\lfloor x\right\rfloor$ and $[x]$ are the same concept?,The function $f(x)$ integer part of $x$ is defined as the largest integer less than or equal to $x$ . Generally I have always use this symbol to indicate the integer part of $x$ $$f(x)=[x]$$ instead of $$f(x)=\left\lfloor x\right\rfloor.$$ Are there some differences or are they represent the same quantity?,['functions']
3372606,Let $a$ be an element of a group $G$ and $|a| = 7.$ Show $a$ is the cube of some element of $G$.,Having some trouble understanding how to proceed w/ this hw question: Let $a$ be an element of a group $G$ and $|a| = 7.$ Show $a$ is the cube of some element of $G$ . Attempt: $a^7 = e$ by hypothesis. So need to show $a = b^3$ for some $b \in G$ $a^7 = (b^3)^7 = b^{21} = e$ But is this even true? Have I actually shown something?,['group-theory']
3372618,Help on a hard integral,"So, I'm doing an extensive homework of electromagnetism and we are searching for the total electromagnetic angular momentum of the Thomson dipole. In the end, there is one integral we cannot solve. By '""we"" I mean the entire class: nobody is getting how to solve it. Professor swears there are no expansions or approximations, so... The integral is, in spherical coordinates $$\int_0^{2\pi} d\phi  \int_0^\pi \frac{r^2(r-q\cos\theta)\cdot \sin\theta \cdot \cos\theta}{(r^2+q^2-2rq\cdot \cos\theta)^{3/2}}d\theta.$$ The solution is supposed to be $\frac{8\pi}{3}\frac{q}{r}$ for $r > q$ (q is the distance separating the monopoles). I could find only $\frac{4\pi}{3}\frac{q}{r}$ , and with a negative sign. Can anybody illuminate me on a possible solution?","['integration', 'electromagnetism']"
3372648,how to get a linear ricci flow equation??,"one could use linear approximation $g_{ab}=\eta_{ab}+h_{ab}$ to get linear ricci flow equation (2). How to do that? is any process ?
I am studying general relativity , i just use the linear approximation : $g_{ab}=\eta_{ab}+\gamma_{ab}$ , and calculate the christoffl symbol : $\Gamma_{a b}^{(1) c}=\frac{1}{2} \eta^{c d}\left(\partial_{a} \gamma_{b d}+\partial_{b} \gamma_{a d}-\partial_{d} \gamma_{a b}\right)$ ,finally, get the Ricci tensor: $R_{a b}^{(1)}=\partial^{c} \partial_{(a} \gamma_{b) c}-\frac{1}{2} \partial^{c} \partial_{c} \gamma_{a b}-\frac{1}{2} \partial_{a} \partial_{b} \gamma$ but there are two extra term : $\partial^{c} \partial_{(a} \gamma_{b) c}$ and $-\frac{1}{2} \partial_{a} \partial_{b} \gamma$ , which not match the linearization ricci flow equation(2). what is wrong with me?","['ordinary-differential-equations', 'ricci-flow', 'linear-pde', 'partial-differential-equations', 'differential-geometry']"
3372677,"Verifying the integrability of the distribution $\omega(x,y,z)=\operatorname{ker}(x\,dx + y\,dy + z\,dz)$ on $\Bbb R^3$","Consider the linear functional $\omega \in (\mathbb{R}^3)^{*}$ , gived by $\omega(x,y,z)=x\cdot dx + y\cdot dy + z\cdot dz$ (The base $\{ dx,dy,dz \}$ for $(\mathbb{R}^3)^{*}$ is dual in the canonical base of $\mathbb{R}^3$ $\{ e_1,e_2,e_3\}$ ).The $2$ -distribution $D_2:\mathbb{R}^3 \rightarrow T\mathbb{R}^3; (x,y,z) \mapsto ker(\omega(x,y,z))$ is integrable? The kernel is the sets of the elements $(x,y,z)$ that satisfy $\omega(x,y,z)=0$ , then: $\begin{equation}
\begin{aligned}
\omega(x,y,z) &= x\cdot dx(x,y,z) + y\cdot dy(x,y,z) + z\cdot dz(x,y,z) \\
& = x\cdot dx(e_1+e_2+e_3)+y\cdot dy(e_1+e_2+e_3)+z\cdot dz(e_1+e_2+e_3) \\
& = x+y+z
\end{aligned}
\end{equation}$ The equality $\omega(x,y,z)=0$ give a set of vectors generated by $\{(1,0,-1),(1,-1,0)\}$ , then $ker(\omega(x,y,z))=span\{(1,0,-1),(1,-1,0)\}$ . So, the vector fields associated are $X=dx-dz, Y=dx-dy$ . I need to see if the Lie bracket is involutive, so using the Frobenius theorem i can show that $D_2$ is integrable $\begin{equation}
\begin{aligned}
\space[ X,Y ] &= X(1)dx-X(1)dy-Y(1)dx+Y(1)dz \\
& = 0 
\end{aligned}
\end{equation}$ Since the Lie bracket is $0$ then $D_2$ is involutive, using the Frobenius theorem, then $D_2$ is integrable. It's right my solution?","['solution-verification', 'differential-geometry']"
3372751,Characterization of Absolute Continuity of Measures,"Let (X,Σ) be a measurable space and μ,ν two (positive) measures defined in Σ. We say that ν is absolute continuous with respect to μ ( and we write ν<<μ) if the following holds: $$ ( \forall E\in Σ , μ(E)=0) \Rightarrow (ν(E)=0) $$ Now, if ν is also a finite measure then we have an ε-δ characterization of the above definition which is: $ ν<<μ \Leftrightarrow \forall \epsilon\ >0, \exists δ=δ(\epsilon)>0 ,$ such as $\forall E\in Σ,$ with $μ(E)<δ$ we have $ν(E)<\epsilon $ . I am interested  in the case where ν is not a finite measure and so the above characterization is not valid. Suppose that we are at the measurable space $ (\Bbb N, \mathcal{P} (\Bbb N)) $ and we define two measures as following: $$
μ(E)= \sum_{n\in E} \displaystyle{\frac{1}{2^n}}
$$ $$
ν(E)= 
\begin{cases}
card(E),  & \text{if E is finite} \\
\infty, & \text{if E is infinite}
\end{cases}
$$ Then, of course ν is absolute continuous w.r.t μ, but the ε-δ condition must fail in this case. $$
\text{If we take},\epsilon =1 ,\text{then for all $n$ $\in \Bbb N$} ,\text{we can find $E_n:= \{n,n+1,n+2,....\} $},\text{such as} 
$$ $$ μ(E_{n})= \sum_{j \in E_{n}} \displaystyle{\frac{1}{2^j}}= \sum_{j=n}^\infty \displaystyle{\frac{1}{2^j}}= \displaystyle{\frac{1}{2^{n-1}}} < \displaystyle{\frac{1}{2^{n-2}}} \\
\text{but}, ν(Ε_{n})=card(E_{n})=card(\{n,n+1,....\})=\infty >1 $$ and so the ε-δ condition is not true. Is this correct or i'm missing something? 
Thanks you in advance.",['measure-theory']
3372782,Understanding the global residue theorem,"I am studying the global residue theorem which applies for multivariate residues. The theorem is reported in the following references e.g. ( 1 ) (Eq.87) , ( 2 ) (Eq. 109) in different formulations, that now I would try to understand. 
It is unnecessary to reveal that my education is not from math studies, so I am sorry if I won't be precise. First of all, let us focus on the following definition ( 1 ) (see Eq.87). Theorem 2. (Global residue theorem). Let $\omega$ denote a
  meromorphic $n$ -form defined on a compact manifold $M$ . Given an open
  covering $\{U_i\}$ , let $\omega$ take the local form $$ \omega=\frac{h(z)dz_1 ∧ · · · ∧ dz_n}{f_1(z)· · · f_n(z)} $$ where $f(z) = (f_1(z), . . . , f_n(z)):\mathbb{C}^n \rightarrow \mathbb{C}^n$ and $h(z): \mathbb{C}^n \rightarrow C$ are holomorphic functions. Let $D_j= \{z ∈ M : f_j (z) = 0\}$ with $j = 1, . . . , n$ denote the divisors of $\omega$ , and assume that $V = D_1 ∩· · ·∩ D_n$ is a finite set.
  Then $$ \sum_{p∈V} Res_p\omega = 0$$ where each $Res_p\omega$ is
  evaluated locally on a patch $U_i$ which contains $p$ . Strictly speaking, if we have a form which is defined on $\mathbb{C}^n$ , the theorem does not apply. This is why in ( 1 ) is suggested to compactify $\mathbb{C}^n$ into $\mathbb{CP}^n$ and then apply the theorem. This is done through the change of coordinate $$
z_1 = \frac{w_1}{w_0}\,,\,. . . \,,\,z_n = \frac{w_n}{w_0}
$$ and the open covering $\{U_k\}$ is defined as $$
U_k = \{(w_0,w_1,...w_n): w_k=1\}\,, \text{for } k=0,1,...n
$$ The form $\omega$ on the patch $U_k$ then takes the expression (see Eq. 92 in ( 1 )) $$
\omega|_{U_k} = \frac{(-1)^k\, h(w/w_0)\, dw_0\,\wedge\,...\wedge dw_n}{w_0^{n+1}f_1(w/w_0)...f_n(w/w_0)}
$$ Question: Is there a sufficient condition on the polynomials $h(z),f_i(z)$ such that the zeros of $f(z) = (f_1(z),...,f_n(z))$ are all the points contained in the set $V$ in the open covering $U_0$ ? According to ( 2 ) the theorem can also be stated in the following way Let $\omega = h\,dz/f_1 ...f_n$ be defined by polynomials $h$ and $f_i$ . Let $F_i = \{z ∈ \mathbb{C}^n : f_i(z) = 0\}$ be the hypersurface (i.e. $n − 1$ dimensional subspace) associated with $f_i$ and $Z = F_1 ∩F_2 ∩...∩F_n$ be the set of zeroes of $f$ . Here we assume that $Z$ is a discrete set of points. Then one defines the Global residue of $h$ with respect to the map $f$ as $$
Res_f (h) = \sum_{a∈Z} res(ω)_a.
$$ Now, the Global Residue Theorem (GRT) states that if $deg(h) < deg(f_1) + . . . + deg(f_n) − n$ then $Res_f (h) = 0$ . This formulation seems to give an answer to my question about the sufficient condition, i.e. provided $deg(h) < deg(f_1) + . . . + deg(f_n) − n$ . However, I don't understand it. For example, this formulation does not talk about compact manifolds and seems to be very general. However, it does not seem quite exact (maybe I am misunderstanding). Consider for example the form $$
\omega = \frac{z_2^2 z_1 dz_1\wedge dz_2}{(1-z_2 -z_1 +2z_1 z_2)z_1(z_2-z_1)z_2(z_1-1)}
$$ with the map $f(z) =\left((1-z_2 -z_1 +2z_1 z_2)z_1(z_2-z_1),z_2(z_1-1)\right) $ . The set $Z$ is given by discrete points $Z=\{(0,0),(1,0),(1,1)\} $ . Moreover, $\text{deg}(h) = 3$ and $\text{deg}(f_1)+\text{deg}(f_2) = 5 + 1 = 6 $ , then the condition $3 < 6 - 2 = 4$ is satisfied and I would expect the theorem holds. Instead, by direct computation (by hand) I get a non-zero global residue. If you don't want to do computations by hand, you can use the Mathematica package MultivariateResidues , the code is shown below.
Where am I wrong? Is the definition of $\text{deg}(...)$ more complicated? You can copy and past the following Mathematica Code to reproduce my result Get[""MultivariateResidues.m""];
sols = {(1 - w2 + w1 (-1 + 2 w2)) w1 (w2 - w1) == 0, w2 (w1 - 1) == 0} // Solve;
listResidues = {};
Print[Dynamic[ii], ""/"", Length[sols]]
For[ii = 1, ii <= Length[sols], ii++,
AppendTo[listResidues, (MultivariateResidue[w2^2 w1, {(1 - 1 w2 + w1 (-1 + 2 w2)) w1 (w2 - w1), w2 (w1 - 1)},sols[[ii]]] // Simplify)]]
listResidues2 /. List -> Plus // Simplify
(* Output: -1 *)","['complex-analysis', 'multivariable-calculus', 'residue-calculus', 'complex-integration']"
3372797,Find the maximum number of triangles that can have equal area,"Here is another math problem that I just can't do. A convex pentagon is partitioned by its diagonals into $11$ regions, $1$ pentagon and $10$ triangles. What is the maximum number of those $10$ triangles that can have equal area? I tried many different ways to do this. The standard regular pentagon has $5$ triangles with equal area. This is a reasonable answer, but many people on my forum claim to have found $6$ to $7$ triangles by Coordinate bashing. They assume the vertexes of the point to be $(0,0), (0,1), (a_1,a_2), (b_1,b_2), (c_1,c_2)$ , and they hash out all of the coordinates of the diagonals, and figure out the area of each triangle. Is there a simpler and less confusing way to solve the problem? Any help will be greatly appreciated.","['euclidean-geometry', 'convex-geometry', 'geometry', 'polygons']"
3372807,Find the solution to the following differential equation: $ \frac{dy}{dx} = \frac{x - y}{xy} $,"The instructor in our Differential Equations class gave us the following to solve: $$ \frac{dy}{dx} = \frac{x - y}{xy} $$ It was an item under separable differential equations. I have gotten as far as $ \frac{dy}{dx} = \frac{1}{y} - \frac{1}{x} $ which to me doesn't really seem much. I don't even know if it really is a separable equation. I tried treating it as a homogeneous equation , multiplying both sides with $y$ to get (Do note that I just did the following for what it's worth)... $$ y\frac{dy}{dx} = 1 - \frac{y}{x} $$ $$ vx (v + x \frac{dv}{dx}) = 1 - v $$ $$ v^2x + vx^2 \frac{dv}{dx} = 1 - v $$ $$ vx^2 \frac{dv}{dx} = 1 - v - v^2x$$ I am unsure how to proceed at this point. What should I first do to solve the given differential equation?","['differential', 'ordinary-differential-equations']"
3372914,Measurable spaces Proof Explanation,"First, in the below theorem .  Is proving $ \mathcal{F'} = \sigma(\mathcal{A'})$ the same as proving that $T$ is $ \mathcal{F} / \sigma(\mathcal{A'})-measurable$ ? why? $$$$ I have hard time understaning  the highlighted parts Could you please explain for each highlighted part why is it true and why do we mention it or why do we use it?
Also, does the final result mean that $T$ is $ \mathcal{F} / \sigma(\mathcal{A'})-measurable$ ? if Yes why","['proof-explanation', 'measure-theory', 'probability-theory']"
3372923,Is normal the only rotation-invariant distribution whose marginals are normal?,"From Maxwell's theorem, standard normal distribution is the only rotation-invariant distribution with fully-factorized marginals. What if we do not ask the marginals to be independent, but instead require them to be univariate standard normal? Will multivariate standard Gaussian distribution still be the only rotation-invariant distribution?","['statistics', 'probability-distributions', 'probability']"
3372929,Which sets of sequence is countable and Uncountable.,"Consider the sequences $$\displaystyle X=\left\{(x_n): x_n \in \left\{0,1\right\},n \in \mathbb{N} \right\}$$ $$and$$ $$\displaystyle Y=\left\{(x_n)\in X:x_n=1 \;\;\text{for at most finitely many n} \right\}$$ I have to choose which is uncountable and which is countable. Solution i tried- Here $X$ is set of sequence with enteries from $\left\{0,1\right\}$ thus it has number of elements $2^{\aleph_0}$ which is uncountable . Now The set $Y$ it has all the sequences from the set $X$ but some of its elements of sequences is replaced by the only ' $1$ ' so its Cardinality will be less then $2^{\aleph_0}$ ,but by $\textbf{ continuum hypothesis}$ there is no set having Cardinality in-between the ${\aleph_0}$ and $2^{\aleph_0}$ so the set $Y$ will be countable I write this proof but i don't even know this is correct or not but i am sure about set $X$ but not sure about $Y$ please help me with set $Y$ Thank you.","['elementary-set-theory', 'cardinals', 'real-analysis']"
3372955,The identity $(u\times v)\cdot(x\times y)=\begin{vmatrix}u\cdot x&v\cdot x\\ u\cdot y&v\cdot y\\\end{vmatrix}$,"I know by brutal calculation this identity holds always: $$(u × v) \cdot (x × y) = \begin{vmatrix} 
u \cdot x & v \cdot x \\ 
u \cdot y & v \cdot y \\
 \end{vmatrix}$$ for arbitrary vectors $u$ , $v$ , $x$ , $y$ . I'd like to know where it come from naturally. Thank you.","['cross-product', 'multivariable-calculus', 'linear-algebra', 'geometry']"
3372990,Prove Cantor's Theorem,"I have read proofs of the theorem using 'Diagonalization' method or the proof by 'contradiction'. I have shared my approach of the proof. I want to know whether the proof is correct or not. Approach : We can define an injection between the elements of a set $A$ to its power set $2^A$ , such that $f$ maps elements from $A$ to corresponding singleton sets in $2^A$ . Since we have an extra element $\phi$ in $2^A$ which cannot be lifted back to $A$ , hence we can state that $f$ is not surjective.","['elementary-set-theory', 'proof-verification']"
3372993,Simplifying $i \left[ \ln(x+i)-\ln(x-i)-\ln(1+ix)+\ln(1-ix) \right]$,"Can the following expression further be simplified and expressed in terms of usual functions such as inverse hyperbolic or inverse trigonometric functions? $$
f(x) = i \left[ \ln(x+i)-\ln(x-i)-\ln(1+ix)+\ln(1-ix) \right] \, , 
$$ where $x\ge 0$ is a real number. Inputs and ideas welcome.
Thanks","['complex-analysis', 'trigonometry', 'logarithms', 'real-analysis']"
3373011,How to solve this system of hyperbola equations?,"I have a system of 2 equations each describing the branch of a hyperbola. The below equations represent hyperbolae with foci $P_0$ and $C_1$ (or $C_2)$ and transverse axis length $r_1$ (or $r_2)$ .  I'm trying to find the unknowns $x$ and $y$ : $$\sqrt{(x-x_1)^2+(y-y_1)^2}-\sqrt{(x-x_0)^2+(y-y_0)^2}=r_1. \tag{1}$$ $$\sqrt{(x-x_2)^2+(y-y_2)^2}-\sqrt{(x-x_0)^2+(y-y_0)^2}=r_2. \tag{2}$$ Where: $P_0 = (x_0, y_0) = (0.87,-0.5)$ $C_1 = (x_1, y_1, r_1) = (0, 1, 0.13)$ $C_2 = (x_2, y_2, r_2) = (-0.87, -0.5, 0.49)$ These hyperbolae are the result of an algorithm for trilateration I'm trying to compute, known as Time Difference Of Arrival (TDOA) . I just have no idea where to go from here! EDIT: For some additional context, here's a graph of the problem:","['systems-of-equations', 'conic-sections', 'circles', 'geometry', 'multivariate-polynomial']"
3373056,Does the radius of the base equal the height in a right circular cone?,"I am doing question 6 of a practice calculus exam, namely: A container in the shape of a right circular cone with vertex angle a
  right angle is partially filled with water. a) Suppose water is added
  at the rate of 3 cu.cm./sec. How fast is the water level rising when
  the height h = 2cm.? My answer was ${dh\over dt} = {3\over 4\pi} \cdot {c^2}$ , where $c$ is a fixed constant equal to the ratio of the height to the radius. The given solution , however, is ${dh\over dt} = {3\over 4\pi}$ . The solution uses the assumption that "" $r=h$ since this is a right circular cone."" Is this a reasonable assumption? I have checked definition of right circular cone and cannot find anything about this. Thanks, Josh","['related-rates', 'geometry']"
3373067,Prime Generating Irrational Number,"Does there exist an irrational number such that every time it is multiplied by 100 its integer part gives a prime number? $$ \phi= 0,a_0a_1a_2a_3\cdots$$ $$ \lfloor 10^{2n}\phi \rfloor \in \mathbb P,\quad \forall n \in \mathbb N$$ Or in a more general way multiply by $10^{p.n}$ , where p is a fixed prime number. For example, let $\phi = 0,1163\cdots$ $$\lfloor 10^2 \phi \rfloor = 11$$ And $$\lfloor 10^4 \phi \rfloor = 1163$$ While 11 and 1163 are primes, 63 by itself is not. So, $a_{2i}a_{2i+1}$ is not necessarily prime for $i \in \mathbb N$","['number-theory', 'prime-numbers']"
3373085,Cayley-Hamilton...,"Say $A$ is a square matrix over an algebraically closed field. Say $m$ is the minimal polynomial and $p$ is the characteristic polynomial. Of course C-H implies that $m|p$ . Conversely, if we can show $m|p$ then C-H follows; the question is whether one can give a ""simple"", ""elementary"" or ""straightforward"" proof that $m|p$ . Note. What I really want is a proof such that I feel I actually understand the whole thing. Hence in particular no Jordan form allowed. Edit. An Answer has appeared that shows $m|p$ in a very simple way - simply demolishes what I wrote below. Edit. When I posted this is was an honest question that I didn't know the answer to. I think I got it; if anyone wants to say they believe the argument below (or not) that would be great. First, it's clear that linear factors of $m$ must divide $p$ : If $m(\lambda)=0$ then $p(\lambda)=0$ . Because $m(t)=(t-\lambda)r(t)$ , so $(A-\lambda)r(A)=0$ . Minimality of $m$ shows that $r(A)\ne0$ , hence $A-\lambda$ is not invertible, hence $p(\lambda)=0$ . If we could show that $(t-\lambda)^k|m$ implies $(t-\lambda)^k|p$ we'd be set. Some possible progress on that, first restricted to a simple special case: If $t^2|m(t)$ then $\dim(\ker(A^2))\ge 2$ . Proof: Say $X=K^n$ is the underlying vector space. Say $m(t)=t^2q(t)$ .  Let $$Y=q(A)X,$$ $$B=A|_Y.$$ Then $Y\subset\ker(A^2)$ . Say $d=\dim(Y)$ . Now $B^2=0$ , and it follows easily that $B^d=0$ . But $B\ne0$ , hence $d\ge2$ . Similarly If $(t-\lambda)^k|m$ then $\dim(\ker(A-\lambda)^k)\ge k$ . So we only need If $\dim(\ker(A-\lambda)^k)\ge k$ then $(t-\lambda)^k|p$ . Which I gather is true, but only by hearsay; I'm sort of missing what it ""really means"" to say $t^2|p$ . Wait , I think I got it. Say $$m(t)=(t-\lambda)^kq(t),$$ $$q(\lambda)\ne0.$$ The ""kernel lemma"" shows that $$X=\ker((A-\lambda)^k)\oplus\ker(q(A))=X_1\oplus X_2.$$ Each $X_j$ is $A$ -invariant, so we can define $$B_j=A|_{X_j}.$$ Since similar matrices have the same determinant we can use any basis we like  in calculating the determinant $p(t)$ ; if we use a basis compatible with the decomposition $X=X_1\oplus X_2$ it's clear that $$p_A=p_{B_1}p_{B_2},$$ so we need only show that $$p_{{B_1}}(t)=(t-\lambda)^k.$$ In fact it's actually enough to show $(t-\lambda)^k|p_{B_1}$ ,  and that's clear: Lemma. If $B$ is a $d\times d$ nilpotent matrix then $p_B(t)=t^d$ . Proof: We're still assuming $K$ is algebraically closed; $B$ cannot have a non-zero  eigenvalue. So if $d=\dim(\ker((A-\lambda)^k)$ then $$p_{B_1}(t)=(t-\lambda)^d;$$ we've already shown that $d\ge k$ , so $(t-\lambda)^k|p$ . Hmm. Maybe that doesn't look all that simple. It's nonetheless the sort of thing I wanted, because I can give a one-line summary making it at least comprehensible : One-line summary: Since $m$ splits, the kernel lemma (a simple consequence of the fact that $K[t]$ is a PID) shows that $A$ is the direct sum of operators $B_j$ such that $B_j-\lambda_j$ is nilpotent. So it's enough to prove C-H for nilpotent operators, which is not hard .",['linear-algebra']
3373207,Convergence uniform of sequence of measurable non-negative functions in finite measure space.,"It's the first time I'm asking here. I'm having problem with an exercise on integration theory. It's: Let $(X,\mathbb{X},\mu)$ a finite measure space. Denote $M^+(X,\mathbb{X}) = \{f \in [0,\infty]^X\:\:; f\:\: is\:\: measurable\}$ . If $(f_n)$ is a sequence of measurable functions em $M^+(X,\mathbb{X})$ which converges uniformly to a function $f$ , so $f \in M^+(X,\mathbb{X})$ and \begin{equation}
\int_Xfd\mu = \lim_{n \to \infty} \int_Xf_nd\mu
\end{equation} . Well, the natural manner of ""solve"" this is like it was done here Question 4.K of Bartle's Element of Integration , but i think it's incorrect, because this inequality just can be used if we can ensure that the function $|f_n-f|$ is integrable. I think we have to ask theses functions be integrable, or, since $\mu(X)<\infty$ and the convergence is uniform, the function $f$ is bounded. Additional edition: The function $f_n-f$ is, in fact, integrable for n enoughly large, but how can we ensure that \begin{equation}
\int_Xf_nd\mu - \int_Xfd\mu = \int_X(f_n-f)d\mu
\end{equation} A priori, we just can ensure in two situations: Sum of non-negative measurable functions or sum/difference of integrable functions. What do you think?","['measure-theory', 'lebesgue-integral', 'uniform-convergence', 'real-analysis']"
3373212,What are the discrete subgroups of the quaternions?,"I'm trying to find all the discrete subgroups of the quaternions under multiplication. Particularly I'm interested in subgroups of unit quaternions (if we have a non-unit quaternion then we have terms accumulating near $0$ , so it's not discrete. These groups are still interesting, but I want to ignore them for now.) First we have some trivial groups, by which I mean they are contained in a plane: $$\{\cos(2\pi k/n) + v \sin(2\pi k/n)\},$$ where $v$ is any unit quaternion with no real part. These groups are isomorphic to the complex $n$ th roots of unity. As for non trivial discrete groups, I have two so far: $$\{\pm 1, \pm i, \pm j, \pm k\}$$ with 8 elements, and the group generated by $$\{(1+i)/\sqrt{2}, (1+j)/\sqrt{2}\}$$ with $48$ elements. Are there any more?","['group-theory', 'quaternions', 'finite-groups', 'discrete-mathematics']"
3373240,I want to show that $A=\cup_{n=1}^{\infty} A_n$,Let $A=\{x\in X: \varphi(x)>0\}$ and $A_n=\{x\in X: \varphi(x)\geq1/n\}$ (increasing) I want to show that $A=\cup_{n=1}^{\infty} A_n$,['functions']
3373251,How to change the order of integration when limit is a function?,"Consider the double integral: $A=\displaystyle\int_0^5   \left(     \int_0^7    {x'}^2x^3   dx    \right)   dx'$ We can change the order of integration: $B=\displaystyle\int_0^7   \left(     \int_0^5    {x'}^2x^3   dx'    \right)   dx$ Now consider another double integral: $C=\displaystyle\int_0^5   \left(     \int_0^{x'}    {x'}^2x^3   dx    \right)   dx'$ How shall one change the order of this integral, i.e. when the limit is a function? EDIT (Generalization) Now consider another double integral: $D=\displaystyle\int_0^5   \left(     \int_0^{f(x')}    {x'}^2x^3   dx    \right)   dx'$ How shall one change the order of this integral, i.e. when the limit is an arbitrary function $f(x')$ ?","['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'functions']"
3373268,The flux of a vector field through a cylinder.,"The question is by using Gauss’ Theorem calculate the flux of the
  vector field $\overrightarrow{F} = x \hat{i} + y \hat{j}+ z \hat{k}$ through the surface of a cylinder of radius A and height H, which has
  its axis along the z-axis and the base of the cylinder is on the
  xy-plane. So, first of all I converted the vector field into cylindrical coordinates $\overrightarrow{F}= \rho \cos^2 \phi \hat{e}_\rho + \rho \sin^2 \phi \hat{e}_\rho + z \hat{e}_z $ which can be further reduced to- $\overrightarrow{F}= \rho \hat{e}_\rho + z \hat{e}_z$ The  surface of the cylinder has three parts, $ \ S_1 $ , $ \ S_2 $ , and $ \ S_3 $ . $ \ S_1 $ and $ \ S_2 $ are the top and bottom of surface of the cylinder and $ \ S_3 $ is the curved surface. We can write the surface integral over the surface of the cylinder as $\unicode{x222F}_S \overrightarrow{F} . d\overrightarrow{S}=\iint_{S_1} \overrightarrow{F} . d\overrightarrow{S_1} +\iint_{S_2} \overrightarrow{F} . d\overrightarrow{S_2} + \iint_{S_3} \overrightarrow{F} . d\overrightarrow{S_3}  $ As the area element is in $\rho \phi$ plane (for a constant value of z) has the value $\rho d \rho d \phi$ . So an area element on $ \ S_1 $ and $ \ S_2 $ will have magnitude $\rho d \rho d \phi$ , and the outward unit normals to $ \ S_1 $ and $ \ S_2 $ are then $ \hat{e}_z$ and $- \hat{e}_z$ , respectively $\therefore d\overrightarrow{S_1}= \rho d \rho d \phi \hat{e}_z$ and $d\overrightarrow{S_2}= -\rho d \rho d \phi \hat{e}_z$ And the area element for the $d\overrightarrow{S_3}= \rho dz d \phi \hat{e}_ \rho $ Now, keeping the conditions in mind- $0 \le \rho \le A$ ; $0 \le \phi \le 2 \pi$ ; $0 \le z \le H$ $\unicode{x222F}_S \overrightarrow{F} . d\overrightarrow{S}=\iint_{S_1} [\rho \hat{e}_\rho + z \hat{e}_z].[\rho d \rho d \phi \hat{e}_z]+ \iint_{S_2} [\rho \hat{e}_\rho + z \hat{e}_z].[-\rho d \rho d \phi \hat{e}_z]+ \iint_{S_3} [\rho \hat{e}_\rho + z \hat{e}_z].[\rho dz d \phi \hat{e}_ \rho]$ The flux of $d\overrightarrow{S_1}$ and $ d\overrightarrow{S_2}$ will cancel  out each other. Now, integrating $\iint_{S_3} \overrightarrow{F} . d\overrightarrow{S_3}  $ as double integral- $\int _{\phi =0}^{2\pi }\:\int _{z=0}^H\:\rho^2 dz d \phi$ $= 2 \pi A^2 H$ where $\rho = A$ So, the total flux is $= 2 \pi A^2 H$ which I think is wrong, as the flux should be the curved surface area of the cylinder,i.e., $= 2 \pi A H$ I am still learning this topic, so please mention any mistake that I've done while solving it","['integration', 'multivariable-calculus']"
3373295,Golomb non decreasing sequence,"Golomb sequence is a non-decreasing integer sequence where n-th term is equal to number of times n appears in the sequence. It's recursive formula is given by : G(1) = 1
G(n+1) = 1 + G(n + 1 - G(G(n))) It will be very helpful if someone can just explain this recursion and not any kind of closed form solution. I am stuck on this for a few days but still nothing comes to my mind that will explain this recursive formula.","['recursive-algorithms', 'discrete-mathematics']"
3373298,Derangement and anagrams,"How can I calculate the amount of deranged anagrams for specific word.
For example, the word: Anona.
I know that there are $\frac{5!}{2!2!1!} = 30$ anagrams.
How can I calculate the amount of derangements: $$
oanan\
noaan\
nanao\
naaon
$$ Is there any general formula in order to deal with more sophisticated cases? Thanks!","['combinatorics', 'discrete-mathematics']"
3373348,Functional equation problem: $ f \left( y ^ 2 - f ( x ) \right) = y f ( x ) ^ 2 + f \left( x ^ 2 y + y \right) $,"This functional equation problem is from the Latvian Baltic Way team selection competition 2019: Find all functions $ f : \mathbb R \to \mathbb R $ such that for all real $ x $ and $ y $ , $$ f \left( y ^ 2 - f ( x ) \right) = y f ( x ) ^ 2 +  f \left( x ^ 2 y + y \right) \text . \tag 1 \label {eqn1} $$ OK, so I think that the only answer is $ f ( x ) = 0 $ . I just want to see if my proof that it is the only solution is correct. So we start off by plugging $ y = - y $ . We get that $$ f \left( y ^ 2 - f ( x ) \right) = - y f ( x ) ^ 2 + f \Big( - y \left( x ^ 2 + 1 \right) \Big) \text . $$ Then we add the two equations together getting that $$ 2 f \left( y ^ 2 - f ( x ) \right) = f \Big( - y \left( x ^ 2 + 1 \right) \Big) + f \Big( y \left( x ^ 2 + 1 \right) \Big) \text . $$ From the above equations we get that $$ \frac { f \Big( - y \left( x ^ 2 + 1 \right) \Big) + f \Big( y \left( x ^ 2 + 1 \right) \Big) } 2 = y f ( x ) ^ 2 + f \left( x ^ 2 y + y \right) \text. \tag 2 \label {eqn2} $$ Now, if we plug $ x = - x $ then we will get that the LHS is the same and that the RHS is $ y f ( - x ) ^ 2 + f \left( x ^ 2 y + y \right) $ . So we proceed by subtracting the two and getting that $$ 0 = y f ( x ) ^ 2 - y f ( - x ) ^ 2 \text . $$ So, lets assume that $ y \ne 0 $ getting that $$ 0 = \big( f ( x ) - f ( - x ) \big) \big( f ( x ) + f ( - x ) \big) \text . $$ Now we do a two case analysis, 1) the function is even and 2) the function is odd. Lets start with the function being even then from \eqref{eqn2} we get that $$ 0 = y f ( x ) ^ 2 \text , $$ which of course implies that the function is just $ 0 $ . OK, now the odd case. Since the function is odd, $ f ( 0 ) = 0 $ . Then plugging $ x = 0 $ in \eqref{eqn1} we get that $$ f \left( y ^ 2 \right) = f ( y ) \text , $$ which implies that $ f $ is also an even function. Since $ f $ is both even and odd, it can only be $ 0 $ . Since we got that $ f $ is zero in both cases, the only solution to the equation is $ f ( x ) = 0 $ .","['contest-math', 'functional-equations', 'algebra-precalculus']"
3373387,Second order homogeneous variable coefficient ODE (periodic perturbation),"I have trouble solving the following ODE: $$y''(t)+[\alpha -i\beta \cos(\omega t)]y'(t)-\alpha^2 y(t)=0.$$ where $$\alpha\gg\beta, \alpha\gg\omega$$ I was wondering is there any approach to get an approximate solution to this ODE. 
I have tried the exploratory solution like this： $$y(x)=Y(x)*e^{-i\frac{\beta}{\omega}\sin\omega t}$$ , but it's not work.","['periodic-functions', 'ordinary-differential-equations', 'perturbation-theory']"
3373427,Generators of conformal isometries on two-dimensional manifolds,"I'm studying Conformal Field Theory via Paul Ginsparg's lecture notes and have a doubt regarding the Witt algebra. Let $(M,g)$ be a smooth manifold with a metric tensor $g$ which we shall assume to have Euclidean signature for simplicity. A conformal isometry is a diffeomorphism $\phi :M\to M$ such that $\phi_\ast g = \Omega^2 g$ for some conformal factor. We wish to understand the generators of such transformations, i.e., vector fields $X\in \Gamma(TM)$ whose flow are conformal isometries. This demands that the Lie derivative of $g$ be $$\mathfrak{L}_Xg=\alpha g.$$ Now, in components this is $$\nabla_\mu X_\nu+\nabla_\nu X_\mu=\alpha g_{\mu\nu}.$$ Now suppose that $D = 2$ and $g_{\mu\nu}=\delta_{\mu\nu}$ . The resulting equations are the Cauchy-Riemann equations: $$\partial_1 X_2 = -\partial_2 X_1,\quad \partial_1 X_1 = \partial_2 X_2. \tag{1}$$ This I can understand. The problem is that one usually takes a big leap from here and says: That $\phi : M\to M$ must be a holomorphic or anti-holomorphic function; If we introduce complex coordinates on $M$ , $(z,\bar{z}) : U\subset M\to \mathbb{C}^2$ this means $\phi$ is either a function only of $z$ or only of $\bar{z}$ . That the algebra of the generators of conformal isometries is generated by $$\ell_n=-z^{n-1}\partial_z,\quad \bar{\ell}_n=-\bar{z}^{n-1}\partial_\bar{z}.$$ Now I fail to see how do we get from Eq. (1) to the two conclusions. All I know is that if the function $f : \mathbb{C}\to \mathbb{C}$ written $f(x,y) = u(x,y)+i v(x,y)$ has $u,v$ satisfying the Cauchy-Riemann equations, then $f$ is holomorphic . It is not the case here. What satisfies the Cauchy Riemann equations are the components of vector fields generators of conformal isometries , not the maps $\phi : M\to M$ themselves. Also, I fail to see why $\ell_n,\bar{\ell}_n$ are the generators. So how do we get from the vector fields satisfying the Cauchy Riemann equations to the conclusion that the finite diffeomorphisms are holomorphic/anti-holomorphic and that such vector fields are spanned by the $\ell_n,\bar{\ell}_n$ ?","['complex-analysis', 'manifolds', 'conformal-geometry', 'mathematical-physics', 'differential-geometry']"
3373480,How can I find this limit applying squeeze theorem?,How can I evaluate this limit applying squeeze theorem? $$\lim_{x\to6}\frac{1-\sqrt{ 3-\sqrt{x-2}}}{x-6}$$ I found this limit with standart way: $\lim_{x\to6}\frac{1-\sqrt{ 3-\sqrt{x-2}}}{x-6}=\lim_{x\to6}\frac{\left(1+\sqrt{ 3-\sqrt{x-2}} \right)\times \left( 1-\sqrt{ 3-\sqrt{x-2}}\right)}{(x-6)\times \left( 1+\sqrt{ 3-\sqrt{x-2}}\right)}=\frac 18$ I want to write this limit using squeeze theorem.,"['limits', 'calculus', 'limits-without-lhopital']"
3373521,Derivative of matrices product AXA^T with respect to A. (Plus result when A is a vector.),"I want to know how to find an expression for $$\frac{\partial (AXA^T)}{\partial A}$$ where no information is given a priori on the dimensions of $A$ and $X$ . The question is related to machine learning but I am not given any additional details on the nature of the matrices; I am only given the result: $$\frac{\partial (AXA^T)}{\partial A}=A(X+X^T)$$ (in Andrew's answer below it is shown this is only the result if A is size $(1\times k)$ , i.e. a row vector) I have seen similar questions on the forum and was trying to approach this by differentiating the given product: \begin{align}\mathrm{d}(AXA^T)&=\mathrm{d}(AX)A^T+AX\mathrm{d}(A^T)= \left[ \mathrm{d}AX+A\mathrm{d}X \right]A^T+AX(\mathrm{d}A)^T=\\
&=\mathrm{d}AXA^T+AX(\mathrm{d}A)^T+
A\mathrm{d}XA^T\end{align} Then setting $\mathrm{d}X$ to zero (since we are derivating with $X$ constant): \begin{align}
\partial(AXA^T)= \partial AXA^T+AX(\partial A)^T
\end{align} Here I get stuck because I am unable to express it in a way I can premultiply by $(\partial A)^{-1}$ and obtain my derivative. I have tried by attempting to transpose twice the second term on the right hand side to get \begin{align}\partial(AXA^T)= \partial AXA^T + \left((\partial A)X^TA^T\right)^T
\end{align} and thought maybe there are symmetry assumptions in the solution I was given, to finally lead to it. I have also seen quite similar results in the Matrix Cookbook (e.g. formulae 79 and 80), but they are not the same and  are given in index notation which is confusing me a little bit more; also I would like to actually learn how to calculate them since I have never come up with this kind of derivatives (with respect to matrices) and do not even know how exactly they are defined. I have also tried to proceed with the calculus rules (product rule of derivatives) but felt I was probably missing things and am not sure if they hold in their usual form here. I would appreciate your help in any of those questions. EDIT: The clarification given by the authors of this exercise is to just use the simple product rule (I am unsure if this is actually possible with matrices, at least without introducing any special products): \begin{align}
\frac{\partial (AXA^T)}{\partial A} = \frac{\partial A}{\partial A}XA^T+A\frac{\partial XA^T}{\partial A} = (XA^T)^T+AX=AX^T+AX=A(X+X^T)
\end{align} saying on the side that they have applied the property: $\frac{ \partial A}{\partial A}B= B^T$ , which according to them follows from $\left[ \frac{\partial A}{\partial A}B\right]_i=\frac{\partial \sum_{k=1}^n A_k B_k}{\partial A_i}=B_i$ , ""for the i-th element"". (I cannot see how this property follows from there either, and how these operations are performed that way with a just single index and with different-dimension matrices.)","['matrices', 'machine-learning', 'matrix-calculus', 'partial-derivative', 'derivatives']"
3373584,Derivative on $\mathbb{R}^n$,"Definition. Let $A \subset \mathbf{R}^{m} ;$ let $f : A \rightarrow \mathbf{R}^{n} .$ Suppose $A$ contains a
neighborhood of a. Given $\mathbf{u} \in \mathbf{R}^{m}$ with $\mathbf{u} \neq 0$ , define $$f^{\prime}(\mathbf{a} ; \mathbf{u})=\lim _{t \rightarrow 0} \frac{f(\mathbf{a}+t \mathbf{u})-f(\mathbf{a})}{t}$$ provided the limit exists. This limit depends both on a and on u; it is called the directional derivative of $f$ at a with respect to the vector $\mathbf{u}$ . Definition. Let $A \subset \mathbf{R}^{m},$ let $f : A \rightarrow \mathbf{R}^{n}$ Suppose $A$ contains $a$ neighborhood of $a$ . We say that $f$ is differentiable at $a$ if there is an $n$ by $m$ matrix $B$ such that $$\frac{f(\mathbf{a}+\mathbf{h})-f(\mathbf{a})-B \cdot \mathbf{h}}{|\mathbf{h}|} \rightarrow \mathbf{0} \quad \text{as} \quad \mathbf{h} \rightarrow \mathbf{0}$$ The matrix $B,$ which is unique, is called the derivative of $f$ at a; it is denoted $D f(\mathbf{a}) .$ EXAMPLE. Define $f : \mathbf{R}^{2} \rightarrow \mathbf{R}$ by setting $f(\mathbf{0})=0$ and $$
f(x, y)=x^{2} y /\left(x^{4}+y^{2}\right) \text { if }(x, y) \neq \mathbf{0}
$$ We show all directional derivatives of $f$ exist at $0,$ but that $f$ is not differen-
tiable at $0 .$ Let $\mathbf{u} \neq \mathbf{0}$ . Then $$\begin{aligned} \frac{f(0+t u)-f(0)}{t} &=\frac{(t h)^{2}(t k)}{(t h)^{4}+(t k)^{2}} \frac{1}{t} \text { if } u=\left[\begin{array}{l}{h} \\ {k}\end{array}\right] \\ &=\frac{h^{2} k}{t^{2} h^{4}+k^{2}} \\ \text { so that } & \text { if } k \neq 0 \\ f^{\prime}(0 ; \mathbf{u}) &=\left\{\begin{array}{ll}{h^{2} / k} & {\text { if } k \neq 0} \\ {0} & {\text { if } k=0}\end{array}\right.\end{aligned}$$ Thus $f^{\prime}(\mathbf{0} ; \mathbf{u})$ exists for all $\mathbf{u} \neq \mathbf{0}.$ My Question: How can I show the function $f$ is not differentiable at $0$ ? How can I show $f^{\prime}(0 ; \mathbf{u})$ is not a linear function of $\mathbf{u}$ ? Can you help? Thanks...","['multivariable-calculus', 'derivatives', 'real-analysis']"
3373585,Partial Differential Equations - Characteristic Curves,"I am going through a book on partial differential equations, and am slightly confused by this paragraph: Consider the equation $\frac{dy}{dx}=p(x, y)$ . a) Think of solutions to this equations as curves in the $xy$ plane. 
  The equation is telling us that the slop of the tangent line to a
  solution curve at the point $(x_0, y_0)$ is equal to $p(x_0, y_0)$ . b) We can also say that the solution curve at the point $(x_0, y_0)$ is changing in the direction of the vector $(1, p(x_0, y_0))$ . Where does the vector $(1, p(x_0, y_0))$ come from? Is this simply the gradient?","['partial-derivative', 'slope', 'derivatives', 'characteristic-functions']"
3373603,How did we discover the method of solving the linear 1st order ODE?,"I have the first order linear ode in the standard form which I'd like to solve: $ \frac{d y}{d x}+P(x) y=f(x) $ The solution in the book I'm reading (A First Course in Differential Equations with Modeling Applications by Dennis Zill 10th edition) is as follow: The method for solving the differential equation above hinges on a
  remarkable fact that the left hand side of the equation can be recast
  into the form of the exact derivative of a product by multiplying both
  sides of the equation by a special function $\mu(x)$ . It is relatively
  easy to find the function $\mu(x)$ because we want: $ \frac{d}{d x}[\mu(x) y]=\mu \frac{d y}{d x}+\frac{d \mu}{d x} y=\mu\frac{d y}{d x}+\mu P y $ The equality is true provided that: $\frac{d \mu}{d x}=\mu P$ The last equation can be solved by separation of variables.
  Integrating $\frac{d \mu}{\mu}=P d x \quad \text { and solving } \quad \ln|\mu(x)|=\int P(x) d x+c_{1}$ gives $\mu(x)=c_{2} e^{\int P(x) d x}$ . Even though there are an
  infinite choices of $\mu(x)$ (all constant multiples of $e^{\int P(x) d x}$ ), all produce the same desired result. Hence we can simplify
  life and choose $c_{2}=1$ . The function $\mu(x)=e^{\int P(x) d x}$ is called an integrating
  factor for the differential equation. Here is what we have so far: We multiplied both sides of the DE by the
  integrating factor and, by construction, the left-hand side is the
  derivative of a product of the integrating factor and y: $\begin{aligned} e^{\int P(x) d x} \frac{d y}{d x}+P(x) e^{\int P(x) d x} y &=e^{\int P(x) d x} f(x) \\ \frac{d}{d x}\left[e^{\int P(x) d x} y\right] &=e^{\int P(x) d x} f(x) \end{aligned}$ Finally, we discover why $\mu(x)$ is called an integrating factor. We
  can integrate both sides of the last equation, $e^{\int P(x) d x} y=\int e^{\int P(x) d x} f(x)+c$ and solve for y. The result is a one-parameter family of solutions of
  the DE: $y=e^{-\int P(x) d x} \int e^{\int P(x) d x} f(x) d x+c e^{-\int P(x) d x}$ I have a few points that I couldn't understand in this method: ""The method for solving the differential equation above hinges on a remarkable fact that the left hand side of the equation can be recast into the form of the exact derivative of a product by multiplying both sides of the equation by a special function $\mu(x)$ "". Where did this fact come from and why? Was this method done by construction? It wasn't clear to me how could this method be discovered for the first time, like is it intuitive? What should you have in mind in order to do the construction done in this proof? i.e. what was the motivation to multiply the DE by $\mu(x)$ ? In summary, I did not understand the first few steps of this proof and why we chose to do it that way. In practice, while solving linear DE using this method we tend to multiply the whole equation by $\mu(x)$ and then set the left hand side of the DE equal to $ \frac{d}{d x}[\mu(x) y]$ ,  and then find the solution. So I want to know why we do that procedure in the first place. P.S: It would be nice if you can give me a little bit of history about who developed this method and when did he do it.","['proof-verification', 'ordinary-differential-equations']"
3373697,When are the eigenvalues of a matrix containing all squared elements irrational/rational?,"Forgive me in advance if any of this is trivial. After looking at many 2x2 matrices it seems that if all of the elements in matrix are unique squared integers then the eigenvalues are irrational. So I tried to investigate this: $\det \begin{pmatrix} \lambda -a^2 & b^2\\ c^2& \lambda -d^2\end{pmatrix}= \lambda^2 -(a^2+d^2)\lambda + (a^2d^2-c^2b^2)$ after applying the quadratic formula this gives a radical of, $\sqrt{a^4+4b^2c^2-2a^2d^2+d^4}$ If the stated observation is true, is there a way to show that this is irrational? Furthermore it looks like on the surface that for 3x3 matrices the eigenvalues for a matrix containing all  unique squared entries that the eigenvalues will also be irrational. Are either of these statements true? Is there a generalization of this for an nxn matrix? Edit: I'm not entirely sure I derived the radical correctly, but I'd still like to have some direction on the questions above also I'd like to examine cases where the eigenvalue is not zero Example: \begin{pmatrix}
2^2 & 4^2\\ 
3^2 & 6^2
\end{pmatrix} has eigenvalues 40 and 0. Edit 2: still looking for rational eigenvalues of a $3x3$ have been with imposed restrictions and nonzero eigenvalues/entries.","['matrices', 'number-theory', 'linear-algebra']"
3373712,How can three vectors be orthogonal to each other?,"In a scenario, say that: Vectors $\mathbf{U}$ , $\mathbf{V}$ and $\mathbf{W}$ are all orthogonal such that the dot product between each of these $(\mathbf{UV}\;\mathbf{VW}\;\mathbf{WU})$ is equal to zero. I imagine that for any potential vector space $\mathbf{R}$ this would only be possible in two situations. 1) $\mathbf{U}$ , $\mathbf{W}$ and/or $\mathbf{V}$ is the zero vector. 2) $\mathbf{U}=(1, 0, 0)$ , $\mathbf{V} = (0, 1, 0)$ and $\mathbf{W} = (0, 0, 1)$ . Is there any other situation where three vectors are all orthogonal to each other?","['orthogonality', 'linear-algebra', 'vectors']"
3373722,Is there a continuous function such that $f(f(x)) = g(x)$?,"Let $g:\mathbb{R}\to\mathbb{R}$ be strictly increasing, continuous, and satisfy $g(x) > x$ and $\lim\limits_{x\rightarrow -\infty}g(x) = -\infty $ . 
Is there a continuous function $f :\mathbb{R} \rightarrow \mathbb{R} $ such that $f(f(x)) = g(x)$ ? I have noted a few properties about a candidate $f$ ; one must have (1) $f$ is injective (2) $f$ is strictly increasing or strictly decreasing (3) $f$ is surjective We also have that $g$ has a continuous inverse $g^{-1}$ . The following maybe useful; We can define an equivalence relation $\sim$ on $\mathbb{R}$ where $x \sim y$ if $y =g_k(x)$ (here $g_{m+1}(x) = g(g_{m}(x))$ , $g_{m-1}(x) = g^{-1}(g_m(x))$ , $g_{0}(x) = x$ ). One can also observe that for a candidate $f$ that if $f(a) = b$ then $a \nsim b$ as if $f(a) = g_k(a)$ then $f(f(a)) = f(g_k(a))$ or $g_1(a) = f(g_k(a))$ ; hence $f(g_1(a)) = g_{k+1}(a)$ ; thus by two-sided induction we must have for $u \in \mathbb{Z} $ that $f(g_u(a)) = g_{k+u}(a)$ ; hence $f(g_k(a)) = g_{2k}(a)$ but from above $f(g_k(a)) = f(f(a)) = g(a)$ ; hence $g(a) = g_{2k}(a)$ which is impossible as $g$ has no fixed points.","['continuity', 'functional-equations', 'real-analysis']"
3373731,Twin Primes Conjecture and related problems,"I am wondering if my approach below, based on probability theory and thus heuristic (it is by no means a proof) is new and worth pursuing. Applications are numerous. Let $S_f$ be a random set of positive integers defined as follows: the positive integer $n$ belongs to $S_f$ with probability $f(n)$ . In other words, for each integer $n$ , generate a uniform deviate $U_n$ on $[0, 1]$ . If and only if $U_n < f(n)$ then add $n$ to $S_f$ . The deviates $U_n$ are independent. Now consider $f(n) = \frac{1}{\log n}$ if $n > 2$ , and $f(1)= f(2) = 0$ . With this particular choice of $f$ , the set $S_f$ has the same density as the set of prime numbers, and behaves in a very similar way. Let us define the following functions (actually, they are random variables): $M(n)$ is the number of elements in $S_f$ that are smaller than $n$ . $T(n)$ is the number of twin pairs $(k, k+1)$ with both $k, k+1 \in  
   S_f$ and $k \leq n$ . The twins in $S_f$ play the same role as twin primes. It is is easy to prove that $M(n)$ (its expectation) is asymptotically equal to the logarithm integral $\mbox{Li}(n)$ . The same is true for the prime number counting function. Furthermore, the prime numbers themselves is a particular case of $S_f$ , a particular realization of the random set $S_f$ for the same function $f$ . (you need to ignore even integers in the construction process, but other than that, it's straightforward.) Then we also have: $$\mbox{E}[T(n)] \sim C\sum_{k=1}^{n-1} f(k)f(k+1)\sim C'\int_2^n \frac{dt}{(\log t)^2}  .$$ Here E denotes the expectation. The same result is conjectured to be true for twin primes. It is easy to show that there are infinitely many twin numbers in $S_f$ , and assess whether the series consisting of the reciprocals of these twins, converge. The same arguments could be applied to twin primes, but of course, it would no constitute a proof, just some heuristic argumentation. Are there any useful references related to the approach discussed here? Update : I plan on using the same methodology for the Goldbach conjecture, and in particular to obtain an asymptotic value for the expected number of ways an even number $n$ can be represented as a sum of two primes. With my notation, it should be asymptotically equivalent to $$\sum_{k=1}^{\lfloor n/2 \rfloor} f(k)f(n-k) \sim \frac{n}{2(\log n)^2}.$$ See also here and here .","['number-theory', 'probability-theory', 'prime-numbers', 'sequences-and-series']"
3373795,"$a,b$ are conjugate if every $f(a),f(b)$ are","Let $G$ be a finite group, let $a,b\in G$ . For every homomorphism $f$ from $G$ to some symmetric group $S_n$ , we are given that $f(a)$ and $f(b)$ are conjugate in $S_n$ . Prove or give a counterexample to disprove that $a,b$ are conjugate in $G$ . Here is what i have thought so far: We know that an even permutation $\sigma$ 's conjugate class in $S_n$ are consist of even permutations. And if we consider the conjugate class of $\sigma$ in alternating group $A_n$ , one of two things will happen, either conjugate class doesn't change or it splits into two conjugate classes. For example there exist two 5-cycles $a,b$ in $A_5$ don't conjugate to each other. But they are conjugate in $S_5$ . So do we have for every $f:A_5\to S_n$ , $f(a)$ and $f(b)$ are conjugate?","['group-theory', 'abstract-algebra']"
3373797,Prove the Rational Limit Theorem.,"Rational Limit Theorem. For $f(x, y) =\frac{|x|^a|y|^b}{|x|^c+|y|^d}$ , with $a, b, c, d$ positive, $$\lim_{(x,y)→(0,0)}f(x, y) \text{ exists and equals zero}\Leftrightarrow \frac{a}{c}+\frac{b}{d}>1.$$ We will break this down into three parts:  first proving one direction by our techniques to show limits don’t exist, then using a famous inequality to help prove the other direction. As a guideline, each proof can be written in two or three lines. $1.$ Show that if $\frac{a}{c}+\frac{b}{d}≤1$ , then the limit does not exist. For the next two problems, you are free to use the following inequality: Young’s Theorem. For positive real numbers $w$ , $z$ and any $0≤t≤1$ , $$w^tz^{1−t}≤tw+ (1−t)z$$ $2.$ Show  that  if $\frac{a}{c}+\frac{b}{d}=  1$ ,  where $a, b, c, d$ are  all  positive,  then $f(x, y)≤1$ for  all $(x, y)∈\mathbb{R^2}\backslash\{(0,0)\}$ . $3.$ Show that if $\frac{a}{c}+\frac{b}{d}>1$ , then $$\lim_{(x,y)→(0,0)}f(x, y) = 0.$$ Before I start to prove this, i'm thinking why $1-3$ together implies $$\forall a,b,c,d>0,\lim_{(x,y)→(0,0)}\frac{|x|^a|y|^b}{|x|^c+|y|^d} \text{ exists and equals zero}\Leftrightarrow \frac{a}{c}+\frac{b}{d}>1$$ "" $1.$ "" looks like the contrapositive of direction "" $\Rightarrow$ "", actually, "" $1.$ "" implies "" $\Rightarrow$ "", but "" $\Rightarrow$ "" doesn't implies "" $1.$ "", this makes the statement stronger, which is good. To show "" $1.$ "" implies "" $\Rightarrow$ "" Let $f(x,y)=\frac{|x|^a|y|^b}{|x|^c+|y|^d}\text{ and },a,b,c,d > 0$ , assume "" $1.$ "" we have $$\frac{a}{c}+\frac{b}{d}\le1\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ not exists}$$ $$\Rightarrow(\frac{a}{c}+\frac{b}{d}\le1 \wedge \lim_{(x,y)→(0,0)} f(x,y)=0)\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ not exists}$$ Since $((a \wedge b)\rightarrow c)\Leftrightarrow(a\rightarrow(\neg b \vee c))$ we have: $$\Leftrightarrow\frac{a}{c}+\frac{b}{d}\le1\rightarrow (\lim_{(x,y)→(0,0)} f(x,y) \text{ not exists} \vee \lim_{(x,y)→(0,0)} f(x,y)\neq0)$$ Which is the contrapositive of "" $\Rightarrow$ "", so this make sense $\dots$ logically. $\tag*{$\square$}$ Then I suppose "" $2.$ "" and "" $3.$ "" together should implies direction "" $\Leftarrow$ "". "" $2.$ "" states the following: $$\forall a,b,c,d>0, \frac{a}{c}+\frac{b}{d}=1\rightarrow \forall (x,y)\in\mathbb{R^2}\backslash\{(0,0)\},f(x,y)\le 1$$ "" $3.$ "" says that: $$\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)}f(x, y) = 0.$$ And together should implies "" $\Rightarrow$ "": $$\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)} f(x,y) \text{ exists} \wedge \lim_{(x,y)→(0,0)} f(x,y)=0$$ Proof. Assume "" $3.$ "" have: $$\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)}f(x, y) = 0.$$ Since $$\lim_{(x,y)→(0,0)} f(x,y)=0\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ exists}$$ Directly implies "" $\Rightarrow$ "" $$\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)} f(x,y) \text{ exists} \wedge \lim_{(x,y)→(0,0)} f(x,y)=0\tag*{$\square$}$$ $1.$ $$\text{WTS }\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}\le1\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ not exists}$$ Proof. Let $a,b,c,d\in\mathbb(0,\infty)\cap{\mathbb{R}}, S=\mathbb{R^2}\backslash\{(0,0)\}$ Assume $$\frac{a}{c}+\frac{b}{d}\le1$$ Show the limit does not exist Let $x=t^\frac{1}{c}, y=mt^\frac{1}{d}$ where $m\ge0$ , we have the following $$\lim_{(x,y)→(0,0)}f(x,y)=\lim_{t→0}\frac{|t^\frac{1}{c}|^a|mt^\frac{1}{d}|^b}{|t^\frac{1}{c}|^c+|mt^\frac{1}{d}|^d}$$ Try approch $t$ from right side, so everything is positive, then we have: $$\lim_{t→0^+}\frac{mt^{\frac{a}{c}+\frac{b}{d}}}{(m+1)t}
=\lim_{t→0^+}\frac{1}{m+1}t^{\frac{a}{c}+\frac{b}{d}-1}$$ Consider two cases: Case 1: $\frac{a}{c}+\frac{b}{d}=1$ Have $$\lim_{t→0^+}\frac{1}{m+1}t^{0}=\frac{1}{m+1}$$ The limit depend on the value of $m$ , that implies limit d.n.e Case 2: $\frac{a}{c}+\frac{b}{d}<1$ Have $\frac{a}{c}+\frac{b}{d}-1$ is negative, implies limit diviges, that $$\lim_{t→0^+}\frac{1}{m+1}t^{\frac{a}{c}+\frac{b}{d}-1}=\infty$$ Therefore in both cases limit d.n.e $\tag*{$\square$}$ $2.$ $$\text{WTS }\forall a,b,c,d>0, \frac{a}{c}+\frac{b}{d}=1\rightarrow \forall (x,y)\in\mathbb{R^2}\backslash\{(0,0)\},f(x,y)=\frac{|x|^a|y|^b}{|x|^c+|y|^d}\le 1$$ Maybe I can use Young's Theorem here $$\forall w,z\in\mathbb{R},t\in[0,1]\cap\mathbb{R}, w^tz^{1−t}≤tw+ (1−t)z$$ Let $p=\frac{a}{c}>0,q=\frac{b}{d}>0,r=|x|^c,s=|y|^d$ , have $$\frac{|x|^a|y|^b}{|x|^c+|y|^d}=\frac{(|x|^c)^{\frac{a}{c}}(|y|^d)^{\frac{b}{d}}}{|x|^c+|y|^d}=\frac{r^ps^q}{r+s}=\frac{r^ps^{1-p}}{r+s}s^{p+q-1}=\frac{r^ps^{1-p}}{r+s}$$ $$\le\frac{pr+(1-p)s}{r+s}\le(\frac{pr}{r}+\frac{(1-p)s}{s})=1\tag*{$\square$}$$ $3.$ $$\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)}f(x, y) = 0.$$ Proof. Let $a,b,c,d\in\mathbb(0,\infty)\cap{\mathbb{R}}$ Assume $\frac{a}{c}+\frac{b}{d}>1$ Show $\lim_{(x,y)→(0,0)} \frac{|x|^a|y|^b}{|x|^c+|y|^d}=0$ Let $p = \frac{ad}{c}-d+b$ , that $\frac{a}{c}+\frac{b-p}{d}=1$ , we can apply 2) on this Implies $0\le\frac{|x|^a|y|^{b-p}}{|x|^c+|y|^d}\le1$ Have $$\lim_{(x,y)→(0,0)} \frac{|x|^a|y|^b}{|x|^c+|y|^d}$$ $$=\lim_{(x,y)→(0,0)} |y|^p\frac{|x|^a|y|^{b-p}}{|x|^c+|y|^d}=0\tag*{$\square$}$$","['limits', 'multivariable-calculus', 'proof-verification', 'inequality']"
3373858,Directional derivative of the $\ell_1$ norm,"The following paragraph is taken from Nocedal and Wright's Numerical Optimization (p. 628 of the second edition). Consider for instance the $\ell_1$ norm function $f(x)=\|x\|_1$ . We have from the definition (A.51) that $$
D(\|x\|_1;p)
=\lim_{\epsilon\to0}\frac{\|x+\epsilon p\|_1-\|x\|_1}\epsilon
=\lim_{\epsilon\to0}\frac{\sum_{i=1}^n|x_i+\epsilon p_i|-\sum_{i=1}^n|x_i|}\epsilon.
$$ If $x_i>0$ , we have $|x_i+\epsilon p_i|=|x_i|+\epsilon p_i$ for all $\epsilon$ sufficiently small. If $x_i<0$ , we have $|x_i+\epsilon p_i|=|x_i|-\epsilon p_i$ , while if $x_i=0$ , we have $|x_i+\epsilon p_i|=\epsilon|p_i|$ . Therefore, we have $$
D(\|x\|_1;p)
=\sum_{i|x_i<0}-p_i+\sum_{i|x_i>0}p_i+\sum_{i|x_i=0}|p_i|,
$$ so the directional derivative of this function exists for any $x$ and $p$ . How do they obtain that $|x_i+\epsilon p_i|=\epsilon|p_i|$ when $x_i=0$ ? It seems that it should be $|x_i+\epsilon p_i|=|\epsilon||p_i|$ . Then the limits when $\epsilon\downarrow0$ and $\epsilon\uparrow0$ do not coincide and hence the derivative does not exist. Is it true that the $\ell_1$ norm has directional derivatives for any $x$ and $p$ ? Any help is much appreciated!","['multivariable-calculus', 'derivatives']"
3373870,"Prove $R(a \times b) = Ra \times Rb$ given $R \in \mathcal{SO}(3)$ and $a,b \in \mathbb{R}^3$.","Here, $R$ is a proper rotation matrix, and $(\times)$ is the cross-product. I already found 3 ways of proving this, all have problems, and I am requesting an elegant approach. (1) Direct calculation, $R = R_{\mathbf{u},\theta}$ has an explicit expression, where $\mathbf{u}$ is the vector about which the rotation of $\theta$ is carried out. It is essentially possible to calculate both sides and compare. Inelegant. (2) Using antisymmetric matrices: $Ra \times Rb=S(Ra)Rb=RS(a)R^\top Rb=RS(a)b=R(a\times b)$ . My issue with this is that the equality I am trying to prove, is used to prove $RS(a)R^\top=S(Ra)$ . And so using this feels circular. (3) $Ra \times Rb=\|Ra\|\|Rb\|\sin(Ra,Rb)\mathbb{\hat{u}_1}$ and $a \times b=\|a\|\|b\|\sin(a,b)\mathbb{\hat{u}_2}$ . Here, essentially $\|Ra\|$ should equal $\|a\|$ since $R$ only affects orientation. Because the relative orientation does not change, the $\sin(\cdot)$ term should be equal. Likewise, $\mathbb{\hat{u}_1}$ and $\mathbb{\hat{u}_2}$ intuitively I know are equal but I am having a hard time expressing it. Lastly, I have no idea how to bridge $(a \times b)$ to $R(a \times b)$ . I intuitively see it, and perhaps $\det R = 1$ might be useful, but I feel it is hard to write. Please, a fourth approach is welcome, and insight is always appreciated.","['vector-spaces', 'matrices', 'solution-verification', 'linear-algebra', 'rotations']"
3373876,"Calculate the volume enclosed by the paraboloid $f(x,y)=1-\frac{1}{2}x^2-\frac{1}{2}y^2$","Calculate the volume enclosed by the paraboloid $f(x,y)=1-\frac{1}{2}x^2-\frac{1}{2}y^2$ and the plane $z=0$ , when $f(x,y)$ is defined in $D=[0,1]\times [0,1]$ . I used polar coordinates and I had the following integral, $$\int _{0}^{\frac{\pi}{2}} \int _{0}^{1} \left(1-\frac{1}{2}r^2\right)\,dr\,d\theta =\dfrac{3\pi}{16}$$ Is it right?","['multivariable-calculus', 'volume']"
3373884,Making sense of the complex exponential function,"I read in a book that if we want to make sense of the $e^z$ where $z=x+iy$ , we already know how to interpret $e^x $ , so the only thing we have to make sense of is $e^{iy}$ . For $e^{iy}$ to make sense it has to obey the calculus rule. So $e^{iy}$ should be defined as the solution to the initial value problem: $$\frac{d}{dx}e^{iy}=ze^{zt}; e^{z*0}=1$$ I don't understand why for the $e^{iy}$ to make sense should be defined as the solution to the initial value problem?","['calculus', 'ordinary-differential-equations']"
3373899,"How do I prove the limit $\frac{\sin(xy)}{\sqrt{x^2 + y^2}}$ as (x, y) approaches (0, 0) using $\epsilon - \delta$","I know that I can convert this limit to polar coordinates and solve the limit, but I want to see how I would do it using the $\epsilon - \delta$ definition of a limit. This is my work so far: We know that $$\left|{\frac{\sin(xy)}{\sqrt{x^2 + y^2}}} - 0\right| < \epsilon$$ and $$
\left| \sqrt{x^2 + y^2} \right| < \delta $$ Then, $$
\begin{align}
\left|\sin(xy)\right| &< \epsilon \left|\sqrt{x^2 + y^2}\right| \\
\frac{\left|\sin(xy)\right|}{\epsilon} &< \left|\sqrt{x^2 + y^2}\right|
\end{align}
$$ I am stuck here, as normally I would get an expression that matches $\delta$ , but here the signs are switched.","['limits', 'multivariable-calculus', 'epsilon-delta']"
3373975,Why does the growth rate of integers $n$ for which $\varphi(x) = n$ has no solutions increase suddenly after a point?,"A nontotient number is an even natural number $n$ for which $\varphi(x) = n$ has no solutions, where $\varphi(x)$ is the Euler totient function. The smallest nontotient numbers are $$14, 26, 34, 38, 50, 62, 68, 74, 76, 86, 90, 94, 98, 114, 118, 122, 124, 134, \ldots$$ I generated the $n$ -th nontotient number $T_n$ for all $n$ upto $10^2, 10^3, 10^4, 10^5, 10^6$ and and observed that the relation between $n$ and $T_n$ was almost perfectly linear. To test this intuition further, I generted the data for $n$ upto $10^7$ but this time, I observed that somewhere at about $n \approx 5.6 \times 10^6$ the linear relation breaks down and $T_n$ starts grown at a super linear rate i.e. faster than $n$ . Question 1 : What is known about the growth rate of $T_n$ ? Question 2 : What is happening at about $n = 5.6 \times 10^6$ when $T_n$ when starts to quickly deviate away from linearity?","['divisibility', 'number-theory', 'elementary-number-theory', 'computational-mathematics', 'prime-numbers']"
3374069,How to simplify the ratio $1 - \cos2x + i\sin2x \over 1 + \cos2x - i\sin 2x$,"The ratio is as follows: $$1 - \cos2x + i\sin2x \over 1 + \cos2x - i\sin 2x$$ I am unsure how to simplify this, as the numerator poses a problem as I try to multiply this equation by $\operatorname{cis}(2x)$ to get a real denominator.","['trigonometry', 'complex-numbers']"
3374091,Find $x \in \mathbb{R}$ such that $(x^2+p^2)/xp < -2$,"I'm trying to solve the following inequality for $x\in\mathbb{R}$ : $$\frac{x^{2}+p^{2}}{xp} < -2, $$ where $p$ is a real parameter.  I want to have my $x$ on the left side and everything else on the right side. The first step I would take is to multiply both sides of the inequality with $xp$ , however I'm not sure how to do so. Do I need to split my solution into 4 branches (1. where $x>0$ and $p<0$ , 2. where $x<0$ and $p>0$ , 3. where $x>0$ and $p>0$ and 4. where $x<0$ and $p<0$ ), or can I just simple split it into two branches, where $xp>0$ and where $xp<0$ ?","['algebra-precalculus', 'inequality']"
3374098,What is a practical use for this metric?,"Showing $\rho (x,y)=\frac{d(x,y)}{1+d(x,y)}$ is a metric That post has a more generalized form of a metric I occasionally see $d(x,y) = \frac{|x-y|}{1+|x-y|}$ . When would using this metric be useful exactly? It occasionally comes up when I study analysis, but I don't know why, I don't know what people use it for or what benefit it could ever bring over the standard metric. I've merely only seen it as an example of a metric in books or sites, but if so many sources mention it, then it's very unlikely that it's useless.","['general-topology', 'metric-spaces', 'analysis']"
3374121,"> Let $X$, $Y$, and $Z$ be three independent uniform random variables on $[0, 1]$. What is $P(XY < Z^2)$?","Let $X$ , $Y$ , and $Z$ be three independent uniform random variables on $[0, 1]$ . Compute the probability $P(XY < Z^2)$ . I used the following approach : Step 1 : Calculated the Probability distribution for $XY$ . It turns out to be $P(XY \leq K) = \frac 1K$ . Step 2 : Calculated the Probability distribution for $Z^2$ . It turns out to be $P(Z^2 \leq L) = P(-\root \of{L} \leq Z \leq \root \of{L}) = \root \of{L}$ . Step 3: Calculate the joint density function by multiplying the above functions and differentiating. I get $$f_{Q_1Q_2} (q_1,q_2) = \frac {-1}{2q_1^2\root\of{q_2}}$$ . where $Q_1 = XY$ , $Q_2 =Z^2$ . Step 4: Calculate the probability using the integral below. $$\int_{0}^{1}\int_{q_1}^{1}\frac{-1}{2q_1^2\root\of{q_2}}dq_2dq_1$$ There is definitely something wrong with this procedure. Any help will be appreciated.",['probability']
3374155,Prove that subsets $A$ where $A=f^{-1}(f(A))$ is sigma-algebra,"Let $f:X\to Y$ be a function. Show that $\mathscr{T}=\{A\in\mathscr{P}(X)\mid A=f^{-1}(f(A)) \}$ is a $\sigma$ -algebra on $X$ . It is immediate that $f^{-1}(f(\varnothing))=f^{-1}(\varnothing)=\varnothing$ . Let $A\in\mathscr{T}$ , then we know that $A=f^{-1}(f(A))$ (*) (or $f|_A$ is injective). I am having trouble with proving that $X\setminus A\in\mathscr{T}$ . The elementary set theory is boggling my mind. We cannot say that $f^{-1}(f(X\setminus A))=f^{-1}(Y\setminus f(A))$ , because for non-injective $f$ this does not work. I tried taking the complement at both sides of (*): $X\setminus A=X\setminus f^{-1}(f(A))=f^{-1}(Y\setminus f(A))$ , but I don't no how to get further. I need to use somewhere that $A=f^{-1}(f(A))$ . I think there is an easy thing I am overlooking. Could someone provide any help?","['elementary-set-theory', 'measure-theory']"
3374236,Limit question unknown function,"If $\lim_{x \rightarrow 0} f(x)+f(2x)=0$ , prove or disprove with example, that $\lim_{x \rightarrow 0} f(x)=0$ for any function $f(x)$ . f(x) can be a piecewise functions as well. I tried too disprove it considering several functions but I wasn't able to do so. So I guess that there statement is true but how do we prove it?","['limits', 'calculus', 'functions']"
3374241,If $A$ and $B$ are groups then prove that $A\times B\cong B\times A$,"My approach
If $f(a,b)$ where $a\in A$ and $b\in B$ be the transformation from $A\times B$ to $B\times A$ then $f(a,b)=(b, a)$ . I proved homorphism like this $f((a, b) +(a', b'))=f((a+a',b+b'))=(b+b',a+a')$ . $f(a,b)+f(a',b')=(b+b',a+a')$ . Is this correct or should I prove that $f((a, b). (a', b'))=f(a,b).f(a',b')$ . How do I prove onto and one one aren't those very trivial and how do I write the proof?","['direct-product', 'group-theory', 'group-isomorphism']"
3374263,How to solve the following ordinary differential equation?,"Consider the ODE $$y' + x = \sqrt{x^2+y}.$$ I have no idea how to solve this equation. I have tried several ways but neither of them worked. Can anybody help me? Many thanks. I have tried substituting $u=\sqrt{x^2+y}$ , as well as squaring both sides of the ODE.",['ordinary-differential-equations']
3374265,Counting primes in residue classes,"Suppose you are sorting all of the prime numbers between $1$ and some large number $N$ , from smallest to largest; into buckets which correspond to their residue classes modulo some prime $p_i$ . Now suppose you actually had two copies of the prime numbers between $1$ and some large number $N$ ; one to be sorted by residue classes modulo prime $p_i$ , and the other to be sorted by residue classes modulo prime $p_{i+1}$ . At each step in time you place the smallest prime number and its copy into the two buckets as instructed above. At what point in time can you be sure that, all of the buckets that correspond to residue classes of $p_i$ (excluding bucket $[0]_i$ ) contain more prime numbers than any of the buckets corresponding to residue classes of $p_{i+1}$ . Or can you never be sure?","['number-theory', 'modular-arithmetic', 'elementary-number-theory', 'prime-numbers']"
3374326,"Equilateral triangle ABC has side points M, D and E. Given AM = MB and $\angle$ DME $=60^o$, prove AD + BE = DE + $\frac{1}{2}$AB","I would like to ask if someone could help me with solving the following probelm. The triangle $ABC$ is equilateral. $M$ is the midpoint of $AB$ . The points $D$ and $E$ are on the sides $CA$ and $CB$ , respectively, such that $\angle DME=60^o$ Prove that $AD+BE = DE + \frac{1}{2}AB$ . Thank you in advance.","['triangles', 'geometry']"
3374386,Finding side length from two scalene triangles with common angle and side and ratio between sides. Trigonometry,"I have two scalene triangles with a common angle and side. I would like to find the length of $x_2$ .
I have all variables in green: the angles $\angle$ B, $\angle$ C, $\angle$ D, $\angle$ E and side ""length"". I also have the ratio $\frac{x_2}{x_1}$ .
Stumped on this trigonometry question. I have tried using Sine rule and solving simultaneously but always end up canceling out all my terms. In my head there are enough equations to solve for the variables but can't seem to figure out how I would do it.
Any help greatly appreciated.","['angle', 'geometry', 'triangles', 'trigonometry', 'computer-vision']"
3374399,Evaluate $\sum _{n=1}^{\infty } \sin \left(\pi \sqrt{n^2+1}\right)$,"How can one prove $$\sum _{n=1}^{\infty } \sin \left(\pi  \sqrt{n^2+1}\right)=-\frac{1}{2}\pi Y_1(\pi )-\int_0^{\infty } \exp ^{\frac{\pi}{2}  \left(t-t^{-1}\right)} (\theta(2 \pi  t)-1) \, dt$$ Here $\theta$ denotes Theta function of the third kind. Correpsonding cosine case is solved here but not so helpful. Any help will be appreciated.",['sequences-and-series']
3374500,Expected Value of random variables using standard deviation?,"Question: The manager of a popular seafood restaurant estimates that the daily consumption of shrimp is normally distributed with a mean of 15 pounds and a standard deviation of 2.7 pounds. He makes it a point to buy the right amount of shrimp everyday to prevent waste and shortage. Calculate the amount of shrimp that should be bought daily so that it meets demand 92% of the days. I'm under the impression to use the mean/expected value equation: ExP(x), but I'm not sure which values to place into the formula (my textbook doesnt use standard deviation in any of the examples). Or, is this question using binomial distribution probability? Can anyone help me better understand how to compute this? The answers are out of: a.  12.44
b.  19.43
c.  18.93
d.  17.57
e.  10.57","['expected-value', 'statistics', 'random-variables']"
3374748,"Calculate partial derivatives $f_x(x,y)$ and $f_y(x,y)$ if $f(x,y)=\int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t)dt$","Calculate partial derivatives $f_x(x,y)$ and $f_y(x,y)$ if $f(x,y)=\int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t)dt$ Idea: if $G'(t)=g(t)$ for all $t$ . Then $\int_y^xg(t)dt= G(x)-G(y)$ and $\int_x^yg(t)dt=G(y)-G(x)$ . Hence $f(x,y)=\int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t)dt=\int_{G(x)-G(y)}^{G(y)-G(x)}g(t)dt=G(G(y)-G(x))-G(G(x)-G(y))$ . Then $f_x(x,y)=-G'(G(y)-G(x))G'(x)-G'(G(x)-G(y))G'(x)=-g(x)(g(\int_x^yg(t)dt)+g(\int_y^xg(t)dt))?$ Is correct my idea? Thanks...","['analysis', 'multivariable-calculus', 'calculus', 'partial-derivative', 'derivatives']"
3374750,How to calculate the Tangent space of a line in $\mathbb{R}^2$?,"Calculate the Tangent Space of a line $y=mx; m\in \mathbb{R}$ in a point $p=(x,y)$ I know that a line that passes through the origin is a manifold $M$ and the chart is $(M,\varphi); \varphi(x)=(x,mx)$ . I know that $T_pM$ is generated by one element, but i don't know how to find the element $d/dx^i$ I appreciate your help.",['differential-geometry']
3374787,"True/false : If $f_n(x)\to 0$ almost everywhere , then $\int_{a}^{b}f_n(x)dx \to 0$","Is the following sttaement is true/false ? let { $f_n$ } be sequence of integrable functions defined on an interval $[a,b]$ . If $f_n(x)\to 0$ almost everywhere , then $\int_{a}^{b}f_n(x)dx \to 0$ My attempt : i know that $m(\mathbb{Q}) = 0$ and $f(\mathbb{Q}) =0$ is almost everywhere because the $f$ is not going to $0$ and measure is $0$ in $[0,1]$ Now consider the function $f_n(x)$ , on the interval $[0,1]$ . Let $\{r_1, r_2, \dots\}
$ is the enumerarion of rationals on $[0,1]$ . $$f_n(x) = \begin{cases} 1 & \text{x = $r_1, r_2, \dots r_n$} \\ 0 & \text{elsewhere} \end{cases}$$ Thus $f_n(x) \rightarrow f(x)$ , which is $0$ almost everywhere $$f(x) = \begin{cases}1 & x \in \mathbb{Q}\cap [0,1] \\0 & x \in [0,1] - \mathbb{Q}\cap [0,1]\end{cases}$$ But $\int_0^1 f(x)$ does not exists. so the given statement is false Is its true ?","['measure-theory', 'real-analysis']"
3374801,Book recommendation: Differential equations with differential geometry,"I have been doing some self-study of differential equations and have finished Habermans' elementary text on linear ordinary differential equations and about half of Strogatz's nonlinear differential equations book. The thing that I am noticing is just how much these text avoid engaging the underlying differential geometry/topology of phase spaces. It also feels like the further I got in this differential equations, the more important it is to understand the underlying differential topology--for instance understanding Hamiltonian systems and symplectic manifolds, etc. Indeed, the only text that I have seen that seems to engage the differential topology of phase spaces seems to be Arnold's 1973 book on Ordinary Differential Equations. This seems to be a really good book. The challenge is that Arnold can be a bit terse sometimes, so I was hoping to find a book to supplement Arnold's text. I have enough background in differential topology by watching Fredric Schuller's lectures and then working through some of Renteln and John Lee's books. Hence, I was hoping to find a book that elaborates on the differential topology side of differential equations. So all of these topics about vector fields on a manifold are fair game. Now I looked at Hirsch and Smale 1974, but this did not really get into the differential topology stuff. Perko's book was also pretty terse and did not systematically develop the topology. If anyone has any good recommendations, that would be appreciated. Thanks.","['manifolds', 'differential-topology', 'ordinary-differential-equations']"
3374804,Alternative axioms for groups.,"The usual axioms I've seen for a group are: associativity; existence of two-sided identity; existence of two-sided inverses for all elements. $$\forall a,b,c\in G: a\left(bc\right)=\left(ab\right)c$$ $$\exists e\in G, \forall a\in G: ae=a=ea$$ $$\forall a\in G \exists a'\in G: aa'=e=a'a$$ I recently came across a different axiomatisation, and there were no proofs of equivalence. They were: associativity; existence of left-identity; existence of left-inverses. $$\forall a,b,c\in G: a\left(bc\right)=\left(ab\right)c$$ $$\exists e\in G, \forall a\in G: ea=a$$ $$\forall a\in G, \exists a'\in G: a'a=e$$ Are these equivalent? I kind of doubt it, since we have associative semigroups with left but not right identities, but maybe the left-inverses part changes things. There was a proof that given these axioms, a left-inverse is a right-inverse, and hence that the original inverses axiom is proven, but what about right-identity? Proof: 
Let $g\in G$ then $g$ has a left inverse, call it $g'\in G$ and this too has a left inverse, call it $g''\in G$ . Then, $g'g=e$ , $g''g'=e$ and so $$gg'=egg'=g''g'gg'=g''g'=e$$ so $g'$ is the right-inverse of $g$ also.","['group-theory', 'axioms']"
3374830,"Dominated Convergence Theorem with ""Almost Surely"" replaced by ""Convergence in Probability"" [duplicate]","This question already has answers here : Generalisation of Dominated Convergence Theorem (4 answers) Closed 4 years ago . I want to show that if $\{X_n\}$ is a sequence of random variables such that: (1) $\exists X$ (measurable) such that $X_n \xrightarrow{P} X$ (2) $\exists Y$ with $E(|Y|) < \infty$ such that $|X_n| \le Y \quad \forall n \in \mathbb{N}$ Then $E(X_n) \rightarrow E(X)$ and $E(|X_n - X|) \rightarrow 0$ How do I go about showing this?  This is of course the standard dominated covergence theorem changing almost sure convergence to $X$ to convergence in probability.  The approach to proving the standard theorem involves showing that $X$ is integrable and subsequently bounding $E(|X_n - X|)$ , but I don't even understand how to do that part in this new set of assumptions.  Can someone help with this?  Once that part is done then the theorem is complete I believe?  I'm aware this might be a duplicate but I cannot understand these things very well and I'm very confused.  Thanks for any help you can give!","['measure-theory', 'convergence-divergence', 'probability-theory']"
3374838,de Moivre's Formula for Odd $n$,"I am trying to show that: $$(\sin(\theta) +i\cos(\theta))^n = \begin{cases} &\sin(n \theta) + i \cos(n\theta) & \quad \text{when $n = 4m+1$}\\ -&\sin(n \theta) - i \cos(n\theta) & \quad \text{when $n = 4m+3$} \end{cases} $$ The hint that I am given is to use the fact that $\sin$ and $\cos$ are the same if you translate one horizontally by $\pi/2$ radians, but I am not sure how this helps. Any hints, or solutions, would be appreciated.","['trigonometry', 'complex-numbers']"
3374843,Positive definite when multiplying by two matrices,"I have a square $n \times n$ matrix $\mathbf{X}$ and a non-square $n \times m$ matrix $\mathbf{A}$ . The product $\mathbf{M} = \mathbf{A}^T \mathbf{X} \mathbf{A}$ gives a $m \times m$ matrix. If I know that $\mathbf{X}$ is positive definite (or positive-semi definite), is there any way to know $\mathbf{M}$ is positive definite (or positive-semi definite). If it is not possible, is there any constraints we can make on $\mathbf{A}$ (or maybe $\mathbf{X}$ ) to have this property? Any pointers will be helpful. Thanks!","['positive-semidefinite', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'positive-definite']"
3374856,How to calculate $\int_0^1\frac{\ln^2x\ln^2(1-x^2)}{1-x^2}\ dx$?,"Using the derivative of beta function, find $$I=\int_0^1\frac{\ln^2x\ln^2(1-x^2)}{1-x^2}\ dx$$ setting $x^2=y$ gives $$I=\frac18\int_0^1\frac{\ln^2y\ln^2(1-y)}{\sqrt{y}(1-y)}\ dy=\frac18\left.\frac{\partial^4}{\partial a^2\partial b^2}\text{B}(a,b)\right|_{a\mapsto 1/2\\b\mapsto0^{+}}$$ Any good software that can find the 4th derivative and also gives the final result? Wolfram fails to calculate it (or maybe I do not know how to use it well) and when I tried to do it manually, some terms involve $\psi(b)$ and if we take the limit, then $\psi(0)$ is undefined and even if I take the limit of $\psi(b)$ together with other terms, still undefined. I do not know how to avoid this problem as I am not experienced with the beta function. Thank you. Note: Solution should be done without using harmonic series.","['integration', 'real-analysis', 'harmonic-numbers', 'calculus', 'beta-function']"
3374862,The minimum $N \times N$ square covering problem for $1 \times 4$ shaped tetrominoes,"OK, so me and my friend are working on a problem in which you do the opposite to trying to stuff as many of the I shape tetrominoes in a square as possible. Trying to find the smallest number of I shape tetrominoes that you have to place in the square such that another I shaped tetromino cannot be placed in the square. So I define $I_n$ to be the sequence such that the rules of the above problem are satisfied. My friend made a program that finds the values of this sequence. So far we have found that $I_1 = 0, \ I_2 = 0, \ I_3 = 0, \ I_4 = 4, \ I_5 = 4, \ I_6 = 6, \ I_7 = 7, \ I_8 = 9.$ The problem is that we have got no proofs for any of the cases where $n > 4$ . At first I tried using the pigeonhole principle to try to prove a few cases. For example, when $n = 5$ my argument went a little bit like this. Lets assume WLOG that an I shaped tetromino was placed in the first row of the square and that a second I shaped tetromino was placed in another row. Now we just place the third shape in the suquare, then that means that there are $5*5 - 4*3 = 13$ remaining free squares. Now using the pigeonhole principle and the assumption of the placement of the first 2 shapes we we get that $13 - 2 = 11$ of the squares are distributed on the 3 rows in which the first $2$ I shapes were not placed. Now by the pigeonhole principle we get that one of the rows must have $\lceil{\frac{11}{3}}\rceil = 4$ free squares. And this is the point where the proof breaks down. I neglected the fact that you can place other I shapes such that even if the row has 4 free squares you still cannot place a new I shape tetromino in it. So I ask anyone if they have any good ideas that can progress our investigation. P.S.  If you have proofs for your assertions, then please write them out :) Here I have added visual examples starting from $n = 5$ : $n = 5$ $n = 6$ $n = 7$ $n = 8$ I and my friend are also looking into the same problem with square tetrominoes.
I have a general formula for the squares: $\lceil\frac{n-1}{3}\rceil^2$ . My proof starts off by showing how we can do a minimal covering of a $5\times n$ rectangle. We notice that if we always put new squares in such that they are one cell away from the boundaries of the rectangle and from each other, then looking through a few cases we can come to the conclusion that the formula is $\lceil\frac{n-1}{3}\rceil$ . Now the same principe of being one cell apart from everything else when it can be done also applies for the $n\times n$ case. So then the smallest count would be attained by a certain configuration (I called it the perfect configuration) of the squares. Now it is not hard to notice that the number of squares in the $n \times n$ case is simply the square of the $5 \times n$ case. My proof basically went along the lines that if you try to remove a square or displace it from the position in which it would be in the perfect configuration, then you could always add a new square, thus gaining a covering that takes up more squares then the perfect configuration one so, therefore the perfect configuration covering must be the minimal. P.S. If anyone knows any good sources or literature concerning this topic, then please do share them. Thank you!","['graph-theory', 'polyomino', 'recreational-mathematics', 'sequences-and-series']"
3374864,Non-abelian finite group in which more than half of the elements have order $2$,"Is there an non-abelian finite group, in which more than half of the elements have order $2$ I only know that if there is one, then all elements (except identity) cannot have order $2$, otherwise it would be abelian, so there is at least one element of order $>2$, say $x$. If I conjugate $x$ with another element, then conjugation preserves order but any $2$ conjugation might not give distinct elements, for the conjugacy class of an arbitrary element would be always the whole group. Or is there such a group","['group-theory', 'finite-groups']"
3374896,Recursive integrals a la $\int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t)dt$,"Inspired by this question I was wondering whether ""recursive"" integrals have been studied or if they appear anywhere in applications. What I mean is the following: Let $I(x, y) = \int_x^y g(t) dt$ and define $$\begin{aligned}
I_0 &= \int_x^y g(t) dt &&= I(x, y)\\
I_1 &=   \int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t) dt &&= I(-I(x,y), +I(x, y)) \\
I_2 &= \int_{\int^{\int_y^xg(t)dt}_{\int_x^yg(t)dt}g(t) dt}^{\int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t) dt} g(t) dt
&&=I(-I(-I, +I), +I(-I, +I)) \\
&&\vdots \\
I_{n+1} &= I(-I_n, +I_n)
\end{aligned}$$ What can be said about convergence criteria for this process? It should be straight-forward to show that for non-negative functions a blow up happens if $\int^x g(t) dt$ grows asymptotically faster than $x$ . Otherwise it should converge I think. But who knows what could happen when fast oscillating functions taking both positive and negative values are involved.","['integration', 'convergence-divergence']"
3374915,"How do Hilbert's foundational geometry axioms I,7-8 show that space is three dimensional?","In Hilbert's Foundations of Geometry he gives his axioms for geometry. Group I, called the axioms of incidence, details the way points, lines and planes exist.  The last two axioms of the group are I, 7. If two planes $\alpha$ , $\beta$ have a point $A$ in common then they have at least one more point $B$ in common I, 8. There exists at least four points which do not lie in a plane In the next paragraph he explains that ""Axiom I, 7 expresses the fact that space has no more than three dimensions, whereas Axiom I, 8 expresses the fact that space has no less than three dimensions."" Effectively saying that space has exactly three dimensions. How do these two axioms intuitively express these two statements?","['geometry', 'axioms']"
3374919,$\frac{7}{n}$ as a sum of three unit fractions,could $\frac{7}{n}$ be represented as sum of three positive unit fractions given that $n = 2 \mod 7$ ? I know $\frac{7}{2}$ is not but for all $n>2$ and $n = 2 \mod 7$ it seems that its representable.,['number-theory']
3374937,Prove it is not need to be Field ( very confused),"Suppose that $Ω \in \mathcal{F}$ and that $\mathcal{F}$ is closed under the formation of the complements and finite disjoint unions.
Show by an example that $\mathcal{F}$ need not be a field Solution: Hint: I think the solution is given as follows Consider $ X=\{1,2,\ldots,2n\}$ for some $n \in \mathbb{N}$ and $$\mathcal{F} := \{A \subseteq X; \sharp A \, \text{is even}\}, $$ where $\sharp A$ denotes the cardinality of the set $A$ . I am not completely sure what was the thought of solving process. My Questions: First of all, is the only reason that $\mathcal{F}$ is not always a field is because of the word ""disjoint""? Does this mean that the set $A \in \mathcal{F}$ could look like $A = \{ 1,2\}$ or $A = \{1,2,3,4\}$ etc.?
I don't understand what the even condition secures and why $\mathcal{F}$ is closed under the formation of the complements and finite disjoint unions. To prove that $\mathcal{F}$ is not a field  we say $Ω \in \mathcal{F}$ $A \in \mathcal{F}$ and $A^c \in \mathcal{F}$ The second is true because if $A = \{ 1,2\}$ then $\sharp A = 2$ , so $A$ is of even cardinality.  Then $A^c = \{ 3,4..,2n\}$ implies that $\sharp A = 2n-2$ which is also of even cardinality. $A \in \mathcal{F}$ , $B \in \mathcal{F}$ so we must DISPROVE that $A \cup B  \in \mathcal{F}$ ??  How can we do that?","['measure-theory', 'probability-theory']"
3374957,Differentiation using l´Hopital,I need to use L´Hopital's rule with this functions: $$\lim_{x\rightarrow\frac{\pi}{2}} (1-\sin(x))^{\cos(x)}$$ $$\lim_{x\rightarrow\frac{\pi}{4}} (\tan(x))^{\tan(2x)}$$ I take the exponent down using the properties of logarithms and then make the denominator like: $\lim_{x\rightarrow\frac{\pi}{2}} \frac{\cos(x)}{\frac{1}{\ln(1-\sin(x))}}$ but I still get stuck.,"['limits', 'calculus', 'algebra-precalculus']"
3374977,Limit Of A Sequence Formal Proof,"Let $\left (x_n \right )_{n=1}^{\infty}$ be a convergent sequence in $\mathbb{R}$ with limit $x$ . Show that $\lim_{x\rightarrow \infty}\frac{1}{n}\sum_{k=1}^{n}x_k = x$ . I had been working at it for awhile and was completely lost upon looking at the solution I had many questions even after staring at it for awhile, the solution states: Let $\varepsilon > 0$ . As $\lim_{x\rightarrow \infty}x_n = x$ , there is $n_1 
\in \mathbb{N}$ s.t. $|x_n -x| < \frac{\varepsilon}{2}$ for $n \ge n_1$ . Choose $n_2 \in \mathbb{N}$ s.t. $$\frac{1}{n_2}\left|\sum_{k=1}^{n_1 - 1}(x_k-x)\right| < \frac{\varepsilon}{2}. \tag{1}$$ Set $n_\varepsilon := \max\{n_1,n_2\}$ . For $n \ge n_\epsilon$ , we have: \begin{align*}
\left|\sum_{k=1}^{n}x_k-x\right| &= \left|\sum_{k=1}^{n_1 -1}(x_k-x)\right| \\
&\le \frac{1}{n}\left|\sum_{k=1}^{n}x_k-x\right| + \frac{1}{n}\sum_{k=n_1}^{n}|x_k-x| \\
&\le \frac{1}{n_2}\left|\sum_{k=1}^{n}x_k-x\right| + \frac{1}{n}\sum_{k=n_1}^{n}|x_k-x| \\
&< \frac{\varepsilon}{2} + \frac{n + 1 -n_1}{n} \max_{k=n_1,\ldots, n} |x_k - x| \\
&< \frac{\epsilon}{2} + \frac{\epsilon}{2} \\
&< \epsilon.
\end{align*} The thing I don't understand is how we get: 1) I don't quite get the thought process that goes into determining (1) before we start the proof. 2) $\frac{1}{n}\sum_{k=n_1}^{n}|x_k-x| < \frac{n + 1 -n_1}{n} \max_{k=n_1,\ldots, n} |x_k - x|$ Clarification on either of these steps would be greatly appreciated thank you.","['proof-explanation', 'sequences-and-series', 'real-analysis']"
3374988,Asymptotic behavior of the summatory Liouville function,"EDIT : This is a major revision in light of sources provided by @ErickWong's comment. The summatory Liouville function (OEIS sequence A002819 ) is $$L(n)=\sum_{1\le k\le n}(-1)^{\Omega(k)}\quad (n=1,2,3,\ldots)$$ where $\Omega(k)$ is the number of prime factors of $k$ counted with multiplicity. Thus $L(n)$ is the sum of the ""parities"" ( $+1$ if even, $-1$ if odd) of the first $n$ values of $\Omega.$ Here is a plot of the first $10^9$ values of $L$ , including overlays of some approximating curves: The overlays are as follows: A ""central"" curve (gray) ${\sqrt{n}\over\zeta({1\over 2})}$ whose functional form is suggested by Tanaka's 1980 proof (mentioned here and proved here ) that $\ L(n)-{\sqrt{n}\over\zeta({1\over 2})}\ $ changes sign infinitely often as $n\to\infty.$ A ""sinusoid"" (red) which oscillates about the central curve with increasing ""wavelength"" and amplitude: $$\hat{L}(n) = {\sqrt{n}\over\zeta({1\over 2})}\left( 1 + \sqrt{1\over 2}\,\sin\left(t_1\ln(n) + 2.75\right) \right)
$$ where $\zeta$ is the Riemann zeta function and $t_1$ is the imaginary part of the first nontrivial zero of $\zeta$ . Thus $\zeta({1\over 2})=-0.684765236\ldots$ and $t_1=14.1347251\dots.$ The occurrence of $t_1$ in the argument of the sine function is suggested by the ""summary description"" of a Wikipedia plot here . I've adjusted the parameters of the sinusoid, including amplitude and phase, to obtain a visual fit. Q1 : The mentioned ""summary description"" cites this paper (""On Differences of Zeta Values""), which proves an asymptotically oscillatory behavior of the coefficients in a certain series representation of Riemann zeta.  How does this lead to a zero of Riemann zeta appearing in $\hat{L}?$ Q2 : Can it be shown that regular oscillatory behavior of $L(n)$ persists for arbitrarily large $n$ ? NB : This behavior says nothing about how often $L(n)>0$ may occur (such as the ""spike"" in the plot at $n\approx 9\cdot 10^8$ ); i.e., it may be independent of whether $L$ changes sign infinitely often.","['number-theory', 'asymptotics', 'prime-factorization', 'prime-numbers']"
3375017,Gradient-based optimization: A small change in the input obtains a corresponding change in the output,"My textbook, Deep Learning by Goodfellow, Bengio, and Courville, says the following in a section on gradient-based optimization: The derivative of this function is denoted as $f'(x)$ or as $\dfrac{dy}{dx}$ . The derivative $f'(x)$ gives the slope of $f(x)$ as the point $x$ . In other words, it specifies how to scale a small change in the input to obtain the corresponding change in the output: $f(x + \epsilon) \approx f(x) + \epsilon f'(x)$ . The derivative is therefore useful for minimizing a function because it tells us how to change $x$ in order to make a small improvement in $y$ . For example, we know that $f(x - \epsilon \text{sign}(f'(x)))$ is less than $f(x)$ for small enough $\epsilon$ . We can thus reduce $f(x)$ by moving $x$ in small steps with the opposite sign of the derivative. Just to be clear, the reason why we can conclude that a small change in the input obtains a corresponding change in the output is because the function is continuous at every point in its domain (and must be so, since it is assumed to be differentiable)? Without this being the case, it seems to me that a small change in the input would not necessarily obtain a corresponding change in the output; and nor would the function necessarily be differentiable at every point in its domain, since there could be discontinuities or anomalies? I just want to be confirm that my thoughts about this are correct. Thank you.","['real-analysis', 'machine-learning', 'numerical-methods', 'gradient-descent', 'derivatives']"
3375029,Demonstrate divergence and rotational.,"Show that $$DIV(A)=\lim_{\Delta s\rightarrow0}\frac{\displaystyle\int\int_{\Delta v}A\cdot nds}{\Delta v}$$ and, $$ROT(A)\cdot n=\lim_{\Delta s\rightarrow 0}\frac{\displaystyle\oint_{C}A\cdot dr}{\Delta s}$$ Is there a demonstration for these results? One suggestion they gave me is to use the mean value theorem for integrals. Any suggestions?","['multivariable-calculus', 'calculus', 'electromagnetism']"
3375111,Convergence of $\sum_{n=1}^\infty x_n^k$,"Let $S\subseteq \mathbb Z^+$ be a set of positive odd numbers. I am asked to prove that there exists a sequence $(x_n)$ such that for any positive integer $k$ , $$
\sum_{n=1}^\infty x_n^k
$$ converges iff $k\in S$ . I have no idea where to start. Even in the special case $S=\{1\}$ I don't know if any sequence would work. Any hints? If $S=\{1\}$ , then we have to find a sequence such that $\sum x_n$ converges but $\sum x_n^k$ doesn't for $k\geq 2$ . However, for a positive sequence, if $\sum x_n^k$ converges then $\sum x_n^{k+1}$ converges, so we must not choose $(x_n)$ to be a sequence of positive terms. But what can I do next?","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
3375126,limit of a n-square root and series of exponents,"Again, I'm having trouble with the infinite limits: $$  (1) .... \lim_{n \to \infty} \sqrt[n]{ a^n+b^n } $$ with $a,b$ positive reals. and to show if the following series is divergent or convergent $$ (2) ......\sum_{n=1}^{\infty} \frac{5^{n}-2^{n}}{7^n-6^n} $$ To be honest, don't have any idea how to approach them, at least for the (2) I may use their exponential representations, as follows: $$\sum_{n=1}^{\infty} \frac{e^{n \ln 5}-e^{n\ln 2}}{e^{n\ln 7}-6^{n \ln 6}}  $$ and in that case the series will diverge. But for (1) don't know. Thanks in advance!","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
3375133,Can't get the right result for a derivative of a trigonometric function,"so I need to find the derivative of the following expression: $$y=(-\csc x)(-\sin x)$$ This is what I have done so far by applying the product rule: $$y'=\csc x\cot x(-\sin x) + (-\csc x)(-\cos x)=-\sin x\csc x\cot x + \csc x\cos x$$ $$y'=\csc x(-\sin x\cot x + \cos x)$$ Unfortunately, my textbook displays the result as: $$y'=\cos x\cot^2x$$ Am I doing something wrong, or both results are equivalent? If they are equivalent, can someone show me step by step how to get to the textbook's result? Thank you so much in advance!","['analysis', 'real-analysis', 'calculus', 'trigonometry', 'derivatives']"
3375177,Solve 2nd order Ordinary differential equation $y d^2y/dx^2 -2 (dy/dx)^2 = y^2$,"$$y \dfrac{d^2y}{dx^2} -2\left( \dfrac{dy}{dx} \right)^2 =y^2$$ let $y \dfrac{dy}{dx} = t$ $$\left( \dfrac{dy}{dx}) \right)^2 +y\dfrac{d^2y}{dx^2}=\dfrac{dt}{dx}$$ put in equation after this i am stuck I don't how to proceed because I got differential in $x ,y$ , and $t$ please help",['ordinary-differential-equations']
3375232,Probability of sequence of events,"Question : Let ${A_n}$ be a sequence of events with P( $A_n$ )=1 $\forall n$ $\geq$ 1. Find P( $\cup A_n$ ) and P( $\cap A_n $ ) What I did :
  Since it is given that P( $A_n$ )= $1$ we can easily say that all $A_n$ are sure events for all n $\geq 1$ .So as all are sure events so each $A_n$ contains every other events so basically, we can think $\cup A_n$ as $A_k$ for any k $\in {1,2,....n}$ thus
P( $\cup A_n$ )=P( $A_k$ )= $1$ Similarly for ( $\cap A_n $ ) we get it as $ 1$ . Am I doing any mistake ? Any help is appreciated.","['probability-theory', 'probability']"
3375272,How can we explain there is not exist $6$-digit self-descriptive number?,"a self-descriptive number is an integer $m$ that in a given base $b$ is $b$ digits long in which each digit $d$ at position n (the most significant digit being at position $0$ and the least significant at position $b - 1$ ) counts how many instances of digit $n$ are in $m$ . For example, in base $10$ , the number $6210001000$ is self-descriptive because of the following reasons: In base $10$ , the number has $10$ digits, indicating its base; It contains $6$ at position $0$ , indicating that there are six $0$ s in $6210001000;$ It contains $2$ at position $1$ , indicating that there are two $1$ s in $6210001000$ ; It contains $1$ at position $2$ , indicating that there is one $2$ in $6210001000$ ; It contains $0$ at position $3$ , indicating that there is no $3$ in $6210001000$ ; It contains $0$ at position $4$ , indicating that there is no $4$ in $6210001000$ ; It contains $0$ at position $5$ , indicating that there is no $5$ in $6210001000$ ; It contains $1$ at position $6$ , indicating that there is one $6$ in $6210001000$ ; It contains $0$ at position $7$ , indicating that there is no $7$ in $6210001000$ ; It contains $0$ at position $8$ , indicating that there is no $8$ in $6210001000$ ; It contains $0$ at position $9$ , indicating that there is no $9$ in $6210001000$ . There is not exist $6$ -digit self-descriptive number. But I don't know how to explain it. Please give me some help.","['number-theory', 'discrete-mathematics']"
3375282,How to integrate $\int_{0}^{\frac{\pi}{2}}\frac{x\ln{\cos(x)}}{\tan x}dx$? [duplicate],"This question already has answers here : Integral: $\int_0^1\frac{\mathrm{Li}_2(x^2)}{\sqrt{1-x^2}}dx$ (4 answers) Closed 4 years ago . I'm confronted with a problem: prove that $\displaystyle \int_{0}^{\frac{\pi}{2}}\frac{x\ln{\cos(x)}}{\tan x}dx=-\frac{\pi}{4}\ln^22$ I've already known $\displaystyle \int_{0}^{\frac{\pi}{2}}\frac{x}{\tan x}dx=\frac{\pi}{2}\ln2\ $ and $\displaystyle \int_{0}^{\frac{\pi}{2}}\ln{\cos(x)}dx=-\frac{\pi}{2}\ln2\ $ (which are actually 
equivalent to each other) But they're not sufficient to solve this, I guess.","['integration', 'calculus', 'improper-integrals', 'analysis']"
3375289,The integral is not iterated integral. Will this fact prevent us from swapping the order of surface and volume integrals? Why? Why not?,"Consider a continuous charge distribution in volume $V'$ . Draw a closed surface $S$ inside the volume $V'$ . Consider the following multiple integral: $$B= \iint_S       \Biggl(     \iiint_{V'}   \left[  \dfrac{\cos(\hat{R},\hat{n})}{R^2}   \rho'\   \right] dV' \Biggl)  dS$$ where $R=|\mathbf{r}-\mathbf{r'}|$ $\mathbf{r'}=(x',y',z')$ is coordinates of source points $\mathbf{r}=(x,y,z)$ is coordinates of field points $\cos(\hat{R},\hat{n})$ is the angle between $R$ and normal to surface element $\rho'$ is the charge density and is continuous throughout the volume $V'$ Since $\mathbf{r} \in S$ , the function is not integrable in domain $V'$ . So we use change of variables: $$B= \iint_S        \Biggl(    \iiint_{V'}   \left[  \dfrac{\cos(\hat{R},\hat{n})}{R^2}   \rho'\ {r'}^2 \sin \theta' \right] d\theta' d\phi' dr'  \Biggr) dS$$ $\bbox[yellow]{\text{Note that in this equation, $\theta'$ and $\phi'$ are w.r.t point $\mathbf{r} \in S $}}$ The inner volume integral is an iterated integral. While computing this iterated integral in spherical coordinates (here $\mathbf{r'}$ varies and $\mathbf{r}$ is constant), we take the origin of our spherical coordinate at point $\mathbf{r} \in S$ . That is, $\mathbf{r}=(0,0,0)$ Therefore after computing this iterated integral, i.e. after applying the limits, we will not be getting a function of $(r,\theta,\phi)$ or $(x,y,z)$ . Instead we will be getting a number. This means we cannot carry out the surface integral by iterated integrals. Will this fact prevent us from swapping the order of surface and volume integrals? Why? Why not? If swapping the order of surface and volume integrals is valid: \begin{align}
B &= \iiint_{V'}        \Biggl(    \iint_S   \left[  \dfrac{\cos(\hat{R},\hat{n})}{R^2}   \rho'\ {r'}^2 \sin \theta' \right] dS   \Biggr) d\theta' d\phi' dr'\\
  &= \iiint_{V'}        \Biggl(    \iint_S   \left[  \dfrac{\cos(\hat{R},\hat{n})}{R^2}    \right] dS   \Biggr) \rho'\ {r'}^2 \sin \theta' d\theta' d\phi' dr'\\
\end{align} $\bbox[yellow]{\text{Here in this last equation, $\theta'$ and $\phi'$ are w.r.t. which point?}}$ Special note for the answerer This question may or may not be related to Fubini's theorem. If it is, please give the complete statement of Fubini's theorem and please show step-by-step how my question can be explained by Fubini's theorem. Please consider that I am a graduating student of Mathematics and I am learning real analysis (single and multivariable). Please try to explain within the scope of real analysis.","['multivariable-calculus', 'calculus', 'spherical-coordinates', 'multiple-integral', 'potential-theory']"
3375296,Is there a name for this sort-of-inverse function?,"I'm looking for the mathematical name of a particular function which is available in several functional programming languages. If $f$ is a function $f:X\to Y$ , then define $g : Y \to 2^X$ as follows. $g(y) = \{x \in X \mid f(x) = y\}$ For example, if $f(0) = 1, f(1) = 2, f(2) = 1, f(3) = 3$ , then $g(0) = \emptyset, g(1)=\{0,2\}, g(2) = \{1\}, g(3)=\{3\}$ .","['elementary-set-theory', 'relation-algebra', 'inverse-function']"
3375322,What is the expectation of the order of automorphism group in Erdos-Renyi random graphs?,"Suppose $\Gamma(V, E) \sim G(n, p)$ is an Erdos-Reyi random graph with $n$ vertices and edge probability $p$ . What is the expected size of the automorphism group of $\Gamma$ ? What have I tried: Suppose, $I_{Aut(\Gamma)}$ is the indicator function of $Aut(\Gamma)$ in $Sym(V)$ . Then $|Aut(\Gamma)| = \sum_{\sigma \in Sym(V)} I_{Aut(\Gamma)}(\sigma)$ . Thus $$E(|Aut(\Gamma)|) = E(\sum_{\sigma \in Sym(V)} I_{Aut(\Gamma)}(\sigma)) = \sum_{\sigma \in Sym(V)} E(I_{Aut(\Gamma)}(\sigma)) = \sum_{\sigma \in Sym(V)} P(\sigma \in Aut(\Gamma))$$ However, I do not know how to proceed further. Here is also the analysis of the situation for small $n$ : If $n = 1$ , then $E(|Aut(\Gamma)|) = 1$ because $|Aut(\Gamma)| = 1$ almost surely. If $n = 2$ , then $E(|Aut(\Gamma)|) = 2$ because $|Aut(\Gamma)| = 2$ almost surely. If $n = 3$ , then $|Aut(\Gamma)| = 6$ with probability $p^3 + (1 - p)^3$ and $|Aut(\Gamma)| = 2$ with probability with probability $1 - (p^3 + (1 - p)^3)$ . Thus $E(|Aut(\Gamma)|) = 2 + 4(p^3 + (1 - p)^3)$ . Also note, that because the automorphism group of a graph is always isomorphic to the automorphism group of its complement graph, that value is invariant under the map $p \mapsto (1-p)$","['automorphism-group', 'random-graphs', 'graph-theory', 'group-theory', 'probability']"
3375351,Relationship between order of a pole and decay rate of a taylor series?,"Out of curiosity, is there a known relationship between the order of a functions pole and the rate of decay (/growth?) of the coefficients of a Taylor Series? Or one that can be proven? For example, say that we take a taylor series at $z=0$ of a function holomorphic at $z=0$ but with a simple pole at $z=a$ . Then the radius of convergence of this taylor series is at most equal to $a$ . From my understanding, the terms can also be shown to decay at rate $\dfrac{1}{a^n}$ (so that the series may converge). Are there any differences that may occur if the pole is of higher order? Because intuitively, this changes the nature of the function about the pole itself in terms of how quickly it ""goes off to infinity"" (for lack of a better term) and such.","['complex-analysis', 'meromorphic-functions', 'taylor-expansion']"
3375354,Evaluating $\lim_{x\to 0}\frac{x}{2^x-1}$ without using L'Hôspital's rule,"So lately, when one my friends had asked for help, he showed me this task: Evaluate the following limit $$\lim_{x\to 0}\frac{x}{2^x-1}$$ The problem is that one is not ought to use L'Hôspital's rule (which yields $\frac{1}{\ln 2}$ ), because derivatives weren't even introduced yet. I am sorry that I can't see a way to change this term algebraically and it would be a pleasure if you helped me.","['limits', 'limits-without-lhopital']"
3375412,The $L^p$ Norm of the Heat Kernel and its Divergence and Gradient,"I am working with the Heat Kernel defined on Euclidean space with dimension $N$ as: $G(t,x) := \frac{1}{(4 \pi t)^{-N/2}} e^{-|x|^{2}/4t}$ I have been told by my professor, and found on this post the following expression for the $L^p$ norm of the Heat Kernel: $||G(t)||_{L^{p}(\mathbb{R}^{N})} = C_{p} t^{(-N/2)(1-\frac{1}{p})}$ , where $C_p$ is some positive constant, $1 < p < \infty$ . I also have an expression for the norm of the gradient: $||\nabla G(t)||_{L^{p}(\mathbb{R}^{N})} = C_p t^{-1/2}$ . My first question is simply where I can find a source with a proof of these expressions! I have been unable to find them on Google... My second question is whether or not we have an upper bound on the norm of the divergence of $G$ . Specifically, I need an estimate for the following expression: $|| a \cdot \nabla G(t)||_{L^{1}(\mathbb{R}^{N})}$ , where $a \in \mathbb{R}^{N}$ constant. I presume we could easily deal with this constant by considering: $^{\text{max}}_{1 \leq i \leq N} \ a_{i} ||\text{div }G(t)||_{L^{1}(\mathbb{R}^{N})}$ . So we just need an estimate for the $L^1$ norm of $\text{div }G(t)$ . The end goal of the problem I am working on is to show that the following map $\phi$ is a contraction map on $C([0,T]; L^{1}(\mathbb{R}^{N}) \cap L^{\infty}(\mathbb{R}^{N}))$ equipped with the norm $||u|| = ^{\text{sup}}_{0<t<T}(||u||_{L^{1}(\mathbb{R}^{N})} + ||u||_{L^{\infty}(\mathbb{R}^{N})})$ . $\phi(u)(t) := G(t) \ast u_0 + \int^{t}_{0} a \cdot \nabla G(t-s) \ast (|u(s)|^{q-1}u(s)) \text{d}s $ . Here, $u_0 \in L^{1}(\mathbb{R}^{N}) \cap L^{\infty}(\mathbb{R}^{N})$ is the time-initial value of $u$ , and $\ast$ denotes convolution of functions: $(G(t)\ast u_0)(x) = \int_{\mathbb{R}^{N}} G(t,y)u_0(x-y) \ \text{d}y$","['normed-spaces', 'heat-equation', 'real-analysis', 'lp-spaces', 'functional-analysis']"
3375415,Find the thousandth number in the sequence of numbers relatively prime to $105$.,"Suppose that all positive integers which are relatively prime to $105$ are arranged into a increasing sequence: $a_1 ,  a_2 ,  a_3, . . . .$ Evaluate $a_{1000}$ By inclusion exclusion principle I found the following equation: $n - \left( \left\lfloor \dfrac {n} {3} \right\rfloor + \left\lfloor \dfrac {n} {5} \right\rfloor + \left\lfloor \dfrac {n} {7} \right\rfloor \right) + \left( \left\lfloor \dfrac {n} {15} \right\rfloor + \left\lfloor \dfrac {n} {21} \right\rfloor + \left\lfloor \dfrac {n} {35} \right\rfloor \right) - \left\lfloor \dfrac {n} {105} \right\rfloor=1000$ I tried to solve the equation by setting $n=105k+r ; 0 \leq 0<105$ .  But this method is so tiring and lengthy to solve. I thought to invoke inequalities like $a-1 <\lfloor a \rfloor \leq a$ , but I couldn't do as the equation contains positive as well as negative terms .Please provide me any other method to solve such equations involving floor function. Any help would be appreciated.","['coprime', 'ceiling-and-floor-functions', 'combinatorics', 'inclusion-exclusion']"
3375473,Self-learning Calculus. Where does Lang's First Course in Calculus stay when compared to Apostol/Spivak/Courant,"I need Calculus book that suits my level. (Or at least primary book which I will follow more closely) I don't have much formal education but have recently read Lang's Basic Mathematics (cover to cover, doing all almost all of the exercises). In my search for books I found: Spivak - Calculus Apostol - Calculus 1 and 2 Courant - Introduction to Calculus and Analysis vol 1 and 2 Lang - First Course in Calculus I found info about the other books, and it seems that Apostol's Calculus will be best suited for self learner and covers more than Spivak (also gives some applications), while Courant covers even more than Apostol but has less and harder problems. I have read a little bit (about derivatives, limits) of Lang's book and it's quite easy to follow. So, where Lang's book stays? Are the other books too advanced for me? Which book I should use as primary text if time is a concern and I want to cover more things? (Sorry if that's too many questions) Thanks. EDIT: There is a thing in the college I want to go called ""Mathematics and Informatics"" (that's in Eastern Europe). I'm kind of in hurry because I want to get in college and I have the chance  to skip the first year if I know enough. (first year is mostly C++, Calculus and little bit of Linear Algebra and I already know enough C++) So if I'm to skip the first year I should be ready for the Mathematical Optimization, Discrete Math, Differential Equations, Information theory (IDK if I translate correctly). All of these are intro level. I have around 6-7 months before I try to get the exams. And little bit more before I eventually start college. BTW, Basic Mathematics was quite challenging and there have been some exercises that got me to look at the back of the book for solutions or search on the internet. Don't wanna make it sound like I've done 100% of them (someone in my situation my get discouraged after reading this), but I tried.","['self-learning', 'calculus', 'book-recommendation', 'reference-request']"
3375477,How to expand in powers of delta?,"I am working through Probability Theory: The Logic of Science, by E.T. Jaynes, and am stuck at equation 16.1, where he performs an expansion in powers of $\delta$ . For full context, the preceding paragraph and equation is: Thus, if the sampling distribution has the functional form $f(\frac{x}{\sigma})$ , and $x$ and $\sigma$ are already known to be about equal to $x_0$ and $\sigma_0$ , we are really making inferences about the small corrections $q \equiv x - x_0$ and $\delta \equiv \sigma - \sigma_0$ . Expanding in powers of $\delta$ and keeping only the linear term, we have: $$\frac{x}{\sigma} = \frac{x_0 + q}{\sigma_0 + \delta} = \frac{1}{\sigma_0}(x - \theta + \dots)$$ where $\theta \equiv \frac{x_0 \delta}{\sigma_0}$ . I am confused as to what he means by ""expanding in powers of $\delta$ "", and not sure how he arrives at the expression on the right. I have been trying different Taylor series expansions, specifically: $$f(a) + f'(a)(x-a)$$ Letting $a = \delta$ , but still I am not able to derive the correct result. Is a Taylor series expansion what Jaynes is referring to in this case? Should I be working with a multivariate Taylor series, since $f$ is parameterized by both $x$ and $\sigma$ ? If not, could anyone point me in the right direction to what he is referring to? For reference, I have checked the usual sources at this point (wikipedia, google, math stack exchange), but to no avail. I think the main issue is not quite understanding what Jaynes means by ""expanding in powers of..."" . As far as I know, generally expanding in powers of something refers to the expression being raised to ascending powers; i.e. in the standard Taylor series expansion: $$f(a) + f'(a)(x-a) + f''(a)\frac{(x-a)^2}{2} + \dots$$ we could say ""expanding in powers of $(x-a)$ "". Any clarity/insight here would be greatly appreciated, and I can provide any additional context that is needed. Thank you!","['taylor-expansion', 'probability-theory', 'probability']"
3375504,Statistical Proof of Turán’s Graph Theorem: Weight of Vertex Uniformly Distributed?,"If a graph $G = (V, E)$ on $n$ vertices has no $p$ -clique, $p \geq 2$ ,
then, $|E| \leq (1- \frac{1}{p-1}) \frac{n^2}{2} \cdots (1)$ . We get a proof from the book ""Proofs from THE BOOK"" as given below- Now I could not understand below line- We infer that the maximal value of $f(w)$ is attained for $ w_i =
\frac{1}{k} $ on a $k$ -clique... because, then it menas that all vertices have same weight, but earlier, it was assumes that- Suppose $w$ is any distribution In general, can anyone please explain how one can infer that the maximal value of $f(w)$ is attained for $ w_i =\frac{1}{k} $ from $f(w')=f(w)+\epsilon (w_1-w_2)$ ?","['graph-theory', 'proof-explanation', 'statistics', 'probability-distributions']"
3375527,Maximum value of $x$ such that $3^x-2^n$ is a prime.,"What is the maximum possible value of $x$ such that  expression $3^x - 2^n $ results in  a prime,
  where $n$ is the maximum value such that $2^n<3^x$ and $2^{n+1} > 3^x$ ? Using some brute force, till now I have found that $x = 33077 $ to be the maximum value for which the difference is a prime number. But is this the maximum value? Can anyone please explain, for what values would we get a prime number. Also, Would changing any of the constants from 2 and 3 to other values ,  give much more interesting results ? Would it be even solvable?","['number-theory', 'elementary-number-theory', 'discrete-mathematics', 'algebra-precalculus', 'prime-numbers']"
3375533,Why Study Quadratic Forms?,"I am trying to settle upon a topic for my Bachelor Thesis, and Classifying Quadratic Forms over the Rationals was suggested to me. What I was hoping to learn, is why would this topic be useful in the context of developing mathematically. 1) Gauss and other impressive Mathematicians seem to have done a not insignificant amount with Quadratic Forms too, is there a particular reason why these people spent time on this topic? 2) Will learning about Quadratic Forms provide me with some of the tools a Graduate will want before pursuing more number theory, algebra, etc.? (A simple yes or no is all I ask in response to this, I don't expect a History lesson) 3) Are there fields or topics where an understanding of quadratic forms will be of significant relevance? To be clear, I am asking what specific examples are there of how this topic benefited past Mathematicians and where it has links to other Mathematical Topics.","['abstract-algebra', 'quadratic-forms']"
