question_id,title,body,tags
4789914,Significance of t-tests and outlieres,"I was performing paired t-tests and came across the example from the R function.
They show that addition of strong outlier can make the test more compliant about differences: t.test(1:10, y = c(7:20))      # P = .00001855

t.test(1:10, y = c(7:20, 200)) # P = .1245    -- NOT significant anymore How is this possible ?
Does that mean that a t-test shouldn't be used in the case of outliers presence in the data ?",['statistics']
4789926,Connected components of real matrices such that $M^2=I$,"Suppose that $S$ is the set of all real $n\times n$ matrices $A$ such that $A^2=I_n$ . Since these matrices are diagonalizable with $\pm 1$ as eigenvalues, we get the partition $S=\cup_{0\leq k \leq n} S_k$ where $S_k$ is the set of all matrices of the form $PJ_k P^{-1}$ where $P$ is a real invertible matrix and $J_k$ the diagonal $(1,\cdots,1,-1,\cdots,-1)$ (the first $k$ components are equal to $1$ , and the other $n-k$ components are equal to $-1$ ). My question is : are the $S_k$ the connected components of $S$ ? (and if yes, why ?) If not, what are the connected components of $S$ ?","['matrices', 'general-topology', 'linear-algebra']"
4789949,Matrix exponentials of Jordan blocks,"Let $f:\Bbb C→\Bbb C$ be an analytic function1.  Let $\lambda \in\Bbb C$ and let $$
J=\begin{pmatrix}
\lambda & 1 &  & &\\
 &\ddots& \ddots \\
 &  & \ddots& 1\\
& & &\lambda\end{pmatrix} = \lambda\begin{pmatrix}
1& 0 &  & &\\
 &\ddots& \ddots \\
 &  & \ddots& 0\\
& & &1\end{pmatrix} +\begin{pmatrix}
0 & 1 &  & &\\
 &\ddots& \ddots \\
 &  & \ddots& 1\\
& & &0\end{pmatrix}=\lambda I+N\in \Bbb C ^{l×l}$$ (Such matrices J appear as Jordan blocks in the Jordan canonical form of a matrix.)Consider the Taylor expansion of f around $\lambda∈\Bbb C$ , i.e., $f(a) =\sum_{k=0}^n\frac{f^{(k)}(\lambda)}{k!}(a−\lambda)^k$ .We define a matrix valued function via the formal power series $F(A) =\sum_{k=0}^\infty \frac{f^{(k)}(\lambda)}{k!} (A−\lambda I)k\in\Bbb C^{l×l}$ .(By definition, $A^0=I$ for any square matrix A.)One can show that the matrix function F(A) is well-defined, i. e., the above power series converges (absolutely) for any matrix $A\in\Bbb C^{l×l}$ , because f is analytic. Establish the following explicit finite formula for the matrix $F(J)\in \Bbb C^{l×l}$ in terms of f and its derivatives in $\lambda$ : $$
F(J) =\begin{pmatrix}
f(\lambda) &\frac{f′(\lambda)}{1!} &\frac{f′′(\lambda)}{2!} &\cdots&\frac{f^{(l−1)}(\lambda)}{(l−1)!} &\\
0 &f(\lambda)& \frac{f′(\lambda)}{1}&\ddots&\vdots\\
 0& 0 & \ddots& \ddots&\frac{f′′(\lambda)}{2!}\\
\vdots &\cdots&\ddots&f(\lambda)& \frac{f′(\lambda)}{1!}\\
0 &\cdots&\cdots&0&f(\lambda)\end{pmatrix}$$ Answer:
To establish the finite formula for the matrix F(J), we first note that J can be expressed as J = A + N, where A is a diagonal matrix with the eigenvalue $\lambda$ and N is a nilpotent matrix $(i.e., N^n = 0)$ as  mentioned. Now, we want to compute the powers of N inductively: $N^1= N$ $N^2 = N ✕ N = 0$ (since N is nilpotent) $N^3 = N ✕ N^2 = 0$ ... So, we see that $N^k = 0$ for $ k \ge 2$ . Now, let's compute F(J) using the Taylor expansion: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]^* (J - \lambda^*I)^k$ Since $N^2 = 0$ , the expansion simplifies: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}] (A + N - \lambda I)^k$ Now, we can apply the binomial theorem, which states that for any matrices A and B: $(A + B)^k = \sum_{j=0}^k (k choose j) A^{(k-j)} B^j$ In our case, $A = A - \lambda I$ and B = N, and k can be any non-negative integer. So, the above expression becomes: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]$ $\sum_{j=0}^k (k  choose  j)  (A - \lambda I)^{(k-j)}  N^j$ Now, since $N^2 = 0$ , the terms with $j \ge 2$ in the inner sum vanish, and we're left with: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  [(A - \lambda I)^{(k-0)}  N^0] + [\frac{f^{(k)}(\lambda)}{k!}]  [(A - \lambda I)^{(k-1)}  N^1]$ Since $N^0$ is the identity matrix I, we have: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - λI)^k + \sum_{k=0}^\infty[\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)}  N$ Now, we can separate the terms involving A and N: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^k + N  \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}] (A - \lambda I)^{(k-1)}$ The second sum involving N can be simplified using the property of the derivative of the matrix function: $N * \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)} = N  f'(\lambda)  \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)}$ Now, we can recognize that the second sum is the derivative of the matrix function with respect to A evaluated at $\lambda$ : $\sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)} = f'(\lambda)  ∂F(A)/∂A|_{(\lambda)}$ So, we have: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^k + N  f'(\lambda)  ∂F(A)/∂A|_{(λ)}$ Now, the formula for F(J) in terms of f and its derivatives in X is established. I want to know is my answer correct for this question as i am in the new place , and i don’t know anyone here to ask. Or can anyone provide me hints.","['jordan-normal-form', 'matrices', 'solution-verification', 'linear-algebra', 'matrix-equations']"
4789955,How to solve the integral $\int_0^\pi\ln(\sin(\theta))\sin(\theta)d\theta$?,"Could you help me with a comprehensive one?  I have tried to solve it but I can't reach the result. The integral I am trying to evaluate is $$\int_0^\pi\ln(\sin(\theta))\sin(\theta)d\theta$$ and according to Wolfram alpha the result should be $\ln(4)-2$ . I already have tried to evaluate it using integration by parts but, when doing so, I find a term of $\cos(\theta)\ln(\sin(\theta)) $ which is evaluated from $0$ to $\pi$ and this is where I stay because those values make the result ​​indeterminate. I have tried to make variable changes but I have not been able to find a suitable one that allows me to avoid indeterminate results. I would like you to provide me with a suggestion to avoid the indeterminacies.","['integration', 'definite-integrals', 'mathematical-physics']"
4790055,Derivative of indicator function involving time,"Take the function $f : \mathbb{R}^{+} \times \mathbb{R} \to \mathbb{R}$ defined by $$f(t,x) = \mathbb{1}_{[1-t,2-t]}(x) $$ I am trying to figure out what $\partial_{t}f$ should be in the distributional sense. My idea is to use the definition of the derivative $$\partial_{t}f = \lim_{h \to 0} \frac{f(t+h,x) - f(t,x)}{h}. $$ using that I got $\partial_{t}f = \delta(x+t-1) - \delta(x+t-2)$ . I am not sure if my answer is correct since the function is a multivariable function and I know that the derivative is often defined differently to what I gave above. If we were to find $\partial_{x}f$ then it would be $\delta(x+t-1) - \delta(x+t-2)$ and I am fine  with this. We can just fix time and treat it like a single variable function. But when time is involved I become unsure since we can't just fix $x$ and treat it as a single variable function..","['multivariable-calculus', 'distribution-theory', 'functional-analysis', 'partial-differential-equations']"
4790060,"Conservation of swept area (Kepler's $2$nd law), rigorous proof","Briefly, in class we proved Kepler's $2$ nd law like this: We've some random trajectory and two position vectors, $\mathbf r$ and $\mathbf{r}+d\mathbf{r}$ . Supposing $dr$ is small, we can approximate it to be the arc length, $ds$ . Thus, we've got that the differential area swept would be a triangle with height $r$ and base $dr$ : $$dA=\dfrac{1}{2}rdr\approx\dfrac{1}{2}rds=\dfrac{r^2}{2}d\theta.$$ We then ""divide"" by $dt$ and make use of conservation of angular momentum $\left(\dot \theta=\frac{L}{mr^2}\right)$ to finally prove it: $$\dfrac{dA}{dt}=\dfrac{r^2}{2}\dot \theta=\dfrac{L}{2m}=\text{ct.}$$ However, I'm not satisfied with this proof since there are some unrigorous steps and I was wondering whether maybe it could be done using double integrals and differentiation wrt time under the integral sign. If not, what other ways to prove it would be correct and rigorous too?","['classical-mechanics', 'multivariable-calculus', 'polar-coordinates']"
4790069,The Lebesgue measures of open sets of points of epsilon distance from a compact set converge to the Lebesgue measure of that compact set,"I’ve found a few questions on this website relating to this question, but none that answer this specific question directly. Let $E$ be a compact subset of $\mathbb{R}$ with Lebesgue measure $\lambda(E)=0$ . For $\epsilon > 0$ , let $E_\epsilon = \{x : d(x,E)<\epsilon\}$ where $d(x,E)$ is the distance between a point and a set. How do I show that $\lim_{\epsilon \rightarrow 0} \lambda(E_{\epsilon}) = \lambda(E) = 0$ ? I understand from the hint here that I need to be able to use that $\cap_{\epsilon}E_\epsilon = E$ for $\epsilon>0$ and that somehow the fact that $E$ being compact implies that if $d(x,E) = 0$ , then $x\in E$ . However I’m not sure how to go about this. Is this generalizable? What if $\lambda(E) = \alpha < \infty$ , can I show $\lim_{\epsilon \rightarrow 0} \lambda(E_{\epsilon}) = \lambda(E) = \alpha$ ? Any help is appreciated!","['measure-theory', 'lebesgue-measure', 'geometric-measure-theory', 'compactness']"
4790107,Locally free circle actions on Euclidean space,"I've been wandering whether or not there are locally free smooth circle actions on $\mathbb{R}^n$ , or, more specifically, if an Euclidean space would admit a smooth nonvanishing vector field $X$ whose orbits are all periodic. I know this cannot happen in $\mathbb{R}^2$ , and I suspect it is true in every dimension. I know that, since $S^1$ is compact, we can construct an invariant metric on $\mathbb{R}^n$ using an averaging procedure, and therefore consider the $S^1$ -action to be an action by isometries. If we could somehow show that these isometries must necessarily be some sort of rotation then they would have fixed points, and therefore could not be locally free. This seems to work in $\mathbb{R}^3$ , but I'm not sure how to turn this idea into a rigorous argument, or if it would still hold in higher dimensions. Any tips, better arguments or counterexamples would be welcome. A related but more general question is whether or not $\mathbb{R}^n$ admit regular foliations by circles.","['vector-fields', 'foliations', 'differential-topology', 'group-actions', 'differential-geometry']"
4790161,Is delta-epsilon proof necessary for multivariable limits?,"The specific problem is from James Stewart's 7th edition of Multivariable Calculus. The problem is $$\lim _{(x,y)\to (0,0)}\left(\frac{x^2y}{\:x^2+y^2}\right)$$ To evaluate this limit, do I need to use the delta epsilon proof? Is there no algebraic way to do this?","['limits', 'multivariable-calculus']"
4790219,Proving my IVP for a Piecewise Decay Function (Diff Eq),"Setup So... I kinda handled most of my proof but I need help with some of the stuff I just kinda went with until it worked out. The problem relates to medicine and its decay in the body. We are given that the medicine will release over a period of $b$ hours and another dose is given at time $T$ . Known Values Decay Constant is 1. Each dose contains 1 gram of medicine. We know that $b=\frac{5}{4}$ & $T=\frac{5}{2}$ . We are also given that $y(0)=0$ y being the amount of medicine in the body at time $t$ . Finally we're given the simple equation $rate=rate_{in}-rate_{out}$ . Looking at our values we find that $\frac{4}{5}$ grams are released per hour over the course of $\frac{5}{4}$ hours. Using this and our value of T it's possible to make a piecewise function for the release $(rate_{in})$ of the medicine, $g(t)$ . This function is ""on"" over the intervals $0<t<\frac{5}{4}$ & $\frac{5}{2}<t<\frac{15}{4}$ (note that $\frac{15}{4}$ just comes from $\frac{5}{2}+\frac{5}{4}$ ) and is ""off"" (to explain only having decay) between $\frac{5}{4}<t<\frac{5}{2}$ & $\frac{15}{4}<t<5$ (Idk if those should be $\le$ signs or not but that's less important right now). I only know how to turn this into a rectangular window function like: $u(\frac{5}{4})-u(\frac{5}{2})$ and so on and so on but regardless... we can gather that because our $rate_{out}$ is reliant of the amount of grams present, that it's just something times our $y(t)$ function. Okay here's the two parts I need help on (three technically because I'm still not happy with my explanation for why the out function is $y(t)$ : What I Need Help With I set up an initial value problem that looks like this: $y'(t)=\frac{4}{5}g(t)-\frac{4}{5}y(t)$ and I'm pretty confident it's right.... but I don't know how to explain why I multiplied by $\frac{4}{5}$ (I just guessed based off of searches I had been doing but I've lost what I actually searched up) because the only thing I can think of is that because $\frac{4}{5}$ is $\frac{g}{h}$ that means that there's SOME kinda rate of change equal to $\frac{4}{5}$ but surely it can't be $\frac{dy}{dt}$ since then our IVP wouldn't work right? I guess it could also be a way of making our $rate=rate_{in}-rate_{out}$ equation EQUAL $\frac{4}{5}$ or something silly? Idk, this one probably needs more help than the next. Also once I found that the Laplace transform of this solves to: $\frac{\frac{4}{5}G(s)}{s+\frac{4}{5}}$ I then basically tried to get $G(s)$ to equal the laplace of what I thought was the rectangular window function: $g(t)=(u(t)-u(t-\frac{5}{4}))+(u(t-\frac{5}{4})-u(t-\frac{5}{2}))$ but you can already see the problem right there. Stuff would cancel out. I remedied this by realizing that the equation through the next bounds $(\frac{5}{4}<t<\frac{5}{2})$ would have to pass through the last point given by the equation in the previous bounds... or in other words, to find the correct $y(t)$ we'd have to multiply say $y(t-b)\cdot(u(t-b)-u(t-c))$ by the result of $y(b)\cdot(1-u(b-b))$ (1 comes from $u(t)=1$ ) and so on and so forth and so I ended up getting: $y(t)=(1-e^{-\frac{4}{5}t})(u(t)-u(t-\frac{5}{4}))+(1-e^{-1})(e^{-\frac{4}{5}(t-\frac{5}{4})})((u(t-\frac{5}{4})-u(t-\frac{5}{2}))+(e^{-\frac{4}{5}(t-\frac{5}{2})})(1-(1-(e-1)e^{-2}))((u(t-\frac{5}{2})-u(t-\frac{15}{4}))+etc$ but see the problem is that we get that weird $1-(e-1)e^{-2}$ for the 3rd window which I can't come up with a good explanation for other than some weird shenanigans turning making one of our $u$ somehow equal $u(t)$ ? Like best I got is that because technically we're looking at the point $\frac{5}{2}$ for both sides you actually get 1-whatever because the step functions are interacting with each other??? Anyways if someone's willing to help just prove or AT LEAST EXPLAIN WITH WORDS why these things work that'd be much appreciated. Just something so I can justify what I did because while it did in fact work... it's been like a day since I wrote half of this stuff down and for some reason I didn't write why it works so... EDIT: MAJOR BREAKTHROUGH ON THE SECOND HALF (Potentially) (I think I've got the first half but it doesn't hurt to check so I'm leaving it), I still need some help explaining why it works BUT, I've found that for whatever reason, you can find the bit out in front of the second window (the $(1-e^{-1})$ part of $(1-e^{-1})(e^{-\frac{4}{5}(t-\frac{5}{4})})$ just by subtracting $e^{-\frac{4}{5}t}$ from $(e^{-\frac{4}{5}(t-\frac{5}{4})}$ . It's just that the $(1-e^{-1})(e^{-\frac{4}{5}(t-\frac{5}{4})})$ is oversimplified, that entire equation is equal to $-e^{-\frac{4}{5}t}+e^{-\frac{4}{5}(t-\frac{5}{4})}$ The only problem is... where does that negative $e^{-\frac{4}{5}t}$ come from? My current way of thinking about this is that it's almost as if the first bound $u(t), u(t-5/4), etc$ is what's looked at during the inverse Laplace transform? Because all the unit step functions should be (in theory according to my IVP) multiplied by $1-e^{-\frac{4}{5}(t-a)}$ (a is $u(t-a)$ ) once the inverse Laplace transform is all said and done. However, if this were truly the case many things would cancel out before actually separating in order to do this leading me to believe that either the first $u(t-a)$ speaks for the entire rectangular window function OR it's something even deeper in the algebraic solving, something like: $(-e^{-\frac{4}{5}t}+e^{-\frac{4}{5}(t-\frac{5}{4})})+(1-e^{-\frac{4}{5}\left(t-\frac{5}{2}\right)})$ (this one is specifically for the third window function and I found it to be correct). Thing is... WHY? Why does that work? New Update I think I've figured out a way to make this work actually. What if we just set $g(t)=0$ when $\frac{5}{4}<t<\frac{5}{2}$ and when $\frac{15}{4}<t<5$ since we said that $g(t)$ was our rate in right? Well I think we have that by just setting $g(t)=1$ for when the medicine is active we end up with the correct function THAT I'VE KNOWN ALL THIS TIME BTW BUT DOUBTED IT WOULD BE ACCEPTED: $\left(1-e^{-\frac{4}{5}t}\right)u\left(t\right)-\left(1-e^{-\frac{4}{5}\left(t-\frac{5}{4}\right)}\right)u\left(t-\frac{5}{4}\right)+\left(1-e^{-\frac{4}{5}\left(t-\frac{5}{2}\right)}\right)u\left(t-\frac{5}{2}\right)-\left(1-e^{-\frac{4}{5}\left(t-\frac{15}{4}\right)}\right)u\left(t-\frac{15}{4}\right)-\left(1-e^{-\frac{4}{5}\left(t-5\right)}\right)u\left(t-5\right)$ . I think that this just straight up works so... idk I'll leave this up but this is probably our answer.","['solution-verification', 'exponential-function', 'ordinary-differential-equations']"
4790220,Intuition for Reinhardt domains,"I'm taking a complex analysis course, and in a lecture on several complex variables my professor defined Reinhardt domains in $\mathbb{C}^n$ as domains invariant under the action of the n-dimensional torus. I'd like help to better understand this concept. Some questions: The only examples I can think of are either n-dimensional balls or shells. Are there any other Reinhardt domains? My professor drew a box with the top right corner cut out (in the space of absolute values) and claimed this was also a Reinhardt domain in $\mathbb{C}^2.$ I don't see why that would be: aren't there rotations that would land part of this domain into that corner? I don't yet understand why this would be a useful definition to make. Why is this the right concept for domains for functions of several variables?
Any help would be appreciated","['complex-analysis', 'several-complex-variables', 'intuition']"
4790260,Can you cover $\mathbb R$ with countably many smol sets?,"Say a set $X\subset\mathbb R$ is smol if one can't cover $\mathbb R$ with countably many translates of $X$ . So, for example, null-measure sets are smol. I guess sets of positive measure are not, but I have not been able to prove it yet. Non-measurable sets are a mystery. I wonder: can you cover $\mathbb R$ with countably many smol sets? This question came as an effort to find a maximal family $\mathcal F\subset\mathcal P(\mathbb R)$ such that any countable union over $\mathcal F$ is not $\mathbb R$ . This would give a rough notion of measure (only telling if a set is big or small) to every subset of $\mathbb R$ .","['measure-theory', 'recreational-mathematics', 'real-analysis']"
4790271,The rational function $\tan\left(n\arctan x\right)$,"While messing around with the formula $$\arctan x+\arctan y=\arctan\frac{x+y}{1-xy},\tag1$$ I decided to iteratively find expressions for $n\arctan x$ by applying the formula $(1)$ $n$ times. I was able to reach $n=9$ before it got too annoying. Here they are: $$\begin{align}
2\arctan x&=\arctan\left(2x\cdot\frac{1}{1-x^2}\right), &|x|&<1\\
3\arctan x&=\arctan\left(x\frac{3-x^2}{1-3x^2}\right),&|x|&<1/\sqrt3\\
4\arctan x&=\arctan\left(4x\frac{1-x^2}{1-6x^2+x^4}\right),&|x|&<\sqrt2-1\\
5\arctan x&=\arctan\left(x\frac{5-10x^2+x^4}{1-10x^2+5x^4}\right),&|x|&<\sqrt{1-2/\sqrt{5}}\\
6\arctan x&=\arctan\left(2x\frac{3-10x^2+3x^4}{1-7x^2+7x^4-x^6}\right),&|x|&<2-\sqrt{3}\\
7\arctan x&=\arctan\left(x\frac{7-35x^2+21x^4-x^6}{1-21x^2+35x^4-7x^6}\right),&|x|&<\alpha_7\\
8\arctan x&=\arctan\left(8x\frac{1-7x^2+7x^4-x^6}{1-28x^2+70x^4-28x^6+x^8}\right),&|x|&<-1+\sqrt{2}\left(-1+\sqrt{2+\sqrt2}\right)\\
9\arctan x&=\arctan\left(x\frac{9-84x^2+126x^4-36x^6+x^8}{1-36x^2+126x^4-84x^6+9x^8}\right),&|x|&<\alpha_9
\end{align}$$ Here, $\alpha_7$ and $\alpha_9$ are the smallest positive roots of $1-21x^2+35x^4-7x^6$ and $1-36x^2+126x^4-84x^6+9x^8$ respectively. There are clearly some patterns going on here. Taking $R_n(x)=\tan(n\arctan x)=p_n(x)/q_n(x),$ we can explore some of them. Firstly, it is easy to see that $$\arctan R_{n+1}(x)=(n+1)\arctan x=n\arctan x +\arctan x\\
=\arctan R_n(x)+\arctan x=\arctan\frac{x+R_n(x)}{1-xR_n(x)},$$ so that $$R_{n+1}(x)=\frac{x+R_n(x)}{1-xR_n(x)}=\frac{xq_n(x)+p_n(x)}{q_n(x)-xp_n(x)}.$$ From this, it is clear to see that $x|p_n(x)$ for all $n\ge1$ , as $$x|p_n(x)\Rightarrow x|(xq_n(x)-p_n(x))\Rightarrow x|p_{n+1}(x).$$ Furthermore, the relation $p_n(1/x)=(-1)^{\tfrac{n-1}{2}}x^{-n}q_n(x)$ is satisfied by the cases $n=3,5,7,9$ . Similarly, the relations $p_n(1/x)=(-1)^{\tfrac{n}2-1}x^{-n}p_n(x)$ and $q_n(1/x)=(-1)^{n/2}q_n(x)$ are satisfied by $n=2,4,6,8$ . Put more simply, $$\begin{align}
R_2(1/x)&=-R_2(x), &R_3(1/x)&=1/R_3(x),\\
R_4(1/x)&=-R_4(x), &R_5(1/x)&=1/R_5(x),\\
R_6(1/x)&=-R_6(x), &R_7(1/x)&=1/R_7(x),\\
R_8(1/x)&=-R_8(x), &R_9(1/x)&=1/R_9(x).\\
\end{align}$$ Indeed, this pattern can be shown to continue. Specifically, if we suppose that $R_{2k}(1/x)=-R_{2k}(x)$ for some $k\ge1$ , then $$R_{2k+1}(\tfrac1x)=\frac{\tfrac1x+R_{2k}(\tfrac1x)}{1-\tfrac1x R_{2k}(\tfrac1x)}=\frac{\tfrac1x-R_{2k}(x)}{1+\tfrac1x R_{2k}(x)}=\frac{1-xR_{2k}(x)}{x+R_{2k}(x)}=\frac{1}{R_{2k+1}(x)}.$$ Similarly, if $R_{2k-1}(1/x)=1/R_{2k-1}(x)$ for some $k\ge1$ , we get $$R_{2k}(\tfrac1x)=\frac{\tfrac1x+R_{2k-1}(\tfrac1x)}{1-\tfrac1xR_{2k-1}(\tfrac1x)}=\frac{\tfrac1x+\tfrac1{R_{2k-1}(x)}}{1-\tfrac1{xR_{2k-1}(x)}}=\frac{R_{2k-1}(x)+x}{xR_{2k-1}(x)-1}=-R_{2k}(x).$$ Another thing we may notice is the following. Here I will abbreviate $R_s(x)$ as $R_s$ for simplicity. $$\arctan R_{n+m}=(n+m)\arctan x=n\arctan x+m\arctan x=\arctan R_n+\arctan R_m=\arctan\frac{R_n+R_m}{1-R_nR_m}.$$ That is $$R_{n+m}(x)=\frac{R_n(x)+R_m(x)}{1-R_n(x)R_m(x)},$$ and consequentially $$\frac{p_{n+m}(x)}{q_{n+m}(x)}=\frac{p_n(x)q_m(x)+p_m(x)q_n(x)}{q_n(x)q_m(x)-p_n(x)p_m(x)}.$$ These are some pretty remarkable identities, so I can't possibly be the only person to consider these rational functions. Do they have a name? A generating function? A general closed form (other than $\tan(n\arctan x)$ and similar)? Where can I learn more about them?","['reference-request', 'elementary-functions', 'polynomials', 'sequences-and-series', 'trigonometry']"
4790283,"Linear Programming: What exactly is the basis matrix B, and how do we find its inverse from a given simplex tableau?","The explanation given in my textbook, where $A$ is the coefficient matrix of the linear program: Let $\mathbf{A}_j$ denote the $j^{\text{th}}$ column of $A$ and $\mathscr{B} = \{B_1,B_2,\dots,B_m\} ⊆ \{1,2,\dots,n\}$ be such that
the columns $\mathbf{A}_{B_1},
\mathbf{A}_{B_2},\dots,\mathbf{A}_{B_m}$ are linearly independent.
Then the $m×m$ matrix $B = [\mathbf{A}_{B_1}, \mathbf{A}_{B_2},\dots,
\mathbf{A}_{B_m}]$ is invertible.
Let $\mathscr{N} = \{1,2,\dots,n\} − \mathscr{B} = \{ℓ_1, ℓ_2,\dots, ℓ_{n−m}\}$ and $N = [\mathbf{A}_{ℓ_1}, \mathbf{A}_{ℓ_2} ,\dots, \mathbf{A}_{ℓ_{n−m}}]$ . I'm struggling to understand how $B$ isn't just always the identity matrix (if it's the linearly independent columns of the coefficient matrix), and I have so many questions. How exactly is $B$ , or $B^{-1}$ , obtained from a simplex tableau? I've had a ton of abstract stuff in this book so I'm looking for something at least mildly concrete and intuitive. I'm starting learning about the revised simplex method, so apparently this requires knowing exactly what $B^{-1}$ is on every iteration of the revised method.","['matrices', 'linear-algebra', 'linear-programming']"
4790372,A question about Cauchy product,"Consider the following two properties of a real divergent sequence $\{c_n\}$ . $(1) \;$ $\displaystyle\lim_{n\to\infty}\frac{c_n}{n}=M$ for some $M\in\mathbb{R}$ . $(2) \;$ There exist two real convergent  sequences $\{a_n\}$ , $\{b_n\}$ such that $c_n=\sum_{k=0}^{n}a_kb_{n-k}$ . Are $(1)$ and $(2)$ equivalent? I know how to prove $(2) \implies (1)$ , but I don't know whether $(1)$ implies $(2)$ .",['analysis']
4790435,How bistable is my system?,"Description: given ODE: $\dot{x} = a + bx +cx^2 +dx^3$ , I have mutliple combinations of the coefficients $a,b,c,d$ that I want to understand whether they make a bistable or not system. For this purpose, I started with the discriminant $\Delta$ of the cubic equation for $\dot{x}=0$ , which: for $\Delta > 0$ , 3 real solutions/fixed points (I guess 2 stable and 1 unstable) for $\Delta = 0$ , 1 and 1 double real solutions/fixed points (stability ?) for $\Delta<0$ , 1 real solution/fixed point and 2 imaginary. So, using $\Delta > 0$ I am able to find numerically which combinations of coefficients give me 3 real solutions and a clearly bistable system. But how strongly bistable are my systems? My current approach: I made the leap to hypothesize that the more positive my $\Delta$ is, the more bistable my system is. Basically I thought that because it seems that $\Delta$ is actually a metric of separating the solutions, since for $\Delta=0$ the double solution is just two collapsed solutions to the same point. So, it actually feels like $\Delta$ is like a control parameter of the system, which by varying it you cross bifurcation points and generate solutions. However, probably sth like that is not proved and it is like my approach makes some sort of sense but it is illegitimate. So, any comments on that would be highly appreciated. Energy landscapes ( $-\int\dot{x}dx$ ) for different combinations of $a,b,c,d$ and a range of positive $\Delta$ Another approach: More legitimate I thought that a proper stability analysis would be. So that would be to find numerical solutions of $\dot{x}=0$ , and do linearized stability analysis (get the Jacobian over the fixed points) to see the nature of the fixed points. But, my next question is, what should I look for to judge how strong the bistable system is? Is it how positive the unstable fixed point is (greater instability of the fixed point, more bistable)? Or is it a combination with how negative the stable fixed points are? Other approaches: At the end, I am not sure whether some sort of statistical significance could be made to determine how strong bistability exists (and stats is my weakest quality).","['nonlinear-system', 'stability-theory', 'discriminant', 'ordinary-differential-equations']"
4790441,Why do we only allow for finite intersections in the definition of a topology?,"I have read the strongly related questions ( ""Why do we require a topological space to be closed under finite intersection?"" , ""For the definition of a topological space, why is the internal union allowed to be infinite, while the internal intersection is restrained to be finite?"" , ""Topology definition: finite intersections vs. infinite unions"" and ""Why for unions we can have arbitrary number of sets and for intersections it's only finite?"" ) and most of those threads answer this question either vaguely by stating that the resulting topology wouldn't be very interesting (as the standard topology is reduced to the discrete topology of R) or via neccessity by showing that the topology would then contain closed sets as well (Although via the topological definition of an open set these would still be open). However, I would like to build upon the first of the answers stated above. In M. Nakahara's book ""Geometry, Topology and Physics"", there is an exercise on page 49 regarding this very issue: ""Exercise 2.25 In definition 2.23, axioms (ii) [closure with respect to unions] and (iii) [closure with respect to intersections] look somewhat unbalanced. Show that, if we allow infinite intersection in (iii), the usual topology in $\mathbb{R}$ reduces to the discrete topology (thus not very interesting)."" While I was able to show this for $\mathbb{R}$ , I was not able to show this for any arbitrary topology/set. My question now is whether any topology is reduced to the discrete topology, if inifinite intersections are allowed? If so, then this would explain why these topologies aren't ""interesting"".","['axioms', 'general-topology', 'set-theory']"
4790444,Determining the distribution of random variable by observation,"We have two probability distributions $\mu, \nu$ on $\mathbb R$ . There is a positive measurable function $f$ on $\mathbb R$ with $$\mu(A) = \int_A f \text d\nu $$ for all Borel measurable $A$ . Let $X$ be a random variable whose distribution is $\mu$ or $\nu$ . We want to determine which is right distribution after observing an instance of $X$ . We have a following strategy: Choose a Borel set $B$ a priori. Observe $X$ ; after observation, we claim that $\mu$ is right if $X \in B$ and $\nu$ is right for $X \not\in B$ . The problem suddenly suggests the function $\phi$ , as $$\phi(B,a) = a \mu(B^c) + (1-a)\nu (B)$$ for al fixed real number $a \in (0,1)$ . The problem says: describe the meaning of $\phi$ , and for a given $a\in(0,1)$ , find $B$ such that $\phi(B,a)$ is minimal. Additionally, the problem asks: what is $f$ , if both $\mu$ and $\nu$ admit positive probability density functions in $\mathbb R$ ? However, I have no idea why the problem suggested such a function $\phi$ and what is $\phi$ having to do with guessing the distribution of $X$ . Plus, why is problem asking what happens if $\mu,\nu$ admit density? I cannot see the link between the setting (guessing the distribution) and the actual thing that the problem is asking for. Thanks in advance for any form of help, hint, or solution.","['measure-theory', 'probability-distributions', 'probability-theory', 'probability', 'random-variables']"
4790535,"Proving that $X$ is bijective to $\{ a, b \} \times X$ if $X$ is an infinite set","I'm trying to prove that for any infinite set $X$ , not assumed to be in bijection with $\mathbb N$ or with $\mathbb R$ , $X$ is bijective with $\{ a, b \} \times X$ . I'm trying to use the Cantor Bernstein theorem, but I'm unable to prove that there exists an injection $\{ a, b \} \times X \rightarrow X$ . I've seen for example here an example of an injection for sets of cardinality $| \mathbb N |$ , and here for sets of cardinalty $| \mathbb R |$ , but these don't seem to generalize to arbitrarily infinite sets. I've also seen that the situation is more complicated when trying to prove that $X \times X$ injects into $X$ , as seen from the second answer here , and that this result relies on the axiom of choice as the first answer points out. To be honest, I have not gone through the proof given in this answer as it seems tedious. I would expect however that there is a shorter proof with a different idea for this case, as it is only a cartesian product with a finite set, after all. So, does $|\{ a,b \} \times X| = |X|$ rely on the axiom of choice? And, more importantly, is there a different proof than that given in the answer to the question about $|X \times X| = |X|$ ?","['elementary-set-theory', 'set-theory']"
4790546,Finding the diameter of a circle projected on a plane,"I am an engineer working on a machine designed to measure the diameters of cylindrical bores. To provide a clear and engaging context for my question, I'll describe the physical scenario in detail. Ideal Senario: Imagine a round measuring tool, represented as a dark blue circle with diameter $d$ , with three probes labeled as $Pa$ , $Pb$ , and $Pc$ . These probes are evenly spaced at angles of $62$ degrees ( $\theta$ ) from each other. The measuring tool is inserted into a round bore, represented as the red circle, to measure its diameter ( $D$ ). The goal is to find $D$ in all senarios. In this ideal scenario, calculating the bore diameter is straightforward since the measuring tool parallel to the part bore and we have 3 points! $Pa=[Pa_x,Pa_y,Pa_z]=[-(9.322+25)Sin⁡(62^{\circ}), -(9.322+25)Cos⁡(62^{\circ}), 0]$ $Pb=[Pb_x,Pb_y,Pb_z]=[0, -(9.988+25), 0]$ $Pc=[Pc_x,Pc_y,Pc_z]=[(15.603+25)Sin⁡(62^{\circ}), -(15.603+25)Cos⁡(62^{\circ}), 0]$ If you're interested in the solution for this ideal case, it can be found below. $x_1 = -(a + r) Sin(\theta)=-(9.322+25)Sin⁡(62^{\circ})$ $y_1 = -(a + r) Cos(\theta)=-(9.322+25)Cos⁡(62^{\circ})$ $x_2 = 0$ $y_2 = -(b + r)=-(9.988+25)$ $x_3 = (c + r) Sin(\theta)=(15.603+25)Sin⁡(62^{\circ})$ $y_3 = -(c + r) Cos(\theta)=-(15.603+25)Cos⁡(62^{\circ})$ $A1 = x_1 (y_2 - y_3) - y_1 (x_2 - x_3) + x_2 y_3 - x_3 y_2$ $B1 = (x_1^2 + y_1^2) (y_3 - y_2) + (x_2^2 + y_2^2) (y_1 - y_3) + (x_3^2 + y_3^2) (y_2 - y_1)$ $C1 = (x_1^2 + y_1^2) (x_2 - x_3) + (x_2^2 + y_2^2) (x_3 - x_1) + (x_3^2 + y_3^2) (x_1 - x_2)$ $D1 = (x_1^2 + y_1^2) (x_3 y_2 - x_2 y_3) + (x_2^2 + y_2^2) (x_1 y_3 - x_3 y_1) + (x_3^2 + y_3^2) (x_2 y_1 - x_1 y_2)$ $Diameter = 2\sqrt{\frac{B1^2 + C1^2 - 4 A1 D1}{4 A1^2}}=80.0002$ This agrees with the CAD! Actual Senario: However, in practical situations, we encounter both lack of measurement accuracy and repeatibility. This is because the measuring tool is not perfectly aligned parallel to the bore. Instead, the bore is at some angle relative to the measuring tool, which we'll refer to as the ""runout angle"" ( $\alpha$ ). Consider the 3d Model of this below: From the perspective of the measuring tool, the measuring tool is now measuring an oval shape. In this case the ideal senario diameter calculation does not hold! See 2D drawing below: We therefore have the following points on the part bore: $Pa=[Pa_x,Pa_y,Pa_z]=[-(9.51+25)Sin⁡(62^{\circ}), -(9.51+25)Cos⁡(62^{\circ}), 0]$ $Pb=[Pb_x,Pb_y,Pb_z]=[0, -(8.71+25), 0]$ $Pc=[Pc_x,Pc_y,Pc_z]=[(15.273+25)Sin⁡(62^{\circ}), -(15.273+25)Cos⁡(62^{\circ}), 0]$ Adding 3 additional probes (which can determine α) Our goal is to determine the actual bore diameter. To achieve this we add three additional probes, labeled as $P1$ , $P2$ , and $P3$ , which are mounted perpendicular to the measuring tool. These probes measure the plane that the part bore is ""mounted"" to! This plane is perpendicular to axis of the bore! Using these probes we are able to determine the runout angle ( $\alpha$ ). Consider the 3D and 2D represntations of this below: Calculating the runout angle is straightforward because it involves three points. $P1 = [P1_x, P1_y, P1_z] = [-25,0,25]$ $P2 = [P2_x, P2_y, P2_z] = [0,-25,15]$ $P3 = [P3_x, P3_y, P3_z] = [25,0,13]$ If you're interested in the solution for finding the runout angle ( $\alpha$ ) from these 3 points, it's available below: We find two vectors on the plane by subtracting one point from another: $\overrightarrow{a}=P2-P1=[25,-25,-10]$ $\overrightarrow{b}=P3-P1=[50,0,-12]$ Now we compute the cross product of these two vectors. This is the vector normal to the part bore. $\overrightarrow{v}=\overrightarrow{a}×\overrightarrow{b}=[v_x,v_y,v_z]=[300,-200,1250]$ We can also define the vector normal to the measuring tool.
This is the vector normal to the plane $z=0$ , which is simply $\overrightarrow{n}=[0,0,1]$ . With both vectors $\overrightarrow{n}$ and $\overrightarrow{v}$ established, we can now calculate the total angle $\alpha$ between them. First, let's calculate the magnitude of $\overrightarrow{v}$ : $M = \sqrt{v_x^2+ v_y^2+ v_z^2}=\sqrt{(300)^2+(-200)^2+(1250)^2}=1300.961$ Next we need to determine the dot product $d$ of the two vectors $\overrightarrow{n}$ and $\overrightarrow{v}$ . $d=v_x×0 + v_y×0+ v_z×1= v_z = 1250 $ Finally we can compute the total runout angle $\alpha$ $\alpha=Cos^{-1}(\frac{d}{M})=Cos^{-1}(\frac{1250}{1300.961})=16.09^{\circ}$ This agrees with the CAD! Now the question arises: Is it possible to obtain the true diameter of the bore from the  points $Pa,Pb,Pc,P1,P2,P3$ ?","['euclidean-geometry', 'projective-geometry', 'circles', 'geometry', 'trigonometry']"
4790556,A short proof of de Finetti's theorem,"Consider de Finetti's theorem in the following form. Theorem. Let $E$ be a Polish space and $(X_1,X_2,\ldots)$ an exchangable sequence of $E$ -valued random variables. Then there exists a (necessarily unique) random probability measure $\mu$ on $E$ such that, for every $n\in \mathbb{N}$ , and $A_1,\ldots,A_n \subset E$ measurable, $$\label{eq1}\tag{1}
\mathbb{P}(X_1\in A_1,\ldots , X_n \in A_n) = \mathbb{E}\left[\mu(A_1) \ldots \mu(A_n)\right].
$$ Furthermore, the weak limit of the empirical measures $$
Z^n = \frac 1n \sum_{i=1}^n \delta_{X_i}
$$ as $n\to \infty$ exists almost-surely and is equal in law to $\mu$ . I wasn't aware of the theorem in this form, and ""accidentally"" reproved it. The proof is quite short and only builds on standard facts from the theory of weak convergence, see the proof sketch below. Question : Is a proof of this length and method novel or interesting? The proof does not feel extremely innovative so I would assume it to be standard, but I am struggling to find out. I found some proofs in the literature that are much more complicated but also more general (like in the paper of Hewitt and Savage), and in standard textbooks like that of Kallenberg the proof is short but builds on so many lemmas developed in the book that I find it hard to say what the main argument is and how complicated it is. Proof sketch: Denote by $\mathcal{M}_1(X)$ for a Polish space $X$ the space of probability measures on $X$ , equipped with the topology of weak convergence. To show that the sequence of empirical measures $(Z^n)$ is relatively compact as a subset of $\mathcal{M}_1(\mathcal{M}_1(E))$ it is sufficient to find for every $\varepsilon > 0$ a compact set $K\subset E$ such that $\mathbb{P}(Z^n(E\setminus K) > \varepsilon) < \varepsilon$ for all $n\in \mathbb{N}$ . Given $\varepsilon > 0$ there exists $K\subset E$ compact such that $\mathbb{P}(X_1 \in E\setminus K) < \varepsilon^2$ , so $$
\mathbb{P}(Z^n(E\setminus K) > \varepsilon) = \mathbb{P}\left( \sum_{i=1}^n \mathbf{1}_{\{X_i \in E\setminus K\}} > n\varepsilon \right) \le \frac{1}{n\varepsilon} \sum_{i=1}^n \mathbb{P}(X_i \in E\setminus K) < \varepsilon
$$ for every $n\in \mathbb{N}$ , where we used that all $X_i$ have the same marginal distribution by exchangability.
Now suppose that a subsequence (which we denote by $Z^n$ again for ease of notation) converges in law to a random probability measure, $Z^n \implies \mu$ . Let $k\in \mathbb{N}$ , and $f_1,\ldots f_k \colon E \to \mathbb{R}$ be continuous and bounded. Then $m \mapsto \int f_1 dm \ldots \int f_k dm$ is a continuous bounded functional on $\mathcal{M}_1(E)$ , so \begin{align}
\mathbb{E}\left[\int f_1 d\mu \ldots \int f_k d\mu\right] 
&= \lim_{n\to \infty} \left[\int f_1 dZ^n \ldots \int f_k dZ^n\right]\\
&=\lim_{n\to \infty} \frac{1}{n^k}\sum_{l_1,\ldots l_k=1}^n \mathbb{E}\left[ f_1(X_{l_1}) \ldots f_k(X_{l_k})\right]\\
&= \mathbb{E}\left[ f_1(X_1) \ldots f_k(X_k)\right],
\end{align} where the last equation uses exchangability and the fact that the number of summands where some number of indices collide is $O(n^{k-1})$ . This implies uniqueness of the subsequential limit and thus convergence of $Z^n$ in law to a random probability measure $\mu$ which satisfies \eqref{eq1}. Almost sure convergence of the $Z^n$ then follows from the general fact that the sequence of normalised empirical measures of a sequence of i.i.d. samples converges almost-surely to the probability measure from which they are sampled. Hewitt, Edwin; Savage, Leonard J. , Symmetric measures on Cartesian products , Trans. Am. Math. Soc. 80, 470-501 (1955). ZBL0066.29604 .","['weak-convergence', 'probability-theory', 'probability']"
4790575,A shape that resembles a fractal,"This shape is modified from Koch snowflake. This line is not smooth and jagged but has finite length unlike the Koch snowflake. I am wondering if there is a name for this kind of shapes. I can't find anything related from google. The shape: Take a line segment, divide the line segment into three segments of equal length. Draw an isosceles triangle which the angle close to the initial line segment is $\frac\pi{3\cdot2^{k}}$ that has the middle segment from step 1 as its base and points outward. Remove the line segment that is the base of the triangle from step 2. Repeat the above steps. Where $k$ in the number of iterations starting from $k=0$ . The totoal length $\prod_{k=0}^\infty{\frac1 3} \left( {2+cot \left(\frac\pi{3\cdot2^{k}}\right)} \right)$ converges. $$\prod_{k=0}^\infty{\frac1 3} \left( {2+cot\left(\frac\pi{3\cdot2^{k}}\right)} \right)$$ $$=exp\left[{\sum_{k=0}^\infty ln \left( {\frac2 3+\frac1 3cot\left(\frac\pi{3\cdot2^{k}}\right)} \right)} \right]$$ $$\le exp\left[{\sum_{k=0}^\infty ln \left( {\frac2 3+\frac1 3 + \frac1 3 \frac1 2{\left(\frac\pi{3\cdot2^{k}}\right)}^2 } \right)} \right]$$ $$\le exp\left[{\sum_{k=0}^\infty \frac1 6{\left(\frac\pi{3\cdot2^{k}}\right)}^2 }  \right]$$ $$\le exp\left[\frac{\pi^2}{6\cdot9}{\sum_{k=0}^\infty {\frac1{2^{2k}}} }  \right]$$ edit: I forgot to mentoin self-similarity. This shape is self-similar but in a distorted way. See graphs below, the ratio which x-axis and y-axis scales are different.","['geometry', 'fractals']"
4790649,$\pi$ approximation method confusion,"I am reading a book ( A History of Pi ) in it there is a story about how Indian mathematicians found the value of $\pi$ by inscribing the polygons in a circle with diameter of 100 and doubling the sides with a formula for side lengths and finally at 384 sides polygon he calculated the value of pi.
This one thing confuses me and I tried every trial and erroneous method but couldn't find a satisfying explanation. Where did this square root of $98694$ came from? assuming a circle diameter of $100$ if we calculate a side length of hexagon and put in the formula it gives square root of negatives Pease help. Thanks.","['math-history', 'pi', 'geometry']"
4790715,Evaluating $\lim\limits_{n \to \infty} ((n^3 + n^2 + n + 1)^{1/3} - n)$,"I am trying to find the limit of the following sequence as $n \to \infty$ . $$\lim\limits_{n \to \infty} ((n^3 + n^2 + n + 1)^{1/3} - n)$$ At first glance, this limit looks like $\infty - \infty$ , which is indeterminate. I do not have access to L'Hospital's rule yet, so I tried to multiply by the quantity $$
1 = \frac{(n^3 + n^2 + n + 1)^{2/3} + n}{(n^3 + n^2 + n + 1)^{2/3} + n},
$$ which treats the cubed root in the numerator but creates a problem in the denominator. After simplifying, the limit turns into $$
\frac{n^3 + n + 1}{(n^3 + n^2 + n + 1)^{2/3} + n}.
$$ I then tried dividing the numerator and denominator by $n^3$ . The numerator simplifies nicely, but the factor $(n^3 + n^2 + n + 1)^{2/3}$ does not. I can work the $n^3$ into the cubed root either by writing $n^3 = (\sqrt{n})^3$ or by rewriting the root as $$
(n^3 + n^2 + n + 1)^{2/3} = \frac{(n^3 + n^2 + n + 1)^2}{n^3}.
$$ I then tried to expand out the numerator of this and cancel, but I still have several terms with powers of $n$ . I don't think I can divide by a higher power of $n$ at risk of creating an expression in the overall numerator $n^3 + n + 1$ that goes to $0$ . So I'm a bit stuck on how to proceed.","['limits', 'proof-explanation']"
4790727,"Find $a_n,b_n$ so that $b_n(X_n-a_n)$ converges to a non-degenerate limit","I have the following problem: Let $X_n$ be the maximum of a random sample $Y_1,...,Y_n$ from the density $f(x)=2(1-x), x\in [0,1]$ . Find constants $a_n,b_n$ so that $b_n(X_n-a_n)$ converges in distribution to a non-degenerate limit. I did the following: Calculate $F(x)=x(2-x), x\in [0,1]$ and find distribution of $X_n$ : $$ F_{X_n}(x) = \mathbb{P}[max{Y_i} \leq x] = \mathbb{P}[Y_1 \leq x,...,Y_n \leq x] = \mathbb{P}[Y_1 \leq x]^n = x^n(2-x)^n$$ So $$ \mathbb{P}[b_n(X_n-a_n) \leq x] = \mathbb{P}[X_n \leq \dfrac{x}{b_n}+a_n] = F_{X_n}(\dfrac{x}{b_n}+a_n)=(\dfrac{x}{b_n}+a_n)^n (2-\dfrac{x}{b_n}-a_n)^n$$ My intuition here is to use that $(1+\dfrac{x}{n})^n \rightarrow e^x$ and choose $a_n=1 , b_n=n$ In that case $$ \mathbb{P}[b_n(X_n-a_n) \leq x] \rightarrow e^xe^{-x}=1$$ However if I understand correctly a ""non-degenerate"" limit means that it should not be a constant. Can someone please correct me? What should $a_n,b_n$ be?","['extreme-value-theorem', 'statistics', 'weak-convergence', 'probability-distributions', 'probability']"
4790746,On what intervals does $\;a\sin[f(x)]+b\cos[g(x)]\;$ equal $\sqrt{a^2+b^2+2ab\sin(f(x)-g(x))}\sin\left(\frac{f(x)+g(x)}{2}+\arctan M\right)\;?$,"Let $$M=\frac{a\sin\left(\frac{f\left(x\right)-g\left(x\right)}{2}\right)+b\cos\left(\frac{f\left(x\right)-g\left(x\right)}{2}\right)}{a\cos\left(\frac{f\left(x\right)-g\left(x\right)}{2}\right)+b\sin\left(\frac{f\left(x\right)-g\left(x\right)}{2}\right)}$$ Suppose that either it is the case that $a$ and $b$ are a pair of single-variable functions of $x$ or it is the case that $a$ and $b$ are a pair of constants. The question is: On what union of intervals $I$ is it true that $$a\sin(f(x))+b\cos(g(x))$$ $$=$$ $$\sqrt{a^2+b^2+2ab\sin(f(x)-g(x))}\sin(\frac{f(x)+g(x)}{2}+\arctan{M})?$$ What is known - I claim - is that for all $x$ $\in$ $ℝ\setminus{I}$ we have: $$a\sin(f(x))+b\cos(g(x))$$ $$=$$ $$-\sqrt{a^2+b^2+2ab\sin(f(x)-g(x))}\sin(\frac{f(x)+g(x)}{2}+\arctan{M})$$ I'm interested in defining the union of intervals $I$ in terms of $a,b,f$ and $g$ In the case of $a$ and $b$ being functions of $x$ , it's worth pointing out that the process of condensing a sum of two terms like $a(x)\sin(f(x))$ and $b(x)\cos(g(x))$ into one term can be iterated in order to consolidate an expression with arbitrarily many sine and cosine terms into one term, by taking two at a time until you end up with only one. The potential to consolidate large expressions of that form in this way provides me the motivation for wanting to know how it works when we have only two terms. An immediate observation that can be made is that trying a calculus-based approach will inevitably result in very large and messy expressions. Here you can see how long and messy of an expression the derivative is. Here (1) in desmos I've set up a way to easily see that on some intervals it would be appropriate to use the positive square root (on these intervals the graphs overlap perfectly) and on every other interval it would instead be appropriate to use the negative square root (on these intervals the graphs do not overlap) Here (2) in desmos I've set up the same thing but with $a$ and $b$ as functions instead of constants Here is the derivation: $$a\sin\left[f\left(x\right)\right]+b\cos\left[g\left(x\right)\right]=a\sin\left(\frac{f\left(x\right)+g\left(x\right)}{2}+\frac{f\left(x\right)-g\left(x\right)}{2}\right)+b\cos\left(\frac{f\left(x\right)+g\left(x\right)}{2}-\frac{f\left(x\right)-g\left(x\right)}{2}\right)$$ Let $$u=\frac{f\left(x\right)+g\left(x\right)}{2}$$ $$v=\frac{f\left(x\right)-g\left(x\right)}{2}$$ With that substitution we then have $$a\sin\left(u+v\right)+b\cos\left(u-v\right)=\left[a\cos\left(v\right)+b\sin\left(v\right)\right]\sin\left(u\right)+\left[a\sin\left(v\right)+b\cos\left(v\right)\right]\cos\left(u\right)$$ Now suppose this can be expressed as $A(x)\sin\left(u+\phi\right)$ . Then we have $$A(x)\sin\left(u+\phi\right)=A(x)\sin\left(u\right)\cos\left(\phi\right)+A(x)\cos\left(u\right)\sin\left(\phi\right)$$ Combining this with the above we have $$A(x)\cos\phi=a\cos (v)+b\sin (v)$$ and $$A(x)\sin\phi=a\sin (v)+b\cos (v)$$ This implies that $$[A(x)]^{2}=a^{2}+b^{2}+4ab\cdot\sin\left(v\right)\cos\left(v\right)=a^{2}+b^{2}+2ab\cdot\sin\left(2v\right)$$ and $$\tan\phi=\frac{a\sin (v)+b\cos (v)}{a\cos (v)+b\sin (v)}$$ Here's a bit of progress I made. Case 1 This is a special case of case 3 Suppose $a$ and $b$ are constant coefficients and $f(x)=g(x)$ .
By testing out all possible combinations of $a,b$ (-,-), (-,+), (+,-), (+,+), you can deduce the following: If a is nonnegative, then $$a\sin(f(x))+b\cos(f(x))=\sqrt{a^2+b^2}\sin(f(x)+\arctan{(\frac{b}{a}}))$$ If a is negative, then $$a\sin(f(x))+b\cos(f(x))=-\sqrt{a^2+b^2}\sin(f(x)+\arctan{(\frac{b}{a}}))$$ Here (3) in desmos I've set up a way to verify that these hold by adjusting the values for $a$ and $b$ and modifying $f(x)$ to be any function of your choosing Case 2 Suppose that $a(x)$ and $b(x)$ are $x^\alpha$ and $x^\beta$ respectively and $f(x)=g(x)$ For all $x$ $\in$ $[0,∞)$ we have $$a(x)\sin(f(x))+b(x)\cos(f(x))=\sqrt{(a(x))^2+(b(x))^2}\sin(f(x)+\arctan{(x^{\beta-\alpha}})).$$ Here (4) in desmos I've set up a way to verify that this indeed holds regardless of what values you choose for $\alpha$ and $\beta$ and regardless of what function you choose $f$ to be. More does need to be said about this case but for now this is all I've worked out for case 2. Case 3 Suppose that $a$ and $b$ are constant coefficients Let $$u=\frac{f(x)+g(x)}{2}$$ Let $$v=\frac{f(x)-g(x)}{2}$$ Let $$\tan\psi=\frac{a\sin(v)+b\cos(v)}{a\cos(v)+b\sin(v)}$$ Then $$\cos^{2}\left(ψ\right)=\frac{a^{2}\cos^{2}\left(v\right)+ab\cdot\sin\left(2v\right)+b^{2}\sin^{2}\left(v\right)}{a^{2}+b^{2}+2ab\cdot\sin\left(2v\right)}$$ and $$\sin^{2}\left(ψ\right)=\frac{a^{2}\sin^{2}\left(v\right)+ab\cdot\sin\left(2v\right)+b^{2}\cos^{2}\left(v\right)}{a^{2}+b^{2}+2ab\cdot\sin\left(2v\right)}$$ We want to express $a\sin\left[f\left(x\right)\right]+b\cos\left[g\left(x\right)\right]$ in the form $A(x)\sin(u+\psi)$ We can see that $A(x)\sin(u+\psi)=A(x)\cos(\psi)\sin(u)+A(x)\sin(\psi)\cos(u)$ Thus, we can observe that we have to carefully choose the appropriate
square root of $\cos^{2}\left(ψ\right)$ as well as carefully choose
the appropriate square root of $\sin^{2}\left(ψ\right)$ We can rewrite $a\sin\left[f\left(x\right)\right]+b\cos\left[g\left(x\right)\right]$ as $a\sin(u+v)+b\cos(u-v))$ which then becomes $(a\cos(v)+b\sin(v))\sin(u)+(a\sin(v)+b\cos(v))\cos(u)$ Thus, we have $(a\cos(v)+b\sin(v))\sin(u)+(a\sin(v)+b\cos(v))\cos(u)=A(x)\cos(\psi)\sin(u)+A(x)\sin(\psi)\cos(u)$ This gives us the following two facts: $a\cos(v)+b\sin(v)=A(x)\cos(\psi)$ $a\sin(v)+b\cos(v)=A(x)\sin(\psi)$ If $b$ is nonnegative then $a\cos(v)+b\sin(v)=\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{a}{b})})$ If $b$ is negative then $a\cos(v)+b\sin(v)=-\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{a}{b})})$ Here (5) in desmos you can verify this for yourself by adjusting the sliders for $a$ and $b$ If $a$ is nonnegative then $a\sin(v)+b\cos(v)=\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{b}{a})})$ If $a$ is negative then $a\sin(v)+b\cos(v)=-\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{b}{a})})$ Here (6) in desmos you can verify this for yourself by adjusting the sliders for $a$ and $b$ Thus, we have: If $b$ is nonnegative then $A(x)\cos(\psi)=\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{a}{b})})$ If $b$ is negative then $A(x)\cos(\psi)=-\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{a}{b})})$ If $a$ is nonnegative then $A(x)\sin(\psi)=\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{b}{a})})$ If $a$ is negative then $A(x)\sin(\psi)=-\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{b}{a})})$ We effectively demonstrated earlier that $$\cos\left(ψ\right)=\frac{±(a\cos\left(v\right)+b\sin\left(v\right))}{\sqrt{a^{2}+b^{2}+2ab\cdot\sin\left(2v\right)}}$$ and $$\sin\left(ψ\right)=\frac{±(a\sin\left(v\right)+b\cos\left(v\right))}{\sqrt{a^{2}+b^{2}+2ab\cdot\sin\left(2v\right)}}$$ And we also know that $$A(x)=±\sqrt{a^{2}+b^{2}+2ab\cdot\sin\left(2v\right)}$$ Subcase A Suppose both $a$ and $b$ are nonnegative constant coefficients Then: $A(x)\cos(\psi)=\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{a}{b})})$ $A(x)\sin(\psi)=\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{b}{a})})$ Subcase B Suppose $a$ is a nonnegative constant coefficient and $b$ is a negative constant coefficient Then: $A(x)\cos(\psi)=-\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{a}{b})})$ $A(x)\sin(\psi)=\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{b}{a})})$ Subcase C Suppose $a$ is a negative constant coefficient and $b$ is a nonnegative constant coefficient Then: $A(x)\cos(\psi)=\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{a}{b})})$ $A(x)\sin(\psi)=-\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{b}{a})})$ Subcase D Suppose both $a$ and $b$ are negative constant coefficients Then: $A(x)\cos(\psi)=-\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{a}{b})})$ $A(x)\sin(\psi)=-\sqrt{a^2+b^2}\sin(v+\arctan{(\frac{b}{a})})$",['trigonometry']
4790867,Help finding extrema of a function using lagrange multipliers,"I am trying to find the extreme values of the function $f(x,y,z)=x^2+y^2+z^2$ given the constraints $g(x,y,z)=x-y=1$ and $h(x,y,z)=y^2-z^2=1$ . I am rather lost on this question as it feels as though I am taking the right steps but keep coming to impossible solutions. Here is my current work: $$\nabla f = \begin{bmatrix}2x\\ 2y\\ 2z\end{bmatrix}$$ $$\nabla g = \begin{bmatrix}1\\ -1\\ 0\end{bmatrix}$$ $$\nabla h = \begin{bmatrix}0\\ 2y\\ -2z\end{bmatrix}$$ Let $2z = -2bz\implies b = -1$ , and $a=2x$ ; we have $$2y = -a + 2by$$ $$2y = -a - 2y$$ $$4y = -a$$ $$2x = -4y$$ $$x = -2y$$ $x - y = 1\implies$ $$-3y = 1 \implies \begin{cases} y = -\frac{1}{3} \\ x = \frac23\end{cases} $$ Substituting this into $$y^2 - z^2 = 1$$ Gives us $$z^2 = -\frac89$$ A squared value cannot be negative (complex numbers don't make sense in this case). I don't understand where I have gone wrong. I would like to know where I might've gone wrong.","['multivariable-calculus', 'calculus', 'solution-verification', 'lagrange-multiplier']"
4790912,When is $\mathbb{F}_{p^k}^{\times} \cong \mathbb{Z}_m^{\times}$ for $k>1$?,"Let $\mathbb{F}_{p^k}^{\times}$ denote the group of units of a finite field, and let $\mathbb{Z}_m^{\times}$ denote the group of units modulo some integer $m$ . I'm curious about when $\mathbb{F}_{p^k}^{\times} \cong \mathbb{Z}_m^{\times}$ . Two relevant and well known results are: The group of units of a finite field is cyclic . $\mathbb{Z}_m^{\times}$ is cyclic iff $m=2, 4, q^k$ or $2q^k$ where $q$ is an odd prime and $k \geq 1$ . For $k=1$ , we get a solution whenever $|\mathbb{Z}_m^{\times}| = \varphi(m) = p-1$ and $\mathbb{Z}_m^{\times}$ is cyclic (by the first result and since cyclic groups of the same order are isomorphic). Then by the second result, $m=p$ and $m=2p$ are the only such values where $p$ is a prime. For $k>1$ , by the first and second result we need to find $\mathbb{F}_{p^k}^{\times} \cong \mathbb{Z}_{q^s}^{\times} \cong \mathbb{Z}_{2q^s}^{\times}$ for an odd prime $q$ . And since cyclic groups of the same order are isomorphic, it suffices to have $|\mathbb{F}_{p^k}^{\times}| =|\mathbb{Z}_{q^s}^{\times}|$ . Thus, we get the diophantine equation $p^k - 1 = q^s - q^{s-1}$ which has solutions for odd primes $p,q$ iff $\mathbb{F}_{p^k}^{\times} \cong \mathbb{Z}_{q^s}^{\times} \cong \mathbb{Z}_{2q^s}^{\times}$ . One might suspect that this would not have solutions, but there is this one: $7^3 - 1 = 19^2 - 19 \implies \mathbb{F}_{7^3}^{\times} \cong \mathbb{Z}_{19^2}^{\times} \cong \mathbb{Z}_{2 \cdot19^2}^{\times}$ I've checked for $p, q, k, s \leq 300$ and this appears to be the only such solution. I have tried to prove this and have a few very partial results. For instance, any solution must satisfy the following conditions: $q > p \iff (p-1) \lvert (q-1)$ This follows fairly easily after reducing modulo $p-1$ . $q^{s-1} < p^k < q^s$ Since $p^k - 1 = q^s - q^{s-1} < q^s$ and we cannot have $p^k = q^s$ for distinct primes $p, q$ so, $p^k < q^s$ . For the other inequality, $p^k > p^k - 1 = q^s - q^{s-1} = q^{s-1}(q-1) > q^{s-1}$ . Any suggestions how one can show that $p^k - 1 = q^s - q^{s-1}$ has no solutions with $k > 1$ beyond $(p, q, k, s) = (7, 19, 3, 2)$ ? EDIT (22/10/23): This ""proof"" is currently wrong, but I will keep it here for future reference (see comments) I believe we can show $s=2$ for any solution. Let $s > 1$ . Since $(p-1) \lvert (q-1)$ , we of course get that $t := \frac{q-1}{p-1}$ is an integer. Writing the equation as, $$p^k - 1 = q^{s-1}(q - 1)$$ factoring, $$(p-1)(1 + p + \cdots + p^{k-1}) = q^{s-1}(q-1)$$ $$(1 + p + \cdots + p^{k-1}) = q^{s-1} \frac{q-1}{p-1}$$ $$(1 + p + \cdots + p^{k-1}) = q^{s-1} t = q^{s-2} \cdot qt$$ We want to show that $GCD(1 + p + \cdots + p^{k-1}, q^{s-2}) = 1$ . Otherwise, we would get that $1 + p + \cdots + p^{k-1} = q^r$ for $r \leq s-2 \implies p^{k-1} < q^r \leq q^{s-2}$ . We also have, $p^k > p^k - 1 = q^s - q^{s-1} > q^{s-1}$ since $q > 2$ . Putting these together, we get that $p^{k-1} < q^{s-2} < q^{s-1} < p^k$ which is a contradiction since $q > p$ . Thus, $GCD(1 + p + \cdots + p^{k-1}, q^{s-2}) = 1$ and since $1 + p + \cdots + p^{k-1} = q^{s-2} \cdot qt$ we have, $$q^{s-2} = 1 \implies s = 2$$ .","['field-theory', 'number-theory', 'group-theory', 'diophantine-equations']"
4790960,Conjecture for Integrals of the Form $\int_{0}^{1}\frac{\ln^n\left(1-x^{2}\right)}{1+x}dx$,"Recently I got interested in Logarithmic Integrals from this $\int_{0}^{1}\frac{\ln^4\left(1-x^{2}\right)}{1+x}dx$ and sought out to find Higher Power variations. Let, $$I_n=\int_{0}^{1}\frac{\ln^n\left(1-x^{2}\right)}{1+x}dx$$ Consider the following expression : $$E=\zeta(\alpha_1)\zeta(\alpha_2)...\zeta(\alpha_n)\ln^\alpha2$$ whose weight is $\alpha_1+\alpha_2+...+\alpha_n+\alpha.$ $$\boxed{\text{Conjecture: All $I_n$ can be written as a Linear Combination involving Expression $E$ of weight $n+1$.}}$$ Here are the first few examples : $$I_1=-\frac{1}{2}\zeta\left(2\right)+\ln^{2}2$$ $$I_2=2\zeta\left(3\right)-2\zeta\left(2\right)\ln 2+\frac{4}{3}\ln^32$$ $$I_3=-\frac{27}{4}\zeta\left(4\right)+12\zeta\left(3\right)\ln2-6\zeta\left(2\right)\ln^22+2\ln^42$$ $$I_4=72ζ\left(5\right)-24ζ\left(3\right)ζ\left(2\right)-54ζ\left(4\right)\ln2+48ζ\left(3\right)\ln^{2}2-16ζ\left(2\right)\ln^{3}2+\frac{16}{5}\ln^{5}2$$ $$I_5=-\frac{1185}{4}ζ\left(6\right)+120ζ^{2}\left(3\right)+720ζ\left(5\right)\ln2-240ζ\left(3\right)ζ\left(2\right)\ln2-270ζ\left(4\right)\ln^{2}2+160ζ\left(3\right)\ln^{3}2-40ζ\left(2\right)\ln^{4}2+\frac{16}{3}\ln^{6}2$$ $$I_6=6480ζ\left(7\right)-2160ζ\left(5\right)ζ\left(2\right)-1620ζ\left(4\right)ζ\left(3\right)-3555ζ\left(6\right)\ln2+1440ζ^{2}\left(3\right)\ln2+4320ζ\left(5\right)\ln^{2}2-1440ζ\left(3\right)ζ\left(2\right)\ln^{2}2-1080ζ\left(4\right)\ln^{3}2+480ζ\left(3\right)\ln^{4}2-96ζ\left(2\right)\ln^{5}2+\frac{64}{7}\ln^{7}2$$ Could the Conjecture be Proved or Disproved. I had made a Similar Conjecture for another Logarithmic Integral of Similar Form but it stopped working from Weight $8$ .","['integration', 'calculus', 'conjectures', 'definite-integrals']"
4791008,Indefinite Integral of all even function $= 0$?,"I am wondering what's wrong with the following decent-looking proof: \begin{align*}
\int x^2 dx &= \int (-u)^2(-du)\quad\text{ Let } x=-u\\
&=-\int u^2 du\\
&=-\int x^2 dx \quad \text{Change of dummy variable} \\
2\int x^2 dx&=0\\
\therefore \quad \int x^2 dx&=0
\end{align*} Which is obviously wrong as $\int x^2 dx=\frac{x^3}{3}+C$ . But I am not quite sure which part went wrong, great thanks for your help.","['integration', 'indefinite-integrals']"
4791030,how to visualise DAE as ODE on manifold,"In many papers and textbooks, there is always a brief comment that differential-algebraic equations (DAE) can be considered as a system of ordinary differential equation (ODE) on manifold. But then nowhere is it explained what it actually means. I desperately want to see what does it mean and visualize that manifold. What are the necessary steps to do? Is the algebraic part defining some sort of manifold and if so, how? Some (possibly schematic only) concrete example is much appreciated. Edit. To keep things simple, lets focus only on index-1 DAEs as suggested in the comment. Edit2 Is it like the follwoing? For every (consistent) initial values, I have to find all solutions and the result is the manifold?
Is the resulting quite different from what I would get when DAE is further reduced do ODE?","['differential-algebraic-equations', 'ordinary-differential-equations']"
4791032,Triangle inside a unit square.,"Consider a square of size 1 and a triangle of sides $a$ , $b$ and $c$ . Is there a way to know if the triangle can fit inside the square? Clearly, if $a>\sqrt{2}$ or $b>\sqrt{2}$ or $c>\sqrt{2}$ , it is not possible, but a triangle of sides $\frac{7}{5}$ , $\frac{7}{5}$ and $\frac{1}{4}$ does not fit. My first idea was to rotate the triangle and see if the maximum distance between points in each axis is less than 1...","['euclidean-geometry', 'triangles', 'geometry', 'algorithms']"
4791044,Universal ring with zero matrix product is an integral domain?,"Let $n \geq 0$ . The universal commutative ring which has two $n \times n$ -matrices whose product is zero is $$\textstyle R := \mathbb{Z}[(X_{ij})_{1 \leq i,j \leq n}, (Y_{ij})_{1 \leq i,j \leq n}] / \langle \sum_{k} X_{ik} Y_{kj} = 0 : 1 \leq i,j \leq n \rangle$$ So we have $2n^2$ generators and $n^2$ relations. If $n = 1$ , then $R$ is just $\mathbb{Z}[u,v] / \langle uv = 0 \rangle$ , hence not an integral domain. Question. If $n > 1$ , is $R$ an integral domain? And if not, can we at least show that $R$ is reduced? Unfortunately I don't have an idea how to attack this problem, already for $n=2$ . I vaguely remember that there are methods from algebraic geometry to show that a ring is an integral domain, but I don't know if this applies here.","['matrices', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4791075,Understanding the construction of the split extension of group,"This is the 2nd part of another question, mainly general extension . Please have a look to understand the notation. A brief description was copied from that thread, Let $\phi$ be an isomorphism of $G/H$ onto $K$ . Let $X$ be a left transversal of $H$ in $G$ . If $g\in G, g=xh$ for some $x\in X,h\in H$ Then $$gx\in \text{some coset }yH\implies gx=yh=ym_{g,x}\tag1$$ $(gH)\phi=k\in K$ , let $x_k\in X$ be the representative of $gH$ . $X=\{x_k:k\in K\}$ and $x_1=1$ . Now, $(x_kx_{k'}H)\phi=kk'$ and using $(1)$ , $$x_kx_{k'}=x_{kk'}m_{x_k x_{k'}}=x_{kk'}m_{kk'}\quad[\text{shorthand notation}]$$ Consider the split extension where $m_{x,x'}=1$ for all $x,x'\in X$ in $(1)\implies xx'=x''$ . Question: Can we always guarantee the existence of $m_{x,x'}=1$ ? Then $X<G,H\triangleleft G$ and $G=XH$ . $G$ in an extension of $H$ by $X$ .
Therefore we may suspect that if we are given, a group $H$ , a group $K$ , a homomorphism $\alpha$ of $K$ into the automorphism group of $H$ then we can create a splitting extension of $H$ by $K$ , $$
\begin{align}
G=K\times H=&\{(k,h):k\in K,h\in H\}\\
(k,h)(k',h')=&(kk',h(k'\alpha)h')\\
(k,h)^{-1}=&(k^{-1},h^{-1}(k^{-1}\alpha))
\end{align}
$$ Question: How they come up with this multiplication and the inverse? Lastly, I couldn't catch the whole story, it seems like the construction isn't intuitive enough. And I couldn't get the similarity with the Wikipedia definition , A split extension is an extension $1\to K\to G\to H\to 1$ with a homomorphism $s\colon H\to G$ such that going from $H$ to $G$ by $s$ and then back to $H$ by the quotient map of the short exact sequence induces the identity map on $H$ i.e., $\pi \circ s={\mathrm  {id}}_{H}$ . In this situation, it is usually said that $s$ splits the above exact sequence.","['reference-request', 'group-extensions', 'abstract-algebra', 'group-theory', 'soft-question']"
4791099,An algebraic stack as a non-linear analog of a complex of vector spaces,"In their paper Derived Quot Schemes Kapranov and Ciocane-Fontanine write in the introduction: Indeed, an algebraic stack is a nonlinear analog of a complex of vector spaces situated in degrees $[-1,0]$ and, for example, the tangent ""space"" to a stack at a point is a complex of this nature. Can someone explain this analogy? Pointers to the literature would also be appreciated.","['homological-algebra', 'tangent-spaces', 'algebraic-stacks', 'algebraic-geometry', 'schemes']"
4791179,Definition of local homeomorphism,"According to Wikipedia : A function $f:X→Y$ between two topological spaces is called a local homeomorphism if every point $x\in X$ has an open neighborhood $U$ whose image $f(U)$ is open in $Y$ and the restriction $f\rvert_U:U→f(U)$ is a homeomorphism (where the respective subspace topologies are used on $U$ and on $f(U)$ . However, if we only ask that for every $x \in X$ there is a neighborhood $U$ such that $f\rvert_U:U→f(U)$ is a homeomorphism, do we get a local homeomorphism? It seems to me that yes but I'm sure I'm wrong because this definition would be simpler. Let $x \in X$ and $U$ a neighborhood of $x$ such that $f\rvert_U : U \rightarrow f(U)$ is a homeomorphism. There exists an open $O$ such that $x \in O \subseteq U$ so $f(x) \in f(O) \subseteq f(U)$ and $f(O) = f\rvert_U(O)$ is open because $f\rvert_U$ is a homeomorphism. Moreover, $f\rvert_O : O \rightarrow f(O)$ is a homeomorphism because $O \subseteq U$ . So the open $O$ satisfies the first definition. And the other implication is obvious so the two definitions would be equivalents. Can someone explain me where is my mistake? EDIT: So we should ask that for every $x \in X$ there is a neighborhood $U$ such that $f(U)$ is a neighborhood of $f(x)$ and $f\rvert_U:U→f(U)$ is a homeomorphism? EDIT2: If we  ask that for every $𝑥 \in 𝑋$ there is a neighborhood $U$ such that $f(U)$ is a neighborhood of $f(x)$ and $f\rvert_U: U \rightarrow f(U)$ is a homeomorphism : Let $O_X$ an open of $X$ such that $x \in O_X \subseteq U$ and $O_Y$ an open of $Y$ such that $f(x) \in O_Y \subseteq f(U)$ then we consider the open $O_X \cap f^{-1}(O_Y)$ whose image $f(O_X) \cap O_Y$ is an open of $Y$ (because it is an open of $O_Y$ and $O_Y$ is open) and we have a homeomorphism $\overline{f} : O_X \cap f^{-1}(O_Y) \rightarrow f(O_Y) \cap O_Y$ . However I don't understand the terminology of étale space because we say that $X$ is an étale space of $Y$ if there is a local homeomorphism $f : X \rightarrow Y$ so an open subset of $Y$ would be an étale space. Am I wrong?
Thank you",['general-topology']
4791202,"If $4^x + 4^{-x} = 5$, what is $8^x + 8^{-x}$?","I found this competition math problem that I haven't been able to solve. If $4^x + 4^{-x} = 5$ , find $8^x + 8^{-x}$ . Setting $a = 4^x$ , we see the problem is equivalent to saying: If $a + a^{-1} = 5$ , find $a^{3/2} + a^{-3/2}$ . So $a$ is the solution to the quadratic equation $a^2 - 5a+ 1 = 0$ .  Explicitly, $a = \frac{5 + \sqrt{21}}{2}$ , which is a unit in the ring integers of $\mathbb Q(\sqrt{21})$ .  The expression $a+a^{-1}$ is then equal to the trace of $a$ .  Then $a^{3/2} + a^{-3/2}$ should be another trace. I would love to see if there is a nice solution to this problem using algebraic number theory.  But it should also be possible to solve using elementary methods, since it is a high school competition math problem.",['algebra-precalculus']
4791219,Can the linear space of all finite signed measures be put an inner product so that $\mu\bot\nu$ exactly when $\mu$ and $\nu$ are mutually singular?,"In Folland's ""Real Analysis"", two signed measures $\mu,\nu$ on a measurable space $(X,\mathcal{M})$ are said to be mutually singular if there is a decomposition of $X=E\cup F$ so that $E$ is null for $\mu$ and $F$ is null for $\nu,$ and he denotes it by $\mu\bot\nu.$ The notation "" $\bot$ "" caught my interest, since we often use it to represent perpendicularity in inner product space. So if we set $$V=\left\{\text{all finite signed measures on a measurable space}~(X,\mathcal{M})\right\}$$ it's clear that $V$ is a real vector space. So my question is: can we put an inner product $\langle\cdot,\cdot\rangle$ on $V$ such that $\langle\mu,\nu \rangle=0$ exactly when $\mu,\nu$ are mutually singular? (If this is true, then the Radon-Nikodym theorem just states that for any fixed $\mu\in V,$ $V$ has an orthogonal decomposition of $MS_{\mu}$ and $AC_{\mu},$ where $MS_{\mu}$ consists of all $\nu\in V$ that is mutually singular to $\mu$ and $AC_{\mu}$ consists of all $\nu\in V$ that's absolutely continuous to $\mu,$ which is really interesting.)","['measure-theory', 'radon-nikodym', 'real-analysis']"
4791259,An identity related to the series $\sum_{n\geq 0}p(5n+4)x^n$ in Ramanujan's lost notebook,"While browsing through Ramanujan's original manuscript titled ""The Lost Notebook"" (the link is a PDF file with 379 scanned pages, so instead of a click it is preferable to download) I found this identity (numbered (4.5))  on page 139 $$x\cdot\frac{\left\{(1-x^5)(1-x^{10})(1-x^{15})\dots\right\} ^5} {(1-x)(1-x^2)(1-x^3)\dots} =\frac{x} {(1-x)^2}-\frac{x^2}{(1-x^2)^2}-\frac{x^3}{(1-x^3)^2}+\frac{x^4}{(1-x^4)^2}+\frac{x^6}{(1-x^6)^2}-\frac{x^7}{(1-x^7)^2}-\dots\tag{1}$$ which can be more compactly written as $$x\prod_{n\geq 1}\frac{(1-x^{5n})^5}{1-x^n}=\sum_{n\geq 1}\left(\frac{n}{5}\right)\frac{x^n}{(1-x^n)^2}$$ Next Ramanujan says ""it follows from $(1)$ that $$\{(1-x)(1-x^2)(1-x^3)\dots \} ^5\{p(4)x+p(9)x^2+p(14)x^3+\dots\}=5\left\{\frac{x} {(1-x)^2}-\frac{x^2}{(1-x^2)^2}-\frac{x^3}{(1-x^3)^2}+\frac{x^4}{(1-x^4)^2}+\frac{x^6}{(1-x^6)^2}-\dots\right\}\tag{2}$$ and hence that $$p(4)+p(9)x+p(14)x^2+p(19)x^3+\dots=5\cdot\frac{\{(1-x^5)(1-x^{10})(1-x^{15})\dots\}^5 } {\{(1-x)(1-x^2)(1-x^3)\dots\}^6}\tag{3} $$ Here $p(n) $ denotes the number of unrestricted partitions of a positive integer $n $ and $\left(\dfrac{a}{p} \right) $ denotes the Legendre symbol . The identity $(3)$ mentioned above is famous and is used to derive many partition congruences with $$p(5n+4)\equiv 0\pmod{5}$$ as an immediate consequence. A complete proof of $(3)$ based on different ideas is available in this answer . It took me sometime to figure out as to how $(2)$ follows from $(1)$ . The technique Ramanujan uses here is to expand both sides in powers of $x$ and then pick only the terms with $x^{5n}$ and replace $x^5$ by $x$ . In this process we also make use of the fact that $$1+\sum_{n\geq 1}p(n)x^n=\frac{1}{(1-x)(1-x^2)(1-x^3)\dots}\tag{4}$$ What is really mysterious is the origin of the identity $(1)$ and I am trying to find a proof. I did check the nearby pages of the Lost Notebook and did not find any proofs or hint. However on the same page 139 of the notebook there is another identity (numbered (4.6)) $$\frac{\left\{(1-x)(1-x^2)(1-x^3)\dots\right\} ^5} {(1-x^5)(1-x^{10})(1-x^{15})\dots} =1-5\left(\frac{x} {1-x}-\frac{2x^2}{1-x^2}-\frac{3x^3}{1-x^3}+\frac{4x^4}{1-x^4}+\frac{6x^6}{1-x^6}-\frac{7x^7}{1-x^7}-\dots\right) \tag{5}$$ The identities $(1),(5)$ appear intimately tied to each other and seem to come out of nowhere. Any proofs or references regarding proofs of the identities $(1),(5)$ are desired.","['integer-partitions', 'q-series', 'sequences-and-series']"
4791286,Reinhardt's Polyhedron as the first couterexample to the second part of Hilbert's 18th problem,"In 1928 Karl Reinhardt published a first solution to the second part of Hilbert's 18th problem ""Über die Zerlegung der euklidischen Räume in kongruente Polytope"" in ""Sitz. Ber. Preuß. Ak. Wiss. Phys. math. Kl."", p. 150ff, however, it seems that this is not easily accessible on the web. Google claims that it is a 6-page paper. The solution should consist of a 3-dimensional anisohedral polyhedron. The wikipedia article on anisohedral tilings features only the 2-dimensional polygon from Heinrich Heesch . Similarly John Milnor's article ""Hilbert's problem 18: On crystallographic groups, fundamental domains, and on sphere packing"" in ""Mathematical developments arising from Hilbert problems, part 2"" from 1976, explicitly only describes 2-dimensional polygons, including Heesch's polygon, referring to Reinhardt's 3-dimensional polyhedron as a rather complicated 3-dimensional counter-example omitting any further details about it. Question. Where can one find Reinhardt's original paper on the web? How does Reinhardt's polyhedron and the associated tiling look like? Where can one find pictures?","['euclidean-geometry', 'polyhedra', 'geometry', 'reference-request', 'tiling']"
4791296,"Find all differentiable functions $g: \mathbb{R} \rightarrow \mathbb{R}$, for which $g'$ is continuous such that $g'(x)\geq (g(x))^2$ for every $x$","Find all differentiable functions $g: \mathbb{R} \rightarrow \mathbb{R}$ , for which $g'$ is continuous such that $g'(x)\geq (g(x))^2$ for every $x \in \mathbb{R}$ My work: The only function $g$ that I was able to find is $g=0_{\mathbb{R}}$ . Firstly, it is obvious that $(g(x))^2\geq0$ , so $g'(x)\geq0$ for every $x \in \mathbb{R}$ , therefore, $g$ is an increasing function. My best try for this problem was to try to choose $x \in \mathbb{R}$ , and $m \in \mathbb{R_+}$ such that I will somehow be able to apply the fact that $g'(c)=\frac{g(x+m)-g(x)}{m}$ , for some $c\in(x,x+m)$ by Lagrange's theorem. From here it follows that $\frac{g(x+m)-g(x)}{m} \geq g(c)^2$ From here I got completely stuck, and I don't know how to continue. Intuitively, I feel like if $g$ is not the zero function, then somehow, $(g(x))^2$ will increase too fast at some point for $g'(x)$ to keep being bigger than it.","['continuity', 'functions', 'derivatives']"
4791353,"$n$ points in the plane can be connected with $n-1$ clockwise, non-intersecting line segments from any starting point","This conjecture is based on a mobile game that I've published. The object of the game is: Given $n ≥ 3$ points in the Cartesian plane in general position (no $3$ of those points are a straight line): Connect all the points with $n - 1$ line segments, drawn continuously (without lifting pencil from paper). Drawing lines in this way forms a permutation of the set of $n$ points, with each line segment being defined by the $n-1$ subsequences of $2$ consecutive points in that permutation. (No line segment is drawn between the first and last points in the permutation.) Every subsequence of $3$ consecutively connected points must be clockwise oriented in the order that they were connected. No two of the line segments may intersect. Equations that precisely define clockwise orientation and line intersection are provided under the ""Insights"" section of this post. I've already proven that every instance of this game is solvable if the player gets to choose the point at which they start: Start at a point on the convex hull of the set of points, then continue making connections in a clockwise spiral to minimize each successive angle between line segments until every point has been reached. Example solution: However, I conjecture that the game is also solvable by using any arbitrary point as the starting point, even if it's not on the convex hull of all the points. I have not yet manually created a counterexample to this conjecture, and I've also searched millions of possible levels using a computer program without finding a counterexample. However, I have yet to formally prove the conjecture. Clarification: Here's another wording of the problem: Given a set of $n ≥ 3$ points $\{P_1, P_2, ..., P_n\}$ in general position, and given an arbitrary point $Q$ from that set, does there always exist a permutation of that set satisfying the following conditions: The first element of the permutation is $Q$ None of the following line segments intersect with each other: $\overline{P_i P_{i+1}}$ for each $i \in [1, n-1]$ Referring to $P_i$ , $P_{i+1}$ , and $P_{i+2}$ as $(A_x, A_y)$ , $(B_x, B_y)$ , and $(C_x, C_y)$ for each $i \in [1, n-2]$ , the following condition is always satisfied: $\begin{vmatrix}(B_x - A_x)&(B_y - A_y)\\(C_x - A_x)&(C_y - A_y)\end{vmatrix}<0$ Alternative strategy: If an alternative conjecture can be proven where $Q$ is the last element of the permutation instead of the first element, then the original conjecture also holds. Reasoning: If the original conjecture holds for a set of points $S$ , then the alternative conjecture holds for $S'$ , where $S'$ has the points from $S$ reflected across a given arbitrary line (such as the y-axis). Insights: Whether two line segments intersect depends upon whether trios of their points are clockwise-oriented. This correspondence may be usable to generalize the problem, though I am not yet sure how. Let $c(A, B, C)$ be true iff A, B, and C are clockwise-oriented and false otherwise. Let $i(A, B, C, D)$ be true iff $\overline{AB}$ and $\overline{CD}$ intersect with each other and false otherwise. Then: $c(A, B, C) = (\begin{vmatrix}(B_x - A_x)&(B_y - A_y)\\(C_x - A_x)&(C_y - A_y)\end{vmatrix}<0)$ $i(A, B, C, D) = (c(A, B, C) \oplus c(A, B, D)) \land (c(C, D, A) \oplus c(C, D, B))$ The following identities hold on the function $c$ : $c(A, B, C) = c(B, C, A) = c(C, A, B)$ $c(A, B, C) \oplus c(C, B, A)$ always evaluates to true The function $f(N)$ that gives the number of sets of N points that are distinct (defined below) is given by A000930 , where $f(N)$ is the Nth element of that sequence. (Obviously, $f(1)$ and $f(2)$ are meaningless in the context of this problem.) For example, there is only one distinct set of $3$ points — they form a triangle. For $4$ points, there are $2$ distinct sets — one with a convex hull of 4 points and one with a single point within a convex hull of 3 other points. Distinctness for the purpose of this result is defined by: the number of times the convex hull of the set of points can be removed before no points remain (i.e. the number of nested convex hulls in the initial set of points), and the numbers of points on each of the nested convex hulls. I believe, but have not yet formally proven, that the solution for any set of points $S$ is generalizable to any other set of points $T$ if $S$ and $T$ are not distinct by the above definition. Insights from answers: Carlyle proved that for a set of points $S$ , a solution can always be found if the initial point is in $T$ or $U$ , where $T$ is the convex hull of $S$ , and $U$ is the convex hull of $(S - T)$ . It follows that a solution can be found for any initial point in a set of $6$ or fewer points. Insights from comments: Thank you so much to everyone who's left comments! Here is a summary of the ideas and insights derived from my and others' comments. There have been misconceptions about the meaning of ""clockwise"". This term is precisely defined in the ""clarification"" and ""insights"" sections of my question. Observe the manner in which the following set of points is connected according to the rules in the problem statement: https://i.sstatic.net/gOMgp.png . The above set of points does not generalize to all sets of points with 3 convex hulls within each other (e.g. 3 convex hulls inscribed within concentric circles). Why does this not generalize? Consider the case of 4 convex hulls inscribed within concentric circles. Here is the general solution (with an arbitrarily large number of points on each). Here is a specific case, with a different solution than the general case. Mobile game screenshots: These screenshots illustrate the conjecture in the context of the mobile game (""Clockwise!"" by me, Roy Sianez, available on the iOS App Store within the US). I'm attaching them as an image because the App Store link may not show the product page outside of the US.","['recreational-mathematics', 'combinatorial-geometry', 'geometry']"
4791354,"Weak Convergence of Measures on $(\mathbb{R}^{n},\mathcal{B}(\mathbb{R}^{n}))$ using distribution functions.","I am searching for a general proof for the following: On $(\mathbb{R}^{n},\mathcal{B}(\mathbb{R}^{n})$ , $\mu_{n}\longrightarrow\mu$ weakly if and only if $F_{n}(x_{1},\dots, x_{n})\longrightarrow F(x_{1},\dots, x_{n})$ at all continuity point of $F$ , where $\mu_{n},\mu$ are measures and $F_{n},F$ are their induced distribution functions, respectively. If $n=1$ , then it is standard result for weak convergence. However, it is hard for me to prove this for higher $n$ . The proof of the case of $n=1$ builds upon the fact that every open subset of $\mathbb{R}$ is a countable union of open intervals, and there are only countably many discontinuity points of the distribution function. Then, we can rearrange those intervals so that the endpoints are not discontinuity points, and since everything is countable, we can still have a countable union of open intervals and the proof continues. This technique does not work on higher $n$ , even though any open set in $\mathbb{R}^{n}$ is a countable union of open rectangles, because the distribution function now can have uncountably many discontinuity points. I believe that this is a standard result with a tedious but smart construction, but I cannot find a reference for this. Is there any online notes or books I can go to for such a proof? The books I have checked: Probability,  A. N. Shiryaev Theory of Probability and Random Processes, Y.G. Sinai and L.B. Koralov Probability Theory, A. Klenke Foundations of Modern Probability, O. Kallenberg Online notes by Amir Dembo (I do not read Durret).","['measure-theory', 'weak-convergence', 'probability-distributions', 'reference-request', 'probability-theory']"
4791368,Enclosing a lamp in space via Minkowski's theorem,"I'm currently working on a problem on the chapter of Geometry and Numbers from Andreescu and Dospinescu's Problems from the Book (highly recommended to read). The problem statement is the following: Consider a lamp (a point) in space. Prove that no matter how we place a finite number of closed spheres of equal radius, the light of this lamp will be able to go to infinity (that is, there exists a direction in which the light will not hit any of these spheres). The spheres must not touch. (Iran 2003) The main result we are supposed to use is Minkowski's theorem: Suppose $A\subseteq\mathbb R^n$ is a bounded, centrally symmetric, convex (and measurable) set with volume strictly bigger than $2^n$ . Then $A$ contains a lattice point different from the origin. First attempt I originally interpreted the problem being in two dimensions and placed the lamp at the origin. Trying to blot out the light, I placed three spheres around it and then placed another 3 in the holes as follows: This attempt led me to believe that I was not understanding the idea correctly, maybe light bounced off something or I wasn't quite seeing something. However I then understood (or maybe I'm still sidetracked and not seeing things correctly) that the problem was meant to be in 3D. After the realization Taking into account the third dimension, I want to proceed similarly to the first example in the chapter. We suppose by contradiction that it's possible to blot out the light with a finite sphere packing. This tells us that somewhere out there is a packing of non-touching spheres which blots out the light. If I was to use Minkowski's theorem I would like to find a set with volume greater than $8$ and in that set I would get a lattice point. Something tells me that the lattice point in question gives us the direction of the ray of light in which light escapes. However, the packing doesn't help to build our convex set. At first, I thought that I should take the convex hull of the sphere packing but that might not work. That hull might not be centrally symmetric. And even if it did, finding a lattice point inside the convex hull doesn't guarantee that light won't touch any of the spheres. My question Is the way I'm proceeding a correct way to approach the problem? Could you point me in the right direction in order to see the light ? I believe that the issue with the problem is to find the correct solid and from there use the idea that the lattice point is the correct direction. Also, if you know, can you recommend me similar problems to this one, where the statement is simple to read and we may use Minkowski's theorem to do them? Thanks in advance for your help.","['contest-math', '3d', 'convex-geometry', 'integer-lattices', 'geometry']"
4791373,Continuum many reals with pairwise irrational difference,"In ""Problems and Theorems in Classical Set Theory"" by Péter Komjáth and Vilmos Totik, in the Solutions to Chapter 30, they claim: ""It is easy to give continuum many reals with pairwise irrational difference"". However, I'm having a hard time constructing such a set. I've tried using an identification of $[0, 1]$ with $2^\omega$ and then constructing a subset of $2^\omega$ such that taking the bitwise difference results in a sequence that does not eventually repeat, meaning that the real number it corresponds to is not rational. However, I can't translate the bitwise difference operation on $2^\omega$ to the subtraction operation on $[0, 1]$ . Edit: I should note that in this chapter, the axiom of choice is explicitly not assumed","['real-numbers', 'measure-theory', 'set-theory']"
4791428,Is it possible to assemble copies of this shape into a cube?,"A couple of friends of mine were discussing a problem concerning this shape: Is it possible to assemble enough of these to form a cube? I have discovered a lot of impossible positions but was not successful in creating something useful. We have managed to build a 12x12x4 tower with leftover blocks at the top, however. Maybe someone here has any ideas on how to tackle this problem? My next steps would be to try and extend our 12x12x4 tower, and if that doesn't work, to write a program to search for solutions.","['puzzle', 'combinatorics', 'tiling']"
4791468,How to prove that the cross product doesn't satisfy any kind of generalized associativity?,"It's well known that the cross product in $\mathbb{R}^3$ doesn't obey the associative law of $$ A \times (B \times C) = (A \times B) \times C $$ We can define a ""Generalized Associative Law"" as an expression involving equation two sets of $N$ same ordered symbols turned into a binary expression tree via non-redundant parenthesis. An example in 4 symbols of such a generalized associativity could be: $$ A \times (B \times (C \times D)) = (A \times B) \times (C \times D) $$ After performing a computerized search I was surprised to find that there is NO generalized associative law for the cross product at least up to 15 symbols. For the interested reader the number of such parenthesized expressions on $N$ symbols grows according to the Catalan Numbers While I can try optimizing my code and increasing its performance to check against more symbols I naturally have to ask: If the cross product doesn't have ANY generalized associative law. How would we even go about proving it?","['cross-product', 'abstract-algebra', 'combinatorics', 'associativity']"
4791474,Prove that there exists a unique partition of $\mathbb{N}$ into A and B so that neither of $A\oplus A$ and $B\oplus B$ has a prime.,"For any subset $S\subseteq \mathbb{N}, $ let $S\oplus S = \{a + b : a,b \in S , a\neq b\}$ . Prove that there exists a unique partition of $\mathbb{N}$ into disjoint subsets A and B so that neither of $A\oplus A$ and $B\oplus B$ has a prime. Clearly we can just let $A$ and $B$ be the set of even and odd positive integers respectively, but I'm not sure how to prove that these values are unique. But how would we prove that this partition is unique? I think it might be useful to first show that for such a partition, all the elements of $A\oplus A$ and $B\oplus B$ must be even integers greater than 2. If there were an odd integer in $A\oplus A$ , A would have to contain both an odd integer and an even integer, but I don't see how to get a contradiction from here.","['contest-math', 'divisibility', 'elementary-number-theory', 'combinatorics', 'discrete-mathematics']"
4791599,Continued fraction of $\frac{\prod_{k=1}^{p-1}(2^k-1)}{2^p-1}$ for prime p.,"Investigating the q-factiorals. And found experimentally (at least frist 1000 primes) that the continued fraction of $$\frac{\prod_{k=1}^{p-1}(2^k-1)}{2^p-1}$$ for prime p has level 3 and the last element of continued fraction is equal to $p$ . So: $$\frac{\prod_{k=1}^{p-1}(2^k-1)}{2^p-1}=a+\frac{1}{b+\frac{1}{c}}$$ and $c=p$ . If continue: $$\frac{\prod_{k=1}^{p-1}(2^k-1)}{2^p-1}=\frac{cba+a+c}{cb+1}$$ so if $c=p$ $$\frac{\prod_{k=1}^{p-1}(2^k-1)}{2^p-1}=\frac{pba+a+p}{pb+1}$$ Because of Fermat's little theorem, the $b$ is integer. $$2^p-2=pb$$ Now $pb+1=2^p-1$ and $pba+a+p=a(pb+1)+p$ or $pba+a+p=a(2^p-1)+p$ or $$\prod_{k=1}^{p-1}(2^k-1)-p = a(2^p-1)$$ . So if we prove that the $$\prod_{k=1}^{p-1}(2^k-1) \equiv p \pmod {2^p-1}\tag{*}$$ then we prove the initial idea. There is also very interesting thing about this $$\gcd\big(\prod_{k=1}^{p-1}(2^k-1), {2^p-1}\big)=1$$ This and Fermat's little theorem I assume, should be enough to prove the $(*)$ formula, but hang on the proving $(*)$ .","['number-theory', 'continued-fractions']"
4791612,"Given $A^2+B^2=\left(\begin{smallmatrix}1402&2022\\2022&1402\end{smallmatrix}\right)$ for which $A,B\in M_2(\mathbb{R})$, show that $AB\neq BA$","$M_2(\mathbb{R})$ is the set of all $2\times2$ matrices that their entries are in $\mathbb{R}$ . Now consider $A,B\in M_2(\mathbb{R})$ . We have $$A^2+B^2=
\begin{bmatrix}1402&&2022\\
2022 && 1402\\
\end{bmatrix}
$$ Show that $AB\neq BA$ . I tried writing $(A+B)^2=A^2+AB+BA+B^2$ and we know if we assume $AB=BA$ , then $(A+B)^2=A^2+2AB+B^2$ . Then I tried to find the form of a squared matrix and compare these to reach a contradiction but I couldn't. I even don't know if it helps or not. Any help is appreciated!","['matrices', 'matrix-equations', 'linear-algebra']"
4791618,Fourier transform and the heat semigroup,"Let $f \in H^{1}(\mathbb{R}^{d})$ and $t > 0$ . I want to prove that: $$\mathcal{F}({e^{t\Delta}f})(k) = e^{-t|2\pi k|^{2}}\mathcal{F}(f)(k),$$ where $\Delta$ denotes the Laplacian operator on $\mathbb{R}^{d}$ and $\mathcal{F}$ denotes the Fourier transform: $$(\mathcal{F}f)(k) = \int_{\mathbb{R}^{d}}e^{-2\pi i \langle k,x\rangle}f(x)dx.$$ I know the following result from functional analysis: let $\mathscr{H}_{1}$ and $\mathscr{H}_{2}$ be Hilbert spaces and $T: \mathscr{H}_{1} \to \mathscr{H}_{1}$ and $S: \mathscr{H}_{2} \to \mathscr{H}_{2}$ be two self-adjoint (possibly unbounded) which are unitarily equivalent, that is, $T = U^{-1}SU$ for some unitary operator $U: \mathscr{H}_{1} \to \mathscr{H}_{2}$ . Then, for every continuous and bounded function $f: \mathbb{R} \to \mathbb{C}$ , one has $f(T) = U^{-1}f(S)U$ . Can I use this result to prove the formula above? My idea is: let $f(x) = e^{-x}$ and consider $T = -t\Delta$ and $U = \mathcal{F}$ , which is an unitary map. The point that is not clear to me is: $f(x) = e^{-x}$ is not bounded on $\mathbb{R}$ , but it is bounded on $[0,\infty)$ , which is the spectrum of $-t \Delta$ . Can I use the formula in this case? In other words, is it enough that $f$ is bounded and continuous on the spectrum of $T$ ?","['fourier-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations']"
4791697,Find center of externally tangent circle,"I've been struggling to find a way to resolve the following problem: Let $C_1$ a circle of center $V$ and of radius $r_1$ .
Let $A$ and $B$ two points outside of $C_1$ , and $L$ a line passing by them.
Let $W$ a point on $L$ .
Let $C_2$ a circle of center $W$ passing through $A$ (radius $r_2$ is the distance between $A$ and $W$ ). I am looking for the coordinate of the point $W$ so that $C_2$ intersects $C_1$ in a single point (that is that $C_1$ and $C_2$ are externally tangent). What do you think? I tried to visualize a logic behind it using GeoGebra but couldn't figure it out. Right now what I'm doing in my code (this is for generative design) is increasing the distance between $A$ and $W$ by small steps until the distance $VW$ is equal to $r_1 + r_2$ , but I'm looking for the elegant, geometrical solution!","['tangent-line', 'circles', 'geometry']"
4791738,Determining whether an operator has maximal rank,"Consider the following We showed that Lie point symmetries are useful for solving ODE's and PDE's. But how do we find them the first place? 5.4.1 Trivial Case
Lets try to find Lie point symmetries of an 0th order ODE $$
\Delta[x, u]=0 .
$$ By definition $g^{\varepsilon}:(x, u) \mapsto(\tilde{x}, \tilde{u})$ is a Lie point symmetry if $$
\Delta[x, u]=0 \quad \Longrightarrow \Delta[\tilde{x}, \tilde{u}]=\Delta\left[g^{\varepsilon}(x, u)\right]=0 .
$$ Question: Can we reduce this to a statement about the generator of $g^{\varepsilon}$ ?
The answer is yes, but will need to assume that $\Delta$ is of maximal rank. Defn (Maximal Rank) The operator $\Delta$ is of maximal rank if the matrix of derivatives $$
\frac{\partial \Delta_j}{\partial y_i}
$$ is of maximal rank, where the $y_i$ runs over $x, u$ , and in general all coordinates. The definition makes no sense to me. The text gives $$
    \Delta[x, u] = x^{2}.
$$ as an operator which does not have maximal rank.  The issue for me is that this is not a vector, so how can I take its $i$ -th component? Question: Could someone clarify here what the partial derivatives mean with this indentation?","['lie-algebras', 'analysis', 'partial-differential-equations']"
4791759,The functional differential equation $f'(x) = f(f(x))$ [duplicate],"This question already has answers here : Does a non-trivial solution exist for $f'(x)=f(f(x))$? (6 answers) Closed 8 months ago . In Maths 505 I found an interesting functional differential equation (FDE) problem ( https://youtu.be/C6fZVwqhbnE?si=Trxk-KYNmQUeEPCv ) which asks for the solution of the equation $$f'(x) = f(f(x))\tag{1}$$ No further conditions were given. I had never before studied this type of equations. Here is what I did so far. The first attempt was suggested by the author of the problem: make the power ansatz with two parameters $$f(x) = a x^{b }\tag{2}$$ Then we have $f'(x) = a b x^{b-1}$ and $f(f(x)) = a( a x^{b })^{b} = a^{1+b} x^{b^2}$ from which we deduce $b^2=b-1$ and $b=a^{b}$ . These equations have the solutions $$b_{\pm} = \frac{1\pm i \sqrt{3}}{2}=e^{\pm \frac{i \pi}{3}}\tag{3a}$$ and $$a=b^{1/b}\tag{3b}$$ So that we have found two solutions to $(1)$ $$f_\pm(x) = b_\pm^{1/b_\pm} x^{b_\pm}\tag{3c}$$ Expanding this using Euler's formula we can calculate the real and imaginary parts of $f(x)$ .
This shows that we have obtained two complex functions of the argument $x$ which we assumed he to to be real. We can study the properties of the solution, plot graphs etc. but one problem appears - and remains - urgently: a dfferential question of first order must have one free parameter to adapt the solution to an initial condition. Here I am stuck since I can't accomodate an arbitrary constant in the solution found by a power ansatz. Second approach: Let us assume the initial condition $$f(1)=1\tag{4}$$ This simplifies the arithmetic considerably. Indeed, we find $f'(1) = f(f(1)) = f(1) = 1$ , and next $f''(x) = \frac{d}{dx} f(f(x)) = f'(f(x))\cdot  f'(x) = f(f(f(x))) \cdot f(f(x))$ giving $f''(1) = 1$ , $f'''(1) = 2$ etc. Continuing this procedure (conveniently done in Mathematica) we find for $f(1)$ and the first nine derivatives of $f$ at $x=1$ the strongly increasing series $$f^{(k)}(1)|_{k=0}^{k=9} = \{1,1,2,7,37,269,2535,29738,421790,7076459\}\tag{5}$$ This series is contained in OEIS as https://oeis.org/A001028 and reads
A001028     E.g.f. satisfies A'(x) = 1 + A(A(x)), A(0)=0. This remark confirms our suspection: ""The e.g.f. is diverging"". Hence we have not obtained a valid series expansion around $x=1$ . Here I'm stuck again. Hopefully someone can solve the problem with, a proper initial condition, say f(1) = 1/2$. EDIT I am grateful to a user (who deleted the contribution shortly after publication) who notified me that the problem was studied earlier by Alex Jones in: https://www.quora.com/How-can-I-solve-f-x-f-f-x where my first attempt was described. The general problem was not attacked there.","['functional-equations', 'ordinary-differential-equations']"
4791818,Almost surely uniform convergence using ergodic Theorem,"I have a question rearding the almost surely uniform convergence using ergodic Theorem. Let $\left\lbrace X_t\right\rbrace_t$ be a stationnary ergodic process. By the ergodic theorem, we have as $m\to\infty$ $$\dfrac{1}{m} \sum_{t=1}^m X_t\longrightarrow \mathbb{E} X_1\;\;\; a.s. $$ Let $\lfloor x\rfloor$ denote the flooring perator, i.e., the largest integer smaller than or equal to $x$ . My question is, by the above convergence how can we prove the following  convergence, as $m\to\infty$ $$\sup\limits_{\tau\in\Pi}| \dfrac{1}{\lfloor m\tau\rfloor}\sum_{t=1}^{\lfloor m\tau\rfloor}X_t-\mathbb{E} X_1|\longrightarrow 0\;\;\;a.s $$ with $\Pi\subset [0,1]$ ?
Thank you very much in advance for your help.","['statistics', 'probability-limit-theorems', 'ergodic-theory', 'uniform-convergence', 'probability-theory']"
4791852,Find all entire functions that satisfy the following equality,"For $n\geq 2$ I need to find all entire functions $f:\mathbb{C} \rightarrow \mathbb{C}$ Such that $f(z^n)=f(z)^n$ for all complex numbers $z$ . I've tried expanding it's series at 0 and have found some relations with the coefficients, but none seem to finish the problem. I couldn't conclude using Liouville's theorem neither.. My gut feeling tells me that the only the functions of the form $z^k$ will work, but I am a bit stuck.","['complex-analysis', 'functional-equations', 'entire-functions']"
4791856,Paraboloid and Sphere,"Find the volume of the solid region inside the sphere $x^2+y^2+z^2=6$ and above the paraboloid $z=x^2+y^2.$ I set both equations equal to each other and obtained $z=2$ and $z=-3$ . Since clearly $z>0,$ that means I only consider $z=2.$ Subbing this into both equations gives $x^2+y^2=2$ . I sketch this circle and obtain $0 \leq \theta \leq 2 \pi$ and $0 \leq r \leq \sqrt2.$ Also, clearly $r^2 \leq z \leq \sqrt{6-r^2}.$ Now I want to convert into spherical coordinates. I don't know how to find the limits for $\phi$ . I think the lower limit is $0$ but I am not sure about the upper limit. I think the upper limit for $p$ is $6$ . Not sure how to find the rest.","['integration', 'geometry', 'multivariable-calculus', 'calculus', 'spherical-coordinates']"
4791890,Volume of a polyhedron inside another polyhedron created by joining centers of faces of a cube.,"Start with a cube (let us denote it by A). Put a point at the center of its faces and create a polyhedron (denoted by B). Put a point at the center of the faces of this polyhedron and create a second polyhedron (denoted by C) with these new points as vertices.
What is the volume of A if C has a volume of 1? My Understanding: I can see that a regular octahedron (B) of edge 1/√2 times the edge length of cube A is formed. And then a hexagonal prism C seems to be forming inside B. The volume of B is 1/6 of A but I can't find the relation between B's and C's volume. Edit: I was incorrect in understanding that C will be a hexagonal prism. It turns out a cube. Any help would be appreciated. This problem was asked in a Quant Hiring Test. Follow up: Can a general relation be derived for n such ""deep"" polyhedrons?","['vectors', 'geometry']"
4791907,Confusion over $\iff$ and $\implies$ in basic set theory definitions of union and intersection.,"I am working through Chapter 3 of Terence Tao's Analysis I on basic set theory. Doing the exercises has highlighted some gaps in my understanding. This question is about one such gap. Compare the following two examples of what I think are logically valid steps. Example 1 $$\begin{align}x \in (A \cup B) &\iff (x \in A) \lor (x \in B) \\ \\ &\implies (x \in B) \lor (x \in B) \\ \\ & \implies x \in B \end{align}$$ The first line uses a $\iff$ biconditional because that is the definition (Axiom 3.5 in Tao's 4th edition). The next line uses $A \subseteq B$ which is given by the exercise. It means $(x \in A) \implies (x \in B)$ . Thus, we can replace $x \in A$ with $x \in B$ . But we can't replace $x \in B$ with $x \in A$ , and this is the reason for the second line using $\implies$ and not $\iff$ . Question - is this correct? Example 2 $$\begin{align}x \in (A \cap B) &\iff (x \in A) \land (x \in B) \\ \\ &\implies (x \in A)\end{align}$$ I am happy with $P \land Q \implies P$ and also that $P \not \implies P \land Q$ . This is why the second line uses a $\implies$ and not have a $\iff$ . Question - is this correct?","['elementary-set-theory', 'logic']"
4791957,Prove that $a_n = \sum_{k=0}^{\lfloor n/2\rfloor} 1/(k+1) {2k\choose k} {n\choose 2k}$,"Consider the sequence defined by $a_0 = a_1 = 1$ and $a_{n+2 } = a_{n+1} + \sum_{k=0}^n a_k a_{n-k}$ for $n\ge 0$ . Prove that $a_n = \sum_{k=0}^{\lfloor n/2\rfloor} 1/(k+1) {2k\choose k} {n\choose 2k}$ . The $\sum_{k=0}^n a_k a_{n-k}$ term in the recurrence relation for $a_{n+2}$ reminds me of the Catalan numbers, as does the ${2k\choose k}/(k+1)$ term in the proposed formula for $a_n$ . So I think it might be useful to use properties of Catalan numbers to solve this problem (e.g. the generating function for the Catalan numbers $f(x)$ satisfies $xf(x)^2 - f(x)+1=0$ ). It might be useful to find an explicit formula for the generating function $f(x) = \sum_{n\ge 0} a_n x^n$ , and this may be related to that of the Catalan numbers in some way. But the given recurrence seems very hard to work with, as well as the stated formula. Is there some way to simplify the formula? To simplify the recurrence, one trick that's used is to substitute $n+1$ for n in the recurrence relation for a given sequence and then subtract common terms. But there don't appear to be a lot of common terms in $a_{n+1} + \sum_{k=0}^n a_k a_{n-k}$ and $a_{n+2} + \sum_{k=0}^{n+1} a_k a_{n+1-k}$ . I'm not sure if induction is useful.","['contest-math', 'recurrence-relations', 'combinatorics', 'discrete-mathematics', 'induction']"
4791981,Is is true that the intersection of two dense subspaces of a linear normed space is also dense?,"I tried to find a counterexample for it but I failed. If the statement is true, I think it is sufficient to prove that the intersection of two dense subspaces is also dense in the unit ball, but I don't know how to approach it.",['functional-analysis']
4792063,"prove that for any $|x|\leq 1$, $|f(x)|\leq 5/4$ [duplicate]","This question already has answers here : A quadratic polynomial proof (2 answers) Closed 8 months ago . Let $f(x)\in \mathbb{R}[x]$ be a quadratic polynomial with $|f(-1)|\leq 1, |f(0)| \leq 1, |f(1)|\leq 1.$ Prove that for any $|x|\leq 1$ , $|f(x)|\leq 5/4$ . I'm not sure how to solve this problem, and below is my attempt. One example quadratic is obviously $f(x)=x^2$ . Write $f(x) = ax^2 + bx+c.$ Note that we may assume WLOG that $a>0$ because for $a<0,$ we can replace $f(x)$ with $-f(x)$ and obtain the same conclusion ( $|f(x)|=|-f(x)|$ ). Then $|a-b+c|, |c|, |a+b+c|\leq 1.$ Suppose $|x|\leq 1.$ We need to show that $|ax^2 + bx+c|\leq 5/4.$ Note that since $f$ is convex, on any interval $[s,t]$ , the maximum value of $f(x)$ is attained at either $a$ or $b$ . Since f is continuous, the minimum of $f(x)$ is attained in $[s,t]$ for any $s<t\in \mathbb{R}.$ We also know that $|f(x)|$ attains its maximum either when f attains its minimum or maximum. Indeed suppose $f(x)$ is neither a min nor a max. WLOG, suppose $f(x)$ is negative. Then $f(x)$ can be decreased, which increases $|f(x)|$ . Perhaps Lagrange interpolation might be useful?","['contest-math', 'inequality', 'calculus', 'polynomials', 'algebra-precalculus']"
4792068,Is there a infinite dimensional topological vector space with the weight of topology larger than the algebraical dimension?,"A topological vector space is a vector space endowed with a topology such that the sum of vectors and the multiplication by scalars are continuous. The weight of the topology $\tau$ is the smallest cardinal $\alpha$ such that there is a basis $\mathcal B$ of cardinality $\alpha$ for $\tau$ . By algebraical dimension, I mean the classical cardinality of its Hamel basis.
So, my question is: is there some infinite dimensional topological vector space $V$ such that the weight of $V$ is larger than the dimension of $V$ ? If useful, I'm okay with using some axioms such as the Continuum Hypothesis. To give some context, my interest in this question arose from a theorem that holds if the weight of the topology is less than or equal to the dimension. So, I want to know if there are counterexamples if the weight is greater than the dimension, which includes knowing whether there are spaces with this property.","['topological-vector-spaces', 'linear-algebra', 'functional-analysis', 'general-topology', 'set-theory']"
4792134,Proving a Probability Limit is Non Zero,"I am reading these lecture notes : on page 1, it mentions (indirectly) that : Define the Score Function as the first derivative of the likelihood If the Expected Value of the Score Function is not equal to 0, then the resulting MLE estimator will not be Consistent . I am trying to understand why this is true. I understand that this is the definition of Consistency in Statistics: An estimator $\hat{\theta}$ is consistent if when the sample size $n$ grows to infinity, the empirical estimator $\hat{\theta}_n$ subtracted from the theoretical estimator $\theta$ has a 0 probability of being greater than some small number $\epsilon$ : $$\lim_{{n \to \infty}} P(|\hat{\theta}_n - \theta| > \epsilon) = 0$$ I also understand how to obtain the Score Function for a given Likelihood Function: A random variable $X$ . A probability density function (pdf) $f(x; \theta)$ . The likelihood of $f(x; \theta)$ given data $x_1, x_2, ..., x_n$ is $$L(\theta; x) = \prod_{i=1}^{n} f(x_i; \theta)$$ The log-likelihood is $$\log L(\theta; x) = \sum_{i=1}^{n} \log f(x_i; \theta)$$ The derivative of the log-likelihood is $$\frac{d}{d\theta} \log L(\theta; x) = \frac{d}{d\theta} \sum_{i=1}^{n} \log f(x_i; \theta)$$ The MLE estimator of $\theta$ is the only solution $\hat{\theta}$ of the equation $$\frac{d}{d\theta} \log L(\theta; x) = 0,$$ provided this equation has a unique solution. The expected value of the derivative of the log-likelihood is zero, i.e., $$E\left[\frac{d}{d\theta} \log L(\theta; x)\right] = 0$$ My Question: But mathematically, how can we show that if $E\left[\frac{d}{d\theta} \log L(\theta; x)\right] \neq 0$ , then $$\lim_{{n \to \infty}} P(|\hat{\theta}_n - \theta| > \epsilon) \neq 0$$ ??? Can someone please show me how this can be proven? Thanks!","['statistics', 'proof-writing', 'calculus', 'limits', 'probability']"
4792169,Is there a general relation between min (f(x) / g(x)) and min f(x) / max g(x)?,"Intuitively, it seems that $$
\min \frac{f(x)}{g(x)} \geq \frac{\min f(x)}{\max g(x)}
$$ if both $f$ and $g$ are positive-valued functions, but is there a more general relationship? Can one also deduce a similar relationship with the $\min$ and $\max$ reversed?",['functions']
4792188,What exactly does $X - (Y ∪ Z)$ mean?,Does the above mean: $x$ is in $X$ but [ $x$ is not in $Y$ or $x$ is not in $Z$ ] OR $x$ is in $X$ but [ $x$ is not in $Y$ and $x$ is not in $Z$ ] ?,"['elementary-set-theory', 'propositional-calculus']"
4792229,"On the cubic counterpart of Ramanujan's $\sqrt{\frac{\pi\,e}{2}} =1+\frac{1}{1\cdot3}+\frac{1}{1\cdot3\cdot5}+\frac{1}{1\cdot3\cdot5\cdot7}+\dots$?","We have Ramanujan's well-known, $$\sqrt{\frac{\pi\,e}{2}}
=1+\frac{1}{1\cdot3}+\frac{1}{1\cdot3\cdot5}+\frac{1}{1\cdot3\cdot5\cdot7}+\dots\color{blue}+\,\cfrac1{1+\cfrac{1}{1+\cfrac{2}{1+\cfrac{3}{1+\ddots}}}}$$ However, as discussed in this post , the series can also be expressed as a nice continued fraction. And since $\Gamma\big(\tfrac12\big) = \sqrt{\pi}$ , we can then express the identity as, $$\Gamma\big(\tfrac12\big)\sqrt{\frac{e}{2}} = 1+\cfrac{2/2}{2+\cfrac{3/2}{3+\cfrac{4/2}{4+\cfrac{5/2}{5+\ddots}}}} \; \color{blue}+ \; \cfrac1{1+\cfrac{1}{1+\cfrac{2}{1+\cfrac{3}{1+\ddots}}}}$$ It turns out it may have a cubic counterpart, $$\Gamma\big(\tfrac13\big)\sqrt[3]{\frac{e}{9}}
= 1+\cfrac{2/3}{2+\cfrac{3/3}{3+\cfrac{4/3}{4+\cfrac{5/3}{5+\ddots}}}} \; \color{blue}+ \; \cfrac1{1+\cfrac{2}{1+\cfrac{3}{1+\cfrac{\color{red}5}{1+\ddots}}}}$$ where the 4th continued fraction is missing numerators $P(n)=3n+1 = 4,7,10,13,\dots$ The four cfracs apparently have closed-forms as, \begin{align}
\Gamma\big(\tfrac12\big)\sqrt{\frac{e}{2}} 
&= \sqrt{\frac{e}{2}}\times\Big(\Gamma\big(\tfrac12\big)-\Gamma\big(\tfrac12,\tfrac12\big)\Big)\; \color{blue}+ \, \sqrt{\frac{e}{2}}\times\Big(\Gamma\big(\tfrac12,\tfrac12\big)\Big)\\[6pt]
\Gamma\big(\tfrac13\big)\sqrt[3]{\frac{e}{9}} 
&= \sqrt[3]{\frac{e}{9}}\times\Big(\Gamma\big(\tfrac13\big)-\Gamma\big(\tfrac13,\tfrac13\big)\Big)\; \color{blue}+ \; \sqrt[3]{\frac{e}{9}}\times\Big(\Gamma\big(\tfrac13,\tfrac13\big)\Big)
\end{align} and their decimal expansions are A060196 , A108088 , A108744 , A108745 , respectively. Questions : Given Pochhammer symbol $(x)_n$ , how do we prove that, \begin{align}
\sum_{n=1}^\infty \frac{1}{2^n (\frac12)_n} 
&= \sqrt{\frac{e}{2}} \times\,\Gamma\big(\tfrac12\big)\,\operatorname{erf}\Big(\sqrt{\tfrac 12}\Big)\\ 
&= \sqrt{\frac{e}{2}}\times\Big(\Gamma\big(\tfrac12\big)-\Gamma\big(\tfrac12,\tfrac12\big)\Big) = 1+\cfrac{2/2}{2+\cfrac{3/2}{3+\cfrac{4/2}{4+\cfrac{5/2}{5+\ddots}}}}
\end{align} Similarly, how do we show that, $$\sum_{n=1}^\infty \frac{1}{3^n (\frac13)_n} = \sqrt[3]{\frac{e}{9}}\times\Big(\Gamma\big(\tfrac13\big)-\Gamma\big(\tfrac13,\tfrac13\big)\Big) = 1+\cfrac{2/3}{2+\cfrac{3/3}{3+\cfrac{4/3}{4+\cfrac{5/3}{5+\ddots}}}}$$ P.S. Part of Question 1 has already been answered in this post , but I included it for comparison and to see if an alternative or modified proof can be found that covers both Question 1 and 2.","['gamma-function', 'closed-form', 'sequences-and-series', 'continued-fractions', 'error-function']"
4792251,Number of loops of a ball bouncing in a room with obstacles,"Introduction With a friend of mine we were studying the following problem: given a $m\times n$ grid draw this pattern (I don't know how to describe it in words) The first image has $3$ loops and the second image has only $1$ loop. The question is: is it possible to know the number of loops ( $=l$ ) in advance by knowing the size of the grid? The answer is yes and simply the answer is: $$l=\gcd(m,n)$$ Question After finding this result we complicated the problem: Is it possible to know the number of loops if there are holes in the grid? We noticed that if you have an $n\times n$ grid and you remove the corner the number of loops is $n-1$ , but in general we did not notice a pattern for a generic situation. But we have made these observations: The location of the holes affects the result The number of loops is invariant for rotations and symmetries The presence of a hole can increase or decrease the number of loops. Here is an example of why rule $1$ applies: Here the first graph has $2$ loops and the second graph has $1$ loop (in both cases you have a $3\times 3$ grid with only $1$ hole) Here is an example of why rule $3$ applies: In the first case there is one less loop compared to the $3\times3$ scheme (the image is above), in the second image instead there is one more loop In general however we wanted to consider any grid with any number of holes in any position My idea My idea was to consider a binary matrix in which $$(a)_{ij}=\begin{cases}1&\text{There is }\textbf{not}\text{ an hole}\\0&\text{There is an hole}\end{cases}$$ This consideration is useful because hypothetically I could consider any grid as a larger grid where the rest it's empty Obviously this problem can also be seen as: A bouncing ball in a room with obastacles. A laser reflected in a room of mirrors The purpose of the problem however is to know how many loops there are given a given grid. Update I don't know if it can be useful, but in knot theory there are these two formulas: Given an adjacency matrix $\mathbf{A}$ , let $E$ the set of the edges in the graph and $T$ the set of the triangles in the graph. $$\text{trace}(\mathbf{A}^2)=2\cdot|E|$$ $$\text{trace}(\mathbf{A}^3)=6\cdot|T|$$ Maybe there is a similar relationship for this case.","['number-theory', 'geometry', 'matrices', 'combinatorics', 'intuition']"
4792272,Why is the expected value of Kolmogorov Smirnov statistic converges to 0?,"Let $X_1,...,X_n$ be a random sample with distribution function $F$ and let $F_n$ be the corresponding empirical distribution function: $$F_n(x) = \frac{1}{n} \sum_{i=1}^{n}1_{(-\infty,x]}(X_i)$$ My book says that $\lim_{n->\infty}E(\sup_{x\in R}|F_n(x) - F(x)|) = 0$ can be proved? How do I do this using Glivenko Cantelli?",['statistics']
4792294,algebraic fibre space induces algeraically closed extension of function fields,"Lazarsfeld said in his book (Positivity in AG 1,example 2.1.12) that if $f:X\rightarrow Y$ is a projective surjective morphism of normal (complex)varieties, and $\mathbb{C}(Y)\subset\mathbb{C}(X)$ the corresponding extension of function fields. Then $f$ is a fibre space if and only if $\mathbb{C}(Y)$ is algebraically closed in $\mathbb{C}(X)$ . Here $f:X\rightarrow Y$ is a fibre space means that $f$ is a projective surjective morphism s.t. $f_{*}\mathcal{O}_X=\mathcal{O}_Y$ . I can not understand the ""only if"" part, the following is the method in his book: Suppose $\mathbb{C}(Y)$ is not algebraically closed in $\mathbb{C}(X)$ , then $f$ facors as a composition $X\stackrel{u}\dashrightarrow Y'\stackrel{v}\dashrightarrow Y$ of rational maps where $Y'$ is generically finite of degree >1 over Y. Replacing $Y'$ and $X$ by suitable birational modifications(projective birational morphism to $X$ and $Y'$ ), since $X$ is normal, it doesn't affect the fibre space hypothesis, one can suppose that $u$ and $v$ are in fact morphisms, and then f fails to be a fibre space since some fibres may not be connected. I don't know how to find a ""suitable modification"" of $X$ so that one can assume $u$ a morphism, can we take the graph of the rational map $u$ ?, but then how can we guarantee that the morphism $\Gamma_u\rightarrow X$ is a projective morphism? I hope I can get some help, thank you!","['algebraic-geometry', 'birational-geometry']"
4792338,Lower Semicontinuity of $L^p$ norms with varying exponents,"In a previous post (see continuity of $L^p$ norms with respect to $p$ ) it is shown that in a measure space $(\Omega,\Sigma,\mu)$ , if $1\leq p_0\leq p\leq p_1\leq+\infty$ ,  then the function $\Phi\colon L^{p_0}(\Omega)\cap L^{p_1}(\Omega)\times [p_0,p_1]\rightarrow \mathbb{R}$ defined as $$ (f,p)\mapsto \|f\|_{p}, $$ where $\|f\|_p$ denotes the norm of $f$ in $L^p(\Omega)$ , is continuous with the respect to the metric $d((f_1,q_1),(f_2,q_2)):=\|f_1-f_2\|_{p_0}+\|f_1-f_2\|_{p_1}+|q_1-q_2|$ on the product space $L^{p_0}(\Omega)\cap L^{p_1}(\Omega)\times [p_0,p_1]$ . Is it true that it is sequentially weakly lower semicontinuous as well, i.e., is it true that if $ f_j\rightharpoonup f$ weakly in $L^{q}(\Omega)$ for any $q\in[p_0,p_1]$ and $p_j\rightarrow p$ then $$\|f\|_{p}\leq \liminf_{j\to\infty} \|f_j\|_{p_j}?$$ It would be enough to prove that the function defined above is convex, but I did not manage doing so. I also tried using Mazur's Lemma to exploit the convexity of the function in the first variable and the fact that it is continuous with respect to the strong convergence in $L^{p_0}(\Omega)\cap L^{p_1}(\Omega)\times [p_0,p_1]$ , but it did not help much. I am not able to produce simple counterexamples with periodic functions either (nor more complex), so I am tempted to say that it is true. Do you have any references, hints or an actual proof (or counterexample) for this?","['measure-theory', 'weak-convergence', 'calculus-of-variations', 'semicontinuous-functions', 'lp-spaces']"
4792360,Compute the characteristic function of $X\sim\text{Poisson}(Z)$ where $Z$ is exponentially distributed.,"The question comes from this post Poisson Process with Randomly Distributed Time . Let $X\sim\text{Poisson}(Z)$ where $Z$ is exponentially distributed. Suppose that $X$ and $Z$ share the same parameter $\lambda$ . Then, following the post above, we can see that $$\mathbb{P}(X(Z)=n)=\dfrac{1}{2^{n+1}},$$ which is geometric distribution with parameter $\frac{1}{2}$ with $X(Z)$ measuring the total number of failures in Bernoullis trails before the first success coming out I would like to see if I can conclude the same thing from characteristic function. It is not hard to derive the characteristic function. Recall that $\varphi_{X}(t)=e^{\lambda (e^{it}-1)}$ and $\varphi_{Z}(t)=\frac{\lambda}{\lambda-it}$ . Then, we have $$\mathbb{E}(e^{itX})=\mathbb{E}(\mathbb{E}(e^{itX}|Z))=\mathbb{E}(e^{Z(e^{it}-1)})=\varphi_{Z}\Big(\dfrac{e^{it}-1}{i}\Big)=\dfrac{\lambda}{\lambda-e^{it}+1}.$$ However, I do not know how to reverse this. I checked the Fourier transform of this and did not find anything special.. Edit 1: I just found out that if we put $\lambda=1$ , then $$\mathbb{E}(e^{itX})=\dfrac{1}{2-e^{it}}=\sum_{n=0}^{\infty}e^{itn}\frac{1}{2^{n+1}},$$ so we can go back to geometric distribution. However, before I was not assuming any specific value of $\lambda$ . What is happening??","['characteristic-functions', 'probability-distributions', 'fourier-transform', 'stochastic-processes', 'probability-theory']"
4792372,Checking the Convergence of the sequence $a_n=\frac{1}{n}\left(1+\frac{1}{2}+\frac{1}{3}+....\frac{1}{n}\right)$ [duplicate],"This question already has answers here : Test for the convergence of the sequence $S_n =\frac1n \left(1 + \frac{1}{2} + \frac{1}{3} + \cdots+ \frac{1}{n}\right)$ (4 answers) Closed 8 months ago . Check the Convergence of the sequence $a_n=\frac{1}{n}\left(1+\frac{1}{2}+\frac{1}{3}+....\frac{1}{n}\right)$ My effort: I actually took the help of inequality(not well known I guess) $$H_n=1+\frac{1}{2}+\frac{1}{3}+...+\frac{1}{n}<2\sqrt{n}, \forall n \in \mathbb{N}$$ Now we have $a_n \geq 0$ and $a_n=\frac{H_n}{n} < \frac{2\sqrt{n}}{n}=\frac{2}{\sqrt{n}}$ Thus we have $$0 \leq a_n <\frac{2}{\sqrt{n}}$$ By Sandwich Theorem, we end up with $$\lim a_n=0$$ But is there a way without Sandwich Theorem?","['limits', 'convergence-divergence', 'sequences-and-series']"
4792379,determine the general solution of $y^{2/3}+y'^{2/3}=a^{2/3}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 8 months ago . Improve this question Determine the general solution of $y^{2/3}+y'^{2/3}=a^{2/3}$ . I have this question in my workbook and the answer for this is $y = a\cos^{3}t$ and $x= 3(cot(t)+t)$ which i think they set t as y' but I'm not sure about it and I don't know how to lead to the answer, I tried the polar coordinates but it didn't seem to work. Can I get a hint or a solution for this problem? Thank you all!","['calculus', 'ordinary-differential-equations']"
4792400,Variance of largest eigenvector of bounded random matrix,"Given a random $n\times n$ bounded positive semidefinite matrix $X$ with $\mathbb{E}[X]=A$ and some constant matrix $B$ , define an estimator $\hat{o} = tr[BX^T]$ with $\mathbb{E}[\hat{o}] = \mu$ and $\mbox{Var}[\hat{o}] = \sigma^2$ . We immediately have that $$
\mu = tr[BA] \quad \mbox{and} \quad \sigma^2 = \mathbb{E}[tr[BX^T]^2] - \mu^2
$$ Now pick up the eigenvalue ordering for the variable $X$ as $1 \geq \lambda_1 \geq \lambda_2 \geq \dots \lambda_n \geq 0$ and define the vector $v$ $(vv^T = 1)$ such that $Xv = \lambda_1 v$ i.e. the first eigenvector of $X$ . What can be said about the variance $\mbox{Var}[tr[Bvv^T]]$ of this eigenvector estimator in terms of $X, \mathbb{E}[X]$ and $\mathbb{E}[tr[BX^T]^2]$ ? I am mostly interested in upper bounds on the eigenvector estimator variance but do not know how to even begin to treat this when I am considering eigenvectors of a random matrix $X$ as a starting point, rather than $v$ as the random variable itself. Since $X$ is promised to be positive definite it seems like taking the eigendecomposition of $X$ might yield an upper bound similar to $ \frac{\sigma^2}{\lambda_1^2} $ but this seems rather weak. Any help as to where to begin would be greatly appreciated.","['matrices', 'linear-algebra', 'random-variables']"
4792455,Generalising $I_n=\int_0^1 \arcsin(\sqrt{1-x^n})\mathrm dx$.,"Consider the integrals of the form $$I_n=\int_0^1 \arcsin(\sqrt{1-x^n})\mathrm dx$$ With the help of calculators , I noticed: $$I_{3}=\frac{3\Gamma{\left(\frac{11}{6}\right)}}{5\Gamma{\left(\frac{4}{3}\right)}}\sqrt{π}$$ $$I_{5}=\frac{5\Gamma{\left(\frac{17}{10}\right)}}{7\Gamma{\left(\frac{6}{5}\right)}}\sqrt{π}$$ $$I_{7}=\frac{7\Gamma{\left(\frac{23}{14}\right)}}{9\Gamma{\left(\frac{8}{7}\right)}}\sqrt{π}$$ $$I_{13}=\frac{13\Gamma{\left(\frac{41}{26}\right)}}{15\Gamma{\left(\frac{14}{13}\right)}}\sqrt{π}$$ So , based on the pattern, I thought that if $n$ is a positive integer greater than $3$ , then : $$I_{n}=\frac{n\Gamma{\left(\frac{3n+2}{2n}\right)}}{(n+2)\Gamma{\left(\frac{n+1}{n}\right)}}\sqrt{π}$$ Question: How can we prove the above generalisation ? Can we extend it for even larger set of numbers ?","['integration', 'calculus', 'definite-integrals', 'gamma-function']"
4792464,"Find local minima, maxima, and saddle points for $f(x,y) = \sin x + \cos y + \cos(x-y)$ when $0\le x\le\frac\pi2$ and $0\le y\le\frac\pi2$","What I have done for this problem: I differentiated $f(x,y)$ with respect to $x$ and $y$ . Then, I set $f_x$ and $f_y$ to $0$ to find stationary points. $$f(x,y) = \sin(x) + \cos(y) + \cos(x-y)$$ $$f_x = \cos(x) - \sin(x-y)$$ $$f_y = -\sin(y) + \sin(x-y)$$ $$f_x = f_y = 0$$ $$\cos(x) = \sin(x-y)$$ $$\sin(y) = \sin(x-y)$$ $$\cos(x) = \sin(y)$$ Note: $\cos(x) = \sin(y)$ implies $\cos(y) = \sin(x)$ . $$\sin(\pi/2 - x) = \sin(y)$$ $$y = \pi/2 - x \,\,\{0\le x \le\pi/2, 0\le y \le\pi/2\}$$ Using the Second Derivative Test, I categorized the points on the line segment ( $y=\pi/2 - x$ ). $$f_{xx} = -\sin(x) - \cos(x-y)$$ $$f_{yy} = -\cos(y) - \cos(x-y)$$ $$f_{xy} = \cos(x-y)$$ $$f_{yx} = \cos(x-y)$$ $$D = f_{xx}f_{yy} - f_{xy}f_{yx}$$ $$D = \cos(y)(\cos(y) + 2\cos(x-y))$$ Finally, the Second Derivative Test is inconclusive when $y = \pi/2$ and $x = 0$ , and for all the other points on the line segment $f(x,y)$ when $0 \lt x \le \pi/2$ and $0\le y \lt \pi/2$ represents local maximum because $D \gt 0$ and $f_{xx} \lt 0$ . This does not make sense because when I graph $f(x,y)$ on Desmos 3D graphing calculator it only shows one local maximum and one local minimum ( https://www.desmos.com/3d/326ec8bd3d ).",['multivariable-calculus']
4792492,Is this a new point on the nine-point-circle of a triangle?,"I was trying to get a feel for how to solve another question about the largest triangle that can fit in a unit square, by constructing the smallest enclosing square of a triangle in Geogebra. While doing this I 'discovered' what appears to be a new point on the nine-point circle of a triangle, that relates a rectangle to a triangle via a circle. Here the intersection with the nine-point-circle is outside both the rectangle and the triangle. The intersection in this case, is the only point on the nine-point-circle that is entirely within the triangle. If the triangle is a right triangle and the rectangle is constructed so that side of the rectangle lies on the midpoint between the shared vertex and the right angle, the rectangle is a square. The point is very easy to construct.
Construct a rectangle such that each of the (possibly extended) sides lies on a vertex of the triangle. Since there are 4 vertices on the rectangle and only 3 on the triangle, the rectangle and triangle must always have a vertex in common. Now draw lines from each of the unshared vertices of the rectangle to the midpoint of the associated side of the triangle.
These 3 lines will always intersect at a point that lies on the nine-point-circle of the triangle, although I cannot prove that formally. This remains true even if the rectangle overlaps the triangle or is smaller that the triangle. I think this point is interesting because it shows how a triangle and a rectangle and a circle are related, and I have never seen a triangle centre or point on a nine-point-circle constructed like this before. Here is a link to an interactive construction uploaded to the Geogebra website. Any of the blue points on the construction can be moved. I cannot relate this point to any other well know centre or line of a triangle. At the moment the point can only be found by constructing the rectangle first. If it was possible to construct the point independently, it could be helpful in constructing the smallest possible enclosing square of a given triangle and help answer the question posed by the other member. Wondering if this is a new point I found an online Encyclopaedia of Triangles but have no idea how to search that database. Is it new? Has anyone seen it before and does it have a name?","['rectangles', 'circles', 'geometry', 'triangles', 'triangle-centres']"
4792671,Finding normal vector of rotated square (parallelogram),"I'm not a mathematician, so please excuse any misuse of terms. :) I have a 3D model of a cube. Each side of the cube is a different color. A camera is observing the cube. The camera takes a screenshot and passes it to an image processing algorithm. The algorithm draws around the most prominent side of the cube (i.e. the side of the cube that the camera can see most of). This shape is a parallelogram, as the shape is not necessarily directly facing the camera. My question is, knowing the four corners of the parallelogram, how do I decide the relative angle that the side is facing compared to the camera? I think I need the normal vector, but how do I calculate this from the corners? In the screenshot I have provided, the green outline of the red face is where I need to calculate the relative position that the red face is facing.",['geometry']
4792691,Limits and multivariable,"I want to find the limit of the function $f(x,y)$ . Note that $f: \mathbb{R}^2 \to \mathbb{R}$ . $$
f(x,y) = \begin{cases} 
\dfrac{e^{-x^2}-e^{-y^2}}{x^2+y^2} 
&\text{if } (x,y) \neq (0,0), \\[2pt]
k&\text{if } (x,y) = (0,0). 
\end{cases}
$$ I want to find a function $F(x) = \lim_{y \to 0} f(x,y)$ . The way I have done has been to first substitute $0$ into $y$ , and then I got the first expression, where $x \neq 0$ . In the second I used L'Hôpital's rule to get that $k = -1$ . Note that on $x \neq 0$ the function is undefined. $$
F(x) = \begin{cases} 
\dfrac{e^{-x^2}-1}{x^2} 
&\text{if } x \neq 0, \\[2pt] 
-1 &\text{if } x = 0.
\end{cases}
$$ However, I am in doubt whether my answer is correct.","['limits', 'multivariable-calculus']"
4792708,Differential densities of rank $k<n$.,"Motivation In "" Gelfand Transforms and Crofton formulas "" the authors state that ""Roughly speaking, densities are the most general objects that can be integrated over submanifolds independently of parameterization and orientation"". So I wanted to understand these concepts better. There was a very good answer about densities (of rank $n$ ) here , and I was wondering how this extends to densities of rank $k$ . Density of rank $k$ In "" Gelfand Transforms and Crofton formulas "" the authors define a $k$ -density (a density of rank $k$ , not to be confused with the weight) over a vector space $V$ as a continuous function $\phi:\underbrace{V\times\ldots\times V}_k\to\mathbb{R}$ such that for any set of $k$ linearly independent vectors $\{v_j\}_{j=1}^k$ and $A\in GL(k,\mathbb{R})$ we have $$\phi(Av) = |\det A|\phi(v).$$ If I understood it correctly, the notation $Av$ means $(\sum_j A_{j1} v_j, \ldots, \sum_j A_{jk} v_j)$ . It is clear that for $k=n=\dim V$ this is just a usual (volume) density. For $k<n$ , $\phi$ behaves like a density when restricted to any $k$ -dimensional subspace of $V$ . In fact it is less constrained than an $n$ -density on the full space. Did I understand this correctly? Examples of $k$ -densities: $|\alpha|$ , $\langle \alpha, \beta\rangle$ ? Let $\alpha = \alpha_{i_1\ldots i_n} \, e^1\wedge\ldots \wedge e^n$ be a $n$ -covector for an $n$ -dimensional space. Then from $\alpha(Av) = (\det A) \,\alpha(v)$ , and thus $|\alpha(Av)| = |\det A| \, |\alpha(v)|$ is a density. That is, it is enough to take the absolute value of an $n$ -covector to get an $n$ -density. If $V$ is a space over the complex numbers then $|z| = z^*z$ . It's also clear that I can get a density of weight $s$ from $|\alpha|^s$ . Now let $\beta = \beta_{i_1\ldots i_n} \,e^1\wedge\ldots \wedge e^n$ then I believe the following object is an $n$ -density of weight $2$ : $$\langle\alpha,\beta\rangle = \alpha^*_{i_1\ldots i_n}G_{i_1\ldots i_n}\beta_{i_1\ldots i_n} (e^1\wedge\ldots\wedge e^n)^*(e^1\wedge\ldots\wedge e^n) = \alpha^*_{i_1\ldots i_n}G_{i_1\ldots i_n}\beta_{i_1\ldots i_n}|e^1\wedge\ldots\wedge e^n|^2.$$ This should hold because $$|(e^1\wedge\ldots \wedge e^n)(Av)|^2 = |\det A|^2\, |(e^1\wedge\ldots \wedge e^n)(v)|^2.$$ So if I want to consider $k<n$ I can try to see whether the above approaches work for $k$ -densities. Proof that $|\alpha|$ is a $k$ -density Let $\alpha = \sum_{|I|=k} \alpha_I \, (e^{i_1}\wedge\ldots\wedge e^{i_k})$ . I want to show that $|\alpha(Av)| = |\det A| |\alpha(v)|$ . I will show $\alpha(Av) = (\det A) \alpha(v)$ from which the former will follow. Let $\{e_j\}_{j=1}^n$ be the basis of $V$ that is biorthogonal to the basis $\{e^i\}_{i=1}^n$ for $V^*$ , biorthogonal meaning $e^i(e_j) = \delta^i_j$ . Now let $\{v_r\}_{r=1}^k$ be $k$ vectors from $V$ , then their coordinates with respect to $\{e_i\}_{i=1}^n$ are given as the $n\times k$ matrix $C_{ij} = e^i(v_j)$ . Then it follows that: \begin{equation}
\begin{aligned}
\alpha(Av) &= \sum_{|I|=k}\alpha_I \,(e^{i_1}\wedge\ldots\wedge e^{i_k})(Av)  \\
&=\sum_{|I|=k}\alpha_I \, \det C_IA \\
&= \sum_{|I|=k}\alpha_I \, \det C_I \det A \\
&= (\det A)\, \alpha(v).
\end{aligned}
\end{equation} Here $C_I$ was the $k\times k$ submatrix of $C$ with rows from $I$ (in ascending order). Taking powers $|\alpha|^s$ results in a $k$ -density of weight $s$ . Proof that $\langle \alpha, \beta \rangle$ is a $k$ -density Let $\alpha = \sum_{|I|=k} \alpha_I\, (e^{i_1}\wedge\ldots\wedge e^{i_k})$ and $\beta = \sum_{|J|=k} \beta_J\, (e^{j_1}\wedge\ldots\wedge e^{j_k})$ . I define their ""inner product"" by distributing the terms: \begin{equation}
\begin{aligned}
\langle \alpha, \beta\rangle
&= \left(\sum_{|I|=k}\alpha_I\,(e^{i_1}\wedge\ldots\wedge e^{i_k})\right)^*G
\left(\sum_{|J|=k}\beta_J\,(e^{j_1}\wedge\ldots\wedge e^{j_k})\right)\\
&= \sum_{|I|=k}\sum_{|J|=k} \alpha_I^*G_{I,J}\beta_J \, (e^{i_1}\wedge\ldots\wedge e^{i_k})^*(e^{j_1}\wedge\ldots\wedge e^{j_k}),
\end{aligned}
\end{equation} where $^*$ is complex conjugation. The product is to be understood pointwise and not as a wedge or something else, i.e. $$\bigl((e^{i_1}\wedge\ldots\wedge e^{i_k})^*(e^{j_1}\wedge\ldots\wedge e^{j_k})\bigr)(v) = \bigl((e^{i_1}\wedge\ldots\wedge e^{i_k})(v)\bigr)^*\cdot\bigl((e^{j_1}\wedge\ldots\wedge e^{j_k})(v)\bigr),$$ where $\cdot$ is multiplication of complex numbers. Now let $C_{ij} = e^i(v_j)$ , then I can write: \begin{equation}
\begin{aligned}
\langle \alpha, \beta \rangle(Av)
&= \left(\sum_{|I|=k}\alpha_I \,(e^{i_1}\wedge\ldots\wedge e^{i_k})(Av)\right)^*G\left(\sum_{|J|=k}\beta_J \,(e^{j_1}\wedge\ldots\wedge e^{j_k})(Av)\right) \\
&= \left(\sum_{|I|=k}\det \alpha_I \,\det C_IA\right)^*G\left(\sum_{|J|=k}\det \beta_J \,\det C_JA\right) \\
&= \left(\sum_{|I|=k}\alpha_I \,\det C_I \det A\right)^*G\left(\sum_{|J|=k}\det \beta_J \,\det C_J \det A\right) \\
&= (\det A)^*(\det A)  \left(\sum_{|I|=k}\alpha_I \,(e^{i_1}\wedge\ldots\wedge e^{i_k})(v)\right)^*G\left(\sum_{|J|=k}\beta_J \,(e^{j_1}\wedge\ldots\wedge e^{j_k})(v)\right)  \\
&= |\det A|^2 \langle \alpha, \beta\rangle (v).
\end{aligned}
\end{equation} I can take powers $\langle \alpha,\beta\rangle^{s/2}$ to get densities with weight $s$ . I should also be able to formulate a similar operation such as $\langle \alpha_1,\ldots,\alpha_r\rangle$ by expanding things in a similar manner, which should yield a $k$ -density of weight $r$ (provided $r$ is even). Interpretation If $|\omega|$ is indeed a density for $k<n$ , then what would be the physical/geometrical meaning of it in an integration problem? For instance a $2$ -form in 3D $\omega = \star (f_1dx + f_2 dy + f_3 dz)$ can be used to integrate surface flux given a vector field $f$ . It results in $f\cdot \hat{n} dS$ . The corresponding $2$ -density $|\omega|$ then integrates $|f\cdot \hat{n}|dS$ , which is some kind of quantity that ""doesn't care"" about whether $f$ is in the positive or negative halfspace w.r.t. $n$ - it always takes the normal that would produce a positive result. Are there physical examples of something that would integrate like this over a surface? Some kind of non-oriented flux? Densities of the form $\|\omega\|_2 = \sqrt{\langle \omega, \omega\rangle}$ should represent $k$ -volume measure I believe. As an example for $k=1$ and $G_{ij} = \delta_{ij}$ I have $vol_1 = \sqrt{\sum_i (e^i)^2}$ . So $G_{ij}$ here plays the role of metric tensor coefficients I guess, and the variants with more slots are some kinds of generalizations of this. I have no idea what $\langle \alpha, \beta\rangle$ for $\alpha\ne \beta$ is though. Double Cover and Densities Finally, if I were to integrate a $2$ -form $\omega$ over the double cover of a non-orientable surface, e.g. a Mobius strip in $\mathbb{R}^3$ , is it equivalent to twice the integral of $|\omega|$ or $\|\omega\|_2$ or some other $2$ -density? Or is it something else? Is there a theorem about this anywhere? More generally, is the correspondence: form $\to$ oriented manifold, pseudoform $\to$ potentially non-oriented but orientable manifold, density $\to$ potentially non-orientable manifold.","['differential-forms', 'smooth-manifolds', 'differential-geometry']"
4792717,Solving $\lim_{x\to0^+}e^{1/x}\bigl(1-\sec(x)\bigr)$ algebraically,I want to solve the following limit using algebraic techniques learnt in calculus class (including l'Hospital rule): $$\lim_{x\to0^+}e^{1/x}\bigl(1-\sec(x)\bigr)$$ I have tried simplifying the expression and putting over the same denomniator. $$\lim_{x\to0^+}\frac{e^{1/x}(\cos(x)-1)}{\cos(x)}$$ Then I just need to solve this expression: $$\lim_{x\to0^+}e^{1/x}(\cos(x)-1)$$ I have tried expressing it as a fraction to apply l'Hospital's rule but it only seems to get worst. $$\lim_{x\to0^+}\frac{\cos(x)-1}{e^{-1/x}}=\lim_{x\to0^+}\frac{-x^2\sin(x)}{e^{-1/x}}$$,"['limits', 'calculus', 'exponential-function', 'trigonometry']"
4792718,"Solving an ""almost differential"" equation [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 8 months ago . Improve this question This problem comes from a problem set that my calculus professor assigned to me. Determine all functions $f: \mathbb{R} \to \mathbb{R}$ such that $$
f(x) = f\Bigl(\frac{x}{2}\Bigr) + f'(x)\, \frac{x}{2} 
\quad \forall x \in \mathbb{R}.
$$ The answer is all linear ones, for the record. I have an elementary, though ad-hoc, solution using specific supremum and infimum properties. What I wonder is whether or not problems of this type are prone to a specific or ""known"" method, and if so, how this method can be applied to this particular problem. I tried to prove it in a more standard manner but didn't find something useful: I differentiated both sides two times and deduced $f''(0)=0$ . Inductively, one can prove $f^{(n)}(0)=0$ for $n\geq 2$ . Now, would Taylor series suffice to prove $f(x)=ax+b$ ? Thanks in advance","['functional-equations', 'calculus', 'supremum-and-infimum', 'ordinary-differential-equations']"
4792747,"Reduction of Order to solve, but can't get right answer","Given the equation y'' + 2y' + y = 0  and the solution  y1 = ${xe}^{-x}$ . Solve for a second solution $y_2$ . I've solved it twice and get $y_2 = {xe}^{-x}(-{x}^{-1}+c)$ , but that's not being counted as correct. I distributed the ${xe}^{-x}$ in my actual answer.
Can someone please walk me through how to get the solution? I solved once using $y_2 = y_1(x)\int\frac{e^{-\int P(x) dx}}{y_1^2}$ and again using $y_2 = y_1(x)u(x)$ .","['reduction-of-order-ode', 'ordinary-differential-equations']"
4792762,What is the probability that a matrix with i.i.d. normal entries is stable?,"Let $A$ be an $n \times n$ random matrix, such that the entries $a_{ij}$ are i.i.d. from the standard normal distribution. I'm curious on the probability that $A$ is Schur stable. That is, $$P(\rho(A) < 1),$$ where $\rho(A)=\max\{|\lambda|:\lambda \in spec(A)\}$ . Are there practical lower bounds (i.e. something other than insanely tiny values)? The best I was able to figure out was $P(\rho(A)<1)=\sum_{j=1}^nP(|\lambda_j(A)|<1)$ , where $\lambda_j$ is the $j$ th eigenvalue of $A$ .","['statistics', 'eigenvalues-eigenvectors', 'stability-theory', 'linear-algebra', 'probability']"
4792768,Optimal Betting Strategy in Coin Toss,"Goal: Participate in a coin-tossing game with the aim to maximize your earnings. A fundamental part of the game is formulating a strategy that optimizes your earnings based on the observation of the coin toss outcomes. Rules: The Coin : A coin is tossed, landing on ""HEADS"" with an unknown probability  p . Observation and Betting : You have the option to either observe the outcome of the toss without betting or place a fixed bet on an interval where you believe  p  lies. Placing Bets : Betting involves selecting a confidence interval where you believe the probability  p exists. The bet is fixed, meaning you bet the same amount each time. Earnings : If you choose an interval that includes p , you earn a point. If you narrow down the interval size compared to the previous round while  p  is still within the interval, your earnings are quadratically increased based on the reduction in the interval size. Strategy : The goal of the game is to find the best strategy that maximizes your total earnings. The strategy involves when to observe, when to bet, and how to adjust the betting intervals based on the observed outcomes. Gameplay: In each round, the game simulates a coin toss, revealing whether it lands on ""HEADS"" or ""TAILS"". Based on the outcomes, you make a decision—either to observe or to bet by choosing a probability interval. Implement your strategy based on the observations to optimize the betting intervals and maximize earnings. Conclusion of the Game: The game proceeds for a set number of rounds (e.g., 100 rounds). At the end of the game, the actual value of  p is disclosed, and your total earnings are calculated, showcasing the effectiveness of your strategy. Example: For instance, you decide to observe for the first five rounds, noting the outcomes. In the subsequent round, you may choose to bet with a certain interval based on your observations and strategy. Continually adjust your strategy based on the outcomes and the objective to find the most profitable betting intervals. Remember, the fundamental aim is to develop and apply a strategy that maximally increases your earnings throughout the game, using observation and intelligent betting based on the revealed outcomes.","['gambling', 'optimization', 'statistics', 'probability']"
4792769,Some general question about statistics,"Consider an experiment of picking a real number between 0 and 1. Let's say I picked 0.5. Two interpretations are possible: Something extraordinary has happened, because the probability of picking 0.5 from the interval $[0,1]$ is zero. Nothing extraordinary has happened, because the probability that the number you picked is between 0 and 1 is 1. Both interpretations seem reasonable, but they have completely different conclusions. What is going on?","['statistical-inference', 'statistics']"
4792802,Show a free group has no relations directly from the universal property,"The free group is often defined by its universal property. A group $F$ is said to be free on a subset $S$ with inclusion map $\iota : S \rightarrow F$ if for every group $G$ and set map $\phi:S \rightarrow G$ there exists a unique homomorphism $\overline{\phi}:F \rightarrow G$ such that $\overline{\phi} \circ \iota (s) = \phi (s)$ $ \forall s \in S$ . It is said that the existence of $\overline{\phi}$ is what determines there are no relations. My question is (without defining the (free) group of reduced words) can you show there are no relations just by picking groups G to map into? That is, can you show no reduced words on $S^{\pm1}$ (excluding the empty word) are equal to the identity element  in the free group? Example attempt: Suppose the reduced word $w$ is not the empty word, so it contains some letter $a$ . Suppose adding all the powers of $a$ in the word $w$ gives the integer $k$ . Define the set map $\phi : S \rightarrow \mathbb{Z}/( \lvert k \rvert +1)\mathbb{Z} $ by $\phi (s) = \left\{
     \begin{array}{@{}l@{\thinspace}l}
       0  & \text{ if } s \neq a\\
       1 & \text{ if } s = a\\
     \end{array}
   \right.$ then the homomorphism from $F$ to $\mathbb{Z}/( \lvert k \rvert +1)\mathbb{Z}$ extending the set map $\phi$ sends $w$ to $k$ . As homomorphisms preserve group identities, and k is not the identity in $\mathbb{Z}/( \lvert k \rvert +1)\mathbb{Z}$ , then $w$ is not the identity in $F$ . This was my first idea, but fails because it misses the case where $k=0$ .","['combinatorial-group-theory', 'universal-property', 'abstract-algebra', 'free-groups', 'group-theory']"
4792826,How to prove the resonate theorem using Gelfand Lemma.,"Here is the Gelfand lemma :Suppose $X$ is a Banach space, $p:X\to\mathbb{R}$ satisfies : $p(x)\ge 0$ ; $p(\lambda x)=\lambda p(x),\quad \forall \lambda>0$ ; $p(x_1+x_2)\le p(x_1)+p(x_2)$ ; $\liminf\limits_{n\to\infty}p(x_n)\ge p(x)$ where $x_n\to x$ . Then there exists $M>0$ such that $p(x)\le M||x||$ . I have proved it and try to use it to prove the uniform boundness principle : Let $X,Y$ be Banach spaces and $\{A_\lambda:\lambda\in\Lambda\}$ be a family of bounded linear operators from $X$ to $Y$ . If $\{A_\lambda:\lambda\in\Lambda\}$ is pointwise bounded on X, then $\{A_\lambda:\lambda\in\Lambda\}$ is uniformly bounded, i.e., $\sup∥T_λ∥<\infty$ . I considered to show that $\sup\limits_{\lambda\in\Lambda}||A_\lambda x||$ is the $p(x)$ in Gelfand Lemma but I failed.","['bilinear-operator', 'functional-analysis']"
4792862,Math Problem of toilet paper [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 months ago . Improve this question When I go # $2$ , I like to fold the toilet paper into layers so that I don’t get my hand dirty. My process is that I take a piece of paper with $8$ rectangles. Folding it in half, I get $4$ rectangles which results in a piece with two layers. Next, I fold that piece in half resulting in a piece with $2$ rectangles which results in three layers. Then I fold that piece which  results in one rectangle having four layers. Then I do my business. My question is, what can describe this process (an operation, a function, etc.)? Can it be generalized? Is it reversible? Is the domain continuous or discrete?
Can this be applied as a math lesson in school and at what grade? Can this lesson have connections with different areas of mathematics or other subjects? Let $x$ be the number of rectangles in the toilet paper ( $x \in (\Bbb{N} \cup{{0}})$ ). The pattern would indicate that $log_{2}(x)$ is a good function if I fold the paper in half evenly every time. This function would work because the paper with 8 rectangles folded in half three times collapse to only one rectangle with 4 layers. I see it as 8 rectangles collapsing to one rectangle after folding three times in half. I see undoing the folding as the inverse function of $log_{2}(x)$ with its corresponding domain, namely $2^{x}$ . If I fold the paper evenly in thirds, I would get $log_{3}(x)$ and its corresponding inverse $3^{x}$ . Let $a$ be the number of foldings. Then $log_{a}(x)$ would be the function and it’s inverse would be $a^{x}$ for $a \in \Bbb{N}$ and $x \in (\Bbb{N} \cup{{0}})$ . Now, this would be a good lesson for algebra 2 class or pre-calculus class. Some of the concepts to talk about could be function, domain, range, codomain, inverse function, continuity in domain, etc. But $x$ and $a$ could be extended to a more abundant number system, say the nonnegative real numbers. I would say that since $x$ is the number of rectangles in paper and the rectangles can be folded unevenly and at any point in the length of paper (as long as the foldings are the same size), the domain of $log_{a}(x)$ is continuous. That is, $x$ and $a$ are in the nonnegative real numbers as long as each folding is the same. For example, fold 8.125 rectangles into equal parts of 1.75 rectangles. In this sense, both $x$ and $a$ are continuous and discrete.","['functions', 'problem-solving']"
4792881,Basic Die Game expected payout after re-roll,"Alice rolls a fair 6−sided die with the values 1−6 on the sides. She sees that value showing up and then is allowed to decide whether or not she wants to roll again. Each re-roll costs
$1. Whenever she decides to stop, Alice receives a payout equal to the upface of the last die she rolled. Note that there is no limit on how many times Alice can re-roll. Assuming optimal play by Alice, what is her expected payout on this game","['expected-value', 'dice', 'probability-theory']"
4792884,Expected number of packs until two cards are collected,"Say we want to collect two cards A and B, each appearing independently in the pack with probability $p$ . Note that a pack may contain both A and B at the same time with probability $p^2$ . I want to find the expected number of packs that I need to buy to until I obtain at least one A card and at least one B card. My Attempt:
We can have 3 possible events in the experiment where I draw packs until my condition is satisfied: $E_A$ : obtaining an A before a B $E_B$ : obtaining a B before an A $E_{AB}$ : obtaining A and B at the same time Let the probabilities of the above events be $p_A$ , $p_B$ and $p_{AB}$ respectively. We have $p_A+p_B+p_{AB}=1$ and by symmetry $p_A=p_B$ . We can compute $p_{AB}$ as the probability of getting a pack with A and B before seeing a pack with only A or a pack with only B (the latter two packs have a probability $p(1-p)$ each). $$p_{AB} = \frac{p^2}{p^2 + 2p(1-p)}$$ Now, my approach was to use the law of total expectation. If X was the number of packs I had to buy, then \begin{align}
E[X] &= 2p_A E[X| E_A] + p_{AB}E[X|E_{AB}]
\end{align} My intuition was that if I condition on the event that A or B appears first in a pack on their own, then I can possibly model the waiting time as the sum of two geometric random variables with parameter $p$ . And if I condition on them appearing at the same time ( $E_{AB}$ ), it would be one geometric random variable with parameter $\frac{p^2}{p^2 + (1-p)^2}$ since I would be working with a reduced sample space with only packs that contain both or neither. I am having trouble justifying the step where I replace the conditional expectations with the geometric expectations. Is my intuition off here?","['conditional-probability', 'geometric-probability', 'probability-theory', 'probability']"
4792913,"Evaluating $J=\int_0^{\infty} \frac{x^3 \ln \left(e^x+\frac{x^3}{6}+\frac{x^2}{2}+x+1\right)-x^4}{\frac{x^3}{6}+\frac{x^2}{2}+x+1}\,dx$","$$\int_0^{\infty} \frac{x^3 ln \left(e^x+\frac{x^3}{6}+\frac{x^2}{2}+x+1\right)-x^4}{\frac{x^3}{6}+\frac{x^2}{2}+x+1}\,dx$$ My attempts: I wanted to make sure if this integral exists, ran this through Wolfram, check here ; $$J=4.9348$$ Tried some rudimentary techniques; $$J=\int_0^{\infty} \frac{x^3 ln \left(e^x+\frac{x^3}{6}+\frac{x^2}{2}+x+1\right)}{\frac{x^3}{6}+\frac{x^2}{2}+x+1}\,dx-\int_0^{\infty}\frac{6x^4}{x^3+3x^2+6x+6}\,dx$$ The second integral was not convergent, so this attempt failed. Tried multiple substitutions and trying to match along generalized forms, but that did not help either. The last attempt includes; Rewriting the integral; $$p(x)=\frac{x^3}{6}+\frac{x^2}{2}+x+1=\sum_{r=0}^{3}\frac{x^r}{r!}$$ $$J=\int_0^{\infty} \frac{x^3 ln \left(e^x+p(x)\right)-x^4}{p(x)}\,dx$$ I am stuck here and am interested in learning how to carry this forward. Edit : Found this integral's value to be $\frac{\pi^2}{2}$ .","['integration', 'definite-integrals', 'contest-math']"
4793014,Is there an injection $f:\mathbb{R}\to\mathbb{R}$ that is unbounded on every interval?,"I'm going to say a function $f:\mathbb R \to \mathbb R$ is unbounded on every interval if for all reals $a,b,M$ with $a<b$ , there exists $x\in[a,b]$ such that $|f(x)| > M$ . It is easy to that such functions exist, by for instance exploiting the density of the rationals. It's not too much harder to construct surjective examples: using e.g. decimal expansions, one can construct a function $f:\mathbb{R}\to\mathbb{R}$ which is 'surjective on every interval' in the sense that for any $a<b$ and $c$ there exists $x\in[a,b]$ with $f(x)= c$ . However, any such construction doesn't give you much 'control' about how $f$ actually behaves. Is there such a function which is unbounded on every interval, and is injective over $\mathbb{R}$ ?","['functions', 'examples-counterexamples', 'real-analysis']"
4793018,"Show that if the ratios of coefficients is unbounded, then the power series converges only at 0","I have a question about Exercise 6.5.8(d) from Understanding Analysis by Stephen Abbott. Problem:
Let $\sum a_nx^n$ be a power series with $a_n\neq0$ . Show that if $|\frac{a_{n+1}}{a_n}|$ is unbounded, then the original series $\sum a_nx^n$ converges only when $x = 0$ . I have come up with a possible counterexample.
Let $(a_n)=(1,\frac{1}{2},1,\frac{1}{3},1,\frac{1}{4},\dots)$ . Then $|\frac{a_{n+1}}{a_n}|$ is unbounded. For $x=\frac{1}{2}$ , $$
\sum a_n x^n =\sum a_n \frac{1}{2^n}
$$ Since each $0<a_n\leq1$ , the partial sums of the series is bounded by the infinite geometric series $\sum \frac{1}{2^n}$ . As each term is positive, we can use the monotone convergence theorem to show the series converges at $x=\frac{1}{2}$ . However, this contradicts the claim in the question that the series only converges when $x=0$ . So, my question is: why does this counterexample not work? Thanks","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4793029,Prove that if $A\sim B$ then $\mathscr{P}(A)\sim \mathscr{P}(B)$. Is my proof correct?,"I think I understand the way it should be proved, I think I get the idea, but I'm unsure about writing it. That is my proof. Since $A \sim B$ there is a function $f:A\to B$ that is one-to-one and onto. We can use this function to make the function $g:\mathscr{P}(A)\to\mathscr{P}(B)$ . For any $X\subseteq A$ we can take $g(X) = f(X)$ i.e. we use image of $X$ under the function $f$ as our function $g$ from $\mathscr{P}(A)$ to $\mathscr{P}(B)$ . (I'm not sure about that. May be I could use the symbol $f$ without introducing $g$ but I wanted to avoid using $f$ both for the function from $A$ to $B$ and for the function from $\mathscr{P}(A)$ to $\mathscr{P}(B)$ ) Now we should check that $g$ is one-to-one and onto. Let's assume $X_1\in \mathscr{P}(A)$ and $X_2\in \mathscr{P}(A)$ , $X_1\neq X_2$ but $g(X_1) = g(X_2) = Y \in \mathscr{P}(B)$ . Since $X_1 \neq X_2$ let's choose $x\in X_1$ but $x\notin X_2$ . If $X_1 \subseteq X_2$ we should switch the subscripts before choosing $x$ . Let $y = f(x)$ and $y\in Y$ since $g(X_1) = Y$ . But then there is some $x' \in X_2$ so that $f(x') = y$ . But $f$ is one-to-one so $x = x'$ and $x\in X_2$ . Contradiction. Using this steps to be more rigorous it's easy to proove that $X_1 \subseteq X_2$ and $X_2 \subseteq X_1$ . So $g$ is one-to-one. Let's choose any set $Y \in \mathscr{P}(B)$ . Since $f$ is onto then for any nonempty set $Y$ there is nonempty set $X\in \mathscr{P}(A)$ so that $g(X) = Y$ . And for empty set $Y$ we have $g(\varnothing) = Y$ . So $g$ is onto. Does this proof clear? May be it should be improved.","['elementary-set-theory', 'solution-verification']"
4793046,"Suppose $A$ is positive definite with $a_{ij}\leqslant 0 (\forall i\neq j)$. Is the matrix $-A+2\mathrm{diag}(a_{11},\dots,a_{nn})$ positive definite?","Suppose $A=(a_{ij})$ is positive definite with $a_{ij}\leqslant 0\ (i\neq j)$ . Is the matrix $-A+2\mathrm{diag}(a_{11},\dots,a_{nn})$ positive definite? My attempt: I have shown that the proposition is true for $n\leqslant 3$ , using Hurwitz criterion for positive definite matrices. But I have no idea about bigger $n$ . Maybe another way is to show $\lambda_i<2a_{ii}$ , where $\lambda_i$ are eigenvalues, but I have no progress. Any hints/counterexamples are welcomed. THANKS!","['matrices', 'linear-algebra']"
4793106,How to compute the conditional expectation of a jointly multivariate normal distribution?,"In the paper Functional Linear Discriminant Analysis , on page 7, they use the EM algorithm. The model involves random variables $\gamma\sim\mathcal{N}_m(0, \Gamma)$ and $\epsilon\sim\mathcal{N}_n(0, \sigma²I)$ and is of the form $$\mathbf{Y}=S(\beta + \gamma) + \epsilon$$ , where $S$ is a design matrix, $\beta$ is the parameter and $\mathbf{Y}$ is the regression target. They then try to compute the MLE of $\Gamma, \sigma²$ and $\beta$ via the EM algorithm, treating $\gamma$ as a latent variable. Clearly, the joint log likelihood is $$l(\sigma², \Gamma, \beta)\propto-\frac{||\mathbf{Y} - S(\beta+\gamma)||²}{\sigma²}-\gamma^T\Gamma^{-1}\gamma-\log(|\Gamma|)-n\log(\sigma²)$$ However, they then compute the expectation of $\gamma$ for the E-step as $$\mathbb{E}[\gamma|\mathbf{Y}, \beta, \Gamma, \sigma²]=(\sigma²\Gamma^{-1} + S^TS)^{-1}S^T(\mathbf{Y}-S\beta)$$ I cannot move ahead after computing $S\gamma = \mathbf{Y}-S\beta-\epsilon$ . I know that this means $S\mathbb{E}[\gamma|\mathbf{Y},\beta,\Gamma,\sigma²]=\mathbf{Y}-S\beta$ , but how do I transfer $S$ to the RHS?","['statistics', 'statistical-inference', 'matrices', 'optimization', 'probability']"
4793121,Proving $\sum_{n=-\infty}^\infty n^2e^{-\pi n^2}=\frac{\Gamma (1/4)}{4\sqrt{2}\pi^{7/4}}$,"I conjecture that $$\sum_{n=-\infty}^\infty n^2e^{-\pi n^2}=\frac{\Gamma (1/4)}{4\sqrt{2}\pi^{7/4}}$$ because the left-hand side and right-hand side agree to at least $50$ decimal places. Is the identity true? This series popped up when I was trying to write down the Hadamard factorization of a certain function related to an elliptic function with a square period lattice. The series in question is $(-1/4)$ times the second derivative of a Jacobi theta function evaluated at zero: $$\sum_{n=-\infty}^\infty n^2e^{-\pi n^2}=-\frac{1}{4}\theta_3''(0,e^{-\pi}),$$ in the notation of DLMF. I was trying to look it up in DLMF ( https://dlmf.nist.gov/20.4 ). I found that $$\frac{\theta_3''(0,e^{-\pi})}{\theta_3(0,e^{-\pi})}=-8\sum_{n=1}^\infty \frac{e^{-(2n-1)\pi}}{(1+e^{-(2n-1)\pi})^2},$$ while $\theta_3(0,e^{-\pi})$ is known ( Proving $\sum_{n=-\infty}^\infty e^{-\pi n^2} = \frac{\sqrt[4] \pi}{\Gamma\left(\frac 3 4\right)}$ ). This translates the series in question to an alternative ""Lambert-like"" series ( https://en.wikipedia.org/wiki/Lambert_series ), but I'm unable to proceed.","['complex-analysis', 'gamma-function', 'elliptic-functions', 'closed-form', 'sequences-and-series']"
4793165,optimisation problem that minimises the sum of absolute errors of the simple linear model $y = a + bx$.,"Consider the following sample: $(x_1, y_1) = (1, 1)$ , $(x_2, y_2) = (2, 5)$ , $(x_3, y_3) = (3, 8)$ , $(x_4, y_4) = (4, 18)$ . How can I write down an optimisation problem that minimises the sum of absolute errors of the simple linear model $y = a + bx$ . Also, is this optimisation problem constrained? Is the objective function differentiable?
(Optional: Is the objective function convex?) For 1) is it going to be just: $$min\sum_{i=1}^{4}|y_i-(a+bx_i)|$$","['statistics', 'data-mining', 'data-analysis', 'optimization', 'probability']"
