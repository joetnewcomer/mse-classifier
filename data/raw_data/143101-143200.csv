question_id,title,body,tags
2325645,"Does the system of equations a + b, a * b provide a one-to-one correspondence between ordered and unordered sets?","Let's write ordered sets as $[a,b,c,...]$ and $\{a,b,c,...\}$ for unordered sets. Now let $f(a,b)=[a+b,a b]$. Note $f(a,b)=f(b,a).$ Consider $g([x,y])=\{a,b\}$ where $a+b=x,a b=y$. Mathematica's Solve function came up with $\left\{\frac{1}{2} \left(x-\sqrt{x^2-4 y}\right),\frac{1}{2} \left(\sqrt{x^2-4 y}+x\right)\right\}$ as the value of $\{a,b\}$ given $[x,y]$, and in my testing, it worked for everything I threw at it. (On a philosophical note it is interesting that just from the definitions provided there's no implied order for $a$ and $b$ given $[x,y]$.) Is this a correct one-to-one correspondence from ordered sets of two elements to unordered sets of two elements in the domain of imaginary numbers? If so, does it have a name? I'd like to find out more about it. As an aside, the one element and three element cases seem to work also. Mathematica spent a long time trying to find equations for my guess at the four element case $a+b+c+d,a b+a c+a d+b c+b d+c d,a b c+a b d+a c d+b c d,a b c d$, but eventually I aborted it. I suspect if there is a solution, it might generalize to $n$ elements. Also, I think it works with ordered lists and unordered lists. Many thanks! Edit: as DanielWainfleet points out in the comments ""if $x^2=4y$ then $g([x,y])=\{x/2\}$ which has only one element. So the above can't go from ordered sets of two elements to unordered sets of two elements. So I'd like to revise the question to the highly related case of including sets of one OR two elements (unordered sets stay at two elements).",['elementary-set-theory']
2325647,"Find the ratio of lengths in a triangle with cevians, without using mass-points","In $\triangle ABC$, $D$ is the midpoint of $BC$ and $E$ is the midpoint of $AD$. $CE$ is extended to meet $AB$ at $F$. The question is to find the ratio of the lengths of $AF$ to $FB$. This is trivial using mass-points geometry : $$C=1\to B=1\to D=2\to A=2\to E=4\to F=3$$ and thus $AF:FB=1:2$. But how would one solve this without using mass-point geometry? Thanks!","['euclidean-geometry', 'triangles', 'geometry']"
2325650,Prove that a circle exists for which at least $n \in \mathbb{N}$ points have integer coordinates,"This question is based on the fourth question in the 2003 edition of the Flemish Mathematics Olympiad . Consider a grid of points with integer coordinates. If one chooses the number $R$ appropriately, the circle with center $(0, 0)$ and radius $R$ crosses a number of grid points. A circle with radius $1$ crosses 4 grid points, a circle with radius $2\sqrt{2}$ crosses 4 grid points and a circle with radius 5 crosses 12 grid points. Prove that for any $n \in \mathbb{N}$, a number $R$ exists for which the circle with center $(0, 0)$ and radius $R$ crosses at least $n$ grid points. I have tried to solve this question by induction, considering a given point $(i, j)$, $i \gt j$, on the circle with radius $R$ and attempting to extract multiple points from this on a larger circle. In this case, the coordinates $(i+j,i-j)$ and $(i-j, i+j)$ are both on a circle with radius $\sqrt{2}R$. However, since $(j, i)$ is also a point on the circle, the number of crossed grid points remains the same. What is a correct way to prove the above statement?","['circles', 'geometry']"
2325656,Calculate $\lim_{R \to \infty} \int_{|z| = R} \frac{f(z)}{(z-a)(z-b)} dz$,"Let $f$ be an entire function and let $a$, $b \in \mathbb{C}$ with $a \neq b$. 1) Calculate $ \int_{|z| = R}  \frac{f(z)}{(z-a)(z-b)} dz$, if $R > |a|, |b|$. 2) Suppose $f$ is bounded and evaluate $\lim_{R \to \infty} \int_{|z| = R}  \frac{f(z)}{(z-a)(z-b)} dz$ For question (1) I used the fact that $ \int_{|z| = R}  \frac{f(z)}{(z-a)(z-b)} dz = \frac{1}{a-b}   (\int_{|z| = R}  \frac{f(z)}{z-a} dz - \int_{|z| = R} \frac{f(z)}{z-b} dz ) \mbox{(partial fractions)}$, and then applied the Cauchy Integral formula to get that, $\frac{1}{a-b}   (\int_{|z| = R}  \frac{f(z)}{z-a} dz - \int_{|z| = R} \frac{f(z)}{z-b} dz ) = 2\pi i \ \frac{f(a)-f(b)}{a-b}$ Now I'm stuck at the second question because I've found the integral to not be dependent on $R$.","['complex-analysis', 'complex-numbers']"
2325660,Function from closed set to closed set - both true and not true?,"Consider the function $f:\mathbb{R}^2\to \mathbb{R}$ defined by $f((s,t))=2s$. Is it true or not that $f$ maps closed sets to closed sets? I thought that it is not true, because we could consider the set $A:=\{(s,0)\in \mathbb{R}^2:s\in \mathbb{R}\}$, which is closed in $\mathbb{R}^2$. Then $f(A)=(-\infty,\infty)$, which is open in $\mathbb{R}$. However, $f(A)$ is also closed in $\mathbb{R}$. So, from the logic standpoint, can we say that this is a counterexample, or not? Or can we say that this example can be both true and false?","['general-topology', 'real-analysis', 'functions']"
2325675,"If the expected value of $X^n$ is $n!$, what is the probability density function of the random variable $X$?","I am working through my homework and this problem has me stumped. I don't know how  to come up with a pdf just by being given an expected value? We are learning about exponential distributions, but this detail isn't stated. Is that because simply the fact that the function is $X^n$ implies it's an exponential distribution? I am also wondering if it has to do with the gamma function because I know that $$E[X^s] = \frac{\Gamma(s+1)}{\gamma^s}.$$ Any direction at all would be greatly appreciated as I am thoroughly confused.","['probability-distributions', 'statistics', 'probability', 'exponential-distribution', 'gamma-function']"
2325683,Expected waiting time for email,"Question: Johnson's mobile has a Gmail app and the arrival time of an email $T$ has following density:
$$T \sim \lambda e^{-\lambda t}$$ When an email arrives in time t, Johnson's mobile email software will elicit a beep: $$b_{t}=
\begin{cases}
1 & \text{with probability $z$}\\
0 & \text{with probaility $1-z$}
\end{cases}$$ Otherwise, if there is no email, $b_{t}=0$ always holds. It will take time $t^{*}$ for Johnson to wait for the email arrival, Johnson will stop waiting in following two situations. Situation 1: If $b_{t}=1$, Johson will stop waiting. $$t_{1}=\min\{t:b_{t}=1\}$$ Situation 2: If $b_{s}=0,s \leq t$, Johnson will form belief in time t:
$$P(\text{The email has arrived before time t}|b_{s}=0,s\leq t)$$
When $P(\text{The email has arrived before time t}|b_{s}=0,s \leq t)=p$, Johnson will also stop waiting. $$t_{0}=min\{t:P(\text{The email has arrived before time t}|b_{s}=0,s\leq t)=p\}$$ Thus we can define:
$$t^{*}=\min\{t_{1},t_{0}\}$$ The question is: What is Johnson's expected waiting time $E[t^{*}]$? In order to help to understand above question, I show the extreme cases of above question: When $z=1$, it means that once the email arriving, the mobile always elicit a beep, the expected waiting time is in fact the expected arrival time:
$$\frac{1}{\lambda}$$ When $z=0$, it means that the mobile never elicit a beep no matter the email arrives or not, then after time t, you will believe that the email arrival probability is:
$$1-e^{-\lambda t}$$ You will check the email when you belief of email arriving equals to p:
$$1-e^{-\lambda t^{*}}=p$$ 
Thus in this situation, the waiting time is degenerate and will be always:
$$t^{*}=-\frac{\ln{(1-p)}}{\lambda}$$ It is easy to calculate the expected waiting time in above two extreme situations($z=1$ and $z=0$), but once $z \in (0,1)$, what is the expected waiting time? The answer by original author is: $$\tilde{t}(z)=\frac{1-(1-p)^{\frac{z}{1-z}}}{\lambda z}$$ It is easy to check that:
$$\tilde{t}(1)=\frac{1}{\lambda}$$
$$\lim_{z \to 0}\tilde{t}(z)=-\frac{\ln(1-p)}{\lambda}$$
The boundary condition holds","['stochastic-processes', 'probability-theory', 'probability', 'probability-distributions']"
2325684,Are solutions $z$ of $Im(z^z)=0$ dense in $\mathbb{C}$?,"Inspired by this question ... More specifically, say $z$ is a solution of $Im(z^z)=0$ if there exist real numbers $\theta$, $r>0$, and integer $n$ such that $z=r e^{i\theta}$ and
$$
r (\ln r \sin \theta + \theta \cos \theta) = n \pi
$$
The left hand side is $Im(z \ln z)$ for some branch of $\ln z$ (not necessarily the principal one). Is the set of such $z$ dense in $\mathbb{C}$?","['complex-analysis', 'exponentiation']"
2325694,Basic Statistical Values,"I wanted to make my own problem and solve it to learn some new stuff -- Let's say we have some type of standardized test, where a rough data curve depicts $P(x)=x^2e^{-\frac{x}{14}}$ which is the number of people who got that score. Now, we have $\displaystyle \int_0^{100}P(x)\,dx = 5342.0$ approximately. So, we scale our probabilities so we have $\displaystyle p(x)=\frac{1}{5342}x^2e^{-\frac{x}{14}}$. Then, to get the mean, we evaluate $\displaystyle \int_0^{100}xp(x)\,dx$, right? I don't think we have to divide by $100$ because it's a weighted average and we have $p(x)$ being the weight of $x$. This would give us $39.928$, which kind of makes sense from the graph! Then, I think that variance would be $\displaystyle \int_0^{100}(x-39.928)^2p(x)\,dx=434.55$ Then, standard deviation would equal to $\sqrt{434.55}=\boxed{20.84}$
Are all my steps correct? What can I do with a standard deviation? Does the $68-95-99.7$ rule apply? UPDATE I now let $X$ range from $0$ to $+\infty$, and the results are really nice! We have the density $p(x)=\displaystyle \frac{1}{5488}x^2e^{-\frac{x}{14}}$ We have mean $42$. We have variance $588$. We have standard deviation $\sqrt{588}$.",['statistics']
2325713,Why multiply a matrix with its transpose?,"This might be a very stupid question, but I do not seem to understand why I would multiple a matrix with its transpose. I am not a mathematician, but I am very interested in understanding the practical usage of equations: Imagine I have three products sales Apple, Orange and Pear for the last 3 days in a matrix form called A: 
$$ A= 
\begin{bmatrix}
        Apple & Orange & Pear \\
        10 & 2 & 5 \\
        5 & 3 & 10 \\
        4 & 3 & 2 \\
        5 & 10 & 5 \\
        \end{bmatrix}$$ What will $AA^{\rm T}$ tell me? I have seen this long answer link: Is a matrix multiplied with its transpose something special? , but I did not get it at all. I see that a lot of equations use the product $AA^{\rm T}$ and I really hope that someone will give a very simple answer.","['matrices', 'transpose']"
2325720,Are the Euler-Maclaurin formula and the Poisson summation formula related?,"The Euler-Maclaurin formula , beautifully explained here by Justin Rheinstadter is expressed as: $$\sum_{i=m}^{n}f(i)=\int_{m}^nf(x)dx\;-\frac{1}{2}\left(f(n) - f(m)\right)\;+\sum_{k=1}^{p}\frac{B_{2k}}{(2k)!}\left(f^{2k-1}(n)-f^{2k-1}(m)\right)\;+f(n)$$ while the Poisson summation formula is $$\sum_{n\in \mathbb Z}f(n)=\sum_{n=-\infty}^{\infty}f(n)=\sum_{k=-\infty}^{\infty}\hat f(k)$$ where $\hat f$ is the Fourier transform of $f$. This is the focus of my interest, trying to make some inroads into this other question . Unfortunately for the amateur math-curious, there is a paucity of didactic videos online on this latter topic, basically restricted to this authoritative Princeton video by Winston Ou with so much ambient noise, tiny handwriting, and talking softly to the blackboard that it is difficult to follow, or this horrendously screechy video by Steven Miller (otherwise a very engaging teacher). I am getting familiarized with both these formulas for the first time, and I'd like to ask whether there are motivational or practical connections (sum from $m$ to $n$ in EMF versus over all integers in PSF), as well as for a bit of context for each one of them.","['poisson-summation-formula', 'complex-analysis', 'fourier-analysis', 'complex-numbers']"
2325724,Eigenvalue of an Euler product type operator?,"Background Let us have the following orthonormal basis such that: $$ \langle m | n \rangle = \delta_{mn}$$ Consider the following operators defined as: $$ \hat 1 = | 1 \rangle \langle 1 | + | 2 \rangle \langle 2 | + | 3 \rangle \langle 3 | + \dots $$
$$ \hat 2 = | 1 \rangle \langle 2 | + | 2 \rangle \langle 4 | + | 3 \rangle \langle 6 | + \dots $$
$$ \hat 3 = | 1 \rangle \langle 3 | + | 2 \rangle \langle 6 | + | 3 \rangle \langle 9 | + \dots $$
$$ \vdots $$
$$ \hat n = | 1 \rangle \langle n | + | 2 \rangle \langle 2n | + | 3 \rangle \langle 3n | + \dots $$ Hence, we notice it these operators have the following properties: $$ \hat a \cdot \hat b = \hat b \cdot \hat a = (\hat{ab}) $$ For example: $$ \hat 2 \cdot \hat 2 = \hat 4 $$ We notice that it has nice multiplicative properties and hence, define the following operator: $$ \hat H = \hat 1^s + \hat 2^s + \hat 3^s + \hat 4^s + \dots $$ Using the Euler product formula (which seems to hold in this case as well): $$ \hat H = (\hat 1-  \hat 2^s)^{-1} \cdot (\hat 1-  \hat 3^s)^{-1} \cdot (\hat 1-  \hat5^s)^{-1} \cdot \dots    $$ Questions What is the eigenvalues and eigenvectors of this operator $\hat H$? Can one define $H$ as some sort of Hamiltonian or density operator? Edit I just thought of the following idea. What if we define the Hamiltonian as: $$ H' =  \left[ {\begin{array}{cc}
   0 & H \\
   H^\dagger  & 0 \\
  \end{array} } \right]$$ The above is a Hermitian operator. What would it physically correspond to?","['number-theory', 'quantum-mechanics', 'linear-algebra']"
2325729,"Find the minimum and maximum distance between the ellipsoid of ecuation $x^2+y^2+2z^2=6$ and the point $P=(4,2,0)$","I've been asked to find, if it exists, the minimum and maximum distance between the ellipsoid of ecuation $x^2+y^2+2z^2=6$ and the point $P=(4,2,0)$ To start, I've tried to find the points of the ellipsoid where the distance is minimum and maximum using the method of Lagrange multipliers. First, I considered the sphere centered on  the point $P$ as if it were a level surface and I construct: $f(x,y,z)=(x-4)^2+(y-2)^2+z^2-a^2$ And I did the same with the ellipsoid, getting the following function: $g(x,y,z)=x^2+y^2+2z^2-6$ Then, I tried to get the points where $\nabla$$f$ and $\nabla$$g$ are paralell: $\nabla$$f=\lambda$$\nabla$$g$ $\rightarrow$ $(2x-8,2y-4,2z)=\lambda(2x,2y,4z)$ So, solving the system I got that  $\lambda=1/2$, so in consquence $x=8$ and $y=4$ But when I replaced the values of $x,y$ obtained in $g=0$ to get the coordinate $z$ I get a complex number. I suspect there is something on my reasoning which is incorrect, can someone help me?","['optimization', 'algebra-precalculus', 'multivariable-calculus', 'parametrization', 'lagrange-multiplier']"
2325773,Extending a vector on a 2-Riemannian manifold,"On a 2-Riemannian manifold $M$ one can try to extend a vector field in the following manner. Given $p \in M$, $V \in T_pM$, choose a coordinate neighborhood $(x^1,x^2)$ around $p$. Can first parallel transport $V$ along the $x^1$ axis, then parallel transport the resulting vectors along the $x^2$ lines. This would be an extension of the vector field, denoted by $Z$. Then I was told that one could argue that on a flat manifold $\nabla_{\partial_1}Z = 0$. Two questions: 1). Why is the resulting $Z$ smooth? Generally, the method used to prove its smoothness is to write it in terms of bases, but I find it hard to do so here. 2).Why is it good to have $\nabla_{\partial_1}Z = 0$ given our initial mission is just to extend a vector?","['riemannian-geometry', 'differential-geometry']"
2325775,"Can we define addition, subtraction, and multiplication operators as a relation using sets on Natural Numbers? [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Or are they considered to be axioms, and don't have any definition in terms of a set?",['elementary-set-theory']
2325792,Factorization of certain polynomials over a finite field,"Suppose we have the polynomial $$F_s(x)=cx^{q^s+1}+dx^{q^s}-ax-b \in \mathbb{F}_{q^n}[x]$$ where $ad-bc \neq 0$. The fact that $ad-bc \neq 0$ means that we can take the coefficients of $F_s(x)$ as entries of a matrix $A \in GL(2,q^n)$. This problem was considered by Stichtenoth and Garefalakis for $A=\left(
\begin{array}{cc}
a&b\\
c&d
\end{array}\right)
$ and $A=\left(
\begin{array}{cc}
a&b\\
0&1
\end{array}\right)
$ respectively. In both cases $A$ was taken to be in $GL(2,q)$. In Garefalakis, the exact number of irreducible polynomials of degree say $r$ in the factorization of $F_s(x)$ is obtained. Now, how I can I find the number of irreducible polynomials(factors) of degree $r$ say in the factorization of $F_s(x)=cx^{q^s+1}+dx^{q^s}-ax-b \in \mathbb{F}_{q^n}[x]$ where the minimal polynomial of $A$ is an irreducible quadratic polynomial over $\mathbb{F}_{q^n}[x]$ where $gcd(n,r)\neq 1$ and $s\mid nr$?","['finite-fields', 'group-theory', 'polynomials', 'linear-algebra']"
2325807,Second Derivative of a Determinant,"How does one evaluate the second derivative of the determinant of a square matrix? Jacobi's formula tells us how to evaluate the first derivative but I can't find anything for the second. This is my attempt: We can start using the partial derivative formulation of Jacobi's formula, assuming A is invertible:
\begin{equation}\frac{\partial}{\partial \alpha}\det A = (\det A) \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right)\end{equation}
        Taking a second derivative:
        \begin{equation}\frac{\partial^2}{\partial \alpha^2}\det A = \frac{\partial}{\partial \alpha}\left[(\det A) \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right)\right]\end{equation}
        Applying product rule:
        \begin{equation}= \frac{\partial}{\partial \alpha}(\det A) \cdot \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +
    (\det A) \frac{\partial}{\partial \alpha} \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right)\end{equation}
Replacing $\frac{\partial}{\partial\alpha}\det A$ with Jacobi's formula:
        \begin{equation}= (\det A) \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) \cdot \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +(\det A) \text{tr}\left( \frac{\partial}{\partial \alpha}\left(A^{-1} \frac{\partial}{\partial \alpha} A \right)\right)\end{equation}
        Factoring out $\det(A)$:
        \begin{equation}= (\det A) \left[\text{tr}^2\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +
     \text{tr}\left( \frac{\partial}{\partial \alpha}\left(A^{-1} \frac{\partial}{\partial \alpha} A \right)\right)\right]\end{equation}
         Another product rule:
        \begin{equation}= (\det A) \left[\text{tr}^2\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +
     \text{tr}\left(\frac{\partial}{\partial \alpha}A^{-1} \frac{\partial}{\partial \alpha} A +
     A^{-1} \frac{\partial^2}{\partial \alpha^2} A \right)\right]\end{equation}
         Finally, using $A_{\alpha}$ to denote the partial of A wrt to $\alpha$ we have
        \begin{equation}\frac{\partial^2}{\partial \alpha^2}\det A= \det(A) \left[\text{tr}^2\left( A^{-1} A_{\alpha} \right) +
     \text{tr}\left(A^{-1}_{\alpha} A_{\alpha}\right) + \text{tr} \left( A^{-1} A_{\alpha^2} \right)\right]\end{equation}
         The second trace actually reduces to N, for an NxN matrix:
         \begin{equation}\text{tr}\left(A^{-1}_{\alpha} A_{\alpha}\right)=\text{tr}(I)\end{equation}
         \begin{equation}=\sum_{diagonals}1\end{equation}
         \begin{equation}=N\end{equation} \begin{equation}\therefore\frac{\partial^2}{\partial \alpha^2}\det A= \det(A) \left[\text{tr}^2\left( A^{-1} A_{\alpha} \right) +
     \text{tr} \left( A^{-1} A_{\alpha^2} \right)+N\right]\end{equation} I'm not overly confident with the matrix calculus, but the result is nice enough to seem plausible. Is there another method, or is this proof valid? Thanks!","['matrices', 'partial-derivative', 'linear-algebra', 'determinant']"
2325808,Intuition behind $\phi(n) = \sum\limits_{d\mid n}\mu(d)\frac{n}{d}$,"I would like to know if the following intuition is valid: Suppose $n = \prod\limits_{i=1}^k p_i^{a_i}$ The only divisors we care about are those that have distinct prime factors (with no squares). So $\sum\limits_{d\mid n}\mu(d)\frac{n}{d} = n - \sum\limits_{i=1}^k\frac{n}{p_i}
+ \sum\limits_{i=1,j= 1, i<j}^k\frac{n}{p_ip_j} - \sum\limits_{i=1,j= 1,m=1,i<j<m}^k\frac{n}{p_ip_jp_m} \ldots $ Essentially this is using PIE to find numbers that are relatively prime to n. Hence I claim that this is equal to $\phi(n)$. Is this correct?","['number-theory', 'elementary-number-theory']"
2325818,Sequence of functions in $L^p$ and derivative,"if we have a sequence of functions,  $(f_n)_n \subset C^1(\mathbb{R})$ such that $f_n, f'_n \in L^p(\mathbb{R}) $ and $$f_n\rightarrow \phi_0 \, in \,L^p, $$
and $$f'_n\rightarrow \phi_1 \, in \,L^p.$$ Also, we can show that $(f_n)_n $ and $(f'_n)_n $ converge uniformly on $\mathbb{R}$. Can we conclude that $ \phi_0$ and $ \phi_1$ are in fact this uniform limits and in this case, $$ \phi_0 \in C^1(\mathbb{R})$$and $$\phi'_0=\phi_1.$$ Thank you for your help.","['derivatives', 'lebesgue-integral']"
2325841,Is the pullback of a connection compatible with a bundle metric also compatible with the pullback metric?,"Let $M$ be a smooth manifold, and $E\to M$ a smooth vector bundle. Then if we have a smooth morphism $f:N\to M$ of smooth manifolds, then we obtain the following commutative diagram, 
$$\require{AMScd} \begin{CD}f^*E @>>> E \\
@VVV @VVV \\
N @>f>> M
\end{CD}$$
where $f^*E$ is the pullback of $E$. Therefore, if we have some bundle metric $g\in E^*\otimes E^*$, we can define the pullback bundle metric $\tilde g:=f^*g\in f^*E^*\otimes f^*E^*$ in the natural way, and for any connection $$\nabla:\Gamma(\mathrm TM)\otimes\Gamma(E)\to\Gamma(E)$$ we can define the pullback connection $$f^*\nabla:\Gamma(\mathrm TN)\otimes\Gamma(f^*E)\to\Gamma(f^*E)$$ as follows: Let $(x^i)$ be coordinates on $N$ mapping into coordinates $(y^j)$ on $M$, and let $E$ be locally trivial with basis $(e_k)$ on $M$, then if $\nabla_{\partial/\partial y^i}e_j=\Gamma_{ij}^k e_k$, then if we set $$\tilde\Gamma_{ij}^k=\frac{\partial f^l}{\partial x^i}\Gamma_{lj}^k\circ f$$ then we can define $\tilde\nabla$ by letting $\tilde\nabla_{\partial/\partial x^i} e_j=\tilde\Gamma_{ij}^k$. Now, suppose $\nabla$ is compatible with $g$, that is to say, for $a,b\in\Gamma(E)$ and $X\in\Gamma(\mathrm T M)$, $$X\langle a,b\rangle = \langle\nabla_Xa,b\rangle+\langle a,\nabla_Xb\rangle$$ then I wish to prove that for $a,b\in\Gamma(f^*E)$ and $X\in\Gamma(\mathrm T N)$, $$X\langle a,b\rangle = \langle\tilde\nabla_X a,b\rangle+\langle a,\tilde\nabla_Xb\rangle$$ but is this true? I've been having trouble proving this even for $X=\frac{\partial}{\partial x^i}$ and $a=e_j$, $b=e_k$. In this case, I obtain that $$\frac{\partial}{\partial x^i}\langle e_j,e_k\rangle = \frac{\partial g_{jk}}{\partial x^i} = \frac{\partial g_{jk}}{\partial y^l}\frac{\partial f^l}{\partial x^i}$$ and $$\langle\tilde\nabla_{\partial/\partial x^i}e_j,e_k\rangle + \langle e_j,\tilde\nabla_{\partial/\partial x^i}e_k\rangle
=\tilde\Gamma_{ij}^l g_{lk} + \tilde\Gamma_{ik}^l g_{lj} = \Gamma_{mj}^l\frac{\partial f^m}{\partial x^i}g_{lk} + \Gamma_{mk}^l\frac{\partial f^m}{\partial x^i}g_{lj}
$$
but I don't see why these two expressions should be equal.","['riemannian-geometry', 'differential-geometry']"
2325864,Find the intervals on which $f(x)=x + \frac{1}{x}$ is monotonically increasing and decreasing,"Given $f(x) = x + \frac{1}{x}$. Graphically, the function looks like this: https://www.wolframalpha.com/input/?i=plot+y+%3D+x+%2B+1%2Fx . I have taken the first derivative and found that $f'(x)=1 - \frac{1}{x^{2}}$. I've also found the turning points of the function by setting $f'(x)=0$. They are $x=\pm 1$. I've also played with the signs of the first derivative, that is, figuring out when it's positive or negative. So are the intervals $(-\infty, -1) \cup (1, \infty)$ on which $f$ is increasing and $(-1, 0) \cup (0, 1)$ on which $f$ is decreasing?","['functions', 'proof-verification']"
2325870,A-Posteriori Probability,"I have a question to the following: Let $\lambda$ ~ $Exp(1)$ and for a given $\lambda$ the $X_1, \ldots, X_n$  are i.i.d. with $X_i \mid \lambda$ ~ $Exp(\lambda)$. What is the a-posteriori distribution of $\lambda$ and what is the a-posteriori probability for $\lambda \geq 2$ if $n = 5$ and the mean value of the $X_i$ is given by $1.2$? Ma idea was the following: The likelihood is given by: $$ l(\lambda \mid \underline{x}) = \prod_{i=1}^n \lambda \exp(-\lambda x_i) = \lambda^n\exp(-\lambda \sum_{i=1}^n x_i)$$ The a-priori is given by $\pi(\lambda)=\exp(-\lambda)$. Therefore the a-posteriori is given by: $$ \pi(\lambda \mid \underline{x}) \propto l(\lambda \mid \underline{x})\pi(\lambda) = \lambda^n \exp(-\lambda(\sum_{i=1}^n x_i+1))$$ Therefore $\pi(\lambda \mid \underline{x})$ ~ $Ga(n+1, 1+n\overline{x})$ where $\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i$. Now for the probabilty my idea was to calculate $$ P(\lambda \geq 2 \mid \underline{x}) = 1-P(\lambda \leq 1 \mid \underline{x}) = 1-\int_0^1 \frac{(1+5*1,2)^6}{\Gamma(6)}\lambda^5\exp(-\lambda(1+5*1,2)) d\lambda $$ but is this approach the correct one?","['bayes-theorem', 'probability-theory', 'statistics', 'probability', 'exponential-distribution']"
2325878,"Definition of a section of a vector bundle being in the Sobolev space $H^{k,r}(E)$.","This is taken from Jost's text: We let $E$ be a vector bundle over $M$, $s : M \to E$ be a section of $E$ with compact support. We say that $s$ is contained in the Sobolev space $H^{k,r}(E)$, if for any bundle atlas with the property that on compact sets all coordinates changes and all their derivatives are bounded, and for any bundle chart from such an atlas, $$\varphi : E\vert_U \to U \times \mathbb{R}^n$$ we have that $\varphi \circ s\vert_U$ is contained in $H^{k,r}(U)$. Two questions: Why do we require that the coordinate changes and all their derivatives are bounded? How does that have anything to do with $s$ being in the Sobolev space? By the definition of $\varphi$, wouldn't we have to require that $\varphi \circ s\vert_U \in H^{k,r}(U \times \mathbb{R}^n)$?","['riemannian-geometry', 'differential-geometry', 'sobolev-spaces']"
2325887,"Numbers of the form $8k^2-1$ in a sequence defined by $a_0=-1$, $a_1=1$ and $a_{n+2}=6a_{n+1}-a_n$.","Suppose a sequence $\{a_n\}_{n\in\mathbb{N}}$ is defined by $a_0=-1$, $a_1=1$ and $$a_{n+2}=6a_{n+1}-a_n$$ for all $n\in\mathbb{N}$. Find all $n$ such that $a_n$ is of the form $8k^2-1$ for some $k\in\mathbb{N}$. The problem comes from Bulgaria National Olympiad 2003, Problem 3 : Given the sequence $\{y_n\}_{n\in\mathbb{N}_+}$ defined by $y_1=y_2=1$ and
  $$y_{n+2}=(4k-5)y_{n+1}-y_n+4-2k,\qquad n\in\mathbb{N}_+.$$
  Find all $k\in\mathbb{Z}$ such that $y_n$ is a perfect square for all $n\in\mathbb{N}_+$. I attempted to solve this problem by using the fact that $y_2=2k-2$ and $y_3=8k^2-20k+3$ are perfect squares. Let $2k-2=(2u)^2$ and $8k^2-20k+3=v^2$, and we have negative Pell's equation $$(8u^2-1)^2-2v^2=-1.$$ The general solution to $x^2-2y^2=-1$ is given by $x_n+y_n\sqrt{2}=(1+\sqrt{2})^{2n+1}$, thus leading to my original question. Obviously, $a_0=-1$ and $a_2=7$ is of the form $8k^2-1$. How do I prove that these are the only solutions?","['number-theory', 'contest-math', 'pell-type-equations', 'elementary-number-theory']"
2325904,"Subgroups of $(\mathbb R, +)$ are either dense or cyclic.","I was trying to prove that any subgroup of $(\mathbb R, +)$ is either dense in $\mathbb R$ or is a cyclic subgroup of $(\mathbb R, +)$. Thanks in advance for any help.","['abstract-algebra', 'real-analysis', 'group-theory']"
2325915,Does there exist an infinite non-abelian group such that all of its proper subgroups become cyclic?,"We have simple examples of finite groups (that may be abelian or not) that are not cyclic but all their proper subgroups are cyclic (e.g. Klein's $4$ -group and $S_3$ respectively for abelian and non-abelian). In recent times, I have been able to produce a few examples of infinite abelian groups that are not cyclic but all their proper subgroups are cyclic. But currently, I am pondering whether the same can also be said for some infinite non-abelian group or not, precisely, does there exist an infinite non-abelian group such that all of its proper subgroups become cyclic? And if there do exist such groups, what can be an example? And if possible , it will be very much helpful if someone can give a general algorithm for constructing such a group.","['abstract-algebra', 'group-theory']"
2325925,"An ""isomorphism"" between continuous and discrete mathematics","First of all, I should inform everyone that I am not a mathematician and my question might sound not at all rigorous or maybe even absurd to many of you. But I have been thinking about this problem for some time and I decided to ask it here. I am an engineer and use math, mostly calculus and differential equations, to model physical systems. The problem is this: What if nature is ""discrete"" and the underlying continuity assumption of calculus renders it (or will render it in the future) impossible to model physical systems accurately? Not getting into what a physical system is, I think it would be possible to solve this problem purely mathematically if one can prove (or disprove) that a discrete mathematical system has inherent properties that a continuous mathematical system cannot map or vice versa. So, is that possible? Does this question even have a rigorous meaning? PS: I asked my mathematical friend this question and although he was very uncomfortable with my ""non-rigorous"" sequence of arguments, he did give a very interesting comment. He said that the essence of continuous mathematics, the only thing that sets it apart from discrete mathematics, is the ""completeness axiom"" which states that a bounded set must have a supremum. [For expample, the set $ (X: X^2 < 2)$ does not have a supremum if we stick to rational (discrete) numbers.] Is that true? It was this same friend that mentioned the word ""isomorphism"" to me which I interpreted to mean something along the lines of ""functionally similar"". Just as complex numbers are similar to 2-d vectors on a plane. So I took the audacity to put that word up on the question. Please be gentle in your criticism!","['continuity', 'soft-question', 'philosophy', 'discrete-mathematics']"
2326014,Contour integral around circle - Complex Analysis,"Exercise : Show that : $$\frac{1}{2πi}\oint_{|z|=r} \frac{e^{1/z}}{z-1}dz = \Bigg\{\begin{matrix}
1-e & r<1\\ 
1 & r >1
\end{matrix}
$$ I came across this exercise while I was studying for my complex analysis semester exams and I would really appreciate any guidance, tips or thorough solution. I know I am not providing an attempt (probably the only time I have done so in all my questions) but I can't grasp on how to work depending on the radius $R$. Most integrals I've solved can be worked around with the Theorem of Residues or the Trigonometric/General Integrals techniques, but I cannot seem to work on this one. Maybe one idea is to work through poles and singular points. I would really value a thorough explanation.","['complex-analysis', 'integration', 'contour-integration', 'residue-calculus']"
2326024,"Isometry group of $SL(2,\mathbb{R})$","What is the cleanest way to see that the $SL(2,\mathbb{R})$ group manifold has the isometry group $SL(2,\mathbb{R})_{L} \times SL(2,\mathbb{R})_{R}$? Please be gentle with your answers - I am a physicist by training! My main motivation behind this question is to understand the isometry group of $AdS_{3}$.","['general-relativity', 'isometry', 'group-theory', 'manifolds', 'differential-geometry']"
2326042,Suppose that $\lim_{x\to \infty} f'(x) = a$. Is it true that $\lim_{x\to \infty} {f(x)\over x} = a$,"Suppose that $\lim_{x\to \infty} f'(x) = a$. Is it true that $\lim_{x\to \infty} {f(x)\over x} = a$ If so, can you prove it? Thanks!",['limits']
2326082,Showing that all Kernel functions are symmetric,"I want to show that any Kernel function is symmetric, i.e. $K(x_i, x_j) = K(x_j, x_i)$.
I am not sure how I should begin, any tips will help, thanks.","['machine-learning', 'lagrange-multiplier', 'functions']"
2326098,An elegant proof that the pairity of a permutation is well-defined,"I want to explain to non-mathematicians a very nice proof that the 15-puzzle with 14 and 15 replaced is not solvable. For that, one crucial argument is the fact that: If we can write the same permutation as a product of transpositions in
  two different ways, then the pairity of the number of transpositions
  is the same. I know a standrad induction proof to this statement, but it's quite boring, technical and can't be properly explained to ""ordinary"" people, as all other proofs I know. I'm looking for a proof as elegant as one can find. Maybe a proof using coloring of some kind (it feels to me like there must be one), or visual graph theory, or  binary arithmetic, etc. I'm curious to hear your answers!","['permutations', 'alternative-proof', 'discrete-mathematics']"
2326103,What is the name of this class of (combinatorial?) problems?,"Judging from the number of similar questions, I've found myself in a rather common situation: I've come up with a problem, encountered a dead end and am now searching for the name of the problem in order to learn more about it. An example of the problem as follows: Suppose we have a number of strings of ones and zeroes. For example: $s_1: \fbox{0}\fbox{1}\fbox{0}\fbox{1}\fbox{0}$ $s_2: \fbox{0}\fbox{0}\fbox{1}\fbox{0}\fbox{1}\fbox{0}\fbox{0}$ $s_3: \fbox{0}\fbox{0}\fbox{0}\fbox{1}\fbox{0}\fbox{0}\fbox{0}\fbox{1}\fbox{0}\fbox{0}\fbox{0}$ Now we construct a new string $H$ by horizontally shifting each sequence $s_n$ by some amount $t_n$ and applying logical disjunction to the columns. What is the longest possible sequence of consecutive ones in the resulting string, if we're allowed to choose $t_n$? By exhaustive search, a solution to this particular problem would be 5, which is given by $t_1=2$; $t_2=2$; $t_3=0$ : $s_1: \phantom{0000}\fbox{0}\fbox{1}\fbox{0}\fbox{1}\fbox{0}$ $s_2: \phantom{0000}\fbox{0}\fbox{0}\fbox{1}\fbox{0}\fbox{1}\fbox{0}\fbox{0}$ $s_3: \fbox{0}\fbox{0}\fbox{0}\fbox{1}\fbox{0}\fbox{0}\fbox{0}\fbox{1}\fbox{0}\fbox{0}\fbox{0}$ $H: \fbox{0}\fbox{0}\fbox{0}\fbox{1}\fbox{1}\fbox{1}\fbox{1}\fbox{1}\fbox{0}\fbox{0}\fbox{0}\fbox{0}$ Clarification: Shorter strings can extend past the longer ones, as long as the final string has no gaps. In principle, if all strings contained only $1$'s then the maximum stretch would be just $\sum_{i=1}^{n}{\lvert s_i \rvert}$, where $\lvert s_i \rvert$ is the length of $i$-th string. The closest I could find is the Restricted Strip Covering problem, which is a special case of Geometric Set Covering, but it's not quite what I'm looking for. Progress: The problem can be stated in terms of discrete job scheduling problem: Suppose we have a number of employees who can work within a certain time span. Within that time span they may take certain gaps for leisure. Given a set of such employees, what is the maximum stretch of time in which at least one employee is working? The distinction between this problem and job-scheduling problem is important though - finding the optimal values for $t_n$ is NP-Hard, but maybe it's possible to calculate the maximum time without calculating $t_n$ in polynomial time.","['optimization', 'combinatorics', 'discrete-optimization', 'reference-request']"
2326107,Are these two functions numerically stable?,"Given are two (identical) functions $$f(x)=\sqrt{x^2+1}-1 \\ g(x)=
\frac{x^2}{\sqrt{x^2+1}+1}$$ Are the calculation specifications for $f$ and $g$ numerically stable? I have absolutely no idea how this could be solved and I might need to know that in the exam : / I have read that something is stable if there aren't round-off errors that sum up and therefor give a bad solution. In other words, if something (in this case the functions) is insensitive towards a little disorder / change. That's how I understood it. But how can this be checked for these two functions? Shall I manually choose some very small values, insert them into the function. Then do little changes with the inserted values, then compare both solutions? If big changes $\rightarrow$ numerically unstable, else numerically stable. Isn't there are better way to do this? If not, would my cheap idea work?","['numerical-methods', 'numerical-linear-algebra', 'rounding-error', 'functions']"
2326134,"showing that a multivariate function is not continuous at the origin, although Wolfram Alpha says it is","I am being asked to show that the following function is NOT continuous at the origin. $f(x,y) =\begin{cases}\frac{x^4y^5}{x^8 + y^{10}} &&(x,y) \not= (0,0) \\0 &&(x,y) = (0,0) \end{cases}$ Thus, I've tried to show that the following limit does not equal to zero, with different paths $y=mx, y=x^2$, and so forth, but without success. $\lim_{(x,y) \to (0,0)} \frac{x^4y^5}{x^8 + y^{10}}$ Eventually I checked with Wolfram Alpha, and it computed that the limit is indeed 0, as I have suspected, so I figured there might be some mistake in the exercise... In fact, the whole exercise is to show that even though the function is not continuous at the origin, its partial derivatives exist. 
I have had several exercises like this, where Wolfram contradicted what I was asked to prove. And I know that Wolfram has been wrong before, so I would appreciate some help in the matter - Is Wolfram wrong, or the exercise? Thanks in advance.","['multivariable-calculus', 'calculus', 'limits']"
2326138,What is the rigorous justification for treating differentials as fractions while solving differential equations? [duplicate],"This question already has answers here : What am I doing when I separate the variables of a differential equation? (5 answers) Closed 7 years ago . For example, given the differential equation
$\dfrac{\mathrm{d}y}{\mathrm{d}x}=y\tag*{}$
We do,
$\dfrac{\mathrm{d}y}{y}=\mathrm{d}x\tag{*}$
At which point we go and integrate both sides.
Is there a rigorous justification for being able to go to $(*)?$","['ordinary-differential-equations', 'calculus']"
2326139,An equality involving trigonometry,I am not able to prove the following equality: $$\frac{\sin12°\sin48°\sin18°}{\sin84°\sin18°+\sin12°\sin 48°\cos18°}=\tan6°$$ Please help. Thanks!,"['trigonometry', 'fractions']"
2326149,Arranging 12 men and 12 women into mixed groups of three.,"Suppose there is a group of 24 people, 12 men and 12 women. I’d like to arrange them into groups of three, so that each group contains at least one man and at least one woman. How many ways are there to do this? Here is how I thought one might approach the problem: Since there will be 8 groups of three, first let’s choose 8 women. Then we’ll choose 8 men to ‘pair up’ with the 8 women. This will leave 4 men and 4 women, which we’ll distribute randomly onto the pairs. So, formally, this amounts to:
$${12 \choose 8}\cdot{12 \choose 8}\cdot8!\cdot8!$$ (The first 8! is for pairing the 8 men with the 8 women. The second 8! is for distributing the 8 remaining people onto the pairs.) The problem is that we are counting some triples more than once. E.g., the method yields the triple ‘Jill-Jack-Mary’, and also ‘Mary-Jack-Jill’. But they are the same triple, of course. Note that the process does NOT yield any of the following: ‘Jack-Jill-Mary’, ‘Jack-Mary-Jill’, ‘Jill-Mary-Jack’, ‘Mary-Jill-Jack’. That’s because the process picks a woman first and then a man (and then either a man or a woman). So, although we are arranging into triples, I think we need to divide by 2, and not by 3!, to compensate for the over-counting.
That gives us: $$\frac{{12 \choose 8}\cdot{12 \choose 8}\cdot8!\cdot8!}{2}$$ Am I at all close to the right answer?
Thanks!",['combinatorics']
2326164,Distance between two partitions of a set,"I want to compare two different partitions of a set consisting of $n$ elements: ${1,2,3,...,n}$. How can I count the number of elements that are in different subsets when comparing partition 1 with partition 2? Examples with $n=6$ Partition 1: $\{\{1,2,3\},\{4,5,6\}\}$, partition 2: $\{\{1,2,4\},\{3,5,6\}\}$ should yield $2$ since the elements $3$ and $4$ are in different subset. Partition 1: $\{\{1,2,3\},\{4,5,6\}\}$, partition 2: $\{\{1,2\},\{3\},\{4,5,6\}\}$ should yield $1$ since the element $3$ is in a different subset. Partition 1: $\{\{1,2,3\},\{4,5,6\}\}$, partition 2: $\{\{1,2,3,4,5,6\}\}$ should yield $3$ since the elements $4,5,6$ are in different subsets. Background:
I want to compare two different community partitions of a network. I would like to know how many nodes I have to ""move"" to a different community to get from one partition to the other. In this picture you can see a possible community partition (the communities are color-coded, i.e. nodes with the same color belong to the same community).","['set-partition', 'elementary-set-theory']"
2326215,A conjecture about primes,"Conjecture: For each prime $p$ there are an infinite number of primes $q$ such
  that $p+q$ is a perfect square. I have done a lot of tests using Bigz and I believe that it's possible to prove. I change from possible to prove to not possible to disprove. Define for all primes $q$, 
$w_q(n)=|\{p\in\mathbb P|p<n\wedge \exists k\in\mathbb N:p+q=k^2\}|$. Then I've observed that it seems that for all primes $q$ there exists a natural number $N$ and a $\alpha\in\mathbb R_+$ such that $m>N$ implies that $w_q(10^m)>\alpha2^m$. This would imply the conjecture and is maybe possible to disprove.","['number-theory', 'conjectures', 'perfect-powers', 'prime-numbers']"
2326219,How do you evaluate high powers in modulo expressions?,"How would you evaluate $ 2^{2143} \pmod{17}?$ For instance, I was looking through my notes and the example I have is :
 $5^{131} \pmod{14}$ We look at the powers $ 5^i$  modulo $14$ and notice that they repeat at $i = 6$ Now since $131 = 5 \pmod{6}$ then $5^{131} = 5^5 = 3 \pmod{14}$ The answer is $3$. However, I do not understand this logic at all.",['discrete-mathematics']
2326226,Solving differential equations by parametrization,"In my book they solve the following two first-order differential equations by parametrization. 1) $x^4=y'^3-x^2y'$ 2) $y=y'^2+2lny'$ Notice that there is no y in the first equation and no x in the second equation.
That's the signal that tells me to use parametrisation. Now for the first one they use $t=\frac{y'}{x}$ and for the second one they use $ t = y' $. How do I know which parametrization I should use? Thanks in advance!",['ordinary-differential-equations']
2326237,A way to solve the gap between prime numbers,"Some time ago I found this sum of prime numbers converge(*) $$\sum_{k=1}^\infty  \frac{p(k+1)-2p(k+2)+p(k+3)}{p(k)-p(k+1)+p(k+2)}\ \approx \frac{5}{7}\zeta(3)^{-2}
$$
where: $\zeta(s)$ is the Riemann zeta function $p(n)$ is the $n^{th}$  prime number Working a bit on I found that: 
$$\sum_{k=1}^n  \frac{p(k+1)-p(k+2)+p(k+3)-p(k+4)+p(k+5)}{p(k)-p(k+1)+p(k+2)-p(k+3)+p(k+5)}\ \approx n
$$ this means that $$F(k)=\frac{p(k+1)-p(k+2)+p(k+3)-p(k+4)+p(k+5)}{p(k)-p(k+1)+p(k+2)-p(k+3)+p(k+5)}\approx 1$$ $F(k):$ $F(k)=1 \pm \varepsilon_{n}$ $p(k+1)-p(k+2)+p(k+3)-p(k+4)+p(k+5)=p(k) \pm \varepsilon_{n}p(k)-p(k+1)\pm \varepsilon_{n}p(k+1)+p(k+2)\pm \varepsilon_{n}p(k+2)-p(k+3)\pm \varepsilon_{n}p(k+3)+p(k+5)\pm \varepsilon_{n}p(k+5).$ $E_k=\pm\varepsilon_n(p(k)+p(k+1)+p(k+2)+p(k+3)+p(k+5))$ $ \ $ $$p(k+4)=2p(k+3)-2p(k+2)+2p(k+1)-p(k)+E_{k}$$ $E_{k}:$ In this way you can look for the next prime number taking into account the error $E_{k}$. How can I calculate the range of $E_{k}$? I do not know if this way has already been used by someone, I had the pleasure of sharing it because of my lack of mathematical knowledge I would not know how to go on (*) Does this sum of prime numbers converge?","['number-theory', 'prime-gaps', 'prime-numbers', 'riemann-sum']"
2326259,"Prove that ideal $I = \langle X^2,X+1\rangle$ is principal or not (in $\mathbb{Z}[X]$ and $\mathbb{Q}[X]$ )","I tried the following $$I = \langle X^2,X+1\rangle =\langle X^2,X+1,X^2+2(X+1)\rangle =\langle X^2,X+1,(X+1)^2+1 \rangle$$ Yet no matter how I arrange it, I cannot obtain $1$. Can someone help me out?","['abstract-algebra', 'ideals']"
2326301,Divergence theorem application.,"I need to prove that $$\lim_{r \to 0} \frac 1 {\operatorname{vol}(B_r(p))} \iint_{\partial B_r(p) } f \, dA = \operatorname{div}(f) $$ We have by the Divergence Theorem that $\dfrac 1 {\operatorname{vol}(B_r(p))} \iint_{\partial B_r(p)} f \, dA  =\dfrac 1 {\operatorname{vol}(B_r(p)} \iiint_{B_r(p)} \nabla f \, dV $ where $\nabla f = \dfrac{\partial f}{\partial x} +\dfrac{\partial f}{\partial y} +\dfrac{\partial f}{\partial z}$. Im not sure how to continue from here, I tried working with Spherical Coordinates but I can't compute this triple integral.","['multivariable-calculus', 'divergence-operator']"
2326355,"$8\sin x\cos x-\sqrt{6} \sin x- \sqrt{6} \cos x+1=0$, Solve for $x$","Solve for $x:0\leq x \leq \frac{\pi}{2}$ $$8\sin x\cos x-\sqrt{6} \sin x- \sqrt{6} \cos x+1=0$$ My attempt, I changed it into $$1+4 \sin 2x-2\sqrt{3} \sin (x+\frac{\pi}{4})=0$$","['algebra-precalculus', 'substitution', 'trigonometry', 'quadratics']"
2326378,Function question $f(x+1)$ and $f(x)$,"Given that $f(x+1)-f(x)=4x+5$, $f(0)=6$. Find $f(x)$. My attempt, $f(1)-6=5$, $f(1)=11$ How to proceed then? I've never solve this kind of question before.","['algebra-precalculus', 'functions']"
2326419,Prove that each divisor of $5^{(4n+1)}+5^{(3n+1)}+1 =$ $1 \pmod {10}$,"Prove that if $d$ divides $5^{(4n+1)}+5^{(3n+1)}+1$ for any $n$, then $d =$ $1 \pmod {10}$. A similar statement can be proven, where $d$ divides $(3^{(2n+1)}+1)/4$, for any $n$, then $d =$ $1 \pmod {6}$ by first showing that if $-3$ is a quadratic residue $\pmod p$, then $p = 1 \pmod 3$. For the base $5$ expression I would start by showing that if $-5$ is a quartic residue $\pmod p$, then $p = 1 \pmod {10}$. Any  help on proving the original statement with base $5$ expression? Thanks!!!","['number-theory', 'quadratic-residues', 'elementary-number-theory']"
2326434,"Increment of a functionnal, derivative function","Consider a path $x:[0;T] \rightarrow \mathbb{R}^2$ such that 
$x(0)=x(T)$ and the dynamic is given by $\dot x = f(x)$. Considering some variations on the intial state $x(0)$ and on the final time $T$, I would like to analyze the equation $x(0)=x(T)$. The answer is 
$$
\delta x(0) = d x(T) = \delta x(T) + f(x(T))\,d T.
$$
which I don't understand. The increment of $x$ at time $0$ and $T$ wrt $x(0)$ and $T$ is
$$
 \Delta (x(0),\delta x(0),dT) = \Delta (x(T),\delta x(0),dT). \qquad (*)
$$ But $\delta x(0)$ is the linear approximation of $\Delta x(0)$:
$$
\Delta (x(0),\delta x(0),dT) = \delta (x(0),\delta x(0),dT) + H.O.T
$$
so taking only the first order term in $(*)$ 
$$
\delta (x(0),\delta x(0),dT) = \delta (x(T),\delta x(0),dT)
$$
i.e.
$$
\underbrace{\delta (x(0),\delta x(0))}_{\delta x(0)}  =^? \delta (x(T),x(0))+\delta (x(T),dT)=\delta x(T) + dx(T) = \delta x(T) +  f(x(T))dT
$$ but it doesn't yield the result First, why  $dx(T) = \delta x(T) + f(x(T))\,dT$? This is taken from the article (see equation (9) ) : [article][1] http://sci-hub.io/10.1109/TAC.1984.1103482","['functional-analysis', 'analysis', 'derivatives']"
2326447,Convergence in probability and subsequence,"Let $\mathrm{X}$ and $\mathrm{(X_n)_{n\in\mathbb{N}}}$ be a sequence of random variables such that $\mathrm{X_n}$ converges to $\mathrm{X}$ in probability. Prove that there is a subsequence $\mathcal({n_k})_{k\in\mathbb{N}}$ such that $\mathrm{X_{n_{k}}}$ converges almost surely to $\mathrm{X}$. My thoughts so far: If I let $\mathcal{n_k}$ be such that $\forall$ $\epsilon$ $\gt$ 0: P($\vert$$\mathrm{X_{n_{k}}}$ - $\mathrm{X}$$\vert$$\ge$$\epsilon$)$\le$$\frac{1}{k^2}$, then the assumption would hold with the following lemma: If $\sum_{n=1}^\infty$P($\vert$$\mathrm{X_n}$-$\mathrm{X}$$\vert$$\gt$$\epsilon$)$\lt$$\infty$ for all $\epsilon$$\gt$0, then $\mathrm{X_n}$ converges to $\mathrm{X}$ almost surely. For some reason I am not entirely convinced by this. Is there another way to prove the assumption.","['almost-everywhere', 'probability-theory', 'convergence-divergence']"
2326464,A regular surface is of measure zero,"Recall first the definition of a regular surface: A subset $S\subseteq\mathbb{R}^3$ is called a regular surface if for every point $p\in S$, there is an open ball $B_r(p)$ around $p$ and an open set $A\subseteq\mathbb{R}^2$ and a continuously differentiable function $f:A\to S\cap B_r(p)$ such that $rank(Df) = 2$ and $f$ is surjective. To my understanding, the notion that captures the fact that surfaces are '2 dimensional' in some intuitive sense, is the fact that $rank(Df) = 2$, and that $A\subseteq \mathbb{R}^2$, so in some intuitive sense, the image of $f$ cannot be '3 dimensional'. Given that, it should be quite intuitive that regular surfaces must be of measure zero ('2 dimensional' object in 3 dimensional world). I have had many failed attempts, including attempts to construct some Lipschitz function and to find some set of measure zero that will map to the surface (since Lipschitz functions preserve zero measure). Although intuitive, it seems non-trivial.","['lebesgue-measure', 'differential-geometry', 'surfaces']"
2326505,Proof/Derivation of Closed form of Binomial Expression $\sum\limits_{k=0}^{2n}(-1)^k\binom{2n}{k}^2$,"The binomial expression given as follows:
$$\sum_{k=0}^{2n}\left(-1\right)^{k}\binom{2n}{k}^{2}$$
results nicely into the following closed form:
$$(-1)^{n}\binom{2n}{n}$$
I wish to know how exactly is it being done? I haven't been able to make much progress in solving it.
My approach:
\begin{align}
\sum_{k=0}^{2n}(-1)^{k}\binom{2n}{k}^{2} = \binom{2n}{0}^{2} - \binom{2n}{1}^{2} + \binom{2n}{2}^{2} - ... -\binom{2n}{2n-1}^{2} + \binom{2n}{2n}^{2} \\
= \binom{2n}{0}.\binom{2n}{0} - \binom{2n}{1}.\binom{2n}{1} + \binom{2n}{2}.\binom{2n}{2} - ... -\binom{2n}{2n-1}.\binom{2n}{2n-1} + \binom{2n}{2n}.\binom{2n}{2n} \\
\text{By Symmetry of binomial coefficients} \\
= \binom{2n}{2n}.\binom{2n}{0} - \binom{2n}{2n -1}.\binom{2n}{1} + \binom{2n}{2n-2}.\binom{2n}{2} - ... -\binom{2n}{1}.\binom{2n}{2n-1} + \binom{2n}{0}.\binom{2n}{2n} \\
= \binom{2n}{2n}.\binom{2n}{0} + \binom{2n}{2n -1}.\binom{2n}{1} + \binom{2n}{2n-2}.\binom{2n}{2} + ... +\binom{2n}{1}.\binom{2n}{2n-1} + \binom{2n}{0}.\binom{2n}{2n} - 2.\left(\binom{2n}{2n -1}.\binom{2n}{1} + \binom{2n}{2n -3}.\binom{2n}{3} + ... + \binom{2n}{1}.\binom{2n}{2n-1}\right) \\
\text{By Vandermond's identity, first component, i.e.not enclosed within -2.(...) evaluates to C(4n, 2n)} \\
\binom{4n}{2n} - 2.(\binom{2n}{2n -1}.\binom{2n}{1} + \binom{2n}{2n -3}.\binom{2n}{3} + ... + \binom{2n}{1}.\binom{2n}{2n-1})
\end{align}
I'm lost beyond this point. It will be extremely helpful if someone can direct me in the right direction or provide the answer to this perplexing and challenging problem. Thank you.","['combinatorics', 'proof-writing', 'binomial-coefficients', 'closed-form']"
2326518,Why is this a safe neighborhood for uniform convergence of the sequence implicitly defining a function?,"This discussion pertains to Theorem III-1.4 in C.H. Edwards, Jr.'s Advanced Calculus of Several Variables. I am trying to understand why the neighborhood described in the proof
of the implicit function theorem is ""safe"" for convergence of
the contraction mapping with the inductive hypothesis for the sequence
: $f_{n+1}[x]=f_{n}[x]-\frac{G[x,f_{n}[x]]}{D_{2}G[a,b]}$, $f_{0}[x]=b$. The neighborhood is given as follows: $\varepsilon\ge\lvert x-a\rvert\bigwedge\varepsilon\ge\lvert y-b\rvert\implies\lvert D_{2}G[x,y]-D_{2}G[a,b]\rvert\le\frac{1}{2}\lvert D_{2}G[a,b]\rvert$ $\varepsilon>\delta\ge\lvert x-a\rvert\implies\lvert G[x,b]\rvert\le\frac{1}{2}\varepsilon\lvert D_{2}G[a,b]\rvert$ The graph depicts the trace on the surface $\{x,y,G[x,y]\}$ of the
square $[a-\varepsilon,a+\varepsilon]\times[b-\varepsilon,b+\varepsilon]$
in green. The further $\delta$ restriction is indicated in magenta, as is the
trace of the line $y=b$. The trace of $x=a$ is blue. The white angle represents the projection of the point $\{a,b+\frac{\varepsilon}{2},\frac{\varepsilon}{2}D_{2}G[a,b]\}$. $\frac{\varepsilon}{2}D_{2}G[a,b]$ also gives the height of the upper
plane. The actual solution curve $\{x,f[x],0\}$ is the intersection of the
lower plane and the surface $\{x,y,G[x,y]\}$. The red parallelogram represents the condition $G[a,y]=2(y-b)D_{2}G[a,y]$
which leads to an infinite loop. The choice of $\varepsilon$ prevents
that from happening on the blue curve. This is explained in my answer to a previous question: Neighborhoods necessary for convergence of a sequence. What I am trying to determine is why the rectangle $[a-\delta,a+\delta]\times[b-\varepsilon,b+\varepsilon]$
is safe for convergence for all $x\in[a-\delta,a+\delta]$. In other
words, how it avoids the infinite loop condition. A second diagram shows a simpler depiction of the problem using a different $G[x,y]$. Edit to add: I failed to mention a potentially significant assertion regarding the second image.  The rectangle in that image was chosen so that the trace of the line segment $G[{c_1,d_1},{c_2,d_1}]$ is completely below the plane $z=0$, and $G[{c_1,d_2},{c_2,d_2}]$ is completely above the surface of $z=0$. $D_2[x,y]>0$ holds everywhere in the rectangle $[c_1,c_2]\times[d_1,d_2]$. The objective is to show that $f_{n+1}[x_*]=f_{n}[x_*]-\frac{G[x_*,f_{n}[x_*]]}{D_{2}G[a,b]}$ converges to a point on the solution curve for all $x_*$ in some neighborhood within the rectangle $[c_1,c_2]\times[d_1,d_2]$.  The neighborhood described above would lie within that rectangle. Edit to add: Edit to modify: I observe that the portions of the green curve with constant $x$ never intersect the plane $z=0$ in the neighborhood, so it is right to exclude them.  Though the magenta curves do intersect the solution curve, that is not a guarantee that they would do so with a different $G[x,y]$. Edit to add: I also note that the hypotheses of the theorem only stipulate $G$ is $\mathscr{C}^1$. That makes talking about inflection points difficult.  But they seem relevant. It seems proper to examine what happens at the extreme conditions such as: $D_2G[x_*,y]=\frac{1}{2}D_2G[a,b]$, $D_2G[x_*,y]=\frac{3}{2}D_2G[a,b]$ and $\lvert D_{2}G[x,y]-D_{2}G[a,b]\rvert=\frac{1}{2}\lvert D_{2}G[a,b]\rvert$ I've removed my own speculations about those conditions because I believe they were too restrictive (IOW, wrong). Edit to add: Another observation that seems significant is that all sequences begin with the base state $f_0[x]=b$. I'm pretty sure that means all subsequent $f_n[x]$ will be on the same side of the plane $y=b$ as $f_1[x]$. Unfortunately I don't have a good exact vocabulary, nor theory of this problem domain.  IOW, I'm hand-waving it.  All I'm seeking at this point is an intuitively satisfying geometric explanation. Edit to add: A third graphic showing a ""side view"" of the situation, with the $x$ dimension suppressed.  By collapsing the problem into the consideration of a family of curves in two dimensions which adhere to the stated conditions, the problem might reduce to one of real analysis. The yellow lines depict the range of allowable slopes for any curve in the neighborhood, determined by
$\varepsilon\ge\lvert x-a\rvert\bigwedge\varepsilon\ge\lvert y-b\rvert\implies\lvert D_{2}G[x,y]-D_{2}G[a,b]\rvert\le\frac{1}{2}\lvert D_{2}G[a,b]\rvert$ The blue curve and tangent line represent a safe state.  The red parallelogram and chord depict a loop state and the slope of twice the loop tangent, respectively.  The green vertical lines represent $y=\pm\varepsilon$.  The vertical magenta line segment represents the restriction: $\varepsilon>\delta\ge\lvert x-a\rvert\implies\lvert G[x,b]\rvert\le\frac{1}{2}\varepsilon\lvert D_{2}G[a,b]\rvert$. That is, (I believe) every 2-dimensional projection of a curve must intersect the magenta line segment.  Furthermore, the base term of every sequence is $f_0[x]=b$.  The first iteration is therefore: 
$f_{1}[x]=b-\frac{G[x,b]}{D_{2}G[a,b]}$.","['real-analysis', 'implicit-function-theorem', 'differential-topology', 'multivariable-calculus', 'sequences-and-series']"
2326522,"Proof or counterexample: Let $A$ be a square matrix, then:","If $A$ is diagonalizable, then so is $A^2$ I answered yes. I argued that since $A$ is diagonalizable there exists an eigenbasis, and since $A^2$ has the same eigenvectors than $A$, and its eigenvalues are those of $A$ squared, there is also an eigenbasis for $A^2$, so it is diagonalizable If $A^2$ is diagonalizable, then so is $A$ I am pretty sure the answer is no, but I can't think of a counterexample. Thank you in advance","['eigenvalues-eigenvectors', 'linear-algebra']"
2326580,Question about the proof of the maximum modulus principle,"Let $G \subseteq \mathbb{C}$ be open and connected and $f$ be
  holomorphic in  $G$. If $|f|$ has a local maximum in some point $z_0
 \in G$, then $f$ is constant on $G$. Proof. Let $U \subseteq G$ be a neighbourhood of $z_0$ (according to the terminology in the book not necessarily open, it just contains an open ball around $z_0$) such that $$|f(z)| \leq |f(z_0)|$$ holds for any $z \in U$. Then we have that $$f(U) \subseteq \overline{B}_{|f(z_0)|}(0)$$ Now  $f(U)$ is not a neighbourhood of $f(z_0)$. The author wants to apply the following theorem: Let $f$ be a nonconstant holomorphic function on an open and connected subset of $\mathbb{C}$. Then also the image of this subset is open and connected. Now the author concludes from the above, that $f(z) \equiv f(z_0)$ in a neighbourhood of $z_0$. Why is this so?",['complex-analysis']
2326607,Normal bundle of exceptional divisor,"I'm trying to understand the conifold singularity from this post It says the normal (or maybe the conormal?) bundle to $P^1$ is the kernel of the map
$$
O^4 \to O(1) \oplus O(1) \oplus O
$$
given by the matrix given by partial derivatives of the equations with respect to $x,y,z,w$ at $x = y = z = w = 0$. Can anyone explain explicitly how to get this? ------------------------update---------------------- I've quickly browsed Fulton's book Riemann-Roch Algebra. I guess the basic idea here is: If $i:X \to Y$ is smooth, then $C_{Y/X} \to i^*\Omega_{X/Y}$ is an isomorphism by lemma 3.8 of Fulton's book. Therefore, the exact sequence of the conormal sheaf can be viewed as the relative cotangent exact sequence. Then the above map is given by the partial derivative because the map between tangent spaces is given by Jacobian matrix and the dual map is the transpose. Can anyone confirm this and maybe elaborate a little bit more?",['algebraic-geometry']
2326625,How to solve $x^x+1=y^2$ in positive integers?,"For integers $x, y>0$ solve the following Equation:
  $$x^x+1=y^2$$ Well, $x$ must be odd. Because for $x=2z$ we get difference of two squares as $1$, which is impossible in positive integers. We have $$x^x=(y-1)(y+1).$$ Thus for every prime divisor of $x$ like $p$, if $v_p(x)=k$, then $y=p^{kx}z \pm 1$. Where $\gcd(p, z)=1$. I am not sure how to use this! Any ideas? Edit: Mihăilescu's theorem will do this easily. I am interested in an alternative elementary method. If you can think of one please let me know.","['number-theory', 'diophantine-equations', 'elementary-number-theory']"
2326651,"Proof about sequences in metric spaces, Hilbert's cube","I am having trouble convincing myself that the following proof is correct. The question is from the exercies in a real analysis book and the section is on sequences in metric spaces. Background: Let $(H^\infty,d)$ be a metric space such that $H^\infty \subset \mathbb{R}^\infty$ and if $\{a_n\}_{n=1}^{\infty} \in H^\infty$, then $|a_n| \leq 1$ for all $n \in \mathbb{N}$. In other words, $H^\infty$ is ""Hilbert's cube."" The metric $d$ in this case is defined by $d(a,b)=\sum_{n=1}^{\infty}\frac{|a_n-b_n|}{2^n}$. Problem: Prove that if $\{a^k\}_{k=0}^\infty$ is a sequence of points in $H^\infty$ which converges to $a=\{a_n\}_{n=1}^{\infty} \in H^\infty$, then $\lim_{k \to \infty} a_j^k = a_j$ for all $j \in \mathbb{N}$. Proof: Let $j\in \mathbb{N}$ and $\epsilon > 0$. Since $\lim_{k \to \infty} \{a^k\}=a,$ there exists $N \in \mathbb{N}$ such that $k \geq N$ implies that $$d(a^k,a) = \sum_{n=1}^{\infty}\frac{|a^k_n-a_n|}{2^n} < \frac{\epsilon}{2^j}.$$ We also have $$\frac{|a^k_j-a_j|}{2^j} < \sum_{n=1}^{\infty}\frac{|a^k_n-a_n|}{2^n} < \frac{\epsilon}{2^j},$$ which implies that $|a^k_j-a_j| < \epsilon$, so $\lim_{k \to \infty} a_j^k = a_j$ for all $j \in \mathbb{N}$. My confusion: I don't feel good about the fact that $N$ depends on $j$... but maybe this is OK. How does the proof look?","['real-analysis', 'metric-spaces', 'sequences-and-series', 'limits']"
2326658,A wrong proof for an (evident) lemma,"( Eliashberg, Y.; Mishachev, N.M. , Wrinkling of smooth mappings and its applications. I , Invent. Math. 130, No.2, 345-369 (1997). ZBL0896.58010 . \cite{EM}) Let $ \alpha  : [a, b] \to \mathbb{R}$  is a 1-dimensional wrinkle. Then for
  any positive numbers $s_1, \dots , s_l $ such that  $ s:= \sum\nolimits_{1}^{l} s_i \ge
 {\text{span} \left(\alpha \right)} $, there exists a wrinkled map
  $\beta : [a, b] \to \mathbb{R}$ which is $\left( s - \text{span}
 \left(\alpha \right) \right)$-close to $\alpha$ in $C^0$-norm,
  coincides with $\alpha$ near the end-points of the interval $[a,b],$ 
  and which has exactly $l$ wrinkles whose spans are equal to $s_1,
 \dots , s_l$. I prove for the case $l=2$. By hypothesis, $\alpha $ is a cubic function with two critical points $c, d \in  \left[ a, b \right] , a< c<d<b$, such that $c$ is the maximum and $d$ is the minimum. 
For convenience, we assume $\alpha (d)=0$. It implies that $\alpha (c) =\text{span}(\alpha)$. Let $\varepsilon := s_1+s_2 -  \text{span} \left(\alpha \right)$. 
We have to prove that there is a wrinkled map $\beta : [a, b] \to \mathbb{R}$ such that $\beta$ is $\varepsilon$-close to $\alpha$ in $C^0$-norm, coincides with $\alpha$ near $a, b$, and has $2$ wrinkles $\alpha_1$ and $\alpha_2$ whose spans are equal to $s_1, s_2.$ The required function $\beta$ is a wrinkled map, i.e., by definition, there are disjoint subsets $[a_1, e_1]$ and $[e_2, b_1]$, where $a \le a_1 \le e_1  \le e_2 \le b_1 \le b$, of the interval $[a,b]$ such that over them, $\beta$ are wrinkles, which we denoted $\alpha_1, \alpha_2$; and outside them, $\beta$ is a submersion. For the points $a_1 \le e_1  \le e_2 \le b_1 $ in $[a,b]$ will be chosen later, we can define $\beta$ by
$$ \beta :=
\begin{cases} 
 \text{a smooth path connected } a \text{ and } a_1 \text{ which is a  submersion};\\
 \alpha_1 \text{ is a one-dimensional wrinkle on } [a_1, e_1];\\
 \text{a smooth path connected } e_1 \text{ and } e_2 \text{ which is a  submersion};\\ 
 \alpha_2 \text{ is a one-dimensional wrinkle on } [e_2, b_1]; \\
 \text{a smooth path connected } b_1 \text{ and } b \text{ which is a  submersion}.
\end{cases}$$
The required functions $\alpha_1, \alpha_2$ are cubic,
hence it suffices to indicate four independent equations for each $\alpha_i, i=1, 2$. If we suppose $\alpha_1$ reaches maximum at $c$ and $\alpha_2$ reaches minimum at $d$, we would get two equations for each $\alpha_i, i=1,2$. 
$$ \begin{cases} 
 \alpha_1 (c) := t_1;\\
 \alpha_1^\prime (c) = 0;
\end{cases} 
\text{ and }
 \begin{cases} 
\alpha_2 (d) := t_2;\\
 \alpha_2^\prime (d) = 0;
\end{cases} 
$$
where the values $t_1$ and $ t_2$ will be chosen later, such that the condition that $\beta$ is $\varepsilon$-close to $\alpha$ in $C^0$-norm holds, i.e., 
\begin{equation}\label{chopeq232} \begin{cases}
\Vert \alpha - \alpha_1 \Vert_{C^0[a_1, e_1]} = \mathop{\text{max}}\limits_{x \in [a_1, e_1]} \vert \left( \alpha - \alpha_1 \right)(x)  \vert < \varepsilon; \\
\Vert \alpha - \alpha_2 \Vert_{C^0[e_2, b_1]} = \mathop{\text{max}}\limits_{x \in [e_2, b_1]} \vert \left( \alpha - \alpha_2 \right)(x)  \vert < \varepsilon. 
\end{cases}  \end{equation} 
The point $c$ is the critical point of both functions $\alpha $ and $ \alpha_1$ on $[a_1, e_1]$, so the derivative $$ \left( \alpha - \alpha_1 \right)^\prime (c) = \alpha^\prime (c)  - \alpha_1^\prime (c) = 0.$$ 
Therefore $c$ is one of two critical points of $\alpha - \alpha_1 $  on $[a_1, e_1]$. We could choose, for example, 
$$t_1:= \alpha(c)+ \varepsilon = \text{span}(\alpha)+\varepsilon,$$
then $ \vert \left( \alpha - \alpha_1 \right)(c)  \vert = \varepsilon.$ 
Similarly $d$ is one of two critical points of $\alpha - \alpha_2 $  on  $[e_2,b_1]$. We could choose, 
$$t_2:= \alpha(d) - \varepsilon =  -\varepsilon,$$ 
then $ \vert \left( \alpha - \alpha_2 \right)(d)  \vert = \varepsilon.$
Now we want to find the other extremum of $\alpha - \alpha_1 $  on $[a_1, e_1]$ and of $\alpha - \alpha_2 $  on  $[e_2,b_2]$. Because the spans of $\alpha_1$ and $\alpha_2$ are equal to $s_1, s_2$, we have one more equation for each wrinkle $\alpha_1$ and $\alpha_2$.
$$  \begin{cases}
\mathop{\text{min}}\limits_{x \in [a_1, e_1]} \alpha_1 (x) = \mathop{\text{max}}\limits_{x \in [a_1, e_1]} \alpha_1 (x)  - s_1 =  \text{span}(\alpha)+\varepsilon -s_1 =s_2;\\ % \text{span}(\alpha)+ s_1 + s_2  -\text{span}(\alpha)  -s_1 =  
\mathop{\text{max}}\limits_{x \in [e_2, b_1]} \alpha_2 (x) = \mathop{\text{min}}\limits_{x \in [e_2, b_1]} \alpha_2 (x) + s_2 = s_2 - \varepsilon. 
\end{cases}  $$
We define the minimum value of $\alpha$ on $[a_1, e_1]$ and the maximum of $\alpha$ on $[e_2, b_1]$ so that the condition $\varepsilon$-close to $\alpha$ in $C^0$-norm holds:
$$  
\mathop{\text{min}}\limits_{x \in [a_1, e_1]} \alpha (x) := s_2 + \varepsilon, \text{ and }
\mathop{\text{max}}\limits_{x \in [e_2, b_1]} \alpha (x)  := s_2 - 2 \varepsilon.
  $$
Let 
$$  \begin{cases}
e_0 := \alpha^{-1} \left(s_2 + \varepsilon\right);\\ 
e_3 := \alpha^{-1} \left(s_2 - 2 \varepsilon\right).
\end{cases}  $$
Thus we defined two cubic functions, $f_1$ reaches maximum at $c$ with value $ \text{span}(\alpha) + \varepsilon$ and minimum at $e_0$ with value $s_2$; and $f_2$  reaches maximum at $e_3$ with value $ s_2 - 2 \varepsilon $ and minimum at $d$ with value $-\varepsilon$. The intersections of $f_1$ and $\alpha$ give us two points, which we denoted $(a_1,\alpha_1(a_1))$, $(e_1,\alpha_1(e_1))$; and the intersections of $f_2$ and $\alpha$ give us two other points, which we denoted $(e_2, \alpha_2(e_2))$, $(b_1,\alpha_2(b_1))$. Finally we get the wrinkles $\alpha_1$ and $\alpha_2$. In conclusion, for the given conditions, we could construct a wrinkled map $\beta$ which is $\varepsilon$-closed to $\alpha$ in $C^0$-norm, coincides with $\alpha$ near $a, b$, and has two wrinkles $\alpha_1, \alpha_2$ whose spans are $s_1, s_2$. Unfortunately it is wrong. In fact as the construction, the $C^0$-norm of $\alpha_1-\alpha$ would be greater than $\epsilon$. The point is, I constructed $ e_0 $ to be the point which $\alpha_1$ reaches its local minimum $s_2$ and $\alpha(e_0)=s_2+\epsilon$. But $\alpha_1-\alpha$ does not reach its local minimum at $ e_0$. Vague idea: the equations and inequalities for $\alpha_1$ are: 1)     Maximality at $c$; 2)     Span$(\alpha_1)$ equals $s_1$; 3)     Value at $a$ (or $a_1$); 4)     [Inequality] range of $\alpha_1-\alpha$ is smaller than $\epsilon$. I am really need your help. I only got so far. To specify everything, like the existence of the solution, I don't know...
Thanks.","['differential-geometry', 'linear-algebra', 'calculus']"
2326668,Understanding a one-to-one function with regards to a combinatorial equality,"Show $$\sum_{i=0}^n \binom{n}{r-i} \binom{n-r+i}{s-i} \binom{n-r-s+2i}{i} = \binom{n}{r} \binom{n}{s}$$ For the LHS, we remove the sum for a second and look at the components. This equals the number of triples $(A,B,C)$ such that: $A \subset N_n, |A| = r-i$ $B \subset N_n -A, |B| = s-i$ $C \subset N_n - (A \cup B), |C| = i$ Then $|X_o \cup X_1 \cup \dots \cup X_n | = \sum_{i=0}^n \binom{n}{r-i} \binom{n-r+i}{s-i} \binom{n-r-s+2i}{i}$. Now we do the same for the RHS: $Y = \{ (D,E): D \subset N_n, |D| = r, E \subset N_n, |E| =s \} $ Now we have to define a one-to-one correspondence to finish the exercise. We do it as follows: $f: X_o \cup X_1 \cup \dots \cup X_n \to Y: (A,B,C) \to ( A \cup B, B \cup C) $ I had more trouble with the inverse, and I found the following in the correction sheet: $f^{-1} : Y \to X_o \cup X_1 \cup \dots \cup X_n: (D,E) \to (D-E, E-D, D \cap E) $ I don't understand this however. $|D-E| = r -s \neq r -i ; |E-D = s-r \neq s-i$. So what gives?","['combinatorics', 'elementary-set-theory']"
2326679,Continuous spectrum of unbounded operator,"I would like to ask about continuous spectrum of unbounded, densely defined closed   operator.
Let $A\colon X\supset\mathcal{D}_A\to X$, where X is Banach space, $\overline{\mathcal{D}_A}=X$ be a unbounded linear operator. When I read some books I find two a bit different definitions of continuous spectrum: (a) $\sigma_c(A)=\{\lambda\in\mathbb{C}\, |\,\lambda I-A \textrm{ is injective }, \overline{R(\lambda I -A)}=X,\, R(\lambda I -A)\neq X \}$ (b) $\sigma_c(A)=\{\lambda\in\mathbb{C}\, |\,\lambda I-A \textrm{ is injective }, \overline{R(\lambda I -A)}=X,\, (\lambda I-A)^{-1} \textrm{ is unbounded} \}.$ Could you explain me  why that definitions are equivalent?","['functional-analysis', 'spectral-theory', 'operator-theory']"
2326694,When is a complex manifold of the form $\mathbb{C}^g/\Lambda$ an algebraic variety?,"Given $\mathbb{C}^g/\Lambda$ , when is it an algebraic variety? When $g=1$ , there is a theorem that every one-dimensional Riemann surface is a projective algebraic variety, but what about higher dimensions? I suspect this an overly hard question, but perhaps it will be eased into plausibility by with a particular $g$ and $\Lambda$ in mind. Let $A$ be the three dimensional complex manifold $\mathbb{C}^3/\mathbb{Z}[\zeta_7]$ over $\mathbb{C}$ where $\mathbb{Z}[\zeta_7]$ is embedded in $\mathbb{C}^3$ by the following three homomorphisms: $$\sigma_1, \sigma_2, \sigma_3: \mathbb{Q}(\zeta_7) \to \mathbb{C} \times \mathbb{C} \times \mathbb{C}$$ Here, $\sigma_a$ is the homomorphism $\mathbb{Q}(\zeta_7) \to \mathbb{C}$ which sends $\zeta_7 \mapsto \zeta_7^\text{ a}$ . Q1. How can we see that $A$ is a complex algebraic variety? I am not sure why $A$ admits a polarization, nor why polarizations are entirely relevant to answering Q1. From reading Hartshorne, I see that we have some options: we can prove that $A$ is Hodge, or prove that $A$ is both Kähler and Moishezon; both of these conditions imply projective algebraic variety. I haven't the slightest of how to show these things. Q2. How do we derive the equations which cut out $A$ ? Q3. In general, given $C^g/\Lambda$ , when is it an algebraic variety?","['kahler-manifolds', 'riemannian-geometry', 'algebraic-geometry']"
2326755,Confusion with derivative of the Dirac Delta function.,So I was told by my instructor that $$L(\delta(t)) = 1 $$ And that $$\delta * f(t) = f(t)$$ For any $f(t)$ So $$\delta *1 = 1.$$ But this is $$\int_0^{t}\delta(z)dz.$$ So $$(1)'  = 0 = (\int_0^{t}\delta(z)dz)' = \delta(t).$$ But then wouldn't we have $$L(\delta) = 0\neq 1?$$ Am I misunderstanding something? How do we reconcile these facts? I am using the following definition of the convolution: $$f*g = \int_0^t f(t-z)g(z)dz$$,"['laplace-transform', 'dirac-delta', 'calculus']"
2326778,Help Solving a Differential Equation with Trigonometric Functions,"I am having some trouble solving/understanding:
$$
b\cos(bx)\sin(2y) = -2\cos(2y)\sin(bx)\frac{dy}{dx}
$$
for $y(x)$, where $b\in\mathbb{R}$ is constant. My attempt:
\begin{align}
b\tan(2y) &= -2\tan(bx)\frac{dy}{dx} \\
b\cot(bx) &= -2\cot(2y)\frac{dy}{dx} \\
\int b\cot(bx)dx &= \int -2\cot(2y) dy \\
-\log(\sin(bx)) &= \log(\sin(2y))-C_1 \\
y(x) &= \frac{1}{2}\arcsin\left( \exp(C_1 - \log(\sin(bx))) \right) \\
y(x) &= \frac{1}{2}\arcsin\left( e^{C_1}\csc(bx) \right) \\
\end{align} Now, this solution  lines up with Mathematica, but it is sort of bizarre. Problem 1: the original DE is well-defined everywhere, but this solution has singularities. Edit: it is not actually well-defined when the $dy/dx$ has been zeroed out. Problem 2: most importantly, notice that $y(x)=\frac{n\pi}{2}$ for $n\geq 0,n\in\mathbb{Z}$ is a solution to the DE. But the solution does not really reflect this. What am I missing?","['trigonometry', 'integration', 'ordinary-differential-equations', 'trigonometric-integrals']"
2326813,Triangle inscribed within a unit circle,"A triangle $ABC$ is inscribed within the unit circle. Let $x$ be the measure of the angle $C$. Express the length of $AB$ in terms of $x$. A.) $2\sin x$ B.) $\cos x + \sin x - 1$ C.) $\sqrt{2}(1 - \cos 2x)$ D.) $\sqrt{2}(1 - \sin x)$ I am unsure how to illustrate the circle and calculate the length $x$. Thank you.
`","['algebra-precalculus', 'trigonometry']"
2326823,Difference between critically damped systems and overdamped systems,"Consider a spring system whose equation is given by
$$my''+\mu y'+ky=0$$ and let $D=\mu^2-4mk$. Now there are three cases and I am considering the cases that $D=0$ and $D>0$: When $D=0$, the solution is of the form $y=(a+bt)e^{rt}$. (Critically damped) When $D>0$, the solution is of the form $y=c_1e^{r_1t}+c_2e^{r_2t}$. (Overdamped) While I understand that these two cases are very different and the function $y=(a+bt)e^{rt}$ is very different from the function $y=c_1e^{r_1t}+c_2e^{r_2t}$, it also seems to me that the two functions have very similar graphs (in particular, very similar end behaviours). Question Why are the graphs of the solution in these two cases so similar (while in the other case $D<0$, the graph is very different)? How to tell from the graph whether we have a critically damped system or an overdamped system?","['ordinary-differential-equations', 'graphing-functions']"
2326833,Is the set $\left\{x \in Q\colon x^2 \le 2\right\}$ open or closed?,"Is the following set open or closed? I am almost certain it is open as the limits are not included in the rational set. $$\left\{x \in \mathbb{Q}\colon x^2 \le 2\right\}$$ What I really don’t understand is the proper closure of the set. I think it would be: $$\left\{x \in \mathbb{Q}\colon x^2 \le 2\right\} \cup \left\{\pm\sqrt{2}\right\}$$ But then again, the following seems reasonable (although not an “efficient” closure): $$\left\{x \in \mathbb{R}\colon x^2 \le 2\right\} = \left[-\sqrt{2},\sqrt{2}\right]$$","['general-topology', 'real-analysis', 'irrational-numbers', 'elementary-set-theory']"
2326851,Area of a circle inscribed in a polygon,"If a circle is inscribed in a polygon, show that, $$\dfrac{\text{(Area of inscribed circle)}}{\text{(Perimeter of inscribed circle)}} = \dfrac{\text{(Area of Polygon)}}{\text{(Perimeter of Polygon)}}$$","['circles', 'polygons', 'geometry']"
2326869,Limit of power of Markov matrix,"Recently, a friend gave me the following problem: Let $$M = \begin{pmatrix} p & q & r \\ q & r & p \\ r & p & q \end{pmatrix}$$ where $p, q, r > 0$ and $p + q + r = 1$. Prove that $$\lim_{n \rightarrow \infty} M^n = \begin{pmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{pmatrix}$$ Some of my observations include that $M^{2^n}$ remains cyclic, so I tried to bound (with no luck) the maximum and minimum to see if they both converged to $1/3$. It would be nice if someone could post an elementary solution to this problem.","['matrices', 'markov-chains', 'linear-algebra', 'limits']"
2326934,"Determine if $\lim_{(x,y)\to(0,0)} \frac{x^3y^4}{(x^4 + y^2)^2}$ exist","What I tried: Let $$\ f(x,y) = \frac{x^3y^4}{(x^4 + y^2)^2}$$ For points of the form$\ (x,0)$ then $\ f(x,0)=0$, similarly, for$\ (0,y)$ then $\ f(0,y)=0$, so lets suppose that: $$\lim_{(x,y)\to(0,0)} \frac{x^3y^4}{(x^4 + y^2)^2} =0$$ So, for$\ ε>0$ if$\ δ=ε$ we have to prove that: $$ |\frac{x^3y^4}{(x^4 + y^2)^2} -0|<ε$$ But I'm having a hard time trying to prove the last part, I tried: $$ |\frac{x^3y^4}{(x^4 + y^2)^2} -0|=|\frac{x^3y^4}{(x^4 + y^2)^2}| ≤ |\frac{(x^3y^4)(x^4 + y^2)^2}{(x^4 + y^2)^2}|=|(x^3y^4)|$$",['multivariable-calculus']
2326942,Asymptotic expansion of $\int_{0}^{\infty} \frac{\sin(\frac{x}{n})}{x(1+x^2)}dx$?,"What are the first terms of the asymptotic expansion of
$$
I_n=\int_{0}^{\infty} \frac{\sin(\frac{x}{n})}{x(1+x^2)}dx\ ?
$$
Using the dominated convergence theorem to 
$$
n\bigg(I_n-\int_{0}^{\infty} \frac{\frac{x}{n}}{x(1+x^2)}dx\bigg)
$$ 
I obtain $I_n \sim \frac{\pi}{2n}$. But I have no idea for the second term. ADD-ON: Thank you for the beautiful answers. 1) Does there exist an elementary method (without complex analysis) for obtain the second term ? Indeed, the question is provided by a elementary course on real analysis. 2) I guess the first term with the equivalence $\sin(x) \underset{0}{\sim} x$. Why intuitively the approximation $\sin(x) \approx x-\frac{x^3}{6}$ does not give the second term ?","['asymptotics', 'integration', 'definite-integrals', 'calculus']"
2326951,Find all such polynomials [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Find all polynomials $P(x)$ with real coefficients, such that, $$ |P(x)| \leq \sum_{r=0}^{\infty} |\sin (x)|^{r} \quad \forall x\in\mathbb{R}$$ This question looks daunting. Please help.","['polynomials', 'trigonometry']"
2326997,Sine Waves with Increasing Wavelength,With a function such as $$y = \frac{1}{8}\sin(\pi x)$$ How can the function be modified so that the wavelength increases by 2 each period?,"['trigonometry', 'linear-algebra', 'geometry']"
2327036,Minkowski inequality for integrals,"Let $(X_1,\mu_1)$ and $(X_2,\mu_2)$ be two $\sigma$-finite measure spaces. Let $f(x_1,x_2)$ be a measurable complex valued function. Stein and Shakarchi's 4th book asks to prove that 
$$ \|{\int f(x_1,x_2)d\mu_2}\|_{L^p(X_1)}\leq\int \|f(x_1,x_2)\|_{L^p(X_1)}d\mu_2 $$
when the right side of the inequality is finite. I don't understand why we need the right side to be finite. Even if the right side were $+\infty$, won't the inequality still remain trivially true as the left side can be $+\infty$ at max? I think that the right side being finite has something to do with $\int f(x_1,x_2)d\mu_2$ being well defined. How can I show that this is true?","['inequality', 'integration', 'integral-inequality', 'measure-theory']"
2327038,Lax-Milgram on Parabolic Equation 2,"Let $\Omega=(0,\pi)\times(0,\pi)$and the problem $$ \left\{ \begin{array}{lc}
      \frac{\partial u}{\partial t}-\Delta u+u_{x}-u=t\sin(x+y)
&
(0,1)\times \Omega \\
\frac{\partial u}{\partial n}+u=1 
& 
(0,1)\times \partial \Omega \\
u(0,x)=1& \Omega \end{array} \right. $$ I was asked to find the variational form and to prove that this problem has an unique solution. I found the variational form:
$$\frac{d}{dt}(u(t),v)+\int_{\Omega}\nabla u \nabla v-\int_{\partial{\Omega}}v+\int_{\partial{\Omega}}uv+\int_{\Omega}u_{x}v-\int_{\Omega}uv=\int_{\Omega}t\sin(x+y)v,$$where $v\in H^{1}_{0}$. How can I prove the coercitivity? Can you give me some suggestions? Thank you!","['ordinary-differential-equations', 'partial-differential-equations']"
2327043,Find all the $z\in\mathbb{C}$ that satisfy $(z+1)^4=(z+i)^2$,"I am studying for an algebra final exam and I am stuck with this exercise: Find all the $z\in\mathbb{C}$ that satisfy $(z+1)^4=(z+i)^2$ We have been shown a couple of similar examples in class, but none of them work with this equation. The ones we studied are much nicer. For instance, the complex numbers inside the parenthesis are not the same (one is $(z+1)$ while the other is $(z+i)$ ) and no solutions are easy to find to make things simpler. I mention this because in previous exercises those observations worked. I tried a couple of things: I noticed that they do share their absolute value, and thought of using that to find the argument, with no luck. After that, I tried finding solutions and writing a different factorization but could not find a way to smooth the terms of the equation. So, I tried the crude path of distributing the powers, but I have the feeling that there must be a more clever way of solving this. Also, even this path does not seem to be working. Maybe there is a trick we haven't studied or something simpler that I am just too tired to notice. In any case, I'd welcome some help.","['algebra-precalculus', 'polynomials', 'complex-numbers']"
2327045,roots of the polynomial,"For the following polynomial function $f(x) =28x^4 - 8x^3 -36x^2 + 112x -144$ complete the two items below: factor the polynomial completely find all the zeroes What I have done so far is calculated the value of the function at $x=-1, x=0, x=1, x=2,x=-2$ which is $f(-2)=0$ $f(-1) =-32$ $f(0)=-144$ $f(1)=-48$ $f(2)=320$ so i know one root is $x=-2$ i know the other root would be between $1$ and $2$ as the sign of $f(x)$ changes from $x=1$ to $x=2$. How to find this root? And also other two roots?","['algebra-precalculus', 'roots', 'polynomials']"
2327046,Theorem 6.12 (a) in Baby Rudin: $\int_a^b \left( f_1 + f_2 \right) d \alpha=\int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha$,"Here is part (a) of Theorem 6.12 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f_1 \in \mathscr{R}(\alpha)$ and $f_2 \in \mathscr{R}(\alpha)$ , then $$f_1 + f_2 \in \mathscr{R}(\alpha), $$ and $$ \int_a^b \left( f_1 + f_2 \right) d \alpha = \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha,$$ And, here is Rudin's proof. If $f = f_1 + f_2$ and $P$ is any partition of $[a, b]$ , we have $$ \tag{20} L\left(P, f_1, \alpha \right) + L\left(P, f_2, \alpha \right) \leq L\left(P, f, \alpha \right) \leq U\left(P, f, \alpha \right) \leq U\left(P, f_1, \alpha \right) + U\left(P, f_2, \alpha \right).$$ If $f_1 \in \mathscr{R}(\alpha)$ and $f_2 \in \mathscr{R}(\alpha)$ , let $\varepsilon > 0$ be given. There are partitions $P_j$ $(j = 1, 2)$ such that $$ U \left( P_j, f_j, \alpha \right) -  L \left( P_j, f_j, \alpha \right) < \varepsilon. $$ These inequalities persist if $P_1$ and $P_2$ are replaced by their common refinement $P$ . Then (20) implies $$ U (P, f, \alpha) - L (P, f, \alpha) < 2 \varepsilon, $$ which proves that $f \in \mathscr{R}(\alpha)$ . With this same $P$ we have $$ U \left( P, f_j , \alpha \right) < \int f_j d \alpha + \varepsilon \qquad (j = 1, 2); $$ hence (20) implies $$ \int f d \alpha \leq U ( P, f, \alpha) < \int f_1 d \alpha + \int f_2 d \alpha + 2 \varepsilon. $$ Since $\varepsilon$ was arbitrary, we conclude that $$ \tag{21}  \int f d \alpha \leq \int f_1 d \alpha + \int f_2 d \alpha. $$ If we replace $f_1$ and $f_2$ in (21) by $- f_1$ and $- f_2$ , the inequality is reversed, and the equality is proved. Now here is my reading of Rudin's proof. Let $f = f_1 + f_2$ , and let $P = \left\{ x_0, x_1, \ldots, x_n \right\}$ , where $$a = x_0 \leq x_1 \leq \cdots \leq x_{n-1} \leq x_n = b,$$ be any partition of $[a, b]$ . For each $i = 1, \ldots, n$ , if $m_{1i}$ , $m_{2i}$ , and $m_i$ are the infima of $f_1$ , $f_2$ , and $f$ , respectively, on $\left[ x_{i-1}, x_i \right]$ , then we see that $$ m_{1i} \leq f_1(x), \qquad m_{2i} \leq f_2(x) \qquad (x_{i-1} \leq x \leq x_i ),$$ and so $$ m_{1i} + m_{2i} \leq f_1(x) + f_2(x) = f(x) \qquad (x_{i-1} \leq x \leq x_i ),$$ which in turn implies that $$ m_{1i} + m_{2i} \leq m_i \qquad ( i = 1, \ldots, n  ),$$ and as $\alpha$ is a monotonically increasing function on $[a, b]$ , so $$  m_{1i} \Delta \alpha_i + m_{2i}  \Delta \alpha_i \leq m_i  \Delta \alpha_i \qquad ( i = 1, \ldots, n  ),$$ where $ \Delta \alpha_i = \alpha\left( x_i \right) - \alpha\left( x_{i-1} \right)$ . Adding all the inequalities for $i = 1, \ldots, n$ , we thus obtain $$ L\left( P, f_1, \alpha \right) +  L\left( P, f_2, \alpha \right) \leq L ( P, f, \alpha). \tag{20(1)} $$ Similarly, for each $i = 1, \ldots, n$ , if $M_{1i}$ , $M_{2i}$ , and $M_i$ are the suprema of $f_1$ , $f_2$ , and $f$ , respectively, on $\left[ x_{i-1}, x_i \right]$ , then we see that $$ M_{1i} \geq f_1(x), \qquad M_{2i} \geq f_2(x) \qquad (x_{i-1} \leq x \leq x_i ),$$ and so $$ M_{1i} + M_{2i} \geq f_1(x) + f_2(x) = f(x) \qquad (x_{i-1} \leq x \leq x_i ),$$ which in turn implies that $$ M_{1i} + M_{2i} \geq M_i \qquad ( i = 1, \ldots, n  ),$$ and as $\alpha$ is a monotonically increasing function on $[a, b]$ , so $$  M_{1i} \Delta \alpha_i + M_{2i}  \Delta \alpha_i \geq M_i  \Delta \alpha_i \qquad ( i = 1, \ldots, n  ),$$ and, adding together all these inequalities for $i = 1, \ldots, n$ , we thus obtain $$ U\left( P, f_1, \alpha \right) +  U\left( P, f_2, \alpha \right) \geq U ( P, f, \alpha). \tag{20(2)} $$ From (20(1)) and (20(2)) we obtain (20) in Rudin's proof. Let $\varepsilon > 0$ be given. As $f_1$ and $f_2$ are Riemann-integrable with respect to $\alpha$ over $[a, b]$ , so (by Theorem 6.6 in Baby Rudin, 3rd edition) we can find partitions $P_1$ and $P_2$ of $[a, b]$ such that $$ U \left( P_1, f_1, \alpha \right) - L \left( P_1, f_1, \alpha \right) < \frac{\varepsilon}{2}, \qquad  U \left( P_2, f_2, \alpha \right) - L \left( P_2, f_2, \alpha \right) < \frac{\varepsilon}{2}. \tag{1} $$ Now let $P = P_1 \cup P_2$ . Then $P \supset P_1$ and $P \supset P_2$ . Then (by Theorem 6.4 in Baby Rudin, 3rd edition) we have $$ L \left( P_1, f_1, \alpha \right) \leq L (P, f_1, \alpha) \leq U(P, f_1, \alpha) \leq U(P_1, f_1, \alpha), $$ and $$  L \left( P_2, f_2, \alpha \right) \leq L (P, f_2, \alpha) \leq U(P, f_2, \alpha) \leq U(P_2, f_2, \alpha), $$ and so we can conclude that $$ U \left(P, f_1, \alpha \right) - L \left(P, f_1, \alpha \right) \leq U ( P_1, f_1, \alpha) - L(P_1, f_1, \alpha) < { \varepsilon \over 2 }, $$ and $$  U \left(P, f_2, \alpha \right) - L \left(P, f_2, \alpha \right) \leq U ( P_2, f_2, \alpha) - L(P_2, f_2, \alpha) < { \varepsilon \over 2 }, $$ which implies $$ U \left(P, f_1, \alpha \right) - L \left(P, f_1, \alpha \right) < { \varepsilon \over 2 }, \qquad  \mbox{ and  } \qquad  U \left(P, f_2, \alpha \right) - L \left(P, f_2, \alpha \right) < { \varepsilon \over 2 }, \tag{2} $$ and this in turn implies that $$
\begin{align}
&\ U \left(P, f_1, \alpha \right) + U \left(P, f_2, \alpha \right) - \left[ L \left(P, f_1, \alpha \right) + L \left(P, f_2, \alpha \right)  \right] \\ 
&= \left[  U \left(P, f_1, \alpha \right) - L \left(P, f_1, \alpha \right) \right] + \left[  U \left(P, f_2, \alpha \right) - L \left(P, f_2, \alpha \right)   \right] \\
&< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
&= \varepsilon. 
\end{align} \tag{3} 
$$ But (20) implies that $$ 
\begin{align}
& U ( P, f, \alpha) - L(P, f, \alpha) \\
&\leq  \left[ U \left(P, f_1, \alpha \right) + U \left(P, f_2, \alpha \right) \right] - \left[ L \left(P, f_1, \alpha \right) + L \left(P, f_2, \alpha \right) \right]. \tag{4} 
\end{align}
$$ Now (3) and (4) together imply that $$ U(P, f, \alpha) - L(P, f, \alpha ) < \varepsilon. \tag{5} $$ Thus for every real number $\varepsilon > 0$ we can find a partition $P$ of $[a, b]$ such that (5) holds.  So by Theorem 6.6 in Baby Rudin we can conclude that $f \in \mathscr{R}(\alpha)$ . Thus $$ \underline{\int}_a^b f \ d \alpha = \overline{\int}_a^b f \ d \alpha, $$ that is, $$ \sup \left\{ \ L(Q, f, \alpha) \ \colon \ Q \mbox{ is a partition of } [a, b] \ \right\} =  \inf \left\{ \ U(Q, f, \alpha) \ \colon \ Q \mbox{ is a partition of } [a, b] \ \right\}. $$ Moreover, this common value is denoted by $\int_a^b f \ d \alpha$ . Now from (2) we find that $$ U\left( P, f_1, \alpha \right) < L \left( P, f_1, \alpha \right) + { \varepsilon \over 2 } \leq \int_a^b f_1 d \alpha + { \varepsilon \over 2 },  $$ and $$  U\left( P, f_2, \alpha \right) < L \left( P, f_2, \alpha \right) + { \varepsilon \over 2 } \leq \int_a^b f_2 d \alpha + { \varepsilon \over 2 },   $$ and so $$  U\left( P, f_1, \alpha \right) < \int_a^b f_1 d \alpha + { \varepsilon \over 2 },  \qquad \mbox{ and } \qquad  U\left( P, f_2, \alpha \right) < \int_a^b f_2 d \alpha + { \varepsilon \over 2 },  $$ and upone adding the last two inequalities we obtain $$ U\left( P, f_1, \alpha \right) + U\left( P, f_2, \alpha \right) < \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha + \varepsilon. \tag{6} $$ Now using  (20) we can also conclude $$ \int_a^b f d \alpha \leq U(P, f, \alpha) \leq  U\left( P, f_1, \alpha \right) + U\left( P, f_2, \alpha \right), $$ which implies $$ \int_a^b f d \alpha \leq  U\left( P, f_1, \alpha \right) + U\left( P, f_2, \alpha \right). \tag{7} $$ Therefore  from (6) and (7) we can conclude that for every real number $\varepsilon > 0$ , we have $$ \int_a^b f d \alpha < \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha + \varepsilon, $$ which implies that $$ \int_a^b f d \alpha \leq  \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha. \tag{A}$$ Again from (2) we see that $$ \int_a^b f_1 d \alpha - { \varepsilon \over 2 } \leq U \left( P, f_1, \alpha \right) - { \varepsilon \over 2 } <  
L \left( P, f_1, \alpha \right), $$ and $$ \int_a^b f_2 d \alpha - { \varepsilon \over 2 } \leq U \left( P, f_2, \alpha \right) - { \varepsilon \over 2 } <  
L \left( P, f_2, \alpha \right), $$ which imply $$ \int_a^b f_1 d \alpha - { \varepsilon \over 2 } < L \left( P, f_1, \alpha \right), \qquad \mbox{ and } \qquad 
\int_a^b f_2 d \alpha - { \varepsilon \over 2 } < L \left( P, f_2, \alpha \right), $$ and upon adding the last two inequalities we obtain $$ \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha -  \varepsilon < L \left( P, f_1, \alpha \right) + 
L \left( P, f_2, \alpha \right). \tag{8}$$ But from (20) we obtain $$ L \left( P, f_1, \alpha \right) + 
L \left( P, f_2, \alpha \right) \leq L(P, f, \alpha) \leq \int_a^b f d \alpha,$$ which implies $$ L \left( P, f_1, \alpha \right) + 
L \left( P, f_2, \alpha \right) \leq \int_a^b f d \alpha. \tag{9} $$ From (8) and (9) we have $$ \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha -  \varepsilon < \int_a^b f d \alpha, $$ and so $$ \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha  < \int_a^b f d \alpha + \varepsilon  $$ for every real number $\varepsilon > 0$ . Therefore $$ \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha  \leq  \int_a^b f d \alpha. \tag{B}  $$ Now from (A) and (B) we can conclude that $$ \int_a^b f d \alpha = \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha, $$ as required. Is my rendering of Rudin's proof correct (and as intended by Rudin)?","['real-analysis', 'integration', 'definite-integrals', 'analysis', 'solution-verification']"
2327083,Show Regularity of two dimensional lebesgue measure,"I want to show the regularity of the $n$ -dimensional lebesgue measure.
  I figured, that if I can show it for the two-dimensional case, I should be able to generalize this result to the $n$ -dimensional case.
  More precisely I want to show that \begin{gather*}
m^n(A)=\inf\{m^n(V)\mid A\subset V, V \text{ open}\} \\
m^n(V)=\sup\{m^n(K)\mid K\subset V, K \text{ compact}\}.
\end{gather*} Prelims It should be noted that I know that the one dimensional lebesgue measure is regular. So I am hoping to be able to utilize this fact. The way the $n$ -dimensional lebesgue measure was formulated as the product measure of $n$ one-dimensional lebesgue measures. In particular, for the two dimensional case the formulation for $m^2$ I know is the following Consider all measurable rectangles $A\times B\in \mathcal{B}\times\mathcal{B}$ , where $\mathcal{B}$ is the Borel $\sigma$ -algebra on $\mathbb{R}$ . Consider the algebra of finite unions of disjoint measurable rectangles and define a premeasure $\mu_{0}$ over this algebra as follows. $$
\mu_{0}\left(\bigcup_{i=1}^{k}(A_{i}\times B_{I})\right)
=\sum_{i=1}^{n}m(A_{i})m(B_{i})$$ Then, using the caratheodory extension theorem, we get a measure on $\mathcal{B}\otimes \mathcal{B}=\sigma(\mathcal{B}\times \mathcal{B})$ .
The reason why I am saying all this is because I am aware that there are many formulations of the $n$ -dimensional lebesgue measure, however I would like answers that appeal to this construction of the lebesgue measure (even though I am aware that all the formulations give the same thing, note I only have awareness, the only formulation I know is the one described above). My attempt My attempt for outer regularity: I have tried first showing outer regularity on measurable rectangles. Consider a measurable rectangles $A\times B\in \mathcal{B}\times \mathcal{B}$ .
Then, since the lebesgue measure is outer regular, for $\epsilon>0$ there exist open sets in \mathbb{R} $V_{A}$ and $V_{B}$ such that $m(V_{A}-A)<\epsilon$ and $m(V_{B}-B)<\epsilon$ . Then, we have that $$
m^2(V_{A}\times V_{B}-A\times B
)<\epsilon^2 + \epsilon m(A) + \epsilon m(B).
$$ Provided $m(A)$ and $m(B)$ are finite, we can choose epsilon to be arbitrarily small and hence make $m^2(V_{A}\times V_{B}-A\times B)$ arbitrarily small.
I am concerned if one of $m(A)$ or $m(B)$ is infinity and the other is zero. In such a case $m^2(A\times B)$ is defined to be zero and we cannot use the above argument to show outer regularity, what should I do? Supposing that all worked though, I can see how this would work for general set in $\mathcal{B}\otimes \mathcal{B}$ . Since every element of $\mathcal{B}\otimes \mathcal{B}$ is the countable union of measurable rectangles we can find open rectangles of the form $V_{i}\times W_{i}\in \mathcal{B}\times \mathcal{B}$ such that $$
m^2(V_{i}\times W_{i}-A_{i}\times B_{i})<\frac{\epsilon}{2^{i}}.
$$ My attempt for inner regularity: Since there is a countable basis for the topology of $\mathbb{R}\times \mathbb{R}$ we know that any open set in $\mathbb{R}\times \mathbb{R}$ can be expressed as the countable union of elements of the form $V_{i}\times W_{i}$ where $V_{i}$ and $W_{i}$ are in the countable basis for the topology of $\mathbb{R}$ . Let $U\in \mathbb{R}\times \mathbb{R}$ be an open set and let $U=\bigcup_{i=1}^{\infty}V_{i}\times W_{i}$ .
For each $V_{i}\times W_{i}$ we can use a similar trick we used in the proof of outer regularity to find $K_{i}^1$ and $K_{i}^2$ , which are compact sets, such that $$
m^2(V_{i}\times W_{i}-K_{i}^1\times K_{i}^2)
<\frac{\epsilon}{2^{i}},
$$ which is all good, but the set $\bigcup_{i=1}^{\infty}K_{i}^1\times K_{i}^2$ is not necessarily compact anymore. Any help on how to solve this question is greatly appreciated. Sorry my attempt and explanation are a bit long.","['lebesgue-measure', 'measure-theory']"
2327110,Does this system of equations have any solutions?,"I would just like to know if this system has a solution: $$\begin{cases}
x = \dfrac{1}{\sqrt{2}} \cos(x+y) \\
y = \dfrac{1}{\sqrt{2}} \sin(x+y)  
\end{cases}$$ I don't know how to do it at all. Could someone help me?","['trigonometry', 'systems-of-equations']"
2327174,The Definition of an Almost Complex Manifold (Nakahara),"I am having problems making sense of Michio Nakahara's definition of the Almost Complex Structure/Almost Complex Manifold, such as it appears in Geometry, Topology and Physics (2nd Edition). On p. 318-319, he writes The tensor field $J$ is called the almost complex structure of a complex manifold $M$. Note that any $2m$-dimensional manifold locally admits a tensor field $J$ which squares to $-I_{2m}$. However, $J$ may be patched across charts and defined globally only on a complex manifold. What confuses me is when he later on defines the almost complex manifold on p. 342: Let $M$ be a differentiable manifold. The pair $(M,J)$, or simply $M$, is called an almost complex manifold if there exists a tensor field $J$ of type (1,1) such that at each point $p$ of $M$, $J_p^2 = - \text{id}_{T_p M}$. The tensor field $J$ is also called the almost complex structure . What confuses me is that this definition seems to be in contradiction to the first one. As far as I can understand it, $J$ in the second definition is defined globally in the sense that it is defined at every single point on the manifold $M$. Yet if this was the case, then by the first definition, it would not only be an almost complex manifold we were looking at, but would in fact be a full complex manifold. My question therefore is, when Nakahara on p. 318-319 writes that $J$ can only be defined globally on a complex manifold, in what sense does he mean globally? And what does he mean by ""be patched across charts""? In what sense cannot $J$ for an almost complex but still non-complex manifold not be ""patched across charts and defined globally""? Many thanks.","['almost-complex', 'complex-manifolds', 'differential-geometry']"
2327181,Why aren't all intervals called infinite?,"I'm a bit confused about the definition of finite sets/intervals. I know that a set S is called finite when it has a finite number of elements, or formally, when there exists a bijection $f:S\to\{1,...,n\}$ for some natural number n. However, the interval $(1,2)$ is called finite. I don't understand why; $(1,2)$ is not even countable, and there definitely does not exist a bijection $f:(1,2)\to\{1,...,n\}$. Why do we call $(a,b)$ with $a,b\in\mathbb{R}$, finite? Did we just agree to do so, or is it incorrect to use the interval $(a,b)$ as a set like I did in the definition of finiteness above? Thanks!","['terminology', 'elementary-set-theory', 'order-theory']"
2327197,How to express Mathieu function's first derivative in terms of Mathieu function?,"Mathieu function $\operatorname{ce}_m(q,x)$ and $\operatorname{se}_m(q,x)$ are periodic solution of Mathieu equation, which is analogous to $\cos(x)$ and $\sin(x)$. However, unlike $\cos(x)$ and $\sin(x)$, the derivative of Mathieu function $\operatorname{ce}^\prime_m(q,x)$ and $\operatorname{se}^\prime_m(q,x)$ are not equal to $-\operatorname{se}_m(q,x)$ and $\operatorname{ce}_m(q,x)$. My question is, how to express Mathieu function's first derivative in terms of Mathieu function? Explicitly, in the following expansion, what is the analytical expression of $A_m^k$(not in the integral form)? $$
\operatorname{ce}^\prime_m(q,x) = \sum_{k}a_m^n \operatorname{se}_n(q,x)
$$ If no analytical expression is available, does anyone known the existing numerical library which can generate these coefficients conveniently?","['special-functions', 'ordinary-differential-equations', 'sequences-and-series']"
2327229,"Theorem 6.12 (b) in Baby Rudin: If $f_1 \leq f_2$ on $[a, b]$, then $\int_a^b f_1 d\alpha \leq \int_a^b f_2 d\alpha$","Suppose $f_1$ and $f_2$ are Riemann-integrable with respect to $\alpha$ over $[a, b]$. If $f_1(x) \leq f_2(x)$ on $[a, b]$, then 
  $$ \int_a^b f_1 d \alpha \leq \int_a^b f_2 d \alpha. $$ This is (essentially) Theorem 6.12 (b) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition. Here is my proof: As $f_1 \leq f_2$ on $[a, b]$, so, for any partition $P = \left\{ \ x_0, \ldots, x_n \right\}$ of $[a, b]$, we have 
  $$ \inf_{x_{i-1} \leq x \leq x_i } f_1(x) \leq  \inf_{x_{i-1} \leq x \leq x_i } f_2(x), \  
\mbox{ and } \ \sup_{x_{i-1} \leq x \leq x_i } f_1(x) \leq  \sup_{x_{i-1} \leq x \leq x_i } f_2(x)$$ 
  for each $ i = 1, \ldots, n$, and therefore 
  $$ L \left( P, f_1, \alpha \right) \leq L \left( P, f_2, \alpha \right), \ \mbox{ and } \ U \left( P, f_1, \alpha \right) \leq U \left( P, f_2, \alpha \right) \tag{0} $$
  for every partition $P$ of $[a, b]$. Now as $f_1 \in \mathscr{R}(\alpha)$ and $f_2 \in \mathscr{R}(\alpha)$, so, for $j = 1, 2$, we have $$ \sup \left\{ \ L \left( P, f_j, \alpha \right) \ \colon \ P \mbox{ is a partition of } [a, b] \ \right\} = \int_a^b f_j d \alpha = \inf \left\{ \ U \left( P, f_j, \alpha \right) \ \colon \ P \mbox{ is a partition of } [a, b] \ \right\}. $$ 
  Therefore, for $j = 1, 2$, we have 
  $$ L \left( P, f_j, \alpha \right) \leq \int_a^b f_j d \alpha \leq U \left( P, f_j, \alpha \right) \tag{1}$$
  for every partition $P$ of $[a, b]$; moreover, for every real number $\delta > 0$, there exist partitions $P_j$, $Q_j$ of $[a, b]$ such that 
  $$ \int_a^b f_j d \alpha - \delta  < L \left( P_j, f_j, \alpha \right), \mbox{ and } U \left( Q_j, f_j, \alpha \right) < \int_a^b f_j d \alpha + \delta, \tag{2} $$
  and, hence if $S_j$ is any partition of $[a, b]$ such that $S_j \supset P_j$ and $S_j \supset Q_j$, then (by Theorem 6.4 in Baby Rudin, 3rd edition) we must have 
  $$ L \left( P_j, f_j, \alpha \right) \leq L \left( S_j, f_j, \alpha \right) \leq U \left( S_j, f_j, \alpha \right) \leq U \left( Q_j, f_j, \alpha \right). \tag{3} $$
  From (2) and (3) we can conclude that, for each $j = 1, 2$, there exists a partition $S_j$ of $[a, b]$ such that 
  $$  \int_a^b f_j d \alpha - \delta < L \left( S_j, f_j, \alpha \right) \leq U \left( S_j, f_j, \alpha \right) <  \int_a^b f_j d \alpha + \delta. \tag{4} $$
  Now let $P$ be any partition of $[a, b]$ such that $P \supset S_1$ and $P \supset S_2$. Then (again by Theorem 6.4 in Baby Rudin, 3rd edition) we have for each $j = 1, 2$, $$ L \left( S_j, f_j, \alpha \right) \leq L \left( P, f_j, \alpha \right) \leq U \left( P, f_j, \alpha \right) \leq U \left( S_j, f_j, \alpha \right). \tag{5} 
$$ Thus, for every real number $\delta > 0$, we see that 
  $$ 
\begin{align}
\int_a^b f_1 d\alpha &\leq U \left( P, f_1, \alpha \right) \qquad \mbox{ [ by (1) above ] } \\
&\leq U \left( P, f_2, \alpha \right) \qquad \mbox{ [ by (0) above ] } \\
& \leq U \left( S_2, f_2, \alpha \right) \qquad \mbox{ [ by (5) ] } \\
& < \int_a^b f_2 d \alpha + \delta \qquad \mbox{ [ by (4) ] }, 
\end{align}
$$
  which implies that 
  $$ \int_a^b f_1 d \alpha \leq \int_a^b f_2 d \alpha, $$
  as required. Is this proof correct? If so, then is my presentation clear and optimal enough? If not, then where lie the pitfalls in my reasoning? Have I superfluously used any of the partitions $P_j$, $Q_j$, $S_j$ for $j = 1, 2$, or the partition $P$ at the end?","['real-analysis', 'proof-verification', 'integration', 'definite-integrals', 'analysis']"
2327232,Is my piecewise function right? Problem given below.,"Problem:
A computer shop charges 20 pesos per hour (or a fraction of an hour) for the first two hours and an additional 10 pesos per hour for each succeeding hour. Represent your computer rental fee using the function R(t) where t is the number of hours you spent on the computer. My answer:
$$R(t) = \cases{20 \text{ pesos}  & if $0 <t\le 2$\cr
({t-2}) \cdot 10 \text{ pesos}  +40 \text{ pesos} & if $t>2$}$$ Please correct me if I have any mistake.",['functions']
2327233,intuitive description of $\beth_2$ cardinality,"I was wondering if there is some intuitive way of describing the size of a set of cardinality $\beth_2$ ($2^c$) ?
For the natural and real numbers the I think the intuitive understanding is quite easy, as is implied by the names: $\aleph_0$ is ""countable"" , i.e. we can imagine it as set of discrete objects that can be labeled and counted, while the continuum $c$ is, well, ""continuous"". Is there any similar intuitive way of explaining in what way $2^c$ is qualitatively different than $c$ ?","['intuition', 'cardinals', 'elementary-set-theory']"
2327273,Number of leaves in a tree,"If a tree has 5 vertices of degree 2, 3 vertices of degree 3, 4 vertices of degree 4, then how many leaves are there in that tree? I know the tree has at least 12 vertices and so it must have at least 11 edges.  Also the number of leaves must be odd but I could not proceed further.","['graph-theory', 'discrete-mathematics']"
2327280,Can I solve this integral without spherical coordinates?,"Def. $$
f: \; [-r, r]^3 \rightarrow \mathbb{R}, \; (x_1, x_2,x_3) \mapsto \begin{cases}
\sqrt{r^2 - x_1^2 - x_2^2 - x_3^2}, & \text{for } x_1^2 + x_2^2 + x_3^2 \leq r^2, \\
0, & \text{else}.
\end{cases}
$$ with $r > 0$. I need to prove: $$
\int_{-r}^r \int_{-r}^r \int_{-r}^r f(x_1, x_2, x_3) \mathrm{d}x_1 \mathrm{d}x_2 \mathrm{d}x_3 = \frac{\pi^2}{4}r^4.
$$ I was able to solve the integral by first transforming $(x_1, x_2, x_3)$ into spherical coordinates $(p, \theta, \varphi)$ but I am still at the point in my studies where I do not know about coordinate transformations of integrals. My question: Is there a way to calculate the integral without spherical coordinates or is there a simple proof such that I can do it in spherical coordinates? What I did: \begin{align}
& \int_{-r}^r \int_{-r}^r \int_{-r}^r f(x_1, x_2, x_3) \mathrm{d}x_1 \mathrm{d}x_2 \mathrm{d}x_3 \\
=& \int_{0}^r \int_{0}^\pi \int_{-\pi}^\pi \sqrt{r^2-p^2} \cdot p^2\sin\theta\; \mathrm{d}\varphi \mathrm{d}\theta \mathrm{d}p \\
=& \dots \\
=& 4\pi \int_0^r \sqrt{r^2-p^2} \cdot p^2 \mathrm{d}p \\
=& \dots \\
=& 4\pi \left[ \frac{1}{8} \arcsin\frac{p}{r} - \frac{1}{8}\sin\left( 4\arcsin\frac{p}{r} \right) \right]_{p=0}^{p=r} \\
=& \frac{\pi^2}{4} r^4
\end{align}","['multivariable-calculus', 'integration', 'spherical-coordinates']"
2327286,"Theorem 6.12 (c) in Baby Rudin: If $f\in\mathscr{R}(\alpha)$ on $[a, b]$ and $a<c<b$, then $f\in\mathscr{R}(\alpha)$ on $[a, c]$ and $[c, b]$","Here is Theorem 6.12 (c) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $a < c < b$, then $f \in \mathscr{R}(\alpha)$ on $[a, c]$ and on $[c, b]$, and 
  $$ \int_a^c f d \alpha + \int_c^b f d \alpha  =  \int_a^b f d \alpha. $$ Here is my proof: Let $\varepsilon > 0$ be given. As $f \in \mathscr{R}(\alpha)$ on $[a, b]$, so we can find a partition $P$ of $[a, b]$ such that 
  $$ U(P, f, \alpha ) - L(P, f, \alpha) < \varepsilon. $$
  Let $Q$ be any refinement of $P$ such that $Q$ also contains the point $c$. 
  Then (by Theorem 6.4 in Baby Rudin, 3rd edition) we have 
  $$ L(P, f, \alpha ) \leq L(Q, f, \alpha) \leq U(Q, f, \alpha) \leq U(P, f, \alpha), $$
  and so 
  $$ U(Q, f, \alpha) - L(Q, f, \alpha) \leq  U(P, f, \alpha ) - L(P, f, \alpha) < \varepsilon. \tag{1} $$
  Let $$ Q = \left\{ x_0, \ldots, x_{k-1}, c, x_k, \ldots, x_n \ \right\},$$
  where 
  $$ a = x_0 < \cdots < x_{k-1} < c < x_k < \cdots < x_n = b.$$
  Let $$Q_1 \colon= \left\{ \ x_0, \ldots, x_{k-1}, c \ \right\}, \qquad Q_2 \colon= \left\{\ c, x_k, \ldots, x_n \ \right\}.$$ 
  Then $Q_1$ and $Q_2$ are partitions, respectively, of $[a, c]$ and $[c, b]$, and $$ Q = Q_1 \cup Q_2. $$
  Also
  $$ L(Q, f, \alpha) = L\left( Q_1, f, \alpha \right) + L\left( Q_2, f, \alpha \right), \tag{2}$$
  and 
  $$ U(Q, f, \alpha) = U\left( Q_1, f, \alpha \right) + U\left( Q_2, f, \alpha \right), \tag{3}$$
  where 
  $$ L\left( Q_1, f, \alpha \right) \colon= \sum_{i=1}^{k-1} \left( \inf_{x_{i-1}\leq x \leq x_i} f(x) \right) \left( \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) \right) + \left( \inf_{x_{k-1}\leq x\leq c} f(x) \right) \left( \alpha (c) - \alpha \left( x_{k-1} \right) \right), $$
  and 
  $$L\left( Q_2, f, \alpha \right) \colon= \left( \inf_{c\leq x\leq x_k} f(x) \right) \left( \alpha \left(x_k \right) - \alpha(c) \right) +  \sum_{i=k+1}^n  \left( \inf_{x_{i-1}\leq x \leq x_i} f(x) \right) \left( \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) \right), $$
  and similarly for $U\left( Q_1, f, \alpha \right)$ and $U\left( Q_2, f, \alpha \right)$. Moreover, for each $j= 1, 2$, 
  $$ U \left( Q_j, f, \alpha \right) -  L \left( Q_j, f, \alpha \right) \geq 0, $$
  which together with (1) implies that, for each $j = 1, 2$, 
  $$ U \left( Q_j, f, \alpha \right) -  L \left( Q_j, f, \alpha \right) \leq U (Q, f, \alpha ) - L(Q, f, \alpha) < \varepsilon, $$
  from which it follows that $f$ is Riemann-integrable with respect to $\alpha$ on $[a, c]$ and on $[c, b]$. And, from (1)and (2) above we obtain 
  \begin{align}
\int_a^b f d \alpha &\leq U(Q, f, \alpha ) \\ 
&< L(Q, f, \alpha) + \varepsilon \qquad \mbox{ [ by (1) above ] } \\
&= L\left(Q_1, f, \alpha \right) + L \left( Q_2, f, \alpha \right) + \varepsilon \qquad \mbox{ [ by (2) above ] } \\
&\leq \int_a^c f d \alpha + \int_c^b f d \alpha + \varepsilon 
\end{align}
  for every real number $\varepsilon > 0$, which implies that 
  $$ \int_a^b f d\alpha \leq \int_a^c f d\alpha + \int_c^b f d \alpha. \tag{A}$$ Now from (1) and (3) above, we obtain 
  \begin{align}
\int_a^c f d \alpha + \int_c^b f d \alpha &\leq U \left( Q_1, f, \alpha \right) + U \left( Q_2, f, \alpha \right) \\
&= U(Q, f, \alpha ) \qquad \mbox{ [ by (3) above ] } \\
&< L(Q, f, \alpha ) + \varepsilon \qquad \mbox{ [ by (1) above ] } \\
&\leq \int_a^b f d \alpha + \varepsilon
\end{align}
  for every real number $\varepsilon > 0$, which implies that 
  $$ \int_a^c f d \alpha + \int_c^b f d \alpha \leq  \int_a^b f d \alpha. \tag{B}$$ 
  From (A) and (B), we conclude that 
  $$ \int_a^c f d \alpha + \int_c^b f d \alpha = \int_a^b f d \alpha, $$
  as required. Is the above proof correct (and as required by Rudin)? If so, then is my presentation good enough too? If not, then where lie the pitfalls?","['real-analysis', 'proof-verification', 'integration', 'definite-integrals', 'analysis']"
2327287,Is it possible to express $f(n)$ and $g(n)$ in a single formula?,"Yes, this seems to be silly question, but please listen to my thought. Example: $n\in\mathbb{N}$ , Let $f(n)=2n-1$ and $g(n)=2n$, Now look at this function: $$H(n)=n$$
  If We ""combine"" $f(n)$ and $g(n)$ we get $h(n)$ Question: İf $n\in\mathbb{N}$ and $$f(n)=\frac{2^{8n-3}-2^{2n-1}-3}{9}$$
$$g(n)=\frac{2^{8n-4}-2^{2n}-3}{9}$$ Is it possible to express $f(n)$ and $g(n)$ in a single formula? Ps. English is not my mother language. I may not have been clear my question. For a better understanding of the problem please, edit.
Thank you!","['number-theory', 'functions']"
2327300,Why was the Butterfly Effect so surprising?,"I was reading a popular science book about the history of  Chaos theory and was curious about the part when Edward Lorenz discovered the so-called butterfly effect . Long story short, he omitted to key in an extra decimal place into his computer simulation of weather, thinking that it wouldn't make much a difference, but instead got a completely different result in a relatively short period of time. This led him to his theory about chaotic system. Sure, it might be counter-intuitive to think that a small error can magnify that fast, but even with hindsight this shouldn't be THAT surprising at all. I mean, for example, Hadamard had coined the term well-posedness and sensitivity to initial condition decades before that. Edward Lorenz, being a fine mathematician as he was, should be familiar with these things shouldn't he? I'm not an expert in this so there could be many points that I overlooked. I hope that the question is not too broad.","['dynamical-systems', 'math-history', 'ordinary-differential-equations', 'soft-question']"
2327320,"Theorem 6.12 (d) in Baby Rudin: If $\lvert f(x) \rvert \leq M$ on $[a, b]$, then $\lvert \int_a^b f d\alpha \rvert \leq \ldots$","Here is Theorem 6.12 (d) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $\lvert f(x) \rvert \leq M$ on $[a, b]$, then 
  $$ \left\lvert \int_a^b f d\alpha \right\rvert \leq M \left[ \alpha(b) - \alpha(a) \right]. $$ Here is my proof of this assertion. As $\lvert f(x) \rvert \leq M$ on $[a, b]$, so $-M \leq f(x) \leq M$ on $[a, b]$ and for every partition $P$ of $[a, b]$, we have 
  $$ -M \left[ \alpha(b) - \alpha(a) \right] = - M \sum_{i=1}^n \left[ \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right)  \right] \leq L(P, f, \alpha ) \leq U(P, f, \alpha) \leq M \sum_{i=1}^n \left[ \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right)  \right] = M \left[ \alpha(b) - \alpha(a) \right]. \tag{1} $$
  And, as $f \in \mathscr{R}(\alpha)$ on $[a, b]$, so for every partition $P$ of $[a, b]$, we also have 
  $$ L(P, f, \alpha ) \leq \int_a^b f d \alpha \leq U(P, f, \alpha). \tag{2} $$
  From (1) and (2) we obtain 
  $$-M \left[ \alpha(b) - \alpha(a) \right] \leq   L(P, f, \alpha ) \leq \int_a^b f d \alpha \leq U(P, f, \alpha) \leq M \left[ \alpha(b) - \alpha(a) \right], $$
  and so 
  $$-M \left[ \alpha(b) - \alpha(a) \right] \leq  \int_a^b f d \alpha \leq M \left[ \alpha(b) - \alpha(a) \right], $$
  which implies that 
  $$ \left\lvert \int_a^b f d \alpha \right\rvert \leq M \left[ \alpha(b) - \alpha(a) \right], $$
  as required. Is this proof lacking in logic, rigor, or presentation?","['real-analysis', 'proof-verification', 'integration', 'definite-integrals', 'analysis']"
2327344,"Geometric meaning of $H=\langle x, \nu \rangle$","$M$  is a $n$-dimensional smooth manifold without boundary .  $F: M \rightarrow \mathbb R^{n+1}$ is a smooth  embedding. $A$ is the second foundamental form , and $H$ is mean curvature.  $\nu$ is the normal vector.  $x$ is  position vector. If 
$$
H=\langle x, \nu \rangle           \tag{1}
$$
it is easy to see hyperplane, sphere, cylinder satisfy the equation. But I don't know whether there are other manifold satisfy this equation, especially  , manifold with negative mean curvature? In the Huisken's Asymptotic behavior for singularities, he prove that if $M$, $n\ge 2$, is compact with nonnegative mean curvature $H$ and satisfy  (1), then $M$ is sphere of radius $\sqrt n$. Parts of this proof is to prove  $\frac{|A|^2}{H^2}$ is constant. I don't know why he know to calculate this quantity.  Exactly , this quantity has maximum principle. Whether there are any geometric view  ? Last, I guess the geometric essence of (1) is  the principle curvature of  must be constant or zero, right ?","['mean-curvature-flows', 'riemannian-geometry', 'differential-geometry']"
2327359,"If $M$ is a martingale and $σ_n↓$ and $τ_n↓τ$, then $\text E[M_{τ_n}\mid\mathcal F_{σ_n}]\xrightarrow{n→∞}\text E[M_τ\mid\mathcal F_σ]$","Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $(\mathcal F_t)_{t\ge0}$ be a filtration of $\mathcal A$ $M$ be a $\operatorname P$-almost surely right-continuous $(\operatorname P,\mathcal F)$-martingale $\sigma,\tau$ be $\mathcal F$-stopping times with $$\tau\le T\;\;\;\operatorname P\text{-almost surely}\tag1$$ for some $T\ge0$ $\mathcal F_\varsigma:=\left\{A\in\mathcal A:A\cap\left\{\varsigma\le t\right\}\in\mathcal F_t\text{ for all }t\in I\right\}$ for any $\mathcal F$-stopping time $\varsigma$ Now, let $$\sigma_n:=\frac1{2^n}\lceil 2^n\sigma\rceil\text{ and }\tau_n:=\frac1{2^n}\lceil 2^n\tau\rceil\;\;\;\text{for }n\in\mathbb N\;.$$ Note that $\sigma_n,\tau_n$ are $(\mathcal F_{k2^{-n}})_{k\in\mathbb N_0}$-stopping times and $(\sigma_n)_{n\in\mathbb N},(\tau_n)_{n\in\mathbb N}$ are nonincreasing with $$\sigma_n\xrightarrow{n\to\infty}\sigma\text{ and }\tau_n\xrightarrow{n\to\infty}\tau\tag2\;.$$ I want to conclude that $$\operatorname E\left[M_{\tau_n}\mid\mathcal F_{\sigma_n}\right]\xrightarrow{n\to\infty}\operatorname E\left[M_\tau\mid\mathcal F_\sigma\right]\tag3\;\;\;\operatorname P\text{-almost surely}\;.$$ Since $M$ is $\operatorname P$-almost surely right-continuous, we obtain $$M_{\tau_n}\xrightarrow{n\to\infty}M_\tau\;\;\;\operatorname P\text{-almost surely}\tag4\;.$$ However, I don't know how I need to proceed (especially with the indexed $\sigma$-algebra in the conditional expectation).","['stochastic-processes', 'probability-theory', 'stopping-times', 'martingales', 'conditional-expectation']"
2327382,"Theorem 6.12 (e) in Baby Rudin: If $f \in \mathscr{R}\left(\alpha_1\right)$ and $f \in \mathscr{R}\left(\alpha_2\right)$, then $\ldots$","If $f \in \mathscr{R}\left(\alpha_1\right)$ and $f \in \mathscr{R}\left(\alpha_2\right)$, then $f \in \mathscr{R}\left(\alpha_1 + \alpha_2 \right)$
  and 
  $$ \int_a^b f d\left(\alpha_1 + \alpha_2 \right) = \int_a^b f d\alpha_1 +  \int_a^b f d\alpha_2.$$ This is part of Theorem 6.12 (e) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition. Here is my proof: Let $$\alpha \colon= \alpha_1 + \alpha_2. $$ As $\alpha_1$ and $\alpha_2$ are monotonically increasing functions defined on $[a, b]$, so is $\alpha$. Moreover, if $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$ is any partition of $[a, b]$, then 
  $$\begin{align}
 L(P, f, \alpha) &= \sum_{i=1}^n \left( \inf_{x_{i-1}\leq x \leq x_i } f(x) \right) \left[ \alpha \left(x_i \right) - \alpha \left( x_{i-1} \right) \right] \\
&=  \sum_{i=1}^n \left( \inf_{x_{i-1}\leq x \leq x_i } f(x) \right) \left[ \alpha_1 \left(x_i \right) - \alpha_1 \left( x_{i-1} \right) \right] \\
& \qquad +  \sum_{i=1}^n \left( \inf_{x_{i-1}\leq x \leq x_i } f(x) \right) \left[ \alpha_2 \left(x_i \right) - \alpha_2 \left( x_{i-1} \right) \right] \\
&= L \left( P, f, \alpha_1 \right) + L \left( P, f, \alpha_2 \right), \tag{1}
\end{align}
$$
  and similarly, 
  $$  U (P, f, \alpha) = U \left( P, f, \alpha_1 \right) + U \left( P, f, \alpha_2 \right). \tag{2}$$ Now as $f \in \mathscr{R}\left(\alpha_1\right)$ and $f \in \mathscr{R}\left(\alpha_2\right)$, so for every real number $\varepsilon > 0$ we can find partitions $P_1$ and $P_2$ of $[a, b]$ such that 
  $$ U \left( P_1, f, \alpha_1 \right) - L \left( P_1, f, \alpha_1 \right) < { \varepsilon \over 2 } \  \mbox{ and } \  U \left( P_2, f, \alpha_2 \right) - L \left( P_2, f, \alpha_2 \right) < { \varepsilon \over 2 }. \tag{3} $$ Now if $P$ is any partition of $[a, b]$ such that $P \supset P_1$ and $P \supset P_2$, then we have, for each $j = 1, 2$, 
  $$ L \left( P_j, f, \alpha_j \right) \leq L \left( P, f, \alpha_j \right) \leq U \left( P, f, \alpha_j \right) \leq U \left( P_j, f, \alpha_j \right),$$
  and so 
  $$  U \left( P, f, \alpha_j \right) - L  \left( P, f, \alpha_j \right) \leq 
U \left( P_j, f, \alpha_j \right) - L \left( P_j, f, \alpha_j \right). \tag{4} $$
  From (3) and (4), we see that, for each $j = 1, 2$, we have 
  $$  U \left( P, f, \alpha_j \right) - L  \left( P, f, \alpha_j \right) < { \varepsilon \over 2 }, $$
  which together with (1) and  (2) yields 
  $$
\begin{align}
 U (P, f, \alpha) - L (P, f, \alpha)  &= \left[  U \left( P, f, \alpha_1 \right) + U \left( P, f, \alpha_2 \right) \right] - \left[  L \left( P, f, \alpha_1 \right) + L \left( P, f, \alpha_2 \right) \right] \\
&=  \left[  U \left( P, f, \alpha_1 \right) - L \left( P, f, \alpha_1 \right) \right] + \left[  U \left( P, f, \alpha_2 \right) -  L \left( P, f, \alpha_2 \right) \right] \\
&< { \varepsilon \over 2 } + { \varepsilon \over 2 } \\
&= \varepsilon,
\end{align}
$$
  and thus it follows that $f \in \mathscr{R}\left(\alpha_1 + \alpha_2 \right)$. Finally, 
  $$
\begin{align}
\int_a^b f d\alpha &= \int_a^b f d \left( \alpha_1 + \alpha_2 \right) \\
&= \inf \left\{ \ U(P, f, \alpha) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\
&= \inf \left\{ \ U \left(P, f, \alpha_1 \right) + U \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\
&\geq \inf \left\{ \ U \left(P, f, \alpha_1 \right) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} + \inf \left\{ U \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\
&= \int_a^b f d \alpha_1 + \int_a^b f d \alpha_2, \tag{5}
\end{align} 
$$
  and 
  $$
\begin{align}
\int_a^b f d\alpha &= \int_a^b f d \left( \alpha_1 + \alpha_2 \right) \\
&= \sup \left\{ \ L(P, f, \alpha) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\
&= \sup \left\{ \ L \left(P, f, \alpha_1 \right) + L \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\
&\leq \sup \left\{ \ L \left(P, f, \alpha_1 \right) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} + \sup \left\{ L \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\
&= \int_a^b f d \alpha_1 + \int_a^b f d \alpha_2. \tag{6}
\end{align} 
$$
  From (5) and (6), we can conclude that 
  $$ \int_a^b f d \left( \alpha_1 + \alpha_2 \right) = \int_a^b f d \alpha_1 +  \int_a^b f d \alpha_2,$$
  as required. Is this proof sound enough in terms of logic and rigor?","['real-analysis', 'proof-verification', 'integration', 'definite-integrals', 'analysis']"
2327443,Finding the sum of an alternating series,"I want to find the sum of $$\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n^2}$$ I know that this is equal to $\frac{\pi^2}{12}$ thus I was thinking this must just be a taylor series of some trigonometric function but after looking it up, I cannot seem to find one that satisfies this. Any suggestions are greatly appreciated. From below the user states that $$\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n^2} = \sum_{n=0}^{\infty}\left(\frac{1}{2n+1} \right)^2 -\sum_{n=1}^{\infty}\left(\frac{1}{2n}\right)^2 = \sum_{n=1}^{\infty}\frac{1}{n^2} - 2\sum_{n=1}^{\infty}\left(\frac{1}{2n}\right)^2 = \ldots = \frac{\pi^2}{12}$$ I want to know the details of the $\ldots$ part.","['sequences-and-series', 'calculus']"
2327447,Ideals generated by polynomials of the form $X_iY_j$ and $X_iY_j-Y_iX_j$ are radical?,"Given $n \ge 1$ a integer and $C_1,C_2 \subset \{1,\dots,n\} \times \{1,\dots,n\}$ disjoint subsets, construct 
$$I(C_1,C_2) \subset k[X_1,\dots,X_n,Y_1,\dots,Y_n]$$
the ideal generated by the set of polynomials
$$\{X_iY_j | (i,j) \in C_1\} \cup \{X_iY_j-Y_iX_j | (i,j) \in C_2\}.$$
Is $I(C_1,C_2)$ a radical ideal? If not, in which conditions does it fail to be? I came across this kind of ideal during my PhD thesis study, so I have no idea about the veracity of the affirmation.","['ideals', 'algebraic-geometry', 'commutative-algebra']"
2327538,Can we identify the limit of this arithmetic/geometric mean like iteration?,"Let $a_0 = 1$ and $b_0 = x \ge 1$. Let
$$
a_{n+1} = (a_n+\sqrt{a_n b_n})/2, \qquad b_{n+1} = (b_n + \sqrt{a_{n+1} b_n})/2.
$$ Numeric computation suggests that regardless of the choice of $x$, $a_n$ and $b_n$ always converge to the same value. Can we prove this? Moreover, if we assume this is true and define $f(x) = \lim_{n \to \infty} a_n$, then numeric computation shows that
$$
f'(1) \approx 0.57142857142857 \approx 4/7 \\
f''(1) \approx -8/49 \\
f^{(3)}(1) \approx 1056/4459 \\
f^{(4)}(1) \approx -65664/111475.
$$
There seems to be some patterns here.
Can we actually find the limit with these clues? I was actually trying to check this recursion
$$
a_{n+1} = (a_n+\sqrt{a_n b_n})/2, \qquad b_{n+1} = (b_n + \sqrt{a_{n} b_n})/2.
$$
I made a mistaked in my code and computed the one at the top. See The Computer as Crucible , pp 130.","['computer-algebra-systems', 'experimental-mathematics', 'analysis']"
2327547,"Let $f$ be a non-negative differentiable function on $[0,1]$.","Let $f$ be a non-negative differentiable function on $[0,1]$ such that $\int_{0}^{x} \sqrt {1-\{f'(t)\}^2}\ dt = \int_{0}^{x} f(t)\ dt$, $0 \le x \le 1$ and $f(0)=0$. Then which of the following is true? $(1)$ $f(\frac {1} {2}) < \frac {1} {2}$ and $f(\frac {1} {3}) > \frac {1} {3}$. $(2)$ $f(\frac {1} {2}) > \frac {1} {2}$ and $f(\frac {1} {3}) > \frac {1} {3}$. $(3)$ $f(\frac {1} {2}) < \frac {1} {2}$ and $f(\frac {1} {3}) < \frac {1} {3}$. $(4)$ $f(\frac {1} {2}) > \frac {1} {2}$ and $f(\frac {1} {3}) < \frac {1} {3}$. I have first differentiated the above given expression and obtained $\{f(t)\}^2 + \{f'(t)\}^2=1$ for all $t \in [0,1].$ Clearly from this expression it follows that $|f(t)| \le 1$ and $|f'(t)| \le 1$ for all $t \in [0,1].$ Since $f$ is non-negative on $[0,1]$ so we have $0 \le f(t) \le 1$ for all $t \in [0,1].$ Now let us construct a function $g$ on $[0,1]$ defined by $g(t)=f(t)-t$ , $t \in [0,1]$.Then clearly $g$ is differentiable on $[0,1]$ since $f$ is so. Now $g'(t)=f'(t)-1$ , for all $ t \in [0,1]$. Since $|f'(t)| \le 1$ for all $t \in [0,1]$ so we have $-2 \le g'(t) \le 0$ for all $t \in [0,1].$ This shows that the function $g$ is monotonic decreasing on $[0,1].$ So $g(\frac {1} {2}) \le g(0)=0$ and $g(\frac {1} {3}) \le g(0)=0$ since it is given that $f(0)=0.$ So we have $f(\frac {1} {2}) \le \frac {1} {2}$ and $f(\frac {1} {3}) \le \frac {1} {3}.$ Now from the above fact it is clear that $(1), (2)$ and $(4)$ are all false. Hence I think $(3)$ is the correct option. But I have confused about the strict inequality in option $(3)$ which I have failed to found. Please help me in this regard and also please mention the reason for holding strict inequality. Thank you in advance.","['derivatives', 'real-analysis', 'integration']"
2327555,Proving the general limit $\lim_{n\to\infty} \frac{an+c}{bn+d} = \frac{a}{b}$,"I'm trying to prove the general $$ \lim_{n\to\infty} \frac{an+c}{bn+d} = \frac{a}{b} $$ using the definition of the limit of a sequence, for any $\varepsilon > 0 \,\, \exists N \in \mathbb{N}$ s.t. for every $n \geq N$ $|a_n - a| < \varepsilon$. My (not so valiant) efforts: \begin{align*} 
\left| \frac{an+c}{bn+d} - \frac{a}{b} \right| &< \varepsilon \\
\left| \frac{abn + bc - abn - ad}{b^2n+bd} \right| &< \varepsilon \\
\left| \frac{bc-ad}{bn+d} \right| &< \varepsilon \left| b \right| \\
\left| bn+d \right| &> \left| \frac{bc - ad}{\varepsilon b} \right| \\
\left| bn+d \right| &> \left| \frac{c}{\varepsilon} - \frac{ad}{\varepsilon b} \right|
\end{align*} I'm used to being able to manipulate constants or remove absolute values when solving these kinds of problems, but with general constants I'm not sure what to do next.","['real-analysis', 'sequences-and-series', 'limits']"
