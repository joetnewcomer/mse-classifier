question_id,title,body,tags
3114176,Each group of order 8 has a subgroup of order 2 and a subgroup of order 4.,"So, I was trying to prove the following theorem: Let $G$ be a group of order $8$ . So $G$ has a subgroup of order $2$ and a subgroup of order $4$ . First I proved that if a group has a finite even order, it has an element $g_0$ of order $2$ . So $H=\{g_0,e\}$ is a group of order $2$ . Now I'm trying to find the group of order $4$ . By Lagrange theorem I can understand that: $$|G|=|H|\cdot |G\,:\,H| \Rightarrow |G\,:\,H|=4$$ So we have $4$ Cosets. But how to continue from here? I feel like I miss the last one-two lines of the proof.","['group-theory', 'abstract-algebra']"
3114199,Sum of three independent random variables,"Let $X$ , $Y$ , and $Z$ be independently and identically distributed variables, each uniformly distributed between $0$ and $2$ . What is the probability that $X+Y+Z\leq2$ ? All the other answers on similar questions refer to various things like ""convolutions"" that I've never heard of or use integrals and change the limits from $[-\infty,+\infty]$ to other things etc. so I'm very confused and don't understand any of it. In particular, I thought we could just work out $P(X+Y\leq2-Z)=\frac{1}{8}(2-Z)^2$ and thus do $\int_{0}^{2}\frac{1}{8}(2-Z)^2 \text{d}Z$ , and I don't understand why this doesn't give the right answer of $\frac{1}{6}$ . Please would someone take the time to write out all the steps with full explanation for someone who has only basic knowledge of probability.",['probability']
3114218,Machine learning book with robust linear algebra approach,"I am looking for machine learning book - neural network, deep learning etc etc - that use linear algebra in a robust manner. I found satisfactory the old book of Simon Haykin : Neural Networks : A Comprehensive Foundation -1998.
Do you know if exist a text book recently released in the same trace? 
Many thanks","['neural-networks', 'book-recommendation', 'reference-request', 'machine-learning', 'linear-algebra']"
3114227,"Why does $\int_a^b f(x)h'(x) \, \mathrm{d}x=0$ imply that $f$ is constant?","As an assignment, I have to prove the following: If $f(x)$ is a piecewise continuous function and $\int_a^b f(x)h'(x) \, \mathrm{d}x=0$ for all piecewise continuously differentiable $h(x)$ that satisfy $h(a)=h(b)=0$ , then $f$ is constant on $[a,b]$ . The assignment also provides some hints: Define $$c:=\frac{1}{b-a} \int_a^b f(x) \, \mathrm{d}x=\frac{1}{b-a} \sum_{i=1}^{m} \left ( \int_{x_{I-1}}^{x_i} f(x) \, \mathrm{d}x \right )$$ and use $$h(x)=\int_a^x f(s)-c \, \mathrm{d}s$$ Show that $$\int_a^b \left ( f(x)-c \right )h'(x) \, \mathrm{d}x=0$$ as well as $$\int_a^b \left ( f(x)-c \right )h'(x) \, \mathrm{d}x = \int_a^b \left ( f(x)-c \right )^2 \, \mathrm{d}x$$ and use this to conclude that $f(x)-c=0$ for all $x \in [a,b]$ . Now, from what I understand, $h(b)=0$ : $$h(x)=\int_a^b f(s)-c \, \mathrm{d}s=\int_a^b f(s) \, \mathrm{d}s-\left . cs \right |_{s=a}^b=(b-a)c-(cb-ca)=0$$ Obviously, $h(a)=0$ too. What I don't understand, is how I'm supposed to manipulate $\int_a^b \left ( f(x)-c \right )h'(x) \, \mathrm{d}x$ to obtain $\int_a^b \left ( f(x)-c \right )^2 \, \mathrm{d}x$ . I've tried numerous things (including integration by parts, which looks promising), but to no avail.","['integration', 'calculus', 'derivatives']"
3114233,Showing weakly continuous operators are continuous without using weak topology,"Let $X$ and $Y$ be Banach spaces, and let $T:X\rightarrow Y$ be a linear map such that $f\circ T$ is continuous for all $f\in Y'$ .  Show that $T$ is continuous. Now I think this problem is trivial once you have the notion of weak topology.  But without that notion, I'm not sure how to approach this.  I tried using the inequalities $|(f\circ T)(x)|\leq(||f\circ T||)(||x||)$ and $|(f\circ T)(x)|\leq(||f|||)(||Tx||)$ , but I don't see how that gets us any closer to find what $||Tx||$ is less than or equal to.  Maybe a judicious choice of $f$ would help?","['banach-spaces', 'weak-convergence', 'operator-theory', 'normed-spaces', 'functional-analysis']"
3114253,Infinity matrix norm is maximum row sum norm,"I want to prove that the infinity matrix norm is maximum row sum norm.
I've shown that for $\|x\|_{\infty}=1$ $$||Ax||_{\infty} =  \max_{i}\left|\sum^n_{j=1}a_{ij}x_j \right| \leq \max_{i}\sum^{n}_{j=1} |a_{ij}|\|x\|_{\infty}= \max_{i}\sum^{n}_{j=1} |a_{ij}|.$$ Now I need to show that there exists vector $x$ with $\|x\|_{\infty}=1$ for which this inequality becomes equality. And I'm stuck here. How do I proceed? What is the correct $x$ ?",['linear-algebra']
3114283,Two definitions of a connection,For me a connection $\nabla$ on a vectorbundle $E$ over a smooth manifold $M$ is a $\mathbb{R}$ -bilinear map $\Gamma(TM)\times\Gamma(E)\rightarrow\Gamma(E)$ which is tensorial in the first slot and satisfies the Leibniz rule in the second. Now i have seen a different definition that a connection is a ${R}$ -linear map $\nabla : \Gamma(E) \to \Gamma(E\otimes T^*M)$ which also satisfies some Leibniz rule. My question is: What exactly is the bundle $\Gamma(E\otimes T^*M)$ and how are these definitions equivalent. Thanks in advance!,"['vector-bundles', 'riemannian-geometry', 'differential-geometry']"
3114318,Computing the product $(\frac{d}{dx}+x)^n(-\frac{d}{dx}+x)^n$,"I want to compute the product $$
(\frac{d}{dx}+x)^n(-\frac{d}{dx}+x)^n,
$$ for a natural number $n$ . For $n$ equal to 0 or 1, the computation is very simple but for such a low number as 2 the brute force calculation begins to be rather cumbersome and I cannot see any pattern emerging. I tried to find some connection with the Rodrigues' formula for the Hermite polynomials but I could not. These operators come up in the algebraic approach to the quantum harmonic oscillator . Explicit Example To avoid any misunderstanding, I am going to show explicitly the computation for the case $n=1$ : $$
(\frac{d}{dx}+x)(-\frac{d}{dx}+x)=-\frac{d^2}{dx^2}+1+x\frac{d}{dx}-x\frac{d}{dx}+x^2=-\frac{d^2}{dx^2}+x^2+1.
$$ One can think of a function $f$ the operators are acting on. For example, $$
(\frac{d}{dx}\circ x) f= (\frac{d}{dx}x)f+x\frac{d}{dx}f=(1+\frac{d}{dx})f,  
$$ then $$
\frac{d}{dx}\circ x=1+x\frac{d}{dx}
$$","['quantum-mechanics', 'derivatives', 'operator-theory', 'linear-algebra']"
3114319,$f(x)/x \to l$ and $f''(x) = O(1/x)$,"$f \in C^2(\mathbb{R}, \mathbb{R})$ such that $f(x)/x \to l \in \mathbb{R}$ as $x \to + \infty$ , and such that $f''(x) = O(1/x)$ at $+\infty$ . Find : $$\lim_{x \to +\infty} f'(x)$$ Some thoughts : It seems to me that the limit is $l$ , but I am unable to prove it. Moreover it seems that we have $f''(x) = o(1/x)$ . I think I need somehow to relate the derivatives to each other, maybe using  the series expansion of $f$ , but it doesn't seem to work. Thank you!","['integration', 'limits', 'sequences-and-series', 'real-analysis']"
3114323,Is $0$ the only vector in the kernel of every bounded linear functional?,"Let $X$ is a normed vector space, and let $x_0\in X$ have the property that for every bounded linear functional $f:X\rightarrow K$ , $f(x_0)=0$ .  Then does $x_0=0$ ? I think the answer is clearly yes, but I'm not sure how to prove it.    How do you construct a bounded linear functional which is nonzero on a given nonzero vector?","['operator-theory', 'normed-spaces', 'linear-algebra', 'functional-analysis', 'linear-transformations']"
3114336,"How to prove that $f(x) =1/x$ is unbounded in $(0,1)$?","Let $f(x) = 1/x$ for all $x\in (0,1)$ By assuming $f(x)$ is bounded or in any other way but without using limits I assumed that $f$ is bounded above, then there is $M>0$ s.t $f(x) \leq M$ for all $x$ in $(0,1)$ . $$1/x \leq M \implies  x \geq 1/M$$","['functions', 'real-analysis']"
3114397,T shaped tetris figures on a plane,"I am just wondering how many (and by how many I mean countably or uncountably many) T shaped figures can we place on a XY plane. I assume that that T consists of 2 perpendicular lines and has 0 area. It is easy to understand for L shaped figures or U shaped ones, one can place uncountably many of those. But I am unable to construct a similar bijection for T shaped ones. I believe that it should be countable and some sort of diagonal argument should be made but I don't know how. A reasonable followup is for the X shaped ones. How many of those can we fit on a plane?","['geometry', 'packing-problem', 'general-topology', 'problem-solving', 'set-theory']"
3114416,Gale's Theorem Analog for smaller caps,"Let $S^k$ denote the unit hypersphere in $\mathbb{R} ^{k+1}$ . For a point $a\in S^{k}$ let $H_{\epsilon}(a):= {\{ x | \langle x,a \rangle > \epsilon}\}$ . If $\epsilon =0$ , then $H_{0}(a)$ is simply the open hemisphere centered at the point $a$ . Gales Theorem states that if $n$ and $k$ are positive integers, then there is a set $V \subset S^k$ with $2n+k$ elements such that $|H_{0}(a) \cap V| \geq n$ for each $a \in S^k$ . I'm interested in extensions of this theorem for ""smaller caps"" - i.e. $\epsilon >0$ .  That is, I would like to obtain something of the form: If $n$ and $k$ are positive integers, and $\epsilon$ >0 then there is a set $V \subset S^k$ with $2n+k$ elements such that $|H_{\epsilon}(a) \cap V| \geq f(n,k,\epsilon$ ) for each $a \in S^k$ . I'm primarily interested in the case where $\epsilon$ is small $\approx \Theta \frac{1}{\sqrt{k}}$ Any pointers or references would be greatly appreciated. Upper or lower bounds are also of interest.","['geometry', 'reference-request', 'geometric-topology', 'linear-algebra', 'combinatorics']"
3114445,Understanding the proof $C(X)$ is complete,"Prove $(C_X,||.||)$ ,where $||.||$ is the maximum norm and X is compact, is complete. The following proof was given. It is the one I am striving to understand: Let $(f_n)$ be a Cauchy sequence: $\forall\epsilon>0\exists N\in\mathbb{N}:n,m\geqslant N\implies ||f_m-f_n||<\epsilon$ $\forall t\in X$ $0\leqslant |f_n(t)-f_m(t)|\leqslant \max_{x\in X}|f_n(x)-f_m(x)|\to 0$ as $m,n\to\infty$ $\forall t\in X\:, (f_n(t))_{n\in\mathbb{N}}$ is a Cauchy sequence in $\mathbb{R}$ . Then $(f_n)_n\to f$ uniformly then $f$ is continuous. This is how the proof was handed to me. I think I can fill the gaps but I would need someone to back me on that. So first the author considers a Cauchy sequence and assumes it converges in $C(X)$ Then it arrives to the following inequality: $0\leqslant |f_n(t)-f_m(t)|\leqslant \max_{x\in X}|f_n(x)-f_m(x)|\to 0$ as $m,n\to\infty$ since the it assumed $\max_{x\in X}|f_n(x)-f_m(x)|\to 0$ then $|f_n(t)-f_m(t)|\to 0$ So the convergence in $C(X)$ verifies that the same Cauchy sequence converges in $\mathbb{R}$ that is by assumption complete with the usual topology.
Since $X$ is compact then $f_n$ converges uniformly in $\mathbb{R}$ and it converges to a continuous function. Therefore it converges in $C(X)$ proving the latter is complete. Question : Is this the reasoning behind the proof? Thanks in advance!","['proof-explanation', 'proof-writing', 'analysis', 'real-analysis', 'functional-analysis']"
3114448,Riemann Sphere Mapping,"this is my first post so sorry if my question is too vague. I didn't see a related question posted, hence why I'm asking. I can't find any resources on it, but there's supposed to be a bijective mapping from the complex plane to the reimann sphere, correct? The only mapping I've seen is by creating a line from a point in the complex plane to the top of the sphere and the intersection point with the sphere is the function value. How can this be expressed, and how is it injective? On a intuitive note, how can the complex plane be isomorphic to a sphere? I'd think you could 'unfold' the sphere which would create a finite plane hence not being injective.","['complex-analysis', 'complex-numbers']"
3114457,Find if terms are terms of the same arithmetic progression,"Is it possible that numbers $\frac{1}{2}, \frac{1}{3}, \frac{1}{5}$ are (not necessarily adjacent) terms of the same arithmetic progression?
  Hint: Yes. Try $\frac{1}{30}$ as a difference. I  was going back and forth how they found out that difference. My idea was since we have an arithmetic sequence defined as $a, a+d, a+2d,...$ I thought I could solve for the difference $d=\frac{1}{30}$ . Since: $$\frac{1}{3} = \frac{1}{2}+nd$$ And $$\frac{1}{5} = \frac{1}{3}+md$$ Then $nd = \frac{1}{3} - \frac{1}{2} = -\frac{1}{6}$ and $md = \frac{1}{5} - \frac{1}{3} = -\frac{2}{15}$ Since it is also part of the same sequence we can find: $$nd + md = -\frac{1}{6} -\frac{2}{15} = -\frac{3}{10}$$ Now I'm stuck since I can't see how this brings me any closer to find $m, n, d$ .","['algebra-precalculus', 'sequences-and-series']"
3114485,"Find $a,b$ at $f(x)=\frac{x^2+x-12}{x^2-ax+b}$","An High school question: Given : $$f(x)=\frac{x^2+x-12}{x^2-ax+b}$$ it's given that $x=3$ is a vertical asymptote find $a$ and $b$ . I tried: Since $x=3$ is a vertical asymptote then $3^2-3a+b=0$ , but now what","['calculus', 'functions', 'algebra-precalculus']"
3114498,"Simplify [p∧ (¬(¬p v q)) ] v (p ∧ q) so that it become p, q, ¬p, or ¬q","Had a question on a test that asked for us to simplify (using rules of inference) the following proposition: [p∧ (¬(¬p v q)) ] v (p ∧ q) to p, q, or their negation (¬p, ¬q). Here is what I did: 1) [p∧ (¬(¬p v q)) ] v (p ∧ q) 2) [p∧ (p ∧ ¬q)) ] v (p ∧ q)  DeMorgan's Law 3) [(p∧ p) ∧  ¬q] v (p∧ q) Associative law 4) [p ∧  ¬q] v (p∧ q) Idempotent law Step 4 is where I got stuck. I didn't know what to do afterwards. So I substituted S= (p∧ q) and did distributive law 5) [p ∧  ¬q] v S 6) (S v p) ∧ (S v ¬q) Distributive law 7) [ (p∧q) v p ] ∧ [ (p∧q) v ¬q] Substitute S=(p∧q) back in 8) [ (pvp) ∧ (pvq) ] ∧ [ (¬q v p) ∧ (¬q v q) ] Distributive law 9) [ p ∧ (pvq) ] ∧ [ (¬q v p) ∧ T ] Idempotent law and Domination law 10) p  ∧ (¬q v p) Absorption and Identity law I stopped there. I didn't know how to further simplify step 10 into p, q, ¬p, or ¬q. I was thinking maybe absorption law? But  p ∧ (¬q v p) is not the same as  p ∧ (q v p), so I couldn't simplify it to p, correct? Can someone help me further simplify it? Do you think I will get most the marks for this question or was my approach completely wrong?","['boolean-algebra', 'propositional-calculus', 'logic', 'discrete-mathematics']"
3114511,Radical equation - can I square both sides with more than 1 radical on one side?,"I'm familiar with equations like: $\sqrt{x+1} - \sqrt{x+2} = 0 $ Has no solutions, it's just an example off the top of my head Just move the negative square root to the other side, square both sides and solve. $\sqrt{x+1} = \sqrt{x+2}$ $x+1 = x+2$ 0 = 1 My question is, if there are two square roots on one side, then can I still square both sides in this way: $\sqrt{x+1} - \sqrt{x+2} = \sqrt{x+3}$ $x+1 - (x+2) = x+3$ $x+1 - (x-2) = x+3$ $x = -4$ Or does squaring both sides cause something strange to happen on the left hand side?","['algebra-precalculus', 'radicals']"
3114515,Density of integers $n$ with all prime factors of order $O(\log n)$?,"For a rational integer $n \in \mathbb{Z}_{+}$ , let $\mathfrak{p}(n)$ denote the set of (distinct) prime factors of $n$ . Then for a positive constant $c$ , let $$f(x) = \vert\{n\in\mathbb{Z}_{+}:\ n\leq x,\textit{ and } \max{\mathfrak{p}(n)} \leq c \log n\}\vert.$$ I.e. $f(x)$ is the number of integers $n$ below $x$ such that there largest prime factor is bounded by $c\log n$ . Is the density of such numbers $f(x)/x$ known? Could you refer me to some relevant results on this? I am aware of some density results on B-smooth numbers but that's not what I'm looking for.","['analytic-number-theory', 'number-theory', 'prime-factorization', 'prime-numbers']"
3114533,"If $Q$ is a proper orthogonal transformation matrix, deduce that $\det(1-Q)=0$.","Show that if $Q$ is orthogonal transformation matrix, then $Q^t(Q-1)=(1-Q)^t$ . Deduce that if $Q$ is also proper, then $\det(1-Q)=0$ . Hence show that transformation has nonzero vector that has the same components in both coordinate system. I tried to solve this problem.I think I got the first part right, $$Q^t(Q -1)= Q^t  Q- Q^t=1- Q^t=(1- Q)^t$$ The second part, $$-Q ^t(1-Q)=(1-Q)^t$$ $$\det(-Q^t)\det(1-Q)=\det((1-Q)^t$$ $$(-1)^n\det(Q)\det(1-Q)=\det((1-Q)^t)$$ since the orthogonal matrix is proper which means $\det(Q)=1$ and for any matrix, its determinant equals the determinant of its transpose. $$(-1)^n\det(1-Q)=\det(1-Q)$$ So, it's always true for $\det(1-Q)=0$ But that's not what the question asks. I haven't done linear algebra for a while and I am not sure from the concepts I used, so I would be glad if you clarify any mistake I made.","['orthogonality', 'linear-algebra']"
3114547,Is an element of a vector a scalar or 1x1 vector or 1x1 matrix?,"In econometrics, by Hayashi, they defined the error vector of n observations in a $ (n \times K)$ regression funcntion as: $\epsilon = \begin{bmatrix}\epsilon_{1} \\\epsilon_{2} \\\vdots \\\epsilon_{n}\end{bmatrix}$ , where $\epsilon_{i}$ is the ith observation's error term, and K-dimension x vector of the ith observation as, $x_{i} = \begin{bmatrix}x_{i1} \\x_{i2} \\\vdots \\x_{ik}\end{bmatrix}$ The book says that the cross moment of two random variables E[xy] is zero means that these two random variables are orthogonal. It's not hard to see the point using [0,1] and [1,0] to check their cross product for orthogonality. But in the book it has a formula for strict exogeneity assumption: $E[x_{j}\epsilon_{i}] = \begin{bmatrix}x_{j1}\epsilon_{i} \\x_{j2}\epsilon_{i} \\\vdots \\x_{jk}\epsilon_{i}\end{bmatrix} = 0_{(K\times1)}$ So, here the $\epsilon_{i}$ is an element of the $\epsilon$ vector, and the cross moment for a $(k\times1)$ vector and an element of a vector, which are orthogonal, is a $(k\times1)$ 0 vector. My question is that, what is a cross moment of two random variables, is it the expected value of the inner product? And shall I view the $\epsilon_{i}$ as a scalar, or $(1\times1)$ vector, or $(1\times1)$ matrix? Thank you very much.","['statistics', 'vectors', 'vector-spaces', 'economics', 'linear-algebra']"
3114563,Evaluate the indefinite integral,"$ I = \int (x^2 + 2x)\cos(x) dx $ Integration by Parts, choose $u$ : $$\begin{align}
u &= \cos(x) \\
dv &= (x^2 + 2x)dx \\
du &= -\sin(x) \\
v &= \frac{1}{3}x^3 + x^2
\end{align}
$$ Substitute into formula: $$
\begin{align}
\int udu &= uv - \int vdu \\
&= \cos(x)\left(\frac{1}{3}x^3 + x^2\right) - \int\left(\frac{1}{3}x^3 + 2x\right)(-\sin(x)) \\
&= \cos(x)\left(\frac{1}{3}x^3 + x^2\right) + \int\left(\frac{1}{3}x^3 + 2x\right)(\sin(x))
\end{align}
$$ At this point, it doesn't look like I can use the substitution rule on the the right hand integral, so I decide to use the substitution rule again. Integration by Parts II, choose $u$ : $$\begin{align}
u &= sinx \\
dv &= (\frac{1}{3}x^3 + 2x)dx \\
du &= cosx \\
v &= \frac{1}{12}x^4 + x^2
\end{align}
$$ Substitute into formula: $$\begin{align}
\int_{}udu &= uv - \int_{}vdu \\
&= (sinx)(\frac{1}{12}x^4 + x^2) - \int_{} (\frac{1}{12}x^4 + x^2)(cosx)dx
\end{align}
$$ Combining the two integration by parts together and I feel like I am no closer to evaluating the integral than whence I started...The integral is still there and I feel another parts by integration won't work. $$\int(x^2 + 2x)\cos(x) = (\cos(x))\left(\frac{1}{3}x^3 + x^2\right) + (\sin(x))\left(\frac{1}{12}x^4 + x^2\right) - \int\left(\frac{1}{12}x^4 + x^2\right)(\cos(x))dx$$ Did I do the math wrong and make a mistake somewhere? Or am I supposed to approach this differently?","['indefinite-integrals', 'calculus']"
3114592,"How can I prove that $m!|(k+1)(k+2)...(k+m)$, where $m$ is a positive integer and $k$ is bigger or equal to $0$ [duplicate]","This question already has answers here : Show that the product of any $m$ consecutive positive integers is divisible by $m!$ [duplicate] (3 answers) Closed 5 years ago . I took a course of discrete mathematics the last semester. There were some exercises in the book that we used that where really hard, but I did the majority of them. the problem above is one that I have not been able to do. I would like to see the proof so it can help me to solve further problems.
 sorry for my english.","['divisibility', 'logic', 'discrete-mathematics']"
3114613,Find volume of solid using shell method,"Use the method of cylindrical shells to find the volume of the solid obtained by rotating the region bounded by the curves $y=x^2$ , $y=0$ , $x=1$ , and $x=2$ about the line $x=4$ . Here is what I attempted with the shell method, but it is clearly wrong:","['integration', 'calculus', 'functions']"
3114619,Name for multi-valued analogue of a limit,"Let the ""multilimit"" $L_{x \to v}f(x)$ denote a set of values that $f(x)$ can approach as $x$ approaches $v \in \mathbb{R}$ . More formally, $$ \xi \in L_{x \to v}f(x) \stackrel{\text{def}}{\iff} \exists s \in \text{Seq}[\mathbb{R}] . \left(\lim_{t \to \infty} s_t = v \right) \land \left(\lim_{t\to\infty} f(s_t) = \xi\right) \tag{101} $$ This bizarre higher-order function of sorts, $L$ , has the nice property of being total. The multilimit is a singleton if and only if the limit exists, but there's no harm in having an empty or non-singleton multilimit. I'm curious whether there's an established usage of a construction like this. A practical application to me seems to be defining a differential operator that always succeeds and can handle isolated poles (like in $\frac{1}{x}$ ) (102). $$ D(f)(x) \stackrel{\text{def}}{=} L_{h \to v} \frac{f(x+h)-f(x-h)}{2h} \tag{102} $$ I've tried searching for something resembling a multivalued limit or for something resembling a multivalued generalization of the derviative but have only found the subderivative so far, which seems similar but doesn't directly tackle totality.",['limits']
3114631,"Lift $O(\mathbb{Z}/p\mathbb{Z})$ to ""something"" in $O(\mathbb{Z}/p^2\mathbb{Z})$","So the question was from a result of Serre which basically says if $H$ is a closed subgroup of $Sp_{2n}(\mathbb{Z}_p)$ that maps surjectively onto $Sp_{2n}(\mathbb{Z}/p\mathbb{Z})$ , then $H=Sp_{2n}(\mathbb{Z}_p)$ . It seems natural to believe a similar result would also hold for orthogonal group $O_n$ , and this is essentially asking: If H is a subgroup of $O_{n}(\mathbb{Z}/p^2\mathbb{Z})$ that maps onto $O_{n}(\mathbb{Z}/p\mathbb{Z})$ , would this imply $H=O_{n}(\mathbb{Z}/p^2\mathbb{Z})$ ?  Can anyone point out if this claim is known to be true or false? My thought was that if denote the kernel of $O_{n}(\mathbb{Z}/p^2\mathbb{Z}) \twoheadrightarrow O_n(\mathbb{Z}/p\mathbb{Z})$ by $\Gamma$ , then $\Gamma=\{E+pM |M^\intercal+M=0\}$ is abelian, and it turns out that $\Gamma$ as $\mathbb{F}_p[O_n(\mathbb{Z}/p\mathbb{Z})]$ module is irreducible so $\Gamma 
 \cap H=E$ , which also implies $H\Gamma= O_n(\mathbb{Z}/p\mathbb{Z})\rtimes \Gamma=O_{n}(\mathbb{Z}/p^2\mathbb{Z})$ . This looks pathological but I couldn't tell myself why.","['orthogonality', 'group-theory', 'abstract-algebra', 'finite-groups']"
3114641,projection from a point to a constrained hyperplane,"I am trying to find the closest point on the following constrained hyperplane to a general point $\vec x$ : $$ \vec \omega \!\cdot\! \vec 1 = 1  \ \ s.t \ \ \alpha_i \le\omega_i\leq\beta_i $$ $$ 0\leq\alpha_i\lt\beta_i\leq1$$ I have projected $\vec x$ onto the plane. Next, I think I should find the midpoint of the constrained hyperplane and perform an iterative loop (am writing code) to get as close to the center until all the inequalities are satisfied. Will this work? , and how can I find the midpoint of the constrained hyperplane? I can find the midpoint in 2d and 3d but I can't see how it generalizes to higher dimensions.","['projection', 'geometry', 'constraints', 'constraint-programming']"
3114691,How to find $\lim\limits_{n\to\infty} A^n$?,"Given $4\times4$ matrix, $$A=
\begin{bmatrix}
0.4&0.3&0.2&0.1\\
0.1&0.4&0.3&0.2\\
0.3&0.2&0.1&0.4\\
0.2&0.1&0.4&0.3
\end{bmatrix}
.$$ Can we prove that $$\lim\limits_{n\to\infty} A^n=
\begin{bmatrix}
0.25&0.25&0.25&0.25\\
0.25&0.25&0.25&0.25\\
0.25&0.25&0.25&0.25\\
0.25&0.25&0.25&0.25
\end{bmatrix}$$ manually? I tried to find $A^2$ , and $A^3$ for a long long time. Any ways to find $\lim\limits_{n\to\infty} A^n$ ?","['matrices', 'linear-algebra']"
3114734,Distributing 5 distinct balls into 3 distinct boxes,"Suppose $5$ distinct balls are distributed into $3$ distinct boxes such that each of the $5$ balls can get into any of the $3$ boxes. What is the Probability that exactly one box is empty. Also What is the best Probability that all the boxes are occupied. For the first part my answer comes out to be $$\frac{3\cdot2^5}{3^5}$$ Logic: $3$ choices for each ball $= 3^5$ . And $3$ ways of choosing the box that will be empty ; Each balls has then $2$ choices of  going in remaining of the $2$ boxes. Is it correct? For the Second Part, I think Collectors Problem will be used. Can anyone please confirm it? P. S I am new here so any feedback regarding the way of putting questions is welcome :).","['permutations', 'statistics', 'combinatorics', 'probability-theory', 'probability']"
3114735,Proving $\lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}=f''(a)$,"The following question can be found in Bartle, Introduction to real analysis as well as Walter Rudin, Principle of Mathematical analysis: Let $f$ be differentiable function defined on an open interval $I$ Suppose $f''(a)$ exist at $a \in I$ Then $\lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}=f''(a)$ The first approach suggested by the Rudin is applying L'hopital's rule to $\lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}$ This yields $\lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}=\lim_{h\to0}\frac{f'(a+h)-f'(a-h)}{2h}=\lim_{h\to0}\frac{f'(a+h)-f'(a)+f'(a)-f'(a-h)}{2h}=f''(a)$ And thus proving the statement. However, I encounter a difficulties when I tried to prove the statement by mean value theorem. Here is my attempt. By mean value theorem, $\exists c_2 \in (a,a+h)$ such that $f(a+h)-f(a)=hf'(c_2)$ . Similarly, $\exists c_1 \in (a-h,a)$ such that $f(a)-f(a-h)=hf'(c_1)$ Hence $f(a-h)-f(a)=-hf'(c_1)$ Therefore, we have $\frac{f(a+h)-2f(a)+f(a-h)}{h^2}=\frac{f'(c_2)-f'(c_1)}{h}=\frac{f'(c_2)-f'(a)+f'(a)-f'(c_1)}{h}$ As $f''(a)$ exists, $\forall \epsilon>0,\exists\delta>0$ such that $|\frac{f'(x)-f'(a)}{h}-f''(a)|<\epsilon$ when $ |x-a|<\delta$ Hence if $|h|<\delta$ then $|c_2-a|<\delta,|c_1-a|<\delta$ and we get $lim_{h \to 0}\frac{f'(c_2)-f'(a)+f'(a)-f'(c_1)}{h}=2f''(a)$ which is inconsistent with the statement. Can anyone tell me about what is wrong in my attempt.
Thank you.","['limits', 'calculus', 'derivatives', 'real-analysis']"
3114739,Solve nonlinear first order ordinary differential equation,"Given nonlinear first order ordinary differential equation \begin{eqnarray}
\dfrac{dx_1}{dt}&=&4x_1\left(1-\dfrac{x_2}{2}\right)\\
\dfrac{dx_2}{dt}&=&3x_1\left(\dfrac{x_2}{3}-1\right)\\
\end{eqnarray} with initial value $x_1(0)=3$ and $x_2(0)=5$ , $0\leq t\leq 1$ . I can't solve that because the system is nonlinear. I only studied to solve linear system of ODE. How to solve that nonlinear first order ordinary differential equation? \begin{eqnarray}
 \dfrac{dx_1}{dt}&=&4x_1\left(1-\dfrac{x_2}{2}\right)\\
 \dfrac{dx_2}{dt}&=&3x_1\left(\dfrac{x_2}{3}-1\right)\\
 \end{eqnarray} \begin{eqnarray}
\dfrac{\dfrac{dx_1}{dt}}{\dfrac{dx_2}{dt}}=\dfrac{4x_1\left(1-\dfrac{x_2}{2}\right)}{3x_1\left(\dfrac{x_2}{3}-1\right)}\\
\dfrac{dx_1}{dx_2}=\dfrac{4\left(\dfrac{2-x_2}{2}\right)}{3\left(\dfrac{x_2-3}{3}\right)}\\
\dfrac{dx_1}{dx_2}=\dfrac{2(2-x_2)}{x_2-3}\\
x_1=\int\dfrac{2(2-x_2)}{x_2-3}dx_2
\end{eqnarray}",['ordinary-differential-equations']
3114746,U-Substitution Intuition,"I've had a very hard time wrapping my mind around u-substitution. I understand how the chain rule applies with the following intuition: Say I have some car whose position function is defined as: $x=3t$ Now say we have another car whose position changes with respect to the first cars position with the following equation: $y=(x)^2$ . I understand that when taking the derivative of the second car with respect to time, we would take the derivative of $y$ with respect to $x$ and get $\frac{dy}{dx}= 2x$ . To take the derivative of $x$ with respect to $t$ and get $\frac{dx}{dt}= 3$ . Now to find the derivative of $y$ with respect to $t$ we would multiply both quantities (I know it's not 100% formal, but it is intuitive) to get $\frac{dy}{dt} = 2x*3 = 6x$ . Then we replace this x with the position defined by $3t$ (because we are talking about $t$ here, and we don't need an $x$ : $\frac{dy}{dt} = 18t$ . However, I'm having a hard time extending this argument to an integral. I know how to do u-substitution, but I can't intuitively understand it. Especially this: why can't dx=du if they are both approaching 0? Can someone please walk me through an intuitive explanation? Thanks","['integration', 'calculus', 'intuition']"
3114759,Must an equation contain at least one variable? Can we call 1+1=2 an equation?,"According to the Wikipedia and Encyclopædia Universalis, an equation must contain at least one variable but there is no such condition mentioned in other definitions. Thus can we call the following equalities equations? Columbia Encyclopedia says yes but this contradicts with Wikipedia and Encyclopædia Universalis definitions. $$1+1=2$$ $$9+4=13$$ In mathematics, an equation is a statement of an equality containing one or more variables. - Wikipedia The original 2 citations mentioned in the Wikipedia article are mentioned later in this question Equation, Statement of equality between two expressions consisting of variables and/or numbers. - Encyclopedia Britannica An equation is a mathematical expression stating that two or more quantities are the same as one another - Wolfram Mathworld a mathematical statement in which you show that two amounts are equal using mathematical symbols - Cambridge Dictionary A statement that the values of two mathematical expressions are equal (indicated by the sign =) - Oxford Dictionary Equation, in mathematics, a statement, usually written in symbols, that states the equality of two quantities or algebraic expressions, e.g., x+3=5. (...) A numerical equation is one containing only numbers, e.g., 2+3=5 - Columbia Encyclopedia, 6th ed The Wikipedia definition cites 2 different sources. I will quote them here: An equation is an equality between two mathematical expressions, therefore a formula of the form A=B, where the two members A and B of the equation are expressions in which one or more variables , represented by letters, appear - Encyclopædia Universalis , French-language general encyclopedia published by Encyclopædia Britannica, Inc (Translated by Google Translate, emphasis mine.) ""A statement of equality between two expressions. Equations are of two types, identities and conditional equations (or usually simply ""equations"")"". « Equation », in Mathematics Dictionary , Glenn James [de] et Robert C. James [de] (éd.), Van Nostrand, 1968, 3 ed. 1st ed. 1948, p. 131. So, I am still confused. The first definition of the above two definitions, says that there must be a variable and the 2nd one has no such condition. The reason, I am asking the question because, in India, some popular textbooks have mentioned that an equation must contain a variable. Here is the definition used in the NCERT class 7 mathematics book (Page 79). Here is another Government published book, WBBSE class 7 mathematics textbook (language: Bengali) where they instructed the students to find out which of the followings are equations and which are not. In the solutions, they didn't consider (f) and (g) as equations. People having the idea that an equation must contain an unknown, can be found often though. For example, let's consider this similar unanswered question on this forum. There are only 2 comments and they contradict each other. Also, this question have some answers where the users believe that an equation should have an unknown. An equation is meant to be solved, that is, there are some unknowns You solve an equation, while you evaluate a formula.","['algebra-precalculus', 'definition', 'terminology']"
3114791,Limit of $\lim \limits_{x \to\infty} (\frac{x}{1-x})^{2x}$,I've been struggling with this term for a while now: $$\lim \limits_{x \to\infty} (\frac{x}{1-x})^{2x}$$ I know it has to do something with $\lim \limits_{x \to\infty} (1+\frac{n}{x})^x=e^n$ but didn't come further than this though: $$\lim \limits_{x \to\infty}  (1+ \frac{2x-1}{1-x})^{2x} = \lim \limits_{x \to\infty} (1+ \frac{2x-1}{1-x})^{1-x})^{\frac{2x}{1-x}} \overset{?}{=} e^{2x-1 {\lim \limits_{x \to\infty} \frac{2x}{1-x}}}  $$,"['limits', 'limits-without-lhopital', 'real-analysis']"
3114815,Derivative with respect to diagonal of diagonal matrix,"Suppose I have a diagonal matrix $\pmb{D}$ and a symmetric matrix $\pmb{X}$ that is not a function of $\pmb{D}$ , and I wish to find the following derivative: $$
\frac{\partial}{\partial \mathrm{diag}(\pmb{D})} \mathrm{vec}\left(\pmb{D}\pmb{X}\pmb{D}\right),
$$ in which $\mathrm{diag}(\pmb{D})$ represents the diagonal of $\pmb{D}$ . I know the following derivative: $$
\frac{\partial}{\partial \mathrm{vec}(\pmb{D})} \mathrm{vec}\left(\pmb{D}\pmb{X}\pmb{D}\right) = \left(\pmb{D}\pmb{X} \otimes \pmb{I} \right) + \left(\pmb{I} \otimes \pmb{X}\pmb{D}\right)
$$ So I guess I can find the answer by multyplying this with $\frac{\partial \mathrm{vec}\left( \pmb{D} \right)}{\partial \mathrm{diag}(\pmb{D})}$ , which should be some straightforward matrix with zeroes and ones. So my question is, (a) does this matrix have a name and can it easily be determined? and (b) isn't there some simpler way to do this?","['vectors', 'matrices', 'jacobian', 'matrix-calculus', 'derivatives']"
3114832,language kleene star union not equal to union of language kleene star,"Find languages A and B such that $A^* \cup B^* \neq (A \cup B)^*$ . Is this even possible? I tried: $A:\{$ $\epsilon $ $\}$ and B: $\{$ $1$ $\}$ $A^*= \{\epsilon \}$ and $B^*=\{\epsilon,1,11,111,....\}$ $A \cup B = \{ \epsilon , 1\}$ $A^* \cup B^*= \{\epsilon, 1, 11, 11,111,.....\}$ $(A \cup B)^*=\{\epsilon, 1, 11, 11,111,.....\}$ because lets say $C=(A \cup B)$ and $C=\{ \epsilon,1 \}$ , so $C^*=\{\epsilon, 1, 11, 11,111,.....\} $ which is exactly not what I want. Problem I have is whatever $A \cup B$ outputs, they will get the kleene star outside of the parenthesis which will basically make it the same as the union of kleene stars. Also kleene star will add $\epsilon$ to the resulting languages so I can't trick it by using that. Is my logic flawed?","['elementary-set-theory', 'regular-expressions', 'formal-languages', 'regular-language']"
3114835,Doubt in Understanding Lebesgue measure.,"I am studying Measure theory form Stein and Shakarachi:Real Analysis. I come across observation regarding the outer measure. For any $E\in R^d$ $m_*(E)=\inf m_*(O)$ where $O $ is the open set containg E. i.e $\forall \epsilon>0 \exists O\in R^d$ such that $m_*(O)<m_*(E)+\epsilon$ and $E\subset O$ . Also By Lebesgue measurable set definition E is said to be Lebesgue measurable iff $\forall \epsilon>0,\exists O\in R^d$ such that $m_*(O\setminus E)<\epsilon$ and $E\subset O$ From this, I thought both definitions are the same. then every set in $R^d$ become Lebesgue measurable which is of course not true .where is my interpretation fails? When it is possible that $m_*(O \setminus E)\neq m_*(O)-m_*(E)$ Please help me out to solve this misinterpretation. Any help will be appreciated","['measure-theory', 'examples-counterexamples', 'analysis', 'real-analysis']"
3114836,"If $\gamma\colon[a,b]\to\mathbb{C}$ is continuous and $\gamma(b)=-\gamma(a)$, must the curves $\gamma$ and $e^{ic}\gamma$ intersect for all real $c$?","If $[a, b]$ is a compact interval of $\mathbb{R}$ and $\gamma: [a, b] \to \mathbb{C}$ is continuous, denote the connected,
compact set $\gamma([a, b])$ by $[\gamma]$ . If $h$ is a complex
number of unit modulus, define the curve $h\gamma \colon [a, b] \to \mathbb{C}$ by $(h\gamma)(t) = h(\gamma(t))$ . (If $h = -1$ , write $h\gamma$ as $-\gamma$ .) If $\gamma(b) = -\gamma(a)$ , must we have $[\gamma] \cap [h\gamma] \ne \emptyset$ ? This trivially true for $h = \pm1$ , and it has been proved (although not
easily) for $h = \pm i$ . We can assume without loss of generality
that $h = e^{ic}$ , where $0 < c < \pi$ . The question for the case $h = \pm i$ was first asked by
Herman Tulleken, with the proviso that an answer was allowed to make
almost arbitrary special assumptions about $\gamma$ , subject only to
applicability to a particular problem related to polyominoes. Accordingly, @YiFan asked the question again,
still for the case $h = \pm i$ , but this time only specifying that $\gamma$ is
injective (and of course continuous), but should otherwise be
arbitrary.  @Jens observed, in a comment on this question, that the
result seemed to be true for a general rotation, not just $90^\circ$ . I had made the same comment on the first question, not knowing that
a second question had been asked - but I have also made many badly
mistaken comments on both questions! Although my judgement on this problem is evidently suspect, I am
fairly confident of the validity of Hagen von Eitzen's answer to the
second question, and even of my own answer to it.  Both answers
(independently) use the same construction, which is to concatenate $\gamma$ with $-\gamma$ , forming a closed curve; it is then
necessary to argue that if $z$ belongs to $[\gamma] \cup [-\gamma]$ and $[i\gamma] \cup [-i\gamma]$ , then one of $\pm z, \pm iz$ belongs
to $[\gamma] \cap [i\gamma]$ . My initial vague impression that both
arguments could handle the case of a general rotation seems, sadly,
to be just another mistake. Neither of these answers to the second question used the assumption
of injectivity, so the case $h = \pm i$ (equivalently, w.l.o.g., $c = \frac{\pi}{2}$ ) seems to be settled. The evidence for the general case is admittedly weak.  I don't find
it intuitively obvious (even though I keep kidding myself that I
glimpse ""reasons"" why it should be true). The mere fact that it
holds for $h = \pm i$ is not compelling.  Beyond this, I only have
the conviction gained from doing simple experiments with GeoGebra:
creating $\gamma$ as a polyline object, moving the vertices around,
and observing the intersections with one or more rotated copies. Nevertheless, for what it's worth, I feel quite strongly that the general result
is true.  I also feel, slightly less strongly, that it probably has
a simple proof using only elementary results about continuity.  I
don't think that anything as deep as the Jordan Curve Theorem is
needed.  Even my proof for the case $h = \pm i$ (which is short, if
you allow the use of a lemma that seems to be of more general use)
is probably more complex than the ""Book"" proof of the general
result. However, if no elementary proof is forthcoming, I will accept an
answer using advanced methods.  I won't be competent to judge such
an answer myself, but I will happily take advice (in a chatroom, if
there isn't space in the comments).","['plane-curves', 'general-topology', 'geometry']"
3114837,Is a set of measure zero in $\mathbb{R}$ totally disconnected?,"Let $M \subset \mathbb{R}$ be a nonempty set of Lebesgue measure zero. Does it follow that $M$ is totally disconnected in the sense that for any $x<y$ , with $x,y\in M,$ there exists $z\notin M$ such that $x<z<y$ ? I think the answer to the questions is yes, since otherwise one could argue by contradiction and say that then there is an interval contained in $M$ and hence it cannot have measure zero. Is the reasoning above sound? Also just as side question, connectedness of nonempty sets does not imply positive measure in $\mathbb{R}^n$ , since a line in $\mathbb{R}^2$ is connected and has measure zero, right? Thank you for your time and appreciate any feedback.","['measure-theory', 'proof-verification', 'lebesgue-measure', 'real-analysis']"
3114843,"Given that A, B and C do not lie on the same line...find the area of triangle ABC.","\begin{array} { c } { \text { Given that } A , B \text { and } C \text { do not lie on the same line.If } \vec { O A } + \vec { O B } + \vec { O C } = 0 , | \vec { O A } | = \sqrt { 6 } \text { , } } \\ { | \vec { O B } | = 2 \text { and } | \vec { O C } | = \sqrt { 14 } , \text { find the area of } \triangle A B C . } \end{array} I found this problem online and I am not sure what should I do. I have tried some geometric transformations but couldn't find the answer. I guess it isn't as easy as I thought at first.","['euclidean-geometry', 'vectors', 'vector-spaces', 'geometry']"
3114847,Interpretation of Differentials,"$$
\newcommand{\qa}{P}
\newcommand{\qb}{Q}
\newcommand{\da}{dP}
\newcommand{\db}{dQ}
\newcommand{\positiverealnumbers}{\mathbb{R}_+}
\newcommand{\realnumbers}{\mathbb{R}}
\newcommand{\naturalnumbers}{\mathbb{N}}
\newcommand{\positiveintegers}{\mathbb{Z}_+}
$$ We frequently solve geometrical and physical problems by obtaining an
approximate expression for differential $\da$ in terms of differential $\db$ and then integrating $\da$ to obtain $\qa$ . We assume that the
expression for $\qa$ is exact even though we used an approximate
formula for $\da$ . This is justified by saying that the differentials
are infinitely small quantities. For example, when we derive an
expression for the area of a circular disc (see example 1) we set $dA = 2 \pi r dr$ which is an approximate expression when the
diffentials are interpreted as real numbers. In this article we try to
define a method for computing $\qa$ so that we don't need approximate
expressions in the derivation. Theorem 1 Let $a,b \in \realnumbers$ and $a < b$ .
Let $f$ be a function from $[a,b]$ into $\realnumbers$ and define $\Delta f = f(x + \Delta x) - f(x)$ where $x \in \realnumbers$ and $\Delta x \in \realnumbers \backslash \{0\}$ and $x, x + \Delta x \in [a, b]$ .
Suppose that $$
  \Delta f = g(x) \Delta x + h(x, \Delta x)
  $$ where $x$ and $\Delta x$ are defined as before.
Suppose also that $g$ is Riemann integrable and $$
    \lim_{\Delta x \to 0} \frac{h(x,\Delta x)}{\Delta x} = 0
  $$ for all $x \in [a, b]$ . Now $df = g(x) dx$ and $$ f(x) - f(a) = \int_a^x g(t) dt . $$ Proof This is a direct consequence of the definition of differentiability
and Fundamental Theory of Calculus. $$\tag*{$\blacksquare$}$$ The condition (1) can be weakened to $$
  \lim_{\Delta x \to 0+} \frac{h(x, \Delta x)}{\Delta x} = 0 .
  \tag{2}
$$ Theorem 2 A sufficient condition for inequality (2) is that there
exist $S, C \in \positiverealnumbers$ so that $$
  \vert h(x, \Delta x) \vert < C \vert \Delta x \vert^2
  $$ for all $x, x + \Delta x \in [a, b]$ and $0 < \Delta x < S$ . Proof $$
  \left\vert \frac{h(x, \Delta x)}{\Delta x} \right\vert
  < C \vert \Delta x \vert \to 0
  $$ as $\Delta x \to 0+$ . $$\tag*{$\blacksquare$}$$ Example 1 Derive an expression for the area of a disc whose inner radius is $r_a$ and outer radius $r_b$ . The area to be computed in example 1 Solution Define $\Delta A$ to be the area of a disc with inner radius $r$ and
width $\Delta r$ .
We have $$
2 \pi r \Delta r \leq \Delta A \leq 2 \pi (r + \Delta r) \Delta r
$$ By setting $g(r) := 2 \pi r$ and $h(r, \Delta r) := 2 \pi (\Delta r)^2$ we get $A = \pi r_b^2 - \pi r_a^2$ by Theorems 1 and 2. Example 2 Suppose that a particle is moving under influence of a constant force $F = ma$ for time $T$ and the particle is initially at rest. Derive
an expression for the kinetic energy of the
particle. Assume that the work done by a constant force $F$ is $W = F s$ where $s$ is the distance that the particle moves in the
direction of the force. Assume also that the kinetic energy of a
particle at rest is $0$ . Solution We define $\Delta s$ to be the distance that the particle moves in the
time interval $[t, t + \Delta t]$ .
We have $v = at$ , $$
a t \Delta t \leq \Delta s \leq a (t + \Delta t) \Delta t ,
$$ and $$
a (t + \Delta t) \Delta t = a t \Delta t + a (\Delta t)^2 .
$$ Set $g(t) := a t$ and $h(t, \Delta t) := a (\Delta t)^2$ and it follows from Theorems 1 and 2 that the distance that the
particle moves in time $T$ is $$
s  = \int_0^T a t dt = \frac{1}{2} a T^2
$$ By setting $v_f = a T$ we obtain $$
E_k = W = \frac{1}{2} F a T^2 = \frac{1}{2} m a^2 T^2
= \frac{1}{2} m v_f^2 .
$$ Alternative Solution Assume that the particle moves distance $\Delta s$ in time $\Delta
t$ . Define $\Delta W := F \Delta s$ . Now the acceleration $a =
\Delta v / \Delta t$ is a constant and we have $$
  \Delta W = m a \Delta s = m \Delta v \frac{\Delta s}{\Delta t}
  \tag{2}
$$ Let $v_{\mathrm{min}}$ and $v_{\mathrm{max}}$ be the minimum and
maximum velocities of the particle. We now have $$
  v_{\mathrm{min}} \leq \frac{\Delta s}{\Delta t} \leq
  v_{\mathrm{max}} .
$$ If $\Delta v \geq 0$ we get $$
  m v_{\mathrm{min}} \Delta v \leq \Delta W \leq m v_{\mathrm{max}}
  \Delta v ,
$$ which is equivalent to $$
  m v \Delta v \leq \Delta W \leq m (v + \Delta v) \Delta v .
$$ If $\Delta v < 0$ we have $$
  v + \Delta v \leq \frac{\Delta s}{\Delta t} \leq v ,
$$ from which it follows that $$
  m v \Delta v \leq \Delta W \leq m (v + \Delta v) \Delta v .
$$ Define $$
  h(v, \Delta v) := \Delta W - m v \Delta v .
$$ Now $$
  0 \leq h(v, \Delta v) \leq m \Delta v^2 .
$$ By setting $g(v) = m v$ and assuming that the kinetic energy is zero
when $v = 0$ it follows from Theorems 1 and 2 that $$
  E_k = W = \frac{1}{2} m v^2 .
$$ Do you find this formalism useful? Tommi Höynälänmaa","['integration', 'limits', 'calculus', 'real-analysis']"
3114854,Is there any proof that there doesn't exist a Hadamard matrix of size $6 \times 6$?,"A matrix $H \in {\pm 1}^n $ is Hadamard matrix if $HH^T=nI_n$ , where $I$ is $n\times n$ identity matrix. Hadamard's conjecture said that there exists Hadamard matrix of order 1,2 or $4n$ , for every positive integer $n$ . My question is how to prove there does not exists a $6 \times 6$ Hadamard matrix ?","['matrices', 'hadamard-matrices', 'combinatorics']"
3114856,Solving a variant of the Poisson Boltzmann equation,"The following is an equation I've derived in my personal research: $$ \frac{d^2V}{dx^2}=e^{\alpha x} \sinh(V) $$ I'm looking to solve it explicitly for V(x).
It's a variant of the Poisson-Boltzmann equation, which has the form $ \frac{d^2V}{dx^2}= \sinh(V) $ , and can be solved by multiplying both sides by $ \frac{dV}{dx} $ and integrating, rearranging and integrating again. I've tried several methods, changes of variables, separation of variables, integration by parts, etc. The best I can do is a change of variable, $ y=e^{\alpha x} $ which yields: $$ \alpha^2(V_{yy} \cdot y +V_v) = \sinh(V)  $$ OR equivalently... $$ \alpha^2 \frac{d}{dy}(yV_y) = \sinh(V)  $$ But again I can't get any further than this. A) Can it be solved explicitly? If so, how? B) Is there a name for this equation so I can find other information on it and it's properties? I have a suspicion that some change to a complex variable and contour integrals might be involved in solving this but I can't be sure since I took a took an introductory complex analysis class almost 10 years ago.","['complex-analysis', 'poissons-equation', 'ordinary-differential-equations']"
3114864,Consistency for maximum likelihood estimator with a single sample,"Suppose you have a finite family of probability measures $\{\mu_\theta: \theta \in S\}$ on a finite space $\Omega$ (with respect to the discrete sigma algebra). Let $X$ be a random element of $\Omega$ distributed according to $\mu_{\theta_0}$ , for some fixed $\theta_0 \in \Omega$ . Consider the maximum likelihood estimator for $\theta_0$ : $\hat{\theta}_{MLE} = \arg\max_\theta L(\theta | X)$ . Here $L$ is the usual likelihood, $L(\theta | X) = \mu_\theta(X)$ . I'm curious about consistency for the MLE in this simple setup, but not in the usual sense of 'asymptotic consistency:' rather, I want to know if there are some conditions under which the MLE is consistent for a single sample , i.e. $\arg \max_\theta \mathbb{P}(\hat{\theta}_{MLE} = \theta) = \theta_0$ . (Sorry if my notation is a bit weird -- let me know if it's confusing and I'll try to clarify.) I am guessing this can fail in general -- is there a simple example? Are there some conditions under which it holds? It is true that the expected value of $L(\theta|X)$ is maximized at $\theta = \theta_0$ . Indeed, by Cauchy-Schwarz: $\mathbb{E}L(\theta | X) = \mathbb{E}\mu_\theta(X) = \sum_\omega \mu_\theta(\omega) \mu_{\theta_0}(\omega) \leq \sqrt{\sum_\omega \mu_\theta(\omega)^2 \sum_\omega \mu_{\theta_0}(\omega)^2}$ Equality occurs when $\theta = \theta_0$ . But this doesn't imply that the density of the MLE is maximized there. Edit 1: To be careful, the conclusion should be that $\theta_0$ is a point that maximizes the density of the MLE (not necessarily the only point).","['statistics', 'probability', 'maximum-likelihood']"
3114876,"Prove that $\lim_{n\to\infty}\int_{0}^{1}\cos^n\left(\frac{1}{x}\right)\,dx=0$","Prove that: $$\lim_{n\to\infty}\int_{0}^{1}\cos^n\left(\frac{1}{x}\right)\,dx=0$$ I have a idea about this,but I can't complete proof. Hint: We can split the integral into two parts,then estimate the two parts separately. I tried to let $t=1/x$ ,but there are many difficulties in the proof. Any help would be greatly appreciated :-)","['integration', 'limits', 'real-analysis']"
3114883,"Let a,b and c be the side lengths of triangle ABC respectively...find the greatest value of b*c.","Let a,b and c be the side lengths of triangle ABC respectively. If the perimeter of $\Delta$ ABC is 7, and that $\cos A=-\frac{1}{8}$ , find the greatest value of $b*c$ . This is how I start the solution: $$a+b+c=7 \implies b+c=7-a,\quad \cos A=-\frac{1}{8}\\
a^2=b^2+c^2-2bc\cdot \cos A\\
\implies a^2=b^2+c^2+\frac{bc}{4} \\
\implies a^2=(7-a)^2-2bc+\frac{bc}{4} \\
\implies bc=4(7-2a)$$","['trigonometry', 'geometry']"
3114912,Solving an ODE system which depends on a periodic function,"I have the following ODE system and want to show that all solutions $\gamma(t)=(x(t),y(t),z(z))$ exist for all times (or can be extended on all of $\mathbb{R}$ ). The only problem is that the system depends on an unknown function $c:\mathbb{R}\rightarrow (0,\infty)$ , which is smooth, positive and periodic. \begin{align*}
\ddot{x}(t)&=\frac{1}{2}\frac{c'(z(t))}{c(z(t))}\dot{y}(t)^2 \\
\ddot{y}(t)&=-\frac{c'(z(t))}{c(z(t))}\dot{y}(t)\dot{z}(t)\\
\ddot{z}(t)&=-\frac{c'(z(t))}{c(z(t))}\dot{z}(t)^2.
\end{align*} I can rewrite the last equation to get $c(z)\ddot{z}+c'(z)\dot{z}^2=0$ , so $\frac{d}{dt}(\dot{z}c(z))=0$ , which tells me that $\dot{z(t)}c(z(t))=a_1$ for some constant $a_1\neq 0$ . Solving this gives the implicit expression \begin{align*}
a_1 t+a_2=\int_{a_3}^{z(t)}c(\xi)d\xi
\end{align*} for $a_2,a_3$ constants. Substituting $\dot{z}=\frac{a_1}{c(z)}$ in the second equation I get $\ddot{y}=-a_1\frac{c'(z)}{c(z)^2}\dot{y}$ which can also be written as $\ddot{y}=\frac{\ddot{z}}{\dot{z}}\dot{y}$ . Now I don't know how to continue. I'm pretty sure one cannot proceed solving this without knowing what the function $c$ is. But since $c$ is periodic (so in particular $c$ and $c'$ are bounded) I have hope that one nevertheless can show that all solutions exist for all time.","['nonlinear-system', 'calculus', 'ordinary-differential-equations', 'real-analysis']"
3114931,Exponential/Logarithmic equation system,"Solve the following equation system over the real numbers $$\begin{cases}
x(1-\log_{10}(5))=\log_{10}(11-3^y)\\ 
 \log_{10}(35-4^x)=y\log_{10}(9) \\ 
\end{cases}
$$ For the functions in the above relations to be well defined, I've determined
that $0<x<\log_4(35)$ and $0<y<\log_3(11)$ . I also know that each of the equations(taken separately by itself) has exactly one solution because of their monotonicity (in each case one is strictly increasing and the other is strictly decreasing, so they can't have more than one intersection.) By substituting $(1-\log_{10}(5))$ with $\log_{10}(2)$ in the first equation the unique solution for it is found to be $x=\log_2(11-3^y).$ I've tried to proceed further but to no avail and come here looking for help; I think that there is no real solution to the system  but haven't been able to prove it.","['algebra-precalculus', 'systems-of-equations', 'exponential-function', 'logarithms']"
3114933,"Proof (without use of differential calculus) that $e^{\sqrt{x}}$ is convex on $[1,+\infty)$.","Prove (without use of  differentiation) that $f(x)=e^{\sqrt{x}}$ is convex on $[1,+\infty)$ . Attempt . Function $x\mapsto e^x$ is convex and increasing, but $x\mapsto \sqrt{x}$ is concave, so we cannot use the proposition of composition: $$(convex~\&~increasing)\circ convex=convex.$$ Definition would require to prove for all $x,~y\geqslant 1$ and $\lambda \in [0,1]$ : $$e^{\sqrt{\lambda x+(1-\lambda) y}}\leqslant \lambda e^{\sqrt{ x}}+(1-\lambda)e^{\sqrt{y}}$$ but squaring doesn't work here. If we used continuity in order to prove mid-convexity, the problem would go like: $$e^{\sqrt{\frac{x+y}{2}}}\leqslant \frac{e^{\sqrt{x}}+e^{\sqrt{y}}}{2},$$ equivalently: $$e^{\sqrt{\frac{x+y}{2}}}-e^{\sqrt{x}} \leqslant  e^{\sqrt{y}}-e^{\sqrt{\frac{x+y}{2}}}$$ (but without MVT what would we do with these differences?) Thanks in advance for the help.","['convex-analysis', 'analysis', 'real-analysis']"
3115002,How many distinguishable outcomes from rolling 6 identical dice?,"Ignoring order, how many distinguishable outcomes are there from rolling 6 identical dice? Answer = $462$ I tried a variety of ways such as $\frac{6^6}{6!}$ and can't seem to get the answer. Struggling how to incorporate no order and distinguishable at the same time. Please help.","['statistics', 'combinatorics']"
3115018,We have $n$ charged and $n$ uncharged batteries and a radio which needs two charged batteries to work.,"We have $n$ charged and $n$ uncharged batteries and a radio which needs two charged batteries to work. Suppose we don't know which batteries are charged and which ones are uncharged. Find the least number of attempts sufficient to make sure the radio will work. An attempt consists in putting two batteries in the radio and check if the radio works or not. I can prove that it is $\leq n+3$ (or $\leq n+4$ , look at the comment): Say we have batteries $B_1,B_2,....B_{2n}$ , $n$ of them charged and other not. If in one of $n$ attempts $$\{B_1,B_2\}, \{B_3,B_4\}, ...\{B_{2n-1},B_{2n}\}$$ radio work we are done. If non of them work, then in each pair we must have charged and uncharged batteries. Thus in one of next 4 pairs: $$\{B_1,B_3\}, \{B_1,B_4\}, \{B_2,B_3\}, \{B_2,B_4\}$$ must be a pair of charged batteries and we are done (so if 3 times doesn't work we know the last pair is charged). But, can we reduce the number of attempts?","['contest-math', 'graph-theory', 'extremal-combinatorics', 'combinatorics', 'discrete-optimization']"
3115043,"$f(x) = \begin{cases} 0 & x\in( \mathbb{Q}\cap [0,1])^c \\ \frac{1}{q} & x=\frac{p}{q} \in \mathbb{Q} \cap [0,1], (p,q)=1 \end{cases}.$","let $f$ be the function defined by $$f(x) = \begin{cases} 0 & x\in( \mathbb{Q}\cap [0,1])^c \\ \frac{1}{q} & x=\frac{p}{q} \in \mathbb{Q} \cap [0,1], (p,q)=1 \end{cases}.$$ Prove that $f$ is continuous at every irrational number in $[0,1]$ , and it discontinuous at every rational number in $[0,1]$ . Is this function? I mean that $0=\frac{0}{5}=\frac{0}{6}$ , so $f(0)$ may be equals $\frac{1}{5}$ or $\frac{1}{6}$ , so $0$ have more than $1$ image! “Sorry, I don’t speak English well”","['continuity', 'functions', 'real-analysis']"
3115053,Convolution square of the Cantor set,"For $0\leq d\leq 1$ , let $\eta_d$ be the $d$ -dimensional Hausdorff measure on $\mathbb{R}$ (for some normalization); recall that it is translation-invariant. Motivation for what follows: Up to recently, I was convinced that if $f$ and $g$ are in $L^1(\mathbb{R},\eta_d)$ (that is, Borel functions $\mathbb{R}\to\mathbb{R}$ , integrable w.r.t. $\eta_d$ , modulo equality $\eta_d$ -a.e) one could define their convolution as usual by $(f*g)(x) = \int_{\mathbb{R}} f(y)\, g(x-y)\, \mathrm{d}\eta_d(x)$ and that this would satisfy $\|f*g\|_1 \leq \|f\|_1\,\|g\|_1$ as expected (where $\|f\|_1 := \int_{\mathbb{R}} f(t)\, \mathrm{d}\eta_d(t)$ ).  Upon thinking about this question , I realized that this is not the case: the usual proof relies on translation-invariance and Fubini, but Fubini itself relies on $\sigma$ -finiteness which is not true of $\eta_d$ ; for the Haar measure on a locally compact abelian group, which is not necessarily $\sigma$ -finite, one can get away with ""regularity"" instead (even though things appear to be confused ), but now I'm struggling to understand what works or does not work for Hausdorff measures.  So let me ask about a concrete case: Let $d = \log 2/\log 3$ and let $\eta = \eta_d$ be the $d$ -dimensional Hausdorff measure on $\mathbb{R}$ , normalized so that the usual Cantor middle-third set $C$ has $\eta(C)=1$ .  Let $f = \mathbf{1}_C$ be the indicator function of $C$ and $\nu = f \eta$ (that is, $E \mapsto \int_E f\, \mathrm{d}\eta$ ) be the ""uniform"" probability measure on $C$ .  We define the convoluted function $g := f\mathbin{*_\eta}f = f*\nu$ as $$x \mapsto \int_{\mathbb{R}} f(y)\, f(x-y)\, \mathrm{d}\eta(y) = \int_{C} f(x-y)\,\mathrm{d}\nu(y)$$ and we define the convoluted measure $\xi := \nu*\nu$ as $$E \mapsto (\nu\times\nu)(\{(y,z)\in\mathbb{R}^2 : y+z\in E\})$$ To put it more simply, $g(x)$ is the probability that, if $y$ is chosen uniformly in the Cantor set, $x-y$ also falls in the Cantor set; whereas $\xi(E)$ is the probability that two given reals $y,z$ both chosen uniformly and independently in the Cantor set have a sum in $E$ . Naïvely one would like to think that $\int_E g\, \mathrm{d}\eta = \xi(E)$ (for $E$ a Borel set) with the following ""proof"": $$\begin{aligned}\int_E g\, \mathrm{d}\eta
&= \int_E\left(\int_{\mathbb{R}} f(x-y)\, \mathrm{d}\nu(y)\right) \mathrm{d}\eta(x)\\
&= \int_{E\times\mathbb{R}} f(x-y)\, \mathrm{d}\nu(y)\, \mathrm{d}\eta(x)\\
&= \int_{\{(y,z)\in\mathbb{R}^2 : y+z\in E\}} f(z)\, \mathrm{d}\nu(y)\, \mathrm{d}\eta(z)\\
&= \int_{\{(y,z)\in\mathbb{R}^2 : y+z\in E\}} \mathrm{d}\nu(y)\, \mathrm{d}\nu(z)\\
&= \xi(E)\end{aligned}$$ but as explained above, this proof does not work.  However, both $g$ and $\xi$ are still well-defined objects. Questions: How can we describe $g = f\mathbin{*_\eta}f$ and $\xi = \nu * \nu$ more concretely? Specifically, what is the support of $g$ ? What is the exact relation between $g$ and $\xi$ ? Specifically, is $\xi$ absolutely continuous w.r.t. $\eta$ ?  (I believe it isn't.) Bonus question: I am aware that the Fourier-Stieltjes transform $\hat\xi$ of $\xi$ is $u\mapsto e^{-2i\pi u} \prod_{j=1}^{\infty} \cos^2(2\pi u/3^j)$ (the square of that of $\nu$ ).  But what is the Fourier transform of $g$ ?","['measure-theory', 'cantor-set', 'hausdorff-measure', 'real-analysis']"
3115054,"Collection of less well-known, non-trivial, elegant story proofs (ie, ""double counting proofs"") of combinatorial identities","By story proof I mean proving a combinatorial identity by counting the number of elements of some carefully chosen set in two different ways to obtain the different expressions in the identity . The following is my favorite: Prove (try the algebraic method!) $$\sum _ { k = 0 } ^ { n } {n \choose k} 2 ^ { k } { {n - k }\choose { \left[ \frac { n - k } { 2 } \right]} } = { {2 n + 1 } \choose { n }},$$ Proof: Assume there are $2n+1$ people, where one of them T is single (it's me), the rest of them are $n$ pairs of lovers $(a_n,b_n)$ . Now we need to choose $n$ among the $2n+1$ people for a dance party. There are $2$ methods: A. Choose $n$ people arbitrarily, which accounts for ${ {2 n + 1 } \choose { n }}$ combinations in total. B. Fix $k$ , where $0\leq k \leq n $ . Choose $k$ pairs of lovers, and demand only one of them can go to the party, which accounts for ${n \choose k} 2 ^ { k }$ combinations. Choose ${ \left[ \frac { n - k } { 2 } \right] }$ from the remaining $n-k$ pairs, and the two of them don't need to be separated. If $n-k$ is odd, $$ k + 2 \left[ \frac { n - k } { 2 } \right] = n - 1,$$ T can also join the party! If $n-k$ is even, $$ k + 2 \left[ \frac { n - k } { 2 } \right] = n,$$ T cannot join the party :( When $k$ is fixed, the total number of combinations of the $n$ people is ${ {n - k }\choose { \left[ \frac { n - k } { 2 } \right]} }$ . Let $k$ vary from $0$ to $n$ , then method B produces $\sum _ { k = 0 } ^ { n } {n \choose k} 2 ^ { k } { {n - k }\choose { \left[ \frac { n - k } { 2 } \right]} }$ combinations. As the 2 methods should result in the same number of combinations, the identity is proved. Source: My variant of Gu Jian's proof from Collection of CMO (Chinese Mathematics Olympiad) Problems (Sorry for can't provide a picture as it's written in Chinese). I would like to collect everyone's favorite story proof. Apologize if this is a duplicate. Blue's link: https://en.wikipedia.org/wiki/Double_counting_(proof_technique) Any other less well-known ones?","['big-list', 'combinatorics', 'combinatorial-proofs']"
3115060,Asymptotic behavior of the integral $H(\beta)=\frac{2}{\pi}\beta\int_{0}^{\infty}\exp\left(-x^{3/2}\right)\sin(\beta x)xdx$,"I found the integral $H(\beta)$ (which is called Holtsmark distribution) in Holtsmark's theory of ion field in plasma. In a book there is its asymptotic representation at small and great $\beta$ : $$
H(\beta)\approx
\left\{\begin{array}{l}
\frac{4\beta^2}{3\pi}\left(1-0,463\beta^2\right) &\beta\ll 1\\
\\ 
1,496\beta^{-5/2}\left(1+5,107\beta^{-3/2}+14,43\beta^{-3}\right)&\beta\gg 1
\end{array}\right.
$$ I know, how to get the first line: use Taylor series for $\sin(\beta x)$ . But what should I do to prove the second line? I tried to write the integral in the other form: $$
H(\beta)=\frac{2}{\pi\beta}\int_{0}^{\infty}\exp\left(-\left(\frac{y}{\beta}\right)^{3/2}\right)\sin(y)ydy
$$ and use Taylor series for $e^{-\left(y/\beta\right)^{3/2}}$ , but faced with the divergent integral, which appears due to the first term of series: $$\int_{0}^{\infty}\sin(y)ydy$$","['integration', 'improper-integrals', 'probability-distributions', 'asymptotics', 'physics']"
3115079,Drawing the graph of $x=i^2y$. Where was I wrong?,"I was asked to draw the graph of $x=i^2y$ where $i$ =iota in an interview. So, I sketched it as a normal $\ x+y=0\ $ line in the $x$ - $y$ graph. But, the interviewer said it was wrong. He further said that the flaw in my graph is that: at the point $(0,0)$ which is the origin it is broken, meaning that the ""line"" (requested set of points) will not cross through the origin. Namely, it should be like { $x+y=0$ minus the origin point} . However, he didn't tell me the reason. So, my doubt is that why is it so? Why it is not defined at origin?","['geometry', 'complex-numbers']"
3115090,"Is this a Riemann sum (if so, I can't figure out which one)?","This was supposedly an easy limit, and it is suspiciously similar to a Riemann sum, but I can't quite figure out for what function. $$\lim_{n\to\infty}{\frac{1}{n} {\sum_{k=3}^{n}{\frac{3}{k^2-k-2}}}}$$ Well, even the fact that $\frac{3}{k^2-k-2} = \frac{1}{k-1}-\frac{1}{k+2}$ doesn't seem to simplify the problem. I thought this would be a telescoping sum, but it's clearly not. Is that a Riemann sum at all?","['limits', 'riemann-sum']"
3115136,Find the difference of ceiling functions,"For $k \geq 1, 1 \leq r \leq k, t \geq 1, x \geq 1$ , is there a  lower bound or upper bound on: $$\left\lceil \dfrac{k(t+x)}{r} \right\rceil - \left\lceil \dfrac{k(t)}{r} \right\rceil$$ edit: all the variables $k,r,t,x$ are integers","['algebra-precalculus', 'discrete-mathematics', 'ceiling-and-floor-functions']"
3115212,Cube and unit cubes,"Consider a $3\times 3\times 3$ cube consisting of smaller $1\times 1\times 1$ unit cubes. The big cube is painted black on the outside. Suppose we disassemble the cube and pick a random unit cube, look at only one face and see it is black, without looking at the other faces. What is the probability the unit cube we picked is one of the 8 corner cubes? This one seems simple to me but I am not sure I am right:
The big cube consists of 27 unit cubes of which one only, the middle one, does not have any painted face. All the others (26) have at least one face painted and 8 have 3 faces painted.
Thus the requested probability is $\frac{8}{26}$ —is this so?",['probability']
3115332,Ricci equation is trivial in codimension $1$,"Let $f:M\to\overline{M}$ an isometric immersion and assume $\dim(M)=\dim(\overline{M})-1$ . I'm asked to show that the Ricci equation offers no information. I guess what I have to show is that the Ricci equation gives something like $0=0$ . What I have as Ricci equation is that for all $X,Y$ vector fields on $M$ and for all $\eta,\xi$ normal vector fields on the image of $f$ we have $\langle \overline{R}(\overline{X},\overline{Y})\overline{\eta},\overline{\xi}\rangle=\langle R^\perp (X,Y)\eta,\xi\rangle -\langle [A_\eta,A_\xi](X),Y\rangle $ where $R$ and $\overline{R}$ are the curvature tensors on $M$ and $\overline{M}$ , the bar represents an extension to $\overline{M}$ and $A_\eta$ is the Weingarten endomorphism. I've been manipulating this equation using facts like $A_\eta(X)=-\nabla_X\eta$ and that $T_p^\perp M$ has dimension one (and therefore all normal vector fields are proportional). However, I haven't been able to reach any conclusion comparable to ""the Ricci equation offers no information"". For instance, on the RHS, on the one hand I get $
\langle [A_\eta,A_\xi](X),Y\rangle = \langle A_\eta(A_\xi(X))-A_\xi(A_\eta(X)),Y\rangle =\langle A_\eta(A_\xi(X)),Y\rangle- \langle A_\xi(A_\eta(X)),Y\rangle =
$ $
\langle A_\xi(X),A_\eta(Y)\rangle-\langle A_\eta(X),A_\xi(Y)\rangle= \langle \nabla_X\xi,\nabla_Y\eta\rangle -\langle \nabla_X\eta, \nabla_Y\xi\rangle=\langle \nabla_X(\xi-\eta),\nabla_Y(\eta-\xi)\rangle
$ On the other hand $
R^\perp(X,Y)\eta=\nabla_Y^\perp \nabla_X^\perp\eta-\nabla_X^\perp \nabla_Y^\perp\eta+\nabla_{[X,Y]}^\perp\eta
$ So $
\langle R^\perp (X,Y)\eta,\xi\rangle= \langle\nabla_Y^\perp \nabla_X^\perp\eta,\xi\rangle -\langle\nabla_X^\perp \nabla_Y^\perp\eta,\xi\rangle+\langle\nabla_{[X,Y]}^\perp\eta,\xi\rangle
$ which doesn't seem to be equal to the previous expression. On the LHS it is essentially the same but without $\perp$ and writing bars, so I don't see why that should vanish. What can be then deduced from the Ricci equation in the codimension $1$ case?","['submanifold', 'riemannian-geometry', 'geometry', 'smooth-manifolds', 'differential-geometry']"
3115354,Does $D_4$ have a verbal subgroup of order 4?,"Does $D_4$ have a verbal subgroup of order 4? How did this question arise: In the comments $Q_8$ ad $D_4$ were pointed to be a possible counterexample to this question: Is it true, that for any two non-isomorphic finite groups $G$ and $H$ there exists such a group word $w$, that $|V_w(G)| \neq |V_w(H)|$? However, $Q_8$ does not have a characteristic subgroup of order $4$ , whereas $D_4$ has $3$ normal subgroups of order $4$ . So, if one of those subgroups happens to be verbal for some group word $w$ , then $Q_8$ ad $D_4$ are definitely not a counterexample. If such $w$ exists, then it is clearly not an identity in $D_4$ . Also, $w \neq [x, y]$ ,  as $D_4’ \cong C_2$ , $w \neq x$ ,  as $V_{x^3}(D_4) \cong D_4$ , $w \neq x^2$ as $V_{x^2}(D_4) \cong C_2$ and $w \neq x^3$ ,  as $V_{x^3}(D_4) \cong D_4$ . However, I do not know, how to proceed further.","['dihedral-groups', 'finite-groups', 'verbal-subgroups', 'abstract-algebra', 'group-theory']"
3115370,Show that $n^2-1+n\sqrt{d}$ is the fundamental unit in $\mathbb{Z}[\sqrt{d}]$ for all $n\geq 3$,"Let $n\in \mathbb{Z}$ , $n\geq3$ and $d=n^2-2$ . I want to show that $n^2-1+n\sqrt{d}$ is the fundamental unit in $\mathbb{Z}[\sqrt{d}]$ . Substituting $n=3,4,5$ gives the elements $8+3\sqrt{7}$ , $15+4\sqrt{14}$ and $24+5\sqrt{23}$ respectively, which, by inspection, are the fundamental units of $\mathbb{Z}[\sqrt{7}]$ , $\mathbb{Z}[\sqrt{14}]$ and $\mathbb{Z}[\sqrt{23}]$ respectively. So by empirical observation, the statement seems to be true, at least for the first few values of $n$ . My ideas so far on how to go about proving this have been the following: We know that any unit in $\mathbb{Z}[\sqrt{d}]$ must be expressible as some power of the fundamental unit, or the additive inverse of some power of the fundamental unit, and furthermore, we know that only the fundamental unit has this property. So if we can show that any unit in $\mathbb{Z}[\sqrt{d}]$ must be expressible in the form $\pm(n^2-1+n\sqrt{d})^r$ for some $r\in \mathbb{Z}$ , this would prove that $n^2-1+n\sqrt{d}$ is the fundamental unit. However, I seem to be at a loss as to how to prove this, and I suspect that there may be a simpler proof of the statement. All help or input would, as always, be highly appreciated. Update: In response to a highly relevant comment, I add the following: If $d$ is not square free, the statement seems to fail. $\textit{E.g.}$ if we take $n=10$ . Then $d=10^2-2=98+2*7^2$ , and we get the element $99+10\sqrt{98}$ , but the fundamental unit in $\mathbb{Z}[\sqrt{98}]$ is demonstrably $1+\sqrt{2}$ . In fact, the fundamental unit is only defined for $\mathbb{Z}[\sqrt{d}]$ , when $\mathbb{Z}[\sqrt{d}]$ is the ring of integers of some quadratic number field. This is only the case when $d$ is square-free. We observe that as $d \not\equiv 1\ (\textrm{mod}\ 4)$ for all $n\geq3$ , $\mathbb{Z}[\sqrt{d}]$ does indeed constitute the ring of integers of some quadratic number field whenever $d$ is square-free. But I do still suspect that the above statement is true for $d$ square-free.","['number-theory', 'ring-theory', 'algebraic-number-theory']"
3115386,Proof of separability of polynomials without derivatives,"Is there a known proof without differentiating that proves that all irreducible polynomials over $\mathbb{Q}$ are separable? (Or even better, for all fields of characteristic $0$ .) EDIT: As people seem to question this thread; I do know a proof with derivatives - my motivation for one without is simply curiosity. Multiple approaches are always nice.","['separable-extension', 'field-theory', 'alternative-proof', 'abstract-algebra', 'polynomials']"
3115408,Inequality involving inner product and norm,"If $\|\cdot \|$ is the norm induced by the inner product $\langle,\rangle$ , how to prove the following interesting inequality? $$\langle x,y\rangle(\|x\|+\|y\|) \leq\|x+y\|\,\|x\|\,\|y\|$$ This is a exercise in my textbook which is available only in portuguese called ""Topologia e Análise no Espaço $\mathbb{R}^n$ "". The above inequality is obvious when $\langle x,y\rangle \leq 0$ , but I don't know how to proceed to prove the other case.","['inner-products', 'linear-algebra']"
3115413,"If $‎\lim\limits_{x\to\infty}‎\frac{f(x)}{g(x)} = 1‎$,‎ then $\lim\limits_{x\to\infty}(f(x) - g(x)) = 0‎$.","‎Suppose ‎ $‎f‎$ ‎and ‎‎ $‎g‎$ ‎ ‎‎are real functions such that $‎‎‎\displaystyle{\lim_{x\to\infty}}‎\frac{f(x)}{g(x)} = 1‎$ ‎‎. ‎My ‎question ‎is‎: ‎‎‎
‎‎What other condition is required ‏‎that $‎‎‎\displaystyle{\lim_{x\to\infty}}(f(x) - g(x)) = 0‎$ ‎‎‎‎‎‎‎‎‎?‎ ‎‎ 
‎Generall‏‎y,‎ $‎‎‎\displaystyle{\lim_{x\to\infty}}‎\frac{f(x)}{g(x)} = 1‎\nRightarrow‎‎‎‎‎\displaystyle{\lim_{x\to\infty}}(f(x) - g(x)) = 0‎$ ‎. ‏‎For ‎example, ‎‎ $‎f(x) = x^2 + x‎$ ‎and ‎‎ $‎g(x) =‎ ‎x^2‎$ ‎.‎","['limits', 'real-analysis']"
3115425,A poker hand contains five cards. Find the probability that a poker hand can be…,"d) one pair Since there are 13 ranks, we pick 1 rank. $\binom{13}{1}$ , for each of these ways we want to pick 2 cards out of 4 that belong in this rank. So $\binom{13}{1} \binom{4}{2}$ . Now we want 3 other cards where their face values are different, but their ranks can be the same? But if we pick from 13 ranks, that'll include a card that may have been picked already. So we pick 3 from 12 remaining ranks, $\binom{12}{3}$ and for each of these ways, we want to pick 1 card out of 4, 3 times. So the total number of ways is: $$\binom{13}{1}\binom{4}{2}\binom{12}{3}\binom{4}{1}\binom{4}{1}\binom{4}{1}$$ And then the probability would just be the result after we divide by $$\binom{52}{5}$$ Is this correct? e) two pairs 13 ranks, choose 2 ranks out of 13. For each of these ways, we pick 2 cards out of 4. And then want 1 last card that that can technically be any rank but needs to have a different face value. But if we do $\binom{13}{1}$ that might count a card that was already picked? So do we pick from 11 ranks instead? $\binom{11}{1}$ and for each of these ways we have $\binom{4}{1}$ ways to pick 1 card. So the total number of ways is: $$\binom{13}{2}\binom{4}{2}\binom{11}{1}\binom{4}{1}$$ And then divide by $$\binom{52}{5}$$ to get the probability? Thanks","['discrete-mathematics', 'combinatorics', 'probability']"
3115429,"For $F(n)$ the $n$-th Fibonacci number, is $F(a)F(b)-F(a+1)F(b-1)$ always $\pm F(m)$ for some $m$?","For $F(n)$ the $n$ th Fibonacci number, the expression $$F(a)F(b)-F(a+1)F(b-1)$$ seems to be $\pm F(m)$ for some $m$ . I can't specify $m$ or the sign in terms of $a,b,$ and have not tried it out extensively. My question is: Is there a link to such a formula (and/or its proof) that someone could cite? I'd appreciate it. I've tried via Binet formulas, and several terms cancelled, but I couldn't get it to go through. Thanks for any help.","['elementary-number-theory', 'fibonacci-numbers', 'discrete-mathematics']"
3115447,I didn't understand this recurrence relation solution.,"Recently, i was trying to solve this recurrence relation $$ a_{n+4} = \frac{-\alpha(x)}{(n+4)} \cdot a_{n+3} +\frac{-\beta(x)}{(n+3)\cdot (n+4)} \cdot a_{n+2} $$ But i can't solve for $a_n$ I've tried to solve using Wolfram|Alpha and i got this result . How do i get this result and how to solve it. Edit: n = 0,1,2,3....","['combinatorics', 'recurrence-relations', 'discrete-mathematics', 'sequences-and-series']"
3115522,Is the moment generating function smooth by definition?,"So I'm going through Casella's Statistical Inference , and in Definition 2.3.6 he defines the moment generating function of a random variable $X$ with cdf $F_X$ , denoted by $M_X(t)$ , as $$M_X(t) = Ee^{tx},$$ provided that the expectation exists for $t$ in some neighborhood of $0$ . In Theorem 2.3.7, he then states that if $X$ has a mgf $M_X(t)$ , then the nth moment of $X$ is equal to the nth derivative of $M_X(t)$ evaluated at $0$ . But why do all of these derivatives have to existed? Is it implicitly assumed without it being stated, or does it somehow just follow from the definition above? If so, how?","['expected-value', 'moment-generating-functions', 'derivatives']"
3115527,Mellin inverse of $\sum_{n=0}^{\infty}\frac{\Gamma(n+s)\zeta(n+s)\zeta(n+1+s)}{\zeta(s)n!}\left(-\omega\right)^{n}$,"I am trying to compute the inverse Mellin transform of : $$\sum_{n=0}^{\infty}\frac{\Gamma(n+s)\zeta(n+s)\zeta(n+1+s)}{\zeta(s)n!}\left(-\omega\right)^{n}$$ w.r.t. the complex number $s$ . $\omega$ being a real parameter. My Attempt : it can be easily verified that the function $\phi(s,\omega)$ given by : $$\phi(s,\omega)=\frac{(s-1)}{\Gamma(s)^{2}}\sum_{n=0}^{\infty}\frac{\Gamma(n+s)\zeta(n+s)\zeta(n+1+s)}{n!}\left(-\omega\right)^{n}$$ is entire in $s$ . Thus, the Mellin inverse may be written as : $$\frac{1}{2\pi i }\int_{\sigma-i\infty}^{\sigma+i\infty}\frac{\Gamma(s)^{2}\phi(s,\omega)}{(s-1)\zeta(s)}x^{-s}ds$$ which can be computed using the residue theorem. The problem now is to find $\phi(s,\omega)$ , and it's derivatives at negative integers, and the non-trivial zeros of the Riemann zeta function. hence my question.","['complex-analysis', 'number-theory', 'mellin-transform']"
3115607,Subgroup of a finite group $H$ and $G$,"I regret to admit that this has been confusing me for much longer than I would like. I just can't wrap my head around some parts of this question and I'd like some guidance. Let $G$ be a finite group, and let $S$ be a nonempty subset of $G$ . Suppose $S$ is closed with respect to multiplication. Prove that $S$ is a subgroup of $G$ . (HINT: It remains to prove that $S$ contains $e$ and is closed with respect to inverses. Let $S$ = { $a_1$ ... $a_n$ }. If $a_i$ $∈$ $S$ , consider the distinct elements $a_ia_1$ , $a_ia_2$ , $...$ $a_ia_n$ I think if I can understand one tiny segment then I can complete my proof. I know I need to show $e$ $∈$ $S$ and that $S$ is closed w.r.t inverses, but I can't get across a tiny step in proving $e$ $∈$ $S$ . The proof I'm trying to figure out goes something like: Since $S$ is finite and closed under the group operation, then we have $a_1$ = $a_1a_k$ for some $a_k$ $∈$ $S$ . Then from this we can find that $a_k$ = $e$ . My question is how we know that we have $a_1$ = $a_1a_k$ for some $a_k$ $∈$ $S$ . I have no idea why this would be the case. I've also seen another post that used a map from $S$ $\rightarrow$ $S$ defined by left multiplication by $a_1$ . The map is one-to-one and since $S$ is finite, it is injective as well. I understand this, but then the poster goes on to say that this somehow shows that we have $a_1s$ = $a_1$ for some $s$ $∈$ $S$ . I have no idea why this would be the case either. If I can figure this out I think I can do the rest of the proof. Thanks in advance.","['elementary-set-theory', 'group-theory', 'abstract-algebra', 'finite-groups']"
3115654,"Normed space $C^2[0,1]$ with norm $\lVert f\rVert:=\max_{t\in[0,1]}\{\lvert f(t)\rvert+\lvert f''(t)\rvert\}$ is Banach space","The problem is as follows: I want to show that the normed space $C^2[0,1]$ with norm defined as $$\lVert f\rVert:=\max_{t\in[0,1]}\{\lvert f(t)\rvert+\lvert f''(t)\rvert\}$$ is a Banach space (and I have shown that this is indeed a norm). In order to show that this space is a Banach space, I want to show that this normed space is complete; i.e. all Cauchy sequences converge. So I thought about taking sequences of functions that are Cauchy sequences. The problem is that I don't know if I can, in addition, assume that the Cauchy-sequence are $C^r$ -stable; i.e. the distances between the $r$ -th derivatives (w.r.t. this norm) are bounded for arbitrarily small values of the norm. I also don't know if I'm even thinking in the right direction since at first sight this question doesn't seem to be that challenging. I think I miss some important theory of converging function w.r.t. its $r$ -th derivatives (although I'm familiar with $C^r$ stability as described above). Any useful words are appreciated, thanks in advance.","['complete-spaces', 'banach-spaces', 'normed-spaces', 'functional-analysis']"
3115658,Relation between tame symbol and residue on a curve,"For an discrete valuation field $K$ we can define the tame symbol: $$(\,,\,)_K:K^\times\times K^\times\to \overline K^\times$$ $$(a,b)\mapsto(-1)^{v(a)v(b)}\overline{a^{v(b)}b^{-v(a)}}$$ Consider now a smooth projective curve $X$ over a finite field $k$ and let $K=K(X)$ . We denote with $(\,,\,)_x$ the tame symbol for the complete discrete valuation field $K_x$ . For any element in the space of differential forms $K_xdt$ we have also the notion of residue. Suppose that $$\omega:=dt\sum a_it_i\in k(x)((t))\,.$$ then we define $\operatorname{res}_x(\omega):=a_{-1}\in k(x)$ . What is the relation between $(\,,\,)_x$ and $\operatorname{res}_x$ ? The both satify reciprocity laws: Weyl reciprocity law, and Tate reciprocity law. So I guess that we can express: $$(a,b)_x=\operatorname{res}_x(f(a,b))\,,$$ where $f:K_x^\times\times K_x^\times\to K_x^\times dt$ is ""a function"". Is this true, can we calculate explicitly the tame symbol by using the residue? For example we know that for $a$ in the invertible part of the local ring of $K_x$ : $$(a,t)_x=\operatorname{res}_x\left(\frac{a}{t}dt\right)$$ Many thanks in advance","['curves', 'algebraic-geometry', 'differential-forms', 'valuation-theory']"
3115671,Minimal conditions to ensure uniqueness of solutions of Schrödinger equation IVP,"Let us consider the free non-relativistic Schrödinger equation $$i\partial_t \psi =-\frac{1}{2}\partial_x^2 \psi=:H\psi.$$ Adapting Fritz John's pathological solution to the heat equation , I find that the non-zero smooth function $$\varphi:\mathbb{R}^2 \to \mathbb{C}:(x,t) \mapsto \sum_{n=0}^\infty f^{(n)}(t)\frac{x^{2n}(-2i)^n}{(2n)!}, \qquad f(t)\equiv e^{-1/t^2}$$ solves the free Schrödinger equation while reducing identically to zero as $t\to 0$ . This establishes that the Schrödinger equation, regarded as a PDE at face value, never offers a unique solution to an initial value problem. Traditionally, we add in the constraint that the solution of the Schrödinger equation ought to maneuver inside $L^2(\mathbb{R})$ in order to make the Born rule operable. However, usual treatments also add in strong-continuity-like ingredients so that we can finally handle the Schrödinger equation with a cosy and standard functional-analytic framework. However, the physical interpretation and requirement of these continuity ingredients is a bit obscure to me, certainly so since they are to a certain degree non-local (e.g. in the semigroup context , it is demanded that there is a dense core of ""classical solutions"" $t \mapsto \psi(t)$ characterized by $\forall t \in \mathbb{R}: H\psi(t) \in L^2(\mathbb{R})$ , which indeed has the flavour of a non-local weight condition). Q: Does Born's integrability condition $\psi(t)\in L^2(\mathbb{R})$ suffice to select unique solutions for the Schrödinger-equation-related IVP (or do we really need the additional $\partial_x^2\psi(t)\in L^2(\mathbb{R})$ or similar strong-continuity requirements)? EDIT(18/02/19): One is of course tempted to use $\psi \in L^2(\mathbb{R})$ to our advantage by allowing us to use the Fourier transform in the direction of $x$ : the Schrödinger equation then reads $i\partial_t \hat{\psi} = p^2 \hat{\psi}$ from where uniqueness seems easy to obtain. I'm unsure though what to say about the necessary ""differentiations under the integral sign"" and partial differentiations that are required along this line of thinking.","['partial-differential-equations', 'functional-analysis', 'mathematical-physics', 'reference-request']"
3115689,"What volume is enclosed by $k$ evenly-spaced, overlapping American footballs whose axes are diameters of a unit sphere?","Take $k \in \Bbb N$ intersecting American footballs and configure them inside a unit sphere such that each football touches two opposite ends of the sphere. Each of the shapes are spaced evenly apart. In the case $k\to \infty$ the volume should be equal to the volume of a sphere. Related Problem (intersection of $n$ congruent cylinders). I want to tackle the case $k=2$ first. So I'm using ""Cavalieri's Lemon"" which is the surface of revolution of a parabolic arc, to model a football. The equation for the parabolic arc to be revolved about the $x-$ axis is: $f(x)=x(1-x).$ What is the volume in the region where the (two) footballs overlap? I've calculated that the volume of one football is $\frac{\pi}{30}.$ If the interior shape is a well-known shape I can proabably search for the volume of that shape, but I'm having trouble visualizing the interior shape with the enclosed volume.","['volume', 'geometry', 'analysis', 'calculus', 'differential-geometry']"
3115712,Vector differential equation,"In electromagnetism we often have a perpendicular constant magnetic field causing a charge to move in a circle. My question is, how do we formally solve this differential equation which involves a cross product? $\frac{\mathrm{d} \mathbf{v}}{\mathrm{d}t} = \frac{qB_0}{m}\mathbf{v}\times{{\hat{\mathbf{k}}}}$ So I think I have made progress by splitting into 3 equations. Using that cartesian unit vectors are linearly independent and don't change with time. $\frac{\mathrm{d} v_x}{\mathrm{d}t} = \frac{qB_0}{m}v_y$ and $\frac{\mathrm{d} v_y}{\mathrm{d}t} = -\frac{qB_0}{m}v_x$ and $\frac{\mathrm{d} v_z}{\mathrm{d}t} = 0$ $v_z = v_z(0)$ , which is trivial. But what about the others? Also I can see that the derivative is a linear transformation of the vector $v_x,v_y$ and have found the matrix. What now?","['multivariable-calculus', 'electromagnetism', 'vector-analysis', 'ordinary-differential-equations']"
3115750,How to show that the space of probability measures on $\mathbb{R}$ is separable under Lévy metric,"The Lévy metric between distribution functions $F$ and $G$ is given by: $$\rho(F,G) = \inf\left\{\epsilon : F(x-\epsilon)-\epsilon\leq G(x)\leq F(x+\epsilon)+\epsilon\right\}.$$ Another way to write this is: $$\rho(F,G) = \inf\left\{\epsilon : F(x)\leq G(x+\epsilon)+\epsilon ; G(x)\leq F(x+\epsilon)+\epsilon\right\}.$$ I also know that $\rho(F_n,F)\rightarrow 0 \iff F_n\rightarrow F$ weakly. The question is to show that the space of probability measures, say $P$ , on $\mathbb{R}$ is separable with the Lévy metric. In other words, have to show $\exists$ a countable dense subset of $P$ . I was thinking about discretizing the distribution function of any given probability measure, along $x$ , and estimate the value in an interval by $\sup\{q: q\leq F(x), q\in Q\}$ i.e. approximating $F(x)$ by a rational number. But I am not sure as to how use Lévy metric to show that in any $\epsilon$ neighborhood of $F$ , $\exists F_Q$ . Thanks and appreciate a hint!","['weak-convergence', 'separable-spaces', 'real-analysis', 'convergence-divergence', 'probability-theory']"
3115772,"Difference between the projection matrices arising from 1) the normal equations, and 2) an orthonormal basis of a subspace.","I'm having a bit of trouble tying two related ideas together, and I think I'm just missing a silly detail somewhere. In the standard formulation of linear least-squares, it can be shown that given a full-rank matrix $A$ , one can form the projection matrix $$
P_A = A(A^TA)^{-1}A^T
$$ which acts on vectors by orthogonally projecting them onto the column space of $A$ . On the other hand, given a subspace $S \subseteq V$ of a vector space, where $S = \mathrm{Span}(\mathbf u_1, \mathbf u_2, \cdots \mathbf u_k)$ and the $\mathbf u_i$ form an orthonormal basis of $S$ , then one can construct a matrix with each $\mathbf u_i$ in the $i$ th column: $$ 
U = [\mathbf u_1, \mathbf u_2, \cdots \mathbf u_k].
$$ One can then form the projection $$
P_S = UU^T
$$ which projects vectors in $V$ onto the subspace $S$ . I have two questions: The matrix $U$ seems to be orthogonal, which would seem to mean it satisfies $UU^T = I$ . What exactly am I missing? The column space of $U$ is $S$ by construction, so how would one recover the formula for $P_S$ by letting $A=U$ in the formula for $P_A$ ? If $U$ is orthogonal, I see how $(A^TA)^{-1} = I$ , but by the first question, it would also seem to reduce the remaining $AA^T$ to $I$ as well.","['matrices', 'matrix-equations', 'linear-algebra', 'projection-matrices']"
3115812,"About Theorem 5.13 in ""Principles of Mathematical Analysis"" by Walter Rudin L'Hospital's Rule L'Hopital's Rule","I am reading ""Principles of Mathematical Analysis"" by Walter Rudin. Thank you Saaqib Mahmood. I copied and pasted your text Theorem 5.13 on p.109: Suppose $f$ and $g$ are real and differentiable in $(a, b)$ , and $g^\prime(x) \neq 0$ for all $x \in (a, b)$ , where $-\infty \leq a < b \leq +\infty$ . Suppose $$ \frac{f^\prime(x)}{g^\prime(x)} \to A \ \mbox{ as } \ x \to a. \tag{13} $$ If $$ f(x) \to 0 \ \mbox{ and } \ g(x) \to 0 \ \mbox{ as } \ x \to a, \tag{14} $$ or if $$ g(x) \to +\infty \ \mbox{ as } \ x \to a, \tag{15} $$ then $$ \frac{f(x)}{g(x)} \to A \ \mbox{ as } \ x \to a. \tag{16}$$ The analogous statement is of course also true if $x \to b$ , or if $g(x) \to -\infty$ in (15). Let us note that we now use the limit concept in the extended sense of Definition 4.33. Here is Definition 4.33: Let $f$ be a real function defined on $E \subset \mathbb{R}$ . We say that $$ f(t) \to A \ \mbox{ as } \ t \to x, $$ where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t) \in U$ for all $t \in V \cap E$ , $t \neq x$ . And, here is Rudin's proof: We first consider the case in which $-\infty \leq A < +\infty$ . Choose a real number $q$ such that $A < q$ , and then choose $r$ such that $A < r < q$ . By (13) there is a point $c \in (a, b)$ such that $a < x < c$ implies $$ \frac{ f^\prime(x) }{ g^\prime(x) } < r. \tag{17} $$ If $a < x < y < c$ , then Theorem 5.9 shows that there is a point $t \in (x, y)$ such that $$ \frac{ f(x)-f(y) }{ g(x)-g(y) } = \frac{f^\prime(t)}{g^\prime(t)} < r. \tag{18} $$ Suppose (14) holds. Letting $x \to a$ in (18), we see that $$ \frac{f(y)}{g(y)} \leq r < q \qquad \qquad \qquad  (a < y < c) \tag{19} $$ Next, suppose (15) holds. Keeping $y$ fixed in (18), we can choos a point $c_1 \in (a, y)$ such that $g(x) > g(y)$ and $g(x) > 0$ if $a < x < c_1$ . Multiplying (18) by $\left[ g(x)- g(y) \right]/g(x)$ , we obtain $$ \frac{ f(x) }{ g(x) } < r - r \frac{ g(y) }{g(x)} + \frac{f(y)}{g(x)} \qquad \qquad \qquad (a < x < c_1). \tag{20}$$ If we let $x \to a$ in (20), (15) shows that there is a point $c_2 \in \left( a, c_1 \right)$ such that $$ \frac{ f(x) }{ g(x) } < q \qquad \qquad \qquad (a < x < c_2 ). \tag{21} $$ Summing up, (19) and (21) show that for any $q$ , subject only to the condition $A < q$ , there is a point $c_2$ such that $f(x)/g(x) < q$ if $a < x < c_2$ . In the same manner, if $-\infty < A \leq +\infty$ , and $p$ is chosen so that $p < A$ , we can find a point $c_3$ such that $$ p < \frac{ f(x) }{ g(x) } \qquad \qquad \qquad ( a< x < c_3), \tag{22} $$ and (16) follows from these two statements. Rudin didn't write $g(x) - g(y) \neq 0$ for any $x, y$ such that $a < x < y < b$ in $$ \frac{ f(x)-f(y) }{ g(x)-g(y) } = \frac{f^\prime(t)}{g^\prime(t)} < r. \tag{18} $$ Is this fact so obvious? If so, please tell me the reason why this fact is so obvious. I didn't think this fact was so obvious, so I proved: By assumption, $g'(x) \neq 0$ on $(a, b)$ . Let $x, y$ be any real number such that $a < x < y < b$ . Let $x', y'$ be any real number such that $a < x' < x$ and $y < y' < b$ . Then, $g$ is a differentiable function on $[x', y']$ . By the Intermediate Value Theorem for derivatives (Theorem 5.12 on p.108), $g'(x) > 0$ for all $x \in [x', y']$ or $g'(x) < 0$ for all $x \in [x', y']$ . So, $g$ is strictly monotonically increasing on $[x', y']$ or $g$ is strictly monotonically decreasing on $[x', y']$ . So $g(x) < g(y)$ or $g(x) > g(y)$ . So, $g(x) - g(y) \neq 0$ .","['limits', 'calculus']"
3115820,"Why is the definition of cardinal number as the set of all sets equivalent to a given set ""problematical""?","In Fundamentals of Mathematics, Volume 1 Foundations of Mathematics: The Real Number System and Algebra , after defining set equivalence as the ability to put the elements of the related sets in one-to-one correspondence, the following statement appears: The cardinal number $\tilde{x}$ of a set $x$ is then regarded as representing ""that which is common"" to all sets that are equivalent to $x$ .  Thus, we might say that the cardinal number of $x$ is simply the set of all sets that are equivalent to $x$ , although such a definition is problematical on account of its relationship to the universal set. The term problematical can have a slightly different connotation than the term problematic .  The former implying requires expert handling .  In other words, this may not be grounds for completely rejecting the definition.  Unfortunately I do not have access to the German Language original to know what ""problematical"" was translated from. Regardless of that nuance, the authors are certainly indicating that their proposed definition leads to difficulty in ""relationship to the universal set"".  Is this difficulty simply Russell's antinomy?","['elementary-set-theory', 'definition', 'cardinals']"
3115841,"What is a phase shift in trigonometry, and how can I determine them given a graph?","(Yes, I know there are already some answers about phase shifts on here, but none of them have helped me much.) I am a Pre-Calculus student learning about trigonometric function transformations, and I am extremely confused about the definition of a phase shift (and hence very, very frustrated). We are learning about functions of the following two general forms: $y=A \sin(Bx-C)+D$ $y=A \cos(Bx-C)+D$ The definition of phase shift we were given was as follows: ""The horizontal shift with respect to some reference wave."" We were then provided with the following graph (and given no other information beyond that it was a transformed sine or cosine function of one of the forms given above): (Ignore my erased pencil markings in the graph!) We were asked to find the graphed function's phase shift with respect to the cosine parent function $y=\cos(x)$ . We were also asked to find the graphed function's phase shift with respect to the sine parent function $y=\sin(x)$ . (Of course, our answers would be eye-balled estimates based on the graph.) I was utterly mystified. The definition given above told me virtually nothing I wanted to know. And when I asked the teacher for an explanation, it did not end my confusion. Does anyone here have a good explanation of what a phase shift is, and perhaps also how (given solid knowledge of the definition) I could go about solving the graph problem mentioned above? I have spent a lot of time today searching around online -- in vain -- to try to end my confusion. Nothing has helped much. A lot of people seem to talk about phase shifts in terms of where a function ""starts."" But a periodic function that repeats on and on forever in either direction does not seem to me to have an intuitive ""start"" point. Even if one were to select some period to be the ""starting period,"" it is not clear to me how one would choose such a period. Moreover, while I understand that phase shifts have something to do with horizontal translations of the parent functions, it is not yet clear to me what precisely that relationship is -- particularly if the function we are analyzing has also been transformed in other ways (e.g. dilated and vertically translated), making things more complicated. Thank you for any help you can provide!","['algebra-precalculus', 'trigonometry']"
3115873,"Why is $\, \int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dy \} \, dx \, \neq \,\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dx \} \, dy \,$?","As far I know: Double Integrals of a function depend only on (i)Region of Integration and (ii)Function, and not on its order of integration . In this case : (i)Region of integration is a square (ABCD with AB=BC=CD=DA= $1$ units) where one of its vertices (A) lies on the origin and the opposite vertex is at C=( $1,1$ ); (ii) Function: $ f(x,y)=\frac{x-y}{(x+y)^3}$ Then, why is $$\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dy \} \, dx \, \neq \,\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dx \} \, dy \,\,\,\,?$$ i.e., $$\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dy \} \, dx \, =0.5 \,\, \& \,\,\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dx \} \, dy \,=-0.5 $$ Have I missed any concepts? Please help...","['multivariable-calculus', 'definite-integrals']"
3115883,Probability that $\displaystyle \vert x\vert +\vert y\vert +\vert z\vert +\vert x+y+z\vert=\vert x+y\vert +\vert x+z\vert +\vert y+z\vert$,"Real numbers $x, y$ , and $z$ are chosen from the interval $[−1, 1]$ independently and uniformly at random.
  What is the probability that $$\vert x\vert +\vert y\vert +\vert z\vert +\vert x+y+z\vert=\vert x+y\vert +\vert x+z\vert +\vert y+z\vert$$ Now if all of $x, y, z$ are positive or all negative then the equation is of course satisfied.  Hence if we consider a 3D space , it denotes two unit cubes, one in the first octant centred at $\left( \frac 12,\frac 12,\frac 12\right)$ and the other in seventh octant centred at $\left( -\frac 12,-\frac 12,-\frac 12\right)$ . The total measure of universal set is the cube with edge length $2$ centred at origin. But now I have a problem about what if any two of $x, y, z$ are positive while the other remaining be negative or the other way around.  Even if I try to make cases it seems to be quite a cumbersome task to approach since we will also need to check signs of $\vert x+y\vert$ and similarly others as well as that of $\vert x+y+z\vert $ I also thought to give a shot using vectors but didn't reach any specific result. Any help would be quite beneficial. Edit: I would also be happy to see a geometrical intuitive way to attack the problem.","['coordinate-systems', 'probability-theory', 'probability', '3d']"
3115885,Support for Fulton reading (chap 3 and 5) [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 5 years ago . Improve this question I am studying on my own the book of the Fulton (algebraic curves). My goal is to get to chapter 5. I happen to encounter difficulties in two chapters, 3 and 5. So I would like to know if there is another book, lecture notes etc ... that can help me in parallel to the Fulton before these two chapters. My experience in algebraic geometry is very small. But I know that for example answers like: use the Hartshone, is a joke of bad taste. And I really would like some help (a more detailed text) to go through Fulton's Chapters 3 and 5. Thanks","['algebraic-curves', 'algebraic-geometry', 'reference-request']"
3115903,Calculating iterated integral using Sagemath,"I would like to calculate the integral over the following domain (with order $x,y,z$ ) using Sagemath $$0 \le z \le 3, \max\{ 0,\frac{z-1}{2}\} \le y \le 1, \max \{y,z-2y\} \le x \le 1. \tag{1} \label{1}$$ This is equivalent to finding the integral (with order $z,y,x$ ) $$\int_{0}^{1} \int_{0}^{x}\int_{0}^{x+2y}  \mathrm dz\,\mathrm dy\,\mathrm dx=2/3.$$ However, when I tried to write this onto Sagemath as follow integrate(integrate(integrate(1,x,max(z-2*y,y),1),y,max(0,(z-1)/2),1),z,0,3) I obtained a wrong answer of $3/2$ . I tried to break $\eqref{1}$ down into smaller domain (see here ), I then got the correct answer. var(""x y z"")
X=integrate(integrate(integrate(1,x,z-2*y,1),y,0,z/3),z,0,1)
Y=integrate(integrate(integrate(1,x,y,1),y,z/3,1),z,0,1)
Z=integrate(integrate(integrate(1,x,y,1),y,z/3,1),z,1,3)
W=integrate(integrate(integrate(1,x,z-2*y,1),y,(z-1)/2,z/3),z,1,3)
C=X+Y+Z+W I would like to ask why is it so? How does Sagemath deal with $\max$ function in this case?","['multivariable-calculus', 'sagemath']"
3115912,"If $x = a \cos t^3 , y = b \sin t^3$ then what is $d^3y/dx^3$?","If $ x = a \cos t^3 $ , $ y = b \sin t^3 $ , then what is $ \frac{d^3y}{dx^3} $ ? 
I tried doing this problem by dividing $ \frac{d^3y}{dt^3} $ by $ \frac{d^3x}{dt^3} $ and got $ \frac{b}{a} $ . 
However my book says the third derivative doesn't exist. Why is this so?","['calculus', 'derivatives', 'trigonometry']"
3115918,"For what values of $a$ and $b$ is the function $\frac{x^ay^b}{x^2+y^2}$ continuous at $(0,0)$?","I have the function $$f(x,y)=\begin{cases}\dfrac{x^ay^b}{x^2+y^2} &(x,y)\neq(0,0)\\ 0 &(x,y)=(0,0) \end{cases}$$ I am trying to figure out what constants $a$ and $b$ will make the function continuous at $(0,0)$ . I know that the limit has to be $0$ as $(x,y)\to(0,0)$ for it to be continuous. Using polar coordinates, I think that $a+b \geq 3$ since $x^2+y^2= r^2$ and the limit of any polar function $r^x$ with $x > 0$ as $r\to0$ is $0$ . Am I on the right track or am I missing something?","['multivariable-calculus', 'limits', 'calculus', 'continuity']"
3115931,Finding eigenvector only knowing others eigenvectors.,"The matrix $A \in M_3(\mathbb{R})$ satisfy $A^t=A$ and $(1,2,1), (-1,1,0)$ are eigenvectors of $A$ . Which vector is also an eigenvector of $A$ ?
Alternatives: $(0,0,1)$ ; $(1,1,-3)$ ; $(1,1,3)$ ; There is no other eigenvector . The problem with this exercise is that I don't know the matrix $A$ , and I don't have any eigenvalue to start with. I can get a matrix with less variables using $A = A^t$ , but there's still 6 variables. Any tips or guidance is appreciated.","['linear-algebra', 'eigenvalues-eigenvectors']"
3115943,Proof that continuous curve in $\mathbb{R}^2$ has Lebesgue measure zero,"Suppose $\Gamma$ is a curve $y = f(x)$ in $\mathbb{R}^2$ , where $f$ is continuous. Show that $m(\Gamma)=0.$ [Hint: Cover $\Gamma$ by rectangles, using the uniform continuity of $f$ .] My attempt: Let $\varepsilon>0$ be given. Take the real line and consider the segments with endpoints at the integers. Without loss of generality now consider the segment $[0,1].$ Note that this is a closed and bounded set, so $f$ is uniformly continuous on it. Let $0<\delta<1$ be the one that works for this uniform continuity, and if necessary, pick a smaller $\delta$ which can partition the interval evenly. Then we can write $[0,1]=[0,\delta]\cup[\delta,2\delta]\cup\cdot\cdot\cdot\cup[n\delta,1],$ for some $n\geq1$ . Then for $x\in [i\delta,(i+1)\delta],$ with $i=0,\dots,n-1$ , the curve is enclosed inside a rectangle of width $\delta$ and height $2\varepsilon$ . Hence the measure of that portion of the curve is smaller than or equal to $2\delta\varepsilon.$ Since $\varepsilon>0$ was  arbitrary, it follows that each such portion of the curve has measure zero, and hence the same follows for every other portion. Repeating this process for every other interval in the real line with endpoints at the integers yields the same result, and thus by countable sub-additivity, the result follows. Is the proof above correct? Any comments are welcomed, be it about correctness or the quality of the style of the proof. Thank you for your time and feedback.","['measure-theory', 'proof-verification', 'lebesgue-measure', 'real-analysis']"
3115955,Prove that $(a+b) (a^2 + b^2) (a^4 + b^4)...(a^{32} + b^{32}) = a^{64} - b^{64}$ if $b = a-1$,"Prove that if $b = a-1$ , then $(a + b) (a^2 + b^2 ) (a^4 + b^4 ) ... (a^{32} + b^{32} ) = a^{64} - b^{64}$ . I saw this in a website and it wrote this: hint: Write down the equality $1 = a+b$ and use the formula $k^2 -n^2 = (k-n) (k + n) $ Decision: We write the equality $1 = a+b$ and use the formula $k^2 -n^2 = (k-n) (k + n)$ (If you are not familiar with the formulas of reduced multiplication, prove this formula: multiply the expression on the right-hand side.). We write the expression $1*(a+b)(a^2+b^2)(a^4+b^4)…(a^{32}+b^{32})= (a-b)(a+b)(a^2+b^2)(a^4+b^4)…(a^{32}+b^{32})= (a^2-b^2)(a^2+b^2)(a^4+b^4)…(a^{32}+b^{32})= (a^4-b^4)(a^4+b^4)(a^8+b^8)(a^{16}+b^{16})(a^{32}+b^{32})= (a^8-b^8)(a^8+b^8)(a^{16}+b^{16})(a^{32}+b^{32})= (a^{16}-b^{16})(a^{16}+b^{16})(a^{32}+b^{32})= (a^{32}-b^{32})(a^{32}+b^{32})=a^{64}-b^{64}$ I don't know how this formula $k^2 - n^2 = (k-n)(k+n)$ come out?
Also, I don't understand the decision part?","['contest-math', 'algebra-precalculus']"
3115978,Theorem of Pappus,"Given a surface of revolution $S$ which can be parametrized by the map $$
\mathbf x(u,v) = (f(v)\cos u,f(v)\sin u,g(v)),
$$ over the open set $U =\{(u,v) \in \mathbb R^2 \mid 0 < u < 2\pi, a < v < b\}$ , I computed the area of $S$ to be \begin{align*}
\int_a^b\int_0^{2\pi} |\mathbf x_u \times \mathbf x_v| \, du \, dv = 2\pi\int_a^b |f(v)| \sqrt{(f'(v))^2+(g'(v))^2} \, dv.
\end{align*} If $l$ is the length of the generating curve $C$ , how does one then get the area of $S$ to also be written $$
2\pi \int_0^l \rho (s) \, ds,
$$ where $\rho=\rho(s)$ is the distance to the rotation axis of the point $C$ corresponding to $s$ ? I think that the arc length $s=\int_a^b |\alpha'(t)| \, dt$ , where $\alpha$ is the space curve, but I'm not sure in particular how one changes the interval $[a,b]$ to $[0,l]$ when changing the variable $v$ to $s$ .","['surface-integrals', 'surfaces', 'differential-geometry']"
3116001,What is the largest possible expectation of difference between two i.i.d. random vectors in the separable Hilbert space?,"Suppose $M$ is a compact subset of the separable Hilbert space $l_2$ . Suppose, $X$ and $Y$ are i.i.d. random vectors with support in $M$ . What is the largest possible $E \| X - Y \| $ ? Suppose $M$ is a compact subset of the separable Hilbert space $l_2$ . Then $M$ is closed and bounded, and there exist such $x$ and $y$ in $M$ , such, that $\| x - y \| = \operatorname{diam}(M)$ . Now, suppose $X$ and $Y$ are i.i.d. random vectors, such that $P(X = x) = P(X = y) = \frac{1}{2}$ . One can see, that $$E\| X - Y \| = \frac{1}{4}\| x - x \| + \frac{1}{4}\| x - y \| + \frac{1}{4}\| y - x\| + \frac{1}{4}\| y - y \| = \frac{1}{2}\operatorname{diam}(M)$$ Now let’s prove that it is the maximal possible expected distance, or to be more exact, that if $X = (X_n)_{i = 1}^{\infty}$ and $Y = (Y_n)_{i = 1}^{\infty}$ are i.i.d. random vectors with support in $M$ , then $E\| X - Y \| \leq \frac{1}{2}\operatorname{diam}(M)$ . By Hölder inequality: $$E\| X - Y \| \leq \left(E(\| X - Y \|^2)\right)^{\frac{1}{2}}.$$ And one can see, that \begin{align*}
E(\| X - Y \|^2)
&= E\langle X - Y, X - Y \rangle
 = 2E\langle X, X \rangle - 2E\langle X, Y \rangle \\
&= 2\left(\sum_{n = 1}^\infty E(X_n)^2 - \sum_{n = 1}^\infty EX_n Y_n \right) \\
&= 2\left(\sum_{n = 1}^\infty E(X_n)^2 - \sum_{n = 1}^\infty EX_n EX_n\right ) \\
&\leq 2\left(\sum_{n = 1}^\infty E(X_n)^2 \right) \\
&= 2E\langle X, X \rangle = 2E(\| X  \|^2)
\end{align*} Now, suppose, that the least closed ball containing $M$ is the closed ball with radius $\frac{1}{2}$ and center $0$ . That will result in $\| X  \|$ being a random variable on $[-1, 1]$ . So, its second moment does not exceed $\frac{1}{4}$ (There are several proofs of this fact here: What is the largest possible variance of a random variable on $[0; 1]$? ), and we get $E\| X - Y \| \leq \frac{1}{\sqrt{2}}$ . And now let’s return to the general case. Suppose $z$ is the center of the least closed ball containing $M$ . Then $\frac{M - z}{\operatorname{diam}(M)}$ is such a subset, that the least closed ball containing it is the closed ball with radius $\frac{1}{2}$ and center $0$ . So $$E\| X - Y \| = \operatorname{diam}(M)E \left\| \frac{X - z}{\operatorname{diam}(M)} - \frac{Y - z}{\operatorname{diam}(M)}\right\| \leq \frac{1}{\sqrt{2}}\operatorname{diam}(M)$$ So we know, that the largest possible $E\| X - Y \|$ we search is certainly $\geq \frac{1}{2}\operatorname{diam}(M)$ and certainly $\leq \frac{1}{\sqrt{2}}\operatorname{diam}(M)$ . However, I do not know, how to find its exact value. This question is partially inspired by the following question: Probability distribution to maximize the expected distance between two points","['hilbert-spaces', 'expected-value', 'linear-algebra', 'probability-theory', 'probability']"
3116013,Differentation of vector,"in the below equation, $\mathbf w$ is a vector with components $w_0$ and $w_1$ . $x^{(i)}$ and $y^{(i)}$ are constants. how to differentiate $j(\mathbf w)$ w.r.t. $w_0$ and $w_1$ $j(\mathbf w) = \frac{1}{10}\sum_{i=1}^5(x^{(i)}+w_1y^{(i)}-w_0)$","['multivariable-calculus', 'calculus', 'vector-analysis']"
3116014,Tricky integral? $\int_0^{\frac{\pi}{2}}\arccos(\sin x)dx$ My answer doesn't match an online calculator,"I tried to calculate this integral: $$\int_0^{\frac{\pi}{2}}\arccos(\sin x)dx$$ My result was $\dfrac{{\pi}^2}{8}$ , but actually, according to https://www.integral-calculator.com/ , the answer is $-\dfrac{{\pi}^2}{8}$ . It doesn't make sense to me as the result of the integration is $$x\arccos\left(\sin x\right)+\dfrac{x^2}{2}+C$$ and after substituting $x$ with $\dfrac{{\pi}}{2}$ and $0$ , the result is a positive number. Can someone explain it? Thanks in advance!","['integration', 'trigonometry', 'definite-integrals']"
3116029,Exercise 36 Ch 1 in Stein's Real Analysis [duplicate],"This question already has an answer here : Construct a Borel set on R such that it intersect every open interval with non-zero non-""full"" measure (1 answer) Closed 5 years ago . Construct a measurable set $E\subset [0,1]$ such that for any
  non-empty open sub-interval $I$ in $[0,1]$ , both sets $E\cap I$ and $E^c\cap I$ have positive measure. [Stein's Hint: For the first part, consider a Cantor-like set of positive measure, and add in each of the intervals that are omitted in the first step of its construction, another Cantor-like set. Continue this procedure indefinitely.] So I tried using the hint to construct the desired set, but I must be misunderstanding something because I do not think the construction works. My Thoughts: I believe the basic idea is that we generate a Cantor-like set $C_1$ by removing repeatedly open intervals of some appropriate length at each stage of the construction starting from $[0,1]$ . Then as said in the hint, at the first step in the construction we removed centrally some open interval $I_1$ from $[0,1]$ . Then we generate another Cantor-like set $C_2$ from $I_1$ . During the first stage of $C_2$ 's construction we removed $I_2$ from $I_1$ . Repeat indefinitely... My problem with this is that during the second stage of the construction of $C_1$ , we removed $2$ open intervals, call one of them $\mathcal{I}$ , from $[0,1]\setminus I_1$ .
If we take the union of all these Cantor-like sets, denote it as $E$ , then wouldn't $E\cap \mathcal{I}=\emptyset$ ? If true, then it seems we would have to apply this to all the open intervals that $C_1$ removes and generate a collection of Cantor-like sets from them. Each of which would need the same procedure done to them. This doesn't seem correct to me and we may need to define $E$ as the countable union of countable unions of Cantor-like sets. Any help thinking correctly about this problem would be much appreciated. Note : It's proven (I've proved) that the Cantor-like sets Stein is talking about have positive measure (Exercise 4 in Stein).","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3116044,"Is there any natural number triangle whose inscribed circle's radius is $1$ except length $(a,b,c)=(3,4,5)$?","Is there any natural number triangle that inscribed circle's radius is $1$ except length $(a,b,c)=(3,4,5)$ ? I found that there are no right triangle except $(3,4,5)$ . Thm. There are only one natural number right triangle that inscribed circle's radius is 1. Proof ) WLOG , $0<a<b<c$ . Than $$a^2+b^2=c^2$$ $$a+b+c=ab$$ . Because it is right triangle, $$a-1+b-1=c$$ .So, $$b=\frac{2(a-1)}{(a-2)}$$ If $a$ is even, $b$ is not a natural number.Because if $a=2k$ , $b=\frac{2k-1}{k-1}=2+\frac{1}{k-1}$ . Only integer solution in $k=2$ . If $a$ is odd, $a=2k+1$ , $a-1$ is even but $a-2$ is odd. So $b$ is not integer except $a-2=1$ . I want to know general case about this. Such triangle is exist or not exist. I think It doesn't exist. I think about it and I got one. Check It for me wrong or not. We could make $$(a,b,c) =(x+y,y+z,z+x)$$ and by Heron's formula radius $$1= \frac{\sqrt{xyz}}{\sqrt{x+y+z}}$$ so $$x+y+z=xyz$$ and we knows only solution of this equation is $(x,y,z)=(3,2,1)$ .","['elementary-number-theory', 'triangles', 'geometry', 'euclidean-geometry']"
3116053,show $p\mid 2^n-n$ for infinitely many $n$ [duplicate],This question already has answers here : Infinitely many $n$ such that $2^n-n$ is divisible by a prime (2 answers) Closed 5 years ago . Show that $p\mid 2^n-n$ for infinitely many $n$ . $p$ is a prime and $n$ is an integer. I tried using Fermat's little theorem and got $2^p-p\equiv2\pmod p$ and $2^{p-1}-(p-1)\equiv2\pmod p$ . So I can't even find one $n$ that satisfy the condition. Any helps and hints appreciated. (even just one $n$ that works for every $p$ ),"['number-theory', 'discrete-mathematics', 'modular-arithmetic', 'elementary-number-theory']"
3116075,Is there any elementary solution for this problem on colored interval?,"The problem is as following. Assume $m,n$ are two coprime odd numbers, consider the interval $[0,mn]$ . We cut the interval by $m,2m,\ldots,(n-1)m$ and $n, 2n,\ldots, (m-1)n$ into $m+n-1$ pieces of small intervals. And we color them from left to right by black-and-white periodically and black first. The question is to show $$(\textrm{the length of  black})-(\textrm{the length of white})=1$$ For example, if $m=3,n=5$ , $$\begin{array}{c*{31}}0 &&&&&& 3 &&&&&& 6 &&&&&& 9 &&&&&& 12 &&&&&& 15 \\ \mid & \blacksquare && \blacksquare &&\blacksquare &\mid & \square && \square & \mid  & \blacksquare & \mid  & \square&& \square&& \square &\mid & \blacksquare &\mid & \square&& \square&\mid & \blacksquare&& \blacksquare&& \blacksquare & \mid  \\ 0
 &&&&&&&&&& 5 &&&&&&&&&& 10 &&&&&&&&&& 15\end{array} $$ The length of black is $8$ and the length of white is $7$ . In my blog , I presented two ""analytic"" solutions, which are both due to Liu Ben, using the trick of exponential sums. They are so nice in the sense of ""analytic"" approach. At the end of the post, I came up a ""elementary"" solution I want to explain more precisely here. Consider a billiards table of size $m\times n$ , and struck the billiard from one of corners on $45^{\circ}$ . Every time the  ball knockes the boundary, it changes its color. Then $$(\textrm{the length of black interval})=\sqrt{2}\times (\textrm{the length of orbit of black ball})\qquad {(*)}$$ and $$(\textrm{the length of  white  interval})=\sqrt{2}\times (\textrm{the length of orbit of white ball})\qquad {(**)}$$ So it suffices to count the length of the grid at the  direction of $\diagup$ and $\diagdown$ , which is not difficult to compute. But in this solution, it is not easy to explain why $(*)$ and $(**)$ holds, though we know it from ""life experience"". And I think there may be some more elementary method to solve this problem. So my questions are Is there any elementary solutions to this problem? Is there any ""strict process"" for the billiard problem ? Moreover, although I learnt it no more than 1 mouth ago, I think it is so genius that it must be classical and discovered by ancients. So are there any reference for this problem ?","['number-theory', 'combinatorics', 'elementary-number-theory', 'reference-request']"
3116077,"Given $n^2$ different numbers to form a $n$ -degree matrix, prove that the number of possible determinants is at most $\frac{n^2!}{(n!)^2}$","Given $n^2$ different numbers from a field to form a $n$ -degree matrix, prove that the determinant can take  at most $$\frac{(n^2)!}{(n!)^2}$$ values. The number of different matrices is $(n^2)!$ . 
So it suffices to prove that for every matrix, there are $(n!)^2$ matrices which have the same determinant. For a given matrix, we can arrange its columns arbitrarily, and after each column arrangement, we can perform a row arrangement to it. So under such operations, this matrix can produce $(n!)^2$ different matrices. If we want the later matrix to have the same determinant as the given one, we should confirm that the compound odevity of the column arrangement and the row arrangement to be even. Thus, only $
\text{even}\times\text{even}
$ and $
\text{odd}\times\text{odd}
$ can produce the same determinant. Therefore, I can only find $\frac{(n!)^2}{2}$ matrices that have the same determinant for each given matrix. So my question is where are the ""other"" $\frac{(n!)^2}{2}$ matrices?","['matrices', 'determinant', 'linear-algebra']"
