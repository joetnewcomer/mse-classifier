question_id,title,body,tags
1386802,Find the value of this infinite term,"goes on till infinity. I get two solutions by rewriting the term in the form of the equation $x = 3-(2/x)$ , which are $1$ and $2$ . But in my opinion this term should have only one possible value. Then which one is wrong and why?","['limits', 'continued-fractions']"
1386806,How does WolframAlpha solve quintic equations (5th degree polynomials)?,There exists no general formula for quintic equations. Then how does WolframAlpha solve quintic or higher degree polynomial. Is there a sure way to get values of roots of quintic or higher degree polynomials without brute forcing (especially non integer roots)?,"['calculus', 'wolfram-alpha', 'number-theory', 'real-analysis', 'algebra-precalculus']"
1386818,"Does my insurance company commit Gambler's Fallacy, or do I?","I would not be able to put this into symbols, but I ask here because I think it's the correct place to ask. Would the chance of my parked car getting damaged (bumped or scraped) by other cars parking nearby increase over time? Gambler's fallacy says: if something happens less frequently than normal during some period, it will happen more frequently in the future. (wikipedia) For every time I leave my car parked, there is a chance it will get damaged. If I don't want to commit the gambler's fallacy, I should consider the chance of damage the same every time I park. But if I park at the same spot every day for many years, the chance that my car would have been damaged after all those years, would surely be greater than if I just parked there one day, right? How does this not contradict the gambler's fallacy? My insurance company asks a higher premium if I park on the street all year round, than in a garage, so somehow they must figure that the chance is higher than if I just park on the street one day. How does this not contradict the gambler's fallacy?","['logic', 'probability', 'statistics']"
1386829,Spectral resolution of multiplication operator,"Kosaku YOSIDA claims in his book ""Functional Analysis"" that it is easy to see that the multiplication operator $Hx(t) = tx(t)$ in $L^2(-\infty,+\infty)$ admits the spectral resolution $H = \int_{-\infty}^{+\infty} \lambda \, \mathrm{d}E(\lambda)$, where the resolution of the identity is defined via $E(\lambda)x(t) = x(t)$ for $t \leq \lambda$, $=0$ for $t > \lambda$. At this point in the book (Chapter XI.6 Normed Rings and Spectral Representation: The Spectral Resolution of a Self-adjoint Operator) the Spectral theorem is NOT yet proven. I have problemes with the ""easy to see""-part. I think he tries to argue that $\int_{-\infty}^{+\infty} \lambda^2 \, \mathrm{d} \| E(\lambda) x \|^2 = \|Hx\|^2$ and $\int_{-\infty}^{+\infty} \, \mathrm{d} \langle E(\lambda) x , y \rangle = \langle Hx, y \rangle$. But how does this show the claim?","['analysis', 'spectral-theory', 'hilbert-spaces', 'functional-analysis']"
1386859,Why is $f(x)=\sqrt x $ not a function?,"Why is $f(x)=\sqrt x$ not a function? I understand that the definition of a function states that every ""input"" must be related to exactly one ""output"", but I am curious as to the WHY.",['functions']
1386864,"Why can $\int_{x=0}^{\infty} x\,\mathrm{e}^{-\alpha x^2}\mathrm {d}x$ not be evaluated by parts to obtain $\frac{1}{2\alpha}$?","Can $\int_{x=0}^{\infty} x\mathrm{e}^{-\alpha x^2}\,\mathrm {d}x$ be evaluated by parts to show that $\int_{x=0}^{\infty} x\mathrm{e}^{-\alpha x^2}\,\mathrm {d}x= \frac{1}{2\alpha}$ I know that this can be done without parts by  means of the substitution $-\alpha x^2=u \Rightarrow -2\alpha x\, \mathrm {d}x=\mathrm {d}u$ , then $x\mathrm dx=\frac {-1}{2\alpha}\mathrm du$ ; such that $\displaystyle\int_{x=0}^{\infty} xe^{-\alpha x^2}\,dx=\frac {-1}{2\alpha}\displaystyle\int_{x=0}^{\infty} e^u \mathrm du= \left [ \frac {-1}{2\alpha}\displaystyle e^{-\alpha x^2} \right]_{x=0}^{\infty}=\frac{1}{2\alpha}$ However; Can it be done like this? $\color{blue}{I}$ $=\displaystyle\int_{x=0}^{\infty} xe^{-\alpha x^2}\,dx= x\int_{x=0}^{\infty}e^{-\alpha x^2} - \int_{x=0}^{\infty}\left(\int_{x=0}^{\infty}e^{-\alpha x^2}\mathrm dx\right)\mathrm{d}x$ Now $$\int_{x=0}^{\infty}e^{-\alpha x^2}\mathrm dx= \frac{1}{2}\left(\frac{\pi}{\alpha}\right)^\frac{1}{2}\tag{1}$$ Now by insertion of $(1)$ into $\color{blue}{I}$ yields, $\color{blue}{I}=\displaystyle\int_{x=0}^{\infty} xe^{-\alpha x^2}\,\mathrm{d}x=\underbrace{ \left [ x\frac{1}{2}\left(\frac{\pi}{\alpha}\right)^\frac{1}{2} \right ]_{x=0}^{\infty} - \int_{x=0}^{\infty} \frac{1}{2}\left(\frac{\pi}{\alpha}\right)^\frac{1}{2} \mathrm {d}x}_{\large\text{$\color{red}{\mathrm{Undefined}}$}}\ne \frac{1}{2\alpha}$ which is clearly a contradiction. I realize that this may be blatantly obvious to many of you why this will never work, but it is not clear to me. What I can't understand is that if asked to evaluate say $\int x\sin x\,\mathrm{d}x$ by parts I would do the following: $\int x\sin x\,\mathrm dx= x\int \sin x\,\mathrm dx-\int\left(\int \sin x\,\mathrm dx\right)\mathrm{d}x= \sin x -x\cos x$ + C. But the method does not work for $\int_{x=0}^{\infty} x\mathrm{e}^{-\alpha x^2}\,\mathrm dx$ by parts as shown above. Why is this? Could someone please explain to me why I cannot perform this integral by parts to get the answer $\dfrac{1}{2\alpha}$ ?","['calculus', 'exponentiation', 'gaussian-integral', 'definite-integrals', 'integration']"
1386875,Inner Functions in Annuli: Not Likely!,"The other day someone reminded me of something I'd thought about some years ago. As back then it took me a little while to see why there was any problem; this time I got much farther on a solution than I did back then. This is supposed to be a question. Question: Is the stuff below (especially the result at the very end) actually true? Question: Reference? Background If $\Omega\subset\Bbb C$ is open the notation $H^\infty(\Omega)$ denotes the bounded holomorphic functions in $\Omega$. If $\Bbb D$ is the unit disk it's very well known that $f\in H^\infty(\Bbb D)$ has radial limits, in fact ""non-tangential"" limits, at almost every boundary point. A function $f\in H^\infty(\Bbb D)$ is an inner function if the boundary values have modulus $1$ almost everywhere. If $f\in H^\infty(\Bbb D)$ and the zeroes of $f$ are $(a_j)$ then $$\sum(1-|a_j|)<\infty.$$Conversely, this condition implies that $(a_j)$ is the zero set of some inner function (a Blaschke product). The Problem Now fix $R\in(0,1)$ and let $A$ be the annulus $R<|z|<1$. Define ""inner function"" in the obvious way. What are the zero sets of the inner functions in $A$? Obvious Conjecture (OC) The obvious conjecture is that $(a_j)$ is the zero set of an inner function in $A$ if and only if $$\sum d(a_j,\partial A)<\infty.$$Seems like an obvious conjecture to me anyway. The zero set of any (non-trivial) $f\in H^\infty(A)$ satisfies this  condition; this follows in any of various ways from the result in the disk. Conversely, given $(a_j)$ with $\sum d(a_j,\partial A)<\infty$, separate the $a_j$ into two classes, those closer to $|z|=1$ and those closer to $|z|=R$. Let $B_1$ be the Blachke product formed from the $a_j$ in the first class, and let $B_2$ be the Blaschke product with zeroes at $R/a_j$ for $a_j$ in the second class. Let $$f(z)=B_1(z)B_2(R/z).$$For minute I thought that was an inner function. Of course it's not. But it is a bounded holomorphic function, so we've established this: Theorem The sequence $(a_j)$ is the zero set of some $f\in H^\infty(A)$ if and only if $\sum d(a_j,\partial A)<\infty$. To get an inner function with the right zero set I thought I'd try constructing some things analogous to Blaschke products. So given $a\in A$ I want to construct or say something about an inner function with just one zero, at $a$; then I could hope that a product of such things converged. Getting an inner function with zero set $\{a\}$ is very simple. Just say $$f(z)=(z-a)e^{u(z)+iv(z)},$$where $u$ is the solution to the Dirichlet problem with boundary data $-\log|z-a|$ and $v$ is a harmonic conjugate of $u$. For a few hours some years ago and a few hours the other day I thought that did it. Oops , harmonic functions need not have harmonic conjugates. Too much time in the disk... So this raises the question of which harmonic functions in $A$ have harmonic conjugates. And then come to think of it, $e^{u+iv}$ can be single-valued even if $v$ is not; the actual question of interest is which harmonic functions in $A$ are equal to $\log|f|$ for some non-vanishing holomorphic function $f$. I was surprised that both these questions have simple answers. In general let $u^\#$ be the radialization or radial part of $u$: $$u^\#(z)=\frac1{2\pi}\int_0^{2\pi}u(e^{it}z)\,dt.$$ Calculus $$r\frac d{dr}u^\#(r)=\frac1{2\pi i}\int_{|z|=r}2\frac{\partial u}{\partial z}\,dz.$$(Express $\partial u/\partial z$ in polar coordinates...) Now note that if $u$ is harmonic in $A$ then there exist $a,b\in\Bbb R$ with $$u^\#(r)=a+b\log(r).$$We will say $b(u)=b$ below. Theorem Suppose $u$ is harmonic in $A$. (i) The function $u$ has a harmonic conjugate if and only if $b(u)=0$. (ii) There exists a nonvanishing holomorphic $f$ with $u=\log|f|$ if and only if $b(u)\in\Bbb Z$. Proof (i) One direction is just Cauchy's Theorem. For the other direction, suppose $b(u)=0$. Note that $\partial u/\partial z$ is holomorphic. The calculus above shows that $\int_\gamma\partial u/\partial z=0$ for any closed curve $\gamma$. Hence $\partial u/\partial z$ has an antiderivative: There exists a holomorphic $f$ with $2\partial u/\partial z=f'=\partial f/\partial z$. This says that $2u-\overline f$ is holomorphic. Hence $2u-(\overline f+f)$ is holomorphic and real-valued, hence constant. (ii) Suppose that $u=\log|f|$. Then $2\partial u/\partial z=f'/f$. Since $\frac1{2\pi i}\int_{|z|=r}f'/f\in\Bbb Z$ the calculus above shows that $u^\#=c+n\log(r)$, $n\in\Bbb Z$. Conversely, suppose $b(u)=n\in\Bbb Z$. Let $v(z)=u(z)-n\log|z|$. Part (i) shows that $v$ has a harmonic conjugate $w$, and hence $v=\log\left|e^{v+iw}\right|$. So $u=\log\left|z^ne^{v+iw}\right|$. QED. We can now determine which finite subsets of $A$ are the zero sets of inner functions. Given $a_1,\dots,a_n\in A$, let $$p(z)=\prod_{j=1}^n(z-a_j),$$and let $u$ be the solution to the Dirichlet problem with boundary data $-\log|p|$. There exists an inner function with zero set the same as $p$ if and only if there exists $f$ with $u=\log|f|$. This happens if and only if $b(u)\in\Bbb Z$. One can calulate $u^\#(1)$ and $u^\#(R)$ explicitly, and it turns out that Theorem Suppose $a_1,\dots,a_n\in A$. There exists an inner function with precisely these zeroes if and only if $$\frac1{\log(R)}\sum_{j=1}^n\log|a_j|\in\Bbb Z.\quad(*)$$That seems interesting enough to justify reading this far, not that anybody will. Corollary Inner functions are not as fundamental in $A$ as they are in $\Bbb D$. Corollary $n=1$ is impossible: There is no inner function in $A$ with exactly one zero. But $n=2$ is possible. Very curious. That's as far as I got way back then. What about inner functions with infinitely many zeroes? Condition ($*$) makes no sense. Say $R<R'<1$. An analysis like the above, with that $B_1(z)B_2(R/z)$ thing in place of $p$, proves this: Theorem . Suppose $a_1,\dots\in A$. There exists an inner function with zero set $(a_j)$ if and only if $\sum d(a_j,\partial A)<\infty$ and $$\frac1{\log(R)}\left(\sum_{|a_j|>R'}\log|a_j|-\sum_{|a_j|\le R'}\log\left|\frac{R}{a_j}\right|\right)\in\Bbb Z.$$I'll spare you the details. Heh.","['hardy-spaces', 'reference-request', 'proof-verification', 'complex-analysis']"
1386885,Self-avoiding rook walks on small rectangular chessboards (contest question),"I am not sure how to get a closed-form formula for $R(3,n)$ as the recursion involves a summation. Maybe the best that can be achieved is a recursion that does not involve a summation having an upper index that increases with grid size. The WolframAlpha article considers a similar but not identical diagonal walk on a rectangular grid. This is problem number five from the 2008 Canada National Olympiad: A self-avoiding rook walk on a chessboard (a rectangular grid of unit squares) is a path traced by a sequence of moves parallel to an edge of the board from one unit square to another, such that each begins where the previous move ended and such that no move ever crosses a square that has previously been crossed, i.e., the rook’s path is non-self-intersecting. Let $R(m, n)$ be the number of self-avoiding rook walks on an $m×n$ ($m$ rows, $n$ columns) chessboard which begin at the lower-left corner and end at the upper-left corner. For example, $R(m, 1) = 1$ for all natural numbers $m$; $R(2, 2) = 2$;
  $R(3, 2) = 4$; $R(3, 3) = 11$. Find a formula for $R(3, n)$ for each natural number $n$ . So far, I have deduced that: Once the rook moves to the top row, it will need to move left on the next move, and at least as far as the rightmost open square on an ""open"" block of, say $p$ squares $(p\ge2)$, in the second row. This is equivalent to the rook making a forced move to exactly $(3,p)$ then deciding where to move in a $2\times{p}$ rectangle, so is the rotated equivalent of the original problem for $R(p,2)$. (There may be multiple disjoint open blocks in the second row). Once the rook completes a move to the left in the second row, it will then need to move up to row 3, then proceed as per rule 1. Sub-problem 1: finding $R(p,2)$ The path is completely determined by the $(p-1)$ choices of which rows (excluding the last, potentially including the first) on which to move from one side to another. So $R(p,2) = 2^{p-1} \tag{1}$ This is also applicable if the rook starts out one square to the right (and the walk is between diagonally opposite corners). Enumerating possible walks Introduce the following further notation: $W(3,q)$ : a complete walk for the $R(3,q)$ problem, $q < n$ $W(p,2)$ : a complete walk for the $R(p,2)$ problem, $p < n$ $U^i,D^i,L^i,R^i$ : move of $i$ squares in the up,down,left,right direction to be followed by a move in another direction ($i$ may be omitted if equal to $1$) $R^{i+}, \dots$ : move of $i$ squares to the right,$\dots$ that can be followed by a move in $any$ direction (including a continuation in the same direction) I have identified the following distinct walks: $U^2$ (trivial) $UR^xU$ followed by forced moves, $1 \le x \le n-1$ $UR^xDR^{1+} \rightarrow W(3,n-x-1) \rightarrow\text{ forced},\:1 \le x \le n-2$ $R^xU^2L^{1+} \rightarrow W(x,2),\:1 \le x \le n-1$ $R^xUL^{1+} \rightarrow W(x,2),\:1 \le x \le n-1$ $R^xUR^yU \rightarrow L^{y+1} \rightarrow W(x,2),\:x,y \ge 1,\:x+y \le n-1$ $R^xUR^yDR^{1+} \rightarrow W(3,n-x-y-1) \rightarrow L^{y+2} \rightarrow W(x,2),\:x,y \ge 1,\:x+y \le n-2$ Of these the path count involves: single summation for cases 2-5 double summation for cases 6,7 recursion in cases 3,7 For the contribution from case 7, I get: $R_7(3,n) = \cases{\displaystyle\sum\limits_{x=1}^{n-3}\sum\limits_{y=1}^{n-x-2}{R(3,n-x-y-1)\cdot2^{x-1}}, n>3 \\ 0, \text{ otherwise}}$ This seems a bit too complicated; is there a neater way of classifying walks?","['contest-math', 'combinatorial-geometry', 'combinatorics']"
1386890,Least dimension of Lie group acting transitively of a manifold,I guess that the least possible dimension of a Lie group $G$ acting smoothly and transitively on a compact manifold $M$ is $\operatorname{dim}(M)$. Is this correct and is there a ref?,"['lie-groups', 'differential-geometry', 'group-actions']"
1386903,Let $M$ be an arbitrary point located inside the triangle $ABC$. Prove that $\cot\angle MAB + \cot\angle MBC + \cot\angle MCA \geq 3\sqrt{3}$,Let $M$ be an arbitrary point located inside the triangle $ABC$. Prove that $$\cot\measuredangle MAB + \cot\measuredangle MBC + \cot\measuredangle MCA \geq 3\sqrt{3}$$,"['geometry', 'triangles', 'trigonometry']"
1386915,An efficient way to find anagrams,"Consider a set of words where you want to divide the set into subsets of words, where all members of each subset are anagrams (same letters, different arrangement). You can do this computationally in several ways, the ""best"" way is sorting the letters from A through Z in each word. If two words are equal when their respective letters are sorted, then they must be anagrams. Otherwise, they are not. For the general case we cannot do any better. The sorting, which is $\mathcal{O}(M \log M)$ for each word of length $M$, is the computationally hardest part. But here is something I've been discussing. Suppose we know there is a longest word with $M$ characters. Then imagine we have a so-called ""hash function"" $f$ which maps a word to a positive integer. Let each letter A,B,C,...,Z have a specific weight $a_1,a_2,a_3,\dots,a_{26}$. For a word $x$, the value $f(x)$ is the sum of the corresponding weights without regard to their position within the word. $f(ZABC) = a_{26} + a_1 + a_2 + a_3$. This has linear complexity, $\mathcal{O}(M)$ for each word. If we can select these weights such that only words that are anagrams map to the same value, then a computer can easily calculate whether two words are anagrams by looking at their respective function value. The question is of course, how do we choose the weights ? I can formulate the problem as such: For fixed $M$ and $N$, find $\{a_1,a_2,\dots,a_{N}\}$, where $a_1 < a_2 < \dots < a_{N}$, such that $$c_ia_i \neq c_1a_1 + c_2a_2 + \dots + c_{i-1}a_{i-1} + c_{i+1}a_{i+1} + \dots + c_{N}a_{N}$$ for any combination of $c_j \in \{0,1,2,\dots,M\}$ except $c_1=c_2=\dots=c_{N}=0$. Basically, we want to find a linearly independent $N$-subset of $\Bbb Z_{>0}$ subject to a constraint on the coefficients. Is there a clever method for this? A very simple solution is to let $a_1 = 1$ and recursively define $a_{i+1} = Ma_i + 1$ but this grows very quickly and $a_{26}$ is too large for use in computers. For small $M$ and $N$ I have been able to find better solutions. Is there a better solution for $N=26$ and say $M \approx 15$? What is the smallest possible $a_{26}$?","['hash-function', 'computational-mathematics', 'linear-algebra', 'combinatorics']"
1386917,"Prove that continuity in x of the Gateaux derivative, $f'(x;y)$, implies Frechet differentiability","Prove that continuity in x of the Gateaux derivative implies Frechet differentiability Let $x$ be te point, $y$ the direction and $f'(x;y)=y·a(x)$. First, I considere the function $g(\varepsilon)=f(x+\varepsilon y)-f(x)-\varepsilon y·a(x)$. Then I have $g(0)=0$, $g(1)=f(x+ y)-f(x)-y·a(x)$, and $g(\varepsilon)=o(\varepsilon)$. Note: $o(h(x))$ is a function such that $$\lim\limits_{\|y\|\rightarrow 0}\dfrac{\|o(h(x))\|}{\|h(x)\|}=0.$$ And I want to show that $$f(x+y)-f(x)-y·a(x)=o(y)$$ So I think I have to show that $g(1)=o(y)$, but I don't know how can I do that.","['gateaux-derivative', 'real-analysis', 'derivatives']"
1386923,Divergent or convergent? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question If 
$$A_n = \frac{n(n+3)}{(n+1)^2}$$  then is the sum of the sequence $\sum(A_n)$ convergent or divergent?","['sequences-and-series', 'convergence-divergence', 'divergent-series']"
1386966,how to solve $3 - \cfrac{2}{3 - \cfrac {2}{3 - \cfrac {2}{3 - \cfrac {2}{...}}}}$,"$$A = 3 - \cfrac{2}{3 - \cfrac {2}{3 - \cfrac {2}{3 - \cfrac {2}{...}}}}$$ My answer is: $$\begin{align}
&A = 3 - \frac {2}{A}\\
\implies &\frac {A^2-3A+2}{A}=0\\
\implies &A^2-3A+2=0\\
\implies &(A-1)\cdot(A-2)=0\\
\implies &A=1\;\text{ or }\; A=2
\end{align}$$ I should note that I'm not sure if the above answer is true. Because I expected just one answer for A (A is a numeric expression), but I found two, $1$ and $2$. This seems to be a paradox.","['continued-fractions', 'algebra-precalculus']"
1386970,Correct way to write the set theoretic definition of a relation?,"I want to write the set theoretic definition of a relation $\preceq$ on a set $X$. So I thought I need to write that we have either $a \preceq b$ or $a \not\preceq b$. However writing $\forall a,b: (a \preceq b) \vee \neg(a \preceq b)$ won't do the job I believe since we have that $P \vee \neg P$ is always true even if we have not defined the value (True of False) of $P$. So what is the correct way to write it?","['elementary-set-theory', 'notation']"
1386976,Generating Functions and closed form [closed],Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question I read somewhere that we can use generating functions to find closed form of a sequence. So what is the difference between a generating function and closed form of a sqeunce?,"['closed-form', 'sequences-and-series', 'generating-functions']"
1387033,Formula for alternating sequences,"I am looking for a general formula for alternating sequences.
I know that the formula $f(x)=(-1)^x$ gives the sequence $1,-1,1,-1,...$ but I want more a general formula; for example the function $f(a,b,x)$ which returns the series $a,b,a,b,a,b,...$ as $x$ increases. So for example the function $f(3,5,x)$ returns the series $3,5,3,5,3,5,...$
What would such a function $f(a,b,x)$ be?","['sequences-and-series', 'number-theory', 'functions']"
1387040,Prove that $f$ is integrable if and only if $\sum^\infty_{n=1} \mu(\{x \in X : f(x) \ge n\}) < \infty$,"Problem statement: Suppose that $\mu$ is a finite measure. Prove that a measurable, non-negative function $f$ is integrable if and only if $\sum^\infty_{n=1} \mu(\{x \in X : f(x) \ge n\}) < \infty$. My attempt at a solution: Let $A_n = \{x \in X : f(x) \ge n\}$. To show that $\sum^\infty_{n=1} \mu (\{x \in X : f(x) \ge n\}) < \infty$ implies $f$ is integrable, the Borel Cantelli lemma tells us that almost all $x \in X$ belong to at most finitely many $A_n$. Thus, the set $\{x \in X : f(x) = \infty\}$ has measure $0$. Now, this, together with the fact that $\mu(X) < \infty$, should give us that $f$ is integrable, but I can't figure out how to prove that! It seems fairly obvious, but I can't figure out if it is then ok to say that $f$ is bounded almost everywhere? It seems like $f$ is then pointwise bounded, but I'm not sure if that means I can find some $M$ such that $f(x) \le M$ for all $x$. For the reverse implication, I haven't come up with anything useful - I have been trying to show that the sequence of partial sums, $\sum^m_{n=1}\mu(A_n)$, is bounded, but I'm not sure how to do so.","['analysis', 'real-analysis', 'lebesgue-integral', 'lebesgue-measure']"
1387071,Let $\lambda$ be an eigenvalue of $A$. Prove that $\lambda^{-1}$ is an eigenvalue of $A^{-1}$.,"Let $\lambda$ be an eigenvalue of $A$. Prove that $\lambda^{-1}$ is an eigenvalue of $A^{-1}$. My approach: Suppose $\lambda$ is an eigenvalue of $A$. Then $Ax=\lambda x$ for some $x\neq 0$. Since $A$ is invertible, $Ax=\lambda x \implies A^{-1}Ax=A^{-1}\lambda x$. So: $$Ix=A^{-1}\lambda x \iff Ix-A^{-1}\lambda x=0\iff x(I-A^{-1}\lambda)=0 $$ Since $x \neq 0$ and $A$ is invertible, $\lambda^{-1}=A^{-1}$ (I'm unsure of this equality). I think this somewhat shows it, this might seem elementary but aren't $\lambda, \lambda^{-1}$ scalars? So basically, if this proof is true $\lambda\cdot\lambda^{-1}=I$ also? Any other way to show this? Thanks",['linear-algebra']
1387075,Proving that $\det(A) = 0$ when the columns are linearly dependent,"Proposition: Let $A$ be a $(n \times n)$-matrix. If the columns of $A$ are linearly dependent, then $\det(A) = 0$. Attempt at proof: Let $A = (A_1, A_2, \ldots, A_n)$, where each $A_i$ is a column vector. Since the columns are linearly dependent, we have \begin{align*} \lambda_1 A_1 + \ldots + \lambda_n A_n = 0 \end{align*} where not all $\lambda_i$ are zero. Suppose, without loss of generality, that $\lambda_1 \neq 0$. Then we get \begin{align*} A_1 = - \frac{\lambda_2}{\lambda_1} A_2 - \ldots - \frac{\lambda_n}{\lambda_1} A_n. \end{align*} It follows that \begin{align*} \det(A_1, \ldots, A_n) &= \det (- \frac{\lambda_2}{\lambda_1} A_2 - \ldots - \frac{\lambda_n}{\lambda_1} A_n; A_2, A_3, \ldots, A_n) \\ &= \sum_{i=2}^n - \frac{\lambda_i}{\lambda_1} \det(A_2 + \ldots + A_k + \ldots + A_n; A_2, A_3, \ldots, A_n) \end{align*} This follows since the determinant is linear in each column. Now, I want to write \begin{align*} = \sum_{i=2}^n - \frac{\lambda_i}{\lambda_1} \det(A_k, \ldots, A_k, \ldots) = 0 \end{align*} since two columns are equal. But I'm not sure if my notation is correct in the last step? How do I break down the $A_2 + \ldots + A_k + \ldots$ etc.? Suggestions are appreciated!","['determinant', 'linear-algebra', 'proof-verification', 'matrices']"
1387109,Help with Spivak's Calculus: Chapter 1 problem 21,"I've been stuck on this problem for over a day, and the answerbook simply says ""see chapter 5"" for problems 20,21, and 22. But I want to complete the problem without using knowledge given later in the book, so I've been banging my head against the wall trying all sorts of things, but nothing I do seems to lead me anywhere. The problem is as follows: Prove that if $|x - x_0| < min (\frac{\varepsilon}{2(|y_0| + 1)}, 1)$ and $|y - y_0| < min (\frac{\varepsilon}{2(|x_0| + 1)}, 1)$ then $|xy - x_0 y_0| < \varepsilon$. Here are some of the things I've been thinking about, I don't know which of these are useful (if any), but they somewhat outline the logic behind my various attempts. Since at most $|x - x_0| < 1$ and $|y - y_0| < 1$ then it follows that $(|x-x_0|)(y-y_0|) < |x-x_0|$ and $(|x-x_0|)(y-y_0|) < |y-y_0|$ Also, $(|x - x_0|)(|y_0| + 1) < \frac{\varepsilon}{2}$ and $(|y - y_0|)(|x_0| + 1) < \frac{\varepsilon}{2}$ so $(|x - x_0|)(|y_0| + 1) + (|y - y_0|)(|x_0| + 1) < \varepsilon$. and since $|a + b| \leq |a| + |b| < \varepsilon$ I've tried multiplying things out, and then adding them together to see if anything cancels, but I can't make anything meaningful come out of it. Also since $|a - b| \leq |a| + |b|$ I've also tried subtracting one side from the other, but to no avail. I was also thinking that since $(|x-x_0|)(y-y_0|) < |x-x_0|$, then I could try something along the lines of $(|x-x_0|)(y-y_0|)(|x_0| + 1) + (|x-x_0|)(y-y_0|)(|y_0| + 1)< \varepsilon$ and various combinations as such, but I just can't seem to get anything meaningful to come out of any of these attempts. I have a sneaking suspicion that the road to the solution is simpler than I'm making it out to be, but I just can't see it.","['analysis', 'calculus']"
1387135,Exercise of measure theory,"I need help with this exercise. Let $(\Omega,\mathcal{F},\mu)$ be a measure space and $(A_n ;n\geq 1)\subseteq \mathcal{F}$, such that $\mu (\bigcup_{n=1}^{\infty}A_n)<\infty$  and $\inf\{\mu(A_n)\mid n \in \mathbb{N}\}=\alpha >0$. Show that the set $A$, formed by all elements of $\Omega$ belonging to an infinite number of sets $A_n$ is measurable and also $\mu(A) \geq \alpha$.",['measure-theory']
1387167,Prove that $\varphi^{-1}$ is the inverse of $\varphi$,"This is exercise 2.27 of Lee's introduction to topological manifolds. I proved (geometrically) that $$\varphi(x,y,z)=\frac{(x,y,z)}{\sqrt{x^2+y^2+z^2}}$$
and that $$\varphi^{-1}(x,y,z)=\frac{(x,y,z)}{\max\{|x|,|y|,|z|\}}$$
How can I prove directly that $\varphi\circ\varphi^{-1}=id$ and $\varphi^{-1}\circ\varphi=id$ ? @AndrewD.Hwang $$\varphi^{-1}(\varphi(x,y,z))=\frac{\frac{(x,y,z)}{\sqrt{x^2+y^2+z^2}}}{\max{\left(\frac{|x|}{\sqrt{x^2+y^2+z^2}},\frac{|y|}{\sqrt{x^2+y^2+z^2}},,\frac{|z|}{\sqrt{x^2+y^2+z^2}}\right)}}=\frac{\frac{(x,y,z)}{\sqrt{x^2+y^2+z^2}}}{\frac{\max{(|x,|y|,|z|})}{\sqrt{x^2+y^2+z^2}}}=\varphi^{-1}(x,y,z)$$ And the same problem for $\varphi\circ\varphi^{-1}$.",['functions']
1387170,Any set with more elements than the dimension of vector space is linearly dependent,"Let $V$ be a vector space with dimension $n$ such that $\{v_1,\cdots,v_n\}$ is its basis. Take $A\equiv\{a_1,\cdots,a_p\}\subset V$ with $p>n$. How do can I show that $A$ is linearly dependent without having to do all those summations . Is there an easier more direct proof than that? Thanks for helping!!!",['linear-algebra']
1387198,Find the value below,"Let $a,b,c$ be the roots of the equation $$8x^{3}-4x^{2}-4x+1=0$$ Find $$\frac{1}{a^{3}}+\frac{1}{b^{3}}+\frac{1}{c^{3}}$$ It's just for sharing a new ideas, thanks:)",['algebra-precalculus']
1387207,"Find $f(2015)$ given the values it attains at $k=0,1,2,\cdots,1007$","Let $f$ be a polynomial of degree $1007$ such that $f(k)=2^k$ for $k=0,1,2,\cdots, 1007$. Determine $f(2015)$. Taking $f(x)=\sum_{n=0}^{1007} a_n x^n $, (whence $a_0=1$), I tried to combine $$a_1=1-\sum_{n=2}^{1007} a_n$$ and the easily derivable $$ a_n = \frac{2^n-1}{n^n} - \sum_{k=1}^{n-1} a_k n^{k-n} - \sum_{k=n+1}^{1007} a_k n^{k-n},$$ hoping to find several null coefficients, but that didn't work. I looked for the solution on Yahoo Answers, and I came to know it's about binomial coefficients, Tartaglia in particular; however the explanation is confusing to me, I'd like a better one, or even a different approach.","['contest-math', 'polynomials', 'algebra-precalculus']"
1387210,Digit Code Combination Problem,"Based on real life experience, I just considered the following combinatorial challenge: In a workplace with currently $n$ employees each employee has its own unique 4-digit code used to pass through certain doors at certain times of day. Tomorrow $r$ new employees will start working there and each must fill in a form suggesting two different 4-digit codes. They have no knowledge of each others codes or the $n$ current employee's codes. When everyone has filed their forms, it is attempted to assign each new employee one of their suggested codes so that all $n+r$ persons working there have unique codes. For simplicity, let us assume that the new employees choose their two different 4-digit suggestions uniformly at random. Then what is the probability $P(n,r)$ that the workplace is unable to assign unique codes to all the new employees?","['probability', 'recreational-mathematics', 'algebra-precalculus', 'combinatorics']"
1387214,The induced map on stalks is well-defined,"$\require{AMScd}$ Let $\phi:\mathscr{F\to G}$ be a morphism of sheaves on $X$, let $\mathscr F_P$ be a stalk of $\mathscr F$ at $P\in X$, and let the stalk map $\phi_P:\mathscr F_P\to\mathscr G_P$ take the class containing $(U,f)$ to the class containing $(U,[\phi(U)(f)])$. We aim to show that this defines a function, by showing that it is well-defined. The requirement is to show that if $(U,f)\sim(V,g)$ then $(U,\phi(U)(f))\sim(V,\phi(V)(g))$. The assumption implies that there is an open set $W\subseteq U\cap V$ such that $f|_W=g|_W$. From the definition of morphism, we have commutative diagrams $$\begin{CD}
\mathscr F(U) @>\phi(U)>> \mathscr G(U)\\
@VV\rho_{UW}V @VV\rho'_{UW}V\\
\mathscr F(W) @>\phi(W)>> \mathscr G(W)
\end{CD}\qquad\qquad
\begin{CD}
\mathscr F(V) @>\phi(V)>> \mathscr G(V)\\
@VV\rho_{VW}V @VV\rho'_{VW}V\\
\mathscr F(W) @>\phi(W)>> \mathscr G(W)
\end{CD}$$
which imply $$\rho'_{UW}\circ\phi(U)=\phi(W)\circ\rho_{UW}\qquad\qquad\rho'_{VW}\circ\phi(V)=\phi(W)\circ\rho_{VW}.$$ This gives $$\phi(U)(f)|'_W=\rho'_{UW}(\phi(U)(f))=\phi(W)(\rho_{UW}(f))=\phi(W)(g|_W)$$ and $$\phi(V)(g)|'_W=\rho'_{VW}(\phi(V)(g))=\phi(W)(\rho_{VW}(g))=\phi(W)(g|_W)$$ so that $\phi(U)(f)$ agrees with $\phi(V)(g)$ on $W$ in the sheaf $\mathscr G$, and hence the well-definedness is verified. Is this rather ugly calculation the best way of showing this simple result? More generally, do most sheaf-related problems ultimately reduce to such computations without sufficient abstraction?","['algebraic-geometry', 'sheaf-theory']"
1387221,How to find sum of infinite series of non-geometric series?,"I am familiar with radius of convergence, power series, Taylor series/Maclaurin series, and fundamental infinite series convergence tests (ratio, root, integral, comparison, etc.) introduced in a first semester college Calculus class. I have always had to determine convergence or divergence for most non-geometric or arithmetic series but I do not know a process to find $$\sum\limits_{k=1}^\infty \frac{(-1)^{k+1}k^2}{k^3+1}. $$ $\DeclareMathOperator{\sech}{sech}$
I have put the answer through Wolfram|Alpha and got $$\frac{1}{3}\left ( 1-\ln(2)+\pi \sech\left ( \frac{\sqrt3}{2}\pi \right ) \right ).$$","['sequences-and-series', 'calculus', 'real-analysis', 'discrete-mathematics']"
1387331,An entire function which is real on the real axis and map upper half plane to upper half plane,"Suppose that $f$ is an entire function that satisfies $f(z)$ is real when $z$ is real and if $Imz>0$ then $Imf(z)>0$. Prove that $f$ can have at most one zero and that the zero, if it occurs, is real. Show also that if $f$ has no zero then $f$ is constant. By the open mapping theorem the zeros can not be in the upper half plane and I think that I will also need to use Lioiville's theorem at some point but I couldn't achieve. Thanks for any help.",['complex-analysis']
1387336,Finding the net outward flux of a sphere,"Use the Divergence Theorem to compute the net outward flux of: $$ F = \langle x^2, y^2, z^2 \rangle $$ $S$ is the sphere: $$ \{(x,y,z): x^2 + y^2 + z^2 = 25\} $$ First, I took: $$ \nabla \cdot F = 2x + 2y + 2z $$ Then, I tried setting up the triple integral with spherical coordinates, but it is just not working out for me.",['multivariable-calculus']
1387365,"If $\ne: X \times X \to S$ is continuous, is X hausdorff?","The Sierpiński space is defined like so:
$$S = (\{\top, \bot\}, \{\emptyset, \{\top\}, \{\top, \bot\}\})$$
(A nice way to visualize is to take [0, 1], and glue 0 on $\bot$ and (0,1] onto $\top$.) Now, given a topological space $X$, we can define a function $\ne:X \times X \to S$, where $X \times X$ has the product topology , like so.
$$a \ne b \begin{cases} \bot & \text{if } a = b \\ \top & \text{otherwise} \end{cases}$$
I saw it claimed somewhere on mathoverflow that $\ne$ is continuous iff $X$ is hausdorff (for every two points $a$ and $b$ in $X$, there are open sets $A \ni a$ and $B \ni b$, such that $A \cap B = \emptyset$). Is this true? What is the proof?","['products', 'continuity', 'functions', 'general-topology', 'inequality']"
1387367,Is there a continuous function $f(x)$ such that the inverse function is $1/f(x)$?,A student came to me with this question and I cracked my head for one hour but I couldn't unambiguously prove that it exists or it doesn't exist. Continuous and invertible function such that $f^{-1}(x) =1/f(x)$ on its domain of definition.,['functions']
1387378,Show that $U(\mathbb Z_{p^n})$ is cyclic by considering the order of $1+p$,"Show that $U(\mathbb Z_{p^n})$ (the group of units) is cyclic for $p$ an odd prime, $n \in \mathbb N$. We are given a hint to consider the order of $1+p$ in this group. I have no idea how this leads to a proof. For example, the order of $4$ in $\mathbb Z_9$ is $3$, so it's clearly not a generator. I'm familiar with the proof that uses induction and the lifting lemma, but I don't think that's what we are supposed to use here.","['group-theory', 'cyclic-groups']"
1387391,Alternate Axiom of Infinity,"The Axiom of Infinity states that there is a set $S$ containing $\varnothing$ such that if $x$ is an element of $S$ then so is $x\cup\{x\}$. Is the following variant equivalent? There exists a nonempty set $S$ such that if $x$ is an element of $S$ then so is $\{x\}$. This one also guarantees an infinite set, and it's shorter. The fact that we use the longer, more complicated one makes me think that the shorter version isn't as strong. Is this correct? EDIT: To be clear, I'm wondering if they're equivalent if I leave the other axioms of ZFC unchanged.","['infinity', 'elementary-set-theory', 'axioms']"
1387396,PMF estimation: concentration inequalities for the $l_1$ and $l_\infty$ errors,"Assume that you are given $n$ i.i.d samples $X_1, ..., X_n$ drawn from a discrete distribution $p = (p_1, ..., p_k)$. We would like to estimate $p$ using the empirical estimator
\begin{equation}
\hat{p}_i = \frac{1}{n} \sum_{j=1}^{n}\mathbb{1}_{\{X_j =i\}}.
\end{equation}
Using classical Chernoff type concentration inequalities, we can easily derive the following tight bound
\begin{equation}
\mathbb{P}\left(|\hat{p}_i - p_i| \geq \alpha \right) \leq 2 e^{-n\alpha^2/4},
\end{equation}
for any $i \in \{1, ..., k\}$. Can we make use of the above inequality to prove tight upper bounds on the $l_1$ and $l_\infty$ errors? Precisely, we would like to find tight upper bounds on
\begin{equation}
\mathbb{P}\left(\sum_{i=1}^{k} |\hat{p}_i - p_i| \geq \alpha \right),
\end{equation}
and
\begin{equation}
\mathbb{P}\left(\max_{i \in \{1, ..., k\}}|\hat{p}_i - p_i| \geq \alpha \right).
\end{equation}
Notice that the $|\hat{p}_i - p_i|$'s are correlated. Also, notice that the vector of counts $(Y_1, ..., Y_k)$, where 
\begin{equation}
Y_i = \sum_{j=1}^{n}\mathbb{1}_{\{X_j =i\}},
\end{equation}
is distributed according to a Multinomial$(n, p_1, ..., p_k)$ distribution. Therefore, the above problem is equivalent to finding tight upper bounds on 
\begin{equation}
\mathbb{P}\left(\sum_{i =1}^{k} |Y_i - \mathbb{E}[Y_i]| \geq n\alpha \right),
\end{equation}
and
\begin{equation}
\mathbb{P}\left( \max_{i \in \{1, ..., k\}}|Y_i - \mathbb{E}[Y_i]| \geq n\alpha \right),
\end{equation}
where $(Y_1, ..., Y_k) \sim$ Multinomial$(n, p_1, ..., p_k)$.
We can possibly use the bound provided in Lemma 3 of this paper , but it only holds for large $n$. Further, it is unclear whether or not that bound is tight.","['probability-theory', 'concentration-of-measure', 'probability-limit-theorems', 'statistics', 'machine-learning']"
1387419,"Is the minimum number of relations in a free product, the sum of the minimum number of relations in the free factors?","Say $\rho(G)$ is the minimum number of relations required to present the group $G$. Is $\rho(A*B)= \rho(A)+\rho(B)$? What can be said about $\rho(A*B)$? A while ago I was thinking about $C_3*C_4$, thinking it was obvious that this group should not be a one-relator group. I am pretty sure at one point I came up with an argument but it seemed awfully complicated, and using specific information about one-relator groups for something that ""feels"" obvious. Using the comments below as a guide, one can prove that if the abelianization of $A,B$ is finite and $\rho(A)=r(A),\rho(B)=r(B)$ where $r(G)$ is the rank of the group (minimum number of generators), then we can get that $\rho(A*B)=\rho(A)+\rho(B)$. It is known that $r(A*B) =r(A)+r(B)$, this can be found in Ch IV Cor 1.9 in Lyndon and Schupp. We also have that $\rho(A*B) \leq \rho(A)+\rho(B)=r(A)+r(B)$. If $\rho(A*B) < r(A*B)$, then a presentation witnessing that inequality can be shown to have an infinite abelianization, since the number of generators would be strictly greater than the number of relations ( see this answer ). This contradicts that the abelianization of $A*B$ is finite. So finite free products of finite cyclic groups works and other groups too. I could see a careful analysis the abelianization of f.g. groups giving more more information. And it does seem that a somewhat related concept, group deficiency , is related to some cohomology groups (I don't know any cohomology, but I would welcome an answer, even if it used cohomology).","['group-presentation', 'group-theory']"
1387435,There is a no set which every line meets the ball,"Does there exist the set of balls $X=\{B_i\subset\mathbb{R^2};i\in I\}$, satisfing following properties?(Note that the ball has a positive real radius) Let the set of all lines in plane to be $L$.  For each  $l \in L,\ l\cap B_i \ne \varnothing $ for some $i\in I$. $\{N_l=\text{the number of balls which the line} \ l \ \ \text{transversals}\mid l\in L\}$  is bounded. (This does not mean the number of ball which line transversals is finite. This statement is stronger.) I think there is no such set but I can not prove. My try is think the line in the plane as a point on the sphere but I can not go further. Does anyone has any idea to prove or disprove?","['euclidean-geometry', 'combinatorics']"
1387456,Why do we distinguish between infinite cardinalities but not between infinite values?,"More specifically, why are we ""allowed"" to denote $|\mathbb{N}|<|\mathbb{R}|$ but not $\sum\limits_{n\in\mathbb{N}}1<\sum\limits_{r\in\mathbb{R}}1$? Can we distinguish between ""countable divergence"" and ""uncountable divergence""? I apologize in advance for the ""rather naive"" question.","['infinity', 'elementary-set-theory', 'divergent-series', 'cardinals']"
1387477,On a problem of weak convergence for a particular convolution of pr. measures,"Assume that $\{P_n: n \in N\} $ and $\{Q_n: n \in N\} $ are sequences of probability measures. 
Assume that $P_n \stackrel{w}{\to} P. $
Also, assume that $Q_n = \delta_{b_n}, $ the Dirac measure and $b_n \in R. $ If we knew that $P_n\star Q_n $ (the convolution) converges weakly to some probability measure, does it follow that there must exist a $b $ such that $b_n \to b $ as $n \to \infty $ ? I think that the answer is positive, but I do not seem to be able to make the argument rigorous. Can anyone help? Thank you Maurice","['probability-theory', 'probability', 'weak-convergence', 'convolution']"
1387479,Is there such a number N such that any group of order N is simple?,"Is there such a number N, such that any group of order N is simple?","['group-theory', 'finite-groups']"
1387492,Non-CM totally imaginary number fields,"Is there a name for the totally imaginary number fields that are not CM-fields? Any important subclass of number fields with that property, or perhaps a reference where those field are studied in some detail? Thanks in advance for any relevant information!","['number-theory', 'algebraic-number-theory']"
1387498,Branch points and branching number on Riemann surface,"I am attempting to analyze the Riemann surface of the algebraic function $w=(\sqrt{z} - 1)^{1/4}$. To do this, I started out by writing as a polynomial, $P(w,z) = w^8 +2w^4 + 1 -z = 0$. Next, I want to identify the branch points. I understand these points to be the points where the inverse function theorem fails, so I $\dfrac{\partial P}{\partial w}=0,$ identify the values of $w$ satisfying this, and use them to determine some corresponding $z$'s. This gives the ordered pairs $(w,z) = (0,0), (1,3), (i,3), (-1,3), (-i,3)$. (Here I am just listing the pairs). Next I would like to compute a branching number for each of these points. I understand that this amounts to determining the minimal number $l \in \mathbb N$ so that $\dfrac{\partial^l P}{\partial w^l}(w,z) \neq 0.$ The branching number is defined as $l-1$. Using this, it is easy to compute the branching numbers. The branching number for the first is $3$, and all the others have branching number $1$. From here I want to compute the genus of the Riemann surface of this polynomial. To do this, I should expect the sum over all the branching numbers to be an even number, by considering Riemann-Hurwitz. However, their sum is currently $7$. This made me think I had missed a point, and was thinking about the point at infinity. However, I do not understand (or maybe it's obvious and I've just forgotten) how to determine the branching number of the point at infinity. I am a bit unsure about even my work so far though, because it is a result in Schlag that the degree of a polynomial is equal to the number of sheets of the Riemann surface. This means that this surface has $8$ sheets. By Riemann-Hurwitz, we would expect something of the form $g_{RS} = 1+8(g-1)+\dfrac{\sum B(p)}{2}$, where $g$ is the genus of the Riemann sphere, which is $0$. In other words, $g_{RS} = -7 + \dfrac{\sum B(p)}{2}$, implying that the branching number of infinity must be very large, since genus must be at least $0$. So my question is, did anything go wrong in this analysis? If so, where? Furthermore, how do I compute this branching number at infinity?","['complex-analysis', 'riemann-surfaces']"
1387505,An abelian number field is either totally real or CM-field,"The wikipedia article of totally real number fields says: The totally real number fields play a significant special role in
  algebraic number theory. An abelian extension of Q is either totally
  real, or contains a totally real subfield over which it has degree
  two. Can someone give me a reference for this fact, or a sketch of a proof? Thanks in advance!","['number-theory', 'galois-theory', 'algebraic-number-theory']"
1387519,When has one sufficiently mastered an area of mathematics?,"This is a rather soft question regarding the mastery of various mathematical subjects, such as undergraduate subjects. In particular, say, when has one mastered undergraduate analysis? Is it realistic to expect some individual to be able to prove every theorem and do every exercise in Baby Rudin? What about analysis not covered in Baby Rudin? Say, should one also be able to prove compactness is equivalent to sequential compactness, and sequential compactness is equivalent to closed and boundedness in $\mathbb{R}^n$? Should one be able to prove every major statement about completeness? Continuity? Integration? Or is it enough to be able to read and understand any proof in a particular area? When can one feel satisfied with what they know about a particular area of basic mathematics? Can the seasoned mathematician prove these theorems by heart without much thought? If so, is this due to brute memorization, or simply mathematical maturity? In short, what should one reasonably expect an undergraduate to know about any particular subject after completing a course in said subject? I ask specifically about undergraduate subjects, since these are often the areas expected to be mastered by the time one begins graduate school and beyond. This question applies to all areas of mathematics, graduate and undergraduate. Interested to hear different interpretations of the words sufficiently and mastered .","['analysis', 'real-analysis', 'soft-question', 'proof-writing']"
1387524,Example of a nowhere dense subset of a metric space.,"We recall the definition of a nowhere dense subset of a metric space: ""A subset $A$ of a metric space $(X,d)$ is nowhere dense if $Int(\bar A)=\emptyset$"" I don't understand how it is that $\mathbb Z\subset \mathbb R$ is nowhere dense; how can the interior of its closure be the empty set?","['analysis', 'functional-analysis', 'general-topology', 'metric-spaces']"
1387569,Minimum of an apparently harmless function of two variables,"I would like to prove that the minimum of the function $$
f(x,y):=\frac{(1-\cos(\pi x))(1-\cos (\pi y))\sqrt{x^2+y^2}}{x^2 y^2 \sqrt{(1-\cos(\pi x))(2+\cos(\pi y))+(2+\cos(\pi x))(1-\cos(\pi y))}}
$$ over the domain $[0,1]^2$ is $2\sqrt{2}$. Looking at the 2D plot of the function one immediately notices that the minimum is $f(1,1) = 2\sqrt{2}$. However, I can't figure out how to prove this in a rigorous way, even if the expression of $f$ seems to have a nice, ""quasi-separable"" structure... Any suggestion is welcomed!","['optimization', 'multivariable-calculus', 'real-analysis']"
1387583,Non linear system of differential equations,"Is there a specific name to the following type of non linear ODEs $\begin{array}{c} \dot{x}_1 &=   c_1 \, x_2\, x_3 \\  \dot{x}_2 &=   \,  c_2 x_1 x_3 \\ \dot{x}_3  &= c_3 \, x_2 x_1   \end{array} $ where $c_i$ are real constants. More specifically, for a higher dimensional version, $\dot{x}_i = \sum_{\begin{array}{c}r,s \\ r \neq s \neq i  \end{array}} 
c_{ir} x_r x_s$ Are there any known transformations to simplify things? I conjecture the solutions will be elliptic functions of some sort, but you might not want to write them down. For the three dimensional case one can obtain $ \frac{d}{dt}(\frac{1}{2} ( x_2^2 + x_3^3) ) = 0$ $ \implies x_2^2 + x_3^3 = 2 C$ And use this to transform some of the equations, eventually finding  elliptic solutions for all the $x_i$. In higher dimensional case I would expect a similar situation.","['systems-of-equations', 'elliptic-functions', 'multivariable-calculus', 'ordinary-differential-equations']"
1387589,Proving that cows have same weight without weighing it!,"My friend gave me this problem and I have no clue how to go about it: A peasant  has $2n + 1$ cows. When he puts aside any of his cows,
the remaining $2n$, can be divided into two sub-flocks of $n$ cows and each having
the same total weight. How can we prove that the cows all have the same weight? How can I approach this?","['contest-math', 'linear-algebra', 'combinatorics']"
1387604,Prove that jump functions are measurable,"This question comes from the exercises of Stein and Shakarchi's Real analysis Ex. 5.14. Define $$
    j_n(x)= 
\begin{cases}
    0& \text{if } x< x_n\\
    \theta_n              & \text{if } x=x_n\\
    1  &\text{if } x>x_n
\end{cases}
$$ where $\{x_n\}$ is a sequence of real number and $0\le\theta_n\le 1$. Further define $$J(x)=\sum^{\infty}_{n=1}\alpha_nj_n(x)$$ where $\alpha_n>0$ and $\sum\alpha_n$ converges. Prove that for any $\epsilon$, the set of $x$ satisfying 
$$\limsup_{h\to0}\frac{J(x+h)-J(x)}{h}>\epsilon$$ is measurable. So far what I can prove is that the sum in $J(x)$ can converge absolutely. And I know the limit of measurable functions is measurable. However, Since $J$ is not continuous, I cannot assume $h\to0$ as $\frac1n;n\to\infty$, then I cannot figure out whether the above set is measurable. Edit: There is a hint in the book: consider $$F^N_{k,m}(x)=\sup_{1/k\le|h|\le1/m}\left|\frac{J_N(x+h)-J_N(x)}{h}\right|$$
where $J_N$ is the $N^{\text{th}}$ partial sum of $J$ Show that $F^N_{k,m}$ is measurable (which I couldn't tell why) and so does
 $$\lim_{m\to\infty}\lim_{k\to\infty}\lim_{N\to\infty}F^N_{k,m}(x)$$ (Which I also cannot relate it to the original set because I am confused of the multiple limit and supremum)","['lebesgue-measure', 'real-analysis', 'measure-theory']"
1387636,Ramanujan theta function and its continued fraction,"I believe Ramanujan would have loved this kind of identity.
After deriving the identity, I wanted to share it with the mathematical community. If it's well known, please inform me and give me some links to it.
Let $q=e^{2\pi\mathrm{i}\tau}$, then
$$(1+q^{2}+q^{6}+q^{12}+q^{20}+q^{30}+\cdots)^{2}=\cfrac{1}{1+q-\cfrac{q(1+q)^2}{1+q^3+\cfrac{q^2(1-q^2)^2}{1+q^5-\cfrac{q^3(1+q^3)^2}{1+q^7+\ddots}}}}$$
for $|q|\lt1$.
If possible, please provide more examples of this nature, available in the literature.","['modular-forms', 'q-series', 'number-theory', 'theta-functions', 'continued-fractions']"
1387651,Alternative proof that base angles of an isosceles triangle are equal,"The ""classic textbook proof"" of equality of base angles of an isosceles triangles which I studied in my school days is as follows: Let $\Delta ABC$ be a triangle with $AB = AC$ and let $D$ be the mid point of $BC$. Now the triangles $ADB$ and $ADC$ are congruent via $SSS$ criterion. Hence $\angle ABD = \angle ACD$ and this is the same as $\angle ABC = \angle ACB$. I studied another proof some years ago in the Douglas R. Hofstadter's famous masterpiece Godel, Escher, Bach which was discovered by a computer program: Consider the triangles $\Delta ABC, \Delta ACB$. These are congruent via $SSS$ criterion and hence $\angle ABC = \angle ACB$. This one line proof is amazing, but at the same time looks confusing (some might think it is fishy but it is not) because the original triangle has been mirrored to exchange vertices $B, C$ and then considered congruent to original one. My question is a pedagogic one: Is this proof simpler to understand (compared to the standard version) for students of age 12-13 years? Should textbooks at least include this proof as well in some supplement or appendix?","['education', 'geometry', 'alternative-proof', 'soft-question']"
1387703,How to solve this seemingly simple initial value problem? $xy' - y = x^2$,"$$xy' - y = x^2,\quad y(-1) = 0$$ I found this question in the old exams paper of a Calculus II course. I tried finding $y'(-1) = 1$ but it doesn't seem to be helpful at all. Can anyone help? Thanks a lot in advance!","['calculus', 'ordinary-differential-equations']"
1387708,$\sum_{n=1}^\infty a_n = \sum_{n=1}^\infty n a_n$ possible?,"Recently I found (somewhere on math.se) a nice proof for $\sum_{n=0}^\infty \frac{n}{2^n} = 2$ and thought “oh, that‘s surprising, as also $\sum_{n=0}^\infty \frac{1}{2^n} = 2$ and it ‘feels like’ the first series should be greater than the later one.” However the surprise did not last very long, as I noticed the first summand of the first series is zero. So basically one has $\sum_{n=1}^\infty \frac{n}{2^n} = 2 > 1 = \sum_{n=1}^\infty \frac{1}{2^n}$, which is less surprising. So I asked myself whether there exists a series $\sum_{n=1}^\infty a_n$ with $\sum_{n=1}^\infty a_n = \sum_{n=1}^\infty n a_n$ $a_n \gt 0$ for $n \in \mathbb{N}$ (ie. no $0 = 0$ tricks) both series converge (ie. no $\infty = \infty$ tricks) I tried $a_n = \frac{1}{n!}$, but then $\sum_{n=1}^\infty a_n = e-1 \lt e = \sum_{n=1}^\infty n a_n$ (interestingly, the difference is $1$ again!) and then I failed to find an example (or a proof that this is impossible). Does such a series exist?",['sequences-and-series']
1387728,Am I right in calculating $\sin(2\arcsin(\frac{1}{3}))$ as $\frac{4\sqrt{2}}{9}$?,"I've been solving a problem in my textbook, and my result is at odds with the textbook's: $$\sin\left(2\arcsin\left(\frac{1}{3}\right)\right)$$ My answer is $$\frac{4\sqrt{2}}{9}$$ I've used the double-angle identity for sine. $$\sin\left(2\arcsin\left(\frac{1}{3}\right)\right)=2\sin\left(\arcsin\left(\frac{1}{3}\right)\right)\cos\left(\arcsin\left(\frac{1}{3}\right)\right)$$ and this identity: $$\cos(\arcsin(x))=\sqrt{1-x^2},$$ yielding $$2*\frac{1}{3}*\sqrt{1-\frac{1}{9}}=\frac{2\sqrt{8}}{3*3}=\frac{4\sqrt{2}}{9}$$ But the textbook's answer is $$\frac{2\sqrt{2}}{3}$$","['algebra-precalculus', 'trigonometry']"
1387744,Proving that $1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots+\frac{1}{2n-1}-\frac{1}{2n}=\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{2n}$,"I have written the left side of the equation as $$\left(1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{2n-1}\right)-\left(\frac{1}{2}+\frac{1}{4}+\frac{1}{6}+\cdots+\frac{1}{2n}\right).$$ I don't know how to find the sums of these sequences. I know the sums for odd and even integers, but I can't figure this out.","['summation', 'sequences-and-series', 'induction']"
1387753,Prove identity: $\frac{1+\sin\alpha-\cos\alpha}{1+\sin\alpha+\cos\alpha}=\tan\frac{\alpha}{2}$,"Prove identity : $$\frac{1+\sin\alpha-\cos\alpha}{1+\sin\alpha+\cos\alpha}=\tan\frac{\alpha}{2}.$$
My work this far: we take the left side $$\dfrac{1+\sqrt{\frac{1-\cos2\alpha}{2}}-\sqrt{\frac{1+\cos2\alpha}{2}}}{1+\sqrt{\frac{1-\cos2\alpha}{2}}+\sqrt{\frac{1+\cos2\alpha}{2}}}=$$
then after some calulations I come to this
$$=\frac{\sqrt{2} +\sqrt{1-\cos2\alpha}-\sqrt{1+\cos2\alpha}}{\sqrt{2} +\sqrt{1-\cos2\alpha}+\sqrt{1+\cos2\alpha}}$$ but now I'm stuck...",['trigonometry']
1387775,How to show that the Dini Derivatives of a measurable function is measurable?,"Let $f:(0,1)\to\mathbb R$ be measurable. Then, the (right upper) Dini derivative
$$ D^+ f(x) = \limsup_{h\to 0^+} \frac{f(x+h) - f(x)}{h} $$
is also measurable (a well known result of Banach). Can someone give me a source in English (or German) or a proof sketch? If it makes thing much easier, we can assume $f$ is monotone (in that case please don't argue that $f$ is differentiable a.e. That feels like cheating :)","['derivatives', 'real-analysis', 'measure-theory']"
1387792,What is the interpretation of $\mu(dx)$ in Lebesgue integral?,"For estimating the integral $\int_a^b f(x)dx$ we calculate the area of rectangles of height $f(x)$ and base $dx$ (Riemann sums). Therefore, we go from $a$ to $b$ with steps of $dx$. what 's the interpretation of $\mu(dx)$ in $\int f(x)\mu(dx)$ and how can we relate it to Lebesgue integral interpretation ? I already Know Lebesgue integral basics.","['measure-theory', 'integration']"
1387820,"A proper definition of $i$, the imaginary unit [duplicate]","This question already has answers here : Refining my knowledge of the imaginary number (8 answers) Closed 8 years ago . Back when I was in high school, which was a long time ago, I recall my math teacher telling me that the definition of $i$, the imaginary unit, is $\sqrt{-1}$. Knowing little, at the time, I accepted it without thinking twice. Several years later when I was in my Complex Analysis class, one of my classmates asked for the distinction between defining $i$ in the normal way, i.e., $i=\sqrt{-1}$, and more ambiguous way, $i^2=-1$. Now that I thought deeply about it, I have some suspicions about what my high school teacher taught me at the time. Firstly, at least at the level of high school, one normally defines a square root of $x$, $\sqrt{x}$ as the positive quantity of either number that satisfies the property $\sqrt{x}\sqrt{x}=x$. Clearly, when $x<0$, such notion of sign makes no sense, so one cannot honestly talk about $\sqrt{-1}$ with the naive definition of the square root. Fine, we are better than that, and we may say that $i$ is the principal root of the equation $x^2=-1$. We then just denote it by $\sqrt{-1}$. But this way of denoting $i$ brings with its convenience a litany of disasters, including the famous $1=-1$ fallacy. Namely, one can show that $1=-1$ by $1=\sqrt{1}=\sqrt{(-1)(-1)}=\sqrt{-1}\sqrt{-1}=i^2=-1$. Many a people have pointed out the haphazardness of assuming that the familiar law $\sqrt{x}\sqrt{y}=\sqrt{xy}$ holds when $x,y<0$. But at the same time we have no shame in writing $\sqrt{-5}$ as $\sqrt{5}i$ (in fact, I think this is the very reason of inventing the imaginary unit ). Is it not terribly unnatural that the law holds for odd number of negative factors, and not so for the even ones? In fact, is there an example where this native rule (that when you have a negative radicand, you can pretty much apply the familiar laws of exponents)? (I guess it makes the first question.) Secondly (so this officially marks the second, and the last question), which ought to be the definition of $i$, in your opinion? I believe that many people choose to write $i=\sqrt{-1}$ as it gives some illusion of determinancy, whereas $i^2=-1$ does not. But I still prefer the latter definition, and it seems to be the consensus of every complex analysis textbook that I've ever laid my hands on. Better yet, I believe that the complex numbers shold be defined as the algebraic completion of reals or an isomorphic field to $\mathbb{R}\times\mathbb{R}$, with some special addition and multiplication rules, but I guess it is a little bit out of high school students' league (at least for most of them). EDIT: Thank you all for your insightful responds, but there still one thing none of you has yet answered... Is there a conunter example to the law where we have $\sqrt{-A}$ for $A>0$, we have $\sqrt{A}i$?","['complex-numbers', 'complex-analysis', 'soft-question']"
1387839,Showing quotient map $q$ is surjective and there exists another function $\bar{f}$ such that $f = \bar{f} \circ q$.,"Problem: Let $\sim$ be an equivalence relation over a set $X$ and let $X / \sim $ be the corresponding quotient set. There is a function \begin{align*} q: X \rightarrow X / \sim \ : x \mapsto [x] \end{align*} which maps each element $x \in X$ to its corresponding equivalence class in $X / \sim$. This mapping is called the quotient map corresponding to $\sim$. 1) Show that $q$ is surjective. 2) Suppose that $f : X \rightarrow Y$ is a function with the property that \begin{align*} x_1 \sim x_2 \Rightarrow f(x_1) = f(x_2). \end{align*} Prove that there exists an unique function $\bar{f} : X/ \sim \rightarrow Y$ with the property that \begin{align*} f= \bar{f} \circ q. \end{align*} Attempt at proof: For part 1) I reasoned as follows: Let $[x] \in X/ \sim$ be arbitrary. Then we have to show that there exists an element $x \in X$ such that $q(x) = [x]$. Since no equivalence class in $X / \sim$ is empty, there always exists an $x \in [x]$ for each $x \in X$. This proves that $q$ is surjective. 2) For this part, I'm not sure how to proceed. We need to construct the function $\bar{f}$ I think. So I would let $[x_1] \in X / \sim$. Then $\bar{f} [x_1] = y_1$ for some $y_1 \in Y$. Then we need to show somehow that $f = \bar{f} \circ q$ holds? How can I do that?","['abstract-algebra', 'proof-verification', 'analysis']"
1387867,"Finding a hidden ""heavy"" subset of random variables.","Let $X_1,\dots, X_n$ be independent non-negative random variables (with finite expectation and variance), and $0 < m < n$ be a fixed integer such that there exists a subset $S\subseteq [n]$ of size $\lvert S\rvert =m$ with $\sum_{k\in S} \mathbb{E}[X_k] \geq 100$; and $\sum_{k\notin S} \mathbb{E}[X_k] \leq 1$ and $\sum_{k\in S} \operatorname{Var}[X_k] \leq \frac{1}{100} \left(\sum_{k\in S} \mathbb{E}[X_k]\right)^2$; and $\sum_{k\notin S} \operatorname{Var}[X_k] \leq \frac{1}{100}$. In other words, there is an unknown subset of elements that, altogether, are ""heavy"", and its complement is ""light."" If I define $T$ as the indices of the $2m$ top values among $X_1,\dots, X_n$, what is the probability that $\sum_{[n]\setminus T} \mathbb{E}[X_i] < 2$? (where the probability is taken over the realization of the $X_i$'s). In other terms, what is the probability that we removed (almost) all the weight of the""heavy set""? If $m=1$, then this is clear that is happens with high probability by Chebyshev's inequality: if $S=\{i\}$, then the probability that $X_i$ is less than $50$ is at most $\frac{1}{25}$, and similarly the probability that the sum of all other elements (so a fortiori any of them) is greater than 2 is at most $\frac{1}{100}$, so that overall $X_i$ is the greatest element (and therefore among the two top elements) with probability at least $19/20$. Is there a way to generalize this argument to $m \geq 2$?","['order-statistics', 'probability', 'statistics']"
1387903,When does a representation of $H\subset G$ on $V$ extend to a representation of $G$ on $V$?,"Let $G$ be a finite group, $H$ a subgroup, and $\varphi:H\rightarrow GL(V)$ a finite-dimensional representation of $H$ over a characteristic zero, algebraically closed field. Let $\chi$ be the character of this representation. If $\chi$ takes different values on two different conjugacy classes in $H$ that become conjugate in $G$, then clearly $\varphi$ cannot extend to a representation of $G$ on $V$. Is this the only obstruction? I.e. if $\chi$ is the restriction of a class function on $G$, is $\varphi$ the restriction of a representation of $G$ on $V$? Another way to ask the question is, with this assumption about $\chi$, does there exist a class function that extends $\chi$ and is equal to an $\mathbb{N}$-linear combination of irreducible characters of $G$? Or are there other things that can go wrong? And if there are, are they straightforward to test for, or is there some whole subtle cohomological theory of obstructions, or what? Thanks in advance!","['group-theory', 'representation-theory', 'group-cohomology']"
1387913,Product of a Finite Number of Matrices Related to Roots of Unity,"Does anyone have an idea how to prove the following identity? $$
\mathop{\mathrm{Tr}}\left(\prod_{j=0}^{n-1}\begin{pmatrix}  
x^{-2j} & -x^{2j+1} \\
1 & 0
\end{pmatrix}\right)=
\begin{cases}
2 & \text{if } n=0\pmod{6}\\
1 & \text{if } n=1,5\pmod{6}\\
-1 & \text{if } n=2,4\pmod{6}\\
4 & \text{if } n=3\pmod{6}
\end{cases},
$$
where $x=e^{\frac{\pi i}{n}}$ and the product sign means usual matrix multiplication. I have tried induction but there are too many terms in all of four entries as $n$ grows. I think maybe using generating functions is the way?","['recurrence-relations', 'polynomials', 'number-theory', 'matrices']"
1387921,Radon measure times a function is still a Radon measure?,"Given $\Omega\subset \mathbb R^N$ is open and let function $\varphi$: $\Omega\to [1,+\infty]$, $\varphi\in L^1_{loc}(\Omega)$ be given. Suppose $\mu$ is a finite Radon measure on $\Omega$ and we future have
$$
\int_\Omega \varphi \,d\mu<\infty.
$$
Then can I claim that $\varphi\mu$ is also a finite Radon measure? It looks true but I would like to have some details. Thank you! My try: First of all, for each compact set $K\subset \Omega$, we have 
$$
\int_K \varphi\,d\mu\leq \int_\Omega \varphi\,d\mu<\infty.
$$
Now I only need to show $\varphi\mu$ is Borel regular. Let $\nu:=\varphi \mu$, we have $\nu$ is Borel since $\varphi$ is Borel measurable and $\mu$ is Radon. To finish my proof, I need to show for each $A\subset \Omega$, there exists a Borel set $B$ such that $A\subset B$ and $\nu(A)=\nu(B)$, and I got stuck here... I found another similar post here ... Please feel free to close mine.","['real-analysis', 'measure-theory']"
1387956,Differential equation related to Golomb's sequence,"While studying Golomb's sequence , the following differential equation arouse:
$$
f(f(x))=\frac{1}{f'(x)}
$$
I don't know much about differential equations so I am a bit clueless. Is there a way to solve it nicely? Edit: With the link provided by Michael Galuza, I could find a function which satisfies the given equation, namely:
$$
f(x)=\varphi^{2-\varphi}x^{\varphi-1}
$$
Where $\varphi=\frac{1+\sqrt5}{2}$ is the golden ratio. However, I did not succeed in proving that this is the only one. Is this feasible?",['ordinary-differential-equations']
1387960,Integrate $\int_{-\infty}^{\infty} \frac{\cosh(\beta x)}{1+\cosh( \beta x )} e^{-x^2} x^2 \rm{d}x$,"Integrate 
$$ \int_{-\infty}^{\infty} \frac{\cosh(\beta x)}{1+\cosh( \beta x )} e^{-x^2} x^2 \rm{d}x, $$
with $\beta \in \mathbb{R}$ and $\beta > 0$. Numerical integration shows that this integral exists, but I have been unable to find a closed analytical expression (using contour integration). I have tried to use a rectangular contour $(-R,0) \to (R,0) \to (R,i\eta) \to (-R,i\eta) \to (-R,0)$. The vertical (imaginary direction) integrals at $\pm R$ vanish for $R \to \infty$. I am unable to find an $\eta$ that allows me to relate the horizontal parts of the contour, which would in turn allow me to equate the result to the sum of residues of the enclosed poles.","['contour-integration', 'complex-analysis']"
1387973,Limit of: $\cos(x) + \sin(x)\cos(x) + \sin(x)\sin(x)\cos(x)$,"From playing around with a graph, I noticed that: $$
\lim_{a \to \infty}
\sum_{n=0}^{a}
      \left( \sin \left( x-\frac{\pi }{2} \right) \right)^n
      \cos \left( x-\frac{\pi }{2} \right) 
  \approx \tan \left( 0.5x \right)
$$ Can anyone prove this?",['trigonometry']
1387976,Continuity ( Functions of 2 variables ).,"Given , $$ f(x,y) = \begin{cases} 
      \dfrac{xy^{3}}{x^{2}+y^{6}} & (x,y)\neq(0,0) \\
      0 & (x,y)=(0,0) \\ 
\end{cases} $$ We need to check whether the function is continuous at $(0,0)$ or not.. The solution says it is continuous at $(0,0)$. What I tried was the following; For the function to be continuous at the point $(0,0)$, the limit $$\lim_{(x,y) \to (0,0)} f(x,y)$$ should exist. Consider $$\lim_{(x,y) \to (0,0)} \frac{xy^{3}}{x^{2}+y^{6}}$$ I choose a path $y=mx^{\frac{1}{3}}$ and approach $(0,0)$ along this path, thus the above expression becomes $$\lim_{(x,y) \to (0,0)} \frac{xm^{3}x}{x^{2}+m^{6}x^{2}}$$ which comes out to be $$\frac{m^{3}}{1+m^{6}}$$ Clearly the limit isn't unique and should not exist, but the solution says that the function is continuous at $(0,0)$. How ? Can anyone help? What am I doing wrong ?","['continuity', 'multivariable-calculus', 'limits']"
1387983,extension by zero for sobolev functions.,"Given some function in $H^1(A)$, and if $B$ is an open subset (A also open) containing $A$, do we get an element of $H^1(B)$ if we just extend by 0? I dont think so, but what would be a simlple counterexample?  Why exactly is it impossible to do this in general?","['sobolev-spaces', 'functional-analysis']"
1387986,Power series expression for $\exp(-\Delta)$,"I know it should be true, but for some reason I can't get the calculations to work out in order to show that if $f$ is smooth and compactly supported, the power series 
$\sum_{j=0}^\infty \frac{(\partial_x^2)^j}{j!} f $ converges in $L^2$. I hope someone can help!","['power-series', 'functional-analysis', 'hilbert-spaces', 'mathematical-physics', 'partial-differential-equations']"
1387990,How to simplify this combinatorial expression?,"Find 
\begin{eqnarray}
\sum_{j\in\mathbb{N}}(n-2j)^k\binom{n}{2j-m}
\end{eqnarray} Note that this question is a generalization of this one . I tried to imitate the steps in the answer given in that post but without success. Any idea?","['summation', 'polynomials', 'combinatorics']"
1388038,How do I find the remainder of $4^0+4^1+4^2+4^3+ \cdots + 4^{40}$ divided by 17?,"Recently I came across a question, Find the remainder of $4^0+4^1+4^2+4^3+ \cdots + 4^{40}$ divided by 17? At first I applied sum of G.P. formula but ended up with the expression $1\cdot \dfrac{4^{41}-1}{4-1}$. I couldn't figure out how to proceed further. Secondly I thought of using the fact $(a+b+\cdots) \pmod {17} = (r_a+r_b\dots) \pmod {17}$ but it is getting more messier. Please explain in detail. And also mention the formula being used.","['arithmetic', 'elementary-number-theory', 'modular-arithmetic', 'algebra-precalculus']"
1388051,Extending a smooth function of constant rank,"Let's denote $\mathbb{H}^m = \{(x_1, \ldots, x_m) \in \mathbb{R}^m\ |\ x_m \geq 0\}$. For an open subset $U \subset \mathbb{H}^m$, a function $f : U \to \mathbb{R}^n$ is called smooth if it can be locally extended to smooth functions (defined on open subsets of $\mathbb{R}^m$). By a simple partition of unity argument, this is equivalent to there being a smooth extension $\widetilde{f} : \widetilde{U} \to \mathbb{R}^n$, where $\widetilde{U} \supset U$ is an open subset of $\mathbb{R}^m$. Let us suppose now that $f$ has differential of constant rank equal to $k$ (by taking unilateral partial derivatives, it is clear that the differential of smooth extensions of $f$ doesn't depend on the choice of the extension throughout $U$, so we therefore may talk about the differential of $f$ on the boundary as well). My question is: can we guarantee the existence of a smooth extension $\widetilde{f}$ of $f$ that also has differential of constant rank $k$?","['analysis', 'smooth-manifolds', 'multivariable-calculus']"
1388062,Does this double sum $\sum_{m=0}^\infty \sum_{n=0}^\infty \frac{m+n+mn}{2^m(2^m+2^n)}$ converge?,"$$\sum_{m=0}^\infty \sum_{n=0}^\infty \frac{m+n+mn}{2^m(2^m+2^n)}$$
I have no experience evaluating double sums, but what intuition I have about single sums suggests to me that this series should converge. However, Mathematica fails to evaluate the sum and WolframAlpha tells me the series diverges . I'm wondering if the averaging technique used here might yield something useful, but I don't if I'm adapting it properly. If $S$ denotes my sum, then
$$2S=\sum_{m=0}^\infty\sum_{n=0}^\infty \frac{m+n+mn}{2^{m+n}}$$
which I'm told also diverges . I don't think this method will work since I can't split the numerator into a product $a_mb_n$.","['sequences-and-series', 'convergence-divergence', 'divergent-series']"
1388132,Question about limits using delta - epsilon definition,"So I was trying to understand better limits using the $\epsilon$-$\delta$ definition and I decided to disprove a limit which is obviously false. Say, $\lim_{x\to 1} (x + 2) = 10$. Ok, this is false because the limit is  $3$, but using the definition: $|(x+2) - 10|< \epsilon$ (this is my epsilon) $|(x-1)| < \delta$ (this is my delta) $|x-1 -7| < \epsilon$ $|x-1|< \epsilon+7$ so it looks like if my $\delta$ is equal to $\epsilon +7$, I should get an $\epsilon$ bigger than $\delta$, since the limit is false. But if $\epsilon$ is say $20$, $\delta$ can be at most $27$, so let us pick $x$ to be $26$ $|(26-2) - 10| < 20$ $|24 - 10| < 20$ $|14| < 20$ So it works!! so my question is, where is my mistake?","['calculus', 'limits']"
1388171,$\cosh^4x-\sinh^4x=\cosh2x$,I need to show that $\cosh^4(x)-\sinh^4(x) = \cosh(2x)$ First I found myself going in circles.. $$\cosh (2 x)=\frac{1}{2} \left(e^{-2 x}+e^{2 x}\right)= \sinh(2x)$$ Now I'm trying to get somewhere using the identity $$\cosh ^2(x)-\sinh ^2(x)=1$$ if $\cosh ^2(x)-\sinh ^2(x)=1$ then $$\cosh ^2(x)-\sinh ^2(x)=\left(\frac{1}{2} \left(e^{-x}+e^x\right)\right)^2-\left(\frac{1}{2} \left(e^x-e^{-x}\right)\right)^2$$ yet the same doesn't apply when I take them to the $4^\text{th}$ power. Please could someone point me in the right direction as I'm getting very lost here.,"['hyperbolic-functions', 'trigonometry']"
1388176,to prove that a particular element is a central element,"If $x$ to $x^n$ is an automorphism of $G$. Show that for all $x\in G$, $x^{n-1}$ is a central element of $G$. I am not really sure how to go about the proof so I am looking for some clue on how to go about it.",['group-theory']
1388187,Determine if matrices A and B are similar,Determine if matrices $A$ and $B$ are similar. $A=\begin{pmatrix}1 & 1 & 0 & 0 \\0 & 1 & 0 & 0 \\0 & 0 & 3 & 0 \\0 & 0 & 0 & 3\end{pmatrix}\: $ and $B=\begin{pmatrix}1 & 1 & 0 & 0 \\0 & 1 & 0 & 0 \\0 & 0 & 3 & 1 \\0 & 0 & 0 & 3\end{pmatrix}$ I found the characteristic polynomials to be the same for both matrices $(1-λ)(3-λ)(3-λ)(1-λ)$ so I went to plug in the values to find the eigenvectors for matrix $A$ and I think I may have screwed up because $\begin{pmatrix}0 & 1 & 0 & 0 \\0 & 0 & 0 & 0 \\0 & 0 & 2 & 0 \\0 & 0 & 0 & 2\end{pmatrix}$ I got an eigenvector for $λ=1$ to be $\begin{pmatrix}0 \\0 \\0 \\0\end{pmatrix}$ which I'm not even sure if I did it correctly or not because I don't think this is possible. for $λ=3$ I got $\:\begin{pmatrix}1 \\\frac{1}{2} \\0 \\0\end{pmatrix}$ If I did do this correctly then is the geometric multiplicity for both = 1?,"['eigenvalues-eigenvectors', 'linear-algebra', 'matrices']"
1388203,First-order definition of nonnegative in integers,"Given the structure $(\mathbb Z,+,-,\times,0,1)$, what's the easiest way to write ""$x\ge0$"" in that structure? I know that this works:
$$\exists a\exists b\exists c\exists d,a^2+b^2+c^2+d^2=x$$
because of Lagrange's four-square theorem, but this seems like an unnecessarily complicated way to do it (because the theorem is such a nontrivial result). Is there an easier way?","['first-order-logic', 'number-theory', 'integers', 'elementary-number-theory', 'logic']"
1388206,Show that $\sqrt{4+2\sqrt{3}}-\sqrt{3}$ is rational using the rational zeros theorem,"What I've done so far: Let $$r = \sqrt{4+2\sqrt{3}}-\sqrt{3}.$$ Thus, $$r^2 = 2\sqrt{3}-2\sqrt{3}\sqrt{4+2\sqrt{3}}+7$$ and $$r^4=52\sqrt{3}-28\sqrt{3}\sqrt{4+2\sqrt{3}}-24\sqrt{4+2\sqrt{3}}+109.$$ I did this because in a similar example in class, we related $r^2$ and $r^4$ to find a polynomial such that $mr^4+nr^2 = 0$ for some integers $m,n$. However, I cannot find such relation here. Am I doing this right or is there another approach to these type of problems.","['analysis', 'proof-verification']"
1388234,Gauss sums and module endomorphisms,"Let $p$ be an odd prime and $n \in \mathbb{N}$. Let $a,b,c$ be arbitrary integers such that $ab \neq 0$. We write $p^{\alpha}A = a$ and $p^{\beta}B = B$ for some $\alpha, \beta \in \mathbb{N}_0$ and $A, B \in \mathbb{Z}$ are such that $(AB,p)=1$. Further, we assume that $c$ is chosen such that $p^{\alpha} \mid c$. Observe that 
\begin{align*}
4a(ax^2+bxy+cy^2) &= (2ax+by)^2 + (4ac-b^2)y^2\\
p^{\alpha}(ax^2+bxy+cy^2) &\equiv (4A)^{-1}(2ax+by)^2+(4A)^{-1}(4ac-b^2)y^2 \mod{p^n}.
\end{align*} Let $X := 2p^{\alpha}Ax + by$ and let $e(q):= e^{2\pi \imath q}$ for any rational number $q$. Then consider the double sum
\begin{align*}
\frac{1}{p^{2\alpha}}& \sum_{x,y=0}^{p^{n+\alpha}-1} e\left(\frac{p^{\alpha}(ax^2+bxy+cy^2)}{p^{n+\alpha}}\right)\\
&= \frac{1}{p^{2\alpha}}\sum_{y=0}^{p^{n+\alpha}-1} e\left(\frac{(4A)^{-1}(4ac-b^2)y^2}{p^{n+\alpha}}\right) \sum_{x=0}^{p^{n+\alpha}-1} e\left(\frac{(4A)^{-1}X^2}{p^{n+\alpha}}\right).
\end{align*} It can be shown that if $\beta < \alpha$ then
\begin{align*}
\sum_{x=0}^{p^{n+\alpha}-1} e\left(\frac{(4A)^{-1}X^2}{p^{n+\alpha}}\right) = 0.
\end{align*} Let $R:= \mathbb{Z}/p^{n+\alpha}\mathbb{Z}$. From my previous question (see here ), I can define an $R$-module endomorphism $\phi: R \times R \rightarrow R \times R$ by $\phi(x,y) = (2p^{\alpha}Ax+by,y)$. Under this endomorphism, we have that $\phi(R \times R) \simeq \mathbb{Z}/p^{n}\mathbb{Z} \times R$. I want to now argue that
\begin{align}
\sum_{x=0}^{p^{n+\alpha}-1} e\left(\frac{(4A)^{-1}X^2}{p^{n+\alpha}}\right) = \sum_{X=0}^{p^{n+\alpha}-1} e\left(\frac{(4A)^{-1} p^{2\alpha}(2Ax+by)^2}{p^{n+\alpha}}\right). (1)
\end{align} I think I need to use the fact that $p^{\alpha}(2Ax)+by$ is an $R$-module and then use the 1st and 3rd isomorphism theorems. I am not sure how to proceed though. I may have it wrong, but I think that $\phi$ is an onto map, and the elements of the first component $R$ will get mapped to $\mathbb{Z}/p^{n}\mathbb{Z}$, and will yield $p^{\alpha}$ copies. I basically need an argument so that I can replace the index $x$ by $X$ in my sum. I can obtain my desired result by considering the two cases $\alpha \leq \beta$ and $\alpha > \beta$, so I know that (1) will hold (with possibly an extra square factor $p^{2\alpha}$ missing.) But I'm pretty sure there's some clever way to show this holds using the properties of $R$-module endomorphisms. I would really like to avoid specifying cases for $\beta$. Any suggestions on things to try or any corrections would be really helpful! Thanks!","['abstract-algebra', 'gauss-sums', 'number-theory', 'modules']"
1388242,Function grows slower than $\ln(x)$,What function grows slower than $\ln(x)$ as $x \rightarrow\infty$ ? How am I supposed to find it besides just trying finding limits of all known functions? I am looking for functions that are unbounded and strictly increasing for real $x>0$ and grow slower than $\ln(x)$,"['calculus', 'real-analysis', 'limits']"
1388263,How to find eigenvalues of matrix $\begin{bmatrix} 3& a+1\\a+1&3 \end{bmatrix}$,I want to find the eigenvalues of the following matrix $\begin{bmatrix} 3& a+1\\a+1&3 \end{bmatrix}$ expressed in $a$ using $\begin{bmatrix} \lambda - 3& a+1\\a+1&\lambda-3 \end{bmatrix}$. But the $a$-term makes it difficult for me to find it. I hope someone can show me how to do this. Thanks in advance ! EDIT: Determinant = $4a^2+8a+4$ Using the abc rule I get $(6\overset{+}{-} \sqrt{4a^2+8a+4})/2$,"['eigenvalues-eigenvectors', 'linear-algebra']"
1388289,$2^{nd}$ order PDE: Solution,"I am trying to solve the following equation:
$$\frac{\partial F}{\partial t} = \alpha^2 \, \frac{\partial^2 F}{\partial x^2}-h \, F$$
subject to these conditions: $$F(x,0) = 0, \hspace{5mm} F(0,t) = F(L,t)=F_{0} \, e^{-ht}.$$
I know that I am suppose to simplify the equation with:
$$F(x,t)=\phi(x,t)e^{-ht}$$
My initial guess is to divide by $$\alpha^2$$
and have this:
$$\frac{d^2F}{dx^2}-\frac{1}{\alpha^2}\frac{dF}{dt}-\frac{h}{\alpha^2} \, F=0.$$
I am having trouble with the next steps. Should I assume a solution of the exponential form?","['ordinary-differential-equations', 'partial-differential-equations']"
1388303,If $\sum c_{n} (z-a)^n$ has a radius of convergence $R$ show $\sum c_{n} n(z-a)^{n-1}$ does as well,"If the series, $$\sum_{n=0}^{\infty} c_{n} (z-a)^{n}$$ has a radius of convergence, $R$ . Show, $$\sum_{n=0}^{\infty} c_{n} n(z-a)^{n-1} $$ has the same radius of convergence. My proof: The power series $$\sum_{n=0}^{\infty} c_{n} (z-a)^{n}$$ having a radius of convergence $R$ means that, $$\lim_{n \to \infty}|\frac{c_n}{c_{n+1}}|=R$$ Let $k_n=nc_n$ , by definition $$\sum_{n=0}^{\infty} k_{n} (z-a)^{n-1} $$ is also a power series so the same theorem should apply. $$\lim_{n \to \infty} |\frac{nc_{n}}{(n+1)c_{n+1}}|=R \lim_{n \to \infty} |1-\frac{1}{n}|=R$$ Q.E.D Is my proof correct? Does anyone else have another proof method?","['sequences-and-series', 'limits', 'complex-analysis', 'proof-verification']"
1388310,Show that $\sum^\infty_{n=1} \mu(\{x : |f_n(x) - f(x)| > \epsilon\}) < \infty$ implies $f_n \to f$ a.e.,"Show that $\sum^\infty_{n=1} \mu(\{x : |f_n(x) - f(x)| > \epsilon\}) < \infty$ implies $f_n \to f$ a.e, where $f_n$ and $f$ are measurable functions. My attempt: The Borel Cantelli lemma gives us that almost all $x$ are in at most finitely many $A_n = \{x : |f_n(x) - f(x)| > \epsilon\}$, so, for each $x$, there exists some $N$ such that for all $n \ge N$, $|f_n(x) - f(x)| \le \epsilon$. Since this is true for all $\epsilon$, we let $\epsilon$ go to zero, and we are done. This is what I have, but someone told me that I have an issue with countability, and that in order to fix it, I need to consider $\epsilon = 1/k$. But, I don't see why my proof is incorrect, so I don't see how that would fix it.","['elementary-set-theory', 'real-analysis', 'measure-theory']"
1388345,Simplifying $\cos(\arcsin x)$?,"I remember reading somewhere that we can simplify $\cos(\arcsin x)$ and $\sin(\arccos x)$ in terms of a polynomial by making the substitution $m=\arcsin x$ or $m=\arccos x$ (respectively), then constructing a right angle triangle with appropriate ratios. Does anyone know what I'm talking about? I don't really remember how to do this, so if someone can show me I would really appreciate it! Thanks.",['trigonometry']
1388361,Show that $R/I\otimes_R R/J\cong R/(I+J)$,"Let $R$ be a commutative ring with identity and $I$ and $J$ be ideals in $R$. Show that $R/I\otimes_R R/J\cong R/(I+J)$ as $R$-modules. This is what I tried. Define a map $f:R/I\times R/J\to R/(I+J)$ as $f(\bar r, \bar s)=rs+(I+J)$.
This is a well-defined bilinear map. By the universal property of tensor products, we get an $R$-linear map $F:R/I\otimes_R R/J\to R/(I+J)$ which sends $\bar r\otimes \bar s$ to $rs+(I+J)$. This map is surjective. So all we need to show is that the kernel of $F$ is $0$. But I am unable to do this.","['abstract-algebra', 'tensor-products', 'modules']"
1388366,Find the integer solutions of the equation $x_1 - x_2 - x_3 + x_4 = 0$,Considering the equation: $x_1 - x_2  - x_3 + x_4 = 0$ such that: $x_i \le 1000$ and $x_i > 0$ Find the number of integer solutions. How to consider the subtractions?,"['integers', 'discrete-mathematics']"
1388390,How to prove the convergence in probability?,"$Y_n$ , $n = 1, 2, ...$ is a sequence of nonnegative random variables with their means converging to $0$ . Can we show $Y_n \rightarrow 0$ in probability? Thanks!","['probability-theory', 'statistics']"
1388403,"Integrate by parts to prove that this integral provides an analytic continuation ,","Suppose $f(z) = \sum_0^\infty a_nz^n$ converges for $|z| \le 1$. a) Prove $\phi(z) = \sum_0^\infty \frac{a_n}{n!}z^n$ is entire and $|\phi(z)|\le Me^{|z|}$. b) Prove $f(z) = \int_0^\infty e^{-s}\phi(sz)ds$. (Hint: Integrate by parts.) Apply this result to $f(z) = \sum_0^\infty z^{2n}$, which converges in $|z| < 1$ and show that the integral provides an analytic continuation of $f(z)$. I have proved part(a), but am stuck on part(b). With integration by parts, I am currently at: $$-\phi(sz)e^{-s}|_0^{\infty} + z\int_0^\infty e^{-s}\phi'(sz)ds$$ Thanks, EDIT:  if I ignore the convergence of $f(z)$ on the boundary, and just assume that the convergence is for $|z|<1$, then repeated integration by parts, and the fact that $\phi$ is infinitely differentiable, pushes out the terms $a_0 + a_1z + a_2z^2 + ... = \sum_0^\infty a_nz^n = f(z)$, which is what we needed to prove for part(b).  But, is it ok to ignore the convergence on the boundary? I.e., only consider $z$ such that $|z|<1$, but the I feel that we haven't proved that the integral is exactly $f(z)$.  Or, I might just be interpreting the question incorrectly, too.","['analyticity', 'sequences-and-series', 'integration', 'convergence-divergence', 'complex-analysis']"
1388407,"PDF of $X = \max\{X_1,X_2\}$, being $X_1$ and $X_2$ independent Normal distributed random variables","Let $X_1 \sim \mathcal{N}(\mu_1,\sigma_1^2)$ and $X_2 \sim \mathcal{N}(\mu_2,\sigma_2^2)$, what's the CDF of $X = \max\{X_1,X_2\}$? Both variables are assumed to be independent. I tried the following: \begin{equation}
F_X(x) = \text{Prob}\{X<x\} = \text{Prob}\{X_1<x, X_2<x\} = \text{Prob}\{X_1<x\}\cdot \text{Prob}\{X_2<x\} = F_{X_1}(x)\cdot F_{X_2}(x)
\end{equation} Since $X_1$ and $X_2$ are normal distributed variables, their CDFs are well known: \begin{equation} \tag{1}
F_X(x) = F_{X_1}(x)\cdot F_{X_2}(x) = \bigg(1-\mathcal{Q}\Big(\frac{x-\mu_1}{\sigma_1}\Big)\bigg)\cdot \bigg(1-\mathcal{Q}\Big(\frac{x-\mu_2}{\sigma_2}\Big)\bigg)
\end{equation} where $\mathcal{Q}\big(\frac{x-\mu}{\sigma}\big) = \int_x^{\infty} \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx$. However, MATLAB simulation shows that this is not correct. The MATLAB code and the plot result comparing simulation and theoretical is shown below. S=1e6;

% Define Normal Distributed RV #1
mu_1 = 1;
sigma_1 = 1;
X_1 = sigma_1/sqrt(2)*(randn(1,S))+mu_1;

% Define Normal Distributed RV #1
mu_2 = 1;
sigma_2 = 1;
X_2 = sigma_2/sqrt(2)*(randn(1,S))+mu_2;

% Define variable X = max{X_1,X_2}
X = max(X_1,X_2);
[X_pdf, X_var] = ecdf(X); % Obtain empirical CDF (similar to function hist)

X_pdf_th = (1-qfunc((X_var-mu_1)/sigma_1)).*(1-qfunc((X_var-mu_2)/sigma_2));

plot(X_var,X_pdf); hold on;grid on;
plot(X_var,X_pdf_th,'r');  legend('Simulation CDF', 'Theory CDF'); hold off;","['probability-theory', 'normal-distribution', 'probability', 'probability-distributions']"
1388414,"Show $\lim_{m \to \infty ,n \to \infty } f(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}) = f(x,y)$","Suppose $f(x,y)$ is defined on $[0,1]\times[0,1]$ and continuous on each dimension, i.e. $f(x,y_0)$ is continuous with respect to $x$ when fixing $y=y_0\in [0,1]$ and $f(x_0,y)$ is continuous with respect to $y$ when fixing $x=x_0\in [0,1]$ . Show $$\lim_{m \to \infty ,n \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = f(x,y)$$ My attempt: First, I know $$\lim\limits_{m \to \infty ,n \to \infty } \left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = (x,y)$$ Secondly it looks $$\lim\limits_{m \to \infty }\lim\limits_{n \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = \lim \limits_{m \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},y\right) =  f(x,y)$$ and $$\lim\limits_{n \to \infty } \lim\limits_{m \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = \lim\limits_{n \to \infty } f\left(x,\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = f(x,y)$$ since $f(x,y)$ is continuous on each dimension. However, I am not sure if this can infer $\lim\limits_{m \to \infty ,n \to \infty } f(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}) = f(x,y)$ . Can anyone provide some help? Thank you! Added: I am now sure $\lim\limits_{m \to \infty } \lim\limits_{n \to \infty } {a_{mn}} = \lim\limits_{n \to \infty } \lim\limits_{m \to \infty } {a_{mn}} = L$ does not imply $\lim\limits_{m \to \infty ,n \to \infty } {a_{mn}} =L$ in general. Hope someone can help solve the problem.","['continuity', 'limits', 'real-analysis']"
1388421,reference for linear algebra books that teach reverse Hermite method for symmetric matrices,"May 11, 2019. Evidently the original method should be attributed to Lagrange in 1759. I got confused, Hermite is much more recent. January 13, 2016: book that does this mentioned in a question today, Linear Algebra Done Wrong by Sergei Treil. He calls it non-orthogonal diagonalization of a quadratic form, calls his first method completion of squares, pages 201-202, section 2.2.1.  In section 2.2.2, pages 202-205, he describes this method, calling it Diagonalization using row/column operations. The method I mean is useful for symmetric matrices with integer, or at least rational entries. It diagonalizes but does NOT orthogonally diagonalize. The direction I do it, I usually call it Hermite reduction or Hermite's method. At the end, I need to find the inverse of my matrix (which usually has determinant one so it is not so bad). This other method produces an answer directly, a cookbook method not conceptually different from row reduction of matrices, especially using that to find its inverse. This method is very similar to Gauss reduction for positive binary quadratic forms, just allowing rational coefficients in the elementary matrices used; Gauss stuck with integers. The method is mostly Gauss reduction, intended for binary positive forms. We deal with two variables (row/column pairs) at a time. As long as one of the two diagonal entries is nonzero there is no trouble, no choices to be made.
We start with a symmetric matrix $A_0.$ At each step, call it step $n,$ we are going to use some elementary matrix $E_n,$ same as in row reduction, such that $A_n =E_n^T A_{n-1} E_n$ has one fewer pair of off-diagonal nonzero entries. We also began with $P_0=I,$ then each step we take $P_n=P_{n-1}E_n.$ Eventually we get to some $n=N$ such that $A_N=D$ is diagonal and $P_N=P,$ with $P^T A P = D$ by construction. Oh, also by construction, $P$ has determinant $1.$ I JUST PUT AN EXAMPLE AT Find the transitional matrix that would transform this form to a diagonal form. not yet typeset, it is input and output from gp-pari and should not be too difficult to read, indeed one may copy the individual commands into pari and see how it progresses. I also put a 4 by 4 answer, the final answer typeset otherwise gp-pari output, at Given a $4\times 4$ symmetric matrix, is there an efficient way to find its eigenvalues and diagonalize it? Let me go through the two examples, the second involves a choice because we get a zero diagonal element at one point. First: Let $$A = \left(\begin{array}{cc} 2&3 \\ 3&4  \end{array}\right) \in
M_n(\mathbb{C})$$ Find $P$ such that $P^TAP = D$ where $D$ is a diagonal matrix. So here's the solution: $$A = \left(\begin{array}{cc|cc} 2&3&1&0\\ 3&4&0&1    \end{array}\right) \sim \left(\begin{array}{cc|cc} 2&0&1&-3/2\\ 0&-1/2&0&1    \end{array}\right)$$ Therefore, $$P = \left(\begin{array}{cc} 1&-3/2\\ 0&1    \end{array}\right) \\ P^TAP = \left(\begin{array}{cc} 2&0\\ 0&-1/2    \end{array}\right) $$ So, this one was just Gauss reduction, allowing a rational off-diagonal entry in my $E_1$ in order to force the $1,2$ and $2,1$ pair of  positions to become zero. As long as the upper left of the two diagonal coefficients is nonzero, we may take our $E_n$ to be upper triangular. If we are faced with a zero diagonal entry in the first row/diagonal that possesses any nonzero (therefore off-diagonal) entries,  we need to do an extra step to force a nonzero diagonal element. So, let's do the ever popular form $2xy$ this way. $$ A = A_0 =
\left(
\begin{array}{cc}
0 & 1 \\
1 & 0
\end{array}
\right) 
$$ As both diagonal entries are zero, switching row/columns 1 and 2 will still give $0$ in the 1,1 position. We do not like that. Instead, we take
a lower triangular $E_n,$ here $$ E_1 = 
\left(
\begin{array}{cc}
1 & 0 \\
1 & 1
\end{array}
\right) 
$$ The way i am numbering the matrices, this gives $$ A_1 = E_1^T A E_1 = 
\left(
\begin{array}{cc}
2 & 1 \\
1 & 0
\end{array}
\right), 
$$ also $$ P_1 = E_1.  $$ Next, we go back to the more common upper triangular elementary matrices, with $$ E_2 = 
\left(
\begin{array}{cc}
1 & -\frac{1}{2} \\
0 & 1
\end{array}
\right). 
$$ $$ D= A_2 = E_2^T A_1 E_2 = 
\left(
\begin{array}{cc}
2 & 0 \\
0 & -\frac{1}{2}
\end{array}
\right), 
$$ also $$  P = P_2 = P_1 E_2 = E_1 E_2 =
\left(
\begin{array}{cc}
1 & -\frac{1}{2} \\
1 & \frac{1}{2}
\end{array}
\right), 
$$ Note that, from $A_1 = E_1^T A E_1 $ and $D= A_2 = E_2^T A_1 E_2$ we indeed have $$\color{red}{
D= A_2 = E_2^T (E_1^T A E_1) E_2 = E_2^T E_1^T A E_1 E_2 =  (E_1 E_2)^T A (E_1 E_2)}
$$ which is why $P = E_1 E_2.$ The solution manual that has this would use ""augmented"" matrices, 4 by 2, not record the individual $E_i,$ just the $A_i$ augmented by $P_i.$ At least, given how I am numbering things, this is how I  prefer to write such a summary, it may be slightly different for the examples in the other question: $$ (A_0|P_0) = \left(\begin{array}{cc|cc} 0&1&1&0\\ 1&0&0&1    \end{array}\right)$$ $$ \mapsto  (A_1|P_1) =  \left(\begin{array}{cc|cc} 2&1&1&0\\ 1&0&1&1    \end{array}\right)$$ $$ \mapsto  (A_2|P_2) = \left(\begin{array}{cc|cc} 2&0&1&-\frac{1}{2}\\ 0&-\frac{1}{2}&1&\frac{1}{2}    \end{array}\right)$$ I have been seeing this method lately, but do not know any book that teaches it (or in what language). It would appear to be a book about matrix theory or linear algebra, and may never mention quadratic forms, hard to predict. Or, it may do quadratic forms over the reals, as is pretty common, and ignore the case of integer coefficients. I suspect nobody on MSE has taught this method, perhaps it is a recent book. Here are recent occurrences, apparently two by the same guy, then two by another person. To find others, look for my answers that use the phrase Hermite reduction. One of the latter is answered my way, just called repeated completing the square, which is exactly right. Finding $P$ such that $P^TAP$ is a diagonal matrix Diagonalize a symmetric matrix Find the transitional matrix that would transform this form to a diagonal form. Diagonalizing a particular $3\times3$-matrix. Very similar to the method in a Schaum's outline as seen in this answer: Given a $4\times 4$ symmetric matrix, is there an efficient way to find its eigenvalues and diagonalize it? Indeed, here is the image uploaded by el.Salvador there:","['quadratic-forms', 'reference-request', 'linear-algebra']"
1388460,Find the Integrating Factor of $xdy-3ydx=\frac{x^4}{y}dy$,"This is integrating factor by inspection, $xdy-3ydx=\frac{x^4}{y}dy$ I've been trying to look for the Integrating factor for this problem but I can't still get one right. I think I really need to use the $3ydx$ in the problem since it has the $dx$ but i can't remove the $-3$ any ideas how?",['ordinary-differential-equations']
1388468,"Is it true that if two open sets in a topological space intersect each other and share the same boundary, they are the same?","Suppose we have a topological space $(X,\mathcal T)$. Open sets $A,B$ have the following properties: $A\cap B\ne\emptyset$ $\partial A=\partial B$ Then $A=B$. Is it correct? If so, how to prove it?",['general-topology']
1388471,Is the positive or negative variation of a signed measure finite?,"I'm studying measure theory and read about signed measure. A signed measure is a function $\nu:\mathcal{A}\to \mathbb{R}\cup\{\pm\infty\}$, where $\mathcal A$ is a certain $\sigma-$algebra, such that $\nu(\varnothing)=0$ $\nu$ is $\sigma-$aditive $\nu$ can take the $\infty$ value or the $-\infty$ value, but not both. I manage the next definitions. The positive variation of $\nu$ is defined by $\nu^+(A):=\sup\{\nu(B): B\subseteq A,B\in\mathcal{A}\},\quad\forall A\in\mathcal A$, and the negative variation of $\nu$ is defined by $\nu^-:=(-\nu)^+$. I prove that both are positive measures, but I don't know if anyone of them is finite. I read on wikipedia ( https://en.wikipedia.org/wiki/Signed_measure , Properties section) that one of them is finite, but it uses Hahn decomposition theorem to define the positive and negative variations and I am not acquainted with that theorem.",['measure-theory']
1388480,Norm of an integral operator $L^1 \rightarrow L^\infty$,"Let $T:L^1(\mathbb{R}^n)\rightarrow L^\infty(\mathbb{R}^n)$ be an integral operator, i.e. there exist $K:\mathbb{R}^n\times\mathbb{R}^n\rightarrow\mathbb{R}^n$ such that for all $f\in L^1(\mathbb{R}^n)$ it holds
$$Tf(x):=\int_{\mathbb{R}^n}K(x,y)f(y)\:dy\quad\text{for all }x\in\mathbb{R}^n.$$ Does anyone knows how to prove that $\|T\|_{L^1\rightarrow L^\infty}=\sup_{x,y}|K(x,y)|$?","['integral-transforms', 'functional-analysis']"
1388486,Finding the angle of a triangle inscribed inside two congruent circles.,"Two congruent circles, centered at $A$ and $B$, intersect at $C$. When $AC$ is extended, it intersects the circle centered at $B$ at $D$. If $\angle DAB$ is $43^{\circ}$, then find $\angle DBA$, in degrees. What I attempted to do was to project a line from $A$ upward until it touched the circle and connect it with $C$. I then extend a line from $C$ to $B$. I though these two new triangles were congruent due to similar sides and found the $\theta$ above $A$ to be $47^{\circ}$ so thought $D$ would also be $47^{\circ}$ leaving the final  $\angle DBA$ to be $90^{\circ}$ which is not correct. Thanks for any help my geometry is quite bad which is why I'm trying to do problems and look up relevant information when I get stuck. I don't have much information so I know I have to extend some lines but other then that I'm unsure.",['geometry']
