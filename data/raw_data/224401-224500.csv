question_id,title,body,tags
4618231,"Coordinate system $(\phi, U, x_1, x_2)$ around $p \in S^1$ such that $S^1 \cap U = \{(x_1, x_2): x_2 = 0\}$.","Consider $S^1$ as a subspace $\Phi^{-1}(1) \subseteq \mathbb{R}^2$ , where $\Phi(x_1, x_2) = x_1^2 + x_2^2$ . Using the Implicit Function Theorem, show that for all points $p \in S^1$ , there is a coordinate system $(\phi, U, x_1, x_2)$ around $p$ such that $S^1 \cap U = \{(x_1, x_2): x_2 = 0\}$ . I think that I have to show the existence of a chart $(\phi, U)$ around $p\in U \subseteq \mathbb{R}^2$ such that $\pi_2\circ \phi(u, v) = x_2(u, v) = 0$ precisely when $(u, v) \in S^1 \cap U$ , i.e. $\Phi(u, v) = 1$ . However, I am not sure how to involve the Implicit Function Theorem here.","['smooth-manifolds', 'multivariable-calculus', 'implicit-function-theorem', 'manifolds', 'differential-geometry']"
4618234,"Find minimum value of $(ab-2a+4)^2 + (bc-2b+4)^2+ (ca-2c+4)^2$ where $0 \leq a,b,c \leq2$","Find minimum value of $(ab-2a+4)^2 + (bc-2b+4)^2+ (ca-2c+4)^2$ where $0 \leq a,b,c \leq2$ For this kind of problems it's usually easy to guess that the minimum value is obtained at $a=b=c$ or when the answer is at the boundary. When $a=b=c$ we can easily find minimum = 27. When we are at the boundary, we let $c=2$ we can see the expression is greater than or equal to 24. But this solution seems hacky and i'd like to find a better way.","['optimization', 'algebra-precalculus', 'lagrange-multiplier', 'inequality']"
4618256,Integral transform for finding what polynomial solves a differential equation.,"If the following kind of differential equations: $$
\frac{d^2f}{d^2t} + \frac{df}{dt} + f = 0 \\
$$ Have a solution made up by a linear combination of exponentials; which can be easily find by using the laplace transform: $$
\mathcal L\{ f(t) \} = \int_{0}^{\infty} { f(t) e^{-st} dt} \\
$$ I was wondering if, assuming the solution of the following differential equation: $$
t^2\frac{d^2f}{d^2t} + t\frac{df}{dt} + f = 0
$$ Is a linear combinations of terms like: $$
f(t) = A t^r
$$ Could be silly to define a transform like this: $$
J\{ f(t) \} \equiv \int_{0}^{\infty} { f(t) \; t^{s} dt} \\
$$ My problem is: $$
J\{ tf'(t) \} = \int_{0}^{\infty} { f'(t) \; t^{s + 1} dt} = t^{s + 1} f(t)\Bigg |_{t = 0}^{\infty} - (s + 1) \int_{0}^{\infty} f(t) \; t^{s} dt
$$ This integral do not converge, so I cannot find the solution of the equation. Is is possible to assume that: $$
\lim_{t \to \infty} t^{s + 1} f(t) = 0
$$ It is possible to apply this transform, or it is a non-sense? I was expecting something like (take this as an example): $$
\hat{f}(s) = \frac{s^2 f''(0) + s f'(0)}{s^2 - s + 1} 
$$ And last but not least, how would I find the inverse transform?",['ordinary-differential-equations']
4618282,Does partial maximization/minimization always give the the same result as global optimization?,"Suppose I want to optimize the function,f(x), where x is a vector. x can be seperated into two sub-vectors, $(x)=(x_{1},x_{2})$ . I can first partially optimize $x_{1}$ , and treat $x_{2}$ as constant, so that I get the optimal $x^{*}_{1}$ as a function of $x_{2}$ .Then I can optimize the object function as a function of $x_{2}, i.e. f(x^{*}_{1}(x_{2}),x_{2})$ . My question is does this always give the same result, as I directly optimize f(x) over x? Can this be proved? The question arised from the maximum likelihood estimation of normal distribution, where $\sigma$ and $\mu$ are all unknown. We always calculate $\hat{\mu}$ first, then express $\hat{\sigma}$ as a function of $\hat{\mu}$ . I want to know does this procedure can be applied to any kind of problem?","['optimization', 'statistics', 'maximum-likelihood']"
4618283,Splitting six friends into two pairs and two singles,"Six friends agree to meet at the hotel Acropolis in Athens. It happens that there are four hotels with the same name. Each of the six friends picks one hotel at random and goes there. What is the probability that two friends end up alone and the rest four in pairs? My solution: Since we have 6 friends and 4 possible choices for each the sample space consists of $4^6$ possible events. There are $\binom{6}{2}$ choices for the first pair and $4$ hotel choices. There are $\binom{4}{2}$ choices for the second pair and $3$ hotel choices. There $\binom{2}{1}$ choices for the first single person and $2$ hotel choices. The last person has only one hotel choice. Since we have four groups and we do not care about order, we have to divide our results by $4!$ . Putting it all together: $$
P(A) = \frac{\frac{4\binom{6}{2}3\binom{4}{2}2\binom{2}{1}1}{4!}}{4^6} \approx 0.0439
$$ The author finds $$
P(A) = \frac{12\binom{6}{2}\binom{4}{2}}{4^6} \approx 0.2637
$$ Who is correct?","['combinatorics', 'probability']"
4618349,Prove that if $\forall x \in \mathbb{R} f''(x) \geq 0$ and $\lim\limits_{x \to \infty} f(x)=0$ then $\forall x \in \mathbb{R} f(x) \geq 0$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question As stated in the title, I need help proving the following: If $\forall x \in \mathbb{R} f''(x) \geq 0$ and $\lim\limits_{x \to \infty} f(x)=0$ then $\forall x \in \mathbb{R} f(x) \geq 0$ I have absolutely no idea as to how to even approach this. I know that $f'$ must be monotone, but I don't see how it helps me. Any help will be appreciated, thanks in advance!","['calculus', 'functions', 'derivatives']"
4618370,Probability of winning a game guessing $k$ random numbers in sequence with optimal strategy,"We are playing a game as follows, suppose we have $k$ spaces. One at a time, we will pick $k$ random integers from the range $[1,n]$ , without replacement. After selecting a number, we must choose to place it in one of our spaces before selecting the next number. We ""win"" if after selecting our $k$ numbers, our sequence on our spaces is in increasing order. Here is an example of a winning game where $n=10, k=3$ . Initial state: _ _ _ Randomly select $5$ , place in middle slot: _ $5$ _ Randomly select $6$ , place in last slot: _ $5$ $6$ Randomly select $1$ , place in first slot: $1$ $5$ $6$ Note that had the last number been a $7,8,9,10$ instead of $1$ , then we would have lost as none of those values are $<5$ . Clearly, if you are just randomly selecting the slot at every iteration, the probability of winning is $\frac{1}{k!}$ . However, we can do better by having good strategy; why place a $1$ in the last slot when thats an instant loss, for example. Define $P(n,k)$ to be the maximum probability of winning the game given $k$ slots and $n$ random numbers. When we say ""maximum probability"" we mean that whenever we select a random number, we put it in the slot where from there we have the highest probability of winning the game. Edit: I have since realized I need more base cases. $P(n,0)=1$ and $P(j,k)=0$ if $j<k$ . $P(j, j)=1$ We will prove our formula recursively with induction. Notice that in our base case, $P(n, 1)=1$ for all values of $n$ . Assume that for all values of $m<n$ and $j<k$ that $P(m, j)$ provides the maximum probability of winning for $j$ slots with selection $[1,m]$ . We now define the formula for the $n,k$ case. We have $n$ options for what we can pick as our first number, for the sake of argument suppose we selected the number $x\in[1,n]$ . With this first number we can now calculate the probability of winning the $n, k$ case, given our first selected number is $x$ . Notate this as $P(n,k;x)$ [^1]. Since there is a $\frac{1}{n}$ chance of selecting any particular $x$ , we know that $$
P(n,k) = \frac{1}{n}\sum_{x=1}^n P(n,k;x)
$$ Now to calculate $P(n,k;x)$ , we must determine the optimal location to place $x$ in the slots. If I was to place $x$ in slot $i$ , I would partition the board into two sections, one of length $i-1$ on the left, and one of length $k-i$ on the right. In order to win, we must select exactly $i-1$ numbers to place on the left that are $<x$ , and exactly $k-i$ numbers on the right that are $>x$ , otherwise we lose as we have placed $x$ in the wrong spot for our selected numbers. The probability of this occuring is $$
\frac{\binom{x-1}{i-1}\binom{n-x}{k-i}}{\binom{n-1}{k-1}}
$$ However, just getting the correct numbers is not enough, as we must put them in the correct locations, but these partitioned boards are equivilent to playing the same game twice! So if we get valid numbers (per above) we only need to win the partitioned boards in order to win the game. Since $x-1, n-x < n$ and $i-1, k-i<k$ we can use our inductive hypothesis to say that the probability of winning if we place $x$ in slot $i$ is $$
\frac{\binom{x-1}{i-1}\binom{n-x}{k-i}}{\binom{n-1}{k-1}}P(x-1, i-1)P(n-x, k-i).
$$ In order to optimize our strategy, we select the $i$ for which this value is maximized which gives us $$
P(n,k;x)=\max_{1\leq i\leq k}\left[\frac{\binom{x-1}{i-1}\binom{n-x}{k-i}}{\binom{n-1}{k-1}}P(x-1, i-1)P(n-x, k-i)\right]
$$ Which means we can sum over all values of $x$ and divide by $n$ to get the value of $P(n,k)$ Q.E.D. This recursive formula was also not too tough to program up, here is a plot of when $n=50$ , notice that the y axis is a log plot so the variations can be seen. Minimal solution is when $k=35$ with $P(50,35)\approx 3.28\cdot 10^{-5}$ $n=50$ "" /> As an aside, has anyone seen this before? I was thinking of writing this into a blog post for my students and if this is a known thing than I'd like to reference a paper on it [^1]: I understand my notation is starting to get a bit garbo while writing this down","['combinations', 'solution-verification', 'combinatorics', 'probability']"
4618392,Simplifying $\frac {\cos^4 x}{\cos^2 y}Ôºã\frac {\sin^4 x}{\sin^2 y}Ôºù1$,How do you simplify the following? $$\frac {\cos^4(x)}{\cos^2(y)}Ôºã\frac {\sin^4(x)}{\sin^2(y)}Ôºù1$$ What I've tried: $$\frac {\cos^4(x)}{\cos^2(y)}Ôºã\frac {\sin^4(x)}{\sin^2(y)}Ôºù1$$ $$\sin^2(x)+\cos^2(x)=1$$ $$\implies\frac {\cos^4(x)}{\cos^2(y)}Ôºã\frac {\sin^4(x)}{\sin^2(y)}Ôºù\sin^2(x)+\cos^2(x)$$ $$\implies \frac {\cos^4(x)}{\cos^2(y)}Ôºç\cos^2(x)Ôºù\sin^2(x)Ôºç\frac {\sin^4(x)}{\sin^2(y)}\\~\\\implies \frac {\cos^4(x)Ôºç\cos^2(x)\cos^2(y)}{\cos^2(y)}Ôºù\frac {\sin^2(x)\sin^2(y)Ôºç\sin^4(x)}{\sin^2(y)}\\~\\\implies \frac {\cos^2(x)}{\cos^2(y)}\left(\cos^2(x)Ôºç\cos^2(y)\right)Ôºù\frac {\sin^2(x)}{\sin^2(y)}\left(\sin^2(y)Ôºç\sin^2(x)\right)$$,['trigonometry']
4618425,"$\min_{x\in\mathbb{R}^n}(\langle z,x-y\rangle+\lVert x-y\rVert^2)=\min_{r\ge0}(-r\lVert z\rVert+r^2)$","Let $y,z\in\mathbb{R}^n$ . Then $\min_{x\in\mathbb{R}^n}(\langle z,x-y\rangle+\lVert x-y\rVert^2)=\min_{r\ge0}(-r\lVert z\rVert+r^2)$ . Proof) $-r\lVert z\rVert+r^2$ has the minimum at $r=\frac{\lVert z\rVert}{2}$ . Take $x_1$ such that $\lVert x_1-y\rVert=\frac{\lVert z\rVert}{2}$ . Then for any $x\in\mathbb{R}^n$ , $$ -\lVert x_1-y\rVert\lVert z\rVert+\lVert x_1-y\rVert^2\le -\lVert x-y\rVert\lVert z\rVert+\lVert x-y\rVert^2\le \langle z,x-y\rangle+\lVert x-y\rVert^2.$$ So $\min_{x\in\mathbb{R}^n}(\langle z,x-y\rangle+\lVert x-y\rVert^2)\ge\min_{r\ge0}(-r\lVert z\rVert+r^2)$ . How do I prove the converse?","['inner-products', 'multivariable-calculus', 'linear-algebra', 'real-analysis']"
4618448,"Are the ""bounded, uniformly continuous functions"" analogous to test functions as seen in Schwartz Distributions?","I am reading this passage from Billingsley's Convergence of Probability measures. Theorem 1.2. Probability measures $P$ and $Q$ on $\mathcal{S}$ coincide if $P f=Q f$ for all bounded, uniformly continuous real functions $f$ . Proof. For the bounded, uniformly continuous $f$ of (1.1), $P F \leq$ $P f=Q f \leq Q F^\epsilon$ . Letting $\epsilon \downarrow 0$ gives $P F \leq Q F$ , provided $F$ is closed. By symmetry and Theorem $1.1, P=Q$ . Because of theorems like this, it is possible to work with measures $P A$ or with integrals $P f$ , whichever is simpler or more natural. We defined weak convergence in terms of the convergence of integrals of functions, and in the next section we characterize it in terms of the convergence of measures of sets. Are the ""bounded, uniformly continuous"" analagous to the ""test functions"" $C_c^\infty(X)$ seen in the ""theory of distributions? Is this describing that we can think of probability measures as ""distributions"" or ""measures"" by the reisz representation theorem? Why do they use ""bounded, uniformly continuous"" real functions instead of infinitely differentiable continuous functions with compact support as used in Schwartz distributions? https://en.wikipedia.org/wiki/Distribution_(mathematics)","['probability-distributions', 'analysis', 'stochastic-processes', 'functional-analysis', 'probability']"
4618458,Basic questions in Real Analysis,"I started studying Real Analysis 3 and stumbled on the new definition of differentiability, approximation by line/plane. It seems easy to digest but I have some questions about it. Is $f(x,y)=x$ differentiable at $(0,0)$ ? I think it should be differentiable but I keep getting, it's not differentiable: $f(h_a,h_b)=f(0,0)+\alpha0+\beta0+g(0,0,h_a,h_b)$ gives that $g(0,0,h_a,h_b)=h_a$ and $g(0,0,h_a,h_b)$ divided by norm of $h$ approaches $\frac{1}{\sqrt{2}}$ if I take $h_a=h_b$ . Here, $g(a,b,h_a,h_b)$ is the error term and $f(a,b)+\alpha h_a+\beta h_b$ represents my plane for approximating the value of $f(a+h_a,b+h_b)$ . Is it not differentiable? --- This has been answered. Another question, let's say the partial derivative of a function w.r.t. $x$ and $y$ is $0$ . Based on my knowledge and physics background, I see that function is not changing along the $x$ and $y$ -axis. Doesn't it imply our particle at origin is not moving at all? If it changes in any direction then it must have components along the $x$ and $y$ -axis but there are no changes in those directions. Doesn't it imply that the function must be differential at the origin if partial derivatives are $0$ ? I know, there exist some continuous functions with partial derivatives $0$ at origin and are non-differentiable.","['multivariable-calculus', 'real-analysis']"
4618501,Double integral exercise - check answer,"I need to calculate $$\mathop{\iint}_{R} (x^2 + y^2) \,dx \,dy $$ where $R$ is the region (in the plane) bounded by the lines $y=x$ , $x=2$ and the hyperbola $xy=1$ . To calculate this integral I broke $R$ into two regions. $R_1:0 \le x \le 1,   0 \le y \le x$ $R_2:1 \le x \le 2,   0 \le y \le 1/x$ And I calculated the integral of $(x^2 + y^2)$ over both of them. At the end I got this answer 47/24
but my book says the answer is 27/8 Which one is correct?","['integration', 'multivariable-calculus', 'calculus']"
4618520,"If $f:R\to R, f(x)=8x^3+3x$, then find $\lim_{x\to 0} \frac{f^{-1}(8x)-f^{-1}(x)}{x}$","This relates to a previous MSE question involving an asymptotic limit: If $f(x)=8x^3+3x$ , $x\in\mathbb{R}$, how do I find $\lim_{x \to \infty}\frac {f^{-1}(8x)-f^{-1}(x)}{x^{1/3}}$? Here, we seek the near-zero limit of the same expression. So the new question is $f:R\to R, f(x)=8x^3+3x$ , then find $$\lim_{x\to 0} \frac{f^{-1}(8x)-f^{-1}(x)}{x}.$$ Near zero $3x$ term ,so we take $f(x)\sim 3x \implies f^{-1}(x)\sim\frac{x}{3}.
$ Hence, $$\lim_{x\to 0} \frac{f^{-1}(8x)-f^{-1}(x)}{x}=\lim_{x\to 0} \frac{8x/3-x/3}{x}=\frac{7}{3}.$$ The question here is question is whether the problem and solution are correct and what could be alternate ways of solving it? EDIT: More rigorously as per the suggestion of @Paramanand Singh, notet that if $y=f(x)$ and inverse exists then $$\frac{d f^{-1}(y)}{dy}|_{y=y_0}=\frac{1}{f(x_0)}, y_0=f(x_0)$$ This limit can be seen as due to L-Hospital, wherein we can use $(f^{-1})'(8x)=\frac{8}{24x^2+3}=\frac{1}{3}$ , when ${x \to 0}$","['limits', 'calculus', 'functions']"
4618529,Uniform Choice Functions and Naturality,"Some choice functions can be specified explicitly, while in other cases no definite choice function is known. An example of the former is a choice function for non-empty subsets of natural numbers, where one can always pick the smallest element. An example of the latter is a choice function for non-empty subsets of real numbers. A common intuition is that these explicit choice functions have uniform definitions (or at least the number of non-uniform cases must be limited). This is reminiscent of natural transformations, which can capture the idea of a morphism defined uniformly with respect to objects in a category. Is there a way to formalize the uniformity intuition for definite choice functions? Specifically, is there some invariance/coherence condition the ""pick the minimum"" on the naturals satisfies that no choice function on the reals could satisfy?","['elementary-set-theory', 'axiom-of-choice', 'natural-transformations', 'category-theory']"
4618535,Is a group with transitive automorphisms necessarily a vector space?,"Let $G$ be a group with transitive automorphisms on $G-\{e\}$ . I.e. for any $a,b\neq e$ in $G$ , there exists some $f \in \operatorname{Aut}(G)$ such that $f(a) = b$ . Is it then necessarily the case, that $G$ is a vector space (over $\mathbb F_p$ or $\mathbb Q$ )? If $G$ is finite, this is the case (cf. Groups with transitive automorphisms ) but I don‚Äòt know how to prove it for infinite $G$ or how to construct a counterxample. I can show that either $k=\mathbb F_p$ or $k=\mathbb Q$ embeds into $G$ , but then I am stuck. I tried constructing a maximal embedding $k^I \to G$ of some vector space into $G$ via Zorn‚Äôs lemma, but given two embeddings $f,g \colon k \to G$ with disjoint image I can‚Äôt show whether $f \times g $ defines an embedding of $k^2$ into $G$ and this is the crucial step missing in applying Zorn‚Äôs lemma. This argument would work, if we assumed $G$ to be abelian of course, but I think we don‚Äôt need that assumption.","['group-theory', 'vector-spaces']"
4618592,"Positive real numbers $x_1,x_2,\ldots,x_n$ satisfy $\sum_{i=1}^{n}{x_i}=n$ , prove: $\displaystyle\sum_{sym}{\prod_{i=1}^{n}{x_i^i}}\leqslant n!$","If positive real numbers $x_1,x_2,\ldots,x_n$ satisfy $\displaystyle\sum_{i=1}^{n}{x_i}=n$ , prove or falsify: $$\sum_{\text{sym}}{\prod_{i=1}^{n}{x_i^i}}\leqslant n!$$ Here I'll explain the notation 'sym', $$
\sum_\text{sym}{f(x_1,x_2,\ldots,x_n)=\sum_{\sigma\in S_n}{f(x_{\sigma(1)},x_{\sigma(2)},\ldots,x_{\sigma(n)})}}
$$ where $S_n$ is the permutation group of degree $n$ . For example, $$
\sum_{\text{sym}}{x^3y^2z}=x^3y^2z+x^3yz^2+x^2y^3z+x^2yz^3+xy^3z^2+xy^2z^3
$$ The above example corresponds to the LHS of the case $n=3$ of this problem. I haven't push forward this question much, all I can prove is the case $n=2$ , which is immediately true by AM-GM. For $n=3$ , I tried to homogenise the inequality and use the 'SOS' method(Sum Of Squares), but it's apparent that this attempt can't be generalised. I've managed to solved the $n=3$ case by the $pqr$ method. First, note the identity $$
\sum_{\text{sym}}{a^3b^2c}=abc(a+b+c)(ab+bc+ca)-3(abc)^2
$$ Let $p:=a+b+c,~q:=ab+bc+ca,~r:=abc$ , the inequality is equivalent to $$
pqr-3r^2\leqslant 6
$$ By Schur's Inequality of degree 3, we have $$
a(a-b)(a-c)+b(b-a)(b-c)+c(c-a)(c-b)=p^3-4pq+9r\geqslant0
$$ From this we may obtain $q\leqslant(p^3+9r)/(4p)$ , and as constrained we have $p=3$ ,these reduce the inequality to $$
3r^2-3r~\left(\frac{27+9r}{12}\right)+6=3(r-1)(r-8)\geqslant0
$$ By AM-GM, we obtain $r\leqslant(p/3)^3=1$ ,which proves the inequality. Here's a proof on the site 'zhihu', I've reposted it as community wiki. This proof is invalid because its lemma is wrong. Thanks to Sangchul Lee, this inequality is not true for $n\geqslant6$ , which means the cases $n=4$ and $n=5$ might be very difficult to prove.","['algebra-precalculus', 'inequality']"
4618600,Unbiased estimator of a complex function,"Let $X_1, X_2 \cdots X_N$ be random variables, which follow a Gaussian distribution. \begin{equation}
X \sim N(\mu, \sigma^2)
\end{equation} Let the parameters $\mu$ and $\sigma^2$ be unknown.
I know that the unbiased estimator of $\mu$ is \begin{equation}
\bar{X}=\frac{1}{N}(X_1+X_2 \cdots X_N)
\end{equation} and that of $\sigma^2$ is \begin{equation}
U = \frac{1}{N-1}\left((X_1 -\bar{X})^2 \cdots (X_N -\bar{X})^2\right)
\end{equation} Here, a new parameter $f$ is defined as \begin{equation}
f = \frac{1}{\sigma^2 + \sigma_t^2} 
\exp\left(-\frac{1}{2}(\mu -\mu_t)^2(\sigma^2 + \sigma_t^2)^{-1}\right)
\end{equation} , where $\mu_t$ and $\sigma_t^2$ are known parameters. I'm not sure how to obtain the unbiased estimator of $f$ . The simplest case is when $\mu$ is a known parameter and $\mu = \mu_t$ .
In this case, \begin{equation}
f = \frac{1}{\sigma^2 + \sigma_t^2}
\end{equation} and I'd like to calculate the mean value of $\tilde{F}=\frac{1}{U + \sigma_t^2}$ \begin{equation}
\begin{split}
E\left[\tilde{F} \right]&= E\left[\frac{1}{U + \sigma_t^2}\right] \\
&=  E\left[\frac{N-1}{\sigma^2 Y +  (N-1)\sigma_t^2}\right] \quad (Y \sim \chi_{N-1}^2) \\
&= (N-1) \int_0^{\infty}
\frac{1}{\sigma^2 y + (N-1)\sigma_t^2}
\frac{y^{\frac{N-1}{2}-1}e^{-\frac{y}{2}}}
{2^{\frac{N-1}{2}} \Gamma(\frac{N-1}{2})} dy
\end{split}
\end{equation} How can I calculate this value?
So, my question is how to obtain the unbiased estimator of $f$ how to calculate $E[\tilde{F}]$ Either one or the other answer is fine. I faced this problem when I was studying quantum mechanics. Gaussian quantum states can be represented by Gaussian distributions of operators of $\hat{x}$ and $\hat{p}$ . A vector of the mean values of these operators can be defined as $\boldsymbol{m} = (\left<\hat{x}\right>, \left< \hat{p}\right>)^{\top}$ and the covariance matrix can be defined as \begin{equation}
V = 
\begin{pmatrix}
(\Delta \hat{x})^2 & \Delta \hat{x} \Delta \hat{p} \\
\Delta \hat{p} \Delta \hat{x} & (\Delta \hat{p})^2
\end{pmatrix}
\end{equation} Fidelity $f$ , which is a measure of closeness between two quantum states, can be written as \begin{equation}
\begin{split}
f&=
\frac{\hbar}{\sqrt{\operatorname{det}\left(V_1+V_2\right)}} \exp \left[-\frac{1}{2}\left(\boldsymbol{m}_1-\boldsymbol{m}_2\right)^\top\left(V_1+V_2\right)^{-1}\left(\boldsymbol{m}_1-\boldsymbol{m}_2\right)\right] \\
&0 \le f \le 1
\end{split}
\end{equation} If the two states are the same ( $\boldsymbol{m}_1 = \boldsymbol{m}_2$ , $V_1 = V_2$ ), the fidelity is 1 since $\operatorname{det} V=(\frac{\hbar}{2})^2$ .
The Gaussian states can be defined using two parameters $\hat{x}$ and $\hat{p}$ , but the question is changed to one parameter in order to make the problem simpler.","['statistics', 'parameter-estimation']"
4618607,"Frame challenge: Find the maximum $n$ such that circles of radius $1, \frac12, \frac13, ..., \frac1n$ can be held immobile by a convex frame.","Find the maximum $n$ such that circles of radius $1, \frac12, \frac13, ..., \frac1n$ can be held immobile by a convex frame, or show that there is no maximum. Here is an example with $n=7$ . By ""immobile"", I mean no circle can move without overlapping other circles or the frame, either individually or simultaneously . The frame is rigid. It seems to get increasingly difficult to keep adding circles (and expanding the frame), while maintaining the conditions that the circles are immobile and the frame is convex. Or maybe there is a clever way to arrange the circles so that you can include all of them. (This question was inspired by What is the minimum area of a rectangle containing all circles of radius $1/n$ ? )","['discrete-geometry', 'convex-geometry', 'circles', 'geometry', 'packing-problem']"
4618609,"How to prove that $\mathbb{E}[ \varphi(X, Y) | \mathcal{G}] = \psi(Y)$ where $\psi(y) := \mathbb{E}[ \varphi(X, y)]$?","$\newcommand{\diff}{\mathrm d}$ I'm trying to prove Proposition 12.4. (given without proof) in this note. Let $\mathcal{G}$ be a sub- $\sigma$ -field of $\mathcal{F}$ and $X, Y$ two random variables such that $X$ is independent of $\mathcal{G}$ and $Y$ is $\mathcal{G}$ -measurable. Let $\varphi: \mathbb{R}^2 \rightarrow \mathbb{R}$ be Borel-measurable such that $\mathbb{E}[|\varphi(X, Y)|] < \infty$ . Then $$
\mathbb{E}[ \varphi(X, Y) | \mathcal{G}] = \psi(Y) \quad \text { a.s.} \quad \text{where} \quad \psi(y) := \mathbb{E}[ \varphi(X, y)].
$$ In below attempt, I'm stuck at showing $$
\int_\mathbb R \left[ \int_\mathbb R \varphi (x,y) \diff \color{blue}{\mu'}(x) \right ] \mathrm d \nu'(y) = \int_\mathbb R \left[ \int_\mathbb R \varphi (x,y) \diff \color{blue}{\mu}(x) \right ] \mathrm d \nu'(y).
$$ Could you elaborate on how to finish the proof? Proof Let $\mu, \nu$ be the distributions under $\mathbb P$ of $X, Y$ respectively. We have $X, Y$ are independent and thus the distribution $\lambda$ of $(X, Y)$ is the product measure of $\mu$ and $\nu$ , i.e., $\lambda = \mu \otimes \nu$ . By Fubini's theorem, $\varphi(X, y)$ is integrable for $\nu$ -a.e. $y\in Y$ and the map $y \mapsto \mathbb{E}[ \varphi(X, y)]$ is Borel. Clearly, $\psi(Y)$ is $\mathcal G$ -measurable. Fix $A \in \mathcal G$ . Let's prove that $$
\int_A \varphi (X, Y) \diff \mathbb P = \int_A \psi(Y) \diff \mathbb P.
$$ Let $\mathcal G'$ be the sub- $\sigma$ -algebra of $\mathcal G$ induced by $A$ and $\mathbb P'$ the restriction of $\mathbb P$ to $\mathcal G'$ . We now consider integration w.r.t. $(A, \mathcal G', \mathbb P')$ . Clearly, $$
\int_A \varphi(X, Y) \diff \mathbb P = \int_A \varphi(X, Y) \diff \mathbb P'
\quad \text{and} \quad
\int_A \psi(Y) \diff \mathbb P = \int_A \psi(Y) \diff \mathbb P'.
$$ Notice that $X,Y$ are still independent under $(A, \mathcal G', \mathbb P')$ . Let $\mu', \nu'$ be the distributions under $\mathbb P'$ of $X, Y$ respectively. Then the distribution of $(X, Y)$ under $\mathbb P'$ is $\lambda' :=\mu' \otimes \nu'$ . By change of variables formula and Fubini's theorem, $$
\int_A \varphi (X, Y) \diff \mathbb P' = \int_{\mathbb R^2} \varphi (x,y) \diff \lambda(x, y) = \int_\mathbb R \int_\mathbb R \varphi (x,y) \diff \mu'(x) \diff \nu'(y).
$$ By change of variables formula, $$
\int_A \psi(Y) \diff \mathbb P' = \int_\mathbb R \psi(y) \diff \mathbb \nu'(y) = \int_\mathbb R \left[ \int_\Omega \varphi (X,y) \diff \mathbb P \right ] \mathrm d \nu'(y) = \int_\mathbb R \left[ \int_\mathbb R \varphi (x,y) \diff \mu(x) \right ] \mathrm d \nu'(y).
$$","['conditional-expectation', 'measure-theory', 'probability-theory']"
4618610,Permutation analog of a character table,"I‚Äôm trying to learn a little bit about representation theory because it sometimes comes up in discussions in some math hobby groups that I‚Äôm in. I have no background in representation theory and my question is really, really naive. I don‚Äôt know how to compute a character table for a small, easy-to-work-with group by hand. Is there an analog to a character table for permutation representations and is it simpler to compute by hand? Richard Borcherds‚Äô first lecture on representation theory introduces some of the basic concepts and constructs a character table for $S_3$ at 20m25s . $$
  \begin{array}{lccc}
    & \begin{matrix} \\ \\ \mathrm{id} \end{matrix}
    & \begin{matrix} (12) \\[-0.2em] (23) \\[-0.2em] (31) \end{matrix}
    & \begin{matrix} \\ (123) \\[-0.2em] (132) \end{matrix} \\[0.5em]
    \hline
    \chi_1 & 1 & \phantom{-}1 & \phantom{-}1 \\
    \chi_2 & 1 & -1 & \phantom{-}1 \\
    \chi_3 & 2 & \phantom{-}0 & -1
  \end{array}
$$ And then he says some facts about this table that I find absolutely amazing: It‚Äôs square. The columns are orthogonal. The rows are orthogonal (when weighted by conjugacy class size). I didn‚Äôt get this at first, but $\chi_3$ corresponds to the standard representation , which only costs you two dimensions in $\mathbb{C}$ to represent (as opposed to 3 if you do it naively by translating permutations into matrices). What follows is my attempt to build a permutation character table for $S_3$ . Wikipedia‚Äôs entry on character tables mentions that the table itself encodes information about irreducible representations over $\mathbb{C}$ . However, you can take traces of matrices in any vector space , this makes me wonder whether it is possible to construct something analogous to this table for permutation representations of $S_3$ . This would be a binary matrix, since traces would give us $1$ or $0$ and would count the parity of the number of fixed points. I‚Äôll call my new pseudo-characters $\psi_1, \psi_2, \dotsc$ We have the identity pseudocharacter $\psi_1$ , which sends everything to $1$ , associated with the trivial permutation representation. We have the representation that sends elements of the transposition conjugacy class to $\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$ and everything else to the identity matrix. This representation is associated with a pseudocharacter $\psi_2$ which sends everything to $0$ because it‚Äôs two-dimensional and two is even. I guess this thing is irreducible as a permutation representation, but I don‚Äôt know how to prove it one way or the other. It certainly is not irreducible as a complex representation, though, since it isn‚Äôt listed in the above character table. And finally we have the faithful permutation representation, which we use in the above table to name the elements of the conjugacy classes of $S_3$ above. The identity permutation has three fixed points, a transposition has one, and derangements have no fixed points, so $\psi_3$ sends the identity and the transpositions to $1$ and the others to $2$ . We end up with the following table, which is much less nice. $$
  \begin{array}{lccc}
    & \begin{matrix} \\ \\ \mathrm{id} \end{matrix}
    & \begin{matrix} (12) \\[-0.2em] (23) \\[-0.2em] (31) \end{matrix}
    & \begin{matrix} \\ (123) \\[-0.2em] (132) \end{matrix} \\[0.5em]
    \hline
    \psi_1 & 1 & 1 & 1 \\
    \psi_2 & 0 & 0 & 0 \\
    \psi_3 & 1 & 1 & 0
  \end{array}
$$ We can also count the number of fixed points instead, which is also invariant under conjugation. $$
  \begin{array}{lccc}
    & \begin{matrix} \\ \\ \mathrm{id} \end{matrix}
    & \begin{matrix} (12) \\[-0.2em] (23) \\[-0.2em] (31) \end{matrix}
    & \begin{matrix} \\ (123) \\[-0.2em] (132) \end{matrix} \\[0.5em]
    \hline
    \psi_1 & 1 & 1 & 1 \\
    \psi_2 & 2 & 0 & 2 \\
    \psi_3 & 3 & 1 & 0
  \end{array}
$$ Anyway, since our group $S_3$ has three normal subgroups and we are effectively counting the number of fixed points in permutation representations of quotient groups of $S_3$ , I think we can use the above table to tell us what kinds of numbers of fixed points for each conjugacy class are possible in any permutation representation of $S_3$ , for example $\begin{bmatrix} 4 & 2 & 1 \end{bmatrix}$ is possible if we act on three elements standardly and leave a fourth untouched.","['group-theory', 'abstract-algebra', 'representation-theory', 'characters']"
4618727,Determine the exponential generating function $A$ and deduce an expression for $a_n$,"Let $a_n$ satisfy the recursion $a_n + na_{n‚àí1} = n!$ $\forall$ $n$ $\in$ $\mathbb{N}$ ‚à™ {0}. Determine the exponential generating function $A$ and deduce an expression for $a_n$ . So we start with $a_n = -na_{n‚àí1} + n!$ and the fact that $A(n) = \sum_{n=0}^\infty a_n \frac{x^n}{n!}$ . Then, $A(n) = \sum_{n=0}^\infty (-na_{n-1} + n!) \frac{x^n}{n!}$ gives $A(n) = \sum_{n=0}^\infty -na_{n-1} \frac{x^n}{n!} + \sum_{n=0}^\infty x^n $ However, I am not sure how to proceed. Any help would be grateful.","['recurrence-relations', 'discrete-mathematics', 'generating-functions']"
4618734,How to approximate a parameter that gives a tangent line to three circles?,"Three disks are placed on the ground like this: From left to right, their radii are $\frac{1}{x-1}, \frac{1}{x}, \frac{1}{x+1}$ metres. They lie in a plane perpendicular to the ground. The middle disk touches the other two disks. Using only paper and pen, approximate the value of $x$ such that the middle disk is tangent to the line that is tangent to and above the other two disks. You may assume that the earth is a sphere of radius $R$ metres. (Before you read the last sentence, it seems like there's something wrong with the question, because it seems like the middle disk should never touch the line. But the ground is actually a circular arc of the earth, so the middle disk is ""pushed up"" and touches the line for some value of $x$ .) The answer turns out to be, elegantly, $x\approx R/2$ . But the algebra seems to be horrendous and I needed to use my computer to find the answer. My attempt Call the angles at the centre of the middle disk $A, B, C, D, E$ with $A$ at the lower-left and going clockwise. $A=\arccos{\left(\dfrac{\left(\frac{1}{x}+\frac{1}{x-1}\right)^2+\left(R+\frac{1}{x}\right)^2-\left(R+\frac{1}{x-1}\right)^2}{2\left(\frac{1}{x}+\frac{1}{x-1}\right)\left(R+\frac{1}{x}\right)}\right)}$ $B=\arcsin{\left(\dfrac{\frac{1}{x-1}-\frac{1}{x}}{\frac{1}{x-1}+\frac{1}{x}}\right)}$ $C=\dfrac{\pi}{2}$ $D=\arccos{\left(\dfrac{\frac{1}{x}-\frac{1}{x+1}}{\frac{1}{x}+\frac{1}{x+1}}\right)}$ $E=\arccos{\left(\dfrac{\left(\frac{1}{x}+\frac{1}{x+1}\right)^2+\left(R+\frac{1}{x}\right)^2-\left(R+\frac{1}{x+1}\right)^2}{2\left(\frac{1}{x}+\frac{1}{x+1}\right)\left(R+\frac{1}{x}\right)}\right)}$ We assume that the middle disk is tangent to the line that is tangent to and above the other two disks. This implies: $$A+B+C+D+E=2\pi$$ I am utterly unable to approximate $x$ without a computer, even after attempting to simplify it. And yet the computer-assisted answer is just $x\approx R/2$ . Can $x$ be approximated without a computer? (This question was inspired by a frame challenge .)","['approximation', 'circles', 'geometry']"
4618757,Find $x$ such that $\sqrt{x+1} - \sqrt{1-x} = 1$,"To solve this equation, I started by putting the condition $x\in [-1, 1]$ , then squared a few times: $\sqrt{x+1} - \sqrt{1-x} = 1 \iff x + 1 +1-x-2\sqrt{1-x^2} =1 \iff 2\sqrt{1-x^2}=1 \iff 4(1-x^2)=1 \iff 4x^2=3 \iff x=\pm \frac{\sqrt{3}}{2}$ This, however, is not the right solution, as $-\frac{\sqrt{3}}{2}$ returns $-1$ , not $1$ . My question is where did I miss a condition that excludes the negative ""solution""? I expect somewhere along the line I squared where I wasn't allowed to square without an additional condition, hoping that I don't have to check these solutions every time.","['algebra-precalculus', 'solution-verification']"
4618813,Convergence of series $\sum_{n \geq 1} {\frac{1^2+2^2+ \cdots + n^2}{n^4}}$,"In the study of the following series $$
\sum_{n \geq 1} {\frac{1^2+2^2+ \cdots + n^2}{n^p}}
$$ it is not hard to prove that it diverges for $p \leq 3$ , since the sequence itself does not converge to 0. You can also conclude that the series converges for $p > 4$ by comparison with Riemann series. Raabe's test yields that the series diverges for p between 3 and 4. However it does not give any information for the case $p=4$ .","['convergence-divergence', 'calculus', 'sequences-and-series', 'real-analysis']"
4618851,Unbiased estimator of binomially distributed random variables,"Let be $X_1, X_2\dots X_n$ independent binomially distributed random variables with probability $p$ and length $m$ . We denote $P(X_i=k)$ , the probability of $k$ sucesses, by $b(k;m,p)$ . Both parameters $p$ and $m$ are unknown. We know that $X_{(n)}=\max(X_1,X_2,...,X_n)$ is a consistent estimator for length $m$ (see https://math.stackexchange.com/a/4618523/579544 ). Is $X_{(n)}$ unbiased? My approach: For sake of simplicity I assume that $n=3$ and $X_{(3)}=\max(X_1,X_2,X_3)=k$ . By inclusion-exclusion principle we see that \begin{align*}
&P(X_{(n)}=k)=P(\{X_1=k\}\cup \{X_2=k\}\cup \{X_3=k\})\\
&=P(X_1=k)+P(X_2=k)+P(X_3=k)\\
&-P(\{X_1=k\}\cap \{X_2=k\})-P(\{X_1=k\}\cap \{X_3=k\})-P(\{X_2=k\}\cap \{X_3=k\})\\
&+P\left(\{X_1=k\}\cap \{X_2=k\}\cap \{X_3=k\}\right)\\
&=3\cdot b(k;m,p)-3\cdot b(k;m,p)^2+b(k;m,p)^3.
\end{align*} So the expected value $\mathbb{E}(X_{(n)})$ can be calculated by \begin{align*}
\mathbb{E}(X_{(n)})=\sum\limits_{m=k}^{\infty}3m\cdot b(k;m,p)-3m\cdot b(k;m,p)^2+m b(k;m,p)^3.
\end{align*} I don't know how to proceed/manipulate the series to get any further results? Is this a valid approach at all? Or is there a different/easier one? Maybe the expected value doesn't exist at all?","['expected-value', 'statistics', 'parameter-estimation']"
4618881,Cohomology of submanifolds,"Suppose I have a manifold $M$ and a submanifold or a boundary $N\subset M$ . By the natural inclusion $\iota:N\hookrightarrow M$ we can easily see that $$\omega\in\mathrm{H}^k(M) \quad\implies\quad \iota^*\omega\in\mathrm{H}^k(N).$$ On the other hand, obviously $$\iota^*\omega\in\mathrm{H}^k(N) \quad\not\kern{-0.5em}\implies\quad \omega\in\mathrm{H}^k(M).$$ Is there something to be said about those forms that are in the cohomology of $N$ but not in that of $M$ ? Precisely, does either the space $$\mathrm{Hmm}_{(1)}^k(N,M):=\left\lbrace\omega\in\mathrm{H}^k(N)\ \middle|\ \omega\neq\iota^*\eta,\quad \eta\in\mathrm{H}^k(M)\right\rbrace$$ or the space $$ \mathrm{Hmm}_{(2)}^k(N,M) := \mathrm{H}^k(N)\Big/\left\lbrace\omega\in\mathrm{H}^k(N)\ \middle|\ \omega=\iota^*\eta,\quad \eta\in\mathrm{H}^k(M)\right\rbrace$$ have a simple description? 1 I have a hunch that it should come from some relative cohomology but I wasn't able to make it precise. Even if there is no simple description, can we say something about its dimension? I am more interested in the case where $N=\partial M$ (so if there's something to be said there but not in the general case, I'm perfectly happy), but the general case seems interesting too. 1 I'm writing both spaces to maximize my chances, see the helpful comments of @Thorgott and @Osama Ghani","['submanifold', 'de-rham-cohomology', 'manifolds-with-boundary', 'homology-cohomology', 'differential-geometry']"
4618900,Can rationals be approximated by increasingly large-denominator rationals?,"Lets denote by $A_n$ the set of all relatively prime fractions with denominator $n\,.$ Then one should observe that the members of $A_n$ become more densely populated in the real line as $n \to \infty$ . To see this, note that $$A_1 = \mathbb{Z}, A_2 = \Big\{\frac{1}{2}, \frac{3}{2}, \dots,\Big\},\dots,A_{56} = \Big\{\frac{1}{56},\frac{3}{56}, \frac{5}{56}, \dots\Big\},\; \dots$$ Thus, it would seem intuitively true that the statement I have proposed is true, but seeing how I could find $N \in \mathbb{N}$ so that this is true is elusive to me. Any suggestions here?","['elementary-number-theory', 'real-analysis']"
4618922,I think my school got a probability question wrong.,"An Olympic final consists of two runners from each of four different
countries. An ordered selection of three of the eight runners win the
medals (gold, silver, and bronze). How many different possibilities are
there if the medal winners are all from different countries?# They said its 192 (8x6x4) but I think its 192 * 3 since it's an ordered selection no? Edit: My confusion came from a different variation of the question. How many different possibilities are
there if two of the medal winners are from the same country? The answer for this was 144 and to my knowledge because of 8x1x6 and x3 for the order","['permutations', 'combinatorics']"
4618963,How to find variance in Poisson?,The number of ice creams sold per hour from Mr Fishy‚Äôs van is observed to be a Poisson random variable with parameter ùúÜ=8. Each ice cream costs two pounds but Mr Fishy has to pay five pounds per hour for the pitch. What is the variance of his hourly profit (measured in pounds and ignoring all other income and expenses)?,"['statistics', 'poisson-distribution']"
4618979,Are there any perfect cubes form of $611\dots1$?,"Can $6111\cdots111$ be a perfect cube? I think it can be since a perfect cube can end in $2$ or more odd digits (like $456533$ ) and $611\cdots111$ is $7 \pmod 8$ , which is one of the cubic residues of $8$ . Also, $611\dots1$ is sometimes $0,1,8\pmod{9}$ . Can $611\cdots111$ be a perfect cube? If not, what is the proof?","['number-theory', 'discrete-mathematics', 'elementary-number-theory']"
4618993,Find the function $f(x)$ and $c$,"Let $ \mathbb{N}$ denote the set of all positive integers. Find all real numbers $c$ for which there
exists a function $f : \mathbb{N} ‚Üí \mathbb{N}$ satisfying: for any $x, a ‚àà \mathbb{N}$ , the quantity $\frac{f(x+a)-f(x)}{a} $ is an integer if and only if $a = 1$ ; for all $x ‚àà \mathbb{N}$ , we have $|f(x) ‚àí cx| < 2023$ . I don't know why but I'm getting feelings that, $f(x+a)-f(x)‚â°a-1 \pmod {a}$ $f(x+a)-f(x) =ax+a-1$ Which gives $$f(x) = \frac{x(x-1)}{2}+c$$ At $a=1$ But that's not the solution. Could someone help me figure it out.","['contest-math', 'functional-equations', 'functions']"
4619007,"show that $|\langle b,v\rangle|\leq \frac{1}{2\alpha}\|b\|^2 +\frac{\alpha}{2}\|v\|^2$.","Let H be hilbert space $A:H\to H$ and $J:H\to \mathbb R$ $A$ symetric $\langle Ax,x\rangle=\langle x,Ax\rangle$ and coercive $\exists \alpha >0$ such that $\langle Ax,x\rangle\ge \alpha \|x\|$ let $b\in H$ s.t we have $J(v)=\frac 12 \langle Ax,x\rangle -\langle b,v\rangle$ Show that $J$ is continuous and show that $|\langle b,v\rangle|\leq \frac{1}{2\alpha}\|b\|^2 +\frac{\alpha}{2}\|v\|^2$ . I proved that $J$ is bounded from below so continuous ? and I dont know how to prove the second question.","['hilbert-spaces', 'functional-analysis']"
4619009,Maximum likelihood estimator for non-standard distribution,"This is a question from an old exam-paper: ""Suppose that we have data $y = (y_1, . . . , y_n)$ . Each data-point $y_i$ is assumed to be generated by a
distribution with the following probability density function: $$p(y_i|\theta)=\frac{\theta^2}{y_i^3}e^{-\frac{\theta}{y_i}},y_i\ge0 $$ The unknown parameter is $\theta$ , with $\theta>0$ . Write down the likelihood for $\theta|y$ . Find an expression for the maximum likelihood estimate $\hat{\theta}$ ."" My approach is as follows: $$L(\theta;y) = \prod_{i=1}^n \frac{\theta^2}{y_i^3}e^{-\frac{\theta}{y_i}},y_i\ge0  $$ Now, $$\prod_{i=1}^n \theta^2=\theta^{2n}$$ $$\prod_{i=1}^n \frac{1}{y_i^3}=\frac{1}{(\prod y_i)^3} $$ and $$\prod_{i=1}^n e^{-\frac{\theta}{y_i}}=e^{-\theta \sum\frac{1}{y_i}} $$ So overall, $$L(\theta;y)=\theta^{2n} \frac{1}{(\prod y_i)^3} e^{-\theta \sum\frac{1}{y_i}}$$ The log-likelihood is $$\ell(\theta;y)=2n\ln(\theta) + \ln\left(\frac{1}{(\prod y_i)^3}\right) -\theta \sum\frac{1}{y_i}$$ Where all the products and sums happen over $i=1,2,3,\dots,n$ . Differentiating with respect to $\theta$ : $$ \frac{d\ell}{d\theta}=\frac{2n}{\theta}-\sum\frac{1}{y_i}=0$$ This implies that $$\hat{\theta}=\frac{2n}{\sum\frac{1}{y_i}} $$ Is the likelihood and the MLE correct?","['statistics', 'solution-verification', 'bayesian', 'maximum-likelihood']"
4619012,Does $\displaystyle \lim_{x \rightarrow 0} \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)}$ exist?,"I was playing around with the function $f : \mathbb R \rightarrow \mathbb R$ , defined as follows. $$f(x) = \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)}$$ This function is undefined at $x = \frac{1}{n\pi}$ for all $n \in \mathbb N$ . Namely, this suggests $$\forall \delta > 0 : \exists x : 0 < |x| < \delta \wedge f(x) \text{ is undefined.}$$ I'm curious about $\lim_{x \rightarrow 0} f(x)$ . If this value exists, say set $\lim_{x \rightarrow 0} f(x) = L$ , we naturally must have that $$\forall \varepsilon > 0 : \exists \delta > 0 : 0 < |x| < \delta \Rightarrow |f(x) - L| < \varepsilon.$$ But, since $f$ is not defined for all $x$ in the set $0 < |x| < \delta$ , the conclusion $|f(x) - L| < \varepsilon$ cannot always be guaranteed to hold. So, I'm inclined to say that the limit $L$ does not exist. That being said, the graph looks like this and based on the graph, $f$ appears to satisfy this inequality: $$\forall x : \frac{\sin x}{x} \leq f(x) < 1$$ Since $\displaystyle \lim_{x\rightarrow 0} \frac{\sin x}{x} = 1$ , this suggests in a squeeze-theorem-esque way that $L$ ""wants"" to take the value $1$ , but of course we cannot actually apply the squeeze theorem here for the same reason that we couldn't apply the $\varepsilon$ - $\delta$ definition directly. So, my questions are: Is my analysis correct that $\displaystyle \lim_{x \rightarrow 0} \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)}$ does not exist? Is there any looser definition of a limit in common use (say, ${\lim}^\star$ ) that would set $\displaystyle {\lim_{x \rightarrow 0}}^\star \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)} = 1$ in a way that captures the spirit of what I described above?","['limits', 'continuity', 'epsilon-delta']"
4619016,"Definition of presheaf, given in Basic Algebraic geometry 2 Shafarevich","I was reading Basic Algebraic geometry 2 by Shafarevich,
In the definition of presheaf, $\mathscr F(\emptyset)=1$ To illustrate this definition they have given an following example: suppose first that $A$ has no zerodivisors, and write $K$ for its field of fractions. In this case $A$ is a subfield of $K$ . For an open set $U\subset\operatorname{Spec}A$ we denote $\mathscr O(U)$ the set of elements $u \in K$ such that for any point $x \in U$ we have an expression $ u=\frac{a}{b}$ with a,b $\in A$ and $b(x)\neq 0$ that is, b is not an element of the prime ideal $x$ . Now $\mathscr O(U)$ is obviously a ring. Since all the rings $\mathscr O(U)$ are contained in K, we can compare them as subsets of one set. If $U\subset V$ then clearly $\mathscr O(V)$ $\subset$ $\mathscr O(U)$ . We write $\rho_{U}^{V}$ for the inclusion $\mathscr O(V)$ $\hookrightarrow$ $\mathscr O(U)$ . A trivial verification shows that we get a presheaf of rings. According to definition of presheaf $\mathscr O(\emptyset)$ should be 1 but I am getting $\mathscr O(\emptyset)=K$ I could not figure out why: can anyone kindly explain what is wrong with this? Thanks in advance","['algebraic-geometry', 'schemes', 'commutative-algebra']"
4619063,Why are there two standard forms for parabolas?,"I am helping my girlfriend do her math homework.  They have been calling $$ (x-h)^{2} = 4a(y-k) $$ the standard form.  Suddenly in her homework, a different equation has been referred to as standard form: $$ y = ax^{2} + bx + c, $$ which is familiar to me from high school. It's been frustrating for her to see both of these equations called the standard form, especially since questions are asking to convert a freeform equation into 'standard form' when standard form seems ambiguous. Both equations represent parabolas in different ways, but does anyone have an idea why they are both referred to as standard form?","['algebra-precalculus', 'conic-sections']"
4619110,What shape provides the best square drill bit?,"The square drill bit is based on the Reuleaux triangle. The Reuleaux triangle can drill about $98.77$ % of a square. Specifically, the maximum fraction of a square that it can drill without drilling outside the square is $2\sqrt{3} + \pi/6 - 3.$ Is there any drill bit that can mathematically drill a greater fraction of the square while, at all times, touching all four sides of the square (so that its rotation is restricted)? All such drill bits must be shapes of constant width.","['area', 'geometry']"
4619121,Why does doubling the exponent very often double the amount of digits?,"This is not an absolute rule as you easily find counter examples. However, in $[\![0;100]\!]$ there are 51 integers which follow this rule. Many times, the number of digits in $2^{n+1}$ is twice the number of digits in $2^{n}$ . Examples: $$
\begin{split}
2^{13} & = 8192\\
2^{13 \times 2} = 2^{26} & = 67108864 \\
2^{26 \times 2} = 2^{52} & = 4503599627370496\\
2^{52 \times 2} = 2^{104} & = 20282409603651670423947251286016
\end{split}
$$ Again, we don't observe a universal rule. However, a large part of integers behave this way. I was wondering if it is easily explainable, or if it's just a coincidence.",['algebra-precalculus']
4619133,"Is there a ""closed form"" expression for the powerset of a complement?","Let us consider some consistent subset of na√Øve set theory , in which a universal set $U$ exists, the power set $\mathcal{P}(A)$ and complement $A'=U-A$ exists of any set $A$ , on top of the usual binary operations $\cup$ , $\cap$ , $-$ , $\Delta$ (symmetric difference), etc. Considering that $\mathcal{P}(U)=U$ (as $U$ is the universe, rather than some arbitrary domain of discourse), since all subsets of $U$ are elements of $U$ and vice-versa, is there a ""closed-form"" expression for $\mathcal{P}(A')$ ? By ""closed-form"" here, I mean an expression only in terms of $A$ and $U$ under the given operations (so, no set-builder notation). An immediate answer evades me, partly because na√Øve set theory is often unintuitive, but also because I am sleep deprived (so I apologize If I have overlooked something trivial). We may first ask the question of what exactly $\mathcal{P}(A')$ represents. It is not so hard to deduce that $\mathcal{P}(A')=U-\Gamma$ , where $\Gamma$ is the family of all sets which are not disjoint with $A$ . I.e; $$\Gamma=\{x\in U:x\cap A\neq\emptyset\}.$$ If we were allowed set-builder notation, this would indeed be our answer! However, since we don't, the question becomes whether or not $\Gamma$ can be represented in some closed-form. Equivalently, $$\mathcal{P}(A')=\{x\in U:x\cap A=\emptyset\}$$ can be seen to be the family of all sets which are disjoint with $A$ (which is, IMO, an interesting property!). Whether this is easier to be shown to have or not have a ""closed form"" is not obvious to me, but worth mentioning. Any and all advice to this recreational problem would be greatly appreciated.","['elementary-set-theory', 'nonstandard-models']"
4619140,generalized vector multiplication without identity element,"Is it possible to define (ideally infinitely) differentiable functions $f_n : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}^n$ and $g_n : \mathbb{R}^n \times (\mathbb{R}^n \setminus \{0\}) \rightarrow \mathbb{R}^n$ where $f_n(x, g_n(y, x)) = y$ and $f_n(x, y) = 0 \iff (x = 0 \vee y = 0)$ ? It's possible for $n = 1$ using $f_1(x, y) = xy$ and $g_1(x, y) = \frac{x}{y}$ . By interpretation of $\mathbb{R}^2$ as complex numbers, it's also possible for $n = 2$ using $f_2(x, y) = \left(
\begin{array}{c}
 Re (x_1 + x_2 i) (y_1 + y_2 i) \\
 Im (x_1 + x_2 i) (y_1 + y_2 i) \\
\end{array}
\right)$ and $g_2(x, y) = \left(
\begin{array}{c}
 Re \frac{x_1 + x_2 i}{y_1 + y_2 i} \\
 Im \frac{x_1 + x_2 i}{y_1 + y_2 i} \\
\end{array}
\right)$ . $n = 4$ should also work using a similar approach using quaternions. But is it possible for the general case? I'm not sure whether this is related to this ([1]) question since it might not be necessary to have full algebraic characteristics in order to define $f_n$ and $g_n$ . [1] Is there a third dimension of numbers?","['abstract-algebra', 'analysis']"
4619141,Cantor Function Clarification,"I am reading some lecture notes and came across the Cantor Function. However, I have some questions about it after reading. Recall the Cantor set $C \subseteq [0, 1]$ is compact, has Hausdorff dimension $\alpha = \frac{\ln 2}{\ln 3}$ and $H^\alpha$ denotes the Hausdorff measure of dimension $\alpha$ .
We define the Cantor function as: $$
f(x) = \frac{H^\alpha(C \cap [0, x])}{H^\alpha(C)}.
$$ Intuitively, this is the fraction of the Cantor set that lies to the left of $x$ . Now define the function $$
g(x) = \inf\{ y: f(y) = x \}.
$$ It can be observes that $f(g(x)) = x$ as $f$ is increasing and continuous and thus the infimum is achieved. The lecture note then claims $g$ is not continuous and I would like to understand the reasoning behind it. In particular, I would like to understand why: $$
g(\frac{1}{2}) = \frac{1}{3}
$$ and $$
g(y) \geq \frac{2}{3}
$$ for any $y > \frac{1}{2}$ .","['measure-theory', 'cantor-set', 'proof-explanation', 'hausdorff-measure', 'real-analysis']"
4619158,Does this topology have a name?,"Suppose that $(X,\tau)$ is a topological space, $A\subset X$ is an open  subset. Define a new topology, $\tau_A$ , on $X$ , $$
\tau_A=\{U\subset X: U\cap A\in \tau\}.  
$$ It is easy to see that $\tau_A$ is again a topology on $X$ . Question. Is there a standard name for the topology $\tau_A$ ? Edit. ""Coinduced topology"" seems to be the most esthetically pleasing name for this topology.","['general-topology', 'terminology', 'reference-request']"
4619205,"Examples of sets that are an ""essence"" of another set in predicate logic","In my lecture course on Set Theory, the ""essence of a set"" is defined in the following way: Essence: we say that $x$ is an essence of $y$ if: $$\forall w \space (w \in x) \leftrightarrow (y \in w) $$ In other words, every element of $x$ contains $y$ and every set containing $y$ is an element of $x$ . Are there any examples of sets that have this particular property. I haven't been able to find any references of this particular concept outside of my lecture course, so finding specific examples of this property has been difficult. If there are no examples of sets with this specific property, I would also be interested in a justification as to why this is the case.","['elementary-set-theory', 'logic', 'examples-counterexamples']"
4619226,When is $e^f$ convex for non-convex $f$?,"Assuming that $f(x)\in C^2(\mathbb{R})$ is non-convex, when is $e^{f(t)}$ convex? Clearly the function $\exp:\mathbb{R}\to(0,\infty)$ preserves convexity under composition: $$\frac{d^2}{dt^2} e^{f(t)}=e^f\left(f'^2+f''\right)\ge 0$$ whenever $f''\ge 0$ . But when does it create convexity when it previously did not exist? The question is equivalent to $f$ satisfying the differential inequality $$0>f''\ge-(f')^2.$$ Let $f'(t)=v(t)$ . Then the inequality reads $$0>v'\ge-v^2.$$","['calculus', 'convex-analysis', 'analysis', 'ordinary-differential-equations']"
4619278,Add a minimum of edges to make the graph Hamiltonian,"We have graph $G=K_{9,15}$ . I need to add some edges to it for make Hamilton Graph. So, we know: $$V(G) = 24,\
E(G) = 135.
$$ Every vertex has to be equial or more than 12. (24 / 2). So the question is, how can I find the minimum number of edges I need to add to my original graph without draw?
I can't find it in google, maybe I did it wrong?","['graph-theory', 'hamiltonicity', 'discrete-mathematics', 'discrete-optimization']"
4619318,"Finding $\int_{0}^{1} [x-(1-x^k)^{1/k}]^{2n} dx, k>0, n\in N$","We invoke a very interesting property of definite integrals given in MSE If $f(f(x)) = x$ and $f(0) = 1$ then what is the value of $\int_0^1 (x - f(x)) ^{2n} dx$ if $f(f(x))=x, f(0)=1$ , then $\int_{0}^1 (x-f(x))^{2n}dx=\frac{1}{2n+1}.$ Here, we choose a rare self inverse function: $f:[0,1] \to [0,1], f(x)=(1-x^k)^{1/k},k>0$ to get a very interesting result as $$\int_{0}^{1} [x-(1-x^k)^{1/k}]^{2n} dx=\frac{1}{2n+1}, k>0, n\in N \dots (*)$$ The question is: How else this integral (*) can be obtained?","['integration', 'functions', 'definite-integrals']"
4619348,polynomial approximation of a circle,"First of all, I am not a mathematician and my mathematics are fairly rusty. I would appreciate some help I believe there is not polynomial equation of a circle (is that right?) but take a look at this picture In it we can see two equations. One a circle $30x^2+30y^2=300000$ and the other one a parabola $y=120+-0.003x^2$ Obviously they are different but the parabola in the vicinity of (0,100) seems fairly parallel to the circle This coefficients are randomly chosen but my question is if I have a circle of radius r , how can I find the coefficients of a parabola that is a bit parallel in: a point in the axis any point of the circle?",['geometry']
4619365,"Given positive real numbers $x_1,x_2,\ldots,x_n$ such that $\sum_{k=1}^{n}{\frac{1}{x_k+1}}=n-2$ , how to prove the following inequality?","Given positive real numbers $x_1,x_2,\ldots,x_n$ such that $\sum_{k=1}^{n}{\frac{1}{x_k+1}}=n-2$ , prove that $$
\sum_{i=1}^{n}{\frac{1}{x_i}}+\frac{n(n-2)}{\sum_{i=1}^{n}{x_i}}\geqslant(n-1)(n-2)
$$ This problem was initially asked on the site 'zhihu'. The case $n=3$ has been proved by ' SOS' method , and I've given a $pqr$ method proof. How to prove (or falsify) this generalised inequality? I've tried to set $a_i=1/x_i$ , then $\sum_{k=1}^{n}{\frac{a_k}{a_k+1}}=n-2$ and the inequality is equivalent to $$
nA+(n-2)H\geqslant(n-1)(n-2)
$$ where $A$ is the Arithmetic Mean among $a_i$ , and H is the Harmonic Mean among $a_i$ , and maybe we can apply some theorems relevant to these means.","['multivariable-calculus', 'algebra-precalculus', 'inequality']"
4619370,What is the maximum number of points at which an $n$-degree polynomial intersects $e^{x^2}$?,"What is the maximum number $\phi(n)$ of roots of the function $e^{x^2}-p(x)$ , where $p(x)$ is an $n$ -degree polynomial. It is known that an $n$ -degree polynomial intersects the function $e^x$ at at most $n+1$ points ( Number of solutions of $P(x)=e^{ax}$ if $P$ is a polynomial ). It follows from the fact that the function $e^x$ has strictly positive $(n+1)$ -order derivative whilst $p^{(n+1)}\equiv 0$ , so $e^x-p(x)$ has strictly positive $(n+1)$ -order derivative. By inspecting the derivatives of the function $f(x)=e^{x^2}$ : \begin{align*}
  f'(x) &= 2x e^{x^2}, \\
  f''(x) &= 2(2x^2+1)e^{x^2} \\
  f'''(x) &= 4x (2x^2+3) e^{x^2},\\
 & \dots
\end{align*} we can observe (and prove recursively) that all of the derivatives $f^{n}$ of $f$ are strictly positive for $n$ even; strictly positive on $(0,\infty)$ and strictly negative on $(-\infty,0)$ for $n$ even. We can therefore conclude that $f$ intersects any $n$ -degree polynomial $p$ at at most $n+1$ points whenever $n$ is odd. And considering interpolation (@GregMartin's comment) we have that: $\phi(n)=n+1$ for $n$ odd. However, using the same argument for $n$ even we can only conclude that $f-p$ has at most $n+1$ roots on $(0,\infty)$ , and likewise (due to symmetry about $y$ -axis) at most $n+1$ points on $(-\infty,0)$ . Considering that another intersection might occur at $x=0$ , we can be sure that $f$ intersects $p$ at at most $2n+3$ points. Is there a better estimate than $\phi(n)\leq 2n+3$ for $n$ even?","['roots', 'derivatives', 'exponential-function', 'real-analysis']"
4619420,Trick to find continuity of a multivariate function [duplicate],"This question already has answers here : Multivariable limit proof: $\lim\limits_{(x,y)\rightarrow (0,0)}\frac{\left|x\right|^a\left|y\right|^b}{\left|x\right|^c + \left|y\right|^d} = 0$ (4 answers) Closed last year . Let $f(x,y)=\frac{x^2y}{x^4+y^2} $ if $(x,y) \ne (0,0)$ and $f(0,0)=0$ if $(x,y)=(0,0)$ This is a question from a university entrance exam. Generally i get stuck in these types of problems where the numerator and denominator powers are tough to cancel out and using the polar coordinates makes it more complicated.(Atleast on my part) I came across the solution which went like this: $m=2,n=1,i=4,j=2 (even)$ and $mj+ni=ij$ so the function is not continuous at $(0,0)$ Does anyone have any idea behind the logic of this trick?","['limits', 'multivariable-calculus']"
4619423,Prove that the vector space of real convergent power series does not have a countable basis,"Define $V$ as the subset of $\mathbb{R}[[x]]$ , the $\mathbb{R}$ -vector space of formal power series with real coefficients, such that
for any $f\in V$ , for any $r\in\mathbb{R}$ , the series $f$ always converges when evaluated at $r$ .
It can be shown that $V$ is a subspace of $\mathbb{R}[[x]]$ . I am trying to show that $V$ does not have a countable basis.
Suppose for contradiction that $S$ is a countable basis of $V$ .
Write the $i$ th element in $S$ as $f_i:=\sum_j a_{ij}x^j$ .
One attempt to construct an element $g$ in $V$ such that $g$ is not spanned by $S$ would be some kind of diagonal argument, i.e. $g:=\sum_j h(a_{jj})x^j$ for some suitable function $h$ .
However, I struggle to find an $h$ such that $g$ is convergent
because the $a_{jj}$ can be quite arbitrary.
Is there any other promising way I can do this?","['hamel-basis', 'linear-algebra', 'analysis', 'sequences-and-series']"
4619442,On a peculiar operation on polynomials,"Consider the polynomials with natural number coefficients. Take two polynomials $P, Q$ . If $Q(x) = \sum_{i} c_i x^i$ , then we define $$(Q*P)(x) = \prod_{i} [P(x+i)]^{c_i}.$$ Is this operation studied anywhere in the literature? What is the name for it? I do not know what to search for, so I didn't find much. This operation satisfies some interesting properties, for instance $$P*(Q*R) = (P \times Q)*R; \quad (P*R)\times(Q*R) = (P+Q)*R$$ where $P\times Q$ is the usual multiplication of polynomials.
This makes the operation behave very much like exponentiation. I think this can even generalize to the multivariable case. Also, this seems to work for infinite (formal) power series, so perhaps it is relevant to generating functions?","['combinatorics', 'polynomials', 'generating-functions']"
4619458,Proof that countable union of countable sets is valid?,"Analysis beginner here. Of course this has been asked many times, but all the others proofs I found were slightly different from what I have. So I was just wondering whether this is valid, and in particular, if it is equivalent to the diagonal argument that is found in Rudin, for example. I think so, but I am not sure. Thank you! Let $A_1$ , $A_2$ , ... be a sequence of countable sets. Then for each $i \in \mathbb{N}$ , we have $A_i = \{a_{i,1},a_{i,2},...\}$ . Define $B=\bigcup_{i=1}^{\infty} A_i$ . For each element $b\in B$ , we have $b \in A_i$ for at least one $i$ , so $b = a_{i,j}$ for some positive integers $i,j$ . Pick some element of $B$ and call it $b_k$ . Define $f(k) = min\{ i \in \mathbb{N}: b_k \in A_i \}$ and $g(k, f(k)) = \{j \in \mathbb{N}: b_k = a_{f(k),j} \}$ Then define the sequence $k \rightarrow b_k$ by $\{b_1, b_2, ...\}=\{a_{f(1),g(1,f(1))},a_{f(2),g(2,f(2))},...$ }. This sequence contains each element of $B$ exactly once, so $B$ is countable.","['elementary-set-theory', 'solution-verification', 'analysis']"
4619480,"Circles of radius $1, 2, 3, ..., n$ all touch a middle circle. How to make the middle circle as small as possible?","Non-overlapping circles of radius $1, 2, 3, ..., n$ are all externally tangent to a middle circle. How should we arrange the surrounding circles, in order to minimize the middle circle's radius $R$ ? Take $n=10$ for example. On the left, going anticlockwise the radii are $1, 2, 3, 4, 5, 6, 7, 8, 9, 10$ , and $R\approx 10.77$ . On the right, going anticlockwise the radii are $10, 2, 9, 4, 7, 6, 5, 1, 8, 3$ and $R\approx 9.98$ . How should the circles be arranged to minimize $R$ ? Here is a desmos graph where you can try different arrangements for the case $n=10$ . My attempt For general $n$ , call the radii of the surrounding circles going anti-clockwise, $r_1, r_2, r_3, ..., r_n$ . Draw line segments from the centre of the middle circle to the centre of each surrounding circle. So we have $n$ angles at the centre of the middle circle. Each of these angles can be expressed in terms of $R, r_k, r_{k+1}$ , using the law of cosines . The sum of the $n$ angles is $2œÄ$ . So we have: $$\sum\limits_{k=1}^n \arccos{\left(\frac{(R+r_k)^2+(R+r_{k+1})^2-(r_k+r_{k+1})^2}{2(R+r_k)(R+r_{k+1})}\right)}=2\pi \text{ (where }r_{n+1}=r_1)$$ This simplifies to: $$\sum\limits_{k=1}^n \arccos{\left(1-\frac{2r_k r_{k+1}}{(R+r_k)(R+r_{k+1})}\right)}=2\pi$$ We want to assign each $r$ a unique value among $1, 2, 3, ..., n$ so that $R$ is minimized. But how? Is there a general pattern? Conjectured answer I think the following general procedure will make the middle circle as small as possible. Take $n=10$ for example. We ignore the $1$ at first; it will be placed last. First put down the numbers $10, 9, 8, 7, 6$ in pyramid-fashion, from top to down and from left to right, like the red numbers below. Then put down the numbers $2, 3, 4, 5$ in pyramid-fashion also, but between the previous rows, like the blue numbers below. This gives the order of radii going around the circle: $7, 4, 9, 2, 10, 3, 8, 5, 6$ . Then put the circle with radius $1$ anywhere you like, as long as it fits without disturbing other circles. For larger values of $n$ , there may be multiple small circles that can fit between other circles without disturbing them. Put those last. Intuitively, this procedure minimizes the tendency of the large surrounding circles to take up space around the middle circle. You can see this in the diagrams above with $n=10$ , looking specifically at the two largest surrounding circles in each case. Anyway, it's just a conjecture for now. Notice that this conjecture implies that, for $n=10$ , the arrangement I showed above on the right, yields the minimum radius of the middle circle. I don't think the middle circle can be any smaller.","['optimization', 'discrete-geometry', 'circles', 'geometry']"
4619486,About closed subspaces in infinite Dimensional HIlbert spaces,"How can I prove the following? Let $H$ be an infinite-dimensional Hilbert space. Show there exist closed vector subspaces $\{X_t:t\in[0,1]\}$ in $H$ so that $X_s\subset X_t$ for every $0\leq s < t \leq 1$ . My attempt Given that $H$ is isomorphic to $L^2[0,1]$ , I move to the function space.
Clearly considering $X_s = \{f\in L^2: f(x) = 0 \ \forall x > s\} $ then the condition is clearly satisfied. So two doubts here: The question doesn't mention separability, but it's necessary for my argument (isomorphism). Is it necessary to prove the statement? Can it be proven without resorting to working in another isomorphic space?","['hilbert-spaces', 'functional-analysis']"
4619502,First order ODE - appropriate method,"I'm trying to solve the following ODE $y'(x)=2y -x$ I tried to solve it with the method of variation of constants and it worked. I considered the homogenous equation without the last term ( $-x$ ),and I found the solution of the homogenous equation $y_h=C e^{2x}$ . Then, I considered the function $y(x)=C(x)e^{2x}$ , and $y^{'}(x)=C^{'}(x)e^{2x}+2C(x)e^{2x}$ . By plugging these 2 equations in the initial ODE, I got $C^{'}(x)=\frac{-x}{e^{2x}}$ Then, I integrated $C'(x)$ , and the particular solution , using $C(x)$ , is given by $y_p=\frac{1}{4} (2x+1)$ Then, the general solution is given by $y(x)=y_h(x)+y_p(x) =\frac{1}{4}(2x+1) +Ce^{2x}$ The question is: why am I allowed to use the method of variation of constants? I understood how to use it, but when I can use it?","['calculus', 'ordinary-differential-equations']"
4619514,"""Actual"" points vs rational points of a scheme over a field.","Let $X$ be a scheme over a field $k$ , and consider a point $x\in X$ , i.e. a point in the underlying topological space of $X$ . Does there exist a field extension $K$ of $k$ such that $x\in X(K)$ ? I have a hard time relating the notions of ""actual point"" and ""rational point"" to one another. Every $K$ -rational point $Spec(K) \to X$ has an image in $X$ , which is an actual point. My question is about the converse, can every actual point be realized as (the image of) a $K$ -rational point of some (big enough?) field extension $K$ of $k$ ?",['algebraic-geometry']
4619560,"A good upper bound of $S_n := \sum_{k=1}^n e^{-\lambda} \frac{\lambda^k}{k!} \frac{1}{(k \varepsilon)^k}$ in terms of $\varepsilon,\lambda,n$","Fix $\varepsilon \in (0, 1), \lambda >0$ , and a positive integer $n$ . I'm interested in upper bound the quantity $$
S_n := \sum_{k=1}^n e^{-\lambda} \frac{\lambda^k}{k!} \frac{1}{(k \varepsilon)^k}
$$ in terms of $\varepsilon,\lambda,n$ . The term $e^{-\lambda} \frac{\lambda^k}{k!}$ is the probability mass from Poisson distribution with parameter $\lambda$ . The term $\frac{1}{(k \varepsilon)^k}$ can be considered the loss associated to $k$ . So $S_n$ can be seen as the average loss up to $n$ . Of course, we have a trivial upper bound $$
S_n \le \sum_{k=1}^\infty e^{-\lambda} \frac{\lambda^k}{k!} \frac{1}{\varepsilon^k} = \exp \left (-\lambda + \frac{\lambda}{\varepsilon} \right ).
$$ Could you elaborate on some techniques to have a tight upper boud of $S_n$ ? Any reference is appreciated!","['poisson-distribution', 'inequality', 'probability-theory', 'summation']"
4619579,Continuously extending a function,"Let $X$ , $Y$ be topological spaces and let $f\colon E\to Y$ be continuous where $E\subseteq X$ . Let $S\subseteq \overline E\setminus E$ and $g\colon S\to Y$ be such that $f(x)\to g(c)$ whenever $x\to c$ (according to the first definition here ). Now, extend $f$ to $h\colon E\cup S\to Y$ via $g$ . Question: Is $h$ continuous? The answer is shown to hold when $X$ , $Y$ are taken to be metric spaces, $f$ taken to be uniformly continuous, and $S = \overline E\setminus E$ .","['limits', 'general-topology', 'continuity']"
4619597,"Character Space of $C^1[0,1]$ and Gelfand Representation","I have recently been working through some exercises in Murphy's ""C*-Algebras and Operator Theory,"" and I am having some trouble with Exercise 10 in Chapter 1. The exercise is as follows: Let $A = C^1[0,1]$ . Let $x: [0,1] \longrightarrow \mathbb{C}$ be the inclusion. Show that $x$ generates $A$ as a Banach algebra. If $t \in [0,1]$ , show that $\tau_t$ belongs to $\Omega(A)$ , where $\tau_t$ is defined by $\tau_t(f) = f(t)$ , and show that the map $[0,1] \longrightarrow \Omega(A)$ , $t \mapsto \tau_t$ , is a homeomorphism. Deduce that $r(f) = \|f\|_{\infty}$ ( $f\in A$ ). Show that the Gelfand representation is not surjective for this example. So far, I have been able to show that $x$ generates $A$ via Stone-Weierstrass. The claim that $\tau_t$ is in the character space seemed quite clear as well. I am having some trouble with the rest. To show the homeomorphism, it should be enough to show surjectivity, since continuity and injectivity should be clear, but I am not sure how to approach this. I am also not quite sure how to approach the last two claims either. Any help would be appreciated.","['c-star-algebras', 'spectral-radius', 'functional-analysis', 'gelfand-representation', 'spectral-theory']"
4619621,How to calculate multiplication of transfinite nimbers with a Cantor normal form,"I failed to calculate nimber multplication in the form of $[\omega^\alpha]*[\omega^\beta]$ , according to the ""mex"" definition. The cases when $\alpha<3,\beta<3$ are easy, while $[\omega^3]*[\omega^3]$ and others seem to be too difficult, not to mention the ordinals that are way too big to be represented by $\omega$ , so I hope to have a result for ordinals with Cantor normal forms.","['ordinals', 'abstract-algebra', 'combinatorics', 'combinatorial-game-theory', 'transfinite-recursion']"
4619637,"Intuition behind Filtrations, Martingales and Stopping times","I would like to gain some intuition or context for the study of Martingales. What I have seen so far seemed to be motivated more by measure theory that probability theory. In the context of studying Martingales I came upon Filtrations. Let $(\Omega, \mathcal F,\mathbb P)$ be a probability space and $(\mathcal F_n)_n$ a filtration of $\mathcal F$ . This means for all $n\in\mathbb N: \mathcal F_{n}\subseteq\mathcal F_{n+1}$ . The canonical filtration is $\mathcal C_n = \sigma(X_1,\dots, X_n)$ , where $X_n$ are $\mathcal C_n$ -measurable functions. $(X_n)_n$ is called a Martingale, if $\mathbb E[X_{n+1}|\mathcal F_n]=X_n$ . How can I interpret a filtration $(\mathcal F_n)_n$ from conditional expectation context and why do we use the canonical filtration $(\mathcal C_n)_n$ aside from measure theoretic considerations? I believe stopping times come from the study of stochastic processes but the examples I have seen have all been measure theoretical. $N$ is a stopping time, if $\forall n\in\mathbb N:\{N>n\}\in\mathcal F_n$ . I know that $n_o\in\mathbb N_o$ is a stopping time and that if $N_1, N_2$ are stopping times that $N_1\land N_2$ and $N_1\lor N_2$ are stopping times. The term stopping time seems like it should be more tangible. What is a practical interpretation of a stopping time and what are examples of proofs of stopping times that go beyond properties of $\sigma$ -algebras?","['martingales', 'stopping-times', 'probability-theory', 'intuition']"
4619652,Solve the equation $8^x+3\cdot2^{2-x}=1+2^{3-3x}+3\cdot2^{x+1}$,"Solve the equation $$8^x+3\cdot2^{2-x}=1+2^{3-3x}+3\cdot2^{x+1}$$ The given equation is equivalent to $$2^{3x}+\dfrac{12}{2^x}=1+\dfrac{8}{2^{3x}}+6\cdot2^x$$ If we put $a:=2^x>0$ , the equation becomes $$a^3+\dfrac{12}{a}=1+\dfrac{8}{a^3}+6a$$ which is $$a^6-6a^4-a^3+12a^2-8=0$$ The LHS factors as $(a+1)(a-2)(a^4+a^3-3a^2-2a+4)$ , which is in no case obvious. Let's say that we find the roots $1$ and $-2$ , then how do we show that $(a^4+a^3-3a^2-2a+4)$ does not factor any more? Taking these into consideration, I believe there is an another approach. Any ideas would be appreciated.",['algebra-precalculus']
4619661,"If $\sum_{i=1} ^{2022}\sin^{-1}(x_i) = 1011\pi$, then find $\sum_{i=1}^{2022} x_i$","A question is given in my book which I'm unable to solve. If $\displaystyle\sum_{i=1}^{2022} \sin^{-1}(x_i) = 1011\pi
$ , then what is the value of $\displaystyle\sum_{i=1}^{2022} x_i$ ? Answer of the above problem is given to be $2022$ . In general, this problem is of the form that if $\displaystyle\sum_{i=1}^{n} A_i = k$ , then what is the value of $\displaystyle\sum_{i=1}^{n} \sin(A_i).$ So we need to find the sum of sines of numbers, when sum of numbers is given. But I can't continue from here because of my little knowledge. Alternatively , I thought of using the formula: $\sin^{-1}(x) + \sin^{-1}(y) = \sin^{-1}(x\sqrt{1-y^2} + y \sqrt{1-x^2})$ to simplify the sum $\displaystyle \sum_{i=1}^{2022} \sin(x_i)$ . But here, because of $2022$ terms, it's very difficult to apply the formula. I think there would be surely any method/formula for simplifying $\sum_{i=1}^n \sin^{-1}(x_i)$ in general, which I'm unaware of.","['trigonometric-series', 'trigonometry', 'inverse-function', 'sequences-and-series']"
4619689,"If $(AB)^3=BA$, then all matrices commute for a closed set","Let $\mathcal{M}$ be a subset of $M_n(\mathbb{R})$ , the vector space of all $n\times n$ matrices over $\mathbb{R}$ . If $AB\in\mathcal{M}$ and $(AB)^3=BA$ hold for all matrices $A, B\in \mathcal{M}$ , then (1) $AB=BA$ for all $A, B\in \mathcal{M}$ ; (2) if $I_n\in\mathcal{M}$ , then ${\rm det}(A)=\pm 1$ or $0$ for all $A\in\mathcal{M}$ . It seems to have some connections with abstract algebra. The first statement seems similar to proving that for a semi-group $G$ , if $(gh)^3=hg$ , then $G$ is abelian. The assumption of possessing the identity matrix in the second statement makes it a monoid. Maybe we do not need to apply abstract algebra here, just a thought. But I‚Äôm stuck when trying to proceed. Any help is highly appreciated.","['abstract-algebra', 'linear-algebra']"
4619709,"I really don't 'belong' here, but I'd love to have this question answered for my granddaughter....","one of my teen grandkids asked me a question about the purpose of using 'complicated' equations when an answer could be determined with 'simple' multiplication. As an uneducated man, the only answer I could come up with was it uses a lot fewer #s and space, so I decided to look for a better response. I found this site and I found the following equation that was posted in a previous question and it's a perfect example of what she was asking about.
Why is it better to use the equation than to use multiplication? Thanks for your help, she's a smart kid, I want to do anything that will add to her knowledge. John Savarese,
Waltham, MA On the first Sunday of 2003, Rizzo and Frenchie start a chain letter, each of them sending five letters (to ten different friends between them). Each person receiving the letter is to send copies to five new people on the Sunday following the letter‚Äôs arrival. After the first seven Sundays have passed, what is the total number of chain letters that have been mailed? How many were mailed on the last three Sundays? The way I solved it was to add them together to find the number of letters that have been mailed. a) $2 \cdot 5 + 2 \cdot 5^2 + 2 \cdot 5^3 + 2 \cdot 5^4 + 2 \cdot 5^5 + 2 \cdot 5^6 + 2 \cdot 5^7 = 195310$ b) $2 \cdot 5^5 + 2 \cdot 5^6 + 2 \cdot 5^7 = 193750$","['soft-question', 'discrete-mathematics']"
4619831,Proving $S_1-S_2$ is non-positive,"I have system number (1) as: $S_1‚Äô=-bS_1I_1+(1-c)aI_1$ $I_1‚Äô=bS_1I_1-aI_1$ And on the other hand I have system number (2) as: $S_2‚Äô=-bS_2I_2+aI_2$ $I_2‚Äô=bS_2I_2-aI_2$ All parameters $a,b,c>0$ . Both systems have the same initial conditions as in $S_1(0)=S_2(0)=s_0>0$ and $I_1(0)=I_2(0)=i_0>0$ . I want to prove that $S_1-S_2$ is non-positive.
What I did so far: I found $S_2$ explicitly though not sure it is useful as $S_1$ can‚Äôt be found explicitly unfortunately. Using the initial conditions then I have $S_1=  \frac{(1-c)a}{b}+(s_0- \frac{(1-c)a}{b}$$ )e^{-\int_{0}^{t} bI_1(t) \ dt}$ . Similarly, I obtained $S_2=  \frac{a}{b}+(s_0- \frac{a}{b}$$ )e^{-\int_{0}^{t} bI_2(t) \ dt}$ . So from 2 and 3 it is deduced that $(S_1-S_2)(0)=0$ and $$\lim_{t\rightarrow \infty}(S_1-S_2)(t)=-ca/b.$$ By manipulating the equation for $S_1$ then we have the relationship $e^{-\int_{0}^{t} bI_1(t) \ dt}= \frac{|bS_1(t)-(1-c)a|}{|bs_0-(1-c)a|} $ Despite all that I can‚Äôt seem to prove that $(S_1-S_2)(t)$ is negative for $t>0$ . I checked this via simulations and indeed it is the case but I‚Äôm trying by contradiction and can‚Äôt find something that can lead to a contradiction in anything. Help is appreciated!","['ordinary-differential-equations', 'real-analysis']"
4619836,Why are monotonicity and convexity so intensely studied in comparison to their higher-order analogies?,"I've been recently contemplating what can be said about functions $f:\Bbb R \to \Bbb R$ with positive third-order derivative and their properties: It puzzles me why monotonicity and convexity , i.e. the properties associate with the first- and second-order derivatives of a function being positive, are frequently used, whilst  their higher-order alternatives are so rarely referred to. I found that the third-order derivative at a point is called jerk in physics and its meaning is discussed here . I found also a discussion on the topic why are third-order concepts so rare . However, what I'm interested in are the uses of the fact that third-order derivative of $f$ is positive in analysis. For example, whilst strict convexity of $f$ implies that every critical point of $f$ is the unique global minimum of $f$ and that $f$ has at most two roots, having strictly positive third-order derivative would guarantee that every point with $f''(x)=0$ is the unique inflection point of $f$ , that the function has at most one local minimum and one local maximum, and no more than 3 roots. One could argue that it would be informative enough to know that there is $x_2\in \Bbb R$ such that $f''(x_2)=0$ and that $f$ is concave on $(-\infty,x_2]$ and convex on $[x_2,\infty)$ . However, analogously one could argue that convexity is not needed for analyzing function's minima because it is enough to know that there is $x_1\in \Bbb R$ such that $f'(x_1)=0$ and that $f$ is decreasing on $(-\infty,x_1]$ and increasing on $[x_1,\infty)$ . The concept of $n$ -th order convexity (having positive $n$ -th order derivative) is essential for the analysis of the number of roots of one-variable functions: A function with positive $n$-th derivative has at most $n$ roots ‚Äì an inequality version of the Fundamental theorem of Algebra. Consider the question on what is the maximum number of strict local minima that a degree $k$ polynomial $p(x,y)$ in two variables can have ? In case of quadratic polynomial one can readily answer: The polynomial has a strict local minima only if it is strictly convex, and then the minimum must be unique. In case of cubic polynomial , there is also at most one strict local minimum because if there were two, say at points $a$ and $b$ , then the third degree polynomial $q(t)=p\big((1-t)a+tb\big)$ would need to have two strict local minima ‚Äì impossible. [1] In case of quartic polynomial , none of the above arguments apply and the only ready estimate follows from applying B√©zout's theorem to the partial derivatives of $p$ , and so we can be sure that $p$ has is no more than $3\times 3$ isolated critical points. [2] I expected that the above analysis would be trivial for the case of quadratic polynomials thanks to the concept of convexity. However, the analysis is equally trivial in case of cubic polynomial, the only difference is that there is no name for the ""third-order convexity"" that gives the result. In fact, the analysis becomes difficult as late as in the case of quartic polynomials. This suggests that third-order convexity has practical applications, only the higher-order alternatives would be less practical for the analysis of functions in two or more variables. Does importance of $n$ -th derivative drop? Let me compare the count of search results of terms a) convex b) monotone or monotonic (sum up the count) \begin{array} {|r|r|r|}\hline 
 & \text{Google} & \text{site:SME} & \text{site:mathoverflow} & \text{G Scholar} \\ \hline 
\text{a)} & 148M & 79K & 229 & 4.3M  \\ \hline 
\text{b)} & 98M & 77K & 54 & 2.5M \\ \hline  
\end{array} Why is the second-order property (convexity) more prevalent than the first-oder one (monotonicity) and yet the third-order one is almost never heard of? Questions: Is there a standardized name for functions with $f^{(3)}>0$ ? Is the main reason that $f^{(3)}$ is so rarely analyzed the fact that for most problems it is enough to determine on which regions $f$ is convex/concave and the effort needed to analyze $f^{(3)}$ would typically not be justified? Is there a known set property of $\mathop{epi}(f)$ for $f$ with $f^{(3)}>0$ , akin to strict convexity of $\mathop{epi}(f)$ for $f$ with $f^{(2)}>0$ ? Related Posts: Let me share some observations and conjectures that I came up with when contemplating on this topic: Functions not necessarily differentiable that behave like those with $f^{(3)}\geq 0$ : Geometric characterization of functions with positive third derivative . ""Quasi"" generalization of the condition that $f^{(3)}>0$ : Is there a third-order analogy of quasi-convexity? An attempt to find the geometric property whose special case $\mathop{epi}(f)$ of $f$ with $f^{(3)}\geq 0$ satisfies: Second-order star convex set: A set whose intersection with any conics passing through two given points consists of at most two connected curves.","['convex-geometry', 'soft-question', 'real-analysis', 'maxima-minima', 'convex-analysis']"
4619900,What do symmetries of equations in presentation of a group tell us about the group?,"Suppose we have some group $G$ , presented with some set of equations. If we are to collect up some subset of  equations which define the presentation as $P$ (don't need all), then does swap of variables which preserve the actual group implied by the presentation $P$ , tell us anything interesting about the group $G$ ? Example: Let's say we have for instance a group generated by $\{a,b,c\}$ with the relations $abc=b$ , $abc =a$ . Then in the for this group, we see that if we swap $a \leftrightarrow b$ , then the actual relation is same.  The swap of variables give same equation hence the group implied is same.  By implied group I mean, the group one gets when one applies the equations on the free group generated by the set. By the way, It could also have been the case that we get different poly equation which imply the group as same.","['combinatorial-group-theory', 'group-theory']"
4619924,Is there any simple set of properties that uniquely characterizes differentiation in the space of complex functions?,"The transformation of differentiation is a linear operator over the vector space of entire functions (call this space $\mathbb{C}^E.$ ) Is there any simple set of properties that uniquely determines this linear operator that uses only the field structure of the complex numbers and the vector space structure of $\mathbb{C}^E$ and how they ""interact"" with each other, rather than using an ""absolute value"" that cannot be defined in terms of the field structure alone?","['complex-analysis', 'linear-transformations', 'derivatives', 'soft-question', 'analytic-functions']"
4619950,Measure Preserving Self-Map of Compact Abelian Group Commuting with Ergodic Translation,"Let $K$ be a compact abelian group with its probability Haar measure, and let $S:K\to K$ be an ergodic translation automorphism. Suppose that $T:K\to K$ is a measure-preserving map that commutes with $S$ . Is $T$ itself necessarily a translation (almost everywhere)? This was an optional problem from a class exam, but we never got a solution set and the professor didn't reply to an email query, so we still can't figure it out. We've tried a few things that seemed somewhat promising (namely considering the map $T-\text{Id}$ , or using character theory on $L^2$ ) but didn't end up getting anywhere.","['measure-theory', 'harmonic-analysis', 'ergodic-theory', 'topological-groups', 'haar-measure']"
4619955,Grafakos Classical Fourier Analysis problem 1.1.14,"I'll first type the problem. Let $(X,\mu)$ be a measure space and let $s>0$ . (a) Let $f$ be a measurable function on $X$ . Show that if $0 < p < q < \infty$ , we have $$ \int_{|f| \leq s} |f|^q \leq \frac{q}{q-p} s^{q-p} \|f\|_{L^{p,\infty}}^p. $$ (b) Let $f_j, 1 \le j \le m$ , be measurable functions on $X$ and let $0 < p < \infty$ . Show that $$\bigg|\bigg|\max_{1 \leq j \leq m} |f_j|\bigg|\bigg|_{L^{p,\infty}}^p \leq \sum_{j=1}^m \| f_j\|_{L^{p,\infty}}^p. $$ (c) Conclude from part (b) that for $0 < p < 1$ we have $$\|f_1 + \dots + f_m \|_{L^{p,\infty}}^p \leq \frac{2-p}{1-p} \sum_{j=1}^m \|f_j\|_{L^{p,\infty}}^p. $$ Hint: Part (a): Use the distribution function. Part (c): First obtain the estimate $$d_{f_1+\dots+ f_m}(\alpha) ‚â§ \mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|‚â§\alpha})+d_{\max_j| f_j|}(\alpha)$$ for all $\alpha > 0$ and then use part (b). I have managed to solve part (a) and (b) and to prove the estimate of the hint given for part (c) but I haven't been able to solve part (c). What I have done is this $$\| f_1+\dots+f_m\|_{L^{p,\infty}}^p = \sup_{\alpha> 0} \alpha^p d_{f_1+\dots+f_m}(\alpha). $$ Thus I need to bound the right hand side for any $\alpha>0$ . We have, by the estimate given in the hint \begin{align*}\alpha^p d_{f_1+\dots+ f_m}(\alpha) &\le \alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|‚â§\alpha})+\alpha^pd_{\max j} | f_j|(\alpha) \\
&\leq \alpha^p\mu({| f_1+\dots+ f_m|>Œ±,\max| f_j|‚â§\alpha}) + \bigg|\bigg|\max_{1 \leq j \leq m} |f_j|\bigg|\bigg|_{L^{p,\infty}}^p\\
& \leq \alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|‚â§\alpha})+\sum_{j=1}^m \| f_j\|_{L^{p,\infty}}^p
\end{align*} So I need to estimate $$\alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|\le\alpha})$$ I noticed that if in part (a) I plug $q = 1$ and $s =1$ for $g= \max |f_j|$ I would obtain the result but I can't see how to actually relate $\alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|‚â§\alpha})$ to $\int_{|\max_j |f_j|| \leq 1} | \max |f_j||$ Any help is appreciated.","['weak-lp-spaces', 'fourier-analysis', 'analysis', 'real-analysis', 'inequality']"
4619982,"Why range of $\sin(x) - \cos(x)$ is not [-2,2]","If I want to calculate range of $$f(x)=\sin(x) - \cos(x) $$ Watching solution I got to know that we have to change this in a single trigonometric ratio (that is whole equation in form of sine or cosine)
And then range will be $[-\sqrt2,\sqrt2]$ But my doubt is that why can't we use method like below As we know $$  -1\le \sin(x) \le1$$ $$ -1\le \cos(x) \le1$$ Then $$ -2 \le \sin(x) - \cos(x) \le 2$$ But it is wrong I want explaination  that why using this method I am getting wrong",['trigonometry']
4619985,Derivative of a single valued function as a limit of a function in $\mathbb{R}^2$,"I am very familiar to the usual definition of derivative of a function $f(x) : \mathbb{R} \rightarrow \mathbb{R}^n$ as $\lim_{x \rightarrow a}\frac{f(x)-f(a)}{(x-a)}$ . My question is when can I relax the condition of ""fixing a $a$ ""? That is, when the following also holds: $$\lim_{(x,y)\rightarrow(a,a)} \dfrac{f(x)-f(y)}{x-y} = f'(a) \;\text{?}$$ I am convinced that this will work most of the time but I'm having trouble proving it.","['derivatives', 'real-analysis']"
4620041,"Check my proof that $\operatorname{Proj}\ k[x_0,...,x_n]/I$ is a reduced projective $k$ scheme for $I$ radical","I was doing Ravi's FOAG book exercise 5.3 E which asks me to prove that : $\operatorname{Proj}\ k[x_0,...,x_n]/I$ is reduced projective $k$ scheme for $I$ is radical homogeneous ideal To show it's projective $k$ - scheme is easy, since $k[x_0,...,x_n]/I$ is a finite generated $k$ algebra generated by $x_0,...,x_n$ . The problem, therefore, lies in showing it's reduced, since it's covered by $D(x_i)$ (as $x_i$ are generators). and $D(x_i)$ is affine, the problem reduces to show that the coordinate ring of $D(x_i)$ is reduced, then taking localization will preserve the reducedness. therefore we needs to figure out what $$((k[x_0,...,x_n]/I)_{x_i})_0 \tag{*}$$ is, where the inner subscript ${x_i}$ means taking localization at $x_i$ the outer subscript $0$ means the degree zero part of it. And it's sufficient to prove (*) is reduced. For $I$ is radical iff $(k[x_0,...,x_n]/I)$ is a reduced ring. Since localization preserves reduces, we have $(k[x_0,...,x_n]/I)_{x_i}$ is reduced. Since the degree zero part is a subring of it, and the subring of the reduced ring is reduced therefore we have (*) reduced. Is my proof correct?","['algebraic-geometry', 'solution-verification', 'commutative-algebra']"
4620096,How to show if this set is closed or not?,"Be $$A = \left\{(-1)^n \left(1 + \frac{2}{n+1}\right),\ n\in\mathbb{N}\right\}$$ How to show if it's closed or not? attempts I wrote it as $$A = \left\{(-1)^n \left(\frac{n+3}{n+1}\right),\ n\in\mathbb{N}\right\}$$ I thought about some qualitative analysis like: as $n\to +\infty$ I obtain $\pm 1$ , and I would think then I could write $A$ as $$A = \left(-2, -1) \cup (1, \frac{5}{3}\right)$$ which is a union of opens, hence it's open. Or again, since $\partial A \not \in A$ can I conclude the set is not closed. Or else I also know that a set is closed iff it contains ALL its limit points.
I think that $1$ is a limit point for $A$ since $\forall \epsilon$ I can always find a ball $B = (1, \epsilon) \ni p$ where $p$ is a point in $A$ . Am I right or wrong? That was an intuition, but how to prove that $1$ is indeed a limit point?","['elementary-set-theory', 'general-topology', 'solution-verification']"
4620098,Ordered iid Exp(1) Random Variable: Conditional Density Function,"I'm working on the following exercise from Achim Klenke's ""Probability Theory: A Comprehensive Course"" (3rd Ed, Exercise 15.1.3): Let $n \in \mathbb N$ and let $X_1, \ldots, X_n$ be i.i.d. exponentially distributed random variables with parameter $1$ . Let $Y_1, \ldots, Y_n$ be independent exponentially distributed random variables with $\mathbf P_{Y_k} = \exp_k$ . That is, $$(Y_1, \ldots, Y_n) \stackrel{\mathcal D}{=} (X_1, X_2/2, X_3/3, \ldots, X_n/n)$$ where $\stackrel{\mathcal D}=$ denotes equivalently distributed. Finally, sort the values of $X_i$ by size $X_{(1)} > X_{(2)} > \cdots > X_{(n)}$ . Show that $$
\left( X_{(n)}, X_{(n-1)}, \ldots, X_{(1)}\right) \stackrel{\mathcal D}= \left( Y_n, Y_{n-1} + Y_n, \ldots, Y_1 + Y_2 + \cdots + Y_n\right).
$$ Hint: First check that $X_{(n)} \stackrel{\mathcal D}= Y_n$ . Show that the conditional distribution $\mathcal L\left[\left(X_{(1)} - X_{(n)}, \ldots, X_{(n-1)} - X_{(n)}\right) | X_{(n)}\right]$ does not depend on $X_{(n)}$ and that it equals the (unconditional) distribution of the ordered values of $X_1, \ldots, X_{n-1}$ . Here Distribution of ordered independent exponential random variables there is a theoretical solution. Since I am not able to finish it, I think I am doing something wrong in the conditional law's density calculation. Indeed, for the distribution of the ordered values $X_{(1)},...,X_{(n-1)}$ I get, by integrating out the minumim (please, mind that $X_{(1)}$ is the maximum in the notation used by prof. Klenke) from the joint distribution of the ordered statistics, this density: $f_{X_{(1)},...,X_{(n-1)}}(x_1,...,x_{(n-1)})= n! f_{X_1}(x_1)...f_{X_1}(x_{n-1})F_{X_1}(x_{n-1})=n!e^{-(x_1+...+x_{n-1})}(1-e^{x_{n-1}})$ . By applying the change of variables theorem I get for the conditional density above: $$\frac{f_{X_{(1)}-x_n,...,X_{(n-1)}-x_n,X_{(n)}}(x_1,...,x_{n-1},x_n)}{f_{X_{(n)}}(x_n)}=
\\\quad \frac{f_{X_{(1)},...,X_{(n-1)},X_{(n)}}(x_1+x_n,...,x_{n-1}+x_n,x_n)}{f_{X_{(n)}}(x_n)}=
\\\quad \frac{n!f_{X_1}(x_1+x_n)...f_{X_1}(x_{n-1}+x_n)f_{X_1}(x_{n})}{{f_{X_{(n)}}(x_n)}}=
\\\quad \frac{n!e^{-(x_1+...+x_{n-1})}e^{-nx_n}}{ne^{-nx_n}}$$ This latter should be equal to the unconditional density that I calculated above. Any suggestions? Edit Could it be something like the following equations based on the iid and ‚Äúmemoryless‚Äù properties of the sample? $P[X_{(1)}-X_{(n)}<y_1,‚Ä¶,X_{(n-1)}-X_{(n)}<y_{n-1}| X_{(n)}=x_n]= \\= n! P[X_{1}-X_{n}<y_1,‚Ä¶,X_{n-1}-X_{n}<y_{n-1}| X_{n}=x_n]= \\=n!P[X_{1}<y_1+x_n,‚Ä¶,X_{n-1}<y_{n-1}+x_n| X_{n}=x_n]=\\=n!P[X_1<y_1]‚Ä¶P[X_{n-1}<y_{n-1}]$ Basically I am considering in the $n!$ permutation also the fact that the lowest observed value could come from any observation in the sample. Thus I condition on the (arbitrarily) first observation to be the lowest and no more on the minimum $X_{(n)}$ . Before I tried to divide the joint density of the ordered statistics by the density of the minimum. Is my reasoning right? Thank you.","['conditional-probability', 'probability-theory', 'density-function']"
4620136,Bayesian MAP inference with change of variables theorem,"Question: I want to do a Bayesian inference on a constrained variable. I want to use change of variables theorem to remove the constraint, before doing the Bayesian inference. How does the prior on the variable should change? Context: Say, I have a likelihood function, $P(y|\theta_a, \theta_b)$ . Further suppose that the parameter $\theta_a$ is unconstrained, yet the parameter $\theta_b$ is constrained such that $\theta_b \geq 0$ (non-negativity constraint). MLE solution is not very attractive, so I decided to use MAP estimate. To do so, we can use Bayes' rule as follows: $P(\theta_a, \theta_b|y) \propto P(y|\theta_a, \theta_b)P(\theta_a)P(\theta_b)$ Here, I am assuming $P(\theta_a, \theta_b) = P(\theta_a)P(\theta_b)$ , independence between the variables. Assuming we have a specific prior distribution for $\theta_a$ (not important for this question), but we want to use a uninformative uniform prior on $\theta_b$ , such that $p(\theta_b) \propto 1$ . Then the posterior reduces to $P(\theta_a, \theta_b|y) \propto P(y|\theta_a, \theta_b)P(\theta_a)$ Then the MAP solution can be obtained by solving the following optimization problem. $\hat{\theta}_a, \hat{\theta}_b = argmax_{\theta_a, \theta_b} L(\theta_a, \theta_b|y) + L(\theta_a)$ subject to $\theta_b \geq 0$ Where $L(\theta_a, \theta_b|y), L(\theta_a)$ are log-likelihoods. Now, I want to solve the above constrained optimization problem with an unconstrained optimization algorithm. So, I can change the variable, such that $\theta_b = e^{\theta'_b}$ . with this change of variable, we solve a different unconstrained optimization problem, $\hat{\theta}_a, \hat{\theta}'_b = argmax_{\theta_a, \theta'_b} L(\theta_a, e^{\theta'_b}|y) + L(\theta_a)$ The optimum $\hat{\theta}_b$ can be recovered by $\hat{\theta}_b = e^{\hat{\theta}_b'}$ The question is, don't we have to change the variable for the $p(\theta_b)$ ? Change of variable theorem will yield a different prior for $\theta'_b$ , like $p_{\theta'_b}(\theta'_b) = p_{\theta_b}(e^{\theta'_b})\lvert \frac{d\theta_b}{d\theta'_b} \rvert \propto e^{\theta'_b}$ ? Then adding the log-likelihood to the above unconstrained optimization problem will yield a different solution. I cannot seem to find what is wrong with these two trains of thoughts: one without changing the variable for $p(\theta_b)$ and one with the change of variable for $p(\theta_b)$ . They'll clearly yield different solutions. Why?","['parameter-estimation', 'bayesian', 'optimization', 'bayes-theorem', 'probability']"
4620138,Statistical guarantee on the effectiveness of a kernel density estimation,"I have recently started studying about kernel density estimations. For reference, if one is given if $(x_1,x_2,...,x_n)$ is an i. i. d. sample drawn from an (unknown) distribution with an unknown $f$ density function, then the kernel density estimator of $f$ is the following: $$\hat{f}_h(x) := \frac{1}{nh}\sum_{i=1}^n K \Big(\frac{x-x_i}{h} \Big),$$ where $K$ is the kernel. I started to look for any literature, which states something about the ""effectiveness"" of this estimation, such as 1 and 2 . What I seem unanble to find is a theoretical guarantee for the norm of the difference between the original and the estimated density function, namely the term $||\hat{f}_h - f||_p$ for some $p$ . Is there a theorem that states something as follows: for every $\varepsilon > 0$ , there exists a $\delta > 0$ , such that $$\mathbb{P}(||\hat{f}_h - f||_p > \varepsilon) < 1-\delta?$$ Any help is greatly appreciated!","['statistics', 'estimation', 'normed-spaces', 'density-function', 'reproducing-kernel-hilbert-spaces']"
4620159,"Proof $|A\setminus B| = |A| - |A\cap B|$ with $A$, $B$ finite sets via induction","How to show the following $ |A\setminus B| = |A| - |A\cap B|$ via induction (if possible). First approach was: Induction base case $ \forall B \in \epsilon_0 $ (countable-sets cardinality  0) $ \forall A \in \epsilon : |A\setminus B| = |A| - |A\cap B|$ , with $ B \in \epsilon_0 \implies |B| = 0 \iff B = \emptyset  $ . Hence $ |A\setminus \emptyset| = |A| - |A\cap \emptyset| \iff |A| = |A|  $ Induction step $ \forall B \in \epsilon_{n+1} \forall A \in \epsilon : |A\setminus B| = |A| - |A\cap B|$ , with assumption $ \forall B \in \epsilon_{n} \forall A \in \epsilon : |A\setminus B| = |A| - |A\cap B|$ However I cant seem to figure out how to use the assumption in the induction step. Possibly by using $D:=B \setminus \{x\} \in \epsilon_{n}  $ . Could some one provide some intuition on this or whether this approach is flawed ?","['elementary-set-theory', 'induction']"
4620172,Derivative of adjoint operator-valued function,"Consider an infinite dimensional complex Hilbert space $H$ . I think that for a bounded operator-valued function $A: x\mapsto A(x) \in \mathcal B(H)$ , where $x\in \mathbb R$ , we can define the derivative $A^\prime(x)$ as the (unique) operator which obeys $$\lim\limits_{h\to 0} \left\|A^\prime(x) - \tfrac{A(x+h)-A(x)}{h}\right\|_{\mathrm{op}} =0 \quad, \tag 1$$ if the limit exists. Here $\|\cdot\|_{\mathrm{op}}$ denotes the operator norm. Now I think that from $\|A(x)^*\|_{\mathrm{op}}=\|A(x)\|_{\mathrm{op}}$ , where $^*$ denotes the adjoint, we can show that the derivative of $A^*: x\mapsto A(x)^*$ exists at $x$ and is simply the adjoint of the derivative of $A$ at $x$ , i.e we have $$ (A^\prime(x))^* = (A^*)^\prime (x) \quad .\tag{2}$$ Question: Can we find the same result as in $(2)$ if we define the derivative of bounded operator-valued functions in the strong operator topology instead of the uniform topology $(1)$ ? Or is there a weaker but similar result under some conditions?","['operator-theory', 'hilbert-spaces', 'functional-analysis', 'adjoint-operators', 'derivatives']"
4620182,Calculate $\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \int_{0}^{1}\frac{x^{n+1}}{x+1}dx$,"Calculate $\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \int_{0}^{1}\frac{x^{n+1}}{x+1}dx$ I'd like to exchange the integral with the serie in order to do the calculus. I'm not sure under which conditions I can do the following (and why is permitted theoretically): $\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \int_{0}^{1}\frac{x^{n+1}}{x+1}dx$ = $ \int_{0}^{1}\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \frac{x^{n+1}}{x+1}dx$ Because then, knowing that $\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} x^{n+1} = ln(1+x)$ solving the integral becomes simpler.","['multivariable-calculus', 'calculus', 'sequences-and-series']"
4620205,Topology of rationals as subspace of the reals vs rationals with euclidean topology,"I am trying to compare different topologies on the set of rationals. I feel like the following two are distinct (i.e. some sets are open in one and not in the other), but I can't write a formal enough argument unfortunately. Any feedback is very appreciated! Here are the topologies on $\mathbb{Q}$ : $\tau_1$ is the subspace topology inherited from the real numbers (i.e. taking elements as $U \cap \mathbb{Q}$ for $U$ open in euclidean topology on $\mathbb{R}$ ) $\tau_2$ is the euclidean topology on the rationals (i.e. given by the basis of open balls centered on rationals) Let $c \in \mathbb{R} \setminus \mathbb{Q}, \epsilon \in \mathbb{Q}$ . My clain is that ""visually"" $(c - \epsilon, c + \epsilon) \cap \mathbb{Q} = U$ is open in $\tau_1$ but not in $\tau_2$ . I feel like this is not a valid counter-example, however this is the closest I found.",['general-topology']
4620207,Studying $\lim_{x \rightarrow 0^+} \frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}\cos(x^2)}$,"Study the following limit with $\alpha$ and $\beta$ parameters: $$\lim_{x \rightarrow 0^+} \frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}\cos(x^2)}$$ My attempt: $$\frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}\cos(x^2)} \;\sim_{0}\; \frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}}$$ Now I use Hospital three times and I have: $$\frac{(-12+96x^2-64x^4)e^{-2x^2}+\cos(x)+6 \beta}{\alpha (\alpha-1)(\alpha-2)x^{\alpha-3}}\;\sim_0\; \frac{-11 +6 \beta}{\alpha (\alpha-1)(\alpha-2)x^{\alpha-3}}$$ Now if $\beta = \frac{11}{6}$ and $\alpha \gt 3$ , I have $0 \cdot \infty$ indeterminate form. What can we say about the case $\beta = \frac{11}{6}$ and $\alpha \gt 3$ ?",['limits']
4620213,Self-adjoint operators and analytic semigroups,"Let $A$ be a self-adjoint operator on a Hilbert space such that $A$ is the generator of a $C_0$ -semigroup $(T_t)_{t\ge 0}$ . Must $(T_t)_{t\ge 0}$ be analytic? I know that self-adjoint operators on Hilbert space generate a bounded analytic semigroup if and only if they are sectorial. However, I was wondering if we're already given a self-adjoint operator is a generator, can we then guarantee that the semigroup would be analytic? I haven't been able to locate a counter-example.","['semigroup-of-operators', 'operator-theory', 'functional-analysis']"
4620242,Solving a cubic diophantine in integers $a^3 + b^3 + c^3 = 10^{2k}a + 10^{k}b + c$,"I wish to solve a cubic diophantine in integers. Here it is $a^3 + b^3 + c^3 = 10^{2k}a + 10^{k}b + c$ for $k = 1,2,3...$ I tried using the identity that $a^3 + b^3 + c^3 = (a+ b+ c ) ( a^2 + b^2 + c^2 ‚Äì ab ‚Äì bc ‚Äì ca ) + 3abc$ , but got nowhere. I also tried the following steps $a^3 + b^3 + c^3 - c = 10^{2k}a + 10^{k}b$ $a^3 - 10^{2k}a + b^3 - 10^{k}b + c^3 - c = 0$ $a(a^2 - 10^{2k}) + b (b^2 - 10^{k}) + c(c^2 - 1) = 0$ One thing that I have noticed is that all 3 digit Armstrong Numbers do satisfy this property. Any help would be appreciated!","['number-theory', 'algebra-precalculus']"
4620268,"Find $x$, where $3^{\log_6x} + 4^{\log_6x} + 5^{\log_6x} = x$","Find $x$ , where $3^{\log_6x} + 4^{\log_6x} + 5^{\log_6x} = x$ . As solutions, my workbook gives the following choices: a) $x\in (0,60)$ , b) $x \in(60,100)$ , c) $x \in (100, 200)$ , d) $x \in (200,300)$ , e) $x \in (300, \infty)$ What I tried so far is to switch the arguments of the logarithms with the base of the exponentials and got: $$x^{\log_63} + x^{\log_64} + x^{\log_65}= x$$ At this point, being stuck but given the a-e choices, I tried to guess the answer, but I was totally off. Without a calculator which I assume could help with finding the values of the logarithms, how would I go about finding $x$ here? Is there a way to approach such problems to find exactly the solution or at least a very narrow interval? Could analysis be used to speed up getting an approximate value of $x$ ?","['algebra-precalculus', 'logarithms']"
4620286,"Evaluate $\int_{0}^{1}\frac{K(x)\ln\left(1-x^2\right)}{\sqrt{1-x^2}}\text{d}x,\int_{0}^{1}\frac{xK(x)^2\ln\left(1-x^2\right)}{\sqrt{1-x^2}}\text{d}x$","I am recently interested in integrals containing an elliptic integral $K(x)$ , which is defined by $\int_{0}^{1} \frac{1}{\sqrt{1-t^2}\sqrt{1-x^2t^2}  }\text{d}t$ for $|x|<1$ and $x$ is the elliptic modulus. Then I come across the following two integrals $$
\begin{aligned}
&\int_{0}^{1} \frac{K(x)\ln\left ( 1-x^2 \right ) }{
\sqrt{1-x^2} }\text{d}x=-\frac{\Gamma\left(\frac14\right)^4}{24},\\
&\int_{0}^{1} \frac{xK(x)^2\ln\left ( 1-x^2 \right ) }{
\sqrt{1-x^2} }\text{d}x=-\frac{\pi^4}{4}\,_4F_3\left ( \frac12,\frac12,\frac12,\frac12;1,1,1;1 \right ).
\end{aligned}
$$ by estimation. Where generalized hypergeometric function ( $\,_pF_q$ ) is used. For the first one, I write $$
\begin{aligned}
\int_{0}^{1} \frac{K(x)\ln\left ( 1-x^2 \right ) }{
\sqrt{1-x^2} }\text{d}x
&=\frac{\pi}{2} \frac{\mathrm{d}}{\mathrm{d}n} \frac{\sqrt{\pi}\,\Gamma\left ( \frac{n+2}{2}  \right ) }{
\Gamma\left ( \frac{n+3}{2}  \right ) }\,_3F_2
\left ( \frac12,\frac12,\frac12;1,\frac{3+n}{2} ;1 \right )\Bigg|_{n=-1}\\
&=\frac{\pi}{2}\left [ -\pi\ln(2)\cdot\frac{\pi}{\Gamma\left ( \frac34 \right )^2 } 
+\pi \frac{\mathrm{d}}{\mathrm{d} n} \,_3F_2
\left ( \frac12,\frac12,\frac12;1,\frac{3+n}{2} ;1 \right )\Bigg|_{n=-1}\right ] .
\end{aligned}
$$ Edit: We just need to prove $$
\,_3F_2
\left ( \frac12,\frac12,\frac12;1,\frac{3+n}{2} ;1 \right )
\Bigg|_{n=-1}
=-\frac{(\pi-3\ln(2))\Gamma\left ( \frac14 \right )^4 }{12\pi^3}.
$$ Or we equivalently write( $H_n$ denotes harmonic numbers) $$
\sum_{n=0}^{\infty} \frac{\left ( \frac12 \right )_n^3 }{
(n!)^3}H_n=\frac{(\pi-3\ln(2))\Gamma\left ( \frac14 \right )^4 }{6\pi^3}.
$$ Which has been evaluated here . Question : Can we verify these closed-forms? Thanks for reaching my hand.","['integration', 'real-analysis', 'calculus', 'elliptic-integrals', 'hypergeometric-function']"
4620291,$\frac{Y}{Y+Z}$ is independent of $Y+Z$,"Let $$X = Y + Z,$$ where $Y,Z$ are independent continuous random variables. Can it happen that $$
X \text{ and }\frac{Y}{X} \text{ are independent}?
$$ If yes (I believe it can happen, but only in some weird degenerative case), can you somehow characterize such pairs of variables? I was trying to work it out by conditioning. Write $W(a):= [Y\mid X=a]$ which is a random variable $Y$ conditioned on $X=a$ . Then, $ W(a)/a \overset{d}{=} W(b)/b$ for all $a,b$ in the support of $X$ . However, explicitly writing down a distribution of $W(a)$ is typically non-trivial. Some other ideas on how to proceed?","['statistics', 'conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
4620319,"Finding $\cot(\beta)$, knowing $\sin(\alpha+\beta)=4/5$ and $\sin(\alpha-\beta)=3/5$","Let's assume that for $0<\beta<\alpha<\frac{\pi}{2}$ , $\sin(\alpha+\beta) = \frac{4}{5}$ , and $\sin(\alpha-\beta) = \frac{3}{5}$ . Then, how could we find $\cot(\beta)$ ? $$\sin(\alpha+\beta)+\sin(\alpha-\beta) = 2\sin(\alpha)\cos(\beta) = \frac{7}{5}$$ $$\sin(\alpha+\beta)-\sin(\alpha-\beta) = 2\sin(\beta)\cos(\alpha) = \frac{1}{5}$$ $$\tan(\alpha)\cot(\beta) = 7$$ But I am not sure where this would lead us.",['trigonometry']
4620357,"Are there such things as ""function bundles"", analogous to vector bundles?","I am interested in whether or not the typical vector bundle construction can be extended to cases where each fiber is an infinite dimensional vector space, possibly with other structure associated with it. I have tried to search along these lines but I am not sure if the answers I have found fully answer my question. This is motivated by the following problem, for context. I have a smooth manifold $M$ (actually, in my case $M$ is just $\mathbb{R}^n$ so it is trivially a manifold). At each point in $M$ , I want to be able to associate a function $f: [a,b] \to \mathbb{R}$ such that I have a ""function field"" on the manifold. If I were dealing with finite dimensional vectors as opposed to functions, I could do this by defining, for example, the tangent bundle $TM$ and vector fields would be a smooth section of the bundle. To provide more context, I have also have a functional $L(v)[\cdot]$ indexed by $v \in \mathbb{R}^n$ , and I want to choose the specific $f$ at each point that will maximize this functional. This seems to me to be defining a section of a ""function bundle"" of sorts, which would be useful to me as I am hoping to be able to compute information about the $f$ at nearby points given the $f$ associated with one point, i.e. to ""move along the section"" in a well defined way. I apologize if this is too vague of a question. I have spent some time searching and have not encounter a construct quite like this. I am aware that you can have Banach bundles in which each fiber is infinite dimensional, but the examples of that I have seen are such that both the base space and the fibers are infinite dimensional Banach spaces. In my case the base space is finite dimensional. I would appreciate even just a pointer in the right direction, i.e. some sources or keywords to follow up on. Thank you very much.","['vector-bundles', 'functional-analysis', 'differential-geometry']"
4620365,Fisher information of poisson distributed random variable,"Let's consider a printer queue. We know that the expected number of printer jobs almost obeys a Poisson distribution, so $P_{\vartheta}(X=k)=e^{-\vartheta}\frac{\vartheta^k}{k!}$ , where $\vartheta\in]0,\infty[$ . We estimate the expected number of printer jobs $\vartheta$ by $\frac{1}{n}\sum\limits_{i=1}^nX_i$ . Compute the Fisher information $I(\vartheta):=\mathbb{E}_{\vartheta}\left(\left(\frac{d\ln(P_{\vartheta}(X))}{d\vartheta}\right)^2\right)$ . We know that if $(X_1,\dots,X_n)$ are independent random variables with a distribution like $P_X(\vartheta)=f(X_1,\vartheta)\dots f(X_n,\vartheta)$ , then $$\mathbb{E}_{\vartheta}\left(\left(\frac{d\ln(P_{\vartheta}(X))}{d\vartheta}\right)^2\right)=n\cdot \mathbb{E}_{\vartheta}\left(\left(\frac{d\ln(P_{\vartheta}(X_i))}{d\vartheta}\right)^2\right).$$ If we consider the $n$ -many independent observations $X:=(X_1,\dots,X_n)$ , where each $X_i$ is the number of printer jobs in a certain period of time, the probability is given by \begin{align*} 
&P_{\vartheta}(\{X=(x_1,\dots,x_n)\})=P_{\vartheta}(\{X_1=x_1\})\cdots P_{\vartheta}(\{X_n=x_n\})\\ 
&=\frac{e^{-\vartheta}\vartheta^{x_1}}{(x_1!)}\dots \frac{e^{-\vartheta}\vartheta^{x_n}}{(x_n!)}=\frac{e^{-n\vartheta}\vartheta^{\sum\limits_{i=1}^nx_i}}{\prod\limits_{i=1}^n(x_i!)}. 
\end{align*} Applying the above statement yields after some manipulations $I(\vartheta)=\frac{n}{\vartheta}$ . However, the sample solution says: \begin{align*}
&I(\vartheta)=\sum\limits_{x=0}^{\infty}\frac{\left(\frac{d\ln(P_{\vartheta}(X))}{d\vartheta}\right)^2}{P_{\vartheta}(X)}=\sum\limits_{x=0}^{\infty}\frac{1}{x!}e^{\vartheta}\vartheta^{-x}\left(-e^{-\vartheta}\vartheta^x+e^{-\vartheta}x\vartheta^{x-1}\right)\\
&= \sum\limits_{x=0}^{\infty}\frac{\vartheta^x}{x!}e^{-\vartheta}\left(\frac{x}{\vartheta}-1\right)^2=\mathbb{V}(X)\frac{1}{\vartheta}^2=\frac{1}{\vartheta}.
\end{align*} This makes no sense to me? Why $x\to\infty$ and why is there only one random variable instead of a vector which represents $n$ -many obersavtions?","['statistics', 'proof-explanation', 'expected-value', 'fisher-information', 'random-variables']"
4620383,"Why are coercive functions called coercive, and why is it useful?","A function $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is called coercive if $$ \frac{f(x)\cdot x}{\|x\|} \rightarrow \infty \;\; \text{as} \;\;\|x\|\rightarrow \infty.$$ I came across this requirement in calculus of variations, where a coercivity condition is needed to show that a sequence of functions gamma-converges to some limiting function. It seems related to showing that the compactness condition that usually accompanies Gamma-convergence - does coerciveness imply compactness? I don't see how they are related, but they seem to be. Edit: The ""compactness condition"" I refer to is that any sequence of arguments $\{u_n\}_{n\in \mathbb{N}}$ has a convergent subsequence. I think that this compactness condition is implies equi-coerciveness of the sequence $\{u_n\}$ . I don't know why coerciveness is used in proofs of gamma-convergence instead of compactness directly. Anyways, I still want to know why the functions are called ""coercive"".","['calculus-of-variations', 'functional-analysis', 'analysis', 'terminology']"
4620418,What is the value of the following limit: $\lim\limits_{n\to \infty} 4^n(1-a_n)$ where $a_{n+1} = {\sqrt{\frac{1+a_n}{2}}}$?,"I have the following recursive $(a_n)_{n>0}$ sequence defined with $a_{n+1} = {\sqrt{\frac{1+a_n}{2}}}$ and where $-1 \leqslant a_n \leqslant 1$ . And I must calculate the following: $$\lim\limits_{n\to \infty} 4^n\left(1-a_n\right)$$ I tried figuring it out with proving that the sequence is decreasing, but I could not prove it with any way. The limit calculated is clearly $\,1\,$ , but it just give the case of $\,\infty\cdot0\,$ which is not very helpful. I can think of the Cesaro-Stolz theorem maybe as a starting point, but could not work it out. Thank you in advance for helping out!","['limits', 'calculus', 'recursion', 'sequences-and-series']"
4620449,"Simplify $\sum_{k=0}^a\binom ak\prod_{j=0}^{a-1}(b-k-j)$, the number of injections from $A$ to $B$","The number of injections from the set $A$ to the set $B$ is $\prod_{k=0}^{a-1}(b-k)$ , typically denoted by the Pochammer symbol $(b)_a$ , where $a=|A|$ and $b=|B|$ . I'm interested in determining the same number, but for injections that are "" partial functions "", i.e., functions where not all members of $A$ are assigned an output. In other words, $f(x)$ does not exist for all $x\in A$ , but when it does, we still have that $f(x)=f(y)\implies x=y$ . Suppose that precisely $k$ out of the members of $A$ get no output, these can be chosen in $\binom ak$ ways. The remaining $a-k$ all get an output, so there are $(b)_{a-k}$ injections in this case. Summing over $k$ gives the answer, $$\sum_{k=0}^a\binom ak(b)_{a-k}.$$ I'm wondering if it's possible to simplify this quantity in some way.","['elementary-set-theory', 'functions', 'combinatorics']"
4620513,Understanding Erlangen Program,I am trying to understand the idea of Erlangen Program. Roughly I have understood that it says geometry is the study of invariants under certain transformations. But the following is not clear to me: Suppose we have a set $X$ and transformation set $T$ . Then we have to study about the subsets $Y$ of $X$ satisfying $g(Y)=Y$ for all $g\in T$ or we have to study some properties of $X$ which are invariant under transformations? Here is a reference: https://encyclopediaofmath.org/wiki/Erlangen_program,"['abstract-algebra', 'linear-algebra', 'geometry']"
4620524,Maximizing/minimizing $f(\theta) = \sqrt{2}\cos(\theta)-4\sin(\theta)$,"Assume that $f : [0, 2\pi]\rightarrow \mathbb{R}$ is a function such that $f(\theta) = \sqrt{2}\cos(\theta)-4\sin(\theta)$ . Then, how can we maximize/minimize $f$ ? We can re-parametrize our function $f$ by defining another function $g : [-1, 1]\rightarrow \mathbb{R}$ function such that for every $t\in [-1, 1]$ , $$g(t) = \sqrt{2}\sqrt{1-t^2}-4t$$ $$\frac{dg}{dt} = \frac{d}{dt}\left(\sqrt{2}\sqrt{1-t^2}-4t\right) = \frac{\sqrt{2}t}{\sqrt{1-t^2}} + 4 = 0$$ From which we conclude that $g$ attains its maximum/minimum at $\left(-\frac{2\sqrt{2}}{3}, g\left(-\frac{2\sqrt{2}}{3}\right)\right), (1, g(1))\in \mathbb{R}^2$ respectively.","['calculus', 'trigonometry']"
4620608,What is the meaning of the slope of the tangent line at a point to a parametrically defined curve that is not smooth at that point?,"Suppose you have a parametric curve $r(t) = (x(t),y(t))$ . From my understanding, we typically require a smoothness condition that its derivative is not equal to the zero vector for all $t$ in $r(t)$ 's domain. Suppose we ignore that condition. Ignoring some potential uninteresting edge cases, it is easy to find two polynomials $x(t)$ , $y(t)$ such that there exists a $c$ so that $c$ is a non-repeated root of $x'$ and $y'$ . If we then define ""the slope of the line tangent to the curve $r(t)$ "" as $m = \lim_{t\rightarrow c} \frac{y'(t)}{x'(t)}$ then this limit will exist and the equation of the tangent line can be written as $$y = m(x-x(c)) + y(c).$$ It seems to me that this is the ""right"" way to try to define a ""tangent line"" and you could relax the smoothness condition to allow the case when $(x'(c),y'(c)) = (0,0)$ but the limit of $m$ exists at $c$ . My question is then, what am I actually doing here when I apply this procedure? Is this definition of a tangent line reasonable and does this slope really tell me anything? I suppose as a follow up, is this equation of the tangent line unique and independent of the parameterization?",['multivariable-calculus']
4620663,How many strict local minima a quartic polynomial in two variables might have?,"What is the maximum number $N$ of strict local minima that a degree 4 polynomial $p:\Bbb R^2\to \Bbb R$ can have? One dimension: For a single variable quartic polynomial $q:\Bbb R \to \Bbb R$ the answer to this question would be easy: The polynomial has at most two local minima. Indeed, the derivative $q'$ is a cubic polynomial, so by Fundamental Theorem of Analysis $q$ has at most 3 critical points. However, when $q$ has 3 critical points, then all of them are simple roots, and so two consecutive critical points of $q$ can not be both local minima (local minima/maxima are alternating). Along the lines: My first thought was to study $p$ along the lines $(x,y)=(a+bt,c+dt)$ as one would do to prove that a quadratic polynomial $p:\Bbb R^2\to \Bbb R$ has no more than one strict local minimum. However, this argument fails in the attempt to show that a quartic polynomial $p$ has at most two strict local minima. The problem is that unlike two points, three points in $\Bbb R^2$ can not in general be interpolated by a line. Three points in $\Bbb R^2$ can be interpolated by a quadratic curve like $(x(t),y(t))=(t,at^2+bt+c)$ , but then $t\mapsto p(x(t),y(t))$ could be a polynomial of degree $8$ . At lest 4: Since the polynomial $q(t)=(t^2-1)^2$ has minima at the two points $t=\pm 1$ , the polynomial $p(x,y) = q(x) + q(y)$ has minima at the four points points $(x,y)=(\pm 1,\pm 1)$ . Thus $N\geq 4$ . At most 9: As commented by @GerryMyerson, the partial derivatives $p_x,p_y$ of $p$ are cubic polynomials, and so by B√©zout's Theorem the curves $p_x(x,y)=0$ and $p_y(x,y)=0$ intersect at no more than $3\times 3$ isolated points. Hence, $p$ has no more than 9 critical points. The question then is how many critical points $p$ needs to have in order to have $N$ strict local minima: For the case of $N=2$ and a general function $f$ this was answered in the post: If a two variable smooth function has two global minima, will it necessarily have a third critical point? I asked the related question for a degree 4 polynomial and $N\geq 2:$ Is there a quartic polynomial in two variables that have multiple local minima and no other critical points? If you could not determine the exact value of $N$ , could you at least give some lower or upper bound, like that $N>4$ or that $N$ is finite?","['roots', 'real-analysis', 'maxima-minima', 'multivariable-calculus', 'polynomials']"
4620694,Are Characteristic Functions Analytic?,"Let $X$ be a real random variable defined on a probability space $(\Omega, F, P)$ . Define its characteristic function $\phi: \mathbb{R} \to \mathbb{C}$ by $\phi(t) = \mathbb{E}[e^{itX}]$ for every real $t$ . Is $\phi$ necessarily analytic on $\mathbb{R}$ ? In other words, is $\phi$ locally representable by power series in $\mathbb{R}$ ? This should be the case if $X$ has moments of all orders. But can this hypothesis be weakened?
Specific counterexamples are appreciated.","['characteristic-functions', 'real-analysis', 'power-series', 'probability-theory', 'analytic-functions']"
