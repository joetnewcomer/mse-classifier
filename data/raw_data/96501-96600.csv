question_id,title,body,tags
1329370,"How can the limit exist for a function on an interval (a,b) but not be continuous on that interval?","This is from a practice test true/false question. The statement I was given is that If $\lim_{(x,y)→(a,b)} f(x, y)$ exists, then $f(x, y)$ is continuous at $(a, b)$. I put True but the answer is False. I don't see how how this can be untrue?","['calculus', 'limits', 'multivariable-calculus']"
1329398,"Quartic Solution on Wikipedia special cases problem $S=0$ how to ""change the choice of cubic root""?","So, I've posted a question regarding Wikipedia's quartic page.  This was from the first question. I'm trying to implement the general quartic solution for use in a ray tracer, but I'm having some trouble. The solvers I've found do cause some strange false negatives leaving holes in the tori I'm testing with. Most implementations use the depressed quartic solutions, I don't understand the math involved and can't figure out why I'm having false non-intersections (link to layman explanation would be great). So I'm trying to implement the general solution at this wikipedia page . I got the stuff up until the special cases implemented, but at that point I have an issue. With lots of rays being traced most of the special cases become common.  I've found a set of coefficients that Wolfram Alpha tells me has two real roots , but my code was just returning NaN, further searching I found my S was coming up as $\sqrt{-4.9 \times 10^{-11}}$ Floating point precision error, means this should equate to 0, so I need the special case for S=0, it says we need to ""change choice of cubic root in Q"" but it does not explain how to do this.  I did try changing the sign of Q when S=0, but that doesn't work.  Does anyone know what this means and how I can do it?","['polynomials', 'algebra-precalculus']"
1329426,Question about set of all functions and the power set of a set,"Hi so we know the set of all functions from a set X $\phi \rightarrow$ {0,1} create a one to one correspondence from the power set of X to the set of all functions but we are looking at certain subsets of the set of all functions. What if we look at the set of all permutation of a particular set X will it also be one to one correspondence with the power set ?",['elementary-set-theory']
1329444,Is the following a conic section,"All vectors are in $\mathbb{R}^3$ and only $\mathbf{r} = \left[ x; y; z \right]$ is unknown. My question is does the following system define a conic section in the $x-y$ plane and, if so, how can I find it: $$
\begin{align}
\mathbf{r}^\mathrm{T} \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array} \right]
 & = 0 \\
\\
\mathbf{v}_1^\mathrm{T} \frac {\mathbf{r} - \mathbf{r}_1} {\Vert \mathbf{r} - \mathbf{r}_1 \Vert } + \mathbf{v}_2^\mathrm{T} \frac {\mathbf{r} - \mathbf{r}_2} {\Vert \mathbf{r} - \mathbf{r}_2 \Vert } & = c
\end{align}
$$ If either $\Vert \mathbf{v}_1 \Vert = 0$ or $\Vert \mathbf{v}_2 \Vert = 0$, then the above is the intersection of a cone and the $z=0$ plane. Likewise if $\mathbf{r} = \mathbf{r}_1$ or $\mathbf{r} = \mathbf{r}_2$. However, I have been unable to figure out what the above represents in the general case.","['geometry', 'conic-sections', 'analytic-geometry']"
1329463,Composition of almost everywhere continuous functions $\mathbb{R}\rightarrow\mathbb{R}$,"If $f,g:\mathbb{R}\rightarrow\mathbb{R}$ are continuous it is well known that the composition $f\circ g$ is also continuous. But what happens if we assume that $f,g$ are only almost everywhere continuous? Is the composition also almost everywhere continuous? The question Composition of almost everywhere differentiable functions makes me believe the answer is negative, but I am unable to provide a counterexample for the case $f,g:\mathbb{R}\rightarrow \mathbb{R}$.","['real-analysis', 'measure-theory']"
1329487,Number of graphs such that two sides remain connected after some edges are removed,"This problem was actually from a programming problem, but it has more of a math flavor, so I am asking on Math stack exchange! Problem: Initially, you are given a graph as in the first image, where red lines are edges and grey circles are vertices. You want to reach from the top platform to the bottom platform. Now, 0 or more edges are deleted from the graph. How many such configurations of graph are there so that there is a path from the top to the bottom? The second image is an example of a configuration where it is possible to reach from the top to the bottom, and the third image is an example of a configuration where it is impossible. The hint I got was that this problem is only solvable (or relatively easily solvable) only when the graph is N by N + 1 (the picture shows a graph of 3 by 4). I don't really want an exact solution, but rather some hints on how to proceed (completely stuck at the moment). I tried setting up some recurrence relation, but had trouble doing so.","['graph-theory', 'combinatorics']"
1329510,Why is this not a proof of Schroeder-Bernstein?,"We can show that if $f: A \rightarrow B$ is injective then $|A| \leq |B|$ and if $g: B \rightarrow A$ is injective then $|B| \leq |A|$ so $|A| = |B|$. By the definition of having equal cardinality, there exists a bijection between $A$ and $B$. Whenever a textbook proves the theorem, however, a more complicated proof is shown. I must be missing something.","['elementary-set-theory', 'relations', 'cardinals']"
1329525,What is wrong with my application of Lebesgue Dominated Convergence Theorem in these two examples?,"Background I seem to be having issues recognizing valid bounding functions when applying the Lebesgue Dominated convergence theorem. Here are two examples I did that I do not think are justified. Example 1: Show that $$\lim_{n\to\infty} \int_0^\infty ne^{-nx}\sin(1/x) \ dx$$ exists and determine its value. Solution: Let $u=nx$. Then we may rewrite the integral as $$\int_0^\infty e^{-u}\sin\left(\frac{n}{u}\right) \ du.$$
Note that $$\bigg\lvert e^{-u}\sin\left(\frac{n}{u}\right)\bigg\rvert\leq e^{-u},$$ and since $e^{-u}$ is integrable, the LDCT applies. The limit is then $0$. Example 2: Let $g:\mathbb{R} \rightarrow \mathbb{R}$ be integrable and let $f:\mathbb{R} \rightarrow \mathbb{R}$ be bounded, measurable, and continuous at $1$. prove that $$\lim_{n\to\infty}\int_{-n}^nf\left(1+\frac{x}{n^2}\right)g(x) \ dx$$ exists and determine its value. Solution: $f$ is bounded so for all $x$,$|f|\leq K$ for some $k \in \mathbb{R}$. Thus $$\bigg\lvert f\left(1+\frac{x}{n^2}\right)g(x) \chi_{[-n,n]}\bigg\rvert\leq k|g(x)|.$$ Since $k|g(x)|$ is integrable, we again have by the LDCT that the integral converges. The limiting value is $\int_\mathbb{R}f(1)g(x) \ dx.$ My Question I am not really looking for the solution to these problems. Rather I wish to know how I am applying the LDCT wrong in both of these (I am fairly sure that I am applying it wrong).","['lebesgue-integral', 'measure-theory']"
1329530,"Prove that the equation $az^3-z+b=e^{-z}(z+2)$ has two solutions in the right half-plane $\{z\in\mathbb{C}\,:\,\Re z>0\}$ when $a>0$ and $b>2$.","Prove that the equation
  $$
az^3-z+b=e^{-z}(z+2)
$$
  has two solutions in the right half-plane $\{z\in\mathbb{C}\,:\,\Re z>0\}$ when $a>0$ and $b>2$. This is an old qualifying exam question. I'm sure it uses Rouché's theorem somehow, but I can't quite figure it out. From my experience, the way Rouché's theorem works is that you basically create an equation that you know has the right number of zeroes by adding/subtracting terms from your original equation, then compare it to your original. The problem is I can't create an equation which has the number of zeroes I want. The best choice I have is comparing it to $az^3-z+b$, but after I would even show that it has the same number of zeroes as $az^3-z+b-e^{-z}(z+2)$, I would have to somehow justify that $az^3-a+b$ has $2$ zeroes in the right half-plane. I also considered using the conformal map $z\mapsto\frac{1-z}{1+z}$ and working with the unit disk as the domain which I generally find to be easier to work with (as opposed to using some arbitrarily large rectangle), but it doesn't seem to help. Any help is greatly appreciated. Thanks in advance.","['roots', 'complex-analysis']"
1329539,How to solve the Few Scientists Problem (big word problem) in its general form?,"I'm trying to figure out how to solve this word problem. I'm pretty sure it involves calculus or something even harder, but I don't know how to solve the general form . Let me start with the concrete form, however: Concrete Form: You start with 5 scientists. A scientist can train 50 students for 5 years, after which each student becomes a scientist. (Assume a perfect graduation rate always, and assume you have an infinite population from which to draw students). Or a scientist can work on a project. The problem is you have 30 Type-A projects, 50 Type-B projects, and 75 Type-C Projects, and they all need to be completed in minimal time. Each Type-A Project requires at least 10 scientists and takes 200/x years to complete, where x is the number of scientists assigned to them. Type-B's require at least 18 scientists and take 150/x years to complete. Type-C's require at least 25 scientist and take 120/x to complete. What is the minimum time necessary to complete all projects, and what is the ""event-order"" of such an optimal solution? I could solve this numerically by doing simulations in a computer program (although that will still be a pain in the neck), but what I really need is how to solve this in its general form. General Form: Just assign constants to everything. You start with s scientists, who can train t students for y years. There are A type-a projects, B type-b's, and C type-c's. Respectively, they require a minimum of d, e, and f scientists, and take g/x, h/x, and i/x years to complete. How do you go about solving this? Is that even possible? Solving this requires finding an optimal solution (completing all projects in minimal time), and proving that no other solution exists that has a smaller finish time. EDIT: Thanks to @Paul for this clarification. For projects, scientists can join or leave at any time. This is indiscrete time. For training, however, only 1 scientist can train a group of 50. (Two scientists training 50 does not make it go 2x faster.) The training has to be ""atomic"", which I think is the right word.","['optimization', 'calculus', 'multivariable-calculus']"
1329540,Measuring incoming communication in a Markov Model,"Given a standard Markov Chain on discrete time and finite statespace, represented by a matrix $M$, with $\sum_{j=1}^d m_{ij}=1$. I have a certain absorbing state k, where the incoming communication is very interesting, so i wish to measure that somehow . Calculating $(I-T)^{-1}$ is not possible due to numerical issues. Until now i have just calculated $e_n=\sum_{i=1}^d m^n_{ik}-m^n_{kk}$. Where $m^n_{ij}$ is the i,j entry in $M^n$ for different $n$ values. For example we could have
$$M=\left(\begin{array}{c} 0.5 & 0.3 & 0.2
\\0.1 & 0.4 & 0.5 
\\ 0 & 0 & 1  \end{array}\right).$$
where we are interested in the communication to state 3. (The real matrix is very large). We would get
$e_1=0.2+0.5=0.7.$ My question is:
Is there a better way to describe incoming communication to a state, than the column sum? If not, how do i interpret it then? If not, what can you suggest?","['probability', 'statistics', 'markov-chains']"
1329553,Prove a matrix of binomial coefficients over $\mathbb{F}_p$ satisfies $A^3 = I$.,"(This problem is problem $1.16$ in Stanley's Enumerative Combinatorics Vol. 1 ). Let $p$ be a prime, and let $A$ be the matrix $A = \left[\binom{j+k}{k} \right]_{j,k = 0}^{p-1}$, taken over the field $\mathbb{F}_p$. Show that $A^3 = I$, the identity matrix. (Note that $A$ vanishes below the main antidiagonal, i.e. $A_{jk} = 0$ if $j + k \geq p$).  Moreover, how many eigenvalues of $A$ are equal to $1$? In the case of $p = 5$, for instance, we have $$A = \left(\begin{array}{ccccc}
1 & 1 & 1 & 1 & 1\\
1 & 2 & 3 & 4 & 0\\ 
1 & 3 & 1 & 0 & 0\\
1 & 4 & 0 & 0 & 0\\
1 & 0 & 0 & 0 & 0\\
\end{array} \right). $$ Somewhat surprisingly, we have $$A^2 = \left(\begin{array}{ccccc}
0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & 4 & 1\\ 
0 & 0 & 1 & 3 & 1\\
0 & 4 & 3 & 2 & 1\\
1 & 1 & 1 & 1 & 1\\
\end{array} \right), $$
that is the matrix flips horizontally and vertically; this can also be seen as reflecting across the main antidiagonal.  In all other cases I've tested, this held as well; the strategy I came up with for proving $A^3 = I$ is to show that this flipping occurs, and that the flipping happens again when squaring $A^2$.  This would show $A^4 = A$.  Since $A$ is clearly full rank, this would imply $A^3 = I$. Numerically, this flipping is captured in the statement $$ 
(A^2)_{i,j} = \sum\limits_{k = 0}^{p-1}\binom{i + k}{k} \binom{k + j}{j} \equiv \binom{2p - i - j - 2}{p - i - 1} \pmod p.
$$ I haven't been able to make any sense of this equivalence, combinatorially or otherwise; I've also made no progress on the eigenvalue part of the question, but admittedly I haven't spent as much time thinking about it.  Any help---even if it has nothing to do with the strategy that I've tried---is greatly appreciated. EDIT: I just figured out how to prove $A^3 = I$, and am now working on the eigenvalue problem.  Here's the proof for $A^3 = I$. Lemma: $A_{p - i - 1,j} \equiv (-1)^{j}\binom{i}{j} \pmod{p}$.
Proof of Lemma: We note that $A_{p - i - 1,j} = \binom{p - i + j - 1}{j}$ is a polynomial in $p$ with integer coefficients.  Thus, when viewed $\pmod{p}$, we have that it is equivalent to its constant term.  The constant term is $$ \frac{(-i + j - 1)(-i + j - 2)\cdots(-i)}{j!} = (-1)^j\frac{i(i-1)\cdots(i - (j - 1))}{j!} = (-1)^{j}\binom{i}{j} $$ as claimed. We now have \begin{align*}
A^2_{p-i-1,p-j-1} &= \sum\limits_k A_{p-i-1,k}A_{k,p-j-1} \\
&= \sum\limits_k A_{p-i-1,k}A_{p-j-1,k} &\text{ due to symmetry of } A \\
&= \sum\limits_k (-1)^{2k}\binom{i}{k}\binom{j}{k} &\text{ by the Lemma} \\
&= \sum\limits_k \binom{i}{k}\binom{j}{j - k} \\
&= \binom{i + j}{j} &\text{ by Vandermonde's Identity} \\
&= A_{i,j}.
\end{align*} Similarly, we have \begin{align*} 
A^4_{i,j} &= \sum\limits_k A^2_{i,k} A^2_{k,j} \\
&= \sum\limits_k A_{p - i - 1,p - k - 1} A_{p - k - 1, p - j -1}\\
&= \sum\limits_k A_{p - i - 1,p - k - 1} A_{p - j - 1, p - k -1} &\text{ by symmetry of }A \\
&= \sum\limits_k (-1)^{p - k - 1}\binom{i}{p - k - 1}(-1)^{p - k -1}\binom{j}{p - k -1} &\text{ by the Lemma} \\
&= \sum\limits_k \binom{i}{p - k - 1}\binom{j}{p - k - 1} \\
&= \sum\limits_k \binom{i}{k} \binom{j}{k} &\text{ by redefining }k \\
&= \binom{i + j}{j} &\text{ by Vandermonde}\\
&= A_{i,j}. 
\end{align*} Thus $A^4 = A$.  Since $A$ is full rank, this implies that $A^3 = I$, as desired.","['binomial-coefficients', 'combinatorics']"
1329555,Can we see directly from the cocycle condition that 2-cocycles are symmetric?,"Let $A$ be an abelian group and let $C$ be a cyclic group. All central extensions of $C$ by $A$ are abelian because in any such extension $$ 1\rightarrow A\rightarrow E\rightarrow C\rightarrow 1$$ any lift $x$ of a generator for $C$ to $E$ generates $E$ over $A$. Since $A\subset E$ is central, $x$ commutes with $A$, thus $E=\langle x,A\rangle$ is abelian. Any 2-cocycle $f\in Z^2(C,A)$ allows the construction of such an $E$, in the usual way: $E:= \{(a,c):a\in A, c\in C\}$, with the group operation given by $$(a,c)(a',c') = (aa'f(c,c'), cc')$$ since the action of $C$ on $A$ is trivial. Since $E$ is abelian by the above argument, this formula implies that $f$ is symmetric in its inputs! Since this works for any cocycle, it must be that the symmetry is forced only by the cocycle condition $$f(x,y)+ f(xy,z) = f(y,z) + f(x,yz)$$ and the fact that $C$ is cyclic. Is there a direct way (without going through the group extensions) to see that the cocycle condition together with $C$ being cyclic imply that $f(x,y) = f(y,x)$? My first thought was to let $x=r^a, y=r^b,z=r^c$, where $r\in C$ is a generator, and substitute into the cocycle condition. This basically reduces to the case $C=\mathbb{Z}$, but I don't see what to do next. The argument must use the cyclicness in an essential way, i.e. just abelianness is insufficient, since there do exist nonabelian central extensions of (noncyclic) abelian groups (e.g. $Q_8$ or more generally any extraspecial group).","['homology-cohomology', 'homological-algebra', 'group-theory', 'finite-groups', 'group-cohomology']"
1329575,Simplifying the equation $\log y = 10 + 0.5x$,"Solve for $y$. When expressed in simplest form, what familiar kind of equation results?
  $$\log y = 10 + 0.5x$$ For this question, I would get rid of log first right? So, I would get $$\begin{align*}
10^{\log y}&= 10^{10 +0.5x}\\
y &= 10^{10 + 0.5x}? 
\end{align*}$$
     Yea... I don't know what I'm doing.. I don't think that's the answer. Can someone help explain?","['algebra-precalculus', 'logarithms']"
1329596,Can you say that the function $ y(x)=1/(1/x)$ admits no solution for $x = 0$?,"Can you say that the function $ y(x)=1/(1/x)$ admits no solution for $x = 0$? Or is that function too ""close"" to y(x)=x to say that?","['linear-algebra', 'limits', 'algebra-precalculus']"
1329602,"Falling factorial counts permutations, what does rising factorial count?",Rising factorial example: Let $x = 7$ and $r = 4$. Then $7^{(4)} = 7(8)(9)(10) = 5040$. If we divide $7^{(4)}$ by $4!$ it counts multisubsets. But what kind of combinatorial problem does rising factorial solve on its own?,['discrete-mathematics']
1329611,Fixed Point with Permutation,"I have this homework question I'm kind of stumped on... ""A permutation of the numbers $(1,2,3,\ldots,n)$ is a rearrangement of the numbers in which each number appears exactly once. For example, $(2,5,1,4,3)$ is a permutation of $(1,2,3,4,5)$. Let $\pi = (x_1,x_2,x_3,\ldots,x_n)$ be a permutation of the numbers $(1,2,3,\ldots,n)$. A fixed point of $\pi$ is an integer $k~(1\le k\le n)$ such that $x_k=k$. For example, $4$ is a fixed point of the permutation $(2,5,1,4,3)$. How many permutations of $(1,2,3,4,5,6,7)$ have at least one even fixed point?"" Here's my work so far. Am I going in the right direction? Should I be thinking differently? $(1)$ can have 1 fixed point permutation. $(1,2)$ can have 1 fixed point permutation $(1, 2, 3)$ can have 4 fixed point permutations? $(1, 2, 3, 4)$ can have 15 fixed points? I couldn't find any correlations or patterns here... $(1)$ can have 0 even fixed point permutations $(1, 2)$ can have 1 even fixed point permutation $(1, 2, 3)$ can have 2 even fixed point permutations $(1, 2, 3, 4)$ can have 6 even fixed point. It's not factorials, or triangular numbers, so I'm kind of stuck here. What should I do next?",['combinatorics']
1329632,How to compute the derivative of $\sqrt{x}^{\sqrt{x}}$?,I know have the final answer and know I need to use the natural log but I'm confused about why that is. Could someone walk through it step by step?,"['derivatives', 'logarithms', 'calculus', 'integration']"
1329657,The convex hull of every open set is open,"Let $X$ be a topological vector space. Prove that the convex hull of every open subset of $X$ is open. I tried using definition of Convex Hull and Open Set, but I couldn't prove the statement.","['convex-analysis', 'functional-analysis', 'topological-vector-spaces']"
1329661,"If two metric spaces are Cauchy-equivalent, then are they topologically equivalent?","If two metrics $d_i$ on the same set $X$ have the same Cauchy sequences (i.e. if a sequence is Cauchy for the first metric, it is also Cauchy for the other one and vice versa), does this imply that the two metric spaces are equivalent (i.e. generate the same topology)? Please help!","['metric-spaces', 'cauchy-sequences', 'general-topology']"
1329667,How can I use a precise definition to find values of delta that correspond with given epsilon values,"I have been given this problem:
For the limit $$\lim_{x\to 2}({x^3-3x+4})=6$$ illustrate ""Definition 2"" (I have included this below) by finding values of $\delta$ that correspond to $\varepsilon=0.2$ and $\varepsilon=0.1$ ""Definition 2:"" My textbook says to this definition ""Let $f$ be a function defined on some open interval that contains the number $a$, except possibly $a$ itself. Then we say that the limit of $f(x)$ as $x$ approaches $a$ is $L$, and we write $$\lim_{x\to a}f(x)=L$$
if for every number $\varepsilon>0$ such that $$if\;0<|x-a|<\delta\;\;\;then\;|f(x)-L|<\varepsilon$$ This has been a seriously frustrating problem, I've been working on it for two days, I just don't understand this concept at all. The only thing I think I understand is that the values of $\delta$ correspond to values on the x-axis, and $\varepsilon$ is related to the limit. How do I go about solving this problem?","['calculus', 'limits', 'definition']"
1329760,Proving that a compact subset of a Hausdorff space is closed,"I am having trouble understanding the answers here . I am trying to prove that a compact subset of a Hausdorff space is closed. Following the proof is difficult, perhaps because Brian reused letters for different things(although I get they are arbitrary, I can't follow it.) The second answer uses nets and filters, which I don't know of, so would like to avoid for now. Now trying to follow Brians proof, I get something like the following: Let $S$ be our Hausdorff space, and $C\subset S$ be a compact subspace of $S$. (easy to remember, think $S$ for space, $C$ for compact) To show that this subspace is closed, we can show the complement of the subspace is open. I.e. $S/C$ is open. Now following the proof of Brian, we take some element $x\in S/C$ and since $S$ is Hausdorff(which means $C$ is Hausdorff), we have for each $y\in C$ that there are disjoint open sets $U_y,V_y$ such that $x\in U_y$ and $y\in V_y$ now here is a little confusing, either he has reused $x$, or for some reason $x\in C$, which shouldn't be possible, since $x\in S/C$ . Let's assume this is reusing the letter. Then the collection of all of these disjoint open sets, lets call it $\{\alpha_i,i\in C\}$ is an open cover of $C$, and since $C$ is compact, it has a finite subcover, $\{\alpha_i|u\in F\}$ where $F$ is a finite subset of $C$. Let $$U=\bigcap_{x\in F}V_y=\emptyset$$
Right? So then $U$ is open, since all empty sets are open... Now the proof doesn't even seem to follow. I have no idea what's going on!","['functional-analysis', 'separation-axioms', 'general-topology', 'compactness']"
1329792,If $x_i \gt 0$ for $1\leq i\leq n$ and $x_1+...+x_n=\pi$ Then the greatest value of $\sin x_1+...+\sin x_n$ is $n \sin \left(\frac{\pi}{n}\right)$.,"If $x_i \gt 0$ for $1\leq i\leq n$ and $x_1+x_2+x_3+...+x_n=\pi$ Then the greatest value of 
$\sin x_1+\sin x_2+...+\sin x_n$ is $n \sin \left(\frac{\pi}{n}\right)$. Prove it. I have no idea how to prove it. Please help.","['inequality', 'trigonometry']"
1329838,$\sigma+$-field of a Brownian motion,"For a standard Brownian motion define \begin{align}\mathcal{F}_{0+} &= \bigcap_{t>0} \mathcal{F_t},\\ \mathcal{F_t} &= \sigma(W_s, 0 \le s \le t)\end{align} Which of the following events belong to $\mathcal{F_{0+}}$ ? (i) $$\left\{ W_t > \sqrt{t} \text{ for infinitely many } t>0\} = \limsup \{W_t > \sqrt{t}\right\}$$ (ii) $$\limsup\left\{W_{\frac{1}{n^2}} > \sqrt{\frac{1}{n}}\right\}$$ (iii) $$\left\{W_{t_n} > \sqrt{t_n} \text{ for $(t_n)_n$ converging to $0$ monotonically}\right\}$$ First of all I guess that (i) does not belong to $\mathcal{F_{0+}}$ , since for a realisation of W, where the ""part-event"" { $W_t > \sqrt{t}$ } occurs infinitely often just for $t>s>0$ , one cannot decide with just knowing W at 0 (with an infinitesimal view in the future). On the other hand, for (ii) and (iii) the ""part-events"" have to occur infinitely often arbitrary close to 0. So I guess at least (ii) is in $\mathcal{F_{0+}}$ (for (iii) all ""part-events"" have to occur).
This is anyway just my guess, how to show it properly? For sigma-algebras one usually has to argue with the generators of the algebras. The borel algebra of $\mathcal{R}$ is generated (for example) by $\{(a,\infty) , a \in \mathcal{R}\}$ .
The $\mathcal{F_t}$ are hence generated by $\{ W_s > a, 0 \le s \le t, a \in \mathcal{R}\}$ . So in the case of (ii) we have $$\left\{W_{\frac{1}{n^2}} > \sqrt{\frac{1}{n}}\right\} \in \mathcal{F_t} \text{ for } \frac{1}{n^2} \le t$$ so $$\bigcup_{m>n}\left\{W_{\frac{1}{m^2}} > \sqrt{\frac{1}{m}}\right\} \in \mathcal{F_t} \text{ for } \frac{1}{n^2} \le t$$ and $$\bigcap_{i>n} \bigcup_{m>i}\left\{W_{\frac{1}{m^2}} > \sqrt{\frac{1}{m}}\right\} \in \mathcal{F_t} \text{ for } \frac{1}{n^2} \le t$$ For arbitrary $t>0$ chose $n$ big enough and since the intersection goes over monotonically decreasing sets it follows that $$\bigcap_{i>0} \bigcup_{m>i} \left\{W_{\frac{1}{m^2}} > \sqrt{\frac{1}{m}}\right\} \in \mathcal{F_t} \text{ for } 0 < t.$$ In the case of (iii) I just know that for $t>0$ , $$\bigcap_{n \ge m} \left\{W_{t_n} > \sqrt{t_n}\right\} \in \mathcal{F_t} \text{ for } t_m \le t$$ since the sequence is monotone. So $$\bigcap_{n \ge 1} \left\{W_{\hat{t}_n} > \sqrt{\hat{t}_n }\right\} \in \mathcal{F_t} \text{ for }  \hat{t}_n = t_{n+m}$$ and $(\hat{t}_n)_n$ is also a monotone decreasing sequence smaller than $t$ . Since $t>0$ was arbitrary (iii) is also in the intersection sigma-algebra. Is this argumentation okay? And how to show that (i) is not in the sigma-algebra? Thanks in advance (I'm sorry for missspelling) One more thing to (iii) Is not $$\left\{W_{t_n} > \sqrt{t_n} \text{ for a sequence } t_n \to 0\right\} = \bigcup_{(t_n)_n} \bigcap_{n \ge 1} \left\{W_{t_n} > \sqrt{t_n}\right\},$$ so my argument is not working since the union is uncountable?","['probability-theory', 'brownian-motion', 'measure-theory']"
1329844,Questions on branch points on elliptic curve,"So let $(E,p)$ be an elliptic curve over a field $k$ with a choice of $k$-valued point $p$. Then by Riemann-Roch, there are two global sections of $\mathcal{O}_{E}(2p)$ which gives a double cover of $\mathbb{P}^{1}$ and I let this morphism be called $\phi$. Then one of the sections which give the map to  $\mathbb{P}^{1}$ vanishes at $p$ of order $2$, meaning that the preimage of $0$ is one point, so $p$ is a branch point. Write this section as $\sigma_{p}$. Then by Riemann-Hurwitz, there are three other branch points, so we pick $q$ to be one of them. Why do we have that $2p\sim 2q$ as divisors (reference [Hartshorne IV Example 4.8.2], [Vakil, Ex 19.9.A])? The only way I can think of is that by the same argument, $\mathcal{O}_{E}(2q)$ also has 2 sections, one of which vanishes at $q$ of order 2. Write this as $\sigma_{q}$. Now, I am very tempted to say that hence $2p-2q$ is principal because we can take $\sigma_{p}/\sigma_{q}$. But I think this proof is missing some crucial steps because what if I take a non-branched point say $z$ of the morphism $\phi$ and run the whole argument, I will arrive at the same conclusion... which looks to me absurd... What should be the correct argument in this case? A more elaborate explanation of my doubt I should be more precise with my question: I was thinking of what happens if instead we take $\mathcal{O}_{E}(2z)$. I am afraid of a contradiction here because we already know that $z$ is not a branched point of $E$, and as you have said any sections $\tau$ that vanishes at $z$ must vanish at $z'$ also. Taking into account that $\deg div(\tau)=2$, you can't have $\tau$ that vanishes at $z$ of order 2. On the other hand, if we choose $(E,z)$ as a elliptic curve instead of $p$, by Riemann-Roch, we have  $\mathcal{O}_{E}(2z)$ having two global sections? one vanishing at $z$ at two points again. Are we talking about two different elliptic curves, or two different coverings $E\rightarrow\mathbb{P}^1$?","['elliptic-curves', 'algebraic-geometry', 'algebraic-curves']"
1329853,Laying cable across a river,"This is a very interesting calculus word problem that I came across in an old textbook of mine. So I know its got something to do with minimising distance, but other than that, the textbook gave no hints really and I'm really not sure about how to approach it. However, I did manage to make a picture or diagram of it. Any guidance hints or help would be truly greatly appreciated. Thanks in advance :) So anyway, here the problem goes: A power house, $P$, is on one bank of a straight river $200\textrm{ m}$ wide, 
  and a factory, $F$, is on the opposite bank $400\textrm{ m}$ downstream from $P$. The cable has to be taken across the river,
  under water at a cost of $\text{\$}6/\textrm{m}$. On land the cost is $\text{\$}3/\textrm{m}$. What path should be chosen so that the cost is minimized? Edit: Thanks guys, I think I found out the answer. I shall post it on MSE.","['word-problem', 'algebra-precalculus']"
1329866,Compact Hausdorff spaces are normal,"I want to show that compact Hausdroff spaces are normal. To be honest, I have just learned the definition of normal, and it is a past exam question, so I want to learn how to prove this: I believe from reading the definition, being a normal space means for every two disjoint closed sets of $X$ we have two disjoint open sets of $X$. So as a Hausdorff space, we know that $\forall x_1,x_2\in X,\exists B_1,B_2\in {\Large{\tau}}_X|x_1\in B_1, x_2\in B_2$ and $B_1\cap B_2=\emptyset$ Now compactness on this space, means we also have for all open covers of $X$ we have a finite subcover of $X$. Now if we take all of these disjoint neighborhoods given by the Hausdorff condition, we have a cover of all elements, I am not sure how to think of this in terms of openness, closedness. How does one prove this?","['separation-axioms', 'general-topology', 'compactness']"
1329897,Riemannian geometry vs Hyperbolic geometry,"I am learning differential geometry in this semester. Concerning the riemannian geometry, if the cross-sectional curvature (riemannian metric ) is negative at every point, the manifold which arises is hyperbolic. At the other hand hyperbolic geometry is another form of non-euclidean geometry just like the riemannian geometry. I am wondering if a manifold with negative curvature in the framework of the riemannian geometry is to be understood as being part of hyperbolic geometry ? If the answer is affirmative, does it mean that hyperbolic geometry is part of the Riemannian geometry ? If the answer is negative, can one study hyperbolic geometry in the framework of differential manifolds ? Thanks for your comment.",['differential-geometry']
1329899,Is the Champernowne constant an automatic number?,"The Champernowne constant in base $b \geq 2$ is obtained by concatenating the $b$-ary expansion of every integer. For example, in base $10$ this is
$$
0.123456789101112131415\dotsc
$$ Question: Is the sequence of $b$-ary digits of the base $b$ Champernowne constant automatic ? My guess is no, but I don't know enough about this to give a definitive answer. Note: Automatic numbers, i.e. numbers whose sequence of digits in some base $b$ is automatic, can very well be irrational. For example the $2$-ary number whose sequence of digits is the Thue-Morse sequence is irrational. Even more, in 2007 Adamczewski and Bugeaud proved that every automatic number is either rational or transcendental.","['number-theory', 'automata', 'finite-automata']"
1329907,Nonparametric changepoint detection for point process,"This is a replication of a question I've recently asked on Cross Validated. It hasn't received an answer or much attention, so I've posted it here. I have a family of point processes representing neural firing data. In each of these point processes, there is a marked pause in events beginning and ending at around the same time. I would like to measure the length of this pause. The model generating this data seems quite complicated and, so far, it has resisted fitting to any elementary distributions. This makes changepoint detection difficult. I've developed a non-statistical method inspired by Canny edge detection, though it doesn't work as well as I'd like. Are there known methods for detecting changepoints in general, nonstationary point processes?","['statistics', 'signal-processing', 'stochastic-processes']"
1329911,Show that there is no analytic bijection from the unit disc to $\mathbb{C}$,Show that there is no analytic bijection from the unit disc to $\mathbb{C}$. I am quite unsure how to proceed here. I know for a fact that there is no analytic function from $\mathbb{C}$ to the open disc by Louvilles theorem. Suppose that $f$ is ideed an analytic bijection from the unit disc to $\mathbb{C}$. Then consider the map $g(z)=f(\frac{1}{z})$. Now $g$ is an analytic bijective function from $\mathbb{C}/\{0\}$ to $\mathbb{C}$. Can this be extended to a proof?,"['complex-analysis', 'complex-numbers']"
1329921,Convergence and integral for nets.,"We say that the non-empty set $S$ with partial order $\leq$ is directed set if for any $s,t\in S$ we have a $u\in S$ such that $s\leq u, t\leq u$. A net is a function from directed $S$ into the any space $X$. In the topological space $X$ we say that a net $\{s_\alpha\}\subset X$ converges to $s\in X$ ( where $s_\alpha=f(\alpha)$ for some directed set $S$) if for any open set $V\subset X$ with $s\in V$ we have some $\alpha_0$  such that for any $\alpha_0\leq\alpha$, $s_\alpha\in V$ and we write $s_\alpha\to s$. When $S$ is countable we name $\{s_\alpha\}$  a sequence.
On the other side it was proven that for any positive measure $\mu$ on $X$ and any sequence $\{h_n\}$ of $\mu$-measurable complex function if $h(x)=\lim_{n\to\infty} h_n(x)$ and if there is $g\in L^1(x,\mu)$ such that for any $n\in N$ and any $x\in X$ $$|h_n(x)|\leq g(x)$$
then we'll have $h\in L^1(x,\mu)$ and $$\lim_{n\to\infty}\int_X h_nd\mu=\int_Xh d\mu$$
Now the question is this: Is the above theorem true for nets which are not sequence?","['nets', 'real-analysis', 'general-topology', 'integration', 'complex-analysis']"
1329943,sequence of open sets,"Find the sequence of open sets in $\Bbb{R}$ like $\{G_n\}$ such that $\Bbb{Z}=\bigcap _{n=1} ^{\infty}G_n$. I think an answer is this:
$$G_n=\bigcup_{m=1} ^{\infty}(m-\tfrac{1}{n+1},m+\tfrac{1}{n+1})$$ 
where $m\in\Bbb{Z}$, $n \in \Bbb{N}$, and $ \Bbb{Z}\subseteq \cap_{n=1} ^{\infty}{G_n} $ because $\{i\}=\cap_{n=1} ^{\infty}(i-\frac{1}{n+1},i+\frac{1}{n+1})$ for any $i\in\Bbb{Z}$, but I can't prove that $\cap_{n=1} ^{\infty}{G_n}\subseteq\Bbb{Z}$.","['metric-spaces', 'general-topology']"
1329967,Double integral finding new limit $\int_1^2\int_0^1 \frac{u}{\sqrt{u^2+4v}} dudv$,"I don't understand how to find new limits when I use substitution method. I have this integral : $$\int_1^2\int_0^1 \frac{u}{\sqrt{u^2+4v}} dudv=$$ $$t=u^2+4v, dt=2udu, \frac{dt}{2}=udu$$ The new limits for $du$ are $\int_{4v}^{1+4v}$ after I substitute $u=1,u=0$ for $t$. But for some reason in my book they get $\int_{0}^{1}$, but I don't understand what about $4v$ Any ideas? Any help will be appreciated, Thanks in advance! EDIT (The full answer in the book): $$\int_1^2\int_0^1 \frac{u}{\sqrt{u^2+4v}} dudv=\int_1^2(\sqrt{u^2+4v}|_{u=0}^{u=1})dv=\int_1^2(\sqrt{4v+1}-2\sqrt{v})dv=\\\frac{2}{3}(\frac{1}{4}\sqrt{(4v+1)^3}-2\sqrt{v^3} |_{1}^{2}=\frac{35}{6}-\frac{5\sqrt{5}}{5}-\frac{8\sqrt{2}}{3}$$","['multivariable-calculus', 'definite-integrals', 'integration']"
1329979,How to find the function for six step operation,"I am trying to find a function for the following scenario: Rotating the red arrow will produce a nice sine wave as illustrated to the right of the hexagon.
But I need to rotate the blue arrow, and at the same time limit the magnitude according to the hexagon.
That means that the magnitude will be 1 at every 60 degrees, and decrease to 0.866 at 60+30 degrees before increasing back to 1. Something like this: Lastly, i am going to use this varying value in a new sine wave function.
That function will look like the dotted curve in the picture below: But I cannot find the actual function to produce such a curve.
(I need it to make a figure about six step operation in electric motor drives) Hope you can help me",['functions']
1329984,A bound on the nth prime.,"Is there any combinatorial argument to show that the nth prime $p_n = \mathcal{O}(n^k)$ for fixed $k$ ? There is a problem in the book by Apostol to find upper bounds on $p_n$, the Prime Number Theorem and simple estimates can be used to prove that. I was wondering if a combinatorial argument could be used.","['prime-numbers', 'elementary-number-theory', 'combinatorics']"
1329992,$\tau$ structure of the sixth Painlevé equation,"I am studying the isomonodromic deformations theory, which leads in the case of a $\mathcal{C}_{0,4}$ Riemann surface to the sixth Painlevé equation. I read that this equation had a $\tau$-structure, so we can determine a $\tau$ function associated to isomonodromic deformations. Here is my silly question : what is a $\tau$-structure, why is it important ?","['integrable-systems', 'riemann-surfaces', 'ordinary-differential-equations', 'differential']"
1330004,"How would you call geometric objects that lie on a single surface, e.g. a sphere, plane, torus, etc.","I'm looking for an extension of the name coplanar to something like ""cosurfacial"", but I guess their must be a correct term.. Edit: In the comments, the context was asked for where I would use that term. I should have done that in the first place. My intent is to design an algorithm that checks for all faces of a solid (in euclidian 3D space) which faces share the same underlying surface. That could be a plane, but also a cylinder, a sphere, etc.. So these faces could be called ""co-surfical"" (but that doesn't sound very well)","['geometry', 'terminology']"
1330006,$ a + a^2 = 90 $. Need hint.,"From trig text. Am supposed to find complementary angles $a$ and $a^2$.
Tried completing the square, got $(a + 1/2)^2 = 361/4$. Stuck. Help.","['quadratics', 'algebra-precalculus']"
1330020,"What is ""approximate compactness""? What is an example of an approximately compact set?","I read this: A property of a set $M$ in a metric space $X$ requiring that for any $x\in X$, every minimizing sequence $y_n\in M$ (i.e. a sequence with the property $\rho(x,y_n)\to\rho(x,M)$) has a limit point $y\in M$. I'm confused with the concept! Can someone give me an example of an approximately compact set? And also an approximately compact set with respect to $A$? Thanks.","['definition', 'compactness', 'metric-spaces', 'real-analysis', 'general-topology']"
1330022,Connectedness of the Hausdorff distance.,"Does anyone know a proof of connectedness of the Hausdorff distance? I mean a proof of the following: Theorem If $(X, \rho )$ is a connected metric space, then $(F(X), d_h )$ is also connected. Remark: 
$F(X) = \{ A \subseteq X | A \text{ is nonempty, closed and bounded } \} $, and
$d_h$ stands for the Hausdorff distance.","['metric-spaces', 'hausdorff-distance', 'connectedness', 'general-topology']"
1330062,Finding the closest point to a set of lines in 2D,"I would need to write an algorithm to find the closest point to a set of lines. These lines are infinite and are not parallel between each other.
Closest point means that point where the sum of the squared distances between the point and each of these lines is minimum. Let's suppose a simple 2D case, where I have a line passing through $a_0 (1, 3)$ and $b_0 (2, 2)$ and another one through $a_1 (1, 1)$ and $b_1 (2, 2)$ I tried a couple of algorithms reading some other questions like this one but I couldn't get a valid result from the general formula 
$$0 = \sum_{i=0}^m\vec c - \vec a_i - \vec d_i \frac{(\vec c - \vec a_i)\cdot \vec d_i}{\|\vec d_i\|^2}$$ I tried to set a system made by 
$$
\left\{ 
\begin{array}{l}
0 = c_x-a_{x0}-d_{x0}\dfrac{(c_x-a_{x0})\cdot d_{x0}}{\|d_{x0}\|^2} + c_x-a_{x1}-d_{x1}\dfrac{(c_x-a_{x1})\cdot d_{x1}}{\|d_{x1}\|^2} \\ 
0 = c_y-a_{y0}-d_{y0}\dfrac{(c_y-a_{y0})\cdot d_{y0}}{\|d_{y0}\|^2} + c_y-a_{y1}-d_{y1}\dfrac{(c_y-a_{y1})\cdot d_{y1}}{\|d_{y1}\|^2}
\end{array}
\right. 
$$ but I end up with $$
\require{cancel}
\left\{ 
\begin{array}{l}
0= \cancel{c_x} - 1 - \cancel{c_x} + 1\\
0=\cancel{2c_y} -4 -\cancel{c_y} +3 - \cancel{c_y} -1
\end{array}
\right. 
$$ So I guess I didn't apply properly the equations.. Then I also followed this other answer I took two points on the line $$a_0 (1, 3)$$ and $$b_0 (2, 2)$$ subtracted to get a vector $$\vec v = b_0 - a_0 = (1, -1)$$ rotated by 90° $$(x, y) \to (-y, x)$$ that is $$(1, -1) \to (1, 1)$$ and divided by its length $$\sqrt{1^2+1^2} = \sqrt2$$ that is $$\left(\frac {1}{\sqrt2}, \frac {1}{\sqrt2}\right)$$ but I don't know how to get the formula $$a_ix + b_iy+c_i$$ And then I also found this one , but it sounds too complicated.. How can I solve?","['geometry', 'calculus', 'algorithms']"
1330088,Subgroups of semidirect product,"If a group $G$ is a direct product of two subgroups $U$ and $V$ such that $\lvert U \rvert$ and $\lvert V \rvert$ are coprime then any subgroup of $G$ is a direct product of subgroups of $U$ and $V$. Now if $G$ is only a semidirect product, $\lvert U \rvert$ and $\lvert V \rvert$ still coprime, what can we say about the subgroups of $G$? Do they have a similar form?",['group-theory']
1330101,"Elliptic curves, reduction map, $E_n$","Let $E$ be the elliptic curve and set $\phi: E(\mathbb{Q}_p) \rightarrow E(\mathbb{F}_p)$ to be the reduction morphism. Define $E_n := \{(x:y:z) \in \ker \phi | x/y \in p^n\mathbb{Z}_p\}$. I'm busy studying elliptic curves and found this exercise online: Show that $\forall n \geq 1$ and $(x:y:z) \in E_n$ it holds that $z/y \in p^{3n}\mathbb{Z}_p$. Now, I've been busy on this for a while now and this is what I've come up with: In order to show that, I started with dividing the projective equation by $y^3$, which is allowed because of the fact that $(x:y:z) \in \ker \phi$: 
$$z/y= x^3/y^3 + axz^2/y^3 + bz^3/y^3.$$
Because $(x:y:z) \in E_n$, it follows that $x/y \in p^n \mathbb{Z}_p$ and because $(x:y:z) \in \ker \phi$, we have that $z/y \in p \mathbb{Z}_p$. Any ideas on how to continue? As always, any help would be greatly appreciated.","['elliptic-curves', 'number-theory', 'algebraic-number-theory']"
1330118,"Little-o, Big-O and differentiation","The functions $f,g, h: \mathbb{R} \rightarrow \mathbb{R}$ have the origin 0 as an internal point of their domain. Prove that if $f = \mathcal{O}(x^{k})$, $f = \mathcal{o}(x^{k-1})$ Prove that if $g = \mathcal{o}(x)$, g is differentiable in 0. Next assume that for $m \in \mathbb{N}_{\leq 1}$, $g = \mathcal{o}(x^{m})$ and $g$ is $m$ times differentiable. Calculate $g^{(k)}(0)$ for $k \in \{0, 1, ..., m\}$ Assume $h$ is $n+1$ times differentiable with $h^{(n+1)}$ continuous and $h = \mathcal{o}(x^{n})$. Prove that $h = \mathcal{O}(x^{n+1})$. What I have tried is the following. As $f = \mathcal{O}(x^{k})$, there exists an open interval $I \subset Dom(f)$ with $0 \in I$ and $C>0$ so that if $x \in I$, $0 \leq |f(x)| \leq C |x|^{k}$. Using the squeeze theorem it follows that $\lim_{x \rightarrow 0} f(x) = 0$, and because $0$ is an internal point of the domain of $f$, $f(0)=0$ so that the first condition for $f$ to be an element of $\mathcal{o}(x^{k-1})$ has been met. Next observe that $I = ]a,b[$ for some $a, b \in \mathbb{R}$ with $a < b$. Because $0 \in I$, $a < 0 < b$. Let $\delta < min\{|a|,|b|,1\}$ and name $D = ]-\delta,\delta[$. It follows that $0 \in D \subset I$. For $x \in D$, $|x| < 1$ and $|x|^{k} < |x|^{k-1}$, so $|h(x)| \leq C|x|^{k} \leq C|x|^{k-1}$. Is this the right direction to go in? Because I cant find the answer from here. Because $g = \mathcal{o}(x)$, g(0) = 0 and $lim_{x \rightarrow 0} \frac{g(x)}{|x|} = 0$. Then $lim_{x \rightarrow 0} \frac{g(x)}{|x|} = lim_{x \rightarrow 0} \frac{g(x)-g(x)}{|x|-0} = lim_{x \rightarrow 0} \frac{g(x)-g(x)}{x-0} = g'(0) = 0$. Doesnt it follow from this that every higher derivative of $g$ also equals 0? Nothing much, cause I have no idea how to start. Any help is greatly appreciated. edit : The definitions used are: $f = \mathcal{o}(x^{k}) \leftrightarrow f(0) = 0, \lim_{x \rightarrow 0} \frac{f(x)}{|x|^{k}} = 0$ $f = \mathcal{O}(x^{k}) \leftrightarrow \forall x \in I, |f(x)| \leq C|x|^{k}$ For $I \subset Dom(f)$ open containing $0$, and $C > 0$.","['analysis', 'real-analysis']"
1330161,"How to ""rotate"" points through 90 degree?","I am trying to do some intersection tests and so the math gets weird if two certain points have the same $x$ coordinate and so infinite slope. The points can be anywhere in any quadrant. I want to ""rotate"" all my points through $90^o$ which will preserve what I need while making the math easier. For a point $(x, y)$ is it just changing it to $(y, x)$?","['geometry', 'coordinate-systems']"
1330168,Limits involving factorials $\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}}$,"I am trying to calculate the following limit $$\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}}$$ where $k$ can be any number between $0$ and $N$ . I thought of the following: If I take the logarithm of the expression then I get: $$\lim_{N\to\infty} \left(\log(N!)-\log((N-k)!)-k\log N\right)$$ Using the Stirling formula this can be approximated as: $$\lim_{N\to\infty} \left(N\log(N)-(N-k)\log(N-k)-k\log N\right)$$ Now there are two cases: If $k$ is $N$ , then the second term vanishes and the remaining terms cancel. If $k$ is smaller than $N$ , then I can drop the $k$ inside the second logarithm and all the terms cancel.
So the limit $$\lim_{N\to\infty} \log\left(\frac{N!}{(N-k)!N^{k}}\right)=0$$ Which means that: $$\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}}=1$$ I don't know if this is mathematically rigorous. Would like some help. Thanks","['factorial', 'calculus', 'limits']"
1330201,Limit(s) of a Sequence from the decimal expansion of $\pi$,"I found a statement in a book concerning the decimal expansion of $\pi$ that I do not really understand. The statement is my problem number 2 , where problem number 1 really looks like a reference request. Here there is the setting. Take $\pi$, and construct a sequence $x_1 := 0.1$, $x_2 := 0.41$, $x_3
 := 0.592$, etc, where for example the first element of the sequence is
   the first digit of the decimal expansion of $\pi$, and the second element of the sequence corresponds to the second and the third digit of the decimal expansion of $\pi$. 1. Apparently, it has been proved somewhere that this sequence converges at least to a limit point. I would like to know if somebody knows about this result. 2. I do not really get how this sequence, if it converge (as it has been proved), can converge to more than one point . Is there somebody who can clarify point 2? Thank you in advance.","['pi', 'limits', 'soft-question', 'sequences-and-series', 'reference-request']"
1330210,How to check if a point is in the direction of the normal of a plane?,"I have a plane, defined by a normal vector $n$ and a point $p$. I also have a point $a = (x, y, z)$. Based on this information, how do I know if the point $a$ exists somewhere past the plane in the direction of the normal?","['geometry', 'linear-algebra', '3d']"
1330213,Calculating in closed form $\sum_{n=1}^{\infty} \sum_{m=1}^{\infty} \frac{1}{m^4(m^2+n^2)}$,How would you tackle this series by real analysis? $$\sum_{n=1}^{\infty} \sum_{m=1}^{\infty} \frac{1}{m^4(m^2+n^2)}$$,"['sequences-and-series', 'calculus', 'real-analysis']"
1330224,Optimal strategy for 2 players Lights Out game variation,"Consider a turn-based game for 2 players. They're both playing on the same board. The board is 8x8, randomly generated and each cell has 0 or 1 (with equal probabilities), for example: 01101100
01010101
10111100
00001111
10101010
00000001
11100011
00101000 In turn 1 player 1 moves, in turn 2 player 2 moves, in turn 3 player 1 moves and so on. Move consists of choosing coordinates (x, y) where 1 is. It flips elements on positions (x, y), (x+1, y) and (x, y+1), if they are in  bounds of the board. By flips I mean turns 1 to 0 or 0 to 1 . (0, 0) is top left corner. So, after playing in (1, 1) board looks like this: 01101100
00110101
11111100
00001111
10101010
00000001
11100011
00101000 And after playing in (7, 1) like this: 01101100
00110100
11111101
00001111
10101010
00000001
11100011
00101000 You win when after your move there are only 0 on board. My question is: what is the optimal strategy for this game? Research so far: Lights Out description on MathWorld is worth reading. Presented game is a variation of Lights Out, but instead of flipping 4 neighbor cells, we flip only 2 and we can move only in cells with 1 . It's also similar to Nim . In the same way like MathWorld describes we can find $S$ - a minimum set of moves to win. For 8x8 board there will be always exactly one solution. Since matrix addition is commutative, the order in which the moves are performed is irrelevant. If $|S|$ (minimal number of moves to win) is odd - it's a winning position. For example if it's 1 - we make only one move and we win. If it's 3 - we make a move from $S$ and opponent gets a board with 2 moves in $S$. Now, if he makes the move from $S$, he will lose, because we will have the final move. So, if he has an option, he will not make a move from $S$. By trying some examples I find out that opponent move not from $S$ increases our $S$ by 1 move - exactly the move he made. So, we get back the board with 3 (different) moves in $S$. But, he can't stop us like this forever - finally all his possible moves will be in $S$. So, my hypothesis is: When you don't make a move from $S$, $|S|$ will increase exactly by 1 and his ""destroying"" move will be added to the set (so later we can ""undo"" his ""destroying"" move). Which would mean that the opponent can only do -1 (by playing a move from $S$) or +1 (by not playing a move from  $S$), so he can't change the parity of our $|S|$! So, if all of above is true, it means that players have no influence on the result of the game. We could even play by random and if the initial $|S|$ was odd - we will win, if it was even - we will lose. It's hard for me to believe in that. What will be the sense of making AI challenge on HackerRank with such a game? Players in the leader-board have different scores and these scores are consistent each time after recalculation (same people in the top 10), suggesting that players actually have an influence on the game result. Maybe you can see some mistake I'm making?","['puzzle', 'combinatorial-game-theory', 'linear-algebra', 'game-theory']"
1330225,Differential equation $y'' \cdot y^3 = 1$,"I use these substitutions $y'=p(y)$ and $y'' = p' \cdot p$ to solve the equation, thus I have the consequence of the solution's steps: $$ p'py^3 = 1 \implies p'p = \frac{1}{y^3} \implies \frac {dp}{dy} p = \frac {1}{y^3} \implies \int p dp = \int \frac{1}{y^3} dy \implies \\ p = \sqrt{C_1 - \frac {1}{y^2}}$$ Then I try to reverse my substitution and get: $$ y' = \sqrt{C_1 - \frac {1}{y^2}}$$ What kind of differential equations is it? How do I have to solve it?",['ordinary-differential-equations']
1330241,Tangent space of the space of compatible complex structures,"Let $(M,\omega)$ be a symplectic manifold, and let $\mathcal{J}(M,\omega)$ denote the space of complex structures on $M$ which are compatible with $\omega$. I have been told the following fact: We have an isomorphism $T_J\mathcal{J}(M,\omega)\cong\Omega^{0,1}(M)$, where $\Omega^{0,1}(M)$ is intended with respect to the complex structure $J\in\mathcal{J}(M,\omega)$. I am trying to prove this fact, but without success until now. Does anyone have ideas, or better still a nice reference? I will post if I find anything. Note: The case that interests me the most is when $M$ is a compact, oriented surface (of genus $g_M\ge2$).","['complex-geometry', 'differential-geometry', 'symplectic-geometry', 'kahler-manifolds']"
1330244,Why do we know that $\left\{\lvert X_n-X\rvert >\epsilon\right\}$ is an event?,"I hope my question is not too stupid. By definition a sequence $(X_n)$ of random variables converges in probability towards the random variable $X$ if for all $\epsilon >0$ we have
  $$
P(\lvert X_n-X\rvert >\epsilon)\to 0\text{ as }n\to\infty.
$$ My question: Why do we know that the set $\left\{\lvert X_n-X\rvert > \epsilon\right\}$ is an event at all, i.e. that we can give it a probability?",['probability-theory']
1330265,Multivariable caculus: Am I or is Maple wrong?,"Well, the problem goes as follows: $$\int_{\Omega}(x^2+y^2)dxdy\quad\Omega:=\{(x,y)\mid x^4+y^4\le 1\}$$ My approach: By symmetry I needed only to find
$$\int_{\Omega_1}(x^2+y^2)dxdy\quad\Omega_1:=\{(x,y)\mid x^4+y^4\le 1\wedge x,y\ge 0\}$$
Let $(x,y)=(r\cos\theta, r\sin\theta)$ where $\theta\in[0,\frac{\pi}{2}]$ and $r\in[0,(\cos^4\theta+\sin^4\theta)^{-1/4}]$ (the constraint for $r$ follows naturally from the boundary equation, I think). Thus the transformation Jacobi determinant is $\partial(x,y)/\partial(r,\theta)=r$, and
$$\int_{\Omega_1}(x^2+y^2)dxdy=\int_{D_{r\theta}}r^2\cdot rdrd\theta\quad D_{r\theta}:=\{(r,\theta)\mid \theta\in[0,\frac{\pi}{2}]\wedge r\in[0,(\cos^4\theta+\sin^4\theta)^{-1/4}]\}$$
After some simple adjustment I got
$$\int_{D_{r\theta}}r^2\cdot rdrd\theta=\frac14\int_{0}^{\frac{\pi}{2}}\frac{d\theta}{\sin^4\theta+\cos^4\theta}=\frac14\frac{\pi}{\sqrt 2}$$
which was also confirmed by Maple. Therefore
$$\int_{\Omega}(x^2+y^2)dxdy=\frac{\pi}{\sqrt 2}$$ However, when I switch to another approach: directly applying Fubini's Theorem, the result seems to be different. By Fubini, we immediately have
$$\int_{\Omega_1}(x^2+y^2)dxdy=\int_{0}^{1}dx\int_{0}^{(1-x^4)^{1/4}}(x^2+y^2)dy=\int_{0}^{1}(x^2\cdot(1-x^4)^\frac14+\frac13(1-x^4)^\frac34)dx$$
This integral was undoable by hand. So I turned to Maple for a numerical result just to see if my previous result is right, and Maple gave me this
$$\int_{0}^{1}(x^2\cdot(1-x^4)^\frac14+\frac13(1-x^4)^\frac34)dx\approx0.9443468503$$
But $\frac14\frac{\pi}{\sqrt 2}\approx 0.55536$. It was quite disappointing. I went through my previous approach one more time and still couldn't find out where I was wrong. I think maybe I'm right, but I have always trusted Maple... Smart people of MSE, could you help me out? I'd be grateful to any kind of clarification. EDIT I was using the ""evalf"" function, (BTW I'm a newbie at Maple and literally don't know how to do multivariable calculus on that)","['solution-verification', 'calculus', 'multivariable-calculus', 'integration']"
1330268,"$L(1,\chi) = \sum_{n=1}^{\infty}\frac{\chi (n)}{n} > 0$, for $\chi$ be the non-trivial real character","Let q be an odd prime and $\chi$ be the non-trivial real character modulo q. I am trying to prove that $L(1,\chi) = \sum_{n=1}^{\infty}\frac{\chi (n)}{n} > 0$. Note: this question was first asked here: Summation of Legendre symbol but I have some additional questions based on my attempt at the solution. There are multiple well-known proofs of the claim $L(1,\chi) \not = 0$. Now, based on that, the fact that $L(s,\chi)$ is representable as an Euler product of non-negative terms for $Re(s)>1$, and the fact that $L(s,\chi)$ is continuous at 1, one could conclude that $L(1,\chi) > 0$, right? I was wondering if the above holds, and if there was a direct proof of the original claim, thereby proving the $L(1,\chi) \not = 0$ in a different way? Thanks.","['number-theory', 'analytic-number-theory']"
1330311,To prove that $\sum\limits_{n=1}^{\infty}\frac{n^{n-1}}{n!e^n}=1$,"$\sum\limits_{n=1}^{\infty}\frac{n^{n-1}}{n!e^n}=1?$
I have noticed that $\lim\limits_{n\to\infty}{\frac{1}{\sqrt[n]\frac{n^{n-1}}{n!}}}=1/e$, so the series $\sum\limits_{n=1}^{\infty}\frac{n^{n-1}}{n!}x^n$converges in $[-\frac1e,\frac1e]$, then I suppose to find the sum-function of $\sum\limits_{n=1}^{\infty}\frac{n^{n-1}}{n!}x^n$. However, this is surely difficult for me. Could anyone help me out? Sincerely thanks for your help.","['analysis', 'sequences-and-series']"
1330337,"$a=b^x+c^x$, How to solve for $x$?","If $a=b^x$, then $x$ could be written in terms of $a$ and $b$; $x=\dfrac{\log(a)}{\log(b)}$. What about $a=b^x+c^x$? Could $x$ be written in terms of $a, b$ and $c$?
$x={}$?","['linear-algebra', 'logarithms']"
1330356,Rudin's definition of derivative,"Walter Rudin's Principle of Mathematical Analysis defines the derivative as follows in Definition 5.1: Let $f$ be defined (and real-valued) on $[a,b]$. For any $x \in [a,b]$ form the quotient
  $$\phi(t) = \frac{f(t)-f(x)}{t-x}\qquad(a<t<b, t \neq x)$$
  and define
  $$f'(x) = \lim_{t\to x}\phi(t),$$
  provided this limit exists (we use the epsilon-delta definition here). We thus associate with the function $f$ a function $f'$ whose domain is the set of points $x$ at which the limit exists; $f'$ is called the derivative of $f$. If $f'$ is defined at a point $x$, $f$ is differentiable at $x$. I have a doubt about this definition, and really hope someone can help me out! If $[a,b]$ is the domain of $f$, how is differentiability for arbitrary functions with holes defined? For $f$ to be differentiable at $x$, can we just take any closed interval $[a,b]$ such that $x \in [a,b]$, and if the above limit exists, then $f$ is differentiable at $x$? Clearly, the answer to the previous question must be no. For otherwise, for some $x$, I can always take the interval $[x,x]$ and the limit won't be defined there. Or, I can also take the interval $[x,x+5]$ where the limit might exist, but the limit might not exist in $[x-5,x+5]$. Thus, can we define $f$ to be differentiable at $x$ if and only if either there exists $[a,b]$ such that $a < x < b$ and Rudin's definition above holds for this $[a,b]$ or there does not exist such $[a,b]$, so $x$ is an endpoint in the domain of $f$, and Rudin's definition holds for some $[a,b]$ for which $x \in [a,b]$. I think the derivative is uniquely defined for each $x$ in such a definition. However this seems to be a very convoluted definition, and I don't see it anywhere. As an example, consider the function $f:[1,2]\cup[3,4]\to\mathbb{R}$, with $f(x)=x$. Is the function differentiable at $x=3$?","['real-numbers', 'calculus', 'real-analysis', 'functions', 'derivatives']"
1330357,Show that four points are coplanar,"I read all posts online regarding how to show four points are coplanar. However, none of them discuss the idea behind the method. Can someone explain how the triple scalar product works?",['linear-algebra']
1330377,Is a compact subset of a topological group G/N closed if G is hausdorff?,I just used this hypothesis when I was proving a theorem.But I was not sure if this hypothesis is correct.,"['locally-compact-groups', 'quotient-spaces', 'topological-groups', 'general-topology']"
1330384,"Verify $\lim\limits_{n\rightarrow\infty}\int^\infty_0 \! e^{-nx} \sin(e^x) \, \mathrm{d}x = 0.$","How to verify $\lim\limits_{n\rightarrow\infty}\int^\infty_0 \! e^{-nx} \sin(e^x) \, \mathrm{d}x = 0$? My idea is to use the dominant convergence theorem with $f_n(x):= e^{-nx} \sin(e^x)$ and $f(x):=\lim\limits_{n\rightarrow\infty}f_n(x)$. $\Rightarrow \lim\limits_{n\rightarrow\infty}\int^\infty_0 \! e^{-nx} \sin(e^x) \, \mathrm{d}x = \int^\infty_0 \! \lim\limits_{n\rightarrow\infty} e^{-nx} \sin(e^x) \, \mathrm{d}x = \int^\infty_0 \!\lim\limits_{n\rightarrow\infty}0\, \mathrm{d}x = 0$ Can I use this here?","['calculus', 'limits', 'definite-integrals', 'integration', 'convergence-divergence']"
1330409,Is every projection on a Hilbert space orthogonal?,"I'm highly doubtful that the answer is ""yes,"" but I fail to see what's incorrect about this very basic proof I've thought of. If someone could point out my error, I'd appreciate it. My logic is as follows: Claim: Every projection on a Hilbert space is orthogonal. ""Proof"": 1. For any linear space $X$, it is true that given a projection $P: X \rightarrow X$ (where $P$ is a projection iff $P$ is linear and satisfies $P^2 = P$), we have $X = \text{ran}(P) \oplus \text{ker}(P)$. 2. Assume X is a Hilbert space (which is, by definition, linear). Since $\text{ker}(P)$ is a closed linear subspace of $X$, then by the projection theorem, $X = \text{ker}(P) \oplus \text{ker}(P)^\perp$ is an orthogonal direct sum. 3. Therefore, $\text{ker}(P)^\perp = \text{ran}(P)$, so $X = \text{ran}(P) \oplus \text{ker}(P)$ is an orthogonal direct sum. Thus, $P$ is an orthogonal projection.","['hilbert-spaces', 'functional-analysis']"
1330421,Iteration with bounded converging sequence as input,"Consider the sequence of vectors $( x_k )_{k=0}^{\infty}$ such that $x_k \in X \subset \mathbb{R}^n$ for all $k$, where $X$ is compact, and $x_k \rightarrow \bar{x} \in X$. Consider a vector $y_0 \in Y$ and a Lipschitz continuous function $f : \mathbb{R}^n \times \mathbb{R}^n \rightarrow Y \subset \mathbb{R}^n$, where $Y$ is compact, with the following property. For all $z \in X$, there exists $y \in Y$ such that the sequence of vectors $\left( y_{k+1}:= f(y_k, z) \right)_{k=0}^{\infty}$ is such that $y_k \rightarrow y$. Prove or disprove the following statement: 
$$\exists \bar{y} \in Y : \ y_{k+1} := f( y_k, x_k ) \rightarrow \bar{y} \in Y$$ Comments: I have tried to take $\bar{k}$ large enough, so that $x_k \in \bar{x} + \epsilon B$ for all $k \geq \bar{k}$. But then I am not sure how to exploit the Lipschitz continuity of $f$ to claim that $y_k$ stays close to $f(\bar{y},\bar{x})$. Clearly, if the sequence $(y_k)_k$ converges to some $\bar{y} \in Y$, then $\bar{y} = f(\bar{y}, \bar{x})$.","['limits', 'real-analysis', 'sequences-and-series', 'lipschitz-functions', 'analysis']"
1330424,The Cantor set and ternary expansions,"I'm trying to prove that the Cantor set $\mathcal{C}$ contains all numbers $x \in [0,1]$ with ternary expansion $x = \sum_{k=1}^\infty \frac{a_k}{3^k}$, such that $a_k=0$ or $a_k=2$. I'm going by induction, proving that $x$ belongs to every $\mathcal{C}_k$, where $\mathcal{C} = \bigcap_{k=1}^\infty \mathcal{C}_k$, and $\mathcal{C}_k$ is the usual $k$-th set in the construction of the Cantor set (i.e., a disjoint union of $2^k$ closed intervals, each of length $\frac{1}{3^k}$, etc.). Here's my work so far: The base case, $k=1$, was easy: If $a_1=0$, then the geometric sum yields $x \leq \frac{1}{3}$; similarly, if $a_1=2$, then $x \geq \frac{2}{3}$. For the inductive step, I assume $x \in \mathcal{C}_k$, and try to prove $x \in \mathcal{C}_{k+1}$. First, I assume there is an interval $[a,b]$ of length $\frac{1}{3^k}$ containing $x$, where $[a,b]$ is one of the $2^k$ intervals that make up $\mathcal{C}_k$. I can write $[a,b]$ as $$
[a,b] = \left[a, a + \frac{1}{3^{k+1}} \right] \cup \left( a + \frac{1}{3^{k+1}},  b - \frac{1}{3^{k+1}} \right) \cup \left[ b- \frac{1}{3^{k+1}}, b \right],
$$ and of course I'd like to show, for instance, that $x$ is in the leftmost of these intervals if $a_{k+1} = 0$. The geometric sum gives me the bound $x \leq \sum_{j=1}^k \frac{a_j}{3^j} + \frac{1}{3^{k+1}}$, but I want $x \leq a + \frac{1}{3^{k+1}}$. I've tried and failed to get past here. I feel like I'm missing something silly, but can anybody help me reach finish off the argument? Thanks!","['elementary-set-theory', 'cantor-set', 'measure-theory']"
1330447,Find the derivative of a triple integral function,"The function $f(x)$ is differentiable. And I need to find the derivative of the triple integral function:  $$F(t)=\iiint_V \ f(\ xyz \ ) \ dxdydz \ ,
\ \ and \ V=\{(x,y,z)|0\leq x\leq t,\ 0\leq y\leq t,\ 0\leq z\leq t,   \ 0\leq t \}$$ I try to let $x=ut,\ y=vt,\ z=wt $ Then the function is $$ F(t)=\iiint_V \ f(\ uvwt^3\ ) \ t^3dudvdw$$
But I don't know how to do it after that.","['calculus', 'multivariable-calculus', 'integration']"
1330462,"I want to know, why $Re\Big\{ \frac{zf''(z)}{f'(z)}\Big\}=|z|\frac{\partial}{\partial|z|} Re(log(f'(z)))$","I don't know how to prove this, $Re\Big\{ \frac{zf''(z)}{f'(z)}\Big\}=|z|\frac{\partial}{\partial|z|} Re(log(f'(z)))$.
I know it is related whit cauchy riemann equations and partial derivation.",['complex-analysis']
1330482,Integer solutions to the equation $a_1^2+\cdots +a_n^2=a_1\cdots a_n$,"What is the general solution to the equation
$$\sum_{j=1}^n a_j^2=\prod_{j=1}^n a_j,$$
$n\in \mathbb N$ , $n \ge 2$ over $\mathbb N_0$ ? WLOG, we can assume $0\le a_1 \le a_2\le \cdots \le a_n$ For every $n$, there is a solution, the trivial one: $a_1=\cdots =a_n=0$. For $n=2$ , the equation has only the trivial solution. For $n=3$, the solutions with $0\le a_1\le a_2\le a_3\le 100$ are : ? for(a=0,100,for(b=a,100,for(c=b,100,if(a^2+b^2+c^2==a*b*c,print(a,"" "",b,""  "",c
)))))
0 0  0
3 3  3
3 3  6
3 6  15
3 15  39
6 15  87 For $n=4$, the solutions with $0\le a_1\le a_2\le a_3\le a_4\le 100$ are : ?     for(a=0,100,for(b=a,100,for(c=b,100,for(d=c,100,if(a^2+b^2+c^2+d^2==a*b*c*d,pr
int(a,"" "",b,""  "",c,"" "",d))))))
0 0  0 0
2 2  2 2
2 2  2 6
2 2  6 22
2 2  22 82 For $n=5$, the solutions with $0\le a_1 \le a_2 \le a_3 \le a_4 \le a_5 \le 100$ are : ? for(a=0,100,for(b=a,100,for(c=b,100,for(d=c,100,for(e=d,100,if(a^2+b^2+c^2+d^2
+e^2==a*b*c*d*e,print(a,"" "",b,""  "",c,"" "",d,"" "",e)))))))
0 0  0 0 0
1 1  3 3 4
1 1  3 3 5
1 1  3 4 9
1 1  3 5 12
1 1  3 9 23
1 1  3 12 31
1 1  3 23 60
1 1  3 31 81
1 1  4 9 33
1 1  5 12 57
1 3  3 4 35
1 3  3 5 44 For $n=6$ and $0 \le a_1 \le a_2 \le a_3 \le a_4 \le a_5 \le a_6 \le 100$, the only solution is the trivial one. What is known about the general solution (finite many or infinite many solutions, solutions with pairwise different numbers etc.) ?","['vieta-jumping', 'number-theory', 'diophantine-equations']"
1330488,Prove there's a simple path of length $k$ in a simple graph $G$ where all the vertices have degree of at least $k$,"Prove there's a simple path of length $k$ in a simple graph $G$ where all the vertices have degree of at least $k$. Relevant definitions: $G$ is a simple graph that consists of a vertex set $V(G) = \{v_1, v_2, ..., v_n\}$ and an edge set $E(G) = \{e_1, e_2, ..., e_m\}$ where each edge is an ordered pair of vertices. The edge $\{u,v\}$ is denoted $uv$.  A walk of length $k$ is a sequence $v_0,e_1,v_1,e_2,...,e_k,v_k$ of vertices and edges such that $e_i=v_{i-1}v_i$ for all $i$. A path is a walk with no repeated vertex. My attempt: Induction, for $k=1$ it's obvious. Suppose for $k-1$ and we'll prove for $k$. Let $G$ be a simple graph such that $\forall v \in V : d(v)\ge k$. We can assume that there are at least $k+1$ vertices since there are $k$ neighbours to every vertex. Let $v_0$ be some vertex, it has $k$ neighbours, we'll move to one of its neighbours say $v_1$ and remove $v_0$. So now there are $k$ vertices with a degree of at least $k-1$ and from the induction hypothesis we'll have a path of length $k$. Is using only $k$ vertices from $n$ right? Does it keep the generality when using only all the neighbours of $v_0$?","['graph-theory', 'proof-verification', 'combinatorics', 'inequality']"
1330493,How do you prove $\sum \frac {n}{2^n} = 2$? [duplicate],This question already has answers here : Why $\sum_{k=1}^{\infty} \frac{k}{2^k} = 2$? [duplicate] (12 answers) Closed 9 years ago . How do you prove $$\sum_{n=1}^{\infty} \frac {n}{2^n} = 2\ ?$$ My attempt: I have been trying to find geometric series that converge to 2 which can bind the given series on either side. But I am unable to find these. Is there a general technique to find the sum? This is a high school interview question and must be easy enough to solve in a few minutes. Please give any hints for the first step towards a solution.,['sequences-and-series']
1330502,How to find $\sum (t-\bar{t})^2$,"Given that $n=150$, $\sum t=645$ and $\sum t^2=8287.5$. How to find $\sum (t-\bar{t})^2$ where $\bar{t}$ denotes the mean of $t$.",['statistics']
1330538,Curve meeting itself everywhere,"(related, but not a duplicate: curve which crosses itself at every point ) When reading the comments to the question above, it has been pointed out that if by ""cross"" we mean that for every $\alpha\in [0,1]$ there is $\beta\neq\alpha$ such that $\gamma(\alpha)=\gamma(\beta)$, then there are simple examples of such curves. However, all the examples I can think of have the following property: There are two disjoint (non-degenerate) intervals $I_1,I_2\subseteq [0,1]$ such that $\gamma(I_1)=\gamma(I_2)$. (intuitively, this means that the curve goes over some segment twice in the same way or reversed). My question is, does this always have to happen? To be precise: Does there exist a continuous curve $\gamma:[0,1]\rightarrow\Bbb R^2$ such that for every $\alpha\in [0,1]$ there is $[0,1]\ni\beta\neq\alpha$ such that $\gamma(\alpha)=\gamma(\beta)$, but there are no two disjoint intervals $I_1,I_2\subseteq [0,1]$ we have that $\gamma(I_1)=\gamma(I_2)$? I believe the answer is yes , and that this is achieved by some space-filling curve, but I am not sure. Thanks in advance.","['geometry', 'general-topology']"
1330545,Show that one cannot make a 8×8 square using 15 T-tetrominoes and 1 square tetromino,Show that one cannot make a 8×8 square using 15 T-tetrominoes and 1 square tetromino. Its a coloring problem. Unable to solve. please help.,"['coloring', 'combinatorics']"
1330576,Determinant of a Certain Block Structured Positive Definite Matrix,"PLEASE FIND THE EDITED VERSION OF THIS QUESTION HERE: Asymptotic behavior of the minimum eigenvalue of a certain Gram matrix with linear independence I WILL ALSO PUT UP A BOUNTY FOR THE EDITED VERSION.
Is there a lower bound for the determinant or minimum eigenvalue of the following $d$ by $d$ matrix in terms of $d$? $$\Gamma=\left( {\begin{array}{cc}
		I & B \\
		B^{*} & I \\
		\end{array} } \right)$$
Where $I$ is the identity matrix and the the moduli of entries of $B$ and those of its conjugate $B^{*}$ are all equal to $\frac{1}{\sqrt{d}}$. Also the blocks are all $\frac{d}{2}$by$\frac{d}{2}$. It is a Gram matrix and further assume that the rows and columns are linearly independent. Hence we know that the lower bound is larger than zero but can we say anything more? For simplicity we can assume the field of the matrix is real. Hence the entries of the off-diagonal blocks ($B$ and $B^{T}$) are $\pm\frac{1}{\sqrt{d}}$. I appreciate any input very much!","['eigenvalues-eigenvectors', 'determinant', 'linear-algebra', 'matrices']"
1330649,Difference between topology and sigma-algebra axioms.,One distinct difference between axioms of topology and sigma algebra is the asymmetry between union and intersection; meaning topology is closed under finite intersections sigma-algebra closed under countable union. It is very clear mathematically but is there a way to think; so that we can define a geometric difference? In other words I want to have an intuitive idea in application of this objects.,"['soft-question', 'general-topology', 'measure-theory']"
1330655,Check my proof of a property of the greatest integer function?,"Prove that $\forall n \in \mathbb{Z}, \lfloor x + n \rfloor = \lfloor x \rfloor + n $. Proof: Let $K = \{\ k\ |\ k\in\mathbb{Z},\ k \leq x+n\}$. Then, by definition, $$ \lfloor x + n \rfloor = j = \sup(K) $$ Let $k \in K.$ Since $ n,k \in \mathbb{Z},\ k-n\in \mathbb{Z}$. In addition, $\forall k, j \geq k,$ so $j-n \geq k-n$. Therefore, if we define a new set $\ T = \{\ k-n \ | \ k -n \in \mathbb{Z},\ k \leq x+n\}= \{\ k-n \ | \ k -n \in \mathbb{Z},\ k - n\leq x\},\ j-n = \sup(T)$. Thus, $\lfloor x \rfloor = j-n,$ or $j = \lfloor x + n \rfloor =\lfloor x \rfloor + n,$ as desired. $_{\square}$ I'm currently independently reading Apostol's Calculus for fun - while I understand the concepts, I'm not sure if I wrote an acceptable proof, or made any unjustified assumptions (I'm new at writing proofs). Even if I did prove it correctly, I feel like there's an easier way. Please point out mistakes or give any general comments/tips/suggestions to help me improve in writing proofs. Thanks!","['ceiling-and-floor-functions', 'calculus', 'proof-verification', 'functions']"
1330669,Related Rates - possible textbook error,"Recently I've been asked to do some exercises from a textbook but I cannot understand how the author derived the correct answer. Here is the exercise in question: At $8$:$00$ boat $A$ is located $25\,km$ south of boat $B$. If boat $A$ is traveling west at $16\,km/h$ and boat $B$ south at $20\,km/h$, at what rate is the distance between the boats changing at $8$:$30$? Textbook answer: $25.6\,km/h$ My answer: $-10.12\,km/h$ I have likely made a mistake somewhere. Could someone help me determine which answer is correct?","['geometry', 'calculus']"
1330679,"Concerning existence of subsequence of converging integrals on subsets of $[0,1]$ of a sequence $(f_n)\in[0,1]$","Problem Statement Let $\{f_n\}$ be a sequence of real-valued, measurable functions on $[0,1]$ that is uniformly bounded. Show that if $A$ is a Borel subset of $[0,1]$ then there exists
subsequence $n_j$ such that $\int_A f_{n_j}(x) \ \mathrm{d}x$
converges. Show that if $(A_i)$ is a countable collection of Borel measurable
subsets of $[0,1]$, then there exists a subsequence $n_j$ such that
$\int_{A_i} f_{n_j}(x) \ \mathrm{d}x$ converges for each $i$. Show that there exists a subsequence $n_j$ such that $\int_A
    f_{n_j}(x)$ converges for each Borel subset of $A$. Attempt At first I started thinking of using a diagonalization argument and to approach this problem step by step. Instead, I am wondering what might be wrong with the following naive approach. $f_n$ is uniformly bounded on $[0,1]$ so $\int_{[0,1]}f_n  \ \mathrm{d}x$ is an infinite sequence of real numbers on the compact set $[-2k,2k]$, where $|f_n|\leq k$. So there is a convergent subsequence $\int_{[0,1]}f_{n_k}\ \mathrm{d}x$. This subsequence also converges for any borel subset of $[0,1]$ so we have the result for all three of the above problems. Question What major concept(s) am I missing here? Note that I am not asking for a full solution to the problem but rather some feedback on my attempt at solving it. I'm sorry for this silly question but I find it hard to dig into a problem until I realize why my ""initial naive attempt"" fails.","['lebesgue-integral', 'measure-theory']"
1330689,"Prob 12, Sec 26 in Munkres' TOPOLOGY, 2nd ed: Why we need continuity to show the result?","Let $f: X\mapsto Y$ be a closed continuous surjective map such that $f^{-1}(y)$ is compact, for each $y\in Y$ . Show that if $Y$ is compact, then $X$ is compact. My question is why do we need $f$ to be continuous? It seems I can prove this result without continuity. Here is my proof: Let $\left\{ U_{\alpha}:\alpha\in J\right\} $ be an open covering
  of $X$ . Since $f$ is surjective and $f^{-1}\left(y\right)$ is compact,
  for any $y\in Y$ , there exists a finite set $J_{y}\subset J$ such
  that $\left\{ U_{\alpha}:\alpha\in J_{y}\right\} $ is an open covering
  of $f^{-1}\left(y\right)$ . Suppose we can find a finite set $\tilde{Y}\subset Y$ ,
  such that $\left\{ U_{\alpha}:\ \alpha\in\cup_{y\in\tilde{Y}}J_{y}\right\} $ covers $X$ , then we are done. For any $y\in Y$ , $\cup_{\alpha\in J_{y}}U_{\alpha}$ is an open
  set in $X$ . Hence, $X-\cup_{\alpha\in J_{y}}U_{\alpha}$ is closed.
  Since $f$ is a closed mapping, we know $Y-f\left(X-\cup_{\alpha\in J_{y}}U_{\alpha}\right)$ is open. Since $y\in Y-f\left(X-\cup_{\alpha\in J_{y}}U_{\alpha}\right)$ ,
  there exists a neightbourhood $V_{y}$ of $y$ , such that $V_{y}\subset Y-f\left(X-\cup_{\alpha\in J_{y}}U_{\alpha}\right)$ .
  In other words, $f^{-1}\left(V_{y}\right)\subset\cup_{\alpha\in J_{y}}U_{\alpha}$ .
  Since $Y$ is compact, there exists a finite set $\tilde{Y}$ such
  that $Y\subset\cup_{y\in\tilde{Y}}V_{y}$ . Therefore, $$
X=f^{-1}\left(Y\right)\subset f^{-1}\left(\cup_{y\in\tilde{Y}}V_{y}\right)=\cup_{y\in\tilde{Y}}f^{-1}\left(V_{y}\right)\subset\cup_{y\in\tilde{Y}}\left(\cup_{\alpha\in J_{y}}U_{\alpha}\right)=\cup_{\alpha\in J'}U_{\alpha}
$$ where $J'=\cup_{y\in\tilde{Y}}J_{y}$ is a finite set.  Q.E.D. [I've read an earlier post ( Prob 12, Sec 26 in Munkres' TOPOLOGY, 2nd ed: How to show that the domain of a perfect map is compact if its range is compact? ) and the proof therein, but I still cannot get a clue. As a new commer, I cannot leave a comment there.]","['general-topology', 'compactness']"
1330693,Extending a bounded holomorphic function past its boundary,"Suppose I have a bounded holomorphic function on the unit disc, centred at the origin. Can I always extend this beyond the origin to say a disc of radius $1 + \epsilon$ for some $\epsilon > 0$? My guess is that this should be possible since we can take a Taylor expansion about 0, but I can't show that there are not going to be poles/ a limit point of poles at the boundary.","['taylor-expansion', 'complex-analysis']"
1330699,Combinatorial interpretation of identity $\sum_{k=0}^m\binom{m}{k}\cdot \frac{(-1)^k}{n+k+1}=\frac{n!\cdot m!}{(n+m+1)!}$,"I recently came across the identity $$\sum_{k=0}^m\dbinom{m}{k}\cdot \frac{(-1)^k}{n+k+1}=\dfrac{n!\cdot m!}{(n+m+1)!},$$ while working on evaluating $$\int_0^1 x^n(1-x)^m\, dx.$$ I ended up showing that both sides of the identity were equal to this integral, but I was wondering if there was a way to show directly (either by manipulation or some combinatorial argument) that one was equal to the other. Any help is much appreciated.","['combinatorial-proofs', 'summation', 'binomial-coefficients', 'combinatorics']"
1330717,Are bounded analytic functions on the unit disk continuous on the unit circle?,"Let $f(z)$ be holomorphic on the open disk $\mathbb{D} = \{z \in \mathbb{C}: |z| < 1\}$.
Moreover, let $f$ be bounded on the boundary of $\mathbb{D}$, i.e.
$$ \sup_{\varphi \in [0,2\pi]} |f(e^{i\varphi})| < \infty $$
(This class of functions is sometimes called $H^\infty$). My question regards the boundary function $\tilde{f}(\varphi) := \lim_{r \rightarrow 1} f(r e^{i\varphi})$. Is it continuous?","['analyticity', 'continuity', 'complex-analysis', 'hardy-spaces']"
1330740,How to compute $\sum_{n\text{ odd}}\frac{1}{n\sinh n\pi\sqrt 3}$?,"I came across an old question asking to show that $$\displaystyle\sum_{n\text{ odd}}\frac{1}{n\sinh n\pi}=\frac{\ln 2}{8}.\tag{1}$$ Although I have managed to prove this formula, my proof uses various theta functional relations and looks like an overkill. On the other hand, it suggests a few more identities, for example $$\displaystyle\sum_{n\text{ odd}}\frac{1}{n\sinh n\pi\sqrt3}=\frac{\ln \left(8-4\sqrt{3}\right)}{4}.\tag{2}$$ Question : can one prove (1) and (2) in a more elementary way? Ideally, the proof should make clear further generalizations.","['closed-form', 'sequences-and-series', 'hyperbolic-functions']"
1330747,Solving the recurrence $T(n) = \sqrt n T(\sqrt{n}) + \sqrt{n}$,"A former student of mine was TA-ing an algorithms class last quarter and asked students to solve this famous recurrence relation: $$T(n) = \sqrt n T(\sqrt{n}) + n$$ There are several ways to solve this recurrence relation and this question has already been asked here. I was talking to my student about this problem and mentioned that it's quite hard to solve and is pretty dependent on the particular choices being made. As an example, I suggested this variant of a recurrence relation that was less simple to solve: $$T(n) = \sqrt n T(\sqrt{n}) + \sqrt{n}$$ The TA and I tried solving this recurrence for about an hour and a half without making any progress. Here are a few things we tried: My approach to solving the initial recurrence relation was to draw out a recursion tree and notice that each level of the tree contributes $n$ to the total and that there are $O(\log \log n)$ layers, so the recurrence solves to $O(n \log \log n)$. When I tried doing this here, I noticed that the work per layer was no longer constant; instead, the top layer sums to $\sqrt{n}$, the second layer to $n^{3/4}$, the third to $n^{7/8}$, etc. We got stuck working with the summation $n^{1/2} + n^{3/4} + n^{7/8} + ...$. We tried using the iteration method to unroll the recurrence. With the original recurrence relation, this works out nicely; here, we got stuck at the same summation given above. I'm completely stuck trying to figure out how to solve this recurrence relation. There's nothing riding on it per se - I don't need to solve it for any particular reason - but the fact that we arrived at it by a straightforward modification of a common algorithms problem set question makes it all the more enticing. Any idea how to solve this recurrence? Thanks!","['recurrence-relations', 'discrete-mathematics']"
1330759,Time complexity of LU decomposition,"I am trying to derive the LU decomposition time complexity for an $n \times n$ matrix. Eliminating the first column will require $n$ additions and $n$ multiplications for $n-1$ rows. Therefore, the number of operations for the first column is $2n(n-1)$. For the second column, we have $n-1$ additions and $n-1$ multiplications, and we do this for $(n-2)$ rows giving us $2(n-1)(n-2)$. Therefore, the total number of operations required for the full decomposition can be written as $$
\sum_i^n 2(n-i) (n-i+1)
$$ How do we get from this sum to a total cost of $\frac{2}{3}n^3$?","['numerical-linear-algebra', 'matrix-decomposition', 'asymptotics', 'gaussian-elimination', 'linear-algebra']"
1330786,Is an injective morphism from a Lie group to itself surjective?,"I have a question about Lie groups. Let $G$ be a finite dimensional (real or complex) Lie group and $f:G \rightarrow G$ an homomorphism of Lie groups. Edit : in the view of some counter-examples let's also suppose $G$ connected. Q1 : Suppose that $f$ is injective, is it then surjective ? Here are my attempts so far to adress this question. It is known that an injective Lie homomorphism is an immersion so the differential $df_g$ of $f$ in any point $g\in G$ is injective. For reasons of dimension, $df_g$ is then bijective. Thus, by the inversion theorem, $f$ is a diffeormorphism from $G$ to $f(G)\leq G$. Moreover, it is also well known that $f(G) \simeq G/Ker(f) \simeq G/\lbrace e \rbrace \simeq G$. So we have a subgroup $f(G)$ of $G$ isomorphic to $G$. Then, we can reduce the question to : Q2 : Let $H$ be a Lie subgroup of $G$. If $H \simeq G$, do we have $H=G$ ? Now, maybe we can progress with some topological argument. Typically, I think of the fundamental group, that is useful to link the classification of Lie groups to the classification of Lie algebras. As $H$ is diffeomorphic to $G$, we have $\pi(H)=\pi(G)$. Hence a third question : Q3 : Let $H$ be a Lie subgroup of $G$. If $Lie(H)=Lie(G)$ and $\pi(H)=\pi(G)$, do we have $H=G$ ? After that, I have to admit that I don't have ideas about where to look. So if someone as an answer, a hint or a reference, that would be awesome. In addition, I have a few ""bonus"" questions around this subject : B1 : If the answer to the questions above is no, what is a counterexample ? Is there a simple hypothesis we can add so that it becomes true ? B2 : What are the possible generalisations to all this discussion ? For example with infinite dimensional Lie groups ? With topological groups ? B3 : In the other way around, if $f\in Hom(G,G)$ is surjective, is it injective ? Thanks a lot to all who have read till the end and to the ones who will answer.","['lie-groups', 'differential-geometry', 'group-homomorphism']"
1330794,Self-independent random variable,"Let $X$ be a self-independent random variable. Show that $X$ is almost sure constant. My proof (by condradiction): Assume that there are two disjoint Borel set $A$,$B$ such that:
$\Pr(X \in A)>0$ and $\Pr(X \in B)>0$ Since $X$ is self-independent, there must be $\Pr(X \in A)=1$ and $\Pr(X \in B)=1$.
It's a contradiction, because then $\Pr(X \in A \cup B)>1$. Is everything ok?","['probability-theory', 'probability']"
1330795,"Part (a) of Exercise 13 of first chapter of Rudin's book ""Functional Analysis""","I would really appreciate it if you could give me some advice on the part (a) of Exercise 13 of first chapter of Walter Rudin's book "" Functional Analysis "": Let $C$ be the vector space of all complex continuous functions on $[0, 1]$. Define
  \begin{equation}
d(f,g) = \int_0^1 \frac{\lvert f(x) - g(x) \rvert}{1 + \lvert f(x) - g(x) \rvert} \ dx \ .
\end{equation}
  Let $(C, \sigma)$ be $C$ with the topology induced by this metric. Let $(C, \tau)$ be the topological vector space defined by the semi-norms
  \begin{equation}
P_x(f) = \lvert f(x) \rvert, \qquad (0 \leq x \leq 1),
\end{equation}
  Prove that every $\tau$-bounded set in $C$ is also $\sigma$-bounded and that the identity map $id: (C, \tau) \rightarrow (C, \sigma)$ therefore carries bounded sets into bounded sets. I tried using the theorem that says a set $E \subseteq C$ is bounded if and only if every semi-norm in our semi-norms is bounded on $E$. This theorem tells us that if $E$ be a bounded set in $(C, \tau)$, then for every $x \in [0, 1]$, $P_x(E)$ is bounded, i.e.
\begin{equation}
\forall x \in [0, 1] \ \exists M_x, \quad s.t. \quad \forall f \in E, \quad \lvert f(x) \rvert \leq M_x \ .
\end{equation}
I think now we should use Uniform boundedness principle and obtain $M > 0$ such that
\begin{equation}
\forall x \in [0, 1] \ \forall f \in E, \quad \lvert f(x) \rvert \leq M \ .
\end{equation}
Then we have $d(f, 0) \leq \frac{M}{1+M}$ for all $f \in E$. So $E$ is bounded in $(C, \sigma)$. In the last step, to use Uniform boundedness principle, I think we should prove that $(C, \tau)$ is a Banach space, and $(C, \sigma)$ a normed space. I don't know what should I do in this step.","['functional-analysis', 'topological-vector-spaces']"
1330803,Why do roots span dual space of maximal toral subalgebra?,"Suppose $\Phi$ is the root system of a semi simple Lie algebra with maximal toral subalgebra $H$. I read that $\Phi$ spans $H^\ast$. The Killing form on $H$ is nondegenerate, so $H\cong H^\ast$ by identifying $\phi\in H^\ast$ with the unique $t_\phi$ such that $\phi(t)=\kappa(t_\phi,t)$. Every proof is by contradiction, beginning by saying if $\Phi$ does not span $H^\ast$, then there exists nonzero $h\in H$ such that $\alpha(h)=0$ for all $\alpha\in\Phi$. I understand the proof except for this line. Can anybody clarify why such $h$ must exist?","['abstract-algebra', 'lie-algebras', 'representation-theory']"
1330813,Continuous surjections onto $\mathbb{R}$,"I have two questions about continuous functions: Suppose $X \subseteq \mathbb{R}$ and $X$ has same cardinality as $\mathbb{R}$. Can we find a continuous function from $X$ onto $\mathbb{R}$? Suppose $X \cup Y = \mathbb{R}$. Can we find a continuous function from one of $X, Y$ onto $\mathbb{R}$?","['descriptive-set-theory', 'real-analysis', 'general-topology']"
1330815,How to prove $1+x \leq e^x~\forall x \in \mathbb{R}?$,"How to prove $$1+x \leq e^x~\forall x \in \mathbb{R}$$ I'm stuck, I tried taking logs but didn't know how to proceed.","['exponential-function', 'functions']"
1330833,Mean Square Error of Monte Carlo,"Trying to develop the expression for the Mean Square Error (MSE) of Monte Carlo, I found myself a bit lost when going through a simple proof in the literature. I am working in the context of mathematical finance, where the aim is to find an approximation to the ""true value"" of the function $V$. Let's define $V$ as $V = \mathbb E[f]$, whose discrete approximation is $\hat{V} = \mathbb E[\hat{f}]$ and its Monte Carlo estimate is formulated as
$$
\hat{Y} = \frac{1}{N}\sum_{n=1}^{N}\hat{f}^{\,(n)}
$$ The expression for the MSE in the aforementioned proof is obtained by doing the following:
\begin{eqnarray*}
\mathbb E\left[\left(\hat{Y} - V \right)^2 \right]&=&\mathbb E\left[\left(\hat{Y} - \mathbb E\left[\,\hat{f}\,\right] + \mathbb E\left[\,\hat{f}\,\right] - \mathbb E\left[\,f\,\right] \right)^2 \right] = \\
&=& \mathbb E[(\hat{Y} - \mathbb E\left[\,\hat{f}\,\right])^2] + \left(\mathbb E[\hat{f}] - \mathbb E[f] \right)^2 + \mathbb E\left[2\left(\hat{Y} - \mathbb E [\,\hat{f}]\right)\left(\mathbb E\left[\,\hat{f}\,\right] - \mathbb E\left[\,f\,\right]\right) \right] = \\
&=& \frac{1}{N}\mathrm{Var}(\,\hat{f}\,) + \left(\mathbb E[\hat{f}] - \mathbb E[f] \right)^2
\end{eqnarray*} It is the last line that flusters me. Particularly, my concerns are: I am assuming that the third element of the second line on the right side tends to $0$ since $\mathbb E [\hat{f}] \rightarrow \mathbb E [f]$ and that's why it disappears in the next line, but is this the real reason why this element dissappears? I would think that the first element of the second line, $\mathbb E[(\hat{Y} - \mathbb E\left[\,\hat{f}\,\right])^2]$ is exactly the variance of $\hat{Y}$, so why do we obtain in the last line the variance term multiplied by the factor $\frac{1}{N}$? If we expanded the expression for $\mathrm{Var}(\hat{Y})$, we should obtain the common factor $\frac{1}{N^2}$ by the variance property, and not just $N^{-1}$. Maybe it is just that I got unnecessarily confused with simple math, so any kind of clarification would be welcome.","['probability-theory', 'stochastic-calculus', 'stochastic-processes', 'probability', 'stochastic-analysis']"
1330851,Steinhaus-like problem,"I know there are similar problems on here, but I believe this is not a duplicate. Let $E \subset \mathbb{R}$ be a measurable set of positive finite measure. Define $f:[0,\infty) \rightarrow \mathbb R$ by  $$f(t)= m(E \cap E_t),$$ where $E_t=\{t+x:x\in E\}$. Prove that $f$ is continuous on $[0,\infty)$. I wanted to rewrite $f$ as a convolution of two sufficiently nice functions (in this case $L^1$ and $L^\infty$) which we know to be continuous: $$f(t)=\int_{E\cap E_t}1 dx= \int_E 1_{E_t} dx= \int_E 1_{E}(x-t) dx=\int_{\mathbb R} 1_{-E}(t-x) 1_E dx= 1_{-E}*1_E(t), $$ and $1_{-E}$ is $L^1$ and $1_E$ is $L^\infty$. Alternative solutions (assuming this actually is one) are welcome, too. Also, a good reference for convolution results like the one used here would be much appreciated.","['real-analysis', 'convolution', 'measure-theory']"
1330858,Path lengths on a unit square,"Suppose I'm at $(x=0,y=0)$ and I want to get to $(x=1,y=1)$. The shortest path is the diagonal and it has length $\sqrt{2}$. But what if I'm only allowed to make moves in coordinate directions---e.g., $1/2$ along $x$, $1/2$ along $y$, another $1/2$ along $x$, and a final $1/2$ along $y$. Then the length of my path is $2$. In fact, any coordinate-constrained path has length $2$. Let the path $p_n$ be $1/n$ along $x$, followed by $1/n$ along $y$, followed by $1/n$ along $x$, etc., until I get to $(1,1)$. Presumably, the limit of $p_n$ as $n\rightarrow\infty$ is the diagonal line. But the path length of each $p_n$ is $2$, while the path length of the limit is $\sqrt{2}$. Weird, right? Is this just an example that shows that you can't exchange limit and path length?",['limits']
1330872,What to know about convergence of integrals,"According to the values of p>0 examine the convergence of the integral:
$$\int_0^{+\infty} \dfrac{\ln(1+2x^{3p})}{(x+x^2)^{4p}\arctan(x)^{1/2}}dx$$
I didn't find a good explanation about this kind of problems,so i will be glad if someone say a few words about it.","['analysis', 'real-analysis', 'improper-integrals']"
1330902,Does measurability really matter?,"I am studying applied math  and I currently got stuck on proving that a function, which emerges in a model is measurable (Borel functon), so we can integrate it. I know, that there are examples of non-measurable sets w.r.t. the Lebesgue measure ( Vitali set ) so its characteristic(indicator) function will be non-measurable too. But my question is the following: Is there an example of real-life application where the verification of measurability really matters? For example a mechanism, for which we can try to derive its behavior (for example, stability properties), ignoring measurability check (just writing integrals mindlessly), but which do not follow our prediction exactly because we assumed some function within the model to be measurable but it is actually not? In the other words, I want historical ""proof"" of importance of this particular type of mathematical correctness.","['lebesgue-measure', 'applications', 'real-analysis', 'measure-theory']"
1330909,Proving that $\sum \deg(v) = 2m$ for any Graph $G$,"Here is My proof, please correct me if wrong, I try to be formal. Proof by Induction: Let $\sum \deg(v)=2m$ assumption... when #of nodes is $n=0$.
so here the equation is  $\sum \deg(v)=2(0)=0$ [true for 0] .... (1) now if we add new node $w$, and connect it to node v then we expect that only the degree of $v$ and $w$ will increased by $1+1 =2$ , and so doing that like the following:$\sum \deg(v)=2 + 2m = 2(m+1)$,
then our assumption is true because when we add 1 edge, total  number of edges increased by $2(m+1)$. Is it correct and formal proof ?","['graph-theory', 'discrete-mathematics']"
1330934,Minimum Surface Area of a Closed Cylindrical Container,"This is a trivial question; but I just want to make sure: A closed cylindrical container has a capacity of $128\pi \,{\rm m}^3$. Determine the minimum surface area. The answer is $96\pi$. Volume of Cylinder, $V = \pi r^2 h = 128\pi$ (eq1) Surface Area of Cylinder, $SA = 2\pi(r^2 + rh)$ (eq2) Substitute eq(1) and eq(2); solve for the derivative of zero:
$$\frac{\rm d}{{\rm d}r}( r^2 + 128/r) = 0,$$ solve for $r$. We get $r=4$. Put back into $V$ formula,
$h = 2.54647$ Calculate Surface area: $52\pi$ Is it possible answer is wrong? I double checked with wolfram Alpha and all my derivatives are valid.","['volume', 'calculus', 'derivatives']"
1330974,Is there a rule for $\sqrt{a+b}$?,"You learn in algebra that 
$$\sqrt{ab}=\sqrt{a} \sqrt{b}$$
and that 
$$\sqrt{\frac ab}=\frac {\sqrt a}{\sqrt b}$$
You also learn to never make the fatal mistake of thinking
$$\sqrt{a+b}=\sqrt{a}+\sqrt{b}$$ However, I am wondering if there is a rule for $\sqrt{a+b}$. I would think it would be pretty complex, if it exists at all. $$\sqrt{a+b}=\text{?}$$",['algebra-precalculus']
1330987,The limit of a product of functions equals the product of the limits: Is this proof rigorous?,"All the proofs I've seen so for the limit of a product of functions equaling the product of the limits are based on the following : Let $f$ and $g$ be real or complex functions having the limits $$\lim_{x\to x_0}f(x) = F \quad \mbox{and} \quad \lim_{x\to x_0}g(x) = G.$$ Then also the limit $\displaystyle\lim_{x\to x_0}f(x)g(x)$ exists and equals $FG$ . Let $\varepsilon$ be any positive number. The assumptions imply the existence of the positive numbers $\delta_1,\,\delta_2,\,\delta_3$ such that \begin{align}
|f(x)-F| < \frac{\varepsilon}{2(1+|G|)}\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_1\tag{1}
\end{align} \begin{align}
|g(x)-G| < \frac{\varepsilon}{2(1+|F|)}\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_2\tag{2}
\end{align} \begin{align}
|g(x)-G| < 1\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_3\tag{3}
\end{align} According to the condition (3) we see that $$|g(x)| = |g(x)\!-\!G\!+\!G| \leqq |g(x)\!-\!G|+|G| < 1\!+\!|G|\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_3.$$ Supposing then that, $0 < |x-x_0| < \min\{\delta_1,\,\delta_2,\,\delta_3\}$ , and using (1) and (2) we obtain \begin{align*}
|f(x)g(x)-FG|\;& = |f(x)g(x)-Fg(x)+Fg(x)-FG|\\
               & \leqq |f(x)g(x)\!-\!Fg(x)|+|Fg(x)\!-\!FG|\\
               & = |g(x)|\cdot|f(x)\!-\!F|+|F|\cdot|g(x)\!-\!G|\\
               & < (1\!+\!|G|)\frac{\varepsilon}{2(1\!+\!|G|)}+(1\!+\!|F|)\frac{\varepsilon}{2(1\!+\!|F|)}\\ 
               & = \varepsilon
\end{align*} This settles the proof. But after having a go myself, I came up with the following: Let $\varepsilon$ be any positive number. The assumptions imply the existence of the positive numbers $\delta_1,\,\delta_2$ such that \begin{align}
|f(x)-F| < {\varepsilon}\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_1\tag{1}
\end{align} \begin{align}
|g(x)-G| < {\varepsilon};\;\mbox{when}\;\;0 < |x-x_0| < \delta_2\tag{2}
\end{align} Supposing then that, $0 < |x-x_0| < \min\{\delta_1,\,\delta_2\}$ , and using (1) and (2) we obtain \begin{align*}
|f(x)g(x)-FG|\;& = |(f(x) - F)(g(x) - G) + G(f(x) - F) + F(g(x) - G)|\\
               & \leq |f(x) - F||g(x) - G| + |G||f(x) - F| + |F||g(x) - G|\\
               & < \varepsilon^2 + \varepsilon(|F| + |G|)\\
               & = \varepsilon'
\end{align*} where $\varepsilon'$ is any postive number, giving $\varepsilon = -1/2(|F|+|G|) +1/2\sqrt{(|F| + |G|)^2 + 4\varepsilon'}$ This settles the proof. Is this(my proof) rigorous enough?","['calculus', 'limits']"
