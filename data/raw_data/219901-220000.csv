question_id,title,body,tags
4498652,Are Hilbert spaces smooth?,"I'm new in functional analysis. I have a question about smooth normed spaces. A normed space $X$ is called smooth if for every $0\neq x\in X$ , there exists a unique functional $\varphi \in X^*$ such that $\| \varphi \|=1$ and $\varphi (x)=\| x\|$ . Is a Hilbert space smooth?","['hilbert-spaces', 'functional-analysis']"
4498677,Random variable is poisson distributed,"Let $\Omega = \mathbb{N}_0^2, \mathcal{A}=Pot(\Omega)$ and $\mathbb{P}$ the product of two poisson distributions with paremeter $\lambda_1,\lambda_>0$ , i.e $$ \mathbb{P}(\{(n_1,n_2)\})=\frac{\lambda_1^{n_1} \lambda_2^{n_2}}{n_1!n_2!} e^{-\lambda_1-\lambda_2}$$ Define then $X:\Omega\rightarrow \mathbb{N}_0,\ (n_1,n_2)\mapsto n_1+n_2$ .
Prove that X is poisson distributed with parameter $\lambda_1+\lambda_2$ I've seen various proofs that similar to this one, Poisson Distribution of sum of two random independent variables $X$, $Y$ , but they are basically between independent variables. In this job it's about one single random variable. So what to prove is that $X$ ~ $\mathcal{P}(\lambda_1+\lambda_2)$ My attempt \begin{align}\mathbb{P}(X=n)
&=\mathbb{P}(n_1+n_2=n)\\&=\sum_{k=0}^n\mathbb{P}(n_1=k,n_2=n-k)\\&=
\sum_{k=0}^n\mathbb{P}(n_1=k)\mathbb{P}(n_2=n-k)
\\&=\sum^n_{k=0}\frac{\lambda_1^k\lambda^{n-k}}{k!(n-1)!}e^{-(\lambda_1+\lambda_2)}
\\&=\frac{(\lambda_1+\lambda_2)^n}{n_!}e^{-(\lambda_1+\lambda_2)}
\end{align} This is obviously enough for $X$ ~ $\mathcal{P}(\lambda_1+\lambda_2)$ But my worries is that, in the brackets of $\mathbb{P}$ am I allowed to use this kind of notation $n_1=k, n_2=n-k$ , because from what I've learnd, in the brackts it should be a random variable equals to some real number, like $\mathbb{P}_X(\{t\})=\mathbb{P}(X=t)$ , but in the proof it is a real number equals to some real number, I don't know it's correct","['probability-distributions', 'probability-theory', 'probability']"
4498691,Curvature of an Embedded Curve,"If one has a a compact Riemannian surface $\Sigma$ (ie. a Riemannian manifold of dimension $2$ ), the Ricci curvature can be written as $R_{ij} = \frac{1}{2} R g_{ij}$ . In other words, the curvature can be expressed by one single function.  The scalar curvature is equal to double the Gaussian curvature $K$ , so one can also re-write $R_{ij} = K g_{ij}$ . Does this mean that the curvature of a smooth embedded close curve in $\Sigma$ is also determined by the Gaussian curvature $K$ for the surface by Gauss-Bonnet theorem?","['curves', 'surfaces', 'curvature', 'differential-geometry']"
4498751,Arithmetic progressions of sums of two squares,"Let $E=\square+\square$ denote the set of integers of the form $a^2+b^2$ . It is well-known that $n\in E$ iff for any prime $p\equiv 3\pmod{4}$ we have that $\nu_p(n)$ is even, so it is not difficult to prove that the density of $E$ is zero, or even more precise bounds like $$\left| E\cap [1,n]\right| \sim \frac{cn}{\sqrt{\log n}}. $$ On the other hand we know many subsets of $\mathbb{N}$ with density zero but containing arithmetic progressions of arbitrary length: for instance the set of primes, as shown by Green and Tao. I was wondering if $E$ also contains APs with arbitrary length. It is fairly trivial to prove the existence of infinite APs in $E$ with four terms, namely $$(n-8)^2+(n-1)^2,\quad (n-7)^2+(n+4)^2,\quad (n+7)^2+(n-4)^2,\quad (n+8)^2+(n+1)^2 $$ but I was not able to find longer parametric APs or a reply to my question in the literature. Any help is appreciated. Addendum : the answer should be affirmative, for instance by applying Gowers-type attacks to the discrete Fourier transform of the indicator function of $E\cap[1,n]$ , or just by considering that any long AP of primes $\equiv 1\pmod{4}$ is also a long AP in $E$ . For instance $214861583621 + 37692995340n$ for $n\in[0,9]$ gives an AP with length $10$ . UPDATE : I have found parametric $5$ -APs and proved the existence of infinite $6$ -APs. The idea was just to identify some $n$ s such that $(n+8)^2+(n+1)^2+12n = 2n^2+30n+65$ belongs to $E$ . Luckily $E$ is a semigroup, hence it is enough to impose that $4n^2+60n+130=(2n+15)^2-95\in E$ , or to prove that for infinite values of $m$ we have that $5m^2-19$ belongs to $E$ . If we brutally compute the density of $m\in[1,N]$ such that $5m^2-19\in E$ we have that this density drops to zero, albeit very slowly (as expected). On the other hand we may try to find parametric solutions to $$ 5m^2-A^2-B^2 = 19 $$ given by quadratic forms. We have $5=2^2+1^2$ and $5\cdot 2^2-1^2=19$ , hence a reasonable choice is $$ m = M^2+aM+2,\quad A=2M^2+bM,\quad B=M^2+cM+1. $$ By picking $b$ as $2a+2$ and $c$ as $a-4$ we have $$ \frac{5m^2-A^2-B^2-19}{M} = 8+18a-2M $$ hence $M=9a+4$ gives that for any $m=(9a+4)^2+a(9a+4)+2=90a^2+76a+18$ we have $5m^2-19\in E$ . This implies the existence of infinite $5$ -APs in $E$ , for instance the previous $4$ -APs with $n=100a^2-65a+10$ (these are induced by $5=2^2+1^2$ and $19=5\cdot 3^2-5^2-1^2$ ): $(100a^2-65a+2)^2+(100a^2-65a+9)^2$ $(100a^2-65a+3)^2+(100a^2-65a+14)^2$ $(100a^2-65a+17)^2+(100a^2-65a+6)^2$ $(100a^2-65a+18)^2+(100a^2-65a+11)^2$ $(100a^2-85a+22)^2+(100a^2-45a+9)^2$ These numbers are all $\equiv 0\pmod{5}$ : they lead to a $5$ -AP whose first term is $(60a^2-39a+4)^2+(20a^2-13a-1)^2$ and with common difference $12(20a^2-13a+2)$ . We may impose that $$(60a^2-39a+4)^2+(20a^2-13a-1)^2+60(20a^2-13a+2)\\=(20a^2-25a+37-3v)^2+(60a^2-35a+v)^2,$$ leading to a $6$ -AP, by finding integer points on $$ 616-392a-111v+40av+5v^2 = 0.$$ Actually it is enough to find some rational point, since we may scale the elements of an AP by an arbitrary square. We have the rational point $(a,v)=(0,11)$ , hence we have infinite rational points by Vieta jumping and infinite $6$ -APs in $E$ . This also gives substance to the dream of an elementary proof. In the last lines we proved that a parametric $4$ -AP (with parameter $n$ ) can always be extended to a parametric $5$ -AP if $n$ is taken among the values of a quadratic polynomial $q(a)$ . At this point it looks reasonable that by taking $a$ among the values of a quadratic polynomial $q_2(b)$ we can write down parametric $6$ -APs and so on. If this actually works, the first $k$ -AP has to appear before $2^{c\cdot 2^k}$ . SECOND UPDATE We may also form APs in $E$ by looking at rational points $P_k=(x_k,y_k)\in S^1$ such that $x_k+y_k$ form an AP. The range of $x_k+y_k$ over the rational points of $S^1$ is exactly given by the values of $\pm \frac{m^2-2m-1}{m^2+1}$ for $m\in\mathbb{Q}^+$ by Vieta jumping. It is possible to list the elements of $\mathbb{Q}^+$ as in the Stern-Brocot tree and check for APs in the range of $f:x\mapsto \pm\frac{x^2-2x-1}{x^2+1}$ . The triple $\frac{23}{65},\frac{35}{65},\frac{47}{65}$ is easily found by hand, and leads to the fact that $$(65n-33)^2+(65n+56)^2,\quad (65n-25)^2+(65n+60)^2,\quad (65n-16)^2+(65n+63)^2$$ is a $3$ -AP in $E$ (with common difference $1560n$ ) for any $n\geq 1$ . And it looks extremely reasonable that the range of $f$ contains arbitrarily long APs, since $r_2(n)$ is unbounded. Another interesting fact is that the ""linear"" $4$ -AP shown at the beginning can be extended to a $6$ -AP if $n$ is chosen in such a way that both $2n^2\pm 30n+65$ belong to $E$ . If $2n^2+30n+65\in E$ for any $n$ in the range of an odd cubic polynomial, $n=c(m)$ , then $$ 2c(m)^2+30 c(m)+65 = d(m)^2+e(m)^2 $$ automatically implies $2n^2-30n+65\in E$ via $2c(m)^2-30c(m)+65=d(-m)^2+e(-m)^2$ . Actually I have not been able to find an odd cubic  polynomial fulfilling this, but I managed to find an odd rational function, which up to rescaling gives parametric $6$ -APs in terms of polynomials with degree $7$ .","['number-theory', 'elementary-number-theory']"
4498754,On minimizing a summation,"I have the following inequality: $$ A > \Big(\frac{1}{2k}-\frac{1}{2n}\Big)n^{2} \tag{*}$$ where $n\geq k$ and $n$ and $k$ are both positive integers. We also know that $$ A = \sum_{j=1}^{k}|S_j| \left( |S_j| - 1 \right) $$ where $|S_j|\in \mathbb{Z}^{+}$ for each $j\in[k]$ and $\sum_{j=1}^{k}|S_j|=n$ . Now, I am trying to find values of $|S_j|$ such that the equation $(*)$ holds. Moreover, I would want to find a lower bound for $\min_{j \in [r]}\{|S_j|\}$ such that $(*)$ holds. My attempt: Let $T$ denote $\min_{j\in[k]}\{S_j\}$ and $R:=\Big(\frac{1}{2k}-\frac{1}{2n}\Big)n^2$ . By this, equation $(*)$ becomes the following: $k\cdot T(T-1)>R \Leftrightarrow T^{2}-T>\frac{R}{k}$ When solving this equation, Wolframalpha seems to get me the following: $T>\frac{1}{2}(\sqrt{4R+1}+1)=\frac{1}{2}\Bigg[1+\sqrt{4n^{2}\frac{\Big(\frac{1}{2r}-\frac{1}{2n}\Big)}{r}+1}\Bigg]$ . However, if possible, I would like to be able to express the lower bound for $T$ as a function of $n$ in a way that $(*)$ would hold.","['calculus', 'combinatorics', 'discrete-mathematics']"
4498785,Malliavin derivative of adapted processes,"Let $(\mathcal{F}_t)_{t\ge 0}$ be a filtration . A stochastic process $(X_t)_{t\ge 0}$ is adapted with respect to such a filtration, if $X_t$ is $\mathcal{F}_t$ -measurable for all $t\ge 0$ . Now consider two adapted processes $(u_t)_{t\ge 0}$ and $(v_s)_{s\ge 0}$ . Why these statements are (trivially?) true: If $s<t$ then $D_tv_s = 0$ where $D_t$ is the Malliavin derivative . If $s>t$ then $D_su_t = 0$ . The statemets appear in page 57 of Introduction to Malliavin Calculus of D. Nualart and E. Nualart. The authors mention these statements as trivial facts, without demonstration or further comment. I don't know what key observation I am overlooking, according to which both would be trivial or obvious statements.","['probability-theory', 'stochastic-calculus', 'malliavin-calculus']"
4498801,Relating the change of variables formula for measures to that of $\mathbb{R}^n$,"I am trying to deeply understand the similarities between these two theorems; the first being a generalization of the second. Theorem 16.13. If $f$ is nonnegative, then $$
\int_{\Omega} f(T \omega) \mu(d \omega)=\int_{\Omega^{\prime}} f\left(\omega^{\prime}\right) \mu T^{-1}\left(d \omega^{\prime}\right) .
$$ A function $f$ (not necessarily nonnegative) is integrable with respect to $\mu T^{-1}$ if and only if $f T$ is integrable with respect to $\mu$ , in which case (16.17) and $$
\int_{T^{-1} A^{\prime}} f(T \omega) \mu(d \omega)=\int_{A^{\prime}} f\left(\omega^{\prime}\right) \mu T^{-1}\left(d \omega^{\prime}\right)
$$ hold. For nonnegative $f$ , (16.18) always holds. (2.47) Theorem. Suppose $\Omega$ is an open set in $\mathbf{R}^{n}$ and $G: \Omega \rightarrow \mathbf{R}^{n}$ is a $C^{1}$ diffeomorphism.
(a) If $f$ is a Lebesgue measurable function on $G(\Omega)$ , then $f \circ G$ is Lebesgue measurable on $\Omega$ . If $f \geq 0$ or $f \in L^{1}(G(\Omega), m)$ , then $$
\int_{G(\Omega)} f(x) d x=\int_{\Omega} f \circ G(x)\left|\operatorname{det} D_{x} G\right| d x
$$ (b) If $E \subset \Omega$ and $E \in \mathscr{L}^{n}$ , then $G(E) \in \mathscr{L}^{n}$ and $m(G(E))=$ $\int_{E}\left|\operatorname{det} D_{x} G\right| d x$ . Why is the second theorem not written as $$
\int_{\Omega} f(G(x)) d x=\int_{G(\Omega)} f(x) \left| \operatorname{det} D_{x} G\right| d x
$$ This would make a lot more sense to me as we could think of this as $G$ is a function that changes the underlying measure space, and we integrate w.r.t. the pushforward measure which turns out to be $\left| \operatorname{det} D_{x} G\right| d x$ . Otherwise, I cannot see how to make the second version fit within the statement of the first.","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'calculus', 'probability']"
4498862,Square roots of finite abelian groups,"For a finite cyclic group $\mathbb{Z}/n\mathbb{Z}$ we can define an epimorphism $$\pi: \mathbb{Z}/\text{gcd}(2,n)n\mathbb{Z} \to \mathbb{Z}/n\mathbb{Z}, \quad [x] \mapsto \frac{2}{\text{gcd}(2,n)} [x].$$ I would think of $\sqrt{\mathbb{Z}/n\mathbb{Z}}=\mathbb{Z}/\text{gcd}(2,n)n\mathbb{Z}$ as the group of all squareroots of $\mathbb{Z}/n\mathbb{Z}$ (similar to a branch covering). My questions are: For a general finite abelian group $G = \bigoplus_{i =1}^r\mathbb{Z}/n_i\mathbb{Z}$ we could define $\sqrt{G}$ factorwise. I don't think that this depends on the factor decomposition, but I am not sure. Does it? If the above is true, is there a more abstract characterization of $\sqrt{G}$ (maybe in terms of a universal property) If all of the above is true, this group probably has a name. What is it? EDIT: I think that $G \xrightarrow{\iota} \sqrt{G}\xrightarrow{\pi} G$ is the unique (up to isomorphism) mono-epi factorization of the square map $G \xrightarrow{2} G$ such that $G = \operatorname{ker} p\circ\pi$ . Here $p:G \to G/2G$ is the canonical quotient map. In particular $2\sqrt{G}=G$ .","['group-theory', 'abstract-algebra', 'abelian-groups']"
4498883,Is there a minimal generating set of reals which additively generate all the reals?,"Is there a set $S$ of real numbers such that the submagma generated by $S$ under addition is the entire set of real numbers, but such that no proper subset of $S$ generates the entire set of real numbers?","['real-numbers', 'group-theory', 'abstract-algebra', 'semigroups']"
4498906,(Why) Is there no analogue to the classification of finitely generated abelian groups for abelian groups?,"Every finitely generated abelian group is a direct sum of cyclic groups. Does this hold for all abelian groups in general? If not, what fails?","['finitely-generated', 'group-theory', 'abelian-groups']"
4498923,"Let $Y$ be $b(300,p)$. If the observed value of $Y$ is $y=75$, find the approximate $90\%$ confidence interval for $p$.","I tried solving this problem by what I've learned about confidence intervals for proportions, but something goes wrong at some point because I'm not matching the textbook answer. For this problem, since $Y$ is not normally distributed, let's consider the following general pivot random variable: $$
Z = \frac{\overline{X}-\mu}{\frac{\sqrt{S^2/n}}{n}}
$$ where $\overline{X}$ is the sample mean (mle of $\mu$ ), $\mu$ is the mean (the parameter we wish to estimate), $S^2$ is the sample variance and $n$ is the sample size. By the central limit theorem and by the independence of the samples $Y_1, \ldots, Y_n$ , $Z$ approximately follows a standard normal distribution, i.e. $N(0,1)$ . A $(1-\alpha)100\%$ confidence interval for $\mu$ is one such that $$
\text{Pr}\left(\overline{X} - z_{\alpha/2}{\frac{\sqrt{S^2/n}}{n}} < \mu < \overline{X} + z_{\alpha/2} \frac{\sqrt{S^2/n}}{n}\right) = 1-\alpha
$$ where $0 < \alpha < 1$ . In our case, \begin{align}
\alpha &= 0.10 \\
z_{\alpha/2} &= 1.645 \\
n &= 300 \\
\overline{X} &= \widehat{p} = 75/300 = 1/4 \\
\\
S^2 &= \sigma^2 = 300\widehat{p}(1-\widehat{p})
\end{align} With these values in mind, our confidence interval for $p$ is: \begin{align}
\left(1/4 - 1.645\frac{\sqrt{300(1/4)(1-1/4)/300}}{300}, 1/4 + 1.645\frac{\sqrt{300(1/4)(1-1/4)/300}}{300}\right) &= 
(0.2476, 0.2524)
\end{align} My textbook has the following confidence interval: $$
(0.2089,0.2911)
$$ What did I do wrong?","['statistics', 'confidence-interval', 'probability']"
4498952,Bayes' Theorem in Conditional Probability,"The scenario given by the problem is as follows: We are testing for a disease D that we think is present, D+, with probability 0.4, and absent, D-, with probability 0.6.  We believe that a test has sensitivity P{T+|D+}=0.75 and specificity P{T-|D-}=0.8. Q1: What is our probability that the disease is present if we perform the test and it is positive, T+, and our probability the disease is absent if that test is negative, T-? My ans: We can apply Bayes' formula to calculate P{D+|T+} and P{D-|T-}. P{D+|T+} = 5/7 P{D-|T-} = 24/29 Q2: Suppose we perform three tests, conditionally independent given D.  Given each possible
number of positive test results, 0, 1, 2, or 3, what is our probability that the disease is present? My ans: Let k denote the number of positive test results. We know that P{ [exactly] k successes in n trials | p } = (nCk)x(p^k)x((1−p)^(n−k)), hence we can calculate P{k=0|D+} and P{k=0|D-}. We can then apply Bayes' formula to calculate P{D+|k=0}. P{D+|k=0} = 0.692 Using the same approach for k=1:3, we derive: P{D+|k=1} = 0.5 P{D+|k=2} = 0.308 P{D+|k=3} = 0.165 Would really appreciate it if someone can verify whether my reasoning and answers are correct. Thank you for your help in advance.","['conditional-probability', 'bayes-theorem', 'probability']"
4498960,"What numbers are these, and why do they show up here?","This came up when trying to evaluate $\sum n^3/2^n$ . In order to do this I had to repeatedly differentiate and multiply by $x$ to make the function match the summation. What are these numbers, and why do they show up here? I would appreciate some more insight as to how exactly these coefficients of the expansion come about through this iterated process","['power-series', 'sequences-and-series']"
4498984,Closed form of negative root of $x+x^2+x^4+x^8+x^{16}+...=0$,"I was investiging the series $\sum_{k=0}^\infty{x^{(2^k)}}$ where $x\in\mathbb{R}$ , and came up with this question: Is there a closed form of the negative root of $\sum_{k=0}^\infty{x^{(2^k)}}=0$ ? On Desmos, taking the first $5$ terms, and taking the first $1001$ terms, both give the negative root as $-0.6586$ to $4$ decimal places. Here are my thoughts. According to this , ""there is no solution in radicals to general polynomial equations of degree five or higher with arbitrary coefficients"". So if we take the first five terms of the series, I would not expect a closed form of the negative root. But, based on my experience with power series, if we take the limit as the number of terms approaches $\infty$ , I would be more inclined to believe that a closed form might exist.","['real-analysis', 'closed-form', 'polynomials', 'sequences-and-series', 'power-series']"
4499014,Can all non-monotonic functions be treated as monotonic functions if we adequately partition it into sub-intervals?,"Monotonic functions defined in an interval must either increase or decrease(strictly or otherwise), whereas non-monotonic functions don't. But can we say that a non-monotonic function can be partitioned into many monotonic functions in various intervals? Say, $y=x\sin x$ , then for interval [0, 2.029] it is monotonic? Then again it seems for [2.029, 4.913] it is monotonic, or am I missing something? Because then it would imply that all non-monotonic functions can be monotonic functions when defined at appropriate intervals. I was reading ""Calculus, Volume 1 - Tom Apostol"" when I encountered monotonic
functions. Also any suggestions on learning calculus would be appreciated. Thank you for your time.","['calculus', 'functions']"
4499060,Asking for help to understand how one can apply the Dominated Convergence Theorem in a proof for Oxtoby's Ergodicity Theorem,"The following is from the sixth lecture notes (pp.11-12) of MAGIC010 Ergodic Theory . The claim: Let $X$ be a compact metric space, $T:X\to X$ a continuos mapping and define $C(X, \mathbb{R}) = \{f:X\to \mathbb{R}\mid \text{$f$ is continuous}\}$ . Then the following are equivalent: 1.) $T$ is uniquely ergodic. 2.) for all $f \in C(X, \mathbb{R})$ there exists a constant $c(f) \in \mathbb{R}$ such that $\lim_{n\to\infty}\sum_{j=0}^{n-1}(f\circ T^j)(x) = c(f)$ uniformly for $x \in X$ . The given proof that $2.) \implies 1.)$ is as follows: $
        \int fd\mu = \lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}\int f\circ T^jd\mu = \int \lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^jd\mu = \int c(f)d\mu = c(f)$ . Where to my knowledge the author uses the Dominated Convergence Theorem. But 1.) the DCT requires the bounding function to be non-negative and I don't see any reason why $c(f)$ couldn't be negative. Is there an implicit assumption that $f$ is non-negative or in what other way can the use of DCT be justified? If it is so that DCT is not strictly necessary for the proof that $2.) \implies 1.)$ , the given proof that $1.) \implies 2.)$ relies entirely upon DCT.","['measure-theory', 'ergodic-theory', 'probability-theory', 'real-analysis']"
4499076,Do locally compact Hausdorff spaces admit local bases consisting of normal open neighbourhoods?,"Let $X$ be a locally compact Hausdorff topological space. By locally compact we mean that every $x\in X$ admits a local basis consisting of compact neighbourhoods. Then since compact Hausdorff spaces are normal, each $x\in X$ admits a local basis consisting of normal neighbourhoods. My question is whether it must be the case that each $x\in X$ admits a local basis consisting of normal open neighbourhoods. An issue preventing this from being a completely straightforward deduction is the existence of compact Hausdorff spaces which are not completely normal, e.g. the Tychonoff plank .","['general-topology', 'separation-axioms']"
4499142,"If the square of a real number lies between 1500 and 1600, then what can we conclude?","If the square of a real number lies between 1500 and 1600, then the real number lies between: a. 25, 30 b. 30, 35 c. 35, 40 d. 40, 45 e. 45, 50 My answer is that the real number lies between 10√15 and 40 .
Is the correct answer of the question be one of these choices?",['algebra-precalculus']
4499143,Classification of a finite group $G$,"Can we classify a finite $G$ with the given condition: for non-identity elements $x, y \in \mathbb Z_p \times \mathbb Z_p \leq G$ with $ \langle x \rangle \neq \langle y \rangle$ , we have $ \langle g_1x g_1^{-1} \rangle = \langle g_2yg_2^{-1} \rangle$ for some $g_1, g_2 \in G$ ? Here $\mathbb Z_p$ is a cyclic group of prime order. I have noted the following observations: $Z(G)$ is a cyclic subgroup of $G$ if $G$ satisfies the above property; non cyclic abelian group does not satisfy the above property; symmetric group $S_n$ does not satisfy the above property. if every Sylow subgroup of $G$ is either cyclic or generalized quaternion group (for $p= 2$ ), then $G$ satisfies the above properties. Also, I have searched lots on Google  but did not find any clue. I am so thankful for providing any literature or any classification regarding the problem.","['group-theory', 'finite-groups']"
4499156,Solve for $m^3 + m - 10 = 0$,"By the theorem of rational roots, I know that the only rational root is $m = 2$ . But I want to solve by factoring too, from Wolfram I know that $(m - 2)$ and $(m^2 + 2m + 5)$ are the factors, but I don't know how to find out these factors. Knowing the factors, I think its something to do with difference of cubes, but I was unnable to relate to it, since the equation have three terms instead of the two from the diference of cube formula. Can you explain me how to find these factors? Thanks in advance.","['algebra-precalculus', 'factoring', 'polynomials']"
4499158,"$A$ is an $n$ by $n$ matrix over $\mathbb C$ such that $Rank(A)=1$ and $Tr(A)=0$, prove that $A^2=0$","This is what I did:
Because $\mbox{rank}(A)=1$ , then from rank nullity theorem $$\dim\ker A + \mbox{rank} A = n \implies \dim \ker A = n-1$$ and $gm(0)=dimE(0)=dimKerA=n-1$ where $E(0)$ is the eigenspace of the eigenvalue $0$ . So the characteristic polynomial of $A$ is: $C_A(x)=x^{n-1}(x-\lambda)$ where $\lambda$ could be $0$ . It is known that the sum of all eigenvalues of A equals the trace of A (Shown this using the fact that all matrices under $\mathbb C$ are triangularizable), so from that we'll conclude  that $\lambda=0$ and $C_A(x)=x^n$ From here the minimal polynomial of A is $M_A(x)=x^k$ where $k\leq n$ From Cayley–Hamilton $M_A(A)=0 \implies A^k=0$ I'm not sure how to progress from here, I don't really know what I can say about the index of a nilpotent matrix when I know its rank. And I don't think I can say anything about the minimal polynomial given that I know the geometric multiplicity of the only eigenvalue","['matrices', 'nilpotence', 'matrix-rank', 'linear-algebra']"
4499159,"For arbitrary billiard tables with elastic boundary reflections, is the ""Lebesgue measure"" an invariant of the flow maps?","This question pertains to billiard dynamics and their invariant measures. Specifically, it concerns the oft-quoted 'fact' that billiard flow maps (built using specular reflection boundary conditions) admit the Liouville measure (also known as the restriction of the Lebesgue measure to the tangent bundle) as an invariant measure for all times. To be precise, if the billiard table $\mathcal{P}\subset\mathbb{R}^{N}$ is a 'reasonably-nice' subset of Euclidean space (for instance, it admits the structure of a manifold with corners, an example of which would be the unit square if $N=2$ ), I'd like to prove that billiard flow maps $T^{t}:T\mathcal{P}\rightarrow T\mathcal{P}$ admit the property that $$T^{t}\#\mathscr{L}_{2N}\mathsf{L}T\mathcal{P}=\mathscr{L}_{2N}\mathsf{L}T\mathcal{P}$$ for all $t\in\mathbb{R}$ , where: $T\mathcal{P}$ is the tangent bundle of the table $\mathcal{P}$ , considered as a subset of Euclidean space $\mathbb{R}^{2N}$ ; $T^{t}$ is the Lebesgue almost everywhere-defined billiard map built using 'elastic reflection boundary conditions' on $\partial\mathcal{P}$ , whose associated trajectories $t\mapsto T^{t}((x_{0}, v_{0}))$ are piecewise linear and continuous in the spatial variable, but lower semi-continuous in the velocity variable for any initial point $(x_{0}, v_{0})\in T\mathcal{P}$ ; $\mathscr{L}_{2N}$ denotes the Lebesgue measure on $\mathbb{R}^{2N}$ ; $\#$ denotes the pushforward operation, and $\mathsf{L}$ denotes the restriction measure operation, whence $\mathscr{L}_{2N}\mathsf{L}T\mathcal{P}$ is the restriction of the Lebesgue measure on $\mathbb{R}^{2N}$ to the set $T\mathcal{P}$ . One of the main issues in proving this statement is that the dynamics is not smooth, and one cannot appeal to the classical Liouville theorem of symplectic geometry to prove it. Does anyone know of a reference in which the above statement is proved, at least for some class of tables $\mathcal{P}$ ? I have spent quite some time going through the literature, but I have so far come up empty handed. I was trained as a mathematical analyst, so I'm looking for a proof that someone in that community would consider as complete.","['measure-theory', 'billiards', 'analysis', 'statistical-mechanics', 'mathematical-physics']"
4499174,determine whether it is possible that $a_n$ is composite only finitely many times,"Form a sequence $(a_n)$ as follows: Let $a_1$ be any positive integer. Let $a_{n+1}$ be formed from $a_n$ by appending any decimal digit to the end of $a_1$ . Determine, with proof, whether it is possible that $a_n$ is composite only finitely often (i.e. if there exists a value of $a_1$ that makes $a_n$ composite only finitely often). The digits $0, 2, 4, 5, 6, 8$ can only be used finitely many times as otherwise one would get infinitely many composites. The digits $1, 7$ can only be used finitely many times as, after we stop using $2, 5$ and $8$ , they are the only ones to change the remainder modulo $3$ and both add $1$ to it (otherwise there would be infinitely many multiples of $3$ ). Both $3$ and $9$ must be used infinitely many times because, if at some point a prime $p$ is reached, adding at most $p$ of the same digit yields another multiple of $p$ . Even with these restrictions the question seems very hard. For instance, the following are primes of length $9$ : $1979339333, 1979339339$ .","['contest-math', 'divisibility', 'elementary-number-theory', 'discrete-mathematics', 'sequences-and-series']"
4499186,An ideal product with a non zero-divisor,"Let $A$ be a regular domain of finite type over a field. Let $I\subset A$ be an ideal and $f\in A$ be a non zero-divisor. We assume that $f\notin \sqrt{I}$ , and every minimal prime ideal of $I$ has height one. Then it is always true that $I\cdot (f)=I\cap (f)$ ? It is easy to see that $I\cdot (f)\subset I\cap (f)$ . Now for an element $t\in I\cap (f)$ , by the definition of $(f)$ we can write $t=a\cdot f$ for some $a\in A$ . When $I+(f)=(1)$ , i.e. $I$ and $(f)$ are coprime, we know that this is true. But I don't know how to show that we can always choose such $a$ in $I$ in general. I'm wondering if this statement is true, or if there is a counter-example?","['algebraic-geometry', 'commutative-algebra']"
4499202,Does a sphere always admit a triangulation in which the link of a vertex is a sphere?,"In this question , it is asked whether for any triangulation $C$ of a sphere $S^k$ , and for any vertex $v$ of $C$ , the link of $v$ is homeomorphic to a sphere $S^{k-1}$ . This answer shows a concrete example of a triangulation of $S^5$ that does not have this property. MY QUESTION: given a length parameter $\delta$ , does there always exist some triangulation of $S^k$ with simplices of diameter at most $\delta$ , that has the above property? My guess is that the barycentric triangulation should satisfy this property, but I do not know how to prove it (if it is true).","['general-topology', 'triangulation', 'combinatorics', 'simplicial-complex']"
4499216,"Non $0,\pm 1$ integer coefficient polytopes with integral extreme points","Suppose we have a linear system $$Ax\leq b\quad \text{where}\quad A\in \mathbb{Z}^{m\times n},b\in \mathbb{Z}^m.$$ In integer programming literature,   we usually have that $A$ has only $\{0,\pm 1\}$ entries, including totally unimodular matrices. Suppose instead $A$ is integer-valued in general, is there any well-known class of polytopes that has integer-valued extreme points in the literature? Thanks.","['polytopes', 'network-flow', 'integer-programming', 'discrete-mathematics', 'discrete-optimization']"
4499226,Understanding embedded points of a scheme,"Let $X$ be a scheme finite type over a field. The embedded points of $X$ are defined as the embedded points of $\mathcal{O}_X$ as $\mathcal{O}_X$ -module. I'm wondering that is there a more geometric way to understand embedded points? For example, what are embedded points look like when $\dim X=1$ ? If $X$ has no embedded point, then one can show that every irreducible component of $X$ is one-dimensional. I'm wondering if the converse holds, i.e. if every irreducible component of $X$ has dimension one, then $X$ has no embedded point?","['algebraic-geometry', 'schemes', 'commutative-algebra']"
4499235,Proving Hadamard's Variational Formula $\dot{\lambda} = -\int_{\partial U(\tau)}\left|\frac{\partial w}{\partial v}\right|^2v\cdot \nu dS$,"Setting to this question is the following problem from Evans' book Partial Differential Equations (second edition, page 369): Let $U(\tau)_{\tau \in \mathbb{R}}$ be a family of smooth bounded domains in $\mathbb{R}^n$ , which depend smoothly upon the parameter $\tau \in \mathbb{R}$ . Suppose that as $\tau$ changes each point on $\partial U(\tau)$ moves with velocity $v$ . For each $\tau$ , let us consider eigenvalues $\lambda = \lambda(\tau)$ and the corresponding eigenfunctions $w = w(x, \tau):\begin{cases}-\Delta w = \lambda w&: w \in U(\tau)\\w = 0&: w \in \partial U(\tau)\end{cases}$ normalized such that $||w||_{L^2(U(\tau))} = 1$ . Suppose that $\lambda$ and $w$ are smooth functions of $\tau$ and $x$ . Show the Hadamard's variational formula : \begin{equation}
    \begin{aligned}
        \frac{d\lambda}{d\tau} &= -\int_{\partial U(\tau)}\left|\frac{\partial w}{\partial \nu}\right|^2v\cdot \nu dS
    \end{aligned}
\end{equation} This question has already been asked in Hadamard variational formula Evans chapter 6 problem 15 , but I have some questions regarding the given explanation: https://math.stackexchange.com/a/1198921/820472 Namely, 1.) from what does it follow a priori that $\lambda(\tau) = \int_{U}w(-\Delta w)dx = \int_{U}|\nabla w|^2dx$ ? Please see the edit. 2.) I am also stuck at trying to conclude the claimed formula $\dot{\lambda} = -\int_{\partial U(\tau)}\left|\frac{\partial w}{\partial v}\right|^2v\cdot \nu dS$ . Namely, assuming the claimed equality holds for $\lambda(\tau)$ , if I apply the Leibniz formula as suggested, I get $\dot{\lambda}(\tau) = \int_{U(\tau)}(w(-\Delta)w)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\frac{\partial}{\partial \tau}(w(-\Delta)w)dx$ . Substituting $-\Delta w = \lambda w$ gives $\dot{\lambda}(\tau) = \int_{U(\tau)}(\lambda w^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\frac{\partial}{\partial \tau}(\lambda w^2)dx = \int_{U(\tau)}(\lambda w^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\lambda_\tau w^2 + 2\lambda ww_\tau dx$ But as $w = 0$ on $\partial U(\tau)$ , it follows that $\dot{\lambda}(\tau) = \int_{U(\tau)}(\lambda w^2)v \cdot \nu dS(\tau)$ and I have no clue how to proceed in this branch. The other suggested equality yields $\dot{\lambda}(\tau) = \int_{U(\tau)}(|\nabla w|^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\frac{\partial}{\partial \tau}(|\nabla w|^2)dx = \int_{U(\tau)}(|\nabla w|^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\sum_{i=1}^n\frac{\partial}{\partial \tau}\left(\frac{\partial^2}{\partial x_i^2}w\right)^2dx$ where $x := (x_1,\dots,x_n)$ which seems almost the equality we want, provided that we can show $\int_{\partial U(\tau)}\sum_{i=1}^n\frac{\partial}{\partial \tau}\left(\frac{\partial^2}{\partial x_i^2}w\right)^2dx = \int_{\partial U(\tau)}\sum_{i=1}^n2\left(\frac{\partial^2}{\partial x_i^2}w\right)(\frac{\partial^3}{\partial \tau\partial x_i^2}w)dx = 0$ . Unfortunately I don't know how to finish the proof. Edit: I realized just after posting this question that the Hadamard's Variational Formula follows (almost) immediately after applying the Leibniz rule to the equality $\lambda(\tau) = \int_{U}|\nabla w|^2dx$ , as $w \equiv 0$ on $\partial U$ implies that $\int_{\partial U(\tau)}\sum_{i=1}^n\frac{\partial}{\partial \tau}\left(\frac{\partial^2}{\partial x_i^2}w\right)^2dx = 0$ . Therefore my renewed questions are the original 1.) and why $\nabla w || \nu \implies \left|\frac{\partial w}{\partial \nu}\right|^2 = |\nabla w|^2$ ? That is, why if the gradient is parallel to $\nu$ we have that the derivative of $w$ w.r.t. $\nu$ is the squared gradient of $w$ ?","['divergence-theorem', 'multivariable-calculus', 'partial-differential-equations', 'differential-geometry']"
4499236,Name of algebraic structure (group-like),"What is the name of an algebraic structure (group, quasi group, monoid) defined like the following: It is a set of three elements: $a,b,c$ ; Has an operation such that when applied to two elements of the group it returns the other element of the group (example: $ab=c$ ; $cb=a$ ); Has an ""identity"" structure such as $aa=a$ , $bb=b$ , $cc=c$ ; It is commutative: $ab=ba$ ; also $a(ba)=ac=b$ . This is no homework. I'm just trying to define a structure like this in order to learn, but as I'm reading about group-theory and semigroups it doesn't seem to really define this (for example, the ""identity"" here is ambiguous - so how should I call it?) Thank you!","['group-theory', 'abstract-algebra', 'abelian-groups', 'category-theory']"
4499241,Interpreting probability question in terms of measure theory,"Let $X$ and $Y$ be real valued random variables with joint pdf $$f_{X,Y}(x,y)=\begin{cases}\frac{1}{4}(x+y), & 0\leq x\leq y\leq 2 \\ 0, & \text{ otherwise }\end{cases}
$$ Calculate the probability $\mathbb{P}\{Y<2X\}$ . I am trying to view this problem in a measure-theoretic perspective and I am wondering if I am thinking about this properly. Let $(\Omega ,\mathcal{F},\mathbb{P})$ be a probability space on which we define two random variables (measurable functions) $X$ and $Y$ . We then consider their joint distribution, the pushforward measure of the random variable $T(\omega )=(X(\omega ),Y(\omega ))$ on $\left (\mathbb{R}^2,\mathcal{B}\times \mathcal{B}\right )$ defined by $$T_\star \mathbb{P}(A)=\mathbb{P}\left (T^{-1}(A)\right )=\mathbb{P}((X(\omega ),Y(\omega ))\in A).$$ Then by the question, we have that $\dfrac{dT_\star \mathbb{P}}{d\lambda}=f_{X,Y}$ i.e. the Radon-Nikodym derivative of the pushforward of $T$ with respect to the Lebesgue measure is the pdf of $(X,Y)$ . How can I then calculate $\mathbb{P}\{Y<2X\}$ ? Somehow I have to relate the probability of this set to the pushforward measuere of which I know the density. What is the theorem that allows me to relate these two measures? Essentially, im looking for the measure of the set $\{\omega :Y(\omega )<2 X(\omega )\}\subset \Omega$ . I know the density of a measure on $\mathbb{R}^2$ which is a different set than $\Omega$ . How do I know that $\{(x,y):y<2x\}\subset \mathbb{R}^2$ is the subset of $\mathbb{R}^2$ with which I need to integrate over?","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4499258,"How is the Lorentz group, $\text{O}(1,3)$, defined using set theoretic notation?","Context I am studying special relativity. I am trying to understand how to define the group elements of the Lorentz group , $\text{O}(1,3)$ .  I understand from [1]  that the Lorentz group is has (at least 3) subgroups. These are $\text{O}^+(1,3)$ , $\text{SO}(1,3)$ and $\text{SO}^+(1,3)$ . From [1], Every element in $\text{O}(1,3)$ can be written as the semidirect product of a proper, orthochronous transformation [i.e., an element of $\text{SO}^+(1,3)$ ] and an element of the discrete group $\left\{1, P, T, PT\right\}$ , where P and T are the parity and time reversal operators [i.e., $P = \textrm{diag}(1, −1, −1, −1)$ and $T = \textrm{diag}(−1, 1, 1, 1)$ ]. This quote makes clear how $\text{O}(1,3)$ and $\text{SO}^+(1,3)$ are related to each other. This fact is referred to by @arctic tern in his/her answer in [2]. However, this quote does not help me define $\text{O}(1,3)$ . To be clear what I am looking for is something like a set theoretic definition of $\text{O}(1,3)$ . Something like what I see in the example of Lie groups found in [3]. For example, I believe that $\text{O}(1,3)\subset \text{GL}(4, \mathbb{R})$ . Therefore, I should be able to define the Lorentz group with some additional predicates. $$\text{O}(1,3) \equiv \left\{M\in \text{GL}(4, \mathbb{R}) \mid \operatorname{Predicate 1}, \operatorname{Predicate 2}, \ldots \operatorname{Predicate n}\right\}.$$ The Lorentz group is given as an orthogonal group . However, though the inverse of the Lorentz boosts are equal to the transpose of the Lorentz boosts , it does not appear that the the inverse of the Lorentz rotations are equal to the transpose of the Lorentz rotations . Question How is the Lorentz group, $\text{O}(1,3)$ , defined using set theoretic notation? Bibliography [1] https://en.wikipedia.org/wiki/Lorentz_transformation#boost [2] Difference between the Lorentz group and the restricted Lorentz group [3] https://en.wikipedia.org/wiki/Lie_group","['special-relativity', 'group-theory', 'lie-groups']"
4499327,Questions about Poincaré–Bendixson theorem,"I just finished reading Schwartz's article on the generalization of the Poincaré-Bendixson theorem to compact two-dimensional manifolds. Which says: Let $M$ be a two-dimensional differentiable manifold of class $C^2$ compact and connected. Let $\varphi: \mathbb R \times M \to M$ be a flow of class $C^2$ in $M$ . A minimal set $\mu\subset M$ can be either: a singular point, or a periodic orbit, or an entire manifold $M$ which is homeomorphic to the torus $T^2$ . But a few questions popped into my head that I couldn't answer: Do you know any references where I can find the proof of the Poincaré-Bendixson theorem in the projective plane $\mathbb P^2$ ? What happens to $\mathbb P^3$ ? Does anyone know any example of dimension 3 that is under the conditions of Schwartz's theorem whose omega limit is different from a singular point, a closed orbit and the torus? Do you know any differentiable manifold of dimension greater than two where the flow is interesting to study? That is to say, some variety in which it is possible to attempt some sort of smooth generalization of the Poincaré-Bendixson theorem? I was quite impressed with the $C^2$ class flows in the torus. Does anyone know what happens to $C^2$ flows in the $2-$ torus? or in the $n-$ torus? I know there are a lot of questions and I'm sure they'll get my attention because of that, but all the questions are closely related to the same topic and I didn't want to create too many separate questions because there might be some future readers wondering the same thing. I hope you can help me clear these doubts and continue learning.","['smooth-manifolds', 'ordinary-differential-equations', 'dynamical-systems']"
4499329,"drawing two coins, what is the probability you get head on one coin, and tail on the other?","there are three coins, probability that you get a head in the first coin is 0.5 (a fair coin), probability that you get head in the second coin is 0.75, and third coin is a two-headed coin. if you draw two coins randomly (without replacement), and you toss these coins, what is the probability you get exactly head on one coin, and tail on the other coin? This exercise gives a tip: order doesn't matter, and therefore I have a combination. I know that when I have the and operator, I have to do a product because it's an intersection, meaning the first must be true, and so the second. I know it's without replacement too, and so it's an hypergeometric distribution. Hypergeometric distribution accepts three parameters, namely $(h, t, n)$ , $h$ is head, $t$ is tails, and $n$ is  the number of coins I'm drawing. hypergeometric distribution has a product in the numerator, and that product is define as $C(h, k) \cdot C(t, n-k)$ , and at the denominator I have the sample space and it's defined as $C(h+t, n)$ (this notation means combinations). I don't know how to use what I've written above to solve this assignment. what are values of $h, k$ , and $t$ ? I'm having problems understanding the assignment.","['combinatorics', 'probability-theory']"
4499350,Minimum of a function defined by a sum,"I have the following function, for a fixed integer $n\in \mathbb N$ , $\displaystyle\varphi(x) = \sum_{k=0}^{n} \frac{(-1)^k}{x-k}$ , for $x\neq l$ for all $l\in\{0,\ldots,n\}$ . I am suspecting that $\min\limits_{x\in (0,n)} \left|\varphi(x)\right|\ge 2$ and want to prove it. So my question is Is it true what I am suspecting for all $n\in\mathbb N$ ? If not what values of $n$ this is true? I checked for $n=1$ and I proved easily that the minimum is larger than $4$ . Can anyone help me with this in general case?","['maxima-minima', 'functions', 'real-analysis']"
4499354,1954 Miklos Schweitzer - Limit Distribution,"Source : 1954 Miklos Schweitzer, Problem 5 Let $\xi _{1},\xi _{2},\dots ,\xi _{n},... $ be independent random variables of uniform distribution in $(0,1)$ . Show that the distribution of the random variable $$\eta _{n}= \sqrt[]{n}\prod_{k=1}^{n}(1-\frac{\xi _{k}}{k}) (n= 1,2,...)$$ tends to a limit distribution for $n \to \infty $ . Attempt : First I factor out a $\frac{1}{k}$ term from the product to get $$\sqrt[]{n}\prod_{k=1}^{n}(1-\frac{\xi _{k}}{k}) = \frac{1}{\sqrt{n}(n-1)!}\prod\limits_{k=1}^n (k-\xi_k).$$ Here, we have $k-\xi_k\sim \text{Unif}(k-1,k)$ and so then with $Y_k=-\log(k-\xi_k)$ and $\eta_n=-\log(\sqrt{n}(n-1)!)-\sum_{k=1}^n Y_k$ , we have $$f_{Y_k}(y)=\frac{1}{k-(k-1)}\cdot |-e^{-y}|=e^{-y}.$$ I thought this may be promising as it looks exponential, but the support doesn't match up - here we have $y\in[-\log(k),-\log(k-1)]$ . Additionally, I took the approach I did because I wanted to make this appear as exponentials whose sum is Gamma, but I realized this is dumb as we have an infinite number of them so even with aligned support, the resulting random variable is infinite WP1. The (n-1)! term on the denominator made me think of the Erlang distribution, but the exponential-looking random variables are not identically distributed and again this is limiting, not finite. Alternatively, I figured by inspection, it looks like we could apply the CLT and obtain something normal-looking, but I got tired looking at the problem. Edit : As per Brian's suggestion, we could take the log first $$\log(\eta_n) = \log(\sqrt{n})+\sum\limits_{k=1}^n \log(1-\frac{\xi_k}{k}).$$ Then letting $X_k=\log(1-\frac{\xi_k}{k})$ , we have for $x\in(\log(1-\frac{1}{k}),0)$ , $$f_{X_k}(x)=ke^x.$$ Taking the Laplace transform gives us $$\mathcal{L}(f_{X_k})=\mathbb{E}\left[e^{-tX_k}\right]=\int\limits_\Omega e^{-tx}\cdot ke^x\;dx$$ $$=\frac{k}{t+1}\left[\left(1-\frac{1}{k}\right)^{-(t+1)}-1\right].$$ This looks a bit horrid (possible I messed up the algebra), then with this, the idea is we'd have $$\mathbb{E}\left[e^{-t\sum_{k=1}^n X_k}\right]=\prod\limits_{k=1}^n \frac{k}{t+1}\left[\left(1-\frac{1}{k}\right)^{-(t+1)}-1\right]$$ by independence and we'd be able to identify this distribution. But two problems with this for me: 1) there's the lingering $\sqrt{n}$ term I don't know what to do with and 2) not sure what distribution this looks like. Question : I am all for hints on how to approach this correctly - thanks.","['contest-math', 'limits', 'probability-distributions', 'probability']"
4499358,Finding the MSE of some Estimator $\hat{\theta}$ the best way possible.,"I'd glad if you could help me. I was given the following question in my exam, but i just couldn't finish that on time. I wonder if there's some easier way to solve this. Given the following random samples $(X_1, X_2,...,X_n)$ of independent random variables with the same probability density function: $$f_X(x)=
\begin{cases}
e^{\lambda - x},  & \text{$x\ge \lambda$ } \\
0, & \text{$x < \lambda$}
\end{cases}$$ where $\lambda >0$ . We want to estimate the unknown parameter $\theta=\lambda$ . We are given with the following Estimator: $\hat{\theta}=X_{(1)}-\frac{1}{n}$ where $X_{(1)}=min\{X_1,X_2,...,X_n\}$ and we need to find it's MSE (Mean Square Error: $E\big((\hat{\theta}-\lambda)^2\big)$ . I already solve this problem, but that was too complicated. I wonder if there's a ""catch"". I don't know, maybe we can identify some familiar distributaion along the way, something that can help us find the Expected Value much quicker. That's usually the case. My calculation was: Fiding CDF of $X_{(1)}: \quad  F_{X_{(1)}}(t)=1-e^{n(\lambda -x)}$ Fiding PDF of $X_{(1)}: \quad  f_{X_{(1)}}(t)=ne^{n(\lambda -x)}$ Fiding Expected Value of $X_{(1)}:$ $$ E(X_{(1)})= \int_{\lambda}^{\infty}xne^{n(\lambda -x)}dx= \text{...long calculation...} =\lambda + \frac{1}{n}$$ here we can see that $\hat{\theta}=X_{(1)}-\frac{1}{n}$ is unbiased: $E\big(\hat{\theta} \big) = E \big( X_{(1)}-\frac{1}{n} \big) = 
E\big( X_{(1)}\big) - E\big( \frac{1}{n} \big) = \lambda + \frac{1}{n} - \frac{1}{n} = \lambda = \theta$ , so one last thing to do is to find the Variance of $\hat{\theta}$ : Find $$ V(\hat{\theta}) = V( X_{(1)}-\frac{1}{n} ) = V( X_{(1)} )  =  E \big( (X_{(1)} - \lambda)^2 \big) = \int_{\lambda}^{\infty} {(x-\lambda)^2ne^{n(\lambda -x)}}dx= \text{...too long calculation. I calculated using WolframAlpha} = \frac{2}{n^2} = MSE(\hat{\theta})$$","['statistics', 'mean-square-error', 'probability-distributions', 'parameter-estimation', 'probability']"
4499377,Interpreting the ratio test for $\lim_{n\to\infty} \sum_{k=1}^n \frac{n}{n^2+k} $,"We want to find the limit of this. $$\lim_{n\to\infty} \sum_{k=1}^n \frac{n}{n^2+k} $$ I would have done it as follows: $$\lim_{n \to \infty} \bigg| \frac{a_{n+1}}{a_n}\bigg| = \lim_{n\to\infty}\frac{\frac{n+1}{(n+1)^2+n}}{\frac{n}{n^2+n}} =\lim_{n\to\infty} \frac{n+1}{(n+1)^2+n} \cdot \frac{n^2+n}{n} = \lim_{n\to\infty}\frac{n^3+2n^2+n}{n^3+3n^2+n} \\
=\lim_{n\to\infty} \frac{n^3 \cdot \bigl(1+\frac{2}{n} + \frac{1}{n^2} \bigr)}{n^3\cdot\big(1+\frac{3}{n} + \frac{1}{n^2} \bigr)} = \frac{1}{1} = 1$$ According to the ratio test, the series converges if $\lim_{n \to \infty} \bigg| \frac{a_{n+1}}{a_n}\bigg| <1$ and it diverges if it's $> 1$ . But since we get $1$ here, the series converges towards that value, no? But according to the ratio test, we can't make a statement about the series of the limit of $\lim_{n \to \infty} \bigg| \frac{a_{n+1}}{a_n}\bigg| = 1$ What am I misunderstanding here?","['limits', 'calculus', 'sequences-and-series']"
4499386,Is it known whether there exist arbitrarily large gaps between consecutive Carmichael numbers?,"Do you have any references in the literature where an argument is given to answer this question? Is it a well known result? If it is, a simple Google search returned nothing. By a simple brute force search, I will have to bet that the existence of such gaps is unlikely.","['number-theory', 'reference-request']"
4499427,A especial case of Eichler and Shimura,"in Silverman Tate, Rational Points on Elliptic Curves the exercise 4.6 describes a special case of a theorem of Eichler and Shimura, the exercise is about the elliptic curve $$C:y^2=x^3-4x^2+16,$$ let $M_p=\#C(\mathbb{F}_p)$ and $F(q)$ the formal power series $$
F(q)=q\prod_{n=1}^{\infty}(1-q^n)^2(1-q^{11n})^2=q-2q^2-q^3+2q^4+\dotsb
$$ let $N_m$ be the coefficient of $q^n$ in $F(q)$ $$F(q)=\sum_{n=1}^{\infty}N_mq^n$$ . The exercise ask for compute the sum $M_p+N_p$ and formulate and prove a conjecture. I run a computation in Sage and its seems that the conjecture is that for every prime $p$ we have $M_p+N_p=p$ . I have the following questions Is this the result of Eichler and Shimura? I see the product in LMFB and $F(q)$ is equal to $\eta(z)^2\eta(11z)^2$ , so I think that the curve $C$ is the modular curve $X_0(11)$ , is this correct? I saw the same product in Taylor Modular Arithmetic: Driven by Inherent Beauty and Human Curiosity , in this case for the curve $y^2+y=x^3-x^2$ , are this the same curve $C$ ? (I mean birrationaly isomorphic curves) I dont know about modular forms, I look at the paper The modular curves $X_0(11)$ and $X_1(11)$ , and its seems that there is a path from the modular form to the elliptic curve, there is also a path that begins in the elliptic curve and produces $F(Q)$ ? (I know that this is the Modularity Theorem Shimura-Taniyama), I mean in this specific case there is a elementary path that produces the modular form $F(q)$ from the curve $C$ ? In general what are the suggestions for this special case, I mean in this case is an exercise of a book for undergraduates, so I think that the author considers that an advanced undergraduate can do this exercise, but I don see any suggestions in the book. Thanks in advance! Sorry for my bad English 😔.","['number-theory', 'modular-forms', 'elliptic-curves']"
4499429,Prove the convergence of the sequence given by the recurrence relation,"I have the following problem: Let us $v_1^{(0)}, v_2^{(0)}, v_3^{(0)} > 0$ . Define the sequence $$ v_1^{(k+1)} = \dfrac{3}{ \dfrac{2}{v_1^{(k)} + v_3^{(k)}} + \dfrac{4}{v_1^{(k)} + v_2^{(k)}} } $$ $$ v_2^{(k+1)} = \dfrac{4}{ \dfrac{4}{v_1^{(k)} + v_2^{(k)}} + \dfrac{2}{v_2^{(k)} + v_3^{(k)}} } $$ $$ v_3^{(k+1)} = \dfrac{1}{ \dfrac{2}{v_1^{(k)} + v_3^{(k)}} + \dfrac{2}{v_2^{(k)} + v_3^{(k)}} } $$ Prove that the sequence $\{v^{(k)}\}_k = \{(v_1^{(k)}, v_2^{(k)}, v_3^{(k)})\}_k$ converges. Most of the proof methods that I know require knowledge of the limit of the sequence. In this case, it is difficult for me to guess what the expression for the sequence limit should be. Without knowing the limit, it would be possible to prove that the sequence is fundamental, but the calculations look too cumbersome. I tried some numerical experiments and it looks like the sequence converges for any non-zero $v_1^{(0)}, v_2^{(0)}$ and $v_3^{(0)}$ . In this case, for different starting values ​​ $v_1^{(0)}, v_2^{(0)}$ and $v_3^{(0)}$ , the sequence converges to different limit points, however, all limit points lie on a straight line with direction vector $(0.5333965 , 0.8173959 , 0.21760542)$ . But this fact looks clear from the form of the recurrence formula. I tried to calculate the Jacobian of the recurrence mapping at the limit points. Predictably, all eigenvalues ​​are less than 1 except one, which is 1. The eigenvector corresponding to an eigenvalue equal to 1 is $(0.5333965 , 0.8173959 , 0.21760542)$ . This result also looks predictable. Another interesting observation that I got from numerical experiments is that the ratio $$ \dfrac{\|v^{(k+1)} - v^{*}\|_\infty}{\|v^{(k)} - v^{*}\|_\infty}$$ stabilizes fairly quickly as $k$ increases. Here $v^{*}$ denotes the limit of the sequence. An example of $ \dfrac{\|v^{(k+1)} - v^{*}\|_\infty}{\|v^{(k)} - v^{*}\|_\infty}$ ratio values ​​when running from a random point. $$0.08030587438292239$$ $$0.2095128000211839$$ $$0.31076298843195116$$ $$0.35864703173973156$$ $$0.35707344342023817$$ $$0.3565920036480916$$ $$0.3564327245958076$$ $$0.3563787026001393$$ $$0.3563602243686815$$ $$0.35635388477426155$$ However, these are all numerical experiments, I do not have any rigorous evidence. I welcome any advice, ideas, observations, or references to the literature. Thanks in advance for your help!","['recurrence-relations', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
4499442,"Prove that a compact subset of a Hausdorff space is closed. Give an example which shows that the ""Hausdorff"" assumption is required.","Prove directly from the definitions, that a compact subset of a Hausdorff space is closed. Give an example which shows that the ""Hausdorff"" assumption is neccessary. Let $X$ be a topological space and $A \subset X$ compact. Let $x \in X \setminus A$ . For every $a \in A$ pick disjoint neighborhoods $U_a$ of $a$ and $V_x(a)$ of $x$ . Since $A$ is compact, the set $\{U_a \mid a \in A\}$ is a cover of $A$ and thus has a finite subcover $\{U_a \mid a \in F \}$ for $F$ finite. Now $$U= \bigcup \{U_a \mid a \in F\} \text { and } V=\bigcap\{V_x(a) \mid a \in F\}$$ are disjoint neighborhoods of $A$ and $\{x\}$ . Using this we can find a neighborhood $V_x$ for $x \in X \setminus A$ that is disjoint from $A$ and therefore $x \in V_x \subset X \setminus A$ implies that $X \setminus A$ is open and $A$ closed. I don't know how I can give an example that this doesn't work without the Hausdorff condition. The simplest non-Hausdorff space I could think of was $(X, \{X, \emptyset\})$ , but I don't know if I can work with this as I don't have any neighborhoods for the points except for $X$ itself?",['general-topology']
4499444,Proof Verification: $\int_{-1}^{1}\frac{\ln{(x+1)}}{x}dx = \frac{\pi^2}{4}$,"While curiosity gets the best of me in the fascinating world of complex analysis, I have decided to tackle the following integral and prove it's equal to $\frac{\pi^2}{4}$ : $$\int_{-1}^{1}\frac{\ln{(x+1)}}{x}dx.$$ Proof. Let $f(z) = \frac{\text{Log}(z+1)}{z}$ where $\text{Log}$ denotes the principal logarithm. We will traverse, in a counterclockwise direction, a path that closely resembles a semicircle of radius 1 above the real axis such that: there is a small indented semicircle path $\gamma_1$ above $z=0$ ; another path $\gamma_2$ closely resembling a small quarter circle to the right of $z=-1$ ; a path $\Gamma$ representing the circumference. We will call this path $C$ , which is $$C = \left[-1+ \epsilon, -\epsilon\right] \cup \gamma_1 \cup \left[\epsilon,1\right] \cup \Gamma \cup \gamma_2.$$ There are no singularities inside $C$ , so by Cauchy's Theorem, we have $$\oint_Cf(z)dz = 0.$$ But we also have $$\eqalign{
\oint_Cf(z)dz &= \int_{-1+\epsilon}^{\epsilon}f(z)dz + \int_{\gamma_1}f(z)dz + \int_{\epsilon}^{1}f(z)dz + \int_{\Gamma}f(z)dz + \int_{\gamma_2}f(z)dz \cr
&= \int_{-1+\epsilon}^{\epsilon}f(z)dz - \int_{-\gamma_1}f(z)dz + \int_{\epsilon}^{1}f(z)dz + \int_{\Gamma}f(z)dz - \int_{-\gamma_2}f(z)dz.
}$$ We will call each integral $I_1$ , $I_2$ , $...$ , $I_5$ , respectively. First, we will prove that $I_2$ goes to $0$ . Parameterizing $z = \epsilon e^{i\theta}$ where $\theta \in \left[0,\pi\right]$ , we get $$\eqalign{
I_2 :&= \int_{-\gamma_1}\frac{\text{Log}(z+1)}{z}dz \cr
&= i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta.
}$$ Finding the upper bounds of that integral, we observe that $$\eqalign{
0 &\leq \left|i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta\right| \cr
&\leq \epsilon \int_{0}^{\pi}\left|\text{Log}\left(\epsilon e^{i\theta} + 1\right)\right|d\theta \cr
&= \epsilon \int_{0}^{\pi}\left|\text{Log}\left|\epsilon e^{i\theta} + 1\right| + i\text{Arg}\left(\epsilon e^{i\theta} + 1\right)\right|d\theta \cr
&\leq \epsilon \left(\left|\text{Log}\left|\epsilon e^{i\theta}+1\right|\right| + \pi\right) \cr
&\leq \epsilon(\epsilon + 1 + \pi).
}$$ Taking the limit as $\epsilon$ goes to $0$ , we can apply the Squeeze Theorem: $$\lim_{\epsilon \to 0} 0 \leq \lim_{\epsilon \to 0}\left|i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta\right| \leq \lim_{\epsilon \to 0}\epsilon(\epsilon + 1 + \pi)$$ $$0 \leq  \lim_{\epsilon \to 0}\left|i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta\right| \leq 0.$$ This shows that $$\lim_{\epsilon \to 0}i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta = 0.$$ Albeit more tedious work, I can apply the same strategies for proving $I_5$ goes to $0$ . Let $\Phi_{\epsilon}$ represent the small arc length cut off from $\Gamma$ . Parameterize $z = -1 + \epsilon e^{i\Phi}$ where $\Phi \in \left[0, \frac{\pi}{2} - \Phi_{\epsilon}\right]$ Then (skipping some work) we get $$\eqalign{
I_5 :&= \int_{-\gamma_2}f(z)dz \cr
&= \int_{0}^{\frac{\pi}{2} - \Phi_{\epsilon}}f(-1 + \epsilon e^{i\Phi})d(-1 + \epsilon e^{i\Phi}) \cr
&= i\epsilon \ln{(\epsilon)}\int_{0}^{\frac{\pi}{2}-\Phi_{\epsilon}}\frac{e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}d\Phi - \epsilon \int_{0}^{\frac{\pi}{2}-\Phi_{\epsilon}}\frac{\Phi e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}d\Phi.
}$$ Both integrals go to $0$ as $\epsilon$ goes to $0$ since $$\left|\frac{e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}\right| \leq \frac{1}{1-\epsilon} \text{ and } \left|\frac{\Phi e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}\right| \leq \frac{\Phi}{1-\epsilon}.$$ For $I_4$ , we can parameterize $z = e^{it}$ where $t \in \left[0, \pi - \Phi_{\epsilon}\right]$ to get $$\eqalign{
I_4 :&= \int_{\Gamma}f(z)dz \cr
&= \int_{0}^{\pi - \Phi_{\epsilon}}f(e^{it})d(e^{it}) \cr
&= i\int_{0}^{\pi - \Phi_{\epsilon}}\text{Log}\left(1+e^{it}\right)dt \cr
&= i\int_{0}^{\pi - \Phi_{\epsilon}}\text{Log}\left(2e^{i\frac{t}{2}}\cos{\left(\frac{t}{2}\right)}\right)dt \cr
&= i\int_{0}^{\pi - \Phi_{\epsilon}}\ln{\left(2\cos{\left(\frac{t}{2}\right)}\right)}dt - \frac{\left(\pi-\Phi_{\epsilon}\right)^2}{4}.
}$$ As both $\epsilon$ and $\Phi_{\epsilon}$ approach $0$ , we see that $\ln{\left(2\cos{\left(\frac{t}{2}\right)}\right)}$ is Lebesgue integrable on $\left[0,\pi\right]$ . It follows that $$\int_{0}^{\pi}\ln{\left(2\cos{\left(\frac{t}{2}\right)}\right)}dt = 0.$$ As $\Phi_{\epsilon}$ goes to $0$ , we see that $I_4$ goes to $-\frac{\pi^2}{4}$ . Going back to $C$ , as $\epsilon$ and $\Phi_{\epsilon}$ approach $0$ , we get $$\eqalign{
\lim_{\epsilon,\Phi_{\epsilon} \to 0}0 &= \lim_{\epsilon,\Phi_{\epsilon} \to 0}\left(\int_{-1+\epsilon}^{\epsilon}f(z)dz - \int_{-\gamma_1}f(z)dz + \int_{\epsilon}^{1}f(z)dz + \int_{\Gamma}f(z)dz - \int_{-\gamma_2}f(z)dz\right) \cr
0 &= \int_{-1}^{0}f(z)dz - 0 + \int_{0}^{1}f(z)dz - \frac{\pi^2}{4} - 0.
}$$ Therefore, $$\int_{-1}^{1}\frac{\ln{(x+1)}}{x}dx = \frac{\pi^2}{4}.$$ Q.E.D. Is there a need for the $\gamma_1$ indented path? Because we know on the real line, the limit as $x$ goes to $0$ of $f(x)$ exists unlike when $x$ goes to $-1$ from the right, which results in $f(x)$ going off to $\infty$ since $x=-1$ is a vertical asymptote. Other than that, I want to say my proof is good enough. Let me know if you know any other strategies you have (I know a much simpler way but I wanted to find different ways to solve the problem). If you have any suggestions on optimizing the solution or find any errors, please do not hesitate to share them with me!","['integration', 'definite-integrals', 'complex-analysis', 'solution-verification', 'complex-integration']"
4499463,Injectivity condition of parametrised statistical manifold,"I am currently learning information geometry by following this note .  It starts by defining the set of probability distribution functions $$
S=\{p_\xi=p(x,\xi)\mid \xi=(\xi_1,\ldots, \xi_n)\in O\subset \mathbb{R}^n\},
$$ where $p(x,\xi)$ is a probability distribution function  on space $\Omega$ . It calls $O$ the parameter space, and requires $\xi\mapsto p_\xi$ to be injective, and $O$ is open. However, I find the injectivity condition is very strong for many nonlinear models. For example, let us  consider  the  following softmax models: $$
S=\left\{p_\xi(i)=\frac{e^{\xi_i}}{\sum^n_j e^{\xi_j}}, i=1,\ldots, n\mid \xi=(\xi_1,\ldots, \xi_n)\in \mathbb{R}^n\right\}.
$$ Even though $p_\xi(i)=\exp\left(\xi_i-\log(\sum^n_j e^{\xi_j})\right)$ is of the exponential form, the map $\xi\mapsto p_\xi$ is not injective. Indeed, $p_{\xi}=p_{\tilde{\xi}}$ if $\xi=\tilde{\xi}+C\boldsymbol{1}$ for any $C\in \mathbb{R}$ , where $\boldsymbol{1}$ denotes the vector with all entries being $1$ .  Similarly, for many neural networks, the parameterisation is noninjective. However the   optimisation problem $\inf_{p\in S}J(p)$ can still be well-defined, in the sense that there exists a unique minimiser $p^\star\in S$ (associated with many parameterisations). Question . May I know whether this injectivity condition is essential to apply the geometric techniques  to study the set $S$ ?
More precisely, can we still equip $S$ with   the well-known Fisher-information metric,
and make $S$ a Riemannian manifold? If the injectivity is crucial, does it mean the information geometry is not suitable to study optimisation over softmax models, or more general neural network models? I am not sure whether there is a well-known technique to address this difficulty.","['information-geometry', 'statistics', 'riemannian-geometry', 'differential-geometry']"
4499540,"Find the number of one-one functions $f:\{a,b,c,d\}\to\{0,1,2,3,...,10\}$ such that $2f(a)-f(b)+3f(c)+f(d)=0$.","Find the number of one-one functions $f:\{a,b,c,d\}\to\{0,1,2,3,...,10\}$ such that $2f(a)-f(b)+3f(c)+f(d)=0$ . My Attempt I rearranged the equation like this $f(b)=2f(a)+3f(c)+f(d)$ . Now, if $f(c)=0$ then $2f(a)+f(d)\leq 10$ Can the problem be solved by any method other than counting all cases manually. I kept on taking cases and ended up with 31 cases. The answer given was $31$ . Can it be done by some other approach","['permutations', 'algebra-precalculus', 'functions', 'diophantine-equations']"
4499553,Is there Any Study on the Sum of $e^{-\sqrt{n}t}$?,"I've been messing around with some things after looking at Jacobi Theta Functions and I happened to stumble upon these unexpectedly (at least to me) nice Laurent expansion and integral representation of a curious sum: $$
\begin{split}
\sum_{n=0}^\infty e^{-\sqrt{n}t}
&=
\frac12+\frac1{\sqrt{\pi}}\int_0^\infty\coth\left(\frac{t^2}{8x^2}\right)e^{-x^2}dx
\\
&=
\frac2{t^2}+\frac12-\frac1{\sqrt{\pi}}\sum_{n=1}^\infty \sin\left(\frac{\pi k}4\right) \frac{(-t)^k\zeta(1+\frac k2)}{(8\pi)^{k/2}\Gamma(\frac{1+k}{2})}
\end{split}
$$ Has this series been studied somewhere? Is there literature that I could possibly read on it? I don't even know what to call the thing, much less how to search for resources on it online.",['sequences-and-series']
4499631,Solve for eigenvector as rational function of eigenvalue,"Let $A$ be an $n$ by $n$ matrix with entries in $\{0,1\}$ , and such that some power $A^k$ is strictly positive, so Perron-Frobenius stuff applies, i.e. there is a real top eigenvalue $\lambda$ with multiplicity $1$ . Let $v$ be a corresponding eigenvector, $Av = \lambda v$ . Q: Is it always possible to write $v$ as a quotient of polynomials in $\lambda?$ If so, is there a simple procedure to determine those rational functions? I'm also wondering if the tools to do this in Python (or Sage perhaps?) already exist, i.e. given the matrix $A$ , spit out an explicit expression for the eigenvector $v$ in terms of $\lambda$ . A bit of guessing works for small examples, e.g.: $A = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}$ has characteristic polynomial $x^3 - x^2 - x - 1$ , with top eigenvalue $\lambda \approx 1.839$ , and with a bit of elbow grease, $v = \begin{bmatrix} 1 \\ \lambda - 1 \\ \lambda^{-1} \end{bmatrix}$ is a corresponding (right) eigenvector. I want to be able to do the same thing in general. I'm particularly interested in the $8$ by $8$ matrix $\begin{bmatrix} 1& 1& 0& 0& 0& 0& 0& 0 \\ 0& 1& 1& 0& 0& 0& 0& 0 \\ 0& 1& 0& 1& 0& 0& 0& 0 \\ 1& 0& 0& 0& 1& 0& 0& 0 \\ 0& 0& 1& 0& 0& 1& 0& 0 \\ 0& 1& 0& 0& 0& 0& 1& 0 \\ 0& 0& 0& 1& 0& 0& 0& 1 \\ 0& 1& 0& 0& 0& 0& 0& 0 \end{bmatrix}$ . If someone can handle this guy, even with ad-hoc ideas, it would be helpful! Edit: Ok, the 8x8 matrix isn't too hard to do by hand. So it's not so enlightening regarding a general method. Edit 2: To clarify, I'm looking for a formula or method that's simpler than Gaussian elimination or the like. There are sometimes clever methods for these sorts of things -- the 'cover up' method for partial fractions comes to mind.","['linear-algebra', 'rational-functions', 'eigenvalues-eigenvectors']"
4499642,How much could rearrangements affect Hoeffding's inequality?,"Suppose $X,X_1,X_2,X_3\dots$ is a $\mathbb{P}$ -i.i.d. family of $[-1,1]$ -valued random variables with $\mathbb{E}[X] = 0$ . By Hoeffding's inequality , we know that \begin{equation*}
   \forall T \in \mathbb{N}, \forall \delta \in(0,1), \qquad\mathbb{P}\bigg[ \frac{1}{T} \sum_{t=1}^T X_t \ge \sqrt{\frac{2}{T} \log\Big(\frac{1}{\delta}\Big)} \bigg] \le \delta\;.
\end{equation*} I'm wondering if a better upper bound than the $(2^T-1) \cdot \delta$ (that follows from a union bound) holds on the quantity \begin{equation*}
\mathbb{P}\Bigg[ \bigcup_{\emptyset\neq A \subset \{1,\dots,T\}} \bigg\{\frac{1}{|A|} \sum_{t\in A} X_t \ge \sqrt{\frac{2}{|A|} \log\Big(\frac{1}{\delta}\Big)} \bigg\} \Bigg]\;,
\end{equation*} where $|A|$ is the number of elements in $A$ . Specifically, I'm looking for an upper bound of (nearly) the form $O(T^\alpha)\cdot \delta$ , for some $\alpha > 0$ . I suspect this could be true, due to the highly entangled structure of this union. To get a more specific idea of why this could be the case, this answer to a question similar in spirit proves that the bound coming from a union bound is far from tight. So far, I tried to apply the aforementioned answer in the following way. We say that a set $\{A_1, \dots, A_T\}$ is a string if $A_1 \subset \dots \subset A_T$ , $|A_k| = k$ and $A_k \subset \{1,\dots,T\}$ , for each $k \in \{1,\dots,T\}$ . We say that a family of strings $\mathcal{A}_1,\dots,\mathcal{A}_m$ is a string-cover of $\{1,\dots,T\}$ if for each $A \subset \{1,\dots,T\} \backslash \{\emptyset\}$ there exists $k \in \{1,\dots,m\}$ such that $A \in \mathcal{A}_k$ . Then if $\mathcal{A}_1,\dots,\mathcal{A}_m$ is a string-cover of $\{1,\dots,T\}$ , by the previous answer and a union bound we have that the probability we are trying to upper bound is upper bounded by $ m \cdot \log(T) \cdot e^2 \cdot \log(e/\delta) \cdot \delta  $ . However, note that if $\mathcal{A}_1,\dots,\mathcal{A}_m$ is a string-cover of $\{1,\dots,T\}$ then $m \ge \frac{2^T-1}{T}$ , so this idea leads to something that at best is still exponential in $T$ . Any other ideas?","['statistics', 'concentration-of-measure', 'large-deviation-theory', 'stochastic-processes', 'probability-theory']"
4499669,Computation of $ \lim_{t\to\infty}\int_t^\infty\frac{g(x)}{\sigma g(t)}f(\frac{t-x}{\sigma}+k)dx$ with $g$ and $\varphi$ density functions.,"In How can we study $\lim_{t\to\infty}\int_{\mathbb{R}}\psi_t(x)dx$, when $\lim$ and $\int$ do not commute? I was wondering about general techniques to evaluate limits of integral functions when theorems exchanging $\int$ and $\lim$ do not apply. As I learned there, thanks to the valuable effort of many incredibly nice users, in general we cannot say much. Here I propose an explicit computation for this particular case. Let $f$ be the density funnction of a standard normal and restrict to $g$ continuous, always positive, unbounded support and eventually monotonically decreasing. Let $I$ be the limit to study. @Bey smartly proved (for $\sigma=1$ , but the proof should generalize) that if: $$\rho=-\lim_{t\to\infty}\frac{g'(t)}{g(t)}=\lim_{t\to\infty} \frac{g(t)}{1-G(t)}$$ is $\infty$ (equivalently, the inverse is $0$ ) then $\mathcal{I}=0$ and if it is finite, $\mathcal{I}<C$ , where $C$ is a constant. A different take on the problem may be as follows.
Oberve that $$I=\lim_{t\rightarrow \infty}\frac{1}{g(t)}\int_{t}^{\infty}\frac{1}{\sigma }f\left( \frac{t-x }{\sigma }%
+k\right) g(\theta )dx$$ ,changing variable as $
p=G(x)$ , $dp=g(x)dx$ ,  rewrites as $$I=\lim_{t\rightarrow \infty}\frac{1}{g(t)}\int_{G(t)}^{1}%
\frac{1}{\sigma }f\left( \frac{t-G^{-1}(p)}{\sigma }+k\right)
dp$$ The limit is of the $\frac{0}{0}$ form. Applying de l'Hopital: $$I=\lim_{t\rightarrow \infty}\frac{1}{g^{\prime }(t)}\underbrace{%
\left[ \int_{G(t)}^{1}\frac{1}{\sigma }f\left( \frac{%
t-G^{-1}(p)}{\sigma }+k\right) dp\right] ^{\prime }}_{H}$$ Via Leibniz integral rule: $$H=-\left[\frac{1}{\sigma}\right]\left[g(t)f\left( k\right) +\int_{G(t)}^{1}\frac{1}{\sigma }f^{\prime }\left( \frac{t-G^{-1}(p)}{\sigma }%
+k\right) \frac{1}{g\left( G^{-1}(p)\right) }dp\right]$$ Changing variable in the right integral as $x =G^{-1}(p)$ , $dx =%
\frac{1}{g\left( G^{-1}(p)\right) }dp$ : $$H=\left[ -\frac{1}{\sigma }\right] \left[ g(t)f\left( k\right)
+\int_{t}^{\infty}\frac{1}{\sigma }f^{\prime }\left( \frac{t-x }{%
\sigma }+k\right) dx \right]$$ A primitive of the right integrand is: $-f\left( \frac{t-x }{\sigma }%
+k\right) $ , so that evaluating and rearranging terms: $$H=\lim_{s\to\infty}\left[\frac{1}{\sigma }\right] \left[f\left( \frac{t-s}{\sigma }%
+k\right)-f\left(
k\right) -g(t)f\left(
k\right) \right]=\frac{-f(k)[1+g(t)]}{\sigma}$$ as pointed out in the comments. Substituting back inside $I$ : $$I=\lim_{t\rightarrow \infty}\left[-\frac{1}{\sigma}\right]\left[\frac{f(k)(1+g(t))}{g'(t)}\right]=+\infty$$ Which is a contradiction with what found in the previous question.  There must be an error in my calculations, but I think the approach can be revealing. Any help would be most welcome!","['integration', 'measure-theory', 'real-analysis', 'calculus', 'probability']"
4499681,bound on the distance of an isometry to the identity,"Consider the space $$
\mathbb{S}^{3,1} := \{ x \in \mathbb{R}^5 : (x_1)^2 + (x_2)^2 + (x_3)^2 +(x_4)^2=1+(x_5)^2 \}
$$ with the lorentz metric on $\mathbb{R}^5$ : $$
\langle x,y\rangle _{lor} := x_1 y_1 + x_2y_2 + x_3 y_3 +x_4 y_4 - x_5 y_5
$$ We denote $SO(4,1)$ the group of matrices that preserve this lorentzian scalar product. In particular, they are isometries of $\mathbb{S}^{3,1}$ . On $\mathbb{S}^{3,1}$ , I consider the quantity (which is a kind of lorentzian distance) $$
d(x,y) = \inf \left\{ \int_0^1 |\langle \dot{\gamma}(t), \dot{\gamma}(t) \rangle_{lor}|^{1/2} dt\ \Big| \ \gamma \in C^1([0,1]; \mathbb{S}^{3,1}),\ \gamma(0)=x,\ \gamma(1)=y \right\}
$$ The question is : is the quantity $\sup_{x\in \mathbb{S}^{3,1}} d(Mx,x)$ finite ? And can we show that there exists $C>1$ such that for any $M\in SO(4,1)$ : $$
C^{-1} |M-I_5| <\sup_{x\in \mathbb{S}^{3,1}} d(Mx,x) < C|M-I_5|
$$ where $|\cdot|$ is some matrix norm, and $I_5$ is the identity matrix ? An other question is : if we have a given Riemannian manifold $(X,g)$ with an isometry $F : X\to X$ , can we estimate $\text{dist}(F(x),x)$ in terms of $F$ and any geometric quantity of $X$ , in the case where $X$ is a homogeneous space or a Lie group ?","['isometry', 'semi-riemannian-geometry', 'differential-geometry']"
4499736,"What is the distribution of $Y_n=\ln\left(\text{ }1+\big(\frac{1}{n}\sum_{i=1}^{n}X_i\big)^2\text{ }\right)$, where $\forall i: X_i \sim G({1\over2})$","Given $\big(X_i\big)_{i=1}^{\infty}$ a series of independent random variables, $X_i \sim G({1\over2})$ (Geometric distribution) for all $i \ge 1$ . For $n \ge 2$ , we mark the following: $$\bar X_n = \frac{1}{n}\sum_{i=1}^{n}X_i  \quad\quad\quad 
Y_n=\ln\big(1 + (\bar X_n)^2 \big)  \quad\quad\quad
T_n = (Y_n - \ln5)^2
$$ And we need to find the value $C_n$ and the distribution of $T$ , which satisfies: $$
\frac{1}{C_n} \sum_{n=1}^{12} T_n  \overset{d}{\to} T
$$ $$$$ Well, i'll save you the trouble. The final and correct answer is $$C_n=\frac{32}{25n}, \quad T_n \sim \chi^2_{(12)} \text{  (chi distribution with 12 degrees of freedom)}$$ But i can't understand why. This is all I know so far: Let $\big(Z_i\big)_{i=1}^{n}$ be a series of independent random variable with the same distribution $Z_i \sim N(\mu, \sigma^2)$ , then we know that $\frac{Z_i - \mu}{\sigma} \sim N(0,1)$ for all $i$ , and we know that the distribution of the following sum is: $\sum_{i=1}^n  \big( \frac{Z_i-\mu}{\sigma} \big)^2 \sim \chi^2_{(n)}$ . My guess is that in my question: $Z_i := Y_n = \ln\big(1 + (\bar X_n)^2 \big)$ $\mu := E(Y_n)  = \ln5$ $\sigma^2 := V(Z_i) = C_n$ and thus: $$\sum_{i=1}^n  \big( \frac{Z_i-\mu}{\sigma} \big)^2  =  
\frac{\sum_{i=1}^n  ( Z_i-\mu )^2}{{\sigma}^2} :=
\frac{\sum_{n=1}^{12} T_n}{C_n}   \overset{d}{\to} T
$$ But i can't understand how is that $E(Y_n)=\ln5$ , or even what is the distribution of $Y_n$ ? Am I on the right track? What are your thoughts about this question? How would you solve it?","['statistics', 'probability-distributions', 'probability']"
4499819,About extreme values of $\{f(x)-x\}^2$ when $f(x)$ is a cubic function.,"$t \ge 6$ , $t \in \mathbb{R}$ $f(x) = \frac{1}{t}\left( \frac{1}{8}x^3 + \frac{t^2}{8}x+2\right)$ $\{f(x)-x\}^2$ has an extreme value on $x = k$ Sum of such $k = g(t)$ $g(p) = -1$ for some $p \in \mathbb{R}$ $$\int_{6}^{p} g'(t)(8t-t^2)dt = \,?$$ My approach: I. For condition 3: Let $h(x) = \{f(x)-x\}^2$ . Since $h(x)$ has an extreme value on $x=k$ , $h'(k) = 0$ and $h''(k) \ne 0$ . For $h'(k) = 0$ , $h'(k) = 2\{f(k)-k\}(f'(k)-1) =0$ , so $f(k)=k$ or $f'(k)=1$ . For $h''(k) \ne 0$ , $h''(k)=2\left[\{(f'(k)-1)^2 + (f(k)-k)f''(k)\}\right] \ne 0$ Combining both 3 and 4, If $f(k) = k$ , $f'(k) \ne 1$ . If $f'(k) = 1$ , $f''(k)\ne0 \rightarrow k \ne 0$ and $f(k) \ne k$ . II. Applying $f(x)$ : Equation $f(k)=k$ becomes $8t-t^2=k^2 + \frac{16}{k}$ . Equation $f'(k)=1$ becomes $8t-t^2=3k^2$ . So, step I-5 becomes like: If $8t-t^2=k^2 + \frac{16}{k}$ , $8t-t^2\ne3k^2$ . If $8t-t^2=3k^2$ , $8t-t^2\ne k^2 + \frac{16}{k}$ and $k \ne 0$ . We can draw graphs of $y=3k^2$ , $y=k^2 + \frac{16}{k}$ , and $y = 8t-t^2$ which will be a constant graph. And we observe points which satisfies step 3. Constant graph( $y=8t-t^2$ ) has a value of less or equal than $12$ since $t \ge 6$ from condition. But the intersect $(2, 12)$ (where $t=6$ ) is the only point where it does not meet condition of step 3, since it is on both functions. So, we can say that $g(6) =(-4) + (-2) = -6$ . Below that, all intersects are correct points, so the sum $g(t)$ becomes the $x$ -coordinate which satisfies $x^2 + \frac{16}{x} = 8t-t^2$ , since sum of $x$ -coordinates on $y=3x^2$ becomes $0$ due to symmetry. So we can say that $\left(g(t) \right)^2 + \frac{16}{g(t)} = 8t - t^2$ . III. Getting the answer. $$\int_{6}^{p} g'(t)(8t-t^2)dt = \int_{6}^{p} g'(t)\left(\left(g(t) \right)^2 + \frac{16}{g(t)}\right)dt  $$ Let $g(t) = s$ , $$\int_{g(6)}^{g(p)} \left(s^2 + \frac{16}{s}\right)ds$$ Since $g(p) = -1$ from condition and $g(6) = -6$ from what we've got, $$\int_{-6}^{-1} \left(s^2 + \frac{16}{s}\right)ds = \left[ \frac{1}{3}s^3 + 16\ln{\left| s\right|} \right]_{-6}^{-1} = \frac{215}{8}-\ln{6}$$ And it was wrong. The correct answer was $21 - 32\ln2$ , and it seems like the value $g(6)$ was slightly off. I tried to explain my process as specific as possible, because I want to know if I am making a inefficient approach or I use too vague logic. And, the reason of my wrong answer. Any comment would be so helpful to me right now. +edit: By If a function $f(x)$ has an extreme value on $x=k$, $f''(k) \ne 0$? , step I-2 is a completely wrong logic. But $h(x)$ is a polynomial function, so checking $h'(x)=0$ is still a valid approach(not that it approves it is an extrema). By manually checking if the point $k=2$ is also an extrema, $k^2 + \frac{16}{k}$ is a $(+)$ , and $3k^2$ goes from $(-)$ to $(+)$ , it becomes an extrema. Thus, $g(6)=-4$ , and we can get the correct answer.","['integration', 'calculus', 'derivatives', 'cubics']"
4499840,Edges in convex point sets that split the set into odd or even sets of points,"I am trying to prove or disprove whether there exists a convex set of $2n$ points (denote this by $\mathcal{P}$ ) such that the following is true: we split $\mathcal{P}$ into two sets of equal cardinality, let these sets be $P_1$ and $P_2$ . Now, we want $P_1$ and $P_2$ chosen in a way such that if we draw an edge from any point $p_1\in P_1$ to any $p_2\in P_2$ , then this edge splits the convex point set $\mathcal{P}$ into two odd components. This property should hold for any pair $\{p_1,p_2\}$ . It seems like there is no way to choose $P_1$ and $P_2$ from $\mathcal{P}$ such that the property above holds. However, I am struggling to prove that this is actually the case. My idea would be to just without loss of generality fix some $p_1\in P_1\subseteq \mathcal{P}$ , and make sure that we place all the points in $P_2$ in a manner where drawing an edge from $p_1$ to any $p_2\in P_2$ splits $\mathcal{P}$ into two odd sets of points. However, in order to have $|P_1|=|P_2|$ , it seems like we are forced to place one of the points in $P_2$ in a manner where we would be able to draw an edge between the fixed some point in $P_1$ and some point in $P_2$ such that this edge splits $\mathcal{P}$ into two even components, which would be a contradiction. This is quite vague, and I am not sure if this reasoning makes sense / is correct.","['combinatorics', 'geometry', 'discrete-mathematics']"
4499876,"What is the equation of the line that ""creates an intercept of $-8$ from the $x$-axis and makes an angle of $45^\circ$""?","This question came in the Chittagong University admission exam 11-12 What is the equation of the line that creates an intercept of $-8$ from the $x$ -axis and makes an angle of $45^\circ$ ? (a) $x+y+8=0$ (b) $3x+8y=1$ (c) $x-y=8$ (d) $y=8$ (e) $x=8$ My attempt: I interpreted creates ""an intercept of -8 from the x-axis and makes an angle of 45"" meaning that the x-intercept is -8 and the slope is $\tan(45^{\circ})$ .  According to this interpretation, the equation should be $x-y=-8$ . None of the options have this equation. The closest one is (c), and the third-party question bank says that the correct answer is (c). Isn't (c) wrong?","['algebra-precalculus', 'solution-verification']"
4499944,Equate $e^{-i|Ψ⟩⟨Ψ|Δt}e^{-i|x⟩⟨x|Δt}$ to $(\cos^2Δt/2-\sin^2Δt/2ψ.z)I\\-2i\sinΔt/2[\cosΔt/2\frac{ψ+z}{2}+\sinΔt/2\frac{ψ\times z}{2}].\vec{σ}$,"In my reference, Page 259, Quantum Computation and Quantum Information by Nielsen and Chuang it is given that Given the unitary operator $U(\Delta t)\equiv \exp(-i|\psi\rangle\langle\psi|\Delta t)\exp(-i|x\rangle\langle x|\Delta t)$ where $|x\rangle\langle x|=\dfrac{I+Z}{2}=\dfrac{I+\hat{z}.\vec{\sigma}}{2}$ where $\hat{z}=(0,0,1)$ , and $|\psi\rangle\langle\psi|=\dfrac{I+\vec{\psi.\vec{\sigma}}}{2}$ where $\vec{\psi}=(2\alpha\beta,0,\alpha^2-\beta^2)$ Here $X,Y,Z$ are Pauli matrices with $\vec{\sigma}=(X,Y,Z)$ and $\alpha,\beta$ are real numbers, and we can also prove that $\exp(-i|\psi\rangle\langle\psi|\Delta t)=e^{-i\Delta t}|\psi\rangle\langle\psi|$ and $\exp(-i|x\rangle\langle x|\Delta t)=e^{-i\Delta t}|x\rangle\langle x|$ , please refer to Hamiltonian Simulation Circuit for Grover's Search . How do we obtain the expression $$U(\Delta t)=\Big(\cos^2(\Delta t/2)-\sin^2(\Delta t/2)\vec{\psi}.\hat{z}\Big)I\\-2i\sin(\Delta t/2)\bigg[\cos(\Delta t/2)\dfrac{\vec{\psi}+\hat{z}}{2}+\sin(\Delta t/2)\dfrac{\vec{\psi}\times\hat{z}}{2}\bigg].\vec{\sigma}$$ up to an unimportant global phase factor ? Note: This is the Final Expression for the Unitary Operator for Grover's Search Hamiltonian Simulation My Attempt $$
U(\Delta t)=\exp(-i|\psi\rangle\langle\psi|\Delta t).\exp(-i|x\rangle\langle x|\Delta t)=e^{-i\Delta t}|\psi\rangle\langle\psi|.e^{-i\Delta t}|x\rangle\langle x|\\
=e^{-i\Delta t}.e^{-i\Delta t}(\dfrac{I+\vec{\psi}.\vec{\sigma}}{2})(\dfrac{I+\hat{z}.\vec{\sigma}}{2})=\frac{e^{-i\Delta t}}{2}\big(\cos(\Delta t)-i\sin(\Delta t)\big).\frac{1}{2}.\Big(I+(\vec{\psi}+\hat{z}).\vec{\sigma}+(\vec{\psi}.\vec{\sigma})(\hat{z}.\vec{\sigma})\Big)\\
$$ Making use of the identity $(\vec{a}.\vec{\sigma})(\vec{b}.\vec{\sigma})=(\vec{a}.\vec{b})I+i(\vec{a}\times\vec{b}).\vec{\sigma}$ $$
=\frac{e^{-i\Delta t}}{2}\big(\cos(\Delta t)-i\sin(\Delta t)\big).\frac{1}{2}.\Big(I+(\vec{\psi}+\hat{z}).\vec{\sigma}+(\vec{\psi}.\hat{z})I+i(\vec{\psi}\times\hat{z}).\vec{\sigma}\Big)\\
=\frac{e^{-i\Delta t}}{4}.\bigg[\color{red}{\cos(\Delta t)I-i\sin(\Delta t)(\vec{\psi}+\hat{z}).\vec{\sigma}+\cos(\Delta t)(\vec{\psi}.\hat{z})I+\cos(\Delta t)i(\vec{\psi}\times\hat{z}).\vec{\sigma}}-i\sin(\Delta t)I\\+\cos(\Delta t)(\vec{\psi}+\hat{z}).\vec{\sigma}+-i\sin(\Delta t)(\vec{\psi}.\hat{z})I-i\sin(\Delta t).i(\vec{\psi}\times\hat{z}).\vec{\sigma}\bigg]\\
=\frac{e^{-i\Delta t}}{4}.\bigg[\color{red}{(2\cos^2(\Delta t/2)-1)I-i2\sin(\Delta t/2)\cos(\Delta t/2)(\vec{\psi}+\hat{z}).\vec{\sigma}\\+(1-2\sin^2(\Delta t/2))(\vec{\psi}.\hat{z})I+i(1-2\sin^2(\Delta t/2))(\vec{\psi}\times\hat{z}).\vec{\sigma}}-i\sin(\Delta t)I+\cos(\Delta t)(\vec{\psi}+\hat{z}).\vec{\sigma}\\-i\sin(\Delta t)(\vec{\psi}.\hat{z})I+\sin(\Delta t)(\vec{\psi}\times\hat{z}).\vec{\sigma}\bigg]\\
$$ $$
=\frac{e^{-i\Delta t}}{4}.\bigg[\color{red}{2\cos^2(\Delta t/2)I}-I\color{red}{-2i\sin(\Delta t/2)\cos(\Delta t/2)(\vec{\psi}+\hat{z}).\vec{\sigma}}\\+\vec{\psi}.\hat{z}I\color{red}{-2\sin^2(\Delta t/2)\vec{\psi}.\hat{z}}I\color{red}{-2i\sin^2(\Delta t/2)(\vec{\psi}\times\hat{z}).\vec{\sigma}}+(\vec{\psi}\times\hat{z}).\vec{\sigma}-i\sin(\Delta t)I+\cos(\Delta t)(\vec{\psi}+\hat{z}).\vec{\sigma}\\-i\sin(\Delta t)(\vec{\psi}.\hat{z})I+\sin(\Delta t)(\vec{\psi}\times\hat{z}).\vec{\sigma}\bigg]\\
=\frac{e^{-i\Delta t}}{4}.\bigg[\color{red}{2\cos^2(\Delta t/2)I}\color{red}{-2i\sin(\Delta t/2)\cos(\Delta t/2)(\vec{\psi}+\hat{z}).\vec{\sigma}}\\\color{red}{-2\sin^2(\Delta t/2)\vec{\psi}.\hat{z}I}\color{red}{-2i\sin^2(\Delta t/2)(\vec{\psi}\times\hat{z}).\vec{\sigma}}-I+\vec{\psi}.\hat{z}I+(\vec{\psi}\times\hat{z}).\vec{\sigma}-i\sin(\Delta t)I+\cos(\Delta t)(\vec{\psi}+\hat{z}).\vec{\sigma}\\-i\sin(\Delta t)(\vec{\psi}.\hat{z})I+\sin(\Delta t)(\vec{\psi}\times\hat{z}).\vec{\sigma}\bigg]\\
$$ The red coloured terms constitute that of the required expression, but how do I deal with the rest of the terms ?","['unitary-matrices', 'trigonometry', 'linear-algebra', 'group-theory', 'quantum-computation']"
4499960,Rotating spherical shells doesn't change volume,"Let $\gamma\colon \mathbb R\to SO(n)$ be a path (but not necessarily a one-parameter group). It leads to a diffeomorphism $f\colon \mathbb R^n\to \mathbb R^n$ $$f(x) = \gamma\left(\lVert x \rVert^2\right)x,$$ which rotates around the origin every spherical shell, according to a rotation specified by the distance from the origin. I wonder if $f$ is volume-preserving, i.e., at every point $\vert\det f'(x)\vert = 1$ . This is what I know: Proof that $f$ is a diffeomorphism is easy, as its inverse is just $x\mapsto \gamma(\lVert x \rVert ^2)^{-1}x$ . For $n=2$ this is easy, but tedious – $SO(2)\simeq S^1$ and I was able to write a general $\gamma$ as $t\mapsto \exp(i u(t))$ for some function $u\colon \mathbb R\to \mathbb R$ and verify the required identity manually. I'm speculating now, but the Borel algebra on $\mathbb R^n$ has a basis being (a high-dimensional analogue of) a steradian spanned between radii $r_1$ and $r_2$ . I think it may be possible to prove using Fubini's theorem something like the Cavalieri's principle – the area of each ""steradian"" doesn't change (as we are rigidly rotating it by an element of $SO(n)$ ) so that the total volume doesn't change either. (And pass from this basis to arbitrary Borel sets). I however wasn't able to fill in the details and turn this intuition into a formal proof. In general, $f'(x) \neq \gamma(\lVert x \rVert^2)$ , but I believe that $\det f'(x) = 1 = \det \gamma(\lVert x \rVert^2)$ .","['measure-theory', 'lie-groups', 'differential-geometry']"
4499961,Second derivative of a matrix function,"I am trying to find the $\textit{second}$ derivative of $X \mapsto (F \circ G) (X)$ , where $G(X)=\frac{AXA}{\operatorname{tr}(AXA)}$ and $F(X)=\operatorname{tr}\left((CXC)^\frac{1}{2}\right)$ are functions of the positive matrix X. The matrices A and C are constant and also positive. I computed the first derivative and got $\frac{\partial (F \circ G)}{\partial X}=\frac{1}{2}\operatorname{tr}\left( AXA\right)^{-1}\left(AC(CG(X)C)^{-\frac{1}{2}}CA -\operatorname{tr} \left((CG(X)C)^\frac{1}{2} \right)A^2\right)$ notably thanks to: Derivative of $\mbox{Tr} \left(\left(AXA\right)^{\frac{1}{2}}\right)$ with respect to $X$ Derivative of $X \mapsto \frac{AXA}{\operatorname{Tr} \left( AXA \right )}$ However, to derive again, I am stuck with the inverse square root $(CG(X)C)^{-\frac{1}{2}}$ . Would you know how to deal with it?","['matrices', 'matrix-calculus', 'derivatives']"
4499969,Confusion with the Fourier Transform and Complex Differentiability: example with compact-supported function,"I have a misconception when applying the Fourier Transform to a compacted-supported function and the characteristics of the function obtained. Intro I am going to list what I believe is true so you can identify were I am making my conceptual mistake: The Fourier Transform of a square-integrable function which is compact-supported in the real line must be an entire analytic function due the Paley–Wiener theorem . An entire function is a complex-valued function that is holomorphic on the whole complex plane, so it is complex differentiable everywhere, so it is satisfying being Analytic , infinitely differentiable or Smooth , and its real-imaginary decomposition constituents fulfill the Cauchy–Riemann equations . If a function $g(z)$ is complex differentiable with the complex variable being described as $z=\sigma+iw$ so the function could be expressed as: $g(\sigma+iw)=u(\sigma,w)+iv(\sigma,w)$ with $u,\,v$ real-valued functions, then each constituents fulfill the Cauchy–Riemann equations as mentioned, which imply that each constituent function is individually an Harmonic function so both functions $u,\,v$ fulfill $\nabla^2 u = \frac{\partial^2 u}{\partial \sigma^2}+\frac{\partial^2 u}{\partial w^2}=0$ and $\nabla^2 v = \frac{\partial^2 v}{\partial \sigma^2}+\frac{\partial^2 v}{\partial w^2}=0$ . The Fourier Transform $\hat{f}(iw)$ of an ""even function"" $f(t)=f(-t)$ is a real-valued function $\hat{f}(iw)\in \mathbb{R}$ . The Fourier Transform $\hat{f}(iw)$ of an real valued function $f(t)$ fulfill that $\hat{f}(iw)^* = \hat{f}(-iw)$ is an Hermitian function . I am using the electrician notation for the Fourier transform using the angular frequency including the imaginary unit as part of the variable. Main text With this, I am going to use the following example to present the problem: $$f(t) = \left(\frac{1-t^2+|1-t^2|}{2}\right)^4 \tag{Eq. 1}\label{Eq. 1}$$ which is ploted here fulfilling is a real-valued function supported on the real line, but different from zero only in $t \in [-1,\,1]$ so it has compact-support. Also it can be seen is an ""even function"" due $(-t)^2 \equiv t^2$ for real-valued $t$ , and it is also square-integrable since $\int\limits_{-\infty}^{\infty}\left|\left(\frac{1-t^2+|1-t^2|}{2}\right)^4\right|^2 dt = \frac{65536}{109395}\approx 0.6 \ll \infty$ . So the function should be fulfilling all the $5$ points of the introduction. As it can be seen here , the Fourier Transform of $f(t)$ of \eqref{Eq. 1} could been described using the Bessel function of First kind $J_{\nu}(w)$ and the Gamma function $\Gamma(w)$ as: $$ \begin{array}{r c l}
\hat{f}(iw) = \int\limits_{-\infty}^{\infty} f(t)\ e^{-iwt}dt & = & \text{sgn}(w)\sqrt{\pi}\left(\frac{2}{w}\right)^{4+\frac{1}{2}}\Gamma(4+1)J_{4+\frac{1}{2}}(w)\\
& = & 24\sqrt{\pi}\,\text{sgn}(w)\left(\frac{2}{w}\right)^{\frac{9}{2}}J_{\frac{9}{2}}(w) \tag{Eq. 2}\label{Eq. 2}
\end{array}$$ Since this Fourier Transform $\hat{f}(iw)$ should be holomorphic, but is a real-valued function, I am confused how it is verified is being complex differentiable: How is analyzed a real-valued function for complex differentiability? Since the Fourier Transform $\hat{f}(iw) = \hat{f}(\{\sigma\equiv 0\}+iw)$ it will imply that $u(\sigma,w) = 24\sqrt{\pi}\,\text{sgn}(w)\left(\frac{2}{w}\right)^{\frac{9}{2}}J_{\frac{9}{2}}(w)$ and $v(\sigma,w)=0$ : naturally the trivial solution zero function will fulfill $\nabla^2 v = 0$ , but for reviewing $u(\sigma,w)$ I don´t know how to take the derivative respect to $\sigma$ since it is zero , neither I have a function related to $\sigma$ to make some manipulation. Even so, if I take $\frac{\partial^2}{\partial w^2}u(\sigma,w) \neq 0$ is definitely not zero as it can be seen here , so if the Fourier Transform is complex differentiable as is stated on the initial points, surely must be $\sigma$ -related component that is missing. Surely I am having a conceptual mistake, but I cannot figure it out so far. Hope you could explain what I am doing wrong, displaying which functions are going to be $u(\sigma,w)$ and $v(\sigma,w)$ for the Fourier Transform of \eqref{Eq. 2} showing in detail how is proved is complex differentiable. Beforehand, thanks you very much.","['fourier-analysis', 'fourier-transform', 'complex-analysis', 'finite-duration', 'derivatives']"
4499993,trigonometry question asked in the STEP exam in 2021,"Prove, from the identities for cos(A ± B), that cos a cos 3a ≡ 1/2 (\cos 4a +\cos 2a). Find a similar identity for sin a cos 3a $$\cos(a+3a) =\cos(a)\cos(3a) -\sin(a)\sin(3a)$$ then maybe $$\cos(4a) + \sin(a)\sin(3a)  -> \cos(4a) + \sin(x)[\sin(a)\cos(2a)+\sin(2a)\cos(a)] $$ again on the sin(2a) $$\cos(4a) +\sin(x)[\sin(a)\cos(2a) + (\sin(a)\cos(a)+\sin(a)\cos(a))\cos(a)] $$ $$\cos(4a) + \sin(x)[\sin(a)\cos(2a) + \cos(a)(\sin(a)\cos(a)+\sin(a)\cos(a))]$$ $$\cos(4a) + \sin(x)[\sin(a)\cos(2a) + \cos(a)(2\sin(a)\cos(a))]$$ $$\cos(4a) + \sin^2(x)[\cos(2a) + 2\cos^2(a))]$$ $$\cos(4a) + [\cos^2-1][\cos(2a) + 2\cos^2(a))]$$ $$\cos(4a) + [\cos^2-1][\cos(2a) + 2\cos^2(a))]$$ It's the first time I'm doing something like this and I think I am doing rubbish, could someone please give me hints on how to do it?","['proof-explanation', 'trigonometry']"
4500045,Are $\Bbb{R\times R}$ and $\Bbb{R}$ equinumerous?,"Prove that the sets $\Bbb{R\times R}$ and $\Bbb{R}$ are equinumerous, without $\mathsf{AC}$ . (The solutions in this , this and this posts are a bit more complicated, so I'm trying a different approach). My attempt: As $\Bbb{R}$ and $(0,1)$ are equinumerous, and as there is an obvious injection $\Bbb{R} \to \Bbb{R\times R}$ , by using using Cantor–Schröder–Bernstein theorem, it is sufficient to find an injection $(0,1)\times (0,1)\to(0,1)$ . For an ordered pair $\langle r,s\rangle$ of reals between $0$ and $1$ , let $$
r=0.r_1 r_2 \ldots r_n\ldots
\\
s=0.s_1 s_2 \ldots s_n\ldots
$$ be some decimal expansion of them. Send $\langle r,s\rangle$ to the number defined by $$
0.r_1 s_1 0 r_2 s_2 0 \ldots r_n s_n 0\ldots
$$ This number must have a unique decimal expansion, as it cannot have any infinite series of nines in it. As we can send each real of form $0.a_1 a_2 0 a_3 a_4 0\ldots$ to the two reals $0.a_1 a_3 a_5 \ldots$ and $0.a_2 a_4 a_6 \ldots$ , the described function must be one-to-one. Is this solution OK? Edit: There's a problem with the fact that $r$ and $s$ in general might not have a unique decimal expansion. We can overcome this problem by excluding from $(0,1)$ the set of numbers that have a decimal expansion with almost all digits being nines. That set is obviously countable.","['elementary-set-theory', 'axiom-of-choice', 'solution-verification']"
4500073,$\int_0^1 \frac{\arcsin x\arccos x}{x}dx$,"Someone on Youtube posted a video solving this integral.
I can't find on Math.stack.exchange this integral using search engine https://approach0.xyz It is related to $\displaystyle \int_0^\infty \frac{\arctan x\ln x}{1+x^2}dx$ Following is a solution that is not requiring the use of series: \begin{align}J&=\int_0^1 \frac{\arcsin x\arccos x}{x}dx\\
&\overset{\text{IBP}}=\underbrace{\Big[\arcsin x\arccos x\ln x\Big]_0^1}_{=0}-\underbrace{\int_0^1 \frac{\arccos x\ln x}{\sqrt{1-x^2}}dx}_{x=\cos t }+\underbrace{\int_0^1 \frac{\arcsin x\ln x}{\sqrt{1-x^2}}dx}_{x=\sin t}\\
&=\int_0^{\frac{\pi}{2}} t\ln(\tan t)dt\\
&\overset{u=\tan t}=\int_0^\infty \frac{\arctan u\ln u}{1+u^2}du\\
&\overset{\text{IBP}}=\underbrace{\left[\arctan u\left(\int_0^u \frac{\ln t}{1+t^2}dt\right)\right]_0^\infty}_{=0}-\int_0^\infty \frac{1}{1+u^2}\left(\underbrace{\int_0^u \frac{\ln t}{1+t^2}dt}_{y(t)=\frac{t}{u}}\right)du\\
&=-\int_0^\infty \left(\int_0^1 \frac{u\ln(uy)}{(1+u^2)(1+u^2y^2)}dy\right)du\\
&=-\int_0^\infty \left(\int_0^1 \frac{u\ln u}{(1+u^2)(1+u^2y^2)}dy\right)du-\int_0^1 \left(\int_0^\infty \frac{u\ln y}{(1+u^2)(1+u^2y^2)}du\right)dy\\
&=-\int_0^\infty \left[\frac{\arctan(uy)}{1+u^2}\right]_{y=0}^{y=1}\ln udu-\frac{1}{2}\int_0^1 \left[\frac{\ln\left(\frac{1+u^2}{1+u^2y^2}\right)}{1-y^2}\right]_{u=0}^{u=\infty}\ln ydy\\
&=-J+\int_0^1 \frac{\ln^2 y}{1-y^2}dy\\
&=\frac{1}{2}\int_0^1 \frac{\ln^2 y}{1-y}dy-\frac{1}{2}\underbrace{\int_0^1 \frac{y\ln^2 y}{1-y^2}dy}_{z=y^2}\\
&=\frac{7}{16}\int_0^1 \frac{\ln^2 y}{1-y}dy\\
&=\frac{7}{16}\times 2\zeta(3)=\boxed{\frac{7}{8}\zeta(3)}
\end{align} NB:I assume $\displaystyle \int_0^1 \frac{\ln^2 y}{1-y}dy=2\zeta(3)$ Feel free to post your solution.","['integration', 'definite-integrals']"
4500110,An Almost Everywhere Measurable Function is Not Almost Everywhere Equal to a Measurable Function,"Here I give the definition of an almost everywhere measurable function: Given $(X, \mathcal{F}, \mu)$ a measurable space. A function $f: X \to \mathbf{R}$ is $\mu$ -almost everywhere measurable if there exists $N \in \mathcal{F}$ with $\mu(N) = 0$ such that for all $\alpha \in \mathbf{R}$ that $f^{-1}((\alpha, +\infty)) \cap (X - N) \in \mathcal{F}$ . I would think that if a function $f$ is almost everywhere measurable, then $f$ must be equal to a measurable function $g$ almost everywhere. However, I am starting to think this might not be the case. My attempt is to note $f = f\chi_{X - N}$ almost everywhere and show $f\chi_{X - N}$ is measurable by definition: We let $\alpha \in \mathbf{R}$ . Then $f\chi_{X - N}^{-1}((\alpha, +\infty)) = \begin{cases}
f^{-1}(\alpha, +\infty) \cap (X - N) &, \mbox{ if } \alpha \geq 0; \\
f^{-1}(\alpha, + \infty) &, \mbox{ otherwise}.
\end{cases}$ However, it is not necessarily true that $f^{-1}(\alpha, +\infty) \in \mathcal{F}$ as $f$ is not necessarily measurable. I couldn't figure out how to fix this. Is the claim that I am trying to show here false entirely? Update: I just realized that in the case when $\alpha < 0$ , we really have $(f^{-1}(\alpha, +\infty) \cap (X - N)) \cup N$ , which would be measurable. Is this correct?","['measure-theory', 'real-analysis', 'measurable-sets', 'solution-verification', 'measurable-functions']"
4500195,"Let $\alpha$ be an increasing function on $[a,b]$. Show that $\int^a_b\alpha d \alpha = \frac{1}{2}[\alpha (b)^2 - \alpha(a)^2]$","I am wanting to try to prove the question below, but there is a step that I can't get pass. I know that the proof is worthless if I assume incorrectly, and should have stopped proving from there, but I feel that I am close and possibly just missing a theorem or something that might be able to salvage the proof. But if ther is no way then I will just try a different attempt altogether. Book: here page: 166 I would really appreciate any help\insight you can offer. Question Let $\alpha$ be an increasing function on $[a,b]$ and suppose $\alpha \in R(\alpha )$ on $[a,b]$ . Show that $\int^a_b\alpha d \alpha = \frac{1}{2}[\alpha (b)^2 - \alpha(a)^2]$ Note: $\alpha \in R(\alpha )$ this is showing that $\alpha$ is Riemann-integrable My attempt Let $P$ be a partition on $[a,b]$ Let as $\alpha$ is increasing, thus $\alpha(x) \leq \alpha(y)$ where $x<y$ for $x,y\in[a,b]$ Let $M_k = sup\{\alpha(x) | x_{k-1} \leq x \leq x_k\} = \alpha(x_k)$ Let $m_k = inf\{\alpha(x) | x_{k-1} \leq x \leq x_k\} = \alpha(x_{k-1})$ let $\Delta\alpha_k =  \alpha(x_k) - \alpha(x_{k-1})$ Now the upper Stieltjies integral: $U(P,\alpha,\alpha) = \sum\limits_{k=1}^n M_k\Delta\alpha_k = \sum\limits_{k=1}^n\alpha(x_k)\Delta\alpha_k$ and the lower Stieltjies integral: $L(P,\alpha,\alpha) = \sum\limits_{k=1}^n m_k\Delta\alpha_k = \sum\limits_{k=1}^n\alpha(x_{k-1})\Delta\alpha_k$ As $\alpha$ is Riemann-integrable thus the upper Stieltjies integral $=$ lower Stieltjies integral, thus $\inf\{U(P,\alpha,\alpha)|$ where is P is a partition on $[a,b]\}$ $ = \sup\{L(P,\alpha,\alpha)|$ where is P is a partition on $[a,b]\}$ $ = \int^a_b\alpha d \alpha$ Let $\int^a_b\alpha d \alpha = \frac{1}{2}[U(P,\alpha,\alpha) + L(P,\alpha,\alpha)]$ <<< this is the problem step $= \frac{1}{2}[\sum\limits_{k=1}^n\alpha(x_k)\Delta\alpha_k + \sum\limits_{k=1}^n\alpha(x_{k-1})\Delta\alpha_k]$ $= \frac{1}{2}\sum\limits_{k=1}^n[\alpha(x_k)+\alpha(x_{k-1})]\Delta\alpha_k$ $= \frac{1}{2}[(\alpha(x_1) + \alpha(x_0))(\alpha(x_1)- \alpha(x_0))+ (\alpha(x_2) + \alpha(x_1))(\alpha(x_2)- \alpha(x_1))+\cdots]$ $= \frac{1}{2}[\alpha(x_1)^2 - \alpha(x_0)^2+ \alpha(x_2)^2 - \alpha(x_1)^2+\cdots]$ $= \frac{1}{2}[\alpha(x_{last})^2 - \alpha(x_0)^2] = \frac{1}{2}[\alpha(b)^2 - \alpha(a)^2]$ the problem this $\int^a_b\alpha d \alpha = \frac{1}{2}[U(P,\alpha,\alpha) + L(P,\alpha,\alpha)]$ should be $\int^a_b\alpha d \alpha = \frac{1}{2}[\inf\{U(P,\alpha,\alpha)|$ for $P$ on $[a,b]\} + \sup\{L(P,\alpha,\alpha)| $ for $P$ on $[a,b]\}]$ but I can't get rid on the $\inf$ and $\sup$ . Is there away to do this?","['stieltjes-integral', 'solution-verification', 'riemann-integration', 'real-analysis']"
4500224,Support of the invariant measures of the logistic family,"Let $a\in (0,4]$ be the logistic family $Q_a:[0,1] \to [0,1]$ , $Q_a(x) = ax(1-x).$ In the book ""One-Dimensional Dynamics - W. de Melo, S. van Strien"", it is stated that the set $$\mathcal C =\{a\in (0,4]; \mbox{ $Q_a$ admits an invariant measure $\mu_a$, such that 
 $\mu_a(\mathrm dx) \ll \mathrm{Lebesgue}(\mathrm dx)$}\},$$ has $4$ as an accumulation point (see the result at the end of this question). I searched online if there is any information about the support of the measures $\{\mu_a\}_{a\in \mathcal C}.$ I believe that there exists a subsequence $\{a_n\}_{n\in\mathbb N} \subset \mathcal C,$ such that $\lim_{n\to \infty} a_n= 4$ and supp $(\mu_{a_n}) \to [0,1]$ (in the Hausforff metric ), however I could neither find a proof from this fact nor prove it by myself. Does anyone have any suggestions for attacking this problem or a reference? Corollary (Jakobson). Let $Q_a : [0,1] \to [0,1]$ , $a \in (0,4]$ , be the quadratic family $Q_a(x) = ax(1-x)$ . There exists a subset $\mathcal{C} \subset (0,4]$ of positive Lebesgue measure with the following properties: If $a \in \mathcal{C}$ then $Q_a$ has an absolutely continuous invariant probability measure with positive entropy. The parameter value $a=4$ is a Lebesgue density point of $\mathcal{C}$ , namely, $$\lim_{\epsilon \to 0} \frac{\lambda(\mathcal{C} \cap [4-\epsilon,4])}{\epsilon} = 1.$$","['measure-theory', 'ergodic-theory', 'dynamical-systems']"
4500225,"MLE of cdf, consistency and asymptotic confidence interval","Let $\{X_i\}_{i=1}^{n}$ be i.i.d. random variables with distribution $N(\mu, 1)$ . Let $p = \mathbb{P}[X_i > 0]$ . Find the MLE for $p$ and compute the 95% asymptotic confidence interval. My attempt: I know that $\bar{X}$ is the MLE for $\mu$ in normal distribution, and $$p = \mathbb{P}[X_i > 0] = \mathbb{P}[Z > -\mu] = 1 - \Phi_Z\left(-\mu\right) = 1 - \left[1-\Phi_Z(\mu) \right] = \Phi_Z\left(\mu\right)$$ then, the MLE for $p$ is $\hat{p} = \Phi_Z(\bar{X}).$ The part where you have to calculate the asymptotic confidence interval has me a bit confused since I am working with the estimator of a probability, but I did the following procedure $$\sqrt{n}(\hat{p}-p) \sim N(0, I(p)^{-1})$$ Then, by calculating Fisher's information, I obtained the following: $$I(p) = -\mathbb{E}\left[\dfrac{\partial^2}{\partial p^2}\ \log f(X;p)\ \bigg\vert \ p \right] = -\mathbb{E}\left[\dfrac{\partial^2}{\partial p^2}\ \left( \log\left(\dfrac{1}{\sqrt{2\pi}}\right) - \dfrac{(X-p)^2}{2} \right)\ \bigg\vert \ p \right] = -\mathbb{E}[-1] = 1. $$ Then, the 95% asymptotic confidence interval would be as follows: $$\hat{p} - \dfrac{1.96}{\sqrt{n}}\ \leq \ p \ \leq \ \hat{p} + \dfrac{1.96}{\sqrt{n}}$$ My doubts are as follows: Is my procedure for finding the asymptotic confidence interval correct? I know that $\bar{X}$ is consistent for $\mu$ in a Normal random sample, and $\Phi_Z(\cdot)$ is a real-valued function continuous in $\mathbb{R}$ , so, $\hat{p} = \Phi_Z(\bar{X})$ is consistent. Using this information, is there any other way I can compute the asymptotic confidence interval? I have a doubt about consistency: assuming that the sample of random variables is not normal, is it correct to assume that the estimator would still be consistent? My intuition says yes, however, I get confused when I start demonstrating convergence in probability because apparently I am calculating the probability of a probability $\left(i.e.\ \mathbb{P}[\vert \hat{p} - p\vert > \epsilon] \right) $ . Can you tell me if my intuition is correct that the estimator would be consistent regardless of the sample distribution, and if possible, give me a hint so I can find the necessary convergence?","['statistical-inference', 'statistics', 'parameter-estimation', 'normal-distribution', 'maximum-likelihood']"
4500236,Evaluate the integral $I=\int_0^1 \sqrt{-1+\sqrt{\frac{4}{x}-3}}\ dx$,"To evaluate the integral $$I=\int_0^1 \sqrt{-1+\sqrt{\frac{4}{x}-3}}\ dx$$ I define $t=-1+\sqrt{\frac{4}{x}-3}$ , then we have $x=\frac{4}{(t+1)^2+3}$ and $$dx=\frac{-8(t+1)}{(t^2+2t+4)^2}dt$$ So we get $$\int_0^1 \sqrt{-1+\sqrt{\frac{4}{x}-3}}\ dx=8\int_0^\infty \frac{\sqrt{t}(t+1)}{(t^2+2t+4)^2}\ dt$$ Again, let $s=\sqrt{t}$ , then $dt=2sds$ and the integral becomes $$8\int_0^\infty \frac{s(s^2+1) 2s}{(s^4+2s^2+4)^2}\ ds=16\int_0^\infty \frac{s^2(s^2+1)}{(s^4+2s^2+4)^2}\ ds$$ Next, I write the integrand as $$\frac{s^2(s^2+1)}{(s^4+2s^2+4)^2}=\frac{1}{s^4+2s^2+4}-\frac{s^2+4}{(s^4+2s^2+4)^2}$$ so I have to evaluate $$I_1=\int_0^\infty \frac{1}{s^4+2s^2+4}\ ds\  and\ I_2=\int_0^\infty \frac{s^2+4}{(s^4+2s^2+4)^2}\ ds$$ But when I evaluate the integral $I_1$ , I get a weird result as follows: $$I_1=\int_0^\infty \frac{1}{(s^2+2)^2-(\sqrt{2}s)^2}\ ds
=\int_0^\infty \frac{1}{(s^2+\sqrt{2}s+2)(s^2-\sqrt{2}s+2)}\ ds=\int_0^\infty \left(\frac{\frac{1}{4\sqrt{2}}s+\frac{1}{4}}{s^2+\sqrt{2}s+2}+\frac{\frac{-1}{4\sqrt{2}}s+\frac{1}{4}}{s^2-\sqrt{2}s+2}\right)\ ds$$ and if we separate two parts and evaluate the integrals, we would get two divergent improper integrals. So how can I find $I_1$ ?(Hope that the method is elementary and without complex analysis.) Another question: I'm a beginner at learning complex analysis. I conjecture that we can evaluate the integral $I$ in complex analysis (or maybe not worked). Hope everybody can give me some hints or solutions with the method in complex analysis.","['integration', 'improper-integrals', 'analysis', 'complex-analysis', 'calculus']"
4500249,"What does it mean for a signed measure to be ""regular"" in Riesz-Markov-Kakutani theorem?","I'm reading RKM theorem from this lecture note by professor Tomasz Kochanek. Theorem 3.23 (Riesz-Markov-Kakutani for $\left.C_{0}(\boldsymbol{X})^{*}\right)$ . Let $X$ be a locally compact Hausdorff space and $\Lambda \in C_{0}(X)^{*}$ be a continuous linear functional on the Banach space (over $\mathbb{R}$ ) of real-valued continuous functions on $X$ vanishing at infinity. Then, there exists a unique regular Borel $\sigma$ -additive signed measure $\mu$ on $X$ such that $$
\Lambda f=\int_{X} f \mathrm{~d} \mu \quad \text { for every } f \in C_{0}(X) .
$$ Moreover, we have $\|\Lambda\|=|\mu|(X)$ . On the other hand, every $\mu \in \mathcal{M}(X)$ gives rise to an element $\Lambda$ of $C_{0}(X)^{*}$ via formula (3.4). Consequently, the map $\Lambda \mapsto \mu$ is an isometric isomorphism $$
C_{0}(X)^{*} \cong \mathcal{M}(X).
$$ Previously, the author said Definition 3.14 Let $\mu$ be a positive Borel measure on a locally compact Hausdorff space $X$ . A Borel set $E \subseteq X$ is called outer regular (resp. inner regular) if $$
\begin{gathered}
\mu(E)=\inf \{\mu(V): E \subseteq V, V \text { is open }\} \\
\text { (resp. } \mu(E)=\sup \{\mu(K): K \subseteq E, K \text { is compact }\}) .
\end{gathered}
$$ The measure $\mu$ is called regular if every Borel subset of $X$ is both outer and inner regular. Could you elaborate on how a signed Borel measure is defined to be ""regular""? I have two options in mind but I'm not sure if any of them is correct in this context of RKM theorem? We use the same Definition 3.14 for signed measures. We use Hahn decomposition to get $\mu = \mu_+ - \mu_-$ and define $\mu$ is regular if both $\mu_+$ and $\mu_-$ are regular in the sense of Definition 3.14","['measure-theory', 'signed-measures', 'riesz-representation-theorem', 'borel-measures', 'definition']"
4500300,$\begin{bmatrix}\mathbf O & U\\ V^T & 0\end{bmatrix}$ is diagonalizable?,"I was trying to solve this problem: find all vectors $U, V\in \mathbb R^{n}$ such that $$\begin{bmatrix}\mathbf{O} & U \\
V^T & 0\end{bmatrix}$$ is a diagonalisable matrix. I was able to see that the above matrix is of rank at most $2$ so I tried to find the remainning two eigenvalues of the matrix, but I did not find them in terms of $U$ and $V$ . Is there any way to do it?","['matrices', 'matrix-rank', 'diagonalization']"
4500303,Measurability of $X_{\tau-}$ if $X$ only has left-limits at the specific point $\tau$,"Let $E$ be a topological space, $(\Omega,\mathcal A)$ be a measurable space, $(X_t)_{t\in[0,\:\infty]}$ be an $E$ -valued progressively measurable process on $(\Omega,\mathcal A)$ and $\tau$ be a stopping time on $(\Omega,\mathcal A)$ with respect to the filtration on $(\Omega,\mathcal A)$ generated by $(X_t)_{t\in[0,\:\infty]}$ . Assuming that $X(\omega):[0,\infty]\to E$ has a left-limit $X_{\tau-}(\omega)$ at $\tau(\omega)$ for all $\omega\in\Omega$ , are we able to show that $X_{\tau-}$ is measurable (at least with respect to $\mathcal A$ )? I only know how I can prove measurability under the stronger assumption that $X$ has left-limits at every point ... BTW, we need Hausdorfness to ensure that the left-limit is unique, right?","['stochastic-processes', 'measure-theory', 'probability-theory']"
4500315,"If A is a $2×2 $ matrix, does $A^2=3A$ imply $ A=0$ or $A=3I$?","Problem : Let $A$ be a $2\times 2$ matrix, $O$ be the null matrix, and $I$ be the identity matrix. Is the following statement true? $$\text{if } A^2=3A \implies A=O\text{ or }3I$$ I tried proving this just by factorising $A^2-3A=O \implies A(A-3I)=O \implies A-3I=O, A=O$ . However, I cannot tell if this is correct. Do polynomials work the same way with matricies as they do with real/complex numbers?","['matrices', 'linear-algebra']"
4500332,Why does a cusp form correspond to holomorphic differential form?,"Consider a cusp form $f \in S_2(\Gamma)$ for a certain congruence subgroup $\Gamma$ . I would like to understand why $f(z)dz$ is a holomorphic 1-form on the curve $X_\Gamma$ . Its invariance is exactly the modularity condition, it remains to understand what holomorphic means and how to prove it. I don't see at all what to write here even for points away from the cusps (but apparently the difficulty comes at the cusp because of `` ramification"").","['modular-forms', 'automorphic-forms', 'riemannian-geometry', 'differential-geometry']"
4500348,Divergence of tensor fields,"I have found numerous definitions for the divergence of a tensor which makes me confused as to trust which one to use. In Itskov's Tensor Algebra and Tensor Analysis for Engineers , he begins with Gauss's theorem to define \begin{equation}
\text{div} ~\boldsymbol{S} = \lim_{V \to 0} \frac{1}{V} \int_{\partial V} \boldsymbol{S} ~\boldsymbol{n} ~da
\end{equation} which, resorting to some coordinates system, gives \begin{equation}
\text{div} ~\boldsymbol{S} = \boldsymbol{S}_{,i} ~\boldsymbol{g}^i = S_{j}^{~~i} |_i ~\boldsymbol{g}^j
\end{equation} I actually like this definition because of its naturalness from beginning with Gauss's theorem. However, it requires choosing a basis. To define a coordinate-free divergence, I have come across multiple definitions: One from this wiki article defines the divergence as \begin{equation}
(\boldsymbol{\nabla \cdot S}) \boldsymbol{\cdot a}  =   \boldsymbol{\nabla \cdot} ~(\boldsymbol{S ~a})
\end{equation} where $\boldsymbol{a}$ is an arbitrary constant vector. This gives \begin{equation}
\boldsymbol{\nabla \cdot S}  =  S^{i}_{~~j} |_i ~\boldsymbol{g}^j
\end{equation} where the first index is contracted. Yet, another wiki article defines \begin{equation}
  (\boldsymbol{\nabla}\cdot\boldsymbol{T})\cdot\mathbf{c} =
    \boldsymbol{\nabla}\cdot\left(\mathbf{c}\cdot\boldsymbol{T}^\textsf{T}\right)
\end{equation} to give the exact same result as the other wiki article. (Here I presume that Reddy's notation were used, where he uses dot product for denoting any product he can find! One problem with Reddy's notation is that I cannot figure out how he dot products a vector into a dyad, as in $\boldsymbol{e}_k \cdot \boldsymbol{e}_i\otimes\boldsymbol{e}_j$ , so please do not advise me using his notation. This being said, I don't know what $\mathbf{c}\cdot\boldsymbol{T}^\textsf{T}$ means; is it $\mathbf{c}~\boldsymbol{T}^\textsf{T}$ where $\boldsymbol{T}^\textsf{T}$ is acting from the left on the vector $\mathbf{c}$ ? If so, I don't think this holds for a general curvilinear basis. I guess $\mathbf{c}~\boldsymbol{\cdot}\boldsymbol{T}^\textsf{T} = \boldsymbol{c}^\textsf{T}\boldsymbol{T}^\textsf{T}$ is more appropreate, but I don't reckon Reddy means this way.) This article also says that Itskov's result (contracting the second index) is actually true only for symmetric tensors, which Itskov never assumes. Abeyratne's lecture notes (p. 64) uses this definition \begin{equation}
(\text{div} ~\boldsymbol{T})\cdot\mathbf{c} =
    \text{div} \left(\boldsymbol{T}^\textsf{T}\mathbf{c}\right)
\end{equation} where he claims that the second index gets contracted. I don't know whether $\text{div}$ and $\boldsymbol{\nabla \cdot}$ are different or the same. Ogden's ""Nonlinear Elastic deformations"" puts it in a very nice way: that there are three possible contractions for the gradient of a 2nd rank tensor $\boldsymbol{\nabla}\otimes \boldsymbol{T}$ , so defining the divergence is a matter of convention. He contracts the first index. But still, which one should one choose for a throughout consistency in his calculations. What is the definition of divergence? Kelly's lecture notes were a little helpful, yet because of its different notations from other, I always get caught wondering if I am doing the right way. For example, he finds for the gradient of a tensor field that $\text{grad}~\boldsymbol{v} = (\boldsymbol{\nabla}\otimes\boldsymbol{v})^\textsf{T}$ , but Ogden finds it with the transpose, and I believe they have used the same definitions to start with, namely the directional derivative. This will make much mess for me, as to define the divergence of the vector field whether as $\boldsymbol{\nabla\cdot v} = \text{tr} (\boldsymbol{\nabla}\otimes\boldsymbol{v})^\textsf{T}$ or as $\boldsymbol{\nabla\cdot v} = \text{tr} (\boldsymbol{\nabla}\otimes\boldsymbol{v})$ . Please help me organize my mind on the subject, and share with me your experience regarding the same notation conflictions and how you have overcome them.","['continuum-theory', 'differential-forms', 'differential-geometry']"
4500358,$3$ versions of Riesz–Markov–Kakutani theorem,"I'm reading RKM theorem from this lecture note by professor Tomasz Kochanek. I have no question here. This thread is to summarize $3$ versions of the theorem (in an increasing order of generality). I try to include related definitions to remove any ambiguity, but It's likely that there are subtle mistakes that I could not recognize. I'm very happy to receive your suggestions. Let $\mathbb K \in \{\mathbb R, \mathbb C\}$ . $X$ be a topological space, $\mathcal B$ the Borel $\sigma$ -algebra of $X$ $E :=C_c (X)$ the space of $\mathbb K$ -valued compactly supported continuous functions on $X$ . The support of a map $f:X \to \mathbb K$ is $\operatorname{supp} f := \overline{\{x \in X \mid f(x) \neq 0\}}$ . If $\mathbb K = \mathbb R$ then a linear functional on $E$ is real-valued and $\mathbb R$ -linear. If $\mathbb K = \mathbb C$ then a linear functional on $E$ is complex-valued and $\mathbb C$ -linear. Theorem 3.16 Let $X$ be locally compact Hausdorff and $\Lambda$ a linear (not necessarily continuous) functional on $E$ which is positive , i.e., $\Lambda f \ge 0$ for all $f \in E$ such that $f \ge 0$ . Then there exist a $\sigma$ -algebra $\mathfrak{M}$ on $X$ such that $\mathcal B \subset \mathfrak M$ , and a non-negative (not necessarily finite) measure $\mu$ on $\mathfrak{M}$ such that (a) $\Lambda f=\int_{X} f \mathrm{~d} \mu$ for every $f \in E$ ; (b) for every compact set $K \subset X$ we have $\mu(K)<\infty$ ; (c) $\mu$ is outer regular on for every $B \in \mathfrak{M}$ , i.e., $\mu(B) = \inf \{\mu(U) \mid B \subset U, U \text{ open}\}$ . (d) $\mu$ is tight on every open set $B$ and every $B \in \mathfrak{M}$ with $\mu(B)<\infty$ , i.e., $\mu(B) = \sup \{\mu(K) \mid K \subset B, K \text{ compact}\}$ . (e) $(X, \mathfrak{M}, \mu)$ is complete, i.e. if $E \in \mathfrak{M}, \mu(E)=0$ and $A \subset E$ , then $A \in \mathfrak{M}$ . Moreover, the measure $\mu$ is unique in the class of non-negative measures on $\mathfrak{M}$ satisfying conditions (a)-(d). Definition 3.20. Let $\mu:\mathfrak M \to \mathbb C$ be a complex measure defined on a $\sigma$ -algebra $\mathfrak{M}$ of subsets of a set $X$ . For any $B \in \mathfrak{M}$ we denote by $\Pi(B)$ the collection of all measurable finite partitions of $B$ , i.e., $$
\Pi(B)=\left\{\left(B_{1}, \ldots, B_{n}\right) \,\middle\vert\, n \in \mathbb{N^*}, B_{i} \in \mathfrak{M}, B_{i} \cap B_{j}=\varnothing \text { for } 1 \leq i \neq j \leq n, \bigcup_{i=1}^{n} B_{i}=B\right\} .
$$ The variation $|\mu|$ of $\mu$ is defined by $$
|\mu|(B)=\sup \left\{\sum_{i=1}^{n}\left|\mu\left(B_{i}\right)\right| \,\middle\vert\, \left(B_{1}, \ldots, B_{n}\right) \in \Pi(B)\right\} \quad \forall B \in \mathfrak{M}.
$$ The value $|\mu|(X)$ is called the total variation of $\mu$ . Proposition 6.1. For any complex measure $\mu$ , its variation $|\mu|$ is a non-negative finite measure. A complex Borel measure $\mu$ is called regular if its variation $|\mu|$ is both tight and outer regular on every Borel set. Definition A signed (or real) measure is a measure that takes values in $\mathbb R$ . This implies a signed measure is not allowed take the values $\pm \infty$ . Let $X$ be a topological space, $\mathcal M(X)$ the space of regular signed Borel measures on $X$ . $E :=C_0 (X)$ the space of $\mathbb R$ -valued continuous functions on $X$ vanishing at infinity. $E^*$ the continuous dual of $E$ . Then $\Lambda \in E^*$ is real-valued and $\mathbb{R}$ -linear. Theorem 3.23 Let $X$ be locally compact Hausdorff and $\Lambda \in E^*$ . Then there exists a unique regular Borel signed measure $\mu$ on $X$ such that $$
\Lambda f=\int_{X} f \mathrm{~d} \mu \quad \text {for every} \quad f \in E \quad (\star).
$$ Moreover, we have $\|\Lambda\|_{E^*} = |\mu|(X)$ . On the other hand, every $\mu \in \mathcal{M}(X)$ gives rise to an element $\Lambda \in E^*$ via formula $(\star)$ . Consequently, the map $\Lambda \mapsto \mu$ is an isometric isomorphism $$
E^* \cong \mathcal{M}(X).
$$ Let $X$ be a topological space, $\mathcal M(X)$ the space of regular complex Borel measures on $X$ . $E :=C_0 (X)$ the space of $\mathbb C$ -valued continuous functions on $X$ vanishing at infinity. $E^*$ the continuous dual of $E$ . Then $\Lambda \in E^*$ is complex-valued and $\mathbb{C}$ -linear. Theorem 6.10 Let $X$ be locally compact Hausdorff and $\Lambda \in E^*$ . Then there exists a unique regular Borel complex measure $\mu$ on $X$ such that $$
\Lambda f=\int_{X} f \mathrm{~d} \mu \quad \text {for every} \quad f \in E \quad (\star).
$$ Moreover, we have $\|\Lambda\|_{E^*} = |\mu|(X)$ . On the other hand, every $\mu \in \mathcal{M}(X)$ gives rise to an element $\Lambda \in E^*$ via formula $(\star)$ . Consequently, the map $\Lambda \mapsto \mu$ is an isometric isomorphism $$
E^* \cong \mathcal{M}(X).
$$","['measure-theory', 'signed-measures', 'riesz-representation-theorem', 'general-topology', 'compactness']"
4500360,Simple vector bundles are stable,"It is clear that stable vector bundles over a projective scheme are simple, i.e. $\operatorname{Hom}(F,F) \cong \mathbb{C}$ . Let $X$ now be an elliptic curve. Then every simple vector bundle is stable. How does one prove stability? I assume one would approach this by contradiction. So let $E\subsetneq F$ be a subbundle of $F$ such that $\mu(E) > \mu (F)$ . As we are over an elliptic curve, we know that $\operatorname{Hom}(E,F) = 0$ . But what now?","['coherent-sheaves', 'vector-bundles', 'algebraic-geometry', 'elliptic-curves']"
4500378,Find 3rd point of triangle given two points and one angle,"I have the following problem. I want to find the position of a point $(x, y)$ on a 2D Plane, by knowing three fixed points ( $A$ , $B$ and $C$ ) and two angles ( $\alpha$ , $\theta$ ). The angles are between the unknown point ( $U$ ) and the known points. My first intuition was to set up a system with two formulas, one for each of the angles, and then solving it. I am pretty sure this will result in two possible results, but I don't need to worry about that. I have tried solving it, but had no idea how I could solve it. My second thought was more of a guess, if it was possible to solve it by using circles, that would always create a specific angle along the edge (shown in the lower diagram on picture 2). picture 2 I would then just find the intersection of the circles and have the missing 4th point. Is that a valid option as well? Is there an easier way to calculate the point ( $U$ ) other, than how I wanted to do it?","['trigonometry', 'geometry']"
4500396,Counterexample: convergence of difference quotient in Backward Euler method,"I am pretty sure that, for the iterates of the backward Euler's method for $y'=f(t,y)$ one cannot obtain $\displaystyle \frac{y_n-y_{n-1}}{\delta t}\rightarrow y'(t^*)$ , for a suitable choice of $n$ . The reason is that $y_n=y(t_n)$ , but only at first order, so that $y_n-y_{n-1} = \delta t y'(t^*) + C\delta t$ , and this $C$ need not to approach $0$ . I was only able, though, to construct examples where we get convergence also of the difference quotients (e.g. $y'=ay$ ...). Do you have a counterexample, and possibly a descriptions of the class of functions $f$ on which backward Euler is of higher order than $1$ ? It seems that $f(t,y)=y$ , for instance, is in this class. Edit Actually, the derivative approximation property might be true by a bootstrapping argument: $\displaystyle \frac{y_n-y_{n-1}}{\delta t} = f(t_n,y_n)=f(t^*,y(t^*)) + f_t(t^*,y(t^*))(t-t^*)+f_y(t^*,y(t^*))(y_n-y(t^*)) + o((t-t^*, y_n-y(t^*)))$ . So that $f\in C^1$ suffices, thanks to the convergence $t_n\rightarrow t^*, y_n \rightarrow y(t^*)$ . What do you think?","['numerical-methods', 'ordinary-differential-equations']"
4500398,"Why is $K(1,3)$ a forbidden subgraph of a line graph $G$?","My professor taught about Forbidden subgraphs in class.
"" $H$ is forbidden subgraph of $G$ for a property $P$ , if $G$ has property $P$ and $G$ cannot contain an induced subgraph isomorphic to $H$ "" He gave an example that $K(1,3)$ is a forbidden subgraph of a line graph $G$ . I tried solving by contradiction. $G$ is a line graph of $H$ containing $K(1,3)$ as an induced subgraph.
If $v$ is a vertex if degree $3$ in $K(1,3)$ and $v_1$ , $v_2$ , $v_3$ are adjacent to $v$ in $K(1,3)$ , then edge $e$ corresponding to $v$ in $H$ is adjacent to edges $e_1$ , $e_2$ , $e_3$ corresponding to vertices $v_1$ , $v_2$ , $v_3$ . This is where I get stuck. Can anyone please help me. Where did I go wrong? Is this approach correct?","['graph-theory', 'discrete-mathematics']"
4500507,Show that the projection map of $S^n_+$ is orientation preserving $\iff$ $n$ is even,"This is problem $22.10(b)$ from Introduction to Manifolds (Tu). This question has been posed here and here , but no proof has been given. The text provided the answer to $22.10(a)$ , which is $(-1)^n dx^1 \wedge \ldots \wedge dx^n$ . We must first show that $\pi$ is a diffeomorphism. I get stuck here. I want to show that $\sigma : U \rightarrow \mathbb{R}^n \times \{0\}$ , defined by $$\sigma(x^1, \ldots, x^{n+1}) = (x^1, \ldots, x^n, 0), $$ is a diffeomorphism. But the Jacobian is clearly not invertible. If I could show that $\sigma$ was a diffeomorphism, then I would finish the proof as follows. Since $\mathbb{R}^n\times \{0\} \simeq \mathbb{R}^n$ , we now only consider the latter. A nowhere-vanishing form on $\pi(U) \subset \mathbb{R}^n$ is $dx^1 \wedge \ldots \wedge dx^n$ , and its pullback by $\pi$ is the same expression. Therefore, $\pi$ is orientation preserving $\iff$ $n$ is even. $\square$","['smooth-manifolds', 'differential-geometry']"
4500512,"$f(x,y)=(x^3-3xy^2,3x^2y-y^3)$ is a local diffeomorphism, but not a diffeomorphism.","Show that the map $f\colon \mathbb R^2\backslash\{0\}\to \mathbb R^2\backslash\{0\}$ given by $f(x,y)=(x^3-3xy^2,3x^2y-y^3)$ is a local diffeomorphism, but not a (global) diffeomorphism. It is a local diffeomorphism as one can show using the inverse function theorem. Indeed $$ df_{(x,y)}=
\begin{bmatrix}
3x^2-3y^2& -6xy \\
6xy& 3x^2-3y^2
\end{bmatrix}
$$ Its determinant is $(3x^2-3y^2)^2+36x^2y^2\neq 0$ for $(x,y)\in\mathbb R\backslash\{0\}$ . To show that it is not a diffeomorphism, I tried to show that it fails to be injective. Trying to finding distinct points giving same $f$ was not fruitful. Any idea of how to proceed from there!
Thanks in advance!","['smooth-manifolds', 'differential-geometry']"
4500514,Prove that a solution to differential equation is unbounded,"Consider the following system: $$ \begin{cases}
x'=x-6y\\
y'=-2x-y
\end{cases} $$ I want to prove that the solution to the system which satisfies $$ x\left(0\right)=1,\thinspace\thinspace y\left(0\right)=0 $$ is unbounded, Without solving the equation. My work so far: Notice that this system is hamiltonian system, meaning: $$ \begin{cases}
x'=\frac{\partial}{\partial y}H\left(x,y\right)\\
y'=-\frac{\partial}{\partial x}H\left(x,y\right)
\end{cases} $$ Where $ H\left(x,y\right)=x^{2}+xy-3y^{2} $ and the solution $\varphi$ which satisfies $\varphi(0)=(1,0)$ also satisfies $$ \varphi\left(0\right)\in\Lambda_{1}=\left\{ \left(x,y\right)\thinspace:\thinspace H\left(x,y\right)=1\right\} =\left\{ \left(x,y\right)\thinspace:\thinspace x^{2}+xy-3y^{2}=1\right\}  $$ And the level curve $\Lambda_1$ is hyperbola (which means it is unbounded). Now, I tried to assume by contradiction that $\varphi(t) $ indeed is a bounded solution, which means that both $x(t)$ and $y(t)$ are bounded, but I cannot find how to reach a contradiction. Any help would be appreciated.","['calculus', 'ordinary-differential-equations']"
4500518,Does the following proof use the axiom of choice.,I was looking at a proof for the countability of set of isolated points of any subset of separable metric space posted on another stack exchange thread . I suspect (but I'm not sure) that this proof uses the axiom of choice in determining $r_x$ and the function $f(x)$ discussed in the answer. Can someone help confirm if this is truly the case. If yes is there a way to proof this without using the axiom of choice?,"['general-topology', 'real-analysis']"
4500545,Riemann Roch over finite fields according to Weil,"In the somewhat old (but extremely nice) reference Adeles and Algebraic Groups by Weil, he cites Riemann-Roch as part of an argument for computing the volume of $\mathbb{A}_k/k$ (paraphrased slightly): Let $k$ a function field in one variable over $\mathbb{F}_q$ , and $\pi$ a uniformizer at a place $v$ of degree $n$ . The Riemann-Roch theorem shows at once that the number of cosets of $(\pi)$ in $k_v$ [which he has been using for the local field $\operatorname{Frac}\widehat{\mathcal{O}}_v$ ] is $q^{g+d-1}$ ( $g$ the genus). Can anyone see what Weil is trying to say here? This argument doesn't make much sense to me for a few reasons. First, where is he counting cosets in? Taking the simplest possible example of an $\mathbb{F}_q$ -point on $\mathbb{P}^1$ , it seems like we are asking about the index of $(t)$ in $\mathbb{F}_p((t))$ , which is definitely infinite. To get finite-index, it seems like we need to take the index in something integral , say $\mathcal{O}_v$ , which just gives the residue cardinality. Second, the application of Riemann-Roch seems odd. For any divisor $D$ of degree $n$ , $q$ -exponentiating RR gives $$
   [\;  \#H^0(D) : \#H^0(K-D) \;] = \frac{\#H^0(D)}{\#H^0(K-D)} = q^{n - g + 1}.
$$ None of the obvious choices $D = (\pi), D = - (\pi), K-D = (\pi)$ seem to match his numerics, even putting aside this confusion about where to count cosets in.","['function-fields', 'algebraic-number-theory', 'algebraic-geometry']"
4500549,Showing $E[X] \geq P(X\geq 2)$ for a random variable under certain conditions,"Let $X$ be a random variable with finite expectation, and suppose that $E[e^{-X}] \leq 1$ . Prove that $E[X] \geq P(X\geq 2)$ . My first thought was to use Markov's inequality, but I can only use that if $X$ is non-negative, and in any case, the resulting inequality doesn't seem very helpful. Even if I assume that $X\geq -c$ and apply it to $Y = X+c$ , I'm left to show $P(X\geq 2) \geq \frac{c}{c+1}$ , which doesn't seem like it's always true. I'm primarily unsure of how to use the $E[e^{-X}]$ condition; Jensen's inequality tells me that the expectation  is non-negative but there's presumably more to it. Additionally, I tried rewriting the inequalities in terms of integrals and playing around with parts, but that didn't seem to lead anywhere.","['probability-theory', 'probability', 'random-variables']"
4500583,What subsets of the integers still yield dense rational numbers?,"I am looking for a set of positive integers $(\mathbb{S} \space\subset\space \mathbb{N} := \{1,2,3,...\})$ such that its ""2-element quotients"" are dense over the full set of positive rational numbers (which would also make it dense over the full set of positive real numbers). One obvious satisfying set is $\{An+B \space\space|\space\space n\in\mathbb{N}\}$ where $A$ and $B$ are any given positive integers. What about higher powers ( $C$ is a given integer greater than 1) or square-free numbers? $$\{An^C+B \space\space|\space\space n\in\mathbb{N}\}$$ $$\{\prod _{primes} {p^{b_p} \space\space|\space\space b_p \in \{0,1}\}\}$$ By the way, I believe I could eliminate any finite number of elements from any satisfying set and still have a satisfying set, so I am really thinking about the set's ""asymptotic form"".","['number-theory', 'prime-numbers', 'rational-numbers']"
4500602,Particulate solution for $y''(x)-3y'(x)=2x+1+e^{3x}$,"I am in the process of solving this differential equation: $$y''(x)-3y'(x)=2x+1+e^{3x}$$ I have the homogeneous solution. However, I am not getting anywhere with the particular solution. One of the homogeneous solutions is $e^{3x}$ , which is why we would multiply an $x$ to it in the particulate approach. However, the other homogeneous solution is $1$ and I don't know exactly how it works. Could someone help me with this?",['ordinary-differential-equations']
4500613,Vector Calculus and the Navier Stokes Equation,"I am reading through a paper that derives the thin-film equation from the Navier Stokes equation; however, am having some trouble understanding the vector calculus notation used. For example, the material derivative is defined as: $$ \frac{D\mathbf{v}}{dt} = \frac{\partial\mathbf{v}}{\partial t}+\mathbf{v}\cdot\nabla\mathbf{v}$$ where in $2D$ we have $\mathbf{v}=(u,w)$ and $\nabla=(\partial_x,\partial_z)$ . Hence, the gradient of $\mathbf{v}$ is the transpose of the Jacobian given by: $$ \nabla\mathbf{v} = \begin{bmatrix} u_x & w_x \\ u_z & w_z \end{bmatrix} $$ Now my question is what to make of: $$ \mathbf{v}\cdot\nabla\mathbf{v}=\begin{bmatrix} u \\ w \end{bmatrix} \cdot \begin{bmatrix} u_x & w_x \\ u_z & w_z \end{bmatrix} $$ As far as I am aware the dot product can be defined for two vectors and can be extended to two square matrices but I don't quite know what to make of this when you have a vector and a square matrix. Specifically, for this case I already know from past experience that: $$ \frac{D\mathbf{v}}{dt} = \frac{\partial \mathbf{v}}{\partial t}+\mathbf{v}\cdot\nabla\mathbf{v} = (u_t+uu_x+wu_z,w_t+uw_x+ww_z) $$ but upon coming back to this I am a little confused as to how to show this given the notation above.","['fluid-dynamics', 'multivariable-calculus', 'vector-analysis', 'partial-differential-equations']"
4500637,Implicit function differentiation methods,"Q: If $$x\sqrt{1+y}+y\sqrt{1+x}=0$$ where $x,y\in\mathbb{R}$ then prove that: $$\dfrac{dy}{dx}=-\dfrac{1}{(1+x)^2}$$ My approach: $$x\sqrt{1+y}+y\sqrt{1+x}=0$$ $$\implies x\sqrt{1+y}=-y\sqrt{1+x}$$ $$\implies x^2(1+y)=y^2(1+x)$$ $$\implies x^2-y^2=xy(y-x)$$ $$\implies x+y=-xy$$ $$\implies y=-\frac{x}{1+x}$$ Now having converted to explicit form $y=f(x)$ we can differentiate and get the answer. I want to know if there is any other approach which avoids squaring or cancellation of terms?","['implicit-function', 'calculus', 'implicit-differentiation', 'derivatives']"
4500647,Trying to integrate $\int_0^{+\infty} \frac{x^{1/3}\ln x}{x^2+4}\; dx$,"This is a problem from a previous complex analysis qualifying exam that I'm working through to study for my own upcoming exam. I'm struggling with this combination of a fractional exponent and logarithm. Below is my attempt, but I need help with how to finish it or if there's a better method for this type of problem. Problem: Evaluate the improper integral: $$
I = \int_0^{+\infty} \frac{x^{1/3}\ln x}{x^2+4}\; dx
$$ Explain all steps carefully (show contours, introduce branches, etc.). Attempted Solution: Let $f(z) = \frac{z^{1/3}\ln z}{z^2+4}$ and consider the following contour with a branch cut along the positive real axis, where $\Gamma$ is the entire closed curve, $C_R$ is the outside circle of radius $R$ , $C_r$ is the inside circle of radius $r$ , $A$ is the line from $r$ to $R$ , $B$ is the line from $R$ back to $r$ , and $0\leq \arg z \leq 2\pi$ . Using the Residue Theorem, we have $$
\begin{split}
\oint_\Gamma f(z) dz
 &= 2\pi i \Big(\text{Res}(f,2i)+\text{Res}(f,-2i)\Big) \\
 &= \int_{C_R} f(z)dz + \int_{C_r} f(z)dz
  + \int_r^R f(z)dz + \int_R^r f(z)dz,
\end{split}
$$ where $\int_r^R f(z)dz$ uses $\arg z = 0$ and $\int_R^r f(z)dz$ uses $\arg z = 2\pi$ . Note that $z^{1/3} = e^{\frac{1}{3}(\ln z)} = e^{\frac{1}{3}(\ln |z|+i\arg z)}$ , thus we can write $$
f(z) = \frac{e^{\frac{1}{3}(\ln |z|+i\arg z)}\Big(\ln|z|+i\arg z\Big)}{z^2+4}.
$$ Using the ML estimation lemma, we find that $\int_{C_r} f(z)dz$ goes to zero as $r\to 0$ and $\int_{C_R}$ goes to zero as $R\to\infty$ . (Work omitted here but can be shown if needed.) We're left with $$
\begin{align*}
2\pi i \Big(&\text{Res}(f,2i)+\text{Res}(f,-2i)\Big) \\
&= \int_r^R f(z)dz + \int_R^r f(z)dz\\
&= \int_r^R \frac{e^{\frac{1}{3}(\ln |z|)}\ln|z|}{z^2+4}dz + \int_R^r \frac{e^{\frac{1}{3}(\ln |z|+2\pi i)}\Big(\ln|z|+2\pi i\Big)}{z^2+4}dz\\
&= \int_r^R \frac{x^{1/3}\ln x}{x^2+4}dx - e^{\frac{2\pi}{3}i} \int_r^R \frac{x^{1/3}(\ln x+2\pi i)}{x^2+4} dx 
\end{align*}
$$ Now from here, I know how to evaluate the residues and I see that we can isolate the term we're looking for as $$
\lim_{r\to 0}\lim_{R\to\infty} \int_r^R \frac{x^{1/3}\ln x}{x^2+4}dx,
$$ but I don't know what to do with the remaining integral term on the righthand side. How do I complete this? Or should I have used a different contour or branch cut?","['complex-analysis', 'contour-integration', 'branch-cuts']"
4500672,Generalization of Taylor series,"I noticed the similarity between Taylor series and Newton series: $$\sum\limits_{n \geq 0} \frac{D^nf(0)}{n!} x^n $$ $$\sum\limits_{n \geq 0} \frac{\Delta^nf(0)}{n!} x^{\underline{n}} $$ where $Df = df/dx$ , $\Delta f(x) = f(x+1)-f(x)$ , $x^\underline{n} = x(x-1)\ldots(x-n+1)$ . The natural question that comes to mind is that we can generalize that for a more general linear operator $A$ and a family of functions $\{ p_n \}_{n\geq 0}$ which satisfies properties $$  p_0 (x) = 1 $$ $$ p_n(0)= 0, \ n \geq 1$$ $$ A p_n = n p_{n-1} $$ $$ A1 = 0$$ So we will have the series $$\sum\limits_{n \geq 0} \frac{A^nf(0)}{n!} p_n (x) $$ Has it been studied already?
Which conditions should satisfy $A$ , $\{ p_n \}_{n\geq 0}$ so the series converges to $f$ ? What are the other examples of such series?","['analysis', 'calculus', 'taylor-expansion', 'sequences-and-series', 'newton-series']"
4500699,"Given n objects shared by several sets of known sizes, what is the minimum number of objects shared by all the sets?","Here is what seems to be an easy case.  Suppose there are 13 objects and 3 sets A, B, and C of size 9 sharing those objects.  Set A can have any 9 of the objects.  Set B can have at most 4 objects not in A, so A and B share at least 5 objects.  Set C must share at least 5 objects with A and 5 objects with B. C can only have 9 objects, so it must share at least one of them with both A and B.  Is this reasoning correct? How can the general case of such a problem be solved?  You know the size of the union of all sets of objects and the sizes of  each set.  How can you determine the minimum size for the intersection of all the sets?","['elementary-set-theory', 'combinatorics', 'extremal-combinatorics', 'discrete-optimization']"
4500707,Are these two integrals equal for all $n$?,"I am trying to prove that these two integrals are equal $$
\int_0^\infty\int_0^\infty\dots\int_0^\infty 
\exp(-x z_1-z_1z_2-z_2z_3-\dots-z_{n-1}z_{n}-z_n)
x^{c_1-1}  
z_1^{c_1+c_2-1} z_2^{c_2+c_3-1} \dots z_{n}^{c_n+c_{n+1}-1}
dz_1 dz_2 \dots dz_{n}
$$ $$
\int_0^\infty\int_0^\infty\dots\int_0^\infty
\exp(-x ^{w_n} z_1-z_1z_2-z_2z_3-\dots-z_{n-1}z_{n}-z_n)
x^{{w_n}\,c_{n+1}-1}  
z_n^{c_1+c_2-1} z_{n-1}^{c_2+c_3-1} \dots z_{1}^{c_n+c_{n+1}-1}
dz_1 dz_2 \dots dz_{n}
$$ where $x>0,c_n>0,\forall n$ and $$w_n = \begin{cases} 1 & \text{even}\,\,n  \\ -1 & \text{odd}\,\,n \end{cases}.$$ Using Mathematica, I was able to prove up to $n=5$ , but is this valid for all $n$ ?","['integration', 'calculus', 'derivatives']"
4500720,Proof that $A \cap (B -C)=(A \cap B)-(A \cap C)$,"I'm trying to prove the following: $A \cap (B -C)=(A \cap B)-(A \cap C)$ My line of reasoning is going astray, can someone help point me to where I'm making my mistake(s)? I'm new to creating mathematical proofs so I suspect I my error may be pretty basic. My line of reasoning is as follows: $$\begin{align}A \cap (B -C)
&= \{ x \in \varepsilon:x \in A \wedge(x\in B \wedge x \notin C) \}
\\&= \{x \in \varepsilon: x \in A \wedge (x \in B \wedge x \in C') \}
\\&= \{x \in \varepsilon: (x \in A \wedge x \in B) \wedge x \in C' \}
\\&= \{x \in \varepsilon: (x \in A \cap B) \wedge x \in C' \}
\\&=(A\cap B) - C\end{align}$$",['elementary-set-theory']
4500734,Fitting a largest area ellipse into a rectangle,"Given: a rectangle with vertices $A(0,2), B(0.5, 2.5), C(2.5, -0.5)$ and $D(3,0)$ . How to find the largest ellipse that can fit inside this rectangle?I'm confused about this rotational angle that I need to find. I know that the procedure is like the one here https://xaktly.com/Ellipse.html but not sure about angle.","['conic-sections', 'geometry']"
4500745,"Can one motivate and logically start Group Theory from cosets, as in Lagrange's theorem, instead of the definition of a group?","In Lagrange's theorem one must start with a group $G$ and subgoup $H$ . Let us say instead, sets $H \subset G$ with a binary operation inherited from $G$ . Then say we would like the sets to be partionable into distinct cosets under the natural binary coset operation $gH$ where $g \in G$ . The fact that they are distinct will force (motivate) an identity, associativity, and inverses via cancellation as follows:
If $x$ $\in g_1H$ $\cap g_2H$ $\rightarrow x = g_1h_1 = g_2h_2$ The process of showing $g_1H \subset g_2H$ gives, showing containment in one direction by assuming $g' \in g_1H$ or $g' = g_1h'$ for some $h' \in H$ The point here: we must use associativity inverses and identity to solve and get $g_1=g_2h_2h_1^{-1}$ in a natural or normal way. These are exactly what we need for the definition of a group! This is why I think Group Theory could start with Lagrange's theorem as a motivator for the definition of a group. We could say that ""If this is to work then the group properties are exactly what we need."" Can you show that this fails to force the definition of a group in any way? Is this logically adequate to create the definition of a group starting with cosets (as equivalence classes)? Can you get to distinct cosets without forcing the cancellation process to work? Please explain.","['group-theory', 'definition']"
4500748,"Bounding Turán numbers $t_3(11,5)$ and $t_3(16,7)$","I am looking to find the upper bound for Turán numbers $t_3(11,5)$ and $t_3(16,7)$ . Here $t_r(n,m)$ denotes the smallest integer $k$ such that each $r$ -uniform hyper graph on $n$ vertices with $k+1$ edges must contain a complete graph on $m$ vertices. Observe that when $H'$ is a complete graph on $m$ vertices, we have $t_{r}(n,m)=t_{r}(n,H')$ . I am looking for any known results for bounds of the Turán numbers $t_3(11,5)$ and $t_3(16,7)$ , but I cannot seem to find any. There seems to be known results for the case of $t_3(n,4)$ (see https://www.sciencedirect.com/science/article/pii/S0097316598929612 ).","['combinatorial-designs', 'graph-theory', 'extremal-combinatorics', 'combinatorics', 'discrete-mathematics']"
4500786,Characterization of convex functions,"From now on $I$ will always denote an open interval of $\mathbb{R}$ Let $f \; : \; I \to \mathbb{R}$ be a function, $f$ is said to be convex iff $$f((1-\gamma)x + \gamma y) \leq (1 - \gamma)f(x) + \gamma f(y) \hspace{0.4cm} \forall x,y \in I \; , \; \forall \gamma \in ]0,1[$$ A well known criterion for convexity is the following First criterion of convexity If $f \; I \to \mathbb{R}$ is a function such that $$(A1)\;\;f'(x) \text{ exists } \;\; \forall x \in I$$ $$(A2)\;\;f'(x) \leq f'(y) \hspace{0.3cm} \forall x,y \in I \; : \; x \leq y$$ Then $f$ is convex The criterion gives a sufficient condition which is not necessary, the example $f(x) = |x|$ shows that. I tried to generalize the criterion to find a condition which is sufficient and necessary for the convexity, this is what I managed to do. Let $f$ be a convex function, it can be proved that the function $$\begin{split} \Phi \; : \; (I \times I) \setminus \Delta &\to \mathbb{R} \\
(x,y) &\mapsto \Phi(x,y) = \frac{f(x) - f(y)}{x - y} \end{split}$$ where $(I \times I) \setminus \Delta = \{ (x,y) \in I \times I \; : \; x \neq y\}$ is increasing both in $x$ and in $y$ . From this property it's pretty easy to show that $$(B1) \; f'^-(x) \text{ and } f'^+(x) \hspace{0.4cm} \text{ both exists and are finite} \hspace{0.3cm} \forall x \in I$$ $$(B2) \; f'^-(x) \leq f'^+(x) \hspace{0.5cm} \forall x \in I$$ $$(B3) \; f'^+(x) \leq f'^-(y) \hspace{0.5cm} \forall x,y \in I \; : \; x < y$$ where $f'^-(x) := \lim_{y \to x^-}{\frac{f(x) - f(y)}{x-y}}$ , $f'^+(x) := \lim_{y \to x^+}{\frac{f(x) - f(y)}{x-y}}$ now let $D := \{ x \in I \; \text{ such that $f'(x)$ doesn't exist} \}$ , in other words $D$ is the set of the point of non differentiability of $f$ from the properties stated above it's easy to see that $D \subseteq \{ \text{ discontinuity points of $f'^-$} \}$ and because $f'^-$ is increasing ( what I mean with ""increasing"" is what some people mean with ""non-decreasing"" ) the discontinuity points of $f'^-$ are at most countable, therefore the point of non derivability of $f$ are at most countable, therefore $f$ is almost everywhere differentiable So although $f$ isn't necessarily everywhere differentiable it is almost everywhere differentiable Generalized first criterion of convexity Let $f \; : \; I \to \mathbb{R}$ be a function, if $f$ is convex if then (B1),(B2) and (B3) holds. furthermore the non differentiable points of $f$ are at most countable. My question is, are the condition $(B1),(B2)$ and $(B3)$ also sufficient for the convexity of $f$ ?
If so, how can I prove it? If they're not, what is a counterexample? There is also another criterion of convexity Second criterion of convexity if $f \; : \; I \to \mathbb{R}$ is a function such that $$(C1) f''(x) \;\; \text{ exists } \forall x \in I$$ $$(C2) f''(x) \geq 0 \;\; \forall x \in I$$ then $f$ is convex Can the Second criterion of convexity be generalized in a similar way to the first? Is there a result about the maximum cardinality of the points where $f$ isn't twice differentabile ? (similar to the result ""the points where $f$ isn't differentiable are at most countable"")","['calculus', 'convex-analysis', 'derivatives', 'analysis']"
4500810,Tough integral: $\int_0^1 \frac{\arctan(x^{3+\sqrt{8}})}{1+x^2}dx$,"$$I=\int_0^1 \frac{\arctan(x^a)}{1+x^2}dx=\frac{1}{16}\ln(2)\ln(a)~~~~~~~a=3+\sqrt{8}$$ I try the substitution: $u=1/x$ $$I=\int_1^\infty \frac{\arctan(1/u^a)}{1+u^2}du=\int_1^\infty \frac{\frac{\pi}2-\arctan(u^a)}{1+u^2}du$$ But I cannot add them to seal the integral limit. Also, the given number $a=(\sqrt{2}+1)^2$ is very tricky. I am not sure if this complete square will play some role. Or, if I do integration by part, I get: $$I=\frac{\pi^2}{4}-a\int_0^1 \frac{\arctan(x)}{1+x^{2a}}x^{a-1}dx$$ The power index $a$ is irrational, so how should I proceed, any hint will be appreciated!","['integration', 'algebraic-number-theory', 'definite-integrals', 'number-theory', 'zeta-functions']"
4500823,I need some help to solve a plane geometry problem.,"I been doing some geometry of some problems from Olympiads. I found one that I really haven't been able to solve. It's been 3 days since I
found it. I have to admit that I haven't been all the time doing this problem, but I've spend around 3 and half hours doing this problem and I couldn't really crack the nut. In an acute triangle $ABC$ , an arbitrary point $P$ is chosen on the altitude $AH$ . The points $E$ and $F$ are the midpoints of $AC$ and $AB$ , respectively. The perpendiculars from $E$ on $CP$ and from $F$ on $BP$ intersect at the point $K$ . Show that $KB = KC$ . Well, at first I tried Euclidean geometry and tried to use the midpoint theorem and some constructions and I even discovered something cool about the circle with radius $KC$ , then I realized I could prove that $K$ lies on the perpendicular bisector of $BC$ , use the Pythagorean theorem and I even noticed that the point $K$ was the intersection of 3 perpendicular lines, so I could hypthecally prove that $K$ was the circuncenter of some triangle involving these sides. At the end , I did use Cartesian coordinates, but, there were so many equations and really, didn't get do much satisfaction. Does someone has some good idea on how to solve it with Euclidean geometry? I pretty much did all of that.","['contest-math', 'geometry']"
4500879,Is there a $p$-adic analogue of the Hodge index theorem and Hodge--Riemann relations?,"This question may be quite vague and weird, but let me try to make the motivation clear. Let $X$ be a compact Kähler manifold of dimension $n$ . The Hodge index theorem says that The signature of the intersection form on $H^n(X,\mathbb R)$ is $\sum_{a,b}(-1)^ah^{a,b}(X)$ . And the Hodge--Riemann relations imply that Let $\omega$ be a Kähler class. Then for any even $k\leq n$ , the signature of the bilinear form $$ H^k(X,\mathbb R)\times H^k(X,\mathbb R)\to \mathbb R$$ given by $(\alpha, \beta)\mapsto \int_X\alpha\wedge\beta\wedge\omega^{n-k}$ is totally determined by the Hodge structure of $H^*(X,\mathbb R)$ . Proofs of both results can be found in Voisin's book Hodge Theory and Complex Algebraic Geometry Section 6.3. Let $X$ be a complex projective manifold with an ample line bundle $L$ . Let $\omega=c_1(L)$ be the first Chern class of $L$ . Let $k\leq n$ be an even number. In an attempt to understand the symmetric bilinear form $$H^k(X,\mathbb Q)\times H^k(X,\mathbb Q)\to \mathbb Q$$ given by $(\alpha, \beta)\mapsto \int_X\alpha\wedge\beta\wedge\omega^{n-k}$ , we may pass to coefficients $\mathbb R$ or $\mathbb Q_p$ . The Hodge--Riemann relations roughly say that with coefficients $\mathbb R$ , the nondegenerate bilinear form is determined by the Hodge structure of $H^*(X,\mathbb Q)$ , since a real nondegerate symmetric bilinear form is essentially determined by its signature. Knowing information about this bilinear form with $\mathbb Q_p$ -coefficients would be helpful for the understanding of the bilinear form with $\mathbb Q$ -coefficients itself. Can we say something about this bilinear form with $\mathbb Q_p$ -coefficients (e.g. the discriminant, the $\epsilon$ -invariant)?","['hodge-theory', 'complex-geometry', 'reference-request', 'algebraic-geometry', 'quadratic-forms']"
4500892,A problem of convergence of stochastic processes,"Let $X = (X_t)_{t \in \mathbb{Z}} \sim P$ and $Y = (Y_t)_{t \in \mathbb{Z}}\sim Q$ be two stochastic processes. In order to define the Mallows metric, for all $m\in \mathbb{N}$ , let $\mathcal{M}_m$ be the random vectors $(\tilde{X},\tilde{Y})$ having marginals $P\circ\pi_{1,...,m}^{-1}$ and $Q\circ\pi_{1,...,m}^{-1}$ , where $\pi_{1,...,m}(  (X_t)_{t \in \mathbb{Z}} )= (X_{t_1},..., X_{t_m})$ . So: $$d( X,Y)= \sum_{m=1}^\infty d^{(m)}(P\circ\pi_{1,...,m}^{-1}, Q\circ\pi_{1,...,m}^{-1})2^{-m}$$ where $$d^{(m)}(P\circ\pi_{1,...,m}^{-1}, Q\circ\pi_{1,...,m}^{-1}) = \inf_{(\tilde{X},\tilde{Y})\in \mathcal{M}_m}{(E||X-Y||^2)^{\tfrac{1}{2}}}.$$ The following characterization is useful: $d(X_n,X) \to 0,\,(n \to \infty)$ is equivalent to $X_{t_1,...,t_m;n}\implies X_{t_1,...,t_m}\, (n \to \infty)$ for all $t_1,...,t_m \in \mathbb{Z}, m \in \mathbb{N}$ $E[X_{0,n}^2] \to E[X_{0}^2], (n \to \infty)$ . i.e., all finite dimensional distributions at $t_1,...,t_m$ converge weakly and the
variance of the one-dimensional marginal converges. Now I want to show the following: given $X = [X_t : t \in \mathbb{Z}] \sim P$ be a stationary process with $E[X_t]=0$ . Then there exists a sequence of stationary and ergodic processes $(X^{n})_{n \in \mathbb{N}}$ with $E[X_t^n]=0$ for all $n$ such that $$d(X_n,X) \to 0,\,(n \to \infty)$$ Help!","['self-learning', 'weak-convergence', 'stochastic-processes', 'time-series', 'probability-theory']"
4500913,"Finding general expression of series expansion $A^{-1} = \sum_{n=0}^{\infty} (-1)^n a_n x^n$, $A = \sum_{n=0}^{\infty} a_n x^n$","Let \begin{align}
&A^{-1} = \sum_{n=0}^{\infty} (-1)^n a_n x^n \\
&A = \sum_{n=0}^{\infty} a_n x^n
\end{align} Using the relation $A^{-1} A =1$ , I want to find the general expression for $a_n$ . My assumption and initial conditions are given as follows with $a_0=1, a_1=1$ . With this I find explicitly, $a_2= \frac{1}{2}$ , $a_3 = \frac{1}{4}$ , $a_4 = \frac{1}{8}$ , $a_5=0, \cdots$ . How I can set the general expression for $a_n$ ? After seeing the comment from @metamorphy,  I want additionally impose all odd powers vanishes except $a_3$ . i.e., $a_{2n+1}=0$ for $n>1$ . In this case, is this fix the uniqueness? Additionally I relax the positiveness condition for $a_n$ .","['power-series', 'formal-power-series', 'recurrence-relations', 'sequences-and-series']"
