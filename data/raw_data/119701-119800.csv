question_id,title,body,tags
1786876,Why is this set not a manifold?,"Set $M = \{ \, (x, y) : x^2 = y^2 \, \}$.  If for every point $(a, c)$ in $M$, there exists a neighborhood $U$ containing $(a, c)$ and function $\phi(x, y)$ such that: $\phi(x, y) = 0$ on $M \cap U$; The Jacobian matrix associated with $\phi$ has rank $1$ on $U$.  (In general, it does not have to be rank $1$.  But here the only choice is $1$.) Then, $M$ is a manifold.  If the Jacobian matrix has ranks greater than $0$, then we have use $\phi$ to carry out the implicit function theorem, and construct a function such that $(x, y) = (x, f(x))$ on $M \cap U$.  But I don't know how to go in reverse; what is the contradiction if $M$ is a manifold? An educated guess says that $(0, 0)$ is our trouble spot.  The function $\varphi(x, y) = x^2 - y^2$ equals $0$ on $M$.  But the Jacobian matrix has zero rank at $(0, 0)$.  So, we cannot use $\varphi$ to carry out the implicit function theorem...",['multivariable-calculus']
1786896,Rounding error of trapezoidal method,"I'm working with the Modified Euler method sometimes called Heun's method or explicit trapezoidal method. I have a book on ordinary differential equations numerical analysis that claims: The effect of rounding error on the accuracy of the numerical solution
   is very similar to the error of the numerical differentiation
   formulas: the truncation error decreases with h but the rounding error
   increases and there exist an optimal value for which the sum of both
   errors is minimum. This optimal value for $h$ is very little (for example for Euler's
   method is $\sqrt\mu$ where $\mu$ is the accuracy of the machine) and so
   it's computationally expensive. I have an example of what he is talking about with numerical differentiation formulas so for example take the differentiation formula: $$f'(x_0) \simeq \frac{f(x_0+h)-f(x_0)}{h}~\text{  with error }~-h \frac{f''(\theta)}{2}$$ then I write $e(x)$ the rounding error on point $x$ and so the total error made while aproximating $f'(x_0)$ is $$
\frac{e(x_0+h)-e(x_0)}{h} - h \frac{f''(\theta)}{2}.
$$ Assuming that rounding errors are bounded by $\epsilon$ and that $f''$ is bounded by M in $[x_0,x_0+h]$ then the total error verifies: $$
\left|f'(x_0)-\frac{f_1(x_0+h)-f_1(x_0)}{h}\right| \leq 2 \frac{\epsilon}{h} + \frac{h}{2}M
$$ where $f_1$ is the approximation of $f$ with rounding error. So, when $h$ decreases the error of the formula (truncation error) decreases but rounding error increases. My question is very simple, how can I get a similar situation for the trapezoidal method?","['numerical-methods', 'ordinary-differential-equations', 'rounding-error', 'error-propagation']"
1786936,Weierstrass Approximation Theorem for a Product Space?,"I am faced with the following problem: Let $X$ and $Y$ be compact Hausdorff spaces and $f$ belong to $C(X \times Y)$. Show that for each $\epsilon > 0$, there are functions $f_{1}, f_{2}, \cdots , f_{n}$ in $C(X)$ and functions $g_{1}, g_{2}, \cdots, g_{n}$ in $C(Y)$ such that $\displaystyle \left \vert f(x,y) - \sum_{k=1}^{n}f_{k}(x)\cdot g_{k}(y) \right \vert < \epsilon$ for all $ (x,y) \in X \times Y$. The only idea that I have is that this is kind of like a Weierstrass Approximation Theorem for the product space $X \times Y$, but I am at a loss for how to prove it. Any suggestions on how to proceed would be most welcome. Also, please be willing to answer follow-up questions, because I'm the kind of person who has them. Thanks.","['general-topology', 'products', 'real-analysis']"
1786945,Prove that $1^8+2^8+\cdots+99^8 \equiv 1^4+2^4+\cdots+99^4 \pmod{25}.$,"Prove that $$1^8+2^8+\cdots+99^8 \equiv 1^4+2^4+\cdots+99^4 \pmod{25}.$$ Attempt: We can easily show that an eighth power can be expressed as a fourth power since $x^8 = (x^2)^4$. Conversely, by Fermat's Little Theorem, $x^{\phi(25)} = x^{20} \equiv 1 \pmod{25}$ if $\gcd(x,25) = 1$ and thus $x^4 = x \cdot x^3 \equiv x^{21} \cdot x^3 \equiv x^{24} \equiv (x^3)^8 \pmod{25}$. $\square$ I am not sure if the above proves the result, but it does show that if we have an $8$th power of an integer modulo $25$ and it is relatively prime to $25$, then it is equal to a fourth power of an integer modulo $25$ and vice-versa. Does that therefore mean they are equivalent?",['number-theory']
1786953,Dimension of solution space of homogeneous system of linear equations,"I have the homogeneous system of linear equations $$
3x_1 + 3x_2 + 15x_3 + 11x_4 = 0,
$$
$$
x_1 âˆ’ 3x_2 + x_3 + x_4 = 0,
$$
$$
2x_1 + 3x_2 + 11x_3 + 8x_4 = 0.
$$ I have converted to a augmented matrix and row reduced to 
$$\begin{bmatrix}1 & 0 & 4 & -3 & 0\\0 & 1 & 1 & 2/3 & 0\\0 & 0 & 0 & 0 & 0\end{bmatrix}$$ And came up with the general solution:
$$\begin{bmatrix}x_1 \\x_2\\x_3\\x_4\end{bmatrix} = s\begin{bmatrix}4 \\-1\\1\\0\end{bmatrix}+ t\begin{bmatrix}-3\\-2/3\\0\\1\end{bmatrix}$$ I know that the basis is: $$\left\{\begin{bmatrix}4 \\-1\\1\\0\end{bmatrix},\begin{bmatrix}-3\\-2/3\\0\\1\end{bmatrix}\right\}$$ But how do I determine the dimension of the solution space?","['matrices', 'homogeneous-equation', 'linear-algebra']"
1786971,Inverting an $n \times n$ matrix using determinant,"We're asked to invert the following matrix with the help of guided questions.
$$\begin{pmatrix} 1 + a_1 & 1 & \cdots & 1 \\ 1 & 1+a_2 & \ddots & \vdots \\ \vdots & \ddots & \ddots & 1 \\ 1 & \cdots & 1 & 1 + a_n \end{pmatrix}$$ To do that, the problem considered the following determinant : Let $(r_1,r_2,\cdots,r_n)$ be distinct real numbers and
$(a,b)\in\mathbb{R}^2$
$$\Delta(x) = \begin{vmatrix} r_1 + x & a+x & \cdots & a+x \\ b+x & r_2+x & \ddots & \vdots \\ \vdots & \ddots & \ddots & a+x \\ b+x & \cdots & b+x & r_n + x \end{vmatrix}$$ Then we are asked to prove there exists two real numbers A and B such that for all real numbers $x$ $$\Delta(x) = Ax + B$$ I have done a row reduction on the matrix to simplify it. For $i$ going from $n-1$ to $1$ $L_{i+1} \leftarrow L_{i+1} - L_i$ $$\Delta(x) = \begin{vmatrix} r_1 + x & a+x & \cdots & a+x \\ b-r_1 & r_2-a & 0  & 0 \\ 0 & \ddots & \ddots & 0 \\ 0 & 0 & b- r_n & r_n-a\end{vmatrix}$$ Then I developped with respect to the first row. Let $\Delta_{i,j}$ be the minor of $\Delta$ then $$\Delta(x) = (r_1 + x)\Delta_{1,1} + (a+x)\sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j} $$ Rearranging the forula gives A and B independent of $x$ which proves their existence : $$\Delta(x) = x[\Delta_{1,1} + \sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j}] + r_1\Delta_{1,1} + a\sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j} $$ The real problem i'm facing is the next question: 
Given the polynome $P(X) = \prod\limits_{k=1}^n(r_k-X)$ and supposing $a \neq b$ we need to prove that  $$\Delta(0) = \frac{aP(b)-bP(a)}{a-b}$$
To use the first question I noticed $\Delta(0) = B$ and simplified it
$$
   B = r_1\Delta_{1,1} + a\sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j} 
\\ \Delta_{1,1} = \prod\limits_{i=2}^n (r_k-a) 
$$ Since $\Delta_{1,1}$ is an inferiour triangular matrix, then similarly :
$$\Delta_{1,n} = \prod\limits_{i=1}^{n-1} (b-r_k) $$
For other minors I noticed that we can split them into diagonal inferiour and superior triangular blocks $$
\Delta_{i,j}=
\left|
\begin{array}{c|c}
B' & O \\
\hline
O & A'
\end{array}
\right|
$$
and thus $\forall j\in \{2,\cdots,n-1\} $
$$\Delta_{1,j} = \prod\limits_{i=2}^{j-1} (b-r_k)\prod\limits_{i=j+1}^{n} (r_k-a)$$
So i ended up with this 
$$\Delta(0) = B = r_1\prod\limits_{i=2}^n (r_k-a) + a\left(\sum\limits_{j=2}^{n-1} (-1)^{1+j}\prod\limits_{i=2}^{j-1} (b-r_k)\prod\limits_{i=j+1}^{n} (r_k-a)\right) + \\ a\times(-1)^{1+n}\times\prod\limits_{i=1}^{n-1} (b-r_k) $$ I feel like i am close to getting $P(b)$ and $P(a)$ from the first and last terms but i can't seem to see how? Sorry for any mistakes in advance, i'm new around here.","['matrices', 'inverse', 'linear-algebra', 'determinant']"
1786979,Shortest distance between two lines in 3-dimensional space [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Can someone explain to me how to solve this question? Find the shortest distance between the lines 
$L_1 = \left\{t \begin{bmatrix} 1\\ 1\\ 1\end{bmatrix} : t \in \mathbb{R}\right\}$ and $L_2 = \left\{s \begin{bmatrix} 1\\ 2\\ 3\end{bmatrix} + \begin{bmatrix} 1\\ 0\\ 0\end{bmatrix}: s \in \mathbb{R}\right\}$ Thanks","['optimization', 'least-squares', 'convex-optimization', 'linear-algebra']"
1786984,Index of a Jordan curve,"Winding number theorem : If $J\subset \mathbb{C}$  is a Jordan curve and a point $z$ lies in its interior domain, then the winding number $n(J,z)=\pm 1$. Now suppose that $J$ is smooth and we have the Jordan curve theorem . Is there any simple complex analysis proof for winding number theorem? I have found only tedious (non-analytical) proofs for the case of continuous curves. EDIT : Possible proof should use the facts that in each component of $\mathbb{C}\setminus J$ the winding numer is constant, and winding number is zero in the unbounded component; Jordan curve theorem tells us that there are only two different components. But I have no idea how to analytically conclude that $|n(J,z_{\text{inside}})-n(J,z_{\text{outside}})|=1.$","['plane-curves', 'differential-topology', 'complex-analysis', 'winding-number', 'general-topology']"
1786997,Triple integral vs double integral to find volume of an object,Is it possible to find the volume of an object bounded by two surfaces in both of these two ways?: -a triple integral of 1 dV (I know this works) -a double integral of the top surface - bottom surface dA (over domain of the intersection) How do you tell if a double integral or triple integral is best for a certain situation?,"['volume', 'calculus', 'multivariable-calculus', 'integration', 'surfaces']"
1787001,Conditional Distributions vs. Stochastic Processes,"Is the concept of a version of a stochastic process related to the concept of a version of a conditional distribution? And is a regular version of a stochastic process somehow the same thing as the regular version of a conditional distribution? I find this confusing because both concepts seem like they are supposed to resolve ambiguities arising when going from the countable to the uncountable case, but I rarely see authors use or explain the terminology rigorously, seemingly because they already understand it. Note: regular version of a stochastic process refers to a version which is regular in the sense that it has regular paths (right-continuous, cadlag, continuous). I saw a more precise definition in a textbook once, but now I can't remember which textbook it was. It was in any case definitely along the lines of the implicit definition used in other texts, namely a version of the stochastic process with regular paths. UPDATE: I found the following definition of regular function on Wikipedia in the context of integration by parts: https://en.wikipedia.org/wiki/Lebesgue%E2%80%93Stieltjes_integration#Integration_by_parts It is very similar to the notions I mentioned earlier for functions with countably many jump discontinuities (since one can modify the value of the function at those points, with no change to its integrals, to make it regular vis-a-vis cadlag). A function $f$ is regular at a point $a$ if and only if the left and right hand limits $f(a-)$ and $f(a+)$ both exist and the function takes the average value at $a$, i.e. $$f(a)= \frac{f(a-)+f(a+)}{2}$$","['stochastic-processes', 'probability-theory', 'terminology', 'measure-theory', 'conditional-expectation']"
1787015,Showing that an intersection of indexed sets is a subset of every individual indexed set,"I am required to show that for every $k \in I$, $\bigcap_{i\in I}A_{i}\subseteq A_{k}$ where $I$ is an index for a collection of subsets $A_{i}\subseteq S$, $i \in I$. This seems obvious to me from the definition of intersection. That is: $\bigcap_{i\in I}A_{i}= \{x \in S: \forall i \in I, x \in A_i\}$. Since every element in the set is, by definition, in $A_i$ for all $i \in I$, it is, of course, also in $A_k, k \in I$. Therefore, the set is a subset of $A_k$. Is this all I need to show? Or is there an actual proof I can/should write?","['definition', 'elementary-set-theory', 'proof-verification']"
1787070,The Main Theorems of Calculus,"From the J. Taylor's book, ''The completeness property is the missing ingredient in most calculus course. It is seldom discussed, but without it, one cannot prove the main theorems of calculus.'' My question is: Why (without it), one cannot prove the main theorems of calculus?","['real-analysis', 'calculus']"
1787073,fundamental period of function $f(x)$ is,"If $f:\mathbb{R}\rightarrow \mathbb{R}$ and $f(2+x) = f(2-x)$ and $f(20-x) = f(x)\;\forall x\in \mathbb{R}$ and $f(2)\neq f(6)$ Then fundamental period of function $f(x)$ is $\bf{My\; Try::}$ Given $f(2+x) = f(2-x)\;,$ Now replace $x\rightarrow (2-x)\;,$ We get $f(4-x)=f(x)$ and given $f(20-x)=f(x)\;,$ So we get $f(4-x)=f(20-x)$ Now Replace $x\rightarrow (4-x)\;,$ We get $f(x)=f(x+16)$ So period of function $f(x)$ is $=16\;,$ But Options given as $(a)\;\;\;\; 1\;\;\;\; (b)\;\;\;\; 8$ $(c)\;$ Period can not be $1\;\;\;\; (d)\; $ may be one Help required, Thanks",['functions']
1787084,What is an integer?,"When we define an integer, we say it is a whole number that can be positive or negative or equivalently it is a number with no fractional part. Does that mean it is a number with no fractional part in base $10$ or in any base? Because if so, then the definition would be fine since it is impossible to represent an integer in a different base that has a fractional part. I am just confused how we define what an integer is.","['number-theory', 'terminology']"
1787090,Gradient and Divergence in Riemannian Manifold,"Let $M$ a riemannian manifold. Let $X\in\chi(M)$ and $f$ a function $C^{\infty}$ in $M$ . Define the divergence of $X$ as a function $\operatorname{div}X\colon M\to\mathbb{R}$ given by $\operatorname{div}X(p)=\mbox{trace of the linear application } Y(p)\to\nabla_YX(p) $ , $p\in M$ and gradient of $f$ as the vector field $\operatorname{grad} f$ in $M$ define by $\langle\operatorname{grad}f(p),v\rangle=df_{p}(v)$ , $p\in M$ , $v\in T_{p}M$ . Let $E_{i}$ a geodesic frame in $p\in M$ . Show that $$\nabla(f)=\sum_{i=1}^{n}{(E_{i}(f))E_{i}(p)}\quad \mbox{ div}X(p)=\sum_{i=1}^{n}{E_{i}(f_{i})(p)}$$ Where $X=\sum_{i}{f_{i}E_{i}}$ . My approach: Note that, $\langle\mbox{grad} (f)(p),V\rangle=df_{p}(v)$ , $p\in M$ , $v\in T_{p}M$ . So, $\langle\mbox{grad} (f)(p),E_{i}(p)\rangle=df_{p}(E_{i})=E_{i}(f)(p)$ , then $$\langle\sum_{i=1}^{n}{a_{i}E_{i}(p)},E_{j}(p)\rangle
=\sum_{i=1}^{n}{a_{i}}\langle E_{i}(p),E_{j}(p)\rangle
=\sum_{i=1}^{n}{a_{i}\delta_{i,j}}=a_{j}$$ I don't know this is right or not. For the second equation, suppose that $Y(p)=\sum_{i=1}^{n}{a_{i}E_{i}(p)}$ and $X(p)=\sum_{j=1}^{n}{b_{j}E_{j}(p)}$ , then $$\nabla_YX\vert_p
=\sum_{i=1}^{n}{a_{i}\nabla_{E_{i}}(X)}\vert_{p}
=\sum_{i=1}^{n} a_i \left({\sum_{j=1}^{n}{b_{j}\nabla_{E_{i}}E_{j}}}+E_{i}(b_{j})E_{j}\right)\vert_{p}
=\sum_{j=1}^{n}\left(\sum_{i=1}^{n}{a_{i}E_{i}(b_{j})}\right)E_{j}\vert_{p}$$ Now, how I see that $\operatorname{div}X=\sum_{i=1}^{n}{E_{i}(b_{i})(p)}$ . Thanks!","['riemannian-geometry', 'differential-geometry']"
1787094,First year calculus student: why isn't the derivative the slope of a secant line with an infinitesimally small distance separating the points?,"I'm having trouble with the limit approach to calculus ever since I heard about the infinitesimal definition. Maybe you can help me settle what's been bothering me this year. Looking at the limit definition of the derivative equation makes sense. However, what trips me up is the fact that because the slope function is not defined when $\Delta x $ equals zero, how can we say the derivative is tangent instead of an infinitely accurate secant line? Because from my understanding, in order for it to be a tangent line, it intersects the curve at one point only, however $\Delta x$ approaches zero, it never reaches it, so $\Delta x$ must be greater than zero, however infinitesimally small, correct? Mathematicians have generally abandoned this idea now from what I understand with the exception of non-standard analysis. Can somebody explain where my thinking is wrong?","['derivatives', 'limits', 'calculus', 'infinity', 'infinitesimals']"
1787096,Solving the roots of Redlich Kwong Equation,"Good evening everyone. After studying some equations of state, I've read about the mathematical steps formulated to model some. In the particular case of Van der Waals, where
$$P=\frac{RT}{v-b}-\frac{a}{v^2}$$ For those not familiar with this expression, this equation describes general characteristics of a real gas, P standing for pressure, R for the universal gas constant, T for temperature, v for molar volume (Yep, I know the little line is missing, sorry!), and a and b are characteristic constants for the gas . There are some special values for Temperature, Pressure and Molar volume. Finding them, apparently, helps me in my quest for a and b. I am told the following steps to do so in VdW equation: 1) Derive the function twice. 2) Solve RT for each derivative and make them equal to zero.Make an equation of of the derivative and the second derivative, then solve for v 3) Plug the value of V in the first derivative, and solve for T, now called critical temperature. 3 4) Plug the values of V and T in the original function, thus obtaining the critical pressure. 4 5) Solve ""a"" for critical T and critical P, then making the equation and obtaining ""b""[5] 6) Plug ""b"" in critical T, now obtaining ""a""[6] Now, back to my problem, I'm looking for ""a"" and ""b"" again, but now in the Redlich Kwong equation:
$$P=\frac{RT}{v-b}-\frac{a}{\sqrt(T)*(v^2+vb)}$$
I've already derived the function twice and solved for RT:
$$P'=\frac{-RT}{(v-b)^2}+\frac{a(2v+b)}{\sqrt(T)*((v^2+vb)^-2)}=0$$
$$RT=\frac{a(2v+b)*((v-b)^2}{\sqrt(T)*((v^2+vb)^-2)}$$ $$P''=\frac{2RT}{(v-b)^-3}+\frac{2a}{\sqrt(T)*((v^2+vb)^-2)}-\frac{2a*(2v+b)^2}{\sqrt(T)*((v^2+vb)^-3)}=0$$
$$RT=\frac{a}{\sqrt(T)}*((v-b)^3)*(\frac{(2v+b)^2}{((v^2)+vb)^3}-\frac{1}{(v^2)+vb)^2})$$ After putting the two =0, I've obtained this far:
$$3v+2b=(v-b)*((2v+b)^2)((v^2)+vb)$$ As it isn't as simple as in the VdW case, I certainly don't know where to go now in order to obtain a value of v based on b. I would be so grateful if someone checked this, thank you!","['algebra-precalculus', 'calculus']"
1787130,"Integration by parts: ""math is broken"" [duplicate]","This question already has answers here : Integration by parts of $\cot x$ (2 answers) Closed 8 years ago . just trying to solve a small example on integration by parts, and a weird thing happens: I come to an original expression increased by one. Please help me find out where the flaw is! The task is to calculate the following indefinite integral:
$$
\int\tan^{-1}x\text{d}x
$$ Integration by parts formula (just in case):
$$
\int f(x)g'(x)dx = f(x)g(x) - \int f'(x)g(x)\text{d}x
$$ Let's expand our original integral:
$$
\int\tan^{-1}x\text{d}x = \int\cos x \sin^{-1}x\text{d}x
$$ If
$$
f(x) = \sin^{-1}x
$$
$$
g'(x) = \cos x
$$
then
$$
f'(x) = -\sin^{-2}x\cos x
$$
$$
g(x) = \sin x
$$ Applying integration by parts formula:
$$
\int\cos x \sin^{-1}x\text{d}x = \sin^{-1}x\sin x - \int-\sin^{-2}x\cos x\sin x\text{d}x = 1 + \int\tan^{-1}x\text{d}x 
$$ So, where have I made a mistake?","['integration', 'integration-by-parts']"
1787189,"Solving differential equation using Laplace transform, problem finding inverse","Given
$$y'' + 4y' + 5y =  H(t-3)e^{-2t}, t>0, y(0) = 1, y'(0)=2 $$
To solve this diff. equation using Laplace transform. Seems very straightforward. On one side, we have
$$\mathscr{L}\{y''+4y'+5y\} = \mathscr{L}\{y''\} + 4\mathscr{L}\{y'\} +5\mathscr{L}\{y\}=\\
	\left [s^2\mathscr{L}\{y\}-sy(0)-y'(0)\right ] + 4\left [s\mathscr{L}\{y\}-y(0)\right ] + 5\mathscr{L}\{y\} =\\
	\mathscr{L}\{y\}\left (s^2+4s+5\right ) -sy(0) -4y(0) -y'(0) = \mathscr{L}\{y\}\left (s^2+4s+5\right ) -s-6 $$
and on the other
$$\mathscr{L}\{H(t-3)e^{-2t}\} = \int_0^\infty e^{-(s+2)t}H(t-3)\mbox{d}t = \int_0^3 0\mbox{d}t + \int_3^\infty e^{-(s+2)t}\mbox{d}t = -\frac{1}{s+2}e^{-(s+2)t}\bigg\vert_3^\infty = \frac{e^{-3(s+2)}}{s+2} $$
Which ultimately yields:
$$\mathscr{L}\{y\} = \frac{s+6}{s^2+4s+5} +\frac{e^{-3(s+2)}}{(s+2)(s^2+4s+5)} $$
Since $\mathscr{L}$ is a linear operator, I take its inverse is also linear. Finding the inverse of the first summand is a piece of cake, however, how do we do the second one?
$$\mathscr{L}^{-1}\left\lbrace\frac{e^{-3(s+2)}}{(s+2)(s^2+4s+5)} \right\rbrace $$ Oh, missed a useful theorem:
$$\forall r>0: \mathscr{L}\{f(t-r)\} = e^{-sr}\mathscr{L}\{f(t)\} $$","['complex-analysis', 'ordinary-differential-equations', 'laplace-transform']"
1787194,Value of a trigonometric series [duplicate],"This question already has an answer here : Find the sum : $\frac{1}{\cos0^\circ\cos1^\circ}+\frac{1}{\cos1^\circ \cos2^\circ} +\frac{1}{\cos2^\circ \cos3^\circ}+......+$ (1 answer) Closed 8 years ago . Question: If $x = \sin 1^\circ$, find the value of the expression: 
  $$\frac{1}{\cos0^\circ \cos1^\circ} + \frac{1}{\cos1^\circ\cos2^\circ} + ... + \frac{1}{\cos44^\circ\cos45^\circ}$$
  in terms of $x$ I really can't see how I would simplify this expression in terms of $x$. Any hint would be appreciated.","['trigonometry', 'sequences-and-series']"
1787196,Non-negative determinant of a block matrix,"Here's the problem I've been stuck on for some time now. Let $A,B \in M_n(\mathbb{R})$. Let $C=
        \begin{bmatrix}
        A & B \\
        -B & A \\
        \end{bmatrix}
$ be a real $2n \times 2n$ matrix. Prove $\det(C) \geq 0$. What I've tried so far are: First I tried to write determinant of $C$ as the sum of $2^{2n}$ matrix determinants by expanding all rows of $C$ such that for each binary sequence of length $2n$ like $a = (a_1, a_2, ..., a_{2n})$, if $a_i = 0$ then the first $n$ entries of $i$-th row are zero and if $a_i = 1$ then the second $n$ entries of $i$-th row are zero. But couldn't come close to any answer. Second we know that determinant is the product of eigenvalues. The characteristic polynomial of $C$ has real coefficients hence its complex roots come in conjugate pairs and have positive product. What remains is to prove that each negative eigenvalue has even multiplicity which I couldn't prove. Any sort of hints and/or ideas are appreciated.","['matrices', 'determinant']"
1787213,How can I prove that interior product obeys a graded Leibniz rule?,"I want to prove that $i_{X}(\omega\wedge\phi)=i_{X}\omega\wedge\phi+(-1)^{k}\omega\wedge i_{X}\phi.$
I was thinking I many be able to adapt the proof that the exterior derivative obeys the graded Leibniz rule. Failing that I have no idea how to prove this.","['manifolds', 'differential-geometry']"
1787230,$f(x)=\int_0^x\sin(t^2-t+x)dt$. Find $f''(x)+f'(x)$,"$f(x)=\int_0^x\sin(t^2-t+x)dt$. Find $f''(x)+f'(x)$. Using leibnitz integral rule, $$f'(x)=\int_0^x\cos(t^2-t+x)+\sin(x^2)dt$$
$$f''(x)=-\int_0^x\sin(t^2-t+x)dt+2x\cos(x^2)$$ Answer given is $(2x+1)\cos(x^2)$. I wont get this if I add $f''(x)+f'(x)$","['derivatives', 'integration', 'calculus']"
1787266,$f(x)$ and $xf(x)\in L^2(\mathbb{R})$ then $f(x)\in L^1(\mathbb{R})$,"If $f(x)$ and $xf(x)\in L^2(\mathbb{R})$ then $f(x)\in L^1(\mathbb{R})$. I know that if $E$ is of finite measure, then we can infer from $f(x)\in L^2(E)$ to get $f(x)\in L^1(E)$. However, now $E=\mathbb{R}$, I don't know how to get the result now. How to apply $xf(x) \in L^2$?","['functional-analysis', 'real-analysis', 'lebesgue-integral', 'lebesgue-measure']"
1787277,Calculating the Confidence Interval,"It is found that in a random sample of 100 Science students, there are 48 studying statistics . To test whether the true proportion of students in statistics is 50% or not, suitable null and alternative hypotheses are:
(a) H0 :p=0.48;H1 :pÌ¸=0.48 (b) H0 :p=0.5;H1 :p>0.5 (c) H0 :p=0.5;H1 :p<0.5 (d) H0 :p=0.5;H1 :p=0.48 (e) H0 :p=0.5;H1 :pÌ¸=0.5 Find a 95% Confidence Interval for p in Q1. I have determined the answer to Q 1), which I believe is e).
For question 2), I have been told the CI= estimator +/- (table value)(Standard error). The estimator is 0.48, and I have found the SE to be 0.05. However, I am unsure about how to calculate the ""table value"". Thank you.","['statistics', 'confidence-interval']"
1787308,Determining the number of subgroups of $\Bbb Z_{14} \oplus \Bbb Z_{6}$,"I want to determine how many subgroups does the additive group $G:=\Bbb Z_{14} \oplus \Bbb Z_{6}$
  have? There are many related posts in our site, for instances: here and there .
However, it seems that those problems worked with small numbers.
I know that $|G|=84=7.2^2.3$ so if $H$ is a subgroup of $G$ then $|H|$ can be taken $12$
possible values as divisors of $84$. Listing all subgroups with given possible orders
seems to be not a good choice. Is there any other efficient method to deal with
this particular problem? Could it be easier for us if we write $G \simeq \Bbb Z_{2} \oplus \Bbb Z_{2} \oplus\Bbb Z_{3} \oplus \Bbb Z_{7}$?","['finite-groups', 'abstract-algebra', 'abelian-groups', 'group-theory']"
1787322,Solve $\frac{dx}{dt}=\frac{at-\cos{x}}{at^2\tan{x}+t}$,"Solve $\begin{align*}\frac{dx}{dt}=\frac{at-\cos{x}}{at^2\tan{x}+t}\end{align*}\\\\
$ Am I justified in doing the following substitution? If not, can a closed-form solution be found? Let $t=r\cos{x}$ and $dt=-r\sin{x}dx$ $\begin{align}
\frac{dx}{dt}&=\frac{(ar-1)\cos{x}}{ar^2\cos^2{x}\tan{x}+r\cos{x}}\\\\
\frac{dx}{dt}&=\frac{(ar-1)\cos{x}}{(ar\sin{x}+1)r\cos{x}}\\\\
dx&=-\frac{(ar-1)}{(ar\sin{x}+1)r}r\sin{x}dx\\\\
1&=-\frac{(ar-1)\sin{x}}{(ar\sin{x}+1)}\\\\
ar\sin{x}+1&=-(ar-1)\sin{x}\\\\
1&=(1-2ar)\sin{x}\end{align}$ Therefore, $\begin{align}
1&=-(1-2ar)\frac{dt}{rdx}\\\\
\int{rdx}&=\int(2ar-1){dt}\\\\
x&=(2a-\frac{1}{r})t+c
\end{align}$ EDIT: Here is some background on the above differential equation. The following equations describe the requirements of a curve $f$ which elastically reflects particles in a desired fashion, the details of which I will not cover. $\begin{equation}f(x,t)=t \tan{x}+\frac{a}{2}t^2 \sec^2{x}+h, \ \ \ \ \ 0<x<\pi \\\\
\frac{\partial f(x,t)}{\partial t}=\tan{\left(\frac{x}{2}-\frac{\pi}{4}\right)}
\end{equation}$ where $x$ is t-dependent. We can differentiate the first equation above with respect to t, taking care to evaluate the derivative of $x$, as well.
\begin{align*}\frac{\partial f\left(x, t\right)}{\partial t}&=\tan{x}+t\sec^2{x}\frac{dx}{dt}+at\sec^2{x}+at^2\tan{x}\sec^2{x}\frac{dx}{dt}\\\\
\tan{\left(\frac{x}{2}-\frac{\pi}{4}\right)}&=\tan{x}+\left[\left(t+a t^2\tan{x}\right)\frac{d x}{dt}+at\right]\sec^2{x}\\\\
\tan{x}-\sec{x}&=\tan{x}+\left[\left(t+a t^2\tan{x}\right)\frac{d x}{dt}+at\right]\sec^2{x}\\\\
\frac{dx}{dt}&=\frac{at-\cos{x}}{at^2\tan{x}+t}\\\\
\end{align*}","['substitution', 'integration', 'ordinary-differential-equations']"
1787343,Convergent Fourier series of continuous function,Let $f$ be a continuous function. It is known that its Fourier series is convergent almost everywhere to $f$ and it may fail to converge on some measure zero set. However I would like to know whether one can find a continuous function $f$ with the property that its Fourier series is convergent everywhere but not to $f$ (in other words for each $x$ the partial sums $S_N(x)$ converge to $S(x)$ and there are some points $x$ such that $S_N(x)$ converges to $S(x)\neq f(x)$).,"['fourier-series', 'fourier-analysis', 'analysis']"
1787353,"Set Interview Question, Any Creative Way to solve?","I ran into a simple question, but I need an expert help me more to understand more: The following is True: $ A - (C \cup B)= (A-B)-C$ $ C - (B \cup A)= (C-B)-A$ $ B - (A \cup C)= (B-C)-A$ and the following is False: $ A - (B \cup C)= (B-C)-A$ this is an interview question, but how we can check the these sentence as true or false quickly?","['elementary-set-theory', 'discrete-mathematics']"
1787381,"Example in Combination, is there any solution?!","Is there any idea to solve such a question? I have $40$ pens that includes $20$ white pens and $20$ black pens, I decide to distribute these pens among $4$ students that every student gets at least $2$ pens and at most $7$ pens from each color (thus, each student must receive at least $2$ black pens and $2$ white pens). if the number of possible ways to do this distribution be $n$,  the sum of two rightmost digits of $n$ is equal to $13$. How this value calculated? Note: the order is not important. Thus, switching the pens between two students does not give a different allocation.","['combinations', 'combinatorics', 'discrete-mathematics']"
1787389,Show that if $\operatorname{trace}(AB) = 0$ and $\operatorname{rank} (A)=1$ then $ABA=0$,I know that $$AB-BA=A \iff A \text{ is singular}$$ $A$ and $B$ can be complex. Any hints?,"['matrices', 'trace', 'matrix-rank']"
1787415,Probability of having at least $j$ collisions when tossing $m$ balls into $n$ bins,"Suppose that we throw $m$ balls into $n$ bins uniformly and independantly at random. We consider collisions as distinct unordered pairs, e.g., if 3 balls are tossed in one bin, we count 3 collisions. What is the probability of having at least $j$ collisions when we throw $m$ balls into $n$ bins, such that at least $j$ bins are empty at the end of the process? ##Case $m \leq n$ :
As we suppose $m < n$ , this question is in fact equivalent to computing the probability that we toss $m$ balls in $m-j$ bins. Indeed, we throw our $m-j$ first balls in distinct bins, and the $j$ remaining balls in these $m-j$ bins.
We can view tossing $m$ balls in $m-j$ as the following equation: $$x_1 + x_2 + \cdots + x_{m-j} = m$$ where $x_i$ is the number of balls in bin $i$ . The number of solutions in $\mathbb{N}$ of this equation is a $m$ -combination on a set of size $m-j$ , i.e., $\binom{m+m-j-1}{m} = \binom{2m - j - 1}{m}$ . The number of ways to choose $m-j$ bins out of $n$ is $\binom{n}{m-j}$ . The number of possibilities to throw $m$ balls in $n$ bins is $n^m$ , thus $$\mathbb{P}[\text{at least } j \text{ collisions}] = \frac{\binom{2m - j - 1}{m} \cdot \binom{n}{m-j}}{n^m}.$$ What do you think of this approach? Am I overcounting/undercounting something? ##Case $m > n$ :
In my opinion, this case is much harder to solve. As I count collisions as distinct unordered pairs, the total number of collisions is $$\sum_{i = 1}^n \binom{x_i}{2}$$ where $x_i$ is the number of balls in bin $i$ . Hence, to have exactly $j$ collisions, a necessary condition is to have $\sum_{i=1}^{n} \binom{x_i}{2} = j$ . Let's tweak this equation a bit. $$ \begin{align*}
\sum_{i=1}^{n} \binom{x_i}{2} = j & \iff \sum_{i=1}^n \frac{x_i!}{2 \cdot (x_i-2)!} = j\\
& \iff \sum_{i=1}^{n} \frac{x_i!}{(x_i-2)!} = 2j\\
& \iff \sum_{i=1}^n x_i(x_i-1) = 2j\\
& \iff \sum_{i=1}^n x_i^2 = 2j + \sum_{i=1}^n x_i\\
& \iff \sum_{i=1}^n x_i^2 = 2j + m
\end{align*}
$$ where the last equality follows from the fact the sum of the balls in each bins is the number of balls tossed. Therefore, this problem is equivalent to solving the following problem: how many representations as a sum of $n$ squares does a number have? This problem seems far from trivial to me, as I did not find easy explicit formulas to work with. However this approach was for finding exactly $j$ collisions, and I am interested in finding at least $j$ collisions. As $m > n$ , we will have some collisions. We can express $m = qn + r$ for some $q, r \in \mathbb{N}$ . The least collisions we will have is when the balls are uniformly distributed amongst the bins, thus we will have at least $$C_{min} = \sum_{i=1}^r \binom{q+1}{2} + \sum_{i=1}^{n-r} \binom{q}{2}$$ collisions, which forces $j > C_{min}$ . The problem thus becomes, in how many ways can we distribute $m$ balls into $n$ bins such that $\sum_{i=1}^n \binom{x_i}{2} \geq j$ . Here I am stuck, I can't find an easy way with combinatorics to solve this. I am fine with this problem being solved asymptotically, so if you have any ideas and/or references on the subject... :). Thanks for reading and helping #Edit
I think both cases above are wrong, because I was undercounting some things. $m \leq n$ "" /> This image shows that the probability of having $j$ collisions do not increase nor decrease. I was expecting something like a gaussian, or at least something with a low probabiliy of having a few collisions, a high probability of having an average number of collisions, and a low probability of having a lot of collisions. I thought about another way of counting, and I think the results I obtain are better. For that, I need the following Lemma, taken from this paper, page 16 . Lemma 1: The probability that exactly $k$ bins are not empty after throwing $m$ balls is $\frac{\binom{n}{k}k!S(m,k)}{n^m}$ , where $S(m,k)$ is a Stirling number of the second kind. With this Lemma, I obtain the following result. Lemma 2: Let $m$ be the number of balls thrown uniformly and independently ar random into $n$ bins. The probability of having at least $j>1$ collisions and at least $j$ empty bins is $$ \mathbb{P}[\text{at least } j \text{ collisions}] = \frac{1}{n^{m}} \sum_{i=1}^{n-j} \binom{n}{i} i! S(m,i). $$ Proof: As explained previously, the probability of having at least $j$ collisions and at least $j$ empty bins (or at most $n-j$ non-empty bins) is equal to the probability of tossing $m$ balls in at most $n-j$ bins. Let $A$ be the event ""at most $n-j$ bins are not empty"" and let $A_i$ be the event ""exactly $i$ bins are not empty"". Then $A = \cup_{i=1}^{n-j} A_j$ and $A_i \cap A_k = \emptyset$ if $i \neq k$ , hence $\mathbb{P}[A] = \sum_{i=1}^{n-j}\mathbb{P}[A_j]$ and Lemma 1 concludes the proof. $\square$ We can see on the next plot that this results already seems closer to reality (this was plotted for $n = 500$ , $m = 200$ and $j$ from $1$ to $1000$ ). Moreover, with this approach, we don't need to separate the cases $m \leq n$ and $m > n$ (at least I think, you might need to correct me on this). The downside is that now we have to work with a sum... and Stirling numbers of the second kind... Do you guys have any idea to remove the sum and obtain an upper bound of this?","['balls-in-bins', 'sums-of-squares', 'probability', 'combinatorics', 'solution-verification']"
1787421,"Is the universal quantification symbol $\forall\;$ known as ""for any x"" or ""for all x"" in First Order Logic & why is this different to Discrete Math?","I'm reading a book by Mark Tarver called Logic, Proof and Computation . Chapter 8 (starting p71) is about First Order Logic . On page 77 (of Chapter 8) the author writes: For any value for 'x', in '(taller x Elton_John)' the resulting proposition is true. In first order logic this is written ( $\forall x\;$ (taller x Elton_John)) $\forall \;$ is the universal quantifier . Now when I studied Discrete Maths at Uni 15 years ago - the universal quantifier $\forall\;$ meant for every x . When I look at functional programming (Lisp, Haskell etc) for all and any are two distinct concepts. In functional programming for all means (every? function elements) and any means (some? function elements) . That is the first applies the function to every element regardless of the result of the function, and the latter applies the function lazily, until it returns true. My question is: Is the universal quantification symbol $\forall\;$ known as ""for any x"" or ""for all x"" in first order logic, and why is this different to discrete math?","['first-order-logic', 'predicate-logic', 'quantifiers', 'terminology', 'discrete-mathematics']"
1787425,How to prove this continued fraction connection between $\gamma$ and $e$?,"There is apparently a curious connection between Euler-Mascheroni constant $\gamma$ and $e$ in the form of an infinite series and continued fraction: $$e \gamma=e \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n!~n}-\cfrac{1}{2-\cfrac{1}{4-\cfrac{4}{6-\cfrac{9}{8-\cfrac{16}{10-\cdots}}}}}$$ As can be seen, the partial denominators and numerators have the form $2n$ and $-n^2$ respectively. How can we prove this? Might it be a useful method to compute $\gamma$?","['special-functions', 'continued-fractions', 'sequences-and-series', 'euler-mascheroni-constant']"
1787459,First-order nonlinear ODE similar to Bernoulli DE,"I know that the Bernoulli equations, i.e. equations in the form $$ y' + p(x) y + q(x) y^{\alpha}=0$$ Can be easily solved with a change of variables. But what about equations in the form $$y'  + q(x) y^{\alpha} + p(x)=0$$ Is there any easy way to solve those too?",['ordinary-differential-equations']
1787466,"Modelling the ""Moving Sofa""","I believe that many of you know about the moving sofa problem; if not you can find the description of the problem here . In this question I am going to rotate the L shaped hall instead of moving a sofa around the corner. By rotating the hall $180^{\circ}$ what remains between the walls will give the shape of the sofa. Like this: The points on the hall have the following properties: \begin{eqnarray} A & = & \left( r\cos { \alpha  } ,t\sin { \alpha  }  \right)  \\ { A }' & = & \left( r\cos { \alpha  } +\sqrt { 2 } \cos { \left( \frac { \pi  }{ 4 } +\frac { \alpha  }{ 2 }  \right)  } ,t\sin { \alpha  } +\sqrt { 2 } \sin { \left( \frac { \pi  }{ 4 } +\frac { \alpha  }{ 2 }  \right)  }  \right)  \\ { B } & = & \left( r\cos { \alpha  } -\frac { t\sin { \alpha  }  }{ \tan { \left( \frac { \alpha  }{ 2 }  \right)  }  } ,0 \right)  \\ { B }' & = & \left( r\cos { \alpha  } -\frac { t\sin { \alpha  }  }{ \tan { \left( \frac { \alpha  }{ 2 }  \right)  }  } -\frac { 1 }{ \sin { \left( \frac { \alpha  }{ 2 }  \right)  }  } ,0 \right)  \\ C & = & \left( r\cos { \alpha  } +t\sin { \alpha  } \tan { \left( \frac { \alpha  }{ 2 }  \right)  } ,0 \right)  \\ { C }' & = & \left( r\cos { \alpha  } +t\sin { \alpha  } \tan { \left( \frac { \alpha  }{ 2 }  \right)  } +\frac { 1 }{ \cos { \left( \frac { \alpha  }{ 2 }  \right)  }  } ,0 \right)  \end{eqnarray} Attention: $\alpha$ is not the angle of $AOC$, it is some angle $ADC$ where $D$ changes location on $x$ axis for $r\neq t$. I am saying this because images can create confusion. Anyways I will change them as soon as possible. I could consider $r=f(\alpha)$ and $t=g(\alpha)$ but for this question I am going to take $r$ and $t$ as constants. If they were functions of $\alpha$ there would appear some interesting shapes. I experimented for different functions however the areas are more difficult to calculate, that's why I am not going to share. Maybe in the future. We rotate the hall for $r=t$ in the example above: In this case: point A moves on a semicircle The envelope of lines between A' and C' is a circular arc. One has to prove this but I assume that it is true for $r=t$. If my second assumption is correct the area of sofa is $A= 2r-\frac { \pi r^{ 2 } }{ 2 } +\frac { \pi  }{ 2 } $. The maximum area is reached when $r = 2/\pi$ and it's value is:
$$A = 2/\pi+\pi/2 = 2,207416099$$ which matches with Hammersley's sofa. The shape is also similar or same: Now I am going to increase $t$ with respect to $r$. For $r=2/\pi$ and $t=0.77$: Well, this looks like Gerver's sofa . I believe thearea can be maximized by finding the equations of envelopes above and below the sofa. Look at this question where @Aretino has computed the area below $ABC$. I don't know enough to find equations for envelopes. I am afraid that I will make mistakes. I considered to calculate area by counting number of pixels in it, but this is not a good idea because for optimizing the area I have to create many images. I will give a bounty of 200 for whom calculates the maximum area. As I said the most difficult part of the problem is to find equations of envelopes. @Aretino did it. PLUS: Could following be the longest sofa where $(r,t)=((\sqrt 5+1)/2,1)$ ? If you want to investigate further or use animation for educational purposes here is the Geogebra file: http://ggbm.at/vemEtGyj Ok, I had some free time and I count number of pixels in the sofa and I am sure that I have something bigger than Hammersley's constant. First, I made a simulation for Hammersley's sofa where $r=t=2/\pi$ and exported the image to png in 300 dpi (6484x3342 pixels) and using Gimp counted number of pixels which have exactly same value. For Hammersley I got $3039086$ pixels. For the second case $r=0.59$ and $t=0.66$ and I got $3052780$ pixels. To calculate area for this case: $$\frac{3052780}{3039086}(2/\pi + \pi/2)=2.217362628$$ which is slightly less than Gerver's constant which is $2.2195$. Here is the sofa:","['conic-sections', 'rotations', 'geometry']"
1787514,Does the Upper and Lower Derivative Definition Imply the Standard Derivative Definition?,"The upper and lower derivatives of a function f on the interior of a set $E$ are defined as follows: for $x\in E\strut^\mathrm{o}$, $$\overline Df(x)= \lim _{h\rightarrow0}\left[\sup_{0<|t|\leq h} \frac{f(x+t)-f(x)}{t} \right];$$ $$\underline Df(x)=\lim _{h\rightarrow0}\left[\inf_{0<|t|\leq h} \frac{f(x+t)-f(x)}{t} \right].$$ The function $f$ is differentiable in $x$ $\iff$ $\overline Df(x)=\underline Df(x)$. My question is that, does the definition above of differentiability imply the 'standard' definition of differentiability? That is, if $\overline Df(x)=\underline Df(x)$, does it imply that $$\lim_{h\rightarrow0}\frac{f(x+h)-f(x)}{h}?$$","['derivatives', 'real-analysis']"
1787537,Help with understanding the proof for: $AB$ and $BA$ have the same characteristic polynomial (for square complex matrices),"I saw many proofs but they all use advanced techniques and are impossible to understand.
I'm looking for a proof that $AB$ and $BA$ have the same characteristic polynomial for any square matrices $A$ and $B$ over $\mathbb C$ . It's really easy when dealing with invertible matrices, but hard to prove for singular matrices. I found several solutions that I could not understand: This solution says it is not too difficult to show that $AB$ , and $BA$ have the same characteristic polynomial ... If the matrices are in $M_n(\mathbb C)$ , you use the fact that $GL_n(\mathbb C)$ is dense in $M_n(\mathbb C)$ and the continuity of the function which maps a matrix to its characteristic polynomial . There are at least 5 other ways to proceed I've bolded every term that I am not familiar with. This solution I could not understand as well (it uses the limit definition when $\lambda$ approaches zero but I hardly understand how that solves the issue). I'm looking for a simpler solution using more basic linear algebra.","['matrices', 'characteristic-functions', 'determinant', 'continuity', 'linear-algebra']"
1787562,Singularity at $z=0$ for $1-\cos(z)\sin(\frac{1}{z})$,"Any ideas for solving this problem, mentioned in our last exam, is highly appreciated. What is the residue of $f(z)=(1-\cos z)\sin \frac{1}{z}$ at the isolated point $z=0$ ? Our notes say the answer is: $ - \sum_{n=2}^{\infty} \frac {1}{n!} \frac {1}{(n+1)!}  $ what is the step that reach to above solution?","['real-analysis', 'calculus', 'complex-analysis', 'contest-math', 'linear-algebra']"
1787601,How to prove the limit $\lim\limits_{n\to\infty}n\sin\frac{2\pi}{n}\cos\frac{1}{n}$ doesn't exist?,"I am at a loss...
how do I prove that the limit
$\lim_{n\to\infty}(n\sin\frac{2\pi}{n}\cos\frac{1}{n})$
doesn't exist?","['limits', 'trigonometry', 'sequences-and-series', 'calculus', 'convergence-divergence']"
1787603,"Real Analysis, Folland Problem 1.3.11 Measures","Background information - Let $X$ be a set well equipped with a $\sigma$-algebra $M$. A measure on $M$ (or on $(X,M)$ or on $X$ if $M$ is understood) is a function $\mu: M \rightarrow [0,\infty]$ such that i.) $\mu(\emptyset) = 0$ ii.) if $\{E_j\}_{1}^{\infty}$ is a sequence of disjoint sets in $M$ then $\mu(\cup_{1}^{\infty}E_j) = \sum_{1}^{\infty}\mu(E_j)$ Property (ii.) is called countable additivity and implies finite additivity: ii.*) if $E_1,\ldots, E_n$ are disjoint sets in $M$ then $\mu(\cup_{1}^{n}E_j) = \sum_{1}^{n}\mu(E_j)$ because one can take $E_j = \emptyset$ for $j >n$. A function $\mu$ that satisfies (i.) and (ii.*) but not (ii.) is called a finitely additive measure. A finitely additive measure $\mu$ is a measure if and only if it is continuous from below. If $\mu(X) < \infty$, $\mu$ is a measure if and only if it is continuous from above. The proof of the first part is provided in the answer here Attempted proof of second part - Let $\{E_j\}_{1}^{\infty}$ be a sequence of disjoint sets in $M$ and $\mu(X) < \infty$. Let $$F_k = \bigcap_{1}^{\infty}E_j$$ As you can see $F_1\supset F_2 \supset \ldots$, so the $F_k$'s are a decreasing sequence of sets and clearly $$\bigcap_{1}^{\infty}F_k = \bigcap_{1}^{\infty}E_j$$
Now from continuity from above $$\mu\left(\bigcap_{1}^{\infty}E_j\right) = \mu\left(\bigcap_{1}^{\infty}F_k\right) = \lim_{k\rightarrow \infty}\mu(F_k)$$
Now what I want to do is apply De'Morgans law and conclude that we will have continuity from above being the same as continuity from below but I am not sure if that I can actually do that. Any suggestions is greatly appreciated.","['real-analysis', 'measure-theory']"
1787644,(Differential Galois Theory) Where is the proof that the three-body-problem is unsolvable?,"I'm looking for a proof,
which shows that ""the 3-body-problem"" in physics 
is mathematically unsolvable. Does anyone know some URLs that contain a proof in mathematical detail? You know, in Astrodynamics, 
this problem is famous as an extension of Kepler problem 
or two-body-problem. It says that we can solve equations of 2 bodies,
such as the motions of earth and moon. But when it comes to 3 objects, 
we can NOT solve equations about these three.
Analytically, we can't get the solution. Why is that impossible? Related keywords are here: (1) In 3-body-problem, there's a lack of ""first-integral"" (conservatives). (2) This matter is related to system of ""integrable systems"". (3) This matter is also related to ""differential Galois Theory"".
because motions are described by differential equations. (When we want to prove that some equations are unsolvable in algebra,
Galois Theory is a useful tool.
And differential Galois Theory is higher concept.) (4) In some special conditions, 3-body-problem is solvable.
For example, they have solutions such that 3 objects are on one line,
or 3 objects draws a shape like ""8"". But it's just in a special conditions.
Generally, 3-body-problem is unsolvable.
I am talking about the general case. I can not find any specific proof about this problem, 
though it's very important well-known result of science... Where on the www can I see the mathematical proof?
Thanks in advance.","['galois-theory', 'physics', 'ordinary-differential-equations']"
1787649,Twin prime conjecture proof error,"I am absolutely sure this is wrong but I can't find why. For every integer $n$ there exist a finite number of primes less than $n$. Take the set containing those primes and multiply them together to get $x$. Aren't $x+1$ and $x-1$ prime, implying there is an infinite number of twin primes? Follow up question is there guaranteed to be a prime between n and $x^{.5}$? What about for large n? this prime wouldn't have to devide x just exist in the given range","['proof-verification', 'number-theory', 'twin-primes', 'prime-numbers', 'elementary-number-theory']"
1787666,How to tell if a system of polynomial equations has no real solutions,"I have a system of $3n + 3$ polynomial equations in $6n$ variables, where $n$ is probably going to be less than about $5$.  I can compute its Groebner basis and I see that it does not contain $\{1\}$, so I know that it has complex solutions at least.  However, I know for a fact that, by varying some of the parameters, I can cause it to have no real solutions.  Is there a way to check that the number of real solutions is nonzero? It doesn't have to be fast.  In fact, if I could turn the problem around and get a set of constraints on my parameters for which a real solution exists that would be great, but I'm not sure if that's even possible...","['real-algebraic-geometry', 'polynomials', 'algebraic-geometry']"
1787679,"Explicit computation of $H^2(\mathbb{F}_p^n, \mathbb{R}/\mathbb{Z})$.","I'm interested in the computation of the second cohomology group of the elementary abelian group $\mathbb{F}_p^n$ with coefficients in $\mathbb{R}/\mathbb{Z}$: $$H^2(\mathbb{F}_p^n, \mathbb{R}/\mathbb{Z}) \cong \mathbb{F}_p^{n(n-1)/2}$$ where for simplicity let's suppose $p$ is odd.  I'm pretty sure this is right. The following discussion of how the right hand side arises is cribbed from [ Tata Lectures on Theta III , Mumford, Nori and Norman]; any mistakes my own. The space $\mathbb{F}_p^{n(n-1)/2}$ on the right corresponds naturally to the space of alternating bilinear maps $\mathbb{F}_p^n \times \mathbb{F}_p^n \to \mathbb{R}/\mathbb{Z}$.  The link to $H^2$ comes from looking at the commutator map $G \times G \to \mathbb{R}/\mathbb{Z}$ in the corresponding central extension $$0 \to \mathbb{R}/\mathbb{Z} \to G \to \mathbb{F}_p^n \to 0$$ and noting it descends to an alternating bilinear map  $\mathbb{F}_p^n \times \mathbb{F}_p^n \to \mathbb{R}/\mathbb{Z}$ on the quotient.
If $\rho \colon \mathbb{F}_p^n \times \mathbb{F}_p^n \to \mathbb{R}/\mathbb{Z}$ is an explicit 2-cocycle, then this alternating form is just
$$\sigma(a,b) = \rho(a,b) - \rho(b,a) $$
and is readily seen to be invariant under adding coboundaries to $\rho$. So, this defines a homomorphism
$$H^2(\mathbb{F}_p^n, \mathbb{R}/\mathbb{Z}) \to \mathbb{F}_p^{n(n-1)/2}$$
and it's not hard to show it's surjective, i.e., to construct very explicit Heisenberg-group-type examples of group extensions or cocycles showing that every $\sigma$ arises in this way.  It is also clear that the kernel consists precisely of even cocycles (i.e., $\rho(a,b) = \rho(b,a)$) modulo coboundaries. My question is: Question: can someone give me an fairly explicit proof that this map is injective , and therefore that $H^2(\mathbb{F}_p^n, \mathbb{R}/\mathbb{Z})$ is what we think it is? At some point a proof must use the fact that $\mathbb{R}/\mathbb{Z}$ hasn't been replaced by, say, $\mathbb{F}_p$, as then cocycles corresponding to $\mathbb{Z}/p^2 \mathbb{Z}$ will be in the kernel. What I would love to have, in order of preference, is: an algorithm that takes any even cocycle on $\mathbb{F}_p^n$ and spits out an explicit function whose coboundary is $\rho$; any proof from which such an algorithm could in principle be recovered, even if doing so involves a lot of work diving into proofs of standard results; or any proof at all. As ever, any pointers or references greatly appreciated.","['finite-groups', 'group-cohomology', 'group-extensions', 'group-theory']"
1787683,"The relationship between random variables, distribution functions and probability measures","Given a probability space $(\Omega,\mathcal{F},P)$, and a random variable $X\colon\Omega\to\Bbb{R}$, we can associate with it its distribution function $F\colon \Bbb{R}\to[0,1]$ defined as
\begin{align*}
F(x)&=P(X\le x)=P(\{\omega\in\Omega:X(\omega)\le x \}).
\end{align*}
Further, we can define an induced probability measure $\mu\colon \mathcal{B}\to[0,1]$ by
\begin{align*}
\mu(A) &= P(X\in A)=P(\{\omega\in\Omega:X(\omega)\in A \}).
\end{align*} I read in this answer that $\mu$ and $F$ uniquely determine eachother. My question is: can we do this ""in reverse"", i.e. starting with a probability measure $\mu\colon \mathcal{B}\to[0,1]$, can we then get a unique random variable? On $(\Bbb{R},\mathcal{B},P)$, I tried to do this starting with $\mu(A)=P(A)$ for all $A\in\mathcal{B}$, where $P(A)=\mathrm{Leb}(A\cap[0,1])$. I believe this yields $X(x)=x$ for all $x\in\Bbb{R}$, and $F(x)=\min(1,\max(0,x))$, so it works in this simple case.","['probability-theory', 'measure-theory', 'random-variables', 'probability-distributions']"
1787723,Limit Brownian Bridge Integral,"As a solution of the Brownian Bridge SDE, we arrive at the solution
\begin{align}
X_t = (1-t) \int_0^t \frac{1}{1-s}\ dB_S
\end{align}
defined for $0 \leq t <1$. In order to show that for any $g \in C[0,1]$ 
\begin{align}
\lim_{t \uparrow 1}\ (1-t) \int_0^t \frac{g(s)}{(1-s)^2}\ ds = g(1),
\end{align}
I am considering two cases. Case 1: \begin{align} 
\lim_{t \uparrow 1}\ \int_0^t \frac{g(s)}{(1-s)^2}\ ds = \lim_{t \uparrow 1}\ \frac{1}{1-t} = \infty,
\end{align}
such that we apply l'HÃ´pital's rule and find that
\begin{align}
\lim_{t \uparrow 1}\ \frac{\frac{g(t)}{(1-t)^2}}{\frac{1}{(1-t)^2}} = g(1).
\end{align} Case 2: \begin{align} 
\lim_{t \uparrow 1}\ \int_0^t \frac{g(s)}{(1-s)^2}\ ds \neq \lim_{t \uparrow 1}\ \frac{1}{1-t} = \infty.
\end{align}
Now, clearly
\begin{align}
(*) = \lim_{t \uparrow 1}\ \frac{ \int_0^t \frac{g(s)}{(1-s)^2}\ ds }{\frac{1}{1-t}} \to 0 \qquad \text{since} \qquad \lim_{t \uparrow 1}\ \frac{1}{1-t} = \infty.
\end{align}
However, how to show that $(*) \to g(1)$ as well?","['stochastic-analysis', 'probability-theory', 'brownian-motion', 'limits']"
1787741,Conditional independence if and only factorization applies,"Murphy, in his Machine Learning: A Probabilistic Perspective , defines two conditionally independent random variables essentially as follows: $X$ and $Y$ are conditionally independent given $Z$, i.e., $X
 \perp Y \mid Z$, if and only if
$$p(X, Y \mid Z) = p(X \mid Z)p(Y \mid Z)\text{.}$$
where $p$ denotes the (conditional) pmfs/pdfs. (I'm not a fan of the notation, but let's put that aside for now.) One theorem in here is: $X \perp Y \mid Z$ iff there exist functions $g$ and $h$ such that
  $$p(x, y \mid z) = g(x, z)h(y,z)$$ for all $x, y, z$ such that $p(z) >
 0$. ""$\Longrightarrow$"" is easy to see from the definition. The difficulty with ""$\Longleftarrow$"" is that it's hard to show that $g$ and $h$ are unique, and $g(x,z) = p(X \mid Z)$ and $h(y, z) = p(Y \mid Z)$. At least, I think this is what I need to show. I've tried using
$$p(X, Y \mid Z) = \dfrac{p(X, Y, Z)}{p(Z)}$$
but this does not lend itself to clean factoring, as desired in the theorem.",['probability']
1787748,Given $f(z)=\dfrac{U(z)}{V(z)}=\dfrac{2z^3-3z^2+7z-8}{z^4-5z^3+4z^2-6z+1}$ find $f(1-\sqrt{2}i)$ without lots of complex arithmetic.,"Such a problem is usually done either by direct substitution (ugh!) or synthetic division. Synthetic division after several complex products and additions gives $U(1-\sqrt{2}i)=-8-3\sqrt{2}i$ After several more complex operations one finds, barring errors, that $V(1-\sqrt{2}i)=9+7\sqrt{2}i$ 
leaving the solution to the straightforward evaluation of \begin{equation}
	f(1-\sqrt{2}i)=\frac{-8-3\sqrt{2}i}{9+7\sqrt{2}i}
\end{equation} All standard stuff. The question is this: Can this be done without so many complex operations?","['algebra-precalculus', 'complex-numbers']"
1787769,On half open and half closed intervals.,Which interval is commonly referred to as half closed? Is there any problem if I refer to it as half open?,['elementary-set-theory']
1787793,The diameter of a specific 3-regular graph,"On my HW assignment we were asked to prove the following claim: Let $G=(V,E)$ be a $3$-regular graph, and $m$ a natural number so that
  $n=|V|\geq 3(2^m)-1$. Prove that the diameter of $G \geq m+1$. Can anyone can help with the solution or even with a way of approaching this prove?","['combinatorics', 'graph-theory']"
1787796,Are linear combinations of powers of a matrix unique?,"One can see from the Cayley-Hamilton Theorem that for a $n\times n$ matrix, we can write any power of the matrix as a linear combination of lesser powers and the identity matrix, say if $A\neq cI_n$, $c\in \Bbb{C}$ is a given matrix, it can be written as a linear combination of $I_n,A^{-1},A,A^2,\cdots,A^{n-1}$. Is this representation unique? If so, is it under special cases? As an example let's consider this: Let $A\neq cI_n$ , $c\in \Bbb{C}$ be a $3\times 3$ matrix over $\Bbb{C}$ and $A^3=k_3A^2+k_2A+k_1I_3=m_3A^2+m_2A+m_1I_3$ for $k_i,m_i\in \Bbb{C}$. Does it hold that $k_i=m_i \forall i=1,2,3?$ If we take a formalistic approach I guess we can say that the equivalence above consitutes a system of $3$ equations with $6$ variables ($k_i,m_i$). That is, we evaluate the two linear combinations and by matrix equivalence we demand that all entries are equal: $a_{ij}=b_{ij}=c_{ij}$. But in general, unless the matrix $A$ has a ""special"" form, that will have infinite solutions. Is this-overly simplistic-approach correct? Should I try to find counter examples perhaps or the question has some trivial answer that I overlook?","['matrices', 'linear-algebra']"
1787811,find the largest number,"I have a question regarding this problem that is to find the largest number. Just by looking at the problem, I know the answer should be (d), but how can I prove that (d) is larger than (c) in a simple way? Which of the following is the largest number: $$\begin{align}
  &\textrm{(a)}\quad 3.14^3   
  &\quad\textrm{(b)}\quad 3^{3.14} 
  &\quad\textrm{(c)}\quad 3.14^{3.14} 
  &\quad\textrm{(d)}\quad (1/3)^{-4} 
\end{align}$$","['algebra-precalculus', 'arithmetic']"
1787850,"If $a$ and $b$ are roots of $x^4+x^3-1=0$, $ab$ is a root of $x^6+x^4+x^3-x^2-1=0$.","I have to prove that: If $a$ and $b$ are two roots of $x^4+x^3-1=0$, then $ab$ is a root of $x^6+x^4+x^3-x^2-1=0$. I tried this : $a$ and $b$ are root of $x^4+x^3-1=0$ means : $\begin{cases}
a^4+a^3-1=0\\
b^4+b^3-1=0
\end{cases}$ which gives us : $(ab)^4+(ab)^3=a^3+b^3+a^4+b^4+a^4b^3-a^3b^4-1$ can you help me carry on ? or propose another solution ? thanks in advance","['real-analysis', 'roots', 'real-numbers', 'calculus', 'functions']"
1787867,Prove $\frac{2\cos x}{\cos 2x + 1 }= \sec x$,"Prove that $\dfrac{2\cos x}{\cos 2x + 1 }= \sec x$. So far I have:
$\dfrac{2\cos x}{\cos 2x + 1 }= \dfrac 1 {\cos x}$ Where do I go from here?","['trigonometry', 'functions']"
1787971,Can we find a regular ($C^k$) parametrization for this surface?,"I have here a surface whose curvature properties I want to study, represented in cylindrical coordinates:
$$f(r,\theta) = r^2\cos4\theta$$
The problem, however, is that the parametrization is not differentiable (indeed, not defined) at exactly the point I want to look at - the origin! I have a plot here made by Mathematica: Trying to use the usual conversion to rectangular coordinates doesn't work either - at best, I can get $f(x,y) = (x^2 + y^2)\cos\cos^{-1}\left(\frac{x}{\sqrt{x^2+y^2}}\right)$, which is also not differentiable at the origin. Now, I'm not sure the surface itself is actually $C^\infty$, but I'm pretty sure it's at least $C^2$. How can we find a parametrization that's differentiable at the origin?","['multivariable-calculus', 'parametrization', 'differential-geometry', 'surfaces']"
1788000,"How to show that if $X$ is Hausdorff and $ \big\{ (x, y) : x \sim y \big\} \subseteq X \times X$ is closed then $Y$ is Hausdorff?","Let $\sim$ be an equivalence relation on a topological space $X$, and let $Y = X/\sim$ be equipped with the quotient topology.
How to show that if $X$ is Hausdorff and the set $\big\{ (x, y) : x \sim y \big\} \subseteq X \times X$ is closed, then $Y$ is Hausdorff. My main question would be that is this true without the assumption that the quotient map is open.","['general-topology', 'quotient-spaces']"
1788080,"Existence and uniqueness of SDE, is the independence requirement needed?","In Bernt Ã˜ksendals Stochastic differential equations he has this theorem in chapter 5: $\\\\\\$ However, in the proof I can not see where he uses the independence condition I marked in red. Do you know if this theorem also holds if we do not assume independence? The proof is long, so I might have missed where he uses it. So do you know if the theorem holds without this? If you are familiar with the proof, do you see where he uses this condition? $\\\\\\$ reference: http://th.if.uj.edu.pl/~gudowska/dydaktyka/Oksendal.pdf chapter 5.2","['stochastic-processes', 'probability-theory', 'stochastic-calculus', 'stochastic-analysis']"
1788082,How many examples exist of Lie groups that are 2-dimesional surfaces?,"It is relatively easy to show that $\mathbb{R}^2$ or $\mathbb{T}^2$ are 2-dimensional surfaces with a structure of Lie groups. I can not find other surface which are also a Lie group, there are more examples? Many thanks!","['examples-counterexamples', 'smooth-manifolds', 'differential-geometry', 'surfaces', 'lie-groups']"
1788113,Matrix with a certain pattern,"Consider the following matrix: $$\left[\begin{array}{cccc} 1+x_1y_1 & 1+x_1y_2 & \ldots & 1+x_1y_n  \\
1+x_2y_1 & 1+x_2y_2 & \ldots & 1+x_2y_n \\
1+x_3y_1 & 1+x_3y_2 & \ldots & 1+x_3 y_n \\
\vdots & & & \\
1+x_ny_1 & 1+x_ny_2 & \ldots & 1+x_n y_n \\
\end{array}  \right]$$
with $x_k, y_k$ being real numbers. I suppose that there must be some neat way of computing the determinant of this matrix. Those ones are annoying to me.","['matrices', 'determinant']"
1788140,Matrix consisting of cosines of differences,"Consider the following matrix: $$\left[\begin{array}{cccc} \cos(x_1-y_1) & \cos(x_1-y_2) & \ldots & \cos(x_1-y_n)  \\
\cos(x_2-y_1) & \cos(x_2-y_2) & \ldots & \cos(x_2-y_n) \\
\cos(x_3-y_1) & \cos(x_3-y_2) & \ldots & \cos(x_3-y_n) \\
\vdots & \vdots & \ddots & \vdots\\
\cos(x_n-y_1) & \cos(x_n-y_2) & \ldots & \cos(x_n-y_n) \\
\end{array}  \right]$$
with $x_k, y_k$ being real numbers. I cannot really apply any kind of functional calculus, I believe, to get a closed form formula for the determinant of this matrix. Any hints or ideas how to proceed?","['matrices', 'determinant']"
1788191,sum of a function series,what would be the first step to determine sum of $\sum\limits_{n=1}^{\infty}ne^{-n^2/4x}.$ I think I should try putting $y=e^{-1/4x}$. Then $y$ changes from $0$ to $1$ and I get $\sum\limits_{n=1}^{\infty}ny^{n^2}$ but I still don't know what to do with this.. Could you give me a hint?,"['sequences-and-series', 'functions']"
1788196,"The smallest number $m$, such that $m\uparrow \uparrow (n+1)>n\uparrow\uparrow n$","A natural number $n\ge 3$ is given. Denote $a\uparrow\uparrow b$ to be a power tower of $b$ $a's$ . Let $m$ be the smallest natural number , such that $m\uparrow\uparrow(n+1) > n\uparrow\uparrow n$ How can I efficiently determine $m$ , in particular if $n$ is very large. Is there a formula $m(n)$ giving $m$ directly ? Since $n\uparrow \uparrow (n+1)>n\uparrow\uparrow n$ , we have $m\le n$ . It seems that $m\le n-1$ holds for $n\ge 4$ .","['number-theory', 'big-numbers', 'tetration', 'power-towers']"
1788204,Iterative trapezoidal method for differential equations,"I am studying numerical methods for differential equations. I came accros the trapezoidal method in two forms, an explicit and an iterative one. I would like to know the advantages and disadvantages of each of those methods. Furthermore, how can I study the stability for the iterative method? Which stability definition is the better one and why?. I explain both methods below. Consider an initial value problem given by $y' = f(t, y)$ and $y(a) = t_0$, where $f$ is defined in $[a, b]\times[\alpha, \beta]$ and satisfies the Lipschitz property with Lipschitz's constant $L$. Given a natural number $n$ and $h = \frac{b-a}{n}$, we are trying to approximate the unique solution of the problem at $t_i = a + ih \ \forall i = 0, 1 \ldots n$. If $y$ is the solution, we call $y_i = y(t_i)$ and we denote $w_i$ to the approximations obtained by the applied method. One of the methods studied is the explicit trapezoidal method. It follows the following rule: $w_{i+1} = w_i + \frac{h}{2} \left[f(t_i,w_i) + f(t_{i}+h, w_i + h f(t_i,w_i))\right]$ We have proved that it has a local error of order 3 and, hence, a global error of order 2. Then, reading some books I came accros the iterative trapezoidal method, which solves the following implicit equation: $ w_{i}= w_{i-1} + \frac{h}{2} \left[f(t_{i-1}, w_{i-1}) + f(t_i, w_{i})\right] $ The idea is taking an initial approximation $w_i^{(0)}$ and defining the following sequence: $w_{i} ^{(j+1)} = w_{i-1} + \frac{h}{2} \left[f(t_{i-1}, w_{i-1}) + f(t_i, w_{i}^{(j)})\right]$ The limit of that sequence is taken as $w_i$. I have proved that the sequence converges if $hL/2 < 1$ and that if we use $w_i$, then the local error is $O(h^3)$. However, why is this method useful and how can I study its stability?","['numerical-methods', 'ordinary-differential-equations']"
1788208,Showing that $\mathbb{R}$ is locally isometric to $S^1$,"Show that $f:\mathbb{R}\to S^1$ given by $f(t)=e^{i t}$ is a local isometry between Riemanninan manifolds. So, basically we need to show that for each $p\in\mathbb{R}$ there exists $U\subseteq\mathbb{R}$ open with $p\in U$ such that $f:U\to f(U)$ is an isometry, i.e., $\left<{u,v}\right>_q=\left<{df_q(u),df_q(v)}\right>_{f(q)}$ for every $q\in U$ and $u,v\in T_q(U)$. I assume we can just take $T_q(U)=\mathbb{R}$ for every $q$. But I don't really know how to start. Would anyone give me a hint? How can we compute the differential $df_p$? Thank you.","['riemannian-geometry', 'differential-geometry']"
1788235,What are the essential tools and proof techniques for beginning smooth manifolds and differential topology?,"I am an undergraduate currently taking a first course in smooth manifolds. I feel that I understand the material intuitively. But, I'm having trouble turning my intuition into proofs. I was hoping that I could get answers to the two following questions: What are the essential basic tactics for attacking smooth manifold and differential topology problems? What are the best things to do to improve my technique and ability to solve problems? Let me give an example of what I mean. I just asked another question about showing a particular map is a submersion. I have an intuitive understanding of what is going on. In the context of the problem, I can see what is happening in the case $\pi: SO_3 \to S^2$. The idea is that if I look at a point on a sphere, and I look at the directions coming off of that point, there is a rotation of the sphere that moves it in that direction. For instance, if I'm looking at a point on the equator, there is a rotation that moves the point northward, and a rotation that moves it westward, and that's enough to show I can move the point in any direction I like. But, turning that into a proof is a little bit of a daunting task. So daunting, in fact, I was compelled to ask about it. When I asked my professor about this problem, he drew me a picture and explained the intuition. I think intuition is important, but what I'm having trouble with it turning my intuition into a proof. The reason I'm having trouble with that is I don't exactly know what the essential proof techniques are. So, I firstly want to know what these proof techniques are, and secondly want to know what I can do to be acquainted with their use. I feel I should clarify what I'm saying somewhat. When I say tools here, I mean something along the lines of ""use Sard's theorem"" or ""partitions of unity."" For example, I would say that using the regular level set theorem, instead of constructing charts, to show that some set is a manifold is a tool. I understand that showing something is s submersion requires showing its differential is surjective, that being the definition of the term. What I mean is, what are good ways to exploit the structure I have? What are the proof techniques unique to this structure?","['differential-topology', 'problem-solving', 'smooth-manifolds', 'manifolds', 'differential-geometry']"
1788247,Why does the homotopy lifting property imply that fibers are homotopy equivalent if the base is path connected?,"Suppose that $\pi:E \to B$ has the homotopy lifting property, so that for any space Y with a map $f:Y \to E$ and a homotopy $G$ of $g = \pi \circ f$, we have a homotopy $F: Y \times I \to E$ that lifts $G$. Every book I read claims that if $B$ is path connected, then $\pi^{-1}(b_0)$ is homotopy equivalent to $\pi^{-1}(b_1)$ for any $b_0,b_1 \in B$. I am aware that, in proving this claim, we are supposed to consider $Y = \pi^{-1}(b_0)$, with $f: \pi^{-1}(b_0) \hookrightarrow E$ being inclusion, and with $G: Y \times I \to B$ such that $G(y,t) = \gamma(t)$ for some path $\gamma$ from $b_0$ to $b_1$. It is then supposed to be easy to show that $\pi^{-1}(b_0) \times \{1\} \to \pi^{-1}(b_1)$ is a homotopy equivalence, but I do not see why this is true. Why is this true? In particular, why does $\pi:D^2 \to I$ with $\pi(re^{i\theta}) = r$ not have the homotopy lifting property? I cannot seem to find any homotopy that cannot be lifted.","['algebraic-topology', 'general-topology', 'homotopy-theory']"
1788249,How do you compute a complex exterior derivative?,"The context is deriving cauchy riemann equations using green's/stoke's theorem. The function is the complex function $f(x,y)=u(x,y)+iv(x,y)$ with associated one form $u(x,y)dx+iv(x,y)dy$. Here is my work so far: $$d(u(x,y)dx+iv(x,y)dy)=(\frac{\partial u(x,y)}{\partial x}dx
  +\frac{\partial u(x,y)}{\partial y}dy)\wedge dx+i(\frac{\partial v(x,y)}{\partial x}dx
  +\frac{\partial v(x,y)}{\partial y}dy)\wedge dy=(-\frac{\partial u(x,y)}{\partial y}+
  i\frac{\partial v(x,y)}{\partial x})dx\wedge dy$$ Is this correct? Should there be an $i$ coefficient on the differential $dy$ since it is in the purely imaginary direction? I haven't been able to find many resources on complex exterior differentiation, so any help is appreciated.","['complex-analysis', 'differential-geometry', 'complex-integration', 'differential-forms']"
1788270,How to calculate this integral when the function is unknown?,"Let $f :[0,1]\to \mathbb{R}$ with $f'(x) = \sqrt{1+f^2(x)}$ for all $x \in [0,1]$. If $f(0) + f(1) = 0,$ calculate the integral $$I=\int_{0}^{1}f(x)dx$$ Any help would be appreciated. Thanks.","['calculus', 'functions']"
1788275,Can the directional derivative fail to be linear?,"Is it possible for the directional derivative for a function $f$ in the direction of a vector $v$ , $D_vf(x) = \lim_{h \to 0} \frac{f(x + hv) - f(x)}h$ to exist for every vector $v$ , and yet $v \mapsto D_vf(x)$ fails to be linear? Here we have a function which seems to be displaying that very phenomenon: $$f(x,y) = \frac{32x^3}{x^2 + y^2} - \frac{16x^5}{(x^2 + y^2)^2} - 14x$$ As you can see, if you follow the map at the origin in the direction of any vector, it appears to follow a straight line, i.e. the derivative in that direction is constant in the direction of $v$ . Yet the ""slope"" in each direction is clearly not behaving in a linear manner. Is this really the case? Or is it just a misleading graph? I'm inclined to believe the former, but what I'm looking for is a rigorous argument. Further, under what conditions might a derivative fail to be linear? Or, what are necessary conditions to ensure that a proof that ""the directional derivative is linear"" actually works?","['multivariable-calculus', 'real-analysis']"
1788288,Stating the domain of a function using set notation,"I'm trying to write the domain of $f(x+y) = \frac{1}{1+ x + y}$ given $f(x) = \frac{1}{1+ x}$ using set notation. I'm thinking first we'd have something like $Domain[ f(x) ] = \mathbb{R} \setminus \{ -1 \}$ because division by zero is not defined. If $f(x+y) = \frac{1}{1+ x + y}$, then $1 + x + y \neq 0$ by the same reasoning. $1 + x + y \neq 0 \Leftrightarrow x + y \neq -1$ So, the domain of $f(x+y) = \frac{1}{1+ x + y}$ is the set of all x and y such that x and y are both real, x does not equal -1 and $x + y \neq -1$ Next, trying to separate x and y for the sake of writing the domain cleanly using set notation, we have: $x + y \neq -1 \Leftrightarrow x \neq -( y + 1 )$ and $x + y \neq -1 \Leftrightarrow y \neq -( x + 1 )$ Trying to write that using set notation, I would write something like this: $$
\begin{align}
x &= \mathbb{R} \setminus \{ -1, -( y + 1 ) \} \\
y &= \mathbb{R} \setminus \{\ \  -( x + 1 ) \} \\
Domain[ f(x + y) ] &= x \cup y
\end{align}
$$ Is the above statement correct? Do we need different variables for the set of all values of x in the domain of f? A different variable for the set of all values of y in the domain of f?","['elementary-set-theory', 'functions']"
1788294,Prove that if $X_2$ has a uniform distribution then $X_1 \oplus_2 X_2$ too,"Assume we have two independent random variables $X_1$, $X_2$ with values in the set $Z_2 = {0,1}$. Prove that if $X_2$ has a uniform distribution then $X_1 \oplus_2 X_2$ has also the uniform distribution. (Used in ""coin tossing by phone""). I know from a theorem that this is true and that the uniform probability distribution will be $P(y=0)=1/2$ and $P(y=0)=1/2$. But I don't know how to prove it. Any idea? Thanks for your help.","['number-theory', 'statistics', 'cryptography']"
1788306,Radius of convergence from recurrence with variable coefficients,"I am solving via power series the ivp
$$y'-2xy=0,\quad y(1)=2.$$
The ""solution"" is
$$y(x)=2\left(1+2(x-1)+3(x-1)^2+\frac{10}{3}(x-1)^3+\frac{19}{6}(x-1)^4+\frac{26}{10}(x-1)^5+\cdots\right)$$
with coefficients generated by the recurrence:
$$(n+1)a_{n+1}-2a_{n-1}-2a_n=0,\quad n>0$$
or,
$$a_{n+1}=\frac{2(a_n+a_{n-1})}{n+1}.$$
Due to the variable coefficient I'm not sure how to determine the radius of convergence from the recurrence.  My naive attempt was to use
$$\frac{a_{n+1}}{a_n}=\frac{2\left(1+\frac{a_{n-1}}{a_n}\right)}{n+1}$$
and assume that
$$\lim_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|=R,\quad 0<R<\infty$$
to obtain a contradiction indicating that the radius is either 0 or infinite, but that is not very helpful.  The generating function methods don't seem particularly helpful, but I'm quite likely wrong about that. I know that the solution (valid for all $x$) is $$y(x)=2\mathrm{e}^{x^2-1}.$$ Question: How would one obtain the radius of convergence of this series from the recurrence.  As an aside are there any general methods that might indicate that the radius is greater than zero?","['recurrence-relations', 'ordinary-differential-equations', 'power-series']"
1788328,Find the Numerical Value of $\sin 10^\circ \sin 50^\circ \sin 70^\circ$.,"Prerequisite This problem is found in ""Trigonometry"" by I. M. Gelfand [in English] . It is asked in the section ""Double the angle"". So, assume that I know the sin/cos angle additions [i.e.: $\sin(A + B) = \sin A \cos B + \cos A \sin B$ , etc.] as well as everything learned prior. I've checked other sources and they say to use Morrie's Law, however, I have not learned it in the book. Problem Find the numerical value of $\sin 10^\circ \sin 50^\circ \sin 70^\circ$ . Hint: If the value of the given expression is $M$ , find $M \cos 10^\circ$ .",['trigonometry']
1788345,$p$-groups have normal subgroups of each order [duplicate],"This question already has an answer here : A $p$-group of order $p^n$ has a normal subgroup of order $p^k$ for each $0\le k \le n$ (1 answer) Closed 8 years ago . Suppose $|G|=p^n$. Then $G$ has a normal subgroup of order $p^m$ for every $0\le m\le n$. By induction. It is clearly true for $n=0$. Now suppose $k<n$ and $H_i$ is a normal subgroup of $G$ of order $p^i$ for $i=0,...,k$. I read in another thread that if we consider $Z(G/H_k)\neq 1$ and the Cauchy's theorem, this will provide us a group of order $p^{k+1}$. I don't see why yet. By that theorem there exists $gH_k\in Z(G/H_k)$ with order $p$. How can we construct a group of order $p^{k+1}$ from here? Thank you.","['finite-groups', 'abstract-algebra', 'group-theory']"
1788368,How does one parameterize $x^2 + xy + y^2 = \frac{1}{2}$?,"Parameterize the curve $C$ that intersects the surface
  $x^2+y^2+z^2=1$ and the plane $x+y+z=0$. I have this replacing equations: $$ x^2+y^2+(-x-y)^2=1$$ and clearing have the following: $$ x^2+xy+y^2=1/2$$ which it is the equation of an ellipse but I find it difficult parameterization values Any advice will be of much help, thanks in advance","['multivariable-calculus', 'parametric', 'integration', 'line-integrals']"
1788370,Is it possible to study the properties of sequences by studying the family of polynomials generated with the elements as coefficients?,"Suppose there is an integer sequence $\{a_0,a_1...a_n...\}$ and a family of polynomials is defined as follows: $p_0 = a_0$ $p_1 = a_0x+a_1$ $p_2 = a_0x^2+a_1x+a_2$ $p_n = a_0x^n+a_1x^{n-1}+...+a_n$ ... Where the coefficients are the elements of the sequence. The characteristics of $p_n$ are interesting because $p_n(0)=a_n$ and $p_n(1)=\sum_{n}a_n$ For instance, applied to the MÃ¶ebius function: $p_0 = 1$ $p_1 = x-1$ $p_2 = x^2-x-1$ $p_3 = x^3-x^2-x$ $p_4 = x^4-x^3-x^2-1$ ... $p_n(0)=\mu(n)$ ( $n^{th}$ element of the MÃ¶bius sequence) and $p_n(1)=\sum_{n}\mu(n)$ is the partial summation up to $n$ of the MÃ¶bius function, in other words, it is the Merten's function . This is the graph of the first $100$ polynomials, $p_0$ to $p_{99}$ And this is a zoom of the segment [0,1] so it is possible to see the ramifications of each polynomial from the position of the last $\mu(n)$ to the position of the last $\sum_n\mu(n)$ . The shape of the paths is quite curious because it is a representation of the surjective-only diagram of $\mu(n) \to \sum_n \mu(n)$ for each $n$ in $[0,100]$ . I would like to ask the following questions: Is it possible to know something about the properties (e.g. the convergence of the accumulated sum) of a sequence as the $lim_{n \to \infty}p_n(1)=lim_{n \to \infty}\sum_{n}a_n$ by the calculation of the shape of the generic polynomial generated with the elements of the sequence as coefficients? e.g. finding the the shape of the ""limit"" polynomial $p_n$ when $n \to \infty$ ? I tried to find some papers about this kind of approach, to understand if it leads to something or it is just visually interesting. Are there any papers regarding the generation of polynomials by using the elements of sequences as coefficients? Thank you!","['polynomials', 'reference-request', 'convergence-divergence', 'mobius-function', 'sequences-and-series']"
1788395,For every set $X$ show that $\emptyset+ X=\emptyset X = \emptyset$,"For every set $X$ show that $\emptyset+ X=\emptyset X=\emptyset$. For example, let  $X$={1,2}. Then, $\emptyset+ X=\emptyset X=\emptyset$. My question is HOW? I did not understand this example.",['elementary-set-theory']
1788404,Trying to prove Cantor-Bernstein-SchrÃ¶der following these steps,"I know a proof for this theorem is a recurrent issue but I've checked wikipedia's proof and several posts in this forum about it and even if I found some similarities I couldn' solve my problem. Let $X$ be a set with an injection $h:X \to X$ and $Y$ a subset such that $h(X) \subset Y \subset X$. The goal is to prove there's a bijection between $Y$ and $X$ following these steps: $(1)$ Prove there exists a minimal subset $Z \subset X$ such that $(X-Y)\subset Z$ and $h(Z)\subset Z$ $(2)$ Prove that $h(Z)=Z\cap Y$ $(3)$ Using the injective function $h$ and the subset $Z\subset X$ defined in $(1)$ construct a bijective function $h':X\to Y$ $(4)$ Prove, using the steps above, that if $A$ and $B$ are sets such that there exist two injections $f:A\to B$ and $g:B\to A$, then there is a bijection between those sets. . This is what I've done: $(1)$ Let $I=\lbrace V\subset X | (X-Y) \subset V \land h(V) \subset V \rbrace$ and define  $Z=\bigcap_{V\in I} V$ It can be proven that $Z$ satisfies all the conditions required (I omit the details but if someone asks me to I will write them) and in addition $Z$ is not empty. $(2)$ Here is where I'm stuck. It is clear that $h(Z)\subset Z$ by definition and because of $h(X)\subset Y$ then $h(Z)\subset Y$,so one of the inclusion is obvious but I can't prove the other one. I've tryed assuming there is some $x\in Z\cap Y$ which is not in $h(Z)$ but I didn't find any contradiction under this assumption (which is what I'm needing) Perhaps reductio ad absurdum is not the right way to get it, any advise on this point would be greatly appreciated. $(3)$ Let $h':X\to Y$ be defined as follows: $$h'(x)=h(x) \ \mbox{if $x \in Z$} \\ h'(x)=x \ \mbox{if $x \not\in Z$}$$ Again I omit the details but it can be proven that $h'$ is a bijection. $(4)$ Finally, here i tryed to ""emulate"" what I've done in the steps before. So, if $f:A\to B$ and $g:B\to A$ are injections and I let $I= \lbrace V \subset A| (A-g(B)) \subset V \land f(V) \subset g^{-1}(V) \rbrace$, then I define $Z=\bigcap_{V\in I}V$ but I'm not sure if what I'm doing is useful, it seems to me that I should prove that $Z\in I$ and $f(Z)=g^{-1}(Z)$ and then define some function $t:A\to B$ (hopefully bijective). I tryed defining it this way: $$t(x)=f(x) \ \mbox{if $x \in Z$} \\ t(x)=g^{-1}(x) \ \mbox{if $x \not\in Z$}$$ But I don't feel completly confident with it, I think something is not working with this function I've defined. . Any guiding idea, advice or attempts to solve would be very helpful. Thanks","['elementary-set-theory', 'proof-verification']"
1788425,The minimum value of $\frac{a^3 + b^3 + c^3 }{\sqrt{a^4 + b^4 + c^4 }}$ . When $a^2 + b^2 + c^2 = 1 $,"Asume $a, b, c $ is non-negative real.
I got above equation at this situation ; $\vec {x}= (a, b, c)$ , 
$\vec {y} = (a^2 , b^2 , c^2 ) $ $$ cos \phi = \frac{ \vec x \cdot \vec {y}} { \Vert {\vec {x} \Vert} \times\Vert {\vec {y} \Vert}} = \frac{ a^3 + b^3 + c^3 } { \sqrt{ a^4 + b^4 + c^4 }}$$
In this case I want to know the Maximum value of $\phi$ .
But I can't compute. 
I want your help.","['inequality', 'linear-algebra', 'geometry']"
1788426,Showing $\displaystyle\lim_{z\rightarrow ^{-}1}\displaystyle\sum_{n\geq 0}z^{2^n}$ does not exist,"I have been trying to bound this below, as the TA suggested, by some taylor series of a function I know diverges at $x=1$, like $\log(\frac{1}{1-x})$ taylor expanded around zero: $$-\log(1-x)=x+\frac{x^2}{2}+\frac{x^3}{3}+\frac{x^4}{4}+...$$ Which obviously doesn't work for $x<1$. Any tips? I am also curious to see some different ways of proving this.","['sequences-and-series', 'calculus', 'limits']"
1788439,Inclusionâ€“Exclusion Identical Computers Problem,"Find the number of ways to distribute 19 identical computers to four schools, if School A must get at least three, School B must get
  at least two and at most five, School C get at most four, and School D gets the rest. a) Solve using inclusion-exclusion b) Solve using generating functions I've been tackling this question for a couple of days and I am pretty confused where to even start. So I bet the answer is in the form $x_1 + x_2 + x_3 + x_4 = 19$,
where $x_1 \ge 3; 2 \le x_2 \le 5; x_3 \le 4$ and $x_4$ is whatever is left over. And then I get confused. I've looked all over the internet, through textbooks and I'm not getting anywhere. Any help would be appreciated, thank you.","['generating-functions', 'inclusion-exclusion', 'discrete-mathematics']"
1788448,There is no 'nice' complex logarithm,"Two problems are meant to establish that no 'nice' logarithm function exists for complex numbers. The first is Let $U$ be an open set in $\mathfrak{C}\setminus\{0\}$. Suppose $h:U \to \mathfrak{C}$ is a continuous function, such that $$e^{h(z)}=z \ \ \ \text{for every} \ z\in U$$ Show that $h$ is holomorphic by computing the defining formula for the derivative. $$lim_{w \to z} \frac{h(w)-h(z)}{w-z}$$ While the second is to prove no such function can exist on $\mathfrak{C}\setminus\{0\}$. My attempt so far has been to write $h(z)=f(z)+ig(z)$  which then gives $f(z)=\ln|z|$ and $g(z)=\arg(z)$ which contradicts the assumption of continuity, proving the second part. This does not prove the first part, which is that if such an $h$ did exist then $h$ is holomorphic","['complex-analysis', 'complex-numbers', 'analysis']"
1788468,Homotopy PoincarÃ© conjecture - no map inducing the isomorphism on homology,"$\newcommand{\Z}{\mathbb{Z}}$
In Terence Tao's notes on page 18, concerning the PoincarÃ© conjecture , he gave the following sketchy proof of the homotopy PoincarÃ© conjecture. Given $M^3$ a 3-manifold with $\pi_1(M^3)=0$, then automatically $0=H_1(X,\Z)=H^2(X,\Z)=H_2(X,\Z)$.  Moreover the action of $\pi_1(M_3) \curvearrowright H^{n-1}(S^{n-1},\Z)$ where $S^{n-1} \hookrightarrow Sp(T(M^3)) \to M^3$ is the fiber of the sphere bundle over the basepoint of $M$, is trivial since $\pi_1(M^3)=0$, so that $M^3$ is orientable. Therefore $H^3(M^3,\Z)=\Z$ and $H^i(M^3,\Z)=0$ for $i>3$ because $M^3$ is a 3 dimensional manifold (easy argument from excision). Terence Tao then says that it follows from the Whitehead theorem that $M^3 \to S^3$ is a homotopy equivalence. It does not follow since there is no map inducing the isomorphisms on homology between $S^3$ and $M^3$. Is there a Postnikov decomposition trick to show that we don't need the map to induce the isomorphism or is there an obvious map that I am missing?","['algebraic-topology', 'differential-geometry', 'homotopy-theory']"
1788490,Does integration wrt to a differential form always come from a measure?,"More precisely, is there an $n$-manifold $M$ with an $n$-form $\omega$ such that there is no measure $\nu$ on $M$ satisfying $$\int f \omega = \int f d\mu $$ for all compactly supported smooth functions $f$? EDIT: More generally, assuming the answer to the above is yes, what if we have a $k$-form  $\omega$ ($k\leqslant n$)? Then for every oriented $k$-submanifold $S$ we have the functional $$f\mapsto \int _S f \omega,$$ so for every such $S$ there is a measure such that this is integration wrt it. But is there a single measure for all such $S$? I guess you'd have to say something about the orientation here...","['differential-forms', 'integration', 'measure-theory']"
1788505,"Show that $(L^{p},\|\|_{p})$ is a Banach space.","Show that $(L^{p},\|\|_{p})$ is a Banach space. My approach: I prove the statement for $(L^{1},\|\|_{1})$ , of the following way, first all, is easy show that $\|\|_{1}$ is a norm. So, $(L^{1},\|\|_{1})$ is vector space. To show that is a Banach space, note that Prop. 1: Let $(f_{n})_{n\in\mathbb{N}}\subset\mathcal{L}^{1}$ (where $\mathcal{L}^{1}=\mathcal{L}^{1}(X,\tau,\mu)$ is the space of all integrals function), such that $\sum_{i}{\|f_{n}\|}_{1}<\infty$ , then the sequences $\left(\sum_{n=1}^{N}{f_{n}}\right)_{n\in\mathbb{N}}$ converges almost everywhere to the integral function, that we called $\sum_{n\in\mathbb{N}}{f_{n}}$ . Furthermore $$\sum_{n\in\mathbb{N}}{\int{f_{n}d\mu}}=\int{\sum_{n\in\mathbb{N}}{f_{n}d\mu}}\quad\lim_{N\to\infty}{\|\sum_{n=1}^{N}{f_{n}}-\sum_{n\in\mathbb{N}}{f_{n}}\|_{1}}=0$$ Prop. 2: Let $(E,\|\|)$ a normed vector space. Then $E$ is a Banach space if and only if for all $(e_{k})_{k\in\mathbb{N}}\subset E$ such that $\sum_{k\in\mathbb{N}}{\|e_{k}\|}<\infty$ , $\left(\sum_{k=1}^{n}{e_{k}}\right)_{n\in\mathbb{N}}$ converges in $E$ We take the proposition 2. Let $(f_{n})_{n\in\mathbb{N}}\subset L^{1}$ such that $\sum_{n}{\|\hat{f}_{n}\|_{1}}<\infty$ . Let $f_{n}\in\hat{f}_{n}$ , then $\|f_{n}\|_{1}=\|\hat{f}_{n}\|_{1}$ and then $\sum_{n}{\|f_{n}\|_{1}}<\infty$ , by prop. 1, there exist $F\in\mathcal{L}^{1}$ such that $\|\sum_{n=1}^{N}{f_{n}-F\|_{1}}\to 0$ , and then $\|\sum_{n=1}^{N}{\hat{f}_{n}-F\|_{1}}\to 0$ . Finally note that $\hat{F}\in L^{1}$ is a Banach space. This was my answer for $L^{1}$ space, but how I prove the general statement for $L^{p}$ (I want to do a similar response). Edit: I'm stuck in the problem, any idea or hint is appreciated.Thanks!!","['real-analysis', 'proof-verification', 'functional-analysis', 'lp-spaces', 'measure-theory']"
1788515,Can Stochastic Integration be Further Generalized?,"Is the idea of stochastic integration to accept convergence towards the stochastic integrals in probability instead of almost surely (pathwise)? I.e. to accept a weaker form of convergence for the Riemann sums in exchange for having a wider range of integrators? (Since desired integrators, like Brownian motion, are almost surely of unbounded variation on any interval, convergence a.s. of the Riemann sums is impossible.) Can the range of integrators be expanded even further beyond semimartingales if we ask only for convergence in distribution instead of convergence in probability? (Since using Ito isometry we actually show convergence of the Riemann sums in L2, which implies their convergence in probability, I believe.)","['probability-theory', 'stochastic-calculus', 'soft-question', 'stochastic-integrals']"
1788534,Suppose $E[X_1] <\infty$. Show that $\lim_{n\rightarrow \infty} \frac{X_n}{S_n}=0$ a.s.,"Let $X_1,X_2,X_3,...$ be i.i.d. with $P(X_1 >0)=1$. Define $S_n =\Sigma_{i=1}^{n} X_i$. (a) Suppose $\mathbb{E}[X_1] <\infty$. Show that $\lim_{n\rightarrow \infty} \frac{X_n}{S_n}=0$ a.s. I think here $\frac{X_n}{S_n}=\frac{n}{S_n}\times \frac{X_n}{n}$ and $\frac{X_n}{n}\rightarrow 0$ because here $X_n$ is a $L^{1}$ function. I don't think it's rigorous, or may be even wrong. (b) Constrct an example in which $\mathbb{E}[X_1] =\infty$ and $\lim_{n\rightarrow \infty} \frac{X_n}{S_n}=0$ a.s is false. No idea about the second part.","['law-of-large-numbers', 'probability-theory', 'probability', 'probability-distributions']"
1788542,Summation of a trigonometric series - $\frac{\sin n}{n}$,"Question: Find the sum of the series:
$$\lim_{n \to \infty}\frac{\sin1}{1}+\frac{\sin2}{2}+\frac{\sin3}{3}+...+\frac{\sin n}{n}$$ I have no clue how to find this. Obviously I can see the sum will be convergent as the denominator gets increasingly bigger while the numerator is bound between $1$ and $-1$.","['trigonometry', 'sequences-and-series']"
1788550,Is 4.99999......... exactly equal to 5? [duplicate],"This question already has answers here : Is it true that $0.999999999\ldots=1$? (31 answers) Closed 8 years ago . I'm a student of 10th std. Recently our teacher asked a Question that ""Is 4.999...equal to 5 or not?""
Everyone said that is isn't equal or it is approximately equal. Teacher too agreed to that. But I did't agree. I opposed the teacher as I think it is precisely equal to 5. I also prove but then too she isn't satisfied. Can you please explain what is right and what is wrong in this case?Please Justify. Proof that I showed : let x=4.999...  (a)
                           therefore, 10x = 49.999...
                                      10x -x = 49.999... -4.999...
                                      9x     =  45
                                      x = 45%9
                                      X= 5     (b)
                         hence (a) =(b).
                                    And therefore, ***4.999... = 5***",['functions']
1788568,$\frac{d}{dt}$ on $\mathbb{R}$ is not a Fredholm operator?,"I encountered a statement in a book that $\frac{d}{dt} : L^2_1(\mathbb{R}) \longrightarrow L^2 (\mathbb{R})$ is not a Fredholm operator, where $L^2_1$ is the first Sobolev space of the $L^2$ space. Consider a sequence of function $g_n$ such that $g_n \equiv 1$ on $[-n,n]$, increasing on $[-n-1,-n]$ and decreasing on $ [n,n+1]$, and vanishes outside of $[-n-1,n+1]$. We have $\operatorname{lim}_{n\longrightarrow \infty}\limits \lVert g_n \rVert_{L^2_1} = \infty$ and $\lVert \frac{d}{dt} g_n \rVert_{L^2} \leq C$ for some constant $C$. Then the author says if $\frac{d}{dt} $ is Fredholm, then the sequence will imply a non-trivial kernel which is a contradiction. My question is how he knows the implication of non-trivial kernel? Thank you edit: The book I'm reading is ""Floer homology groups in Yang-Mills theory"" by Donaldson and this problem is on page 58.","['functional-analysis', 'real-analysis']"
1788606,"What does ""identically distributed"" mean? [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question When two distributions have the same variance and shape, do we call them identically distributed, regardless of their mean (as this is usually a location parameter)?","['definition', 'probability-theory', 'random-variables', 'probability-distributions']"
1788618,How to find normal subgroups from a character table?,I know that normal subgroups are the union of some conjugacy classes Conjugacy classes are represented by the the columns in a matrix How could we use character values in the table to determine normal subgroups?,"['abstract-algebra', 'normal-subgroups', 'representation-theory', 'characters']"
1788711,Local defining functions on real hypersurfaces.,"I'm currently reading Real Submanifolds in Complex Space and their Mappings by Baouedi, Ebenfelt and Rothschild. I'm currently stumped by what the author's claim to be an easy check (really blowing my confidence): on page 4, he says that if $\rho$ and $\rho'$ are two defining functions for the real hypersurface $M$ near $p_0$, then there is a non vanishing real-valued smooth function $a$ defined in a neighbourhood of $p_0$ such that $\rho = a \rho'$ near $p_0$. I don't even know where to start to verify this seemingly easy fact. Any hint is much appreciated!","['manifolds', 'complex-manifolds', 'differential-geometry', 'complex-geometry']"
1788715,What is the moment of inertia of a Gosper island?,"We know that regular hexagons can tile the plane but not in a self-similar fashion. However we can construct a fractal known as a Gosper island , that has the same area as the hexagon but has the property that when surrounded by 6 identical copies produces a similar shape, but with dimensions scaled by a factor of $\sqrt{7}$. What is the distance between two of the centers? Is it the same as the distance between hexagons of the same area? ie. If I start with a hexagon of area A, then construct a Gosper island and place it next to an identical copy, would the distance still be the same as if they were hexagons? Or does the scaling factor come into play somewhere? Right now I think the answer is $\sqrt{3}/2$, as for the hexagon. The reason I ask is that I'm trying to calculate the Gosper island's moment of inertia through an axis through its centre of mass and perpendicular to the plane of the island. If we assume that the moment of inertia is always proportional to the mass, and proportional to the square of a characteristic length scale, then 
$$
I = \gamma Ml^2,
$$
where $\gamma$ is a constant, $l$ is the 'diameter' of the island, in a hexagon this would be the distance between two opposite vertices. Shrink the Gosper island by the scaling factor and surround it by six others. This self-similarity technique is super cute, and can be used to calculate the moment of inertia of an equilateral triangle, and can be extended to a square/rectangle quite easily. Fractals, having a high degree of self-similarity, seem amenable to this technique - here I calculate the moment of inertia for a Koch snowflake. $\hspace{1.3cm}$ Using the principle of superposition,
$$
I = I_{\text{centre}} + 6I_{\text{edge}},
$$
where
$$
I_{\text{centre}} =\gamma \frac{M}{7}\left(\frac{l}{\sqrt{7}}\right)^2 = \gamma \frac{Ml^2}{49} = \frac{I}{49}.
$$ Now, by the parallel axis theorem $\displaystyle I_{\text{edge}} = I_{\text{COM}} + Md^2$ where 
$$
\displaystyle I_{\text{COM}} = \frac{I}{49}
$$
and $\displaystyle d= \frac{\sqrt{3} l}{2}  $ (this was one source of error), so
$\displaystyle I_{\text{edge}} = \frac{I}{49} + \frac{3Ml^2}{4},$ and
\begin{align*}
I &= \frac{I}{49} + 6\left(\frac{I}{49}+ \frac{3Ml^2}{4}\right), \\
I & = \frac{I}{7} + \frac{9Ml^2}{2}, \\
\frac{6I}{7} & = \frac{9Ml^2}{2}, \\
I & = \frac{21Ml^2}{4}.
\end{align*} This seems incorrect? It feels wrong, comparing to a disk of radius $l/2$ which has moment of inertia $Ml^2/4$ it seems far too large. It would also be nice if we could verify our answer numerically or otherwise. Any references are also appreciated.","['physics', 'fractals', 'geometry']"
1788830,"Real Analysis, Folland Problem 1.3.15 Measures","Given a measure $\mu$ on $(X,M)$ , define $\mu_0$ on $M$ by $$\mu_0(E) = \sup\{\mu(F): F\subset E \ \text{and} \ \mu(F) < \infty\}$$ a.) $\mu_0$ is a semifinite measure. It is called the semifinite part of $\mu$ . b.) If $\mu$ is semifinite, then $\mu = \mu_0$ (Use Exercise 14) c.) There is a measure $\nu$ on $M$ (in general, not unique) which assumes only the values of $0$ and $\infty$ such that $\mu = \mu_0 + \nu$ . The proof of Exercise 14 can be found here For a.) I believe we need to show that $\mu_0(E) = \infty$ . Perhaps we can do this by proof of contradiction, similarly to what the proof in the link above does (actually it seems identical). For b.) I am not sure how we can show $\mu = \mu_0$ I think maybe we have to show by construction that $\mu \leq \mu_0$ and $\mu \geq \mu_0$ For c.) I haven't any idea for. Any suggestions is greatly appreciated.","['real-analysis', 'measure-theory']"
1788853,Solution to General Linear SDE,"In order to find a solution for the general linear SDE
\begin{align}
dX_t = \big( a(t) X_t + b(t) \big) dt + \big( g(t) X_t + h(t) \big) dB_t,
\end{align}
I assume that $a(t), b(t), g(t)$ and  $h(t)$ are given deterministic Borel functions on $\mathbb{R}_+$ that are bounded on each compact time interval. To find a suitable integrating factor $Z_t$,
\begin{align}
dX_t - \big( a(t) dt - g(t) dB_t \big) X_t =  b(t) dt + h(t)  dB_t
\end{align}
brings me to
\begin{align}
Z_t = e^{-\int_0^t ( a(s) - \frac{1}{2}g^2(s) )ds - \int_0^t g(s) dB_s}.
\end{align}
So,
\begin{align}
d(Z_tX_t) &=  Z_t \big( b(t) dt + h(t)  dB_t \big) \\
X_t &= Z_t^{-1} \big(X_0 + \int_0^t  Z_s b(s) ds + \int_0^t Z_s h(s)  dB_s \big).
\end{align}
To find an explicit solution for the SDE, I am considering
\begin{align}
X_t &= Z_t^{-1} Y_t \\
dX_t &= d(Z_t^{-1} Y_t) \\
&= Z_t^{-1} dY_t + Y_t dZ_t^{-1} + d[Z_t^{-1}, Y_t].
\end{align}
Where $dZ_t^{-1} = Z_t^{-1} \big( a(t) dt + g(t) dB_t \big)$ and $dY_t = Z_t \big( b(t) dt + h(t) dB_t \big)$. My question is, how to find $d[Z_t^{-1}, Y_t]$ and how to work towards the explicit solution for the SDE?","['stochastic-analysis', 'probability-theory', 'stochastic-calculus', 'stochastic-differential-equations']"
1788896,"Are $X'\otimes Y$ and $\mathfrak L(X,Y)$ isomorphic?","Let $\mathbb F\in\left\{\mathbb C,\mathbb R\right\}$ $X$ and $Y$ be normed $\mathbb F$-vector spaces $X'$ be the topological dual space of $X$ $\mathfrak L(X,Y)$ be the set of bounded, linear operators from $X$ to $Y$ $\mathfrak B(X\times Y)$ be the set of bilinear forms on $X\times Y$ Let $$(x\otimes y)(A):=A(x,y)\;\;\;\text{for }A\in\mathfrak B(X\times Y)$$ for $(x,y)\in X\times Y$ and $$X\otimes Y:=\operatorname{span}\left\{x\otimes y:(x,y)\in X\times Y\right\}\;.$$ Can we show that there is an isomorphism $\iota$ between $X'\otimes Y$ and $\mathfrak L(X,Y)$? It's clear that any $u\in X'\otimes Y$ with $$u=\sum_{i=1}^n\varphi_i\otimes y_i$$ for some $n\in\mathbb N$ and $(\varphi_i,y_i)\in X'\times Y$ yields some $L\in\mathfrak L(X,Y)$ via $$L:=\varphi_iy_i\;.$$ However, is the mapping $X'\otimes Y\to\mathfrak L(X,Y)$ which maps $u$ to $L$ injective? The other inclusion is not clear to me at all. However, the question arose as I saw people identifying $X'\otimes Y$ with $\mathfrak L(X,Y)$.","['functional-analysis', 'tensor-products', 'operator-theory']"
1788909,Limit of $\sqrt{\frac{\pi}{1-x}}-\sum\limits_{k=1}^\infty\frac{x^k}{\sqrt{k}}$ when $x\to 1^-$?,"I am trying to understand if $$\sqrt{\frac{2\pi}{1-x}}-\sum\limits_{k=1}^\infty\frac{x^k}{\sqrt{k}}$$ is convergent for $x\to 1^-$. Any help? Update: Given the insightful comments below, it is clear it is not converging, hence the actual question now is to find $$ \lim_{x\to 1^-}\left(\sqrt{\frac{\pi}{1-x}}-\sum_{n\geq 1}\frac{x^n}{\sqrt{n}}\right)$$","['real-analysis', 'riemann-zeta', 'asymptotics', 'sequences-and-series']"
1788911,Intuition behind speciality of symmetric matrices,"What is the geometric intuition behind the fact that only matrices that are similar to a symmetric matrix are diagonizable? So e.g. why is it important that the multiplier of the the first component of the last basis vector be the same as the multiplier of the the last component of the first basis vector(i.e. that in an $n*n $ matrix $(n,1)$ be the same as $(1,n)$)?",['linear-algebra']
