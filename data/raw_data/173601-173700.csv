question_id,title,body,tags
3093675,Prime Integer Topology is $T_2$ but not $T_3$,"According to this $\pi$ -Base page , the ""Prime Integer Topology"" is an example of a topological space which is $T_2$ but not $T_3$ . The space is defined as $(\mathbb{Z}^+,\tau)$ where $\tau$ is the topology generated by a basis consisting of sets of the form: $$U_p(b)=\{b+np:n\in\mathbb{Z}\}$$ Where $p$ is prime and $b$ is a positive integer. I have managed to prove this is indeed a $T_2$ space since given positive integers $y>x$ , if we denote $d=x-y$ , and we denote by $p$ the smallest integer greater than $d$ , then $U_p(x)$ and $U_p(y)$ are disjoint open neighborhoods. However, I was unable to prove this space isn't a $T_3$ space. I managed to prove that the set of prime numbers together with $1$ is closed in this topology, but it didn't help me, and I'm not sure that's the right direction. In addition, my number theory background is somewhat limited so I may not have the right tools to solve this problem. Any hints or suggestions would be appreciated.","['general-topology', 'prime-numbers', 'separation-axioms']"
3093710,"The kernel of a morphism of quasi-coherent sheaves on a scheme $(X,\mathcal{O}_X)$ is quasi-coherent.","Let $(X,\mathcal{O}_X)$ be a scheme. I know that an $\mathcal{O}_X$ -module $\mathcal{F}$ is quasi-coherent if for each $x \in X$ there exists an open neighborhood $U$ of $x$ and an exact sequence of $\mathcal{O}_X$ -modules $ \left. \mathcal{O}_X^{(J)} \right\vert_{U} \to  \left. \mathcal{O}_X^{(I)} \right\vert_{U} \to \mathcal{F} \to 0$ . How can i use that definition to prove that given a morphism $\alpha \colon \mathcal{F} \to \mathcal{G}$ of quasi-coherent $\mathcal{O}_X$ -modules, then $\mathrm{Ker} \mbox{ } \alpha$ is a quasi-coherent $\mathcal{O}_X$ -module?","['quasicoherent-sheaves', 'algebraic-geometry', 'schemes', 'sheaf-theory']"
3093712,If $k^{\log_{2}5}=16$ then find $k^{{(\log_{2}5})^2} = $?,"$$k^{\log_{2}5}=16$$ then find $$k^{{(\log_{2}5})^2}$$ Note: the exponent(entire log) is squared unlike the value
Inside the log squared.","['algebra-precalculus', 'logarithms']"
3093718,Orthogonal matrix with single $0$ entry,"To my surprise there is an orthogonal matrix dimension $3 \times 3$ with a single $0$ entry as  it was shown in this   answer . Moreover it was possible to identify the pattern for matrix entries to satisfy this condition i.e. $$Q=\begin{bmatrix} 0 & -a & -b \\  a   &  -b^2 &  ab \\ b & ab & -a^2\end{bmatrix}$$ when $a,b$ are constrained by $a^2+b^2=1$ . For orthogonal matrix dimension $2 \times 2$ single $0$ entry is not possible. Here my new questions: Is it possible a single $0$ entry for orthogonal matrix dimension $4 
   \times 4$ ? Can we  also provide for this case some more general formula? (to be more specific in a similar form as it was shown above for 3d) Is the method listed above the only one for orthogonal matrices dimension $3 
   \times 3$ or can we generate such matrix also with some other substantially different algorithm?","['matrices', 'orthogonal-matrices', 'linear-algebra']"
3093732,Find cardinality of $B = \left\{ f : \mathbb N \rightarrow \mathbb N \mid \forall n. f(n)\le n) \wedge \forall m\; \exists n( f(n) > m) \right\}$,"Find cardinality of $$B = \left\{ f : \mathbb N \rightarrow \mathbb N \mid \forall n(f(n)\le n) \wedge \forall m\; \exists n (f(n) > m)  \right\}$$ My try I have solved this, but I am not sure if it is correct (I have not a lot of experience in set theory). Can somebody check that or give some tips (or both)? $|B| < \mathfrak{c}$ because $|B| < |\mathbb N|^{|\mathbb N|}$ from the other hand I can define $G$ injective such as: $$G: (\mathbb N \rightarrow \left\{0,1 \right\}) \rightarrow B $$ $$ G(\alpha)(n) = \begin{cases}
\alpha(n) + G(\alpha)(n-1), &\text{if }n \neq 0 \\
0, &\text{if }n = 0.
\end{cases} $$ The function $G$ increases and is injective, and its power is $\mathfrak{c} $ so $|B| = \mathfrak{c}$","['elementary-set-theory', 'functions']"
3093744,How can I claim a one-sided limit doesn't exist?,"I have to find the limit $$\lim_{x\to 0^+} \frac{\ln(1+2x)\sin x}{\sqrt {x^3}} $$ Now, I tried using L'Hôpital's rule, but it doesn't lead anywhere. Manually trying to convert the functions to another form doesn't seem to go anywhere either, so I determined that the limit must be undefined. However, I cannot prove it. What can I do?","['logarithms', 'proof-explanation', 'limits-without-lhopital', 'real-analysis', 'limits']"
3093762,Quadratic first integrals,"I started reading chapter II.16 of Solving Ordinary Differential Equations I in order to understand nummerical methods more. There, they state that symplectic methods don't always conserve the first integrals of the system. So for example, if a system is described by some Hamiltonian $H(p,q)$ then the nummerics don't need to conserve it. What is conserved (Theorem 16.7 of said book or Sanz-Serna 1988) is the quadratic first integral that is also a first integral of the system. Now, a quadratic first integral is a function of the shape $y^T C y$ for some symmetric $C$ and $y:=(p,q)$ . Now, for this to be the first integral we would need to have $H(y) : = y^T C y$ right? But then, the conservation of $H$ for most numerical cases wouldn't be possible! Take for example the Lennard-Jones potential. There we have: $$H(p, r) \approx \sum_ip_i^2 + \sum_{i,j} \frac{A}{r_{ij}^{12}}-\frac{B}{r_{ij}^6}$$ and we can't conserve this Hamiltonian because $y^TCy$ can only contain terms with $r$ to the power of $0,1,2$ and never negative. What is going on here? Is it only possible to (nummericly) conserve Hamiltonians of the shape: $$
H(p,q) = \sum_{i,j} a_{ij} p_i p_j + b_{ij}p_{i}q_{j} + c_{ij} q_{i}q_j
$$ with the additional constraint that the constants $a,b,c$ need to be symmetric (so $a_{ij} = a_{ji}, b_{ij} = b_{ji}, c_{ij} = c_{ji}$ )? My specific application is to show that: $$H(p,q) = \sum_i |p_i| - \sum_{ij}\frac{1}{r_{ij}}$$ doesn't need to be preserved by leapfrog algorithm and can become unstable (see related question ) What conservation laws can be derived here? What C can I find that will be conserved?","['mathematical-physics', 'numerical-methods', 'symplectic-linear-algebra', 'ordinary-differential-equations']"
3093824,Existence of conditional expectations onto masas.,"Given an inclusion $N\subset M$ of von Neumann algebras, a conditional expectation is a map $E:M\to N$ that is a projection ( $E^2=E$ ) and it has $\|E\|=1$ . This automatically implies that $E$ is completely positive, and that $$\tag1
E(axb)=aE(x)b,\ \ \ a,b\in N,\ x\in M. 
$$ Conversely, a completely positive idempotent satisfies $\|E\|=1$ . In the particular case where $N=A$ is a masa (maximal abelian subalgebra), is it true that a conditional expectation exists for any masa $A\subset M$ ? Is it always normal?","['von-neumann-algebras', 'functional-analysis', 'operator-algebras']"
3093838,Under what condition are $U$ and $V$ uncorrelated?,"Let $X$ and $Y$ be independent random variables with finite variances, and let $U = X + Y$ and $V = XY$ . Under what condition are $U$ and $V$ uncorrelated? MY ATTEMPT We say that two variables are uncorrelated if their covariance equals zero. Bearing this in mind, we have \begin{align*}
\textbf{Cov}(U,V) & = \textbf{E}(UV) - \textbf{E}(U)\textbf{E}(V)\\\\
& = \textbf{E}(X^{2}Y + XY^{2}) - \textbf{E}(X+Y)\textbf{E}(XY)\\\\
& = \textbf{E}(X^{2})\textbf{E}(Y) + \textbf{E}(X)\textbf{
E}(Y^{2}) - \textbf{E}(X)^{2}\textbf{E}(Y) - \textbf{E}(X)\textbf{E}(Y)^{2}\\\\
& = \textbf{E}(Y)[\textbf{E}(X^{2}) - \textbf{E}(X)^{2}] + \textbf{E}(X)[\textbf{E}(Y^{2}) - \textbf{E}(Y)^{2}]\\\\
& = \textbf{E}(Y)\textbf{Var}(X) + \textbf{E}(X)\textbf{Var}(Y) = 0
\end{align*} Is there a specific name for this last expression? Any contribution is appreciated. Thanks :)","['independence', 'covariance', 'probability-theory', 'probability']"
3093897,Can't understand step in LU decomposition proof,"UPDATE : The author of the linked article has updated the proof and now it's perfectly clear. I'm reading about the LU decomposition on this page and cannot understand one of the final steps in the proof to the following: Let $A$ be a $K\times K$ matrix. Then, there exists a permutation matrix $P$ such that $PA$ has an LU decomposition: $$PA=LU$$ where $L$ is a lower triangular $K\times K$ matrix and $U$ is an upper triangular $K\times K$ matrix. I'll reproduce the proof here: Through Gaussian elimination, $A$ can be reduced to a row-echelon (hence upper triangular) matrix $U$ via a series of $n$ elementary operations: $$E_n\bullet\ldots\bullet E_1\bullet A = U$$ Any elementary matrix $E_i$ used in Gaussian elimination is either a permutation matrix $P_i$ or a matrix $L_i$ used to add a multiple of one row to a row below it. Thus, $L_i$ will be lower triangular with non-zero diagonal $\implies$ invertible. Suppose that the first permutation matrix we encounter is in position $j$ , so that we have: $$E_n\bullet\ldots\bullet E_{j+1}\bullet P_j\bullet L_{j-1}\bullet\ldots\bullet L_1\bullet A = U$$ Since a permutation matrix is orthogonal, $P_j^T P_j = I$ and hence $$E_n\bullet\ldots\bullet E_{j+1}\bullet P_j\bullet L_{j-1}\bullet P_j^TP_j \bullet\ldots\bullet P_j^TP_j\bullet L_1\bullet P_j^TP_j\bullet A = U$$ or $$E_n\bullet\ldots\bullet E_{j+1}\bullet\tilde{L}_{j-1}\bullet\ldots\bullet\tilde{L}_1\bullet P_j\bullet A = U$$ for $i=1,\ldots,j-1$ . A matrix $L_i$ , used to add $\alpha$ times the $i$ -th row to the $s$ -th row (in this case $s>i$ ), can be written as a rank one update to the identity matrix: $L_i=I+M$ , where $M$ is a matrix whose entries are all zeros except $M_{si} = \alpha$ . We have that $$\tilde{L}_i = P_jL_iP_j^T = P_j(I+M)P_j^T = P_jP_j^T + P_jMP_j^T = I+P_jMP_j^T$$ So far so good. Then we have: The permutations performed on the rows and columns of $M$ by $P_j$ and $P_j^T$ can move the only non-zero element of $M$ , but that element remains below the main diagonal (because $j>i$ ). I can't understand this. $M$ is derived from $L_i$ , which means that $M_{si} = \alpha \neq 0$ and all other elements of $M$ are $0$ . Nothing has been said about $P_j$ . I'm guessing $P_j$ swaps $j$ -th row with $r$ -th row, where $r>j$ . What do the indices $s,i$ have anything to do with the indices $r,j$ ? Would be grateful if anyone could clear this up!","['matrices', 'linear-algebra']"
3093933,Upper bound CP tensor rank,"I have a question about CP tensor ranks. In the following, $\mathcal X \in \mathbb R^{n_1 \times n_2 \times n_3}$ is a third-order tensor of CP rank $R$ , i.e., there exist vectors $a_i$ , $b_i$ and $c_i$ for $i = 1, \ldots, R$ of appropriate dimensions such that $$\mathrm{vec}(\mathcal X) = c_1 \otimes b_1 \otimes a_1 + \ldots + c_R \otimes b_R \otimes a_R.$$ The following theorem holds. Theorem. Let $R_\mu$ be the rank of the matricization $X^{(\mu)}$ of $\mathcal X$ for $\mu = 1, 2, 3$ . Then $$ \max\{R_1,R_2, R_3\} \leq R \leq \min\{R_1R_2, R_1R_3, R_2R_3\}.$$ While I have no issues with the lower bound (i.e., left), I can neither prove nor find a reference for the proof of the upper bound. I can prove that $R \leq \min\{n_1n_2, n_1n_3, n_2n_3\}$ , but that is not enough, as $R_\mu \leq n_\mu$ for $\mu = 1, 2, 3$ . EDIT: Proof of $R \leq \min\{n_1n_2, n_1n_3, n_2n_3\}$ (w.l.o.g. prove $R \leq n_1 n_2$ .) Consider $X^{(1)} \in \mathbb R^{n_1\times n_2n_3}$ the first matricization of $\mathcal X$ , and write $$
X^{(1)} = [x_1, \ldots, x_{n_2n_3}]
$$ Then we have for the canonical basis vectors $e_i \in \mathbb{R}^{n_2n_3}$ $$
X^{(1)} = \sum_{i = 1}^{n_2 n_3} x_i e_i^\top
$$ And by vectorization $$
\mathrm{vec}(\mathcal X) = \mathrm{vec}(X^{(1)}) = \sum_{i = 1}^{n_2 n_3} e_i \otimes x_i,
$$ which implies that $R \leq n_2 n_3$ by constructing vectors $g_i$ and $h_i$ such that $g_i \otimes h_i = e_i$ .","['matrix-rank', 'tensor-rank', 'tensors', 'linear-algebra', 'numerical-linear-algebra']"
3093958,Quick way of solving the contour integral $\oint \frac{1}{1+z^5} dz$,"Consider the contour integral in the complex plane: $$\oint \frac{1}{1+z^5} dz$$ Here the contour is a circle with radius $3$ with centre in the origin. If we look at the poles, they need to satisfy $z^5 = -1$ . So the solutions of the poles are given by: \begin{align*}
z_0 &= \cos(\frac{\pi}{5}) + i \sin(\frac{\pi}{5})\\
z_1 &= \cos(\frac{3\pi}{5}) + i \sin(\frac{3\pi}{5})\\
z_2 &= \cos(\pi) + i \sin(\pi) = -1\\
z_3 &= \cos(\frac{7\pi}{5}) + i \sin(\frac{7\pi}{5})\\
z_4 &= \cos(\frac{9\pi}{5}) + i \sin(\frac{9\pi}{5})
\end{align*} So one can use Cauchy's formula or the residue theorem to calculate for every solution the integral and then adding them up to get the full integral. But I have the feeling that there needs to be a more simple way of calculating the full contour integral. Can one just calculate the integral for one solution $z_i$ (like the simple solution $-1$ ) and then multiply by it $5$ , suggesting that the others have the same value. This would make the calculation much efficienter. EDIT:
I now see that $4$ solutions are symmetric (the solutions except $z=-1$ ) in the complex plane. If one approximates the solutions of the poles in decimals, one finds: \begin{align*}
z_0 &= 0.81 + 0.58i\\
z_1 &= -0.31 + 0.95i\\
z_2 &= -1\\
z_3 &= -0.31 -0.95i\\
z_4 &= 0.81 -0.58i
\end{align*} So there are four symmetric solutions. For instance $z_0$ is symmetric with $z_4$ , they are mirrored around the x-axis. Could this mean they cancell each other out so we only need to calculate the integral for $z_2 = -1$ ?","['integration', 'complex-analysis', 'contour-integration', 'complex-integration']"
3093989,"closed form for $\int_{1/4}^{3/4} x^n(1-x)^n \, dx$","The integrand being a polynomial, I used the binomial formula to separate the monomials: $$\int_{\frac{1}{4}}^{\frac{3}{4}} x^n(1-x)^n \, dx = \sum_{k = 0}^{n}{ n \choose k}(-1)^{k}\int_{\frac{1}{4}}^{\frac{3}{4}} x^{n+k} \, dx = \sum_{k = 0}^{n}{ n \choose k}(-1)^{k}\left[\frac{(\frac{3}{4})^{n+k+1}-(\frac{1}{4})^{n+k+1}}{n+k+1}\right]. $$ I'm looking to get a closed form of the following sum or at least make it 'nicer' in terms of looks: $$f(a) =\sum_{k = 0}^{n}{ n \choose k}(-1)^{k}\frac{a^{n+k+1}}{n+k+1},\,\, 0<a < 1.$$ We have $$f'(a) =\sum_{k = 0}^{n}{ n \choose k}(-1)^{k}a^{n+k} $$ Then : $$f'(a) =a^n(-1)^n\sum_{k = 0}^{n}{ n \choose k}(-1)^{n-k}a^{k} = a^n(-1)^{n}(a-1)^n = a^n(1-a)^n $$ so we are back to square one. Past this circular simplification, I thought of integrating but that will just make the formula much messier. Maybe there's another approach? Edit 1: My main goal is to get a closed form of the integral, so if $\frac14$ and $\frac34$ give some cancellations, it would be great if someone pointed them out. Edit 2: the original problem : let $(X_1,\cdots,X_{2n+1})$ be a $2n+1$ sample of independant, identically distributed random variables that are uniformly distributed over $[0,1]$ -Find the probability that the median $\in [\frac14,\frac34]$ . Edit 3: using @JamesArathoon Observation $$\begin{align}I_n & =\int_0^1 x^n (1-x)^n \, dx-2\int_0^{\frac{1}{4}} x^n (1-x)^n \, dx = B(n+1,n+1) - 2B(\frac14;n+1,n+1)
\end{align}$$ where first special function is the beta function and second one is the incomplete beta. $$\begin{align}I_n & = B(n+1,n+1) - 2B(\frac14;n+1,n+1) = \frac{\Gamma^2(n+1)}{\Gamma(2n+2)} -\frac{2}{4^{n+1}n+1}{_{2}}F_{1}(n+1,-n,n+2,\frac{1}{4}) \\
&=\frac{(n^2)!}{(2n+1)!} -\frac{2}{4^{n+1}n+1} {_{2}}F_{1}(n+1,-n,n+2,\frac{1}{4})
\end{align}$$ because one of the arguments of the hypergeomtric function is negative then the series terminates and is given by $$\begin{align}{_{2}}F_{1}(n+1,-n,n+2,\frac{1}{4}) &=  \sum_{k=0}^{n} (-1)^k \binom{n}{k} \frac{(n+1)_k}{4^k(n+2)_k}   \\
&= 1 + \sum_{k=1}^{n} (-1)^k \frac{n!}{k!(n-k)!} \frac{(n+1)(n+2)\cdots(n+k)}{4^k(n+2)(n+3)\cdots(n+k+1)} \\
&=1 + \sum_{k=1}^{n} (-1)^k \frac{n!}{k!(n-k)!} \frac{(n+1)}{4^k(n+k+1)} \\
\end{align}$$ now that looks like it's got the potential to be turned into a binomial formula with $a = -1, b = \frac14$ but I still can't see it Thanks in advance!","['integration', 'summation', 'definite-integrals', 'closed-form', 'sequences-and-series']"
3093995,Requirements for an integer root of cubic equation,"If we have the quadratic equation $$a x^2 + b x + c = 0$$ with $a,b,c$ integers, then a requirement for $x$ to have an integer solution is for $b^2 - 4ac$ to be a square integer. This condition is necessary, but not sufficient. However it is simple enough to sometimes be useful when solving Diophantine equations. Furthermore, note that this does not come from Vieta's formulas . While those are useful for other purposes, they do not yield a restrictive form in the integers like the condition on $b^2 - 4ac$ extracted from the general solution. So with that introduction out of the way: I would like to know if there are similar conditions for the cubic equation $$a x^3 + b x^2 + c x + d = 0.$$ In the quadratic case, if one root is integer, the other is at least rational. But in the cubic case, one root could be integer, with the others irrational or imaginary. So it looks like it would be harder to extract such a condition from the general solution ( vanderbilt.edu , wikipedia ). In particular, just like in casus irreducibilis where imaginary values will invariably show up during calculation of the roots even when all the roots are real, it seems inevitable that we could have irrationals  like $\sqrt{n}$ show up only to cancel later in the calculation of an integer root. So how can we extract from the general cubic solution some useful conditions on $a,b,c,d$ for an integer solution?  Particularly nice, in analogy to the quadratic case, is if there is some term that is required to be a perfect cube.","['cubics', 'number-theory', 'elementary-number-theory', 'diophantine-equations']"
3094002,Sufficient statistics for a discrete distribution,"Let { $X_1, X_2,...,X_n$ } be a random sample from a population with the following pmf $$f(x|\theta,p)= \left\{ \begin{array}{lcc}
             (1-p)p^{x-\theta}  &  x=\theta, \theta+1,... \\
             \\ 0 &\mbox{ otherwise}, \\
             \\ 
             \end{array}
   \right.$$ where $\theta$ is an positive integer and $0<p<1$ , both are unknown. Find a sufficient statistic for the parameter vector $(\theta,p)$ . My Approach Using Neyman-Fisher Factorization $$
f_X(x|\theta, p) = \prod_{i=1}^n (1-p)p^{x_i-\theta} = (1-p)^{n} p^{\sum_{i=1}^n x_i-n\theta} I_{(x_i \in {\theta,\theta+1,... })}.
$$ This can be written $$
f_X(x|\theta,p) = g(T_1(x),T_2(x);\theta,p)\, h(x) \, ,
$$ where $g(T_1(x),T_2(x))=p^{\sum_{i=1}(x_i-n\theta)}$ and $h(x) = (1-p)^n$ , which demonstrates that $(X_{(1)},\sum_{i=1}^n X_i)$ is a sufficient statistic for the parameter $(\theta,p)$ . Is the above correct? What am I missing?","['statistical-inference', 'statistics']"
3094007,Why does my textbook say that equation $y = 7-3x$ has infinite number of solutions while it has only one root?,"Anywhere I've looked, the definition of solution of equation is root(s) of that equation. But why does a textbook say that equation $y=7-3x$ has infinite number of solutions? Thanks","['analytic-geometry', 'functions']"
3094042,"$f(f(f(x)))=x,$ but $f(x)\neq x$","Let $f: \mathbb{R}\to \mathbb{R}$ be strictly increasing function such that $f(f(f(x)))=x$ then by this $f(x)=x.$ Does the result fail if we drop the hypothesis that f is strictly increasing? 
I find such an example if I change the domain and codomain to $\mathbb{R}^2$ and $f=rotation \ by\  120^o,$ but could not think of a map from $\mathbb{R}\to \mathbb{R}.$ Any help is appreciated.","['functions', 'real-analysis']"
3094057,"Underflow while evaluating softmax, despite using exp-normalize","I have a very simple linear classifier that uses softmax (with exp-normalize trick) for the output: h = self.A.dot(x)
z = self.B.dot(h)
nz = z - max(z)
e = np.exp(nz)
s = np.sum(e)
p = e / s
loss = -np.log(p[k]) I understand that softmax is given by: $\frac{e^{Z_i}}{\sum_{j=0}^{n} e^{z_j}}$ For numerical stability, the exp-normalize trick is used: $\frac{e^{Z_i} - max(Z)}{\sum_{j=0}^{n} e^{z_j} - max(Z)}$ I believe that I understand the motivation for this, and how it eliminates underflow/overflow for the exponentiation. The problem is, that in some cases, some elements of the numerator are very very small. Dividing this element by anything greater than 1.0 results in underflow, and I am not sure how to handle this. For example, this is the specific case I am debugging ( I've truncated most of the numbers to two decimal places for readability ): z = [10869.44 , 10837.85, 10851.28, 10136.48] nz =  [0., -31.58, -18.15, -732.95] e = [1.0, 1.91e-014, 1.30e-008, 4.80e-319] s = 1.000000013039 Dividing the last element of e , ( 4.80e-319 ) by s results in underflow. I've looked at many questions on SE and lots of blogs that talk about the exp-normalize trick, but none of them seem to address how to achieve numerical stability during this division.","['statistics', 'machine-learning', 'numerical-linear-algebra', 'numerical-methods', 'computer-science']"
3094080,"Graphically, why is $\int_{0}^{1} \frac{1}{x} dx$ divergent but $\int_{0}^{1} \frac{1}{x^{0.999}} dx$ convergent?","When the power of $x$ is less than 1, it seems that the improper integral converges. I understand the math, but I don't understand how the graphs of the two cases $\frac{1}{x}$ and $\frac{1}{x^{0.999}}$ are fundamentally different.","['integration', 'calculus']"
3094104,"Sufficient statistic for $N(\mu,1)$","Let $X_1,\ldots,X_n$ be a random sample from $N(\mu,1)$ , where $\mu$ is an unknown parameter. Show that $(\overline{x}/{S^2}, S^2)$ is a sufficient statistic for $\mu$ , where $S^2$ is the sample variance. My Approach $$
\sum_{i=1}^n (x_i-\mu)^2 = n(\bar x-\mu)^2 + \sum_{i=1}^n (x_i-\bar x)^2.
$$ By Neyman Fisher Factorisation, \begin{align}
f_{X_1,\ldots,X_n}(x_1,\ldots,x_n) \propto {} & \prod_{i=1}^n \frac 1 {(\sqrt2\pi)^n} \exp\left( \frac{-1} 2 \left( {x_i-\mu} \right)^2 \right) = \frac 1 {(\sqrt2\pi)^n} \exp\left( \frac{-1}{2} \sum_{i=1}^n (x_i-\mu)^2  \right) \\[10pt]
= {} & (\sqrt2\pi)^{-n} \exp\left( \frac{-1}{2} \left( n(\bar x - \mu)^2 + \sum_{i=1}^n (x_i - \bar x)^2 \right) \right) \\[10pt]
= {} & (\sqrt2\pi)^{-n} \exp\left( - \frac n {2} ((\bar x - \mu)^2 + s^2) \right).
\end{align} How do I proceed from here?","['statistical-inference', 'statistics']"
3094114,Comparing Regular-Faced Toroidal Polyhedra,"Many apologies ahead of time, I have no idea how to phrase this question, and I'm certainly way out of my element.  I'll do my best but please go easy on me. I wanted to make a polyhedra that was in the shape of a doughnut.  For whatever reason, I thought it would be cool if that polyhedra had regular-polygon faces.  With some online-research, I found these things called Stewart Toroids that were seemingly what I was looking for.  The problem was that they all looked... well... ugly?  I think Stewart had some rules for how he made his toroids and maybe that had something to do with why they didn't really look much like a doughnut to me. Anyway I set about making my own, and I made something that looked more doughnut-like.  The driving rule I used to make it was to try and make every vertex (where faces met) as smooth as possible, which I interpreted to mean I needed to minimize the angle change between all the faces.  I think for that exact characteristic (least angle change between faces) you can't do better than this shape does (worst angle change is 36 deg) (sorry, I know those pictures are bad) So I guess my question is if any of that makes sense. Is it okay thinking one polyhedra could be more ""torus"" than another? Is there a smart way to measure that, or is this purely a subjective thing? Does the idea of smoothing out a polyhedra actually help or is that really more cosmetic? Is minimizing the angle between adjacent faces the right way to maximize smoothness? EDIT:  It seems I've attracted a number of people also interested in making more such polyhedra, which is fine, I certainly had fun making this thing.  But the question is not ""can you make these polyhedra"", it is ""how can we compare them?"" There's nothing stopping me from making a torus with a million little square faces, approximating a torus in the same fashion a bunch of pixels can approximate a circle.  The thing I want to know is if math tells us how similar two shapes are, such that there can be an official method for comparing two polyhedral doughnuts.","['polyhedra', 'geometry', '3d']"
3094124,"Evaluate $\int x^x \ln x\, dx$","The integral $$\int x^x \ln x\, dx= ?$$ I know of the integral $\int x^x dx$ can be further simplified as $\int e^{x\ln x} dx$ . And this requires identity to simplify. What about the product in the integral $\int x^x\ln x\,dx=\int e^{x\ln x}\ln x\, dx.$ Is there any identity to be used for this one.","['integration', 'power-series']"
3094131,What are the semi-direct products of $\mathbb{Z}$ with itself? (Check my work please),"I am just starting out with semi-direct products. I would like to list and describe the semi-direct products of $\mathbb{Z}$ with itself. I first need to find the automorphisms $\varphi$ from $\mathbb{Z}$ to $\mathbb{Z}$ . These automorphisms are determined by $\varphi(1)$ by the additive property of such morphisms : $\varphi(n+m)=\varphi(n)+\varphi(m).$ Therefore, I only need to determine the possible values of $\varphi(1)$ . But I know that an automorphism must send generators on generators, and the generators of $\mathbb{Z}$ are $\pm1$ , so there are at most two automorphisms : $Id:x \mapsto x$ and $-Id:x \mapsto -x$ (and they are automorphisms, so these are the only ones). Therefore, a semi-direct product $\mathbb{Z} \rtimes_{\psi}\mathbb{Z}$ is given by a morphism $\psi:\mathbb{Z}\rightarrow Aut({\mathbb{Z}}) \cong \mathbb{Z}/2\mathbb{Z}$ . The only possibilites, if I didn't make any mistakes, are $\psi:n \rightarrow Id$ (constant morphism) and $\psi:n\rightarrow (-1)^nId$ ( $\psi$ is determined by $\psi(1)$ which is either $Id$ yielding the constant morphism or $-Id$ which yields the second one.) So there are only two semi-direct products possible, one of which is the direct product (I guess?). $\mathbb{Z} \rtimes_{n \rightarrow id}\mathbb{Z}\cong\mathbb{Z}^2$ and $\mathbb{Z} \rtimes_{n \rightarrow (-1)^nId} \mathbb{Z}$ Did I miss any of them, and is there anything to say about this last semi-direct product? Is $\mathbb{Z} \rtimes_{n \rightarrow (-1)^nId} \mathbb{Z}$ isomorphic to any known groups? I'm not sure what to say now, I should give it a brief description but I don't know what else there is to say than $\mathbb{Z} \rtimes_{n \rightarrow (-1)^nId} \mathbb{Z}$ . Its multiplicative law is $(x,y)*(z,t)=(x+\psi(y)(z),z+t)=(x+(-1)^yz,z+t)$ , so it kind of ""oscillates"" and it doesn't sound familiar to me... I don't recall any groups with such a weird multiplication law. It's not even commutative I guess, since $(1,1)*(2,2)=(1-2,4)=(-1,4)$ but $(2,2)*(1,1)=(2+1,2)=(3,2) \neq(-1,4)...$","['semidirect-product', 'group-theory', 'abstract-algebra', 'proof-verification']"
3094155,Extension of Du-Bois-Raymond lemma to Vector Fields on a Riemannian Manifold,"Edit: Reviving this thread because I still could not prove or find a proof of this. A sketch of a proof attempt can be found in the previous edit of this post. I am trying to show the following extension of the Du Bois Raymond lemma: Let $M$ be a smooth Riemannian Manifold and $\omega: [0,1] \rightarrow M$ be a $W^{1,2}$ curve on M. Consider a tangential $L^2$ vector field along $\omega$ denoted by $v \in L^2(\omega^*TM)$ . If $$\int_{0}^{1} \langle v, \frac{Du}{\partial t} \rangle \text{ dt} = 0 \ \ \ \ \ \text{for all } u \in W ^{1,2}(\omega^*TM) \text{ with } u(0)=u(1)=0$$ Then $$v \in W^{1,2}(\omega^*TM)\ \ \text{ with   }\ \  \frac{Dv}{\partial t} = 0 \ \ a.e.$$ where $\frac{Du}{\partial t}$ denotes the covariant derivative of $u$ along the curve $\omega$ . For my purposes it would be sufficient to show this for the simplified case where $M = S^2$ and the covariant derivative becomes the projection onto the respective tangent space, i.e., $\frac{Du}{\partial t} = u' - \langle u', \omega \rangle \omega$ . Kind regards.","['calculus-of-variations', 'manifolds', 'riemannian-geometry', 'differential-geometry']"
3094268,Why is the dimension of the zero subspace 0 and not 1?,"A related question: If a single non-zero vector serves as a basis for a subspace, then is the dimension of that subspace 1 or 0? I'm almost certain the answer to the above question is 1. But my confusion lies with the fact that the dimension of the zero subspace (which consists only of the zero vector) is zero. There is one vector in this subspace (namely, the zero vector), so shouldn't the dimension be 1? Is there an intuitive way to think about this, or is this just how it's been defined as this is the only way everything works? On this similar post, a commenter said: ""The zero vector itself does not have a dimension. The vector space consisting of only the zero vector has dimension 0. This is because a basis for that vector space is the empty set, and the dimension of a vector space is the cardinality of any basis for that vector space."" I don't see see how this is true, unless the empty set is not counted when counting cardinality (i.e. a set of vectors with 5 vectors as well as the empty set would be counted as having 5 vectors and not 6). Again, is this just how we've defined things?","['linear-algebra', 'vector-spaces']"
3094272,"Confusing domain of simple $f(x, y)$","I'm not sure if my way of solving this task is correct. Perhaps I've done some ""math-grammar"" mistakes, or some calculating errors? Especially speaking of assumptions 2. Would anyone double-check my calculations? Given function $f(x,y) = \sqrt{\pi-3\arcsin\frac{x+y}{\sqrt{3}}} + \ln(1-y^2-x^2)$ find the domain of this function. Assumption 1: $$-1 \leq \frac{x+y}{\sqrt{3}} \leq 1$$ $$-\sqrt{3} \leq x+y \leq \sqrt{3}$$ $$\Rightarrow -\sqrt{3} -x\leq y \quad \land \quad y \leq -x + \sqrt{3}$$ $$\Longrightarrow  y \geq -x -\sqrt{3} \leq \quad \land \quad y \leq -x + \sqrt{3}$$ Assumption 2: $$\pi - 3\arcsin{\frac{x+y}{\sqrt{3}}} \geq 0$$ $$- 3\arcsin{\frac{x+y}{\sqrt{3}}} \geq -\pi$$ $$3\arcsin{\frac{x+y}{\sqrt{3}}} \leq \pi$$ $$\arcsin{\frac{x+y}{\sqrt{3}}} \leq \frac{\pi}{3}$$ $$\Rightarrow \sin{\frac{\pi}{3}} \leq \frac{x+y}{\sqrt{3}}$$ $$\frac{\sqrt{3}}{2} \leq \frac{x+y}{\sqrt{3}}$$ $$\frac{3}{2} \leq x+y$$ $$y \geq -x + \frac{3}{2}$$ Assumption 3: $$1 - y^2 - x^2 > 0$$ $$- y^2 - x^2 > - 1$$ $$y^2 + x^2 < 1$$ Hence: $$\Longrightarrow D = \Bigg( (x,y) \in \mathbb{R}^2 : (y^2 + x^2 < 1), (y \geq - x + \frac{3}{2} ), (y \leq -x + \sqrt{3}) \Bigg) $$ Is this a good enough answer? Do you see some obvious mistakes? Thanks.","['algebra-precalculus', 'functions', 'solution-verification', 'trigonometry']"
3094315,Show that an ideal of the ring of integers of a real number field is not principal,"The number field in question is $K=\mathbb{Q}(\sqrt{82})$ , and the ideal considered is in its ring of integers $R=\mathcal{O}_K=\mathbb{Z}[\sqrt{82}]$ . The ideal is: $\mathfrak{p}=(2,\sqrt{82})_R$ It is a prime ideal (this follows from a theorem on the splitting behaviour of ideals generated by prime integers), and it appears quite naturally in the unique factorisation $\mathfrak{p}^2=(2)_R$ , so we know that $N(\mathfrak{p})=\pm2$ . I want to show that the ideal is not principal. My attempts so far have been to consider the equation $x^2-82y^2=\pm2$ reduced modulo small integers and then arriving at a contradiction, but this has been to no avail. I have therefore resorted to asking the community the following: (1) Is there any general method to proving that an ideal in the ring of integers of a number field is non-principal, or are there general methods for separate cases $\textit{e.g.}$ in the case of a quadratic number field? The case of $K=\mathbb{Q}(\sqrt{d})$ for $d<0$ square-free of course admits fairly simple solution, as it can be easily observed that $x^2-dy^2=c$ for some $c\in K$ has no solutions when this is the case. (2) If the answer to (1) is either that there is no known general method or that any known general method is impracticable, and that in the case of an ideal in the ring of integers of a quadratic number field $K=\mathbb{Q}(\sqrt{d})$ the best one can do is consider equations of the form $x^2-dy^2=c$ modulo small integers, is there any general algorithm or technique that applies here other than some wit and a keen eye? All help would, as always, be highly appreciated.","['algebraic-number-theory', 'number-theory', 'maximal-and-prime-ideals', 'ring-theory', 'ideals']"
3094323,Existence of a real valued function satisfying $f'(x)=f(x+1)$ where $f(x)\ne0$,"I am wondering about the existence of a real valued function that satisfies $f'(x)=f(x+1)$ other than the trivial solution $f(x)=0$ . 
I thought a likely solution would be a repeating (perhaps sinusoidal) function, where $f(0)=0, f'(0)=1,f'(1)=0,f'(2)=-1,f'(3)=0$ , etc. I tried to find such a function on Desmos, however I failed after realizing that for a Sin function the derivative at $0$ is always greater than the value of the maximum since between the zero and the maximum the function is strictly decreasing. In my exploration, I was able to find a solution to the similar differential equation, $f'(x)=f(x+1)*{\pi\over2}$ , with the solution $f(x)=\sin{({\pi\over2}x)}$ . This was easier to find due to the coefficient of $\pi\over2$ , which removes the obstacle of the previous inequality. Perhaps this provides hope for a solution to the other one. Overall, the question is, is there any functional solution for the equation $f'(x)=f(x+1)$ where $f(x)\ne0$ ?","['functional-equations', 'delay-differential-equations', 'derivatives', 'ordinary-differential-equations']"
3094368,"Are problems in ""Arithmetica"" of Diophantus all solved now?","It's well-known that Diophantus had written ”Diophantus“ which contains many problems about solving arithmetic equations. I wonder whether all of them has been solved using modern techniques, as some of them are counting rational points on higher genus curves.. For example in problem $17$ of book $VI$ , Diophantus poses a problem which comes down to finding positive rational solutions to $y^2 = x^6 + x^2 + 1$ , which is of genus $2$ , and it is solved using Chabauty-Coleman method as in Joseph Loebach Wetherell's PHD thesis in $1998$ . Is there any more difficult and unsolved problem?","['elliptic-curves', 'number-theory', 'elementary-number-theory', 'math-history', 'arithmetic-geometry']"
3094404,"Multiplication operator by $x$ on $L^2(0,1)$ is isomorphic to its square","The following is Exercise II.8.7 from J. B. Conway's A Course in Functional Analysis . Let $A:L^2(0,1)\to L^2(0,1)$ be defined by $(Af)(x)=xf(x)$ for $f$ in $L^2(0,1)$ and $x$ in $(0,1)$ . Show that $A\cong A^2$ . Here $A\cong A^2$ means there is an unitary isomorphism $U:L^2(0,1)\to L^2(0,1)$ such that $UAU^{-1}=A^2$ . I guess I should construct an explicit isomorphism, because $A$ is not compact, and the author has not proved any structure theorem for non-compact operators yet. However, I don't know how I can find such an isomorphism. I understand that $L^2(0,1)$ has Hilbert basis $e^{2\pi inx}$ , and I computed the matrix of $A$ with respect to this basis, but I can't see how $A\cong A^2$ . Any hints will be appreciated!","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
3094419,General solution to linear system with general form,"I am trying to find the general solution of the system $$X' = \begin{bmatrix}
    a & b  \\
    c & d
  \end{bmatrix}X$$ where $a+d \not= 0 $ and $ad-bc=0$ I find that the eigenvalues are 0 and $a+d$ with corresponding eigenvectors both $[0,0]$ which implies that the general solution is simply $X(t)=0$ but this solution does not seem right to me. Is there something I have done wrong?",['ordinary-differential-equations']
3094426,"Show that if $a,b,c\in \mathbb{N}$ and ${a^2+b^2+c^2}\over{abc+1}$ is an integer it is the sum of two nonzero squares","I was reading about the fascinating problem in the IMO ( $1988 $ # $6$ ) that asks: Let $a$ and $b$ be positive integers such that $(ab+1) | (a^2+b^2)$ . Show that ${a^2+b^2}\over{ab+1}$ is a perfect square. I wondered what would happen with the natural extension of this problem to three constants. Upon exploration with Mathematica, I found that the integral values of ${a^2+b^2+c^2}\over{abc+1}$ all seemed to be expressible as the sum of two nonzero perfect squares. The converse also seems to be true (given that a number is the sum of two nonzero squares, it seems to be expressible in the form of this fraction). So I pose two questions: If $a,b,c\in \mathbb{N}$ and $k={{a^2+b^2+c^2}\over{abc+1}}$ is an integer, is $k$ the sum of two nonzero squares? and Given that $k$ is expressible as the sum of two nonzero perfect squares, do there exist $a,b,c\in\mathbb{N}$ for which ${{a^2+b^2+c^2}\over{abc+1}}=k$ ?","['number-theory', 'square-numbers', 'sums-of-squares', 'vieta-jumping']"
3094455,Constructing an isomorphism of group products,"To introduce some symbolism for clarity's sake, here is the problem I'm faced with in an assignment: Find an example of four groups $A,B,C,D$ such that $$A \times B \cong C \times D$$ but at the same time neither of $A,B$ are isomorphic to either of $C,D$ . I feel like I touched on a possible means of solving this, but I'm not sure it's sufficient/correct, and even if it is constructing the isomorphism is proving ... not too easy. Initial Motivation: So, we know: for two groups to be isomorphic, a necessary - but not sufficient! -condition ( as noted and proved here ) is that their underlying sets have the same cardinality. I took this as a sort of starting point to start finding such groups. So, in this problem, I now seek $A,B,C,D$ such that $$|A \times B| = |C \times D| = |A|\cdot |B| = |C| \cdot |D|$$ So it seems sufficient to think of $|A\times B|=|C \times D| = n$ for $n$ finite, and to find factors $a,b,c,d$ of $n$ such that $$n=ab=cd$$ where $a,b,c,d$ are the factors of $n$ , and the orders of groups $A,B,C,D$ . In my case, $n=12$ seems a convenient example, as $$12 = 2\cdot 6 = 3 \cdot 4$$ Some of the first groups that come to mind of the corresponding orders are, each with modular addition, $$A = \Bbb Z / 2 \Bbb Z \;\;\;\;\;B = \Bbb Z / 6 \Bbb Z \;\;\;\;\;C = \Bbb Z / 3 \Bbb Z \;\;\;\;\;D = \Bbb Z / 4 \Bbb Z$$ Problems in What Remains: So what remains is to construct an isomorphism $f : (\Bbb Z / 2 \Bbb Z \times \Bbb Z / 6 \Bbb Z) \to (\Bbb Z / 3 \Bbb Z \times \Bbb Z / 4 \Bbb Z)$ and demonstrate it, but I'm running into problems there, nothing seems to really work out, at least nicely. I also tried to just show each product was isomorphic to $\Bbb Z / 12 \Bbb Z$ but ran into the same fundamental problem. So I'm guessing this product isn't the easiest to deal with. Indeed I also tried a slightly simpler $$A = \Bbb Z / 2 \Bbb Z \;\;\;\;\;B = \Bbb Z / 2 \Bbb Z \;\;\;\;\;C = \Bbb Z / 4 \Bbb Z \;\;\;\;\;D = \Bbb Z / 1 \Bbb Z = \langle e \rangle$$ but I couldn't construct a nice function that wouldn't rely on a bunch of cases in proving it's an isomorphism. Is there perhaps a function that I'm overlooking, or an easier set of products to work with?","['group-theory', 'group-isomorphism', 'finite-groups']"
3094515,Invertible Matrices within a Matrix,"Suppose A, B are invertible matrices of the same size. Show that $$M = \begin{bmatrix} 0& A\\ B& 0\end{bmatrix}$$ is invertible. I don't understand how I could show this. I have learned about linear combinations and spanning in my college class, but I don't know how that would help in this case.","['matrices', 'linear-algebra', 'inverse', 'block-matrices']"
3094525,Does $\sum_{n=1}^\infty \frac{\cos{(\sqrt{n})}}{n}$ converge?,"The series is: $$\sum_{n=1}^\infty \frac{\cos(\sqrt{n})}{n}$$ Considering it isn't always positive, I replace $\frac{\cos{\sqrt{n}}}{n}$ with its absolute value and I find that: $$\vert \frac{\cos{\sqrt{n}}}{n}\vert\gt \frac{\cos^2{\sqrt{n}}}{n}=\frac{\ 1+\cos{2\sqrt{n}}}{2n}=\frac{1}{2n}+\frac{\cos{2\sqrt{n}}}{2n}$$ if $\sum_{n=1}^\infty \vert\frac{\cos{\sqrt{n}}}{n}\vert $ converges, then $\sum_{n=1}^\infty \frac{\cos{\sqrt{n}}}{n}$ converges.Using Comparison test,we can draw the conclusion that $\sum_{n=1}^\infty\frac{1}{2n}$ converges , which is impossible.
So I get that $\sum_{n=1}^\infty \frac{\cos{\sqrt{n}}}{n}$ absolutely diverges. But I can't figure out whether $\sum_{n=1}^\infty \frac{\cos{\sqrt{n}}}{n}$ converges or not. I have tried Dirichlet's test, but I can't figure out whether $$S_{n}=\sum_{k=1}^n \cos{\sqrt{k}}$$ is bounded. (This is my first time to ask question.Maybe there exist some mistakes in my conclusion.Thanks. :)","['trigonometry', 'analysis', 'sequences-and-series']"
3094548,"What is a differetial structure, exactly?","A structure in general is a set and some operations on that set or ordersrelations of some kind. In algebra and topology this is rather clear, but in differential geometry one often consider ""differential structure"". I understand that an atlas is related to this matter and my impression is that an atlas induces a differential structure. But what is this structure? In topology we can relate the structure to continuity i.e any topological space space that has the same topological structure also have the same continuous functions(even if it is not defined on the exact same sets). Hence one would think that differential structure and differentiable functions have the same dynamics. Something tells me this is related to the tangent spaces and how they look as linear spaces since this is what the atlas induces at each point via the partials of the charts. Does anyone have  good answer for what the differential structure consists of or how to think about it? Two different manifolds with different atlases should be able to have the same ""differential structure"" as far as I understand.","['riemannian-geometry', 'differential-geometry']"
3094609,Lower bound for $\Gamma(x+i y)$ where $x>0$ involving only the real part,"I am trying to have some inequalities involving the special Gamma function. I am able to get an upper bound for $\Gamma(x+i y)$ , for $x>0$ , $$
\begin{align}
|\Gamma(x+iy)|
&=\left|\int_0^\infty e^{-t}\,t^{x+iy-1}\;\mathrm{d}t\right|\\
&=\left|\int_0^\infty e^{-t}\,t^{x-1}\,e^{iy\log(t)}\;\mathrm{d}t\right|\\
&\le\int_0^\infty\left|e^{-t}\,t^{x-1}\,e^{iy\log(t)}\right|\;\mathrm{d}t\\
&=\int_0^\infty e^{-t}\,t^{x-1}\;\mathrm{d}t\\
&=\Gamma(x)\\
&=|\Gamma(x)|\tag{1}.
\end{align}
$$ However, I did not succeed to get a lower bound for $\Gamma(x+i y)$ such that I get rid of the imaginary part. Any help in this direction? Using the suggestion of @reuns and the Euler's reflection formula, one can shows that \begin{align} \left|\Gamma(x+i y)\right| &= \frac{\pi}{\left|\Gamma(1-(x+i y)\right| \left|\sin(\pi (x+i y))\right|} \\
&\ge \frac{\pi}{\left|\Gamma(1-x)\right| \left|\sin(\pi (x+i y))\right|}
 \end{align} But I can not still get a lower bound for the complex sine term such that I get rid of the imaginary part! Any suggestion?","['complex-analysis', 'inequality', 'special-functions', 'gamma-function']"
3094635,Generating function for the number of graphs with $k$ connected components,"There are $$b_n = \frac{(n-1)!}{2}$$ ways to form a cycle on $n$ labelled vertices, for $n\geq 3$ . The exponential generating function for this sequence is $$ f(x) = \frac{1}{2}\sum_{n\geq 3} (n-1)! \frac{x^n}{n!} =\frac{1}{2}\sum_{n\geq 3} \frac{x^n}{n},$$ and if we want the number of graphs on $n$ vertices, the components of which are cycles, the EGF is $$ g(x) = \exp(f(x)) = \exp \left(\frac{1}{2}\sum_{n\geq 1} \frac{x^n}{n} - \frac{x}{2} - \frac{x^2}{4}\right)$$ $$ =\exp \left( - \frac{x}{2} - \frac{x^2}{4}\right) (1-x)^{-1/2}.$$ (This is example 5.2.8 in Stanley's Enumerative Combinatorics, Vol II) I'd like to do a similar thing with the number of connected components in a graph. Fix $n$ and $m$ , and consider the graphs on $n$ labelled vertices, with $m$ edges. (We may assume $m\ll n$ .) Suppose we know there are $C_{j,l}$ ways to get a connected graph on $j$ vertices and $l$ edges. So, in particular, $C_{j,l}=0$ in case $l<j-1$ , or $l>\binom{j}{2}$ . Question: Is it feasible to get an EGF for the sequence $a_k$ of the number of graphs with $k$ connected components ? Clarification: Let us use the term $(n,m)$ -graph, for a graph with $n$ vertices and $m$ edges. We fix $n$ and $m$ , and may assume that $m\ll n$ . Now, there are $\binom{\binom{n}{2}}{m}$ such $(n,m)$ -graphs. Let $a_k$ be the number of $(n,m)$ -graphs with $k$ connected components. Clearly, $\sum_{k=1}^n a_k = \binom{\binom{n}{2}}{m}$ . I'm interested in the EGF $$ \sum_{k=1}^n \frac{a_k}{k!} x^k.$$","['graph-theory', 'combinatorics', 'generating-functions']"
3094653,Understanding submersions in differential topology,"I'm having trouble understanding an example in Guillemin and Pollack. If $f:R^k\rightarrow R$ be defined by $f(x) = |x|^2 = x_1^2+...+x_k^2$ The derivative $df_x$ at the point $a = (a_1,..,a_k)$ has matrix $(2a_1,..,2a_k)$ . Thus $df_a:R^k\rightarrow R$ is surjective unless $f(a)=0$ , so every nonzero real number is a regular value of $f$ . I can't understand how to check $df_a$ is surjective and why it is obvious that every nonzero real number is a regular value of $f$ ? A hint is appreciated. Thanks.","['multivariable-calculus', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
3094701,Reference on Lipschitz property of the infimum of a family of Lipschitz functions,"I can prove the following fact:  the infimum, or supremum, of any family of L-Lipschitz functions is L-Lipschitz, as long as the constant L is fixed. However, since this is a very basic result, I am interested in a reference where it is proved. Any suggestions?","['nonlinear-optimization', 'lipschitz-functions', 'reference-request', 'non-convex-optimization', 'functional-analysis']"
3094706,Why aren’t continuous functions defined the other way around?,"Continuity of function $f:X\to Y$ from topological space $X$ to $Y$ is defined by saying that for any open set $U_Y$ , $f^{-1}(U_Y)$ is also an open set. Intuitively, I find this weird. If we interpret “open set” informally as “a set whose elements are nearby each other” (indeed it is a set which is a neighbourhood of all its elements), then it makes intuitive sense to say that a continuous function $f$ is a function that does not “rip elements away from its neighbours”, i.e. if you input an open set $U_X$ (a set whose elements are “nearby each other”), then this should not produce a set where some elements are “not nearby each other”, i.e. it should produce an open set. So is there an intuitive explanation at this level of abstraction (i.e. without reference to metric spaces for example) of why we don’t define continuity as “for any open set $U_X$ , $f(U_X)$ is an open set”?","['continuity', 'general-topology', 'intuition']"
3094789,Distribution of Gaussian Random variable. Concentrated measure.,"Let $\mu$ be the standard Gaussian distribution on $\mathbb{R}$ . Show that if $B$ is a Borel set (w.r.t the Euclidean metric) and $\mu(B)\geq 1/2$ then $$\mu(B_{r})\geq 1-\frac{1}{2}e^{-\frac{t^2}{2}} $$ For all $r>0$ , where $B_{r} := \{x \in \mathbb{R} : \inf_{y\in B} |x-y|<r \}$ . Attempt : Since $\mu(B)\geq 1/2$ (and since we are centred) it follows $\mu(B_{r})\geq \mu((-\infty,r])$ So $$\mu(B_{r})\geq 1- \int_{r}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-(t^2)/2}dt\geq 1-\frac{1}{\sqrt{2\pi}r}e^{-(r^2)/2}$$","['measure-theory', 'normal-distribution', 'probability', 'gaussian-integral']"
3094828,Push-Forward Algebra of $\mathcal{B}(\mathbb{R})$ by $f(x) = \tan(x)$,"Specifically $f(x)$ is defined to be $\tan(x)$ when $\cos(x) \neq 0$ , and $0$ elsewhere. I am required to determine the $\sigma$ -algebra $\mathcal{F} := \{A \subset \mathbb{R}: f^{-1}(A) \in \mathcal{B}(\mathbb{R}) \}$ My intuition suggests that this push-forward algebra is $\mathcal{B}$ , as the preimage of any Borel subset of $\mathbb{R}$ is countably many copies of what is essentially a ""continuously deformed"" version of that Borel subset. It is easy to show that $\mathcal{B}(\mathbb{R})$ is a subset of the push-forward algebra, as the preimage of an open interval is a countable union of open intervals (with countably many isolated points $(..., -\frac{\pi}{2}, \frac{\pi}{2}, ...)$ thrown in if the open interval contains zero). Thus, as the Borel algebra on the reals is the smallest $\sigma$ -algebra containing the open intervals, and the push-forward algebra contains the open intervals, clearly $\mathcal{B}(\mathbb{R})$ is contained in the push-forward algebra. For the reverse inclusion, however, I have no idea, and any help would be much appreciated!","['integration', 'measure-theory']"
3094843,Solving $\int_0^{\infty} \ln^m(x)\sin\left(x^n\right)\:dx$,"Spurred on by this question, I decided to investigate a more generalised form: \begin{equation}
I_{m,n} = \int_0^{\infty} \ln^m(x)\sin\left(x^n\right)\:dx
\end{equation} Where $n,m \in \mathbb{N}$ I have formed a solution in terms of the Gamma Function but I'm unsure whether it can be expressed in terms of other Non-Elementary and/or Elementary Functions. Also very interested to see other approaches (Real + Complex Analysis). To solve, we first observe that: \begin{equation}
I_{n,k} = \lim_{\phi\rightarrow 0^+} \frac{d^m}{d\phi^m}\int_0^\infty x^\phi \sin\left(x^n\right)\:dx
\end{equation} Here let: \begin{equation}
 J_{n}(\phi) = \int_0^\infty x^\phi \sin\left(x^n\right)\:dx
\end{equation} We observe that we first must solve $J_{n,k}(\phi)$ . To achieve we employ Feynman's Trick coupled with Laplace Transforms. This is allowable as the integrand conforms with both Fubini's Theorem and the Dominated Convergence Theorem . Here we introduce: \begin{equation}
H_{n}(t,\phi) = \int_0^\infty x^\phi \sin\left(tx^n\right)\:dx
\end{equation} Where \begin{equation}
J_{n}(\phi) = \lim_{t\rightarrow 1^+} H_{n}(t,\phi)
\end{equation} We now take the Laplace Transform of $H_{n}(t,\phi)$ with respect to $t$ : \begin{align}
\mathscr{L}_t\left[H_{n}(t,\phi)\right] = \int_0^\infty x^\phi \mathscr{L}_t\left[\sin\left(tx^n\right)\right]\:dx = \int_0^\infty x^\phi \frac{x^n}{s^2 + x^{2n}} \:dx = \int_0^\infty \frac{x^{\phi + n}}{s^2 + x^{2n}} \:dx
\end{align} Thankfully (and as I address here ) this integral can be evaluated easily: \begin{align}
\mathscr{L}_t\left[H_{n}(t,\phi)\right] = \int_0^\infty \frac{x^{\phi + n}}{s^2 + x^{2n}} \:dx = \frac{1}{2n} \cdot \left(s^2\right)^{ \frac{\phi + n + 1}{2n} - 1}\cdot B\left(1 - \frac{\phi + n + 1}{2n}, \frac{\phi + n + 1}{2n} \right)
\end{align} Using the relationship between the Beta Function and the Gamma Function: \begin{equation}
\mathscr{L}_t\left[H_{n}(t,\phi)\right] = \frac{1}{2n} s^{ \frac{\phi + n + 1}{n} - 2}\Gamma\left(1 - \frac{\phi + n + 1}{2n} \right)\Gamma\left(\frac{\phi + n + 1}{2n} \right)
\end{equation} We now resolve $H_{n}(t, \phi)$ by taking the Inverse Laplace Transform: \begin{align}
H_{n}(t,\phi)&=\mathscr{L}_s^{-1}\left[ \frac{1}{2n} s^{ \frac{\phi + n + 1}{n} - 2}\Gamma\left(1 - \frac{\phi + n + 1}{2n} \right)\Gamma\left(\frac{\phi + n + 1}{2n} \right)\right]\\
& = \frac{1}{2n} \cdot \frac{1}{\Gamma\left(2 - \frac{\phi + n + 1}{n}\right)t^{-\left(\frac{\phi + n + 1}{n} - 2  + 1\right)} } \cdot \Gamma\left(1 - \frac{\phi + n + 1}{2n} \right)\Gamma\left(\frac{\phi + n + 1}{2n} \right)
\end{align} We can now solve $J_n(\phi)$ : \begin{equation}
J_{n}(\phi) = \lim_{t\rightarrow 1^+} H_{n}(t,\phi) =  \frac{\Gamma\left(1 - \frac{\phi + n + 1}{2n} \right)\Gamma\left(\frac{\phi + n + 1}{2n} \right) }{2n\:\Gamma\left(2 - \frac{\phi + n + 1}{n}\right) }  
\end{equation} And finally we have \begin{equation}
I_{m,n} = \int_0^{\infty} \ln^m(x)\sin\left(x^n\right)\:dx = \lim_{\phi\rightarrow 0^+} \frac{d^m}{d\phi^m} \left[\frac{\Gamma\left(1 - \frac{\phi + n + 1}{2n} \right)\Gamma\left(\frac{\phi + n + 1}{2n} \right) }{2n\:\Gamma\left(2 - \frac{\phi + n + 1}{n}\right) }  \right]
\end{equation} For example, using the example as linked above we have $m = 2$ , $n = 2$ : \begin{equation}
I_{2,2} = \int_0^{\infty} \ln^2(x)\sin\left(x^2\right)\:dx = \lim_{\phi\rightarrow 0^+} \frac{d^2}{d\phi^2} \left[\frac{\Gamma\left(1 - \frac{\phi + 2 + 1}{2\cdot 2} \right)\Gamma\left(\frac{\phi + 2 + 1}{2\cdot2} \right) }{2n\:\Gamma\left(2 - \frac{\phi + 2 + 1}{2}\right) }  \right]
\end{equation} I was too lazy to do it by hand, but evaluated through WolframAlpha we observe that: \begin{equation}
I_{2,2} = \int_0^{\infty} \ln^2(x)\sin\left(x^2\right)\:dx = \frac{1}{32}\sqrt{\frac{\pi}{2}}(2\gamma-\pi+4\ln2)^2
\end{equation} As required","['integration', 'trigonometric-integrals', 'definite-integrals']"
3094870,"Such that ""|"" and "":"", what's the difference?","I have encountered a statement like this: $z=\{x \in X | \forall y \in Y : d_{xy} = NULL\}$ It's supposedly getting as set of $z$ out of table $d$ where in column $x$ , row $y$ is empty. But the symbol $|$ used, isn't it supposed to be $\land$ ? Or ""such that"" is a legitimate way of expression in this case? Then what's the difference between $|$ and $:$ , aren't they both ""such that""?","['elementary-set-theory', 'notation']"
3094872,Sum involving $\ln{(2)}$,"I got this sum. How do can this sum be equal to $8\ln{(2)}?$ $$\sum_{n=2}^{\infty}\frac{(-1)^n}{n}\left[\frac{35n-37}{(2n-1)(n-1)^2}+\frac{35n+37}{(2n+1)(n+1)^2}\right]=8\ln{(2)}$$ I have try to expand out the sum but it is too messy. Dealing the sum in this form, I haven't got any idea. Any help.","['sequences-and-series', 'real-analysis']"
3094900,Finding if $\phi(t) = \frac{\cos(t)}{1 + t^4}$ is a characteristic function,Let's consider a function: $$\phi(t) = \frac{\cos(t)}{1 + t^4} \tag{1}.$$ How can I check whether $(1)$ is a characteristic function? I tried using Polya's criterion. Unfortunately it doesn't work here.,"['characteristic-functions', 'probability-theory']"
3094925,Weighted Least Squares,"I understand the concept of least squares but I'm not able to wrap my head around weighted least squares (the matrix form).
We convert $Ax = b$ to $WAx = Wb$ . What exactly happens when we multiply the equation with $W$ ? Is the column space of A modified based on the changed equations? Also how do I find this matrix $W$ , assuming I have the given data (The probability of each observation according to a textbook example ""Linear Algebra and it's Applications"" by Gilbert Strang, page 174, question 42). Suppose you guess your professor's age, making errors $e = -2, -1, 5$ with probabilities $1/2, 1/4, 1/4$ . If the professor guesses too (or tries to remember), making errors $-1, 0, 1$ with probabilities $1/8, 6/8, 1/8$ , what weights $w_1$ and $w_2$ give the reliability of your guess and the professor's guess?","['statistics', 'linear-algebra']"
3094962,Conservative field defined over a not simply-connected region,"I am wondering if a field can be conservative if the region where it is defined is not simply-connected. By definition $F$ is conservative if there exists a differentiable function which satisfies $F=grad(u)$ If I find such function which is defined in the not simply connected region, am I OK?","['multivariable-calculus', 'calculus', 'vector-fields']"
3094971,Evaluating $\sin\frac{\pi}{2}\sin\frac{\pi}{2^2}\sin\frac{\pi}{2^3}\cdots\sin\frac{\pi}{2^{11}}\cos \frac{\pi}{2^{12}}$,"Evaluate : $$\sin\frac{\pi}{2} \times \sin \frac{\pi}{2^2} \times \sin \frac{\pi}{2^3} \cdots \times \sin\frac{\pi}{2^{11}} \times \cos \frac{\pi}{2^{12}}$$ I tried to solve it by using double angle formula, by replacing $\sin\displaystyle\frac{\pi}2$ with $2 \sin\displaystyle\frac{\pi}4 \cos\displaystyle\frac{\pi}4$ and similarly replacing $\sin\displaystyle\frac{\pi}{2^2}$ with double angle formula. But I'm not getting anywhere from this, can you please suggest me how can I approach to this problem, especially that cosine term at last is making everything so weird. The thing which is most important to ask is : although I know every theory which applies over here but then also why I'm unable to solve this question? What is the reason for my failure?","['trigonometry', 'problem-solving']"
3094974,Boundary defining function,"I'm confused about the proof of Prop 5.43 in Lee's Introduction to Smooth Manifolds Prop 5.43 Every smooth manifold with boundary admits a boundary defining function. A boundary defining function is defined just a few lines earlier: If $M$ is a smooth manifold with boundary, a boundary defining function for $M$ is a smooth function $f: M \to [0, \infty)$ such that $f^{-1}(0) = \partial M$ and $df_p \neq 0$ for all $p \in \partial M$ . The proof basically lets ${(U_\alpha, \phi_\alpha)}$ be a collection of smooth charts whose domains cover $M$ ; defines smooth functions $f_\alpha : U_\alpha \to [0, \infty)$ , and lets ${\psi_\alpha}$ be a partition of unity subordinate to this cover. He defines $f = \sum_\alpha \psi_\alpha f_\alpha$ , and then has the following equation: \begin{equation}
df_p(v) = \sum_\alpha (f_\alpha (p) d \phi_\alpha \rvert_p(v) + \phi_\alpha(p) d f_\alpha \rvert_p (v)),
\end{equation} My question is why does: $f = \sum_\alpha \psi_\alpha f_\alpha \Longrightarrow df_p(v) = \sum_\alpha (f_\alpha (p) d \phi_\alpha \rvert_p(v) + \phi_\alpha(p) d f_\alpha \rvert_p (v))?$ I don't have a lot of diff geo background other than the first five chapters of Lee's Introduction to Smooth Manifolds, so if explanations could be confined to using that knowledge, it would help me out a lot. Thanks! This seems like something that should be easy to see, but here's what I am thinking and am stuck on: Let $v \in T_{p} M$ , so $v: C^\infty (M) \to \mathbb{R}$ , and $g \in C^\infty (\mathbb{R})$ . If we use the definition of a differential, we get: \begin{equation}
d f_p(v)(g) = v(g \circ f) = v (g \circ \sum_\alpha \phi_\alpha f_\alpha) = v (\sum_\alpha g \circ \phi_\alpha f_\alpha) = \sum_\alpha v \circ g \circ \phi_\alpha f_\alpha,
\end{equation} where in the third equality, we used the linearity of the derivation $v$ . From looking at the expression we want, it seems like we use the chain rule property of derivations, but that would apply if our expression were $v \circ \phi_\alpha f_\alpha$ rather than what we have, which is $v \circ g \circ \phi_\alpha f_\alpha$ .","['manifolds', 'differential-geometry']"
3094976,How do we know the gamma function and Riemann zeta function combine in such a nice way?,"Let $\zeta(s) = \prod\limits_p (1 - p^{-s})^{-1}$ be the Riemann zeta function.  If we define $L(s) = \pi^{-\frac{s}{2}}\Gamma(\frac{s}{2})\zeta(s)$ , then $L$ admits a meromorphic continuation to the complex plane satisfying the functional equation $$L(s) = L(1-s)$$ The proof techniques are well established, but at the time were very sophisticated, making use of the Fourier transform, Poisson summation, and complex line integrals. My question is, how could anyone have known to thought to combine the gamma function and the Riemann zeta function in this way? I see that the proof works, but I don't see how anyone could have ever thought of doing this.  The same idea has carried over to prove the meromorphic continuation of other L-functions, but Riemann's combination of the gamma and zeta function seems to be the first of its kind.","['complex-analysis', 'number-theory', 'riemann-zeta']"
3094982,Calculating limit of definite integral,"I need to calculate: $$
\lim_{x\to \infty} \int_x^{x+1} \frac{t^2+1}{t^2+20t+8}\, dt
$$ The result should be $1$ . Is there a quicker way than calculating the primitive function? I thought about seperating to $\int_0^{x+1} -\int_0^x$ but still can't think of the solution.","['limits', 'definite-integrals']"
3094996,How to scale a polygon such that all the points lie within the original polygon?,"With a convex shape, like a circle, we can create a set of similar shapes, all contained within one another, by centering the shape at the origin and scaling it. So we can get the following: With a concave shape however, cetnering it at its centroid and scaling won't keep the points inside the original polygon, we would get something like this: (Diagram was made by hand so this may not be the mathematical result, but it illustrates the point) My goal is to be able to deform the original polygon such that the result converges to a single point contained in the original polygon, and every new ring is fully contained in the previous ring, as in this image: How can this be done? EDIT: Maintaining the topology in new rings isn't necessary as long as the number of points doesn't change, so if the shape passes from being concave to being convex, that;s fine as well. Example:","['vectors', 'linear-algebra', 'geometry', 'polygons']"
3095000,Multiplicity and degree of irreducible projective subschemes.,"Suppose $X \subset \mathbb{P}^n$ is an irreducible projective scheme. Then its multiplicity $\mu_X$ is defined as the length of the local ring $\mathcal{O}_{X,\eta}$ over itself, where $\eta$ is the generic point of $X$ . The degree $d_X$ of $X$ is defined as $\frac{c}{n!}$ , where $c$ is the leading coefficient of the Hilbert polynomial $P_X$ of $X$ , and $n$ is the degree of $P_X$ . I would like to show that $d_X = \mu_X \cdot  d_{X_{red}}$ , where $X_{red} \subset X \subset \mathbb{P}^n$ is the reduced subscheme with the same set of underlying points, but I don't see how to relate the Hilbert polynomial to the multiplicity. So any ideas or hints would be appreciated. One thought was that maybe even $P_X = \mu_X\cdot P_{X_{red}}$ holds.  But on further thought this is inplausible, because $X$ might have embedded components, which do not appear in $Y$ , but contribute a part to $P_X$ . But doing a (homogeneous) primary decomposition we can decompose $X = X_1 \cup \dots \cup X_r$ scheme theoretically , and $X_1$ corresponds to the maximal component. Because $X_2, \dots X_r$ do not contribute to the leading coefficient of $P_X$ , we might assume wlog that $X$ does not have any embedded components. Does this imply $P_X = \mu_X \cdot P_{X_{red}}$ ?","['algebraic-geometry', 'projective-geometry', 'projective-schemes', 'schemes']"
3095153,How do I prove that the second derivative of a function $f:M\to\mathbb{R}$ defined on a surface $M\subset\mathbb{R}^n$ is well defined?,"QUESTION. How do I prove that the second derivative of a function $f:M\to\mathbb{R}$ defined on a surface $M\subset\mathbb{R}^n$ is well defined? QUESTION. If, as  says the user Amitai Yuval, the second derivative of a function defined on a $ M $ surface can not be well defined in general what is the explanation for this impossibility? By a second derivative I mean an application $D^{(2)}f$ that at each point $ p \in M $ associates a well-defined bilinear aplication $$
D^{(2)}f(p):T_pM \times T_pM \to\mathbb{R}. 
$$ Let me make a background in a few steps to try to make the question more precise. Step one. Here $ M $ designates a $C^k$ surface of dimension $ m $ within the Euclidean space $ \mathbb{R}^n$ , i.e . a set in $\mathbb{R}^n$ which satisfies the following properties: there is a family of open sets $\{O_i\}_{i\in I}$ such that $M\subset \bigcup_{i\in I} O_i$ for any $U=M\cap O_i$ there are a open set $U_0$ in $\mathbb{R}^m$ and a $C^k$ ( $k\geq 2$ ) parametrization $\varphi:U_0\to U$ . Step two. I'm assuming that, fixed a point $ p \in M $ , for any two parametrizations $\varphi:U_0\to U\subset M$ and $\psi:V_0\to V\subset M$ such that $p\in V\cap U$ it is proved that $$
(\varphi^{-1}\circ\psi):\psi^{-1}(U\cap V)\to \varphi^{-1}(U\cap V)
$$ and $$
( \psi^{-1}\circ \varphi):\varphi^{-1}(U\cap V)\to \psi^{-1}(U\cap V)
$$ are $C^k$ ( $k\geq 2$ ) diffeormorphisms. Step three. I am also assuming that the tangent plane $T_pM$ is the well defined vector space given by any parameterization $\varphi:U_0\to U\subset M$ such that $\varphi(a)=p$ as $$
T_pM= D\varphi(a)(\mathbb{R}^m).
$$ By well defined I mean $ D\varphi(a)(\mathbb{R}^m)=D\psi(b)(\mathbb{R}^m)$ for any other parameterization $\psi:V_0\to V\subset M$ such that $\psi(b)=p$ . Step four. A function $ f: M \to \mathbb{R}$ is $C^r$ ( $1\leq r<k$ ) differentiable at point $p$ if there is a parameterization $\varphi:U_0\to U\subset M$ with $\varphi(a)=p$ such that $f\circ\varphi$ is $C^r$ differentiable in $a$ . Once the parameter change $(\varphi^{-1}\circ\psi):\psi^{-1}(U\cap V)\to \varphi^{-1}(U\cap V)$ is differentiable it follows that the application $f\circ\psi$ is differentiable in $b$ for all parameterization $\psi:V_0\to V\subset M$ such that $\psi(b)=p$ . Step five If $ f:M\to \mathbb{R} $ is $C^r$ $(r>1)$ differentiable at point $ p \in M$ then its derivative at that point is the linear transformation $ Df(p):T_pM\to\mathbb{R} $ defined as follows. Let's take a parameterization $\varphi:U_0\to U\subset M$ with $\varphi(a)=p$ . Given a vector $u\in T_p M $ there exists unique a vector $ \mu \in \mathbb{R}^m$ such that $u=Df(a)\mu$ .The derivative of $ f $ at point $ p $ is then simply defined by $$
Df(p)\cdot u =D (f\circ \varphi)(a)\cdot\mu
$$ Step six. The linear transformation of step five is well defined. That is, if $\psi:V_0\to V$ is any other parameterization  with $\psi(b)=p$ and $u=D\psi(b)\zeta$ for some vector $\zeta\in\mathbb{R}^m$ , then $$
D (f\circ \varphi)(a)\cdot\mu= D (f\circ \psi)(b)\cdot\zeta.
$$ Indeed, $\psi=\varphi\circ(\varphi^{-1}\circ\psi)$ at where $(\varphi^{-1}\circ\psi):\psi^{-1}(U\cap V)\to \varphi^{-1}(U\cap V)$ is a $C^{k}$ diffeomorphism such that $(\varphi^{-1}\circ\psi)(b)=a$ . We have \begin{align}
     D\varphi(a)\cdot \mu=& u \\
                    =& D\psi(b)\cdot \zeta \\
                    =& D((\varphi\circ \varphi^{-1})\circ\psi )(b)\cdot \zeta\\
                    =& D(\varphi\circ(\varphi^{-1}\circ\psi))(b)\cdot \zeta\\
                    =& D\varphi(a)\cdot \Big(D(\varphi^{-1}\circ\psi)(b)\cdot\zeta\Big)
\end{align} Since $ D \varphi(a)$ is injective we have $\mu=D(\varphi^{-1}\circ\psi)(b)\cdot\zeta$ .
Therefore, \begin{align}
D(f\circ\psi)(b)\zeta =& D(f\circ(\varphi\circ\varphi^{-1})\circ\psi)(b)\cdot\zeta \\
                    =& D(f\circ \varphi\circ(\varphi^{-1}\circ\psi))(b)\cdot\zeta \\
                    =& D(f\circ \varphi)(a) \cdot\Big( D(\varphi^{-1}\circ\psi)(b)\cdot\zeta\Big) \\
                    =& D(f\circ \varphi)(a) \cdot\mu
\end{align} Let us end the background here and return to the question. Below is my attempt to answer the question. By analogy to the first order derivative the second derivative would work as follows. DEFINITION. Let $M$ a $C^k$ ( $k>2$ ) surface. Let $ f:M\to \mathbb{R} $ is $C^2$ differentiable at point $ p \in M$ . Its second derivative $D^{(2)}f$ associates each point $p$ the bilinear transformation $$ 
D^{(2)}f(p):T_pM\times T_pM\to\mathbb{R} 
$$ such that for all parameterization $\varphi:U_0\to U\subset M$ , with $\varphi(a)=p$ , and all vectors $u,v\in T_p M $ $$
D^{(2)}f(p)\cdot (u,v) \mathop{=}^{_{\rm def}}D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)
$$ with $ \mu,\nu \in \mathbb{R}^m$ such that $
u=D\varphi(a)\mu \quad \mbox{ and }\quad v=D\varphi(a)\nu.
$ Similarly to the case of the first derivative, the second derivative will be well defined if for any other parameterization $\psi:V_0\to V$ such that $\psi(b)=p$ , $$
D\psi(b)\eta=u \quad \mbox{ and } \quad D\psi(b)\zeta=v
$$ for some $\eta\in\mathbb{R}^m$ and for some $\zeta\in\mathbb{R}^m$ we have $$
D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)=D^{(2)} (f\circ \psi)(b)\cdot(\eta,\zeta)
$$ To show the above equality I tried to imitate step six. We have \begin{align}
D^{(2)} (f\circ \psi)(b)\cdot(\eta,\zeta)
=& 
D^{(2)} (f\circ(\varphi\circ\varphi^{-1})\circ \psi)(b)\cdot(\eta,\zeta)
\\
=& 
D^{(2)} (f\circ\varphi\circ(\varphi^{-1}\circ \psi))(b)\cdot(\eta,\zeta)
\end{align} But in this last equality I can not use the chain rule. On the other hand the development of the second derivative $D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)$ in terms of the partial derivatives $\frac{\partial^{2}f}{\partial x_i\partial x_j}(a)$ was more productive. Look \begin{align}
D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)
=&
\left[
	\frac{\partial}{\partial \nu} 
	\left(
		\frac{\partial}{\partial \mu} (f\circ\varphi) 
	\right)
\right](a)
\\
=&
\left[
	\frac{\partial}{\partial \nu} 
	\left(
		\sum_{j=1}^{m}\mu_j\cdot \frac{\partial}{\partial x_j} (f\circ\varphi) 
	\right)
\right](a)
\\
=&
\sum_{i=1}^m\sum_{j=1}^{m}
\nu_i\cdot\mu_j\cdot \frac{\partial^2}{\partial x_i \partial x_j} (f\circ\varphi)(a) 
\\
\end{align} By $\frac{\partial}{\partial \mu} (f\circ\varphi)(a)= D(f\circ \varphi)(a) \cdot\mu =u= D(f\circ\psi)(b)\eta= \frac{\partial}{\partial \eta} (f\circ\psi)(b)$ we have \begin{align}
D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)
=&
\left[
	\frac{\partial}{\partial \nu} 
	\left(
		\frac{\partial}{\partial \mu} (f\circ\varphi) 
	\right)
\right](a)
\\
=&
\left[
	\frac{\partial}{\partial \nu} 
	\left(
		\frac{\partial}{\partial \eta} (f\circ\psi)
	\right)
\right](b)
\\
=&
\left[
	\frac{\partial}{\partial \nu} 
	\left(
		\sum_{j=1}^{m}\eta_j\cdot \frac{\partial}{\partial x_j} (f\circ\psi) 
	\right)
\right](b)
\\
=&
\sum_{i=1}^m\sum_{j=1}^{m}
\nu_i\cdot\eta_j\cdot \frac{\partial^2}{\partial x_i \partial x_j} (f\circ\psi)(b) 
\\
=&
D^{(2)}(f\circ\psi)(a)\cdot(\eta,\nu) 
\end{align} By an entirely analogous calculation and applying the Schwarz theorem we have $$
D^{(2)}(f\circ\varphi)(a)\cdot(\mu,\nu)= D^{(2)}(f\circ\psi)(a)\cdot(\mu,\zeta).
$$ QUESTION. The question then becomes the following. How to use the identities below \begin{align}
D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)=&D^{(2)}(f\circ\psi)(a)\cdot(\eta,\nu) 
\\
D^{(2)}(f\circ\varphi)(a)\cdot(\mu,\nu)=&D^{(2)}(f\circ\psi)(a)\cdot(\mu,\zeta).
\end{align} to prove the  well definiteness of $ D^{(2)}f$ ?","['real-analysis', 'multivariable-calculus', 'differential-topology', 'derivatives', 'differential-geometry']"
3095171,Showing that an estimator for covariance is consistent?,"I'm having trouble proving that a certain estimator is consistent. I know that to show an estimator is consistent, I have to show that the variance of the estimator approaches zero as n grows large/goes to infinity. The estimator for $Cov(X_i,Y_i)$ from a random sample $(X_i,Y_i)$ for $i = (1, ... , n)$ is as follows: $$\frac{1}{n}\sum_{i=1}^{n} (X_{i}-\overline{X})(Y_i-\overline{Y})$$ I let $\hat{\theta}$ be the estimator and thought that doing this would work: $$V(\hat{\theta}) = V(\frac{1}{n}\sum_{i=1}^{n} (X_{i}Y_{i}-X_{i}\overline{Y}-Y_{i}\overline{X}+\overline{X} \overline{Y}))$$ I expanded the terms but I have no idea how to go from here. Any help is greatly appreciated!","['statistics', 'covariance', 'variance']"
3095215,How to change the variables in $\frac{1}{(z+1)^{n+1}}=\frac{(-1)^n}{n!}\frac{d^n}{dz^n}\left( \frac{1}{z+1}\right)$?,We know that $$\frac{1}{(z+1)^{n+1}}=\frac{(-1)^n}{n!}\frac{d^n}{dz^n}\left( \frac{1}{z+1}\right).$$ If we put $z=e^t$ how can we change the variables above equation? I know that $z\frac{d}{dz}=\frac{d}{dt}$ . So how can we write $\frac{d^n}{dz^n}\left( \frac{1}{z+1}\right)$ ? Is $z^n \frac{d^n}{dz^n}=\frac{d^n}{dt^n}$ ?,"['calculus', 'derivatives']"
3095280,Checking if matrix $A$ is positive definite via Cholesky decomposition,How can we show that a matrix $A$ is not a positive definite matrix using the Cholesky decomposition? If we are not able to complete the algorithm and we cannot factor the matrix with a Cholesky decomposition does it then mean that the matrix is not positiv definite? Or is there an other way to check whether the matrix is positiv definite or not?,"['matrices', 'cholesky-decomposition', 'positive-definite', 'matrix-decomposition']"
3095296,Showing weighted average is consistent estimate,"Here's the problem statement: Let $X_1$ , . . . , $X_n$ be independent random variables with common mean $\mu$ and variances $σ_i^2$ . To estimate $μ$ , we use the weighted average $T_n$ = $\sum_{i=1}^n w_iX_i$ with weights $w_i$ = $\frac{\sigma_i^{−2}}{\sum_{i=1}^n \sigma_j^{-2}}$ Show that the estimate $T_n$ is consistent (in probability) if $\sum_{i=1}^n \sigma_j^{-2} \Rightarrow \infty$ .
End Problem. So I know that a consistent estimator is one that asymptotically approaches the parameter as the sample size goes to infinity. So by the problem statement, if the denominator of each of the weightings goes to infinity, then each of the $w_iX_i$ would just go to 0, yes? I'm a little confused on what this problem is trying to show. Thanks for any help!","['law-of-large-numbers', 'probability-theory', 'estimation']"
3095338,Different ways we can paint the faces of a tetrahedron using $4$ colors,"Suppose we want to paint the faces of a tetrahedron using $4$ different colors, assuming that we allow different faces to be painted with the same color. By not taking the symmetries of tetrahedron into account, there are $|X|=4^4=256$ ways. Now, when its symmetries are introduced, from Burnside's orbit-counting theorem : $$
r=\frac{1}{|G|}\sum_{g \in G}|X_g|
$$ where $G$ is the symmetry group of the tetrahedron (considering only orientation preserving symmetries), $X_g$ are the elements fixed by $g$ and $r$ is the number of orbits of $X$ under $G$ 's action. Therefore, we need to keep track of how many elements are fixed by every $g \in G$ . $\bullet \space $ The identity element keeps everything unchanged, so $X_e=256$ $\bullet \space $ Let $\rho^j_i$ denote the rotations about the vertex $i$ by $j$ degrees. For the element to stay fixed, the adjacent faces to this vertex must be of the same color. So $$|X_{\rho^{120}_1}|=|X_{\rho_1^{240}}|=\dots=|X_{\rho^{240}_4}|=4 \cdot4=16$$ $\bullet \space $ There are $3$ more symmetries left to examine, which are the $180^o$ rotations, $m_1,m_2, m_3$ . Elements stay fixed only if they have two pairs of similarly colored adjacent faces. Thus $$
|X_{m_1}|=|X_{m_2}|=|X_{m_3}|=4 \cdot 4=16
$$ Taking all this into account, we yield: $$
r=\frac{1}{12}(256+8 \cdot 16 + 3 \cdot 16)=\frac{432}{12}=36
$$ Therefore, there are $36$ different ways to color the faces of a tetrahedron using $4$ colors.","['proof-verification', 'abstract-algebra', 'combinatorics', 'group-theory', 'group-actions']"
3095352,Ideal in polynomial ring which contains no non-zero prime ideal,"Let $J$ be a non-zero ideal in $\mathbb C[X,Y]$ such that $J$ contains no non-zero prime ideal. Then is it true that $J$ has height $1$ ? Possible approach: Since $\mathrm{ht}(J^n)=\mathrm{ht}(J)$ for every $n>1$ so $\mathrm{ht}(J)=1$ iff $\mathrm{ht}(J^n)=1$ iff $J^n$ is contained in a proper principal ideal ... don't know where to go from here. For motivation see my comments to this question When the element-wise product of two ideals produces an ideal .","['noetherian', 'unique-factorization-domains', 'krull-dimension', 'algebraic-geometry', 'commutative-algebra']"
3095421,$X'$ finite-dimensional implies $X$ finite-dimensional,"How would one prove, for any normed space $X$ that if $X'$ is finite dimensional, then $X$ is finite-dimensional? Here $X'$ denotes the space of all bounded functionals $f: X \to\mathbb F$ If anyone would give me a hint, no need for full answer. I really don't know where to start. Appreciate all the answers.","['normed-spaces', 'functional-analysis', 'dual-spaces']"
3095434,Covariant derivative of a symmetric tensor,"Assume that a symmetric $(0,2)$ satisfies $$\nabla_iT_{jk}+\nabla_jT_{ik}+\nabla_kT_{ji}=0$$ where $T=T_i^i$ is constant and $\nabla_jT_{ik}\ne 0$ . What are the values of the constants $a,b,c$ such that $$a\nabla_iT_{jk}+b\nabla_jT_{ik}+c\nabla_kT_{ji}=0$$ Is there any difference if the tensor $T$ is the Ricci tensor. I think $(a,b,c)$ where $a=b=c$ are the only solution set to it however I failed to prove it nor to find the solution. Thanks in advance.","['tensors', 'tensor-products', 'riemannian-geometry', 'differential-geometry']"
3095435,"Alphabet with 6 vowels and 12 consonants, find the amount of words without two consonants in a row.","I just took an exam and as usual with exams, the answers come to you when you're done with the exam and you are sitting in your favourite chair at home. I want to verify my solution as part of my learning process to learn from my mistakes in case I might want to schedule a resit Consider an alphabet $A$ consisting of $6$ vowels and of $12$ consonants. Valid words consist of no two consonants in a row, so AART is not valid, nor is JUDITH, but JUDIT is fine and so is AAR, as is AIAIAIAIAIAIAIAIAI. $a_n$ denotes the amount of valid words. a) find $a_0$ , $a_1$ , $a_2$ , $a_3$ $a_0=1$ , the empty word $a_1=12+6=18$ (just one letter) For $a_2$ we considers words like $AT$ , $TA$ , $IA$ (different vowels) and $AA$ (same vowels) $a_2= 2 \times 6 \cdot 12 + 5 \cdot 6 + 6=144 +30 +6=180$ We expand to three symbols by either adding a vowel to the end of a 2-letter word or by adding a vowel and consonant to a 1-letter word $a_3=180 \cdot 6 + 6 \cdot 12 \cdot 18 =1080+1296=2376$ (b) Find a recurrence relation (c) solve it We make a case distinction for a valid word of length $n$ , it either ends in a consonant or in a vowel. If it ends in a consonant, we must have obtained it from a valid word of length $n-2$ by placing a vowel followed by a consonant behind it. In all other situations we simply place a vowel behind a word of length $n-1$ . We get for $n\geq 2$ : $$ a_n = 6 \cdot a_{n-1} +  6 \cdot 12 \cdot a_{n-2}$$ One can verify that this indeed gives $180$ for $a_2$ . We can solve this recursion via an auxiliary equation of the form: $$ r^2 = 6r + 6 \cdot 16 $$ $$ r^2 - 6r - 6 \cdot 16 =0$$ Which factorises as: $$ (r-12)(r+6)=0$$ So we get solutions $a_n = A r_1^n + B r_2^n$ : $$ a_n = A \cdot 12^n + B \cdot (-6) ^n$$ We can now plug in our initial conditions $a_0=1$ and $a_1=18$ $$1=A+B$$ $$ 18= 12A - 6B=18A -6 \implies 18A=24 \implies A=\frac{4}{3}, B=-\frac{1}{3}. $$ We get: $$ a_n =  \frac{4}{3}\cdot 12^n -\frac{1}{3} (- 6)^n$$ I feel that this is probably correct, but I am unsure. Can someone please verify?","['proof-verification', 'combinatorics', 'recurrence-relations']"
3095519,How do i calculate $\lim_{h\to0}\frac{f(a+h^2)-f(a+h)}{h}$?,"All do i know about this problem is that f can be derived in ""a"". What troubles me is the h squared,i just can't get rid of it or make it useful,no matter what i do, i always end up with it giving me an undefined limit, so it stays like that,any idea on how to get rid of it? or any rule i can use to make this easy?","['limits', 'calculus', 'derivatives']"
3095533,Proof by series of equalities,"Here’s my attempt and proving $(A-C)\cap (A \cap B) = \varnothing$ by a series of equalities but I’m not sure if I went about this right or where to go from here? Any help will be greatly appreciated.! My question is #3 under the algebraic proofs section. ( https://i.sstatic.net/SqA8H.jpg )
( https://i.sstatic.net/JtMFO.jpg )",['elementary-set-theory']
3095590,Demonstrate that these two Pell's equations have no integer solutions,"I would like to demonstrate that the following four Pell's equations have no integer solutions: $x^2-82y^2=\pm2$ $x^2-82y^2=\pm3$ I do realise that such problems are often solved by algebraic manipulations, reducing modulo prime numbers, and arriving at some contradiction. After some  blind trial and error fumbling in the dark with the above mentioned method, I have decided to consult the community. All help or input would, as always, be highly appreciated.","['number-theory', 'pell-type-equations', 'elementary-number-theory']"
3095630,Prove $\sqrt{b} - \sqrt{a} < \sqrt{b-a}$,"Prove that if $0 < a < b$ then $$\sqrt{b} - \sqrt{a} < \sqrt{b-a}$$ This is what I have so far: square both sides to get $a + b -2\sqrt{ab} < b-a$ subtract $b$ from both sides $a-2\sqrt{ab} < -a$ add $a$ to both sides $2a-2\sqrt{ab} < 0$ than add $2 \sqrt{ab}$ to both sides and get $2a < 2\sqrt{ab}$ divide by $2$ and get $a < \sqrt{ab}$ Thus we know $\sqrt{ab}$ is bigger than $a$ so that $\sqrt{a \cdot a} < \sqrt{ab}$ which means $\sqrt{a} < \sqrt{b}$ . Therefore together with the given $a < b$ , we have $\sqrt{b} - \sqrt{a} < \sqrt{b - a}$ I'm confused as to where I went wrong and how to fix this.","['algebra-precalculus', 'inequality']"
3095640,Graph of continuous functions,"Prove or disprove: There exists a family of continuous functions $f_n:[0,1]\rightarrow \mathbb{R}$ , $n\in \mathbb{N}$ with graphs $T(f_n)$ such that $[0,1]\times \mathbb{R}=\cup_{n\in \mathbb{N}}T(f_n)$ . I claim that there is no such a family of functions. By contradiction suppose there exists such a family of functions. So ${0}\times \mathbb{R}=\cup_{n\in \mathbb{N}} (0,f_n(0))$ . So $\mathbb{R}$ is a countable unions of real numbers. Contradiction. However, I didn't use the assumption of continuity in the problem. I'm doubtful my solution is correct.","['limits', 'functions', 'continuity', 'real-analysis']"
3095741,Doubt in the definition of principal bundle.,"I am following Kobayashi and Nomizu( Foundations of differential geometry) Volume 1. In page number 50 while defining principle $G$ -bundle $P(M,G)$ they said that the action of the Lie group $G$ is Free and didn't specify that the action is Proper . But they stated $M=P/G$ where $P/G$ is the quotient space of $P$ by the action of $G$ .  By equality I feel that they have assumed diffeomorphism between $M$ and $P/G$ and hence must have assumed a smooth manifold structure on $P/G$ . The first thing that came to my mind is the Quotient Manifold Theorem (QMT) in order to assume the smooth manifold structure on $P/G$ . But in the hypothesis of (QMT) the action of $G$ is assumed to be Proper. So technically we can't apply (QMT) to give a differentiable structure on $P/G$ unless we prove that the the given Free action of $G$ is actually a Proper action. My questions are following: 1 Do we have to prove that the action of $G$ is Proper or it is assumed to be proper so that we can apply (QMT)? 2 Or we dont have to use (QMT) and somehow in a different way we can give  a diffrentiable structure on $P/G$ ? 3 Or they have not assumed any differentiable structure on $P/G$ and by the equality $M= P/G$ they just mean a bijection between two sets and later they gave the smooth manifold structure on $P/G$ using the differentiable structure of $M$ ? Thanks in advance if somone can help me out!","['principal-bundles', 'fiber-bundles', 'smooth-manifolds', 'lie-groups', 'differential-geometry']"
3095781,Explicit computation of euler number of tangent bundle of sphere,"This is related to Bott-Tu Sec 11, Sphere Bundles. I would like to compute the euler characteristic of sphere bundle concretely. I have figured out the following so far. Let $S^k$ be the $k-$ sphere. Consider vector field $v$ starting from south pole to north pole by flowing. Clearly this gives a section to $T(S^k)$ . Now normalize this section by considering any riemannian metric $||\cdot||$ by $\frac{v}{||v||}$ and this gives a section on sphere bundle of $S^k$ with fiber $S^{k-1}$ away from 2 points(i.e. south and north pole). Since Euler number can be computed by sum of local degrees at south pole and north pole, I can compute the local degree at local trivializations of sphere bundle at south pole $a$ and north pole $b$ . WLOG, one can assume local trivializations look like $D(x)\times S^{k-1}$ where $D(x)$ is some open disk on sphere $S^k$ centered at $x$ . Then consider the following maps. $\partial D(x)\xrightarrow{s}D(x)\times S^{k-1}\xrightarrow{\pi_2} S^{k-1}$ and denote total map as $f_x$ . Pick volume form $\omega$ on $S^{k-1}$ where $\omega$ can be standard volume form. Since the degree is computed locally, one can consider $\int_{\partial D(x)}f_x^\star(\omega)=\int_{\partial D(x)}s^\star\psi$ where $\psi$ is the global angular form defined over $E$ and $x$ can be either south or north pole. This follows from $f_x^\star$ defines form restricts cohomology generator on each fiber and thus same cohomology class as global angular form. Hence they differ by a total differential and then one can apply stoke theorem to see the integral agrees.(Here I have already assumed the support of $\psi$ is away from $D(x)$ and one can shrink the size of disk if necessary.) If I denote $s(x)=(x,v(x))$ with $v(x)\in S^{k-1}$ , then $\pi_2\circ s(x)=v(x)$ . Hence $\int_{\partial D(x)}f_x^\star(\omega)=\int_{\partial D(x)} v^\star(\omega)$ . I can take $\omega$ to be standard volume form of $k-1$ sphere. $\textbf{Q:}$ Now I do not see an obvious way to compute this integral though I can write down the formula. How do I proceed further? Ref. Bott-Tu Differential Forms in Algebraic Topology, Sec 11, Thm 11.16, Exercise 11.21","['general-topology', 'geometry', 'algebraic-topology', 'differential-geometry']"
3095790,General formula for $\int_0^{\pi/2} \tan^{\alpha}(x) dx$?,There are already questions about how to find $\int \tan^{1/2}(x) dx$ . But how to derive a general formula for $\int_0^{\pi/2} \tan^{\alpha}(x) dx$ (which converges if $|\alpha|<1$ ) ? More details: I have to evaluate this integral because I want to derive Euler's reflection formula for gamma functions. The integral above is the result of using a substitution $v=\tan^2(x)$ in $\int_0^\infty \frac{v^\beta}{1+v} dv$,"['integration', 'calculus', 'improper-integrals', 'gamma-function']"
3095798,Sobolev embedding for the $L^q$ norm.,"Suppose $f \in H^1(\mathbb R^2)$ , where $H^1$ is the Sobolev space, then how to use this information to bound $\Vert f \Vert_{L^q}$ , where $q>2$ ? It seems like Sobolev embedding, but it's not.","['real-analysis', 'fractional-sobolev-spaces', 'sobolev-spaces', 'partial-differential-equations', 'regularity-theory-of-pdes']"
3095800,A Topological Invariant for $\pi_3(U(n))$,"Recently, I saw a construction of topological invariant for $\pi_3(U(n))$ with $n\geq 2$ : $$
N=\frac{1}{24\pi^2}\int_{S^3} d^3x\ \epsilon^{ijk} Tr[(U^{-1}\partial_{x_i}U)(U^{-1}\partial_{x_j}U)(U^{-1}\partial_{x_k}U)]\ ,
$$ where $U\in U(n)$ depends on $\boldsymbol{x}=(x_1,x_2,x_3)\in S^3$ , $\epsilon^{ijk}$ is the Levi-Civita symbol, $i,j,k=1,2,3$ , and the duplicated indexes are summed over. It is claimed that $N$ is an integer, but why? Update 02/02/2019 I think I got an argument for $n=2$ . In this case, $U=e^{i \varphi} q$ with $q\in SU(2)$ .
Due to the trace and the Levi-Civita symbol in $N$ , $\varphi$ does not contribute to $N$ . As $Tr[q^{\dagger}\partial_i q]=0$ and $(q^{\dagger}\partial_i q)^{\dagger}=-q^{\dagger}\partial_i q$ , $q^{\dagger}\partial_i q$ in geneeral has the form $q^{\dagger}\partial_i q=i(A_i \sigma_x +B_i \sigma_y+C_i \sigma_z )$ with $A_i,B_i,C_i$ real and $\sigma$ 's being the Pauli matrices. As a result, we have $$
N=\frac{1}{2\pi^2}\int_{S^3} d^3x\ \epsilon^{ijk} A_i B_i C_i\ .
$$ Furthermore, $SU(2)$ is diffeomorphic to $S^3$ , and thus $q$ can be parametrized as the three angles $\boldsymbol{\Omega}=(\psi,\theta,\phi)$ of $S^3$ . By choosing the right convention, $N$ can be further written as $$
N=\frac{1}{2\pi^2}\int_{S^3} d^3x \left|\frac{\partial\boldsymbol{\Omega}}{\partial\boldsymbol{x}}\right| \sin(\theta)\sin^2(\phi)\ ,
$$ where $\left|\frac{\partial\boldsymbol{\Omega}}{\partial\boldsymbol{x}}\right|$ is the determinant of the Jacobian matrix. 
Clearly, $N$ indicates how many times the $\boldsymbol{x}$ - $S^3$ warps around the $\boldsymbol{\Omega}$ - $S^3$ . For $n>2$ , I may guess that some $SU(2)$ subgroup of $U(n)$ accounts for $N$ , but I can be wrong.","['algebraic-topology', 'multivariable-calculus', 'vector-bundles', 'differential-topology', 'lie-groups']"
3095816,Does the box topology have a universal property?,"Given a set of topological spaces $\{X_\alpha\}$ , there are two main topologies we can give to the Cartesian product $\Pi_\alpha X_\alpha$ : the product topology and the box topology.  The product topology has the following universal property: given a topological space $Y$ and a family $\{f_\alpha\}$ of continuous maps from $Y$ to each $X_\alpha$ , there exists a continuous map from $Y$ to $\Pi_\alpha X_\alpha$ .  Now the box topology does not have this universal property, but my question is, does it have some other universal property? On a related note, does there exist some category whose objects are topological spaces and whose morphisms are something other than continuous maps, such that the Cartesian product endowed with the box topology is the correct product object in that category?","['box-topology', 'product-space', 'category-theory', 'universal-property', 'general-topology']"
3095843,Can the Gauss-Bonnet theorem be proven from Stokes's theorem?,"In a comment to this question , John Ma claims that the Gauss-Bonnet theorem can be proven from Stokes's theorem, but does not explain how. For two dimensions, Stokes's theorem says that for any smooth 2-manifold (i.e. surface) $S$ and one-form $\omega$ defined on $S$ , $$\oint_{\partial S} \omega = \iint_S d\omega.$$ I could vaguely imagine coming up with some kind of one-form $\omega$ that depends on the metric, such that (a) along the boundary curve $\omega$ maps the boundary tangent vector to the geodesic curvature and (b) in the surface interior $\ast d\omega$ equals the Gaussian curvature. (In more concrete vector-field language, this corresponds to a vector field $\vec{\omega}$ defined over the surface such that (a) on the boundary curve $\vec{\omega} \cdot d\vec{l}$ equals the curve's geodesic curvature and (b) in the surface interior $(\vec{\nabla} \times \vec{\omega}) \cdot d\vec{S}$ equals the Gaussian curvature.) This would reproduce part of the Gauss-Bonnet formula, but how could you possibly get out the Euler characteristic term?","['stokes-theorem', 'algebraic-topology', 'differential-geometry']"
3095845,Finding a continuous branch of $F(z)=\sqrt{\frac{(z^2-1)(z-2)}{z}}$,"I am having problems understanding branch cuts. For instance, I am given the following function, for $z \in \mathbb{C},$ let $$F(z)=\sqrt{\frac{(z^2-1)(z-2)}{z}}=\sqrt{\frac{(z+1)(z-1)(z-2)}{z}}.$$ Determine if there is a continuous branch $f$ of $F,$ with $\Re{(f(i))}>0$ , that is defined on $\mathbb{C} \setminus \{ [-1,0] \cup [1,2] \}.$ I have seen examples where the function is written using the principal value of the square root function, and then the signs of the principal values are determined to fit the case at hand, but it seemed arbitrary. I have no idea how to proceed. How does the process of finding a continuous branch works?","['complex-analysis', 'branch-cuts']"
3095900,"What is the formal definition of ""probability distribution""?",Can someone please provide a useful reference on the definition of probabilistic distribution. A very popular site (top of Google search) states: A probability distribution is a table or an equation that links each outcome of a statistical experiment with its probability of occurrence. https://stattrek.com/probability-distributions/probability-distribution.aspx I feel that this definition is very unsatisfactory. I need a better one with a reference. Thank you!,"['probability-distributions', 'reference-request', 'definition', 'probability-theory', 'probability']"
3095903,Is there any other way to establish this trig identity? $\frac{\sec(x) + 1}{\tan(x)} = \frac{\sin(x)}{1 - \cos(x)} $,"I needed to verify this trig identity: $$\frac{\sec(x) + 1}{\tan(x)} = \frac{\sin(x)}{1 - \cos(x)} $$ what I did is I worked on both sides individually: LHS: $$\frac{\frac1{\cos(x)} + 1}{\frac {\sin(x)}{\cos (x)}} =\frac{1 + \cos(x)}{\sin(x)}$$ RHS: $$\frac{\sin(x)}{1 - \cos(x)}=\frac {\sin(x)(1-\cos^2(x))}{1 -\cos^2(x)}=\frac {\sin(x)(1-\cos^2(x))}{\sin^2(x)}= \frac{1 +\cos(x)}{\sin(x)} $$ QED. I can't think of other way to verify this, like just working on one side. Is there any?","['algebra-precalculus', 'trigonometry']"
3095915,"If $f(x)=\int_{0}^{x}\sqrt {f(t)}dt$, then find $f(6)$.","Let $f:[0,\infty)\to[0,\infty)$ be continuous on $[0,\infty)$ and differentiable on $(0,\infty)$ . If $f(x)=\int_{0}^{x}\sqrt {f(t)}dt$ , then find $f(6)$ . $$f(x)=\int_{0}^{x}\sqrt {f(t)}dt$$ $$g(x):=\sqrt {f(x)}\implies(g(x))^2=f(x)=\int_{0}^{x}g(t)dt\implies2g(x)g'(x)=g(x)$$ [by FTC-1] $$\implies g=0 \vee g'(x)=\frac{1}{2} $$ $$(g(0))^2=f(0)=0\implies g(x)=\frac{x}{2}\vee g=0 \implies f(6)=9 \vee f(6)=0$$ Is this correct? Also, how do I rule out $f(6)=0$ since my source only gives $9$ as the answer. I found this post after writing mine but I still don't think that $f(6)=0$ can be ruled out.","['integration', 'continuity', 'definite-integrals', 'derivatives']"
3095922,Is there a simple combinatoric interpretation of this identity? [duplicate],"This question already has answers here : Combinatorial proof of summation of $\sum\limits_{k = 0}^n {n \choose k}^2= {2n \choose n}$ (9 answers) Closed 5 years ago . I came across an exercise in which we are asked to prove the identity: $${2n\choose n}=\sum_{k=0}^n{n\choose k}^2$$ The exercise gives the hint: $$\left(1+x\right)^{2n}=\left[(1+x)^n\right]^2$$ It's not too difficult to use the hint to prove the identity (the expressions in the identity are the coefficients of $x^n$ in the respective expansions of the expressions in the hint, which of course must be the same number), but I was wondering whether there is a neater equivalent-counting interpretation... It's clear that ${2n \choose n}$ is the number of ways in which we can choose half the elements in a set (where this is possible): how can we interpret $\sum_{k=0}^n{n\choose k}^2$ equivalently?","['binomial-coefficients', 'combinatorics']"
3095984,Neither open nor closed subspace of a vector space?,Consider a vector space $V$ over $\mathbb{C}$ with some norm (and topology induced by that norm). I am trying to find a subspace $W \subset V$ such that it is neither open nor closed in the topology mentioned above? Motivation: I have studied that every banach space is essentially a closed subspace of $\mathcal{C}(X)$ for some compact Hausdorff space $X$ and hence I thought that we could also have some structure for normed linear spaces if the answer to my question is negative.,"['general-topology', 'normed-spaces']"
3095997,What functions satisfy $f(x)+f(y-x) = g(y)$?,"What functions $f$ have the property that, for all $x$ and $y$ : $$
f(x) + f(y-x) = g(y)
$$ i.e., the sum does not depend on $x$ ? Linear functions obviously this property: if $f(x) = ax+b$ , then: $$
f(x) + f(y-x) = ay+2b = g(y).
$$ On the other hand, if we look only at differentiable functions, then only linear functions have this property. By taking the derivative of the first equation as a function of $x$ : $$
f'(x) - f'(y-x) = 0 \iff f'(x)=f'(y-x)
$$ Since this is true for every $y$ , $f'(\cdot)$ must be a constant function, so $f$ must be linear. Are there non-differentiable functions with this property?",['functions']
3096021,Is there a simple abelian group $G$ with infinite order?,"I am reading ""An Introduction to Algebraic Systems"" by Kazuo Matsuzaka. There is the following problem in this book: On p.80 Problem 8: Show that a simple abelian group $G \neq \{e\}$ is a cyclic group whose order is prime. Did the author intend the following problem? If this problem were the following, I could solve it: Problem 8': Show that a simple finite abelian group $G \neq \{e\}$ is a cyclic group whose order is prime. Proof: Because $G \neq \{e\}$ , there is an element $g \in G$ which is not equal to $e$ . $H := \{g^i | i \in \mathbb{Z}\}$ is a subgroup of $G$ and $G$ is abelian. So $H$ is a normal subgroup of $G$ . And $G$ is simple. And $H \ni g \ne e$ . So $H = G$ . Let $n := \#H = \#G$ . Then, $n$ is prime. If $n$ is not prime, then we can write $n = d d'$ , $1 < d < n$ , $1 < d' < n$ . Then $H' := \{(g^d)^i | i \in \mathbb{Z}\}$ is a subgroup of $G$ whose order is $d'$ . So, $H'$ is neither $\{e\}$ nor $G$ . Becasue $G$ is abelian, $H'$ is a normal subgroup of $G$ . And $G$ is simple. This is a contradiction.","['simple-groups', 'group-theory', 'abstract-algebra']"
3096040,Function such that $f^{(n)}(x) = \frac{x}{f(x)^n}$,"Let $n$ be a fixed positive integer. Find all function $f:(0, \infty) \to \mathbb{R}$ that can be differentiated $n$ times such that $f^{(n)}(x) = \frac{x}{f(x)^n}$ if $f^{(n)}(x)$ is the $n$ -th derivative of $f$ . I tried to differentiate the given identity and I wrote $$f^{(n+1)}(x)=\frac{f(x)^n-nxf(x)^{n-1}f'(x)}{f(x)^{2n}}=\frac{f(x)-nxf'(x)}{f(x)^{n+1}}=\frac{1}{f(x)^n}-\frac{nxf'(x)}{f(x)^{n+1}}$$ I tried to connect this with $f^{(n)}(x)$ but the relations didn't get to anything. Also, if $n=2$ , I am not able to find any example of a function $f$ that satisfies the equation. It appears from the comments that the solutions are very complicated. Does the problem become more easy if we replace $f(x)^n$ with $f^n(x)=(f \circ f \circ...\circ f)(x)$ ?","['calculus', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
3096049,Proving $\epsilon$-$\delta$ convergence of Lebesgue integrals just from the definition,"I am pretty lost with how to do this. The problem is to show that, in a measure space $(X,M,\mu)$ with a measurable function $f: X \to [0,\infty)$ such that $\int_X f d\mu < \infty$ . Then, for all $\epsilon > 0$ , there is a $\delta > 0$ such that for all $E \in M$ if $\mu(E) < \delta$ then $$\int_E f d\mu < \epsilon$$ Apparently, I should do this from just the definition, which is $$\int_Efd\mu = \sup_{0 \leq s \leq f} \int_E s d\mu$$ and $$\int_Esd\mu = \sum_{j=1}^n \alpha_j \mu(A_j \cap E)$$ But I have no idea how to continue","['general-topology', 'lebesgue-integral', 'measure-theory', 'real-analysis']"
3096105,Irreducible vs. indecomposable representation,"I'm currently reading Serre's Linear Representations of Finite Groups , and I'm kind of confused regarding the concepts of irreducibility and indecomposability. If I'm understanding it correctly, an irreducible representation is a representation $(\rho, V)$ of a group $G$ which does not has a non-trivial subrepresentation (i.e. a representation $(\rho|_W, W)$ where $W\subseteq V$ is a $G$ -stable subspace). While an indecomposable representation is a representation that is not isomorphic to any direct sum of other representations. Wikipedia and other sources say that irreducibility implies indecomposability, which seems logical (not 100 % sure why though), while Serre says the following: Let $\rho:G \rightarrow GL(V) $ be a linear representation of $G$ . We say that it is irreducible or simple if $V$ is not $0$ and if no vector subspace of $V$ is stable under $G$ , except of course $0$ and $V$ . By theorem I [which says that there exists a $G$ -stable complement $W^0$ of a $G$ -stable subspace $W\subseteq V$ ], this second condition is equivalent to saying $V$ is not the direct sum of two representations. Does this not mean that irreducibility is equivalent indecomposability, and thereby going against what Wikipedia says? Thanks! P.S. I'm getting kind of annoyed at this book for being ""to concise"" and skipping a lot of details. Does anyone have any other recommendations on introductory books on representation theory?","['representation-theory', 'group-theory', 'abstract-algebra', 'linear-algebra']"
3096123,Convergence/Divergence of infinite series $\sum_{n=1}^{\infty} \frac{(\sin n+2)^n}{n3^n}$,"$$ \sum_{n=1}^{\infty} \frac{(\sin n+2)^n}{n3^n}$$ Does it converge or diverge? Can we have a rigorous proof that is not probabilistic? For reference, this question is supposedly a mix of real analysis and calculus.","['convergence-divergence', 'calculus', 'sequences-and-series', 'real-analysis']"
3096262,value of $x$ in Trigonometric equation,Find real $x(0<x<180^\circ)$ in $\tan(x+100^\circ)=\tan(x-50^\circ)+\tan(x)+\tan(x+50^\circ)$ what i try $\displaystyle \tan(x+100^\circ)-\tan(x)=\tan(x+50^\circ)+\tan(x-50^\circ)$ $\displaystyle \frac{\sin(100^\circ)}{\cos(x+100^\circ)\cos x}=\frac{\sin(2x)}{\cos(x+50^\circ)\cos(x-50^\circ)}$ $\displaystyle \frac{\sin(100^\circ)}{\cos(2x+100^\circ)+\cos(100^\circ)}=\frac{\sin(2x)}{\cos(2x)+\cos(100^\circ)}$ How do i solve further Help me please,['trigonometry']
3096324,Solving $2\sin\theta\cos\theta + \sin\theta = 0$,"The question is to solve the following question in the range $-\pi \le \theta \le  \pi$ $$2\sin\theta\cos\theta + \sin\theta = 0$$ I missed the obvious sin factorisation so proceeded as below. I see the correct solutions should be $\pm2/3\pi$ and the values when $\sin\theta = 0$ . Although I missed the early factorisation I don't know what I'm doing to actually arrive at an incorrect answer: $$\begin{align}
2\sin\theta\cos\theta + \sin\theta &= 0 \qquad\text{(square)} \tag{1} \\
4\sin^2\theta\cos^2\theta + \sin^2\theta &= 0 \tag{2}\\
4\sin^2\theta(1-\sin^2\theta) + \sin^2\theta &= 0 \tag{3} \\
4\sin^2\theta - 4\sin^4\theta + \sin^2\theta &= 0 \tag{4} \\
5\sin^2\theta  - 4\sin^4\theta &= 0 \tag{5}
\end{align}$$ and then solving by substitution/the quadratic equation I get $\sin\theta = \pm\sqrt(5)/2$ and $0$ but as this out of bounds for sin so cannot be the answer. I know using Symbolab that this solution is correct for the quadratic I've generated, so I must be going wrong somewhere above after missing that factoristion. I feel like it is in the squaring step but not sure what would be wrong here... Thanks a lot for your help.",['trigonometry']
3096326,Conditional expectation of asymptotically independent random variables,"Suppose that $W_n \to W_{\infty}$ a.s.   where $W_{\infty}$ is independent of random variable $V$ .   Moreover, suppose that $E[|V|]<\infty$ . Is it true that \begin{align}
\lim_{n \to\infty} E[V|W_n]=  E[V|W_\infty]=E[V] \text{ a.s.}
\end{align} The last equality is of course trivial. Therefore, I am looking the proof of the first.   This looks like some kind of continuity result, and I am actually not sure if it holds.   If the result doesn't hold, I would like to know what extra conditions can be added for it to hold.","['conditional-expectation', 'probability-theory']"
3096330,Show that $Y_n := (\prod_{i=1}^{n} X_i)^{1/n}$ converges with probability 1,"I'm dealing with a problem about stochastic and statistics and hope some of you can help me! On $[0,1]$ we have a sequence of independent, equally distributed probability variables $(X_n)_{n \in \mathbb{N}}$ . I have to show: a) $Y_n := (\prod_{i=1}^{n} X_i)^{1/n}$ converges with probability $1$ . b) calculate the exact limit of $Y_n$ I've already done some calculations, but I'm really not sure, whether everything is fine. Some pre-considerations:
To get rid of the product I took the logarithm: $\ln(Y_n) = \frac{1}{n} \sum_{i=1}^{n} \ln(X_i)$ After taking the logarithm the sequence $\ln(X_i)$ still is equally distributed and independent. a) I found a theorem in my lecture notes, which states, that $\frac{S_n}{n}$ (the $n$ -th partial sum of a sequence) converges and has a finite limit with probability $1$ if the sequence is integrable. It seems to me, that this Theorem might fit, but my concern is, that the logarithm of $X_i = 0$ (allowed since $X_i$ is a sequence on $[0,1]$ ) isn't integrable. b) This part some kind of ""smells"" to me like Kolmogorov's law, which states that for a sequence of indempendent and identically distributed probability variables with finite expectation value it holds: $$\lim_{n\rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n}X_k = \mathbb{E}(X_1) \quad \text{a.s.}$$ So the limit would be $\lim_{n} \ln(Y_n) = \mathbb{E}(\ln(X_1))$ almost sure. But I don't see, why the expectation value of $\ln(X_i)$ should be finite for $X_i = 0$ again. So due to this concerns at $X_i = 0$ I'm not sure, whether I'm on the right track, or the problem needs to be solved differently. I would be very grateful if some of you can help me! Thanks in Advance! pcalc","['convergence-divergence', 'probability-theory']"
3096347,"Given that A and B are sets such that A ⊆ B, write down P(A) − P(B) in a simplified form","The question is: Given that A and B are sets such that A ⊆ B, write down P(A) − P(B) in a simplified form I'm not sure what they mean by simplified form",['elementary-set-theory']
3096397,Sum of Beta Random Variables,"Supose I have $I$ independently (but not necessarily identically) distributed beta random variables, $X_i = \text{beta}(\alpha_i,\beta_i)$ , for $i=1,\dots,I$ . Is there a known distribution for the sum of these r.v.s, $Y=\sum_i X_i$ ? Furthermore, is there a known distribution for the mixture of these r.v.s, as in $Y=\sum_i \omega_i X_i$ , where $\sum_i \omega_i = 1$ ?","['statistics', 'probability-distributions', 'random-variables']"
3096411,"A Suitable Function for Terrain, Mountain Modeling","On Google Maps and various other mapping programs, one can see contour lines that correspond to elevation. Sometimes these contour lines are concentric corresponding to a mountain. My question is what would be the most suitable function to represent such data? I thought the simplest choice would be multiple Gaussian ""bumps"" placed additively on a map; whose composition would define a certain map? Are there any other functional representation methods that could give this output? My next goal would be fitting such function to a set of elevation data collected on a grid.","['contour-integration', 'functions', 'surfaces', '3d']"
3096419,Find sum $u_0 u_1 + u_1u_2+...+u_{n-2}u_{n-1} $,I have $$ u_k = \cos\frac{2k\pi}{n} + i \sin\frac{2k\pi}{n}$$ And I should calculate: $$ u_0 u_1 + u_1u_2+...+u_{n-2}u_{n-1}+u_{n-1}u_0 $$ But I have stucked: Firstly I calculate $$u_0 u_1 + u_1u_2+...+u_{n-2}u_{n-1} $$ and put $$ \alpha_k = u_k \cdot u_{k+1} = ... = e^{\frac{i\pi(2k+1)}{n}} $$ and sum $$ \alpha_0 + ... + \alpha_{n-2} = ... = e^{\frac{i\pi}{n}} \cdot \frac{1-e^{\frac{2(n-1)i\pi}{n}}}{1-e^{\frac{2i\pi}{n}}}$$ and I don't know how ti finish that. If it comes to $$u_{n-1}u_0  =  e^{i\pi} = -1 $$,"['complex-analysis', 'summation']"
3096435,Dimension image of morphism of projective varieties,Let $f: \mathbb{P}^n \to \mathbb{P}^m$ be a rational map. Then there exists $U \subset \mathbb{P}^n$ open so that $f_{|U}$ is a morphism. What can we say about the dimension of $\overline{f(U)}$ ? We have that $\dim\mathbb{P}^n = \dim U$ and $\dim f(U)= \dim\overline{f(U)}$ . Can I conclude that $n=\dim U \geq \dim\overline{f(U)} $ by the surjectvity of $f : U \to f(U)$ ? I think that's not true because $U$ in general is not a projective variety and $f$ need not be a morphism on $\overline{U}$ . How can I proceed? Can I find more information assuming that $U \subset \mathbb{A}^n$ ? Thanks for the help!,"['algebraic-geometry', 'dimension-theory-algebra', 'projective-space']"
3096455,topology induced by function from set to power set?,"tl;dr: I've tried to construct a different way to formalize ""topological spaces"" than via open sets or neighborhoods. I have not seen this approach but it may have been done before. The definition as it currently stands is not satisfactory (see discussion below). t-topologies Consider a pair $(X,t)$ where $t:X\to \mathcal P(\mathcal P(X))$ , i.e. $t(x)\subseteq \mathcal P (X)$ (which intuitively gives for every $x$ the set of subsets of $X$ that ""x touches"") satisfies the following axiom: Axiom 1 . For any $x\in X$ and any $U\subseteq S\subseteq X$ , if $U\in t(x)$ then $S\in t(x)$ . (intuitively, if $x$ ""touches"" the set $U$ , then it also touches the same set with arbitrary additions to it). Axiom 2 . For any $x\in X$ and any $U,V\subseteq X$ , if $U,V\notin t(x)$ then $U\cup V\notin t(x)$ (easily extended to infinite unions). (intuitively, if $x$ doesn't touch either of the sets $U,V$ , then it also doesn't touch the combined set). Call this pair $(X,t)$ a "" touch-topology "" or ""t-topology"" (because they're defined via ""touch-relations"" rather than open sets). Such a pair induces a topology: $(X,T)$ , where $U\in T$ iff $\forall x\in U, U^c\notin t(x)$ . (Intuitively, a set is open if none of its members touch anything outside the set). Indeed this is a topology: the empty set obviously is in $T$ . for any $U_i\in T$ , we know that for all $x\in U_i$ , $U_i^c\notin t(x)$ , therefore since $(\bigcup _j U_j)^c\subseteq U_i^c$ we have $(\bigcup _j U_j)^c \notin t(x)$ by axiom 1. (intuitively, if $x$ touches nothing outside $U_i$ , then it also doesn't touch anything outside $\bigcup_j U_j$ ). for any $U,V\in T$ , we know that for any $x\in U\cap V$ we have $U^c\notin t(x)$ and $V^c \notin t(x)$ . Hence by axiom 2, $U^c\cup V^c = (U \cap V)^c\notin t(x)$ . Hence $U \cap V\in T$ . (easily extended to infinite intersections). Definition . Let $\mathcal X=(X,t_X)$ , $\mathcal Y=(Y,t_Y)$ be t-topologies. A function $f:X\to Y$ is touch-continuous at $x$ if for all $U\in t_X(x)$ it holds that $f(U) \in t_Y(f(y))$ . (intuitively, a function is touch-continuous if it doesn't change the things that a point touches). Theorem. Let $\mathcal X= (X,t_X), \mathcal Y = (Y,t_Y)$ be touch-topologies. If a function $f:X \to Y$ is t-continuous at $x$ then it is also ""classically"" continuous at $x$ in the topologies $(X,T_X), (Y,T_Y)$ induced by $\mathcal X$ and $\mathcal Y$ . proof. Assume $U\in T_Y$ (an open set). We must show that $f^{-1}(U)\in T_X$ . Since $U\in T_Y$ , we know that for all $y\in U$ , $U^c\notin t_Y(y)$ . Hence by t-continuity, $f^{-1}(U^c)\notin t_X(x)$ for all $x\in f^{-1}(U)$ . Hence $f^{-1}(U)\in T_X$ . I don't think we can show that continuity implies t-continuity, though I'm not sure. What's missing from this approach. My intention with this was to represent a topology according to the sets that an object ""touches"", rather than according to ""open sets"" (i.e. sets whose elements don't touch the outside). This approach is similar to the ""neighborhood"" definition of topology. However, my approach is not complete, in the sense that we cannot turn any arbitrary topology into a unique t-topology. That is, for any topology, there are multiple t-topologies (non-isomorphic) that induce that topology: for example, consider the topology induced by a directed graph $(X,R)$ ( $R$ is a relation on $X$ denoting the arrows of the graph), where the t-topology is: given a node $x$ , $U\in t_X(x)$ iff $U$ contains at least one node $y$ such that $Rxy$ . It is not hard to show that the t-topology of the transitive closure of any graph induces the same topology as the t-topology of the graph itself. My questions are: Has this approach been done before? Can we add a third axiom so that there is a bijection between topologies and t-topologies? And so that t-continuity and ""classical"" continuity are equivalent? are there any problems with this approach?",['general-topology']
3096457,Prove $\operatorname{rank }(A) \le 4$ if $Ax_1=\cdots=Ax_7$ and $x_i$s are linearly independent,"Let $A \in \mathbb R^{10\times10}$ , $x_{1},x_{2},...,x_{7}\in \mathbb R^{10}$ which are linearly independent vectors and $Ax_{1}=Ax_{2}=\cdots=Ax_{7}$ . Prove that $ \operatorname{rank} (A) \le 4$ . I know that it is truth because if I have seven linearly independent vectors and I multiply a matrix with these vectors I must have at least $6$ zero rows because I have $Ax_{1}=Ax_{2}=\cdots=Ax_{7}$ . For example if I have $5$ zero rows then this vectors are not linearly independent. However I don't know how to prove it in an elegant way.","['matrix-rank', 'linear-algebra']"
