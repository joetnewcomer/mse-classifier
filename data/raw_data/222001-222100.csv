question_id,title,body,tags
4550711,Probability a coin is heads given 5 heads and distribution,"A machine produces a weighted coin that lands on heads with an unkown
probability $p$ , and we know that $P(p\le x) = x^4$ .  You flip the
coin $5$ times, and every time it lands on heads.  What is the
probability that the next flip will also land on heads? I am not exactly sure how to tackle this problem. I attempted going a conditional probability route, but was not sure what to put as the values for the numerator and denominator. It looks like the probability the coin is less than x is a cumulative distribution function, but I don't know if that intuition is correct or helpful. How should I go about solving this?","['conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
4550777,How to prove $(1 + x)^{n} \leq 1 + 2nx$,"I am currently working on a math problem, and it boils down to proving $(1 + x)^{n} \leq 1 + 2nx$ , for a small $x$ By the Binomial expansion, it is clear that $$(1 + x)^n= 1+ nx + \frac{n(n-1)}{ 2!}x^2 + \frac{n(n-1)(n-2)}{3!}x^3 + \cdots$$ However, how can we prove that $$nx \geq \frac{n(n-1)}{ 2!}x^2 + \frac{n(n-1)(n-2)}{3!}x^3 + \cdots $$ which would prove my claim. Any ideas?","['analysis', 'real-analysis', 'taylor-expansion', 'polynomials', 'inequality']"
4550795,"Let $X_1, X_2$ be independent random variables, then $\mathbb{E}[X_1]$ increases implies $\mathbb{E}[\max(X_1,X_2)]$ increases?","Suppose I have independent random variables $X_1,X_2$ taking values on $[0,1]$ . $X_1$ follows a cdf $P_1(x)$ and $X_2$ follows a pdf $P_2(x)$ . Then \begin{equation}
\mathbb{E}[X_1] = \int_0^1 xdP_1(x) = \int_0^1 xP'_1(x)dx
\end{equation} and \begin{equation}
\mathbb{E}[\max(X_1,X_2)] = \int_0^1 xd(P_1(x)P_2(x)) = \int_0^1 x(P_1(x)P'_2(x) + P'_1(x)P_2(x))dx
\end{equation} Now, suppose that I replace the distribution for $X_1$ by $\tilde{P}_1$ with higher $\mathbb{E}[X_1]$ . Is it true that $\mathbb{E}[\max(X_1,X_2)]$ also increase? i.e. \begin{equation}
\int_0^1 xd\tilde{P}_1(x) \geq \int_0^1 xdP_1(x) \implies \int_0^1 xd(\tilde{P}_1(x)P_2(x)) \geq \int_0^1 xd(P_1(x)P_2(x))?
\end{equation} How do I show this? Thanks!","['calculus', 'probability-theory', 'probability']"
4550796,Rationalize a denominator with 3 nth roots.,"How would you rationalize a denomiator with $3$ or more arbitary roots, like $\frac{1}{\sqrt{5}+\sqrt[3]{2}+\sqrt[7]{3}}$ ? I knew an absolutely awful formula for $3$ cube roots, but aside from that I could only find special cases. Does there exist a general formula? If so, what is it? (I know this question is pointless, but still.) Apparently it's really difficult to do this by hand for arbitrary radicals. I posted this a long time ago, and since then, I've found a few new formulas through experimentation. $$\frac{1}{\sqrt{a}+\sqrt{b}+\sqrt{c}}=\frac{\left(\sqrt{a}-\sqrt{b}-\sqrt{c}\right)\left(2\sqrt{bc}+a-b-c\right)}{\left(a-b-c\right)^{2}-4bc}$$ $$\frac{1}{\sqrt[3]{a}+\sqrt[3]{b}+\sqrt[3]{c}}=\frac{\left(\sqrt[3]{a^{2}}+\sqrt[3]{b^{2}}+\sqrt[3]{c^{2}}-\sqrt[3]{ab}-\sqrt[3]{ac}-\sqrt[3]{bc}\right)\left(\left(3\sqrt[3]{abc}+a+b+c\right)^{2}-3\left(a+b+c\right)\sqrt[3]{abc}\right)}{\left(a+b+c\right)^{3}-27abc}$$ $$\frac{1}{\sqrt[4]{a}+\sqrt[4]{b}+\sqrt[4]{c}}=\frac{\left(\sqrt[4]{a}+\sqrt[4]{b}-\sqrt[4]{c}\right)\left(\sqrt[4]{a}-\sqrt[4]{b}+\sqrt[4]{c}\right)\left(\sqrt[4]{a}-\sqrt[4]{b}-\sqrt[4]{c}\right)\left(2\sqrt{bc}-2\sqrt{ab}-2\sqrt{ac}-a-b-c\right)\left(\left(12a+4b+4c\right)\sqrt{bc}+\left(a-b-c\right)^{2}+4bc\right)}{\left(12a+4b+4c\right)^{2}bc-\left(\left(a-b-c\right)^{2}+4bc\right)^{2}}$$ $$\frac{1}{\sqrt[5]{a}+\sqrt[5]{b}+\sqrt[5]{c}}=\frac{2abc\left(\left(\sqrt[5]{a^{2}}+\sqrt[5]{b^{2}}\right)\sqrt[5]{ab}+\left(\sqrt[5]{a^{2}}+\sqrt[5]{c^{2}}\right)\sqrt[5]{ac}+\left(\sqrt[5]{b^{2}}+\sqrt[5]{c^{2}}\right)\sqrt[5]{bc}-\sqrt[5]{a^{4}}-\sqrt[5]{b^{4}}-\sqrt[5]{c^{4}}-\sqrt[5]{a^{2}b^{2}}-\sqrt[5]{a^{2}c^{2}}-\sqrt[5]{b^{2}c^{2}}-\left(2\sqrt[5]{a}+2\sqrt[5]{b}-3\sqrt[5]{c}\right)\sqrt[5]{abc}\right)\left(\left(\sqrt{4a^{2}b^{2}c^{2}v^{3}+1}-1\right)q_{1}\left(\frac{1}{a^{2}b^{2}c}\right)+2abcq_{2}\left(\frac{1}{a^{2}b^{2}c}\right)\right)p\left(\frac{\sqrt[5]{a^{3}b^{3}c^{4}}}{abc}\right)}{\left(q_{1}\left(\frac{1}{a^{2}b^{2}c}\right)-2abcq_{2}\left(\frac{1}{a^{2}b^{2}c}\right)\right)^{2}-\left(4a^{2}b^{2}c^{2}v^{3}+1\right)q_{1}\left(\frac{1}{a^{2}b^{2}c}\right)^{2}}$$ $$\frac{1}{\sqrt[6]{a}+\sqrt[6]{b}+\sqrt[6]{c}}=\frac{\left(\sqrt[3]{a}+\sqrt[3]{b}+\sqrt[3]{c}-\sqrt[6]{ab}-\sqrt[6]{ac}-\sqrt[6]{bc}\right)\left(\left(3\sqrt[6]{abc}+\sqrt{a}+\sqrt{b}+\sqrt{c}\right)^{2}-3\left(\sqrt{a}+\sqrt{b}+\sqrt{c}\right)\sqrt[6]{abc}\right)\left(\left(a+3b+3c\right)\sqrt{a}+\left(3a+b+3c\right)\sqrt{b}-\left(3a+3b+c\right)\sqrt{c}+21\sqrt{abc}\right)\left(2\left(a+3b+3c\right)\left(3a+b+3c\right)\sqrt{ab}+42\left(3a+3b+c\right)\sqrt{abc^{2}}-a\left(a+3b+3c\right)^{2}-b\left(3a+b+3c\right)^{2}+c\left(3a+3b+c\right)^{2}+441abc\right)}{4ab\left(21c\left(3a+3b+c\right)+\left(a+3b+3c\right)\left(3a+b+3c\right)\right)^{2}-\left(a\left(a+3b+3c\right)^{2}+b\left(3a+b+3c\right)^{2}-c\left(3a+3b+c\right)^{2}-441abc\right)^{2}}$$ Where $$p\left(x\right)=\frac{xq\left(x^{5}\right)}{5abcx^{3}-\left(a+b+c\right)x-5}$$ $$q\left(x\right)=u^{5}x^{3}+3Au^{4}x^{2}+3u^{2}\left(A^{2}u+BC\right)x+\left(A^{3}u^{2}-C^{3}u+B^{3}+3ABCu\right)=q_{1}\left(x\right)u+q_{2}\left(x\right)$$ $$A=\left(5v^{4}-5u^{2}v\right)$$ $$B=\left(10u^{2}v^{2}-v^{5}\right)$$ $$C=\left(10uv^{3}-u^{3}\right)$$ $$u=-\frac{\sqrt{4a^{2}b^{2}c^{2}v^{3}+1}+1}{2abc}$$ $$v=-\frac{a+b+c}{15abc}$$ and $q_1(x)$ and $q_2(x)$ have rational coefficients. As for the general case, it seems to involve a matrix inversion. I don't know linear algebra yet, so this wasn't very clear to me. I'm not anywhere near the age I'd need to be to learn it in school, and in fact, most of my peers don't even know how to rationalize $\frac{1}{\sqrt[3]{a}+\sqrt[3]{b}}$ . I really don't feel like waiting for the curriculum to get there, so I have $2$ more questions. Is there a way to do the arbitrary case using only elementary algebra, and where would be a good resource to learn linear algebra in order to do this? UPDATE: In order to do this in general, use complex numbers so that $n$ th roots have $n$ branches. Then take the product over all combinations of the branches. In the example problem, this leads to $2\cdot3\cdot7=42$ possible factors, $1$ of which is the original denominator. For $3$ $n$ th roots, the product $$N=\prod_{u,\ v,\ w}^{\ }\left(\sqrt[n]{a_{u}}+\sqrt[n]{b_{v}}+\sqrt[n]{c_{w}}\right)$$ is an integer, where subscripts are used to distinguish branches so it's easier to see what's happening here. In complex exponential form, you have $$N=\prod_{u,\ v,\ w}^{\ }\left(e^{\frac{2\pi ui}{n}}\sqrt[n]{a}+e^{\frac{2\pi vi}{n}}\sqrt[n]{b}+e^{\frac{2\pi wi}{n}}\sqrt[n]{c}\right)$$ This answer was given by Christophe Leuridan.","['rationalising-denominator', 'algebra-precalculus', 'polynomials']"
4550804,Use Stokes Theorem to Prove Cauchy Integral Formula,"In my undergrad complex analysis, our professor used a quick way to derive the Cauchy integral formula (without Goursat's theorem and all that) with the following setup: Let $U$ be an open set of $\mathbb{C}$ . Consider a complex function $f:U\to\mathbb{C}$ as a zero form on $U$ , i.e., $f\in \Lambda^0(U).$ Then analyticity of $f$ is equivalent to Cauchy-Riemann equation $\partial_{\bar{z}}f=0$ , which is equivalent to the form $f(z)dz\in\Lambda^1(U)$ being closed: $$d(fdz)=df\wedge dz=\partial_{\bar{z}}fd\overline{z}\wedge dz+\partial_{z}fdz\wedge dz=0.$$ Then one can prove the Cauchy integral formula as follows: Suppose $\Omega\subset U$ is a relatively compact set with piecewise smooth boundary and $U$ is an
open subset of complex plane, and $f:U\to\mathbb{C}$ is holomorphic. Then for any $a\in \Omega$ , we have $$f(a)=\frac{1}{2\pi I}\int_{\partial\Omega}\frac{f(z)}{z-a}dz.$$ Proof: Let $\Omega_\epsilon=\Omega-D_\epsilon(a).$ Then we consider the one form $f(z)/(z-a)\in\Lambda^1(\Omega_\epsilon).$ By Stoke's theorem ( $\int_{\Sigma}d\omega=\int_{\partial \Sigma}\omega$ ) we have $$
\int_{\partial\Omega_\epsilon}\frac{f(z)}{z-a}dz=\int_{\Omega_\epsilon}d\left(\frac{f(z)}{z-a}dz\right)=0
$$ since $f/(z-a)$ is holomorphic on $\Omega_\epsilon$ . Now $$
\int_{\partial\Omega_\epsilon}\frac{f(z)}{z-a}dz=\int_{\partial\Omega}\frac{f(z)}{z-a}dz-\int_{\partial D_\epsilon(a)}\frac{f(z)}{z-a}dz=0.
$$ Taking $\epsilon\to 0$ the second term goes to $2\pi if(a).$ This concludes the proof. However, my graduate complex analysis professor told us that one cannot apply Stoke's theorem directly until one verifies that $f'$ is integrable. I never learned Stoke's theorem properly so I wonder: What do we need to know prior to the proof above about $f,f'$ in order for the proof to be valid? (differentiability of $f'$ or something like that, or even smoothness of $f$ ?) What should we have proved before this point to make the above proof work? Is there a way of using differential forms and Stokes theorem to prove the Cauchy integral formula, without going into the traditional Goursat's theorem for triangles and polygons and all that? Are there any recommended references on this topic? Thanks in advance","['complex-analysis', 'stokes-theorem', 'differential-forms']"
4550807,Conditions for generalized projection matrix of size (2x2)? My results seem incorrect...,"I am trying to derive the general conditions that must be imposed upon the real-valued entries of a $2 \times 2$ orthogonal projection matrix. However, I am coming to a conclusion that seems wrong and hope someone can point me in the right direction. The prompt reads as follows: Find conditions on $a,b,c,d \in \mathbb{R}$ that guarantee the matrix $\begin{pmatrix} a & b \\ c & d\end{pmatrix}$ defines a rank-1 orthogonal projection. I begin by imposing the conditions that all projection matrices $P$ must fulfill, namely (i) $P = P^T$ (ii) $P = P^2$ Applying condition (i), $\begin{pmatrix} a & b \\ c & d\end{pmatrix} \overset{!}{=} \begin{pmatrix} a & c \\ b & d\end{pmatrix}$ I get $b=c$ and so I continue working with $\begin{pmatrix} a & c \\ c & d\end{pmatrix}$ and move on to condition (ii): $\begin{pmatrix} a & c \\ c & d\end{pmatrix} \overset{!}{=} \begin{pmatrix} a & c \\ c & d\end{pmatrix}\begin{pmatrix} a & c \\ c & d\end{pmatrix}=\begin{pmatrix} a^2 +c^2 & ac + cd \\ ac +cd & d^2 + c^2\end{pmatrix}$ From that, I get three equations: (1) $a = a^2 +c^2$ (2) $c=ac+cd=c(a+d)$ (3) $d=d^2 + c^2$ Further working out (2) by striking $c$ from each side, the system is then (1) $a = a^2 +c^2$ (2) $1=a+d$ (3) $d=d^2+c^2$ I re-arrange (1) and (3) to get (1) $c^2 = a - a^2$ (3) $c^2 = d - d^2$ and thus $a-a^2 = d-d^2$ from which I conclude that $a = d$ . Returning to equation (2), I then get (2) $1 = a+d = 2a$ and so $a = \frac{1}{2}$ and $d = \frac{1}{2}$ . Plugging either one of these into equation (1) or (3) then allows me to solve for $c$ , $\frac{1}{2} = \left ( \frac{1}{2} \right )^2+c^2$ or $c^2 = \frac{1}{4}$ And thus $c=\pm \frac{1}{2}$ . So, according to these results, to be guaranteed an orthogonal projection matrix of rank-1, my matrix $P$ must be one of two possibilities (as indicated by $\pm$ ): $P = \frac{1}{2} \begin{pmatrix} 1 & \pm 1 \\ \pm 1 & 1\end{pmatrix}$ This result is unsettling. First of all, I was expecting a broader range of possibilities for $a,b,c,d$ or at least for one or two variables. It seems odd that $a$ and $d$ are wholly constrained to one value and that $c$ only has two options. Is this correct? It seems like there should be more here that is possible. Aren't matrices such as $\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$ or $\begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}$ also to be included here? For example, when I apply $\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$ to a general vector $\begin{pmatrix} x \\ y \end{pmatrix}$ , it's easy for me to see the orthogonal projection: $\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} x \\ y\end{pmatrix} = \begin{pmatrix} x \\ 0\end{pmatrix}$ But when I apply the resulting projection matrix from above to the same generalized vector, I get a result that does reveal orthogonal projection to me (using $c = + \frac{1}{2}$ ): $\begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2}\end{pmatrix} \begin{pmatrix} x \\ y\end{pmatrix} = \frac{1}{2} \begin{pmatrix} x+y \\ x+y\end{pmatrix}$ How can that be an orthogonal projection? Isn't that simply a change in length along the same direction? Am I not understanding the fundamental concept? Thanks for any help you might be able to provide.","['projection', 'vectors', 'vector-spaces', 'matrices', 'linear-algebra']"
4550814,Relationship between weak and nuclear topologies,"Let $(X,\Vert\cdot\Vert)$ be a Banach space. Is it always true that $X$ equipped with the weak topology $\sigma(X,X')$ is a nuclear space?","['nuclear-norm', 'functional-analysis', 'analysis', 'weak-topology']"
4550826,how to prove the general distributive law,"I've been self-studying the book on set theory by Donald Monk (""Intro to Set Theory""). I'm getting stuck in the proof of this part of the general distributive law (p.54): $\bigcup_{i\in I}\bigcap_{j\in J}\boldsymbol{A}_{ij}\supseteq\bigcap_{\boldsymbol{F}\in\,{}^{I}J}\bigcup_{i\in I}\boldsymbol{A}_{i,\boldsymbol{F}i}$ where $\boldsymbol{A}$ is an indexed family of sets with domain $I\times J$ , where $I$ and $J$ are sets. Proof: Let $x\in\bigcap_{\boldsymbol{F}\in\,{}^{I}J}\bigcup_{i\in I}\boldsymbol{A}_{i,\boldsymbol{F}i}$ . Then $x\in\bigcup_{i\in I}\boldsymbol{A}_{i,\boldsymbol{F}i}$ , $\forall\boldsymbol{F}\in{}^{I}J$ . Thus, for each $\boldsymbol{F}\in{}^{I}J$ , $\exists i\in I$ s.t. $x\in\boldsymbol{A}_{i,\boldsymbol{F}i}$ . To complete the proof, I need to show that, for some $i_{0}\in I$ , $x\in\boldsymbol{A}_{ij}$ , for all $j\in J$ . I know that, for each $j\in J$ , we can easily define a function $\boldsymbol{F}\in{}^{I}J$ s.t. $j=\boldsymbol{F}i$ , for some $i\in I$ : e.g., let $\boldsymbol{F}=I\times\left\{ j\right\}$ .  Thus $x\in\boldsymbol{A}_{ij}$ for every $j\in J$ and some $i\in I$ (that corresponds to this $j$ ). I have no idea how to proceed, or if I'm on the right track at all. Any help?",['elementary-set-theory']
4550845,How to interpret symmetrical $2\times2$ matrix geometrically as a composition of simple transformation?,"Let's say I have this matrix: $$A = \left[
\begin{matrix}
  1&3\\
  3&1
\end{matrix}
\right]$$ I only know simple transformation like reflection, rotation, scaling, and shear. Can I interpret matrix $A$ as a composition of these simple transformation? How? I have tried to imagine the transformation of matrix $A$ and intuitively, it looks some kind of reflection or rotation with some scaling, but I got stuck because it has -2 and 4 scaling, which makes me confused.","['matrices', 'geometry', 'linear-transformations', 'affine-geometry']"
4550870,Does uniform convergence imply Hölder convergence?,"Let $\alpha\in(0,1)$ and define the $\alpha$ -Hölder norm of a function $f : [0,1]\to\mathbb{R}$ by $$
\lVert f\rVert_\alpha:=|f(0)| + \sup_{0\le s<t\le 1}\frac{|f(t)-f(s)|}{|t-s|^\alpha}.
$$ Let $f_n : [0,1]\to\mathbb{R}$ be a sequence of $\alpha$ -Hölder functions satisfying $\sup_n\lVert f_n\rVert_\alpha<\infty$ , and suppose $f_n\to f$ uniformly. It follows immediately that $\lVert f\rVert_\alpha<\infty$ . Does it also follow that $\lim_{n\to\infty}\lVert f_n-f\rVert_\alpha =0$ ? My intuition says no, but I am struggling to find a counterexample. If in fact the convergence in Hölder norm follows, I am curious to hear whether it also follows when the $f_n$ take values in an arbitrary Banach space.","['banach-spaces', 'real-analysis', 'functional-analysis', 'uniform-convergence', 'holder-spaces']"
4550888,Finding a closed form for this sequence ((A080416)),"I was working on a combinatorics problem that arose from one of my mathematical excursions, and the final formula includes numbers from a sequence that I've never encountered before: https://oeis.org/A080416 The sequence is actually a number triangle, which looks like this: $$1 \\
1,1 \\
1,4,1 \\
1,12,10,1 \\
1,32,67,20,1 \\
1, 80, 376, 252, 35, 1 \\
...$$ My goal is to find a closed form formula (not a generating function) for the number triangle $a(n,k)$ , but I don't even understand how this sequence is formed. I understand the ""paired decomposition of tetrahedral numbers"" part of the derivation. Basically, tetrahedral numbers can be decomposed in the following way: $$\begin{array}{ l l }
 \text{Tet}(1) = 1: & 1 \\
 \text{Tet}(2) = 4: & 2+2 \\
 \text{Tet}(3) = 10: & 3+4+3 \\
 \text{Tet}(4) = 20: & 4+6+6+4 \\
 \text{Tet}(5) = 35: & 5+8+9+8+5
\end{array}$$ In general, a tetrahedral number can be decomposed as such: $$\text{Tet}(n)=\sum_{k=1}^n k(n-k+1)$$ Then, the OEIS page states that once we decompose the tetrahedral numbers in this fashion, we can arrive at the goal number triangle by a Stirling-like process (whatever that's supposed to mean). I can't figure out what the process is, and thus how to find the closed form (if it even exists). Any help would be appreciated. UPDATE: I found a heuristic algorithm for computing the rows of the number triangle. For example, take the 5th row of the number triangle (1 32 67 20 1). It can be calculated like this: STEP 1: list the specific combinations of $4 \choose 1$ as such: oxxx, xoxx, xxox, xxxo Now, take the paired decomposition of the 4th tetrahedral number (here: 4 6 6 4) and assign these integers to the four respective slots in the diagram above. Whenever o is at a specific slot, that's the value of the combination. Finally, sum the values of all combinations together. $4+6+6+4=20$ . STEP 2: now, list the combinations of $4\choose 2$ : ooxx, oxox, oxxo, xoox, xoxo, xxoo Take the paired decompositon of the 3rd tetrahedral number (here: 3 4 3). This time each combination will have a slightly different (more general) numbering scheme of slots. Start with a $3$ (the first number in the paired decomposition) on the left. Whenever you encounter an ""o"", repeat the number assigned to that slot for the next slot. Whenever you encounter an ""x"" simply continue parsing through the paired decomposition. The value of a combination will be the multiplication of all slot values where an ""o"" occurs. For example: ooxx will have a value of $3\cdot 3=9$ because we start numbering the slots with a $3$ . Then we repeat the $3$ for the second slot because we had an ""o"". So the values of individual slots are $3,3,4,3$ . The first two slots have the ""o""s so the value of the combination is $3\cdot 3=9$ . Another example: xoox will have a value of $4\cdot 4=16$ because we start numbering with a $3$ . Then we go to next number in the decomposition sequence ( $4$ ) because we have an ""x"". But we have an ""o"" in the second slot, so the value of the third slot is 4 as well. So the values of the individual slots in order are $3,4,4,3$ and we have ""o""s in the middle two positions, so the value of the combination is $4\cdot 4=16$ . We do this for the rest of the combinations, and sum the values of all of the combinations, resulting in $9+12+9+16+12+9$ (respectively), which is $67$ . STEP 3: now, list the combinations of $4\choose 3$ : ooox, ooxo, oxoo, xooo Repeat the same procedure for numbering slots, but this time with the decomposition of the 2nd tetrahedral number (here: 2 2). The values of each combination end up being $8,8,8,8$ respectively ( $2^3$ in each case). Finally, sum all of the combination values to end up with $8+8+8+8=32$ . FINAL STEP: take all of the sums from all of the steps, and arrange them in the reverse order. So here, $32, 67, 20$ . Notice how these are the three non-trivial entries of the 5th row of the desired number triangle. You can generalize this algorithm to generate any $n$ th row by simply listing the combinations of $n-1\choose k$ in each $k$ th step, numbering the slots using the paired decomposition of $(n-k)$ th tetrahedral number, finding the values of each combination by multiplying the values of slots with ""o""s, and finally summing the  values of each combination for that step. The sums in reverse order will spell out the $n$ th row of the triangle. Note that I ommited steps $0$ and $n-1$ as they will always trivially sum to $1$ (multiplication over the empty set in the $0$ th step and multiplication of $n-1$ ones in the last step). Also note that I have no idea why this algorithm generates the number triangle, I just found it by using very messy heuristic logic. I still need help in arriving at the final closed form formula for the sequence, and trying to derive the Stirling-like process that the author of the OEIS page had in mind.","['set-partition', 'binomial-coefficients', 'combinatorics', 'closed-form', 'stirling-numbers']"
4550894,What is the truth value of this nested quantified statement,"$$ \forall x \exists y ((x \lt y) \implies (x^2 \lt y^2)) \space where \space x,y \in \mathbb{R}$$ I used proof by contradiction and found that it is true. However my professor thinks it is false because if for example x=-5 and y=-2 then $x^2 \lt y^2$ would be false. To my understanding, the statement means that for every x there exist at least 1 y that satisfies the statement. Am I misunderstanding this or is my professor incorrect?","['quantifiers', 'predicate-logic', 'logic', 'discrete-mathematics']"
4550902,"Is the type $(1,1)$ Kronecker delta tensor, $\delta_a^{\,\,b}$ equal to the trace of the identity matrix or always $1$ when $a=b$ and zero otherwise?","I'll ask this question using very simple examples working in flat cartesian space (just $2$ spatial dimensions). I'll be using the Einstein summation convention throughout this question, but since I'm very new to this I will explicitly write the summation symbol at times. According to this article on raising and lowering indices on Wikipedia the identity matrix , can be represented as a Kronecker delta metric tensor (of type $(0,2)$ ), $$\delta_{ij}=\begin{pmatrix}1&0\\0&1\end{pmatrix}=\begin{cases}1 & \text{if} \, i=j \\ 0 &\text{if}\, i\ne j\end{cases}\tag{A}$$ and its' inverse of type $(2,0)$ , $$\delta^{ij}=\left({\delta_{ij}}\right)^{-1}=\begin{pmatrix}1&0\\0&1\end{pmatrix}=\begin{cases}1 & \text{if} \, i=j \\ 0 &\text{if}\, i\ne j\end{cases}\tag{B}$$ are just the $2$ d identity matrices. But how does one interpret a type $(1,1)$ Kronecker tensor metric, $\delta_a^{\,\,b}$ ? Here are some examples to put this question into context. Suppose we have a matrix $A_{ij}=\begin{pmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{pmatrix}$ . Some of the following are correct expressions for $A_{ij}$ : $$\delta_{ik}A^{k}_{\,\,j}\tag{1}$$ $$\delta_{ik}\delta_{j\ell}A^{k\ell}\tag{2}$$ $$\delta_{k\ell}\delta^{k\ell}A^{ij}\tag{3}$$ $(1)$ is correct (equal to $A_{ij}$ ) as the dummy index $k$ in the matrix is 'lowered' using the metric. Then according to the definition, $(\mathrm{A})$ , $\delta_{ii}=1$ and $k$ is summed upon. $(2)$ is also correct as $$\delta_{ik}\delta_{j\ell}A^{k\ell}=\delta_{ik}A^k_{\,\,j}=A_{ij}$$ $(3)$ is not correct as $$\delta_{k\ell}\delta^{k\ell}A^{ij}=\delta_k^{\,\,k}A^{ij}=\sum_{k=1}^2 \delta_k^{\,\,k}A^{ij}=2A^{ij}\ne A_{ij}\tag{C}$$ In the first equality of $(\mathrm{C})$ , I think of this as 'raising' the second index of the first Kronecker metric in $\delta_k^{\,\,k}\delta^{kk}$ . Since for a non-zero contribution $\ell=k$ , which means $\delta^{kk}=1$ , which is fine as this is what equation $(\mathrm{A})$ is telling me to do. So from this it seems like the $\delta_k^{\,\,k}$ is the trace of the matrix given in eqn $(\mathrm{A})$ (or $(\mathrm{B})$ ). But I have another $2$ examples that seem to contradict this, suppose we have two column vectors, $U^i$ and $V^i$ along with their respective row vectors, $$U^i=\begin{pmatrix}u_1\\u_2\end{pmatrix}\,\,\text{so that}\,\,\,\,U_i=\delta_{ij}U^j=\begin{pmatrix}u_1&u_2\end{pmatrix}$$ $$V^i=\begin{pmatrix}v_1\\v_2\end{pmatrix}\,\,\text{so that}\,\,\,\,V_i=\delta_{ij}V^j=\begin{pmatrix}v_1&v_2\end{pmatrix}$$ Now suppose we want to compute the inner product, $U\cdot V$ . There are obviously many ways of doing this, and one could simply write $U\cdot V=U_iV^i$ , but I want to purposely use the Kronecker delta metric to make a point. Here are some possible expressions for the inner product, $U\cdot V$ : $$\delta_{ij}U^iV^j\tag{4}$$ $$V^j\delta_{j\ell}U^{\ell}\tag{5}$$ $$U^aV_b\delta^b_{\,\,a}\tag{6}$$ $$U_iV^a\delta_a^{\,\,b}\delta_b^{\,\,i}\tag{7}$$ $(4)$ is correct as $\delta_{ij}U^iV^j=U_i\delta_{ij}V^j$ , and the way I've understood this is that the $j$ index on the $V^j$ has been 'lowered' using the metric and the only non-zero contribution is when $i=j$ , so writing these steps out explicitly, $$U_i\delta_{ij}V^j=U_i\delta_{ii}V^i=U_iV^i$$ since $\delta_{ii}=1$ according to the prescription in $(\mathrm{A})$ . $(5)$ is also correct for the same reasons as $(4)$ . Now for eqn, $(6)$ this is where the problem starts for me, since I am not sure what $\delta^b_{\,\,a}$ actually means, I can only guess that $\delta^b_{\,\,a}$ is non-zero only when $a=b$ , so from this I conclude that $$U^aV_b\delta^b_{\,\,a}=U^aV_a\delta^a_{\,\,a}\stackrel{\color{red}{?}}{=}2U^aV_a\ne U^aV_a\tag{D}$$ For $(\mathrm{D})$ , since we are working in $2$ d flat Cartesian space, this $\delta_{\,\,a}^a=\sum_{a=1}^2\delta_{\,\,a}^a=2$ is the trace or sum of the diagonal elements of $(\mathrm{A})$ , (as $a$ is a dummy index and hence summed over). In a similar way, I think eqn. $(7)$ should be $$U_iV^a\delta_a^{\,\,b}\delta_b^{\,\,i}=U_iV^a\delta_a^{\,\,i}\delta_i^{\,\,i}$$ $$\stackrel{\color{red}{?}}{=}2U_iV^a\delta_a^{\,\,i}\stackrel{\color{red}{?}}{=}2U_iV^i\delta_i^{\,\,i}\stackrel{\color{red}{?}}{=}4U_iV^i\ne U_iV^i\tag{E}$$ In the first equality of $(\mathrm{E})$ there is a contraction of the $b$ index, where it get sets equal to $i$ since this is the only way to get a non-zero contribution out of the expression, I interpret this as the trace and that is where the factor of $2$ comes from in the second equality (marked with a red question mark above it). I have then done exactly the same thing for the third equality and by my logic there is another trace so there should be another factor of $2$ which is in the fourth equality (these equalities are marked with red question marks as I'm not sure if these statements are true). Now here is the problem, eqns $(4-7)$ are all correct expressions for the inner product, $U\cdot V$ . So, the factors of $2$ I have introduced should not be there. But the question is, why are these manipulations in $(6)$ and $(7)$ wrong, when eqn. $(\mathrm{C})$ used the trace as $\delta_k^{\,\,k}=2$ ?","['matrices', 'inner-products', 'tensors', 'kronecker-delta']"
4550981,How to give a high-probability uniform estimation of a potential having access to noisy pointwise estimates of the associated vector field?,"As in the title, our goal is to estimate uniformly and with high probability (and up to a constant) a potential having access to noisy pointwise estimates of the associated vector field (i.e., the gradient of the potential). Here how I tried to formalize the problem. Suppose $d \in \mathbb{N}$ and let $\mathcal{B}_1^d$ be the unitary closed ball of $\mathbb{R}^d$ centered in the origin. Let $\mathcal{F}$ be the class of all $C^1$ functions $F : \Omega \to \mathbb{R}$ , where $\Omega$ is any open set of $\mathbb{R}^d$ containing $[-1,1]^d$ , and such that $\forall x\in[-1,1]^d, \nabla F(x) \in \mathcal{B}_1^d$ . Suppose that, for each $F \in \mathcal{F}$ , we have the following interaction protocol: For each $t=1,2,\dots$ We select a point $X_t \in [-1,1]^d$ (a selection based on past observations and possibly some randomization, but not on the knowledge of $F$ ) We observe a $\mathcal{B}_1^d$ -valued random variable $Y_t^F$ such that $\mathbb{E}[Y_t^F \mid X_1,Y_1^F,\dots,X_{t-1},Y_{t-1}^F,X_t] = \mathbb{E}[Y_t^F \mid X_t] =\nabla F(X_t)$ . Basically, we query a point in the domain and we see a noisy (and bounded) reconstruction in that point of the vector field associated to the gradient of the potential $F$ . Regardless which potential $F \in \mathcal{F}$ we are interacting with, our goal is to give a uniform reconstruction of $F-F(0)$ with high-probability using the previous interaction protocol , i.e. we want to find a strategy to select the points $X_1,X_2,\dots$ and a family of estimators $(\Phi_{x,t})_{x \in (-1,1)^d,t \in \mathbb{N}}$ , where $\forall x \in [-1,1]^d, \forall t \in \mathbb{N}, \Phi_{x,t} : \big([-1,1]^d\times\mathcal{B}_1^d\big)^t \to \mathbb{R}$ , such that $$\forall \varepsilon >0, \forall \delta \in (0,1), \exists T \in \mathbb{N}, \forall t \ge T, \forall F \in \mathcal{F}, \\
\mathbb{P}\bigg[ \sup_{x \in [0,1]^d}\Big|\Phi_{x,t}(X_1,Y_1^F,\dots,X_t,Y_t^F) - \big(F(x)-F(0)\big)\Big| \ge \varepsilon \bigg] \le \delta.$$ Notice that estimating $F-F(0)$ instead of $F$ is just a trick to get rid of the constant we never see. It is interesting to start with the case $d = 1$ , where I'm quite confident that the problem is solvable via Monte Carlo integration in the following way. Define $$\Phi_{x,t}(x_1,y_1,\dots,x_t,y_t) = \frac{1}{t} \sum_{s=1}^t \big(y_t \cdot \mathbb{I}\{\min(x,0)\le x_t \le \max(x,0)\} \cdot \operatorname{sgn}(x)\big)$$ and select the random variables $X_1,X_2, \dots$ just as family of independent $[-1,1]$ -valued uniform random variables. But what about the general case? What are sensible estimators and strategies to solve the problem? And what are the best achievable decaying rate depending on $t, \varepsilon$ and on the dimension $d$ for the quantity $$\mathbb{P}\bigg[ \sup_{x \in [0,1]^d}\Big|\Phi_{x,t}(X_1,Y_1^F,\dots,X_t,Y_t^F) - \big(F(x)-F(0)\big)\Big| \ge \varepsilon \bigg] ?$$","['statistical-inference', 'statistics', 'parameter-estimation', 'machine-learning', 'probability-theory']"
4550991,If you write all the whole numbers from 1 to 9999 but skip every number that contains the digit 4. How many times would you have to write the digit 1?,"This is question is taken from an early round of a Norwegian national math competition where you have on average 5 minutes to solve each question. I tried to solve the question by writing every number with four digits and with introductory zeros where it was needed. For example 0001 and 0101 would be the numbers 1 and 101. I then divided the different numbers into four groups based on how many times the digit 1 appeared in the number. I called these group 1,2,3 and 4. 0001 would then belong to group 1 and 0101 to group 2. I first found out in how many ways I could place the digit 1 in each group, then multiplied it by the number of combinations with the other possible eight digits (0,2,3,5,6,7,8,9). This would be the number of combinations for each group and I lastly multiplied it with the number of times 1 appeared in the number. This is done for all of the groups under: $\binom{4}{1}\cdot8^3\cdot1$ times in group 1 $\binom{4}{2}\cdot8^2\cdot2$ times in group 2 $\binom{4}{3}\cdot8^1\cdot3$ times in group 3 $\binom{4}{4}\cdot8^0\cdot4$ times in group 4 The sum of all these calculations will be the 2916 and the correct number of times 1 appears (I think). Is this calculation/way of thinking correct? And is there a more efficient way to do it?",['combinatorics']
4551008,How to prove $|f(x)| \leq C \epsilon |x|^N$?,"Suppose that $f\in C^{\infty}(\mathbb R^d)$ and $D^{\alpha}f(0)=0$ for all $0\leq |\alpha|\leq N.$ We may assume that $$|D^{\alpha}f(x)| \leq \epsilon \quad \text{for all} \quad |\alpha|=N$$ in small neighbourhood of origin. Question: How to show that $$|f(x)| \leq C \epsilon |x|^N$$ ? My thought: Maybe I've to invoke mean value theorem.  But I do not know how when $N\neq 1.$ Edit : Notation: $D^{\alpha}= \frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1}\cdots \partial x_N^{\alpha_N}},  \quad |\alpha|= \alpha_1+ \cdots + \alpha_n, \alpha_i \in \mathbb N_0$","['analysis', 'real-analysis', 'multivariable-calculus', 'calculus', 'inequality']"
4551010,Ideal of functions vanishing on a plane algebraic curve,"Note: The specific question I am asking in this post is marked $(*)$ below. Let $k$ be a field and let $P\in k[X,Y]$ be an irreducible polynomial. Denote by $$\mathcal{V}(P) = \big\{(x,y)\in k^2\ |\ P(x,y)=0\big\}$$ the vanishing locus of $P$ . When $k$ is algebraically closed, Hilbert's Nullstellensatz implies that the ideal of polynomials vanishing on the zero locus of $P$ is the ideal generated by $P$ : $$\mathcal{I}\big(\mathcal{V}(P)\big) = \sqrt{(P)} = (P).$$ As $P$ is irreducible, it generates a prime ideal in $k[X,Y]$ , so $\mathcal{V}(P)$ is irreducible when $k$ is algebraically closed. When $k$ is not algebraically closed, the latter statement is no longer necessarily true: An irreducible polynomial $P\in k[X,Y]$ whose zero set in $\mathbb{R}^2$ is not irreducible . But in that example, $\mathcal{V}(P)$ is finite. And in Exercise 3 on page 24 of Perrin's textbook Algebraic Geometry: an introduction , the author says that, if one assumes additionally that $\mathcal{V}(P)$ is infinite , then the ideal of polynomials vanishing on the zero locus of $P$ coincides with the ideal generated by $P$ : $$\mathcal{I}\big(\mathcal{V}(P)\big) = (P).$$ Here is how Perrin phrases it: Link to the image. Since the ideal generated by $P$ is still prime in $k[X,Y]$ , this proves that $\mathcal{V}(P)$ is irreducible when $P\in k[X,Y]$ and $\mathcal{V}(P)$ is infinite. However, I am not sure how to prove that \begin{equation}(*)\quad \big(P\ \mathrm{irreducible\ in}\ k[X,Y]\ \mathrm{and}\ \mathcal{V}(P)\ \mathrm{infinite\ in}\ k^2\big) \  \Rightarrow \  \mathcal{I}(\mathcal{V}(P)) = (P).\end{equation} It has to be specific to two variables, because there are counter-examples in higher dimension: Zero set of homogeneous polynomials over the reals . Would anyone have some pointers? I am interested in the statement for $k$ an abstract (infinite) field, but if there is a specific argument over $\mathbb{R}$ , it would also be interesting.","['irreducible-polynomials', 'algebraic-geometry', 'algebraic-curves']"
4551094,System of first order differential equation.,"Consider the linear system $y’=Ay+h$ where $$
A=\begin{bmatrix}
1 & 1\\
4 & -2
\end{bmatrix}
$$ and $$h=\begin{bmatrix}
3t+1\\
2t+5
\end{bmatrix}$$ Suppose $y(t)$ is a solution such that $$\lim_{t\to\infty}\frac{y(t)}{t}=k\in\mathbb R^2$$ What is the value of $k?$ $1$ . $\begin{bmatrix}
\frac{-4}{3}\\
\frac{-5}{3}
\end{bmatrix}$ . $2$ . $\begin{bmatrix}
\frac{4}{3}\\
\frac{-5}{3}
\end{bmatrix}$ . $3$ . $\begin{bmatrix}
\frac{2}{3}\\
\frac{-5}{3}
\end{bmatrix}$ . $4$ . $\begin{bmatrix}
\frac{-2}{3}\\
\frac{-5}{3}
\end{bmatrix}$ . I find eigen value of corresponding homogeneous system as $2$ and $-3$ and corresponding eigen vectors as $\begin{bmatrix}
1\\
1
\end{bmatrix}$ and $\begin{bmatrix}
1\\
-4
\end{bmatrix}$ . Therefore general solution of corresponding homogeneous differential equation is given by $y_c=\Phi(x)c=\begin{bmatrix}
e^{2t} & e^{-3t}\\
e^{2t} & -4e^{-3t}
\end{bmatrix}c$ , for arbitrary constant $c$ . Now as I know that by using variation of parameter general solution is given by $$y=\Phi(t)c+\Phi(t)\int {\Phi(x)}^{-1}h(x)dx= \begin{bmatrix}
e^{2t} & e^{-3t}\\
e^{2t} & -4e^{-3t}
\end{bmatrix}c+ \begin{bmatrix}
e^{2t} & e^{-3t}\\
e^{2t} & -4e^{-3t}
\end{bmatrix}\int {\begin{bmatrix}
e^{2x} & e^{-3x}\\
e^{2x} & -4e^{-3x}
\end{bmatrix}}^{-1} \begin{bmatrix}
3x+1\\
2x+5
\end{bmatrix}dx$$ Now I am unable to reach at final answer . Please help. Thank you.",['ordinary-differential-equations']
4551132,Structure of the stalk of a pushforward etale sheaf,"Let $X$ be an algebraic scheme over an algebraically closed field of characteristic $p$ , let $U$ be an
open dense subset and let $\mathcal F$ be a lisse $\overline{\mathbb Q_l}$ sheaf on $U$ . It is known that $\mathcal F$ corresponds to a finite dimensional $\overline{\mathbb Q_l}$ vector space $V$ together with a continuous $\pi_1(U, \bar x)$ -action (for an $x \in |U|$ ). Here I wonder what is $(j_*\mathcal F)_{\bar s}$ , where $s \in X-U$ and $j : U \rightarrow X$ is the open immersion. The textbook I read says it is $V^{I_s}$ , where $I_s \subseteq \pi_1(U, \bar x)$ is the ""ramification group"" at $s$ , which seems to be the absolute Galois group of the quotient field of the strict henselization of the local ring at $s$ in $\bar s$ . I don't see why the absolute Galois group is contained in $\pi_1(U, \bar x)$ and $(j_*\mathcal F)_{\bar s} = V^{I_s}$ . Thanks in advance.","['etale-cohomology', 'algebraic-geometry']"
4551157,Representations of abelian groups,"A classical result is Theorem: Let $G$ be a abelian group and $(V, \rho)$ be a irreducible representation of $G$ over a algebraically closed field $k$ . If $V$ is finite dimensional (more generally, if $\mathrm{dim}_k V < |k|)$ then $\mathrm{dim}_k V = 1$ . This is essencially Schur/Dixmier's lemma, the proof is based at $\rho(g)$ having a eingenvalue, $\forall g \in G$ . But what if i don't suppose anything about the dimension of $V$ ? $$
$$ Question: Are every irreducible $G$ -module finite dimensional? Maybe the unitary ones? Do I really need to go topological world and suppose $G$ compact abelian and consider only strongly continuous representations or something like this? $$
$$ An idea: $G$ -modules are equivalent to $k[G]$ -modules and if $G$ is abelian then this is a commutative algebra over $k$ . If $G$ is finite, then $k[G]$ is finite dimensional over $k$ and $V = k[G]v$ for $v \in V \setminus 0$ , we are done. More generally, this proves that all irredutible representations of a finite group are finite dimensional. In abelian case, $k[G]^*$ is also a commutative cocommutative Hopf algebra. A $k[G]$ -module $V$ gives a $k[G]^*$ - comodule $V^*$ and simple $k[G]^*-$ comodules are finite dimensional over $k$ (see here ). But dualizing transforms injections into surjections,  I don't know if still $V^*$ is a simple comodule.","['coalgebras', 'abelian-groups', 'group-theory', 'representation-theory']"
4551174,Local behavior around critical points in high dimensions?,"Let $f:\mathbb{R}^n\rightarrow \mathbb{R}$ be a function that is (at least) $C^2$ . In the standard calculus classes one learns: If the gradient of $f$ at $x_0$ , then $f$ has a local extremum (minimum or maximum) at $x_0$ . Hence the gradient test is a necessary but not sufficient condition (as, e.g., $x\mapsto x^3$ shows). Using the (semi)positive/negative definiteness of the Hessian, one can disambiguate and identify among the previously found extremum candidates some that are local minima or maxima. But so far this doesn't give complete classification of the extrema, as it is possible one the one-dimensional case for a test involving $n$ derivatives, and it seems that this problem is rather difficult for higher dimensions, involving potentially Morse theory, as I found it mentioned online. Can someone clarify whether a nice, complete description (i.e. a nice characterization) of which points are minima and maxima exists in all dimension or point me to a definitive reference (that ideally also outlines the state of the art)? There are various online references, that provide partial answers: What is the higher-order derivative test in multivariable calculus? (but this seems to give a test involving all derivatives, but following the discussion there it seems this test is rather hard to check in practice and one has to rely on numerical computations; in the comments it is actually stated ""there is not an analogue test in multivariable calculus"" , and this comment is backed up in the comments by a user I trust, KCd, which seems to indicate that the problem I outlined is open. Morse theory is mentioned, but not particular information is given. I would be also interested in a statement along the lines of whether for every $n$ one can construct a function such that for any ""feasible"" test using higher order derivatives, any ""natural"" test using higher order derivatives that allows ""easy checking"" fails.) http://www.u.arizona.edu/~mwalker/MathCamp2021/UnconstrainedOptimization.pdf (this gives actually necessary and sufficient conditions, but I'm not happy with them, because they are different conditions, so the don't characterize extrema completely) https://www.ripublication.com/adsa20/v15n2p11.pdf (it's from 2020, but I'm not 100% convinced this is really a legit or novel article; nonetheless it's interested to read) EDIT: I have also asked this question on Mathoverflow now.","['non-convex-optimization', 'analysis']"
4551181,Determine a function from a monotone condition,"Let $f\in C^\infty((0,1),\mathbb{R}_+)\cap C_0([0,1])$ and $$\lim_{t\to 0^+}f'(t)=+\infty,\lim_{t\to1^-}f'(t)=-\infty.$$ Furthermore, if $$F(t):=\frac{f''(t)}{f(t)(1+(f'(t))^2)^2}$$ is monotoinc in $(0,1)$ , find $f$ . The function $\sqrt{x-x^2}$ is obviously a solution. And $F$ is a constant at that time. But I am unable to determine whether it is the unique function that satisfies the conditions. The condition ""monotonic"" seems too weak for me. My attempts: The numerator of $F'$ is $$-f'f''(1+(f')^2+4ff'')+ff'''(1+(f')^2).$$ And we can suppose $F'\geq0$ since otherwise we can change $f(t)$ to $f(1-t)$ . I tried to analyze the behavior of $F'$ at the maximum point of $f$ and use the comparation theorems in ODE, but I didn't success, and I still don't know how to use the conditions of $f'$ . I also tried to let $g=f^2$ and wanted to show $g'''=0$ , but it didn't work as well. Any suggestion or counter-example will be appreciated. EDIT: I heard that $F$ is actually the Gaussian curvature of the rotational surface of $f$ about the $t$ -axis. And the conditions of $f'$ promises the surface to be $C^1$ . Will this help?","['derivatives', 'functions', 'analysis', 'ordinary-differential-equations']"
4551191,Choosing a $\lambda$ s.t. $ \left(\frac{1}{1-\lambda}\right)^n \exp(-\lambda\varepsilon) \leq \left(\frac\varepsilon n\right)^n \exp(n-\varepsilon). $,"Context: The motivation for the following inequality comes from bounding tail probabilities of sums of i.i.d. random variables where one applies Markov's Inequality. For a fixed $\varepsilon>0$ and $n \in \mathbb{N}$ , I want to prove that I can choose a $\lambda \in (0,1)$ such that $$
\left(\frac{1}{1-\lambda}\right)^n \exp(-\lambda\varepsilon) \leq \left(\frac\varepsilon n\right)^n \exp(n-\varepsilon).
$$ Some initial observations / things to keep in mind: Note $\lambda$ can (and certainly must) depend on both $n$ and $\varepsilon$ . We know nothing about the relation of $n$ and $\epsilon$ . The left hand side is a product of an increasing function and a decresing function in $\lambda \in(0,1)$ . Hence, optimization could cause some trouble. It is not necessary to compute the exact value of $\lambda$ . It would suffice to show there exists such a $\lambda$ as specified Solution Attempt 1 Here I try to find the value of $\lambda$ . I do not know how to systematically tackle this, so I have tried different values of $\lambda$ to see if anything would work. For instance choosing $$
\lambda = 1-\frac{\text{min}(\varepsilon,n)}{\varepsilon} \in (0,1)
$$ yields $$
\left(\frac{1}{1-\lambda}\right)^n \exp(-\lambda\varepsilon) = \left(\frac{\varepsilon}{\text{min}(\varepsilon,n)}\right)^n \exp\left(\frac{\text{min}(\varepsilon,n)}{n}-\varepsilon\right) \leq
\left(\frac{\varepsilon}{\text{min}(\varepsilon,n)}\right)^n \exp(n-\varepsilon).
$$ The problem is know as mentioned above that it is not so easy to get the the upper bound from here as the other factor becomes smaller if the minimum is replaced by one of the elements. I have tried to extend this idea further by considering ""nested"" max/min on the form $\text{max}(\text{min}(...,...),\text{min}(...,...))$ but I have become convinced that this is a dead end and not something that can ever work. Solution Attempt 2 Here I will simply try to show the existence of $\lambda$ . The left hand side is continuous in $\lambda$ (and based on plots also seem convex, but I have not shown this (yet)). For $\lambda \to 0$ the right-hand side goes to 1 and for $\lambda \to 1$ the right hand side goes to $\infty$ . One idea would then be to use the intermediate value theorem to get the existence of $\lambda$ . Note we could further upper-bound the right-hand side if need be before using the IVT. But I have not managed to achieve anything here either as the left hand side easily can become less than 1. Can anyone help me show this inequality?","['inequality', 'exponential-function', 'real-analysis']"
4551198,Uniqueness of longest run of Bernoulli experiment,"In our lecture we prove the statement (Erdös-Rényi law of runs): We consider the probability space $(\Omega:=\{0,1\}^{\mathbb{N}},\mathcal{F},\mathbb{P})$ and define a Bernoulli experiment of length $n$ with probability of success $p$ . Let be $R_n$ the length of the longest run, i.e. $$
R_n:=\max\left\{l-k\mid 0\leq k<l\leq n, \frac{S_l-S_k}{l-k}=1\right\}
$$ where $S_l$ , $S_k$ are the number of successes until the $l$ -th and $k$ -th step.
Then $$
P\left(\lim\limits_{n\to\infty}\frac{R_n}{\ln(n)}\text{ exists and equals }\frac{1}{\ln\left(\frac{1}{p}\right)}\right)=1.
$$ The proof relies on the heuristic assumption that the longest run of the Bernoulli experiment of length $n$ is unique, so that we can use the fact $$1=np^{R_n}\implies R_n=\frac{\ln(n)}{\ln\left(\frac{1}{p}\right)}.$$ To make it clear, if we conduct the experiment $n$ -times, then there is exactly one tupel $\omega\in\Omega$ which contains $R_n$ -many $1$ 's in a row, e.g. $\omega=(0,1,0,\underset{R_n-\text{ many}}{\underbrace{1,1,1,1,\dots,1}},1,1,0,0,1,0,1,1,0,\dots)$ . If we conduct the experiment $(n+1)$ -times, then there is exactly one tupel $\omega'\in\Omega$ which contains $R_{n+1}$ -many $1$ 's in a row, e.g. $\omega'=(0,1,0,\underset{R_{n+1}-\text{ many}}{\underbrace{1,1,1,1,\dots,1}},1,1,0,0,1,0,1,1,0,\dots)$ . And so on... I don't understand why we can simply make this assumption? Maybe someone is more familiar with this and can explain it to me?","['combinatorics', 'probability-theory', 'probability']"
4551261,Complex contour integral using Cauchy integral formula,"I am trying to get integral in contour  ​​​​​​​ $C=\left \{ 3\cos(t)+2i\sin(t) : 0\leq t\leq 2\pi \right \}$ $$\oint_{C} \frac{z}{(z+1)(z-1)^2}dz$$ What I tried was using partial fractions and integrate them separately $$\oint_{C} \frac{z}{(z+1)(z-1)^2}dz =\oint_{C} \frac{1}{4(z-1)}dz +\oint_{C} \frac{1}{-4(z+1)}dz+\oint_{C} \frac{1}{2(z-1)^2}dz$$ For $\int_{C}^{} \frac{1}{4(z+1)}dz$ + $\int_{C}^{} \frac{1}{-4(z-1)}dz$ , using Cauchy's integral fomula they are zero. For $\int_{C}^{} \frac{1}{2(z-1)^2}dz$ , I tried to use Cauchy's integral formula like: $$\frac{1}{2}\int_{C}^{} \frac{\frac{1}{(z-1)}}{(z-1)}dz$$ but can't solve it... What am I missing??","['complex-analysis', 'contour-integration']"
4551306,How to prove that the following object is not a set?,"Let us consider $C: = \{\lambda \in \text{Card} \ / \ \omega + \lambda = \lambda \}$ (note that we consider here the operations on ordinals and we work in ZFC). I want to prove that $C$ is not a set. It seems to be an easy question of elementary set theory but I cannot figure out which arguments to use (in general, this kind of question is linked with the Russell's paradox.) Any hints would be helpful. Moreover what would happened if we took $\lambda \in \text{On}$ ? Will $C$ be a set then ? Thanks in advance !","['elementary-set-theory', 'ordinals', 'cardinals']"
4551319,Recurrence relation for substring,"Let us consider strings that are build up with symbols from the set $\{\&, \#, \$\}$ and digits from
the set $\{0, 1, 2, 3\}$ . Let $a_n$ be the number of strings of length $n$ that contain the substring ’#1’. Find a recurrence relation for $a_n$ . Argue carefully. My solution: I have found 3 cases: The string doesn't end with '#', which means the '#1' has to be in the $n-1$ space. That gives me $6*a_{n-1}$ . The string ends with '#1', which means we can have anything in the n-2 space. That gives me $7^{n-2}$ . The string ends with only '#' (but not '#1') which means the string has to be in the n-2 space. That gives me $6*a_{n-2}$ . Now this would give me the recurrence relation: $6*a_{n-1}$ + $7^{n-2}$ + $6*a_{n-2}$ .
I know that $a_{1} = 0$ and $a_{2}$ = 1. Then $a_{3}$ should be 13 according to my formula, but I know that that isn't correct (I brute calculated it and I know it should be 14). Am I thinking this wrong?","['solution-verification', 'combinatorics', 'recurrence-relations', 'discrete-mathematics']"
4551368,Eigenvalues of $A^\dagger A$,"With a $2\times 2$ matrix $A$ , let $u$ be an eigenvector of $B=A^\dagger A$ . My question is: when can the eigenvalue $\lambda = u^\dagger B u$ lie between $0$ and $1$ , i.e., what are the conditions under which $\lambda \in [0,1]$ ? Here $\dagger$ denotes the Conjugate-Transpose. Also, $u$ is normalized i.e., $u^\dagger u = I$ .","['matrices', 'eigenvalues-eigenvectors']"
4551381,Solution the ode $x^2y''+4xy'+2y=f(x)$,"I'm trying to find a solution for the second order ode $x^2y''+4xy'+2y=f(x)$ , where $f\in\mathcal{C}^1(\mathbb{R})$ .
I already found that the solution for the homogeneous part is equal to $y_h(x)=\frac{C_1}{x^2}+\frac{C_2}{x}$ . But now I'm stuck trying to get the nonhomogeneous solution for this equation. I tried substituting $y=f(x)$ but this leads me to nowhere. Any help is greatly appriciated.",['ordinary-differential-equations']
4551426,Do we get a product regular conditional probability for conditionally independent random variables in Polish spaces?,"Theorem 8.37 in Klenke's Probability Theory ensures the existence (albeit not uniqueness) of a regular conditional probability for Borel spaces (in particular Polish spaces). Now, say we have two random variables $X,Y$ living in Polish spaces equipped with the Borel algebra. Can we then find a regular conditional distribution $\kappa$ of $(X,Y)$ given $\mathcal F$ that is a product measure (almost) everywhere (i.e. $\kappa_\omega=\kappa_{1,\omega}\otimes\kappa_{2,\omega}$ is the product of its marginals), whenever $X$ and $Y$ are conditionally independent given $\mathcal F$ ? I asked around, I went through all of my favorite books and online resources. I have not tried to prove or disprove it myself. My not too educated guess would be that this should hold, for similar reasons that ensure the existence of a regular conditional distribution. BONUS QUESTION : Does this also hold for countable products?","['measure-theory', 'probability-distributions', 'probability-theory', 'probability']"
4551441,Why can't we define the directional derivative of vector fields on a manifold in the same way as in $\mathbb{R}^n$.,"First of all I apologize for my english, it is not my first language. I am reading the book ""Differential Geometry"" by Loring W. Tu. In subsection 4 of chapter 1, Tu defines the directional derivative at $p$ of a $C^\infty$ vector field $Y=\sum b^i \frac{\partial}{\partial x^i} \Bigr\rvert_{p}$ on $\mathbb{R}^n$ in the direction $X_p$ as \begin{align}
D_{X_p}Y=\sum (X_p b^i)\frac{\partial}{\partial x^i} \Bigr\rvert_{p}.
\end{align} When $X$ is a $C^\infty$ vector field on $\mathbb{R}^n$ , not just a vector at $p$ , we define the vector field $D_XY$ on $\mathbb{R}^n$ by \begin{align*}
(D_xY)_p=D_{X_p}Y 
\end{align*} for all $p\in \mathbb{R}^n$ . Later, in subsection 6, which deals with affine connections, Tu writes: ""Consider a smooth vector field $Y$ on a manifold $M$ and a tangent vector $X_p \in T_pM$ at a point $p$ in $M$ . To define the directional derivative of $Y$ in the direction $X_p$ , it is necessary to compare the values of $Y$ in a neighborhood of $p$ . If $q$ is a point near $p$ , in general it is not possible to compare the vectors $Y_q$ and $Y_p$ by taking the difference $Y_q-Y_p$ , since they are in distinct tangent spaces. For this reason, the directional derivative of a vector field on an arbitrary manifold $M$ cannot be defined in the same way as in Section 4"". My problem is that I don't see in the definition of directional derivative where the values ​​of $Y_p$ and $Y_q$ are compared. In $\mathbb{R}^n$ , we have that to compute the directional derivative of $f$ at $p$ in the direction $X_p$ , we first write down a set of parametric equations for the line through $p$ in the direction $X_p$ : \begin{align*}
x^i=p^i+ta^i
\end{align*} with $i=1,...,n$ . Let $a=(a^1,...,a^n)$ . Then $D_{X_p}f$ is \begin{align*}
D_{X_p}f&=\lim_{t \to 0} \frac{f(p+ta)-f(p)}{t}=\frac{d}{dt}\Bigr\rvert_{t=0} f(p+ta)\\
&=\sum \frac{\partial f}{\partial x^i}\Bigr\rvert_{p} \frac{dx^i}{dt}\Bigr\rvert_{0} =\sum \frac{\partial f}{\partial x^i}\Bigr\rvert_{p} a^i\\&=\big(\sum a^i \frac{\partial}{\partial x^i} \Bigr\rvert_{p}\big)f=X_p f.
\end{align*} When $X$ and $Y$ are $C^\infty$ vector field on $\mathbb{R}^n$ , then \begin{align*}
(D_xY)_p=D_{X_p}Y=\sum (X_p b^i)\frac{\partial}{\partial x^i} \Bigr\rvert_{p}&=\sum \big(\lim_{t \to 0} \frac{b^i(p+ta)-b^i(p)}{t}\big)\frac{\partial}{\partial x^i} \Bigr\rvert_{p}\\
&=\sum \lim_{t \to 0} \frac{\big(b^i(p+ta)\frac{\partial}{\partial x^i} \Bigr\rvert_{p}\big)-\big(b^i(p)\frac{\partial}{\partial x^i} \Bigr\rvert_{p}\big)}{t} \tag{1}
\end{align*} is this last line right?, I mean, is (1) right? or should I write \begin{align}
=\sum \lim_{t \to 0} \frac{\big(b^i(p+ta)\frac{\partial}{\partial x^i} \Bigr\rvert_{p+ta}\big)-\big(b^i(p)\frac{\partial}{\partial x^i} \Bigr\rvert_{p}\big)}{t}. \tag{2}
\end{align} If (2) is the right way, then I see why is not possible to compare $Y_q$ and $Y_p$ , and the reason is because they are in distinct tangent spaces. In $\mathbb{R}^n$ we do not have this problem, because there are a canonical basis for all the tangent spaces, and that is why we can compare tangent vectors in distinct tangent spaces. But if (1) is right then I do not see why we can not define the directional derivative of vector fields on a manifold in the same way is done in $\mathbb{R}^n$ . What am I not understanding correctly? Thanks for your help.","['manifolds', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4551504,Tubular neighborhood of a surface in a 3-manifold,"Consider a smooth $3$ -manifold $M$ , a submanifold $P$ of dimension $2$ and a non-vanishing vector field $V$ transverse to $P$ . Does there exist a neighborhood of $P$ diffeomorphic to $P\times (-\varepsilon, \varepsilon)$ , such that $V$ is pushed forward to $\partial z$ ? Here $\partial z$ is vector field corresponding to $(-\varepsilon, \varepsilon)$ direction. If that is not the case, what would be necessary conditions for that? I am not sure if this helps but this question is motivated by trying to think about convex surfaces for Reeb vector field in contact geometry.","['contact-geometry', 'submanifold', 'smooth-manifolds', 'differential-geometry']"
4551523,Deriving a second-order system of differential equations that describes the motion of a planet (in cartesian coordinates),"I got a question that’s probably very basic but I just can’t figure it out. I want to derive a system of second order differential equations that describes the motion of a planet in (x,y)-coordinates (cartesian coordinates). I’ve seen ways of doing this by switching to polar coordinates but I want to know how to derive it without switching to polar coordinates.
I don’t know where to start. If we have the following equations: $$
F = \frac{-GMm}{r^2} \text{ and } F = ma
$$ We can combine these two and then write it as $$
a = \frac{-GM}{r^2}
$$ Where $r = \sqrt{x^2+y^2}$ which will give us $$
a = -\frac{GM}{x^2+y^2}
$$ But this doesn’t seem right. If anyone could help me figure out how to derive this so that I get a system of second order differential equations I would be very grateful. Thanks!","['celestial-mechanics', 'ordinary-differential-equations']"
4551571,"Quadrilateral with 2 known coordinates, one known edge vector and 4 known lengths, what are the missing coordinates?","I'm doing some generative design for architecture and I can't quite get my high school geometry over the line on this one. I have filled pages with pythagoras; I must be ignorant of a more powerful method: Simple diagram I know all four lengths of a convex quadrilateral, AB, BC, CD, DE.
I know the coordinates of A and B
I know that CD is flat ie. it has a normalized vector of (1,0,0)
I need to find the coordinates of C and D (they share Y values) (I'm aware that there are two solutions either side of AB, but it will be trivial to know which one I want)","['quadrilateral', 'geometry']"
4551587,"Did I solve this funky integral right? $\int_{-\infty}^{\infty}\frac{\sin(\sin(x))}{x}\,dx$","So I tried to solve this integral: $$\int_{-\infty}^{\infty}\frac{\sin(\sin(x))}{x}\,dx$$ By turning it into a contour integral with a small semi-circle to avoid the singularity at z=0, and taking the imaginary part: $$\oint\frac{e^{\sin(z)i}}{z}\,dz$$ The answer I got is $\pi$ , but Wolfram Alpha can’t evaluate the integral. How can I check my answer ?","['integration', 'definite-integrals', 'complex-analysis', 'calculus', 'residue-calculus']"
4551590,"What is $E[1/\|x\|^4]$ where $x\sim$ Gaussian($0,C\cdot $diag($1,1/2,1/3,1/4,\ldots$))?","Let $\Sigma_d$ be a diagonal matrix with diagonal $1,1/2,1/3,\ldots,1/d$ Let $E_d$ denote expectation w.r.t normally distributed $x$ which is centered at 0 with covariance matrix $\frac{\Sigma_d}{\operatorname{Tr}\Sigma_d}$ What is the value of the following limit? $$\lim_{d\to \infty} E_d\left[\frac{1}{\|x\|^4}\right]$$ You can show that $\lim_{d\to\infty} E_d[\|x\|^4]=1$ by using Gaussian fourth-moment formula $E[\|x\|^4]=(\operatorname{Tr}\Sigma)^2+2\operatorname{Tr}(\Sigma^2)$ Numerical simulation suggests the limit in question might be equal to $1$ , any tips? notebook Closely related issue was discussed on stats.SE and on Mathoverflow in the last month. Question is still open.","['integration', 'statistics', 'probability-theory', 'normal-distribution']"
4551630,What is wrong with this cardinality-based proof of the Cantor–Schröder–Bernstein theorem?,"Let $A$ and $B$ be sets, not necessarily finite. Let $f: A\rightarrow B$ be injective, and let $g:B\rightarrow A$ be injective. The Cantor–Schröder–Bernstein theorem then says there must be a bijection between $A$ and $B$ . I once got the proof of this assigned on a problem set, and when I presented the following proof, the TA said that it was insufficient. Take $\vert X\vert$ to be the cardinality of a set $X$ . $f$ injective $\implies \vert A\vert\le\vert B\vert$ $g$ injective $\implies \vert A\vert\ge\vert B\vert$ Therefore, $\vert A\vert=\vert B\vert$ , and there is a bijection between $A$ and $B$ . $\square$ I have never understood why this proof is insufficient but assume it has something to do with $\le$ , $\ge$ , and $=$ when it comes to comparing infinite cardinalities. What is inadequate about that proof?","['elementary-set-theory', 'cardinals', 'functions', 'fake-proofs']"
4551639,Flaw in my proof that $\mathbb Q$ is countable?,"I got an idea about an easy proof that $\mathbb Q$ is countable, one that does not involve the typical ""snake around a grid"" argument . However, I fear that I am making the same mistake in using an inequality with infinite cardinalities that I seemed to make in an early attempt at a short proof of the Cantor–Schröder–Bernstein theorem . Have I made a mistake, or is this a legitimate proof? If this proof is not correct, can it be remedied with a simple fix (in your own judgment of what a ""simple"" fix would be)? Take it as known, for countable $A$ and $B$ , $A\times B$ also is
countable. Also, take it as known that $\mathbb Z$ and $\mathbb
 Z_{>0}$ are countable. Let $f:\mathbb Z\times\mathbb Z_{>0}\rightarrow \mathbb Q$ be defined
by $f(a,b)=\frac{a}{b}$ . Then, given $q\in \mathbb Q$ , there is $(a,b)\in Z\times\mathbb Z_{>0}$ such that $f(a,b)=q$ , so $f$ is
surjective. Therefore, $cardinality(Z\times\mathbb Z_{>0})\ge cardinality(\mathbb
 Q)$ . Since $Z\times\mathbb Z_{>0}$ is countable, then $\mathbb Q$ must be
countable, too. $\square$ (A simple fix might be that it requires proof to say that inequalities with infinite cardinalities work the way we might hope, but that such proof is given in the proof of the CSB theorem, to which an answer to the linked question alludes.)","['elementary-set-theory', 'cardinals', 'fake-proofs', 'rational-numbers']"
4551701,Sum of reciprocal of primes failed computation,"Set $$X:=\sum_p \dfrac{1}{p^2}=\dfrac{1}{2^2}+\dfrac{1}{3^2} +\dfrac{1}{5^2}+\cdots$$ As $X$ is absolute convergent and less than $1$ , we have (not sure for infinite rearrangement) naive calculation implies $$\dfrac{\pi^2}{6}-1=\sum_{\substack{n\\ \Omega(n)=1}}\dfrac{1}{n^2}+\sum_{\substack{n\\ \Omega(n)=2}}\dfrac{1}{n^2}+\cdots,$$ where $\Omega(n)$ denotes the number of prime factors of $n$ counting multiple. The right term above is $$X+X^2+\cdots.$$ Thus, $X=1-6/\pi^2<0.4$ , but with the answer in the following link this is false. https://mathoverflow.net/questions/53443/sum-of-the-reciprocal-of-the-primes-squared Is it possible to fill the gap of the calculation?","['algebra-precalculus', 'summation', 'analysis', 'prime-numbers']"
4551757,Poincare Formula in measure theory,"Let $E_1,\dots,E_n$ be a finite sequence of measurable sets of $\mathbb{R}^m$ of finite measure w.r.t. $\mu$ .
For all integer $p\in [1,n]$ , we consider $$
\sigma_p = \sum_{i_1<\dots <i_p} \mu(E_{i_1}\cap \dots \cap E_{i_p}) 
$$ The aim of the exercise is to prove the Poincaré Lemma and I took the exercise from page 37 of Georges, Exercices in Integration, 1984 available here . He defines for $A\subset \{1,\dots, n\}$ : $$
E_A = \bigcap_{i\in A} E_i, \quad E'_A =  E_A \setminus \bigcup_{i\notin A} E_i
$$ and he says that $E_A'$ are mutually disjoint. I can't see this point. I made some computation using De Morgan property and I get","['elementary-set-theory', 'measure-theory']"
4551780,Why can't wolfram alpha solve this simple quintic?,"So I found out that some transformations break wolfram alpha's ability to solve polynomials. The simplest case I could find is the polynomial $$2x^{5}+5x^{4}+5x^{2}+1=0$$ for which the solution is $$x=\frac{1}{\sqrt[5]{\sqrt{2}-1}-\sqrt[5]{\sqrt{2}+1}}=\frac{\sqrt[5]{12\sqrt{2}-17}-\sqrt[5]{12\sqrt{2}+17}-\sqrt[5]{2\sqrt{2}+3}+\sqrt[5]{2\sqrt{2}-3}-1}{2}$$ Which derives simply from the solvable Moivre quintic $$x^{5}+5ax^{3}+5a^{2}x+2b=0,\ x=\sqrt[5]{\sqrt{a^{5}+b^{2}}-b}-\sqrt[5]{\sqrt{a^{5}+b^{2}}+b}$$ which wolfram alpha can solve. But for some reason, when I try this polynomial, wolfram alpha can only approximate it. Weirder, wolfram alpha is entirely capable of solving other quintics with a sum of $4$ radicals and a rational number, like $$x^{5}+5x^{4}+10x^{3}+10x^{2}+8=0,\ x=\frac{\sqrt[5]{750\sqrt{25+5\sqrt{5}}+375\sqrt{25-5\sqrt{5}}-1250\sqrt{5}-3125}+\sqrt[5]{375\sqrt{25+5\sqrt{5}}-750\sqrt{25-5\sqrt{5}}+1250\sqrt{5}-3125}-\sqrt[5]{750\sqrt{25+5\sqrt{5}}+375\sqrt{25-5\sqrt{5}}+1250\sqrt{5}+3125}-\sqrt[5]{375\sqrt{25+5\sqrt{5}}-750\sqrt{25-5\sqrt{5}}-1250\sqrt{5}+3125}-5}{5}$$ and that example is considerably more complicated than this $1$ . A few more sophisticated examples exist, like $ax^{5}+bx^{2}+c=0$ , which is solved with hypergeometric functions. Wolfram alpha does use hypergeometric functions to solve other polynomials, like $ax^{5}+bx+c=0$ , so what gives?","['irreducible-polynomials', 'algebra-precalculus', 'polynomials']"
4551804,Troubles with a degenerate conics,"I'm studying the following conics with respect upon a parameter $t$ (real). $$2tx^2 + 2txy + 4y + 1 = 0$$ For those kinds of problems I have always followed this Wikipedia page, which I find the most clear among all the notes and websistes I have been searching though, https://en.wikipedia.org/wiki/Matrix_representation_of_conic_sections Yet There is something I cannot catch. So the matrix invariants in this case are $$\text{det} \begin{pmatrix} 2t & t & 0 \\ t & 0 & 2 \\ 0 & 2 & 1 \end{pmatrix} = -t(t+8)$$ Thence I get a degenerate conics if $t = 0$ or $t = -8$ . So far so good. The second invariant is $$\text{det} \begin{pmatrix} 2t & t \\ t & 0 \end{pmatrix} = -t^2$$ Which is always negative for every real $t$ , and it's zero for $t =0$ Now I am interested in the degenerate cases, but when I am going to study the $t = 0$ case I get stuck. If $t = 0$ the conics is degenerate, and also the determinant of the second matrix is zero which means it's a pair of parallel lines. If I plot the resultin equation for $t = 0$ I simply get $4y + 1 = 0$ which I expect to be ONE single line. However, suppose they are two parallel lines. Next step tells me that those lines are distinct, coincident or imaginary respectively if $D^2 + E^2$ is greater, equal or less than $4 (A+C) F$ . Here $D = 0$ and $E = 4$ . $A + C = 0$ hence I get $16 > 0$ , which means two parallel distinct lines. I am stuck over this for I cannot get where those two lines are.","['matrices', 'conic-sections', 'linear-algebra', 'geometry']"
4551805,Factor of 4 appears in Jacobian coordinate transformation,"I am was reading the wikipedia page on metric tensors , when I saw something that was hard to grasp in the coordinate transformation section. This topic is a little bit uncomfortable to me, so maybe I have missed something, but there appears to be a factor of 4 that appears when working everything out by hand? With r being a vector valued function $\vec{r}(u,\,v) = \bigl( x(u,\,v),\, y(u,\,v),\, z(u,\,v) \bigr)$ , and with the following identity $$
\begin{bmatrix}
\frac{\partial r}{\partial u}\frac{\partial r}{\partial u} &
\frac{\partial r}{\partial u}\frac{\partial r}{\partial v} \\
\frac{\partial r}{\partial u}\frac{\partial r}{\partial v} &
\frac{\partial r}{\partial v}\frac{\partial r}{\partial v}
\end{bmatrix}
=
\begin{bmatrix}
E  &
F  \\
F  &
G
\end{bmatrix}
$$ the coordinate transformation is given by, $$
\begin{aligned}
\begin{bmatrix}
E^\prime & F^\prime \\ F^\prime & G^\prime
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial u}{\partial u^\prime} & \frac{\partial u}{\partial v^\prime} \\
\frac{\partial v}{\partial u^\prime} & \frac{\partial v}{\partial v^\prime} \\
\end{bmatrix}^\top
\begin{bmatrix}
E & F \\ F & G
\end{bmatrix}
\begin{bmatrix}
\frac{\partial u}{\partial u^\prime} & \frac{\partial u}{\partial v^\prime} \\
\frac{\partial v}{\partial u^\prime} & \frac{\partial v}{\partial v^\prime} \\
\end{bmatrix}
\end{aligned}
$$ With the following substitution in the coordinate transformation matrix, $$
\begin{bmatrix}
\frac{\partial u}{\partial u^\prime} & 
\frac{\partial u}{\partial v^\prime} \\
\frac{\partial v}{\partial u^\prime} &
\frac{\partial v}{\partial v^\prime}
\end{bmatrix}
=
\begin{bmatrix}
A & 
B \\
C &
D
\end{bmatrix}
$$ The transformation then becomes, $$
\begin{aligned}
\begin{bmatrix}
A & B \\ C & D
\end{bmatrix}^\top
\begin{bmatrix}
E & F \\ F & G
\end{bmatrix}
\begin{bmatrix}
A & B \\ C & D
\end{bmatrix}
= 
\begin{bmatrix}
\underbrace{A^2 E + 2 ACF + C^2G}_{E^\prime} & \underbrace{ABE + BCF + AFD + CDG}_{F^\prime} \\
\underbrace{ABE + BCF + AFD + CDG}_{F^\prime} & \underbrace{B^2E + 2BFD + D^2G}_{G^\prime}
\end{bmatrix}
\end{aligned}
$$ Plugging the values into the variables give the following expressions, $$
\begin{aligned}
E^\prime &= \frac{\partial u}{\partial u^\prime}\frac{\partial u}{\partial u^\prime}
 \frac{\partial r}{\partial u}\frac{\partial r}{\partial u} 
+ 2 \frac{\partial u}{\partial u^\prime}\frac{\partial v}{\partial u^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v}
+ \frac{\partial v}{\partial u^\prime}\frac{\partial v}{\partial u^\prime}\frac{\partial r}{\partial v}\frac{\partial r}{\partial v} \\
F^\prime &= \frac{\partial u}{\partial u^\prime}\frac{\partial u}{\partial v^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial u} + \frac{\partial u}{\partial v^\prime}\frac{\partial v}{\partial u^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v} + \frac{\partial u}{\partial u^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v}\frac{\partial v}{\partial v^\prime} + \frac{\partial v}{\partial u^\prime}\frac{\partial v}{\partial v^\prime}\frac{\partial r}{\partial v}\frac{\partial r}{\partial v} \\
G^\prime &= \frac{\partial u}{\partial v^\prime}\frac{\partial u}{\partial v^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial u} + 2\frac{\partial u}{\partial v^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v}\frac{\partial v}{\partial v^\prime} + \frac{\partial v}{\partial v^\prime}\frac{\partial v}{\partial v^\prime}\frac{\partial r}{\partial v}\frac{\partial r}{\partial v}
\end{aligned}
$$ after simplifying by cancelling out similar factors in the numerator and denominator and summing the result, everything comes out to, $$
\require{cancel}
\begin{aligned}
E^\prime &= \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\cancel{\partial u}}{\partial u^\prime}
 \frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial u}} 
+ 2 \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\cancel{\partial v}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}}
+ \frac{\cancel{\partial v}}{\partial u^\prime}\frac{\cancel{\partial v}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial v}}\frac{\partial r}{\cancel{\partial v}} \\
F^\prime &= \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\cancel{\partial u}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial u}} + \frac{\cancel{\partial u}}{\partial v^\prime}\frac{\cancel{\partial v}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}} + \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}}\frac{\cancel{\partial v}}{\partial v^\prime} + \frac{\cancel{\partial v}}{\partial u^\prime}\frac{\cancel{\partial v}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial v}}\frac{\partial r}{\cancel{\partial v}} \\
G^\prime &= \frac{\cancel{\partial u}}{\partial v^\prime}\frac{\cancel{\partial u}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial u}} + 2\frac{\cancel{\partial u}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}}\frac{\cancel{\partial v}}{\partial v^\prime} + \frac{\cancel{\partial v}}{\partial v^\prime}\frac{\cancel{\partial v}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial v}}\frac{\partial r}{\cancel{\partial v}}
\end{aligned}
$$ which then reduces to the following by adding the remaining terms, $$
\begin{bmatrix}
E^\prime &
F^\prime \\
F^\prime &
G^\prime
\end{bmatrix} 
=
4\begin{bmatrix}
\frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial u^\prime} &
\frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial v^\prime} \\
\frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial v^\prime} &
\frac{\partial r}{\partial v^\prime}\frac{\partial r}{\partial v^\prime}
\end{bmatrix}
$$ The Wikipedia article states (above equation 2') that the values of $E^\prime, F^\prime, G^\prime$ are in fact $
E^\prime = \frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial u^\prime}, \;\; 
F^\prime = \frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial v^\prime}, \;\;
G^\prime = \frac{\partial r}{\partial v^\prime}\frac{\partial r}{\partial v^\prime}
$ which then means that I get the following expression after simplifying my manual transformation above, and comparing it with the definition of $E^\prime, F^\prime, G^\prime$ from Wikipedia. $$
\begin{aligned}
\begin{bmatrix}
E^\prime & F^\prime \\ F^\prime & G^\prime
\end{bmatrix}
\neq
4\begin{bmatrix}
E^\prime & F^\prime \\ F^\prime & G^\prime
\end{bmatrix}
\end{aligned}
$$ Questions How can I reconcile that this factor of 4 comes out? Is it just because the factor of 4 becomes irrelevant for an infinitesimal difference? Or have I made a terrible error somewhere? Generally, how can I understand coordinate transformations when dealing with a Jacobian matrix, are they the same thing as a change of basis in linear algebra when we see the form $P^{-1}AP$ ? What is the significance here that this form is $P^\top AP$ with a transpose instead of an inverse? The Jacobian is highly unlikely to be orthonormal (right?), so the transpose is definitely not the inverse.","['metric-spaces', 'jacobian', 'multivariable-calculus', 'calculus', 'linear-algebra']"
4551837,How to prove triangle inequality for euclidean distance on $\mathbb{R}^2$ without using Cauchy-Schwarz?,"Joseph Muscat's Functional Analysis asks the reader to prove $d(x,y)= \sqrt{|a_1-b_1|^2+|a_2-b_2|^2}$ on $\mathbb{R}^2$ satisfies the triangle inequality: $\sqrt{|a_1-b_1|^2+|a_2-b_2|^2}\leq\sqrt{|a_1-z_1|^2+|a_2-z_2|^2}+\sqrt{|z_1-b_1|^2+|z_2-b_2|^2}$ This is before the Cauchy-Schwarz inequality is explained. I figured it would be the same as proving the triangle inequality for $|a_1-b_1|$ on $\mathbb{R}$ , but I seriously don't know where to go from there to the inequality. This was my attempt: $d(x,y) = \sqrt{|a_1-b_1|^2+|a_2-b_2|^2} =\sqrt{|a_1-z_1+z_1-b_1|^2+|a_2-z_2+z_2-b_2|^2}$ $|a_1-z_1+z_1-b_1|\leq|a_1-z_1|+|z_1-b_1|$ So $\sqrt{|a_1-z_1+z_1-b_1|^2+|a_2-z_2+z_2-b_2|^2}\leq\sqrt{(|a_1-z_1|+|z_1-b_1|)^2+(|a_2-z_2|+|z_2-b_2|)^2}$ Expanding the right-hand side, we get: $\sqrt{|a_1-z_1+z_1-b_1|^2+|a_2-z_2+z_2-b_2|^2}\leq\sqrt{(|a_1-z_1|^2+|z_1-b_1|^2+2|a_1-z_1||z_1-b_1|+|a_2-z_2|^2+|z_2-b_2|^2+2|a_2-z_2||z_2-b_2|}$ or $\sqrt{|a_1-b_1|^2+|a_2-b_2|^2}\leq\sqrt{(|a_1-z_1|^2+|z_1-b_1|^2+2|a_1-z_1||z_1-b_1|+|a_2-z_2|^2+|z_2-b_2|^2+2|a_2-z_2||z_2-b_2|}$ I don't know where to go from here.
Is this approach reasonable?","['euclidean-geometry', 'inequality', 'functional-analysis', 'metric-spaces']"
4551857,"If $T$ is strictly upper triangular, can we find upper triangular $A,B \in M_n(\Bbb C)$ such that $T = AB- BA$?","Consider $M_n(\Bbb C)$ , the vector space of matrices taking entries from $\Bbb C$ ,  for $n \ge 2$ . We know that the collection of upper triangular matrices in $M_n(\Bbb C)$ forms a subspace of $M_n(\Bbb C)$ . Additionally, if we start with two upper triangular matrices $A,B \in M_n(\Bbb C)$ , then the commutator $AB-BA$ is strictly upper triangular. I believe the converse is true, i.e., given a strictly upper triangular matrix $T \in M_n(\Bbb C)$ , we can find two upper triangular matrices $A,B \in M_n(\Bbb C)$ such that $T = AB - BA$ . I have thought of some examples (as follows) where this can be done, but I have not yet written a proof that takes care of all cases. If $T = 0$ , then any two commuting matrices $A,B \in M_n(\Bbb C)$ will do. In fact, we can just pick $A,B$ to be any two diagonal matrices. This already tells us that the choice of $A,B$ need not be unique. One of the simpler cases to consider is $n = 2$ . If $A_1 = \left(\begin{matrix} p_1 & q_1\\ 0 & r_1 \end{matrix}\right)$ and $A_2 = \left(\begin{matrix} p_2 & q_2\\ 0 & r_2 \end{matrix}\right)$ , then $$A_1A_2 - A_2A_1 = \left(\begin{matrix} 0 & p_1q_2 + q_1r_2-p_2q_1-q_2r_1\\ 0 & 0  \end{matrix}\right)$$ Given a strictly upper triangular matrix $T = \left(\begin{matrix} 0 & \lambda \\ 0 & 0 \end{matrix}\right)$ , we need to solve $$p_1q_2 + q_1r_2-p_2q_1-q_2r_1 = \lambda,$$ i.e., one equation in eight variables. This system is overdetermined, so we can pick values of $p_1, p_2, \ldots$ , etc. that solve the equation. There are infinitely many solutions! To repeat the same process for $M_n(\Bbb C)$ is cumbersome, and I'm hoping a proof by induction of some sort might do the trick. Thank you!","['matrices', 'abstract-algebra', 'linear-algebra', 'induction', 'matrix-equations']"
4551880,What is $\mathcal O(S\times \mathbb A^n_\mathbb Z)$?,"After many edits, here is my question: Is it true for a scheme $S$ that $\mathcal O(S\times \mathbb A_\mathbb Z^n) = \mathcal O(S)\otimes_\mathbb Z \mathcal O(\mathbb A^n)$ or not? Let $\mathbb A$ be the forgetful functor on the category of commutative rings. It is a ring object internal to the category of functors on $\text{cRing}$ . For each functor $X:\text{cRing}\to \text{Set}$ , let $\mathcal O(X) = \text{Hom}(X,\mathbb A)$ be the set of natural transformations form $X$ to $\mathbb A$ . Each $\mathcal O(X)$ get a ring structure from the internal ring structure of $\mathbb A$ . Is it true that $$\mathcal O(S\times \mathbb A^n) = \mathcal O(S)\otimes_\mathbb Z \mathbb Z[x_1,...,x_n] = \mathcal O(S)[x_1,...,x_n]$$ as commutative rings? I read lecture notes from Marc Nieper-Wißkirchen which claim this, but I am not able to compute it myself. I feel really stupid. There is an adjunction $\text{Hom}_{\text{Pr}(\text{cRing}^{op})}(X,yA) = \text{Hom}_{\text{cRing}}(A,\mathcal OX)$ , but it only implies that $\mathcal O$ sends colimits to limits. I know that the equation is true for representable $S$ . Do I need to write the functor $S$ as a colimit of representables? Edit. It seems like the result is not true for a general functor. We can pass to the smaller subcategories of sheaves in the Zariski topology and of (functorial) schemes. Colimits in the big Zariski topos Zar of sheaves in the Grothendieck topology are computed differently than in the presheaf category (sheafification). The ring $\mathcal O(S\times \mathbb A^n)$ does not depend on the surrounding category though. Does the equation $\mathcal O(S\times \mathbb A^n) = \mathcal O(S)[x_1,...,x_n]$ at least hold if $S$ is a scheme? Edit. Edit. Okay, now I am totally confused. Qiaochu Yuan claims below that $\mathcal O(S\times \mathbb A^n) \neq \mathcal O(S)[x_1,...,x_n]$ for general schemes, but I have the following argument. According to the stacks project the projection map $\mathbb A^n\times S\to S$ is up to isomorphism $\underline{Spec}_S(\mathcal O_S[T_1,...,T_n])\to S$ . The Spec thing here is the relative spec construction and it acts on a $\mathcal O_S$ -algebra! Let me denote the structure map by $$p:\underline{Spec}_S(\mathcal O_S[T_1,...,T_n])\to S$$ Now the relative Spec construction satisfies in general that $p_*\mathcal O_{\underline{Spec}_S\mathcal A} = \mathcal A$ for any $\mathcal O_S$ -algebra $\mathcal A$ . So I can compute the global functions on the scheme over $S$ as follows. \begin{align}\mathcal O(S\times \mathbb A^n) &= \mathcal O(\underline{Spec}_S(\mathcal O_S[T_1,...,T_n])) \\&= \mathcal O_{\underline{Spec}_S(\mathcal O_S[T_1,...,T_n])}(p^{-1}S)\\
&= p_*\underline{Spec}_S(\mathcal O_S[T_1,...,T_n])(S)
\\&= \mathcal O_S[T_1,...,T_n](S)\\
&= \mathcal O(S)[x_1,...,x_n]
\end{align} Here $\mathcal O_S$ is a sheaf of algebras and $\mathcal O(S)$ is a ring! What is wrong with my calculation? The step where I am not sure about is what the global sections of the sheaf $\mathcal O_S[T_1,...,T_n]$ are. Is the counterexample incorrect? (I must admit that I do not understand it completely). The original lecture notes can be found at the end of the article as a pdf, and the claim by M. Nieper-Wißkirchen is on page 58 equation (4.5.12) Also here is an argument which I believe does work for schemes which can be covered by finitely many open affines. Write $S$ as a colimit $S = \text{colim}_iU_i$ of finitely many open affine subfunctors. Make the following computation \begin{align}
\mathcal O(\mathbb A^n\times S) &= \mathcal O(\mathbb A^n \times (\operatorname{colim}_{i}U_i))\\
&= \mathcal O(\operatorname{colim}_i(\mathbb A^n\times U_i)) \\
&= \operatorname{lim}_i\mathcal O(\mathbb A^n\times U_i) \\
&= \operatorname{lim}_i(\mathcal O\mathbb A^n \otimes_\mathbb Z \mathcal OU_i) \\
&= \mathcal O\mathbb A^n \otimes_\mathbb Z \operatorname{lim}_i\mathcal OU_i \\
&= \mathcal O\mathbb A^n\otimes_\mathbb Z\mathcal OS
\end{align} where I now used that $-\otimes_\mathbb Z\mathcal O\mathbb A^n$ is exact and commutes with finite limits.","['algebraic-geometry', 'category-theory']"
4551885,Is there any application of Banach fixed point theorem in number theory?,Is there any application of Banach fixed point theorem in number theory? I have seen some applications in integral equations and economics. I am curious about number theory.,"['number-theory', 'soft-question', 'fixed-point-theorems', 'real-analysis']"
4552020,"Why does this formula yield a clockwise rotation for curves defined by a cartesian equation, and a counterclockwise rotation in parametric form?","Here is the reasoning I've attemped in order to espablish a formula for the counterclockwise rotation of a curve. Let $M=(x,y)$ be any point on the plane, associated with a cartesian coordinate system $<0, \vec { i}, \vec{j}>$ . The position vector of ths point is $\vec {OM} = \vec A+ \vec B= x\vec { i} + y \vec{j} $ Rotating point $M$ counterclockwise by a positve angle $\alpha$ amounts to rotate counterclockwise  the components of its postion vector by the same angle. So , with $M'$ the image of $M$ under ths rotation, we will have : $\vec {OM'} = \vec {A'}+\vec {B'}$ with $\vec {A'}$ , the image of $\vec {A}$ and $\vec {B'}$ , the image of vector $\vec {B}$ . Now , $\vec {A'}= x cos (\alpha) \vec i + x sin (\alpha) \vec j$ . $\vec {B'}= y cos (\alpha + \pi /2) \vec i + x sin (\alpha+ \pi/2) \vec j $ $= y ( - sin (\alpha) )\vec i + x cos(\alpha) \vec j$ $= -y  sin (\alpha) )\vec i + x cos(\alpha)\vec j$ . Therefore , $\vec{OM'} =  \vec {A'} + \vec {B'} = \Large{[} x cos (\alpha) - y  sin (\alpha)\Large{]} \vec i + \Large{[}x sin( \alpha) + y cos(\alpha)\Large{]} \vec j$ , implying that $M' = (x cos (\alpha) - y  sin (\alpha) , x sin( \alpha) + y cos(\alpha))$ . My problem is that : (1) when I apply this reasoning to a parametric curve, it seems to work , that is, if I have a curve $C= < f(t), g(t)> $ and ask for $$C' = <f(t) cos (\alpha) - g(t)  sin (\alpha) , f(t) sin( \alpha) + g(t) cos(\alpha)>$$ what I get is that $C'$ rotates counterclockwise, as desired. Example with $\alpha = 2 $ rd: https://www.desmos.com/calculator/daaouk87yw (2) but if I substitute the coordinates of a generic $M'$ point in a cartesian "" $x-y$ "" equation, the curve rotates clockwise . Example : https://www.desmos.com/calculator/vm5pbtxzge I'm sure some confusion prevents me from understanding what is happening, but I can't manage to see which one.","['analytic-geometry', 'algebra-precalculus', 'geometry', 'rotations']"
4552057,What is the defininition of phase Space in simple terms?,"I’m currently studying a Differential Equations Course, and I’m focusing on a chapter called “Systems of ODE”, the text gives a definition of phase space, but I can’t seem to visualise or understand what it is graphically. Could someone break it down? Here is the definition of phase space I’ve been given: For an autonomous $N$ -th order ODE system $dx/dt = f(x)$ , $x$ , $f \in \mathbb R^N$ , the phase space is $\text{dom }(f) \subset \mathbb R^N$ , that is, the sub-set of the $N$ -dimensional space, where the right-hand sides of the system are defined. Thank you","['systems-of-equations', 'ordinary-differential-equations', 'differential-geometry']"
4552083,Equivalence Relations from $\mathbb Z \to\mathbb Z$,"The question is, ""Which of these relations on the set of all functions from $\mathbb Z$ to $\mathbb Z$ are equivalence relations."" The first relation to consider is $\{(f,g)|f(1)=g(1)\}$ ,it seems easy ,but I don't know how to handle The second relation to consider is, $\{(f,g)|f(0)=g(0)∨f(1)=g(1)\}$ For this one, I am not very certain where to begin. The third one, $\{(f,g)|f(x)−g(x)=1,\forall x∈\mathbb Z\}$ I can see how this isn't reflexive, because f(x)−f(x)=0 is always true for any function. I can also see how it isn't symmetric, because although $f(x)−g(x)=1$ could be true, it's counter-part, $g(x)−f(x)=−1$ , won't be true. Despite me being able to see those facts, I can not see how it isn't transitive. How would I show that? The fourth one, $\{(f,g)|$ for some $C ∈\mathbb Z,\forall x∈\mathbb Z,f(x)−g(x)=C\}$ I had the idea that it wasn't an equivalence relation based on the fact that if we let $f(x)=x$ , and $g(x)=x−1$ , and say x=1, then $f(1)−g(1)=C\implies C=1$ , but $g(1)−f(1)=C\implies C=−1$ . The C values aren't the same, implying the relation wouldn't be symmetric, right? Also, I am not sure how to prove or disprove that the relation is transitive. The last one is similar to the first one: $\{(f,g)|f(0)=g(1)∧f(1)=g(0)\}$ ; and like the first one, I am not sure where to begin. Sorry that this post is rather long. But thank you for reading! and I hope that you can help me.","['equivalence-relations', 'functions', 'relations']"
4552092,Equidistributed Random Variables[No Fancy Stuff],"Let me first set up the background. It might be tedious but not fancy I promise. Let the probability space be the interval $[0,1]$ of the real line. Let the sigma algebra be the sigma algebra generated by Lebesgue measurable sets on $[0,1]$ . The probability measure is the Lebesgue measure on $[0,1]$ . Note that for every $\omega \in [0,1]$ , we have the binary expansion as follows: $$\omega=\sum_{n=1}^\infty \beta_n 2^{-n},\,\text{where } \beta_n=\beta_n(\omega) \text{ which is either 0 or 1}.$$ Just by convention, we require one more condition that $\sum \beta_n = \infty$ except for $\omega=0$ . This condition basically means that in the expansion there are infinitely many $1$ 's. Therefore, we can view $\beta_n$ as a function from $[0,1]$ to $\{0,1\}$ , which is a random variable in our probability space. For every fixed $j\in \mathbb N$ , we may further define $$\omega_j=\omega_j(\omega)=\sum_{n\in \mathbb N} \beta_{m(n,j)}2^{-n},\text{ where }m \text{ is an arbitrary but fixed bijection between } \mathbb N^2 \mbox{ and } \mathbb N.$$ The author then wrote that, each of $\omega_j$ is equidistibuted on $[0,1]$ , i.e., given a subinterval $I$ of $[0,1]$ , the probability of the $\omega$ -set where $\omega_j(\omega)\in I$ is the length of $I$ . My question is, why the probability of the $\omega$ -set where $\omega_j(\omega)\in I$ is the length of $I$ ? My thoughts: Given a fixed $\omega\in I$ , we have its binary expansion, which determines all $\beta_n$ 's. Fix $j\in \mathbb N$ , $\{m(n,j)| n\in \mathbb N\}$ is a proper subset of $\mathbb N$ , which means that, $\omega_j$ is defined by only using some of $\beta_n$ 's. For those $n$ which are not used in defining $\omega_j$ , even if we alter the values of the corresponding $\beta_n$ resulting in new $\tilde{\omega}$ , we still have $\omega_j(\omega)=\omega_j(\tilde{\omega})$ . Therefore, I guess we should have $|\omega_j^{-1}(I)|>|I|$ , where $|\cdot|$ means Lebesgue measure rather than cardinality. ( I know my reasoning is all about cardinality and it is different from measure but they are also somehow related, especially speaking of Lebesgue measure. My argument may not be sufficient, but I still somehow believe my guess that $|\omega_j^{-1}(I)|>|I|$ shall be correct ). Any comment or insight is welcome. Thanks!","['measure-theory', 'probability-theory', 'random-variables']"
4552097,evaluate $\lim\limits_{N\to\infty} \dfrac{\ln^2 N}{N}\sum_{k=2}^{N-2} \dfrac{1}{\ln k \cdot \ln (N-k)}$,"Evaluate $\lim\limits_{N\to\infty} \dfrac{\ln^2 N}{N}\sum_{k=2}^{N-2} \dfrac{1}{\ln k \cdot \ln (N-k)}$ where $\ln$ denotes the natural logarithm. Let $A_N = \dfrac{\ln^2 N}{N}\sum_{k=2}^{N-2} \dfrac{1}{\ln k \cdot \ln (N-k)}.$ Clearly $A_N \ge \dfrac{\ln^2 N}N \cdot \dfrac{N-3}{\ln^2 N} = 1-3/N$ for all $N$ so the main question is whether $A_N$ converges to 1 as $N\to\infty.$ Fix $2\leq M < N/2.$ We want to find an upper bound for $A_N$ in terms of $N$ and $M$ that converges to 1 as $N\to\infty$ when $M$ is chosen carefully enough. Note that by differentiating $f(x)=\dfrac{1}{\ln x\cdot \ln (N-x)},$ one can conclude that it is decreasing on $(1,N/2]$ and increasing on $[N/2,N-2]$ (one can equivalently analyze the behaviour of $\dfrac{1}{f(x)}$ by differentiating to simplify the calculations).  Using this observation, we have $\sum_{k=2}^{N-2} \dfrac{1}{\ln k\cdot \ln(N-k)} = (\sum_{k=2}^{M} + \sum_{k=M+1}^{N-M-1} + \sum_{k=N-M}^{N-2} )\dfrac{1}{\ln k\cdot \ln(N-k)}\leq \dfrac{2(M-1)}{\ln 2 \ln (N-2)} +\sum_{k=M+1}^{N-M-1} \dfrac{1}{\ln k\cdot \ln(N-k)}.$ But I'm not sure how to simplify the above sum.","['calculus', 'sequences-and-series', 'limits', 'inequality', 'derivatives']"
4552098,Whats the probability for person A to win the dice game?,"The rules: Two people rolls a single dice. If the dice rolls 1,2,3 or 4, person A gets a point. For the rolls 5 and 6, person B gets a point. One person needs a 2 point lead to win the game. This is a question taken from my math book. The answer says the probability is $\frac{4}{5}$ for person A to win the game. Which I dont understand. My thought process: Lets look at all the four possible outcomes with the two first roles. These would be AA, BB, AB or BA. AA means person A gets a point two times a row. Under is the probability for all these scenarios: $P(AA)=(\frac{2}{3})^2=\frac{4}{9}$ $P(BB)=(\frac{1}{3})^2=\frac{1}{9}$ $P(AB)=\frac{2}{3}\cdot\frac{1}{3}=\frac{2}{9}$ $P(BA)=\frac{1}{3}\cdot\frac{2}{3}=\frac{2}{9}$ If AB or BA happens they have an equal amount of points again, no matter how far they are into the game. The probability of this would then be $2\cdot\frac{2}{9}=\frac{4}{9}$ . Since they have an equal amount of points, you can look at that as the game has restarted. Meaning person A has to get two points a row to win no matter what. Would that not mean the probability is $\frac{4}{9}$ for person A to win? Can someone tell me where my logic is flawed and what the correct logic would be?","['markov-chains', 'probability']"
4552132,Does the set of rational numbers include repeating values? [duplicate],"This question already has answers here : Why can't a set have two elements of the same value? (9 answers) Closed 1 year ago . In my Discrete Math lecture, we were told that we should omit any number that repeats in the set of rational numbers $\mathbb{Q}$ . In other words, we should only keep $\frac{1}{1}$ and omit $\frac{2}{2}, \frac{3}{3}$ , etc. But I don't see why this would be the case, since the set is defined as $\mathbb{Q} = \{\frac{a}{b}|a,b\in\mathbb{N}, b\ne 0\} $ Does the set $\mathbb{Q}$ include these repeating values?","['discrete-mathematics', 'rational-numbers']"
4552139,"Example of a sequence of measures that does not converge to a measure despite converging ""pointwise""","So, I am trying to solve the following problem: given a sequence of measures $(\mu_n)_{n \in \mathbb{N}}$ on $(X, A)$ such that for each $B \in A$ the limit $\lim_{n \to \infty}  \mu_n(A) \in [0, \infty]$ exists, we define $\mu(A) = \lim_{n \to \infty}  \mu_n(A)$ . I want to find an example where $\mu$ is not a measure on $A$ . Clearly $\mu(B) \geq 0 \; \forall B$ and $\mu(\emptyset) = 0$ . So the only thing that should break is the countable additivity. I have already proven that if the sequence of measures $(\mu_n)_{n \in \mathbb{N}}$ is monotonically increasing, i.e. $\mu_n(B) \leq \mu_{n+1}(B) \; \forall B \in A$ and $\forall n \in \mathbb{N}$ , $\mu$ will be a measure. I also see that $\sum_j \mu(B_j) \leq \mu(\cup_j B_j)$ for any such $\mu$ - so, for $\mu$ to not be a measure I want to create an example where $\sum_j \mu(B_j) < \mu(\cup_j B_j)$ with the strict inequality for some $(B_j)$ . The following sequence of measures almost works on $\mathbb{N}$ : $\mu_i(B) = n_i(B) / i$ , where $n_i(B)$ is the count of natural numbers less than $i$ in $B$ . For every finite set of natural numbers $C$ we have $\lim_i\mu_i(C) = 0$ , but $\lim_i \mu_i (\mathbb{N}) = 1$ , so countable additivity breaks. However, such sequence of measures does not converge ""pointwise"" for all subsets of natural numbers, and I fail to see what $\sigma$ -algebra I need to consider for it to converge on all sets. Could someone please help me, either with my example (if it is even fixable), or with any other example? Thank you very much!","['measure-theory', 'real-analysis']"
4552143,Difference in interval notation,"What is the difference between these two exercise questions (regarding the intervals)? Show that for each $\epsilon > 0$ , $f_n$ is uniformly convergent on $[\epsilon, \infty)$ Show that $f_n$ is not uniformly convergent on $(0, \infty)$ I mean in the first interval we don't have the 0 because $\epsilon > 0$ and in the second we don't have the 0 because it is an open interval. So what is the difference?","['functional-analysis', 'real-analysis']"
4552158,Is it that easy to be Zariski dense?,"I've just heard about diagonizable matrices being Zariski dense as a consequence of Cayley-Hamilton, and this is the proof I came up with (in $\mathbb{C}$ ): Let $O_{\cal F} = \{M\in{\cal M}_n(\mathbb C);\ \exists f \in {\cal F}, f(M)\neq 0\}$ be a non-empty open set, and $f\in {\cal F}$ .
Let $\lambda$ be such that $P_\lambda = X - \lambda$ does not divide $f$ . The matrix $\lambda I_n$ has $P_\lambda$ as its minimal polynomial, which does not divide $f$ , so $f(\lambda I_n) \neq 0$ and it belongs to $O_{\cal F}$ . We showed that there are diagonizable matrices in every open set, Q.E.D. This proof seems strange to me, we proved it quite easily, without directly involving Cayley-Hamilton, and in fact proved something much stronger that is to say that $\mathbb C I_n$ , a line, is Zariski dense. Moreover every other proof I've found in 5 minutes of googling looked much more complicated and involved more advanced concepts. Basically my proofs generalizes to any set that contains matrices with minimal polynomials with arbitrary roots. Did I understand Zariski's topology wrong, made a mistake in my proof, or is it really that easy to be Zariski dense ?","['matrices', 'general-topology', 'linear-algebra']"
4552256,Obtaining the formula of Bessel functions of the second kind $Y_{n}(x)$ as a series,"I am reading a textbook about engineering mathematics. To solve the Bessel differential equation $$
x^2 y'' + xy' + (x^2 - \nu^2) y = 0
$$ the book first obtains the function $J_{n}(x)$ using the series method as follows: $$
J_n(x) = x^n \sum_{m=0}^{\infty} 
\frac{(-1)^m x^{2m}}{2^{2m+n} m! (n+m)!}
$$ then obtains the Bessel function of the second kind of zero order $Y_{0}(x)$ using the series method as follows: $$
Y_{0}(x) = \frac{2}{\pi} 
\left[ J_0(x) \left( \ln\frac{x}{2} + \gamma \right) 
+ \sum_{m=1}^{\infty} 
\frac{(-1)^{m-1} h_m}{2^{2m}(m!)^2} x^{2m} \right]
$$ in which $$
h_m = 1 + \frac12 + \cdots + \frac1m
$$ and $\gamma \simeq 0.5772$ is the so-called ''Euler constant'', which is defined as the limit of $$
1 + \frac12 + \cdots + \frac1s - \ln s
$$ as $s$ approaches infinity. Then the author states that the Bessel function of the second kind of order $n$ is obtained in a similar way as: \begin{align}
Y_{n}(x) &= \frac{2}{\pi} J_{n}(x) 
\left(\ln\frac{x}{2} + \gamma \right) \\[2pt]
&\quad{} + \frac{x^n}{\pi} \sum_{m=0}^{\infty} 
\frac{(-1)^{m-1}(h_{m} + h_{m+n})}{2^{2m+n} m! (m+n)!} x^{2m} \\
&\quad{} - \frac{x^{-n}}{\pi} \sum_{m=0}^{n-1} 
\frac{(n-m-1)!}{2^{2m-n} m!} x^{2m}
\end{align} The book does not provide any proof for the latter formula. Can you explain me how to get it? The mentioned book is ""ADVANCED ENGINEERING MATHEMATICS"" by ""ERWIN KREYSZIG"". If you know a textbook that explains how to get this formula well, please introduce it to me Edit: I found a solution myself, but I'm not sure if the details are correct. here is the solution: According to the Frobenius method, a second solution is obtained from this formula: $${\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}={\it kJ}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}+{\it x}^{{\rm -}{\it n}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}{\it x}^{{\it m}}}$$ By differentiating this function twice, we get $y{_{2}}^{\prime}$ and $y{_{2}}^{\prime\prime}$ $$y{_{2}}^{\prime}(x)=kJ{_{n}}^{\prime}(x)Lnx+{{KJ_{n}(x)}\over{x}}+\sum\limits_{m=0}^{\infty}{(m-n)a_{m}x^{m-n-1}}$$ $$y{_{2}}^{\prime\prime}(x)=kJ{_{n}}^{\prime\prime}(x)Lnx+{{2KJ{_{n}}^{\prime}(x)}\over{x}}-{{KJ_{n}(x)}\over{x^{2}}}+\sum\limits_{m=0}^{\infty}{(m-n)(m-n-1)a_{m}x^{m-n-2}}$$ Substituting ${\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}$ , $y{_{2}}^{\prime}(x)$ , and $y{_{2}}^{\prime\prime}(x)$ into Bessel equation $x^{2}y''+xy'+(x^{2}-\upsilon^{2})y=0$ , we get $$kx^{2}J{_{n}}^{\prime}(x)Ln(x)+2kxJ{_{n}}^{\prime}(x)+kxJ{_{n}}^{\prime}(x)Ln(x)+k(x^{2}-n^{2})J_{n}(x)Ln(x)$$ $$+\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}{\it m}-{\it n}{\rm )(}{\it m}-{\it n}-{\rm 1}{\rm )}{\it a}_{{\it m}}{\it x}^{{\it m}{\rm -}{\it n}}}+\sum\limits_{{\it m}{\rm =0}}^{{\rm \infty}}{{\rm (}{\it m}-{\it n}{\rm )}}{\it a}_{{\it m}}{\it x}^{{\it m}{\rm -}{\it n}}+\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}}{\it x}^{{\it m}{\rm -}{\it n}{\rm +2}}-{\it n}^{{\rm 2}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}}{\it x}^{{\it m}{\rm -}{\it n}}={\rm 0}$$ $$\Rightarrow 2kxJ{_{n}}^{\prime}(x)+\sum\limits_{m=0}^{\infty}{\left[{(m-n)^{2}-n^{2}}\right]}a_{m}x^{m-n}+\sum\limits_{m=0}^{\infty}{a_{m}}x^{m-n+2}=0$$ or $$\Rightarrow 2kxJ{_{n}}^{\prime}(x)+\sum\limits_{m=0}^{\infty}{m(m-2n)}a_{m}x^{m-n}+\sum\limits_{m=0}^{\infty}{a_{m}}x^{m-n+2}=0\ \ \ \ \ \ (1)$$ find the derivative of ${\it J}_{{\it n}}{\rm (}{\it x}{\rm )}$ $$J_{n}(x)=\sum\limits_{m=0}^{\infty}{{{(-1)^{m}}\over{2^{2m+n}m!(m+n)!}}}x^{2m+n}\Rightarrow J{_{n}}^{\prime}(x)=\sum\limits_{m=0}^{\infty}{{{(-1)^{m}(2m+n)}\over{2^{2m+n}m!(m+n)!}}}x^{2m+n-1}$$ Substituting in $(1)$ we have: $${\rm 2}{\it k}\sum\limits_{{\it m}{\rm =0}}^{{\rm \infty}}{{{{\rm (}-{\rm 1}{\rm )}^{{\it m}}{\rm (}{\rm 2}{\it m}+{\it n}{\rm )}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm +}{\it n}}{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}}{\it x}^{{\rm 2}{\it m}{\rm +}{\it n}}+\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it m}{\rm (}{\it m}-{\rm 2}{\it n}{\rm )}{\rm a}_{{\it m}}{\it x}^{{\it m}{\rm -}{\it n}}}{\rm  }+\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}}{\it x}^{{\it m}{\rm -}{\it n}{\rm +2}}={\rm 0}$$ For the coefficient of $x^{-n}$ : $0a_{0}=0$ so $a_{0}$ is an arbitrary constant For the coefficient of $x^{1-n}$ : $1(1-2n)a_{1}=0\Rightarrow a_{1}=0$ And in general for the coefficient of $x^{p-n+2}$ $(p\leq 2n-3)$ : $${\rm (}{\it p}+{\rm 2}{\rm )(}{\it p}+{\rm 2}-{\rm 2}{\it n}{\rm )}{\it a}_{{\it p}{\rm +2}}+{\it a}_{{\it p}}={\rm 0}\Rightarrow{\it a}_{{\it p}{\rm +2}}=-{{{\it a}_{{\it p}}}\over{{\rm (}{\it p}+{\rm 2}{\rm )(}{\it p}+{\rm 2}-{\rm 2}{\it n}{\rm )}}}$$ And according to this recursive relation, we have: $${\it a}_{{\rm 3}}={\it a}_{{\rm 5}}=\cdots={\it a}_{{\rm 2}{\it n}{\rm -1}}={\rm 0}$$ That is, all the odd coefficients up to ${\it a}_{{\rm 2}{\it n}{\rm -1}}$ are equal to zero, but this result is also true for the odd coefficients greater than ${\it a}_{{\rm 2}{\it n}{\rm -1}}$ , because if the power of $x$ is $2k+n$ , ${\it m}$ is $2n+2k$ in the second sigma and $2n+2k-2$ in the third sigma, that is, the even coefficients of ${\it a}$ are produced So the equation $${\it a}_{{\it p}{\rm +2}}=-{{{\it a}_{{\it p}}}\over{{\rm (}{\it p}+{\rm 2}{\rm )(}{\it p}+{\rm 2}-{\rm 2}{\it n}{\rm )}}}$$ holds for all odd coefficients and we can conclude that all odd coefficients are equal to zero and we only calculate even coefficients. $${\it a}_{{\rm 2}}=-{{{\it a}_{{\rm 0}}}\over{{\rm 2}{\rm (}{\rm 2}-{\rm 2}{\it n}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )}}}$$ $${\it a}_{{\rm 4}}=-{{{\it a}_{{\rm 2}}}\over{{\rm 4}{\rm (}{\rm 4}-{\rm 2}{\it n}{\rm )}}}={{{\it a}_{{\rm 2}}}\over{{\rm 4}{\rm (}{\rm 2}{\it n}-{\rm 4}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 4}\times{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )(}{\rm 2}{\it n}-{\rm 4}{\rm )}}}$$ $${\it a}_{{\rm 6}}=-{{{\it a}_{{\rm 4}}}\over{{\rm 6}{\rm (}{\rm 6}-{\rm 2}{\it n}{\rm )}}}={{{\it a}_{{\rm 4}}}\over{{\rm 6}{\rm (}{\rm 2}{\it n}-{\rm 6}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 6}\times{\rm 4}\times{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )(}{\rm 2}{\it n}-{\rm 4}{\rm )(}{\rm 2}{\it n}-{\rm 6}{\rm )}}}$$ and in general for $k\leq n-1$ : $${\it a}_{{\rm 2}{\it k}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}{\it k}{\rm (}{\rm 2}{\it k}-{\rm 2}{\rm )}\cdots\times{\rm 4}\times{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )(}{\rm 2}{\it n}-{\rm 4}{\rm )}\cdots{\rm (}{\rm 2}{\it n}-{\rm 2}{\it k}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\it k}}\times{\it k}{\rm !}\times{\rm 2}^{{\it k}}{\rm (}{\it n}-{\rm 1}{\rm )}\cdots{\rm (}{\it n}-{\it k}{\rm )}}}$$ $$={{{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !(}{\it n}-{\rm 1}{\rm )}\cdots{\rm (}{\it n}-{\it k}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !}{{{\rm (}{\it n}-{\rm 1}{\rm )!}}\over{{\rm (}{\it n}-{\it k}-{\rm 1}{\rm )!}}}}}={{{\rm (}{\it n}-{\it k}-{\rm 1}{\rm )!}{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !(}{\it n}-{\rm 1}{\rm )!}}}$$ We assume that ${\it a}_{{\rm 0}}=-{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!}$ . with this assumption, the coefficients will be in this form: $${\it a}_{{\rm 2}{\it k}}={{-{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!(}{\it n}-{\it k}-{\rm 1}{\rm )!}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !(}{\it n}-{\rm 1}{\rm )!}}}=-{{{\rm (}{\it n}-{\it k}-{\rm 1}{\rm )!}}\over{{\rm 2}^{{\rm 2}{\it k}{\rm -}{\it n}{\rm +1}}{\it k}{\rm !}}}{\rm                       (}I{\rm )}$$ In particular ${\it a}_{{\rm 2}{\it n}{\rm -2}}=-{{{\rm 1}}\over{{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!}}}$ . For the coefficient of $x^{n}$ : $${\rm 2}{\it k}{{{\it n}}\over{{\rm 2}^{{\it n}}{\it n}{\rm !}}}+{\rm 0}{\it a}_{{\rm 2}{\it n}}+{\it a}_{{\rm 2}{\it n}{\rm -2}}={\rm 0}\Rightarrow{{{\it k}}\over{{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!}}}-{{{\rm 1}}\over{{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!}}}={\rm 0}$$ $$\Rightarrow k=1$$ The above relationship holds for all values of ${\it a}_{{\rm 2}{\it n}}$ , so ${\it a}_{{\rm 2}{\it n}}$ is an arbitrary parameter and we can assume that $${\it a}_{{\rm 2}{\it n}}=-{{{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +1}}{\it n}{\rm !}}}$$ For the coefficient of $x^{2+n}$ : $${\rm 2}{{{\rm (}-{\rm 1}{\rm )(}{\rm 2}+{\it n}{\rm )}}\over{{\rm 2}^{{\rm 2+}{\it n}}{\rm (}{\rm 1}+{\it n}{\rm )!}}}+{\rm (}{\rm 2}{\it n}+{\rm 2}{\rm )(}{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +2}}+{\it a}_{{\rm 2}{\it n}}={\rm 0}$$ $$\Rightarrow-{{{\it n}+{\rm 2}}\over{{\rm 2}^{{\it n}{\rm +1}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}+{\rm 2}{\rm (}{\rm 2}{\it n}+{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +2}}-{{{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +1}}{\it n}{\rm !}}}={\rm 0}$$ $$\Rightarrow{\rm 2}^{{\rm 2}}{\rm (}{\it n}+{\rm 1}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +2}}={{{\it n}+{\rm 2}}\over{{\rm 2}^{{\it n}{\rm +1}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}+{{{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +1}}{\it n}{\rm !}}}={{{\it n}+{\rm 2}+{\rm (}{\it n}+{\rm 1}{\rm )(}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +1}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}$$ $$\Rightarrow{\it a}_{{\rm 2}{\it n}{\rm +2}}={{{\rm 1}+{{{\rm 1}}\over{{\it n}+{\rm 1}}}+{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}$$ For the coefficient of $x^{4+n}$ : $${\rm 2}{{{\rm 4}+{\it n}}\over{{\rm 2}^{{\rm 4+}{\it n}}{\rm 2}{\rm !(}{\rm 2}+{\it n}{\rm )!}}}+{\rm (}{\rm 2}{\it n}+{\rm 4}{\rm )(}{\rm 4}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +4}}+{\it a}_{{\rm 2}{\it n}{\rm +2}}={\rm 0}$$ $${{{\it n}+{\rm 4}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}+{\rm 2}^{{\rm 3}}{\rm (}{\it n}+{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +4}}+{{{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}={\rm 0}$$ $${\rm 2}^{{\rm 3}}{\rm (}{\it n}+{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +4}}=-{{{\it n}+{\rm 4}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}-{{{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}=-{{{\it n}+{\rm 4}+{\rm 2}{\rm (}{\it n}+{\rm 2}{\rm )}\left[{{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\right]}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}$$ $${\it \Rightarrow}{\it a}_{{\rm 2}{\it n}{\rm +4}}=-{{{{{\it n}+{\rm 4}}\over{{\rm 2}{\rm (}{\it n}+{\rm 2}{\rm )}}}+{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +5}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}=-{{{{{\rm 1}}\over{{\rm 2}}}+{{{\rm 1}}\over{{\it n}+{\rm 2}}}+{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +5}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}$$ $$=-{{{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}{\rm )}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}+{{{\rm 1}}\over{{\it n}+{\rm 2}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +5}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}$$ And in general, it can be shown by induction that: $${\it a}_{{\rm 2}{\it m}}={\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}{\rm +1}}{{{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}{\rm +1}}{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{\rm   }{\rm (}{\it m}\geq{\it n}{\rm )}{\rm               (}II{\rm )}$$ Therefore, according to relations ${\rm (}I )$ and ${\rm (}II )$ $${\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}+\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{-{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}{\rm +1}}{\it m}{\rm !}}}}{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}}+\sum\limits_{{\it m}={\it n}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}{\rm +1}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}{\rm +1}}{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}}$$ $$={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{{{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}}}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\it n}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{{{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}}}}$$ $$={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\it n}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}$$ If we reduce the lower limit of the last sigma by $n$ units, we have: $${\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}+{\it n}}}{\rm )}}\over{{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm +}{\it n}}$$ Then, instead of the solution ${\it y}_{{\rm 2}}$ , we consider the following solution which is linearly independent with ${\it J}_{{\it n}}{\rm (}{\it x}{\rm )}$ : $${\it Y}_{{\it n}}{\rm (}{\it x}{\rm )}={\it a}{\rm (}{\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}+{\it bJ}_{{\it n}}{\rm (}{\it x}{\rm ))}$$ where ${\it a}={{{\rm 2}}\over{{\rm \pi}}}$ and ${\it b}={\rm \gamma}-{\it Ln}{\rm 2}$ . So the solution is $${\it Y}_{{\it n}}{\rm (}{\it x}{\rm )}={{{\rm 2}}\over{{\rm \pi}}}\left({{\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}+{\rm (}{\rm \gamma}-{\it Ln}{\rm 2}{\rm )}{\it J}_{{\it n}}{\rm (}{\it x}{\rm )}}\right)$$ $${\it Y}_{{\it n}}{\rm (}{\it x}{\rm )}={{{\rm 2}}\over{{\rm \pi}}}\left({{\rm (}{\it Lnx}+{\rm \gamma}-{\it Ln}{\rm 2}{\rm )}{\it J}_{{\it n}}{\rm (}{\it x}{\rm )}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}+{\it n}}}{\rm )}}\over{{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm +}{\it n}}}\right)$$ $$={{{\rm 2}}\over{{\rm \pi}}}{\rm (}{\it Lnx}+{\rm \gamma}-{\it Ln}{\rm 2}{\rm )}{\it J}_{{\it n}}{\rm (}{\it x}{\rm )}-{{{\rm 1}}\over{{\rm \pi}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm \pi}}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}+{\it n}}}{\rm )}}\over{{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm +}{\it n}}$$ .","['special-functions', 'ordinary-differential-equations', 'derivation-of-formulae', 'power-series', 'bessel-functions']"
4552279,How to show this mapping from the upper half space to the unit ball is a bijection,"I want to show this mapping from the upper half space to the unit ball is a bijection: So one thing I tried is to just use the direct definitions of being injective and surjective, but it didn't take too long to get an algebraic mess. But then I was starting to suspect this mapping is in fact the real and imaginary parts of some Mobius transform in complex analysis. However, finding out the coefficients of the Mobius transform is also not an easy task, but is there a way to do so?","['complex-analysis', 'multivariable-calculus']"
4552294,what does triple integral represent geometrically?,"If a single integral represents the area under the curve,
double integral represents volume under the curve,
then what does triple integral represent geometrically?",['multivariable-calculus']
4552319,"If $f(0)=0$ and $f''(x)$ exists for all $x>0$, then show that","If $f(0)=0$ and $f''(x)$ exists for all $x>0$ , then show that $$
f'(x)-\frac{f(x)}{x}
 = \frac{1}{2} xf''(\zeta),  \quad 0<\zeta <x
$$ Also deduce that if $f''(x)$ is positive for positive values of $x$ , then $f(x)/x$ strictly increases as $x$ increases. For the first part i defined $ \phi(x) = xf'(x) -f(x)  $ . Applying Lagrange mean value theorem to it i got $ f'(x)-f(x)/x=xf''(\zeta)$ . The factor of 1/2 is missing. Can someone point out the mistake. Also kindly help with second part of question that talks about strictly increasing $f(x)/x$","['mean-value-theorem', 'calculus', 'derivatives', 'real-analysis']"
4552338,Applications of geometric topology [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 1 year ago . Improve this question I am a master student and will apply for PhD soon. I like geometry and topology, especially knot theory and three/four manifolds, and have taken some courses on them. It's hard to survive in my country if doing pure math, but I dislike typical applied math such as computational PDEs, so I am asking whether there are any applications of geometric topology. I know topological data analysis is a popular area, but it seems that it's far from ""geometric"", and just requires very basic algebraic topology. Geometric visualization seems fun. Are there any other applications of geometry and topology to the real life? Could you introduce some references? Which places/universities in Europe have such research areas? Thanks in advance.","['geometry', 'applications', 'geometric-topology', 'differential-topology', 'algebraic-topology']"
4552342,Finding the ODE form of impulse response function $h(t)=e^{-at}H(t)t^n$,"Consider a linear system that has a known impulse response function $h(t)=e^{-at}H(t)(at)^n$ where $a$ and $n$ are known coefficients and $H(t)$ is the Heaviside function. If there is an external force applied to the system, $x(t)$ , the output of the system can be written as a convolution $$
y(t) = \int_{-\infty}^\infty h(t - \tau) x(\tau)\ d\tau.
$$ My question is for non-integer $n > 0$ , how can we write the system in the first order ODE format? That is, $$
\frac{d \mathbf{s}}{dt} = \mathbf{f}(\mathbf{s}, x)
$$ for some states $\mathbf{s}\in\mathbb{R}^m$ and some function $\mathbf{f}(\cdot)$ , and the output is determined by $$
y = g(\mathbf{s})
$$ with some function $g(\cdot)$ . What I have done For integer $n$ , I know how to solve it. First, take the simplest example where $n = 0$ . The output $y$ can be written as $$
y(t) = \int_{-\infty}^\infty e^{-a(t-\tau)}H(t - \tau) x(\tau) d\tau
$$ where the derivative is $$
\frac{dy}{dt}(t) = \int_{-\infty}^\infty \left[-a H(t - \tau) + \delta(t-\tau)\right] e^{-a(t-\tau)} x(\tau) d\tau
$$ with $\delta(\cdot)$ as the Dirac delta function, and can be simplified into $$
\frac{dy}{dt}(t) =-a y(t) + x(t)
$$ So in the case of $n = 0$ , we can write $ds/dt = f(s, x) = -a s + x$ with the output $y = g(s) = s$ .
The similar procedure can be repeated for $n = 1$ , but in this case we need to differentiate $y$ twice, and get $$
\frac{d^2y}{dt^2}(t) =-2 a \frac{dy}{dt}(t) - a^2 y(t) + 2 a x(t).
$$ With the equation above, we can write the first order ODE form by choosing the states $\mathbf{s} = (dy/dt, y)$ . Similar procedures can be done for integer $n \geq 0$ . However, it is unclear to me how to get such equation with non-integer $n$ .","['integration', 'control-theory', 'linear-control', 'ordinary-differential-equations']"
4552379,"If $X$ and $Y$ are negatively correlated and $Y,Z$ are negatively correlated, does that mean $X,Z$ are positively correlated?","If $Cov(X,Y)<0$ and $Cov(Y,Z)<0$ does that necessarily mean $Cov(X,Z)>0$ ? Intuitively I'm thinking yes, but I'm trying to prove it.  My thoughts were to look at $E(XZ)$ and condition that on $X$ to see if that helps somehow. I was also thinking that you could consider covariance to be like the angle between two random variables and relate that to cosine somehow but in this case we're not assuming the random variables are centered either.","['statistics', 'probability-theory', 'correlation']"
4552386,Alternate multinomial theorem for $\frac{d^n}{dx^n}\prod\limits_{k=1}^m f_k(x)$ without $\sum\limits_{k_1+\dots+k_m=n}$ nor Kronecker delta.,"The generalized product rule complicates putting series coefficients into closed or hypergeometric form. There are 2 forms with Lagrange $n$ th derivative notation and the multinomial $\binom n{n_1,\dots,n_j}$ ; the $\displaystyle\sum_{\sum\limits_jn_j=n}$ version : $$\left(\prod_{k=1}^m f_k\right)^{(n)}=\sum_{\sum\limits_jn_j=n} \binom n{n_1,\dots,n_m} \prod_{k=1}^m f_k^{(n_k)}$$ and the the $\displaystyle\sum_{n_1,\dots,n_j=0}^n$ version with Kronecker delta $\delta_{a,b}$ : $$\left(\prod_{k=1}^m f_k\right)^{(n)}=\sum_{n_1=0}^n\dots\sum_{n_m=0}^n\delta_{n,\sum\limits_{j=1}^mn_j} \binom n{n_1,\dots,n_m} \prod_{k=1}^m f_k^{(n_k)}.$$ But evaluating these sum by sum is complicated for given $f_k(x)$ , so here is an alternate multinomial theorem without a single sum over a multivariable restriction nor a tensor function via repeated generalized Leibniz rule : $$(fg)^{(n)}=\sum_{k=0}^n\binom nkf^{(k)}g^{(n-k)}$$ Trying with $5$ functions, one gets: $$\begin{align}(f_1f_2f_3f_4f_5)^{(n)}=\sum_{n_1=0}^n\binom n{n_1}f_1^{(n-n_1)}\sum_{n_2=0}^{n_1}\binom{n_1}{n_2}f_2^{(n_1-n_2)}(f_3f_4f_5)^{(n_2)}= \sum_{n_1=0}^n\binom n{n_1}f_1^{(n-n_1)}\sum_{n_2=0}^{n_1}\binom{n_1}{n_2}f_2^{(n_1-n_2)}\sum_{n_3=0}^{n_2}\binom{n_2}{n_3}f_3^{(n_2-n_3)}\sum_{n_4=0}^{n_3}\binom{n_3}{n_4}f_4^{(n_3-n_4)}f_5^{(n_4)}=\sum_{n_1=0}^n\sum_{n_2=0}^{n_1}\sum_{n_3=0}^{n_2}\sum_{n_4=0}^{n_3}\frac{n!f_1^{(n-n_1)} f_2^{(n_1-n_2)} f_3^{(n_2-n_3)} f_4^{(n_3-n_4)}f_5^{(n_4)}}{(n-n_1)!(n_1-n_2)!(n_2-n_3)!(n_3-n_4)!n_4!}\end{align}$$ Therefore: $$\boxed{\left(\prod_{k=1}^mf_k\right)^{(n)}\mathop=^?\sum_{n_1=0}^n\sum_{n_2=0}^{n_1}\dots\sum_{n_{m-1}=0}^{n_{m-2}}\binom n{n_1}f_1^{(n-n_1)}f_m^{(n_{m-1})}\prod_{k=2}^{m-1}\binom{n_{k-1}}{n_k}f_k^{(n_{k-1}-n_k)}=\sum_{n_1=0}^n\sum_{n_2=0}^{n_1}\dots\sum_{n_{m-1}=0}^{n_{m-2}}\frac{n!f_1^{(n-n_1)}f_m^{(n_{m-1})}}{(n-n_1)!n_{m-1}!}\prod_{k=2}^{m-1}\frac{f_k^{(n_{k-1}-n_k)}}{(n_{k-1}-n_k)!}}\tag1$$ Similarly, but without $(n_{k-1}-n_k)$ in any but $1$ $n$ th derivative and factorial: $$(f_1f_2f_3f_4f_5)^{(n)}=\sum_{n_1=0}^n\binom n{n_1}f_1^{(n_1)}(f_2f_3f_4f_5)^{(n-n_1)}= \sum_{n_1=0}^n\binom n{n_1}f_1^{(n_1)}\sum_{n_2=0}^{n-n_1}\binom{n-n_1}{n_2}f_2^{(n_2)}\sum_{n_3=0}^{n-n_1-n_2}\binom{n-n_1-n_2}{n_3}f_3^{(n_3)}\sum_{n_4=0}^{n-n_1-n_2-n_3}\binom{n-n_1-n_2-n_3}{n_4} f_4^{(4)} f_5^{(n-n_1-n_2-n_3-n_4)}=\sum_{n_1=0}^n\sum_{n_2=0}^{n-n_1}\sum_{n_3=0}^{n-n_1-n_2}\sum_{n_4=0}^{n-n_1-n_2-n_3}\frac{n!f_1^{(n_1)}f_2^{(n_2)}f_3^{(n_3)}f_4^{(n_4)}f_5^{(n-n_1-n_2-n_3-n_4)}}{n_1!n_2!n_3!n_4!(n-n_1-n_2-n_3-n_4)!}$$ Therefore: $$\boxed{\left(\prod_{k=1}^mf_k\right)^{(n)}\mathop=^?\sum_{n_1=0}^n\sum_{n_2=0}^{n-n_1}\dots\sum_{n_{m-1}=0}^{n-\sum\limits_{k=1}^{m-2}n_k}\frac{n!f_m^{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)}}{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)!}\prod_{k=1}^{m-1}\frac{f_k^{(n_k)}}{n_k!}= \sum_{n_1=0}^n\sum_{n_2=0}^{n-n_1}\dots\sum_{n_{m-1}=0}^{n-\sum\limits_{k=1}^{m-2}n_k}\binom n{n_1,\dots,n_{m-1}}\frac{f_m^{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)}}{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)!}\prod_{k=1}^{m-1}f_k^{(n_k)}}\tag2$$ Are $(1),(2)$ correct and fully simplified?","['kronecker-delta', 'multinomial-theorem', 'multinomial-coefficients', 'derivatives', 'index-notation']"
4552398,Is Gromov-Hausdorff distance realized when one space is compact?,"The Gromov-Hausdorff distance between two complete $^1$ metric spaces $M,N$ is defined as the infimum, over all pairs $f,g$ of embeddings of $M,N$ into some third metric space $U$ , of the Hausdorff distance in $U$ between the images $f[M]$ and $g[N]$ . In general, this infimum need not be achieved by any specific pair of embeddings; however, if $M$ and $N$ are each compact, then a standard argument shows that the infimum is achieve. See here . However, unless I'm having a silly moment, that argument seems to break down if only one of the spaces involved is compact. So my question is: Suppose $M,N$ are complete metric spaces, $M$ is compact, and $N$ is bounded. Must there be isometric embeddings $f,g$ of $M,N$ into some metric space $U$ such that $d_{GH}(M,N)=d_{H}^U(f[M],g[N])$ ? (Here $d_{GH}$ is the Gromov-Hausdorff distance, and $d_H^U$ is the Hausdorff distance within $U$ .) $^1$ There is nothing gained by considering non-complete spaces here, so I'm restricting attention to complete spaces for simplicity.","['hausdorff-distance', 'metric-spaces', 'gromov-hausdorff-limit', 'general-topology', 'compactness']"
4552441,Prove an identity for elementary and complete symmetric homogeneous polynomials,"I stumbled upon this formula while I was playing around with some equations, and I was wondering if anyone had any deeper insight into this problem. Let $E_i(x_1,\ldots,x_k)$ be the elementary symmetric polynomial of degree $i$ , and let $H_i(x_1,\ldots,x_k)$ be the complete homogeneous symmetric polynomial of degree $i$ . Then it seems that for any values of $p, k \geq 0$ with $k \leq p$ $$\sum_{i=0}^k (-1)^i E_i(x_1,\ldots,x_{p - 1})H_{k-i}(x_{k+1},\ldots,x_p) = \prod_{j = 1}^k (x_p - x_j),$$ which is really interesting, but, I must confess, I do not have enough experience working with these objects to understand why this seems to be the case. The first couple of cases are fairly simple to check by hand, but I generally did it using the known recursion relations for the symmetric polynomials, and for higher-order cases, this approach becomes rapidly intractable and seems to require deeper insight that I do not possess at this point. For anyone that is curious, I did check for $p \leq 25$ already using Maple, but, seeing as that is a paid software, I transferred the code into Sage for $k < p$ (Sage doesn't like the multiplication for $k = p$ ) so that you may check things yourself: S = SymmetricFunctions(ZZ)
h = S.homogeneous(); e = S.elementary()

x = var(','.join('x%s'%i for i in range(100)))


for p in (2..12):
    for k in (0..p-1):
        print(f""p={p}, k={k}"")
        # The next line will print 0 when the formula holds
        print(add( ((-1)^i)*e[i].expand(p, alphabet = [x[j] for j in (1..p)])*h[k-i].expand(p-(k)+1, alphabet = [x[j] for j in (k+1..p+1)]) for i in (0..k)  ) - expand(prod(x[p+1] - x[i] for i in (1..k)))) If anyone knows how to prove that this formula is true, I would greatly appreciate the help. Thank you!","['abstract-algebra', 'combinatorics', 'symmetric-polynomials']"
4552453,Why does this formula for an integrating factor for first-order ODE work?,"The book Shaum's Outlines: Differential Equations , 3rd edition (page 33) provides the following condition (among others) for determining that a first-order ODE is amenable to solution via an integrating factor: $$
M = yf(xy)
$$ and $$
N = xg(xy)
$$ This is where the ODE is written as: $$
M dx + N dy = 0
$$ In this case, the integrating factor is: $$
I(x,y) = {1 \over xM - yN}
$$ I am trying to figure out why this is so, and if possible, derive an explicit expression for the function whose exact differential is ${1 \over xM - yN}(M dx + N dy)$ . If this formula had a distinctive name, I could easily search the web for more information, but Shaum's only calls it Equation 5.10. I found another answer which appears to be related somehow , but can't quite figure out the connection. Can anyone help?","['integrating-factor', 'ordinary-differential-equations']"
4552460,Transformation Identities of the $_2F_1$ function,"From Wolfram Functions we have the following identities for the hypergeometric function $_2F_1$ : $$\begin{align}
_2F_1\left(a,c-b;c;\tfrac{z}{z-1}\right)&=(1-z)^a\,_2F_1(a,b;c;z)\tag1\\
_2F_1\left(a,a+\tfrac12;c;z(2-z)\right)&=\left(1-\tfrac{z}2\right)^{-2a}\,_2F_1\left(2a,2a-c+1;c;\tfrac{z}{2-z}\right)\tag2\\
_2F_1\left(a,b;2b;\tfrac{4z}{(z+1)^2}\right)&=(1+z)^{2a}\,_2F_1\left(a,a-b+\tfrac12;b+\tfrac12;z^2\right),\tag3
\end{align}$$ where $$_2F_1(a,b;c;z)=\frac{\Gamma(c)}{\Gamma(b)\Gamma(c-b)}\int_0^1t^{b-1}(1-t)^{c-b-1}(1-zt)^{-a}dt.\tag{$\star$}$$ As a recreational project, I have been trying to prove the three above identities using $(\star)$ . So far, I've been able to prove $(1)$ . The proof is very simple and uses transformation $t\mapsto1-t$ in the integral. The identity follows immediately after simplification. This prompted me to try to prove the other two identities using substitutions in the integral $(\star)$ At this point, I have been able to rewrite $(2)$ as $$\,_2F_1\left(2a,2a-c+1;c;z\right)=(1+z)^{-2a}\,_2F_1\left(a,a+\tfrac12;c;\tfrac{4z}{(z+1)^2}\right)$$ via the substitution $\tfrac{z}{2-z}\mapsto z$ .This rewritten form looks very similar to $(3)$ , suggesting that they are related. Beyond this I have no leads. I have however examined the function $$U(\alpha,\beta,\gamma,\delta;z)=\int_0^1(1+t)^\alpha t^\beta (1-t)^\gamma (1-zt)^\delta dt,$$ considering its transformation identities given by substitutions which send $[0,1]$ to itself in the integral.
This seems related, as clearly $$_2F_1(a,b;c;z)=\frac{\Gamma(c)}{\Gamma(b)\Gamma(c-b)}U(0,b-1,c-b-1,-a;z).$$ For example, the substitution $t\mapsto \tfrac{1-t}{1+t}$ gives the identity $$U(\alpha,\beta,\gamma,\delta;z)=2^{1+\alpha+\gamma}(1-z)^{-\delta}U\left(-\alpha-\beta-\gamma-\delta-2,\gamma,\beta,\delta;\frac{z+1}{z-1}\right).$$ And while setting $\alpha=\beta+\gamma+\delta+2=0$ yields $$\,_2F_1(-\delta,\beta+1;\beta+\gamma+2;z)=2^{\gamma+1}(1-z)^{-\delta}\,_2F_1\left(-\delta,\gamma+1;\beta+\gamma+2;\frac{z+1}{z-1}\right),$$ this is not one of the transformation identities I was trying to prove. So, I would like some hints on how to prove $(2)$ and $(3)$ , if it is possible just using the integral $(\star)$ .","['integration', 'definite-integrals', 'special-functions', 'analytic-number-theory', 'hypergeometric-function']"
4552475,The components $W_{ij}^k$ of the difference $W=\nabla-\overline{\nabla}$ between two Levi-Civita connections,"The following text comes from the book Geometric Relativity written by Dan A. Lee. The expression $(\nabla_i-\overline{\nabla}_i)(v_j)$ may be a little perplexing, but according to what I've learned so far, that means $$\nabla_{v_i}v_j-\overline{\nabla}_{v_i}v_j.$$ Then we would have $$W_{ij}^k=\Gamma_{ij}^k-\overline{\Gamma}_{ij}^k=\frac{1}{2}g^{k\ell}(g_{\ell i,j}+g_{\ell j,i}-g_{ij,\ell})-\frac{1}{2}\overline{g}^{k\ell}(\overline{g}_{\ell i,j}+\overline{g}_{\ell j,i}-\overline{g}_{ij,\ell}).\tag{*}$$ Now I'd like to show that $$W_{ij}^k=\frac{1}{2}g^{k\ell}(\overline{\nabla}_i g_{\ell j}+\overline{\nabla}_j g_{i\ell}-\overline{\nabla}_\ell g_{ij}),\tag{**}$$ where $\overline{\nabla}_i g_{\ell j}$ is used to denote the components of $\overline{\nabla}g$ . My strategy is to expand the RHS of ( $**$ ) and try to arrive at ( $*$ ). The expansion is done by recalling that $$\overline{\nabla}_k g_{ij}=g_{ij,k}-\overline{\Gamma}_{ki}^\ell g_{\ell j}-\overline{\Gamma}_{kj}^\ell g_{\ell i}.$$ Then we see $$\overline{\nabla}_i g_{\ell j}+\overline{\nabla}_j g_{\ell i}-\overline{\nabla}_\ell g_{ij}=g_{\ell j,i}-\overline{\Gamma}_{i\ell}^k g_{kj}-\overline{\Gamma}_{ij}^k g_{k\ell}
+g_{\ell i,j}-\overline{\Gamma}_{j\ell}^k g_{ki}-\overline{\Gamma}_{ji}^k g_{k\ell}
-g_{ij,\ell}+\overline{\Gamma}_{\ell i}^k g_{kj}+\overline{\Gamma}_{\ell j}^k g_{ki}.$$ This is as far as I can go because I don't know how to get rid of the Christoffel symbols of $\overline{g}$ . Can someone tell me what to do next? Thank you. Edit 1. I'm very sorry. It seems that I have mixed up coordinate frames and the frame $\{v_1,\ldots,v_n\}$ . Let me think about it and fix it later. I apologize. Edit 2. As mentioned in Edit 1, I failed to rightly tell two different frames apart, so I will redo the problem here. Let me start from ( $*$ ). The connection coefficients $\Gamma_{ij}^k$ and $\overline{\Gamma}_{ij}^k$ are not necessarily the Christoffel symbols because $\{v_1,\ldots,v_n\}$ may not be a coordinate frame. Write $$g=g_{ij}e^i\otimes e^j\text{ and }\overline{g}=\overline{g}_{ij}e^i\otimes e^j$$ with $\{e^1,\ldots,e^n\}$ denoting the coframe dual to $\{v_1,\ldots,v_n\}$ .
Then we have $$\begin{align}
W_{ij}^k&=\Gamma_{ij}^k-\overline{\Gamma}_{ij}^k\\
&=\frac{1}{2}g^{k\ell}(v_i g_{j\ell}+v_j g_{i\ell}-v_\ell g_{ij}-g_{jm}c_{i\ell}^m-g_{\ell m}c_{ji}^m+g_{im}c_{\ell j}^m)\\
&\quad-\frac{1}{2}\overline{g}^{k\ell}(v_i \overline{g}_{j\ell}+v_j \overline{g}_{i\ell}-v_\ell \overline{g}_{ij}-\overline{g}_{jm}c_{i\ell}^m-\overline{g}_{\ell m}c_{ji}^m+\overline{g}_{im}c_{\ell j}^m),\tag{***}
\end{align}$$ where $c_{ij}^m$ are the functions defined by $$[v_i,v_j]=c_{ij}^m v_m.$$ For more information, one can see the book Introduction to Riemannian manifolds by John M. Lee. Now, on the other hand, we have $$\overline{\nabla}_k g_{ij}=v_k g_{ij}-\overline{\Gamma}_{ki}^\ell g_{\ell j}-\overline{\Gamma}_{kj}^\ell g_{i\ell}.$$ This gives $$\frac{1}{2}g^{k\ell}(\overline{\nabla}_i g_{\ell j}+\overline{\nabla}_j g_{i\ell}-\overline{\nabla}_\ell g_{ij})=\frac{1}{2}(g^{k\ell}v_i g_{\ell j}-\overline{\Gamma}_{ij}^k-n\overline{\Gamma}_{ij}^k+g^{k\ell}v_j g_{i\ell}-n\overline{\Gamma}_{ji}^k-\overline{\Gamma}_{ji}^k-g^{k\ell}v_\ell g_{ij}+\overline{\Gamma}_{ji}^k+\overline{\Gamma}_{ij}^k).$$ Now the thing is, how do I dispense with $\overline{g}^{k\ell}$ in ( $***$ )? Thank you.","['riemannian-geometry', 'differential-geometry']"
4552481,Sum of the areas of all n-sided polygons inscribed within each other,"Imagine you take an $n$ -sided regular polygon with side length $x$ , and connect the midpoints of each of its sides to construct a smaller but identical $n$ -sided regular polygon within it. Now perform the same process with the smaller polygon, and then with the next one, and the next and so on. If you continue this, endlessly inscribing smaller and smaller regular $n$ -sided polygons, what is the sum of all of their individual areas in terms of $x$ and $n$ ? Below is an image describing this question when n = 6: I know there must be a common ratio between each of the areas, so i'm thinking about turning this into a geometric series of the form $\frac{a}{1-r}$ (since $r$ < 1), where $a$ = the area of the first polygon, however I do not know how to find the specific values for $a$ and $r$ in terms of $x$ and $n$ . Any help with this question would be greatly appreciated!","['summation', 'area', 'geometry', 'polygons', 'sequences-and-series']"
4552676,Show that equation has solution (Fixed Point Theorem),"Let $A,B\in\mathcal{M}_{n\times n}(\mathbb{R})$ be matrices such that $\|A\|_{F}\cdot\|B\|_{F}<\frac{9}{16}$ , where $\|\cdot\|_F$ denotes the Frobenius norm. Show that in $B[0_{n\times n},1/2]$ (closed ball) the equation: $$AX^{2}B-\frac{1}{3\sqrt{n}}I=X$$ has a solution using the Fixed Point Theorem. I defined the aplication $X\mapsto\varphi(X)=AX^{2}B-\frac{1}{3\sqrt{n}}I$ to use the fixed point theorem. The ball is closed and I already prove that $B[0_{n\times n},1/2]\subseteq\varphi(B[0_{n\times n},1/2])$ , then I just need to show that it is a contracting function: Let $X,Y\in B[0_{n\times n},1/2]$ , then: $\|\varphi(X)-\varphi(Y)\|_F=\|AX^{2}B-AY^{2}B\|_F\leq\|A\|_{F}\|X^2-Y^2\|_F\|B\|_F<\frac{9}{16}\|X^2-Y^2\|_F$ I don't know how to make appear $\|X-Y\|_F$ . Any hint?","['fixed-point-theorems', 'real-analysis', 'matrices', 'calculus', 'functions']"
4552683,The tight bound for conditional mutual information: how much could conditional mutual information be greater than mutual information?,"Given random variables $X$ , $Y$ and another random variable $Z$ , it is known that there are cases when the conditional mutual information $I(X;Y|Z)$ is greater than mutual information $I(X;Y)$ . For example, let $Y = X + Z$ and $X \perp Z$ . It could be shown that: \begin{align}
I(X;Y|Z) = H(Y|Z) = H(X) \geq I(X;Y).
\end{align} However, what is the upper bound of $I(X;Y|Z)$ given $X$ and $Y$ ? We want to solve the following maximum problem: $$ \max_Z I(X;Y|Z),
$$ where $Z$ is any random variable. An upper bound (not necessarily tight) of $I(X;Y|Z)$ could be readily obtained as: \begin{align}
I(X;Y|Z) &= I(X;Y,Z) - I(X;Z) \\
&= H(X)  - H(X|Y,Z) - I(X;Z) \\
&\leq H(X).
\end{align} With symmetry, we also have $I(X;Y|Z) \leq H(Y)$ . To summarize, $$ \max_Z I(X;Y|Z) \leq \min\{ H(X),H(Y)\}.
$$ However, could the inequality above be an equality? Or else, for any given random variables $X$ , $Y$ (with given joint distribution), what is the exact solution to $\max_Z I(X;Y|Z)$ ?","['entropy', 'mutual-information', 'information-theory', 'probability-theory', 'probability']"
4552697,Find area bounded between two concentric circles whose radii are connected by given equations,If radii of three concentric circles are related as $r_1(r_2+r_3)+r_2(r_3+r_1)+r_3(r_1+r_2)=118$ and $$\sum_{cyclic}\frac{r_1^2+r_2^2}{r_1r_2}=\frac{44}{5}$$ then area of enclosed region between any two circles can be (A) $21\pi$ (B) $45\pi$ (C) $32\pi$ (D) $19\pi$ I tried eliminating $r_1$ $r_1=\frac{59-r_2r_3}{r_2+r_3}$ The second condition may be written as $r_1(\frac{r_2+r_3}{r_2r_3})+\frac{1}{r_1}(r_2+r_3)+\frac{(r_2+r_3)^2-2r_2r_3}{r_2r_3}=\frac{44}{5}$ Clearly we are looking for something like $r_3^2-r_2^2$ but calculation appears very tricky. Could there be a geometrical way to go about it,"['contest-math', 'algebra-precalculus', 'geometry', 'analytic-geometry']"
4552713,$L^1$ convergence kernel convolutions implies weak convergence of probability measures,"I was reading a paper where they claim the following. Let $\mathscr{M}$ be the space of measures on compact $\mathfrak{X} \subset \mathbb{R}$ . Let $\phi(\cdot - \theta)$ be the normal kernel with location $\theta$ : $$ f_P(x) = \int \phi (x - \theta) dP(\theta)$$ They claim the following: The map $P \mapsto f_{P}$ is one-to-one, onto $\mathscr{F}$ , where $\mathscr{F}=\left\{f_{P}: P \in \mathscr{M}\right\}$ . Further $P_n \rightarrow P_0$ weakly if and only if $\left\|f_{P_n}-f_{P_0}\right\|_1 \rightarrow 0$ . For the part abount injectivity I found solution here: Is ""Convolution operator"" well-defined and injective? Clearly, by the definition of $\mathscr{F}$ it is also onto. So on this part I agree. For the direction $\Rightarrow$ the claim follows upon noticing that the Gaussian kernel in one variable is bounded Lipschitz continuous (see Is the Gaussian density Lipschitz continuous? ). Then an application of Portmanteau's Lemma yields for any fixed $x$ : $$ \int \phi dP_n \rightarrow \int \phi dP_0$$ Then using again the boundedness of the kernels and nonnegativity of our integrals, an application of Dominated Convergence Theorem yields: $\left\|f_{P_n}-f_{P_0}\right\|_1 \rightarrow 0$ . Now for the other direction, I am stuck. I tried to use Scheffe's Lemma to use later the Portmanteau's Lemma with the classical convergence $P_n \rightarrow P_0$ weakly iff $P_n(\theta) \rightarrow P_0 (\theta)$ at all continuity points $\theta$ . Indeed the application of Scheffe's Lemma gives from $\left\|f_{P_n}-f_{P_0}\right\|_1 \rightarrow 0$ that: $$\int \int \phi(x-\theta) dP_n(\theta) dx \rightarrow \int \int \phi(x-\theta) dP_0(\theta) dx$$ Then one could use Fubini's Theorem and the fact that for fixed $\theta$ it holds: $\int \phi(x-\theta) dx = 1$ since it is a Gaussian Kernel. However, it doesn't give the claim eventually. I also tried to argue by contradiction but I did not manage to get anywhere useful. I also tried to use this knowledge: Is this theorem an extension of Scheffé Lemma Maybe you will find it helpful. Q: Do you have any idea on how to tackle this problem? Or any reference that I could consult?","['measure-theory', 'statistics', 'weak-convergence', 'probability-theory', 'probability']"
4552747,Solving $a(b - c) < d$ for $a$ in two ways gives opposite results,I have $$a(b - c) < d$$ I want to solve for $a > ???$ . I can divide off the parentheses to get $$a < \frac{d}{b - c}$$ . But... let's say instead I subtract both sides $$- a(b - c) > - d$$ I can absorb the negative into the parentheses. $$a(c - b) > - d$$ Divide off the parentheses. $$a > - \frac{d}{(c - b)}$$ And then absorb the negative into the denominator $$a > \frac{d}{(b - c)}$$ This is exactly the opposite of what I got the first time. Where is my mistake? Can I actually solve for $a > ???$,['algebra-precalculus']
4552797,"$\exists f$ such that $f(X,Y)$ is independent of $X+Y$?","Let $X,Y$ be independent continous random variables. Does there exist a non-constant measurable function $f$ such that $f(X,Y)$ is independent of $X+Y$ ? I am only interested in existence (and not the form of $f$ ). I suppose that it can depend on the distributions of $X,Y$ . It would be interesting to see at least an example when $f$ either exists or it can be proven that it does not exist. My thoughts are that a transformation $(X,Y)\to X+Y$ starts from independent components, so there should be another ""perpendicular"" transformation that will be independent of $X+Y$ .","['statistical-inference', 'probability-distributions', 'probability-theory', 'probability']"
4552798,"If $|X_n - Y_n| \overset{P}{\longrightarrow} 0$, does $|F_{X_{n}}(t) - F_{Y_n}(t)| \to 0$ for all $t \in \mathbb R\,$?","Suppose that $(X_n)$ and $(Y_n)$ are sequences of random variables, all defined on the probability space $(\Omega, \mathcal F, P)$ . Suppose the difference of the random variables converges in probability to $0$ , i.e. $$
|X_n - Y_n| \overset{P}{\longrightarrow} 0 \quad \text{as $n \to \infty$.}
$$ Does the difference of their c.d.f.'s then converge pointwise to zero? That is: Does the convergence $$
\big| F_{X_n}(t) - F_{Y_n}(t) \big| = \big| P(X_n \leq t) - P(Y_n \leq t)\big| \to 0 \quad \text{as $n \to \infty$}
$$ hold for all $t \in \mathbb R\,$ ? Thoughts Intuitively, this seems reasonable, but I'm not sure how to show this. On can reason that $$
\big| P(X_n \leq t) - P(Y_n \leq t)\big| 
\leq P(|X_n - Y_n| > 0) = \lim_{\varepsilon \downarrow 0} P(|X_n - Y_n| > \varepsilon ),
$$ so $$
\limsup_{n \to \infty} \big| P(X_n \leq t) - P(Y_n \leq t)\big| \leq \limsup_{n \to \infty} \lim_{\varepsilon \downarrow 0} P(|X_n - Y_n| > \varepsilon ).
$$ If we could swap the order of the limits on the right hand side of the inequality above, we would be finished. I don't see why that would be justified, though.",['probability-theory']
4552842,"Finding the value of $\int_{-\pi/4}^{\pi/4}\frac{(\pi-4\theta)\tan\theta}{1-\tan\theta}\,d\theta$.","It is given that $$
I=\int_{-\pi/4}^{\pi/4}\frac{(\pi-4\theta)\tan\theta}{1-\tan\theta}\,d\theta=\pi\ln k-\frac{\pi^2}{w}
$$ and was asked to find the value $kw$ . Here is my try on it: Substituting $\theta = \dfrac{\pi}{4}+x$ , we get $$\begin{equation} \label{eq1}
\begin{split}
I &= \displaystyle\int_{-\frac{\pi}{2}}^0\dfrac{(-4x) \tan (\frac{\pi}{4}+x)}{1-\tan (\frac{\pi}{4}+x)} \, dx \\
  &= -4\displaystyle\int_{-\frac{\pi}{2}}^0\dfrac{x.\dfrac{1+ \tan x}{1- \tan x}}{1- \dfrac{1+ \tan x}{1- \tan x}} \, dx \\
&= 2 \displaystyle\int_{-\frac{\pi}{2}}^0\dfrac{x(1+\tan x)}{\tan x} \, dx
\end{split}
\end{equation}$$ From here I am stuck on this, I don't know how to proceed from here to get answer in terms of $\pi \ln k-\frac{\pi^2}{w}$ Thank you for any help.","['integration', 'calculus']"
4552923,Is $l_2$ on $\mathbb{R}^n$ the only norm for which it is equal to its dual norm?,"Given any norm $\|.\|$ on $\mathbb{R}^n$ , its dual norm $\|.\|^D$ is defined as the following: $\|v\|^D = \sup_{\|x\|\leq 1} |(v,x)|$ , where $(,)$ is the standard Euclidean Inner product. Under that definition, it turns out that the dual norm of the $l_p$ norm is $l_q$ , when $1/p+1/q=1$ . That implies that for $l_2$ , it is equal to the dual norm. Now I wanted to think about the converse: If $\|.\|$ is a norm such that $\|.\| = \|.\|^D$ , then is $\|.\|=l_2$ ? After being unable to find anything online, I wrote the following justification: Consider any vector in the boundary of the ball of radius $1$ in $\|.\|$ : \begin{align*}
    1 = \|x\| = \|x\|^D = \sup\left\{\left|\sum^n_i x_iv_i\right|: \|v\| \leq 1\right\}
\end{align*} Using C-S Inequality (with $(,)$ denoting the usual inner product) we have: \begin{align*}
    |(x,v)| \leq \|x\|_2 \|v\|_2 
\end{align*} where equality holds if and only if $x,v$ are linearly dependent. Considering that, let $v = x$ . Then $\|v\| = 1$ and we have: \begin{align*}
     |{(x,v)}| = \|x\|_2^2 \implies \|x\|^D = \|x\| = 1 \geq \|x\|^2_2 \implies 1 \geq \|x\|_2
\end{align*} Suppose $\exists v_0$ such that $\|v_0\|<1$ , and $\|x\|^D \leq |{(x,v_0)}|$ . Then consider $v' = \frac{v}{\|v_0\|}$ . We will have $\|{(x,v)}\| \leq \|{x}\|^D \leq |(x,v_0)| \implies \|v_0\|\geq 1$ . Hence we have a contradiction. Consider the functional defined by $f: \overline{B_1}(0)\rightarrow \mathbb{R}$ as $f(v) = |(x,v)|$ , where $B$ denotes the ball in $\|.\|$ . Then by the compactness of the unit ball in any norm, and continuity of $f$ , it attains maxima; let that point be denoted by $v_0$ . Then by the earlier paragraph, $\|v_0\|=1$ . However, we showed that whenever $\|v_0\| =1$ , we should get $\|v_0\|_2 \leq 1$ . Hence we get: \begin{align*}
    \left|{\sum^n_i x_iv_i}\right| \leq |{(x,v_0)}| \leq \|x\|_2\|v_0\|_2 \leq \|x\|_2 \implies 1 = \|x\|^D \leq \|x\|_2
\end{align*} Hence we get the following for any unit vector: \begin{align*}
   \|x\| = \|x\|_2
\end{align*} Now given a general non zero vector, consider: \begin{align*}
    1 = \|{\frac{x}{\|x\|}}\| =  \|{\frac{x}{\|x\|}}\|_2 \implies \|x\|= \|x\|_2\  \square
\end{align*} Is the proposed solution correct? Is there a better way to see the result? Can this notion be meaniningfully extended in any sense to infinite dimensions?","['real-analysis', 'linear-algebra', 'functional-analysis', 'dual-spaces', 'matrix-analysis']"
4552955,How to solve $\sum\limits_{k=0}^{n-a-b}\binom{n-a-b}{k}(a+k-1)!(n-a-k)!$,"I'm solving a probability problem, and I've ended up with this sum: $$\sum\limits_{k=0}^{n-a-b}\binom{n-a-b}{k}(a+k-1)!(n-a-k)!$$ WolframAlpha says I should get the answer $\frac{n!}{a\binom{a+b}{a}}$ , but I don't see how to get there. I tried to get to something containing $\binom{a+k-1}{k}$ so that I could use the Hockey Stick Theorem, but I wasn't successful. So any hints would be very welcome, thanks for any help","['summation', 'binomial-coefficients', 'combinatorics', 'probability']"
4552959,Orientation of $3$-tuples of $\mathbb{P}_K^1$,"Let $K$ be a field, and let $(a,b,c)\in\mathbb{P}_K^1$ be arbitrary distinct points. Then since the action of $\mathrm{PGL}_2(K)$ on $\mathbb{P}_K^1$ is simply $3$ -transitive, there is a unique $g\in\mathrm{PGL}_2(K)$ such that $g(0)=a,g(1)=b,g(\infty)=c$ . There is a well-defined map $\det\colon\mathrm{PGL}_2(K)\to K^\times/(K^\times)^2$ , so to any $3$ -tuple $(a,b,c)$ of points of $\mathbb P_K^1$ , one can associate an element $\det(g)$ of $K^\times/(K^\times)^2$ . In particular, when $K=\mathbb F_q$ with $q\equiv1\pmod 4$ , we have defined an ""orientation"" of any $3$ -element subset $\{a,b,c\}\subset\mathbb{PF}_q^1$ , as an element of $\mathbb F_q^\times/(\mathbb F_q^\times)^2\cong C_2$ . Indeed, swapping the elements $a,b,c$ corresponds to composing $g$ with one of: $$\{I_2,\begin{pmatrix}-1&1\\1\end{pmatrix},\begin{pmatrix}&1\\-1&1\end{pmatrix},\begin{pmatrix}1&\\1&-1\end{pmatrix},\begin{pmatrix}&1\\1\end{pmatrix},\begin{pmatrix}1&\\1&-1\end{pmatrix}\},$$ which all have determinant $\pm1\in(\mathbb F_q^\times)^2$ . Thus, the orientation of $\{a,b,c\}$ is well-defined. Is there a nice interpretation of the ""orientation"" of a $3$ -tuple of elements of $\mathbb{PF}_q^1$ , as defined above? Example: the positively oriented subsets in $\mathbb{PF}_5^1$ are: $$\{0,1,\infty\},\{1,2,\infty\},\{2,3,\infty\},\{3,4,\infty\},\{0,4,\infty\},\{0,1,3\},\{1,2,4\},\{0,2,3\},\{1,3,4\},\{0,2,4\}$$","['finite-fields', 'group-theory', 'finite-groups', 'algebraic-groups']"
4552984,When is the intersection of two irreducible variety irreducible?,"I am learning AG. At first, I thought the intersection of two irreducible varieties was irreducible. This isn't true in general. For instance, take the parabola and a general line. They will intersect at two points, so it isn't irreducible. I am curious, though, when is the intersection irreducible?",['algebraic-geometry']
4552993,Inequality involving $\arcsin$: step in a proof,I am reviewing a proof in a textbook of a calculus theorem. In one step of it the author establishes the following inequality and I don't know how it can be justified: $$\frac{1}{|\arcsin(x^2+y^2+x)-\arcsin(x)|}<\frac{1}{|(x^2+y^2+x)-x|}.$$ I don't get which relationship between the $\arcsin(x)$ and $x$ is using. Any idea? Thanks.,"['calculus', 'trigonometry', 'real-analysis']"
4553039,"Find the total number of roots of $(x^2+x+1)^2+2=(x^2+x+1)(x^2-2x-6)$, belonging to $(-2,4)$.","Find the total number of roots of $(x^2+x+1)^2+2=(x^2+x+1)(x^2-2x-6)$ , belonging to $(-2,4)$ . My Attempt: On rearranging, I get, $(x^2+x+1)(3x+7)+2=0$ Or, $3x^3+10x^2+10x+9=0$ Derivative of the cubic is $9x^2+20x+10$ It is zero at minus zero point something and minus one point something. So, even at local minima, the cubic is positive. It means it would cross x-axis only once. At $x=-2$ , cubic is positive and at $-3$ , it is negative. It means the only root is minus two point something. Is there any other way to solve this question? Something that doesn't involve calculator? Or maybe something that doesn't involve calculus?","['cubics', 'roots', 'algebra-precalculus', 'quadratics', 'complex-numbers']"
4553040,Analitic solution to ${df(x)\over{dx}} + {1\over2} f^2 (x) = ax^2 +bx + c$,"What is the analytic solution to the following differential equation? $${df(x)\over{dx}} + {1\over2} f^2 (x) = ax^2 +bx + c$$ I know that this is a nonlinear nonhomogeneous first-order ODE. However, I do not know how to solve it.","['nonlinear-system', 'ordinary-differential-equations']"
4553046,Really: why is the Kelvin transform harmonic?,"So, it is a famous fact that if $u:\mathbb{R}^n \to \mathbb{R}$ is an harmonic function, then its Kelvin transform $$
(Ku)(x) := \frac{1}{|x|^{n-2}} u\left(\frac{x}{|x|^2} \right)
$$ is harmonic too. Apparently, all the proofs of this fact that I have seen are some variation of ""do the computations and all the terms simplify"" . Moreover, I already did the computation once and so I am not interested in seeing the computations done in some other equivalent form . My point is the following: why, really, is this function harmonic? I don't buy the fact that the first person that discovered this just randomly did the computations with the correct power of the norm in front. For $n=2$ this can be proved very easily by properties of holomorphic functions and indeed there's a geometric motivation, in this case $n=2$ , on why it sends harmonic functions to harmonic functions. But what for general $n$ ? Does anyone have at least some geometric/physical intuition on why $Ku$ is harmonic if $u$ is? I tried many characterizations of harmonic functions like the mean value property or integrating against test functions, and all seem not to bring the result in a clean way. Indeed, at some point, there's always a big computation (being that some tangential Jacobian or Laplacian of composition) that I find basically equivalent to doing the computation from the beginning. I am searching for a proof or at least some motivation on why would someone think that this transformation is the natural candidate to send harmonic functions to harmonic functions.","['harmonic-functions', 'harmonic-analysis', 'complex-analysis', 'math-history', 'differential-geometry']"
4553071,Find all functions $f:\mathbb{R}\rightarrow\mathbb{R}$ where $f(xf(y)+f(x)+y)=xy+f(x)+f(y)$,"Find all functions $f:\mathbb{R}\rightarrow\mathbb{R}$ for two real numbers $x$ and $y$ where $f(xf(y)+f(x)+y)=xy+f(x)+f(y)$ For $x=0$ and $y=-f(0)$ then $f(-f(0))=0$ . So, there is a real root $r_0$ for function $f$ . For $x=r_0$ and $y=r_0$ we have $r_0^2=0$ , so $f(0)=0$ and zero root is unique. Please help me to complete the proof.","['real-numbers', 'functional-equations', 'functions', 'real-analysis']"
4553074,Can we get the concentration inequality of the inner product of two unit vectors distributed on the sphere?,"Let $u$ and $v$ be two random vectors on $R^n$ that are independent and uniformly distributed on the unit sphere. That means we can represent it as Gaussian random vectors $g\sim N(0, I_n)$ , $$u=\frac{g}{\|g\|_2}.
$$ But do we have the concentration inequality of the $u\cdot v$ ? That looks like $$P(u\cdot v\le x)\ge 1-Ce^{-c x}$$","['statistics', 'analysis', 'probability']"
4553116,Operator norm of a matrix with elements normalized by geometric mean of row and column sum,"Suppose $M \in \mathbb{R}^{n \times n}$ is a square matrix with nonnegative entries, and suppose all its row and column sums are positive. Normalize it to form an equivalently sized matrix $A \in \mathbb{R}^{n \times n}$ by dividing each element by the square root of both its row and column sum: $$A_{i j} = \frac{M_{i j}}{\sqrt{\left(\sum_{k} M_{i k}\right)\left(\sum_{k} M_{k j}\right)}}$$ Can we show that the $\ell_2$ operator norm of A is $\| A \|_2 = 1$ ? I have been able to show that $\|A\|_2 \geq 1$ as follows. Let $c = \sum_{i j} M_{i j}$ . Take $v \in \mathbb{R}^n$ with $v_i = \sqrt{\frac{1}{c} \sum_{k} M_{k i}}$ . Clearly $\|v\|_2 = \sqrt{\frac{1}{c}\sum_i \sum_{k} M_{k i}} = 1$ . Also, $$\|A v\|_2 = \sqrt{\sum_i \left(\sum_j A_{i j} v_j \right)^2}$$ $$= \sqrt{\sum_i \left(\sum_j \sqrt{\frac{\sum_{k} M_{k j}}{c}} \frac{M_{i j}}{\sqrt{\left(\sum_{k} M_{i k}\right)\left(\sum_{k} M_{k j}\right)}} \right)^2}$$ $$= \sqrt{\sum_i \left(\frac{\sum_j M_{i j}}{\sqrt{c \left(\sum_{k} M_{i k}\right)}} \right)^2}$$ $$= \sqrt{\frac{1}{c} \sum_i \sum_j M_{i j}} = 1.$$ However, I can't seem to show that $\| A \|_2 \leq 1$ .","['matrices', 'linear-algebra', 'matrix-norms']"
4553133,Computing sectional curvature on flat torus,"There have been several questions similar to mine, but none of them fleshed out to a complete answer. I would like someone to look over this proof (I am NOT looking for alternative solutions), as it's the first time I've seriously computed with second fundamental form, Gauss equation, etc. Also, I can never read classical DG notation, so I'll stick to modern notation throughout. I want to show the sectional curvature of the Clifford torus, that is the image of the map $F:\mathbb{R}^2\to \mathbb{R}^4$ given by $$F(\theta,\phi)=\frac{1}{\sqrt{2}}(\cos\theta,\sin\theta,\cos\phi,\sin\phi),$$ has zero sectional curvature. Let us denote the image of $F$ by $T^2$ . Splitting $\mathbb{R}^4=\mathbb{R}^2_{r,\theta}\times \mathbb{R}^2_{s,\phi}$ into two copies of the plane with polar coordinates, let us take an orthonormal frame on $T^2$ as $e_1=\frac{1}{r}\partial_\theta$ and $e_2=\frac{1}{s}\partial_\phi$ . Now, we compute $$sec_{(\theta,\phi)}(T^2)=R^{T^2}(e_1,e_2,e_2,e_1)=R(e_1,e_2,e_2,e_1)+g(\Pi(e_1,e_1),\Pi(e_2,e_2))-g(\Pi(e_1,e_2),\Pi(e_1,e_2))$$ where the last equality follows by Gauss equation. Since R is the curvature tensor of Euclidean space, the first term vanishes. Therefore, it remains to compute the second fundamental forms. We compute $$\Pi(e_1,e_2)=-(\nabla_{\frac{1}{r}\partial_\theta}\frac{1}{s}\partial_\phi)^\perp=-(\frac{1}{sr}\Gamma^i_{\theta\phi}\partial_i)^\perp=0,$$ where the last equality is by consideration of $g_{\mathbb{R}^4}=dr^2+r^2d\theta^2+ds^2+s^2d\phi^2$ . Similarly $$\Pi(e_1,e_1)=-(\nabla_{\frac{1}{r}\partial_\theta}\frac{1}{r}\partial_\theta)^\perp=\left(\frac{\partial_r}{r}\right)^\perp=\frac{\partial_r}{r},$$ since $\partial_r$ is already perpendicular to the torus; similarly $\Pi(e_2,e_2)=\frac{\partial_s}{s}.$ But $\partial_r,\partial_s$ are orthogonal as we have a direct-sum decomposition, so all second fundamental form terms must vanish. Thus, the sectional curvature on the torus vanishes.","['curvature', 'solution-verification', 'riemannian-geometry', 'differential-geometry']"
4553175,Product of Inverse Hankel Matrix,"Consider $H_n$ , the $n\times n$ Hankel matrix of the Catalan numbers starting from $2$ : $$H_n = \begin{bmatrix}
2 & 5 & 14 & 42 & 132\\
5 & 14 & 42 & 132 & 429\\
14 & 42 & 132 & 429 & 1430 & \cdots\\
42 & 132 & 429 & 1430 & 4862\\
132 & 429 & 1430 & 4862 & 16796\\
&&\vdots\end{bmatrix}$$ It is known that $\text{det}(H_n) = n + 1$ . (see Hankel Matrix ) Consider the column vector, $$c_n = \begin{bmatrix}1 \\ 2 \\ 5 \\ 14 \\ \vdots \end{bmatrix}$$ that contains the first $n$ Catalan numbers. I have found a pattern that I have checked up to $n=240$ , that $$(c_n)^T(H_n)^{-1}(c_n) = \frac{n}{n+1}$$ Is there any method I can take to prove this, or is there a counterexample? Note also that this product is the only non-zero eigenvalue of $(c_n)(c_n)^T(H_n)^{-1}$ .","['matrices', 'hankel-matrices', 'catalan-numbers', 'linear-algebra']"
4553211,Foreign trig problem from maths challenge book [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question The question says (translated from Spanish): 9. Find the “ $x$ ” value for every case: But I’m only interested in case a. I initially tried to solve it as a proportion but that yielded a far too complicated answer. My next guess was to assume that the arrows signify ""similarity"" so I'm thinking that these triangles are mirror images and one is smaller than the other. I'm thinking that the answer can be found by thinking of this as a proportion but we have one side as an unknown and that we have to solve for the extra side.","['trigonometry', 'triangles']"
4553293,"Which rectangles can be tiled with L-trominos, when only two orientations are allowed?","This is a question that I got after reading this: https://www.cut-the-knot.org/Curriculum/Games/LminoRect.shtml .  (This link already gave me the same result as theorem 1.1 of the article https://www.sciencedirect.com/science/article/pii/S0012365X08000629. )
I hope that some people here might be able to help me solve it. Let $m$ and $n$ be integers greater than 1.  Find all pairs $(m,n)$ such that it is possible to tile an $m\times n$ table with
right-oriented L-shaped trominos. The pictures below shows what right/left-oriented trominos are.  (No rotations allowed.  So,
left-oriented trominos can't be used.) Clearly either $m$ or $n$ must be divisible by $3$ .  Two right-oriented trominos can be combined to form a $2\times 3$ rectangle.  Therefore it can be seen that if $6\mid mn$ , then the table can be tiled with these trominos.  I tried to deal with the case where $3\mid mn$ but $2\nmid mn$ .  It seems that there is always a $3\times 1$ or $1\times 3$ leftover.  My guess is that $6\mid mn$ is indeed the necessary and sufficient condition.  Please help!","['divisibility', 'recreational-mathematics', 'combinatorics', 'polyomino', 'tiling']"
4553331,On functions satisfying a functional inequality,"Suppose $f \colon (0,1) \to (0,\infty)$ is monotonically decreasing and integrable on (0,1). Denote $F(x) = \int_0^x f(y) dy $ .
Suppose that there exists a constant $C>0$ such that $$F(q) - qf(q)^2 \le C$$ for all $q \in (0,1)$ . Is there anything that can be said about $f$ ? Is there such an $f$ , and if so, what does it look like? If one makes the ansatz that $f(q) = aq^k$ for some $k \in [-1/2,0)$ and $a>0$ , then this inequality holds true by Young's inequality. I wonder if these are already all possible functions. Any comment or insight is welcome. As $F(0) = 0$ we deduce that the limit $\lim_{q \to 0} qf(q)^2 $ has to exist and needs to be finite.",['functions']
4553334,How does the water level rise when you fill a hemispherical bowl at constant volumetric flow rate?,"This is a question that occurred to me when actually filling such a bowl. I saw this post , which however does not seem to give a final formula for the water level/height vs time. Here is how I approached it (and why I still have a doubt about it). Knowing (e.g. from here ) that the filled volume as a function of the sphere's radius $r$ and cap height (= water level) $h$ is: $$V = \frac {\pi h^2} 3 (3 r - h)$$ and assuming that the volumetric flow rate is $F$ , at a time $t$ , the volume of water must be: $$V = F t$$ Equating the two: $$F t = \frac {\pi h^2} 3 (3 r - h)$$ Solving this equation for $h$ should give the desired $h(t)$ . However, the expression I got was very complicated, so I tried some simplifications. The maximal possible time is the one at which the hemisphere is full ( $h = r$ ): $$F t_{max} = \frac {\pi r^2} 3  (3 r - r) = \frac {2 \pi r^3} 3$$ Defining: $$T = \frac t {t_{max}}$$ implies: $$t = \frac {2 \pi T r^3} {3 F}$$ Defining: $$H= \frac h r$$ implies: $$h = H r$$ Replacing $t$ and $h$ with their expressions in terms of $T$ and $H$ , which are both bound to $[0,1]$ , and cancelling out the constants: $$2 T = 3 H^2 - H^3$$ Implicit plot of this equation: This shows that the level rises faster at the beginning, and more slowly as $T$ approaches $1$ , as expected intuitively. However, if I ask my CAS to solve this equation for $H$ , I get 3 solutions, the first 2 with imaginary terms, and the last one without imaginary terms, but clearly not the applicable one, as $H$ is always greater than $1$ . So my question is: when I know that the intended variable $H$ I am solving this cubic equation for is real and bounded to $[0,1]$ , how can I obtain (or identify) the correct solution? Note that the CAS I am using allows to calculate a 'realpart' and 'imagpart' of an expression, and when I substitute numerical values of $T$ I can see that the 'imagpart' of all 3 solutions approaches $0$ , whereas only the realpart of one of them is within $[0,1]$ . So in a way I know which solution is the correct one. But I am looking for a cleverer method and for an expression of the solution that does not have imaginary terms in it, assuming it is possible to find it. EDIT added solution from CAS $$H = 1 + ( - \frac 1 2 - \frac {\sqrt {3} i} 2 ) (-T + i \sqrt {2-T} \sqrt T +1)^{1/3} + \frac {- \frac 1 2 + \frac {\sqrt {3} i} 2} {(-T + i \sqrt {2-T} \sqrt T +1)^{1/3}}$$ The real part calculated by the CAS is: $$H = 1 + \sqrt 3 \sin {(\frac {atan2 (\sqrt {2-T} \sqrt T, 1-T)} 3}) - \cos {(\frac {atan2 (\sqrt {2-T} \sqrt T, 1-T)} 3})$$ Definition of $atan2(y,x)$ by the CAS: $$atan2(y,x) = \arctan(\frac y x) = z, z \in [-\pi, \pi]$$ The imaginary part reduces to $0$ , as expected. EDIT 2 further simplification Knowing that: $$\sin(a) \sin(b) - \cos(a) \cos(b) = -\cos(a+b)$$ and noting that: $$\sin(\frac {\pi} 3) = \frac {\sqrt 3} 2$$ $$\cos(\frac {\pi} 3) = \frac {1} 2$$ it follows that: $$H = 1 + 2 \sin(\frac {\pi} 3) \sin {(\frac {atan2 (\sqrt {2-T} \sqrt T, 1-T)} 3}) - 2 \cos(\frac {\pi} 3) \cos {(\frac {atan2 (\sqrt {2-T} \sqrt T, 1-T)} 3}) =$$ $$= 1 - 2 \cos {(\frac {\pi + atan2 (\sqrt {2-T} \sqrt T, 1-T)} 3})$$","['algebra-precalculus', 'geometry']"
4553342,Finding the angle between a tangent line to a function and the x axis.,"If we have a function $f(x)>0$ that has the following property: $$f''(x)>0, $$ where this property only holds for $x>0$ . What is the angle $\theta$ that a tangent line to the function makes with the $x$ -axis, given that the tangent line must pass through the origin, and the point of tangency with $f(x)$ occurs in the first quadrant? Here is what I have done: Let the tangent line be $y=mx$ , where $m>0$ So we let the two functions equal each other, and take the derivative of both sides with respect to $x$ to give us another equation as well, since their slopes will be equal at the intersection. Now we have the two equations: $$f(x)=mx$$ $$f'(x)=m$$ Now after solving for $m$ , I obtained the following result: $$m=f'\left(\frac{f(x)}{f'(x)}\right)$$ Which will leave us with: $$\theta=tan^{-1}\left(f'\left(\frac{f(x)}{f'(x)}\right)\right)$$ to the $x$ -axis. Are my steps in solving this correct? Will this leave us with an angle independent of x if we plug in a function that satisfies the conditions?","['angle', 'calculus', 'functions']"
4553358,Proof request: every polytope has a facet,"When I was reading Ziegler's book ""Lectures on Polytopes"" this statement appeared to be never proven formally. Question: does every (convex, bounded, non-empty) polytope have a facet? Here I assume that the polytope $P\subset\Bbb R^d$ has dimensions $d$ , and a facet is a face of dimension $d-1$ . Depending on the amount of theory developed to this point there are many ways to prove this. But it seems some element in the chain towards such a proof it missing in the book. E.g, proving any of the following statements might already be sufficient: the face lattice is graded of length $d+1$ (stated but never proven in Ziegler) the face lattice is co-atomic (proven using duality and the next unproven fact) given a $\delta$ -face $f$ of $P$ , its dual face in $P^\circ$ is of dimension $d-1-\delta$ (not proven as far as I can tell) Maybe I am just overlooking the proof in the book, but an elementary self-contained proof of the existence of a facet is welcome anyway.","['polyhedra', 'discrete-geometry', 'polytopes', 'convex-geometry', 'discrete-mathematics']"
4553375,Closed Form Formula for Nonlinear Recurrence $a_{n+1}=\frac{a_{n}}{2} + \frac{5}{a_{n}}$,"I'm trying to find a closed form solution to the sequence $a_{n+1}=\frac{a_{n}}{2} + \frac{5}{a_{n}}$ I tried using a generating function approach in the following way: Let $$f(x) = \sum_{n=1}^\infty a_n x^n$$ Then multiplying the original equation with $x^n$ and summing over all $n$ , we get: $$\sum_{n=1}^\infty a_{n+1}x^n= \sum_{n=1}^\infty \frac{a_{n}}{2} x^n + \sum_{n=1}^\infty\frac{5}{a_{n}} x^n$$ $$= \frac{f(x) - a_1}{x} = \frac{f(x)}{2} + 5 \sum_{n=1}^\infty \frac{x^n}{a_n}$$ But now I don't know how to simplify the last term and I get stuck. I put it on Wolframalpha and
found the general solution to the recurrence relation as: $$a_n = -i \sqrt{10} \cot\left(c_1 2^n\right)$$","['closed-form', 'recurrence-relations', 'sequences-and-series']"
4553376,Complex analysis and generating functions problem or question,"I've been studying some complex analysis lately and I find very intriguing the relation between this field and generating functions. I've seen this video by 3Blue1Brown and it's really fascinating. Essentially, I'd like to find some kind of problem or question in which I could use this types of techniques in order to ilustrate the procedures for a project. I'd like to remark that this is a project, not actual research, so I just need to find a problem  so I can apply this techniques and show some further applications. Ideally, the problem is cloesly related to something like computer science, or anything that allows me to create something, since the project is meant to be original and I'm not so naive to belive that with my knowledge I could discover something new.","['complex-analysis', 'generating-functions']"
