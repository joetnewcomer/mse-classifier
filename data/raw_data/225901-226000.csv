question_id,title,body,tags
4660927,"If $a^k\equiv a\mod p$ for all $a$, show $(p - 1)\mid (k-1)$","Problem statement. Suppose that $k$ is a positive integer and $p$ a prime such that $a^k\equiv a\mod p$ for all positive integers $a$ .
Show that $(p - 1)$ divides $(k-1)$ . The proof is simple if we can use the fact that $U(p):=(\mathbb{Z}/p\mathbb{Z})^\times$ is cyclic.  In that case, we simply observe that $|U(p)|=p-1$ and select any generator $a$ .  It is a well-known result that if in a finite cyclic group generated by $a$ we have $a^i=a^j$ , then the order of $a$ divides $i-j$ .  Hence $p-1$ divides $k-1$ . The problem is, the student who needs to prove this result isn't allowed to use the fact that $U(p)$ is cyclic.  And all the proofs I know for $U(p)$ being cyclic are way too advanced for him, so he isn't able to prove it for himself.  Hence I am asking: Question. Is there an elementary proof of the above, appropriate to an undergraduate abstract algebra course, that doesn't use the fact that $U(p)$ is cyclic? (Then again, maybe I am wrong that it is too advanced for him to prove that $U(p)$ is cyclic.  If you have any ideas there, they are welcome too.) Thanks guys!","['group-theory', 'cyclic-groups', 'finite-groups']"
4660934,Show that a sequence is bounded.,"Let $f: \mathbb{R}_+ \to \mathbb{R}$ be a Lipschitz continuous function, i.e. there exits some $C > 0$ such that for all $x,y \in \mathbb{R}_+$ , we have $$
|f(x) - f(y)| \leq C|x-y|.
$$ If $N \sim Poi(\lambda)$ , $\lambda>0$ , we can consider the random variable $f(N)$ . Assume that $\mathbb{E}[f(N)] = 0$ , i.e. $e^{-\lambda}\sum_{n=0}^\infty \frac{\lambda^n}{n!}f(n) = 0$ . Now, we consider the sequence $$
a_l := \frac{l!}{\lambda^{l+1}} \sum_{n=0}^l\frac{\lambda^n}{n!}f(n).
$$ The goal is to show that $a_l$ is a bounded sequence. My thoughts: I was able to prove an equivalent representation of $a_l$ given by $$
a_l = \frac{\mathbb{E}[f(N)1_{\{N \leq l\}}]}{\lambda \mathbb{P}[N = l]}.
$$ Since $$
a_l := \underbrace{\frac{l!}{\lambda^{l+1}}}_{\to \infty} \underbrace{{\sum_{n=0}^l\frac{\lambda^n}{n!}f(n)}}_{\to 0}
$$ it would suffice that the partial sums on the RHS converge fast enough to 0.
However, I am not sure how to proceed. It is not clear to me on how I can apply the Lipschitz condition in order to show boundedness. Some help or guidance into the right direction would be really appreciated. Thanks in advance!","['lipschitz-functions', 'poisson-distribution', 'sequences-and-series']"
4661103,Estimating $\lambda$ in a Poisson Distribution from a set of data,"I need to estimate $\lambda$ from this data. The observed frequencies / probabilities are obtained by doing each total number observed divided by $280$ . I know that $P(X=0) = \frac{e^{-\lambda}\cdot \lambda^0}{0!} = 0.514$ , so this gives $\lambda = 0.666$ . My notes say this is a correct way of doing it. However, my notes also say I can solve by obtaining a sample average to give $\lambda = 0.684 $ . My notes say this gives the theoretical values in the table. How do I do this ? I don't know how the theoretical values were obtained.","['statistics', 'poisson-distribution', 'probability-distributions', 'parameter-estimation', 'probability']"
4661119,Is there a non trivial smooth function that has uncountably many roots?,"(on a bounded domain).
I believe that such a function could not exist since every $C^\infty$ function can be approximated by a sequence of polynomials and every polynomial has a finite number of roots so it would not be possible for something that vanishes countably often to converge to something that vanishes uncountably often.",['analysis']
4661130,How do I find the probability distributions for multiple dice rolls for dice with a differing number of sides?,"I have been learning how to play Dungeons and Dragons recently, and have bought my first set of dice. The standard DnD dice are: 1d4 1d6 1d8 1d10 1d10 × 10 [10, 20, 30... 90, 100] 1d12 1d20 I was thinking about inventive ways to determine successes or failures as a DM, and I came up with the Super Roll ! A super roll is rolling all seven of the standard dice, and being asked to roll above a particular number. I think this limit should be constant when asked to make a super roll, but I need to determine what this limit should be. I therefore want to calculate the probability distribution so I can pick a limit with an appropriate chance of succeeding at a super roll. I could simulate this with Python and get an approximation of the distribution, but where's the fun in that? I want to do this in a proper way and learn some maths along the way. But I've hit a wall. Researching this, there is a lot of information about rolling multiple dice. But in every case they always roll dice with the same number of sides . I watched 3Blue1Brown's video about convolutions where Grant begins by explaining how convolutions can be used to add two random variables. In his example, he selects two 6-sided dice. He goes on to say how this becomes a useful tool if, for example, the weightings of these probabilities for a given side isn't uniform. And he gives the formula for a discrete convolution: $$(a * b)_{n} = \sum_{\substack{i,j \\ i+j=n}} a_{i}\cdot b_{j}$$ But I don't want this. My dice aren't biased, instead it is only the number of side that is changing. I have worked through an example by hand using the sliding windows method to convolve two dice rolls, 1d4 with 1d6. It worked, and I methodically calculated the probabilities P(X=x). However in the video Grant then uses python to calculate a convolution. He convolves $(1,2,3) * (4,5,6) = (4,13,28,27,18)$ . For my example, 1d4 and 1d6. The convolution is $(1,2,3,4) * (1,2,3,4,5,6) = (1,4,10,20,30,40,43,38,24)$ , or if I do it in the order I did for the sliding windows, $(1,2,3,4) * (6,5,4,3,2,1) = (6,17,32,50,40,30,20,11,4)$ And I don't see how this is helpful to my problem. What do I need to do here? How can I use convolutions to calculate all values of P(X=x). Any help would be greatly appreciated.","['convolution', 'dice', 'probability-theory', 'probability', 'random-variables']"
4661150,"Classify $U(1)$ bundle over $\mathbf{P}^3$, and its topological invariants [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I am interested in knowing the classification of the $U(1)$ bundle over the complex projective space $\mathbf{P}^3$ . This is effectively a $U(1)$ bundle over the $6$ -manifold $M^6$ . What are the possible values of the $$c_1 p_1$$ for that $6$ -manifold $M^6$ with $U(1)$ bundle over $\mathbf{P}^3$ ? Here $c_1$ is the first Chern class of $U(1)$ bundle, and the $p_1$ is the first Pontryagin class of the tangent bundle of the $6$ -manifold? Can you give a configuration where $\int_{M^6} c_1 p_1=1$ or $\int_{M^6} c_1 p_1=4$ ?","['algebraic-topology', 'algebraic-geometry', 'homology-cohomology', 'characteristic-classes', 'differential-geometry']"
4661169,Is the set of linear transformations without unit norm eigenvalues dense?,"Let $E$ be a Banach space and $$H_p=\{f\in\mathcal L(E): f(v)=\lambda v\ (v \neq 0)\implies |\lambda|\neq 1\}$$ be the set of linear operators which don't have an eigenvalue with norm $1$ . Is $H_p$ dense in $\mathcal L(E)$ ? This is false if we consider the set of hyperbolic transformations $$H=\{f\in\mathcal L(E):\lambda \in \sigma(f)\implies |\lambda|\neq 1\}$$ as José Alves shows in these lecture notes , on example 5.3. The space $H_p$ feels much bigger than $H$ , plus the proof that the hyperbolic matrices in a finite dimensional space are dense by perturbing the eigenvalues makes me think that this is true.","['spectral-theory', 'functional-analysis']"
4661202,An orthonormal basis of $L^{2}(\mathbb{R}^{n})$ which is pointwise in $\ell^{2}(\mathbb{N})$?,"Does there exist an orthonormal basis of the Hilbert space $L^{2}(\mathbb{R}^{d})$ , say $(e_{n})_{n=1}^{\infty}$ , such that all elements $e_n\in L^{2}(\mathbb{R}^{d})\cap C^{0}(\mathbb{R}^{d})$ and such that for every $x\in\mathbb{R}^{d}$ , the sequence $(e_{n}(x))_{n=1}^{\infty}\subset \mathbb{C}$ is in $\ell^{2}(\mathbb{N})$ , i.e. such that the sum $$\sum_{n=1}^{\infty}|e_{n}(x)|^{2}$$ is finite? I was thinking about the case $d=1$ , and using the basis of $L^{2}(\mathbb{R})$ constructed from the Hermite polynomials, but I don't get enough decay to make the series converge.","['integration', 'hilbert-spaces', 'orthogonality', 'real-analysis']"
4661263,Is the fact that $e^{ix}$ and $\cos(x) + i \sin(x)$ have the same derivative and a point in common enough to imply they’re equal,"If we let $$f(x) = e^{ix}$$ and $$g(x) = \cos(x) + i \sin(x),$$ then \begin{align*}
f(0) &= g(0) = 1,\\
f’(x) &= if(x),\\
g’(x) &= ig(x).
\end{align*} Is this enough to imply they’re the same - is it possible for any other function $k(x)$ to take the form $k’(x) = i\ k(x)$ and where $k(0) = 1$ .","['derivatives', 'complex-numbers', 'ordinary-differential-equations']"
4661306,Contradiction in derivatives as linear approximations,"From the definition of a derivative, we have that $$f'(a) = \lim\limits_{x\to a}\frac{f(x)-f(a)}{x-a}$$ or $$\lim\limits_{x\to a}f'(x) = \lim\limits_{x\to a}\frac{f(x)-f(a)}{x-a}$$ This leads me to believe we can write $$\lim\limits_{x\to a}(f(a)+(x-a)f'(x))=\lim\limits_{x\to a}f(x)$$ Which to me, implies that derivative allow us to obtain a good approximation for a function near $a$ , and that this approximation will become increasingly accurate as $x\to a$ . However, couldn't we just as easily write $$\lim\limits_{x\to a}(f(a)+2(x-a)f'(x))=\lim\limits_{x\to a}f(x)$$ since $$\lim\limits_{x\to a}(f(a)+(x-a)f'(x))=f(a)+\lim\limits_{x\to a}((x-a)f'(x))$$ and $$\lim\limits_{x\to a}((x-a)f'(a)) = 0 = \lim\limits_{x\to a}(2(x-a)f'(a))$$ provided that $\lim\limits_{x\to a}f'(a)$ exists. Wouldn't this imply instead that $\lim\limits_{x\to a}(f(a)+2(x-a)f'(x))$ can be used to provide good approximations for the function near $a$ ? In fact, couldn't we replace $2$ with any other constant or function which has a limit at $a$ ? Using such approximations, however, would obviously produce incorrect results in proofs such as those for the chain and product rules. So how can this contradiction be dealt with? Why is using $f'(x)$ simply more correct than using $2f'(x)$ , even when the math doesn't necessarily seem to be demonstrating this?","['linear-approximation', 'calculus', 'derivatives']"
4661345,Which holomorphic functions have constant argument on rays from the origin? On circles centered at the origin?,"Multiples $$f(z) = c z, \qquad c \in \Bbb C \setminus \{0\},$$ of the identity function on $\Bbb C \setminus \{0\}$ trivially all satisfy the following special condition: Condition A : All points on a given open ray $\{\arg z = \theta_0\}$ centered at the origin have the same argument under $f$ , that is, whenever $\arg z = \arg w$ we also have $\arg f(z) = \arg f(w)$ . Put another way, the function $\arg \circ f$ descends via $\arg$ ; in that case we can interpret the descent as a function $S^1 \to S^1$ . That $f(z) = c z$ satisfies Condition A is visible on an argument plot of the function, wherein each point $z$ is colored according to the argument $\arg f(z)$ (the case $c = 1$ , i.e., the identity function, is shown). Question A: What are all of the holomorphic functions that satisfy Condition A? We can ask just as well for a dual condition: Condition B : All points on a given circle $\{|z| = r_0\}$ centered at the origin have the same argument under $f$ , that is, whenever $|z| = |w|$ we also have $\arg f(z) = \arg f(w)$ . The function $\arg \circ f$ descend via the modulus function $|\cdot|$ to a function $\Bbb R_+ \to S^1$ ? The argument plot of such a function is rotationally invariant: Question B: What are all of the holomorphic functions that satisfy Condition B?","['complex-analysis', 'cauchy-riemann-equations', 'exponential-function', 'partial-differential-equations']"
4661371,Clarification of the $u$-substitution theorem,"I came across this phrasing of the theorem justifying u-substitution: Let $F(x)$ be an antiderivative of $f(x)$ in an interval $I.$ Let $\phi$ from $J$ to $I$ , $\phi(t) = x$ be a differentiable function. Then $\int f(x)dx=\int f(\phi(t))\phi^{\prime}(t)dt.$ I am confused about the assumptions part - first of all, why can we assume that there exists a function $\phi$ such that $\phi(t)=x$ ? Secondly, we know that the image of $\phi$ over $J$ is a subset of $I$ . Why aren't we demanding that the image of $\phi$ will be equal to $I$ , and not just a subset? In my mind, we are ""losing"" some $x$ values that are not given by $\phi(t)$ if it is strictly contained. Lastly, in the final steps of the proof of this theorem, we said that $F(\phi(t))+c=F(x)+c=\int f(x)dx.$ Why can we treat $x$ just like a ""dummy"" variable? We assumed it equals a function $\phi(t)$ after all.","['integration', 'indefinite-integrals', 'calculus', 'substitution']"
4661375,Measurability of a classical topological surface and its measure,"Let $\Sigma \subset \mathbb{R}^3$ be a set with the following property: Given any $p\in \Sigma$ , $\exists$ $W_p \subset_{\text{open}} \mathbb{R}^3$ , $U_p \subset_{\text{open}} \mathbb{R}^2$ such that $p \in W_p$ , and there exists a homeomorphism $\varphi_p: U_p \rightarrow W_p \cap \Sigma$ (Both having the subspace topology induced from usual topology of $\mathbb{R}^2,\mathbb{R}^3$ respectively). In essence $\Sigma$ is a 'topological' surface. Regarding this definition I had 2 questions which I could not find online: If $\mathcal{L}(\mathbb{R}^3)$ denotes the lebesgue sigma algebra in $\mathbb{R}^3$ , is it true that $\Sigma \in \mathcal{L}(\mathbb{R}^3)$ always? Intuitively, it feels like a surface is essentially a collection of patches which are like deformed planar regions, and so should have $0$ volume. Then if $\Sigma$ is a surface such that $\Sigma \in  \mathcal{L}(\mathbb{R}^3)$ , then is it true that $m(\Sigma)=0?$ for any such surface? (where $m$ denotes the lebesgue measure) It seems to me that if the surface has some differentiable structure (eg. regular surfaces) then the following properties should be true, as in locally I can use implicit function theorem and write it locally as a graph of a $\mathbb{R}$ valued continuous function; in that case we can most likely show that each patch has measure $0$ and by second countability/ $\sigma$ -finiteness of $\mathbb{R}^3$ , the surface is measurable and has measure $0$ , but what about the general case, in the sense allowing weird kind of pointy surfaces?","['general-topology', 'geometric-measure-theory', 'measure-theory', 'differential-geometry']"
4661383,Polynomial complex equation with conjugates,"I am looking for bibliographical resources where I can read about polynomial equation with conjugates of the following form $$\sum_{k+l\leq n}a_{k,l}z^k\overline{z}^l=0$$ For example for $n=2$ $$a_{0,2}\overline{z}^2+a_{2,0}z^2+a_{1,1}z\overline{z}+a_{0,1}\overline{z}+a_{1,0}z+a_{0,0}=0$$ I have found it interesting because the set of solutions cannot be trivially described as in the case without conjugates. Isn't it a good research topic for undergraduate dissertation?","['algebraic-curves', 'algebraic-geometry', 'complex-numbers']"
4661410,How do I solve this differential equation? $y'' - 2y' +4y = e^x\sin (x)$,I have calculated the wronskian and while finding particular solution using variation of parameters. The integral becomes $$\int \sin(\sqrt{3}x)\sin(x)dx$$ I am stuck here. Have I done anything incorrectly prior to this? Please explain. The general solution to corresponding homogenous equation is $e^x(c_1\cos(\sqrt3x) + c_2\sin(\sqrt3x))$ .,['ordinary-differential-equations']
4661418,Finding the general term of a Taylor series expansion of $f(x)=\frac{1}{\sqrt{x}}$,"I came across a question in a workbook that asked me to find the general expression of the function $f(x)=\frac{1}{\sqrt{x}}$ centred at $x=4$ . I approached this problem by first finding its fourth-degree taylor series (the problem did not specify the degree of the taylor series, just the expression of the general term). $$f(x)=\frac{1}{\sqrt{x}}\Longrightarrow f(4)=\frac{1}{\sqrt{4}}=\frac{1}{2}$$ $$f'(x)=\frac{-1}{2x^\frac{3}{2}}\Longrightarrow f'(4)=\frac{-1}{2{\sqrt{4}}^3}=\frac{-1}{16}$$ $$f''(x)=\frac{3}{4x^{\frac{5}{2}}}\Longrightarrow f''(4)=\frac{3}{4{\sqrt{4}}^5}=\frac{3}{128}$$ $$f'''(x)=\frac{-15}{8x^{\frac{7}{2}}}\Longrightarrow f'''(4)=\frac{-15}{8{\sqrt{4}}^7}=\frac{-15}{1024}$$ $$f^{(4)}(x)=\frac{105}{16x^{\frac{9}{2}}}\Longrightarrow f^{(4)}(4)=\frac{105}{16{\sqrt{4}}^9}=\frac{105}{8192}$$ As such, the fourth-degree taylor series may be written as: $$P_4(x)=\frac{1}{2}-\frac{1}{16}(x-4)+\frac{3}{256}(x-4)^{2}-\frac{15}{6144}(x-4)^{3}+\frac{105}{196608}(x-4)^4$$ However, this is where the problem appears. I am struggling to find a representation for the numerator part of the coefficients. The pattern that I found seems to be $1\times3\times5...\times(2n-1)$ . However, I have no idea how I may represent this algebraically. As such, the best general term expression that I can do is $\sum_{n=0}^{\infty} {\frac{(-1)^{n}}{n!}}⋅{\frac{1⋅3⋅5⋅...⋅(2n-1)}{2^{n}⋅2^{2n+1}}}⋅(x-4)^{n}$ , which I am not quite satisfied with. Is there a better way to show this?","['factorial', 'calculus', 'taylor-expansion', 'sequences-and-series', 'derivatives']"
4661489,"Are there other, non-probabilistic ways to calculate: $\lim_{n\to\infty}\frac{1}{n}\ln\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!}$?","In section five of this nice exposition of moment generating functions, we prove the following theorem: Take an i.i.d sequence of random variables $(X_n:\Omega\to\Bbb R)_{n\in\Bbb N}$ whose common cumulant generating function $\Lambda$ is finite in a neighbourhood of the origin (the value $+\infty$ is permitted in general). Let $X:=X_1$ . Define $\Lambda^\ast$ to be the ""Fenchel-Legendre transform"" of $\Lambda$ : $$\Lambda^\ast:\Bbb R\to\overline{\Bbb R},\,x\mapsto\sup_{t\in\Bbb R}(x\cdot t-\Lambda(t))$$ And define for every $n\in\Bbb N$ the empirical means: $$Y_n:=\frac{1}{n}\sum_{j=1}^nX_j:\Omega\to\Bbb R$$ If $\alpha>\mathbb{E}(X)$ and $\mathrm{Pr}(X>\alpha)>0$ then $0<\Lambda^\ast(\alpha)<\infty$ and we get the asymptotics of the ""large deviations"": $$\frac{1}{n}\ln\mathrm{Pr}(Y_n>\alpha)\overset{n\to\infty}{\longrightarrow}-\Lambda^\ast(\alpha)=\inf_{t>0}(\Lambda(t)-\alpha\cdot t)$$ With convergence from below. The proof is - to me - a fairly complex and delicate application of probability theory. I don't yet know enough to follow it all the way through. I wanted to ""see it in action"" so I explicitly calculated $\Lambda^\ast$ for the Poisson distribution. Let's fix some $\lambda>0$ . If we take $(X_n)_n$ which follow the Poisson distribution with parameter $\lambda$ , it is easy to check that $\Lambda(t)=\lambda(e^t-1)$ for all $t$ . Some basic calculus optimisation will find: $$\Lambda^\ast(\alpha)=\lambda+\alpha(\ln(\alpha\cdot\lambda^{-1})-1)$$ Whenever $\alpha>\lambda$ . So the theorem predicts that: $$\lim_{n\to\infty}\frac{1}{n}\ln\left(\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!}e^{-n\lambda}\right)=-\lambda+\alpha(1-\ln(\alpha\cdot\lambda^{-1}))$$ A little rearranging brings that to: $$\tag{$\ast$}\forall\,\,0<\lambda<\alpha:\quad\quad\quad\lim_{n\to\infty}\frac{1}{n}\ln\left(\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!}\right)=\alpha(1-\ln(\alpha\cdot\lambda^{-1}))$$ Which seems highly nontrivial to do by other means. I am very curious to see whether or not anyone on the site has the expertise to supply a ""real analytic"" proof of this identity. I know that the probabilistic proof is just a real analytic proof with a particular flavour, but I mean to ask if there are other ways of doing this which aren't purely motivated by ideas from probability or measure theory (the proof seems to involve a ""change-of-measure"" trick, which is a new one on me). Or, if this special Poisson case admits a simpler probabilistic proof, that would be interesting to see too. I think this special case is an interesting problem. Since I don't fully understand the proof I've already seen, I reckon that verifying $(\ast)$ is way above my paygrade. I hope to learn from your answers :) Just one thing I can helpfully observe: By the Stolz-Cesaro theorem, it would suffice to show that (although I do not know if this is true): $$\lim_{n\to\infty}\frac{\sum_{m>(n+1)\alpha}\frac{((n+1)\lambda)^m}{m!}}{\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!}}=\left(\frac{\lambda e}{\alpha}\right)^\alpha$$ Oh, and a related cool (I think) expression can be deduced by applying the same technique to a sequence of binomial variable variables with parameters $n,1/2$ : If $n\in\Bbb N$ and $\frac{n}{2}<\alpha<n$ then: $$\lim_{m\to\infty}\frac{1}{m}\ln\left(\sum_{k>m\alpha}\binom{mn}{k}\right)=n\ln n-\alpha\ln\alpha-(n-\alpha)\ln(n-\alpha)$$ More generally, if $0<p<1$ , $n\in\Bbb N$ and $np<\alpha<n$ : $$\lim_{m\to\infty}\frac{1}{m}\ln\left(\sum_{k>m\alpha}\binom{mn}{k}\left(\frac{p}{1-p}\right)^k\right)=\alpha\ln\left(\frac{p}{1-p}\right)+n\ln n-\alpha\ln\alpha-(n-\alpha)\ln(n-\alpha)$$ This also seems quite nontrivial without this “master theorem”. I would be equally interested to see a proof of that by other means.","['limits', 'real-analysis']"
4661496,Relationship between the weak law of large numbers and characteristic functions,"I'm trying to learn some probability theory atm and got stuck with the following exercise in Durrett's Probability: Theory and Examples : Exercise 3.3.17. Let $X_1, X_2, \ldots $ be i.i.d. with characteristic function $\varphi$. If $\varphi'(0) = ia$ and $S_n = X_1 + \dots + X_n$, then $S_n/n\to a $ in probability. If $S_n/n\to a$ in probability then $\varphi(t/n)^n\to e^{iat}$ as $n\to \infty$. Use 2. and the uniform continuity of $\varphi$ to show that $(\varphi(h)-1)/h \to -ia$ as $h\to 0$. Thus the weak law holds if and only if $\varphi'(0)$ exists. I would really appreciate some help with the third part of this exercise (I don't quite see the connection between $\varphi(t/n)^n$ and $(\varphi(h)-1)/h$, yet). Thanks for your help! =) My thoughts on 1, 2 : I managed to prove 1. using the inequality $\mu\{x\, : \, |x|>u/2\} \le u^{-1} \int_{-u}^u (1-\varphi(t)) \, dt$, where $\mu$ is the pushforward measure of a random variable $X$ and $\varphi$ is its characteristic function. Using the fact that the ch.f. of $S_n/n - a$ is given by $e^{-iat}\varphi(t/n)^n$, this leads to $$P\left[\left|\frac{S_n}n - a\right| > 2/u \right]\le u^{-1} \int_{-u}^u (1-e^{-iat}\varphi(t/n)^n) \, dt$$ Now 1. implies $\varphi(t/n)^n \to e^{iat}$, so the RHS goes to zero as $n\to \infty$ for every fixed $u$. 2.: Using $|e^{i\epsilon t} - 1| \le 2\epsilon |t|$ for small enough $\epsilon>0$: \begin{align}
\left|\varphi (t/n)^n - e^{iat}\right| 
&= \left| E\left[e^{iS_n/nt} - e^{iat}\right]\right| \\
&\le E\left|e^{i(S_n/n - a)t} - 1\right| \\
&\le 2 \epsilon |t| + 2P[|S_n/n - a|> \epsilon]
\end{align} So $\limsup_{n\to\infty}\, \left|\varphi (t/n)^n - e^{iat}\right| \le 2\epsilon |t|$ and since $\epsilon>0$ was arbitrary (apart from being small) this proves $\varphi(t/n)^n\to e^{iat}$.",['probability-theory']
4661548,A counting function that is Borel measurable,"Question: Let $F:\mathbb{R^2\to R}$ be a continuous function. Define $p(x)$ as the number of $y$ such that $F(x,y)=0$ ,i.e. $p(x)=\#\{y\in\mathbb{R}|F(x,y)=0\}$ . Prove that $p(x)$ is (extended) Borel measurable(as $p(x)$ valued in $[0,+\infty]$ ). Attempt: It's not hard to see that $\{(x,y)|F(x,y)=0\}$ and $\{y|F(x_0,y)=0\}$ are both closed set.But I don't know how to deal with the 'counting function' $p(x).$ Thanks in advance!","['measure-theory', 'analysis', 'real-analysis']"
4661551,KL divergence for distribution representing sums of iid random variables,"Sorry if my description is inaccurate, I hope it's understandable. Given $X_1,...,X_n$ , a series of $n$ iid Bernoulli RVs with means $p$ , and a similar series $Y_1,...,Y_n$ with means $q$ , we know that the KL divergence between the probability measure $P$ corresponding to the sum $X=X_1+...+X_n$ and $Q$ that corresponds to $Y=Y_1+...+Y_n$ is (since $X$ and $Y$ are binomial) $$KL(P,Q)=nd(p,q),$$ where $d(p,q)$ stands for the KL divergence between measures of Bernoulli RVs with means $p$ and $q$ . A similar principle also holds for the divergence between measures corresponding to sums of independent Gaussians. My question is whether we can claim this holds in general.
Meaning, given i.i.d $X_1,...,X_n$ such that each RV has a measure $P_x$ and similarly for $Y_1,...,Y_n$ and $Q_y$ , define $X=X_1+...+X_n$ and $Y=Y_1+...+Y_n$ where measure $P$ corresponds to $X$ and $Q$ to $Y$ . Can we say that $$KL(P,Q)=nKL(P_x,Q_y)?$$ Thank you in advance!","['statistics', 'entropy', 'probability', 'information-theory']"
4661563,Half spaces are measurable,"I am trying to do exercise 7.4.3. in Tao's Analysis II: Prove that the half space $E:=\{(x_1,\cdots, x_n)\in \mathbb{R}^n| x_n>0\}$ is measurable. i.e. $m^{\ast}(A)=m^{\ast}(A\cap E)+m^{\ast}(A\setminus E)$ for any subset $A\subseteq \mathbb{R}^n.$ The hint he gave is to first prove that if $A$ is an open box $(a_1,b_1)\times \cdots \times (a_n,b_n)$ in $\mathbb{R}^n$ , and $E$ is the half-plane $E:=\{(x_1,\cdots, x_n)\in \mathbb{R}^n| x_n>0\}$ , then $m^{\ast}(A)=m^{\ast}(A\cap E)+m^{\ast}(A\setminus E)$ . I was able to prove the hint but I am not sure how it helps to prove half spaces are measurable. I have to show that for any set $A$ , we have $m^{\ast}(A)=m^{\ast}(A\cap E)+m^{\ast}(A\setminus E)$ . We have showed that it is true if $A$ is an open box but I don't know how to extend that to arbitrary sets. Any help would be appreciated.Thanks.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4661625,"In an open set $U$, can we always extend a disc to an open ball contained in $U$?","Let $X$ be a metric space and $U$ be an open set containing $D_R(x_0) := \{x\in X : d(x, x_0)\le R\}$ . Now, can we say that there will be an $R_1 > R$ such that $B_{R_1}(x_0)\subseteq U$ ? If I replace $D_R(x_0)$ by $B_R(x_0)$ , then it's obviously false.","['general-topology', 'metric-spaces']"
4661658,Schauder estimates on a punctured disk.,"The following is related to my previous question [1], which I think can be resolved if this question is resolved. Is it true for $u\in C^{1,\alpha}(B-p)$ where $B=B_p(r)$ a function satisfying the spacetime Laplacian $\Delta u+K|\nabla u|=0,$ that the following estimate holds, $$r^2|\nabla\nabla u|\leq C r^{1+\alpha}?$$ Here, $K$ is the trace of some prescribed symmetric $(0,2)$ tensor $k$ . For simplicity, let us assume $K=0$ , so that $u$ is harmonic. Classical Schauder estimates from Gilbarg-Trudinger tell us that the following bound holds, $$r^2|\nabla\nabla u|\leq C |u|_{C^0(B-p)}.$$ Does it follow that for harmonic $u$ , we have $|u|_{0;B-p}\leq C r^{1+\alpha}$ ? References: [1]: PDE inequalities to show a gradient is integrable","['partial-differential-equations', 'general-relativity', 'differential-geometry']"
4661714,"Name for symmetric functions, $f(x, y)=g(x) h(y)+h(x) g(y)$.","I was playing around with some ideas and thinking that this function $$f(x, y)=x \sqrt y+y \sqrt x $$ looked like it might be fun to explore. But then I considered looking at the set of functions with the same structure. $$f(x, y)=g(x) h(y)+h(x) g(y)$$ and this might be even more interesting. Of course, this idea could be extended to functions
in 3 variables like $$ f(x, y, z)=xy+yz+zx $$ If there was a name for this type of function, then that would give me a lead and help me look up what other people have found out.  If this type of function does have a name. Or maybe there are some interesting places to start exploring? I think the extension to three variables that I would use would use a cycle rule... $$f(x, y, z)=g(x)h(y)j(z)+h(x)j(y)g(z)+j(x)h(y)g(z) $$","['functions', 'terminology']"
4661720,Prove that $\sum_{l=0}^{N-1}\frac{\sin^2(\pi x)}{\sin^2(\frac{\pi}{N}(l-k+x))}=N^2$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question When I numerically compute the sum below it is always $1$ . How can I prove this? $N$ is an integer number and $k$ is an integer number between $0$ to $N-1$ and $x$ is real number between $0$ and $0.5$ $${\frac{1}{N^2}\sum_{l=0}^{N-1}\frac{\sin^2(\pi x)}{\sin^2(\frac{\pi}{N}(l-k+x))}}$$","['trigonometry', 'summation', 'sequences-and-series']"
4661723,Finding rational points on a circle such that $X^2+Y^2=r^2=k \in \mathbb{Z}$,"I am interested in finding rational points on a circle with radius $r$ , such that $r^2=k$ is an arbitrary integer. I tried reducing the problem to the unit circle, and maybe use pythagorean triples as solutions and modify them accordingly, but this would require finding rational points on the unit circle whose denominator is divisible by $k$ , and I do not see a way to do that. Note that $r$ is not necessarily an integer, only its square is necessarily an integer. More formally, I am interested in finding rational solutions $(X,Y)$ to the equation $$X^2+Y^2=k$$ where $k$ is an integer. Is there any general way to do that? Can I then systematically create all solutions whose numerator and denominator are not larger than some bound $L$ ?","['number-theory', 'pythagorean-triples', 'circles', 'discrete-mathematics', 'rational-numbers']"
4661727,Generalizing Hall's marriage theorem,"(This question has been posted on mathoverflow: Generalizing Hall's marriage theorem ) Fix positive integers $m,n,k$ such that $n\geq k$ . Consider a bipartite graph between two sets of vertices $A$ and $B$ consisting of $n$ and $mn$ vertices respectively. Suppose each vertex in $A$ is connected to exactly $mk$ vertices in $B$ and each vertex in $B$ is connected to exactly $k$ vertices in $A$ . (Assume there are no double edges between any pair of vertices.) Is it possible to select a subset $B'$ of $B$ of size $n$ such that each vertex of $A$ is connected to exactly $k$ vertices in $B'$ ? This is trivially true when $k=1$ , whereas the case $k=2$ is equivalent to Hall's marriage theorem. (This follows from Peterson's 2-factor theorem.) Can we say something for general $k$ ?","['graph-theory', 'matching-theory', 'combinatorics']"
4661797,Distribution of pre-transformed Gaussian Random variables,"Let $f,g:\mathbb{R} \to \mathbb{R}$ be monotonic and differentiable functions, and suppose that $(X, Y)$ is a random vector such that $$
(f(X),g(Y))^T \sim N (\mu, \Sigma),
$$ where $\mu = (\mu_1, \mu_2)^T$ and $\Sigma$ is a 2x2 covariance matrix. We therefore have that $$
g(Y)|f(X) \sim N(\mu_2 + \frac{\sigma_{21}}{\sigma_1^2} (f(X) - \mu_1), \sigma^2_2 - \frac{\sigma_{21}}{\sigma_1^2}).
$$ I'm interested in the distribution of $Y|X$ though. Is there a closed form expression?","['conditional-probability', 'probability-distributions', 'probability-theory']"
4661819,A ternary relation on a group,"Let $G$ be a group. Consider the ternary relation $R \subset G^3$ defined by $$R(x,y,z) \Leftrightarrow x y^{-1} z x^{-1} y z^{-1} = 1 $$ Show that $R$ is a symmetric relation, that is if $R(x,y,z)$ , then $R(x',y',z')$ for all permutations $(x',y',z')$ of $(x,y,z)$ . Notes The expression above can be encountered in Pappus' theorem . There the group $G$ is the $2\times 2$ lower triangular matrices. For the case when $R$ is a ternary equivalence relation , see the paper ""Groups with a ternary equivalence relation"". This is where it was stated that the symmetry holds for all groups $G$ ( a surprise! ). $\bf{Added:}$ There were some very crisp answers. From what I understand, one idea seems to be this: Consider the $n$ -ary relation on a group $G$ $$E(x_1, \ldots, x_n) \Leftrightarrow x_1 \cdots x_n= 1$$ Then $E$ is invariant under cyclic permutations, and also $$x_1 \cdots x_n = 1 \implies x_n^{-1} \cdots x_1^{-1} = 1$$ These two properties take care of the proof.
We also get $$R(x,y,z) \implies R(x^{-1}, y^{-1}, z^{-1})$$ and $$R(x,y,z) \implies R(a x, a y , a z), R(xa, y a, z a)$$ I've found another way to express $R$ : Note that if $x y^{-1} = w$ , $z x^{-1} = v$ , $y z^{-1} = u$ , then $x = w y$ , $z = v x$ , $y = u z$ .  So $w$ , $v$ , $y$ are (left) displacements. Now it is easy to see that $$u v w = y z^{-1} \cdot z x^{-1} \cdot x y^{-1} = 1$$ But the condition says that on top of this we have $w v u = 1$ . This is equivalent to $u$ , $v$ , $w$ commute. So that is the meaning of $R$ : the (left) displacements corresponding to $x$ , $y$ , $z$ commute. It is now easy to see why this condition is symmetric.  Moreover it is equivalent to the condition for the right displacements. This gets us back to the problem where this expression originates. In the Pappus theorem, the projections from one line to another through one of the points $X$ , $Y$ , $Z$ are elements of $PGL(2, k)$ . We have the condition $R$ if and only if the points $X$ , $Y$ , $Z$ are collinear. That means that the relation $R$ has the property $$R(x,y,z) \& R(x,z, t) \implies R(x,y,t)$$ Now, the paper quoted above does state some theorems, but the fact is this: $R$ satisfies the above ( similar to collinearity) if and only if the commutant of any non-trivial element is abelian. This can be expressed as follows: the relation $x \simeq y$ on $G \backslash \{e\}$ if $x$ , $y$ commute is an $equivalence$ relation (    a CA-group ). One should check that for the group $G = PGL(2, k)$ it is true that the centralizer of any element $\ne 1$ is abelian .","['group-theory', 'geometry', 'relations']"
4661836,Find the cracked area,"You have a square of side 1m, consider that both triangles have vertices at the midpoint of the square, find the cracked area $S=2 S\triangle - S_ \boxed{LMNP}=2. \frac{1}{2}.\frac{1}{2}.1 - S \boxed{}=\frac{1}{2}- S_\boxed{LMNP}$ Need to find the area $S_\boxed{LMNP}??$","['euclidean-geometry', 'geometry', 'plane-geometry']"
4661841,Confusion Over The Definition of a Transposition Cipher,"In our Discrete Mathematics class, the way the textbook introduces the transposition cipher is as follows: As
a key we use a permutation $\sigma$ of the set $\{1, 2, \ldots , m\}$ for some positive integer $m$ , that is, a one-to-one function from $\{1, 2, \ldots, m\}$ to itself. To encrypt a message we first split its letters into blocks of size $m$ . We encrypt the block $p_1p_2\ldots p_m$ as $c_1c_2\ldots c_m = p_{\sigma(1)}p_{\sigma(2)}\ldots, p_{\sigma(m)}$ . To decrypt a ciphertext block $c_1c_2, \dots c_m$ , we transpose its letters using the permutation $\sigma^{−1}$ , the inverse of $\sigma$ . And then an example is brought up, where a transposition cipher is done. Using the transposition cipher based on the permutation $\sigma$ of the set $\{1, 2, 3, 4\}$ with $\sigma(1) = 3$ , $\sigma(2) = 1$ , $\sigma(3) = 4$ , and $\sigma(4) = 2$ , encrypt the plaintext message PIRATE ATTACK...We first split the letters of the plaintext into blocks of four letters. We obtain PIRA TEAT TACK. To encrypt each block, we send the first letter to the third position, the second letter to the first position, the third letter to the fourth position, and the fourth letter to the second position . We obtain IAPR ETTA AKTC. The confusion arises because given the notation $c_1c_2\ldots c_m = p_{\sigma(1)}p_{\sigma(2)}\ldots p_{\sigma(m)}$ , wouldn't the ciphered text be in the form $p_3p_1p_4p_2$ (rather than the suggested $p_2p_4p_1p_3$ )? I've looked online and it may be that the difference is caused because the permutation is an active permutation, not a passive one, but I still find this to not make sense.","['permutations', 'discrete-mathematics', 'cryptography']"
4661850,"Equation of a circle that passes through $(4,3)$ and which, touches the $y$ axis and another given circle","Find the equation of a circle which passes through $M(4,3)$ , touches the $y$ axis and touches the circle $(x-2)^2 + y^2 =1$ Now, if I suppose that the equation is looks like $$(x-p)^2 + (y-q)^2 = r^2$$ Since the circle touches another circle whose radius is $1$ , we can write that $q=1+r$ . However, right here is where I'm stuck. I don't know how to express the other terms, and I'm not sure what I need the point $M(4,3)$ for. Could anyone help?","['analytic-geometry', 'geometry']"
4661907,"Are there ""measurable"" properties?","Consider two measurable spaces $(X,\mathcal{A})$ and $(Y,\mathcal{B})$ , consisting of sets $X$ and $Y$ and some $\sigma$ -algebras $\mathcal{A}$ and $\mathcal{B}$ defined on each of them respectively. Suppose we defined that $(X,\mathcal{A})$ and $(Y,\mathcal{B})$ were ""measurably homeomorphic"" if and only if there existed a bijection $h: X \to Y$ that also implictly defined a bijection between $\mathcal{A}$ and $\mathcal{B}$ , akin to how a homeomorphism between topological spaces also defines a bijection between topologies. What are some good references to learn and study what properties of $\sigma$ -algebras are conserved under ""measurable homeomorphism""? Is the study of such ""measurable properties"", akin to the study of topological properties as things conserved under homeomorphism, trivial or not? (Trivial here meaning, for example, that nearly all measurable spaces are measurably homeomorphic or something like that.)","['measure-theory', 'reference-request']"
4661931,Find Jordan Basis of triple differentiation operator in $\mathbb{R}_8[x]$,"General information and notation at first.
Let $A$ be matrix of $T$ and let $T$ be such operator, that $(1, x, x^2, \cdots x^8) \to (0, 0, 0, 6, 24x, 60x^2, 120x^3, 210x^4, 336x^5)$ $T^2$ : $(1, x, x^2 \cdots x^8) \to (0, 0, \cdots, 720, 5040x, 20160x^2)$ Transformation is nilpotent, since $T^3$ leads to zeroes. Also the only non-zero values are at the diagonal that is above of the main diagonal. Hence, all 9 eigenvalues equal to $0$ . In Jordan normal form of $A$ we have three Jordan blocks of size $3$ with $0$ at diagonal. Now back to the question. I suspect, there is some pretty interesting way to find Jordan basis. Something that involves playing around with coefficients. Factorials (perhaps, not completely) involved too, since, for example $x^8 \to 336x^5$ , where $336 = 8\cdot 7 \cdot 6$ . But I can't grasp this idea, unfortunately. I am acquainted with some general algorithms for finding basis but seems like I lack understanding of this theme.
Anyway, what can we do here?","['matrices', 'jordan-normal-form', 'linear-algebra']"
4661953,Integrals of $\int_{0}^{1} \left ( \frac{K^\prime}{K} \right )^{s-1} f(k)\text{d}k$,"Consider a type of integrals $$
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
f(k)\text{d}k
$$ where $K=K(k),K^\prime=K(\sqrt{1-k^2})$ are complete elliptic integrals, and $k$ is an elliptic modulus. $f(k)$ will be chosen if it satisfies some properties. And meanwhile their values can be obtained in brief forms(expressed by Dirichlet $L$ -series in most cases). Several cases have been considered in Question.1 , Question.2 . The paper gave out numerous and rich examples, e.g. $$
\left ( \frac{2}{\pi}  \right )^2
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
K(k)\text{d}k=2^{s+2}\pi^{-s}\Gamma(s)\lambda(s)\beta(s-2).
$$ To start, I should list the $L$ -series to be used. $$
\beta(s)=\sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)^s},\eta(s)=(1-2^{1-s})\zeta(s),\lambda(s)=(1-2^{-s})\zeta(s),\\
$$ $$
L_8(s)=\sum_{n=0}^{\infty}\left ( \frac{1}{(8n+1)^s}
-  \frac{1}{(8n+3)^s}- \frac{1}{(8n+5)^s}+ \frac{1}{(8n+7)^s}\right ),\\L_{-8}(s)=\sum_{n=0}^{\infty}\left ( \frac{1}{(8n+1)^s}+ \frac{1}{(8n+3)^s}- \frac{1}{(8n+5)^s}-\frac{1}{(8n+7)^s}\right ),\\
L_{-20}(s)
=\sum_{n=1}^{\infty} \left ( \frac{-20}{n}  \right )
\frac{1}{n^s}.
$$ $(\frac{m}{n})$ is the Kronecker symbol. The main instructions are the same as what did in the paper. Consider $$
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
g(k)\text{d}\left ( \frac{K^\prime}{K}  \right ).
$$ By making a substitution $x=K^\prime/K$ , we have $$
\int_{0}^{\infty}x^{s-1}g\left ( \frac{\theta_2(q)^2}{\theta_3(q)^2}  \right )
\text{d}x.
$$ It's clear that $$
\frac{2K}{\pi} =1+2\sum_{n=1}^{\infty} \frac{1}{\cosh(\pi nx)},x=K^\prime/K.
$$ Taking mellin transforms both sides. Suppose for $s$ large sufficiently, we have \begin{aligned}
\int_{0}^{\infty}x^{s-1}\left ( \frac{2K}{\pi} -1 \right )\text{d}x 
& = 2\int_{0}^{\infty} \sum_{n = 1}^{\infty} \frac{x^{s-1}}{\cosh(\pi n x)}
\text{d} x\\
&=2\sum_{n=1}^{\infty}\frac{1}{(\pi n)^s} \int_{0}^{\infty} \frac{x^{s-1}}{\cosh(x)} \text{d}x\\
&=4\pi^{-s}\Gamma(s)\zeta(s)\beta(s).
\end{aligned} Now consider series of functions, $$
f(z)=\operatorname{ns}(z,k^\prime)
\left(\frac{\mathrm{d}^{2n}}{\mathrm{d}y^{2n}} 
\frac{1}{\cosh(y)} \right)\Big|_{y\rightarrow\frac{\pi z}{2K} }.
$$ Follow same steps we obtain: \begin{aligned}
&\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{1}{(1-k^2)K(k)} \text{d}k
=2^{s+2}\pi^{-s}\Gamma(s)\lambda(s)\beta(s),\\
&\left ( \frac{2}{\pi}  \right )^2
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
K(k)\text{d}k=2^{s+2}\pi^{-s}\Gamma(s)\lambda(s)\beta(s-2),\\
&\left ( \frac{2}{\pi}  \right )^4
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
(1-5k^2)K(k)^3\text{d}k=2^{s+2}\pi^{-s}\Gamma(s)\lambda(s)\beta(s-4),\\
&\left ( \frac{2}{\pi}  \right )^6
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
(1-46k^2+61k^4)K(k)^5\text{d}k=2^{s+2}\pi^{-s}\Gamma(s)\lambda(s)\beta(s-6),\\
&\left ( \frac{2}{\pi}  \right )^8
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
(1-411k^2+1731k^4-1385k^6)K(k)^7\text{d}k=2^{s+2}\pi^{-s}\Gamma(s)\lambda(s)\beta(s-8),\\
&\left ( \frac{2}{\pi}  \right )^{10}
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
(1-3692k^2+41838k^4-88412k^6+50521k^8)K(k)^9\text{d}k=2^{s+2}\pi^{-s}\Gamma(s)\lambda(s)\beta(s-10)
\end{aligned} and etc. Consider $$
f(z)=\operatorname{nc}(z,k^\prime)
\left(\frac{\mathrm{d}^{2n+1}}{\mathrm{d}y^{2n+1}} 
\frac{1}{\cosh(y)} \right)\Big|_{y\rightarrow\frac{\pi z}{2K} },
$$ which generates $\beta(s)\beta(s-2n-1)$ : $$
\left ( \frac{2}{\pi}  \right )
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{1}{\sqrt{1-k^2}}\text{d}k=2^{s+2}\pi^{-s}\Gamma(s)\beta(s)\beta(s-1),
$$ $$
\left ( \frac{2}{\pi}  \right )^3
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{(1-2k^2)K(k)^2}{\sqrt{1-k^2}}\text{d}k=2^{s+2}\pi^{-s}\Gamma(s)\beta(s)\beta(s-3),
$$ $$
\left ( \frac{2}{\pi}  \right )^{11}
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{P_{11}(k)K(k)^{10}}{\sqrt{1-k^2}}\text{d}k=2^{s+2}\pi^{-s}\Gamma(s)\beta(s)\beta(s-11),
$$ where $P_{11}(k)=1-11074k^2+210112k^4-729728k^6+884480k^8-353792k^{10}$ . Consider $$
f(z)=\operatorname{sc}(z,k^\prime)
\left(\frac{\mathrm{d}^{2n}}{\mathrm{d}y^{2n}} 
\frac{\cosh(y)}{\cosh(2y)} \right)\Big|_{y\rightarrow\frac{\pi z}{2K} },
$$ which generates $\lambda(s)L_{-8}(s-2n)$ . We have $$
\frac{2\sqrt{2} }{\pi^2} 
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{\left(1+k^2+\sqrt{1-k^2}\right) }{\sqrt{1-k^2}  ( 1+\sqrt{1-k^2} )^{3/2} }K(k) \text{d}k
=2^{s+1}\pi^{-s}\Gamma(s)\lambda(s)L_{-8}(s-2)
$$ Or can be written in a pleasant form: $$
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{(1+3k)K(k)}{\sqrt{k(1+k)} } \text{d}k
=2^{2s-1}\pi^{2-s}\Gamma(s)\lambda(s)L_{-8}(s-2).
$$ Consider $$
f(z)=\operatorname{nc}(z,k^\prime)
\left(\frac{\mathrm{d}^{2n+1}}{\mathrm{d}y^{2n+1}} 
\frac{\cosh(y)}{\cosh(2y)} \right)\Big|_{y\rightarrow\frac{\pi z}{2K} },
$$ which generates $\beta(s)L_{-8}(s-2n-1)$ . We have $$
\frac{4\sqrt{2} }{\pi^3} 
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{\left(1-2k^2+\sqrt{1-k^2}(1+4k^2)\right) }{(1-k^2)^{3/4} \left ( 1+\sqrt{1-k^2}  \right )^{3/2} }K(k)^2\text{d}k
=2^{s+1}\pi^{-s}\Gamma(s)\beta(s)L_{-8}(s-3)
$$ Some other examples are considered: $$
\frac{\sqrt{2} }{\pi} 
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{1}{\sqrt{1-k^2}\sqrt{1+\sqrt{1-k^2} }  } \text{d}k
=2^{s+1}\pi^{-s}\Gamma(s)\lambda(s)L_8(s-1),
$$ $$\frac{4\sqrt{2} }{\pi^3} 
\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{\left ( 1+k^2+\frac{1-5k^2}{\sqrt{1-k^2} }  \right ) }{(1+\sqrt{1-k^2})^{3/2}  }K(k)^2\text{d}k
=2^{s+1}\pi^{-s}\Gamma(s)\lambda(s)L_8(s-3).
$$ Question: How to discover more generalizations, or how to find a method extensively calculating these integrals?","['integration', 'calculus', 'contour-integration', 'elliptic-functions', 'elliptic-integrals']"
4662018,"Pigeonhole principle, a sum question","$$\text{Let }\space S\subset\{1,2,\ldots,101\}\text{ s.t }\space|S|=52.\\\text{Prove that there exist different values }a,b,c\in S\text{ s.t }\\a+b=c.$$ That question appeared at my last Discrete math exam. One of the solutions I've tried to understand is defining a function $F$ from $S$ to $\{1,2,\ldots,50\}$ so $F(n)$ returns $n$ if $n\leqslant50$ , otherwise it returns $m-n$ while $m$ is the maximum value at set $S$ . I can’t figure why this works, I mean, I know it works, but I want to be able to use the same idea at similar variations. edit: F is defined from S\{m} to {1, ..., 50}","['pigeonhole-principle', 'discrete-mathematics']"
4662023,Find the derivative of a difficult integral,"This exercise is very difficult for me.
Find the derivative of the function: $$
\int_{0}^{\ln x}f(t) dt
$$ I use this formula: $$
\int(b(x)) \cdot b'(x) - \int(a(x)) \cdot a'(x)
$$ where this is $b'(x)$ the derivative of $b$ and this is $a'(x)$ the derivative of $a$ . And the end my answer is $$
\int(\ln(x)) \cdot \frac{1}{x}
$$ My question is: Is my answer right? The problem is that the condition was to use the definition of $$F(x) = \lim_{\varepsilon \rightarrow 0}\frac{1}{\varepsilon }[F(x+\varepsilon) - F(x) ].$$ Also I have to use if the function is continuous in the interval [a,b], then there exists a point $k$ for which $a<k<b$ satisfying: $$\int_{a}^{b}f(x)dx = (b-a)f(k).$$ Finally, I make a boundary transition $\varepsilon \to 0$ and use the continuity of $f(x)$","['functions', 'derivatives', 'definite-integrals', 'epsilon-delta']"
4662073,How can I compute the differential of this function between surfaces $C$ and $G$?,"Let me consider the surface $S:=\{(x,y,z): x^2+y^2=1\}$ and $G:=\{(0,y,z): y,z\in \Bbb{R}\}$ and define $f:C\rightarrow G$ by $f(x,y,z)=(0,y,z)$ . Let us take the following two patches for $C$ and $G$ : $$\sigma_1(u,v)=(\cos(u),\sin(u), v)$$ and respectively $$\sigma_2(w,z)=(0,w,z)$$ Now I want to compute the differential at an arbitrary point $p\in C$ and express it in the base of the tangent plane. My idea was the following: Let me pick an arbitrary point $p\in C$ , then since $\sigma_1$ is a patch we know that $p=\sigma_1(u_0,v_0)$ for some $u_0,v_0$ . Then let us define $$\begin{align}\gamma(t)&=\sigma_1(u_0+t,v_0)\\\beta(t)&=\sigma_1(u_0,v_0+t)\end{align}$$ we see that $\gamma\perp \beta$ . At this point also remark that $\gamma'(0)=\sigma_{1,u}(u_0,v_0)$ and $\beta'(0)=\sigma_{1,v}(u_0,v_0)$ Then let us compute $$\begin{align}Df_p(\gamma'(0))&=(f\circ\gamma)'(0)\\&=\frac{d}{dt}~\left(f(\gamma(t))\right)\big|_{t=0}\\&=(0,\cos(u_0),0) \end{align}$$ and similarly $$\begin{align}Df_p(\beta'(0))&=(f\circ\beta)'(0)\\&=\frac{d}{dt}~\left(f(\beta(t))\right)\big|_{t=0}\\&=(0,0,1) \end{align}$$ so we have found two tangent vectors which are linealy independent. Now since the differential is linear we know that for a general curve $\alpha:[-1,1]\rightarrow C$ with $\alpha(0)=p$ and $\alpha'(0)=a\sigma_{1,u}(u_0,v_0)+b\sigma_{1,v} (u_0,v_0)$ we have $$\begin{align}Df_p(\alpha'(0))&=Df_p(a\cdot\sigma_{1,u}(u_0,v_0)+b\cdot\sigma_{1,v} (u_0,v_0))\\&=a\cdot Df_p(\sigma_{1,u}(u_0,v_0))+b\cdot Df_p(\sigma_{1,v}(u_0,v_0))\\&=a\cdot Df_p(\gamma'(0))+b\cdot Df_p(\beta'(0))\\&=a\cdot (0,\cos(u_0),0)^T+b\cdot (0,0,1)^T\\&=a\cdot \cos(u_0) \sigma_{2,w}+b\cdot \sigma_{2,v}\end{align}$$ Does this work or am I wrong?","['differential', 'differential-geometry', 'tangent-spaces', 'real-analysis']"
4662086,"Variational formulation, weak formulation","I'd like to find the weak formulation of the problem $-u''+au=f$ on $(0,1)$ $u(0)=0$ $u'(1)=b$ $a>0$ and show that there exists a unique solution using Lax-Milgram. What I did:
By multiplying the first term with a test function $v$ and doing partial integration I get that \begin{align}
-\int_{0}^{1}u''(t)v(t)dt=-u'(1)v(1)+u'(0)v(0)+\int_{0}^{1}u'(t)v'(t)dt.
\end{align} Since I want the first part of the equation to be zero, I set $v \in H_{0}^{1}$ .
So the formulation of the problem is \begin{align}
\int_{0}^{1}u'(t)v'(t)dt+a\int_{0}^{1}u(t)v(t)dt=\int_{0}^{1}f(t)v(t)dt
\end{align} for all $v \in H^{1}_{0}$ Is that part correct? I'm confused where the boundary condition $u'(1)=b$ is taken into account. Is $u$ in $H^{1}_{0}$ as well or do I need to define a new space $ V=\{v \in L^2(0,1), v' \in L^2(0,1), v'(1)=b, v(0)=0\}$ ? For the second part of the question (uniqueness + existence of solution):
Using Cauchy Schwarz I get for the bilinear form $a(u,v)$ \begin{align}
|a(u,v)|=\int_{0}^{1}u'(t)v'(t)+au(t)v(t)dt \leq ||u'||_{L^2}||v'||_{L^2}+a||u||_{L^2}||v||_{L^2}\leq||u'||_{H^{1}_{0}}||v||_{H^{1}_{0}}+a||u||_{H^{1}_{0}}||v||_{H^{1}_{0}}
\end{align} So this shows continuity of $a$ .
I'm not sure though which norm to use for $u$ since I'm not sure which space to choose.
Using Poincare I get \begin{align}
|a(u,u)|=||u'||_{L^2}^2+a||u||_{L^2}^2 \geq ||u'||_{L^2}^2 \geq C ||u||_{L^2}^2
\end{align} showing that the bilinear form is coercive.
The last thing I need to show is that $F$ , my linear functional, is continuous (bounded). This follows from \begin{align}
|F(v)| \leq ||f||_{L^2}||v||_{L^2} \leq ||f||_{L^2}||v||_{H^1_0}
\end{align} Hence, using Lax Milgram I've shown existence and uniqueness of a solution. Is it correct what I am doing? I am very new to this topic so I am not so sure about some steps, expecially how to choose the space for my $u$ .","['boundary-value-problem', 'functional-analysis', 'ordinary-differential-equations']"
4662147,How to compute the area of the given region (NBHM $2022$)?,"This is a question from NBHM $2022$ exam. It asks to find the area of the region $\{z+\frac{z^2}{2} \mid z\in \mathbb{C},|z| \leq 1\}$ Now $z+\frac{z^2}{2}$ = $\frac{(z+1)^2}{2}-\frac{1}{2}$ . The $-\frac{1}{2}$ part is just a translation, so it does not change the area. But I couldn't understand what is happening in the $\frac{(z+1)^2}{2}$ part. The resulting region is not a circular shape as the images of $i,-1,-i$ are $i,0,-i$ which lies on the same line. My question is how to understand what's happening in the $\frac{(z+1)^2}{2}$ part.
Edit: the answer is given $\frac{3\pi}{2}$","['complex-analysis', 'complex-integration', 'area']"
4662155,Weak convergence of functionals $g_n^*(f) = n\int_0^1 x^nf(x)dx$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Show that sequnce of functionals $g_n^*(f) = n\displaystyle{\int_0^1 x^nf(x)dx}, f \in C[0,1]$ converges weakly and find its limit functional. Does it converge in the norm of space $C^*[0,1]$ ? I don't even know how to start, I guess I have to guess first which is limit functional, and then prove $g_n^*$ converges to it, but I don't have any idea what it could be.","['weak-convergence', 'normed-spaces', 'functional-analysis', 'uniform-convergence', 'limits']"
4662201,Probability of certain ordering of people around a round table,"Given $20$ people - $10$ males and $10$ females that are sitting around a round table. Find the probability for which the order of the sitting is the following: Between any pair that consists of $2$ females, there is at least one male. For example a valid sitting order can be of the form: $\langle$ female,female,male,female,female,male,male,female,female,male,female,female,male ... $\rangle$ The answer should be one of the following: (a) $ 0.0655 $ (b) $ 0.0131 $ (c) $ 0.0027 $ (d) $ 0.0014 $ I tried combining each pair of females into one and finding the number of arrangements around the table to get an upper bound for the answer, but I am getting a number which is lower than the options which available, this is what I did: $\displaystyle{\frac{\displaystyle{10 \choose 2}2!14!}{19!}}$ There are $\displaystyle{10 \choose 2}$ ways to split the females into groups of $2$ , because the order matters here I multiplied it by $2!$ , then I multiplied by the number of ways to arrange $15$ people around a round table which is $(15-1)!$ but I am getting a number which is lower than the options for the answer in the question so I'm stuck. I'll appreciate any insights, thanks !","['discrete-mathematics', 'combinatorics', 'probability']"
4662219,The limit $\lim\limits_{n\to\infty}\frac{x^n}{n!}$,"Evaluate $$\lim_{n\to\infty}\frac{x^n}{n!}$$ where $x\in\mathbb R$ This is a common limit and has been asked and answered many times here on this site. However, I present another approach with Stolz-Cesaro( $x\notin[-1,1]$ . When $x\in[-1,1]$ , the limit is trivially zero): $$L=\lim_{n\to\infty}\frac{x^n}{n!}=\lim_{n\to\infty}\frac{x^{n+1}-x^n}{(n+1)!-n!}$$ $$L=\lim_{n\to\infty}\frac{x^n}{n!}\frac{(x-1)}{n}=L\frac{(x-1)}{n}$$ The above equation holds for all $x$ in domain only when $L=0$ . Hence the limit is just $0$ . I have never seen Stolz-Cesaro being used this way to solve a limit, hence the lack of surety.  It would be great to get this verified!","['limits', 'calculus', 'solution-verification']"
4662274,Locally free resolution of coherent sheaves on nonsingular curves,"This question is from Exercise II 6.11 of Hartshorne. Let $X$ be a nonsingular curve over an algebraically closed field $k$ .  For any coherent sheaf $\mathcal{F}$ on $X$ , show that there exist locally free sheaves $\mathcal{E}_1$ and $\mathcal{E}_0$ of finite ranks and an exact sequence $0\to \mathcal{E}_1 \to \mathcal{E}_0 \to \mathcal{F}\to 0$ . I can prove that if $\mathcal{E}_0$ exists, then we can take $\mathcal{E}_1$ be the kernel of $\mathcal{E}_0\to \mathcal{F}$ .  So it suffices to prove that $\mathcal{F}$ can be written as a quotient of a locally free sheaf of finite rank. The following is my attempt: Firstly, we can take an open subscheme $U=\operatorname{Spec}(A)$ such that $\mathcal{F}|_U\simeq \widetilde{M}$ , where $M$ is a finitely generated $A$ -module with generators $m_1,\cdots,m_n$ . I want to extend $m_i$ to the global section of $\mathcal{F}$ .  Since $X$ is a curve, $X-U$ consists of finite points.  For $p\in X-U$ , we can take $U'=\operatorname{Spec}(B)$ containing $p$ and $f\in B$ such that $p=V(f)$ .  Furthermore, we can assume that $m\in \mathcal{F}(D(f))\simeq \mathcal{F}(U')_f$ , then $f^N m\in \mathcal{F}(U')$ for some $N$ .  In this way I can extend $m$ to $U'$ by multiplying $f^N$ , but what is $f^N m$ in $\mathcal{F}(U)$ ?","['algebraic-curves', 'algebraic-geometry', 'coherent-sheaves']"
4662286,Dividing 200 people in 3 unequal rooms and probability of certain combinations,"We have 3 rooms (say 1, 2 and 3). Room 1 can take 50 people, room 2 can take 50 and room 3 can take 100. I understand that the number of ways of assigning 200 people to these 3 rooms is $200 \choose 50$$ 150 \choose 50$ . The question is what is the probability that Alice (A) and Bob (B) end up in the same room while Charlotte (C) ends up in a different room. My solution I first divided the event $E$ = {A and B in same room, C in different room} in the disjoint events $E1$ = {{A and B in room 1, C in room 2} $\cup$ {A and B in room 1, C in room 3}}  , $E2$ = {{A and B in room 2, C in room 1} $\cup$ {A and B in room 2, C in room 3}} and $E3$ = {{A and B in room 3, C in room 1} $\cup$ {A and B in room 3, C in room 2}}. I calculated the possible cases for E1 in the following way: $197 \choose 48$$149 \choose 49$ $+$ $197 \choose 47$ $149 \choose 50$ . My reasoning for the case A and B in 1 while C in 2 was that I first pick the people that will make up room 1 alongside A and B (excluding C) and then pick the people that will make up room 2 alongside C (room 3 will then be uniquely determined). The reasoning for the case A and B in 1 while C in 3 was analogous. Similar reasonings were employed to calculate the favorable cases for $E2$ and $E3$ : $|E2|$ = $197 \choose 49$$148 \choose 48$ $+$ $197 \choose 50$ $147 \choose 48$ $|E3|$ = $197 \choose 49$$148 \choose 50$ $+$ $197 \choose 50$ $147 \choose 49$ The probability was then simply calculated by summing the favorable cases for $E1$ , $E2$ and $E3$ and finally dividing by the total number of cases. I am wondering if this method is correct. Furthermore, if it is correct, I am wondering if there is a simpler method since this method requires dealing with awfully big numbers. Thank you for your help.","['combinatorics', 'probability']"
4662362,"Existence of ""induced measure"" on fibers of a measurable function between measure spaces?","Let $f : X\rightarrow Y$ be a measurable function between measure spaces $X,Y$ with measures denoted $\mu,\nu$ respectively. Suppose singleton subsets of $Y$ are measurable; hence fibers of $f$ are measurable subsets in $X$ . My question is, in general does there exist a collection $\{\lambda_y\}_{y\in Y}$ where $\lambda_y$ is a measure on $f^{-1}(y)$ for each $y\in Y$ , such that for every measurable subset $A \subset X$ we have $$
\mu(A) = \int_{y \in Y} \big(\lambda_y(A \cap f^{-1}(y))\big)\,d\nu(y)
\;\;?
$$ If not in general, then does this hold when, for example, $\mu,\nu$ are $\sigma$ -finite and there is some kind of absolute continuity condition, as in the Radon-Nikodym theorem? My guess would be that this is somehow related to the R-N theorem; however, I'm not sure how to construct any precise relationship between the two situations. Would anyone have any suggestions or hints on how to think about this?","['measure-theory', 'ergodic-theory', 'measurable-functions', 'analysis']"
4662431,Witten's proof of Wick Formula of QFT,"Let $\mathcal{S}$ be a finite dimensional real vector space with a positive definite summetric bilinear form $B$ . Let $dv$ be a Lebesgue measure on $\mathcal{S}$ such that $$\int_{\mathcal{S}}e^{-B(v,v)}dv =1\tag{1.10}$$ (convenient normalization). We want to prove the following. If $$\langle f_1 \dots f_N \rangle_0 := \int_{S} f_1(v)\dots f_N(v) e^{-B(v,v)/2} dv\tag{1.12}$$ where $f_1, \dots, f_N \in S^*$ , then as long as $N = 2K$ is even (obviously the integral vanishes for odd $N$ ), $$\langle f_1 \dots f_N \rangle_0 = \sum_{s \in S_{2K}/ \sim} B^{-1}(f_{s(1)},f_{s_(2)}) \dots B^{-1}(f_{s(2K-1)},f_{s(2K)})\tag{1.13}$$ where $B^{-1}$ is the unique inverse form of $B$ , $S_{2K}$ is the symmetric group, and $s_1 \sim s_2$ , $s_1,s_2 \in S_{2K}$ if they define the same term in the above. In the lecture, Witten gives the proof as Since both sides of [the claimed identity] are symmetric polylinear functions, the proposition reduces to the case $f_1 = \dots = f_{2N} = f$ , which is straightforward. Question: I understand the gist of this argument, but not sure why symmetric polylinearity reduces it to that special case (or then what the straightforward calculation exactly is). Can someone walk through his explanation? Reference: ""Quantum Fields and Strings: A course for mathematicians"", vol.1. ""Perturbative quantum field theory"" lecture 1: Renormalization of Feynman Diagrams (pg 425).","['proof-explanation', 'tensors', 'combinatorics', 'quantum-field-theory', 'mathematical-physics']"
4662450,Weak derivative of a step function,"Consider the function \begin{align} v(x)=\begin{cases}1&~\text{ if }x\in (0,1)\\ k&~\text{ if }x=0\\ -1&~\text{ if }x\in (-1,0)\\0&~\text{ if } x\geq1 \text{ or }x\le-1\end{cases}\end{align} I am proving that this function has no weak derivative and I want to know if my proof is correct: If there were a weak derivative $w$ of $v$ it would be zero almost everywhere since $v$ is constant almost everywhere: \begin{align}
\int_{-1}^{1}{v\phi'}=v\phi|_{-1}^{1}-\int_{-1}^{1}v'\phi=-\int_{-1}^{1}v'\phi
\end{align} then: \begin{align}
\int_{-1}^{1}{(v'-w)\phi}=0~~~\forall\phi\in C^{\infty}_0(\mathbb{R}).
\end{align} Also if we consider an fix $\alpha>0$ such that $\alpha\in(1/2,1)$ and let be $\phi_{\alpha}\in C^{\infty}_0(\mathbb{R})$ such that $\phi_{\alpha}(x)=0$ if $x\in (-\infty,-1-\alpha)$ and $\phi_{\alpha}(x)=1$ if $x\in [-1,1/2]$ then: \begin{align}
\int_{-1}^{1}{v\phi_{\alpha}'}=\int_{1/2}^{\alpha}{v\phi_{\alpha}'}=\phi_{\alpha}(\alpha)-\phi_{\alpha}(1/2)=-1
\end{align} but \begin{align}\int_{-1}^{1}{v\phi_{\alpha}'}=-\int_{-1}^{1}{w\phi_{\alpha}}=0\end{align} GPC","['solution-verification', 'functional-analysis', 'weak-derivatives']"
4662499,"How do we get $ J_{f \circ \psi} (x) = J_\psi(x) J_f(y) $ for $f$ on a submanifold, and $\psi$ is local coordinates?","The following is taken from Leon Simon Geometric Measure Theory: Let $f: M \to \mathbb{R}^P$ for $P \geq n$ . Where $M$ is an $n$ dimensional smooth submanifold of $\mathbb{R}^{n+l}$ , and $f$ is locally Lipschitz. For a tangent $\tau \in T_y M$ , we define the directional derivative $D_\tau f \in \mathbb{R}^P$ by $$ D_\tau f = \frac{d}{dt} f(\gamma(t))|_{t=0}$$ $\gamma: (-1,1) \to M$ is a $C^1$ curve, with $\gamma(0) = y$ and $\dot{\gamma}(0) = \tau$ . We go to local coordinates. Fix $y \in M$ . Let $$\varphi : U \cap M \to V $$ where $U \in \mathbb{R}^{n+l}$ and $V \in \mathbb{R}^n$ . Take the inverse $$ \psi:V \to M \cap U.$$ Apply Radamacher's Theorem (which says a Lipschitz function is differentiable almost everywhere) to $f \circ \psi: V \subset \mathbb{R^n} \to \mathbb{R}^P$ , so that there is $E_0 \in V$ with $H^n(E_0) = 0$ and $f \circ \psi$ is differentiable in $V \setminus E_0$ . So for any $\eta \in \mathbb{R}^n$ , $x \in V \setminus E_0$ , $$ D_\eta(f \circ \psi)(x) = \frac{d}{dt} f ( \psi(x + t \eta))|_{t=0} $$ exists and is linear in $\eta$ . But $\psi(x + t \eta)$ is a curve as in the definition above, so letting (by chain rule) $\tau = \dot{\psi} = \sum_{j=1}^n \eta_j D_j \psi(x)$ , we have that $$ \tag{*}D_\tau f(\psi(x))) = D_\eta(f \circ \psi )(x) $$ exists for all $\psi(x) \in U\cap M \setminus \psi(E_0)$ . $H^n(\psi(E_0))= 0$ because $\psi $ is locally Lipschitz on $V$ . Letting $\eta = e_i$ , so that the curve is $\psi(x + te_i)$ in $(*)$ and $\tau_1, ... \tau_n$ an orthonormal basis for $T_y M$ , we have that $\tau = D_i \psi(x)$ , and $$ D_i \psi(x) = \sum_{l=1}^n (D_i \psi(x) \cdot \tau_j) \tau_l $$ $$ D_i(f \circ \psi) (x) = \sum_{k=1}^n D_{\tau_k} f(y)( D_i \psi(x) \cdot \tau_k ) $$ $$\tag{**} D_i(f \circ \psi) \cdot D_j(f \circ \psi) = \sum_{k,m = 1}^n (D_i \psi \cdot \tau_k)(D_j \psi \cdot \tau_m)   D_{\tau_k}f(y) \cdot D_{\tau_m} f(y) $$ I understand so far . I dont get how the next few lines follow from the previous computations: Since $\det AB = \det A \det B$ for square matrices $A,B$ and $$ \sum_{k=1}^n D_i \psi(x) \cdot \tau_k D_j \psi(x) \cdot \tau_k = D_i \psi(x) \cdot D_j \psi(x) $$ this implies $$ J_{f \circ \psi} (x) = J_\psi(x) J_f(y) .$$ where $J_{f \circ \psi}(x)= \sqrt{\det(D_i (f \circ \psi)(x) \cdot D_j (f \circ \psi)(x))}$ , $J_\psi(x) = \sqrt{\det(D_i(\psi(x)) \cdot D_j(\psi(x))}$ and $J_f(y) = \sqrt{\det G(y)}$ where $G(y)$ is the $n \times n$ matrix with $(D_{\tau_k} f(y) \cdot D_{\tau_m} f(y))$ in the $k$ th row and $m$ th column. END SIMON I understand that equation $(**)$ is supposed to somehow lead us to matrix multiplication, and then we can take determinants etc. But I really dont understand how it all fits together, how the statements lead to the final conlusion. Where does the double sum in $k,m$ go? and I dont see how the matrix multiplication comes about. Can someone spell out the details?","['submanifold', 'linear-algebra', 'geometric-measure-theory', 'differential-geometry']"
4662507,"Prove that the following inequality is true for all $m \in (0, 3)$","Prove that the following inequality is true for all $m \in (0, 3)$ $$\sqrt {{m^2} + 1}  + \sqrt {{{\left( {3 - m} \right)}^2} + 1}  \le \sqrt {\frac{{2\left( {4{m^2} - 12m - 15} \right)}}{{(m - 3)m}}}  - 1.$$ I have defined $$f\left( m \right) = \sqrt {{m^2} + 1}  + \sqrt {{{\left( {3 - m} \right)}^2} + 1} ,g\left( m \right) = \sqrt {\frac{{2\left( {4{m^2} - 12m - 15} \right)}}{{(m - 3)m}}}  - 1.$$ but they do not make the function monotonous. The derivative of the function $h(m)=f(m)-g(m)$ is quite complicated. Another approach is to square both sides. I have tried squaring it four times, but it is still quite complicated. I would appreciate it if you could give me some ideas to prove the inequality. Thank you!","['calculus', 'cauchy-schwarz-inequality', 'algebra-precalculus', 'inequality']"
4662511,"Expand $\cos^a x$ in terms of $\cos kx$, $\sin mx$","If $a\geq 0$ , expand $\cos^a x$ in terms of $\cos kx$ , $\sin mx$ $$\cos^a x=\left(\frac{e^{ix}+e^{-ix}}2\right)^a$$ Since $a$ is a non negative real number, so by General Binomial theorem $$\cos^a x=\frac1{2^a}\sum_{k=0}^\infty\binom ake^{ikx}e^{-i(a-k)x}$$ where $\binom ak=a(a-1)(a-2)...(a-k+1)$ $$\cos^a x=\frac1{2^a}\sum_{k=0}^\infty\binom ake^{i(2k-a)x}$$ Any help would be appreciated.","['real-analysis', 'complex-analysis', 'trigonometry', 'binomial-theorem', 'complex-numbers']"
4662527,"If $x,y∈(-π,π]$, then find the area of the polygon formed by points $(x,y)$ satisfying the equation $\lfloor|\sin x|\rfloor+\lfloor|\cos y|\rfloor=2$.","If $x,y∈(-π,π]$ , then find the area of the polygon formed by points $(x,y)$ satisfying the equation $\lfloor|\sin x|\rfloor+\lfloor|\cos y|\rfloor=2$ . My attempts include using a graphing tool and taking $\sin x= \pm1$ and $\cos y=\pm1$ for $x=\pm \displaystyle{\frac{\pi}{2}}$ and $y=0,\pi$ .
How do you solve this further? $\lfloor\,\cdot\,\rfloor$ represents the greatest integer function / floor function.","['integration', 'area', 'graphing-functions', 'calculus', 'trigonometry']"
4662529,Is there a $q$-analog for the product of binomial coefficients?,"The $q$ -analog of the binomial coefficient $\binom{n}{k}$ may be defined as the coefficient of $x^k$ in $\prod_{i=0}^{n-1}(1+q^ix)$ . Classical arithmetic identities tend to have $q$ -analogs. I am interested in the following identity: $$\binom{n}{a}\binom{n}{b}=\sum_{i=0}^{\mathrm{min}(a,b)}\binom{a+b-i}{i,a-i,b-i}\binom{n}{a+b-i}$$ This can be proved by comparing the coefficient of $x^ay^b$ in both $(1+x)^n(1+y)^n$ and $(1+x+y+xy)^n$ . Question: Is there a $q$ -analog of this identity? For example, we have $\binom{n}{1}^2=2\binom{n}{2}+n$ , and its $q$ -analog is $$\binom{n}{1}_q^2=(q+q^2)\binom{n}{2}_q+q^{n-1}\binom{n}{1}_q$$ What replaces $\binom{a+b-i}{i,a-i,b-i}$ in general? Is there a version where the coefficients don't depend on $n$ ?","['binomial-coefficients', 'combinatorics', 'algebraic-combinatorics', 'q-analogs']"
4662550,How to solve the following limit,"If $$\lim_{x\to0}\frac1{x^m}\prod_{k=1}^n \int_0^x\big[k-\cos(kt)\big]\mathrm dt$$ exists and is equal to $20$ (where $m,n\in\mathbb N$ ) then what is the value of $n$ ? I started this question with the general term and am slightly confused on whether to apply L'hospital method on this particular problem, are there any other ways to evaluate this?","['integration', 'calculus']"
4662610,Find the limit $\displaystyle{\lim_{x\to 0}\frac{\sin(x)\sin^{-1}(x)-\sinh(x)\sinh^{-1}(x)}{x^2(\cos(x)-\cosh(x)+\sec(x)-\text{sech}(x))}}$,"Here is an example from the book ""Asymptotic Analysis and Perturbation Theory"" by William Paulsen. Example 1.10 p.18. Find $$\lim_{x\to 0}\frac{\sin(x)\sin^{-1}(x)-\sinh(x)\sinh^{-1}(x)}{x^2(\cos(x)-\cosh(x)+\sec(x)-\text{sech}(x))}$$ As this is an asymptotic book, the solution in the book uses the Maclaurin expansion of all the trig functions and simplifies the denominator to $x^8/6+\mathcal{O}(x^{10})$ . Then keeping just the right number of terms, the numerator is computed as $x^8/15+\mathcal{O}(x^{10})$ . Finally, $$\frac{\sin(x)\sin^{-1}(x)-\sinh(x)\sinh^{-1}(x)}{x^2(\cos(x)-\cosh(x)+\sec(x)-\text{sech}(x))}\sim\frac{x^8/15+\mathcal{O}(x^{10})}{x^8/6+\mathcal{O}(x^{10})}=\frac{2}{5}+\mathcal{O}(x^2)$$ I wonder if there are any ways to find the higher terms in the asymptotics or just any other ways to evaluate the limit. Thanks in advance.","['limits', 'approximation', 'asymptotics']"
4662619,Are these formulas equivalent?,"I am solving the problem from the textbook, and g) part states ""There is exactly one person whom everybody loves."" L(x, y) is ""x loves y."" (1) The first and easiest solution is: $\exists !y\forall xL\left(x,y\right)$ (2) Then I came up with the second solution: $\exists \:y\left(\forall \:x\left(L\left(x,\:y\right)\:\wedge \forall \:z\left(L\left(x,\:z\right)\rightarrow \left(z=y\right)\right)\right)\right)$ Which I think is also the correct one. (3) The third solution is: $\exists x\left(\forall yL\left(y,\:x\right)\:\wedge \forall z\left(\left(z\ne x\right)\rightarrow \exists w\neg L\left(w,z\right)\right)\right)$ (4) And the solution from the book is: $\exists x\left(\forall yL\left(y,x\right)\wedge \forall z\left(\left(\forall wL\left(w,z\right)\right)\rightarrow z=x\right)\right)$ Is the (2) second formula correct in this case? and if so, are they equivalent?","['quantifiers', 'predicate-logic', 'discrete-mathematics']"
4662689,Calculating Expected Increase In Prize Line Count,"Hello and thanks in advance. I'm trying to solve a problem I'm having with a pattern matching game. The game's board consists of a 5x5 grid of numbers where column 1 (on the far left) contains 5 random numbers from a pool of numbers in the range 1-15, column 2 contains 5 random numbers in the range 16-30 etc. up to column 5 (on the far right) containing 5 random numbers from 61-75. We end up with a game board such as the following: [5]  [26] [43] [54] [75]

[13] [23] [39] [46] [72]

[12] [21] [33] [59] [71]

[10] [29] [34] [60] [67]

[8]  [18] [40] [55] [65] which is very similar to a traditional 75 ball bingo card but has a number in the centre square (usually replaced with a star in bingo but not relevant to this game). The patterns to be matched are identical to the prize lines in 75 ball bingo. There are 12 possible patterns (prize lines) to match 5 horizontal lines (e.g. 5->26->43->54->75), 5 vertical lines (e.g. 43->39->33->34->40) and 2 diagonal lines (e.g. 5->23->33->60->65). The game proceeds by the player having a number of turns, the specific count of turns being irrelevant to this question. Each turn consists of a draw of one number for each column (5 numbers drawn per turn) from number pools each dedicated to a specific column. The goal of the game is to complete as many of the patterns/prize lines as possible, with a higher count of completed patterns yielding a higher score. What I'm trying to calculate is the expected increase in the prize line count from a single turn. I'd like to obtain a formula or series of steps for calculating this expectation from any given board state . For example, the player might be half way through their game and may have daubed (checked off) a random selection of half of their numbers on the board. I asked a similar question elsewhere and some very smart members of the maths community (one in particular - thanks Timothy!) suggested that this question was better asked here but also showed that by iterating over the incomplete lines and calculating the probability of that line being completed on the next spin, I could then sum those probabilities to calculate the expected increase in the count of complete lines. This worked perfectly for me where the pools of numbers contained only the numbers in the column's range. I could even unbalance their distribution within the column's pool and still get correct expectation values. However, if I introduce a wildcard symbol that will match any number in the column, the probabilities using the above method no longer sum to the expected test value. Here are some examples of the solution first of all working with balanced number pools and then unbalanced number pools and then not working with a wildcard introduced. Firstly, I'm scaling down the board size to make brute forcing the probabilities much simpler for an example and using the following board with only 4 squares: [1] [3]
[x] [4] The number pools are also simplified to contain only the numbers on the board. Column 1 number pool: 1,2
Column 2 number pool: 3,4 So every turn either 1 or 2 will be drawn from column 1's number pool and either 3 or 4 will be drawn from column 2's number pool. For the purposes of the example, we have pre-daubed the bottom left square (which previously held the number 2). Now there are no complete prize lines and 6 remaining incomplete prize lines (2 horizontal, 2 vertical, 2 diagonal). We calculate the probability in this simple example ahead of time by brute forcing every possible draw outcome, of which there are only 4, and manually counting the additional prize lines that will be completed as a result of each one: 1,3 = 3 additional prize lines completed
1,4 = 3 additional prize lines completed
2,3 = 1 additional prize line completed
2,4 = 1 additional prize line completed On average therefore (8 prize lines / 4 possible draw combinations), we know ahead of time that the expected increase in the number of completed prize lines is 2.0 from a single turn. Now we apply our proposed solution to the same board state by calculating the probability of completing each incomplete prize line as follows: Line 1->3 = ½ * ½ = 0.5 * 0.5 = 0.25
Line 2->4 = ½ = 0.5
Line 1->2 = ½ = 0.5
Line 3->4 = 0
Line 1->4 = ½ * ½ = 0.5 * 0.5 = 0.25
Line 2->3 = ½ = 0.5 The sum of these probabilities is equal to our ahead of time calculation (2.0) despite the fact that multiple combinations of these prize lines are mutually exclusive. This first example is a successful example of summing probabilities using the solution proposed above. The second example is with an unbalanced number pool for one of the columns. This time we increase the complexity by expanding the board and number pools as below. Now there are 9 numbers on the board and 8 prize lines. There are now 36 possible number draw combinations (3x3x4). [1] [4] [7]
[2] [5] [8]
[3] [6] [9]

Number pools:
 1   4   7
 2   5   8
 3   6   9
         9 To start with, we’ve pre-daubed the middle vertical prize line as follows, leaving 7 incomplete prize lines. [1] [x] [7]
[2] [x] [8]
[3] [x] [9] We have to brute force all possible number pool draw combinations in order to list the increase in completed prize lines for each and calculate the expected overall increase. Below are those possible combinations and the additional complete prize lines they yield. Note there are 2 9s in column 3’s number pool so we have apparently repeated combinations including the second 9. In brackets are the additional number of completed prize lines resulting from each combination. 147(1),148(0),149(1),149(1),157(1),158(0),159(1),159(1),167(1),168(0),169(1),169(1) 
247(0),248(1),249(0),249(0),257(0),258(1),259(0),259(0),267(0),268(1),269(0),269(0)
347(1),348(0),349(1),349(1),357(1),358(0),359(1),359(1),367(1),368(0),369(1),369(1) 21 additional prize lines / 36 draw combinations = expected increase in prize line count of 0.58333 (rounded to 5 dp) If we now apply our proposed solution to each incomplete prize line (7 total) as we’ve done previously, the results are as follows: Line 1->4->7 = ⅓ * ¼ = 0.33333 * 0.25 = 0.08332
Line 2->5->8 = ⅓ * ¼ = 0.33333 * 0.25 = 0.08332
Line 3->6->9 = ⅓ * ½ = 0.33333 * 0.5 = 0.16667
Line 1->2->3 = 0 (not possible to complete this line in a single draw)
Line 7->8->9 = 0 (not possible to complete this line in a single draw)
Line 1->5->9 = ⅓ * ½ = 0.33333 * 0.5 = 0.16667
Line 3->5->7 = ⅓ * ¼ = 0.33333 * 0.25 = 0.08332 Summation of the above probabilities = 0.5833 which ~= 0.58333 from our brute force example (more precision in these numbers will make them very close). So far so good. Finally though, I add a wildcard '*' to the number pool for the 3rd column as follows: [1] [4] [7]
[2] [5] [8]
[3] [6] [9]

Number pools:
 1   4   7
 2   5   8
 3   6   9
         * The * symbol will match any number in that column. This time the brute forced combinations are as follows: 147(1),148(0),149(1),14*(1),157(1),158(0),159(1),15*(1),167(1),168(0),169(1),16*(1) 
247(0),248(1),249(0),24*(1),257(0),258(1),259(0),25*(1),267(0),268(1),269(0),26*(1)
347(1),348(0),349(1),34*(1),357(1),358(0),359(1),35*(1),367(1),368(0),369(1),36*(1) Predictably, this yields more complete prize lines. 24 / 36 = 0.66667 (rounded up to 5dp). However, if we now apply our solution, we see the following: Line 1->4->7 = ⅓ * ½ = 0.33333 * 0.5 = 0.16667
Line 2->5->8 = ⅓ * ½ = 0.33333 * 0.5 = 0.16667
Line 3->6->9 = ⅓ * ½ = 0.33333 * 0.5 = 0.16667
Line 1->2->3 = 0 (not possible to complete this line in a single draw)
Line 7->8->9 = 0 (not possible to complete this line in a single draw)
Line 1->5->9 = ⅓ * ½ = 0.33333 * 0.5 = 0.16667
Line 3->5->7 = ⅓ * ½ = 0.33333 * 0.5 = 0.16667 The sum of these probabilities is 0.83335, which is wildly different from our predicted 0.66667. Please could you help me understand why the wildcard is so different from additional, repeated numbers? I know it will match any of the numbers of course but why do the two calculations diverge? If anyone can see an alternative or preferable method of calculating this expectancy without brute forcing on board states, I'm all ears! Thank you for reading and thanks in advance for any input.","['probability-theory', 'probability-distributions', 'game-theory', 'problem-solving', 'probability']"
4662698,Let $X_n \to X$ and $Y_n \to Y$ in probability such that $B \subset \{X_n = Y_n\}$ for all $n$. Is $B \setminus \{X=Y\}$ a null set?,"Let $(\Omega, \mathcal F, \mathbb P)$ be a complete probability space. Let $X, Y, X_n, Y_n:\Omega \to \mathbb R$ be random variables such that $X_n \to X$ in probability and $Y_n \to Y$ in probability. Let $B \in \mathcal F$ such that $$
B \subset \{X_n = Y_n\} \quad \forall  n\in \mathbb N.
$$ Is $B \setminus \{X=Y\}$ a null set? My attempt We have $$
\mathbb P [ B \setminus \{X=Y\} ] \subset \mathbb P [ \{X_n = Y_n\} \setminus \{X=Y\} ] = \mathbb E [1_{ \{X_n - Y_n =0\} \setminus \{X-Y=0\}}].
$$ Convergence in probability implies almost sure convergence of a subsequence. We assume $X_{n_k} \to X$ a.s. and $Y_{n_k} \to Y$ a.s. for some subsequence $(n_k)$ . I don't know the point-wise convergence of $1_{ \{X_n - Y_n =0\} \setminus \{X-Y=0\}}$ .","['convergence-divergence', 'probability-theory', 'random-variables']"
4662736,"Why do sometimes , the answers I derive from equations not actually provide solution in inverse trignometry?","This is the question: Solve for $x$ $$\arcsin(1-x)-2\arcsin(x) = \frac{\pi}{2}$$ I solved this by these steps: $\arcsin(1-x) = \frac{\pi}{2} + 2\arcsin(x)$ $\sin$ function on both sides $1-x=\sin(\frac{\pi}{2} + 2\arcsin(x))$ $\implies 1-x = \cos(2\arcsin(x))$ $1-x = 1-2\sin^{2}(\arcsin(x))$ $\implies 1-x=1-2x^{2}$ Solving this we would get either $x=0$ or $x=1/2$ but when we substitute $1/2$ in the original question, it does not give out the answer of $\frac{\pi}{2}$ . Why does this happen?","['trigonometry', 'inverse-function']"
4662752,"$(-1,0,1)$-square matrix has different line sums?","Let $A$ be a $n\times n$ matrix with coefficients from the set $\{-1,0,1\}$ . Let $r_i$ and $c_i$ denote the sum of the elements of the $i$ -th row and column of $A$ respectively. For which $n$ is it possible that the numbers $r_1,...,r_n,c_1,...,c_n$ be different pairwise? If $n$ is even, I am able to construct such a matrix, which I call $A_n$ . $$
A_2=\begin{pmatrix}1&1\\-1&0\end{pmatrix}\\
A_4=\begin{pmatrix}1&1&1&1\\-1&1&1&1\\-1&-1&0&1\\-1&-1&-1&0\end{pmatrix}\\
A_{n+2}=\begin{pmatrix}1& \begin{matrix}1&\cdots&1\end{matrix} &1\\ \begin{matrix}-1\\\vdots\\-1\end{matrix} &A_n&\begin{matrix}1\\\vdots\\1\end{matrix}\\-1&\begin{matrix}-1&\cdots&-1\end{matrix}&0\end{pmatrix}
$$ Not hard to check that these matrices have all different line (row or column) sums. However, if $n$ is odd, it seems impossible to construct such a matrix. I'd like to prove that if $n$ is odd, any $n\times n$ matrix with coefficients from the set $\{-1,0,1\}$ has two lines with the same sum of elements. Any ideas on how to proceed?","['matrices', 'number-theory', 'random-matrices', 'block-matrices']"
4662759,Given this weak property is it possible to demonstrate that the difference of expected value is negative?,"Let's assume that we have $X,Y$ as random variables and we have as hypothesis that $$X-\mathbb{E}_x \leq Y-\mathbb{E}_y$$ where $\mathbb{E}_x$ is the expected value of x. Is it possible to demonstrate the following sentence? $$ P(X>l) \leq P(Y>l)$$ A classic result is obtained when $X\leq Y$ , but I was wondering if it is possible to obtain using the previous equation. If no, can you give me a counter example? What if we restrict $X,Y$ to be only positive random variables?","['statistics', 'expected-value', 'probability-theory', 'probability', 'random-variables']"
4662793,Prove that Kahler form induced by Fubini-Study metric on $\Bbb{P}^n$ is the generator of the $\Bbb{Z}$ coefficient cohomology,"I try to prove that the Kahler metric induced from the Fubini-Study metric on $\Bbb{P}^n$ is a generator of the cohomology $H^2(\Bbb{P}^n,\Bbb{Z})$ My attempt first by the Poincare duality we know that integral induce an isomorphic between $$H^2(X,\Bbb{R}) \to \Bbb{R}$$ while the integral $\int \omega_{\text{FS}} = 1$ it will be the generator for the $\Bbb{R}$ coefficient cohomology. Secondly,I claim that $[\omega_{\text{FS}}] \in H^2(\Bbb{P}^n,\Bbb{Z})$ ,   since the image of the line bundle $\mathcal{O}(1)$ under the first Chern class is $$c_1(\mathcal{O}(1)) = [\omega_{\text{FS}}]$$ it has to lie in $H^2(\Bbb{P}^n,\Bbb{Z}) \cong \Bbb{Z}$ (which is torsion-free). Is it possible to deduce that it's the generator of $H^2(\Bbb{P}^n,\Bbb{Z})$ without involving results from algebraic topology, only based on the observations above?","['complex-analysis', 'algebraic-topology', 'differential-geometry']"
4662803,First chern class of canonical line bundle on $CP^n$,"I am trying to calculate the first chern class $c_1(K)$ of the canonical bundle $K = \Lambda^n(T^*\mathbb{CP}^n)^{1,0}$ , where my definition of the first chern class is $c_1(K)=\frac{i}{2\pi}[F(A)] \in H^2_{dR}(\mathbb{CP}^n)$ for any connection $A$ and its curvature $F(A)$ . First, we know $K = O(-n-1)$ . Cover $X=\mathbb{CP}^n$ by $U_i$ , the affine patch with nonzero $i$ 'th coordinate. We define $h_i = (1+\sum_{j \neq i} |z_j/z_i|^2)^{-n-1}$ , which patch together to give a fibrewise Hermitian inner product on $O(-n-1)$ . i.e., under the trivialization $U_i$ , if $e_i$ is the section corresponding to $1 \in \mathbb{C}$ , and $h=|e_i|^2$ , then by direct calculation $|s_ie_i|^2=|s_je_j|^2$ for all sections $s_i$ , where $s_j = (z_j/z_i)^{n+1} s_i$ are the transition functions of $O(-n-1)$ . Then we know that $A = \partial \log h$ patches together to give a connection, and hence $\frac{i}{2\pi}F(A) = \frac{i}{2\pi}\bar{\partial}\partial \log h \in \Lambda^{1,1}T^*X$ gives a representative of $c_1$ . Note that the Fubini-study form is $\omega|_{U_i} = \frac{i}{2\pi} \partial \bar{\partial} \log (1+\sum_{j\neq i} |z_j/z_i|^2)$ , so we see that $\frac{i}{2\pi}F(A) = (n+1) \omega$ (using $\partial \bar{\partial} = -\bar{\partial}\partial$ ). Hence, $c_1(K)$ can be represented by a positive (1,1)-form, since $\omega$ is a positive (1,1)-form. However, I was expecting that $c_1(X) = -c_1(K)$ can be represented by a positive $(1,1)$ form (i.e., $\mathbb{CP}^n$ is a Fano manifold) instead, which contradicts the above. (A real (1,1) form $\omega$ is a positive if $-i\omega(a,\bar{a}) >0$ for all nonzero $a \in T^{1,0}X$ .) What's the mistake in this argument?","['complex-geometry', 'kahler-manifolds', 'differential-geometry']"
4662810,Is it possible to recover the Cartan-Leray Spectral Sequence for Group Cohomology from the Leray Spectral Sequence for Sheaf Cohomology?,"Let $G$ be a discrete group acting freely and cellularily on a CW-complex $X$ . I am interested in the Cartan-Leray spectral sequence from Eilenberg and Cartan's Homological Algebra, Theorem XVI.8.4, which goes $$E^2_{p,q}=H^p(G,H^q(X;M))\Rightarrow H^{p+q}(X/G;M)\,,$$ where $M$ is a left $G$ -module intepreted as a local coefficient system on $X$ rsp. $X/G$ . The proof presented there is quite explicit, and I was wondering if there is a more abstract way to prove this using the Leray spectral sequence : For a continous map $f:A\to B$ and a sheaf $\mathcal F$ on $A$ , there is a spectral sequence $$E^2_{p,q}=H^p(B;R^q(f_\ast)(\mathcal F))\Rightarrow H^{p+q}(A;\mathcal F)\,.$$ My idea was to construct a map $f:X/G\to K(G,1)$ , where $K(G,1)$ is an Eilenberg-MacLane complex for $G$ , and set $\mathcal F:=\underline{M}_{X/G}$ . Then $H^{p+q}(A;\mathcal F)=H^{p+q}(X/G;M)$ , which is good. But $$H^p(B;R^q(f_\ast)(\mathcal F))=H^p(K(G,1);\operatorname{Sheafification of}H^q(f^{-1}(-);M))\,,$$ of which I am not sure why it should equal $H^p(K(G,1);\underline{H^q(X;M)}_{K(G,1)})=H^p(G,H^q(X;M))$ . Is there a way to make this work? Did I go wrong somewhere along?","['sheaf-cohomology', 'spectral-sequences', 'abstract-algebra', 'group-cohomology', 'eilenberg-maclane-spaces']"
4662840,How to exponentiate the vector field $v$?,"i didn't succed to exponentiate these vector field $$\boxed{v=\sqrt{x}\sqrt{y}\frac{\partial}{\partial x}-\sqrt{x}\sqrt{y}\frac{\partial}{\partial y}+2b\sqrt{x}\sqrt{y}u\frac{\partial}{\partial u}}$$ where $x,y,t$ are independents variables and $u$ is an dependant variable .
this vector field is one of the basis element of a Lie algebra of symmetry associated to a certain pde .the solution of the pde is of the form $u(x,y,t)$ Remark : i succeded to exponentiate the vector field $$w=\sqrt{x}\sqrt{y}\frac{\partial}{\partial x}-\sqrt{x}\sqrt{y}\frac{\partial}{\partial y}$$ it gives $$\exp \left(\varepsilon w\right)(x,y, t, u)=(\tilde{x},\tilde{y},\tilde{t}, \tilde{u})=\left(\sqrt{x}\sqrt{y}\sin(\varepsilon)+(\frac{x-y}{2})\cos(\varepsilon)+(\frac{x+y}{2}),-\sqrt{x}\sqrt{y}\sin(\varepsilon)-(\frac{x-y}{2})\cos(\varepsilon)-(\frac{x+y}{2}),t,u\right) $$ where $exp : \mathfrak{g}\longrightarrow G$ and $\mathfrak{g}$ is the Lie algebra of $G$ . to exponentiate a vector field of the form $\mathbf{v}=\displaystyle{\sum_{k=1}^{p}\xi_{k}(x,u)\frac{\partial}{\partial x_{k}}+\sum_{i=1}^{q}\phi_{i}(x,u)\frac{\partial}{\partial u_{i}}}$ we solve the system of odes $ \displaystyle{\frac{d\tilde{x_{i}}}{d\varepsilon}=\xi_{i}(\tilde{x_{i}})\,\,,\,\, \frac{d\tilde{u}}{d\varepsilon}=\phi(\tilde{x},\tilde{t},\tilde{y})}$ with initial conditions $\tilde{x_{i}}(0)=x_{i}$ , $\tilde{u}(0)=u$ Or we can just develop this Lie series for the flow, given by $$
\exp (\varepsilon \mathbf{v}) \cdot x = x+\varepsilon \xi(x)+\frac{\varepsilon^2}{2} \mathbf{v}(\xi)(x)+\cdots=\sum_{k=0}^{\infty} \frac{\varepsilon^k}{k !} \mathbf{v}^k(x)
$$ where $\xi=\left(\xi^1, \ldots, \xi^m\right), \mathbf{v}(\xi)=\left(\mathbf{v}\left(\xi^1\right), \ldots, \mathbf{v}\left(\xi^m\right)\right)$ , I tried to develop the serie but it's getting more and more complicated from order 3 i guess Thanks for the help !:)","['exponentiation', 'lie-algebras', 'vector-fields', 'lie-groups', 'differential-geometry']"
4662856,Axioms for finite cyclic groups,"tl;dr: What is a minimal definition of a finite cyclic group, without resorting to the definition of a group? Let $G$ be a finite set and $\star$ be a binary operation on $G$ . My question is whether a cyclicity condition can imply some of the standard group axioms (as it implies for instance abelianity). Let us take a following cyclicity axiom: $$\exists g\in G, \forall x\in G, \exists n\in ℤ_{>0}, x = g^n$$ where $g^n = g\star g\star\dotsb\star g$ ( $n$ times).
Already, this cyclicity condition assumes associativity to make sense of $g\star\dotsb\star g$ . This implies that $g^{m+n} = g^m\star g^n$ for $m$ , $n\inℤ_{>0}$ . Note that I do not define $g^n$ for $n = 0$ or $n < 0$ that would assume identity or the existence of inverses. Some thoughts about the other (standard) axioms: Closure : It seems required to me, as some kind of a converse to cyclicity (for all $n >0$ , $g^n\in G$ ). This could be paired with cyclicity by writing something like: ""a finite set $G$ is a cyclic group if $G = \{g^n:n\inℤ_{>0}\}$ for some $g\in G$ ."" Identity : Since $G$ is finite, there must exist $m < n$ such that $g^m = g^n$ . With only the other axioms, can we prove that there exists $k$ such that $g^k = id$ (with standard definition for identity)? Inverses : If we assume identity, we can give a sense to $g^0$ and prove that for all $x\in G$ , there exists $0 ≤ n < k$ such that $x = g^n$ . Then assuming closure, $g^{k-n}\in G$ and $g^{k-n}$ is the inverse of $x$ . My impression is that closure is required, inverses are not and I do not know for identity. But there may exist other ways to define finite cyclic groups.","['group-theory', 'definition', 'finite-groups', 'cyclic-groups']"
4662873,How to prove this asymptotic formula:$J_n=\int_{0}^{1}(1-x^2+x^3)^ndx\sim \sqrt{\frac{\pi}{4n}}$,"How to prove this asymptotic formula: $$J_n=\int_{0}^{1}(1-x^2+x^3)^ndx\sim \sqrt{\frac{\pi}{4n}}$$ My idea is to solve this $J_n=\int_{0}^{1}(1-x^2+x^3)^ndx$ .Is it feasible to calculate its recursion formula or general term formula？Or maybe there is a relationship between $J_n=\int_{0}^{1}(1-x^2+x^3)^ndx$ and $\sqrt{\frac{\pi}{4n}}$ which I need to find out?
Any idea is ok. This problem is important to me. I really need your help.Thank you very much.","['integration', 'calculus', 'real-analysis']"
4662902,"Find the possible values for $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}]$ and provide an example of $(\alpha,\beta)$ for each possible value","Let $\alpha, \beta \in \mathbb{C}$ s.t. $[\mathbb{Q}(\alpha):\mathbb{Q}] = [\mathbb{Q}(\beta):\mathbb{Q}] = 4$ Find the possible values for $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}]$ and provide an example for each possible value. We know that $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}] \leq [\mathbb{Q}(\alpha):\mathbb{Q}] [\mathbb{Q}(\beta):\mathbb{Q}] = 4 \cdot 4 = 16$ On the other hand, $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}] = [\mathbb{Q}(\alpha,\beta):\mathbb{Q}(\alpha)][\mathbb{Q}(\alpha):\mathbb{Q}] = k \cdot 4 = 4k$ We got that $4|[\mathbb{Q}(\alpha,\beta):\mathbb{Q}]$ and $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}] \leq 16$ , implying the possible values are in the set $\{4,8,12,16\}$ These are the examples I have $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}] = 4$ Take $\alpha = \beta = \sqrt[4]{2}$ Here we have that $m_\alpha(\mathbb{Q}) = x^4 - 2$ , then $[\mathbb{Q}(\alpha):\mathbb{Q}] = 4$ . Since $\beta  = \sqrt[4]{2} \in \mathbb{Q}(\sqrt[4]{2}) = \mathbb{Q}(\alpha)$ , we have $m_\beta(\mathbb{Q}(\alpha)) = x - \sqrt[4]{2}$ , then $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}(\alpha)] = 1$ Therefore, $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}] = [\mathbb{Q}(\alpha,\beta):\mathbb{Q}(\alpha)] [\mathbb{Q}(\alpha):\mathbb{Q}] = 1 \cdot 4 = 4$ $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}] = 8$ Take $\alpha = \sqrt[4]{2}, \beta = \sqrt[4]{2} i$ Here we have that $m_\alpha(\mathbb{Q}) = x^4 - 2$ , then $[\mathbb{Q}(\alpha):\mathbb{Q}] = 4$ . Let $x=\sqrt[4]{2} i$ . Squaring, $x^2 = - (\sqrt[4]{2})^2$ , then $x^2 + (\sqrt[4]{2})^2 = 0$ . Since $(\sqrt[4]{2})^2 \in \mathbb{Q}(\sqrt[4]{2}) = \mathbb{Q}(\alpha)$ , we have $m_\beta(\mathbb{Q}(\alpha)) = x^2 + (\sqrt[4]{2})^2$ , then $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}(\alpha)] = 2$ . Therefore, $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}] = [\mathbb{Q}(\alpha,\beta):\mathbb{Q}(\alpha)] [\mathbb{Q}(\alpha):\mathbb{Q}] = 2 \cdot 4 = 8$ $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}] = 16$ Take $\alpha = \sqrt[4]{2}, \beta = \sqrt[4]{3}$ Here we have that $m_\alpha(\mathbb{Q}) = x^4 - 2$ , then $[\mathbb{Q}(\alpha):\mathbb{Q}] = 4$ . Since $\beta = \sqrt[4]{3} \notin \mathbb{Q}(\sqrt[4]{2}) = \mathbb{Q}(\alpha)$ , we have that $deg(m_\beta(\mathbb{Q}(\alpha))) > 1$ . Clearly $\beta = \sqrt[4]{3}$ solves $x^4 - 3 = 0$ . We now look for possible quadratic or cubic factors on $\mathbb{Q}(\sqrt[4]{2})[x]$ . $x^4 - 3 = (x^2 - \sqrt{3})(x^2 + \sqrt{3})$ . But $\sqrt{3} \notin \mathbb{Q}(\sqrt[4]{2})$ . $x^4 - 3 = (x-\sqrt[4]{3})(x^3+\sqrt[4]{3}x^2+(\sqrt[4]{3})^2x+(\sqrt[4]{3})^3)$ . But $\sqrt[4]{3} \notin \mathbb{Q}(\sqrt[4]{2})$ . This implies $m_\beta(\mathbb{Q}(\alpha)) = x^4 - 3$ , then $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}(\alpha)] = 4$ . Therefore, $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}] = [\mathbb{Q}(\alpha,\beta):\mathbb{Q}(\alpha)] [\mathbb{Q}(\alpha):\mathbb{Q}] = 4 \cdot 4 = 16$ I got this problem a while ago in an Abstract Algebra II midterm, on which the professor later decided that the $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}] = 12$ example wasn't required to earn full credit, given its extreme difficulty level. I would like to know an example for $[\mathbb{Q}(\alpha,\beta):\mathbb{Q}] = 12$ , for the sake of curiosity.","['field-theory', 'minimal-polynomials', 'abstract-algebra', 'polynomials', 'extension-field']"
4662922,Does a bijective function exists behind every recurrence relation?,"Consider these 2 questions where recurrence relations can be applied: Q1) Given an (nxm) where n denotes rows and m denotes columns of a grid, find the number of unique paths ( $a_{n,m}$ ) that goes from the top left corner of the grid to the bottom right corner of the (nxm) grid. Rule: Can only move downwards or rightwards when travelling on the grid For Q1) the solution is as follows: Base Cases: when n or m is equals to 1 (i.e. $a_{1,m} =1 $ and $a_{n,1} = 1$ ) Considering the last action: There are 2 cases - 1) moving downwards 2) moving rightwards to reach the bottom left corner of the grid. Hence, the recurrence relation is of the form: $a_{n,m} = a_{n-1,m} + a_{n,m-1}$ . But notice here that there is a 1-1 correspondence between $a_{n,m}$ and ( $a_{n-1,m} + a_{n,m-1}$ ) -- for every unique path in $a_{n-1,m}$ we move 1 grid downwards and for every unique path in $a_{n,m-1}$ we move 1 grid rightwards - doing so will create a 1-1 correspondence with the unique paths in $a_{n,m}$ . .
. Q2) The ""Tower of Hanoi"" Problem where the recurrence relation is of the form: $a_n = 2a_{n-1} + 1$ where $a_n$ denotes the minimum number of steps needed to move n disks from one bar to another bar .
. Although Q1) seems to have a 1-1 function underneath its recurrence relation, I am wondering whether the same can be argued for the ""Tower of Hanoi"" problem as I am not able to think of it..","['functions', 'recurrence-relations', 'discrete-mathematics']"
4662948,non-decreasing surjective map from reals to rationals [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I have a feeling that no such map exists, i.e., there is no non-decreasing surjective function from $\mathbb{R}$ to $\mathbb{Q}$ . But I am just unable to write an argument. Any hint or sketch of proof will be helpful.","['functions', 'real-analysis']"
4662956,"Two Quadratics, the roots of one are the coefficients of the other and vice versa.... can they exist?","I spent hours on this... feel pretty pathetic.  I read this on an old AMATYC exam: Let $\quad c, \quad$ and $\quad d\quad$ be the roots of $\quad x^2+ax+b$ and let $\quad a, \quad$ and $\quad b\quad$ be the roots of $\quad x^2+cx+d$ What is the value of $$a+b+c+d$$ Somehow, I am to reason that this value is $\quad -2$ I do realize that this must mean that $$\begin{cases}
a&=-(c+d) \\
b&=cd \\
c&=-(a+b) \\
d&=ab \\
\end{cases}$$ would have a solution, but I cannot show one exists, can you find an example? ... the first and third equations alone imply that you can set two of the four variables free: $$\begin{pmatrix}1&0&1&1 \\ 1&1&1&0\end{pmatrix}\cdot \begin{pmatrix}a\\b\\c\\d\end{pmatrix}=0 \quad \sim \quad\begin{pmatrix}1&0&1&1 \\ 0&1&0&-1\end{pmatrix}\cdot \begin{pmatrix}a\\b\\c\\d\end{pmatrix}=0$$ so that $$\begin{cases}
a&=-c-d \\
b&=d \\
c&=c \\
d&=d\\
\end{cases}$$ perhaps new letters?... kidding but: $$x^2-(r_1+r_2)x+r_1r_2 \qquad \text{vs}\qquad x^2+(r_1+r_2-r_1r_2)x-r_1r_2(r_1+r_2)$$ works only one way: the roots of second are the coefficients of the first alright, but not the other way around.","['algebra-precalculus', 'quadratics', 'polynomials']"
4662966,What are all $L^pL^q$ estimates for the heat equation (with gain of derivatives)?,"The heat equation and the heat kernel. Consider the heat equation on $\mathbb R$ : $$ \left\{\begin{aligned}u_t-\Delta u&=f\\u(0,x)&=0\quad\forall x\in\mathbb R. \end{aligned}\right. $$ It is well-known that under suitable hypotheses there exists a unique solution $u$ given by the Duhamel formula (see below). So, we consider the operator $\Gamma:=(\partial_t-\Delta)^{-1} $ which maps $f$ to the solution $u$ , that is, $u=\Gamma f$ . Given the heat kernel $$ G(t,x)=\frac{1}{(4\pi t)^{1/2}}e^{-\frac{x^2}{4t}}, $$ one can show that $u$ is given as a space-time convolution between $G$ and $f$ : $$ u(t,x)=\left(\Gamma f\right)(t,x)=\int_0^\infty\int_{\mathbb R^d} G(t-s,x-y)f(s,y)\,\,dyds. $$ The problem. Problem. Given $s\in[0,2]$ , find all quadruples $(p,q,r,\sigma)$ such that the
following estimate holds for all test functions $f\in\mathcal
 D((0,\infty)\times\mathbb R)$ : $$ \||\partial_x|^s\Gamma f
 \|_{L^rL^\sigma}\leq C\|f\|_{L^pL^q}. \qquad (1)$$ The norm in the space $L^pL^q$ is given by $$ \|f\|_{L^pL^q}^p:=\int_0^\infty\|f(s,\cdot)\|^p_{L^q}ds, $$ so it's the classical mixed $L^p-L^q$ norm in which you take first the $L^q$ norm in the $x$ variable and then the $L^p$ norm in the $t$ variable. The fractional derivative $|\partial_x|^s$ is defined via the Fourier transform. Basically, the problem is to find all the 'Strichartz estimates' of the heat equation with fractional gain of derivatives. Note that this is not about the fractional heat equation: we simply want to prove estimates on the derivatives of the solution to the classical heat equation. Considerations and known estimates. By scaling considerations, one immediately finds that the coefficients must satisfy $$ \left(\frac{1}{r}+\frac{1/2}{\sigma}\right)=\left(\frac{1}{p}+\frac{1/2}{q}\right)-1+\frac{s}{2}. \qquad (2)$$ Thus, in what follows I will always assume $(2)$ . I will also assume $p\leq r$ and $q\leq\sigma$ (essentially because convolution with a given function maps $L^p$ to $L^q$ with $q\geq p$ ). Preliminarly, I state without proof the following estimates for the heat kernel: $$ \||\partial_x|^sG(t,\cdot)\|_{L_x^p}\lesssim |t|^{-\frac{1}{2}(1+s)+\frac{1}{2p}}, \quad s\geq 0,\,\,1\leq p\leq \infty. \qquad (3)$$ What I already know is the following: The estimate holds in the cases $$ s\in\{0,1\},\quad 1\leq q\leq \sigma\leq \infty,\quad 1<p<r<\infty. $$ This follows from a direct use of the Hardy-Littlewood-Sobolev inequality and the estimates $(3)$ for $s=0$ and $s=1$ (see this book ). This is not in the book, but with the same proof, using the full range of estimates $(3)$ , one covers all the cases $s\in[0,2)$ with the same conditions on $p,q,r,\sigma$ as above. The estimates hold in the case $(r,\sigma)=(\infty,2)$ with $1\leq q\leq 2$ . This follows by integrating the heat equation against $u$ and perform standard energy estimates. What is left is a lot of endpoint cases (most prominently, $s=2$ , $p=r$ , $p=1$ , and $r=\infty$ ). Finally, my question. My question is about the case $r=\infty$ , $1<\sigma<\infty$ and $s\in(0,2)$ . It seems that energy estimates do not help for the full range of exponents $(p,q)$ and work only for certain values of $\sigma$ anyway. Do you know a way to cover all cases? I am reasonably sure that one can invert the roles of $x$ and $t$ and use Hardy-Littlewood-Sobolev, but that would work only for $p\leq q$ . I am especially interested in the case $(r,\sigma)=(\infty,2)$ , $q=2$ for $s\in(0,2)$ (which would basically cover all other cases $(p,q)$ by Sobolev embedding), or even $q<2$ if the case $q=2$ is too hard. Even the case $s=1$ is not clear to me at the moment, except when $(p,q)=(2,2)$ . Besides my specific question, it would be nice to collect some references that address the estimate in all known ranges of exponents (unless you can think of a paper which treats all these estimates at once, which would be very appreciated). So, if you have a reference for any of the remaining ranges of exponents, even for higher dimensions, that would be very appreciated. Related posts and references: A previous post with a similar question (basically the case $s=0$ ) Something about the case $s=2$","['heat-equation', 'real-analysis', 'lp-spaces', 'functional-analysis', 'partial-differential-equations']"
4662974,An analog of Jordan curve theorem for various type of smooth manifolds,"I would like to know whether the following tentative generalization of the Jordan curve theorem in higher dimensions and for smooth manifolds are true, and in case I ask for references proving them. Let $M$ be an orientable connected smooth n-manifold ( $n>1$ ) without boundary. Suppose $\iota: X_{n-1}\rightarrow M$ is a compact connected orientable $n-1$ dimensional submanifold. I expect the following to be true: If the map induced in homology $\iota _* : H_{n-1}(X_{n-1})\rightarrow H_{n-1}(M)$ is trivial, then $M-X_{n-1}$ is the disjoint union of two connected n-manifolds $A$ , $B$ such that $\partial A=\partial B= X_{n-1}$ . For this statement I essentially have a proof: since $X_{n-1}$ is trivial in homology $X_{n-1}=\partial A$ , and then I can define $B=M-\overline{\partial A}$ . Is this obvious that $B$ is connected or there might be some subtlety? Furthermore let us assume $M$ to be also compact. Then I expect the converse to be also true: If $M$ is compact, then $M-X_{n-1}$ is the disjoint union of n-manifolds $A$ , $B$ such that $\partial A=\partial B= X_{n-1}$ if and only if $\iota _* : H_{n-1}(X_{n-1})\rightarrow H_{n-1}(M)$ is trivial. Is this statement true? An obvious counterexample to the second statement if we drop compactness is $\mathbb{R}^2-\left\{0\right\}$ .","['differential-topology', 'algebraic-topology', 'differential-geometry']"
4663089,Convergence of a weighted alternating binomial series,"Consider the alternating series $$S_n = \sum_{k=0}^{n} (-1)^k {n\choose k} a_k$$ where the weights $a_k$ are non-negative, bounded and monotonically decreasing ( $a_{k+1} < a_k$ ). Can it be shown that $\lim_{n \to \infty} S_n = 0$ for any choice of $a_k$ ? What I've worked out so far is that setting $a_k = 1$ gives the simple result of $0$ . Then using Abel's test, we can show that $S_n$ converges, although it doesn't give the limiting value of the series. One method I can think of is to use a polynomial interpolation of the weights $a_k$ , and then make use of the property that $$\sum_{k=0}^{n} (-1)^k {n\choose k} k^l = 0, \quad 0 \leq l < n,$$ but I'm wondering if there is a more elementary proof.","['convergence-divergence', 'binomial-coefficients', 'sequences-and-series', 'real-analysis']"
4663095,Expectation for an $x_i$ in a permutation.,"Any tips on how to approach the next steps of this problem? $X$ is a set of $N ≥ 2$ distinct numbers and let $x_1x_2\cdots x_n$ be a permutation of $X$ . For $i = 2, 3, . . . , n$ we say that position $i$ in the permutation is a step if $x_{i−1} < x_i$ .
We consider the position $1$ as a step.
What is the expected number of steps in a random permutation of $X$ ? First I defined the expectation of $X$ as the sum of the expectation of the random variables for each $x_1+ x_2+\cdots+x_n$ or $\mathbb E[X]= \sum_{i=1}^{n} x_i$ . Now to figure out the expectation of any $x_i$ , this is the part I am having trouble with. Each $x_i$ the preceding $x_i-1$ can either be lower or higher so $\mathbb E[ x_i ]= (1\cdot 1/2)-(0\cdot 1/2)$ or $\mathbb E[x_i]= 1/2$ . Since we have $n-2$ terms then $\mathbb E[X]= (n-2)\cdot (1/2)$ . This reduces so the answer can be shown as $\frac{n-2} 2$ . Is this right? If not please let me know what I did wrong and what the right track is, thanks.","['expected-value', 'probability-theory', 'probability']"
4663127,"Uniform convergence of $\sum_{n=1}^\infty x \sin\frac{1}{x^2n^2}$, $x \in (0,+\infty)$","Consider a series $\sum_{n=1}^\infty f_n(x)$ , where $f_n(x) = x \sin\frac{1}{x^2n^2}$ and $x \in (0, +\infty)$ . If one fix an arbitrary $x \in (0, +\infty)$ , then for sufficiently large $n \in \mathbb{N}$ one would have $$
|f_n(x)| = \left|x \sin\frac{1}{x^2 n^2}\right| < x\frac{1}{x^2 n^2}.
$$ The series $\sum_{n=1}^\infty \frac{1}{xn^2}$ converges for any fixed $x \in (0, +\infty)$ , so $\sum_{n=1}^\infty f_n(x)$ converges pointwise for $x \in (0, +\infty)$ . But does $\sum_{n=1}^\infty f_n(x)$ converge uniformly on $(0, +\infty)$ ? It is clear, that if $x \in [1, +\infty)$ , then $0 < \sin\frac{1}{x^2n^2} < \frac{1}{x^2n^2}, \; \forall n \in \mathbb{N}, $ $$
\left|x \sin\frac{1}{x^2 n^2}\right| < \frac{1}{xn^2} < \frac{1}{n^2},
$$ so $\sum_{n=1}^\infty x \sin\frac{1}{x^2 n^2}$ converges uniformly on $[1, +\infty)$ . But I don't understand what we can do with the series when $x \in (0,1)$ . Any help would be appreciated.","['uniform-convergence', 'analysis', 'real-analysis']"
4663144,Differential Geometry's lecture notes by professor Will Merry,"I have recently got to know these notes from this answer, i.e., Will Merry, Differential Geometry : beautifully written notes (with problems sheets!), where lectures 1-27 cover pretty much the same stuff as the above book of Jeffrey Lee. Lectures 28-53 also center around metrics and connections, but the notion of parallel transport is worked out much more thoroughly than in Jeffrey Lee's book. It's unfortunate that the author passed away and the link (to the notes given in that answer) does not work anymore. I would like to ask if someone keeps these notes and can share them. Thank you so much!","['reference-request', 'differential-geometry']"
4663195,Find the measure of the smallest positive angle $\theta$ in degrees for which $\tan\theta=\frac{\cos25^\circ+\cos85^\circ}{\sin25^\circ-\sin85^\circ}$,"I'm preparing for a math competition, and was stumped by this problem. The original problem is shown below, and the correct answer is $120°$ . I'm posting this here to ask for explanation on how this answer was reached. The equation given is $$\tan\theta={{\cos5°\cos20°+\cos35°\cos50°-\sin5°\sin20°-\sin35°\sin50°}\over{\sin5°\cos20°-\sin35°\cos50°+\cos5°\sin20°-\cos35°\sin50°}}$$ I was able to simplify the numerator using $\cos(x+y)=\cos(x)\cos(y)-\sin(x)\sin(y)$ , and simplify the denominator using $\sin(x+y)=\sin(x)\cos(y)+\sin(y)\cos(x)$ to get the expression given in the title of this question, but I don't know how to go any further, or if this is even the right first step.",['trigonometry']
4663205,Converse to Vitali's Convergence Theorem for a subset of the sigma algebra.,"The converse to Vitali's Convergence theorem, in part, states the following. Given a measure space $(X, \mathcal{A}, \mu)$ , suppose that there is a sequence of real-valued $\mu$ -measurable functions $\{f_n\}_{n=1}^{\infty}$ for which the limit $\displaystyle \lim_{n\to\infty}\int_Af_n\ d\mu$ is finite for every $A\in\mathcal{A}$ . Then, there is an integrable function $g$ for which \begin{equation}
\displaystyle \lim_{n\to\infty}\int_A f_n\ d\mu=\int_Ag\ d\mu \hspace{1in}  (1)
\end{equation} for every $A\in\mathcal{A}$ . Here is my question. The requirement that $\displaystyle \lim_{n\to\infty}\int_Af_n\ d\mu$ be finite for every $A\in\mathcal{A}$ may be difficult to establish sometimes. If $\displaystyle \lim_{n\to\infty}\int_Af_n\ d\mu$ is finite for each set $A$ in some subset $\mathcal{S}$ of $\mathcal{A}$ , will there still exist an integrable function $g$ for which
(1) holds for every set $A\in\mathcal{S}$ ? If this is not the case for every arbitrary subset of $\mathcal{A}$ , is there some setting in which this is the case where $\mathcal{S}$ is a strict subset of $\mathcal{A}$ ? In particular, consider the measure space $(\mathbb{R}^n, \mathcal{B}_n, \lambda_n)$ where $\mathcal{B}_n$ is the Borel algebra on $\mathbb{R}^n$ generated by the open sets (with the usual topology), and $\lambda_n$ is the Lebesgue measure on $\mathbb{R}^n$ . Suppose that $\mathcal{S}$ is the collection of all rectangles in $\mathbb{R}^n$ . Will (1) hold for every rectangle $A\in\mathcal{S}$ ?","['measure-theory', 'real-analysis']"
4663209,Asymptotic behavior of sequence,"I am trying to analyze the sequence $g(n)$ , defined as follows: $g(1)=0$ $g(2)=1$ For $n\geq 2$ , $g(n+1)=g(n)\left(1+\displaystyle{\frac{n}{n-1}\ln\left(\frac{n+1}{n}\right)}\right)$ . A computational experiment suggests that, as $n\rightarrow\infty$ , $\displaystyle{\frac{g(n)}{n}}$ approaches a constant of $\approx 0.758097$ , but I don't know how to prove it.  How would you do so?","['logarithms', 'sequences-and-series']"
4663213,The Definition of Orthogonal Complement,"In Linear Algebra Done Right by Axler, the author defines the orthogonal complement as follows: If $U$ is a subset of a vector space $V$ , then the orthogonal complement of $U$ , denoted by $U^\perp$ , is the set of all vectors in $V$ that are orthogonal to every vector in $U$ : $$U^{\perp}=\{v\in V : \langle v, u\rangle = 0 \quad\forall u\in U\}$$ Then, he says ""for example, if $U$ is a line in $\mathbb{R}^3$ , then $U^\perp$ is the plane containing the origin that is perpendicular to $U$ "".
Below this comment, there is a theorem that says that for any subset $U$ of $V$ , $U\cap U^\perp \subset \{0\}$ . I have some questions and confusions. Is it common to define the orthogonal complement for any subset of $V$ instead of a subspace of $V$ ? From the example he mentioned, if $U$ is a line not going through the origin and $U^\perp$ is the plane containing the origin that is perpendicular to $U$ , I would expect that there would be an intersection of $U$ and $U^\perp$ which is not the origin. But the theorem implies this cannot happen. Graphically, how could one picture $U$ and $U^\perp$ such that $U\cap U^\perp=\emptyset$ ? All of these questions will be answered if we define the orthogonal complement only for a subspace, not for any set. I wonder what his intension was to define it for an arbitrary subset.",['linear-algebra']
4663236,Expectancy and Variance of getting different-colored pairs of marbles from two urns,"I have two urns, each containing $N$ marbles. Urn $A$ has $K_A$ blue marbles and the rest are red, while Urn $B$ has $K_B$ blue marbles and the remaining marbles are red. I then sample pairs of marbles, one from each urn, without replacement. If I repeat this $k \leq N$ times, on average how many pairs of marbles will have mismatched colours? What is the variance of this random variable? My first attempt Let $X$ be the random variable denoting the number of pairs of marbles with mismatched marbles. $$E[X] = \sum_{i=1}^{k}i\cdot P(X=i)$$ where $$P(X=i) = \frac{\# \text{ microstates with exactly $i$ mismatches}}{\#  \text{ all possible microstates}}\,.$$ For the denominator, we may enumerate all possible combinations as in the hypergeometric distribution and use Vandermonde's identity to simplify: $$ \sum_{l} \ {K_A \choose l}{N-K_A \choose k-l}\sum_{m} {K_B \choose m}{N-K_B \choose k-m} = {N \choose k}^2\,.$$ Though it remains to find a similar combinatoric expression for the numerator... My second attempt Let $I_i$ be an indicator variable denoting occurrence of a mismatch in the $i^{th}$ pair of marbles. Note that $X = \sum_{i=1}^{k} I_i$ . Invoking linearity of expectation: $$ E[X] = E[\sum_{i=1}^{k} I_i] = \sum_{i=1}^{k} E[I_i] $$ Since $E[I_i] = P(I_i = 1)$ , $$ 
\begin{align} E[X] &= \sum_{i=1}^{k} P(I_i = 1) = \sum_{i=1}^{k}\sum_{\mathbf{I} \in \{0,1\}^{i-1}}  P(I_i = 1\;|\;\mathbf{I}) \\ &= \sum_{i=1}^{k}\sum_{\mathbf{I} \in \{0,1\}^{i-1}}  P(I_i = 1\;|\;I_{i-1} \land I_{i-2} \dots \land I_{1})P(I_{i-1} \;|\;I_{i-2} \land I_{i-3} \dots \land I_{1})\cdots P(I_1 ) \end{align}$$ This method requires calculating exponentially many conditional probabilities unless there is some recursive relationship (which I'm struggling to derive): $$ 
\begin{align}P(I_1 = 1) &= \frac{K_A}{N}(1-\frac{K_B}{N}) + \frac{K_B}{N}(1-\frac{K_A}{N}) = \frac{K_A}{N}+\frac{K_B}{N}-2\frac{K_A}{N}\frac{K_B}{N}\\
\\
P(I_1 = 0) &= 1 - P(I_1 = 1)\\
\\
P(I_2 = 1\;|\;I_1 = 0) &=\;?\end{align}$$ CORRECTION It seems that $$E[X] = \sum_{i=1}^{k} E[I_i] = k\cdot E[I_1] = k\cdot P(I_1 = 1)$$ But it is not clear to me why it can be argued from symmetry that $E[I_i] = E[I_1]$ ?","['combinatorics', 'probability']"
4663242,The complete sufficient statistics for uniform distribution [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question Given uniform distribution $(0, \theta)$ , we know the complete sufficient statistics is $X_{(n)}$ . We can show it is complete by definition of the completeness. Another uniform distribution is $(\theta, 2\theta)$ . Now the  sufficient statistics is $(X_{(1)}, X_{(n)})$ . It is not complete, however. The uniform $(\theta, 2\theta)$ is a scale family, with standard pdf f(z) ~ uniform (1,2). So if $z_1,...,z_n$ is a random sample from a uniform $(1,2)$ , then $x_1=\theta z_1,..., x_n = \theta z_n$ . So the distribution of $(X_{(1)}, X_{(n)})$ does not depend on $\theta$ . It is thus not complete. My question: why we cannot say uniform distribution $(0, \theta)$ is a scalar distribution with Uniform (0,1)?","['statistical-inference', 'statistics']"
4663253,proof on regular functions on complete connected varieties is constant,"The complete reference is Cor 7.24 of this notes: https://www.mathematik.uni-kl.de/~gathmann/de/alggeom.php We want to prove the statement ""Let $X$ be a connected complete variety, then $\mathcal{O}_X(X)=k$ ."" The proof goes like this: A global regular function $\varphi\in\mathcal{O}_X(X)$ gives a morphism $\varphi:X\to\mathbb{A}^1$ . Then we extend the target to $\mathbb{P}^1=\mathbb{A}^1\cup\{\infty\}$ and the image $\varphi(X)$ of the new map does not contain the point at infinity. But (by the corollary I will write down below), since $X,\mathbb{P}^1$ are varieties and $X$ is complete, we have $\varphi(X)$ must be a complete closed subvariety of $\mathbb{P}^1$ , which is a set of finite points. Then as the image of a connected space, $\varphi(X)$ must be connected, and hence must be a single point. My question is, why does the author extend the target to $\mathbb{P}^1$ ? If we do not do that and stick to $\mathbb{A}^1$ , then we still have the same result since, $\mathbb{A}^1$ is an affine variety hence also a variety so that we are able to use the corollary below. Corollary. Let $f:X\to Y$ be a morphism between varieties, if $X$ is complete then $f(X)$ is a complete closed subvariety of $Y$ . Also, I am not so sure how do we conclude that, the proper closed subsets of $\mathbb{P}^1$ are finite sets.  Because in this case, when we are discussing $V_p(I)$ , we have $I\subseteq k[x_0,x_1]$ . Then we cannot proceed in the same way as we did to $\mathbb{A}^1$ . Any help is appreciated! Thanks in advance.","['algebraic-geometry', 'projective-geometry', 'projective-space']"
4663254,Tight bounds for $L_1$ norm of Hermite polynomial: $\int_{-\infty}^\infty |\operatorname{He}_n(x)| \frac{1}{\sqrt{2 \pi}} \exp(-\frac{x^2}{2} ) dx$,"Is there a closed-form expression (or tight upper bound) for the integral $$C^{(1)}_n =\int_{-\infty}^\infty |\operatorname{He}_n(x)|  \frac{1}{\sqrt{2 \pi}} \exp\left(-\frac{x^2}{2} \right) \mathrm dx$$ Here $\operatorname{He}_n$ is a Hermite polynomial (probabilists' version). Motivation: we know that \begin{align}
C_n^{(2)}=\sqrt{\int_{-\infty}^\infty |\operatorname{He}_n(  x)|^2 \frac{1}{\sqrt{2 \pi}}\exp\left(-\frac{x^2}{2} \right) \mathrm dx }=  \sqrt{ n!}
\end{align} I am curious in how different is $C^{(1)}_n$ from $C^{(2)}_n$ . Using Jensen's inequality, it is not difficult to show that \begin{align}
\frac{C_n^{(1)}}{C_n^{(2)}} \le 1
\end{align} I am curious if there is a more refined bounds, for example, the one that would exactly characterize what \begin{align}
\lim_{n \to \infty} \frac{C_n^{(1)}}{C_n^{(2)}} =?? 
\end{align}","['hermite-polynomials', 'normal-distribution', 'real-analysis']"
4663300,"If we expand the definition of the general quadratic $ax^{2}+bx+c=0$ to include the case $a=0$, can we arrive at a general solution?","Through a simple mathematical substitution, I have stumbled upon an alternative formula for solving a quadratic equation: $$x=\frac{2c}{-b \pm \sqrt{b^{2}-4ac}}$$ (Please refer to my formula derivation here ; this is my first question, so cannot include images yet). I am not aware if this has been mentioned elsewhere. I was just curious for a formula that reduces to the linear solution ( $x=-\frac{c}{b}$ ) for the standard quadratic $ax^2 + bx + c = 0$ , if we allow the case $a=0$ . The standard solution formula $x=\frac{-b \pm \sqrt{b^{2}-4ac}}{2a}$ crashes for $a=0$ because division by zero is not allowed. This alternative formula $x=\frac{2c}{-b \pm \sqrt{b^{2}-4ac}}$ covers the case $a=0$ , wherein it reduces to the linear equation $(bx + c = 0)$ solution ( $x=-\frac{c}{b}$ ) for the negative square root of the discriminant, but the positive square root of the discriminant needs to be ignored/is a problem in that case. What are your thoughts? Can we arrive at a general formula for a quadratic equation which includes all the cases ( $a=0$ and/or $b=0$ and/or $c=0$ )?","['quadratics', 'linear-algebra']"
4663305,Equivalence of different properties,"Let $E \subseteq \mathbb{R}$ . Show that the following are equivalent: (i) $E$ is Lebesgue measurable ; (ii) $|I| \geq m(I \cap E)+m(I \backslash E)$ forall interval $I \subset \mathbb{R}$ of finite lenght; (iii) $E \cap[n, n+1)$ is measurable forall $n \in \mathbb{Z}$ ; (iv) $m([n, n+1) \cap E)+m([n, n+1) \backslash E)=1$ forall $n \in \mathbb{Z}$ . There is only one thing I have to show: (iv) implies (ii), because I have shown that i) and ii) are equivalent. I was thinking about that if (iv) is true for all $n$ if I replace $[n, n+1)$ with $I=\bigcup_{n \in \mathbb{Z}}([n, n+1)\cap I)$ and I write 1 as $[n, n+1)$ is still right and I have the result? Are there other ways?","['measure-theory', 'lebesgue-measure']"
4663315,Two definitions of continuous differentiability,"Recently I have seen two different definitions of continuous differentiability for a real-valued function of several variables $f(\mathbf{x}) = f(x_1,\ldots,x_n)$ on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ : Real-valued function of several variables $f(\mathbf{x}) = f(x_1,\ldots,x_n)$ is said to be continuously differentiable on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ if for each $\mathbf{x}_0 \in E$ the following holds: all first-order partial derivatives of $f$ are defined in some neighborhood $U(\mathbf{x}_0)$ and are continuous at $\mathbf{x}_0$ . Real-valued function of several variables $f(\mathbf{x}) = f(x_1,\ldots,x_n)$ is said to be continuously differentiable on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ if it is differentiable at every point of $E$ and its derivative $f'(\mathbf{x})$ is continuous at every point of $E$ (as vector function). I guess that these two definitions are equivalent if the set $E$ is open (I think this basically follows from Theorem 9.21 from Rudin's Principles of Mathematical Analysis). But they  don't seem to be equivalent if the set $E$ is not open. So which of these two definitions is more correct and popular, especially in the context of mathematical optimization problems ? ( as far as I know, in optimization problems we often have a function $f(\mathbf{x})$ which is continuously differentiable on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ , where the set $E$ is called the feasible set of the problem $\min_{\mathbf{x} \in E} f(\mathbf{x})$ ) Edit. I think there exist two more definitions that seem reasonable. However, I have not seen them in textbooks (for the considered case of $E \subseteq \operatorname{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ ). Real-valued function of several variables $f(\mathbf{x}) = f(x_1,\ldots,x_n)$ is said to be continuously differentiable on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ if there is an open set $U$ , such that $E \subseteq U \subseteq \operatorname{int} \mathrm{dom} f$ and $f \in C^1(U)$ Real-valued function of several variables $f(\mathbf{x}) = f(x_1,\ldots,x_n)$ is said to be continuously differentiable on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ if $f|_{\operatorname{int} E} \in C^1(\operatorname{int} E)$ and all first-order partial derivatives of $f|_{\operatorname{int} E}$ extend continuously up to $E$ .","['partial-derivative', 'optimization', 'multivariable-calculus', 'real-analysis']"
4663398,"Divide n+m different items among n people,each of them must have at least one item and item i cannot be assigned to individual i","Problem:Divide $n+m$ different items $a_1,\cdots,a_n,a_{n+1},\cdots,a_{n+m}$ among n people $A_1,\cdots,A_n$ . Each of them must have at least one item and $a_i$ cannot be divided among $A_i$ . Suppose there are $l(n,m)$ kinds of different methods, and find the counting formula of $l(n,m)$ . The answer in my book is $$l(n,m)=\sum_{k=0}^{n}(-1)^k\left( \begin{array}{c}
	n\\
	k\\
\end{array} \right)(n-k-1)^{n-k}(n-k)^{m+k}$$ This problem is an exercise about the Inclusion–exclusion principle.So my attempt to solve this problem is as follows. Let $S$ be the way to divide n+m items among n people,and each of them must have at least one item. $B_i$ be the way that $A_i$ have $a_i$ . But I don't know how to calculate $|B_{i_1}\cap B_{i_2}\cap\cdots \cap B_{ir}|$ . Then I try to use recurrence relation, because $l(n,0)$ is the $n$ -th derangement number.I worte the recurrence relation: $$l(n,m+1)=n\cdot l(n-1,m)+n\cdot l(n,m)$$ But I don't know how to compute the general formula. The other question is let $m=0$ in the answer, we have $$l(n,0)=\sum_{k=0}^{n}(-1)^k\left( \begin{array}{c}
	n\\
	k\\
\end{array} \right)(n-k-1)^{n-k}(n-k)^{k}$$ But the $n$ -th derangement number is $n!\sum_{k=0}^{n}\frac{(-1)^k}{k!}$ .Are they equal? Why?","['inclusion-exclusion', 'combinatorics']"
4663423,Banach Decomposition Theorem from Fixpoint Theorem,"Let $A$ be a set. Knowing that every monotone set function $F: \mathcal{P}(A)\to\mathcal{P}(A)$ (in the sense that $X\subseteq Y\subseteq A\implies F(X)\subseteq F(Y)$ ) has a fixpoint, prove the Banach Decomposition Theorem:
Let $X,Y$ be sets and $f:X\to Y$ and $g:Y\to X$ arbitrary functions. Then there are disjoint decompositions $X=X_1\cup X_2$ and $Y=Y_1\cup Y_2$ such that $f[X_1]=Y_1$ and $g[Y_2]=X_2$ . Hint: apply the monotone fixpoint theorem to $F(S)=X-g[Y-f[S]] $ . Now, I have been able to prove that $F: \mathcal{P}(X)\to\mathcal{P}(X)$ defined as above is monotone, so it has a fixpoint, call it $S$ . Therefore: $$ S=X-g[Y-f[S]]\implies X=S\cup g[Y-f[S]]\implies X=X_1\cup X_2 \ \text{with} \ X_1=S,X_2=g[Y-f[S]]$$ (Easy to check that $X_1\cap X_2=\varnothing$ .)
Now, to find a decomposition for $Y$ , what I did is: $$X-S=g[Y-f[S]]\implies g^{-1}[X-S]=Y-f[S]\implies Y=Y_1\cup Y_2=f[S]\cup g^{-1}[X-S] $$ I am not sure about that last step. First, can I apply $g^{-1}$ ? Since $g$ is an arbitrary function, its inverse need not be a function (in principle only a relation). Also, I am pretty sure $g^{-1}[g[Y-f[S]]]\neq Y-f[S]$ in general. How can I proceed otherwise?",['elementary-set-theory']
4663440,"Understanding a proof that, if $|a-1|+|b-1|=|a|+|b|=|a+1|+|b+1|$, then the minimum value of $|a-b|$, over distinct reals $a$ and $b$, is $2$.","I saw this epic question in Advanced Problems in Mathematics by Vikas Gupta: If $$|a-1|+|b-1|=|a|+|b|=|a+1|+|b+1|$$ then find the minimum value of $|a-b|$ , where $a$ and $b$ are distinct real numbers. My Solution : So my answer is correct, and I happily turned to the solution page to see what the author has written, and here's the MINDBLOWING solution I found: Despite reading the solution many times, I'm still unable to understand what was the motivation behind taking $f(x)=|x-a|+|x-b|$ and supposing $a>b$ . Also on what basis did he write $f(0)=f(1)=(-1)$ and the remaining steps . Could you please explain the motivation behind each step written by the author? Any help would be appreciated. Thanks! EDIT: I also wanted to add, if you guys know any better ways than the 2 ways given posted by me, I would love to know that as well:)","['reference-works', 'algebra-precalculus', 'functions', 'absolute-value']"
4663466,Find the sides of an AAA triangle inscribed in a circle with known radius,I have all of the angles for a triangle. I want to know the length of each of the sides such that the three vertices will all lie on a circle with a known diameter. How do I do it? (The application is for a multichannel audio workflow where arbitrary microphone placement needs to be re-encoded in standard surround sound format),"['trigonometry', 'geometry']"
4663497,Finding the largest square that can be inscribed inside the astroid curve $x^{\frac{2}{3}}+y^{\frac{2}{3}}=4$,"Finding the largest square that can be inscribed inside the astroid curve $$x^{\frac{2}{3}}+y^{\frac{2}{3}}=4$$ A square is described by four vertices. There will be one vertex of the square in each quadrant. I believe the vertex in the first quadrant will be where the astroid curve intersects the line $y=x$ and thus will have coordinates $(2\sqrt{2},2\sqrt{2})$ . Similarly, the vertices of the square in the other quadrants will be the horizontal/vertical/origin reflections of $(2\sqrt{2},2\sqrt{2})$ Is this correct? How can I solve this question rigorously? Thanks!","['optimization', 'calculus', 'geometry']"
4663552,Question on showing that $\frac{1}{n}\sum_{k=0}^{n-1}U^kf$ converges in norm to the orthogonal projection $Pf$ to the space $\{f\in H: Uf = f\}$,"Edit: The reference I am reading is Yves Coudène's Ergodic Theory and Dynamical Systems , chapter 1, proof of theorem 1.1 on page 5. Let $H$ be a Hilbert space and $U:H\to H$ a bounded linear operator with $\|U\|\leq 1$ . Define $\mathrm{Inv} = \{f\in H\mid Uf = f\}$ . I am currently trying to understand a proof that $\lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n-1}U^k f = Pf$ where $P:H\to \mathrm{Inv}$ is an orthogonal projection. Denote $S_nf := \sum_{k=0}^{n-1}U^kf$ . If $f\in \mathrm{Inv}$ the claim is clear. The proof I am reading says that since $$\big\|\tfrac{1}{n}S_n(f)\big\|^2 = \left<f, \tfrac1nS_n^*\tfrac1n S_nf\right>$$ it suffices to show that for every $f\in \mathrm{Inv}^\perp$ , the sequence $f_n := \frac{1}{n}S_n^*\frac{1}{n}S_n(f)$ converges weakly to $0$ , or that all accumulation points of the sequence $f_n$ are $0$ . Up to this point I am fully with the proof, since if what is said about the limit points of $f_n$ is true then $\left<f, f_n\right>\to 0, n\to \infty$ by e.g. Cauchy-Schwarz. ( Questions: ) But then the mystery begins with the following claim made in the proof: Because they [(the accumulation points)] are in $\mathrm{Inv}^\perp$ , it suffices to prove that they are invariant under $U$ or $U^*$ . 1.) I don't really follow why the accumulation points of $f_n$ are necessarily in $\mathrm{Inv}^\perp$ given everything that has been said so far. But this is potentially only a minor issue if the weak convergence can be concluded without really considering where the limit points are. With this in mind, the following (bold marked part later in this post) confuses me even more: It is quite simple to conclude that for every $h\in H:(I - U^*)\frac{1}{n}S_n^*h = \frac{1}{n}(I - U^*)h$ . The author uses this fact to choose $h = \frac{1}{n}S_nf$ and writes $$\big\|(I - U*)\tfrac1n S_n^*\tfrac1n S_nf\big\|\leq \tfrac1n \|I - U^{*n}\|\big\|\tfrac1n S_nf\big\|\leq \tfrac2n\|f\|\to 0,n\to\infty$$ ( Confusing part: ) and states Convergence in norm implies weak convergence. 2.) I don't understand why this shows what we want, that $f_n$ tends weakly to zero. The norm inequality shows precisely that $\lim_{n\to\infty}(I - U^*)\left(f_n\right) = 0$ . My current understanding is that this would be great if we would know that $\frac{1}{n}S_n^*\frac{1}{n}S_nf$ converges to something, as then we could say that the limit of $\frac{1}{n}S_n^*\frac{1}{n}S_nf$ is invariant under $U^*$ and consequently under $U$ . How can the weak convergence be deduced?","['projection', 'orthogonality', 'hilbert-spaces', 'linear-algebra', 'functional-analysis']"
4663597,Stability of numerical solution of ODE,"I want to solve the ODE \begin{array}{ll}
-u''(x)=f(x) & x\in (0,1) \\
u(0)=g(0) \\
u(1)=g(1)\,  \\
\end{array} with finite differences using $u''(x)\approx \frac{u(x-h)-2u(x)+u(x+h)}{h^2}$ .
To approximate $[0,1]$ I use a grid $T=\{0=x_0<\dots , x_n=1\}$ . This leads to the equations $\frac{1}{h^2}(-u_{i-1}+2u_i-u_{i+1})=f(x_i)$ for $1\leq i\leq n-1$ , $u_0=g(x_0), u_n=g(x_n)$ and the linear system \begin{align}
 A=\frac{1}{h^2}\begin{pmatrix}
1 & 0 & 0 &  &\cdots & 0\\
-1 & 2 & -1 & 0 & \cdots & 0 \\
0 & -1 & 2 & -1 & \cdots & 0 \\
0 & 0 & \ddots & \ddots & \ddots & 0 \\
0 & \cdots & 0 & -1 & 2 & -1 \\
0 & \cdots & 0 & 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
u_0\\
\vdots \\
u_n
\end{pmatrix}=
\begin{pmatrix}
\frac{g(0)}{h^2} \\
f(x_1)\\
\vdots\\
f(x_{n-1})\\
\frac{g(1)}{h^2}
\end{pmatrix}
\end{align} I already showed that $(Av)_i<0 \Rightarrow v_i<0$ for all $i=0,\dots ,n$ .
How can I show that there exists $C>0$ so that $\|u\|_\infty\leq C\big (\|f\|_\infty+\|g\|_\infty\big )$ ?( $u$ is a solution of the linear system)","['stability-in-odes', 'numerical-methods', 'ordinary-differential-equations']"
4663603,Does this limit exist and if it does what is it? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I was playing around with a few values of $k$ between $0$ and $1$ on Wolfram alpha and found that for all of them $$\sum_{n= 1}^\infty \frac{\sin(n)}{n^k}$$ converges and I was wondering about the limit as k goes to 0 from the right.","['limits', 'convergence-divergence']"
4663619,Can every Lie group be realized as conformal group of smooth manifolds,"I was reading a paper Saerens, Rita; Zame, William R. , The isometry groups of manifolds and the automorphism groups of domains , Trans. Am. Math. Soc. 301, 413-429 (1987). ZBL0621.32025 . Here they showed, We prove that every compact Lie group can be realized as the
(full) automorphism group of a strictly pseudoconvex domain and as the (full) isometry group of a compact, connected, smooth Riemannian manifold. Now my question is: Are their any papers or theorems which shows that every lie group can also be realized as the full Conformal Group of some smooth manifold.","['smooth-manifolds', 'conformal-geometry', 'group-actions', 'lie-groups', 'differential-geometry']"
4663620,Is there a canonical coordinate representation for Lie algebras?,"If we are given a Lie algebra of vector fields $\{X_i\}_{i = 1} ^N$ on a manifold $M ^n$ , is it possible
to determine in local coordinates the $X_i$ 's if we know the value of $m := \dim \text{span} \{X_i\}_{i = 1} ^N$ ? As a first example, consider $N= 2 = m$ , and $[X_1, X_2] = 0$ . Then by Frobenius we can find coordinates
s.t. $X_i = \partial_i$ , so in this case it is indeed possible to determine $X_1$ and $X_2$ . If we had instead
assumed that $[X_1, X_2] = X_1$ , Frobenius allows us to represent $X_1 = \partial_1$ and $X_2 = a \partial_1 + b \partial_2$ . The structure equations then show that $a = x ^1 + c(x ^2)$ and $b = b(x
^2)$ . Changing coordinates via \begin{align*}
  x ^1 &= y ^1 + f(y ^2),
  \\
  x ^2 &= g(y ^2)
\end{align*} transforms $X_1$ and $X_2$ into \begin{align*}
  \tilde{X}_1 &= \partial_1,
  \\
  \tilde{X}_2 &= (y ^1 + f(y ^2) + c(g(y ^2)) + f'(y ^2)b(g(y ^2)))\partial_1 + b(g(y ^2))g'(y ^2) \partial_2.
\end{align*} Since $X_1$ and $X_2$ are everywhere independent, $b$ is never zero, so on any small enough interval we can
solve $g'(y ^2) = 1/b(g(y ^2))$ and then find $f$ solving the linear ODE $$
b(g(y ^2))f'(y ^2) + f(y ^2) + c(g(y ^2)) = 0,
$$ implying $\tilde{X}_2 = y ^1 \partial_1 + \partial_2$ . In other words, for $N = 2 = m$ , we can indeed
determine the $X_i$ 's from just the structure equations. However, consider the case $N = 3$ , $m = 2$ , and \begin{align*}
  [X_1, X_2] &= X_3,
  \\
  [X_2, X_3] &= X_1,
  \\
  [X_3, X_1] &= X_2.
\end{align*} By the Frobenius theorem, we can find a coordinate system $(x ^i)$ s.t. $$
X_1 = \partial_1, \quad X_2 = a \partial_1 + b \partial_2, \quad X_3 = c \partial_1 + d \partial_2.
$$ The structure equations $[X_1, X_2] = X_3$ and $[X_1, X_3] = -X_2$ force \begin{align*}
  \partial_1 a &= c, \quad -\partial_1 c = a \Rightarrow \partial_1 ^2
                 a + a = 0,
  \\
  \partial_1 b &= d, \quad - \partial_1 d = b \Rightarrow \partial_1 ^2
                 b + b = 0,
\end{align*} from which we deduce that $a = a_1 \sin(x ^1) + a_2 \cos(x ^1)$ and $b = b_1 \sin(x ^1) + b_2 \cos(x ^1)$ . Plugging this into $[X_2, X_3] = X_1$ yields \begin{align*}
  b_2 \partial_2a_1-b_1 \partial_2a_2-a_1^2-a_2^2&=1,
                                                       \\
 b_2 (\partial_2b_1-a_2)-b_1
   (a_1+\partial_2b_2) &= 0.
\end{align*} One can play around with different guesses for $a_1$ and $b_1$ to see that there are a wealth of solutions to
these equations. Since there are no derivatives $\partial_{x ^i}$ with $i > 2$ , we further have great freedom
in choosing the ''constants of integration'', which only results in even more families of solutions. It is not
obvious how to choose new coordinates which ensure all of these families are equivalent like in the previous
example. Thus, I am left with the following Question What additional information on the $X_i$ 's would be required to single out a canonical solution in general or ensure all solutions are equivalent
(e.g. via a change of variables)?","['vector-fields', 'lie-algebras', 'differential-geometry']"
4663638,Strassmann's thoerem and irrationality measure of certain number,"In this note from Keith Conrad, he explains an interesting application of Strassmann's theorem to the divergence of certain linear recurrence integer sequence. More precisely, the sequence defined as $a_0 = a_1 = 1$ and $a_{m} = 2a_{m-1} - 3a_{m-2}$ satisfies $\lim_{m\to \infty} |a_m| = \infty$ . It seems somewhat easy to prove at first glance (and the actual behavior of $|a_m|$ is exponential), one needs to deal with possible cancellation. The general term is given by $$
a_m = \frac{1}{2} ((1 + \sqrt{-2})^{m} + (1 - \sqrt{-2})^{m}) = \sqrt{3}^m \cos (m \alpha)
$$ where $\alpha = \arctan(\sqrt{2})$ , and if $m\alpha$ is sufficiently close to the odd multiple of $\pi / 2$ , then $\cos(m\alpha)$ could be very close to zero. So one may need to show that $\cos(m\alpha)$ can't be exponentially small with respect to $m$ in some sense, or use $p$ -adic method as in Conrad's note. What I thought is that once we know $\lim_{m\to\infty}|a_m| = \infty$ , this would tell us that $\alpha / \pi$ can't be very close to rational numbers, and may tell something about irrationality measure of $\alpha / \pi$ . I tried some but didn't get anything useful at this moment. Is it possible to deduce some information on the irrationality measure of $\alpha /\pi$ using divergence, at least finiteness or infiniteness?","['number-theory', 'p-adic-number-theory', 'irrationality-measure', 'analysis', 'sequences-and-series']"
4663694,"Is $ \mu(E \times F) \leq \nu(E \times F) \forall (E,F) \implies \mu(A) \leq \nu(A) \forall A \in \mathcal{E} \otimes \mathcal{F} $ true?","Let $(X,\mathcal{E})$ and $(Y,\mathcal{F})$ denote two measurable spaces and let $\mu,\nu$ denote two finite measures on $(X \times Y, \mathcal{E} \otimes \mathcal{F})$ , where $\mathcal{E} \otimes \mathcal{F}:= \sigma(\mathcal{E} \times \mathcal{F})$ . Consider the claim $$
\mu(E \times F) \leq \nu(E \times F) \text{ for all } (E,F) \in \mathcal{E} \times \mathcal{F} \implies \mu(A) \leq \nu(A) \text{ for all } A \in \mathcal{E} \otimes \mathcal{F}.
$$ Question: Is this claim true? Attempt: Define the set $$
M = \{A \in \mathcal{E} \otimes \mathcal{F} \colon \mu(A) \leq \nu(A)\}.
$$ Then we want to show that $M = \mathcal{E} \otimes \mathcal{F}$ . Note that clearly $M \subseteq \mathcal{E} \otimes \mathcal{F}$ . For the other inclusion, we have by assumption that $\mathcal{E} \times \mathcal{F} \subseteq M$ so it follows that $\mathcal{E} \otimes \mathcal{F}= \sigma(\mathcal{E} \times \mathcal{F}) \subseteq \sigma (M)$ and hence it would suffice to show $M$ is a $\sigma$ -algebra. However I am not sure this is the case. I can't seem to show that $M$ is closed under complements and countable unions. In particular say for the union part, we can do the following: $$
\mu(\bigcup_{n \in \mathbb{N}}A_n) \leq \sum_{n \in \mathbb{N}} \mu(A_n) \leq \sum_{n \in \mathbb{N}} \nu(A_n),
$$ but then we cannot go back to $\nu(\bigcup_{n \in \mathbb{N}}A_n)$ . So this is where I am stuck. I have also tried to look at Dynkin's $\pi$ -theorem but at least I could not see how this was useful here. Also I haven't been able to come up with any counterexample yet. As a final remark, I will just need to apply this result for $\mathcal{E},\mathcal{F} = \mathcal{B(\mathbb{R}^d}),\mathcal{B(\mathbb{R}^m})$ but I would like to prove it higher generality if this is indeed possible. If this is not possible, are there any conditions we can impose for it to hold? Any feedback/help is much appreciated!","['measure-theory', 'inequality']"
