question_id,title,body,tags
634865,Conditional Probability Summation Rule Problem,"From Blitzstein, Introduction to Probability (2019 2 edn), Chapter 2, Exercise 25, p 87. A crime is committed by one of two suspects, A and B. Initially, there is equal
evidence against both of them. In further investigation at the crime scene, it is
found that the guilty party had a blood type found in 10% of the population.
Suspect A does match this blood type, whereas the blood type of Suspect B is
unknown. (a) Given this new information, what is the probability that A is the guilty
party? So here is my approach. Let $A$ stands for ""A gulity"", $M$ for ""A matching the blood type"" and $N$ for ""B matching the blood type"". Then $$P(A|M) = P(A|M,N)P(N|M) + P(A|M,N^{C})P(N^{C}|M)$$ $$P(A|M) = \frac{1}{2}\frac{1}{10} + 1\frac{9}{10} = \frac{19}{20}$$ Here it is inferred that the blood type of B is independant of that of A and that if both have the matching blood type they are equally likely to be guilty. Where is the flaw in the application of the summation rule above? The correct answer is $\frac{10}{11}$ .","['probability-theory', 'probability', 'conditional-probability']"
634869,K topology: Examples,"Why would the interval $(-3,1)$ be open in the $k$-topology? (I'm using Munkres). Can I have some other examples of intervals in $k$-topology? What exactly does $(a,b)$ $\cup$ $(a,b)-k$ for $k=\left\{\frac{1}{n}, n\geq1\right\}$ mean?","['general-topology', 'examples-counterexamples']"
634890,Has Prof. Otelbaev shown existence of strong solutions for Navier-Stokes equations? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question Moderator Notice : I am unilaterally closing this question for three reasons. The discussion here has turned too chatty and not suitable for the MSE framework. Given the recent pre-print of T. Tao (see also the blog-post here ), the continued usefulness of this question is diminished. The final update on this answer is probably as close to an ""answer"" an we can expect. Eminent Kazakh mathematician
Mukhtarbay Otelbaev, Prof. Dr. has published a full proof of the Clay Navier-Stokes Millennium Problem. Is it correct? See http://bnews.kz/en/news/post/180213/ A link to the paper (in Russian): http://www.math.kz/images/journal/2013-4/Otelbaev_N-S_21_12_2013.pdf Mukhtarbay Otelbaev has published over 200 papers, had over 70 PhD students, and he is a member of the Kazak Academy of Sciences. He has published papers on Navier-Stokes and Functional Analysis. please confine answers to any actual mathematical error found!
thanks","['partial-differential-equations', 'analysis']"
634912,Integration of a Gaussian multiplied by a Bessel function,"I currently have a hard time figuring out the following integral: Integrate[r*Exp[-r^2/h^2]*BesselJ[0,i*k*r/z],{r,0,a}] I've written it down in the Mathematica typeset and hope you can help me!
Thanks $$\int_0^a r e^{-\frac{r^2}{h^2}} J_0\left(\dfrac{i k r}{z}\right) \, dr$$ I have tried integration by parts, but I cannot get rid of the resulting integral either. I can keep doing integration by parts, but I will also have an integral I cannot solve","['definite-integrals', 'integration']"
634914,Distribution of likelihood ratio in a test on the unknown variance of a normal sample,"EDIT: I have followed up to this discussion with a second question: https://math.stackexchange.com/questions/635567/hypothesis-test-on-variance-of-normal-sample I am preparing for a stat exam and I was trying to derive the distribution of the likelihood ratio statistic for the hypothesis test below. Let $X_1 ... X_{n}$ be a random sample from a $N(\mu,\sigma^2)$ distribution, where $\mu$ is known and $\sigma^2$ is unknown . I want to test the hypothesis $H_0 : \sigma^2 = \sigma_{0}^{2} $ vs. $H_1 : \sigma^2 \neq \sigma_{0}^{2}$ (and, trivially, $\sigma^2 >0$). The generic joint pdf for the n independent random variables (ie. the likelihood function for the random sample) is: $L=\prod_{i=1}^{n} \large(\frac{1}{\sqrt{2\pi\sigma^2}})\cdot e^-\frac{(X_i - \mu)^2}{2\sigma^2}= \large(\frac{1}{\sqrt{2\pi\sigma^2}})^{n}\cdot e^-\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{2\sigma^2}$ Under the null hypothesis, the maximum value taken by $L$ is: $\large(\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}})^{n}\cdot e^-\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{2\sigma_{0}^{2}}$ If we do not constrain $\sigma^{2}$ to be equal to $\sigma_{0}^{2}$, then $L$ is maximised by deriving the maximum likelihood estimator for $\sigma^{2}$, ie $\hat{\sigma}^{2}=\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{n}$ In this case, the maximum likelihood becomes:
$\large(\frac{1}{\sqrt{2\pi\hat{\sigma_{0}}^{2}}})^{n}\cdot e^-\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{2\hat{\sigma_{0}}^{2}}$ Setting these as numerator and denominator, respectively, I get the following likelihood ratio statistic $\Lambda = \LARGE\frac{\large(\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}})^{n}\cdot e^-\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{2\sigma_{0}^{2}}}{\large(\frac{1}{\sqrt{2\pi\hat{\sigma_{0}}^{2}}})^{n}\cdot e^-\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{2\hat{\sigma_{0}}^{2}}}
\\ = \large(\frac{\hat{\sigma_{0}}^{2}}{\sigma_{0}^{2}})^{n/2}\cdot {e^-\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{2\sigma_{0}^{2}}}\cdot{e^\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{2\hat{\sigma_{0}^{2}}}} 
\\ = \large(\frac{\hat{\sigma_{0}}^{2}}{\sigma_{0}^{2}})^{n/2}\cdot {e^-\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{2\sigma_{0}^{2}}}\cdot{e^{\sum_{i=1}^{n} (X_i - \mu)^2\cdot0.5\cdot\frac{n}{\sum_{i=1}^{n} (X_i - \mu)^2}}}
\\ = \large(\frac{\hat{\sigma_{0}}^{2}}{\sigma_{0}^{2}})^{n/2}\cdot {e^-\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{2\sigma_{0}^{2}}}\cdot{e^{0.5}}$ Since the test corresponds to $\Lambda \leq k$ for some constant k, we can write: $\Lambda= \large(\frac{\hat{\sigma_{0}}^{2}}{\sigma_{0}^{2}})^{n/2}\cdot {e^-\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{2\sigma_{0}^{2}}}\cdot{e^{0.5}} \leq k$ And hence: $\large(\frac{\hat{\sigma_{0}}^{2}}{\sigma_{0}^{2}})^{n/2}\cdot {e^-\frac{\sum_{i=1}^{n} (X_i - \mu)^2}{2\sigma_{0}^{2}}} \leq k'$ When I get here, there are two things I do not understand: I do not think that you can also bring ${e^-\frac{\sum_{i=1}^{n} (X_i
 - \mu)^2}{2\sigma_{0}^{2}}}$ to the right side as this is a function of the random sample. Am I missing something? If the previous statement is correct, how do you find the
distribution of the left hand side? Any clarification, link or reference would be extremely helpful.","['statistics', 'statistical-inference']"
634921,Beyond Calculus?? Integral Convergence using Measure Theory & Real Analysis,"$$
\mbox{Does}\quad
\int_{\pi}^{\infty}
{{\rm d}x \over x^{2}\sin^{2/3}\left(x\right)}\quad \mbox{diverge ?}
$$ Is this problem suitable for a calculus class ?. I'm not sure exactly how to solve but I think it requires measure theory. Can someone lead me in the right direction ?.","['definite-integrals', 'measure-theory', 'real-analysis']"
634949,Finding parameterization of a curve,"Let $z=4-2x-2y$ be a plane having a curve $\gamma$ on it. 
The projection of $\gamma$ on $z=0$ is the circle $x^2 + y^2 =1$ . Find a parameterization of $\gamma$ . How can I do it ? I know that the surface is $ x(u,v) = (u,v, 4-2u-2v) $ , and that our curve must be of the form $ \gamma(t) = (u(t), v(t) , 4-2u(t)-2v(t) ) $ . 
After taking $ z=0, x^2+y^2=1 $ we get $3-8v+5v^2 =0 $ and I can't understand how it helps ... Will you please help ? Thanks",['multivariable-calculus']
634956,$2^n=C_0+C_1+\dots+C_n$,Could anyone give me hints for this one? $2^n=C_0+C_1+\dots+C_n$ Thats all I can say and I know tricks like  integrating or differentiating both side and then put $x=1$ or what ever we need. but the given expression seems very critical to me. Thank you for helping.,"['summation', 'algebra-precalculus', 'binomial-coefficients']"
634973,Evaluating $\prod_{n=2}^\infty {n^3-1\over n^3+1}$,"The value of the infinite product $$P = \frac 79 \times \frac{26}{28} \times \frac{63}{65} \times \cdots \times \frac{n^3-1}{n^3+1} \times \cdots$$ is (A) $1$ (B) $2/3$ (C) $7/3$ (D) none of the above I wrote first 6 terms and tried to cancel out but did not get any idea what will be the last term I did one same kind of problem $\prod (1-{1\over k^2}),k\ge2$ whose answer is $1\over 2$ here I  think the answer will be $D$ , none of this?","['sequences-and-series', 'infinite-product']"
634975,"How can one prove the impossibility of writing $ \int e^{x^{2}} \, \mathrm{d}{x} $ in terms of elementary functions?","Can we express $ \displaystyle \int e^{x^{2}} \, \mathrm{d}{x} $ in terms of elementary functions? (Note: Infinite series are not allowed.) If not, then is there a proof that $ \displaystyle \int e^{x^{2}} \, \mathrm{d}{x} $ cannot be written in such a manner?","['exponential-function', 'integration', 'indefinite-integrals', 'elementary-functions']"
634985,"Given integral $\iint_D (e^{x^2 + y^2}) \,dx \,dy$ in the domain $D = \{(x, y) : x^2 + y^2 \le 2, 0 \le y \le x\}.$ Move to polar coordinates.","Given integral $\iint_D (e^{x^2 + y^2}) \,dx \,dy$ in the domain $D = \{(x, y) : x^2 + y^2 \le 2, 0 \le y \le x\}.$ Move to polar coordinates. First of all I tried to find the domain of $x$ and $y$: $0 \le y \le x$ (given) $0 \le x^2 + y^2 \le 2 \implies -y^2 \le x^2 \le 2 - y^2 \implies y \le x \le \sqrt{2 - y^2}$. If I rewrite the integral: $$\int^0_x\int_y^{\sqrt{2-y^2}} e^{x^2 + y^2} \,dx\,dy,$$ I'm getting that $x$ dependent with $y$, and $y$ dependent with $x$. I guess this is why I need to move to polar coordinates. But How can I do it? Thanks in advance.","['multivariable-calculus', 'polar-coordinates', 'integration']"
634986,Central Limit theorem Application on Poisson Distribution,"Suppose that $X_1,\ldots,X_n$ is an iid sample from the Poisson distribution with mean $\lambda$. Use the Central Limit theorem to find $P(|\bar X - \lambda| < 0.1) $ as $n$ goes to infinity. My question is, if $n$ goes to infinity, the variance for $\bar X$ would be zero and it does not make any sense to me.","['statistics', 'central-limit-theorem', 'probability-distributions']"
635016,Is the matrix $R$ in the $QR$ decomposition unique?,"I'd like to know for positive diagonal elements why is $R$  in $QR$ decomposition unique. My guess is it must have something to do with linearly independence of the column of $R$, but then I can think of a property that lead me to uniqueness of $R$.",['linear-algebra']
635031,How does Cauchy's theorem follow from Sylow's theorem?,"Very quickly, Sylow's first theorem says a Sylow $p$ -subgroup of order $p^r$ exists and Cauchy's theorem says if $p \mid |G|$ then there is an element of order $p$ . It's often said that Cauchy's follows easily from Sylow's, but I just don't see it! I don't see why a Sylow $p$ -subgroup must have an element of order $p$ ; why couldn't they all be of order $p^n,\ 2<n<r$ ?","['sylow-theory', 'finite-groups', 'group-theory', 'abstract-algebra']"
635035,Intuition behind proof and verification partially ordered sets,"Hi everyone in the book that I read I have trouble to understand the argument of the proof at the below proposition. There is a lot of point which are  left as exercises, which is great. One of these is the following claim I put all the proposition for the sake of the completeness but not the entire proof. My questions are: Is the proof of the claim correct (I use the hints which the book give us)? What is the intuition behind the claim? I mean, the entire construction of the ""good sets"" is rather artificial to me, what would be a possible motivation for them? I don't quite understand all the approach. Any suggestion, advice, whatever would be great thanks in advance. If someone consider opportune I'd put the entire proof or change the title (if there is one better). Proposition : Let $X$ be a partially ordered set with ordering relation $\le_X $, and let $x_0\in X$. Then there is a well ordered subset $Y$ of $X$ which has $x_0$ as its minimum element, and which has no strict upper bound. Proof: Suppose for the sake of contradiction that every well-ordered subset $Y$ of $X$ which has $x_0$ as its minimum element contain at least one strict upper bound. Using the axiom of choice we can thus assign a strict upper bound $s(Y)$ to each well-ordered subset $Y$ of $X$ which has $x_0$ as its minimum element. Let us define a special class of subsets $Y$ of $X$. We say that a subset $Y$ of $X$ is a good set iff is well-ordered, contain $x_0$ as its minimum element and obey the property $$x=s(\{y\in Y: y<x\})\; \text{for all }\;  x\in Y\backslash \{x_0\}$$ The collection $\Omega:=\{ Y: Y\; \text{is a good set}\;\}$ is non-empty since contains the set $\{x_0\}$. Claim 1 : Let $Y, Y' \in \Omega$, i.e., both are good sets. Then each element in $Y'\backslash Y$ is a strict upper bound for $Y$. Similarly each element in $Y\backslash Y'$ is a strict upper bound for $Y'$. Proof of the Claim 1: Note that $Y\cap Y' \not= \varnothing$ because both contains at least $x_0$. First we'd like to show that for all $x\in Y\cap Y'$ we have the following equality: $$\{y\in Y: y\le x \}=\{y\in Y': y\le x \}=\{y\in Y\cap Y': y\le x \}$$ We may use strong induction for this purpose (this is possible because everyone of the above sets is well-ordered). Suppose that the assertion hold for each $y\in Y\cap Y'$ such that $y<x$, in other words we have the equality $\{y\in Y: y< x \}=\{y\in Y': y< x \}=\{y\in Y\cap Y': y< x \}$. We shall show that also hold when $y=x$. We have $x=s(\{y\in Y: y< x \})=s(\{y\in Y': y< x \})$ because both are good sets. Then the equality of the first two sets hold. So, if $y\in Y,\; y=x$ and $y\in Y,\;y= x$ are equal for $x\in Y\cap Y'$. Thus the claim follows for the third set. (2) Now we wish to show that $Y\cap Y'$ is itself a good set. Note that automatically is a well-ordered set, since every subset of a well-ordered set is well-ordered. And also contain $x_0$ as its minimum element. So, only we have to show that obey the third property. In other words our task is to show $$x=s(\{y\in Y \cap Y': y<x\})\; \text{for all }\;  x\in (Y\cap Y')\backslash \{x_0\}$$ We may assume $(Y\cap Y')\backslash \{x_0\} \not= \varnothing$ since otherwise is a good set and there is nothing to prove. Let $x\in (Y\cap Y')\backslash \{x_0\} $, and we set $\{y\in Y \cap Y': y<x\}$. For the first part of the proof we already known $\{y\in Y \cap Y': y<x\}=\{y\in Y: y< x \}$ for all $x\in (Y\cap Y')\backslash \{x_0\} \subset Y\cap Y'$. Thus $x=s(\{y\in Y: y< x \})=s(\{y\in Y \cap Y': y<x\})$, which shows that the set is a good set. (3) For the above part of the claim we know that $s(Y\cap Y')$ exists. We shall show that if $Y'\backslash Y\not= \varnothing$, then  $s(Y\cap Y')= \text{min}(Y'\backslash Y)$. Similarly with the roles of $Y$ and $Y'$ interchanged. Suppose the set $Y'\backslash Y$ is non-empty, then contains a minimum element because is a well-ordered set. Let a call it for brevity $x_0$. Thus for all $y<x_0$ we have $\{y\in Y': y<x_0 \} \subset Y$ using the minimality of $x_0$. Also is easy to check that $y\in Y \cap Y'$ iff $y<x_0$. So $\{y\in Y': y<x_0 \}=\{y\in Y\cap Y': y<x_0 \}$. Then $x_0=s(\{y\in Y\cap Y': y<x_0 \})$ because as we have shown is a good set. In other words $x_0= \text{min} (Y'\backslash Y) =s(\{y\in Y\cap Y': y<x_0 \})$. It's trivial to show $s(\{y\in Y\cap Y': y<x_0 \}) =s(Y\cap Y') $ this follows immediately for the way in which $x_0$ was specified since $\{y\in Y\cap Y': y<x_0 \}=Y\cap Y'$, one inclusion is obvious and the other follows because if $y\in Y\cap Y' \subset Y$ thus it can't be greater than $x_0$. Note that exactly the same argument applies only with the roles interchanged of $Y, Y'$ when we assumme $Y\backslash Y'\not= \varnothing$. (4) If $Y,Y' \in \Omega$ are good sets. Then either $Y\backslash Y'$ is non-empty or $Y'\backslash Y$ is non-empty but not both at the same time. Suppose for the sake of contradiction that both cases occurs at the same time, i.e., $Y\backslash Y'\not= \varnothing$ and $Y' \backslash Y\not= \varnothing$. Let $x= \text{min} (Y\backslash Y')$ and $x'= \text{min} (Y'\backslash Y)$. We know by (3) that $x= s(Y\cap Y')$ and also  $x'=s(Y\cap Y')$, which would imply that $x=x'$ but this means that $x,x'\in Y\cap Y'$ which is a contradiction. Hence, it is not possible that both holds at the same time. Also we can show that either $Y\subset Y'$ or $Y' \subset Y$, if were not the case, i.e., $Y\not\subset Y'$ and $Y' \not\subset Y$. Then $Y'\backslash Y$ and $Y\backslash Y'$ are both non-empty which as we have shown above leads a contradiction. (5) To conclude the proof of the claim we shall show that all the elements in $Y'\backslash Y$ are strict upper bounds for $Y$. Note it is not necessary shows that the elements in $Y\backslash Y'$ a strict upper bounds for $Y'$ because exactly the same arguments apply. Let $Y,Y' \in \Omega$, so both are good sets.Then either $Y'\backslash Y$ is empty or not. If the set is empty is vacuously true that each of its element is a strict upper bound for $Y$. If were not the case, this meant $Y\subset Y'$ and $Y\cap Y'=Y$ for what shown in (4). Then $s(Y\cap Y')= s(Y)= \text{min}(Y'\backslash Y)$. So, the minimum element of $Y'\backslash Y$ is an upper bound for $Y$ and so, each element in $Y'\backslash Y$ is strictly greater than each element in $Y$ by transitivity, i.e., are strict upper bounds for $Y$. Hence the claim follows  as desired. I think there is an intimate relation of this proposition and the Zorn's lemma. Am I right?","['order-theory', 'self-learning', 'elementary-set-theory', 'proof-verification', 'proof-writing']"
635042,Sequences of sets property,"I'm having trouble to prove the following question: Supose $\{A_n\}_{n\in\mathbb{N}}$ is a family of sets such that $A_1\subset A_2\subset A_3\subset\ldots$ (it's possible to have $A_n=A_{n+1}$). I need to prove that $$\lim_{n\to\infty}A_n = A_1\cup\bigcup_{n=2}^\infty(A_{n}\backslash A_{n-1}).$$ I'm not sure if this is useful, but I proved that $A_{n}\backslash A_{n-1}$ and $A_{n'}\backslash A_{n'-1}$ are disjoint if $n\neq n'$. Thank you.",['elementary-set-theory']
635046,Derivative of $x^a$,"How to calculate the derivative of function $f$ such, that:
$f(x)=x^a$, where $x>0$ and $a\in \Bbb R$. Do you know any formula for $(x+h)^a$?","['derivatives', 'real-analysis']"
635053,Strategies for swapping the order of integration with dependent bounds,"What are the general strategies for swapping the order of integration given dependent bounds?  Specifically, in $\mathbb{R}^2$, Fubini's theorem allows us the following
$$
     \int_{a}^b\int_{c}^d f(x,y) dy dx= \int_c^d\int_a^b f(x,y) dx dy
$$
However, if we have a dependent bound, such as the integral
$$
     \int_a^b\int_{c(x)}^{d(x)} f(x,y) dy dx
$$
the exchange of integrals isn't so simple.  Sometimes, we can fiddle with things algebraically, but I'm interested if there's a general strategy for doing so (some sort of change of variables trick?).  More generally, I'm interested in the measure theoretic case where we have
$$
     \int_A\int_{B(x)} f(x,y) \mu(dy) \lambda(dx)
$$
and $\mu$ and $\lambda$ are measures.  Basically, I'd like to be able to write this as
$$
     \int_{B}\int_{A(y)} f(x,y) \lambda(dx) \mu(dy)
$$
or something similar.","['multivariable-calculus', 'measure-theory', 'calculus', 'integration']"
635066,"Maximum number of edges that a bipartite graph with $n,m$ vertices can have when it doesn't contain $4$-cycle","Let $A_{n,m}$ be the maximum number of edges that a bipartite graph with $n,m$ vertices can have when it doesn't contain $4$-cycle. I have calculated some values: $A_{2,2}=3$, $A_{3,3}=6$, $A_{4,4}=9$, $A_{4,5}=10$, $A_{5,5}=12$. I am trying to find formula for $A_{n,m}$. Does anyone know it or a hint how to find? Especially I need the value of $A_{6,6}$. I only know that $A_{6,6}\ge16$.","['bipartite-graphs', 'graph-theory', 'combinatorics']"
635076,Can we take images of equivalence relations?,"Given a function $f : X \rightarrow Y$, it is well-known that we can take the image under $f$ of any subset $A \subseteq X$, and we can take the preimage under $f$ of any subset $A \subseteq Y$. This is fundamental. A little less well-known is that we can take the preimage under $f$ of an equivalence relation $\cong$ on $Y$. In particular, we define $f^{-1}(\cong)$ to be the unique equivalence relation $\sim$ on $X$ such that the following holds. $$x \sim x' \iff f(x) \cong f(x')$$ It is easy to verify that this is indeed an equivalence relation. Question. If $f : X \rightarrow Y$ is an arbitrary function (so that, in particular, we're not assuming that $f$ is surjective), is it the case that to every equivalence relation $\sim$ on $X$, there is a naturally corresponding equivalence relation $f(\sim)$ on $Y$? Basically, I can see at least two possible ways of doing this. In one approach, every two elements in $Y \setminus f(X)$ is equivalent; in the other, no two distinct elements in $Y \setminus f(X)$ are equivalent. But how do we know which is the ""correct"" definition?","['equivalence-relations', 'elementary-set-theory', 'functions', 'relations', 'category-theory']"
635077,Proof of the angle sum identity for $\sin$,$$\sin(a+b) = \sin(a) \cos(b) + \cos(a) \sin(b)$$ How can I prove this statement?,['trigonometry']
635110,Is $2A^2 \geq AB+BA$ when $A\geq B\geq 0$ ? Always true?,"Let $A$ and $B$ be real (symmetric) and positive definite. It follows that $AB+BA$ is not necessarily positive definite (it can be indefinite, negative definite or positive definite). But now suppose $A\geq B$. Can one always say $2A^2 \geq AB+BA$? Since this ordering implies $2A^2-AB-BA\geq0$ and adding indefinite/neg-def/pos-def to a pos-def matrix may still be pos-def I am supposing the question is well-defined.",['matrices']
635119,How do you calculate the derivative after a change of variables?,"How would you calculate $df \over dθ$ if $f(x,y) = x^2+y^2$ where $x = \sin 2θ$ and $y = \cos 2θ$? I tried Wolfram and using the product rule but I can't seem to get anywhere.","['differential', 'trigonometry', 'calculus', 'derivatives']"
635156,Doubt in the definition of the Fourier transform in $L^{2}(\mathbb R^n)$,"I am trying to understand the definition of the Fourier transform in $L^{2}(\mathbb R^n)$ . I am understand of this manner : Let $f \in L^{2}(\mathbb R^n)$ and $n$ a natural number. Define $f_n = f {\chi}_{B(0,n)}$. The Fourier transform of $f$ is the function $\hat{f}$ given by $$ \hat{f} = \lim_n \hat{f_n}  \ \  \text{(the limit is in $L^2{(\mathbb R^n)}$)}$$ and the  Fourier transfom of $f$ in a point $\xi$ is given by $$ \hat{f}(\xi) = \lim_{n} \int_{B(0,n)} f_n(x)e^{-2 \pi i \langle x,\xi\rangle} \ dx$$ I am right? Thanks in advance","['harmonic-analysis', 'fourier-analysis', 'analysis']"
635171,Show that this is a stopping time,"Show that $\sigma=\inf \{ t\ge 0 : |B_t|= \log t \}$  is a stopping time with
respect to $(\mathcal F_t^B)_{t\ge0}$. I've been trying to put the set $\{\sigma\le t\}$ equal to a countable union and then showing that this union belongs in $\mathcal F_t^B$. I am struggling to derive a countable union to this as I am only familiar with when $B_t$ is equal to a number and not a function of $t$. definitions $(\Omega,\mathcal F, \mathbb P)$ is a probability space.  $(\cal F_t)_{t \ge 0}$ is a stochastic basis: that is, for each real $t \ge 0$, $\mathcal F_t$ is a sigma-algebra contained in $\mathcal F$, and if $s \le t$ then $\mathcal F_s \subseteq \mathcal F_t$.  Perhaps we even assume the stochastic basis is right-continuous , namely
$$
\mathcal F_t = \bigcap_{s>t}\mathcal F_s
$$
for all $t$.  $(B_t)$ is a stochastic process adapted to $(\mathcal F_t)$: that is, for each $t$, $B_t : \Omega \to \mathbb R$ is a random variable, measurable with respect to $\mathcal F_t$.  Define random variable $\sigma$ as above, $\sigma :  \Omega \to [0,+\infty]$.  We want to show $\sigma$ is a stopping time for $(\mathcal F_t)$: that is, for every real $t$,
$$
\{\sigma \le t\} := \{\omega \in \Omega : \sigma(\omega) \le t\}
$$
belongs to $\mathcal F_t$.","['probability-theory', 'stopping-times', 'brownian-motion']"
635185,"If $p\in R[X_1,\dots,X_n]$ is irreducible, is it still irreducible in $R[X_1,\dots,X_n,\dots,X_N]$?","It is a known fact that if $R$ is a UFD, then $R[X_1,X_2,\dots]$ is also a UFD, but there is a subtlety that is making me uncomfortable. The standard approach essentially goes something along the lines of...let $f$ be a polynomial, so it only involves finitely many variables, say up to $X_1,\dots,X_n$. Then since $R[X_1,\dots,X_n]$ is a UFD, $f$ has a factorization into irreducibles. Then $f$ cannot have a factorization involving indeterminates $X_N$ not in $R[X_1,\dots,X_n]$. If it did, then $f$ would have two factorizations in the UFD $R[X_1,\dots,X_n,\dots, X_N]$, a contradiction. This seems to assume that the prime factorization in $R[X_1,\dots,X_n]$ is still a prime factorization in $R[X_1,\dots,X_n,\dots,X_N]$. But how can you see that irreducibles in $R[X_1,\dots,X_n]$ are still irreducibles in $R[X_1,\dots,X_m]$ for $m>n$? This does not seem obvious. Let $R_k=R[X_1,\dots,X_k]$. The explanantion I found is this: Suppose $p\in R[X_1,\dots,X_n]$ is irreducible in $R_n$. Suppose $p=ab$ for $a,b\in R_m$. Evaluating $X_1,\dots,X_n$ at $1$, you get 
$$
\bar{p}=\overline{ab}\in R[X_{n+1},\dots,X_m]
$$
But $\bar{p}\in R$, which implies that $a$ and $b$ do not involve any variables $X_{n+1},\dots,X_m$. So $a,b\in R_n$, and thus one is a unit, hence a unit in $R_m$. The part I don't follow is how evaluation at $1$, implies $a$ and $b$ do not have indeterminates other than $X_1,\dots,X_n$. Isn't it possible that some of the higher indexed indeterminates disappear when you evaluate? Suppose for instance $p\in R_1$, and $a\in R_2$ is $a=-X_1X_2+X_2$. Then evaluting at $1$ gives $\bar{a}=0$ so we may not get the desired contradiction since $\overline{ab}=0\in R$?","['commutative-algebra', 'ring-theory', 'irreducible-polynomials', 'abstract-algebra']"
635188,What is the parity of permutation in the 15 puzzle?,"You might know the 15 puzzle : $\hskip1.4in$ Concerning the solvability, Wiki says: The invariant is the parity of the permutation of all 16 squares plus the parity of the taxicab distance (number of rows plus number of columns) of the empty square from the lower right corner. This is an invariant because each move changes both the parity of the permutation and the parity of the taxicab distance. In particular if the empty square is in the lower right corner then the puzzle is solvable if and only if the permutation of the remaining pieces is even. I don't get what exactly is meant with the parity of the permutation in this special case?","['permutations', 'recreational-mathematics', 'group-theory']"
635195,Find a limit in an efficent way,"I'm trying to calculate the following limit: $$\mathop {\lim }\limits_{x \to {0^ + }} {\left( {\frac{{\sin x}}{x}} \right)^{\frac{1}{x}}}$$ What I did is writing it as: $${e^{\frac{1}{x}\ln \left( {\frac{{\sin x}}{x}} \right)}}$$ Therefore, we need to calculate: $$\mathop {\lim }\limits_{x \to {0^ + }} \frac{{\ln \left( {\frac{{\sin x}}{x}} \right)}}{x}$$ Now, we can apply L'Hopital rule, Which I did: $$\Rightarrow cot(x) - {1 \over x}$$ But in order to reach the final limit two more application of LHR are needed. Is there a better way?","['calculus', 'real-analysis', 'limits']"
635202,"limit of $\frac{2xy^3}{7x^2+4y^6}$, different answers.","Good evening, everyone I've tried my possible best to evaluate the limit as $(x,y) \to (0,0)$ but Using sage the answer is 0 either ways but one textbook is saying the limit doesn't exist.  Could the 2 answers be correct?","['multivariable-calculus', 'real-analysis', 'limits']"
635227,"What are the properties of symmetric, anti-symmetric, and diagonal matrices","I know the definition of each one but I don't know how to answers questions about them, or what their properties are and how I can use them to prove/disprove statements about them. If P, Q, and D are symmetric, anti-symmetric, and diagonal matrices (of the same size) respectively, how would I go about proving if $Q^{2012} + D^{2013} $ is symmetric? Or if $(P + Q)(P - Q)$ is anti-symmetric? For the first part how do I prove that all square diagonal matrices multiplied by square diagonal matrices are still diagonal? And are anti-symmetric matrices still anti-symmetric if multiplied by themselves?","['matrices', 'linear-algebra']"
635229,What is the integrating factor of $2xy'+x^{2}e^{1-x^2}y=2$,"I know how to start solving it, its dividing everything by $2x$, but I can't solve $$\int \dfrac{x^2e^{1-x^2}}{2x}\,dx$$","['ordinary-differential-equations', 'calculus']"
635233,A connection over a 1-dim manifold is flat,"Let $M$ be a 1-dimensional manifold and let $E$ be its vector bundle. I want to show that every connection $D$ on this vector bundle is flat. A connection $D$ is flat means that we have
$$D_v D_w s-D_w D_v s -D_{[v, w]}s=0 $$
for all vector field $v, w$ and section $s$. I am not matured in differential geometry and I don't know where to start. I don't have any idea how to use the condition that $M$ is -dimensional. I appreciate any help. Also, do you have any geometric intuition when you read this kind of sentence? If so, how did you learn that intuition?","['connections', 'vector-bundles', 'differential-geometry']"
635247,Two definitions of Scheme theoretic image,"Suppose $f：X \to Y $ is a morphism. 
I saw two definitions of scheme theoretic image. The first one requires $f$ to be quasi-compact and quasi-separated, or quasi-compact, which ensures the kernel $J=O_Y\to f_*(O_X)$ be quasi-coherent. Thus $J$ defines a closed subscheme of Y. The second one requires nothing, it define one by the quasi-coherent sheaf $\sum J_W$ where $J$ is the defining ideal of $W$, and $W$ runs through all the the closed subschemes $W$ which $f$ factors through. What is there difference?","['algebraic-geometry', 'schemes']"
635259,Using substitution to evaluate indefinite integral $ \int{x\sqrt{4x+1}}dx$,"Evaluate this indefinite integral.
  $$I= \int{x\sqrt{4x+1}}dx$$ Let $u=4x+1$ $$\frac{du}{dx}=4\rightarrow{dx=\frac{du}{4}}$$
$$I=\int{x}\sqrt{u}\frac{1}{4}du=\frac{1}{4}\int{x}\sqrt{u}du$$ Then I got stuck at this point.","['calculus', 'integration', 'indefinite-integrals']"
635266,"Content of the book ""Group Theory Seminar Lectures:1960-1961""","Is there any one who read the book ""Group Theory Seminar Lectures:1960-1961""? Please tell me the content about this book. We know in 60s, there are many important results. N. Blackburn is one of my favorite group theorists. I want to know the content of this book. But there is no information I can find.",['group-theory']
635273,Metric on tangent vectors to tangent space,"Let $M$ be a Riemannian manifold and $p$ be a point of $M$. Let $v$, $v'$ be  tangent vectors to $M$ at $p$. Of course we have $\langle v,v'\rangle_p$ defined. Let $u$, $w$ be tangent vectors to $T_p(M)$ at $v$. How is $\langle u,w\rangle_v$ defined? How is it related to the metric of $M$?",['differential-geometry']
635287,"$W_n=\int_0^{\pi/2}\sin^n(x)\,dx$ Find a relation between $W_{n+2}$ and $W_n$","Set $$W_n=\int_0^{\pi/2}\sin^n(x)\,dx.$$ Compute $W_0$ and $W_1$. Find
  a relation between $W_n$ and $W_{n+2}$ and deduce a formula for $W_n$. What I have so far is: $$W_{2k}=\frac{1}{2^k}\left( \sum_{\substack{0\leq j \leq k \\ j \text{ even }}} \binom{k}{j} W_j \right)\\[30pt]W_{2k+1}= \sum_{j=1}^k \binom{k}{j}\frac{(-1)^j}{2j+1}.$$ How I got this is: $$\int_0^{\pi/2}\sin^{2k}x\,dx=\int_0^{\pi/2}\frac{1}{2^k}\left(1-\cos2x\right)^k = \frac{1}{2^{k+1}}\int_0^{\pi}(1-\cos x)^k$$ the odd powers evaluate to zero, so
$$=\frac{1}{2^{k+1}}\int_0^{\pi}\sum_{\substack{0\leq j \leq k \\ j \text{ even }}}\binom{k}{j}\cos^jx=\frac{1}{2^k}\int_0^{\pi/2}\sum_{\substack{0\leq j \leq k \\ j \text{ even }}}\binom{k}{j}\cos^j x = \frac{1}{2^k}\left( \sum_{\substack{0\leq j \leq k \\ j \text{ even }}} \binom{k}{j} W_j \right).\\[40pt]\int_0^{\pi/2}\sin^{2k+1} x\,dx=\int_0^1(1-u^2)^kdu = \sum_{j=1}^k \binom{k}{j}\frac{(-1)^j}{2j+1}.$$ But I haven't found a relation between $W_n$ and $W_{n+2}$ as such. Any ideas?","['recurrence-relations', 'calculus', 'integration', 'definite-integrals', 'trigonometry']"
635301,Prove that $f(\Bbb C)=\Bbb C$,I need some help with the following problem: Let $f:\Bbb C \to \Bbb C$ be continuous satisfying that $f(\Bbb C)$ is an open set and that $|f(z)| \to \infty$ as $z\to \infty$. Prove that $f(\Bbb C)=\Bbb C$. My idea on this one is to prove by contradiction and assume that $S=f(\Bbb C)\ne\Bbb C$ to get some contradiction with the given two properties of the function. But I have no idea on how to proceed next. Thanks in advance.,['complex-analysis']
635302,Minimum L1 norm may not obtain the sparsest solution?,"Here is a paper called For Most Large Underdetermined Systems of Equations, the Minimal L1-norm Near-Solution Approximates the Sparsest Near-Solution . However, I did not quite get its definition of sparse, and under what conditions, the minimal L1 norm solution is not the sparsest one. Actually I construct a matrix (in Matlab): A = [1     0     1     1     1     0     0     0     0     0; ...
     0     1     1     1     0     1     0     0     0     0;...
     0    1/4    0    7/16   0     0     0     0     0     0;...
    1/3    0    7/9    0     2     0     0     0     0     0;...
    1/9   1/8 119/216  0     0     4     0     0     0     0]; and, b = [9 8 2 3 4]'; According to the paper, the L1 norm is to
$min||x||_1$ subject to $||b - Ax||_2 \leq \epsilon$. Suppose its solution is $\hat x_{\epsilon}$. It also mentioned whenever there exists a sparse solution $x_0$, $Ax_0 = b$, and there are at most $(1+M^{-1})/4$ nonzero elements ($M$ is the maximal coherence between any two columns of $A$), then it satisfies, $||\hat x_{\epsilon} - x_0||_2\leq 3\epsilon$ When I tried to solved such L1 norm linear equation ( L1 magic package ), I got the solution [1 0 3.4286 4.5714 0 0 0 0 0 0]'; Yes this is sparser than L2 norm solution: [2 1 3 4 0 0 0 0 0 0]'; But in fact this is also a solution of the original matrix equation: [9 8 0 0 0 0 0 0 0 0]'; and it is sparser. The reason why L1 norm minimization does not pick this vector as the solution is because its L1 norm is larger ($17$ compared with $9$). Did I miss something? I did not find the definition of sparse in the paper, is it possible that my linear equation construction doesn't meet some of the conditions mentioned in the paper? I look forward to hear a reasonable analysis on what's going on with my simulation.
Thanks.","['matrices', 'normed-spaces', 'linear-algebra', 'convex-optimization', 'numerical-methods']"
635314,Solving a challenging differential equation,How would one go about finding a closed form analytic solution to the following differential equation? $$\frac{d^2y}{dx^2} +(x^4 +x^2+x+c)y(x) =0 $$ where $c\in\mathbb{R}$,"['ordinary-differential-equations', 'analysis']"
635315,Sum of a Hyper-geometric series. (NBHM 2011),"How to find the sum of the following series $$\frac{1}{5} - \frac{1\cdot 4}{5\cdot 10} + \frac{1\cdot 4\cdot 7}{5\cdot 10\cdot 15} - \dots\,.?$$ I have no idea. I have written the general term and tested its convergence by Gauss' test for convergence, but they are neither the question nor the answer.","['sequences-and-series', 'algebra-precalculus', 'real-analysis']"
635324,Checking my understanding: $1 - 1 + 1 - 1 + 1 - ... = \frac{1}{2}$,"I've recently run into a proof that claims that $1 - 1 + 1 - 1 + 1 - 1 ... = \frac{1}{2}$ that proceeds as follows: Let $S = 1 - 1 + 1 - 1 + 1 - 1 + ...$. Then $$S = 1 - (1 - 1 + 1 - 1 + 1 - 1 ...) = 1 - S$$ Therefore, $2S = 1$, so $S = \frac{1}{2}$. QED I was under the impression that this sum doesn't converge, and therefore this step of the proof is invalid: Let $S = 1 - 1 + 1 - 1 + 1 - 1 + ...$ That is, it's not even valid to suppose that the sum is equal to some (implicitly real or complex) number $S$, and therefore the reasoning that follows is meaningless because $S$ doesn't exist and therefore has any properties we want it to have. Is the reasoning I've given above correct? That is, is it correct for me to claim that the proof fails because $S$ doesn't exist? (I've heard that there are techniques for evaluating divergent integrals by using complex analysis and Taylor series, so perhaps there's another way to prove that the summation is $\frac{1}{2}$; I just wanted to see whether my reasoning is sufficient to explain why this particular proof is incorrect. If the proof actually is correct, then I stand corrected!) Thanks!","['divergent-series', 'sequences-and-series']"
635340,How to better spot the right integration by parts,"I was having trouble integrating
$$
\int_0^{\pi/2}\sin^{n}\left(x\right)\cos^{2}\left(x\right)\,{\rm d}x
$$ and someone pointed out to me that it was a somewhat simple integration by parts. Does anyone have some tips for me to better spot integrations by parts? I feel like I often miss them. Do you just proceed by trial and error, looking for $dv$ and $u$? One general tip I heard was in choosing $u$, use LIATE, that is, the best things to make $u$, in order, are 1. Logs 2. Inverse trig functions 3. Algebraic functions 4. Trig functions 5. Exponential. But that's if you already know you have to use integration by parts, and doesn't help you to recognize it. Alternately, if someone has a reference for good practice, I'd love to hear it. Thanks!","['integration', 'soft-question']"
635365,Taking repeated sin then cos,When you keep taking alternating sin and cos of any number as follows: $$\sin(\cos(\sin(\cos(\sin(\cos...(N))))...)$$ it seems to converge at about 0.69. Is there any way to find the exact value it converges at?,"['trigonometry', 'convergence-divergence']"
635379,Calculating the odds of winning a card game,"So my friend made this game and I want to the odds of winning his game. So his game is basically I pay \$$1$ to draw $2$ cards from the deck and guess $2$ numbers and one suit. For each correct guess I get \$$1$. Doubled guesses still only count for one correct guess. So either I lose \$$1$ ($0$ correct guess), break even ($1$ correct guess), profit \$$1$ ($2$ correct guesses) or profit \$$2$ ($3$ correct guesses). To win, I have to profit at least \$$1$ so that means I need to be able to guess either both numbers, or one number and one suit. I was wondering what my odds of winning are or what the odds of all possibilities are.","['statistics', 'probability', 'analysis']"
635389,Finite fiber- unramified morphisms,"I'm in trouble understandig the proof of Proposition 3.2 Chapter 1 of Milne's Book  ""Étale Cohomology"".
Let $f:Y\rightarrow X$ be locally of finite-type. The following are equivalent. $(a)$ f is unramified. $(b)$ for all $x\in X$ the fiber $Y_x\rightarrow spec(k(x))$ over $x$ is unramified. $(d)$ for all $x\in X$, $Y_x$ has an open covering by spectra of finite separable $k(x)$-algebras. A morphism is unramified if for all $y \in Y$    $k(y)$ is a finite separable extension of $k(x)$ and $m_y=m_x\mathcal{O}_{Y,y}$. For $(b) \Rightarrow (d)$ Let $U$ an open afine subset of $Y_x$ and $\mathfrak{q}$ a prime ideal in $B=\Gamma(U,\mathcal{O}_{Y_x})$, acording to $(b)$ $B_\mathfrak{q}=k(\mathfrak{q})$ is a finite separable field extension of $k(x)$. Also $$k(x)\subset B/\mathfrak{q}\subset B_\mathfrak{q}/\mathfrak{q}B_\mathfrak{q}=B_\mathfrak{q}$$ then $B/\mathfrak{q}$ is also a field. I don't understand why $B/\mathfrak{q}$ is a field. I understand that the result follows from that if $B/\mathfrak{q}$ is a field then $\mathfrak{q}$ is maximal and then $B$ is an Artin ring. Is that because the field extension is finite and separable?? Thanks a lot in advance for your help.","['algebraic-geometry', 'schemes', 'abstract-algebra']"
635392,Finding the least rational $r>0$ such that $\prod_{n=0}^3(2\cos(2^n\pi r)-1)=1$,"Earlier a friend showed me a tricky problem he needed help with. I was able to find a possible solution but I've been unable to check it. Find the least rational $r>0$ such that $x=\pi r$ satisfies $(2\cos(8x) - 1)(2\cos(4x) - 1)(2\cos(2x) - 1)(2\cos(x) - 1)=1$ If we look for $r\in\mathbb{Z}^+$ it's clear that $\cos(2^n\pi r)=(-1)^{2^n r}$. Given that our equation is simply $$\prod_{n=0}^3(2\cos(2^n\pi r)-1)=1$$it is trivial to note that all even $r$ work. It would be nice then that the least solution merely be $r=2$. Unfortunately I have no idea whether there is some $r\in\mathbb{Q}\cap(1,2)$ that could potentially work. Does anyone have any insights?","['trigonometry', 'algebra-precalculus']"
635406,Example of a simple graph isomorphic to a permutation group.,"I'm taking a first course in graph theory this semester and I'm working trough Graph Theory with Applications by J.A. Bonday and U.S.R. Murty. I can't find an answer to question 1.2.12(f): (a) Show, using execise 1.2.5 that an automorphism of a simple graph $G$ can be regarded as a permutation on $V$ which preserves adjacency,and that the set of such permutations form a group $\Gamma(G)$ (the automorphism group of $G$) under the usual operation of composition. (e) Consider the permutation group $\Lambda $ with elements (1)(2)(3), (1, 2, 3) and (1, 3, 2). Show that there is no simple graph $G$ with vertex set {1, 2, 3} such that $\Gamma(G)=\Lambda$. (f) Find a simple graph G such that $\Gamma(G)\cong\Lambda$.","['graph-theory', 'permutations', 'group-theory']"
635407,Dirac delta of a function with zero derivative,"It is known that: $$\int_{-\infty}^\infty f(x) \, \delta(g(x)) \, dx =
 \sum_{i}\frac{f(x_i)}{|g'(x_i)|}$$ Where $x_i$ are the roots of $g(x)$. My question is, what happens when $g'(x_i)$ is zero, but $f(x_i)$ is not? It seems that the integral on the left shoul exist irrespective of the value of $g'(x)$, so: Is there a different formula for the integral one should use in this case, or conversely, is this indeed an indicator that the integral diverges? Bounty Edit As user88595 has explained this may be a consequence of $g(x)$ not having a simple root. I'm looking for a proof or a counterexample that for any $g(x)$ which does not have a simple root at $x_i$, the integral diverges. Edit : I thought I'd give a concrete example. Let's first look at:
$$\int_{-\pi/2}^{\pi/2}e^x \delta\left(\sin(x)\right)dx$$
Since $\sin x$ only has one zero in the interval, and since the derivative at zero is one, the integral is equal to $e^0=1$. Now look at:
$$\int_{-\pi/2}^{\pi/2}e^x \delta\left(\sin^3(x)\right)dx$$
Which leads to infinity by the above rule (since $g'(x) = 0$), and indeed diverges. Is this the case in general?","['dirac-delta', 'measure-theory', 'functional-analysis']"
635436,Hodge double star operator,"I want to prove that $**\omega=\left(-1\right)^{k\left(n-k\right)}\omega$, where $*$ is the Hodge star operator acting on differential $k$-forms $\omega$ on $\mathbb{R}^n$. Where can I find the proof of this?",['differential-geometry']
635443,Sum of exponential random variable with different means,"Suppose that $X$ and $Y$ are independent exponential random variables with pdf's $f(x)=\lambda e^{-\lambda x}$ and $f(y) = \mu e^{- \mu y}$. What is $\;P \{ X+Y <t \}$ ie what is the cdf of the sum? I know that the distribution is gamma when the parameter is the same, but I'm not sure of a closed form when the parameters are different.","['probability', 'random-variables']"
635462,"Let $a_1,a_2,\cdots,a_n$ be $n$ numbers such that $a_i$ is either $1$ or $-1$.If $a_1a_2a_3a_4+\cdots+a_na_1a_2a_3=0$ then prove that $4\mid n$.","Let $a_1,a_2,\cdots,a_n$ be $n$ numbers  such that $a_i$ is either $1$ or $-1$. If $$a_1a_2a_3a_4+a_2a_3a_4a_5+\cdots+a_na_1a_2a_3=0$$ 
  then prove that $4 \mid n$. My work: By multiplying all the terms, we get, $$a_1^4a_2^4\ldots a_n^4=1.$$ I think that I will be able to represent $4n$ a power of $1$, but getting no clue. Please help!
I also think that this problem can be done with invariance and extremal principal too. Please help with these approaches too!","['elementary-number-theory', 'sequences-and-series', 'contest-math', 'combinatorics']"
635519,Equivalent defining Markov property,"Consider the stochastic process $(X_t)_{t \in \mathbb{R}}$ and show the equivalence of the following two Markov properties: (a) $P(X_t \in A \mid  X_u, u \leq s) = P(X_t \in A\mid  X_s) \qquad \forall A \in \mathcal{S}, \forall t>s \in \mathbb{R}$; (b) $P(A\cap B\mid  X_t) = P(A\mid X_t)P(B\mid X_t) \text{ almost surely} \qquad \forall t \in \mathbb{R}, \forall A \in \mathcal{F}_t, \forall B \in \mathcal{T}_t$. ""The future and the past are independent given the present."" As suggested I try to show the discrete case first, where the random variables take discrete values and time is discrete. So far with the given comments and answers: (b) => (a) Set $A := \cap_k [X_{k < n} = i_k], B := [X_n+1 = i], C := [X_n = i_n]$, then (b) implies $P(A\cap B|C) = P(A|C)P(B|C) \iff \frac{P(A\cap B\cap C)}{P(C)} = \frac{P(A\cap C) P(B\cap C)}{P(C)^2} $
$\iff P(B|C) = P(B|A\cap C)$ For these special $A,B,C$ the latter is equivalent to (a) (as we have shown for the discrete case in a previous evercise). (a) => (b) Analog/reverse to above reasoning, (a) implies (b) for events $A := \cap_k [X_{k < n} = i_k], B := [X_n+1 = i], C := [X_n = i_n]$ since for such events in above reasoning all steps where biimplications. It remains to conclude that (a) implies (b) for events of the form $A=\bigcap_i[X_{u_i}=x_i]$, $B=\bigcap_j[X_{s_j}=z_j]$ where $u_i\leqslant n\leqslant s_j$ as well: Again $P(A\cap B | C) = P(A|C)P(B|C) \iff P(B|C) = P(B|A\cap C)$ so we need to show that the latter holds also for the general events $A$ and $B$ only assuming (a) and the first part we have already shown; and it indeed follows from the first bit: $P(B|A\cap C) = P(\bigcap_j[X_{s_j}=z_j] | A\cap C) = P(\bigcap_j[X_{s_j}=z_j] | C)$ since $s_j > n$. Is this the discrete case completed or am I missing something? I think somehow I need to handle the special cases where we have one $s_j = n$ or $u_i=n$. How can I derive the continuous case from the discrete case then? And where does the ""almost surely"" some in in the continuous case?","['probability-theory', 'stochastic-processes', 'markov-process']"
635530,Is the problem that Prof Otelbaev proved exactly the one stated by Clay Mathematics Institute?,"Recently, mathematician Mukhtarbay Otelbaev published a paper Existence of a strong solution of the Navier-Stokes equations , in which he claim that he solved one of the Millennium Problems: existence and smoothness of the Navier-Stokes Equation. Unfortunately, the paper is in Russian but I cannot read Russian. There is a summary in English at the end of the paper, in which I found that the problem he proved was Let $Q \equiv (0,2\pi)^3\subseteq \mathbb{R}^3$ be a 3-dim domain, $\Omega=(0,a)\times Q$, a>0. $\textbf{Navier-stokes problem}$ is to find unknowns: a speed vector $u(t) = (u_1(t,x), u_2(t,x), u_3(t,x))$ and a scalar pressure function $p(t,x)$ at the points $x\in Q$ and time $t\in (0,a)$ satisfying the system of the equations ... initial
  $$ u(t,x)\vert_{t=0} =0$$ ... But the problem that stated by the  Clay Mathematics Institute was, see http://www.claymath.org/sites/default/files/navierstokes.pdf , $\textbf{(B)  Existence and smoothness of Navier–Stokes solutions in $\mathbb{R}^3/\mathbb{Z}^3$.}$ ... Let $u^0$ be any smooth, divergence-free vector field satisfying $u^0(x+e_j) = u^0(x)$; we take $f(x, t)$ to be identically zero. Then there exist smooth functions $p(x,t)$, $u_i(x,t)$ on $\mathbb{R}^3 \times[0,\infty)$. that ... So my question is: 1) Can the $a>0$ in his proof be $\infty$? Or is it enough to prove the problem for arbitrary $a>0$? 2) Is there any theory that can turn the problem with arbitrary initial value $u^0$ (of course satisfying some condition) into a problem with initial value being $0$? 3) The problem that formulated by the Clay Institute assumes $f\equiv 0$.  Prof Otelbaev proved his result for all $f\in L_2(\Omega)$. Is this result much stronger? Update : There is an article stating that the $L_2$ estimate is not enough to solve the problem. (in Spanish)","['fluid-dynamics', 'partial-differential-equations', 'analysis']"
635544,"Prove $\frac{c_n(a_1,\ldots,a_n)}{c_{n-1}(a_2,\ldots,a_n)}=a_1 + \frac{1}{a_2 + \frac{1}{\ddots + \frac{1}{a_{n-1}+\frac{1}{a_n}}}}$","For $n>0$ and $a_1,...,a_n \in K$ let $c_n(a_1,...,a_n)$ be the determinant of the matrix $$
  \begin{pmatrix}
  a_1 & 1 & 0 & \cdots & 0 \\
  -1 & a_2 & \ddots & \ddots & \vdots \\
  0 & \ddots & \ddots & \ddots & 0 \\
  \vdots & \ddots & \ddots & \ddots & 1 \\
  0 & \cdots & 0 & -1 & a_n
  \end{pmatrix}
$$ Show for $n \ge 2$ that following holds: $$
  \frac{c_n(a_1,...,a_n)}{c_{n-1}(a_2,...,a_n)}
  = 
  a_1 + \cfrac{1}{a_2 + \cfrac{1}{\ddots + \cfrac{1}{a_{n-1}+\frac{1}{a_n}}}}
$$ I want to show it using induction over n but I already fail at the initial step: For $n = 2$ I have to show:
$$
  \frac{c_2(a_1,a_2)}{c_{2-1}(a_2)}
  = 
  a_1 + \frac{1}{a_2}
$$ which is
$$
  \frac{a_1a_2 + 1}{a_2} \neq a_1 + \frac{1}{a_2}
$$ My also have no idea for the induction step where I have to show:
$$
  \frac{c_{n+1}(a_1,...,a_{n+1})}{c_n(a_2,...,a_{n+1})}
  = 
  a_1 + \cfrac{1}{a_2 + \cfrac{1}{\ddots + \cfrac{1}{a_n+\frac{1}{a_{n+1}}}}}
$$ So first I calculate $c_{n+1}(...)$ by developing after the first column which gives me:
$$
  a_1 \cdot \det
  \begin{pmatrix}
  a_2 & 1 & & & & \\
  -1 & a_3 & 1 & & & \\
  & -1 & \ddots & & & \\
  & & & \ddots & & \\
  & & & & & 1 \\
  & & & & -1 & a_{n+1}
  \end{pmatrix}
  +
  \det
  \begin{pmatrix}
  1 & 0 & & & & \\
  -1 & a_3 & 1 & & & \\
  & -1 & a_4 & & & \\
  & & & \ddots & & \\
  & & & & & 0 \\
  & & & & 0 & a_{n+1}
  \end{pmatrix}
$$
which after developing after the first row gives me
$$
  a_1 \cdot c_n(a_2,...,a_{n+1}) + c_{n-2}(a_3,...,a_{n+1}).
$$ Analog for $c_n(...)$:
$$
  c_n(...) = a_2 c_{n-1}(...) + c_{n-3}(...)
$$ So I have
$$
  \frac{c_{n+1}(...)}{c_n(...)} = \frac{a_1c_n(...) + c_{n-2}(...)}{a_2c_{n-1}(...) + c_{n-3}(...)}
$$ written in another way it is
$$
  \frac{a_1c_n(...)}{c_n(...)} + \frac{c_{n-2}}{a_2c_{n-1}(...) + c_{n-3}(...)}
$$ I write it as
$$
  a_1 + \frac{1}{a_2 \frac{c_{n-1}(...)}{c_{n-2}(...)} + \frac{c_{n-3}(...)}{c_{n-2}(...)}}
$$ and then
$$
  a_1 + \frac{1}{a_2 \frac{c_{n-1}(...)}{c_{n-2}(...)} + \frac{1}{\frac{c_{n-2}(...)}{c_{n-3}(...)}}}
$$ so
$$
  a_1 + \frac{1}{a_2 \cdot \left( a_3 + \cfrac{1}{a_4 + \cfrac{1}{\ddots + \cfrac{1}{a_n+\frac{1}{a_{n+1}}}}} \right) + \frac{1}{\left( a_4 + \cfrac{1}{a_5 + \cfrac{1}{\ddots + \cfrac{1}{a_n+\frac{1}{a_{n+1}}}}} \right)}}
$$ I dont know how to preced from here, any help or simpler solutions are appreciated!","['induction', 'linear-algebra', 'determinant']"
635576,"Intuition - If $Ax = b$ has infinitely many solutions, why can't $Ax = c$ have only one solution? [Strang P165 3.4. 22]","If $\mathbf{Ax = b}$ has infinitely many solutions, why is it impossible for $\mathbf{Ax = c}$ (where $\mathbf{c}$ is a new right side) to have only one solution? Proof : Take two solutions of $\mathbf{Ax = b} :$ $\mathbf{Ax_1 = b}$ and $\mathbf{Ax_2 = b} \implies \mathbf{\color{green}{A(x_1 - x_2) = 0}}$. Thus, if $\mathbf{Ax_0 = c}$ then add the homogenous solution in green to $\mathbf{x_0}$: $\mathbf{Ax_0 + \color{green}{A(x_1 - x_2)} = c + \color{green}{0} }$ so $\mathbf{x_0}$ is not unique. If $\mathbf{c}$ is not in $colspace(A)$, then no solution to $\mathbf{Ax = c}$. $\Large{{1.}}$ I accept the proof but I don't perceive the intuition? Would someone please explain/uncloak it? I recollect: If $\mathbf{c}$ is not in $colspace(A)$, then no solution to $\mathbf{Ax = c}$. $\Large{{2.}}$ How would you divine/previse the strategy of subtracting two solutions to $\mathbf{Ax = b} $ and then adding this homogeneous solution to $\mathbf{Ax = c}$  ? $\Large{{3.}}$ Does the above prove that $\mathbf{Ax = c}$ has infinitely many solutions also, beyond $\mathbf{x_0}$ and $\mathbf{x_0 + \color{green}{(x_1 - x_2)}}$?","['linear-algebra', 'intuition']"
635621,Clayton copula and Kendall's tau,"I'm currently preparing for an exam in Risk Management (mathematics) by doing exercises from old exams. One of these exercises proved to be too difficult because of the following: Given Kendall's tau $ \tau = 1/3 $ and the Clayton copula $$
C(u,v) = (u^{-\theta} + v^{-\theta} - 1)^{-1/\theta}
$$ we can calculate the parameter $ \theta $ by $$
\tau = \theta / (\theta + 2),
$$ which we see is $ \theta = 1 $. My problem is that I can't see why this is so, how does this work? After some more reading I have arrived at this: $$
\begin{align}
\tau & = 4 \cdot \mathbf{E} [C(U,V)] - 1 \\ 
     & = 4 \int_{0}^{1} \int_{0}^{1} (u^{-\theta} + v^{-\theta} - 1)^{(-1/\theta)} dudv - 1,
\end{align}
$$ but I'm not able to solve this double integral. I've checked on the internet, tried Wolfram Alpha integral calculator as well as using a mathematics handbook. Any help as to how I should proceed in order to solve this integral would be much appreciated!","['statistics', 'integration']"
635649,Why is the space of all connection on a vector bundle an affine space?,"I think this result is very well known, but I don't understand its proof. Let E a vector bundle over a manifold M, and $\Omega^i(E):=\Gamma(\Lambda^iT^*M\otimes E)$ the space of E-valued differential forms of degree i. A connection on E is a map $\nabla: \Omega^0(E)=\Gamma(E)\rightarrow\Omega^1(E)$. The claim is that for two arbitrary connetions $\nabla_1$ and $\nabla_2$ on E, their difference $A=\nabla_1-\nabla_2$ is in $\Omega^1(End(E))$, so the space of connections is an affine space for the vector space $\Omega^1(End(E))$. My problem is the appearance of $\Omega^1(End(E))$. The key point of the proof is to show that for any section $s\in\Gamma(E)$, the expression $A(s)(p) = ((\nabla_1-\nabla_2)s)(p)$ depends only on s(p) . The conclusion is: Since $(A(s))(p)$ depends on $s$ only through $s(p)$, it follows that $A\in\Omega^1(End(E))$. Why can one make this conclusion? I have been given more details on that: The following map is $\mathbb{R}$-linear:
$$\begin{align}
E_p&\stackrel{A_p}{\longrightarrow}T^*_pM\otimes E\\
E_p\ni s&\mapsto (A(s))(p)
\end{align}$$
therefore
$$\begin{align}
A_p &\in Hom(E_p, T^*_pM\otimes E_p)\\
&=E^*_p\otimes T^*_pM\otimes E_p\\
&=(E^*_p\otimes E_p)\otimes T^*_pM\\
&=End(E_p)\otimes T^*_pM
\end{align}$$
Here, I don't understand the second line: why is $(A(s))(p)$ in $T^*_pM\otimes E$? In other words, why is $A$ of the form
$$
A(s) = \omega\otimes s'(s),\qquad\omega\in\Omega^1(M),\enspace s'(s)\in\Gamma(E)
$$
and $\omega$ does not depend on $s$ ? EDIT: what I have learned from the answer of Jeremy Daniel Let $E\rightarrow M$ a vector bundle, $\nabla$ a connection on E and $A=\nabla_1-\nabla_2$. Then locally, for any $s\in\Gamma(E)$, we have the assignment:
$$\begin{align}
\nabla:s&\mapsto\sum_i\left(\omega_i(s):p\mapsto(\omega_is)(p)\right)\otimes dx^i&(\nabla s):M\rightarrow T^*M\otimes E\\
A:s&\mapsto\sum_i\left(\alpha_i:p\mapsto\alpha_i(s(p))\right)\otimes dx^i&(As):M\rightarrow T^*M\otimes E
\end{align}$$
Although both $(\nabla s)$ and $(As)$ are maps $M\rightarrow T^*M\otimes E$, the difference is in the maps $\omega_i$ and $\alpha_i$, which are of different nature. To assign a value to $p$, $\omega_i$ has to consider information in a neighbourhood of $p$ , e.g. derivatives of a given section $s$ at p, only with this data $(w_is)(p)$ can be computed. In contrast, $\alpha_i$ does not need this information and depends only on the value $s(p)$ at that given point, thus it is a tensor . It maps points in a fiber $E_p$ to points in the same fiber, thus $\left.\alpha_i(s)\right|_{E_p}\in End(E_p)$. Comments to this are appreciated!","['connections', 'vector-bundles', 'differential-geometry']"
635663,About nonlinear functions $f$ and $g$ such that $f(x+y)=g(x)+g(y)$,"Does there exist nonlinear functions $f$ and $g$ such that $f(x+y)=g(x)+g(y)$ for every $x$ and $y$? Maybe there is a trivial solution but I can't see it. 
Thank you.",['functions']
635665,very simple limit question $\lim_{t\to0}\frac{t^2}{1-\cos^2t}$,"I'm trying to solve some limit problems and i found this one $$ \lim_{t \to 0}\frac{t^2}{1-\cos^2t}  $$ 
what i did was, i changed $1-\cos^2t$ by $\sin^2t$ and i solve it like 
$$( \frac{t}{\sin t} \cdot \frac{t}{\sin t} )$$ and it gives me $1$. But when i did it with L’Hospital’s Rule i get $0$. i go with L’Hospital’s Rule, but what kind of error do i did (for 1st method) or why i get two different results? thank you.","['trigonometry', 'calculus', 'limits']"
635670,Show that the least squares line must pass through the center of mass,"My problem: The point $(\bar x, \bar y)$ is the center of mass for the collection of points in Exercise 7. Show that the least squares line must pass through the center of mass. [ Hint : Use a change of variables $z = x - \bar x$ to translate the problem so that the new independent variable has mean 0.] I have already solved Exercise 7: Given a collection of points $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$, let $\mathbf x = (x_1, x_2, \ldots, x_n)^T$, $\mathbf y = (y_1, y_2, \ldots, y_n)^T$, $\bar x = \frac 1n \sum_1^n x_i$, $\bar y = \frac 1n \sum_1^n y_i$ and let $y = c_0 + c_1 y$ be the linear function that gives the best least squares fit to the points. Show that if $\bar x = 0$, then $c_0 = \bar y$ and $c_1 = \frac {\mathbf x^T \mathbf y}{\mathbf x^T \mathbf x}$. It is obvious that if $x = \bar x$ then $y = c_0 + c_1x = \bar y + 0 = \bar y$, however the hint suggests that the problem should be solved in another way. Edit I have found an answer. It makes use of the following theorem: If A is an m x n matrix of rank n , the normal equations $ A^T A \mathbf x = A^T \mathbf b$ have a unique solution $ \hat {\mathbf x} = (A^TA)^{-1}A^T \mathbf b$ and $ \hat {\mathbf x} $ is the unique least squares solution of the system $ A \mathbf x = \mathbf b $. Now let $ \hat {\mathbf x} = \mathbf c = (c_0, c_1)^T, A = \begin{pmatrix}1 & \cdots & 1 \\x_1 & \cdots & x_n \\\end{pmatrix}, \mathbf b = \mathbf y = (y_1, \ldots, y_n)^T $ such that $c = (A^TA)^{-1}A^Ty$, then
$$\begin{pmatrix}c_0\\c_1\\\end{pmatrix} = \begin{pmatrix}n & \sum x_i\\\sum x_i & \sum x_i^2\\\end{pmatrix}^{-1} \begin{pmatrix}\sum y_i\\\sum x_iy_i\\\end{pmatrix} $$
which gives values for $c_0$ and $c_1$. These values should be used in the formula $c_1x + c_0$, which, together with $ x = \bar x = \frac 1n \sum x_i$, indeed results in $ \bar y $.","['regression', 'linear-algebra', 'least-squares']"
635684,transforming a vector from cartesian to spherical and cylindrical co-ordinate system,"I know the formula(which i don't know how to copy here but it was in matrix form) for transforming a vector from cartesian system to spherical or cylindrical coordinate system.
But, I want to know its derivation .
I tried searching it on web but all i got was some jacobian formulas , that i have no idea about.","['vector-spaces', 'matrices', 'calculus', 'multivariable-calculus']"
635686,How to convert a permutation to permutation polynomial?,"Let $\mathbf{F}_q$ be the finite field with $q$ elements, where $q$ is a prime power.
A permutation on $\mathbf{F}_q$ is a bijection from $\mathbf{F}_q$ to itself.
Let $\mathbf{F}_q$ be the ring of polynomials in a single indeterminate $x$ over $\mathbf{F}_q$ .
A polynomial $f \in \mathbf{F}_q[x]$ is called a permutation polynomial of $\mathbf{F}_q$ if it induces a one-to-one map from $\mathbf{F}_q$ to itself.
It is well known that every permutation on $\mathbf{F}_q$ can be expressed as a
permutation polynomial over $\mathbf{F}_q$ . Problem 1: How to convert a permutation to a permutation polynomial? Please note that Lagrange polynomial as polynomial interpolation is possible. Any other method? Problem 2: as problem (1) but with $\mathbf{F}_q$ and $q$ composite (not a prime).","['permutations', 'linear-algebra', 'polynomials']"
635687,disconnected ordered set,"Is there a totaly ordered infinite set $A$ with the least element $a$ and the greatest element $b$ such that for any sequence $\{\alpha_n\}$ and $\{\beta_n\}$ in $A$ which satisfies  $\alpha_n<\beta_n$, we can deduced that $\lim\alpha_n<\lim\beta_n$ (or even $\sup \alpha_n<\sup\beta_n$) ?( the last inequalities is remained strict)","['lattice-orders', 'elementary-set-theory', 'order-theory']"
635707,Brownian Bridge as a Gaussian Process,"Let $B=\{B_t:t\geq 0\}$ be a standard Brownian motion. Define the Brownian brige $X=\{X_t:t\geq0\}$ as
$$
X_t=B_t-tB_1\quad t\in[0,1]
$$
Show that $X$ is (i) Gaussian and find its (ii) mean and (iii) covariance. TWO questions: Due to my lack of basics on the subsject, can I see a full proof of (ii) and (iii) ? Can anybody check my attempt on (i)? Attempt on (i). Given that $X$ is a stochastic process by definition only need to show that $\sum_i^n \lambda_i X_{t_i}$ is Gaussian for some real $\lambda_1,\lambda_2,...,\lambda_n$.
\begin{equation*}
\begin{split}
\sum_i^n \lambda_i( B_{t_i}-t_iB_1) & = \sum_i^n \lambda_i B_{t_i}-\gamma_iB_1 \ \ \text{ where }\gamma_i=\lambda_it_i \\
& = \sum_i^n \phi_iB_{t_i} +\gamma_i(B_{t_i}-B_1)\text{ where }\phi_i=\lambda_i-\gamma_i\\
& = \sum_i^n \phi_iB_{t_i} +\sum_i^n \gamma_i(B_{t_i}-B_1)\\
& = \sum_i^{n }\theta_i(B_{t_i}-B_{t_{i-s}} )+\sum_i^n \gamma_i(B_{t_i}-B_1),
\end{split}
\end{equation*}
scalar multiplication for normal distribution was used in every step. Then using the independence property of BM and the sum of independent normal variable property we satisfy the definition of a Gaussian process (doubts on the last equality/rearranging, namely on $\theta$ and $n$ in the first sum). $\blacksquare$","['normal-distribution', 'probability', 'brownian-motion']"
635731,D'Alembertian $\Box$,"This question has to do with the D'Alembertian operator on a general manifold with a metric $g_{\mu\nu}$. I understand that the definition of the D'Alembertian is 
$$\Box \phi\equiv g_{\mu\nu}\partial^\mu\partial^\nu \phi$$
So why is it also given in the form $$\Box \phi=\frac{1}{\sqrt{-g}}\partial_\mu(\sqrt{-g}g^{\mu\nu}\partial_\nu \phi)$$where $g=$det$(g_{\mu\nu})$? I don't understand how the $\sqrt{-g}$ factors come about in the second equation. Could someone kindly point it out?
Many thanks.","['manifolds', 'metric-spaces', 'differential-geometry']"
635781,"Can something like $\text{Hom}(V,K)$ be visualised?","I have no trouble visualising vector spaces like $\Bbb R^3$ and (e.g.) a subspace of dimension $2$, which would just be a plane through the origin of a $3$-D space, but I'm having trouble visualising something like ""$W°$ is a subspace of $V^*$"" when you know that: $V$ is a vector space, $W$ is a subspace of $V$, $W°:=\{f\in V^*:W\subseteq \ker f\}$, $V^*=\operatorname{Hom}_{_K}(V,K)$. Even visualising $V^*$ is something I don't understand: it's a collection of functions; so how can it be a vector space?
I have the proof, and I understand it, but it seems impossible to intuitively ""see"" things like that. Is it even possible?","['vector-spaces', 'continuous-homomorphisms', 'geometric-interpretation', 'abstract-algebra', 'visualization']"
635788,Why is the map bewteen a matrix and its characteristic polynomial continous?,"One may define the following map :
$\begin{array}{l|rcl}
f : & M_n(\mathbb R) & \longrightarrow & \mathbb R_n[X] \\
    & A & \longmapsto & p_A \end{array}$ Why is $f$ continuous ? Which norms are convenient  for this problem ? Note that $f$ isn't linear. I tried to use the sequential way, without success...","['matrices', 'linear-algebra']"
635855,Christoffel Symbols Equality Solution?,"They changed the exercise, so I tried to solve it again: I have to prove the following: Let $\Omega \subseteq \mathbb{R}^d$ be open and $g$ a metric field on $\Omega$. For every $\phi \in \mathrm{Diff}(\Omega)$ let $\Xi^i_{jk}[\phi]$ be functions on $\Omega$ that transform in the same way as the Christoffel symbols $\Gamma^i_{jk}[\phi]$
$$
\Xi^i_{jk}[\phi](y) = \frac{\partial x^p}{\partial y^j}(y) \frac{\partial x^q}{\partial y^k}(y) \Xi^r_{pq}[\mathrm{id}](\phi^{-1}(y))\frac{\partial y^i}{\partial x^r}(\phi^{-1}(y)) + \frac{\partial y^i}{\partial x^m} (\phi^{-1}(y)) \frac{\partial^2 x^m}{\partial y^j \partial y^k}(y).
$$
Assume that for each $y_0 \in \Omega$ there is a $\phi_0 \in \mathrm{Diff}(\Omega)$ such that $\Xi^i_{jk}[\phi_0](\phi_0(y_0)) = 0$ and $(\partial_a g_{bc})[\phi_0](\phi_0(y_0))=0$. Show $\Xi^i_{jk}[\phi] =\Gamma^i_{jk}[\phi]$ for all $\phi \in \mathrm{Diff}(\Omega)$. Here is my try: We define: $T^{i}_{\,jk}[\phi](y) := \Xi^i_{\,jk}[\phi](y) - \Gamma^i_{\,jk}[\phi](y)$ Thus we see that this transforms as a Tensor: $T^{i}_{\,jk}[\phi](y)= \Xi^i_{jk}[\phi](y) - \Gamma^i_{\,jk}[\phi](y) = \bigg(\frac{\partial x^p}{\partial y^j}(y) \frac{\partial x^q}{\partial y^k}(y) \Xi^r_{pq}[\mathrm{id}](\phi^{-1}(y))\frac{\partial y^i}{\partial x^r}(\phi^{-1}(y)) + \frac{\partial y^i}{\partial x^m} (\phi^{-1}(y)) \frac{\partial^2 x^m}{\partial y^j \partial y^k}(y) \bigg)- \bigg(\frac{\partial x^p}{\partial y^j}(y) \frac{\partial x^q}{\partial y^k}(y) \Gamma^r_{pq}[\mathrm{id}](\phi^{-1}(y))\frac{\partial y^i}{\partial x^r}(\phi^{-1}(y)) + \frac{\partial y^i}{\partial x^m} (\phi^{-1}(y)) \frac{\partial^2 x^m}{\partial y^j \partial y^k}(y)\bigg) = \frac{\partial x^p}{\partial y^j}(y) \frac{\partial x^q}{\partial y^k}(y)  (\Xi^r_{pq}-\Gamma^r_{\,pq})[\mathrm{id}](\phi^{-1}(y))\frac{\partial y^i}{\partial x^r}(\phi^{-1}(y))$ Now we see that for a given pair $y_0$, $\phi_0$: $\Gamma^i_{\,jk}[\phi_0](\phi_0(y_0))=\frac{1}{2} g^{il}(\partial_k g_{lk} + \partial_j g_{lk} - \partial_l g_{jk})[\phi_0](\phi_0(y_0))= \frac{1}{2} g^{il}\left(\partial_k g_{lk}[\phi_0](\phi_0(y_0)) + \partial_j g_{lk}[\phi_0](\phi_0(y_0)) - \partial_l g_{jk}[\phi_0](\phi_0(y_0))\right)=0$ So for $y_0$ and $\phi_0$ : $T^{i}_{\,jk}[\phi_0](\phi_0(y_0)) := \Xi^i_{\,jk}[\phi_0](\phi_0(y_0)) - \Gamma^i_{\,jk}[\phi_0](\phi_0(y_0))=0$ Thus $\frac{\partial x^p}{\partial y^j}(\phi_0(y_0)) \frac{\partial x^q}{\partial y^k}(\phi_0(y_0))  (\Xi^r_{pq}-\Gamma^r_{\,pq})[\mathrm{id}](\phi_0^{-1}(\phi_0(y_0)))\frac{\partial y^i}{\partial x^r}(\phi_0^{-1}(\phi_0(y_0)))= \frac{\partial x^p}{\partial y^j}(\phi_0(y_0))\frac{\partial x^q}{\partial y^k}(\phi_0(y_0))  \left({(\Xi^r_{pq}-\Gamma^r_{\,pq})[\mathrm{id}](y_0)}\right)\frac{\partial y^i}{\partial x^r}(y_0)\overset{!}{=}0$ Now my BIG question is: can I say that: Because $T^{i}_{\,jk}$ is a Tensor: $(\Xi^r_{pq}-\Gamma^r_{\,pq})[\mathrm{id}](y_0)\overset{???}{=}0$ Because IF so then the exercise becomes trivial since:
$T^{i}_{\,jk}[\phi](\phi(y_0))=\frac{\partial x^p}{\partial y^j}(\phi(y_0))\frac{\partial x^q}{\partial y^k}(\phi(y_0))  \overset{\underbrace{=0}}{\left({(\Xi^r_{pq}-\Gamma^r_{\,pq})[\mathrm{id}](y_0)}\right)}\frac{\partial y^i}{\partial x^r}(y_0)=0$ for all $\phi\in Diff(\Omega)$ and we are done If not my old proof/idea does not hold anyway and I have got no other clue on how to overcome the problem...",['differential-geometry']
635858,Inverse function of isomorphism is also isomorphism,"Let $G$ be a group, and let $p:G\rightarrow G$ be an isomorphism. Why is $p^{-1}$ also an isomorphism? We know that $p(a)p(b)=p(ab)$ for any elements $a,b\in G$. We also know $p(a^{-1})=p(a)^{-1}$ for any element $a\in G$ (follows from the first statement.) How would it show $p^{-1}(ab)=p^{-1}(a)p^{-1}(b)$?","['group-theory', 'abstract-algebra']"
635906,Order of element equal to least common multiple [duplicate],"This question already has answers here : Order of elements is lcm-closed in abelian groups (6 answers) Closed 8 years ago . Let $G$ be a group, and $a,b\in G$. Suppose $\operatorname{ord}(a)=m, \operatorname{ord}(b)=n$, and that $ab=ba$. Prove that there is an element $c\in G$ such that $\operatorname{ord}(c)=\operatorname{lcm}(m,n)$. Let $A=\operatorname{lcm}(m,n)$. I consider $(ab)^A=a^Ab^A=1$, so the order of $ab$ divides $A$. What can we do to find an element with order exactly $A$?","['group-theory', 'abstract-algebra']"
635913,Riccati differential equation,"I have to solve the following equation :
   $$ \frac{dx}{dt}(t)=-q x^2(t) +1  $$
with $x(0)=1$ and $q>0$. At first I consider the two cases: $q=1$, then I take the change of variable  $ x= \frac{u^{\prime}}{u}$ then with small calculations I got the second ord er linear homogeneous differential equation $ u^{\prime\prime} -u =0$, a solution of this equation is $u=c_1+c_2e^t$ and again to our x we get $x=\frac{c_2e^t}{c_1+c_2e^t}$ and with the initial condition  $x(0)=1$ we get $x=1$ if $q$ is not $1$, directly I assume that $-q x^2(t) +1 $ is non zero and solve the equation by simple integration of $$ \int \frac{dx}{1-qx^2}= \int 1 dt$$
then I got the following solution $$ x(t)= \frac{1}{\sqrt{q} \tanh(t\sqrt{q} +c)}$$ where c is the constant determined from the initial condition $c=\operatorname{atanh}(\sqrt{q})$ Are these discussions and solution steps correct?  May I assume in the second case that $-q x^2(t) +1 $ is non zero directly?   and in the first case, considering the change of variable  $ x= \frac{u^{\prime}}{u}$  with no assumptions on $u$ is correct? Thanks for any ideas.","['ordinary-differential-equations', 'calculus']"
635931,Exercise 5C10 in Isaacs' Finite Group Theory,"Problem: Suppose that $G$ is simple group and has an abelian Sylow $2-$subgroup of order $8$. Show that the order of $G$ is divisible by $7$. Is there any hint to solve this problem? I'll be glad if one gives an answer. Here's my start: Let $|G| = 8k$ with $2\nmid k$. Then $n_2 = 1 \mod 2$, and $n\mid k$. Since $G$ is simple, $n_2 \neq 1$. After that I get stuck.","['finite-groups', 'group-theory']"
635983,Does $\operatorname{Log}(1+i)^2 =2\operatorname{Log}(1+i)$,"And similarly, does $\operatorname{Log}(1-i)^2=2\operatorname{Log}(1-i)$? If we were dealing with real numbers, it would hold. But I'm guessing that the fact that there are imaginary numbers involved changes things?","['complex-numbers', 'logarithms', 'complex-analysis', 'analysis']"
636022,Finding a function which can describe a probability density function,"Let the function 
$$
f(x) = \begin{cases} ax^2 & \text{for } x\in [ 0, 1], \\0 & \text{for } x\notin [0,1].\end{cases}
$$ Find $a$, such that the function can describe a probability density function. (later I'm also to find CDF, standard deviation and such but I think that's the least problematic thing here) So first of all - it's not a homework, I'm trying to prepare for a test where it's said to appear but I was absent on the class so my only hopes are the notes I was able to obtain which are quite illegible for me. Thus, it's more important for me to be able to solve such class of problems rather than exactly this one and nothing else. So the notes I have seem to suggest I should take the integral from $-1$ to $1$ of the said function - but is that really all I need to say a function can describe a probability density function?","['statistics', 'probability']"
636058,even square numbers represented as two primes added together,Can every even square number be written as the sum of 2 prime numbers? How do you prove the result?,['number-theory']
636089,"Finding CDF, standard deviation and expected value of a random variable","Let the function 
$$
f(x) = \begin{cases} ax^2 & \text{for } x\in [ 0, 1], \\0 & \text{for } x\notin [0,1].\end{cases}
$$ Find $a$, such that the function can describe a probability density function. Calculate the expected value, standard deviation and CDF of a random variable X of such distribution. So thanks to the community, I now can solve the former part of such excercises, in this case by $\int_0^1 ax^2 \, dx = \left.\frac{ax^3}{3}\right|_0^1 = \frac a 3$ so that the function I'm looking for is $f(x)=3x^2$. Still, I'm struggling with finding the descriptive properties of this. Again, it's not specifically about this problem - I'm trying to learn to solve such class of problems so the whole former part may be different and we may end up with different function to work with. So as for the standard deviation, I believe I should find a mean value $m$ and then a definite integral (at least that's what the notes suggest?) so that I end up with $$\int_{- \infty}^\infty (x-m) \cdot 3x^2 \,\mathrm{d}x$$ As for the CDF and expected value, I'm clueless though. In the example I have in the notes, the function was $\frac{3}{2}x^2$ and for the expected value there is simply some $E(X)=n\cdot m = 0$ written while the CDF here is put as $D^2 X = n \cdot 0.6 = 6 \leftrightarrow D(X) = \sqrt{6}$ and I can't make head or tail from this. Could you please help?","['statistics', 'probability']"
636128,Calculating the number of possible paths through some squares,I'm prepping for the GRE. Would appreciate if someone could explain the right way to solve this problem. It seems simple to me but the site where I found this problem says I'm wrong but doesn't explain their answer. So here is the problem verbatim: Find the number of paths from x to y moving only right (R) or down (D). My answer is 6. What am I missing?? Thanks for any help.,"['permutations', 'combinations', 'combinatorics']"
636130,Does $\big|\sum_{k=1}^na_k\big|\leq\sqrt{n}$ imply the convergence of $\sum_{1}^\infty \frac{a_k}{k}$?,"Suppose $\big|\sum_{k=1}^na_k\,\big|\leq\sqrt{n}$ for all $n\geq 1$. Show that $\sum_{1}^\infty \frac{a_k}{k}$ converges. Summation by part doesn't help. ([ Added : Thanks to the comments, I should have made my thought clear so that it would not cause confusion. It is not that ""Summation by part doesn't help"", but that "" I don't see how 'summation by part' might work"".]) I tried several convergence tests without any progress. The assumption gives an estimate for the Cesaro mean:
$$
\left|\frac{\sum_{k=1}^n a_k}{n}\right|\leq \frac{1}{\sqrt n}.
$$
It seems that if I can get
$$
a_n\sim \left|\frac{\sum_{k=1}^n a_k}{n}\right|
$$
then things will be done since 
$$
\sum\frac{1}{\sqrt k \cdot k}
$$
converges. How shall I go on?","['sequences-and-series', 'real-analysis']"
636141,Ratio of diameter to area of a set,"It may be quite a basic and common thing but I haven't found much after a while of searching and I failed to figure that myself... Let's have a (connected) set $M$ and let $\text{diam}(M)$ be its diameter .
How big can his area be? Or in other words - evaluate this expression: $$\sup_{M}\left\{\frac{\text{area}(M)}{\text{diam}^{2}(M)}\right\}$$ For example let $M$ be a square with side $a$ . Then: $$\frac{\text{area}(\text{square})}{\text{diam}^{2}(\text{square})} = \frac{a^{2}}{\left( a\sqrt{2} \right)^{2}} = \frac{1}{2}$$ I'm really looking forward to see a (sketch of a) proof of such thing because no matter how simple it looks I just don't even know where to start...",['general-topology']
636142,Local martingale is locally uniformly integrable martingale?,"Is a local martingale locally uniformly integrable martingale ? Here I define a local martingale to be the process with a localizing sequence $\tau_n$ such that the stopped process is martingale. But how can we find a localizing sequence such that the stopped process is a uniformly integrable martingale ? The solution I gave is $\min (\tau_n , n)$ , could somebody please confirm ? Thanks in advance !","['probability-theory', 'stochastic-processes', 'uniform-integrability', 'martingales']"
636143,Show that the Cauchy integral formula implies the Cauchy-Goursat Theorem,"I'm struggling with this question, the integral formula states: $$f(z_0) = \frac{1}{2\pi i} \int_{C}\frac{f(z)}{z-z_0}\,dz$$ and the Cauchy-Goursat theorem states: If $f$ is holomorphic in a simply connected domain $D$ and $C$ contained in $D$ is a closed curve, then $\int_{C}f(z)\,dz =0$ To be perfectly honest I'm not at all sure where to start with question.","['analyticity', 'complex-analysis', 'analysis']"
636190,Evaluate $\lim\limits_{n\to\infty}\left(a+\tfrac1n\right)^n$,"Let $a\gt0$ real. Evaluate the following limit:
$$\displaystyle\lim_{n\to\infty}\left(a+\tfrac1n\right)^n $$
I try'd expressing this limit in such a way that I could use the known limit: $\displaystyle\lim_{n\to\infty}\left(1+\tfrac1n\right)^n = e $. However that hasn't really helped me solve the problem. Does anyone have another idea?","['real-analysis', 'limits']"
636216,"Prove that $|f'(x)| \le \frac{A}2 \forall x \in [0,1] $ [duplicate]","This question already has answers here : Proving that if $|f''(x)| \le A$ then $|f'(x)| \le A/2$ (2 answers) Closed 10 years ago . Let $f$ be twice differentiable in $[0,1]$ $f(0) = f(1) = 0$ , $|f''(x)|\le A$ . Prove that $|f'(x)| \le  \frac{A}2, \forall x \in [0,1] $ . Well this is what I came up with, $f'(c_1) = \dfrac{f(x) -f(0)}{x-0} = \dfrac{f(x)}{x}$ $f'(c_2) = \dfrac{f(x) -f(1)}{x-1} = \dfrac{f(x)}{x-1}$ for $0\lt c_1,c_2 \lt 1$ from rolle's theorem we know that there is : $0\lt c_3 \lt 1$ such that $f'(c_3) =0$",['calculus']
636217,When does homeomorphism imply diffeomorphism?,"In $R^n$, suppose $U$ and $V$ and two homeomorphic open sets. Then, is $U$ diffeomorphic to $V$? If not, can we impose stronger conditions such that this true?","['general-topology', 'differential-geometry']"
636228,Evaluate $\lim_{n\to\infty}(\sqrt{4n^2+n}-2n)$ [duplicate],"This question already has answers here : How would you prove that $\lim\limits_{n\to\infty}(\sqrt{4n^2+n}-2n)=\frac14$? (4 answers) Closed 6 years ago . Evaluate the following limit:
$$\displaystyle\lim_{n\to\infty}(\sqrt{4n^2+n}-2n)$$
So far I've come up with this: $\displaystyle\lim_{n\to\infty}(\sqrt{4n^2+n}-2n)$ = $\displaystyle\lim_{n\to\infty}(\sqrt{4n^2(1+\frac{1}{4n}})-2n)$ 
= $\displaystyle\lim_{n\to\infty}(2n\sqrt{(1+\frac{1}{4n}})-2n)$ = $\displaystyle\lim_{n\to\infty}(2n(\sqrt{(1+\frac{1}{4n}})-1))$. I think it's pretty clear from here that this goes to infinity, but how can I justify that the 2n grows stronger to infinity than the part in the brackets goes to zero? I know standard rules about exponential functions growing harder than polynomials, but not about this.","['radicals', 'calculus', 'real-analysis', 'limits']"
636246,Show that function is differentiable but its derivative is discontinuous.,"Let $g(x)=x^2\sin(1/x)$, if $x \neq 0$ and $g(0)=0$. If $\{r_i\}$ is the numeration of all rational numbers in $[0,1]$, define
  $$
f(x)=\sum_{n=1}^\infty \frac{g(x-r_n)}{n^2}
$$
  Show that $f:[0,1] \rightarrow R$ is differentiable in each point over [0,1] but $f'(x)$ is discontinuous over each $r_n$. Is possible that the set of all discontinuous points of $f'$ is precisely $\{r_n\}$? I'm not seeing how this function is working. I could not even derive it. I need to fix some $ n $ to work? And to see the discontinuity of $ f '(x) $ after that? Can anyone give me any tips? I am not knowing how to work with this exercise and do not know where to start.","['sequences-and-series', 'calculus', 'real-analysis']"
636270,Proof that the product of two differentiable functions is also differentiable,"Let $f:A\subset \mathbb{R^p}\rightarrow\mathbb{R^q}$ and $\phi:A\subset \mathbb{R^p}\rightarrow\mathbb{R}$ differentiable in $c\in A$ . I have to prove that $g(x)=\phi (x)f(x)$ is differentiable, where $Dg(c)u=(D \phi(c)u)f(c)+\phi(c)(Df(c)u)$ for any $u \in \mathbb{R^p}$ . I have done the following: $g(x)=\phi (x)f(x)$ is differentiable if and only if: $$\lim_{x\to c}\frac{||g(x)-g(c)-Dg(c)(x-c)||}{||x-c||}=0$$ $$\lim_{x\to c}\frac{||\phi (x)f(x)-\phi (c)f(c)-D \phi(c)(x-c))f(c)-\phi(c)(Df(c)(x-c))||}{||x-c||}=\lim_{x\to c}\frac{||\phi (x)f(x)-\phi (c)f(c)-\phi(c)f(x)+ \phi(c)f(x)-D \phi(c)(x-c))f(c)-\phi(c)(Df(c)(x-c))||}{||x-c||}< \lim_{x\to c}\frac{|\phi(c)| ||f(x)-f(c)-(Df(c)(x-c))||}{||x-c||} +
 \lim_{x\to c}\frac{||\phi (x)f(x)-\phi(c)f(x)-D \phi(c)(x-c))f(c)||}{||x-c||}<|\phi(c)|\lim_{x\to c}\frac{ ||f(x)-f(c)-(Df(c)(x-c))||}{||x-c||} + \lim_{x\to c}\frac{||\phi (x)f(x)-\phi(c)f(x)-D \phi(c)(x-c))f(c)||}{||x-c||}<\lim_{x\to c}\frac{||\phi (x)f(x)-\phi(c)f(x)-D \phi(c)(x-c))f(c)||}{||x-c||}...$$ What can I do with the second part? thank you very much!","['multivariable-calculus', 'calculus']"
636276,Showing the consistency of an equivalence relation over *,"Let $E$ be an equivalence relation on the set of all ordered pairs of non-negative integers ($N\times N$). It is defined as $$(a,b)E(x,y) \Longleftrightarrow a+y = b+x$$ Multiplication ($*$) is defined as $$(a,b)*(x,y) = (ax+by, ay+bx)$$ Without using substraction or division, how can I show that $E$ is consistent with $*$ ? By consistent I mean that if $(a,b)E(a', b')$ and $(x,y)E(x',y')$ then $(a,b)*(x,y)E(a',b')*(x',y')$ Right now I have laid down all the information about the problem: Let $(a,b)$, $(a',b')$, $(x,y)$ and $(x',y')$ $\in N \times N$. Suppose $(a,b)E(a',b')$ and $(x,y)E(x',y')$. We have:
$$(a,b)E(a',b') \Longleftrightarrow a+b'= b+a'$$
$$(x,y)E(x',y') \Longleftrightarrow x+y'= y+x'$$
and I want to show that the following is true 
$$(a,b)*(x,y)E(a',b')*(x',y')$$
The expansion of the preceding statement gives this:
$$ax+by+b'x'+a'y'=^{?} bx+ay+a'x'+b'y'$$ I spent some time thinking about it but I can't find a way to do that.
Thanks for the help!",['elementary-set-theory']
636319,Midpoint approximation over/under estimation,So left handed approximation underestimates the area under a increasing curve and over estimates for decreasing curves. And right handed approximation overestimates for increasing curves and underestimates for decreasing curves. My question is regarding midpoint approximations of area under a curve for both increasing and decreasing functions. There doesnt seem to be an obvious answer to this without evaluating the integral itself and comparing. So does the midpoint approximation rule over or under estimates a increasing and decreasing function?,"['approximation', 'calculus', 'integration']"
636323,A question on finitely generated abelian groups,"Let $M$ be the torsion subgroup of a finitely generated abelian group $A$ with $M = 0$. Then it is not difficult to prove that there exists a $r \in \mathbb{Z}_{\geq 0}$ such that $A \cong Z^r$ . Furthermore, this $r$ is unique. Now, suppose that we are working with a finitely generated abelian group $B$, of which the torsion subgroup does not neccesarily need to be just $0$. How can we prove that $$B \cong N \times \mathbb{Z}^r$$ Where $N$ is the torsion subgroup of $B$. Surely the idea must be to consider the group $B/N$. Then $B/N \cong \mathbb{Z}^r$ for some $r \in \mathbb{Z}_{\geq 0}$. However, this doesn't directly imply that $B \cong N \times \mathbb{Z}^r$ or does it? How would I start solving this?",['group-theory']
636327,Intuition for differentiating beneath the integral,"I apologize in advance for a vague question. There is a theorem: If both $f(x,s)$ and $\partial _sf(x,s)$ are continuous in $x$ and
  $s$, then $$\partial_s\int_a^bf(x,s)\,dx=\int_a^b
 \partial_sf(x,s)\,dx$$ If in addition $\int_{-\infty}^\infty \partial_s f(x,s)\,dx$ converges uniformly
  in a neighborhood of $s_0$, then $$\partial_s \int_{-\infty}^\infty
 f(x,s)\,dx=\int_{-\infty}^\infty \partial_s f(x,s)\,dx.$$ The proof I know relies on integrating in $s$ and then switching the order of integration by uniform convergence. But beyond the mechanics of the proof, I am trying to develop an intuition for this fact. It does not seem intuitive to me that $$\partial_s \int f(x,s)\,dx = \int \partial_s f(x,s)\,dx.$$ I think the reason why it seems surprising to me is that you're integrating with respect to a different variable than the integration. I am familiar with some real analysis and measure theory, so feel free to pitch an answer on that level.","['soft-question', 'calculus', 'intuition', 'real-analysis']"
636332,The origin of the function $f(x)$ notation,What are the historical origins of the $f(x)$ notation used for functions? That is when did people start to use this notation instead of just thinking in terms of two different variables one being dependent on the other? Any references would be appreciated.,"['notation', 'math-history', 'functions']"
636351,Quadratic covariation of Itô processes,"I haven't found any similar question in the forum, so I trust some of you will find this thought-provoking (at the very least). Perhaps you can help me. Let's consider first the two following stochastic differential equations:
$$dX_t = (6+3X_t)dt + (2+X_t)dB_t, X_0 = 1$$
$$dY_t = 3Y_tdt + Y_tdB_t, Y_0 = 1$$ Of course $B_t$ is a standard Brownian Motion started at zero. Now the question is: how can I compute the quadratic covariation of these two processes, and then obtain its expectation? That is, how can I compute $E\left(\left<X,Y\right>_t\right)$. Regards and thanks in advance.","['stochastic-processes', 'probability-theory', 'quadratic-variation', 'stochastic-calculus', 'brownian-motion']"
636367,Linear Independence easy question,"I have these vectors $B = \{u, v, w\}$ with $$u = (-1, 1, -1),\, v = (19, 10, -9),\, w = (-1, x, y)$$ And i want to prove that these vectors are linearly independent. I have no problem to proove that three vectors without unknown variables are linearly indepedent but i have difficulties in this that i have two unknown variables $(x, y)$. I find that $\det(B)$ is $-28x - 29y -1$ but i do not know how this helps. The actual question is to prove that B forms a basis in R3 PS. New to Linear Algebra, dont shoot !",['linear-algebra']
636398,Klein four-group as automorphism group of a graph.,Every finite abstract group is the automorphism group of some graph. Can someone show an example of a graph whose automorphism group is isomorphic to the Klein four-group?,"['graph-theory', 'group-theory']"
636423,Proving the Exterior (Outer) Measure of Rectangle is Equal to Volume,"I'm having trouble understanding one step of Stein and Shakarchi's proof that the exterior measure of a rectangle is equal to its volume. The proof I reference is part of Example 4 in section 1.2 of the Real Analysis book. To summarize the proof, first they show $|R| \leq m_*(E)$. To show the reverse inequality, the book divides $\mathbb{R}^d$ into cubes of side length $1/k$. Then, it considers the sets $\mathcal{Q}$ and $\mathcal{Q}'$. The claim is that if $\mathcal{Q}$ consists of the finite collection of all cubes entirely contained in rectangle $R$ and $\mathcal{Q}'$ the finite collection of all cubes that intersect the complement of $R$, then there are $O(k^{d-1})$ cubes in $\mathcal{Q}'$, where $O$ denotes standard Big-Oh notation. From here, using the volumes of the cubes, the text uses: $$
\sum_{Q \in (\mathcal{Q} \cup \mathcal{Q}')} |Q| \leq |R| + O(1/k)
$$ to complete the proof. Question: Why are there $O(k^{d-1})$ cubes in $\mathcal{Q}'$? I interpret $\mathcal{Q}'$ to be the set of all cubes in the complement, i.e. $\mathcal{Q} \cup \mathcal{Q}' = \mathbb{R}^d$. If this is the case, then I believe $\mathcal{Q}'$ is not a finite collection of cubes and a Big-Oh approximation of its size doesn't make sense.","['measure-theory', 'real-analysis']"
636450,Product of Summations for All Subsets,"We have a set $X$ of $n$ integers $\{$$x_1$, $x_2$, .. , $x_n$$\}$, for which there are $2^n$ total subsets. The summation $s$ of a subset $X'$ is simply the sum of all integers present in $X'$, denoted by $s(X')$. What is a simplified formula of the product of the individual summations of all nonempty subsets of the set $X$? For example, let $X$ contain $\{1, 2, 3\}$. Our nonempty subsets $X'$ are $\{1\}$, $\{2\}$, $\{3\}$, $\{1, 2\}$, $\{1, 3\}$, $\{2, 3\}$, $\{1, 2, 3\}$, and their summations $s$ are $1, 2, 3, 3, 4, 5,$ and $6,$ respectively. We wish to find a simplified way of representing their product $(1)(2)(3)(3)(4)(5)(6)$ without having to multiply it all out.","['products', 'summation', 'discrete-mathematics', 'elementary-set-theory']"
636456,Are there real numbers such that ...,"$x^2 + xy =3$ and $x - y^2 = 2$?
I graphed it and there are no intersections so obviously there is no real number solutions, but is there a ""mathier"" (read algebraic) way to prove this?",['algebra-precalculus']
636479,Conjecture: the sequence of sums of all consecutive primes contains an infinite number of primes,"Starting from 2, the sequence of sums of all consecutive primes is: $$\begin{array}{lcl}2 &=& 2\\
    2+3 &=& 5 \\
    2+3+5 &=& 10 \\
    2+3+5+7 &=& 17 \\
    2+3+5+7+11 &=& 28 \\
    &\vdots&
\end{array}
$$
If the $n^\text{th}$ prime is $P_n$, then we can write $S_n=\sum_{i=1}^n P_n$. I conjecture that the sequence $S_n$ contains an infinite number of primes. I doubt I'm the first person ever to think this, but I cannot find reference to the idea, nor can I conceive a proof, nor a disproof.  Computationally, it can be verified that
    $$S_{13,932}=998,658,581=P_{50,783,012},$$
and the sequence shows no sign of slowing down. Can you prove the sequence $S_n$ contains an infinite number of primes?","['sequences-and-series', 'number-theory', 'elementary-number-theory', 'prime-numbers', 'summation']"
