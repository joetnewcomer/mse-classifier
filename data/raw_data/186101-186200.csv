question_id,title,body,tags
3445421,Limiting distributions (attractors) associated with the discrete difference operator - application to error detection,"Let $Z = (Z_1,Z_2,\cdots)$ be a sequence of real numbers, either deterministic or with the $Z_i$ 's being i.i.d. random variables. As usual, the successive discrete difference sequences are recursively defined as follows: $D_{0} = Z$ If $n\geq 0$ , then $D_{n+1} = (D_{n+1,1}, D_{n+1,2}, D_{n+1,3},\cdots)$ with $D_{n+1,k}=D_{n,k}-D_{n,k+1}$ . For a given $n$ , all $D_{n,i}$ 's have the same distribution. If the $Z_i$ 's are i.i.d, then $\mbox{E}(D_{n,i}) = 0$ if $n>0$ and $\mbox{Var}(D_{n,i})=\mbox{Var}(Z_1)\cdot (2n)!/(n!)^2$ . This is based on the fact that $$D_{n,i} =\sum_{k=0}^n (-1)^k \frac{n!}{k!(n-k)!} Z_{k+i} .$$ Thus $$\mbox{Var}(D_{n,i})=\mbox{Var}(Z_1)\cdot\sum_{k=0}^n \Big(\frac{n!}{k!(n-k)!}\Big)^2 =\mbox{Var}(Z_1)\cdot \frac{(2n)!}{(n!)^2}.$$ We define the normalized vector $C_n$ as $C_n = D_n/\sqrt{\mbox{Var}(D_{n,1})}$ . 1. Question Find at least one attractor distribution (also called limit distribution) $F$ such as when $n\rightarrow\infty$ , we have $P(C_{n,i} <z) \rightarrow F(z)$ as $n\rightarrow\infty$ . For a given $n$ , all $C_{n,i}$ 's have the same distribution. Of course $F$ may depend on $Z$ .  Note that the she support domain for $F$ is infinite, see section 4. 2. Experimentation I played with various deterministic sequences that look just like random variables, namely $Z_k = \{b^k \log 2\}$ where $b>1$ , and the brackets represent the fractional part function. Keep in mind that in this context, the $Z$ sequence is auto-correlated: the correlation between two successive values is equal to $1/b$ if $b$ is an integer, and lag- $m$ autocorrelation is equal to $1/b^m$ (so the autocorrelations are decaying exponentially fast). Below is the limit distribution obtained if $b=1.1$ . This is a non-standard case. The chart below features its empirical percentile distribution: And now the typical limit distribution (actually the percentile distribution) for a standard case, $b=3$ : It would be interesting to see what we obtain if $Z_k$ is uniform on $[0, 1]$ , or normal, assuming the $Z_i$ 's are i.i.d. this time. 3. Characteristic function It is easy to verify that the characteristic function of $C_{n,1}$ is given by $$\mbox{E}[\exp(itC_{n,1})] = \prod_{k=0}^n\mbox{E}\Big[\exp\Big(it (-1)^n \cdot \frac{n!\sqrt{(2n)!}}{k!(n-k)!n!} \cdot Z_k\Big)\Big]$$ where the $Z_k$ 's are i.i.d. and have same distribution as $Z_1$ . Thus is $Z_1$ has a stable distribution , then $C_{n,1}$ will have a distribution from the same family. This is the case if $Z_1$ has a normal distribution. 4. The support domain of the limiting distribution is infinite In order to prove this, let's focus on the maximum value for $D_{n,1}$ , assuming $Z_1$ is Bernouilli of parameter $1/2$ . The maximum is equal to $2^{n-1}$ . Also $C_{n,1}=D_{n,1}/\sqrt{\mbox{Var}(D_{n,1})} = (n!/\sqrt{(2n)!})\cdot D_{n,1}$ . Using the Stirling approximation for the factorials, the maximum for $C_{n,1}$ is thus of the order $n^{1/4}$ and tends to infinity as $n$ tends to infinity. The same logic can be used for the minimum. And to get a general proof, the final step is to consider an arbitrary distribution for $Z_1$ , rather than the Bernouilli. Some restrictions may apply on the distribution of $Z_1$ for this result to be valid in the general case. I tried with a uniform distribution on $[0, 1]$ for $Z_1$ , and at this stage it is still unclear to me if the support domain is infinite. In this case, it looks like $\max D_{n,1} \propto \alpha^n$ with $\alpha$ very close to $2$ , but possibly smaller. We need $\alpha = 2$ for the support domain of $C_{\infty,1}$ to be infinite. 5. Potential application Successive applications of the discrete difference operator are very useful to identify errors or outliers in a sequence of numbers, as these errors propagate exponentially fast across $D_n$ , as $n$ increases. The theory discussed here could lead to a statistical test to identify these errors. Specifically, the statistic of the test could be a combination of the following ratios: $$T_n =\frac{\mbox{Var}(D_{n+1,i})}{\mbox{Var}(D_{n,i})} = \frac{2(2n+1)}{n+1}, n = 0, 1, 2,\cdots.$$ Any departure from the expected value of $T_n$ , for $n=0, 1$ or $2$ , could suggest that there is some unexpected pattern in your data, or possibly that your data points are auto-correlated. Indeed, I noticed this effect when comparing i.i.d $Z_i$ 's with some that are auto-correlated, as in the examples pictured in this discussion (though the discrepancy tends to disappear as $n\rightarrow \infty$ ). Finally, keep in mind that as $n\rightarrow\infty$ , the lag- $m$ auto-correlation computed on the $C_{\infty}$ sequence is strong for $m<6$ . The table below gives a rough approximation. 6. Conclusions In many cases where $Z_1$ has a continuous distribution, it looks like the limiting distribution will be normal. If $Z_1$ has a discrete distribution, we have several possibilities, see below. Note that the references in the list below are not related to the difference operator, but rather to some other systems that produce the same kinds of limiting distributions. So, the various cases below apply to a large class of chaotic systems: it is a general summary for all these systems. The limiting distribution can be normal or some other regular smooth
distribution, either well known or not The limiting distribution can look very smooth yet be differentiable nowhere, and yet very close to normal or related (see here , here , and here ) The limiting distribution can look very un-smooth The limiting distribution can be very peculiar, as in the fist chart    in this article, yet computable The limiting distribution can be a piecewise linear mix of uniform distributions or some other weird mix The limiting distribution can be very wild, totally chaotic as in the picture below (as if it was a Fourier series with a bunch of
missing terms) not unlike the Weierstrass function . In some of the chaotic cases, the distribution has a fractal behavior (see here .) The above figure shows the wild percentile distribution for $C_{\infty, 1}$ if $Z_k = \{ (e/2)^{k}\cdot\log 2\}$ , corresponding to the last case in the above bullet list (the brackets represent the fractional part function). This taxonomy applies to many of the systems I have studied recently.  My next step is to create a taxonomy of all the potential attractors (the limiting distributions) for this system, and other systems. Below is one last example where the $Z_i$ 's are very heavily auto-correlated, with very long (infinite) range strong auto-correlation. It corresponds to $Z_k = \{ne\}$ .","['probability-distributions', 'combinatorics', 'discrete-mathematics', 'sequences-and-series', 'dynamical-systems']"
3445473,How many batteries will be working after 280 minutes?,"A study of data collected at a company manufacturing flashlight
  batteries shows that a batch of 8000 batteries have a mean life of 250
  minutes with a standard deviation of 20 minutes. Assuming a Normal
  Distribution, estimate: How many batteries will continue working after 285 minutes? This is my answer to this question does it look correct or are there any improvements I can make? Batch: 8000
Mean: 250 minutes 
SD: 20 minutes


(285-250)/20 = 1.75
Z-Score of 1.75 = .4599
(.5-4599)*8000 = 320.8 batteries will be working","['statistics', 'normal-distribution', 'probability']"
3445492,How to prove this for the spectrum of a self-adjoint operator?,"Let $T$ be a bounded self-adjoint operator. Prove: A number $\lambda \in \mathbb{R}$ belongs to the spectrum of $T$ if and only if $\mathbb{1}_{(\lambda - \epsilon, \lambda + \epsilon)} (T)$ is non-zero for any $\epsilon > 0$ . I think I should approximate the characteristic function of the interval with a continuous function. But I'm still not sure how to prove this. Help is appreciated.","['operator-theory', 'functional-analysis']"
3445503,Let $U\subseteq X$ be an open set such that $\bigcap\limits_{n=1}^\infty A_n\subseteq U$. Show that $A_m\subseteq U$ for some $m\geq 1$,"Let $X$ be a compact topological space and let $A_1,A_2,...$ be closed sets in $X$ with $A_1\supseteq A_2\supseteq...$ . Let $U\subseteq X$ be an open set such that $\bigcap\limits_{n=1}^\infty A_n\subseteq U$ . Show that $A_m\subseteq U$ for some $m\geq 1$ . My attempt: Since $X$ is compact we know that each $A_n$ is also compact. Suppose on the contrary that $A_m\nsubseteq U$ for all $m\geq 1$ . Then $\bigcap\limits_{n=1}^\infty A_n\subset A_m$ , but $A_m\nsubseteq\bigcap\limits_{n=1}^\infty A_n$ . Since the countable intersection of closed sets is closed, $\bigcap\limits_{n=1}^\infty A_n$ is compact. At this point I'm out of ideas to go further, and I'm not sure where to go from here.","['general-topology', 'compactness']"
3445533,"minimizing distance between $A_t$ & $B_t$ , intersections of tangent","So I have a function $$f(x)=\frac{a^{n+1}}{x^n} $$ It's derivative is: $$f'(x)=-n\frac{a^{n+1}}{x^{n+1}} $$ Where $a>0, n \in \mathbb{N}.$ So let $t$ be a tangent to the graph of function $f$ .
So let $A_t$ be point of intersection on x-axis and $B_t$ on y-axis of the before mentioned tangent $t$ . So tangent has the equation: $y-y_0=f'(x_0)(x-x_0)$ I have to find such tangent t, that the distance between $A_t  (A,0)$ and $B_t(0,B)$ will be minimal. So the function will be: $d=A^2+B^2$ And i have to get either $A$ or $B$ as a function of the other, so that i can calculate the minimum. So my initial idea; should i just insert $A_t  (A,0)$ , $B_t(0,B)$ into the tangent equation, or is that the wrong approach.
Any help would be appreciated. Thank you in advance.","['calculus', 'derivatives', 'real-analysis']"
3445574,"Given joint density $f_{X,Y}(x,y)$ find covariance, correlation, and specific expectations","Given the joint density of $X$ and $Y$ , $$f_{X,Y}(x,y)=\begin{cases}e^{-y}&\text{for }0\le x<y<\infty\\0&\text{otherwise}\end{cases},$$ (a) find the covariance and correlation of $X$ and $Y$ ; (b) find $E[X\mid Y=y]$ and $E[Y\mid X=x]$ ; and (c) find $E[X]$ and $\mathrm{Var}[X]$ . I know that $$\mathrm{Cov}[X,Y]=E[XY]-E[X]E[Y],$$ $$\mathrm{Corr}[X,Y]=\frac{\mathrm{Cov}[X,Y]}{\sqrt{\mathrm{Var}[X]\mathrm{Var}[Y]}},\text{ and}$$ $$E[XY]=\int_0^\infty\int_0^yxye^{-y}\,\mathrm dx\,\mathrm dy=3,$$ so theoretically, I should be able to compute the covariance by first finding the marginal densities for $X$ and $Y$ , then computing the expectations of $X$ and $Y$ . Alternatively, since $$E[X]=E[E[X\mid Y=y]],$$ I can accomplish the same thing by finding the conditional densities and their expectations. However, the ordering of the questions has me wondering: (1) Can I compute the covariance and correlation directly knowing just the joint PDF? As for finding the (necessary?) expectations, I've run into a complication... The marginal density for $Y$ is $$f_Y(y)=\int_0^ye^{-y}\,\mathrm dx=xe^{-y}\bigg|_0^y=\begin{cases}ye^{-y}&\text{for }y\ge0\\0&\text{otherwise}\end{cases}$$ so the conditional density of $X$ given $Y=y$ is $$f_{X\mid Y}(x\mid y)=\begin{cases}\frac{e^{-y}}{ye^{-y}}=\frac1y&\text{for }0\le x<y<\infty\\0&\text{otherwise}\end{cases}$$ Then the conditional expectation of $X$ given $Y=y$ is $$E[X\mid Y=y]=\int_0^y\frac xy\,\mathrm dx=\frac{x^2}{2y}\bigg|_0^y=\frac y2$$ and so $$\boxed{E[X]=E[E[X\mid Y=y]]=E\left[\frac y2\right]=\frac y2}$$ Similarly, the marginal density for $X$ is $$f_X(x)=\int_x^\infty e^{-y}\,\mathrm dy=-e^{-y}\bigg|_x^\infty=\begin{cases}e^{-x}&\text{for }x\ge0\\0&\text{otherwise}\end{cases}$$ so the conditional density of $Y$ given $X=x$ is $$f_{Y\mid X}(y\mid x)=\begin{cases}\frac{e^{-y}}{e^{-x}}=e^{x-y}&\text{for }0\le x<y<\infty\\0&\text{otherwise}\end{cases}$$ and the expectation of $Y$ given $X=x$ would be $$E[Y\mid X=x]=\int_x^\infty ye^{x-y}\,\mathrm dy=-(y+1)e^{x-y}\bigg|_x^\infty=x+1$$ and so the expectation of $Y$ is $$\boxed{E[Y]=E[E[Y\mid X=x]]=E[x+1]=x+1}$$ The complication is that I get different expectations for $X$ and $Y$ when trying to verify their values using the corresponding marginal densities: $$\boxed{E[X]=\int_0^\infty xe^{-x}\,\mathrm dx=1\\E[Y]=\int_0^\infty y^2e^{-y}\,\mathrm dy=2}$$ I thought that perhaps the problem lies with the supports of $f_X$ and $f_Y$ . We have $0\le x<y<\infty$ to start with, so the marginal densities could instead be $$f_X(x)=\begin{cases}e^{-x}&\text{for }0\le x<y\\0&\text{otherwise}\end{cases}$$ $$f_Y(y)=\begin{cases}ye^{-y}&\text{for }x\le y\\0&\text{otherwise}\end{cases}$$ But even then, $$\boxed{E[X]=\int_0^yxe^{-x}\,\mathrm dx=1-(y+1)e^{-y}\neq\frac y2\\E[Y]=\int_x^\infty y^2e^{-y}\,\mathrm dy=(x^2+2x+2)e^{-x}\neq x+1}$$ (2) Which expectations are correct? Why the discrepancy between methods?","['expected-value', 'statistics', 'conditional-expectation']"
3445583,Prove that $\sum_{i=1}^{n}\frac{1}{\left(n+i\right)^{2}}\sim\frac1{2n}$,"I would like a proof of the asymptotic relationship $$\sum_{i=1}^{n}\frac{1}{\left(n+i\right)^{2}}\sim\frac1{2n}$$ without assuming that the sum is a Riemann sum. This problem arose from Question 1909556 , which asks about the Riemann sum of $\int_1^2\frac1{x^2}\ \mathrm{d}x=\lim_{n\to\infty}\frac1n\sum_{i=0}^n\frac{n^2}{(n+i)^2}=\frac12$ . It features a nebulous clue that $\lim_{n\to\infty}\frac{\sum_{i=1}^{2n}\frac1{i^2}-\sum_{i=1}^{n}\frac1{i^2}}{1/n}=\frac12$ . I can't figure out how this clue works but one way of showing it could be with the asymptotic relationship is true, and from calculations, it seems to work. But I can't find any feasible way of proving it without assuming the value of the integral. I would like to avoid assuming that the sum is simply the integral so that I can prove the integral from the sum. It also seems like a fairly simple relationship, so I would imagine there could be a nice proof.","['square-numbers', 'asymptotics', 'sequences-and-series']"
3445644,How to prove the Heine-Borel property for Fréchet spaces?,"The Heine-Borel theorem states that a subspace of Euclidean space is compact if and only if it is closed and bounded. This theorem does not hold as stated for general metric and topological vector spaces, but some infinite-dimensional Fréchet spaces do have the so-called Heine-Borel property, and we call them Montel spaces. I already know the proof of the Heine-Borel theorem for general Euclidean spaces, but how can I prove this property for those Fréchet spaces, say, $\mathcal{S}(\mathbb{R}^n)$ ? The definition of Montel space here gives some ideas. I think the Arzelà-Ascoli theorem may be useful, and the Montel's theorem should play a role, but I have no more idea about the complete proof. Can anyone show me the proof for the Schwartz space $\mathcal{S}(\mathbb{R}^n)$ ?","['harmonic-analysis', 'topological-vector-spaces', 'real-analysis', 'functional-analysis', 'partial-differential-equations']"
3445649,Proving the regular expression identity $(a(a + b)^*)^* = (ab^*)^*$,"I'm struggling to prove the regular expression identity $$(a(a+b)^*)^* = (ab^*)^*$$ The recommendation is to use induction on the star operator. My strategy was to first prove that $$(a(a+b)^*)^* \subseteq (ab^*)^*$$ and then prove $$(ab^*)^* \subseteq (a(a+b)^*)^*$$ I started with the first one, using induction on the inner start of the left side. So the base case is: $$(a\epsilon)^* \subseteq (ab^*)^*$$ Which holds. Then For the inductive hypothesis I assume: $$(aX)^*\subseteq(a)$$ And then set out to prove $$(aX(a+b)^*)^* \subseteq (ab^*)^*$$ Now I know there is a set identity that would make it sufficient to prove: $$(aX(a+b)^*) \subseteq (ab^*)$$ Then by the I.H: $$(a(ab^*)^*(a+b)^*) \subseteq (ab^*)$$ And this is as far as I've been able to get! I don't know how to manipulate what I have here to reach the goal. Am I on the right track? Any hints, identities that could be useful to help me finish this proof. Thanks.","['induction', 'regular-expressions', 'discrete-mathematics']"
3445736,Is it possible to generate a composite number with no information about its factors?,"Are there functions or algorithms which can generate integers which are necessarily composite, yet not yield any information about what factors it has? For example, $f(x):=x^2-1$ is not what I'm after, as you immediately know it has factors of $x-1$ and $x+1$ . Closer are Sierpinski numbers and primefree sequences , but my understanding is that both of them are fundamentally built on top of small, finite covering sets, such that it would be trivial to spit out an element of that covering set which would divide any of those numbers. The only approach I know of that would work is to simply pick some arbitrarily large number at random and do a Miller-Rabin test or similar to confirm compositeness and then return it, so I'll go ahead and say that's not what I'm looking for either. It's hacky, and ideally the generating function would work for any input, not just spot-check and prune.","['number-theory', 'integers', 'prime-factorization', 'elementary-number-theory']"
3445739,Integral models of $p$-divisible groups,Let $G$ be a $p$ -divisible group over $\mathbb{Q}_p$ . Suppose that $G_{\mathbb{C}_p}$ has a model over $\mathcal{O}_{\mathbb{C}_p}$ . Does $G$ have a model over $\mathbb{Z}_p$ ?,"['number-theory', 'p-adic-number-theory', 'algebraic-geometry', 'galois-representations']"
3445741,Formalizing a proof for $ \sum_{n=0}^\infty \sum_{k=n}^{\infty} a_k = \sum_{n=0}^{\infty} (n+1)a_n$,"Let $(a_n)_n$ be a sequence of non-negative real numbers. I fail to see a formal proof of the following (conjectured) equality: $$
\sum_{n=0}^\infty \sum_{k=n}^{\infty} a_k = \sum_{n=0}^{\infty} (n+1)a_n. 
$$ Roughly speaking the claim follows from expanding the LHS: $$
\sum_{n=0}^\infty \sum_{k=n}^{\infty} a_k = \sum_{k=0}^{\infty} a_k + \sum_{k=1}^{\infty} a_k + \ldots + \sum_{k=i}^{\infty} a_k + \ldots 
$$ Now it is easy to see that each term $a_k$ appears in exactly $(k+1)$ terms in RHS (e.g. writing column-wise the different sums), so it is intuitively clear that it has to be $\sum_{n=0}^{\infty} (n+1)a_n$ . However I am not satisfied of this informal hand-waving proof. Any hints/helps to make it formal? Thanks.","['calculus', 'sequences-and-series', 'summation', 'real-analysis']"
3445751,Is this function a non-zero function?,"Assume that $(a_i,b_i)^T \in \mathbb{R}^2\setminus \{ (0,0) \}$ for $i=1, \dots, n$ and let $s_1, \dots, s_n \in \mathbb{R}\setminus \{0 \}$ be distinct constants. Define the function $$
f(t) = \sum_{i=1}^{n} a_i\sin(s_it) - b_i \cos(s_it).
$$ Is this a nonzero function? It looks very much nonzero to me but somehow I cannot prove this. One just has to find a value $t$ such that $f(t) \neq 0$","['trigonometry', 'functions', 'analysis']"
3445764,The real projective space $\mathbb{R}P^{2}$ is the adjunction space $D^{2} \cup_{f} S^{1}$.,"I would like to show that the 2-dimensional real projective space $\mathbb{R}P^{2}$ is homeomorphic to the adjunction space $D^{2} \cup_{f} S^{1}$ where $f : S^{1} \longrightarrow S^{1}$ is the function given by $f(z) = z^{2}$ (considering $S^{1}$ as a subspace of $\mathbb{C}$ ). I know this question has already been asked here , but no one gave a complete answer for it. This is a quite important fact for calculating the homology groups of $\mathbb{R}P^{2}$ so I would like to have a rigorous proof of it. My attempt: 1) Considering $\mathbb{R}P^{2}$ as the quotient space $D^{2}/\sim$ obtained by identifying antipodal points on the boundary $S^{1}$ of $D^{2}$ , I tried to construct first a function from the disjoint union $D^{2} \amalg S^{1}$ into $\mathbb{R}P^{2}$ by defining two continuous functions $g : D^{2} \longrightarrow \mathbb{R}P^{2}$ and $ h : S^{1} \longrightarrow \mathbb{R}P^{2}$ given by $g(z) = [ z^{2} ]$ and $h(z) = [z]$ (where the brackets [ ] denote the equivalence class of an element of $D^{2}$ in the quotient space $\mathbb{R}P^{2}$ ). 2) Then, it's easy to see that the continuous function $F : D^{2}\amalg S^{1} \longrightarrow \mathbb{R}P^{2}$ given by $F(x) = g(x)$ for $x$ in the copy of $D^{2}$ in the disjoint union $D^{2}\amalg S^{1}$ and $F(x) = h(x)$ for $x$ in the copy of $S^{1}$ in the disjoint union $D^{2}\amalg S^{1}$ is constant on the fibers of the quotient map $p : D^{2}\amalg S^{1} \longrightarrow D^{2} \cup_{f} S^{1}$ , so we obtain a continuous function $G : D^{2} \cup_{f} S^{1} \longrightarrow \mathbb{R}P^{2}$ such that $G \circ p = F$ . 3)Since $D^{2} \cup_{f} S^{1}$ is compact and $\mathbb{R}P^{2}$ is Hausdorff, $G$ would be the desired homeomoprhism if it was bijective, but if we take two antipodal points $x$ and $-x$ in the copy of $S^{1}$ in the disjoint union $D^{2} \amalg S^{1}$ , then these two points are not equivalent in the adjunction space $D^{2} \cup_{f} S^{1}$ , so that $p(x) \neq p(-x)$ , but, by definition of the relation $\sim$ on $D^{2}$ used to obtain $\mathbb{R}P^{2}$ , we have that $G(p(x)) = F(x) = h(x) = [x] = [-x] = h(-x) = F(-x) = G(p(-x))$ , showing that $G$ is not injective. I tried to modify my original functions $g : D^{2} \longrightarrow \mathbb{R}P^{2}$ and $ h : S^{1} \longrightarrow \mathbb{R}P^{2}$ to fix this issue without any success. Am I in the right direction or should I try something different? By the way, I can use any of the equivalent definitions of $\mathbb{R}P^{2}$ (by identifying lines through the origin in $R^{3} \setminus \{0\}$ , antipodal points in $S^{2}$ or antipodal points in the boundary of $D^{2}$ as I did in my attempt). Thank you very much in advance and any hints, ideas, or feedback would be greatly appreciated.","['general-topology', 'algebraic-topology', 'projective-space']"
3445768,Solve a nonlinear differential equation of the first order,"I am trying to solve a nonlinear differential equation of the first order that comes from a geometric problem ; $$x(2x-1)y'^2-(2x-1)(2y-1)y'+y(2y-1)=0.$$ edit1 I am looking for human methods to solve the equation edit2  the geometric problem was discussed on this french forum http://www.les-mathematiques.net/phorum/read.php?8,1779080,1779080 We can see the differential equation here http://www.les-mathematiques.net/phorum/read.php?8,1779080,1780782#msg-1780782 edit 3 I do not trust formal computer programs: look at Wolfram's answer when asked to calculate the cubic root of -1 https://www.wolframalpha.com/input/?i=%7B%5Csqrt%5B3%5D%7B-1%7D%7D%29+ .","['calculus', 'ordinary-differential-equations']"
3445791,"Find $\bigcup_{r\in R}\ \{(x,y): (x-r)^2 + (y+2r)^2 < r^2+1\}$","I have to obtain an union: $\bigcup_{r\in R}\ \{(x,y): (x-r)^2 + (y+2r)^2 < r^2+1\}$ I know this is a series of circles that is limited by two hyperbolic functions that are symmetrical in relation to $y = -2x$ . But how to get an union? I tried putting $y= \frac{1}{2}x$ into the circle's equation to get the intersection points but it works only for $r = 1$ . It would require putting $y = \frac{1}{2}x + b, \forall b \in R$ but it doesn't look as a significant simplification. I have also thought of representing an intersection as a translation from $(x, -2x)$ by a vector $\pm\ [\frac{\sqrt{5}}{5} \sqrt{r^2+1}, \frac{2\sqrt{5}}{5}\sqrt{r^2+1}]$ but it hasn't turned out to be usefull aswell. I'm running out of ideas. I'd be really thankful for any kind of help.","['envelope', 'multivariable-calculus']"
3445799,Balanced orientation of infinite graphs,"all I am asked to prove that given a loopless multigraph $G$ with vertices all having even degree, it is possible to orient the graph in such way that the in-degree of every vertex is equal to the out degree of every vertex.  Now for the case when the number of vertices finite, this is easy using Eulerian trails.  However, I am curious if the same statement holds for a graph having countably infinite vertices all with even degree and finitely many edges?  Any help in see why or why the infinite graph case is true would be awesome.","['graph-theory', 'combinatorics']"
3445825,Gradient of the Euclidean distance function,"If I calculate the Euclidean distance $d$ of two sets of $x,y$ points $A_{1} \in \mathbb{R}^{n\times2}$ and $B_{2} \in \mathbb{R}^{n\times2}$ as $d = \sqrt{(A_{1}-B_{1})^2 + (A_{2}-B_{2})^2}$ How would I calculate the gradient of $d$ ? Based on THIS answer I interpreted it as the gradient would be: $\nabla d =\frac{A-B}{\sqrt{(A_{1}-B_{1})^2 + (A_{2}-B_{2})^2}}$ However, this doesn't seem to give me the results I was expecting.","['euclidean-geometry', 'calculus', 'derivatives', 'vector-analysis']"
3445859,Simplifying $\frac{\sin(x)-\csc(x)}{\cos(x)}$,"I'm trying to simplify $\frac{\sin(x)-\csc(x)}{\cos(x)}$ , here is what I did: $\frac{\sin(x)-\csc(x)}{\cos(x)} = \frac{\sin(x)}{\cos(x)}-\frac{\csc(x)}{\cos(x)}=\frac{\sin^2(x)}{\sin(x)\cos(x)}-\frac{1}{\sin(x)\cos(x)}$ Where to get the last equality I multiplied $\frac{\sin(x)}{\cos(x)}$ by $\frac{\sin(x)}{\sin(x)}$ and used the identity $\csc(x)=\frac{1}{\sin(x)}$ . Now that there is a common denominator, I can continue as so: $\frac{\sin^2(x)}{\sin(x)\cos(x)}-\frac{1}{\sin(x)\cos(x)}=\frac{\sin^2(x)-1}{\sin(x)\cos(x)}=\frac{-\cos^2(x)}{\sin(x)\cos(x)}=\frac{-\cos(x)}{\sin(x)}=-\cot(x)$ And thus $\frac{\sin(x)-\csc(x)}{\cos(x)}=-\cot(x)$","['algebra-precalculus', 'proof-verification']"
3445884,Solve $2 \cos^2 x+ \sin x=1$ for all possible $x$,"$2\cos^2 x+\sin x=1$ $\Rightarrow 2(1-\sin^2 x)+\sin x=1$ $\Rightarrow 2-2 \sin^2 x+\sin x=1$ $\Rightarrow 0=2 \sin^2 x- \sin x-1$ And so: $0 = (2 \sin x+1)(\sin x-1)$ So we have to find the solutions of each of these factors separately: $2 \sin x+1=0$ $\Rightarrow \sin x=\frac{-1}{2}$ and so $x=\frac{7\pi}{6},\frac{11\pi}{6}$ Solving for the other factor, $\sin x-1=0 \Rightarrow \sin x=1$ And so $x=\frac{\pi}{2}$ Now we have found all our base solutions, and so ALL the solutions can be written as so: $x= \frac{7\pi}{6} + 2\pi k,\frac{11\pi}{6} + 2\pi k, \frac{\pi}{2} + 2\pi k$","['algebra-precalculus', 'proof-verification', 'quadratics', 'trigonometry']"
3445915,Why doesn't the limit in polar form imply the existence of the limit,"First, I'll give my definition of polar coordinates. Let $f:U\subseteq\mathbb{R}^2\to\mathbb{R}$ . Changing to polar coordinates we get $g:V\subseteq\mathbb{R}^2\to\mathbb{R},g(r,\theta)=f(r\cos\theta,r\sin\theta)$ , where $r>0$ and $\theta\in[0,2\pi)$ . Now my question is, why is this true: $$\lim_{(x,y)\to(0,0)} f(x,y)=L \Rightarrow \lim_{r\to0^+} g(r,\theta)=L $$ . But this isn't: $$\lim_{r\to0^+} g(r,\theta)=L\Rightarrow\lim_{(x,y)\to(0,0)} f(x,y)=L$$ I've seen various counterexamples of this last statement so I know it is false, however I can't really understand why it doesn't work, so any help will be appreciated. Thanks.","['multivariable-calculus', 'calculus', 'polar-coordinates']"
3445931,Calculus Critical Point Confusion - 2nd Derivative Confusion,"I am working with the function: $f(x) = 4x^5 - 3x^4 + 2x^3 - x^2 + 3$ . One of the critical points $f'(x) = 0$ occurs when $x=0$ . Checking the second derivative, we see that $f''(0) = -2$ . This would lead one to the conclusion that the point $x=0$ is a local maximum. However, looking at the plot of the function, we see that $x=0$ looks more like a saddle: So, I am just wondering how we could conclude it is a saddle point if the second derivative is (incorrectly?) saying it is a local max. Thanks.","['calculus', 'derivatives']"
3445955,"Showing ""directly"" that a fat Cantor set contains a non-measurable subset","Let $L$ be the Volterra set made by removing the middle open interval of length $5^{-\nu}$ at each step. In the limit, this will yield a perfect compact set of positive Lebesgue measure, i.e. a ""fat Cantor set"" (per a previous homework). I'm the grader in a basic measure theory class, and I'm writing a solution set for a homework problem that says to show that the fat Cantor set $L$ has a nonmeasurable subset. I know that in general, it's the case that any subset of $\mathbb{R}$ with positive measure contains a nonmeasurable subset, and that this can be shown through Steinhaus's Theorem, or at least I've seen it called that. My question in whether this could be done in this particular setting without reference to Steinhaus's Theorem, and without too much machinery. The text covers the ""construction"" of the Vitali set, but does nothing else with nonmeasurable sets until this problem. If there's a solution that doesn't stray too far from what the text has already covered, I wanna use that rather than the (in my opinion not at all intuitive) Steinhaus's Theorem. I have two thoughts for possible solutions. One thought was to devise a measure space equivalence between $L$ (equipped with a normalized measure) and an interval in $\mathbb{R}$ by factoring through $\{ 0 , 1 \}^{\mathbb{N}}$ , pulling a Vitali set back through this mapping to a nonmeasurable subset of $L$ , and then trying to rephrase all of this without the language of measure space isomorphism. I know this would be possible, but I feel like it'd take a lot of writing and it might not really be clear what's happening. Another thought I had was to just build a Vitali set contained entirely in $L$ , but I know that's not possible, since it'd imply that $\bigcup_{q \in \mathbb{Q}} (K + q) = \mathbb{R}$ , which I don't think should be possible if $K$ is nowhere-dense, since that'd mean $\mathbb{R}$ is a countable union of nowhere-dense sets. It's also possible that there's some other way of showing $L$ contains a non-measurable subset that's much simpler or intuitive than what I've considered. If so, I don't know what it'd be. Is there a more intuitive solution that I haven't considered here? Thanks in advance.","['measure-theory', 'lebesgue-measure', 'cantor-set', 'real-analysis', 'measurable-sets']"
3445958,Number of trials rolling six 6-sided dice to get 6 unique values,"I have six 6-sided dice, and I want to roll them until I have all six distinct values (assume them to be naturals 1-6), in no particular order. The naive strategy would be to roll all 6 dice until I have all 6 values. The expected number of trials until success is $\frac{6^6}{6!} = 64.8$ . A smarter strategy would be to roll a die until its value is distinct from the values of other dice already rolled. Eg. the first die gets rolled once, its value is 3, and then I roll the second one until its value is something else than 3, say 4. Then I roll the 3rd one until its value is neither 3 nor 4, etc. This way, the expected number of trials is $\sum_{i=1}^{6}\frac{6}{i} = 14.7$ . An even smarter strategy would be to first roll all 6 dice, then pick the duplicates ‒ all except one ‒ and roll these again. Repeat untill I have all 6 values. For example: first roll: 1 1 2 4 4 4 (the order doesn't matter) pick 1 4 4, leave 1 2 4 on the table roll the 3 dice (which had values 1 4 4) again repeat this until there are values 1-6 on the table Question: what is the expected number of trials with this strategy?","['expected-value', 'dice', 'probability']"
3445962,Show Process is a Poisson Process (Check 3 conditions).,"Suppose $A_t$ is a Process with rate 1. Show that $B_t=A_{2t}$ is a Poisson process. I have to check three conditions: A Poisson process with rate $\lambda$ is a random process $N=(N_t: t\geq 0$ ) so that: $1.$ $N$ is a counting process $2.$ $N$ has independent increments, for any positive integer $m$ , and any $t_0<t_1<...t_m$ , the increments $N_{t_1}-N_{t_0}$ , $N_{t_2}-N_{t_1}$ ,.... $N_{t_m}-N_{t_{m-1}}$ are mutually independent. $3.$ $N(t)-N(s)$ has Poisson( $\lambda(t-s))$ for $t \geq s$ . My main problem is with step 2. My Attempt: Since $A_t$ is a counting process, $B_t=A_{2t}$ must also clearly be a counting process. (Not sure if I should give more explanation...) I'm having trouble writing this part formally or properly, I think some of my logic is a bit confused or not precise: Let $m$ be a positive integer and consider $t_0<t_1<...t_m$ and $t_m< t_{m+1}< t_{m+2},....<t_{2m}$ . We have that the increments $A_{t_1}-A_{t_0}, A_{t_2}-A_{t_1},A_{t_3}-A_{t_2}, A_{t_4}-A_{t_3}...,A_{t_m}-A_{t_{m-1}}, A_{t_{m+1}}-A_{t_{m}},.....A_{t_{2m}}-A_{t_{2m-1}}$ are mutually independent since $A_t$ is a Poisson Process. Thus the increments, $A_{t_{2m}}-A_{t_{2m-1}}+A_{t_{2m-1}}-A_{t_{2m-2}}=A_{t_{2m}}-A_{t_{2m-2}}=B_{t_{m}}-B_{t_{m-1}}$ , $A_{t_{2m-2}}-A_{t_{2m-3}}+A_{t_{2m-3}}-A_{t_{2m-4}}=A_{t_{2m-2}}-A_{t_{2m-4}}=B_{t_{m-1}}-B_{t_{m-2}}$ , ..... $A_{2t_1}-A_{2t_0}=B_{t_{1}}-B_{t_{0}}$ are mutually independent. Hopefully this is right: Let $t \geq s$ . 
We have that $B(t)-B(s)=A(2t)-A(2s)=A(2t)-A(2t-1)+A(2t-1)-A(2t-2)+....A(2s+1)-A(2s)$ .
We know that $A(2t)-A(2t-1), ....A(s+1)-A(s)$ are all independent Poisson random variables with rate $1$ . We know that the sum of independent Poisson random variables is Poisson with the sum of the rates. There are $2(t-s)$ terms in the above sum. Thus: $A(2t)-A(2s)=A(2t)-A(2t-1)+A(2t-1)-A(2t-2)+....A(2s+1)-A(2s)$ is Poisson with rate $2(t-s)$ . Hence $B(t)-B(s)$ is Poisson with rate $2(t-s)$ for $t \geq s$ . Hence $B_t$ is a Poisson process with rate 2.","['statistics', 'real-analysis', 'stochastic-processes', 'probability-theory', 'probability']"
3445998,"Definition of ""identical in distribution""","While one can find various online references (and in books) for the definition of ""convergence in distribution"", I have several times today been stumped (as a non-probabilist) by the notion of ""identical/equal in distribution"" or ""identity/equality in distribution"".   This seems to be notated with $\stackrel{d}{=}$ .  Does it mean something like equal almost everywhere in analysis?  Are there any easy/obvious examples someone with only a first course in probability could understand? I could not find a reference in the books in front of me (unfortunately I didn't have Feller handy) nor on the internet, including this site - the latter of which I found pretty surprising.  But I'll be happy to close this if it's a dup.","['definition', 'probability-theory']"
3446005,Difference between domain and co-domain in sets?,"Let's say I have a subset of the Cartesian plane, for example: $\{(x, y) \in R \times R: 2x+3 > 5\}$ . If I am asked to find the co-domain of the following set, how would I do so? I know how to find the domain, which is done by finding all possible $(x,y)$ ordered pairs and then placing all the $x$ values in a set. But how is this different from the co-domain? Thank you very much!","['elementary-set-theory', 'relations']"
3446019,Differentiable manifolds as locally ringed spaces,"Let $X$ be a differentiable manifold.
Let $\mathcal{O}_X$ be the sheaf of $\mathcal{C}^\infty$ functions on $X$.
Since every stalk of $\mathcal{O}_X$ is a local ring, $(X, \mathcal{O}_X)$ is a locally ringed space.
Let $Y$ be another differentiable manifold.
Let $f\colon X \rightarrow Y$ be a differentiable map.
Let $U$ be an open subset of $Y$.
For $h \in \Gamma(\mathcal{O}_Y, U)$, $h\circ f \in \Gamma(\mathcal{O}_X, f^{-1}(U))$.
Hence we get an $\mathbb{R}$-morphism $\Gamma(\mathcal{O}_Y, U) \rightarrow \Gamma(\mathcal{O}_X, f^{-1}(U))$ of $\mathbb{R}$-algebras.
Hence we get a morphism $f^{\#} \colon \mathcal{O}_Y \rightarrow f_*(\mathcal{O}_X)$ of sheaves of $\mathbb{R}$-algebras.
It is easy to see that $(f, f^{\#})$ is a morphism of locally ringed spaces. Conversely suppose $(f, \psi)\colon X \rightarrow Y$ is a morphism of locally ringed spaces, where $X$ and $Y$ are differentiable manifolds and $\psi\colon \mathcal{O}_Y \rightarrow f_*(\mathcal{O}_X)$is a morphism of sheaves of $\mathbb{R}$-algebras.
Is $f$ a differentiable map and $\psi = f^{\#}$?","['sheaf-theory', 'differential-geometry']"
3446044,Limit of $\sum \frac1{1^2+2^2+\cdots+n^2}$,it is asked to compute the limit of the sequence $$ u_n :=\displaystyle\sum_{k=1}^n \dfrac{1}{1^2+2^2+\cdots+k^2}$$ I used the following $$1^2+2^2+\cdots+k^2=\dfrac{k(k+1)(2k+1)}{6}$$ to prove that $$u_n=24(H_n-H_{2n+1})+\dfrac{6}{n+1} + 18$$ where $(H_n)$ is the harmonic series. I don't if this is of any use to find the desired limit. Thanks,"['convergence-divergence', 'sequences-and-series']"
3446045,Justification for expression for Convex Hull,"I am new to convex hulls and I have encountered the following statement: Given a set of n points in a vector space $\{x_1 ... x_n \}$ , every point $x_0$ in the convex hull they form is obtained from the expression: $$ x_0 = \sum_{i=1}^n \alpha_i x_i$$ where $\alpha_i\geq0$ and $\sum_{i=1}^n\alpha_i=1$ , with different coefficients $\alpha_i$ giving different points, always within the convex hull. I have seen in this other answer a very clear example in $\mathbb R^2$ with only three points, for which the convex hull is just the convex combination of those three points, and corresponds to the points within the triangle formed by them. In $\mathbb R^2$ , but for $n$ points, the convex hull is just the set of all points inside the polygon formed by the ""outer"" points of the set - enclosing the rest of ""inner"" points. (Formally, the smallest convex set which contains all the points in our set of $n$ points.) If we added another point inside the triangle, the convex hull would remain the same; however, based on the expression I provide, that new point should also be included in the weighed sum (the sum goes from $0$ to $n$ ), in spite of lying inside such polygon and therefore not altering the convex hull. Why would such inner points be included in the sums? Are perhaps their coefficients zero (i.e. $\alpha_i=0$ precisely for those ""inner"" points)? Additionally, if possible, I would like to know how the expression I give for the most general case is obtained, or at least how I could intuitively think about it to make it a bit more transparent.","['discrete-geometry', 'convex-analysis', 'geometry', 'convex-hulls']"
3446052,How to solve this simple nonlinear ODE using the Galerkin's Method,"I'm trying to solve a more complicated differential equation using the Galerkin's Method, but before that, I'm trying to understand how I would solve this simpler one: $$ \cfrac{d^2u}{dx^2} + u^2 = 1;\quad\text{where} \;u(-1) = u(1) = 0.$$ I have to use the basis functions $\;\phi_j(x) = \sin(j\pi x)\;$ . Therefore, an approximation to the solution has the form: $$ u(x) \approx \sum_{j=1}^{N} c_j \phi_j(x)$$ I notice that this choice of basis functions automatically satisfies the boundary conditions. If I plug this approximation into the equation, I get: $$ \sum_{j=1}^{N} c_j \phi_j''(x) + \left(\sum_{j=1}^{N} c_j \phi_j(x)\right)^{\!2} = 1.$$ So the residual can be defined as: $$ r(x) = \sum_{j=1}^{N} c_j \phi_j''(x) + \left(\sum_{j=1}^{N} c_j \phi_j(x)\right)^{\!2} - 1$$ and the Galerkin's Method imposes that ${\displaystyle \int_{-1}^{1}} r(x) \cdot \phi_i(x)\,dx \:=\:0,\;$ for $i = 1,2,\dots,N$ . When I substitute the residual in the integral above, however, I face the nonlinear term $$ \int_{-1}^{1} \left(\sum_{j=1}^{N} c_j \phi_j(x)\right)^{\!2} \cdot \phi_i(x)\,dx,$$ which evaluates to zero for any $i$ and $j$ . So this means that the nonlinear term $u^2$ at the original equation doesn't make any difference at all? I'm pretty sure to be missing something here...","['galerkin-methods', 'ordinary-differential-equations']"
3446078,Apparent circularity of proofs for properties of natural number multiplication,"On page 51 of Naive Set Theory , Halmos states the following: Multiplication is associative and commutative. The proofs are
  straightforward adaptations of the ones that worked for addition. The
  distributive law (i.e., the assertion that $k \cdot (m + n) = k \cdot
 m + k \cdot n$ whenever $k$ , $m$ , and $n$ are natural numbers) is
  another easy consequence of the principle of mathematical induction.
  (Use induction on $n$ ) However, with the tools developed up to this point in the book, it is not apparent to me how this is so. Any attempt to define any one of the above properties seems to depend on another property being usable. Consider that multiplication is here defined as a function $p_m$ such that $p_m(0) = 0$ and $p_m(n^+) = p_m(n) + m$ for all $n$ in the set of natural numbers. Let us attempt to prove commutativity. In the trivial case, we can first establish that using zero in multiplication always results in a product of zero, like so: $$p_0(0) = 0 \cdot 0 = 0 = 0 \cdot 0$$ Induction hypothesis: $$0 \cdot n = 0 = n \cdot 0$$ To be demonstrated: $$\forall n \in \omega(0 \cdot n^+ = n^+ \cdot 0)$$ This proof is simple enough: $$0 \cdot n^+ = (0 \cdot n) + 0 = 0 + 0 = 0 = n^+ \cdot 0$$ This allows us to say that for $0$ , the commutativity of multiplication is established. We may now take as our induction hypothesis the following: $$m \cdot n = n \cdot m$$ To be demonstrated: $$\forall n,m  \in \omega(m \cdot n^+ = n^+ \cdot m)$$ Between our definitions and hypothesis, we can do the following: $$m \cdot n^+ = (m \cdot n) + m$$ $$= (n \cdot m) + m$$ . . . but this is where I get stuck. This would appear to require distributivity as an assumption to move forward. However, any proof of distributivity that would actually be useful would, in turn, appear to depend on commutativity already being proven. I am aware that there are proofs independent of the way Halmos constructs set theory, but since he so casually states that these proofs should be straightforwardly adapted from earlier proofs, I am curious how he imagines we may prove any of these properties using just what's on offer in the book to that point.","['elementary-set-theory', 'proof-explanation']"
3446127,Is a continuous real-valued function with an almost everywhere continuous derivative necessarily differentiable on $\mathbb{R}$? [duplicate],"This question already has answers here : If a derivative of a continuous function has a limit, must it agree with that limit? [duplicate] (2 answers) Closed 4 years ago . Setting: Let $a\in \mathbb{R}$ , and let $f$ be a function which is continuous on $\mathbb{R}$ , differentiable on $(\infty,a)\cup (a,\infty)$ , and has the property that $\lim_{x\to a^{-}}f'(x) = \lim_{x\to a^{+}}f'(x) = L$ for some $L\in\mathbb{R}$ . Question: Does it automatically follow that $f$ is differentiable at $a\in \mathbb{R}$ ? My intuition wants to say yes since we are placing such strong conditions on $f$ , and if I were going to try to prove this I would pursue the following logic: Argument: \begin{align*}
f'(a) &= \lim_{h\to 0}\frac{f(a+h) - f(a)}{h}\\
&= \lim_{h\to 0}\left[\lim_{x\to a^{-}}\frac{f(x+h) - f(x)}{h}\right]\text{using continuity of} f \text{at }a\\
&= \color{red}{\lim_{x\to a^{-}}\left[\lim_{h\to 0}\frac{f(x+h) - f(x)}{h}\right]}\\
&= \lim_{x\to a^{-}}f'(x)\\
&= L
\end{align*} or using essentially the same argument from the other side..... \begin{align*}
f'(a) &= \lim_{h\to 0}\frac{f(a+h) - f(a)}{h}\\
&= \lim_{h\to 0}\left[\lim_{x\to a^{+}}\frac{f(x+h) - f(x)}{h}\right]\text{using continuity of} f \text{at }a\\
&= \color{blue}{\lim_{x\to a^{+}}\left[\lim_{h\to 0}\frac{f(x+h) - f(x)}{h}\right]}\\
&= \lim_{x\to a^{+}}f'(x)\\
&= L
\end{align*} But the steps that I've outlined in red and blue correspond to geniune gaps in my argument.  We cannot always interchange such limits without caution, but perhaps due to the strong assumptions on $f$ we can in this case. Is there an example of a function $f$ (of course, satisfying the conditions give above) for which this logic fails? If not, how can I fix my argument?","['calculus', 'derivatives']"
3446195,Proving $\lim n^{\frac{1}{n}} =1.$,"I wanted to find a way of doing this without employing the exponential. So, let $a_n = n^{\frac{1}{n}} -1.$ Then \begin{align*} n &= (a_n +1)^n \\ &= \sum_{k=0}^n \binom{n}{k} a_n^{k} \\ &\geq \frac{n(n-1)}{2} a_n^2 \\ &\geq 0. \end{align*} So that $$0\leq a_n \leq \sqrt{\frac{2}{n-1}}.$$ Letting $n$ sail to infinity we see that $$\lim a_n =0$$ which is what we wished to prove. My question is this:
I assumed $a_n \geq 0$ . Is this true? How would a proof of this look? Any hints are appreciated.","['sequences-and-series', 'real-analysis']"
3446220,Rudin's RCA Lebesgue-Radon-Nikodym,"While I was reading the proof of the theorem of Lebesgue-Radon-Nikodym presented in the Rudin's Real and Complex Analysis, there is a part I cannot catch the point clearly. It says: (1) Observe how the completeness of $L^2(\phi)$ was used to guarantee the existence of $g$ . (2) Observe also that although $g$ is defined uniquely as an element of $L^2(\phi)$ , $g$ is determined only a.e. $[\phi]$ as a point function on $X$ . Firstly, I guess that the completeness of $L^2(\phi)$ is necessary to apply Theorem 4.10: Every nonempty, closed, convex set $E$ in a Hilbert space $H$ contains a unique element of smallest norm. Then, we can apply Theorem 4.11 and 4.12 to guarantee that there exists a unique $g\in L^2(\phi)$ such that \begin{equation*}
\int_Xfd\lambda = \int_Xfgd\phi
\end{equation*} for every $f\in L^2(\phi)$ .
It seems quite obvious, so I am not certain if it is all meant in the first sentence. Secondly, it is quite confusing to understand the meaning of ""as a point function on $X$ "" in the second sentence. I understand that the set of equivalence classes of $L^2(\phi)$ is a Hilbert space, so the uniqueness actually applies to the equivalence class of $g$ . Then, what does ""as a point function on $X$ "" mean? Does it mean as a point on $L^2(\phi)$ or as a constant function on X?","['measure-theory', 'analysis']"
3446272,I can't understand how probability makes sense,"I have a lot of questions regarding probability. Please forgive me if I have made mistakes. I actually tossed a coin 200 times. 54% of the time it landed on heads and 46% it landed on tails. What is the reason that there is a fair chance of the coin landing on heads or tails? Is it the randomness that causes this? If so, then in a purely random experiment would the result be 50-50? Even though probability only projects the likelihood of an event, why are the outcomes in favor of this projection? If I eliminate all the external factors during a coin toss, like air resistance, the coin is tossed in a vacuum chamber, the force to flip the coin is fixed,etc will the experiment still be random? Or will I be able to predict the outcomes? If the outcomes are indeed predictable, will the experiment be still random if I add a single atom into the chamber? If not at what point does it become random again?",['probability']
3446313,Proving function has a specific value in its domain,"If $a$ and $b$ $\in \Bbb{R}$ , $a \neq b$ , and $f(x)=(x-a)^2 (x-b)^2 + x$ . Show that $f$ gets the value $\frac{a+b}{2}$ at some $x$ in its domain. I'm trying to prove this by the intermediate-value theorem. Since $f$ is continuous in its domain, we can say that $f(x) = \frac{a+b}{2}$ exists if $$f(x_1)\leq f(x)=\frac{a+b}{2}\leq f(x_2)$$ is true for some $x_1 , x_2$ in the domain of $f$ . One way to show this, is to differentiate $f$ with respect to $x$ and find $f_{min}$ and/or $f_{max}$ , but since $f'$ is quite convoluted I don't think its necessarily the best idea. We can plug in $0$ and show that $f(0)=a^2 b^2 > f(x)$ , i.e $\frac{a+b}{2}$ is not the largest value of $f$ . Other than that I don't know how to proceed.","['calculus', 'functions', 'real-analysis']"
3446319,Commensurability and normal subgroups,"Two groups $G_1,G_2$ are (abstractly) commensurable if there exist isomorphic finite-index subgroups $H_1 \leq G_1$ , $H_2 \leq G_2$ . Question: If $G_1,G_2$ are commensurable, do there exist two isomorphic normal finite-index subgroups $H_1 \lhd G_1$ , $H_2 \lhd G_2$ ? Here, I am assuming that $G_1$ and $G_2$ are finitely generated. As a consequence, one of the two subgroups can be chosen normal, but can they be chosen normal simultaneously? If the answer is negative in full generality, I am also interested in partial positive answer. For instance, we may assume that $G_1$ and $G_2$ are residually finite.","['group-theory', 'normal-subgroups']"
3446362,Application of L'Hospital's Rule on the definition of a derivative.,"I'm currently taking an introduction to Calculus course and I've come across the following identity: How would one come up with this? My best guess is using L'Hospital's Rule on $$\lim_{x\rightarrow a}{\frac{f(x)-f(a)}{x-a}}$$ but I'm not very sure how, since differentiating both the numerator and denominator merely yields $$\lim_{x\rightarrow a}{f'(x)} = f'(a)$$","['limits', 'real-analysis']"
3446368,"Prove that for every finite, nonempty set $B ⊆ A$ there is some $x \in B$ such that $∀y \in B((x, y) \in R ◦ R)$","Suppose $R$ is a relation on $A$ , and $∀x \in A∀y \in  A(x Ry \lor y
 Rx)$ . Prove that for
  every finite, nonempty set $B ⊆ A$ there is some $x \in  B$ such that $∀y \in B((x, y) \in  R ◦ R)$ . Below we will refer to such $x$ as LSE of $B$ . My attempt: By induction. Base case: Take $B \subseteq A$ such that $|B| = 1$ . Let $B = \{b\}$ . Since $R$ is reflexive, $bRb$ and thus $(b,b) \in R \circ R$ . $b$ is LSE of $B$ Induction step: Suppose for any set $K \subseteq A$ with $n$ elements exists LSE Let $B \subseteq A$ such that $|B| = n+1$ . Take arbitrary $b \in B$ . Let $B' = B \setminus \{b\}$ By inductive hypothesis, we know $B'$ has LSE, call it $c$ . Consider set $B$ . We know that either $cRb$ or $bRc$ Suppose $cRb$ . Since $cRc$ , $(c,b) \in R \circ R$ . $c$ is LSE of $B$ . Suppose $bRc$ Suppose exists $a \in A$ such that $cRa$ and $aRb$ . Then $(c,b) \in R \circ R$ . Hence $c$ is LSE Suppose there doesn't exist $a \in A$ such that $cRa$ and $aRb$ . Since $bRb$ , we have $(b,c) \in R \circ R$ . Take arbitrary $y \in B'$ . We know that $(c,y) \in R \circ R$ . Exists some $k$ such that $cRk$ and $kRy$ . By assumption, we know that $bRk$ . Which means that $(b,y) \in R \circ R$ . Since $y$ was arbitrary, we have $\forall x \in B'\bigl((b,x) \in R \circ R\bigr)$ . Hence $\forall x \in B\bigl((b,x) \in R \circ R\bigr)$ . $b$ is LSE of $B$ . Therefore, in all cases, $B$ will have LSE. $\Box$ Is it correct?","['elementary-set-theory', 'proof-verification', 'graph-theory']"
3446384,Prove that : $\lim_{n\to +\infty}\left(x_{0}x_{1}...x_{n-1}\right)^{1/2^{n}}=\frac{3+\sqrt{5}}{2}$,Let sequence $x_{n+1}=x_{n}^{2}-2$ with $x_{0}=3$ Then prove that : $$\displaystyle\lim_{n\to +\infty}\left(x_{0}x_{1}...x_{n-1}\right)^{\frac{1}{2^{n}}}=\frac{3+\sqrt{5}}{2}$$ I don't know how I started but my result in try is : I see $\frac{3+\sqrt{5}}{2}+\frac{2}{3+\sqrt{5}}=3=x_{0}$ I don't know  where this can help me I have already to see your hints or ideas to approach it,"['limits', 'calculus', 'sequences-and-series']"
3446399,Correspondence between infinite matrices and $\ell_2$ operators,"When calculating the numerical range of the matrix $$
C := \begin{pmatrix}
0 & 1 \\ 0 & 0 \end{pmatrix}
$$ and the left-shift operator on the Hilbert space $\ell_2$ $$
T: \ell_2(\mathbb{N}) \to \ell_2(\mathbb{N}), \ 
 (x_1, x_2, \ldots) \mapsto (x_2, \ldots)
$$ I noticed that the latter can be considered an infinite-dimensional generalisation of the first as $$
\begin{pmatrix}
0 & 1 \\ 0 & 0 
\end{pmatrix}
\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}
= \begin{pmatrix} x_2 \\ 0 \end{pmatrix}
\quad \text{and} \quad
\begin{pmatrix}
0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 
\end{pmatrix}
\begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix}
= \begin{pmatrix} x_2 \\ x_3 \\ 0 \end{pmatrix}
$$ and so on.
If we now continue this pattern to infinity (I know this is not perfectly rigorous, but considering the norm of the difference of $T x$ and $T_n x$ , where $T_n$ are the matrices, we see it goes to zero as $\ell_2$ sequences are zero sequences), we end up with $T$ ! A similar example of course is the right shift operator on $\ell_2$ with ""finite-dimensional analogon"" $$\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}.$$ For $$T: \ell_2 \to \ell_2, \ (x_1, x_2, x_3, \ldots) \mapsto \left(x_1, \frac{x_2}{2}, \frac{x_3}{3}, \ldots \right)$$ this is more difficult to find such an analogon, since one could argue like above that $$
T_1 := 
\begin{pmatrix}
1 & 0 \\
0 & \frac{1}{2}
\end{pmatrix}
$$ is the finite-dimensional analogon as $$
T_1 \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}
= \begin{pmatrix} x_1 \\ \frac{x_2}{2} \end{pmatrix},
$$ but this somewhat doesn't contain the ""essence"" of the operator like in the examples of shift-operators above, as $\begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0\end{pmatrix}$ exhibits a certain self-similarity, in the way that you can find the lower dimensional matrix $C$ in it two times (though they are overlapping). Questions Are there other examples of such a corresponded (one could also consider $(x_1, x_2, x_3) \mapsto \left(x_1, \frac{x_2}{2!}, \frac{x_3}{3!}\right)$ and similar variations) and what might be a reason that $T$ has a ""finite-dimensional analogon""? Consider $D := \{ y \in \ell_2: \exists N \in \mathbb{N}: y_n = 0 \ \forall n > N\}$ , pick $x \in \ell_2 \setminus D$ and define $$
\hat{T}: \text{span}(x) + D \to \text{span}(x), \ 
cx + d \mapsto cx,
$$ where $c \in \mathbb{C}$ and $d \in D$ . $\hat{T}$ is still linear, but unbounded (and not closable).
Can there still be a ""finite-dimensional analogon""?","['matrices', 'functional-analysis', 'examples-counterexamples']"
3446434,Essential differences between Runge-Kutta and Adams-Bashforth,"I do not quite understand the essential difference between these
  methods. I understand that there are differences (which I wrote at the
  bottom of this text), but what does it all mean in the end? In other
  words, if the orders are the same, why should students learn these two
  methods? Let us keep the order of accuracy at 2 for both of these methods. Just to be clear, I will write out these well known formulas. Though there are many, let us take a look at Heun's Runge-Kutta method of order 2 (RK2): $$y_{k+1} = y_k + \frac{h}{2}[f(t_k, y_k) + f(t_k + h, y_k + hf(t_k,y_k))]$$ and compare it with the second order Adam Bashforth method (AB2): $$y_{k+2} = y_{k+1}+\frac{3h}{2}f(t_{k+1}, y_{k+1})-\frac{h}{2}f(t_k,y_k)$$ Here's my understanding of their differences: They look different. RK2 is from Taylor series approximation, AB2 is from the quadrature's polynomial interpolation. AB2 is unique, while there are infinitely many formulas for RK2 (though only one of them has the lowest local error). RK2 is single step, AB2 is two-step. Both have their respective implicit versions of all orders, which can be used to perform PECE. I see all these differences, yet still do not understand when one should prefer one over the other. I feel like I have the details, but I am missing out on the big picture.","['numerical-calculus', 'numerical-methods', 'soft-question', 'ordinary-differential-equations']"
3446494,"The Fourier coefficients of a stochastic process on $[a,b]$","Let $(\Omega, \mathcal A, P)$ be a probability space. Let $X(\omega,t)$ be a stochastic process such that $X(\omega,.)$ is in $L^2[a,b]$ for every $\omega \in \Omega$ . For simplicity, we can assume $a=0$ , $b=1$ . Let us take a complete ortonormal basis of $L^2[a,b]$ , denoted $P_n(x)$ (for example the shifted Legendre polynomials for $L^2[0,1]$ ). The following integrals make sense for each $\omega \in \Omega$ $$Y_n(\omega)=\int_a^b X(\omega,t) P_n(t) \, dt$$ and define the ""random"" Fourier coefficients of $X$ . If we know the distribution of $X$ for every $t \in [a,b]$ , can we find the distribution of $Y_n$ ? What is the literature on this problem and what tools are available ? You can suppose that $X(w,.)$ is continuous and non-decreasing for each $\omega \in \Omega$ if it simplifies the problem. Or add other hypotheses that do not make the problem trivial. Intuitively, we feel that this integral is a Riemann sum for each $\omega$ , which might hint that we must use a CLT to find the distribution of $Y_n$ , but independence does not make sense for continuous time processes. Maybe a martingale CLT ? I never used one of those. Edit: it would be also interesting to fix the distributions of the $Y_n$ 's and try to retrieve the distributions of $X$ at each $t$ .","['fourier-analysis', 'real-analysis', 'stochastic-processes', 'functional-analysis', 'probability-theory']"
3446497,Homomorphisms from abelian groups to $S^1$ separate points,"It is stated in the book Natural Dualities for the Working Algebraist by Clark and Davey (and also in the context of Pontryagin Duality) that for every abelian group $A$ and elements $a \neq b$ of $A$ , there exists a homomorphism $f: A \rightarrow S^1$ such that $f(a) \neq f(b)$ . Here, $S^1 = \{z \in \mathbb C \mid |z| = 1\}$ is the (multiplicative) circle group. Is there an easy argument we're missing? In the case of $A$ being finitely generated it is easy to see using the classification of finitely generated abelian groups. This shows that it is enough to show that homomorphisms into finitely generated abelian groups separate points.","['general-topology', 'abstract-algebra', 'commutative-algebra', 'group-theory']"
3446532,A refinement of a famous inequality on the forum .,"It's related to a big problem Olympiad Inequality $\sum\limits_{cyc} \frac{x^4}{8x^3+5y^3} \geqslant \frac{x+y+z}{13}$ .I have this (For one time I take the time to check it ) Let $a,b,c>0$ such that $a+b+c=1$ then we have : $$\sum_{cyc}\frac{a^4}{8a^3+5b^3}> \sum_{cyc}\frac{\tan(a^4)}{8\tan(a^3)+5\tan(b^3)}\geq \frac{3\tan\Big(\frac{1}{81}\Big)}{13\tan\Big(\frac{1}{27}\Big)} $$ The difficulty exceeds the level of an Olympiad I think . Furthermore I think that we cannot use Jensen's inequality (it's not homogeneous) and Cauchy-Schwarz is really too weak .Don't tell me that $\tan(x)\geq x $ is a good approximation in this case it will be a joke . Maybe we can prove this kind of inequality : $$\frac{a^4}{8a^3+5b^3}+\frac{b^4}{8b^3+5a^3}\geq \frac{\tan(a^4)}{8\tan(a^3)+5\tan(b^3)}+\frac{\tan(b^4)}{8\tan(b^3)+5\tan(a^3)}$$ But even if it's works it doesn't decide the problem . I discouraged to use power series it's really awful. So comments and hints are welcome but don't try it alone . Thanks for sharing your time and knowledge. Update : I think it's not so hard if we remark that we have : $$\frac{a^4}{8a^3+5b^3}>\frac{\tan(a^4)}{8\tan(a^3)+5\tan(b^3)}$$ For $a,b>0$ and $a+b<1$ Maybe someone can prove this and prove the LHS","['multivariable-calculus', 'trigonometry', 'inequality']"
3446539,Is 0 divided by a non-zero indeterminate equal to 0.,"Consider the following solution: $$ \lim\limits_{(x, y) \to (0, 0)} \dfrac{xy^4}{x^4+y^4}$$ Divide both numerator and denominator by $y^4$ $$ =  \lim\limits_{(x, y) \to (0, 0)} \dfrac{x}{\left(\dfrac{x}{y}\right)^4+1}$$ $$ =  \dfrac{ \lim\limits_{(x, y) \to (0, 0)}x}{ \lim\limits_{(x, y) \to (0, 0)}\left(\dfrac{x}{y}\right)^4+1}$$ The numerator is 0 and the denominator is non-zero, hence the limit is 0 ============================= Are you satisfied with this solution, if not, why? Edit: Thank You for the help guys. My mistake was looking at the final limit and not paying attention to how I got to that stage, ""by dividing both numerator and denominator by y^4"", that step itself is not allowed because y can be 0. The above solution will be complete if I include another case where $y=0$ , because when you divide by $y^4$ , you are implicitly stating that y is not 0.","['limits', 'multivariable-calculus', 'proof-verification', 'limits-without-lhopital']"
3446567,Understanding Linear Recurrence Theorem,"I have been trying to understand this theorem but I could not get past a point. The proof has been taken from Kenneth Rosen's Discrete Mathematics. THEOREM 1: Let c 1 and c 2 be real numbers.
  Suppose that r 2 − c 1 r − c 2 = 0 has
  two distinct roots r 1 and r 2 . Then the sequence
  {a n } is a solution of the recurrence relation a n = α 1 a n−1 + α 2 a n−2 if and only if a n =α 1 r 1 n +α 2 r 2 n for n=0,1,2,...,where α 1 and α 2 are constants. PROOF : We must do two things to prove the theorem. First, it must be shown that if r 1 and r 2 are the roots of the
  characteristic equation, and α 1 and α 2 are
  constants, then the sequence {a n } with a n =
  α 1 r 1 n +
  α 2 r 2 n is a solution of the recurrence
  relation.  Second, it must be shown that if the sequence
  {a n } is a solution, then a n =
  α 1 r 1 n +
  α 2 r 2 n for some constants
  α 1 and α 2 . Now we will show that if a n =
  α 1 r 1 n +
  α 2 r 2 n , then the sequence
  {a n } is a solution of the recurrence relation.Because
  r 1 and r 2 are roots of r 2 −
  c 1 r − c 2 = 0, it follows that
  r 1 2 = c 1 r 1 +c 2 ,
  r 2 2 =c 1 r 2 +c 2 . Now until this part what I have understood(if I am right) that they have just taken a quadratic equation and split it into its roots. Now the following algebraic part did not make any sense to me. I want to understand this part: c 1 a n−1 + c 2 a n−2 =
  c 1 (α 1 r 1 n−1 +
  α 2 r 2 n−1 ) +
  c 2 (α 1 r 1 n−2 +
  α 2 r 2 n−2 ) = α 1 r 1 n−2 (c 1 r 1 + c 2 ) + α 2 r 2 n−2 (c 1 r 2 +
  c 2 ) = α 1 r 1 n−2 r 1 2 + α 2 r 2 n−2 r 2 2 = α 1 r 1 n + α 2 r 2 n = a n . This shows that the sequence {a n } with a n =
  α 1 r 1 n +
  α 2 r 2 n is a solution of the recurrence
  relation. Now I have omitted rest of the proof because I could no get past this point. If someone requires it I can edit it. or If someone can explain further part in simple english I would be very grateful for that. But for now I just want to understand the above part.","['combinatorics', 'discrete-mathematics', 'roots']"
3446579,"Simply connected neighbourhood of a simply connected, locally path connected, closed set","This is a follow up of this question. Let $M$ be a smooth manifold and let $C \subseteq M$ be a closed, simply connected and locally path connected subset. 
Is it possible to find an open, simply connected, neigbourhood of $C$ ?","['connectedness', 'smooth-manifolds', 'general-topology', 'algebraic-topology', 'locally-connected']"
3446624,Is there an accepted term for the opposite of mode in statistics?,"In descriptive statistics, there are terms for all sorts of things.  The mean, median, and mode for a set of data are each three very frequently thrown around examples. The mode in particular is defined as the result that has occurred the most frequently. Is there a name for the opposite of this?  Is there a name for the result which has occurred the least frequently ( but still a nonzero number of times )? If I were to guess at a name or come up with one myself, perhaps a name like "" anti-mode "" might be appropriate, but I was wondering if there was an already accepted name.","['descriptive-statistics', 'statistics', 'terminology']"
3446629,The proof of the countability of a set,Let a function $f: A \rightarrow R $ be defined on $A$ and have a local extremum at each non-isolated point of this set. How to prove that the set $ f (A)$ is either finite or countable?,"['elementary-set-theory', 'general-topology', 'real-analysis']"
3446633,The dimension of the projection of a variety,"Let $V\subseteq\mathbb{C}^n$ be a variety given as the zero set of some homogeneous polynomials and let $m\leq n$ . I am interested in the dimension of the variety $$
W=\{(v_1,\dots,v_m)\in\mathbb{C}^{n\times m}\mid\mbox{ there exists }0\neq (\lambda_1,\dots,\lambda_m)\in\mathbb{C}^m, \lambda_1 v_1+\dots+\lambda_m v_m\in V\}.
$$ Now, there is a map $$
\varphi:(\mathbb{C}^m-\{0\})\times\mathbb{C}^{n\times m}\rightarrow\mathbb{C}^n
$$ mapping $(\lambda_1,\dots,\lambda_m,v_1,\dots,v_m)$ to $\sum_{i=1}^m \lambda_i v_i$ . Then, $W$ is the projection of $\varphi^{-1}(V)$ to $\mathbb{C}^{n\times m}$ . Since $\varphi$ is linear in the variables $\lambda_i$ , for any $0\neq \lambda=(\lambda_1,\dots,\lambda_m)$ , the fiber of $\lambda$ in $\varphi^{-1}(V)$ is isomorphic to $V\times\mathbb{C}^{n\times (m-1)}$ . Thus, the codimension of $W$ should be codim $(V)-m+1$ . However, this is not very rigorous. I am also having problems to show that $W$ is actually Zariski closed. By some analysis arguments, I can show that $W$ is closed with respect to the Euclidean topology and since $W$ is the projection of a Zariski closed set, I think $W$ must be Zariski closed. But again, this is not very rigorous. Any help is appreciated.","['ring-theory', 'algebraic-geometry']"
3446636,Hypothesis testing: carrying capacity of bridges,"A manufacturer claims that, at the moment of stockage, the carrying
  capacity (CC) of the produced bridges follows a normal distribution
  with $\mu = 2500\, kg$ and $\sigma^2=150^2\, kg^2$ . Moreover the CC of
  the different bridges are independent. The CC of the bridges decreases
  as time goes by, so after 2 years the manufacturer has to decide
  whether to get rid of the bridges or not (independence, normality and
  variance are assumed to be preserved!). How many bridges do we have to test such that if the average CC is $2500\, kg$ , then this can be claimed with a certainty of $95\%$ . if the average CC decreases by $150\, kg$ , then it is possible to detect this decrement with a probability of $90\%$ . My attempt: We set $H_0: \mu = 2500$ and $H_A: \mu < 2500$ . Suppose that $\bar{X}$ denotes the average CC of bridges in a sample of size $n$ . Then it is clear that $\bar{X} = N(2500,150^2/n)$ under $H_0$ , where $n$ is the unknown we're solving for. The first bullet points assumes that we're working under the assumption that $H_0$ is true. The second one states that $\bar{X} = N(2350,150^2/n)$ . I feel like I'll have a system of two equations to solve for $n$ . I just don't know how the express 'being able to detect with a probability of $\alpha\%$ ' mathematically. I feel like confidence intervals are an option. For the first one, we have that the sample average will lie in $$[2500-1,96\cdot \frac{150}{\sqrt n}, 2500+1,96\cdot \frac{150}{\sqrt n}].$$ And then we can do the same for the second bullet point. But how do I find $n$ from these intervals? Thanks.","['statistics', 'hypothesis-testing']"
3446661,Find the $n^{th}$ derivative of $y=\dfrac {x^n}{x-1}$.,"Find the $n^{th}$ derivative of $y=\dfrac {x^n}{x-1}$ . My Attempt: $$y=\dfrac {x^n}{x-1}$$ $$y=x^n\cdot(x-1)^{-1}$$ Differentiating both sides, $$y_{1}=x^n\cdot(-1)\cdot(x-1)^{-2}+(x-1)^{-1}\cdot n\cdot x^{(n-1)}$$ $$y_{1}=x^n\cdot(-1)\cdot(x-1)^{-2}+(x-1)^{-1}\cdot n \cdot\dfrac {x^n}{x}$$","['calculus', 'derivatives']"
3446685,"Group action on subspace of $L^2([0,1])$","I need to find a totally discontinuous $D_4$ action on $S^\infty\subset L^2([0,1])$ . Background: Here $L^2([0,1])=\{\text{measurable }f: [0,1]\to \mathbb{C} : \int |f|^2<\infty\}$ , $S^\infty:=\{f\in L^2([0,1]) : ||f||_{L^2}=1\}$ and $D_4$ is the dihedral group with 8 elements. To say $D_4$ acts ""totally disconinuously"" on $S^\infty$ is to say that for all $x\in S^\infty$ , there exists open $U\ni x$ such that $gU\cap U\neq \emptyset \Rightarrow g=e$ . Basically, this just means that every non-identity element moves every element in $S^\infty$ . My attempt: So $D_4$ is generated by $r$ and $s$ where $r^2=s^4=(rs)^2=e$ . The natural choice for the action of $r$ on $S^\infty$ is $rf\to -f$ . This map is (lipschitz) continuous and clearly moves every element. Now I was trying to find transformation of order 4 that moves every element of $S^\infty$ which also satisfies $(rs)^2=e$ . Here are some things I have tried: Some possible actions of order 4 are $sf=if$ , $s f=g$ where $g(t)=f(1-t)$ , $sf=\overline{f}$ . But all of these commute with $r$ . I then tried to do one thing to $f$ on half of the interval and something else to $f$ on the other half, but nothing that I could find works. I am wondering now If I chose the wrong action for $r$ but $rf=-f$ feels to me like the only natural choice. I would appreciate any sort of hint! Edit: the maps $sf=g$ and $sf=\overline{f}$ specified above are not order 4...I meant something more like $sf=ig$ and conjugating then switching the real and imaginary parts. Second edit: It is possible that this cannot be done... I have my doubts","['general-topology', 'group-actions', 'functional-analysis', 'algebraic-topology']"
3446710,Adjoint of Sum = Sum of Adjoints?,"Didnt find this anywhere, just verifying. We know that: $$ (A+B)^T= A^T+B^T $$ . Does it follow that: $$ (A+B)^† = A^† +B
^† $$ for all A and B matrices (the dagger here representing the adjoint)? If so what is the proof?","['matrices', 'transpose']"
3446779,Is there a homomorphism from $\mathbb R / \mathbb Q$ into the circle?,"I'm trying to construct a counterexample for my student. Does anyone know if there exists (or doesn't exist) a non-trivial group homomorphism: $$g: \mathbb R/\mathbb Q \to S^1$$ where $S^1$ denotes the unit circle in $\mathbb C$ or equivalently ${[0,2\pi]}/_{0\,\sim\,\pi}$ . Thanks!","['group-homomorphism', 'group-theory', 'quotient-group', 'abelian-groups']"
3446864,On a definition of a probability measure on the space of all continuous functions.,"I am following Continuous Time Markov Processes by Thomas M. Liggett. and I reached chapter 1.7 where he introduces formally the Markov property. To do so he first wants to define a probability measure on the space of continuous functions: Let $\Omega$ be the set $C[0, \infty)$ be the set of all continuous functions $\omega $ on $[0, \infty)$ , the $\sigma$ -algebra $\mathcal{F}$ is taken to be the smallest one for which the projection $\omega \rightarrow \omega(t)$ is measurable. We now have a family $\{ P^x \}$ of probability measures on $(\Omega, \mathcal{F})$ indexed by $x \in \mathbb{R}$ . The probability measure $P^x$ is the distribution of $x + B(\cdot)$ where $B$ is a standard Brownian motion. This last sentence I don't understand, if I have a set $A \in \mathcal{F}$ what would be $P^x(A)$ ? Also afterwards it is stated that $$E^x[ f(B(t)) ] = E[f(x+B(t))]$$ where $E^x$ is the expectation wrt the probability measure $P^x$ . So first we consider $f(B(t))$ as a function from $\omega \in \Omega \rightarrow \mathbb{R}$ and this becomes a regular expectation on the Brownian motion? How is this true?","['stochastic-processes', 'measure-theory', 'brownian-motion', 'probability-theory']"
3446887,Best textbook on Algebraic Geometry [duplicate],"This question already has answers here : Best Algebraic Geometry text book? (other than Hartshorne) (10 answers) Closed 4 years ago . I plan to self-learn Algebraic Geometry. I have an undergraduate degree in Math. I cannot find a clear answer on what the clear textbook could be. There are also some notes online on the subject, like Gathmann and Milne.","['algebraic-geometry', 'soft-question', 'reference-request']"
3446952,Decomposing a Measurable Set into its interior and boundary.,"I recently learn that every open set and closed set is Lebesgue Measurable. However, from what I understand, every set $S$ can be decomposed into the union of its interior, $\text{int}(S)$ , which is open, and its boundary, $\partial{S}$ , which is closed. So ${S}=\text{int}(S)\cup \partial{S}$ is a finite union of measurable sets. Then shouldn't any set S should be measurable too? I know this is incorrect by the existence of non-measurable sets, so what goes wrong? Edit: by correction in comments: $\overline{S}=\text{int}(S)\cup \partial{S}$","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis']"
3447031,Pointwise convergence vs. almost sure convergence,"I do not understand the difference between these two types of convergence for random variables. Actually, I am not seeing a lot of people using the notion of pointwise convergence for random variables. So, what is the difference? Does anyone have a memorable example illustrating the difference?","['probability-theory', 'probability', 'random-variables']"
3447049,"Intuitive understanding of 2-forms, (1,1)-tensors, and other fundamental objects of exterior algebra or tensor algebra","My background consists mostly of a good level in linear algebra, abstract algebra, undergrad calculus, topology & probability, and some working knowledge of geometric algebra and category theory. I'm currently learning differential geometry and tensor calculus, and hope to move onto geometric calculus and information geometry. However, I have a lot of trouble integrating abstract formulas and concepts if I don't have some intuitive mental model on which to rely. My question is very broad: what is a coherent system of visualization for $(m,n)$ -tensors ? Since this question can be answered in many ways, I'll be asking more specific questions as well. To help guide your answer (and hopefully help other learners like myself), here's some insight I've built up over the years on how to visualize some basic objects. Scalars are simply single numbers from a field $F$ . They are $(0,0)$ -tensors. A scalar field can be viewed as a coloring over a manifold, where each point gets darker and redder as the scalar at that point tends towards positive infinity, whiter as it tends towards zero, and darker and bluer as it tends towards negative infinity. Vectors are $(1,0)$ -tensors. I understand and visualize them either as an oriented arrow, or as a ""line with cyclic colors"" (1D subspace with a repeated hue shift). The norm of your vector is equivalently the distance from the tail to the head of the arrow, or the distance to get from a specific shade to the next occurence of the exact same shade on your line. The order of the hue shift RGB or BGR encodes the orientation. This second picture is often useful in seeing geometric algebra as the algebra of subspaces of a vector space. Vectors, in 'matrix form', are column vectors ( $n*1$ matrices). A vector field is a flow field where a particle follows the arrows, with one vector defined at each point on the manifold (like arrows for winds on a globe of the earth in the context meteorological data). A smooth vector field can also (sometimes? always?) be seen as a foliation, a space of solutions to a differential equation which divides the spaces into non-intersecting flow curves. Covectors, 1-forms, $(0,1)$ -tensors, I think I understand as well. By definition they are linear functions from $V$ to the underlying field $F$ . They can be seen stacks of regularly spaced  parallel (hyper)planes. They're your row vectors ( $1*n$ matrices). They ""eat"" vectors by returning a scalar, operating somewhat like a dot product (for a euclidean metric). You can also see covectors as stacked (hyper)planes that hue-shift, and if the plane that passes through the origin is ""pure red"", the result of the dot-product-like operation (applying the linear form to a vector $v$ ) is the number of times our vector $v$ passes through ""pure red"" planes, plus whatever fraction of the way to the next red plane the arrowhead happens to lie on. A higher norm for a covector means that the sheets in the stack are closer together (ie, the same vector will pass through more sheets, returning a larger scalar). Covector fields are like topographical maps. Say a particle is going through the covector field: the more it goes ""towards the top of the mountain"", the denser the material it has to go through becomes, the more its speed is ""eaten up"" by the covector field; and vice-versa, it speeds up when ""going down the mountain"", because it's going from a dense medium to a fluid medium. 2-vectors, bivectors, $(2,0)$ -tensors, are oriented areas. You can see them as parallelograms (formed as the outer product of two non-colinear vectors) or ellipses, or any 2D shape, so long as orientation (clockwise vs counter-clockwise rotation in the plane; or equivalently transparent vs textured side of the plane) and area (the norm of the bivector) are the same. I wonder if for this reason there's also a hue-shift-type visualization for bivectors; like representing a bivector as a specific ""hue-shift tiling"" of a full 2D subspace ? I can't quite work it out. Also, given that bivectors are $(2,0)$ -tensors, am I correct to assume that you could make a ""column vector of column vectors"" that could ""eat two successive 1-forms, or a single 2-form"", as a ""more appropriate"" matrix form ? (By this latter question, I mean that I have trouble understanding how $(2,0)$ -tensors, $(1,1)$ -tensors and $(0,2)$ -tensors should all look like rectangular matrices; it seems incoherent to me, like an abuse of notation: perhaps useful, but in need of clarification.) As for 2-forms (which I guess one could also call cobivectors or bicovectors), I'd expect there be some intuitive version of ""a shape, or repetition of parallel shapes, that eats bivectors"" but I can't wrap my head around it. Part of my gut intuition from working on geometric algebra, which associates covectors to $(n-1)$ -vectors, and associates applying a covector to a vector to obtaining a pseudoscalar value from the product of an $(n-1)$ -vector and a $1$ -vector which are independent (though I have no idea if this analogy is legitimate) tells me I'd have to take a stack of $(n-2)$ -dimensional spaces to represent a 2-form. Would this mean that if I consider a 1D linear subspace of $R^3$ and the set of all affine spaces parallel to this line (like a bundle of straws stretching out infinitely), a ""closeness"" or density of each straw to its neighbors would somehow encode the intensity of a flow through a bivector, and thus a 2-form ? Given that they are (0,2)-tensors, am I correct to assume that they could be represented as ""a line vector of line vectors"" ? And if this image of 2-forms is correct, how does this relate to the idea of metric tensors ? Linear maps from $F^n \to F^m$ are $(m*n)$ -matrices. They can be understood as mapping $(1,0)$ -tensors of dimension $n$ to $(1,0)$ -tensors of dimension $m$ (by right multiplication with a vector), so I suppose they are $(1,1)$ -tensors (as in, the covariant part of the matrix is combined with the contravariant column vector, and only the contravariant part of the matrix/(1,1)-tensor remains). This is also coherent with left multiplication of an $(m*n)$ -matrix by an $m$ -dimensional covector (like in the context of Markov chains), as in it consumes an $m$ -dimensional $(0,1)$ -tensor and returns an $n$ -dimensional $(0,1)$ -tensor. Finally, you often see generalized dot products (bilinear symetric forms) being used with the pattern "" $x^TSy$ "", with $S$ a symmetric matrix. Am I correct to assume that they are $(1,1)$ -tensors which are simultaneously fed $(1,0)$ -tensor and a $(0,1)$ -tensor ? Or are they indeed like ""metric tensors"", and thus (0,2)-tensors ? Given what you've just read, are there any glaring mistakes in this picture ? Are there useful visualization and insights you care to share that might go well with the current picture (both to illustrate intuitive and counter-intuitive ideas) ? Visualization tools or models for higher-valence mixed tensors ? For tensor fields (other than the basic ""bundle = hairy manifold where each hair is a tensor"") ? Of the idea of vectors, covectors, etc, $(m,n)$ -tensors in infinite-dimensional vector spaces (which I haven't even approached here) ? Any insight on seeing tensor operations as operations on hypermatrices ? All insight from any branch that touches upon these themes is welcome ! I'm sorry if this makes a lot of diffuse questions, it's just that I'm having trouble making the whole picture of "" $(m,n)$ -tensors, their geometry, their algebraic representation and how one computes with them"" all coherent in my mind. Thanks for reading, and for your help !","['geometric-algebras', 'tensor-rank', 'exterior-algebra', 'differential-geometry']"
3447070,Multiple sampling from the same probability distribution is automatically independent?,"Suppose you have (a discrete - for simplicity) probability distribution: E.g., $\Omega=\{a,b,c\}$ with $\mathbb{P}(a)=0.2$ , $\mathbb{P}(b)=0.3$ and $\mathbb{P}(c)=0.5$ . Suppose I have some device (e.g. a computer program, such as the numpy library's randint() function) that contains the description ""This will provide you with as many random samples from $\mathbb{P}$ as you want"" . Applying this device I obtain the sequence of such samples $x_1,\ldots,x_n\in\Omega$ . How can I prove that these were generated in an independent way? Or how can I at least determin the probability that these were generated in an independent way? Is it even possible to do that, or is my question actually meaningless? (Independence is a concept that is only defined for random variable or events (as far as I know), so what would the random variable or events be, that I need to consider to make the previous question formal?) Please note: I know graduate-level mathematics (think: measure theory), but I have trouble connecting the abstract machinery, that I know, to the real world, where you actually deal with samples and stuff.","['independence', 'probability-theory', 'sampling']"
3447091,Find a circle perpendicular to two other circles.,"Here are two circles $C_1 = [ x^2 + y^2 = 1]$ and $C_2 = [ (x-2)^2 + y^2 = 3 ]$ .  The radii are $1$ and $3$ and the two circles are orthogonal to each other $C_1 \cdot C_2 = 0$ .  What is the equation of a third circle cutting the other two at right angles?  I suspect there are only two solutions which are equivalent by a reflection . Looking for a geometric solution if possible.  Counting the number of solutions, finding the equation of the circles and maybe some insights into the method. Related: Find a circle orthogonal to two other circles The Inner Product of Two Circles If $C = [ a(x^2 + y^2) - 2px - 2qy + r = 0]$ the inner product between two such circles would be: $$ [C_1 \cdot C_2] = \frac{1}{2}[2p_1p_2 + 2q_1q_2 - r_1a_2 - r_2a_1] $$ So we are trying to find three circles $C_1, C_2, C_3$ the first two given, such that: $$ [C_1 \cdot C_2] = [C_2 \cdot C_3] = [C_3 \cdot C_1 ]= 0 $$ The inner product can also be written in terms of the radii of the circles and the distance between the centers: $$ [C_1 \cdot C_2 ] = \frac{1}{2} [ R_1^2 + R_2^2 - D^2]$$ and if $C_1 \perp C_2$ we have that this inner product is zero, $[C_1 \cdot C_2 ] = 0$ . Not too many resources on the ""algebra of circles"", e.g. Geometry a Comprehensive Coruse by Daniel Pedoe. Descartes theorem deals with mutually tangent circles.","['conformal-geometry', 'inversive-geometry', 'geometry']"
3447098,Questions on convergence of formula for $\zeta(s)$,"This question assumes definition (1) below and relationship (2) below. With respect to the integral in (2) below, I selected $\frac{1}{2}$ as the lower integration bound because this is the ideal location for minimizing the undesirable contribution of the step of $S(x)$ at $x=0$ while simultaneously maximizing the desirable contribution of the step of $S(x)$ at $x=1$ . (1) $\quad S(x)=x-\left(\frac{1}{2}-\frac{1}{\pi}\sum\limits_{k=1}^f\frac{\sin(2\,\pi\,k\,x)}{k}\right),\quad f\to\infty$ (2) $\quad\zeta(s)=s\int\limits_{1/2}^\infty S(x)\,x^{-s-1}\,dx$ I originally illustrated a couple of formulas for $\zeta(s)$ based on definition (1) and relationship (2) above in my earlier question Are these formulas for the Riemann zeta function $\zeta(s)$ globally convergent? which involved the hypergeometric $_1F_2$ function. The question here is about formula (3) below which was also derived from definition (1) and relationship (2) above but is also based on this answer to my follow-on question What is $s\int_1^\infty\sin(2\,\pi\,n\,x)\,x^{-s-1}\,dx$ ? (3) $\quad\zeta(s)=\underset{f\to\infty}{\text{lim}}\quad 2^{\,s-1}\left(\frac{s}{s-1}-1+\sum\limits_{n=1}^f\left(E_s(i n \pi)+E_s(-i n \pi)\right)\right)$ Formula (3) above for $\zeta(s)$ is illustrated following the questions below. Question (1) : Is formula (3) for $\zeta(s)$ above globally convergent as $f\to\infty$ ? Question (2) : If so, does global convergence of formula (3) for $\zeta(s)$ have any implications with respect to the Riemann Hypothesis? Question (3) : If not, what is the convergence range of this formula? Formula (5) below defines another globally convergent formula for $\zeta(s)$ based on relationship (4) below and this second answer to my question What is $s\int_1^\infty\sin(2\,\pi\,n\,x)\,x^{-s-1}\,dx$ ? . (4) $\quad\zeta(s)=s\int\limits_1^\infty S(x)\,x^{-s-1}\,dx$ (5) $\quad\zeta(s)=\underset{K\to\infty}{\text{lim}}\left(\frac{s}{s-1}-\frac{1}{2}+\sum\limits_{n=1}^K\left((2 \pi  i n)^{s-1} \Gamma (1-s,2 \pi i n)+(-2 \pi i n)^{s-1} \Gamma (1-s,-2 \pi  i n)\right)\right)\\$ $\qquad\qquad\quad=\underset{K\to\infty}{\text{lim}}\left(\frac{s}{s-1}-\frac{1}{2}+\sum_\limits{n=1}^K\left(E_s(2 \pi i n)+E_s(-2 \pi i n)\right)\right)$ Note formula (3) for $\zeta(s)$ above was derived from the relationship $\zeta(s)=s\int\limits_{1/2}^\infty S(x)\,x^{-s-1}\,dx$ and formula (5) for $\zeta(s)$ above was derived from the relationship $\zeta(s)=s\int\limits_1^\infty S(x)\,x^{-s-1}\,dx$ . Question (4) : Can a globally convergent formula for $\zeta(s)$ be derived from the more general integral $\zeta(s)=s\int\limits_a^\infty S(x)\,x^{-s-1}\,dx$ for any $0<a\le 1$ ? The following figure illustrate formula (3) for $\zeta(s)$ in orange where formula (3) is evaluated with the upper limit $f=20$ . The underlying blue reference function is $\zeta(s)$ . Figure (1) : Illustration of formula (3) for $\zeta(s)$ evaluated at $f=20$ The following four figures illustrate the absolute value, real part, imaginary part, and argument of formula (3) for $\zeta(s)$ evaluated along the critical line $s=\frac{1}{2}+i\,t$ in orange where formula (3) is evaluated with the upper limit $f=20$ .  The underlying blue reference function is $\zeta(\frac{1}{2}+i\,t)$ . The red discrete portion of the plot illustrates the evaluation of formula (3) at the first $10$ non-trivial zeta-zeros in the upper half-plane. Figure (2) : Illustration of formula (3) for $\left|\zeta\left(\frac{1}{2}+i\,t\right)\right|$ evaluated at $f=20$ Figure (3) : Illustration of formula (3) for $\Re\left(\zeta\left(\frac{1}{2}+i\,t\right)\right)$ evaluated at $f=20$ Figure (4) : Illustration of formula (3) for $\Im\left(\zeta\left(\frac{1}{2}+i\,t\right)\right)$ evaluated at $f=20$ Figure (5) : Illustration of formula (3) for $\text{Arg}\left(\zeta\left(\frac{1}{2}+i\,t\right)\right)$ evaluated at $f=20$","['number-theory', 'riemann-hypothesis', 'sequences-and-series', 'riemann-zeta', 'mellin-transform']"
3447153,Are there primes arbitrarily close to powers?,"For $n \geq 3$ let $r(n)$ be the previous prime to $n$ ; i.e., the largest prime strictly less than $n$ . For example, $r(3) = 2$ , $r(10) = 7$ , and so on. I have noticed that $r(n^p)$ is very close to $n^p$ . In fact, I suspect that $$\lim_{n\to\infty}\frac{r(n^p)}{n^p} = 1$$ for any positive integer power $p$ , where the convergence is faster if $p$ is big. Is this true? Are there effective bounds on the rate of convergence? Is there a simple proof of this fact? Here's an argument that doesn't work: By Bertrand's postulate, there exists a prime between $n^p / 2$ and $n^p$ (roughly). Therefore $$\frac{r(n^p)}{n^p} \geq \frac{1}{2}.$$ But this is pretty far from $1$ . I think that I can prove this using some fancy number theory results , but they seem like sledgehammers. I'd like something simpler.","['number-theory', 'prime-numbers']"
3447154,Bound for the number of edges of a linear uniform hypergraph,"A hypergraph $H$ is a pair $H = (X,E)$ where $X$ is a set of elements called nodes or vertices, and $E$ is a set of non-empty subsets of $X$ called hyperedges or edges. We say a hypergraph is $k$ -uniform if all its hyperedges have size (cardinality) $k$ . We say a hypergraph is linear if every two hyperedges have at most one vertex in common. I am looking for an upper bound for the number of hyperedges of a $k$ -uniform linear hypergraph with $n$ nodes. A naive bound that doesn't take in count the ""linearity"" would be $\binom nk $ This bound is tight in the case $k=2$ since such hypergraphs would simply be graphs, but in the general case I am not sure. Can anyone help me with this? Thanks","['graph-theory', 'combinatorics', 'discrete-mathematics', 'hypergraphs']"
3447229,Positive semidefiniteness of block matrix when diagonal blocks are not invertible,"Let $$M =\left[\begin{array}{cc} A & B\\ B^{T} & D\end{array}\right]$$ where blocks $A$ and $D$ are not invertible, but both are positive semidefinite. Are there conditions such that $M$ is positive semi-definite? For example, consider the case where $$A=\left[\begin{array}{cc} a & -a\\-a & a\end{array}\right], \qquad B=\left[\begin{array}{cc} b_{1} & b_{2}\\ b_{2} & b_{1}\end{array}\right], \qquad D=\left[\begin{array}{cc}0 & 0\\0 & 0\end{array}\right]$$ where $a, b_1, b_2 \in \Bbb R$ . I read that if $D$ is invertible and $A-BD^{-1}B^T$ is positive semidefinite then $M$ is positive semidefinite (using Schur complements). I am wondering if there is a way to show positive semidefiniteness when neither $A$ nor $D$ is invertible; especially, when the matrices $A$ , $B$ , and $D$ could be written in the form given in the example.","['schur-complement', 'positive-semidefinite', 'matrices', 'positive-definite', 'block-matrices']"
3447261,How to Find the Spectrum of an Integral Operator,"I need to find the spectrum of an operator $T: C([0,1]) \to C([0,1])$ defined by $(Tf)(t) = \int_0^t f(x) dx$ . I know that the spectrum is the set of all values $\lambda$ such that $\lambda I - T$ is not invertible, but I'm not sure how to go about finding those values when integrals are involved. I've found several other posts about finding the spectrum of an integral operator here, but they all seem to involve functions of two variables, and that's throwing me off. Any advice would be appreciated.","['integral-operators', 'operator-theory', 'spectral-theory', 'functional-analysis']"
3447290,Odd degree element over $\mathbb{Q}$,"I am working on the following problem: Let $\alpha$ be an element of odd degree over $\mathbb{Q}$ . Prove that $\alpha$ is in the field $\mathbb{Q}(\beta)$ generated over $\mathbb{Q}$ by $\beta = \alpha + \frac{1}{\alpha}$ . I'm not quite sure how to approach this one. If $\alpha$ is an element of odd degree over $\mathbb{Q}$ , I know that the corresponding minimal polynomial of $\alpha$ over $\mathbb{Q}$ has odd degree, call it $n$ . Then, since $\alpha$ is algebraic of degree $n$ over $\mathbb{Q}$ , my idea was to show that $\beta$ is algebraic over $\mathbb{Q}$ of degree greater than $n$ . Then, this would mean that $\mathbb{Q}(\alpha)$ is actually a subfield of $\mathbb{Q}(\beta)$ , and so it would follow that $\alpha$ is contained in the field $\mathbb{Q}(\beta)$ . How can I show that $\beta$ is algebraic of degree greater than $n$ over $\mathbb{Q}$ ? Does this amount to exploiting the equation $\beta = \alpha + \frac{1}{\alpha}$ ? I don't automatically know that, for example, $\alpha^n = 0$ , so I wasn't sure how to do this to prove what I want. Thanks!","['field-theory', 'minimal-polynomials', 'abstract-algebra', 'polynomials']"
3447340,Separation of geometric sequence do not have equal sum.,"Suppose we remove some terms (taking at least 1 item and leaving at least 2 items) from the geometric sequence, $$1, k, k^2, \cdots, k^{n}$$ (with $k>2$ ) and separate the remaining terms into two groups. Prove these two group's sum can never be the same. I'm not even sure how to start on this since you can't find a closed form for a sum of group. The entire sum is $$\frac{1(1-k^{n+1})}{1-k}$$ So the we prove that $$\frac{\frac{1(1-k^{n+1})}{1-k}-(k^p+\cdots)}{2}$$ cannot be attained by the remaining terms.",['algebra-precalculus']
3447369,Number theory problem involving linear congruences,"What three positive integers, upon being multiplied by 3, 5, and 7 respectively and the products divided by 20, have remainders in arithmetic progression with common difference 1 and quotients equal to remainders? My thought process is to solve the system of linear congruences (1) $r \equiv3n_1 \pmod{20}$ (2) $r + 1 \equiv 5n_2 \pmod{20}$ (3) $ r + 2 \equiv 7n_3\pmod{20}$ If I could solve this for $r$ , using the chinese remainder theorem, my thought process is that I could then find $n_1, n_2, n_3$ which work. Plugging (1) into (2) gives $3n_1\equiv 5n_2-1 \pmod{20}$ . Since the inverse of $3$ mod $20$ is $7$ , I could isolate $n_1\equiv 7(5n_2-1) \pmod{20}$ . Now I have that $r\equiv3(7(5n_2-1))=105n_2-21$ which I could then plug into the third equation. I now have $105n_2-21\equiv7n_3-2 \pmod{20} \Rightarrow 105n_2\equiv 7n_3+19 \pmod{20} $ . However, since 105 does not have an inverse mod 20 since $105$ and $20$ are not coprime, I am unable to proceed from here. Am I on the right track? What can I do from here to solve this problem? I also feel I am disregarding the part of the question which says ""and quotients equal to remainders"".","['number-theory', 'abstract-algebra', 'discrete-mathematics']"
3447382,Finding a Galois Extension,"My question is part of a larger problem: I'm supposed to find the minimal polynomial over $\mathbb{Q}$ of $1 + \sqrt[3]{2} + \sqrt[3]{4}$ ""using the automorphisms of the corresponding Galois extension."" After talking to my professor, I know that the Galois group I'm looking for is $S_3$ . However, I don't know how to ascertain what the Galois extension is with the information I've been given.","['field-theory', 'galois-theory', 'abstract-algebra']"
3447402,"Minesweeper odds for this scenario, 2 different calculations","I am trying to calculate the odds, for every square (Except M or Q) of a mine being there, without knowing the total mines on the board. I've found 2 different formulas online, which are similar except for one portion & although produce the same number for some sections, a very different answer is given for sections of the board. For example, the blue section I have 2 different answers for. As you can see I've split up the squares into logical sections, where the probability will be the same. For better explanation, the board looks like this: ABCDE
F3G1H
IJ1KL
MNOPQ The sections, broken up by the number they 'touch': Section           # of bombs in section:
-------           ----------------------
(A+B+C+F+G+I+J) = 3
(C+D+E+G+H+K+L) = 1
(G+J+K+N+O+P)   = 1 Note: that I am using the # of bombs to mean the number of bombs contained inside the squares. For example, The green section (A+B+F+I) are 4 squares. At most 4 squares can have 4 mines (1 mine per square). In our case green cannot contain 4 mines though, because of the '3'. Further broken up, Here we get the sections you see in the image. By breaking up sections when we know which squares will give the same odds. I will call these the 'known solutions', or 'absolute solutions' (The right column is the # of bombs inside all of the squares combined): (A+B+F+I) + (C) + (G) + (J) = 3
(G) + (C) + (D+E+H+L) + (K) = 1
(J) + (N+O+P) + (K) + (G) = 1 Here we calculate all of the possible solutions. We do this by making assumptions. First we assume (C) has 1 bomb. In other words, the 'C' square is a bomb. (C is chosen at random, but I prefer to start with a small section). I'll call the first solution 'A1-1': (C) = 1 Since (C) = 1, and ((G) + (C) + (D+E+H+L) + (K)) = 1, we know that (G), (K) and (DEHL) are must be 0: (G) = 0
(D+E+H+L) = 0
(K) = 0 But now we need to make another assumption. I chose (J) = 1. Again, I prefer starting with small sections. This gives us an entire solution (A1-1): Grouping   # of bombs
--------   -----------
(C)       = 1
(D+E+H+L) = 0
(K)       = 0
(G)       = 0
(J)       = 1
(N+O+P)   = 0
(A+F+I+B) = 1 I'll keep assuming (C) is 1 until we've come up with every solution (Note the 'absolute solutions' must always hold true, since that's how Minesweeper works, and we want to utilize what we know: (a1-2)      # of bombs
----        ----------
(C)       = 1
(D+E+H+L) = 0
(K)       = 0
(G)       = 0
(J)       = 0
(N+O+P)   = 1
(A+F+I+B) = 2 That's all for C = 1, so next we assume G=1: a2-1        # of bombs
----        ----------
(C)       = 0
(G)       = 1
(D+E+H+L) = 0
(K)       = 0
(N+O+P)   = 0
(J)       = 0
(A+F+I+B) = 2

a2-2
----
(C)       = 0
(G)       = 0
(J)       = 1
(A+F+I+B) = 2
(N+O+P)   = 0
(D+E+H+L) = 1
(K)       = 0

a2-3
----
(C)       = 0
(G)       = 0
(J)       = 0
(K)       = 1
(D+E+H+L) = 0
(A+F+I+B) = 3
(N+O+P)   = 0

a2-4
----
(C)       = 0
(G)       = 0
(J)       = 0
(K)       = 0
(D+E+H+L) = 1
(A+F+I+B) = 3
(N+O+P)   = 1 That gives us every solution. Now we list the number of bombs in every possible solution : Note that: 
(A+F+I+B) is Green, 
(C)       is Pink, 
(D+E+H+L) is orange, 
(G)       is brown, 
(J)       is Yellow, 
(K)       is Purple
(N+O+P)   is blue: #:      A1  A12 A21 A22 A23 A24
GREEN:  1   2   2   2   3   3
PINK:   1   1   0   0   0   0   
ORANGE: 0   0   0   1   0   1
BROWN:  0   0   1   0   0   0
YELLOW: 1   0   0   1   0   0
PURPLE: 0   0   0   0   1   0
BLUE:   0   1   0   0   0   1 Now we calculate the combinations possible for every solution. This is done by using nCr ( Binomial coefficient ). Where N = Number of Squares and B = numberOfBombs. Combinations = N NCR B. For the first solution (A1-1) these are the combinations: (GREEN)   = 4 NCR 1 = 4
(PINK)    = 1 NCR 1 = 1
(ORANGE)  = 4 NCR 0 = 1
(BROWN)   = 1 NCR 0 = 1
(YELLOW)  = 1 NCR 1 = 1
(PURPLE)  = 0 NCR 1 = 1
(BLUE)    = 3 NCR 0 = 1 Multiplying these combinations we get: 4*1*1*1*1*1*1 = 4 combinations for this solution (A1-1). Doing the same for all solutions we get: #:      A1  A12 A21 A22 A23 A24
GREEN:  4   6   6   6   4   4
PINK:   1   1   1   1   1   1   
ORANGE: 1   1   1   4   1   4
BROWN:  1   1   1   1   1   1
YELLOW: 1   1   1   1   1   1
PURPLE: 1   1   1   1   1   1
BLUE:   1   3   1   1   1   3
TOTALS: 4   18  6   24  4   48

Total combinations = 104 Note: In the above table, to get 'TOTALS' we multiply all combinations to get the total combinations for that solution. Now for the part that I am conflicted on. I choose 'Blue' to demonstrate, since I am getting a different answer using either method. Method 1: For each solution take the number of mines divided by the number of squares (3) and multiply by the combinations: A1-1      A1-2      A2-1    A2-2     A2-3    A2-4
(0/3*4)   (1/3*18)  (0/3*6) (0/3*24) (0/3*4) (1/3*48) Adding those numbers up (Taking away the 0's to make it easier): (1/3*18) + (1/3*48) = 22. Now divide by the total combinations (104): 22/104 = 0.212. However there are 3 squares, so we can divide by 3 if we want the odds of a single square in the section: 0.212/3 = 0.0705 Method 2 Multiply the total combinations for the non-zero values (48 + 18), divide by the total combinations (104): 1*66/104 = .635. Again we can divide by 3 if we want the odds of a single square: .635/3 = .212 So, are my odds for hitting a mine on any given blue square .212% , .0705% , or something else?","['combinations', 'probability']"
3447415,Solve inequality $\cos(x)+2\tan(x)\le2+\sin(x)$,Solve inequality $\cos(x)+2\tan(x)\le2+\sin(x)$ My proof: $\cos(x)+2\tan(x)\le2+\sin(x)\\\cos^2(x)+2\sin(x)\le2\cos(x)+\sin(x)\cos(x)\\\cos(x)\left(\cos(x)-2 \right )+\sin(x)\left(2-\cos(x) \right )\le0\\\left(\cos(x)-\sin(x) \right )\left(\cos(x)-2 \right )\le0\\\sqrt{2}\left(\sin\left(x-\frac{\pi}{4} \right ) \right )\left(\cos(x)-2 \right )\le0\\-\sqrt{2}\cos\left(x+\frac{\pi}{4} \right )\left(\cos(x)-2 \right )\le0$ I stopped at this moment and I have no idea what to do now,['trigonometry']
3447421,"Existence of ""checkerboard"" grid whose vertices intersect all given points on plane","This is a question about identifying a theorem or maybe even a subject matter. I've asked myself a question, but I don't know where to look for answers. Typing a problem statement in a search engine doesn't really work unless you know a theorem exists and the exact statement it addresses. I'm going to try to present my thoughts/questions with hopes that someone might recognize a more specific theorem/subject. My thoughts/questions which follow are not meant to be rigorous...just trying to help convey what I'm looking for. Part I Given a set of unique points arbitrarily distributed over the domain $\mathbb{R}^2$ , there exists a uniform grid in which all points coincide with a vertex of the grid. (When I say ""uniform grid"", think checkerboard...can be oriented any angle, but all elements must be squares) Part II Under what conditions is Part I true or false? (Maybe it's always true or always false) Part III If Part I is true, how does the edge length of the square element relate to the distribution of the points? Again, the specific questions aren't meant to be answered... I'm trying to find references to research further. EDIT The set of points would be finite. Imagine dumping a bucket of marbles on the floor of an empty room. Where the marbles stop, those are your points. The grid has to be generated such that every marble coincides with a vertex of the checkboard. The boundaries do not bound the grid (they're only there to contain the marbles) Initially I was thinking that you have to relate it to the distance between the points. In which case, I would think that all distances would have to be a multiple of the smallest. Then I was thinking, what if the distances were primes? I don't think anything would work. Then I was thinking that the only possible case which would work for all, would be if the smallest distance was the diameter of the point. But then what is the diameter of a point if its just a point...therefore, the only possible grid would be the plane itself. I don't know if that would ""qualify"" as a grid, but I think you get my point. So we have some potential worst case and best case scenarios...I'm looking for more rigorous theorems or proofs which help answer these questions. I just assumed they exist as the question is not that abstract.","['optimization', 'geometry', 'finite-differences', 'differential-geometry']"
3447470,"Given a deformed cube, find the closest undeformed cube","Given a deformed cube, how can I find the closest matching undeformed cube? More precisely, given a deformed cube defined by its eight vertices $\langle p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8 \rangle$ and its radius $r$ , how can I find the vertices of the nearest undeformed cube of the same radius such that the distance between the matching vertices of the deformed and undeformed cube is minimized? To accomplish this, the following equation must be minimized: $$E=\sum_{n=1}^8 \Vert p_n-u_n\Vert$$ where $p_n$ are the points of the deformed cube and $u_n$ are the points of the undeformed cube.  But the equation must also be subject to the constraint that the points $u_{1,2,3...8}$ form a perfect cube of radius $r$ . We can assume that the given cube is only slightly deformed, so that it is easy to tell which vertex matches with its undeformed counterpart. I imagine this will involve some sort of numerical iterative procedure. Minimal computational cost is also desirable. Any ideas? EDIT: It's probably okay to assume that the center of the undeformed cube is the average of the deformed cube's vertices. Under this assumption, we only need to determine the rotation of the cube. Hopefully this simplifies the problem. EDIT 2: It looks like this is very similar to Wahba's problem .","['optimization', 'numerical-methods', 'geometry']"
3447480,"Solve for $x,y$ $\begin{cases}\sin^2x+\sin^2y=\frac{3}{4}\\x+y=\frac{5\pi}{12}\end{cases}$","Designate $x,y\in\left(0,\frac{\pi}{2}\right)$ which fulfills $\begin{cases}\sin^2x+\sin^2y=\frac{3}{4}\\x+y=\frac{5\pi}{12}\end{cases}$ My proof: $\cos2x=1-2\sin^2x\Rightarrow \sin^2x=\frac{1}{2}-\frac{1}{2}\cos2x$ $\sin^2x+\sin^2y=\frac{3}{4}\Rightarrow1-\frac{1}{2}\left(\cos2x+\cos2y\right)=\frac{3}{4}\Rightarrow \cos2x+\cos2y=\frac{1}{2}\\ \cos x+\cos y=2\cos\frac{x+y}{2}\cos\frac{x-y}{2}\\\cos2x+\cos2y=\frac{1}{2}\Rightarrow \cos(x+y)\cos(x-y)=\frac{1}{4}\\\cos\left(\frac{5\pi}{12}\right)\cos\left(2x-\frac{5\pi}{12}\right)=\frac{1}{4}\\\cos2x=2\cos^2x-1\Rightarrow\cos\left(\frac{10\pi}{12}\right)=2\cos^2\left(\frac{5\pi}{12}\right)-1\Rightarrow\cos\left(\frac{5\pi}{12}\right)=\sqrt{\frac{-\frac{\sqrt{3}}{2}+1}{2}}=\frac{\sqrt{2-\sqrt{3}}}{2}\\\cos\left(2x-\frac{5\pi}{12}\right)=\frac{1}{4}\cdot\frac{2}{\sqrt{2-\sqrt{3}}}=\frac{1}{2\sqrt{2-\sqrt{3}}}$ I don't know what to do next,I don't know if it makes sense what I wrote",['trigonometry']
3447525,Show that function $E$ is non-multiplicative.,"Definition 1: A function $f$ is said to be non-multiplicative if $$f(ab)\ne f(a)f(b)$$ for all coprime integers $a,b>1$ . Definition 2: We define the function $E$ as $$E(n)= n+1-\tau (n)- \phi(n)$$ $$=\sum_{(n,d)\notin\{1,d\} \\ \ \ \ \ 1<d<n}1.$$ Here, $(n,d)$ denotes gcd $(n,d)$ The values of $E(n)$ A045763 Question: Show that the function $E$ is non-multiplicative. $\\$ Example: Let $a=10$ , $b=7$ , $ab=n=70$ . We have $E(70)=39$ , $E(10)=3$ , $E(7)=0$ , and $E(70)\ne E(10)E(7)$ .","['number-theory', 'elementary-number-theory']"
3447527,"Solutions $a,b,c \mid 1+a+b+c $ over positive integers algebraically (avoiding brute force)?","I arrived at the diophantine problem for $3$ positive integers that should maintain all 3 divisibilities jointly: $$ a | 1+a+b+c \\\
   b | 1+a+b+c \\\
   c | 1+a+b+c \tag 1 \\\
$$ I tried putting up a matrix expression introducing positive integer parameters $(i,j,k)$ $$\begin{array}{} & \\ 
   \begin{bmatrix}
   -i&1&1  \\
    1&-j&1  \\
    1&1&-k  \\
\end{bmatrix} & \cdot \begin{bmatrix} a\\b\\c \end{bmatrix}  
   =\begin{bmatrix} -1\\-1\\-1 \end{bmatrix}
\end{array} \tag 2 $$ but the fiddling with the occuring formulae using $(i,j,k)$ now is again not yet conclusive. I didn't find another convincing ansatz towards a formula. By brute force ( $2\le a \le b \le c \le 120$ ) I found the following solutions (avoiding symmetries) for $[1,a,b,c]$ [1, a, b, c]
--------------
[1, 2, 2, 5]
[1, 2, 3, 6]
[1, 2, 6, 9]
[1, 3, 4, 4]
[1, 3, 8, 12]
[1, 4, 5, 10]        
[1, 6, 14, 21] Searching using eq(2) with $1\le i \le j \le k \le 32$ I got [i, j, k] ---> [1, a,   b, c]  rotated
-------------------------------
[1, 2, 6] ---> [1,21,14, 6]      R
[1, 2, 7] ---> [1,12, 8, 3]      R
[1, 2, 8] ---> [1, 9, 6, 2]      R
[1, 2,11] ---> [1, 6, 4, 1]
[1, 3, 4] ---> [1,10, 5, 4]      R
[1, 3, 5] ---> [1, 6, 3, 2]      R
[1, 3, 7] ---> [1, 4, 2, 1]
[1, 4, 4] ---> [1, 5, 2, 2]      R
[1, 5, 5] ---> [1, 3, 1, 1]
[2, 2, 3] ---> [1, 4, 4, 3]      R
[2, 2, 5] ---> [1, 2, 2, 1]
[3, 3, 3] ---> [1, 1, 1, 1] which are simply rotated versions or trivials (having $a or b or c=1$ ) which are excluded in the above list. I guess these are all possible solutions, but don't find the argument... Q1: How can I approach that problem algebraically? Q2: is the number of solutions finite or infinite? Q3.1: if the set of solutions is infinite is there a parametrization? Q3.2: if the set of solutions is finite, what is that set? update In generalizing the problem towards [h,a,b,c] with $h \in \mathbb N^+$ I seem to get the full list of $14$ solutions (avoiding solutions with $\gcd()>1$ and rotations) which are [h, a, b, c]         some interpretations 
--------------------------------------------
[1, 1, 1, 1]
[1, 1, 1, 3]
[1, 1, 2, 2]
[1, 1, 2, 4]
[1, 1, 4, 6]
[1, 2, 2, 5]          1+2+2=5:   5=5/1
[1, 2, 3, 6]        1+2=3:  3=1*3  6=2*3
[1, 2, 6, 9]        1+2=3:  6=2*3  9=3*3
[1, 3, 4, 4]        1+3=4:  4=1*4  4=1*4
[1, 3, 8, 12]       1+3=4:  8=2*4 12=3*4
[1, 4, 5, 10]       1+4=5:  5=1*5 10=2*5
[1, 6, 14, 21]      1+6=7: 14=2*7 21=3*7
[2, 3, 3, 4]          2+3+3=8:  4=8/2
[2, 3, 10, 15]      2+3=5: 10=2*5 15=3*5 A bit context: this is a detail-problem in an earlier question where I explore the general conditions in the Lehmer's totient problem. In the earlier question I've considered a three-variable $(R,S,T)$ diophantine system, and looked at solutions of the form $(R,S,T)=(R^1,R^a,R^b)$ Here I generalize it to $4$ variables and solutions of $(Q,R,S,T)=(Q^1,Q^a,Q^b,Q^c)$ resp. $(Q,R,S,T)=(Q^h,Q^a,Q^b,Q^c)$ ( $\gcd(h,a,b,c)=1$ ) and determine the solutions in terms of $(1,a,b,c)$ resp. $(h,a,b,c)$ . I'll later generalize to more variables but first I want to get some grip about the general limitations and whereabouts - at best in a form which support later generalizations...","['modular-arithmetic', 'divisibility', 'number-theory', 'elementary-number-theory', 'diophantine-equations']"
3447529,On the relationship between $\Re\operatorname{Li}_n(1+i)$ and $\operatorname{Li}_n(1/2)$ when $n\ge5$,"Motivation $\newcommand{Li}{\operatorname{Li}}$ It is already known that: $$\Re\Li_2(1+i)=\frac{\pi^2}{16}$$ $$\Re\Li_3(1+i)=\frac{\pi^2\ln2}{32}+\frac{35}{64}\zeta(3)$$ And by this question , assuming $\Li(1/2)$ is a simpler form than $\Re\Li(1+i)$ , we are able to simplify $\Re\Li_4(1+i)$ . $$\Re\Li_4(1+i)=-\frac5{16} \Li_4\left(\frac{1}{2}\right)+\frac{97 \pi ^4}{9216}-\frac{5}{384} \ln^42+\frac{1}{48} \pi ^2 \ln^22$$ Going further, I tried to find some numerical evidence to simpify $\Re\Li_5(1+i)$ , and I conjectured that $$\Re\Li_5(1+i)\stackrel?=\frac{5}{32} \text{Li}_5\left(\frac{1}{2}\right)+\frac{2139 \zeta (5)}{4096}-\frac{1}{768} \ln^52+\frac{1}{288} \pi ^2 \ln^32+\frac{97 \pi ^4 \ln2}{18432}$$ As I verified it up to 1000 digits, I believe it holds. How can we prove it? Thoughts Well, I honestly does not have much thoughts on it but I think turning it into an integral or Euler sum is a possible pathway. Therefore, I turned it into an integral: $$\int_0^1\frac{54x^2-150x+59}{2x^3-6x^2+5x-2}\ln^4xdx\stackrel?=\ln^52-\frac83\pi^2\ln^32-\frac{97}{24}\pi^4\ln2-\frac{6417}{16}\zeta(5)$$ Further Question Can $\Re\Li_n(1+i)$ be simplified? Or at least can it be turned into a combination of $\Li(1/2)$ and some other simpler constants? (Assuming Multiple Zeta Values are simpler) Update Another numerical experiment suggests $$\Re\operatorname{Li}_6(1+i)\stackrel?=-\frac{5 \text{Li}_6\left(\frac{1}{2}\right)}{64}+\frac{69 S}{512}+\frac{207 \zeta (3)^2}{4096}+\frac{2139 \zeta (5) \log (2)}{8192}+\frac{18779 \pi ^6}{30965760}-\frac{\log ^6(2)}{9216}+\frac{\pi ^2 \log ^4(2)}{2304}+\frac{97 \pi ^4 \log ^2(2)}{73728},$$ where $S=\sum_{n=1}^\infty\frac{(-1)^{n-1}H_n}{n^5}$ . Further numerical experiments suggest that $$J_n=\int_0^1\left(\frac{1-x}{x^2-2 x+2}+\frac{5 x}{x^2-2}\right) \ln^nxdx$$ may be turned into the same form of the result of ""logarithm integral"", mentioned in the paper in the comment, containing $1−x,x,x+1$ only, of weight $n+1$ . If we can find a substitution that can directly convert it into ""log integral"", the whole question will be solved by evaluating this integral. Another Update An interesting phenomenon discovered by observing the coefficient between adjacent terms: $$J_{n-1}=\frac4n\left(\frac{\ln2}2\right)^n\sum_{k=0}^na_kP_k(n)$$ where $$P_k(n)=n(n-1)(n-2)\cdots(n-(k-1))(4\ln2)^{-k}$$ and $$\begin{array}{|c|c|c|c|c|c|c|c|}\hline k&0&1&2&3&4&5&6\\\hline a_k&-1&0&5\zeta(2)&0&-343\zeta(4)&4278\zeta(5)&8832S+3312\zeta^2(3)-53383\zeta(6)\\\hline\end{array}$$ Therefore, it may be true that all $a_k$ is a linear combination of alternating multiple zeta function with integer coefficients of weight $k$ . (i) Also, there might be a formula to calculate $a_k$ directly. (ii) $$\begin{array}{|c|c|}\hline k&7\\\hline a_k&-304707\zeta(7)-2208(7\zeta(3)\zeta(4)+6\zeta(2)\zeta(5)+16\zeta(-5,1,1)+8\zeta(-4,2,1))\\\hline k&8\\\hline a_k&\frac1{51}\left(-\frac{199260125\zeta(8)}{144}+280600\zeta(3)\zeta(5)-261088\zeta(-7,1)-34827\zeta(6,2)+2944(3\zeta^2(3)\zeta(2)+7\zeta(-3,3,2)+4\zeta(-3,3,1,1))\right)\\\hline\end{array}$$ This supports the conjecture (i). (Note that $S$ can also be regarded as Multiple Zeta Value.)","['calculus', 'polylogarithm', 'definite-integrals', 'summation']"
3447539,Is it correct to move the $\lim$ operator inside $\| \cdot \|$ in deriving this partial derivative?,"Let $X$ be open in $\mathbb R^n$ , $F$ a Banach space, and $m \in \mathbb N^*$ . Suppose $f:X \to F$ such that $\partial_{j_1} \cdots \partial_{j_{m+1}} f$ and $\partial^m f$ exist in a neighborhood of $a$ for all $j_1, \ldots, j_{m+1} \in \{1,\ldots,n\}$ . Assume $h^i = \left  (h_1^i, \ldots, h_n^i\right ) \in \mathbb R^n$ with $1 \le i \le m$ .  We define a map $A$ by $$\begin{array}{l|rcl}
A & {(\mathbb R^n)}^m
 & \longrightarrow & F \\
    & \left  [h^1, \ldots,h^m\right ] & \longmapsto & \sum_{j_1, \ldots, j_m =1}^n  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \left  (h^1_{j_1} \cdots h^m_{j_m}\right ) \end{array}$$ I have verified that $A$ is a multilinear map and thus $A \in \mathcal L^m(\mathbb R^n, F)$ . Moreover, it follows from the definition of mixed partial derivative that $\partial^m f(a) \in L^m(\mathbb R^n, F)$ . In the following, I try to prove that $$\partial_j (\partial^m f)(a) = A$$ I'm not sure if, in (5), I correctly move the lim operator inside the norm operator. Could you please verify if this step is correct? Thank you so much! My attempt: First, we have $$\begin{aligned}
&\frac{\partial^m f(a +te_j) - \partial^m f(a)}{t} \left  [h^1, \ldots,h^m\right ] - A \left  [h^1, \ldots,h^m\right ] \\
={}& \frac{\partial^m f(a +te_j)[h^1, \ldots,h^m] - \partial^m f(a) [h^1, \ldots,h^m]}{t}  - A [h^1, \ldots,h^m]\\
={}&  \frac{ \sum_{j_1, \ldots, j_m =1}^n \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) \left   (h^1_{j_1} \cdots h^m_{j_m}\right )-  \sum_{j_1, \ldots, j_m =1}^n \partial_{j_1}  \cdots \partial_{j_m} f (a)  \left  (h^1_{j_1} \cdots h^m_{j_m}\right )}{t}  \\
& \quad - \sum_{j_1, \ldots, j_m =1}^n  \partial_j \partial_{j_1} \cdots \partial_{j_m} f (a)  \left  (h^1_{j_1} \cdots h^m_{j_m}\right ) \\
={}&  \sum_{j_1, \ldots, j_m =1}^n \left (h^1_{j_1} \cdots h^m_{j_m} \right) \cdot \left ( \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) -\partial_{j_1}  \cdots \partial_{j_m} f (a) }{t}   -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \right )
\end{aligned}$$ It follows that $$\begin{aligned}
   & \lim_{t \to 0} \left \| \frac{\partial^m f(a +te_j) - \partial^m f(a)}{t} - A\right \| \\
\overset{(1)}{=}{}& \lim_{t \to 0} \sup_{\|h^1\|_1\le1,\ldots,\|h^m\|_1 \le 1} \left \|  \sum_{j_1, \ldots, j_m =1}^n  \frac{\partial^m f(a +te_j) - \partial^m f(a)}{t} \left  [h^1, \ldots,h^m\right ] - A \left  [h^1, \ldots,h^m\right ] \right \|\\
\overset{(2)}{\le}{}& \lim_{t \to 0} \sup_{\|h^1\|_1\le 1,\ldots,\|h^m\|_1 \le 1}   \sum_{j_1, \ldots, j_m =1}^n \left |h^1_{j_1} \cdots h^m_{j_m} \right | \cdot\bigg \| \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t}  \\
\overset{(3)}{\le}{}& \lim_{t \to 0} \sup_{\|h^1\|_1\le 1,\ldots,\|h^m\|_1 \le 1}   \sum_{j_1, \ldots, j_m =1}^n \|h^1\|_1 \cdots \|h^m\|_1  \cdot\bigg \| \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t}  \\
&  \quad -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\bigg \|\\
\overset{(4)}{\le}{}& \lim_{t \to 0}  \sum_{j_1, \ldots, j_m =1}^n \left \| \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t} -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\right \|\\
 ={}&  \color{blue}{\sum_{j_1, \ldots, j_m =1}^n \lim_{t \to 0} \left \|  \frac{  \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t} -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\right \|}\\
\overset{(5)}{=}{}&  \color{blue}{\sum_{j_1, \ldots, j_m =1}^n \left \| \lim_{t \to 0} \frac{  \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t} -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\right \|}\\
={}&   \sum_{j_1, \ldots, j_m =1}^n \left \|  \partial_j \left ( \partial_{j_1}  \cdots \partial_{j_m} f \right ) (a)  -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \right \|\\
={}&   \sum_{j_1, \ldots, j_m =1}^n \left \|  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)  -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \right \|\\
={}&  0
\end{aligned}$$ $(1)$ : This follows from the definition of the operator norm of a multilinear map. $(2)$ : This follows from triangle inequality. $(3)$ : It follows from the definition of $\|\cdot\|_1$ that $|h^1_j| \le \|h^1\|_1,\ldots, |h^m_j| \le \|h^m\|_1$ for all $j \in \{1,\ldots,n\}$ . As such, $\left |h^1_{j_1} \cdots h^m_{j_m} \right | =\left |h^1_{j_1}\right | \cdots \left | h^m_{j_m} \right | \le \|h^1\|_1 \cdots \|h^m\|_1$ . $(4)$ : It follows from $\|h^1\|_1\le 1,\ldots,\|h^m\|_1 \le 1$ that $\|h^1\|_1 \cdots \|h^m\|_1 \le 1$ . Hence $$\partial_j (\partial^m f)(a) = A$$","['proof-verification', 'real-analysis', 'multivariable-calculus', 'partial-derivative', 'derivatives']"
3447549,"Munkres Lemma 31.1, no need for $T_1$","In Munkres we have the following lemma: Lemma 31.1. Let $X$ be a topological space. Let one-point sets in $X$ be closed.
(a) $X$ is regular ii and only if given a point $x$ of $X$ and a neighborhood $U$ of $x$ ,
there is a neighborhood $V$ of $x$ such that $\operatorname{cl}(V)\subseteq U$ . I do not see the use of $T_1$ assumption (Let one-point sets in $X$ be closed.) in the proof. I proved it myself before finding the result in Munkres without needing $T_1$ . (1) Could you confirm my statement? (2) Can we say the same for the case of normal spaces (Lemma 31.1(b))? I've yet to start with the proof.","['general-topology', 'separation-axioms']"
3447556,What is the missing discrete valuation ring?,"Let $k$ be an algebraically closed field and $K$ a function field of dimension 1 over it. Then we have that the projective 1-space $P^1$ over $k$ is isomorphic to the abstract variety $C_{K/k}$ (using the terminology and notations as in Hartshornes book) whose underlying space is the set of all discrete valuation rings of $K/k$ , endowed with the finite complement topology. Also, we know the affine line lacks the “point at infinity” with respect to $P^1$ . So my question is, what is the corresponding discrete valuation ring that is the stalk at the point at infinity? I know that if we embed $A^1$ into $P^1$ by mapping $x$ to $(x,1)$ , then the missing point is just $(1,0)$ which has stalk isomorphic to $k[x,y]_{(y)}$ but this is isomorphic to the stalk $k[x,y]_{(x)}$ at $(0,1)$ which is weird since it implies the only discrete valuation ring missing is actually isomorphic to another that is not??","['algebraic-geometry', 'abstract-algebra', 'valuation-theory']"
3447575,Why is the tangent of an angle called that?,"I am teaching the foundations of trig and find it a bit weird that tangent is called that. I've never questioned it before, but what I keep finding online is the phrase 'tangent of an angle'. Is anyone able to explain, maybe using some visual intuition, why we call it the tangent of an angle? Especially in the context of the unit circle. Does it relate to the definition of a tangent to a curve?","['trigonometry', 'terminology']"
3447588,Solve for $y'\cos{y}=\sin(x+y)$,"Solve for $y'\cos{y}=\sin(x+y)$ My attempt $$y'\cos{y}=\sin x \cos y + \cos x \sin y$$ Divide both side by $\cos y$ $$\frac{dy}{dx}=\sin x + \cos x \tan y$$ Integrate both sides and I got $$y = -\cos x + \int \cos x \tan y \,dx$$ As Maximilian Janisch said that $\int\cos(𝑥) \tan(𝑦(𝑥))d𝑥≠\sin(𝑥) \tan(𝑦(𝑥))$ I didn't know what to do next.","['calculus', 'ordinary-differential-equations']"
3447589,"A meromorphic function $f$ with poles of order $2$ at $\sqrt { n }$ ($n=1,2,3,...$),","find a meromorphic function $f$ with poles of order 2 at $\sqrt { n }$ ( $n=1,2,3,...$ ), the Residue at each
pole is $2$ , and $\lim _ { z \rightarrow \sqrt { n } } ( z - \sqrt { n } ) ^ { 2 } f ( z ) = 1$ for all $n \in \mathbb N$ . I tried with $f ( z ) = \frac { 1 } { \left( \sin \pi z^ { 2 } \right) ^ { 2 } }$ , but conditions are not fully
conciding",['complex-analysis']
3447626,Are trigonometry functions Ratios or Distance?,"Consider a point, say $S(2,3)$ . Now here $3$ indicate that $S$ is $3$ units away from x axis. Right? Now consider what Wikipedia says : The trigonometric functions cos and sin are defined, respectively, as
  the x- and y-coordinate values of point A. This definition of $sin$ and $cos$ is based on unit circle. In this definition sin is defined as Y-coordinate of point $A$ on unit circle. But now what do we mean by Y-coordinate ? Y-coordinate is distance between point $A$ to $x$ - axis ( Right? ).
How could sin or for that matter any trigonometric function can be a distance? Trigonometry functions, for acute angle, are defined as ratios of sides. How could they be ""distance""(with unit) in one definition and ""ratio"" (unitless) in other?","['coordinate-systems', 'trigonometry']"
3447630,Partition of unity on a manifold with a non-vanishing global vector field,"Suppose $M$ is manifold with a global non-vanishing vector field $X$ . Let $\{U\}_{\alpha\in I}$ be a locally finite covering of $M$ such that for each $\alpha$ and $p\in U_{\alpha}$ the maximal integral curve of $X$ containing $p$ is contained in $U_{\alpha.}$ Is it possible to find a partition of unity $\{\phi_{\alpha}\}_{\alpha\in I}$ subordinate to the covering such that $X(\phi_{\alpha})=0,$ for all $\alpha\in I$ ?","['manifolds', 'differential-geometry']"
3447686,"Let $A,B,X$ be sets such that $A \cup B = X$ and $A \cap B= \phi$. Show that, $A = X\setminus B$ and $B=X\setminus A$. (Tao's Analysis I)","I would like someone to verify my proof. If there's a better/cleaner way to do it, I'd really love to know. Let $A,B,X$ be sets such that $A \cup B = X$ and $A \cap B= \phi$ . Show that, $A = X\setminus B$ and $B=X \setminus A$ . Proof. (My attempt). Claim. $A = X \setminus B.$ ( $\Rightarrow$ direction). Let $x$ be an arbitrary element in the set $A$ . $\begin{align}
&x \in A \\
\text{So, } & x \in (A \cup B) \\
\therefore \text{ } & x \in X \\
\therefore \text{ } & (x \in B) \lor (x \in X \setminus B) \\
& \text{If x is in B, then x belongs to both A and B. This is a}\\ 
& \text{contradiction as the intersection is an empty set.}\\
\implies & (x \in X \setminus B)\\
\therefore \text{ } & A \subseteq (X \setminus B)
\end{align}$ ( $\Leftarrow$ direction). Let $y$ be an arbitrary element in the set $X \setminus B$ . Therefore, $y \in (A \cup B)$ . Which means that $(y \in A) \lor (y \in B)$ . But, $y \notin B$ (because $y$ belongs to $X \setminus B)$ . Therefore, $y \in A$ . This closes the proof.","['elementary-set-theory', 'proof-verification']"
3447717,Proof of Monotone convergence theorem.,"In this proof of MCT, I don't understand the the use of the number $\alpha$ . When i ignore it and read the proof I don't see where it fails. Thanks for help. Proof. Let $(X,\mathbb{X},\mu)$ be a measure space. Then since $f_{n} \leq f$ for all $n$ it follows that $\lim _{n \rightarrow \infty} \int f_{n} \mathrm{d} \mu \leq$ $\int f \mathrm{d} \mu .$ To show the inequality in the other direction we fix a simple function $\phi \leq f, 0<\alpha<1$ and let $$
A_{n}=\left\{x: \alpha \phi(x) \leq f_{n}(x)\right\}
$$ Note that for each $n$ , $A_{n} \in \mathbb{X}$ and $A_{n} \leq A_{n+1} .$ We also have that $X=$ $\cup_{n=1}^{\infty} A_{n} .$ By Lemma 4.5 we can define a measure $\nu$ on $(X, \mathbb{X})$ by $$
\nu(A)=\int_{A} \phi \mathrm{d} \mu(x)
$$ for all $A \in \mathbb{X} .$ It follows from Lemma 3.4 that $$
\int \phi \mathrm{d} \mu=\nu(X)=\nu\left(\cup_{n=1}^{\infty} A_{n}\right)=\lim _{n \rightarrow \infty} \nu\left(A_{n}\right)
$$ However for all $n \in \mathbb{N}$ $$
\int_{X} f_{n} \mathrm{d} \mu \geq \int_{A_n} f_{n} \mathrm{d} \mu \geq \alpha \int_{A_{n}} \phi \mathrm{d} \mu=\alpha \nu\left(A_{n}\right)
$$ and thus $\alpha \int \phi \mathrm{d} \mu \leq \lim _{n \rightarrow \infty} \int f_{n} \mathrm{d} \mu .$ Since this holds for all $0<\alpha<1$ and
simple functions $\phi \leq f$ it follows that $$
\int f \mathrm{d} \mu \leq \lim _{n \rightarrow \infty} \int f_{n} \mathrm{d} \mu
.$$","['measure-theory', 'lebesgue-measure', 'proof-explanation', 'lebesgue-integral', 'limits']"
3447749,"Associated vector bundles: Given a vector bundle $E$ show that $F(E)\times \mathbb{R}^n/GL(n,\mathbb{R})\cong E$","I am trying to understand the map between vector bundles and principal fibre bundles and would like to work out the following example explicitly. Moreover in this example I am only currently looking at one direction, I would like to understand this well to attempt the other direction after this. Given a vector bundle $ \pi:E\rightarrow M$ of rank $n$ , we can consider the frame bundle $F(E)$ . Since $F(E)$ is a principle fibre bundle we can consider the vector bundle associated to $F(E)$ which is $F(E)\times \mathbb{R}^n/GL(n,\mathbb{R})$ . Show that $\mathcal{P}=F(E)\times \mathbb{R}^n/GL(n,\mathbb{R})\cong E$ . Intuitively this makes sense but to fully understand this I should be able to produce a rigour argument. As I understand we can think elements of $F(E)$ fibrewise as maps $p: \mathbb{R}^n\rightarrow E_x$ . For $\mathcal{P}$ an element is an equivalence class $[p,v]$ where $p$ is a map as above, $v\in E_x$ and $(p,v)\sim(pg,\rho(g^{-1})v)$ (I'm not entirely sure what $\rho(g^{-1})v$ is, I know $\rho$ is the ""standard representation of $GL(n,\mathbb{R})$ in $\mathbb{R}^n$ but I don't know what this representation is). Taking inspiration from the wikipedia page, I claim that the isomorphism is given by $[p,v]\mapsto p(v)$ . First we show this is well defined. By definition of this map $[pg,\rho(g^{-1})v]\mapsto pg(\rho(g^{-1})v)$ ? If I understood the representation It would probably make sense why the $g$ and $\rho(g^{-1})$ cancel. It seemed like surjectivety should be clear, as for any $v\in E$ we can just take $[id,v]$ but this doesn't make sense. Also should the elements in $E$ be pairs $(v,p)$ with $p$ a point in the base manifold?","['principal-bundles', 'vector-bundles', 'differential-geometry']"
3447769,"Prove that relation ""a - b is even"" is an equivalence relation","I have a relation $R$ defined on the set of integers as follows: $(a,b) \in R ⇔ a-b\text{ is even}$ I have to prove that $R$ is an an equivalence relation. What I have: I first check whether $R$ is reflexive. If $a = b$ , $a-b = 0$ . Zero is an even number. Therefore reflexivity is OK. Than I check for symmetry:
If $$a-b = 2Z   \\
b-a = (-1)(a-b)  ⇒ (b-a) = -2Z$$ $-2Z$ looks also an even number, so symmetry i OK. Then I should check for transitivity but don't know how to do it. Can someone please explain? Also: are my tests for reflexivity and symmetry correct? I didn't do math for several years, so my formulation of the solution is probably also incorrect. Edit: I think I have it. Let's say that $a-b = 2k$ and $b-c =2l$ , where $k$ and $l$ are some integers. Then I sum the equations: $a + b - b - c = 2l +2k$ Which is: $a - c = 2(k+l)$ The right side is an integer times 2. So, when $(a-b) \in R$ and $(b-c) \in R$ , $(a-c)$ is also $\in R$ . Is this correct?","['elementary-set-theory', 'equivalence-relations', 'relations']"
3447900,How do I recover a convex region from the set of its tangent planes?,"Let $S\subset\mathbb R^n$ a convex region, and suppose we know how to characterise this region in terms of its tangent planes. Explicitly, this means that, for every direction $\hat n_\theta\in S^{n-1}$ , we know that all the points $x\in S$ satisfy $$\langle \hat n_\theta, \boldsymbol x\rangle\le M(\theta)$$ for some known function $M(\theta)$ . Moreover, we know that this inequality is tight (and so there is some point in $S$ which saturates it). How can we go from this description to an algebraic description for the surface? As a simple example in $\mathbb R^2$ , suppose $S$ is a circle, but we don't know this. Instead, we only know that it is a region such that, for every direction $\hat n_\theta$ , it lies below a given plane. We can visualise this as follows: where the green arrow represents the directions $\hat n_\theta$ , and the blue line is given algebraically for each angle $\theta$ by $\cos(\theta) (x-x_0)+\sin(\theta)(y-y_0)= r,$ where $x_0,y_0,r$ are the coordinates of the center of the circle and its radius, respectively.
The function $M(\theta)$ is thus, in this case, given by $M(\theta)=\cos(\theta) x_0 + \sin(\theta) y_0 + r$ . How do I use this $M(\theta)$ to retrieve an algebraic characterisation for the region, which in this toy example would be $(x-x_0)^2+(y-y_0)^2=r^2$ ?","['convex-geometry', 'convex-analysis', 'geometry', 'convex-hulls']"
3447902,Inverse of sigmoid equation,"I hope someone can help me. I have this green function f (a sigmoid), and I would like to have the equation of the blue function that is the ""mirror"" of f by the red function. I have no idea how to achieve that. Can someone help me please ?",['functions']
3447922,Show $\alpha(G) \leq 2$ implies G contains $K_{\left \lceil \frac{n}{3} \right \rceil}$ as a minor,"G is simple. I want to show $\alpha(G) \leq 2$ implies G contains $K_{\left \lceil \frac{n}{3} \right \rceil}$ as a minor. This is what I have so far: If $\alpha(G) = 1$ , the graph $G$ is complete and it must contain the desired minor. So we assume $\alpha(G) = 2$ , thus for any choice of three vertices in $G$ , there must be at least one edge between the three. Thus, the compliment of $G$ , denoted $G^c$ , cannot contain $K_3$ as a subgraph. Notice that if there exists some vertex $v$ in $G^c$ with $\deg(v) = t$ , then if we look at the neighbours of $v$ , there cannot be any edges between the neighbours, (by the fact that no $K_3$ subgraph in $G^c$ ), so we have an independent set of size $t$ in $G^c$ . Equivalently, we have a $K_t$ subgraph in $G$ . So it suffices to show that $G^c$ contains some vertex $v$ with $\deg(v) \geq {\left\lceil \frac{n}{3} \right \rceil}$ or equivalently, that $G$ contains some vertex $v$ with $\deg(v) \leq n - {\left\lceil \frac{n}{3} \right \rceil} = {\left \lfloor \frac{2n}{3} \right \rfloor}$ Suppose it did not contain such a vertex, then for all vertices $v$ in $G$ , we have that: $\deg{v} \geq {\left\lceil \frac{2n}{3} \right \rceil}$ I know that, from here, I can look at a vertex in G and it's $ {\left\lceil \frac{2n}{3} \right \rceil}$ neighbours, and use the fact that the largest independent set is 2 to show that there are many edges between the neighbours of v. It seems like this ""connectedness"" should either give a subgraph of size at least $ {\left\lceil \frac{n}{3} \right \rceil}$ , or contradict the independent set condition. However, I am unsure how to proceed. Any help/advice would be appreciated. Thanks","['graph-theory', 'combinatorics', 'graph-minors']"
3447940,What is the actual meaning of $\frac{\partial}{\partial{x}}$,"When I searched total derivative on wikipedia it says: For $L=L(t,x_1(t),x_2(t),x_3(t)...x_n(t))$ the total derivative is given by: $$\dfrac{\rm{d}L}{\rm{d}t}=\dfrac{\partial L}{\partial t}+\sum_i^n\dfrac{\partial L}{\partial x_i}\dfrac{\partial x_i}{\partial t}~.$$ So here $\dfrac{\partial L}{\partial t}$ is derivative of $L$ w.r.t an explicit independent variable $t$ . However, when I look for generalized chain rule , wikipedia says: For $$y=y\left(u_1(x_1,x_2,...,x_i),u_2(x_1,x_2,...,x_i),...,u_m(x_1,x_2,...,x_i)\right)$$ the chain rule is given by: $$\frac{\partial y}{\partial x_i}=\sum_{l=1}^m\dfrac{\partial y}{\partial u_l}\dfrac{\partial u_l}{\partial 
x_i}$$ Now the partial derivative operator $\dfrac{\partial}{\partial{x_i}}$ looks more like a total derivative, rather than a derivative w.r.t an explicit independent variable. Could anybody tell me what on earth is $\dfrac{\partial}{\partial{x_i}}$ ? Thanks!","['multivariable-calculus', 'calculus']"
3448019,Prove that if $AA^T=A^TA$ and $AB=BA$ then $AB^T=B^TA$,Prove that if $AA^T=A^TA$ and $AB=BA$ then $AB^T=B^TA$ where $A$ and $B$ are matrices. It doesn't say of what order they are. Can somebody help me with this exam problem? I know I should put my work here but I really don't know what to do. I would really like some help.,"['matrices', 'matrix-equations', 'linear-algebra']"
3448047,System of three non-linear differential equations,"I've been interested in a problem that involves a fairly simple looking set of differential equations: $$
\frac{dx}{dt}=yz\;\;\;\frac{dy}{dt}=xz\;\;\;\frac{dz}{dt}=xy
$$ From this it follows that $\frac{dx}{dy}=\frac{y}{x}$ , $\frac{dy}{dz}=\frac{z}{y}$ , etc. Besides the trivial solutions of $x=0, y=0, z=0$ , and where one of $x, y$ or $z$ equals some constant $k$ and the rest equal 0, I'm not sure how to continue with this.",['ordinary-differential-equations']
3448064,Sum of the series $\sum_{n=0}^{\infty} \lfloor nr \rfloor x^n$ where $r$ is rational?,Can we find the exact sum of the series $\sum_{n=0}^{\infty} \lfloor nr \rfloor x^n$ where $r$ is rational? There is a special case given here but I don't know how to prove it and can we get the sum for the general case?,"['functions', 'rational-numbers', 'analysis', 'real-analysis']"
