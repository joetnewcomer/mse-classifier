question_id,title,body,tags
4512365,Integrating $\int^{\infty}_0\frac{\ln^2(x)}{\left(x^2+1\right)^2}\text{ d}x$,"I want to integrate $$I=\int^{\infty}_0\frac{\ln^2(x)}{\left(x^2+1\right)^2}\text{ d}x=\frac{\pi^3}{16}$$ using contour integration. I set up the function $$f(z)=\frac{\ln^3(z)}{\left(z^2+1\right)^2}$$ and used a keyhole contour with a branch cut about the positive real axis The integrals about $\Gamma$ and $\gamma$ I believe both go to $0$ if I did not make any mistakes in my calculations but afaik I should be right. The integrals about $T$ and $B$ is $$\int^{\infty}_{0}\frac{\ln^3(x)\text{ d}x}{\left(x^2+1\right)^2}+\int^{0}_{\infty}\frac{\left(\ln(x)+2\pi i\right)^3\text{ d}x}{\left(x^2+1\right)^2}$$ which expands and simplifies into $$\int^{\infty}_{0}\frac{\ln^3(x)\text{ d}x}{\left(x^2+1\right)^2}+\int^{0}_{\infty}\frac{\ln^3(x)\text{ d}x}{\left(x^2+1\right)^2}+\int^{0}_{\infty}\frac{6i\pi\ln^2(x)\text{ d}x}{\left(x^2+1\right)^2}-\int^{0}_{\infty}\frac{12\pi^2\ln(x)\text{ d}x}{\left(x^2+1\right)^2}-\int^{0}_{\infty}\frac{8i\pi^3\text{ d}x}{\left(x^2+1\right)^2}$$ $$\implies -6i\pi I +\int_{0}^{\infty}\frac{12\pi^2\ln(x)\text{ d}x}{\left(x^2+1\right)^2}+\int_{0}^{\infty}\frac{8i\pi^3\text{ d}x}{\left(x^2+1\right)^2}$$ Since $f(z)$ has poles of order $2$ at $z=\pm i$ I use the higher order residue formula and get $$\implies\mathop{\mathrm{Res}}_{z = i}\frac{\ln^3(z)}{\left(z^2+1\right)^2}=\lim_{z\to i}\frac{\text{d}}{\text{d}z}\left[\frac{\ln^3(z)}{\left(z+i\right)^2}\right]=\lim_{z\to i}\frac{\ln^{2}(z)\left(3i+3z-2z\ln(z)\right)}{z\left(z+i\right)^{3}}=-\frac{\pi^{3}}{32}-\frac{3i\pi^{2}}{16}$$ $$\implies\mathop{\mathrm{Res}}_{z = -i}\frac{\ln^3(z)}{\left(z^2+1\right)^2}=\lim_{z\to -i}\frac{\text{d}}{\text{d}z}\left[\frac{\ln^3(z)}{\left(z-i\right)^2}\right]=\lim_{z\to i}\frac{\ln^{2}(z)\left(3z-3i-2z\ln(z)\right)}{z\left(z-i\right)^{3}}=-\frac{\pi^{3}}{32}+\frac{3i\pi^{2}}{16}$$ which when added and multiplied with $2\pi i$ give $-\dfrac{i\pi^4}{8}$ . Equating only imaginary parts of both sides gives $$-6\pi I + 2\pi^4 = -\frac{\pi^4}{8}$$ $$\implies I=\frac{17\pi^3}{48}$$ This is obviously wrong. Plus, the real part is nonzero too so my final equation is nonsense. Where did I make a mistake? Thanks in advance!","['complex-analysis', 'improper-integrals', 'contour-integration', 'definite-integrals']"
4512394,Could we use DCT with pointwise convergence in probability?,"I'm interested in finding the limit in probability of an integral $$
\int_{0}^\infty f(X_n, t) \, \pi(\mathrm{d} t),
$$ where $X_n$ is a sequence of random variables and $\int_{0}^\infty \pi(\mathrm{d} t) = 1.$ I know the pointwise limit in probability of $f(X_n, t)$ and I can bound the function $|f(x, t)|$ by a function $h(t)$ that is integrable $\pi( \mathrm{d} t)$ . Is that enough to bring the probability limit inside the integral? More precisely, I have \begin{align*}
f(X_n, t) &\rightarrow_P f(X, t) \\
|f(x, t)| &\le h(t),\, \, \int_{0}^\infty h(t) \, \pi(\mathrm{d} t) < \infty
\end{align*} Is it true that $$
\int_{0}^\infty f(X_n, t) \, \pi(\mathrm{d} t) \rightarrow_P \int_{0}^\infty f(X, t) \pi(\mathrm{d} t) ?
$$ My thoughts: It looks like I should be able to apply the dominated convergence theorem (DCT), somehow. I'm confused because it differs from the textbook examples I saw when I took a basic course in analysis. For example, I'm taking a limit in probability outside the integral. I haven't applied DCT like that before: the limit outside the integral was a ""regular"" limit. References or answers are both appreciated. Thanks!","['probability-theory', 'real-analysis']"
4512437,Prove that $\frac {x \csc x + y \csc y}{2} < \sec \frac {x + y}{2}$,"If $0 < x,y < \frac {\pi}{2}$ , prove that: $$
\frac {x \csc x + y \csc y}{2} < \sec \frac {x + y}{2}
$$ My attempt . First, I tried to change this inequality: $$
\frac {\frac{x}{\sin x} + \frac{y}{\sin y}}{2} < \frac{1}{\cos \frac{x+y}{2}}
$$ Then,it's easy to know: $$
LHS = \frac{1}{2} \left ( \frac {x}{2\sin \frac {x}{2} \cos \frac{x}{2}} + \frac{y}{2 \sin \frac{y}{2} \cos \frac{y}{2}} \right ) < \frac{1}{2} \left ( \frac{1}{\cos^{2} \frac{x}{2}} +\frac{1}{\cos^{2} \frac{y}{2}} \right )
$$ $$
RHS > \frac{1}{\cos \frac{x}{2} \cos \frac{y}{2}}
$$ How to solve it next? It seems this way is wrong.","['trigonometry', 'inequality']"
4512446,Geometric genus of projective bundle over a scheme?,"This is Hartshorne's AG, exercise III.8.4(d): Let $Y$ be a noetherian scheme and $\mathscr{E}$ be a locally free $\mathscr{O}_Y$ -module of rank $n+1$ . Let $X=\mathbb{P}(\mathscr{E})=\underline{\mathrm{Proj}}_Y(\mathrm{Sym}(\mathscr{E}))$ . Let $\pi:X\to Y$ . (d) Show that the geometric genus $p_g(X)=0$ (Hint: Use II.8.1, the first exact sequence of kahler differentials). (I) We defined the geometric genus only over the $k$ -schemes where $k$ be a field, so here we may assume that $X,Y$ are all $k$ -schemes. (II) The hint is effective now only when $Y$ is smooth. Indeed for now we get $$0\to\pi^*\Omega_{Y/k}\to \Omega_{X/k}\to\Omega_{X/Y}\to0.$$ Hence we have $$\omega_{X/k}=\pi^*\omega_{Y/k}\otimes\omega_{X/Y}.$$ The (b) of this exercise told us that $\Omega_{X/Y}=(\pi^*\bigwedge^{n+1}\mathscr{E})(-n-1)$ , hence $$\omega_{X/k}=\pi^*(\omega_{Y/k}\otimes\bigwedge^{n+1}\mathscr{E})(-n-1).$$ But why $p_g(X)=\dim_k\Gamma(X,\omega_{X/k})=0$ ? (III) Actually consider an affine cover $U_i\subset Y$ such that $\mathscr{E}|_{U_i}$ are all trivialized, then we have $V_i:=\pi^{-1}(U_i)\cong\mathbb{P}_{U_i}^n$ . So we take any $s\in\Gamma(X,\omega_{X/k})$ , we have $s|_{V_i}=0$ as over $U_i$ , $\omega_{X/k}|_{V_i}=O_{V_i}(-n-1)$ . Hence by the axiom of sheaves, we must have $s=0$ ! Hence $p_g(X)=0$ .
Is this right? If this is right, the hint in Hartshorne's book of this (d) is meaningless... Thank you for your help!",['algebraic-geometry']
4512447,Proving or disproving the only co-prime solutions,"Prove or disprove that the only solutions of $$\begin{align}
xy = z\pmod {x+y}   \\
xz = y \pmod {x+z}\\
yz = x \pmod {y+z}\\
\end{align}
$$ in positive co-prime integers $x,y,z$ are $(1,1,1)$ and $(5,7,11)$ . My work: The system of congruence is clearly symmetrical so we can assign the parameter labels so that $a<b<c$ . In explicit form, the system of congruences can be written as $$\begin{align}
xy = z+c(x+y)   \\
xz = y+b(x+z)\\
yz = x+a(y+z)\\
\end{align}
$$ where $a,b,c$ are integers. I tried adding and factoring these equations but nothing useful came out. I am stuck here. Any help is greatly appreciated.","['number-theory', 'modular-arithmetic', 'elementary-number-theory']"
4512448,Prove or disprove that $u=0$ a.e. on $\Bbb R^d$,"Let $\Omega\subset\Bbb R^d$ be an open set. Let $k:\Bbb R^d\to [0,\infty)$ be measurable such that $0\in \operatorname{supp}k$ . This implies that $\Omega\subset \Omega_k=\Omega+\operatorname{supp}k$ . Recall that ( see here ) \begin{split}
\operatorname{supp} k
&= \Bbb R^d\setminus\bigcup\big\{O\,:\, \text{$O$ is open and $k=0$ a.e. on $O$}\big\}\\
&= \bigcap\big\{ \Bbb R^d\setminus O\,:\, \text{$O$ is open and $k=0$ a.e. on $O$}\big\}. 
\end{split} Let $u:\Bbb R^d\to \Bbb R$ be measurable. Assume that $k$ is symmetric, i.e. $k(x)=k(-x)$ . $k(x)>0$ for all $x\in \operatorname{supp}k$ . $u=0$ a.e. $\Bbb R^d\setminus\Omega_k$ and $u=0$ a.e. $\Bbb \Omega$ . $$I=\iint\limits_{\Omega \Omega_k}|u(y)|^2k(x-y)d y dx=0.$$ Question: Prove or disprove that $u=0$ a.e. on $\Bbb R^d$ that is, we do have $u=0$ a.e. on $\Omega_k\setminus \Omega$ ? Note that when $\mathrm{supp}\ k=\Bbb R^d$ we easily have $u=0$ a.e. on $\Bbb R^d$ .  Indeed, in this case we get $\Omega_k= \Omega+\Bbb R^d= \Bbb R^d$ and hence $$I=\iint\limits_{\Omega \Bbb R^d}|u(y)|^2k(x-y)d y dx= \iint\limits_{\Omega \Bbb R^d}|u(x+h)|^2k(h)d h dx=0.$$ Then we pick $a\in \Omega$ such that $$\int_{\Bbb R^d}|u(a+h)|^2k(h)d h=0.$$ Since $k>0$ on $\Bbb R^d$ we $u(a+h)=0$ for almost every $h\in \Bbb R^d$ and thus $u=0$ a.e. on $\Bbb R^d$ . General observations Note that $x-y\not\in \mathrm{supp}\ k$ for $x\in \Omega$ and $y\not\in \Omega_k$ . Thus $k(x-y)=0$ for $x\in \Omega$ and $y\not\in \Omega_k$ . This implies $$I=\iint\limits_{\Omega \Omega_k}|u(y)|^2k(x-y)d y dx=\iint\limits_{\Omega \Bbb R^d}|u(y)|^2k(x-y)d y dx\\= \iint\limits_{\Omega \Bbb R^d}|u(x+h)|^2k(h)d h dx= \iint\limits_{\Omega \operatorname{supp} k}|u(x+h)|^2k(h)d h dx$$ PS: A good answer to this question may be awarded a bounty","['measure-theory', 'geometric-measure-theory', 'measurable-functions', 'analysis']"
4512468,Trouble visualising geometrical proof of $\sin(\alpha+\beta)=\sin\alpha\cos\beta+\cos\alpha\sin\beta$,"We were taught a geometrical proof of the identity $\sin(\alpha+\beta)=\sin\alpha\cos\beta+\cos\alpha\sin\beta$ which consisted of drawing a triangle in a rectangle and looked like this:- While I have no problem understanding this proof as long as the sum of the angles is an acute angle, I am not able to visualize how this proof works if the sum of the angles is not an acute angle.
How would this proof work if one angle is $120^\circ$ while the other is $285^\circ$ or $195^\circ$ ?","['trigonometry', 'triangles']"
4512478,Prove that $\max _{M} f-\min _{M} f$ is invariant with the choice of a $K$-invariant real symplectic form $\omega$ on manifold $M$,"Let $X \in \mathfrak{k}:=\operatorname{Lie}(K)$ be a real vector field on a compact connected smooth manifold $M$ with an effective action of a compact real Lie group $K$ . By choosing a $K$ -invariant real symplectic form $\omega$ on $M$ , assume $f \in C^{\infty}(M)_{\mathbb{R}}$ is such that $$
d f=i_{X} \omega
$$ Show that the value $\max _{M} f-\min _{M} f$ is independent of the choice of $\omega$ as far as $\omega$ defines the same de Rham cohomology class $[\omega]$ . This is a question from Yau's contest in 2019, I was confused by how relate the conditions with the conclusion. From the conditions, we know that $M$ is $2n$ dimensional for some $n $ and $M $ is a symplectic manifold, so it is orientable, and the n fold product of the form $\omega$ must be a multiple of its volume form. I have considered the case $n=1$ and assume $\omega$ just be its volume   form, I conjectured that the conclusion may be deduced from integration along some specific curve, but I am really not sure and do not know how to deal with higher dimension problem? For reference, I found some theorem about symplectic manifold in GTM218, they may be helpful for solving the problem.","['symplectic-geometry', 'geometry', 'lie-groups']"
4512538,Differential Geometry Textbook for PDE students,"I am a graduate student in partial differential equations. When I was reading the literature sometimes I came across concepts from geometry, such as curvature, Riemannian geometry, and so on. I took differential geometry as an undergraduate but forgot a lot of it, and the textbooks at that time were biased towards differential geometry students, in other words I thought it was too specialized. So I want to review with another book. So I would like to ask what is a relatively simple differential geometry textbook. My personal guess; It would suit me better if it was written by an expert in partial differential equations. Note: My major is toward free boundary problems and fluid mechanics, not geometric analysis.","['partial-differential-equations', 'book-recommendation', 'differential-geometry']"
4512548,Graphically showing $\operatorname{Re}\left(\frac{1+z}{1-z}\right) = 0$ for $z=\cos\theta+i\sin\theta\neq 1$,"The given complex number is $z = \cos\theta + i\sin\theta$ , where $z$ is not $1$ I have to show that $$\operatorname{Re}\left(\frac{1+z}{1-z}\right) = 0$$ Algebraically, I have managed to do that using trigonometry and Euler's equation. But I can't succeed to imagine it on an Argand diagram using basic angle rules, without solving anything. In this case division, so angle subtraction. The angle should either be $90^\circ$ or $270^\circ$ since it should end up only on the complex part, I tried drawing it on the whiteboard but perhaps I am doing something wrong since I cannot get it to add up. Conceptually this is right.","['complex-analysis', 'polar-coordinates', 'complex-numbers']"
4512585,How to show that linear map is surjective?,"I have the following linear map: $$T:\mathcal{P}^2 \to \Bbb{R}^2 $$ where $\mathcal{P}^2$ denotes the vector space of polynomials  with real coefficients having degree at most $2$ $ T $ is defined  by $$T(ax^2+bx+c)=[a+b , b−c]^t$$ I do not know how to prove that $T$ is surjective. I know its not injective. Yet,i do not now how to formally show that is a surjection. I tried following the answer at How to show that a linear map is surjective? Using the formula described: $\dim(V) = 2$ , $\dim(\operatorname{range}T)=1$ According to the formula ""will be surjective if $\dim V= \dim \operatorname{range} T$ ""
this map would not be surjective. Yet, in the website i took this exercise from, it says this map is a surjective. So, i am wondering what i did wrong, and if there are better ways to show that this map is a surjective. I came across on this exercise on this website.","['systems-of-equations', 'linear-algebra', 'vector-spaces', 'linear-transformations']"
4512657,"if $f(x)f(y) = y^h f(x/2) + x^k f(y/2)$ for all positive reals $x,y$, then $f$ must be identically zero","Prove that if $h\neq k\in\mathbb{R}$ and $f$ is a real-valued function so that $f(x)f(y) = y^h f(x/2) + x^k f(y/2)$ for all positive reals $x,y$ , then $f$ must be identically zero. If one equates $f(y)f(x) = f(x)f(y)$ (using the original functional equation), one gets that for $x\neq 1, f(x/2) = A((2x)^h - (2x)^k)$ for some constant $A$ . If one assumes that the coefficients of terms with equal exponents must match, then we get $A=0$ (in more detail, we equate $A^2((2x)^h - (2x)^k)((2y)^h - (2x)^k) = (x^h - x^k) A y^h + x^k A (y^h - y^k).$ But since $h$ and $k$ are real, it's not clear that equating coefficients on both sides is even valid. For polynomials, of course it would be valid since nonconstant polynomials only have finitely many roots. I'm not sure if the set $\{x^i : i \in \mathbb{R}\}$ linearly independent over $\mathbb{R}$ .
If the given set is in fact linearly independent, then for any $k\ge 1$ and constants $c_1,\cdots, c_k\in \mathbb{R}$ , if $c_1 x^{i_1} + c_2 x^{i_2}+\cdots + c_k x^{i_k} = 0$ (i.e. it's the zero function) then $c_1=c_2=\cdots = c_k = 0.$","['contest-math', 'functional-equations', 'calculus', 'algebra-precalculus']"
4512663,When is $A+A^T$ positive definite?,"Suppose $A$ is a real valued $d\times d$ matrix. Is there a way to characterize set of $A$ such that that it's ""symmetrization"" $B$ is positive definite? $$B=\frac{1}{2}(A+A^T)$$ Largest eigenvalue of $B$ is known as the numerical abscissa of $A$ and determines the starting slope of $\text{exp}(t A)$ Is there meaning attached to the ""smallest eigenvalue"" of $B$ ? (prompted by related question where this condition on $A=CD$ with p.s.d $C,D$ turns out necessary for random update $x=x-\alpha C x$ on $x$ with covariance matrix $D$ to reduce variance of $x$ in all directions)","['linear-algebra', 'ordinary-differential-equations']"
4512692,Are sheaves a sheaf?,"Let $X$ be a topological space. Can the assignment $U\in\mathrm{Open}(X)\mapsto \{\text{sheaves on $U$}\}$ be made into a sheaf, rigorously? There is some hint in Kedlaya's course notes in which it says ""a sheaf of sheaves is a sheaf"" (see the corollary there). Also, in MacLane and Moerdijk's Sheaves in Geometry and Logic it says in chapter II, section 1: In fact, since the notion of a sheaf is ""local"", this functor is itself almost a sheaf. I wonder: why ""almost""?","['topos-theory', 'algebraic-geometry', 'sheaf-theory']"
4512697,Sheaf inclusion for globally generated coherent sheaf,"This is an Exercise in Popa's note: Kodaira dimension of algebraic varieties, page 32. Let $\mathcal F$ be a globally generated coherent sheaf of generic rank $r$ on a variety $X$ . Then there exists a sheaf inclusion: $$\mathcal O_X^{\oplus r}\hookrightarrow \mathcal F.$$ I know a baby version stating that if a line bundle $L$ admits a nonzero global section, then we have a sheaf inclusion $\mathcal O_X\hookrightarrow L$ defined by $1_x\mapsto s_x.$ But for our case, I cannot see how to find the suitable map to verify the injectivity. Since the generic rank is $r$ , we could at least find $r$ global nonzero sections, then how to explain the injectivity?","['complex-geometry', 'algebraic-geometry', 'several-complex-variables', 'birational-geometry']"
4512700,Does the shortest distance between two curves exist along common normal?,"I read here that the shortest distance between two differentiable non-intersecting curves is along their common normal. But if we consider $y^2=x+1$ , $y^2 = x$ , their common normal is actually the largest possible distance: Maybe this is because the minimum distance between these two curves never exists in the first place... Is this a condition we should add in the statement? Thanks","['analytic-geometry', 'optimization', 'calculus', 'derivatives']"
4512704,Open set around intersection of two convex sets,"Consider two closed (compact if needed) convex sets $E$ and $F$ . Define the $\epsilon$ neighborhood of set $E$ as $$E_\epsilon = \cup_{x \in E} B_\epsilon (x),$$ where $B_\epsilon(x)$ is the open $\epsilon$ -ball around $x$ . Now, given a $\delta>0$ , is there always an $\epsilon>0$ , such that $$E_\epsilon \cap F \subseteq (E\cap F)_\delta.$$ One counter example for $E$ not closed: The intersection $E\cap F$ is the top right corner, and its $\delta$ -neighborhood is just the $\delta$ -ball.
However, for any $\epsilon>0$ , $E_\epsilon \cap F = F$ , which would easily be out of $(E\cap F)_\delta$ . Some backgrounds:
I started with the problem without the closedness and convexity assumption, and intuitively it makes sense. Then I started to see counter examples. With (almost) the strongest assumption, is the above statement correct? One more assumption I can think of is to assume that $\mathrm{int}(E) \cap \mathrm{int}(F) \ne \emptyset$ . Thanks for any help / suggestions / hints!","['elementary-set-theory', 'general-topology', 'real-analysis']"
4512760,How does an inner product induce a definition for angles?,"A metric gives a notion of distance, a norm gives a notion of lengths and distance, and an inner product give a notion of angles, lengths, and distances. But how does it do so? In $\Bbb R^n$ , you can use $cos ^{-1}(\sqrt {\langle u,v \rangle}/ \|u\| \|v\|)$ and in hyperbolic space, you can use $cosh ^{-1}(\sqrt {\langle u,v \rangle}/ \|u\| \|v\|)$ , but that requires an a priori knowledge of the cosine function that corresponds to a given space. How can you derive this cosine variant solely through the inner product?","['inner-products', 'definition', 'angle', 'functional-analysis']"
4512779,Compute the inner product,"For $n\in \mathbb{Z}$ , let $e_n: [-\pi,\pi] \rightarrow \mathbb{C}$ be defeind by $e_n(t)=e^{int}$ . We let $C[-\pi, \pi]$ denote the space of continuous $\mathbb{C}$ -valued functions on the interval $[-\pi,\pi]$ . Compute the inner product $\langle f, \frac{1}{\sqrt{2\pi}}e_n \rangle$ , for $f$ given by $f(t)= \cosh(t) = \frac{1}{2}(e^t+e^{-t})$ , $-\pi \leq t\leq \pi$ . We have $\langle f, \frac{1}{\sqrt{2\pi}}e_n \rangle = \frac{1}{2\sqrt{2\pi}}\int_{-\pi}^{\pi} (e^{t}+e^{-t})e^{int}dt = \frac{1}{2\sqrt{2\pi}}\int_{-\pi}^{\pi} e^{t}e^{int}+e^{-t}e^{int}dt  = \frac{1}{2\sqrt{2\pi}}[(\frac{e^{\pi(in-1)}}{in-1}+\frac{e^{\pi(in+1)}}{in+1})- (\frac{e^{-\pi(in-1)}}{in-1}+\frac{e^{-\pi(in+1)}}{in+1})]$ . From here, is there any way to simplify? I think I need some sort of identities here Thanks in advance!","['complex-analysis', 'functional-analysis']"
4512800,Joint entropy of a random variable with itself,"If A and B are two independent random variables with $n_A$ and $n_B$ number of possible values, then the joint random variable AB would have $n_A \times n_B$ number of possible values and the joint entropy is simply additive $$H(AB)  = H(A) + H(B)$$ But what is $H(AB)$ if A and B are not independent, for example, if they are identical?","['information-theory', 'statistics', 'random-variables']"
4512804,number of solutions to $x^2 + xy + y^2 = 0\mod p$,"Let $p$ be an odd prime congruent to $2$ modulo $3$ and $c$ an integer between $1$ and $p-1$ . Let $\chi(x)$ denote $\left(\dfrac{x}{p}\right)$ (the Legendre symbol modulo $p$ for $x$ ). How many solutions $(x,y)$ are there to $x^2 + xy+y^2 = c$ where $0\leq x,y < p$ ? How many are there when $0\leq x < y < p$ ? Evaluate $\prod_{0\leq x <y < p}(x^2 + xy + y^2)\mod p$ (both when $p\equiv 3\bmod 4$ and $p\equiv 1\bmod 4$ ). I know that modulo p, the solutions to a quadratic congruence $ax^2 + bx + c\equiv 0, a\neq 0,$ exist if and only if $b^2 - 4ac \equiv y^2$ has a solution (for y). Also, if $x=0$ there are only $\chi(c)$ solutions. Similarly there are $\chi(c)$ solutions if $y=0$ . But I'm not sure how to find the number of solutions for arbitrary nonzero $x,y$ . Also, by the first two questions, one can simplify the product to $\prod_{c=1}^{p-1} c^{f(c)},$ where $f(c)$ is the number of times $c$ appears as $x^2 + xy + y^2$ modulo $p$ . $x^2 + xy+y^2 \equiv 0\mod p\Rightarrow (x+y)^2 \equiv xy \mod p,$ so it's not clear that the product in question is even nonzero.","['number-theory', 'quadratic-residues', 'modular-arithmetic', 'elementary-number-theory']"
4512831,Finding the range of $\frac{5 \cos x-2 \sin ^{2} x+4 \sin x-3}{6|\cos x|+1}$,"Here is the expression: \begin{equation}
\frac{5 \cos x-2 \sin ^{2} x+4 \sin x-3}{6|\cos x|+1}
\end{equation} I have tried to maximize the denominator, so I got the maximum value of denominator equal to 7, but I am lost what to do next I did the graph of this expression, so the minimum value is equal to -9, but how to get the answer without a graph, which is difficult to do by hand?","['algebra-precalculus', 'functions', 'trigonometry']"
4512847,Entire function $f: \mathbb{C} \to \mathbb{C}$ maps every unbounded sequence to an unbounded sequence then $f$ is a polynomial?,"I came across this problem. Hopefully my last for the exam tomorrow. (i) Entire function $f: \mathbb{C} \to \mathbb{C}$ maps every unbounded sequence to an unbounded sequence then $f$ is a polynomial? The other question is (ii) Is the same conclusion true if $f$ is holomorphic in $ \mathbb{C} \setminus\{ 0\}?  $ My thoughts go to the Liouville's Theorem first, since $f$ is unbounded it can't be constant. From there I would say that the function $f$ is analytic (bc holomorphic) with $\sum\nolimits_{n=0}^ \infty a_n(z - z_0)^n$ and some $a_n \neq 0$ for an $n > 0$ since $f$ is not constant. And therefore it must be a polynomial? But I dont have the gut feeling that I am here right or am I? :D On the other question (ii) I can't find a thing why it not should work like my thought befor actually. Thank you guys",['complex-analysis']
4512871,Formula for the $n^{\text{th}}$ Derivative of the Reciprocal of a Polynomial,"Is there a formula for the $n^{\text{th}}$ derivative of the reciprocal of a polynomial $f(x) = \sum_{k=0}^{K} a_k x^k$ ? Question 1: In particular, does there exist a formula for $$g_n(x) = \frac{d^n}{dx^n} \left(\frac{1}{\sum_{k=0}^{K} a_k x^k}\right)$$ In my particular case a partial fraction decomposition cannot be found for the denominator. If anyone knows of the answer to the first question, I am also curious about an equation of the following form. Question 2: Does there also exist a formula (in terms of $n$ ) for the following derivative? $$\hat{g}_n(x) = \frac{d^n}{dx^n} \left(\frac{x}{\sum_{k=0}^{K} a_k x^k}\right)$$","['induction', 'derivatives']"
4512896,"Showing the existence of $\int_0^1\sqrt{x}\ln (x)\, dx$ and calculating it","Show that the integral $$\int_0^1\sqrt{x}\log (x)\, dx$$ exists and calculate that. To calculate that integral I have done the following : We substitute $z=\ln (x) \Rightarrow x=e^z$ . When $x=0$ then $z=-\infty$ When $x=1$ then $z=0$ We have that $\frac{dz}{dx}=\frac{1}{x} \Rightarrow dx=xdz=e^zdz$ Then we get $$\int_0^1\sqrt{x}\ln (x)\, dx=\int_{-\infty}^0\sqrt{e^z}z\, e^zdz=\int_{-\infty}^0e^{z/3}z\, dz$$ Is that the correct way to calculate the integral? At the beginning where we have to show the existence, do we show that just by calculating the integral or is there also an other way to show the existence?","['integration', 'calculus', 'improper-integrals', 'analysis']"
4512913,"Spivak, Ch. 20, Problem *4(i): Write down a sum which equals $\sin{1}$ with an error of less than $10^{-10^{10}}$.","In Chapter 20 of Spivak's Calculus , entitled ""Approximation by Polynomial Functions"", in problem 4(i) we are asked to find a polynomial that approximates $\sin{1}$ with an error of less than $10^{-10^{10}}$ . Let me cut to the chase and tell you what my question is. As I show below, there are two very simple answers to this problem (and they are in the solution manual): a Taylor polynomial of order $10^{10^{10}}-1$ , or even better, a Taylor Polynomial of order $10^{10}-1$ . I came up with a different solution that finds a minimum bound for the order of the polynomial as $$10^{10}\frac{\log{10}}{\log{2}}-1\tag{3}$$ My question is: is this answer useful? Since this number is not a natural number, it seems that we'd need to figure out a natural number larger than this to find the Taylor Polynomial required to approximate $\sin{1}$ . Is this easy to do? I will now go through all the steps involved in this problem, and show my attempted solution. If we write $$f(x)=\sin{x}=P_{n,a}(x)+R_{n,a}(x)$$ where $P_{n,a}(x)$ is the Taylor polynomial of degree $n$ for $f$ at $a$ , and $R_{n,a}(x)$ is the remainder term, then by Taylor's Theorem we have $$\sin{x}=\sum\limits_{i=0}^{2n+1} \left [\sin^{(i)}{(a)}\frac{(x-a)^{2n+1}}{(2n+1)!}\right ]+\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}(x-a)^{2n+2}, t\in (a,x)$$ $$R_{2n+1,a}(x)=\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}(x-a)^{2n+2}, t\in (a,x)$$ $$R_{2n+1,0}(x)=\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}x^{2n+2}, t\in (0,x)$$ $$R_{2n+1,0}(1)=\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}, t\in (0,x)$$ $$\leq \frac{1}{(2n+2)!}$$ Therefore, we want $$\frac{1}{(2n+2)!}<10^{-10^{10}}$$ $$\implies (2n+2)!>10^{10^{10}}$$ The solution manual has the following two solutions as possibilities One immediate option is to have $2n+2=10^{10^{10}}$ . This works and we have that $$\sum\limits_{i=0}^{\frac{10^{10^{10}}-2}{2}}(-1)^{i}\frac{1}{i!}$$ is within $10^{-10^{10}}$ of $\sin{1}$ . Another option is to choose $2n+2=10^{10}$ ""since $(10^{10})!$ is clearly larger than $10^{10^{10}}$ "". In this case, we have $$\sum\limits_{i=0}^{\frac{10^{10}-2}{2}}(-1)^{i}\frac{1}{i!}$$ (I missed this last solution though; I asked another question about how to prove that $(10^{10})!$ is larger than $10^{10^{10}}$ ) My solution is as follows For any $a>0$ and $\epsilon>0$ there is some $N$ such that $\frac{a^n}{n!}<\epsilon$ for $n>N$ . Proof Let $n\geq 2a$ . Then $\frac{a^{n+1}}{(n+1)!}=\frac{a}{n+1}\frac{a^n}{n!}<\frac{1}{2}\frac{a^n}{n!}$ . For any $n_0 \in \mathbb{N}$ and $n_0\geq 2a$ we have $$\frac{a^{n_0+1}}{(n_0+1)!}<\frac{1}{2}\frac{a^n_0}{n_0!}$$ $$\frac{a^{n_0+2}}{(n_0+2)!}=\frac{a}{n_0+2}\frac{a^{n_0+1}}{(n_0+1)!}<\frac{1}{2}\frac{1}{2}\frac{a^n_0}{n_0!}$$ Thus $$\frac{a^{n_0+k}}{(n_0+k)!}<\frac{1}{2^k}\frac{a^{n_0}}{n_0!}\tag{1}$$ Now, $\lim\limits_{x\to\infty} 2^{-k}=0$ . This means $$\forall \epsilon_1>0\ \exists N>0\ \forall k, k>N\implies
> 2^{-k}<\epsilon_1$$ Let $\epsilon_1=\frac{\epsilon n_0!}{a^{n_0}}$ . Then $\exists N\
> \forall k, k>N\implies 2^{-k}<\frac{\epsilon n_0!}{a^{n_0}}$ . But from $(1)$ this means that $$\frac{a^{n_0+k}}{(n_0+k)!}<\epsilon$$ Note that the $N$ in the proof above is $$2^{-k}<\epsilon \implies 2^k>\frac{1}{\epsilon}\implies k>\log_2(1/\epsilon)=N$$ Back to the main problem, let $2n=k$ , where for the time being these numbers are real. Then we want $$\frac{1}{(2+k)!}<10^{-10^{10}}=\epsilon$$ Let $\epsilon_1=\epsilon\cdot 2!$ . Then $\frac{1}{(2+k)!}<\epsilon$ if $k=2n>\log_2\left [ \frac{1}{\epsilon\cdot 2!} \right ]=\log_2 \left [ \frac{10^{10^{10}}}{2!} \right ]$ That is $$n>\frac{1}{2}\log_2 \left [ \frac{10^{10^{10}}}{2!} \right ] \implies \frac{1}{(2n+2)!}<10^{-10^{10}}\tag{2}$$ Now, this lower bound for $n$ is not a natural number. This chapter is about polynomial approximations, and as far as I have understood this is very practical chapter. With the initial two solutions, we had natural numbers and could compute the sum of polynomial terms with enough computing power. In my proof, I found a minimum value for $n$ that is not a natural number. In fact it is $$10^{10}\frac{\log{10}}{\log{2}}-1\tag{3}$$ which is not better than the solution in the solution manual of a Taylor polynomial of order $10^{10}-1$ . However, I am curious to know if this answer is even acceptable in practical terms. We'd have to compute the logarithmic values, I assume using polynomial approximation as well. Assuming my solution is correct, is it a good answer? By that I mean, can we easily find a natural number larger than the number in $(3)$ ?","['integration', 'calculus', 'polynomials', 'taylor-expansion', 'derivatives']"
4512956,Is it hard to evaluate $\int_{0}^{\infty} \frac{\ln \left(1-x+x^{2}\right)}{1+x^{2}} d x$?,"Latest Edit As suggested by @Quanto, $I(a)$ can be utilised to give more examples as below. $$
\boxed{\begin{aligned}
I(a)&= \int_{0}^{\infty} \frac{\ln \left(1+2 x \sin a+x^{2}\right)}{1+x^{2}} d x \\&= \pi\ln \left|2 \cos \frac{a}{2}\right|+a\ln \left|\tan \frac{a}{2}\right|-2 \operatorname{sgn} (a) \int_{0}^{\frac{|a|}{2}} \ln (\tan x) d x
\end{aligned}}
$$ For examples: Example 1 $$\begin{aligned}\quad \int_{0}^{\infty} \frac{\ln \left(1-x+x^{2}\right)}{1+x^{2}} d x&= 
I\left(-\frac{\pi}{6}\right)\\&= \frac{\pi}{2}[\ln (2+\sqrt{3})]+\frac{\pi}{6} \ln (2+\sqrt{3})+2\left(-\frac{2}{3} G\right)\\& =\frac{2 \pi}{3} \ln (2+\sqrt{3})-\frac{4}{3} G \end{aligned}$$ Example 2 $$
\begin{aligned}
&\int_{0}^{\infty} \frac{\ln \left(1+\sqrt{2} x+x^{2}\right)}{1+x^{2}}\\=&I\left(\frac{\pi}{4}\right) \\
=& \pi \ln \left(2 \cos \frac{\pi}{8}\right)-\frac{\pi}{4} \ln \left(\tan \frac{\pi}{8}\right)-2 \int_{0}^{\frac{\pi}{8}} \ln (\tan x) d x \\
=& \frac{\pi}{2} \ln (2+\sqrt{2})+\frac{\pi}{4} \ln (\sqrt{2}+1) -2\left[\frac{\pi}{8} \ln (\sqrt{2}-1)-\Im\left(\operatorname{Li}_{2}(i(\sqrt{2}-1))\right]\right.\\=& \pi \ln [\sqrt[4]{2}(\sqrt{2}+1)] +2 \Im\left(\operatorname{Li}_{2}(i(\sqrt{2}-1))\right.
\end{aligned}
$$ where the last integral see post We are going to prove that $$\boxed{J=\int_{0}^{\infty} \frac{\ln \left(1-x+x^{2}\right)}{1+x^{2}} d x =\frac{2 \pi}{3} \ln (2+\sqrt{3})-\frac{4}{3} G }\tag*{} $$ by Feynman’s Technique Integration. We first deal with its partner integral $$\displaystyle I=\int_{0}^{\infty} \frac{\ln \left(1+x+x^{2}\right)}{1+x^{2}} d x \tag*{} $$ which is parameterised by $\displaystyle I(a)=\int_{0}^{\infty} \frac{\ln \left(1+2 x \sin a+x^{2}\right)}{1+x^{2}} d x,\tag*{} $ where $ \displaystyle a\in [-\frac{\pi}{2}, \frac{\pi}{2}]. $ Differentiating $I(a)$ w.r.t. $a$ yields $\displaystyle \begin{aligned}I^{\prime}(a) &=\int_{0}^{\infty} \frac{2 x \cos a}{\left(1+x^{2}\right)\left(1+2 x \sin a+x^{2}\right)} d x \\&=\cot a\int_{0}^{\infty}\left(\frac{1}{1+x^{2}}-\frac{1}{1+2 x \sin a+x^{2}}\right) d x \\&=\cot a\left[\tan ^{-1} x-\frac{1}{\cos a} \tan ^{-1}\left(\frac{x+\sin a}{\cos a}\right)\right]_{0}^{\infty} \\&=\cot a\left[\frac{\pi}{2}-\frac{1}{\cos a}\left(\frac{\pi}{2}-a\right)\right]\end{aligned}\tag*{} $ Integrating $I’(a)$ back, we have $\displaystyle \begin{aligned}I\left(\frac{\pi}{6}\right)- \underbrace{I(0)}_{=\pi\ln 2} &=\int_{0}^{\frac{\pi}{6}} \cot a\left[\frac{\pi}{2}-\frac{1}{\cos a}\left(\frac{\pi}{2}-a\right)\right] d a \\&=\frac{\pi}{2} \underbrace{ \int_{0}^{\frac{\pi}{6}}\left(\cot a-\frac{1}{\sin a}\right) d a}_{=\ln \left(\frac{2+\sqrt{3}}{4}\right)} + \underbrace{\int_{0}^{\frac{\pi}{6}} \frac{a}{\sin a} d a}_{K}\end{aligned}\tag*{} $ $\displaystyle \begin{aligned}K &=\int_{0}^{\frac{\pi}{6}} \frac{a}{\sin a} d a=\int_{0}^{\frac{\pi}{6}} a\, d\left[\ln \left(\tan \frac{a}{2}\right)\right] \\&=\left[a \ln \left(\tan \frac{a}{2}\right)\right]_{0}^{\frac{\pi}{6}}-\int_{0}^{\frac{\pi}{6}} \ln \left(\tan \frac{a}{2}\right) d a \\&=\frac{\pi}{6} \ln \left(\tan \frac{\pi}{12}\right)-2 \int_{0}^{\frac{\pi}{12}} \ln (\tan a) d a \\&=-\frac{\pi}{6} \ln (2+\sqrt{3})+\frac{4}{3} G,\end{aligned}\tag*{} $ where $G$ is the Catalan’s constant and the last integral refer to the post . Now we can conclude that $\displaystyle \int_{0}^{\infty} \frac{\ln \left(1+x+x^{2}\right)}{1+x^{2}} d x =I\left(\frac{\pi}{6}\right)=\frac{\pi}{3} \ln (2+\sqrt{3})+\frac{4}{3} G\tag*{} $ Back to our integral $\displaystyle J=\int_{0}^{\infty} \frac{\ln \left(1-x+x^{2}\right)}{1+x^{2}} d x,  \tag*{} $ using the result from my post , $\displaystyle \int_{0}^{\infty} \frac{\ln \left(x^{4}+x^{2}+1\right)}{x^{2}+1} d x=\pi \ln (2+\sqrt 3) \tag*{} $ yields immediately: $\displaystyle \begin{aligned}\int_{0}^{\infty} \frac{\ln \left(1-x+x^{2}\right)}{1+x^{2}} d x&=\int_{0}^{\infty} \frac{\ln \left(x^{4}+x^{2}+1\right)}{x^{2}+1} d x-\int_{0}^{\infty} \frac{\ln \left(1+x+x^{2}\right)}{1+x^{2}} d x\\&=\pi \ln (2+\sqrt{3})-\frac{\pi}{3} \ln (2+\sqrt{3})-\frac{4}{3} G \\&=\frac{2 \pi}{3} \ln (2+\sqrt{3})-\frac{4}{3} G\end{aligned} \tag*{} $ Is there any method other than Feynman’s Technique? Your comments and alternative methods are highly appreciated.","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'catalans-constant']"
4512961,"Find a synthetic solution: $T(12^{\circ}, 24^{\circ}, 54^{\circ}, 18^{\circ}, 30^{\circ})$","Figure: (Show: $?= 30^{\circ}$ ) I was able to solve this using the trigonometric ceva. $$\frac{\sin{(72^{\circ}-\alpha)}}{\sin{(12^{\circ})}}\frac{\sin{(24^{\circ})}}{\sin{(54^{\circ})}}\frac{\sin{(18^{\circ})}}{\sin{(\alpha)}}=1$$ But I couldn't find a synthetic solution. For this, I tried to choose a point $E$ with $AB=AE$ on the $[BC$ line. $AC=CE$ and $\angle{AFC}=\angle{CFE}=30^{\circ}$ can be found if we construct an equilateral triangle $AEF$ below $[AE]$ . But after that I couldn't get anything. Any help will be appreciated.","['euclidean-geometry', 'geometry']"
4512982,How to prove $\lim_{x\to1} \log(x)=0$ in a fair way?,"I am trying to prove the following limit with the definition: $$\lim_{x\to1} \log(x)=0$$ That is - I must prove that for all $\epsilon>0$ , there exists $\delta >0$ such that: $$ 0<|x-1|< \delta \implies |\log x - 0|< \epsilon$$ So - my trouble is to understand what is ""fair"" to use here. I know the following: $x<e^x \implies \log(x)<x$ And it would be nice if we could find that $\log(x)\leq x-1$ From this I think we get: $$|\log(x)|\leq|x-1| \text{ for }x\in[1,\infty]\qquad |\log(x)|\geq|x-1| \text{ for }x\in[-\infty,1]\tag{$\star$}$$ The trouble for me is that I know $\log(x)\leq x-1$ is true because of two things: Because $\log(1)=1-1=0$ and hence as $\log(x)$ and $x-1$ start in the same point and $\log'(x)=\frac{1}{x}$ and $(x-1)'=1$ then $\log (x)$ increases slower than $x-1$ when $x>1$ and decreases faster than $x-1$ when $x<1$ . But is it fair to use derivatives? In the book I am reading, we didn't even reached derivatives yet. I looked a plot of both functions, which also doesn't seems fair. Also, is it fair to assume we know that $\log(1)=0$ ? For me, knowing this seems to defeat the purpose of the problem somehow and the inequalities  I obtained in $(\star)$ , the first would be helpful but I don't see how the second would help me. So how can we proceed to prove this in a ""fair"" way?",['limits']
4513030,Can we always find this kind of unitary matrix to diagonalize this special form hermitian matrix?,"Suppose we have a hermitian matrix $M$ of form $$\left( \begin{matrix}
	\alpha&		-\beta ^*\\
	\beta&		-\alpha ^*\\
\end{matrix} \right) $$ where $\alpha$ is a hermitian matrix and $\beta$ is an antisymmetric matrix with equal dimension, and I use the star $*$ for complex conjugate . Since $M$ is hermitian, then it can be diagonalized into $$TMT^{\dagger}=\left( \begin{matrix}
	d&		0\\
	0&		-d\\
\end{matrix} \right)$$ where $d$ is diagonal, and I used the special form of $M$ to deduce that the eigenvalues are real and appear in matched pairs $\pm \lambda $ . But I want to say more about the unitary matrix $T$ , i.e., can $T$ always be chosen to be of form $\left( \begin{matrix}
	\gamma&		\mu\\
	\mu ^*&		\gamma ^*\\
\end{matrix} \right) $ where $\gamma$ and $\mu$ are two matrices with equal dimensions? Edit The question is found in this notes(from the bottom of page 6 to 7) for introduction of Jordan-Wigner transformation. The author states the above rules while I really can't see how to show that. I quoted the closely related part for your convenience: One way of obtaining a rigorous proof is to find a $T$ satisfying $$
T M T^{\dagger}=\left[\begin{array}{cc}
d & 0 \\
0 & -d
\end{array}\right]
$$ and then to apply the cosine-sine (or CS) decomposition from linear algebra, which provides a beautiful way of representing block unitary matrices, and which, in this instance, allows us to obtain a $T$ of the desired form with just a little more work. Thanks in advance!","['unitary-matrices', 'matrices', 'hermitian-matrices', 'linear-algebra', 'matrix-decomposition']"
4513032,The evaluation map of Brauer-Manin obstruction is zero?,"Let $X$ be a proper $\mathbb{Z}_p$ scheme. The point $x: \mathbb{Z}_p \to X$ induces the map $(A \mapsto A(x))$ , $\operatorname{Br}(X) \to \operatorname{Br}(\mathbb{Z}_p)$ . Since $\operatorname{Br}(\mathbb{Z}_p) = 0$ we have $A(x)=0$ . Now the valuative criterion of properness implies that the natural map $X(\mathbb{Z}_p) \to X(\mathbb{Q}_p)$ is bijective. This would make the Brauer-Manin obstruction vacuous, which isn't true. What went wrong?","['algebraic-geometry', 'arithmetic-geometry']"
4513080,Computing $\int_{0}^{\pi} \ln (\sin x+2) d x$ and $\int_{0}^{\pi} \ln (2-\sin x) d x$,"I first encountered this integral $$
I=\int_{0}^{\pi} \ln (\sin x+2) d x
$$ several months ago without any idea and had tried many methods such as integration by parts, substitution and Fourier series etc. but all are in vain. Today, I tried the tangent substitution and succeeded.  Now I want to share with you and seek any other alternatives. For simplicity, I convert, by symmetry, the integral into $$
I=2 \int_{0}^{\frac{\pi}{2}} \ln (\sin 2 x+2) d x
$$ and substitute $t=\tan x$ , and get $$
\begin{aligned}
I &=2 \int_{0}^{\frac{\pi}{2}} \ln (\sin 2 x+2) d x \\
&=2 \int_{0}^{\infty} \ln \left(\frac{2 t}{1+t^{2}}+2\right) \frac{d t}{1+t^{2}} \\
&=2 \int_{0}^{\infty} \frac{\ln 2+\ln \left(t^{2}+t+1\right)-\ln \left(1+t^{2}\right)}{1+t^{2}} d t \\
&=\pi \ln 2+2 \int_{0}^{\infty} \frac{\ln \left(t^{2}+t+1\right)}{1+t^{2}}dt-2 \int_{0}^{\infty} \frac{\ln \left(1+t^{2}\right)}{1+t^{2}} d t \\
&=\pi \ln 2+2  \underbrace{  \left[\frac{\pi}{3} \ln (2+\sqrt{3})+\frac{4}{3} G\right]}_{(*)} -2  \underbrace{\pi \ln 2}_{(**)} \\
&=-\pi \ln 2+\frac{2 \pi}{3} \ln (2+\sqrt{3})+\frac{8}{3} G
\end{aligned}
$$ Note:
(*): post 1 , (**): post 2 For the second integral, we use the similar technique and arrive at \begin{aligned}
J&=2 \int_{0}^{\infty} \frac{\ln 2+\ln \left(1-t+t^{2}\right)-\ln \left(1+t^{2}\right)}{1+t^{2}} d t \\
&=2 \int_{0}^{\infty} \frac{\ln 2}{1+t^{2}} d t+2 \int_{0}^{\infty} \frac{\ln \left(1-t+t^{2}\right)}{1+t^{2}} d t-2 \int \frac{\ln \left(1+t^{2}\right)}{1+t^{2}} d t \\
&=\pi \ln 2+2  \underbrace{\left(\frac{2 \pi}{3} \ln (2+\sqrt{3})-\frac{4}{3} G\right)}_{(***)}-2 \pi \ln 2 \\
&=-\pi \ln 2+\frac{4 \pi}{3} \ln (2+\sqrt{3})-\frac{8}{3} G
\end{aligned} Note:(***) post 3 Eager to know whether there are any alternatives! Comments and alternative solutions are highly appreciated.","['integration', 'improper-integrals', 'calculus', 'trigonometric-integrals', 'catalans-constant']"
4513126,Evaluate $\int_{0}^{\pi}\sqrt\frac{1+\cos2x}{x^2+1}dx$.,"Evaluate $$\int_{0}^{\pi}\sqrt\frac{1+\cos2x}{x^2+1}\,\mathrm dx$$ We have $\sqrt{1+\cos2x}=\sqrt{2\cos^2x}=\sqrt{2} |\cos x|$ . Then we need to solve $\int_{0}^{\pi}\frac{\sqrt{2} |\cos x|}{\sqrt{x^2+1}}dx$ . Using symmetry of $\cos x$ from $0$ to $\pi$ the integral becomes $I=2\sqrt2\int_{0}^{\pi/2}\frac{ \cos x}{\sqrt{x^2+1}}dx$ . I tried using ""By Parts"", it didn't helped much. Take the substitution $x=\tan \theta$ then we have $I=\int_{0}^{\tan^{-1}\pi/2}\cos (\tan \theta)\sec \theta d\theta$ . I am having difficulty in solving with the existing methods that I know. Edit : After knowing that I can not assume the symmetry of the given integral the question will be to solve $$\sqrt2\left(\int_{0}^{\pi/2} \frac{\cos x}{\sqrt {x^2+1}}\,\mathrm dx-\int_{\pi/2}^{\pi} \frac{\cos x}{\sqrt {x^2+1}}\,\mathrm dx\right) $$ Again I will face same problem to proceed ahead.","['integration', 'calculus', 'definite-integrals']"
4513253,Limit of $n \tan(\frac{x}{n})$,"Let $f_n :[-1/2, 1/2] \to \mathbb{R}$ , be a seq. of functions, where $f_n(x) = n \tan(\frac{x}{n})$ . Im asked to show that this sequence converges pointwise and determine the limit function. I have no idea where to even start. It seems that $\forall x: f_n(x) \to x$ Any hints? I know that $\cos(x/n) \to 1$ and that $\sin(x/n)$ behaves like $x/n$ as $n$ increases. So it makes ""sense"" that the limit function is $x$ , but I'm failing to show it rigorously.","['pointwise-convergence', 'general-topology', 'trigonometry']"
4513260,The third covariant of a tensor in Riemannian manifolds [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I'm reading Petersen's book ""Riemannian Geometry 3rd"", No.171 in GTM. On page 82, there's a formula reads: $$\nabla^3_{X,Y,Z}W=\nabla^2_{X,Y}(\nabla_ZW)-\nabla_{\nabla^2_{XY}Z}W$$ I'm quite puzzled with this formula. On the right hand site, it doesn't seem to be tensorial in the ""Z"" position. Is this formula false?","['tensors', 'riemannian-geometry', 'differential-geometry']"
4513283,How can I argue that $f(x) = (2-x)^3-x+\frac{3}{2}$ is decreasing with a polynomial?,"$$f(x) = (2-x)^3-x+\frac{3}{2}$$ I have to give a mathematical argument for the said function in the title being decreasing. My first thought was finding $f'(x)$ but I got a polynomial and for that reason I don't know how to argue that it is decreasing since $f'(x)<0$ has to be true. What I mean is that I can't see it in the polynomial, which is: $f'(x)=-3x^2+12x-13$ .
However, the second derivative gives me something I can work with: $f''(x)=-6x+12$ . So my question is, how do I argue that $f(x) = (2-x)^3-x+3/2$ is decreasing with the first derivative being a polynomial, $f'(x)=-3x^2+12x-13$ ?","['calculus', 'derivatives']"
4513286,Does the sequence of iterated derivatives of $2^x$ converge uniformly to the zero function?,"Consider the function $2^x$ . As you keep taking derivatives of that function over and over again, it converges to the zero function, at least pointwise. My question is, does it converge uniformly to the zero function on the entire real line? I used the example of $2^x$ , but I could have used any exponential function $b^x$ for a number $b$ between $1$ and $e$ .","['derivatives', 'real-analysis']"
4513297,"Solve general quadratic systems, or find rational points on a Fano variety","Suppose we have $2n+1$ variables $\mathbf{x}=[x_0,x_1,x_2,\ldots,x_{2n}]^\mathsf{T}$ and $n$ symmetric matrices $(\mathbf{A}_1,\mathbf{A}_2,\ldots,\mathbf{A}_n)$ of dimension $(n+2)\times(n+2)$ and having coefficients in $\mathbb{Q}$ . Define $\mathbf{u}={[1,x_0,x_1,x_2,\ldots,x_n]}^\mathsf{T}$ , consider a system of $n$ quadratic equations $$\begin{cases}\mathbf{u}^\mathsf{T}\mathbf{A}_1\mathbf{u}=x_{n+1}^2\\\mathbf{u}^\mathsf{T}\mathbf{A}_2\mathbf{u}=x_{n+2}^2\\\vdots\\\mathbf{u}^\mathsf{T}\mathbf{A}_n\mathbf{u}=x_{2n}^2\end{cases}$$ It is known that the solutions $\mathbf{x}$ of the above system form a $n+1$ dimensional Fano variety, which is rationally connected and is thus supposed to have a lot of rational points in $\mathbb{Q}$ (or at least in a finite extension of $\mathbb{Q}$ ). However, instead of knowing how many rational solutions there are, I'd rather want to get some explicit rational solutions in $\mathbb{Q}$ , provided that there is any. This can be done by applying a generic rational point finding algorithm, which is somewhat useful when $n$ is small. However, the complexity grows exponentially with $n$ so it quickly becomes infeasible even for moderately large $n$ (e.g., $n=100$ ). I can also try to compute a Gröbner basis, but the complexity is even worse (double exponential) so it is not likely to give any help either. I would like to know that, apart from those completely generic methods (exhaustive search, or Gröbner basis), is there any more efficient algorithm in finding rational solutions , that can take advantage of the fact that the defining equations are limited to quadratic forms and the resulting variety is rationally connected/Fano? I am aware that the degree alone will not play an essential role as it is possible to convert any system of polynomials into system of quadratics, but I believe that being rationally connected really should mean something , or otherwise it will seem really strange to me, that despite of the variety being supposed to have a lot of rational points, and the rational points on it possess a particularly nice structure , I will have no way to do better in finding them than doing a generic search on a variety of generic type . The algorithm is not required to be deterministic, nor is it required to prove the existence or non-existence of a rational solution. It would be acceptable as long as when there does exist rational solutions, it has a non-trivial success rate to find one (by non-trivial success rate I mean a success rate guaranteed to be larger than a positive absolute constant independent of $n$ ). It is not required to have completely polynomial complexity either. It would be acceptable as long as it runs in sub-exponential time and is capable of handling reasonably large $n$ like $n=100$ . If such an algorithm (of finding rational points in sub-exponential time) does not exist, then at least I would like to know an algorithm that, given two rational solutions $\mathbf{x}_1,\mathbf{x}_2$ of the above system, it constructs an explicit rational curve on the variety passing through those given points. Such a rational curve must exist (due to the very definition of being ""rationally connected""), but instead of knowing ""it must exist"" I'd rather want to know an algorithm to explicitly compute it . Does anyone has ideas on this? Any materials and references on explicit computations in rationally connected/Fano varieties are also much appreciated.","['systems-of-equations', 'number-theory', 'computational-mathematics', 'algebraic-geometry', 'algorithms']"
4513303,Calculating a surface integral,"I'm working on a problem which involves calculating a surface integral. The problem is as follows: Consider S the part of the surface with equation $x^2+y^2=13-z$ which lies above the plane $z=4$ . Calculate the following integral $$ I =\iint_S \vec{r} \cdot\vec{n} \mathrm{d}O$$ where $\vec{r} = x\vec{u_1}+y\vec{u_2}+z\vec{u_3}$ and $\vec{n}$ is the normal to $S$ pointing upwards. In my textbook I found that I can use that $ I =\iint_S \vec{r} \cdot\vec{n} \mathrm{d}O = \iiint_G \mathrm{div} \vec{v}$ where $G$ is the area in $\mathbb{R}^3$ enclosed by $S$ . $\mathrm{div} \vec{v}$ was obvious and easy to calculate and equal to $3$ . I feel the problem will be easiest if I convert to cylindrical coordinates so I put $$ \begin{cases} x = r\cos\theta \\ y=r\sin\theta \\ z = 13-r^2  \end{cases} $$ Now I my problem lies with finding out my boundaries in cylindrical coordinates. I know that $0 \leq \theta \leq 2\pi$ and that $ 4\leq z$ but further I'm not sure what the boundaries would be. Is my working so far correct? If so what would the remaining boundaries be? If not, what would be the correct method to work this problem out? Is there any other method to working this problem out? Thanks in advance for any help.","['integration', 'multivariable-calculus', 'calculus']"
4513361,closed curve mapping problem,"I have this interesting, seemingly difficult problem I came across while think about rendering. It seems simple enough that I suspect it might have a name already (and hopefully be solved), but I can't find anything. So I'll state the problem here, maybe it sounds familiar. Consider some finite closed curve in 2D (for simplicity). Take some random point inside (strictly) the curve. Draw lines radially outward from the point onto the curve. This process creates some map $R(\theta)$ . It's obvious that this won't always be a function, sometimes a $\theta$ corresponds to multiple $R$ , i.e. the line intersects the curve twice. Is there a way to, given a curve in some form, figure out if $R(\theta)$ can be a function? So figure out if there exists a point somewhere such that no line intersects the curve twice? And better then proving existence, can such a point be found reliable through some algorithm? Better yet, for curves where multiple points are necessary, can that be solved too and can some points be found? This whole thing reminds me of the illumination problem. It's like a reflectionless illumination problem, except I need to know specifically where the point(s) need(s) to be for an arbitrary curve. The reason I'm interested in this btw is that it might be useful to encode surfaces by mapping distance functions like this, which can then be compressed with spherical harmonics. To be able to do an arbitrary surface I need to solve this problem pretty much. Update: Turns out this is called the 'art gallery problem' (if you replace the curves by a polygon). It's a very hard CS problem, nothing close to a solution exists, just an upper bound on the number of points necessary. Any ideas and resources on approximately solving these questions (so using more points than necessary) are appreciated!","['optimization', 'analytic-geometry', 'geometry', 'differential-geometry']"
4513380,Placing red and black colors on $2 \times 4$ chessboards [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question Suppose that two chessboards are also considered equivalent (aside from
rotational symmetry) if one can be obtained from the other by complementing
red and black colors. How many different $2 × 4$ chessboards are there? The given answer is $\frac{1}{4}(2^8+2^4+0+2^4)$ , but i did not understand the zero in the parentheses. I thought that the expression must have been $\frac{1}{4}(2^8+2^4+2^4+2^4)$ such that $2^8$ came from identity permutation , $2^4$ 's came from $180$ degree rotation , vertical and horizontal flippings. Can you explain the answer given by the book , and say why my answer is wrong ?","['permutation-cycles', 'abstract-algebra', 'combinatorics', 'solution-verification', 'discrete-mathematics']"
4513398,"Is the remainder term in a Taylor polynomial approximation for $\log{(1+x)}$ correct in Spivak's Calculus, Ch. 20?","In Chapter 20 of Spivak's Calculus 4th edition page 427 is written From the calculations on page 413, we see that for $x\geq 0$ we have $$\log{(1+x)}=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+...+\frac{(-1)^{n-1}x^n}{n}+\frac{(-1)^n}{n+1}t^{n+1}\tag{1}$$ where $$\left | \frac{(-1)^n}{n+1}t^{n+1}\right |\leq \frac{x^{n+1}}{n+1}$$ and there is a slightly more complicated estimate when $-1<x<0$ (Problem 16). As far as I can tell the last term in $(1)$ $$\frac{(-1)^n}{n+1}t^{n+1}\tag{2}$$ is the remainder term when we use a Taylor polynomial approximation. However, isn't this remainder supposed to be $$R_{n,0}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-0)^{n+1}, t\in (0,x)$$ $$=\frac{(-1)^n \cdot n!}{(1+t)^{n+1}}\cdot \frac{1}{(n+1)!}\cdot x^{n+1}, t\in (0,x)$$ $$=\frac{(-1)^n x^{n+1}}{(1+t)^{n+1}(n+1)}\tag{3}$$ ? I used $$f^{(k)}(x)=(-1)^{k-1} \frac{(k-1)!}{(1+x)^k}$$ thus $$f^{(k+1)}(t)=(-1)^{k} \frac{k!}{(1+t)^{k+1}}$$ Am I making some silly mistake here? Is the remainder term $(2)$ or $(3)$ ?","['integration', 'calculus', 'polynomials', 'taylor-expansion', 'derivatives']"
4513405,Coin tosses with unknown success probability,"Suppose I have a coin with unknown probability of success (let say Heads) $p$ which is uniformly distributed on $[0, 1]$ . I toss a coin $N$ times. What is the probability that I have got $n \leq N$ Heads? Ok. I've calculated (using Wolfram) that it is exactly $\frac{1}{N+1}$ , i.e. every $n$ is equiprobable. What is the intuitive explanation for this fact? I can understand by symmetry argument, why getting $k$ Heads should be exactly the same as getting $k$ Tails. But why getting $0$ Heads is the same as getting $7$ Heads (in case $N \neq 7$ ) is well beyond me. Update: Calculations involved: $$P(\text{# of Heads} = k) = \int\limits_0^1 {N \choose k} p^k (1-p)^{N-k} dp, $$ solving it gives: $$P(\text{# of Heads} = k) = {N \choose k} \frac{\Gamma(k+1)\Gamma(N-k+1)}{\Gamma(N+2)},$$ which turns out to be $\frac{1}{N+1}$ .",['probability']
4513414,How much Differential Geometry do you need to do PDE?,"Ever since I took functional analysis and I read in Brezis's famous book how to apply this to PDE I have been extremely interested in PDE and I have kept reading more and more of the standard material on PDE. This is all, of course, in $\mathbb{R}^n$ , so you don't really need to know any differential geometry, but I saw that there are people doing PDE on Riemannian manifolds for instance. I for one am not a big differential geometry fan, I do know the basics, but I don't enjoy it much and I am not really eager to study it further. However, I can't help but wonder if I am going to enocunter differential geometry if I am going to further study PDE and whether it would be helpful to improve my background in differential geometry if PDE is what I am interested in. Disclaimer: I am well aware of the fact that PDE has many subfields and one of them is even called ""Geometric PDE"" (or Geometric analysis if you prefer) and is basically a field that deals precisely with what I am trying to ""run away"" from, but my question is more of the sort ""do all PDE involve differential geometry if you immerse yourself deep enough into their study?"".","['self-learning', 'partial-differential-equations', 'reference-request', 'differential-geometry']"
4513433,Bound the probability of multivariate gaussian vector norm.,"Let's say $v \in \mathbb{R}^n \sim \mathcal{N}(0, \sigma I)$ . That is, $v$ is a gaussian random vector, whose entries are distributed $\mathcal{N}(0, \sigma)$ i.i.d. From the book ""C. Giraud. Introduction to high-dimensional statistics"", it can be concluded that $$
P\left(\frac{\sigma}{\sqrt{2}} \le \frac{\|v\|}{\sqrt{n}} \le \left( 2 - \frac{1}{\sqrt{2}} \right) \sigma \right) \ge 1 - (1 + e^2) e^{-n/24}
$$ Now, $W \in \mathbb{R}^{m \times n}$ is a deterministic matrix with normalized rows. My goal is to bound the event $$
P\left(\frac{\sigma}{\sqrt{2}} \le \frac{\|W v\|}{\sqrt{m}} \le 2 \sigma \right)
$$ preferably with a similar bound as above.
We can see that $W v \in \mathbb{R}^m \sim \mathcal{N}(0, \sigma W W^T)$ . My first idea was to define it as a generalized chi-squared distribution. It seems as an overkill, since this case is a lot simpler than the generalized one and it doesn't have a closed form. Second, I tried writing it as a sum of weighted chi-squared, where the weights are the eigenvalues of $W$ . This could work, but I'd rather have it in terms of $W$ , not its eigenvalues. Is there a better way? I would like it very much to hear new interesting approaches. Thank you! I really appreciate the help!","['statistics', 'probability-distributions', 'normal-distribution', 'chi-squared', 'probability']"
4513440,Why zeta function is related to this logarithmic function's integral?,"I've been going through ""103 (almost) impossible integrals, sums and series"" which is a book containing lots of interesting integrals.
I saw a very intriguing definite integral that the book stated has a relation with Riemann's zeta function in the following manner: $$
\int_{0}^{1} \frac{\ln^2(x+1)}{x} dx = \frac{1}{4}\zeta(3)
$$ I thought this connection's meaning would become clear when I prove it but now that I've proved the relation I still feel uncomfortable with it. It's just so out of place. So my question is, what is the intuition behind this equation? Why would zeta function have anything to do with this particular function's integral?","['integration', 'intuition', 'riemann-zeta', 'analysis']"
4513441,On the explicit series expansion for the inverse exponential/logarithmic integral li$(x)$ and Ei$(x)$,"Here is a possible explicit series expansion for an inverse of the Exponential Integral function $\text{Ei}(x)$ and logarithmic integral li $(x)$ . It uses the $n$ th derivative formula of Inverse Gamma regularized $\frac{d^n Q^{-1}(a,z)}{dz^n}$ . From Inverse function of li $(x)$ , we know that Ei $\displaystyle\left(-\lim_{a\to0}Q^{-1}(a,-ay)\right)=y<0$ Here is a Taylor series formula about $z=Q(a,x)$ with Gamma Regularized , the Pochhammer Symbol $(u)_v$ , and Kronecker delta $\delta_n$ and $\delta_{n,m}$ : $$Q^{-1}(a,z)=\sum_{n=0}^\infty\frac{(z-Q(a,x))^n}{n!}\left(x\delta_n+\left(-\frac{\Gamma(a)e^x}{x^{a-1}}\right)^n\sum_{j_2=0}^n\cdots\sum_{j_n=0}^n(-1)^{\sum\limits_{m=2}^nj_m}\delta_{\sum\limits_{m=2}^n (m-1)j_m,n-1}\Gamma\left(n+\sum_{m=2}^n j_m\right)\prod_{m=2}^n\frac1{j_m !}\left(\frac{a!e^x x^{1-a-m}}{m!}\right)^{j_m} \left(\sum_{k=0}^m(-1)^{m-k}\binom mk (1-a-k)_{m-1}Q(a+k,x)\right)^{j_m}\right)\tag1$$ We have to use the above Taylor series for this branch about $z=\text{Ei}(x)$ : $$\text{Ei}^{-1}(z)=x+xe^{-x}(z-\text{Ei}(x))+\frac14e^{-2x} (x-x^2) (z-\text{Ei}(x))^2+e^{-3x}\left(\frac{x^3}3-\frac{2x^2}3+\frac x6\right)(z-\text{Ei}(x))^3+e^{-4x}\left(\frac x{24}-\frac{x^4}4+\frac{3x^3}4-\frac{11x^2}{24}\right)(z-\text{Ei}(x))^4+…$$ shown by this special case . Now we use the formula: $$\text{Ei}^{-1}(x)=-\lim_{a\to0}Q^{-1}(a,-ax)=-\lim_{a\to0}\sum_{n=0}^\infty\frac{(z-\text{Ei}(x))^n}{n!}\left(-ax\delta_n+\left(-\frac{\Gamma(a)e^{-ax}}{(-ax)^{a-1}}\right)^n\sum_{j_2=0}^n\cdots\sum_{j_n=0}^n(-1)^{\sum\limits_{m=2}^nj_m}\delta_{\sum\limits_{m=2}^2 (m-1)j_m,n-1}\Gamma\left(n+\sum_{m=2}^n j_m\right)\prod_{m=2}^n\frac1{j_m !}\left(\frac{a!e^{-ax} (-ax)^{1-a-m}}{m!}\right)^{j_m} \left(\sum_{k=0}^m(-1)^{m-k}\binom mk (1-a-k)_{m-1}Q(a+k,-ax)\right)^{j_m}\right)$$ Clearly $-ax\delta_n\to0,-\frac{\Gamma(a)e^{-ax}}{(-ax)^{a-1}}\to x$ , but what is $$\lim_{a\to0}\prod_{m=2}^n\frac1{j_m !}\left(\frac{a!e^{-ax} (-ax)^{1-a-m}}{m!}\right)^{j_m} \left(\sum_{k=0}^m(-1)^{m-k}\binom mk (1-a-k)_{m-1}Q(a+k,-ax)\right)^{j_m}?$$ It may help that the explicit formula looks like a special case of the explicit series reversion formula $$$$ Maybe we substituted $Q^{-1}(a,z)\to -\lim\limits_{a\to0}Q^{-1}(a,-az)$ incorrectly. What is the correct way to use $(1)$ to expand $ \text{Ei}^{-1}(z)=-\lim\limits_{a\to0}Q^{-1}(a,-az)$ and what is its radius of convergence?","['special-functions', 'inverse-function', 'gamma-function', 'taylor-expansion', 'limits']"
4513445,Why do we need a Lie derivative of a vector field?,"Lie derivative of a smooth vector field $Y$ in the direction of a smooth vector field $X$ is defined (at least in our geometry course) as $L_X Y = \frac{d}{dt}\mid_{t=0} (\psi_\star Y)$ where $({\psi_t}_{t \in I})$ is the local flow of $X$ and $\psi_\star$ is the pushforward of $\psi$ , which is defined as the differential $d\psi$ . To my understanding, the definition says that we take the ""small change"" (derivative) of the local flow corresponding to ""small change"" in $Y$ (this is my interpretation of the $(\psi_\star Y)$ . And that we make one more derivative of this, that is, one ""small change"", this time corresponding to change in $t$ (this part is the $\frac{d}{dt}$ .) My question is: Do I understand this correctly? Or what is a better intuition behind this? And also, why do we even need to see how the $L_X Y$ look? What does it tell us to know how one vector field changes and it is somehow connected to changing the other vector field? I don´t understand why this is useful. Thank you very much!","['riemannian-geometry', 'differential', 'vector-fields', 'lie-derivative', 'derivatives']"
4513447,Does there exist a such function from a subset of $\mathbb{R}^2$ to $\mathbb{R}$ satisfying these conditions?,"Let $D$ be the set $\{ (x, y) \in \mathbb{R}^2 : x \geq y \geq 0 \}$ . Does there exist a function $f : D \to \mathbb{R}$ satisfying all of the following conditions? For all $x, x_1, x_2, y, y_1, y_2 \in \mathbb{R}$ , Suppose $x_1 - y_1 = x_2 - y_2$ . If $x_1 < x_2$ , then $f(x_1, y_1) > f(x_2, y_2)$ . Suppose $\frac{x_1}{y_1} = \frac{x_2}{y_2}$ . If $x_1 < x_2$ then $f(x_1, y_1) < f(x_2, y_2)$ . If $y_1 < y_2$ , then $f(x, y_1) > f(x, y_2)$ . $f(x, x) = 0$ . $f$ is continuous. $f(x, y) \geq 0$ . And finally, a somewhat optional item (which I still would like): Fix $y$ . Then $\lim_{x \to \infty} f(x, y) = \infty$ . I've been trying to come up with such a function but haven't yet been able to do so. I want to use the function $f$ as a quantitative description of how close $x$ is to $y$ for some particular use I have in mind. EDIT: Let me thank user psl2Z for answering the original question in a comment. I am still interested in this slight modification of the above: For 1, I should have stated that $x_1 - y_1 = x_2 - y_2 > 0$ .","['calculus', 'functions']"
4513450,Determinant of Jacobian of matrix multiplication,"Let $A \in \mathbb R^{n \times n}$ . We consider the map $$f_A : \mathbb R^{n \times n} \to \mathbb R^{n \times n} ,\quad X \mapsto AX.$$ By considering easy examples of $A$ one comes up quite fast with the conjecture $\det(D f_A) = \det(A)^n$ . Here $D f_A$ is the Jacobian matrix of $f_A$ . Is there an elegant proof which does not result in a long confusing computation? Idea (edit): I just came up with an idea. Obviously, the statement has only to be proven for non-singular $A$ . They are generated by elementary matrices. So it suffices to consider them since we have $f_A \circ f_B = f_{AB}$ .","['jacobian', 'determinant', 'linear-algebra', 'real-analysis']"
4513472,A question involving non-integer bases and integer exponents,"I'm a high school math student. My last math class was Algebra $2$ Honors, so I'm not super well-versed when it comes to set theory - that being said, since I first thought about this problem, it's been nagging at me. Let there be two variables, a and b, defined like this: $a \in \Bbb{R}$ , $a \notin \Bbb{Z}$ , and $b \in \Bbb{Z}$ . By those constraints, could the statement $a^b \in \Bbb{Z}$ ? For the record, I know that $b^a$ can! For example, if $a = 1.5$ and $b = 9$ , then $b^a = 9 \times 3 = 27$ , and $27 \in \Bbb{Z}$ . EDIT: I saw an answer using $a = \root\of2$ and $b = 2$ , which works perfectly! I do wonder, though: Could you extrapolate this any further? For example, if $a \in \Bbb{Q}$ , could there still be a set of values where $a^b \in \Bbb{Z}$ ? :)","['elementary-set-theory', 'recreational-mathematics']"
4513505,find the number of maximal three-square-free subsets of $S$,"Let $S = \{1,\cdots, 15\}$ . Call a subset $A$ of $S$ three-square-free if there are no three distinct elements of $A$ so that their product is a perfect square. Find the number of three-square-free subsets of $S$ with the maximum cardinality, and determine what this cardinality is. As an example, $\{1,2,3\}$ is a three-square-free subset of $S$ as no three distinct elements have a product that's a square. I know the maximum cardinality of a three-square-free subset of $S$ is $10$ . Indeed, at least one element cannot be chosen from each of the following three disjoint sets: $\{1,4,9\}, \{2,6,12\}, \{3,5,15\}, \{7,8,14\},$ so the maximum cardinality is at most $11.$ Suppose for a contradiction that a three-square-free set $A$ has size $11.$ Then $10 \in A,$ and $A$ must exclude at least one element from each of the following disjoint sets: $\{1,4,9\}, \{2,5\}, \{6,15\}, \{7,8,14\}.$ Since $A$ has size $11,$ it must include $\{3,12\}$ as a subset, so it must exclude one element from each of the sets $\{1\}, \{4\}, \{9\}, \{2,5\}, \{6,15\}, \{7,8,14\}$ , meaning it has size at most $10$ , a contradiction. Finally, note that $A = \{1,3,4,5,6,7,10,11,13,14\}$ works.","['contest-math', 'divisibility', 'elementary-number-theory', 'combinatorics', 'discrete-mathematics']"
4513521,"Derivative of $f(r\cos\theta,r\sin\theta)$ at $(0,0)$","Can someone help with any hints or a proof to this question? Question. Suppose $n>0$ , $r>0$ , and $0<\theta<2\pi$ . Define $f:\mathbb{R^2}\rightarrow \mathbb{R}$ by $$f(r\cos(\theta),r\sin(\theta))=r^n \cos(\theta) \ \text{ and } \ f(0,0)=0.$$ Determine for which $n$ , $f$ is partially differentiable with respect to $x$ and $y$ at $(0,0)$ . For which $n$ is it true that f is differentiable at $(0,0)$ ? Find the value of derivative at $(0,0)$ ？ Since $x=r\cos(\theta), y=r\sin(\theta$ ), I tried to replace $f(r\cos(\theta),rsin(\theta))=r^n \cos(\theta)$ by $\require{enclose}
    \enclose{horizontalstrike}{f(x,y)=x(x^2+y^2)^{n-3}}$ $\color{red}{(f(x,y)=x(x^2+y^2)^{\frac{n-1}{2}}),}$ then computed the partial derivative by definition directly. I'm not sure if this process is right or not.","['partial-derivative', 'calculus', 'derivatives', 'real-analysis']"
4513547,Finding the pattern of Lagrange Inversion Formula for $x+\sin x$,"Today I read about the Lagrange Inversion Theorem, which to restate: $$f\text{ is analytic at point }a, f'(a) \neq 0 \\ \implies f^{-1}(x)=a+\sum_{n=1}^\infty g_n\frac{(x-f(a))^n}{n!}, \\ g_n=\lim_{t\rightarrow a}\bigg(\frac{d^{n-1}}{dt^{n-1}}\bigg[\Big(\frac{t-a}{f(t)-f(a)}\Big)^n\bigg]\bigg)$$ To test it, I chose $f(x)=x+\sin x$ and $a=0$ . The formula then became: $$f^{-1}(x)=\sum_{n=1}^\infty g_n\frac{x^n}{n!},g_n=\lim_{t\rightarrow 0}\bigg(\frac{d^{n-1}}{dt^{n-1}}\bigg[\Big(\frac{t}{t+\sin t}\Big)^n\bigg]\bigg)$$ I computed $g_n$ for small $n$ first. The first odd ones include $$g_1=\frac{1}{2},g_3=\frac{1}{16},g_5=\frac{1}{16},g_7=\frac{43}{256},g_9=\frac{223}{256},g_{11}=\frac{60623}{8192},g_{13}=\frac{764783}{8192};$$ { $g_2,g_4,\dots,g_{14}$ }  have all shown to be zero. The only noticeable pattern I can see in $g_n$ for odd n is that for the denominators, which I'll call $d_n$ , is that $d_{n+4}=2^{(n+11)/4}d_n$ ; I cannot see a pattern for the numerators. How do I find the formula for the coefficients? Or does no closed formula (other than the limit above) exist?","['limits', 'lagrange-inversion', 'derivatives', 'sequences-and-series']"
4513552,Solve $(x^3+1)=2\sqrt[3]{2x-1}$ algebraically?,"I'm trying to solve the said equation in the thread title algebraically. $$(x^3+1)=2\sqrt[3]{2x-1}$$ Cubing both sides and simplifying: $$x^9+3x^6+3x^3-16x+9 = 0$$ Not sure if this can be solved algebraically? Edit: WA gives $3$ solutions $x=1,\frac{1}{2}(-1-\sqrt{5}),\frac{1}{2}(-1+\sqrt{5})$","['algebra-precalculus', 'inverse-function', 'polynomials']"
4513553,Expected Value Notation Question,Does the red marked box this mean $E[(X-E[X])^2]$ or $E[X-E[X]]^2$ ?,"['expected-value', 'statistics', 'probability']"
4513557,"$\int_{3}^{ab+2}f\left(x\right)dx=\int_{3}^{a+2}f\left(x\right)dx+\int_{3}^{b+2}f\left(x\right)dx$ where $a,b\in \left(1,\infty \right)$ find $f(x)$","$f(x)$ is a differentiable function defined for x>2 satisfying $$\int_{3}^{ab+2}f\left(x\right)dx=\int_{3}^{a+2}f\left(x\right)dx+\int_{3}^{b+2}f\left(x\right)dx$$ where $a,b\in \left(1,\infty \right)$ and find $f(x)$ I found this question on an Australian math exam and it was actually a mulitple choice question where the intended method was trial and error. I was wondering if there was any more intuitive approach to the question. The question can be accessed here with the MCQ options","['integration', 'calculus', 'functions', 'definite-integrals']"
4513590,How to use chain rule in this equation?,"Source: Prof. Meinrenken's notes on Differential Geometry, page No 72: Theorem $4.1$ . let $(U,\varphi)$ be  a  coordinate  chart  around $p$ .A linear map $v :C^{\infty}(M) \to \mathbb{R}$ is in $T_pM$ if and  only  if it  has  the  form $$v(f)=\sum_{i=1}^{m} a^i\frac{\partial(f \circ \varphi^{-1})}{\partial u^i}|_{u=\varphi(p)}$$ for some $a=(a^1,.....,a^m)\in \mathbb{R}^m$ Proof : Given a linear  map v of this form.Let $\bar{\gamma}: \mathbb{R} \to \varphi(U)$ be  a curve  with $\bar\gamma(t)=\varphi(p) +ta $ for $|t|$ suffiently  small .Let $\gamma = \varphi^{-1} \circ \bar\gamma$ .Then $$\frac{d}{dt}|_{t=0} f(\gamma(t))=\frac{d}{dt}|_{t=0}(f\circ \varphi^{-1})(\varphi(p) +ta)=\sum_{i=1}^{m} a^i\frac{\partial(f \circ \varphi^{-1})}{\partial u^i}|_{u=\varphi(p)} $$ by chain rule My confusion :How to use   chain rule  in this equation why $$\frac{d}{dt}|_{t=0}(f\circ \varphi^{-1})(\varphi(p) +ta)=\sum_{i=1}^{m} a^i\frac{\partial(f \circ \varphi^{-1})}{\partial u^i}|_{u=\varphi(p)} ?$$ I know  that chain rule mean $$\frac{\partial (f \circ F)}{\partial x^i} =\sum_{j=1}^m\frac{\partial f}{\partial y^j}\frac{\partial F^j}{\partial x^i}.$$ where $f:V\to\Bbb R$ on an open set $V\subseteq\Bbb R^m$ , with its partial derivative with respect to the $j$ -th coordinate denoted $\frac{\partial f}{\partial y^j}$ . and $F:U\to V$ on an open set $U\subseteq\Bbb R^n$ , with its $j$ -th component function's partial derivative with respect to the $i$ -th coordinate denoted $\frac{\partial F^j}{\partial x^i}$ . My attempt: $$\frac{d}{dt}|_{t=0}(f\circ \varphi^{-1})(\varphi(p) +ta)=\frac{d}{dt}|_{t=0}((f\circ \varphi^{-1})(\varphi(p))  + (f\circ \varphi^{-1})ta)=\frac{d}{dt}|_{t=0}((f(p))  + (f\circ \varphi^{-1})ta) $$ After  that Im not able to proceed further",['differential-geometry']
4513719,Is it nonsensical to try to 'prove' Euler's 'formula' in real numbers? What is Wikipedia/proofwiki even doing?,"Euler's formula (...'for complex numbers'?) is simply $$e^{iz} = \cos(z)+i\sin(z)$$ And 1 way to prove this is using complex Maclaurin series for the complex exponential, sine and cosine. Now, there's a way to 'prove' the 'theorem' $e^{it} = \cos(t)+i\sin(t)$ , which is often said to be 'Euler's formula (...for real numbers?)' without using the original formula above such as by using Maclaurin . And ok fine, I guess you can indeed get the Maclaurin series expansion of $e^z$ by defining $e^z$ simply as the unique solution of $g:\mathbb C \to \mathbb C, g'=g,g(0)=1$ without explicitly getting $e^z = e^x(\cos(y)+i\sin(y))$ then you can just let $z=it$ (i.e. let $(x,y)=(0,t)$ ). But... There's a 'proof' that Wikipedia does, again without using the original formula, ' by differentiation ' ( proofwiki does this too ) where we consider the function $f:\mathbb R \to \mathbb C,f(t) = \frac{\cos(t)+i\sin(t)}{e^{it}}$ , where $f$ is indeed well-defined and then show $f$ is identically 1. Question 1: So what exactly is $h:\mathbb R \to \mathbb C, h(t)=e^{it}$ here assuming $e^z$ is defined simply as the unique solution of $g:\mathbb C \to \mathbb C, g'=g,g(0)=1$ ? Goal : I'm asking if this option is strictly speaking sensible. Like those probability textbooks that talk about independence but are actually strictly speaking nonsensical. I wanna make sure there's no imprecision actually. If it's actually nonsensical, then I wanna be sure to specify that the same way I teach probability without measure theory: 'It's nonsensical but just play along.' And then in this case, which parts are nonsensical, and how should they be taught? Like in probability when we teach some nonsensical parts by 'assuming the function is ""well-behaved""' or something. Question 2 : Alternatively, is there some textbook that 'proves' Euler's formula for real numbers based on the unique $g$ definition? Anyhoo, guess for Question 1 : It's $h(t)=g(0,t)$ . Explicitly: I think $h(t)=g(0,t)=u(0,t)+iv(0,t)$ , where $g(z)=g(x,y)=u(x,y)+iv(x,y)$ is the unique solution to $g:\mathbb C \to \mathbb C, g'=g,g(0)=1$ ? And then we can indeed show that $h$ is nowhere zero? (Probably we can show $g$ is nowhere zero?) $\frac{d}{dt} h(t) = ih(t)$ , i.e. we have both $\frac{d}{dt} u(0,t) = -v(0,t)$ and $\frac{d}{dt} v(0,t) = u(0,t)$ ? (I think this is what proofwiki assumes too in another proof .) $-\frac{d}{dt} h(-t) = -ih(-t)$ ? As for differentiability of $h$ , well I guess it follows from differentiability (or at least holomorphicity) of $g$ ? Notes : No need for specifics. I can work it out. I just wanna know if Wikipedia is just doing nonsense or if there's something really possible behind 'proving' the 'formula' from an alternate definition of the complex exponential. Currently, my thought is that they're doing just some heuristic definition that $e^{(\text{whatever})(x)} = \text{whatever} \ e^{(\text{whatever})(x)}$ . But eh maybe there's a rigorous justification that is somehow possible without getting the formula for $e^z$ explicitly.",['complex-analysis']
4513721,"If A is a $C^*$ algebra, then there is a $C^*$ algebra $A_1$ with an identity such that $A_1$ contains A as an ideal.","This is in the book Functional analysis by John B. Conway. Chapter 9 prop: 1.9.
Basically in the book, he takes $A_1=\{a+\alpha \}$ where $(a+\alpha )$ is just a formal sum. And defined multiplication and addition as: i) $(a+\alpha )(b+\beta )=(ab+\alpha b+\beta a+\alpha \beta )$ ii) $(a+\alpha )+(b+\beta )=(a+b)+(\alpha +\beta )$ And norm as: $$\|a+\alpha \|=\sup \{ \|ax+\alpha x\|:x\in A,\|x\|\leq 1\}.$$ I am unable to prove that $A_1$ is a Banach algebra. I am trying to show if $\|a+\alpha \|=\|a\|+|\alpha |$ or not but unable to do so.","['c-star-algebras', 'operator-algebras', 'operator-theory', 'real-analysis', 'functional-analysis']"
4513748,"Solving $y'=f(x, y)$ where $\forall k>0. f(kx, ky) = f(x, y) \ne f(-x, -y) = f(-kx, -ky)$ (Tom Apostol Calculus vol. $1$, ex. $8.26.6$)","Tom Apostol wrote that a homogeneous first-order equation is of a form $y' = f(x, y)$ , where $\forall x,y \in R. \forall t \ne 0. f(x, y) = f(tx, ty)$ . Now, his excercise $8.26.6$ is: $xy' = y - \sqrt{x^2 + y^2}$ Can be transformer into something which looks like a first-order homogeneous equation: $y' = \frac{y - \sqrt{x^2 + y^2}}{x}$ However, I'm not sure why it can be seen as such, because for negative $t$ , it does not hold that $f(x, y) = f(-x, -y)$ . Namely: $f(x, y) = \frac{y - \sqrt{x^2 + y^2}}{x}$ $f(-x, -y) = \frac{-y - \sqrt{x^2 + y^2}}{-x} = \frac{y + \sqrt{x^2 + y^2}}{x} \ne f(x, y)$ However, if we proceed and solve it as a homogeneous solution (e.g. by introducing $v = \frac{y}{x}$ ), we can solve it. Why is that justified, even though $f(x, y) = f(tx, ty)$ only for $t>0$ ? Thanks!",['ordinary-differential-equations']
4513788,Finding the radius of a neutron star that allows all points on its surface to be seen,"At first glance, this fascinating question may seem better placed on Physics Stack Exchange. But, since I am only questioning the mathematics of the solution I decided it was more appropriate to place it here. How small should a neutron star be so that all points on its surface are visible? Hints: use the form of the photon orbit equation. Consider a trajectory where $du/dφ = 0$ for $u = 1/R_n$ and $φ = 0$ where $R_n > R$ is the ‘radius’ of the neutron star. Determine numerically, what the ratio $R_n/R$ should be so that $u = 0$ for $φ = π$ . My interpretation of this question is to find the ratio of neutron star radius, $R_n$ to that of the Schwarzschild radius, $R$ given that all points on the neutron star are visible. Taking the most extreme point for photons emanating from a point directly behind the neutron star, since we can't see through the star this point would otherwise be invisible. However, if the curvature of spacetime is so large it will deflect light coming from the back of the star, then it is possible to see this point. If this point can be seen, then by that logic all points of the neutron stars' surface will be visible too. Above is my attempt to draw the situation. The outer blue ring of the neutron star has a radius of $R_n$ and the inner (red) ring is characterized by the Schwarzschild radius, $R$ . Of course, the situation I drew above is highly contrived as we all know that the light rays emanating from the point at the back of the star will keep bending as there is no reason why they should end up in a perfect straight line. Here is the author's solution which I just cannot understand: $$\left(\frac{du}{d \varphi}\right)^2+u^2-Ru^3={R_n}^{-2}-R{R_n}^{-3}\tag{1}$$ so that $u=1/R_n$ when $du/d\varphi=0$ . Separating the variables gives $$\int_{u=0}^{1/R_n}\frac{du}{\sqrt{{R_n}^{-2}-R{R_n}^{-3}-u^2+Ru^3}}=\int_{\varphi = 0}^{\pi}d\varphi=\pi\tag{2}$$ So that all the surface is visible it is assumed that light that grazes the surface at $φ=0$ escapes to $r=∞$ or $u=0$ at $φ=π$ . Using the change of variable $p=Ru$ $$\int_{p=0}^{R/R_n}\frac{dp}{\sqrt{{(R/R_n)}^2-{(R/R_n)}^3 -p^2+p^3}}=\pi\tag{3}$$ To determine $x=R/R_n$ we need to solve $$\int_{p=0}^{R/R_n}\frac{dp}{\sqrt{{x}^2-x^3-p^2+p^3}}=\pi\tag{4}$$ Solve this numerically. The solution is $x ≈ 0.568$ giving the ratio $R_n/R = 1/x = 1.76$ . That is if the radius of a neutron star is less than $1.76$ times its Schwarzschild radius the whole surface is visible due to the deﬂection of light. Current estimates on neutron star sizes give a ratio larger than $1.76$ suggesting that the entire surface of real neutron stars is not visible. I have 2 questions regarding this solution: Below equation $(1)$ it is written that "" $u=1/R_n$ when $du/d\varphi=0$ "". But  since $du/d\varphi=0$ , a simple rearrangement of $(1)$ should mean that $$\frac{du}{d \varphi}=\pm\sqrt{{R_n}^{-2}-R{R_n}^{-3}-u^2+Ru^3}=0\tag{5}$$ If so then the denominator in the integrand of $(2)$ will diverge, so how can it ever be equal to $\pi$ ? Moreover, with $du/d\varphi =0$ then $\left(du/d\varphi\right)^2=0$ and by my logic there is no differential equation to solve. Put another way, since $u=1/R_n$ this implies that $x=p$ , then substituting $x=p$ into equation $(4)$ leads to the same fate. Why was the negative root in $(5)$ discarded? Just below equation $(4)$ it says ""Solve this numerically"". I really don't know how to do this and I'm not much of a programmer, but all I want to do is verify that this numerical integration results in $x\approx 0.568$ . Could someone please show or explain to me how this is done? Remark: One reason why I am so sceptical about the authors' solution is while typesetting the solution I have already identified 2 typos which I show below underlined in red: As can be seen above, a square root is missing in the denominator of the second equation and the ratio should strictly be written as ' $1.76$ '.","['ordinary-differential-equations', 'calculus', 'physics', 'numerical-methods', 'mathematical-physics']"
4513794,Parallel transport on the Group manifold,"We think of the group as a manifold G (called a Group manifold), whose points are the elements of our Lie group. More generally, we could think of any manifold H on which the elements act
as smooth transformation The infinitesimal group elements are to be pictured as particular vector fields on G (or, indeed, H). That is, we think of ‘moving G’ infinitesimally along the relevant vector field $\zeta$ on G, in order to express the transformation that corresponds to pre-multiplying each element of the group by the infinitesimal element
represented by $\zeta$ . ... The relevant notion of ‘parallelism’ comes from the group action, supplying the needed notion of ‘parallel transport’, which
actually gives a connection with torsion but no curvature. Page-312, 313 Roger Penrose's Road to Reality Could someone explain what Penrose is meaning by the bolded part? I understand the group action is to move the points under flow of vector field but what does mean for the parallelism to come from it? I have read these answers but found the discussion to be too algebraic. Could a more visual understanding of what's going on be provided?","['connections', 'group-theory', 'differential-geometry']"
4513798,Is this a measure?,"Let $\mu: \mathscr B \left(\mathbb R^d\right) \to \mathbb R$ be defined by $$ \mu(A) = \left\{\begin{array}{ll} \|\mathscr F(\chi_A)\|_{L^2(\mathbb R^d)}^2 & \chi_A\in L^1(\mathbb R^d) \\
\infty & \,  \chi_A\not\in L^1(\mathbb R^d)\end{array}\right. $$ where $\mathscr F(\chi_A)$ denotes the Fourier transform of $\chi_A$ . Is $\mu$ a measure? I know $\mu(A)\geq 0$ and $\mu(\emptyset)=0$ . So I have to show that $$\mu \left( \bigcup_{n \in \mathbb N} A_n \right) = \sum_{n\in\mathbb N} \mu(A_n)$$","['measure-theory', 'fourier-transform']"
4513809,"Visualizing vector fields ""multiplied"" by a function","If $M$ is a smooth manifold,
how do I interpret an expression such as $f X$ where $f \in C^{\infty}(M)$ and $X$ is a vector field, i.e. a map $M \rightarrow TM$ ? To my understanding, a vector field is a derivation, a map with certain properties. I imagine it as the set of ""directions"" in each point of the manifold. Now I came accross the definition of a covariant derivative of (smooth) vector fields $X, Y$ , which confuses me. The properties satisfied by the covariant derivative are the following: $\nabla_{fX + gY} Z = f \nabla_X Z + g \nabla_Y Z$ $\nabla_X (Y + Z) = \nabla_X Y + \nabla_X Z$ $\nabla_X (fY) = X(f)Y + f \nabla_X Y$ My question: What does the $f X$ says? If $f$ just takes points from $M$ and assigns them a value (I guess in $\mathbb{R}^n$ ?), then how can it be multiplied or composed with a vector field that goes from $M$ to the tangent space? And what is the difference between $X(f)$ and $fX$ in the conditions? Thank you very much.","['vector-fields', 'smooth-functions', 'smooth-manifolds', 'manifolds', 'derivatives']"
4513819,Coloring $5 \times5$ transparent chessboard in space.,"We have $5 \times 5$ chessboard floating in space , so rotation and reflection are allowed. We want to color it using $m$ distinct colors such that when we color a square , the both sides of that square will be colored , so the both faces of chess board will be coloring while we are coloring one face. (You may think that this chessboard is transparent ) How many nonequivalent coloring are there ? My work: $\psi(\pi_0^*)=m^{25}$ , the identity permutation. $\psi(\pi_1^*)=m^{7}$ , $90$ degree rotation. $\psi(\pi_2^*)=m^{13}$ , $180$ degree rotation. $\psi(\pi_3^*)=m^{7}$ , $270$ degree rotation. $\psi(r_1^*)=m^{15}$ , horizontal reflection. $\psi(r_2^*)=m^{15}$ , vertical reflection. $\psi(r_3^*)=m^{15}$ , diagonal reflection (positive slope diagonal). $\psi(r_4^*)=m^{15}$ , diagonal reflection (negative slope diagonal). $$\frac{1}{4}(m^{25}+2m^7 +m^{13} +4m^{15})$$ Is my solution correct ? EDITED ANSWER: $$\frac{1}{8}(m^{25}+2m^7 +m^{13} +4m^{15})$$","['solution-verification', 'combinatorics', 'discrete-mathematics', 'permutation-cycles']"
4513821,The behavior of an $L^2(\mathbb{R})$ function close to $x=0$.,"Is there a way to prove that an $L^2(\mathbb{R})$ function close to $x=0$ is $\mathcal{O}(x^{-1/2})$ ? In other words, can we prove that there is a function $\tilde{f}$ equal to $f$ almost everywhere such that $$
\limsup_{x\to 0} \left(|x|^{1/2} |\tilde{f}|\right)<\infty?
$$ I am working on a proof involving $L^2(\mathbb{R})$ , and this fact would be really useful. It seems reasonable since the function needs to be integrable across $x=0$ but I am not exactly sure how to prove that. No need of the detailed proof, just an idea of what tool I should be using.","['approximation-theory', 'asymptotics', 'real-analysis', 'hilbert-spaces', 'functional-analysis']"
4513842,How do I evaluate the definite integral?,"I have the following definite integral: $$
I = \int_{0}^{2\pi}{\sqrt{1+2a\cos{x}+a^2}\,\mathrm dx}
$$ I have tried several techniques to evaluate it without success. I have tried also using Wolframalpha but it fails (standard computation time exceeded). Is there a way to evaluate this integral? I appreciate your help","['integration', 'definite-integrals']"
4513854,How Zorn's lemma is used here?,"I am studying the following theorem in Advanced modern algebra/ Joseph J. Rotman. - Third edition,(Graduate studies in mathematics ; volume 165), A left $R$ module $M$ over a ring $R$ is semisimple if and only if every submodule of $M$ is a direct summand. I have not understood how Zorn's lemma is used in the converse part of the theorem : By Zorn's lemma, there is a family $(S_j)_{j \in I}$ of simple sub-modules of $M$ maximal such that the sub-module $U$ they generate is their direct sum: $U = \oplus_{j \in I} S_j.$ I have taken (I hope this is a right direction) $$F = \{ (S_j)_{j \in I}  : \text{The submodule generated by }(S_j)_{j \in I} \text{ is their direct sum where each $S_j$ are simple}\}$$ The previous part of the argument tells : Every non-zero sub-module $B$ contains a simple summand. This implies that $M$ contains a simple summand and consequently $F$ is nonempty. But somehow I am not able to show rigorously that every chain has an upper bound. Any help is appreciated.","['axiom-of-choice', 'abstract-algebra', 'semi-simple-rings', 'modules']"
4514023,Find the radius of the circle cutting sides of equilateral triangle,"Consider the figure below. The triangle is equilateral. Find the radius of the circle. My try: I have dropped perpendiculars from the center of the circle to respective chords naming them $x,y,z$ and named the other line segments as $m,n,p,q,s,t$ as shown below. By Viviani's theorem we have $$x+y+z=\frac{\sqrt{3}a}{2}$$ Where $a$ is the side length of the triangle. By Chords theorem we have: $$\begin{aligned}
m(m+5) &=t(t+3) \\
q(q+4) &=s(s+3) \\
p(p+4) &=n(n+5) \\
m+n+5 &=p+q+4=s+t+3=a
\end{aligned}$$ Also by Pythagoras theorem if $R$ is the radius of the circle we have: $$R^{2}-x^{2}=\frac{9}{4}, \quad R^{2}-y^{2}=4 ,\quad R^{2}-z^{2}=\frac{25}{4}$$ I guess this is a very tedious task? Any better ways?","['contest-math', 'circles', 'geometry', 'triangles', 'plane-geometry']"
4514078,On step-4 of Munkres' proof of the Urysohn lemma,"In this question my reference is the book Topology by Munkres. The proof of Urysohn's lemma can be found in section 33. In the last step of the proof, the author says: Now we prove continuity of $f$ . Given a point $x_{0}$ of $X$ and an open interval $(c, d)$ in $\mathbb{R}$ containing the point $f\left(x_{0}\right)$ , we wish to find a neighborhood $U$ of $x_{0}$ such that $f(U) \subset(c, d)$ . Choose rational numbers $p$ and $q$ such that $$
c<p<f\left(x_{0}\right)<q<d .
$$ We assert that the open set $U=U_{q}-\bar{U}_{p}$ is the desired neighborhood of $x_{0}$ . My question is: what if $\bar U_p=U_q$ and thus $U=\varnothing$ ? The book's inductive construction of the sets $\left\{U_p:p\in[0,1]\cap\mathbb Q\right\}$ doesn't seem to exclude this possibility: Let $P$ be the set of all rational numbers in $[0,1]$ . Since $P$ is countable, we may list numbers in $P$ as an infinite sequence, and for convenience we may assume that the first two numbers are $1$ and $0$ . First we shall define $U_1=X-B$ . Now, $A$ is a closed set contained in the open set $U_1$ , so by normality of $X$ we may choose an open set $U_0$ such that $A\subset U_0$ and $\bar U_0\subset U_1$ . In general, let $P_n$ be the first $n$ rational numbers in the infinite sequence, and assume that $U_p$ has been defined for all $p\in P_n$ , satisfying the condition that $$
p<q\quad\Rightarrow\quad \bar U_p\subset U_q.
$$ Let $r$ be the next rational number following $P_n$ , and we want to define $U_r$ . Since $P_n$ is finite and $r$ is neither $0$ nor $1$ , $r$ must have an immediate predecessor $p$ and successor $q$ in $P_n$ . Since $\bar U_p$ is a closed set contained in the open set $U_q$ , we may find by normality of $X$ an open set, defined to be $U_r$ , such that $\bar U_p\subset U_r$ and $\bar U_r\subset U_q$ . We assert that the above relation now holds for every pair of elements of $P_{n+1}$ . If both elements lie in $P_{n}$ , then the condition holds by the induction hypothesis. If one of them is $r$ and the other is a point $s$ of $P_{n}$ , then either $s \leq p$ or $s \geq q$ , so $$
\bar{U}_{s} \subset \bar{U}_{p} \subset U_{r}~~~(s\le p),\qquad \bar{U}_{r} \subset U_{q} \subset U_{s}~~~(s\ge q)
$$ In both cases the condition is satisfied, so $U_p$ has been defined for all $p\in P_{n+1}$ . By induction it has been defined for all $p\in P$ . Connectedness of the space $X$ seems irrelevant here. But if $X$ is disconnected, it is possible that $\bar U_p=U_q$ . Any insight is appreciated. Thanks in advance.",['general-topology']
4514135,Number of Inflection Points for Compositions of Exponential Functions,"Lately I've been studying some functions of the form $f_1(f_2(\cdots(f_n(x))\cdots))$ where each $f_i(x) = a_i^x$ for some positive, nonunity, real numbers $a_i$ . So for example, if $a_1 = 2$ and $a_2 = 0.3$ the function would be $2^{0.3^x}$ . A more elaborate example would be the function, $0.9^{e^{1.01^{0.5^x}}}$ . I'm not studying any one particular function, but rather a whole family of them together. Something I've noticed is that they all seem to have at most only one inflection point (alternatively their derivatives have at most one relative extremum). But I cannot seem to prove this result, so it remains an observation. The problem for me has been that the derivatives, even in the small $n=3$ cases are a little too unwieldy. For anyone who is curious, if we denote $F_m^n(x) = f_m(f_{m+1}(\cdots (f_n(x))\cdots))$ (and trivially let $F_{i+1}^i(x) = x$ ) then $$\big(F_m^n\big)^{\prime}(x) = \prod\limits_{i=m}^n F_i^n(x)\ln(a_i)$$ and $$\big(F_m^n\big)^{\prime\prime}(x) = \big(F_m^n\big)^{\prime}(x)\times\bigg(\sum\limits_{i=m}^n\ln(a_i)\Big(F_{i+1}^n\Big)^{\prime}(x)\bigg)$$ Setting the latter factor (the one with the sum) in the second derivative equal to $0$ hasn't given me much success. My suspicion is that there is some argument to be made where you could group the factors of $\big(F_m^n\big)^{\prime}(x)$ by whether they are increasing or decreasing functions (it isn't hard to see that a composition of some number of exponential functions will be either always increasing or always decreasing), like so: $$\big(F_m^n\big)^{\prime}(x) = \Big(\prod\limits_{i=m}^n\ln(a_i)\Big)\times\Big(\prod\limits_{\text{increasing ones}}F_i^n(x)\Big)\times\Big(\prod\limits_{\text{decreasing ones}}F_i^n(x)\Big)$$ and then argue that as $x$ increases from $-\infty$ one of the two latter factors will dominate the other (causing, for example, the whole derivative to increase), and then there will be a point where it switches and the other factor becomes more dominant (causing then an overall decreasing trend). But of course, I could be completely off base on this. I was wondering if anyone knew of some resources to check out for these kinds of functions, maybe knew a counterexample, or perhaps even knew of an actual proof that each of these kinds of functions have at most one inflection point.","['functions', 'derivatives', 'exponential-function', 'function-and-relation-composition']"
4514178,the expected value is a non-negative random variable is always greater than the expected value of its subset?,Let's say $X$ is a non-negative random variable. And say $c$ is a positive constant. Why is $$E(X)\geq E\left(X_{\chi (X \geq c) }\right)$$ I don't know why the expected value is a non-negative random variable is always greater than the expected value of its subset? The second inequality that confused me is $$E\left(X_{\chi (X \geq c) }\right) \geq c P(X \geq c)$$ Cany anyone help me explain these 2 inequalities?,['statistics']
4514197,Direct limit of sequences induced by fusing together copies of $\mathbb{Z}$,"Let $A=l^\infty(\mathbb Z,\mathbb Z)$ be the abelian group of bounded sequences $\mathbb Z\to\mathbb Z$ . Define a homomorphism $f\colon A\to A$ by $$f(a)(n)=a(2n)+a(2n+1),$$ for $a\in A$ and $n\in\mathbb Z$ . Consider the directed system $$A\xrightarrow{f}A\xrightarrow{f}A\xrightarrow{f}...$$ Question: What is the direct limit of this system? Thoughts: The map $f$ is induced by ""fusing together"" two the $(2n)$ -th copy of $\mathbb Z$ and the $(2n+1)$ -th copy, i.e. $f$ is a direct sum of the homomorphisms $$A\supseteq\mathbb Z_{(2n)}\oplus\mathbb Z_{(2n+1)}\to\mathbb Z_{(n)}\subseteq A,$$ $$(x,y)\mapsto x+y,$$ where a subscript in parentheses denotes the index of a copy of $\mathbb Z$ in $A$ . In particular, $\underbrace{f\circ f\circ\ldots\circ f}_{k\text{ times}}$ is a surjective map for any $k$ . So I'm tempted to think that the direct limit is isomorphic to $A$ , but I don't see a natural way to write down elements of the direct limit.","['limits-colimits', 'abstract-algebra', 'sequences-and-series', 'group-theory', 'abelian-groups']"
4514216,"Idea behind ""distance measure""","From Mattila's Fourier Analysis and Hausdorff Dimension (page 59): Theorem 4.6(a): If $A \subset \mathbb{R}^n$ is Borel and its dimension is greater than $(n+1)/2$ , then the interior of $D(A):=\{\|x-y\|:x,y \in A\} \in \mathbb{R}$ is nonempty. In the proof of this statement, Mattila introduces the concept of a distance measure $\delta(\mu)$ defined on $B \in \mathbb{R}$ induced by a $\mu \in \mathcal{M}(A)$ (the set of nonzero finite measures compactly supported on $A$ ) by defining $$\delta(\mu):= \int\mu\{y: \|x-y\|\in B\} d\mu (x).$$ What is the idea behind this definition? I can't really wrap my mind around it. Also, how is it that we know that for any continuous $\varphi$ on $\mathbb{R}$ , we have $$\int \varphi d\delta(\mu)= \iint \varphi(\|x-y\|) d\mu(x)d\mu(y)?$$","['measure-theory', 'hausdorff-measure', 'dimension-theory-analysis', 'geometric-measure-theory']"
4514263,"Statistics, PDF and CDF pre-university (A level)","This is a question from an A level textbook on continuous random variables. It states that the CRV $T$ has pdf $f(t)=0.5 ~for~ 1<t<3$ and then goes on to ask us to find the CDF (easy peasy $F(t)=\frac{1}{2} (t-1)$ and to show that the probability of selecting two independent observation less than 2.5 is $\frac{9}{16}$ (again, fine, $F(2.5)\times F(2.5)$ ).
The third part, however, is a bit weird. I'll quote exactly "" $S$ is the larger of two independent observations of $T$ . By considering the CDF of $S$ show that $S$ has pdf $g(s)=\frac{s-1}{2}$ and then the details about the ranges it applied to"" I couldn't get anywhere with this and cheated looking up their worked solutions which amounted to $P(S=s)=P(T=s)\times P(T\leq S)\times 2=0.5\times \frac{s-1}{2}\times 2=\frac{s-1}{2}$ In other words, you pick a value of $T=s$ and the next one needs to be smaller than it ( $T\leq s$ ) but it could be the other way around (hence $\times 2$ ). Now, I'm really concerned about $P(T=s)$ , surely this is zero? Your help will be much appreciated.","['statistics', 'cumulative-distribution-functions', 'probability-distributions', 'continuous-variables', 'random-variables']"
4514288,Can we approximate a real-analytic function and its derivatives by non-analytic smooth functions in the following way?,"Let $C^\omega(\Bbb R)$ denote the set of all real-analytic functions on $\Bbb R$ . I was trying the following question: $(\mathscr{Q1})$ Let $f \in C^\omega(\Bbb R) \cap L^2(\Bbb R)$ such that $f^{(2k)} \in L^2(\Bbb R) \:, \forall k \ge 1$ . Then given $\varepsilon >0$ does there exist $f_{\varepsilon} \in C^\infty(\Bbb R) \setminus C^\omega(\Bbb R)$ such that $\|f^{(2k)}- f^{(2k)}_{\varepsilon}\|_{L^2(\Bbb R)} < \varepsilon ,\:  \forall k = 0,1,2,\dots \:?$ My most naive approach was to obtain $f_{\varepsilon}$ by modifying $f$ on some set of measure $0$ , while keeping its smoothness intact. But of course, it does not work, because then $f-f_{\varepsilon}$ would be a $C^\infty$ function which is non-zero only on a set of zero measure, which is not possible! Then I thought about considering a convolution with non-analytic mollifier or approximate identity kind of argument, but I don't know how to preserve the norm closeness at the derivatives level. Is there some Sobolev density result which would be useful here? $(\mathscr{Q2})$ The higher dimensional analogue of $(\mathscr{Q1})$ with the even derivatives replaced by the iterates of the Laplacian? Thanks in advance for help!","['lp-spaces', 'derivatives', 'approximation-theory', 'real-analysis']"
4514338,Current value of option,"risk free rate= $r$ volatility of stock price= $\sigma$ continuous dividend rate= $q$ $a>0,K>0$ If your stock price S becomes below $K$ at maturity T,
the option A pays you $aS_T$ .
Otherwise, this option pays you zero. I have to prove that the curret value( $v_0$ ) of this option A is $v_0=aS_0e^{-qT}\phi(-d)$ where $d=\frac{(ln\frac{S_0}{K}+(r-q+\sigma^2/2))}{\sigma\sqrt{T}}$ and $\phi$ is the cumulative distribution function of standard normal distribution. I learned Black-Scholes formula but I can't even figure out how to start.
Any suggestions please?","['economics', 'statistics', 'finance', 'normal-distribution']"
4514354,Equivalent definitions of a group acting on a group?,"I've always seen this definition of a group $G$ acting on a set $\Omega$ , making this latter a $G$ -set: Given a group $G$ and a set $\Omega$ we say a group acts on the set $\Omega$ when there's a function $\Omega\times G \mapsto \Omega$ such that, denoting with $\alpha^g$ the image of $(\alpha,g)$ it holds: $$\alpha^{gh}=(\alpha^g)^h, \alpha\in\Omega, g, h \in G \\ \alpha^e=\alpha, e=G_{Id}$$ In particular this definition applies also if $\Omega$ is yet another group $H$ , for instance. So far, so good. Recently, studying the group extension problem and the short exact sequence, I faced the cohomology and, trying to get a grasp out of it (even if a bit too advanced for my current group training), I came to this definition: Given two groups $G$ , $H$ and a homomorphism $\phi:G \mapsto {\rm Aut}(H)$ , we say G acts on H through $h^\sigma=h^{\phi(\sigma)}, h \in H, \sigma \in G$ I was wondering if these two definitions are equivalent and the second should be preferred to the former, when the target set is a group as well, since more coincise and using ${\rm Aut}(H)$ without the need to specify the other conditions in the former definition. Thank in advance.","['symmetric-groups', 'group-theory', 'group-actions']"
4514358,Representability of 2 dimensional $p$-adic Galois representations,"Let $G = G_{\mathbb{Q}_p}$ be the absolute Galois group of $\mathbb{Q}_p$ , and let $\overline{D}$ be a residual pseudorepresentation of $\mathbb{Z}_p[G]$ over $\mathbb{F}_p$ . Denote by $\text{Rep}^d_{\overline{D}}$ the functor which associates to a rigid analytic space $X/\text{Spm}(\mathbb{Q}_p)$ the category of locally free rank- $d$ $\mathcal{O}_X$ -modules, equipped with a  continuous linear action of $G$ , and having residual pseudorepresentation $\overline{D}$ . A famous theorem of Wang-Erickson (see here ) says that the functor $\text{Rep}^d_{\overline{D}}$ is representable as a formal algebraic stack over some universal deformation ring associated to the residual pseudorepresentation, $\overline{D}$ . I wonder if it is possible that the functor $\text{Rep}^d_{\overline{D}}$ is representable by an algebraic stack instead. Is there some reason why this shouldn't be the case? How does one go about proving such a statement? I am particularly curious about the case $d=2$ , in case it should be any easier. Finally, are there any reading references you might recommend to me so I could get a more structured exposition into this topic. Thanks in advance!","['algebraic-geometry', 'abstract-algebra', 'galois-representations', 'representable-functor']"
4514380,Product analogue of Egyptian fractions,"Background An Egyptian fraction is a finite sum of distinct unit fractions, in which each denominator is not bigger than the next one. In other words, it is a representation of $a/b$ such that $$\frac{a}{b} = \sum_{n=1}^{N} \frac{1}{c_{n}} $$ with $c_{1} \leq c_{2} \leq c_{3} \leq \dots c_{N} .$ For instance, we have $$ \frac{80}{99} = \frac{1}{2} + \frac{1}{4} + \frac{1}{18} + \frac{1}{396} .$$ I'm currently exploring the idea of an Egyptian product (*). It is a representation of a fraction $a/b$ with $b>a$ such that $$\frac{a}{b} = \prod_{k=1}^{K} \left(1-\frac{1}{a_{k}} \right) ,$$ where $a_{1} \leq a_{2} \leq a_{3} \leq \dots \leq a_{K} .$ If we take the previous fraction again as an example, we find: $$ \frac{80}{99} = \left(1-\frac{1}{9} \right) \left( 1-\frac{1}{11} \right). $$ Questions Has this notion of an Egyptian product been described in the mathematical literature before? Does every fraction $0 < \frac{a}{b} < 1 $ have an Egyptian product representation? Is the Egyptian product a new and ""interesting"" representation in the sense that they can't be reduced to finding Egyptian fraction sums (or perhaps a particular type thereof) somehow? Notes (*) Sometimes, a particular type of Egyptian fraction - the Engel expansion - is also called an Egyptian product. The concept of an Egyptian product I have in mind is different.","['products', 'number-theory', 'egyptian-fractions', 'reference-request']"
4514395,Determining if a function is uniform continuous based on information of the limit of its derivative,"I'm studying real analysis more specifically derivative functions from $\mathbb{R}$ to $\mathbb{R}$ . I'm struggling a bit between the relation of the derivative of a function and the function itself. For example let's say we have a function $$f:\mathbb{R}^+ \rightarrow \mathbb{R}$$ which is derivable and for which $\lim_{x \to \infty}f'(x) = 1$ . Can we using this info determine if the function is uniformly continuous on $\mathbb{R}$ ? I started of by noticing that for such function the $\lim_{x \to \infty}f(x) = \infty$ and also that if I can somehow show the derivative function is bounded that we can easily conclude that the function is indeed uniformly continuous. Now I don't now how to proceed with this. I could maybe say that if the derivative function is also continuous that is should be bounded? Or maybe because of the fact that we don't know if the derivative function is continuous we can't really say it is bounded and thus we can't conclude if f is uniformly continuous, but in this case and in general I can't find a counter-example. Any tips on how to furthur proceed with this problem would be greatly appreciated :)) P.S: $\mathbb{R}^+ = [0,\infty)$","['uniform-continuity', 'derivatives', 'real-analysis']"
4514418,Why does double hashing create a permutation?,"Question Let $\mathbb{N} = \{0, 1, 2, \dots \}$ , $\mathbb{N}^+ = \mathbb{N} - \{0\}$ , $m \in \mathbb{N}^+$ , $[m] := \{0, 1, \dots, m-1\}$ and define the function $f:[m]\to[m]$ as below $$f(i) = (a + i \, b) \bmod m,$$ where $a, \, b \in \mathbb{N}$ . I want to show that $\big( f(0), f(1), \dots, f(m-1) \big)$ is a permutation of $(0, 1, \dots, m-1)$ provided that $b$ and $m$ are relatively prime. Motivation In open address hash tables, hash functions are of the form $$h:U \times \{0, 1, \dots, m-1\} \to \{0, 1, \dots, m-1\}, \tag{1}$$ where $U$ is the set of all possible keys. It is required that the sequence of probes defined as $$p(k) := \big(h(k, 0), \, h(k, 1), \dots, \, h(k, m-1)\big) \tag{2}$$ generates a permutation of $(0,1,\dots,m-1)$ for every key $k \in U$ , which guarantees that all of the hash table slots will be visited in a probe sequence. One way to get around this is by double hashing. In this method, the hash function is defined as below $$h(k,i):=f(k) + i \, g(k) \bmod m, \tag{3}$$ where $f$ and $g$ are functions of the form $$f:U \to \{0, 1, \dots, m-1\}, \qquad g:U \to \{0, 1, \dots, m-1\}.$$ In order to have the permutation property, we have the following theorem. Theorem . If $g(k)$ and $m$ are relatively prime then $p(k)$ is a permutation of $(0,1,\dots,m-1)$ .","['permutations', 'number-theory', 'hash-function', 'elementary-number-theory']"
4514441,John Lee Problem 6-10,"The following is John Lee's Introduction to Smooth manifolds Problem 6-10. Suppose $F:N\to M$ is a smooth map that is transverse to an embedded submanifold $X\subset M$ , and let $W = F^{-1}(X)$ . For each $p\in W$ , show that $T_pW = (dF_p)^{-1}(T_{F(p)}X)$ . My attempt: Let $\dim M = m,\dim X = k$ . As $X$ is an embedded submanifold, each point in $X$ has a neighborhood $U\subset M$ such that $X\cap U$ is a regular level set of local defining map $\Phi:U\to\Bbb R^{m-k}$ . So for each $p\in F^{-1}(X\cap U)$ , $\ker d\Phi_{F(p)} = T_{F(p)}X$ . Since $F$ is transverse to $X$ , $dF_p(T_pN)+T_{F(p)}X = T_{F(p)}M$ . Since $W$ is an embedded submanifold of $N$ , $\color{red}{\Phi\circ F:F^{-1}(U)\to\Bbb R^{m-k}\ \text{is a local defining map of}\ W.}$ Now for each $p\in F^{-1}(X\cap U)$ , $T_pW = \ker(d\Phi_{F(p)}\circ dF_p)$ , $v\in T_pW\iff dF_p(v)\in\ker d\Phi_{F(p)} = T_{F(p)}X\iff v\in (dF_p)^{-1}(T_{F(p)}X)$ . Hence, $T_pW = (dF_p)^{-1}(T_{F(p)}X)$ . I'm not sure the highlighted red part is true since $F$ is just a smooth map. Does transversality ensure this?","['solution-verification', 'smooth-manifolds', 'differential-geometry']"
4514457,Can Peano's existence theorem be proved by the implicit function theorem?,"In The implicit function theorem:history,theory and applications written by Krantz & Parks, it's said that the implicit function theorem can prove the following existence theorem of ODE: Theorem 4.1.1 If $F(t,x), (t,x) \in R×R^N$ , is continuous in the $(N+1)$ -dimensional region $(t_0−a,t_0+a)×B(x_0,r)$ , then there exists a solution $x(t)$ of $$\frac{\mathrm{d} x}{\mathrm{d} t} =F(t,x),x(t_0)=x_0$$ defined over an interval $(t_0−h,t_0+h)$ . which is also called Peano existence theorem.
The proof in the book uses this version of the implicit function theorem： Theorem 3.4.10 Let $X,Y,Z$ be Banach spaces. Let $U \times V$ be an open subset of $X \times Y$ . Suppose that $G:U \times V \to Z$ is continuous and has the property that $d_2 G$ exists and is continuous at each point of $U \times V$ . Assume that the point $(x, y) \in X \times Y$ has the property that $G(x,y)=0$ and that $d_2G(x,y)$ is invertible. Then there are open balls $M=B_X(x, r)$ and $N=B_Y(y,s)$ such that,for each $\zeta \in M$ , there is a unique $\eta \in N$ satisfying $G(\zeta,\eta)=0$ . The function $f$ , thereby uniquely defined near $x$ by the condition $f(\zeta)=\eta$ , is continuous. The proof process in the book is as follows： The proof process in the book For the proof given in the book, I am puzzled: if the implicit function given by Theorem 3.4.10 is unique, why is the function $x(t)$ finally obtained in this proof not necessarily unique?
My questions are as follows: Is the proof given in the book correct? If the proof in the book is wrong, is there a method to prove Peano's existence theorem by using the implicit function theorem? I have this doubt because in some papers the authors claim that some version of the implicit function theorem can be used to prove Peano's existence theorem, but I looked at their references and found nothing. The most relevant literature I can find on the Internet is this book. I also didn't find convincing results in the Q&A on MSE. Related link: Does the implicit function theorem imply Peano existence theorem Using the Peano existence theorem (ODE's) to imply the implicit function theorem About the second link: In Hale's Ordinary Differential Equations (1980 version) that I can find, Peano's existence theorem is proved by Schauder fixed point theorem instead of the implicit function theorem.","['ordinary-differential-equations', 'inverse-function-theorem', 'analysis', 'real-analysis', 'implicit-function-theorem']"
4514468,"How did Euler obtain this ratio of infinite series from a continued fraction, the terms of the series not being equal to the convergents?","If one feels disinclined to read the contextual preamble, I have made some partial progress in clarifying the question. Skip to the bottom! The motivation behind this is to understand the derivation for the simple continued fractions of $\tanh(1/k)$ , $e^{2/k}$ : Euler’s work has been the only source I could find which attempts to explain the results. One may find here a translation of one of Euler's essays on continued fractions. Therein, he claims to prove some celebrated continued fraction formulae (with the proof starting in the final sections of the paper). I am finding his work very hard to follow, not from the complexity of the mathematics but more from his very terse style of writing! I am talking about section 31. For some $a,n$ he considers: $$s=a+\frac{1}{(1+n)a+\frac{1}{(1+2n)a+\frac{1}{(1+3n)a+\cdots}}}$$ And builds a series of approximations using his formula in section $10,14$ : $$a,\frac{(1+n)a^2+1}{(1+n)a},\frac{(1+n)(1+2n)a^3+(2+2n)a}{(1+n)(1+2n)a^2+1},\text{etc.}$$ Which are the first few convergents. Now, in section $32$ he continues: ""If these fractions are continued further the law by which they are formed will be easily observed. From this law it may be concluded that after both the numerator and the denominator are divided by the first term of the denominator, the limiting fraction will be:"" $$\frac{a+\frac{1}{1\cdot na}+\frac{1}{1\cdot2\cdot1(1+n)n^2a^3}+\frac{1}{1\cdot2\cdot3\cdot1(1+n)(1+2n)n^3a^5}+\text{etc.}}{1+\frac{1}{1(1+n)na^2}+\frac{1}{1\cdot2(1+n)(1+2n)n^2a^4}+\frac{1}{1\cdot2\cdot3(1+n)(1+2n)(1+3n)n^3a^6}+\text{etc.}}$$ Now, he's not wrong - the numerics check out to an excellent accuracy. However I am really confused about where this fraction comes from; no multiplication of terms in the numerator or denominator comes to mind that reconciles that with the sequence of convergents. Moreover, the ratio of truncated series does not exactly equal any of the convergents, so not only do I need to guess the manipulation, but the manipulation leaves you with some kind of vanishing error term. In particular, what he intended the ""first term"" of the denominator to be, I really don't know - that seems the crux of the matter. I calculated the above ratio, truncated at the $\cdots$ , and compared it to the fourth convergent: $$\frac{\left(1+3n\right)a+3\left(1+2n\right)\left(1+3n\right)na^{3}+6\left(1+n\right)\left(1+2n\right)\left(1+3n\right)n^{2}a^{5}+6\left(1+n\right)\left(1+2n\right)\left(1+3n\right)n^{3}a^{7}}{6\left(1+n\right)\left(1+2n\right)\left(1+3n\right)n^{3}a^{6}+6\left(1+2n\right)\left(1+3n\right)n^{2}a^{4}+3\left(1+3n\right)na^{2}+1}$$ Versus the convergent: $$\frac{(1+n)(1+2n)(1+3n)a^4+3(1+n)(1+2n)a^2+1}{(1+n)(1+2n)(1+3n)a^3+2(1+2n)a}$$ But I see no clear link. I would really appreciate it if anyone could comment suggestions or full answers as to what link Euler intended, to obtain the ratio of infinite series. EDIT: some progress: I discovered through a pattern recognition that the numerators of the convergents $(p_k)_{k=1}^\infty$ , indexing from $p_1:=a$ , satisfy: $$p_k=\sum_{m=0}^{\lfloor k/2\rfloor}\binom{k-m}{m}\left(\prod_{\nu=m}^{k-m-1}(1+n\nu)\right)a^{k-2m}$$ I also find that the denominators $q_k$ , indexing with $q_1=1$ , satisfy ( $k>1$ ): $$q_k=\sum_{m=0}^{\lfloor(k-1)/2\rfloor}\binom{k-m-1}{m}\left(\prod_{\nu=m+1}^{k-m-1}(1+n\nu)\right)a^{k-2m-1}$$ But I haven’t proved it yet, this is just by a comparison. Hopefully this will help determine the generality of Euler’s expressions (he leaves much hidden behind “ $\text{etc.}$ ”!). The challenge made concrete : prove that, for any positive integer $a,n$ : $$\lim_{k\to\infty}\frac{\sum_{m=0}^{\lfloor k/2\rfloor}\binom{k-m}{m}\left(\prod_{\nu=m}^{k-m-1}\right)a^{k-2m}}{\sum_{m=0}^{\lfloor(k-1)/2\rfloor}\binom{k-m-1}{m}\left(\prod_{\nu=m+1}^{k-m-1}\right)a^{k-2m-1}}=\lim_{N\to\infty}\frac{a+\sum_{m=1}^N\frac{a^{1-2m}}{n^mm!\prod_{\nu=0}^{m-1}(1+n\nu)}}{\sum_{m=0}^N\frac{a^{-2m}}{n^mm!\prod_{\nu=0}^m(1+n\nu)}}$$ Which can, in some mysterious way, be made obvious, according to Euler. The curious thing - the two expressions are not equal at any partial $k$ or $N$ . As mentioned in the preamble, Euler must have employed something analogous to “creative telescoping”, as some on this site would say, to get a simpler expression modulo a vanishing error term.","['number-theory', 'real-analysis', 'sequences-and-series', 'limits', 'continued-fractions']"
4514479,Show that $\cos^2\left(x\right)\left(2+\cos 2x\right)-2 \cos^2\left(x+\frac{\sin(2x)}{4}\right)\ge0$,"Show that for all $x\in\mathbb{R},$ \begin{equation}
\cos^2\left(x\right)\left(2+\cos 2x\right)-2 \cos^2\left(x+\frac{\sin 2x}{4}\right)\ge0
\end{equation} I have checked this numerically and this is correct but I cannot show analytically. I have used the following useful formulas but had no luck: \begin{gather}
\sin(2x)=2\sin (x)\cos (x),\newline
\cos (2x)=2\cos^2(x)-1,\newline
\sin^2(x)=1-\cos^2(x).
\end{gather} I guess there is some bounding trick that I am missing. Any help is appreciated",['trigonometry']
4514529,Plot and study of a function numerically challenging,"my question is simple and I'm a little bit lost how to proceed. I explain : Let $f : x \mapsto 1 - \tanh\left(512x^3\right)$ , I'd like to study $$
g\left(x\right) = \frac{f\left(2.5x\right)}{f\left(x\right)}
$$ For example, for $x=0.8$ , I would like to know the value of $f\left(0.8\right)$ , $f\left(2\right)$ and overall, $g\left(0.8\right)=\frac{f\left(2\right)}{f\left(0.8\right)}$ . However every software I tried give me $f\left(0.8\right)=f\left(2\right)=0$ and then the ratio does not exist. I'd like to know if there is a simple way to plot $g$ , or at least see what range of values this function will take ? Thanks in advance","['functions', 'graphing-functions']"
4514537,"$f(\frac{a+b}{2})=\frac{f(a)}{2}+\frac{f(b)}{2}, \forall a,b\in\mathbb{R}$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I am required to show that if $$f\left(\frac{a+b}{2}\right)=\frac{f(a)}{2}+\frac{f(b)}{2}, \forall a,b\in\mathbb{R},$$ then $$f\left(\frac{a_1+a_2+\cdots+a_n}{n}\right)=\frac{f(a_1)}{n}+\frac{f(a_2)}{n}+\cdots+\frac{f(a_n)}{n}, \forall a_i\in\mathbb{R}$$ I tried to use PMI, but I could not get anywhere.","['means', 'functions']"
4514538,Simplifying the double summation $\sum_{m=0}^{\infty}\sum_{n=0}^{\infty} \alpha \frac{2m+n}{m!n!} (p)^{m+n} =1$ to find alpha,"Background In studying probability, I came across a question asking to find a constant of proportionality alpha that determines a discrete joint probability distribution. The exercise has a solution, however, there is little explanation provided which has made it quite hard to follow. I’ve detailed my progress and also what parts I am stuck with below. The Question Find the value of $\alpha > 0$ such that, for all $m,n \in \{0,1,2,3$ … $\}$ : $$P(X=m, Y=n) = \alpha \frac{2m+n}{m!n!} (p)^{m+n}$$ My Attempt It is clear that if we sum over all values of $m$ and $n$ , then this should be equal to $1$ by the properties of the joint density density function - i.e. $$\sum_{m=0}^{\infty}\sum_{n=0}^{\infty} \alpha \frac{2m+n}{m!n!} (p)^{m+n} =1$$ By splitting up the fraction, we see that this is equivalent to: $$\sum_{m=0}^{\infty}\sum_{n=0}^{\infty} \frac{2m}{m!n!} (p)^{m+n} + \frac{n}{m!n!} (p)^{m+n} =\frac{1}{\alpha}$$ However, I am unsure of how to proceed from here. The model solution says that this can be rewritten as: $$3\sum_{m=0}^{\infty}\sum_{n=0}^{\infty} \alpha \frac{1}{m!n!} (p)^{m+1+n} =1$$ It’s both parts of this line that are unclear to me. I don’t understand how the expression on the left hand side is arrived at. And I also don’t see how this simplifies to give us the required result to solve for alpha. I would be grateful for any help in understanding this.","['statistics', 'summation', 'probability-distributions', 'discrete-mathematics', 'probability']"
4514546,The espace étalé as a scheme,"On Wikipedia , we find the following remark without reference: (here $\pi:E \to X$ is the espace étalé of a (pre)sheaf on $X$ ) It is possible to turn $E$ into a scheme and $\pi$ into a morphism of schemes in such a way that $\pi$ retains the same universal property, but $\pi$ is not in general an étale morphism because it is not quasi-finite. It is, however, formally étale. So I'm wondering: what is the scheme structure on $E$ ? Do you we need some assumptions on the sheaf, such as quasicoherence?","['algebraic-geometry', 'schemes', 'sheaf-theory']"
4514547,Understanding Differential Operator?,"$\frac{d}{dx}$ is an operator used often in calculus, but I don't understand this use of notation, for example I saw recently in a problem an expression's derivative given as $\frac{d}{dx}(f(x)$ , I'm confused about this, as $f(x)$ refers to a number, and represented as an expression, we cannot have $\frac{d}{dx}$ for example is $\frac{d}{dx}$ acting on the number, or the expression, I understand it mapping between a function $f$ and a function $f'$ but $f(x)$ is not a function but a value. We even see something where we use $\frac{d}{dx}$ without even specifying the function, just as $\frac{d}{dx}(e^x)$ . Do we see it as 'the operator on the function defined by the expression'? If we cannot substitute into $x$ when an expression is enclosed by the differential operator, we get $\frac{d}{dx}$ (a) which makes no sense, so is $x$ bound in this expression and how does this work considering the expression $\frac{d}{dx}f(x)$ depends on the value $x$ which is not how bound variables work.","['notation', 'calculus', 'derivatives']"
4514561,What does it mean for the hexagon to be efficiently packing space?,"The general claim goes something like this the best regular polygon that tiles the 2D (Euclidean?) plane with equal size units and leaves no wasted space is the hexagon I have seen similar claims like this but I do not get what I have to do to have an idea of what ""best"" means here. I would have thought that I have to make some kind of circle, tile it with hexagons and compare it with another circle of the same size tiled with squares. But should I use squares of the same area as the hexagons? The same radius/side lenght? What should I count? number of polygons? I could ask questions many more questions. Should I find the limit for large areas of the circle or should I verify it for other finite spaces. I have sometimes seen that it has to do with the perimeter of the tiling, in what way? The closest I could find is the honeycomb conjecture Any partition of the plane into regions of equal area has perimeter at least that of the regular hexagonal grid but I would like to still understand what partition and perimeter mean here. Could somebody provide an  example on how hexagons are better than squares?","['tiling', 'geometry', 'polygons']"
4514562,Uniform random variables generated by sums of independent Bernoulli random variables. Are they independent?,"For $i\in \mathbb{N}$ , let $\epsilon_i : \Omega \rightarrow \{0, 1\}$ be a Bernoulli random variable and suppose that this sequence is i.i.d. Let $\phi : \mathbb{N}^2 \rightarrow \mathbb{N}$ be a bijection, and define \begin{align}
Y_i = \sum_j \epsilon_{\phi(i,j)} 2^{-j}. 
\end{align} I want to show that the sequence $(Y_i)_{i=1}^\infty$ is independent. My attempt:
For each $i$ , the sequence of $\sigma$ -algebras $\{\sigma(\epsilon_{\phi(i,j)}) : j \in \mathbb{N}\}$ is independent.
Since $Y_i$ is $\sigma(\epsilon_{\phi(i,j)} : j \in \mathbb{N})$ measurable, it follows that the sequence $(Y_i)_{i=1}^\infty$ is necessarily independent.","['measure-theory', 'independence', 'probability-theory', 'random-variables']"
4514583,Show that $4\sin^2(24^{\circ})+4\sin(24^{\circ})\sin(12^{\circ}) = 1$.,"The problem asks us to show that the following equation holds true. $$4\sin^2(24^{\circ})+4\sin(24^{\circ})\sin(12^{\circ}) = 1$$ This equation has been verified on my calculator. Perhaps some basic trigonometric formulas will be enough to solve the problem. I've tried the following: $$\begin{align} 4\sin^2(24^{\circ})+4\sin(24^{\circ})\sin(12^{\circ})&=16\sin^2(12^{\circ})\cos^2(12^{\circ})+8\sin^2(12^{\circ})
\cos(12^{\circ})\\
\\
&=8\sin^2(12^{\circ})\cos(12^{\circ})\Big(2\cos(12^{\circ} ) + 1\Big)\end{align}$$ As you can see, I was trying to simplify the expression so that it'll contain only $\sin(12^{\circ})$ and $\cos(12^{\circ})$ , since I thought by unifying the angles I would have a bigger chance of solving it. However, I couldn't find a way to make any further progress. Can someone show me the way?",['trigonometry']
4514668,"Find the sum of radicals without squaring, Is that impossible?","Find the summation: $$\sqrt {3-\sqrt 5}+\sqrt {3+\sqrt 5}$$ My attempts: \begin{align*}
&A = \sqrt{3-\sqrt{5}}+ \sqrt{3+\sqrt{5}}\\
\implies &A^2 = 3-\sqrt{5} + 3 + \sqrt{5} + 2\sqrt{9-5}\\
\implies &A^2 = 6+4 = 10\\
\implies &A = \sqrt{10}
\end{align*} So I was wondering about a way to find this sum without squaring?  It seems impossible, but I still want to ask.","['contest-math', 'algebra-precalculus', 'radicals']"
4514674,"Show that $\int_A f=\underline{\int_Q} (f_{+})_A - \underline{\int_Q} (f_{-})_A.$ Is my solution ok or not? (in ""Analysis on Manifolds"" by Munkres)","I am reading ""Analysis on Manifolds"" by James R. Munkres. The following exercise is Exercise 7 on p.133 in Section 15 in this book: Exercise 7. Let $A$ be a bounded open set in $\mathbb{R}^n$ ; let $f:A\to\mathbb{R}$ be a bounded continuous function. Let $Q$ be a rectangle containing $A$ . Show that $$\int_A f=\underline{\int_Q} (f_{+})_A - \underline{\int_Q} (f_{-})_A.$$ I solved this exercise, but I am not sure my solution is ok or not. The following is the definition of the extended integral of $f$ over $A$ . Definition. Let $A$ be an open set in $\mathbb{R}^n$ ; let $f:A\to\mathbb{R}$ be a continuous function. If $f$ is non-negative on $A$ , we define the (extended) integral of $f$ over $A$ , denoted $\int_A f$ , to be the supremum of the numbers $\int_D f$ , as $D$ ranges over all compact rectifiable subsets of $A$ , provided this supremum exists. In this case, we say that $f$ is integrable over $A$ (in the extended sense). More generally, if $f$ is an arbitrary continuous function on $A$ , set $$f_{+}(x)=\max\{f(x),0\}\text{ and }f_{-}(x)=\max\{-f(x),0\}.$$ We say that $f$ is integrable over $A$ (in the extended sense) if both $f_{+}$ and $f_{-}$ are; and in this case we set $$\int_A f=\int_A f_{+}-\int_A f_{-},$$ where $\int_A$ denotes the extended integral throughout. The author made the following convention: Convention. If $A$ is an open set in $\mathbb{R}^n$ , then $\int_A f$ will denote the extended integral unless specifically stated otherwise. So, $\int_A f$ in Exercise 7 is an extended integral (an improper integral). $\underline{\int_Q} (f_{+})_A$ and $\underline{\int_Q} (f_{-})_A$ are ordinary lower integrals. The author proved the following theorem on p.127 in Section 15: Theorem 15.4. Let $A$ be a bounded open set in $\mathbb{R}^n$ ; let $f:A\to\mathbb{R}$ be a bounded continuous function. Then the extended integral $\int_A f$ exists. If the ordinary integral $\int_A f$ also exists, then these two integrals are equal. My solution is here: By Theorem 15.4, $\int_A f$ exists in the extended sense. Since $(f_{+})_A$ and $(f_{-})_A$ are bounded, $\underline{\int_Q} (f_{+})_A$ and $\underline{\int_Q} (f_{-})_A$ exist. (in the ordinary sense.) By Theorem 15.4, $\int_A f$ exists. So by the definition of the extended integral of $f$ over $A$ , $\int_A f_{+}$ and $\int_A f_{-}$ exist and $$\int_A f=\int_A f_{+}-\int_A f_{-}$$ holds. We prove that $\int_A f_{+}=\underline{\int_Q} (f_{+})_A$ and $\int_A f_{-}=\underline{\int_Q} (f_{-})_A$ hold. To show $\int_A f_{+}=\underline{\int_Q} (f_{+})_A$ and $\int_A f_{-}=\underline{\int_Q} (f_{-})_A$ hold, we prove the following holds: If $f$ is a non-negative bounded continuous function on $A$ , $\int_A f=\underline{\int_Q} f_A$ holds. Let $P$ be a partition of $Q$ . \begin{align*}
L(f_A,P)
&=\sum_R m_R(f_A)v(R)\\
&=\sum_{R\subset A} m_R(f_A)v(R)\\
&\leq\sum_{R\subset A} \int_R f_A\\
&=\sum_{R\subset A} \int_R f\\
&=\int_{\cup\{R\mid R\subset A\}} f\\
&\leq \int_A f.
\end{align*} Therefore, $\underline{\int_Q} f_A\leq\int_A f.$ Let $D$ be a compact rectifiable subsets of $A$ . \begin{align*}
L(f_D,P)
&=\sum_R m_R(f_D)v(R)\\
&=\sum_{R\subset D} m_R(f_D)v(R)\\
&=\sum_{R\subset D} m_R(f_A)v(R)\\
&\leq\sum_{R\subset A} m_R(f_A)v(R)\\
&\leq\underline{\int_Q} f_A.
\end{align*} Therefore $\int_D f_=\int_Q f_D=\underline{\int_Q} f_D\leq\underline{\int_Q} f_A.$ Therefore $\int_A f\leq\underline{\int_Q} f_A.$","['multivariable-calculus', 'solution-verification', 'improper-integrals']"
4514676,Generalization of Karamata’s theorem to integrals,"Karamata's theorem states the following: let $f: \mathbb{R}\to \mathbb{R}$ be a convex function, $x_1\ge x_2\ge \cdots \ge x_n$ and $y_1\ge \cdots \ge y_n$ be reals such that $\sum\limits_{j=1}^n x_j=\sum\limits_{j=1}^n y_j$ and $\sum\limits_{j=1}^k x_j \ge \sum\limits_{j=1}^k y_j$ for all $k\in \{1,\cdots,n\}$ . Then $\sum\limits_{j=1}^n f(x_j) \ge \sum\limits_{j=1}^n f(y_j)$ . I am wondering if the following are true: 1.1 Let $a,b: \mathbb{R}\to \mathbb{R}$ be decreasing functions and $f$ be a convex function such that $f(a(x))$ is Riemann integrable. If $\int_0^1 a(x) dx = \int_0^1 b(x) dx$ and $\int_0^k a(x) dx \ge \int_0^k b(x) dx$ for all $0\le k\le 1$ , then $$\int_0^1 f(a(x)) dx \ge \int_0^1 f(b(x)) dx$$ 1.2 have riemann integrable changed to generally integrable (can be Lesbegue Integrable for example) 1.3 $a$ is decreasing but $b$ is not necessarily decreasing. All other conditions hold (for simplicity we will assume $f\circ a, f\circ b$ are riemann integrable. Thank you for your interest!","['integration', 'calculus', 'functions', 'inequality']"
4514703,"R is symmetric and transitive relation, if and only if $R = R^{-1}\circ R$","How do I prove that the following: Let $R$ be a (binary) relation on a set. Prove that $R$ is symmetric and transitive relation, if and only if $R = R^{-1}\circ R$ . Here, $R^{-1} \circ R$ is defined as the relation $\{(u,v) | \exists x, uR^{-1}x \land xRv\}$ , where $xRu$ means $\left(x,u\right) \in R$ . I only have a problem with assuming the 2nd, and deriving the first, any tips on the proof technique?","['elementary-set-theory', 'equivalence-relations', 'relations', 'function-and-relation-composition']"
4514750,How to find a discrete Morse function on this simplicial complex,"I have a graph called partition graph. This graph gives rise to a simplicial complex called box complex $B_{edge}$ . Since this simplicial complex is too big and studying the topological features of it is too difficult I want to simplify this simplicial complex using a discrete Morse function. Definition .
Every vertex of a partition graph $\mathcal{P}(3^3)$ is a partition of $\{1,2, ..., 9 \}$ into $3$ cells of size $3$ . Two vertices $u$ and $v$ are adjacent if the intersection of each cell of $u$ with each cell of $v$ is nonempty. This graph is vertex- edge- arc transitive. Definition . The box complex $B_{edge}(G)$ of a graph $G$ is a simplicial complex which vertices are the directed edges of $G$ ; that is, ordered pairs $(u,v)$ with $\{u,v\}\in E$ . The simplices of this box complex are subsets of edge sets of complete bipartite subgraphs of $G$ , where the edges are oriented from the first shore to the second shore. \begin{align*}
    B_{edge}(G):=&\{ \overrightarrow{F} \subset A'\times A'': \varnothing \neq A',A'' \subset V,\\
    &A'\cap A''=\varnothing, G[A',A'']\ \text{is complete}\}
\end{align*} I want to find a discrete Morse function on the box complex $B_{edge}(\mathcal{P}(3^3))$ . A function with less critical simplicies is better.","['graph-theory', 'combinatorics', 'algebraic-topology', 'morse-theory']"
