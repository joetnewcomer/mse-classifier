question_id,title,body,tags
3532633,"How to prove that when $n$ is even, and $A = (a_{ij})_{i,j}$, $1\leq i,j\leq n$ to show that $\mathrm{det} A = 1$? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How to prove that when $n$ is even, and $A = (a_{ij})_{i,j}$ , $1\leq i,j\leq n$ with $$a_{ij}=\begin{cases} 1& i<j,\\ 0 & i=j,\\ -1& i>j,\end{cases}$$ to show that $\mathrm{det} A = 1$ ?","['matrices', 'determinant', 'linear-algebra']"
3532677,Issue when applying the residue theorem,"I encounter a problem when computing the following integral in two ways $$ \frac{1}{2\pi i}\int_\gamma \frac{e^{s^2}}{s\cosh(\pi As)}ds$$ where $\gamma$ is contour defined by the rectangle $[-1,1]\times[-K,K]$ , $AK\in\mathbb{N}$ and $A>0$ . On the one hand, the poles are $0$ and $iA^{-1}(k+1/2)$ with $k$ integer with residue (respectively) $1$ and $(-1)^{k+1}i\pi^{-1} e^{-(k+1/2)A^{-1}}(k+1/2)^{-1}$ .
The real part of the integral must the be $1$ . On the other hand, one can bound directly the integral. For the vertical lines (for instance $\Re(s) = 1$ ), one gets $$\frac{1}{2\pi}\int_{-K}^K \frac{e^{(1+it)^2}}{(1+it)\cosh(\pi A(1+it))}dt\ll e^{-\pi A}$$ with the constant independant of $K$ .
For the horizontal lines, one has $$\cosh(\pi A(x+iK))^{-1}\leq 2 $$ so the integrals are bounded by $O(e^{-K^2})$ with constant independant on $A$ . In the end, after applying the residue theorem, I get $$1-i\pi^{-1}\sum_{{|k+1/2|<K}} (-1)^{k}e^{-((k+1/2)A^{-1})^2}(k+1/2)^{-1} =O(e^{-\pi A}+e^{-K^2})$$ . Now taking the real part and $K\to \infty$ (everything is converging), I get a contradiction of the type $1 \ll e^{-A}$ which is stupid since $A$ can be as big as we want. I don't seem to find my mistake and any help would be welcome. EDIT: To be clear I will write how I get the estimates for the bounds on the vertical lines. First we have $\cosh(A\pi(\pm 1+it))^{-1}< 2e^{-\pi A}$ uniformly in $t$ . This gives $$ \left|\frac{1}{2\pi}\int_{-K}^K \frac{e^{(\pm 1+it)^2}}{(\pm 1+it)\cosh(\pi A(\pm 1+it))}dt\right|< 2e^{-\pi A}\frac{1}{2\pi}\int_{-K}^K \frac{e^{1-t^2}}{\sqrt{1+t^2}}dt<Ce^{-\pi A}$$ where $$ C = \pi^{-1}\int_{-\infty}^\infty \frac{e^{1-t^2}}{\sqrt{1+t^2}}dt.$$ EDIT 2: For the horizontal lines, let us use the fact that, for $-1\leq\sigma\leq 1$ , one has $|\cosh(\pi A \sigma \pm i\pi AK)| = |\cosh(\pi A \sigma)|\geq1$ since $AK$ was chosen to be an integer. This gives $$\left|\frac{1}{2\pi i}\int_{-1}^1\frac{e^{\sigma^2-K^2}}{(\sigma\pm iK)\cosh(\pi A(\sigma\pm iK))}d\sigma\right|\leq (2\pi K)^{-1}e^{-K^2}\int_{-1}^1e^{\sigma^2}d\sigma.$$ Clearly, when $K$ gets big this goes to $0$ .","['complex-analysis', 'contour-integration', 'residue-calculus']"
3532764,What's wrong with my calculation of a limit here?,"So, I have to find the following limit: $\lim _{x\to \:0+}\left(\frac{\left(1-\cos \left(2x\right)\right)^{14}\left(1-\cos \left(7x\right)\right)^2\sin ^{14}\left(9x\right)}{\tan ^{14}\left(x\right)\left(\ln \left(8x+1\right)\right)^{30}}\right)$ I solved it by splitting it into three limits as follows: $$\begin{align}
&\lim _{x\to \:0+}\left(\frac{\left(1-\cos \left(2x\right)\right)^{14}\left(1-\cos \left(7x\right)\right)^2\sin ^{14}\left(9x\right)}{\tan ^{14}\left(x\right)\left(\ln \left(8x+1\right)\right)^{30}}\right)
\\
&=\lim _{x\to 0+}\frac{\left(1-cos\left(2x\right)\right)^{14}}{\tan ^{14}\left(x\right)}\cdot \frac{\left(1-cos\left(7x\right)\right)^2}{\left(\ln \left(8x+1\right)\right)^2}\cdot \frac{sin\left(9x\right)^{14}}{\left(\ln \left(8x+1\right)\right)^{28}}
\\
&=\lim_{x\to 0+}\frac{\left(1-cos\left(2x\right)\right)^{14}}{\tan ^{14}\left(x\right)}\cdot \lim_{x\to\:0+}\frac{\left(1-cos\left(7x\right)\right)^2}{\left(\ln\left(8x+1\right)\right)^2}\cdot\lim_{x\to 0+}\frac{sin\left(9x\right)^{14}}{\left(\ln\left(8x+1\right)\right)^{28}}
\\
&=\left(\lim _{x\to \:\:0+}\frac{\left(1-cos\left(2x\right)\right)}{\tan \left(x\right)}\right)^{14}\cdot \left(\lim _{x\to 0+}\frac{\left(1-cos\left(7x\right)\right)}{\:\ln \left(8x+1\right)}\right)^2\cdot \left(\lim _{x\to 0+}\frac{sin\left(9x\right)}{\left(\ln\left(8x+1\right)\right)^2}\right)^{14}
\end{align}$$ Using L'Hospital's rule to solve these separate limits, you get $$\left(\lim _{x\to \:\:0+}\frac{\left(1-cos\left(2x\right)\right)}{\tan \left(x\right)}\right)^{14}\cdot \left(\lim _{x\to 0+}\frac{\left(1-cos\left(7x\right)\right)}{\:\ln \left(8x+1\right)}\right)^2\cdot \left(\lim _{x\to 0+}\frac{sin\left(9x\right)}{\left(\ln\left(8x+1\right)\right)^2}\right)^{14}
\\
=0^{14}\cdot 0^2\cdot \left(-\frac{9}{16}\right)^{14}
\\
=0$$ However, the automated homework system did not accept 0 as the correct answer. What did I do wrong?","['limits', 'calculus', 'solution-verification']"
3532789,Are groups with this property already studied?,"Let $\Omega$ be a finite group, let $G$ be a subgroup of $\Omega$ and let $S$ be a set of subgroups of $\Omega$ such that for $H, H'\in S$ we have $H\cap H' \in S$ and $\langle H, H' \rangle \in S$ . One can show that if $GH = GH'$ then also $GH = G\langle H, H' \rangle$ . So given $H \in S$ we can find $H_{\max}\in S$ such that $G H = G H_{\max}$ and $H_{\max}$ is maximal with that property. We say $H_{\max}$ is $G$ -maximal in $S$ . One can also show that if $G\cap H = G\cap H'$ then also $G\cap H = G\cap (H\cap H')$ . So given $H\in S$ we can find $H_{\min} \in S$ such that $G\cap H = G\cap H_{\min}$ and $H_{\min}$ is minimal with that property. We say $H_{\min}$ is $G$ -minimal in $S$ . In the context of my work I'm interested in groups in $S$ that are $G$ -minimal and $G$ -maximal at the same time. One can show that they always exist. So my question is: What properties do such groups have and is there any literature for this? An example would be if $\Omega= S_4$ , $G = D_8$ and $S$ are all Young-subgroups in $S_4$ . Then the groups that are $D_8$ -maximal and $D_8$ -minimal in $S$ are eaxactly the $S_4$ and all $V_4$ 's in $S$ .","['group-theory', 'finite-groups']"
3532841,"ordered pair of $(p,q)$ in $20!$","A rational number given in the form $\displaystyle \frac{p}{q},\;\;p,q\in \mathbb{Z}^{+}\;,\frac{p}{q}\in(0,1)$ and $p,q$ are coprime to each other.If $pq=20!.$ Then number of ordered pair of $p,q$ are what i try $20!=2^{18}\cdot 3^{8}\cdot 5^4\cdot 7^2\cdot 11\cdot 13\cdot 17\cdot 19$ Let $p=2^{a_{1}}\cdot 3^{a_{2}}\cdot 5^{a_{3}}\cdot 7^{a_{4}}\cdot 11^{a_{5}}\cdot 13^{a_{6}}\cdot 17^{a_{7}}\cdot 19^{a_{8}}$ $q=2^{b_{1}}\cdot 3^{b_{2}}\cdot 5^{b_{3}}\cdot 7^{b_{4}}\cdot 11^{b_{5}}\cdot 13^{b_{6}}\cdot 17^{b_{7}}\cdot 19^{b_{8}}$ $0\leq a_{1}\leq 18,0\leq a_{2}\leq 8,0\leq a_{3}\leq 4,0\leq a_{4}\leq 2,0\leq a_{5}\leq 1$ $0\leq a_{6}\leq 1,0\leq a_{7}\leq 1,0\leq a_{8}\leq 1$ and $0\leq b_{1}\leq 18,0\leq b_{2}\leq 8,0\leq b_{3}\leq 4,0\leq b_{4}\leq 2,0\leq b_{5}\leq 1$ $0\leq b_{6}\leq 1,0\leq b_{7}\leq 1,0\leq b_{8}\leq 1$ How do i solve it Help me please","['elementary-number-theory', 'combinatorics']"
3532857,How to upper bound of $\int_{I} \frac{1} {(1+|x|^2)^{d/2}} dx$?,"Formally, we  know  that $\int \frac{1}{(1+x^2)^{1/2}} dx = \log (x+ \sqrt{1+x^2}) + C$ some constant $C$ . Let $B>0$ and $I=[-B, B)^d \subset \mathbb R^d.$ My Question is : How to compute $I_1:=\int_{I} \frac{1} {(1+|x|^2)^{d/2}} dx$ ? Can we say that $I_1 \leq C_1 (\log 2B)$ ? $C_1$ is some constant. My attempt : $I_1= C_2 \int_0^{B} \frac{1}{(1+r^2)^{d/2}} r^{d-1} dr$ (please correct me if I'm wrong here). Now I might need to invoke now one dimensional formula? Edit : $\int \frac{1}{1+x^2} dx= \arctan x +c$ (this might need to the case $d=2$ )","['integration', 'multivariable-calculus', 'analysis', 'real-analysis']"
3532862,"Do Voronoi cells ""converge"" towards their centroid?","If you do the following: Place random points on a finite surface Draw the Voronoi diagram using the points as germs Calculate the centroid of each cell Repeat step 2 and 3 indefinitely using the centroids obtained in step 3 as germs. How does that look like? Does it converge to a diagram where all the germs are in the centroid of their cell? I suspect there's a convergence most of the times, but is there a set of starting points where it doesn't converge? Secondary question: Does the fact that the surface is finite change anything? I suspect it does, considering borders affects the position of the centroid around the edge (so after a few iterations, it affects all cells) but not the Voronoi cell.","['graph-theory', 'voronoi-diagram', 'geometry']"
3532877,"Solve $\int_0^1\ln^2\Gamma(x)\,\mathrm{d}x$","I want to solve the following integral but after some work I didn't find a way to go. Could anyone give me a hint? \begin{equation}
I=\int_{0}^{1}\ln^2\Gamma(x)\,\mathrm{d}x
\end{equation} The answer is \begin{equation}
I=\frac{\ln^2 (2\pi)}{3}+\frac{\pi^2}{48}+\frac{\gamma\ln(2\pi)}{6}+\frac{\gamma^2}{12}+\frac{\zeta''(2)}{2\pi^2}-\frac{\zeta'(2)\ln (2\pi)}{\pi^2}-\frac{\gamma\zeta'(2)}{\pi^2}
\end{equation} They only give a hint (using the Fourier Series) which I looked up at https://de.wikipedia.org/wiki/Gammafunktion . \begin{equation}
\ln\Gamma(x) = \left(\tfrac{1}{2}-x\right) \bigl(\gamma + \ln(2\pi)\bigr) + \frac{1}{2} \ln\frac{\pi}{\sin(\pi x)} + \frac{1}{\pi} \sum_{k=2}^\infty \frac{\ln k}{k} \sin(2\pi k x)
\end{equation} Want I have tried so far: squared the series integration by parts and the the fourier series","['integration', 'calculus', 'definite-integrals']"
3532895,Differential of Riemannian exponential map,"Let $(M^n,g)$ be a Riemannian $n$ -manifold. Let $p\in M$ , and let $v\in T_pM$ . By the existence and uniqueness theorem (of ODEs, hence of geodesics), there is a unique geodesic $\gamma$ on $M$ such that $\gamma(0)=p$ and $\gamma'(0)=v$ . The exponential map at $p$ , denoted by $exp_p:T_pM\to M$ , is then defined by \begin{align}
exp_p(v)=\gamma_v(1)
\end{align} It is standard that the differential of $exp_p$ at $0\in T_pM$ is the identity map on $T_pM$ , after identifying $T_0(T_pM)\simeq T_pM$ . In this question I'm interested in the following map $\mathcal{E}:TM\to M$ defined by \begin{align}
\mathcal{E}(p,v):=exp_p(v)
\end{align} for any $p\in M$ and any $v\in T_pM$ . This is sort of a 'global' version of exponential map, and I'm interested in: Compute the differential of $\mathcal{E}$ at any point $(p,v)\in TM$ . If $\{x^i\}_{i=1}^n$ are local coordinates of $p$ and $\{a^i\}_{i=1}^n$ are local coordinates of $v$ : \begin{align}
v=a^i\frac{\partial}{\partial x^i}\bigg|_p
\end{align} then one of the greatest difficulty I have is that I have no idea how to compute the partial derivatives $\frac{\partial E}{\partial x^i}$ since I have no idea how $exp_p$ depends on $p$ . Actually I'm also not quite sure about the other part: the partial derivatives $\frac{\partial E}{\partial a^i}$ . I suspect Jacobi fields may come in handy but I'm not sure how. Any comment, hint or answer is greatly welcomed and appreciated.","['differential', 'riemannian-geometry', 'differential-geometry']"
3532935,Classify all $3$-manifolds such that this map is injective,"Is it possible to classify all the compact, connected and orientable $3$ -manifolds $M$ with nonempty boundary such that the map $H_2(M, \partial M) \to H_1(\partial M)$ , appearing in the long exact sequence of the pair $(M, \partial M)$ , is injective? Obviously, a sufficient condition is that $H_2(M) = 0$ , but can we describe the manifolds in the general case?","['smooth-manifolds', 'algebraic-topology', 'differential-geometry']"
3533061,Operators that improve strong convergence to norm convergence?,"It is well known that if $K$ is a compact operator on a Hilbert space, and $T_n$ is a sequence of operators converging strongly to $T$ , then $K T_n$ converges in norm to $K T$ . Question : are there other operators $K$ that also satisfy this property, but are not compact ? Thanks","['hilbert-spaces', 'operator-theory', 'compact-operators', 'functional-analysis']"
3533074,Finding the ring of integers of a cubic extension of $\mathbb{Q}$,"Let $f(x)=x^3-9x-6$ with $a\in \mathbb{C}$ a root of $f(x)$ . I need to show that the ring of integers of $L=\mathbb{Q}(a)$ is $O_L=\mathbb{Z}[a]$ . I computed the discriminant of $f(x)$ to be 1944. Even by Stickelberger's theorem, I narrowed down $(O_L:\mathbb{Z}[a])$ to 1 or 3. I need to show it must be 1. Any ideas how to proceed? Thanks.","['number-theory', 'ring-theory', 'abstract-algebra', 'algebraic-number-theory']"
3533104,"Solve a system of equation: $\cos(2x) + \cos(y) = 1$, $\sin(2x) + \sin(y) = 1$",Solve a system of equation: $$\cos(2x) + \cos(y) = 1$$ $$\sin(2x) + \sin(y) = 1$$ My idea: Let's see what is product of this two equations. $$\cos(2x)\sin(2x) + \cos(2x)\sin(y) + \cos(y)\sin(2x) + \cos(y)\sin(y) = 1$$ $$\cos(2x)\sin(2x) + \sin(2x+y) + \cos(y)\sin(y) = 1$$ But this idea didn't give me anything. Also if I sum I have problem... but this is high school problem so it must have some easy solution.,"['systems-of-equations', 'trigonometry', 'functions']"
3533167,Wronskian of $x|x|$ and $x^2$.,"Wikipedia says wronskian of $x|x|$ and $x^2$ is identically zero.
But it is not LD. I know why these two are LI and not LD. since x|x| is not differentiable function,how to find their wronskian????
 And plz suggest ways to check LI and LD when functions are not differentiable. Thanks in advance.
Plz help.","['calculus', 'wronskian', 'ordinary-differential-equations']"
3533211,Generalization for L'Hôpital's rule and Stolz-Cesàro theorem?,"Due to the similarity of L'Hôpital's rule and Stolz-Cesàro theorem I guess that it must exists a generalized theorem where each of these two theorems are special cases. Note that in the setting of finite calculus the $\Delta $ operator is the inverse of the $\sum$ operator, that is, if $f:\Bbb N \to \Bbb R $ then $\Delta  f$ can be seen as the finite calculus derivative of $f$ and $\sum_0^n f$ as a finite calculus primitive of $f$ . Indeed in a measure-theoretic context one can see $f$ as the Radon-Nikodym derivative of $f \,\mathrm d \mu $ where $\mu $ is the counting measure in $\Bbb N $ (and $\Delta f$ can be seen as some Radon-Nikodym derivative of the measure $\Delta f \,\mathrm d \mu $ respect to the counting measure $\mu $ ). So suppose we have an operator $T$ , what condition we will need on $T$ (or the space where it is defined) to have a version of the Stolz-Cesàro theorem? I mean, when it holds that $$
\lim_{n\to \infty }\frac{Tf_n}{Tg_n}=h \implies \lim_{n\to \infty }\frac{f_n}{g_n}=h
$$ assuming that $(g_n)$ is eventually not zero or that it diverges to infinity, and that the RHS of above is an indeterminate form of the kind $0/0$ or $\infty/\infty$ ? There is some theorem about that, or some paper or bibliography?","['operator-theory', 'soft-question', 'analysis', 'reference-request']"
3533219,Show that $N_{t}:=\sup\{n:S_{n}\leq t\}$ is a stochastic process with time $\mathbb{R}_{\geq 0}$.,"I am working on an exercise about stochastic process as follows: Let $(X_{n})_{n\in\mathbb{N}}$ be an i.i.d positive sequence. Define $S_{n}:=X_{1}+\cdots+X_{n}$ , and $S_{0}=0$ . Further, define $N_{t}:=\sup\{n:S_{n}\leq t\}$ for $t\geq 0$ . Prove that $S_{t}:=S_{[t]}$ and $(N_{t})_{t\in\mathbb{R}_{\geq 0}}$ are stochastic process. I have showed that $S_{t}$ is a stochastic process with time $\mathbb{T}:=\mathbb{R}_{\geq 0}$ by showing $S_{n}$ is a random variable for all $n\in \mathbb{Z}_{\geq 0}$ . This is basically showing that if $X_{1}\cdots X_{n}$ are random variables, then so is $S_{n}$ . In the case of $n=0$ , we just have a constant which is a random variable. I have a proof for $(N_{t})_{t\in\mathbb{R}_{\geq 0}}$ being a stochastic process, but I don't know if it is right. Let's denote the probability space as $(\Omega,\mathcal{F},\mathbb{P})$ . Below is the proof: To show $(N_{t})_{t\in\mathbb{R}\geq 0}$ is a stochastic process, we only need to show that $N_{t}$ is a random variable for all $t\in \mathbb{R}_{\geq 0}$ . But this follows from $$\{N_{t}< n\}=\{S_{n}>t\}\in\mathcal{F},$$ since we have showed that $S_{n}$ is a random variable for each $n\in\mathbb{Z}_{\geq 0}$ . I have doubt in my proof because: It seems that showing $\{N_{t}< n\}\in\mathcal{F}$ for all non-negative integers $n$ is not sufficient for $N_{t}$ being a random variable. I think I need to show $\{N_{t}\leq r\}\in\mathcal{F}$ for all $r\in\mathbb{R}$ . It seems that I only showed $N_{t}$ is a stopping time (if it is random) What should I do? Thank you! Edit 1: (Complete Proof) Okay I think I figured it out. Here is my proof: To show $(N_{t})_{t\in\mathbb{R}_{+}}$ is a stochastic process, fixing $t\in\mathbb{R}_{+}$ , we need to show that $N_{t}$ is a random variable. To do this, let $x\in\mathbb{R}$ , and consider $\{\omega:N_{t}(\omega)> x\}$ . Firstly, as $N_{t}(\omega)\in\mathbb{Z}_{\geq 0}$ for all $\omega\in\Omega$ , we must have $\{\omega:N_{t}(\omega)>x\}=\Omega\in\mathcal{F}$ for all $x<0$ .  On the other hand, if $x\geq 0$ , we have $$\{\omega:N_{t}(\omega)>x\}=\{\omega:N_{t}(\omega)\geq[x]+1\}=\{\omega:S_{[x]+1}(\omega)\leq t\}\in\mathcal{F},$$ the last set is in $\mathcal{F}$ because we have showed that $S_{n}$ is a random variable for all $n\in\mathbb{Z}_{\geq 0}$ . Thus, $\{\omega: N_{t}(\omega)>x\}\in\mathcal{F}$ for all $x\in\mathbb{R}$ , and thus $N_{t}$ is a random variable. Since $t\in\mathbb{R}_{+}$ is arbitrarily fixed, we are done. I will leave the post open for a few days in case I made a mistake or some further discussion happens. Then I will just answer my own post.","['stochastic-processes', 'solution-verification', 'probability-theory', 'random-variables']"
3533252,Show that a certain solution of an ODE is a stochastic process.,"I am working on an exercise as follows: Let $v_{1}, v_{2}:\mathbb{R}^{2}\longrightarrow\mathbb{R}^{2}$ be $C^{\infty}$ vector fields over $\mathbb{R}^{2}$ . Let $(X_{n})_{n\in\mathbb{N}}$ be an i.i.d positive sequence, and set $S_{n}:=X_{1}+\cdots+X_{n}$ . Define $N_{t}:=\sup\{n:S_{n}\leq t\}$ . Let $i(n)=1$ if $n$ is odd and $i(n)=2$ if n is even. Let $Z_{t}$ denote the solution of ODE $$\dfrac{d}{dt}Z_{t}=v_{i(N_{t})}(Z_{t}),\ Z_{0}=x.$$ Show that $(Z_{t})_{t\geq 0}$ is a stochastic process. To show $(Z_{t})_{t\geq 0}$ is a stochastic process, we need to show that $Z_{t}$ is a random variable for each $t$ . That is, if the probability space is $(\Omega,\mathcal{F},\mathbb{P})$ , we need to show $$\{\omega:Z_{t}(\omega)\leq r\}\in\mathcal{F}\ \text{for all}\ r\in\mathbb{R}.$$ But how could I connect this with the ODE?? Thank you! Edit 1: Okay I figured it out. This is not a really straightforward proof. Also this exercise is really loosely stated. I will talk about the reformulation of this exercise, and what we need to assume. Also, I need to clarify that, any argument using the probability theory here is not really useful, since we don't really know if $Z_{t}$ is a random variable or not. So anything written out will be purely functional analysis, or ODE analysis. The thing similar to random walk will not be a random walk since $Z_{t}$ may not be random at all, so it is also not really doable by using SLLN or something like that. In addition, the differential equation in the exercise is not stochastic differential equation. I think it is called Random ODE, or something like that. I will answer my own question to post my proof. It is also really appreciated if anyone could post their proof, since the bounty will otherwise be wasted... I believe there must be a better proof than mine :)","['stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
3533322,A peculiar series involving $\pi$,"A mathematical Facebook group posted a picture with the following claimed identity: $$
\sum_{n=0}^{\infty} \frac{1+14 n+76 n^{2}+168 n^{3}}{2^{20 n}}\binom{2n}{n}^{7}=\frac{32}{\pi^{3}}.
$$ they didn't provide any source for this claim, but it appears that it might be a conjecture. Is this a familiar series? If so, what are its origins?","['pi', 'analysis', 'sequences-and-series']"
3533394,Jordan canonical form over $\mathbb{R}$,"The theorem about Jordan canonical form states that for any operator $f:V\to V$ where $V$ is a vector space over $\mathbb{C}$ there is a basis such that the matrix of this operator in this basis is a union of Jordan blocks. And in my lecture notes there is a remark that the theorem is not true for vector spaces over $\mathbb{R}$ . After some time I came up with an example: indeed let's take an operator $f:\mathbb{R}^2\to \mathbb{R}^2$ given by matrix $$A_f=\begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix}.$$ Suppose it is true and $J$ is a Jordan canonical form then there is a matrix $C$ such that $\det C\neq 0$ with $J=C^{-1}A_fC$ or $CJ=A_fC$ . But $J$ can be one of the following options: $\begin{bmatrix}
\lambda & 0 \\
0 & \lambda
\end{bmatrix},\begin{bmatrix}
\lambda & 1 \\
0 & \lambda
\end{bmatrix},
\begin{bmatrix}
\lambda & 0 \\
0 & \mu
\end{bmatrix} (\lambda\neq \mu)$ .
In other words, we can have two block $J_1(\lambda)$ , one block $J_2(\lambda)$ to two blocks with different diagonal elements. If $C=\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}$ then comparing elements of the first column in matrix equation $CJ=A_fC$ we have: $$a\lambda=-c \quad \text{and} \quad c\lambda=a.$$ Since $\det C\neq 0$ , i.e. $ad\neq bc$ then it is easy to claim that $a\neq 0$ . Then $\lambda=-\frac{c}{a}$ and using the second equality we have $\dfrac{-c^2}{a}=a$ or $a^2+c^2=0$ . And since $a,c$ are reals then $a=c=0$ which is contradiction. Is this reasoning correct? Would be very grateful for any remarks.",['linear-algebra']
3533402,Does this set of functions separate points of $\Omega$?,"Let $\Omega$$\subset\mathbb{C}$ be a open connected set (domain). Let $z\in\Omega$ I want to show that if all bounded holomorphic functions on $\Omega$ are not just the constant functions (denoted by $H^\infty(\Omega)\not\equiv\mathbb{C}$ ), then, for every $0\neq\xi\in\mathbb{C}$ , there exists a bounded holomorphic function $f$ on $\Omega$ such that $f’(z)\xi\neq 0$ The proof goes like this (see proof of Proposition 2.5.1 in this book ).
Suppose $H^\infty(\Omega)\not\equiv\mathbb{C}$ and let $f_0\in H^\infty(\Omega), a_1,a_2\in\Omega$ , such that $f_0(a_1)\neq f_0(a_2)$ . Define for $j=1,2$ , $f_j$ as \begin{equation*}
    f_j(z) = \begin{cases}
              \frac{f_0(z)-f_0(a_j)}{z-a_j}& \text{if } z\neq a_j,\\
               f_0’(a_j) & \text{if } z= a_j.
          \end{cases}
\end{equation*} Then it is said that $f_0,f_1,f_2$ separates points of $\Omega$ . And $rank(f_0’,f_1’,f_2’)=1$ on $\Omega$ and hence proved. I do not understand how $f_0,f_1,f_2$ separates points of $\Omega$ ? And what is the meaning of $rank(f_0’,f_1’,f_2’)=1$ ?","['complex-analysis', 'functional-analysis']"
3533451,Will there at some point be more numbers with $n$ factors than prime numbers for any $n$? [duplicate],"This question already has an answer here : Limit of ratios of numbers with $m$ factors and primes (1 answer) Closed 4 years ago . Let $ \pi(x) $ be the prime counting function: the number of numbers $\leq x$ with just one prime factor. 
Let $ \pi_n(x) $ count the number of numbers $\leq x$ with exactly $n$ prime factors (counted with multiplicity). When plotting the values for different $n$ up to large $x$ , it seems that for every $n$ there will be a point from where on $ \pi_n(x) > \pi(x) $ .
I'm conflicted about my intuition on this. On one hand, it seem's to be plausible because numbers with more and more prime factors become more and more common. On the other hand, numbers with, say, 100 prime factors seem like they are so rare that they will be always less than $ \approx \frac{x}{\log x}$ My question is: Will there at some point be more numbers with $n$ factors than prime numbers for any $n$ ? Bonus question for positive answer: Is there a way to find the point this happens for a given $n$ other than the naive approach? Bonus question for negative answer: What is the largest $n$ which surpasses the prime counting function? $\pi_n(x)$ for $x \lt 100$ , at $\approx 25$ there are more numbers with $2$ factors than prime numbers. $\pi_n(x)$ for $x \lt 100.000$ , at $\approx 40000$ there are more numbers with $5$ factors than prime numbers. $\pi_n(x)$ for $x \lt 10.000.000$ , at $\approx 4.000.000$ there are more numbers with $6$ factors than prime numbers. (Please ignore the 'Divisors' in the chart legend, it should read 'Factors')","['number-theory', 'prime-factorization', 'elementary-number-theory', 'prime-numbers']"
3533481,Volume between surfaces,"Find $V(T),$ where $T$ is the region bounded by the surfaces $y = kx^2+kz^2$ and $z=kx^2+ky^2,$ where $k\in\mathbb{R}, k > 0.$ I tried solving for the area over which these curves intersect, which gave me $y-kz^2 = z-ky^2.$ Solving gives $y+ky^2 - (z+kz^2) =0\Rightarrow (y-z)(1 + k(y-z)) = 0.$ Since $y$ and $z$ are nonnegative, this implies $y=z.$ We thus obtain $y = kx^2 + ky^2\Rightarrow ky^2 - y +kx^2 = 0\Rightarrow (y -\dfrac{1}{2k})^2 +x^2 = \dfrac{1}{4k^2},$ which is a circle centered at $(0,\dfrac{1}{2k})$ with radius $\dfrac{1}{2k}.$ I am able to solve for $A(x),$ the area in terms of $x$ at a given value of $x,$ and I know $x$ ranges from $-\dfrac{1}{2k}$ to $\dfrac{1}{2k},$ but the resulting integral I have to evaluate is absolutely disgusting! Is there a ""cleaner"" integral I can use?","['integration', 'multivariable-calculus']"
3533492,Calculate the volume determined by the paraboloid $4z=x^2+y^2$ and the planes $z=1$ and $z= \dfrac{1}{2}$. Also $y≤x$.,"Calculate the volume determined by the paraboloid $4z=x^2+y^2$ and the planes $z=1$ and $z= \dfrac{1}{2}$ . Also $y≤x$ . I made a variable change: $x=r\cos \theta $ and $y=r\sin \theta$ then $z=\dfrac{r}{2}$ with $\dfrac{1}{4}≤r≤\dfrac{1}{2}$ and here is my first doubt: is $\theta$ going from $\dfrac{- \pi}{4}$ to $\dfrac{\pi}{4}$ ? because if I have two circles in the $xy$ plane centered on $(0,0)$ and I draw the line $y = x$ then the values ​​of $x$ less than or equal to $y$ are swept by the angle that goes from $\dfrac{- \pi}{4}$ to $\dfrac{\pi}{4}$ . Finally, I would like to consult what intergral I should use to obtain this volume and if it can be obtained only by a triple integral. Because I have seen cases in which a volume is obtained through a double integral and I don't understand how this is possible and when I can do it. I appreciate all help!","['integration', 'volume', 'analysis', 'multivariable-calculus', 'calculus']"
3533566,Determining the limit of a sequence of sets?,"(a) Let $\Omega = \mathfrak{R}$ . Define $A_n = \big{[}0, \frac{n}{n+1}\big{)}$ . Determine if $\lim_{n \to \infty} A_n$ exists. If yes, what is it? (b) Show that $\lim_{n \to \infty} \big{[}0, 1 + \frac{1}{n} \big{)} = [0, 1]$ Definitions : $$\inf_{k \geq n}A_k = \bigcap_{k = n}^{\infty} A_k$$ $$\sup_{k \geq n}A_k = \bigcup_{k = n}^{\infty} A_k$$ $$\lim_{n \to \infty}\inf A_n = \bigcup_{n = 1}^{\infty}\bigcap_{k = n}^{\infty} A_k$$ $$\lim_{n \to \infty}\sup A_n = \bigcap_{n = 1}^{\infty}\bigcup_{k = n}^{\infty} A_k$$ The sequence of sets $\{A_n\}$ is said to converge to its limit A if: $$\lim_{n \to \infty}\inf A_n = \lim_{n \to \infty}\sup A_n = A$$ Question : Just looking at the sets in (a) and (b) and 'plugging' in $\infty$ I can obtain an answer, but I do not think that is the correct way to approach this problem. Is there a simple way to evaluate the infimum and supremum of a set? I understand the infimum is the greatest element that is less than or equal to all elements of a set $S$ whereas the supremum is the least element that is greater than or equal to all elements of $S$","['limits', 'measure-theory', 'probability-theory', 'probability']"
3533631,Necessary and sufficient condition for two components of least square estimate to be negatively correlated,"Consider a linear model in a matrix form $Y = X\beta + \epsilon$ where $Y$ is a response vector, $X$ is a $n$ by $p$ ( $p < n$ ) full rank matrix of predictors, $\beta$ is a parameter vector, and $\epsilon$ is an error vector whose components are i.i.d. with a variance of $\sigma^2 > 0$ . (a) Derive the variance-covariance matrix of $\hat{\beta}$ , the least square estimator of $\beta$ . (b) Let $X$ be given by the matrix \begin{bmatrix}
1 & x_1\\
1 & x_2\\
. & .\\
. & .\\
. & .\\
1 & x_n
\end{bmatrix} Determine the necessary and sufficient condition for the two components of $\hat{\beta}$ being negatively correlated. Here are my thoughts so far: Part (a) has been asked on this network quite a bit, so sparing the details, one can work out that the variance-covariance matrix of $\hat{\beta}$ is  given by $(X'X)^{-1} \cdot \sigma^2$ (where the prime symbol here represents the transpose). Now, for part (b), let $\beta = (\beta_1, \beta_2)'$ , and let $\hat{\beta} = (\hat{\beta_1}, \hat{\beta_2})'$ be the LSE of $\beta$ . Then we have covariance matrix given by $(X'X)^{-1} \cdot \sigma^2 = \begin{bmatrix}
n & \sum_{i = 1}^nx_i\\
\sum_{i = 1}^nx_i & \sum_{i = 1}^n x_i^2
\end{bmatrix}\sigma^2 = \begin{bmatrix}
Var(\hat{\beta_1}) & Cov(\hat{\beta_1} \hat{\beta_2})\\
Cov(\hat{\beta_1} \hat{\beta_2}) & Var(\hat{\beta_2})
\end{bmatrix}\sigma^2$ Now, I believe that the two components $\hat{\beta_1}$ and $\hat{\beta_2}$ of $\hat{\beta}$ are negatively correlated if and only if $Cov(\hat{\beta_1}, \hat{\beta_2})$ is negative. Following this thought, since $\sigma^2$ is positive, this means that $\hat{\beta_1}$ and $\hat{\beta_2}$ are negatively correlated in our case if and only if $\sum_{i = 1}^n x_i < 0$ . Thus, the necessary and sufficient condition for the two components of $\hat{\beta}$ being negatively correlated is $\sum_{i = 1}^n x_i < 0$ . Is this a correct solution ? Or can I take this further to express a more desirable condition for my solution ? Thanks for your time !","['statistics', 'covariance', 'variance', 'least-squares', 'linear-algebra']"
3533649,Subharmonicity implies mean-value inequality,"If we take as definition that if $\Omega \subseteq \Bbb R^n$ is open and $u \in \mathscr{C}^2(\Omega)$ is subharmonic if $\triangle u \geq 0$ , then for every $x \in \Omega$ and small enough $r>0$ we have the inequality $$u(x) \leq \frac{1}{|B_r(x)|}\int_{B_r(x)} u(y)\,{\rm d}y,$$ where $|B_r(x)|$ denotes the volume of the ball. There's a proof for the harmonic case in Chapter 1 of Evans which is easy to follow and adapt, and he would first prove that $$u(x) \leq \frac{1}{|\partial B_r(x)|}\int_{\partial B_r(x)} u(y)\,{\rm d}S(y),$$ to then integrate using polar coordinates and get the result. This is all fine. I tried to give a direct proof avoiding the surface integral and got something absurd. Fix $x \in \Omega$ and define $\varphi\colon [0,\epsilon) \to \Bbb R$ (for some maximal $\epsilon>0$ which exists due to openness of $\Omega$ ) by $$\varphi(r) = \frac{1}{|B_r(x)|}\int_{B_r(x)} u(y)\,{\rm d}y.$$ Lebesgue's differentiation theorem says that $$ \varphi(0) = \lim_{r\to 0^+} \varphi(r) = u(x),$$ so our goal is to show that $\varphi'(r)\geq 0$ (as the desired conclusion is just $\varphi(0) \leq \varphi(r)$ for $r>0$ ). And this is done as follows: first we rewrite $\varphi(r)$ in a more convenient fashion, using that $|B_r(x)| = r^n|B_1(0)|$ and setting $y = x+rz$ with $z \in B_1(0)$ , so that ${\rm d}{y} = r^n \,{\rm d}{z}$ and $$  \varphi(r) = \frac{1}{|B_1(0)|} \int_{B_1(0)} u(x+rz)\,{\rm d}{z}.  $$ So differentiating under the integral (allowed by smoothness of $u$ and compactness of $B_1(0)$ ) gives $$  \varphi'(r) = \frac{1}{|B_1(0)|} \int_{B_1(0)} \langle \nabla u(x+rz),z\rangle\,{\rm d}{z}.    $$ To apply Green's identities, let $v(z) = u(x+rz)$ and $q(z) = \langle z,z\rangle/2$ , so that $\nabla v(z) = r\nabla u(x+rz)$ and $\nabla q(z) = z$ . Also $\triangle v(z) = r^2\triangle u(x+rz)$ . So \begin{align*}
  \varphi'(r) &= \frac{1}{r|B_1(0)|}\int_{B_1(0)} \langle \nabla v(z),\nabla q(z)\rangle\,{\rm d}{z} \\ &= \frac{1}{r|B_1(0)|}\left( -\int_{B_1(0)} q(z)\triangle v(z)\,{\rm d}{z} + \int_{\partial B_1(0)} q(z)\frac{\partial v}{\partial \nu}(z)\,{\rm d}{S}(z)\right) \\  &= -\frac{r}{2|B_1(0)|} \int_{B_1(0)} \|z\|^2 \triangle u(x+rz)\,{\rm d}{z} \color{red}{\leq 0}
\end{align*} I cannot find the mistake. What's the screw up?","['integration', 'harmonic-functions', 'multivariable-calculus', 'solution-verification', 'partial-differential-equations']"
3533667,Show that the given two linear mappings commute,"Given $A\in M_n(\mathbb C)$ , show that the mappings $$\alpha_A(B) = \frac{1}{2}(AB + BA^{*})$$ $$\beta_A(B) = \frac{1}{2i}(AB - BA^{*})$$ define $\mathbb R$ -linear maps $HERM_n(\mathbb C) → HERM_n(\mathbb C)$ . Also show that $\alpha_A, \beta_A$ commute with each other. What I did - First I proved that $\alpha_A$ and $\alpha_B$ are Hermitian. Then I tried to prove that $\alpha_A*\alpha_B=\alpha_B*\alpha_A$ . The final expression was $AB^2A^*=BA^*AB$ but I can't see how they are equal. .","['matrices', 'hermitian-matrices', 'linear-algebra', 'linear-transformations']"
3533707,"When $A$ is $3\times3$, prove that $A^2=0$ iff $A$ has rank less than or equal one and trace zero","Let $A \in M_3$ . Prove that $A^2=0\Leftrightarrow \operatorname{tr}(A)=0,\operatorname{rank}(A)\le 1$ I can easily prove $\operatorname{tr}(A)=0,\,\operatorname{rank}(A)\le 1 \Rightarrow A^2=0$ since $\operatorname{rank}(A)\le 1 \Rightarrow A^2=\operatorname{tr}(A)A$ . For $A^2=0 \Rightarrow \operatorname{tr}(A)=0, \operatorname{rank}(A)\le 1$ , I prove $\operatorname{tr}(A)=0$ by proving eigenvalues of a nilpotent matrix are zeros, but for $\operatorname{rank}\le 1$ I have no idea. Can anyone give me a hint?","['matrix-rank', 'linear-algebra', 'trace']"
3533713,Uniform random distribution on a unit disk,"a) A point is uniformly chosen in the unit disk $0 ≤ x^2 + y^2 ≤ 1$ . Find the probability that its
  distance from the origin is less than $r$ , for $0 ≤ r ≤ 1$ . b) Compute its expected distance from the origin. c)Let the coordinates of the point be $(X, Y )$ . Determine the marginal p.d.f. of $X$ . Are $X$ and $Y$ independent? I did the part a) using geometry that the area of the circle with a radius of $r$ is divided by the area of the unit circle, such that $$P(R\leq r)=\frac{\pi r^2}{\pi\cdot 1^2}=r^2$$ Part b) is attempted to solve by differentiating the cdf, such that $$f(r)=\frac{d}{dr}r^2=2r,\hspace{3mm} E(R)=\int_{0}^1r\times 2r\,dr=\frac{2}{3}.$$ But this result does not seem to be right... And I do not know about part c) A simulation was done using python to visualize this distribution,
and it gives image like this. Why are the points concentrated near the central area? from scipy.stats import uniform
import matplotlib.pyplot as plt
import math  
r = uniform.rvs(scale =1,size=5000)
pi = 3.14159265359
theta = uniform.rvs(scale =2*pi,size=5000)
x = []
y = []
for i in range (5000):
    x.append(r[i]*math.cos(theta[i]) )
    y.append(r[i]*math.sin(theta[i]) )
fig=plt.figure()
ax=fig.add_axes([0,0,2,3])
ax.scatter(x, y)```","['independence', 'probability-distributions', 'geometric-probability', 'probability']"
3533731,Real roots of partial sums of analytic function represented by infinite series :,"Consider the following polynomial: $$P_n(x)= \sum_{k=1}^n a_kx^k$$ We know (and can verify) that , for every $n$ , $P_n(x)$ has only real roots Now, define $$P(x)= \sum_{k=1}^\infty a_kx^k$$ We know that $P(x)$ analytic and entire . Question : Does this mean that $P(x)$ has only real roots ? If not please elaborate with examples Note: I'm not sure this is MSE or MO question (?) Edit: Converse of the above hypothesis isn't true","['real-analysis', 'complex-analysis', 'entire-functions', 'polynomials', 'analytic-functions']"
3533760,Proving that the limit of a function doesn't exist at an essential singularity,"Assume $f$ is analytic on a deleted neighborhood $B'(0;a)$ . Prove that
  the limit of the function as $z$ approaches $0$ exists (possibly
  infinite) if, and only if there exists an integer $n$ and a function $g(z)$ analytic on $B(0;a)$ , with $g(0)$ not zero such that $f(z)=z^ng(z)$ in $B'(0;a)$ The ""if"" direction is simple but I am struggling with the other direction. The ""only if"" part is the same as proving that if the principle part of Laurent series expansion for the function around $0$ is an infinite sum, the limit doesn't exist which is the same as proving that it depends on the direction in which we approach $0$ . I tried to exhibit two different limits but failed mainly due to the fact that I couldn't deal with the arbitrary coefficients of the series (since this should hold for any function with an essential singularity). This is a problem in Apostol's mathematical analysis and he doesn't mention the Casorati–Weierstrass theorem so I suppose one should be able to solve the problem without depending on it.","['laurent-series', 'analyticity', 'singularity', 'complex-analysis', 'sequences-and-series']"
3533764,Convergence of positive sequence in which each term is less than the average of preceding 2 terms [duplicate],"This question already has answers here : $x_{n+m}\le \frac{x_n+x_{n+1}+\cdots+x_{n+m-1}}{m}$. Prove that this sequence has a limit. (3 answers) Closed 4 years ago . It is easy to prove any sequence generated by $$
x_{n+1}:=\frac{x_n+x_{n-1}}{2}
$$ is convergent. But how about if the sequence only satisfies $$
0\leq x_{n+1}\leq\frac{x_n+x_{n-1}}{2}
$$ with positive $x_0$ and $x_1$ ? @Martin R gives a proof. Could this result be extended to $n$ preceding terms?","['convergence-divergence', 'sequences-and-series']"
3533810,"$PSL(2, \Bbb{Z})\cong \Bbb{Z}_2 \ast \Bbb{Z}_3$","I am trying to show that $PSL(2, \Bbb{Z})\cong \Bbb{Z}_2 \ast \Bbb{Z}_3$ . What I've shown is that, for $$
    A=
    \left(\begin{matrix}
    0  & 1 \\
    -1 & 0 \\
    \end{matrix}\right)
$$ and $$
    B=
    \left(\begin{matrix}
    1 &  1 \\
    -1 &  0 \\
    \end{matrix}\right),
$$ $\{A, B \}$ generates $PSL(2, \Bbb{Z})$ with $A^2=B^3=I$ . Here I can't proceed further as I don't see how to ensure that there exists no more restriction on the elements so that I can conclude that $A^2, B^3$ are the only relators and get it done. Any help will be appreciated. Edit: Thanks to the comment I've become aware of the ping-pong lemma, and as it is my first application of this theorem I want my solution below to be examined if it is not flawed. Claim: $PSL(2, \Bbb{Z})$ is the free product of $\langle{A}\rangle$ = $\{I, A \}$ and $\langle{B}\rangle$ = $\{I, B, B^2\}$ . proof) Let $X=\Bbb{R}^2/\sim$ be the real plane with antipodals identified on which $PSL(2, \Bbb{Z})$ acts by the left multiplication. (Regarding each point of $X$ as a column vector.) And let $X_1$ be the 1st (and 3rd) quadrant and $X_2$ be the 2nd (and 4th) quadrant of $X$ . Then $A$ maps $X_2$ into $X_1$ , and both $B$ and $B^2$ map $X_1$ into $X_2$ as the image below. Hence it follows that $PSL(2, \Bbb{Z})$ = $\langle{A}\rangle \ast \langle{B}\rangle$ by the ping-pong lemma.","['combinatorial-group-theory', 'geometric-group-theory', 'group-theory']"
3533835,Sum of infinite geometric progression (Paradox of Zeno),"In I.M Gelfand's Algebra book, there is a question which as follows: - Achilles and a turtle have a race and turtle is given a head start.
  Achilles runs ten times faster than the turtle. When the race starts Achilles comes to the place where the turtle
  initially was, but during that time even turtle moved $\frac{1}{10}th$ of the initial distance. When Achilles covers that distance, then turtle moves $\frac{1}{100}th$ of the initial distance and so on. So naturally we will get a Geometric Progression (G.P.) which as follows: - $$ 1, \frac{1}{10}, \frac{1}{100}, \frac{1}{1000} ... $$ Where the common ration $(q)$ would then be equal to $\frac{1}{10}$ $$q = \frac{1}{10}$$ We already know that: - The sum of G.P. is given by the formula: - $$1 + q + q^2 + q^3 + ..... + q^{(n-1)} = \frac{q^n - 1}{q - 1}$$ In our case since $q = \frac{1}{10}$ and $n$ will be infinitely
  bigger, we can write our summation formula as:- $$ \frac{1}{1 - q} $$ Where after subsitution of the value $q = \frac{1}{10}$ , we shall get $S = \frac{10}{9}$ Till here I understood the logic of what Gelfand is trying to say,(that Achilles shall get ahead of the turtle after $\frac{10}{9}$ meters) but after that he continues the question and says: - Imagine now that Achilles is running ten times more slowly than the
  turtle. When he comes to the place where the turtle initially was, it
  is at the distance ten times than the initial one, and so on. So we shall get G.P. now as: - $$ 1, 10, 100, 1000 ... $$ Where in this case the $q = 10$ . After that Gelfand asks us to put $q = 10$ in the formula: - $$ \frac{1}{1 - q} $$ After which we shall get an absurd answer of $S = \frac{-1}{9}$ My question is how can we put $q = 10$ in the above formula? Shouldn't we put it in the formula $\frac{q^n - 1}{q - 1} ?$ We transformed $\frac{q^n - 1}{q - 1} to \frac{1}{1 - q}$ only because of the fact that $\displaystyle{\lim_{n \to \infty}} q^n = (\frac{1}{10})^n = 0$ . But the second case is totally different. In second case $q = 10$ . Gelfand further goes on and says: - Is it possible to give a reasonable interpretation of the (absurd)
  statement ""Achilles will meet the turtle after running $- \frac{1}{9}$ meters""? Hint. Yes, it is. So can someone please explain how can we interpret the ""absurd"" statement? Is it because we used the wrong formula (my guess) or am I missing something here? Edit: - I found the online solution for my question here (problem number 222). Howsoever I am not able to understand its connotation and denotation.","['algebra-precalculus', 'sequences-and-series']"
3533908,Proving the range of this function,"While preparing for an exam, I came across this question (it was previously asked in the exam): By taking a few different values of $x$ , I noticed a pattern: if $x$ is a multiple of 5, then the function value reduces to $f(5)$ , else it always reduces to $f(1)$ . $f(15) = f(20) = f(10) = f(5)$ $f(3) = f(8) = f(4) = f(2) = f(1)$ So, looks like $f$ can take only two values: $f(1)$ and $f(5)$ . So, the answer to the above question should be $2$ . And it is indeed the correct answer. But how do I go about proving it so that I can be absolutely sure about the answer? (The exam awards negative marks for incorrect answers). I am not even sure where to begin with the proof. Note: the exam is for computer science graduates and discrete mathematics is one of the subjects.","['functions', 'discrete-mathematics']"
3533912,Non-abelian finite group $G$ has at least 2 conjugacy classes which contain at least 2 elements,"Show that each non-abelian finite group $G$ has at least $2$ conjugacy classes which contain at least $2$ elements. I have a solution which uses the class equation. However, as we have not dealt with this equation in class, we do not have it at our disposal. Is it possible to prove this proposition without the class equation? My attempt looks like this: As $G$ is non-abelian, there are $a$ and $b$ with $a\neq b$ such that $aba^{-1}\neq b $ and also $bab^{-1}\neq a $ . Hence, in the conjugacy class of $b$ , there are $b$ itself and $aba^{-1}$ , which is why this class contains at least two distinct elements. The same can be done for the conjugacy class of $a$ which contains at least $a$ and $bab^{-1}$ . At first glance, both classes seem to be distinct. However, could it be possible that both conjugacy classes are equal? Then, this attempt of a proof would be invalid, of course.","['alternative-proof', 'group-theory', 'abstract-algebra']"
3533921,Hyperbolic Spaces Embedded in Euclidean Spaces,"I want an embedding of the hyperbolic space of dimension $n$ , or the manifold $\mathbb{H}^n$ in a Euclidean space of some dimension. I know this can be embedded in the Minkowski space and this can not be done for $\mathbb{E}^3$ . A reference would do. Also, $n=2$ is my concern. But I guess it is not that hard to generalize this to higher dimensions.","['hyperbolic-geometry', 'differential-geometry']"
3533962,Can a curve fill a disk?,"How can I prove that there is no $C^1$ - class curve $\gamma : [0,1] \rightarrow \mathbb{R^2}$ that can fill the unit disk?","['differential-topology', 'differential-geometry']"
3534043,Finding fundamental period of functions,"How should we find fundamental periods of I) $$\cos(\frac{3x}{5})-\sin(\frac{2x}{7})$$ II) $$\frac{\sin(12x)}{1+\cos^2(6x)}$$ III) $$\sec^3(x)+\ {cosec}^3(x)$$ What i did was found the fundamental period of individual functions and took L.C.M and got correct answer. But then a thought occurred to me that their L.C.M is bound to be their period but it's not necessarily their fundamental period. For example fundamental period of $|sin(x)|+|cos(x)|$ is $\frac{π}{2}$ ,so here fundamental period of $|\sin(x)|$ is ${π}$ and |{cos(x)}| is also $π$ . So how should i find these function's fundamental periods?How should i know where L.C.M is the fundamental period and when it is not?",['functions']
3534136,Can we find / express all matrices so that ${\bf P}^2 = {\bf P+I}$?,"Can we find/express all matrices, so that: $${\bf P}^2={\bf P+I}$$ Own work: For the eigenvalues must then hold: $$\lambda^2-\lambda-1=0$$ In other words : $$\lambda_1 = \frac{1-\sqrt{5}}2\\\lambda_2 = \frac{1+\sqrt{5}}2$$ So as long as we construct a matrix with any combination of such eigenvalues, we shall be fine. $${\bf P =SDS}^{-1},\\
 {\bf D} = \text{diag}(\text{AnyCombinationOf}\{\lambda_1,\lambda_2\})$$ for any matrix $\bf S$ .
For my experiments this does seem to give numerically reasonable results for lots of such generated $\bf P$ of sizes $2-6$ . These numerical experiments do however not disprove that such matrices $\bf P$ could exist which do not fit this description. Does this description or representation catch all possible such matrices, or do we miss some? How to prove?","['eigenvalues-eigenvectors', 'cayley-hamilton', 'matrices', 'minimal-polynomials', 'linear-algebra']"
3534166,Error in calculation of expectation of function chi square random variable,"Suppose that $X$ follows a chi-square distribution $\chi_n^2$ and that $Y=\sqrt{2X}$ . Find the pdf of $Y$ and show that $\mathbb{E}(Y) = \frac{\Gamma((n+1)/2)}{\Gamma(n/2)}$ . I have calculated the pdf using the change of variable formula as $$f_Y(y) = \frac{y^{n-1} e^{-y^2/4}}{2^{n-1} \Gamma(n/2)}$$ Now $$
\begin{split}
\mathbb{E}(Y)
 &= \int^{\infty}_{0} y \cdot \frac{y^{n-1} e^{-y^2/4}}{2^{n-1} \Gamma(n/2)}dy \\
 &= \int^{\infty}_{0} \frac{y^{n} e^{-y^2/4}}{2^{n-1} \Gamma(n/2)}dy \\
 &= \int^{\infty}_{0}\frac{y^{n-1}e^{-y^2/4}}{2^{n-2}\Gamma(n/2)} \frac{y}{2}dy \\
 &= \int^{\infty}_{0}\frac{2^{n-1}(y^2/4)^{(n-1)/2}e^{-y^2/4}}{2^{n-2}\Gamma(n/2)}
\frac{y}{2}dy \\
 &= \int^{\infty}_{0}\frac{2(y^2/4)^{(n+1)/2-1}e^{-y^2/4}}{\Gamma(n/2)} \frac{y}{2}dy.
\end{split}
$$ Finally using the definition that $\Gamma(\alpha) = \int^{\infty}_{0} x^{\alpha-1}e^{-t}dt$ and making the substitution $t=y^2/4 \Leftrightarrow dt = y/2\,dy$ we get $$
\mathbb{E}(Y)
 = \frac{2}{\Gamma(n/2)} \int^\infty_0  t^{(n+1)/2-1}e^{-t}\,dt
 = \frac{2\Gamma((n+1)/2)}{\Gamma(n/2)}
$$ which is off by a factor of $2$ of what they get. Where is the mistake?","['statistics', 'probability-distributions', 'calculus', 'substitution', 'probability']"
3534175,An expression for $\lim_{n\to\infty}\frac1{2^n}\left(1 + x^{1/n}\right)^n$,"I am looking for a closed form answer to the limit ( $x<1$ ): $$\lim_{n\to\infty}\frac1{2^n}\left(1 + x^{1/n}\right)^n$$ For context, I was studying weighted averages and considered $$\left(\frac12(x^{1/n} + y^{1/n})\right)^{n}$$ to be a good way to weight averages in favour of the lower number (similar to root mean squared, but kinda reversed). I think was studying what happens for different values of $n$ . For $x=3$ , $y=5$ , I found that this limit seems to converge to $3.78962712197\dots$ but I do not recognise where this number comes from. Rearranging the above average formula gives $$\frac{y}{2^n}\left(1+\left(\frac xy\right)^{1/n}\right)^n$$ and this is what inspired the question. I see it looks similar to some kind of exponential, but it isn't quite there. I also tried exponentiating and logging the whole expression to bring down the $n$ , but I didn't know how to deal with the power inside the brackets then. My main issue is that since $1/n$ goes to $0$ , the thing thats raised to this power goes to $1$ (and so is not small) so series expansions can't be used. Thanks!","['average', 'limits', 'algebra-precalculus']"
3534227,The projection of an ellipse is still an ellipse,"While studying the problem of determining the orbit of a binary star system, my astronomy notes say: ""the projection of an ellipse onto a plane is still an ellipse (except the special case in which it's a segment)"".
How could I prove this? EDIT: As suggested i'm adding what I know about ellipses. I know the basics of ellipses in the plane, the equation of an ellipse in a plane in cartesia coordinates, the equation of an ellipse in polar coordinates, the fact that ellipses can be obtained by the intersection of planes with a cone. In my geometry course we defined ellipses as the the points of the plane such that the ratio between the distance from a point and a line is constant and less than 1. I also know that the sum of the distances of the points of the ellipse from the foci is constant.","['conic-sections', 'geometry']"
3534260,Differential equation- Solutions to initial condition problem,"The equation given is $$x(t)=C_{1}e^{-t}+C_{2}e^{2t}$$ $$x^{''}-x^{'}-2x=0$$ The first and second derivatives are given respectfully $$-C_{1}e^{-t}+2C_{2}e^{2t}$$ $$C_{1}e^{-t}+4C_{2}e^{2t}$$ I have verified that the problem does in fact equal zero.
The given conditions are: $$x(0)=10$$ $$x^{'}(0)=8$$ After substitution and evaluation of the original and first prime equations I got $C_2=\frac{8}{3}$ and $C_1=\frac{16}{3}$ and having no one around to check if I'm correct,I was hoping that someone could tell me if I'm right or if I made a small mistake.","['initial-value-problems', 'ordinary-differential-equations']"
3534297,How fast can the sum of a square-summable sequence grow?,"Suppose $x_t$ is a nonnegative sequence satisfying $$ \sum_{t=1}^{+\infty} x_t^2 < \infty.$$ I am trying to get a precise estimate for how fast $\sum_{t=1}^T x_t$ can grow as a function of $T$ . Application of Cauchy-Schwarz gives that $$\sum_{t=1}^T x_t \leq \sqrt{T} \sqrt{\sum_{t=1}^{+\infty} x_t^2},$$ so $O(\sqrt{T})$ is one upper bound. My question is whether in fact $$ \lim_{T \rightarrow +\infty} \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t = 0.$$ Here is why one might hope that such a thing is true. First, Cauchy-Schwarz is tight when the two vectors are multiples of each other, and since $x_t \rightarrow 0$ , the vector $(x_1, \ldots, x_T)$ is very far from being a multiple of $(1,...,1)$ . Second, if we try to come up with a tight example, the natural guess might be $x_t = 1/(\sqrt{t} \log^c(t))$ for some $c>0$ , since its square is close to being the slowest decaying summable sequence. But in that case $\sum_{t=1}^T x_t = O(\sqrt{T}/\log(T))$ , and the limit is indeed zero.","['sequences-and-series', 'real-analysis']"
3534313,Is this alternative representation of $f(x)=xe^x$ as Maclaurin series correct?,"Let $f(x)=xe^x$ . I know $$e^x=\sum_{n=0}^\infty \frac{x^n}{n!}$$ and so $$f(x)=x\sum_{n=0}^\infty \frac{x^n}{n!}=x(1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+...)=x+x^2+\frac{x^3}{2!}+\frac{x^4}{3!}+...=\sum_{n=0}^\infty \frac{x^{n+1}}{n!}$$ But shouldn't this other representation be also correct? $f'(x)=e^x+xe^x, f''(x)=2e^x+xe^x, f'''(x)=3e^x+xe^x,... \implies f^{(n)}(x)=ne^x+xe^x \implies f^{(n)}(0)=n$ Because I want $f(x)=\sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!}x^n$ then $$f(x)=\sum_{n=0}^\infty \frac{n}{n!}x^n$$ If this other representation is also correct, then how $\sum_{n=0}^\infty \frac{n}{n!}x^n=\sum_{n=0}^\infty \frac{x^{n+1}}{n!}$ ?","['power-series', 'taylor-expansion', 'sequences-and-series']"
3534334,"Proof of $\mathbb E[\exp( \lambda XY)]=\mathbb E[\exp( \lambda^2 X^2)/2]$, where $X,Y$ are independent standard normal random variables","I have a heuristic proof of the fact $\mathbb E[\exp( \lambda XY)]=\mathbb E[\exp( \lambda^2 X^2)/2]$ ,where $X,Y$ are independent standard normal random variables as follows: Using the fact that $\mathbb E[\exp( \lambda Z)]=\exp(\lambda^2/2)$ , where $Z$ is standard normal. By conditioning, we have $\mathbb E[\exp( \lambda XY)]=\mathbb E (\mathbb E[\exp( \lambda XY)\mid X])$ , at this point, one may treat $X$ as a constant (heuristically I take $\mathbb E[\exp( \lambda XY)\mid X]=\exp(\lambda^2 X^2/2) $ , but is it true and why is it true? ) and use the formula above to get $E[\exp( \lambda^2 X^2)/2]$ . But how to make this argument formal and rigorous? Any other proof will also be appreciated!",['probability-theory']
3534358,Does the inverse Laplace transform of $F(s)=\frac{\sin(\xi\sqrt{s})}{\sqrt{s}}$ exist for some $\xi\in\mathbb{C}$?,"In particular, I am interested in $\xi = z\sqrt{i}$ , with $z>0$ . To begin with, formal considerations, such as $$\begin{align*}\mathcal{L}^{-1}\!\left[F(s)\right]=\frac{1}{2i}\,\mathcal{L}^{-1}\!\left[\frac{e^{i\xi\sqrt{s}}}{\sqrt{s}}\right]-\frac{1}{2i}\,\mathcal{L}^{-1}\!\left[\frac{e^{-i\xi\sqrt{s}}}{\sqrt{s}}\right]=\frac{e^{\frac{1}{4t}(-i\xi)^2}}{2i\,\sqrt{\pi t}}-\frac{e^{\frac{1}{4t}(i\xi)^2}}{2i\,\sqrt{\pi t}}=0,\end{align*}$$ and similar ones all seem to imply $\mathcal{L}^{-1}[F(s)]=0$ . More rigorously, invoking the Bromwich inversion formula $$\mathcal{L}^{-1}\!\left[F(s)\right] = \int_{\alpha-i\infty}^{\alpha+i\infty}F(s)\,e^{st}ds,$$ and applying Cauchy's integral theorem to the closed contour depicted below, I obtain $$\mathcal{L}^{-1}\!\left[F(s)\right] = \lim_{T\to\infty} I_{AB} = - \lim_{\epsilon\to0^+}\big(I_{EH}+I_{KL}\big).$$ The branch cut of $\sqrt{s}$ is taken along the negative real axis. Note as well that the integrals arising from the arcs vanish in the limit $R\to\infty$ by Jordan's Lemma (i.e., considering only $\xi=z\sqrt{i}$ , $z>0$ ). However, $$\begin{align*}I_{EH}+I_{KL}&=\int_{-\infty}^0\frac{\sin\big[z\sqrt{i}\,\sqrt{x+i\epsilon}\big]}{\sqrt{x+i\epsilon}}\,e^{xt}+\int_0^{-\infty}\frac{\sin\big[z\sqrt{i}\,\sqrt{x-i\epsilon}\big]}{\sqrt{x-i\epsilon}}\,e^{xt}\\[5pt]&=\int_0^{\infty}\frac{\sin\big[z\sqrt{i}\,\sqrt{i\epsilon-u}\big]}{\sqrt{i\epsilon-u}}\,e^{-ut}-\int_0^{\infty}\frac{\sin\big[z\sqrt{i}\,\sqrt{-u-i\epsilon}\big]}{\sqrt{-u-i\epsilon}}\,e^{-ut}\end{align*},$$ letting $u=-x$ in each integral above. But, $$\int_0^{\infty}\frac{\sin\big[z\sqrt{i}\,\sqrt{\pm i\epsilon-u}\big]}{\sqrt{\pm i\epsilon-u}}\,e^{-ut} = \frac{\sin\big[(-1)^{3/4}z\sqrt{u}\big]}{i\sqrt{u}}\pm\mathcal{O}(\epsilon),$$ consequently $$\mathcal{L}^{-1}\!\left[F(s)\right] = - \lim_{\epsilon\to0^+}\big(I_{EH}+I_{KL}\big) = 0\qquad :($$ I am expecting $\mathcal{L}^{-1}\!\left[F(s)\right]$ to be a distribution.","['integration', 'contour-integration', 'laplace-transform', 'inverse-laplace']"
3534421,Finding the limit of the following sequence: $\left(\frac{2m^2 + m + 4}{2m^2 + 3m + 5}\right)^{3m-3}$,"So I am trying to find $$ \lim_{m \to \infty} \left(\frac{2m^2 + m + 4}{2m^2 + 3m +5}\right)^{3m-3} $$ I believe this should be represented as $e$ somehow to the power of something, however I can't get to do the algebra in the right way to get to $e$ , I divide by $2m^2$ but then I don't know what to do next.","['limits', 'calculus', 'sequences-and-series']"
3534496,An extension of the determinant to non square matrices,"I'm an undergraduate student in mathematics and today I've been asked the following question by a friend of mine: let $v, w$ be vectors in $\mathbb{R}^3 $ and $ A=\left( \begin{array}{cc} v_1 \ v_2 \ v_3 \\ w_1 \ w_2 \ w_3 \end{array} \right)$ , she asked for a geometrical interpretation of the fact that $\det (AA^T)=$ area of the parallelogram enclosed between $v$ and $w$ squared. The algebra turns out to be correct (we proved it in this particular case), but I started wondering: is this true in general? Or, to put it in a better way: does it make sense to extend $\det$ to non square matrices as $\sqrt{\det(AA^T)}$ ? My reasoning is the following: we know that $\det(AA^T)=\det(A)^2 \ \forall A \in \mathbb{R}^{n\times n}$ . For now, let's just pretend that there exists a function $\operatorname{Area}: M(m, n, \mathbb{R}) \mapsto \mathbb{R}$ such that $Area(A) = \det(A)$ when $A$ is square and that behaves similarly to $\det$ (that is, is invariant under transposition, obeys Binet theorem ecc.). Thus, $\operatorname{Area}(M)^2=\operatorname{Area}(MM^T)=\det(MM^T) \  \forall M \in M(n, m, \mathbb{R})$ . So such a function must be identically equal to $\pm\sqrt{\det(M M^T)}$ (note that $\det(MM^T)$ is always positive, giving a hint that this extension is very likely to be sensible). Now comes the question: does any of this actually make sense? I.e.: Unfortunately the definition is not so straight forward, in the sense that $\operatorname{Area}(A^T)=\sqrt{\det(A^TA)}$ which is generally different from $\sqrt{\det(AA^T)}$ , so it looks like the definition might be flawed BUT I noticed (see edit) that one of the two is always zero, so $\operatorname{Area}$ could just be defined as the one that is not zero. Is it true that $\operatorname{Area}(A) \neq 0 \iff A$ has full rank? More specifically, does this actually still represent the area of the parallelogram (or hyperparallelogram in general) enclosed between the vectors $Ae_j$ where $\{e_j: 1\leq j \leq m\}$ is the standard basis of $\mathbb{R}^m$ ? If the previous point was true, how would one decide the sign of the result such that it still reflects the ""flipping"" somehow? Does ""flipping"" even make sense for a function $:\mathbb{R}^m \mapsto \mathbb{R}^n$ ? Any comment is appreciated! Edit: as was pointed out in the comments, if $m<n, A \in M(m, n, \mathbb{R})$ then $\det{A^TA}=0$ , because $A^TA \in M(n, n, \mathbb{R})$ and $rank(A^TA) \leq rank(A) \leq m < n$ which means that $rank(A) < n$ and therefore $A^TA$ is not invertible, so $\det(A^TA)=0$ .","['euclidean-geometry', 'determinant', 'area', 'matrices', 'linear-algebra']"
3534500,Differences between the geometry and topology books of A. T. Fomenko,"Anatoly T. Fomenko has authored and coauthored a number of books on differential geometry and topology. He may be more known for other things , but I have been told his books are quite good for learning geometry and topology. As an undergraduate student planning on learning differential geometry, this piqued my interest. They all seem to be translations of works originally written in Russian. Can anyone compare and contrast them? Here I list them with his coauthors. Dubrovin, B. A., Novikov, S. P. (1984, 1985, 1990). Modern Geometry — Methods and Applications: Parts 1, 2, 3. Graduate Texts in Mathematics. Springer. Differential Geometry and Topology . (1987). Consultants Bureau. Mishchenko, A. S. (1988). A Course of Differential Geometry and Topology . Mir Publishers. Novikov, S. P. (1990). Basic Elements of Differential Geometry and Topology. Mathematics and Its Application . Kluwer Academic Publishers. Visual Geometry and Topology. (1994). Springer. Mishchenko, A. S. (2009). A Short Course in Differential Geometry and Topology . Cambridge Scientific Publishers. Bonus: His wife also seems to have coauthored a book on topology: Borisovich, Yuri G., Bliznyakov, Nikolai M., Fomenko, Tatyana N., Izrailevich, Yakov. A. (1995). Introduction to Differential and Algebraic Topology. Kluwer Texts in the Mathematical Sciences. Kluwer Academic Publishers. From what I can make of them, book (6) seems to be a trimmed version of (3), and (1) seems to be more geared towards physics despite being a GTM book, but I'm not sure.","['differential-topology', 'book-recommendation', 'differential-geometry']"
3534505,How to prove monotonicity in this case?,"Let $0<a \le 1, \alpha<0$ and $\beta>0$ . How to prove that the function: $$f(x)=\frac{(\Gamma(a)-\Gamma(a,\alpha \ln(\beta x))) (\alpha\ln(x))^a}{(\alpha\ln(\beta x))^a (\Gamma(a)-\Gamma(a,\alpha \ln(x)))},$$ is decreasing for $\beta <1$ and increasing for $\beta>1$ . This question is motivated by the following inequality after drawing the graph for some values with wolfram. I tried the sign of derivative but it is more delicate.","['derivatives', 'monotone-functions']"
3534519,Prove that two random variables are independent distributed normally,"Let $M$ and $N$ are independent random variables distributed Uniform $[0, 1]$ . Define $(M_n)_{n\geq 1}$ and $(N_n)_{n\geq 1}$ which are two independent sequences of iid random variables distributed uniformly over $[−1, 1]$ . Let $Z = \inf \{ n ≥ 1, 0 < M^2_n + N^2_n< 1 \}$ and $X = M_Z \sqrt{\frac{-2 \log (M_Z^2 + N_Z^2)}{M_Z^2 + N_Z^2}}~~$ and $Y = N_Z \sqrt{\frac{-2 \log (M_Z^2 + N_Z^2)}{M_Z^2 + N_Z^2}}$ What is the distribution of $Z$ ? show that $X$ and $Y$ are two independant random variables distributed $N(0,1)$ . Addition A previous required question related to this exercise is asking to show that $X$ and $Y$ are independent random variables distributed $N(0, 1)$ , knowing that: $ X = \sqrt{-2 \log(M)} \cos(2 \pi N)$ and $Y = \sqrt{-2 \log(M)} \sin(2 \pi N)$ I already proved this part using the change of variables transformation. Then to show that they are independent, the joint density of $X,Y$ can factor into separate densities of $X$ and $Y$ .","['independence', 'probability-distributions', 'probability', 'random-variables']"
3534544,Analyticity of determinant formula for Gaussian integral,"It is a well known fact that $\int_{\mathbb{R}^n} e^{-\frac{1}{2}x \cdot A x} dx = \sqrt{\frac{(2\pi)^n}{\det{A}}}$ for real, positive definite $A$ . The left hand side of the equation make sense for any complex-symmetric $A$ with real part positive definite. The left hand side is also analytic in $A$ under this assumption of absolute convergence. The right hand side is analytic in $A$ except for a potential branch cut of the square root function. However I'm unsure under what conditions one can analytically continue this formula. I have produced a counterexample where $A$ is complex symmetric and $\Re{A}$ is positive definite but yet the formula is wrong by a phase, e.g. take $A = e^{i\phi} \text{Id}$ and let $\frac{\pi}{n} < |\phi|< \frac{\pi}{2}$ (obviously we need $n \geq 3$ for this example). Under what circumstances is the formula valid?","['complex-analysis', 'linear-algebra', 'analytic-continuation', 'gaussian']"
3534545,$X^*$ is separable then $X$ is separable [Proof explanation],"$X^*$ is separable then $X$ is separable Proof: Here is my favorite proof, which I think is simpler than both the one suggested by David C. Ullrich and the one I had given earlier, elaborating on David Mitra’s hint. It uses only the Hahn–Banach theorem, but not Riesz’s lemma. It is based on the hint presented in Exercise 5.25, Folland (1999, p. 160) . If $X^*$ is separable, let $\{f_n\}_{n\in\mathbb N}$ be a countable dense subset of it. By the definition of the operator norm $$\|f_n\|\equiv\sup_{\substack{x\in X\\\|x\|\leq 1}}|f_n(x)|,$$ it is possible, for each $n\in\mathbb N$ , to choose some $x_n\in X$ such that $\|x_n\|\leq 1$ and $$|f_n(x_n)|\geq\frac{1}{2}\|f_n\|\tag{$\clubsuit$}$$ (if $f_n=0$ , then simply choose $x_n=0$ ; if $\|f_n\|>0$ , use the definition of the supremum). Let $C\equiv\{x_1,x_2,\ldots\}$ . I claim that $\operatorname{span} C$ is dense, which implies that $X$ is separable (see the last claim in my previous post). To see this, suppose, for the sake of contradiction, that $\operatorname{span} C$ is not dense; then $Y\equiv\overline{\operatorname{span} C}$ is a proper closed subspace. By the Hahn–Banach theorem, it is possible to choose $f\in X^*$ such that \begin{align*}
f(y)=&\,0\quad\forall y\in Y,\\
\|f\|=&\,1;
\end{align*} see again Theorem 5.8(a) in Folland (1999, p. 159) . Since $\{f_n\}_{n\in\mathbb N}$ is dense in $X^*$ , there exists some $n\in\mathbb N$ such that $\|f_n-f\|< 1/3$ . But then \begin{align*}
|f_n(x_n)|=|f_n(x_n)-\underbrace{f(x_n)}_{=0}|\leq\|f_n-f\|<\frac{1}{3},\tag{$\diamondsuit$}
\end{align*} whereas \begin{align*}
1=\|f\|\leq\|f-f_n\|+\|f_n\|<\frac{1}{3}+\|f_n\|,
\end{align*} so that $\|f_n\|>2/3$ . Putting this into ( $\diamondsuit$ ), $$|f_n(x_n)|<\frac{1}{3}<\frac{1}{2}\|f_n\|,$$ which contradicts ( $\clubsuit$ ). I don't understand the part apply Hahn Banach Theorem. This part in special: $\begin{align*}
f(y)=&\,0\quad\forall y\in Y,\\
\|f\|=&\,1;
\end{align*}$ Can someone clarify me that part in special? Thanks for answer! The answer is of this post: Showing $X^*$ is separable implies $X$ is separable using the Riesz lemma","['proof-explanation', 'hahn-banach-theorem', 'functional-analysis']"
3534566,Bernoulli First Order ODE,I want to know if my answer is equivalent to the one in the back of the book. if so what was the algebra? if not then what happened? $$x^2y'+ 2xy = 5y^3$$ $$y' = -\frac{2y}{x} + \frac{5y^3}{x^2}$$ $n = 3$ $v = y^{-2}$ $-\frac{1}{2}v'=y^{-3}$ $$\frac{-1}{2}v'-\frac{2}{x}v = \frac{5}{x^2}$$ $$v'+\frac{4}{x}v=\frac{-10}{x^2}$$ this is now a first order linear ODE where: $$y(x)=\frac{1}{u(x)}\int u(x)q(x)$$ $u(x)=e^{4\int\frac{1}{x}}=x^4$ $q(x) = \frac{-10}{x^2}$ $$\frac{1}{x^4}\int x^4 \frac{-10}{x^2}=\frac{1}{x^4}\frac{-10x^{2+1}}{2+1}+C$$ which leaves us with : $$\frac{1}{y^2} = \frac{-10}{3x}+x^{-4}C$$ naturally $$y^2= \frac{1}{\frac{-10}{3x}+x^{-4}C}$$ The book states the answer as being: $$y^2= \frac{x}{2+Cx^5}$$,"['integration', 'calculus', 'algebra-precalculus', 'ordinary-differential-equations']"
3534635,$p$-adic metric,"This is a question from Robert Strichartz: the way of analysis, page 385. He defines a $p$ -adic metric on $\mathbb{Z}$ as follows. $p$ is a fixed prime. For any integer $z$ , we have $z = \pm \sum_{j=0}^N a_j p^j$ . $$|z|_p = p^{-k}$$ where $k$ is the smallest integer such that $a_k \neq 0$ . (a) Show that $d(x, y) = |x - y|_p$ is a metric. (b) Show that $d(x, z) \leq \max\left(d(x, y), d(y, z) \right)$ My understanding: First, I am assuming $x - y$ should be an integer. Second, from what he wrote, it does not follow that $|0|_p = 0$ so I am going to assume that. Any hints about how to start proving the triangle inequality? The answer to part (b) implies the triangle inequality only if I can solve part (b) without using the fact that $d$ is a metric.","['metric-spaces', 'real-analysis']"
3534653,Derivative of Square Root of Matrix with respect to a Scalar,"Let $X(\Omega)$ be a positive-semi-definite matrix which is a function of a set of parameters $\Omega$ . I am interested in both cases where the matrix is real, or is Hermitian. What is the derivative of the square root of this matrix with respect to an individual parameter $\Omega_i$ , i.e $
{\partial_{\Omega_i}\sqrt{X(\Omega)}}
$ ? 
Can this derivative be reduced to a form in terms of ${\partial_{\Omega_i}X(\Omega)}$ ?","['matrices', 'matrix-equations', 'derivatives', 'matrix-calculus']"
3534661,Searching vertices with minimal “variance”,"Suppose $\Gamma(V, E)$ is a strongly connected oriented graph and $|V| = n$ . Let’s define distance from vertex $v_1$ to vertex $v_2$ as the minimal possible length $d(v_1, v_2)$ of an oriented path from $v_1$ to $v_2$ . Let’s define variance of a vertex $v$ as $\sum_{u \in V} (d(v, u))^2$ . What is the computational complexity of finding a vertex with minimal variance? The graph is given by its adjacency matrix. On one hand, it is clearly $\Omega(n^2)$ because it takes that time to process the input. On the other hand, there exists an algorithm that works for $O(n^3)$ : Firstly, we use DFS to determine the distances from any vertex to any vertex. DFS is $O(n^2)$ and will be initiated $n$ times. Thus $O(n^3)$ .  Then we calculate the variances of each vertex using the distances. That’s $O(n^2)$ . And then we choose the vertex with the minimal one. Thats $O(n)$ . $O(n^3) + O(n^2) + O(n) = O(n^3)$ . But maybe some better algorithm exists...","['graph-theory', 'combinatorics', 'discrete-mathematics', 'algorithms', 'computational-complexity']"
3534670,Show that the collection of cylinders form an algebra.,"I am working on an exercise in stochastic process stating that: Denote $\mathbb{R}^{\mathbb{T}}$ to be the set of all functions $x:\mathbb{T}\longrightarrow\mathbb{R}$ , where $\mathbb{T}$ is some index sets. Let $B\in\mathcal{B}(\mathbb{R}^{n})$ , then the cylinder set is defined by $$\mathcal{C}(t_{1},\cdots, t_{n}, B):=\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{n}})\in B\}.$$ Show that the set of all cylinders form an algebra. I have some attempt, and I think I showed the closure under complement, but I was stuck in the closure under finite intersection. Here is my attempt: Denote $\mathfrak{C}$ to be the collection of all cylinder sets. To show the closure under complement, let $E\in\mathfrak{C}$ , then $E$ can be written as $$E=\mathcal{C}(t_{1},\cdots, t_{n}, B)=\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{n}})\in B\},$$ for some $B\in\mathcal{B}(\mathbb{R}^{n})$ and $t_{1},\cdots, t_{n}\in\mathbb{T}$ . Then, \begin{align*}
E^{c}&=\{y\in\mathbb{R}^{\mathbb{T}}:(y_{t_{1}},\cdots, y_{t_{n}})\notin B\}\\
&=\{y\in\mathbb{R}^{\mathbb{T}}:(y_{t_{1}},\cdots, y_{t_{n}})\in B^{c}\},
\end{align*} but the complement of a Borel set is still a Borel set, so the last set is still a cylinder set. Thus, $E^{c}\in\mathfrak{C}$ . But is it necessary that if $(y_{t_{1}},\cdots, y_{t_{n}})\notin B$ then $(y_{t_{1}},\cdots, y_{t_{n}})\in B^{c}$ ? if so, why? To show the closure under finite intersection, let $C_{1}, C_{2}\in\mathfrak{C}$ , then $$C_{1}=\mathcal{C}(t_{1},\cdots, t_{n}, B_{1})=\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{n}})\in B_{1}\},$$ $$C_{2}=\mathcal{C}(s_{1},\cdots, s_{n}, B_{2})=\{y\in\mathbb{R}^{\mathbb{T}}:(y_{s_{1}},\cdots, y_{s_{n}})\in B_{2}\},$$ for some $B_{1}, B_{2}\in\mathcal{B}(\mathbb{R}^{n})$ and $t_{1},\cdots, t_{n}, s_{1}\cdots, s_{n}\in\mathbb{T}$ . Then $$C_{1}\cap C_{2}=\{z\in\mathbb{R}^{\mathbb{T}}:(z_{t_{1}},\cdots, z_{t_{n}})\in B_{1}, (z_{s_{1}},\cdots, z_{s_{n}})\in B_{2}\},$$ but what I should do next? Thank you so much!","['stochastic-processes', 'measure-theory', 'solution-verification', 'probability-theory']"
3534676,"Evaluate $\displaystyle\int_V xy^9z^8 (1-x-y-z)^4\,dx\,dy\,dz$","Evaluate $\displaystyle\int_V xy^9z^8 (1-x-y-z)^4\,dx\,dy\,dz,$ where $V$ is the pyramidal region $x, y, z \geq 0, x+ y+ z \leq 1.$ So the integral is equivalent to $$\int_0^1\displaystyle\int_0^{1-z} \int_0^{1-y-z} xy^9z^8(1-x-y-z)^4\,dx\,dy\,dz.$$ I can evaluate this directly, which gives the answer $\dfrac{9!8!4!}{25!},$ but that method is way too tedious. Is there a faster approach?","['integration', 'multivariable-calculus']"
3534684,Conditions under which a $2\times2$ block matrix has complex eigenvalues,"Consider a matrix $A$ in $\mathbb{R}^2$ : \begin{equation}
A=\begin{bmatrix}
0 & -c \\
1 & -b
\end{bmatrix}
\end{equation} Then it can be shown that the matrix has complex eigenvalues if $b^2-4c < 0$ . Can a similar relation be derived also for the elements of a similarly shaped 2x2 block matrix, ie: \begin{equation}
M=\begin{bmatrix}
0_n & -C \\
I_n & -B
\end{bmatrix}
\end{equation} where $B, C$ are matrices in $\mathbb{R}^n$ , such that $\lambda_M$ are complex?","['matrices', 'linear-algebra', 'block-matrices', 'eigenvalues-eigenvectors']"
3534711,"Suppose $\emptyset\neq H\subseteq G$ and if $a, b\in H$, then $a^{-1}b^{-1}\in H$. Prove or disprove that $H \leq G$.","I want to answer the following: Suppose that $H$ is a nonempty subset of a group $G$ with the property that if $a$ and $b$ belong to $H$ then $a^{-1}b^{-1}$ belongs to $H$ . Prove or disprove that this is enough to guarantee that $H$ is a subgroup of $G$ . After looking at this question on someone else's stack exchange question, I saw that the answer is no. However, I was wondering if there is any sort of disproof for this, or if literally the only way to disprove the statement is by showing a specific counterexample. I would appreciate more insight into how we can tell that the statement fails. Specifically, if there is any way to show that the statement fails in general terms or to show that it fails using a one-step subgroup test or something of this nature, it would be greatly appreciated.","['group-theory', 'abstract-algebra']"
3534758,Generalized version of calculating expected minimum value rolled until $5$ is obtained from a fair die,"The following is a modified interview question. Given an $n$ -sided fair die where $n\geq 1.$ You roll a die until you get a $m$ where $1\leq m\leq n$ . 
  Calculate the expected value of the minimum rolled. The original interview question takes $n=6$ (standard fair die) and $m=5$ . 
I mange to solve the problem and I reproduce my attempt below. The expected minimum value rolled is $\frac{137}{60}$ because if $X$ is the minimum value rolled up to and including $5,$ then $$P(X=x) = \frac{1}{x(x+1)} \quad \text{for }x=1,2,3,4 \quad \text{and} \quad P(X=5) = \frac{1}{5}.$$ So, $$E(X) = \sum_{x=1}^5 xP(X=x) = \frac{137}{60}.$$ I am trying to solve the generalized version of the problem. By the same spirit, let $Y$ be the minimum value rolled up to and including $m.$ Then $$P(Y=y) = \frac{(y-1)!}{(y+1)!} = \frac{1}{y(y+1)} \quad \text{for }y =1,2,...,m-1 \quad \text{and}\quad P(Y=m) = \frac{1}{m}.$$ Therefore, $$E(Y) = \sum_{y=1}^m y P(Y=y) = 1 + \sum_{y=1}^{m-1} \frac{1}{y+1} = 1 + \sum_{y=2}^{m} \frac{1}{y}.$$","['discrete-mathematics', 'combinatorics', 'probability']"
3534779,Lighter Version of Mean-Value Theorem Involving Directional Derivatives,"If $f$ is real-valued and $f'(\mathbf{c}+t\mathbf{u};\mathbf{u})$ exists for $t\in [0,1]$ , show there exists a $\theta\in (0,1)$ such that $f(\mathbf{c}+\mathbf{u})-f(\mathbf{c})=f'(\mathbf{c}+\theta \mathbf{u};\mathbf{u})$ . At first I thought maybe I should define an auxliary function with respect to $t$ ? I was thinking $g(t)=f\circ \gamma$ , where $\gamma(t)=(1-t)\mathbf{c}+t(\mathbf{c}+\mathbf{u})$ . Then $g(1)-g(0)$ gets me the left result. However, I am not sure this is what is needed. If the directional derivative exists, does this mean $f'(\mathbf{c}+t\mathbf{u};\mathbf{u})=\sum_{k=1}^{n}D_{k}f(\mathbf{c}+t\mathbf{u})u_{k}$ ? I don't think I can assume this because $f$ is not assumed differentiable. A lot of problems are due to the fact that $f$ is not assumed differentiable. How do I apply the Mean-Value Theorem? Any tips are appeciated on what I should begin doing!","['multivariable-calculus', 'real-analysis']"
3534863,Covariant derivative is orthogonal projection,"I'm studying some Riemannian geometry and am having difficulty understanding some calculations and notions with the covariant derivative. In particular I am currently stuck on the following: Let $S^n \subset \mathbb{R}^{n+1}$ be the usual unit sphere with the induced Riemannian metric from the Euclidean metric of $\mathbb{R}^{n+1}$ . If $\gamma : [a,b] \rightarrow S^n$ a curve and $V(t)$ a vector field on $S^n$ along the curve $\gamma$ , that is, $V(t) \in T_{\gamma(t)} S^n$ for all $t \in [a,b]$ , show the covariant derivative (defined using the Levi-Civita connection on $S^n$ is given by: $$
\frac{DV}{dt} = \text{pr}_{T_{\gamma(t)}S^n}(V'(t))
$$ where pr denotes orthogonal projection and $V'(t)$ is the usual derivative in $\mathbb{R}^{n+1}$ . Bonus: generalise this to arbitrary submanifold $N$ of a Riemannian manifold $(M,g)$ with the induced metric on $N$ . My idea was to expand the covariant derivative as follows: $$
\nabla_{\dot{\gamma}} V = \sum_{i=1}^{n} \left(\frac{dV_i}{dt} + \sum_{j,k=1}^{n} \Gamma_{jk}^{i} \frac{d\gamma_j}{dt} V_k(t)\right)\frac{\partial}{\partial x_i}
$$ and then hope to simplify this via calculating the Christoffel symbols or something of the kind but I'm having some trouble from here.","['riemannian-geometry', 'differential-geometry']"
3534866,Probability and expected steps for two ants to meet on cube,"Here I present an extension to the famous ant on a cube question: Two ants, A and B, are placed on diametrically opposite corners of a
  cube. With every step, each ants move from one vertex to an adjacent vertex (with 1/3 probability of moving along each of the joining edges). What is i) the probability that A and B collide before either
  ant reaches the diametrically opposite corner; and ii) the expected
  number of steps before they collide? I fully understand how the law of iterated expectations work for a single ant reaching the diametrically opposite corner, however I am unsure of how to extend it for this case. I read in a separate question (lost the link sadly, please edit if you find it) about two players meeting on a random walk, and how characteristic functions were involved, but I did not really understand it. Could someone provide some insight? Cheers! Edit: second part makes more sense after drawing the Markov chain, could someone prod me in the right direction for constructing the Markov chain for the first part?","['expected-value', 'conditional-expectation', 'probability']"
3534871,Prove that $x + \frac{2x^3}{3} + \cdots + \frac{2\cdot 4 \cdot \cdots 2nx^{2n+1}}{3\cdot 5 \cdot (2n+1)}+\cdots = \frac{\arcsin(x)}{\sqrt{1-x^2}}$,"Prove that $x + \dfrac{2x^3}{3} + \cdots + \dfrac{2\cdot 4 \cdot \cdots 2nx^{2n+1}}{3\cdot 5 \cdot (2n+1)}+\cdots = \dfrac{\arcsin(x)}{\sqrt{1-x^2}}.$ Let $f(x) = x + \dfrac{2}3x^3 +\dfrac{8}{15}x^5 + \cdots + \dfrac{1}2n!\cdot\dfrac{n!}{(2n+1)!}(2x)^{2n+1}+\cdots.$ Then $xf(x) = x^2 + \dfrac{2}3 x^4 + \dfrac{8}{15}x^6 + \cdots + \dfrac{1}{4}n!\cdot \dfrac{n!}{(2n+1)!}(2x)^{2n+2}+\cdots.$ Also, $f'(x) = 1 + 2x^2 + \dfrac{8}3 x^4 + \cdots + n!\cdot \dfrac{n!}{(2n)!}(2x)^{2n}+\cdots$ and $(1-x^2)f'(x) =1+x^2 + \dfrac{2}3 x^4 + \cdots + [n!\cdot \dfrac{n!}{(2n)!}2^{2n}-(n-1)!\cdot \dfrac{(n-1)!}{(2n-2)!}2^{2n-2}]x^{2n}+\cdots\\
=1+x^2 + \dfrac{2}3 x^4 + \cdots + \dfrac{1}{4}\cdot(n-1)!\cdot \dfrac{(n-1)!}{(2n-1)!}\cdot(2x)^{2n}+ \cdots.$ Hence the derivative of $\sqrt{1-x^2}f(x)$ is $\sqrt{1-x^2}f'(x)-\dfrac{xf(x)}{\sqrt{1-x^2}} = \dfrac{1}{\sqrt{1-x^2}}((1-x^2)f'(x) - xf(x))=\dfrac{1}{\sqrt{1-x^2}}.$ Integrating, we see that $\sqrt{1-x^2}f(x) =\arcsin(x)+C.$ Plugging in the constant $C=0$ and dividing both sides by $\sqrt{1-x^2}$ gives the desired result. Are there other approaches to solving this problem?","['integration', 'calculus', 'derivatives']"
3534902,Finding the largest minimum,"Let $A$ be the $5\times 5$ matrix $\begin{bmatrix}11& 17 & 25 & 19 & 16\\
24& 10 & 13 & 15 &3 \\
12& 5 & 14 & 2 & 18\\
23 & 4 & 1 & 8 & 22\\
6& 20 & 7 & 21 & 9\end{bmatrix}.$ Find the group of $5$ elements, one from each row and column, whose minimum is maximized and prove it so. I think this problem involves maximizing the choices picked per column. For the $2$ nd and $4$ th columns, we may only pick one of $20$ and $21.$ Similarly, for the $2$ and $4$ th rows, we may only pick one of $23$ and $24.$ For the first row, we can pick only one of $25, 16, 17, 19.$ how can i continue from here?","['matrices', 'latin-square']"
3534921,What percent of primitive Pythagorean triples have an even number as their smallest leg?,"What I'm trying to ask here is, if you take a larger and larger set of consecutive primitive Pythagorean triples, what percent of that set will have an even number as their smallest leg? Ex: 8,15,17. There's a way to generate a Pythagorean triple for every odd integer, $a^2+(\frac{a^2-1}{2})^2=(\frac{a^2-1}{2}+1)^2,$ but Pythagorean triples that have a even number as their smallest leg are not so easy. Can anybody help/give suggestions? Thanks!","['number-theory', 'statistics', 'pythagorean-triples']"
3534968,Integrate $\int_{0}^{\pi} \frac{1}{1+3^{\cos x}} dx.$,"I am not able to solve the following integration, $$\int_{0}^{\pi} \frac{1}{1+3^{\cos x}} dx.$$ I have tried in different ways but most good one I think, \begin{align*} \int_{0}^{\pi} \frac{1}{1+3^{\cos x}} dx &= \int_{0}^{\pi} \frac{\sin x}{\sin x (1+3^{\cos x})} \\ &= \int_{-1}^{1} \frac{dz}{\sqrt{1-z^2} (1+3^{z})}.\end{align*} Now how could I proceed. Please help me.","['integration', 'definite-integrals']"
3534998,Proving $(f^{-1}(x))'\cdot f'(f^{-1}(x))\equiv 1$,"I was playing around with an integral formula I came across recently: $$\int g'(x)f(g(x))f'(g(x))dx=\frac12f^2(g(x))+C$$ I decided to try $g(x)=f^{-1}(x)$ to see what happens. Note I'm assuming inverses and differentials exist for the function It gives: $$\int (f^{-1}(x))'\cdot x\cdot f'(f^{-1}(x)) dx=\frac12f^2(f^{-1}(x))+C=\frac{x^2}{2}+C$$ But, $$\int x dx =\frac{x^2}{2}+C$$ so this implies that the rest of the integrand is equivalent to $1$ , but I can't seem to find a way to show that fact directly, without the use of the integral formula.","['inverse-function', 'derivatives', 'real-analysis']"
3535042,Topological manifolds which cannot be embedded into any Euclidean space,"I take the definition that a (topological) manifold is a topological space which is second-countable, Hausdorff, and locally Euclidean. Munkres proves that every compact manifold can be embedded into some Euclidean space. What is an example of an (topological) $m$ -manifold which cannot be embedded into $\mathbb{R}^n$ for any $n\in\mathbb{N}$ ? What is the smallest $m\in\mathbb{N}$ for which such an $m$ -manifold exists?","['manifolds', 'general-topology']"
3535047,Correlated discrete random variable generated by Markov Chains,"I am dealing with following problem. I have $n=10$ weather stations and for each station I have information about wind direction: a discrete variable with 4 states $\{s, n, w, e\}$ . If I need generate wind direction scenarios for one station I am using a first order Markov chain. But now I need to generate scenarios for all weather stations and i need to include correlation between wind directions for each stations. Any idea how to model this  situation?","['correlation', 'statistics', 'markov-chains']"
3535051,Gradient of exponential scalar function of a matrix variable,"Write the explicit formula of the gradient of $$ E(u)= \sum_{i=2}^{n-1} \sum_{j=2}^{m-1} \exp \left( - \frac{(u_{i+1,j} - u_{i,j-1})^2 + (u_{i,j+1} - u_{i-1,j})^2}{2 {\cal E} ^ 2}  \right) $$ with respect to $n \times m$ matrix $u$ , and where ${\cal E} > 0$ is a given
  constant. Show all the steps of your calculations. Define fourth-order tensors $\Omega_1$ and $\Omega_2$ with components $$\eqalign{
\Omega_{1ijkl} &= \begin{cases}
+1  &{\rm if}\quad(k = i+1)\;{\rm and}\;(l = j) \\
-1  &{\rm if}\quad(k = i)\;{\rm and}\;(l = j-1) \\
\;\;\,0  &{\rm otherwise}\ \
\end{cases} \\
}$$ and $$\eqalign{
\Omega_{2ijkl} &= \begin{cases}
+1  &{\rm if}\quad(k = i)\;{\rm and}\;(l = j+1) \\
-1  &{\rm if}\quad(k = i-1)\;{\rm and}\;(l = j) \\
\;\;\,0  &{\rm otherwise}\ \
\end{cases} \\
}$$ and use it to define the following matrices $$\eqalign{
A_1 &= \Omega_1:U\quad&\implies A_{1ij}
 = \sum_{k=1}^n\sum_{l=1}^m\Omega_{1ijkl} U_{kl} \\
A_2 &= \Omega_2:U\quad&\implies A_{2ij}
 = \sum_{k=1}^n\sum_{l=1}^m\Omega_{2ijkl} U_{kl} \\
X_1 &= A_1\odot A_1\quad&\implies X_{1ij} = A_{1ij}A_{1ij} \\
X_2 &= A_2\odot A_2\quad&\implies X_{2ij} = A_{2ij}A_{2ij} \\
X_3 &= \frac{-1}{2 {\cal E} ^ 2} (X_1 + X_2) \quad&\implies X_{3ij} = \frac{-1}{2 {\cal E} ^ 2} ( X_{1ij} + X_{2ij}) \\
C &= \exp(X_3)\quad&\implies C_{ij} = \exp(X_{3ij}) \\
\quad&\implies dC = C\odot dX_3 \\
B &= (\frac{-1}{ {\cal E} ^ 2} ((A_1 : \Omega_1 ) + (A_2 : \Omega_2))) \\
}$$ where $(:)$ denotes the double-dot product, $(\odot)$ denotes the element-wise product, and all functions are applied element-wise on their arguments. $$\eqalign{
m &= {\tt1}:C \\
dm
 &= {\tt1}:dC \\
 &= {\tt1}:(C\odot dX_3) \\
 &= C:dX_3 \\
 &= C:(\frac{-1}{2 {\cal E} ^ 2} (2A_1 \odot dA_1 + 2A_2 \odot dA_2 ) ) \\
 &= C:(\frac{-1}{ {\cal E} ^ 2} (A_1 \odot dA_1 + A_2 \odot dA_2 ) ) \\
 &= C:(\frac{-1}{ {\cal E} ^ 2} (A_1 \odot \Omega_1:dU + A_2 \odot \Omega_2:dU ) ) \\
 &= C:(\frac{-1}{ {\cal E} ^ 2} ((A_1 : \Omega_1 ) + (A_2 : \Omega_2))) \odot dU \\
 &= C:B \odot dU \\
 &= C \odot B : dU \\
\frac{\partial{\cal m}}{\partial U} &= C \odot B \\
}$$ Is it correct? If it is not correct why? Can anybody help me? Tnx","['matrices', 'matrix-calculus', 'derivatives']"
3535094,Maximum probability to win a number guessing game,"The following is an interview question from Jane Street. A number from $0$ to $1$ is put on a piece of paper. You and person $X$ are playing a game. You must get less than the number on the paper, but above $X$ ’s guess. You know $X$ chooses to pick a number at random. What is lowest number you can choose to maximize your probability of winning? An answer given at glassdoor seems to be $0.25.$ But the explanation there does not seem rigorous.","['recreational-mathematics', 'probability-theory', 'probability']"
3535106,"Find angle in triangle $ABC$ with cevian line $AD$, such that $AB=CD$.","As you can see in the picture, there is a triangle $ABC$ with $∠C=30°$ and $∠B=40°$ . Now we assuming that $AB=CD$ , try to find the exact value of $∠CAD$ . My attempt: Denote $∠CAD$ by $x$ , we know that $$\frac{\sin C}{AD}=\frac{\sin{x}}{CD},\quad\frac{\sin B}{AD}=\frac{\sin{(x+C)}}{AB}$$ Then we have ( $∠C=30°=\frac{\pi}{6},∠B=40°=\frac{2\pi}{9}$ ) $$\frac{\sin{\frac{\pi}{6}}}{\sin{x}}=\frac{\sin{\frac{2\pi}{9}}}{\sin{(x+\frac{\pi}{6})}}=\frac{AD}{AB}$$ Everything looks ok so far, but I have trouble solving the equation. What's more, Wolfram tells me that the answer is $x=\frac{5\pi}{18}$ . This exercise is in my sister's assignment, so I think this exercise should have a high-school (or high-school olympic) level answer. More: The exercise appears in geometry part, so a pure geometric method will be better.","['euclidean-geometry', 'geometry', 'triangles', 'trigonometry', 'geometric-transformation']"
3535121,Evaluate $\lim_{n\to \infty}\sqrt[n]{\frac{(17n)!}{(n!)^{17}}}$,Evaluate $$\lim_{n\to \infty}\sqrt[n]{\frac{(17n)!}{(n!)^{17}}}$$ I know that if we can show that $\lim_{n\to \infty}\frac{a_{n+1}}{a_n}=L$ then $\lim_{n\to\infty}\sqrt[n]{a_n}=L$ So we have to look at $$\lim_{n\to \infty}\frac{a_{n+1}}{a_n}=\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{[17(n+1)]!}{[(n+1)!]^{17}}}}{\sqrt[n]{\frac{(17n)!}{(n!)^{17}}}}$$ I managed to continue to: $$\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)[17n]!}{(n+1)^{17}[(n!)]^{17}}}}{\sqrt[n]{\frac{(17n)!}{(n!)^{17}}}}$$ So it is of the form: $$\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)a}{(n+1)^{17}b}}}{\sqrt[n]{\frac{a}{b}}}=\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)}{(n+1)^{17}}}\sqrt[n+1]{\frac{a}{b}}}{\sqrt[n]{\frac{a}{b}}}=\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)}{(n+1)^{17}}}\sqrt[n+1]{\frac{a}{b}}}{\sqrt[n]{\frac{a}{b}}}=\lim_{n\to \infty}{\sqrt[n+1]{\frac{(n+1)}{(n+1)^{17}}}\sqrt[n(n+1)]{\frac{a}{b}}}$$ But I can see how to proceed,"['limits', 'calculus', 'factorial', 'sequences-and-series']"
3535139,Simplify binomial expression,"I have this expression $$\sum_{k=0}^{10} {30 \choose k}{30 \choose 10-k}$$ I studied Pascals triangle, and also the binomial coefficients, but didn't manage to figure out how to solve this one.","['binomial-coefficients', 'discrete-mathematics']"
3535217,In how many ways can a permutation cycle be decomposed as a product of transpositions?,"I know that every permutation of a finite set can be decomposed into product of disjoint cycles and every cycle can be decomposed into product of transpositions (cycles of length 2). However the decomposition into transpositions is not unique. I'm aware of at least two ""algorithms"" for decomposing a cycle into a product of transpositions: $$(a_1, a_2, \ldots, a_n) = (a_1, a_n)(a_1, a_{n - 1}) \cdots (a_1, a_2)$$ $$(a_1, a_2, \ldots, a_n) = (a_1, a_2)(a_2, a_3) \cdots(a_{n - 1}, a_n)$$ Is there a theorem for counting all possible ways of decomposing a cycle into a product of transpositions (given that I only care about decomposition into $n-1$ transpositions)?","['symmetric-groups', 'group-theory', 'finite-groups', 'permutation-cycles']"
3535237,Evaluate limit of the form $0^{\infty}$,$$\lim_{n\to\infty}\left(\frac{n^4-3\cdot n^3-n^2+2\cdot n-1}{n^5+n^4-n^3-3\cdot n^2-3\cdot n+1}\right)^{\frac{6\cdot n^5-2\cdot n^4-2\cdot n^3+n^2-2\cdot n}{9\cdot n^4-2\cdot n^3+n^2+3\cdot n}}$$ It is $$\lim_{n\to \infty}\left(\frac{1}{n}+o\left(\frac{1}{n^2}\right)\right)^{(n+o(1))}$$ How can I bring it to a form that I can compute?,"['limits', 'calculus']"
3535278,Is a smooth surjective map between disks which is injective a.e. everywhere injective?,"Let $0<\lambda$ , and let $D_1,D_{\lambda} \subseteq \mathbb{R}^2$ be the closed Euclidean disks of radii $1$ and $\lambda$ respectively. Suppose we have a smooth surjective map $f:D_1 \to D_{\lambda}$ , and that for almost every $y \in D_{\lambda}$ , the set $f^{-1}(y)$ is a singleton. Is it true that $f$ is injective? i.e. can we deduce that $f^{-1}(y)$ is a singleton for every $y \in D_{\lambda}$ , not just for almost every $y$ ? Does the answer change if we assume in addition that $\det(df_x)=\lambda^2$ for every $x \in D_{1}$ ?","['measure-theory', 'geometric-measure-theory', 'real-analysis', 'differential-topology', 'differential-geometry']"
3535384,Is an elliptic curve a function?,I am currently reading the book Analysis 1 from Terence Tao. The way he defines a function says that it should pass the vertical line test. Sometimes ago I came across an elliptic curve and saw that it doesn't pass the vertical line test. Is the elliptic curve a function ?,"['algebra-precalculus', 'functions']"
3535389,Probability of Detection,"There exist radars used for detecting aircrafts passing through the state of Iowa.  The state only has three radars, with each having a 25% chance of failing to detect a plane in the state.  Suddenly, a plane has entered the state. What is the probability the plane is detected? Let’s say the plane was detected.  What is the probability that at least two of the radars detected the plane? For the first question I computed $(1-.25)^3$ since there are three aircrafts and it would be a $75%$ percent chance the planes are detected. For the second part I am less sure but would it be $(.25)(.75)^2$ since there are two radars that detect and one that doesn’t?","['combinatorics', 'probability']"
3535406,"Reflexivity of $(C^1([a,b]),\left\lVert \cdot\right\rVert_{\infty})$","I know $(C^1([a,b], \left\lVert\cdot\right\rVert_{C^1})$ is not reflexive (and neither $(C^0([a,b]),\left\lVert \cdot\right\rVert_{\infty})$ is), but I was wondering if maybe using a weaker norm we can make it reflexive (altough I don't think it is true). I know that if $E$ is a reflexive space then the closed unit ball is weakly sequentially compact, and $B_{(C^1([a,b]),\left\lVert \cdot\right\rVert_{\infty})}\subset B_{(C^0([a,b]),\left\lVert \cdot\right\rVert_{\infty})}$ which is not weakly sequentially compact, but I don't know how this would help me to conclude. Moreover, I suspect $(C^1([a,b]),\left\lVert \cdot\right\rVert_{\infty})$ is not closed in $(C^0([a,b]),\left\lVert \cdot\right\rVert_{\infty})$ (take $f_n(x)=|x|^{1+\frac{1}{n}}$ : $f_n$ 's are continuous but they're derivatives are not), but again, how closedness could help me is a mistery. Any hint would be appreciate, thanks in advance.","['general-topology', 'functional-analysis', 'reflexive-space']"
3535466,Is the graph of a continuous function homeomorphic to its domain?,"Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be a continuous real function and we consider its graph identified by the subset: $$\mathcal{G}(f)=\{(x,f(x))\in\mathbb{R}^{n+1}:x\in\mathbb{R}^n\}$$ of $\mathbb{R}^{n+1}$ equipped with the usual euclidean topology that induced on $\mathcal{G}(f)$ a subspace topology. Well with these condictions we demonstrate that $\mathcal{G}(f)$ is homeomorphic to $\mathbb{R}^n$ . So we consider the function $$ h:\mathcal{G}(f)\owns(x,f(x))\rightarrow x\in \mathbb{R}^n$$ since the domain of $f$ is $\mathbb{R}^n$ , clearly $h$ is surjective on $\mathbb{R}^n$ and then is also injective because if $h(x,f(x))=h(y,f(y))$ then $x=y$ ; now to prove the assertion we have to demonstrate of that $h$ and $h^{-1}$ are continuous funcion or that $h$ is open and continuous, but unfortunately I'm not be able to do this so can someone help me?","['general-topology', 'differential-geometry']"
3535509,What is $x$ if $x^{x^x} = {(1/2)}^{\sqrt 2}$?,"What is $x$ if $x^{x^x} = {(1/2)}^{\sqrt 2}$ ? Answer provided more or less like this : \begin{align}{\left(\frac 12\right)}^{\sqrt 2}&=\frac1{2^{\sqrt 2}}\\
2^{\sqrt 2} &= 2^{(2^{(2/4)})} \\
&= 2^{(4^{(1/4)})}\\
& = 4^{1/2(4^{(1/4)})} \\
&= 4^{2^{-1}(4^{(1/4)})} \\
&= 4^{4^{-1/2}(4^{(1/4)})}\\
&= 4^{(4^{(-1/4)})}\\
&= 4^{({1/4}^{(1/4)})}  \\
{\left(\frac 12\right)}^{\sqrt 2} &= \frac1{2^{\sqrt 2}} \\
&= \frac1{4^{({1/4}^{(1/4)})}} \\
&= {\left(\frac14\right)}^{({1/4}^{(1/4)})}  \\
x&=\frac 14\end{align} Is there more elegant way that show $x=\frac 14$ the only answer? Edit : source https://youtu.be/d-E5isaIDTA","['alternative-proof', 'algebra-precalculus', 'exponential-function']"
3535514,"Solve system of trigonometric equations $24^2=l^2+45^2-90 l\cos\alpha$, $51^2=l^2+45^2-90 l \cos(\frac{\pi}{3}-\alpha)$","Recently, I have found this problem: If $\alpha\in (0,\pi/3)$ , then find the solutions of the trigonometric system: $$\left\{\begin{matrix}
24^2=l^2+45^2-90 l\cos\alpha
\\ 51^2=l^2+45^2-90 l  \cos(\frac{\pi}{3}-\alpha)
\end{matrix}\right.$$ I have definitely no idea of how to proceed: I think that maybe the sum-substraction method can be useful, but when I reach: $$24^2-51^2=2\cdot l \cdot \left (  \cos\left ( \frac{\pi}{3}-\alpha \right )-\cos\alpha\right )$$ I am stuck. Any idea of how to go further?","['trigonometry', 'systems-of-equations']"
3535575,Prove the inequality: $\int_0^2 \frac{1}{2+\arctan x} dx \geq \ln 2$,"I have started taking calculus 2 class without any specific knowledge of calculus 1. Our current topic is concerning definite integral . Could you please help to explain how I can evaluate the following inequality, also suggest which topics do I have to learn in order to catch up with indefinite integral. $$\int_0^2 \frac{1}{2+\arctan x} dx \geq \ln 2$$","['integration', 'calculus', 'functions', 'inequality', 'derivatives']"
3535612,"Why does the form ""some p(x), q(x)"" expressed logically as $\exists x(A(x) \wedge P(x))$","I have searched the internet for this, but never found an explanation that I find satisfactory, in the sense that I could actually comprehend it (if there was even an explanation). There was not much explanation of this to began with. For the following problem, where we are asked to logically express: All apples have spots. Where the domain is all fruits, then we can create two predicates (open sentences) as follows: A(x) = "" x is an apple"" P(x) = "" x has spots"" And hence the above can be expressed as: $$\forall x(A(x)\implies P(x))$$ At least, according to my distinguished professor. However if we were asked to logically express: Some apples have spots. And using the same two predicates we defined above, P(x) and A(x), the following statement for some apples have spots: $$\exists x(A(x)\implies P(x))$$ is expressed incorrectly. Instead, the correct way to express that ""some apples have spots"", where the domain is fruits, is: $$\exists x(A(x) \wedge P(x))$$ So the question is, why is the above incorrect and the bottom is correct for ""some apples have spots""? Thank you.","['logic', 'discrete-mathematics']"
3535661,Computing $\sum_{n=1}^\infty\frac{(-1)^{n-1}H_n^{(2)}}{(2n+1)^4}$,"I managed to find $$S=\sum_{n=1}^\infty\frac{(-1)^{n-1}H_n^{(2)}}{(2n+1)^4}=20\beta(6)+\frac12\zeta(2)\beta(4)-\frac{7\pi^3}{32}\zeta(3)-\frac{31\pi}{8}\zeta(5)$$ where $\beta(a)$ is the Dirichlet Beta function . and would like to see different approaches. Here is my work Following the same approach here \begin{align}
6S&=6\sum_{n=1}^\infty\frac{(-1)^{n-1}H_n^{(2)}}{(2n+1)^4}\\
&=\sum_{n=1}^\infty(-1)^{n-1}H_n^{(2)}\int_0^1-x^{2n}\ln^3x\ dx\\
&=\int_0^1\ln^3x\sum_{n=1}^\infty(-x^2)^nH_n^{(2)}\ dx\\
&=\int_0^1\frac{\ln^3x\operatorname{Li}_2(-x^2)}{1+x^2}\ dx\\
&=\int_0^\infty\frac{\ln^3x\operatorname{Li}_2(-x^2)}{1+x^2}\ dx-\underbrace{\int_1^\infty\frac{\ln^3x\operatorname{Li}_2(-x^2)}{1+x^2}\ dx}_{x\mapsto 1/x}\\
&=\int_0^\infty\frac{\ln^3x\operatorname{Li}_2(-x^2)}{1+x^2}\ dx+\int_0^1\frac{\ln^3x\operatorname{Li}_2(-1/x^2)}{1+x^2}\ dx\\
&\left\{\small{\text{add the integral} \int_0^1\frac{\ln^3x\operatorname{Li}_2(-x^2)}{1+x^2}\ dx=6S\text{ to both sides}}\right\}\\
12S&=\int_0^\infty\frac{\ln^3x\operatorname{Li}_2(-x^2)}{1+x^2}\ dx+\int_0^1\frac{\ln^3x[\color{red}{\operatorname{Li}_2(-x^2)+\operatorname{Li}_2(-1/x^2)}]}{1+x^2}\ dx\\
S&=\frac1{12}\int_0^\infty\frac{\ln^3x\operatorname{Li}_2(-x^2)}{1+x^2}\ dx+\frac1{12}\int_0^1\frac{\ln^3x[\color{red}{-2\ln^2x-\zeta(2)}]}{1+x^2}\ dx\\
&=\frac1{12}\underbrace{\int_0^\infty\frac{\ln^3x\operatorname{Li}_2(-x^2)}{1+x^2}\ dx}_{I}-\frac16\underbrace{\int_0^1\frac{\ln^5x}{1+x^2}\ dx}_{-120\beta(6)}-\frac{\zeta(2)}{12}\underbrace{\int_0^1\frac{\ln^3x}{1+x^2}\ dx}_{-6\beta(4)}\\
&=\frac1{12}I+20\beta(6)+\frac12\zeta(2)\beta(4)\tag1
\end{align} \begin{align}
I&=\int_0^\infty\frac{\ln^3x\operatorname{Li}_2(-x^2)}{1+x^2}\ dx\\
&=\int_0^\infty\frac{\ln^3x}{1+x^2}\left(\int_0^1\frac{x^2\ln y}{1+yx^2}\ dy\right)\ dx\\
&=\int_0^1\ln y\left(\int_0^\infty\frac{x^2\ln^3x}{(1+x^2)(1+yx^2)}\ dx\right)\ dy\\
&=\int_0^1\frac{\ln y}{1-y}\left(\int_0^\infty\frac{\ln^3x}{1+yx^2}\ dx-\underbrace{\int_0^\infty\frac{\ln^3x}{1+x^2}\ dx}_{0}\right)\ dy\\
&=\int_0^1\frac{\ln y}{1-y}\left(-\frac{\pi^3}{16}.\frac{\ln y}{\sqrt{y}}-\frac{\pi}{16}.\frac{\ln^3y}{\sqrt{y}}\right)\ dy,\quad \sqrt{y}=x\\
&=-\frac32\pi^3\underbrace{\int_0^1\frac{\ln^2x}{1-x^2}\ dx}_{\frac74\zeta(3)}-2\pi\underbrace{\int_0^1\frac{\ln^4x}{1-x^2}\ dx}_{\frac{93}{4}\zeta(5)}\\
&=-\frac{21}{8}\pi^3\zeta(3)-\frac{93}{2}\pi\zeta(5)\tag2
\end{align} Plug $(2)$ and $(1)$ we get $$S=20\beta(6)+\frac12\zeta(2)\beta(4)-\frac{7}{32}\pi^3\zeta(3)-\frac{31}{8}\pi\zeta(5)$$ Thank to @Zacky for spotting couple mistakes in my solution","['integration', 'alternative-proof', 'harmonic-numbers', 'closed-form', 'sequences-and-series']"
3535670,Finding number of real solutions,"The question is to  find number of real solutions of $F(x)=x^3+1=2\sqrt[3]{2x-1}=G(x)$ So first i tried to find the roots of it's derivative , which was not helpful as the equation formed was hard to solve. Then i tried to visualize the graph which was again not very helpful. Luckily i found the above equation is true for $x=1$ . At $x=0$ , $G(x)$ is under $F(x)$ but it's derivative is greater than $F(x)$ till $x=1$ (easy to show). And at $x=1$ also derivative of $G(x)$ is greater than that of $F(x)$ , so here $G(x)$ overtakes (not before $x=1$ ) $F(x)$ . Now after $x=1$ , once the $F'(x)$ becomes greater than $G(x)$ , it would remain like that so $F(x)$ once again over take $G(x)$ and hence we get 2 solutions of for $x>0$ . Then again when we go from $x=0$ to negative $x$ -axis , we will see once $F(x)$ starts decreasing more rapidly than $G(x)$ it would remain that way, so we will get one solution for $x<0$ . So i got the correct answer of $3$ . But this is a lucky solution. I would be happy if you can provide me with any type of solution. There should be a better way to solve this problem right?","['calculus', 'functions']"
3535746,Find all polynomials such that $P (x) = (x-P (0))(x-P (1))...(x-P (n-1))$,"Find all polynomials $P(x)$ of degree $n>0$ with integer coefficients such that, for every real number $x$ , we have: $$P(x) = (x-P (0))(x-P (1))...(x-P (n-1)) $$ I thought that i proved that $P(0) = 0$ but actually the conclusion was wrong, as showed in the comments. I'm back to the incial point. I don't now how to proceed, can somebody help me?","['functions', 'polynomials']"
3535888,"Show $1+a+b+c\leq 2\lfloor\sqrt{at^2+bt+c}\rfloor$ for all $a,b,c,t\in\Bbb N\cup\{0\}$ with $a\neq 0$, $t\ge 2$ and $a,b,c\le t-1$.","Related to a project I'm writing, I came across the problem of showing that $$1+a+b+c \leq 2\lfloor \sqrt{ at^2+bt+c}\rfloor$$ holds for all nonnegative integers $a,b,c,t$ satisfying $a \neq 0$ , $t \geq 2$ and $a,b,c \in \{0,1,\ldots,t-1\}$ . I have checked numerically that it holds true for all valid choices of $a,b,c$ for $t\leq 300$ , but I'm not able to prove that the statement is always true. I'm suspecting that there is some simple argument to why, but unfortunately I don't see it. Can anyone help ? :)","['algebra-precalculus', 'inequality']"
3535922,"Getting a cat, fish, dog, and your lunch across a river","You're trying to get a cat, a fish, a dog, and your lunch across a river, but there's a troll in the way. The troll says, ""I'll allow you to cross the river, but only if you play this game with me. I have a die here showing a cat, a fish, a dog, and your lunch. I'll roll that die, and then you must bring that item across the river, no matter which side it's on. Once you do that, I'll roll the die again. If you can get everything to the other side, I'll let you go."" You quickly realize this is a bad idea: If you leave the cat and fish alone on one side, the cat will eat the fish, and if you leave the dog and lunch alone on one side, the dog will eat your lunch. (If the cat, the fish, and something else are alone on one side, nothing will be eaten. Likewise, if the dog, your lunch, and something else are alone on one side, nothing will be eaten.) You tell this to the troll, who says, ""Fine. When I absolutely need to, I'll re-roll the die to make sure none of your precious cargo is harmed."" Suppose that you make a move when you bring something from one side of the river to the other. (If the troll re-rolls their die, this does not count as a move.) Find the expected number of moves you'll need to make before everything is on the other side of the river. I honestly don't know where to start with this problem and a solution would be greatly appreciated.","['expected-value', 'combinatorics', 'probability']"
3535969,Show that the number of nonisomorphic finite groups of order $n$ is at most $n^{n^2}.$,"I have a question. I am supposed to prove the following theorem: Let $n \in \mathbb{N}$ be a natural number. Show that: the number of nonisomorphic groups of order $n$ is less than or equal to $n^{n^2}$ . My reasoning: A set $G$ is a group if it's equiped with a binary operation/map: $G\times G \rightarrow G, \space (g_1,g_2)\rightarrow g_1 \circ g_2$ . Additionally two groups $G,H$ are isomorphic $G \cong H$ if there exists a isomorphism $f:G \rightarrow H$ . Now, for each such map $G\times G \rightarrow G$ we get a different group. Because there are $n^{n^2}$ such different maps (for $|G|=n$ ), the number of all possible different groups should be $\leq n^{n^2}$ . And since we have different binary operations defined on the same set, we should get different multiplication (Cayley) tables, we would have $f(a)\star f(b)\neq f(a \circ b)$ for $a,b \in G$ and some bijection $f$ . Meaning that, since homomorphisms don't exist, all those different groups are nonisomorphic and we have therefore shown the above theorem to be true. More intuitively: The statement of beeing (non) isomorphic can be translated to (not) having the same multiplication table. If we have a group of order $n$ , then the multiplication table should be of size $n\times n= n^2$ . Each of the $n^2$ entries (in the table) has $n$ possible entries, representing a different choice of the above mentioned binary operation. Because those possibilities multiply, we would have: $n\times \cdots \times n=n^{n^2}$ and each table beeing different, those groups are nonisomorphic. My question: Is my reasoning, in both cases, correct? Am I missing something? How would I prove, more formally,  that for different binary operations defined on the same set we get different nonisomorphic groups? Related questions: Number of distinct groups of order n upto isomorphism, for a fixed integer n. The number of groups of order n(upto isomorphism)is Comment: I am a physicist and don't have major ambitions in abstract algebra, please be nice.","['group-theory', 'abstract-algebra', 'finite-groups']"
3536042,"Differential Entropy and ""Limiting density of discrete points""","I stumbled across the concept of differential entropy and was left puzzled by the Wikipedia page Limiting density of discrete points .
The related ""talk page"" Talk: Limiting density of discrete points added to my confusion. The first points where I struggled is based on the statement that Shannon's differential entropy is not dimensionally correct, which is objected to in the Talk page. The differential entropy is given by $$ h(x) = - \int p(x) \log p(x) \mathrm{d}x $$ The Wiki page argues that as $h$ is to be dimensionless, the probability density must have dimension "" $1 / \mathrm{d}x$ "" , which would result in the logarithm argument not being dimensionless.
This made sense to me actually, as a probability density will not be dimensionless. Yet, on the talk page (second link above), the following objection is presented.
The differential entropy is the limit as $\Delta \to 0$ of a Riemann sum $$ -\sum p(x)\Delta \log \big( p(x) \Delta  \big) $$ which, so it is argued, is dimensionally consistent as $p(x) * \Delta$ is dimensionless.
The latter can be written as $$-\sum p(x)\Delta \log \big( p(x)   \big) -\sum p(x)\Delta \log ( \Delta ) $$ and, while the second term could be proven to vanish,  the first term yields the differential entropy formula. The first question is, who is right in this argument? I understand that a term as $\log (A *B) $ where $A$ and $B$ are quantities with dimensions say of length and inverse of length, can be written as $\log(A) + \log(B)$ . The argument of the logarithm is not dimensionless, but the whole expression is ultimately at least invariant to  a change of units. Yet, in the manipulation presented in the second link, a term vanishes, and the expression is not independent of the units chosen. The second question I have concerns the ""invariant measure"" $m(x)$ used by Jaynes to correct the differential entropy formula, leading to the expression $$ H(x) = - \int p(x) \log \frac {p(x)}{m(x)} \mathrm{d}x $$ I understand how this expression is now dimensionally consistent. Yet its consequences seem quite strange to me.
For example, if a uniform distribution with support over $[a,b]$ is considered, the differential entropy equals $\log[b-a]$ : it depends on the support length, as seems meaningful. IT does however also depend on the system of units chosen to measure length).
To use Jaynes's equation, I believe a legitimate choice is $$m(x)  = \frac{1}{b-a}$$ Then, Jaynes's entropy turns out to equal $0$ , regardless of the support length. The second question then, is this conclusion of mine correct? Would not any constant value do the job, as far as dimensionality is concerned, in lieu of $m(x)$ ? I did read some original papers from Jaynes, but cannot work it out. Adapting, to the best of my understanding, his reasoning to the uniform distribution case I mentioned before, he starts from the discrete entropy expression $$ H_{d} = -\sum_{i} p_i \log(p_i) $$ over discrete points $x_1, x_2, \dots, x_n$ Further by noting $$\lim_{n \to \infty} n (x_{i+1}-x_i) = b-a$$ he writes $$p_i = p(x) \frac{1}{n m(x_i)} $$ which again for the uniform distribution I translate as $$ p_i = \frac{1}{n} = \frac{1}{b-a} \frac{b-a}{n}$$ which does make sense, yet it yields once the sum is turned to an integral, to the term $\log(\frac{p(x)}{n m(x)}) $ which as I said, seems equal to zero for any uniform distribution. I would be most grateful for a clarification. Sorry for the verbosity but I hope that by adding all the passages it will be easier to pinpoint my mistake.","['statistics', 'entropy']"
3536045,Generalization of singular homology of topological spaces to varieties over $k$.,"To define the singular homology of topological spaces we define a sequence of topological spaces $\Delta^n$ and maps $r_i:\Delta^n \rightarrow \Delta^{n+1}$ . I'm kinda skipping a lot of details because someone who is going to be able to answer my question has very likely already seen singular homology. In algebraic geometry consider the varieties over the field $k$ , $k\Delta^n=\{(x_0,...,x_n) \in \mathbb A^{n+1}_k: \sum x_i = 1\}$ and maps $r_i : k\Delta^n \rightarrow k\Delta^{n+1}:(x_0,...,x_n) \mapsto (x_0,...,x_{i-1},0,x_i ,...,x_n)$ , defined for $0 \leq i \leq n+1$ . Then for any variety $X$ over $k$ define $C_i(X)$ to be the free abelian group generated by $\text{Hom}(k\Delta^i,X)$ and the differential $C_i(X) \rightarrow C_{i-1}(X)$ is defined just like in singular homology, namely a map $\phi : k\Delta ^i \rightarrow X$ gets taken to $\sum \phi \circ r_i$ and we extend linearly to formal sums. Then let $H^{\text{var}}_i(X)$ be the homology of $C_*(X)$ at $i$ . How do I compute the homology of some varieties? For finite fields $k$ I should be able to do this numerically... I think? But for fun cases when $k = \mathbb R$ I have zero clue where to even start, for example where do I start with something like $H^{\text{var}}_1(\mathbb A_\mathbb R^2 - \{0,0\})$ ? I know it isn't zero but that's about it. I can't use machinery already existing for singular homology since many theorems are not obvious to prove in algebraic goemetry due to the rigid nature of morphisms of varieties compared to continuous functions. I've managed to prove that $H^{\text{var}}_i (-)$ is homotopy invariant, where a ""homotopy"" is just a map $\mathbb A^1 \times X \rightarrow Y$ but thats about all I have.","['algebraic-geometry', 'algebraic-topology', 'simplicial-stuff']"
3536049,What is the intuition behind the law of quadratic reciprocity?,"Law of quadratic reciprocity states as follows: Law of quadratic reciprocity — Let $p$ and $q$ be distinct odd prime numbers, and define the Legendre symbol as: $$ \left(\frac {q}{p}\right)=\left\{\begin{array}{rl} 1 & \text{if } n^2\equiv q \pmod p \text{ for some integer } n, \\ -1 & \text{otherwise.} \end{array} \right.$$ Then: $${\displaystyle \left({\frac {p}{q}}\right)\left({\frac {q}{p}}\right)=(-1)^{{\frac {p-1}{2}}{\frac {q-1}{2}}}.}$$ I have heard from class that there are hundreds of proof of this theorem. But the proof that I have learned in class is a very elementary one. As we know that many theorems in number theory have some very nice explanations using abstract algebra. Is there a proof of this theorem from the perspective of abstract algebra? And what is the intuition behind it? Thank you!","['number-theory', 'quadratic-residues', 'quadratic-reciprocity']"
3536059,Finding infinitesimal Mobius transformations,"I'm working to understand a bit more about Mobius transformations. I understand that the Mobius group is isomorphic to $SL(2,\mathbb{C})$ , which has as its Lie algebra $\mathfrak{sl}(2,\mathbb{C})$ . However, Mobius transformations are complex functions, whereas the elements of $\mathfrak{sl}(2,\mathbb{C})$ are matrices with trace 0. Can I bring back the connection to functions on the extended complex plane? In particular, I wanted to know whether elements of the algebra could correspond to infinitesimal conformal transformations. In physics, we often think of infinitesimal rotations and translations on a space. I wanted to think of infinitesimal Mobius transformations in the same way.","['complex-analysis', 'lie-algebras', 'mobius-transformation']"
