question_id,title,body,tags
1149012,Bunyakovsky conjecture for cyclotomic polynomials,"This article on Wikipedia: http://en.wikipedia.org/wiki/Bunyakovsky_conjecture says: In fact, it can be shown that if for all natural number $ n $, there exists a natural number $ x > 1 $ such that $ \Phi_n(x) $ is prime, then for all natural number $ n $, there are infinitely many natural number $ x $ such that $ \Phi_n(x) $ is prime. (Where $ \Phi_n(x) $ is the $ n $-th cyclotomic polynomial) However, there is no reference to the proof. Could you please post the proof or a link to it?","['prime-numbers', 'polynomials', 'cyclotomic-polynomials', 'number-theory']"
1149067,How to prove set equivalence with an empty set?,"If I want to prove $A = \emptyset$ for some set A, how would I go? My first thoughts were to prove it like any set equivalence, by assuming that $x \in \emptyset$ which means that $x \in \emptyset$ or $x \in A$ implying $x \in \emptyset \cup A = A$. But I would not know how to go from $y \in A$ to $y \in \emptyset$ which is impossible obviously. I also thought about trying $x \in A$ and try to prove $x \notin A$. Will this imply that $A = \emptyset$?",['elementary-set-theory']
1149085,Implicit differentiation of $e^{x^2+y^2} = xy$,"I just want to reconfirm the steps needed to answer this question. Thank you Find $\dfrac{dy}{dx}$ in the followng: $$e^{\large x^2 + y^2}= xy$$ I got this so far. $\newcommand{\dd}{\mathrm{d}}\frac{\dd x}{\dd y} e^{x^2}\cdot e^{y^2} = \frac{\dd x}{\dd y} xy$ $u=e^{x^2}$: $\frac{\dd u}{\dd x}=2x(e^{x^2})$ $v=e^{y^2}$: $\frac{\dd v}{\dd x}=2y(e^{y^2})\frac{\dd y}{\dd x}$ $u=x$: $\frac{\dd u}{\dd x}=1$ $v=y$: $\frac{\dd v}{\dd x}=\frac{\dd y}{\dd x}$ $$\begin{align}
e^{x^2}\cdot2y(e^{y^2})\frac{\dd y}{\dd x} + e^{y^2}\cdot 2x(e^{x^2}) &= x \frac{\dd y}{\dd x} + y\\
e^{x^2}\cdot2y(e^{y^2})\frac{\dd y}{\dd x} - x \frac{\dd y}{\dd x} &= y - e^{y^2}\cdot 2x(e^{x^2})\\
\frac{\dd y}{\dd x} \left[e^{x^2}\cdot2y(e^{y^2})-x\right] &= y - e^{y^2}\cdot 2x(e^{x^2})\\
\frac{\dd y}{\dd x} &= \frac{y - e^{y^2}\cdot 2x(e^{x^2})}{e^{x^2}\cdot2y(e^{y^2})-x}
\end{align}$$","['calculus', 'derivatives']"
1149113,Finding $P(X\in A)$ given description of $X$,"Consider a random variable $X$ defined on $[0,1]$ as follows: $$X(\omega)=1,\space\space\space\space\space\space \space0\leq\omega<\dfrac{1}{4}$$$$X(\omega)=2\omega^2,\space\space\space\space\space\space\dfrac{1}{4}\leq\omega<\dfrac{3}{4}$$$$X(\omega)=\omega^2,\space\space\space\space\space\space\dfrac{3}{4}\leq\omega\leq1$$ Assume that $P$ is the Lebesgue measure on $[0,1]$. Determine $P(X\in[0,1])$ and $P(X\in[\frac{1}{2},1])$. I am not sure how to do this problem. Actually this is the first type of problem I am facing. I realized that $P(X\in[0,1])=P(\omega\in[0,\dfrac{1}{\sqrt{2}}])+P(\omega\in[\dfrac{3}{4},1])$. But what distribution does $\omega$ follow?","['probability-theory', 'probability-distributions', 'probability']"
1149129,Solving the ODE $y+xy'=x^4 (y')^2$,I am trying to get to the solution which is $$y=c^2 +\frac{c}{x}$$ How would I go about solving this?,['ordinary-differential-equations']
1149155,Topological Invariance of Unique Ergodicity,"Show that unique ergodicity is a topological invariant. Is arguing as follows an overkill (hopefully if the logic is correct â€” I have a feeling that there has to be a way a $T$ -invariant measure has to depend on the $S$ -invariant measure and vice-versa and so that they are unique together)? Let $T$ be a uniquely ergodic transformation acting on the probability space $(X, \mathfrak{A}, \mu)$ and $S$ a transformation acting on $(Y, \mathfrak{B}, \nu)$ . Let $h: (X, \mathfrak{A}) \to (Y, \mathfrak{B})$ be a homeomorphism such that $h \circ T = S \circ h$ . Note that $T^{k}  = h^{-1} \circ S^{k} \circ h$ .  Suppose that $S$ is uniquely ergodic.  We wish to show that $T$ is uniquely ergodic. Let $\phi \in C(X)$ .By one equivalence characterizations of unique ergodicity,we must show that $\frac{1}{n}\sum_{k=0}^{n-1}\phi(T^{k}(x))$ converges to a constant pointwise on $X$ . Since $S$ is uniquely ergodic. We have in particular for a continuous function $\psi = \phi \circ h^{-1} \in C(Y)$ and each point $h(x) = y$ , the time average $\frac{1}{n}\sum_{k=0}^{n-1} (\phi \circ h^{-1})(S^{k}(h(x)) )$ converges pointwise to a constant on $Y$ . But then so is $\frac{1}{n}\sum_{k=0}^{n-1}\phi(T^{k}(x))$ because $\frac{1}{n}\sum_{k=0}^{n-1}\phi(T^{k}(x))= \frac{1}{n}\sum_{k=0}^{n-1} (\phi \circ h^{-1})(S^{k}(h(x)) )$ .","['dynamical-systems', 'measure-theory', 'ergodic-theory', 'real-analysis']"
1149169,"Exponential lower bound for the determiant of a (0,1)-matrix","Give matrices, which only contain 0 and 1, and their determinant grows exponentially.
In other words, show an $n \times n$ matrix for all n, which only contains 0 and 1, and 
$$\det A(n)>d \cdot c^n,$$ where c>1 and d>0. Can't really begin, any ideas? Thanks :)",['matrices']
1149183,"Normed separable space, linearly independent $X_0 \subset X, \ \overline{linX_0} = X $","Could you tell me how to prove that a normed  space $X$ is separable $\iff$ there exists an at most countable set of linearly independent vectors $X_0 \subset X$ such that $ \ \text{lin} X_0$ is dense in$ X $? I know how to prove that if a normed space is separable, then there exists such a set. But I don't know what to do about the opposite direction. Could you help? Thank you","['normed-spaces', 'functional-analysis']"
1149188,Prove that $\nabla \times \vec F =0 \implies \vec F = \nabla f$,"How do I prove that if $\nabla \times \vec F = \vec 0 $ then $\vec F = \nabla f$ for some scalar field $f$? My lecturer only proved the converse, which follows easily from the symmetry of mixed partial derivatives. Preferably I would like a simple method of proof, not something powerful like Stokes theorem.","['multivariable-calculus', 'calculus']"
1149191,Why is the intersection of the empty set the universe? [duplicate],"This question already has answers here : Empty intersection and empty union (7 answers) Closed 9 years ago . I haven't found an answer yet that I can make complete sense of, so I'm asking again to try and clear it up if I have any misconceptions. If one defines $\cap_{i\in I}\alpha_i = \{x:\forall i\in I, x\in\alpha_i\}$ then apparently if $I = \emptyset$ this definition yields the absolute universe. This is just stated as if it is clear why, though I cannot see why. If $i\notin I \forall i$ then there is no set $\alpha_i$ for any $x$ to be a member of...? I know I must be misreading this, but I can't see how by so much... Edit: Let this intersection be $Z$, for convenience.",['elementary-set-theory']
1149270,Probability that binomial random variable is even,"Suppose $X$ is binomial $B(n,p)$. How can I find the probability that $X$ is even ? I know $$P(X = k ) = \frac{ n!}{(n-k)!k!} p^k(1-p)^{n-k} $$ where $X=1,....,n$. Are they just asking to find $P(X = 2m )$ for some $m > 0$ ?",['probability']
1149288,Moebius band not homeomorphic to Cylinder.,"I have been trying to think of a rather basic way of proving this, but it seems a bit elusive. In the case with boundary (looking at them as quotients in $ [0,1] \times [0,1]  $), they can be distinguished from the connectedness of the boundary (thanks Stefan), but I'm interested in the open case (looking at them as quotients in $ [0,1] \times (0,1)  $, i.e. as manifolds without boundary). If there is not such basic way to do this, it would be interesting to read about some advanced methods you guys know about.","['general-topology', 'manifolds']"
1149336,Showing that every field is an integral domain.,"The proof I have starts of with $\;xy=0\;$ in a field. Then $x^{-1}$ exists because it is a field. Then $x^{-1} xy=x^{-1} 0$. Therefore $y=0$. But surely if an integral domain can not have any zero divisors, how can we end the proof by saying $y=0$? Surely then y is a zero divisor and hence the field is not an integral domain?",['abstract-algebra']
1149363,Canonical divisor of product of varieties,"Let $X$ and $Y$ be a projective varieties with the canonical divisors $K_X$ and $K_Y$ respectively. Is it true that the canonical divisor of $X\times Y$ equals $p^*K_X+q^*K_Y$? Here $p:X\times Y\to X$, $q:X\times Y\to Y$ are the projection maps.",['algebraic-geometry']
1149367,Proving $\sum_{n=0}^{\infty }\frac{\sin^4(4n+2)}{(2n+1)^2}=\frac{5\pi ^2}{16}-\frac{3\pi }{4}$,"I dont have an idea to prove it because of exist $\sin(4n+2)^4$
$$\sum_{n=0}^{\infty }\frac{\sin^4(4n+2)}{(2n+1)^2}=\frac{5\pi ^2}{16}-\frac{3\pi }{4}$$",['sequences-and-series']
1149370,Derived version of projection formula,"Let $f \colon X \to Y$ be a continuous map of locally compact spaces. Denote by $Sh(X)$, $Sh(Y)$ the categories of sheaves of $k$-vector spaces for some field $k$ and by $D^b(X)$, $D^b(Y)$ their bounded derived categories. Suppose that $f_!$ has a finite cohomological dimension so that the right adjoint $f^! \colon D^b(Y) \to D^b (X)$ to $\mathbf Rf_!$ exists. I want to show that there is a natural isomorphism
$$
   \mathbf Rf_!(f^{-1} F \otimes G) \cong F \otimes \mathbf Rf_! G, \quad F \in D^b(Y), \; G \in D^b(X).
$$
This isomorphism was used, e.g. in the proof of Theorem 7 (Verdier duality), p. 11 of these notes. I was trying to take an arbitrary $H \in D^b(Y)$ and write
$$
   \mathrm{Hom}_{D^b(Y)} (\mathbf Rf_!(f^{-1} F \otimes G),H) \cong \mathrm{Hom}_{D^b(X)}(f^{-1}F \otimes G,f^! H) \\
   \cong \mathrm{Hom}_{D^b(X)} (G, \mathbf R\mathcal{Hom}(f^{-1} F,f^! H))
$$
Now, if I could use that $\mathbf R\mathcal{Hom}(f^{-1} F, f^! H) \cong f^! \mathbf R \mathcal{Hom}(F,H)$ then I could continue the above sequence of isomorphisms as
$$
   \cong \mathrm{Hom}_{D^b(X)} (G, f^! \mathbf R\mathcal{Hom}(F,H)) \cong \mathrm{Hom}_{D^b(Y)} (\mathbf Rf_! G, \mathbf R\mathcal{Hom}(F,H)) \\
   \cong \mathrm{Hom}_{D^b(Y)}(F \otimes \mathbf Rf_! G, H).
$$
By Yoneda this implies the required statement. But now I have to prove that $\mathbf R\mathcal{Hom}(f^{-1} F, f^! H) \cong f^! \mathbf R \mathcal{Hom}(F,H)$. I'm not sure that it is easier than the original statement. Please, help me.","['category-theory', 'sheaf-theory', 'algebraic-geometry', 'derived-functors']"
1149440,"There is not set of ""dominated set"".","A set $A$ is said to be dominated by ""B"" if there exist an injective function $f:A\rightarrow B$. How can I prove that there is not a set $A$ such that for every set there exist one member in $A$ which dominate it. Any hint? Thanks!",['elementary-set-theory']
1149451,Equivalent Vitali Covering Properties for Differentiation Bases,"N.B. In what follows below, we work exclusively with the Lebesgue measure on $\mathbb{R}^{n}$. Def 1 For each $x\in\mathbb{R}^{n}$, let $\mathcal{B}(x)$ be a collection of bounded open sets $R$ containing $x$ such that there exists a sequence $\left\{R_{k}\right\}\subset\mathcal{B}(x)$ tending to $\left\{x\right\}$. We say that $\mathcal{B}:=\bigcup\mathcal{B}(x)$ is a differentiation basis . If for $R\in\mathcal{B}$, $x\in R$ implies $R\in\mathcal{B}(x)$, we say that $\mathcal{B}$ is a Busemann-Feller differentiation basis . (Hereafter, all differentiation bases will be BF.) Def 2 Let $A\subset\mathbb{R}^{n}$ be a measurable set. $V\subset\mathcal{B}$ is a $\mathcal{B}$-Vitali covering of $A$ if for each $x\in A$ there exists a sequence $\left\{R_{k}\right\}\subset V$ such that $x\in R_{k}$ and $\text{diam }R_{k}\rightarrow 0$. Def 3 We say that a basis $\mathcal{B}$ has the $V_{q}$ property, where $1<q<\infty$, if there exists  constant $C$ such that for every measurable set $A$, for every $\mathcal{B}$-Vitali covering of $A$, and for every $\varepsilon>0$, there exists a countable subcollection $\left\{R_{k}\right\}\subset V$ satisfying $\left|A\setminus\bigcup R_{k}\right|=0$, $\left|\bigcup R_{k}\setminus A\right|\leq\varepsilon$ $\left\|\sum \chi_{R_{k}}\right\|_{L^{q}}\leq C\left|A\right|^{1/q}$ Define the upper and lower derivatives of $\int f$ at $x$ respectively by
$$\overline{D}\left(\int f,x\right):=\sup_{\left\{R_{k}\right\}}\limsup_{k}\dfrac{1}{\left|R_{k}\right|}\int_{R_{k}}f, \ \underline{D}\left(\int f,x\right):=\inf_{\left\{R_{k}\right\}}\liminf_{k}\dfrac{1}{\left|R_{k}\right|}\int_{R_{k}}f$$
where the supremum and infimum are respectively taken over all sequences $\left\{R_{k}\right\}\subset\mathcal{B}(x)$ with $\text{diam} R_{k}\rightarrow 0$. It has been shown by A. Cordoba and R. Fefferman [Theorem 2, p. 2212] that the $V_{q}$ property is equivalent to the property that $\mathcal{B}$ differentiates $\int f$ a.e., for $f\in L_{loc}^{p}$, where $1/p+1/q=1$; i.e.,
$$\overline{D}\left(\int f,x\right)=\underline{D}\left(\int f,x\right)=f(x) \ a.e.,$$ It has also been shown by C.A. Hayes [Theorem 2.4, p. 178] that Definition 3 with condition 2 replaced by condition 2*
$$\left\|\sum_{k}\chi_{R_{k}}-\chi_{\bigcup R_{k}}\right\|_{L^{q}}\leq\varepsilon$$
is equivalent to the differentiation property stated above. My question is whether there is a proof of the equivalence of Cordoba's $V_{q}$ property and Hayes's $V_{q}$ property, which does not use the full result that each is equivalent to the differentiation property.","['measure-theory', 'integration', 'real-analysis']"
1149458,The smooth Nullstellensatz,"Let $n$ be a positive integer, let $f_1, \ldots, f_r : \mathbb{R}^n \to \mathbb{R}$ be smooth functions, let $Z_i = f_i^{-1} \{ 0 \} \subseteq \mathbb{R}^n$, and suppose $Z_1 \cap \cdots \cap Z_r = \emptyset$. Question. Must there exist smooth functions $g_1, \ldots, g_r : \mathbb{R}^n \to \mathbb{R}$ such that $f_1 g_1 + \cdots + f_r g_r = 1$? I believe the answer is no in general. On the other hand: It suffices to show that there exist smooth functions $g_1, \ldots, g_r : \mathbb{R}^n \to \mathbb{R}$ such that $f_1 g_1 + \cdots + f_r g_r$ vanishes nowhere, because we can rescale. Therefore a counterexample will have the property that, for any smooth functions $g_1, \ldots, g_r : \mathbb{R}^n \to \mathbb{R}$, $f_1 g_1 + \cdots + f_r g_r$ vanishes somewhere . If $0$ is a regular value for each $f_i$, then the answer is yes: take a partition of unity $h_1 + \cdots + h_r = 1$ where each $h_i$ has support contained in $\mathbb{R}^n \setminus Z_i$ and then define $g_i = h_i / f_i$. If we replace $\mathbb{R}$ with $\mathbb{C}$ and ""smooth"" with ""polynomial"", then this is a special case of Hilbert's Nullstellensatz.","['differential-geometry', 'smooth-manifolds', 'real-analysis']"
1149468,Differential equation of 2nd order,"I have a differential equation $x''(t)=x(t)^2-x(t)$.
The exercise is as follows: Let $x'(0)=0$. Then the solution $x(t)$ only depends on the initial position $x(0)$. 
Show that there is exactly one value of $x(0)$ for which the solution $x(t)$ is non-constant, yet tens to a finite value as $t$ tends to infinity. Calculate this $x(0)$, as well as the limiting value $x(+\infty)$. What happens to $x(t)$ as $t$ goes to infinity for other values of $x(0)$? This is the question. My intention was to simply solve the reduced system
\begin{align*}
x'(t)&=y(t)\\
y'(t)&=x(t)^2-x(t)
\end{align*}
with barrows formula. But I'm not sure how to do that when the equations depend on each other. Any help on how to do this or maybe if there is an easier approach?",['ordinary-differential-equations']
1149503,$\int_0^\infty \frac{1}{x^2}\left( \left(\sum_{n=1}^\infty\sin\left(\frac{x}{2^n}\right)\right)-\sin(x)\right)\ dx$,"While I was working on my stuff, another question suddenly came to mind, the one you see below $$\int_0^\infty \frac{ \left(\sum_{n=1}^\infty\sin\left(\frac{x}{2^n}\right)\right)-\sin(x)}{x^2} \ dx$$ Which way should I look at this integral?","['improper-integrals', 'calculus', 'integration', 'definite-integrals', 'real-analysis']"
1149514,Is the integral of the sum really the sum of the integrals?,"I was asked to find the mclaurin series of $\int_0^x\frac{\arctan (t)}{t}dt$ using the known mclaurin for arctan: $\arctan(t)=\sum_{n=1}^{\infty} \frac{(-1)^{n+1}t^{2n-1}}{2n-1}$ Ok, so what I did is use the known formula: $$\int_0^x\frac{\arctan (t)}{t}=\int_0^x\frac{\sum_{n=1}^{\infty} \frac{(-1)^{n+1}t^{2n-1}}{2n-1}}{t}dt=\int_0^x\sum_{n=1}^{\infty}\frac{(-1)^{n+1}t^{2n-2}}{2n-1}dt$$ This is nothing more than $\int_0^x1-\frac{t^2}{3}+\frac{t^4}{5}-\cdots dt$. If this was a finite sum, then yes, for sure I can divide it into separate integrals. But this is an infinite sum. The integrand is a polynomial, an integrable and even continuous function so I don't see any reason why we can't separate that integral of the sum into the sum of the integrals, but it's not apparent to me why it's obvious that we can do that either. But if we can, then the mclaurin series we were looking for is $\sum_{n=1}^{\infty}\int_0^x\frac{(-1)^{n+1}t^{2n-2}}{2n-1}dt$ is this correct? if so - can we always separate the integral of the sum into sum of integrals? even infinite sums?","['calculus', 'integration', 'infinity', 'summation', 'taylor-expansion']"
1149524,"Usual and ""unusual"" indeterminate forms","I've always read there are seven ""main"" indeterminate forms, 0/0, âˆž/âˆž,âˆžâˆ’âˆž,0 âˆž ,0 0 ,1 âˆž ,âˆž 0 . I've recently started working more keenly on analysis, and the seven ""main"" forms are never expanded to include ""unusual"" forms. Does anyone know what are they ? I've read somewhere that things likes lim [sin x + sin x] as x goes to âˆž could be one such indetermination. What do you think ? All the best,
G.","['indeterminate-forms', 'analysis']"
1149537,What function could describe this GIF animation?,"I found this image on Beautiful Mathematical GIFs Will Mesmerize You and this GIF really caught my attention. From what I see, it's a 2D circle morphing into the 3D sphere. What function could describe this GIF animation? All comments are appreciated. z < 0 -> red
z > 0 -> green
z = 0 -> blue","['matlab', 'parametric', 'functions']"
1149591,What is the minimal polynomial equation with integral coefficients that the area of the regular 11-gon with side lengths 1 satisfies?,"Let $q_n(x)$ be the minimal polynomial for the area of the regular $n$ -gon. What is $q_{11}(x)$ ? The following are the simpler cases, and $x$ is given by $16 (area^2) $ : For the corresponding triangle the polynomial is $p_3(x) = x-3$ , for the square it is $p_4(x) = x - 16$ , for the pentagon it is $p_5(x) = x^2-50x+125$ , for the hexagon it is $p_6(x) =x-108$ . What is e.g. $p_{11} (x)$ ? What is $p_n(x)$ in general? The question follows from a discussion on the Robbins' formula: http://youtu.be/OeZ6LsZHKcA Robbins' formulas are meant to generalize Heron's and Brahmagrupta's formula to general cyclic n-gons (convex and concave alike). So far the formulas for n up to 9 have been discovered. A viewer of that video suggested we can do regular n-gons first. So for the pentagon case, if  you substitute all five $distance^2$ with 1, then you will  have the following factorized form: $(x^2-50x+125)(x-3)^5 = 0$ . This is the same as $p_5(x)(p_3(x))^5 = 0$ .  This 5 is a result from combinatoric arguments on  the degenerate cases: http://youtu.be/alFkaEZ4cZQ So how do you generalize?","['trigonometry', 'polynomials']"
1149598,How to solve a non-linear matrix equation over integer numbers?,"The equations I want to solve looks like this:
$$
X^n = 6 I
$$
where $I$ is the identity matrix, $n$ is $1,2,3,...""$ and the square-matrix 
$$
X=\begin{pmatrix}
        x_{11} & x_{12} & \cdots  \\
        x_{21} & x_{22} & \cdots \\
        \vdots & \vdots & \ddots \\
        \end{pmatrix}
$$ 
with the $x_{ij} \in \Bbb Z$. If the entries of the matrix $X$ would be allowed to be reals, the problem would be easy to solve. The solution would be
$$
X=\begin{pmatrix}
        \sqrt[n]{6} & 0 & \cdots  \\
        0 & \sqrt[n]{6} & \cdots \\
        \vdots & \vdots & \ddots \\
        \end{pmatrix}
$$
However for $x_{ij}$ being integer the solution seems to be much harder. With guessing and some computer help I was able to find solutions for $n=2$ 
$$
X=\begin{pmatrix}
        0 & 3 \\
        2 & 0 \\
        \end{pmatrix} 
$$
and for $n=3$
$$
X=\begin{pmatrix}
        2 & 3 & 1 \\
        -1 & -1 & -2 \\
        -2 & -1 & -1 
        \end{pmatrix}
$$
I noticed that it didn't seem to be possible to find a two-dimensional matrix $X$ to solve the equation $X^3 = 6 I$. Could it be a general rule, that if you are searching for the $n$th root of a square matrix with integer component, the solution matrix needs to have $n$ dimensions? And I would like to know if there is a better method than just guessing to solve these kind of nonlinear matrix equations over integer numbers?","['matrix-equations', 'matrices', 'integers']"
1149673,Express how many ways you can select a representative,"Assume that a school has these three teams: Chess team with 10 members, Checkers team with 15 members, and College bowl team with 20 members. In how many ways can we select representatives of the school if there should be: exactly 1 representative from each team. 2 persons, and they should be from different teams. 1, 2, or 3 persons but no two from the same team. 2 persons, both from the same team. 2 persons from any team. Answers $20*15*10$ using the product rule $10 \cdot 15 + 10 \cdot 20 + 15 \cdot 20$","['permutations', 'discrete-mathematics', 'combinatorics']"
1149695,How to define a relation,"I was tasked with this question: Let $X = \{0,1,2,3,4\}$ . Define a relation $R$ on $X$ such that $xRy$ , if $x + y = 4$ . I don't understand/know what syntax I should use to define this as a relation. What are they asking when they say "" $x R y$ , if $x + y = 4$ ""? What are $x$ and $y$ here? Does it mean $x$ is from set $X$ ? If that's the case, where is $y$ from? There certainly is no set $Y$ for $y$ . To narrow down what I'm asking: how do I define this relation? I'd appreciate it if you could walk me through the steps. Also, I'm only a month into this subject of Discrete Math, and I'm having difficulty, and I've only learned up to this point about proofs, relations, functions, sets, methods of proof, relations and equivalences, and predicate logic. I know these things well enough, just try not to use any logic that is beyond this scope.","['equivalence-relations', 'discrete-mathematics']"
1149714,Must the limit of the derivative exists under certain assumptions?,"Let $u:(x_0,\infty)\to\Bbb R$ be a monotonically increasing function which is differentiable everywhere and such that $\lim_{x\to\infty}u(x)=l\in\Bbb R$.
Does it follow that $\lim_{x\to\infty}u'(x)$ exists? Surely, if it exists, then it would equal zero.  In fact $$0=\lim_{x\to\infty}[u(x+1)-u(x)]=\lim_{x\to\infty}u'(\xi_x)=\lim_{x\to\infty}u'(x)$$ (this is obtained by applying the mean value theorem:  for every $x$ there exists a $\xi_x$ in between $x$ and $x+1$ such that $u'(\xi_x)=u(x+1)-u(x)$). But I think that under the above assumptions $u'$ must have a limit at infinity.  Is it true?","['real-analysis', 'limits']"
1149727,Tensor calculus on the frame bundle,"Let $M$ be a manifold and let $g$ be a tensor on it, say for example a metric $g\in\Gamma(T^{\ast}M\otimes T^{\ast}M)$. I know how to perform any computation on $g$. For instance, taking its derivative respect to the a connection $\nabla$, evaluating it at a point, taking its Lie derivative, obtaining the curvature of the Levi-Civita connection etc. However, there is a dual formulation on the frame bundle $F(M)$ of $M$, but I never knew how to do the same calculations on the frame bundle, and as I understand it is sometimes simpler to work on the frame bundle. I would like to know how a tensor on $M$ is represented from the point of view of the frame bundle, and how are the typical operations (curvature, Lie derivative etc) implemented. A tensor in $M$ is a section of the corresponding tensor vector bundle. How is this mapped to the frame bundle? For example, given an open set $U$ of the atlas of $M$ I can write $g$ in coordinates as follows $g = g_{ab}\,dx^{a}\otimes dx^{b}$ What would be the analog local expression from the point of view of the frame bundle? Finally, I would like to know a reference where these things are explained in detail. Thanks.","['algebraic-geometry', 'differential-geometry']"
1149729,"Is {âˆ…} a subset of the power set of X, for every set X?","I am reading the book ""A Transition to Advanced Mathematics"" 2014 edition, written by Smith, Eggen and St. Andre. For every set X: âˆ… âŠ†  X âˆ… âˆˆ  power set of X âˆ… âŠ†  power set of X {âˆ…} âŠ†  power set of X Is {âˆ…} âŠ† power set of X wrong? I am an undergraduate student. Please help me! The definition of power set of X is the set whose elements are subsets of X. (page 90 of the book) I have this doubt because I remember that I read somewhere that âˆ… â‰  {âˆ…} because a set with the empty set is not empty. Thanks for your answers! What about this question: If the definition of power set of X is the set whose elements are subsets of X, and {âˆ…} is not a subset of X. How {âˆ…} can be a subset of the power set of X? Is not that an contradiction?",['elementary-set-theory']
1149753,How to prove the existence of a minimum of a quadratic function of two variables?,"I am given function $$
f(x,y)=Ax^2+2Bxy+Cy^2+2Dx+2Ey+F,\quad\text{where }A>0\text{ and }B^2<AC .
$$ Prove that a point $(a,b)$ exists which $f$ has a minimum. I figured out that there is no stationary point for this equation. So, Hessian Matrix seems not helpful. In my book, it says that ""change quadratic part to sum of squares but, Can't think of any way to change it to sum of squares. Also, Why $f(a,b)=Da+Eb+F$ is at this minimum..?","['optimization', 'multivariable-calculus', 'quadratics', 'partial-derivative', 'real-analysis']"
1149771,Best way to find Reduced Row Echelon Form (rref) of a matrix?,"I'm sitting here doing rref problems and many of them seem so tedious. Any tricks out there to achieve rref with less effort or am I stuck with rewriting the matrix for every 2/3 operations? I know TI calculators can do it, but I'm gonna have to do this on my midterm, so I must learn how to do this the most efficient way possible. Thanks.","['matrices', 'linear-algebra']"
1149802,Key differences between almost complex manifolds and complex manifolds,"I know the technical difference between an almost complex manifold and a complex manifold, namely in the former the almost complex structure $J$ may not be integrable while in the later it is. However, in an almost complex manifold one can do many things that can be done in a complex manifold: namely one can still make the Dolbeault decomposition of tensors, and also of the exterior derivative as well as define holomorphic functions. I wonder what are the key differences between the two scenarios, namely what things can be done in a complex manifold that cannot be done in an almost complex manifold because of $J$ being non-integrable. By the way, could someone recommend me a good book about complex differential geometry? Thanks.","['differential-geometry', 'algebraic-geometry', 'complex-analysis']"
1149808,Two questions about ordered pairs in a set,"If $D= \{8,2,4,6\}$ then $(2,4) \subseteq D$. Is this true or false? From what I understand, $(2,4)$ is not the same as $\{2,4\}$ so I assume it is false? If $B= \{5,(1,2),17\}$, then $n(B)= 4$. Does an ordered pair count as one element of a set, or two? I assume it is one and so the given statement would be false. Anyone who can confirm/counter my suspicions would be greatly appreciated!",['elementary-set-theory']
1149830,Is the 3d Schwartz space isomorphic to a subspace of the 1d Schwartz space?,"Are the Schwartz-spaces $\mathscr{S}(\mathbb{R})$ and $\mathscr{S}(\mathbb{R}^3)$ isomorphic (as topological vector spaces)?
Is $\mathscr{S}(\mathbb{R}^3)$ at least isomorphic to a subspace of $\mathscr{S}(\mathbb{R})$? (These are simplified versions of item 3. and 4. from this question . Sorry for reposting them here...) I think that $\mathscr{S}(\mathbb{R})$ is isomorphic to a subspace of $\mathscr{S}(\mathbb{R}^3)$, for example to the subspace of functions of the form $f(x)\exp(-y^2-z^2)$. Is this correct? One hint that $\mathscr{S}(\mathbb{R}^3)$ might indeed be isomorphic to a subspace of $\mathscr{S}(\mathbb{R})$ can be found in the Encyclopedia of Mathematics article Nuclear space : Every nuclear space of type FrÃ©chet is isomorphic to a subspace of the space $\mathscr{E}(\mathbb R)$ of infinitely-differentiable functions on the real line, that is, $\mathscr{E}(\mathbb R)$ is a universal space for the nuclear spaces of type FrÃ©chet (see [10]). [10]    T. Komura, Y. Komura, ""Ueber die Einbettung der nuklearen RÃ¤ume in $(s)^A$"" Math. Ann. , 162 (1965â€“1966) pp. 284â€“288","['topological-vector-spaces', 'functional-analysis', 'schwartz-space']"
1149862,Proving that on a Lie group $G$ the space of left-invariant vector fields is isomorphic to $T_e G$,"Let $G$ be a Lie Group. Given a vector $v\in T_e G$, we define the left-invariant vector field $L^v$ on $G$ by $$L^v(g)=L^v|_g=(dL_g)_e v.$$ I want to show that $v\mapsto L^v$ is a linear isomorphism between $T_e G$ and $\mathfrak{X}^L(G)$ (The set of all left invariant vector fields on $G$). I am comfortable showing the map is linear and 1-1. To show surjectivity, let $X\in \mathfrak{X}^L(G)$ be arbitrary. We want to show there exists $v\in T_e G$ such that $L^v|_g = X|_g$ for all $g\in G$. Define the vector $v$ as $v:=(dL_{g^{-1}})_g X|_g$. Then, $\begin{align*}
L^v|_g&=(dL_g)_e\left[(dL_{g^{-1}})_g X|_g\right]\\
\\
&=(dL_g)_e[X|_e]\\
\\
&=X|_g,
\end{align*}$ where the last two lines are because $X$ is a left-invariant vector field. Is this argument sufficient?","['vector-fields', 'lie-groups', 'differential-geometry', 'smooth-manifolds']"
1149875,Is the characteristic equation in ODE the same characteristic equation in linear algebra?,"Can someone show me whether this ""characteristic equation"" thing in ODE is the same characteristic equation that we derive for a matrix? For example, given $y'' + 2y = 0$, the characteristic equation is $\lambda^2 = -2$ How does this equation correspond to those of a matrix?","['eigenvalues-eigenvectors', 'linear-algebra', 'ordinary-differential-equations']"
1149881,(conditional probability) Compute the probability that the first 2 balls selected are black and the third selected ball is white.,"An urn initially contains 6 white and 8 black balls. Each time a ball is selected, its color
is noted. If the selected ball is white, then it is replaced in the urn along with 3 other black balls. If the
selected ball is instead black, then it is replaced along with 2 other white balls. Compute the probability
that the first 2 balls selected are black and the third selected ball is white. My answer: $P(B_1B_2W_3)=\frac{{8 \choose 1} {6 \choose 0}}{{14 \choose 1}}\frac{{8 \choose 1} {8 \choose 0}}{{16 \choose 1}}\frac{{8 \choose 0} {10 \choose 1}}{{18 \choose 1}}$ which I think equals 10/63 Is my understanding of this problem correct?",['probability']
1149920,Find the MLE estimator for $\theta$,"Let $Y_1 ,Y_2 ,\ldots,Y_n$ be a random sample from a distribution with pdf $f(y) =  e^{-(y -\theta) }$ for $y \geq 0 $ and $0$ else a) Find the Method of Moments estimator for $\theta$ b) Find the MLE estimator for $\theta$ I'm pretty sure I found out how to do a) but b) I'm having trouble with. Everytime I take the logarithm and then take the derivative, $\theta$ disappears, any help?","['statistics', 'maximum-likelihood', 'parameter-estimation']"
1149954,"If $f$ is locally Lipschitz on $X$ and $X$ is compact, then $f$ is Lipschitz on $X$.","If $f$ is locally Lipschitz on $X$ and $X$ is compact, then $f$ is Lipschitz on $X$. My proof: Since $f$ is locally Lipschitz on $X$, for each $x âˆˆ X$ there exists an
open $B_x$ containing $x$ such that $f$ is Lipschitz on $B_x$. Consider the collection
of all such $B_x$. This collection forms an open cover of $X$ and so there is a finite sub-collection $\{B_1, B_2, . . . , B_n\}$ which also covers $X$, since $X$ is compact. Since $f$ is Lipschitz on each $B_i$, there is an $B_i$ such that $d(f(x_i), f(y_i)) \leq M_i d(x_i, y_i)$ for all $x_i, y_i \in Wi$, for $i âˆˆ \{1, 2, . . . , n\}$. Then taking $B = max\{B_1, B_2, . . . , B_n\}$, we see that $f$ is Lipschitz on $X$. Is my solution correct? Can we use the fact of compact metric space that every sequence in $X$ has a convergent sub-sequence to prove the same fact??","['general-topology', 'lipschitz-functions', 'metric-spaces']"
1149963,Strong induction with Fibonacci numbers,"I have two equations that I have been trying to prove. The first of which is: F(n + 3) = 2F(n + 1) + F(n) for n â‰¥ 1. For this equation the answer is in the back of my book and the proof is as follows: 1) n = 1: F(4) = 2F(2) + F(1)  or 3 = 2(1) + 1, true. 2) n = 2: F(5) = 2F(3) + F(2) or 5 = 2(2) + 1, true. 3) Assume for all r, 1 â‰¤ r â‰¤ k: F(r + 3) = 2F(r + 1) + F(r) 4) Then F(k + 4) = F(k + 2) + F(k + 3) = 5) 2F(k) + F(k - 1) + 2F(k + 1) + F(k) = 6) 2[F(k) + F(k + 1)] + [F(k - 1) + F(k)] = 7) 2F(k + 2) + F(k + 1) My first question here is how do I know how many values of n to test for? Here they chose two. My next question is how did they get from line 3 to line 4? I understand how the statement is correct but why is this chosen? I also understand that I need to prove it's true for all values of r because if I do that it implies that it is true for k + 1.  Is it just to find a relation to F(r + 3) on line 3? If that was the case why not just have F(k + 3) = F(k + 2) + F(k + 1)? My final question about this is how did they get from line 4 to 5? The second equation I want to prove is: F(n + 6) = 4F(n + 3) + F(n) for n â‰¥ 1 I'm able to prove n = 1 and n = 2 is true but I get stuck on going from what would be line 3 - 4 on this problem. As this is my problem for homework the answer is not in the back of the book. Now that I've gotten the help I just want to update this with the proof for my second equation (I haven't gotten the formatting down yet so bear with me): F(n + 6) = 4F(n + 3) + F(n) 1) n = 1: F(7) = 4F(4) + F(1) or 13 = 12 + 1, true. 2) n = 2: F(8) = 4F(5) + F(2) or 21 = 20 + 1, true. 3) Assume for all r, 1 â‰¤ r â‰¤ k: F(r + 6) = 4F(r + 3) + F(r) 4) Then F(k + 7) = 4F(k + 4) + F(k + 1) = 5) F(k + 4) + F(k + 4) + F(k + 4) + F(k + 4) + F(k + 1) = 6) F(k + 4) + F(k + 4) + F(k + 4) + F(k + 3) + F(k + 2)  F(k + 1) = 7) F(k + 4) + F(k + 4) + F(k + 4) +F(k + 3) + F(k + 3) = 8) F(k + 5) + F(k + 5) + F(k + 4) = 9) F(k + 6) + F(k + 5) = 10) F(k + 7)",['discrete-mathematics']
1149980,Prove that $\sqrt{a^2+(1-b)^2}+\sqrt{b^2+(1-c)^2}+\sqrt{c^2+(1-a)^2}\geq\frac{3\sqrt{2}}{2}$,"Prove that the following inequality
  $$\sqrt{a^2+(1-b)^2}+\sqrt{b^2+(1-c)^2}+\sqrt{c^2+(1-a)^2}\geq\frac{3\sqrt{2}}{2}$$
  holds for arbitrary real numbers $a$, $b$ and $c.$ Someone says, ""It's very easy problem. It can also be proved by AMâ€“GM inequality."" But I can't reason out the answer to this question.","['inequality', 'cauchy-schwarz-inequality', 'algebra-precalculus']"
1150037,Why are people interested in solving the Navier-Stokes equations if people can find an good approximate solution?,Why are people interested in solving the Navier-Stokes equations if people can find a good approximate solution? Also especially when people have supercomputers?,['ordinary-differential-equations']
1150059,Using complex analysis to evaluate $\int_0^\infty\frac{(\ln x)^3}{1+x^2}d x$,"Here is my attempt: Let $R>1>r$ and $C$ be the closed curve in $\mathbb{C}$ consists of the following pieces:
$$C_1=\{Re^{it}: t\in(0,\pi)\},\quad C_2=[r,R],\quad  C_3=\{re^{it}: t\in(0,\pi)\},\quad C_4=[-R,r]$$
all curves are oriented counterclockwise. It can be seen that $C$ is the boundary of the upper half of the annulus centred at $0$ with inner radius $r$ and outer radius $R$. Let $f(z)=\frac{(\text{Log } z)^3}{1+z^2},\quad Arg(z)\in\left(-\frac{\pi}{2},\frac{3\pi}{2}\right)$. Then $\int_Cf(z)d z=\sum_{i=1}^4\int_{C_i}f(z)d z$. Note that $f$ has a singularity $z=i$ inside $C$, thus by Cauchy's integral formula
$$\int_Cf(z)dz=2\pi i\cdot\frac{(\text{Log }  i)^3}{i+i}=\pi\left(\ln|i|+\frac{\pi}{2}i\right)^3=-\frac{\pi^4}{8}i$$
Now consider $\int_{C_i}f(z)d z$:
 $$\begin{aligned}&\left|\int_{C_1}f(z)d z\right|\leq\int_0^\pi\frac{(|\ln R|+|it|)^3}{|1-R^2e^{2it}|}|Re^{it}|dt\leq\int_0^\pi\frac{R(\ln R+t)^3}{R^2-1}|dt\leq \frac{\pi R(\ln R+\pi)^3}{R^2-1}
\end{aligned}$$
as $RHS\to 0$ as $R\to+\infty$, we have $\lim_{R\to\infty}\int_{C_1}f(z)d z=0$. $$\int_{C_2}f(z)d z=\int_r^R\frac{(\ln x)^3}{1+x^2}d x\quad\Rightarrow\quad\lim_{r\to 0^+, R\to+\infty}\int_{C_2}f(z)d z=\int_0^\infty\frac{(\ln x)^3}{1+x^2}d x$$ $$\begin{aligned}&\left|\int_{C_3}f(z)d z\right|\leq\int_0^\pi\frac{(|\ln r|+|it|)^3}{|1+r^2e^{2it}|}|re^{it}|d t\leq\int_0^\pi\frac{r(\ln r+t)^3}{1-r^2}|d t\leq \frac{\pi r(\ln r+\pi)^3}{1-r^2}
\end{aligned}$$
as $RHS\to 0$ as $r\to0^+$, we have $\lim_{r\to 0^+}\int_{C_3}f(z)d z=0$ $$\begin{aligned}&\int_{C_4}f(z)d z=\int_{[-R,-r]}\frac{(\text{Log } z)^3}{1+z^2}d z=\int_{[-R,-r]}\frac{(\ln|z| +\pi i)^3}{1+z^2}d z=\int_r^R\frac{(\ln x +\pi i)^3}{1+x^2}d x\\
=&\int_r^R\frac{(\ln x)^3}{1+x^2}dx+3\pi i\int_r^R\frac{(\ln x)^2}{1+x^2}d x-3\pi^2\int_r^R\frac{\ln x}{1+x^2}d x-\pi^3 i\int_r^R\frac{1}{1+x^2}d x\end{aligned}$$ At this point I have to evaluate $\int_0^\infty\frac{(\ln x)^2}{1+x^2}d x$, which can be done by complex analysis again. However, this method is way too long and from my point of view, not the most efficient. Is there a shorter way to do this? By the way, the answer is 0.","['residue-calculus', 'integration', 'complex-analysis', 'contour-integration']"
1150090,Iterated function,"Let
$$f(x)=xâˆ’\frac{1}{x}$$
Find the number of real solutions to $f(f(f(f(x))))=1$. Do I evaluate it completely, or is there some other way. After third composition it got nasty, so I left it.","['algebra-precalculus', 'functions', 'function-and-relation-composition']"
1150095,"Is The Statement $b^n\equiv 1\pmod n$ equivalent to ""$x\mapsto b^x-x\pmod n$ is a bijection""?","Suppose that $n$ is a natural number and $b$ is one coprime to it such that $b^n\equiv 1\pmod n$. Does it follow that, if $b^x-x\equiv b^y-y\pmod n$, then $x\equiv y\pmod n$? This is inspired by the comments on a previous answer I wrote , which got me thinking about fixed points on exponentiation. Toying around in Mathematica, I noticed that
$$x\mapsto 9^x-x\pmod{1000}$$
is an injective function. In fact, I noticed that if we replace $9$ by any other number coprime to $1000$, this still holds. Toying around further, I noticed that the list of numbers $n$ other than $1000$ such that all coprime bases $b$ had this property starts the same way as this sequence in OEIS , which is exactly the sequence of $n$ such that $b^n\equiv 1\pmod n$ for $b$ coprime to $n$. I can't prove the lists are equal (but they seem to be), but I can see that $b^n\equiv 1\pmod n$ is a necessary condition for $x\mapsto b^x-x$ to be injective (as it's not even well-defined otherwise). The only insight I have into the problem is that it clearly holds if $b=1$ or $-1$. The latter can be argued as if $(-1)^n\equiv 1\pmod n$ then $n$ is even and $x\mapsto (-1)^x-x$ maps evens to odds and vice versa and is a linear function with unit ($-1$) coefficient of $x$ over those domains - and hence is bijective. I feel like there may be some generalization of this approach by partitioning $(\mathbb Z/n\mathbb Z,+)$ into $\text{ord}_n(b)$ cosets (where $\text{ord}$ is the multiplicative order of $b$ mod $n$), but I have been unsuccessful in finding it if it exists. To be sure, restricting the map in question to a single such coset yields a bijection between it and another coset (as $b^{x}-x$ and $b^{x+\text{ord}_n(b)}-x-\text{ord}_n(b)$ are equal mod $\text{ord}_n(b)$) - but I can't prove that two distinct cosets do not map to the same place, as this requires considering the map over the larger domain where it is less well-behaved. I am primarily interested in the answer to this question, regardless of technique, however I would be especially interested in knowing if the methods I sketched can be gainfully extended to a proof.","['modular-arithmetic', 'exponentiation', 'number-theory']"
1150104,Bound for variance of maximum of normal random variables,"Suppose that $(X_1,\ldots,X_n)=\mathbf{X}\sim N(\mathbf{0},\Sigma)$ is an $n$-dimensional normal random vector. I want to show the bound
$$ \text{Var}\left(\max_{i\leq n} X_i\right)\leq \max_{i\leq n} \text{Var}(X_i)$$
How do I even begin showing this type of bound?","['statistics', 'inequality', 'probability', 'probability-theory']"
1150118,Question about limits $\lim_{x\to\infty}\frac{x-2}{e^{1/x}\cdot x}$,How to calculate this: $$\lim_{x\to\infty}\frac{x-2}{e^{1/x}\cdot x}$$,"['calculus', 'limits']"
1150124,Forty poor thieves,Forty thieves have 4000 gold coins to split between them. A group of five thieves is $poor$ if together they have less than or equal to 500 gold coins. Let N be the minimum number of poor groups of five thieves among all groups of five thieves. Find N. I know the answer can be calculated using Subtraction rule(total groups-the groups which do not satisfy the equation). But how do I apply it?,['combinatorics']
1150183,Calculate $\sum_{n=1}^{\infty}\frac{(2n-1)!!}{(2n)!!\cdot 2^n}$,"Calculate the sum $$\displaystyle \sum_{n=1}^{\infty}\frac{(2n-1)!!}{(2n)!!\cdot 2^n}$$
  where $(2n-1)!!=1\cdot 3\cdots (2n-1)$, $(2n)!!=2\cdot 4 \cdots 2n$ Using Wolframalpha, the result is $\sqrt{2}-1$. But I don't have any idea to come up with that result. Thanks a lot.","['factorial', 'sequences-and-series']"
1150197,Gradient of $X \mapsto a^T X b$ when $X$ is symmetric,"For matrix $X \in \Bbb R^{n \times n}$ , $a \in \Bbb R^n$ , and $b \in \Bbb R^n$ , I know the following holds $$\nabla_X \left( a^T X b \right) = a{b^T}$$ However, it seems that if $X$ is a symmetric matrix ( $X \in \Bbb S^n$ ), then $$ \nabla_X \left( {a^T} X b \right) = \frac{1}{2}(a{b^T} + {b}a^T) $$ How to understand it? If $X \in \Bbb S^n$ , then the dimension of $X$ is $\frac{n(n+1)}{2}$ . Why should we get $n^2$ elements after differentiation?","['symmetric-matrices', 'matrices', 'matrix-calculus', 'scalar-fields']"
1150215,"Find $f'(8.23)$ where $f(x)=23|x|âˆ’37\lfloor x\rfloor+58\{x\}+88\arccos(\sin x)âˆ’40\max(x,0)$","Let
$$f(x)=23|x|âˆ’37\lfloor x\rfloor+58\{x\}+88\arccos(\sin x)âˆ’40\max(x,0).$$
Find $f^\prime(8.23)$. Note: For a real number $x$, $\{x\}=xâˆ’\lfloor x\rfloor$ denotes the fractional part of x. I don't know the derivatives of the few pieces of this function (like the fractional part).","['trigonometry', 'calculus', 'derivatives', 'ceiling-and-floor-functions']"
1150232,Finding the (unit) direction vector given azimuth and elevation,"I want to calculate a unit direction vector of a direction with given the azimuth and elevation (cf. http://en.wikipedia.org/wiki/Azimuth ), respectively
$$\alpha \in [0^{\circ},360^{\circ}), \qquad \beta \in (-90^{\circ},90^{\circ}).$$
I have a right-handed coordinate system with z-up and looking down +y (yes, I have a graphics background :D). I got the hint that it's easy to calculate using trigonometry, but I don't really understand the solution: $$
\begin{pmatrix}x\\y\\z\end{pmatrix} = \begin{pmatrix}\sin(\alpha) \cos(\beta)\\ \cos(\alpha)\cos(\beta)\\ \sin(\beta)\end{pmatrix}
$$ Can someone explain the motivation behind this solution?","['geometry', 'trigonometry', 'spherical-coordinates']"
1150241,Norm of a linear map is not attained,"Prove that the norm of the linear functional $$\phi: l^1 \ni \{x_n \} \rightarrow \sum_{n=1} ^{\infty} (1 - \frac{1}{n} )x_n \in \mathbb{K}$$ equals one but there doesn't exist a sequence $ \{x_n \} \in \mathcal{l}^1$ such that $|| \{ x_n \} || \le 1$ and $| \phi ( \{ x_n \} ) | =1$ My problem is that I don't know how to show that the norm equals one - it's easy to show that it's less or equal $1$. I guess I need to contruct a sequence of elements of $l^1$ whose norms tend to one. And how to prove that there doesn't exist $\{ x_n \} \in l^1$ for which the value $1$ of the norm is attained? Could you explain that to me, please?","['normed-spaces', 'sequences-and-series', 'functional-analysis']"
1150297,Difference between Modification and Indistinguishable,"Would someone be able to offer a layman's explanation of what is means when two stochastic processes are a Modification of each other and when they are Indistinguishable? My Stochastic Analysis notes define the following: a) $\textit{The stochastic processes X and Y are called a modification of each other if}$
\begin{align}
P(X(t) = Y(t)) = 1 \quad \textit{for all t $\in$ I}
\end{align} 
b) $\textit{The stochastic processes X and Y are called indistinguishable}$
\begin{align}
P(X(t) = Y(t) \,\,\,\textit{for all $t \in I$}) = 1
\end{align} I interpret this to mean that: (a) for each $t$ in $I$ the probability that the processes are equal is equal to 1. (b) the probability that the entire path of the processes X is equal to the entire path of the process Y, is equal to 1. I don't understand the difference between the two statements. The first seems to examine the processes at each point. The second examines the paths of the processes (which are made up of the points!). Is anyone able to provide a motivational explanation as to why (a) does not mean that the paths are the same? I find it counter intuitive since if the processes are equal at each point then surely their paths are the same? The classic example I've been given which (apparently) shows the two processes to be a modification and not indistinguishable is: $\textbf{Example:}$ Let $\Omega = [0,\infty), \mathcal{A} = \mathcal{B}([0,\infty))$ and $P$ be a probability measure on $\mathcal{A}$ which has a density. Define two stochastic processes $(X(t): t \ge 0)$ and $(Y(t): t \ge 0)$ by
\begin{align}
X(t)(\omega) = 
\begin{cases} 
1, \text{ if $t = \omega$},\\
0, \text{ otherwise}
\end{cases}
\quad Y(t)(\omega) = 0 \quad \text{for all $t \ge 0$ and all $\omega \in \Omega$.}
\end{align}
Then $X$ and $Y$ are modifications of each other but $X$ and $Y$ are not indistinguishable. $\textbf{Question:}$ With regard to the example above if $t = 0$ and $\omega = 0$ then $X(0)(0)= 1$ but $Y(0)(0) = 0$, then how can their probability be equal to 1 at this point? Hence how can they be a modification of each other? Many thanks, John","['probability-theory', 'stochastic-processes', 'measure-theory']"
1150310,Evaluating $\lim\limits_{x \to 0}\frac{\sin x}{\ln\left(\frac1{1+x}\right)}$,"Can you please give me some hints to evaluate
$$\lim_{x \to 0}\frac{\sin x}{\ln\left(\frac1{1+x}\right)}\ ?$$
I'm stuck.","['trigonometry', 'calculus', 'limits']"
1150355,"Usage of the term ""perfect matching"" for bipartite graphs","A textbook written by my discrete mathematics teacher defines a ""perfect matching"" in a bipartite graph as a matching that covers at least one side of the graph (i.e. for $G = (V_1, V_2, E)$ with $V_1$ and $V_2$ as the two sets of vertices and $E$ as the set of edges, the number of edges in the ""perfect matching"" should be equal to the number of vertices in $V_1$ or $V_2$). However, other sources (including Wikipedia) define a ""perfect matching"" as containing all vertices of a graph. Do these definitions of the term conflict? If yes, which is the correct one?","['bipartite-graphs', 'graph-theory', 'matching-theory', 'discrete-mathematics']"
1150363,Is $\pi$ equal to $180^\circ$?,"$$
\begin{array}{ccc}
\sin{(\theta+180^{\circ})}=-\sin{\theta} & \cos{(\theta+180^{\circ})}=-\cos{\theta} & \tan{(\theta+180^{\circ})}=\tan{\theta} \\
\sin{(\theta+\pi)}=-\sin{\theta} & \cos{(\theta+\pi)}=-\cos{\theta} & \tan{(\theta+\pi)}=\tan{\theta}
\end{array}
$$ If I compare them, I will get $\pi=180^{\circ}$. Why? Isn't $\pi=3.142\ldots $? Can anyone prove this?",['trigonometry']
1150382,Equivalent definition of Cauchy sequence,"A sequence $x_i$ is Cauchy if for all $r>0$, there exists $n$ s.t. $i,j\geq n$ implies $d(x_i,x_j)<r$. My question is, is it equivalent to define Cauchy as follows? $x_i$ is Cauchy if for all $r>0$, there exists $n$ such that $i>n$ implies $d(x_n,x_i)<r$. If $x_n$ is Cauchy then it obviously satisfies my definition (just set $i=n$). If it satisfies my definition, then let's say we're given an $r$. By my definition, there exists $n$ s.t. $i\geq n$ implies $x_i\in B(x_n,r/2)$. So if $i,j\geq n$, then $x_i,x_j$ are both in this ball, so $d(x_i,x_j)<r$ by triangle inequality. I ask because my version seems easier to prove. You only have to consider $(n,i)$ s.t. $i>n$ instead of all pairs $(i,j)$ with $i,j\geq n$. It's also nicer to visualize because I can imagine all $x_i$ s.t. $i\geq n$ contained in this nice ball.","['metric-spaces', 'cauchy-sequences', 'analysis']"
1150417,Find the Range and Domain of the following function,"The function is: $f(x,y) = \frac{2}{\sqrt{3-x}} + \frac{1}{\sqrt{4-y}}$ I have found the domain and the Range intuitively. But how would I formally prove that my assumption of the Range and Domain is true?","['geometry', '3d', 'multivariable-calculus']"
1150422,Tightness and Uniform Integrability,"I'm trying to develop some intuition for the concepts of tightness and uniform integrability, in a probabilistic context. (i) Does uniform integrability imply tightness? (ii) If not, is $X_n(x)=I_{[n, n+1]}$ uniformly integrable?
If we take the measure space composed of Borel $\sigma$-algebra on $\mathbb{R}$ with the Lebesgue measure, $(\mathbb{R}, \mathcal{B}, \lambda)$, then is $f_n(x)=I_{[n, n+1]}$ uniformly integrable? It is certainly not tight, but $\mathbb{E}[|f_n|I_{[|f_n| \geq K]}] \leq \varepsilon$ seems to satisfy the uniform integrability condition.","['probability-theory', 'uniform-integrability']"
1150435,Proving a function of matrix is convex,"I have a function of a matrix and a vector $f(A,b)=y^\top (I-A)^{-1} b$ and I want to know the conditions under which it is convex. For functions of a vector, the positive definiteness of the Hessian is sufficient to claim convexity. How do we extend this to functions of matrices? I know the first derivative $\frac{\partial f}{\partial A}=(I-A)^{-\top}y\ b^\top(I-A)^{-\top}$, but how to extend this for finding convexity?","['nonlinear-optimization', 'matrices', 'convex-analysis', 'convex-optimization']"
1150454,Sketch the proof of $e^{-x^2}$ being uniformly continuous (proof is given),"I am asked to sketch the proof seen below in a graph. We state that
the function $f: \mathbb{R} \to \mathbb{R}$ given by $f(x)=e^{-x^2}$ Is uniformly continuous Proof:
\begin{eqnarray*}
|f(x)|<\epsilon/2&\Longleftrightarrow&
e^{-x^2}<\epsilon/2\\&\Longleftrightarrow&
\log\left(e^{-x^2}\right)<\log (\epsilon/ 2)\\&\Longleftrightarrow&
-x^2<\log \epsilon-\log 2\\&\Longleftrightarrow&
x^2>\log 2-\log \epsilon\\&\Longleftrightarrow&
|x|>\sqrt{\log 2-\log \epsilon}
\end{eqnarray*} By setting $K=\sqrt{\log 2-\log \epsilon}$ we get  $|x|,|y|>K\Longrightarrow |f(x)-f(y)|\leq |f(x)|+|f(y)|<\frac\epsilon2+\frac\epsilon2=\epsilon.$  (1) We can now use our third theoreom ($F: A->\mathbb{R}$ on the set $A \subset \mathbb{R}^{k}$ is uniformly continuous if  $\forall\epsilon>0 \exists\delta>0: ||f(y)-f(x)||<\epsilon$ for all $x,y\in$A with $||y-x||<\delta$ on f in the interval $[-K-1; K+1]$ and conclude that f is uniformly continuous on this interval. We can therefore chose a $\delta'>0$ with the proberty $|f(x)-f(y)|<\epsilon,$ when $|x-y|<\delta'$ and $x,y\in I$  (2) By setting $\delta=min({\delta',1})$ we can parry off (not sure if this is the right term at all...) the chosen $\epsilon$ for all $x,y\in \mathbb{R}$. Set $|x-y|<\delta$ - if both x,y $\in$ I, then (2) says that $|f(x)-f(y)|<\epsilon$. If $x\notin I$ we have $x>K+1$ or $x<-K-1$. If $x>K-1$ we get that $y>K$ because of $|x-y|<1$ whereas (1) gives that $|f(x)-f(y)|<\epsilon$. And if $x<-K-1$ we have that $y<-K$, which gives us that (1) is $|f(x)-f(y)|<\epsilon$ I am having trouble understanding the proof completely - I got the main idea, that something is uniformly continuous if the function behaves nicely. However, sketching this is giving me loads of troubles. Can anyone help out? We have to sketch it with the values $\epsilon,\delta,K$ on the figgure.","['uniform-continuity', 'exponential-function', 'analysis']"
1150459,How many times does the function $f(x)=\cos x\cdot\cos \frac x2\cdot\cos \frac x3\dots\cos \frac x{2009}$ change sign?,"How many times function
  $$f(x)=\cos x\cdot\cos\frac x2\cdot\cos\frac x3\cdots\cos\frac x{2009}$$
  change sign on interval $\left[0,\dfrac{2009\pi}{2}\right]$? My attempt Function $f(x)$ change sing at $f(x)=0$, so we just need to find number of solutions of
$$\cos x=0\lor\cos\dfrac x2=0\lor\dots\lor\dfrac x{2009}=0$$
on given interval. Equation
$$\cos x=0$$
has solutions
$$x=\dfrac\pi2+k\pi,k\in\mathbb{Z}$$
First solution is $\dfrac\pi2$ for $k=0$. I tried to find last solution:
$$\dfrac\pi2+k\pi\le\dfrac{2009\pi}2$$
which gives us $k\le1004$, so we have $1005$ solution. Then I tried to do this for $\cos\dfrac x2$, then for $\cos \dfrac x3$ and so on to find how many solution of that are same as solutions of $\cos x=0$, but I think this is not an easy way. Is there a simple way to solve this?","['trigonometry', 'functions']"
1150478,Is this correct reasoning about Taylor series?,"Is the following correct reasoning about the Taylor series? I'm just trying to build some intuition but just want to make sure it's correct. If a function $f(x)$ has a power series representation it will be of the form:
$$ f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+a_3x^3+ \text{ ... } \;\;\;\; \text{(1)}$$
Now we have written the coefficients as $a_k$, but we could also just as well write them as: 
$$ a_k=\frac{f^{(k)}(0)}{k!} $$
Of course to get this we need to know that $f(x)$ is infinitely differentiable. Since we found $a_0$ by letting $x=0$ therefore giving $f(0)=a_0$; similarly for $a_2$, we let $x=0$ for the derivative of $f(x)$, or $f'(x)=a_1$; and so on. Now according to this , if $f(x)$ has a power series representation then it must be of the form (I've just centred it around $0$):
$$ f(x)=\sum_{n=0}^{\infty}\frac{f^{(n)}(0)}{n!}x^n \;\;\;\;\; \text{(2)}$$
Since effectively - assuming the function is infinitely differentiable - we've just re-written the general coefficients $a_k$ into an equally general form, but just in terms of the functions derivative. Thus if a function is infinitely differentiable then (2) is completely equivalent to (1). It's just written in this way so we have a mechanism for finding the coefficients (though it could also be done by setting $x=0$ and considering the various derivatives of $f(x)$ to find the coefficients manually). The reason I'm asking whether this is correct (though not necessarily rigorous, I'm trying to build more of an intuition), is because I was curious why $f(x)=\sin(x)$ could have a Taylor series representation that is entirely defined by the functions derivative at a single point . From the above reasoning it seems clear(er) that the derivatives at a point are just a mechanism for finding the coefficients of the polynomial. And since if there's a power series expansion for $\sin(x)$ it must be of this form; and I would expect there to exist such a power series that could equal the sine function (given enough terms). It just happens to be in the form of Taylor series (which we've defined in terms of derivatives at a point).","['power-series', 'complex-analysis', 'calculus', 'real-analysis']"
1150493,Find value of a functional equation,"Find $f(x)$ such that
$$2 f(n) + \frac{1}{3}f\left(\frac{1}{n}\right) = 12.$$
Can anybody suggest me a way to solve this kind of functional equations?","['functions', 'functional-equations']"
1150497,"$\sqrt{4 -2 \sqrt{3}} = a + b\sqrt{3}$, where numbers $a$ and $b$ are rational","If $a$ and $b$ are rational numbers such that $\sqrt{4 -2 \sqrt{3}} = a + b\sqrt{3}$ Then what is the value of $a$? The answer is $-1$. $$\sqrt{4 - 2\sqrt{3}} = a + b\sqrt{3}$$ $$4 - 2\sqrt{3} = 2^2 - 2\sqrt{3}$$ Let $u =2$ hence, $$\sqrt{u^2 - \sqrt{3}u} = a + b\sqrt{3}$$ $$u^2 - \sqrt{3}u = u(u - \sqrt{3})$$ $$a + b\sqrt{3} = \sqrt{u}\sqrt{u - \sqrt{3}}$$ What should I do?","['nested-radicals', 'contest-math', 'number-theory', 'elementary-number-theory', 'real-analysis']"
1150501,How would you prove that this sequence converges to $\frac54$?,How would you prove that this sequence converges to $\dfrac 54$? $$a_{n}=\dfrac {5n}{4n-3}$$ Thanks.,"['sequences-and-series', 'limits']"
1150535,Bochner integral vs regulated integral,"I'm reading Serge Lang's Real And Functional Analysis and at some point he introduces the regulated integral in order to prove the Fundamental Theorem Of Calculus (in the context of Banach Spaces), or at least that seems like the only application where he uses regulated integrals. He also introduced the Bochner integral in the same book some chapters earlier, so I wondered, is there some reason why to switch to the regulated integral from the Bochner integral or does it only make the proof of the Fundamental Theorem easier?","['riemann-sum', 'integration', 'derivatives', 'banach-spaces']"
1150544,What is the relationship between the Archimedean Property and Calculus?,"My textbook begin with a chapter dedicaded to the Real Numbers. It introduces firstly integers and rational numbers, then it introduces the irrational numbers, which appeared in the first place as a necessity of expressing the length of incommesurable line segments. To my understanding, all of this helps to introduce the notion of continuity of a variable, which is put into work when treating the continuity of a variable dependent of another variable (if we restrict ourselves to single-valued functions). The Archimedean Property is stated as follows (or as it is written in my textbook): Given any number $c > 0$, there exists  a natural number $n$, such that $n > c$. Given any positive number $\epsilon$, there always exists a natural
  number $n$ such that the inequality  $1/n < \epsilon$ is fulfilled. The last statement is the first one with $c = 1/\epsilon$. Although I understand what is meant by it (I think it can readily be proved by contradiction), what I'm not getting is how the Archimedean Property fits in the preparatory material preceding that dealing with supremum, infimum and then with continuity and limits. Does it have something to do with the concept of limit, perhaps?","['soft-question', 'analysis']"
1150545,Using Girsanov theorem to prove density of stopping time,"Let $B$ be a standard Brownian motion and for $a>0$ and $b>0$, and set $$\sigma_{a,b} = \inf\{t\,:\, B_t + bt = a\}.$$ There are at least two ways to solve the following problem (the other one is using the scaling property and Laplace transform), but I want to use Girsanov theorem to prove that the density of $\sigma_{a,b}$ is equal to 
$$a\,\left(2\pi t^3\right)^{-\frac{1}{2}}\exp\left(-(a-bt)^2\,/\,2t\right)$$ What I have done so far is using the reflection principle:
\begin{equation}
\begin{split}
\mathbb P(\sigma_{a,b}<t) &=& 2\,\mathbb P(B_t + bt \geq a) = 2\,\mathbb P(B_t \geq a - bt) =\\
&=& 2\int_{\frac{a-bt}{\sqrt{t}}}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\,dx
\end{split}
\end{equation} Clearly, differentiating the above expression does not yield the desired density, and I'm also mindful that I have not used Girsanov simply because I do not see a proper way of applying it. I have also thought about defining a new Brownian motion $\widetilde{B_t} = B_t + bt$ and then use Girsanov's theorem such that $\frac{d\mathbb P}{d\mathbb Q} = Z$ but I'm unsure how to proceed down this path. Any help is welcome. Thanks in advance.","['stochastic-processes', 'probability-theory', 'stochastic-calculus', 'brownian-motion', 'stopping-times']"
1150566,"If we have a subgroup of index 3 that is not normal, show there is a subgroup with index 2","Given a a subgroup $H$ of $G$ with index $3$, we have to show there is a subgroup $K$ of $G$ with index $2$, assuming that $H$ is not a normal subgroup of $G$. My line of thinking was the following: So $[G:H]=3$ and $H$ is not normal. This means there is a smaller prime which can divide $G$, thus, $2$ divides $G$. We then use Cauchy's theorem to say that there is an element $g$ in $G$ with order $2$. Will it work to then use the permutation representation of left multiplication on $G$ and the existence of an odd permutation? Any hint in the right direction will be much appreciated. Thanks.","['group-theory', 'normal-subgroups']"
1150567,Similar problem to Taylor's theorem proof,"43. Let $a_1,\dots,a_{n+1}$ be arbitrary points in $[a,b]$ and let
  $$Q(x)=\prod_{i=1}^{n+1}(x-x_i).$$
  Suppose that $f$ is $(n+1)$-times differentiable and that $P$ is a polynomial function of degree $\le n$ such that $P(x_i)=f(x_i)$ for $i=1,\dots,n+1$. Show that for each $x$ in $[a,b]$  there is a number $c$ in $(a,b)$ such that
  $$f(x)-P(x)= Q(x) \cdot \frac{f(n+1)(c)}{(n+1)!}.$$
  Hint: consider the function
  $$F(t)=Q(x)[f(t)-P(t)]-Q(t)[f(x)-P(x)].$$
  Show that $F$ is zero at $n+2$ different points in $[a,b]$, and use Problem 42. Problem 42 was: ""Suppose that $f$ is $n$ times differentiable and that $f(x) = 0$ for $n+1$ different $x$. Prove that $f^{(n)}(x) = 0$ for some $x$. "" I will start: $$F(t) = Q(x)[f(t) - P(t)] - Q(t)[f(x) - P(x)]$$ $$F(a) = Q(x)[f(a) - P(a)] - Q(a)[f(x) - P(x)]$$ $$Q(a)  = \prod_{k=1}^{n+1} x- x_k = \prod_{k=1}^{n+1} a- a_k = (a- a_1)(a - a_2).... (a - a_{n+1})$$ But I don't see any options? Can someone guide me? Thanks! PLEASE DO NOT GIVE A FULL ANSWER","['calculus', 'derivatives', 'real-analysis', 'analysis']"
1150573,Which rational functions $\mathbb{P}^1\rightarrow k$ are regular at the point at infinity?,"I am trying to solve a problem in Â§ 1.7 of Shafarevich's ""Basic Algebraic Geometry 1"": ""Let $k$ be an algebraically closed field. Which rational functions $\mathbb{P}^1\rightarrow k$ are regular at the point at infinity? What order of zero do they have there?"" Here's my attempt so far: Let $u$ be a rational function on $\mathbb{P}^1$ that is regular at the point at infinity $\mathcal{O}:=[1:0]$. I have already proved in another exercise that if $u$ is regular on all of $\mathbb{P}^1$ then it is constant; hence let $u=p/q$ where $p$ and $q$ are homogeneous polynomials in $k[X,Y]$ of the same degree $n\geq 1$, say $p = \sum_{i+j = n} a_{i j} X^i Y^j$ and $q = \sum_{i+j = n} b_{i j} X^i Y^j$. Then since $u$ is regular at $\mathcal{O}$, $q(\mathcal{O}) = b_{n0} \neq 0$ and we see $u(\mathcal{O}) = a_{n 0}/b_{n 0}\in k$. From the question it looks like we ought to be able to show $a_{n 0} = 0$, but I don't really know how to proceed from here. One idea I had was to suppose $a_{n 0}\neq 0$ and look at what happens to $u$ on the affine part $\mathbb{A}_y^1 :=\left\{[a:b]\in\mathbb{P}^1 : b\neq 0\right\}$, but I can't seem to get anywhere with this. Any tips would be appreciated!!","['algebraic-geometry', 'self-learning']"
1150586,Is every Volterra's function unbounded?,"Volterra's function is a function $f\colon\mathbb{R}\to\mathbb{R}$ such that: $V$ is differentiable, $V'$ is bounded, $V'$ is not Riemann-integrable. http://en.wikipedia.org/wiki/Volterra%27s_function Is every Volterra's function unbounded? I've searched the site and found some results, like What is an example that a function is differentiable but derivative is not Riemann integrable Bounded Function Which is Not Riemann Integrable but is doesn't deal with boundedness of a function.","['derivatives', 'integration', 'real-analysis']"
1150589,Differential equation $\cos(f')=\cos(f)'$,The differential equation $$\sin(f')=\sin(f)'$$ has the trivial solution $f=0$. Does the equation $$\cos(f')=\cos(f)'$$ have any solutions?,"['ordinary-differential-equations', 'calculus']"
1150616,Closed form for $\int \left(1-x^{2/3}\right)^{3/2}\:dx$,"Find a closed-form solution to
\begin{align}\int_0^1 \left(1-x^{2/3}\right)^{3/2}\:dx\tag{1},\end{align}
or even more generally, is there a methodology to solving integrals of the type
\begin{align}
\int_0^1\left(1-x^r\right)^{1/r}\:dx,\:\:r\in\left\{s\in\mathbb{Q}:0<s<1\right\}.\tag{2}
\end{align}
This integral arose when trying to calculate the area of a Superellipse. Thank you for your time,","['definite-integrals', 'closed-form', 'integration']"
1150619,Application of change of variable to the density of an exponential family?,"Suppose we have an exponential model that admits a canonical parametrization so that its densities can be written as
$$
l(y;\theta)=C(\theta)\exp\left[\theta^*T(y)+\sum_{i=1}^{p-1}\lambda_iS_i(y)\right]\tag{$*$}
$$
where $\theta^*,T(y)\in\mathbb{R}$ and $\theta=(\theta^*,\lambda_1,\ldots,\lambda_{p-1})'$. Let $S=(S_1,\ldots,S_{p-1})'$. My text then claims: The statistic $(T,S')'$ is sufficient for $\theta$. Thus, we can consider the model induced by this statistic. The probability distributions of the induced model have densities of the form
  $$
l(t,s;\theta)=\hat{C}(\theta)\exp\left[\theta^*t+\sum_{i=1}^{p-1}\lambda_is_i\right]\tag{$**$}.
$$ I understand the claim about sufficiency but where does ($*$$*$) come from? I thought about using a change of variable formula (assuming that the conditions for its application are satisfied). But there would always be an extra term involving $t,s$ that I didn't know how to get rid of. Thank you very much.","['statistics', 'probability-theory']"
1150642,Solve Heat Equation using Fourier Transform (non homogeneous),"I know how to solve heat equation where it's like $u_t=k\cdot u_{xx}$ (using Fourier Transform or using Separation of Variables) but this exercise is really difficult for me. I have this: $$u_t(x,t)=k \cdot u_{xx}(x,t)-a\cdot k \cdot u(x,t)$$
$$u_x(0,t)=0$$
$$u(x,0) = f(x)$$ with $x>0, t>0$ and $a, k$ are positive constants. I have to find $u(x,t)$ and propose a possible $f(x)$ Any help? Thanks I was told I cannot use Fourier Transform, I have to use Fourier Cosine Transform, and I don't know why","['ordinary-differential-equations', 'fourier-transform', 'partial-differential-equations', 'integral-transforms', 'heat-equation']"
1150727,"$M$ maximal in a ring $R$, what is $R/M$?","I just proved that if $R$ is a commutative ring with unity, then $M$ maximal ideal implies that $R/M$ is a field, and the converse is also true. I have the following questions: If $R$ is a ring with unity, would $R/M$ be a division ring? Is the converse true? If $R$ is a ring (possibly without unity), then what is $R/M$? If $R/M$ is a field, what we can say about $R$?","['ring-theory', 'abstract-algebra']"
1150733,$ \sqrt{3} (x \dot{x} + y \dot{y} ) = \dot{x} y - x \dot{y} $,"I am tryting to solve this differential equation for $x=x(t)$, $y=y(t)$ satisfying
                        $ \sqrt{3} (x \dot{x} + y \dot{y} ) = \dot{x} y - x \dot{y}  $ and $ \dot{x} ^2 + \dot{y} ^2 = v^2 =constant $ with initial value $(x(0),y(0))=(1,\sqrt{3})$ if needed. Any hint would be a great help for me. Thanks.","['differential', 'ordinary-differential-equations']"
1150743,A compact infinite topological group with only two closed subgroups,"It can be proved every compact infinite abelian topological group $(A,\tau )$, with $\tau$ nontrivial, has at least three distinct closed subgroups. Is there any compact infinite non-abelian topological group $(G,\mathcal T)$, with $\mathcal T$ nontrivial, which has only two closed subgroups?","['general-topology', 'examples-counterexamples', 'topological-groups', 'group-theory']"
1150778,Inductive definition of power set for finite sets,"I'm stuck on a problem using recursive definitions: Let $X$ be a finite set. Give a recursive definition of the set of all subsets of $X$. Use Union as the operator in the definition. I can see how the union of all subsets separately gives a set of all subsets, but I don't understand how to prove it. I'm not even sure what the base case would be in this situation.","['induction', 'elementary-set-theory']"
1150822,"Closed form for $\int_0^\infty\arctan\Bigl(\frac{2\pi}{x-\ln\,x+\ln(\frac\pi2)}\Bigr)\frac{dx}{x+1}$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I'm trying to find a closed form for this integral:
$$I=\int_0^\infty\arctan\left(\frac{2\pi}{x-\ln\,x+\ln\left(\frac\pi2\right)}\right)\frac{dx}{x+1}$$
Its approximate numeric value is
$$I\approx3.3805825284453469793953592216276992165696856825906055108192183...$$ Any help is appreciated. Thanks!","['closed-form', 'calculus', 'integration', 'definite-integrals', 'logarithms']"
1150859,The sum of the reciprocal of primeth primes,"A few days ago, a friend of mine taught me that the sum of the reciprocal of primeth primes $$\frac{1}{3}+\frac{1}{5}+\frac{1}{11}+\frac{1}{17}+\frac{1}{31}+\cdots$$ converges . Does anyone know some papers which have a rigorous proof with the convergence value?","['prime-numbers', 'reference-request', 'number-theory']"
1150888,If $f(x)=f(x')$ implies $G(x)=G(x')$ then $G(x)=h(f(x))$ for some function $h$,"Suppose I have two functions $f$ and $G$ defined on the same domain $X$. Suppose that whenever $f(x)=f(x')$ for any $x,x'\in X$, we have $G(x)=G(x')$. Then, it's intuitive that knowing $f(x)$ amounts to knowing $G(x)$ so $G$ depends on $x$ only via $f(x)$. But is it possible to demonstrate this more formally please? Thank you very much.","['calculus', 'functions']"
1150909,Finding the character table of $Q_8$,"Assume $G$ is a finite group. I am trying to construct the character table of $Q_8$ , which is defined by $$Q_8=\{\pm 1,\pm i, \pm j,\pm k \}, \ i^2=j^2=k^2=-1, \ ij=k,jk=i,ki=j$$ By considering the $G'$ , I can prove that $|G|=8=1^2+1^2+1^2+1^2+2^2$ . Therefore $G$ has four $1$ -dimensional representations and one $2$ -dimensional representation. So $$ \begin{array}{|c|c|c|c|}
\hline
&1& -1 & \{\pm i \} & \{\pm j \} & \{\pm k \} \\ \hline
\chi_0 & 1 & 1  & 1 & 1 & 1 \\ \hline
\chi_1 & 1 & ? & ?&? & ? \\ \hline
\chi_2 & 1 & ? &? & ?& ? \\ \hline
\chi_3 & 1 & ? & ?&? & ? \\ \hline
\chi_4 & 2 & ? & ? & ? & ? \\ \hline
\end{array}
$$ I guessing the first vertical column is a 2 at the bottom because $tr(I_2)=2$ . The first row is $1$ 's because it is the 1-d trivial representation. I am unsure of how to proceed from here.
Should I be able to identity the $\chi_i$ 's? Please do not give full solutions as otherwise I will not learn. If it helps its fairly easy to show that $Q_8 / Q_8' \simeq \mathbb{Z}_2 \times \mathbb{Z}_2$","['representation-theory', 'group-theory', 'characters']"
1150910,Is there a function that is differentiable but not integrable? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question is there such a function that is defined in [0,1], differentiable in (0,1) but not integrable? Thanks in advance.","['integration', 'derivatives']"
1150912,Fourier transform and dual vector space,"In Serre's A Course In Arithmetic , it says the following: I don't know what it is talking about, I know the definition of $f'$, but what is This is in the last sentence refered to? $f'$ is a function on $V$, not $V'$. So what is such rapid decreasing function $g$ that have domain on $V'$? BTW, it would be helpful if someone can also explained such measure $\mu$ is $invariant$ by what function?","['analytic-number-theory', 'fourier-analysis', 'functional-analysis', 'real-analysis']"
1150955,Is the module of invariant differential forms of a Neron model of an abelian variety a projective module?,"Let $A$ be an abelian variety of dimension $d$ over a number field $K$. Let $\mathcal{A}$ be its Neron model over the ring of integers $O_K$. Let $\Omega_{\mathcal{A}/O_K}$ and $\Omega_{A/K}$ be the sheaves of differential forms of $\mathcal{A}$ and $A$ respectively. Then $\Omega_{A/K}$ and  $\Omega_{\mathcal{A}/O_K}$ are locally free $O_A$ and $O_{\mathcal{A}}$-modules respectively as $A$ and $\mathcal{A}$ are smooth group schemes. 
Furthermore, $\Omega_{A/K}$ is a free $O_A$-module, generated by invariant differential forms (cor 3, page 102, Neron Models, Bosch-Lutkebohmert-Raynauld). Let $\Omega_{\mathcal{A}/O_K}^{inv}(\mathcal{A})$ and $\Omega_{{A}/K}^{inv}({A})$ be the subset of invariant differential forms of $\Omega_{\mathcal{A}/O_K}(\mathcal{A})$ and $\Omega_{{A}/K}({A})$ respectively. I know that $\Omega_{{A}/K}^{inv}({A})$ is a $d$-dimensional $K$-vector space. Is it true that $\Omega_{\mathcal{A}/O_K}^{inv}(\mathcal{A})$ is a projective $O_K$-modules of rank $d$ and $\Omega_{\mathcal{A}/O_K}^{inv}(\mathcal{A}) \otimes_{O_K}K \simeq \Omega_{{A}/K}^{inv}({A})$ ? Thank you very much.","['arithmetic-geometry', 'algebraic-geometry']"
1150990,Dimesion of a subspace subject to linear constraints,"Suppose $X$ is $n\times K$ with full column rank $K$ and $G$ is $q\times K$ with full row rank $q$. If $q<K$, how do I see that $\mathcal{L}\equiv\{Xb,b\in\mathbb{R}^K,Gb=0\}$ has dimension 
  $K-q$? Because $X$ has full column rank, its column space can be spanned by $K$ independent column vectors in $\mathbb{R}^n$. The constraint $Gb=0$ imposes limits on the coefficients contained in $b$ when we form linear combinations of these $K$ vectors. But I don't know how to proceed formally. Thank you for your help.","['vector-spaces', 'matrices', 'linear-algebra']"
1151007,If $a$ is an odd integer then $x^2+x-a = 0$ has no integer solutions,"I'm suppose to prove by contrapositive that if $a$ is an odd integer then the equation $x^2+x-a=0$ has no integer solution. By contrapositive: If the equation $x^2+x - a = 0$ has an integer solution then $a$ is an even integer. So I attempt to apply the quadratic formula and have this result $\frac{-1 \pm \sqrt{1 - 4a}} 2$. I have no idea how I'm suppose to get an integer solution from this, let alone an even solution. I've tried to multiply by the  conjugate but it gets really messy and I feel that I'm over-thinking it. Thanks for your help.","['quadratics', 'algebra-precalculus']"
1151017,Can a product of a Stein manifold and a compact manifold be again Stein?,"A Stein manifold is a manifold which is holomorphically separable and convex. It is well known that a product of two holomorphically convex (resp. Stein) manifolds is again holomorphically convex (resp. Stein). Also, compact complex manifolds automatically have the property of being holomorphically convex but they are not holomorphically separable, of course. This has led me to the following question: Given a Stein manifold $X$ and a compact complex manifold $Y$ (say of dimension $\geq 1$), when is $X \times Y$ Stein? Due to the remark made above, one only has to find a condition for $X \times Y$ to be holomorphically separable. Let $(x,y) \neq (x',y')$ be points of $X \times Y$. If $x \neq x'$, then everything is easy, as one could simply project onto $X$ and choose a separating function there. But what if $x = x'$ and $y \neq y'$? One cannot do the same for $Y$, as every holomorphic function on $Y$ is constant. So, when is it possible to separate points in this case? Never? Always? Does one have to add more conditions on $X$ and $Y$? Remark: I asked myself this question in the much more special situation where $X \subset \mathbb{C}$ is a disc and $Y = \mathbb{P}^1$, but I decided to pose the question in this more general fashion.","['complex-geometry', 'several-complex-variables', 'complex-analysis', 'compact-manifolds']"
1151024,"On the polynomial ring $ \mathbb{R}[x,y] $, and the sine and cosine functions.","I was investigating relationships between commutative algebra and real analysis when the following problem came into mind. Problem. Let $ P \in \mathbb{R}[x,y] $. If $ P(\sin(\theta),\cos(\theta)) = 0 $ for all $ \theta \in \mathbb{R} $, then is it true that $ P $ lies in the principal ideal $ \langle x^{2} + y^{2} - 1 \rangle $? This problem does not appear difficult, but I have yet to find a solution. Thanks!","['ring-theory', 'polynomials', 'trigonometry', 'real-analysis', 'ideals']"
1151040,Closed form of factorial and cascading power sum,"Consider the following sum: $$ \sum_{i =0}^{j} \left( \frac{(j-i)^ix^i \ln(x)^{(j-i)}\ln(x)^i}{(j-i)!i!} \right) $$ I can simplify the sum to: $$ \ln(x)^j\sum_{i =0}^{j} \left( \frac{(j-i)^ix^i}{(j-i)!i!} \right) $$ 
Furthermore I can observe that $$ \frac{1}{(j-i)!(i!)}  = \frac{1}{j!} \begin{pmatrix} j \\ i\end{pmatrix} $$ Thus: $$ \frac{\ln(x)^j}{j!}\sum_{i =0}^{j} \left( \begin{pmatrix}j \\ i \end{pmatrix}(j-i)^ix^i \right) $$ But I don't know how to go in for the kill.","['sequences-and-series', 'calculus', 'generating-functions', 'hypergeometric-function', 'combinatorics']"
1151041,Is every invertible matrix a change of basis matrix?,"In the course that I am having, we are treating change of basis matrices as the matrices of the identity operation from one basis S to another basis say B . So, our instructor introduced a theorem : If A and B are similar, i.e., S âˆ’1 AS = B for an invertible matrix S , then they have the same characteristic polynomial. In particular, they have the same eigenvalues, det( A ) = det( B ) and Trace( A ) = Trace( B ). And hence came the question here... Is every invertible matrix a change of basis matrix? 
Every change of basis matrix is certainly invertible, is the converse true?","['matrices', 'linear-algebra', 'inverse']"
1151059,Is this linear map bounded?,"This question asks to prove the following: Let $X$ and $Y$ be Banach spaces.  If $T: X \to Y$ is a linear map such that $f \circ T \in X^*$ for every $f \in Y^*$, then $T$ is bounded. The assumption that $Y$ is complete seems redundant, and the assumption that $X$ is complete is invoked only when applying the uniform boundedness principle (if $X$ is complete, it must be nonmeager by the Baire category theorem). So I'm trying to either prove that $T$ is still bounded if $X$ and $Y$ are arbitrary normed vector spaces (over $\mathbb{R}$ or $\mathbb{C}$), or else to find a counterexample that illustrates that $X$ must be complete. Any suggestions would be greatly appreciated!","['functional-analysis', 'real-analysis', 'banach-spaces']"
1151090,Minkowski metric on a surface,Do closed surfaces admit a metric with lorentzian signature? Any reference?,"['mathematical-physics', 'surfaces', 'differential-geometry']"
1151094,Maximum element of Perron vector,"Suppose $A$ is an entrywise nonnegative matrix that is symmetric, irreducible and has a zero diagonal. By Perron-Frobenius theorem, the spectral radius $\rho(A)$ is an eigenvalue and $A$ has, up to scaling, a unique entrywise positive eigenvector $v$ (i.e. the Perron vector ) for this eigenvalue. Let $r_i$ denotes the $i$-th row sum of $A$. In general, the maximum row sum of $A$ and the maximum entry of $v$ occur in different positions. For example, when
$$
A=\pmatrix{0&0&0&2\\ 0&0&3&2\\ 0&3&0&3\\ 2&2&3&0},
$$
the Perron vector is $(0.2067,\ 0.5236,\ 0.5908,\ 0.5780)^\top$. Therefore the maximum entry of the Perron vector is the third one, but the maximum row sum of $A$ occurs in the fourth row. However, in the computer experiments that I have carried out, this kind of exceptional cases are relatively infrequent. So, it seems that by imposing some mild conditions, one may force $\arg\max_i r_i=\arg\max_i v_i$. Now my question is: Under what conditions can we conclude that $\arg\max_i r_i=\arg\max_i v_i$? (Let's ignore the pathological cases where $\arg\max_i r_i$ or $\arg\max_i v_i$ are not unique.) This question arose when I was pondering what could happen if we apply Google style ranking to a weighted undirected graph. If $\arg\max_i r_i=\arg\max_i v_i$, there is no need to compute the Perron vector if I only want to find the highest-ranked member.","['matrices', 'linear-algebra', 'nonnegative-matrices']"
