question_id,title,body,tags
2870410,"Prove that $\int_0^\infty\,\frac{\sin(kx)}{x(x^2+1)}\,\text{d}x=\frac{\pi}{2}\,\left(1-\exp(-k)\right)$ for all $k\in\mathbb{R}_{\ge0}$.","I found the following result (I apologize if this has been posted before, but I could not find anything here).  I am wondering whether there is an approach without using contour integration. Let $k$ be a nonnegative real number.  Prove that $$\int_0^\infty\,\frac{\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x=\frac{\pi}{2}\,\Big(1-\exp(-k)\Big)=\pi\,\exp\left(-\frac{k}{2}\right)\,\sinh\left(\frac{k}{2}\right)\,.$$ I am especially interested in a solution not using complex analysis.  However, a complex-analytic solution that is different from mine is very welcome as well.  I have three approaches, but all of them end up using complex analysis as a major part. We also have this very nice consequence.  This result can be proven on its own without knowing the integral $\displaystyle \int_0^\infty\,\frac{\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x$. Let $k$ be a nonnegative real number.  Then,
  $$\int_0^\infty\,\frac{1-\cos(kx)}{x^2\,\left(x^2+1\right)}\,\text{d}x=\frac{\pi}{2}\,\big(k-1+\exp(-k)\big)\,.$$
  Equivalently,
  $$\int_0^\infty\,\frac{\sin^2(kx)}{x^2\,\left(x^2+1\right)}\,\text{d}x=\frac{\pi}{4}\,\big(2k-1+\exp(-2k)\big)\,.$$ Interestingly, during my quest to obtain this integral, I discovered two more integral relations, although I do not know how to obtain the exact values of any of them.  The exact values of these integrals involve the exponential integral $\text{Ei}$, where $\text{Ei}(x)=\displaystyle\text{PV}\int_{-\infty}^x\,\frac{\exp(t)}{t}\,\text{d}t$ for all $x\in\mathbb{R}$.  Both of the results below also came from contour integrations. Let $k$ be a nonnegative real number.  Then,
  $$\int_0^\infty\,\frac{1-\cos(kx)}{x\,\left(x^2+1\right)}\,\text{d}x=-\frac{2}{\pi}\,\int_0^\infty\,\frac{\ln(x)\,\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x\,.$$
  and
  $$\int_0^\infty\,\frac{\sin(kx)}{x^2+1}\,\text{d}x=-\frac{2}{\pi}\,\int_0^\infty\,\frac{\ln(x)\,\cos(kx)}{x^2+1}\,\text{d}x\,.$$ Mathematica says that 
$$\int_0^\infty\,\frac{\sin(kx)}{x^2+1}\,\text{d}x=\frac{\exp(-k)\,\text{Ei}(+k)-\exp(+k)\,\text{Ei}(-k)}{2}$$
and that
$$\int_0^\infty\,\frac{1-\cos(kx)}{x\,\left(x^2+1\right)}\,\text{d}x=\gamma+\ln(k)-\frac{\exp(-k)\,\text{Ei}(+k)+\exp(+k)\,\text{Ei}(-k)}{2}\,.$$
Here, $\gamma\approx 0.57722$ is the Euler-Mascheroni constant . Approach I. Consider the meromorphic function $f(z):=\dfrac{\exp(\text{i}kz)}{z(z^2+1)}$ for all $z\in \mathbb{C}\setminus \{0,-\text{i},+\text{i}\}$.  For $\epsilon\in(0,1)$, let $C_\epsilon$ be the positively oriented contour $$\left[+\epsilon,+\frac{1}{\epsilon}\right]\cup \Biggl\{\frac{\exp(\text{i}\theta)}{\epsilon}\,\Bigg|\,\theta\in[0,\pi]\Biggr\}\cup\left[-\frac{1}{\epsilon},-\epsilon\right]\cup \Big\{\epsilon\,\exp(\text{i}\theta)\,\Big|\,\theta\in[\pi,0]\Big\}\,.$$  Write $\Gamma_r$ for the positively-oriented (with respect to $0$) semicircle $\Big\{r\,\exp(\text{i}\theta)\,\Big|\,\theta\in[0,\pi]\Big\}$ for every $r>0$.  We have $$I(k):=\lim\limits_{\epsilon\to 0^+}\,\oint_{C_\epsilon}\,f(z)\,\text{d}z=2\pi\text{i}\,\text{Res}_{z=\text{i}}\big(f(z)\big)=-\pi\text{i}\,\exp(-k)\,.$$  Now, note that $$\lim_{\epsilon\to0^+}\,\int_{\Gamma_\epsilon}\,f(z)\,\text{d}z=\pi\text{i}\text{ and }\lim_{\epsilon\to0^+}\,\int_{\Gamma_{\frac{1}{\epsilon}}}\,f(z)\,\text{d}z=0\,.$$  Because $$I(k)=\int_0^\infty\,\frac{\exp(+\text{i}kx)-\exp(-\text{i}kx)}{x(x^2+1)}\,\text{d}x-\lim_{\epsilon\to0^+}\,\int_{\Gamma_\epsilon}\,f(z)\,\text{d}z+\lim_{\epsilon\to0^+}\,\int_{\Gamma_{\frac{1}{\epsilon}}}\,f(z)\,\text{d}z\,,$$ we see that $$I(k)=2\text{i}\,\int_0^\infty\,\frac{\sin(kx)}{x(x^2+1)}\,\text{d}x-\pi\text{i}\,.$$ The result follows immediately. Approach II. We apply Richard Feynman's integral trick.  First, define $J(k)$ to be the required integral: $$J(k):=\int_0^\infty\,\frac{\sin(kx)}{x(x^2+1)}\,\text{d}x\,.$$  Thus, by the Leibniz Integral Rule, we have $J'(k)=\displaystyle \int_0^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x$.   Let $g(z):=\dfrac{\exp(\text{i}kz)}{z^2+1}$ for all $z\in\mathbb{C}\setminus\{-\text{i},+\text{i}\}$.  It follows that $$\lim_{\epsilon\to0^+}\,\oint_{C_\epsilon}\,g(z)\,\text{d}z=2\,\int_{0}^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x\,,$$ where $C_\epsilon$ is the positively oriented contour $$\left[+\epsilon,+\frac{1}{\epsilon}\right]\cup \Biggl\{\frac{\exp(\text{i}\theta)}{\epsilon}\,\Bigg|\,\theta\in[0,\pi]\Biggr\}\cup\left[-\frac{1}{\epsilon},-\epsilon\right]\cup \Big\{\epsilon\,\exp(\text{i}\theta)\,\Big|\,\theta\in[\pi,0]\Big\}\text{ for }\epsilon\in(0,1)\,.$$   Furthermore, we have $$\oint_{C_\epsilon}\,g(z)\,\text{d}z=2\pi\text{i}\,\text{Res}_{z=\text{i}}\big(g(z)\big)=\pi\,\exp(-k)\text{ for all }\epsilon\in(0,1)$$   Ergo, $J'(k)=\displaystyle\int_{0}^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x=\dfrac{\pi}{2}\,\exp(-k)$.  Since $J(0)=0$, $$J(k)=\int_0^k\,J'(t)\,\text{d}t=\frac{\pi}{2}\,\int_0^k\,\exp(-t)\,\text{d}t=\frac{\pi}{2}\,\big(1-\exp(-k)\big)\,.$$ Approach III. It is easy to see that $\dfrac{\sin(t)}{t}=\displaystyle\frac{1}{2}\,\int_{-1}^{+1}\,\exp(\text{i}t\tau)\,\text{d}\tau$ for all $t\neq 0$.  That is, the required integral is given by $$\begin{align}J(k):=\int_0^\infty\,\frac{\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x&=\frac{1}{2}\,\int_{0}^\infty\,\frac{k}{x^2+1}\,\int_{-1}^{+1}\,\exp(\text{i}kxt)\,\text{d}t\,\text{d}x\\&=\frac{1}{2}\,\int_{-\infty}^{+\infty}\,\frac{k}{x^2+1}\,\int_{0}^{1}\,\exp(\text{i}kxt)\,\text{d}t\,\text{d}x\,.\end{align}$$  Using Fubini's Theorem, we obtain $$J(k)=\frac{1}{2}\,\int_0^1\,k\,\int_{-\infty}^{+\infty}\,\frac{\exp(\text{i}kxt)}{x^2+1}\,\text{d}x\,\text{d}t\,.$$ For a real number $R>1$, let $\gamma_R$ be the positively oriented contour $$[-R,+R]\cup\big\{R\,\exp(\text{i}\theta)\,\big|\,\theta\in[0,2\pi]\big\}\,.$$  Then, for $\omega \geq 0$, we have $$\lim_{R\to\infty}\,\oint_{\gamma_R}\,\frac{\exp(\text{i}\omega z)}{z^2+1}\,\text{d}z=\int_{-\infty}^{+\infty}\,\frac{\exp(\text{i}\omega x)}{x^2+1}\,\text{d}x=:K(\omega)\,.$$  Ergo, $$K(\omega)=2\pi\text{i}\,\text{Res}_{z=\text{i}}\left(\frac{\exp(\text{i}\omega z)}{z^2+1}\right)=2\pi\text{i}\,\left(\frac{\exp(-\omega)}{2\text{i}}\right)=\pi\,\exp(-\omega)\,.$$ As $\displaystyle J(k)=\frac{1}{2}\,\int_0^1\,k\,K(kt)\,\text{d}t$, we conclude that $$J(k)=\frac{\pi}{2}\,\int_0^1\,k\,\exp(-kt)\,\text{d}t=\frac{\pi}{2}\,\big(1-\exp(-k)\big)\,.$$  From this proof, we also obtain $$\int_0^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x=\frac{1}{2}\,K(k)=\frac{\pi}{2}\,\exp(-k)\,.$$ P.S. I put my solutions in spoilers for people who want to try to solve the problem without being led on by my attempts.  If you do not want to waste your time doing what I have already done, then please look at the spoilers.  All of the solutions in the spoilers are complex-analytic proofs anyhow, so if you are giving me a real-analytic proof, then there is no way you are going to replicate my work.","['integration', 'improper-integrals', 'definite-integrals', 'real-analysis', 'contour-integration']"
2870420,A proof of the product rule using the single variable chain rule?,"A while back I saw someone claim that you could prove the product rule in calculus with the single variable chain rule. He provided a proof, but it was utterly incomprehensible. It is easy to prove from the multi variable chain rule, or from logarithmic differentiation, or even from first principles. Is there an actual proof using just the single variable chain rule?","['alternative-proof', 'calculus']"
2870422,Partition of 4-tuples,"Some $4$-tuples of positive real numbers $(a_1,b_1,c_1,d_1),\dots,(a_n,b_n,c_n,d_n)$ are given, with all $a_i,b_i,c_i,d_i\leq 1$. Can we always partition $\{1,2,\dots,n\}$ into two subsets $X,Y$ so that $$1+\sum_Xa_i\geq \sum_Ya_i\text{ and } 1+\sum_Xb_i\geq \sum_Yb_i$$
and $$\sum_Xc_i\leq 1+\sum_Yc_i\text{ and } \sum_Xd_i\leq 1+\sum_Yd_i?$$ It could be that some inequalities need to be exactly tight, for example if $n=1$ and the only tuple is $(1,1,1,1)$. This also means that the number $1$ in the inequalities cannot be replaced by a smaller number. Mike Earnest shows below that the partition is possible if we replace the $1$'s by $3$'s. What is the best possible value between $1$ and $3$?","['inequality', 'combinatorics']"
2870436,$\emptyset$ is a function & $\emptyset$ is not a function,"The empty set is a function. Let A $\neq$ $\emptyset$. We know that the following is true: 0) $f$: $\emptyset$ $\rightarrow$ A is a function & $g$: A $\rightarrow$ $\emptyset$ is not a function. We also know the following are true: 1) $g$ $\subseteq$ A $\times$  $\emptyset$ = $\emptyset$.  So $g$ = $\emptyset$. Hence: A $\times$  $\emptyset$ = $g$. 2) $f$ $\subseteq$  $\emptyset$ $\times$ A = $\emptyset$. So $f$ = $\emptyset$. Hence:   $\emptyset$ $\times$ A = $f$. Some obvious consequences of this data: $f$ is a function, $g$ is not a function, $f$=$\emptyset$ & $f$=$g$  (by extensionality). This gives us: 3) $\emptyset$ is a function & $\emptyset$ is not a function (since $f$ and $g$ are the same object). This is a contradiction. Question: It's clear that $f$ vacuously satisfies the conditions required for a set to be a function and $g$ fails to satisfy those conditions. But since $f$ and $g$ just are the same set (by 1) & 2) they are the empty set), I'm having difficulty articulating why this doesn't lead to a contradiction. Where have I gone wrong?","['elementary-set-theory', 'functions']"
2870443,Number of inflection points of $(x-2)^6(x-3)^9$,"Find the number of inflection points of $(x-2)^6(x-3)^9$ Attempt: If $f(x)$ has $n$ critical points then even $f(x+a)$ will also have $n$ critical points. So we can simplify it to finding the number of inflection points  of $(x)^6(x-1)^9$. I have evaluated the double derivative to be: $$6(x-1)^7x^4(35x^2-28x +5)$$ Clearly, the double derivative is $0$ at $4$ points, $0$, $1$ and the roots of the quadratic. But curvature is not changing around $x=0$ so it's not an inflection point. Thus, there should be $3$ inflection points. But answer given is $1$ inflection point only. Please let me know what concept am I missing on. I tried my level best to zoom into the graph and catch three critical points but there appears to be only one.","['calculus', 'functions', 'derivatives']"
2870484,Composition of Formal Power Series,"Is there a known closed form for the composition of the following power series:
$$f(x) = \exp(x) = \sum_k \frac{x^k}{k!}$$
$$g(x) = \frac{1}{1-x} = \sum_n x^n$$ I'd like a closed form power series for $g(f(x)) = \frac{1}{1-\exp (x)}$. I've been trying to use the di Bruno formula from Wikipedia but I find it very confusing, I'd appreciate some help.","['power-series', 'sequences-and-series', 'real-analysis']"
2870490,Find the least value for $\sin x - \cos^2 x -1$,"Find all the values of $x$ for which the function $y = \sin x - \cos^2 x -1$ assumes the least value. What is that value? At first I found the first derivative to be $y' = \cos x + 2 \sin x \cos x$. Critical point $0$, $-π/6$ (principal) $y'' = - \sin x + 2(\cos 2x)$ Then substitution of $x$ by critical points I found minima. But my answer is incorrect. Correct minimum value is $-9/4$","['real-analysis', 'maxima-minima', 'optimization', 'trigonometry', 'derivatives']"
2870508,"Given a positive integer $m$ , how can I construct a positive integer $k$ such that $\varphi(n)=k$ has exactly $m$ solutions?","In this article : https://en.wikipedia.org/wiki/Euler%27s_totient_function it is stated that for every positive integer $m\ge 2$, there exists a postive integer $k$, such that $\varphi(n)=k$ has exactly $m$ solutions in positive integers $n$. How can I construct such a number $k$ for a given $m$ ? I determined such a $k$ for $2\le m\le 100$ , but I did not find a useful pattern to see how to get a number for arbitary $m$ Here the values I found : m   k

2  10
3  44
4  4
5  8
6  12
7  32
8  36
9  40
10  24
11  48
12  160
13  396
14  2268
15  704
16  312
17  72
18  336
19  216
20  936
21  144
22  624
23  1056
24  1760
25  360
26  2560
27  384
28  288
29  1320
30  3696
31  240
32  768
33  9000
34  432
35  7128
36  4200
37  480
38  576
39  1296
40  1200
41  15936
42  3312
43  3072
44  3240
45  864
46  3120
47  7344
48  3888
49  720
50  1680
51  4992
52  17640
53  2016
54  1152
55  6000
56  12288
57  4752
58  2688
59  3024
60  13680
61  9984
62  1728
63  1920
64  2400
65  7560
66  2304
67  22848
68  8400
69  29160
70  5376
71  3360
72  1440
73  13248
74  11040
75  27720
76  21840
77  9072
78  38640
79  9360
80  81216
81  4032
82  5280
83  4800
84  4608
85  16896
86  3456
87  3840
88  10800
89  9504
90  18000
91  23520
92  39936
93  5040
94  26208
95  27360
96  6480
97  9216
98  2880
99  26496
100  34272","['number-theory', 'totient-function', 'elementary-number-theory']"
2870519,ln(2) contradiction,"$\ln2\approx.693$, according to my calculator. It can be written as the infinite sum $$1-\frac12+\frac13-\frac14+\frac15-\frac16+\frac17-\frac18+\frac19-\frac1{10}\dots$$ Rearranging this infinite sum by odds and evens gives: $$\left(1+\frac13+\frac15+\frac17+\frac19\dots\right)-\left(\frac12+\frac14+\frac16+\frac18+\frac1{10}\dots\right)$$ This is the same as: $$\left(1+\frac13+\frac15+\frac17+\frac19\dots\right)+\left(\frac12+\frac14+\frac16+\frac18+\frac1{10}\dots\right)-2\left(\frac12+\frac14+\frac16+\frac18+\frac1{10}\dots\right)$$ Combining the first two parentheses: $$\left(1+\frac12+\frac13+\frac14+\frac15+\frac16+\frac17+\frac18+\frac19+\frac1{10}\dots\right) -2\left(\frac12+\frac14+\frac16+\frac18+\frac1{10}\dots\right)$$ Distributing the 2: $$\left(1+\frac12+\frac13+\frac14+\frac15+\frac16+\frac17+\frac18+\frac19+\frac1{10}\dots\right)-\left(1+\frac12+\frac13+\frac14+\frac15+\frac16+\frac17+\frac18+\frac19+\frac1{10}\dots\right)=0$$ But we started out with the expansion of $\ln2\approx.693$. So how did we go from that to $0$? Where did I make my mistake?","['algebra-precalculus', 'fake-proofs', 'logarithms']"
2870574,Unique solution of an ODE with a bounded positive right-hand-side,"Consider the initial value problem $$\dot x(t) = F(t,x), \quad t \in (0,T)$$ with given initial datum $$x(0) = x_0 \in \mathbb R.$$ More precisely we consider the integral equation $$x(t)=x(0)+\int_0^t F(s,x(s))ds.$$ $F$ may be discontinuous, but let us assume that $$0 < m < F(t,x) < M.$$ The common counter-examples to uniqueness (or existence) of ODEs (or their associated integral equations) seem to rely on $F$ switching sign, or being close to $0$ , and my intuition is that the lower bound $m<F(x)$ should imply existence of a unique solution. Question 1: Is it true that there exist a solution under the assumptions above? Question 2: Can we also prove uniqueness?","['measure-theory', 'integral-equations', 'ordinary-differential-equations', 'reference-request', 'functional-analysis']"
2870606,Is the following ODE solvable?,"I would like to find solution to following ODE:
$$\frac{1}{\sin^{n-1}(x)}\frac{d}{dx}(\sin^{n-1}(x)\frac{df}{dx})-\frac{(n-1)f}{\sin^2(x)}=0$$
where $0<x\leq1,\ n\geq 2$ and $f(0)=0$. Is there any general way to solve this kind ODE? what happens if I substitute $\sin(x)$ by general functions. Any hint and reference will be appreciated. Remark: As many of you have realised, this ODE comes from calculating the eigenfunctions of Laplacian on sphere, but actually, I care more about the following quantity (it is Steklov eigenvalue if you have seen it):
$$\frac{f'}{f}\bigg|_{x=1}$$","['ordinary-differential-equations', 'partial-differential-equations']"
2870640,Paths must cross in Lindström-Gessel-Viennot on the lattice,"The following question (which I am going to answer myself) serves
to close a little gap in some combinatorial proofs that use the
Lindström--Gessel--Viennot lemma. Namely, I will show a little lemma,
which is mostly used tacitly. Roughly speaking, the lemma says that if $p$ and
$p^{\prime}$ are two lattice paths (using north and east steps only) in
$\mathbb{Z}^{2}$ such that the starting point of $p^{\prime}$ lies weakly
northwest of the starting point of $p$ but the ending point of $p$ lies weakly
northwest of the ending point of $p^{\prime}$, then $p$ and $p^{\prime}$ must
have a point in common. (I shall state this formally in Lemma 1 below.) See
also Proposition 2 for how this result gets used. The lemma may appear rather
intuitive (it is similar to the fact that the game of Hex cannot end in a
draw ), but the
short geometric arguments would be a pain to formalize, so I will prove it combinatorially. Basic definitions and Lemma 1 The lattice shall mean the (infinite) directed graph whose vertices are all
pairs of integers (that is, its vertex set is $\mathbb{Z}^{2}$), and whose
arcs are
\begin{align*}
\left(  i,j\right)   &  \rightarrow\left(  i,j+1\right)  \qquad\text{for all
}\left(  i,j\right)  \in\mathbb{Z}^2 \text{,} \qquad \text{and}\\
\left(  i,j\right)   &  \rightarrow\left(  i+1,j\right)  \qquad\text{for all
}\left(  i,j\right)  \in\mathbb{Z}^2 \text{.}
\end{align*}
The arcs of the first kind are called north-steps , whereas the arcs
of the second kind are called east-steps . The vertices of the lattice
will just be called vertices . We draw the lattice as the usual
integer lattice in the Cartesian plane. For each vertex $v=\left(  i,j\right)  $, we set $\operatorname*{x}\left(
v\right)  =i$ and $\operatorname*{y}\left(  v\right)  =j$. We refer to
$\operatorname*{x}\left(  v\right)  $ as the x-coordinate of $v$, and
to $\operatorname*{y}\left(  v\right)  $ as the y-coordinate of $v$. A path will simply mean a (directed) path in the lattice. Lemma 1. Let $A$, $B$, $A^{\prime}$ and $B^{\prime}$ be four vertices such
  that
  \begin{align*}
\operatorname*{x}\left(  A^{\prime}\right)    & \leq\operatorname*{x}\left(
A\right)  ,\qquad\operatorname*{y}\left(  A^{\prime}\right)  \geq
\operatorname*{y}\left(  A\right)  ,\\
\operatorname*{x}\left(  B^{\prime}\right)    & \leq\operatorname*{x}\left(
B\right)  ,\qquad\operatorname*{y}\left(  B^{\prime}\right)  \geq
\operatorname*{y}\left(  B\right)  .
\end{align*}
  Let $p$ be a path from $A$ to $B^{\prime}$. Let $p^{\prime}$ be a path from
  $A^{\prime}$ to $B$. Then, $p$ and $p^{\prime}$ have a vertex in common. See Fig. 1 for an illustration of Lemma 1. Fig. 1: Lemma 1. Non-intersecting lattice paths and Proposition 2 Now, fix a nonnegative integer $k$. Let $\left[  k\right]  $ be the set
$\left\{  1,2,\ldots,k\right\}  $. Let $\mathfrak{S}_{k}$ be the group of all
permutations of $\left[  k\right]  $. A $k$-vertex shall mean a $k$-tuple of vertices. If $\mathbf{v}
=\left(  A_{1},A_{2},\ldots,A_{k}\right)  $ is a $k$-vertex, and if $\sigma
\in\mathfrak{S}_{k}$, then $\sigma\left(  \mathbf{v}\right)  $ denotes the
$k$-vertex $\left(  A_{\sigma\left(  1\right)  },A_{\sigma\left(  2\right)
},\ldots,A_{\sigma\left(  k\right)  }\right)  $. If $\left(  A_{1},A_{2},\ldots,A_{k}\right)  $ and $\left(  B_{1},B_{2}
,\ldots,B_{k}\right)  $ are two $k$-vertices, then a NILP from
$\left(  A_{1},A_{2},\ldots,A_{k}\right)  $ to $\left(  B_{1},B_{2}
,\ldots,B_{k}\right)  $ shall mean a $k$-tuple $\left(  p_{1},p_{2}
,\ldots,p_{k}\right)  $ of paths such that each $p_{i}$ is a path from $A_{i}$ to $B_{i}$; no two of the paths $p_{1},p_{2},\ldots,p_{k}$ have a vertex in common. (""NILP"" stands for ""non-intersecting lattice paths"", but note that the paths must
neither cross nor touch.) See Fig. 2 for an example of a NILP (with $k = 3$). Fig. 2: a NILP with $k = 3$. If $\mathbf{u}$ and $\mathbf{v}$ are two $k$-vertices, then $N\left(
\mathbf{u},\mathbf{v}\right)  $ denotes the set of all NILPs from $\mathbf{u}$
to $\mathbf{v}$. A pair $\left(  \mathbf{u},\mathbf{v}\right)  $ of two $k$-vertices
$\mathbf{u}$ and $\mathbf{v}$ is said to be nonpermutable if and only
if every $\sigma\in\mathfrak{S}_{k}$ satisfying $\sigma\neq\operatorname*{id}$
satisfies $N\left(  \mathbf{u},\sigma\left(  \mathbf{v}\right)  \right)
=\varnothing$. Note that we are not requiring that $N\left(  \mathbf{u}
,\mathbf{v}\right)  \neq\varnothing$ here. Proposition 2. Let $\mathbf{u}=\left(  A_{1},A_{2},\ldots,A_{k}\right)  $
  and $\mathbf{v}=\left(  B_{1},B_{2},\ldots,B_{k}\right)  $ be two $k$-vertices
  such that
  \begin{align*}
\operatorname*{x}\left(  A_{1}\right)   &  \geq\operatorname*{x}\left(
A_{2}\right)  \geq\cdots\geq\operatorname*{x}\left(  A_{k}\right)  ;\\
\operatorname*{y}\left(  A_{1}\right)   &  \leq\operatorname*{y}\left(
A_{2}\right)  \leq\cdots\leq\operatorname*{y}\left(  A_{k}\right)  ;\\
\operatorname*{x}\left(  B_{1}\right)   &  \geq\operatorname*{x}\left(
B_{2}\right)  \geq\cdots\geq\operatorname*{x}\left(  B_{k}\right)  ;\\
\operatorname*{y}\left(  B_{1}\right)   &  \leq\operatorname*{y}\left(
B_{2}\right)  \leq\cdots\leq\operatorname*{y}\left(  B_{k}\right)  .
\end{align*}
  Then, the pair $\left(  \mathbf{u},\mathbf{v}\right)  $ is nonpermutable. Note that the situation of Proposition 2 is illustrated on Fig. 2. What is this for? Here is why Proposition 2 is useful. Let $R$ be a commutative ring. Assume
that for each arc $a$ of the lattice, some element $\omega_{a}\in R$ is given.
We call this element $\omega_{a}$ the weight of $a$. If $p$ is any path,
then we let $\omega\left(  p\right)  $ be the product of the weights
$\omega_{a}$ over all arcs $a$ of $p$. If $\mathbf{p}=\left(  p_{1}
,p_{2},\ldots,p_{k}\right)  $ is a NILP from a $k$-vertex $\mathbf{u}$ to a
$k$-vertex $\mathbf{v}$, then we define the weight $\omega\left(
\mathbf{p}\right)  $ of this NILP $\mathbf{p}$ to be the product $\prod
_{i=1}^{k}\omega\left(  p_{i}\right)  $. If $A$ and $B$ are two vertices, then $N\left(  A,B\right)  $ shall denote the
set of all paths from $A$ to $B$. Now, recall that the lattice (as a directed graph) is acyclic. Hence, the Lindström--Gessel--Viennot lemma yields: Theorem 3. Let $\mathbf{u}=\left(  A_{1},A_{2},\ldots,A_{k}\right)  $ and
  $\mathbf{v}=\left(  B_{1},B_{2},\ldots,B_{k}\right)  $ be two $k$-vertices.
  Then,
  \begin{align*}
 \det\left(  \left(  \sum_{p\in N\left(  A_{i},B_{j}\right)  }\omega\left(
p\right)  \right)  _{1\leq i\leq k,\ 1\leq j\leq k}\right) 
& =\sum_{\sigma\in\mathfrak{S}_{k}}\left(  -1\right)  ^{\sigma}\sum
_{\mathbf{p}\in N\left(  \mathbf{u},\sigma\left(  \mathbf{v}\right)  \right)
}\omega\left(  \mathbf{p}\right)  .
\end{align*}
  (Here, $\left(  -1\right)  ^{\sigma}$ denotes the sign of the permutation
  $\sigma$.) If a pair $\left(  \mathbf{u},\mathbf{v}\right)  $ of two $k$-vertices is
nonpermutable, then the sum on the right hand side of Theorem 3 can be
simplified, as only one of its terms (if any) is nonzero. This, and
Proposition 2, leads to the following fact: Corollary 4. Let $\mathbf{u}=\left(  A_{1},A_{2},\ldots,A_{k}\right)  $
  and $\mathbf{v}=\left(  B_{1},B_{2},\ldots,B_{k}\right)  $ be two $k$-vertices
  such that
  \begin{align*}
\operatorname*{x}\left(  A_{1}\right)   &  \geq\operatorname*{x}\left(
A_{2}\right)  \geq\cdots\geq\operatorname*{x}\left(  A_{k}\right)  ;\\
\operatorname*{y}\left(  A_{1}\right)   &  \leq\operatorname*{y}\left(
A_{2}\right)  \leq\cdots\leq\operatorname*{y}\left(  A_{k}\right)  ;\\
\operatorname*{x}\left(  B_{1}\right)   &  \geq\operatorname*{x}\left(
B_{2}\right)  \geq\cdots\geq\operatorname*{x}\left(  B_{k}\right)  ;\\
\operatorname*{y}\left(  B_{1}\right)   &  \leq\operatorname*{y}\left(
B_{2}\right)  \leq\cdots\leq\operatorname*{y}\left(  B_{k}\right)  .
\end{align*}
  Then,
  \begin{align*}
\det\left(  \left(  \sum_{p\in N\left(  A_{i},B_{j}\right)  }\omega\left(
p\right)  \right)  _{1\leq i\leq k,\ 1\leq j\leq k}\right)  =\sum
_{\mathbf{p}\in N\left(  \mathbf{u},\mathbf{v}\right)  }\omega\left(
\mathbf{p}\right)  .
\end{align*} When people apply the Lindström--Gessel--Viennot lemma to the lattice
and don't get an alternating sum like in Theorem 3, they are usually
applying Corollary 4. For example, this is what is being done in the
classical proof of the Jacobi--Trudi identities using Lindström--Gessel--Viennot .","['determinant', 'combinatorics', 'algebraic-combinatorics', 'directed-graphs']"
2870689,Surface with mean curvature proportional to height $z$,"This curve, described with the inverse hyperbolic secant $\text{arsech}(x) = \text{arcosh}(1/x)$ , $$y = \text{arsech}\frac x2 - \sqrt{4-x^2}$$ has its curvature equal to the linear coordinate $x$ : $$\frac{dy}{dx} = \frac{-2}{x\sqrt{4-x^2}} - \frac{-x}{\sqrt{4-x^2}}\quad = \frac{x^2-2}{x\sqrt{4-x^2}}$$ $$\frac{d^2y}{dx^2} = \frac{2x}{x\sqrt{4-x^2}} + \frac{-(x^2-2)}{x^2\sqrt{4-x^2}} + \frac{(x^2-2)x}{x\sqrt{4-x^2\;}^3}\quad = \frac{8}{x^2\sqrt{4-x^2\;}^3}$$ $$k = \frac{\frac{d^2y}{dx^2}}{\sqrt{1+\Big(\frac{dy}{dx}\Big)^2\;}^3} = \frac{\bigg(\frac{8x}{x^3\sqrt{4-x^2\;}^3}\bigg)}{\sqrt{\frac{x^2(4-x^2)+(x^4-4x^2+4)}{x^2(4-x^2)}\;}^3}\quad = x$$ Here's a graph . (This curve can be extended by reflecting it across the $x$ and $y$ axes, and the relation $k = x$ holds, depending on sign conventions.) Is there a surface in 3D with this property? Obviously, the ( generalized ) cylinder based on the above curve would work. What about a surface of revolution? After some calculations in cylindrical coordinates (with $r$ and $z$ functions of $t$ , independent of $\theta$ ), I find the mean curvature: $$2H = \frac{(r'^2+z'^2)z' + (r'z''-r''z')r}{r\sqrt{r'^2+z'^2\;}^3}\quad \overset{?}{=} z$$ If the parameter $t = r$ or $t = z$ , then this simplifies to $$\frac{(1+z'^2)z' + rz''}{r\sqrt{1+z'^2\;}^3} = z$$ ( $z$ is a function of $r$ ) or $$\frac{1+r'^2 - rr''}{r\sqrt{1+r'^2\;}^3} = z$$ ( $r$ is a function of $z$ ). Are these equations solvable? in terms of elementary functions? in terms of elliptic functions?","['curvature', 'surfaces', 'ordinary-differential-equations', 'differential-geometry']"
2870699,What uniquely characterizes the germ of a smooth function?,"Let $X$ be the set of all functions $f:\mathbb{R}\rightarrow\mathbb{R}$ which are infinitely differentiable at $0$.  Let us define an equivalence relation $\sim$ on $X$ by saying that $f\sim g$ if there exists a $\delta>0$ such that $f(x)=g(x)$ for all points in the interval $(-\delta,\delta)$. And let $Y$ be the set of equivalence classes of elements of $X$ under $\sim$. My question is, what characterizes a given equivalence class in $Y$?  The values  of $f(0)$ and $f^{(n)}(0)$ for all $n$ aren't enough, because for any given analytic function $f$ there exists an non-analytic infinitely differentiable function $g$ such that $f(0)=g(0)$ and $f^{(n)}=g^{(n)}$ for all $n$ but where it's not the case that $f\sim g$. So what is the minimum information needed to unambiguously specify an element of $Y$? EDIT: It turns out my concept is an existing mathematical concept, known as a germ .  So my question reduces to, what information uniquely characterizes the germ of a smooth function?","['analyticity', 'real-analysis', 'calculus', 'taylor-expansion', 'analytic-functions']"
2870721,Can someone please explain to me why cotangent graphs look the way they do?,"Can someone please explain to me why cotangent graphs look the way they do?  I want to know why they basically look like mirror reflected tangent graphs. I get that if $\tan\theta=y/x$, then $\cot\theta=x/y$. But why would this lead to a graph that looks like a tangent graph that was reflected over? Can you please try to keep the answers at the level of a high school pre-calc student?","['algebra-precalculus', 'trigonometry']"
2870723,When a holomorphic function between hyperbolic surfaces is a covering map.,"I'm studying Milnor's book ""Dynamics in one Complex Variable"", and he states this problem to the reader in the middle of the proof of Pick's Theorem: If $S$ and $S'$ are two hyperbolic Riemann surfaces (that is, they are both universally covered by the unitary disk $\mathbb{D}$) and let $f: S \longrightarrow S'$ be a holomorphic function between them. Let $\phi_1: \mathbb{D} \longrightarrow S$ and $\phi_2 : \mathbb{D} \longrightarrow S'$ be their universal covering maps. Making some choice of points, we can lift $f$ to a function $F: \mathbb{D} \longrightarrow \mathbb{D}$, such that the diagram below commutes: $\require{AMScd}$
\begin{CD}
\mathbb{D} @>{F}>> \mathbb{D}\\
@V{\phi_1}VV @VV{\phi_2}V\\
S @>{f}>> S'
\end{CD} The statement from Milnor's book is: f is a covering map if and only if F is a conformal automorphism. Supposing that f is a covering map, we can use the universal property of $\phi_2 $ to conclude that F must be a conformal automorphism. The other side of this is bugging me. I've tried doing some arguments using that $\phi_1$ is a local homeomorphism or that $f$ is open (since it is holomorphic), but i couldn't get it right. Hence, the question is how to prove this fact: If $F$ is a conformal automorphism then $f$ is a covering map. Accepting any suggestions and insights to prove this. Note: I don't know if this result is generalizable for greater dimensions or for the smooth case ($S$, $S'$ smooth manifolds and $f$, $F$ being differentiable). Some counterexamples in those directions would be nice too.","['riemann-surfaces', 'general-topology', 'differential-topology', 'covering-spaces']"
2870751,"If E* is separable, so is E. Is this proof correct?","This is my first time posting here. I already saw a post regarding this result, but I wanted to check if this specific proof is correct or not; as the proof presented in the book is different. Also, I'm not a native English speaker, so excuse me if there's any grammar mistakes. Proposition : Let $(E,||.||)$ be a normed space (over $\mathbb{R}$ or $\mathbb{C}$) such that $E^{*}$  is separable. Then $E$  is separable as well. Proof: Let $D = \{\phi_{n}\}_{n \in \mathbb{N}}$ be a dense in $E^{*}$. Define $x_{n} \in E$ such that $||x_{n}||=1$ and $||\phi_{n}|| < |\phi_{n}(x_{n})|+\frac{1}{n}$, which exists by definition of $||\phi_{n}||$. I claim that $\langle x_{1},...x_{k}...\rangle$ is a dense in $E$. If I could prove that, due to a previous result, this would imply that $E$ is separable. We'll define $S=\overline{\langle x_{1},...x_{k}...\rangle}$. We need to prove that $S=E$. Let's assume otherwise. Then there would exist $x \in E-S$, and given that $S$ is a subspace of $E$ we can assume $||x||=1$. By a corollary of the Hahn-Banach theorem (since $S$ is closed), we can find a continious functional $\phi \in E^{*}$ such that $\phi(S)=\{0\}$ and $\phi(x)=1$. Now, we know that there exists a sequence in $D$ such that $\phi_{n_{k}} \rightarrow \phi$, but if that's the case: $$||\phi_{n_{k}}||-\frac{1}{n_{k}}<|\phi_{n_{k}}(x_{n_{k}})|=|\phi_{n_{k}}(x_{n_{k}})-0|=|\phi_{n_{k}}(x_{n_{k}})-\phi(x_{n_{k}})|\leq ||\phi_{n_{k}}-\phi||,$$ which implies that $||\phi_{n_{k}}|| \rightarrow 0$ as $k \rightarrow \infty$. For $k$ large enough, $|\phi_{n_{k}}(y)| < \frac{1}{2}$ for all $y$ with $||y||=1$, in particular, $|\phi_{n_{k}}(x)| < \frac{1}{2}$. Then, $$||\phi-\phi_{n_{k}}|| \geq |\phi(x)-\phi_{n_{k}}(x)| = |1-\phi_{n_{k}}(x)| >\frac{1}{2}$$ But $||\phi-\phi_{n_{k}}|| \rightarrow 0$? We reached our contradiction and conclude that $S=E$. Thank you in advance.","['normed-spaces', 'functional-analysis', 'dual-spaces']"
2870805,why is the curl of a function (which looks like a vortex) zero?,"Can anyone explain why the curl of this function zero ? Clearly it's rotating then how is it irrotational ? Someone on SE said Not to mistake Curl for rotation, they aren't exactly similar. Is this true ? $\vec{V}(x,y,z)=[\frac{−y}{x^2+y^2},\frac{x}{x^2+y^2},0]$ Image Source: WolframAlpha Edit : As pointed by Rahul the topic of irrotational vortices have been already discussed over here , that somehow clears my intuitive understanding of curl: rotating the particles about their axes and not just revolving around center (which Travis has already pointed out in his answer) of the vortex but this still doesn't clear up my doubt about why the intuitive understanding doesn't match the calculations. If the curl is present at the center (z-axis), why doesn't it show up in the result (which is zero)?","['multivariable-calculus', 'curl', 'intuition']"
2870825,How do we calculate the trace over the matrix logarithm $\log((\sigma_2 \otimes I_{n/2})^T\cdot\Omega_{S^n})$?,"How do we explicitly compute the curvature form $\Omega$ of the Levi-Civita connection $\nabla^{L.C.}$ for the $n$-sphere $S^n$? Thus, how do we calculate the trace over the matrix logarithm $\log((\sigma_2 \otimes I_{n/2})^T\cdot\Omega)$ for $\sigma_2$ the second Pauli matrix, and $I_{n/2}$ the identity matrix of dimension $n/2$? Thanks in advance! Any help would be much appreciated.","['matrices', 'kronecker-product', 'linear-algebra', 'differential-geometry']"
2870837,Evaluating the integral $\int \frac1{(2+3\cos x)^2}\mathrm dx$,"Please someone give me an idea to evaluate
this:    $$\int \frac1{(2+3\cos x)^2}\mathrm dx$$
I don't even know how to start cause even multiplying an dividing by $\cos^2x$ does not work, so help me here.","['integration', 'indefinite-integrals', 'calculus']"
2871031,A problem with two interlinked right triangles is driving me crazy,"This is a problem that is driving me crazy: $F$ is the center of the circle. $A\hat{B}C$ is a right angle. $A\hat{B}F$ is a right angle. $D\hat{B}F$ is a right angle. $C\hat{E}F$ is a right angle. The length $\overline{BF}$ is known. The angle $\alpha=B\hat{A}C$ is equal to the angle $B\hat{F}D$ and it is known to be different from zero. Given $\alpha$ and $\overline{BF}$ is it possible to compute $\overline{EF}$? The only thing I can write, apart from a bunch of equations deriving from the Pythagorean theorem like $\overline{CF}^2=\overline{CE}^2+\overline{EF}^2$, is $\overline{DF}=\frac{\overline{BF}}{\cos{\alpha}}$ but I need $\overline{EF}\lt\overline{DF}$ and not $\overline{DF}$. I really can't see the solution... Assuming some more info: given $\alpha$ and $\overline{BF}$ and $\overline{AB}$ is it possible to compute $\overline{EF}$?","['triangles', 'trigonometry', 'geometry']"
2871055,Approximation from within by sets in a generating algebra,"Let $(\Omega, \mathcal{F}, \mu)$ be a finite measure space, and let $\mathcal{A}$ be an algebra generating $\mathcal{F}$. It is well-known that $\mu$ enjoys the following approximation property: For all $\epsilon>0$ and $F\in\mathcal{F}$, there exists $A \in \mathcal{A}$ such that $\mu(F \triangle A) < \epsilon$. My general question is When does the following, stronger approximation property hold? For all $\epsilon>0$ and $F \in \mathcal{F}$, there exists $A \in \mathcal{A}$, $A \subseteq F$, such that $P(F-A)< \epsilon$. More specifically, I'm mainly interested in products of finite spaces. That is, for each $n \in \mathbb{N}$, let $A_n$ be a finite set equipped with discrete topology. Let $\Omega = \prod_n A_n$. Let $\mathcal{F}$ be the Borel $\sigma$-algebra in the product topology, and let $\mathcal{A}$ be the algebra of cylinder sets of the form
$$B_1 \times B_2 \times..., \ B_n \subseteq A_n, \ \text{and} \ B_n = A_n \ \text{for all but finitely many $n$}.$$ In this case, one result that is pretty close to what I want is that $\mu$ can be approximated from within by compact sets. See this . But I don't think that compact sets are necessarily members of $\mathcal{A}$, so it doesn't quite answer my question. What got me thinking about this was this post . In the presence of finite additivity, approximation from within by a compact class is sufficient for countable additivity. The present question, in the special case of products of finite spaces, is a sort of converse: When is countable additivity sufficient for approximation by the compact class that generates $\mathcal{F}$?","['measure-theory', 'real-analysis']"
2871074,"Find CDF of $X+Y$ for independent $U[0,1]$ variables $X$ and $Y$","There are several ways to find the CDF of $Z=X+Y$ with $X$ and $Y$ independent RVs. I want to use the general answer included in this question CDF of two variable to find the CDF when $X$ and $Y$ are continuous and uniform distributed over $[0,1]$. In this case, I use the two densities to the be $1$. The result is $z^2/2$ when $0\le z\le1$ and this I know is correct. The result is $z-\frac{1}{2}$ when $1\le z \le2$ that is not correct, it should be: $1- \frac{1}{2}(2-z)^2$. Where is the mistake, in the cited solution or when I use it? Can you elaborate?","['probability-distributions', 'probability', 'random-variables']"
2871098,Mistake in do Carmo's book Riemannian geometry?,"I'm reading do Carmo's book Riemannian geometry and at the chapter 5 Jacobi fields, is this proposition. My question is where is $a$ in the proof? At the first equal sign we should have $\frac{1}{a}$ in front of $v$ and $w.$ More over in the end we obtain that $\frac{D\overline{J}}{dt}(0)=\frac{w}{a}$ which dose not agree with $\frac{DJ}{dt}(0)=w.$ I'm missing out on something? Thanks a lot.","['geometry', 'riemannian-geometry', 'differential-geometry']"
2871127,"$p >2$ is a prime, any facts about congruence relation between the class number of $Q(\sqrt p)$ and $Q(\sqrt{-p})$?","Let $p$ be an odd prime. This is a question about the class number of $Q(\sqrt p)$ and $Q(\sqrt{-p})$ ,which we denote by $h(p)$ and $h(-p)$ respectively. While doing my research on number theory I came cross an obstacle to smooth which I have to know some relation between $h(p)$ and $h(-p)$ . I only know some basic facts about algebraic number theory. With some efforts, I only find the following paper, which provides some useful results when $p \equiv 3 \pmod 4$ . Click here . That paper discussed the relation between $h(p)$ and $h(-p)$ in congruence sense (modular $p$ ), which is enough for me. However, I didn't find any similar result  when $p \equiv 1 \pmod 4$ . I am desperate to know any facts about relation of $h(p)$ and $h(-p)$ when $p \equiv 1 \pmod 4$ . Hope some experts could help me, thank you very much!!!","['algebraic-number-theory', 'number-theory', 'elementary-number-theory', 'class-field-theory', 'prime-numbers']"
2871208,find flux outward a sphere cutted with $y\le-4$,"$$D=\{x^2+y^2+z^2\le 25,y\le -4\}. \qquad F=\{z^2,y^2,x^2\}$$ In order to find the total flux going outward D I need to evaluate two fluxes(or maybe not ? ): the first one is the flux of the circular region surface $x^2+z^2\le 9$ (which is the intersection between the sphere and $y\le -4$) and the second one is the surface created by cutting the sphere I calculated the first flux by first parametrizing the surface and then by evaluating the double integral over if : $$\iint\limits_{S_1} F\,ds$$ So I have $r(\theta)=(3\cos\theta,-4,3\sin\theta)$ with $0\le \theta \le 2\pi$ and the normal vector is $n=(0,-1,0)$. and eventually the flux will be = $-144\pi$ The problems start when I try to calculate the flux of the second surface : So i have $r(\theta,\phi)=(p\sin\phi\sin\theta,p\cos\phi,p\sin\phi\cos\theta)$ with $0\le \theta \le 2\pi$,$\cos^{-1}{(-4/5)\le \phi \le \pi}$ The normal shoud be : $n = (-p^2\sin^2\phi\sin\theta,-p^2\sin\phi\cos\phi,-p^2\sin^2\phi\sin^2\theta)$ And when I multiply $F(r(\theta,\phi))n = -p^4\cos^2\theta(2\sin^4\phi\sin\theta+\sin\phi\cos\phi)$ Is that really What I should put in the integral? (in order to find tge flux of the second region) What I'd like to get is the total flux! What would you do to?","['integration', 'multivariable-calculus', 'surfaces']"
2871267,Does for two equivalent probability measures $Q \approx P$ boundedness in $L^1(Q)$ imply boundedness in $L^0(P)$?,"Assume we have a measurable space $(\Omega,\mathcal{F})$ with two probability measures $Q$ and $P$, which are equivalent, i.e. for all $A \in \mathcal{F}$ we have
$$Q(A)=0 \iff P(A)=0.$$ I will denote expectations with respect to $P$ as $E_P[\cdot]$ and expectations with respect to $Q$ as $E_Q[\cdot]$. Let $(f_n)_{n \in \mathbb{N}}$ be a sequence of non-negative random variables such that $\sup_{n \in \mathbb{N}}E_Q[f_n] < \infty$, i.e. $(f_n)_{n \in \mathbb{N}}$ is bounded in $L^1(Q)$. Assume that de Radon-Nikodyim derivative satisfies $$\frac{dQ}{dP} \in L^{\infty}(P).$$ I want to show that the sequence $(f_n)_{n \in \mathbb{N}}$ is also bounded in $L^0(P)$, i.e. that the following holds 
$$\sup_{n \in \mathbb{N}}P[f_n > K] \rightarrow 0, \text{ for } K \rightarrow \infty.$$ I cannot seem to get to this conclusion. Does anyone have a hint or a complete proof?
Thanks a lot in advance!","['measure-theory', 'real-analysis', 'probability-theory', 'probability', 'radon-nikodym']"
2871270,A conjecture about an intrinsic similarity of non-isosceles triangles,"Given any non-isosceles triangle $\triangle ABC$, and denoting $AB$ its longest side, the following construction determines the points $DFGE$. In this post is shown that the points $DFCGE$ always determine a circle. Let then consider the center $J$ of this circle: By means of this new point $J$, we can draw the three circles passing by $F,C$ and $J$, by $C,G$ and $J$, and by $D,E$ and $J$. The two intersections of the first two circles with the third one determine two additional points $H$ and $I$. A fourth circle passing by these two points $H$, $I$, and $C$ defines the two points $K$ and $L$ on the sides $AC$ and $BC$, respectively. My conjecture is that the triangle $\triangle LKC$ is similar to $\triangle ABC$. I tried to apply the techniques suggested in this post and in this other post to prove the claim, but with no success. Thanks for your suggestions!","['euclidean-geometry', 'geometric-construction', 'geometry', 'triangles', 'trigonometry']"
2871283,Let $A$ and $B$ be real matrix such that $A+iB$ is non singular show that there exist $t \in \mathbb{R}$ such that $A+tB$ is non singular,"Let $A$ and $B$ be real matrices, with $A+iB$ non-singular. I need to show that there exist a real number $t$ such that $ A+tB $ is non-singular. I don't have any idea how I can approach this question... could I please get a hint?",['linear-algebra']
2871301,algebraic geometry for specific fields means no axiom of choice?,"I'm beginning to study commutative algebra and algebraic geometry, and something confuses me. Many proofs, e.g. of the Nullstellensatz, require the axiom of choice (to get the right results about ideals, I suppose). Is there any way to avoid that axiom, if you start with an algebraically closed field that is countable? What about the complex field? I've been looking for something on this topic on the Internet, with no luck. Thanks for any help!","['axiom-of-choice', 'algebraic-geometry', 'commutative-algebra']"
2871323,Finite morphisms of schemes are closed.,"Let $f : X \rightarrow Y$ be a finite morphism of schemes. I have to show $f$ is closed. I have been able to prove that for any open affine $V= \mathrm{Spec}(B)$ in $Y$, $f : f^{-1}V \rightarrow V$ is a closed morphism. But I am having trouble to extend this globally. I am arguing as follows: say $C$ is some closed set in $X$. Then $C \cap f^{-1}V$ being closed in $f^{-1}V, f(C \cap f^{-1}V)$ is closed in $V$ for any open affine $V$. But how to conclude from this $f(C)$ is closed in $Y$ without any assumption of quasicompactness ?","['algebraic-geometry', 'schemes']"
2871330,Pointwise Convergence of Lipschitz Functions is Uniform,"Say we have $f_n:[a,b]\to \mathbb{R}$ , such that for all $n$ we have $|f_n(x)-f_n(y)|\leq L|x-y|$ and $f_n \to f$ pointwise. Is the convergence uniform? I started with an attempt to prove it by showing Cauchy Criterion for uniform convergence : for any $c\in[a,b]$
$$|f_n(x)-f_m(x)|=|f_n(x)-f_n(c)+f_n(c)-f_m(c)+f_m(c)-f_m(x)|\\\leq|f_n(x)-f_n(c)|+|f_n(c)-f_m(c)|+|f_m(c)-f_m(x)|\leq 2L|x-c|+|f_m(c)-f_n(c)|.$$ Now let $\epsilon>0$, there exists $n,m$ such that $|f_m(c)-f_n(c)|<\frac \epsilon 2$ from pointwise convergence.
Also, if we take $x\in(c-\frac \epsilon {4L},c+\frac \epsilon {4L})$, we get $|f_n(x)-f_m(x)|<2L\frac \epsilon {4L}+\frac \epsilon 2=\epsilon$.
So we have uniform convergence in $(c-\frac \epsilon {4L},c+\frac \epsilon {4L})$ for all $c\in[a,b]$. We can get finite cover of these covers, where we have uniform convergence, and take the maximum $N$ from all of those intervals to get uniform convergence in $[a,b]$. However, My friend presented me with a possible counter-example : $f_n(x)=nxe^{-nx}$ in [0,1], where the derivative is bounded, and the convergence is only pointwise to 0, and not uniform. I couldn't find where my proof fails, and I`d be glad if someone can point it out for me.","['lipschitz-functions', 'real-analysis', 'calculus', 'uniform-convergence', 'sequences-and-series']"
2871337,Show that $f$ is continuous on $\mathbb{R}^2$,"Consider function $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ . Suppose $\forall y \in \mathbb{R}$ , the function $f(\cdot,y)$ is continuous and there is exists a positive number $K$ such that $$|f(x,y') - f(x,y'')| \leq K |y' - y''|, \forall x,y',y'' \in \mathbb{R}$$ Show that $f$ is continuous on $\mathbb{R}^2$ . I have no idea to solve this problem, so I need help. Thanks all!","['continuity', 'multivariable-calculus', 'real-analysis']"
2871352,Conditions for cyclic quotient group,Let $G$ be an arbitrary finite group and $H$ a normal subgroup. What are some good conditions on $H$ that make the quotient $G/H$ cyclic? I want to avoid any further restriction on $G$.,"['quotient-group', 'group-theory', 'cyclic-groups', 'finite-groups']"
2871372,Find the values of $x$ and $y$ that satisfies $\sin(x+y)=\sin x+\sin y$.,"I know that in general the following equality does not hold: $\sin(x+y)=\sin x + \sin y$. However, I have been looking for specific values of $x$ and $y$ that satisfies the given equation. This is what I have done so far: $\sin(x+y)=\sin x\cos y + \sin y \cos x = \sin x + \sin y$. Then from this equation, I got $\sin x(1-\cos y)+\sin y(1-\cos x)=0$.
There are two possibilities: Case 1: $\sin x(1-\cos y)=0$ and $\sin y(1-\cos x)=0$. Solving equations for $x$ and $y$, we get that $x=y=2\pi k$ for some integer $k$. Case 2: $\sin x(1-\cos y)=n$ and $\sin y(1-\cos x)=-n$ where $n$ is a nonzero real number. Then we have $\sin x(1-\cos y)=\sin y(\cos x-1)$ Since $n\neq0$ then we can divide both sides of this equation by $\sin x \sin y)$ to obtain $\csc y - \cot y = \cot x - \csc x$. I'm stuck in here. Any suggestions and hints (not answers) will be welcomed...",['trigonometry']
2871381,Surface integral - cone below plane,"After several years I suddenly need to brush up on surface integrals. Looking through my old Calculus book I have been attempting to solve some problems, but the following problem has really made me hit a wall, even though it probably is quite easy to solve: Find $\int \int_S y dS$, where $S$ is part of the cone $z=\sqrt{2(x^2 + y^2)}$ that lies below the plane $z=1+y$. So far I have found that $dS = \sqrt{3}$, which then means I have to solve the integral: $$\sqrt{3}\int \int_S y dx dy$$. However, I am really stuck on how to proceed from here. I have tried looking at the intersection between the cone and the plane, and transforming the integral to polar coordinates, but can't seem to get anywhere. If someone can help me out a bit here, then I would greatly appreciate it!","['multivariable-calculus', 'surface-integrals']"
2871392,Continuity of an exponential series [duplicate],"This question already has an answer here : continuity of power series (1 answer) Closed 5 years ago . Let $(a_n)_{n \in \mathbb{N}}$ be an infinite positive sequence of integers.
Assume that $f(\lambda) = \sum\limits_{n=0}^{\infty} a_n \lambda^n$ is finite for any $\lambda \in [0, \lambda_0)$. Does this necessarily imply that $f(\lambda)$ is also continuous in $(0, \lambda_0)$, or might something strange happen?","['analysis', 'real-analysis', 'complex-analysis', 'continuity', 'functional-analysis']"
2871419,Which functions satisfy $f^n(x) = f(x)^n$ for some $n \ge 2$?,"Let $n$ be an integer greater than $1$. The notation $f^n$ is notoriously ambiguous: it means either the $n$-th iterate of $f$ or its $n$-th power. I was wondering when the two interpretations are in fact the same. In other words, if we write $f^n(x)$ for $f(f(\dotsb f(x) \dotsb))$ and $f(x)^n$ for $f(x) \cdot f(x) \dotsb f(x)$: For which functions $f \colon I \to \mathbb R$ is $f^n(x) = f(x)^n$ for all $x \in I$? So far I have been able to show that: The constant functions $f(x) = 0$ and $f(x) = 1$ satisfy the condition for all $n$ and $f(x) = -1$ satisfies the condition for all odd $n$. Also, if $f$ satisfies the condition for $n$, then the only fixed points of $f$ can be either $0$, $1$, or if $n$ is odd also $-1$. The squaring function $f(x) = x^2$ satisfies the condition for $n = 2$ and  is essentially the only non-constant differentiable function to do so. Indeed,
$$f(f(x)) = f(x)^2 \implies f'(f(x)) f'(x) = 2 f(x) f'(x)$$
so if we assume $f'(x) \neq 0$ and let $y = f(x)$, we have that
$$f'(y) = 2 y \implies f(y) = y^2 + c$$
and furthermore
$$(y^2 + c)^2 + c = (y^2 + c)^2 \implies c = 0.$$
(Of course we could then consider also, e.g., $f(x) = x^2$ for $x \ge 0$ and $f(x) = 0$ for $x < 0$.) More generally, $f(x) = x^\sqrt[n-1]{n}$ satisfies the condition for any $n > 2$. These functions are only defined on $\mathbb R^+$: this is why I chose an interval $I$ instead of all $\mathbb R$ as the domain, but I actually don't care if a solution is defined on any other non-trivial subset. Are there any other solutions? If not, how can we prove so? Remark : The question can be generalized to $n \in \mathbb Z$ if we assume $f$ to be invertible and denote by $f^{-n}$ either the $n$-th iterate of its compositional inverse or the $n$-th power of its multiplicative inverse (the cases $n = 0, 1$ are trivial). But this seems to be an even harder problem. For the case $n = -1$ see this question .","['functional-equations', 'functions', 'function-and-relation-composition']"
2871433,Show an isomorphism,"If $N,M$ are normal subgroups of $G$ then $\frac{NM}{M} \simeq \frac{N}{N\cap M}$
I have been trying to build a function from $N$ to $\frac{NM}{M}$ and looking at is kernel, but i'm.strugling at what $\frac{NM}{M}$ looks like","['group-theory', 'group-isomorphism']"
2871479,transpose appearing for the chain rule in the matrix form,"I am trying to understand deriving the derivative of a matrix equation of the form: $$a = \tanh(WX + b)$$
in which $W$ is a $M*N$ matrix, $X$ is $N*1$, and $b$ is $M*1$. I'm trying to take the derivative of $a$ with respect to $W$, $X$, and $b$. I already have the final answers as: $$\partial a/ \partial X= W^T(1 - \tanh(WX+b)^2)$$
I don't understand how $W$ moves to the left hand side of $(1 - \tanh(WX+b)^2$ and gets transposed!? I understand that the chain rule is: $$\partial f(u)/ \partial x= f'(u)\partial u/ \partial x$$ so in my example $\partial u/ \partial x$ is on the right hand side of the equation. $$\partial a/ \partial W=(1 - \tanh(WX+b)^2)X^T$$
in which I don't understand how $X$ gets transposed and moves to the left hand side.","['calculus', 'matrix-calculus', 'derivatives']"
2871509,Ad-invariant inner product on Lie algebra of compact Lie group.,"I was reading about representations of compact Lie groups and came upon a certain construction of an $Ad$-invariant inner product on the Lie algebra of a compact Lie group. I'm having trouble showing that it is actually Ad-invariant. Here's my thinking so far: Let $G$ be a compact Lie group with Lie algebra $\mathfrak{g}$, and let $|dg|$ be a left-invariant (hence bi-invariant) smooth density on $G$. The claim is that $$B(x,y) := \frac{1}{\text{vol}(G)} \int_G B' (\text{Ad}_g (x), \text{Ad}_g(y))  \ |dg|$$
is an Ad-invariant inner product on $\mathfrak{g}$ (where $B': \mathfrak{g}\times \mathfrak{g}\to \mathbb{R}$ is an arbitrary inner product). Fixing $x,y\in \mathfrak{g}$ and letting $ \ \  f:G\to \mathbb{R}$, $g\mapsto  B'(\text{Ad}_g(x),\text{Ad}_g(y))$, we compute: \begin{align*}
B(\text{Ad}_h (x), \text{Ad}_h(y)) &= \frac{1}{\text{vol}(G)} \int_G B'(\text{Ad}_g\text{Ad}_h (x), \text{Ad}_g\text{Ad}_h (y)) \ |dg| \\
&=  \frac{1}{\text{vol}(G)} \int_G B'(\text{Ad}_{gh} (x), \text{Ad}_{gh} (y)) \ |dg| \\
&= \frac{1}{\text{vol}(G)} \int_G f(gh) \ |dg| \\
\end{align*}
So, I need to show that $\int_G f(gh) \ |dg| = \int_G f(g) \ |dg|$ for all $h\in G$ $(\star)$. Intuitively this is clear, since $|dg|$ is bi-invariant, and right multiplication by $h$ inside the argument of $f$ just ""shuffles"" the values of $f$ around to different points on $G$. However I can't seem to find an algebraic justification for why this should be true. Here is a thought I had: Let $\chi:G\to \mathbb{R}^*$, $g\mapsto |\det(\text{Ad}_g)|$ be the so-called ""modular function"" of $G$. I know that any left-invariant smooth density $\mu$ satisfies $$(R_h)^* \mu= \frac{1}{\chi(h^{-1})}\  \mu$$ Since $G$ is compact we have $\chi\equiv 1$, hence $(R_h)^* \mu= \mu$. Thus, to prove $(\star)$, it suffices to show that the density $f |dg|$ is left-invariant. However, I wasn't able to show this (and I'm not entirely convinced whether it's even true). Any suggestions would be greatly appreciated!","['representation-theory', 'lie-groups', 'differential-geometry']"
2871531,Does there exist a $20\times20$ boolean matrix such that no two $3\times3$ submatrices are identical?,"I want to create  a 20*20 boolean  matrix that i will be converting to something like a square  bar code.
 I will be looking on this matrix through  a 3*3 window, and what i want is every time i randomly move this window it gives me a unique 3*3 matrix,  so i can determine  where the window is by comparing it to the big matrix.
Thanks",['combinatorics']
2871534,Expressions for integrals of non-elementary integrals,"Functions like $\frac1{\log x}$ have no elementary integrals in terms of standard functions; they are instead represented using special function notations, such as $\mathrm{li}(x)$. That's all and well, but how does one obtain an expression for the integral of these nonelementary integrals? I would expect that there would be "" no result found in terms of standard mathematical functions "", but WolframAlpha actually gives an expression for the logarithmic integral : $$\int \mathrm{li}(x)dx = x \, \mathrm{li}(x) - \mathrm{Ei}(2 \log x) + C.$$ (Or $x \, \mathrm{li}(x) - \mathrm{li}(x^2) + C$, for an appropriate choice of domain.) How does WolframAlpha obtain such a result? Is there a special technique that I am unaware of?","['integration', 'indefinite-integrals', 'calculus']"
2871535,Residue at infinity calculating integrals,"I have the following problem which I want to evaluate at infinity:
$$\oint \dfrac{(z+2)}{(z^2+9)}dz$$ I approach this problem by saying that $z=\dfrac{1}{t}$ and $dz=\dfrac{-1}{t^2}dt$. And I plug them inside my integral and obtain:
$$\oint \dfrac{-(2t+1)}{(t+9t^3)}dz=-2\pi i Res(0)=-2\pi i$$
Yet this result is not in accordance with the usual integration using residues which yield $2\pi i$. I was wondering where am I doing a mistake of minus.","['complex-analysis', 'residue-calculus']"
2871543,Induction differential proof,"So I was doing an induction proof that contained a differential. Now I got through most of it but lastly in order to complete the proof I needed to prove $$\frac{d^k  }{dx^k}x^k=k!$$
Now I don't need to do this via induction or anything it's just that inside the actual induction proof this statement was there and in order to prove the $(k+1)$-th term I need to show this. I can see how this would happen: $\frac{d  }{dx}x^k = k(x^{k-1})$ $\frac{d^2  }{dx^2}x^k  = k(k-1)x^{k-2}$
..
...
..
Until eventually I would get $k\cdot (k-1)\cdot (k-2)\cdot (k-3)\cdot ... \cdot x^{k-k}$ Which in turn would be $k!$ However is there any better way of showing this, like a more formal proof or better strucutred one ?","['induction', 'proof-writing', 'derivatives']"
2871551,Is normalizing automorphism in $\mathbb{R}^n$ by Jacobian still an automorphism?,"Let $n \in \mathbb{N}$.
Suppose $f: \mathbb{R}^n \to \mathbb{R}^n$ is a $C^\infty$ diffeomorphism. Let $J_f: \mathbb{R}^n \to \text{M} (n, \mathbb{R})$ be the Jacobian of $f$. Is the map $F: \mathbb{R}^n \to \mathbb{R}^n$ defined by $F(x) = [J_f(x)]^{-1}\cdot f(x)$ a $C^\infty$ diffeomorphism?","['calculus', 'differential-topology', 'real-analysis']"
2871553,Representation of elements of direct sum of groups,"I am confused about direct sum of groups, which is something that I thought I understood for a long time. By definition, the direct sum $\oplus_{\alpha}G_{\alpha}$ of groups $G_\alpha$ has elements which are tuples of the form $(g_{\alpha})$ where $g_\alpha \in G_ \alpha$ where all but finitely many $g_\alpha$ are zero. However, sometimes we think of $\oplus G_\alpha$ as finite sums of the form $\sum_{\alpha}g_a$ where $g_\alpha \in G_\alpha$. Why are these representations of elements giving the same group?","['group-theory', 'definition', 'direct-sum']"
2871629,How to show that $¥cos(¥sin^{-1}(x))$ is $¥sqrt{1-x^2}$? [duplicate],"This question already has answers here : $¥cos(¥arcsin(x)) = ¥sqrt{1 - x^2}$. How? (7 answers) Closed 5 years ago . How to show that $¥cos(¥sin^{-1}(x))$ is $¥sqrt{1-x^2}$? I remember having to draw a triangle, but I'm not sure anymore.",['trigonometry']
2871644,Can I bring the Kirilov 2-form on coadjoint orbits to adjoint orbits?,"Given a semisimple lie group, there is a symplectic structure on the coadjoint orbits arising from the Kirilov 2-form. Can I use this to define a volume form on the adjoint orbits, perhaps pulling it back via the homeomorphism between corresponding adjoint and coadjoint orbits? I know there is another way to get to volume on adjoint orbits via the Haar measure on the lie group and taking the quotient measure identifying the orbit with the quotient of G by the stabilizer, but I would like to do it ignoring this identification entirely.","['differential-topology', 'lie-algebras', 'lie-groups', 'differential-geometry']"
2871652,What is $f(2s+1)$ when $f(s)=\sum_{n=0}^\infty {\frac{(-1)^n}{(2n+1)^s}}=1-\frac{1}{3^s}+\frac{1}{5^s}-\frac{1}{7^s}+\dots$? [duplicate],"This question already has answers here : Infinite Series $\sum\limits_{n=0}^\infty\frac{(-1)^n}{(2n+1)^{2m+1}}$ (5 answers) Closed 5 years ago . Is there an exact form of 
$$f(s)=\sum_{n=0}^\infty {\frac{(-1)^n}{(2n+1)^s}}=1-\frac{1}{3^s}+\frac{1}{5^s}-\frac{1}{7^s}+\dots$$
when $s$ is odd? Discussion I have been exploring infinite series and will be spending my evening looking for patterns in this particular class.  I invite the interested reader to join me and the not so interested to just move along. I will be updating this question with relevant facts as the evening unfolds. There must (there must!) be some closed form in terms of $\pi$ and $s$ when $s$ is odd and there will definitely be something we can say about how this relates to the generalized zeta function. $f(1)=\frac{\pi}{4}$ $f(2)=$ Catalan . [I will leave a remark about this below.] $f(3)= \frac{1}{64} (ζ(3, 1/4) - ζ(3, 3/4))=\frac{\pi^3}{32}$ $f(4)= \frac{1}{256} (ζ(4, 1/4) - ζ(4, 3/4))$ $f(5)= \frac{1}{1024}(ζ(5, 1/4) - ζ(5, 3/4)) =\frac{5\pi^5}{1536} $ $f(6)= \frac{1}{4096}(ζ(6, 1/4) - ζ(6, 3/4)$ $f(7)=\frac{1}{16384}(ζ(7, 1/4) - ζ(7, 3/4))= \frac{61 π^7}{184320}$ I thought about posting in Meta asking about this type of question. It's a ""call to adventure"" question: Come look at this with me if you so please. If you're not into it... downvote the question/let me know in the comments/move on to some other question that you do enjoy. Update 1: It looks like $$f(s)= \frac{1}{2^{2s}} \Bigg(\zeta(s, \frac{1}{4})-\zeta(s, \frac{3}{4}) \Bigg)$$ Update 2: A remark on Catalan's number and on $s$ even in general. The wiki page claims it to be unknown whether this Catalan's constant is irrational or transcendental. Come on guys? What do we pay you for? Let me just state for the conjectural record that $\sum_{n=1}^\infty\frac{a_n}{n^s}$ for a periodic sequence of integers $a_n$ has just must be transcendental (it must!). I am very confident this is the case when $a_n$ has period of prime $p$ and for $s=1$. It's surprising to me that I would need these conditions. Note that for $f(2)$ the numerators of the series would be $1,0,-1,0 \dots$ and that's not a prime period and also $s \neq 1$ so we cannot use any of those tools to make any statements about Catalan's number but also... one cannot deny the conjecture isn't really too bold. Most numbers should be transcendental and this periodic numerators of these series must be a push in the transcendental direction.","['riemann-zeta', 'catalan-numbers', 'dirichlet-series', 'sequences-and-series']"
2871661,The orbits of semisimple groups in the projective space,"Let $G=R\cdot S$ be a connected  reductive algebraic group where $S$ is normal semisimple and $R$ is central. Suppose that $G$ acts algebraically on the protective variety $\mathbb P(V)$. I wanna understand the $S$-orbits in $\mathbb P(V)$. They are closed in $G$-orbits, but why they are flag varieties?","['algebraic-groups', 'algebraic-geometry', 'lie-groups']"
2871670,Reversing sign of third derivative,I don't understand some part of the solution given to this question: I can understand how the sign of $f''(x)$ can be reversed (i.e. flipping over the x-axis) but I don't get why changing $x$ to $-x$ would change the sign of $f'''(x)$?,"['calculus', 'derivatives']"
2871676,How to understand if $a-c \lt b \lt a+c$ then $b-c \lt a \lt b+c$,"I'm learning the concept of confidence interval. One of the important inference is that ""the probability that sample mean is within two standard deviation of population mean equals to 95%."" is equivalent to ""there is a 95% probability that population mean is within two standard  deviation of sample mean."" It seems the logic is if $a-c \lt b \lt a+c$ then to $b-c \lt a \lt b+c$ where a = population mean, b = sample mean and c = 2SD. I don't understand.","['algebra-precalculus', 'confidence-interval', 'statistics', 'inequality']"
2871711,What about linearity makes it so useful?,"Among all areas of mathematics, linear algebra is incredibly well understood. I have heard it said that the only problems we can really solve in math are linear problems- and that much of the rest of math involves reducing problems to linear algebra? But, what about the exchange of a ""multiplication"" operation and an ""addition"" operation is nice? Why is this interchange desirable, and why - among the many possible properties to specify, is linearity so important? Specifically, I am looking for: An idea of why the exchange that linearity allows is so powerful - whether appealing to some categorical or other argument about why these particular rules are so powerful An idea of why linear problems, or linearization, shows up so frequently","['soft-question', 'group-actions', 'abstract-algebra', 'linear-algebra']"
2871719,What can you do if the higher-order derivative test is inconclusive?,"The second-derivative test states that if $x$ is a real number such that $f'(x)=0$, then: If $f''(x)>0$, then $f$ has a local minimum at $x$. If $f''(x)<0$, then $f$ has a local maximum at $x$. If $f''(x)=0$, then the text is inconclusive. But there's no need to despair if the second-derivative test is inconclusive, because there is the higher-order derivative test.  It states that if $x$ is a real number such that $f'(x)=0$, and $n$ is the smallest natural number such that $f^{(n)}(x)\neq 0$, then: If $n$ is even and $f^{(n)}>0$, then $f$ has a local minimum at $x$. If $n$ is even and $f^{(n)}<0$, then $f$ has a local manimum at $x$. If $n$ is odd, then $f$ has an inflection point at $x$. But the higher-order derivative test can also be inconclusive, if $f^{(n)}(x)=0$ for all $n$.  My question, what can you do if the higher-order derivative test is inconclusive? Is the first-derivative test the only option at that point, or are there other options? EDIT: I’m interested in finding a method that depends only on the germ of $f$. EDIT 2: Let me explain more precisely what I’m saying regarding the germ.  Let $X$ be the set of all functions $f$ infinitely differentiable at $a$ where $f^{(n)}(a)=0$ for all $n$.  Two functions $f$ and $g$ in $X$ belong to the same germ if there is an open interval $I$ containing $a$ such that $f(x) = g(x)$ for all $x$ in $I$.  Now let $Y$ be the set of germs of $X$.  I want to know if there exists a nontrivial function $F:Y\rightarrow\mathbb{R}$ such that if $F$ evaluated at a particular germ yields a positive number, then all the functions in the germ have a local minimum at $a$, and if it yields a negative number then all the functions in the germ have a local maximum at $a$.","['real-analysis', 'maxima-minima', 'calculus', 'optimization', 'derivatives']"
2871773,Generic hyperplane and transversal intersection,"I'm trying to solve an exercise I.7.7 in Hartshorne's Algebraic Geometry: 7.7. Let $Y$ be a variety of dimension $r$ and degree $d>1$ in $\mathbb{P}^n$. Let $P \in Y$ be a nonsingular point. Define $X$ to be
  the closure of the union of all lines $\overline{PQ}$, where $Q \in Y,
> Q\neq P$. (a) Show that $X$ is a variety of dimension $r+1$. (b) Show that $deg X < d$. I've proved (a); it follows from the fact that there is a dominant rational map from the projective cone over $Y$ to $X$. However, to prove (b), I'm stuck. I found that it suffices to prove the following: Let us consider the set $(\mathbb{P}^n)^*$ of all hyperplanes in
  $\mathbb{P}^n$ as the projective $n$-space $\mathbb{P}^n$ with its
  Zariski topology. Let $W$ be the set of all hyperplanes containing $P$. Then W is a
  hyperplane in $(\mathbb{P}^n)^*$. Then, for any (closed) variety $Y \subset \mathbb{P}^n$ which contains
  $P$ and is nonsingular at $P$, the set $$\{ H\in W: \mbox{For every irreducible component $Z$ of } Y\cap H, i(Y,H;Z) = 1. \} $$ contains a nonempty open subset of $W$. Now $i(Y,H;Z)$ is the intersection multiplicity of $Y$ and $H$ along $Z$, i.e., the length of the $\mathcal{O}_Z$-module $\mathcal{O}_Z / (I(Y)+I(H))\mathcal{O}_Z$, where $\mathcal{O}_Z$ is the local ring of $\mathbb{P}^n$ at $Z$. How can I prove this? Thanks. Edited : I found that if the ideal $I(Y)+I(H)$ is a radical ideal, then $i(Y,H;Z) = 1$ for all $Z$.","['algebraic-geometry', 'intersection-theory']"
2871778,Determining all $f : \Bbb R \to\Bbb R$ satisfying $f\bigl(f(x)f(y)\bigr)+f(x+y)=f(xy)$ [duplicate],"This question already has an answer here : Finding all $ f : \mathbb R \to \mathbb R $ satisfying $ f \bigl ( f ( x ) f ( y ) \bigr ) + f ( x + y ) = f ( x y ) $ for all $ x , y \in \mathbb R $ (1 answer) Closed 2 years ago . Let $\Bbb R$ be the set of real numbers. Determine all functions $f : \Bbb R \to\Bbb R$ such that, for all real numbers $x$ and $y$ , $$f\bigl(f(x)f(y)\bigr)+f(x+y)=f(xy)\text.$$ My attempt: Let's first find some partial answers. If this equation has some polynomial answers of degree $d$ then the coefficient of $x^d$ must be equal at both sides i.e. we must have $$\max\left\{d^2,d\right\}=d$$ which leads to $$d=0,1$$ now let's take the general answer of polynomial kind as $$f(x)=ax+b$$ for some $a$ and $b$ . Substituting this in the equation leads to $$f\left(a^2xy+b^2+abx+aby\right)+ax+ay+b=axy+b$$ which leads to $$a^3xy+ab^2+a^2bx+a^2by+b+ax+ay=axy$$ finally we obtain one trivial answer $a=b=0$ and two non-trivial answers $$a=-b=1\\a=-b=-1$$ therefore the only non-trivial polynomials satisfying the equation above are $$f(x)=x-1\\f(x)=-x+1$$ I believe these are the only answers of the question. Here turns out two questions: Are there any other answers except those I found? If so, what are they?","['contest-math', 'functional-equations', 'functions']"
2871781,Sum of a Sequence of Odd Numbers that are Squared [duplicate],"This question already has answers here : Calculate sum of squares of first $n$ odd numbers [closed] (5 answers) Closed 5 years ago . What is the sum of all the numbers in the sequence $1^2 + 3^2 + 5^2 + 7^2 + 9^2 + \ldots + k^2$. Note that all the numbers being squared in the sequence are all odd numbers. This is what I have done so far (sorry if the images are an inconvenience, but this was the clearest way to display my working out): I am a little stuck on what to do next and how to obtain $\frac{n (4n^2 - 1)}{3}$ as the final result as this is what I am meant to end up with. It would be really appreciated if anyone could make suggestions towards completing and improving my method. Thanks! :)","['sums-of-squares', 'summation', 'telescopic-series', 'discrete-mathematics']"
2871798,Proving a Polynomial Limit where x → 0,"Prove $$\lim \limits_{x\to 0} x^3+x^2+x = 0$$
Note
$|f(x)-L| = |x^3+x^2+x|$ Assume
$\ \ |x-c|<\delta \implies |x| < \delta$ $\implies |x^3+x^2+x|<\delta\cdot|x^2+x+1|$ Assume $|x| < 1 \implies -1 < x < 1 \implies 0 < x+1 < 2$ And I am not sure where to go from there, since I can't multiply the inequality by $x$ in order to get $x^2$, because $x$ could be negative or positive.","['limits', 'epsilon-delta']"
2871832,Closed form for $\sin(n\arctan(x))$,"Is there a closed form for the function $\sin(n\arctan x)$, perhaps where $n$ is restricted to being an integer, or if not, perhaps some special integers (such as triangular numbers or some other figurate numbers)? From playing around with a few values, it seems that $$\sin\arctan(x)=\frac{x}{\sqrt{1+x^2}},~\sin(2\arctan x)=\frac{2x}{1+x^2},~\sin(3\arctan x)=\frac{3x-x^3}{(1+x^2)^{3/2}},$$ I can see that the denominator is $(1+x^2)^{\tfrac12n}$ but can't quite see the form of the numerator. Motivation: This is motivated by an inconvenient but necessary change of coordinates from polar to Cartesian when a function involves not $\sin\theta$ but $\sin n\theta$ for some integer $n.$",['trigonometry']
2871845,"For an isogeny of abelian varieties $f : X \to Y$, is $Y = X/ \operatorname{ker}f$?","Let $k$ be a field, $f : X \to Y$ be an isogeny of $k$-abelian varieties.
Then there exists the canonical separable isogeny $\pi : X \to X/\operatorname{ker}f$, such that $X/\operatorname{ker}f$ is a $k$-abelian variety, and on $\overline{k}$-valued points, $\pi$ is the natural quotient map of groups with the kernel $\operatorname{ker}f$.
By III.4.1 of Silverman's The Arithmetic of Elliptic Curves , there exists an isogeny $g : X/\operatorname{ker}f \to Y$ such that $f = g \circ \pi$. Now, is $g$ an isomorphism (of varieties)? The Corollary 1 of section 12 of Mumford's Abelian Varieties says this is true.
However, if so, I think that every isogeny becomes separable: $\pi$ is separable, and $g$ is an isomorphism, in particular separable, thus $f = g \circ \pi$ is also separable.","['algebraic-geometry', 'abelian-varieties']"
2871857,How to find the limit of the sequence $a_n=\frac{n^n}{3^n\cdot n!}$ as $n$ tends to infinity.,"Let $a_n=\dfrac{n^n}{3^n\cdot n!}.$ Show that $a_n\to0$ as $n\to\infty$. I know that I can use the ratio test for sequences, and $n^n$ increases faster than $3^n \cdot n!$ so it will tend towards infinity so I invert the sequence so I have to show that $\frac{3^n \cdot n!}{n^n}$ tends towards $0$. I divide $a_{n+1}$ by $a_n$, I get $$\frac{3^{n+1} \cdot (n+1)! \cdot n^n}{(n+1)^{n+1} \cdot 3^n \cdot n!}.$$ I can get simplify up to get $\frac{3n^n}{(n+1)^{n+1}}$ but I do not know how to simplify any further so I can find the limit as $n$ tends to infinity.","['limits', 'sequences-and-series']"
2871943,Help proving $\sum_{pq \leq x} \log p\log q \frac{\log x}{\log pq} = \sum_{pq \leq x} \log p\log q + O(x)$,"I'm reading Selberg, A. (1949). An Elementary Proof of the Prime-Number Theorem After deriving his formula: $$\sum_{p \leq x} \log^2 p + \sum_{pq \leq x} \log p \log q = 2x\log x + O(x)$$ The author states that ""By partial summation we get from ( formula above )"" to: $$\sum_{p \leq x} \log p + \sum_{pq \leq x} \frac{\log p \log q}{\log pq} = 2x + O(\frac{x}{\log x})$$ I don't understand how to derive this by partial sums but since I know that: $$\sum_{p \leq x} \log^2 p = \log x \sum_{p \leq x} \log p + O(x)$$ I figure that I need the equation in question $$\sum_{pq \leq x} \log p\log q \frac{\log x}{\log pq} = \sum_{pq \leq x} \log p\log q + O(x)$$ problem being that I am unable to prove this so please help. (Also if unclear note that $p$ and $q$ denote primes being not necessarily different)","['analytic-number-theory', 'number-theory', 'prime-numbers', 'sequences-and-series']"
2871947,Any Subgroup containing commutator subgroup is normal.,"I can prove that commutator is minimal subgroup such that factor group of it is abelian. I had encountered one statement as If $H$ is a subgroup containing commutator subgroup then $H$ is
  normal. I.e. we have to show that $\forall g\in G$ such that $gHg^{-1}=H$ with fact that $G'\subset H$ It is for elements in $G'$ to show condition for normality. But how to do for elements not in $G'$ but in $H$ , that is in $H\setminus G'$ ?","['derived-subgroup', 'normal-subgroups', 'group-theory', 'abstract-algebra']"
2871964,Showing the Composition of Two Polynomials is a Polynomial and the Composition of Two Rational Functions is a Rational Function,"This seems very obvious and I am having a bit of trouble producing a formal proof. sketch proof that the composition of two polynomials is a polynomial Let $$p(z_1)=a_nz^n_1+a_{n-1}z^{n-1}_1+...+a_1z_1+a_0 \\ q(z_2)=b_nz^n_2+b_{n-1}z^{n-1}_2+...+b_1z_2+b_0$$ be two complex polynomials of degree $n$ where $a_n,..,a_0\in\mathbb{C}$ and $b_n,..,b_o\in\mathbb{C}$. Now, 
\begin{align}
(p\circ q)(z_2)&=p(q(z_2)) \ \ \ \ \ \text{(by definition)}\\
&=a_n(q(z_2))^n+a_{n-1}(q(z_2))^{n-1}+...+a_1(q(z_2))+a_0
\end{align}
which is clearly a complex polynomial of degree $n^2$. sketch proof that the composition of two rational functions is a rational function A rational function is a quotient of polynomials. Let $$a(z_1)=\frac{p(z_1)}{q(z_1)}, \ b(z_2)=\frac{p(z_2)}{q(z_2)}$$
Now, 
\begin{align}
(a\circ b)(z_2)&=a(b(z_2)) \ \ \ \ \ \text{(by definition)} \\
&=\frac{p\left(\frac{p(z_2)}{q(z_2)}\right)}{q\left(\frac{p(z_2)}{q(z_2)}\right)} \\
&=\frac{a_n\left(\frac{p(z_2)}{q(z_2)}\right)^n+a_{n-1}\left(\frac{p(z_2)}{q(z_2)}\right)^{n-1}+...+a_1\left(\frac{p(z_2)}{q(z_2)}\right)+a_0}{b_n\left(\frac{p(z_2)}{q(z_2)}\right)^n+b_{n-1}\left(\frac{p(z_2)}{q(z_2)}\right)^{n-1}+...+b_1\left(\frac{p(z_2)}{q(z_2)}\right)+b_0} \\
\end{align}
Notice that $\left(\frac{p(z_2)}{q(z_2)}\right)^i \ \ \ \ (i=n, n-1,..,0)$ is a polynomial as
 $$(f\circ g)(z_2)=f(g(z_2))=\left(\frac{p(z_2)}{q(z_2)}\right)^i$$
where $$f(x)=x^i, \ \ g(z_2)=\left(\frac{p(z_2)}{q(z_2)}\right)$$ are both polynomials. 
Hence $(a\circ b)(z_2)$ is a rational function as it is the quotient of polynomials.","['complex-analysis', 'proof-writing', 'functions', 'proof-verification']"
2872036,How to solve equations involving multiple floors of the same variable? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I want to solve equations of this type: $$\lfloor1x\rfloor+\lfloor2x\rfloor+\lfloor3x\rfloor+\lfloor4x\rfloor+\lfloor5x\rfloor=10$$",['algebra-precalculus']
2872043,Size of $L^{\infty}$,"I am confused about the 'size' of $L^\infty$: One the one hand, when we consider $L^\infty(\Omega)$ and $\mu(\Omega)$ is finite (and $\mu$ some measure), then we have the inclusion $L^\infty \subset L^p$ for some $p \geq 1$. So $L^\infty$ is the 'smallest' of the $L^p$ spaces. On the other hand, we know for arbitrary open $\Omega \subset \mathbb{R^n}$ that $L^\infty$ is not separable, so it is in some sense 'too big' to be approximated by a countable subset, but $L^p$ for $1 \leq p < \infty$ are separable. I find this somewhat confusing. What am I missing or getting wrong? Thanks!","['lebesgue-integral', 'functional-analysis']"
2872054,The open disk and closed disk are a regular surfaces,"this is the exercice 2-2 of Do Carmo's page 67 1- is the set $\{(x,y,z)\in \mathbb{R}^3 ; \;z=0 \text{ and } \;x^2+y^2\leq 1 \}$ a regular surface ? 2-is the set $\{(x,y,z)\in \mathbb{R}^3 ; \;z=0 \text{ and }\;x^2+y^2<1 \}$ a regular surface ? my answer 1- the closed disk is not a regular surface,if it's the case the closed disk wich is compacte in $\mathbb{R}^3 $ is homeomorphic to an open set of $\mathbb{R}^2$ and this is contradiction 2- the open disk is a regular surface because the  paramtrisation $X$from the open uniatry disk from $\mathbb{R}^2$ to the open disk in $\mathbb{R}^3 $ verify the definition of a regular sufraces $(x(u,v)=(u,v,0))$ For the second i 'am sur but for my first answer i have some doubt what do you think ?",['differential-geometry']
2872066,Euler's summation formula proof,"The following proof is from Apostol's book: Questions: On the first line of the proof, he uses '{}' just as brackets or do they have other meaning like $[x]$ being the floor function? right before equation (6), why does the summation from $m+1$ up to $k$ become $kf(k)-mf(m)$? at equation (6) when he substitutes back $x,y$ i'm not sure why are the two integrals equal?","['analytic-number-theory', 'number-theory', 'elementary-number-theory']"
2872089,Proving inequality using double integral,"If it's a known inequality or a duplicate - sorry. I’ve searched the question archive and elsewhere. I didn't find anything similar. Let $f$ be a positive continuous function over the interval $[a,b]$ . Prove the inequality $$
\int_{a}^{b}f(x)dx \cdot \int_{a}^{b} \frac{1}{f(x)}dx \geq(b-a)^2
$$ using double integrals. I’ve tried defining $g(x,y)=\frac{f(y)}{f(x)}$ ,  and then integrating over the square $[a,b]\times[a,b]$ , so we get $\int_{a}^{b}\int_{a}^{b}\frac{f(y)}{f(x)}dxdy=\int_{a}^{b}f(y)dy \cdot \int_{a}^{b} \frac{1}{f(x)}dx$ , and the double integral is then equal to the volume under $g(x,y)$ and over the above mentioned square. I didn't manage to prove that the volume is at least $(b-a)^2$ .","['integration', 'multivariable-calculus', 'calculus', 'definite-integrals']"
2872099,Convergence of uniformly continous functions to a uniformly continous function,"Say $f_n,f:\mathbb{R} \to \mathbb{R}$ are uniformly continous functions a $f_n\to f$ pointwise. Is the convergence nessecarily uniform? I couldn't find an appropriate counter-example for this case so I tried proving it : We have $|f_n(x)-f_n(y)|<\epsilon/3$ for all $|x-y|<\delta_1$, also $|f(x)-f(y)|<\epsilon/3$ for $|x-y|<\delta_2$ and for all $n>N_x$ we have $|f_n(x)-f(x)|<\epsilon /3$. Overall for any $|x-y|<\delta=min(\delta_1,\delta_2)$ and any $n>N_x$ we have $|f_n(y)-f(y)|\leq |f_n(y)-f_n(x)|+|f_n(x)-f(x)|+|f(x)-f(y)|<\epsilon $ for any $y\in (x-\delta,x+\delta)$ and for any $x\in A\subset\mathbb{R}$. Now if $A$ is compact, we have a cover of $A$ and we can choose a finite subcover $U$, and take $N=max\{N_x|x\in U\}. $ and for all $n>N$ we have $|f_n(y)-f(y)|<\epsilon$ uniformly . However, if my proof is correct I have only proven uniform convergence in compact sets, and not on the entire number line. Can my proof be modified so we can get uniform convergence on $\mathbb{R}$? Or can someone provide a counter example where it fails in this case? Edit : Thanks to the counter-examples in the answers I can see my proof is false. Can anyone point out where my proof fails?","['uniform-continuity', 'proof-verification', 'real-analysis', 'uniform-convergence', 'sequences-and-series']"
2872156,Understanding the definition of norm of tensors on a Riemannian manifold,"I am teaching myself Riemannian Geometry in order to studying Mean Curvature flow. I was reading Lecture Notes on Mean Curvature Flow by Carlo Mantegazza and I'm trying understand the following definition: The metric $g$ of $M$ extended to tensors is given by $$g(T,S) = g_{i_1s_1} \cdots g_{i_ks_k}g^{j_1z_1} \cdots g^{j_lz_l} T^{i_1 \cdots i_k}_{j_1 \cdots j_l} S^{s_1 \cdots s_k}_{z_1 \cdots z_l},$$ where $g_{ij}$ is the matrix of coefficients of $g$ in local coordinates and $g^{ij}$ is its inverse. Clearly, the norm of a tensor is $$|T| = \sqrt{g(T,T)}.$$ My doubt is why make sense define $g(T,S)$ as defined? I would like to know too if my thoughts below are lead me to the definition of $g(T,S)$ and how can I conclude my thoughts. $\textbf{My attempt in order to understand the definition:}$ Firstly, I know that the squared norm of the second fundamental form is $$|A|^2 = g^{mn}g^{st}h_{ms}h_{nt}$$ by this lecture notes and I know that the second fundamental form $A$ is a $(0,2)$ - tensor. This lead me to think that I would be able to understand the definition given by Mantegazza if I understand how define $g(T,S)$ when $T$ and $S$ are $(0,2)$ - tensors, because if $T$ and $S$ are $(k,l)$ - tensors, then I can see them as $(0,2)-$ tensors just fixing the $k$ coordinates and the $l - 2$ coordinates. I know that there is an isomorphism between the space of endomorphisms of a finite-dimensional vector space $V$ and the space of $(1,1)-$ tensors defined on $V$ , then I thought to raise an index of the tensor $A = (h_{ij})$ in order to obtain a $(1,1)-$ tensor $(g^{ik}h_{kj})$ and I thought define the squared norm of $A$ using the operator norm of the endomorphism associated to $(g^{ik}h_{kj})$ by the isomorphism quoted previously. I'm stuck here in understand how use the operator norm in order to define the squared norm of $A$ . Is it the way to understand the definition of $g(T,S)$ ? If so how can I proceed in order to conclude that $|A|^2 = g^{mn}g^{st}h_{ms}h_{nt}$ ? Thanks in advance!","['riemannian-geometry', 'tensors', 'normed-spaces', 'definition', 'differential-geometry']"
2872178,Is the diagonal of $X \times X$ closed in $\bar{X} \times \bar{X}$?,"Let $X$ be a topological space. Let $\bar{X}$ denote a compactification of $X$ (i.e., $\bar{X}$ is a compact Hausdorff space such that $X$ is an open dense subspace of $\bar{X}$). Notice that in particular this implies that $X$ is Hausdorff.
Define the diagonal $\triangle_X$ of $X$ by
$$\triangle_X=\{(x,x) \in X \times X \mid x \in X\}.$$ It is a well known fact that if $X$ is a topological space, then $X$ is Hausdorff if and only if its diagonal is closed. However, I am interested in the following: Question: Is $\triangle_X$ closed in $\bar{X} \times \bar{X}?$ Any help would be greatly appreciated.","['general-topology', 'compactification']"
2872218,Rolle's theorem: what's the right statement of the theorem?,"In the fourth edition of ""Introduction to Real Analysis"" by Bartle and Sherbert, theorem 6.2.3 (Rolle's theorem) states, Suppose that f is continuous on a closed interval $I := [a, b]$, that
  the derivative of $f$ exists at every point of the open interval $(a, b)$, and that $f(a) = f(b) = 0$.
  Then there exists at least one point $c$ in $(a, b)$ such that the derivative of $f$ is zero at $c$. Now, why are we taking $f(a)=0=f(b)$? Is $f(a)=f(b)$ not sufficient?","['definition', 'rolles-theorem', 'real-analysis']"
2872220,example of infinite solvable groups,"Let $G$ be a solvable group. It is a well-known Theorem that ""the index of every maximal subgroup of $G$ is a power of prime or infinite"".
I want to find an example of infinite case.",['group-theory']
2872227,Algebra - Solving equations with 3 variables.,"a,b,c $\in Z^+$ satisfying the equations: $$ 5a + 5b + 2ab = 92$$ $$ 5b+5c+2bc=136$$ $$5c+5a+2ac=244$$ Find the value of 7a + 8b + 9c. I took cases like a,b are odd numbers, even numbers and using the properties of their last digits i found the answer. But i want to know if there is some other method to solve this question.","['functions', 'linear-algebra']"
2872294,Solve the IVP $\ddot{x}+\dot{x}+x=f(t)$,"The entire question reads: Consider the mass spring system subject to an external force $f(t)$: $$\ddot{x}+\dot{x}+x=f(t)$$ Assume $x(0) = 0$ and $\dot{x}(0) = 0$. Assume also that $f(t)$ is the force describing the striking effect on the mass of the mass spring system in a short time period $0<T<\frac{1}{2}$, given as $f(t) = \frac{\pi}{4T}\sin{\frac{\pi t}{2T}}$ when $0 \leq t<2T$ and $f(t) = 0$ when $t \geq 2T$. (a) Solve the given IVP first for all $t \geq 0$. (b) Compute the limits $\lim_{t \to \infty} x(t)$ and $\lim_{t \to \infty} \dot{x}(t)$. (c) Compute the limits $\lim_{T \to 0}x(T)$ and $\lim_{T \to 0} \dot{x}(T)$, and discuss their physical meanings. I've been running into trouble on part (a). If I understand correctly, the solution for when $f(t) = 0$ is just simply $x(t) = 0$, but I haven't been able to come up with a solution for when $f(t) = f(t) = \frac{\pi}{4T}\sin{\frac{\pi t}{2T}}$. If someone could walk me through the process of finding these solutions, I know I could do parts (b) and (c). Thanks!","['mathematical-modeling', 'ordinary-differential-equations', 'dynamical-systems']"
2872296,Iwasawa Decomposition Uniqueness Proof,"Iwasawa Decomposition (special case): Let $G=SL_n(\Bbb{R})$, $K=$ real unitary matrices, $U=$ upper triangular matrices with $1$'s on the diagonal (called unipotent ), and $A=$ diagonal matrices with positive elements ($0$ everywhere else). Then, the product map $U\times{A}\times{K}\rightarrow{G}$ given by $(u,a,k)\mapsto{uak}$ is a bijection. I have already proved the surjection of this map myself, however, when viewing Lang's proof (Undergraduate Algebra Section 6 Chapter 4 pg246) for the uniqueness of this mapping I became rather stuck and would like to understand it. Lang states the following: ""For uniqueness of the decomposition, if $g=uak=u'a'k'$, let $u_1=u^{-1}u'$, so using $g^tg$ (what does he mean by 'using'?) you get $a^{2t}u_1^{-1}=u_1a'^2$. These matrices are lower and upper triangular respectively, with diagonals $a^2,a'^2$, so $a=a'$, and finally $u_1=I$, proving uniqueness"". What does Lang mean by 'using' $g^tg$ (I presume he means applying $g^tg$ to both sides, but he is unclear...), and so therefore how does he 'get' $a^{2t}u_1^{-1}=u_1a'^2$. Thank you","['matrices', 'abstract-algebra', 'linear-algebra', 'matrix-decomposition']"
2872298,"Evaluate $\iint_D x \sin (y -x^2) \,dA$.","Let $D$ be the region, in the first quadrant of the $x,y$ plane, bounded by the curves $y = x^2$, $y = x^2+1$, $x+y=1$ and $x+y=2$. Using an appropriate change of variables, compute the integral $$\iint_D x \sin (y -x^2) \,dA.$$ I've been reviewing for an upcoming test and this problem was recommended to do for study -- I just can't get it. I've tried many changes of variables and nothing has worked. I would really appreciate a hint or a solution. Thanks in advance!","['integration', 'multivariable-calculus', 'calculus', 'change-of-variable']"
2872317,Peirce decomposition of a ring: must the ideal generators be idempotent in characteristic 2?,"I'm considering this particular statement of the Peirce decomposition of a ring: If a commutative unital ring $R$ can be written as an internal direct sum of two of it's proper ideals $I$ and $J$, so $R = I \oplus J$, then there exists an idempotent element $e \in R$ such that $I = eR$ and $J = (1-e)R$. Since $R$ is an internal direct sum of $I$ and $J$, there is some $e \in I$ and $f \in J$ such that $e+f=1$ (then $f = 1-e$). How do you prove that $e$ is idempotent though? I've written out that
\begin{align*}
e+f &= 1
\\(e+f)^2 &= 1^2
\\e^2+f^2 &= 1 \qquad \text{since $ef=0$ because $IJ = 0$}
\\e^2 + (1-e)^2 &= 1
\\e^2 + 1 -2e + e^2 &= 1
\\2e^2 -2e &= 0
\end{align*} And this implies that $e$ is idempotent if the characteristic of the ring is not $2$ ! Is there a way to prove $e$ is idempotent that avoids this tiny snag?","['ring-theory', 'abstract-algebra', 'idempotents', 'positive-characteristic']"
2872334,Conditional expectation from joint distribution,"I am new to probability and trying to convince myself of the correctness of the equations in this paper on factor analysis. There is a step I am missing. I'll give my understanding so far and then highlight the question below. Given a $p$-dimensional vector $\textbf{x}$ modeled using a $k$-dimensional factor $\textbf{z}$ where typically $k < p$, the model for factor analysis is: $$
\textbf{x} = \Lambda \textbf{z} + \textbf{u}
$$ Where $\Lambda$ is a matrix, $\textbf{u} \sim \mathcal{N}(0, \Psi)$, and $\textbf{z} \sim \mathcal{N}(0, I)$. This means $\textbf{x} \sim \mathcal{N}(0, \Lambda \Lambda^{\top} + \Psi)$ because: $$
\begin{align}
\textbf{x}
&= \Lambda \textbf{z} + \textbf{u}
\\
&= \Lambda \mathcal{N}(0, I_k) + \mathcal{N}(0, \Psi)
\\
&= \mathcal{N}(0, \Lambda \Lambda^{\top} + \Psi)
\end{align}
$$ Now, we have the joint distribution: $$
P\bigg(
\begin{bmatrix}
\textbf{x} \\ \textbf{z}
\end{bmatrix}
\bigg)
=
\mathcal{N}\bigg(
\begin{bmatrix} 0 \\ 0 \end{bmatrix}
,
\begin{bmatrix}
\Lambda \Lambda^{\top} + \Psi & \Lambda
\\
\Lambda^{\top} & I \end{bmatrix}
\bigg)
$$ I can convince myself that this is correct and fairly easily. $\text{Var}(\textbf{x})$ and $\text{Var}(\textbf{z})$ come from their definitions, while $\text{Cov}(\textbf{x}, \textbf{z})$ and $\text{Cov}(\textbf{z}, \textbf{x})$ are easy enough to compute, e.g.: $$
\begin{align}
\text{Cov}(\textbf{x}, \textbf{z})
&= \mathbb{E}[(\textbf{x} - \mathbb{E}[\textbf{x}])(\textbf{z} - \mathbb{E}[\textbf{z}])^{\top}]
\\
&= \mathbb{E}[(\textbf{x} - 0)(\textbf{z} - 0)^{\top}]
\\
&= \mathbb{E}[(\Lambda \textbf{z} + \textbf{u})(\textbf{z})^{\top}]
\\
&= \mathbb{E}[\Lambda \textbf{z} \textbf{z}^{\top} + \textbf{u} \textbf{z}^{\top}]
\\
&= \Lambda \mathbb{E}[\textbf{z} \textbf{z}^{\top}] + \mathbb{E}[\textbf{u} \textbf{z}^{\top}]
\\
&= \Lambda
\end{align}
$$ Where $\mathbb{E}[\textbf{u} \textbf{z}^{\top}] = \mathbb{E}[\textbf{u}]\mathbb{E}[\textbf{z}^{\top}] = 0 \cdot 0$ and $\mathbb{E}[\textbf{z}\textbf{z}^{\top}] = I_k$ because: $$
\begin{align}
\text{Var}(\textbf{z})
&= \mathbb{E}[\textbf{z}\textbf{z}^{\top}] + \mathbb{E}[\textbf{z}] \mathbb{E}[\textbf{z}]^{\top}
\\
I_k &= \mathbb{E}[\textbf{z}\textbf{z}^{\top}] + 0
\end{align}
$$ So far so good. Question The authors then claim that the conditional expectation of the first and second moments of the factors are: $$
\begin{align}
\mathbb{E}[\textbf{z} \mid \textbf{x}] &= \Lambda^{\top} (\Psi + \Lambda \Lambda^{\top})^{-1} \textbf{x}
\\
\\
\mathbb{E}[\textbf{z} \textbf{z}^{\top} \mid \textbf{x}] &= I_k - (\Lambda^{\top} \Psi + \Lambda \Lambda^{\top})^{-1} \Lambda + \Lambda^{\top} (\Psi + \Lambda \Lambda^{\top})^{-1} \Lambda^{\top} \textbf{x} \textbf{x}^{\top} ((\Psi + \Lambda \Lambda^{\top})^{-1})^{\top} 
\end{align}
$$ The authors claim that this comes from ""the joint normality of data and factors"". How was this computed? I've gone through the Wikipedia page on conditional expectation, but I don't see anything that defines it in terms of the joint distribution or conditional distribution.","['conditional-expectation', 'probability']"
2872344,Understanding and proving prop 2.1.16 in Tao's Analysis I concerning recursive definitions,"Notes: Tao's axioms and original proposition statement and proof are given below. Similar questions have been asked here and here , but I was not able to resolve my issues. In particular, I have realized, it seems, there is a great deal of set theory going on ""underneath the hood"", and I find it rather conceptually and logically problematic that Tao introduces this proposition in a chapter before any set theory is even discussed. He remarks in a footnote how the proposition can be formulated more rigorously in the language of set theory, but he leaves that as the last exercise in a section in the chapter on set theory. This seems pretty sloppy to me, but I'm doing the best I can to follow what he's trying to communicate and the how/why of his proof. I found this blog post from Keith Devlin to be interesting but a little over my head. Is there any real good way to understand Tao's proposition without a thorough grounding in axiomatic set theory? EDIT: I have reformulated my question as largely a request for people to comment on or critique my attempt at ""a more fleshed out inductive proof."" Since the proof is by induction, I'll try to outline my thoughts according to the typical way an induction proof works. Any comments as to where I am going wrong or what needs to be fixed would be greatly appreciated. Proof attempt: Base case: The procedure gives a single value to $a_0$ , namely $c$ . My understanding is that we basically assume a natural number $c$ exists due to Assumption 2.6, and we simply make the definition $a_0\colon=c$ . None of the other definitions $a_{n++}\colon=f_n(a_n)$ will redefine the value of $a_0$ because the way in which a natural number $a_n$ is assigned to another natural number $n$ is by the assignment $a_{n++}\colon=f_n(a_n)$ which involves the successor, and Axiom 2.3 tells us that $n++\neq0$ for every natural number $n$ . Hence, it's impossible to define (again) $a_0$ once the procedure has begun. Inductive step: Suppose inductively that the procedure gives a single value to $a_n$ . We want to show that a single value is then given to $a_{n++}$ , namely $a_{n++}\colon=f_n(a_n)$ . We should first note that $a_n$ being single-valued is critical because $a_{n++}$ is going to be defined to be $a_{n++}\colon=f_n(a_n)$ , and it would not be sensible to start thinking about $a_{n++}$ being single-valued if $a_{n++}$ were defined in terms of a function whose argument was multi-valued (i.e., if $a_n$ were multi-valued, then we could reasonably expect $f_n(a_n)$ to return multiple distinct values as well). It may seem as though we can now safely rely on $a_{n++}$ being single-valued, but there is a concern we must address, namely the possibility that for a later natural number $m$ the corresponding definition $a_{m++}\colon=f_m(a_m)$ could somehow cause $a_{n++}$ to be multi-valued. How? If we came across an $m$ for which $n++=m++$ but $f_n(a_n)\neq f_m(a_m)$ , then we would have an issue  because $n++=m++$ implies that $a_{n++}=a_{m++}$ . Why would this be a problem? Well, with $a_{n++}\colon= f_n(a_n)$ and $a_{m++}\colon= f_m(a_m)$ , we would have $a_{n++}=f_n(a_n),f_m(a_m)$ even though $f_n(a_n)\neq f_m(a_m)$ , indicating that $a_{++}$ would not have a single value assigned to it, as desired. How do we resolve this potential issue? Suppose that later in the process we came across an $m$ corresponding to the definition $a_{m++}\colon= f_m(a_m)$ , where we also had $n++=m++$ . Since $n++=m++$ , it must be the case that $n=m$ by Axiom 2.4. Hence, we must also have $f_n(a_n)=f_m(a_m)$ , thus showing that none of the other definitions $a_{m++}\colon= f_m(a_m)$ will redefine the value of $a_{n++}$ , and hence only a single value is assigned to $a_{n++}$ . This completes the induction, and so $a_n$ is defined for each natural number $n$ , with a single value assigned to each $a_n$ . Content by Tao: Axiom 2.1. $0$ is a natural number. Axiom 2.2. If $n$ is a natural number, then $n{++}$ is also a natural number. Axiom 2.3. $0$ is not the successor of any natural number ; i.e., we have $n{++} \ne 0$ for every natural number $n$ . Axiom 2.4. Different natural numbers must have different successors ; i.e., if $n, m$ are natural numbers and $n \ne m$ , then $n{++} \ne m{++}$ . Equivalently, if $n{++} = m{++}$ , then we must have $n=m$ . Axiom 2.5. (Principle of mathematical induction). Let $P(n)$ be any property pertaining to a natural number $n$ . Suppose that $P(0)$ is true, and suppose that whenever $P(n)$ is true, $P(n{++})$ is also true. Then $P(n)$ is true for every natural number $n$ . Assumption 2.6. (Informal) There exists a number system $\mathbb{N}$ , whose elements we will call natural numbers, for which Axioms 2.1-2.5 are true. Proposition 2.1.16 (Recursive definitions). Suppose for each natural number $n$ , we have some function $f_{n} : \mathbb{N} \rightarrow \mathbb{N}$ from the natural numbers to the natural numbers. Let $c$ be a natural number. Then we can assign a unique natural number $a_{n}$ to each natural number $n$ , such that $a_{0} = c$ and $a_{n{++}} = f_{n} (a_{n})$ for each natural number $n$ . Proof . (Informal) We use induction. We first observe that this procedure gives a single value to $a_0$ , namely $c$ . (None of the other definitions $a_{n{++}}:= f_{n} (a_n)$ will redefine the value of $a_0$ , because of Axiom 2.3.) Now suppose inductively that the procedure gives a single value to $a_n$ . Then it gives a single value to $a_{n{++}}$ , namely $a_{n{++}}:= f_{n} (a_n)$ . (None of the other definitions $a_{m{++}}:= f_{m} (a_{m})$ will redefine the value of $a_{n{++}}$ , because of Axiom 2.4.) This completes the induction, and so $a_n$ is defined for each natural number $n$ , with a single value assigned to each $a_n$ .","['logic', 'real-analysis', 'peano-axioms', 'elementary-set-theory', 'induction']"
2872392,Which function on $\mathbf{X}$ has gradient $\mathbf{A} \mathbf{A}^\top \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1} $?,"I learned from The Matrix Cookbook that the gradient of the $\log \det$ function is given by \begin{equation}
\nabla \log \text{det}(\mathbf{X}^\top \mathbf{X})=2\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1},
\end{equation} where $\mathbf{X}\in\mathbb{R}^{n\times r}$. I wonder which function will give the gradient \begin{equation}
2\mathbf{A} \mathbf{A}^\top \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1},
\end{equation} for some matrix $\mathbf{A}\in \mathbb{R}^{n\times r}$.","['matrices', 'derivatives', 'matrix-calculus', 'linear-algebra']"
2872409,Show that $\lim_{m \to \infty} \frac{2^{2m}m!(m+1)!}{(2m+1)!(2m+1)}=0$,"After a long way to solve the differential equation $y''+xy'+3y=0$, I arrived at the solution $$y(x) = A_0 \sum_{m=0}^{\infty} (-1)^m \dfrac{2m+1}{2^mm!}x^{2m}+A_1 \sum_{m=0}^{\infty} (-1)^m \dfrac{2^m(m+1)!}{(2m+1)!}x^{2m+1}$$ and now I need to prove that it is convergent for all $x \in \mathbb{R}$ as the book claims. Applying The Ratio Test, I will need just to show that the two following limits hold:  $$\lim_{m \to \infty} \dfrac{2^{2m}m!(m+1)!}{(2m+1)!(2m+1)}=0 \ \ \ \text{and} \ \ \lim_{m \to \infty} \dfrac{(2m+1)!(2m+3)!}{2^{2m+1}[(m+1)!]^2}=0. $$ But after lots of effort I don't know how to prove that the limits are zero and unfortunately I know not much about infinite products as all Real Analysis books teach only infinite sums! So my questions are: Proving the mentioned limits. A self-learn-able book containing lots of theorems and information about infinite products. -- Added. [after reading @adfriedman's answer]: I have made a mistake regarding the second limit and the correct one is to show that $ \lim_{m \to \infty} \dfrac{(2m+1)!(2m+3)}{2^{2m+1}[(m+1)!]^2}=0$ which holds since $$\dfrac{(2m+1)!(2m+3)}{2^{2m+1}[(m+1)!]^2} = \dfrac{\prod_{k=1}^{2m+3}k}{(2m+2)2^{2m+1}\prod_{k=1}^{m+1}k^2} =\dfrac{\prod_{k=1}^{2m+3}k}{(m+1) \prod_{k=1}^{m+1}(2k)^2} = \dfrac{2m+3}{m+1} \dfrac{1 \times 3 \times  \dots \times (2m+1)}{2 \times 4 \times  \dots \times (2m+2)} \ge \dfrac{2m+3}{m+1} \dfrac{1}{\prod_{k=1}^{2m+1}(1+1/k)} \to 0$$","['infinite-product', 'limits', 'book-recommendation']"
2872410,Proof that every metric space is normal,"I've seen this proof of the fact that a metric space is normal multiple times but I can't understand how it's valid. Notation used: For $x \in X$ , and $Y$ a subset of $X$ , define $D(x,Y)=\inf \{d(x, y): y \in Y\}$ . The above proof uses that if $Y$ is a closed set, then $D(x, Y)>0$ , $\forall x \in X \setminus Y $ and this in turn implies $\space D(x, Y)> \epsilon$ , $\forall x \in X \setminus Y $ for some $\epsilon > 0$ . If the above proof was true, then we could also argue that for any $y \in Y$ we have $B(y, \frac{\epsilon}{3}) \subseteq Y$ (since $d(y, x)> \epsilon, \space \forall x \in X \setminus Y \Rightarrow B(y, \frac{\epsilon}{3}) \cap \{X \setminus Y\} = \emptyset) $ so $Y$ is open which is obviously false. I don't know what I'm missing becuase this kind of proof seems to appear everywhere. Thank you!","['general-topology', 'metric-spaces', 'separation-axioms']"
2872447,Why is u'(t)/u(t) = (ln(u(t)))'?,I saw this from these online notes about differential equations. $\frac{\mu'(t)}{\mu(t)} = p(t)$ Hopefully you will recognize the left side of this from your Calculus I class as the following derivative. ($\ln\mu(t))' = p(t)$ I don't think I understand why $\frac{\mu'(t)}{\mu(t)} = (\ln\mu(t))'$ is true. Why is this the case? Thank you very much!,"['calculus', 'ordinary-differential-equations']"
2872451,"Calc 3 : ""Find the surface area of the part of the sphere $x^2+y^2+z^2= r^2$ where $2r/3<z$",Ive been struggling with this problem. I understand that as a surface integral problem I need to parameterize in terms of spherical coordinates. What I don't know is how does $2r/3<z$ affect the bounds and how I am supposed to integrate this. Any tips would be greatly appreciated. Thanks!,"['integration', 'multivariable-calculus', 'spheres']"
2872455,Iwasawa Matrix Decomposition Proof,"Iwasawa Decomposition (special case): Let $G=SL_n(\Bbb{R})$, $K=$ real unitary matrices, $U=$ upper triangular matrices with $1$'s on the diagonal (called unipotent ), and $A=$ diagonal matrices with positive elements ($0$ everywhere else). Then, the product map $U\times{A}\times{K}\rightarrow{G}$ given by $(u,a,k)\mapsto{uak}$ is a bijection. Here is the proof by Serge Lang in his book Undergraduate Algebra Section 6 Chapter 4 pg 246 (Read below picture for question(s) please :) I have a few questions about this proof (understand it roughly as a whole): 1) How does one get that $B=au$, following $g^{-1}=Bk^{-1}$? 2) Why does it follow that A has positive diagonal elements - that is $a_i=b_{ii}>0$? (My guess is that the QR decomposition guarantees this for R and note $B=R$) 3) I can't see any point in the proof where Lang makes a reference to the fact that $g$ has determinant $1$, in fact it seems that $g$ could have any non-zero determinant, hence $g\in{GL_n(\Bbb{R})}$. Why is this not so? Thank you.","['linear-groups', 'matrices', 'abstract-algebra', 'linear-algebra', 'matrix-decomposition']"
2872469,Proof of $\int_{0}^{\pi}\cos^n(x)\cos(nx)dx=\frac{\pi}{2^n}$,"I am trying to prove the following:
$$\int_{0}^{\pi}\cos^n(x)\cos(nx)dx=\frac{\pi}{2^n}$$ so far I have got:
$$I=\int_{0}^{\pi}\cos^n(x)\cos(nx)dx=\Re\left(\int_{0}^{\pi}\cos^n(x)\left(\cos(nx)+i\sin(nx)\right)dx\right)=\Re\left(\int_{0}^{\pi}\cos^n(x)e^{inx}dx\right)$$
since $e^{ix}=\cos(x)+i\sin(x)$ through Euler's formula and then $(e^{ix})^n=\cos(nx)+i\sin(nx)$ from De Moivre's theorem. We also know that:
$$\cos(x)=\frac{e^{ix}+e^{-ix}}{2} \therefore \cos^n(x)=\frac{(e^{ix}+e^{-ix})^n}{2^n}$$
$$\therefore I=\frac{1}{2^n}\Re\left(\int_{0}^{\pi}e^{inx}\left(e^{ix}+e^{-ix}\right)^ndx\right)=\frac{1}{2^n}\Re\left(\int_{0}^{\pi}e^{inx}\sum_{r=0}^{n}\left(\begin{matrix}n\\r\end{matrix}\right)(e^{ix})^{n-r}(e^{-ix})^rdx\right)=\frac{1}{2^n}\Re\left(\sum_{r=0}^{n}\left(\begin{matrix}n\\r\end{matrix}\right)\int_{0}^{\pi}(e^{ix})^{2n-2r}dx\right)=\frac{1}{2^n}\Re\left(\sum_{r=0}^{n}\left(\begin{matrix}n\\r\end{matrix}\right)\left[\frac{e^{(2n-2r)ix}}{(2n-2r)i}\right]_{0}^{\pi}\right)$$ I have found the same problem but a confusing layout, shown below. Could someone show me where to go from here? https://www.quora.com/Is-it-possible-to-integrate-cos-n-x-cos-nx-taking-limits-as-0-to-%CF%80 (1) EDIT:
$$I=\frac{1}{2^n}\Re\left(\sum_{r=0}^{n}\left(\begin{matrix}n\\r\end{matrix}\right)\left[\frac{e^{(n-r)2i\pi}-1}{2(n-r)i}\right]\right)$$ for when $r=n$ we effectively get $$\lim_{x \to 0}\left(\frac{e^{2xi\pi}}{2xi}\right)=\lim_{x \to 0}\left(\frac{2i\pi\,e^{2xi\pi}}{2i}\right)=\pi$$ FURTHER EDIT:
since all other elements in the summation will be imaginary then the summation will evaluate to $\pi$ therefore I have proved it","['integration', 'complex-analysis']"
2872483,Uncountable Collections of Arcs In the Plane with Prescribed Properties,"I was wondering, if for each angle $0 \leq \theta < 2\pi$ we have an uncountable collection $A_\theta$ of pairwise-disjoint, closed, straight line segments with length $1$ and slope $\theta$ in the plane, is it possible for all of the $A_\theta$'s to be immersed in the plane simultaneously?  That is to say, is there a collection of pairwise-disjoint, closed unit segments in the plane such that for each slope, uncountably many have that slope? I feel like the answer is no.  By covering the plane with a sufficiently small tiling of squares you will find, for each slope, some pair in the covering such that each has uncountably many end points of segments with that slope.  Then you can pick a square 'between' that pair relative to this slope such that its boundary is intersected by uncountably many segments twice.  Another counting procedure then forces the existence of a square such that this happens for uncountably many different angles. The details of those parts are trivial, but now I get stuck showing that this is impossible.  Namely, that this compact square can't contain uncountably many families of uncountably many segments spanning its perimeter, each of whose members are disjoint.  It seems to be some order-theoretic statement, since we know that each of these uncountable sets (for a given slope) contains an uncountable closed set.  We can assume it is the Cantor Set for all but countably many, lest we get an uncountable number of pairwise-disjoint neighborhoods of the square. But there is an uncountable collection of pairwise-disjoint Cantor Sets in the unit interval (equivalently, the circle/square perimeter).  So I am wondering, can this proof be saved by some facts about how these are embedded together? This area of math has been studied some, in the form of decompositions of the plane or ensembles/configurations of various objects in the plane, but I am not familiar with the literature.  Does anybody have a good starting point for the latter viewpoint (i.e. not usc decomposition theory), preferably open access?  I would assume the above question would have been addressed early in its development, but this problem can be generalized in many ways.  Does anybody see some generalizations that this method would still work for?  Haven't bothered to suss them out, so feel free to take the credit haha Any other cool results of this flavor, please share! Thanks!","['cantor-set', 'plane-curves', 'order-theory', 'plane-geometry', 'general-topology']"
2872529,How to get this formula for mean and scalar curvatures for tube,"I'am reading paper HE L, Jiao X X, Zhou X C: $On\ almost\ complex\ curves\ and\ Hopf\ hypersurfaces\ in\ the\ nearly\ K\ddot{a}hler\ 6-sphere, $ Sci China Math, 2014. 57: 1045-1056, doi:10.1007/s11425-014-4777-3 I don't know how to get this formula (3.4) and (3.5) for The mean curvature H and the scalar curvature ρ of tubular hypersurface $M_r ⊂ S^6(1)$? \begin{align}
&H=\frac{(2+5(x_1^2+x_2^2)\lambda^2)\cot\ r-3\cot^3\ r}{5(\cot^2\ r-(x_1^2+x_2^2)\lambda^2)}\dot{\gamma}_\eta(r),\\[0.1cm]
&\rho=20+\frac{2-(12+20(x_1^2+x_2^2)\lambda^2)\cot^2\ r+6\cot^4\ r}{\cot^2\ r-(x_1^2+x_2^2)\lambda^2}.
\end{align} Did anyone see calculations for this in some paper or how to get this formulas?","['algebraic-geometry', 'geometry', 'differential-geometry']"
2872627,Approximations to series of Ramanujan-type,"Recently I have been playing around with series of the form $$\sum_{k=1}^{\infty}\frac{k^{s}}{e^{kz}-1} =  \sum_{k=1}^{\infty}\sigma_{s}(k)e^{-kz}$$ for $s \in \mathbb{Z}$ and where $\sigma_s(k)$ is the sum of divisors function of order $s$.  These series have generated quite a bit of interest over the years, due in large part to some beautiful modular identities of Ramanujan.  The most famous example being $$\alpha^{-n}\left(\frac{1}{2}\zeta(2n+1)+\sum_{k=1}^{\infty}\frac{k^{2n-1}}{e^{2\alpha k}-1}\right) = \\ (-\beta)^n\left(\frac{1}{2}\zeta(2n+1)+\sum_{k=1}^{\infty}\frac{k^{2n-1}}{e^{2\beta k}-1}\right) - 2^{2n}\sum_{k=0}^{n+1}(-1)^k\frac{B_{2k}}{(2k)!}\frac{B_{2n+2-2k}}{(2n+2-2k)!}\alpha^{n+1-k}\beta^k$$ where $\alpha,\beta > 0, \alpha\beta=\pi^2$ and $B_k$ are the Bernoulli numbers and $\zeta(k)$ is the Riemann zeta function.  As far as I know there aren't any similar relations or closed forms when $s \in 2\mathbb{Z}$. In my investigations I was able to find some approximation formula for general $s > 0$ but which unfortunately perform poorer and poorer as $s \rightarrow \infty$. For instance, at $s=2$ we have $$\sum_{k=1}^{\infty}\frac{k^2z}{e^{kz}-1} \approx \frac{2\zeta(3)}{z^2} - \frac{1}{2}-\frac{z}{24} -\sum_{j=0}^{N}B^{(2)}_{j+2}B_{j}\frac{z^{j}}{(j+2)!}$$ where $B^{(k)}_n$ are the Norlund polynomials. I was excited to find this, but then unfortunately realized that since  the sum on the right hand side diverges as $N \rightarrow \infty$ we can only achieve a finite number of accurate digits as the RHS approaches the left from below then surpasses it, growing without bound. For instance letting $N=37$ we have $$\sum_{k=1}^{\infty}\frac{k^2}{e^{k}-1} \approx 2 \zeta (3)-\frac{707928034947324016593079681811720894660110227517}{8567110474102926210628918330759216889856000000000}$$ with the right hand side being correct to the 14-th decimal place.  This is about the best we can do with the above formula. I am curious as to whether someone would be able to provide a better approximation. I am not very familiar with sort of thing... so maybe there is a standard way of achieving approximations like the one above?","['divergent-series', 'sequences-and-series', 'approximation', 'real-analysis']"
2872738,How to find the sum of $1+(1+r)s+(1+r+r^2)s^2+\dots$?,"I was asked to find the geometric sum of the following: $$1+(1+r)s+(1+r+r^2)s^2+\dots$$ My first way to solve the problem is to expand the brackets, and sort them out into two different geometric series, and evaluate the separated series altogether: $$1+(s+rs+\dots)+(s^2+rs^2+r^2s^2+\dots)$$ The only problem is it doesn't seem to work, as the third term, $(1+r+r^2 +r^3)s^3$ doesn't seem to fit the separated sequence correctly. Any help would be appreciated.","['summation', 'geometric-series', 'sequences-and-series']"
2872760,"Existence of finite strategy in a ""synergy""-hopping game","I have in mind the next Game: Given $n$ points on $\mathbb{R}$, two random points are picked and
  moved to the location of their average. E.g., pick points at location $x_1, x_2$ and they both moved to $\frac{x_1 + x_2}{2}$. It is not difficult to show, that center of the mass of such system is stationary, for simplicity let it be located at $0$. I'm interested in possibility of converging all points to the center of mass in finite number of steps (each step is picking a pair of points). Note, that asymptotic convergence is ""obvious"". To clarify on the previous point, the assumption of random picking could be dropped, in favor of showing if points in general position could be led to convergence. For example, for $n = 2^k$, the strategy is to serially collapse pairs of points, then quadruples, etc. Strategy for $n = 4$: pick points at $x_1, x_2$ move to $\frac{x_1 + x_2}{2}$. pick points at $x_2, x_4$ move to $\frac{x_3 + x_4}{2}$. if not yet every point at $0$, pick one point at $\frac{x_1 + x_2}{2}$ and one at $\frac{x_3
   + x_4}{2}$ and move to $0$ (repeat twice) For $n = 3$, if none of the points is initially at $0$, it is impossible to drive all points to $0$ [i.e. no such strategy exists]. For this note that at any point in time after the first step the split is one point vs. pair of two other points. At this point, I have a conjecture, that for all $n \neq 2^k$ a finite strategy does not exist. With a random intuition, that none of $\frac{1}{n}$ are finitely representable in binary. What should be my direction in proving this conjecture? UPDATE: My thoughts on this $\frac{1}{n}$ approach. After setting center of mass to $0$ we know
$$\frac{1}{n}x_1 + \frac{1}{n} x_2 + \ldots + \frac{1}{n} x_n = 0 $$ Let track the points as a linear combinations of the initial points. E.g. if we choose to pair $x_1$ and $x_2$, then we have points
$$\frac{1}{2}x_1 + \frac{1}{2}x_2, \frac{1}{2}x_1 + \frac{1}{2}x_2, 1\cdot x_3, \ldots, 1 \cdot x_n$$ Then after $m$ steps points in general look like
$$\sum\limits_{i=1}^n\frac{A_{1,i}}{2^m}\cdot x_i, \sum\limits_{i=1}^n\frac{A_{2,i}}{2^m}\cdot x_i, \ldots, \sum\limits_{i=1}^n\frac{A_{n,i}}{2^m}\cdot x_i, \text{ with } A_{k,i} \in \{0, 1 \ldots, 2^m\}$$ Suppose, there exist some finite strategy. Then [this is a weak point] we somehow could suppose $(x_i)$ is an independent set over $\mathbb{R}$, and all the coefficients should be equal. But should we then suppose, that $\frac{A_k,i}{2^m} = \frac{1}{n}$?","['analytic-geometry', 'combinatorics', 'combinatorial-game-theory']"
2872761,First Chern class for smooth line bundle,"For holomorphic line bundle we define its first Chern class by exponential sequence $$0\to \mathbb Z \to \mathcal O \to \mathcal O^* \to 0 $$
and we can similarly define Chern class for smooth line bundle by the short exact sequence 
$$0\to \mathbb Z \to \mathcal C^\infty \to (\mathcal C^\infty)^*\to 0$$ Then there is a natural morphism from the first short exact sequence to the second one, so there is a natural map $H^2(\mathbb Z)\to H^2(\mathbb Z)$. Is this map isomorphic? Similarly, is the map $H^1(\mathcal O^*)\to H^1((\mathcal C^\infty)^*)$ just the natural map of on equivalent classes of line bundles? In fact I am almost sure this is true (because they looks natural), but I do not know how to show this formally?","['complex-geometry', 'algebraic-geometry', 'line-bundles']"
2872767,Existence and uniqueness of 2nd order linear differential equations,"I know that the equation $$\frac{d^{2}x}{dt^{2}}+p\left(t\right)\frac{dx}{dt}+q\left(t\right)x=g\left(t\right),$$
has a unique solution on open sets where $p\left(t\right),q\left(t\right)$ and $g\left(t\right)$ are continuous. 
What I was wondering if this fact could be derived from the Picard's Theorem on Uniqueness and Existence of First ODE making the usual substitution $y=x'$ and $y_0=x(t_0)$. If so, why  $p\left(t\right),q\left(t\right)$ do not need to be Lipschitz and only need to be continuous?","['complex-analysis', 'analysis', 'ordinary-differential-equations', 'real-analysis']"
2872784,Fubini's Theorem about double integration in polar coordinates,"So I am studying James Stewart's Calculus 8th edition and Fubini's theorem is defined in the following way... If f is continuous on the rectangle R = {(x, y) | a ≤ x ≤ b, c ≤ y ≤ d}. 
Then the integral over this region R is... $$\int\int_Rf(x, y)\space dA =\int_a^b\int_c^df(x,y) \space dydx = \int_c^d\int_a^bf(x,y) \space dxdy$$ Which I think means that the double integral over a rectangular region can be calculated using a iterated integral. This makes sense. However, in a later section it derives the method of computing double integrals over regions defined by polar coordinates. The following is where I am struggling to understand their logic. So they discuss defining circular regions using polar coordinates and splitting it up into m x n different regions referred to as 'polar rectangles'. Similar to this image here, From there, they take a sample point within each polar rectangle and multiply the value of the function at that sample point by the area of the polar rectangle. They do this for all of the polar rectangles in the region  and sum them together thus arriving at the following Riemann sum. $$\sum_{i=1}^{m}\sum_{j=1}^{n}f(r_i^*\cosθ_j^*,r_i^*\sinθ_j^*)ΔA_i=
\sum_{i=1}^{m}\sum_{j=1}^{n}f(r_i^*\cosθ_j^*,r_i^*\sinθ_j^*)r_i^*ΔrΔθ$$ Where
$$ΔA_i = r_i^*ΔrΔθ$$ is the area of these polar rectangles. From there they make the substitution, $$g(r, \theta) = rf(r\cos\theta, r\sin\theta)$$ So the above Riemann sum becomes, $$\sum_{i=1}^{m}\sum_{j=1}^{n}g(r_i^*, \theta_j^*) ΔrΔ\theta$$ Which is, with no explanation and presumably after taking the limit of this sum as n and m tend to infinity, equated to the double integral, $$\int_\alpha^\beta\int_a^bg(r, \theta)\space drd\theta$$ Where the region in polar coordinates is defined as $$R = \{(r, \theta) \space |\space \alpha ≤ \theta ≤ \beta , a ≤ r ≤ b\} $$ So finally my question is how can Fubini's theorem be used to relate the above Riemann sum to the iterated integral of g when the theorem states that the region must be a rectangle. My intuition tells me that even though the polar region is not a rectangle, it is contained between constant numbers (like a rectangle) so perhaps Fubini's theorem will still hold.","['multivariable-calculus', 'multiple-integral', 'polar-coordinates']"
2872805,Is a 'root' an intrinsic property of a tree,"Is root an intrinsic property of a given tree?(Given a tree, can you uniquely determine the root?) Can't any vertex of a tree be chosen as a root? Aren't all trees rooted in that case?","['graph-theory', 'trees', 'discrete-mathematics']"
2872808,Can we solve $A+D\sin^{2}x=B\sin x+C\cos x$ without having to solve a quartic polynomial?,"Suppose the following equation
$$
A+D\sin^{2}x=B\sin x+C\cos x,
$$
where $A,B,C,D\in\mathbb{R}$ are the real constants. Initially, I tried to find its solution from a simple substitution
\begin{align*}
A-B\sin x+D\sin^{2}x & =\pm C\sqrt{1-\sin^{2}x},
\end{align*}
that after $t=\sin x$ leads to the following quartic equation
$$
(A^{2}-C^{2})-2ABt+(B^{2}+2AD+C^{2})t^{2}-2BDt^{3}+D^{2}t^{4}=0.
$$
The Weierstrass substitution, where $t=\tan\frac{x}{2}$, and
$$
\sin x=2\sin\frac{x}{2}\cos\frac{x}{2}=\frac{2\sin\frac{x}{2}}{\cos\frac{x}{2}}\cos^{2}\frac{x}{2}=\frac{2\tan\frac{x}{2}}{\frac{1}{\cos^{2}\frac{x}{2}}}=\frac{2\tan\frac{x}{2}}{1+\tan^{2}\frac{x}{2}}=\frac{2t}{1+t^{2}},
$$
and
$$
\cos x=\cos^{2}\frac{x}{2}-\sin^{2}\frac{x}{2}=\left(1-\frac{\sin^{2}\frac{x}{2}}{\cos^{2}\frac{x}{2}}\right)\cos^{2}\frac{x}{2}=\frac{1-\tan^{2}\frac{x}{2}}{\frac{1}{\cos^{2}\frac{x}{2}}}=\frac{1-\tan^{2}\frac{x}{2}}{1+\tan^{2}\frac{x}{2}}=\frac{1-t^{2}}{1+t^{2}},
$$
leads to
$$
A+D\frac{4t^{2}}{(1+t^{2})^{2}}=B\frac{2t}{1+t^{2}}+C\frac{1-t^{2}}{1+t^{2}},
$$
which is also the quartic equation for $t$
$$
(A+C)t^{4}-2Bt^{3}+2(A+2D)t^{2}-2Bt+A-C=0,
$$
$$
(A+C)t^{4}-2Bt(t^{2}+1)+2(A+2D)t^{2}+A-C=0.
$$
Is there any better substitution avoiding the transformation of the trigonometric identity to the quartic equation for $t$? Thanks for your help.","['algebra-precalculus', 'quartics', 'polynomials', 'trigonometry']"
2872834,"What's wrong with my proof that $\sigma(a)\subseteq[-\|a\|, \|a\|]$ for $a$ self-adjoint?","Let $U$ be a $C^*$-algebra and $a\in U$ be self-adjoint. I have a simple proof that $\sigma(a)\subseteq [-\|a\|,\|a\|]$, where $\sigma(a)$ is the spectrum of $a$. It goes as follows (the facts used are all known at this point): We have $\sigma(a) = \sigma(a^*) = \overline{\sigma(a)}$, hence $\sigma(a)\subseteq \mathbb R$. Since $a$ is normal, we have $\|a\| = r(a)$ (where $r(a) = \sup \{|\lambda| : \lambda \in \sigma(a)\})$. Hence $\sigma(a)$ is bounded by $\pm \|a\|$, i.e. $\sigma(a) \subseteq [-\|a\|, \|a\|]$. The reason why I think this must be wrong is because both Kadison-Ringrose and Bratelli-Robinson use much more elaborate arguments. What's wrong with my proof?","['c-star-algebras', 'functional-analysis', 'operator-algebras']"
