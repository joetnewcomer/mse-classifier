question_id,title,body,tags
2951085,Differential Equations proof for Prove that $\dim(\ker(TU)) ≤ (\dim\ker(T)) + \dim(\ker(U))$.,"Let $T$ and $U$ be linear transformations $V → V$ with ﬁnite-dimensional kernels. Prove that $\dim(\ker(TU)) ≤ (\dim\ker(T)) + \dim(\ker(U))$ My tutor suggested that I create two new transformations by restricting the range and domain of $U$ and $V$ in the following way $\hat U : V → U(V )$ and $\hat T : U(V ) → V $ . Note that the compositions $TU$ and $\hat T \hat U$ are identical, that $\hat U$ is onto, and that $\dim(\ker(\hat T)) ≤ \dim(\ker(T))$ But I still do not understand.","['proof-explanation', 'proof-writing', 'ordinary-differential-equations']"
2951134,Find range of exponent,"I have the following function I need to find the range for and I'm not sure if I'm on the right direction. $f(x,y) = e^{-x^2-(y-1)^2}$ $x$ & $y$ are real-numbers. I'm thinking that the range is ""all real values for $y$ that are $> 0$ ."" Is this right?","['inequality', 'proof-writing', 'monotone-functions', 'algebra-precalculus', 'exponential-function']"
2951152,What is the limit of $\mathrm{Tr}(G^kM{G^*}^k)^{1/2k}$ when $k$ goes to infinity?,"If $G\in \mathscr M_n(\mathbf C)$ then it's well known that $\lim_{k\to \infty}\|G^k\|^{1/k}=\rho(G)$ where $\rho(G)$ is the spectral radius of $G$ , the value of the limit does not depend on the choosen norm. Consequently if we take the Schur norm we get $$
\lim_{k\to \infty} \mathrm{Tr}(G^k{G^*}^k)^{1/2k}=\rho(G).
$$ Now if $M$ is an hermitian positive definite matrix then $\|A\|_M=\sqrt{\mathrm{Tr}(AMA^*)}$ is still a norm on $\mathscr M_n(\mathbf C)$ and so we also get $$
\lim_{k\to \infty} \mathrm{Tr}(G^kM{G^*}^k)^{1/2k}=\rho(G).
$$ I would like to know what happen when $M$ is only hermitian positive semi-definite .  More precisely my question is the following. Question :
  Let $G\in \mathscr M_n(\mathbf C)$ be an invertible
  matrix and let $M$ be an hermitian positive semi-definite matrix with $\mathrm{Tr}(M)=1$ . Is it true that the limit $$\lim_{k\to \infty}\mathrm{Tr}(G^kM{G^*}^k)^{1/2k}$$ exists and is always equal to the modulus of an eigenvalue of $G$ ? I know that the $\lim \inf$ is larger than the smallest singular value and that the $\lim\sup$ is smaller than the largest singular value.","['banach-algebras', 'trace', 'matrices', 'spectral-radius', 'symmetric-matrices']"
2951180,Must a continuous and periodic functions have a smallest period?,"Let $D\subset\mathbb R$ and let $T\in(0,+\infty)$ . A function $f\colon D\longrightarrow\mathbb R$ is called a periodic function with period $T$ if, for each $x\in D$ , $x+T\in D$ and $f(x+T)=f(x)$ . If $D\subset\mathbb R$ and $f\colon D\longrightarrow\mathbb R$ is continuous and periodic, must there be, among all periods of $f$ , a minimal one? Questions like this one have been posted here before , but in each case, as far as I can see, the domain of $f$ was $\mathbb R$ , which implies that the set $P$ of periods, together with $0$ and $-P$ , is a subgroup of $(\mathbb{R},+)$ . Using that (together with continuity), it is easy to see that a minimal period must exist indeed. But I don't know whether it is true or not in the general case.","['continuity', 'periodic-functions', 'real-analysis']"
2951208,Variation of the rook problem with rooks of two different colours.,"I'm sorry if this is not actually a variation on the rook problem, but it's the most similar problem I could find. The problem is you're given a square board which has $N \times N\ (N \geq 2)$ squares, and you have two kinds of rooks: red and blue. Each row and column must have exactly one of each colour of rook.
I have found that the solution for this problem on an empty board is $N! \cdot !N$ . Is there a formula to figure out the amount of distinct solutions for a $N \times N$ grid with some pre-filled red and blue rooks as shown on this board ? The amount of rooks of a single colour can vary anywhere from $0..N$ and the amount of red and blue rooks can be different.",['combinatorics']
2951249,Why doesn't the Stone-Weierstrass theorem imply that every function has a power series expansion?,"I know that not every function has a power series expansion. 
Yet what I don't understand is that for every $C^{\infty}$ functions there is a sequence of polynomial $(P_n)$ such that $P_n$ converges uniformly to $f$ . That's to say : $$\forall x \in [a,b], f(x) = \lim_{n \to \infty}  \sum_{k = 0}^{\infty} a_{k,n}x^k$$ But then because it converges uniformly why can't I say that : $$\forall x \in [a,b], f(x) =   \sum_{k = 0}^{\infty} \lim_{n \to \infty} a_{k,n}x^k$$ And so $f$ has a power series expansion with coefficients: $\lim_{n \to \infty} a_{k,n}x^k$ .","['power-series', 'calculus', 'real-analysis']"
2951319,Recommended Problem books for undergraduate Real Analysis,"So I am taking an analysis class in my university and I want a problem book for it. The topics included in the teaching plan are Real Numbers: Introduction to the real number field, supremum, infimum, completeness axiom, basic properties of real numbers, decimal expansion, construction of real numbers. Sequences and Series: Convergence of a sequence, Cauchy sequences and subsequences, absolute and conditional convergence of an infinite series, Riemann's theorem, various tests of convergence. Point-set Topology of: : Open and closed sets; interior, boundary and closure of a set; Bolzano-Weierstrass theorem; sequential definition of compactness and the Heine-Borel theorem. Limit of a Function: Limit of a function, elementary properties of limits. Continuity: Continuous functions, elementary properties of continuous functions, intermediate value theorem, uniform continuity, properties of continuous functions defined on compact sets, set of discontinuities. I am already following up Michael J. Schramm's Introduction to Real Analysis for my theory But a problem book with varied questions on the concepts would help me a lot. Please recommend some problem books. Thanks P.S : I have already asked my professor to recommend some books but he always recommends baby Rudin and also doesn't provide a lot of assignments. I am not compatible with Rudin's book. Also his tests are very tough as he wants us to cook up counter examples and I am very poor in that. So I need a good problem book to master real analysis.","['book-recommendation', 'reference-request', 'real-analysis']"
2951334,closed formula for determinant,"Consider the following matrix $$
\begin{equation}A_{r-1} := 
\begin{bmatrix}
\frac{1}{x_{1}}	& -p	& \dots	 & 0  &\dots &0    \\
-q	& \frac{1}{x_{2}} & -p &0 	& \dots  & 0 	  \\
0	& -q & \frac{1}{x_{3}} &-p 	& ~...  & 0 	  \\
0	& 0 &-q &\frac{1}{x_{4}} &-p  	& 0 	  \\
0  &\vdots	 	 & \ddots & -q & \frac{1}{x_{r-2}}  & -p \\
0 	&0 &0 &\dots &-q	 &\frac{1}{x_{r-1}}
\end{bmatrix}
\end{equation}
$$ where $x_{i},p,q \in \mathbb{R}$ , $x_{i} \neq 0$ for all $i = 1,2,...,r-1$ and $r \in \mathbb{N}$ , $r \geq 3$ . I want to find a closed formula for $\det(A)$ . For $r=3$ we have $$
\begin{equation}
\det(A) = 
\begin{vmatrix}
\frac{1}{x_1} & -p   \\
-q & \frac{1}{x_2} \\
\end{vmatrix} = \frac{1 -pq(x_1 x_2)}{x_1
x_2}\end{equation} .
$$ For $r = 4$ we have $$
\begin{equation}
\det(A) = 
\begin{vmatrix}
\frac{1}{x_1} & -p & 0   \\
-q & \frac{1}{x_2} & -p \\
0  &-q &\frac{1}{x_3}  \\
\end{vmatrix} = \frac{1 -pq(x_1 x_2 + x_2 x_3)}{x_1 x_2 x_3}
\end{equation} .
$$ Up to this point I think the formula is given by $$
\det(A) = \frac{1 - pq(x_{r-1}x_{r-2} + x_{r-2}x_{r-3})}{x_{1}x_{2}...x_{r-1}}.
$$ But this is not correct. For $r = 5$ I get $$
\begin{equation}
\det(A) = 
\begin{vmatrix}
\frac{1}{x_1} & -p & 0 &0   \\
-q & \frac{1}{x_2} & -p &0 \\
0  &-q &\frac{1}{x_3} &-p  \\
0 & 0 &-q &\frac{1}{x_4 } \\
\end{vmatrix} = \frac{1 -pq(x_1 x_2 + x_2 x_3 + x_3 x_4) + p^2 q^2x_1 x_2 x_3x_4}{x_1 x_2 x_3 x_4}.
\end{equation} .
$$ Has someone has an idea how to find a closed formula? Thanks for any hints!","['matrices', 'calculus', 'determinant']"
2951432,How to prove that a delta function belongs to the Besov space $B^{-1}$?,"$\def\R{\mathbb{R}}$ $\DeclareMathOperator{\supp}{supp}$ I am trying to understand the definition of the Besov space and to prove that the delta function in $\R^1$ . However I got stuck. First, let us recall that a sequence of smooth functions $\{\phi_i\}_{i\ge-1}$ is called a partition of unity if $\sum_{i=-1}^\infty \phi_i=1$ , $\supp(\phi_{-1})\subset\{x\colon |x|\le 2\}$ ; $\supp(\phi_{i})\subset\{x\colon 2^{i-1}\le |x|\le 2^{i+1}\}$ . The Besov space $B^s_{p,q}$ , where $s\in\R$ , $p,q\ge1$ is defined as the collection of all functions $f$ such that $$
\|f\|_{p,q}^s:=\|(2^{sj}\|F^{-1}\phi_j Ff\|_{L_p})_{j\ge-1}\|_{l_q}<\infty.
$$ Here $F$ is the Fourier transform operator. I would like to prove that for any fixed $x\in\R$ the delta function $\delta_x$ is in $B^{-1}_{\infty,\infty}$ . Clearly, $$
F\delta_x(\lambda)=e^{i\lambda x}.
$$ But now I am confused. How should we estimate $$
\|F^{-1}\phi_j e^{i\lambda x}\|_{L_\infty}?
$$","['besov-space', 'functional-analysis', 'dirac-delta']"
2951443,Why Do We Only Take Norms Over Real/Complex Numbers?,"By definition, norms are defined over some $\mathbb{R}$ or $\mathbb{C}$ vector space. Why do we only restrict ourselves to these fields when other fields give rise to interesting objects as well? (e.g. p-adic evaluation) Is it a historic reason or because other fields would give properties so different that we‘d rather not also associate the term “norm“ with it? If so, then I assume that $\mathbb{R}$ and $\mathbb{C}$ are similar enough to make those the two fields that give rise to norms?","['normed-spaces', 'linear-algebra']"
2951454,Computing the limit of $\lim_{t\rightarrow0}tf(g(t))$ assuming $g(0)=0$ and $g'(0)>0$,"Suppose $f:(0,\infty)\rightarrow\mathbb{R}$ is a continuous function and $g:\mathbb{R}\rightarrow\mathbb{R}$ is a $C^1$ function with $g(0)=0$ and $g'(0)>0$ . If the limit $$
\lim_{t\rightarrow0^+} tf(t)=a
$$ exists, can we necessarily compute the limit $$
\lim_{t\rightarrow0^+} tf(g(t))?
$$ It seems like we can compute it as \begin{align*}
\lim_{t\rightarrow0^+} tf(g(t)) &= \lim_{t\rightarrow0^+} tf\left(t\frac{g(t)}{t}\right) \\&= \lim_{t\rightarrow0^+} tf\left(tg'(0)\right) = \frac{1}{g'(0)}\lim_{t\rightarrow0^+}tf(t) = \frac{a}{g'(0)}
\end{align*} but I'm not sure the step from line 1 to line 2 is valid. Is it?","['limits', 'real-analysis']"
2951459,A function $f:\mathbb{R}^2\rightarrow\mathbb{R}^2$ that maps a circle to a circle,Suppose f is a continuous function from $\mathbb{R}^2$ to $\mathbb{R}^2$ that maps a circle to a circle. How do I prove that f is differentiable? Will the function still be differentiable if continuity is not assumed?,"['multivariable-calculus', 'real-analysis']"
2951461,LU factorization of a nonsingular matrix exists if and only if all leading principal submatrices are nonsingular.,"I'm struggling to prove this theorem. I can prove that if the $LU$ factorization exists, then the leading principal submatrices are nonsingular. To do that, I can show that the determinant of every leading principal submatrix is not zero. (The leading principal submatrix is the product of $L$ and $U$ corresponding leading principal submatrices , and determinant of every $L$ leading principal submatrix is $1$ and determinant of the $U$ leading principal submatrix is product of the diagonal elements). To prove that if the leading principal submatrices are nonsingular, then $LU$ factorization exists, I believe I should use induction, but I'm getting nowhere. Can anyone help me with the proof?","['linear-algebra', 'matrix-decomposition']"
2951478,Dual Numbers and Automatic Differentiation,"I have the following equation. I would like to solve this at the point x = a, but using dual numbers. $$f\left(x\right) = \dfrac{1}{x} + \sin\left(\dfrac{1}{x}\right)$$ Now, the derivative of this function is below, and that is the function we'd like to reach in our answer. $$-\dfrac{1}{x^2} - \dfrac{\cos\left(\dfrac{1}{x}\right)}{x^2}$$ Here is what I have tried, and I am finding it difficult to simplify beyond that. $$\begin{align} f\left(x\right) = \dfrac{1}{x} + \sin\left(\dfrac{1}{x}\right)\\
&= \frac{1}{a^{\prime}+1\epsilon}+ sin(\dfrac{1}{a+1\epsilon})\\ &= \frac{a-1\epsilon}{(a-1\epsilon)(a+1\epsilon)} + sin(\frac{a-1\epsilon}{(a-1\epsilon)(a+1\epsilon)} )\end{align}$$ I do this using the following identities: $$\sin\left(\alpha + \beta \epsilon\right) = \sin\left(\alpha\right) + \cos\left(\alpha\right)\beta\epsilon$$ $$\dfrac{1}{\alpha + \beta\epsilon} = \dfrac{\alpha - \beta\epsilon}{\left(\alpha + \beta\epsilon\right)\left(\alpha - \beta\epsilon\right)}.$$ However, I am not certain how to simplify beyond that painfully obvious first step. Any hints?",['derivatives']
2951486,Uniform boundedness theorem.,"Let $V$ be subspace of $\ell^2$ which contains all 1 summable sequences. For each natural number $n$ , define $T_n: V \to \mathbb R$ by $T_n(x)=\sum_{i=1}^n x_i$ . Then $T_n$ is not uniformly bounded on unit ball $\|x\|_2\leq1$ . My intuition says it has something to do with closed and bounded in infinite dimensional banach space need not be compact. But I don't know how to get a firm answer. Could you please tell me the reason? Thank you very much for your time.","['hilbert-spaces', 'banach-spaces', 'functional-analysis']"
2951517,Simplifying Likelihood Ratio,"(Answered here: https://stats.stackexchange.com/questions/372040/rejection-region-for-likelihood-ratio-test ) I have a data set $((Y_1,x_1),(Y_2,x_2),...,(Y_n,x_n))$ where $Y_i$ is distributed as $N(\theta x_i,1)$ . I want to perform a likelihood ratio test for $\theta$ to investigate the hypothesis $H_0: \theta=\theta_0$ or $H_1: \theta\neq\theta_0$ : $$\lambda(X)=\frac{exp(\frac{-\sum_i(Y_i-x_i\theta_0)^2}{2})}{exp(\frac{-\sum_i(Y_i-x_i\hat\theta)^2}{2})}$$ Where $\hat\theta$ is the maximum likelihood estimator, which I have determined to be $\frac{\sum_i x_iY_i}{\sum_ix_i^2}$ . I want to simplify this ratio as much as possible, but I don't know how to go about this, because $x_i\theta$ varies with $i$ , and as such, cannot be taken out of the sum. Normally, I would solve this by adding $0$ (i.e. adding and subtracting a term), but this approach did not help me this time around. Is there a trick that I am missing? Thanks in advance.","['statistical-inference', 'statistics', 'summation']"
2951520,Fractional Hardy inequality,"From classic literature, I know the following result. Let $\Omega\subset\mathbb{R}^d$ be a bounded open set of class $C^1$ .
  Then there exists $C>0$ such that \begin{equation}\label{1} \|\frac{u}{d}
 \|_{L^2(\Omega)}\le C\|\nabla u\|_{L^2(\Omega)}\qquad \forall\ u \in
 H^1_0(\Omega), \end{equation} where $d(x):=\operatorname{dist}(x,\partial\Omega)$ . I am wondering if a fractional order equivalent of the previous result holds, namely $$
\|\frac{u}{d}
 \|_{H^s(\Omega)}\le C\|\nabla u\|_{H^s(\Omega)}\qquad \forall\ u \in
 H^1_0(\Omega)\cap H^2(\Omega),
$$ for every $0\le s<\frac{1}{2}$ . Does anyone have any ideas or counterexamples? Thanks!","['inequality', 'sobolev-spaces', 'functional-analysis', 'fractional-sobolev-spaces']"
2951534,"If $f:[0,1] \rightarrow \mathbb{R}$ is continuous function such that $f(0)=f(1)$ then there exists $ x\in [0,1]$ such that $f(x) = f(x+\frac{1}{n})$.","If $f:[0,1] \rightarrow \mathbb{R}$ is a continuous function such that $f(0)=f(1)$ , then there exists $ x\in [0,1]$ such that $f(x) = f(x+\frac{1}{n})$ , where $n$ is any natural number. Let $g(x)=f(x+\frac{1}{n})-f(x)$ on $[0,1-\frac{1}{n}]$ . Now, $f$ is continuous function on closed interval.So, it must attain its bounds. Let $m$ be minima at $c_1 \in [0,1]$ and $M$ be maxima at $c_2 \in [0,1]$ . Now if $c_1,c_2 \in [0,1-\frac{1}{n}]$ , apply IVT to $g(x)$ on $[c_1,c_2]$ . 
If $c_1$ or $c_2 \in [0,1-\frac{1}{n}]$ , apply IVT to $[c_1,c_1-\frac{1}{n}]$ or $[c_2,c_2-\frac{1}{n}]$ respectively. If $c_1,c_2$ both do not belong to $[0,1-\frac{1}{n}]$ , then apply IVT to $g$ on $[c_1-\frac{1}{n},c_2-\frac{1}{n}]$ . Is this proof correct? Thanks.","['functions', 'proof-verification', 'real-analysis']"
2951582,"Is ""The empty set is a subset of any set"" a convention?","Recently I learned that for any set A, we have $\varnothing\subset A$. I found some explanation of why it holds. $\varnothing\subset A$ means ""for every object $x$, if $x$ belongs to the empty set, then $x$ also belongs to the set A"". This is a vacuous truth, because the antecedent ($x$ belongs to the empty set) could never be true, so the conclusion always holds ($x$ also belongs to the set A). So $\varnothing\subset A$ holds. What confused me was that, the following expression was also a vacuous truth. For every object  $x$, if $x$ belongs to the empty set, then $x$ doesn't belong to the set A. According to the definition of the vacuous truth, the conclusion ($x$ doesn't belong to the set A) holds, so $\varnothing\not\subset A$ would be true, too. Which one is correct? Or is it just a convention to let $\varnothing\subset A$?","['elementary-set-theory', 'logic']"
2951601,Commutative Semigroup,"Let $S$ be a Semigroup with the two following properties, $(1):$ for all $x$ in $S$ we have $x^3=x$ $(2):$ for any $x,y$ in $S$ we have $xy^2x=yx^2y$ .
   Then prove that this Semigroup $S$ is commutative. I have found the following identities for any $x,y$ in $S$ $(xy)^3=xy=x^3y^3$ $xy^2x=y^2(xy^2x)$ $(xy)^2=y(xy)^2y$ $xy^2x^2x=yx^2yx^2$","['proof-writing', 'abstract-algebra', 'semigroups', 'binary-operations', 'group-theory']"
2951613,$\frac37x + \frac{3\times6}{7\times10}x^2 + \frac{3\times6\times9}{7\times10\times13}x^3+ \cdots$,Convergence of the series for $x>0$ : $$\frac37x + \frac{3\times6}{7\times10}x^2 + \frac{3\times6\times9}{7\times10\times13}x^3+ \cdots$$ The general tem  is $a_n = \frac{3\times6\times \cdots \times(3+(n-1)3)}{7\times10 \times\cdots \times(7+(n-1)3)} x^n$ . I am thinking of using Raabe's Test. but cannot proceed. We have to find $$\lim _{n \to \infty} n\left(\frac{a_n}{a_{n+1} } -1\right) = \lim _{n \to \infty} n \left(\frac{7+3n}{3+3n} \frac1x -1\right)$$ so the limit depends upon $x$ .,"['sequences-and-series', 'real-analysis']"
2951663,gradient of max function,"If I have a function $f=\max \{0, y-t\}$ , and I want to find the gradient of with respect to $[y \ \ t]$ , would that simply be $$
\nabla f = 
\begin{bmatrix}
\max\{0,0\} \\
\max\{1,-1\}
\end{bmatrix} = \begin{bmatrix}
0 \\
1
\end{bmatrix}
$$",['derivatives']
2951700,If $f(x)$ is a polynomial function with integer coefficients find min value of $f(12)$,"$f(x)$ is a polynomial function with integer coefficients satisfying $f(1)=5$ and $f(2)=7$ . What is the smallest possible positive value of $f(12)$ ? I have no idea on how to begin with this question. Also, I can't get any clue on the significance of integer coefficients.","['algebra-precalculus', 'polynomials']"
2951735,Proving If $k \le \left\lfloor \frac{n}{2} \right\rfloor$ then $\binom{n}{k-1} < \binom{n}{k}$,So I'm trying to do a proof for this problem: If $\displaystyle{k \le \left\lfloor \frac{n}{2} \right\rfloor}$ then $$\displaystyle{\binom{n}{k-1} < \binom{n}{k}}$$ I can do it algebraically but my professor asked for a combinatorial proof.  How would one does it?,"['combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'inequality']"
2951757,Why does this method for differentiating work? [duplicate],"This question already has answers here : Proof of this fairly obscure differentiation trick? (2 answers) Why does this differentation output correct result? [duplicate] (1 answer) Closed 5 years ago . Consider the function $$f(x)=x^x.$$ If I differentiate with respect to $x$ treating the exponent as a constant and then sum the derivative treating the base as a constant, I get \begin{align}
f'(x)&= xx^{x-1}+x^x\ln x\\
f'(x)&= x^x(1+\ln x).
\end{align}","['calculus', 'derivatives', 'recreational-mathematics']"
2951766,Intuition on mean curvature being equal to divergence of normal vector,"For starters, such a thing as the divergence of the normal vector is already ""counter intuitive"" to me because of the definition of divergence I like using (local flux density, involving an infinitesimal volume in $\mathbb{R}^3$ , whereas the normal vector is only defined on a surface) I've been introduced to the title's equality in the context of Laplace's law for pressure difference across an interface. I think I would need first an interpretation of ""divergence of normal vector"". .","['vector-analysis', 'differential-geometry']"
2951790,Exact value of Elliptic Integrals.,"I was taking currently in a elementary calculus course where i found how to find arc lengths of a smooth continuous curve. so here is how i started : $$\frac{x^2}{a^2}+\frac{y^2}{b^2}=1\Rightarrow y=\pm \frac{b}{a}\sqrt{a^2-x^2}$$ By applying the formula of the arc length of a function, we get: $$L=4\int_0^a\sqrt{1+\frac{b^2x^2}{a^2(a^2-x^2)}}dx=4\int_0^a\sqrt{\frac{a^4+(b^2-a^2)x^2}{a^2(a^2-x^2)}}dx$$ Now I made a little subsitution recalling trigonometry: $$x=a\sin(u)\\dx=a\cos(u)du$$ So the Integral now can be expressed as: $$L=4\int_0^{\frac{\pi}{2}}a\cos(u)\sqrt{\frac{a^4+(b^2-a^2)a^2\sin^2(u)}{a^2(a^2-a^2\sin^2(u))}}du=\\4\int_0^{\frac{\pi}{2}}a\cos(u)\sqrt{\frac{a^4+(b^2-a^2)a^2\sin^2(u)}{a^2(a^2\cos^2(u)+a^2\sin^2(u)-a^2\sin^2(u))}}du=\\4\int_0^{\frac{\pi}{2}}\sqrt{a^2+(b^2-a^2)\sin^2(u)}du$$ So we have: $$L=4a\int_0^{\frac{\pi}{2}}\sqrt{1+\frac{(b^2-a^2)}{a^2}\sin^2(u)}du$$ Letting $m=\frac{(b^2-a^2)}{a^2}$ we finally get: $$L=4a\int_0^{\frac{\pi}{2}}\sqrt{1+m\sin^2(u)}du$$ BUT later i found these type of integrals are known as elliptic integrals and can not be given in terms of elementary functions. say elliptic integral of another type $$u( k)=\int_{0}^{\frac{\pi}{2}} \frac{1}{\sqrt{1-k^2\sin(\theta)}} \, d\theta  $$ expanding by binomial theorem or mclaurian expansion will lead to : $$u(k)=\frac{\pi}{2}\Biggl[{1+\Biggl(\frac{1}{2}\Biggl)^2k^2+\Biggl(\frac{1.3}{2.4}\Biggl)^2k^4+\Biggl(\frac{1.3.5}{2.4.6}\Biggl)^2k^6+....}\Biggl]$$ Now I am very much interested in finding the values of these series rather than the arc length of ellipse. I just found on wikipedia where AGM method is used as : $$u(k)=\frac{\pi}{2M(1,\sqrt{1-k^2})}$$ But At first glance it is easy to see the following as a hypergeometric series :
i.e. $$_2F_1\bigg[\frac{1}{2},\frac{1}{2};1;k^2\bigg]=\frac{1}{2}u(k)$$ $$LHS=\sum_{n=0}^{\infty}\frac{\bigg[(\frac{1}{2})_n\bigg]^2 k^{2n}}{(1)_n n!}$$ where $(a)_n=\frac{\Gamma_{a+n}}{\Gamma_a}$ simplify a bit $$=\frac{1}{\pi}\sum_{n=0}^{\infty} \bigg(\frac{\Gamma_{\frac{1}{2}+n}}{n!}\bigg)^2 k^{2n}$$ But i really do not know how the values of hypergeometric series are evaluated.I have seen(of course in a basic way) some special cases of $_3F_2$ and $_7F_6$ but do not know what to do with those generated by ellptic integral. Being unexperienced i will admire any suggestion from anyone. please tell me how these elliptic integrals and hypergeometric series are evaluated. THANK YOU FOR GIVING YOUR TIME.","['special-functions', 'elliptic-functions', 'sequences-and-series', 'elliptic-integrals', 'hypergeometric-function']"
2951793,"If $f$ is holomorphic in a compact Riemann surface, it is constant. Why doesn't this work for compact subsets of $\mathbb{C}$?","Let $X$ be a compact Riemann surface. Suppose that $f$ is holomorphic over all of $X$ . Then $f$ is constant I proved this in the following way: The function $f$ is continuous and hence $|f|$ attains a maximum value $M$ . Let $p$ be a point of $X$ such that $|f(p)|=M$ . By the Maximum Modulus Principle, $|f|$ is constant (equal to $M$ ) in a neighborhood of $p$ . That is, the set of all $x$ that $|f(x)|=M$ is open. However such set is also closed since $f$ is continuous. Connectedness implies that $|f|$ is constant. Then $f(X)$ is contained in a circle of radius $M$ . This contradicts the open mapping theorem. I'm fairly confident that this proof is correct. What I don't understand is why the same proof does not apply to proving that a holomorphic function defined on a compact subset of $\mathbb{C}$ is constant, which is not true.","['complex-analysis', 'riemann-surfaces']"
2951811,Nearest signed permutation matrix to a given matrix $A$,"Given $A \in \mathbb{R}^{n \times n}$ , let $Q \in O(n)$ be the orthogonal matrix nearest to $A$ in the Frobenius norm, i.e., $$Q := \text{arg}\min_{M \in O(n)} \| A - M \|_{F}^2$$ It's well known that $Q = U V^{T}$ , where $A = U\Sigma V^{T}$ is the SVD of $A$ (see Orthogonal_Procrustes , Nearest orthogonal matrix ). I'm trying to solve a similar problem: $$S := \text{arg}\min_{M \in \mbox{SP}(n)} \| A - M \|_{F}^2$$ where $\mbox{SP}(n)$ is a group of signed permutation matrices . I know that in the case of permutation matrices, the problem reduces to linear sum assignment and can be solved using the Hungarian algorithm. I suspect in the signed permutation case it will reduce to some linear program. Is it possible to somehow solve this problem using SVD or the Hungarian algorithm? I would really like to avoid general LP solvers, if possible.","['matrices', 'optimization', 'permutation-matrices', 'discrete-optimization']"
2951813,Proof of the chainrule: is this proof correct and did I use the right notation?,"I created this proof of the chainrule. Being a (relative) beginner at math I have a few questions. Is the proof below correct? I was especially in doubt about the use of $h$ on both sides. Is the (Langrange?) notation correct this way? How to write the same proof using Leibniz's notation? I wrestled writing this proof in Leibniz notation, because what would in that case be the meaning of $dg$ ? Is it $g(x+h)-g(x)$ or $k$ or $h$ ? To be proved: If $f(u)$ is differentiable at $u=g(x)$ , and $g(x)$ is differentiable at $x$ then: $$f(g(x))'\stackrel{?}{=}f'(g(x))g'(x)$$ Or similarly $$\lim \limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{h}\stackrel{?}{=}\lim \limits_{k \to 0}\frac{f(g(x)+k)-f(g(x))}{k}\lim \limits_{h \to 0}\frac{g(x+h)-g(x)}{h}$$ Case 1: if $h$ has a value such that $g(x+h)=g(x)$ then: $$\lim \limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{h}=0$$ And $$\lim \limits_{k \to 0}\frac{f(g(x)+k)-f(g(x))}{k}\lim \limits_{h \to 0}\frac{g(x+h)-g(x)}{h}=0$$ Both sides of the equation to prove equal zero, therefore the equation holds in this case. Case 2: if $h$ has a value such that $g(x+h)\ne g(x)$ then: We multiply the lefthandside by $\frac{g(x+h)-g(x)}{g(x+h)-g(x)}$ $$\lim \limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{h}=\lim \limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{g(x+h)-g(x)}\lim \limits_{h \to 0}\frac{g(x+h)-g(x)}{h}$$ Taking $$u=g(x)$$ $$k=g(x+h)-g(x)$$ We get $$\lim \limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{h}=\lim \limits_{h \to 0}\frac{f(u+k)-f(u)}{k}\lim \limits_{h \to 0}\frac{g(x+h)-g(x)}{h}$$ And as $h\to 0, k\to 0$ , therefore $$\lim \limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{h}=\lim \limits_{k \to 0}\frac{f(u+k)-f(u)}{k}\lim \limits_{h \to 0}\frac{g(x+h)-g(x)}{h}$$ Thus $$f(g(x))'=f'(u)g'(x)=f'(g(x))g'(x) \tag*{$\blacksquare$}$$","['notation', 'calculus', 'proof-verification', 'chain-rule']"
2951883,Tuples with all coordinates summing to $3$,"Some $4$ -tuples of positive real numbers $(a_1,b_1,c_1,d_1),\dots,(a_n,b_n,c_n,d_n)$ are given, with $$\sum_{i=1}^na_i=\sum_{i=1}^nb_i=\sum_{i=1}^nc_i=\sum_{i=1}^nd_i=3.$$ It is known that there exists a partition of $N=\{1,\dots,n\}$ into three sets $N_1,N_2,N_3$ such that $$\sum_{N_1}a_i=\sum_{N_2}a_i=\sum_{N_3}a_i=1.$$ Analogous statements hold for $b,c,d$ . Is it always possible to partition $N$ into two sets $X,Y$ so that $$\sum_X a_i,\sum_Y b_i,\sum_Y c_i,\sum_Y d_i\geq 1?$$ Without the given condition, this is certainly not true, e.g. $n=1$ and the only tuple is $(3,3,3,3)$ . I've thought about putting elements from $N$ into $X$ and $Y$ one-by-one starting with one with the highest $a_i$ , then highest $b_i$ , highest $c_i$ , and so on, but that cannot give more than $3/4$ in the inequality.",['combinatorics']
2951901,Phase Portraits in 3D differential equations,"We know that when we have two equations and two variables, there are certain rules that make the phase portrait a saddle, node, etc... based on whether eigenvalues are positive or negative. For example, if one is positive and one is negative, the phase portrait is saddle. If both are negative, it is a sink node, etc... I was wondering if there are sets of similar rules with three variables instead of two. This would mean three eigenvalues and I was wondering if there are ways to construct rules with these three eigenvalues. Any input? Thanks.","['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
2951923,Cardinality of a set defined on the Cartesian product of a power set.,"$2^A$ is the power set of some finite set A. Let $R:= \{(B, C) \in 2^A \times 2^A | B \subseteq C\}$ . Show that $\lvert R\rvert = 3^{\lvert A\rvert}$ . It is the $B \subseteq C$ part in the definition of $R$ that I cannot understand nor its implications. $2^A \times 2^A$ would just be the Cartesian product. However, with the condition $B \subseteq C$ not all elements of the product would be included. I cannot visualize/articulate which would be, though.","['elementary-set-theory', 'combinatorics']"
2951929,Prove $\operatorname{Var}(X+Y) \le 2\operatorname{Var}(X) + 2\operatorname{Var}(Y)$ where $X$ and $Y$ are not necessarily independent,"I know how to show that $\operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X,Y)$ I was also given the hint that I should use the triangle inequality to get $|\operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X,Y)| \le |\operatorname{Var}(x)| + |\operatorname{Var}(Y)| + |2\operatorname{Cov}(X,Y)|$ Honestly, I have no idea where to go from here.","['statistics', 'covariance', 'variance']"
2951941,Number of words in mumbo-jumbo tribe's language,"A tribe named Mumbo-Jumbo has $3$ letters in their alphabet, how many words are there with length no greater than $4$ ? I answered $3^4+3^3+3^2+3^1$ . The author's answer is $40$ , did the author add up $3^3+3^2+3^1+3^0$ by accident?","['permutations', 'combinatorics', 'discrete-mathematics']"
2952014,Show that there does not exist a unique stationary distribution.,"So I was doing some self study and came across a proposition in one of my chemical engineering course's prescribed textbooks. I can't quite get the proof out. It's to do with a particle moving through a medium such that when it makes contact with to either of two plates $L$ units apart (i.e. one at $0$ and one at $L$ ), it remains there. Consider that the movement of a single particle follows a random walk which can be described by a Markov chain with states $[0, L]$ where $P(X_n = -1) = p_{-1}$ , $P(X_n = 0) = p_{0}$ and $P(X_n = 1) = p_{1}$ with $p_{-1} + p_{0} + p_{1} = 1$ . Show that if states $0$ and $L$ are completely absorbing, then there does not exist a stationary distribution. Hint: Start by considering ${\pi} = \pi P$ This makes sense intuitively since we have two recurrent classes $\{0\}$ and $\{L\}$ and one transient class $\{1, 2, ..., L - 2, L - 1\}$ . However, once I try and expand ${\pi} = \pi P$ , I don't know how to proceed next. Ideally I'd like a few more hints rather than an answer.","['markov-chains', 'chemistry', 'probability']"
2952017,Lipschitz continuous and differentiability,"Hey i'm trying to solve this question: Let U be an open interval in R and a ∈ U. Prove that if f : U → R is Lipschitz
continuous, then $g(x) = (f(x) − f(a))^2$ is differentiable at a. so far I have the following, but im not sure how to piece it all together $|f(x) - f(y)| \le k|x-y|$ => $ (f(x) − f(a))^2 \le (k|x-a|)^2$ if $g(x)$ is differentiable at a then $\lim_{x\to a} \frac{g(x) - g(a)}{x - a }$ must exist $\lim_{x\to a} \frac{g(x) - g(a)}{x - a }  = \lim_{x\to a} \frac{(f(x) − f(a))^2 - (f(a) − f(a))^2}{x - a } = \lim_{x\to a} \frac{(f(x) − f(a))^2}{x - a } \le \frac{(k|x-a|)^2}{x-a} = k^2(x-a)$ and from here im not sure where to go to show the limit exists","['derivatives', 'lipschitz-functions', 'real-analysis']"
2952073,Finding a matrix given its characteristic polynomial,"I am asked to find a $2 \times 2$ matrix with real and whole entries given it's characteristic polynomial: $$p^2 -5p +1.$$ This is what I have done thus far: I equated the polynomial to zero, and the roots (eigenvalues) were found to be $2.5 \pm \sqrt{21}/2$ I named the matrix to be solved $C$ , so $\det(C) =$ product of eigenvalues $= 1$ $\text{trace}(C) =$ sum of eigenvalues $=5$ I then tried to find C by solving $T^{-1} \times D \times T$ , where $D$ is a matrix whose diagonal entries are the eigenvalues solved above, and $T$ is any matrix who's determinant is non zero. I used $T$ as a $2 \times 2$ being $$\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}.$$ I solved $T^{-1} \times D \times T$ , and threw it in a calculator to make sure I made no algebraic mistakes, but the answer I received is wrong. I appreciate any help, thank you.","['matrices', 'linear-algebra', 'polynomials']"
2952169,Is $\cot x = \tan (π/2 - x) $ true for any angle $x$?,Is $$\cot x = \tan \Big(\frac{π}{2} - x\Big)$$ true even when $x$ is not an acute angle ?,['trigonometry']
2952217,Finding the function when the second derivative is defined in terms of it.,So I was playing around with derivatives and I realized that I could find a function when it's derivative was given in terms of it. But when I tried this with a second derivative I couldn't find the function. Is there any way to solve for $f(x)$ in this equation? $$ \frac {d^2f}{dx^2} = (f(x))^{-2}$$,"['functions', 'ordinary-differential-equations']"
2952222,Can identically distributed random variables $X$ and $Y$ have $P(X < Y) \geq p$?,"Question comes from Joe Blitzstein's ""Introduction to Probability"". Let $X$ denote days of the week, encoded as $1, 2, ..., 7$ with equal probabilities. Set $Y = (X + 1)$ mod $7.$ It is easy to see that $Y$ and $X$ are identically distributed. Moreover, $$P(X < Y) = 6/7$$ In general, let $X$ be a random variable with support $\{1, 2, 3, ..., N\},$ and let $Y = (X + 1)$ mod $N$ . Similarly to the argument before, $$P(X < Y) = (N-1)/N$$ The problem goes on to ask if it is possible to have $P(X < Y) = 1.$ My argument is that it is possible by $$\lim_{N \to \infty} (N-1)/N = 1$$ Question 1 : $X$ is uniformly distributed with each value having probability $1/N.$ Letting $N$ go to inifinity makes individual probabilities $0$ , so I am not sure if I can make the argument above. Question 2 : What if $X$ and $Y$ are i.i.d ? I think, if they are i.i.d. we can't make any statement of the form $P(X < Y) \geq p,$ since information about $X$ gives us information about $Y$ . For example, letting $p=0.9$ and observing $X=3$ would mean that $P(Y \geq 3) \geq 0.9$ . On the other hand, observing $X = 1$ would mean that $P(Y \geq 1) \geq 0.9$ , assigning less weight to the $Y \geq 3$ area. Can someone hint at a more rigorous proof?","['probability-distributions', 'probability-theory', 'probability']"
2952249,Probability that in one rolling of 5 dice we obtain two '6' and one '5'?,First I made a mistake of not taking into account that this event is dependent. So to get two ' $6$ ' the probability would be: $$ P_5(2) = \frac{5!}{2!(5-2)!} \cdot \left(\frac16\right)^2\cdot\left(\frac56\right)^3  $$ Where $p = \dfrac16$ $q = \dfrac56$ $N= 5$ (the number of elements of the system) Then there are $3$ dice left on the table and we want to know the probability that one of them is a ' $5$ '. We rule out the two dice with the ' $6$ ' face on. So the number of element in the system is now $3$ . So the probability would be: $$ P_3(1) = \frac{3!}{1!(3-1)!} \cdot \left(\frac16\right)^1\cdot\left(\frac56\right)^2  $$ But then the professor told me that $p=\dfrac15$ and not $p=\dfrac16$ That's what I don't get. Of course at the end we multiply those two probabilities to get the final probability that we want.,"['dice', 'probability']"
2952314,"If we have a family of squares such that the sum of their areas is infinite, then can we tile the plane with them?","Let $\{S_{i}\}_{i \in \mathbb{N}}$ be a family of squares such that the Sum of the areas of $S_{i}$ s is infinite. Can we tile the plane $\mathbb{R}^{2}$ with these squares? Note that you can freely move the squares in the plane but they should not overlap. let me rephrase my question, you are given $\{Ai\}_{i\in \mathbb{N}}$ a sequence of numbers, where each $A_{i}$ is positive such that $\sum_{i=1}^{\infty}A_{i}=\infty$ . Is there a tilling of the plane with the countable number of squares such that the area of the i'th square is $A_{i}$ ?","['combinatorics', 'geometry']"
2952316,Finding the volume of a pseudosphere that has been parametrised in $\theta$ and $t$,"I've got a problem calculating the volume of the top half of a pseudosphere. The pseudosphere is parametrised by $$\Phi(t,\theta) = \Big ( \frac{\cos(\theta)}{\cosh(t)}, \frac{\sin(\theta)}{\cosh(t)}, t-\tanh(t)\Big)$$ with $0\le t$ and $0\le \theta \lt 2\pi$ So the volume I'm trying to find is that between the $x-y$ axis and the surface of the inside of the pseudosphere. Now, we've been given that the formula for any region $V$ in $\mathbb R^3$ is $$\iiint_V r\tanh^2(t) \ \mathrm dr \ \mathrm d\theta \ \mathrm d t$$ based on the change of variables $x = r\cos(\theta) \ , \ y = r\sin(\theta) \ $ and $ z = t - \tanh(t)$ Since $t\ge 0$ we have $0 \lt r \le 1$ because when $t=0$ is put into $x = \frac{\cos(\theta)}{\cosh(t)}$ we get $x=\cos(\theta)$ and similarly when $t=0$ is put into y = $\frac{\sin(\theta)}{\cosh(t)}$ we get $y = \sin(\theta)$ Then, $$x^2 + y^2 = \cos^2(t) + \sin^2(t) = 1$$ so the radius at $t=0$ is $1$ , and as $t \to \infty$ we get $\cosh(t) \to \infty$ so both $x$ and $y$ approach $0$ so the radius approaches $0$ . So the integral becomes $$\lim_{b \to \infty}\int_{t=0}^{t=b} \int_{\theta=0}^{\theta = 2\pi} \int_{r=0}^{r=1} r\tanh^2(t) \ \mathrm dr \ \mathrm d\theta \ \mathrm d t$$ $$ = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} \tanh^2(t) \ \mathrm d t$$ $$ = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} 1-\text{sech}^2(t) \ \mathrm d t $$ $$ = \pi \lim_{b \to \infty} \Bigg [ t - \tanh(t)\Bigg]_{t=0}^{t=b}$$ which does not converge. I got close by multiplying the inside of the integral by $$\lvert \vec T_t \times \vec T_\theta \rvert = \frac{\sinh(t)}{\cosh^2(t)}$$ so the integral becomes $$\lim_{b \to \infty}\int_{t=0}^{t=b} \int_{\theta=0}^{\theta = 2\pi} \int_{r=0}^{r=1} r\tanh^2(t)\frac{\sinh(t)}{\cosh^2(t)} \ \mathrm dr \ \mathrm d\theta \ \mathrm d t$$ $$ = \lim_{b \to \infty}\int_{t=0}^{t=b} \int_{\theta=0}^{\theta = 2\pi} \int_{r=0}^{r=1} r\frac{\sinh^3(t)}{\cosh^4(t)} \ \mathrm dr \ \mathrm d\theta \ \mathrm d t$$ $$ = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} \frac{\sinh(t)\sinh^2(t)}{\cosh^4(t)}  \ \mathrm d t$$ $$ = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} \frac{\sinh(t)(\cosh^2(t)-1)}{\cosh^4(t)}  \ \mathrm d t$$ make a u-sub: $$u = \cosh(t) \implies \mathrm du = \sinh(t) \mathrm dt$$ with $ u(b) = \cosh(b)$ and $u(0) = 1$ so the integral becomes $$ \pi \lim_{b \to \infty}\int_{u=1}^{u=\cosh(b)} \frac{u^2-1}{u^4}  \ \mathrm d u $$ $$ = \pi \lim_{b \to \infty} \Bigg [ \frac{-1}{u} + \frac{1}{3u^3}\Bigg]_{u=1}^{u=\cosh(b)}$$ $$ = \pi \lim_{b \to \infty} \Bigg [ -\frac{1}{\cosh(b)} + \frac{1}{3\cosh^3(b)} +1 - \frac{1}{3}\Bigg ]$$ $$ = \frac{2\pi}{3}$$ Which I believe is double the correct answer of $\frac{\pi}{3}$ . Can anyone help if possible? Cheers heaps!","['surface-integrals', 'multivariable-calculus', 'multiple-integral', 'change-of-variable', 'parametrization']"
2952345,Prove a strong inequality $\sum_{k=1}^n\frac{k}{a_1+a_2+\cdots+a_k}\le\left(2-\frac{7\ln 2}{8\ln n}\right)\sum_{k=1}^n\frac 1{a_k}$,"For $a_i>0$ ( $i=1,2,\dots,n$ ), $n\ge 3$ , prove that $$\sum_{k=1}^n\frac{k}{a_1+a_2+\cdots+a_k}\le\left(2\color{red}{-\frac{7\ln 2}{8\ln n}}\right)\sum_{k=1}^n\frac 1{a_k}.$$ The case without $\color{red}{-\dfrac{7\ln 2}{8\ln n}}$ could be shown here . I have no idea how the $\color{red}{\text{red}}$ term comes from. Note: This question should not be closed although there was a duplicated question $4$ years ago (see here ). Duplicate of unanswered question suggests that if there is no accepted answer in the old question, the new question can stay open in the hope of attracting an answer. The question comes from the Chinese Mathematical Olympiad training team and there is no answer provided. Source: See Q.25 here (one of the official accounts that provides Chinese MO questions on January $23^{\rm rd}$ , $2018$ ) It has also appeared here (A blog from the person who set this question on December $17^{\rm th}$ , $2013$ ).","['contest-math', 'summation', 'logarithms', 'real-analysis', 'inequality']"
2952346,"Proving independence of $N(\mu,\sigma^2)$ sample mean $\bar X$ and variance $S^2$ by change of variables","Let $X_1,...,X_n$ are iid sample from $N(\mu,\sigma^2)$ . Then $\bar X$ and $S^2$ are independent. I was stuck on proving above statement. The joint PDF of $(X_1, ... ,X_n)$ is given by $$f(x_1,...,x_n)=\frac{1}{\sqrt {2\pi\sigma^2}}exp \bigg[-\frac{\sum_{i=1}^{n}(x_i-\mu)^2}{2\sigma^2}\bigg]$$ $$=\frac{1}{\sqrt {2\pi\sigma^2}}exp\biggl[-\frac{1}{2\sigma^2}\biggl\{\sum_{i=1}^{n}(x_i-\bar x_n)^2+n(\mu-\bar x_n)^2\biggl\}\biggl] $$ Now, consider the following transformation $y_i=\bar x_n$ and $ y_i=x_i-\bar x_n, i=2,3,...,n$ then $x_1-\bar x_n = -\sum_{i=1}^{n}(x_i-\bar x_n)=-\sum_{i=1}^{n}y_i$ Thus $\sum_{i=1}^{n}(x_i-\bar x_n)^2=\biggl(-\sum_{i=1}^{n}y_i\biggr)^2+\sum_{i=1}^{n}y_i^2$ The joint PDF of $y_1,...,y_n$ is given by $$f(y_1,...,y_n)=J\Biggl(\frac{1}{\sqrt {2\pi\sigma^2}}\Biggr)^n exp\Biggl[\frac{1}{2\sigma^2}\Biggl\{\Biggl(\sum_{i=1}^{n}y_i\Biggr)^2+\sum_{i=1}^{n}y_i^2+n(y_1-\mu)^2\Biggr\}\Biggr]$$ $$=g(y_2,..,y_n)h(y_1)$$ ,where $J$ denotes the Jacobian, $g(y_2,..,y_n)$ is joint PDF of $y2,...,y_n$ and $h(y_1)$ is marginal PDF of $Y_1$ I don't understand how the joint PDF of $y_1,...y_n$ could be broken into such two part. I guess $E(Y_1)=\mu, Var(y_1)=\sigma^2$ such that $Y_1$ follows $N(0,\sigma^2)$ . So, I guess rear part of exponential, $\frac{J}{\sqrt {2\pi\sigma^2}} exp\Biggl[\frac{n(y_1-\mu)^2}{2\sigma^2}\Biggl]$ , means $h$ . But, I'm not sure because of multiple of $n$ . Further, I don't know how $g$ could be derived from  that front part of the exponential. Please give me a hint!","['jacobian', 'statistics', 'independence', 'normal-distribution']"
2952395,A quick question about complex integrals and Cauchy's integral formula,"I'll spare the specifics for brevity's sake, but in essence the problem I'm posed is finding $$\int_C \frac{(z-1)^3 \cdot e^z \cdot cos(z)}{z}dz$$ along two different closed loops $C$ . Each is a rectangle, oriented clockwise. One of them encloses the discontinuity of this function (i.e. $z=0$ ) and another doesn't. Post-Script (December 2018): I recognize that the ""discontinuity"" mentioned is in reality a singularity. I'm mostly just wanting to double-check my approach to this since it's explicitly specified that ""this shouldn't take much computation,"" and want to double-check I'm on the right path. My thoughts on the matter: For $C$ being the closed loop not enclosing the discontinuity, the integral would be $0$ per Cauchy's integral theorem. For $C$ being the closed loop that encloses the discontinuity, the integral would be $2\pi i f(0)$ , from Cauchy's integral formula, where $f(z)$ is the numerator of the integrand (sans the $dz$ of course), and "" $0$ "" coming from being the point of discontinuity. I have a rough intuition for why this might be - it's fairly heuristic and informal though - so I just wanted to make sure I was on the right track. Thanks in advance.","['complex-analysis', 'complex-integration']"
2952411,P value for a z-score of 4.9? Or am I doing this wrong?,"My question is as follows: A fair die is rolled $120$ times. Find the probability that $5$ is on the 
top: a. between $30$ and $40$ times, b. between $18$ and $50$ times, c. more than $70$ times. Hint: Use the approximation of the Binomial distribution to the normal distribution. I am close to solving it. However, I am stuck, since I obtained a standard deviation value of $4.0825$ and therefore one of the $z$ -scores as $4.9$ . My solution is as follows: $x$ = event of having a $5$ shown on top $P(x) = 1/6$ This is a binomial probability experiment $np(1-p) \ge 10$ ? $120 * \frac{1}{6} ( 1 - \frac{1}{6}) = \frac{50}{3} > 10$ mean $= np = 120$ ; standard deviation $= \sqrt{\frac{50}{3}} = 4.0825.$ $X~N(\mu=20, \sigma=4.0825)$ a) $P(30\le x\le 40) =$ ? $z = \frac{x - \mu} {\sigma} $ $P\left(\frac{30 - 20}{4.0825} \le z \le \frac{40 - 20}{4.0825} \right)= P (2.45\le z\le4.9)$ At this point, I am stuck, since the $p$ -value for a $Z$ -score of $4.9$ is impossible to find (an online calculator gave me a value of $0.9999995$ ). Is my solution wrong or should I proceed with the given $p$ -value? Thank you all. : )","['approximation', 'binomial-distribution', 'normal-distribution', 'probability']"
2952440,Roadmap to Iwasawa Theory,"I haven’t found any posts on this, so I figured I would ask. Beyond learning basic algebra (rings, groups, fields) and complex analysis, what must one study if they want to start learning a good amount of iwasawa theory? In what sequence should they study this material?","['number-theory', 'p-adic-number-theory', 'l-functions', 'algebraic-number-theory']"
2952462,Finding the leading digit(s) in any number,"I am somewhat perplexed by this (presumably very simple) issue. Simply, I am trying to find the leading digit(s) in any number (related question ). Here is some nice python code (from this question ): import math

def first_n_digits(num, n):
    # where n is the number of leading digits of interest
    return num // 10 ** (int(math.log(num, 10)) - n + 1) This works great for most numbers, for example: >>> first_n_digits(123456, 1)
1
>>> first_n_digits(123456, 2)
12
>>> first_n_digits(123456, 3)
123
>>> first_n_digits(123456, 4)
1234
>>> first_n_digits(123456, 5)
12345
>>> first_n_digits(123456, 6)
123456 The problem is this does not work for num=1000 and n=1 . first_n_digits(1000, 1)
10 This issue does not appear when num = 10,100,10000,100000 . Just num = 1000 . How can one deal with this edge case? I have not found any others so far. I expect there to be some very simple solution which I have overlooked.","['number-theory', 'elementary-number-theory']"
2952476,$\cos(\pi/6 *2\pi)$ in radians isn't equal to $\cos(30*360)$ in degrees why?,"I put $\cos(\pi/6*2\pi)$ in radians mode on my calculator and i get $-0.9890273166$ and then I put $\cos(30*360)$ in degrees mode and get $1$ , the answer is the complete opposite of what it should be and in case if you don't know $\pi/6*2\pi$ converted to degrees equals $30*360$ , so I really need an explanation. Thanks.","['trigonometry', 'calculator']"
2952498,What is a continuous random variable? A Collection of definitions,"Although this is a question about what's a continuous random variable, it seems that there are at least 2 definitions being used. The Distribution function is continuous. There exists a non-negative function $f$ such that $F(x)=\int_{-\infty}^x f(s) \ ds$ I'm interested in understanding the consequences (limitations/advantages) for using each one, and maybe someone knows other definitions and add them together with an explanation of their consequences.","['definition', 'probability-theory', 'probability', 'random-variables']"
2952507,Inner product of weakly convergent sequences in Hilbert space,"Let $(H, \langle \cdot, \cdot \rangle)$ be a Hilbert space, and let $(u_n), (v_n)$ be two weakly convergent sequences in $H$ with limits $u$ and $v$ respectively, i.e. $\langle u_n, y \rangle \to \langle u, y \rangle, \langle v_n, y \rangle \to \langle v, y \rangle \quad \forall y \in H$ for $n \to \infty$ . I now want to show that $\langle u_n, v_n \rangle \overset{n \to \infty}\longrightarrow \langle u, v \rangle$ . Now my initial thought was to add in a zero and use the inner product properties, i.e. $$	|\langle u, v \rangle | - \langle u_n, v_n \rangle | =  | \langle u, v \rangle - \langle u, v_n \rangle + \langle u, v_n \rangle - \langle u_n, v_n \rangle | \\
	\leq  |\langle u, v - v_n \rangle | + | \langle u - u_n, v_n \rangle| \\$$ Now the left term in this equation should converge to $0$ for $n \to \infty$ because of the weak convergence of $v_n$ ; with the right term however, I'm not sure what to do since the $n$ still appears on both sides. I suppose I now want to show that $|\langle u - u_n, v_n \rangle| \overset{n \to \infty}\longrightarrow 0$ aswell to complete the proof, but I'm not sure how to do that. Since we're in a Hilbert space here, I thought about using the Riesz representation theorem somehow, but I'm not sure how it would help me.","['hilbert-spaces', 'weak-convergence', 'functional-analysis', 'analysis']"
2952516,Limit of $\lim_{t\to 1^+} \int_t^{t^2} \frac{\arctan(x)}{x-1} dx$,"I need to evaluate $$\lim_{t\to 1^+} \int_t^{t^2} \frac{\arctan(x)}{x-1} \; dx.$$ I have tried the partial integrating, and I got confused. Could someone help me, please?","['integration', 'limits']"
2952556,"Translate ""A is not necessary for B"" in math expression","I tried to to solve this but what I found is A is not necessary for B I could be wrong = not(A is necessary for B)
= not(not(B) or A)
= not(A) and B but it doesn't make sense.
Let's take an example: A = understand things
B = argue about things
A is not necessary to B = not(A) and B so Understand things is not necessary to argue about things = Not understand things and argue about things should be the same thing.
I really appreciate any kind of help.","['logic', 'discrete-mathematics']"
2952564,Roots of polynomial equation $x^6+2x^5+4x^4+8x^3+16x^2+32x+64=0$,"If $x_1,x_2,...,x_6$ be the roots of $x^6+2x^5+4x^4+8x^3+16x^2+32x+64=0$ then I have to show that $|x_j|=2\space\space\space\forall j\in\{1,2,3,4,5,6\}$ I get that the roots of the equation must be complex, of the form $a+ib$ where $|a+ib|=\sqrt{a^2+b^2}=2$ for all $|x_i|$ . I don't get how to find the value of $a+ib$ . How do I find the roots?","['algebra-precalculus', 'roots', 'polynomials', 'complex-numbers']"
2952570,Prove $\int_{0}^{\infty}\frac{|\sin x|\sin x}{x}dx=1$,"Prove $$\int_{0}^{\infty}\frac{|\sin x|\sin x}{x}dx=1.$$ I know how to calculate $\int_{0}^{\infty}\frac{\sin x}{x}dx=\frac{\pi}{2}$ , but the method cannot be applied here. So I am thinking $$\sum_{k=0}^n(-1)^k\int_{k\pi}^{(k+1)\pi}\frac{\sin^2 x}{x}dx$$ but I don't know how to proceed.","['integration', 'definite-integrals']"
2952587,"Randomly permute $\{1,\cdots,100\}$. What is the probability that none of the $S_k$'s defined by $\sigma(1)+\cdots+\sigma(k)$ is divisible by $3$?","After randomly permuting the numbers from $1$ to $100$ , what is the
  probability that none of the $S_k$ 's defined by $S_k =\sigma(1)+\cdots+\sigma(k)$ is divisible by $3$ ? I think I have a solution. Here is my proposed solution: Consider all the numbers from $1$ to $100$ modulo $3$ . We have $33$ zeroes, $34$ ones and $33$ twos. Now let's see which permutations are eligible: First of all, it does not matter where those numbers divisible by $3$ are placed after permutation because they are zero modulo $3$ . I claim there exists only one possible pattern: $$1 \hspace{5px} \underbrace{1\hspace{5px} 2\hspace{5px} 1\hspace{5px} 2\hspace{5px} 1\hspace{5px} 2\hspace{5px} 1\hspace{5px} \cdots \hspace{5px} 1 \hspace{5px}2}_{\text{33 times}}$$ where the remaining zeroes can be put anywhere. Why is this the only pattern? Well, notice that after we have chosen $\sigma(1)$ , no two consecutive sigmas can be $1$ modulo $3$ . Because if $S_k\equiv x \pmod{3}$ , then $S_{k+1} \equiv x+1 \pmod{3}$ and $S_{k+2} \equiv x+2 \pmod{3}$ . One of these three must be divisible by $3$ . Hence, no consecutive $1$ 's can be in the list after the first entry. The same argument applies to any two consecutive sigmas that are equal to $2$ modulo $3$ . So, after we have chosen $\sigma(1)$ , the rest of the list should be filled with alternating $1$ 's and $2$ 's with $0$ 's placed arbitrarily. So, we get two options: $$\text{Pattern I: } \hspace{5px} 1 \hspace{5px}1\hspace{5px} 2\hspace{5px} 1\hspace{5px} 2\hspace{5px} 1\hspace{5px} 2\hspace{5px} 1\hspace{5px} \cdots$$ $$\text{Pattern II: } \hspace{5px} 2 \hspace{5px}2\hspace{5px} 1\hspace{5px} 2\hspace{5px} 1\hspace{5px} 2\hspace{5px} 1\hspace{5px} 2\hspace{5px} \cdots$$ The second pattern is impossible. Because the numbers of $1$ 's and $2$ 's are limited and we cannot fit $34$ ones and $33$ twos in the second pattern. Hence, we only need to count the number of ways we can place zeroes in the only remaining pattern (pattern I) to find out the probability. The final answer therefore is equal to: $$\frac{{67+33-1}\choose{33}}{{100 \choose 33} \times {67 \choose 34}}=\frac{{99}\choose{33}}{{100 \choose 33} \times {67 \choose 34}} \approx 4.7\times 10^{-20}$$ So, the answer is extremely small and the probability is almost $0$ .","['permutations', 'contest-math', 'proof-verification', 'stochastic-processes', 'probability']"
2952649,Do the roots of $1+x/1!+x^2/2!+\cdots+x^{2n+1}/{(2n+1)!}$ decrease to $-\infty$? [duplicate],"This question already has answers here : Complex zeros of the polynomials $\sum_{k=0}^{n} z^k/k!$, inside balls (3 answers) Closed 5 years ago . Do the roots of $$P_{2n+1}(x)=1+\frac x{1!}+\frac{x^2}{2!}+\cdots+\frac{x^{2n+1}}{(2n+1)!}$$ decrease to $-\infty$ ? Can we show this? Indeed, $P_{2n+1}(0)=1$ , $P_{2n+1}(-(2n+1))<0$ . And $P_{2n+1}'(x)=P_{2n}(x)>0$ . So $P_{2n+1}$ has only one root $x_n$ . Can we show that $x_n\to-\infty$ ?","['limits', 'calculus']"
2952719,On convergence of sums of the form $\sum_{n=1}^{\infty}\frac{1}{n^{1+f(n)}}$,"The p-series convergence test is a classic and well-known result for sums of the form $\sum_{n=1}^{\infty}\frac{1}{n^p}$ for a real number $p$ . It is known that $\sum_{n=1}^{\infty}\frac{1}{n}$ diverges, but for every $\epsilon>0$ , $\sum_{n=1}^{\infty}\frac{1}{n^{1+\epsilon}}$ converges. It can be shown that series with terms asymptotically smaller than this will also converge, such as $$\sum_{n=2}^{\infty}\frac{1}{n\log^2n}\text{ and even }\sum_{n=2}^{\infty}\frac{1}{n\log^{1+\epsilon}n}\text{ for }\epsilon>0$$ I was introduced to a related series by a coworker of mine, which is the following: $$\sum_{n=1}^{\infty}\frac{1}{n^{1+\sin n}}$$ Supposedly, he was able to prove that this diverged. A natural generalization is to look at series of the form $$\sum_{n=1}^{\infty}\frac{1}{n^{c+\sin n}}$$ for some $c>0$ . It is not hard to show that the series diverges when $c\leq0$ and converges when $c\geq2$ . What I want is to find the smallest value of $c$ such that the series converges, or a tight lower bound. Formally, I want to find $$\inf\left\{c\,:\,\sum_{n=1}^{\infty}\frac{1}{n^{c+\sin n}}<\infty\right\}$$ Any progress on finding this number is appreciated. I would assume that it is greater than 1, but I haven't been able to prove much else.","['divergent-series', 'calculus', 'sequences-and-series', 'real-analysis']"
2952721,What is the probability that a red ball will be selected?,"What is the probability that a red ball will be selected? Suppose there are two jars, $A,B$ $A$ has $2$ red, $4$ green $B$ has $3$ red, $5$ green An urn is selected at random, giving each of the urns a probability of $1/2$ A random urn is selected, and one ball is selected from that urn. What is the probability that a $G$ ball is selected. I can use a tree diagram, in which my answer is very clear: We can see that the probability that a $R$ ball is selected is $(1/2)(2/6)+(1/2)(3/8)=17/48$ The way my textbook does it is using the theorem of total probability (conditional version). That is, $P(R)=P(R|A)P(A)+P(R|B)P(B)$ Thinking on pure intution, $P(R|A)$ is asking me ""what is the probability that you selected a red ball, knowing that you already selected A"". Well thats just $2/6$ . I can basically just look at everything after the $A$ in the diagram. So our equation ends up becoming the same as the thereom of probability. Here is where my question is: Using the formula, $P(R|A)=\dfrac{P(R\cap A)}{P(A)}$ But What is $P(R\cap A)$ ? If $P(A) = 1/2$ , then surely the numerator must be $1/6$ , since our intuitive approach told us that this conditional probability is $2/6$ . But I don't understand where this $1/6$ comes from. I can see that this is probably just $(1/2)(2/6)$ (basically we just multiply the entire branch), but why does this work? I know that you can multiple the probabilities of two independent events , but how is this independent? Selecting box A effected the number of red balls we had.","['statistics', 'probability-theory', 'probability']"
2952769,Integral equations and the Fredholm alternative / theory,"The Fredholm alternative states that either: $$ 0 = \lambda \phi(x) - \int_a^b K(x,y) \phi(y) dy $$ has a non-trivial solution, or: $$ f(x) = \lambda \phi(x) - \int_a^b K(x,y) \phi(y) dy $$ always has a unique solution for any f(x) A sufficient condition is for the kernel K to be square-integrable, but depending on sources there is some confusion whether $\lambda$ must be a non-zero complex number.  For example, Wikipedia says so but also refers to this page which doesn't... (Note that $\lambda=0$ the integral equation is called Fredholm of the first kind, and $\lambda \neq 0$ is of second kind.) Adding to the confusion, some authors prefer to have $\lambda$ scaling the integral rather than $\phi(x)$ , in which case it makes sense to require it to be nonzero. Could anyone clarify whether the Fredholm alternative is only true of second kind equations (i.e. $\lambda \neq 0$ )?  And if so, why...? Thanks!","['hilbert-spaces', 'integral-equations', 'functional-analysis']"
2952786,Factor $10^6-1$ completely,I know kind of a very elementary method to factor this number. Consider the following: $$10^6-1 = (10^3-1)(10^3+1)=9 \times 11 \times (10^2+10+1)(10^2-10+1) = 9 \times 11 \times 111\times 91$$ I would then factor each number individually. Is there a faster method? The great hint is that this number is a rep unit number = $9\ \times 111111$ .,"['exponentiation', 'divisibility', 'number-theory', 'elementary-number-theory', 'factoring']"
2952794,What are classifying spaces of algebraic categories like?,"Let $\mathcal{C}$ be a category. Recall that the nerve $N(\mathcal{C})$ of $\mathcal{C}$ is the simplicial set obtained by defining $N(\mathcal{C})_k$ to be the set of $k$ -tuples of composable morphisms in $\mathcal{C}$ , together with the obvious face and degeneracy maps. To any simplicial set we can associate a CW complex by taking its geometric realization . Intuitively, this means we replace every element of $N(\mathcal{C})_k$ by a $k$ -dimensional triangle (the case $k = 0$ being a point), and gluing them according to the face maps. In particular, we may apply this procedure to the nerve of a category. I believe the resulting space is sometimes called the classifying space of your category. In some sense, one can think of the space as a tangible and geometric way to represent your category, especially if the category we care about is small. This leads me to the following question. Question. What does the classifying space of a category $\mathcal{C}$ look like when $\mathcal{C}$ is an (essentially small) algebraic category? Is it interesting to look at? Can we describe its algebraic invariants, such as the fundamental group or homology groups? For the sake of concreteness, let us specifically focus on the category of all finitely generated groups. But feel free to think about other cases, like the category of finite commutative rings. More geometrically minded people might want to think about the category of finite-type $S$ -schemes or the category of smooth manifolds. Note. The aforementioned categories are not small, though they are essentially small. Of course, one can simply take the skeleton of the essentially small category to obtain a small one, so that the resulting classifying spaces are actual sets. What I tried. A simpler example would be to take the category of all finitely generated abelian groups, which has the advantage that they admit a nice classification. Writing down the definition of this 'space' is not too complicated, but I did not manage to find a way to simplify the construction, let alone think about more complicated categories like the ones described above.","['category-theory', 'algebraic-geometry', 'abstract-algebra', 'algebraic-topology', 'simplicial-stuff']"
2952820,Calculating the $\sum_{x=1}^\infty\left[\prod_{k=0}^s{s\choose k}x^k\right]^{-1}$,"Question: If $s \in \mathbb{N}$ is it true: $\sum_{x=1}^\infty\left[\prod_{k=0}^s{s\choose k}x^k\right]^{-1}={\zeta\left[{s+1 \choose 2}\right] \above 1.5pt \prod_{k=1}^s{s \choose k}};$ where $\zeta\left[{s+1 \choose 2}\right]$ is the Riemann zeta function $^1$ evaluated at the triangular numbers $^2$ ? I believe I can show the answer to the question is yes by way of inspection. I cannot get an actual explicit calculation that answers the question affirmatively - that is what I am asking for ! Solution to question by inspection: For short handedness I write $$\zeta_{\times}(s)=\sum_{x=1}^\infty\Bigg[\prod_{k=0}^s{s\choose k}x^k\Bigg]^{-1}$$ I computed a small list of values of $\zeta_{\times}(s)$ for small $s$ : \begin{array}{ l | l }
s & 2 & 3 & 4 & 5 & 6 & 7\\ \hline
\zeta_{\times}(s) & \frac{\zeta(3)}{2} & \frac{\zeta(6)}{9} & \frac{\zeta(10)}{96} & \frac{\zeta(15)}{2500} & \frac{\zeta(21)}{162000}& \frac{\zeta(28)}{26471025}\\ 
\end{array} Inspection suggests that $\zeta_{\times}(s)$ has numerator equal to the Riemann zeta function at ${s+1 \choose 2}.$ The denominators of $\zeta_{\times}(s)$ can be written explicitly as $2,9,96,2500,162000,\ldots$ ; which appear to be the integer sequence A001142 $^3$ . Listed in the reference section is the following paper by Lagarias: Products of Binomial Coefficients and Unreduced Farey Fractions . $^4$ . Lagarias defines the unreduced Farey fractions $^5$ to be the ordered sequence of all reduced and unreduced fractions between $0$ and $1$ with denominator of size at most $s.$ Following Lagarias' notation: I write $G_s$ for the set of unreduced Farey fractions and let $|G_s|$ and $G_s^*$ denote the cardinality and the product of all elements of $G_s$ respectively. Lagarias shows the following results: $|G_s|={s+1 \choose 2}$ and $G_s^*=\prod_{k=1}^s{s \choose k}.$ I put everything together and get $$\zeta_{\times}(s)={\zeta\big(|G_s|\big) \above 1.5pt G_s^*}$$ and I am done. $\blacksquare$ Source and motivation of the problem: Curiosity is the main driving source of the problem. Here it goes: If $x$ and $s$ are positive numbers I write $\zeta(s)$ for the Riemann zeta function evaluated at the number $s.$ For ease of typing  I write Pascal's Triangle $^{6}$ like this: $$\text{ }\begin{matrix}
1&&&&\\
1&1&&&\\
\color{red}{1}&\color{red}{2}&\color{red}{1}&&\\
\color{blue}{1}&\color{blue}{3}&\color{blue}{3}&\color{blue}{1}&\\
1&4&6&4&1\\
\vdots
\end{matrix}$$ and recall that the first row in Pascal's triangle is numbered at $0$ .  Now look at the second and third rows of the triangle, highlighted in red and blue respectively and observe: $\sum_{x=0}^\infty{ 1 \above 1.5pt \color{red}{1}+\color{red}{2}x+\color{red}{1}x^2}=\zeta(2)$ and $\sum_{x=0}^\infty{ 1 \above 1.5pt \color{blue}{1}+\color{blue}{3}x+\color{blue}{3}x^2+\color{blue}{1}x^3}=\zeta(3).$ This observation shows that $\zeta(s)$ can be recovered from the rows of Pascal's triangle - in particular I can write $\sum_{x=0}^\infty\frac{1}{(1+x)^s}=\zeta(s).$ On the other hand using the binomial theorem $^{7}$ allows me to compute $(1+n)^s$ explicitly with the formula: $\sum_{k=0}^s{s\choose k}x^k$ in which case after substitution: $$\zeta(s)=\sum_{x=0}^\infty\bigg[\sum_{k=0}^s{s\choose k}x^k\bigg]^{-1}$$ Now out of total curiosity I decided to swap the inner summation inside the big brackets and swap it out with a product, noting carefully that if I do that I need to shift the starting point in the outer summation to $1$ otherwise I would be dividing by zero. For short handedness I wrote $$\zeta_{\times}(s)=\sum_{x=1}^\infty\Bigg[\prod_{k=0}^s{s\choose k}x^k\Bigg]^{-1}$$ Numerical inspection suggested $$\zeta_{\times}(s) ={\zeta\left[{s+1 \choose 2}\right] \above 1.5pt \prod_{k=1}^s{s \choose k}}$$ I double checked my numerical hunch against Sloan's Database and encountered the paper by Lagarias. In Lieu of the fact that Lagarias has an explicit formulae for $\log G_s^*$ (see Lagarias paper or reference above for notation) I primarily became interested in computing $\log \zeta_{\times}(s).$ If I could answer the question affirmatively then I would know that $$\log \zeta_{\times}(s)=\log \zeta(|G_s|)-\log(G_s^*)$$","['riemann-zeta', 'conjectures', 'binomial-coefficients', 'sequences-and-series']"
2952832,Supremem of Uniformly Integrable Sequences,"I'm doing an exercise about uniformly integrable random variables: Suppose $(\xi_n)$ is a sequence of uniformly integrable random variables, then \begin{equation}
\lim_{n\rightarrow\infty}\mathbb{E}\bigg[\frac{1}{n}\sup_{1\leq k\leq n}|\xi_k|\bigg]=0.
\end{equation} I try to apply the definition and some other basic properties of uniformly integrable random variables, but it seems like I need more tricks. Any hint would be appreciated.","['uniform-integrability', 'probability-theory', 'real-analysis']"
2952847,Does the sum of two linear operators with real spectra have real spectrum?,"Suppose $C=A+B$ , where $A$ and $B$ are linear operators defined on an infinite dimensional Hilbert space with real spectra, but they are not necessarily self adjoint operators. Is it true that $C$ always have real spectrum? Any comment or reference will be greatly appreciated.",['functional-analysis']
2952857,Gauge transformation of differential equations.,"It is well known that linear ordinary differential equations (ODEs) can be mapped onto each other by an appropriate change of variables. This fact can be than used to find solutions of a given ODE (target ODE) as appropriately rescaled solutions of a different ODE (input ODE).
There are basically three types of transformations that one can apply. A change of abscissa $x \rightarrow \theta(x)$ and $d/d x \rightarrow 1/\theta^{'}(x) d/d x$ , A change of ordinate $y(x) \rightarrow m(x) y(x)$ , A gauge transformation $y(x) \rightarrow r_0(x) y(x) + r_1(x) y^{'}(x)$ . See [1] for a more detailed discussion of those notions. In here we focused on the last possibility and found the following result. Let $f(x)$ be a solution of the following ODE(the input ODE): \begin{equation}
f^{''}(x) + Q(x) f(x)=0
\end{equation} Now define \begin{equation}
g(x) := f(x) + \frac{1}{\int Q(x) dx} \cdot f^{'}(x)
\end{equation} then the function $g(x)$ satisfies the following ODE(the target ODE): \begin{equation}
g^{''}(x) + \left( \frac{Q'(x)}{\int Q(x) \, dx}+Q(x)-\frac{2 Q(x)^2}{(\int Q(x) \, dx)^2}\right) g(x)=0
\end{equation} Likewise define: \begin{equation}
h(x) := \left(\frac{f(x)}{x_0-x} +  f^{'}(x)\right)\cdot \frac{1}{\sqrt{Q(x)}}
\end{equation} then the function $h(x)$ satisfies the following ODE(the target ODE): \begin{equation}
h^{''}(x) + \left(-\frac{3 Q'(x)^2}{4 Q(x)^2}+\frac{(x-x_0) Q''(x)-2 Q'(x)}{2 Q(x) (x-x_0)}+Q(x)-\frac{2}{(x-x_0)^2}\right) h(x)=0
\end{equation} Finally define \begin{equation}
h_1(x) := \left(f(x) + \frac{\imath}{\sqrt{Q(x)}} \cdot f^{'}(x)\right) 
\cdot \frac{Q(x)^{3/4}}{\sqrt{Q^{'}(x)}}
\end{equation} then the function $h_1(x)$ satisfies the following ODE(the target ODE): \begin{equation}
h_1^{''}(x) + \left( \frac{3 Q'(x)^2}{16 Q(x)^2}+\frac{3 i Q'(x)}{2 \sqrt{Q(x)}}-\frac{i \sqrt{Q(x)} Q''(x)}{Q'(x)}+\frac{2 Q^{(3)}(x) Q'(x)-3 Q''(x)^2}{4 Q'(x)^2}+Q(x)\right) h_1(x)=0
\end{equation} As usual we verify those results with the help of Mathematica. We have: In[433]:= Clear[Q]; Clear[g]; Clear[f]; x =.; x0 =.;
g[x_] := f[x] + 1/Integrate[Q[x], x] f'[x];
Simplify[(g''[
     x] + (Q[x] - (2 Q[x]^2)/(\[Integral]Q[x] \[DifferentialD]x)^2 + 
       Derivative[1][Q][x]/\[Integral]Q[x] \[DifferentialD]x) g[
      x]) /. { Derivative[2][f][x] :> -Q[x] f[x], 
   Derivative[3][f][x] :> -Q'[x] f[x] - Q[x] f'[x]}]
Clear[Q]; Clear[g]; Clear[f];
g[x_] := (f[x]/(x0 - x) + f'[x])/Sqrt[Q[x]];
Simplify[(g''[
     x] + (Q[x] - 2/(x - x0)^2 - (3 Derivative[1][Q][x]^2)/(
       4 Q[x]^2) + (-2 Derivative[1][Q][x] + (x - x0) (
           Q^\[Prime]\[Prime])[x])/(2 (x - x0) Q[x])) g[x]) /. { 
   Derivative[2][f][x] :> -Q[x] f[x], 
   Derivative[3][f][x] :> -Q'[x] f[x] - Q[x] f'[x]}]
Clear[Q]; Clear[g]; Clear[f];
g[x_] := (f[x] + I/Sqrt[Q[x]] f'[x])/(Sqrt[Derivative[1][Q][x]]/Q[x]^(
    3/4));
Simplify[(g''[
     x] + (Q[x] + (3 I Derivative[1][Q][x])/(2 Sqrt[Q[x]]) + (
       3 Derivative[1][Q][x]^2)/(16 Q[x]^2) - (
       I Sqrt[Q[x]] (Q^\[Prime]\[Prime])[x])/
       Derivative[1][Q][x] + (-3 (Q^\[Prime]\[Prime])[x]^2 + 
        2 Derivative[1][Q][x] 
\!\(\*SuperscriptBox[\(Q\), 
TagBox[
RowBox[{""("", ""3"", "")""}],
Derivative],
MultilineFunction->None]\)[x])/(4 Derivative[1][Q][x]^2)) g[x]) /. { 
   Derivative[2][f][x] :> -Q[x] f[x], 
   Derivative[3][f][x] :> -Q'[x] f[x] - Q[x] f'[x]}]

Out[435]= 0

Out[438]= 0

Out[441]= 0 Having said all this my question would be firstly are those results known and if yes what other possible gauge transformations can we come up with that lead to relatively simple target ODEs. Update: The result above is actually a special case of a more generic result. Let $f(x)$ satisfy the ODE as above. Now define \begin{equation}
g(x) := \frac{f(x) + r_1(x) \cdot f^{'}(x)}{\sqrt{1+Q(x) r_1(x)^2 + r_1^{'}(x)}}
\end{equation} Then the function $g(x)$ satisfies the following ODE: \begin{equation}
g^{''}(x) + \frac{P(x)}{4\left( 1+Q(x) r_1(x)^2 + r_1^{'}(x)\right)^2} \cdot g(x)=0
\end{equation} where \begin{eqnarray}
&&P(x):=\\
&&4 r_1(x) Q'(x) \left(3 r_1'(x)^2+4 r_1'(x)+1\right)+\\
&&-3 r_1(x)^4 Q'(x)^2+2 r_1(x)^2 \left(Q''(x) \left(r_1'(x)+1\right)-3 Q'(x) r_1''(x)\right)+\\
&&2 Q(x) \left(r_1(x)^4 Q''(x)+2
   r_1(x)^3 Q'(x)+r_1^{(3)}(x) r_1(x)^2+6 r_1'(x)^3+12 r_1'(x)^2+8 r_1'(x)-6 r_1(x) r_1'(x) r_1''(x)+2\right)+\\
&&8 Q(x)^2 r_1(x)^2 \left(2 r_1'(x)+1\right)+4
   Q(x)^3 r_1(x)^4+\\
&&2 r_1^{(3)}(x)-3 r_1''(x)^2+2 r_1^{(3)}(x) r_1'(x)
\end{eqnarray} Now if we take firstly $r_1^{'}(x) + Q(x) r_1(x)^2=0$ and secondly $r_1^{'}(x) + 1=0$ and thirdly $1+Q(x) r_1(x)^2=0$ then we get the first, the second and the third case respectively. Now let us look at some particular cases. Firstly we can also take $Q(x)=0$ then we immediately get the following interesting result:
The ODE : \begin{eqnarray}
g^{''}(x) + \frac{2 r_1^{(3)}(x)-3 r_1''(x)^2+2 r_1^{(3)}(x) r_1'(x)}{4\left( 1 + r_1^{'}(x)\right)^2} \cdot g(x)=0
\end{eqnarray} is solved by \begin{equation}
g(x) = \frac{C_1+C_2(x+r_1(x))}{\sqrt{1+r_1^{'}(x)}}
\end{equation} Note that the result above can still be simplified by defining $u(x) := r_1^{''}(x)/(1+r^{'}(x))$ . Then we have the following ODE: \begin{eqnarray}
g^{''}(x) + \left( 1/2 u^{'}(x) - 1/4 u(x)^2\right) \cdot g(x)=0
\end{eqnarray} which is solved by: \begin{equation}
g(x) = \frac{C_1+C_2\int \exp(\int u(x) dx) dx}{\sqrt{\exp(\int u(x) dx)}}
\end{equation} In[460]:= FullSimplify[(D[#, {x, 
       2}] + (1/2 u'[x] - 1/4 u[x]^2) #) & /@ {(C[1] + 
      C[2] (Integrate[Exp[Integrate[u[x], x]], x]))/
    Sqrt[Exp[Integrate[u[x], x]]]}]

Out[460]= {0} Secondly, we can take : \begin{eqnarray}
Q(x)&=& \frac{B}{x^{2+n}}\\
r_1(x)&=& A x^{n+1}
\end{eqnarray} Then define: \begin{eqnarray}
{\mathfrak A}_0 &=&4 B\\
{\mathfrak A}_1 &=&4 A B (2 A B+3 n+2)\\
{\mathfrak A}_2&=&2 A \left(2 A^3 B^3+2 A^2 B^2 (3 n+2)+A B \left(5 n^2+5 n+2\right)+n \left(n^2-1\right)\right)\\
{\mathfrak A}_3&=&-A^2 n (n+2) (A B+n+1)^2
\end{eqnarray} Then we have that the ODE: \begin{eqnarray}
g^{''}(x) + \left( \frac{{\mathfrak A_0} + {\mathfrak A_1} x^n + {\mathfrak A_2} x^{2 n} + {\mathfrak A_3} x^{3 n}}{4 x^{n+2} \left(A x^n (A B+n+1)+1\right)^2}\right) \cdot g(x)=0
\end{eqnarray} is solved by: \begin{eqnarray}
g(x) = C_+ \frac{y_+(x) + A x^{n+1} y_+^{'}(x)}{\sqrt{1+A(1+n+A B)x^n}} + C_- \frac{y_-(x) + A x^{n+1} y_-^{'}(x)}{\sqrt{1+A(1+n+A B)x^n}}
\end{eqnarray} where \begin{equation}
y_\pm(x)= \sqrt{x} J_{\pm\frac{1}{n}}\left(-2\frac{\sqrt{B}}{n} x^{-n/2} \right)
\end{equation} In[162]:= A =.; B =.; n =.; x =.; Clear[y]; Clear[g];
y1[x_] = Sqrt[x] BesselJ[1/n, -2 Sqrt[B]/n x^(-n/2)];
y2[x_] = Sqrt[x] BesselJ[-1/n, -2 Sqrt[B]/n x^(-n/2)];

eX = (D[#, {x, 2}] + ((
        4 B + 4 A B (2 + 2 A B + 3 n) x^n + 
         2 A (2 A^3 B^3 + 2 A^2 B^2 (2 + 3 n) + n (-1 + n^2) + 
            A B (2 + 5 n + 5 n^2)) x^(2 n) - 
         A^2 n (2 + n) (1 + A B + n)^2 x^(3 n))/(
        4 x^(2 + n) (1 + A (1 + A B + n) x^n)^2)) #) & /@ {(
    y1[x] + A x^(n + 1) y1'[x])/Sqrt[A (1 + A B + n) x^n + 1] , (
    y2[x] + A x^(n + 1) y2'[x])/Sqrt[A (1 + A B + n) x^n + 1]};

{A, B, n, x} = RandomReal[{0, 1}, 4, WorkingPrecision -> 50];
eX

Out[167]= {0.*10^-46 + 0.*10^-46 I, 0.*10^-48 + 0.*10^-47 I} [1] M von Hoeij, R Debeerst, W Koepf, Solving differential equations in terms of Bessel functions, https://www.math.fsu.edu/~hoeij/papers.html",['ordinary-differential-equations']
2952863,Definition of span,"On an old midterm exam, my professor requested the students prove that The span of $S$ (where $S$ is a subset of a vector space $V$ ) is equal to all vectors that can be expressed as linear combinations of the elements in $S$ . Does this make any sense? He's requesting we show that the span of $S$ equals what I believe to be the definition of span. Is there possibly some other definition of span that I should be aware of?","['definition', 'linear-algebra', 'vector-spaces']"
2952887,Not sure how to solve $\lim_\limits{x\to0}{{\sqrt{x^2+1}-1}\over\sqrt{x^2+16}-4}$,"So I got this problem: Determine the following limit value: $$\lim_\limits{x\to0}{{\sqrt{x^2+1}-1}\over\sqrt{x^2+16}-4}$$ What I tried is: $\large{\lim_\limits{x\to0}{{\sqrt{x^2+1}-1}\over\sqrt{x^2+16}-4}\cdot{\sqrt{x^2+16}+4\over\sqrt{x^2+16}+4}=\\\lim_\limits{x\to0}{({\sqrt{x^2+1}-1})(\sqrt{x^2+16}+4)\over x^2+16-16}}$ From this point I just get everything messy, and can't get anything out of it, so I believe this is not the way to solve this. Doing it from a table of values gives 4, but I should solve this without the table.","['proof-writing', 'limits-without-lhopital', 'real-analysis', 'limits', 'radicals']"
2952894,"Show that, for square matrices $A$ and $B$, $A+B=AB$ implies $AB=BA$. [duplicate]","This question already has answers here : Matrices A+B=AB implies A commutes with B (2 answers) Closed 5 years ago . Let $A$ and $B$ be two $n$ -by- $n$ real matrices such that $A+B = AB$ .
  How do I prove that $AB= BA$ ? I have tried using the trace function on $A+B-AB$ . But I could not get any Ideas.
Kindly provide me with hints.","['matrices', 'linear-algebra']"
2952949,Counterexamples in Group Theory and Linear Algebra,"I am studying Group Theory from Basic Algebra(Nathan Jacobson) and Linear Algebra by Hoffman and Kunze. The exercises in both the books are interesting. However when I try to solve question papers of grad school entrance exams, I am often confronted by Group Theory and Linear Algebra questions that require knowledge of counterexamples. I feel that the exercises from the above mentioned books do not help me much in solving such questions. Is there any book which deals with counterexamples in Algebra? I have come across such books for Analysis and Topology but not for algebra. Kindly help.","['soft-question', 'group-theory', 'abstract-algebra', 'linear-algebra']"
2952983,Understanding the completion theorem for metric (vector) spaces,"I am wondering if I have understood the consept of completion of a 
metric/normed space correctly. As I have understood the completion theorem, it is: $$\textbf{Completion theorem for metric spaces}$$ Let $(X,d)$ be a metric space. Then there exists a metric space $(M,D)$ and a linear injective function $\phi: X\rightarrow M$ such that $\overline{\phi(X)}=M$ , and such that $d(x,y)=D(\phi(x),\phi(y)) \quad \forall x,y \in X $ . Moreover, this completion $(M,D)$ of $(X,d)$ is unique up to isomorphism. This means that if $(M',D')$ is another metric space with $\gamma:X\rightarrow M'$ a injective linear function satisfying $\overline{\gamma(X)}=M'$ with $d(x,y)=D'(\gamma(x),\gamma(y)) \quad \forall x,y\in X$ , then there also exists a unique bijection $F:M\rightarrow M'$ satisfying: $1)\quad  (F\circ\phi)(x)=\gamma(x) \quad \forall x\in X$ $2)\quad  D(u,v)=D'(\phi(u),\phi(v)) \quad \forall u,v \in M$ $$\textbf{What i am wondering is:}$$ $1) \quad $ Have i stated this theorem correctly? $2)\quad $ In the lecture notes i was handed they required that $X\subset M$ . But if this was the case then the field $\mathbb{Q}_p$ would per definition not be a completion of $\mathbb{Q}$ with respect to the $p-adic$ norm, since the rationals in $\mathbb{Q_p}$ are isomorphic but not equal to the rationals in $\mathbb{Q}$ (they are defined in terms of equvialence classes of cauchy sequences. ) Am I mistaken? Must a completion contain the actual set? $3) \quad $ Is property 2) of the isomorphism $F$ superfluos/wrong? $4) \quad $ Is $\mathbb{C}$ not per definiton a completion of $\mathbb{Q}$ with respect to the absolute value? (But $\mathbb{R}$ is)","['complete-spaces', 'normed-spaces', 'metric-spaces', 'definition', 'general-topology']"
2952985,Every 4-regular simple graph contains a 3-regular subgraph,"The following result was conjectured by Berge & Sauer, and proved by Tashkinov [T]. Theorem A. Every 4-regular simple graph contains a 3-regular subgraph. A simple graph is the one with no loops or parallel edges. I do not have access to Tashkinov's paper, so I don't know how the proof works. On the other hand, using Combinatorial Nullstellensatz, one can show the following: Theorem B. Every 4-regular graph plus an extra edge contains a 3-regular subgraph. I should note that Theorem B allows the graph to have multiple edges, so in particular, you are allowed to add an extra edge to a pair of adjacent vertices (so that there are now two edges between these vertices). The proof of Theorem B is a special case of $p=3$ in this article here (see Theorem 2.5). Is there any easy way to use Theorem B to prove Theorem A? It is tempting to add a random edge $e$ somewhere and use Theorem B to get a 3-regular subgraph. But then we have to somehow throw out that additional edge $e$ . We may not be able to do this, as the edge $e$ could be an edge appearing in the 3-regular subgraph. Is there a way to get around this, or another trick? [T] Tashkinov. Regular subgraphs of regular graphs. Soviet Math. Dokl. 26 , (1982), 37-38. P.S. I should add that if you know how Tashkinov's original proof goes, please feel free to add it as an answer! This would be very helpful too!","['graph-theory', 'combinatorics', 'problem-solving']"
2952997,Is there a homeomorphism between the sets of Schur stable and Hurwitz stable matrices in companion forms?,"The set of Schur stable matrices is \begin{align*}
\mathcal S = \{A \in M_n(\mathbb R): \rho(A) < 1\},
\end{align*} where $\rho(\cdot)$ denotes the spectral radius of a matrix and the set of Hurwitz stable matrices is \begin{align*}
\mathcal H = \{A \in M_n(\mathbb R): \max_{i=1, \dots, n} \text{Re}(\lambda_i(A)) < 0\},
\end{align*} i.e., matrices with eigenvalues lying on the open left half plane.
Let $\mathcal C$ denote all matrices in companion form . Let $\hat{\mathcal S} = \mathcal S \cap \mathcal C$ and $\hat{\mathcal H} = \mathcal H \cap \mathcal C$ . Now I would like to determine whether there is a homeomorphism between the sets $\hat {\mathcal S}$ and $\hat{\mathcal H}$ . Let us exclude the trivial case $n=1$ . If we consider the sets $\mathcal S$ and $\mathcal H$ only, there is a diffeomorphism $f: \mathcal S \to \mathcal H$ given by \begin{align*}
A \mapsto (A-I)^{-1}(A+I).
\end{align*} But apparently this doesn't work for $\hat {\mathcal S}$ and $\hat{\mathcal H}$ since the inversion and multiplication will not necessarily yield a matrix in companion form.","['matrices', 'general-topology', 'hurwitz-matrices', 'linear-algebra']"
2952998,how do vanishing cycles change under blowups of the central fibre?,"Let $X_t$ be a family of complex manifolds of dimension $n$ over a punctured disc $D^\circ=\{x \in \mathbb{C} \mid 0 < |x| < \epsilon\}$ and assume that we have chosen a model $\mathcal{X}_t$ over the disc $D=\{x \in \mathbb{C} \mid |x| < \epsilon\}$ such that $\mathcal{X}_0$ has finitely many nodes $x_1, \ldots, x_m \in \mathcal{X}_0$ as singularities. The monodromy operator $T: H^n(X_t) \to H^n(X_t)$ is given by the Picard-Lefschetz formula in terms of the vanishing cycles $\delta_i$ of the nodes $x_i$ : $$
Tx = x - (-1)^{n(n-1)/2}\sum_{i=1}^n \langle x, \delta_i \rangle
$$ but it clearly does not depend on a particular model $\mathcal{X}$ . If the central fibre $\mathcal{X}_0$ is modified, for example, by blowing up a node, then the vanishing cycles of the nodes in the central fibre of the new model must be such that the above formula still gives the same transformation. I am trying to see what happens in the example of the degenerating family of elliptic curves $\mathcal{E}_t: y^2 = x(x-1)(x-t)$ . Here the central fibre is a nodal cubic and the corresponding vanishing cycle is one of the generating circles of the torus which becomes pinched in $\mathcal{X}_0$ and turns into a node. Let $\mathcal{E}'$ be the family obtained after blowing up the node, then $\mathcal{E}'_0$ has two nodes $x'_1, x'_2$ . What are the vanishing cycles of $x'_1, x'_2$ ? What happens after I blow up $x'_1$ or after I blow up a smooth point of $\mathcal{E}'_0$ , what will be the vanishing cycles of the new nodes?","['complex-geometry', 'algebraic-geometry', 'algebraic-topology']"
2953071,Determining Bifurcation of a Function,"Hello I am trying to find analyze the bifurcation behavior of $\dot{N} = N(N - e^{\alpha N}) , N \geq 0, \alpha > 0$ as $\alpha$ is varied and find their stability. Playing around with different values of $\alpha$ , I see that when $\alpha$ is not small(e.g. $\alpha \geq 1$ would be considered ""not small"" in regards to this particular system) I find that for positive values of $N$ the only fixed point will be at $N = 0$ when considering only the non negative values of $N$ . This fixed point is stable because all positive values of $N$ are decreasing towards it and more rapidly so the larger $N$ is relative to $\alpha$ . On the other hand an interesting thing seems to occur for a small enough value, say suppose $\alpha = .1$ , assigning random nonnegative but small such as $N = 1$ $\dot{N}$ will be negative since $\dot{N}(1) = 1 - e^.1 < 0$ , positive when $N$ is ""just right in terms of being small or large"" e.g. $N = 10$ causes $\dot{N}$ to be positive since $\dot{N}(10) = 10(10 - e^1) > 0$ but then negative again after $N$ is large enough relative to $\alpha = .1$ since $\dot{N}(100) = 100(100 - e^{10}) < 0$ . Based on this quick qualitative analysis of the simple example of the value I assigned of $\alpha = .1$ , it seems there will now be 3 fixed points, 0 will still be stable but the second fixed point occurring immediately after $N$ starts to be positive will be unstable since values greater than this fixed point will increase away from it. The third fixed point occurring when $\dot{N} = 0$ for when the function then starts to decrease again will also be stable since values before it are increasing but decreasing afterwards. The main difficulty I am having in further studying the bifurcation behavior of this system is determining what is the exact bifurcation point and what type of bifurcation it would be classified as. Currently I hypothesize that the particular bifurcation point would be considered a saddle node bifurcation since the two fixed points appear and vanish before and after alpha is small enough relative to the system and that a logarithmic scale analysis would be helpful when trying to represent the bifurcation. I want to know how to solve for the exact bifurcation value in a quantitative manner, any advice would be much appreciated.","['bifurcation', 'stability-theory', 'ordinary-differential-equations']"
2953092,Why is it called the 'Seesaw theorem'?,"I'm reading Mumford's ""Abelian variety"" and he proved the theorem of cubes by using the Seesaw theorem: Let $X$ be a complete variety, $T$ any variety and $\mathcal{L}$ a line bundle on $X\times T$ . Then the set $$
T_{1} = \{t\in T\,:\, \mathcal{L}_{X\times\{t\}} \text{ is trivial on } X\times\{t\}\}
$$ is closed in $T$ , and if on $X\times T_{1}, p_{2}:X\times T_{1}\to T_{1}$ is the projection then $\mathcal{L}|_{X\times T_{1}}\simeq p_{2}^{*}M$ for some line bundle $M$ on $T_{1}$ . But I can't understand why this theorem is called Seesaw theorem. Could you tell me about any intuition of this theorem? Thanks in advance.",['algebraic-geometry']
2953100,We have to show that $n\times n$ matrices $A$ and $B$ are nilpotent.,"Let $A$ and $B$ be $n\times n$ matrices with entries from some field $\mathbb{F}$ . Let $c_1,\ldots,c_{n+1}$ be $n+1$ distinct elements in $\mathbb{F}$ such that $A+c_1B,\ldots,A+c_{n+1}B$ are all nilpotent. Then how can I show that $A$ and $B$ are nilpotent.","['matrices', 'linear-algebra', 'vector-spaces']"
2953115,Limit of $\sin(\pi \sqrt{4n^2+n})$,We can notice that $\sqrt{4n^2 + n} = \sqrt{4n^2(1+ \frac{1}{4n})} = 2n\sqrt{1+\frac{1}{4n}}$ . Therefore $$\lim_{n \to \infty} \sin (2n\pi \sqrt{1+ \frac{1}{4n}}) \text{ will be an even number}$$ Because the square root becomes $1$ and we end up with an even number: $\sin(\text{even number})$ And sine of an even number is $0$ . But apparently that is not the right answer.,"['limits', 'sequences-and-series', 'real-analysis']"
2953172,Analytic continuation and two versions of Monodromy theorem,"So in complex analysis, the Monodromy Theorem says that: Let $\gamma_0,\gamma_1$ be two paths in $\mathbb C$ s.t $\gamma_0(0)=\gamma_1(0)=a$ and $\gamma_0(1)=\gamma_1(1)=b$ . Let $\{\gamma_s\}_{s\in[0,1]}$ be a homotopy between $\gamma_0$ and $\gamma_1$ fixing the end points. Let germ $[f]_a\in \mathcal O_a$ where $\mathcal O_a$ is the stalk at $a$ . Suppose that $[f]_a$ can be continued analytically along $\gamma_s$ for all $s\in [0,1]$ . Then analytic continuation of $[f]_a$ along $\gamma_0$ and along $\gamma_1$ result the same germ at $b$ Now, I don't see how to conclude the ""classical"" Monodromy theorem from this, I mean how to show that: If a complex function $f$ is analytic in a disk contained in a simply connected domain $D$ and $f$ can be analytically continued along every polygonal arc in $D$ , then $f$ can be analytically continued to a single-valued analytic function on all of $D$ !","['complex-analysis', 'riemann-surfaces']"
2953182,Showing that the unsigned area bounded by plane curve $\gamma$ is $-\frac1{4\pi}\oint_\gamma\oint_\gamma\vec{dx}\cdot\vec{dy}\log(\|x-y\|^2)$,"Let $\gamma$ be a curve in the plane. I wish to show: $$A=\frac{-1}{4\pi}\oint_\gamma\oint_\gamma\overrightarrow{dx}\cdot\overrightarrow{dy}\log\left(\|x-y\|^2\right),$$ where here $A$ is the unsigned area bounded by $\gamma$ . For instance, the unsigned area of the lemniscate is strictly positive whereas its signed area is zero. I believe this formula is true because I have checked it for rectangles and ellipses, and numerically for a variety of other curves. Nevertheless I haven't been able to prove it myself or find a proof. I believe Stokes' theorem is not helpful because I would guess proofs based on it would lead to signed area. The physical motivation for this comes from thinking about electromagnetism in 1+1 dimensions, which might be a helpful starting point.","['geometry', 'vector-analysis']"
2953212,"Prove that if $A$ and $B$ are finite sets such that $A \subseteq B$ and $|A| = |B|$, then $A = B$.","Prove that if $A$ and $B$ are finite sets such that $A \subseteq B$ and $|A| = |B|$ , then $A = B$ . Does the same result hold if $A$ and $B$ are not finite? Solution: To show $A=B$ it remains to show that $B\subseteq A$ . Consider the set $B\setminus A$ . Since $|A|=|B|$ and $A\subseteq B$ , it follows that $B\setminus A$ is empty because $B = A \cup (B \setminus A)$ is a disjoint union. Then $|B| = |A| + |B \setminus A|$ , thus $|B \setminus A|= 0$ as all cardinalities in questions are finite. Hence, $B\subseteq A$ . Finally, $A=B$ . Would the proof above be correct, and ""Does the same result hold if $A$ and $B$ are not finite?"" Not sure",['elementary-set-theory']
2953245,Find an angle in a geometric figure (given) considering triangles,"Question: In the figure below, AC=AB and AD=BC. Find angle $x$ . My attempt: using a geometric approach, consider the following figure (proportions are not exact). Using the notation $AC=AB=b$ and $CD=a$ , consider point P, in such a way that $AP=b$ and $\angle PAB=60^o$ . Therefore, $\triangle APD\cong \triangle CAB$ (SAS), and $PD=b$ . We can also conclude that $\triangle PAB$ is equilateral and $PB=b$ . Finally, $\triangle DPB$ is isosceles with $\angle DPB=160^o$ , so we can conclude that $\angle PBD=10^o$ and $\angle x=10^o$ (as $\angle ABP=60^o$ ). Question: is there a trigonometric approach to the problem? I tried using the sinus law but without success. Any hint or full solution using trigonometric methods or other more straightforward approach will be appreciated.","['contest-math', 'euclidean-geometry', 'geometry', 'triangles', 'trigonometry']"
2953246,Irreducibility of polynomial $x^3-y^2$,"I was told that $x^3-y^2$ is irreducible in $\Bbb C[x,y]$ . But I cannot really give a sounding argument.  I supposed that it may be factored as $g_1, g_2$ , and considered those monomials divisible by $y$ . But possible cancellations makes it hard. EDIT: Thoughts: Suppose $(x^3-y^2)$ is reducible in $k[y][x] \cong k[x,y]$ , then it would factorize as a polynomial in $x$ over the ring $k[y]$ . As the degree is $3$ , there must exists a monomial factorization. Hence there is a polynomial solution $x=P(y)$ , which is impossible.","['algebraic-curves', 'algebraic-geometry']"
2953269,How to prove $ A^c \cup B \subseteq (A - B)^c$,"My try: Suppose $x \in A^c \cup B$ . We know $x \in  A^c$ or $x \in B$ , which is the same as $x \notin A$ or $x \in B$ . From there I don't how to mais the synthesis of the other set.",['elementary-set-theory']
2953291,"Consider the set $A = \{1, 2, \ldots , n\}$. Count the number of subsets of $A$ of cardinality $k$","Consider the set $A = \{1, 2, \ldots , n\}$ . Count the number of subsets of $A$ of cardinality $k$ in two ways. One of those ways must be to count them in two steps as follows. • How many subsets of cardinality $k$ do not contain the numbers $1$ and $2$ ? • How many subsets of cardinality $k$ do contain the numbers $1$ and/or $2$ ? I need to count this $2$ ways and I can't figure out the second way that has to do with the bullet points. Can anyone help?","['combinatorics', 'discrete-mathematics']"
2953437,Does $\lim\limits_{x \to 0}\frac{\sin(x\sin\frac{1}{x})}{x\sin\frac{1}{x}}$ exist or not?,"Denote $$f(x)=\frac{\sin \left(x\sin\dfrac{1}{x}\right)}{x\sin\dfrac{1}{x}}.$$ We want to research the limit $$\lim_{x \to 0}f(x).$$ According to most of the textbooks for common calculus, especially in China which is named Advanced Mathematics not Mathematical Analysis, the existence of the functional limit at some point requires that the funciton has the definition over some deleted neighborhood of that point. Since the domain of $f(x)$ is $$D=\mathbb{R} \backslash M,M=\left\{x\bigg|x=0,~\text{or}~~  x=\pm\frac{1}{k\pi}(k\in \mathbb{N_+})\right\},$$ then there exists no deleted neighborhood of $x=0$ such that $f(x)$ has definition over it. That is because: $$\forall \delta>0, \exists x_0 \in M: 0<|x_0-0|<\delta.$$ Thus, the limit seems not to exist, for the reason that $f(x)$ can not satisfy the fundamental condition which is the premise for the definition of the functional limit. But, according to Mathematical Analysis, the functional limit can be defined at the condensation point. Thus, we have to acknowledge that the limit really exists and may readily evaluate the limit $$\lim_{x \to 0}f(x)=1.$$ Now, the question comes out. Does the limit exist or not on earth? How to reconcile this contradiction especially in teaching and examination?",['limits']
2953460,$\int_{-\infty}^{\infty}{x\choose t}dt=2^x$ for $x\geq0$??,"I noticed that on Desmos, for $m>0$ and $x\geq0$ , $$\int_{-m}^{m}{x\choose t}dt$$ closer and closer approximated $2^x$ . So, does $$\int_{-\infty}^{\infty}{x\choose t}dt=2^x$$ Assuming that ${x\choose t}=\frac{\Gamma(x+1)}{\Gamma(t+1)\Gamma(x-t+1)}$ . I know that $\sum_{k\geq0}{x\choose k}=2^x$ , but there are, of course, differences between $\int_{-\infty}^{\infty}$ and $\sum_{k\geq0}$ . Can someone prove/disprove this?","['integration', 'exponentiation', 'binomial-coefficients', 'improper-integrals']"
2953467,Maximal disjoint sets in a list,"Given $A = [[4, 10, 14], [5, 13, 14], [2, 7, 13], [0, 2, 12], [2, 4, 11], [3, 5, 11], [3, 7, 10], [6, 9, 10], [0, 1, 3]]$ is a list of sets. I want to find the maximum number of sets from the list which are disjoint. (By disjoint, I mean that if I have selected $[4,10,14]$ then I cannot select $[5,13,14]$ since it contains $14$ which I had already chosen in the previous set. The answer for this particular problem would be 4, as I can choose $[0,1,3], [6, 9, 10],[2, 4, 11],[5, 13, 14]$ I tried the greedy approach but it does not seem to be optimal for all cases. Any suggestions on what algorithms/approaches I can try out ?","['discrete-mathematics', 'algorithms']"
2953476,When is a topological space uniquely uniformizable?,"In general, multiple uniformities can induce a given topology.  But if a topological space is a compact Hausdorff space, then there is only one uniformity which induces its topology.  The formal way to say it is, compact Hausdorff spaces are uniquely uniformizable. My question is, what other spaces are uniquely uniformizable?  Are there specific examples of such spaces, and is there a general chatacteization of such spaces?","['uniform-spaces', 'general-topology', 'examples-counterexamples', 'compactness']"
2953527,"Collection of all open balls, centered at the same point, in a dense subset form a base for the containing set?","I'm not even convinced that this is true, and I'm hoping that someone can help me to see why it is true. I am attempting to prove the following: Consider a metric space $F$ and a set $E$ that is dense in $F$ . Show that the set of all open balls in E, centered at $e \in E$ and with rational radii, say $\{B(e, r_i)\}$ , are a base for $F$ . To show that $\{B(e, r_i)\}$ is a base for $F$ , I'd need to show that for any open set $T \subset F$ and element $t \in T$ , there exists a ball $B(e, r_t) \in \{B(e, r_i)\}$ such that the following is true: $$t \in B(e, r_t) \subset T$$ But, I don't even believe that this is true. If all of the balls in $\{B(e, r_i)\}$ must be centered at a fixed point $e$ , then how can one of them always be contained in an arbitrary, open subset of $F$ ? The picture below is meant to illustrate my confusion.","['general-topology', 'metric-spaces', 'real-analysis']"
2953529,Fractional embedding inequality with $L^{\infty}$ norm,"Here we consider the fractional Sobolev spaces and suppose $u$ is a vector function in $\mathbb R^2$ . Is the following always true? $$\Vert Du \Vert_{L^{\infty}(\mathbb R^2)} \leq C\Vert Du \Vert^{1-\alpha}_{L^{q}(\mathbb R^2)}\Vert D^{\beta}u \Vert^{\alpha}_{L^{q}(\mathbb R^2)}$$ where we can calculate the relation between $\alpha$ , $\beta$ and $q$ by scaling.","['analysis', 'real-analysis', 'fractional-sobolev-spaces', 'sobolev-spaces', 'partial-differential-equations']"
2953534,Show $\{-n + 1/n \ : n \in \mathbb{N}\}$ is a closed set,"I have the set $$A = \{-n + 1/n \ : n \in \mathbb{N}\}$$ My attempt I tried to find some limit point in A, but $$
\lim_n (-n + 1/n) = -\infty
$$ Is there anyone to help?",['general-topology']
2953536,Function to calculate t-stat of similarity in survey answers,"I have a survey of a large number of questions.  Each question is multiple choice, and has three possible answers. Users get served random questions to answer.   So they do not all answer the same set of questions. If ""user 1"" and ""user 2"" have both answered 'm' (say 90) questions in common, and of these 'm' questions, 'n' have been the same answer (say 70). If my null hypothesis is ""user 1 and user 2 have uncorrelated answers"", how would I create a t-state from the above to reject / not reject. Thank you!","['bernoulli-numbers', 'statistics', 'probability-distributions', 'probability']"
2953553,Solving A Problem Involving Pythagorean Theorem And Polynomials [duplicate],"This question already has answers here : The position of a ladder leaning against a wall and touching a box under it (8 answers) Closed 5 years ago . I don't know how to solve this problem. What I've done so far is to construct 3 equations. However, I don't know how to solve those 3 equations: Let y be the distance between the box and the ladder, and z be the length of the portion of the ladder that is beneath the top of the box: $(x-1)^2+1=(10-z)^2$ $(y)^2+1=z^2$ $(x)^2+(1+y)^2=(10)^2$ I don't know how to proceed from here, however.","['euclidean-geometry', 'geometry', 'factoring', 'algebra-precalculus', 'quadratics']"
2953555,"What numbers in $[0,1]$ can be generated by tossing a fair coin?","What numbers in the interval $[0,1]$ can be generated by tossing a
  fair coin? By generating a number using a coin, we mean finding an event that its probability is the given number. I think that any number in $[0,1]$ can be generated by tossing a fair coin for an infinite number of times because we can generate the binary expansion. And by generating, I mean finding an event that gives the desired probability. So, it seems that if tossing a coin for an infinite number is allowed, the problem's done. However, what if we disallowed tossing for infinitely many times? Then I think only those numbers whose denominator are a power of $2$ can be expressed. Others cannot be expressed. But I am not sure. Any help is appreciated.",['probability']
2953568,"Sobolev-function $u\in W^{1, p}(\mathbb{R}^n)$ which is unbounded on every open set $U\subset\mathbb{R}^n$","I know that there is a function $u\in B^n(0,1)$ s.t. $u$ is unbounded on every open set of $B^n(0, 1)$ . The usual approach is to pick a countable dense set $\{q_n\}\subset B^n(0, 1)$ and set $$u(x)=\sum_{n=1}^{\infty}\frac{|x-q_n|^{\alpha}}{2^n} $$ for a suitable exponent $\alpha$ , because we know that the function $v(x)=|x|^{\alpha}$ is in $W^{1, p}(B^n(0, 1))$ iff $\alpha=0$ or $\alpha >1-n/p$ . My question is: Can I extend this example to the whole $\mathbb{R}^n$ , because it is also separable? The problem is that the component functions of $u$ are no longer integrable over $\mathbb{R}^n$ . So I was considering cut functions $f_n\in C^{\infty}_0(B(q_n, r_n))$ so that $f_n|_{B(q_n, r_n/2)}=1$ and $0≤f_n≤1$ . But now I'm struggling with the choice of radius $r_n$ such that the integrals of component functions would be independent of $n$ or would contribute in a way that the series converges. Also I'm not sure why I can differentiate the series term by term, because the convergence is not necessarily uniform. Or does this follow from Dominated Convergence? Is my approach correct or am I missing something?","['sobolev-spaces', 'lebesgue-integral', 'functional-analysis']"
2953600,"How to map interval $[0, 100]$ to the interval $[100, 350]$?","I have an interval $[0; 100]$ and would like to map it to this new interval: $[100;350]$ . I thought about multiplying it by $3.5$ , but that would give the interval $[0;350]$ . And adding to each of these elements $100$ would give: $[100;450]$ . Hence my question: is it possible to do what I want? Note that I can settle for the interval $[0;350]$ : in my program, it will be enough if I exclude the numbers present in the interval $[0;99]$ .","['algebra-precalculus', 'arithmetic']"
2953672,Let $G$ be a finite matrix group in $GL_2(Q)$ such that every matrix $A\in G$ has integer entries. Prove $A^{12}= I$ for each $A$.,"Let $G$ be a finite matrix group in $GL_2(Q)$ (general linear group of $2$ by $2$ matrices with rational entries) such that every matrix $A\in G$ has integer entries. Prove that $A^{12} = I$ for every $A \in G$ . Attempt : We have that $A^k = I$ for some natural $k$ since $G$ is finite. Then the minimal polynomial of $A$ divides $x^{k} - 1$ . Also, the characteristic polynomial is of the form $x^2 + ax + b$ for some $a,b$ . Thus, the minimal polynomial has either a root of the form $x-c$ where $c$ is an integer, or it is the characteristic polynomial itself. In the first case, we get $A = I$ or $A =-I$ since $1,-1$ are the only integer roots of unity. Hence $A^2 = I$ . In the second case, we have that either the roots of the characteristic polynomial are $1,-1$ in which case we get $x^2 - 1$ , so $A^2 = I$ again. Otherwise, we have a complex root of unity and it's conjugate. This gives us $b = 1$ as it is the product of these roots, and $a$ is $2 *$ the real part. Hence we get $A^2 + aA + I = 0 $ so $A(A + aI) = -I$ . How do I use this to show $A^{12} = I$ ?","['matrices', 'group-theory', 'finite-groups', 'polynomials']"
