question_id,title,body,tags
4387420,"Calculating a 3 Circle Venn Diagram only knowing A, B, and C?","I came across this question: If there are 40 students in a class, 30 of them got A in Music, 33 of them got A in PE, and 37 of them got A in Art, at least how many students got all 3 As? The first thing that came to my mind is to use Venn Diagram to solve it. But most of the time when we solve Venn Diagram problems, $A \cap B, A \cap C, B \cap C$ , and $A \cap B \cap C$ are provided, namely, calculate the ""total"" amount of students in the class (40 in this case). But this kind of problem must be able to be ""reverse engineered"" just like any other math problem. So I went on and set $A=30, B=33, C=37$ $A+B+C-A \cap B-A \cap C-B \cap C + A \cap B \cap C =40$ (Assuming that nobody got no As) So, $\overline{A}=(B-A \cap B) + (C-A \cap C) - B \cap C=10$ $\overline{B}=(A-A \cap B) + (C-B \cap C) - A \cap C=7$ $\overline{C}=(B-B \cap C) + (A-A \cap C) - A \cap B=3$ And if I sum this up and add another $A+B+C$ , then it's $3(A+B+C-A \cap B-A \cap C-B \cap C)$ , then I could calculate $A \cap B \cap C$ from it. But unfortunately, $3(A+B+C-A \cap B-A \cap C-B \cap C)$ turned out to be exactly 120, so $A+B+C-A \cap B-A \cap C-B \cap C=40$ and $A \cap B \cap C$ is $0$ ! So then I thought, maybe I shouldn't assume that nobody got no As. And set a $\alpha$ as the number of students that got no As. But soon find out this number will be ""canceled"" in the operation and therefore completely useless whatsoever! Could somebody please be so kind and tell me where did I do wrong? Much appreciated! Update: When I keep pondering through the problem, I first thought it might be the case just like the image below: Where there ""not necessary"" have to have $A \cap B \cap C$ . But when I double-check it by changing all 3 circles to 40 (meaning everybody got all straight As, thus $A \cap B \cap C$ should be 40 as well) and run through it with my calculation, I found out that $A \cap B \cap C$ is ""still 0""! Which means this way of calculation is ""completely wrong""! So I thought, maybe I could set it up like and solving for g . Then $a+b+f+g=30$ $b+c+d+g=33$ $d+e+f+g=37$ $a+b+c+d+e+f+g+h=40$ And found out that this linear equation simply ""does not have enough information"" to go on. Could somebody please be so kind and teach me the correct way of doing it? Much appreciated! PS. I found out why it's always 0: Because those 3 equations I add up are originally $\overline{A}, \overline{B},$ and $\overline{C}$ , and if I add another $A+B+C$ to it, I'm essentially adding $A+\overline{A}+B+\overline{B}+C+\overline{C}$ , so of course it will be $3U (120)$ ""no matter what""! But, I still don't undertand why my ""logic"" is wrong, according to the equations, it ""should"" leave $A \cap B \cap C$ , but how come it somehow ""disappeared"" in the process? I really don't understand.","['elementary-set-theory', 'inclusion-exclusion', 'combinatorics']"
4387438,Exercise 4.1.4 from Brown and Ozawa,"The question is to show the following: Let $(\mathcal{A},\alpha)$ and $(\mathcal{B},\beta)$ be $\Gamma$ - $C^*$ -algebras. Let $\varphi:\mathcal{A}\to \mathcal{B}$ be a c.c.p map which is $\Gamma$ -equivariant. Show that the map $\tilde{\varphi}: C_c(\Gamma,\mathcal{A})\to C_c(\Gamma,\mathcal{B})$ , defined by $$\tilde{\varphi}\left(\sum_s a_ss\right)=\sum_s\varphi(a_s)s$$ extends to a continuous map from $\mathcal{A}\rtimes_{\alpha,r}\Gamma$ into $\mathcal{B}\rtimes_{\alpha,r}\Gamma$ . Of course, such a map is going to be $\Gamma$ -equivariant since it is $\Gamma$ -equivariant at the level of $C_c(\Gamma,\mathcal{A})$ . I want to know if $\Gamma$ -equivariance is necessary for this map to be continuous or not? It is clear that if I remove the assumption of $\Gamma$ -equivariance of $\varphi:\mathcal{A}\to \mathcal{B}$ , the map $\tilde{\varphi}$ is no longer going to be $\Gamma$ -equivariant. I am concerned with the continuity of this map. Here is my attempt at the proof: Without any loss of generality, assume that $\mathcal{A}$ and $\mathcal{B}$ have a faithful covariant representation inside $\mathbb{B}(\mathcal{H})$ and $\mathbb{B}(\mathcal{K})$ respectively. Using Fell’s absorption principle, we can view $\mathcal{A}\rtimes_{\alpha,r}\Gamma$ and $\mathcal{B}\rtimes_{\beta,r}\Gamma$ as sub-algebras of $\mathbb{B}(\mathcal{H})\otimes C_r^*(\Gamma)$ and $\mathbb{B}(\mathcal{K})\otimes C_r^*(\Gamma)$ respectively. Using Arveson’s Extension theorem, we can view $\varphi$ as a c.c.p map from $\mathbb{B}(\mathcal{H})\to\mathbb{B}(\mathcal{K})$ which I denote by $\varphi$ again. Now, let us observe that $\tilde{\varphi}$ is nothing but the map $\varphi\otimes\text{id}|_{\mathcal{A}\rtimes_{\alpha,r}\Gamma}$ . Please let me know if I missed something here. Thank you for the help.","['c-star-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras']"
4387503,Zero set of holomorphic function locally isomorphic to zero set of finitely many polynomial?,"Let $f\colon \Omega \to \mathbb{C}$ be a holomorphic function on an open subset $\Omega$ of $\mathbb{C}^n$ . Denote by $Z(f)$ its zero set. It is clear that there exists $f$ such that $Z(f)$ is not the zero set of finitely many polynomials. My question is: Does the above remain true if one allows to locally change the embedding of $Z(f)$ by a biholomorphic map. More precisely, does there exist an $f$ such that there is a point $p\in Z(f)$ with no open neighbourhood $U\subseteq \Omega$ that admits a biholomorphic map $\phi\colon U \to V$ such that $Z(f\circ \phi^{-1})=Z(P_1,...,P_k)$ for some polynomials $P_i$ on $V$ . It is clear that if $Z(f)$ is a complex submanifold, then for every point such a $\phi$ always exists. So, the zero set needs to be singular and $n>1$ . Essentially, my question is: Is every analytic hypersurface $X\subseteq \mathbb{\Omega}$ locally equivalent to an algebraic variety (up to biholomorphic changes of the embedding). If not are there explicit examples? What happens in the case of $Z(f_1,...,f_l)$ for multiple analytic functions.","['complex-analysis', 'complex-geometry', 'algebraic-geometry']"
4387533,Is the unit sphere of a smooth Banach space a smooth manifold?,"A Banach space X is smooth if at every point of the unit sphere there is only one supporting hyperplane of the unit ball.
Can we say for finite dimensional X, this implies the unit sphere to be differentiable manifold?
The reverse implication is almost tautological. If not, what is a counterexample to this?","['banach-spaces', 'differential-geometry']"
4387550,Bound central moments of even order with raw moments of same order,"Let $(\Omega, \mathcal A, P)$ be a probability space and consider a real-valued random variable $X \colon \Omega \to \mathbb{R}$ . It holds $$
\mathrm{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - \mathbb{E}[X]^2 \leq \mathbb{E}[X^2].
$$ Is this true for higher moments of even order? Precisely, does it hold for $p = 1, 2, \ldots$ $$
\mathbb{E}[(X - \mathbb{E}[X])^{2p}] \leq C(p)\mathbb{E}[X^{2p}],
$$ for some positive constant $C(p)$ depending only on $p$ and such that $C(1) = 1$ ? Edit: In this thread it is proved that the inequality above does not hold for $C(p) = 1$ for all $p = 1,2,\ldots$ Still, could it hold with a $p$ -dependent proportionality constant? Edit 2: It is possible to prove that the inequality holds with $C(p) = 2^{2p}$ using Hölder's and Jensen's inequality. In particular, Hölder's inequality yields for any real numbers $a$ and $b$ $$
(a-b)^{2p} \leq 2^{2p-1}(a^{2p}+b^{2p}),
$$ so that by linearity of $\mathbb{E}$ $$
\mathbb{E}[(X - \mathbb{E}[X])^{2p}] \leq 2^{2p-1}\left(\mathbb{E}[X^{2p}]+\mathbb{E}[X]^{2p}\right).
$$ Noew Jensen's inequality yields $\mathbb{E}[X]^{2p} \leq \mathbb{E}[X^{2p}]$ and we can conclude that $$
\mathbb{E}[(X - \mathbb{E}[X])^{2p}] \leq 2^{2p-1} \cdot 2 \mathbb{E}[X^{2p}]  =2^{2p}\mathbb{E}[X^{2p}].
$$ Is there any sharper bound? In this way, for $p = 1$ we obtain $C(1) = 4$ instead of $C(1) = 1$ .","['expected-value', 'inequality', 'probability']"
4387583,Freeing banks from debts- a nice combinatorial problem,"There are $N$ banks with each having some some (possibly negative) integral balance with them. We say, a bank is in debt if its balance is less than rupee $0$ . In each step, a bank may borrow $1$ rupee from every other banks, thus increasing its own balance by rupee $(N-1)$ and decreasing other banks' balances by rupee $1$ each lend $1$ rupee from every other banks, thus decreasing its own balance by rupee $(N-1)$ and increasing other banks' balances by rupee $1$ each If at the beginning, the total amount of money with all the banks is rupee $0$ , then is it possible to free every bank from debt in only a finite number of steps? If not, then what is the minimum total balance required to do so? I could see that rupee $0$ wont work as if we let $N=3$ and if the balances in the three banks are rupee $1$ , rupee $0$ , and rupee $-1$ respectively, then no matter what move we make, either the bank with negative balance goes to more negative, or the balance configuration remains same. Although this looks quite obvious, I have not been able to devise a formal proof of this. For the general case, I couldn't figure out anything. Here is a Numberphile video by Holly Krieger, where she discusses a case for a general graph with only the second condition allowed in each step. In our case, we have a complete graph, and so, with only the second step allowed, we can expect the answer to be $$\text{Total balance}\geq \binom N 2-N+1$$ but as professor Krieger mentions, the general formula of $\text{Total balance}\geq \mathtt{genus}$ requires quite advanced methods from Graph Theory. But, I am expecting an elementary solution for the present problem. Although I recieved quite an elegant answer, I would like to add something to my question. In the original source of this question, it was framed in a little different manner. The question already had a suggestion of three different ways one can take to achieve a solution (any of which may or may not be true) and they were- Write a system of linear equations describing the initial balances and the exchanges, and then solve it to get to an answer. Find a proof of the $N=3$ case and extend it to free other banks from debt. Find an invariant and use it solve the process. While the answer I got seems to use the last suggestion, I was throughout under the impression that the second way is also a valid one. So, is there an inductive way to reach to the solution? I feel like there should be a way to argue inductively. I would like to see some of your attempts to do so. Note: If anybody knows the code for the symbol of Indian Rupee in MathJax, please edit the question, or share the command. I know the code used in Overleaf, but that doesn't seem to work here.","['graph-theory', 'number-theory', 'combinatorics', 'elementary-number-theory']"
4387626,Textbook on elementary set theory that includes all proofs,"NB: I have read the earlier post Textbooks on set theory , but the information in that post is not sufficiently specific to answer my question here. To put it somewhat glibly, I am looking for a book that does for elementary set theory what Edmund Landau's Foundations of Analysis does for analysis. In other words, I am looking for a book on elementary set theory that explicitly proves everything it asserts, no matter how obvious the assertion or how tedious, or ""routine"", the proof. Such a book not only avoids ""proofs"" such as ""obvious"", ""routine"", ""exercise"" but also the likes of ""by induction on $\alpha$ "", or ""proof sketches"" in general. As for coverage, the book should at least cover ordinals and cardinals, and, especially, their respective arithmetics. EDIT: Since this post has received nothing approaching an answer, I think it is in order to relax the requirements somewhat.  Please regard the description above as ""an ideal to strive for,"" and propose candidates that you consider approach it most closely.","['elementary-set-theory', 'reference-request']"
4387660,Odds of testing the good light bulb,"We have three light bulbs on a shelf. We randomly pick one for
testing. We know that among the bulbs there are two relatively
non-optimal ('bad') and one relatively optimal ('good') regarding
luminosity. Let $\alpha, \beta$ and $\gamma$ denote the three bulbs, where $\alpha$ is the appropriate bulb. The corresponding probabilities that a given bulb will produce maximal
luminosity per trial are $P_{\alpha}$ , $P_{\beta}$ , and $P_{\gamma}$ ,
respectively. We carry out $N$ independent tests for which we observe that the
chosen bulb worked well $k$ times. What's the probability that we picked up the the good one for testing? My solution: Let $A$ denote the event that we picked out the appropriate bulb, and let $B$ denote the event that $k$ successes occurred in $N$ trials. Then the desired conditional probability is the following: \begin{align*}
P(A \mid B) &= \frac{P(A \cap B)}{P(B)}\\ &= 
\frac{\binom{N}{k}\cdot P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k}}{\binom{N}{k}\cdot P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k} + \binom{N}{k}\cdot P_{\beta}^{k} \cdot\left( 1 - P_{\beta}\right)^{N -k} + \binom{N}{k}\cdot P_{\gamma}^{k} \cdot\left( 1 - P_{\gamma}\right)^{N -k}}\\
 &= \frac{P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k}}{P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k} + P_{\beta}^{k} \cdot\left( 1 - P_{\beta}\right)^{N -k} + P_{\gamma}^{k} \cdot\left( 1 - P_{\gamma}\right)^{N -k}}.
\end{align*}","['discrete-mathematics', 'conditional-probability', 'combinatorics', 'probability']"
4387686,"$\sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow EX_1=0,E[X_1^2]<\infty$","$\{X_n\}$ are independent identical random variables, $S_n=X_1+...+X_n$ . Proof: for each $\epsilon>0,$ \begin{equation} 
\sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow EX_1=0,E[X_1^2]<\infty
\end{equation} My ideas so far: From Borel 0-1 law: \begin{equation} 
\sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow p(\text{limsup}\frac{|Sn|}{n}\geq \epsilon )=0\Leftrightarrow \text{lim} \frac{|S_n|}{n}=0\Leftrightarrow\text{lim} \frac{S_n}{n}=0
\end{equation} Also, $\sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow E|S_n|<\infty$ (From the formula $\sum p(|S_n|\geq n)\leq E|S_n|\leq 1+\sum p(|S_n|\geq n)$ ), then $E|X_n|<\infty$ . By strong law of large number, we have: \begin{equation}
0=\text{lim}\frac{Sn}{n}=EX_1
\end{equation} Which means: $\sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow EX_1=0$ . I don't known how to prove or use the condition $E[X_1^2]<\infty$ . Are there something wrong with my proof? Thanks in advance for any tips or help in general.","['probability-theory', 'probability', 'sequences-and-series']"
4387698,How to find this double integral?,"Given $\vec F=y\hat i+(x-2xz)\hat j-xy\hat k$ evaluate $$\iint_R(\nabla \times\vec F)\cdot \vec n dS $$ Where $S$ is surface represented by $x^2+y^2+z^2=a^2$ for $z\ge 0$ My attempt: i found curl $$\nabla\times \vec F=\left|\begin{matrix} \hat i & \hat j & \hat k\\ \frac{\partial }{\partial x}&\frac{\partial }{\partial y}&\frac{\partial }{\partial z} \\ y& x-2xz&-xy\end{matrix}\right|=(x,y,-2z)$$ normal vector $$\vec n=\frac{\nabla g}{|\nabla g|}=\frac{(2x, 2y, 2z)}{2a}=(x/a, y/a, z/a)$$ used Stokes theorem andtaking projection on the XY plane: $z=0$ $$\iint_R(\nabla \times\vec F)\cdot \vec n dS=\iint (x,y,-2z)\cdot (x/a, y/a, z/a)dxdy$$ $$=\frac1a\int \int \left(x^2+y^2-2z^2\right)dxdy$$ putting limits of $x=-a$ to $x=a$ and $y=-\sqrt{a^2-x^2}$ to $y=\sqrt{a^2-x^2}$ and $z=0$ $$=\frac1a\int_{-a}^{a} \int_{\sqrt{a^2-x^2}}^{\sqrt{a^2-x^2}} \left(a^2-0\right)dxdy$$ $$=\pi a^3$$ I am not sure where I made mistake. please correct me and help solve this integration. Thanks","['integration', 'vector-fields', 'multivariable-calculus', 'multiple-integral']"
4387729,Proof for combinatorial Identity $\sum_{k=2}^n {k+1\choose3}{2n-2-k\choose n-2}2^k= \frac{1}{3}\cdot\frac{(2n)!}{(n-2)!n!}$,"How to prove the following combinatorial identity? $$\sum_{k=2}^n {k+1\choose3}{2n-2-k\choose n-2}2^k= \frac{1}{3}\cdot\frac{(2n)!}{(n-2)!n!}$$ My attempt: The LHS is equal to the coefficient of $x^{n-2}$ in $\frac{1}{(1-x)^4\cdot(1-2x)^{n-1}}$ . But this doesn't help. I don't see any other approaches right now to tackle this problem. This is an identity I need to prove to solve another problem, which goes as follows: Select $n$ intervals uniformly at random from the range $\left[0,1\right]$ . Show that the probability that at least one interval intersects every other interval, is equal to $\frac{2}{3}$ . The jump from this problem to this identity is non-trivial, but this is basically the background of the problem.","['summation', 'binomial-coefficients', 'combinatorics']"
4387871,Kenmotsu - rotational surfaces,"I was wondering if someone knows which is the parameterization that we made in these cases. I calculated till $$Z(s)=\left(\frac{1}{2iH}(1-e^{-2iHs})+C\right)e^{2iHs},$$ but can't figure out what parameterization was done after that and why we have the following values ​​for $y(s)$ and $x^{'}(s)$ !",['differential-geometry']
4387881,A singular variant of the OEIS sequence A349576,"I refer to a previous post of mine in which it is defined the recurrence relation (now OEIS sequences A349576 and A349982 ) $$ x_{n+1} = \frac{x_{n} + x_{n-1}}{(x_{n},x_{n-1})} + c\;\;\;\;\;\;\;(1)$$ where $(x_{n},x_{n-1})$ is the gcd of $x_{n}$ and $x_{n-1}$ . This recurrence relation often generates periodic sequences with periods depending on the value of the integer constant $c$ . I wondered what dynamics I would have obtained by eliminating the dependence on such constant. One of the most interesting behaviors is shown by the following relation $$ x_{n+1} = \frac{x_{n} + x_{n-1}}{(x_{n},x_{n-1})} + (x_{n},x_{n-1})\;\;\;\;\;\;\;(2)$$ One might expect that the character (periodic or non-periodic) of the sequences generated by relation (2) depends on the choice of the initial conditions $x_0$ and $x_1$ . But things are a little different. I made some numerical experiments. The following screenshot shows the results obtained. The image consists of a $21$ x $21$ matrix $T$ in which: $x_0$ is the row index; $x_1$ is the column index; $t_{\,x_0,\,x_1}$ represents the period of the sequence generated by the recurrence relation $(2)$ starting with the initial conditions $x_0,\,x_1$ (the symbol ""+"" stands for a probably non-periodic sequence). Up to $21$ x $21$ only $3$ possible periods appear: $2901$ , $\,x_n=x_{n+2901}\,$ for $\,n \ge 278$ , with a peak value of $2269429312765395470820$ $(\sim 2.27\cdot 10^{21})$ $155$ , with a peak value of $513582$ (e.g. $x_0=2,x_1=19$ ) $3$ , with a peak value of $60$ (e.g. the pair $x_0=13,x_1=18$ generates the sequence $13,18,32,27,60,32,27,60,32,...$ ). Will different values appear for the period by extending the search scope? And why do the three indicated values ( $2901, 155, 3$ ) appear and not others? Edit Other three period lengths have been found: $9\,$ (see Peter's comment); $53\,$ ( $x_0=70,x_1=207$ ); $84\,$ ( $x_0=38,x_1=200$ ).","['number-theory', 'gcd-and-lcm', 'elementary-number-theory', 'recurrence-relations', 'sequences-and-series']"
4387901,"Show that $\int_0^\infty E[ 1\{f(X) \le f(X+t) \}] \, t dt =E[X^2]$","Consider a symmetric random variable $X$ with the pdf $f$ . We want to study the following expression: \begin{align}
\int_0^\infty E[ 1\{f(X) \le f(X+t) \}] t dt 
\end{align} where $1\{\cdot \}$ is the indicator function. Can we show that \begin{align}
\int_0^\infty E[ 1\{f(X) \le f(X+t) \}] \, t dt =E[X^2]?
\end{align} What I have done: Proof for the case when $f(x)$ is increase for $x<0$ and decreasing for $x>0$ . Under these conditions, we have that for $t\ge 0$ \begin{align}
1\{f(X) \le f(X+t) \}=1 \{ X \le 0, t \le 2|X|\}
\end{align} Therefore, \begin{align}
\int_0^\infty E[ 1 \{ X \le 0, t \le |X|\}] \, t dt &= E \left[1 \{ X \le 0\} \int_0^{2|X|} t dt  \right] \text{ by Tonelli_Fubini}\\
&=E \left[1 \{ X \le 0\} \frac{|X|^2}{2} \right]\\
&=E[X^2] \text{ by symmetry} 
\end{align} The issue is that not all symmetric random variables have this increasing property for the pdf. Therefore, I am not sure if it always holds.","['expected-value', 'probability-theory']"
4387908,Does Hoeffding's inequality hold for uncorrelated random variables?,"I know that Hoeffding's inequality holds for sums of independent random variables. However, we also know that being uncorrelated does not necessarily imply independence. But I wish to understand whether Hoeffding's inequality is valid for sums of bounded uncorrelated random variables? Or is independence a necessity?","['statistics', 'concentration-of-measure', 'independence', 'probability-theory', 'probability']"
4387937,A high school calculus exercise on function continuity.,"Suppose $f:\mathbb{R}\to\mathbb{R}$ is a continuous function and $f(f(x))+3f(x)=12-2x, \hspace{.2cm} \forall x \in \mathbb{R}$ . $a$ . Prove that there is $x_0 \in \mathbb{R}$ such that $f(x_0)=x_0$ $b.$ Find $f(2)$ $c.$ If $$\lim_{x \to +\infty}\frac{f(x)}{x}=\lim_{x \to -\infty}\frac{f(x)}{x}=\mathcal{L} \in (-2, 0),$$ Find $\mathcal{L}$ My work so far: Suppose that $f(x_1)=f(x_2)\Rightarrow f(f(x_1))=f(f(x_1))\Rightarrow f(f(x_1))+3f(x_1)=f(f(x_2))+3f(x_2)\Rightarrow 12-2x_1=12-2x_2\Rightarrow \\ x_1=x_2$ . So we have concluded that $f$ is one-to-one. Now, for part $a$ , suppose that $\nexists \hspace{.1cm} x_0$ such that $f(x_0)=x_0$ , so $f(x_0)\neq x_0\hspace{.1cm} (1) \forall x_0 \in \mathbb{R}$ $(1)\Rightarrow f(f(x_0))\neq f(x_0)\overset{+3f(x_0)}\Rightarrow 12-2x_0\neq  4f(x_0)\Rightarrow 2f(x_0)+x_0-6\neq 0$ . The idea was to suppose some $g(x)=2f(x)+x-6$ and try to check its signs, to show that it may have a solution. Honestly, I'm at a loss. If I could prove $a.$ , part $b$ would be so easy. Because then $f(x_0)=x_0$ for some $x_0$ and with the first equation, $x_0=2$ . I haven't even tried part $c$ , obviously because I can't even do part $a$ . The question was taken from a Greek book printed in 1995 and no longer in the market. I doubt it will help listing it.","['limits', 'calculus', 'functions', 'continuity']"
4388004,Contradiction in simple complex integral: should this pole contribute?,"This is a follow-up question to a post I made on Physics SE . TLDR; Essentially I have derived an apparent contradiction, starting from a simple integral. It feels like I'm missing something quite elementary and trivial, but I cannot seem to resolve it. Consider the following integral, whose approximate form is commonly finds in physics . $$f(x)=\int_{-\infty}^{+\infty}\frac{dz}{\left(-z^2 + m^2 - i\epsilon\right )^2 \left(-(z-x)^2+ M^2 -i \epsilon\right)} \tag{1}$$ $m,M>0$ are positive real numbers, $x$ is a positive real number (which we may allow to be complex), and $\epsilon>0$ is an infinitesimal small number which shifts the poles away from the real axis. In the complex $z$ plane, the integral looks as follows: The blue dots correspond to the double-poles at $z=\pm (m - i\epsilon)$ , and red dots correspond to the single-poles at $z=x\pm (M-i\epsilon )$ . The green line represents the integration contour. Notice that the pole at $z=x-M+i\epsilon$ resides in the $1^{\textrm{st}}$ quadrant. Now let's apply the technique of Feynman parameterization , which combines both the rational terms in the integrand into a single rational function. $$\begin{align}
f(x)&=\int_{-\infty}^{+\infty}dz \int_0^1 du_1 \int_0^{1-u_1} du_2 \frac{2!}{\left[ 
 (1-u_1-u_2)\left( -(z-x)^2+ M^2 -i \epsilon \right)+u_2 \left( -z^2 + m^2 - i\epsilon\right) +u_3 \left( -z^2 + m^2 - i\epsilon \right) \right]^3} \\
&= \int_0^1 du_1 \int_0^{1-u_1} du_2 \int_{-\infty}^{+\infty}dz \frac{2}{\left[-(z-(1-u_1-u_2)x)^2-(u_1+u_2)(1-u_1-u_2)x^2+(1-u_1-u_2)M^2+(u_1+u_2)m^2-i\epsilon\right]^3} \\
&= \int_0^1 du_1 \int_0^{1-u_1} du_2 \int_{-\infty}^{+\infty}dz \frac{2}{\left[-z^2+g(u_1,u_2,x)^2-i\epsilon\right]^3} \tag{2}
\end{align}$$ In the final expression above, note that no matter if $g^2$ is positive or negative, the triple-poles will always lie in the $2^{\textrm{nd}}$ and $4^{\textrm{th}}$ quadrants. Since no poles $1^{\textrm{st}}$ and $3^{\textrm{rd}}$ quadrants, we may rotate the contour counter-clockwise by $90^\circ$ , also known as Wick-Rotation . We now have the following expression: $$f(x)=\int_0^1 du_1 \int_0^{1-u_1} du_2 \int_{-\infty}^{+\infty} i dz \, \frac{1}{\left[z^2+g(u_1,u_2,x)^2 -i\epsilon\right]^3} \tag{3}$$ Let's now reverse all the previous algebraic steps, but using the Wick-rotated expression. This will finally give us (compare with (1)): $$f(x)=\int_{-\infty}^{+\infty}\frac{d(iz)}{\left(z^2 + m^2 -i\epsilon\right )^2 \left(-(iz-x)^2+ M^2 -i\epsilon\right)} \tag{4}$$ But this is none other than the Wick-rotated version of the original integral! This is explicitly telling us that: $$\int_{\Gamma_1} I(z) dz= \int_{\Gamma_2} I(z) dz \tag{5}$$ where $\Gamma_1$ is the contour along the real axis, $\Gamma_2$ is the contour along the imaginary axis, and $I(z)$ is the integrand in (1). Note that this is expression was derived by @QMechanic in the PSE post mentioned previously . The essence of this derivation is found in most introductory textbooks on Quantum Field Theory. On the other hand, if we tried to smoothly rotate the original contour by $90^\circ$ , we would unavoidably pick up the residue of the pole in the $1^{\textrm{st}}$ contour. $$\int_{\Gamma_1} I(z) dz= 2\pi i \,\textrm{Res} \left(I(z);z=x-M-i\epsilon \right)+ \int_{\Gamma_2} I(z) dz \tag{6}$$ Below you can see an illustration of this intuitive equation. Combining (5) and (6), we see that the residue must be zero. But we can calculate this residue and show it isn't zero, a contradiction! $$\textrm{Res} \left(I(z);z=x-M-i\epsilon \right)=\frac{1}{2M\left(-(x-M)^2+m^2\right)}\overset{?}{\neq} 0 \tag{7}$$ So what's going on here? How can I resolve this contradiction? Edit 1: A possible resolution. I've just realized an issue in the algebraic steps. In (2), as per @Maxim's comment, I shifted $z\rightarrow z+(1-u_1-u_2)x$ in order to remove the term linear in $z$ . This was fine, since $x$ was real-valued. However when reversing these steps later, going from (3) to (4), the equivalent shift would be $z\rightarrow z+i(1-u_2-u_2)x$ . This is not valid since for arbitrary $x$ the shift may hop over one of the poles! The resulting shifted-contour would not be homotopic to the original one! For it to be valid we would need $x$ to be imaginary. If we have analytic continuation in mind, we could simply assume at this step that $x$ is actually purely imaginary. But then our original shift $z\rightarrow z+(1-u_1-u_2)x$ would not be valid for the same reason!","['complex-analysis', 'calculus', 'physics', 'complex-integration', 'mathematical-physics']"
4388044,Is the collection of Gaussian functions $\mathcal F=\{\exp(-ax^2):a\geq0\}$ a basis?,"Is the collection of Gaussian functions $\mathcal F=\{\exp(-ax^2):a\geq0\}$ a basis for the space of all smooth functions on $\mathbb R^+$ ? Seems that this collection is linearly independent. Is this also a basis? EDIT: When I say basis, I am allowing infinite sums from this set, that is, given any smooth function $f$ on $\mathbb R^+$ , do we have a measure $\mu_f$ such that $f(x)=\int_{supp(\mu)} \exp(-ax^2)d\mu_f(a)$ for every $x\in\mathbb R^+$ ? I am not sure what $\mathcal F$ would be called if this happens, is this still called a ""basis""?","['functional-analysis', 'analysis', 'real-analysis']"
4388047,How do we know when direct substitution is the proper approach for a limit?,"In the equation $y=\frac{(x+2)(x-2)}{x+2}$ , the limit as $\lim_{x→-2}$ is $-4$ . However, direct substitution would give a result of ""Does not exist"". Given that direct substitution can variously be either appropriate for identifying a limit, or correct in saying that the limit does not exist, or (as in this case) incorrect in saying that the limit does not exist, how do we know if direct substitution should be used? My best guess is that a graphical analysis is the typical first step.","['limits', 'calculus']"
4388057,Complex functions with similar magnitude,"Let $f$ and $g$ be complex-analytic functions on the unit disk $D_1$ , and suppose that $$
\sup_{|z|\leq 1} \big||f(z)|-|g(z)|\big| \leq \epsilon.
$$ I am curious whether there exists some $\theta\in \mathbb{R}$ such that on the half-disk $$
\sup_{|z|\leq 1/2} |f(z)- e^{i\theta} g(z)| \leq C \epsilon.
$$ I can show that this holds when $g=1$ is the constant function.  In this case I can use the fact that $|f|+\epsilon -1$ is subharmonic and positive in order to apply Cacciopoli's inequality to get a bound on $\int_{D_{3/4}} \big|\nabla |f|\big|^2$ .  Then I can use the fact that $\big|\nabla |f|\big| = |f'|$ to show that on $D_{1/2}$ , $$
|f'|\leq C\delta.
$$ This shows that $f$ is close to a constant, as desired. This argument does not work when $g$ is not constant because $|f|-|g|+\epsilon$ is not subharmonic.  I am curious if there is another way to proceed. One barrier that makes the argument difficult to find is the example $f(z)=z$ and $g(z)=\overline{z}$ .  In this case $g$ is not analytic so the hypotheses do not hold, but it does show that the desired bound will not follow from only using bounds such as the maximum principle on $f$ and $g$ in isolation (perhaps one can use that $fg$ is also analytic, for example).",['complex-analysis']
4388100,Is a function between topological spaces continuous if continuous on subspaces?,"Assume that there are two topological spaces $X,Y$ and a function $f:X\rightarrow Y$ . Furthermore, assume that there exists a collection of sets $\mathcal{B}$ such that $\bigcup_{B\in \mathcal{B}} B = X,$ and for each $B \in \mathcal{B}$ the restriction $f|_B$ is continuous with respect to the subspace topology. Is this enough to say that $f$ is continuous on the whole space?",['general-topology']
4388111,Laplacian and inequality of a function,"Let $D=\{(x,y) | x^2+y^2 < 1 \}$ . Let $f$ and $g$ be a $C^2(D)$ be such that g is bounded and $f$ approaches infinity as $x^2+y^2$ approaches $1$ and moreover $\Delta f=e^f$ and $\Delta g \geq e^g$ at all points of $ D$ . Here $\Delta$ is the Laplacian. Prove $f \geq g$ on D. My strategy is to prove by contradiction. Suppose at point say $(x_0,y_0)$ , $g>f$ .
Now we define a new function h=g-f.At this point $\Delta h >0$ . Now I want to conclude that h is less than average of of all the values and arrive at the contradiction. How to proceed after this? This class just the knowledge of real analysis. So I cannot use tools from PDE.","['multivariable-calculus', 'calculus', 'real-analysis']"
4388113,Applications of the Poincaré-Bendixson theorem,"I am reading about the Poincaré-Bendixson theorem in the plane, I really liked the theorem. I have seen common applications in Sotomayor and Perko's book. But I would like to know what other applications outside of the common literature (mentioned above) there are, I mean interesting applications that you create valuable. If possible, mention a reference to your example. Thank you very much","['ordinary-differential-equations', 'dynamical-systems']"
4388121,Posterior of an Interval under Beta Distribution,"Suppose you have a beta prior with parameters $\alpha$ and $\beta$ .  You draw $n \geq 1$ points from a binomial distribution and observe $k > n/2$ successes.  Does your posterior probability in the interval $[1/2,1]$ increase?  Intuitively, I think ``yes'', but I am bad at evaluating integrals.  Since the beta family is conjugate to the binomial distribution, I believe that we can rephrase the question as follows.  If $k$ and $n$ are positive integers such that $1 \leq k \leq n$ and $k > (n-k)$ , is the following true? $$\frac{1}{B(\alpha, \beta)} \int_{1/2}^{1} x^{\alpha-1}(1-x)^{\beta-1}dx < \frac{1}{B(\alpha+k, \beta+(n-k))} \int_{1/2}^{1} x^{\alpha+k-1}(1-x)^{\beta+n-k-1}dx$$ where $B(\cdot, \cdot)$ is the beta-function. More generally, for $r \in [1/2,1)$ , if one observes $k$ many successes in $n$ trials and $k/n > r$ , does it follow that one's posterior in the interval $[r,1]$ increases?","['conditional-probability', 'statistics', 'probability-distributions', 'bayesian']"
4388166,Is the monotonicity of $f$ needed for Lagrange multiplier to be used?,"The sets $S_1$ and $S_2$ are defined as $S_1 = \left\{(x,y,z) \in \mathbb{R}^3_{+} : z = \frac{1}{xy}\right\}$ and $S_2 = \left\{(x,y,z) \in \mathbb{R}^3 : (x-4)^2+(y-4)^2 + z^2 = 5 \right\}$ . Consider the function $f$ that is given by the points of $\max(z(S_1), z(S_2))$ . (Given $x,y$ , if $z$ exists twice, then we choose the larger $z$ and its corresponding $x,y$ .) Find $\max(f)$ subject to $3x+4y \geq 5$ . Here is a picture I plotted using Geogebra. For the function $f$ , we choose the positive values of the sphere (red) in the region where it exists, and everywhere else, it is $\frac{1}{xy}$ (purple/violet). The blue plane denotes the equation $3x+4y=5$ . Can we solve this using the Lagrange multiplier ? I am essentially trying to understand when we can and when we can not use LM. The idea behind this is to draw a function that is not monotone throughout, so I added the sphere. The goal is to check whether the LM method gives $(x,y) = (\frac{3}{5},\frac{4}{5})$ as the optimal value or the point on the sphere $(x,y) = (4,4)$ to maximize $f$ . Ideally, it should be the latter. But I couldn't calculate this. I would like to know if the monotone nature of $f$ is really needed in general or not. (Do consider the possibility of non-linear constraints.) By monotonicity of $f$ , it is referred to the partial derivatives wrt $x$ and $y$ being $> 0$ .","['nonlinear-optimization', 'lagrange-multiplier', 'multivariable-calculus', 'calculus', 'optimization']"
4388184,A weaker version of sample quantile,"If $X_1,\dots, X_n$ iid samples from CDF $F(x)$ and assume that $F$ is first order differentiable at $\xi_p$ with $f(\xi_p)>0$ . Let $F_n$ be the empirical CDF of $F(x)$ . Then $\hat{\xi}_p=F^{-1}_n(p)$ is the sample p-th quantile.  Ghosh (1971) obtained a weaker version of Bahadur’s (1966) representation: $$
\hat{\xi}_p-\xi_p=\frac{p-F_n(\xi_p)}{f(\xi_p)}+o_p(n^{-1/2}).
$$ I try to prove this result as follows. Note that $F(\xi_p)=p$ and $$
\{\sqrt{n}(\hat{\xi}_p-\xi_p)\le t\}=\{p\le F_n(\xi_p+\frac{t}{\sqrt{n}})\}=\{
\frac{F(\xi_p+\frac{t}{\sqrt{n}})-F_n(\xi_p+\frac{t}{\sqrt{n}})}{f(\xi_p)}\le \frac{F(\xi_p+\frac{t}{\sqrt{n}})-F(\xi_p)}{f(\xi_p)}
\} \, (*)
$$ where $$
F(\xi_p+\frac{t}{\sqrt{n}})-F(\xi_p)=f(\xi_p)(\frac{t}{\sqrt{n}})+o(n^{-1/2})
$$ then the RHS on  (*) $$\sqrt{n}\times \frac{F(\xi_p+\frac{t}{\sqrt{n}})-F(\xi_p)}{f(\xi_p)}\to t$$ Also, the LHS on (*) is $$
\sqrt{n}\times\left(\frac{F(\xi_p+\frac{t}{\sqrt{n}})-F_n(\xi_p+\frac{t}{\sqrt{n}})}{f(\xi_p)}-\frac{F(\xi_p)-F_n(\xi_p)}{f(\xi_p)}\right)\to 0
$$ in probability. There is an asymptotic distribution of $\sqrt{n}\frac{F(\xi_p)-F_n(\xi_p)}{f(\xi_p)}$ to a Normal distribution from CLT. But how to prove that $$
\sqrt{n}\left((\hat{\xi}_p-\xi_p)-\frac{p-F_n(\xi_p)}{f(\xi_p)}\right)=o_p(1)?
$$","['statistical-inference', 'statistics', 'analysis', 'probability']"
4388225,Isometry without eigenvalues,"So I'm tasked with finding an $R^2$ isometry that has no eigenvalues in $\mathbb{R}$ . Trouble is, well, I don't really ""get"" isometries as they pertain to eigenvalues and transformation matrices. I could trivially come up with...say the following matrix: $$\begin{bmatrix}1 & -1\\
\frac{1}{4} & 1\end{bmatrix}$$ Now the eigenvalue would be: \begin{align}
(1 - \lambda)^2 - (-\frac{1}{2}) &= 0\\
\frac{5}{4} - 2\lambda + \lambda^2 &= 0\\
\lambda &= \frac{2\pm\sqrt{(-2)^2-4\cdot\frac{5}{4}}}{2}\\
\lambda &= \frac{2\pm\sqrt{4-5}}{2}
\end{align} That would have a complex eigenvalue (of $1 \pm \frac{i}{2}$ ), but that's very certainly not of absolute value 1. Since $a$ in the equation is always 1, I guess I'd need to come up with numbers that result in $\sqrt{2} + \sqrt{2}i$ so that the absolute value would indeed be 1. However, that doesn't really have much to do with the definition of an isometry, which is defined through inner product space: I can't really find the link between the eigenvalue and the inner product space. I guess my question is, how do I go about formulating an isometry that also fills all these conditions?","['isometry', 'linear-algebra', 'eigenvalues-eigenvectors']"
4388274,"Reference request for the space $C^k[a,b]$","I am looking for some notes/references that discuss basic properties of the space $C^k[a,b]$ of $k$ -times differentiable $\mathbb{C}$ -valued functions on an interval $[a,b]$ . I am not very familiar with this space, and the naive questions I would like to be able to answer are: Does $C^k$ here refer to the one-sided derivative? Or does it refer to functions on $[a,b]$ that can be extended to a $C^k$ function on a slightly larger open interval $(a-\varepsilon,b+\varepsilon)$ ? Is $C^k[a,b]$ a Banach $*$ -algebra with respect to the norm $$\|f\|_k=\sum_{i=0}^k\left\|\frac{d^i f}{dx^i}\right\|_\infty?$$ (Again, with derivative suitably interpreted.) Of course, answers to these questions are also welcome.","['banach-spaces', 'derivatives', 'banach-algebras', 'real-analysis']"
4388317,Distance of equally distributed points on a sphere,"Let $S^d = \{x \in \mathbb{R}^{d+1}: \|x\|_2 = 1\}$ be the unit sphere in $\mathbb{R}^{d+1}$ . Given $n \in \mathbb{N}$ , I want to understand how far apart each point from $S^d$ will be to its closest neighbor and how small the maximum of these distances may become across all points. I am trying to find: $$\min_{\{x_1, ..., x_n\} \in S^d} \max_{i \in [n]} \min_{j \neq i} \|x_i - x_j\|$$ This will depend on $n,d$ of course and I guess it will be optimal to space the points equally across the sphere. I am mostly interested in asymptotic bounds on this expression. In two dimensions it should be easy to get them because we can simply cut an arc of length $2\pi$ into $n$ equal parts.","['spheres', 'geometry', 'real-analysis']"
4388329,"Prove $\frac{Card\{X_1,\cdots,X_n\}}{\sqrt{n}}\rightarrow0$ in probability with i.i.d. $X_i\in\mathbb{N}, \ \mathbb{E}[X_1] < \infty$","Let $\{X_n\}$ be a sequence of independent, identically distributed random variables taking values in $\mathbb{N}$ , the set of positive integers. Define $$R_n=Card\{X_1,\cdots,X_n\}$$ i.e., $R_n$ is the number of distinct integers in $\{X_1,\cdots,X_n\}$ . Suppose that $\mathbb{E}[X_1] < \infty$ . Prove that $\frac{R_n}{\sqrt{n}}\rightarrow0$ in probability. $$$$ I suppose that the $\mathbb{E}[X_1] < \infty$ condition is used in law of large numbers, but the answer seems not corresponding, for the $\frac{R_n}{\sqrt{n}}$ it looks like CLE, but the answer is $0$ other than normal distribution. So what could possibly be the way of approaching this problem? Thanks! Edit: I've a possible idea of approaching, assume that $\mathbb{E}[X_1]=T>0$ , then for any $\epsilon>0$ we have $\mathbb{P}[X_1\ge\epsilon\sqrt{n}]\le\frac{T}{\epsilon\sqrt{n}}$ , hence we have (in intuition, approximately) $Card\{X_1,\cdots,X_n\}\le\epsilon\sqrt{n}+\frac{T}{\epsilon\sqrt{n}}\cdot n=\sqrt{n}(\epsilon+\frac{T}{\epsilon})$ (we pick all integers $\le\epsilon\sqrt n$ ,and $X_i$ cannot be too large), so $\frac{R_n}{\sqrt{n}}$ definitely have an upper-bound $2T$ , a more accurate estimation can bound this upper-bound to $T$ , and I think this idea is not far from the solution. The problem is, how to reduce this upper-bound to $0$ ?","['probability-distributions', 'analysis', 'probability-theory', 'probability']"
4388396,Is the set of functions on a compact manifold a Hilbert space?,"Let $M$ be a smooth compact manifold without boundary and let $\omega \in \Omega_M^{\mathrm{top}}$ be a volume form.
Let $C^k(M, \mathbb{R})$ be the set of functions $M \to \mathbb{R}$ whose first $k$ derivatives exist and are continuous.
We may then endow $C^k(M, \mathbb{R})$ with an inner-product $$\langle f, g \rangle := \int_M fg \omega.$$ Question: Is $(C^k(M, \mathbb{R}), \langle \cdot, \cdot\rangle)$ a Hilbert space for some choice of $k$ ? Does the answer change for some specific $\omega$ (e.g. coming from a Riemannian metric)?","['differential-geometry', 'smooth-manifolds', 'real-analysis']"
4388459,Inequality for Fourier transform and Schwartz function,"I have only recently started to study the Fourier transform in Schwartz space: and I've met a problem which I really do not have any idea on how to solve it. The problem is precisely the following: consider $p\in[1,\infty]$ abd $k=2[1+\frac{d}{2}]$ where $d$ is the dimension of the ambient Euclidean space $\mathbb{R}^d$ , and a function $u\in L^p$ . Then I want to find a constant $C>0$ such that $$|\langle\hat{u},\psi\rangle|\leq C\lVert \psi\rVert_{k,\mathcal{S}}\quad\psi\in\mathcal{S}$$ where $\mathcal{S}$ is the Schwartz space and $\hat{u}$ represents the Fourier transform of $u$ . Can somebody help me?","['functional-analysis', 'analysis']"
4388462,Can $Y_t= X_t - t X_1 -(1-t)X_0$ be Gaussian without $(X_t)$ being Gaussian?,"Let $Y_t= X_t - t X_1 -(1-t)X_0$ be a sample-continuous stochastic process defined on $[0,1]$ . If $(Y_t)$ is Gaussian, does it imply that $(X_t)$ is Gaussian on $(0,1)$ ? It feels wrong but I am unable to construct a counterexample. Here are some preliminaries: A time continuous stochastic process $\left\{X_{t} ; t \in T\right\}$ is Gaussian if and only if for every finite set of indices $t_{1}, \ldots, t_{k}$ in the index set $T$ , $$\mathbf{X}_{t_{1}, \ldots, t_{k}}=\left(X_{t_{1}}, \ldots, X_{t_{k}}\right)$$ is a multivariate Gaussian random variable. That is the same as saying every linear combination of $\left(X_{t_{1}}, \ldots, X_{t_{k}}\right)$ has a univariate Gaussian distribution. Using characteristic functions of random variables, the Gaussian property can be formulated as follows: $\left\{X_{t} ; t \in T\right\}$ is Gaussian if and only if, for every finite set of indices $t_{1}, \ldots, t_{k}$ , there are real-valued $\sigma_{\ell j}, \mu_{\ell}$ with $\sigma_{j j}>0$ such that the following equality holds for all $s_{1}, s_{2}, \ldots, s_{k} \in \mathbb{R}$ \begin{equation*}
\mathrm{E}\left(\exp \left(i \sum_{\ell=1}^{k} s_{\ell} \mathbf{X}_{t_{\ell}}\right)\right)=\exp \left(-\frac{1}{2} \sum_{\ell, j} \sigma_{\ell j} s_{\ell} s_{j}+i \sum_{\ell} \mu_{\ell} s_{\ell}\right) .
\end{equation*} where $i$ denotes the imaginary unit such that $i^{2}=-1$ .
The numbers $\sigma_{\ell j}$ and $\mu_{\ell}$ can be shown to be the covariances and means of the variables in the process.","['probability-distributions', 'real-analysis', 'stochastic-processes', 'gaussian', 'probability-theory']"
4388470,Extending a rational map $\psi:C\dashrightarrow\Bbb{P}^1$ into a morphism: concrete example,"Let all varieties be projective over $\Bbb{C}$ . It is well-known that a rational from a smooth curve to another curve extends into a morphism. My question is about a concrete example of this fact. Take $C:y^2z=x^3-xz^2$ on $\Bbb{P}^2$ and $P=(0:0:1)$ (or some smooth cubic and a point on it). Define the rational map \begin{align*}
\phi:\Bbb{P}^2&\dashrightarrow\Bbb{P}^1\\
(x:y:z)&\mapsto(x:y)
\end{align*} The map is defined everywhere except at $P$ . Now let $\psi:=\phi|_C:C\dashrightarrow\Bbb{P}^1$ . Since $C$ is smooth, then $\psi$ extends into a morphism $\psi:C\to\Bbb{P}^1$ . My question is: how can we define $\psi(P)$ ? I've tried a geometric interpretation, namely, if $L:sx+ty=0$ is any line through $P$ , then $(x:y)=(-t:s)$ , so the preimage $\psi^{-1}(-t:s)$ should consist (it seems to me) of $L\cap C$ , which has three points counted with multiplicity, including $P$ . But by this interpretation, $P$ should be in the preimage of every point in $\Bbb{P}^1$ , so it looks like I'm in the wrong path.","['plane-curves', 'algebraic-geometry', 'projective-geometry']"
4388511,Inequality involving matrix inverse elements,"Let $A$ be an $N \times N$ matrix with all nonnegative entries and row sums strictly
less than one, let $v$ be an $N$ dimensional vector with all nonnegative entries and weakly lower than one, let $B\equiv\left(I-A\mathrm{diag}\left(v\right)\right)^{-1}$ and let $B^*\equiv\left(I-A\right)^{-1}$ , where $\mathrm{diag}\left(v\right)$ is the diagonal matrix formed from vector $v$ . I want to show that for any $i,j=1,...,N$ the following inequality holds: $$
v_{j}b_{ji}^2+\sum_{k}\left(1-v_{k}\right) b_{jk}b_{ki}^{2}\leq b_{ji} b_{ii}^{*}.
$$ Simulations suggest that this is true. The case in which $v$ is the vector of all ones follows from the fact that $b_{ji} \leq b_{ii}^{*}$ , which is shown here . The case with $A$ diagonal is trivial for $i \neq j$ , whereas for $i=j$ it boils down to showing $$v_{i}b_{ii}+(1-v_{i})b_{ii}^{2}\leq b_{ii}^{*},$$ which can be shown by plugging in for $$b_{ii}=\frac{1}{1-v_{i}a_{ii}},\quad b_{ii}^{*}=\frac{1}{1-a_{ii}},$$ and basic algebra. Apart from these simple cases, I have been able to show the result for the case in which $j=i$ , but it is an arduous induction proof that does not extend to the case in which $i \neq j$ . We would appreciate hints for approaches that could be useful to prove the claim. The problem above comes from a more general problem in matrix algebra, which is to show that $ii$ of the following matrix is less than $b_{ii}^*$ , $$J \equiv \left(\mathrm{diag}\left\{ B^{T}x\right\} \right)^{-1}B^{T}\left[ \mathrm{diag}\left\{ v\right\}\mathrm{diag}\left\{ x\right\} +\mathrm{diag}\left\{ 1-v\right\} \mathrm{diag}\left\{ B^{T}x\right\} \right]B,$$ with $x$ being an $N$ dimensional vector in the simplex, i.e., $x_j \geq 0,\sum_j x_j=1$ . It can be shown that the diagonal elements will be maximized with respect to $x$ when $x$ is at a corner of the simplex, and that if $x_j = 1$ then $$
J_{ii} = v_{j}b_{ji}+\sum_{k}\left(1-v_{k}\right) \frac{b_{jk}b_{ki}^{2}}{b_{ji}}\leq b_{ii}^{*}.
$$ Multiplying by $b_{ji}$ on both sides leads to the inequality postulated above. A closely related problem is to show that the spectral radius of $J\mathrm{diag}(\iota-A \iota)$ is lower than one (where $\iota$ is the vector of all ones), see here . (Note that if $v=\iota$ then $J\mathrm{diag}(\iota-A \iota)\iota = J(I-A)\iota = \iota$ and so the spectral radius of $J\mathrm{diag}(\iota-A \iota)$ is one.)","['matrices', 'inequality', 'linear-algebra']"
4388521,spectrum of an element of unital banach algebra is closed set,"Let $A$ be a unital Banach algebra and $a \in A$ . Then the spectrum $\sigma(a)$ and set of invertible elements $\text{Inv}(A)$ are defined as: $$
\begin{align}
&\text{Inv}(A):=\{a\in A~|~a \text{ is invertible}\}\\
&\sigma(a):=\{\lambda \in \mathbb C~|~\lambda 1-a \notin \text{Inv}(A)\}.
\end{align}
$$ I want to show that $\sigma(a)$ is closed subset of $\mathbb C$ . But I already know that $\text{Inv}(A)$ is an open subset of $A$ , since $A$ is a Banach algebra. I know there are many approaches to show that $\sigma(a)$ is closed, but I want to show by assuming $\text{Inv}(A)$ is an open subset of $A$ . Taking the set $\mathbb C-\sigma(a)$ , we have to show that this set is open provided $\text{Inv}(A)$ is an open. Can you please give me some hint?","['operator-theory', 'functional-analysis', 'banach-algebras', 'operator-algebras']"
4388661,"Existence of bases for $L^2(X,\mu )$ with special properties.","Let $X$ be a compact metric space and let $\mu $ be a Borel probability measure on $X$ .  The following questions regard
the existence of
orthonormal bases  for
the complex  Hilbert space $L^2(X,\mu )$ with special properties. Does $L^2(X,\mu )$ admit a basis $\{f_i\}_i$ formed by continuous functions with $|f_i(x)|=1$ , for every $i$ and $x$ . Does $L^2(X,\mu )$ admit a basis $\{f_i\}_i$ with $|f_i(x)|=1$ , for every $i$ and almost every $x$ . Does $L^2(X,\mu )$ admit a basis $\{f_i\}_i$ formed by continuous functions with $\sup_{i, x}|f_i(x)|<\infty $ . Does $L^2(X,\mu )$ admit a basis $\{f_i\}_i$ with $\sup_i\|f_i\|_\infty <\infty $ . The inexistence of atoms for $\mu$ might be relevant, so you are welcome to assume this in case it helps. If you need motivation for this question, note that for every compact abelian group $G$ , with normalized Haar measure, the answer to (1) is affirmative, with the group characters forming a basis.","['hilbert-spaces', 'measure-theory', 'functional-analysis']"
4388666,What does the pipe symbol mean in an equation?,"Currently reading through an algorithm and in the algorithm there is an array of random numbers ( $r_1$ , $r_2$ , $r_3$ , and $r_4$ ). The criteria for each number in the array is { $r_n ∈ R \mid 0 ≤ r_n ≤ 1$ }. Which I presume means each value of the array needs to be a real number but I can't work out what the pipe symbol means, does it mean an OR or does it mean it must be a number AND between 0 & 1?","['elementary-set-theory', 'notation', 'computer-science']"
4388719,A Càdlàg non-negative submartingale is class DL,"Let X = $\{X_t\}_t$ be a right-continuous sub-martingale w.r.t. its natural filtration, and assume $X \geq 0 $ a.s. I am trying to show that X is of class DL. According to my definition, this means the family $\{X_T\}_{T \in J_a}$ is uniformly continuous, where $J_a = \{T \text{ is a stopping time} : \mathbb{P}(T \leq a) = 1  \}$ . I proceeded via the optional stopping theorem for bounded stopping times. So, $\mathbb{E}(X_T1_{\{X_T>\lambda\}}) \leq \mathbb{E}(X_a1_{\{X_T>\lambda\}})$ for any $ a> 0, \lambda >0, T \in J_a$ . And by Markov's inequality, $\mathbb{P}(X_T > \lambda) \leq \frac{\mathbb{E}(X_T)}{\lambda} \leq  \frac{\mathbb{E}(X_a)}{\lambda} $ How can I then conclude $ \lim_{\lambda\to\infty} \sup_{T \in J_a} \int_{\{X_T > \lambda\} }X_T d\mathbb{P} = 0    $ ?","['stochastic-processes', 'martingales', 'stopping-times', 'probability-theory', 'stochastic-calculus']"
4388729,How do we naturally extend the average from the Hausdorff Measure for functions with a domain without a gauge function?,"Motivation: According to this question Some sets have a Hausdorff Dimension $\alpha$ but have a
zero-dimensional Hausdorff Measure. These sets may have another dimension function , i.e. a function $h:[0,\infty]\to[0,\infty]$ such that if we change the definiton of the Hausdorff Measure by
replacing $R^{\alpha}$ with $h(R)$ (where $R$ denotes the radius of a
ball in covering), the value of the Hausdorff Measure is positive and
finite. Unfortunately, there are sets with no meaningful gauge function since the sets are either $\sigma$ -finite with respect to the counting measure (i.e. Countably infinite sets) or their gauge function does not exist ( see the examples in Dave L. Renfro's answer ). Despite this, I need a unique and natural extension of the Hausdorff Measure to be positive and finite for sets with no meaningful gauge function (see the previous paragraph) and for all “nice” sets (i.e. sets in the $\sigma$ -algebra of Caratheodory measurable sets) so the average of the function (i.e. the integral of function w.r.t to the extended measure of the domain, divided by the extended measure of the domain) exists. Question: How do we naturally extend the average from the Hausdorff Measure and Integral w.r.t the Measure
to exist for all functions with a domain without a gauge function but in the $\sigma$ -algebra of Caratheodory-Measurable sets? More specifically, the extended average must be unique and defined between the infimum and supremum of the function we are averaging over. Does such an average exist? Edit: Potential Answers See Integration with Filters . This appears to go after the problem stated but the average isn’t unique as is with most integration which use filters. Despite this, there exists a unique minimal filter (in Theorem 7) where its average is compatible with the Hausdorff Measure. Edit 2 : Another answer is from Non-Standard Measure Theory: Hausdorff Measure . This again uses filters and extends the Hausdorff Measure; however, the extension isn’t unique. Edit 3 : Perhaps we could create a choice function
which chooses a specific minimal filter for the filter integral (see Theorem 7 of Integration with Filters ) so the average from the filter integral, for functions with domains that have a gauge function, matches the average from the Hausdorff Measure and Integral as well as give a unique average for functions with a domain with no gauge function but in the $\sigma$ -algebra of Caratheodory-measurable sets. Question 2: Does such a choice function which is natural exist? What is an example?","['measure-theory', 'hausdorff-measure', 'nonstandard-analysis', 'average', 'filtrations']"
4388763,Link between Riesz p-variation and absolute continuity,"Fix $p\in(1,+\infty)$ and for $f:[0,1]\to\mathbb{R}$ define $$
V_p(f)=\sup \sum_{i=0}^{n-1}\frac{|f(t_{i+1})-f(t_i)|^p}{(t_{i+1}-t_i)^{p-1}} 
$$ where the sup is taken over all $n\in\mathbb{N}$ and among all possible partitions $0=t_0<t_1<\dots<t_n=1$ . I have to prove that $V_p(f)<+\infty$ if and only if $f$ is absolute continuous with weak derivative in $L^p([0,1])$ . If $f$ is absolutely continuous with weak derivative in $L^p([0,1])$ , that it suffices to apply Holder inequality and easily it follows that $$
\sum_{i=0}^{n-1}\frac{|f(t_{i+1})-f(t_i)|^p}{(t_{i+1}-t_i)^{p-1}}
\leq
\|f'\|_p^p.
$$ However I have thought a lot about the other implication but I really do not know how to do it. Does anyone know how to prove that?","['p-variation', 'measure-theory', 'lp-spaces', 'absolute-continuity']"
4388787,Limit measure is zero?,"Let $E\subset \mathbb{R}$ be a Borel set and suppose that $(f_k)_{k\in\mathbb{N}}$ is a sequence of Lebesgue measurable functions defined on $E$ .
Assume that $\lim_{k\to+\infty}f_k(x)=f(x)$ for almost every $x$ and that there exists a Lebesgue integrable function $g$ such that $|f_k(x)|\leq g(x)$ for almost every $x\in E$ and every $k\in\mathbb{N}$ . Fix $\varepsilon >0$ and define $A_k^\varepsilon=\{x\in E : |f_k(x)-f(x)|\geq\varepsilon\}$ . Is it true that $$
\lim_{j\to+\infty}\left|\bigcup_{k\geq j} A_k^\varepsilon \right| =0 \quad ?
$$ ( $|\cdot|$ is the Lebesgue measure) The only thing that I noticed is that $|A_k^\varepsilon|\xrightarrow[k\to+\infty]{}0$ , as a consequence of Lebesgue's dominated convergence theorem. Does anyone know if this is true or not (and why?)?","['limits', 'measure-theory', 'lebesgue-measure', 'real-analysis']"
4388824,Concepts behind augmented matrix,"Apparently the strategy for finding a general solution to the equation $Ax = v$ , where $A$ is some $m\times n$ matrix, $x$ is an unknown vector of dimension n, and $v$ is not the zero vector, is to turn $v$ into an extra column of A (resulting in an augmented matrix, $aA$ ) and then simply proceed with the usual technique on $aA$ by putting $aA$ into row-echelon form. But why does this work ? If I try to answer this question myself, the answer I give is because $Ax = v$ represents a system of equations, and moving $v$ from the right hand side of this system to the left does not change it. But in order to actually do that I think you'd have to subtract $v$ from both sides of the system, which would mean you should have $-v$ as the extra column in $aA$ and not $v$ ; but this is not how it works. Where does my reasoning go wrong? What is the correct explanation for why performing row operations on $aA$ gives you the general solution to $A$ ?","['matrices', 'systems-of-equations', 'linear-algebra']"
4388854,Contradictory results from using orthogonality when solving an ODE with Fourier series.,"I am trying to solve the following ordinary differential equation by assuming that the function $f(x)$ is in the form of a Fourier series, in the bounds $-L < x < L$ . $$ \frac{\mathrm{d}f}{\mathrm{d}x} = f(x) \qquad f(x) = \frac{A_0}{2} + \sum_{n=1}^\infty \left[A_n\cos\left(\frac{n\pi x}{L}\right) + B_n\sin\left(\frac{n\pi x}{L}\right)\right] $$ My solution involves first determining the derivative $\frac{\mathrm{d}f}{\mathrm{d}x}$ from the assumed Fourier series form of the solution and then substituting into the original equation. $$ \frac{\pi}{L}\sum_{n=1}^\infty n\left[-A_n\sin\left(\frac{n\pi x}{L}\right) + B_n\cos\left(\frac{n\pi x}{L}\right)\right] = \frac{A_0}{2} + \sum_{n=1}^\infty \left[A_n\cos\left(\frac{n\pi x}{L}\right) + B_n\sin\left(\frac{n\pi x}{L}\right)\right] $$ From here I apply orthogonality with $\cos\left(\frac{n\pi x}{L}\right)$ and $\sin\left(\frac{n\pi x}{L}\right)$ with weight function $r(x) = 1$ between $-L < x < L$ , the necessary integral solutions shown below. $$ \int_{-L}^L\!\cos\left(\frac{n\pi x}{L}\right)\cos\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = \delta_{nm}L \qquad \int_{-L}^L\!\sin\left(\frac{n\pi x}{L}\right)\sin\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = \delta_{nm}L $$ $$ \int_{-L}^L\!\sin\left(\frac{n\pi x}{L}\right)\cos\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = 0 \qquad \int_{-L}^L\!\cos\left(\frac{n\pi x}{L}\right)\sin\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = 0 $$ $$ \int_{-L}^L\!\cos\left(\frac{n\pi x}{L}\right)\,\mathrm{d}x = 0 \qquad \int_{-L}^L\!\sin\left(\frac{n\pi x}{L}\right)\,\mathrm{d}x = 0 $$ Applying these orthogonality conditions to the original equation $$ \text{orthogonality with} \quad \cos\left(\frac{n\pi x}{L}\right), \quad \frac{\pi}{L}\sum_{n=1}^\infty n\left[0 + L\delta_{nm}B_n\right] = 0 + \sum_{n=1}^\infty\left[L\delta_{nm}A_n + 0\right] $$ $$ n\pi B_n = LA_n \qquad B_n = \frac{L}{n\pi}A_n $$ $$ \text{orthogonality with} \quad \sin\left(\frac{n\pi x}{L}\right), \quad \frac{\pi}{L}\sum_{n=1}^\infty n\left[-L\delta_{nm}A_n + 0\right] = 0 + \sum_{n=1}^\infty\left[0 + L\delta_{nm}B_n\right] $$ $$ -n\pi A_n = LB_n \qquad B_n = -\frac{n \pi}{L}A_n $$ These results are contradictory, or to be more specific orthogonality with the cosine seems to give an incorrect result, since derivation of the Fourier series coefficients for the solution to this differential equation, $f(x) = Ae^x$ , shows that $B_n = -\frac{n \pi}{L}A_n$ . Have I made a mistake in my working or am I not understanding something about orthogonality? I may have made some mistakes in typing this up, I have double checked everything but I cannot be sure. I have however done this derivation through many times with pen and paper and always come to these contradictory results.","['fourier-series', 'fourier-analysis', 'ordinary-differential-equations']"
4388917,Confusion about an answer - why does this element have to be unique?,"I'm reading this answer , and I'm stuck at ""... the unique element that some elements of the chain map it to"". Why does this element have to be unique? For example, consider the chain of functions $\mathbb N\to \mathbb N$ under the order described in the answer the first few  elements of which are described as follows: $f_1$ has fixed points $0,1,2,3$ (and only them) $f_2$ has fixed points $0,1,3$ (and only them) $f_3$ has fixed point $1$ (and only them) The condition $f_2\leq f_3$ means $\{1\}\subseteq \{0,1,3\}$ and the restriction of $f_2$ to $\mathbb N\setminus \{0,1,3\}$ equals the restriction of $f_3$ to the same set. In particular, it might be the case that $f_3(0)=15$ whereas $f_2(0)=0$ . If I call the function that is being constructed in the answer $L$ , what should be $L(0)$ ? Should it be $f_3(0)$ or should it be $f_2(0)$ , or something else? Also, just to make sure I understand the other part of the definition of $L$ correctly, the fixed points of $L$ in my example should just contain $1$ (since it's the only point where ""all elements of the chain have fixed points"")?","['elementary-set-theory', 'proof-explanation', 'functions', 'examples-counterexamples']"
4388947,is this a combination or permutation problem?,"I have see the following problem in a Discrete Maths book and it says: In the first case the author solves it like a permutation, so he multiply eight three times which is like using the formula $$P(n,k)=n^{k}$$ obtaining the result of $$n^{k}=8^{3}=512$$ samples of 3 with repetition. However, for me it seems that is a combination problem, because the order is irrelevant in this case; so I could use the following formula: $$\binom{8+3-1}{3}=120$$ samples of 3 students with repetition allowed In the same way for option (b) the author uses the product of $$8.7.6$$ which is like using the formula of permutation without repetition giving 336 samples of 3 students. However, I believe that in this case also the order is not relevant so one could use the formula: $$\binom{8}{3}=56$$ samples of 3 students without repetition. Any thoughts about this trivial problem? Thanks","['permutations', 'combinatorics']"
4388974,Finding $\int_0^\pi\frac{f\left(\alpha+e^{ix}\right)+f\left(\alpha+e^{-ix}\right)}{1+2p\cos(x)+p^2}\mathrm dx$,"Let $f$ be a analytic function in the closed unit circle with its center
at the point $\alpha\in\mathbb{R}$ , then: \begin{equation*}
\int_0^\pi\frac{f\left(\alpha+e^{ix}\right)+f\left(\alpha+e^{-ix}\right)}{1+2p\cos(x)+p^2}\mathrm dx=\frac{2\pi}{1-p^2}f(\alpha+p)	,
\end{equation*} for $|p|<1$ . My attempt: By \begin{align*}
\therefore\quad \sum_{n=1}^{\infty}p^{n}\sin(nx)=\frac{p\sin (x)}{1-2p\cos (x)+p^2},\qquad|p|<1
\end{align*} adjusting $p\to -p$ and highlighting $\displaystyle \frac1{1+2p\cos(x)+p^2}$ : \begin{align*}
\frac1{1+2p\cos(x)+p^2}=-\frac{1}{\sin(x)}\sum_{n=1}^\infty(-p)^{n}\sin(nx).
\end{align*} Thus: \begin{align*}
\int_0^\pi\frac{f\left(\alpha+e^{ix}\right)+f\left(\alpha+e^{-ix}\right)}{1+2p\cos(x)+p^2}\mathrm dx=-\sum_{n=1}^\infty(-p)^{n}\int_0^\pi\left\{f\left(\alpha+e^{ix}\right)+f\left(\alpha+e^{-ix}\right)\right\}\frac{\sin(nx)}{\sin(x)}\mathrm dx\tag{1}
\end{align*} by the \textit{Dirichlet Kernel}: $\displaystyle \sum_{k=0}^{N-1}e^{2ikx}=e^{(N-1)x}\frac{\sin(Nx)}{\sin(x)}$ setting $N\to n$ , $n\in\mathbb{N}$ and then taking $(1)$ , follows that: \begin{align*}
&=-\sum_{n=1}^\infty(-p)^{n}\sum_{k=0}^{n-1}\int_0^\pi\left\{f\left(\alpha+e^{ix}\right)+f\left(\alpha+e^{-ix}\right)\right\}e^{-(n-1)x}e^{2ikx}\mathrm dx,\quad\left(e^{ix}\to z\right)\\
&=...
\end{align*} At this point I'm out of ideas. I would like some light on my last step, or another approach that is similar to this one.","['integration', 'definite-integrals', 'complex-numbers']"
4389036,Sampling from a high-dimensional multivariate Gaussian distribution when low-rank approximation for the inverse of the covariance matrix is available,"I want to get random samples from a high-dimensional (say, $10^4$ or more) multivariate Gaussian distribution. Assume a case of zero mean for simplicity, the probability density is given as $$P({\bf x})=\frac{1}{\sqrt{(2\pi)^n|{\bf \Sigma}|}} \exp(-\frac{1}{2}{\bf x}^T {\bf \Sigma}^{-1} {\bf x})~~~~~~~~~(1)$$ where ${\bf \Sigma}$ is a $n\times n$ covariance matrix. I here have a low-rank approximation for inverse of the covariance matrix ${\bf Q}={\bf \Sigma}^{-1}$ (i.e., precision matrix, NOT the covariance matrix ${\bf \Sigma}$ itself), $${\bf Q} \simeq {\bf P}^T {\bf P}$$ where ${\bf P}$ is a $n\times r$ matrix where $r < n$ . (Such an approximation can be obtained based on truncated singular value decomposition, for instance) When $r \ll n$ , what is the most efficient way to get the random samples from the Gaussian distribution? At first, I was thinking that simple Markov chain Monte-Carlo methods like the Metropolis-Hastings  (MH) algorithm are directly applicable because $P({\bf x})$ is unimodal and we can relatively easily evaluate $P({\bf x})$ for a given ${\bf x}$ because of fast calculation of ${\bf P}^T {\bf P}$ . However, articles like the one below introduces more complicated sampling algorithms for high-dimensional Gaussian distributions instead of such simple ones. I also want to know why they never tell that application of methods like simple MH algorithm is effective?
(Note that the article do not discuss low-rank approximation) Vono, M., Dobigeon, N., & Chainais, P. (2022). High-dimensional Gaussian sampling: a review and a unifying approach based on a stochastic proximal point algorithm. SIAM Review, 64(1), 3-56.","['statistical-inference', 'statistics', 'linear-algebra', 'sampling']"
4389037,Let $f(x)=\begin{cases}2x+a;&x\ge-1\\bx^2+3;&x\lt-1\end{cases}$ and $g(x)=\begin{cases}x+4;&0\le x\le4\\-3x-2;&-2\lt x\lt0\end{cases}$ then $g(f(x))$,"Let $$f(x)=\begin{cases}2x+a;&x\ge-1\\bx^2+3;&x\lt-1\end{cases}$$ and $$g(x)=\begin{cases}x+4;&0\le x\le4\\-3x-2;&-2\lt x\lt0\end{cases}$$ then $g(f(x))$ is not defined if $(1)\;a\in(10,\infty),b\in(5,\infty)$ $(2)\;a\in(4,10),b\in(5,\infty)$ $(3)\;a\in(10,\infty),b\in(0,1)$ $(4)\;a\in(4,10),b\in(1,5)$ My Attempt: $$g(f(x))=\begin{cases}f(x)+4;&0\le f(x)\le4\\-3f(x)-2;&-2\lt f(x)\lt0\end{cases}$$ So, $g(f(x))$ will not be defined if $f(x)\gt4$ or $f(x)\le-2$ Also, $f(x)=2x+a$ is an increasing function. So, its minimum value is at $x=-1$ i.e. $a-2$ For $g(f(x))$ to be not defined $a-2\gt4\implies a\gt6$ Also, $f(x)=bx^2+3$ is a parabola. If $b\lt0$ then it's a downward parabola, and its maximum value is $b+3$ . For $g(f(x))$ to be not defined, $b+3\le-2\implies b\le-5$ (is this correct?) If $b\gt0$ then $f(x)$ is an upward parabola and its minimum value is $b+3$ . For $g(f(x))$ to be not defined, $b+3\gt4\implies b\gt1$ Accordingly, I think option $(1)$ is correct. But is there a way to find the exhaustive domain of $a?$ In the hint, they have written $-2+a\gt8, b+3\gt8$ Seems like they have calculated the value of $g(x)$ at $x=4$ and then compared it with the values of $f(x)$ . But shouldn't we be comparing the domain (and not the range) of $g(x)$ with the range of $f(x)?$","['inequality', 'functions', 'conic-sections', 'piecewise-continuity']"
4389048,For what $\beta$ does the geometric series $I + A + A^2 + A^3 + \cdots$ converge to $(I-A)^{-1}$,"Problem. Consider the matrix $A$ below. $$A = \begin{bmatrix} {\beta\over 2} & 0\\ {\beta\over 2} & \beta
 \end{bmatrix}$$ For what $\beta$ does the sequence $I + A + A^n + \cdots$ converge to $(I-A)^{-1}$ ? How quickly does the sequence converge, as a function of $\beta$ ? As we're only in the second week of an undergraduate course and haven't yet covered them, I'm not able to make use of matrix norms (in which case clearly it must be that $lim_{n\to\infty} |A_n| = lim_{n\to\infty} A_n = 0$ , yielding $0<\beta<2$ by examination of the general form of $A^n$ ), and I'm not quite sure how else to approach this. Here's what I have so far. Notice first that the general form of $A^n$ seems to be given by: $$A^n = \begin{bmatrix}{\beta^n\over 2^n} &0\\ {\beta^n \over 2^n}(2^n-1)& \beta^n\end{bmatrix}$$ Notice also that $(I-A^n)^{-1}$ is then given by: $$\Bigg(\begin{bmatrix}1 & 0\\0 & 1\end{bmatrix} - \begin{bmatrix}{\beta^n\over 2^n} &0\\ {\beta^n \over 2^n}(2^n-1)& \beta^n\end{bmatrix}\Bigg)^{-1} = \begin{bmatrix}1 + {\beta^n\over 2^n} & 0 \\ ({\beta\over 2})^n(2^n-1) & \beta^n+1\end{bmatrix}^{-1} = \begin{bmatrix} {2^n\over {2^\beta +\beta^n}} & 0\\ {{\beta^n(1-2^n)}\over (2^n\beta^n)(\beta^n+1)} & {1\over \beta^n +1}\end{bmatrix}$$ Now, if we look at the sum $A + A^2 + A^3 + \ldots$ , it looks like: $$I+A = \begin{bmatrix}
{\beta+2\over 2} & 0\\
{\beta\over 2} & \beta+1
\end{bmatrix}\;\;, \;\; I+A+A^2 = 
\begin{bmatrix}
{\beta^2 + 2\beta +4\over 4} & 0\\
{\beta(3\beta+2)\over 4} & \beta^2+\beta+1\\
\end{bmatrix}$$ It isn't clear to me that this series is approaching $(I-A^n)^{-1}$ , nor how one would determine for what $\beta$ this holds. Moreover, it isn't clear what is meant by ""how quickly"" the series converges, nor how one would determine that as a function of $\beta$ . Where do I go from here?","['matrices', 'matrix-equations', 'linear-algebra', 'sequences-and-series']"
4389052,Other approach to find $f(x)$ when $f(x+y)=2^xf(y)+4^yf(x)$,"Consider a differentiable function $f:\mathbb R\to\mathbb R$ for which $f(1)=2$ and $f(x+y)=2^xf(y)+4^yf(x)\;\forall\;x,y\in\mathbb R$ , find the minimum value of $f(x)$ . One approach is to replace $x$ and $y$ , subtract the two equations, use the initial condition and obtain $f(x)=4^x-2^x$ . Taking derivative or completing the whole square, we can get the minimum value. But I wonder if there is any other approach to solve it. Maybe taking derivative of the initial equation keeping one variable as constant or maybe using the first principle. e.g. $$f'(x)=\lim_{y\to0}\frac{f(x+y)-f(x)}{y}\\=\lim_{y\to0}\frac{2^xf(y)+f(x)(4^y-1)}{y}$$ Can we finish this apporach?","['calculus', 'functions', 'derivatives']"
4389061,Evaluate $I=\int_{0}^{1}\frac{x^2-x}{(x+1)\ln{x}}dx$,"I am trying to calculate this integral: $$I=\int_{0}^{1}\frac{x^2-x}{(x+1)\ln{x}}dx$$ .
I tried to find the antiderivative but it didn't exist.
So i changed variable by set $t=\frac{x-1}{x+1}$ due to factor numerator is $x(x-1)$ and it led to: $$I=2\int_{-1}^{0}\frac{t^2+t}{(1-t)^3\ln{\frac{t+1}{1-t}}}dx$$ and it seems more harder.
The result from Wolfram Alpha is ok, but i don't know how to evaluate this result. Need some hints or advices from everyone. Thank you.","['integration', 'calculus']"
4389067,Prove or Disprove $\lim_{n\to \infty}\int_{0}^{n}\int_{0}^{n}\frac{1}{(x^n+y^n+1)^n}\mathrm{d}x\mathrm{d}y=1$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I was experimenting with the value some basic double integral(Polar) by changing powers of $x$ and $y$ , limits In the end I observed that as we increase $n$ the value of the following limit approaches 1. $$\lim_{n\to \infty}\int_{0}^{n}\int_{0}^{n}\frac{1}{(x^n+y^n+1)^n}\mathrm{d}x\mathrm{d}y=1$$ I don't know where to start evaluating the limit. Can somebody help me?","['integration', 'limits', 'calculus', 'definite-integrals']"
4389072,Prove $\neg (\;(\;\neg \; p \;\land q \;) \lor (\; \neg \; p \; \land \neg \; q)) \equiv p$ using logical equivalence,"I am a student in a first year discrete mathematics course at university, I have found the following problem within a practice paper. Let $p$ and $q$ be statement variables, prove that $\neg (\;(\;\neg \; p \;\land q \;) \lor (\; \neg \; p \; \land \neg \; q)) \equiv p$ using the laws of logical equivalence. I have included my attempt below. To me, it seems correct but I have been informed that I have made an error. Could someone please be kind enough to point it out for me. $\neg (\;(\;\neg \; p \;\land q \;) \lor (\; \neg \; p \; \land \neg \; q)) $ $\equiv \neg \; (\neg \; p \; \land \; (q \; \lor \neg \;q))$ $\equiv \neg \; (\neg \; p \; \land \; \pmb t ) $ $ \equiv \neg \; ( \; \neg \;p) $ $\equiv p$","['logic', 'discrete-mathematics']"
4389162,"For the function $f(x,y)=|xy|^{p}$ , find the value of $p$ for which $f$ is differentiable at $(0,0)$.","MY ATTEMPT: \begin{equation}
fx(0,0)= \lim_{h\to 0}\frac{f(0+h,0)-f(0,0)}{h}=0
\end{equation} \begin{equation}
fy(0,0)= \lim_{k\to 0}\frac{f(0,0+k)-f(0,0)}{k}=0
\end{equation} By Using the definition of differentiability for the functions of two variables: \begin{equation}
 \lim_{(h,k)\to (0,0)}\frac{f(a+h,b+k)-f(a,b)-hf_x(a,b)-kf_y(a,b))}{{\sqrt{h^2+k^2}}}
\end{equation} \begin{equation}
\lim_{(h,k)\to (0,0)}\frac{|hk|^p}{\sqrt{h^2+k^2}}=\text{0 if and only if p=2n where n=1,2,3..} 
\end{equation} so, $p=2n (n=1,2,3\ldots)$ PLEASE CHECK whether I attempted this question correctly or not and if there is any error please give suggestions to resolve it. Thank you.","['multivariable-calculus', 'calculus', 'real-analysis']"
4389163,Holomorphic tangent bundle over projective line,"I know that the holomorphic tangent bundle to the complex projective line $T^{1,0}\mathbb{CP}^1$ is isomorphic to $\mathcal{O}(2)$ , i.e. the twofold tensor product of the hyperplane bundle of $\mathbb{CP}^1$ . Question : how do I identify under this correspondence the global holomorphic vector fields as sections of this $\mathcal{O}(2)$ ? Setup : the isomorphism between these line bundles can be seen by giving $\mathbb{CP}^1$ the homogeneous atlas given by $$
U_0=\{z_0\neq 0\}\ni (z_0:z_1) \mapsto \frac{z_1}{z_0}=z \in \mathbb{C}
$$ $$
U_1=\{z_1\neq 0\}\ni (z_0:z_1) \mapsto -\frac{z_0}{z_1}=w \in \mathbb{C}
$$ thus the change of coordinates from $z$ to $w$ reads $-\frac{1}{z}$ and the jacobian, which gives the transition functions for $T^{1,0}\mathbb{CP}^1$ is $$
g_{01}(z) = \frac{1}{z^2} \equiv \left(\frac{z_0}{z_1}\right)^2
$$ This is precisely the transition function of $\mathcal{O}(2)$ ! I already know that global holomorphic sections correspond to homogeneous polynomials in two variables $$H^0(\mathbb{CP}^1,\mathcal{O}(2))\simeq \langle Z_0^2, Z_0Z_1, Z_1^2\rangle$$ As far as I know, the ""derivatives"" $\frac{\partial}{\partial z},\frac{\partial}{\partial w}$ give global vector holomorphic vector fields over $\mathbb{CP}^1$ . But what is the third global holomorphic vector field? How can it be linearly independent from these derivative fields?","['complex-geometry', 'projective-space', 'differential-geometry']"
4389180,Consistently estimate the covariance matrix with weakly correlated observations,"Suppose there are T k-dimensional observations following the generating process: $Y_t = \mu + \epsilon_t$ , where $\mu$ is the mean and $\epsilon$ is a weak stationary error with zero mean and time-invariant covariance matrix $\Sigma = E(\epsilon_t\epsilon_t')$ for all t. The goal is to estimate $\Sigma$ by empirical covariance $\frac{1}{T}(Y_t - \mu_Y)(Y_t - \mu_Y)'$ , where $\mu_Y$ is the empirical mean of $Y$ . However, we allow some kind of time dependence in the error term. And the question is under which weak correlation can this estimator be consistent for $\Sigma$ ? I know there is a classic result about weakly correlated random variables and for mean-estimation: if $X_1, X_2,\dots, X_n$ are random variables with the same means and $\operatorname{corr}(X_m, X_{m+t}) \to 0$ as $t \to \infty$ , then the empirical mean $\frac{1}{n}(X_1+X_2+\dots +X_n) \to E(X) $ in probability. But in the vector case and for second-moment estimation, I find it hard to set a good condition, maybe uniformly convergence to zero of all entries of correlation variance should be enough? And it seems a basic problem maybe in time series study or general statistics. I'm wondering if anyone has seen this problem, references or direction are appreciated.","['statistics', 'estimation', 'parameter-estimation', 'time-series', 'law-of-large-numbers']"
4389329,How to prove $2\sum_{k=1}^n{(-1)^{k-1} \cos\frac{k\pi}{2n+1}} = 1$,"Prove $$2\sum_{k=1}^n{(-1)^{k-1} \cos\frac{k\pi}{2n+1}} = 1$$ I got this question when I was going through some basic trigonometric identities as follows $2(\cos\frac{\pi}{5}-\cos\frac{2\pi}{5}) =1\tag1$ very much straightforward to recognise $2(\cos\frac{\pi}{7}-\cos\frac{2\pi}{7} + \cos\frac{3\pi}{7})=1\tag2$ Following steps proves the identity $8\cos\frac{\pi}{7}\cdot\cos\frac{2\pi}{7}\cdot\cos\frac{4\pi}{7} = -1$ $4\cos\frac{\pi}{7}[2\cos\frac{4\pi}{7}\cdot\cos\frac{2\pi}{7}] = -1$ $4\cos\frac{\pi}{7}[\cos\frac{6\pi}{7}+ \cos\frac{2\pi}{7}] =-1    $ $4\cos\frac{\pi}{7}[\cos\frac{2\pi}{7}-\cos\frac{\pi}{7}] =-1    $ $4\cos\frac{\pi}{7}\cdot\cos\frac{2\pi}{7} - 4\cos^{2}\frac{\pi}{7} = -1$ Further simplification will lead to equality 2 $2(\cos\frac{\pi}{9}-\cos\frac{\pi}{9} + \cos\frac{3\pi}{9}- \cos\frac{4\pi}{9}) = 1$ by using transformation formula we can prove above one also But when it comes to following equalities $2(\cos\frac{\pi}{11}-\cos\frac{2\pi}{11} + \cos\frac{3\pi}{11}- \cos\frac{4\pi}{11}+ \cos\frac{5\pi}{11} )= 1$ $2(\cos\frac{\pi}{13}-\cos\frac{2\pi}{13} + \cos\frac{3\pi}{13}- \cos\frac{4\pi}{13}+ \cos\frac{5\pi}{13} -\cos\frac{6\pi}{13} )= 1$ I was able to check the results with brute force in Wolfram|Alpha for above equalities, but not able to get the steps properly even though I tried manually My question is how to generalise the summation formula and is there any method other than trigonometric approach? $$2\sum_{k=1}^n{(-1)^{k-1} \cos\frac{k\pi}{2n+1}} = 1$$","['trigonometry', 'summation']"
4389344,Intuition for the variables $x_i$ that minimise $\sum_i a_i x_i^2$ with constraint $\sum_i x_i = 1$,"Given $a_1, \dots, a_n \in \mathbb R_{\geq0}$ , we wish to choose $x_1, \dots, x_n \in \mathbb R_{\geq 0}$ that minimise $\sum_i a_ix_i^2$ with the constraint $\sum_i x_i = 1$ . This can be solved with Lagrange multipliers the tedious way. But it turns out that the optimal set of $x_i$ s satisfies $a_1x_1 = \dots = a_nx_n$ . Is there an intuitive justification (perhaps statistical or geometric) for why this holds? If so, the problem could be solved by simply observing that the minimum should satisfy $a_1x_1 = \dots = a_nx_n$ , from which the solution falls out trivially. (This problem arises when constructing a minimum variance fully-invested portfolio where all assets are uncorrelated, for example.)","['statistics', 'finance', 'lagrange-multiplier', 'maxima-minima', 'optimization']"
4389363,"To find $n$ such that the expansion of $(1+x)^n$ has three consecutive coefficients $p,q,r$ that satisfy $p:q:r = 1:7:35$.","To find $n$ such that the expansion of $(1+x)^n$ has three consecutive coefficients $p,q,r$ that satisfy $$p:q:r = 1:7:35$$ My work: Suppose the consecutive coefficients are $\binom{n}{k-1}, \binom{n}{k}$ and $\binom{n}{k+1}$ . Then we have $$\dfrac{\binom{n}{k-1}}{\binom{n}{k}} = \frac{k}{n-k+1}= \frac17$$ and $$\dfrac{\binom{n}{k}}{\binom{n}{k+1}} = \frac{k+1}{n-k}= \frac{7}{35}= \frac15$$ Cross multiplying we have $7k = n-k+1 \implies n = 8k-1$ and $5k+5=n-k \implies n = 6k+5$ . Thus $8k-1=6k+5 \implies k = 3$ and hence $n = 23$ . Is my argument fine?","['recreational-mathematics', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'algebra-precalculus']"
4389380,Limits of $u$ and $v$,"I am trying to evaluate a double integral $$I=\int_{0}^2\int_{0}^{2-x}(x+y)^2e^{\frac{2y}{x+y}}dydx$$ I used the transformation $$x+y=v, y=uv$$ That is $$x=v(1-u), y=uv$$ We get the Jacobian as: $$J=\left|\begin{array}{ll}
\frac{\partial x}{\partial u} & \frac{\partial y}{\partial u} \\
\frac{\partial x}{\partial v} & \frac{\partial y}{\partial v}
\end{array}\right|=\left|\begin{array}{cc}
-v & 1-u \\
v & u
\end{array}\right|=-v$$ So we get: $$I=\iint_{D(u,v)}v^3e^{2u}dudv$$ I am unable to figure out the limits of $u$ and $v$","['jacobian', 'multivariable-calculus', 'change-of-variable', 'multiple-integral']"
4389436,"Distinguishability of Continuous Probability Distributions, Statistical Distance and Region of Uncertainty","I am reading a dissertation on quantum metrology in which the second chapter deals with the notion of statistical distance. Kullback–Leibler divergence (relative entropy) is introduced as the measure of distance between probability distributions. The author gives a lemma (based on previously given lemmas ad theorems) which asserts that the probability of a type (I think in most references this is referred to as ""frequency"") $\xi$ generated according to the distribution $p$ can be approximated for large number of trials $n$ as: $$p(\xi) \approx Ce^{-nD(\xi || p)}$$ in which $D(\xi || p)$ is the relative entropy and $C$ is a constant. In order to define a Riemmanian metric on the space of probabilities the KL-divergence of $\xi=p+\delta p$ and $p$ is calculated which after approximation becomes: $$D(p+\delta p||p) \approx \frac{1}{2} \sum_a \frac{(\delta p_a)^2}{p_a}$$ Plugging this approximation in the formula for the probability of type $\xi$ we get: $$p(\xi) \propto exp(-\frac{n}{2}\sum_a \frac{(\delta p_a)^2}{p_a}) \qquad (*)$$ I understand everything up to this point. The author claims that for a neighboring probability vector $\tilde{p}=p+\delta p$ to be distinguishable from $p$ , the probability of getting a type $\xi \approx \tilde{p}$ must be low. Hence, the neighboring states will be distinguishable if: $$\sqrt{n}(\sum_a \frac{(\delta p_a)^2}{p_a})^{\frac{1}{2}}>1$$ and The region of probability space where, according to this equation, probabilities are not distinguishable, is called the region of uncertainty. I don't understand the last equation. We can rewrite the last equation as: $$\frac{n}{2}\sum_a \frac{(\delta p_a)^2}{p_a}>\frac{1}{2}$$ So basically the exponent in the $(*)$ equation must be greater than $\frac{1}{2}$ . I don't understand what is so special about $\frac{1}{2}$ . My understanding is that if two probability distributions are distinguishable, their KL-divergence must be greater than zero. In which case the probability given by $(*)$ will be smaller than one. It is very likely that I don't see a very trivial thing here. Any help is appreciated. P.S. for those of you who have access to ProQuest, the dissertation is titled ""Nonlinear quantum metrology"" by Sergio Boixo and my question is related to page 17 of this document.","['entropy', 'probability-distributions', 'metric-spaces', 'manifolds', 'probability']"
4389468,Confusion about topology on space of maps,"In this set of notes on compactly generated spaces by Charles Rezk, the author states the following result: Proposition 8.6. If $X$ and $Y$ are k-spaces and $Z$ is any space, then a function $f: X \times^k Y \to Z$ is continuous if and only if its adjoint $\widetilde{f}: X \to \mathrm{Map}(Y,Z)$ is defined and continuous. Here, $X \times^k Y$ denotes the compactly generated product, that is, the $k$ -ification of the usual cartesian product of topological spaces, and the topology on the set of maps $\mathrm{Map}(Y,Z)$ is defined as follows: a subset $U \subseteq \mathrm{Map}(Y,Z)$ is open if and only if, for every compact Hausdorff space $K$ and every map $t: K \times Y \to Z$ , the set $\widetilde{t}(U)$ is open in $K$ , where $\widetilde{t}: K \to \mathrm{Map}(Y,Z)$ is the adjoint function of $t$ . I have a question about the forward direction of the statement.
The proof is as follows: The adjoint $\widetilde{f}$ really takes values in $\mathrm{Map}(Y,Z)$ , because $\widetilde{f}(x): Y \to Z$ can be written as the composition $f \circ i_x$ , where $i_x: Y \to X \times^k Y$ is the map defined as $i_x(y)= (x,y)$ for every $y \in Y$ . Notice that $i_x$ is continuous when considering $X \times^k Y$ as the codomain because it can be regarded as the map induced by the constant map $Y \to X$ sending every point to $x$ and the identity $\mathrm{id}_Y: Y \to Y$ .
Here we are using the fact that $\times^k$ is the categorical product in the category $\mathsf{CG}$ of compactly generated spaces and continuous maps, so it is clear that both $X$ and $Y$ need to be compactly generated. If $U \subseteq \mathrm{Map}(Y,Z)$ is open, and $t: K \to X$ is a map, with $K$ compact and Hausdorff, then we have the equality $t^{-1}(\widetilde{f}^{-1}(U)) = \widetilde{g}^{-1}(U)$ , where $g = f \circ (t \times \mathrm{id}_Y)$ .
This is essentially the naturality of the exponential adjunction. $t \times \mathrm{id}_Y$ can be seen as a map of type $K \times Y \to X \times^k Y$ because the usual cartesian product $K \times Y$ is already compactly generated.
This means that $g$ is a map of type $K \times Y \to Z$ , so $\widetilde{g}^{-1}(U)$ is open in $K$ by the definition of the topology on $\mathrm{Map}(Y,Z)$ . This shows that $\widetilde{f}^{-1}(U)$ is compactly open in $X$ , but since $X$ is compactly generated by  hypothesis, it follows that $\widetilde{f}^{-1}(U)$ is open. My question is: doesn't a similar argument work if we suppose only $X$ is compactly generated, but suppose instead that $f$ is of type $X \times Y \to Z$ ? Under these hypothesis, the ""tricky"" parts in the proof above become automatic: $\widetilde{f}$ takes values in $\mathrm{Map}(Y,Z)$ by standard properties of the cartesian product, and $t \times \mathrm{id}_Y$ is a map of type $K \times Y \to X \times Y$ automatically, no need to mess with the compactly generated product $X \times^k Y$ . Is this correct? I think it is, but since this is the only reference I found where a topology is defined directly on $\mathrm{Map}(Y,Z)$ instead of $k$ -ifying the test-open topology or the space of $k$ -continuous maps, I'm a little uncertain.
Moreover, this alternative result implies another one which feels wrong to me. Corollary. For any spaces $X$ and $Y$ , the space $\mathrm{Map}(Y,Z)$ is compactly generated. Proof. Let $A \subseteq \mathrm{Map}(Y,Z)$ be a compactly open subset.
If $K$ is compact Hausdorff and $t: K \times X \to Y$ is a map, since every compact Hausdorff space is compactly generated, by the alternative result above the adjoint $\widetilde{t}: K \to \mathrm{Map}(X,Y)$ is continuous, so $\widetilde{t}^{-1}(A)$ is open in $K$ , which means that $A$ is open in $\mathrm{Map}(X,Y)$ .","['general-topology', 'solution-verification', 'category-theory', 'algebraic-topology']"
4389480,Probability of winning a game by rolling the die first,"Two persons are playing a game where they take turns rolling a die (so A rolls first, then B, then A again and so on). The first person to roll a $6$ wins the game. What is the probability that the person who started the game (rolled the die first) wins? This was the question that I was given, but I feel like the probability depends on the number of turns played? My approach was that the probability of rolling a 6 at any particular turn will be $\frac{1}{6}$ . So, the probability of winning in $n$ turns will be the probability of not rolling a $6$ on the first $n-1$ turns (since if a $6$ had been rolled, that would have been the last, i.e. the $nth$ turn) and then rolling a $6$ on the $nth$ turn. The required probability would then be $$\frac{5}{6}\cdot\frac{5}{6}\cdot\frac{5}{6}\cdot...(n-1\text{ times})\cdot\frac{1}{6} = \frac{5^{n-1}}{6^n}$$ Is this train of thought correct? Or have I misunderstood something?","['dice', 'probability']"
4389481,Derivative of $\pi: G \times \mathfrak{g} \rightarrow G \times_K \mathfrak{g}$,"Let $G$ be a compact Lie group with Lie algebra $\mathfrak{g}.$ Let $K$ be a closed subgroup of $G$ with Lie algebra $\mathfrak{k}.$ We define the manifold $$\mathcal{E}:= G \times_K \mathfrak{g}$$ to be the quotient of the $K$ action on $G \times \mathfrak{g}$ , where $K$ acts on $G$ by right multiplication, and on $\mathfrak{g}$ by the adjoint action. Let $$ \pi: G \times \mathfrak{g} \rightarrow G \times_K \mathfrak{g}$$ be the projection map; which associates to $(g,X) $ its equivalence class $[g,X] $ in $G \times_K \mathfrak{g}$ . Let $(g,X) \in G \times \mathfrak{g}$ , if we identify $T_{(g,X)} (G \times \mathfrak{g})$ with $\mathfrak{g} \times \mathfrak{g}$ and identify $T_{\pi(g,X)}(G \times_K \mathfrak{g})$ with $\mathfrak{g}/\mathfrak{k} \times \mathfrak{g}$ , can we give an explicit expression for the derivative of $\pi$ at $(g,X) ?$ What I've tried is to take $Z_1= \alpha'(t)$ , $Z_2= \beta'(t)$ in $\mathfrak{g} \times \mathfrak{g}$ and $$d\pi_{(g,X)}(Z_1,Z_2)= d\pi_{(g,X)}(Z_1,0)+ d\pi_{(g,X)}(0,Z_2)= \frac{d}{dt} \Bigg|_{t=0} \pi(\alpha(t),X) +\frac{d}{dt} \Bigg|_{t=0} \pi(g,\beta(t)),$$ but then I don't how to continue?","['derivatives', 'lie-algebras', 'lie-groups', 'differential-geometry']"
4389527,How to find / construct a finite group with special properties?,"I'm looking for a finite group $G$ which has the following properties (simultaneously): a) $G$ has a $2$ -subgroup $P$ such that $N_G(P)/P \cong A_6$ , the alternaing group acting on $6$ points b) $|G| > |N|$ , where $N=N_G(P)$ from a) c) the order of $G$ is not too large (roughly $1\ 000 < |G| < 30\ 000$ ). I was thinking about the outer automorphism group of $A_6$ , but it did not lead anywhere. Thank you.","['finite-groups', 'representation-theory', 'ring-theory', 'abstract-algebra', 'group-theory']"
4389529,A conjecture about binary functions on binary strings with partial inverses,"Given a positive integer $n$ we denote $B = \{0,1\}^n$ . I conjecture that for any $f:B^2\to B$ , If there exists $g,h:B^2\to B$ s.t. for all $x,y$ in $B$ , $g(f(x,y),x)=y$ and $h(f(x,y),y)=x$ , then there exists $u,v:B\to B$ s.t. $f = (x,y)\mapsto u(v(x)\oplus y)$ . Note: $\oplus$ denote the bitwise xor. A bit of context : I was trying to prove the noisy channel coding theorem in a different way than the classical ones using AEP or the error exponents and this would fill the last gap but I am a bit stuck with it and also maybe a bit tired/lazy (I am neither a student nor mathematician). It may very well be wrong but then a counterexample would be interesting. Any hint/help in comments appreciated. :) Simple example : If you take $f = \oplus$ then $g = h = \oplus$ and $u = v = id_B$ works.","['category-theory', 'information-theory', 'linear-algebra', 'discrete-mathematics', 'group-theory']"
4389537,Proof that four values have to be rational,"Following is defined: $$z=\frac{cx+dy}{ax+by}$$ I have these four terms: $$v_1=b+az$$ $$v_2=a-bz$$ $$v_3=d+cz$$ $$v_4=c-dz$$ The requirements are: $x,y\in\mathbb{Z};v_1,v_2,v_3,v_4\in\mathbb{Q}$ . Meaning that x & y are whole numbers and the four terms are all rational. From these conditions I want to prove that a, b, c, and d have to be rational . This might not be true and if so I would like to see a counter example or proof that falsifies the claim and I would really like to know the restrictions on a, b, c, and d that these four terms give rise to. Can they be all real numbers or only specific ones etc. I have already tried to algebraically manipulate the term to find a more suitable shape but have not succeeded. $$a-b\frac{cx+dy}{ax+by}=\frac{\frac{a^2}bx+ay-cx-dy}{\frac a b x+y}$$ I have also tried to go about it by considering the general rules for adding and multiplying  rational and irrational numbers, like rational * irrational = irrational and such. But this also did not seem to get me anywhere. Additionally I tried to divide the problem into many multiple cases depending on if a, b, or z were rational or not and could show this claim for the case that z is rational. But ultimately this wasn't an answer either.","['algebra-precalculus', 'systems-of-equations', 'irrational-numbers', 'rational-numbers']"
4389563,About the limit of a function,"We know that if $A\subseteq\mathbb{R}$ and $x_{0}\in \mathbb{R}$ a cluster point of $A$ . Then the real number $L$ is called limit of $f:A\rightarrow \mathbb{R}$ at $x_{0}$ if for all $\epsilon>0$ , there is a $\delta_{\epsilon}>0$ such that $\forall x\in A$ such that $0<|x-x_{0}|<\delta_{\epsilon} \Rightarrow |f(x)-f(x_{0})|<\epsilon$ .
My questions are (i) Why we take a cluster point here instead of any real number?
(ii) $\delta$ depends on $\epsilon$ but in the example $f(x)=\frac{1}{x}$ for all $x\in A=(0,\infty)$ , we have for all $\epsilon>0$ , a $\delta$ is $\delta=\inf\{\frac{x_{0}}{2},\frac{x_{0}^{2}}{2}\}$ so that $f(x)\rightarrow \frac{1}{x_{0}}$ as $x\rightarrow x_{0}$ . Clearly $\delta$ depends on $\epsilon$ and $x_{0}$ . In continuity, we called uniform continuity. Can we called here the convergence uniform convergence if $\delta$ depends only on $\epsilon$ .","['calculus', 'analysis', 'real-analysis']"
4389573,"If $\mu(A_n\Delta A)\to0$, why is $A=\bigcup_{n\ge1}\bigcap_{m\ge n}A_m$? I get that it only holds for a subsequence","$\newcommand{\d}{\mathrm{d}}\newcommand{\M}{\mathcal{M}}\newcommand{\L}{\mathcal{L}^1}$ Let $(X,\M,\mu)$ be a finite measure space. The following appears to be an unusual definition - I looked it up and only saw references to Royden's text - so I'll define it here: Let $\cdot\Delta\cdot$ denote symmetric difference. It can be shown that $A\sim B\iff\mu(A\Delta B)=0$ is a proper equivalence relation, and that $\rho_\mu:\M_{/\sim}\times\M_{/\sim}\to\Bbb R^+$ , $\rho_\mu([A],[B])=\mu(A\Delta B)$ is a proper metric. For convenience, denote $\M_{/\sim}$ also by $\M$ , and use equivalence classes and distinct sets interchangeably. The metric space $(\M,\rho_\mu)$ is the Nikodym metric space. I am given: Let $\{A_n\}\subset\M$ be a sequence converging in metric to the set $A$ . Show that $A=\bigcup_{n\ge 1}\bigcap_{m\ge n}A_m$ . This means that $\mu(A_n\Delta A)$ becomes arbitrarily small as $n$ grows large - it is left as an implicit hint from the previous to exercise to observe that $A_n\Delta A=(A_n\cup A)\setminus(A_n\cap A)$ . I couldn't see a way to use this identity however, so I used a different approach modelled of the proof of the completeness of the Nikodym space. My solution: $$\rho_\mu(A_i,A_j)=\int_X|\chi_{A_i}-\chi_{A_j}|\,\d\mu\to0$$ As $i,j\to\infty$ . Then $\chi_{A_n}$ is a Cauchy sequence in $\L(X)$ and by Riesz-Fischer it converges in norm to $f\in\L(X)$ and there is a subsequence $n_k$ on which $\chi_{A_{n_k}}\to f$ pointwise $\mu$ -a.e., so $f$ takes the values $0,1$ $\mu$ -a.e. If one defines: $$A=\{x\in X:\lim_{k\to\infty}\chi_{A_{n_k}}(x)=f(x)=1\}$$ Then $\chi_A=f$ $\mu$ -a.e. and thus it follows from the integral that $\rho_\mu(A_n,A)\to0$ as $n\to\infty$ . Sidenote - this proves the completeness of the Nikodym metric space. Now note that $x\in A\iff\exists K\in\Bbb N:\forall k\ge K,\,x\in A_{n_k}$ by construction. Then $x\in\bigcap_{k\ge K}A_{n_k}$ iff. $x\in A$ , for some $K$ . By taking union over $\Bbb N$ , one recovers all possible $K$ such that $x\in A$ , so we can write: $$A=\bigcup_{m\ge 1}\bigcap_{k\ge m}A_{n_k}$$ But this is a slightly different expression to the one given as the exercise solution - how can I show that taking a subsequence is unnecessary?","['elementary-set-theory', 'pointwise-convergence', 'measure-theory', 'metric-spaces']"
4389596,Cardinal numbers of right factors of a group,"Let $A$ and $B$ be subsets of a group $G$ . The product $AB$ is called direct (and we denote it by $A \cdot B$ , e.g., see this ) if the representation of each element $x$ of $AB$ as $x=ab$ , $a\in A$ , $b\in B$ is unique (equivalently $A^{-1}A \cap BB^{-1}=\{1\}$ , where $A^{-1}:=\{ a^{-1}:a\in A\}$ ). Question. Let $G$ be a group and $A,B,C$ be subsets such that $G=A \cdot B=A \cdot C$ . Then, is it true that $|B|=|C|$ ? ( $|.|$ denotes the cardinal number) Discussion. If $G$ is finite, then the answer is positive, since $|A \cdot B|=|A||B|=|A||C|$ . If $1\in A$ (without loss of generality) and $A^{-1}A \subseteq AA^{-1}$ , then we can define an injective (projection) map from $B$ to $C$ and also $C$ to $B$ . Therefore, if there is a negative answer, it should be through infinite non-abelian groups $G$ (and infinite subsets $A$ with $A^{-1}A \nsubseteq AA^{-1}$ ). Also see $AA^{-1} \subseteq A^{-1}A$ for every infinite subset $A$ of $G$ .","['group-theory', 'abstract-algebra', 'infinite-groups']"
4389674,Bounded and open simply connected region is smoothly contractible.,"Let $U\subset \Bbb R^2$ be open, bounded, and simply connected. Is it true that $U$ is smoothly contractible to a point? By the Whitney Approximation Theorem, it suffices to find a continuous $H :U\times [0,1]\to U$ s.t. $H(x,0)=x$ and $H(x,1)=x_0\in U$ . If I could assume the Riemann mapping theorem, this would be trivial. Just send $U$ to the unit disk and contract to the origin. Unfortunately, this is a question from Spivak's Differential Geometry books, and so far there has been no mention of the Riemann mapping theorem, so I believe I should not use it in the proof. What has been covered so far is some basic de Rham cohomology, but all I know in this connection is that $H^1(M)=0$ for a simply connected manifold. I do not have any sort of converse result where the cohomology group tells me anything about the underlying space. Am I barking up the wrong tree? Is the proof along another direction?","['connectedness', 'de-rham-cohomology', 'differential-geometry']"
4389675,Normal subgroup of triangle group in GAP,"Consider the hyperbolic (extended) triangle group $\Delta(2,3,7)=\langle a,b,c\mid a^2,b^2,c^2,(ab)^2,(bc)^3,(ca)^7\rangle$ . I construct it in GAP as a finitely presented group, using the standard method: FG:=FreeGroup(""a"",""b"",""c"");;
D:=FG/[FG.1^2,FG.2^2,FG.3^2,(FG.1*FG.2)^2,(FG.2*FG.3)^3,(FG.3*FG.1)^7];; I next construct the set $W=\{\gamma_1,\ldots,\gamma_7\}\subset\Delta(2,3,7)$ as a particular set of seven (complicated) words in the generators $a,b,c$ . I know from mathematical considerations that $W$ should generate an index-336 normal subgroup of $\Delta(2,3,7)$ , i.e., $\Gamma=\langle W\rangle\triangleleft \Delta(2,3,7)$ : GAMMA:=Subgroup(D,gam);; where gam is a list containing the seven words $\gamma_1,\ldots,\gamma_7$ above. Now here is my problem. GAP correctly recognizes that $\Gamma$ is a subgroup of $\Delta(2,3,7)$ of index 336: gap> IsSubgroup(D,GAMMA);
true
gap> Index(D,GAMMA);
336 but it doesn't seem to recognize that it is normal: gap> IsNormal(D,GAMMA);
false What am I doing wrong?","['gap', 'hyperbolic-groups', 'finitely-generated', 'normal-subgroups', 'group-theory']"
4389689,Is every simply connected set in the plane regular for Brownian motion?,"Background Let $\{B_t\}_{t\ge 0}$ be Brownian motion in $\mathbb R^d$ . For $D\subset\mathbb R^d$ , let $\tau_D=\inf\{t>0:B_t\in D\}$ be the hitting time of $D$ for $B_t$ . For $D\subset\mathbb R^d$ open, a point $x\in\partial D$ is said to be regular if $\mathbb P_x(\tau_{D^c}=0)=1$ ; that is, if when the Brownian motion is started from $x$ it hits $D^c$ instantaneously (possibly by returning to $x$ ). An open set $D$ is said to be regular if all of its boundary points are. Motivation If $d=1$ , every set is regular because for any $\varepsilon>0$ the Brownian motion $B_t$ will return to its starting point at infinitely many $t$ in the interval $(0,\varepsilon)$ . If $d\ge 2$ , I believe the unit ball $\mathbb B=\{x\in\mathbb R^d:|x|<1\}$ is regular and simply connected, the annulus $\mathbb A=\{x\in\mathbb R^d:1<|x|<2\}$ is regular but not simply connected, and the punctured ball $\mathbb B\setminus\{0\}$ is neither. If $d\ge 3$ , then I believe $\mathbb B\setminus\left([0,1)\times\{0\}^{d-1}\right)$ should not be regular, because e.g. the second two coordinates of the Brownian motion will both be zero concurrently with probability zero. However, when $d=2$ I haven't been able to think of an open set which is simply connected but not regular. If we look at $\mathbb B\setminus\left([0,1)\times\{0\}\right)$ again, this time I don't think it works. In response to the comments, we'll look at what's going on here more carefully. Let $B_t=(B^1_t,B^2_t)$ . For boundary points $(a,0)$ with $0<a<1$ , as $\varepsilon\to0$ the probability that $B_t^1\in(0,1)$ for all times $t\in(0,\varepsilon)$ approaches one by continuity of probability and continuity of Brownian motion. And conditional on the event $\{\forall t\in(0,\varepsilon), B_t^1\in(0,1)\}$ the probability that $B_t$ hits the boundary at some point in $(0,\varepsilon)$ is one, since $B_t^2$ is zero for infinitely many $t\in(0,\varepsilon)$ with probability one. So the only possible irregular boundary point of $\mathbb B\setminus\left([0,1)\times\{0\}\right)$ is $(0,0)$ . Here I don't have a proof of regularity, just a heuristic. Namely, $B_t^2$ will be zero at infinitely many times on the interval $(0,\varepsilon)$ , and because $B^1$ and $B^2$ are independent I expect that ""half"" of those times the first coordinate will be non-negative. Question Is every simply connected open subset of $\mathbb R^2$ regular, or is there an counterexample that I'm not thinking of?","['brownian-motion', 'probability-theory']"
4389701,Curves with the same speed and distance to origin,"Let $\alpha,\beta:[0,1]\to\mathbb{R}^2$ be two smooth curves satisfying $|\alpha(t)| = |\beta(t)|$ and $|\dot{\alpha}(t)| = |\dot{\beta}(t)|$ for all $t\in[0,1]$ .  That is, $\alpha$ and $\beta$ have the same speed and are at the same distance to the origin. Does there exist a rotation matrix $Q$ (with possibly negative determinant) such that $\alpha(t) = Q\beta(t)$ ? It seems that the answer is yes.  The speed $|\dot{\alpha}|$ should be enough to tell how quickly the curve is going around the circle.  This seems to be enough to recover the angular data of the curve, but I do not know of a good way to make this precise.","['plane-curves', 'differential-geometry']"
4389707,Asymptotic behavior of integral,"I have the following expression: \begin{equation}
\left.\frac{\partial U}{\partial(B/J)}\right|_{JB} = -N \sqrt{JB} \int_{-\pi}^\pi \frac{\text{d}k}{2\pi} \frac{B/J - J/B}{\sqrt{4\sin^2k/2 + (J/B - B/J)^2}}
\end{equation} I am looking at the asymptotic behavior as $B/J - J/B \to 0$ . To me, this seems equivalent to: \begin{equation}
\lim_{X \to 0}\left[-N \sqrt{Y} \int_{-\pi}^\pi \frac{\text{d}k}{2\pi} \frac{X}{\sqrt{4\sin^2k/2 + X^2}}\right] = \left[-N \sqrt{Y} \int_{-\pi}^\pi \frac{\text{d}k}{2\pi} \frac{0}{\sqrt{4\sin^2k/2}}\right] = 0
\end{equation} But I am doubting the results because $k=0$ is within the range of integration. What is the appropriate way to look at the asymptotic behavior?","['integration', 'limits']"
4389788,Question on the sign and direction of the derivative,"Why can't the derivative be defined as $$\lim_{h\rightarrow 0}\dfrac{f(a)-f(a+h)}{h},$$ equivalently $$\lim_{x\rightarrow a}\dfrac{f(a)-f(x)}{x-a}$$ rather than as $$\lim_{h\rightarrow 0}\dfrac{f(a+h)-f(a)}{h}$$ and $$\lim_{x\rightarrow a}\dfrac{f(x)-f(a)}{x-a}$$ respectively? Perhaps switching the sign of the numerator (by swapping $$f(x)-f(a)$$ and $$f(a)-f(x)$$ is unacceptable because it gets the sign of the derivative wrong. But then, how can we use knowledge about the sign of the derivative, in order to help us calculate the derivative? Thank you in advance.","['calculus', 'derivatives']"
4389861,Is it valid to say that $\int_{-\infty}^\infty f(x)dx = \lim_{t\to \infty} \int_{-t}^t f(x)dx$?,"So, I'm a college freshman, and when I was doing a homework problem, I found that the definite integral was equal to zero. It was an integral of the form $\int_{-\infty}^\infty f(x)dx$ and $f(x)$ happened to be an odd function. I never questioned whether the property of odd functions, $\int_{-a}^a f(x)dx = 0$ still works when a is infinity instead of a finite number. But my intuition and that homework problem I did seem to suggest that. My reasoning to justify the idea is that for an odd function $f(x)$ , $\int_{-\infty}^\infty f(x)dx = \lim_{t\to \infty} \int_{-t}^t f(x)dx = \lim_{t\to \infty} 0 = 0.$ This argument only works if the first step is valid. I'm wondering if there are any problems with it since you usually deal with this kind of improper integral as two integrals: $ \int_{-\infty}^\infty f(x)dx = \int_{-\infty}^a f(x)dx + \int_a^\infty f(x)dx$ .","['integration', 'calculus']"
4389868,How do we go from a covariant derivative on a principal bundle to a covariant derivative on an associated bundle,"Let $M$ be a smooth manifold and $\pi:P\to M$ a principal $G$ bundle over $M$ . Suppose that $P$ is equipped with a connection one form $\omega$ . We can define an exterior covariant derivative on $P$ via $D\eta = d\eta \circ \operatorname{hor}$ , where $\operatorname{hor}$ is the horizontal part of any vector in $TP$ . For $G$ -equivariant functions $\phi :P\to F$ (that is $\phi(p\cdot g) =\rho(g^{-1})\phi(p)$ , where $\rho:G\to F$ is a representation of $G$ ), and indeed for $G$ -equivariant forms as well (but we don't need them), the exterior covariant derivative takes the simple form $$D\phi = d \phi + \rho_{*e}(\omega)\wedge \phi,$$ where the wedge product can be defined in a natural way. These $G$ -equivariant functions are in one-to-one correspondence with local sections of the associated bundle $Q :=P \times _G F$ over an open set $U$ of $M$ , and they contain the same information. The question is then, how does one go from the exterior covariant derivative on $G$ -equivariant functions to the covariant derivative of local sections of the associated bundle $Q$ ? One could naturally take the covariant derivative to simply be the local section $\sigma$ associated to $D\phi(X)$ , where $X \in TP$ , but this is not a covariant derivative on $Q$ , since $X$ is not in $TM$ . I have also seen the covariant derivative defined by pulling back $D\phi$ by a local section $s$ of $P$ over an open set $U$ , but this approach doesn't make much sense to me. This approach would make more sense if the correspondence mentioned above was between global sections and $G$ -equivariant functions, in which case we just take the local representation of the section (i.e. $s^*(D\phi)$ ). If anyone could clarify, it would be very helpful.","['principal-bundles', 'connections', 'differential-geometry']"
4389896,Algebraic group vs Lie group,"I know the definitions of algebraic and Lie groups. I know the difference between them in terms of definition; loosely, the first is a variety plus group, while the second is a smooth manifold plus group. However, in terms of examples, the main examples are almost the same. Also, the tangent space of both of them at the identity is a Lie algebra. In addition, both are studied in the big title ""Lie Theory"" and both are classified by Dynkin diagrams, etc. I know there might be some examples of Lie groups that are not algebraic and vice-versa. However, the theory behind them looks to be almost the same. So my question, and I hope it is clear, what is the real difference between them? Do they coincide in some cases and differ in some?","['lie-algebras', 'algebraic-groups', 'representation-theory', 'lie-groups', 'differential-geometry']"
4389926,"If the boundary of a bounded open set is a manifold, what is its dimension?","Let $\Omega \subseteq \mathbb R^n$ be a bounded open set and suppose that the topological boundary $\partial \Omega = \bar \Omega \setminus \Omega$ has the structure of a topological manifold. In particular, $\partial \Omega$ has a dimension $m$ . Can we say anything about $m$ ; is it $m=n-1$ , as intuition suggests?","['manifolds', 'general-topology', 'differential-geometry']"
4389945,Interior points in a convex set can be represented as convex combination of different points from the set,"Can we assume that any interior point $z$ in a convex set $S\subseteq R^n $ be represented by $2$ points $x \in S$ and $y \in S$ such that $z = \lambda x +(1-\lambda)y $ , where $x\neq y \neq z$ , and $\lambda \in (0,1)$ ? What I understand is that every point $\in S$ can be represented as a convex combination of 2 points in $S$ , and that any exterior point (say $v$ ) can't be constructed by 2 different points $x$ and $y$ ( $x \neq y \neq v$ ) such that $v = \lambda x + (1-\lambda)y$ where $\lambda \in (0,1)$ . Intuitively, I think for any interior point it should be true unless the set only has 1 element. How should I go about proving this seemingly trivial-looking result (if it is correct) or can I simply state that?","['convex-geometry', 'general-topology', 'convex-analysis', 'convex-hulls']"
4389993,Which principle is violated here in the logarithmic equation?,"I'm a beginner to mathematics and I'm stuck with a calculus exercise. It seems like I violate a principle, but I cannot yet see what I did wrong here. I hope a second look from a 3rd person will help. The equation I need to solve is: \begin{equation}
16^{x}+4^{(x+1)}=12
\end{equation} With as $x=\frac{1}{2}$ as only real solution. I understand that this can be solved with substitution, but I especially want to solve this problem with writing everything in the same base. Attempt: \begin{aligned}
&16^{x}+4^{(x+1)}=12 \\
&\left(4^{2}\right)^{x}+4^{(x+1)}=12 \\
&4^{2 \cdot x}+4^{(x+1)}=12 \\
&4^{2 \cdot x}+4^{(x+1)}=4^{\left(\frac{\log (12)}{\log (4)}\right)} \\
&2 \cdot x+x+1=\frac{\log (12)}{\log (4)} \\
&3 \cdot x=\frac{\log (12)}{\log (4)}-1 \\
&x=\frac{1}{3}\left(\frac{\log (12)}{\log (4)}-1\right)
\end{aligned} Could I please get feedback?","['calculus', 'algebra-precalculus']"
4390047,Convergence in probability of a random variable multiplied by n,"I'm trying to solve this 2 part example and the first part is trivial to show but I get confused about the second part. Because i know the solution it is easy to fool myself into thinking I understand it but I don't want that. I want to understand. So here goes: For $\boldsymbol{n} \geq \boldsymbol{1}$ , let $\boldsymbol{X}_{\boldsymbol{n}}$ be a Poisson random variable with parameter $1 / n$ . What can you conclude? $\square$ $\boldsymbol{X}_{\boldsymbol{n}} \rightarrow \mathbf{0}$ in probability, but $\boldsymbol{n X}_{\boldsymbol{n}}$ does not converge in probability $\square$ $\boldsymbol{X}_{n} \rightarrow 0$ in probability, $\boldsymbol{n X}_{n} \rightarrow 0$ in probability, and $\mathbb{E}\left[\left(\boldsymbol{n X}_{n}\right)^{2}\right]$ converges. $\square$ $\boldsymbol{X}_{n} \rightarrow 0$ and $n X_{n} \rightarrow 0$ in probability, but $\mathbb{E}\left[\left(n X_{n}\right)^{2}\right]$ does not converge. I started by wanting to prove $X_n$ converges in probability to 0. That was the easier bit: $$\lim_{n\to\infty} P(|X_n-0|>\epsilon)=0\Leftrightarrow\lim_{n\to\infty} P(|e^{-\frac{1}{n}}-0|>\epsilon)=0$$ Where I replaced $X_n$ by its pmf at point 0. Is this ok to write? Now I'm questioning the use of P on the RHS of above as I'm already using: $$\mathbf{P}\left(X_{n}=0\right)=\left(\frac{1}{n}\right)^{0} \frac{1}{0 !} \exp \left(-\frac{1}{n}\right)=\exp \left(-\frac{1}{n}\right)$$ What I ask of you is: Please help me be more precise about these statements Help me prove $\boldsymbol{n X}_{\boldsymbol{n}}$ does converge in probability as well. Thank you in advance!","['statistics', 'probability']"
4390105,Give a differential equation to which the 2-parameter family is a family of solutions,The given 2-parameter family is $$ y= e^{5x}(A+B\sin(2x))$$ Here's what I have so far: $$y'=e^{5x}(5B\sin(2x)+2B\cos(2x)+5A)$$ $$ y''=e^{5x}(21B\sin(2x)+20B\cos(2x)+25A)$$ $$10\cdot 2Be^{5x}\cos(2x)=10y'-10e^{5x}(5B\sin(2x)+5A)=10y'-50y$$ I'm trying to do the same with $y''$ but I get stuck because they have no common factor: $$20Be^{5x}\cos(2x)=y''-e^{5x}(21B\sin(2x)+25A)$$ How do I proceed?,['ordinary-differential-equations']
4390117,Proving $\sum E\left[\frac{X_n^2}{1+|X_n|}\right] < \infty$. Then:$\sum X_n<\infty$ a.s.,"$\{X_n\}$ are independent variables, $EX_n=0, EX_n^2=1$ . $\sum E\left[\frac{X_n^2}{1+|X_n|}\right] < \infty$ . Then: $\sum X_n<\infty$ a.s. My ideas so far:
I tried to use Kolmogorov three series theorem to prove the convergence of $\sum X_n<\infty$ . (1) $\sum p(|X_n|>1)<\infty$ (2) $\sum E(X_nI{_{|X_n|\leq1}})$ converges. (3) $\sum \operatorname{Var}(X_n^2I{_{|X_n|\leq1}}) \leq \sum E(X_n^2I{_{|X_n|\leq1}}) \leq\ \sum 2E\left(\frac{X_n^2}{1+|X_n|}I{_{|X_n|\leq1}}\right)\leq \sum 2E\left(\frac{X_n^2}{1+|X_n|}\right)<\infty$ .
The convergence of series(3) is easy but I don't known how to prove the convergence of (1)(2) (For series (1), the Markov inequality seems not work. For series(2), Since $EX_n=0$ , then the convergence of $\sum E(X_nI{_{|X_n|\leq1}})$ is equivalent to the  convergence of $\sum E(X_nI{_{|X_n|>1}})$ , but I still cannot proof this). Thanks in advance for any tips or help in general.","['independence', 'probability-theory', 'probability', 'sequences-and-series']"
4390202,Question regarding reduction of structure group,Let $\pi:P \to M$ be a principal $G$ -bundle. Given a subgroup $H$ of $G$ one can consider the fibre bundle $P_H :=P\times_{G}G/H$ whose fibres are the coset space $G/H$ . We define a reduction of structure group to be a section of $P_H \to M$ which is also the same as a $G$ -equivariant map from $P \to G/H$ . Here are my questions : I would expect $P_H$ to be a principal $H$ -bundle from the name. But the fibres are $G/H$ . Am I understanding wrong? Another interpretation of $G$ -bundles are by specifying transition functions from $U\cap V \to G$ satisfying cocycle conditions. I would expect reduction to $H$ should somehow mean the transition functions land in $H$ instead of the full $G$ . How can I formulate this? All spaces above are smooth manifolds and groups are Lie groups.,"['principal-bundles', 'smooth-manifolds', 'differential-geometry']"
4390220,$E$ measurable implies $\forall \varepsilon>0\exists A\in\mathcal S_\delta$ s.t. $A\subseteq E$ and $\mu^*(E\smallsetminus A)<\varepsilon$.,"From Royden's Real Analysis 4th edition, section 17.5, problem 36: Let $\mu$ be a finite premeasure on an algebra $\mathcal S$ , and $\mu^*$ the induced outer measure. Show that a subset $E$ of $X$ is $\mu^*$ -measurable if and only if for each $\varepsilon>0$ there is a set $A\in\mathcal S_\delta$ , $A\subseteq E$ , such that $\mu^*(E\smallsetminus A)<\varepsilon$ . Primarily my question is: is this even true?  This seems like it's stated backward.  I'm a noob so maybe I have this wrong, but since $\mathcal S_\delta$ is the set of all intersections of sets in $\mathcal S$ , then I would have thought that we were looking for a set $E\subseteq A$ .  When I try to do the proof that seems like the direction that pops out naturally.  Also, in section 2.4 where there is a corresponding proof for the Lebesgue measure, we claim ""There is a $G_\delta$ set $G$ containing $E$ for which ..."" But when I look up the errata, this is not mentioned in it. If the answer is ""the book stated it correctly"" then I'd appreciate a hint.  I've tried using the sequence of sets such that $\mu^*(E)-\varepsilon < \sum \mu^*(E_k)$ where $\{E_k\}$ covers $E$ .  Intersecting these seems like a dead-end.  I could union them, prove the theorem for $E\subseteq B$ , and then try to use $B$ to construct $A$ .  But any way that I can see of doing that, I lose the property that $A\in \mathcal S_\delta$ .","['measure-theory', 'outer-measure', 'real-analysis']"
4390236,A proof without using net in Brezis's Ex 3.14,"I'm doing Ex 3.14 in Brezis's book of Functional Analysis. Let $E$ be a reflexive Banach space and $I$ a set of indices. Consider a collection $(f_{i})_{i \in I}$ in the dual space $E'$ and a collection $(\alpha_{i})_{i \in I}$ in $\mathbb{R}$ . Let $M>0$ .
Show that the following properties are equivalent: There exists some $a \in E$ with $|a| \le M$ such that $\langle f_{i}, a\rangle=\alpha_{i}$ for every $i \in I$ . One has $|\sum_{i \in J} \beta_{i} \alpha_{i}| \leq M\|\sum_{i \in J} \beta_{i} f_{i}\|$ for every collection $(\beta_{i})_{i \in J}$ in $\mathbb{R}$ with $J \subset I, J$ finite. My below solution uses net characterization of compactness, which is probably not the author's intention. Could you have a check on my attempt and suggest another solution without using net? My attempt: WLOG, we can assume $M:=1$ . The direction $1 \implies 2$ is trivial. Let's prove the converse. Let $B_E$ be the closed unit ball of $E$ . Let $\mathcal J$ be the collection of all finite subsets of $I$ . By Helly's theorem ( Lemma 3.3 in the book), for each $(J,n) \in \mathcal J \times \mathbb N$ , there is $x_{J,n} \in B_E$ such that $$
|\langle f_i, x_{J,n} \rangle - \alpha_i| < 1/n \quad \forall i \in J.
$$ We endow $\mathcal J \times \mathbb N$ with a partial order $\le$ defined by $(J, n) \le (J', n') \iff J \subseteq J' \text{ and } n \le n'$ . Then $(\mathcal J \times \mathbb N, \le)$ is a directed set and thus $(x_{(J,n)})$ a net in $B_E$ . Because $E$ is reflexive, $B_E$ is compact in weak topology $\sigma(E', E)$ . Then there are $a \in B_E$ and a subnet $(x_{\varphi(d)})_{d\in D}$ such that $x_{\varphi(d)} \rightharpoonup a$ . Next we prove that $a$ satisfies the requirement of 1. Assume the contrary that there is $i_0 \in I$ such that $\langle f_{i_0}, a \rangle \neq \alpha_{i_0}$ . There is $n_0$ such that $|\langle f_{i_0}, a \rangle - \alpha_{i_0}| > 1/n_0$ . Let $J_0 := \{i_0\}$ and $$
U := \{x\in E \mid |\langle f_{i_0}, x \rangle - \alpha_{i_0}| > 1/n_0\}.
$$ Then $U$ is an open neighborhood of $a$ in $\sigma(E', E)$ . There is $d_0$ such that $x_{\varphi(d)} \in U$ for all $d \ge d_0$ . There is $d_1 \ge d_0$ such that $\varphi(d_1) \ge (J_0, n_0+1)$ . It follows that $$
1/(n_0+1) > |\langle f_{i_0}, x_{\varphi(d_1)} \rangle - \alpha_{i_0}| > 1/n_0.
$$ This is a contradiction. This completes the proof.","['banach-spaces', 'weak-convergence', 'reflexive-space', 'solution-verification', 'functional-analysis']"
4390254,How do you prove: $\dfrac{1}{\sin ^2 (\frac{2\pi}{9})} - \dfrac{1}{\sin ^2(\frac{4 \pi}{9})} = 8\sin(\frac{\pi}{18})$,"I have to prove: $\displaystyle \tag*{} \alpha={\dfrac{1}{\sin ^2 (\frac{2\pi}{9})} - \dfrac{1}{\sin ^2(\frac{4 \pi}{9})}}= 8\sin(\frac{\pi}{18})$ I tried to make a common denominator of $\alpha$ and use a few identities to arrive at: $\displaystyle \tag*{} \alpha = \dfrac{\sin(\frac{6\pi}{9})}{\sin(\frac{2\pi}{9})\sin^2(\frac{4\pi}
{9})}$ I am not sure how to proceed from this. I even arrived at other 2 forms of $\alpha$ , but they make even more complicated. Any hints would be greatly appreciated. Thanks.",['trigonometry']
4390273,Is this function an alternative solution to the nonlinear pendulum?,"Is this function an alternative solution to the nonlinear pendulum? Introduction I am working with the differential equation of the frictionless nonlinear pendulum: $$\ddot{\theta}(t) + b\,\sin(\theta(t)) = 0 \tag{Eq. 1}$$ Which it already have known exact solution : $$\theta(t)=2\sin^{-1}\left(\sin\left(\frac{\theta_0}{2}\right)\text{sn}\left[K\left(\textstyle{\sin^2\left(\frac{\theta_0}{2}\right)}\right)-\sqrt{b}\,t\,;\,\sin^2\left(\frac{\theta_0}{2}\right)\right] \right)\tag{Eq. 2}$$ with $\text{sn}[z;m]$ one of the Jacobi elliptic functions with mode $m$ , $K(z)$ the Complete elliptic integral of the first kind , and $\theta_0 = \theta(t=0)$ . Also, on Wolfram-alpha the solution to Eq. 1 is given by: $$\theta(t)=\pm\,\text{am}\left[\frac{1}{2}\sqrt{(2b+c_1)(t+c_2)^2}\,;\,\frac{4b}{2b+c_1}\right] \tag{Eq. 3}$$ with $\text{am}[z\,;\,m]$ the Jacobi amplitude function which fulfills $\text{sn}[z\,;\,m] = \sin\left(\text{am}[z\,;\,m]\right)$ , and $c_1,\,c_2$ are integration constants. If the initial conditions $\theta_0 = \frac{\pi}{2}$ and $\theta'(0)=0$ are introduced in Wolfram-Alpha the solution takes the form: $$\theta(t) = 2\,\sin^{-1}\left(\frac{\text{cd}\left[\sqrt{b}\,t\,;\,\frac{1}{2}\right]}{\sqrt{2}}\right)\tag{Eq. 4}$$ where $\text{cd}[z\,;\,m]$ is another secondary Jacobi elliptic function . Main text I was trying to apply the Weierstrass substitution $x(t)=\tan\left(\frac{\theta(t)}{2}\right)$ to Eq. 1 since it made possible to change the sine function into polynomials: $$ \begin{array}{c} 
(i)\,\,\sin(\theta) = \frac{2x}{1+x^2}\,; & (ii)\,\,\dot{\theta} = \frac{2\dot{x}}{1+x^2}\,; & (iii)\,\,\ddot{\theta} = \frac{\ddot{x}(1+x^2)-2x(\dot{x})^2}{(1+x^2)^2}
\end{array}$$ and since $1+x^2 > 0,\,\,\forall x \in \mathbb{R}$ with this change of variable Eq. 1 becomes: $$\ddot{x}+b\,x = \frac{2x(\dot{x})^2}{(1+x^2)} \tag{Eq. 5}$$ where it can be seen that Wolfram Alpha find the solution to Eq. 5 given by complex numbers: $$ x(t) = \pm i\,\text{sn}\left[i\sqrt{b+c_1}\,(t+c_2)\,;\,\frac{c_1}{b+c_1}\right] \tag{Eq. 6}$$ By a ""happy accident"" trying to solve Eq. 5, I found about the existence of the Lemniscate elliptic functions : lemniscate sine function $sl(x)$ and lemniscate cosine function $cl(x)$ , which haves only one variable and fulfills the following properties: $(iv)\quad sl' = cl\,(1+sl^2)$ $(v)\quad cl' = -sl\,(1+cl^2)$ $(vi)\quad (sl\,cl)' = cl^2-sl^2$ $(vii)\quad sl^2+sl\,cl+cl^2 = 1$ $(viii)\quad (sl')^2+sl^4 = 1$ $(ix)\quad sl^4+sl^4cl^2+sl^2cl^2 = sl^2$ $(x)\quad (1+sl^2)(1+cl^2)=2$ $(xi)\quad cl(t) = \text{cn}\left[\sqrt{2}\,t\,;\,\frac{1}{\sqrt{2}}\right]$ $(xii)\quad sl(t) = \displaystyle{\frac{\text{sd}\left[\sqrt{2}\,t\,;\,\frac{1}{\sqrt{2}}\right]}{\sqrt{2}}}$ Here is where the ""issue"" begins, since I found the following by accident, I don´t know if it is completely right, and maybe there is an ""initial constants issues"" that could made it an incomplete solution, as Eq. 4 is different from Eq. 3... but I believe that the following function is a solution to Eq. 5: $$ x(t) = cl\left(\sqrt{\frac{b}{2}}\,t-c_0\right) \tag{Eq. 7}$$ Differentiating Eq. 7 through property $(v)$ which lead to (I will be omitting the argument): $$ x'(t) = -sl\,(1+cl^2)\sqrt{\frac{b}{2}} \tag{Eq. 8}$$ And differentiating Eq. 8 through properties $(iv),\,(v),\,(x)$ which lead to: $$ \begin{array}{r c l} 
x''(t) & = & -\sqrt{\frac{b}{2}}\left\{cl(1+sl^2)(1+cl^2)\sqrt{\frac{b}{2}}+sl\,(2\,cl)(-sl(1+cl^2))\sqrt{\frac{b}{2}} \right\} \\
& = & -\frac{b}{2}\left\{2\,cl-2\,cl\,sl^2(1+cl^2)) \right\} \\
& = & -b\left\{cl-cl\,sl^2(1+cl^2)) \right\} \tag{Eq. 9}
\end{array}$$ Now, solving the Left-Hand-Side (LHS) and Right-Hand-Side (LHS) of Eq. 5 separately will lead to: $$\text{LHS} = x'' + b\,x = -b\,cl + b\,cl\,sl^2(1+cl^2)+b\,cl =  b\,cl\,sl^2(1+cl^2) \tag{Eq. 10}$$ $$\text{RHS} = \frac{2x(\dot{x})^2}{(1+x^2)} = \frac{2\,cl\left(-sl(1+cl^2)\sqrt{\frac{b}{2}}\right)^2}{1+cl^2} = \frac{b\,cl\,sl^2(1+cl^2)^2}{1+cl^2} = b\,cl\,sl^2(1+cl^2) \tag{Eq. 11}$$ If I didn´t make any mistakes, since Eq. 10 = Eq. 11 , I think that Eq. 7 is indeed a solution of Eq. 5, so a solution to Eq. 1 will be: $$ \theta(t) = 2\tan^{-1}\left(cl\left(\sqrt{\frac{b}{2}}\,t-c_0\right)\right) \tag{Eq. 12}$$ Where the integration constant will be given by (using their own inverse functions): $$c_0 = \frac{1}{\sqrt{2}}F\left[\cos^{-1}\left(\tan\left(\frac{\theta_0}{2}\right)\right)\,;\,\frac{1}{\sqrt{2}}\right] \tag{Eq. 13}$$ with $F[z,\,;\,m]$ the Incomplete elliptic integral of the first kind . Here since the constant is dependent of having well obtained Eq. 12, I am not so interested in it, being Eq. 12 the main topic from the questions. The Big Questions Is Eq. 12 a ""true"" solution for the frictionless nonlinear pendulum? ... Hope if there are calculation mistakes or conceptual ones, please show on your answer how to properly find the solution (if there is any by using the Weierstrass Substitution). Are all the presented solutions equivalent? ... I am not familiar with nonlinear differential equations, and I have read that sometimes don´t fulfill uniqueness of solutions, so I would like to know if they are the same solution or not (myself I tried to plot them but ""softwares"" have a bad time working with elliptic functions). Also, I have no formal training on elliptic functions (I discovered them on Wikipedia), so maybe equivalence among solutions could be ""easily"" proved (I hope) through Jacobi Theta Functions or Neville Theta Functions , or other elliptic functions, but unfortunately for me are too complicated and I don´t understand how to work with them from the Wiki web pages. There is a general solution in terms of lemniscatic sine/cosine functions? This for avoiding the second parameter of the Elliptic Functions. Here note that solution of Eq. 7 and 12 only depends of one integration constant!! which must be wrong for a second order ODE!! ... I believe the solution I found works only when starting from rest $\dot{\theta}(t)=0$ , but I am not really sure - Hope you can give the general form of this solution in terms of the lemniscatic function (I tried to work with Eq. 6 on Wolfram Alpha but a complex elliptic function was too heavy for the free version I think). Motivation The nonlinear pendulum with friction is known for being the simple and classical example of a mechanical system with friction included, by so far there is not known exact solution to its differential equation: $$\ddot{\theta}(t) + a\,\dot{\theta}+b\,\sin(\theta(t)) = 0 \tag{Eq. 14}$$ By using the Weierstrass Substitution $x(t)=\tan\left(\frac{\theta(t)}{2}\right)$ to Eq. 14 it will becomes: $$\ddot{x}+a\,\dot{x}+b\,x = \frac{2x(\dot{x})^2}{(1+x^2)} \tag{Eq. 15}$$ which is highly similar to Eq. 5, so I am trying to find if something of the form $x(t)=q(t)\,cl(\sqrt{b/2}\,t-c)$ could be a solution for a function $q(t)$ to be determined (but it rapidly becomes in a mess). Added Later I was trying to develop Eq. 6 in something similar to Eq. 7, but things are more confusing now... using what is said on Wikipedia I can use the following property: $$ (xiii)\quad \text{sn}[ix;\,m] = i\,\text{sc}[x;\,1-m]$$ where $\text{sc}[x;\,m]=\frac{\text{sn}[x;\,m]}{\text{cn}[x;\,m]}$ another Jacobi Elliptic function. But reading the reference of Wikipedia, it takes it from Page 504 of Wiki-Reference where the property is display as $\text{sn}[ix;\,k] = i\,\text{sc}[x;\,k']$ where in Page 493 is defined the complementary modulus as $k^2+k'^2=1$ , which is different from property $(xiii)$ - I don´t know if there is a mistake or both are right because of other properties of the Jacobi elliptic functions, but since in Wolfram-Alpha is implemented the Wikipedia version, I will use it as if it is correct. With this, Eq. 6 becomes: $$ x(t) = \pm \text{sc}\left[\sqrt{b+c_1}\,(t+c_2)\,;\,\frac{b}{b+c_1}\right] \tag{Eq. 16}$$ Which will imply that: $$\theta(t) = 2 \tan^{-1}\left(\text{sc}\left[\sqrt{b+c_1}\,(t+c_2)\,;\,\frac{b}{b+c_1}\right] \right) \tag{Eq. 17}$$ which should be the same solution as Eq. 2 by Uniqueness (as I get convinced from here - but for me is not trivial at all, I don´t know if after matching the integration constants will lead to some new properties, but is of reach of my current skills). At least, using property $(xi)$ in Eq. 7, one can see the proposed solution as: $$x(t) = \text{cn}\left[\sqrt{b}t-c_3;\,\frac{1}{\sqrt{2}}\right] \tag{Eq. 18}$$ but I have not been able to match the constants so Eq. 18 and Eq. 16 becomes equivalent, which should be the case at some pair of integrating constant due Uniqueness of solutions (I don't believe I accidentally found something that drop Uniqueness theorem of Lipschitz Diff. Eq... so If you found the corresponding constants, will be highly appreciated).","['ordinary-differential-equations', 'elliptic-functions', 'solution-verification', 'theta-functions', 'elliptic-integrals']"
4390290,"How to calculate the new $x$, and $y$ coordinates of a scaled rotated rectangle?","I want to scale a rotated rectangle around it's centre point and find the new $x$ and $y$ coordinates - In this diagram given I know $(x, y), \theta , w , h , nw$ , and $nh$ - does anyone ideas how I calculate $(nx, ny)$ ? I'm not massively familiar with vector math or vector notation so please bear with me. EDIT: Sorry, I've updated the diagram to show the rotation origin is not (0,0) - it is unknown.","['programming', 'vectors', 'rectangles', 'trigonometry', 'rotations']"
4390340,Lemma $3$ in Commutators of Operators by Paul R. Halmos,"I need help understanding the proof of Lemma $3$ in the paper Commutators of Operators by Paul R. Halmos. Lemma $3$ . Every Hermitian operator on an infinite-dimensional Hilbert space leaves invariant at least one large subspace with a large orthogonal complement. A subspace $H$ of a Hilbert space is large if $H$ contains infinitely many orthogonal copies of its orthogonal complement, or, in other words, if $\dim H \ge \aleph_0 \dim (H^\perp)$ . Proof. The underlying Hilbert space, if it is not already separable, can be expressed as a direct sum of separable, infinite-dimensional subspaces invariant under the given operator. There is, therefore, no loss of generality in restricting attention to separable Hilbert spaces. Q1. I understand that $\mathcal H$ can be written as an uncountable direct sum of separable subspaces, but why does it suffice to consider the separable case? Suppose $A \in \mathcal B(H)$ is Hermitian and $\mathcal H = \bigoplus_{n=1}^\infty \mathcal H_n$ where $\{\mathcal H_n\}_{n=1}^\infty$ is a family of $A$ -invariant separable subspaces of $\mathcal H$ . So, $A_i:= A\vert_{\mathcal H_i}: \mathcal H_i \to \mathcal H_i$ is also Hermitian for every $i\ge 1$ . Assuming we have worked out the separable case; for every $i\ge 1$ there exists a large $A_i$ -invariant subspace $M_i \le H_i$ such that $H_i - M_i$ (orthogonal complement in $H_i$ ) is also large. Consider $M = \bigoplus_{n=1}^\infty M_i$ . It is clear that $M$ is $A$ -invariant. How do I show that $M$ and $M^\perp$ are also large? If $A$ is Hermitian and $E$ is the spectral measure of $A$ , and if, for every Borel subset $M$ of the real line, $E(M) = \mathbf{0}$ or $\mathbf{I}$ , then $A$ is a scalar multiple of $\mathbf{I}$ .
It follows easily that if, for every $M$ , the dimension of the range of $E(M)$ is finite or co-finite, then $A$ differs from a scalar multiple of $\mathbf{I}$ by a finite-dimensional operator. In the contrary case both $E(M)$ and $\mathbf{I} - E(M)$ have infinite-dimensional ranges for some $M$ . In either case, the conclusion of the lemma is obvious. Q2. Could someone please explain the details of the above proof? It seems very cryptic and not at all straightforward. Thank you! Related Posts : When is a subspace of a Hilbert space large?","['hilbert-spaces', 'proof-explanation', 'operator-theory', 'functional-analysis']"
4390383,Show that $ \sum_{k=1}^n k n = \mathrm{O}(n^3)$,"Cheers, I have to show that $ \sum_{k=1}^n k n = \mathrm{O}(n^3)$ . It's a fairly easy question, but I need some answers as to that I am allowed to do. The first way to solve this is pretty easy I think, so I stated: $$n + 2n + 3n + \cdots + n \cdot n \leq \\ n \cdot n + n \cdot n + n \cdot n + \cdots n \cdot n = n \cdot n \cdot n = n^3 $$ so we proved it one way, basically. Now I also tried to solve it using the limits. So I tried saying something like this: We have to prove that: $$ \lim_{n \to \infty} \frac{\sum_{k=1}^n k n}{n^3} = 0$$ Now at this point, I have a question. Is this fraction even eligible to use L'Hopital's rule, and if yes how would that be applied? I am thinking that the limit would boil down to: $$ \lim_{n \to \infty} \frac{\sum_{k=1}^n k n}{n^3} \stackrel{\frac{\infty}{\infty}(?)}{=} \lim_{n \to \infty} \frac{\sum_{k=1}^n k}{3n^2} = 0 $$ but I don't know If I am exactly allowed to even do that. I also tried to split them, so I'd get: $$ \lim_{n \to \infty} \frac{n}{n^3} + \frac{2n}{n^3} + \frac{3n}{n^3} + \cdots + \frac{n^2}{n^3} = 0 + 0 + 0 + \cdots + 0 = 0$$ Would that be a correct answer as well? Thanks for any help =)","['limits', 'asymptotics', 'discrete-mathematics']"
4390397,"Why must a discrete atomic measure admit a decomposition into Dirac measures? Moreover, what is ""an atomic class""?","$\newcommand{\A}{\mathcal{A}}$ EDIT: Half of this question may be answered by this post, which remarks that the definitions of atomic space indeed are contradictory in general, so my suspicion that Wikipedia was being lazy was correct. I still would like clarification on the discrete measure and atomic class definitions. OP: All of this is a series of quibbles with the very briefly written Wikipedia article on measure-theoretic atoms . Let $(X,\Sigma,\mu)$ be a measure space. An atom is a set $\A\in\Sigma$ for which $0\lt\mu(\A)$ and for every measurable $B\subseteq\A$ , either $\mu(B)=0$ or $\mu(B)=\mu(\A)$ . The space is defined to be atomic, or purely atomic, if every measurable set of strictly positive measure contains an atom. Suppose $X$ is $\sigma$ -finite. Then if $X$ is atomic, there are disjoint atoms $\{\A_n:n\in\Bbb N\}$ such that $X=\bigsqcup_{n\in\Bbb N}\A_n$ . So, $\sigma$ -finitude implies there are disjoint measurable $\{E_n:n\in\Bbb N\}$ with $X=\bigsqcup_{n\in\Bbb N}E_n$ , $\mu(E_n)\lt\infty$ for all $n$ . The fact that $X$ is atomic implies every single one of the $E_n$ is either null or containing an atom. I note that if any of the $E_n$ are null, this makes the proof really very difficult since no null set can contain an atom. I am not even sure if the statement remains true in this case! Assuming all the $E_n$ are $\mu$ -positive, they then all contain atoms $\A_n$ . I cannot see where the argument might lead from here. It is completely possible that $E_n\setminus\A_n\neq\emptyset$ for some $n$ , and then in order for the result to hold we need to find a partition of $E_n\setminus\A_n$ into countably many disjoint atoms. However, it is again still possible that $\mu(E_n\setminus A_n)=0$ for the same $n$ , at which point we have a problem - there is no set of atoms with $E_n$ as their union. I am dubious that the general result is true at all... how do we show this? I am beginning to think this is more a definition rather than a proof, but Wikipedia claims the two are formally equivalent statements. The reverse direction is not even true, since it possible for an atom to have infinite measure (at least, Wikipedia did not define any otherwise). Now about discrete measures: If $\A$ is an atom, all the subsets in the $\mu$ -equivalence class of $[\A]$ of $\A$ are atoms, and $[\A]$ is called an atomic class. I think Wikipedia means by this: the atomic class of $\A$ is the set of all atoms with the same measure as $\A$ . However, they seem to think that the $\mu$ -equivalence class - my only guess is that this means $E\sim F\iff\mu(E)=\mu(F)$ - of $\A$ will contain only atoms, but this is ridiculous so I think "" $\mu$ -equivalence"" is referring to something else. Perhaps they mean the Nikodym metric: $A\sim B\iff\mu(A\Delta B)=0$ where $\Delta$ is symmetric difference. So, there is a confusion on the definition of atomic class. Leaving that aside for now, they say that a discrete space is a $\sigma$ -finite measure space in which all atomic classes have nonempty self-intersection: $$\forall[\A],\,\bigcap_{A\in[\A]}A\neq\emptyset$$ But then they claim that this is equivalent to "" $\mu$ is the countable weighted sum of Dirac measures"", i.e.: $$\mu=\sum_{n=1}^\infty c_n\cdot\delta_{x_n}$$ Where $x_n$ is some point in the intersection of each atomic class (there are countably many since they assert that in a $\sigma$ -finite space there can only be countably many atomic classes - I am unsure of this because I am unsure of the definition of atomic class). However, to me this automatically implies that the singletons $\{x_n\}$ are all atoms since $\mu(x_n)=c_n$ by definition of $\delta$ . This does not follow (in any way that I can see) from the intersection definition of atomic class, so I am suspicious. The reverse implication that discrete spaces are atomic is also not obvious to me, since if we suppose $\mu:=\sum_{n=1}^\infty c_n\delta_{x_n}$ there is no clear way to generate a partition of atoms $\A_n$ which make up $X$ from this. Summary: Is $\sigma$ -finite and ""every set of strictly positive measure contains an atom"" really equivalent to ""there is a disjoint partition of $X$ into atoms""? Answer: no, not in general. Even if one assumes $\sigma$ -finitude of the space, the result can only be proved if one allows for the partition to have up to a null set, or if one includes null sets as atoms (but this is silly to me, since there would no longer be any atomless measures) - see the linked post. I have edited the section of that article accordingly. What does Wikipedia really mean when they define atomic class? How is discrete - as defined by nonempty intersections of atomic classes - equivalent to the measure being a sum of Dirac measures? I am aware that the asking of multiple questions is discouraged but these are individually all fairly brief to answer and they are so related that it would feel strange to ask them separately.","['measure-theory', 'dirac-delta', 'definition', 'discrete-mathematics', 'terminology']"
4390428,Suppose $E[g(W) ] = 2E[1_{ \{V\le cW \}} g(W) ]=0$ for all $g$ where $W=V+N$ and $N$ is Gaussian iff $V$ is Gaussian,"Suppose that $W=V+N$ where $N$ is standard normal independent of $V$ .   Assume that for a given constant $c >0$ , the following equality holds for all functions $g:\mathbb{R} \to \mathbb{R}$ : \begin{align}
E[g(W) ] = 2E[1_{ \{V\le cW \}} g(W) ]
\end{align} Question: Can we show that the only random variable $V$ that satisfies the above is zero mean gaussian (where variance depends on $c$ )? I know how to prove the forward direction, but not the backward direction. Forward Direction First, note \begin{align}
E[  1_{ \{V\le cW \}} g(W) ]&=E \left[ E \left[ 1_{ \{V\le cW \}} |W  \right]g(W) \right]\\
&=E \left[ F_{V|W}(cW|W)g(W) \right]
\end{align} thus the equation of interest can be re-written as \begin{align}
E[g(W) ]=2E \left[ F_{V|W}(cW|W)g(W) \right], \forall g
\end{align} where $F_{V|W}(v|w)$ is a conditional cdf. Now note that if $V$ is gaussian then we have that $V|W=w$ is also gaussian \begin{align}
F_{V|W}(v|w)=\Phi \left( \frac{v-E[V|W=w]}{Var(V|W=w)} \right)=\Phi \left( \frac{v-bv}{b} \right)
\end{align} where we have used that $E[V|W=w]=bw$ and $Var(V|W=w)=b=\frac{E[VW]}{E[W^2]}=\frac{\sigma_V^2}{1+\sigma_V^2}$ .
Therefore, if we choose $c=b$ , then \begin{align}
2E \left[ F_{V|W}(cW|W)g(W) \right]=2E \left[ \Phi \left( \frac{cW-bW}{b} \right) g(W)\right]=2 E \left[ \frac{1}{2} g(W)\right].
\end{align} This concludes the proof of the direct part. Some thoughts I was trying to use the Stein's equation to prove this but never could get it quite right.","['expected-value', 'probability-theory', 'probability']"
4390467,Is there a complex analytic function that acts like sigmoid on the reals?,"I'm seeking for a function $f: \mathbb{C} → \mathbb{C}$ such that: $f$ is complex analytic $f$ has no singularities $f$ is real-valued over the reals $f$ is bounded over the reals $f'$ is positive real over the reals $f''$ is positive (resp. negative) real over the negative (resp. positive) reals In other words, I'm looking for a sigmoid-like real function that extends to an entire, complex analytic function. It can be readily seen that some basic functions fail. For example, ""the"" sigmoid function $f(z) = (1 + \exp(-z))^{-1}$ , has singularities on $\pi i + \langle 2\pi i \rangle$ . For another example, if $f(z) = \arctan z$ , this function is multivalued, and no branch cut admits an analytic function. Due to Liouville's Theorem, such function cannot be bounded on the entire complex plane.  Yet this question requires it to be bounded on the reals. Does this looser requirement enable a solution to be there?",['complex-analysis']
4390480,Showing that $B_0(H)^{**}= B(H)$,"Let $H$ be a Hilbert space. Consider the $C^*$ -algebra of compact operators $B_0(H)$ and its double enveloping von Neumann algebra $B_0(H)^{**}$ . I want to show that $B_0(H)^{**}= B(H)$ as $C^*$ -algebras . Though I didn't check the details, we can probably proceed as follows: Given a $C^*$ -algebra $A$ , there exists a unique $W^*$ -algebra
structure on the Banach space dual $A^{**}$ such that the canonical
inclusion $i: A \hookrightarrow A^{**}$ becomes a $*$ -homomorphism
with weak $^*$ -dense range. Then, using that we can canonically identify $B_0(H)^{**}=B(H)$ as Banach spaces in a way such that the canonical inclusion maps agree under the identifications, it follows that $B_0(H)^{**}=B(H)$ as $C^*$ -algebras. A possible other approach is the following: We check that the identity map $\pi: B_0(H) \hookrightarrow B(H)$ is a universal representation. Since the enveloping von Neumann algebra can be abstractly defined as the bicommutant of the image of a universal representation, we will be done once again. Again, I didn't check all the details but at first sight this also seems to work. Are there other ways to see the isomorphism $B_0(H)^{**}= B(H)$ on the $C^*$ -level? For example, if we follow the GNS-construction to canonically define the universal representation of $B_0(H)$ and take the von Neumann algebra generated by its image, is it clear that we get a $C^*$ -algebra isomorphic to $B(H)$ ? Thanks in advance.","['von-neumann-algebras', 'c-star-algebras', 'operator-algebras', 'functional-analysis', 'dual-spaces']"
4390483,When is the compact-open topology locally compact?,"Let $X$ and $Y$ be topological spaces, and consider the compact-open topology on $C(X,Y)$ , which is generated by open sets of the form $$\{\text{continuous }f\colon X\to Y:f(K)\subseteq U\}\text{ for compact }K\subseteq X\text{ and open }U\subseteq Y.$$ Without assuming that $Y$ is a metric space, are there conditions on $X$ and $Y$ that imply that the compact open topology on $C(X,Y)$ is locally compact? One related result of particular importance is the fact that $\mathrm{Hom}(G,T)$ is locally compact, where $G$ is a locally compact Hausdorff abelian group, and $T$ is the circle group. This is important for Pontryagin duality, because it is one part of knowing that the Pontryagin dual of a locally compact Hausdorff abelian group is still a locally compact Hausdorff abelian group. For this related result, the standard proof uses equicontinuity and Arzela-Ascoli. But this approach only works if $Y$ is a metric space.","['function-spaces', 'arzela-ascoli', 'general-topology', 'equicontinuity']"
4390507,If $g\in G$ and ${\rm ord}(g)=7$ and $h\in G$ s.t. ${\rm ord}(h)=11$ then find the minimun value of $|G|$,"Let $G$ a group.
If $g\in G$ and ${\rm ord}(g)=7$ and $h\in G$ such that ${\rm ord}(h)=11$ then find the minimun value of $|G|$ My attempt As $g\in G$ then by Lagrange theorem we have that ${\rm ord}(g) = 7$ divide $|G|$ The same applies to: $h\in G$ then ${\rm ord}(h)=11$ divide $|G|$ From this we can conclude that the minimun $|G|$ is $77$ . Is correct this?","['group-theory', 'abstract-algebra', 'finite-groups']"
