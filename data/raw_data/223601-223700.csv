question_id,title,body,tags
4598157,Does the forgetful functor from ${\rm Ab}$ to ${\rm Grp}$ preserve coequalizers?,"Let $f, g$ morphisms from $G$ to $H$ abelian groups. The coequalizer $C$ in ${\rm Grp}$ will be an abelian group. Must it coincide with the coequalizer in ${\rm Ab}$ ? It seems like it should because given $\alpha : H \to X$ that coequalizes $f, g$ we are forced to send elements of $C$ to the image of $\alpha$ which is abelian. Is there a simpler argument to prove this, or am I wrong?","['group-theory', 'abelian-groups', 'category-theory']"
4598172,Find a permutation $X \in S_6$ which satisfies the equation,"$\begin{pmatrix}1&2&3&4&5&6\\ 4&3&1&5&2&6\end{pmatrix} \circ X = \begin{pmatrix}1&2&3&4&5&6\\ 3&2&4&5&1&6\end{pmatrix}$ I've tried this solution and checked this website , but none of them were helpful or my problem is little bit different and may require more steps to get the solution. Could anybody help me sort this out?","['permutations', 'functions', 'permutation-matrices', 'permutation-cycles']"
4598174,Doubt about inequalities and integrals - on proving an operator is well defined.,"Exercise. Consider the operator $T\colon \mathcal C[0,1] \to \mathcal C[0,1]$ defined by $$ (Tf)(x) = \int_0^x f(t) \,  dt \quad \forall f \in \mathcal C[0,1].$$ Show that T is well-defined. My solution. To show that $T$ is well-defined, the crucial part is to show that $Tf \in \mathcal C[0,1],$ for all $f \in \mathcal C[0,1].$ To do so, one must show that $Tf$ is continuous in the closed interval $[0,1].$ Thus, fix a function $f \in \mathcal C[0,1],$ a point $c \in [0,1]$ and a value $\varepsilon >0,$ all arbitrarily.  Then, $$ | (Tf)(x) - (Tf)(c) | = \Bigg| \int_0^x f(t) \, dt - \int_0^c f(t) \, dt \Bigg| = \Bigg| \int_c^x f(t) \, dt \Bigg| \color{red}{\leqslant \int_c^x |f(t)| \, dt }.$$ Now, since $f$ is continuous in a compact interval, $f$ is also bounded and thus $|f(t)| \leqslant M,$ for some $M>0.$ Thus, based on the inequality we just found, $$ |(Tf)(x) - (Tf)(c)| \color{red}{\leqslant \int_c^x |f(t)|\, dt}\color{purple}{ \leqslant \int_c^x M dt} = M(x-c) \leqslant M|x-c|.$$ With this, if we take $|x-c| < {\displaystyle \frac{\varepsilon}{M}}$ it follows that $$ \forall \varepsilon > 0, \exists \delta = \delta(\varepsilon) > 0\colon |x-c| < \delta \Rightarrow |(Tf)(x) - (Tf)(c)| < \varepsilon, \quad \forall c \in [0,1].$$ proving the continuity of $Tf$ in $[0,1].$ My concerns. My only problem with my solution is applying integration to inequalities. I know the following general results hold: Theorem. Let $f$ be a real/complex function continuous in the closed interval $[a,b].$ Then, $$ \Bigg| \int_a^b f(t) \, dt \Bigg| \leqslant \int_a^b |f(t)| \, dt. $$ I have used this result in the steps I colored in $\color{red}{red}$ but I am not sure about one thing: I am certainly not guaranteed that $x>c$ . Does this inequality also hold if $x \leqslant c?$ If so,why? The inequation in $\color{purple}{purple}$ represents the same exact problem, because we know that $|f(t)| \leqslant M$ and I can apply integrals in both sides of the inequality if $x>c$ , but what if $x \leqslant c?$ Thanks for any help in advance.","['integration', 'definite-integrals', 'solution-verification', 'functional-analysis', 'inequality']"
4598239,"Does the Law of Large Number ""work better"" for some distributions compared to others?","Does the law of large numbers ""work better"" for certain types of distributions compared to others? For example, imagine one country where the distribution of income is normally distributed - and imagine another country (same sized population as the first) in which most people earn almost no money, and there is one trillionaire. Using the law of large numbers, average incomes calculated from random samples using the first country will tend to reflect the true average income of this country .... but in the second country, random samples will likely not be close to true mean unless the trillionaire citizen is included in the sample, and even if he is included, it might not be close. Thus, in heavily skewed , irregular and multimodal distributions such as the second country - does the law of large numbers some how work ""worse"" than the first country? Although increasing the sample size in both countries will likely produce estimates closer to the real mean income .... I have a feeling that fewer random samples are required if the distribution is less irregular... thus, the law of large numbers indirectly is affected by the type of distribution. Is this true? And if this is true, what does this phenomena (i.e. sample size/accuracy relationship vs irregularity of distribution) referred to? In general, can we relate the ""irregularity"" of a probability distribution to its ""skewness""? Thanks!","['statistics', 'law-of-large-numbers', 'probability']"
4598245,How to measure the “Skewness” of a Probability Distribution?,"I looked at the ""Fisher Measure"" formula for ""skewness of a probability distribution"" ( https://en.wikipedia.org/wiki/Skewness ) - this is related to the expectation of the third moment for some transformed probability distribution. How does this seemingly arbitrary formula using the expectation of the third moment specifically measure how skewed the probability distribution is? In other words, what is the motivation behind it? Why does this not incorporate, for example, the second or fourth moment? Why even involve moments in this calculation to characterize the skewness of a distribution?","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4598275,"Weak convergence is defined in terms of $C_{b}$ functions, but for $\mathbb{R}^{d}$ why is it sufficient to show convergence for $C_{c}$ functions? [duplicate]","This question already has answers here : How to prove that $\mu_n \rightharpoonup \mu$ IFF $\mu_n \overset{*}{\rightharpoonup} \mu$ and $\mu_n (X) \to \mu (X)$? (2 answers) Closed 1 year ago . Let $(X, \mathcal{F})$ be a measurable space, and $\mu_{n}, \mu$ probability measures on it. $\mu_{n}$ is said to converge weakly to $\mu$ if for any bounded continuous functions $f$ on $X$ , $\int f d\mu_{n} \xrightarrow{} \int f d\mu$ . The professor mentioned if $X = \mathbb{R}^{d}$ and $\mathcal{F}$ is the Borel sigma algebra, then it is enough to check the convergence of integrals for any compactly supported continuous function $f$ . But why is this true?","['weak-convergence', 'measure-theory', 'probability']"
4598298,"$f^{-1}(0)=g^{-1}(0)$, $f^{-1}(1)=g^{-1}(1)$, then $f=g$","$f(x)$ and $g(x)$ are two polynomials in $\mathbb{C}[x]$ , and $$f^{-1}(a)=\{z\in \mathbb{C}\mid f(z)=a\}$$ if $$f^{-1}(0)=g^{-1}(0),\;f^{-1}(1)=g^{-1}(1)$$ then $f=g$ holds. I know $f^{-1}(0)=g^{-1}(0)$ implies that the two polynomials have the same set of zero points, but I dont know how to use the property of $f^{-1}(1)=g^{-1}(1)$ .","['linear-algebra', 'analysis', 'real-analysis']"
4598309,Connection between the linear algebra and calculus explanation of linearity of the derivative operator,"This is too-embarrassing-to-ask question that keeps bothering me in the back of my mind: It's clear that $$D(f + g) = D(f) + D(g)$$ and that $$ D(c f)= c D(f)$$ for $f, g$ differentiable functions and $c$ a scalar. On the other hand we can construct the Jacobian matrix of partial derivatives, or a gradient and multiply it times a directional vector. Or we can talk about the best linear approximation of a function, or the affine approximation, and it all makes sense independently. But it bothers me that an operator that transforms nonlinear functions to other nonlinear functions (e.f. polynomials of grade $3$ to $2$ ) can be called linear. How are all the arguments reconciled with each other, and with the nonlinearity of the input functions being modified?","['multivariable-calculus', 'linear-algebra', 'linear-transformations']"
4598317,"Prove $a^x+a^{1/x}\le 2a$ ($\frac{1}{2}\le a < 1$) for any $x\in (0,\infty)$","Prove $a^x+a^{1/x}\le 2a$ (with $\frac{1}{2}\le a < 1$ ) for any $x\in (0,\infty)$ . Is it true? I can't figure out how to prove it.
It may use the conclusion $\ln x+\frac{1-x}{x}\ge 0, \forall x\in (0,+\infty) $ as this question is followed. UPDATED: I have find a complicated method to prove it.
Let $f(x) = a^x+a^{1/x}-2a$ , with $\frac{1}{2}\le a<1, x\in(0,\infty)$ . $\therefore f'(x)=a^{1/x}\ln \frac{1}{a}(\frac{1}{x^2}-a^{x-1/x})$ we have $a^{1/x}\ln\frac{1}{a}>0$ , so we can only consider $g(x)=\frac{1}{x^2}-a^{x-1/x}, x>0, \frac{1}{2}\le a<1$ we can make the observation that $g(1)=0$ , but we are not sure if it has any other roots . So we have to do some careful analysis. If we keep taking derivative, we will get into endless trouble. Note that $g(x)=\frac{1}{x^2}-a^{x-1/x}=a^{-2\log_a x}-a^{x-1/x}$ because $a<1$ , so $a^x$ is decreasing, we can consider $h(x)=-2\log_a x-(x-\frac{1}{x})=-2\log_a x-x+\frac{1}{x},x>0,\frac{1}{2}\le a<1$ This is much easier. $h'(x)=-\dfrac{x^2+\dfrac{2}{\ln a}\cdot x+1}{x^2}$ because $\frac{1}{2}\le a<1$ , so $\frac{2}{\ln a}\le -\frac{2}{\ln 2}\approx -2.9$ . For the quadratic functions in the numerator, $\Delta >0$ , there are two positive zeros $x_1,x_2$ and $0<x_1<1<x_2$ , we have: In $(0,x_1)$ and $(x_2,\infty)$ ， $h'(x)<0$ , so $h(x)$ is decreasing. In $(x_1, x_2)$ , $h'(x)>0$ , so $h(x)$ is increasing. Note that $h(1)=0$ , and when $x\to 0^+$ , $h(x)\to +\infty$ , when $x\to +\infty, h(x)\to -\infty$ , therefore, the graph of $h(x)$ is illustrated as follows: $h(x)$ "" /> Therefore, there is three zeros for $h(x)$ , i.e. $x_3, x_4=1, x_5$ . In $(0,x_3)$ and $(1,x_5)$ , $h(x)>0$ , so $g(x)<0$ , so $f'(x)<0$ , so $f(x)$ is decreasing. In $(x_3,1)$ and $(x_5,+\infty)$ , $h(x)<0$ , so $g(x)>0$ , so $f'(x)>0$ , so $f(x)$ is increasing. Note that $f(1)=0$ , and when $x\to 0^+$ , $f(x)\to 1-2a\le 0$ ; when $x\to +\infty$ , $f(x)\to 1-2a\le 0$ . The graph of $f(x)$ can be illustrated as follows: $f(x)$ "" /> Now, we can surely say, the maximum of $f(x)$ is $f(1)=0$ , which means $a^x+a^{1/x}\le 2a$ , with $\frac{1}{2}\le a<1$ , $\forall x\in(0,+\infty)$ .","['inequality', 'derivatives']"
4598359,Confidence Intervals of Markov Chains?,"I understand that a Discrete Time Markov Chain ( https://en.wikipedia.org/wiki/Markov_chain ) is closely related to a Multinomial Distribution ( https://en.wikipedia.org/wiki/Multinomial_distribution ). As I understand: A Multinomial Probability Distribution is a Discrete Probability Distribution Function which is characterized by a ""k"" number of possible events and a ""k"" number of probabilities corresponding to each of these possible events. As an example, the probability corresponding to the outcome of a dice roll can be characterized as a Multinomial Probability Distribution Function. On the other hand: A Discrete Time Markov Chain has a set of discrete states (e.g. ""k"" number of states) - e.g. State ""S1"", State ""S2"", State ""S3"" A Discrete Time Markov Chain has a corresponding set of probabilities which describe the probabilities of transitioning to one of these ""k"" states conditional on the the Markov Chain being in any other of these ""k"" states - these are referred to as ""Transition Probabilities"" Suppose we consider a Discrete Time Markov Chain with k=3 states - this means that there will be 3 x 3 = 9 transition probabilities : p11, p12, p13, p21, p22, p23, p31, p32, p33. Together, these 9 transition probabilities can be placed into a ""Transition Matrix"" (3x3) . As such, (p11, p12, p13) are in the first row of this transition matrix, (p21, p22, p23) are in the second row of this transition matrix, and (p31, p32, p33) are in the third row of this transition matrix. Now, consider just one row of this transition matrix. These three probabilities describe the ""fate"" of the Markov Chain provided they are currently situated in State ""S1"". As such, at this point we can consider the 3 transition probabilities as parameters of a Multinomial Probability Distribution. Now, this leads me to my question. The Wikipedia page on the Multinomial Distribution shows the properties of a Random Variable ""X"" following a Multinomial Distribution - for example, if a Random Variable ""X"" follows a Multinomial Distribution, we can find out the Mean and the Variance of ""X"" However, given some data (e.g. the Markov Chain was observed over 10 days to be in S1, S1, S2, S1, S2, S3, S1, S1, S1, S2) it is not immediately clear as to how we can estimate the transition probabilities and the variance of these transition probabilities. In other words, how can I estimate the mean and variance for the parameters of a Multinomial Distribution - NOT just the mean and variance for a Random Variable that follows a Multinomial Distribution? After doing some reading online, I came across the following link (e.g. https://www.stat.cmu.edu/~cshalizi/462/lectures/06/markov-mle.pdf , Maximum Likelihood Estimator of parameters of multinomial distribution ) which shows how Maximum Likelihood Estimation can be used to derive formula that can estimate the transition probabilities (i.e. parameters of a Multinomial Distribution) of a Markov Chain. Essentially, this is done by differentiating the log-likelihood function of the Multinomial Distribution with respect to each one of the parameters (i.e. dependent on the number of states), setting them to 0, and then solving the resulting system of equations. Doing this will provide the (""obvious"") estimates for these transition probabilities, e.g. P11 = (Number of Times Markov Chain was in State 1 and Transitioned to State 1) / (Number of Times Markov Chain was in State 1 and Transitioned to State 1 + Number of Times Markov Chain was in State 1 and Transitioned to State 2 + Number of Times Markov Chain was in State 1 and Transitioned to State 3) But I am still confused as to how the variance estimates for the transition probabilities can be calculated. Currently, several ideas come to mind: We can use ""bootstrap sampling"" ( https://en.wikipedia.org/wiki/Bootstrapping_(statistics) ) to randomly sample consecutive sequences of historical states (so that the chronological order of the state sequences are not interrupted) and calculate the transition probabilities based on this random sample. If we repeat this random sampling many times, we will now get a range of transition probabilities (e.g. p11_1st_sample, p11_2nd_sample...p11_nth_sample ...... p31_1st_sample...p33_nth_sample). For each transition probability, if we rank these random samples from smallest to largest and then take the 5th and 95th percentile, we construct a ""crude"" range of possible values that each transition probability could assume and their relative deviance from the mean value. The second way could be through the ""Delta Method"" ( https://en.wikipedia.org/wiki/Delta_method ), in which a formulation for the variance is first constructed, a 2nd Order Taylor Expansion is then approximated -  and finally we evaluate the variance of this 2nd Order Taylor Expansion using the Laws of Expectation. But this looks very complicated and I don't know how to do this? I remember hearing there is an inverse relationship between the Fisher Information of a Probability Distribution and the Variance for the Parameters Belonging to this Probability Distribution - supposedly doing some algebraic manipulations might lead to a formula for the variance of these parameters ... but I am not sure about this, and this seems is even more complicated! Finally, some classic methods involving Method of Moments and Var(x) = E(x^2) - (E(x))^2 might also be possible? I am interested in seeing a theoretical formulation for the variance of Transition Probabilities in a Discrete Time Markov Chain (or equivalently, the parameters of a Multinomial Distribution). I have spent a lot of time searching for a reference that clearly explains this - but I have not been able to find something. Can someone please help me in deriving these variance estimates alongside with providing an explanation as to how you approached and solved this problem? Thanks! Note: Throughout this whole question, I have assumed that the mean estimate for the parameters of a Multinomial Distribution are equal to the mean estimate for the Transition Probabilities of a Discrete Time Markov Chain ... and that the variance estimates for the parameters of a Multinomial Distribution are equal to the variance estimates for the Transitional Probabilities of a Discrete Time Markov Chain. In general, is this even true?","['confidence-interval', 'statistics', 'markov-chains', 'probability']"
4598360,Converting a differential equation to a hypergeometric equation,"I would like  to transform the following differential equation into a hypergeometric equation: ${z^2}\frac{{{d^2}W}}{{d{z^2}}} + z\frac{{{d^{}}W}}{{d{z^{}}}} + \left( {{A^2} - {B^2}\left( {1 - {C_{}}z + D\frac{{z\,}}{{{{\left( {1 - z} \right)}^{}}}}} \right) - \frac{F}{{{{\left( {1 - z} \right)}^2}}}} \right)W = 0.$ Here A, B, C, D and F are constants. I tried by letting I tried by letting $W = {z^p}{\left( {1 - z} \right)^q}y.$ After replacing it into the above equation, I  ran into trouble trying to ""fit"" the values of p and q so that the y  term in the resulting equation would be independent of z.  I thought it was a  straightforward thing to do.  However, the values that I got for p and q did not reduce the equation to a hypergeometric one. Obviously, I am missing something simple here. Any advice will be most appreciated.","['ordinary-differential-equations', 'hypergeometric-function']"
4598390,A projection operator is linear iff $X$ is a Hilbert space,"This question comes from Linear and Nonlinear Functional Analysis with Applications (Philippe G. Ciarlet), Chapter 4, Problem 4.3-4. 4.3-4 Let $\mathcal{P}_n[0,1]=\left\{\left.p\right|_{[0,1]} ; p \in \mathcal{P}_n\right\}$ , where $\mathcal{P}_n$ denotes the space of all polynomials $p: \mathbb{R} \rightarrow \mathbb{R}$ of degree $\leq n$ , and let a number $q>1$ be given. (1) Show that, given any function $f \in \mathcal{C}[0,1]$ , there exists a unique polynomial $P f \in \mathcal{P}_n[0,1]$ such that $$
\|f-P f\|_{L^q(0,1)}=\inf _{p \in \mathcal{P}_n(0,1]}\|f-p\|_{L^q(0,1)} .
$$ (2) Show that the mapping $P: \mathcal{C}[0,1] \rightarrow \mathcal{P}_n[0,1]$ defined in this fashion is linear if and only if $q=2$ (the proof of the ""if"" part is similar to that of Theorem 4.3-1(e)). I have proved (1), my question is about how to prove (2) $""\Rightarrow""$ (only if) part. Any help is appreciated!","['hilbert-spaces', 'projection', 'functional-analysis']"
4598427,Is restriction of a ring automorphism a subring automorphism?,"$\phi:R\to R$ is a ring automorphism. $S \subset R$ is a subring and $\phi(S)\subseteq S$ Is $\phi|_S$ necessarily an automorphism of $S$ ? I can easily check that $\phi|_S:S\to S$ is a homomorphism and injective, but is it necessarily surjective?","['ring-homomorphism', 'ring-theory', 'abstract-algebra']"
4598504,Connection between the $\Bbb R^3$ and $\Bbb R^2$ gradients of a level curve of a hill and the level surface in $\Bbb R^4$ represented by the same hill,"I understand (or at least I think I understand) intuitively why the gradient of a surface in $\Bbb R^3$ (like a hill) must be perpendicular to the level sets that cut through the hill at various heights and that this vector represents the magnitude and direction of most rapid ascent. I think I also understand why the gradient is normal to a plane tangent to a surface at a particular point. It took me a while to reconcile these two things because I wondered how the gradient could be both of those things at once and the conclusion that I came to was that one gradient was a gradient in $\Bbb R^2$ describing the direction of greatest ascent for the hill in $\Bbb R^3$ and the gradient that describes the vector normal to a plane tangent to a surface (in this case, the hill) at a particular point is the gradient of the level surface in $\Bbb R^3$ , the hill being now being considered a level surface to a hyper-surface in $\Bbb R^4$ which is described by a different function. I guess firstly, I would like to know if my intuition is correct so far and then secondly, if it is, I would like to know if the gradient of the hyper-surface (the normal of a plane that's tangent to a particular point on the hill with the hill being considered a level surface to the hyper-surface) has any connection to the two dimensional gradient which lives in the $xy$ -plane and shows the direction and magnitude of greatest ascent up the hill and is orthogonal to the level set that lives in the $xy$ -plane. What my brain was conjuring up was that maybe the $\Bbb R^3$ gradient was connected to the $\Bbb R^2$ gradient in that the $\Bbb R^2$ gradient was the projection of the $\Bbb R^3$ gradient/normal to the tangent plane, or perhaps the normal to the tangent plane's ""underside"" when considering a particular point on the hill. Essentially that the $\Bbb R^3$ gradient (or negative $\Bbb R^3$ gradient) could be considered to produce a ""shadow"" projection beneath it on the $xy$ -plane and that projective shadow would correspond to the $\Bbb R^2$ gradient at that particular point on the hill's projection. I am struggling to explain this in words so I tried to illustrate what I mean on the attached image. I feel like I am not understanding something here, but I don't exactly know what it is. Any input would be greatly appreciated. Thanks.",['multivariable-calculus']
4598507,Why is Lie group unimodularity related to the determinant of the adjoint?,"I'm reading through Michael Taylor's notes from this PDF . The author starts saying that given an $N$ -dimensional Lie group $G$ and some covector $\omega_e \in \bigwedge^N T^*_e G$ , there is a unique differential $N$ -form $\omega_\ell$ that is left-invariant, meaning $\omega_\ell(e)=\omega_e$ and $L_g^*\omega_\ell=\omega_\ell$ with $L_g(h)\equiv gh$ . Similarly, there is a unique right-invariant $\omega_r(e)=\omega_r$ , $R_g^* \omega_r =\omega_r$ .
They then observe that these must also satisfy $$R_h^* \omega_\ell = \alpha(h)\omega_\ell,
\qquad L_h^* \omega_r = \beta(g) \omega_r,$$ for some pair of homomorphisms $\alpha,\beta:G\to(0,\infty)$ .
The group is said to be unimodular iff $\alpha=1$ or $\beta=1$ . Shortly thereafter, the authors discuss the adjoint representation of the group, by defining $$K_g:G\to G, \qquad K_g(h)\equiv ghg^{-1}$$ and defining $\operatorname{Ad}(g)=DK_g(e):T_e G\to T_e G$ , with $D$ denoting the differential.
After a few standard observations about the adjoint representation, they say that comparing the two equations above, we find that $$\alpha(g) = \det(\operatorname{Ad}(g)).$$ Is there a more explicit way to see where this relation comes from? I can see some connection: I can write $K_g=L_g \circ R_g^{-1}$ and thus $$\operatorname{Ad}(g)=DK_g(e) = (DL_g(g^{-1}))\circ (D R_g^{-1}(e)).$$ Still, I think I'm missing something, because I don't how to link this with the statements about left- or right-invariant differential forms made previously, as well as whether we should make a choice of $\omega_e$ for this to work, etc.","['representation-theory', 'lie-groups', 'differential-geometry']"
4598527,How to interpret a strange formula about $\zeta'(s)/\zeta(s)$,"I obtained a strange formula about $\zeta'(s)/\zeta(s)$ $$
\begin{split}
\frac{\zeta'(s)}{\zeta(s)}-(2\pi)^s&\sum_{\Im(\rho)>0} (-i\rho)^{-s}(2\pi)^{-\rho} e^{-i\pi \rho / 2} \Gamma(\rho)\;\;\text{ converges for } \Re(s)>1 \\ &\color{red}{\text{ and it extends analytically to } \Re(s)  >0}
\end{split} \label{1}\tag{1}
$$ In other words the generalized Dirichlet series $\sum_{\Im(\rho)>0} (2i\pi)^{-\rho}\Gamma(\rho) (\frac{\rho}{2i\pi})^{-s}$ has the same non-trivial poles as $\zeta'(s)/\zeta(s)$ , it encodes the Riemann hypothesis. I don't understand if it is obvious, expected, and how to categorize it in the zoo of Riemann zeta formulas. Would you have any insight? The proof of \eqref{1} is as follow: for $\Re(z)  >0$ let $f(z)=\sum_{n\ge 1} \Lambda(n) e^{-nz}$ , so that $$\DeclareMathOperator{\Res}{Res}
\frac{-\zeta'(s)}{\zeta(s)} \Gamma(s) = \int_0^\infty f(x)x^{s-1}dx,\quad\Re(s) > 1.
$$ We have the Mellin inversion and explicit formula, which converges without problems $$
\begin{split}
f(z) &=\frac1{2i\pi}\int_{(2)} \frac{-\zeta'(s)}{\zeta(s)} \Gamma(s) z^{-s}ds = \sum \Res\left(\frac{-\zeta'(s)}{\zeta(s)} \Gamma(s) z^{-s}\right)  \\
& = z^{-1}- \sum_\rho \Gamma(\rho) z^{-\rho}+
\sum_{k=0}^\infty \Res\left(\frac{-\zeta'(s)}{\zeta(s)} \Gamma(s) z^{-s},-k\right)
\end{split}$$ $\sum_\rho$ means sum over non-trivial zeros. $z^{-s}$ means $e^{-s \log z}$ with $\Im(\log z) \in (-\pi/2,\pi/2)$ . Now $\int_\epsilon^\infty f(x)x^{s-1}dx$ is entire so for the Riemann hypothesis we are interested only in the behavior of $f(x)$ as $x\to 0^+$ . $f$ is $2i\pi$ -periodic so we can look instead at the asymptotic of $f(2i\pi+x)$ as $x\to 0^+$ . Because $$
f'(z)= -\sum_{n\ge 1} \Lambda(n)ne^{-nz}=O\left(\sum_{n\ge 1} n e^{-n\Re(z)}\right)= O(1/\Re(z)^2)
$$ we get that $$
f(2i\pi + x)=f(2i \pi e^{-i x/(2\pi)})+\int_{2i \pi e^{-i x/(2\pi)}}^{2i\pi + x}f'(z)dz= f(2i \pi e^{-i x/(2\pi)})+O(1).
$$ It happens that in the sum-over-residues series for $f(2i \pi e^{-i x/(2\pi)})$ everything is $O(1)$ but $$
g(x)=- \sum_{\Im(\rho)>0} \Gamma(\rho) (2i \pi e^{-i x/(2\pi)})^{-\rho}.
$$ Therefore, $\int_0^\epsilon (f(x)-g(x))x^{s-1}dx$ is analytic for $\Re(s) >0$ and due to exponential decay so is $\int_0^\infty (f(x)-g(x))x^{s-1}dx$ which is what \eqref{1} says.","['complex-analysis', 'riemann-zeta', 'analytic-number-theory', 'dirichlet-series']"
4598531,Who is the father of modern PDEs,"Classical PDE theory fails especially when the PDE is nonlinear, i.e. smooth initial data does not imply differentiable solutions. As a result, we often use the notion of weak solution, which has its roots in the theory of distributions in functional analysis. Who connected these two subjects which seem entirely different at least in the first glance. In other words,
who introduced the so called weak formulation/ notion of distributional solutions to PDEs.
Which is the first paper in this direction?","['functional-analysis', 'analysis', 'partial-differential-equations']"
4598601,"Drawing without replacement, order matters - probability function","I'm not a mathematician, so the description of my problem might seem a little verbose: In a multiple-choice test there is a set of three questions and a set of five possible answers. To each of the three questions one of the five answers is the correct one. No answer can be assigned twice. So three of the $5$ answers will be assigned to one of the three questions and two answers will be left.
I would now like to calculate the average score that would result from random answers if each correct answer yields one point. If I'm not mistaken, in this case I have $n!(/n-r)!=60$ possible answer combinations, but I don't know what formula I use to calculate the probability of randomly hitting a combination with $0, 1, 2$ or $3$ correct answers. As far as I can see, this is a draw without replacement where the order matters, so I can neither treat it like a dice problem (e.g. ""How likely are exactly three sixes in ten throws?"" - binomial distribution) nor like a control sample (""How likely is it that exactly $5$ of these $100$ screws are defective?"" - hypergeometric distribution), because then either the non-replacement or the order is not taken into account. I wrote down all $60$ possible permutations and added up how many points each of them would yield. If I counted correctly, there ... -... is one possible permutation that yields all three points, -... are $6$ permutations that yield 2 points, -... are $21$ permutations that yield 1 point, -... are $32$ permutations that yield 0 points. The table is in German, but I think it's not hard to guess what means what: https://i.sstatic.net/g6gZB.jpg Can someone help me with that? Thanks in advance!","['combinatorics', 'probability']"
4598602,Optimal fit of four points to a square,"Suppose I have four points in $2D$ that are approximately the corners of a square. How do I find the minimum movements of the points that turn the approximate square into a perfect square? By ""minimum movements"" I mean, for example, the sum of Euclidean movements of the points.","['optimization', 'geometry']"
4598703,Law of large numbers for the number of connected components in a random graph,"A network evolves similarly to the Preferential Attachment model, with some
important modification. The network starts at time $t = 1$ with one isolated vertex. At any step $t ≥ 2$ , a new vertex, $v_t$ , arrives and connects to precisely one of the already existing vertices with probability $1/t$ , while it connects to itself (forming a loop) also with probability $1/t$ . Let $N_t$ denote the number of connected components in this evolving graph at time $t$ . To show : there exists a deterministic sequence $a_t$ such that $N_t/a_t \xrightarrow{\mathbb{P}} 1$ as $t \to \infty$ . Here, $t = 1, 2, \dotsc$ . My attempt so far: I was thinking of computing $\mathbb{E}[N_t]$ and show that for any $\varepsilon > 0$ , $$
\lim_{t \to \infty}\mathbb{P}(|N_t - \mathbb{E}[N_t]| \geq \varepsilon) = 0.
$$ First I of course need to determine what $\mathbb{E}[N_t]$ is. Let $G_t$ be a graph at step $t$ and $n_t = |V(G_t)|$ . From the given information, I know that for $i, j \in [n_t]$ and $i \neq j$ , $$
\mathbb{P}(i \longrightarrow j | G_t) = \frac{1}{t} = \mathbb{P}(i \longrightarrow i | G_t).
$$ I have that $$
\mathbb{E}[N_{t+1}] = \mathbb{E}[\mathbb{E}[N_{t+1}|G_t]] = \mathbb{E}[\mathbb{E}[N_{t+1} - N_t + N_t|G_t]] = \mathbb{E}[N_t] + \mathbb{E}[\mathbb{E}[N_{t+1} - N_t|G_t]].
$$ I can only think of one possibility that $N_{t+1} - N_t$ is non zero, given $G_t$ . This is because if a vertex at step $t + 1$ connects to any vertex from $G_t$ , then the number of connected components does not increase, meaning that $N_{t+1} = N_t$ . However, if that vertex at step $t + 1$ forms a self-loop, then the number of connected component increases by one with probability $1/(t+1)$ . Therefore, $$
\mathbb{E}[N_{t+1}] = \mathbb{E}[N_{t}] + \frac{1}{t+1}.
$$ This is a recurrence relation that I can solve. Let $\mathbb{E}[N_1] = 1$ , which is reasonable because at $t = 1$ , there is only one isolated point and so there is only one connected component. Then, $$
\mathbb{E}[N_2] = 1 + \frac{1}{2}; \ \mathbb{E}[N_3] = 1 + \frac{1}{2} + \frac{1}{3}; \ \mathbb{E}[N_4] = 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4}; \cdots; \ \mathbb{E}[N_t] = \sum_{k=1}^t\frac{1}{k}.
$$ Now I need to prove the convergence in probability. By using the Chebychev inequality, $$
\mathbb{P}(|N_t - \mathbb{E}[N_t]| \geq \varepsilon) \leq \frac{\operatorname{Var}(N_t)}{\varepsilon^2} = \frac{\mathbb{E}[N_t^2] - \mathbb{E}[N_t]^2}{\varepsilon^2}
$$ Here is where I get stuck: I have no idea what the second moment of $N_t$ is. There is hint that $$
\log(t+1) \leq \sum_{k=1}^t\frac{1}{k} \leq 1 + \log t
$$ But with this I can bound $\mathbb{E}[N_t]^2$ from above. Other thing that I have tried to write the probability as the following: $$
\mathbb{P}(|N_t - \mathbb{E}[N_t]| \geq \varepsilon) = \mathbb{P}(N_t - \mathbb{E}[N_t] \leq-\varepsilon) + \mathbb{P}(N_t - \mathbb{E}[N_t] \geq \varepsilon)
$$ Then, by Markov's inequality and the given hint, $$
\mathbb{P}(N_t - \mathbb{E}[N_t] \geq \varepsilon) \leq \frac{\mathbb{E}[N_t]}{\varepsilon + \mathbb{E}[N_t]} \leq \frac{1 + \log t}{\varepsilon + \log(t+1)^{-1}}.
$$ This won't go to zero and I am not sure how to bound $\mathbb{P}(N_t - \mathbb{E}[N_t] \leq-\varepsilon)$ as well. What should I do?","['random-graphs', 'probability-limit-theorems', 'law-of-large-numbers', 'convergence-divergence', 'probability']"
4598722,To prove $1^1\cdot2^2\cdot 3^3...\cdot n^n<(\frac{2n+1}{3})^{\frac{n(n+1)}{2}} $,"So we have to prove the following for $n\in N $ $$1^1\cdot 2^2\cdot 3^3...\cdot n^n<\left(\frac{2n+1}{3}\right)^{\frac{n(n+1)}{2}} $$ So I used concept of weighted means (arithmetic and geometric) used AM GM inequality. $$AM=\frac{a_1w_1+a_2w_2+...+a_nw_n}{w_1+w_2+...+w_n}$$ $$GM=(a_1^{w_1}\cdot a_2^{w_2}\cdot...\cdot a_n^{w_n})^{\frac{1}{w_1+w_2+...+w_n}}$$ So here I let $w_1=1, w_2=2^1,w_3=3^1..$ and of course $a_1=1,a_2=2^1,a_3=3^2...$ So we get: $$\frac{1^1+ 2^2+ 3^3...+ n^n}{\frac{n(n+1)}{2}}>(1^1\cdot 2^2\cdot 3^3...\cdot n^n)^{\frac{1}{\frac{n(n+1)}{2}}}$$ However on lhs, I cant deal with numerator, and I feel that if it can be simplified, I would get the answer. So please help or if possible suggest new method.","['a.m.-g.m.-inequality', 'sequences-and-series']"
4598765,When do solution to differential equations belonging to the same parametric family intersect?,"General Problem I am interested in studying whether solutions to the Fokker-Planck equation: \begin{equation}
\tag{1}\label{fp}
\frac{\partial p(x, t)}{\partial t} = \textrm{div}(-p(x, t)\nabla\log q(x)) + \frac{1}{2}\Delta p, \quad p(x, 0) = p_0
\end{equation} (where $p_0$ and $q$ are probability densities), can intersect at some time $t^\star$ , when varying the choice of $q$ . More formally, I want to know if there exists a triplet $(q_1, q_2, t^\star)$ such that $p_1(x, t^\star) = p_2(x, t^\star)$ , for all $x$ , and where $p_1$ (resp. $p_2$ ) is the solution of \eqref{fp} with $q=q_1$ (resp. $q=q_2$ ), and $t^\star>0$ . I welcome any pointer to works/fields studying the intersection points of solutions of differential equations (a) belonging to some parametric family and (b) with the same initial conditions. Simpler Setting Looking first at simpler ordinary differential equations (ODEs), note that the following ODE: $$\tag{2}\label{ode1}
\frac{\textrm{d}x}{\textrm{d}t} + ax = 0, \quad x(0) = 0, \, a \neq 0
$$ is such that no triplet $(a_1, a_2, t^\star)$ (with $a_1, a_2 \ne 0$ , and $t^\star > 0$ ) verifies $x_1(t^\star)=x_2(t^\star)$ , where $x_1$ (resp. $x_2$ ) is the solution to \eqref{ode1} with $a=a_1$ (resp. $a=a_2$ ). On the other hand, for the ODE: $$\tag{3}\label{ode2}
\frac{\textrm{d}^2x}{\textrm{d}t^2} + k^2x = 0, \quad x(0) = 0, \, \frac{\textrm{d}x}{\textrm{d}t}\bigg \rvert_{t=0} = 1, \, k \in \mathbb N \setminus \{0\}
$$ the triplet $(1, 2, 2\pi)$ verifies $x_1(2\pi) = x_2(2\pi)$ , where $x_1$ (resp. $x_2$ ) is the solution to \eqref{ode2} with $k=1$ (resp. $k=2$ ). As this simpler setting is related to my original problem, I would also welcome pointers to any work in this latter simpler setting. PS: I added the tag Gradient Flows since solutions to \eqref{fp} are also solutions to the Gradient Flow of $KL(\cdot||q)$ with initial condition $p_0$ .","['stochastic-processes', 'gradient-flows', 'ordinary-differential-equations', 'partial-differential-equations']"
4598888,How to distinguish between vertical and horizontal stretch/shrink when ambiguous?,"Please bear with me. I am trying to help my daughter with her Algebra 1 homework. We are asked to describe the transformation of function f to function g as follows: $$f(x) = x$$ $$g(x) = 2x+3$$ The provided answer states that $g(x)=2x+3$ can be re-written as $$g(x)=2f(x)+3$$ and is therefore a vertical stretch by a factor of 2 (plus a vertical translation up by 3 units). Well and good. However, $g(x)=2x+3$ can also be re-written as $$g(x)=f(2x)+3$$ and be described as a horizontal shrink by a factor of 1/2. But even though this horizontal shrink gives exactly the same graph as the vertical stretch, it is not mentioned as a possible correct answer. I understand that the order of transformations is important and can give completely different graphs if you mess up the order, but this is not the case here. There is at least one more question in the study material that likewise lists the vertical stretch, but not the identical horizontal shrink, as the correct answer. Is it because g is originally expressed as $g(x)=2x+3$ ? Does this necessitate that we think of the transformation only in the vertical axis? Something to do with $y=mx+b$ where $m=2$ ? Many thanks.",['linear-algebra']
4598951,What is a Shimura variety and why should I care about them?,"Shimura varieties have come up tangentially in talks with some of my advisors. My vague understanding is that they are ""things that behave like moduli spaces of abelian varieties having some additional structure"". I am familiar with the modular curves $X(\Gamma)$ for $\Gamma$ a congruence subgroup of $SL_2(\mathbb{Z})$ , and I know they are examples of Shimura varieties. In particular $X_0(N)$ is the moduli space of elliptic curves equipped with a cyclic $N$ -isogeny, etc. I have tried to understand the Wikipedia definition of a Shimura variety, but it is pretty unintelligible to me. However, I can identify some features that are analogous to the modular curves situation, for example Wikipedia's construction involves a double coset space, similarly to constructing $X(\Gamma)$ . (That said, the definition uses a lot of Lie theory and algebraic group theory, topics I am less familiar with than number theory or algebraic geometry.) Perhaps someone could explain why Shimura varieties are important to number theory and how they fit in to problems we care about. I would really appreciate if someone could explain how the relationship between modular curves and their algebraic geometry, modular forms, and elliptic curves generalizes. Another vague intuition I have is that the modular curves are very ""special"", in that they have a complex-analytic-geometry interpretation too. You get this very nice, concrete realization and I would be very surprised if something similar happened if you looked at the moduli space of elliptic curves with $N$ -cyclic isogeny over $\mathbb{Q}_p$ , for example. How does this idea carry over when thinking about Shimura varieties? Is there a modularity theorem for Shimura varieties? Thanks so much!","['abelian-varieties', 'elliptic-curves', 'number-theory', 'modular-forms', 'arithmetic-geometry']"
4598960,Optimal Resetting Strategy for Maximizing Win Rate,"I thought of an interesting probability problem but I'm not sure how to approach it. My knowledge of statistics is not terrible, but this is complex enough that I'm not even sure how to start. Any insight would be appreciated (such as if this is similar to a well-known problem) or how to start on solving this. Say Bob really wants to prove to someone that he is a top player in Video Game. In order to do so, his gamer profile needs to have a win rate greater than or equal to X% with at least Y games played. That is to say, if W is the number of wins on Bob's profile and L is the number of losses, Bob wants W/(W+L) ≥ X% and W+L ≥ Y . For example, assume X% = 80% and Y=50 . At any point between games, Bob can choose to reset his profile, setting both W and L to 0 (Bob has no knowledge of the outcome of future games when choosing to do this). Say Bob's true chance of winning any given game is P% , and he knows this number. Bob wants to strategically reset his profile such that he minimizes the expected number of games he needs to play to attain his goal. What is Bob's optimal strategy for when to reset his profile? How many games will he be expected to play before achieving his goal, and how many times will he be expected to reset his profile? This problem is a bit more complicated than just finding the first occurrence of a sequence of 50 games with a win percentage higher than 80% because that sequence might have started out Win-Loss-Loss, for example, and Bob might have chosen to reset his profile after those two consecutive losses without knowing that he would achieve many wins in the next 47 games. I'm not even sure what the optimal resetting strategy is for Bob. Clearly you're supposed to reset if your profile is 0-1, since that's strictly worse than being 0-0. But if Bob's record is currently at 39-11, for example, is it better to continue onwards in hopes that his next five games will be wins (thereby pushing his record up to 80% win rate with only five additional games played), or reset and be guaranteed to play at least 50 more games? Does this problem even have a nice solution?","['expected-value', 'game-theory', 'probability']"
4598967,Not getting the right derivative of $(x-a)\arctan\frac{(y-b)(z-c)}{(x-a)\sqrt{(x-a)^2+(y-b)^2+(z-c)^2}}$ with respect to $c$,"Derivative of: $$\left(x-a\right)\arctan\left(\dfrac{\left(y-b\right)\left(z-c\right)}{\left(x-a\right)\sqrt{\left(x-a\right)^2+\left(y-b\right)^2+\left(z-c\right)^2}}\right)$$ w.r.t. $c$ is, according to online calculator : $$-\dfrac{\left(x-a\right)^2\left(y-b\right)}{\left(\left(c-z\right)^2+\left(x-a\right)^2\right)\sqrt{\left(x-a\right)^2+\left(y-b\right)^2+\left(z-c\right)^2}}$$ But I am getting: $$-  \dfrac{1}{\sqrt{\left(x-a\right)^2+\left(y-b\right)^2+\left(z-c\right)^2}}  \left[ 
   \dfrac{\left(x-a\right)^2\left(y-b\right)    \left[   \left(  x-a   \right)^2+\left(y-b\right)^2   \right]}{(x-a)^2 \left[ \left(  x-a   \right)^2+\left(y-b\right)^2+\left(c-z\right)^2   \right]+     (y-b)^2 (z-c)^2 }   \right]$$ Why is this so? Are the both expressions equivalent?","['partial-derivative', 'calculus', 'trigonometry', 'calculator']"
4598993,Prove that Kernel(T)=Kernel(T²) if and only if the intersection between Kernel and Im(T) =0,"I was working on this problem:
If you have a linear transformation $T: V \to V$ ( $V$ is a vectorial space) then:
a) $\ker(T)$ is included in $\ker\left(T^2 \right)$ And: b) $\ker(T) = \ker\left(T^2 \right) $ if and only if $\ker(T) \cap \text{im}(T) = \{ 0 \}$ . I proved a) saying that if $v \in \ker(T)$ then, $T(v) = 0$ . But then $T^2(v)= T(T(v))= T(0)$ which is equal to $0$ (because is a linear transformation), so we can say that $v \in \ker\left(T^2 \right)$ , and $\ker(T)$ is included in $\ker\left(T^2 \right)$ . I'm not sure if that's right.
But in part b) I'm a little confused, I was trying to prove it assuming that it's not true and then getting a contradiction, but I'm not sure about how to use: "" $\ker(T) \cap \text{im}(T) = \{0\}$ "".","['linear-algebra', 'linear-transformations']"
4599067,"If $\frac{\sin2x}{\cos x}-1=0$, find $x$ given $\ \frac{\pi}{2}\le x\le\pi$ (cancelling out vs multiplying out the $\cos x$)","To solve the equation $$\frac{\sin2x}{\cos x}-1=0 \tag1$$ I manipulated it into the following: $$\frac{2\sin x\cos x}{\cos x}-1=0 \tag2$$ Then, by multiplying $\cos x$ out, I obtained: $$2\sin x\cos x-\cos x=0 \tag3$$ However, according to the answers I was supposed to cancel out the $\cos x$ which results in $$2\sin x-1=0 \tag4$$ Thus, I myself attained the answers $\frac{5\pi}{6},\ \frac{\pi}{2}$ , whilst the answer booklet merely had $\frac{5\pi}{6}$ . I do not understand the logic behind cancelling out the $\cos x$ - I thought we were supposed to maximise the number of answers obtained by multiplying out rather than cancelling. Any help clearing up my confusion would be much appreciated.","['algebra-precalculus', 'trigonometry']"
4599108,How this exponential family is rewritten using dominating measure,"The exponential family is given by : $$p(x;\theta)=\exp\left\{C(x)+\theta^{i}F_{i}(x)-\psi(\theta)\right\}\hspace{3cm} (1)$$ the $n$ functions $F_{1}(x),...,F_{n}(x)$ are random variables. Let $x_i=F_{i}(x)$ . Suppose we define the probability density function on the $n$ -dimensional random variable $x=[x_i]$ with respect to the dominating measure $$d\mu(x)=\exp\{C(x)\}dx\hspace{3cm}(2)$$ Then we may rewrite equation $(1)$ as $$p(x;\theta)=\exp\{\theta^{i}x_{i}-\psi(\theta)\}\hspace{3cm} (3)$$ Can how we wrote $(3)$ ??","['measure-theory', 'probability-theory', 'statistics']"
4599112,Formation of the Binomial distribution in Cinlar: Probability and Stochastics,"I am wanting to understand a particular argument, set as an exercise in the mentioned book. So we have a sequence of Bernoulli's $X_1, X_2, ...$ all with the same success probability $p$ , failure probability $q$ and $S_n = \sum_i X_i$ The task is to compute $\mathbb{E} z^{S_n}$ and use that to determine $\mathbb{P}\{S_n=k\}$ Now there is a solution to this, which shows why (for any $z \in \mathbb{R}$ ) $\mathbb{E} z^{S_n} = \mathbb{E}z^{X_1} ... \mathbb{E}z^{X_n} = (q + pz)^n = \sum_{k=0}^n {n \choose k} p^k q^{n-k} z^k $ The argument that follows is that this implies that the distribution of $S_n$ will be a counting measure, placing mass ${n \choose k}p^kq^{n-k}$ at $k$ for each $k = 0, ..., n$ . How is this implied?","['measure-theory', 'binomial-distribution', 'probability', 'bernoulli-distribution']"
4599146,Is optimization of torus possible? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 1 year ago . Improve this question I'm a high school student needing to write an maths essay with 20 pages of length. I have an idea of doing an optimization problem with a torus. Either to minimise the surface area with a fixed volume, or to maximise the volume with a fixed surface area. But I'm not sure if such maximum or minimum exists since r can shrink infinitely. Could this idea work, or if not, is there any similar things about the Torus that I can do? Thank you!","['optimization', 'calculus', 'geometry']"
4599195,Can anyone explain implicit differentiation in an intuitive way?,I am in my final year of school. Understand the procedure and can do it well but do not understand why ? Can someone help ?,"['calculus', 'implicit-differentiation', 'ordinary-differential-equations']"
4599230,Intuition behind formula for matrix inverse in terms of partitions,"Suppose we have a square matrix $A$ that can be partitioned into $A=\pmatrix{A_{11}&A_{12}\\ A_{21}&A_{22}}$ . Then its inverse is given by the formula $$A^{-1}=\pmatrix{(A_{11} - A_{12}A_{22}A_{21})^{-1}&-A_{11}^{-1}A_{12}(A_{22} - A_{21}A_{11}A_{12})^{-1}\\ -A_{22}^{-1}A_{21}(A_{11} - A_{12}A_{22}A_{21})^{-1}&(A_{22} - A_{21}A_{11}A_{12})^{-1}}$$ Now, we can simply multiply $A^{-1}$ and $A$ to check that this indeed is the correct formula, given $A_{11}$ and $A_{22}$ are not singular. But is there any intuition (probably geometric) behind why this is so?: The formula does have a pattern in it!","['matrices', 'intuition', 'linear-algebra', 'inverse']"
4599278,Evaluating the limit $\lim_{x\to\infty}\left(\frac {x^5+\pi x^4+e}{x^5+ex^4+\pi}\right)^x$,"How can I evaluate the following limit? $$\lim_{x\to\infty}\left(\frac {x^5+\pi x^4+e}{x^5+ex^4+\pi}\right)^x$$ To solve this limit, I divided the numerator and denominator by $x^5$ and I got $$\lim_{x\to\infty}\left(\frac {1+\frac {1}{x}\pi+\frac {1}{x^5}e}{1+\frac {1}{x}e+\frac {1}{x^5}\pi}\right)^x$$ But I realized when $x\to\infty$ the expression is still indeterminate. I tried changing the $x=\frac {1}{y}$ variable to be able to apply Lophital's rule, but I couldn't get a useful result.","['limits', 'calculus']"
4599290,A problem from Mathematical Statistics and Data Analysis [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question This problem is from Ch. 5, Mathematical Statistics and Data Analysis, 3rd edition, stated as follows, In addition to limit theorems that deal with sums, there are limit theorems that deal with extreme values such as maxima or minima. Here is an example. Let $U_1, \ldots, U_n$ be independent uniform random variables on $[0,1]$ , and let $U_{(n)}$ be the maximum. Find the cdf of $U_{(n)}$ and a standardized $U_{(n)}$ , and show that the cdf of the standardized variable tends to a limiting value. Below is my derivation. The cdf of $U_{(n)}$ is as follows, $$
F_{(n)} = \left\{
    \begin{array}{lr}
        x^n, & \text{if } x \in [0,1]\\
        0, & \text{otherwise}
    \end{array}
\right\}
$$ and pdf $$
f_{(n)} = \left\{
    \begin{array}{lr}
        nx^{n-1}, & \text{if } x \in [0,1]\\
        0, & \text{otherwise}
    \end{array}
\right\}
$$ With pdf we can calculate mean and variance, which is $$
E(U_{(n)}) = \frac{n}{n+1}, \text{Var}(U_{(n)}) = \frac{n}{(n+1)^2(n+2)}
$$ The standardized variable $Z_n$ is $$
Z_{n} = \frac{U_{(n)}-\frac{n}{n+1}}{\sqrt{\frac{n}{(n+1)^2(n+2)}}}
$$ However, the key is quite different from mine. The standardized variable it gives is $Z_n = n(U_{(n)}-1)$ . Did I miss something? Many thanks!","['statistics', 'order-statistics', 'probability']"
4599310,"A description of line bundles on projective spaces, $\mathcal{O}_{\mathbb{P}^n}(m)$ defined using a character of $\mathbb{C}^*$.","I am trying to define line bundles on $\mathbb{P}^n$ as a triplet $E \xrightarrow{p} \mathbb{P}^n$ . It follows the idea. Let $m \in \mathbb{Z}$ a fixed integer. Take $E$ the quotient of $\mathbb{C}^{n+1} \times \mathbb{C}$ by the action of $\mathbb{C}^\times$ given by $$ t \cdot ((z_0,\dots,z_n),\lambda)=(t(z_0,\dots,z_n),t^m\lambda)$$ and let me denote it by $E=\mathbb{C}^{n+1} \times_{\mathbb{C}^\times} \mathbb{C}$ . We have a natural $p$ which is the projection onto the first factor: $$ E=\mathbb{C}^{n+1} \times_{\mathbb{C}^\times} \mathbb{C} \rightarrow \mathbb{P}^n$$ $$ (z,l) \mapsto [z] $$ I'm wondering if it is $\mathcal{O}(m)$ or $\mathcal{O}(-m)$ . Where $\mathcal{O}(m)$ is defined in terms of sheaves as the sheaf of ""holomorphic maps of degree $m$ "".","['differential-geometry', 'representation-theory', 'vector-bundles', 'algebraic-geometry', 'line-bundles']"
4599351,Axiom of choice required?,"I have been attempting to prove that there exists an injective function from $P(\mathbb{N})$ to $ \mathbb{R}$ . My idea has been to show that I can write any finite set in the powerset in order and just map it to the corresponding real number, however I struggle with doing so in the case of infinitely large subsets of $\mathbb{N}$ . Suppose I have such a subset $A$ given, would I only be able to sort it and map it using the Axiom of Choice? My intuition says yes but I don't know whether this is actually true. Example for finite set: $(1,2,49,688)$ would map to 1249688. To summarize: Would this proof work, why or why not? If it works, do I need choice, yes or no? If yes, why? EDIT: Has been easily solved, proof doesn't work. Didn't think of the cases in which different sets can actually map to the same element (see comments).","['elementary-set-theory', 'axiom-of-choice', 'functions']"
4599370,Prove that $RM$ is tangent to $\odot I$ in the given construction,"$\odot I$ , incircle of $\triangle ABC$ is tangent to $AB$ , $BC$ and $CA$ at $F$ , $D$ and $E$ respectively. $DI$ meets $AB$ at $P$ . Let $FQ\perp DF$ intersecting $DE$ at $Q$ . $EF$ intersects $PQ$ at $R$ . If $M$ is the midpoint of $BC$ , show that $RM$ is tangent to $\odot I$ . From $\angle DFQ=\angle IFA=\dfrac\pi2$ , $\angle DFI=\angle QFA$ . In addition, $\angle FQD=\dfrac\pi2-\angle FDQ=\dfrac12\angle A$ . Since $AE=AF$ , $E$ , $Q$ and $F$ are all on some circle centered at $A$ . So $AF=AQ$ . Therefore $\triangle DFI\sim\triangle QFA$ . Using this pair of similar triangles, $$\angle QAE=\angle FAQ-\angle A=\angle FID-\angle A=\pi-\angle A-\angle B=\angle C$$ so $AQ\parallel BC$ . This provides a better (perhaps) definition of $Q$ . $R$ is on $EF$ , the polar line of $A$ with respect to $\odot I$ . So $A$ should be on that of $R$ . From $\angle GFD=\dfrac\pi2$ , $FQ$ and $PD$ intersect $\odot I$ at the same point, which we'll call $G$ . Let $MN$ be tangent to $\odot I$ . Now we only need to prove the two below: $RG$ is tangent to $\odot I$ . $A$ , $G$ and $N$ are collinear.","['euclidean-geometry', 'circles', 'geometry', 'plane-geometry']"
4599382,Hartshorne IV.3.9 To prove a Lemma of Bertini regarding degree $d$ curves in $\mathbb{P}^3$.,"The problem is this: let $X$ be a degree $d$ curve in $\mathbb{P}^3$ . Show that the set of hyperplanes $H$ in $\mathbb{P}^3$ such that $X\cap H$ has $d$ distinct points and no three of which are coplanar form a Zariski dense subset of $(\mathbb{P}^{3})^{*}$ (the space of hyperplanes in $\mathbb{P}^3$ ).
Essentially the point is that a general hyperplane intersects a degree $d$ curve at $d$ points. I am not sure how to prove this properly. I only have heuristics and I can totally believe that this result should be true. One of my approaches is as follows. Fix an embedding $X\hookrightarrow\mathbb{P}^3$ given by a linear system $\mathfrak{d}$ . This linear system has dimension $3$ and the divisors in it are degree $d$ . I want to show that if I take a general hyperplane divisor $H$ in $\mathbb{P}^3$ , then pulling back gives an element of the linear system. If I do this, then I have shown that $X\cap H$ consists of $d$ points (counted with multiplicity). So essentially, I need to do three things (which I am stuck on): (1) I can count without multiplicity. (2) I can make sure that the $d$ distinct points are not coplanar (which follows from the linear system being $3$ -dimensional)? (3) These form a Zariski dense subset of $(\mathbb{P}^3)^*$ ? Can I just get hints for these three things (complete solutions would not be appreciated since I feel very close)?","['algebraic-curves', 'algebraic-geometry']"
4599398,"In $\triangle ABC$ with an internal point $D$, $\angle BCA=2\angle BAC$ and $AB=12$, find $CD$","As the title says, the question is to solve for the length $CD$ in the figure below. I found problem online in a Japanese forum, however it had no replies/solutions, so I've decided to share it here to see what kind of approaches are possible for this problem. Please do not mind the Japanese text in the image, all the relevant information can already be seen, here, $AB=12$ , $\angle BCA=2\angle BAC$ and $CD$ is the unknown. I'll share my approach as an answer below, please share your answers too!","['euclidean-geometry', 'geometry', 'triangles', 'plane-geometry', 'trigonometry']"
4599457,Sum of cubes of three positive integers in arithmetic progression in four ways?,"Consider the sums of cubes of three distinct positive integers in arithmetic progression, i.e. $a^3 + (a+b)^3 + (a+2b)^3$ where $a$ and $b$ are positive integers. $$5643 = 1^3 + 9^3 + 17^3 = 6^3 + 11^3 + 16^3$$ is the smallest positive integer that has two such representations. $$255816 = 8^3 + 34^3 + 60^3 = 18^3 + 38^3 + 58^3 = 43^3 + 44^3 + 45^3$$ is the smallest that has three such representations.
Is there a positive integer with four or more such representations, and if so what is the smallest? The sequence of positive integers with one or more representations is OEIS sequence A306213 .  Those with two or more is A359055 . If representations where the three cubes do not need to be positive are allowed, then I know of examples with up to $11$ representations.  Thus $$ \eqalign{42878854656 &= (-17232)^3 + 24^3 + 17280^3\cr
&= (-6900)^3 + 144^3 + 7188^3\cr
&= (-6704)^3 + 152^3 + 7008^3\cr
&= (-3132)^3 + 528^3 + 4188^3\cr
&= (-1812)^3 + 912^3 + 3636^3\cr
&= (-1032)^3 + 1224^3 + 3480^3\cr
&= (-292)^3 + 1552^3 + 3396^3\cr
&= (-24)^3 + 1672^3 + 3368^3\cr
&= 1038^3 + 2112^3 + 3186^3\cr
&= 1635^3 + 2304^3 + 2973^3\cr
&= 1728^3 + 2328^3 + 2928^3\cr
}$$","['number-theory', 'elementary-number-theory']"
4599489,Squeezing a convex shape between two squares,"You are given a convex shape $S$ in the plane. You are allowed to apply any affine transformation to $S$ . Then, you have to pick an axes-parallel square contained in (the transformed) $S$ , and an axes-parallel square containing $S$ , such that the ratio between the side-lengths of the squares is as small as possible. What is the smallest ratio that can be attained for every $S$ ? Examples: If $S$ is a rectangle, then it can be transformed into an axes-parallel square, so the ratio is $1$ . If $S$ is an ellipse, then it can be transformed into a circle, so the ratio is $\sqrt{2}$ . If $S$ is a triangle, then it can be transformed into an isosceles right-angled triangle, so the ratio is $2$ . Apparently, a triangle is the worst case, but I could not prove it. Is it always possible to attain a ratio of 2? What is the smallest ratio that can always be attained?","['convex-geometry', 'rectangles', 'geometry']"
4599523,Proving $\sum_{k=0}^{m}\binom{n+k}{k}(1-x)^kx^{n+1}=1-\sum_{k=0}^{n}\binom{m+k}{k}x^k(1-x)^{m+1}$ without calculus,"How would you prove this without calculus? $$\forall m,n\in\Bbb N,\ \forall x\in\Bbb R,\ \sum_{k=0}^{m}\binom{n+k}{k}(1-x)^kx^{n+1}=1-\sum_{k=0}^{n}\binom{m+k}{k}x^k(1-x)^{m+1}$$ In this post it is proved with calculus, and I was wondering if there was an easier way to prove it. I tried proving it with induction, by holding $n$ constant, however I didn't seem to get anywhere.","['combinatorics', 'combinatorial-proofs']"
4599532,"A geometry puzzle. $\triangle ACD$ and $\triangle BEC$ are two overlapping right-triangles, find the area of Quadrilateral $BODC$","Like the title suggests, the figure below is two overlapping right triangles. The overlapped area forms a quadrilateral and the goal is to find the area of that region given some lengths: Here, $AB=2, BC=6, CD=8$ and $DE=4$ . This problem is essentially the same problem posted by Professor Michael Penn on YouTube in a video that asks ""can you solve it in a different way?"" And that's essentially why I'm sharing it here. I want to see several different approaches to this problem. Michael Penn attempted it using a coordinate system and setting up an integral, I will share my geometric approach as an answer below. Please share your own approaches!","['triangles', 'trigonometry', 'geometry', 'analytic-geometry']"
4599550,Is this double integral improper?,"Consider the double integral of $f(x,y)=\frac{y^2}{x^2}$ over the triangle with vertices $(0,0)$ , $(2,3)$ , and $(5,3)$ , which is written as $$
\int_0^3\int_{\frac{2}{3}y}^{\frac{5}{3}y}\frac{y^2}{x^2}\,dx\,dy.
$$ This integral can be easily evaluated through conventional means, yielding $$
=\int_0^3\frac{-y^2}{x}\bigg|_{\frac{2}{3}y}^{\frac{5}{3}y}\,dy=\int_0^3\frac{9}{10}y\,dy=\frac{9}{20}y^2\bigg|_0^3=\frac{81}{20}.
$$ One can observe that $f$ is undefined at $(0,0)$ .  Furthermore, the discontinuity of $f$ at $(0,0)$ is nonremovable, as $$
\lim_{(x,y)\to(0,0)}f(x,y)\text{ does not exist.}
$$ And yet, the fact that $f$ is ""bad"" at $(0,0)$ never seems to to show up anywhere in our previous calculation.  (This brings to mind $\int_0^1\frac{1}{\sqrt{x}}\,dx$ , which is improper, but ""takes care of itself"" enough that a blind calculation never notices.)  I still have a few questions: Is this integration, in fact, improper? $f$ is bounded on the triangle described above, as the largest value it can acquire is on the boundary line $y=\frac{3}{2}x$ : $f(x,\frac{3}{2}x)=\frac{9}{4}$ . Does the nonexistence of the limit of $f$ (and $f$ itself, for that matter) at $(0,0)$ cause any problems for us?  Or can we simply say "" $(0,0)$ is a set of measure zero, so no problems""? Can we run into problems if our ""bad"" set of domain values is measure zero but dimension 1?  (Unlike our current example, which is measure zero and dimension 0.) In general, can we ignore any bad behavior on a measure zero set as long as our function is bounded?","['integration', 'multivariable-calculus']"
4599570,"Behavior for small and large times of $\ddot{y}(t)=y^2(t), y(0)=y_0>0, \dot{y}(0)=0$.","I am trying to understand the behavior of solutions for small and large times to the differential equation $$\ddot{y}(t)=3y(t)^2, y(0)=y_0>0, \dot{y}(0)=0.$$ For small times we have that $$\ddot{y}(t)=3y_0^2+6y_0(y-y_0)+O(y-y_0)^2$$ so we expect the solution to behave like a quadratic plus a positive  exponential. I'm not sure if the solution is defined for all times $t\ge 0$ or whether it blows up at some finite time $t$ . We can integrate the equation of motion to get $$\frac{1}{2}\dot{y}(t)^2=y(t)^3-y_0^3,$$ hence, assuming that $\dot{y}(t)\ge 0$ for all $t\ge 0$ , $$\int_{y_0}^{y(t)}\frac{dy}{\sqrt{y^3-y_0^3}}=\sqrt{2}t$$ though I'm not sure how to extract information out of this integral, which is not easily integrated. I thought about using the substitution $y(t)=y_0 e^{S(t)}$ , $S(0)=0$ to obtain $$\ddot{S}(t)+\dot{S}(t)^2=3y_0 e^{S(t)}$$ If we assume that the $\dot{S}^2$ dominates over $\ddot{S}$ , we end up with $$\dot{S}=\sqrt{3y_0}e^{S(t)},$$ i.e. $$1-e^{-S(t)}=\sqrt{3y_0}t. $$ Thus, $$S(t)=\log\left(\frac{1}{1-\sqrt{3y_0}t}\right)$$ giving us blow up at finite time, however $\ddot{S}\sim \dot{S}^2%$ , violating our assumption so this approximation doesn't work. I would appreciate any ideas.
If we have that $y_0>1$ , then $\ddot{y}(t)\ge 3y(t)$ and thus we should have $y(t)\ge y_0\cosh(\sqrt{3} t)$ and so we have super-exponential growth.","['asymptotics', 'ordinary-differential-equations']"
4599572,Prove that $\prod_{i=2}^n (1-1/i^2) = {n+1\over 2n}$,"Prove $$\prod_{i=2}^n (1-1/i^2) = {n+1\over 2n}$$ for all n greater or equal to 2. First of all, I'm well aware this exact question exists here. Unfortunately, this post didn't help me, and is much too old for me to resurrect. For reference, I'm a undergraduate college student, in an introductory proof writing course. Currently, I've made the following progress: We wish to show that $\prod_{i=2}^n (1-1/i^2) = {n+1\over 2n}$ for every integer n greater than or equal to 2. We will proceed via induction. For the base step, let n be equal to 2. Then, $1 – (1/2^2) = ¾$ , and $(2+1)/2(2) = ¾$ . Therefore, $\prod_{i=2}^n (1-1/i^2) = {n+1\over 2n}$ at n = 2. Next, assume that $\prod_{i=2}^k (1-1/i^2) = {k+1\over 2k}$ up until some integer k. We wish to show that this is true at k+1 as well. $\prod_{i=2}^k (1-1/i^2) = {k+1\over 2k}$ can be written as ${(k+1)-1\over k} * {(k+1)+1\over k}$ , or ${k\over k} * {k+2\over k}$ . k/k is 1, of course. but I'm not sure where to go from k+2/k. Or maybe I made some other mistake? I can't decipher user17762's comment under his answer to the linked question, nor the OP's edit after the ""so"". Any guidance is appreciated!","['solution-verification', 'discrete-mathematics']"
4599605,How to minimize $(x-1)^2+(y-1)^2+(z-1)^2$ under the constraint $xyz=s$?,"Let $s \in (0,1)$ be a parameter. Can we find an exact closed form expression for $$
F(s)=\min_{xyz=s,x,y,z>0}(x-1)^2+(y-1)^2+(z-1)^2, \tag{1}
$$ and exact formulas the minimizers $x(s) \le y(s) \le z(s)$ ? I tried using Mathematica but failed (However I am not very skillful). Using Lagrange's multipliers, one gets $$
(x-1,y-1,z-1)=\lambda (\frac{1}{x},\frac{1}{y},\frac{1}{z}),
$$ so $x,y,z$ should all satisfy the quadratic equation $t(1-t)=-\lambda$ . If $t$ is a solution then so is $1-t$ ; thus, if we denote the solution of this equation by $a$ , then $$
\{x,y,z\} \subseteq \{a,1-a \}.
$$ One possible solution is the symmetric solution $x=y=z=\sqrt[3] s$ . If $x,y,z$ do not all coincide, then they must take both values $a$ and $1-a$ . Since we required $x,y,z>0$ this means that $a,1-a$ should be positive, so $0<a<1$ . After a possible a renaming/switching, we may assume that $x=a, y=z=1-a$ , so the value $1-a$ is attained twice. So, if we define, $$
G(s)=\min_{a \in (0,1),a(1-a)^{2}=s} (1-a)^2+2a^2, \tag{2}
$$ then $$
F(s)=\min \{3(1-\sqrt[3]s)^2,G(s)\}.
$$ Since $\max_{a \in (0,1)}a(1-a)^{2}=4/27$ , it follows that the non-symmetric solution is possible only if $s \le 4/27$ . The term $3(1-\sqrt[3]s)^2$ comes from comparing with the symmetric solution $x=y=s$ . I am not sure how to continue, since analyzing explicitly the solutions to the quintic equation is not so nice . Motivation: This problem is a special case of the problem of finding the closest matrix to $\text{SO}_n$ with a given determinant.","['nonlinear-optimization', 'lagrange-multiplier', 'multivariable-calculus', 'optimization', 'symmetry']"
4599636,find the probability the black marble is from box A and the blue one is from C,"(HMMT 2000 Guts Round #43) Box A contains 3 black marbles and 4 blue marbles. Box B has 7 black marbles and 1 blue marble. Box C has 2 black marbles, 3 blue marbles, and 1 green marble. Person A closes their eyes and picks two marbles from 2 different boxes. If it turns out that A gets $1$ black and 1 blue marble, what is the probability that the black marble is from box A and the blue one is from C? My question is at the bottom. We assume the process is equivalent to uniformly choosing 2 distinct boxes and then independently and randomly choosing a marble from each box (!) Let the colours black, blue, and green be denoted by the numbers 1,2,3. A has 3 black and 4 blue marbles, B has 7 black and 1 blue, and C has 2 black, 3 blue, and 1 green. Consider the sample space $\Omega := \{\{(m_1,b_1),(m_2,b_2)\} : b_i \in \{A,B,C\}, m_i \in \{1,2,3\}, i = 1,2, b_1\neq b_2\}$ . Here, the $b_i$ 's in a pair refer to the boxes chosen. Let $X = \{\{(m_1,b_1),(m_2,b_2)\} \in \Omega : (m_1 = 1 \wedge m_2 = 2)\},Y = \{\{(m_1,b_1),(m_2,b_2)\} \in \Omega : (m_1 = 1 \wedge m_2 = 2 \wedge b_1 = A \wedge b_2 = C)  \}.$ For $R\neq S \in \{A,B,C\},$ let $P_{R,S} = \{\{(m_1,b_1),(m_2,b_2)\} \in \Omega : b_1 = R \wedge m_2 = S\}.$ We want to find $P(Y | X) = P(Y\cap X)/P(X)$ . To find $P(X),$ we first find $P(X\cap P_{A,B}) , P(X\cap P_{A,C}) , P(X\cap P_{B,C})$ . Now $P(X\cap P_{A,B}) = P(X | P_{A,B}) \cdot P(P_{A,B}). P(P_{A,B}) = P(P_{A,C}) = P(P_{B,C}) = 1/3,$ since every pair of two distinct boxes is selected with equal probability (by (!)). Now $P(X | P_{A,B}) = 3/7\cdot 1/8 + 4/7\cdot 7/8 = 31/56.$ Similarly, $P(X | P_{A,C}) = 3/7\cdot 3/6 + 4/7\cdot 2/6 = 17/42, P(X|P_{B,C}) = 7/8\cdot 3/6 + 1/8\cdot 2/6 = 23/48.$ Thus $P(X) = 1/3(31/56+17/42+23/48) = (483/1008).$ To find $P(Y\cap X),$ we find $P(Y\cap X \cap P_{A,B}), P(Y\cap X \cap P_{A,C}), P(Y\cap X \cap P_{B,C})$ . $P(Y\cap X \cap P_{A,B}) = P(Y\cap X \cap P_{B,C}) = P(\emptyset) = 0.$ Now $P(Y\cap X \cap P_{A,C}) = P(Y\cap X | P_{A,C}) P(P_{A,C}) = 1/3 \cdot (3/7\cdot 3/6) = 1/14.$ So the desired probability is $\dfrac{24}{161}$ . This is different from the solution on HMMT.org, which is $120/1147$ . Did I make a mistake somewhere, and if so, where and how can I get the correct answer?","['contest-math', 'conditional-probability', 'discrete-mathematics', 'probability']"
4599661,"find the largest number you can write with three 3's and three 8's, using only $+,-,/,\cdot$ and exponentiation","(HMMT 2000 Guts Round #38) Find the largest number you can write with three 3's and three 8's, using only $+,-,/,\cdot$ and exponentiation. Claim 1: if $x>y>e$ are real numbers, then $x^y < y^x$ . Assume that expressions without parentheses are evaluated using regular BEDMAS rules. I initially guessed that the answer is $3^{3^{3^{8^{8^8}}}},$ though I'm not sure how to prove this. From the problem conditions, there are 6 numbers and we can put 5 operations between them. The key to showing the desired result will be that adding in the other arithmetic operations doesn't result in a large enough number. This is fairly intuitive; if one had $a-b$ , where b is positive, one could replace that with $a+b$ to get a larger expression. To conclude, without parentheses, one clearly cannot use the subtraction operator. One also cannot use the division operator, since it can similarly be replaced by multiplication without decreasing the value of the expression. Finally, note that one can assume WLOG that addition is not used. Indeed, we've assumed that we can only have multiplication and division operations. since if a and b exceed 2 then $a+b < a\cdot b$ Finally, we may replace $a\cdot b$ with $a^b$ whenever a and b are integers exceeding 2 to get a larger expression. Hence we may indeed assume only the exponentiation operator is used. And the maximum value is the claimed value. Indeed, assume a $3$ , say b, follows an 8, say c, in the sequence of exponents. We may assume the 3 is preceded by an 8, since there must be an 8 followed by a 3 in the sequence of exponents between b and c. Then we claim that swapping the 3 and the 8 will result in a larger overall value for the expression. Indeed, by assumption, we have an expression of the form $8^{3^a}$ , where $a$ is a sequence of exponents containing $3$ . And the swap results in an expression of the form $3^{8^a}$ . Edit: It turns out that one can just use logarithms: $8^{3^a} < 3^{8^a}\Leftrightarrow \ln 8 \cdot 3^a < 8^a \cdot \ln 3.$ $\ln 8 / \ln 3 < (8/3)^a,$ which clearly holds for any positive integer $a$ .","['contest-math', 'inequality', 'elementary-number-theory', 'calculus', 'recreational-mathematics']"
4599663,Finding the coefficients of the OGF of unary-binary trees,"A unary-binary tree is a plane (unlabeled) tree where each vertex has $0$ , $1$ , or $2$ descendants. Find the OGF $M(z)$ for a unary-binary tree and show that its coefficients are $[z^n]M(z) = \frac{1}{n}\sum_{k=0}^n \binom{n}{k}\binom{n-k}{k-1}$ . Using the symbolic method I get from $$\mathcal{M} = \mathcal{E} + \mathcal{Z} \times CYC_{\le 2}(\mathcal{M} - \mathcal{E})$$ directly the functional equation $$ M(z) = 1 + z (1 + (M(z) - 1) + (M(z) - 1)^2)  = z(1+M(z)+M(z)^2).$$ Rewriting this as $M(z) - z(1+M(z)+M(z)^2)  = 0$ the quadratic solution formula leads to the ogf $$ M(z) = \frac{1 - z - \sqrt{1 - 2 z - 3 z^2}}{2 z}. $$ Since by the functional equation we have $$z = \frac{M(z)}{1+M(z)+M(z)^2}$$ we can use the Lagrange Inversion Formula to obtain \begin{align}
[z^n] M(z) = \frac{1}{n} [z^{n-1}] (1+z+z^2)^n
\end{align} However, I do not get who to proceed from here. Could you please give me a hint?","['power-series', 'trees', 'combinatorics', 'generating-functions']"
4599666,"If $A \times B \not = \emptyset$ then there is $a \in A \land b\in B$ so that $\{a,b\} \cap A\times B = \varnothing $ (Axiom of Regularity)","I have been stuck showing the following:(this is in the section in the book on the Axiom of regularity) If $A \times B \not = \emptyset$ then there is $a \in A \land b\in B$ so that $\{a,b\} \cap A\times B = \emptyset $ We see that there is $a\in A$ such that $\{a\} \cap A\times B = \emptyset$ Since if that does not hold then we see that $A\subseteq A\times B$ which would mean $A = \emptyset$ (already proven) which can't happen Since $A\times B \not=\emptyset$ Thus to finish the proof: we need to show that there is a $b \in B$ such that $\{b\} \cap A\times B = \emptyset$ but i can't seem to do this… is it true that if $B \subseteq A \times B$ then $B = \emptyset$ ?","['elementary-set-theory', 'set-theory']"
4599717,function decomposition given $f(g(x))$ and $g(f(x))$,"Problem Find functions $f(x)$ and $g(x)$ such that $f(g(x))=x^2-2x-4$ and $g(f(x))=x^2-6x+6$ . Finding some points Notice that $f(g(f(x)))=f^2(x)-2f(x)-4=f(x^2-6x+6)$ and the equation $x^2-6x+6=x$ has roots $1$ and $6$ . This gives $f^2(1)-2f(1)-4=f(1)$ and $f^2(6)-2f(6)-4=f(6)$ . The former gives $f(1)=4$ or $f(1)=-1$ while the latter gives $f(6)=4$ or $f(6)=-1$ . Note that $g(f(1))=1$ and $g(f(6))=6$ . We have [( $f(1)=4$ and $g(4)=1$ ) or ( $f(1)=-1$ and $g(-1)=1$ )] and [( $f(6)=4$ and $g(4)=6$ ) or ( $f(6)=-1$ and $g(-1)=6$ )]. Similarly, notice that $g(f(g(x)))=g^2(x)-6g(x)+6=g(x^2-2x-4)$ and the equation $x^2-2x-4=x$ has roots $-1$ and $4$ . This gives $g^2(-1)-6g(-1)+6=g(-1)$ and $g^2(4)-6g(4)+6=g(4)$ . The former gives $g(-1)=1$ or $g(-1)=6$ while the latter gives $g(4)=1$ or $g(4)=6$ . Note that $f(g(-1))=-1$ and $f(g(4))=4$ . We have [( $g(-1)=1$ and $f(1)=-1$ ) or ( $g(-1)=6$ and $f(6)=-1$ )] and [( $g(4)=1$ and $f(1)=4$ ) or ( $g(4)=6$ and $f(6)=4$ )]. Polynomial solutions Assume that $f(x)$ and $g(x)$ are polynomials. Since $f(g(x))$ and $g(f(x))$ are monic and of degree $2=1\cdot 2$ . One of $f(x)$ and $g(x)$ is monic and of degree $2$ and the other is monic and of degree $1$ . Case 1: $f(x)$ is of degree $1$ . Let $f(x)=x+k$ . Then $f^{-1}(x)=x-k$ . Then $g(x)=f^{-1}(f(g(x)))=x^2-2x-4-k$ and $g(x)=g(f(f^{-1}(x)))=(x-k)^2-6(x-k)+6$ . By compare the coefficient of the $x$ term, $k=-2$ . Hence, $f(x)=x-2$ and $g(x)=x^2-2x-2$ . Case 2: $g(x)$ is of degree $1$ . Let $g(x)=x+k$ . Then $g^{-1}(x)=x-k$ . Then $f(x)=g^{-1}(g(f(x)))=x^2-6x+6-k$ and $f(x)=f(g(g^{-1}(x)))=(x-k)^2-2(x-k)-4$ . By compare the coefficient of the $x$ term, $k=2$ . Hence, $f(x)=x^2-6x+4$ and $g(x)=x+2$ . Question Notice that the solution in Case 1 matches the point ( $f(1)=-1$ and $g(-1)=1$ ) and the solution in Case 2 matches the points ( $f(6)=4$ and $g(4)=6$ ). My question is what solution (should be non-polynomial) of $f(x)$ and $g(x)$ matches the remaining points, namely ( $f(1)=4$ and $g(4)=1$ ) and ( $f(6)=-1$ and $g(-1)=6$ ).",['functions']
4599727,Product measurability of the Radon-Nikodym derivative of two Marcov kernels,"Let $(\mathcal{X},\mathcal{A})$ and $(\mathcal{Y},\mathcal{B})$ are Borel spaces. Let $\kappa_i: \mathcal{B} \times \mathcal{X} \to [0,1], ~i=1,2$ be Marcov kernels from $(\mathcal{X},\mathcal{A})$ to $(\mathcal{Y},\mathcal{B})$ ,
i.e., for each $B \in \mathcal{B}$ , $x \mapsto \kappa_i(B, x)$ is a $\mathcal{A}$ -measurable function,
and for each $x \in \mathcal{X}$ , $B \mapsto \kappa_i(B, x)$ is a probability measure on $(\mathcal{Y}, \mathcal{B})$ . Suppose that for any $x \in \mathcal{X}$ , $\kappa_1(\cdot, x) \ll \kappa_2(\cdot, x)$ and
let $f(x,y)$ denote the Radon-Nikodym derivative $\frac{d\kappa_1(\cdot, x)}{d\kappa_2(\cdot, x)}(y)$ . My question : is it true that $(x,y) \mapsto f(x,y)$ is $\mathcal{A} \otimes \mathcal{B}$ -measurable? I am not familiar with the Marcov kernel, but this question is motivated by that I want to resolve my previous question . Any comments are welcome. Thank you very much.","['conditional-probability', 'measure-theory', 'probability-theory', 'radon-nikodym']"
4599757,"Proving $\frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+d}+\frac{d}{d+a}=2$ If $a,b,c,d \in \mathbb{N}$","Given pairwise distinct $a,b,c,d \in \mathbb{N}$ , prove that $$E=2$$ if $E=\frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+d}+\frac{d}{d+a}$ is an integer. My effort:
We have: $$\begin{aligned}
	& E=\frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+d}+\frac{d}{d+a} \\
	& F=\frac{b}{a+b}+\frac{c}{b+c}+\frac{d}{c+d}+\frac{a}{a+d}
\end{aligned}$$ WLOG let $a>b>c>d$ , so $$\begin{aligned}
	&  \frac{b}{a}<1 \\
	& \Rightarrow \quad \frac{1}{1+\frac{b}{a}}>\frac{1}{2} \Rightarrow \frac{a}{a+b}>\frac{1}{2}
\end{aligned}$$ Like-wise: $$\frac{b}{b+c}>\frac{1}{2},\:\frac{c}{c+d}>\frac{1}{2} \Rightarrow E>1.5 \Rightarrow F<2.5 ----(1)$$ Which gives two possibilities $E=F=2$ or $E=3,F=1$ .
How to rule out the second possibility?","['contest-math', 'number-theory', 'elementary-number-theory', 'natural-numbers', 'inequality']"
4599888,To prove an identity related to Gamma function,"Question: How to prove the following identity for all positive integers $k$ and $n$ : \begin{align}\tag{1}
(k+1)(2k+1) \cdots (nk+1) = \sum_{i=1}^n & \binom{n}{i} \left( \frac{i}{n}(k+2)-1 \right) \left[ (k+1)(2k+1) \cdots ((i-1)k+1) \right] \\
&\ \times \left[ (k+1)(2k+1) \cdots ((n-i)k+1) \right].
\end{align} I could check by enumeration in $n$ : \begin{align}
n&=1, & k+1 &= \binom{1}{1} (k+2-1), \\
n&=2, & (k+1)(2k+1) &= \binom{2}{1} \left( \frac{1}{2}(k+2)-1 \right) (k+1) + \binom{2}{2} \left( (k+2)-1 \right) (k+1) \\
& & & = k(k+1) + (k+1)^2, \\
n&=3, & (k+1)(2k+1)(3k+1) &= \binom{3}{1} \left( \frac{1}{3}(k+2)-1 \right) (k+1)(2k+1) \\
& & &\quad + \binom{3}{2} \left( \frac{2}{3}(k+2)-1 \right) (k+1)^2 \\
& & &\quad + \binom{3}{3} \left( (k+2)-1 \right) (k+1)(2k+1) \\
& & & = (k-1)(k+1)(2k+1) + (2k+1)(k+1)^2 + (k+1)^2(2k+1), \\
& \cdots & &
\end{align} But I failed to prove it by induction in $n$ :
Set \begin{equation}
I(n;k) := \sum_{i=1}^n \binom{n}{i} \left( \frac{i}{n}(k+2)-1 \right) \left[ (k+1) \cdots ((i-1)k+1) \right] \left[ (k+1) \cdots ((n-i)k+1) \right].
\end{equation} Then \begin{equation}
\begin{split}
I(n+1;k) - I(n;k) =&\ (k+1) \left[ (k+1) \cdots (nk+1) \right] \\
&\ + \sum_{i=1}^n \binom{n}{i} \left[ (k+1) \cdots ((i-1)k+1) \right] \left[ (k+1) \cdots ((n-i)k+1) \right] \\
&\ \qquad\quad \times \underbrace{\left[ \frac{n+1}{n+1-i} \left( \frac{i}{n+1}(k+2)-1 \right) ((n+1-i)k+1) - \left( \frac{i}{n}(k+2)-1 \right) \right]}_{=:J(n;k,i)}
\end{split}
\end{equation} If the followin equation holds, \begin{equation}\tag{2}
J(n;k,i) = \left( \frac{i}{n}(k+2)-1 \right)(nk-1),
\end{equation} then using the inductive hypothesis $I(n;k) = (k+1) \cdots (nk+1)$ , we have \begin{equation}
\begin{split}
I(n+1;k) - I(n;k) =&\ (k+1) \left[ (k+1) \cdots (nk+1) \right] + (nk-1) I(n;k) = (n+1)k I(n;k)
\end{split}
\end{equation} which yields $I(n+1;k) = ((n+1)k+1) I(n;k) = (k+1) \cdots (nk+1) ((n+1)k+1)$ as desired. But, unfortunately, equation $(2)$ does not hold... I was also thinking that the question may be related to Gamma function , because $$
(k+1)(2k+1) \cdots (nk+1) = k^n \frac{\Gamma(n+1+\frac{1}{k})}{\Gamma(1+\frac{1}{k})}
$$ so that identity $(1)$ turns to \begin{equation}\tag{1'}
k \Gamma\left(n+1+\frac{1}{k}\right) \Gamma\left(1+\frac{1}{k}\right) = \sum_{i=1}^n \binom{n}{i} \left( \frac{i}{n}(k+2)-1 \right) \Gamma\left(i+\frac{1}{k}\right) \Gamma\left(n-i+1+\frac{1}{k}\right).
\end{equation} But still, I have no clue to prove $(1')$ . Could anyone help on it? Any hint or comment will be appreciated. TIA...","['combinatorial-proofs', 'analysis', 'gamma-function', 'combinatorics', 'induction']"
4599898,"$A,B$ are real matrices, similiar in complex field, then they are similar in the real field.","$A,B$ are real matrices, similiar in complex field, then they are similar in the real field. This is well-known. My problem is that if we know there exists an invertible complex matrix $C$ such that $C^{-1}AC=B$ , can we use $A,B,C$ to find a real matrix $D$ such that $D^{-1}AD=B$ . Let $C=C_1+C_2i$ , where $C_1$ and $C_2$ are real matrices, then $AC_1=C_1B$ and $AC_2=C_2B$ . What next?","['matrices', 'linear-algebra']"
4600008,Connection between exponents of a root system and solutions to linear systems over finite fields,"Let $h_1, \ldots, h_r$ be linear forms in variables $x_1, \ldots, x_n$ with integer coefficients. Let $\mathbb F_q$ denote the finite field with $q = p^e$ elements. I am asked to prove that except in a finite number of characteristics $p$ , the number of vectors $v \in \mathbb F_q^n$ such that $h_i(v) = 0$ for all $i$ is given for all $q$ by a polynomial $\chi(q)$ in $q$ with integer coefficients, and that $(-1)^n \chi(-1)$ is equal to the number of connected regions into which $\mathbb R^n$ is separated by the removal of all the hyperplanes $h_i = 0$ . My confusion is with this part (see next paragraph for more context; see below for question). This is an exercise from a set of notes on Lie algebras. The upshot of this result is meant to be the following: if we let the $h_i$ be the root hyperplanes of a finite root system, then $\chi(q)$ has integer roots $e_1, \ldots, e_n$ ; call these the exponents of the root system. The hyperplanes $h_i$ divide $\mathbb R^n$ into Weyl chambers, so in fact the order of the Weyl group is $$(-1)^n \chi(-1) = (-1)^n \prod_{i=1}^n (-1-e_i) = \prod_{i=1}^n (1+e_i)$$ My question is the following: is it not the case that a linear system over $\mathbb F_q$ will always have $q^k$ elements where $k$ is the dimension of the null space of coefficient matrix of the $h_i$ ? For all but finitely many characteristics, we can perform row reduction over the rationals and all of the values by which we multiply will be elements of $\mathbb F_q$ (e.g. if we multiply by, say, $\frac3{10}$ , $\frac{5}{22}$ , and $\frac{2}{15}$ in the row reduction process, then any $p$ larger than $11$ will do). So the rank, and hence the nullity, is some fixed constant for all but finitely many characteristics. This would imply that $\chi(q) = q^k$ , but then $(-1)^n \chi(-1) \in \{\pm 1\}$ , which (except possibly the $h_i$ are all zero) is not the number of connected regions of $\mathbb R^n \setminus \{h_i\}$ . Am I missing something here? I haven't been able to find any reference on the exponents of a root system as defined above (I've found plenty on the exponents of a Weyl/Coxeter group which satisfy the product relation above); perhaps there is some modification to the definition of exponents that makes this work out. Note : this is homework; just a mild push in the right direction would be ideal.","['lie-algebras', 'weyl-group', 'finite-fields', 'root-systems', 'linear-algebra']"
4600009,Approximation of harmonious numbers,"I have a question about an approximation of so-called ""harmonious numbers"". These are a generalisation of the golden ratio, the plastic number, and so on, (related to the Fibonacci and Padovan sequences respectively). Their decimal expansions to $5\ sf$ for illustration: \begin{array}{|c|c|c|c|}
\hline
n& 2 & 3 & 4 & 5 & 6 & 7 & 8\\ \hline
χ_n & 1.6180 & 1.3247 & 1.2207 & 1.1673 & 1.1347 & 1.1128 & 1.0970 \\ \hline
\end{array} Studied by Dutch architect Dom Hans van der Laan in the 1920s, they are basically the positive real solutions to the equations \begin{align}
&x^n = x+1 \quad \quad \ (n\geq 2, \ n\in\mathbb{Z})\\
\end{align} They appear to be quite nicely approximated by \begin{align}
&\frac{2 n - 1 + \log 2}{2 n - 1 - \log 2}\\
\end{align} I was just wondering whether anyone could shed some light on why this might be the case? ref : Plastic number: construction and applications L Marohnić, T Strmečki","['number-theory', 'roots', 'approximation', 'asymptotics']"
4600033,How to solve COVID test validity using the Bayes theorem?,"The following information is given: The COVID tests are $70\%$ sensitive, i.e., ${\Bbb P} ( \text{Positive} \mid \text{COVID} ) = 0.7$ and $98\%$ specific, i.e. Pr(negative|no Covid)=0.98. We need to solve the probability of having covid when the test result is covid given that a)P(Covid)=0.5 b) P(Covid)=0.05. Can someone help me with making the joint probability distribution table and explain how they got it? The work I've done so far is:
P(Covid|Positive) = P(Covid) x P(Positive|Covid) / P(Positive)
P(Covid|Positive) = 0.5 x (0.7) / (0.7 + 0.02) I know the marginal probability is wrong but I don't think I understood the question fully. So having a table would help me understand it better. Thanks in advance.","['bayes-theorem', 'bayesian', 'probability']"
4600036,Hartshorne Chapter 2 Exercise 5.6. d),"In this exercise 5.6. d) in chapter 2 from ""Algebraic Geometry"" by Hartshorne one has to show that for any ideal $\mathfrak a \subset A$ of a noetherian ring $A$ and $A$ -module $M$ the sheaf $\Gamma_{\mathfrak a}(M)^{\sim}$ , we get from the submodule $\Gamma_{\mathfrak a}(M) = \{ m\in M|\mathfrak a^nm = 0 $ for some $n\} $ , is isomorphic to $\mathscr H_Z^0(\mathscr F)$ , where $\mathscr F = M^{\sim}$ and $Z = V(\mathfrak a)$ and $\mathscr H_Z^0(\mathscr F)$ , defined in exercise 1.20. as the sheaf we get by only taking sections of $\mathscr F$ with support in $Z$ , from this exercise we know that this is not only a presheave but a sheave. Now to my question. I think I have done the proof right: As one can gather from the hint, one shows that $\mathscr H_Z^0(\mathscr F)$ is quasi-coherent, as it is a closed subsheave of $\mathscr F$ , so $\mathscr H_Z^0(\mathscr F)(U)$ is a sub-module of $\mathscr F(U)$ , and then because the sections of both $\mathscr H_Z^0(\mathscr F)$ and $\Gamma_{\mathfrak a}(M)^{\sim}$ are zero on $U = X - Z$ ( $X=Spec(A)$ ) and we can show that $\Gamma_{\mathfrak a}(M)\cong \Gamma_Z(\mathscr F)$ , the two sheaves have to be isomorphic. But it somehow eludes me why it is necessary to look at powers of $\mathfrak a$ in the definition of $\Gamma_{\mathfrak a}(M)$ . Why is that necessary? Intuitively it makes a bit of sense that else we maybe wouldn't get enough elements, but I can't really explain why or even if that is actually the case. I would also appreciate an example why we need those powers and an explanation where it is needed in the proof and where i went wrong as i didn't use it.","['quasicoherent-sheaves', 'algebraic-geometry', 'modules']"
4600085,Is this inequality a consequence of strong convexity?,"Let $V:\mathbb R^2\to \mathbb R$ be a strongly convex , smooth function. By definition there exists $\alpha>0$ such that $$x^T \textsf{Hess} V(\xi) x \geq \alpha\,|x|^2$$ for all $x,\xi\in\mathbb R^2$ . $V$ has a unique minimum point: assume it is at $x=0$ . As a consequence it is easy to check that $$ x\cdot \nabla V(x) \geq \alpha\,|x|^2$$ for all $x\in\mathbb R^2$ . Just Taylor expand $\nabla V(x)= \textsf{Hess} V(\xi)\,x\,$ for some $\xi\in(0,x)$ . My question : assume also that derivatives of $V$ of any order have polynomial growth. Are there $\tilde\alpha>0$ and a non-negative polynomial function $p$ such that $$ x_1\cdot \partial_1 V(x_1,x_2) \geq \tilde\alpha\,|x_1|^2 - p(|x_2|)$$ for all $x=(x_1,x_2)\in\mathbb R^2$ ? This seems to me a sort of convexity condition restricted to the first variable only, keeping the second one fixed.
If $V$ is a polynomial , the desired inequality is true.
What about the general case? I don't manage to prove it nor to find a counter-example.","['multivariable-calculus', 'convex-analysis', 'real-analysis']"
4600088,"Given $\{A_n\}$ with $P(A_n)\to 0$ show that for every $\alpha>0$ there exists a subsequence $\{A_{n_k}\}$, such that $P(\cup_k A_{n_k})\leq\alpha$","Given $\{A_n\}$ with $P(A_n)\to 0$ show that for every $\alpha>0$ there exists a subsequence $\{A_{n_k}\}$ , such that $$P(\cup_k A_{n_k})\leq\alpha$$ I found this variation of the problem I am trying to solve. I figure the solution to my problem is similar, but I don't exactly see through it. Because $P(A_n)\to 0$ then I figure there exists a subsequence $A_{n_k}$ such that $\sum_{k=1}^\infty P(A_{n_k})< \infty$ and from Borel-Cantelli I have $P(\lim\sup A_{n_k})=0$ . Moreover I get that $\lim_{k\to\infty} P(\cup_k A_{n_k})=0$ from the properties of limit superior. Does this reasoning help to get to the needed result? I am grateful for any suggestions.","['borel-cantelli-lemmas', 'limsup-and-liminf', 'probability-theory', 'real-analysis']"
4600098,Deterministic Simpson's Paradox Antidote,"Suppose I have eight positive numbers $a_i,b_i,c_i,d_i$ for $i=1,2$ satisfying $$\frac{a_i}{b_i}\le \frac{c_i}{d_i}\hspace{1in}(1)$$ I'm looking for additional conditions that will ensure \begin{align}\frac{a_1+a_2}{b_1+b_2}\le\frac{c_1+c_2}{d_1+d_2} \hspace{1in}(*)\end{align} I found something nice but perhaps not tight enough. I'm hoping to use any slack in equations (1) to tighten (2) and (3) below. Claim : For (*) to hold, it is sufficient that $$\frac{a_1}{b_1}\le\frac{a_2}{b_2}\hspace{1in}(2)$$ and $$\frac{d_1}{b_1}\le\frac{d_2}{b_2}\hspace{1in}(3)$$ Pf : Start by subtracting the two inequalities in (1), $$\frac{a_2}{b_2}-\frac{c_1}{d_1} \le \frac{c_2}{d_2}-\frac{a_1}{b_1}$$ Since the right side is positive (due to combining (1) and (2)), we may multiply by $d_1b_2\le b_1d_2$ (assumed true), \begin{align*}
a_2d_1-c_1b_2\le c_2b_1-a_1d_2\\
\end{align*} Add in $a_1d_1\le c_1b_1$ and $a_2d_2\le c_2b_2$ then factor, \begin{align*}
a_1d_1+a_2d_2+d_1a_2+a_1d_2\le& b_1c_2+c_1b_2+c_1b_1+c_2b_2\\
(a_1+a_2)(d_1+d_2) \le & (b_1+b_2)(c_1+c_2)\\
\frac{a_1+a_2}{b_1+b_2}\le &\frac{c_1+c_2}{d_1+d_2}
\end{align*} Note, I did not require $a_i,c_i>0$ .","['statistics', 'conditional-probability', 'paradoxes', 'inequality', 'probability']"
4600110,Calculus of variations and integrating $\left(\frac{dy}{dx}\right)^2=\alpha^2(1+x)$,"I'm trying to wrap my head around a step in the solution of an exercise that has me stumped. The exercise is the following : Given the functional $$S[y]=\int_0^{1}dx\sqrt{1+x+y'^2}, \quad y(0)=x_0, \quad y(1)=x_1$$ it is asked to show that $y(x)$ defined by $$y'(x)=k\sqrt{1+x+y'(x)^2},$$ where $k$ is a constant, makes the functional stationary. Then, by expressing $y'(x)$ in terms of $x$ , one should show that the solution is $$y(x)=x_0+\frac{x_1-x_0}{2^{3/2}-1}\left((1+x)^{3/2}-1\right).$$ It has been shown previously in the course that given the functional $S[y]=\int_a^{b}dxF(x,y'),\quad y(a)=A, \quad y(b)=B,$ we obtain the following differential equation for the stationary path $y$ of $S$ : $$\frac{\partial}{\partial y'}F(x,y')=k, \quad y(a)=A, \quad y(b)=B,$$ where $k$ is a constant. Now, the solution of the initial problem goes as follows: We have $F(x,v)=\sqrt{1+x+v^2}$ and the general equation (above) becomes $$v=k\sqrt{1+x+v^2}, \quad \text{where}\quad v=y'(x).$$ Rearranging and squaring, we obtain $$\left(\frac{dy}{dx}\right)^2=\alpha^2(1+x), \quad \alpha^2=\frac{k^2}{1-k^2}.$$ Integrating gives the solution $$y(x)-x_0=\alpha\int_0^xdx\sqrt{1+x}=\frac{2\alpha}{3}\left((1+x)^{3/2}-1\right).$$ It then goes on to find the value of $\alpha$ with the boundary conditions. My questioning mostly pertains to the integration part. I feel like it is very vague, and I'm not sure how to go about such an integration and come out with the desired result. What about the integration limits $0$ and $x$ ? Secondarily, I also wonder about the need to introduce the variable $v$ . Any help would be tremendously appreciated.","['integration', 'ordinary-differential-equations', 'calculus-of-variations']"
4600124,"Show $\lim\limits_{n \to \infty } {a_n} = \frac{1}{\pi }{\operatorname{B}}\left( {\frac{1}{2},\frac{3}{4}} \right)$","Let $a_1= 0,a_2=1$ and $a_n = \sqrt {\frac{{{a_{n - 1}} + {a_{n - 2}}}}{2} \cdot {a_{n - 1}}}$ for $n \geqslant 3$ . Is it true that $\lim\limits_{n \to \infty } {a_n} = \frac{1}{\pi}{\operatorname{B}}\left( {\frac{1}{2},\frac{3}{4}} \right)$ , where $\operatorname{B}(x_1,x_2)$ is the Beta function? Having experimented with Maple and wxMaxima, I think this statement should be provable(?). For example, $a_{20}=.7627597635$ while $\frac{1}{\pi }{\rm B}\left( {\frac{1}{2},\frac{3}{4}} \right)=.7627597633...$ . I was also wondering whether this sequence is convergent, and I think I can show that. In fact, the subsequence $\langle a_{2n+1} \mid n\in\mathbb{N} \rangle$ is increasing while $\langle a_{2n} \mid n\in\mathbb{N} \rangle$ is decreasing and both subsequences are bounded, so they converge. Moreover, their limits are the same, so $\langle a_{n} \mid n\in\mathbb{N} \rangle$ converges, and even rough estimates show that the limit lies in $(0.758,0.766)$ . But to show that it is exactly $\frac{1}{\pi }{\rm B}\left( {\frac{1}{2},\frac{3}{4}} \right)$ is more difficult. Nonetheless, I will write down the proof of convergence. Perhaps, it may hint somehow to ${\operatorname{B}}\left( {\frac{1}{2},\frac{3}{4}} \right)$ expressed as an infinite sum or infinite product or whatever. Let's start with the observation that for all $n$ : $0\leq a_{n}\leq1$ . This holds since given two values in the interval $[0,1]$ , their average and product lie again in this interval. So in particular the sequence $\langle a_{n} \mid n\in\mathbb{N} \rangle$ is bounded. What’s more, the subsequence $\langle a_{2n+1} \mid n\in\mathbb{N} \rangle$ is increasing while $\langle a_{2n} \mid n\in\mathbb{N} \rangle$ is decreasing. We shall prove this by induction, making use of the well known inequality $\sqrt{x y}\leq{\frac{x+y}{2}}$ . Hence we get $$\tag{1}a_{n-2}^{\frac{1}{4}}\cdot a_{n-1}^{\frac{3}{4}}\leq a_{n}\leq\frac{a_{n-2}}{4}+\frac{3a_{n-1}}{4}$$ Now we can prove that for all $n$ we have $$\tag{2}a_{1}\lt \cdots\lt a_{2n-1}$$ $$\tag{3}a_{2}\gt \cdot\cdot\cdot\gt a_{2n}$$ $$\tag{4}a_{2n-1}\lt a_{2n}$$ For $n=1$ we see that (2) and (3) are vacuously true and (4) is obvious. And for the induction step we have to show $$\tag{5}a_{2n-1}\lt a_{2n+1}\lt a_{2n+2}\lt a_{2n}$$ We start by observing that $$\tag{6}a_{2n+1}\lt a_{2n}$$ since by definition $\displaystyle a_{2n+1}=\sqrt{\frac{a_{2n-1}+a_{2n}}{2}\cdot a_{2n}}$ and from (4) it follows that $\displaystyle{\frac{a_{2n-1}+a_{2n}}{2}}\,\lt \,a_{2n}$ . Now for the first inequality of (5), note that by the first inequality of (1) we have $a_{2n+1}^{4}\ge a_{2n-1}\cdot a_{2n}^{3}$ , and by (6) this is greater than $a_{2n-1}\cdot a_{2n+1}^{3}$ , so $a_{2n+1}\gt a_{2n-1}$ as required. Similarly, $a_{2n+2}^{4}\geq a_{2n}\cdot a_{2n+1}^{3}\gt a_{2n+1}^{4}$ , which proves the second inequality of (5). For the last inequality of (5) we use the second inequality of (1), so $\displaystyle a_{2n+2}\,\leq\,\frac{a_{2n}+3a_{3n+1}}{4\,a n+1}\,\lt \,a_{2n}$ (by (6)). That completes the induction proof. Now we can easily prove that both subsequences converge to the same limit. This is because by (1) $\displaystyle a_{2n+2}-a_{2n+1}\leq{\frac{a_{2n}-a_{2n+1}}{4}}$ and by (5) this last difference is less than $\displaystyle\frac{a_{2n}-a_{2n-1}}{4}$ . So the distance between $a_{2n+2}$ and $a_{2n+1}$ is less than a quarter of the distance between $a_{2n}$ and $a_{2n-1}$ , and therefore these distances go to zero. That implies both subsequences have the same limit. Regarding the value $l$ of the limit, if - inspired by (1) - we define the sequence $\langle b_{n}|n\in\mathbb{N}\rangle $ by $\displaystyle b_{n}={\frac{b_{n-2}}{4}}+{\frac{3b_{n-1}}{4}}$ and $b_1=0,b_2=1$ , then clearly $a_{n}\leq b_{n}$ . Also, it is not hard to prove that $\displaystyle b_{n}=\frac{4}{5}\left(1-\left(-{\frac{1}{4}}\right)^{n-1}\right)$ , which converges to $0.8$ . In the same way, if we define $c_{n}=c_{n-2}^{\frac{1}{4}}\cdot c_{n-1}^{\frac{3}{4}}$ , then we better ignore the initial value $0$ , so we just put $c_{2}=1,c_{3}=a_{3}={\sqrt{\frac{1}{2}}}$ and when we switch to $d_{n}:=\log_{2}c_{n}$ , then $d_{2}=0,d_{3}=-{\frac{1}{2}}$ and $d_n$ satisfies the same recurrence relation as $b_n$ . Therefore, for $n≥2$ , $\displaystyle d_{n}=\frac{2}{5}\left(-1+\left(-\frac{1}{4}\right)^{n-2}\right)$ , which converges to $-\frac{2}{5}$ , so $c_n$ converges to $2^{-{\frac{2}{5}}}$ , which is approximately $0.758$ . In fact, returning to $b_n$ , but using initial values $b_{2}=1,b_{3}=a_{3}={\sqrt{\frac{1}{2}}}$ , we can show $b_n$ converges to $\displaystyle\frac{1+2{\sqrt{2}}}{5}$ , which is approximately $0.766$ .","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
4600128,Calculating $\lim_{x\to0}\frac{e^{\cos x}-e}{x^2}$ without L'Hôpital,"Calculate $$\lim_{x\to0}\frac{e^{\cos x}-e}{x^2}$$ without L'Hôpital. I tried changing $e$ to $(1+x)^{1/x}$ , tried using $\frac{1-\cos x}{x^2} = \frac{1}{2}$ , tried converting $\cos x$ to $1-2\left(\sin \frac{x}{2}\right)^2$ but I always got stuck at some point, and would really appreciate help.","['limits', 'limits-without-lhopital']"
4600131,Find the value of $\int_0^1f(x)dx$,"If $$f(x)=\binom{n}{1}(x-1)^2-\binom{n}{2}(x-2)^2+\cdots+(-1)^{n-1}\binom{n}{n}(x-n)^2$$ Find the value of $$\int_0^1f(x)dx$$ I rewrote this into a compact form. $$\sum_{k=1}^n\binom{n}{k}(x-k)^2(-1)^{k-1}$$ Now, $$\int_0^1\sum_{k=1}^n\binom{n}{k}(x-k)^2(-1)^{k-1}dx$$ $$=\sum_{k=1}^n\binom{n}{k}\frac{(1-k)^3}{3}(-1)^{k-1}-\sum_{k=1}^n\binom{n}{k}\frac{(-k)^3}{3}(-1)^{k-1}$$ $$=\sum_{k=1}^n\binom{n}{k}\frac{(1-k)^3}{3}(-1)^{k-1}+\sum_{k=1}^n\binom{n}{k}\frac{k^3}{3}(-1)^{k-1}$$ After this, I took $\dfrac13$ common and did some simplifications but nothing useful came out. Any help is greatly appreciated.","['integration', 'calculus', 'binomial-coefficients']"
4600167,Is every vector field on real line complete?,"This question was left as an exercise in my course of Differential Geometry. I am not able to make much progress on it. Question: Is every vector field on the real line complete? Attempt:A vector field X along a curve $\sigma : [a,b] \to M$ is a mapping $X: [a,b]\to T(M)$ which lifts $\sigma$ ; ie $\pi \circ X=\sigma$ . A smooth vector field X on M is called complete if $D_t =m$ where is defined on Page 37 of Frank Warner's book as follows: But I am not able to think in the case of $D_t=\mathbb{R}$ , that under what conditions the equality should hold. So, can you please help me with this? Thanks!","['vector-fields', 'ordinary-differential-equations', 'differential-geometry']"
4600170,Joined squares with concyclic edges,"[EDITED after feedback. Thanks, YNK] Two squares are joined at a corner, I am trying to figure out what conditions are needed to ensure that the vertices of one edge from each square all lie on the same circle. In the diagram below, the edge vertex sets that I would like to be concyclic are $\{A,B, F, G\}$ or $\{A,B, G, H\}$ . Three ways to start with one of the squares and generate another that meets the condition are shown below. I guess my questions are: Are these the only solutions ? If so, how can we prove this? Solution 1 Draw any circle through $A$ , $B$ Take rays from $B$ through the $C$ and the centre of the circle, $O$ , label the points where they cross the circle again as $F$ and $G$ . $\angle CFG = \angle BFG = 90^{\circ}$ . I think it should be easy to show that $A$ , $C$ , $G$ are collinear using symmetry. This makes $\angle GCF = \angle CGF = 45^{\circ}$ , so that $\triangle GCF$ is isosceles and $CF=FG$ . So we can 'complete the square' to get what we want. In terms of transformations, this solution can be obtained from the initial square by a rotation about $C$ of $135^{\circ}$ and a scale (centre $C$ ) by a suitable factor to make $F$ coincide with the circle. Solution 2 Starting as before, with any circle through $A$ , $B$ , we can draw a line through the centre of the circle and $C$ . Reflecting the square in this line gives a congruent one with the desired property. Solution 3 We can obtain another solution by applying starting with solution 1 and applying the operation (reflection) used for solution 2. This leads to the square labelled '3' below. We could have started with solution 2 and applied the operations that were used for solution 1 (rotate and scale) as, I believe, these operations commute. There is a slight difference between solution 3 and solutions 1 and 2. Starting from the point of contact and reading the edges in anti-clockwise order, the second edge coincides with the circle for solutions 1 and 2, while the third edge coincides for solution 3. Update Some playing with Geogebra seems to confirm that there are only 4 squares with the property, once the joining point $C$ is fixed. In the illustration below, $C$ is chosen, point $H$ can move freely around the circumference of the circle, parameterised by $\alpha$ , the angle that the radius to $H$ makes with the positive $x$ -axis. Extending $HC$ to $E$ and using the diameter from $E$ leads to $F$ for which $\angle FHC = 90^{\circ}$ . Varying $\alpha$ between $0$ and $2 \pi$ , we can generate a square whenever $|HF|=|CH|$ . The right hand plot shows the graphs for $|HF|$ and $|CH|$ as $\alpha$ varies, this seems to confirm four solutions that can lead to a square.","['circles', 'geometry']"
4600258,Determine the number of odd binomial coefficients in the expansion of $(x+y)^{1000}$.,"Determine the number of odd binomial coefficients in the expansion of $(x+y)^{1000}$ . Hint: The number of odd coefficients in any finite binomial expansion is a power of $2$ . Is there a way to prove this without using something like Lucas's theorem or any other non-trivial result? It's a problem from a problem solving book and they haven't introduced any theorems in it. They have just given that it should be a power of $2$ , but nothing on how to go about finding it. Computing a few smaller terms shows that $(x+y)^0$ has $1$ odd coefficient, $(x+y)^1$ has $2$ , $(x+y)^2$ has $2$ , $(x+y)^3$ has $4$ , $(x+y)^4$ has 2 and $(x+y)^5$ has $4$ , but no pattern seems to emerge.","['contest-math', 'binomial-coefficients', 'combinatorics']"
4600336,Algebraic calculation with polynomial and complex root.,"Let $f=X^{3}-7 X+7$ be in $\mathbb{Q}[X]$ .
Let $\alpha \in \mathbb{C}$ be a root of $f$ and hence $1, \alpha, \alpha^{2}$ be a basis of the $\mathbb{Q}$ vector space $\mathbb{Q}(\alpha)$ . Let $\beta=3 \alpha^{2}+4 \alpha-14$ . Write $\beta^{2}$ and $\beta^{3}$ as linear combinations of $1, \alpha, \alpha^{2}$ over $\mathbb{Q}$ and conclude that $\beta$ is a zero of $f$ . Is $\beta=\alpha$ ? I found my mistake with your help. The solution: We have $$
\beta^2=(3\alpha^{2}+4 \alpha-14)^2=9 \alpha^4 + 24 \alpha^3 - 68 \alpha^2 - 112 \alpha + 196
$$ To remove the cubic and quartic powers, we use the fact that $\alpha^3-7\alpha+7=0$ holds and gradually eliminate partial polynomials.
We get $$
   \beta^2=9 \alpha^4- 68 \alpha^2 + 24 \alpha^3 + 16\cdot (-7 \alpha) + 28\cdot 7
=9 \alpha^4- 68 \alpha^2 -8\cdot (-7 \alpha) \
+ 4\cdot 7\alpha(9 \alpha^3+10\cdot(-7) \alpha +8\cdot 7+2\alpha)+4\cdot 7=\alpha(-5\alpha -7) + 4\cdot 7=-5\alpha^2-7\alpha+28.
$$ Now we try $\beta^3$ with the same method $$
\beta^3=\beta^2\beta=(3\alpha^{2}+4 \alpha-14)^2(3\alpha^{2}+4 \alpha-14)=(-5\alpha^2-7\alpha+28)(3\alpha^{2}+4 \alpha-14)\
=-15\alpha^4+ 126 \alpha^2 -(41 \alpha^3  +30\cdot(-7\alpha) + 56\cdot 7)=-15\alpha^4+ 126 \alpha^2 -(  -11\cdot(-7\alpha) + 15\cdot 7)\
=-\alpha(15\alpha^3+18\cdot(-7\alpha)  +7\cdot 11) - 15\cdot 7=-\alpha(3\cdot(-7\alpha)  -4\cdot7) - 15\cdot 7=21\alpha^2  +28\alpha-105.
$$ And with this i get indeed $$
f(\beta)=0.
$$ Thanks for your help! Maybe there are different reasonings for why $\beta\neq \alpha$ ?","['abstract-algebra', 'polynomials']"
4600345,How to geometrically determine number of holes for a triangle,"all. I am making a modern Christmas tree out of a flat wooden board. It's triangular in shape. It's real simple and modern. It measures 44"" from top to bottom and at the widest point it's also 44"". I have a strand of 50 LED bulbs that I want to geometrically place by drilling holes through the wood to poke them trough (so only the bulb is seen on the front surface. Is there some type of equation I can use to equally place fifty holes throughout my triangle? I want to start with a single hole near the top point and then gradually increase them with equal distance as I work my way down. Worse case scenario, if I have to have a couple less holes than 50 I could leave the remaining bulbs behind the triangle tree. No one will see what's behind it. But I cannot go over 50 since I only have 50 lights. Can I get some assistance, please?","['nonlinear-optimization', 'geometry']"
4600354,Does almost sure convergence implies that a sequence is cauchy with probability one?,"Well, if $P\left((X_{n}) \ \text{is cauchy}\right)=1$ then $X_n \longrightarrow X \ \text{a.s}$ . But is it true for converse? we have $$(X_n\longrightarrow X) := \bigcap\limits_{ε\in\mathbb{Q^+}}\bigcup\limits_{N=1}^{\infty}\bigcap\limits_{n=N}(|X_n-X|\leq\epsilon) $$ And $$(X_n \ \text{is cauchy}) := \bigcap\limits_{ε\in\mathbb{Q^+}}\bigcup\limits_{N=1}^\infty\bigcap\limits_{n=N}\bigcap\limits_{m=N}(|X_n-X_m|\leq\epsilon)$$ . we see that $(X_n \ \text{is cauchy}) \subset (X_n\longrightarrow X \ \text{a.s})$ so: $ P(X_n \ \text{is cauchy}) \leq P(X_n\longrightarrow X)$ and since $P\left((X_{n}) \ \text{is cauchy}\right)=1$ , hence $P(X_n\longrightarrow X)=1$ . But what about the converse, that if $(X_n)$ converges a.s then $((X_n) \ \text{is cauchy})$ with probability one? intuitively, if $(X_n)$ converges almost surely to a limit X, then if we consider $N\in\mathbb{N}$ too large, then $\forall\epsilon>0\ \exists \ N\in\mathbb{N} \ \forall n,m\geq N :(|X_n-X_m|\leq \epsilon)$ , which is cauchy, and it holds with probability one, right? Hope someone can calrify this. Thanks in advance.","['cauchy-sequences', 'convergence-divergence', 'probability-theory', 'real-analysis']"
4600355,Find an average number by using group action,"In a party there are $n$ individuals. Each of them gives a present in the party. At the end of the party the presents are distributed randomly to each participant. How many individuals receive in average their own presents? We want to solve the problem from the perspective of a particular group action $f: G \times M \rightarrow M$ and a fixed set, $M^g = \{ x \in M\mid g\cdot x = x \}$ . I do not know how to go from here. Do you have any suggestion or a solution proposal? Thanks.","['group-theory', 'group-actions', 'combinatorics']"
4600387,How to prove $\sum_{n=1}^{+\infty}\dfrac{1}{(n+1)\sqrt[e]{n}}$ converges to a number $<e$.,"I can prove that $\sum_{n=1}^{+\infty}\dfrac{1}{(n+1)\sqrt[e]{n}}$ converges, but don't know how to transform the $\sqrt[e]{n}$ item to prove it less than $e$ , could someone help/hint me?","['calculus', 'convergence-divergence', 'sequences-and-series']"
4600408,Integrals of the form $\int_0^1\left(\frac{1}{x+1}-\ln\left|\frac{x+1}{2}\right|-\ln\ln\left|\frac{x+1}{x-1}\right|\right)dx$,"How could we evaluate the following integrals? (Integrands pictures above) Blue: \begin{align}
\int_0^1 \left(\frac{1}{x+1}-\ln\left|\frac{x+1}{2}\right|-\ln\ln\left|\frac{x+1}{x-1}\right|\right)dx &= \gamma - \ln\pi + \ln 2 + 1 \\
\int_0^\infty \left(\frac{1}{x+1}-\ln\left|\frac{x+1}{2}\right|-\ln\ln\left|\frac{x+1}{x-1}\right|\right)dx &= -1 + \ln 2
\end{align} Red: $$\begin{align}
\int_0^1 \left(\frac{1}{x+1}+\ln\left|\frac{x-1}{2}\right|+\ln\ln\left|\frac{x+1}{x-1}\right|\right)dx &= -\gamma + \ln\pi - \ln 2 - 1 \\
\int_0^\infty \left(\frac{1}{x+1}+\ln\left|\frac{x-1}{2}\right|+\ln\ln\left|\frac{x+1}{x-1}\right|\right)dx &= -1 - \ln 2
\end{align}$$","['integration', 'euler-mascheroni-constant', 'definite-integrals', 'logarithms']"
4600425,Why Is The Fisher Information Important?,"I am struggling to understand the relationship between the Fisher Information and the Variance. So far, what I understand: Given a specific choice of Probability Distribution Function, the partial derivative of the Natural Logarithm of the corresponding Likelihood Function is called the Score Function If we square the Score Function and take its Expected Value - this is the Fisher Information (note: when there are multiple parameters, the Fisher Information will be a Matrix) Now, the important result from the above, is that apparently: The (Negative) Inverse of The Fisher Information is equal to Variance As an example, suppose you successfully evaluate the Fisher Information and have a Matrix containing the Fisher Information for all parameters (i.e. if the original Probability Distribution Function has ""p"" parameters, this will be a ""p x p"" Matrix) - if you can somehow manage to take the Inverse of this Matrix, the diagonal components of this matrix will contain the Variance Formulae for each of these parameters. This seems to be a very important fact which is likely very useful in calculating the variance estimates for any probability distribution - but I am not sure why this is true. I tried to consult different references online (e.g. videos, university lecture notes), but I could not come across a source which demonstrated why this result is true. Can someone please help me (i.e. walk me through the math) behind why the (Negative) Inverse of the Fisher Information is equal to the Variance? Is there a proof for this? Thanks!","['fisher-information', 'statistics', 'probability']"
4600456,Can A Probability Ever Be Outside of $0$ and $1$?,"Recently, I have been studying the Multinomial Probability Distribution Suppose you go to a casino and there is a game that involves rolling a six-sided die (i.e. one dice). However, you are not told what is the probability that this die lands on any one of these sides - this raises your suspicions and leads you to believe that perhaps the die might not be fair, therefore it might not be worth playing this game. You are still considering whether its worth playing this game - and suddenly find out that the casino has a large screen television that displays the last $100$ numbers that came from this die. Since you know that a die follows a Multinomial Distribution, you can use this fact to estimate the probabilities of the die assuming any given number, as well as the ""spread"" (i.e. variance) for each of these probabilities. Using the Maximum Likelihood Estimation , I have been trying to derive the formulae for the parameters of the Multinomial Probability Distribution. In short, given an event $i$ (e.g. the number $2$ on a die), the (very obvious) estimate for the probability $p_i$ of this event is $$\hat{p_{i}}_{\text{MLE}} = \frac{n_{i}}{N}$$ where the number of times that the event $n_{i}$ appears and $N$ is the total number of events that were recorded. As always, probabilities are only defined between $0$ and $1$ - therefore these individual estimates for $p_{i}$ can never be greater than $1$ or less than $0$ . Next, using the equivalence between the (inverse) Fisher Information and Variance, I was able to work out the formula for the ""variance of these probabilities"". In short, the variance of $p_{i}$ is given by $$\text{var}(\hat{p_{i}}_{\text{MLE}}) = \frac{p_{i}^{2}}{n_{i}}$$ Finally, using the theory of Asymptotic Normality of MLE , we can derive Confidence Intervals for the estimates of these parameter estimates (i.e. each individual value of $p_{i}$ ). That is, you might have observed that the probability of rolling a $2$ on this die is $0.31$ - but there is also a $95\%$ chance that the probability of rolling a $2$ might be anywhere between $(0.28, 0.33)$ . We can construct a $95\%$ Confidence Interval for any of these probabilities as: $$p_{i} \pm 1.96 \cdot \left( \sqrt{\frac{p_{i}^{2}}{n_{i}}} \right)$$ Question: I am worried that for certain values of $p_{i}$ and $n_{i}$ , this expression $$p_{i} \pm 1.96 \cdot \left( \sqrt{\frac{p_{i}^{2}}{n_{i}}} \right)$$ might be greater than $1$ or less than $0$ . As an example, if $p_{i} = 0.9$ and $n_{i} = 16$ , this results in a range estimate for the probability exceeding $1$ , i.e. $$0.9 + 1.96 \cdot \sqrt{\frac{0.9^2}{16}}$$ Have I done this correctly? Is it really possible for a probability value to be outside a range of $(0,1)$ ? Thanks! Note: I obviously think I have done something wrong, because I don't know much in math - but out of the few things I know, probabilities will never be outside the range of $[0,1]$ .","['statistics', 'confidence-interval', 'estimation', 'probability-theory', 'probability']"
4600460,Can we fill the table such that the number in each field is equal to the sum of the numbers in all the fields with adjacent edges?,"Can you fill $n ^ 2$ real numbers in the table (grid) of size $n \times n$ , which are not all zero, and the number in each field is equal to the sum of the numbers in all the fields with adjacent edges? What conditions does $n$ have to meet, so that there is such a scheme? If there is a corresponding scheme, how many free variables will be among these $n^2$ numbers? For example $$
\begin{array}{|c|c|c|c|}
\hline 2 & 1 & 1 & 2 \\
\hline 1 & -2 & -2 & 1 \\
\hline 1 & -2 & -2 & 1 \\
\hline 2 & 1 & 1 & 2 \\
\hline \end{array}$$ or $$\begin{array}{|c|c|c|c|}
\hline 0 & 1 & 1 & 0 \\
\hline -1 & 0 & 0 & -1 \\
\hline -1 & 0 & 0 & -1 \\
\hline 0 & 1 & 1 & 0 \\
\hline \end{array}$$","['puzzle', 'combinatorics']"
4600506,Group action where all stabilizers are equal,"Let $\cdot:G\times X \rightarrow X$ be a left group action. We know that: $(i)$ The action is faithful , if $g\cdot x=x, \forall x\in X$ implies $g=e$ ; $(ii)$ The action is free , if $g\cdot x=x$ for some $x\in X$ implies $g=e$ ; $(iii)$ Every free action is faithful. Thinking about the converse of $(iii)$ , I realized that a faithful action, in order to be free, needs to have this property $(P)$ : all stabilizer subgroups are equal (to the kernel of the action, of course). Thus, we have the equivalence: $\text{free} \Leftrightarrow \text{faithful}+(P)$ . Since I haven't found in literature any info about such actions with property $(P)$ , I decided to call them quasi-free . I am willing to find out any other properties of quasi-free actions. I have posted two answers, sharing my thoughts on this topic.","['group-theory', 'group-actions']"
4600510,Does this nonlinear equation give eigenvectors for matrix $A$?,"Let $A$ be a matrix with nonnegative entries and $y$ be a vector satisfying $$\sum_{k = 1}^\infty y^k =  Ay $$ where $[y^k]_i = y_i^k$ is the pointwise exponential and $0 < y_i < 1$ . Is $y$ an eigenvector of $A$ ? Then proof shown in the text I am reading now argues as follows. Let $y = \epsilon x $ where $\epsilon > 0$ is arbitrarily small. Dividing the above equation by $\epsilon$ gives $$Ax = x + \epsilon x^2 + O(\epsilon^2)$$ For sufficiently small $\epsilon > 0$ , it reduces to $Ax = x$ . My question is how can we take $\epsilon \to 0$ ? The vector $x$ clearly depends on $\epsilon$ .","['matrices', 'linear-algebra']"
4600549,"Nature of critical points of $f(x,y,z)$","How can I study the nature of the critical points of $f(x,y,z)=3xy^2+6y^2+x^2z+3z^2$ ? Since $$
\begin{cases}
f'_x=3y^2+2xz\\
f'_y=6xy+12y\\
f'_z=x^2+6z
\end{cases}
$$ the only critical point is $(0,0,0)$ . The Hessian matrix is $$
H_f(0,0,0)=\left(
\begin{matrix}
0 & 0 & 0\\
0 & 12 & 0\\
0 & 0 & 6
\end{matrix}\right),
$$ whose determinant is zero. Now I'm stuck, because I've tried to study $f$ along many restriction, and every time I realized that $O$ is a minimum, but I am not able to prove (or disprove) that $O$ is indeed a minimum. What can I do now?","['optimization', 'multivariable-calculus']"
4600561,Solve $y''-y'-6y=e^{3t}+5$,"Solve the following initial value problem $$y'' - y' - 6y = e^{3t} + 5, \quad y(0) = 0, \quad y'(0) = 0 $$ I used the Laplace transform and got $$ Y(s) \left(s^2-s-6\right)=\dfrac 1{s-3}+\dfrac 5s $$ then brought it to the other side $$Y(s)=\dfrac 1{(s-3)(s^2-s-6)}+\dfrac 5{s(s^2-s-6)}$$ Taking user577215664's advice, I got $$s^2−s−6=(s+2)(s−3)$$ so i split it into partial fractions $$\dfrac A{(s-3)}+\dfrac B{(s-3)^2}+\dfrac C{(s+2)}$$ and solving it i got
in part A $$ A=-\dfrac1{17}, \qquad B = C = \dfrac1{17} $$ and in part b
i got $$ B=1\dfrac4{6}, \qquad A = C = -\dfrac5{6} $$ so end up having $$-\dfrac1{17(s-3)}+\dfrac1{17(s-3)^2}+\dfrac1{17(s+2)}+-\dfrac5{6(s)}+\dfrac{10}{6(s-3)}+-\dfrac5{6(s+2)}$$ $$\dfrac1{17}e^{3t}+\dfrac1{17}te^{3t}+\dfrac1{17}e^{-2t}+-\dfrac5{6}+\dfrac{10}{6}e^{3t}+-\dfrac5{6}e^{-2t}$$ which can be simplified to $$\dfrac{82}{51}e^{3t}+\dfrac{79}{102}e^{-2t}+\dfrac1{17}te^{3t}+\dfrac5{6}$$ is this correct?","['inverse-laplace', 'initial-value-problems', 'laplace-transform', 'ordinary-differential-equations']"
4600622,C* algebras with no nilpotent elements are commutative,I am currently following my first course in operator algebras and I heard my teacher say that a C* algebra is commutative if and only its only nilpotent element is zero. The first implication is obvious but I really cant figure out the second one. Thanks for any help!,"['c-star-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras']"
4600645,Understanding notation of norm with three vertical lines,"We usually denote a norm with the notation $||\cdot ||$ . However I've seen someone write $|||\cdot|||$ . Is this the same as $||\cdot||$ ? For instance, I have readed a note regarding functional analysis. A lemma states: Lemma: Let $A$ be a Banach algebra with identity $I$ . Then there is a norm $|||\cdot|||$ on $A$ , equivalent to the original norm, such that $(A,|||\cdot|||)$ is a unital Banach algebra with $|||I|||=1$ . I am just curious. Could we just write the lemma as e.g. Lemma: Let $A$ be a Banach algebra with identity $I$ . Then there is a norm $||\cdot||$ on $A$ , equivalent to the original norm, such that $(A,||\cdot||)$ is a unital Banach algebra with $||I||=1$ .","['banach-spaces', 'operator-theory', 'normed-spaces', 'notation', 'functional-analysis']"
4600690,Finding the formula for the circumference of a circle with sequences,"Today I had an idea on how to find the formula for the circumference of a circle: $C = 2\pi r$ , where $r$ is the radius of a circle. The idea is that we start with an equilateral triangle ( $3$ sides) with a side length of $a$ . The perimeter is $3a$ . Notice that the distance of the ""center"" of the triangle is the same from all sides. Then we look at a square, with a side length of less than $a$ , say $b$ , and its center is also the same distance from all the sides. Its perimeter is $b$ . We continue in this fashion, and form a sequence for the perimeters of those shapes: $a_n = n s_n$ , where $s_n$ is the side length of the nth shape. Now we calculate $a_n$ 's limit, and we get $2 \pi r$ , where $r$ is the limit of the sequence of distances of the center of the shape from the sides: $r_n$ . The only problem is that I don't know how small to make the side each time... Will this idea work? If so, how small should I make the side each time? Thanks!","['circles', 'geometry', 'pi', 'sequences-and-series', 'limits']"
4600738,"Has anyone compiled a ""dictionary"" of graph theory?","I would be really interested in a book that lists out all the important special types of graphs and/or their properties. Edit: The Wikipedia page for ""List of Graph Theory Topics"" is decent, but I'm hoping for something more complete and self-contained.","['graph-theory', 'combinatorics', 'book-recommendation']"
4600742,"Prove formally that $\frac{\hat{p}_1-\hat{p}_2}{\sqrt{\hat{p}(1-\hat{p})\left(\frac{1}{n_1}+\frac{1}{n_2}\right)}}$ converges to $N(0,1)$","I'm not able to prove that $$\frac{\hat{p}_1-\hat{p}_2}{\sqrt{\hat{p}(1-\hat{p})\!\left(\dfrac{1}{n_1}+\dfrac{1}{n_2}\right)}}$$ converges in distribution to $N(0,1)$ . Here, $\hat{p}_1$ and $\hat{p}_2$ are independent estimations of the same proportion, $p$ , with sample sizes $n_1$ and $n_2$ , respectively. Besides, $$\hat{p}=\frac{n_1\hat{p}_1+n_2\hat{p}_2}{n_1+n_2}.$$ I know that $$\frac{\hat{p}_i-p}{\sqrt{\dfrac{p(1-p)}{n_i}}}$$ converges in distribution to $N(0,1)$ , for $i=1,2$ . My idea was to somehow subtract these two fractions to get the desired property, but convergence in distribution is not guaranteed to be preserved under substraction. I also have that $$\text{E}(\hat{p}_1-\hat{p}_2)=0,\ \text{Var}(\hat{p}_1-\hat{p}_2)=p(1-p)\!\left(\dfrac{1}{n_1}+\dfrac{1}{n_2}\right).$$ However, I am not able to come up with a way to apply the Central Limit Theorem or something similar. Any help would be appreciated. PD : sorry if this was asked before, I searched in aproach.xyz and couldn't find anything.","['probability-distributions', 'normal-distribution', 'probability', 'bernoulli-distribution']"
4600757,Absolutely continuous curves in Wasserstein distance and measurability.,"Let $(X, d, \mu)$ be a metric measure space. Let $P^1(X)$ denote the space of probability measures on $(X,d)$ , which have finite first moments, that is: \begin{equation}
\nu \in P^1(X) \implies \int d(x, x_0) \ d \nu < \infty
\end{equation} for some $x_0 \in X$ . Let us endow $P^1(X)$ with the Wasserstein distance $W_1$ , that is: \begin{gathered}
\forall \nu, \nu' \in P^1(X) \quad W_1( \nu, \nu') = \inf_{\eta} \int_{X \times X} d(x, y) \  d \eta(x,y),
\end{gathered} where the infimum is taken over all joint probability distributions $\eta$ which have $\nu$ and $\nu'$ as their marginals. Now, let $\gamma \colon [0,1] \to P^1(X)$ be an absolutely continuous curve. I would like to know what are the minimal requirements that a function $g \colon X \to [0, \infty]$ has to satisfy so that a function \begin{equation}
[0,1] \ni t \mapsto <g, \gamma(t)> =  \int_X g \ d \gamma(t)
\end{equation} is measurable. Since convergence in the Wasserstein distance implies the weak convergence of measures along with the convergence of the first moments, if $g$ is continuous then $t \mapsto <g, \gamma(t)>$ is also continuous. However, I wonder whether this condition could be relaxed to, for example, just the measurability of integrability (with respect to $\mu$ ) of $g$ .","['absolute-continuity', 'measurable-functions', 'functional-analysis', 'wasserstein', 'probability-theory']"
4600807,"On the tribonacci constant with $\cos(2\pi\,k/11)$, plastic constant with $\cos(2\pi\,k/23)$, and others","( This post is indebted to Oscar Lanzi .) Part I. $\color{blue}{p = 11}$ The equation, $$\big(4\sin[3t] - \tan[t]\big)^2 = 11$$ seems to have five solutions, given by $t = \frac{2\pi\,k}{11}$ for $k = 1,2,3,4,5$ , so it ranges from $t = 0.57\; \text{to}\; 2.85$ . However, if you look at its plot, it intersects the $x$ -axis at SIX points, the extra one being less than $0.5,$ As pointed out by Lanzi in an answer , this extra point in fact has a nice closed form involving the tribonacci constant $T$ , $$t = \arccos\big(T/2\big) = 0.40362481\dots$$ the real root of $T^3 = T^2+T+1$ . Part II. $\color{blue}{p = 23}$ I always wonder if things could be generalized, and Lanzi provided the equation for $p=23,$ $$\big(4\sin[2t] + 4\sin[3t] - 4\sin[5t] + 4\sin[6t] - \tan[t]\big)^2=23$$ Analogously, this seems to have eleven solutions, given by $t = \frac{2\pi\,k}{23}$ for $k = 1,2,\dots 11$ , so it ranges from $t = 0.2\; \text{to}\; 3.1$ . But it intersects the $x$ -axis at TWELVE points, the extra one between $1.7$ and $1.8$ , I suspected this would involve the plastic constant $P$ , the real root of $P^3 = P+1$ since $T$ is for $\mathbb{Q}\big(\sqrt{-11}\big)$ while $P$ is for $\mathbb{Q}\big(\sqrt{-23}\big)$ . Using Mathematica's FindRoot command and its integer relations subroutine, its closed-form apparently is, $$t = \arccos\big(\tfrac{1-P}2\big) = 1.73387721\dots$$ Part III. $\color{blue}{p = 31}$ Question: Anybody can find the equation for $p=31?\,$ I assume this may involve the supergolden ratio $S$ , the real root of $S^3=S^2+1$ since $S$ is for $\mathbb{Q}\big(\sqrt{-31}\big)$ , but that's just a guess.","['graphing-functions', 'trigonometry', 'gauss-sums', 'closed-form', 'constants']"
4600837,"Calculating the ""Spreads"" for Different Outcomes in Dice Rolls?","Suppose I roll a 6-sided die 100 times and observe the following data - let's say that I don't know the probability of getting any specific number (but I am assured that each ""trial"" is independent from the previous ""trial""). Below, here is some R code to simulate this experiment: # Set the probabilities for each number (pretend this is unknown in real life)
probs <- c(0.1, 0.2, 0.3, 0.2, 0.1, 0.1)

# Generate 100 random observations
observations <- sample(1:6, size = 100, replace = TRUE, prob = probs)

# Print the observations
print(observations)

  [1] 2 4 2 2 4 6 2 2 6 6 3 4 6 4 2 1 3 6 3 1 2 5 3 6 4 6 1 3 4 2 6 2 4 1 3 3 3 5 2 5 2 3 5 1 4 6 1 6 4 2
 [51] 2 3 2 3 3 5 6 5 4 3 2 3 2 1 2 3 2 2 5 3 2 1 1 1 3 3 2 4 4 3 1 4 4 6 3 3 5 5 2 2 1 3 2 1 6 3 4 3 3 3 As we know, the above experiment corresponds to the Multinomial Probability Distribution Function ( https://en.wikipedia.org/wiki/Multinomial_distribution ): $$
P(X_1 = x_1, X_2 = x_2, \dots, X_k = x_k) = \frac{n!}{x_1!x_2!\dots x_k!}p_1^{x_1}p_2^{x_2} \dots p_k^{x_k}
$$ Using Maximum Likelihood Estimation ( https://en.wikipedia.org/wiki/Maximum_likelihood_estimation MLE), the estimate for the probability for getting any number on this die is given by (e.g. what is the probability that this above die gives you a ""3""?): $$
\hat{p}_{i,\text{MLE}} = \frac{x_i}{n}
$$ Next, the Variance for each of these parameters can be written as follows : $$
\text{Var}(\hat{p}_{i,\text{MLE}}) = \frac{p_i(1 - p_i)}{n}
$$ From here, I am interested in estimating the ""spreads"" of these probabilities - for example, there might be a 0.2 probability of getting a ""6"" - but we can then ""bound"" this estimate and say there is a 0.2 ± 0.05 probability of rolling a 6. Effectively, this ""bounding"" corresponds to a Confidence Interval ( https://en.wikipedia.org/wiki/Confidence_interval ). Recently, I learned that when writing Confidence Intervals for ""proportions and probabilities"", we might not be able to use the ""classic"" notion of the Confidence Interval (i.e. parameter ± z-alpha/2*sqrt(var(parameter))), because this could result in these bounds going over ""1"" and below ""0"", thus violating the fundamental definitions of probability. Doing some reading online, I found different methods that might be applicable for writing the Confidence Intervals for the parameters of a Multinomial Distribution. Bootstrapping ( https://en.wikipedia.org/wiki/Bootstrapping_(statistics) ): By virtue of the Large Law of Large Numbers ( https://en.wikipedia.org/wiki/Law_of_large_numbers ), Bootstrapping works by repeatedly resampling your observed data and using this MLE formulas to calculate the parameters of interest on each of these re-samples. Then, you would sort the parameter in estimates in ascending order and take the estimates corresponding to the 5th and 95th percentiles. These estimates from the 5th and 95th percentiles would now correspond to the desired Confidence Interval. As I understand, this is an approximate method , but I have heard that the Law of Large Numbers argues that for an infinite sized population and an infinite number of resamples, the bootstrap estimates will converge to the actual values. It is important to note that in this case, the ""Sequential Bootstrap"" approach needs to be used such that the chronological order of the observed data is not interrupted. Delta Method ( https://en.wikipedia.org/wiki/Delta_method ): The Delta Method uses a Taylor Approximation ( https://en.wikipedia.org/wiki/Taylor%27s_theorem ) for the function of interest (i.e. MLE variance estimate). Even though this is also said to be an approximate method (i.e. the Delta Method relies on the Taylor APPROXIMATION), there supposedly exists mathematical theory (e.g. https://en.wikipedia.org/wiki/Continuous_mapping_theorem ) which can demonstrate that estimates from the Delta Method ""converge in probability"" to the actual values. This being said, I am not sure how the Delta method can directly be used to calculate Confidence Intervals. Finally, very recently I learned about the Wilson Interval ( https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval ), which is said to be more suitable for writing Confidence Intervals in the case of proportions and probabilities. In the case of the Multinomial Probability Distribution, I think the Wilson Interval for 95% Confidence Intervals on parameter estimates can be written as follows: $$
\left( \hat{\theta} - \frac{z_{\alpha/2} \sqrt{\hat{\theta}(1-\hat{\theta})/n}}{1+z_{\alpha/2}^2/n}, \hat{\theta} + \frac{z_{\alpha/2} \sqrt{\hat{\theta}(1-\hat{\theta})/n}}{1+z_{\alpha/2}^2/n} \right)
$$ However, I am still learning about the details of this. This brings me to my question: What are the advantages and disadvantages of using any of these approaches for calculating the Confidence Interval for parameter estimates in the Multinomial Distribution? It seems like many of these methods are approximations - but I am willing to guess that perhaps some of these approximate methods might have better properties than others. As an example: Perhaps some of these methods might take longer to calculate in terms of computational power for more complex functions and larger sample sizes? Perhaps some of these methods  might be less suitable for smaller sample sizes? Perhaps some of these methods are known to ""chronically"" overestimate or underestimate the confidence intervals? Perhaps some of these methods are simply ""weaker"" - i.e. the guarantee of the true parameter estimate lying between predicted ranges is not ""as strong a guarantee""? In any case, I would be interested in hearing about opinions on this matter - and in general, learning about which approaches might be generally more suitable for evaluating the Confidence Intervals on parameter estimates for the Multinomial Distribution. Thanks! Note: Or perhaps all these differences in real life applications might be negligible and they are all equally suitable?","['statistics', 'confidence-interval', 'probability']"
4600853,$(a-\lambda)^{-1}$ belongs to the $C^{*}$-subalgebra generated by $a$?,"Let $\mathcal{A}$ be a $C^{*}$ -algebra and $a \in \mathcal{A}$ be fixed. The $C^{*}$ -subalgebra generated by $a$ is the closure (with respect to the norm of $\mathcal{A}$ ) of the set of all polynomials in $a$ and $a^{*}$ . Suppose $\lambda \in \mathbb{C}$ is an element of the resolvent of $a$ , so that $(a-\lambda)^{-1}$ exists. I want to prove that this element belongs to the $C^{*}$ -subalgebra generated by $a$ . I honestly don't know exactly how to start. My guess was to formally expand $(a-\lambda)^{-1}$ as a Laurent series in $a$ and argue that it is an element of the desired algebra because it is the limit of polynomials in $a$ . However, this Laurent series is only convergent if $\lambda > \|a\|$ , whereas the statement holds for every $\lambda$ in the resolvent of $a$ , so this is probably not the best approach.","['c-star-algebras', 'abstract-algebra', 'functional-analysis']"
4600931,How to show that a set almost surely does not contain an infinite arithmetic progression?,"Let $S \subset \mathbb{N}$ with each number $n \in S$ with probability $1/2$ . Let further $k,l \in \mathbb{N}$ with $k \ge 2$ and let $A_l$ be the probability that $S$ contains an arithmetic progression of the form $$k-b, k, k+b, \ldots, k+(\lceil \log_2(kl2^{k-1})\rceil-2)b.$$ Show that $\mathbb{P}[A_l] \le 1/l$ and deduce from this that $S$ does not contain an arithmetic progression of infinite length. I think the way to go here is to assume that $S$ is sorted increasingly, i.e.: $$s_1 < s_2 < s_3 \ldots .$$ I recognise that the chance of $S$ containing a specific arithmetic progression is given by $$\frac{1}{2^{\log_2(kl2^{k-1})}} = \frac{1}{kl2^{k-1}},$$ but I do not see how to estimate the probability of $S$ containing no arithmetic progression with this. Could you please give me a hint?","['arithmetic-progressions', 'combinatorics']"
4600938,what is the integral $\int_{0}^{\infty}\frac{1}{(1+x^\alpha)^k} \ dx$ equal to,"I'm trying to evaluate the following integral but I'm stuck. $$I(\alpha,k)=\int_{0}^{\infty}\frac{1}{(1+x^\alpha)^k} \ dx$$ Using complex analysis I know that for $k=1$ it's equal to $$\frac{\pi}{\alpha\sin(\frac{\pi}{\alpha})}$$ (I used a contour shaped like a circle sector around the pole at $z=\exp(\frac{i\pi}{\alpha})$ and let the radius $R$ approach infinity) I however don't know how to evaluate it for a general $k$ as it generates a residue of a pole of order $k$ which would require a $(k-1)$ -th derivative that I don't know how to evaluate. I tried putting in some numbers in WolframAlpha and it I got the pattern that $$I(\alpha,k) = \frac{\Gamma(\frac{\alpha+1}{\alpha})\Gamma(k-\frac{1}{\alpha})}{\Gamma(k)}, \ \ k>\frac{1}{\alpha}$$ which is correct for the $k=1$ case, but I have no idea why it is the case. (The formula above is also defined fo fractional $k$ which I originally didn't consider to avoid nasty branch cuts)","['integration', 'definite-integrals', 'complex-analysis', 'contour-integration', 'closed-form']"
4600974,Calculate estimator's asymptotic distribution,"Let $W_1,\ldots,W_n \sim f_w(w;\lambda)=\frac{3w^2}{\lambda}e^{\frac{-w^3}{\lambda}}\mathbf{1}_{w>0}$ for some $\lambda>0$ . Give the asymptotic behavior of $\sqrt{n}(\lambda_{\text{MoM}}-\lambda)$ as $n\rightarrow \infty$ Give the asymptotic behavior of $\sqrt{n}(\lambda_{\text{MLE}}-\lambda)$ as $n\rightarrow \infty$ Give the asymptotic $(1-\alpha)\cdot100 \% $ confidence interval for $\log(\lambda)$ Note: MoM means method of moment, MLE means maximum likelihood estimator Try: I have use MoM: $\frac{\sum{Xi}}{n}=E[X]$ and $l=\operatorname{Log}(\text{Likelihood})$ to calculate the estimators, but don't know how to solve the asymptoic distribution $\sqrt{n}(\lambda_{\text{estimator}}-\lambda)$ .","['statistics', 'parameter-estimation', 'probability']"
4600992,Questions about the relation between convergence in distribution and convergence in probability,"I have two sequences of random variables $\{ X_n\}$ and $\{Y_n \}$ . I know that $X_n \to^d D, Y_n \to^d D$ . Can I conclude that $X_n - Y_n \to^p 0$ ? If I cannot, what other conditions do I need for the conclusion to hold? Thanks.","['statistical-inference', 'statistics', 'probability']"
4601075,"verification of continuity $\frac{\partial f}{\partial x}$ for $f(x,y) = \frac{xy}{\sqrt{x^2+y^2}}$?","$$
 f(x,y) =
\begin{cases}
\dfrac{xy}{\sqrt{x^2+y^2}}   & \text{if $(x,y)\neq(0,0)$ } \\[2ex]
0 & \text{if $(x,y)=(0,0)$ }  \\
\end{cases}
$$ we have to verify whether $\frac{\partial f}{\partial x}$ is continuous at $(0,0)$ or not. My answer : $\frac{\partial f}{\partial x}=\operatorname{sgn}(y)$ hence, $\frac{\partial f}{\partial x}$ should not be continuous at $(0,0)$ . Unfortunately, I did not get answers to verify it myself. Thank You.","['multivariable-calculus', 'calculus', 'solution-verification']"
4601124,"Prove $EB=EC$ and that $F,M,G,C$ are concyclic in the given figure","Given is a quadrilateral $ABCD$ in which $\angle DAB=\angle CDA=90$ . Point M is the midpoint of side $BC$ and circumscribed circles of triangles $\triangle ABM$ and $\triangle DCM$ meet at points $M$ and $E$ . The line $EC$ intersects the circumscribed circle of $ABM$ second time in $F$ , and lines $AF$ and $CD$ meet at $G$ . Prove $EB=EC$ and that $F,M,G,C$ are concyclic. I've only worked on $EB=EC$ as I assume that's a prerequisite to $F,M,G,C$ being proven to be concyclic. I think we might have to prove that $E$ has to be on line $AD$ as we know that since $\triangle EBC$ should be an isosceles triangle and $M$ is midpoint of $BC$ the angles $\angle EMC=\angle EMB=90$ , but also we know that since $CDEM$ is an cyclic quadrilateral that $\angle EMC+\angle EDC=180$ and $\angle EDC=\angle CDA-\angle ADE$ so $\angle ADE=0$ ? Any help's appreciated, thanks!","['quadrilateral', 'euclidean-geometry', 'geometry', 'plane-geometry']"
4601201,Find all three complex solutions of the equation $z^3=-10+5i$,Let $z\in \mathbb{C}$ . I want to calculate the three solutions of the equation $z^3=-10+5i$ . Give the result in cartesian and in exponential representation. Let $z=x+yi $ . Then we have $$z^2=(x+yi)^2 =x^2+2xyi-y^2=(x^2-y^2)+2xyi$$ And then $$z^3=z^2\cdot z=[(x^2-y^2)+2xyi]\cdot [x+yi ] =(x^3-xy^2)+2x^2yi+(x^2y-y^3)i-2xy^2=(x^3-3xy^2)+(3x^2y-y^3)i$$ So we get $$z^3=-10+5i \Rightarrow (x^3-3xy^2)+(3x^2y-y^3)i=-10+5i \\ \begin{cases}x^3-3xy^2=-10 \\ 3x^2y-y^3=5\end{cases} \Rightarrow \begin{cases}x(x^2-3y^2)=-10 \\ y(3x^2-y^2)=5\end{cases}$$ Is everything correct so far? How can we calculate $x$ and $y$ ? Or should we do that in an other way?,"['complex-analysis', 'calculus', 'systems-of-equations', 'complex-numbers']"
