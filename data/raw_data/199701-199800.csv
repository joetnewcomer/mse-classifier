question_id,title,body,tags
3892831,Understanding Formula for the Pair Correlation for the Farey Sequence,"I'm reading this paper https://arxiv.org/pdf/math/0404114.pdf , and I'm unsure how to interpret the formula in Theorem 2 (1.5), regarding the pair correlation function of the sequence of Farey Fractions. Specifically, I don't understand the $\lambda$ that they used. In (1.6), they wrote as $\lambda \rightarrow \infty$ , $g_2(λ) = 1 + O(λ^{−1})$ . So this means the function approaches 1 as $\lambda$ goes to infinity. This seems reasonable as I could see the function asymptotically approaching 1, the problem is I'm not sure what $\lambda$ means in this context. The only time they mentioned $\lambda$ beforehand is on page 2 when they defined $R_F^{(v)}(\lambda_1,...,\lambda_{v-1})=2^{-v+1}R_F^{(v)}(\prod_{j=1}^{v-1}[-\lambda_j,\lambda_j]).$ My guess right now is that $\lambda$ refers to the interval for which the correlation measures are taken. Thus, as the interval of normalized Farey Fractions become arbitrarily large, the function converges to 1. Is that reasonable? Then $\lambda$ would represent a box in $\mathbb{R}^{v-1}$ for $v \geq3$ . Could someone give me some clarity on what this $\lambda$ represents and how this would change for $v\geq 3$ ? Thanks a lot.","['number-theory', 'correlation', 'analysis']"
3893082,Optimal transport and total variation distance,"I have a question regarding the following concept equating total variation distance with a particular case of optimal transport. I don't understand why equality (6.11) holds.  We know by Kantorovich duality that the RHS is equal to $$2 \sup_{\phi \text{ Lipschitz} \\ |\phi|_{\text{Lip}} \leq 1} \int \phi d\mu - \int \phi d\nu \equiv f(\mu, \nu)$$ as a function is $c-$ convex for a distance function $c = 1(x \ne y)$ if and only if it is $1-$ Lipschitz. As for the total variation , it is defined as $$T(\mu, \nu) \equiv  \sup_{A \in \mathcal{F}} |\mu(A) - \nu(A)|$$ where $\mathcal{F}$ is our $\sigma-$ algebra on whichever Polish space we're working with.  It is obvious that for $\phi(x) = 1_A(x)$ , we have that $\phi$ is $1-$ Lipschitz and therefore $T(\mu, \nu) \leq f(\mu, \nu)$ .  I'm confused why we need the $2$ here, and how the other direction of the inequality would be shown? Specifically, I need that for any $1-$ Lipschitz function, there exists a set $A \in \mathcal{F}$ such that $|\mu(A) - \nu(A)| \ge 2 \int \phi d\mu - \int \phi d \nu$ , but I have no idea how to get this right.  Any help would be massively appreciated. (The excerpt is from Villani (2009))","['total-variation', 'optimal-transport', 'duality-theorems', 'probability-theory']"
3893142,"Why is mathematical induction necessary to prove results (eg, commutativity) for natural numbers but not for real numbers?","I've been studying the construction of the natural numbers, and I can't solve my own question, namely Why is it necessary to use mathematical induction? Let me clarify this. For example, we know that, for all $n,m \in \mathbb{N}$ ,  then $n \cdot m = m \cdot n$ . In order to prove this, we use mathematical induction, but when we think about $n,m \in \mathbb{R}$ (real numbers),  in order to prove $n \cdot m= m \cdot n$ , we don't need mathematical induction. Why sometimes in a proof it's enough to take $x \in \mathbb{R}$ , arbitrary number, but for natural numbers mathematical induction is necessary? Anyone can help me, please? I hope somebody can give me a hint in order to understand this question.","['elementary-set-theory', 'induction', 'natural-numbers']"
3893206,Graphing $f(2-x)$,"Sorry for this very trivial question, but I've become slightly confused by this question. Consider a graph $y=f(x)$ . How would I draw the graph $y=f(2-x)$ ? It seems to me that as this is obviously equal to $y=f(-(x-2))$ this should represent the graph being translated $2$ units in the positve $x$ direction and then reflected in the $y$ axis. Is that true? It doesn't seem to be from the graphs I have plotted using Desmos. If not, please explain why it is incorrect. Thanks for your help. EDIT: I have now slept over my problem and I believe that it lies in the following statement I have been led to believe in class: The graph of $f(\text{Blah}+a)$ is ALWAYS  a translation of $a$ units of the graph $f(\text{Blah})$ in the negative direction. More specifically, I thought that the as graph of $f(x+a)$ is a translation of $a$ units of the graph $f(x)$ in the negative direction, then the graph of $f(-x+a)$ is a translation of $a$ units of the graph $f(-x)$ in the negative direction as well. After thinking it over logically however, I now think this is wrong. This is my reasoning: Consider $y=f(x+a)$ . For a given $y$ value on the $y=f(x+a)$ graph, the $x$ value needed for it must be $a$ smaller than the $x$ value needed if it was just the function $y=f(x)$ ; hence the graph $y=f(x+a)$ must be the graph of $y=f(x)$ but shifted $a$ units to the negative $x$ direction. But, if we consider $y=f(-x+a)$ : For a given $y$ value on the $y=f(-x+a)$ graph, the $x$ value needed for it must be $a$ bigger than the $x$ value needed if it was just the function $y=f(-x)$ ; hence the graph $y=f(-x+a)$ must be the graph of $y=f(-x)$ but shifted $a$ units to the positive $x$ direction. Is my reasoning correct now?  Thanks again for your help.","['graphing-functions', 'functions', 'solution-verification', 'linear-transformations', 'transformation']"
3893379,Proving $\underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}}=\sqrt{2 \pi }$,"This question is the last part of a problem leading to proof of Stirling's approximation. I've already proved that $\underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}}$ exists and that $\underset{n\to \infty }{\text{lim}}\frac{2^{4 n} (n!)^4}{((2 n)!)^2 \ (2 n+1)}=\frac{\pi }{2}$ . Hence, the question asks to assume $\underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}}$ exists, and then asks to use $\underset{n\to \infty }{\text{lim}}\frac{2^{4 n} (n!)^4}{((2 n)!)^2 \ (2 n+1)}=\frac{\pi }{2}$ to show $\underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}}=\sqrt{2 \pi }$ . My attempt goes like this: Since $\sqrt{x}$ is continuous, we can use $\sqrt{\underset{n\to \infty }{\text{lim}}f(n)}=\underset{n\to \
\infty }{\text{lim}}\sqrt{f(n)}$ to get $\underset{n\to \infty }{\text{lim}}\frac{2^{2 n} (n!)^2}{(2 n)! \
\sqrt{2 n+1}}=\sqrt{\frac{\pi }{2}}$ . Then we can eliminate $2^{2 n}n!$ to get $$\frac{2^{2 n} (n!)^2}{\sqrt{1+2 n} (2 n)!}=\frac{n!}{\sqrt{1+2 n} \left(n-\frac{1}{2}\right) \left(n-\frac{3}{2}\right) \cdots  \
\frac{3}{2}\frac{1}{2}}$$ Then factor out $n^n$ and adjust $\sqrt{1+2n}$ to get $$\frac{2^{2 n} (n!)^2}{\sqrt{1+2 n} (2 n)!}=\frac{n!}{n^{n+\frac{1}{2}} \sqrt{2+\frac{1}{n}} \left(1-\frac{1}{2n}\right) \left(1-\frac{3}{2n}\right) \cdots  \
\frac{3}{2n}\frac{1}{2n}}$$ The $\sqrt{2+\frac{1}{n}}$ factor will give a $\sqrt{2}$ , so the remaining $\prod _k^n \left(1-\frac{2 k-1}{2 n}\right)$ must somehow relate to $\sqrt{2} e^{-n}$ . However, I'm not sure how to do this.","['sequences-and-series', 'real-analysis']"
3893399,Understanding meromorphic/holomorphic forms on Riemann surface,"I refer to Rick Miranda - Algebraic curves and Riemann surfaces chapter IV.1 ( p. 105 , p. 106 , p. 107 ). I think I understand the regular Euclidean $\mathbb C$ case: the idea of meromorphic/holomorphic $1$ -form on open set $V_1$ of $\mathbb C$ : $\omega_1 = f(z)dz$ , for $f$ mero/holo function on $V$ and the idea of the transformation rule: for $\omega_2 = g(w)dw$ on open set $V_2$ of $\mathbb C$ with $g$ mero/holo on $V$ , we say that $\omega_1$ transforms to $\omega_2$ under $T$ if $g(w)=f(T(w))T'(w)$ for some holo $T: V_2 \to V_1$ Where it gets fuzzy for me is the case of Riemann surfaces. I wish Miranda would have 1st defined for charts on Riemann surface, but Miranda instead goes straight to Riemann surfaces. Apparently $\omega$ , a mero/holo $1$ -form on Riemann surface $X$ (in this book, all Riemann surfaces are connected), is a 'collection' (see (A1)) of mero/holo $$\{\omega_{\phi} | \phi: U \to V \ \text{is a chart in, I think, the max atlas of X}\} \tag{see (A2)}$$ such that for all charts $\phi_1: U_1 \to V_1$ , $\phi_2: U_2 \to V_2$ , with overlapping domains, we have that $\omega_{\phi_1}$ transforms to $\omega_{\phi_2}$ under $T=\phi_1 \circ \phi_2^{-1}$ . I guess this is $T: \phi_2(U_1 \cap U_2) \to \phi_1 (U_1 \cap U_2)$ . Ostensibly, we have that for, say, $\omega_{\phi_1}$ , the expression for $\omega_{\phi_1}$ is like ' $\omega_{\phi_1} = f_1(z) dz$ ', for coordinate $z = \phi_1(x)$ and some mero/holo $f_1=f_1(z)$ on open subset $V_1$ of $\mathbb C$ . But what I expected was something an expression involving some mero/holo $h_1=h_1(x)$ on the chart $U_1$ of $X$ , like $\omega$ is some map $$\omega: X \to \{\text{probably some bundle thing in complex geometry that I didn't learn yet}\},$$ where the restriction $\omega|_{U_1}$ is a well-defined (because of the transformation rule for overlapping domains) mero/holo $1$ -form on the chart domain $U_1$ ,  given as $\omega|_{U_1} = h_1(x) dx$ , where the ' $|_{U_1}$ ', is just omitted. And then we can map this from $X$ to $\mathbb C$ like maybe there's some correspondence to the mero/holo $1$ -form ' $\omega|_{V_1}$ ' on the chart image $V_1$ , given as something like $\omega|_{V_1} = (h_1 \circ \phi_1^{-1})(z) dz$ or even like $(h_1 \circ \phi_1^{-1})(z) d(\phi_1^{-1}(z))$ . This way $f_1 = h_1 \circ \phi_1^{-1}: \phi_1(U_1) =V_1 \to U_1 \to \mathbb C$ . Question 1 : Are $\omega$ 's indeed locally like $\omega|_U = h(x) dx$ and then converted from $X$ 's local coordinate $x$ on $U$ into $\mathbb C$ 's local coordinate $z$ on $V$ into ' $\omega|_{V}$ ' = $(h \circ \phi^{-1})(z) dz$ ? Question 2 : Later on, there's a definition for order. How should I understand the definition for order in terms of the above? In particular, is my definition as follows correct? The definition is given as ' $ord_p(\omega) := ord_0(f)$ ', for ' $\omega = f(z) dz$ ', where $z=\phi(x)$ , for chart $\phi: (U,p) \to (V,0)$ , centred at $p \in U$ . I understand this as $ord_p(\omega)$ $:= ord_{\{\phi(p)=0\}}(f \circ \phi^{-1}(z))$ , for $\omega|_V = (f \circ \phi_1^{-1})(z) dz$ , which in turn is from $\omega|_U = f(x) dx$ . Therefore, I can make this kind of definition chain: $ord_p(\omega) := ord_p(\omega|_U)$ and then $ord_p(\omega|_U) := ord_p(f)$ (and then finally $ord_p(f) := ord_{\{\phi(p)=0\}} (f \circ \phi^{-1})$ ). In particular, this is why I was hoping we would 1st have a definition for $1$ -forms on charts: like if a Riemann surface $X$ is covered by a single chart $\phi: U = X \to V$ then we can do for its 1-forms $\omega$ like $ord_p(\omega|_U) := ord_p(f)$ (where $\omega$ = $\omega|_U$ since $U=X$ ). Question 2.1 : Btw, for the original definition of ' $ord_p(\omega) := ord_0(f)$ ', for ' $\omega = f(z) dz$ ', can I just instead of any chart, that's not necessarily centred at $p$ ? This way, I would define $ord_p(\omega) := ord_{\phi(p)}(f)$ , whether or not the chart $\phi: U \to V$ , that gives us the local coordinate $z=\phi(x)$ , is centred at $p$ . Of course, it's more convenient to have Laurent series about 0, but just wondering if there's anything particular about the number 0. Edit: Btw, there's also this thing in the text (but this is on 2-forms now) i noticed that goes like $$\int \int_{T} \eta = \int \int_{\phi(T)} f(z, \overline z) dz \wedge d \overline z,$$ where ' $\eta = f(z, \overline z) dz \wedge d \overline z$ '. I mean, if ' $\eta = f(z, \overline z) dz \wedge d \overline z$ ', then one might think you wouldn't have to change the region of integration when replacing $\eta$ with $f(z, \overline z) dz \wedge d \overline z$ . If this were 1-form, like $\eta = f(z) dz$ , I'd think ' $f(z)$ ' is actually like $f \circ \phi^{-1}(z)$ (A1): I guess similar to how a holo function on a non-connected open set is a 'collection' of holo functions on connected open sets. (A2): I think initially mero/holo $1$ -form is defined in Def IV.1.7/3 for every chart in max atlas and then later it's defined for every chart in an atlas in Lemma IV.1.8/4.","['riemann-surfaces', 'complex-geometry', 'complex-analysis', 'differential-forms', 'differential-geometry']"
3893405,"Is there a simple, but tight lower bound for the error made when $\sum_{n=1}^{k}\frac{1}{n^2}$ is used to approximate $\frac{\pi^2}{6}$?","As a math-for-fun exercise, I've recently been seeking bounds for the error $R_k$ made when using $\sum_{n=1}^{k}1/n^2$ to estimate its beautiful sum $\pi^2/6$ . Applying the Comparison Test for series multiple times, I derived the following estimates: $$\frac{1}{k+1}<\frac{\pi\coth(\pi)-1}{2}-\sum_{n=1}^{k}\frac{1}{n^2+1}<R_k<\ln\left(1+\frac{1}{k}\right)<\frac{1}{2k+2}+\frac{1}{2k}$$ The lower estimate $$\frac{\pi\coth(\pi)-1}{2}-\sum_{n=1}^{k}\frac{1}{n^2+1}<R_k$$ which I derived with WolframAlpha's help, is pretty useless. $1/(k+1)<R_k$ is good, but I want to improve it. Given how clean and tight the upper estimate $$R_k<\ln\left(1+\frac{1}{k}\right)$$ is, I figured I could find a similar lower estimate that's just as clean and tight. After thinking for a while, I came up empty handed. I can't seem to find a positive sequence $a_n$ lying between $1/(n^2+1)$ and $1/n^2$ for which $\sum_{n=1}^{k}a_n$ has a clean expression. Any ideas or hints? Edit: I'm not trying to prove the convergence of $\sum_{n=1}^{\infty}\frac{1}{n^2}$ nor any other series.","['calculus', 'sequences-and-series']"
3893415,"Likelihood function of $n$ for $X\sim \text{Bin}(n,0.5)$","I have to find the likelihood function for $X\sim \text{Bin}(n,0.5)$ where we know that $x=14$ and so $n \ge 14$ . Several trials were conducted counting successes (14 of which there were) but the number of trials was forgotten, so we need to estimate $n$ . I know that the pmf of a Binomial random variable is $$P(X=x)=\binom{n}{x}\cdot p^x \cdot(1-p)^{n-x}$$ and in this case we have $$P(X=14)=\binom{n}{14}\cdot 0.5^{14} \cdot(1-0.5)^{n-14}=\binom{n}{14}\cdot0.5^{n}\,,$$ however I'm not sure this would be useful since for $n \ge 14$ , the function seems to increase at an exponential rate that doesn't appear to have a maximum. So I'm not sure using the pmf is appropriate - does anyone have any other ideas? Any help would be greatly appreciated.","['statistics', 'probability-distributions', 'parameter-estimation', 'binomial-distribution']"
3893438,Where is the empty set in a venn diagram?,"""Show that the empty set is a subset of every set.""  This is the problem I'm working on. It seems standard, but the nuance is that if two sets are disjoint, and the empty set is a subset of both, how do we draw the empty set in a Venn Diagram? I searched and found this article: venn diagram of power set and empty set , but the person's response is that the empty set is in the intersection of all the other sets... my point is that those sets could be disjoint, so there would be no intersection.  In that case, how do we draw it? I recognize that the answer might just be that venn diagrams break down and don't illustrate the empty set.  If so, are there other visualization techniques that are general enough? I know enough category theory to briefly think about it from that perspective. Thanks in advance",['elementary-set-theory']
3893543,"Finding the transitive closure of a given relation on $X = \{1,5,7,9\}$","What is the transitive closure of the relation $\{(1,5),(5,7),(7,9),(1,7),(1,9),(5,9)\}$ on $X=\{1,5,7,9\}$ ? I think this relation is already transitive. $(1,5),(5,7) \implies (1,7)$ already exist $(1,5),(5,9) \implies (1,9)$ already exist $(1,7),(7,9) \implies (1,9)$ already exist Did I miss something or this relation is already transitive so my answer which is $\{(1,5),(5,7),(7,9),(1,7),(1,9),(5,9)\}$ correct?","['relations', 'solution-verification', 'discrete-mathematics']"
3893551,"Derivative of a function of the form $f=f(x,g(x))$","Consider a function of the following form $f=f(x,g(x))$ . For the derivative of this function, we do the following. $$
\frac{df}{dx} = \frac{df}{dx} \bigg\rvert_{g}+\frac{df}{dg} \frac{dg}{dx}
$$ I have the following question about the above equation. Since $g$ itself is a function of $x$ , does it even make sense to say "" $f$ is a function of $x$ and $g$ ""? Does the first term of the equation read ""derivative of $f$ with respect to $x$ at a fixed $g$ ""? If so, how does this make sense since if $x$ is changed, $g$ also changes and the derivative is no longer calculated at a fixed $g$ . Please let me know if you need me to make myself more clearer. Thank you for answering.",['derivatives']
3893571,Is there an example of a convex differentiable function which is not continuously differentiable?,"It is easy to see that any convex differentiable function of one variable is also continuous differentiable. But the proof is based on monotony of the derivative and doesn't work in multiple dimensions. So the question is, has any convex differentiable function of many variables to be also continuously differentiable? Or is there some counterexample? Thanks in advance.","['convex-analysis', 'derivatives', 'real-analysis']"
3893572,Hartshorne's Exercise II.4.5(c). A third time.,"It seems that I am the third one to pose this question. I think the proof in this question has a gap. He have not shown the lifting he constructed is compatible with $\mathrm{Spec}k(p) \rightarrow X$ . And the proof given in wikiproof have not shown why Z satisfies the property as described in the exercise, which in my opinion is the most key part. Any way to fill the gap is admired.","['algebraic-geometry', 'schemes']"
3893573,"A set in the Borel $\sigma$-algebra over $[0,1]$ that isn't in the algebra generated by open sets","The Borel algebra on $[0,1]$ is by defintion a $\sigma$ -algebra, the smallest one containing every open subset of $[0,1]$ . I'm wondering how the Borel algebra differs from the algebra generated by the open subsets of $[0,1]$ . What's an example of a set in the Borel algebra that you can't obtain by closing open subsets of $[0,1]$ under complements and finite unions? Added. Does any countable dense set work? Obviously we have to choose a Borel set that is neither open nor closed. The set of rationals in $[0,1]$ fits the bill here, and my intuition is that it cannot be written using finitely many set operations with open sets, but I'm not quite sure how to prove it.","['borel-sets', 'general-topology', 'measure-theory']"
3893583,How is Optimal Transport algorithmically related to the Assignment Problem?,"In optimal transport , we calculate the distance between two probability measures $\mu$ and $\nu$ over the compact set $[a,b]\subset\mathbb R$ , using the Earth Movers distance which is a special case of the Wasserstein formula: $$W=\inf_\pi\int|x-y|\,{\rm d}\pi(x,y),$$ where $\pi$ is interpreted as a transport matrix satisfying $\int\pi(x,y)dy=\mu(x)$ and $\int\pi(y,x)dy=\nu(x)$ . It is said that the Earth Movers distance shown above is solved using the Hungarian algorithm by Munkres, which is also used for solving the Assignment Problem . Are optimal transport and the assignment problem related then, and how mathematically?","['optimization', 'statistics', 'probability-distributions', 'optimal-transport']"
3893648,How to prove : $|A_{1} \Delta \cdots \Delta A_{n}| = \sum_{i} |A_{i}| - 2 \sum_{i<j}|A_{i} \cap A_{j}| + \cdots$?,"The Problem is under the topic 'Inclusion-Exclusion' of Combinatorics. The 'Indicator function' $\chi_{A}(x)$ is defined as : $\chi_{A}(x)= 1, if x \in A$ otherwise $ 0, if x \notin A$ . And has the properties : (1) $\chi_{A \cap B}(x) = \chi_{A}(x) \chi_{B}(x)$ , (2) $\chi_{A \cup B}(x) = \chi_{A}(x) + \chi_{B}(x) - \chi_{A}(x) \chi_{B}(x)$ , (3) $\overline {\chi_{A}(x)} = 1- \chi_{A}(x)$ , and analogiclly so on. With the help of this above defined function we've to prove the following equality : $|A_{1} \Delta \cdots \Delta A_{n}| = \sum_{i} |A_{i}| - 2 \sum_{i<j}|A_{i} \cap A_{j}| +4 \sum_{i<j<k}|A_{i} \cap A_{j} \cap A_{k}| - \cdots$ I started by using this formula : $|A \Delta B| = |A| + |B| - 2 |A \cap B|$ . Edit (my Second attempt) : $A_{1} \Delta A_{2} = (A_{1} - A_{2}) \cup (A_{2} - A_{1}) - (1)$ By the theorem of Inclusion- Exclusion principle, we know that : $|A_{1} \cup \cdots \cup A_{n}| = \sum_{i} |A_{i}| - \sum_{i<j}|A_{i} \cap A_{j}| + \sum_{i<j<k}|A_{i} \cap A_{j} \cap A_{k}| - \cdots + (-1)^{n+1} |A_{1} \cap \cdots \cup A_{n}| -(2)$ . I've been trying to combine these two equations (1) and (2) to get the desired result but no success so far. Any suggestion for this method will be very much helpful. But, I don't know how to proceed next? Please help me!","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
3893725,Is a group isomorphic to the internal product of its Sylow p-subgroups?,"Let $G$ be a group such that its order is a product of distinct primes $p_1, \dots, p_n$ and let $P_i$ denote each Sylow $p_i$ -subgroup. Is $P_1 \dots P_n$ (the internal or Frobenius product) equal to $G$ , that is, $G = P_1 \dots P_n$ ?","['group-theory', 'finite-groups']"
3893737,example of a particular homogeneous topological space,"I encountered a problem a few days ago: what is an example of a homogeneous $T_0$ space which is not $T_1$ ? I tried to solve this using some symmetrical properties of separation axioms which I saw before, but I couldn't. I also find this on web, but I can't prove that the so called space in example 4.4 is homogeneous (the article says that it is an example of what I'm searching for.) because I have some problems taking the greater element to the smaller, applying order topology on an ordered set. Any help would be appreciated!",['general-topology']
3893739,Metrics on $SO(n+1)/SO(n)$,"I am having a question about finding the metric on $SO(n+1)/SO(n)$ such that $\pi:SO(n+1)\rightarrow SO(n+1)/SO(n)$ is a Riemannian submersion, which $SO(n)$ are equipped with the standard bi-invariant metric. I try to use Schur's Lemma to show that it is isometric to the canonical metric on $S^n$ multiplied by a constant. I am confusing about how to find this constant directly.","['geometry', 'lie-groups', 'differential-geometry']"
3893755,Using probability of overlap to calculate number of particles,"I have a setup with multiple synchronized cameras recording an event with an unknown number $N$ particles. Only a subset of the particles are detected in any given camera, we assume an average of $n$ particles are detected in each camera. Of the $n$ particles detected in each camera, there is a certain number of particles $\eta$ that overlap in all cameras. Based on the known values of $n$ and $\eta$ , and assuming a uniformly distributed choice of particles $n$ , it is possible to estimate the value of $N$ with \begin{equation}
N=\left(\frac{n^c}{\eta}\right)^{\frac{1}{c-1}},
\end{equation} where c is the number of cameras used. I have derived this expression using very elementary probabilities and checked that it is correct with simple numerical experiments. Derivation Let there be $N$ particles that are divided into group $\alpha$ with $n=rN$ particles, where $r=1/2$ is the find rate of each camera. A second group $\beta$ contains the other $n=rN$ particles. There are $c=3$ cameras imaging the particles and each camera will randomly sample $n=rN$ particles. For now let us assume that these will be either exactly group $\alpha$ or group $\beta$ . The probability of all three cameras imaging $\alpha$ is \begin{equation}
P(A)=\frac{1}{2}\frac{1}{2}\frac{1}{2}=\frac{1}{8}  \,,
\end{equation} and likewise the probability of all three cameras imaging $\beta$ is \begin{equation}
P(B)=\frac{1}{2}\frac{1}{2}\frac{1}{2}=\frac{1}{8}  \,.
\end{equation} The probability that all three camera image the same group (either all $\alpha$ or all $\beta$ ) is \begin{equation}
P(A \cup B)=P(A)+P(B)= \frac{1}{4}  \,.
\end{equation} This means that there is a 1 out of 4 chance that all three cameras will image the same group of particles. To generalize the statement, we remove the initial constraint of grouping particles into $\alpha$ and $\beta$ and allow any combination of particles $n$ in each camera. Now instead of having $r=1/2$ particles intersecting $P(A \cup B)= 1/4$ of the time, we can equivalently expect \begin{equation}
R=r\,P(A \cup B)=\frac{1}{2}\frac{1}{4}=\frac{1}{8}\,,
\end{equation} of the particles intersecting all the time, where $R=\eta/N$ is the expected number of intersections as a ratio of $N$ . By examining this simple example, I can write a relationship for intersecting sets of random variables \begin{equation}
\label{432}
R=r^c \,,
\end{equation} where $\{r:\, 0\leq r\leq1\}$ is the find rate of each camera and $c$ is the number of cameras. Substituting $R=\eta/N$ and $r=n/N$ into the above equation and solving for $N$ gives \begin{equation}
N=\left(\frac{n^c}{\eta}\right)^{\frac{1}{c-1}}.
\end{equation} Question I am not very satisfied with the way I derived my final equation. I need help coming up with a more mathematical approach to the derivation. I have tried with a Hypergeometric distribution, but could not extend that to beyond $c=2$ . As a next step I would like to derive a similar expression to solve for $N$ for the case where $n$ is no longer uniformly distributed over $N$ , but rather distributed based on a known weighting function. I think if I get a good grasp on how to derive the original expression for $N$ mathematically, extending this to a weighted distribution should not be too difficult.","['statistics', 'probability']"
3893759,projective varieties and morphisms from infinitesimals,"Let $(A, m)$ be a local ring and $X$ a projective $A$ -scheme. Let $f \in m$ . If there is a $\operatorname{Spec} A$ morphism $\operatorname{Spec} A/(f) \rightarrow X$ , then for any $n$ there is a $\operatorname{Spec} A$ morphism $\operatorname{Spec} A/(f^n) \rightarrow X$ . Is this true? I think it is only supposed to be true if $X$ is smooth and $(A, (f))$ is a henselian pair, but I don't understand where the error is in the proof below. Since $X$ is projective, it is locally defined by homogeneous polynomials over $A$ . So morphism $\operatorname{Spec} A/(f) \rightarrow X$ induces $A$ -homomorphism $A[T_1, \dots, T_n]/I \rightarrow A/(f),$ where $I=(P_1, \dots, P_m)$ , and $P_j$ are homogeneous. Let $a_i \in A$ such that $T_i \mapsto a_i  \ \mathrm{mod} (f).$ Then, we can define $A[T_1, \dots, T_n]/I \rightarrow A/(f^n), T_i \rightarrow f^{n-1} a_i \ \mathrm{mod} (f^n),$ and hence have a $\operatorname{Spec} A$ morphism $\operatorname{Spec} A/(f^n) \rightarrow X$ . Why is this not correct? Is there hope of this statement being true under hypotheses weaker than smoothness?","['algebraic-geometry', 'commutative-algebra']"
3893764,$100$ people have $100$ one-dollar bills. Some give bills to others until all have different amounts. What's the least number of people to give money?,"In a group of $100$ people, each of them has a wallet with $100$ one-dollar bills. Some of them gave one or more dollars to one or more of the others and eventually everyone ended up with a different amount $>0$ . What is the least number of people who gave some of their money? I have started by trying to make $10,000$ as a sum of different integers. One way is $4+6+7+\cdots+141$ , which means that $95$ people have given money to others but I don’t think this is the correct approach. Also tried taking one person's amount and sharing it to others, so for example, $4+5+\cdots+14 = 99$ but again this will take for ever and we won't know if it's the optimal. Can anybody help me out? Thank you!","['algebra-precalculus', 'integers', 'combinatorics', 'discrete-mathematics']"
3893841,Minimax Estimator for Normal Random Vector,"Question. Suppose $Y_i \sim N(\mu_1, 1)$ . Let $Y := (Y_1, Y_2)$ , and $T_y = (Y_1, 0)$ . Denote $\Theta$ as the space of all estimators $\mu := (\mu_1, \mu_2)$ . Is it necessarily true that $\hat{\mu}$ is minimax if $$\Theta := \{\mu : \mu_2 = 0\} \quad\quad\text{or}\quad\quad \Theta := \{\mu : \mu_1 = 0\}.$$ I'm not quite sure how to show these statements. Starting with the leftmost, the risk function is $$
\begin{align}
R(\mu, T_Y) &= \mathbb{E}||T_Y - \mu||^2  \\
&= \mathbb{E}||(Y_1 - \mu_1, 0)^T||^2 \\
&= \mathbb{E}\left(Y_1^2 - 2Y_1\mu_1 + \mu_1^2\right) \\
&= \mathbb{E}Y_1^2 - 2\mu_1\mathbb{E}Y_1 + \mu_1^2 \\
&= 1 + \mu_1^2 - \mu_1^2 \\
&= 1.
\end{align}
$$ So, $\hat{\mu}$ is a minimax estimator if $$\sup_{\mu \in \Theta}\mathbb{E}||\hat{\mu} - \mu||^2 = \inf_T\sup_{\mu \in \Theta} R(\mu, T_Y) = 1$$ Not sure how to proceed? I'm assuming the other statement will be similart to solve.","['statistical-inference', 'statistics', 'parameter-estimation', 'decision-theory']"
3893876,Compute $\int_{-\infty}^{\infty} \frac{1}{(x-5)^2}dx$,"I want to compute $$I = \int_{-\infty}^{\infty} \frac{1}{(x-5)^2}dx.$$ We can use the following methods Compute directly $$I = -(x-5)^{-1}\bigg |_{-\infty}^{\infty}=-\bigg(\frac{1}{x-5}\bigg)_{-\infty}^{\infty}=-\bigg(0-0\bigg)=0$$ Compute by residue $$I = 2\pi \text{ Res}(f(z);5)=0, \ \ \ \ f(z) = \frac{1}{(z-5)^2}$$ We know that it has a pole of order $2$ . So its residue is $0$ . However, when checking by computer, I get ""integral not converge"". Not sure where I made mistakes. Thanks!","['integration', 'complex-analysis', 'residue-calculus', 'definite-integrals']"
3893906,Carrying weight of $270$ kg,"Problem: A total of $270$ kg of watermelons, each weighing at most $7$ kg, must be transported by $11$ carriers at once. Show that if each carrier can carry up to $30$ kg at a time, this can be done regardless of the weight of the individual watermelons. $\\$ My opinion about the solution: Let the number of watermelons be $n$ . Let the weight of the watermelons be $0< x_1 \leq x_2 \leq \dots \leq x_n \leq 7$ . Since $$\sum_{k=1}^{n} x_k = 270 $$ and $7n \geq n\cdot x_n \geq 270$ we find $n \geq 39$ . For $39 \leq n \leq 44$ , $\{ x_1, x_2, x_3, x_4\},  \{ x_5, x_6, x_7, x_8\}, \dots, \{ x_{41}, x_{42}, x_{43}, x_{44}\}$ groups can be transported by $11$ carriers. For $n>44$ , I have no enough idea. For sufficiently large $n$ values, some weights of watermelons will decrease. Thanks for your interest...","['inequality', 'discrete-mathematics']"
3893911,"About the property of $m$: if $n < m$ is co-prime to $m$, then $n$ is prime [duplicate]","This question already has answers here : Let n>30. Then prove there exists a natural number $ 1< m\leq n$ such that $(n,m)=1$ and $m$ is not prime. (3 answers) Closed 11 years ago . The number $30$ has a curious property: All numbers co-prime to it, which are between $1$ and $30$ (non-inclusive) are all prime numbers! I tried searching(limited search, of course) for numbers $\gt 30$ that have this property, but could not find any. Are there any such numbers $\gt 30$?","['number-theory', 'elementary-number-theory', 'prime-numbers']"
3893952,ODE $y'+ x\sin( 2y) = x e^{-x^2} \cos^2 (y)$,I have the following ODE: $$y'+ x\sin (2y) = x e^{-x^2} \cos^2 (y)$$ I'm stuck trying to get it into a linear form. I've tried $\sin (2y) = \sin y \cos y $ and then dividing the ODE by $( \cos  y ) ^{-1} $ . This got me to nothing so I tried dividing by $( \sin y )^{-2} $ instead. I got stuck as well. I think I need to make a substitution but I don't know which one. Thanks.,['ordinary-differential-equations']
3893961,Hölder continuous dependence on parameters for solutions of ODE,"We have the following result for continuous dependence of the initial value for ODEs with a continuous right-hand side (Satz 8.18 in https://www.mathematik.hu-berlin.de/~baum/Skript/DGL-2012.pdf ): Let $F:U \subset \mathbb{R}^n \times \mathbb{R} \rightarrow \mathbb{R}^n$ on $U$ be continuous and Lipschitz continuous with respect to the $\mathbb{R}^n$ -variable with Lipschitz constant $L$ .
Let $(x_0,t_0),(x_0^*,t_0) \in U$ and $\varphi_{x_0},\varphi_{x_0^*} : [t_0 - \epsilon, t_0 + \epsilon] \rightarrow \mathbb{R}^n$ be solutions of the ODE $x'=F(x,t)$ with initial values $\varphi_{x_0}(t_0)=x_0$ and $\varphi_{x_0^*}(t_0)=x_0^*$ .
Then: $$| \varphi_{x_0}(t)-\varphi_{x_0^*}(t) |
\leq |x_0-x_0^*| \cdot e^{L|t-t_0|}
\forall t \in [t_0-\epsilon,t_0+\epsilon].$$ Question:
is there a similar result for Holder continuity?
My dream result would be Let $F:U \subset \mathbb{R}^n \times \mathbb{R} \rightarrow \mathbb{R}^n$ on $U$ be smooth and $\alpha$ -Holder continuous with respect to the $\mathbb{R}^n$ -variable, with $\alpha$ -Holder norm bounded by $L$ .
Let $(x_0,t_0),(x_0^*,t_0) \in U$ and $\varphi_{x_0},\varphi_{x_0^*} : [t_0 - \epsilon, t_0 + \epsilon] \rightarrow \mathbb{R}^n$ be solutions of the ODE $x'=F(x,t)$ with initial values $\varphi_{x_0}(t_0)=x_0$ and $\varphi_{x_0^*}(t_0)=x_0^*$ .
Then there exist a universal constant $c$ independent of $F$ , and $\beta \in (0,1)$ such that $$| \varphi_{x_0}(t)-\varphi_{x_0^*}(t) |
\leq c|x_0-x_0^*|^{\beta} \cdot e^{L|t-t_0|}
\forall t \in [t_0-\epsilon,t_0+\epsilon].$$ Note that I am happy to assume my function is smooth in order to have a unique solution.
I also note that I can use a $C^k$ -bound for $F$ to get a $C^k$ bound for the solution depending on the initial value.
But what I need is a $C^{0,\beta}$ -bound for the solution that only depends on the $C^{0,\alpha}$ -bound for $F$ .
I can imagine something like this exists for $\beta=\alpha/2$ , but maybe even $\beta=\alpha$ is possible. I know that the proof of the above theorem cannot be adapted to prove my estimate.
I also found ""Agarwal, Lakshmikantham: Uniqueness and nonuniqueness criteria for ordinary differential equations"" to make some statements about uniqueness of the solution for a Holder-continuous right-hand side $F$ .
But I did not find anything resembling the estimate I need. Context:
I have two metrics on a compact manifold, $g_1, g_2$ , satisfying the estimate $||g_1-g_2||_{C^{1,\alpha},g_1}<c_1$ .
I also have a vector $\eta$ satisfying $|| \eta ||_{C^{1,\alpha},g_1} < c_2$ .
I would like to have an estimate $|| \eta ||_{C^{1,\beta},g_2} < F(c_1,c_2)$ , where $\beta$ can depend on $\alpha$ , but should not depend on $\eta$ , and $F(c_1,c_2)$ is some universal expression in $c_1$ and $c_2$ .
I believe that my dream ODE result from above would give me such an estimate.","['holder-spaces', 'ordinary-differential-equations']"
3893967,Changing variable in partial derivative's,"I have $u\colon:\mathbb{R}^n\times(0,\infty)\to\mathbb{R}$ smooth, $u=u(x,y)$ . Consider the following changing of variables: $$ z=\biggl(\frac{y}{1-a}\biggr)^{1-a},$$ where $a\in(-1,1)$ , i want to prove that: \begin{equation}
y^au_y=u_z.
\end{equation} I have found this change of variables in link . I tried to apply the chain rule but without success. Any help would be appreciated.","['partial-derivative', 'multivariable-calculus', 'change-of-variable']"
3894072,'Guessing' local extrema of a polynomial given its roots,"To start with let's assume that $p$ is a degree $n>1$ polynomial in $x$ and has $n$ distinct roots $\alpha_1, \ldots, \alpha_n$ . Without loss of generality we can also stipulate that $0 = \alpha_1 < \cdots < \alpha_n = 1$ . This guarantees there are $n-1$ local extrema, occurring at locations $x_1 \in (\alpha_1, \alpha_2)$ , $x_2 \in (\alpha_2, \alpha_3)$ , and so on. One might like to guess at the values of $x_i$ based on knowledge of the roots, and I'm curious about any heuristics that could do better than guessing the midpoint of each interval. Of course, if $n$ is small, then there exists an explicit formula; e.g. when $p(x)=(x-\alpha_1)(x-\alpha_2)$ then $x_1 = (\alpha_1 + \alpha_2)/2$ , the midpoint between the roots. But as early as $n=3$ things get murky; if $p(x)=(x-\alpha_1)(x-\alpha_2)(x-\alpha_3)$ , then $$x_{1,2}=\frac{\alpha_1+\alpha_2+\alpha_3}{3}\pm\frac{\sqrt{(\alpha_1+\alpha_2+\alpha_3)^2-3(\alpha_1\alpha_2+\alpha_1\alpha_3+\alpha_2\alpha_3)}}{3}$$ From this we see that the two extrema are centered around the mean of the roots. For example when $\alpha_2=1/2$ , they are at $1/2 \pm \sqrt{3}/6$ , so they're ""pushed out"" toward $0$ and $1$ (as opposed to being evenly distributed at $1/4$ and $3/4$ ). Based on this idea, I can imagine a heuristic that says the extremal values will be nearer some roots and further away from others, where ""nearer"" and ""further"" should be taken in a relative sense; in the simple example above they would be nearer $\alpha_1=0$ and $\alpha_3=1$ , and further from $\alpha_2 = 1/2$ . Once there are more roots, do the roles of the roots alternate? (a ""near"" root, then a ""far root"", then a ""near"" root again?). A more concrete question to ask is: suppose I just guess that the extrema are at the midpoints between each pair of roots, call them $m_1 = (\alpha_1+\alpha_2)/2$ , $m_2=(\alpha_2+\alpha_3)/2$ , and so on. Call the error term $E=\frac{1}{n-1}\sum|m_i - x_i|^2$ . How does $E$ depend on the roots of $p$ ? Is it monotonic with $n$ in some sense? This is an idle curiosity; I'm just trying to dream up interesting Calc I problems and found something that is a little too interesting.","['maxima-minima', 'multivariable-calculus', 'calculus', 'polynomials', 'optimization']"
3894107,Does every affine open of a closed subscheme come from an affine open of the ambient scheme?,"Let $(\iota,\iota^{\#}):(Y,\mathcal{O}_Y)\to (X,\mathcal{O}_X)$ be a closed immersion of schemes. Is it true that for every affine open $V\subseteq Y$ , there exists an affine open $U\subseteq X$ with $\iota^{-1}U=V$ ? Of course there exists an open set $\tilde{U}\subseteq X$ such that $V=\iota^{-1}\tilde{U}$ , and then as $V$ is quasi-compact we may assume that $\tilde{U}$ is a finite union of affines. But can we always reduce it to just being affine?","['affine-schemes', 'algebraic-geometry', 'schemes']"
3894117,Characteristic function of the logistic distribution?,"I recently came across a question in my graduate course where we have to calculate the characteristic function for the Logistic distribution. The Logistic distribution we are working with is given by the following PDF: $$
f(x) = \frac{e^{-x}}{(1 + e^{-x})^2}. 
$$ The way that I went about doing this is the following: $$E\left[ e^{itX} \right]
= E[\cos(tX)] + iE[\sin(tX)].
$$ The $E[\sin(tX)] = 0$ . The real problem for me comes when calculating $E[\cos(tX)]$ . I tried to express $\cos$ in its exponential representation, but I didn't get too far with that. Upon plugging this integral into WolframAlpha, it says that the hypergeometric function is used for it. Any thoughts on how I can analytically compute this? I'd be happy to use the hypergeometric function, but I don't quite see the connection between that and $\text{csch}(x)$ , which is part of the result that WolframAlpha gives (and this result matches the characteristic function listed for the Logistic distribution). Edit: I would like to be able to do this problem without a computer and solely pencil and paper. This is what I mean by an analytic solution.","['fourier-transform', 'probability']"
3894156,Quotient of wreath group by commutator,"[Self studying Robinson, ex 1.6.20] Robinson asks us to prove that $G/[B,K] \cong(H/H') \times K$ (where $G=H \wr K$ , $K\neq 1$ and $B$ is the base group.) As a hint, we are told to show first that $B'\leq [B,K]$ . I can prove the hint.  First, the generators of $B'=[B,B]$ are elements of the form $g^{-1}h^{-1}gh$ .  Elements in different components of B commute, so w.l.o.g. g and h can be taken as elements of the same, single component of B, say the y component $H_y$ (where $y\in K$ ).  It is therefore enough to show that this $[g,h]\in [B,K]$ Since K is not 1 we can choose a non-trivial k in K.  Then $[g,k]$ is the element of B with $g$ as the $yk$ component and $g^{-1}$ as the $k$ component. Then $[g,h] = [g,k][h,k][(gh)^{-1},k]$ and so lies in $[B,K]$ . $\square$ But now I have no idea how to proceed to the conclusion.  I need another hint, please.","['wreath-product', 'group-theory', 'quotient-group']"
3894207,Kreyszig's definition of finite dimensional vector spae,"In his book on functional analysis Kreyszig gives the following definition: Definition. A vector space $X$ is said to be finite dimensional if there is a positive integer $n$ such that $X$ contains a linearly independent set of $n$ vectors whereas any set of $n+1$ or more vectors of $X$ is linearly dependent. $n$ is called the dimension of $X$ , written $n=\text{dim } X$ . By definition, $X=\{0\}$ is finite dimensional and $\text{dim } X=0$ . If $X$ is not finite dimensional, it is said to be infinite dimensional. If $\text{dim } X=n$ , a linearly independent $n$ -tuple of vectors of $X$ is called a basis for $X$ . He then proves the following basic theorem Theorem. Let $X$ be an $n$ dimensional vector space. Then any proper subspace $Y$ of $X$ has dimension less than $n$ . Proof. If $n=0$ , then $X=\{0\}$ and has no proper subspace. If $\text{dim } Y=0$ , then $Y=\{0\}$ , and $X \neq Y$ implies $\text{dim } X\geq 1$ . Clearly, $\text{dim } Y\leq \text{dim } X=n$ . If $\text{dim } Y$ were $n$ , then $Y$ would have a basis of $n$ elements, which would also be a basis for $X$ since $\text{dim } X=n$ , so that $X=Y$ . This shows that any linearly independent set of vectors in $Y$ must have fewer than $n$ elements, and $\text{dim } Y<n$ . Questions: Why is the last sentence in the proof necessary? It seems like a contradiction has already been reached at this point. Am not sure the statement $\text{dim } Y\leq \text{dim } X$ is so clear given his definitions. We cannot have $\text{dim } Y=m>n$ , but how can we rule out the case of $\text{dim } Y=\infty$ ? We idea to rule out $\text{dim } Y=\infty$ is the following argument: If $\text{dim } Y=\infty$ , then in particular $\text{dim } Y\neq n$ . Hence either there exist a set of $n+1$ linearly independent vectors in $Y$ or every set of $n$ vectors in $Y$ is linearly dependent. By definition of $\text{dim } X=n$ it must be that the latter holds. Now we repeat the argument to obtain that every set of $n-1$ vectors in $Y$ is linearly dependent. After $n$ steps we reach the conclusion that $Y=\{0\}$ , contradiction. Is this correct?","['hamel-basis', 'solution-verification', 'linear-algebra', 'vector-spaces']"
3894253,Is $\sin^2 x $ the same as $\sin x^2$? [duplicate],This question already has answers here : Are $\cos^2 \theta$ and $\cos \theta^2$ the same? [duplicate] (3 answers) What are the meanings of $\operatorname{trig}(x)^n$ and $\operatorname{trig}^n(x)$? (4 answers) Closed 3 years ago . I have seen $\sin^2 x$ in an equation and I don't understand how you can square $\sin$ . So does $\sin$ squared $x$ equal $\sin x$ squared? Closed it has been answered,"['notation', 'trigonometry']"
3894270,Find all continuous functions $f: \mathbb{R} \rightarrow \mathbb{N}$ and all continous functions $f: \mathbb{N} \rightarrow \mathbb{R}$.,"Find all continuous functions $f: \mathbb{R} \rightarrow \mathbb{N}$ and all continuous functions $f: \mathbb{N} \rightarrow \mathbb{R}$ . My thinking process went something like this. For the case of $f: \mathbb{R} \rightarrow \mathbb{N}$ , if I think about the function in the $xOy$ plane, if we would have any point at which the value would change from one natural number to some other natural number then at that point we would have a jump discontinuity. So every number from the domain $\mathbb{R}$ needs to be mapped to the same natural number in order to have a continuous function. Thus, we need the function to be something like $$f:\mathbb{R} \rightarrow \mathbb{N} \hspace{1cm} f(x) = n$$ for any $n \in \mathbb{N}$ . In the case of $f: \mathbb{N} \rightarrow \mathbb{R}$ again thinking about the function in the plane $xOy$ , the values of the function at two consecutive points $n$ and $n+1$ are not 'tied' together by anything, there's just empty space, so the function is nowhere continuous. Thus, there are no continuous functions $f: \mathbb{N} \rightarrow \mathbb{R}$ . I hope my reasoning is correct. But my real problem is about the writing process of this proof. Obviously I can't write on the paper all of this story that I just came up with. But how can I create a rigorous proof with what I just wrote (with definitions and all of that fluff). Thinking in terms of pictures is nice, but I have to formalize my thinking with definitions, theorems, and the like and in that regard I am lacking terribly. So how can I approach the writing of this proof?","['continuity', 'functions', 'real-analysis']"
3894390,Finding relation of two sets,"If A = {1, 2, 3} and B = {1, 2, 3, 4}, would R = {(a, b) ∈ A × B | b = a^2} be {(1,1), (2,4)} since the only time that b = a^2 is true is when (A,B) = (1,1) and (A,B) = (2,4)?","['multisets', 'relations', 'discrete-mathematics']"
3894395,Closed-form formula for a multivariate polynomial,"Counting certain walks in threshold graphs, I came upon the following independent problem.
Assume that $x_1,\dots,x_a$ are independent variables and for $k\geq 2$ let $$
P_k(x_1,\dots,x_a)=\sum_{(i_1,\dots,i_k)\in\{1,\dots,a\}^k} x_{i_1}\cdot x_{\min(i_1,i_2)}\cdot x_{\min(i_2,i_3)} \cdots x_{\min(i_{k-1},i_k)} \cdot x_{i_k}.
$$ Can you find a closed-form formula for this polynomial, i.e., find the coefficient of the term $x_{j_1}x_{j_2}\cdots x_{j_{k+1}}$ for any given $1\leq j_1\leq j_2\leq\cdots\leq j_{k+1}\leq a$ ? If this problem appeared somewhere earlier, a reference is more than welcome!","['combinatorics', 'polynomials', 'generating-functions']"
3894437,Is the set of sequences with values in a finite set separable?,"Let $\mathcal{A}$ be a finite set and consider the set of all sequences $\mathcal{A}^{\mathbb{Z}}$ on $\mathbb{Z}$ with values in $\mathcal{A}$ . This set has a cardinality of $\mathcal{A}^{\mathbb{Z}}$ which is not countable, right? (is there actually a simple explanation?) My actual question however is if there is at least a countable and dense subset (w.r.t. to the product topology when endowing each $\mathcal{A}$ with the discrete topology), i.e. is $\mathcal{A}^{\mathbb{Z}}$ separable? Sadly, I couldn't come up with anything myself. I ask this to answer another question from ergodic theory, namely if the two-sided full shift over a finite alphabet is transitive.","['elementary-set-theory', 'general-topology', 'ergodic-theory']"
3894620,Dummit and Foote 4.3.13: Find all the finite groups which have exactly two conjugacy classes. [duplicate],"This question already has an answer here : A finite group with exactly $2$ conjugacy classes isomorphic to $\mathbb{Z}_2$ (1 answer) Closed 3 years ago . Task: Find all the finite groups which have exactly two conjugacy classes. Ideas:
I have been show the class equation and the orbit stabilizer formula and I wonder if I can put them to use. My general intuition so far is such; Given $$|G|=\Sigma_{i=1}^{r}|\mathcal{O}_a|+|Z(G)|,$$ we need $\Sigma|\mathcal{O}_a|=2$ and $|Z(G)|\geq 1$ because the identity is always in the center. It could be that either each of the orbits are size 1 or a single order is size 2. My other idea is to use the orbit stabilizer formula: $|G:G_a|=|\mathcal{O}_a|=2.$ Maybe split the possibilities into $G$ abelian and $G$ not abelian. I'm new to this material so please let me know if either or these ideas are on the right track! Pointers are appreciated.","['group-theory', 'abstract-algebra', 'finite-groups']"
3894623,"What are the ""humpty"" and ""dumpty"" points?","Apologies if this isn't the right place to answer this. What are the HM points and their isogonal conjugates, also known as the Humpty and Dumpty points? I found some information via a google search, but there wasn't much on line that was a comprehensive overview. I ask, now, for a short list of properties of these points. I am asking this because I came across an old USAMO problem (2008/2) and a friend told me it was very easy by noticing that a point (F) was in fact that A-dumpty point. This made me curious, so I decided to ask for some resources here. thanks for your time.","['geometry', 'terminology']"
3894687,"Calculate $\lim _{ n\to \infty } \int _{ |x|<n }{ \int _{ |y|<n }{\sin(x^2+y^2)\,dx\,dy } } $","$$\lim _{ n\to \infty } \int _{ |x|<n }{ \int _{ |y|<n }{ \sin(x^2+y^2)\,dx\,dy } }$$ Seeing that $n\to \infty$ I deduce that yes $|x|<n$ and $|y|<n$ then $-\infty<x<\infty$ and $-\infty<y<\infty$ But I don't know if I can use these limits. Another observation I see is in the argument of the function $\sin(x^2+y^2)$ Which makes me think that I can use polar coordinates. Any suggestions on how to calculate such a limit? How could you write the limits of the integral?","['integration', 'complex-analysis', 'calculus', 'multivariable-calculus', 'algebra-precalculus']"
3894751,infinite sum of inverse binomial coefficient encountered in Bayesian treatment of the German tank problem,"in the Bayesian treatment of the German tank problem in Wikipedia here , they use: $\displaystyle \sum_{n=m}^\infty \dfrac{1}{\binom{n}{k}}=\dfrac{k}{k-1}\dfrac{1}{\binom{m-1}{k-1}}$ how can I prove this in a clever combinatorics fashion? I found this paper, see eqn. (9), which uses Gauss' hypergeometric function-- a bit beyond me. there must be some way via a recursion relation, like I found in this old paper . theorem 1 in that reference has a similar infinite sum of an inverse binomial coefficient.","['combinatorics', 'combinatorial-proofs', 'probability', 'sequences-and-series']"
3894781,Discrete and Combinatorial Mathematics Ralph P. Grimaldi (fifth edition) Problem 18 Section 11.3,"I'm working on problem #18 of section 11.3 from Ralph P. Grimaldi's textbook Discrete and Combinatorial Mathematics an Applied Introduction, fifth edition. Let $k$ be a fixed positive integer and let $G=(V,E)$ be a loop-free undirected graph, where $deg(v)
\geq k$ for all $v \in V$ . Prove that $G$ contains a path of length $k.$ I'm really having trouble understanding how to tackle the problem because the number of vertices is not given. At the end of section 11.1 states that when a graph is a multigraph it will be stated, however it is not stated in this problem that G cannot be a multigraph. If G can be a multigraph then clearly the statement is false: Start with $K_n$ the complete graph on $n$ vertices $v_1,v_2,...,v_n$ then add the edges $\{v_1,v_2\}$ , $\{v_2,v_3\},...,\{v_{n-1},v_n\},\{v_n,v_1\}$ , then every vertex has degree $n-1+2=n+1$ , however there are only $n$ vertices, thus there is no path of length $n+1$ . For example with $K_5$ (the added edges being $\color{red}{red}$ ): How do I go about working this problem?",['discrete-mathematics']
3894923,Limits of integration to find $\int_0^tB_s^2\text{d}s$,"I am currently attempting to find the variance of $\int_0^tB_s^2\text{d}s$ . In particular, I am confused about the integral of $\mathbb{E}\left(\int_0^tB_s^2\text{d}s\right)^2$ . I know that repeated applications of Fubini's theorem leads us to $\mathbb{E}\left(\int_0^tB_s^2\text{d}s\right)^2=\int_0^t\int_0^t\mathbb{E}(B_r^2B_s^2)^2\text{d}r\text{d}s$ . I also know that $\mathbb{E}(B_r^2B_s^2)^2=2s^2+rs$ . What I don't understand is why we cannot integrate this directly, i.e. $$\int_0^t\int_0^t\mathbb{E}(B_r^2B_s^2)^2\text{d}r\text{d}s=\int_0^t\int_0^t2s^2+rs\text{d}r\text{d}s=\int_0^t\frac{2t^3}3+\frac{rt^2}2\text{d}r=\frac{11t^4}{12}.$$ I have read from these links here , here and here that I should split the integral when $r<s$ and $s<r$ , and use the symmetry of the integrand to obtain $$\int_0^t\int_0^t\mathbb{E}(B_r^2B_s^2)^2\text{d}r\text{d}s=2\int_0^t\int_0^s2s^2+rs\text{d}r\text{d}s=\frac{7t^4}{12}.$$ Perhaps I have overlooked something from my elementary multivariable calculus, but I really can't think of any reason why. Help is appreciated, thanks!","['fubini-tonelli-theorems', 'stochastic-processes', 'multivariable-calculus', 'multiple-integral', 'brownian-motion']"
3895007,Solve the equation $\frac{x^3+2x}{x^2-1}=\sqrt{x^2-\frac{1}{x}}$,"$$\frac{x^3+2x}{x^2-1}=\sqrt{x^2-\frac{1}{x}}$$ $$x=?$$ I solved this but the equation $ (2x + 1) (3x ^ 4-x ^ 3 + 2x ^ 2-2x + 1) = 0 $ is formed I answer $ x =- \frac {1} {2} $ I know there is, but I couldn't do the next expression. I need help with that, or someone will solve it in a better way. I'd be happy with that.","['inequality', 'systems-of-equations', 'complex-analysis', 'trigonometry', 'problem-solving']"
3895040,How to show $E[X\mid X = x] =x$?,"I read that $E[X\mid X = x] =x$ but I don't get that when I try to prove it: \begin{align}
E[X\mid X = x] &= \sum x P(X=x|X=x) \\
&= \sum x \frac{P(X=x,X=x)}{P(X=x)} \\
&= \sum x \frac{P(X=x)}{P(X=x)} \\
&= \sum x  \\
\end{align} So what am I doing wrong?","['conditional-probability', 'conditional-expectation', 'probability-theory', 'random-variables']"
3895113,Do curves with $(\phi\circ\gamma_1)'(0)=(\phi\circ\gamma_2)'(0)$ for a chart $\phi$ also have same derivative with respect to another chart?,"As discussed in this other question , given a manifold $M$ and a point $p\in M$ , we can define its tangent vectors in $T_p M$ as the set of equivalence classes $[\gamma'(0)]$ defined so that $\gamma_1,\gamma_2\in[\gamma'(0)]$ iff $(\phi\circ\gamma_1)'(0)=(\phi\circ\gamma_2)'(0)$ for all coordinate charts $\phi:U\to\mathbb R^n$ , where $p\in U\subset M$ . In this definition, is it sufficient to ask for the curves to have same derivative with respect to one coordinate chart defined around $p$ ? In other words, given two charts $\phi,\tilde\phi:U\to\mathbb R^n$ defined on some neighbourhood of $p$ , suppose $$(\phi\circ\gamma_1)'(0)=(\phi\circ\gamma_2)'(0).$$ Does this imply that $(\tilde\phi\circ\gamma_1)'(0)=(\tilde\phi\circ\gamma_2)'(0)$ ? From the definition of a smooth manifold, I know that $\tilde\phi\circ\phi^{-1}$ is a homeomorphism between $\phi(U)$ and $\tilde\phi(U)$ . I would therefore expect that if $$\phi(\gamma_1(\epsilon))-\phi(\gamma_2(\epsilon)) = o(\epsilon),$$ then the same should hold replacing $\phi\to\tilde\phi$ . However, I'm not sure what properties of $\tilde\phi\circ\phi^{-1}$ I could use to show this.","['diffeomorphism', 'smooth-manifolds', 'manifolds', 'tangent-bundle', 'differential-geometry']"
3895311,"Approximating $E[g(Y)]$ with a Taylor series is easy, but $E[g(YX)]$ seems much tougher (potentially impossible)?","Let $X$ and $Y$ be continuous random variables with $Y \sim \mathcal{N}(\mu_Y,\sigma_Y^2)$ . I want to approximate $E[g(YX)]$ where $g(x) = e^{-x^2}$ . Specifically, my plan is to use a Taylor expansion of $Y$ at $\mu_Y$ , which I expect should be accurate when the variance $\sigma_Y^2$ is small. If we had $E[g(Y)]$ instead of $E[g(YX)]$ we could use the Taylor expansion directly to get \begin{align}
E[g(Y)] &\approx g(\mu_Y) + \frac{\sigma_Y^2}{2}g''(\mu_Y).
\end{align} But the $X$ in $E[g(YX)]$ complicates matters. I made an attempt at it and obtained the following expression: \begin{align}
E[g(YX)] & \approx E[g(\mu_Y X)] + \frac{\sigma_Y^2}{2}E[g''(\mu_Y X)].
\end{align} This seems to match up well intuitively with what we have for the simpler expecation $E[g(Y)]$ above. But I'm not certain whether its correct. Here's what I did (note that I'm still just getting to grips with conditional probability): \begin{align}
E[g(YX)] & = E[E[g(YX)|X]] \\
& = \int E[g(YX)|X=x] p_X(x) dx \\
\end{align} Since the expectation is conditioned on $x$ , I treat the $X$ in $g(XY)$ like a constant and Taylor expand with respect to $Y$ . That is, I define $h(y;x) = e^{- y^2 x^2}$ , so that $h(y;x) = g(yx)$ , and then write: \begin{align}
g(Yx) = h(Y;x) &\approx h(\mu_Y;x) + \frac{h''(\mu_Y;x)}{2} (Y-\mu_Y)^2.
\end{align} Taking the conditional expectation gives \begin{align}
E[g(YX)|X=x] = E[h(Y;X)|X=x] &\approx E\bigg[h(\mu_Y;X) + \frac{h''(\mu_Y;X)}{2} (Y-\mu_Y)^2\bigg|X=x\bigg] \\
& = h(\mu_Y;x) + \frac{h''(\mu_Y;x)}{2} \sigma_Y^2.
\end{align} Then I substitute this expansion into the integral from earlier to get \begin{align}
E[g(YX)] & \approx \int \bigg(h(\mu_Y;x) + \frac{h''(\mu_Y;x)}{2} \sigma_Y^2\bigg) p_X(x) dx \\
& = E[h(\mu_Y;X)] + \frac{\sigma_Y^2}{2}E\bigg[h''(\mu_Y;X)\bigg] \\
& = E[g(\mu_Y X)] + \frac{\sigma_Y^2}{2}E[g''(\mu_Y X)].
\end{align} I'm not sure whether what I've done is valid, and if so, if I have fully justified what I have done. If this is not valid, is it possible to correct it such that we can obtain an approximation $E[g(YX)]$ by way of taking a Taylor expansion of $Y$ ..or is what I am trying to fundamentally impossible?","['statistics', 'conditional-expectation', 'expected-value', 'probability-theory', 'random-variables']"
3895407,How to show that a Brownian bridge is a Markov process?,"If $W(t)$ is a standard Wiener process (i.e., for $t \geq 0, W(t)$ is normally distributed with expected value 0 and variance $t,$ and the increments are stationary and independent), then $$
B(t)=W(t)-\frac{t}{T} W(T)
$$ is a Brownian bridge for $t \in[0, \mathrm{T}]$ . How to show that $B$ is a Markov process ? I have 2 ways in mind: some sort of direct proof or showing that $B$ is an Ito process (then it is automatically Markov). I would like to see both.","['stochastic-processes', 'markov-process', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3895489,Is every matrix norm compatible with a vector norm?,"in $\mathbb{C}^n$ , I know every vector norm induces a matrix norm, and an induced matrix norm is compatible to its dedicated vector norm. So for every vector norm there exists a matrix norm, where the matrix norm is compatible with the vector norm. But is the other implication true too? Is there for every matrix norm a vector norm it is compatible with?","['matrices', 'normed-spaces', 'linear-algebra', 'matrix-norms']"
3895520,Derivative of a smooth function as an equivalence class,"In my differential manifolds class, the derivative of a function $f \in C^\infty (M)$ , where $M$ is a manifold, at $p \in M$ was defined as the image of the linear map $$(df)_p := C^\infty (M) \mapsto C^\infty (M)/Z_p$$ where $Z_p$ is the set of all smooth functions that have zero derivative at $p$ . In other words, the derivative is an equivalence class of $C^\infty (M)$ defined by the relation that two functions are related if their derivative vanishes at $p$ , the derivative of a function at a point is an equivalence class. I'm struggling to wrap my head around this. How can a derivative become an equivalence class? Does this hold in elementary calculus too?","['manifolds', 'derivatives', 'differential-geometry']"
3895611,"Why are ""~B"" and ""$\{x : x\notin B\}$"" not the same thing? Trying to understand basic set theory.","Why are they different: ""~B"" vs. $\{x:x \notin B\}$ ?  They both mean elements not in B. As long as that's satisfied, it should work, right? How we use them in class: $A\setminus B$ = { $x: x \in A \text{ and } x\notin B $ } $x \in A\setminus B$ iff $(x \in A) \land (x \notin B)$ Example: Let B = {2, 4, 6, 8}. the integers 1, 3, 5, 7, 9 the real numbers greater than 25 the function $f(x)=x^2+3$ the circle of radius 1 centered at the origin of the plane
the Empire State Building
my uncle Wilbur Solution explains they are different: $\{x:x \notin B\}$ contains all of the above. 1, 3, 5, 7 should be included in ""~B"", and perhaps real numbers greater than 25. But, most likely not the others. And certainly, knowing that my uncle Wilbur is not a member of the set B would contribute little to any discussion of B. Question: What's wrong with including the others? They are clearly not in B. I'm so confused right now.",['elementary-set-theory']
3895660,Having trouble finding the fault in my logic for this coding question(it's heavily math related),"So I have been doing some coding questions to practice my coding and I ran into this problem: http://www.usaco.org/index.php?page=viewproblem2&cpid=989 The general gist of the problem is that there is a person named Bessie is racing a length of $K\ge1$ meters. She starts at $0$ meters per second at the $0$ second mark, and then every second after that, she can either increase her speed by $1$ meter per second, stay at the same speed, or decrease her speed by $1$ meters per second. Her speed can not drop below zero(so if Bessie is at $0$ meters per second, she can not decrease her speed.). Bessie wants to finish the race in an integral amount of seconds, either finishing at the $K$ meter mark or past the $K$ meter mark. But Bessie also doesn't want to finish the race too quick. At the instant when she finishes $K$ meters, she wants to be traveling at a speed of no more than $X\ge1$ meters per second. Bessie wants to know the minimum speed required to finish $K$ meters, given $K$ and $X$ . The logic I use to solve this problem only works for the first 4 test cases, and I'm sure it's not because of an coding error. So my logic is as follows: Before we do anything, we first have to test whether or not a speed of $X$ meters per second can be reached, as the following solution assumes that $X$ meters per second is reachable. To do so, we first note that the quickest way to get to $X$ meters per second is to increase the speed by $1$ each second for $X$ seconds. We then note that if after increasing $X-1$ times, if the distance covered is $<K$ meters, then it is guaranteed that $X$ meters per second is reachable. But if after increasing $X-1$ times the distance covered is $\ge K$ , then we know that $X$ meters per second is unobtainable. To calculate the distance covered after $X-1$ increases, we can calculate the following sum: $$1+2+\cdots+(X-2)+(X-1)$$ which can be represented as $$\frac{X(X-1)}2$$ . We want to test whether or not $$\frac{X(X-1)}2\ge K$$ . If this inequality is false, then go to the solution under the gray line. If this inequality is true, then we know $X$ meters per second is unobtainable, and therefore we need to calculate how many increases are required to surpass $K$ meters. We will call this amount $n$ . To find the value of $n$ that will cause the distance to go over $K$ meters, we first need to find the formula of the distance covered after $n$ increases. That can be represented with the sum $$1+2+3+\cdots+n=\frac{n(n+1)}2$$ . So then we set this sum to be $<K$ , then use this inequality to maximize $n$ : $$\frac{n(n+1)}2<K\\\frac{n^2+n}2<K\\n^2+n<2K\\(n+1/2)^2-1/4<2K\\n+1/2<\sqrt{2K+1/4}\\n<\frac{\sqrt{8K+1}-1}2$$ So the value of $n$ would be: $$n=\left\lceil\frac{\sqrt{8K+1}-1}2\right\rceil$$ (without the ceiling function we would be calculating the largest amount of increases that doesn't surpass $K$ meters, instead of actually passing $K$ meters) First we want to find the maximum speed in which Bessie can go at. Let's say that this maximum speed is $m$ and the target speed(the speed we want to have at $K$ meters) be $X$ meters per second(as stated in the problem). We can find the maximum speed by allowing Bessie to increase its speed every second until it reaches $m$ , then immediately start decreasing her speed until she hits $X$ meters per second. We then know that the total distance traveled after this increase and decrease is(which I will denote as $d$ ): $$d=\underbrace{1+2+3+\cdots+m}_{\text{increasing speed}}+\underbrace{(m-1)+(m-2)+\cdots+(X+1)+X}_{\text{decreasing speed}}$$ . We can then find the formula for this sum to be: $$d=m^2-\frac{X(X-1)}2$$ . This sum has to be $\le K$ (or else we can't decrease enough in time), so we have the following inequality: $$m^2-\frac{X(X-1)}2\le K$$ . $K$ and $X$ are already given as inputs in the problem, so we just have to isolate $m$ . We get that: $$m\le \sqrt{K+\frac{X(X-1)}2}$$ (positive square root). To get the highest $m$ , we just need to take the floor of the RHS so: $$m=\left\lfloor\sqrt{K+\frac{X(X-1)}2}\right\rfloor$$ . Then if $d$ is $<K$ , we need to find out the remaining distance that we need to cover. That is easy to calculate: $K-d$ . From this we can calculate how many seconds we need to stay at $m$ meters per second(if we stay at a speed $<m$ , we can always stay at a higher speed to potentially reach $K$ meters quicker. Not too sure about this logic though). Each second we stay at $m$ meters per second adds an extra $m$ meters to our distance. So we need to divide $K-d$ by $m$ to see how many times we need to add $m$ to $d$ to reach $K$ (I will denote this as $s$ ). So we get that we need to stay $$s=\left\lceil\frac{K-d}m\right\rceil$$ seconds at $m$ meters per second to pass $K$ meters. Then we need to calculate the amount of seconds that passed for traveling $d$ meters using the method stated. To calculate this, we need to count how many terms we added together in the sum. So we need to find the length of this list: $$1,2,3\dots,m,(m-1),(m-2),\cdots,(X-1),X$$ This can be calculated with the following formula: $$2m-X$$ So finally we calculate $$2m-X+s$$ to obtain the final answer. The problem is this only works for the first 4 test cases, and this strategy only works for certain values of $K$ and $X$ past test case 4, and is really close to the actual answer for other values(yes, I downloaded the test data), so I'm actually not too sure where I went wrong here. If you guys want the code I can also put it here, but this is more of a math problem, so I decided not to put the code for now.",['algebra-precalculus']
3895722,Conditional Probability of $P(X|X^2)$ for uniform distribution.,"Assume $X \sim U[-1,1]$ is a uniform distribution. We can tell intuitively that $$P(X=k|X^2=x^2) =
  \begin{cases}
    1/2 \;\;\;\;\;\;\;\text{if} \;\; k=-x  \\
    1/2 \;\;\;\;\;\;\;\text{if} \;\; k=x
  \end{cases}$$ But I want to confirm this using regular conditional probability . However, when I try the definition as $P(X\in A|Y=y):=\lim\limits_{\{Y=y\}\in U}\frac{P(A\cap U)}{P(U)}$ , the conditional probability is $0$ since for example if trying to compute $P(X=2|X^2=4):=\lim\limits_{\epsilon  \to 0}\frac{P(X=2 \;\cap\; X^2 \in [4-\epsilon,4+\epsilon])}{P(X^2 \in [4-\epsilon,4+\epsilon])}$ , which is just $0$ because $X$ is continuous distribution. Maybe I got something wrong. Please help me compute this conditional probability formally.","['conditional-probability', 'measure-theory', 'probability']"
3895765,"Region D is bounded by below by $z=0$, and above by $x^2+y^2+z^2=4$, and on sides $x^2+y^2=1$ is required to be setup in spherical coordinate","I started Spherical coordinate literally yesterday so please bear with me, im still fairly new to the topic, so i came upon this question and graphically it should look like this to me I already have the final answer given by : $\displaystyle\int _0^{2\pi }\int _0^2\int _0^{\frac{\pi }{6}}\rho \ ^2\sin \phi \ d\phi \ d\rho \ d\theta +\ \ \int _0^{2\pi }\int _0^1\int _{\frac{\pi }{6}}^{\frac{\pi }{2}}\rho \ ^2\sin \phi \ d\phi \ d\rho \ d\theta +\ \int _0^{2\pi }\int _1^2\int _{\frac{\pi }{6}}^{\sin ^{-1}\left(\frac{1}{\rho \ }\right)}\rho \ ^2\sin \phi \ d\phi \ d\rho \ d\theta \ $ i understood the limit for $\theta$ which is $0<\theta <2\pi $ , I even understood why we took $0<\rho \ <1$ and $0<\rho \ <2$ and $1<\rho <2$ what i didn't understand at all are these $\phi $ angles limits ( $\sin ^{-1}\left(\frac{1}{\rho }\ \right), \frac{\pi }{6},\frac{\pi }{2}$ ) I want to be able to form the limits of these $\phi$ on my own but I'm thoroughly confused. Please if anybody can help explain the limits of the $\phi$ , it'd be greatly appreciated. Also side note: this particular example has already been asked once but i didnt understand the answer that was given to that question which is why im asking again. So please kindly don't mark this as duplicate, thank you.","['multivariable-calculus', 'multiple-integral']"
3895777,Fubini's formula on fibered Riemannian manifolds.,"Let $\pi:(M,g)\to(N,h)$ be a Riemannian submersion (that is, $\pi$ is a submersion satisfying $g|_{\ker(d\pi)^\perp}=\pi^*h|_{\ker(d\pi)^\perp}$ ). Each fiber of $\pi$ is a Riemannian submanifold of $M$ , given by $(F_y,g_y):=\left(\pi^{-1}(y),g|_{\pi^{-1}(y)}\right)$ . In the case that $\pi$ is a canonical projection on a Riemannian product manifold, and $f\in C^\infty_cM$ is a compactly supported smooth function, we can write Fubini's formula suggestively as $$
\int_Mf(x)d\mu_g(x)=\int_N\left(\int_{F_y}f(x)d\mu_{g_y}(x)\right)d\mu_h(y)
$$ Where $\mu_g$ is the measure on $M$ induced by $g$ , etc. Intuitively, it seems that this formula also should hold for a general Riemannian submersion $\pi$ , but it is not so clear how to show this. $M$ need not look like a Riemannian product space, even locally, since $\ker(d\pi)^\perp$ need not be integrable. As a result, $g$ does not decompose nicely in adapted coordinates, making a direct computation difficult. I suspect there's a more straightforward geometrical argument, or a simple counterexample that I'm missing.","['geometric-measure-theory', 'riemannian-geometry', 'differential-geometry']"
3895779,How many Assumptions can I make in Proof by Induction?,"Let's say I want to prove by mathematical induction that a statement involving $n$ is true for all $n\in\mathbb N$ . A classic example is proving that $$\sum_{r=1}^nr=\frac{1}{2}n(n+1)$$ The base step is trivial. This requires only $1$ assumption to prove by induction; namely that for a given $n=k$ the statement holds and then we go on to prove that this implies the statement is also true for $n=k+1$ . But what if one assumption is not enough to prove the general case? Let's say to prove the general case I needed to assume the statement held for $2$ values: $n=k-1$ and $n=k$ and then I used this to prove that the result follows for $n=k+1$ . Would that be ok? More generally, for a proof by induction must I only make one assumption or am I permitted to use many? Please explain why, whatever the answer is. Thank you for your help.","['algebra-precalculus', 'solution-verification', 'problem-solving', 'induction']"
3895869,Standardized residuals,"I am having a hard time understanding the concept of standardizing residuals and how the variance of a residual is decomposed. In a linear model, we defined residuals as: $e = y - \hat{y} = (I-H)y$ where H is the hat matrix $X(X^TX)^{-1}X^T$ and we defined standardized residuals as: $r_i = \frac{e_i}{s\sqrt{1-h_{ii}}}$ , $i = 1,...,n$ where $s^2$ is the usual estimate of $\sigma^2$ , $var(e_i) = \sigma^2h_{ii}$ , and $h_{ii}$ is the diagonal entry of H at the $i^{th}$ row and $i^{th}$ column However, I am not sure why $r_i$ and $e_i$ are functions of $h_{ii}$ rather than the whole row $h_i$ . Basically I am confused about what $h_{ii}$ stands for as opposed to row $h_{i}$ .","['statistics', 'variance', 'regression', 'linear-algebra', 'standard-error']"
3895881,Geometric Nakayama's Lemma,"This is Vakil 13.7 E, self-study. We are to show that if $X$ is a scheme and $\mathcal F$ is a finite type quasicoherent sheaf on $X$ , then if $p \in U \subset X$ is an open neighborhood of $p$ and $a_1, ... , a_n \in \mathcal F(U)$ have images generating the fiber $\mathcal F_p \otimes \kappa(p)$ , then there must be an affine open neighborhood $p \in \operatorname{Spec} A \subset U$ such that the $a_i$ each restricted to $\operatorname{Spec} A$ generate $\mathcal F(\operatorname{Spec}A)$ as an $A$ -module, and for each $q \in \operatorname{Spec} A$ , the (images of) the $a_i$ generate $\mathcal F_q$ as an $\mathcal O_{X, q}$ -module. Here is my attempt, but something feels off about it: If we assume $U$ is already an affine open $\operatorname{Spec}A$ , then we know $\mathcal F$ is locally a finite type $A$ -module $M$ on $U$ . Then the fiber at $p$ is isomorphic to $M_p/pM_p$ . Since being a finite type $A$ -module is a local property, $M_p$ is a finite type $A_p$ -module. Since $p$ is a prime ideal in $A$ , by version 8 of Nakayama's Lemma from the Stacks Project's tag 07RC, $M_p$ is generated by the images of the $a_i$ . Since $p$ was arbitrary, again by the localness of being finite type, $M$ is finitely generated by the $a_i$ . Using localness once more, $M_q$ is generated by the images of the $a_i$ for any $q \in \operatorname{Spec}A$ . Something about assuming $U$ was affine feels off, almost like I did not quite show what was asked. Also, we did not show that finite type was local in the notes thus far, only that $M$ is finite type over $A$ if and only if $M_{f_i}$ is finite type over $A_{f_i}$ , where the $f_i$ generate $A$ . I am not sure this allows me to conclude the same about localizing at a prime. It also feels like I used localness ""too much.""",['algebraic-geometry']
3895953,Poisson Bracket on non-smooth manifold,It is known that any Poisson bracket on a smooth variety $X$ corresponds to a bivector field. What is the simplest example of a non-smooth variety with a Poisson bracket that does not correspond to a bivector field?,"['algebraic-geometry', 'symplectic-geometry', 'poisson-geometry']"
3895979,Relation between the function image and function divergence,"Let $D = \{x\in \mathbb{R}~|~x\geq 0\}$ and $f:D \rightarrow \mathbb{R}$ be a continuous function with $f[D] = D$ ( $f[D]$ is the image of $D$ via $f$ ). Then, I am going to show that $f(x) \rightarrow \infty $ as $x \rightarrow \infty$ . Let us start by assuming $f(x)$ converges to $c \geq 0$ . $$
\forall \epsilon > 0 \ \exists a \in D : \forall x \in D: x > a \implies |f(x) - c | < \epsilon \implies f(x) \leq c + \epsilon
$$ Then, $a = a(\epsilon)$ . Meanwhile, by the extreme value theorem , $$
\forall a \in D \ \exists b \in D : \forall x \in [0,a] : f(x) \leq b
$$ Then, $b = b(a)$ .
Therefore, we have $$
\forall \epsilon > 0 \ \forall x \in D : f(x) \leq \max \{ c + \epsilon, b(a(\epsilon)) \} < \infty
$$ hence $f[D] \neq D$ , which is a contradiction. Is this correct?","['real-analysis', 'continuity', 'functions', 'solution-verification', 'limits']"
3896096,How do you break two absolute values inside double integral?,How would I split the two absolute values of this double integral? $\int_{-1}^1\int_{-1}^1(|x|+|y|)dxdy$ The answer key shows the integral = 2 with work: $\int_{-1}^1(|x|+|y|)dx$ split into $\int_{-1}^0((-x)-y)dx$ + $\int_{0}^1((x)-y)dx$ but why does y become -y instead of staying |y| inside the dx integral? Could I not carry |y| over and split it inside the dy integral?,"['multivariable-calculus', 'absolute-value']"
3896164,What is the number of ring isomorphisms from $\mathbb{Z}^n$ to $\mathbb{Z}^n$.,"Yesterday, I faced the problem : what is the number of ring isomorphisms from $\mathbb{Z}^2$ to $\mathbb{Z}^2$ . And I got $6$ . After then, I found that if $n=3$ , the number is $3! \times 29$ . Suddenly, I wondered what is the number of isomorphisms from $\mathbb{Z}^n$ to $\mathbb{Z}^n$ . I have tried to solve it but failed. What's more, I couldn't find any answers about that on google. Is it easy to solve? I tried to solve the problem by using recursive relation. But it looks difficult. One of my friends gave me a suggestion to find the number of $n \times n$ invertible matrices with components $1$ or $0$ . At first, we thought that it is exactly $(2^n-1)(2^n-2) \dots (2^n-2^{n-1})$ . But this equation doesn't work because the construction of the equation discards some cases. For example, $n=3$ , the equation discards $ (\begin{smallmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 1 & 0 & 1 \end{smallmatrix} )$ . The equation only works on a finite field. I found that it is a lower bound and upper bound $((n+1)!)^2 2^\frac{n(n+1)}{2}$ by a recursive relation. But I don't know until now what is the exact number of that. +
Due to Perter Franek, I solved it. The answer is $n!$ .
Idempotent elements should go idempotent elements and if $\phi$ is an isomorhpism, then $\phi(0)$ implies $0$ . The candidates of $e_i=(\delta_{ij})_j$ are each or sum of $\{e_i\}$ 's. But if some $e_i$ goes to sum of $e_i's$ , $0=\phi(e_i e_j)= \phi(e_i) \phi(e_j) \not =0$ for some $i \not = j$ .","['matrices', 'abstract-algebra', 'linear-algebra', 'combinatorics']"
3896176,"Given two points $x,y$ in a separated scheme $X$, is there always some function on an open set containing both that distinguishes them?","I'm relatively new to schemes (just started Hartshorne chapter II), and the following question seemed natural to me but I could not determine an answer. Suppose $X$ is a separated scheme and $x,y$ are two distinct points in $X$ . Is there always an open set $U\subset X$ containing $x,y$ and a function $f\in \mathcal{O}_X(U)$ so that $f(x)=0$ and $f(y)\neq 0$ (or the reverse)? If $X$ is affine, say $\operatorname{Spec} A$ , I think I understand why this has to be the case: saying $x\notin\overline{\{y\}}$ is the same thing as saying that the prime ideal corresponding to $y$ is not a subset of the prime ideal correpsonding to $x$ , so we can pick some $f$ in the prime ideal corresponding to $y$ which isn't in the prime ideal corresponding to $x$ , and this function vanishes at $y$ but not $x$ . This also handles the case where $x,y$ are in a common affine open. But I don't know if $x,y$ are always in a common affine open. Edit : Thanks to the comments (Jyrki Lahtonen and Tabes Bridges) for the advice about including the adjective separated. Progress : The problem can be reduced to $X$ integral (if $x,y$ are in different irreducible components, then we can find disjoint neighborhoods and take an indicator function, and nilpotents don't affect the evaluation of functions). Since any two open sets of an irreducible space intersect and the intersection of two affine opens in a separated scheme is again an affine open, we may assume that $X$ is the union of $\operatorname{Spec} A_1$ , containing $x$ , and $\operatorname{Spec} A_2$ , containing $y$ , with intersection $\operatorname{Spec} B$ containing neither $x$ nor $y$ . From here the global functions on $X$ are the functions in $A_1\times A_2$ which lie in the kernel of $A_1\times A_2\to B$ by subtracting the images of their components. I don't see how to get the desired function from this, though.","['algebraic-geometry', 'schemes', 'separation-axioms']"
3896183,"Showing that, if $\tan\beta=\frac{\sin\alpha-\cos\alpha}{\sin\alpha+\cos\alpha}$, then $\sqrt2\sin\beta=\sin\alpha-\cos\alpha$",If $$\tan\beta=\frac{\sin\alpha-\cos\alpha}{\sin\alpha+\cos\alpha}$$ then prove that $$\sqrt2\sin\beta=\sin\alpha-\cos\alpha$$ I have been trying to solve this exercise but I don't get it. I need help. Thanks.,"['trigonometry', 'solution-verification']"
3896187,Prove that $n\leq a_1+a_2+...+a_n \leq n+1$,"If $a_1=2,a_{n+1}=\sqrt{a_n+8}-\sqrt{a_n+3}$ . Prove that $n\leq a_1+a_2+...+a_n \leq n+1$ for every $n\ge1$ and $\lim a_n=1$ . I have showed that by squaring and inequality techniques: $a_i<\sqrt{3}$ for every $i>1$ . If $a_i>1$ then $a_{i+1}<1$ for every $i\ge 1$ I think that $\sqrt{3}$ can be improved, but I am not sure if it's useful.","['contest-math', 'limits', 'inequality', 'sequences-and-series']"
3896191,A ball around of a generic point has a positive ergodic measure,"Let $T:X \to X$ be a continuous function on the compact metric space $X$ . We say that $\mu$ is $T-$ invarint if $\mu(T^{-1}(A))=\mu(A) \hspace{0.2cm} \forall A \in \beta.$ We denote by $\mathcal{M}(X,T)$ the set of $T-$ invariant Borel probability measures. For $\mu \in \mathcal{M}(X, T),$ the set $G_{\mu}$ of $\mu-$ generic points is defined by $$
G_{\mu}:=\left\{x \in X: \frac{1}{n} \sum_{j=0}^{n-1} \delta_{T^{j} x} \rightarrow \mu \text { in the weak }^{*} \text { topology as } n \rightarrow \infty\right\}
$$ where $\delta_{y}$ denotes the probability measure whose support is the single point $y$ . Let $\mu$ be an ergodic measure. By Birkhoff ergodic theorem, $\mu(G_{\mu})=1$ . $\textit{Question:}$ Let $\mu$ be an ergodic measure and $x$ be a $\mu-$ generic point.  Why is $\mu(B(x, \frac{1}{h}))>0$ for every $h \in \mathbb{N}$ , where $B(x, \frac{1}{h})$ is the ball of radius $\frac{1}{h}$ centered at $x.$ ? I think one should consider $\chi_{B(x, 1/h)}$ in Birkhoff ergodic theorem, but I don't know how to get the sum is positive.","['measure-theory', 'ergodic-theory', 'real-analysis', 'probability-theory', 'dynamical-systems']"
3896194,"Find number of words of length $n$ which can be written using letters: $\{A,B,C,D,E\}$, but letter $A$ has to appear even number of times.","Find number of words of length $n$ which can be written using letters: $\{A,B,C,D,E\}$ , but letter $A$ has to appear even number of times. I was thinking of stars and bars method so I started this way: $x_1+x_2+x_3+x_4+x_5=n$ , where $x_i\geq 0$ and $x_1$ is an even number. I dont know if in this task $A$ can appear $0$ times since this is a question from an old test. Let's say that it can appear $0$ times. Now when I try to substitute $y_1=\frac{x_1}{2}$ and $y_i=x_i, i=2,3,4,5$ , I do not know what to do with $n$ on the right side of the equation, if this is the right approach in the first place. Since these are combinations I would have to permute everything at the end...","['combinatorics', 'discrete-mathematics']"
3896229,Solve the initial value problem $y′′+36y=e^{−t}$,"Solve the initial value problem $y′′+36y=e^{−t}$ $y(0)=y_0$ , $y'(0)=y_0'$ Suppose we know that $y(t)\to0$ as $t\to\infty$ . Determine the solution and the initial conditions. So far I have come up with $m^2+36=0$ $m=6i$ $m=-6i$ Guess solution is $y(t)=c_1 \sin(6t)+c_2\cos(6t)+e^{-t}$",['ordinary-differential-equations']
3896376,Minimum number of distances determined by $n$ points on a plane,"Set $S$ is the set of the distances between any two dots in a panel. Prove that given $202$ dots in a panel, the set will have at least $11$ members. I tried using pigeonhole principle. Every three dots can form a equilateral triangle but adding a fourth dot will definitely produce another distance. But I don't know how to proceed.","['combinatorics', 'discrete-mathematics']"
3896386,"If the plane $\frac{x}{a}+\frac{y}{b}+\frac{z}{c}=1$ intersects the axes at points $A,B,C$ then Area of Triangle $= \sqrt{b^2c^2+c^2a^2+a^2b^2}$","I'm working through this problem, Compute a surface area by integration to show that if the plane $\frac{x}{a}+\frac{y}{b}+\frac{z}{c}=1$ intersects the axes at points $A,B,C$ then Area of Triangle $= \sqrt{b^2c^2+c^2a^2+a^2b^2}$ and keep hitting a bump along the way I'm not sure how to overcome.. So far I have: Let $z=f(x,y)=c\left(1-\frac{x}{a}-\frac{y}{b}\right)$ and thus used: $$A(S)=\iint_{S}dS=\iint_{D}\sqrt{1+\left(\frac{\partial{z}}{\partial{x}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\right)^2}dA=\iint_{D}\sqrt{b^2c^2+c^2a^2+a^2b^2}dA$$ Now when considering D (the projection of $S$ onto the $xy$ plane, I have found a triangle with vertices $(0,0), (a,0), (0,b)$ . Thus, I had limits of integration as: $$0\leq x\leq a$$ $$0 \leq y \leq b\left(1-\frac{x}{a}\right)$$ My issue is that evaluating this I am obtaining $$\int_{0}^{a}\int_{0}^{b\left(1-\frac{x}{a}\right)}\sqrt{b^2c^2+c^2a^2+a^2b^2} dydx = \frac{ab}{2}\sqrt{b^2c^2+c^2a^2+a^2b^2}$$ I can see if $a=b=1$ then the desired result of $\frac{1}{2}\sqrt{b^2c^2+c^2a^2+a^2b^2}$ is obtained, but I'm not sure how to state my final result from this, or if I have made an error in the integral itself. Any help massively appreciated!","['integration', 'area', 'multivariable-calculus', 'calculus', 'multiple-integral']"
3896430,Chain Rule for multivariable functions to one variable functions,"I'm trying to understand some questions about the chain rule of multivariable and how it relates with one variable functions. Ok, so, imagine that I have two functions $f: U \subset E \rightarrow \mathbb{R}$ and $\alpha: (-\epsilon, \epsilon) \subset \mathbb{R} \rightarrow E$ , where $E$ is a normed vector space. Then, by the chain rule, the derivative of $f \circ \alpha$ in some point $a \in U$ is: $(f \circ \alpha)'(a) = Df_{\alpha(a)} \cdot D\alpha_a$ . The point that I don't get is that $(f \circ \alpha)'(a)$ is a number, but $Df_{\alpha(a)} \cdot D\alpha_a$ . is a linear application. So, I'm a little bit confused, can the number $(f \circ \alpha)'(a)$ be interpreted in some way by a linear application? Thanks!","['multivariable-calculus', 'derivatives', 'chain-rule']"
3896456,How do you write ${r \over (r+1)!}$ as a difference?,"We are asked to evaluate the following sum $$\sum_{r=1}^\infty {r\over (r+1)!}$$ I thought it best to write the expression out as a difference and use the method of differences to find the sum. However, how would one go about converting it into a difference? I lucked upon the solution, as it turns out: $$
{r\over (r+1)!} \equiv {1 \over r!} - {1\over (r+1)!}
$$ But it was simply a result of me playing about and was not rigorous at all. What would've been a sure-fire method to reach this? More generally how should one approach expression like these when looking to convert them into differences, similar to how converting some expressions like ${1\over 4r^2-1 }$ into partial fractions can ease the solution?","['algebra-precalculus', 'sequences-and-series']"
3896469,Prove $\int_{0}^{\pi/2} \int_{0}^{\pi/2} \frac{\theta\cot\theta-\varphi\cot\varphi}{\cos\theta-\cos\varphi} \text{d}\varphi\text{d}\theta = \pi\ln2$,"Month ago I encounter a nice result numerically checked by Mathematica $$
\int_{0}^{\pi/2} \int_{0}^{\pi/2} \frac{\theta\cot\theta-\varphi\cot\varphi}{\cos\theta-\cos\varphi} \mathrm{d}\varphi\mathrm{d}\theta = \pi\ln2
$$ where the integrated function is actually well-defined even around its singularity $\theta=\varphi=0$ . At my first sight, I thought it might be a trivial conclusion derived from a kind of typical integral like $$
\int_{0}^{\pi} \frac{\cos n\theta}{\cos\theta-\cos\varphi} \mathrm{d}\theta = \pi\frac{\sin n\varphi}{\sin\varphi}
$$ just using a proper series expansion. However, when I review it in detail, the result over $(0,\pi/2)$ will be awkwardly complicated. I realize this double integrals may not be done directly, or I may lack some essential insight to solve it. So I question it here for some further discussion, and thanks in advance for any suggestion.","['calculus', 'multiple-integral']"
3896604,Are periodic geodesics always dense?,"Let $M$ be a smooth closed Riemannian manifold, and let $p 
\in M$ . Given $v \in T_pM$ , let $\gamma_{p,v}$ be the geodesic emanating from $p$ with initial velocity $v$ . Define $A=\{ v \in T_pM |  \, \gamma_{p,v} \, \text{ is periodic } \}$ . Is $A$ always dense in $T_pM$ ? I am especially interested in dimension $2$ . The example I have in mind is the flat torus $\mathbb{T}=\mathbb{S}^1 \times \mathbb{S}^1$ where the velocities with rational values are dense.","['dynamical-systems', 'geodesic', 'riemannian-geometry', 'differential-geometry']"
3896628,Relation between (the $2 \pi$ in) Gauss-Bonnet and in Cauchy's differentation formula,"When I 1st saw Gauss-Bonnet, I was wondering if this $2 \pi$ had any relation with the $2 \pi$ in Cauchy's differentiation formula . Maybe a better question is to ask about the relation between Gauss-Bonnet and Cauchy's differentiation formula. I recall the $2 \pi$ ( or $\tau$ ) in Cauchy's differentiation formula is to do with homotopy with a circle in the original Cauchy's integral formula. I guess Gauss-Bonnet will have something to do with circles or spheres. So far I know only some basic version of Gauss-Bonnet and not yet its generalisations. Okay so not exactly any question yet ummm... Where does the $2 \pi$ come from in Gauss-Bonnet? Is it something to do with circles or spheres (Or $S^n$ )? Is the $2 \pi$ in Gauss-Bonnet related to the $2 \pi$ in Cauchy's differentiation formula? What's the relation of Gauss-Bonnet with Cauchy's differentiation formula?","['riemannian-geometry', 'complex-geometry', 'complex-analysis', 'pi', 'differential-geometry']"
3896728,A question related to $S^{\perp}$ and closure of span of $S$,"This question was asked in my linear algebra quiz previous year exam and I was unable to solve it. Let V be an inner ( in question it's written integer , but i think he means inner) product space and S be a subset of V. Let $\bar S$ denote the closure of S in V with respect to topology induced by the metric given by inner product. Which of the following statements are  true? A $ S $ = $ (S^{\bot})^{\bot}$ B $ \overline S$ = $(S^{\perp})^{\perp}$ C $\overline {\text{span}(S)}$ = $(S^{\bot})^{\bot}$ D $ S^{\bot} $ = $ ((S^{\bot})^{\bot})^{\bot}$ I was completely blank on how can I approach this problem although I have studied linear algebra carefully. Can you please tell on how I should approach the problem . Edit : I tried It again . I marked A ,D but answer is C,D. If A is false I don't see why D must be true. So, I think I am missing some concepts.","['inner-products', 'linear-algebra', 'functional-analysis', 'orthogonality']"
3896749,Repeated application of narcissistic function,"Over on Code Golf.SE , I've asked this challenge about the looping behaviour of a function related to narcissistic numbers . In the question, I define the function as, for a natural number $x = d_1d_2...d_n$ where $d_i$ is a single digit between $0$ and $9$ and $x$ has $n$ digits: $$f(x) = \sum_{i=1}^n d_i^n$$ In this case, a number is narcissistic if it is a fixed point of $f(x)$ i.e. $f(x) = x$ . However, the question is about repeated application of this function. For example, repeatedly applying $f$ to $x = 127$ leads to the following chain of numbers: $$127,352,160,217,352,160,217,...$$ and the triplet $352, 160, 217$ repeats ad infinitum from here. I have two hypotheses which I believe to be correct but have no idea how to prove: For all natural numbers $x > 0$ , repeated application of $f$ eventually leads to a repeating loop (counting a fixed point as a ""loop"" of 1 element). The length of this loop is always less than or equal to $14$ . For example, $x = 147$ is the lowest number that yields $14$ , with a loop of $$537059, 681069, 886898, 1626673, 1665667, 2021413, 18829, 124618, 312962, 578955, 958109, 1340652, 376761, 329340$$ My brute forcing attempts have shown that this is true for all $x < 10^5$ , and is currently (at time or writing) at around $x = 64000$ without having found a counter example to either hypothesis. This is a program which takes an input $x$ and outputs the repeated application of $f$ to $x$ until a repeated value is found, the loop of $x$ and the length of said loop, if you'd like to test out some other numbers.",['number-theory']
3896843,"What does ""$A \leq B : \Longleftrightarrow A \subseteq B$ is an order relation of $\mathcal{P}(N)$"" mean?","I have read the following in some exercise for discrete mathematics.
Let $N$ be a set and $\mathcal{P}(N)$ be its power set. Then $A \leq B : \Longleftrightarrow A \subseteq B$ is an order relation of $\mathcal{P}(N)$ . I am not really sure what is asked here. I know how to show that $\subseteq$ is a partial order on the power set but I am not sure what exactly is asked here. What order relation do I need to show and what should I do with $A \leq B$ ?","['relations', 'order-theory', 'definition', 'discrete-mathematics', 'elementary-set-theory']"
3896934,Is every geodesic-preserving diffeomorphism an isometry?,"Let $M$ be a closed $n$ -dimensional Riemannian manifold. Let $f:M \to M$ be a diffeomorphism and suppose that for every (parametrized) geodesic $\gamma$ , $f \circ \gamma$ is also a (parametrized) geodesic. Must $f$ be an isometry? An equivalent condition on $f$ is that $\nabla df=0$ where $\nabla=\nabla^{T^*M} $ $ \otimes \nabla^{f^*TM}$ is the relevant tensor product connection. Note that this equivalent assumption implies that $df$ has constant singular values, and in particular that the Jacobian $\det(df)$ is constant, hence it must be $1$ (since $f$ was assumed to be a diffeomorphism.) Thus $f$ is volume-preserving. This can certainly be false for manifolds with nonempty boundary in general, as the following example shows: Let $0<a<b$ , and set $
M=D_{a,b}=\biggl\{(x,y) \,\biggm | \, \frac{x^2}{a^2} + \frac{y^2}{b^2} \le 1 \biggr\}
$ to be the ellipse with diameters $a,b$ , endowed with the standard Euclidean metric (induced by $\mathbb{R}^2$ ). Then there exists $A \in \operatorname{SL}_2(\mathbb R) \setminus \operatorname{SO}(2)$ such that $AD_{a,b}=D_{a,b}$ , and $A$ clearly preserves geodesics (it maps straight lines to straight lines.) Indeed one can take $A$ to be of the form $$
A =A_{\theta}:=  \begin{pmatrix} a& 0 \\ 0 & b \end{pmatrix} \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos \theta \end{pmatrix}\begin{pmatrix} 1/a& 0 \\ 0 & 1/b \end{pmatrix}=
 \begin{pmatrix} \cos\theta & -\frac ab \sin\theta \\ \frac ba \sin\theta & \cos \theta \end{pmatrix}.
$$ For $M=\mathbb{S}^n $ the answer is positive, by this answer here .","['geodesic', 'symmetry', 'riemannian-geometry', 'isometry', 'differential-geometry']"
3896952,How to estimate the oscillation range of this differential equation,"Consider the system of differential equation $$\left\{\begin{aligned}\frac{\mathrm{d}x}{\mathrm{d}t} &= y\\\frac{\mathrm{d}y}{\mathrm{d}t} &= x - x^3 - y^3\end{aligned}\right.$$ with initial value $x(0) = y(0) = 1$ . Numerical simulation shows that $(x, y)$ oscillates around $(1, 0)$ , with no sign of convergence even at $t = 1000$ . I would like to estimate the range of this oscillation. My attempt: Let $t_0$ be the smallest positive $t$ at which $y(t) = 0$ . Then when $0 \leq t \leq t_0$ we have $y > 0$ , hence $x$ is increasing, so $x \geq 1$ . Then $x - x^3 < 0$ , so $y$ is decreasing, $y \leq 1$ . Note that $$\frac{\mathrm{d}x}{\mathrm{d}t} + \frac{\mathrm{d}y}{\mathrm{d}t} = x + y - x^3 - y^3 = (x + y)(1 - x^2 - xy - y^2)$$ Since $x \geq 1$ , we have $1 - x^2 \leq 0$ , so $x + y$ is decreasing, so $x + y \leq 2$ . We have $\mathrm{d}y/\mathrm{d}t \geq x - x^3 - (2-x)^3 =
 -6x^2+13x-8$ . Let $t_1$ be the smallest positive $t$ such that $x(t_1) = 7/6$ . When $0 \leq t \leq t_1$ , we have $\mathrm{d}y/\mathrm{d}t \geq -1$ , so $y \geq 1-t, x \geq -t^2/2 + t + 1$ . Hence $t_1 \leq 1 - \sqrt{6}/3$ . On the other hand $t_0 \geq 1 - \sqrt{6}/3$ , because whenever $t \leq t_0$ and $t \leq 1/2$ , we have $x \leq 1 + t \leq 3/2$ , so $\mathrm{d}y/\mathrm{d}t \geq -2$ , hence $y \geq 1 - 2t$ . If $t_0 < 1/2$ , then $y(t_0) \geq 1 - 2t > 0$ , contradicting $y(t_0) = 0$ . Hence $t_1 \leq 1 - \sqrt{6}/3 < 1/2 \leq t_0$ . I'm not sure how to proceed. Numerical experiment seems to show that $\mathrm{d}y/\mathrm{d}t \leq -1$ when $t_1 \leq t \leq t_0$ , but I don't know how to prove that. If that's the case, then since $y(t_1) \leq 2 - x(t_1) = 5/6$ , we have $t_0 \leq t_1 + 5/6 \leq 11/6 - \sqrt{6}/3$ . Since $y(t_1 + t) \leq 5/6 - t$ , we have $x(t_1 + t) \leq 1 + t_1 + 5/6t - 1/2t^2$ . Plug in $t_1 \leq 1 - \sqrt{6}/3$ and $t \leq 5/6$ to get $x(t_0) \leq x_0$ where $x_0 \approx 1.531$ . That's pretty close to what I saw in the numerical experiment. How to estimate the next $x$ value where $\mathrm{d}x/\mathrm{d}t = y = 0$ ? The long-term behavior of this equation seems to be that $x$ oscillates between these two extreme values. Also, can we say anything about the behavior when $t$ goes to negative? Mathematica thinks this is a stiff system when $t \leq -0.55$",['ordinary-differential-equations']
3896990,Is this approach to the limit solution kosher?$ \lim_{n\rightarrow\infty}\frac{(n!)^{\tfrac{1}{n}}}{n}=\frac{1}{e}$,"Here is a non-rigourous approach to a limit: $$ \lim_{n\rightarrow\infty}\frac{(n!)^{\tfrac{1}{n}}}{n}=\frac{1}{e}$$ I know the solution is correct. I'm suspicious of my methods. Rewrite the expression: $$ \lim_{n\rightarrow\infty}\left(\frac{n!}{n^n}\right)^{\frac{1}{n}}$$ I know the $\lim_{n\rightarrow\infty}\left(\frac{n!}{n^n}\right)=0$ . So bear with me. $$\frac{n!}{n^n}=\frac{n(n-1)(n-2)...[n-(n-2)][n-(n-1)]}{n^n}$$ Written this way, the numerator is a polynomial of degree $n$ . For large $n$ , the leading term will prevail. I will write that polynomial as a sum of $n^n$ and a polynomial of degree $n-1$ . $$\frac{n!}{n^n}\Rightarrow\frac{n^n+[\text{polynomial}]^{n-1}}{n^n}\Rightarrow 1+\frac{1}{n}$$ As noted in Step 2, this is not the expected result for this limit. But watch: $$ \lim_{n\rightarrow\infty}\frac{(n!)^{\tfrac{1}{n}}}{n}=\lim_{n\rightarrow\infty}\left(1+\frac{1}{n}\right)^{\tfrac{1}{n}}=\lim_{n\rightarrow\infty}\frac{1}{\left(1+\frac{1}{n}\right)^n}=\frac{1}{e}$$ This is the correct solution. But Step 2 is sketchy. Did I make another error which undid this error? Can this derivation be salvaged, or is my proof ""not even wrong""? Please be gentle with me.","['limits', 'solution-verification', 'limits-without-lhopital']"
3897022,A proof on the Jacobian and error to prove differentiability,"Let $F : \mathbb{R}^n \to \mathbb{R}^m$ . If there exists an $m \times n$ matrix $A$ and $\rho : \mathbb{R}^n \to \mathbb{R}^m$ such that $\forall h \in \mathbb{R}^n$ , $F(a+h) = F(a) + Ah + ||h||\rho(h)$ , where $\rho(h) \to 0$ as $h \to 0$ , then $F$ is differentiable at a. My attempt: Rearranging yields $\dfrac{F(a+h) - F(a) - Ah}{||h||} = \rho(h)$ , and $\rho(h) \to 0$ means the left hand goes to $0$ , and that's the definition of differentiability (if $A$ is the Jacobian). This almost seems too easy but I feel like I'm missing many details: could someone help me out in filling the missing assumptions?","['jacobian', 'multivariable-calculus', 'proof-writing', 'solution-verification']"
3897067,"If not associative, then what?","Consider a binary operation $*$ acting from a set $X$ to itself. It's useful and standard to work with operations which are associative, such that $(a*b)*c = a*(b*c)$ . What about operations which are not associative? Is there any way to characterize all different possible types of such binary operations $*$ which are not associative? Eg. Can we say that if $*$ is not associative,  then it must instead satisfy one of set of other possible properties, depending on any other additional operations that we have on our set $X$ ? If we also add some additional structure to our set $X$ so that we can add elements together and multiply by scalars, it's standard to quantify the amount that two elements of $X$ commute with each other under $*$ by calculating the commutator $[a,b] = a*b - b*a$ .
Is it ever useful to consider an 'associative commutator' $[abc] = (a*b)*c - a*(b*c)$ , for a given non-associative $*$ ? Finally, I know from Lie algebras that if $*$ anticommutes then it can be natural to consider a Jacobi identity $(a*b)*c = a*(b*c) - b*(a*c)$ Are there other natural extensions of associativity in different settings?
Why do Lie algebras use this Jacobi identity and not for example $(a*b)*c = a*(b*c) + k b*(a*c)$ Where k is a scalar?","['binary-operations', 'magma', 'abstract-algebra', 'associativity']"
3897069,Automorphism group of quaternions,"Let $\mathbb{H}$ be the algebra of quaternions. How to prove that the automorphism group $Aut(\mathbb{H})$ is isomorphic to the group $SO_3$ ? I found that all derivations on algebra $\mathbb{H}$ are inner.  Moreover, they are isomorphic to the Lie algebra $so_3$ . It remains to understand whether it is simply connected or not. But I don’t know how to show that the group $Aut(\mathbb{H})$ is not simply connected.","['automorphism-group', 'finite-groups', 'group-theory', 'lie-groups', 'quaternions']"
3897086,Why is it not sufficient to only check the third condition when verifying equality of functions?,"I have been told that two functions $f$ and $g$ are equal if and only if the domain and the subset of the cartesian product of the two functions is the same. My question is, given that a function is a special case of a relation, both $f$ and $g$ are sets, why is it not sufficient to just verify if the cartesian product is the same between the two functions?. In other words, doesn't the third condition implies that both domains are equal?",['functions']
3897109,Ultraproduct construction: are finite hyperreals just a thinly disguised version of Cauchy sequences?,"Periodically I've tried to wrap my head around nonstandard calculus and hyperreals, but I always thought I needed a lot more of a background in formal logic and/or set theory to understand what's going on with them. Last night I had a sudden flash of insight about how they work, based on the ultrapower construction, but it was so anticlimactic that I wonder if I'm missing something. I want to outline my understanding to see if I actually have the basic idea correct. The construction I'm following is per Wikipedia , and I'm heavily paraphrasing: One way to construct the hyperreals is as equivalence classes of real-valued sequences $\mathbf{x} =  \{ x_i \}_{i \in \Bbb{N}} \in \Bbb{R}^\Bbb{N}.$ The construction is not dissimilar to using Cauchy sequences of rationals to define the reals, except that our equivalence relation doesn't necessarily treat Cauchy sequences that converge to the same number as being ""equal""; rather, they are considered to be ""infinitesimally different"". Any operation or function that we can define on the real numbers - addition, multiplication, comparison, absolute value, floor function - is extended to these sequences componentwise. This gives us a ring with a lot of the same nice structure and operations as we had on $\Bbb{R}$ originally; we can identify the elements $r$ of $\Bbb{R}$ with the constant sequences $(r, r, r, r, ...)$ ; and we can put a partial order on these sequences via componentwise comparison $\leq$ : $\mathbf{a} \leq \mathbf{b}$ iff $a_i \leq b_i$ for all $i \in \Bbb{N}$ . This partial order also lets us talk about sequences like $\mathbf{ε} = (1, 0.5, 0.25, 0.125, ..., 2^{-i}, ...)$ , which are ""greater than"" the sequence $\mathbf{0} = (0, 0, 0, 0, ...)$ but ""less than"" any sequence $\mathbf{r} = (r, r, r, r, ...)$ for $r > 0$ , as being infinitesimal in some sense, since $\mathbf{0} < \mathbf{ε} < \mathbf{r}$ for any positive $r$ . However, most sequences in $\Bbb{R}^\Bbb{N}$ are not comparable in this partial order, and the ring of such sequences is also rife with zero divisors, both of which are serious deficiencies if we hope to do any calculus. If possible, we would also like two sequences $\mathbf{a}, \mathbf{b} \in \Bbb{R}^\Bbb{N}$ to be considered ""equivalent"" (i.e. different names for the same sequence) if $a_i = b_i$ for all but finitely many $i$ . We use an ultrafilter to accomplish this, and also to extend the partial order $\leq$ on sequences to a total order on equivalence classes of sequences (where the equivalence classes are given by "" $\mathbf{a} \sim \mathbf{b}$ "" iff "" $\mathbf{a} \leq \mathbf{b}$ and $\mathbf{b} \leq \mathbf{a}$ ""). The set of all equivalence classes, $\Bbb{R}^\Bbb{N} / \sim$ , is the hyperreal numbers $^*\Bbb{R}$ . Every function $f: \Bbb{R} \to \Bbb{R}$ has a nonstandard counterpart $^*f: ^*\Bbb{R} \to ^*\Bbb{R}$ given by $$^*f([x_1, x_2, x_3, ...]) := [f(x_1), f(x_2), f(x_3), ...]$$ where $[x_1, x_2, x_3, ...]$ means the equivalence class of the sequence $(x_1, x_2, x_3, ...) \in \Bbb{R}^\Bbb{N}$ . Under this interpretation, every infinitesimal hyperreal is the equivalence class of a Cauchy sequence with limit zero; every finite hyperreal is the equivalence class of a Cauchy sequence; and the ""infinite hyperreals"" are equivalence classes of unbounded sequences that either approach $+\infty$ or $-\infty$ (which gives an interesting way to rigorously interpret Big O notation ). The standard part of a finite hyperreal is just the limit of a Cauchy sequence in its equivalence class . Now that I understand how this works, it seems pretty clever to treat ""essentially different"" Cauchy sequences as different numbers ""infinitesimally close"" to a given one, since these sequences are the foundation of the limiting processes in calculus.
But it also seems surprising to me that such heavy machinery was applied to proving that nonstandard calculus is logically equivalent to regular calculus, and such pedagogical emphasis (in the sources I've perused, like Robinson's original Nonstandard Analysis ) is placed on the transfer principle.
Because now that I have this understanding, nonstandard analysis almost seems like a trivial rewording of regular calculus; and I have such confidence that it works just like regular calculus, because the rewording is so mechanical once you understand that ""standard part of a finite hyperreal"" means ""limit of a Cauchy sequence in its equivalence class"": Continuity Standard: $f: \Bbb{R} \to \Bbb{R}$ is continuous iff $\{ f(x_i) \}_{i \in \Bbb{N}}$ is Cauchy whenever $\{ (x_i) \}_{i \in \Bbb{N}}$ is Cauchy (using the standard definition of a Cauchy sequence.) Nonstandard: $^*f: ^*\Bbb{R} \to ^*\Bbb{R}$ is continuous iff $\operatorname{st}(^*f(\mathbf{x})) = f(\operatorname{st}(\mathbf{x}))$ . Differentiability Standard: $f: \Bbb{R} \to \Bbb{R}$ is differentiable iff $\{ \frac{f(x + h_i) - f(x)}{h_i} \}_{i \in \Bbb{N}}$ is a Cauchy sequence converging to a specific number $f'(x)$ independent of the sequence $\{ h_i \}_{i \in \Bbb{N}}$ , whenever $\{ h_i \}$ is Cauchy and converges to $0$ . Nonstandard: $^*f: ^*\Bbb{R} \to ^*\Bbb{R}$ is differentiable iff for any infinitesimal $\mathbf{h}$ , $\operatorname{st}\left(\frac{^*f(\mathbf{x}+\mathbf{h}) - ^*f(\mathbf{x})}{\mathbf{h}}\right) = f'(\operatorname{st}(\mathbf{x}))$ independent of the value of $\mathbf{h}$ . So I guess my questions are: Am I understanding this right and, on some level, the ""infinitesimals"" are just a thinly disguised version of Cauchy sequences with limit zero? That seems significantly more mundane than the name's suggestion of ""infinitely small numbers"" would imply. What is the gain in clarity, elegance, speed, etc. one gets by working with ""finite hyperreals"" rather than the representative Cauchy sequences, per se? That is, what are some specific instances when one gains meaningful new insight into a result from standard analysis (as opposed to a spiffy new phrasing of the result) by working in the nonstandard/hyperreal setting?","['cauchy-sequences', 'nonstandard-analysis', 'filters', 'sequences-and-series', 'infinitesimals']"
3897127,Dartboard paradox and understanding independence,"By definition, events $A$ and $B$ are independent if $$P(A \cap
    B) = P(A)\:P(B).$$ Thus if an event $A$ happens almost never $\left(P(A)=0\right),$ then $A$ is independent of all events, including itself. So, hitting the
exact centre of a dartboard (happens almost never) and hitting within its inner ring are independent events. On the other hand, it is standard to characterise
independence as follows: Two events are independent if the occurrence of one does not
affect the probability of occurrence of the other. Since hitting the exact centre of a dartboard guarantees hitting
within its inner ring (which is otherwise not guaranteed), the
two events are dependent. Isn't this a contradiction?? If yes, then do we accept that the definition of independence is not meant to fully correspond to its verbal/intuitive characterisation? Is there any semantic difference between the following two versions? $(i)$ Two events are independent if the occurrence of one does not
affect the probability of occurrence of the other. $(ii)$ Two events are independent if the occurrence of one does not
affect the occurrence of the other. ADDENDUM On further pondering, I have resolved both questions: 2. Consider this experiment: flip two fair coins, letting $H_1$ be the event that the first coin lands on Heads, and $X$ be the event that the coins land on different sides. Then $$ P\left(H_1 \cap X\right)=\frac14=P(H_1)\:P(X);$$ i.e., $H_1$ and $X$ are independent events. Knowledge that $H_1$ happens reduces the possible number of ways that $X$ can eventuate—from $2$ (outcomes HT and TH) to $1$ (outcome HT)—but does not change the probability $\left(\frac12\right)$ of $X.$ 1. The following revision characterises pairwise independence more clearly and accurately: Let $P(A)\neq0.$ Events $A$ and $B$ are independent iff knowing that $A$ happens doesn't change $B$ 's probability. In this informal characterisation, almost-never events are now excluded from being conditioned on. (What does it even mean to say that an almost-never event has happened: in what sense have I hit the exact centre of a dartboard?) It motivates the definition of pairwise independence, which does allow both events to be impossible.","['definition', 'probability', 'terminology']"
3897168,Functions of functions in calculus,"I have just finished working through a chapter on calculus and have coped OK, answering most of the questions correctly. The chapter dealt with composite functions, along with other topics. But I just cannot understand this question. I can see we are dealing with functions of functions but they are not set out in a form I can understand. Any help would be appreciated. In each of the following cases, express $f'$ , the derivative of $f$ (with respect to $x$ ) in terms of $g'$ . The number $a$ is constant: $\begin{align} i) \quad & f(x) = g(x + g(a)) \\
ii) \quad & f(x) = g(a + g(x)) \\
iii) \quad & f(x) = g(x^2) 
\end{align}$","['calculus', 'functions', 'derivatives']"
3897214,"Find a closed curve where $F(x,y,z)=(y,x,x)$ closed line integral isn't 0.","Given the function $F(x,y,z)=(y,x,x)$ , I have to show it isn't conservative and find a close curve where it isn't equal to 0. It was easy to show that $D_1F_3\neq D_3F_1$ , but I can't find an easy curve that isn't 0. I tried unit circle but it equals to zero. Watching the plot I can't figure it out. In general, I don't know how to solve this when I'm in $\mathbb{R^3}.$","['multivariable-calculus', 'line-integrals']"
3897226,"Verify Functional Limit of $\lim_{h \to 0} \frac{\int_{1-h}^{1+h} f(x) \,dx}{h}$","I want to find $$\lim_{h \to 0} \dfrac{\displaystyle\int_{1-h}^{1+h} f(x) \,dx}{h}$$ . Where $f$ is a continuous function defined on $\mathbb{R}$ What I have so far: Since $$\lim_{h \to 0} \int_{1-h}^{1+h} f(x) \,dx = 0 \text{ and } \lim_{h \to 0} h = 0$$ we can use L'Hopital's rule. $\dfrac{d}{dh}h =1$ and $\frac{d}{dh} \displaystyle\int_{1-h}^{1+h} f(x) \,dx = f(1+h)+f(1-h)$ . (Is this part right?). Then from that we apply L'Hopital's rule: $$\lim_{h \to 0} \frac{\displaystyle \int_{1-h}^{1+h} f(x) \,dx}{h} = \lim_{h \to 0} f(1+h)+f(1-h) = f(1) +f(1)=2f(1)$$ Does this look right? Thanks!","['integration', 'limits', 'real-analysis']"
3897273,"If $\alpha = \beta$, why can't the entropy-regularized Wasserstein distance equal $0$?","In optimal transportation theory, the optimal re-allocation of probability distribution $\alpha$ 's mass to another distribution $\beta$ is solved by minimizing the Wasserstein distance with respect to the transport plan. $$W (\alpha, \beta) = \min_{\pi\in \Pi(\alpha\beta)} \int c(x,y) \mathrm{d}\pi(x,y) $$ Alternatively, the relative entropy-regularized Wasserstein distance, also called Sinkhorn distance , can be used: $$W_\epsilon (\alpha, \beta) = \min_{\pi\in \Pi(\alpha\beta)} \int c(x,y) \mathrm{d}\pi(x,y) + \epsilon H(\pi \| \alpha \otimes \beta)$$ where $\epsilon$ is the regularization parameter, and relative entropy is $$H(\pi \| \alpha \otimes \beta) = \int \ln \left(\frac{\mathrm{d}\pi (x,y)}{\mathrm{d}\alpha(x)  \mathrm{d}\beta(y)  } \right) \mathrm{d}\pi (x,y) $$ Aude Genevay said that if you try the extreme case where both the source and target distributions are identical, $\alpha = \beta$ , then we would expect the entropy-regularized Wasserstein distance (Sinkhorn distance) to equal $0$ since there is nothing to move, however it is incapable of doing so . Because of this she proposes the Sinkhorn divergence instead, a normalization which does equal $0$ if $\alpha = \beta$ : $$\bar{W}_\epsilon (\alpha, \beta) = W_\epsilon (\alpha, \beta) - \frac{1}{2} [W_\epsilon (\alpha, \alpha) + W_\epsilon (\beta, \beta) ]$$ In other words, $\bar{W}_\epsilon (\alpha, \alpha) = 0$ . Questions Why (or for what levels of regularization) can't the Sinkhorn
distance, shown earlier, achieve $0$ ? Does standard optimal transport, which uses the unregularized
Wasserstein distance, also suffer from this incapability (even
though I know that the Wasserstein distance by itself, without OT,
will achieve $0$ )? and why, mathematically, does the Sinkhorn divergence?","['statistics', 'entropy', 'probability-distributions', 'optimal-transport', 'regularization']"
3897277,Is this condition equivalent to being differentiable?,"Consider a function $f : U \subset \mathbb R^n \to \mathbb R$ and the following two assertions: $f$ is differentiable at $p \in U$ , i.e., there exists a linear map $df_p : \mathbb R^n \to \mathbb R$ such that $$\lim_{v \to 0} \frac {f(p + v) - f(p) - df_p(v)} {\Vert v \Vert} = 0$$ $f$ is continuous at $p$ , and there exists a linear map $df_p : \mathbb R^n \to \mathbb R$ such that $$df_p(v) = \lim_{t \to 0} \frac {f(p + tv) - f(p)} t$$ Clearly, the first assertion implies the second. Does the converse also hold?","['derivatives', 'real-analysis']"
3897289,How to transform quadratic terms to something like $u^2+v^2$?,"I am told that by the substitution of $x = (a\cos \theta)u - (b\sin \theta) v$ and $y = (a\sin \theta)u + (b\cos \theta)v$ , we can reduce $3x^2 + 2xy + 3y^2$ to $u^2 + v^2$ . I tried to plug in the substitutions, but I did not see how the quadratic term is reduce to $u^2 + v^2$ . What I got was something complicated containing $a$ , $b$ , $\cos \theta$ and $\sin \theta$ . I am asked to reduce $3x^2 + 2xy + 3y^2 - x - 2y$ to $u^2 + v^2$ by similar substitutions. I do not know how to construct this substitution. Could you explain how do find this kind of substitutions in general?","['multivariable-calculus', 'calculus']"
3897329,How to simplify $\frac {\sin 3A - \cos 3A}{\sin A + \cos A} + 1$?,"So I started by using $\sin 3A$ and $\cos 3A$ identities and then I added the lone $1$ to the trigonometric term. (Done in the picture below) But after this I don't have any clue on how to proceed. $$=\frac{3 \sin \theta-4 \sin ^{3} \theta-\left(4 \cos ^{3} \theta-\sin \theta\right)}{\sin \theta+\cos \theta} + 1$$ $$=\frac{3 \sin \theta-4\sin^{3} \theta-4 \cos ^{3} \theta+3 \cos \theta}{\sin \theta+\cos \theta} + 1$$ $$=\frac{3 \sin \theta-4 \sin ^{3} \theta-4 \cos ^{3} \theta+3 \cos \theta+\sin \theta+\cos \theta}{\sin \theta+\cos \theta}$$ $$=\frac{4 \sin \theta-4 \sin ^{3} \theta-4 \cos ^{3} \theta+4 \cos \theta}{\sin \theta+\cos \theta}$$ $$=\frac{4 \sin \theta+\cos \theta-\sin ^{3} \theta-\cos ^{3} \theta}
{\sin \theta+\cos \theta}
$$ Original image",['trigonometry']
3897361,Find the GS of the System of DE's $\begin{cases} x' = x-3y\\ y'=3x+7y \end{cases}$,"Find the GS of the following system of DE's where the independent variable is $t$ and $x$ and $y$ are the dependent variables \begin{cases}
x' = x-3y\\
y'=3x+7y
\end{cases} I know using eigenvalues and eigenvectors or operators is one way to do this. But I wish to double check my answer using a substitution method. So my work: The second DE $y'=3x+7y$ can be rewritten as $x = \cfrac{y'}{3}-\cfrac 73y$ then $x' = \cfrac{y''}{3}-\cfrac73y'$ When we plug these values of $x$ and $x'$ into the first DE ( $x' = x -3y)$ , we get with some rearranging $\cfrac{y''}{3}-\cfrac83y'+\cfrac{16}{3}y = 0$ Which has a characteristic equation of $\cfrac{r^2}{3}-\cfrac83r+\cfrac{16}{3} = 0$ with roots $r_1=4$ and $r_2 = 4$ Then the solution for $y$ is $y$ = $C_1e^{4t}+C_2te^{4t}$ Then we back sub to solve for $x$ using $x = \cfrac{y'}{3}$$-\cfrac73y$ with the solution of y we just found. We get $x =-C_1e^{4t}-C_2te^{4t} + \cfrac{C_2}{3}e^{4t} =-C_1e^{4t}-C_2te^{4t} + C_3e^{4t}$ so the GS to the homo system is \begin{cases}
x = -C_1e^{4t}-C_2te^{4t}+C_3e^{4t}\\
y = C_1e^{4t}+C_2te^{4t}
\end{cases} If this solution is right, then I'm confident that I understand how substitution method works for solving DE systems. (Also it would boost my confidence in using the operator method to solve this as I got the same answer as this using the operator method). I'm a little thrown off on the roots being the same but I still think my methodology is still sound. I would appreciate it if someone could tell me if I've got this right cause then I know I completely understand how to solve a system of DE's. If more work is necessary to show please let me know.",['ordinary-differential-equations']
3897374,Prove alternate form of Littlewood-Offord,"I'm self-studying Bollobás' Combinatorics textbook and am stuck on a question regarding a reformulation of Littlewood-Offord. We are given a vector $x\in {\bf R}^d$ and $n$ other vectors $x_1,\ldots,x_n$ . All of these vectors have length at least $1$ . Now we consider all $2^n$ sums of the form $$\sum_{i=1}^n \epsilon_ix_i$$ where $\epsilon_i\in\{-1,1\}$ and the goal is to show that at most ${n\choose \lfloor n/2\rfloor}$ of these sums can be at a distance $\leq 1$ from $x$ . For $d=1$ , we can assume that all the $x_i$ are positive by multiplying by $-1$ if necessary. For $A\subseteq [n]$ , we let $$x_A = \sum_{i\in A} x_i - \sum_{i\notin A} x_i.$$ Let ${\cal F}$ be the set of all $A\subseteq [n]$ such that $|x_A - x| < 1$ . Let $A$ be a proper subset of $B\subseteq [n]$ and consider $|x_A - x| + |x_B-x|$ . By the triangle inequality, we have $$\eqalign{
|x_A - x| + |x_B - x|&\geq |x_B - x_A| \cr
&= \Big| \sum_{i\in B} x_i -\sum_{i\in A}x_i + \sum_{i\notin A}x_i - \sum_{i\notin B} x_i\Big| \cr
&= 2\Big|\sum_{i\in B\setminus A} x_i\Big|\cr
&\geq 2 \Big(\sum_{i\in B\setminus A}x_i - \sum_{i\in B\setminus A} x_i \Big)\cr
&= 2x_{B\setminus A}\cr
&\geq 2.
}$$ So one of $x_A$ and $x_B$ is not in ${\cal F}$ , meaning we can apply Sperner's theorem. But in arbitrary dimension, the last bit does not work, because introducing the part of the sum that is subtracted, we could actually make the vector longer. There doesn't seem to be an analogue of assuming all the vectors are ""positive"", like we did at the beginning. Instead, I'm guessing that the author intended us to use the Littlewood-Offord theorem statement found in the chapter. I'll rephrase it here: Theorem 2. Let $B$ be a normed space and let $x_1, \ldots, x_n\in B$ be vectors of norm $\geq 1$ . Consider all $2^n$ possible sums (where the null sum has value $0$ ). If we pick a subset $A$ of these sums such that every pair $x,y\in A$ is such that $|\!|x - y|\!| < 1$ , then the subset must have size $\leq{n\choose \lfloor n/2\rfloor}$ . In fact, the author says that these two statements are equivalent, and indeed they seem like they should be, but I'm stuck on the details.","['normed-spaces', 'combinatorics', 'extremal-combinatorics']"
3897488,How many $n \times n$ integer matrices with bounded entries are diagonalizable?,"I am trying to do some calculations involving the total number of diagonalizable $n \times n$ integer matrices whose coefficients are within $[-k,k]$ . Denote this number $g_n(k)$ . Using Mathematica, I have been able to compute $$g_2(5)=11690,g_2(6)=23744, g_2(7)=43290, g_2(8)=72864.$$ I computed these by brute force, ie. checking every $2 \times 2$ matrix with integer entries in $[-k,k]$ . This is quite inefficient! Is there an explicit formula for $g_2(k)$ ? What about for $g_n(k)$ ? I hoped this sequence would be on the OEIS, but it was not.","['matrices', 'linear-algebra', 'combinatorics']"
3897511,"Derivative and divergence of measures, product of measures and vector field","I am having trouble understanding the following notation: this comes from optimal transport theory and the continuity equation. $\mu: [0,1] \to P_2(\mathbb{R}^d)$ is a curve in space of probability measures having finite second moment in $\mathbb{R}^d$ and $v:[0,1] \times \mathbb{R}^d \to \mathbb{R}^d$ is a vector field. Then the continuity equation is $$
\partial_t \mu_t + \nabla\cdot (\mu_t v_t) =0
$$ in the sense of distribution. What does this mean? I do not understand taking derivative of a measure, multiplying a measure and a vector field, and taking the divergence of the result. Any help or reference?
Thank you!","['measure-theory', 'optimal-transport', 'analysis', 'distribution-theory', 'probability-theory']"
