question_id,title,body,tags
2384476,"How to prove that $f(x,y)=xy$ is differentiable at $(x_0,y_0)$?","Prove that $f(x,y)=xy$ is differentiable at $(x_0,y_0)$ using the $\epsilon$ definition. We can use the definition of differentiability: 
$$
f(x_0+\Delta x, y_0+\Delta y)-f(x_0,y_0)=f_x(x_0,y_0)\cdot\Delta x + f_y(x_0,y_0)\cdot \Delta y+\epsilon_1\cdot\Delta x+\epsilon_2\cdot\Delta y
$$
According to the above formula we get:
$$
x(y-y_0)-x_0(y-y_0)=(x-x_0)(y-y_0)=\epsilon_1\cdot(x-x_0)+\epsilon_2\cdot(y-y_0)
$$
By comparing the coefficients we have that $\epsilon_2=x-x_0$ thus:
$$
\lim_{(x,y)\to(x_0,y_0)} (x-x_0)=0
$$
But what about $\epsilon_1$?
We can conclude that $\epsilon_1(x-x_0)=0$ but we can't know if $\epsilon_1$ necessarily goes to $0$.","['multivariable-calculus', 'derivatives']"
2384486,"GCD of $n^a\,\prod\limits_{i=1}^k\,\left(n^{b_i}-n\right)$ for $n\in\mathbb{Z}$","Let $a$ be a nonnegative integer.  For a given positive integer $k$ , let $b_1,b_2,\ldots,b_k$ be odd integers greater than $1$ .  Using this result , it can be shown that, for each integer $n$ , $$f_{a;b_1,b_2,\ldots,b_k}(n):=n^a\,\prod_{i=1}^k\,\left(n^{b_i}-n\right)$$ is divisible by $$\Gamma\left(a;b_1,b_2,\ldots,b_k\right):=2^{\min\left\{k+a\,,\,2k+\sum\limits_{i=1}^k\,v_2\left(b_i-1\right)\right\}}\,\prod_{\substack{{p\in\mathbb{Z}_{>2}}\\p\text{ prime}}}\,p^{t_p}\,,$$ where $$t_p:={\min\left\{k+a\,,\,\sum\limits_{\substack{{1\leq i\leq k}\\{p-1|b_i-1}}}\,\big(1+v_p\left(b_i-1\right)\big)\right\}}
$$ for each prime $p>2$ .  Here, $v_q(m)$ denotes the largest exponent $\nu\in\mathbb{Z}_{\geq0}$ such that the $\nu$ -th power of the given prime $q$ divides the integer $m$ . Question. Fix $a$ and $\left(b_1,b_2,\ldots,b_k\right)$ .  What is the greatest common divisor $G\left(a;b_1,b_2,\ldots,b_k\right)$ of all integers of the form $f_{a;b_1,b_2,\ldots,b_k}(n)$ (i.e., $n$ runs over all the integers)?  Is it equal to $\Gamma\left(a;b_1,b_2,\ldots,b_k\right)$ ?  If not, what is a counterexample? At least, we know that $G\left(a;b_1,b_2,\ldots,b_k\right)=\Gamma\left(a;b_1,b_2,\ldots,b_k\right)$ in the following examples. Case $k=1$ : See this link . Case $a=0$ , $k=2$ , and $\left(b_1,b_2\right)=\left(13,17\right)$ :  See this link . Here is another related problem .","['number-theory', 'prime-factorization', 'polynomials', 'prime-numbers']"
2384521,"Probs. 10 (a), (b), and (c), Chap. 6, in Baby Rudin: Holder's Inequality for Integrals","Here is Prob. 10, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $p$ and $q$ be positive real numbers such that $$ \frac{1}{p} +  \frac{1}{q} =1. $$ Prove the following statements. (a) If $u \geq 0$ and $v \geq 0$ , then $$ u v \leq \frac{u^p}{p} + \frac{v^q}{q}. $$ Equality holds if and only if $u^p = v^q$ . (b) If $f \in \mathscr{R}$ , $g \in \mathscr{R}$ , $f \geq 0$ , $g \geq 0$ , and $$ \int_a^b f^p \ \mathrm{d} \alpha = 1 = \int_a^b g^q \ \mathrm{d} \alpha, $$ then $$ \int_a^b f g \ \mathrm{d} \alpha \leq 1. $$ (c) If $f$ and $g$ are complex functions in $\mathscr{R} (\alpha) $ , then $$ \left\lvert \int_a^b f g \ \mathrm{d} \alpha \right\rvert \leq \left\{ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right\}^{1/p} \left\{ \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right\}^{1/q}. $$ This is Holder's inequality. When $p = q = 2$ it is usually called the Schwarz inequality. (Note that Theorem 1.35 is a very special case of this. ) (d) Show that Holder's inequality is also true for the ""improper"" integrals described in Exercises 7 and 8. Here is Theorem 1.35 in Rudin, 3rd edition: If $a_1, \ldots, a_n$ and $b_1, \ldots, b_n$ are complex numbers, then $$ \left\lvert \sum_{j=1}^n a_j \overline{b_j} \right\rvert^2 \leq \sum_{j=1}^n \left\lvert a_j \right\rvert^2 \sum_{j=1}^n \left\lvert b_j \right\rvert^2. $$ Here are the links to my Math SE posts on Probs. 7 and 8, Chap. 6, in Baby Rudin, 3rd edition: Prob. 7 (a), Chap. 6, in Baby Rudin: If $f$ is integrable on $[c, 1]$ for every $c>0$, then $\int_0^1 f(x) \ \mathrm{d}x = $ . . . Prob. 7 (b), Chap. 6, in Baby Rudin: Example of a function such that $\lim_{c \to 0+} \int_c^1 f(x) \ \mathrm{d}x$ exists but . . . Prob. 8, Chap. 6, in Baby Rudin: The Integral Test for Convergence of Series My Attempt: As $$ \frac{1}{p}  + \frac{1}{q} = 1,$$ so $$ q+p = pq,  $$ which implies that $$ pq- p - q = 0, $$ and hence $$ pq - p - q + 1 = 1, $$ that is, $$ (p-1) (q-1) = 1. \tag{0} $$ So $$ (q-1)p = (q-1)(p-1 + 1) = (q-1)(p-1) + q - 1 = 1 + q-1 = q. \tag{1} $$ in what follows, we will be using (0) and (1). Prob. 10 (a) If $u = 0$ or $v=0$ , then $$ 0 = uv \leq \frac{u^p}{p} + \frac{v^q}{q}. $$ So let us suppose that $u$ and $v$ both are positive real numbers. For any fixed $v > 0$ , let us define a function $f_v \colon (0, +\infty) \longrightarrow \mathbb{R}$ by the formula $$ f_v(u) \colon= \frac{u^p}{p} + \frac{v^q}{q} - uv \ \mbox{ for all } u \in (0, +\infty). $$ Then $$ f_v^\prime(u) = u^{p-1} - v \ \mbox{ for all } u \in (0, +\infty). $$ So $f_v^\prime$ vanishes only at $$ u = v^{\frac{1}{p-1}} = v^{q-1}. $$ [Refer to (0) above.]  In fact, $$
f_v^\prime (u) \ \begin{cases} < 0 \ & \mbox{ if } 0 < u < v^{q-1}, \\ = 0 \ & \mbox{ if } u = v^{q-1}, \\ > 0 \ & \mbox{ if } u > v^{q-1}. \end{cases} $$ Thus $f_v$ is strictly decreasing on $\left( 0, v^{q-1} \right]$ , and, $f_v$ is strictly increasing on $\left[ v^{q-1}, +\infty \right)$ . This implies that $f_v$ has a relative minimum value at $u = v^{q-1}$ , and this relative minimum, being the unique extreme value of $f_v$ within the entire domain of this function, is in fact also the absolute minimum value ; moreover, from (1) above we also have $$
\begin{align}
 f_v \left( v^{q-1} \right) &= \frac{ \left( v^{q-1} \right)^p }{p} + \frac{v^q}{q} - v^{q-1} v \\ 
&= \frac{ v^{p(q-1)} }{p} + \frac{v^q}{q} - v^q \\ 
&= \frac{v^q}{p} + \frac{v^q}{q} - v^q \qquad [ \mbox{ using (1) above } ] \\ 
&= v^q \left( \frac{1}{p} + \frac{1}{q} - 1 \right) \\
&= 0 
\end{align}
$$ because $$ \frac{1}{p} + \frac{1}{q} =1. $$ Hence $$ f_v(u) \geq 0 \ \mbox{ for all } u \in (0, +\infty); $$ that is, $$ \frac{u^p}{p} + \frac{v^q}{q} - uv \geq 0 \mbox{ for all } u \in (0, +\infty), $$ and so $$ uv \leq \frac{u^p}{p} + \frac{v^q}{q} $$ for all positive real numbers $u$ and $v$ . Is this proof correct? Prob. 10 (b) In this proof we will be using Theorem 6.12 (a) and (b) in Baby Rudin. Here are the links to my Math SE posts on these results. Theorem 6.12 (a) in Baby Rudin: $\int_a^b \left( f_1 + f_2 \right) d \alpha=\int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha$ https://math.stackexchange.com/questions/2327134/theorem-6-12-a-in-baby-rudin-if-f-in-mathscrr-alpha-on-a-b-then-c Theorem 6.12 (b) in Baby Rudin: If $f_1 \leq f_2$ on $[a, b]$, then $\int_a^b f_1 d\alpha \leq \int_a^b f_2 d\alpha$ And, we will also be using Theorem 6.13 (a) in Baby Rudin, which is as follows: If $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a, b]$ , then $fg \in \mathscr{R}(\alpha)$ . As $f \geq 0$ and $g \geq 0$ on $[a, b]$ , so we also have $$ f g \leq \frac{f^p}{p} + \frac{g^q}{q} \tag{2} $$ on $[a, b]$ . As $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a, b]$ , so $fg \in \mathscr{R}(\alpha)$ on $[a, b]$ , by Theorem 6.13 (a) in Baby Rudin. Moreover, $$
\begin{align}
\int_a^b f g \ \mathrm{d} \alpha &\leq \int_a^b \left( \frac{f^p}{p} + \frac{g^q}{q} \right) \ \mathrm{d} \alpha \qquad \mbox{ [ using (2) and Theorem 6.12 (b) in Rudin ] } \\
&= \frac{1}{p} \int_a^b f^p \ \mathrm{d} \alpha  + \frac{1}{q} \int_a^b g^q  \ \mathrm{d} \alpha \qquad \mbox{ [ using Theorem 6.12 (a) in Rudin ] } \\
&= \frac{1}{p} \cdot 1 + \frac{1}{q} \cdot 1 \qquad \mbox{ [ using our hypothesis ] } \\
&= 1, \qquad \mbox{ [ using the condition on $p$ and $q$ in our hypothesis ] }
\end{align}
$$ Prob. 10 (c) In what follows, we will be using Theorems 6.11, 6.12 (a) , and 6.13 in Rudin. Here are the links to some Math SE posts on these
theorems. Theorems 6.11 and 6.13 from PMA Rudin Theorem 6.11 of Rudin's Principles of Mathematical Analysis And, here is the link to my Math SE post on Prob. 2, Chap. 6, in Baby Rudin: Prob. 2, Chap. 6, in Baby Rudin: If $f\geq 0$ and continuous on $[a,b]$ with $\int_a^bf(x)\ \mathrm{d}x=0$, then $f=0$ As $f$ and $g$ are complex functions in $\mathscr{R}(\alpha)$ on $[a, b]$ , so $\lvert f \rvert$ and $\lvert g \rvert$ are real functions in $\mathscr{R}$ on $[a, b]$ , by virtue of Theorem 6.13 (b) in Baby Rudin. As $\lvert f \rvert \in \mathscr{R}(\alpha)$ and $\lvert g \rvert \in \mathscr{R}(\alpha)$ on $[a, b]$ , so $\lvert f \rvert$ and $\lvert g \rvert$ are bounded functions on $[a, b]$ . Let us put $$ M \colon= 1 + \max \left\{ \ \sup \{ \ \lvert f(x) \rvert \ \colon \ a \leq x \leq b \ \},  
\sup \{ \ \lvert g(x) \rvert \ \colon \ a \leq x \leq b \ \} \ \right\}. $$ Then $M > 0$ . Let $r$ be a real number. If the mapping $y \mapsto y^r$ is continuous on $[0, +\infty)$ , then we can conclude from Theorem 6.11 in Rudin that $\lvert f \rvert^r $ and $\lvert g \rvert^r $ are also in $\mathscr{R}$ on $[a, b]$ . Thus in particular, $\lvert f \rvert^p $ and $\lvert g \rvert^q $ are in $\mathscr{R}$ on $[a, b]$ . But how to show that the mapping $y \mapsto y^r$ is continuous on $[0, +\infty)$ , especially when $r$ is irrational? I would be really grateful for a rigorous and detailed proof of this (preferably using only the tools we have at our disposal after studying Rudin up to Prob. 9, Chap. 6). We assume that $\lvert f \rvert^p $ and $\lvert g \rvert^q $ are in $\mathscr{R}$ on $[a, b]$ . If $\int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha = 0$ or $\int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha = 0$ , then we have $\lvert f \rvert^p = 0$ or $\lvert g \rvert^q = 0$ on $[a, b]$ , by virtue of Prob. 2, Chap. 6, in Baby Rudin, provided the functions $\lvert f \rvert$ and $\lvert g \rvert$ are continuous on $[a, b]$ (But what if that is not the case? How to proceed then?); this would imply that $f =0$ or $g = 0$ on $[a, b]$ , and hence $fg = 0$ on $[a, b]$ , and so the Holder's inequality would be trivially satisfied, as both sides of the inequality would be zero. So let us assume that $\int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \not= 0$ and $\int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \not= 0$ . Now let us put $$ \tilde{f} \colon= \frac{ \lvert f \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} }, \qquad \tilde{g} \colon= \frac{ \lvert g \rvert }{ \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q} } . \tag{3} $$ Then $\tilde{f}$ and $\tilde{g}$ both are in $\mathscr{R}(\alpha)$ on $[a, b]$ , because of what we have obtained (or assumed) so far together with  Theorem 6.12 (a) in Rudin. Moreover, $$ 
\begin{align}
\int_a^b \tilde{f}^p \ \mathrm{d} \alpha &= \int_a^b \left( \frac{ \lvert f \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} } \right)^p \ \mathrm{d} \alpha \qquad \mbox{ [ using (3) above ] } \\
&= \int_a^b \frac{ \lvert f \rvert^p }{ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha  }  \ \mathrm{d} \alpha \\
&= \frac{ 1 }{ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha  }   \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha  \\
& \qquad \qquad \mbox{ [ using Theorem 6.12 (a) in Rudin since $ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha$ is constant ] } \\
&= 1. 
\end{align}
$$ Similarly, we can show that $$ \int_a^b \tilde{g}^q \ \mathrm{d} \alpha = 1. $$ Thus we have $\tilde{f} \in \mathscr{R}(\alpha)$ , $\tilde{g} \in \mathscr{R}(\alpha)$ , $\tilde{f} \geq 0$ , $\tilde{g} \geq 0$ , and $$ \int_a^b \tilde{f}^p \ \mathrm{d} \alpha = 1 = \int_a^b \tilde{g}^q \ \mathrm{d} \alpha . $$ So by Prob. 10 (b) above we can conclude that $$ \int_a^b \tilde{f} \tilde{g} \ \mathrm{d} \alpha \leq 1. \tag{4}$$ Now using (3) in (4) we obtain $$ \int_a^b \left( \frac{ \lvert f \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} } \frac{ \lvert g \rvert }{ \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q} } \right) \ \mathrm{d} \alpha \leq 1, $$ which is the same as $$ \int_a^b \left( \frac{ \lvert f \rvert \ \lvert g \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}  }  \right) \ \mathrm{d} \alpha \leq 1, $$ which  simplifies to $$ \frac{1}{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}   } \int_a^b \lvert f \rvert \ \lvert g \rvert  \ \mathrm{d} \alpha \leq 1, $$ by virtue of Theorem 6.12 (a) in Rudin because both the quantities $\left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} $ and $\left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q} $ are constant.
The last inequality implies that $$  \int_a^b \lvert f \rvert \ \lvert g \rvert  \ \mathrm{d} \alpha \leq \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}. \tag{5} $$ Finally, as $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a, b]$ , so $f g \in \mathscr{R}(\alpha)$ by Theorem 6.13 (a) in Rudin;  moreover, $$ 
\begin{align}
 \left\lvert \int_a^b f g \ \mathrm{d} \alpha \right\rvert &\leq \int_a^b \lvert f g \rvert  \ \mathrm{d} \alpha  \qquad \mbox{ [ using Theorem 6.13 (b) in Rudin ] }  \\
&=  \int_a^b \lvert f \rvert \ \lvert g \rvert  \ \mathrm{d} \alpha \\
&\leq \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}. \qquad \mbox{ [ using (5) above ] }
\end{align}
$$ Is what I've done so far correct? If so, then is my presentation
rigorous and lucid enough? If the above proofs are all correct, then the following questions remain. Given a real number $r$ , especially an irrational one, how to
rigorously (and only using the tools Rudin has provided us up to this
point in the book) prove that
the map $y \mapsto y^r$ is continuous on $[0, +\infty)$ ? If both of $\lvert f \rvert$ and $\lvert g \rvert$ are discontinuous at some
point(s) of $[a, b]$ , then how to establish the Holder's inequality in
case one of $ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha$ and $ \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha$ is zero?","['real-analysis', 'integral-inequality', 'integration', 'definite-integrals', 'analysis']"
2384554,Expectation of CDF of standard normal distribution,"How to calculate $E[\{\Phi(\alpha X)\}^{K}]$ for K being positive integer, $\Phi()$ is the CDF of standard normal distribution and $X\sim N(0,1)$. I tried using integration by part stuck.",['statistics']
2384562,The free group $Z*Z$ is isomorphic to which group?,"The free group $\mathbb{Z}*\mathbb{Z}$ is basically an infinite non abelian gorup with each of its elements having infinite order except identity. I can't think of any other group which can be thought as an isomorphic group to this free group.
And can someone please define any homomorphism from $\mathbb{Z}*\mathbb{Z}$ to $\mathbb{Z}$?","['algebraic-topology', 'abstract-algebra', 'free-groups']"
2384612,How does one define the complex distribution $1/z$?,"I have read the following formula in a quantum mechanics book, supposedly attributed to Dirac
$$ \lim_{y\,\searrow\, 0} \frac 1 {x+iy} = \operatorname{p.v.} \left(\frac 1 x\right) - i \pi\delta (x)$$
Could anyone elucidate how this formula should be read (or derived)? Is it analogous to the real case, where we integrate against a test function? How should this integral be taken, as an area integral over the complex plane or as some kind of contour integral? I would be grateful for any good references on complex distribution theory from a mathematical standpoint. I learnt real distribution theory from Friedlanders book.","['quantum-mechanics', 'complex-analysis', 'distribution-theory']"
2384632,Symmetrization and isoperimetric inequality,"Let $\Omega\subseteq\mathbb{R}^n$ be a bounded open set with $C^1$ boundary $\partial\Omega$. The isoperimetric inequality states $$\frac{|\partial\Omega|}{|\Omega|^{\frac{n-1}{n}}}\geq \frac{|\partial B_1|}{|B_1|^{\frac{n-1}{n}}},$$ where $B_1$ is the unit ball centred at $0$. I want to prove a weaker version, say that given a fixed volume $V$ the ball has the lowest perimetre among all the regions $\Omega$ with volume $|\Omega|=V$. (*) My professor told me to focus on an $\Omega$ of the form
$$ \Omega=\{(x',x_n)\in\mathbb{R}^{n-1}\times\mathbb{R}:\,x'\in W\subseteq\mathbb{R}^{n-1},\,\varphi_1(x')<x_n<\varphi_2(x')\}, $$ and then to consider $$\Omega_s=\{(x',x_n)\in\mathbb{R}^{n-1}\times\mathbb{R}:\,x'\in W\subseteq\mathbb{R}^{n-1},\,-\frac{\varphi_2(x')-\varphi_1(x')}{2}<x_n<\frac{\varphi_2(x')-\varphi_1(x')}{2}\}.$$ Direct computations show that $|\Omega|=|\Omega_s|$ and $|\partial\Omega|\geq |\partial\Omega_s|$, with equality if and only if $\Omega$ is symmetric with respect to a hyperplane $\{x_n=C\}$. I understand that this provides an intuition for (*). However, I do not see how to extend this idea in order to achieve a formal proof of (*).","['proof-writing', 'differential-geometry', 'vector-analysis']"
2384639,Bootstrap confidence interval in R using 'replicate' and 'quantile',"I have around one thousand measurements (numbers). All these measurements are my observations and I have calculated a 95% confidence interval for the mean and for the variance by using the normal formulas (without software). Then, I have used the replicate function in R with around one hundred thousand simulations for my measurements (observations) and the parameter ""replace"" has been set to ""true"". After that, I used the apply function with the three parameters: my measurements as the data, 2 as the margin and mean as the function. Then to get a 95% confidence interval this way, I used the quantile function containing the variable for the apply function I used, and then 0.025 and 0.975 combined as the second parameter for the quantile function. In that way, I got almost exactly the same 95% confidens interval as calculated with the normal formula (without software). So now I wanted to do exactly the same thing for the variance, i.e. use replicate, apply and quantile to get a 95% confidence interval for the variance. So I just changed the third parameter ""mean"" to ""var"" in the apply function. I then noticed that the outputted 95% confidence interval for the variance (from the quantile function) is a little bit different (it is wider) than the one I calculated by the normal formula without software. So my question is: Did I use the replicate, apply and quantile function correctly for the variance confidence interval? I know I did use the functions right for the mean, since I got almost exactly the same result there as calculated by normal formula. If I did use the functions correctly for the variance, why is the confidence interval a little bit different? Is it because as sample size increases, there might be more observations far away from the mean resulting in a bigger variance?","['statistics', 'math-software', 'confidence-interval']"
2384642,"Given $y=e^{rx}$, for which values of $r$ does $y'' - 2y' - 3y = 0$?","I am trying to solve this problem but I am not able to do it. $y=e^{rx}$ : For what values of $r$ does $y'' - 2y' - 3y = 0$.
I tried to differentiate $y$ with respect to $x$, but I get something like : $e^{rx}\cdot \frac{d}{dx}(rx)$, which does not seem to help me a lot. How I am supposed to do this ?","['derivatives', 'ordinary-differential-equations', 'calculus']"
2384689,Is it true that a second-countable topological space which has an universal cover must have countable $\pi_1$?,"Question: Is there a second-countable, connected, locally path-connected, semi-locally simply connected (and ""perhaps"" Hausdorff) topological space $X$ such that $\#\pi_1(X)>\aleph_0$? Uninteresting story of the question: I was doing an exercise which asked to prove that a Hausdorff, locally compact, second-countable, connected topological manifold must have countable fundamental group.
My idea was observing that, by Poincaré-Volterra theorem, its universal cover is second-countable, so its fibres are countable. However, the exercise hinted towards a more direct, and somehow visual, proof, which I decided to follow. Jokes on me, the work I did implies (in my opinion) way too much. In fact, just by ""following the instructions"", I ended up using only N2 and the ""triple connection"" hypothesis that guarantees the existence of the universal covering. Not even T2, actually. I wanted to be sure I was wrong, before diving back in, though.","['algebraic-topology', 'general-topology', 'examples-counterexamples']"
2384714,Number of functions $f : A \to A$ with $f(f(x)) = x$,"Let $A$ be set such that $n(A)=5$. How many functions can we define on $A$ with the property $(f\circ f)(x)=x$ ?
I think the identity function works but what about others? Should $f$ have an inverse?  I think permutations may be involved, but I am not sure how to progress.","['combinatorics', 'functions']"
2384719,Please prove $\frac{1 + \sin\theta - \cos\theta}{1 + \sin\theta + \cos\theta} = \tan \left(\frac \theta 2\right)$,"Prove that $\dfrac{1 + \sin\theta - \cos\theta}{1 + \sin\theta + \cos\theta} = \tan\left(\dfrac{\theta}{2}\right)$ Also it is a question of S.L. Loney's Plane Trignonometry What I've tried by now: \begin{align}
& =\frac{1+\sin\theta-\sin(90-\theta)}{1+\cos(90-\theta)+\cos\theta} \\[10pt]
& =\frac{1+2\cos45^\circ \sin(\theta-45^\circ)}{1+2\cos45^\circ \cos(45-\theta)} \end{align} Cause I do know \begin{align} & \sin c + \sin d = 2\sin\left(\frac{c+d}{2}\right)\cos\left(\frac{c-d}{2}\right) \\[10pt] \text{and } & \cos c + \cos d = 2\cos\left(\frac{c+d}{2}\right)\sin\left(\frac{c-d}{2}\right)
\end{align} I can't think of what to do next..",['trigonometry']
2384723,Can $\sin(xy)$ be written in terms of trigonometric functions of only $x$ or $y$? [duplicate],"This question already has answers here : Is there a formula for $\sin(xy)$ (5 answers) Closed 6 years ago . Can $\sin(xy)$ be written in terms of trigonometric functions of only $x$ or $y$? I am tempted to say yes, because the double- and half-angle formulae exist, and these would be special cases of $\sin(xy)$. I first looked at the Taylor series, $$\sin(xy) = \sum_{n=0}^\infty\frac{(-1)^nx^{2n+1}y^{2n+1}}{(2n+1)!} = xy-\frac{x^3y^3}{6}+\frac{x^5y^5}{120}-\cdots$$ but as far as I know, neither $x$ nor $y$ can come out of the sum since they are not constants.",['trigonometry']
2384782,"Hints to prove $\tan^2{\theta}=\tan{A}\tan{B}$, given $\frac{\sin{(\theta + A)}}{\sin{(\theta + B)}} = \sqrt{\frac{\sin{2A}}{\sin{2B}}}$","I need some hints on solving this trigonometry problem. Problem If $\dfrac{\sin{(\theta + A)}}{\sin{(\theta + B)}} = \sqrt{\dfrac{\sin{2A}}{\sin{2B}}}$ , then prove that $\tan^2{\theta}=\tan{A}\tan{B}$ . I tried to expand the left hand side of the equation, but no clue what to do next. I also tried to use $\sin{2\alpha} = \dfrac{2\tan{\alpha}}{1 + \tan^2{\alpha}}$ for the right hand side of the equation with no result. I appreciate for any help. Thank you.",['trigonometry']
2384799,Let $f:X\to Y$ be a surjective map such that $\text{int}(f(A))\subset f(\text{int}(A))$ for any $A\in\mathcal{P}(X)$. Show that $f$ is continuous.,"I'm working on an exercise, stated as follows: Let $X$ and $Y$ be topological spaces. Let $f:X\to Y$ be a surjective function satisfying the condition that $\text{int}\big(f(A)\big)\subset f\big(\text{int}(A)\big)$ for any subset $A\subseteq X$. Show that $f$ is continuous. [Here $\text{int}(A)$ means the topological interior of $A$, in case there is any confusion.] I thought I had solved this, but it turns out I was implicitly assuming that $f$ was injective as well. Here is my incorrect ""proof"": Let $U\subseteq Y$ be open. Then
$$
U=\text{int}(U)=\text{int}\Big(f\big(f^{-1}(U)\big)\Big)\subset f\Big(\text{int}\big(f^{-1}(U)\big)\Big).
$$
Thus $f^{-1}(U)\subset\text{int}\big(f^{-1}(U)\big)$. (Here is the issue. We only have $f^{-1}(f(A))\supset A$ for every set $A$, with equality when $f$ injective.) Thus $\text{int}\big(f^{-1}(U)\big)=f^{-1}(U)$ and hence $f^{-1}(U)$ is open. Thus $f$ is continuous. I've also tried other approaches, but all of them seem to require me to have $f$ injective. I'm starting to think the statement may not even be true. Does anyone have a proof or counterexample of this? Thanks","['continuity', 'general-topology']"
2384807,How many people should be on a jury to be representative?,"Assuming a jury is like a sample of a population. Assume the following: The population is a million people.
There are X percent who think the defendant is guilty.
Choose N random people to be a jury. Given X, how many jurors (what sample size) would you need to give 95% accuracy of the juror decision being the same majority decision as the entire population. I would guess that if X is closer to 50% like 50.1% then you would need a very large jury to make sure you got the accurate result. Whereas if X is closer to 0% or 100% you might need just one juror. So assume the everyone in the population chooses guilty or innocent at random 50/50. (So they might do this and 60% have randomly chosen guilty.) On average how many jurors would you need for them to give the same result as a census 95% of the time? [I added this assumption because we need to know how often the population is split and how often the population is in agreement which will affect the results. But thinking about it this condition would mean the jury would only agree with the population 50% of the time! Right??? So perhaps the condition should be "" Provided above 60% of the population is in agreement the jury should agree with the population 95% of the time. How many jurors are needed in the worst case scenario? "" ]. i.e. is there some way to calculate the best number of people for a jury such that the population as a whole will be happy most (95%) of the time? And justice is ""seen"" to be done?",['statistics']
2384849,How fast can one move around an ellipse with bounded acceleration?,"Given a smooth closed planar curve $\Gamma$, I'm looking for its periodic parametrization $\phi : \mathbb{R}\to\Gamma$ such that the second derivative $\phi''$ is bounded by $1$ in the norm: $|\phi''|\le 1$ the period $T$ of the parameterization is as small as possible. The ""real world"" motivation is auto racing: set the best lap time $T$ given that the car's acceleration is limited. Question : what is the smallest $T$ for an ellipse with semiaxes $a,b$? (Auto racing tracks often look like an ellipse , although they are not really.) Special cases The special case $a=b$, a circle, is easy. Assume it centered at the origin. Then $|\phi|^2\equiv a^2$. Differentiate twice: $|\phi'|^2+\phi\cdot \phi'' =0$. This yields $|\phi'|\le \sqrt{|\phi||\phi''|} \le \sqrt{a}$, hence $$T\ge 2\pi a/\sqrt{a} = 2\pi\sqrt{a}$$ 
This lower bound is attained by $\phi(t) = a(\cos (t/\sqrt{a}), \sin (t/\sqrt{a}))$. The degenerate ellipse, $b=0$, is also easy to handle: the car has to stop at the endpoints of the interval $[-a,a]$, and then have full acceleration up to the midpoint $0$. This yields $T=4\sqrt{2a}$, because one loop consists of $4$ segments of constant acceleration or deceleration. In general, the smallest period $T(a,b)$ scales like $T(\lambda a,\lambda b)=\sqrt{\lambda}T(a,b)$ because the function $\lambda \phi(t/\sqrt{\lambda})$ has the same top acceleration as $\phi$.","['optimization', 'conic-sections', 'differential-geometry', 'plane-curves']"
2384855,PDE approaches to calculate the Earth Movers Distance?,"The Earth Movers Distance is a concept in probability theory and information theory. In short one can say it is a distance measure between two probability densities $p_1(x), p_2(x)$ so that probability is moved from one density to another at the cost $|x_2-x_1|d(x_1,x_2)$ if $d(x_1,x_2)\in\mathbb R^2\to \mathbb R^1$ is the total probability density moved from point $x_1$ to $x_2$. ( I am writing from memory and intuition here so please correct me if I am wrong: ) $$d = \min_{d}\left\{\int_{\mathbb R^2} |x_1-x_2| d(x_1,x_2) \right\} \, s.t. \, \cases{f_1(x_1) = \int d(x_1,x)dx\\f_2(x_2)=\int d(x,x_2)dx\\d(x_1,x_2)\geq 0\,\,\, \forall (x_1,x_2)}$$ So we are to find the function $d$ which minimize the above expression and also fulfills the constraints. If we are used to working with optimization we know that we can rewrite the first equality:
$$0 = \int d(x_1,x)dx - f_1(x_1)$$
And turn it into a term in our optimization: $$+\left\|\int d(x_1,x)dx - f_1(x_1)\right\|$$
For some suitable choice of norm. I know of some discrete optimization methods and heuristics (for example the Hungarian algorithm) are being used to solve this problem, but somehow I feel it should be possible to solve with differential equations. Do you know of any methods which use differential equations and / or some energy minimization to find the Earth Movers Distance?","['reference-request', 'probability-theory', 'information-theory', 'soft-question', 'ordinary-differential-equations']"
2384906,Adjacency matrices of multigraphs,"Should the entry (adjacency matrix) for row = g, column = g be ""2"" instead of 1? Also, should the entry (incidence matrix) for row = g, column = e11 be ""2"" instead of 1? I have one lecturer saying that both entries should be ""1"" and another lecturer saying these two entries should have ""2"". This looks like it should be obvious; but, I've been given conflicting information about these entries and want to check if there is a ""right"" answer.","['matrices', 'graph-theory', 'adjacency-matrix', 'discrete-mathematics']"
2384938,Why does Hoeffding's Lemma do taylor expansion in the exponent?,"While reading through the proof, I realized that one part of the proof I took for granted was that Hoeffding's lemma expanded on the exponent portion of the inequality after the reparametrization of the bounds: Note, I will use different variable names. $a$ is the $\theta$, and $x$ is the $u$: https://en.wikipedia.org/wiki/Hoeffding%27s_lemma They took the function $e^{-ax}(1-a+ae^x)$, and redefined it in terms of $e^{f(x)}$ where $f(x) = -ax + log(1-a+ae^x)$, and did a taylor expansion on the top, around $x = 0$, to get $e^{1/2(a(1-a))x^2}$, which translated to the final result after some trivial observations. However, I don't understand what prompted the expansion on $f(x)$, rather than the entire function itself. Can someone elaborate on this?","['machine-learning', 'statistics', 'taylor-expansion', 'convergence-divergence']"
2384941,How to prove $\lim_{x\to 1^+} \frac{x}{x-1} = +\infty$ with the limit definition?,"By using the definition of limit ONLY, prove that
  $$
\lim_{x\to 1^+} \dfrac{x}{x-1} = +\infty
$$
  (Note that you need to use the definition of one-sided limit from the right and you are NOT allowed to use any algebra or any theorem for limit.) I keep getting mixed up with the definitions when trying to answer this, when attempting the question i've started with For all $M\in\mathbb{R}$ we need to find a $\delta>0$ such that $\dfrac{1}{1-x}>M$ for all $x\in\mathbb{R}$ and $0<x<\delta$ Not too sure what to do from here or if this is even correct.","['calculus', 'limits']"
2384946,"Is $[0,1]$ the union of $2^{\aleph_0}$ perfect sets which are pairwise disjoint?","I need represent [0,1] as the union of $2^{\aleph_0}$ perfect sets which are pairwise disjoint. I have thought about removing open disjoint sets but the number of open sets I get is countable. Thanks.","['descriptive-set-theory', 'general-topology', 'metric-spaces']"
2384956,Jacobian of Transformation in the Complex Plane,"Let $$f(z) = \sum_{n=0}^\infty c_nz^n$$ be analytic in the disc $\mathbb{D} \ \{ z \in \mathbb{C} | |z| < 1 \}$. Assume $f$ maps $\mathbb{D}$ one-to-one onto a domain $G$ having area $A$. Prove $$A = \pi \sum_{n=1}^\infty n |c_n|^2$$ Looking at the solution - I lack some fundamental understanding. Why is the Jacobian of the transformation $|f'(z)|^2$, giving us that $A = \int_{\mathbb{D}}|f'(z)|^2 dx dy$. Can someone help me see this by walking through the definition?","['jacobian', 'complex-analysis']"
2384967,"Is the intersection of two orthogonal planes a line, or the zero vector?",I am struggling to grasp a relatively simple concept in linear algebra: I know that the intersection between two orthogonal subspaces is the zero vector. But I also know that the intersection between two orthogonal planes is a line. A plane is a subspace. But....a line is not the zero vector. Where did I go wrong in my logic?,"['linear-algebra', 'vector-spaces']"
2384987,Expectation of a die roll summed,"I know this problem involves conditional probability, but I'm confused as to how to tackle it. Assume a die is rolled over and over, where the total is summed. If the die's roll is $\geq 3$ the game stops and the summed total is read out. What is the expectation of the total? What is the expected number of times the die was rolled?","['expectation', 'conditional-expectation', 'probability']"
2384988,Real and complex nilpotent Lie groups,"It is known that if $G$ is an abelian real Lie group then $\exp :(\mathfrak  g,+)\rightarrow G$ is a surjective homomorphism between two abelian groups. Thus, $G\cong \mathfrak g/\ker(\exp)\cong \mathbb R^n/\Gamma\cong \mathbb R^k\times(\mathbb S^1)^{n-k}$ where $\Gamma$ is a discrete subgroup of $\mathbb R^n$. Now the situation for the complex case is slightly different;  Let $G$ be an abelian complex Lie group, then $G\cong \mathbb C^m\times (\mathbb C^*)^n\times C$ where $C$ is a Toroidal group (i.e a complex connected Lie group that contains a connected normal subgroup $H$ which is a complexified torus (i.e., isomorphic to $(\mathbb C^*)^t$ such that $C/H$ is compact). My question is about mimicking this in the nilpotent case.
So let $G$ be a connected real nilpotent Lie group, then $\exp:\mathfrak g\rightarrow G$ is a surjective smooth map between two manifolds. How to prove that $G$ is diffeomorphic to $\mathbb R^m\times (\mathbb S^1)^n$? Is this also true for the connected complex nilpotent Lie groups. I mean is $G$  holomorphic equivalent to $\mathbb C^m\times (\mathbb C^*)^n\times C$?","['complex-geometry', 'differential-geometry', 'lie-algebras', 'lie-groups']"
2384998,How can a set with one element be equal to a set with two elements,"I'm looking into nonstandard analysis, and am in a chapter which introduces the whole load of basic terms they'll use. One of this is a proof for ordered pairs (Kuratowski definition) by induction. The ordered pairs are defined like this: \begin{equation}
\begin{aligned}
(a)_k    :&=\{a\}  \\
(a,b)_{k} :&= \{\{a\},\{a,b\}\} \\
(a_1,\,...\,,a_n)_k :&= ((a_1,\,...\,,a_{n-1}),a_n)
\end{aligned}
\end{equation} The theorem to show is :
$(a_1,\,...\,,a_n) = (b_1,\,...\,,b_n) \Rightarrow a_k = b_k \text{  for k = 1, ... , n}$ They do it by induction: Case n = 1 is trivial, and case n = 2 (the part I don't understand) goes like this: It is $(a_1 , a_2) = (b_1,b_2) $. This is per definition equal to $\{\{a_1\},\{a_1,a_2\}\} = \{\{b_1\},\{b_1,b_2\}\} $. Now the following cases are possible: $
\begin{align}
 \{a_1\} &= \{b_1\} &\text{and}&\quad\quad \{a_1,a_2\} &= \{b_1,b_2\}  ,\\
\{a_1\} &= \{b_1,b_2\} &\text{and}& \quad\quad\{b_1\} &= \{a_1,a_2\}
\end{align}
$ First case seems simple enough, but I don't understand how a set with one element can be equal to a set with two elements. Even worse, they say for both cases follows $ a_1 = b_1 $ and $ a_2 = b_2$ ... but why?",['elementary-set-theory']
2385021,Probability of visiting all but one state [duplicate],"This question already has answers here : Random walk on $n$-cycle (4 answers) Closed 6 years ago . Suppose I have a circular markov chain. At each state, you are equiprobable to transition to the state immediately to your left or right. I'm interested in the probability of visiting the jth state last?  That is to say, what is the probability of reaching all the other states at least once before the jth state?","['markov-chains', 'probability']"
2385048,A continuous nowhere differentiable function,"I am self-reviewing some basic analysis (undergraduate level), and I stumped upon this question from Abbott's book Understanding Analysis : Let $g$ be a function such that $$g(x) = \sum_{n=0}^{\infty}\frac{1}{2^n} h(2^n x)$$ where $$h(x) = |x|$$ is defined on the interval $[-1,1]$ and we additionally require that $h(x) = h(x+2)$ . The two-part question is the following a). Show that $g(x)$ attains maximum on the interval $[0,2]$ , say $M$ , and find $M$ . b). Define the set $D = \{x \in [0, 2] : g(x) = M\}$ . Is the set $D$ finite, countable, or uncountable? Since $g(x)$ is continuous on $\mathbb{R}$ , it is obvious that $g(x)$ will attain maximum and minimum on $[0,2]$ , because the closed interval is compact. But I cannot figure out the rest of the problem. Any thought is appreciated.","['continuity', 'real-analysis', 'sequences-and-series']"
2385053,Book on quasiconformal mappings?,"I am looking an introductory book on ""quasiconformal mappings"" for self-study. Also I would like to know about motivation and history behind this concept (I am a beginner of this subject). I really appreciate any help you can provide.","['quasiconformal-maps', 'self-learning', 'reference-request', 'complex-analysis', 'book-recommendation']"
2385058,The derivative of the logistic function,"The logistic function is $\frac{1}{1+e^{-x}}$ , and its derivative is $f(x)*(1-f(x))$ . In the following page on Wikipedia , it shows the following equation: $$f(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{1+e^x}$$ which means $$f'(x) = e^x (1+e^x) - e^x \frac{e^x}{(1+e^x)^2} = \frac{e^x}{(1+e^x)^2}$$ I understand it so far, which uses the quotient rule $$\left(\frac{g(x)}{h(x)}\right)' = \frac{g'(x)h(x) - g(x)h'(x)}{h(x)^2}.$$ However, why is it then transformed into $f(x) * (1-f(x))$ ?","['derivatives', 'logistic-regression']"
2385078,How does the criteria $\nabla F(x_0) = \lambda \nabla G(x_0)$ guarantee that $x_0$ is a maximum?,"I'm attempting to figure out why Lagrange Multipliers work, and I have the following setup. Suppose we wish to maximize $F : \mathbb{R}^n\to\mathbb{R}$ with respect to the constraint $G = c$ where $G:\mathbb{R}^n\to\mathbb{R}$. Let $L$ be the level surface $G = c$. Suppose $x_0\in L$, and $u$ is tangent to $L$ at $x_0$. I want to show that if $\nabla F\cdot u > 0$ then we can move along $L$ and find a point $x_0'\in L$ with $F(x_0')>F(x_0)$, hence requiring that $\nabla F\cdot u = 0$ for $x_0$ to be a maximum. My problem is that the condition $f'(x_0)>0$ isn't enough to guarantee that there is an interval $[x_0,\ x_0+\delta]$ on which $f$ is increasing (for example, take $f(x) = x^2\sin^2\left(\frac1x\right) + \frac12 x$ for nonzero $x$, and $f(0)=0$). So essentially, if we look at $f(t) = F(x_0+tu)$, the statement $f'(0) > 0$ is not enough to assume that $f$ is strictly increasing on any neighbourhood of zero, and so $F$ isn't increasing in any neighbourhood of $x_0$ meaning we can't necessarily guarantee that it's increasing along $L$ in the direction of $u$. Hence, $\nabla F\cdot u>0$ isn't enough to guarantee that we can find another point $x_0'\in L$ near $x_0$ so that $F(x_0')>F(x_0)$. If my logic isn't flawed, then how come Lagrange multipliers work? How do we know that $\nabla F\cdot u = 0$ for all tangent vectors $u$ to $L$ at $x_0$ is enough to guarantee that $x_0$ maximizes $F$ on $L$? Is it like, $\nabla F\cdot u>0$ isn't enough to guarantee that $x_0$ isn't a maximum, but $\nabla F\cdot u = 0$ is ?","['multivariable-calculus', 'real-analysis', 'lagrange-multiplier']"
2385120,Area of a circle as it changes to an ellipse when viewing at different angles,"Sorry for the long paragraph, I'm not too good at describing things:
Firstly, to explain the situation, I need to know how the area of a circle can change as you view it from different angles. For example, if you were viewing a circle at a 0 degree angle, it would look like a circle and it's area would be $\pi r^2$. However, as the angle of observation increases or decreases, one side of the circle will ""collapse"" inwards, and become smaller. It would create an ellipse, where the variable 'a' (longest distance from centre to side) stays the same, and where variable 'b' (the smallest distance from centre to side) decreases. I want to know the best way to find the area, based on the angle theta. For clarification, theta refers to the angle from a line perpendicular to the circle. The best way to approach the issue seemed to be the diagram in the link. In the diagram, the 2 cm long line, is the circle. Therefore, the radius is 1 cm. The best approach I could find, was by using a cos ratio in the triangle highlighted in red as whenever I try any trig rules, it always gets too complicated with multiple variables. The angle closest to the centre of the circle would have to be 90-theta and, if the triangle is right angle (which it was made to be this way), the opposite angle must be theta. Hence by using $\cos \theta = a/h$, where h is the 1cm line, 'a' (which refers to adjacent not the other longer 'a') must be cos theta. This only works if the entire line is 2a. But if it's not and either side is different, then I have a problem. So, I wanted to know, am I right with the above working, or am I wrong. And if I'm wrong, how should I approach this problem differently. Thanks.",['trigonometry']
2385139,What's wrong with this truth table for implication?,I completed this truth table in an assignment for an implication and got a few of the outputs wrong. I was wondering if anyone can help explain why I got them wrong. This is part of the truth table that I got wrong: PLEASE FORGIVE THE TYPO IN TABLE 4. It's meant to say $(\neg r \Rightarrow  p) \land (r \Rightarrow q)$ AND NOT $(\neg p \Rightarrow p) \land (r \Rightarrow q)$,['discrete-mathematics']
2385147,"How do I represent an equilateral triangle in cartesian coordinates centered around (0,0) knowing the length of one of the sides","I am trying to figure out how to find the Cartesian coordinates of an equilateral triangle centered around (0,0).",['geometry']
2385159,"Difference between rings of continuous functions on $[0,1]$ and $(0,1)$.","Consider $X=[0,1],Y=(0,1)$. Denote $C(W)=$set of real-valued continuous functions on $W$. Here $X$ and $Y$ are inherited the standard topology from $R$. What is the difference between two rings $C(X),C(Y)$? Clearly $C(Y)$ has functions which are not considered continuous on $X$ as $\frac{1}{(x-1)^r},\frac{1}{x^r}$ for $r\in R_{>0}$. But I can always restrict the functions on $X$ to $Y$ to obtain a continuous map by considering $Y$ as a subspace of $X$. So I have $C(X)\xrightarrow{Res^X_Y}C(Y)$ and this should be mono. The compactness of $[0,1]$ requires the function bounded whereas $C(Y)$ does not have this property. How does the ring structure recognize topological difference or do we pass the topological difference through the ring structure like what we have done in sheaf of regular functions through some sort of localization technique? Is there a generic recipe to construct the structure of these rings from $X$ or $Y$?","['sheaf-theory', 'algebraic-geometry', 'abstract-algebra', 'functional-analysis', 'ring-theory']"
2385171,A question on Lebesgue measure and its absolutely continuous measures,"Let $\lambda$ be the Lebesgue measure and $\mu$ be a measure absolutely continuous with respect to $\lambda$. Suppose for Lebesgue measurable sets $X_n$, $n=1,2,...$, $\lambda(X_n)=1/n$. Do we have  $\mu(X_n)\to 0$ as $n\to \infty$? Thanks.","['real-analysis', 'lebesgue-measure', 'measure-theory', 'analysis']"
2385176,An inequality with $\cos$ and triangle sides,"Here is the problem: Let $ABC$ be a triangle with sides $a, b, c$. Show that $\dfrac{\cos A}{a^3}+\dfrac{\cos B}{b^3}+\dfrac{\cos C}{c^3}\geq\dfrac{3}{2abc}.$ Here's my attempt: By the cosine formula, we have $\cos A = \dfrac{b^2+c^2-a^2}{2bc}$ etc, which the left hand side can be transformed into: \begin{equation*}
\dfrac{a^2+b^2-c^2}{2abc^3}+\dfrac{a^2+c^2-b^2}{2ab^3c}+\dfrac{b^2+c^2-a^2}{2a^3bc}
\end{equation*} And then I'm stuck. Can someone help me? Thanks.","['inequality', 'trigonometry', 'triangles']"
2385214,Most Complete Course Materials Taught Using Munkres? G.F. Simmons? Erwine Kreyszig?,"I'm looking for online resources offering the materials (i.e. lecture notes, homeworks / assignments with solutions, exams with solutions, and videos of lectures) of a first course in topology taught using Munkres' Topology , 2nd edition, or G.F. Simmon's Introduction to Topology and Modern Analysis . I've found the materials of a 2004 course at MIT OcW. Where else do I look? Where is the most complete and comprehensive set of materials available for such a course? And, any such online resources please for course materials of a first course in functional analysis course taught using Erwine Kreyszig's Introductory Functional Analysis With Applications ?","['functional-analysis', 'reference-request', 'general-topology', 'online-resources']"
2385215,Find all solutions to $x^3+(x+1)^3+ \dots + (x+15)^3=y^3$,"Find all pairs of integers $(x, y)$ such that 
  $$x^3+(x+1)^3+ \dots + (x+15)^3=y^3$$ What I have tried so far: The coefficient of $x^3$ is $16$ in the left hand side. It is not useful then to trying bound LHS between, for example, $(ax+b)^3$ and $(ax+c)^3$ and then say that $ax+b<y<ax+c$. I also tried to use modulo a prime. But it seems unlikely to bound variables this way. EDIT : Though, it can be factored as $(2x+15)(x^2+15x+120)=(y/2)^3$. LSH factors are almost co-prime and we can say that $x^2+15x+120=3z^3$ or $x^2+15x+120=5z^3$. These are still too difficult to solve! Any ideas?","['number-theory', 'diophantine-equations', 'elementary-number-theory']"
2385249,For the given function find k such that f(x)≠f(x+k) for any value of x,"Let $f(x)=\frac{x}{x^2+k}$ then find k such that $f(x)≠f(x+k)$ for any
  value of x. In my first attempt, I tried to solve $f(x)=f(x+k)$. But on solving this further, I get a biquadratic(including terms of odd power). Hence, I cannot impose any condition on k from here. Here are some other things I observed about this function: A) Function will be continuous for positive values of k and will be discontinuous for negative values of k. B) $f(x)=f(x+k)$ means that the function is periodic with period k. If the function is periodic about k, then we can write for integers n that $f(x-nk)=f(x+nk)$ . But I don't know where to go with this analysis. How to solve this question?","['calculus', 'functions']"
2385254,proof of product rule on conditonal probabilitiy,"From the basic product rule on conditional probability, we know the following: p(x,y) = P(x|y)P(y). But I cannot understand this formula: p(x,y|z) = p(x|y,z)p(y|z). I have tried to prove this as: p(x,y|z) = p(x|y|z)p(y|z) But i am confused on p(x|y|z)[don't know this notation exists or not.]. And if p(x|y|z) exists then again confused why p(x|y|z) = p(x|y,z)","['statistics', 'probability']"
2385273,Is a constant function periodic? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Can we regard a constant function ""$f(x)=\text{constant}$"" to be a periodic function? If yes, what is its period?","['periodic-functions', 'functions']"
2385294,Actions of groups on categories,"Let $\Gamma$ be a group and denote by $\underline{\Gamma}$ the tensor category whose objects are simply the group elements, hom-sets only contain identities and the tensor product is given by the group product. The following definitions are standard: Let $\mathcal{C}$ be a category. An action of $\Gamma$ on $\mathcal{C}$ is a monoidal functor $\underline{\Gamma}\rightarrow \text{Aut}(\mathcal{C})$. In a similar fashion one has the following definition. Let $\mathcal{C}$ be a monoidal category. An action of $\Gamma$ on $\mathcal{C}$ is a monoidal functor $\underline{\Gamma}\rightarrow \text{Aut}_{\otimes}(\mathcal{C})$. Here $\text{Aut}_{\otimes}(\mathcal{C})$ is the category of tensor auto-equivalences. In the book tensor categories by Etingof, Gelaki, Nikshych and Ostrik, there is the following exercise: Show that any action of $\Gamma$ on $\text{Vec}$ corresponds to an element in $H^2(\Gamma, k^*)$. Show that any action of $\Gamma$ on $\text{Vec}$ viewed as a monoidal category is trivial. I was wondering why any action of $\Gamma$ on $\text{Vec}$ is trivial when we view the latter as a monoidal category. The only difference with the first part of the question is that we only allow tensor-auto-equivalences. How many tensor-auto-equivalences are there on $\text{Vec}$? EDIT For those who are wondering how to prove the first part of the exercise, I'll include the details in this post. Suppose $\mathcal{C}=\ _k\text{Vec}$ which we view as an abelian category. That means that $\text{Aut}(\mathcal{C})$ are those additive functors that are auto-equivalences. Let $\Gamma$ act on $\mathcal{C}$. Then for any $\gamma\in \Gamma$ we have an auto-equivalence $T_{\gamma}$. Since $T_{\gamma}$ is an auto-equivalence, it takes any one dimensional space to a one dimensional space. Moreover, there are natural isomorphisms $\theta_{\gamma_1,\gamma_2}:T_{\gamma_1}\circ T_{\gamma_2}\rightarrow T_{\gamma_1\gamma_2}$. This natural isomorphism is completely determined by it's action on the one-dimensional space $k$, the corresponding isomorphism is given by an invertible number which I will also denote by $\theta_{\gamma_1,\gamma_2}$. Since $T$ is a monoidal functor and since $_k\text{Vec}$ is strict, we have that $$\theta_{\gamma_1\gamma_2,\gamma_3}\theta_{\gamma_1,\gamma_2}=\theta_{\gamma_1,\gamma_2\gamma_3}\theta_{\gamma_2,\gamma_3}.$$
This says that $\theta_{-,-}\in Z^2(\Gamma,k^*)$. I guess one says two actions $T_1,T_2:\underline{\Gamma}\rightarrow \text{Aut}(\mathcal{C})$ are the same it the functors $T_1,T_2$ are monoidally equivalent. One should (probably) find the following statement. The functors $T_1,T_2$ are monoidally equivalent if and only if the corresponding $2$-cocycles are cohomologous. Carefully writing down the definitions should give the proof (I didn't check this though).","['category-theory', 'abstract-algebra', 'monoidal-categories', 'group-theory']"
2385315,Find $\binom{n}{0} \binom{2n}{n}-\binom{n}{1} \binom{2n-2}{n}+\binom{n}{2} \binom{2n-4}{n}+\cdots$ [duplicate],This question already has answers here : Multiplication Principle and Inclusion-Exclusion: $2^n = \sum_{i = 0}^n (-1)^i \binom{n}{i} \binom{2n - 2i}{n - 2i}$ (3 answers) Closed 10 months ago . Find $$\binom{n}{0} \binom{2n}{n}-\binom{n}{1} \binom{2n-2}{n}+\binom{n}{2} \binom{2n-4}{n}+\cdots$$ I have taken $r$th term and modified as follows: $$T_r =(-1)^r \binom{n}{r} \binom{2n-2r}{n}=(-1)^r \frac{n!}{(n-r)!r!} \times \frac{ (2n-2r)!}{n! (n-2r)!}=(-1)^r \frac{(2n-2r)!}{(n-r)! r!(n-2r)!}$$ Can we continue from here?,"['permutations', 'combinatorics', 'binomial-coefficients']"
2385330,Does there exist an English translation of the book Théorie Des Distributions by Laurent Schwartz? [closed],"Closed. This question is off-topic . It is not currently accepting answers. Closed 6 years ago . This question is not about mathematics, within the scope defined in the help center . This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Improve this question Does there exist an English translation of the book Théorie Des Distributions by Laurent Schwartz? The theory of Distributions as it is presented in modern books is very much evolved and for a beginner it is difficult to comprehend the motivation behind it. The father of the subject; Laurent Schwartz wrote his treatise in french. A few pages of the original book was translated into english. It is much easier to comprehend the motivation behind the various concepts,in this book. That's why I wanted to know if there is a complete translated version of the book.","['reference-request', 'book-recommendation', 'analysis']"
2385348,Evaluate the integral $\int_0^{2a}\int_0^\sqrt{2ax-x^2}\frac{\phi'(y)(x^2+y^2)x dxdy}{\sqrt{4a^2x^2-(x^2+y^2)^2}}$,"Change the order of integration in: $$\int_0^{2a}\int_0^\sqrt{2ax-x^2}\frac{\phi'(y)(x^2+y^2)x }{\sqrt{4a^2x^2-(x^2+y^2)^2}}dxdy$$ and hence evaluate it. I changed the order of integration and got the limits $y=0$ to $a$ and $x=a-\sqrt{a^2-y^2}$ to $a+\sqrt{a^2-y^2}$. I then tried changing to polar coordinates, but it is only making the integrand more complex.","['integration', 'definite-integrals']"
2385495,"Uniqueness of $k$th root mod $m$ if $(k, \phi(m)) = 1$.","I'm trying to prove a fact about $k$th roots that says that if $(k, \phi(m)) = 1$ and $(b,m) = 1$, then there is a unique $k$th root modulo $m$ (composite $m$). I'm not sure how to go about it but here's an attempt; Suppose towards a contradiction that there are two solutions $x_1 = b^r$ and $x_2 = b^s$ to the congruence $x^k \equiv b \bmod m$. Then $$x_1^k \equiv x_2^k \equiv b \bmod m$$ so that $b^{kr} \equiv b^{ks} \equiv b \bmod m$, or equivalently, $b^{kr - 1} \equiv b^{ks - 1} \bmod m$. We can then conclude that $m \mid b^{kr - 1} - b^{ks - 1}$. Now we can factor to obtain $b^{kr - 1} - b^{ks - 1} = b^{kr - 1}(b^{k(s-r)} - 1)$ to conclude that $m \mid b^{kr - 1}(b^{k(s-r)} - 1)$. But since $(b, m) = 1$, we must have $m\mid b^{k(s-r)} - 1$, or equivalently, $$b^{k(s-r)} \equiv 1 \bmod m$$ which would imply $k(s-r) = \phi(m)v$ for some $v \in \mathbb Z$. I feel like there could be a way to extract a contradiction from this last step but I can't see it, would somebody be able to check if this is a good way to go?","['proof-verification', 'number-theory', 'congruences', 'modular-arithmetic', 'elementary-number-theory']"
2385499,Is there a definition of $[Y|X=x]$ as a random variable?,"I recently witnessed statisticians discussing $[Y|X=x]$ as if it were a random variable. In particular, they were making assumptions about its distribution. I am familiar with various definitions of conditional expectations, in particular with $\mathbb E[Y|X=x]$. And I know that $\mathbb E[Y|X]$ is a random variable. But don't remember ever seeing $[Y|X=x]$ as a random variable. What could be its definition? A bit of context added : In the context of inference, let $X$ be the predictor r.v. and $Y$ the response. Define $r(x)= \mathbb E(Y|X = x)$ and call it the regression function. Then (according to the statisticians) we make an assumption on the distribution of $(Y|X = x)$.","['probability-theory', 'random-variables', 'statistics', 'conditional-expectation', 'definition']"
2385508,Reflective subcategories of topological vector spaces,"Let $\mathsf{TopVect}$ be the category of TVS over $\mathbb K\in \{\mathbb R, \mathbb C\}$ with continuous linear maps as morphisms. Do the normed spaces or locally convex spaces form a reflective
  subcategory of $\mathsf{TopVect}$? (in the latter case: do normed
  spaces form a reflective subcategory of the locally convex spaces?). Basically: can you ""complete"" a TVS to a locally convex / normed space? I have virtually zero intution regarding this topic, but answering these questions might help with this question .","['functional-analysis', 'category-theory']"
2385544,Coin tosses until doubling amount,"Two friends are playing the following game:
The first player named A has an initial amount of M dollars. By tossing a fair coin once, he bets 1 dollar and if he gets heads, player B gives him 1 dollar, while if he gets tails, A gives his 1 dollar to B. The game ends when A doubles his money or when he loses it all. What is the probability he doubles his money? OK at the first coin toss he starts with M dollars and bets 1. With probability 1/2 he now has M+1 and with probability 1/2 he has M-1. At the second round, he bets another dollar and with probability 1/2 he has M+2 and with probability 1/2 he has M-2 and so on. Since the events are independent, the total probability of winning each time is 1/2*1/2 etc. How do we calculate the total probability?
I clarify that each time he bets only 1 dollar.",['probability']
2385570,Monty hall problem probability 2/6?,"For the Monty hall problem, with 3 doors, two of which have sheep and 1 has a car. I calculated the probability of getting the car if you swap being 2/6 instead of 2/3. I have drawn this tree diagram of how I calculated it: And from it I get that the probability of getting a car if you swap is 2/6 and if you stay it's 1/6, which is not the same as the actual answer 2/3 and 1/3. I have included the probabilities of getting sheep, as well as my overall probabilities, add up so I want to know what is wrong with my tree diagram, which resulted in this wrong answer. In my tree diagram I haven't included the host revealing a door, so could this be a factor as to why my answer is wrong?","['decision-trees', 'monty-hall', 'probability']"
2385596,Dimension of the center of the group algebra is equal to the number of irreducible representations- Without using character theory,"In a representation theory course we recently proved that the number of irreducible representations of a finite group $G$ over the complex numbers is equal to the number of conjugacy classes of $G$. The proof we saw used characters, as is standard. But I was interested in whether there exists a proof that does not use characters, and found that (apparently) $\dim_k(Z(k[G]))$ is equal to both the number of conjugacy classes of $G$ and the number of irreducible representations over the field $k$. I've proven that it is equal to the number of conjugacy classes, but am struggling in providing a proof that it is equal to the number of irreducible representations that doesn't rely on the original result. This is my progress so far: First we decompose $k[G]$ into the direct sum of simple unital rings like so: Let $\mathcal{L}$ be the set of isomorphism classes of simple left ideals and for each $l\in\mathcal{L}$ define $R_l=\sum_{L\in l} L$. Then $R_l$ is a unital ring and $R\cong\oplus_{l\in\mathcal{L}}R_l$. I believe that the identities of each $R_l$ give a basis for $Z(k[G])$. It is obvious that they are linearly independent and are in the center, so the issue is how to prove that they actually span the center. If we assume that we already know the number of conjugacy classes equal the number of irreducible representations, this becomes a simple matter of counting dimensions, so I'm fairly certain that this is a basis. However I have no clue how to prove the result without using this, and any advice would be hugely appreciated.","['representation-theory', 'abstract-algebra', 'group-theory']"
2385606,"Prob. 12, Chap. 6, in Baby Rudin: Denseness of $\mathrm{C}[a,b]$ in $L^2[a,b]$","Here is Prob. 12, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: With the notations of Exercise 11, suppose $f \in \mathscr{R}(\alpha)$ and $\varepsilon > 0$ . Prove that there exists a continuous function $g$ on $[a, b]$ such that $\lVert f-g \rVert_2 < \varepsilon$ . Hint: Let $P = \left\{ \ x_0, \ldots, x_n \ \right\}$ be a suitable partition of $[a, b]$ , define $$ g(t) = \frac{ x_i - t }{ \Delta x_i } f \left( x_{i-1} \right) + \frac{ t- x_{i-1} }{ \Delta x_i } f \left( x_i \right) $$ if $x_{i-1} \leq t \leq x_i$ . My Attempt: Here is the link to one of my Math SE posts in which I have included Definitions 6.1 and 6.2 from Rudin, where all the notation to be used below is  given: Theorem 6.15 in Baby Rudin: If $a<s<b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then . . . Let us first assume that $f$ is a real function defined on $[a, b]$ . As $f$ is  Riemann-Stieltjes integrable on $[a, b]$ , so it is  bounded on $[a, b]$ . Let us put $$ M \colon= \sup \left\{ \ \lvert f(x) \rvert \ \colon \ a \leq x \leq b \ \right\}. \tag{A} $$ Again as $f$ is  Riemann-Stieltjes integrable on $[a, b]$ , so by Theorem 6.4 in Rudin we can find a partition $P = \left\{ \ x_0, \ldots, x_n \ \right\}$ of $[a, b]$ for which $$ U(P, f, \alpha) - L(P, f, \alpha) < \frac{\varepsilon^2 }{4 M \left[ \alpha(b) - \alpha(a) \right] + 1  }. \tag{B}$$ Then for each $i = 1, \ldots, n$ , we have $$ \left( M_i - m_i \right) \Delta \alpha_i  < \frac{\varepsilon^2 }{4 M \left[ \alpha(b) - \alpha(a) \right] + 1  }. \tag{C}$$ For each $i = 1, \ldots, n$ , and for every $t_i$ such that $x_{i-1} \leq t_i \leq x_i$ , we see that $$
\begin{align}
 \left\lvert f\left(t_i \right) - g \left(t_i \right) \right\rvert &= \left\lvert \frac{x_i - t_i + t_i - x_{i-1} }{\Delta x_i } f\left(t_i \right) - \left[ \frac{ x_i - t_i }{ \Delta x_i } f \left( x_{i-1} \right) + \frac{ t_i- x_{i-1} }{ \Delta x_i } f \left( x_i \right)  \right] \right\rvert  \\
&= \left\lvert \frac{x_i - t_i }{\Delta x_i} \left[ f \left(t_i \right) - f \left( x_{i-1} \right) \right] + \frac{ t_i - x_{i-1} }{\Delta x_i} \left[ f \left(t_i \right) - f \left( x_i  \right) \right] \right\rvert \\
&\leq \frac{x_i - t_i }{\Delta x_i} \left\lvert f \left(t_i \right) - f \left( x_{i-1} \right)  \right\rvert + \frac{ t_i - x_{i-1} }{\Delta x_i} \left\lvert f \left(t_i \right) - f \left( x_i  \right)  \right\rvert \\
&\leq \frac{x_i - t_i }{\Delta x_i} \left( M_i - m_i \right)  + \frac{t_i - x_{i-1} }{\Delta x_i} \left( M_i - m_i \right) \\
&  \mbox{ [ where $m_i \colon= \inf \left\{ \ f(t) \ \colon \ x_{i-1} \leq t \leq x_i \ \right\}$, and $M_i \colon= \sup \left\{ \ f(t) \ \colon \ x_{i-1} \leq t \leq x_i \ \right\}$ ] } \\
&= M_i - m_i, 
\end{align}
$$ which implies that $$ \left\lvert f\left(t_i \right) - g \left(t_i \right) \right\rvert^2 \leq \left( M_i - m_i \right)^2, $$ and so $$ \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i \leq \left( M_i - m_i \right)^2 \Delta \alpha_i. $$ Therefore, for each $t_i \in \left[ x_{i-1}, x_i \right]$ for each $i = 1, \ldots, n$ , we have $$
\begin{align}
0 &\leq  \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i  \\
&\leq \sum_{i=1}^n \left( M_i - m_i \right)^2 \Delta \alpha_i \\
&\leq \sum_{i-1}^n 2M \left( M_i - m_i \right) \Delta \alpha_i \qquad \mbox{ [ using (A) above ] } \\
&= \sum_{i=1}^n 2 M \frac{ \varepsilon^2 }{ 4 M \left[ \alpha(b) - \alpha(a) \right] + 1} \Delta \alpha_i \qquad \mbox{ [ using (C) above ] } \\
&= 2M \frac{ \varepsilon^2 }{ 4M \left[ \alpha(b) - \alpha(a) \right] + 1} \left[ \alpha(b) - \alpha(a) \right]\\
&< \frac{\varepsilon^2 }{2}, 
\end{align}
$$ and so $$ - \frac{\varepsilon^2 }{2} \leq \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i \leq  \frac{\varepsilon^2 }{2}. \tag{0}  $$ But $$ L \left(P, \lvert f-g \rvert^2 , \alpha \right) = \inf \left\{ \ \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i  \ \colon \ t_i \in \left[ x_{i-1}, x_i \right] \ i = 1, \ldots, n \ \right\}, \tag{1} $$ and $$ U \left( P, \lvert f-g \rvert^2 , \alpha \right) = \sup \left\{ \ \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i  \ \colon \ t_i \in \left[ x_{i-1}, x_i \right] \ i = 1, \ldots, n \ \right\}. \tag{2} $$ Here is the link to a relevant post of mine here on Math SE: https://math.stackexchange.com/questions/2358808/riemann-stieltjes-upper-and-lower-sums-as-suprema-and-infima Now from (0), (1), and (2)  we can conclude that $$ - \frac{\varepsilon^2 }{2} \leq L \left( P, \lvert f-g \rvert^2, \alpha \right) \leq U \left( P , \lvert f-g \rvert^2 , \alpha \right) \leq \frac{\varepsilon^2 }{2}. \tag{3}   $$ As $g$ is continuous on $[a, b]$ , so $g \in \mathscr{R}(\alpha)$ on $[a, b]$ , by Theorem 6.8 in Rudin. Now as $f, g \in \mathscr{R}(\alpha)$ on $[a, b]$ , so $f - g \in \mathscr{R}(\alpha)$ , by Theorem 6.12 (a) in Rudin. Here are the links to my Math SE post on Theorem 6.12 (a) in Rudin: Theorem 6.12 (a) in Baby Rudin: $\int_a^b \left( f_1 + f_2 \right) d \alpha=\int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha$ https://math.stackexchange.com/questions/2327134/theorem-6-12-a-in-baby-rudin-if-f-in-mathscrr-alpha-on-a-b-then-c Furthermore, as $ f-g  \in \mathscr{R} (\alpha)$ on $[a, b]$ , so $ \lvert f-g  \rvert \in \mathscr{R} (\alpha) $ on $[a, b]$ as well, by virtue of Theorem 6.13 (b) in Rudin. Finally, as $ \lvert f-g  \rvert  \in \mathscr{R} (\alpha) $ on $[a, b]$ and as the map $y \mapsto y^2$ is continuous on all of $[0, +\infty)$ , so we can conclude that $ \lvert f-g  \rvert^2  \in \mathscr{R} (\alpha) $ on $[a, b]$ also, by virtue of Theorem 6.11 in Rudin. Thus $\int_a^b \lvert f-g \rvert^2 \ \mathrm{d} \alpha $ exists.  Moreover, for any partition $Q$ of $[a, b]$ , we have the inequalities $$ L \left( Q, \lvert f-g \rvert^2, \alpha \right) \leq \int_a^b \lvert f-g \rvert^2 \ \mathrm{d} \alpha \leq  U \left( Q, \lvert f-g \rvert^2 , \alpha \right). \tag{4} $$ From (3) and (4) we can conclude that $$ - \frac{\varepsilon^2}{2} \leq \int_a^b \lvert f-g \rvert^2 \ \mathrm{d} \alpha \leq \frac{\varepsilon^2}{2}. \tag{5} $$ But as $\lvert f - g \rvert^2 \geq 0$ on $[a, b]$ , so by Theorem 6.12 (b) in Rudin $$ \int_a^b \lvert f - g \rvert^2 \ \mathrm{d} \alpha \geq 0. \tag{6} $$ Here is the link to my Math SE post on Theorem 6.12 (b) in Rudin: Theorem 6.12 (b) in Baby Rudin: If $f_1 \leq f_2$ on $[a, b]$, then $\int_a^b f_1 d\alpha \leq \int_a^b f_2 d\alpha$ From (5) and (6) we obtain $$ 0 \leq \int_a^b \lvert f - g \rvert^2 \ \mathrm{d} \alpha \leq \frac{\varepsilon^2}{2} < \varepsilon^2. $$ Therefore, $$ \left( \int_a^b \lvert f - g \rvert^2 \ \mathrm{d} \alpha \right)^{1/2} < \varepsilon, $$ which is the same as $$ \lVert f-g \rVert_2 < \varepsilon. $$ And the function $g$ is continuous on $[a, b]$ . Is my proof correct? Is it rigorous enough? If not, then where does it lack? How to rigorously show that this  function $g$ is continuous? Now we assume that $f$ is a complex function defined on $[a, b]$ . Then $f = \Re f + \iota \Im f$ , where $\Re f$ and $\Im f$ are real functions defined on $[a, b]$ . As $f$ is Riemann-Stieltjes integrable with respect to $\alpha$ on $[a, b]$ , so are $\Re f$ and $\Im f$ . Then we have $$ \int_a^b f \ \mathrm{d} \alpha =  \int_a^b \Re f \ \mathrm{d} \alpha +  \iota \int_a^b \Im f \ \mathrm{d} \alpha.  $$ Applying the result just proved for real functions to $\Re f$ and $\Im f$ , we can conclude that there are continuous functions $g_1$ and $g_2$ on $[a, b]$ such that $$ \left\lVert \Re f \ - \ g_1 \right\rVert < \frac{ \varepsilon}{4}, $$ and $$ \left\lVert \Im f \ - \ g_2  \right\rVert < \frac{ \varepsilon}{4}; $$ therefore if we put $ g \colon= g_1 + \iota g_2$ on $[a, b]$ , we  have $$ 
\begin{align}
\lVert f - g \rVert_2 &= \left\lVert \left( \Re f + \iota \Im f \right) \ - 
\ \left( g_1 + \iota g_2 \right) \right\rVert_2 \\
&= \left\lVert \left( \Re f - g_1 \right) \ + \ \iota  
 \left( \Im f -  g_2 \right) \right\rVert_2 \\
&\leq \left\lVert  \Re f - g_1 \right\rVert_2 \ + \ \left\lVert \iota  
 \left( \Im f -  g_2 \right) \right\rVert_2 \\
&= \left\lVert  \Re f - g_1 \right\rVert_2 \ + \ \lvert \iota \rvert \cdot 
\left\lVert \Im f -  g_2  \right\rVert_2 \\ 
&= \left\lVert  \Re f - g_1 \right\rVert_2 \ + \ 
\left\lVert \Im f -  g_2  \right\rVert_2 \\
&< \frac{\varepsilon}{4} + \frac{\varepsilon}{4} \\
&< \varepsilon.
\end{align}
$$ Finally, as $g_1 = \Re g$ and $g_2 = \Im g$ are both continuous on $[a, b]$ , so is $g$ . Is this proof for complex functions correct too? If so,
is it rigorous enough too? If not, then what is it that I am missing
out on?","['real-analysis', 'lp-spaces', 'integration', 'definite-integrals', 'analysis']"
2385666,Expectation value - does it need to be on a Banach space?,"I came across the following definition of a exception value of a random variable $X$ in (Lord, Powell and Shardlow, 2014;pg139; emphasis not mine): Let $X$ be a Banach space-valued random variable on the probability
  space $(\Omega,\mathcal{F},\Bbb{P})$. If $X$ is integrable, the expectation of the $X$ is $$\Bbb{E}[X]:= \int_\Omega X(\omega)d\Bbb{P}(\omega), \tag{4.1}$$ Why is there a resection to Banach space-random variables here? Why won't only old measurable space do?","['probability-theory', 'measure-theory']"
2385682,Difficulty understanding division sign in expression,"I have difficulty understanding following expression:
$$(64x^3÷27a^{-3})^\frac{-2}{3}$$ Should I interpret the division sign as follows:
$$\left(\frac{64x^3}{27a^{-3}}\right)^\frac{-2}{3}$$ Or as I originally interpreted it:
$$\left[\left(\frac{64x^3}{27}\right)\times a^{-3}\right]^\frac{-2}{3}$$ The reason for my confusion is the order of operations as we can write a division as a multiplication (which is how I came to my original interpretation): $$\left[64x^3\times\frac{1}{27}\times a^{-3}\right]^\frac{-2}{3}$$ I know this is wrong as I cannot get the correct answer this way. This leaves me with the conclusion that the division sign they use is the same as everything after it should be on the bottom of the fraction? The link to the exercise including the solution: Mathopolis exercise",['algebra-precalculus']
2385718,How many times on average would one need to retransmit a message before it is received correctly?,In a communication system each message is sent 4 times over an unreliable channel. Assume that individual transmissions are independent from each other. It is known that the probability to receive the message correctly at least once is 0.9984. How many times on average would one need to retransmit a message before it is received correctly?,"['probability-theory', 'probability-distributions', 'statistics', 'binomial-distribution', 'probability']"
2385722,Condition of existence of a triangle,"It's easy to prove that the triangle inequality holds for any triangle with the lengths of sides $a$, $b$ and $c$.
But how can one prove that if the triangle inequality holds for  any given positives $a$, $b$ and $c$  then a triangle (geometric figure) with the lengths of the sides equal to $a,b$ and $c$ can  necessarily be formed?","['inequality', 'analytic-geometry', 'euclidean-geometry', 'triangles', 'geometry']"
2385755,The example of an exact functor that doesn't commute with colimits in abelian category,It is mentioned here that a right exact funtor commute with colimits only under some good conditions. I wonder if there is a concrete counterexample of an exact functor which doesn't commute with colimits.,"['category-theory', 'abstract-algebra']"
2385756,Stochastic stability,"In this book (p. 37, 43, 44) the notion of stochastic stability of a stochastic process is defined by the condition
$$
\mathbb{E} \left[\sum_{j=0}^{\infty} \|x_k\|^2\right] < \infty.\tag{1}
$$
It is shown that for Markovian switching systems of the form $x_{k+1} = f_{\theta_k}(x_k)$ driven by a finite Markov chain $(\theta_k)_k$, stochastic stability is equivalent to mean-square stability. Let us replace the above expectation operator $\mathbb{E}$ by a coherent risk measure $r$ leading to the condition
$$
r \left[\sum_{j=0}^{\infty} \|x_k\|^2\right] < \infty\tag{2}\label{2}
$$ We know that $r \left[\sum_{j=0}^{\infty} \|x_k\|^2\right] \leq \sum_{k=0}^{\infty}r(\|x_k\|^2)$ (subadditivity property), therefore, if $r(\|x_k\|^2)$ converges adequately fast so that $\sum_{k=0}^{\infty}r(\|x_k\|^2)$ is finite, then \eqref{2} will hold. I am, however, interested in the converse: under what additional conditions does \eqref{2} imply that $r(\|x_k\|^2)\to 0$? A first step would be to see whether this holds for the essential supremum; if $\mathrm{esssup} \left[\sum_{j=0}^{\infty} \|x_k\|^2\right] < \infty$ is it true that $\mathrm{esssup}(\|x_k\|^2)\to 0$?","['stochastic-processes', 'probability-theory', 'convergence-divergence', 'sequences-and-series', 'risk-assessment']"
2385792,Integrate an Ordinary Differential Equation [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I found this from a derivation from a Dimensional Analysis book. It starts from the following equation: $$\frac{d^2(F(x))^2)}{dx^2}+\frac{x}{3}\frac{dF(x)}{dx}+\frac{F(x)}{3}=0$$ where $F(x)$ is a function of $x$ This ODE is then integrated with respect to $x$, and it got $$\frac{dF(x)^2)}{dx}+\frac{x\cdot F}{3}= \text{const.}$$ where $const.$ should be some terms which the book chooses not to show I stared at this derivation for a long time, but I could not figure out how this integration makes sense. Please forgive my stupidity if it turns out to be something simple.","['ordinary-differential-equations', 'calculus']"
2385828,What does a $4$-dimensional bell curve look like graphically?,"I have some $4$ D data in $(w,x,y,z)$ quaternion format that I'd like to graph to discern whether or not it follows a Gaussian function. I am using MATLAB and a slider to emulate the fourth dimension. Does anyone know how a bell curve would look like in $4$ D? Thank you.","['algebra-precalculus', 'descriptive-statistics', 'statistics']"
2385830,Gaussian curvature from revolution surface. Where is wrong?,"Assume a curve $\gamma(t) = (r(t), 0, h(t))$, and its revolution surface: $X(t, \theta) = (r(t)\cos\theta, r(t)\sin\theta, h(t))$. After some calculations I find my first and second fundamental form to have expressions identical to this pdf , at page 02, that is:
$$
M_I = 
\begin{bmatrix}
\dot{r}^2 + \dot{h}^2 & 0 \\
0 & r^2 \\
\end{bmatrix}
,\quad\quad\quad
M_{II} = \frac{1}{\sqrt{\dot{r}^2 + \dot{h}^2}}
\begin{bmatrix}
-\ddot r\dot h + \ddot h\dot r & 0 \\
0 & r\dot h \\
\end{bmatrix}
$$ Now, it is my understanding that, gaussian curvature calculates:
$$
k = \frac{\det M_{II}}{\det M_I} = 
\frac{r\dot h\left(-\ddot r\dot h + \ddot h\dot r\right)}{r^2\left(\dot{r}^2 + \dot{h}^2\right)\sqrt{\dot{r}^2 + \dot{h}^2}} = 
\frac{\dot h}{r}\frac{-\ddot r\dot h + \ddot h\dot r}{\left(\dot{r}^2 + \dot{h}^2\right)^{3/2}}
$$ However, everywhere seems to say that correct result is:
$$
k = \frac{\dot h}{r}\frac{-\ddot r\dot h + \ddot h\dot r}{\left(\dot{r}^2 + \dot{h}^2\right)^2}
$$ I fail to see why is it squared, and not $3/2$ from the determinant calculation. However, after some physical checking, I know the later to be correct. I might be making a tiny arithmetic mistake somewhere, but, I couldn't find it.","['differential-geometry', 'surfaces']"
2385832,Number of Real solutions of $e^{x^2}=ex$,"Find a number of real  solutions of the following equation.
   $$e^{x^2}=ex$$ Need a pure calculus approach..","['derivatives', 'real-analysis', 'exponential-function', 'calculus', 'algebra-precalculus']"
2385897,Solve the DFE $xy'^3 - 2yy'^2 - 16x^2 = 0$,"I've started like this: From $xy'^3 - 2yy'^2 - 16x^2 = 0$, we find that $$y = -8x^2/p^2 + 1/2 \cdot xp,$$ with $p = y' = dy/dx$. Taking the derivative yields $$p = -16x/p^2 + 16 x^2p'/p^3 + 1/2p + 1/2 \cdot xp'$$ And working out the denominator gives us $$p^4 = -32xp + 32x^2 p' + xp^2p'.$$
At this point, I don't know how to go on. I tried separating the terms containing $p$ and $p'$, but it didn't get me anywhere.",['ordinary-differential-equations']
2385924,Proper function notation for matrix functions?,"For a vector-valued function we have the notation $f:\mathbb R^n\rightarrow \mathbb R^m$. Is this also a proper notation for a matrix function? Are there any conventions? For a matrix of one variable, $t$, 
  \begin{align}
\mathbf A(t)=
\begin{bmatrix}
a_{11}(t) & a_{12}(t) & \cdots & a_{1n}(t)\\
a_{12}(t) & a_{22}(t) & \cdots & a_{2n}(t)\\
\vdots &\vdots & \ddots & \vdots\\
a_{n1}(t) & a_{n2}(t) &\cdots & a_{mn}(t)
\end{bmatrix},
\end{align}
  is it correct to write $\mathbf A:\mathbb R \rightarrow \mathbb R^{m\times n}$? And for a matrix of several variables $\mathbf x=(x_1, \dots, x_n)$,
  \begin{align}
\mathbf B(\mathbf x)=
\begin{bmatrix}
b_{11}(\mathbf x) & b_{12}(\mathbf x) & \cdots & b_{1n}(\mathbf x)\\
b_{12}(\mathbf x) & b_{22}(\mathbf x) & \cdots & b_{2n}(\mathbf x)\\
\vdots &\vdots & \ddots & \vdots\\
b_{n1}(\mathbf x) & b_{n2}(\mathbf x) &\cdots & b_{mn}(\mathbf x)
\end{bmatrix},
\end{align}
  is the proper notation $\mathbf B:\mathbb R^{n} \rightarrow \mathbb R^{m \times n}$?","['notation', 'matrices', 'functions', 'multivariable-calculus', 'linear-algebra']"
2385937,Find the limit when $x = \pi / 6$,Let $a_0 = 1$ and $a_n = a_{n-1}(\cos  \frac{x}{2^n})$ .Find the limit $\{a_n\}$ when $x = \pi / 6$ . (i.e. find the $\lim_{n \to \infty} a_n$) My try : I solved it but the solution was time-consuming . $\frac{a_n}{a_{n-1}} = \cos \frac{x}{2^n} \to \frac{a_n}{a_0} = \frac{a_n}{a_{n-1}} \times \frac{a_{n-1}}{a_{n-2}} \times \dots \times\frac{a_1}{a_0} = \cos {\frac{x}{2^n}} \times \cos {\frac{x}{2^{n-1}}} \times \dots \times \cos {\frac{x}{2}} \to (2^n \sin{\frac{x}{2^n}})(a_n) = \sin x \to a_n = \frac{\sin x}{2^n\sin{\frac{x}{2^n}}} = \frac{\frac{x}{2^n}}{\sin{\frac{x}{2^n}}} \times \frac{\sin x }{x} \to \lim_{n \to \infty } a_n = 1 \times \frac{\sin{\frac{\pi}{6}}}{\frac{\pi}{6}} = \frac{3}{\pi} $ Note : I did some calculations by hand and didn't write them .,"['sequences-and-series', 'limits-without-lhopital']"
2385948,"Calculate the flux of the field $\mathbf{F} = k\left(\frac{\mathbf{r}}{r^3}\right)$ out of an arbitrary closed surface not intersecting $(0,0,0)$","Calculate the flux of the field $$\mathbf{F} = k\left(\frac{\mathbf{r}}{|\mathbf{r}|^3}\right)$$ where $\mathbf{r}=\langle x,y,z\rangle$ out of an arbitrary closed surface not intersecting $(0,0,0)$. My attempt I get 
$$\operatorname{div} \mathbf{F} = 0$$
Using Gauss’s theorem I get that the flux crossing an arbitrary surface must be $0$ since no flux is produced. The answer is however $4k\pi$ if the surface envelopes $(0,0,0)$ and otherwise it is $0$. How can this be true? How can any flux pass any surface if no flux is created anywhere? My understanding is obviously flawed, but I can’t pinpoint where.","['multivariable-calculus', 'divergence-operator']"
2385955,"Equivalent definition of the variation of the function on $[a,b]$","Let $f : \mathbb{R} \rightarrow \mathbb{R}$. The variation of $f$ over $[a,b] \subset \mathbb{R}$ is defined as
$$
V_{a}^{b}(f) := \sup_{ \substack{ a = t_0 < t_1 < \ldots < t_n = b \\ n \in \mathbb{N}} } \sum_{i=1}^{n} |f(t_i) - f(t_{i-1})|
$$
(i.e. the supremum is taken taken over all possible partitions $a = t_0 < t_1 < \ldots < t_n = b$, $n \in \mathbb{N}$ )? Is it true that $$V_{a}^{b}(f) = \lim_{\delta_n \rightarrow 0} \sum_{i=1}^{n}|f(t_i) -f(t_{i-1})|$$ where $\delta_n = \max_{1 \leq i \leq n}(t_i - t_{i-1}).$ Of course, it can easily be shown by the triangle that the sum increases, as new points are added to the partition. But still, how can one actually show that the supremum is also the limit as the distance of the points goes to zero?","['stochastic-processes', 'calculus', 'stochastic-calculus', 'bounded-variation', 'analysis']"
2385965,The derivative of a matrix transpose with respect to the original matrix,I am just stepping into matrix calculus and I wonder what the following differential is. Thanks. $$ \frac{\partial(A^T)}{\partial A} $$,"['matrices', 'matrix-calculus', 'derivatives']"
2385996,Application of the Dominated Convergence Theorem to differentiation inside the integral.,"I'm currently looking at the p-Laplacian within the context of the calculus of variations. However, I'm struggling to apply the dominated convergence theorem to take a derivative inside an integral. $\mathbf{Question}$ Assume $u$ and $v\neq 0$ are two functions in $W^{1,p}(U)$, $1<p<\infty$ and let $x\in U$ with $|\nabla u(x)|<\infty$ and $|\nabla v(x)|< \infty$ in Given the function
\begin{equation}
g:\tau\in \mathbb{R}\rightarrow g(\tau):=\frac{1}{p}|\nabla(u+\tau v)(x)|^p\in\mathbb{R}
\end{equation}
with $U\subset \mathbb{R}^n$
I can differentiate this with respect to $\tau$ to get 
\begin{equation}
g'(\tau)=|\nabla (u+\tau v)|^{p-2}(\nabla (u+\tau v)\cdot \nabla v)
\end{equation}
But what I would like to do is to apply the DCT to conclude that 
\begin{equation}
f:\tau\in \mathbb{R}\rightarrow \frac{1}{p}\int_U|\nabla(u+\tau v)(x)|^p\;dx\in\mathbb{R}
\end{equation}
is differentiable on $\mathbb{R}$ with
\begin{equation}
f'(\tau)= \frac{1}{p}\int_U|\nabla(u+\tau v)(x)|^{p-2}\big(\nabla u \cdot \nabla v+t|\nabla v|^2\big)\;dx\in\mathbb{R}
\end{equation}
So I'm looking to find an integrable function that dominates, say $|g'(\tau)|\leq h(\tau)$. However, I'm struggling to find such a function. If someone could offer a couple of hints I would be very greatful. I would really like to work through all the details as I'm sure this type of problem will come up again. I think my trouble comes from the fact that I'm dealing with vectors in the integrand which I haven't had much experience with. Sorry if this is a rather trivial question.",['measure-theory']
2386024,Eigenspaces of a symmetric matrix and its principal submatrices,"There are a lot of well known theorems that relate the eigenvalues of a real symmetric matrix $A$ to the eigenvalues of its principal submatrices. I can't help but wonder, are there similar theorems which relate the eigenspaces in these cases? For instance, if $v$ is an eigenvector for the smallest eigenvalue of a principal submatrix, then can we say anything about the extension of $v$ in relation to the eigenspaces of $A$? I was hoping for something like: If $v$ is an eigenvector for the smallest eigenvalue of a principal submatrix $B$ inside $A$, then $(v,0)$ is contained in the span of the two smallest eigenspaces of $A$. I.e. at worst, you need to use the smallest eigenvalue bigger than the eigenvalue associated to $v$ to describe $(v,0)$. Sorry for being so imprecise, I am frankly unsure of what specific questions one can reasonably ask here.",['linear-algebra']
2386027,About a Morse function on the euclidean $n$-sphere.,"If you are in a hurry feel free to jump directly to the emphasized parts. Setup. Let $A$ be a real invertible symmetric square matrix of size $n+1$ whose eigenvalues are:
$$\lambda_0>\lambda_1>\cdots>\lambda_n.$$
Let $\{v_0,\ldots,v_n\}$ be an orthonormal basis of $\mathbb{R}^{n+1}$ formed by eigenvectors of $A$ with $v_i$ associated to $\lambda_i$. From now all vectors of $\mathbb{R}^{n+1}$ will be written in this basis, namely, one has:
$$(x_0,\ldots,x_n):=x_iv^i.$$
Furthermore, let $F\colon\mathbb{S}^n\rightarrow\mathbb{R}$ defined by:
$$F(x):={}^\intercal xAx.$$ Proposition. $F$ defines a Morse function on $\mathbb{S}^n$. I have already computed the critical points of $F$, there set is $\{\pm v_0,\cdots,\pm v_n\}$, namely, one has:
$$\textrm{Crit}(F)=\{(\pm 1,0,\ldots,0),(0,\pm 1,0,\ldots,0),\ldots,(0,\ldots,0,\pm 1)\}.$$
Let $x\in\textrm{Crit}(F)$ and let $\varphi$ a chart of $\mathbb{S}^n$ centered at $x$, I am supposed to prove that:
$$\textrm{Hess}_0(F\circ\varphi^{-1})\in\textrm{GL}_{n+1}(\mathbb{R}).$$ I have been strongly advised to use the following charts:
$$U_p^{\pm}:=\{x\in\mathbb{S}^n;\pm x_p>0\},\varphi_p^{\pm}\colon x\mapsto\left(\frac{x_i}{x_p}\right)_{i\in\{0,\ldots,n\}\setminus\{p\}}.$$
I have computed the inverse of $\varphi_p^{\pm}$ and found that:
$$(\varphi_p^{\pm})^{-1}(x):=\frac{\pm 1}{\sqrt{1+\sum\limits_{i=0}^{n-1}{x_i}^2}}(x_0,\cdots,x_{p-1},1,x_p,\ldots,x_{n-1}).$$
Therefore, one has: $$F\circ(\varphi_p^{\pm})^{-1}(x)=\frac{\lambda_p+\sum\limits_{i=0}^{p-1}\lambda_i{x_i}^2+\sum\limits_{j=p}^{n-1}\lambda_{j+1}{x_j}^2}{1+\sum\limits_{i=0}^{n-1}{x_i}^2}.$$ Questions. I do not feel too confident in the above expression, I do not like those $1$ and $\lambda_p$ being apart. Can someone tell me if have made a mistake in my computations? I can see that $\textrm{Hess}_{0}(F\circ{\varphi_p^{\pm}}^{-1})$ will be diagonal and therefore the index of the critical point $\pm v_p$ will be easy to compute, but is there a clever way to conduct the computations? Any help will be greatly appreciated.","['spheres', 'hessian-matrix', 'differential-geometry', 'morse-theory']"
2386040,Can we express any positive real number with arbitrary precision using a ratio of two prime numbers? [duplicate],"This question already has an answer here : Are fractions with prime numerator and denominator dense? (1 answer) Closed 6 years ago . Although the question might look trivial at first because there are infinitely many prime numbers and for example the ratio of two near prime numbers tends to $1$ at infinity, there is still a point that is missing. If we define the set $S=\{\, \frac{p_{i}}{p_{j}}\mid i,j\in\Bbb N\,\}$ where $p_i$ is prime number $i$, is it dense in the set of non-negative reals? If it is, as much as it looks (prevalently) obvious, I am not sure about which precise property is ensuring this, as neither the infinitude of primes nor the limit of ratio of two successive primes reaching $1$ (which is a theorem on its own) looks sufficient individually. I could imagine something like: the rational set has this property and we can replace each rational number with a ratio of two primes to any desired precision. But, can we? Maybe I am missing something, but it is not obvious whichever way I look at it. Theorem that is expected is like: If $\frac{r_{1}}{s_{1}}>\frac{r_{2}}{s_{2}} > 0$ then there are always two prime numbers $p_{m}$ and $p_{n}$ so that $\frac{r_{1}}{s_{1}}>\frac{p_{m}}{p_{n}}>\frac{r_{2}}{s_{2}}$ if that is to work.","['real-analysis', 'prime-numbers']"
2386055,Can we symmetrize connections on tangent bundles?,"Let $M$ be a smooth manifold. Denote by $\mathcal{A}$ the space of all affine connections on $TM$, and by $\mathcal{S}$ the affine subspace of all the symmetric connections. Is there a projection $P:\mathcal{A}\to \mathcal{S}$? (i.e $P^2=P$).
If there is more than one, is there some natural choice? Note that $S$ is convex, so perhaps introducing some metric on $\mathcal{A}$ will be useful?","['symmetry', 'tangent-bundle', 'differential-geometry', 'connections']"
2386076,"Edge case $X = \varnothing$ in ""there exists exactly one equivalence class""","Consider the following exercise: Let $X$ be a connected topological space and $\sim$ an equivalence
  relation on $X$ such that every equivalence class is open. Then there
  is only one equivalence class, namely $X$. My question is NOT about the solution, but merely about the edge case $X = \varnothing$. The only equivalence relation on $X$ is the empty relation $\varnothing$. So basically there is no equivalence class . So did I misunderstood something or should one impose $X \neq \varnothing$ in this particular exercise? Edit. I use the excellent book written by @ Jack Lee in which the empty space IS connected (topological manifolds, p. 86). This is exercise 4.3, p.87.",['general-topology']
2386086,Generating set for wreath products,"Let $G$ be a finite $n$-generated group and $p$ be a prime not dividing the order $\vert G\vert$; let $C_p$ be the cyclic group of order $p$. According to results by A. Lucchini (Rend. Sem. Mat. Univ. Padova 98 (1997) 67--87 and Arch. Math. 62 (1994) 481--490) the wreath product $C_p^{n-1}\wr G$ is also $n$-generated. Can one write down explicitly a set of $n$ generators of this wreath product (involving generators of $G$)? Edit: what about the case $n=2$? One would need a 2-generating set for
$C_p\wr G\cong \mathbb{F}_pG\rtimes G$ in terms of a 2-generator set of $G$.","['finite-groups', 'group-theory', 'wreath-product']"
2386101,IVP implies EVP?,"I define Intermediate Value Property (IVP) and Extreme Value Property (EVP) as follows: IVP: If $I$ is an interval, and $f:I\rightarrow\mathbb{R}$, we say that 
$f$ has the intermediate value property iff whenever $a<b$ are point's in $I$ and $f(a)\leq c\leq f(b)$, there is a $d\in$  $(a,b)$ such that $f(d)=c$. EVP: If $I$ is an interval, and $f:I\rightarrow\mathbb{R}$, we say that $f$ has the extreme value property iff $f$ has maximum and minimum value, each at least once. That is, $\exists a,b\in I$ such that $f(a)\leq f(x) \leq f(b)$ for all $x\in I$. My question is, does IVP imply EVP? I don't think so, but I still can't find the counter example. Cheers!","['real-analysis', 'calculus', 'functions']"
2386119,$2^n \times 2^n$ Chessboard Covering minus $1$ square,"We are given a $2^n \times 2^n$ chessboard and we must prove that we can cover this chessboard with gamma shapes leaving only one square uncovered. A gamma shape is: So to solve this I proceeded with induction. Here is my proof: For $n=1$ we have a $2 \times 2$ chessboard and if we remove square   then a gamma shape remains so we can cover this and leave exactly one uncovered.
Now suppose that for some $n \in \mathbb{N}$ we can cover the $2^n \times 2^n$ chessboard with gamma shapes living exactly one square out so we can cover it leaving a corner out. Now for $n+1$ we have a $(2 \times2^n) \times (2 \times 2^n)$ chessboard  which is a union of the $4$ smaller $2^n \times 2^n$ chessboards.So we can picture in mind a big chestboard of $4$ smaller. From this union we pick one of the smaller chessboards and we cover it with gamma shapes leaving  exactly one square uncovered at the corner of this chessboard which is a corner of the bigger chessboard . To cover the rest $3$ chessboards (this time all of their squares) we use the induction hypothesis.
  Note that these three remaining chessboard form a big gamma shape. From these chessboards we remove(but not realy, just in our mind) the $3$ centered squares wich form a little gamma shape (in the big gamma shape that form the 3 chessboards) So each of the 3 chessboards we can cover it with gamma shapes using our induction hypothesis and to finish we add another gamma shape to to cover the 3 centered squares we removed in our mind. Thus we are done because we covered a $(2 \times2^n) \times (2 \times 2^n)$ chessboard with gamma shapes leaving only one out. My question is: Is there another more elegant and simpler solution than this or any other solution at all? I'm very curious to know. Thank you in advance.","['discrete-geometry', 'induction', 'recreational-mathematics', 'discrete-mathematics']"
2386122,Eigenvalues of $\ddot {y} +\dot{y}-6y = 0$,"I want to write $\ddot {y} +\dot{y}-6y = 0$ in the form $$\dot{x} = \cdots$$ $$\dot{y} = \cdots$$ then create a matrix from which I can calculate $\lambda_1,~\lambda_2$. But I keep getting the wrong eigenvalues if I let $x = \dot{y}$.","['matrices', 'eigenvalues-eigenvectors', 'ordinary-differential-equations', 'linear-algebra']"
2386195,Prove if $f$ is increasing then $f'(x) \ge 0$,"Let $f:[a, b] \rightarrow \mathbb{R}$ where $a, b \in \mathbb{R}$ with $a <b$. Suppose $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Suppose $f$ is monotonically increasing on $[a,b]$. Show that $f'(x) \ge 0$ for all $x \in (a,b)$. My attempt: I tried using the Mean Value Theorem, but it doesn't quite seem to work. For example, by the MVT we can conclude that there exists a $c \in (a,b)$ such that $f(b) - f(a) = f'(c) (b-a)$. Which implies that $f'(c) = \frac{f(b) - f(a)}{b-a}$. Now since $f$ is monotonically increasing, $f(b) - f(a) \ge 0$ whenever $b>a$, so $f'(c) \ge 0$. But this only shows for one particular $c \in (a,b)$, and the question asks to show this is true for ALL $x \in (a,b)$. What can I do to complete the proof?","['derivatives', 'real-analysis', 'calculus']"
2386212,Central extensions versus semidirect products,"Consider an extension $E$ of a group $G$ by an abelian group $A$.
$$1 \to A \overset{\iota}{\to} E \overset{\pi}{\to} G \to 1$$
Two special kinds of extensions are: Central Extensions:  $A$ is contained in the centre of $E$. Semidirect products: $\pi$ has a section, i.e. a homomorphism $s : G \to E$ with $\pi \circ s = \mathrm{id}_G$. It perhaps reasonable to think of these two types of extensions as ""orthogonal"" since an extension is both central and a semidirect product if and only if it is split, i.e. there is an isomorphism $E \cong A \times G$ through which $
\iota$ and $\pi$ become identified with the standard inclusion and projection. These two types of extensions are nice in the sense that we can construct all of them in terms of certain external data, namely 2-cocycles and actions. Given $\psi:G \times G \to A$ satisfying $\psi(g_1,g_2)  \psi(g_1 g_2,g_3) = \psi(g_1,g_2 g_3)\psi(g_2,g_3),$ we can devise an extension $E_\psi = A\times G$ with product $(a_1,g_1)(a_2,g_2)=(a_1a_2\psi(g_1,g_2),g_1g_2),$ and $\iota,\pi$ given by the standard inclusion and projection. Given a homomorphism $\theta:G \to \mathrm{Aut}(A)$, define $E=A \rtimes_\theta G$ to be $A \times G$ with product $(a_1,g_1)(a_2,g_2) = (a_1 \theta_{g_1}(a_2),g_1g_2)$. Since the cases of central extensions and semi-direct product are somehow ""orthogonal"", I am tempted to ask the following ill-defined question: Main Question: Can we think of all extensions $E$ of a group $G$ by an abelian group $A$ as being somehow built out of these two orthogonal cases of central extensions and semidirect products? and maybe Followup Question 1: Can arbitrary extensions be constructed out of external data in the same way as central extensions and semidirect products? The data would need to be something which mixes the notions of a 2-cocycle and an action. One further question, a bit frivolous, just popped into mind. Followup question 2: If we want to study extensions $A \to E \to G$ where $A$ is nonabelian , we can still talk about the extensions which are semidirect products. However, the notion of a central extension no long makes any sense. Is there a property $(P)$ which is an appropriate analogue of central extension in this context? Can $(P)$ be chosen so that the trivial split extension is the only extensions which satisfies $(P)$ and is also a semidirect product?","['central-extensions', 'abstract-algebra', 'group-theory', 'group-extensions', 'semidirect-product']"
2386227,Must a one-dimensional conservative system with same period in spite of initial condition be a simple harmonic oscillator?,"All the non-trivial solutions ( i.e. $x(t)\not\equiv 0$ )  of $$\frac{d^2x}{dt^2} = f(x)$$  has the same period, independent of the initial condition. Without loss of generality, set $$f(0)=0$$Does it imply that $$ f = -kx$$?
If not, what  assumptions should be made to reach this conclusion? update: What if $f$ is restricted to be an odd smooth (or at least continuous) function, in which case,  we want to determine the continuous diffenrentiable function $F$ such that $$ \int_0^{x_0}\frac{1}{\sqrt{F(x_0)-F(s)}}\mathrm{d}s $$ is a constant, and let $f(x) = - F'(x)$.","['ordinary-differential-equations', 'dynamical-systems', 'analysis']"
2386279,Problems in motion,"Above is a homework problem I'm trying to solve. Since the problem asks for a new coordinate for A, I put it as $\binom{x}{y}$. Then after two seconds, it must have moved $\binom{-8}{-4}$. Meanwhile, B's position stays the same, and it also moves for $2$ seconds. So $\binom{1}{3} + \binom{8}{-2}$  results in $\binom{9}{1}$, where A with the new initial point should end up as well. Then shouldn't $\binom{x}{y}$ be $\binom{17}{5}$? The answer states it is $\binom{31/3}{23/3}$, but I don't understand what is wrong with my method... Update: Thanks to everyone who affirmed that my method was not in error. However, then another problem arises - the brief solution provided in the answer seems to make sense to me as well, albeit resulting in a different answer! Using parametric equations, for A they have come up with a rectangular equation $x=-1+2y$ and for B they came up with $x=13-4y$. Then equating the two and substituting the resulting y value into one of the rectangular equations, they get that the intersection point is at $x=11/3$ and $y=7/3$. Then setting A's initial point as $\binom{x}{y}$, they use $\binom{x}{y} = \binom{11/3}{7/3}+2\binom{4}{2}$. Is their method wrong then? How?","['algebra-precalculus', 'vectors']"
2386351,Why does $ a_n = \frac {a_{n-1} + \frac {2}{a_{n-1}}}{2}$ converge to an irrational number?,"There is a problem in my textbook that goes like this $$ a_n = \frac {a_{n-1} + \frac {2}{a_{n-1}}}{2}$$ and $$a_0 =1$$ for all $n\ge1$. It is monotonically decreasing sequence of rational numbers and bounded below. However, it cannot converge to a rational number. Then the task is to find the limit. The problem itself is easy but I don't understand how the author judged the limit to be irrational even before solving the question? Is there any property or did they just know the answer beforehand?","['real-analysis', 'cauchy-sequences', 'limits', 'real-numbers', 'sequences-and-series']"
2386393,Find the sphere with radius $\sqrt 6$ which touches an ellipsoid in a given point?,"Find the center of a sphere $(x-x_0)^2+(y-y_0)^2+(z-z_0)^2=6$ which touches ellipsoid $3x^2+2y^2+12z^2=42$ in $(2,3,1)$? I thought of finding the normal vector to the tangent plane to the ellipsoid at point $(2,3,1)$. It is:
$$
n=\langle-\frac{x}{4\sqrt{42-3x^2-2y^2}},-\frac{y}{4\sqrt{42-3x^2-2y^2}},-1\rangle
$$
Then I thought getting normal unit vector: $n_u=\frac{n}{||n||}$. Because the distance from center of the sphere to the point $(2,3,1)$ is its radius it is $\sqrt 6$. So I thought to multiply $n_u$ by $\sqrt 6$ and then plug in the point $(2,3,1)$. I tried doing this but the answer is incorrect. What's wrong with my method?","['multivariable-calculus', 'differential-geometry', 'vectors']"
2386433,The relation between the curvature of level curve along the normal direction.,"Illustrate my problem with figure: For a given curve (solid line '-'). $\phi$ is the signed distance function of curve,  $n$ is the normal vector at $\phi^{-1} = 0$, the dashed line '--' is the level curve with distance $\eta$. Define $\kappa$ is the curvature of level curve, I want find the relation between curvature of $\kappa(x)$ and $\kappa(y)$ where $y = x + \eta n$. More detailed, I want express the relation by the form of 
$ \kappa(y) = a \kappa(x) + O(\eta^2) $ 
where $a$ is constant or geometry on given curve. For a simple example. Let curve $\phi^{-1}=0$ is a circle with radius of $r$. Then we have $ \kappa(y) = (1 + \eta \kappa(x) )^{-1} \kappa(x)$
since $\kappa(x) = \frac{1}{r}$, and $\kappa(y) = \kappa(x+\eta n) = \frac{1}{r+\eta}$. I have try to expansion by Taylor expansion, $\kappa(y) = \kappa(x) + \eta n \cdot \nabla \kappa + O(\eta^2)$. But I want more simple formula since $\nabla \kappa$ is complex to calculate in my problem. Is there some books or reference recommend for this kind problem? Thanks","['curves', 'curvature', 'geometry', 'differential-geometry', 'surfaces']"
2386450,Gibbons-Hawking space is resolution of $\mathbb{C}^2/\mathbb{Z}_k$,"I have a question regarding the Gibbons-Hawking construction of Hyperkahler fourfolds. Let ${a_i}\in\mathbb{R}^3, i =1,\dots,k$ be collinear points and construct a Hyperkahler manifold by starting with a circle bundle over $\mathbb{R}^3\setminus\{a_i\}$ and applying the Gibbons-Hawking ansatz. Call the resulting manifold $X$. In a lot of sources I have read it is asserted that $X$ (with a suitable choice of complex structure) is in fact biholomorphic to the minimal resolution of $\mathbb{C}^2/\mathbb{Z}_k$. They usually state that this is shown by plumbing the $(-2)$-curves in a particular way. However, I am not familiar with this argument nor was I able to find any source explaining it thoroughly. For any further details please see these notes by Andrew Nietzke . In particular, I am interested in the exercise 3.16 (special case of the above question for $k=2$ and $X$ is supposed to be biholomorphic to the cotangent bundle of $\mathbb{CP}^1$) as well as example 3.15. Any pointer or reference would be very helpful. Thanks.","['reference-request', 'differential-geometry', 'algebraic-geometry']"
2386473,Distance from a point to a closed subspace,"Let $H$ be a Hilbert space, $a\in H$ and $H_0$ be a closed subspace of $H$. Is it true that $d(a,H_0)=\max\{\langle u,a\rangle: \|u\|=1,\, u\in H_0^\perp\}$?","['functional-analysis', 'hilbert-spaces']"
2386499,"Isolated types in an ACF over a subfield, in one and higher dimensions","I am trying to figure out what are the isolated 1-types an algebraically closed field $K$, with parameters taken from a subfield $k\subseteq K$, and am almost done except for deciding on one specific 1-type (corresponding to the generic point in the Zariski topology). The context is that in chapter 4 of Marker's book on Model Theory, it was proven that $S_n(k)$ coincides with Spec$k[x_1,..,x_n]$ by a continuous bijection $p\mapsto \{f(x)\in k[x]:f(x_1,...,x_n)=0)\in p\}$, i.e by taking those polynomials that define zero inequalities in the type. Now the closed points $J$ of Spec$k[x]$ correspond to Galois orbits of the algebraic closure of $k$, and thus all such $J$ must come from types of elements of $K$ algebraic over $k$. I proved in a previous exercise of Marker's book that types of algebraic elements are isolated. Question 1: However, with the generic point $(0)$ of Spec$k[x]$, I am not sure.  I suspect it is not isolated. So far, I concluded that if $p$ is the corresponding type of $(0)$ then for every nonzero polynomial $f(x)\in k[x]$, the un-equation $f(x)\neq0$  must lie in $p$. This implies $p$ cannot be isolated by a conjunction of polynomial un-equations. I'm not sure how to show formulas of other shapes cannot isolate $p$. Quesiton 2: (note): Thank you Levon, it turns the proof that the isolated types are correspond precisely to the closed points to the above only required a small note. Now this leads me to wonder in general, i.e. in
in higher dimensions, using Marker's identification of $S_n(k)$, are the isolated types exactly those corresponding to closed points of the Zariski topology?","['model-theory', 'logic', 'algebraic-geometry']"
2386530,Which sets of natural numbers generate fractions which are dense in $\mathbb{R}_{+}$?,"This is an extension of this recent question. We assume $0 \notin \mathbb{N}$. Let's say a subset $S \subseteq\mathbb{N}$ is quotient-dense if $$\overline{\left\{\frac{p}{q} :p,q \in S\right\}} = \mathbb{R}_{\geq 0}$$ Can we characterize the quotient-dense subsets of the natural numbers? Some obvious examples are $\mathbb{N}$ itself, and subsets of the form $\{an+b:n \in \mathbb{N}\}$ for fixed natural $a,b$. $S$ must obviously be countably infinite. However, this is not a sufficient condition. If we consider the set $X$ of all powers of two, then $\frac pq = 2^k$ for some $k \in \mathbb{Z}$. The closure of the set is certainly not $\mathbb{R}_{\geq 0}$ The answers in the linked question above show that the set of primes is quotient-dense. One guess is that $S$ is quotient-dense iff $$\sum_{k \in S} \frac{1}{k} = +\infty$$ EDIT: The guess above is wrong, as shown in the comments.","['real-analysis', 'rational-numbers']"
2386626,Intuition for Artin's lemma,"Let $E$ be a field. If $G$ is subgroup of $\text{Aut} (E)$, then the set:
$\text{Inv}(G) = \{a \in E: \forall \ \eta \in G, \eta(a) = a\}$ is a subfield of $E$. Artin's lemma is: Let $E$ be a field, $G$ be a finite subgroup of $\text{Aut}(E)$, and $F = \text{Inv}(G)$. Then, $[E:F] \le |G|$. The proof in the textbook is not very satisfying. It is not clear what is going on. I completely understand it, but it seems to come out of nowhere. Can someone provide some intuition? Here's the proof: ($(17)$ is the equation $[E:F] \le |G|$) (Just ignore the bit about linear equations..)","['intuition', 'galois-theory', 'abstract-algebra', 'group-theory', 'field-theory']"
2386644,Given the cumulative distribution function find a random variable that has this distribution.,"We are given a Cumulative distribution function $(CDF)$ how do we find a random variable that has the given distribution? Since there could be a lot of such variables we are to find anyone given that the function is differentiable and increasing. I read somewhere that we could simply take inverse of the function(if it exists) and that would be the Random variable you are looking for, but I don't understand it, even if it's true(?). If that's true, why? Else, how do we find a random variable for the distribution?","['statistics', 'probability', 'random-variables', 'probability-distributions']"
2386647,Is a finite order endomorphism a rotation?,"Let $V$ be a real $2$-dimensional vector space, and $T\colon V\to V$ be an endomorphism such that
$$
T^q = Id \qquad \textrm{and} \qquad T^j\not= Id\quad\textrm{if}\ 0<j<q,
$$
where $T^0 = Id$ and $T^{j-1}\circ T = T^{j}$.
Moreover, let $T$ be orientation-preserving. Can we say that there exist a basis $B$ for $V$ such that $T$ is the rotation of $\pm\frac{2\pi}{q}$, that is
$$
[T]_B = \begin{bmatrix}\cos(\frac{2\pi}{q}) & \pm\sin(\frac{2\pi}{q})\\
\mp\sin(\frac{2\pi}{q}) & \cos(\frac{2\pi}{q})\end{bmatrix}?
$$",['linear-algebra']
2386650,A function satisfying a vanishing property,"Is there a smooth function $f:[0,\infty)\rightarrow [0,\infty)$ such that $f(x)\rightarrow 0$ as $x\rightarrow 0$, but $f'(x)/f(x)$ does not go to $\infty$ as $x\rightarrow 0$? I'm especially interested in examples where $f'(x)/f(x)$ does not achieve arbitrarily high values as $x\rightarrow 0$. Edit: Apologies, I had meant ""does not go to $\infty$"" in the original version. Now corrected.","['calculus', 'functions']"
2386676,Comparing the proofs of Riemann Roch & Serre Duality in Forster's and Miranda's book,"I am learning about Riemann surfaces, using the books of Forster and Miranda. When stating and proving Riemann-Roch and Serre Duality in both books a space appears which seem somehow related, but this relation is not immediately clear (to me). These spaces are the following: Let $X$ be a compact Riemann surface. In Miranda's book $$H^1(D)= \tau [D](X)/\operatorname {Im}(\alpha_D),$$ where $\tau[D](X)$ is the group of Laurent tail divisors on $X$ corresponding to the Divisor $D$ and $\alpha_D$ is the truncation map which removes all terms of order $-D(p)$ or higher from the Laurent series of a meromorphic function. I.e. $H^1(D)$ measures the failure of being able to solve Mittag-Leffler problems on $X$ of finding a meromorphic function with a given tail. In Forsters book the space of interest is the first cohomology group $H^1(X,\mathcal O_D)$ with $\mathcal O_D$ the sheaf of meromorphic functions $f$ with $\operatorname{div} f \ge -D$. With $\Omega^1_D$ the sheaf of meromorphic 1-forms $\omega$ satisfying $\operatorname{div} \omega \ge -D$. Serre duality then reads as
$$ H^1(D)= \Omega^1_{-D}(X),~~~ \text{ resp } ~~~H^1(X,\mathcal O_D)= \Omega^1_{-D}(X).$$ In both cases the duality map is given by the residue map. And the proofs are somehow similar, as the strategy is to shaw that the suitable defined resiude map is an isomorphism. In order to calculate the Residue of a cohomology class, Forster introduces so called Mittag-Leffler distributions, which for a given open covering $\mathcal U$ are cochains $\mu \in C^0(\mathcal U, \mathcal M^1)$ with $\delta \mu \in Z^1(\mathcal U, \Omega^1)$, i.e. $\mu_j- \mu_i$ is holomorphic on $U_i \cap U_j$ and defines $\operatorname{Res} (\mu)= \sum_{a \in X} \operatorname{Res}_a (\mu)$. He then proves $\operatorname{Res}([\delta \mu])= \operatorname{Res}(\mu)$. I got the feeling, that both authors do basically the same, but use different words to describe it. Using both versions of Serre duality, the spaces are isomorphic, but there seems to be a more direct connection. So what is it? In particular, how are Mittag-Leffler Distributions related to Mittag-Leffler problems? So far I could only observe that for a Mittag-Leffler distribution the Laurent series of the $\mu_i$ have the same principal part. The bounty ended and unfortunately the question was not answered satisfactorily, even a year later. Hopefully somebody can step in.","['riemann-surfaces', 'complex-geometry', 'algebraic-geometry', 'sheaf-cohomology', 'analytic-geometry']"
2386683,Finding the least segment of moving point?,"Let $A=(0,1)$ and $B= (2,0)$ in the  plane. Let $O$ be the origin  and  $C=(2,1)$. Let $P$ move on the segment $OB$ and let $Q$ move on the segment $AC$. Find the coordinates of $P$ and $Q$ for  which  the length of the path consiting of the  segmet $AP$,$PQ$ and $QB$ is the least. I was trying to solve this question . I located all of the coordinates of  $A=(0,1)$ , $B=(2,0)$ and $C =(2,1)$. I was taking the mid point  of $AC =Q(\frac{1}{2},1)$  and  midpoint of $AB=P(1,\frac{1}{2})$, I think this will be the least segment.
I have doubts whether my answer is correct or not. I would be very thankful to anybody who helps me.","['euclidean-geometry', 'geometry']"
2386710,"Why is the word ""complement"" used in set theory?","Maybe this should have been on the English Exchange, but why do we use the word ""complement"" in set theory?  If I have: $$(A \cup B)'$$ Why does ""complement"" mean everything but the union?  Is it because it is ""all the things"" that the original operation is not, thus it ""completes"" it? I looked at the dictionary and wasn't sure. Edit:  $(A \cup B)'$ was only an example so I could use the complement mark.  It was picked at random and was only meant to ask the question of what the word meant.  It was not specifically about a union.  I could have probably picked anything that allowed me to use the ""tick mark"" to indicate complement.  My MathJax is not good and cumbersome for me, so I only used the single example.","['terminology', 'elementary-set-theory']"
2386745,"$\{ x\in[n,n+1) : m(f^{-1}(\{x\}))>0\}$ has measure zero","I am trying to show that if $f: \mathbb{R} \to \mathbb{R}$ is measurable, then the set $S=\{x\in \mathbb{R} : m(f^{-1}(\{x\}))>0\}$ has measure zero where $m$ denotes the Lebesgue measure. We can show that for each integer $n$, $S\bigcap [n,n+1]=\{x\in [n,n+1]: m(f^{-1}(\{x\}))>0\}$ has measure zero. But I got stuck here.
can someone help me?
Thanks","['real-analysis', 'measure-theory']"
2386795,Evaluate integral: $\int_0^1 \frac{\log^3(1-x)}{x}dx$,"I need your help to evaluate the following integral: $$I=\int_0^1 \frac{\log^3(1-x)}{x}dx$$ Using the fact that for $|x|<1$ I get $$\log(1-x)=-\sum_{n=1}^{\infty}\frac{x^k}{k}$$ One can rewrite $I$ as $$I=-\sum_{k=0}^{\infty}\frac{1}{k+1}\left(\int_0^1 x^k\log^2(1-x)dx\right)$$ I tried to rewrite $\log(1-x)$ as a sum but I get some ""monstrous"" summation to calculate.","['improper-integrals', 'summation', 'integration']"
