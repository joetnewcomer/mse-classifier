question_id,title,body,tags
1259887,$5$ dimensional space over $\mathbb{R}$,"When coming up with a double cover of $SO(5)$, I used conjugation by matrices of the form $$\begin{pmatrix} r & q\\ \overline{q} & r \end{pmatrix}$$ where $r\in\mathbb{R}$ and $q$ is a quaternion. These matrices are clearly $5$ dimensional over $\mathbb{R}$, but I'm wondering if someone can identify this space by name so that I can find more information. Edit: I should have also added that for any matrix $A$ of this form, $A=A^*$ where $*$ denotes conjugate transpose. However, this requirement follows from how $A$ was defined, so mentioning it again doesn't add information, but rather is a (potentially) useful observation.","['abstract-algebra', 'lie-groups', 'vector-spaces', 'matrices']"
1259892,Why is something not a field if it's a proper class?,"Why is it a convention to say that, for example, the nimbers and surreal numbers aren't fields because they aren't sets? Is it just pedantry?","['abstract-algebra', 'foundations']"
1259928,How to explain why the probability of a continuous random variable at a specific value is 0?,"Consider X as a continuous random variable which can assume any value in [0, 1]. It is known that P(X=x)=0 where P is the probability density function. I want to understand this intuitively. The math insight article helps me somewhat: In other words, the probability that the random number X is any
  particular number x∈[0,1] (confused?) should be some constant value;
  let's use c to denote this probability of any single number. But, now
  we run into trouble due to the fact that there are an infinite number
  of possibilities. If each possibility has the same probability c and
  the probabilities must add up to 1 and there are an infinite number of
  possibilities, what could the individual probability c possibly be? If
  c were any finite number greater than zero, once we add up an infinite
  number of the c's, we must get to infinity, which is definitely larger
  than the required sum of 1. In order to prevent the sum from blowing
  up to infinity, we must have c be infinitesimally small, i.e., we must
  insist that c=0. But, what if I choose, c=1/N (where N is that large number) for the uniform case? The sum will still be 1 as far as I can understand. For the non-uniform case, I can pick some 0's and others non-zeros and still be theoretically able to get a sum of 1 for all the possible values. Anything that helps me understand this clearly will be of immense help. The answer here helps but I still don't get it.","['probability', 'random-variables']"
1259943,Equivalence Classes Output,"I understand that an equivalence relation is a set that is reflexsive, symmetric, and transitive. I don't quite understand equivalence classes though. For example:
What would the equivalence class be of the equivalence relation {(0.0),(1,1),(2,2),(3,3)} if the (original?) set is {0,1,2,3} ?","['equivalence-relations', 'discrete-mathematics']"
1259965,Four 6-sided dice are rolled. What is the probability that at least two dice show the same number?,"Am I doing this right? I split the problem up into the cases of 2 same, 3 same, 4 same, but I feel like something special has to be done for 2 of the same, because what if there are 2 pairs (like two 3's and two 4's)? This is what I have: For 2 of the same: $5\times 5\times 6\times {4\choose 2}=900$ For 3 of the same: $5\times 6\times {4\choose 3}=120$ For 4 of the same: $6\times {4\choose 4}=6$ Combined: $900+120+6=1026$ Total possibilities: $6^4=1296$ Probability of at least 2 die the same: $\frac {1026}{1296}\approx 79.17$% Confirmation that I'm right, or pointing out where I went wrong would be appreciated. Thanks! Sorry if the formatting could use work, still getting the hang of it.","['discrete-mathematics', 'probability', 'combinatorics']"
1259980,"Proving that $\max(x_1, x_2, x_3) = x_1 + x_2 + x_3 - \min(x_1, x_2) - \min(x_1, x_3) - \min(x_2, x_3) + \min(x_1, x_2, x_3)$","$$\max(x_1, x_2, x_3) = x_1 + x_2 + x_3 - \min(x_1, x_2) - \min(x_1, x_3) - \min(x_2, x_3) + \min(x_1, x_2, x_3)$$ Is there a more elegant proof to this than just trying out all the possibilities and showing that it works?  For example, assuming that the numbers are distinct, this statement is true if
$x_1 < x_2 < x_3$ since the equation would yield $x_1 + x_2 + x_3 - x_1 - x_1 - x_2 + x_1 = x_3$, etc. It also seems to work if the numbers are not distinct, but then there are even more combinations to try.  Also, interestingly, it looks like inclusion-exclusion but that might be a red herring.  Also not sure how to tag this question...",['combinatorics']
1259996,Question about the Definition of Differential Forms,"Question: Why does the definition of a differential form guarantee that when we do integration using differential forms, it is the same as the usual Riemann integral (before we introduce the concept of differential form)? Example: If I want to prove Stoke’s Theorem, and I define the “differential form”, then the “differential form” may have certain rules of calculation. Then I use the “differential form” to prove the Stoke’s Theorem. But why is Stoke’s Theorem in the form of “differential forms” is equivalent to the usual Stoke’s Theorem (which we learn in multivariable calculus) ?","['differential-forms', 'multivariable-calculus']"
1260015,Normalizing factor for product of Gaussian densities - interpretation with Bayes theorem,"The normalizing factor for the product of two multivariate Gaussian densities, $f(x)$ and $g(x)$ with mean vectors $a$ and $b$ respectively, and covariance matrices $A$ and $B$ respectively, is itself a Gaussian with exponent: $-0.5(a-b)^T(A+B)^{-1}(a-b)$ i.e. the normalizing factor $c$ is: $c=\cfrac{1}{\sqrt{2\pi\begin{vmatrix}A+B\end{vmatrix}}}e^{-\cfrac{1}{2}(a-b)^T(A+B)^{-1}(a-b)}$ At first sight, I'd hence interpret the normalizing factor as the value of a (hypothetical) density function with mean $b$ and covariance matrix $A+B$, evaluated at $a$. Consider now an example, where $f(x)$ and $g(x)$ represent densities of two different statisticians' estimates for $x$. Statistician $A$ whose estimate has density $f(x)$ decides to update his estimate, after he saw the estimate for $x$ from the other statistician, named $B$: $x_b$. $A's$ new estimate, i.e. an estimate conditional on $x_b$ has the density: $f(x_a|x_b)$. Now $B$ would in turn use $A's$ initial estimate to update her own estimate. The new estimate of $B$, conditional on the estimate of $A$ would have the density: $g(x_b|x_a)$. Applying the Bayes rule for densities: $g(x_b|x_a)=\cfrac{f(x_a|x_b)g(x_b)}{f(x_a)}$ So here, the normalizing factor is (also) $f(x_a)$ the unconditional (marginal) density of $x_a$. Unfortunately, I cannot see the link between $-0.5(a-b)^T(A+B)^{-1}(a-b)$ and that marginal density.  What am I missing? P.S.:  how to derive the expression above is indicated in the answer to the question given here: Product of two multivariate Gaussian pdfs - normalizing constant - where online references are given as well.","['bayesian', 'probability-distributions', 'normal-distribution', 'bayes-theorem', 'probability']"
1260050,Are derivatives linear maps?,"I am reading Rudin and I am very confused what a derivative is now. I used to think a derivative was just the process of taking the limit like this
$$\lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{(x+h)-x}$$ But between Apostol and Rudin, I am confused in what sense total derivatives are derivatives. Partial derivatives much more resemble the usual derivatives taught in high school $$f(x,y) = xy$$ $$\frac{\partial f}{\partial x} = y$$ But the Jacobian doesn't resemble this at all. And according to my books it is a linear map. If derivatives are linear maps, can someone help me see more clearly how my intuitions about simpler derivatives relate to the more complicated forms? I just don't understand where the limits have gone, why its more complex, and why the simpler forms aren't described as linear maps.","['multivariable-calculus', 'derivatives']"
1260052,"For what real values of $a$ does the range of $f(x)$ contains the interval $[0,1]$?","Question : For what real values of $a$ does the range of $f(x) = \cfrac{x+1}{a+x^2} $ contains the interval $[0,1]$? My doubt lies in the further preceding of this question. The book states : Let $y = \cfrac{x+1}{a+x^2} $ . Which implies - $$yx^2 - x + (ay-1) = 0$$ 
  has real roots for every $\color{blue}{y \in [0,1] }$. I'm not sure how it concluded that it is real for the given interval of y. I'm also known to the fact that : $D \ge 0$ for the quadratic to have real roots. And here, $ D = 1 - 4y(ay-1) \ge 0$ Not sure how to go on from here.","['quadratics', 'functions']"
1260060,14 pencils handed out to 6 people. Each person has at least 1 pencil. Person 6 no more than 3 pencils.,"We have 14 indistinguishable pencils and we want to hand out all of the pencils to 6 people and we want everyone to get at least one pencil. However, we do not want person 6 to get more than 3 pencils. How many different ways could this be done? Is the answer $\binom{10}{6}$ correct? Reserve 1 pencil per person and reserve 3 pencils to person 6. So we now have 6 pencils left that we need to pass out to 5 people (since person 6 can no longer receive a pencil). So the problem is the same as asking how many ways can pass out 6 pencils to 5 people? Or am I looking at this wrong?","['discrete-mathematics', 'combinatorics', 'permutations']"
1260093,Number of integral solutions of $\text{xyz}=3000$,I want to find the number of integral solutions of the equation $$xyz=3000$$ I have been able to solve similar sums where the number on the right hand side was small enough to calculate all the factors of. Such as $xyz=24$ or $xyz=30$. What is the proper method to solve such a problem when the number is too big to consider all the factors of it?,['combinatorics']
1260094,Physical significance and graphical point of view of second derivative of a function $f''(x)$ .,"This was just going through my mind - $f'(x)$ represents slope of a function. Then what does $f''(x)$ represent? For example, we define strictly increasing and strictly decreasing functions in some parts as follows: Strictly Increasing functions : ( If $x_1 < x_2$ then $f(x_1) < f(x_2)$ ) for all $x$ belonging to the domain of the function. Graphically we may represent it as: Now, it can be classified as : $(i)$ Concave up when $f'(x) > 0$ and $ f''(x) > 0 \ \forall \ x \in \text{domain} $ $(ii)$ When $f'(x) > 0$ and $f''(x) = 0$ ; $\forall x \in \text{domain}$ $(iii)$ Concave down when $f'(x) > 0$ and $f''(x) < 0 , \ \forall x \in \text{domain}$ Similarly, these classifications can be done for Strictly Decreasing Functions . To sum up, following image describes: Now, as you might have noticed, there is second derivative coming into play in order to define the shape of the graph, that is, it is whether concave up or concave down . I'm a bit confused that how did it help in deciding the shape of the graph...! Is their any graphical representation for second derivative of a function? Any physical significance of $f''$ ?","['derivatives', 'graphing-functions', 'functions']"
1260138,Method of Steepest descent integral,I am looking to evaluate the following asymptotic integral: Find the leading term of asymptotics as $\lambda\to\infty$ $$I(\lambda)=\int_0^1\cos(\lambda x^3)dx$$ Using method of steepest descent along a certain contour. I am having trouble approaching this problem as I don't understand it well. Any help would be appreciated.,"['definite-integrals', 'contour-integration', 'integration', 'asymptotics', 'complex-analysis']"
1260157,Moments of a random variable in terms of its cumulative distribution function,"Consider a random variable $X$ with distribution function $F(x)$. Calculate the $r$th moment of $X$, $\mathbb E X^r$. I read that the desired moment can be calculated as follows. $$
\begin{align}
\mathbb E X^r &= \int_0^\infty x^r dF(x) - \int_0^\infty (-x)^r dF(-x) \\[5pt]
&=r \int_0^\infty x^{r-1} \left[ 1-F(x)+(-1)^rF(-x) \right] dx.
\end{align}
$$ Could anyone explain to me why this is true, please? In addition, when do we need to find the moments in this way? Thank you!","['probability-theory', 'statistics', 'expectation']"
1260168,Prove that $A\subseteq B$ if and only if $A^{C}\cup B=\mathscr{U}$,"Prove that $A\subseteq B$ if and only if $A^{C}\cup B=\mathscr{U}$. I know we have to show that: if $A\subseteq B$ then $A^{C}\cup B=\mathscr{U}$ if $A^{C}\cup B=\mathscr{U}$ then $A\subseteq B$ my question is regarding whether my approach is correct in proving this part: $$\text{""if $A\subseteq B$ then $A^{C}\cup B=\mathscr{U}$""}$$ Proof: Suppose $A$ and $B$ are sets such that $A\subseteq B$. We must show that $A^{C}\cup B\subseteq \mathscr{U}$ follows from $A\subseteq B$ and $\mathscr{U}\subseteq A^{C}\cup B$ also follows from $A\subseteq B$. $A^{C}\cup B\subseteq \mathscr{U}$ follows from $A\subseteq B$, as by definition of subset, for all $x$, if $x\in A$ then $x\in B$. So by definition of subset and union, if $x\in A^{C}$ or $x\in B$ then $x\in\mathscr{U}$. Since $x\in B$, $A^{C}\cup B\subseteq \mathscr{U}$. $\mathscr{U}\subseteq A^{C}\cup B$ also follows from $A\subseteq B$, as by definition of subset, for all $x$, if $x\in A$ then $x\in B$. So by definition of subset and union, if $x\in\mathscr{U}$ then $x\in A^{C}$ or $x\in B$. Since $x\in B$, we have $\mathscr{U}\subseteq A^{C}\cup B$.","['elementary-set-theory', 'proof-verification', 'proof-writing']"
1260172,Constructing Pythagorean Polygons,"I ran into this idea of ""Pythagorean Polygons"" on a problem from Project Euler, and I thought of an interesting question. A ""Pythagorean Polygon"" is defined as a polygon that is cyclic and has its longest side be the diameter of a circle. It also always has integer sides. Now, is there a way to construct a such polygon with any number of sides, where the number of sides is greater than $3$? Obviously $3$ is true, and $4$ you can do by reflecting a right triangle so that we have an isosceles trapezoid, but I'm not sure what else I can do from here.",['geometry']
1260233,"Inductive Proof that $k!<k^k$, for $k\geq 2$.","Call $P(k): k!<k^k$, for $k\geq 2$ Test it out with 2, and it's true ($2<4$). Assume that $P(k)$ is true for some $k\geq2$. Then show that $P(k+1)$ is true. $P(k+1): (k+1)!<(k+1)^{k+1}$ Ministep: $(k+1)!=k!(k+1)$ Ministep: $(k+1)^{k+1}=(k+1)^{k}(k+1)$ $k!(k+1)<(k+1)^{k}(k+1)$ Can I just pull out that (k+1) and call it a day? That will make $k!<(k+1)^{k}$. We're already assuming that $k!<k^k$, for some integer $k\geq 2$, and I don't think I need to prove that $k^{k}<(k+1)^{k}$. I mean, I guess I could go prove that as well, but let's assume for the moment that I don't have to. Since we're assuming the original $P(k)$ is true, I should be able to write $k!<k^{k}<(k+1)^{k}$ right? That would prove the $k+1$ case. Is this the correct way to go about things? It seems too easy, but makes sense. Thanks for confirmation and/or pointing out mistakes!","['factorial', 'induction', 'discrete-mathematics', 'inequality']"
1260247,"Proving a polynomial has a solution in the interval (0,1) [duplicate]","This question already has answers here : Prove that $\sum_{k=0}^n a_k x^k = 0$ has at least $1$ real root if $\sum_{k=0}^n \frac{a_k}{k+1} = 0$ (2 answers) Closed 5 years ago . I have no idea how to start this problem. I am assuming that the Mean Value Theorem is needed in the proof but I am not exactly sure how to apply it to the given polynomial. Any hints/help would be appreciated. If $ \frac{a_n}{n+1} +  \frac{a_{n-1}}{n} + \cdot \cdot \cdot + \frac{a_1}{2} + a_0 = 0 $, prove that the polynomial equation $ a_n x^n + a_{n-1}x^{n-1} + \cdot \cdot \cdot + a_1x + a_0 = 0$ has a solution in $(0,1)$.","['analysis', 'polynomials']"
1260257,"If the Greeks had been four dimensional, would they have been able to derive the pi squared coefficient for the hypersphere volume without calculus?","I was reading about Archimedes' pre-calculus proof of the volume of the sphere and I realized that the trick he uses (volume of hemisphere + volume of cone = volume of cylinder) doesn't generalize to hyperspheres, so I was wondering if there's any way of proving the volume of a hypersphere is $\frac{π^2}{2}R^4$ with classical methods.","['euclidean-geometry', 'alternative-proof', 'volume', 'geometry', 'spheres']"
1260270,Evaluating $\lim\limits_{n\rightarrow \infty} \frac1{n^2}\ln \left( \frac{(n!)^n}{(0!1!2!...n!)^2} \right)$,"Evaluating $$\lim\limits_{n\rightarrow \infty} \frac1{n^2}\ln \left( \frac{(n!)^n}{(0!1!2!...n!)^2} \right)$$ I'm not quite sure where to start in evaluating this. Some pointers, or a solution, would greatly be appreciated.","['summation', 'gamma-function', 'limits']"
1260330,Finding the two different conjugacy classes in $A_n$ after splitting criterion,"Suppose we have a group $A_n$ for some $n$ (maybe take $A_5$ as an example). We find the conjugacy classes of $S_n$ which are determined by cycle type. Then we use the splitting criterion , http://groupprops.subwiki.org/wiki/Splitting_criterion_for_conjugacy_classes_in_the_alternating_group , as we find a conjugacy class that splits, eg that represented by $(12345)$ in $S_5$. Is there a way of computing/spotting the two conjugacy classes. One will be $(12345)$ but is there a way of getting the other preferably without doing any calculations. Looking for general method.","['group-theory', 'symmetric-groups']"
1260334,About a relation of non-discernability between (classes of) finitely generated groups.,"Let $G$ be the set of finitely generated groups up to isomorphism. Now define $B$ and $C$ two  finitely generated groups to be not discernable if one can find a  finitely generated group $A$ such that $A\times B$ is isomorphic to $A\times C$ . This defines an equivalence relation on $G$ (straightforward verification). The questions I will ask are related to this post  : For groups $A,B,C$, if $A\times B$ and $A\times C$ are isomorphic do we have $B$ isomorphic to $C$? In this post, from the counter-example for $(1)$ not every class is reduced to one (you can not always discern a group from another). From the counter-example to $(2)$ there are many classes (using the abelianization one can discern groups using edit 2). So the ""non-discernability"" is clearly a non-trivial relation (so this is different from  both the finite groups case and the general groups case). I recall that we are in $G$ and hence all groups are finitely generated. (3) Are there groups $B$ such that the ""non-discernability"" class is reduced to $B$ ? If yes, can we caracterize them ? (4) Is the trivial group ""non-discernability"" class reduced to the trivial group? If not, can we caracterize groups within this class ? ( $\infty$ ) Give invariants for the ""non-discernability"" classes.","['group-theory', 'finitely-generated']"
1260352,Pedantic question on function notation and the meaning of domain,"Suppose we have a function $f: A\to B$. Then we know, without specifying what $f$ is, that $f$ may or may not map to every element $b\in B$. If $f$ does map to every element $b\in B$ then it's surjective. However, does the notation $f:A\to B$ imply that every element $a\in A$ is mapped to some unique $b\in B$? Furthermore, is there a difference between domain and natural domain? If there is, how can we tell whether $A$ is a/the natural domain from the notation $f: A\to B$? Suppose we consider the function defined this way: $f: \mathbb{R}\to \mathbb{R}, f=\frac{1}{x}$ Clearly, $f$ is not defined at $x=0$. Would it therefore be incorrect to write  $f: \mathbb{R}\to \mathbb{R}, f=\frac{1}{x}$ instead of $f: \mathbb{R}\backslash \{0\}\to \mathbb{R}, f=\frac{1}{x}$? The question is a bit clumsy. You are welcome to ask for clarification if this isn't clear enough.","['notation', 'functions']"
1260354,Solve $y' + \frac1y + \frac1x =0$ Differential Equation,"Do you have any suggestions for how to sole this differential equation?
$y'+\frac1x + \frac1y =0$ 
? :)
I tried solving this by changing variable in the form of $v=x^\alpha*y^\beta$ but it didn't work! 
Do you have any other ideas?! :)",['ordinary-differential-equations']
1260409,A valid method of finding limits in two variables functions?,"I was wondering if in finding the limit of a two variables function (say, $F(x,y)$), I can choose the path by let $y=f(x)$, then find the limit in the same way of that in one variable functions. For example,
$$
\lim_{(x,y) \to (0,0)} \frac{xy}{x^2+xy+y^2}
$$ (It has no limit there by choosing first $y=0$ and then $y=x$) So I'm asking if the following procedures are correct: Let $y=f(x)$ where $f(0)=0$ since the function passes $(0,0)$ The function then becomes:
$$
\frac{xf(x)}{x^2 + xf(x) +f(x)^2}
$$
Then it's an indeterminate form $[0/0]$, so I differentiate,
$$
\frac{xf'(x)+f(x)}{2x + xf'(x)+f(x) +2f(x)f'(x)}
$$
Then it's still $[0/0]$ so I differentiate again,
$$\frac{xf''(x)+2f'(x)}{2+xf''(x)+2f'(x)
+2f(x)f""(x)+2f'(x)^2}$$ By substituting $x=0$, I get
$$\lim F(x,y) = \frac{2f'(x)}{2+2f'(x)+2f'(x)^2}$$ Since $f'(x)$ depends on the path I choose, the limit depends on the path I choose also.
Thus, the limit at $(0,0)$ does not exist. So that's all, the question I have are is this a valid method to determine existence of limits? is this a valid method to find the limit? (My teacher says it wont work for 2.) but I'm still unclear about his explanations) (Sorry if I made any mistake or this is a very stupid question, I'm very new to this site and this is my first question, thank you in advance!)",['multivariable-calculus']
1260437,Equations with Sinus. How to find equation solution?,"I don't understand how to get a solution for sinus equation. I have: $$\sin x = \frac{1}{2}$$ \begin{align*}
x & = (-1)^k \cdot \arcsin\left(\frac{1}{2}\right) + 180^\circ \cdot k, k \in \mathbb{Z}\\
  & = (-1)^k \cdot 30^\circ + 180^\circ \cdot k, k \in \mathbb{Z}
\end{align*} Answer: $x = \ldots$ How to find x?","['algebra-precalculus', 'trigonometry']"
1260448,Gradient vector derived from the metric tensor,"According to Frankel's book ""The Geometry of Physics"", the components of a contravariant gradient vector can be obtained from the inverse of the metric tensor as follows (in section 2.1d, Page 73): $$
(\nabla f)^i = \sum_j g^{ij} \frac{\partial f}{\partial x^j},
$$ while the metric sensor is: $$
g_{ij} = \bigg\langle \frac{\partial}{\partial x^i}, \frac{\partial}{\partial x^j} \bigg\rangle.
$$ Take the spherical coordinate as example, \begin{align}
x &= r\sin\theta\cos\phi \\
y &= r\sin\theta\sin\phi \\
z &= r\cos\theta
\end{align} and let $J$ be the Jacobi matrix $$
J = \begin{pmatrix}
\sin\theta\cos\phi & -r\sin\phi\sin\theta & r\cos\phi\cos\theta \\
\sin\theta\sin\phi & r\cos\phi\sin\theta & r\sin\phi\cos\theta \\
\cos\theta & 0 & -r\sin\theta
\end{pmatrix},
$$ the metric tensor can be obtained as $$
\left( g_{ij} \right) = J^TJ =
\begin{pmatrix}
1 & 0 & 0 \\
0 & r^2\sin^2\theta & 0 \\
0 & 0 & r^2
\end{pmatrix}.
$$ Finally, the contravariant gradient vector can be obtained as follows according to the first equation: $$
\nabla f = \begin{pmatrix}\displaystyle{
\frac{\partial f}{\partial r} \\
\frac{1}{r^2\sin^2\theta} \frac{\partial f}{\partial \phi} \\
\frac{1}{r^2} \frac{\partial f}{\partial \theta}}
\end{pmatrix}.
$$ However, the correct answer is $$
\nabla f = \begin{pmatrix}\displaystyle{
\frac{\partial f}{\partial r} \\
\frac{1}{r\sin\theta} \frac{\partial f}{\partial \phi} \\
\frac{1}{r} \frac{\partial f}{\partial \theta}}
\end{pmatrix}.
$$ I don't know why I cannot get the correct answer using metric tensor. Could you please help me figure it out? Thank you!",['differential-geometry']
1260470,Divergent series of random variables,"I've been trying to prove that given a sequence of independent random variables with identical distribution $\{X_n\}_{n \in \mathbb{N}}$ such that $P(X_1 \neq 0)>0$, so also $P(X_i \neq 0) >0 \ \  \forall i$,  the series $$\sum_{n \in \mathbb{N}} X_n \ \  \text{is diverent almost surely}.$$ That is, I need to prove that $P(\omega| \ \exists \varepsilon>0 : \forall N \in \mathbb{N} \exists m, n \ge N : |S_m(\omega) - S_n(\omega)| > \varepsilon)$. We are dealing with a series of random variables, so I thought I could use Borel-Cantelli lemma. It implies that the series $\sum_{n \in \mathbb{N}} P(X_m \neq 0) $ is divergent. This is because $\forall n \in \mathbb{N} : P(X_n \neq 0) = c >0$, because the variables have identical distributions. But I don't think it is helpful, because by Markov's inequality we also have for $a \ge 0$: $$\mathbb P (|X| \ge a) \le \frac{\mathbb E(|X|)}{a}.$$ So it would seem that the series is divergent in $L^1.$ But why is it divergent almost surely? Could I use the law of large numbers? If so, how? The variables satisfy all necessary conditions for the following to hold: $$\frac{S_n}{n} = \frac{X_1 + ... + X_n}{n} \to \mathbb{E}X_1 \ \ \text{a.s.}$$ But does that imply that the series is divergent? Could you help me finish that?","['probability-theory', 'convergence-divergence', 'random-variables']"
1260495,Prove that an upper triangular matrix is invertible if and only if every diagonal entry is non-zero.,"Prove that an upper triangular matrix is invertible if and only if every diagonal entry is non-zero. I have proved that if every diagonal entry is non-zero, then the matrix is invertible by showing we can row reduce the matrix to an identity matrix. But how do I prove the only if part?","['linear-algebra', 'matrices']"
1260502,Is it possible that $(f\circ g)(x)=x$ and $(g\circ f)(x)\ne x$?,"Is it possible that $(f\circ g)(x)=x$ and $(g\circ f)(x)\ne x$ In other words, To show $f$ and $g$ are inverse, is it enough to show $(f\text{ o }g)(x)=x$? I have never witnessed a case in which the other composition is not $x$, if one of them equals $x$. EDIT : So this is not always true, but what are the conditions under which this is true. I think this is more interesting !","['inverse', 'calculus', 'algebra-precalculus', 'functions']"
1260510,Rewriting $e^{2xi} + e^{-2xi}$,"I'm trying to solve the differential equation $y^{''} + 2y^{'} + 5y = 0$ I get that the general solution is:
$$y = Ae^{-x + 2xi} + Be^{-x-2xi} $$ $$y = e^{-x}(Ae^{2xi} + Be^{-2xi}) $$ $$ y = e^{-x}(A\cos(2x) + Ai\sin(2x) + B\cos(-2x) + Bi\sin(-2x))$$ In the book the answer is written as:
$$y = e^{-x}(A\cos(2x) + B\sin(2x))$$ How can my answer be rewritten to equal the book's? Or did I do wrong somewhere?","['homogeneous-equation', 'ordinary-differential-equations']"
1260541,Prove that if G has a faithful complex irreducible representation,"I am struggling with the proof that if G has a faithful complex irreducible representation then $Z(G)$ is cyclic: Let $\rho:G \rightarrow GL(V)$ be a faithful complex irreducible representation. Let $z \in Z(G)$ . Consider the map $\phi_z: v \mapsto zv$ for all $v \in V$ . This is a G-endomorphism on $V$ , hence is multiplication by a scalar $\mu_z$ "" I keep coming across the term G-homomorphism. For instance in Schur's Lemma ...""Then any G-homomorphism $\theta:V \rightarrow W$ is 0 or an isomorphism"". What exactly is G-homomorphism? Then the map $Z(G) \rightarrow \mathbb{C}^\times, z \mapsto \mu_z$ , is a representation of $Z$ and is faithful (since $\rho$ is). Thus $Z(G)$ is isomorphic to a finite subgroup of $\mathbb{C}^\times$ , hence is cyclic. What is the justification for this last sentence?","['group-homomorphism', 'group-theory', 'linear-algebra', 'representation-theory']"
1260571,Experiment: Roll three 6-sided dice.,"Are the following probabilities correct? I'm not very confident with probabilities and would just like these double checked please. Thank you. Experiment: Roll three 6-sided dice. a) Find the probability that none of the dice rolled are a 2. $\left(\frac{5}{6}\right)^3$ b) Find the probability that the sum of the dice is 8. $\frac{21}{216}$ c) Find the probability that the sum of the dice is 8 and none of the dice rolled are 2’s. $\frac{9}{216}$ d) Find the probability that at least one of the dice rolled are a 2. $1- \left(\frac{5}{6}\right)^3$ e) Find the probability that the three values are sequential (for example: 4, 3, 5.) $1 \cdot \left(\frac{5}{6}\right) \cdot \left(\frac{4}{6}\right)$ f) Find the probability that none of the dice rolled are a 2 given that the sum is 8. $\frac{9}{21}$ g) Find the probability that the sum is 8 given that none of the dice rolled are a 2. $\frac{9}{125}$ h) Are the events “none of the dice rolled are 2” and “the sum is 8” independent? Explain. Yes, because they don't depend on each other. For example, you can roll a sum of 8 while still rolling a 2 or without a 2. i) Assume that the dice are colored: one is red and two are blue. Find the probability that the sum of the blue dice is equal to the value on the red die. $\frac{15}{216}$ j) Assume that the dice are colored: one is red and two are blue. Find the probability that the sum of the blue dice is equal to the value on the red die given that the red value is 6. $\frac{5}{36}$ k) Assume that the dice are colored: one is red and two are blue. Find the probability that the value of the red die is 6 given that the sum of the blue dice is equal to the value on the red die. $\frac{5}{15}$","['probability', 'discrete-mathematics']"
1260572,Probability of something happening exactly x times given y tries,"I'm working on my stats homework, and I completely forgot how to do this. Also, we were only taught how to do it on a calculator (TI84) and I want to know how to do it without one.
The problem is pretty much ""what is the problem of it happening exactly twice given we try it ten times?"" and the probability of it happening each time is .17.","['probability', 'statistics']"
1260620,Does there exist a non-quasi-split torus?,"In a homework, I was asked to prove that any torus is isomorphic to a quotient of a finitely many product of Weil restrictions $Res_{L/k}\mathbb{G}_m$. While solving this, I got an impression that almost all torus is actually isomorphic to direct product of Weil restrictions, so the word ""quotient"" is unnecessary. I think it reduces to showing that every integral representation $\mathbb{Z}^n$ of a finite group $G$ can be decomposed into representation generated by a single element. I would appreciate if someone can answer this question positively or negatively. Thank you!","['algebraic-groups', 'algebraic-geometry', 'representation-theory']"
1260634,The convergence of a sequence with infinite products,"I have a problem to determine convergence (sum over n). $$\sum_{n=0}^\infty \dfrac {a\left( a+1^{p}\right) \ldots \left( a+n^{p}\right) }{b\left( b+1^{p}\right) \ldots \left( b+n^{p}\right) }$$where $a<b, a>0,b>0$. I have concluded convergence for $p\leq0$ by comparing it to a constructed geometric sequence, as well as for $p=1$, using comparison test with $n^{a-b}$. But I can not use similar methods for $p>1$ and $0<p<1$. I have some thoughts for the two parts: When $p>1$, it seems that the limit of each term is not $0$. If the limit could be evaluated, then the divergence can be proved. My method for $p=1$ is to use the Euler Product of the gamma function, but the $p$ power makes it impossible to use this method. I am wondering if there is any kind of generalization of gamma function that is of this form. when $0<p<1$, I compared it to the case of $p=1$, that could at least tell it converges in the range when $p=1$ converge. But it is inconclusive for the parts remaining. Any help or hints would be appreciated.","['calculus', 'limits', 'convergence-divergence']"
1260659,Is this a valid logical paradox?,"In some recent cases, I have noticed some theorems are accepted to be intuitively or logically true if they themselves, as a unit, have no valid proof, but, their statements can be used to prove another theorem which then, logically, conversely proves the theorem with no proof. And, I was just wondering if this is actually a valid and logical system. To rephrase it more lucidly: ""If theorem $p$ cannot be proved, but, can be used to prove theorem $q$, can $q$ prove $p$, and is $p$ thus provable?""","['elementary-set-theory', 'provability', 'logic']"
1260681,Composition of polynomials over finite fields,"Consider the set of polynomials of degree at most $n$ over a finite field $k_q$ with $q$ elements where $q$ is prime:
$$
P_{n,q} = \left\{ x + c_2 x^2 + \cdots + c_n x^n:\ c_i \in k_q \right\}.
$$
It turns out that this set forms a group under composition of polynomials modulo $x^{n+1}$. Could you please tell, is it something well-known, and if it is so, could you please provide any references?","['abstract-algebra', 'polynomials', 'group-theory', 'finite-fields', 'reference-request']"
1260722,Prove that $f=x^4-4x^2+16\in\mathbb{Q}[x]$ is irreducible,"Prove that $f=x^4-4x^2+16\in\mathbb{Q}[x]$ is irreducible. I am trying to prove it with Eisenstein's criterion but without success: for p=2 , it divides -4 and the constant coefficient 16, don't divide the leading coeficient 1, but its square 4 divides the constant coefficient 16, so doesn't work. Therefore I tried to find $f(x\pm c)$ which is irreducible: $f(x+1)=x^4+4x^3+2x^2-4x+13$ , but 13 has the divisors: 1 and 13 , so don't exist a prime number p such that to apply the first condition: $p|a_i, i\ne n$ ; the same problem for $f(x-1)=x^4+...+13$ For $f(x+2)=x^4+8x^3+20x^2+16x+16$ is the same problem from where we go, if we set p=2 , that means $2|8, 2|20, 2|16$ , not divide the leading coefficient 1, but its square 4 divide the constant coefficient 16; again, doesn't work.. is same problem for x-2 Now I'll verify for $f(x\pm3)$ , but I think it will be fall... I think if I verify all constant $f(x\pm c)$ it doesn't work with this method...  so have any idea how we can prove that $f$ is irreducible?","['abstract-algebra', 'polynomials', 'irreducible-polynomials', 'field-theory']"
1260762,Combinatorial proof $n {2n \choose n} = (n+1) {2n \choose n+1}$,"I want to prove combinatorially that $n {2n \choose n} = (n+1) {2n \choose n+1} $. I have noticed that ${2n \choose n}$ is the number of ways walking only north or east in a square from a corner to another along the sides and ${2n \choose n+1} $ in a rectangle with $n-1 \times n+1$. Another approach I had was noting that the left-hand side can be modeled as the numbers of way we can choose $n$ balls from $2n$ balls and paint exactly one of these. The right-hand side is the number of ways choosing $n+1$ balls and painting exactly one of these (among $2n$ balls). However, I cannot quite realize why they must be equal. Someone have an explanation?","['discrete-mathematics', 'combinatorics']"
1260788,Basis of $\mathbb{F}[[x]]$ over $\mathbb{F}$ without AC,"Does the ring of formal power series $\mathbb{F}[[x]]$ as a vector space over $\mathbb{F}$  admit a basis without assuming the Axiom of choice, at least in some special cases of $\mathbb{F}$? I'm trying to find a basis explicitly. In the case $\mathbb{F}[x]$ we can simply take $B=\{ x^0,x^1,... \}$. But even $\mathbb{F}_2[[x]]$ does not seem to have any obvious basis. For every infinite-dimensional vector space $\; |V| = \max(|F|, \dim V)$, hence we know that the basis is of size $|V|=\max(2^{|\mathbb{F}|},{\aleph})$, and thus intuitively it seems 'too large to be explicit'.","['vector-spaces', 'linear-algebra', 'axiom-of-choice']"
1260795,Different Law of Cosines using Sine instead: $c^2 = a^2 + b^2 - 2ab\sqrt{1-\sin^2(\theta)}$,"Playing around with Trig and the Law of Cosines (LoC), I came up with this formula given a triangle with sides $a$, $b$, $c$ where we are given $a$, $b$ and angle $\theta$ between them: $$c^2 = a^2 + b^2 - 2ab\sqrt{1-\sin^2(\theta)}$$ Far from me the idea that I could've stumbled onto something no one's ever derived before, but I've never seen this formula and was just curious whether it has a name or is never considered because it offers no advantage over the LoC (needing the same amount of initial information) and is slightly more complicated. Also, is my proof correct? Here's my work; here I use $C$ for the angle: $$c^2 = x^2 + h^2$$ $$h = a \sin(C)$$ $$h^2 = a^2 \sin^2(C)$$ $$x = b - (b-x)$$ $$(b-x) = \sqrt{a^2 - h^2} = \sqrt{a^2 - a^2 \sin^2(C)} = \sqrt{a^2 (1-\sin^2(C))}$$ $$x = b-a\sqrt{1-\sin^2(C)}$$ $$x^2 = b^2 - 2ab \sqrt{1-\sin^2(C)} + a^2 (1-\sin^2(C))$$ Therefore: $$c^2 = b^2 - 2ab \sqrt{1-\sin^2(C)} + a^2 (1-\sin^2(C)) + a^2 \sin^2(C)$$ $$c^2 = b^2 - 2ab \sqrt{1-\sin^2(C)} + a^2 (1-\sin^2(C) + \sin^2(C))$$ $$c^2 = a^2 + b^2 - 2ab \sqrt{1-\sin^2(C)}$$","['solution-verification', 'triangles', 'trigonometry']"
1260832,What is a basis for the vector space $ \Bbb{C}^{n} $ (a complex vector space)?,"I know that a basis for $ \Bbb{C} $ is $ \{ 1,i \} $. This set is linearly independent in $ \Bbb{C} $ and spans $ \Bbb{C} $. I think that the dimension of $ \Bbb{C}^{n} $ may be $ 2 n $, but I’m just failing to understand what kind of vectors should be in a basis. Also, please correct me if the dimension of $ \Bbb{C}^{n} $ is not $ 2 n $.","['vector-spaces', 'linear-algebra', 'complex-numbers']"
1260853,"Expressing $ r = \cot(\theta) $ as an equation in terms of Cartesian coordinates $ (x,y) $.","I need to show this equation $r = \cot(\theta)$ as $x$,$y$ using the following laws: $x=r\cos(\theta)$, $y=r\sin(\theta)$ $r^2=x^2+y^2$, $\tan(\theta)=\frac{y}{x}$ This is what I've done : $$r = \cot(\theta) \\ 
r = \frac{\cos(\theta)}{\sin(\theta)} \\
r^2=\frac{r\cos(\theta)}{\sin(\theta)}\\
x^2+y^2=\frac{x}{\sin(\theta)}$$ Now, I'm stuck what should I do with $\sin(\theta)$? Any ideas? Thanks!","['calculus', 'polar-coordinates', 'trigonometry']"
1260855,"Gauss-Bonnet Theorem, External Angles and Orientation","The Global Gauss-Bonnet Theorem states: Let $R\subset S$ be a regular region and $C_1,\ldots,C_r$ be closed, simple, piecewise regular curves forming the boundary of $R$. Suposse $C_i$ is positively oriented and $\theta_1,\ldots,\theta_n$ be the external angles of the curves. Then:
\begin{equation*}
\sum\limits_{i=1}^{r} \int_{C_i} k_g^{C_i}(s) ds + \iint_R K d\sigma + \sum\limits_{j=1}^{n} \theta_j = 2\pi\chi(R)
\end{equation*} My questions are: What is the easiest way to orientate positively the curves? Should we compute the external angles before or after the orientation of the curves? How do we compute easily the sign of the external angles? Suppose we have a region $R$ with is homeomorphic to a square like this: Is it true that $\chi(R)=1-n$, where $n$ denote the number of holes?","['differential-geometry', 'surfaces']"
1260861,Equivalence between conditions for convergence,"Let $(X_k)$ be independant random variables such that $X_k\sim\mathcal{P}(p_k)$ (Poisson distribution with parameter $p_k$). So in particular we have $ \sum_{n=1}^NX_k \sim \mathcal{P}(\sum _{n=1}^Np_k)$ and $\mathbb{E}(X_k)=p_k$. By hypothesis $np_n\to \infty$ and can be write : $p_n=b(n)/n$ where $b(n)$ is slowly varying function. A function $f$ defined on $\mathbb{R}$ is a slowly varying function if for every $\delta >0$, $n^{\delta }f(n)$ is increasing and $n^{\delta }f(n)$ is decreasing if $n$ is large enough. So in particular we have $p_n\to 0$. I want to show equivalence between the following conditions : i) a.s. $\forall t \neq 0 \in \mathbb{T}, \, \sum_{k=1}^N\mathbf{1}_{\{ X_k>0\}}e^{2i\pi kt}=o( \sum_{k=1}^N\mathbf{1}_{\{ X_k>0\}})$ when $N\to \infty$. ii)a.s. $\forall t \neq 0 \in \mathbb{T}, \, \sum_{k=1}^NX_ke^{2i\pi kt}=o( \sum_{k=1}^NX_k)$ when $N\to \infty$. iii)a.s. $\forall t \neq 0 \in \mathbb{T}, \, \sum_{k=1}^NX_ke^{2i\pi kt}=o( \sum_{k=1}^Np_k)$ when $N\to \infty$. The notation $\sum_{k=1}^N\mathbf{1}_{\{ X_k>0\}}e^{2i\pi kt}=o( \sum_{k=1}^N\mathbf{1}_{\{ X_k>0\}})$ means that $$ \frac{\sum_{k=1}^N\mathbf{1}_{\{ X_k>0\}}e^{2i\pi kt}}{\sum_{k=1}^N\mathbf{1}_{\{ X_k>0\}}}\to 0$$ when $N\to \infty$. The torus will be denoted by $\mathbb{T}=[0,1)=\mathbb{R}\setminus  \mathbb{Z}$. First observe that $\sum _k p_k=\infty $. But we can obtain a more precise estimation : $\sum _k^N p_k=a_N\log (N)$ where $a_N\to \infty$ when $N\to \infty$. So by using Chernoff bound we can show (cf Mickael's answer) that $ii)\implies iii)$ since $ \sum _k X_k\leq M \sum _k p_k$ with probability one. The same argument can be used to show that with probability one $ \sum _k p_k\leq m \sum _k X_k$. So $ii)\Leftrightarrow iii)$. It remains to show the equivalence between $i)$ and $ii)$ and $iii)\implies ii)$. I suggest the following argument. Let $Y_k=X_k-\mathbf{1}_{\{ X_k>0\}}$, $\mathbb{P}(Y_k>0)=e^{-p_k}\sum _{n\geq 2}\frac{p_k^n}{n!}$. So $\sum _k\mathbb{P}(Y_k>0)<\sum _n \sum _k \frac{p_k^2}{n!}<+\infty $ (we can assume that $0<p_k<1$ and by hypothesis $\sum p_k^2=\sum b(k)^2/k^2<+\infty$). By Borel-Cantelli lemma $Y_k=0$ a.s. for sufficiently large $k$. Since $\sum _k X_k \to \infty$ the result follows.","['sequences-and-series', 'probability', 'convergence-divergence']"
1260865,Counting identical beads on a necklace,"Suppose I have 11 beads. 4 of them are red and 3 of them are blue. The remaining 4 are all distinct (so just say labelled 1 to 4). If these beads were in a straight line, then computing the number of permutations is fairly simple. It is $\dfrac{11!}{4!3!}$. Now suppose they are on a necklace. Would the answer be as simple as just $\dfrac{10!}{4!3!}$, where we reduced one element from the numerator due to first having a fixed bead as a 'point of reference', as we usually do for circular arrangements? I feel that it is not just this simple, and that we've opened a whole new can of worms by having identical beads. If that is the case, what is wrong with the logic above, and why does the degree of complexity change so vastly with a simple change in the arrangements (straight line vs circular)?",['combinatorics']
1260870,Integration with 2-forms,"Wikipedia says: Let
$$ \omega=f_{z}\, \mathrm dx \wedge \mathrm dy + f_{x}\, \mathrm dy \wedge \mathrm dz + f_{y}\, \mathrm dz  \wedge \mathrm dx $$
be a 2-form on a surface with parametrization 
$$\mathbf{x} (s,t)=( x(s,t), y(s,t), z(s,t))\!$$ defined on some domain $D.$ Then, the surface integral of the two-form on the surface $S$ is given by $$  \int_{S} \omega = \int_D \left[ f_{z} ( \mathbf{x} (s,t)) \frac{\partial(x,y)}{\partial(s,t)} + f_{x} ( \mathbf{x} (s,t))\frac{\partial(y,z)}{\partial(s,t)} + f_{y} ( \mathbf{x} (s,t))\frac{\partial(z,x)}{\partial(s,t)} \right]\, \mathrm ds\, \mathrm dt$$ Now my question is: Is this the same as $$\int_{D} \omega(\mathbf{x}(s,t))(\frac{\partial \mathbf{x}}{\partial s}(s,t),\frac{\partial \mathbf{x}}{\partial t}(s,t)) ds dt ?$$ If anything is unclear, please let me know.","['differential-forms', 'real-analysis', 'manifolds', 'analysis', 'lebesgue-integral']"
1260889,Let p be a prime. Consider the equation $\frac1x+\frac1y=\frac1p$. What are the solutions?,"Write down the set of distinct solutions and prove your list is complete.
$x$ and $y$ are positive integers. I have rewritten it as $\frac{(x+y)}{xy} = \frac1p$, but I don't understand where to go from here?","['number-theory', 'elementary-number-theory']"
1260896,Divisor on curve of genus $2$,"I suffer from lack of concrete examples in Algebraic Geometry, so I will appreciate it if somebody can help me in understanding a bit better this one: Let $\mathcal{C}$ be a genus $2$ curve (projective, smooth and complex). Let $K$ be the canonical class and take a couple of different points $p,q\in\mathcal{C}$. Consider the divisor $K+p+q$. If I'm correct, (using Riemann-Roch theorem) this divisor defines a map taking $\mathcal{C}$ into a plane projective curve with one node. Is this map birational? How can I visualize it in a concrete way?.
Many thanks for any useful comments.","['algebraic-geometry', 'riemann-surfaces', 'algebraic-curves']"
1260918,How to factor the polynomial $x^4-x^2 + 1$,"How do I factor this polynomial: $$x^4-x^2+1$$
The solution is: $$(x^2-x\sqrt{3}+1)(x^2+x\sqrt{3}+1)$$
Can you please explain what method is used there and what methods can I use generally for 4th or 5th degree polynomials?","['algebra-precalculus', 'polynomials', 'factoring', 'quartics', 'elementary-number-theory']"
1260928,"Show that for any prime $ p $, there are integers $ x $ and $ y $ such that $ p|(x^{2} + y^{2} + 1) $.","So we obviously we want $ x^{2} + y^{2} + 1 \equiv 0 ~ (\text{mod} ~ p) $. I haven’t learned much about quadratic congruences, so I don’t really know how to go forward. I suppose you can write it as $ x^{2} \equiv -1 - y^{2} ~ (\text{mod} ~ p) $, but again, I’m not sure where to go. I know about the Legendre symbol and quadratic residues, which I know are involved in this question, but I don’t know how to apply what I know. :(","['quadratic-residues', 'number-theory', 'congruences', 'elementary-number-theory']"
1260929,Under what conditions is $ \max_{a \in A} (f(a) - g(a)) \geq \max_{a \in A} f(a) - \max_{a \in A} g(a) $ true?,"Consider:
$$
\max_{a \in A} (f(a) - g(a)) \geq \max_{a \in A} f(a) - \max_{a \in A} g(a).
$$ Intuitively, it seems obvious it should be true, but I was having a hard time coming up with a rigorous precise proof under which conditions it is true. It seems that to make the RHS big, a should be chosen to make $g(a)$ as big as possible and $f(a)$ as small as possible (ideally). However, the right hand side is doing nearly the exact opposite! So intuitively, I would find it hard to believe that the above inequality is not true considering that the right hand side is clearly not finding the biggest difference. I hope this is right, so I was in search for a proof of this.","['inequality', 'functions']"
1260945,Conceptual explanation of integral of divergence.,"$\textbf{My understanding of divergence:}$ Consider any vector field $\textbf{u}$, then $\operatorname{div}(u) = \nabla \cdot u$. More conceptually, if I place an arbitrarily small sphere around any point of the vector field $\textbf{u}$, divergence measures the amount of ""particles"" exiting the sphere, i.e.  positive divergence represent a vector field which is ""moving faster"" as we move to the right. However, how do I interpret $$ \int_U \operatorname{div}(u) \, dx$$ where $U$ is any bounded open subset of $\mathbb{R}^n$.",['multivariable-calculus']
1260983,"If a matrix as well as its Hermitian part both have determinant one, must the matrix be Hermitian?","If $x\in\mathrm{M}_2(\mathbb{C})$,
$y=\dfrac{x+x^{\dagger}}{2}$, and $z=\dfrac{z-z^{\dagger}}{2}$,
then $x=y+z$.
Also, $y$ and $z$ respectively are Hermitian and anti-Hermitian, i.e. $y^{\dagger}=y$ and $z^{\dagger}=-z$, where $^\dagger$ denotes the conjugate transpose.
Now suppose $\det(x)=\det(y)=1$.
Does this force $x^{\dagger}=x$?
I cannot find a counterexample but I can't prove it either. Some thoughts:
Let $H$ and $A$ respectively denote the sets of $2\times2$ complex Hermitian and anti-Hermitian matrices, and
consider the map $\phi:\mathrm{M}_2(\mathbb{C})\rightarrow H\times A$
which takes $x$ as above to $(y,z)$.
Then $x\in H\iff\phi(x)=(x,0)$.
Also, $\phi$ is a vector space isomorphism if we think of $\mathrm{M}_2(\mathbb{\mathbb{C}})$
as an 8 dimensional real vector space, so taking the first coordinate of its image gives a projection onto $H$ as a 4 dimensional real subspace.  I want to say that this projection cannot fix the determinant of $x$ unless $x$ is already in its image, but it's difficult to say anything in particular about the determinant here because it maps into $\mathbb{C}$ and the vector spaces I'm thinking about are real.",['matrices']
1260996,Discrete Mathematics,"I am having great difficulty trying to understand a question I have found and am keen to finding the solution and would appreciate any help. ""Suppose that ten computer programs have been submitted for batch processing. Only one program may run at a time. In how many ways can the program run if? 1) there is no restrictions on the processing order? 
2) four of the programs are considered higher in priority than the other six and should be processed first? 
3) the programs can be separated into three top priority(to be processed first), five medium priority (to be processed immediately after the top priority programs) and two low priority programs(to be processed last)?",['discrete-mathematics']
1261003,"Are there uncountably many subintervals in $[0,1]$?","Are there an uncountably infinite number of sub-intervals in $[0,1]$ such that the number of real numbers in each of those sub-intervals is uncountably infinite? I would say no, because you would have to make the intervals really small thereby making it impossible to have an uncountably infinite number of real numbers in those sub-intervals, but I'm no expert.",['elementary-set-theory']
1261024,Every net has an ultranet as subnet: direct proof,"I'm currently brushing up my topology using Willard's General Topology .
Currently I'm working through the chapters 11 and 12 on nets and filters.
Chapter 12 deals extensively with ultrafilters and proves (Theorem 12.12) that every filter is contained in an ultrafilter using Zorn's lemma. Theorem 12.17 and the exercises connect nets and filters. In this way, I see a proof that: Every net has a subnet which is an ultranet. For reference: A subnet $x_\mu$ of $x_\lambda$ is an increasing cofinal mapping $\phi: M\to\Lambda$ composed with $x: \Lambda\to X$ . An ultranet is a net $x: \Lambda \to X$ such that: $$\forall E \subseteq X: \exists \lambda_0: \forall \lambda\ge \lambda_0: x_\lambda \in E \lor \forall \lambda\ge \lambda_0: x_\lambda\notin E$$ However, this very statement also occurs as Exercise 11B.2. This suggests an easier proof. After one and a half week of intermittent attempts, I concede and humbly ask your help. I would love to see ""natural proofs"", as opposed to deus ex machina constructions. Thanks in advance. (It should be noted that some choice is necessary, but even in choice proofs, some are more natural than others.)","['filters', 'general-topology', 'nets']"
1261032,Finding the sum of real solutions to an equation,"how to find the sum of real solutions 
if,
$(x+1)(x+\frac14)(x+\frac12)(x+\frac34)=\frac{45}{32}$ I have tried multiplying both sides with 32 and got
$\frac{1}{32}(x+1)(2x+1)(4x+1)(4x+3)=\frac{45}{32}$
then i multiplied and got
$\frac{1}{32}(32x^4+80x^3+70x^2+25x+3)=\frac{45}{32}$
which is equal to
$32x^4+80x^3+70x^2+25x+3=45$
and
$32x^4+80x^3+70x^2+25x-42=0$
but i couldn't come up with something else... Any help is appreciated!",['algebra-precalculus']
1261050,Row and column operations and matrix similarity,"Take for example the following matrix: $$
A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix}
$$ The elementary matrix equivalent to changing the first row with the second is $$
E = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix} 
$$ multiplied from the left. The elementary matrix equivalent to changing the first column with the second is the same matrix $E$ multiplied from the right. After a quick check, I found that $E = E^{-1}$ . Given that, I concluded that: $$
B = \begin{pmatrix} 5 & 4 & 6 \\ 2 & 1 & 3 \\ 8 & 7 &9\end{pmatrix} = EAE^{-1}
$$ and therefore $A\sim B$ . Is this comprehensive? Does changing rows and column necessarily make the outcome similar to the original? Thanks","['linear-algebra', 'matrices']"
1261062,How to prove that $a=z^{p}$ for some $z \in \mathbb{Z_{+}}$?,"Claim : If for a positive, composite integer $a$  and an odd prime $p$, such that $\gcd(a,p)=1$, we are given $$  a^{p^{n-2}(p-1)} \equiv 1 \pmod {p^n} \ \forall \  n \geq 2 \ \ ;\  n \in \mathbb{Z_{+}} $$ Then $a=z^{p}$ for some $z \in \mathbb {Z_{+}}$ Also note that the congruence holds for for all positive integers $n$, not just an arbitrary one. My Approach : At first a glance, it seems trivial to me, because by Euler's Theorem, $a^{\phi(p^{n})} \equiv 1 \pmod {p^n}$ and since $\phi(p^n) = p^{n-1}(p-1)$, I think that $a$ should be a perfect $p^{th}$ power to account for the extra $p$ in the power. But, I don't have a formal proof for this. Is my reasoning enough or do I need a more rigorous argument ? Added Later : On observing more carefully, I found the following. Let the prime factorization of $a$ is $ \displaystyle \prod_{i=1}^{m} {q_{i}}^{k_{i}}$, where $q_{i}$ are prime factors of $a$. If we can prove that whenever the modular equation holds true, there exists at least one $k_{i} \geq p$, then we are done. This is because we can then write $a=j h^{p}$ and plugging it in the modular equation, we will get the same condition for $j$. This will set up an infinite descent and prove that $j=1$ and hence the claim will hold true. Query : As Mr. Greg Martin stated, the claim is false in general. However, If the modular equation is true for a fixed $n$, then, $a \equiv z^p \pmod {p^{n}}$ $\displaystyle \therefore a \equiv {k_{1}}^p \pmod {p}$ $a \equiv {k_{2}}^p \pmod {p^{2}}$ $.$ $.$ $.$ $a \equiv {k_{n}}^{p} \pmod{p^n}$ $\implies a = {k_{n}}^{p} + j p^{n}$ If we choose $n$ to be sufficiently large, then $j=0$ $\implies a={k_{n}}^{p}$ Where is the fault in my reasoning? Please Help.","['prime-numbers', 'number-theory', 'modular-arithmetic', 'algebra-precalculus', 'elementary-number-theory']"
1261063,Finding the vertices of a convex set of matrices,"I'm a little new here so wasn't sure if this was the right area. I've been trying to figure out how to generate a set of random $K \times N$ (for $K<N$) matrices that are subject to a several constraints: all columns sum to 1, all rows sum to $N/K$, and all elements of the matrix are non-negative. I've managed to reduce a problem a bit; because the set of matrices bound by these properties is convex, one could take the vertices of this set and take random weighted averages of the vertices (and use these averages for the pool, and so on); you should end up populating the space. What I'm trying to figure out is how to find the vertices of this set in the first place. So for example for the set of $2 \times 3$ matrices the vertices are the following set of pairs on the 2-D simplex (I think): (if you're reducing the 6-D space to ordered pairs in 3-D space then technically it also includes the re-ordered pairs, but this was just for illustration). Many thanks in advance.","['convex-analysis', 'linear-algebra']"
1261077,Symmetric matrix eigenvalues,"Let $A$ be an $n\times n$ matrix, with $A_{ij}=i+j$.  Find the eigenvalues of $A$.  A student that I tutored asked me this question, and beyond working out that there are 2 nonzero eigenvalues $a+\sqrt{b}$ and $a-\sqrt{b}$ and $0$ with multiplicity $n-2$, I'm at a bit of a loss.","['eigenvalues-eigenvectors', 'linear-algebra', 'matrices']"
1261103,Hitting time for a random walk on the grid,"Consider the grid on $n^2$ nodes composed of points $x$ and $y$ with coordinates in the set  $\{1, \ldots, n\}$, and consider the discrete-time Markov chain which transitions to a random neighbor at each step. Up to a constant factor, what is the hitting time from $(x_1,y_1)$ to $(x_2, y_2)$? A resistance based analysis does not seem to solve this, as resistances capture commute time, not hitting time, and I do not see any argument to ensure that hitting times will be symmetric on this graph. The worst-case hitting time is $O(n^2 \log n)$ on this graph which can be proven by looking at resistances (see this paper ), but I'm interested in the scaling with $|x_1 - x_2|$ and $|y_1 - y_2|$. It seems natural to guess the answer is $\Theta \left( (x_1 - x_2)^2 + (y_1 - y_2)^2 \right)$ with some log factors but I'm not sure how to prove this. I also wonder if there is a neat way to prove such statement - a resistance analysis for the commute time allows us to avoid messing around with a system of equations; does a similar tool exist for hitting time?","['probability-theory', 'probability']"
1261113,calculate the principal part of $\tan(z)$ at $\frac{\pi}{2}$,calculate the principal part of $\tan(z)$ at $\frac{\pi}{2}$. of course $\tan(z) = \frac{\sin(z)}{\cos(z)}.$ Because $\cos(z)$ is of order 1 in  $\frac{\pi}{2}$ we know that our primal part must look like: $c_{-1}(z-\frac{\pi}{2})^{-1}$. The main problem here is that i don't seem to get how to write the sine and cosine. Are there any neat tricks to see this? Kees,['complex-analysis']
1261152,Number of hairs of inhabitants and the population of a city,"There is a town T where the population is greater than the number of hairs of each inhabitant. That is, if we count the number of hairs on the head of any inhabitant of the town, the amount will be smaller than the population of the city. There are no two inhabitants with the same number of hairs on their heads, and there is no one with exactly 618 hairs on their head. What is the greatest possible number of inhabitants of this town?","['discrete-mathematics', 'combinatorics']"
1261190,Visualizing products of $CW$ complexes,"I'm learning about products of CW complexes. The sources I've seen talk about the matter as follows: given topological spaces $X$ and $Y$ with a given CW decomposition, we can then form a CW decomposition of $X \times Y$ by understanding the characteristic maps as the product maps. OK, but starting with $X \times Y$ and then breaking it down into cells assumes (perhaps) that we have some notion of what $X \times Y$ looks like. I'm interested in understanding the structure of $X \times Y$ by inductively building a cell complex. For the most part, the attaching maps seem pretty hard to wrap my head around. OK, these things are hard to visualize, but it's nice to try. Suppose I don't know what $S^1 \times S^2$ is like (I don't). I try to get a handle on its cell decomposition, using for $S^1$ and $S^2$ the decompositions with two cells each. To build it, I'd start with a $0$ cell, attach a $1$ cell like a circle, and then attach the two skeleton which is a single $D^2$ with boundary collapsed to a point. So the two-skeleton is $S^1 \vee S^2 $. Now I should attach a solid tube like a cannoli, and I'm trying to understand the attaching map. The boundary of the tube has two flat sections $D^2$ and one curvy section $S^1 \times I$. I need to attach the flat parts to the sphere and the curvy part to the circle. Essentially I have a solid doughnut where I identify every boundary point that has the same position in the big circle? Is it productive for me to try to visualize things this way? I'm kind of banging my head against it.","['algebraic-topology', 'general-topology', 'cw-complexes']"
1261205,Concentration of measure of inner product in Hilbert space?,"In the finite dimensional Hilbert space of quantum mechanics (one where all vectors have norm one), is a concentration of measure phenomenon observed with the inner product of any two vectors? That is, for any $x,y\in\mathcal{H}$, is there an expression of the form $P(|<x,y> - \gamma| >\epsilon) < f(\epsilon) $ for some $\gamma$ and $f$? It is my intuition that $\gamma = 1/\sqrt{n}$. I'm rather new to concentration of measure though, so I don't see how to support this claim. Would it be simpler (and still convincing) to heuristically postulate this as a consequence of the law of large numbers?","['probability-theory', 'concentration-of-measure', 'hilbert-spaces', 'inner-products']"
1261225,"Find the gradient of $f^*(x)=\langle (\nabla f)^{-1}(x),x)\rangle-f( (\nabla f)^{-1}(x))$ for $x \in \mathbb{R^n}$","I am stuck at the following exercise which serves as a preparation for the upcoming exam: Let $U \subset \mathbb{R}^n$ be open and $f \in C^2(U, \mathbb{R})$ such that $\det Hf(x) \neq 0, \forall x \in U$ and $\nabla f: U \to \nabla f(U) =:V$ bijective. Under the above assumptions the function $f^*: V \subset \mathbb{R}^n \to \mathbb{R}$ given by $$f^*(u):=\langle (\nabla f)^{-1}(u),u)\rangle-f( (\nabla f)^{-1}(u)) \tag{*}$$
is clearly well defined and $f^* \in C^1(V, \mathbb{R})$ thanks to the Hessian Matrix of $f$ being invertible and thus $\nabla f$ is a local diffeomorphism. The bijective nature of $\nabla f$ makes $(\nabla f)^{-1}$ a class $C^1$ function. My problem : I am supposed to find the gradient of $f^*$, in fact I am supposed to verify the identity: $$(\nabla f^*)(u)=(\nabla f)^{-1} (u) \tag{$\alpha$} $$ My approach : Let $u=(u_1, \dots , u_n) \in \mathbb{R}^n$ then it makes sense to only see if the statement in ($\alpha)$ holds for an arbitrary $i \in \lbrace 1, \dots , n \rbrace$. My problem is that I know nothing about how $(\nabla f)^{-1}$ looks like, so I thought the best attempt would be to just let it be in an abstract form. Let $h(u):=\langle (\nabla f)^{-1}(u),u)\rangle$ then I assume $h(u)$ looks like $$ h(u)=(\nabla f)_1^{-1}(u)*u_1 + \dots + (\nabla f)_i^{-1}(u)*u_i + \dots + (\nabla f)_n^{-1}(u)* u_n \\ \implies \frac{\partial h(u)}{u_i}=\frac{\partial(\nabla f)_1^{-1}(u)}{\partial u_i}u_1+ \dots + \frac{\partial(\nabla f)_i^{-1}(u)}{\partial u_i}u_i + (\nabla f)_i^{-1}(u)+ \dots + \frac{\partial (\nabla f)_n^{-1}(u)}{\partial u_i}u_n$$
If I have made no mistakes, all there is left to do is to check the second expression in (*) and hope that it annihilates the partially differentiated terms in my expression above. But all I get is: $$\frac{\partial f(( \nabla f^{-1})(u))}{\partial u_i}= \frac{\partial f}{\partial u_i}( \nabla f)^{-1}(u) \cdot \frac{\partial(\nabla f)_i^{-1}}{\partial u_i}(u)$$
So it would be nice if the first term would simplify to $u_i$ but I really don't see it. Any help or simplification would be appreciated.","['analysis', 'multivariable-calculus']"
1261258,Generalized way to solve $x_1 + x_2 + x_3 = c$ with the constraint $x_1 > x_2 > x_3$?,"On my example final exam, we are given the following problem: How many ways can we pick seven balls of three colors red, blue, yellow given
 also that the number of red balls must be strictly greater than blue and
 blue strictly greater than yellow? The solution I used (and was given) was a brute force counting. In particular, fix the number of red balls for $0, 1, \dots, 7$ and see how many viable cases we procure each time. However, I wanted to try and find a more clever way to do it, but couldn't. Is there a better/general way to do this problem when the numbers get larger? If possible, it would be even better if we solve the following more generalized form: $$x_1 + \dots + x_n = c, x_1 > \dots > x_n \geq 0$$",['discrete-mathematics']
1261272,Geometric mean of prime gaps?,"The arithmetic mean of prime gaps around $x$ is $\ln(x)$.
What is the geometric mean of prime gaps around $x$ ? Does that strongly depend on the conjectures about the smallest and largest gap such as Cramer's conjecture or the twin prime conjecture ?","['average', 'prime-numbers', 'number-theory', 'analytic-number-theory', 'prime-gaps']"
1261273,How to think/see point-set topology abstractly?,"I've started learning point-set topology this semester.  I've learned basic material about: topology on a set topological space open sets closed sets clopen sets closure neighborhoods interior point interior exterior boundary boundary point However, whenever I think of terms such as neighborhood, interior point, boundary $\dots$, I tend to think of them in terms of $\mathbb R^2$. (e.g., circles on the plane) Is there a way for me to think of these, and future topology terms, abstractly?Intuitively? What do you see in your mind, or think about, when you hear these terms? How do you look at this? I like to give this example from abstract algebra for what I mean when I say intuitively: If we have a group and we mod out by the commutator subgroup, what's basically going on is that we are setting all the elements that do not commute equal to the identity and thus we are left with a quotient group where everything commutes.","['real-analysis', 'general-topology', 'analysis', 'self-learning', 'intuition']"
1261286,Real matrix without real eigenvalues commutes with some matrix of square $-I$.,"Let $A\in M_n(\mathbb R)$ be such that, the minimal polynomial of $A$, has not any real root. Prove that there exist some $B\in M_n(\mathbb R)$ which: $B^2=-I_n$ and $AB=BA$. Suppose that $p(x)=x^k+a_{k-1}x^{k-1}+...+a_1+a_0$ be the minimal polynomial of $A$. By the hypothesis,  $a_0\neq 0$ and over ring $\mathbb R[x]$, the polynomial  $p(x)$ has the following decomposition: $$p(x)=(x^2+b_1x+b_2)^{d_1}(x^2+b_3x+b_4)^{d_2}...(x^2+b_mx+b_{m+1})^{ds}$$ With $b_j\in \mathbb R$ for $j=1,2,...,m+1$, and $d_1+d_2+...+d_s=k$. If I prove that there exist $q(x),h(x)\in \mathbb R[x]$ such that: $$(h(x))^2=p(x)q(x)-1$$ Then I am done by putting $B=h(A)$. But how can I prove that $q$ exists?","['eigenvalues-eigenvectors', 'linear-algebra', 'minimal-polynomials', 'matrices']"
1261302,Integral Inequality for Bound on Gradient of Solution to Heat Equation,"My overall aim is to show that, for a bounded solution $u(x,t)$ to the heat equation in $\mathbb{R}^n \times [0,T]$ with boundary condition $u(x,0) = f(x)$, $$\max |\nabla u(x,T) | \leq \frac C{\sqrt{T}} \max \left|f\right|.$$ I've been able to bound the term on the left by $$\left(\frac 1{(4 \pi t)^n 4t^2} \int_{\mathbb{R}^n} f(y)^2 e^{-\frac{|x-y|^2}{2t}} |x-y| dy\right)^{\frac12}.$$ How can I show that this term is less than or equal to the right side of my original inequality? EDIT: I can actually get a slightly stronger bound on the left by using Cauchy-Schwarz in earlier calculations: $$\left(\frac 1{(4 \pi t)^n 4t^2} \int_{\mathbb{R}^n} f(y)^2 dy \int_{\mathbb{R}^n} e^{-\frac{|x-y|^2}{2t}} |x-y| dy.\right)^{\frac12}$$ If $y \in \mathbb{R}^1$ then the second integral becomes simply $2t$, though I'm not sure how the evaluation changes in higher dimensions. But the first integral is, of course, the $L^2$-norm for the function f (once you take into account the power of $1/2$ around the whole expression) so that's something at least.","['vector-analysis', 'multivariable-calculus', 'real-analysis', 'partial-differential-equations']"
1261307,Surjective unbounded linear operator in Banach Spaces,Open mapping theorem says that bounded linear operator $T: X \to Y$ is an open mapping if $X$ and $Y$ are both Banach and $T$ is surjective. I am wondering what about unbounded linear operators? I guess it is not an open mapping but is there any counter example?,['functional-analysis']
1261327,How would I show a functional is linear and bounded?,"Using the following results, for any $f \in H^1(a,b)$, $f$ is continuous on $[a,b]$, and therefore, $$ \int_a^b f(x) dx = f(\zeta)$$ for some $\zeta \in (a,b)$. In addition $$f(c) = f(\zeta) + \int_\zeta^c f'(x)dx.$$ How would I go about showing the functional defined as  $\ell \in (H^1 (a,b))'$ by $$ \ell(v) = v(c), v \in H^1 (a,b)$$ for some $c \in [a,b]$, is linear and bounded on $H^1(a,b)$?","['sobolev-spaces', 'functional-analysis']"
1261336,Line Integrals FT usage on this strange vector field: so what are the exact conditions?,"I really tried thousands of things before deciding to ask here. Searched all over the internet for an answer, but failed to find it. Let's get started with the Fundamental Theorem of Line Integrals. Take the following statement (found in many places, for example, in this Math.SE question ): Suppose $C$ is a smooth curve given by $r(t)$, $a \leq t \leq b$ and suppose $\nabla f$ is continuous on $C$. Then $$\int_{C}\nabla f\cdot dr = f(r(b)) - f(r(a)).$$ Unfortunately, there must be some mistake here. This theorem clearly implies that if C is a closed curve, the integral must be zero. I will show a counterexample soon. But first, I would like to clarify that I already understand ""part"" of the mistake: I know the previous statement must have forgot some condition, like some sort of differentiability and such. So this is my question, what are EXACTLY the requirements? At first I expected this to be an easy google-search question, but it turned out that no one seemed to care enough with that kind of details. I am interested in those details, though. Very well, take a look at this MIT video , where it's shown that the following (famous) vector field is NOT conservative: $$\vec{F}(x,y) = \dfrac{-y\hat{i} + x\hat{j}}{x^2 + y^2}$$ Very well. I understand that. In fact, if you take $C$ to be the counter-clockwise unit circle around the origin, the integral will be equal to $2\pi$, not zero . The problem is, $\vec{F}$ can be written as a gradient field!! Look: $$ f = -arctan(x/y) \implies \vec{F} = \nabla f$$ It is also true that $C$ is a smooth curve (circle) and $\nabla f$ is continuous on $C$. This shows that the quoted statement for the Fundamental Theorem of Line Integrals must be wrong. As I said before, I suspect the quoted statement forgot some conditions. I want to know what are those conditions, and more importantly, why those conditions are needed! I even looked at a proof for the theorem, but couldn't detect any steps that needed extra conditions. All this also raised another question: is it really true that Gradient Fields and Conservative Fields are the same thing (as stated by this Math.SE question )? If they are indeed the same thing, do you agree that Gradient Field was a poor name choice (given that I showed a field, written as a gradient, that is not conservative)? If they are not the same thing (which would disagree with the linked question ), then what's the difference? ( Meta-parentheses: I hope this part is not considered a duplicate, given that I believe the other question to be lacking - if I'm wrong and it is indeed a duplicate, please help me explaining what exactly should I have done instead ). SUMMARY 1. What is wrong with the quoted Fundamental Theorem of Line Integrals? (what conditions are missing?) 2. Why are those extra conditions needed? 3. With all this in mind, is it really true that ""gradient field"" is the same thing as ""conservative field""? (Please note I am aware of this question but I believe it to be incomplete, therefore I believe this part is not a duplicate. Please read the whole question for details on this. I just don't want a duplicate tag here.) Thank you all for your time. EDIT: After searching even more I finally found out a bit of information that helped - this Wikipedia page , but sadly all my questions still stand. It seems that the missing condition is the need of being simply connected , but how exactly does that go in the Fundamental Theorem of Line Integrals? Actually, my textbook (by James Stewart) doesn't mention anything regarding simply-connectedness on the Fundamental Theorem of Line Integrals. There is no region, just a curve! EDIT 2: I just found this question and this question , answered by Shuhao Cao. Although the major portion of the explanations use deeper knowledge that I am not familiar with, I was able to find out that the equivalence between a vector field being conservative, its rotation being zero, it being the gradient of a scalar potential and its path integral being path-independent only holds in simply connected domains (as said by joriki in a comment). I believe that but didn't understand the reason yet - probably the reason is the lack of knowledge to understand Shuhao Cao's explanation. Nevertheless, my questions 1 and 2 still stand. Also, if someone can provide a simpler explanation than Shuhao Cao's ones, that would be great.","['line-integrals', 'vector-fields', 'multivariable-calculus']"
1261346,"$f=u+iv$ holomorphic, $xu+yv = (x^2+y^2)e^x \cos y$, what is $f$?","$f(z)=u(x,y)+iv(x,y)$ holomorphic, $xu+yv = (x^2+y^2)e^x \cos y$, what is $f$? I tried to interprete $xu+yv$ as some part of a new function, for example, as the real part of $\overline{z}f$，but this function is no more holomorphic, so I don't know how to continue. (Maybe $\dfrac{\partial}{\partial\overline{z}}（\overline{z}f)=1$? But how to make use of it?)","['analyticity', 'complex-analysis', 'ordinary-differential-equations']"
1261347,A Functional equation in Complex variables,"I have been stuck on this problem for a long time : If $f(z)=u(x,y)+iv(x,y)$ , prove that a. $f(z)=2u(z/2,(-iz)/2) +$ constant b.$f(z)=2iv(z/2,(-iz)/2) +$ constant This result seems very interesting in itself ( nothing that I have done comes close to a solution) .Any help would be greatly appreciated ....",['complex-analysis']
1261393,Relationship of SDE and Feynman-Kac PDE,"I am struggling with this problem: Given a stochastic differential equation
  $$ dX_t = b(X_t) dt + \sigma (X_t) \,dW_t $$
  where $W$ is a Brownian motion and the functions $b$ and $\sigma$ are bounded and smooth. Assume that for every square-integrable $\xi$ independent of $W$, there exists a unique strong solution $X$ such that $X_0 = \xi$ and $\sup_{t \geq 0} \mathbb{E} ( X^2_t) < + \infty$. Assume that for some constant $k>0$,
  \begin{equation}  \mathbb{E} [(X_t - Y_t)^2] \leq \mathbb{E} [ (X_0 - Y_0)^2] e^{-kt}, \end{equation}
  for any two strong solutions $X$ and $Y$.
  Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a smooth function and let $u: [0, \infty) \times \mathbb{R}$ be a bounded and smooth solution of the (Feynman-Kac) PDE
  $$ \frac{\partial u }{\partial t} = b(x) \frac{\partial u}{\partial x} + \frac{1}{2}  \sigma(x)^2 \frac{\partial^2 u}{\partial x^2}, $$
  with boundary condition
  $$u(0,x) = f(x), \quad \forall x \in \mathbb{R}.$$ Now let $\frac{b}{\sigma^2}$ be locally integrable and let 
  $$ p(x) = \frac{C}{\sigma(x)^2} \exp \bigg\{ \int_0^x \frac{2 b(s)}{\sigma(s)^2} \,ds \bigg\},$$
  where $C>0$ is chosen so that $p$ is a density function.
  Using the fact that 
  $$ \int_{\mathbb{R}} u(t,x) p(x) \,dx = \int_{\mathbb{R}} f(x) p(x) \,dx \quad \forall t \geq 0,$$
  show that $$u(t,x) \rightarrow \int_{\mathbb{R}} f(y) p(y) \,dy \quad \quad \text{ as } \, \, t \rightarrow \infty,$$
  for all $x \in \mathbb{R}.$ I know from standard results that $u(t,x) = \mathbb{E} [ f(X_t) | X_0=x]$. However, how does this relate to the integral equality involving $p(x)$ and the bound involving $\mathbb{E}[ (X_t -Y_t)^2 ]$? Any ideas to this complicated problem?","['probability-theory', 'stochastic-calculus', 'stochastic-integrals', 'brownian-motion', 'partial-differential-equations']"
1261417,Hasse Diagram Correct?,"I've had to make Hasse Diagrams before, but they've always been, for lack of a better word, pretty. The lines haven't had any complicated back and forth or the like. The jump that 4 and 6 have to do to go above 24 is killing me, same thing for that crossing line from 24 to 72. Is that correct? Is there a better way to do it? Any help appreciated, thanks! Edit: In case it wasn't clear, it's using the divides relation.","['elementary-set-theory', 'relations', 'discrete-mathematics']"
1261432,user friendly proof of fundamental theorem of calculus,"Silly question. Can someone show me a nice easy to follow proof on the fundamental theorem of calculus. More specifically, $\displaystyle\int_{a}^{b}f(x)dx = F(b) - F(a)$ I know that by just googling fundamental theorem of calculus, one can get all sorts of answers, but for some odd reason I have a hard time following the arguments.","['calculus', 'real-analysis']"
1261453,Understanding composition of vector fields,"I've finished a first course on differential geometry and I still find it confusing on how to compose/multiply two vector fields. Let's assume that $X$ and $Y$ are two vector fields on a smooth manifold. Then my understanding of $XY$ is that:
$$XY f (p)=X_p (Y f).$$
Therefore if $X$ and $Y$ belong to the Lie algebra of a group $G$, then
$$XY f(g)=X_g(Yf)=\frac{d}{dt} (Yf)(g\exp tX_e)|_{t=0}=
\frac{d}{dt}\frac{d}{ds} f(g\exp tX_e\exp sY_e)|_{s=0}|_{t=0}.$$ Does this sound OK so far? Now I want to set $X=Y$ to get
$$XX f(g)=
\frac{d}{dt}\frac{d}{ds} f(g\exp (t+s)X_e)|_{s=0}|_{t=0}.$$
However, the expression that I have in my notes for $XX$ is
$$XX f(g)=
\frac{d^2}{dt^2}f(g\exp (t)X_e)|_{t=0}.$$ Are these two expressions the same?!",['differential-geometry']
1261459,Lines cutting regions,"$15$ lines are drawn in a plane such that $4$ of them are parallel. What is the maximum number of regions into which the plane is divided? How many of the regions have finite area (bounded)? My Attempt The total number of regions that $n$ lines divides the plane into, is given by $\dfrac{n(n+1)}{2} +1$ . So I got $\dfrac{15\cdot16}{2} + 1 = 121$ . But $4$ lines are parallel. So instead of getting $\dfrac{4\cdot5}{2} + 1 = 11$ regions, we only get $4 + 1 = 5$ regions. So the actual number of regions are $121 - 11 + 5 = 115$ A single line divides the plane in $2$ infinite (unbounded) regions. So $15$ lines can give $30$ infinite regions. But $4$ of them are parallel. So we get only $5$ infinite regions instead of $8$ . So the actual number of infinite regions are $30 - 8 + 5 = 27$ . Hence, the total finite (bounded) regions are $115-27 = 88$ . While i got the answer to the second question as $88$ the original answer is $85$ . Can someone please tell me where I am going wrong? Also if there is any better approach please let me know.","['euclidean-geometry', 'geometry', 'combinatorics']"
1261462,How to reconstruct a symmetric matrix given the eigenvalues and eigenvectors?,"I am trying to reconstruct a symmetric 3 x 3 matrix from just its eigenvalues and eigenvectors. I think the solution involves orthogonalizing two of the eigenvectors using the Gram-Schmidt procedure, but I am not sure why or how. Thanks.","['eigenvalues-eigenvectors', 'linear-algebra', 'symmetric-matrices', 'matrices']"
1261465,$(A_f)_{g/f^{n_0}}\cong A_{fg}$ (localization with the powers of an element),"I'm working in a problem from Hartshorne Algebraic Geometry.
But I need a result from Commutative Algebra. Given a commutative ring $B$ with $1$. For each $b \in B$ define the ring $B_b$ as the localizated ring by the multiplicative set $\{b^n:n\ge 1 \}$. Given a commutative ring $A$ and elements $f,g \in A$ and some $n_0 \in \mathbb{N}$ I want to know if it's true that $(A_f)_{g/f^{n_0}}\cong A_{fg}$. I tried to construct an isomorphism but I could not )=
If this result is true, and I have an explicit isomorphism, then I'll be able to prove that $(X_f,O_X|_{X_f})\cong (Spec(A_f),O_{Spec(A_f)})$ as locally ringed spaces. But I need to prove the above result. Thanks","['algebraic-geometry', 'localization', 'commutative-algebra']"
1261467,"Sets $A,B,C$ with $B\subseteq C$, prove that $(A-B)-C=A-C$","Ran across this and couldn't figure out how you would give a formal proof. It seems intuitive, in that $(A-B)-C$ is the elements in $A$ but not in $B$, and then also remove the elements from $(A-B)$ that are in $C$. But since $B$ is a subset of $C$, it almost feels like the subtraction of $B$ from $A$ is redundant, since they'd be removed when $C$ is subtracted anyway. Help would be appreciated, thanks!","['elementary-set-theory', 'discrete-mathematics']"
1261515,Dog Bone Contour Integral,"Would someone please help me understand how to integrate $$
\ \int_0^1 (x^2-1)^{-1/2}dx\, ?
$$ This is a homework problem from Marsden Basic Complex Analysis. The text book suggested using a ""dog bone"" contour and finding the residue of a branch of $(z^2-1)^{-1/2}$ at infinity. I believe the residue at infinity is 1. After factoring  $$
\ (z^2-1)^{-1/2}\ = (z-1)^{-1/2}\ (z+1)^{-1/2}\ 
$$
I chose a branch cut of $(-\infty , -1] \;$ for $\;(z+1)^{-1/2}$ and $(-\infty , 1]$ for $(z-1)^{-1/2}$.
 I pretty sure that means   $\: -\pi \: <\arg(z-1)< \:\pi$ and $\: -\pi \: <\arg(z+1)< \:\pi$. This problem is so confusing. I've working on it for days and it's driving me crazy. Any help would be greatly appreciated.","['contour-integration', 'complex-analysis', 'residue-calculus']"
1261529,Why are there four independent solutions of Mathieu equation instead of two?,"Consider Mathieu equation: $$\frac{d^2}{d\xi^2}R(\xi)+(a-2q\cos(2\xi))R(\xi)=0.$$ It's a second order ODE, so it should have two linearly independent solutions. One of the choices is to denote one as Mathieu sine $\operatorname{S}(a,q,\xi)$ and another as Mathieu cosine $\operatorname{C}(a,q,\xi)$. But many books instead take some strange analogy to Bessel functions. Namely, they say that two functions, cosine-elliptic $\operatorname{ce}_r(\xi,q)$ and sine-elliptic $\operatorname{se}_r(\xi,q)$ are solutions of the first kind analogous to one Bessel $\operatorname{J}_\nu(x)$ function and two another functions $\operatorname{fe}_r(\xi,q)$ and $\operatorname{ge}_r(\xi,q)$ are solutions of the second kind analogous to one Neumann $\operatorname{Y}_\nu(x)$ function. Actually, the analogy with Bessel functions seems to go only for modified aka radial Mathieu equation, but the number of functions is the same for angular equation. But sine-elliptic $\operatorname{se}$ and cosine-elliptic $\operatorname{ce}$ are already linearly independent, why are they both in the first kind? And why is there another pair of linearly independent functions $\operatorname{fe}$ and $\operatorname{ge}$? What is so much different between Mathieu and Bessel equations that the former has twice the number of solutions than the latter? My guess is that the pairs $\operatorname{se}$ and $\operatorname{fe}$ and similarly $\operatorname{ce}$ and $\operatorname{ge}$ correspond to $\operatorname{S}$ and $\operatorname{C}$ correspondingly, but have differing sets of characteristic exponents: the functions of the first kind have real characteristic exponents (corresponding to bands in solid-state physics) while those of the second kind have complex ones (corresponding to band gaps). Is this right?","['ordinary-differential-equations', 'special-functions']"
1261540,Why does the midpoint method have error $O(h^2)$ [duplicate],"This question already has answers here : Numerical Approximation of Differential Equations with Midpoint Method (3 answers) Closed 9 years ago . In solving an ode $$ y'(t) = f(t, y(t)), \quad y(t_0) = y_0 $$ the midpoint method estimates $$y_{n+1} = y_n + hf\left(t_n+\frac{h}{2},y_n+\frac{h}{2}f(t_n, y_n)\right)$$ But why is the error $O(h^2)$? The estimate for Euler's method is straightforward since it's just the Taylor approximation, but here it doesn't look like a Taylor approximation.","['numerical-methods', 'ordinary-differential-equations']"
1261600,Find the sum of values of $x$ such that $|x+2| +|x-3| +|x+4| + |x+5| = 18$,I tried it by finding the different values at the $4$ inflection points of the graph. Then didn't know how to proceed. Am I correct till here?,['algebra-precalculus']
1261611,Find derivative of integrate square function [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I am finding a solution of that function. Could you have me to resolve it $$F=\left( \int {(ax+b-c)}^2 dx \right) +\lambda_1(a-m)^2+\lambda_2(b-n)^2$$ where $c,m,n ,\lambda_1,\lambda_2$ are constant How to find $$ \frac{\partial F}{\partial a}=?$$ $$ \frac{\partial F}{\partial b}=?$$
$$ \frac{\partial F}{\partial x}=?$$ Update: Sorry, I just one more require How to find $a,b,x$ if I set $$ \frac{\partial F}{\partial a}=0$$ $$ \frac{\partial F}{\partial b}=0$$
$$ \frac{\partial F}{\partial x}=0$$","['derivatives', 'partial-derivative', 'calculus', 'integration']"
1261667,Summation of a function,"Let $n$ is a positive integer. $n = p_1^{e_1}p_2^{e_2}...p_k^{e_k}$ is the complete prime factorization of $n$. Let me define a function $f(n)$ $f(n) = p_1^{c_1}p_2^{c_2}...p_k^{c_k}$ where $c_k = e_k$ if $e_k$ is divisible by $p_k$, otherwise $c_k = e_k - 1$ Example: $72 = 2^33^2$, so $f(72) = 2^{3-1}3^{2-1} = 2^{2}3^{1}=12$ $144 = 2^43^2$, so $f(144) = 2^{4}3^{2-1} = 2^{4}3^{1}=48$, as $4$ is divisible by $2$, exponent of $2$ remains same. Now let $$F(N) = \sum_{n=2}^N f(n)$$ Example: $F(10) = 1 + 1 + 4 + 1 + 1 + 1 + 4 + 3 + 1 = 17$ Now I want to evaluate $F(N)$ for a fairly large value of $N$, say $10^{12}$. Can I do it without factorizing each number?","['inclusion-exclusion', 'summation', 'number-theory', 'project-euler', 'zeta-functions']"
1261680,Showing the almost everywhere equality of two unions of sets,"Sorry for the ambiguous title. I am trying to prove this seemingly simple statement. Let $\{A_i\}$ and $\{B_i\}$ be sequences of measurable sets in a measure space $(X,\mathcal{A},\mu)$ such that $\mu(A_i\triangle B_i) = 0$. Then the following holds
$$\mu((\cup A_i) \triangle (\cup B_i)) = \mu((\cap A_i) \triangle (\cap B_i))=0$$ By hypothesis $\mu(A_i\setminus B_i) = \mu(B_i\setminus A_i) = 0$. But when I expand $((\cup A_i) \setminus (\cup B_i))$ and $((\cup B_i) \setminus (\cup A_i))$ I get cross terms like $(A_i\setminus B_j)$ with $i\neq j$. I don't know how to handle those. I feel like I shouldn't start with expanding the set difference of the unions but I don't know what else to do.",['measure-theory']
1261685,"If a number is of type $n^n$, how to identify that? Example: 256 is $4^4$, 3125 is $5^5$","If a number is of type $n^n$, how to identify that? Example: 256 is $4^4$, 3125 is $5^5$. I have to write a code for that.","['number-theory', 'elementary-number-theory']"
1261688,"Only finitely many $a, b$ such that $2+3^n+5^{n^2}=2^a7^b$ for some $n$?","Let $(a,b)$ be a pair of positive integers such that $$2+3^n+5^{n^2}=2^a7^b$$ for some positive integer $n$. Is it true that there are only finitely many such pairs? I don't know the answer to such question, but I guess that it is positive..","['number-theory', 'diophantine-equations', 'elementary-number-theory']"
1261711,Residues of $z^2\sin(\frac{1}{z})$,"I must find the residues of $z^2\sin(\frac{1}{z})$ at $z = 0$. Since $z = 0$ seems to be an Essential Singularity, i'm not sure how I can continue to find the residue of the function. Usually I am able to apply the Taylor Series and then find the $z^{-1}$ coefficient, but in this case I do not get a $z^{-1}$ term.",['complex-analysis']
1261736,"If $\lim\limits_{x \rightarrow \infty} f'(x)^2 + f^3(x) = 0$ , show that $\lim \limits_ {x\rightarrow \infty} f(x) = 0$ [duplicate]","This question already has answers here : How prove that $\lim\limits_{x\to+\infty}f(x)=\lim\limits_{x\to+\infty}f'(x)=0$ if $\lim\limits_{x\to+\infty}([f'(x)]^2+f^3(x))=0$? (3 answers) Closed 9 years ago . If 
 $f : \mathbb{R} \rightarrow \mathbb{R}$ is a differentiable function and $\lim\limits_{x  \rightarrow \infty} f'(x)^2 + f^3(x) = 0$ , show that $\lim\limits_{x\rightarrow \infty} f(x) = 0$. I really have no clue how to start, I tried things like MVT and using definition of derivatives but I really can't figure this out.","['calculus', 'limits', 'derivatives']"
1261755,Result of a $2D$ random walk with position dependent probabilities,"I was just wondering about $2D$ random walks when I got the idea of a position dependent $2D$ random walk: A man is initially at $(x,y)$ and can move in a line parallel to the  X and Y-axis only. The probability that the man takes a step in the X-direction is $\frac {|x|}{|x|+|y|}$ and the probability that the man takes a step along the Y-direction is $\frac {|y|}{|x|+|y|}$. Given that the man takes a step in the X-direction the probability that he takes a step in $+$ve x-direction is $\frac {|x|}{1+|x|}$ and that he takes a step along the negative X-direction is $\frac 1{1+|x|}$. Given that he takes a step in the y-direction probability that he takes a step along the positive Y-direction is $\frac 1{1+|y|}$ and probability that he takes a step in the negative Y-direction is $\frac {|y|}{1+|y|}$. Find the probability that his motion along a direction stops after $n$ steps. Also find probability that his motion stops along a direction at some point of time. (Consider that reaching the origin at any point of time will terminate his walk.) I have no idea about how to do the question, I think some kind of recurrence relation would help but which one?","['probability-theory', 'recurrence-relations', 'probability', 'combinatorics']"
