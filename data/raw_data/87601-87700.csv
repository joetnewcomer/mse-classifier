question_id,title,body,tags
1171656,Number of non-isomorphic non-abelian groups of order 10,"Number of non-isomorphic non-abelian groups of order 10 Let $G$ be  a group of order 10.Let $a\neq e \in G$ then it is not possible that all elements are of order 2 otherwise $G$ becomes abelian. Let $a$ has order 5.then $H=\langle a\rangle$ then $H$ has order 5. Now $H$ will have two cosets say $bH,H$ Now $b^2\in H$ then $b^2$ is one of $e,a,a^2,a^3,a^4$  but $b^2\neq a^i$ for $i=1,2,3,4$ otherwise $o(b)=10$ contradiction Again $bab^{-1}\in H$ as $H$ is normal  also $bab^{-1}$ is one of $e,a,a^2,a^3,a^4$ Now I am getting possibilities of $bab^{-1}$ to be $a^2,a^3,a^4$ Does that mean there are 3 non-isomorphic non-abelian groups of order 10?",['group-theory']
1171665,Why doesn't Alaoglu's theorem imply that $X^{*}$ is locally compact in the weak* topology?,"I must be missing something basic and simple: If $X$ is a normed vector space and the closed unit ball in $X^{*}$ is weak* compact, and translations and dilations are homeomorphisms, why isn't $X^{*}$ locally compact?","['general-topology', 'topological-vector-spaces', 'functional-analysis', 'banach-spaces']"
1171677,Limit of ratio of areas of triangles defined by tangents to a circle,"Let $AB $ be an arc of a circle. Tangents are drawn at $A $ and $B $ to meet at $C $. Let $M $ be the midpoint of arc $AB $. Tangent drawn at $M $ meet $AC $ and $BC $ at $D $, $E $ respectively. Evaluate $$\lim_{AB \to 0}\frac {\Delta ABC}{\Delta DEC} $$ I don't think evaluating the limit will be a problem. But how do I find the areas of the triangles and in what parameters?","['geometry', 'triangles', 'circles', 'limits']"
1171694,Prove that $1989\mid n^{n^{n^{n}}} - n^{n^{n}}$,Having difficulty in proving this: $1989\mid n^{n^{n^{n}}} - n^{n^{n}}$ for all $n \in \Bbb N$. Prime factorization of $1989$ is $3^2 \times 13 \times 17$. Please Help!,"['modular-arithmetic', 'exponentiation', 'number-theory']"
1171715,Condition for increase in the optimum of a general function,"For a function $f(x,y)$ with the following properties: $f(x,y)$ is strictly increasing as a function of $x$ $f(x,y)$ is strictly decreasing as a function of $y$ $\lim_{x\to\infty}\frac{\partial f(x,y)}{\partial x}=0$ $\lim_{y\to\infty}\frac{\partial f(x,y)}{\partial y}=0$ I'm studying the optimum of $\frac{f(x,y)}{x}$ with respect to $x$, denoted $x_0$: $$
Q=\frac{\partial (f(x,y)/x)}{\partial x}\bigg|_{x=x_0}=0.
$$ Assuming $x_0$ is an optimum, I wish to study how $x_0$ changes as $y$ changes, and I arrive at the following $$
\frac{dx_0}{dy}=-\frac{\partial Q/ \partial y}{\partial Q/ \partial x_0}.
$$ Given that $x_0$ is an optimum, the denominator of the above will be negative and hence the sign of $\frac{dx_0}{dy}$ will be the same as the sign of $\frac{\partial Q}{\partial y}$. And we have the following $$
sgn\bigg(\frac{\partial Q}{\partial y}\bigg)=sgn\bigg(x_0\frac{\partial (\partial f(x_0,y)/\partial x_0)}{\partial y}-\frac{\partial f(x_0,y)}{\partial y}\bigg).
$$ Can we say that the following will always be true? $$
\frac{\partial Q}{\partial y}\neq 0
$$ In other words, if $y$ changes, the optimum $x_0$ must also change. If not, can we say something about the cases where this will not be true? Thanks a lot for your help.","['optimization', 'calculus', 'derivatives']"
1171733,Convergence of Inverse of Convergent Sequence,Let $\{x_n\}$ be a sequence in $\mathbb{R}$ where $\forall n\in\mathbb{N}:x_n\neq 0$ and it converges to some $x\neq 0$. If the sequence is NOT monotone is it ever true that $\frac{1}{x_n}\rightarrow\frac{1}{x}$? If so what other conditions (if any) are needed and how would you show it. Thanks in advance any feedback is greatly appreciated.,"['sequences-and-series', 'real-analysis', 'limits']"
1171761,"Homeomorphism from $(-1,1)$ to $\mathbb R$","I know that $f: (-1,1) \to \mathbb R$ defined by $f(x)=\tan \Big(\dfrac{\pi}2x \Big)$ is a homeomorphism . I am looking for some other homeomorphism between $(-1,1)$ and $\mathbb R$ which is not in such trigonometric form ; preferably algebraic , or at least in logarithmic or exponential form . Please help","['soft-question', 'metric-spaces', 'continuity', 'functions']"
1171777,Prove The Orthogonal Complement of an Intersection is the Sum of Orthogonal Complements,How does one prove that $(A∩B)^⊥=A^⊥+B^⊥$? Seems a bit harder than proving $(A+B)^⊥=A^⊥∩B^⊥$.,"['vector-spaces', 'linear-algebra', 'orthogonality']"
1171792,Prove an integral inequality $|\int\limits_0^1f(x)dx|\leq\frac{1-a+b}{4}M$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $f$ be a differentiable function on $[0,1]$ and $a,b\in(0,1)$ such that $a<b$, $\int\limits_0^af(x)dx=\int\limits_b^1f(x)dx=0$. Show that: $$\left|\int_0^{1} f(x)\,dx\,\right|\leq\frac{1-a+b}{4}\,M$$ where $M=\sup\limits_{x\in[0,1]}|f'(x)|$.","['integration', 'integral-inequality']"
1171842,Finding the symplectic matrix in Williamson's theorem,"tl;dr: How do I construct the symplectic matrix in Williamson's theorem? I am interested in a constructive proof/version of Williamson's theorem in symplectic linear algebra. Maybe I'm just missing a simple step, so here is what I know: Let us fix the symplectic form $J=\begin{pmatrix} 0_n & 1_n \\ -1_n & 0_n\end{pmatrix}$. Recall: Theorem: Let $M\in\mathbb{R}^{2n\times 2n}$ be a positive-definite matrix, then there exists a symplectic matrix $S\in Sp(2n,\mathbb{R})$ and a diagonal matrix $D\in\mathbb{R}^{n\times n}$ such that $$M=S^T\tilde{D}S$$ where $\tilde{D}=\operatorname{diag}(D,D)$ is diagonal. My basic interest is in how to construct the $S$ (i.e. write a matlab program that does this). This theorem is cited and used in various cases, however one cannot find many proofs despite the original. Here is the one proof I found (the most important step for me is unjustified). Let us denote by $\langle \cdot,\cdot \rangle_M:=\langle \cdot, M\cdot \rangle$ the symmetric bilinear form defined by $M$ and by $\sigma(\cdot,\cdot)=\langle \cdot, J\cdot \rangle$ the symplectic form. The theorem asserts that there is an $M$-orthogonal and symplectic basis. The failure of the basis to be $M$-orthonormal is then given by $D$. In order to find such a basis, it seems a good idea to look at $JM$. As this is a real matrix, which is antisymmetric with respect to $\langle\cdot,\cdot\rangle_M$, its eigenvalues come in pairs $\pm i\lambda_j$ ($j=1,\ldots n$) and the corresponding eigenvectors in pairs $e_j\pm if_j$ with real $e_j,f_j$ (the proof in the link considers the eigenvalues/eigenvectors of $M^{-1}J$ instead). The claim in the proof is now that $\{e_j,f_j\}_j$ forms an $M$-orthonormal basis. If this is the case, one can easily see that $\delta_{jk}=\langle e_j,Me_k\rangle=-\lambda_k\sigma(e_j,f_k)$ and $0=\langle e_j,Mf_k\rangle=\lambda_k\sigma(e_j,e_k)$ and similarly for $f_k$, hence the basis is indeed $M$-orthogonal and symplectic after normalization by $\sqrt{\lambda_j}$. The matrix $S$ should then consist of the normalized $f_j$ and $e_j$ as columns. Q1: Why is $\{e_j,f_j\}_j$ an orthonormal basis w.r.t to $M$? Matlab suggests that most of the times, it is, but I seem to miss something fundamental, because I don't see it. Why most of the times? Eigenvalue multiplicities seem to play a role here: Q2: If $\lambda_j\neq \lambda_k$ always, then Q1 seems to be true. If there are some eigenvalue multiplicities, this seems to be wrong, probably because of the non-uniqueness of eigenvectors. How can I fix this? I suspect that some Gram-Schmidt wrt. $M$ is necessary, but since I can't answer Q1, I can't see whether this does the trick.","['linear-algebra', 'bilinear-form', 'symplectic-linear-algebra']"
1171866,Orbits of $2 \times 2$ matrices over $\Bbb F_2$,"Let $X$ be the set of $2 \times 2$ matrices over field $\Bbb F_2$, and $G \subset X$ be the group of invertible $2 \times 2$ matrices over $\Bbb F_2$ Let $G$ act on $X$ by $g*x = gxg^{-1}$ Need to find the orbits of $X$ under this action. Computing for each $x \in X$ would require $6 \times 16 =96$ calculations...What is a quicker way to obtain the distinct orbits?","['group-theory', 'abstract-algebra']"
1171884,"Show that $2\nabla \sqrt f\,+\,x \sqrt f=0$ (a.e.). $\implies$ $\sqrt f\in \mathcal C_0$. (Derivatives are in weak sense)","Show that $2\nabla \sqrt f\,+\,x \sqrt f=0$ (a.e.). $\implies$ $\sqrt f\in \mathcal C_0$. (Derivatives are in weak sense) Given that $f\in L^1(\mathbb R^d),f\geq 0,\int_{\mathbb R^d}f=1, \int_{\mathbb R^d}f(x)|x|^2\leq d$ As $f\in L^1(\mathbb R^d)$ so $\lim_{|x|\rightarrow \infty}\sqrt f(x)=0$ only thing remaining to show is continuity. Author feels that it has to be shown by using Sobolev inequality and bootstrap arguments. Regards. Harish","['sobolev-spaces', 'partial-differential-equations', 'analysis']"
1171941,"If $\sup_{T\in \tau}|y^*(Tx)|<\infty$ then $\tau$ is bounded in $L(X,Y)$","Let $X$ be a Banach space, $Y$ a normed vector space and $\tau\subset L(X,Y)$. Show that if $\sup_{T\in \tau}|y^*(Tx)|<\infty$ for all $x\in X,y^*\in Y^*$ then $\tau$ is bounded in $L(X,Y)$. Seems that we have to use the Banach-Steinhaus theorem which would yield that $\tau\subset L(X,Y)$ is bounded. But I don't know how to show that $\tau$ is points wise bounded. I don't see any connection to the assumption.","['operator-theory', 'functional-analysis', 'banach-spaces']"
1171966,A recurrence relation problem: $\frac {a_{n-1}.a_{n+1}} {a_n^2} = 1 + \frac 1 n$,"I need to solve this recurrence problem to find $a_n$ $\dfrac {a_{n-1}.a_{n+1}} {a_n^2} = 1 + \dfrac 1 n$ It is what I tried so far: $$\log (\dfrac {a_{n-1}.a_{n+1}} {a_n^2}) = \log(1 + \dfrac 1 n)$$
$$=> \log a_{n-1} + log a_{n+1} - 2log a_{n} = -\log n$$
$$\log a_n = b_n ---assume$$
$$b_{n-1}+b_{n+1}-2b_n = -\log n$$ This is a second order recurrence relation. Now to calculate $b_n^h$ (the general solution) : $$b_{n+1} - 2b_n+b_{n-1} = 0$$
$$b_n = Cr^n$$
$$Cr^{n+1} - 2Cr^n+Cr^{n-1} = 0$$
$$ r^2 - 2r+1 = 0$$
$$r_1 = 1, r_2 = 1$$
$$a_n^h = 1^n + n (1^n)$$ My first question is, did I do every thing right in calculating $a_n^h$ so far? The second problem is I don't know how to calculate the private solution, $a_n^p$ I mean. the $f(n) = -\log n$ and I don't know what  $a_n^p$ should be. UPDATE I forgot to include that $a_0 =1 , a_1 = 2$","['recurrence-relations', 'combinatorics']"
1172018,Semisimplicity is equivalent to each simple left module is projective?,"As it is well-known, a ring with unity $R$ is semisimple if and only if each left $R$-module is projective. My question: Is semisimplicity of $R$ equivalent to each simple left $R$-module being projective? I think it is true for a finite dimensional algebra $R$. But, in general, I could not reach any conclusion. Any help would be thanked!","['noncommutative-algebra', 'ring-theory', 'projective-module', 'abstract-algebra']"
1172043,"Finite strings of integers from $\{1,1,2,2,\ldots,n,n\}$ such that there are $k$ elements between the two $k$s","A friend was recently posed with the following interview questions. Write a string of six elements containing two $1$s, two $2$s, and two $3$s such that between the two appearances of the '$k$' elements, there are exactly $k$ other elements. Do the same but with $\{1,1,2,2,3,3,4,4\}$. Show that this cannot be done for $\{1,1,2,2,3,3,4,4,5,5\}$. The first two are rather simple and it just requires playing around with different arrangements. The unique (up to reflection) solution for the first is given by $312132$, and for the second we have $41312432$. The third question is interesting because it has the following elegant (non brute-forced) solution. Proof . Assign the value of the position of each element to that element. So for example if $x_3$ is the leftmost $3$-element and it is found in the second position then set $x_3=2$. We note that if $x_k=a$, then the rightmost $k$-element $y_k$ will be $k+1$ positions to the right and so $y_k=a+k+1$. If we take the sum of all $x_k$ and $y_k$ then we find that $$\sum_{k=1}^{5} x_k+y_k = \sum_{k=1}^{5} 2x_k+k+1 = (2\sum_{k=1}^{5} x_k)+20$$ which is even, but also $$\sum_{k=1}^{5} x_k+y_k = \sum_{i=1}^{10} i = 55$$ which is odd. So we reach a contradiction. $\square$ One quickly finds that this same argument extends to all $n$ which are congruent to $1$ or $2$ modulo $4$ (because the contradiction is reached whenever $\sum_{i=1}^{2n} i\neq \sum_{i=1}^n n+1\mod 2$). My question is, what happens in the other cases. Can this be done for any $n\geq 7$? If so, for how many $n$?","['modular-arithmetic', 'sequences-and-series', 'combinatorics']"
1172077,Term for similarity transformation which is not a translation,"What's the best (i.e. most concise) term to refer to an orientation-preserving similarity transformation which is not a translation? Here are some descriptions I could think of, but all of them feel rather bulky. I hope they are as equivalent to one another as I think they are, and I hope there is something simpler equivalent to all of them. a direct similarity transformation which is not a translation (by a non-zero displacement) a homothety (possibly with factor $1$) followed by rotation (with possibly zero angle) an orientation-preserving similarity with at least one fixed point a direct similitude with a well-defined similitude center , or the identity what $z\mapsto a(z-c)+c$ describes in the complex plane , for some fixed $a,c\in\mathbb C$ (with $a\neq 0$) As a native German, I tend to think about this using the German term “ Drehstreckung ” which literally translates to “rotation-dilation”. I'm somewhat surprised by the difficulty I have in finding an exact English translation for this concept.","['affine-geometry', 'geometry', 'terminology', 'euclidean-geometry']"
1172086,Convergence of $\sum^\infty_{n=1}\arctan(\frac 1 {\sqrt n}) $ and how to approach trigonometric expressions in sums,"Does $$\sum^\infty_{n=1}\arctan\left(\frac 1 {\sqrt n}\right)$$ converge? The series probably diverges and I should probably use the comparison test, but I don't know what to use. Note: no integral test. My other question is, in general, when there are trigonometric expressions in series, what is the recommended approach?","['sequences-and-series', 'divergent-series', 'calculus']"
1172109,When does differentiability of $g\circ f$ and $f$ resp. $g$ imply differentiablity of $g$ resp. $f$?,"To me the following seems intuitively true: If $f$ is differentiable at $x$ with surjective derivative then $g$ is differentiable at $f(x)$ iff $g\circ f$ is differentiable at $x$. On the other hand, if $g$ is differentiable at $f(x)$ with injective derivative then $f$ is differentiable at $x$ iff $g\circ f$ is differentiable at $x$. I assume $f,g$ to be mappings between Banach spaces but I guess this should not make a huge difference.","['calculus', 'banach-spaces', 'derivatives', 'function-and-relation-composition']"
1172118,Bochner Integral: Derivative,"Given a Banach space $E$. Consider a continuous derivative:
$$F'\in\mathcal{C}(\mathbb{R},E):\quad\int_\mathbb{R}\|F'(s)\|\mathrm{d}s<\infty$$ Then its integral computes as:
$$\int_a^bF'(s)\mathrm{d}s=F(b)-F(a)$$
How to prove this by modern tools?","['functional-analysis', 'real-analysis', 'banach-spaces']"
1172119,How to prove $x^3-y^3 = (x-y)(x^2+xy+y^2)$ without expand the right side?,I can prove that $x^3-y^3 = (x-y)(x^2+xy+y^2)$ by expanding the right side. $x^3-y^3 = (x-y)x^2 + (x-y)(xy) + (x-y)y^2$ $\implies x^3 - x^2y + x^2y -xy^2 + xy^2 - y^3$ $\implies x^3 - y^3$ I was wondering what are other ways to prove that $x^3-y^3 = (x-y)(x^2+xy+y^2)$,"['alternative-proof', 'algebra-precalculus', 'polynomials']"
1172144,Problem on limits $\lim_{n \to \infty} \frac{\sqrt{1}+\sqrt{2}+\sqrt{3}+\sqrt{4}+...+\sqrt{n}}{n\sqrt{n}}$,I have the problem with the following limit: $$\lim_{n \to ∞} \frac{\sqrt{1}+\sqrt{2}+\sqrt{3}+\sqrt{4}+...+\sqrt{n}}{n\sqrt{n}}$$,"['radicals', 'calculus', 'limits']"
1172151,First order differential equation with non constant coefficients,"I have the following system : $$\begin{cases}(t^2+1)x'(t)=tx+y+2t^2+1\\(t^2+1)y'(t)=-x+ty+3t\end{cases}$$ How can it be solved ? What I have tried so far : polynomials of the first, second degree as solutions - didn't work One can notice that if we use $X=\begin{bmatrix}x\\y\end{bmatrix},A=\begin{bmatrix}t&1\\-1&t\end{bmatrix},B=\begin{bmatrix}2t^2-1\\3t\end{bmatrix}$ then the system becomes $(t^2+1)X'=AX+B$, and $t^2+1=\det A$. I'm pretty sure that this last result is supposed to help, but I haven't been able to find a way to use it.","['ordinary-differential-equations', 'calculus']"
1172153,Prove that a multivariable function has a global minimum,"I'm doing an Introduction to Machine Learning course by myself using some open university coursebook and it has the following question which I've tried to solve, but to no avail: Let there be a Lipschitz function $g:\mathbb{R}^n\rightarrow \mathbb{R}$. Meaning the following holds: $|g(x)-g(y)| \leq L \,\cdot \parallel x-y\parallel$ for some constant $L\geq 0$. Prove that the following function
  $f:\mathbb{R}^n\rightarrow\mathbb{R}$ defined as: $f(x) = \parallel\,x\,\parallel ^2 + \, g(x)$ has a global minimum OK so I want to show that function basically goes to infinity in every direction but I am having trouble proving it. I tried going directly from the definition but I can't figure out what the Lipschitz function gives me (I'm sure I need to use it somehow) Any help is appreciated!
Thank you to anyone who helps","['multivariable-calculus', 'calculus', 'machine-learning', 'lipschitz-functions']"
1172162,Kummer Theory - Example of Subgroup of $K^{*}$ containing $K^{*m}$ for global fields.,"I am trying to understand Kummer theory and I wish to apply it to global fields, so our field $K$ containing $\mu_m$ should be $\mathbb{Q}(\zeta_m)$. Let $B$ be a subgroup of $K^{*}$ containing $K^{*m}$. We define $K_B:=K(B^{1/m})$ to be the compositum of all $K(a^{1/m})$ such that $a\in B$.
Define $G=\text{Gal}(K_B/K)$. (This is necessarily a Galois extension as $K_B$ is a splitting field for $x^n-a$ for any $a\in B$.) We then have the Kummer Pairing, 
$$\kappa:G\:\text{x}\:B\rightarrow B$$ given by $\kappa(\sigma,a)=\frac{\sigma(\alpha)}{\alpha}$, for $\alpha^m=a$. My question is this: Can we think of an explicit example of $B$ such that $(B:K^{*m})$ is finite? I am struggling to see how this works","['kummer-theory', 'algebraic-number-theory', 'abstract-algebra', 'number-theory']"
1172185,About the divergence of $\sum^\infty_{n=1}\frac 1 {n\cdot n^{1/n}}$,There's a rule of thumb that if $a>1$ then the series: $\displaystyle\sum^\infty_{n=1}\frac 1 {n^a}$ converges. Now the series: $\displaystyle\sum^\infty_{n=1}\frac 1 {n\cdot n^{\frac 1 n}}=\sum^\infty_{n=1}\frac 1 { n^{1+\frac 1 n}}$ diverges (by the limit comparison test) but we have here a series such that the exponenet is larger than $1$ for all $n$. So my question is why is the rule of thumb doesn't work in this case?,"['sequences-and-series', 'divergent-series', 'calculus']"
1172258,Analytic solution of: ${u}''+\frac{1}{x}{u}'=-\delta e^{u}$,"I am trying to find the analytic solution of 
$${u}''+\frac{1}{x}{u}'=-\delta e^{u}$$
given the homogeneous mixed boundary conditions 
$${u'(0)}=0$$
$$u(1)=0$$ How would one attack such a problem? I have been given that the analytic solution is 
$$u=ln\left ( \frac{8a/\delta }{(ax^2+1)^2} \right )$$ 
where $a$ solves $8a=\delta (a+1)^2$. My approach (EDITED): I was given a hint: use the relation $x=e^{-y}$. I proceeded with that and used the chain-rule as follows
$$(u\circ y)'(x)=\frac{d}{dx}u(y(x))=u'(y(x))y'(x)=u'x=-u'e^{y}$$
$$(u\circ y)''(x)=\frac{d}{dx}[u'(y(x))y'(x)]=u''(y(x))[y'(x)]^2+u'(y(x))y''(x)=u''e^{2y}+u'e^{2y}$$
substituting these relations into the ODE yields
$$e^{2y}u''=-\delta e^{u}$$
or in an alternative form
$$u''=-\delta e^{-2y} e^{u}$$ which is separable ( thanks for the help ).","['ordinary-differential-equations', 'solution-verification']"
1172323,"Computing $\lim_{(x,y)\to (0,0)}\frac{\sin(x+y)}{x+y}$","I'm trying to compute the following limits and the textbook that I'm looking at suggested the following method. $$\lim_{(x,y)\to (0,0)}\frac{\sin(x+y)}{x+y}$$ $$\lim_{(x,y)\to (0,0)}\frac{\sin(xy)}{xy}$$ Each of these limits are equal to $\lim_{t\to 0}\frac{\sin(t)}{t}=1$. However, I'm curious how to analytically prove that such change in variables yield the correct value of the original limit. That is, when are we justified in changing a vector $(x,y)$ to a single variable $t$? In the single variable case, I'm familiar with such manipulations, but I have a bit of uneasiness in seeing such manipulations for the first time in multivariable case. I'd appreciate it if anyone could explain to me why and when such transformations are justified in the multivariate case.","['multivariable-calculus', 'continuity', 'real-analysis', 'analysis', 'limits']"
1172332,Finding angles plane geometry,"$\Delta ABC$ is obtuse on $B$ with $\angle ABC = 90 + \frac{\angle BAC}2$ and we have a point $D \in AC$ (in the segment, I mean D is in between A and C) such that $\angle BDA = \angle ABD + \frac{\angle BAC}2$ and $DC = BA$. Find $\angle BCA$. What I've done: I've drawn the bissector of $\angle BAC$ and let it meet $BD$ on $F$, then I've got the point $E \in AD$ such that $AE = DC = AB$ therefore we would have $\Delta ABF \equiv \Delta AEF $. The coolest thing I've got from this is that $AF$ is the perpendicular bissector of $BE$ and the triangles $ABC$ and $BEC$ are similar. Believe me or not I've been stuck here for some days ( ._.) I apreciate the trigonometric solution, if nobody comes with a plane geometry solution I will chose the first answer","['geometry', 'triangles', 'intuition']"
1172378,"Is $f(x,y)=xy/(x^{2}+y^{2}) $ differentiable or continuous?","I'm taking a course in Analysis of several variables and the text we're following is Analysis on Manifolds - Munkres. I'm having issues to interpret properly the results I'm getting in my exercises. I'm trying the second exercise in Section 5: Let $f : \mathbb{R}^{2} \to \mathbb{R}$ be defined by setting $f(0)=0$ and $$f(x,y)=xy/(x^{2}+y^{2}) \quad \textit{if} \quad (x,y) \neq 0.$$ (a) For wich vectors $u \neq 0$ does $f^{\prime}(0;u)$ exist? Evaluate it when it exists. (b) Do $D_{1}f$ and $D_{2}f$ exist at $0$ ? (c) Is $f$ differentiable at $0$ ? (d) Is $f$ continous a $0$? For the item (a) I take a vector $u=(h,k)$ and from the definition of $f^{\prime}(a;u)$ I get $$\lim_{t \to 0} \frac{f[(0,0)+ (th,tk)]-f(0,0)}{t}= \lim_{t \to 0} \frac{1}{t}\frac{t^{2}hk}{(th)^{2}+(tk)^{2}}=\lim_{t\to 0} \frac{hk}{t(h^{2}+k^{2})}$$ After this I don't know how to interpret if it actually exist or no, and don't understand either how or what to evaluate when it exists. About the item (b) I did this and I don't know if that's correct: $$D_{1}f(0,0) = \lim_{t \to 0} \frac{f[(0,0)+t(1,0)]-f(0,0)}{t} = lim_{t \to 0} \frac{f(t,0)}{t} = 0$$ And the same with $D_{2}f(0,0) = 0$ Is that correct? With item (c) tried the same with the definition and I got stuck : $$\lim_{h \to 0} \frac{f(0+h)-f(0)-Bh}{|h|} = \lim_{h \to 0} \frac{f(h_{1},h_{2})-B(h_{1},h_{2})}{|(h_{1},h_{2})|}$$ Once I get here I don't know what to do, should I evaluate $f(h_{1},h_{2})$ ? and if I do, then how can I interprate if it is differentialbe at $0$ ? Finally for the item (d) I have no clue how to do it, I really need help for this part. I really appreciate if you help me understading completely how can I do it. Thank you in advance!","['multivariable-calculus', 'derivatives']"
1172379,Universal covering space of wedge sum,"Consider the wedge sum of the unit circle and real projective plane $S^{1} \vee \mathbb{R}P^{2}$. How would one construct a universal covering space for this kind of wege sum? I've tried constructing it using an identical space to the universal covering space of $S^{1} \vee S^{2}$, which is the union of a line with infinitely many copies of $S^{2}$, except with the antipodal map, but wouldn't one need two lines that intersect with each $S^{2}$? But when I try to construct a space with two lines intersecting infinitely many $S^{2}$, I can't find a way to do it and preserve simple-connectedness.","['general-topology', 'covering-spaces', 'algebraic-topology']"
1172430,Can't solve a recurrence,"I am trying to solve the following recurrence: $$T(n) = 9T(n/3)+n^2$$ If I use the master method, I get $n^2\log{n}$ But, I am trying to solve it using substitution. When I try solving it this way, however, I run into trouble. After I roll out the substitution for a few times, I get the following formula: $$T(n) = 9^kT(n/3^k)+n^2\sum_{i=1}^k3^{j-1}$$ I've tried to simplify the summation several ways, but I don't think I am understanding what the next step should be. When I simplify the problem, I keep getting the wrong answer. I know I should eventually set $k$ to be equal to some logarithm so I an reach the base case for T, but right now my main issue it figuring out how to simplify the summation. I am quite bad at simplifying summations, so any detailed steps are greatly appreciated.","['recurrence-relations', 'discrete-mathematics', 'algorithms']"
1172449,Find formula of sum $\sin (nx)$ [duplicate],This question already has answers here : How can we sum up $\sin$ and $\cos$ series when the angles are in arithmetic progression? (8 answers) Closed 9 years ago . I wonder if there is a way to calculate the $$S_n=\sin x + \sin 2x + … + \sin nx$$ but using only derivatives ?,"['complex-numbers', 'calculus']"
1172472,Differentiable approximation of the absolute value function [duplicate],"This question already has answers here : Approximate $|x|$ with a smooth function (5 answers) Closed 3 years ago . Are there any good approximations of the absolute value function which are $C^2$ or at least $C^1$? I've thought about working with exponentials and then adding in more terms to keep the function from growing too fast away from zero, but I was hoping to find something a bit neater.","['approximation-theory', 'calculus', 'derivatives']"
1172473,Trying to solve the equation $2=9^n-28\cdot 3^{n+1}+245$,"I'm trying to solve this equation for $n$.
$$2=9^n-28\cdot 3^{n+1}+245$$ I don't even know if my path was correct when I split up $9^n$ into $3^{2n}$ and then continued with $2=3^{n}(3^{n}-28\cdot 3)+245$.",['algebra-precalculus']
1172566,Witt vector question,"I've started reading various papers and notes on Schemes over the Witt Vectors. In example 8.8 of these: https://www.uni-due.de/~mat903/books/esvibuch.pdf W2 has addition defined as $k \oplus k\cdot p$ for an addition group. In definition 4.1 of this http://arxiv.org/pdf/1301.0857.pdf , and in some others, addition is defined for $W_2(k)$ by $$S_0 = a_0 + b_0,\quad S_1 = a_1 + b_1 - \sum_{0<i<p}\frac{1}{p}\binom{p}{i}a_0^i b_0^{p-i}.$$ I've tried writing out some simple examples, but am still confused. How does these two definitions compare?","['arithmetic-geometry', 'algebraic-geometry', 'number-theory']"
1172576,"How do I find the Cesaro sum of the series $\{1, -1, 1, -1, ...\}$?",I've seen that the Cesaro sum is $1/2$ but haven't been able to find the steps for figuring that out.,['sequences-and-series']
1172586,Separating addition terms in denominator,If I have a fraction such as: $\frac{1+d(6-4a)}{1-a+d(7-4a)}$ then how can I separate it so I have it as $\frac{1}{1-a}+(some-term)$ Thanks.,"['linear-algebra', 'algebra-precalculus', 'partial-fractions']"
1172593,Bounding the density of random variable,"This is a followup to the question in Bounding the Density of the Maximum of N Random Variables I have a random variable, X, whose cdf is bounded as below: $ \Pr \{X \le x \} \le  \underset{i}{\prod} \Pr\left\{\xi_i \le x\right\} $, How do I bound the density  of $X$ ? Can it be written as $f_X \le \sum_i f_{\xi_i} \prod_{j \ne i} \Pr\left\{\xi_j \le x\right\}$. I am trying to write a proof for the above. Any  hints ?","['statistics', 'probability', 'order-statistics', 'probability-theory']"
1172594,Proving the inverse of a relation exists,"A relation is a set of ordered pairs $(x,y)$ that relates $x$ to $y$ somehow. It's a very weak relation in the sense one thing can be related to many things. A function is a special relation where each thing in the domain is related to only one thing in the image. $xRy\iff(x,y)\in R$ Given a relation $R$ the inverse, $R^{-1}$ is ""given by"" $\{(y,x)|(x,y)\in R\}$ $\text{Dom}(R)=\{x|\exists y:xRy\}$
$\text{Ran}(R)=\{y|\exists x:xRy\}$ My problem is I cannot prove these three sets (inverse, domain and range) exist. What have I tried? If I write $\{x|P(x)\}$ where $P$ is some property of $x$ to show it exists I must find a set $A$ that exists, where $P(x)\implies x\in A$ - then this is (uniquely) the set $\{x\in A|P(x)\}$ - obviously the choice of $A$ isn't unique. So I need to find a set $A$ such that $P((x,y),R)\implies (x,y)\in A$ where $P((x,y),R)$ is $(x,y)\in R$ Regarding domain (range will be essentially the same) I am having trouble building a set which contains the first thing in the ordered pairs that make up the relation - I don't have cardinality yet so I am not sure what ""property"" to formulate to extract them. The Axioms I have so far: Axiom of existence: there exists a set with no elements Axiom of extensionality (equality basically?): if two sets have the same elements they are identical Lemma showing the empty set is unique Axiom of schema of comprehension: $\{x\in A|P(x)\}$ exists where P is a property. Lemma showing the set given in the comprehension axiom is unique (justifying my notation) Axiom of a pair: for any A, B there is a set C such that $x\in C\iff[x\in A\text{ or }x\in B]$ Axiom of union, given a set $S$, $x\in\cup S\iff\exists A\in S:x\in A$ Axiom of power set - I think the key is here I am looking for a very rigorous answer",['elementary-set-theory']
1172599,Indefinite integral question: $\int \frac{1}{x\sqrt{x^2+x}}dx$,How can I solve this integral: $$\int \frac{1}{x\sqrt{x^2+x}}dx$$ I first completed the square and got: $$\int \frac{1}{x\sqrt{(x+\frac{1}{2})^2-\frac{1}{4}}}dx$$ Then I factored out 1/4 and got: $$2\int \frac{1}{x\sqrt{(2x+1)^2-1}}dx$$ Then I substituted $2x+1$ with $t$ and got: $$2\int \frac{1}{(t-1)\sqrt{t^2-1}}dt$$ I'm not sure what to do next. Please give me a hint ;),"['closed-form', 'calculus', 'integration', 'indefinite-integrals']"
1172614,Find two linearly independent solutions of the differential equation $(3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3}$,"I want to find two linearly independent solutions of the differential equation $$(3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3}$$ Previously I have seen that the following holds  for the differential equation $y''+ \frac{1}{x}y'-\frac{1}{x^2}y=0, x>0$ : We are looking for solutions of the differential equation of the form $x^r$ . Then the function $x^r$ is a solution of the differential equation at $(0,+\infty)$ if: $$r(r-1)x^{r-2}+ \frac{1}{x} r x^{r-1}- \frac{1}{x^2}x^r=0 \forall x >0 \Rightarrow r=1 \text{ or } r=-1$$ So, the functions $y_1(x)=x, y_2= \frac{1}{x}$ are solutions of the differential equation and it also holds that they are linearly indepedent since $W(y_1, y_2) \neq 0$ For this differential equation $$(3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3}$$ I thought the following: $(3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3} \Rightarrow  y''+ \frac{3}{3x-1}y'-\frac{9}{(3x-1)^2}y=0$ We are looking for solutions of the differential equation of the form $\left( x- \frac{1}{3}\right)^r$ . Then the function $\left( x- \frac{1}{3}\right)^r$ is a solution of the differential equation at $( \frac{1}{3},+\infty)$ if: $$r(r-1) \left( x- \frac{1}{3}\right)^{r-2}+ \frac{1}{ \frac{3x-1}{3}} r \left( \frac{3x-1}{3}\right)^{r-1}- \frac{9}{(3x-1)^2} \left( x- \frac{1}{3}\right)=0 \Rightarrow \dots \Rightarrow r= \pm 1$$ Therefore, the functions $z_1(x)=x- \frac{1}{3}, z_2(x)=\frac{1}{x- \frac{1}{3}}$ are solutions of the differential equation at $\left( \frac{1}{3}, +\infty\right)$ . $$z_1(x) z_2'(x)-z_1'(x) z_2(x)=\frac{-2}{x- \frac{1}{3}} \neq 0$$ So, $z_1, z_2$ are linearly independent solutions of the differential equation. Thus, the general solution of $y''+ \frac{3}{3x-1}y'-\frac{9}{(3x-1)^2}y=0$ is of the form: $$c_1 \left( x- \frac{1}{3} \right)+ c_2 \left( \frac{1}{x- \frac{1}{3}}\right) | c_1, c_2 \in \mathbb{R}, x> \frac{1}{3}$$ EDIT : We set $t=x-\frac{1}{3}$ and we have: $$\frac{dy}{dt}=\frac{dy}{dx} \frac{dx}{dt}=\frac{dy}{dx}$$ $$\frac{d^2y}{dt^2}=\frac{d}{dt} \left( \frac{dy}{dt}\right)=\frac{d}{dt} \left( \frac{dy}{dx} \right)=\frac{dx}{dt} \frac{d}{dx} \left( \frac{dy}{dx} \right)=\frac{d^2y}{dx^2}$$ $$y''(x)+ \frac{1}{x-\frac{1}{3}}y'(x)-\frac{1}{\left( x-\frac{1}{3}\right)^2}y(x)=0 \\ \Rightarrow y''(t)+\frac{1}{t}y'(t)-\frac{1}{t^2}y(t)$$ Two linearly independent solutions are $y_1(t)=t$ and $y_2(t)=\frac{1}{t}, y \in (0,+\infty)$ . Thus, two linearly independent solutions of $y''+\frac{1}{x-\frac{1}{3}}y'-\frac{1}{\left( x-\frac{1}{3} \right)^2}y=0, x> \frac{1}{3}$ are $y_1(x)=x-\frac{1}{3}, y_2(x)=\frac{1}{x-\frac{1}{3}}$ Is it right or have I done something wrong?",['ordinary-differential-equations']
1172646,"If $\sum_{n=1}^\infty\vert a_n\sin(nx)\vert$ converges, then $\sum_{n=1}^{\infty}\vert a_n\vert<\infty$.","Suppose $\sum_{n=1}^\infty\vert a_n\sin(nx)\vert$
converges for all $x$ in a set of positive measure $A$. I'm trying to prove $\sum_{n=1}^{\infty}\vert a_n\vert<\infty$. The only useful result I can recall of periodic functions is the that if $f\in L^1(\mathbb{R})$ and $g$ is continuous on $\mathbb{R}$ with period $T$ then  $\lim_{n\to\infty}\int_{\mathbb{R}}f(x)g(nx)dx=\bigg(\int_{\mathbb{R}}f(x)dx \bigg)\bigg(\frac{1}{T} \int_0^Tg(x)dx\bigg)$. I don't claim this is the best way to go. But if we let $g(x)=\sin x$, then $T=2\pi$. I am not sure what I should define $f$ as since it is required to be in $L^1(\mathbb{R})$, but if I had a decent $f$ my idea is to write $A\subset\bigcup_{k=N}^M[k,k+1]$ then show $\int_{l}^{l+1}\sum\vert a_n \vert<\infty$ for any $l\in [N,M-1]$. Another potential problem is that with my choice of $g$, $1/T\int_0^Tg=1/2\pi\int_0^{2\pi}\sin=0$. Am I anywhere near showing $\sum_{n=1}^{\infty}\vert a_n\vert<\infty$?","['real-analysis', 'analysis']"
1172678,Probability that P people will have N distinct birthdays,"This question is rather difficult to describe clearly, so I will begin with an example. Suppose I have a 365 people in a room. The odds are very low that all these people have different birthdays.  In fact, ignoring leap years and assuming that every birthday is equally likely , I estimated (using a program) that the most likely scenario is 231 distinct birthdays (with a probability of ~6.69%). More abstractly, out of P people, what are the odds that there are exactly N distinct birthdays amongst them (ignoring leap years and assuming that every birthday is equally likely)? In my above example I used P=365 and I estimated the answer for N=231. I am curious if there is a general solution to the problem which could give an exact answer for all N and P. For those who are interested, the program I created tries random combinations of birthdays and counts the number of unused birthdays. With it, I created a table of estimated probabilities for all N for P=365. Here is the most interesting part of the table: 242 - 1.1837375%
241 - 1.5966975%
240 - 2.0933325%
239 - 2.6695062%
238 - 3.3059700%
237 - 3.9824950%
236 - 4.6610425%
235 - 5.2969675%
234 - 5.8606362%
233 - 6.2995763%
232 - 6.5841737%
231 - 6.6932175%
230 - 6.6143263%
229 - 6.3563137%
228 - 5.9308837%
227 - 5.3896263%
226 - 4.7588988%
225 - 4.0879863%
224 - 3.4110800%
223 - 2.7697687%
222 - 2.1889950%
221 - 1.6793675%
220 - 1.2546263%","['statistics', 'balls-in-bins', 'probability']"
1172681,Non-gradient vector field in $\mathbb{R}^3 - \lbrace\mathbf{0}\rbrace$ with zero curl,"I'm self-studying multi-variable calculus using MIT's publicly available materials.  One of the practice questions for the final exam asks that I determine the truth or falsity of the following statement: Let $\mathbf{F}$ be a vector field on $\mathbb{R}^3 - \lbrace \mathbf{0}\rbrace$ with $\nabla \times \mathbf{F} = 0$.  Then there exists a scalar field on $\mathbb{R}^3 - \lbrace \mathbf{0}\rbrace$ such that $\mathbf{F} = \nabla f$. Now, as I understand it, this assertion should be false, because $\nabla \times \mathbf{F} = 0$ only implies that $\mathbf{F}$ is conservative on a region if the region in question is star convex, which $\mathbf{R}^3 - \lbrace \mathbf{0} \rbrace$ is not (unless I really don't understand the notion of star convexity).  However, to do the exercise well, I need a counterexample, and here I'm having problems. I am of course familiar with the canonical counterexample in $\mathbf{R}^2$ (and, by extension, in $\mathbf{R}^3$ less the $z$-axis), i.e. $\mathbf{F} = \left(-\frac{y}{x^2 + y^2}, \frac{x}{x^2 + y^2}, 0\right)$.  I'm having difficulty extending this to $\mathbf{R}^3 - \lbrace \mathbf{0} \rbrace$, however -- although poking around here and on the Internet generally has led me to suspect that any counterexample is going to be analogous.  (The phrase 'de Rham cohomology' keeps popping up....) At any rate, I would appreciate a gentle hint.  Am I on the right track?  Have I missed something fundamental?",['multivariable-calculus']
1172691,Help me solve $\int \ln(2x+1)dx$,"I'm trying to solve this indefinite integral: $$\int \ln(2x+1)dx$$ I tried integration by parts, letting $u=\ln(2x+1) \qquad dv=dx \qquad du= \frac 2 {2x+1}dx \qquad v=x$ Plugging these into the standard formula, I got $$\int \ln(2x+1)dx \quad = \quad x\ln(2x+1)- \int \frac {2x} {2x+1} dx$$ Now the problem I ran into: I have no idea how to solve $\int \frac {2x} {2x+1} dx$. I tried to use integration by parts on this, but it produced another integral I had no idea how to solve (it was $\int \frac {2x^2} {2x+1} dx$, in case you're curious). Can someone help me out? (The only two integration techniques I know for now are substitution and integration by parts)","['calculus', 'integration']"
1172696,Direct image of vector bundle,"Let $f:X\to Y$ be a morphism of projective varieties and $\mathcal{E}$ be a vector bundle on $X$. How can I compute explicitly $f_*\mathcal{E}$ in various situations? For example, let $Y=\mathbb{P}^1$ and $X$ be a conic. What is $f_*\mathcal{O}_X$?","['algebraic-geometry', 'vector-bundles']"
1172721,"Pascal triangle, getting to the sum of it: $\binom{n}n+\binom{n+1}n+\ldots+\binom{n+m}n$","So we know that $$\binom{n}0+\binom{n}1+\ldots+\binom{n}n=2^n\;.$$ What about the following sum? $$\binom{n}n+\binom{n+1}n+\ldots+\binom{n+m}n\;?$$ (a) Identify several examples of this sum on the Pascal triangle and try to discover what it is equal to. (b) Guided by your work above (hopefully), prove it algebraically by using a property of the binomial coefficient in the Pascal triangle","['summation', 'discrete-mathematics', 'binomial-coefficients']"
1172750,Singular value decomposition for matrices that are not square?,"I understand that the Singular Value Decomposition is defined as SVD = $U\Sigma V^T$ , but I am slightly confused about the calculations when the matrix is not square. 
For example, I have the matrix:
$$
\begin{bmatrix}
1 & -1 \\
-2 & 2 \\
2 & -2
\end{bmatrix}
$$
When I am solving for $V$, however, I am missing the last component. Have I done something wrong when calculating for matrices that are not square matrices? $$\det(A^T A - \lambda I) = 
        \begin{bmatrix}
        2 - \lambda & -4 & 4 \\
        -4 & 8 - \lambda & -8\\
        4 & -8 & 8 - \lambda
        \end{bmatrix}
$$
$\lambda = 0, 2, 16$
Eigenvectors respectively are: 
\begin{bmatrix}             
        1  \\           
        1/2 \\
        0
 \end{bmatrix} \begin{bmatrix}             
        1  \\           
        2/7 \\
       -2/7
 \end{bmatrix} \begin{bmatrix}             
        0  \\           
        1 \\
       -1
 \end{bmatrix} Therefore $$\Sigma = 
\begin{bmatrix}             
        \sqrt 2  & 0 & 0  \\           
        0 & \sqrt 16 & 0 \\
        0 & 0 & 0
 \end{bmatrix}$$
Also, $$V = 
\begin{bmatrix}
       7/\sqrt 57 & 0 & 2/\sqrt 5 \\
       2/\sqrt 57 & 1/\sqrt 2 & 1/\sqrt 5 \\
       2/\sqrt 57 & -1/\sqrt 2 & 0
\end{bmatrix}$$ This is the portion I am confused about.
Is $U = AV / \sqrt\lambda $ ? What if I have am missing a vector so that I can only get the first two columns of $U$?","['matrix-decomposition', 'matrices', 'linear-algebra', 'svd']"
1172760,What are the meanings of $\operatorname{trig}(x)^n$ and $\operatorname{trig}^n(x)$?,"When a trigonometric function has an exponent does that mean multiply itself or apply itself to the result recursively? For example, does $\sin(x)^2$ denote $\sin(x)\sin(x)$ or does it denote $\sin(\sin(x))$ ?  What about $\sin^2x$ ?","['notation', 'trigonometry']"
1172765,$F\big( g(t) \big) - F\big( g(t + h) \big) \leq h$ implies that $g$ is right-continuous?,"Suppose $F$ is a continuous, strictly increasing distribution function. Also,
suppose that $g:[0,1] \longrightarrow [0,1]$ such that for any $t \in [0,1]$,
$h > 0$, and $\epsilon > 0$, 
$$
F\big( g(t) \big) - F\big( g(t + h) \big) \leq h
$$
and
$$
F\big( g(t) \big) - F\big( g(t+h) - \epsilon \big) \geq h - P\big( hf(X) > \epsilon \big)
$$
where $f(\cdot) \geq 0$ is a function such that $E[ f(X) ] < \infty$. Are these conditions sufficient to show that $g$ is right-continuous?","['probability-theory', 'probability', 'real-analysis']"
1172789,what function fulfills these conditions? [duplicate],"This question already has answers here : Can the Identity Map be a repeated composition one other function? (5 answers) Closed 9 years ago . So I know that if $f(x) = x^{-1}$, than $f(f(x)) = x$ but $f(x)$ is not necessarily $x$. So now, is there $g(x)$ such that $g(g(x)) \neq g(x) \neq x$ but $g(g(g(x))) = x$? If so what is it, else why not?","['inverse', 'functions']"
1172797,Need help converting this to Polar integral and evaluating it,"I have to convert this to polar integral and evaluate it. $$\int _{-1}^0\int _{-\sqrt{1-x^2}}^0\:\frac{2}{1\:+\:\sqrt{x^2\:+\:y^2}}\:dy\:dx$$ I attempted the conversion and ended up with this $$\int _{\pi }^{\frac{3\pi }{2}}\int _0^1\:\:2r\:\frac{1}{1\:+\:r}\:dr\:d\theta $$ Now, I'm stuck. Integration by parts does seem to be cooperating on this.","['definite-integrals', 'multivariable-calculus', 'polar-coordinates']"
1172800,finding which map is continuous given topological spaces,"Let $\Omega$ be a discrete topological space and $X$ any space: which maps $f: \Omega \to X $ are continuous?
which maps $f: X \to \Omega $ are continuous for each topology on $Y$ ? Attempt: For the first part, I believe any map $f: \Omega \to  X $ is continuous since the topology of $\Omega$ is $2^{\Omega} $ and so for any open $U $ in $X$ , $f^{-1}(U) \subset \Omega$ must be open. I am stuck on the second question. Any help would be appreciated.",['general-topology']
1172802,Show that any orthogonal matrix has determinant 1 or -1 [duplicate],"This question already has answers here : A better proof for $\det(P) = \pm1$ if $P$ is an orthogonal matrix (3 answers) Closed 9 years ago . Hello fellow users of this forum: Show that for any orthogonal matrix Q, either det(Q)=1 or -1. Thanks","['matrices', 'determinant']"
1172814,Show that if $f : \mathbb{R}^{2} \to \mathbb{R}$ continuously differetiable then $f$ is not inyective,"Well my question this time is: How to show that $f : \mathbb{R}^{2} \to \mathbb{R}$ continuously differetiable then $f$ is not inyective I was trying to consider the function $g(x,y)=(f(x,y),y)$, but I don't know how to proceed with $g$, I know I have to use the implicit function theorem but when I was thinking of it, I remember that nearly $(x_0,y_0)$, $f$ must be zero, well $f(x_0,y_0)=0$, then the only tool I thought it could help I can't apply it. Can someone help me please with this problem?, and once you have it for this case, Can we generalize to $R^{m}$ and $R^{n}$?(of course $m<n$) Thanks a lot in advance My implicit function theorem : Let $A \subset R^{m} \times R^{n}$ open, $(x_0,y_0) \in A$, $f:A \to R^{m}$ continously differentiable nearly $(x_0,y_0)$ and  $f(x_0,y_0)=0$. Let M be the matrix of $m \times m$ given by: $$D_{n+j}f^{i}(x_0,y_0)$$ and suppose that $\det(M) \not= 0$, then there exists an open set $U \subset R^{n}$ $x_0 \in U$, and an open $V \subset R^{m}$, $y_0 \in V $, such that, for each $x$ there exists a unique $g(x) \in V$ such that $f(x,g(x))=0$, $g$ differentiable I don't know Topology, or more than introduction to analysis and a little bit of multivariable analysis, thanks a lot for your help","['multivariable-calculus', 'proof-writing']"
1172843,"Cantor set homeomorphic to $\{ 0,1 \}^\infty$","I would like to show that the Cantor middle-third set with subset topology inherited from $\mathbb R$ is homeomorphic to $\{ 0,1 \}^\infty$ under product topology. I am trying to construct a homeomorphism between the two spaces, but I am not clear what an open set (or closed set) of the second space generally look like. Could someone help me with that? Thanks.","['general-topology', 'cantor-set']"
1172899,"Let $(X,d)$ be a metric space and $f:X\to X$ a function, is $d(x,f(x))$ a lower semicontinous function?","So I was trying to prove that if $f$ satisfies a special property the the function $d(x,f(x))$ is lower semicontinous but then I couldnt come up with a counter example of the following statement:
Let $(X,d)$ be a metric space and $f:X\to X$ a function, is $d(x,f(x))$ a lower semicontinous function
What do you guys think?","['functions', 'metric-spaces', 'functional-analysis', 'real-analysis']"
1172913,Lebesgue Measure: show that a subset of R is equal to R,If B is a subset of R such that I) B' has Lebesgue measure zero II) B is closed under addition Show that B = R this is my first course in measure theory.  I only know that nonempty close and open subset of R is equal to R. What do I have from i and ii?  And what property of R should I use?,"['measure-theory', 'lebesgue-measure', 'real-analysis']"
1172981,What is an nonlinear ordinary differential equation and give an example?,"What is an nonlinear ordinary differential equation and give an example? And what is the difference between an linear ordinary differential equation and non-linear ordinary differential equation in formal way? The second question seems more important to me, however, it seems too long to put in the title","['ordinary-differential-equations', 'terminology']"
1173002,Let Q be orthogonal. Show that Q^tAQ has the same characteristic polynomial. as A.,Do I have to use the diagonalization of A?,['matrices']
1173020,Minkowski functional $p_E$ is continuous if and only if $0\in E^0=\text{int}E$,"Let $X$ be a normed topological vector space. Prove $p_E$ is continuous $\iff 0\in E^0$. In the above $p_E(x)=\inf\{t\ge0: x\in tE\},$ with $E$ an absorbing set $E\subset X$ is the Minkowski functional and $E^0$ denotes the interior of $E$. I already proved the $\Rightarrow$ direction: A previous calculation yielded $E^0= E_1$ whenever $p_E$ is continuous, where $E_1=\{x\in X:p_E(x)\lt 1\}$. Since $E$ is absorbing, $0\in E = E^0$. Now for the converse, I'm trying to prove $p_E$ is continuous at $0$ hoping that continuity everywhere else will follow. 
Now since $0\in E^0$, and $E^0$ there is a ball $B_r(0)\subset E^0$ for some $r>0$. All elements of said ball are in $E^0$ so $p_E(x)<1$ for all $\|x\|<r$: Given $\epsilon>0$ I know that $\| p_E(x)-p_E(0)\|=\|p_E(x)\|<1$ for all $x\in B_r(0)$, but I can't make the above less than $\epsilon$. I also havent used the convexity of the Minkowski functional. Any help would be greatly appreciated.",['functional-analysis']
1173024,Show an example of a function from $\mathbb Z \to \mathbb Z$ that is surjective but not injective,"The question I am working on is this...
Give an example of a surjective function from $\mathbb Z \to \mathbb Z$ that is not injective. My question is simple, when it is worded as above, (which I can't seem to get a straight answer on from others) am I able to put restrictions on this, as an example, only use the positive integers with the formula I create to prove that my formula is surjective? To me, I see this as looking at the infinite set of integers and proving that the infinite set of integers applies to whatever my formula is and that I must prove based on that set of infinite integers that the function I create is surjective. So where am I misunderstanding...someone told me as a hint that I should think about piecewise functions. Comprehension of surjective and injective I got, just the wording of the question throws me off. Don't want answer, just trying to get a clue.",['discrete-mathematics']
1173034,Positive integer solutions of $\frac{1}{a_1}+\frac{2}{a_2}+\frac{3}{a_3}+\cdots+\frac{n}{a_n}=\frac{a_1+a_2+a_3+\cdots+a_n}{2}$,"Find all ordered tuples of positive integers $(a_1,a_2,a_3,\ldots,a_n)$ such that $\frac{1}{a_1}+\frac{2}{a_2}+\frac{3}{a_3}+\cdots+\frac{n}{a_n}=\frac{a_1+a_2+a_3+\cdots+a_n}{2}$ The only thing I have been able to think about is using inequalities. I have tried to apply AM-GM, Titu's lemma etc.. Cauchy-Schwarz gives the following thing: $$(\frac{1}{a_1}+\frac{2}{a_2}+\frac{3}{a_3}+\cdots+\frac{n}{a_n})(a_1+\cdots a_n) \ge (\sqrt{1}+\sqrt{2}+\cdots \sqrt{n})^2$$ $$(a_1+\cdots a_n)^2\ge 2(\sqrt{1}+\cdots \sqrt {n})^2$$ which doesn't really help us at all. I have also tried considering smaller cases. For $n=2$ , $$a_1a_2(a_1+a_2)=4a_1+2a_2$$ which tells us that $2a_2=ka_1$ and $8a_1=pa_2=ka_1p\implies kp=8$ . This should now give us all the solutions by checking all the cases. So how can we even begin to attack this problem?","['elementary-number-theory', 'algebra-precalculus', 'contest-math']"
1173050,Find the derivative of $\arccos\frac{b+a\cos x}{a+b\cos x}$,"Find the derivative of $\arccos\dfrac{b+a\cos x}{a+b\cos x}$ is there a smart way to find this derivative i tried by the conventional chain rule way, and it got very complicated","['calculus', 'derivatives']"
1173056,Lie Groups/Exponential map identity,I have come across this identity a few times and I have absolutely no idea why it holds. $g^{-1}\exp(tX)g=\exp(t(\text{ad}_{g^{-1}}X))$ Would any one be able to explain exactly why this holds or point me to a resource that would explain.? Thanks.,"['manifolds', 'lie-algebras', 'lie-groups', 'differential-geometry']"
1173084,Doubt in the defn of exponential operator.,"definition Let $A$ be an $n\times n$ matrix. Then for $t\in \mathbb R$,
$$e^{At}=\sum_{k=0}^\infty \frac{A^kt^k}{k!}\tag{1}$$ But in this definition, What they are meaning by the term $A^kt^k$, If I give this matrix $$A=\begin{pmatrix}1&-1\\2&3\end{pmatrix},$$ what can we say about the $(1)$, means  How it will be?","['matrices', 'exponential-function']"
1173088,"Where we have used the condition that $ST=TS$, i.e, commutativity?","definition Let $A$ be an $n\times n$ matrix. Then for $t\in \mathbb R$,
$$e^{At}=\sum_{k=0}^\infty \frac{A^kt^k}{k!}\tag{1}$$ Proposition If $S$ and $T$ are linear transformations on $\mathbb R^n$ which commute, then $e^{S+T}=e^Se^T$. Proof By binomial therem, $$(S+T)^n=\sum_{j+k=0}^n n! \frac{S^jT^k}{j!k!}$$ therefore, $$e^{S+T}=\sum_{n=0}^\infty\sum_{j+k=0}^n \frac{S^jT^k}{j!k!}=\sum_{j=0}^\infty \frac {S^j}{j!}\sum_{k=0}^\infty \frac{T^k}{k!}=e^Se^T$$ In the above steps I have used cauchy product and binomial theorem and the absolute convergence of the two series. But My doubt is.. Where we have used the condition that $ST=TS$, i.e, commutativity?","['matrices', 'exponential-function']"
1173095,How to prove $ A^{\perp} $ is a closed linear subspace?,Suppose $ X $ is an inner product space and $ A\subseteq X $. I need to prove that $ A^{\perp} $ is a closed linear subspace of $ X $. Can anyone give me a idea?,"['vector-spaces', 'inner-products', 'functional-analysis']"
1173126,"Finding the largest constant $C$ such that $|\ln x−\ln y| \geq C|x−y|$ for all $x, y \in (0, 1]$","Find the greatest value of C such that
$|\ln x−\ln y|≥C|x−y|$
for any $x,y∈(0,1]$. What should my approach be? I can't think of much options.","['inequality', 'calculus', 'derivatives']"
1173139,(Countable) partition generated $\sigma$-algebra,"I am working on exercise 1.9 b) in the book ""Probability and Stochastics"" by Erhan Çınlar to  practice my understanding of $\sigma$-algebras (although this might possibly be future homework in my classes). Anyway, here's the exercise: Let $\mathcal{C}$ be a (countable) partition of E. Show that every element of $\sigma \mathcal{C}$ is a countable union of elements taken from $\mathcal{C}$. So, $\mathcal{C} = \{ C_n: n \in \mathbb{N}\}$ is a partition of $E$.
To prove the exercise, I have introduced $\mathcal{E} = \{\cup_{i \in I} C_i: I \subseteq \mathbb{N}\}$ and am trying to show that $\mathcal{E}$ is a $\sigma$-algebra. For this, I have to show the following three parts/properties (correct me if I'm wrong): E $\in \mathcal{C}$: $\mathcal{C}$ is a partition of E, so $\cup \mathcal{C}=E.$ Since $\mathcal{C}$ is countable, $\cup \mathcal{C}$ is the union of countably many members of $\mathcal{C}$. Show that $\mathcal{E}$ is closed under countable unions: Since $\mathcal{E}$ consists only of sets which are countable unions of elements of $\mathcal{C}$, $\mathcal{E}$ is closed under countable unions. Show that $\mathcal{E}$ is closed under complement: Let $A \in \mathcal{C}$, so there exists a countable $\mathcal{C}_A \subseteq \mathcal{C}$ such that $A = \cup \mathcal{C}_A.$ Let $\mathcal{D} = \mathcal{C} \setminus \mathcal{C}_A$; then $\mathcal{D}$ is a countable subset of $\mathcal{C}$, and if $D =\cup \mathcal{D},$ then $D \in \mathcal{C}.$ Since $\mathcal{C}$ is a partition on E, $D = E \setminus A = A^c$, and so $A^c\in \mathcal{C}$. Now, in the last part I want to show that $\mathcal{E} = \sigma\mathcal{C}$, but I'm not sure how exactly this is done.","['measure-theory', 'elementary-set-theory']"
1173213,Convergence of $\operatorname E|X_n|^p$ when $0<p<1$,"Let $0<p<1$ and $X_1,\ldots,X_n$ be random variables with finite absolute moments of order $p$. Suppose that the random variables $X_1,\ldots,X_n$ converge in mean of order $p$ to a random variable $X$ as $n\to\infty$ (i.e. $\operatorname E|X_n-X|^p\to0$ as $n\to\infty$). Does it follow that $\operatorname E|X_n|^p\to\operatorname E|X|^p$ as $n\to\infty$? It is indeed the case when $p\ge1$ since $\|X\|_p=(\operatorname E|X|^p)^{1/p}$ is a norm when $p\ge 1$ and we can use the reverse triangle inequality $|\|X_n\|-\|X\||\le\|X_n-X\|$ to obtain the result, but $\|X\|_p$ does not satisfy the triangle inequality when $0<p<1$. Any help is much appreciated!","['probability-theory', 'convergence-divergence', 'lp-spaces', 'expectation']"
1173258,Are there $\omega_1$-many binary sequences with this property?,"Is there a family $$\{f_\alpha:\alpha<\kappa\}\subseteq 2^\omega$$ of size $\mathfrak c$ or $\omega_1$ such that for any $n\in\omega$, $\{\alpha_i:i<n\}\subseteq \kappa$, and $n\times n$ binary matrix $\textbf M\in 2^{n\times n}$, there exists $k\in\omega$ such that $$(f_{\alpha_i} (k+j))_{ij}=\textbf M.$$ Hopefully I explained it well. It is easy to picture in your mind: (1) think of the binary sequences ($f_\alpha$'s) going from left to right (2) pick $n$ of them ($n$ rows) and let a matrix $\textbf M$ be given as above (3) now imagine an $n\times n$ box sliding over the selected sequences I want the $f_\alpha$'s to be chosen (beforehand) so that if I slide the box enough, I get $\textbf M$. If it is too hard to get uncountably many, what about countably many? It seems like you could use some arithmetic to stagger the finite binary sequences or something.","['infinitary-combinatorics', 'elementary-set-theory', 'combinatorics']"
1173274,How to simplify $y = \frac{\sin\rho + \sin2\rho}{\cos\rho + \cos2\rho}$,"How can I simplify this function before I differentiate it? $$y = \frac{\sin\rho + \sin2\rho}{\cos\rho + \cos2\rho}$$ Of course you could immediately start off by using the quotient rule, but that gives a very long and convoluted expression which takes forever to simplify. I know that one of the possible simplifications of the function pre-differentiation is $$y = \tan\frac{3p}{2}$$ However I do not know how this is done. If anyone could shed some light on this or show steps for an equally simple simplification of the original function, this would be much appreciated.","['trigonometry', 'calculus', 'derivatives']"
1173321,Estimation of a combinatorial sum,"Fix a non-negative integer $t\in\mathbb N\,$. I am interested in the following sum
$$S(t) \,:=\, \sum_{m}\,2^m\sum_{k_1+\dots+k_m\leq t}k_1\cdots k_m$$
where clearly $1\leq m\leq t$ and $1\leq k_i\leq t$ for $i=1,\dots,m$. Is it possible to compute $S(t)$ in a close form? Or at least what is a good upper bound for $S(t)$?","['sequences-and-series', 'number-theory', 'combinatorics']"
1173322,a.s. for all $t$ or for all $t$ a.s.?,"Assume that we have some equality,
$$
X (t) = Y(t). \quad  \quad \quad \quad \quad \quad \quad (1)
$$ I imagine that if I say ""(1) holds a.s. for all $t>0$"" it means that 
$$
P\{X (t) = Y(t) \text{ for all } t>0 \} = 1, \quad \quad \quad \quad \quad (2)
$$ 
whereas if I say ""(1) holds for all $t>0$ a.s."" it means that for all 
$$
 \forall t >0: P\{X (t) = Y(t) \} = 1. \quad \quad \quad \quad \quad (3)
$$ Is my interpretation correct? Also, are there ways to distinguish between (2) and (3) unambiguously? Addition . The question also concerns relations. Consider some relation, for exapmle
$$
X (t) \in A. \quad  \quad \quad \quad \quad \quad \quad (1')
$$ Does ""(1') holds a.s. for all $t>0$"" mean 
$$
P\{X (t) \in A \text{ for all } t>0 \} = 1, \quad \quad \quad \quad \quad (2')
$$ 
and does ""(1') holds for all $t>0$ a.s."" mean
$$
 \forall t >0: P\{X (t) \in A \} = 1? \quad \quad \quad \quad \quad (3')
$$","['probability-theory', 'article-writing', 'terminology']"
1173325,"Writing a complex function $f(x,y)=f(x+iy)$ as function of complex variable $z$.","Is it true that to write a differentiable complex function $f(x,y)=f(x+iy)$ as a function of the complex variable $z$ one can replace real variable $x$ with $z$ and put $y=0$? If it is true (even under more conditions) then how can one prove it?",['complex-analysis']
1173339,Can a set be closed in one topology but neither open nor closed in another?,"Can a set be closed in one topology but neither open nor closed in another? If we say that the complement of a open set is a closed set, i.e. if $S \subseteq X$ is open then $X \setminus S$ is closed, I argue that depending on the topology, it could actually be the case. Consider the Euclidean topology on $\mathbb{R}$. Clearly any closed interval $[a,b]$ with $a,b \in \mathbb{R}$ is a closed set in this topology. However, given the trivial topology on $\mathbb{R}$, any interval $[a,b]$ is neither closed or open. But according to this Wikipedia page, an equivalent definition of a closed set is that ""a set is closed if and only if it contains all of its limit points"". And from this point of view, the inverval $[a,b]$ is clearly closed. So i guess that my question is: How can two (supposedly) equivalent definitios of a closed sets, have a set be closed in one and not the other?",['general-topology']
1173342,Shouldn't this function be discontinuous everywhere?,"I was thinking about single point continuity and came across this function. $$
f(x) = \left\{
        \begin{array}{ll}
            x & \quad x\in \mathbb{Q}\\
            2-x &  \quad x\notin \mathbb{Q}
        \end{array}
    \right.
$$ We know this function is continuous only at $x=1$ . But doesn't that contradict our whole idea of continuity? A function is continuous if we are able to draw the function without lifting our pen or pencil. But here both the pieces of the function exist at specific places, so we have to lift our pen. Shouldn't the function be discontinuous everywhere? Looks like a stupid doubt though.","['continuity', 'limits']"
1173357,For which algebras Taylor series and the Fourier series can be generalized?,"I'm not a professional mathematician.
The question is in the title. But most of all I'd like to know about this for quaternions algebra with non commutative multiplication. I'd like to know about truth of that expansions: Q1. For functions with domain as catersian product of real numbers. And function range as some elements from rings, semirings algebra. Q2. For function with domain, and range value from similar aglebra.","['fourier-series', 'discrete-mathematics', 'taylor-expansion']"
1173373,$F$-related vector fields [duplicate],"This question already has an answer here : Find $F$-related vector fields on $M\times N$, where $F(x)=(x, f(x))$ (1 answer) Closed 3 years ago . I'm having a bit of difficulty understanding $F$-related vector fields. I think I understand conceptually what's going on (taking a vector field on a manifold $M$ and getting a smooth vector field on $N$ by applying the differential of a map), but I'm having trouble applying the concept. For example, here's a problem (8.14 in Lee's Smooth Manifolds textbook)
 Let $M,N$ be smooth manifolds with or without boundary and let $f:M\to N$ be a smooth map. Define $F:M\to M\times N$ by $F(x)=(x,f(x))$. Show that for each vector field $X$ on $M$, there is a smooth vector field $Y$ on $M\times N$ which is $F$ related to $X$. Now, I see that $F$ is the graph of $f$, and so $F(M)$ is closed in $M\times N$, and there's a proposition in Lee which says that if we have a smooth vector field on a closed subset, we can extend it to a smooth vector field on the whole manifold. However, I'm really not sure where to go. I see that $F(M)\subset M\times N$ is an embedded manifold, so we have slice charts, but I'm struggling to see how to construct this $Y$.","['manifolds', 'vector-bundles', 'differential-geometry']"
1173380,"Solving a linear, inhomogeneous, ordinary differential equation with constant coefficients","How to solve this?
$\newcommand{\d}[0]{{\rm d}}$
$$a\frac{\d^2y}{\d x^2}+b\frac{\d y}{\d x}+cy=d\\
a,b,c\in \mathbb R;\qquad y\equiv y(x);\qquad d\equiv d(x)$$
I know how to solve when $d=0$ by assuming a solution $y=e^{\alpha x}$ and then finding $\alpha$, for simplicity you can let $f(x)=ax^2+bx+c$ and $f(a_1)=f(a_2)=0$ where $a_1,a_2$ be $f$'s two roots.Therefore in the case of $d=0$, $y=c_1e^{a_1x}+c_2e^{a_2x}$ Actually I'm trying to solve: (from Alternating Current(AC) through a Capacitor, Resistor and Inductor)
$$L\frac{\d^2q}{\d t^2}+R\frac{\d q}{\d t}+\frac qc=v_0\sin \omega t$$
and: (from Simple Harmonic Motion with a Spring, Damping and External agent's force)
$$ m\frac{\d^2x}{\d t^2} +b \frac{\d x}{\d t}+k x =F_0\cos \omega_dt$$ Where c:Capacitance, R:Resistance, L:Inductance, $v_0$:Maximum Voltage, $\omega$:AC frequency  and t:time. Also m:mass, b:damping constant, k:spring constant, $F_0$:maximum external force, $\omega_d$:external agent's frequency and t:time. The analogy is in $L\leftrightarrow m,R\leftrightarrow b, 1/c\leftrightarrow k, F_0\leftrightarrow v_0,\omega\leftrightarrow \omega_d$. Also some useful shorthand notations are:
$$X_L=L\omega\qquad X_c=\frac1{c\omega}\qquad Z=\sqrt{R^2+(X_c-X_L)^2}\qquad \phi=\arctan\frac{X_c-X_L}R$$ My book uses the assumption $q=q_0\sin(\omega t+\theta)$ but I clearly see it's just working backwards from the result. I wonder how physicists arrived at the result for the first time? Mysteriously another method involves vectors/phasers/complex numbers and Pythagoreas theorem!",['ordinary-differential-equations']
1173392,Resolution of singularities of the determinant hypersurface,"Let 
$$\det\nolimits_n=\sum_{\pi\in\mathfrak{S}_n} \operatorname{sgn}(\pi)\cdot x_{1,\pi(1)}\cdots x_{n,\pi(n)} \in \mathbb{C}[x_{ij}\mid 1\le i,j\le n]$$
be the determinant polynomial. It defines a singular hypersurface $Z_n\subseteq \mathbb{C}^{n\times n}$ consisting of the rank-deficient complex $n\times n$ matrices. Since we are in characteristic zero, there is an embedded resolution $\phi_n: Y_n\to \mathbb{C}^{n\times n}$ such that the strict transform of $Z_n$ is a nonsingular subvariety of $Y_n$. My question is, has anyone ever given an explicit description of $\phi_n$, in the sense that $\phi_n$ is expressed as a (sequence of) blow-up(s) along something that possibly has some nice description? Any reference is more than welcome.","['algebraic-geometry', 'singularity-theory', 'reference-request']"
1173396,"If point is zero-dimensional, how can it form a finite one dimensional line?","I have extracted the below passage from the wikipedia webpage - Point (geometry): In particular, the geometric points do not have any length, area, volume, or any other dimensional attribute. I think the above passage imply\ies that the point is zero dimensional. If it is zero dimensional, how can it form a one dimensional line? Physics texts sometimes talk of lines' being made up of points, planes' being made up of lines and so forth. Clearly a line segment, thought of as a connected interval of the real numbers, cannot be built as a countable union of points. What axiom systems define the building up of a line from points, or, how do we rigorously define the building of a line from points? Links: The section one ( Physical meaning of geometrical propositions ) of part one of the book ""Relativity: The Special and General Theory"" seems to be giving Einsteins view on this matter. What was the intended utility of Euclid's definitions of lines and points? Related: History of Euclidean and Non-Euclidean Geometry",['geometry']
1173405,"Intuitive meaning of second, third and fourth derivatives at a point.","Can someone explain me the intuitive meaning of second, third and fourth derivatives of a function say, $f(x)$ at a point (say, $a$)? I know it's hard to explain to someone novice like me! But an intuitive answer of this question can help many people.","['calculus', 'derivatives']"
1173457,Is the product of all conjugates of some subgroup independent of the order?,"Let $G$ be a finite group and $A \le G$. Let $A^G = \{ A_1, A_2, \ldots, A_n \}$ be all the conjugates of $A$, i.e. each $A_i$ equals $A^g$ for some $g \in G$. Then I want to show that
$$
 A_1 A_2 \cdots A_n = A_{\pi(1)} A_{\pi(2)} \cdots A_{\pi(n)}
$$
for each permutation $\pi \in \mathcal S_n$. I guess I have to apply $ba = ab^a$, but If I move some element, i.e. consider $a,b \in A$ and $a^g b^h \in A^gA^h$, then $a^g b^h = b^h (a^g)^{b^h}$ and $(a^g)^{b^h}$ need not be in $A^g$? (also in general I guess two conjugates of a subgroup need not commute, i.e. $A^g A^h \ne A^h A^g$ in general, right?). Any hints (maybe how to apply $ba = ab^a$ correctly...) how to solve it?","['permutations', 'finite-groups', 'group-theory', 'abstract-algebra']"
1173460,Is $53$ expressible in this form?,"It seems as if prime numbers may always be expressed in the form $a\cdot 2^b+c \cdot 3^d$ for some nonnegative integers $b,d$ and $a,c\in \{-1,0,1\}$. Examples: $2=1\cdot 2^1+0\cdot 3^d$ $3=0\cdot 2^b+1\cdot 3^1$ $5=1\cdot 2^1+1\cdot 3^1$ $7=1\cdot 2^2+1\cdot 3^1$ $11=1\cdot 2^3+1\cdot 3^1$ $13=1\cdot 2^2+1\cdot 3^2$ $17=1\cdot 2^3+1\cdot 3^2$ $19=1\cdot 2^4+1\cdot 3^1$ $23=1\cdot 2^5+(-1)\cdot 3^2$ $29=1\cdot 2^5+(-1)\cdot 3^1$ $31=1\cdot 2^5+(-1)\cdot 3^0$ $37=1\cdot 2^6+(-1)\cdot 3^3$ $41=1\cdot 2^5+1\cdot 3^2$ $43=1\cdot 2^4+1\cdot 3^3$ $47=1\cdot 2^7+(-1)\cdot 3^4$ We hit a brick wall at $53$. Can anyone confirm if $53$ is/isn't expressible? What about $n\in \mathbb{N}$ in general? Is it possible to always express $n$ in this form? Thanks. EDIT: I invented this question on my own, there are no sources. Note: The reason I started with prime numbers is because I found it hard to find an expression for $6$, whereas prime numbers continued to be easy to find expressions for (easy until $53$, that is).","['prime-numbers', 'number-theory']"
1173461,Why are irreducible elements non-units?,"I know this may seem trivial but I'm trying to grasp why irreducible elements are non-units. If an element p is a unit and b is its inverse, then $pb = 1, \forall p,b \in R$, R is a ring. Does this imply that b is a factor of p, thus making it reducible?","['ring-theory', 'abstract-algebra']"
1173488,Find derivative of $x^{x^x}$,"Trying to find the derivative of: $$
x^{x^x}
$$ I have a solution but cannot understand the third transition:","['exponentiation', 'calculus', 'derivatives']"
1173495,Are there many fewer rational numbers than reals?,"Today my professor asked me to figure out the probability of getting a rational number from $[0,1]$.
His answer was that the probability is $0$.
Why is this?","['real-numbers', 'analysis']"
1173511,"Counterexamples for ""every linear map on an infinite dimensional complex vector space has an eigenvalue""","Every linear map on a finite dimensional complex vector space has an eigenvalue. Not so in the infinite case. I'm interested in nice counterexamples anyone might have. Here's one: Consider the vector space $\mathbb C^\infty$ of sequences and the right shift map $R$ defined by $$R(a_1, a_2, a_3, ...) = (0, a_1, a_2, a_3, ...)$$ $R$ has no eigenvalue (using the usual convention that there must be a non-trivial eigenvector).","['big-list', 'linear-algebra', 'examples-counterexamples']"
1173512,Why is $A(\cos x)=x/2$ in the unit circle definition of cosine?,"In the Spivak's calculus, there is the following definition: If $0 \leq x \leq \pi$, then $\cos x$ is the unique number in $[-1,1]$
  such that $$A(\cos x)=\frac x 2 \text{ and } \sin x=\sqrt{1-\cos^2 x}$$
  $$\text{where } A(x)=\frac{x \sqrt{1-x^2}}{2}+\int_x^1 \sqrt{1-t^2}dt$$ This seems hardly a definition, but a theorem. Where does $x/2$ come from? It is stated that it is the area but why $x/2$ and not just $x$?","['trigonometry', 'calculus']"
1173523,Definition of 'saturated' set?,"My notes from the lecture says ""Let $X$ be a topological space and let $R$ be an equivalence relation. Then, $A \subseteq X$ is called saturated with respect to $R$ if it is a union of equivalence classes.""
This statement doesn't make sense to me. I have learnt about quotient spaces $X/R$ which is a set of all equivalence classes. So, since $A$ is a subset of $X$, how can it be a union of equivalence classes? I would understand the statement if it said $A$ is a subset of $X/R$? Thanks.",['general-topology']
1173536,Sum of the roots of an equation.,"The real root of the equation $8x^3 - 3x^2 - 3x - 1 = 0$ can be written in the form $\frac{\sqrt[3]a + \sqrt[3]b + 1}{c}$, where $a$, $b$, and $c$ are positive integers. Find $a+b+c$. I used Vieta's to get the wrong answer. If $$f(x) = a_0 + a_1x + a_2x^2 + a_3x^3$$ And $p, b, c$ are the roots then: (here let $a = a_3$ the high degree coefficient) $$f(x) = a(x - p)(x - b)(x - c) = ax^3 - apx^2 - acx^2 + acpx - abx^2 + abpx + abcx - abcp$$ $$ = ax^2 + x^2\overbrace{(-ap - ac - ab)}^{a_2} + x\overbrace{(acp + abp + abc)}^{a_1} - abcp$$ $$f(x) = 8x^3 - 3x^2 - 3x - 1$$ We are after $a_2$ $$-3 = -(ap + ac + ab) \implies p + c + b = 3/a = \frac{3}{8}$$ Which is wrong because, the answer is supposed to be an integer, and the answer is: $$A=98$$","['algebra-precalculus', 'contest-math', 'polynomials']"
1173561,Casorati matrix,"Does anyone know what the Casorati matrix is? I read that the Casorati matrix is useful in the study of linear difference equations. However, I couldn't find what the Casorati matrix is. Any help?","['matrices', 'recurrence-relations', 'discrete-mathematics']"
1173607,Functions so that image of min (resp. max) is a positive definite kernel,"I am trying to determine the functions $\phi : \mathbb{R}^+ \to \mathbb{R}$ such that: Pb 1: $K(s, t) = \phi( \mathrm{min} (s,t))$ is a positive definite kernel on $\mathbb{R}^+$. Pb 2: $K(s, t) = \phi( \mathrm{max} (s,t))$ is a positive definite kernel on $\mathbb{R}^+$. (NB: The 2 problems are independent, I am not trying to find functions statisfying BOTH conditions) For Pb1, since min is a p.d. kernel, I have found that all $\phi$ admitting a power series expansion on  $\mathbb{R}^+$ with positive coefficients will work (e.g. polynoms with positive coeffs, exponential). 
By Cauchy-Schwarz inequality, I also found that $\phi$ must be increasing. For Pb2, Cauchy-Schwarz gives that $\phi$ must be decreasing. It must also take positive values. I have found that positive constant functions work, and I wonder if they are the only ones. Any hints?","['statistics', 'hilbert-spaces', 'functional-analysis', 'analysis']"
1173627,"Is really $[0,1]\times[0,1)$ homeomorphic to $[0,1)\times[0,1)$ with the product topology? [duplicate]","This question already has answers here : Show that $[0, 1)\times[0, 1)$ is homeomorphic to $[0, 1]\times[0, 1)$ but not to $[0, 1]\times[0, 1]$. (5 answers) Closed 7 years ago . I got asked to show that $[0,1]\times[0,1)$ is homeomorphic to $[0,1)\times[0,1)$ with the product topology, but I'm having trouble understanding why that's not false. $[0,1]$ is obviously compact while $[0,1)$ is not. What am I not getting here?","['general-topology', 'product-space']"
1173669,Why is $\partial_z\partial_{\bar z}=\frac14\left(\partial_r^2+\frac1r\partial_r+\frac1{r^2}\partial_{\theta}^2\right)$?,"I have to show the identity I wrote in the title: it should be $\partial_z\partial_{\bar z}=\frac14\left(\partial_r^2+\frac1r\partial_r+\frac1{r^2}\partial_{\theta}^2\right)$ but some computation doesn't match with my book with these operators. We know that $\partial_z\partial_{\bar z}=\frac14(\partial_x^2+\partial_y^2)$.
Then we pass to polar coordinates writing $x=r\cos\theta$ and $y=r\sin\theta$, from which, obviously we have $r=\sqrt{x^2+y^2}$ and $\theta=\arctan\frac yx$.
We use than the chain rule to write
$$
\partial_x=\frac{\partial r}{\partial x}\partial_r+\frac{\partial\theta}{\partial x}\partial_{\theta}=\cos\theta\partial_r-\frac{\sin\theta}{r}\partial_{\theta}
$$
and
$$
\partial_y=\frac{\partial r}{\partial y}\partial_r+\frac{\partial\theta}{\partial y}\partial_{\theta}=\sin\theta\partial_r+\frac{\cos\theta}{r}\partial_{\theta}
$$ and till here all matches with my book. My problem comes now: I used the above cited $\partial_z\partial_{\bar z}=\frac14(\partial_x^2+\partial_y^2)$ but this gives me 
$$
\partial_z\partial_{\bar z}=\frac14\left(\partial_r^2+\frac1{r^2}\partial_{\theta}^2\right)
$$
because the ""crossed"" terms are equal and opposite: squaring $\partial_x$ the ""crossed"" term is $-\frac2r\cos\theta\sin\theta\partial_r\partial_{\theta}$, squaring $\partial_y$ the ""crossed"" term is $\frac2r\cos\theta\sin\theta\partial_r\partial_{\theta}$ hence in the sum they should vanish. Where is the error? EDIT: following Andrea's suggest, I wrote
\begin{align*}
\partial_x^2+\partial_y^2
&=\left(\cos\theta\partial_r-\frac{\sin\theta}{r}\partial_{\theta}\right)\left(\cos\theta\partial_r-\frac{\sin\theta}{r}\partial_{\theta}\right)+\\
&\;\;\;\;\;\,\left(\sin\theta\partial_r+\frac{\cos\theta}{r}\partial_{\theta}\right)
\left(\sin\theta\partial_r+\frac{\cos\theta}{r}\partial_{\theta}\right)\\
&=\cos^2\theta\partial_r^2+\frac{\sin\theta}{r}\cos\theta\partial_{\theta}
+\frac{\sin^2}{r}\partial_r+\cos\theta\frac{\sin\theta}{r^2}\partial_\theta+\\
&\;\;\;\;\;\;\sin^2\theta\partial_r^2-\frac{\cos\theta}{r}\sin\theta\partial_{\theta}
+\frac{\cos^2}{r}\partial_r-\sin\theta\frac{\cos\theta}{r^2}\partial_\theta\\
=&\partial_r^2+\frac1r\partial_r
\end{align*} which is false again! I checked several times but I cannot find where my mistake is! Many thanks!","['operator-theory', 'coordinate-systems', 'functional-analysis', 'complex-analysis']"
1173682,Limits and exponentials,"Asked to find $\lim_{n\to\infty}a_n$ where
$$a_n = \left(1+\dfrac1{n^2}\right)^n$$ I know that the limit = 1, and can get to this by saying $\ln a_n=n\ln\left(1+\dfrac1{n^2}\right)$ and going from there. My question is: would it also be enough to simply direct substitute and say that…? $$\left(1+\left(\frac1\infty\right)^2\right)^\infty = (1+0)^\infty = 1$$",['limits']
1173702,Can a birational morphism surject from an affine to a projective variety?,"Let $X$ be an affine variety and $Y$ a projective variety, both integral (reduced and irreducible). Assume that $\phi:X\to Y$ is a birational morphism. I would venture to say that $\phi$ can not be surjective, but I have no proof for the statement. So, my question: Can $\phi$ be surjective? If no, why?",['algebraic-geometry']
