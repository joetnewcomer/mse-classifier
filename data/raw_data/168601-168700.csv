question_id,title,body,tags
2956443,Deducing the null space from column relations,"Let $A=\begin{bmatrix}
a_{1} & a_{2} & a_{3}
\end{bmatrix}$ and $3a_{1}+2a_{2}+a_{3}=0$ where $a_{i}$ are matrix columns. Find the nullspace of $A$ . So, my initial thought was that the nullspace is the span of the vector $\begin{bmatrix}
3\\ 
2\\ 
1
\end{bmatrix}$ , but I'm not quite sure.","['matrices', 'linear-algebra']"
2956506,Set of Elementary Real Numbers Without Elementary Combination,"Find a set of $n$ real numbers such that none can be created by combining the others with elementary operations ( $+, -, \times, /$ ). This question came up in my attempt to prove an $n$ dimensional version of the fundamental theorem of algebra; however, I am interested in the answer to this regardless of the relevance it has to the associated question. I have thought about this a bit, and so far my best guess is the square roots of the first $n$ primes. Clearly, this fulfills the above property on just multiplication, but I am not sure how to prove this for addition, and especially for a combination of the two. It would probably involve some very strong statements about sums of square roots, but I'm not sure how to proceed. Any help would be appreciated. (I'm also not sure how to tag this, as I don't know what branch of math questions like this belong to. If anyone could help there that'd be appreciated as well.)",['algebra-precalculus']
2956512,Piecewise function and differentiation quotient,"We have function $y = f(x) = |2x + 1|$ Can we simply say that $|2x + 1|' = 2$ ? No because if we use the chain rule then we get $\left(\left|2x+1\right|\right)'\:=\frac{2\left(2x+1\right)}{\left|2x+1\right|}$ Redefine the same function piecewise without using the absolute value. $$\left|2x+1\right|=
\begin{cases} 
      2x+1 & x \geq -\frac12 \\
      -2x-1 & x \leq -\frac12 
   \end{cases}$$ Calculate the differential quotient for each of the
corresponding regions. $f(x)=2x+1$ Differential quotient $$f'(x_0) = \lim_{x→x_0}
\frac{f(x) − f(x_0)}{x − x_0}.$$ $f'(x_0) = \lim_{x→x_0}
\frac{2x+1 − 2x_0-1}{x − x_0}=2$ $f(x)=-2x-1$ $f'(x_0) = \lim_{x→x_0}
\frac{-2x-1 + 2x_0+1}{x − x_0}=-2$ What happens at $x = −\frac12$ ? This is where I am confused. There is no x so that would mean that my
  answer is wrong isn't it?","['analysis', 'calculus', 'functional-analysis', 'derivatives', 'piecewise-continuity']"
2956559,In a Banach space $x_n\text{cos}(nt)+ y_n\text{sin}{nt}\rightarrow 0$ implies both sequences individually go to $0$.,"Let $X$ be a Banach space, and let $\{x_n\}$ , $\{y_n\}$ be two sequences in $X$ . Suppose $x_n\text{cos}(nt)+ y_n\text{sin}(nt)\rightarrow 0$ as $n \rightarrow \infty$ for all $t$ in a non degenerate interval. Show that $\{x_n\}$ and $\{y_n\}$ both go to $0$ . I'm trying to show that both the sequences go to $0$ along each subsequence or at least show that each subsequence has a further subsequence along which both go to $0$ . To do this, I thought of somehow choosing the further subsequence such that for some $t$ in the given interval, $e^{int}$ is quite close to $1$ along the further subsequence which should give me information about $\{x_n\}$ . However, irrational rotations on the circle equidistribute which makes me clueless as to how to achieve this.","['ergodic-theory', 'functional-analysis', 'dynamical-systems']"
2956562,"Showing $\operatorname{Cov}\left(\bar{X}_n,\frac{1}{n}\sum|X_i-\bar{X}_n|\right)=0$ for i.i.d standard normal $X_1,X_2,\ldots,X_n$","Show that for $X_1,X_2,\ldots,X_n$ i.i.d. standard normal, $$\operatorname{Cov}\left(\bar{X}_n,\frac{1}{n}\sum|X_i-\bar{X}_n|\right)=0$$ where $$\bar{X}_n = \frac{1}{n}\sum X_i$$ What I do first is to write $$\operatorname{Cov}\left(\bar{X}_n,\frac{1}{n}|X_i-\bar{X}_n|\right) = \frac{1}{n}E\left(\bar{X}_n \cdot\sum|X_i-\bar{X}_n|\right)-\frac{1}{n}E(\bar{X}_n)E\left(\sum|X_i-\bar{X}_n|\right)$$ Now, I now that the second part is zero, since $E(X_i)=0$ but then, I have no idea what to do with the absolute values in the first part. Any suggestions?","['expected-value', 'statistics', 'covariance', 'normal-distribution']"
2956578,How to solve $\sin^2 (2x) = 2 \sin (2x)$,I am not sure how to solve this: $$\sin^2 (2x) = 2 \sin (2x)$$ I thought I could rewrite it like this: $$4 \sin^2 (x) \cos^2 (x)  = 4 \sin (x) \cos (x)$$ and maybe like this as well: $$ \sin (x) \cos (x)  = 0$$ but I have no idea whether it is correct and how get to the solution .. thanks for help,['trigonometry']
2956595,Find the zeros of $f(z)=z^3-\sin^3z$,"I want to find the zeros of $f(z)$ , $$f(z)=z^3-\sin^3z$$ My attempt $f(z)=0$ $z^3-(z-z^3/3!+z^5/5!-\dots)^3=0$ $z^3-z^3(1-z^2/3!+z^4/5!-\dots)^3=0$ $z^3[1-(1-z^2/3!+z^4/5!-\dots)^3]=0$ So $z=0$ is a zero of order $3$ . I don't feel good about this answer. Please give me some hints if I am incorrect. Edit: What is the order of root $z=0$ ?","['complex-analysis', 'power-series', 'analytic-functions']"
2956615,Finding $\int^{\frac{\pi}{2}}_{0}\ln(\sin x)\cdot \sin xdx$,"Finding $\displaystyle \int^{\frac{\pi}{2}}_{0}\ln(\sin x)\cdot \sin xdx$ What I try:-> Integration by parts assuming $\displaystyle I = \int\ln(\sin x)\cdot \sin xdx = -\ln(\sin x)\cdot \cos x+\int\frac{\cos^2 x}{\sin x}dx$ $\displaystyle I = -\ln(\sin x)\cdot \cos x+\int\frac{1-\sin^2 x}{\sin x}dx$ $ = -\ln(\sin x)\cos x+\ln\bigg(\tan\frac{x}{2}\bigg)-\cos x$ $ \displaystyle \int^{\frac{\pi}{2}}_{0}\ln(\sin x)\cos xdx = \bigg[-\ln(\sin x)\cos x+\ln\bigg(\tan\frac{x}{2}\bigg)-\cos x\bigg]\bigg|^{\frac{\pi}{2}}_{0}=-\ln(0)+\ln(0)$ but answer is $\ln(2/e)$ could some explain me why I have got wrong answer,thanks also explain me How I solve it using double integral",['integration']
2956627,Show that $e^z$ is continuous on $\mathbb{C}$,"I know that $e^z$ is continuous on $\mathbb{R}$ , but how would I show this rigorously on $\mathbb{C}$ using the $\epsilon - \delta$ definition of continuity? I know how to begin: If $|z - z_0| < \delta$ then we want $|f(z) - f(z_0)| < \epsilon$ . To work backwards, I know we want to basically play around with $|f(z) - f(z_0)| = |e^z - e^{z_0}|$ and then pick $\delta$ to have some relationship with $\epsilon$ so that we get the inequality. However, I am having a hard time figuring out how to proceed with expanding $|e^z - e^{z_0}|$ in a way that gets me to a point where I can get $|z - z_0|$ to appear somewhere.","['complex-analysis', 'continuity', 'complex-numbers', 'epsilon-delta']"
2956636,A proof of Transfinite Induction,"This is my proof of Transfinite Induction by filling the gaps in my textbook. It would be great if someone help me verify if i correctly understand what is meant by the authors! Let $P(\alpha)$ is a property defined for all ordinals $\alpha$ . Suppose that $P(\alpha)$ is true for all $\alpha<\beta$ implies $P(\beta)$ is also true. Then $P(\alpha)$ is true for all ordinals $\alpha$ . My attempt: Assume the contrary that $P(\gamma)$ is not true for some ordinal $\gamma$ . Let $F:=\{\alpha \in\gamma\cup\{\gamma\}\mid P(\alpha) \text{ is not true}\}$ . It follows that $\gamma\in F$ and thus $F\neq\emptyset$ , and that $F$ is a set of ordinals and thus is well-ordered. Let $\beta=\min F$ . Then $\alpha\notin F$ for all $\alpha<\beta$ . This implies that $P(\alpha)$ is true for all $\alpha<\beta$ . By inductive hypothesis, $P(\beta)$ is also true and thus $\beta\notin F$ . This is a contradiction. Hence $P(\alpha)$ is true for all ordinals $\alpha$ .","['elementary-set-theory', 'ordinals', 'proof-verification']"
2956667,Can you determine a set of values from a set of sums?,"Consider the following problem: There is an vector $A$ (which you will not see) of $n$ positive integers. You are given the set of sums of the (contiguously indexed) subvectors of $A$ . For example, say $$A = (3,2,1,2)$$ The subvectors are $(3),(2),(1),(2), (3,2), (2,1), (1,2),(3,2,1), (2,1,2),(3,2,1,2)$ .
We would be given the sums $\{1, 2, 3, 5, 6, 8\}$ .  Let us call this set of sums $f(A)$ . Is it always possible to uniquely determine the set of integers in $A$ from $f(A)$ and $n$ ? The answer turns out to be no . I posted a followup to When does a set of sums uniquely determine a set of values? .",['combinatorics']
2956674,Validate the proof that the sequence $x_n = \sum_{k=1}^{n} {1\over n+k}$ is bounded.,"Let $n \in \mathbb N$ and: $$
x_n = \sum_{k=1}^{n} {1\over n+k}
$$ Prove that $x_n$ is a bounded sequence. I'm wondering whether the proof below is valid. Since $n \in \mathbb N$ we have that $x_n$ is strictly greater than $0$ . For the upper bound lets consider the following sequence $y_n$ : $$
\begin{align}
y_n &= {1 \over n + 1} + {1 \over n + 1} + \dots + {1 \over n + 1} = \\
&= \sum_{k = 1}^n {1 \over n+1} = {n \over n + 1}
\end{align}
$$ Since $x_n$ has an increasing denominator in each consecutive term of the sum we may conclude that $x_n < y_n$ . So summarizing the above: $$
0 < x_n < y_n
$$ Which means that the sequence is bounded. Have I missed something?","['algebra-precalculus', 'proof-verification', 'sequences-and-series']"
2956791,Find count of this functions roots:$\sqrt{x+1}-x^2+1=0$,"There is an equation here: $$\sqrt{x+1}-x^2+1=0$$ Now we want to write the equation $f(x)$ like $h(x)=g(x)$ in a way that we know how to draw h and g functions diagram.
Then we draw the h and g function diagrams and find the common points of them. So it will be number of the $f(x)$ roots that here is the equation mentioned top.
Actually now my problem is with drawing the first equation's diagram
I want you to draw its diagrams like $\sqrt{x-1}$ syep by step. Please help me with it!","['roots', 'radicals', 'functions', 'algebra-precalculus', 'quadratics']"
2956801,MAP estimation of a diagonal linear state transition matrix with bounded support,"I have a stochastic linear dynamical system: $x_{t} = A x_{t-1} + w_t,$ where $x_{t}$ is a latent state vector, $A$ is a linear state transition matrix and $w_t$ is a process noise vector drawn from $\mathcal{N}(0,Q)$ with diagonal covariance matrix $Q$ . At each point in time, I obtain an observation: $y_{t} = C x_{t} + v_t,$ where $C$ is an observation matrix and $v_t$ is a measurement noise vector drawn from $\mathcal{N}(0,R)$ . The true value of the matrix $A$ is unknown to me. I want to learn this matrix from data. Specifically, given a sequence of observations $y_{1:T}$ , I want to infer the  maximum a posteriori estimate of the matrix $A$ . However, for reasons of identifiability/interpretability/numerical stability, I want to constrain the values of certain elements of $A$ . Specifically: I want the off-diagonal elements of $A$ to be zero (i.e., I want $A$ to be diagonal). I want the diagonal elements of $A$ to lie within (0,1). I am doing the following: I put a multivariate normal prior on the column vector containing the diagonal elements of $A$ : $\textrm{diag}(A) \sim \mathcal{N}(\mu,\Sigma)$ where $\Sigma$ is a diagonal covariance matrix; by only doing inference on the diagonal elements of $A$ , I assume the off diagonal elements are known to be zero. To achieve a bounded support, I use a truncated multivariate normal prior on $\textrm{diag}(A)$ with support (0,1). The use of a truncated multivariate normal prior is convenient (from the perspective of conjugacy) as the likelihood is multivariate normal due to the Gaussian nature of the process noise. Finding the mode of a truncated multivariate normal posterior distribution (the MAP estimate) is a quadratic programming problem, I believe. However, in my case, because the covariance matrix of the untruncated multivariate normal posterior distribution of $\textrm{diag}(A)$ is diagonal (due to the diagonal nature of both $\Sigma$ and $Q$ ), it is trivial to find the mode of the truncated multivariate normal posterior distribution - I simply set any terms in the mode of the untruncated multivariate normal posterior distribution to 0 (if < 0) or 1 (if > 1). Is this the appropriate way to do this? Should I use a prior other than a truncated multivariate normal distribution? Many thanks in advance","['statistics', 'probability-distributions', 'bayesian', 'probability-theory', 'probability']"
2956810,"In question of ten married couples to be seated at five different tables, why do we not care about the other tables?","The solution to this problem, Ten married couples are to be seated at five different tables, with four people
  at each table. Assume random seating, what is the expected number of
  married couples that are seated at the same table? says that the probability of any individual married couple sitting at a table together is a Bernoulli random variable and can be calculated with $$P(X_i=1) = {{{18}\choose2}\over{{19}\choose3}}$$ where ${19}\choose3$ is all the combinations of people that can join one husband from couple $i$ in the three remaining places at the table and ${{18}\choose2}$ is the all the combinations of people that can join a couple $i$ in the two remaining places at the table. My question is, why are there not more possibilities to consider because of the many combinations of people sitting at the other tables? For example, although there is only ${19}\choose3$ combinations of people that can join one husband from couple $i$ in the three remaining places at the table, is there not ${{16}\choose4}{{12}\choose4}{{8}\choose4}{{4}\choose4}$ combinations of people who can sit at the other tables, multiplying the possibilities? Is this because these possibilities cancel each other out? Do we say $${{{18}\choose2}{{16}\choose4}{{12}\choose4}{{8}\choose4}{{4}\choose4}\over{{19}\choose3}{{16}\choose4}{{12}\choose4}{{8}\choose4}{{4}\choose4}}= {{{18}\choose2}\over{{19}\choose3}}$$ or is there a reason why that cannot be done? Or is there a reason it is unnecessary?","['statistics', 'combinatorics', 'probability']"
2956826,probability that no two people are born on the same day: $n$ people and $k$ days,"Say we have $n$ people and $k$ days. What is the probability that NO two of the $n$ people are born on the same day? This is of course assuming that $n \leq k$ (otherwise the answer is $0$ by the pigeonhole principle!).
I said the answer was: $k \choose n$$\cdot n! \cdot \frac{1}{k^n}$ because in order for this to happen, n distinct birthdays must be chosen, they can be arranged in any way amongst the $n$ people, and there are $k^n$ total arrangements. Is this correct?","['statistics', 'birthday', 'probability']"
2956829,Solve the Differential equation $\frac{dy}{dx}=\frac{5x^3-xy^2-2x}{3x^2y-y^3}$,Solve the Differential equation $$\frac{dy}{dx}=\frac{5x^3-xy^2-2x}{3x^2y-y^3}$$ My try: Let $x^2=X$ and $y^2=Y$ we get $xdx=dX$ and $ydy=dY$ then $$\frac{dY}{dX}=\frac{5X-Y-2}{3X-Y}$$ which is a Non homogenous Differential equation.Is there any formal approach to solve this?,"['algebra-precalculus', 'homogeneous-equation', 'ordinary-differential-equations']"
2956835,"Let $ABCD$ be a rectangle, $E$ midpoint of $DC$ and $G\in AC$ such that $BG\bot AC$. Let $F$ be a midpoint of $AG$. Prove $\angle BFE =\pi/2$.","Let $ABCD$ be a rectangle, $E$ midpoint of $\overline{DC}$ and $G$ point on $\overline{AC}$ such that $\vec{BG}$ is perpendicular to $\vec{AC}$ . Also, let $F$ be  a midpoint of $\overline{AG}$ . Prove that angle $\angle BFE =\pi/2$ . I found already 4 solution which I have posted here Proving right angle using vectors But now I'm interested in geometric transformation solution. I found this one also: Spiral similarity $\mathcal{P}$ with center at $B$ that takes $A\mapsto D$ takes also $G\mapsto C$ and there for it takes midpoint $F$ of $AG$ to a midpoint of $CD$ , that is $E$ . Now this spiral similarity $\mathcal{P}$ induces new spiral similarity $\mathcal{P}'$ which takes $A\mapsto F$ and $D\mapsto E$ and also $B\mapsto B$ , so we have $$\angle EFB = \angle DAB = 90^{\circ}$$ Finaly, here is my question. We notice also $$A\stackrel{\mathcal{P}}{\longmapsto}D \stackrel{\mathcal{S}}{\longmapsto}C \stackrel{\mathcal{P}^{-1}}{\longmapsto}G$$ where $\mathcal{S}$ is reflection across $E$ . So transformation $$ \mathcal{T} = \mathcal{P}^{-1}\circ \mathcal{S} \circ \mathcal{P}$$ is reflection across $F$ that takes $A$ to $G$ . Can we from here deduce that $\angle EFB = 90^{\circ}$ ?","['euclidean-geometry', 'geometric-transformation', 'geometry']"
2956889,"How many number of integer coordinates exists between a line segment, including the end points?","There is a line segment say $AB$ with coordinates of end-points as $A=(x_1, y_1)$ and $B=(x_2, y_2)$ . $x_1, y_1, x_2, y_2$ are integers. I need to find the number of integer coordinates which lie on the line segment including end-points. I read somewhere that it is $\gcd(|x_1 - x_2|, |y_1 - y_2|) + 1$ . But, I cannot understand why this works. I do not get the intuition behind it. I searched for proof but did not find anything intuitive and straightforward. Please help me understand this. I am stuck on it. I am expecting a nice proof with great explanation. Thanks in advance!","['proof-explanation', 'gcd-and-lcm', 'geometry', 'discrete-mathematics', 'algorithms']"
2956917,ODE featuring product of derivatives,"I consider an ODE of the form $$\frac{\mathrm{d}U(u)}{\mathrm{d}u} \frac{\mathrm{d}V(v)}{\mathrm{d}v} = f(u, v).  $$ The function $f$ is known, and the goal is to find $U$ and $V$ , assuming that any initial or boundary conditions which turn out to be necessary have been specified. This equation arises when trying to find the Weyl rescaling upon coordinates $u$ and $v$ leading to the 2D line element $$ ds^2 = -f^{-1}(u, v)  du dv.$$ Despite its apparent simplicitly I find I do not know how to solve or even classify this system: it is not a PDE since $U$ and $V$ are respectively independent of $v$ and $u$ ; it does not obviously separate into a pair of coupled ODEs, etc. Anyway, I would like to solve this system, presumably numerically. Can anyone point me in the right direction? Edit: It has been pointed out that the form given implies $f(u,v) = g(u) h(v)$ . Thus the problem separates into that of solving two coupled ODEs: $\frac{dU}{du} = -\lambda g(u)$ and $\frac{dV}{dv} = -\lambda h(v)$ . Therefore, the remaining substance of the question is to find $g$ and $h$ .","['ordinary-differential-equations', 'differential-geometry']"
2956922,Difference real and complex fourier series,"I'm working on fourier series and I'm trying to compute the fourier transformation for the $2\pi$ -periodic function of $f(x)=x^2$ with $x \in [-\pi,\pi]$ . Now with the real way, that is $$f(x) \sim \frac{a_{0}}{2}+\sum\limits_{n=1}^{\infty}a_{n}\cos(nx)+b_{n}\sin(nx)$$ and I found $$f(x) \sim \frac{\pi^2}{3}+\sum\limits_{n=1}^{\infty} \frac{4}{n^2}(-1)^{n}\cos(nx).$$ Now I also tried to compute with the imaginary way, that is with $$f(x) \sim c_{0}+\sum\limits_{n=-\infty}^{\infty}c_{n} e^{inx},$$ with $$c_{n}=\frac{1}{2\pi} \int\limits_{-\pi}^{\pi}f(x)e^{-inx},$$ and I found $$f(x) \sim \frac{\pi^2}{3}+\sum\limits_{n=1}^{\infty} \frac{2}{n^2} (-1)^{n} e^{inx},$$ which doesn't seem to be the same. I'm sure about the real computation, any suggestions where I go wrong with the imaginary part?","['fourier-analysis', 'fourier-transform', 'complex-analysis', 'fourier-series', 'complex-integration']"
2956923,Is it consistent with ZF that there is no uncountable set of algebraically independent reals?,Exactly what the title asks. This question was inspired by this one which looks for a countable such set. It is fairly easy to construct a size $\mathfrak{c}$ algebraically independent set of reals using a diagonalization of size $\mathfrak{c}$ if you have AC. What about without it though?,"['number-theory', 'set-theory']"
2956925,When does a set of sums uniquely determine a set of values?,"This is a follow up to a question about determining the set of values from a set of sums. We consider the following setup: Consider a vector $A$ (which you will not see) of $n$ positive integers. You are given the set of sums of the (contiguously indexed) subvectors of $A$ . For example, say $$A = (3,2,1,2)$$ The subvectors are $(3),(2),(1),(2), (3,2), (2,1), (1,2),(3,2,1), (2,1,2),(3,2,1,2)$ .
We would be given the sums $\{1, 2, 3, 5, 6, 8\}$ .  Let us call this set of sums $f(A)$ . It turns out it is not always possible to uniquely determine the set of integers in $A$ from the pair $(f(A), n)$ ? For example: $$A = (1,1,3)$$ and $$B = (1,2,2)$$ both give the same set of sums $f(A) = f(B) = \{1,2,3,4,5\}$ . However in some cases it is possible. For example, if $n=3$ and $f(C) = \{1,2,3,5,6\}$ then we know that $\operatorname{set}(C) = \{1, 2, 3\}$ . This raises the following question: Is it possible to characterise which pairs $(f(X), n)$ uniquely 
  determine the set of values in the array $X$ ?",['combinatorics']
2956985,Locally convex topological space,"I got a problem with this task.
Let $M=\{[f] , f:[0,1] \to \mathbb{F} , \int_0 ^1 |f(x)|^x dx<\infty \}$ , where $f$ is a measurable function, be a set and let $\rho(f,g)=\int_0 ^1 |f(x)-g(x)|^x dx$ be a metric. Consider a topology induced by metric $\rho$ . Proof, that set $M$ with this topology is or is not locally convex space. Some hint how on prove it ? My try - I think it is not locally convex space. Proof: Let $U\in T(0)$ , where $T$ is topology on $M$ . We show that $co(U)=M$ .
Let $\epsilon >0$ such that $B(0,\epsilon)\subset U$ and $n\in \mathbb{N}$ such taht $1/\sqrt{n}<\epsilon$ . Consider $f\in M$ . Define $I_i=[\frac{i-1}{n},\frac{i}{n}],~i=1,..,n$ . Define $f_i=\sqrt{n}f\chi_i,~ i=1,..,n$ where $\chi$ is characteristic function. Then holds $\rho(f_i,0)=\int_0^1|f_i(x)|^xdx=\int_{I_i}|\sqrt{n}f(x)|^xdx\leq \sqrt{n} \int_{I_i}|f(x)|^xdx\leq \frac{1}{\sqrt{n}}<\epsilon$ . It will implies $f_i\in B(0,\epsilon)\subset U$ and $f=\sum_1^n \frac{1}{\sqrt{n}}f_i$ , so $f\in co(U)$ and then $co(U)=M$ .
It is correct ?
Thanks.","['calculus', 'general-topology', 'functional-analysis', 'measure-theory']"
2957017,Directional derivative and composite functions,"We have a function $f(p_0)$ with $p_0 \in \mathbb{R}^n$ , and the vector $\vec{v}$ with $||\vec{v}|| = 1$ . The derivative of $f(p)$ to $\lambda$ in the point $p = p_0 + \lambda \vec{v}$ , calculated for $\lambda = 0$ , is called directional derivative along $\vec{v}$ and is indicated with the symbol $$\left( \dfrac{\partial f}{\partial v} \right)_{p_{0}}$$ From the composite function derivation rules, must be $$\left[ \dfrac{d}{d \lambda} f(p_0 + \lambda \vec{v})\right]_{\lambda = 0} = f'_{x_1}(p_0) \cdot v_1 +... f'_{x_n}(p_0) \cdot v_n$$ Although I have consulted several books, this passage is still not clear to me, especially how the function $f(p_0 + \lambda \vec{v})$ should be a composite function. Can you help me? Thank you in advance","['multivariable-calculus', 'functions']"
2957030,Local uniqueness of solution for quasi linear PDE,"I've a little troubles in proving  local uniqueness of solution for Cauchy problems concerning quasilinear PDE's. It's a little bit boring, but I tried to be as clear as possible. Suppose $\Omega$ is an open and connected subset of $\mathbb{R}^2$ and let $a(x,y,z),b(x,y,z),c(x,y,z)$ scalar functions of class $C^1$ in $\Omega \times \mathbb{R}$ . Let $I$ an open interval and $f=f(s)$ , $g=g(s)$ and $h=h(s)$ be $C^1(I)$ . We want to prove local existence and uniqueness of a solution for the Cauchy problem $$\begin{cases} a(x,y,u)u_x+b(x,y,u)u_y=c(x,y,u)\\u(f(s),g(s))=h(s)\qquad s\in I\end{cases},$$ under certain conditions using method of characteristic. Consider for each fixed $s\in I$ the autonomous system of ODE's $$\begin{cases}\frac{d}{dt}x=a(x,y,z)\\\frac{d}{dt}y=b(x,y,z)\\\frac{d}{dt}z=c(x,y,z)\end{cases}$$ with initial conditions $$\begin{cases}x(0)=f(s)\\y(0)=g(s)\\z(0)=h(s)\end{cases}.$$ Because $a,b,c$ are $C^1$ , then for every $s \in I$ I find a unique maximal and global solution $x=X(s,t),y=Y(s,t),z=Z(s,t)$ defined on an open interval $J_s$ containing $0$ such that $X(s,0)=f(s)$ , $Y(s,0)=g(s)$ , $Z(s,0)=h(s)$ . Now consider the function $$(s,t)\longrightarrow (X(s,t),Y(s,t),Z(s,t))\qquad [1] $$ for $s \in I$ , $t \in J_s$ . Fix $s_0 \in I$ and let us reason in a neighborhood of $(s_0,0)$ , trying to define a good domain for $[1]$ and then talk about invertibility. Luckily we can choose an interval $J$ independent of s (on which IVPs for characteristic equations have solutions) provided we are willing to restrict ourselves to an interval $I_0$ containing $s = s_0 $ instead of the entire interval I. Thus we may assume that the domain of the vector-valued function given in $[1]$ is $I_0 × J$ . Thanks to the differentiable dependence of solutions to initial value problems for ODEs, the vector-valued function given in $[1]$ is continuously differentiable. 
Thus we are interested in the invertibility, near $(s, t) = (s_0,0)$ , of the function $[1]$ . Note that at the point $(s, t) = (s_0,0)$ , the Jacobian of the function in $[1]$ is given by $$J=\left|\begin{matrix} X_s(s_0,0) &X_t(s_0,0)\\Y_s(s_0,0) &Y_t(s_0,0)\end{matrix}\right|=\left|\begin{matrix} f'(s_0)& a(f(s_0),g(s_0),h(s_0))\\g'(s_0) &b(f(s_0),g(s_0),h(s_0))\end{matrix}\right|.$$ Provided that $J\neq 0$ we can use inverse function theorem and state that exist a neighbourhood $U$ containing $(s_0,0)$ and a neighbourhood $W$ containing $(f(s_0),g(s_0))$ such that the previous map considered from $U$ to $W$ is invertible. That is, we get two functions $S,T$ defined on $W$ such that $$s=S(x,y),\,\,\,t=T(x,y).$$ If we now define $$u(x,y):=Z(S(x,y),T(x,y))$$ then $u$ solves the Cauchy problem on $W$ . In fact for every $s \in I$ such that $(f(s),g(s))\in W$ we have $$u(f(s),g(s))=Z(S(f(s),g(s)),T(f(s),g(s)))=Z(S(X(s,0),Y(s,0)),T(X(s,0),Y(s,0)))=Z(s,0)=h(s)$$ and it's easy to see that $u$ solves the PDE by differentiating. So a solution exist in a neighborhood $W$ of $(f(s_0),g(s_0))$ . Now come my problems, because I want to prove that $u$ is the unique solution on $W$ . Following the analytical proof given by F. John - Partial Differential Equations-Springer US (1975), suppose that $u'$ is another solution of the Cauchy problem on $W$ .
Let $(x',y')\in W$ . Set $s'=S(x',y')$ and consider the characteristic curve $\Gamma$ that solves $$\begin{cases}\frac{d}{dt}x=a(x,y,z)\\\frac{d}{dt}y=b(x,y,z)\\\frac{d}{dt}z=c(x,y,z)\end{cases}$$ with initial conditions $$\begin{cases}x(0)=f(s')\\y(0)=g(s')\\z(0)=h(s')\end{cases}.$$ Because $u$ and $u'$ solve the Cauchy problem, their corresponding integral surfaces both passes through the point $(f(s'),g(s'),h(s'))$ as the characteristic curve $\Gamma$ does for $t=0$ . So the integral surfaces must contain the part of $\Gamma$ whose projection on $xy$ plane is contained in $W$ . In particular for $t'=T(x',y')$ we have $$u'(x',y')=u'(X(s',t'),Y(s',t'))=Z(s',t')=Z(S(x',y'),T(x',y'))=u(x',y')$$ by definition of $u$ . Here is my question: even if we now that $(s',t')\in U$ , who ensure me that $(s',0)\in U$ too, and so I'm sure that both integral surfaces have the point $(f(s'),g(s'),h(s'))$ in common? I mean, am I sure that if a point $(\overline{x},\overline{y})\in W$ and $s=S(\overline{x},\overline{y})$ then $(f(s),g(s))\in W$ ? I think I have to consider a neighborhood of $(s_0,0)$ contained in $U$ that is a rectangle to be sure that the previous hold: in this way every selected characteristic will pass through the space initial curve $(f(s),g(s),h(s))$ and so the problem is solved.
I apologies for all this words for a problem that is probably trivial and is not about PDE's!! Thanks in advance.","['ordinary-differential-equations', 'cauchy-problem', 'real-analysis', 'partial-differential-equations', 'general-topology']"
2957031,Smooth extension of a smooth map on an non-empty open subset of a manifold to the whole manifold.,"Proposition : Suppose $M$ is a smooth manifold and $\emptyset\neq U\subset M$ is open and $f:U\rightarrow \mathbb{R}$ is a smooth function. Then $f$ does not necessarily extend smoothly to M. Proof : (Counterexample). Let $M=\mathbb{S}^1$ and $U=\mathbb{S}^1\setminus\{p\}$ where $p$ is the 'north pole' (i.e. if we think of $\mathbb{S}^1$ as embedded in $\mathbb{R}^2$ then $p=(1,0)$ ). Define the map $f:U\rightarrow \mathbb{R}$ by stereographic projection, $f:(x_0,x_1)\in \mathbb{S}^1\setminus \{p\}\mapsto \frac{x_1}{1-x_0}\in \mathbb{R}$ which is well-defined and smooth since $x_0\neq 0$ on $U$ . If we attempt to extend this map to $\mathbb{S}^1$ then $p$ must map to $\infty$ by continuity. However $\infty\not\in \mathbb{R}$ , hence $\nexists$ a smooth extension of $f$ to all of $\mathbb{S}^1$ . Is this a valid counterexample or have I missed something obvious? If my proposition is wrong could you please provide a reference for the proof? Thanks!","['proof-verification', 'smooth-manifolds', 'differential-geometry']"
2957072,"Evaluate $\lim_{x \to 4} \frac{x^4-4^x}{x-4}$, where is my mistake?","Once again, I am not interested in the answer. But rather, where is/are my mistake(s)? Perhaps the solution route is hopeless: Question is: evaluate $\lim_{x \to 4} \frac{x^4 -4^x}{x-4}$ . My workings are: Let $y=x-4$ . Then when $x \to 4$ , we have that $y \to 0$ . Thus: $$\lim_{y \to 0} \frac{(y+4)^4 - 4^{y+4}}{y} = \\ = \lim_{y \to 0}\frac{(y+4)^4}{y} - \lim_{y \to 0} \frac{4^{(y+4)}}{y}  $$ And this step is not allowed from the get go, as I am deducting infinities, which is indeterminate. What I should have done though: $$4^4 \lim_{y \to 0} \frac{(1+y/4)^4-1+(4^y-1)}{y} = \\ 4^4 \lim_{y \to 0} \left( \frac{(1+y/4)^4-1}{\frac{y}{4}4} - \frac{4^y-1}{y} \right) = \\
=4^4\left(\frac{1}{4} \cdot 4 - \ln 4 \right) = 256(1-\ln 4)$$","['limits', 'functions']"
2957079,Showing a group does not have a faithful finite-dimensional representation,"I am having some difficulty with this problem. We are asked to show that there does not exist any monomorphism to GL(n, $\mathbb{C}$ ). The group in the domain is defined as follows: Let G = $(P(\mathbb{R}), \delta)$ where $\delta$ is the symmetric difference operator, and $P(\mathbb{R})$ is the power set of the reals. I noted that, for all $A \subseteq P(\mathbb{R})$ , we have the following: A $\delta$ A = $\emptyset$ A $\delta \emptyset$ = A A $\delta\mathbb{R}$ = A $^C$ I am not really sure how to use these facts, but the strategy I initially considered was proof by contradiction. I can't seem to find any way to show that any properties of homomorphisms or injectivity are violated, though. I briefly thought of using a cardinality argument, but I am not sure how to proceed with that either. Any help would be greatly appreciated. Even just a hint would be awesome. Thanks!","['functions', 'group-theory', 'abstract-algebra', 'representation-theory']"
2957160,What is a common framework for these divergent sums?,"If you expand $2^x$ using a finite difference series you end up with the formula $$ 1 + x + \frac{1}{2!}x(x-1) + \frac{1}{3!}x(x-1)(x-2) ... = \sum_{n=0}^{\infty} \frac{(x)_n}{n!} $$ Now these series diverge for negative arguments, but they give some interesting results, i.e. they suggest $$1 - 1 + 1 - 1 \ ... = \frac{1}{2}$$ $$1 - 2 + 3 - 4 \ ... = \frac{1}{4}$$ $$1 - 3 + 6 - 10 \ ... =  \frac{1}{8} $$ and more generally... $$\begin{pmatrix} k \\ k  \end{pmatrix} - \begin{pmatrix} k+1 \\ k  \end{pmatrix} + \begin{pmatrix} k+2 \\ k  \end{pmatrix} ... = 2^{-n}$$ Now what's funny... is that these divergent equalities, can actually be arrived at in a totally different way, which is by differentiating the function $ \frac{1}{1-x}$ $k$ times, dividing by k! and evaluating it at $x=-1$ . These two very distinct methods of infinite series seem to agree and so that has me wondering, is there a natural divergent summation method that encapsulates all this behavior? I want to say something like $$2^x \equiv \sum_{n=0}^{\infty} \frac{(x)_n}{n!} \mod \text{summation method L }$$ $$ \frac{1}{1-x} \equiv \sum_{n=0}^{\infty} x^n \mod \text{summation method L} $$ this sort of abstract framework would be very useful and interesting to explore. But even something like Cessaro summation seems unsatisfactory, and it is not clear if Holder summation is sufficient for me. (I tried to calculate it with it and didn't end up getting the answer I expected).","['divergent-series', 'complex-analysis', 'sequences-and-series', 'formal-power-series', 'finite-differences']"
2957200,How to prove $\sum_{n=1}^{\infty} \frac{\sin n\theta \sin \sqrt{n}}{n}$ is convergent or not,"I want to check whether $$
\sum_{n=1}^{\infty} \frac{\sin n\theta \sin \sqrt{n}}{n}
$$ is convergent or not. $\theta$ is a real number. 
What I know is $$
|\sum_{n=1}^{N}\sin n\theta| = |\frac{\cos \frac{\theta}{2} - \cos(N+\frac{1}{2})\theta}{2\sin\frac{\theta}{2}}| \leq \frac{1}{|\sin \frac{\theta}{2}|}$$ for $\theta\neq 2k\pi$ . Let's assume $\theta \neq 2k\pi$ . So by Dirichlet test, $\sum_{n=1}^{\infty} \frac{\sin n\theta}{n}$ is convergent. But I don't quite know how to solve the original one. Any hint or something? Thank you so much!","['sequences-and-series', 'real-analysis']"
2957207,Inequality on the exponential function,"By playing around, I seem to have come across the following inequality, valid for all $x$ : $$x-(1-e^{-x}) \ge e^{-\frac{2}{x}} x$$ (The constant $2$ is not necessarily the tightest one possible.) Is there an easy way to prove this, and if so, is this inequality known in the literature?  The closest I have been able to come across is $$x-(1-e^{-x}) \ge e^{-\frac{1}{x}} x$$ valid for $x\in[0,1]$ (see e.g., Mond and Paciric, ""Inequalities for exponential functions and means, II"", NAW, 2000. www.nieuwarchief.nl/serie5/pdf/naw5-2000-01-1-057.pdf )","['real-analysis', 'limits', 'inequality', 'derivatives', 'exponential-function']"
2957213,Prime counting function $\phi(x)-c(x)$ vs. $x/\ln(x)$,"So $\pi(x)$ is the prime counting function. That is to say, it counts the number of primes below a given integer $x$ . This function is very important in number theory. I was wondering how well the following counts primes: $$\phi(x)-c(x) = \int_2^x e^{1/\ln(t)}dt-\int_2^x 1 dt.$$ I tried making a table of values for $\pi(x)$ and compared them to $\phi(x)-c(x).$ So for $x=10^7$ , $\pi(x)$ gives: $664,579$ primes. $\phi(x)-c(x)$ gives $687,677.$ So the difference between those two functions is: $23,098.$ The difference between $\pi(x)$ and $x/\ln(x)$ is comparably: $44,158.$ I'm interested in learning more about the asymptotics of $\phi(x)-c(x)$ compared to $x/\ln(x)$ and $Li(x).$ $$Li(x)=\int_2^x1/\ln(t)dt.$$ I want to show that $\phi(x)-c(x)$ counts primes better than $x/\ln(x)$ but worse than $Li(x),$ and that $\phi(x)-c(x)$ asymptotically bounds $\pi(x)$ from above. I also want to show that $$1-\Omega(x)=\int_2^x1-e^{-1/\ln(t)}dt$$ counts primes better than $x/\ln(x)$ and $\phi(x)-c(x)$ but worse than $Li(x)$ and asymptotically bounds $\pi(x)$ from below. Furthermore I want to show that the average of my two functions $$1/2(\phi(x)-c(x)+1-\Omega(x)) =\int_2^x \sinh(1/\ln(t))dt $$ counts primes better than $x/\ln(x),\phi(x)-c(x),$ and $1-\Omega(x),$ but worse than $Li(x),$ and asymptotically bounds $\pi(x).$ Some other approximations to $\pi(x)$ are $$ f(x)=\int_2^x \sin(1/\ln(t))dt, $$ $$ g(x)=\int_2^x \sinh(1/\ln(t))dt, $$ and the average of the two: $$ 1/2(f(x)+g(x)). $$ As I continue to find functions that approximate $\pi(x)$ asymptotically, I am starting to realize that $Li(x)$ is probably the best one, but $1/2(f(x)+g(x))$ is pretty good, and $$B(x)= \int_2^x \sin(.5/\ln(x))+\sinh(.5/\ln(x)) $$ is the best one I've found.","['number-theory', 'elementary-number-theory', 'asymptotics', 'calculus', 'prime-numbers']"
2957243,"How to solve $2y′′+8y′+80y=F(t), y(0)=0, y′(0)=0$?","I am trying to solve the initial value problem, $2y′′+8y′+80y=F(t),   y(0)=0,   y′(0)=0$ where $F(t)=20e^{-t}$ But I am unable to do it. I got the answer of $2Ae^{-t}=20e^{-t}$ But this is wrong I think. I solved the complementary equation and got: $y_{c}=e^{-2t}Acos(6t)+Bsin(6t))$ For the particular solution I guessed (correctly I think) $y_{p}=Ae^{-t}$ Then working some derivates I got: $2Ae^{-t}-8Ae^{-t}+80Ae^{-t}$ I plugged 0 into the corresponding initial values given but this does not produce the right answer. Thank you for you help I got a test tomorrow. This was the only part I neglected -.-","['initial-value-problems', 'calculus', 'proof-verification', 'ordinary-differential-equations']"
2957261,An isometry between totally geodesic submanifolds is smooth?,"I am studying Sharafutdinov's Convex sets in a manifold on nonnegative curvature . I have found the following statement, $S_0$ being a totally geodesic submanifold of an open Riemannian variety $M$ and $h_t$ a map from $S_0$ onto $S_t$ that respects the Riemannian distance (i.e. a metric isometry of $S_0$ onto $S_t$ ). Since $S_0$ is a totally geodesic submanifold and $h_t$ is an isometry of $S_0$ onto $S_t$ , it follows easily that $S_t$ is a totally geodesic submanifold in $M$ and the mapping $h_t$ is smooth. I have trouble in understanding why is true. How precisely do I convert a statement on distances (the fact that $h_t$ is a metric isometry) in the fact that $S_t$ is also a submanifold? EDIT: The part that $h_t$ is smooth follows then from Myers-Steenrod. Thank you in advance.","['isometry', 'riemannian-geometry', 'differential-geometry']"
2957287,Find a probability mass function of a random variable [duplicate],"This question already has answers here : What's the probability that a given permutation has exactly $k$ fixed points. [duplicate] (3 answers) Closed 5 years ago . $\color{red}{Attempt} $ We start with $k=1$ , $P(X=1)$ is the probability that one letter have been put in the correct envelope. Our sample space size is $n$ and since there is only one way that one letter must have been put into the correct envelope and the rest $n-1$ incorrectly and so we see that $P(X=1)= \dfrac{(n-1)!}{n}$ , now for $P(X=2)$ it becomes more complicated, so far I know that ${n \choose 2}$ is the size of the sample space and now we want to count the number of ways in which 2 letters must have been put in the correct envelope. First, of all, the $n-2$ letters that have been put incorrectly we have to count them and we have $(n-2)!$ and then the 2 letters that are put correctly this is done in one way thus $$ P(X=2) = \frac{(n-2)!}{{n \choose 2} }$$ so, in general, we have $$ P(X=k) = \frac{(n-k)!}{n \choose k } $$ is this correct?",['probability']
2957309,(Random Walk) Compute average relative number of consecutive cookies eaten from the right side of the gap,"Currently I am reading the paper ' Excited Random Walk in One Dimension .' At page $8$ left column, the authors obtain the following: Probability that the walk eats precisely $r > 0$ consecutive cookies (we term this event a single “meal”) from the right edge of the cookie-free region is $$P(r) = 2q \frac{\Gamma(L)}{\Gamma(L-2q)} \frac{\Gamma(L+r-1-2q)}{\Gamma(L+r)}$$ where $L-2$ refers to cookie-free gap and $p$ refers to probability of the walk moving to the right and $q$ is the probability of the walk moving to the left. However, when they calculate the average relative number of consecutive cookies eaten from the right side of the gap, they compute $$\int_0^\infty \tilde{r} \tilde{P}(\tilde{r})\,d\tilde{r}$$ where $\tilde{r} = \frac{r}{L}$ and $\tilde{P} = LP(r).$ Question: Why do they integrate with respect to $\tilde{r}$ with integrand $\tilde{P}?$ I thought to find the average number of cookie eaten, one just needs to compute $$\int_0^\infty r P(r)\, dr$$ instead of the above.","['random-walk', 'expected-value', 'stochastic-processes', 'probability', 'density-function']"
2957327,Fermat's Point applies to isosceles triangles,"Fermat's Point applies to equilateral triangles. Recently as I searched isosceles triangles on Wolfram Mathworld, I learnt that the same principle applies to similar isosceles triangles. Besides the fact that the total distance from the three vertices of the triangle to Fermat's Point is the minimum possible, how is Fermat's Point different with the points formed by the isosceles triangles?
Why is Fermat's point recorded in the Encyclopedia of Triangle Centers as X(13), while the other points formed by the isosceles triangles are omitted?",['geometry']
2957334,How many ways can we distribute $r$ identical balls into $n$ distinct boxes with exactly $m$ boxes empty,"There are $\binom{r+n-1}{n-1}$ ways for distributing $r$ identical balls into $n$ distinct boxes. The $m$ part is throwing me off. I would say there are $\binom{r+n-m-1}{n-m-1}$ ways, but I'm not sure if I'm right.","['combinatorics', 'discrete-mathematics']"
2957353,Computation of cohomology with ideal sheaf involved.,"Let $X$ be a complex projective surface an $Z\subset X$ be a finite set of points (reduced closed subscheme of dimension zero). Denote by $\mathcal{I}_Z$ the ideal sheaf of $Z$ . Let $E$ be a vector bundle over $X$ . Under which conditions does the cohomology group $$
\mbox{H}^1(X,E\otimes\mathcal{I}_Z)
$$ vanish? I am aware of some results for $E$ a line bundle but none in higher rank. Any reference will be appreciated. Added: We have the long exact sequence $$
0 \to \mbox{H}^0(X,E\otimes\mathcal{I}_Z) \to \mbox{H}^0(X,E) \to \mbox{H}^0(Z,E|_Z) \to \mbox{H}^1(X,E\otimes\mathcal{I}_Z) \to \mbox{H}^1(X,E) \to 0
$$ and the vanishing of $\mbox{H}^1(X,E\otimes\mathcal{I}_Z)$ implies the vanishing of $\mbox{H}^1(X,E)$ . Thus $\mbox{H}^1(X,E) = \{0\}$ is a necessary condition. What can be imposed to $E$ for this condition also be sufficient? The case that interests me is when $E$ has rank two and $Z$ is the zero  scheme of a global section of $E$ . Hence $h^0(Z,\mathcal{O}_Z) = c_2(E)$ .","['complex-geometry', 'algebraic-geometry', 'sheaf-cohomology']"
2957396,Topology From Sequences,"Let $X$ be a set and $f:X\to\mathcal{P}(X^\omega)$ .  Under which circumstances is there some topology $\tau$ on $X$ such that $f$ maps each point $x$ to the set of converging sequences in $\tau$ with limit $x$ ?  The finest topology which realizes all sequences in $f$ is $$\tau:=\{A\subseteq X:\forall x\in A.\forall s\in f(x).\exists N\ge 0.\{s_n\}_{n\ge N}\subseteq A\}.$$ But how to make sure no new converging sequences emerge in $\tau$ ? There is an (open) neighbourhood base of $\tau$ whose sets $A_x$ can be constructed the following way: Modify $f$ by replacing any sequence with a tail of itself, call the result $f'$ .  Then define inductively $A_0:=\{x\}$ , $A_{i+1}:=A_i\cup(\cup_{y\in A_i,n\ge 0}f'(y)_n)$ and take $A_x=A_{f',x}:=\cup_{i\ge 0} A_i$ .  Since for the construction of $f'$ one only needs to take into account the $f'(y)$ for $y$ 'reachable' from $x$ , which all sit in an $\omega$ -branching tree of height $\omega$ , this base is countable. So one way to make sure $\tau$ doesn't realize new sequences is to demand that for any sequence $s$ and $x\in X$ such that for any $f'$ as above there is some tail of $s$ living in $A_{f',x}$ we must have $s\in f(x)$ . But is there a simpler way to put this? Clearly, for $s\in f(x)$ and any sequence $(k(n))_n$ of natural numbers converging to $\infty$ we need $(s_{k(n)})_n$ to be in $f(x)$ , also $f(x)$ must contain the constant $x$ -sequence and be closed under mergence of a finite number of sequences.  Still, the case is much more subtle than this.  Consider $X=\mathbf{R}$ and $f(u)$ containing all sequences $(u\pm v2^{-n})_n$ for some $v\in[1,2)$ plus their closure under the constructions just discussed.  This $f$ produces the Euclidean topology, but obviously doesn't contain all convergent sequences in the Euclidean space and it's really hard to see (for me) if there is some simple, abstract condition that $f$ fails to satisfy. One more condition I could think of, which is true whenever $\tau$ is first-countable, is that for $s\in f(x)$ and $(t^n)_n$ with $t^n\in f(s_n)$ there exists a sequence of natural numbers $(k(n))_n$ such that any $s'$ with $s'_n\in\{t^n_m\}_{m\ge k(n)}$ must be contained in $f(x)$ . But here take again $X=\mathbf{R}$ with $f(u)$ consisting of all sequences $s$ such that $s_n\in u\pm(v-w_n,v+w_n)2^{-n}$ for some $v\in[1,2)$ and $w\in\mathbf{R}^\omega_{>0}$ converging to $0$ , this satisfies the last condition but doesn't contain all convergent sequences in the Euclidean space, even if we take all the closures discussed before.",['general-topology']
2957504,Seemingly tricky dice question-probability that one event occurs before another event?,"The question is as follows: You roll two fair dice over and over.  Let $A$ be the event you see two even sums.  Let $B$ be the event you see a sum of $7$ four times.  What is the probability that event $A$ occurs before event $B$ ? I know that for mutually exclusive events with independent trials, the probability that event $E$ occurs before event $F$ is $$\frac{\mathbb{P}(E)}{\mathbb{P}(E) + \mathbb{P}(F)}.$$ I tried using this formula, but I ran into a problem.  I calculated $$\mathbb{P}(A)=\frac{1}{2}\cdot \frac{1}{2}=\frac{1}{4} \ \ \ \  \text{and} \ \ \ \  \mathbb{P}(B)=\left ( \frac{1}{6} \right )^4=\frac{1}{1296}.$$ However, I then realized that these probabilites are the events that two even sums occur $\textit{in a row},$ and similarly for my $\mathbb{P}(B).$ Does anyone have any suggestions on how to figure this out, or even if this formula is the one I should be using? Thanks in advance! Edit: I know there is a geometric distribution involved.","['dice', 'probability']"
2957537,Do we have integral test for double series?,"To determine convergence of $$\sum_{n=1}^\infty a_n,$$ one can use the integral test if $f(n)=a_n$ satisfies certain properties. Now, if I would like to determine convergence of double series $$\sum_{i=1}^\infty \sum_{j=1}^\infty a_{ij},$$ do we have some 'integral test' for it? I think one can evaluate $$\int\int f(i,j)\,di\,dj.$$","['integration', 'calculus', 'sequences-and-series', 'real-analysis']"
2957549,Find the coefficient of $x^{10}$,"We have been given the following function. $f(x)$ = $x$ + $x^2$ + $x^4$ + $x^8$ + $x^{16}$ + $x^{32}$ + ...upto infinite terms The question is as follows: What is the coefficient of $x^{10}$ in $f(f(x))$ ? I tried solving it myself and I found the answer too but the method of solving was too much time-consuming. I had solved it manually by only considering the first four terms of $f(f(x))$ . This method took me about 10 minutes. But the problem is that this question was asked in a competitive exam called JEE which requires solving the question in max. 3-4 minutes. So, I wanted to know if there was a faster method to solve this problem. Thanks in advance.",['functions']
2957571,Showing a certain limit or an upper bound of it goes to zero,"I have the following limit which I know is equal to zero (using Mathematica soft.), however, I can't show it analytically. $\lim_{L \rightarrow \infty} (\ln{L})^2\left[1-\left(1-e^{-\frac{1}{2}(\sqrt{L}-2)}\right)^L\right]$ I would appreciate any help.
Thanks","['limits', 'calculus']"
2957628,Finding value of infinite series limit,Finding value of $\displaystyle \lim_{n\rightarrow \infty}\frac{2-\underbrace{\sqrt{2+\sqrt{2+\sqrt{2+........+\sqrt{2}}}}}_{\bf{n\; times}}}{4^{-n}}$ Try: I am trying to convert it into $\cos$ ine series sum Let $$\displaystyle \sqrt{2+2\cos \theta } = 2\cos \frac{\theta}{2}$$ and $$\displaystyle \sqrt{2+\sqrt{2+2\cos \theta}} = 2\cos \frac{\theta}{4}$$ $$\displaystyle \sqrt{2+\sqrt{2+\sqrt{2+2\cos \theta}}} = 2\cos \frac{\theta}{8}$$ could some help me how i write $$\underbrace{\sqrt{2+\sqrt{2+\sqrt{2+........+\sqrt{2}}}}}_{\bf{n\; times}}$$ into cosine series form. thanks,['limits']
2957676,"Does any continuous function on $[0,1]$ have a best $n$th degree polynomial approximation in the supremum norm?","Recently I am stuck in a problem in approximation theory which actually is problem in functional analysis. $C[0,1]$ is a normed vector space with $||\cdot ||_{\infty}$ . $\Pi_n$ is a subspace which contains all the polynomials whose degree is no more than $n$ . It is easy to conclude that $C[0,1]$ is a banach space and $\Pi_n$ is a finite dimensional space. $\forall f\in C[0,1]$ , does there exist a unique $p\in\Pi_n$ such that $||f-p||_{\infty}=\inf_{g\in\Pi_n}||f-g||_{\infty}$ ? As far as I know, a element in normed vector space have a best approximation in a finite dimensional subspace. Moreover, if the subspace is strictly convex, the best approximation is unique. Therefore, $\forall f\in C[0,1]$ , there exist $p\in\Pi_n$ such that $||f-p||_{\infty}=\inf_{g\in\Pi_n}||f-g||_{\infty}$ . However, we cannnot guarantee that $p$ is unique because $\Pi_n$ is not strictly convex.(for example, $1,t\in \Pi_n,||\frac{1}{2} 1+\frac{1}{2} t||_{\infty}=1$ , not less than $1$ .) All I want to know is that whether or not the best approximation element is unique. If it is indeed unique, could we conclude that by functional analysis? Thanks for reading! Detailed comments or proofs are appreciated!","['approximation', 'functional-analysis', 'approximation-theory']"
2957686,Explain about the Correlation of Error Terms in Linear Regression Models,"I would like to ask for the interpretation, both mathematically and intuitively if possible, about the homoscedasticity of the variance of errors in linear regression models. If there is correlation among the error terms, then how it would affect the estimated standard errors of regression coefficients $\beta_i's$ , the confidence and prediction intervals (if we were to keep the assumption of homoscedasticity of errors and run the linear regression models) and how is it compared to the true standard errors $Var(\epsilon)$ (like underestimate or overestimate the true standard errors) and why? My question arises from the section about ""Correlation of Error Terms"" in the book ""Introduction to Statistical Learning"". It is as follows: An important assumption of the linear regression model is that the error terms, $\epsilon_1, \epsilon_2, ..., \epsilon_n$ , are uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that $\epsilon_i$ is positive provides little or no information about the sign of $\epsilon_{i+1}$ . The standard errors that are computed for the estimated regression coefficients or the fitted values are based on the assumption of uncorrelated error terms. If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors. As a result, confidence and prediction intervals will be narrower than they should be. For example, a 95 % confidence interval may in reality have a much lower probability than 0.95 of containing the true value of the parameter. In addition, p-values associated with the model will be lower than they should be; this could cause us to erroneously conclude that a parameter is statistically significant. In short, if the error terms are correlated, we may have an unwarranted sense of confidence in our model.
  As an extreme example, suppose we accidentally doubled our data, leading to observations and error terms identical in pairs. If we ignored this, our standard error calculations would be as if we had a sample of size $2n$ , when in fact we have only n samples. Our estimated parameters would be the same for the $2n$ samples as for the $n$ samples, but the confidence intervals would be narrower by a factor of $\sqrt2$ ! I hope my question is clear. Many thanks in advance for sharing your insights on the question!","['statistics', 'variance', 'linear-regression', 'standard-error', 'probability']"
2957690,Linearise shallow water wave equation to get decoupled equations,"I am trying to construct a set of decoupled system of equations from the following linear shallow water wave model which aims to capture the activity of a tsunami: \begin{align*}
h_t +(Hu)_x &= 0\\
u_t + (gh)_x &=0
\end{align*} Where $H$ is the height of the ocean; assumed to be constant, and $H+h$ is the total height after the tsunami. We also take $g = 10m/s^2$ This is my working so far: I wrote the system as a matrix equation: \begin{align*}
\begin{bmatrix} h\\u \end{bmatrix}_t + \begin{bmatrix} 0 & H\\ g& 0 \end{bmatrix}\begin{bmatrix} h\\u \end{bmatrix}_x = 0
\end{align*} I diagonalized $$A = \begin{bmatrix} 0 & H\\ g& 0 \end{bmatrix}$$ by $$
P = \begin{bmatrix} \sqrt{gH}/g & -\sqrt{gH}/g\\ 1& 1 \end{bmatrix}, \hspace{0.1cm} P^{-1} =\begin{bmatrix} \frac{g}{2\sqrt{gH}} & \frac{1}{2}\\ -\frac{g}{2\sqrt{gH}}& \frac{1}{2} \end{bmatrix}, \hspace{0.1cm} D =\begin{bmatrix} \sqrt{gH} & 0\\ 0& -\sqrt{gH} \end{bmatrix}
$$ Now I am not sure how to proceed, I have tried utilising this diagonalization to get some kind of decoupled system by substituting this diagonalization into the matrix equation, but cant get anywhere. Thanks","['ordinary-differential-equations', 'wave-equation', 'partial-differential-equations', 'mathematical-physics', 'fluid-dynamics']"
2957696,explicit Euler methods : is there a way to correct the gain of energy,"Is there a way to insert a correction term in order to counterbalance the gain of energy due to the explicit Euler method ? In particular, let's assume I know the total energy $E_0$ of my system at time $t=0$ (due to the initial conditions). And the energy is defined locally at time $t$ on a grid which is of total size $N^2$ : $e(i,j,t)$ . Would it make sense to correct at each iteration (time $t$ ): $e'(i,j,t)=e(i,j,t)-\frac{\sum_i e(i,j,t) -E_0}{N^2} $ ?","['numerical-methods', 'ordinary-differential-equations']"
2957706,Is it true that the number is divisible by $p$?,"Question: Let $a, b, c$ be positive integers and $p>3$ be a prime ( $ a$ isn't divisible by $p$ ).
  Consider a quadratic polynomial $P(x) = ax^2+bx+c$ , and assume that there exists $2 p-1$ consecutive positive integers: $$x+1, x+2, ..., x+2p-1$$ satisfying that $P(x+i)$ is a square number for every $i$ $(1 \leq i \leq 2p-1)$ . Is it true that the number $\Delta = b^2-4ac$ is divisible by $p$ ? I found out that if there exists $i$ in which $1 \leq i \leq p-1$ so that $P(x+i)$ is divisible by $p$ , then $\Delta$ is divisible by not only $p$ , but $p^2$ as well. If $P(x+i)$ is divisible by $p$ , then $p^2|P(x+i)$ and $p^2|P(x+i+p)$ , thus $$p^2|P(x+i+p)-P(x+i) \implies p^2|p(a(2x+2i+p)+b) \implies p|2a(x+i)+b$$ and since $4a\cdot P(x+i)=(2a(x+i)+b)^2-\Delta$ , so $p^2|\Delta$ . However I cannot find any conditions of $a, b, c, p$ so that there exists $i$ in which $1 \leq i \leq p-1$ and $p|P(x+i)$ . Is the question correct or there must be some conditions of $a, b, c, p$ for the question to to be true? (Sorry, English is my second language)","['number-theory', 'geometry', 'calculus', 'combinatorics', 'algebra-precalculus']"
2957718,What can be said about the roots of $acx^4 + b(a + c)x^3 + (a^2 + b^2 + c^2 )x^2 + b(a + c)x + ac$?,"Let a, b and c be real numbers. Then the fourth degree polynomial in $x$ , $acx^4 + b(a + c)x^3 + (a^2 + b^2 + c^2 )x^2 + b(a + c)x + ac$ (a) Has four complex (non-real) roots (b) Has either four real roots or four complex roots (c)Has two real roots and two complex roots (d) Has four real roots This question is from a book called 'Test of Mathematics at the $10+2$ Level' published by the Indian Statistical Institute We can see that the expression factorizes to $(ax^2+bx+c)(a+bx+cx^2)$ . If $\alpha,\beta$ are the roots of the first factor then $1/\alpha , 1/\beta$ are the roots of the second expression. And we know that complex roots occur in conjugation. So, we can easily understand that option (b) is a correct. But today, I want to solve this problem in some different way in which it does not require us to factorize the expression in a hit and trial way as I did. Even if we have to factorize, we must use a proper way to find the factors. Please help.","['algebra-precalculus', 'quadratics', 'polynomials']"
2957813,Which number can I erase?,"All positive integers greater than $2$ are written on a board. First we erase number $3$ and $5$ . With 4 positive integers $a,b,c,d$ satisfying $a+b=c+d$ , if $ab$ is erased, then $cd$ can be erased, otherwise $cd$ cannot be erased. For example, $3=3 \times 1$ , $3+1=4=2+2$ , then $2 \times 2 = 4$ is erased. a. What are the conditions of a number that can be erased ? b. If not only $3$ and $5$ , but every prime number is erased at the beginning, can all other numbers be erased as well? If not, what are the conditions of a number to be erased ? (Sorry for my last question, English is my second language)","['elementary-number-theory', 'combinatorics']"
2957844,Linear function ortogonal at each point is a cross product,"Let $f\colon \mathbb{R}^3 \to \mathbb{R}^3$ be a linear function satisfying $$\langle x, f(x) \rangle = 0$$ for all $x \in \mathbb{R}^3$ . I want to prove (and hopefully not disprove) that there exists a vector $F \in \mathbb{R}^3$ s.t. $$f(v) = F \times v$$ The problem is equivalent to $F = v\times f(v)$ being constant. I've tried showing it's derivative is zero but I haven't being lucky in this approach. I've always been mathematically skeptic as to why cross products appear anywhere at all in nature, and an affirmative answer to this question would quench my thirst for a mathematical explanation. It would be even more satisfying if it's a ""coordinate free"" proof, without going down to messy calculations with the partial derivatives of $f$ and so on..","['multivariable-calculus', 'calculus', 'linear-algebra', 'derivatives']"
2957857,"Is every locally Banach, Hausdorff space regular?","I am working on some infinite dimensional differential geometry. I have tried proving a somewhat weaker statement than the above by replacing locally Banach with locally metrizable. But after some research this is obviously wrong as the K-topology is a contradiction to such a statement, see A locally metrizable, Lindelöf Hausdorff space that is not metrizable . My goal is to use this to prove that second countable locally Banach, Hausdorff spaces are metrizable, which is equivalent to locally metrizable, Hausdorff and paracompactness. That is to prove that we need not worry about convention used to define Banach manifold. Does anyone have a reference for an article or such that treats these questions? I have looked into some books on Banach manifolds but without any luck. If it is not the case that every locally Banach, Hausdorff space is regular, then does anyone have a counterexample at hand? (If this is the case the article I am trying to use would be incorrect on this point).","['banach-spaces', 'separation-axioms', 'general-topology', 'differential-topology', 'differential-geometry']"
2957971,For which $a$ is this function increasing? $ f(x) = \left( \frac {a-2}{a-4}\right) ^{-x} $,"For which $a$ is this function increasing? $$  f(x) = \left( \frac {a-2}{a-4}\right) ^{-x} $$ So first I would rewrite this as: $$  f(x) = \left( \frac {a-4}{a-2}\right) ^{x} $$ I was thinking that in order for the function to be increasing the whole fraction has to be bigger than $1$ or smaller than $-1$ So I devided that into two conditions: $   \frac {a-4}{a-2}> 1 $ and $   \frac {a-4}{a-2} < -1 $ I solved both inequalities and the result should be:
for the first inequality: $(   -\infty, 2) $ fot the other one: $( 2, 3) $ ANd now for the final  result I should combine both so that would be $K =\left\{( -\infty, 2) U ( 2, 3) \right\} $ Is this corrrect? I have no idea how else I should find out .. But my intuition tells me that something is not correct .. Thanks for help",['functions']
2957994,Ordinary Differential Equations: Uniqueness but not existence?,"When working with ordinary differential equations, I understand that there are two parts  to the theorem on existence and uniqueness. While looking in several textbooks and online notes, I have found examples where existence of a solution is guaranteed, but uniqueness is not. I am wondering if it is possible to be able to prove the uniqueness part of the theorem, but not have the existence part be met? Or can we only look into uniqueness if we have already proven that a solution exists? Thank You!",['ordinary-differential-equations']
2958029,Finding the first integral of $\ddot{p}-\dot{p}/p+A/p-p^3=0$?,"I am reading a paper ( https://arxiv.org/abs/math/9907210 ), and the author first gives the following differential equation: $$\ddot{p}-\frac{\dot{p}^2}{p}+\frac{A}{p}-p^3=0$$ Later, they state that the first integral is given by $$\dot{p}^2=p^4+Kp^2+A$$ where $K$ is a real constant. I can't get this; here is my attempt. Multiply by $\dot{p}$ and integrate with respect to $s$ , $$\int \dot{p}\ddot{p} ds=\int (\frac{\dot{p}^2}{p}-\frac{A}{p}+p^3)\dot{p}ds$$ Left hand side is good, $\frac{1}{2}\dot{p}^2$ , but the right hand side is not: $$\int (\frac{\dot{p}^2}{p}-\frac{A}{p}+p^3)\dot{p}ds=\int\frac{\dot{p}^3}{p}ds-A\ln(p)+\frac{1}{4}p^4.$$ Maybe my naive understanding of ""first integral"" is incorrect. Can anyone spot my error?",['ordinary-differential-equations']
2958047,Why does this expansion of cosine work? $\cos(\theta) = \frac{e^{i\theta}}{2} + \frac{|e^{i\theta}|^2}{2e^{i\theta}}$,My friend showed me this expansion for cosine. I know it has something to do with Euler's identity $e^{i\pi}+1 = 0$ but I can't quite figure out how they worked this out. $$\cos(\theta) = \frac{e^{i\theta}}{2} + \frac{|e^{i\theta}|^2}{2e^{i\theta}}$$,['trigonometry']
2958059,Expected value of measurement error for the maximum,"Suppose that we have $n$ independently and identically distributed variables $X_1, \cdots, X_n$ which we observe with zero mean measurement error. More precisely, we observe $X^*_i = X_i + U_i$ where the errors $U_1, \cdots, U_n$ are independently and identically distributed, $E[U_i] = 0$ and every $U_i$ is independent of every $X_i$ . I need to show that $$E[U_i \mid X^*_i \geq X^*_j \hspace{0.1cm}\forall j] \geq 0.$$ Intuitively, this is obvious: one reason why $X_i + U_i$ could be maximal is that $U_i$ is high, which in turn suggests that the expected value of $U_i$ is high given that $X_i + U_i$ is maximal. However, I am struggling to show this formally and would be grateful for some help. Here is my 'progress' so far (I may have over-complicated matters, if so please point me towards a better way forward): First, I note that by Bayes' theorem, we can write the density of $U$ given that $X + U$ takes some particular value $c$ : \begin{align*}
f_{U \mid U+X=c}(u) &= \frac{f_{U+X}(c \mid U=u)f_U(u)}{f_{U+X}(c)} = \frac{f_{X}(c-u)f_U(u)}{f_{U+X}(c)}\\
&= \frac{f_X(c-u)f_U(u)}{\displaystyle \int_{-\infty}^{\infty}f_U(k)f_X(c-k)\,\mathrm{d}k}.
\end{align*} From this, I get the expected value of $U$ given that $X+U=c$ : $$E[U \mid U+X=c]= \int_{\underline{u}}^{\bar{u}}f_{U \mid U+X=c}(u)u\,\mathrm{d}u = \int_{\underline{u}}^{\bar{u}}\frac{f_X(c-u)f_U(u)}{\displaystyle\int_{-\infty}^{\infty}f_U(k)f_X(c-k)\,\mathrm{d}k}(u)u\,\mathrm{d}u,$$ where $\bar{u}$ and $\underline{u}$ bound the support of $U$ (I will assume a finite support, but presumably the argument goes through without this assumption?) This tells us the expected value of $U_i$ given that $X_i + U_i = c$ . However, we wanted to know the expected value of $U_i$ given that $X_i + U_i$ is maximal. Fortunately, we know the distribution of $X_i + U_i$ given that it is maximal, i.e. the distribution of the 'highest order statistic' of $X_i + U_i$ : $$f_{X + U \mid X + U \text{ is maximal}}(c) = \left(\int_{-\infty}^{\infty}f_U(k)f_X(c-k)\,\mathrm{d}k\right)^n.$$ By the law of iterated expectations, we thus obtain: \begin{align*}
&\mathrel{\phantom{=}}{} E[U_i \mid X^*_i \geq X^*_j\ \forall j]\\
&= \int{E[U \mid U+X=c]}f_{X+U \mid X + U \text{ is maximal}}(c)\,\mathrm{d}c\\
&= \int\Biggl(\int_{\underline{u}}^{\bar{u}}\frac{f_X(c-u)f_U(u)}{\displaystyle\int_{-\infty}^{\infty}f_U(k)f_X(c-k)\,\mathrm{d}k}u\,\mathrm{d}u\Biggr)\left(\int_{-\infty}^{\infty}f_U(k)f_X(c-k)\,\mathrm{d}k\right)^n \,\mathrm{d}c
\end{align*} It thus seems that I have found the expression I am after. However, when looking at this rather complicated expression, I see no way to show that it is non-negative (the whole point of the exercise!) If you can see a way, or can think of a simpler approach than the one I have adopted, please do let me know. Thanks again in advance!","['conditional-probability', 'expected-value', 'probability']"
2958061,Part of the proof of the first optimality condition. Showing that $f'(x*)=0$ for a local minimum $x*$.,"Let $x^*$ be a local minimum of a differentiable function $f(x)$ , i.e. there exists $r>0$ such that for all $y \in B_n(x^*,r)$ we have $f(y)\ge f(x^*)$ . Since $f$ is differentiable we have $$f(y)=f(x^*)+\langle f'(x^*),y-x^* \rangle + o(\parallel y-x^* \parallel) \ge f(x^*).$$ Thus, for all $\parallel s\parallel =1,$ we have $\langle f'(x^*),s\rangle =0$ . I don't understand how we get this final statement. We basically have $\langle f'(x^*),y-x^* \rangle + o(\parallel y-x^* \parallel) \ge 0$ , but how does this give for all $\parallel s \parallel =1,$ we have $\langle f'(x^*),s\rangle =0$ ?","['calculus', 'derivatives', 'analysis']"
2958113,How to solve an ODE of this form $\dot{P}(t)=A^TP(t)+P(t)A$?,"Background If $\dot{x}(t)=A \,x(t)$ , then we know the solution is $x(t)=e^{At}x(0)$ . Question Now let $\dot{P}(t)=A^TP(t)+P(t)A$ , what is $P(t)$ ? Attempt If we take $P(t)$ as common factor (I'm not sure I'm doing this correctly though) ,then we have: $$
\dot{P}(t)=A^TP(t)+P(t)A=(A^T+A)P(t)
$$ and using the same rationale in the background section, we have $$
P(t)=e^{(A^T+A) t}P(0)=e^{A^T t}P(0)e^{At}
$$ Is this a correct solution? Note I know the answer is $P(t)=e^{A^T t}P(0)e^{At}$ but I'm not sure how to find it.","['lyapunov-functions', 'analysis', 'ordinary-differential-equations']"
2958115,det-1 is irreducible,"I'm interesting about $V(det-1) \subset \mathbb{A}^{n^2}$ with the determinant seen as a polynomial. I know that $det$ is irreducible. But I want to show that $det-1$ is irreducible. In a paper, it says that if $det-1=fg$ is a non trivial factorization so the top homogeneous components of $f$ and $g$ gives a non  trivial factorization for $det$ . I don't understand why. Cordialy,
doeup","['affine-varieties', 'algebraic-geometry']"
2958135,Why do many textbooks on Bayes' Theorem include the frequency of the disease in examples on the reliability of medical tests?,"A ""standard"" example of Bayes Theorem goes something like the following: In any given year, 1% of the population will get disease X . A particular test will detect the disease in 90% of individuals who have the disease but has a 5% false positive rate. If you have a family history of X , your chances of getting the disease are 10% higher than they would have been otherwise. Virtually all explanations I've seen of Bayes' Theorem will include all of those facts in their formulation of the probability. It makes perfect sense to me to account for patient-specific factors like family history, and it also makes perfect sense to me to include information on the overall reliability of the test. I'm struggling to understand the relevance of the fact that 1% of the population will get disease X , though. In particular, that fact is presumably true for all patients who receive the test; that being the case, wouldn't Bayes' Theorem imply that the actual probability of a false positive is much higher than 5% (and that one of the numbers is therefore wrong)? Alternatively, why doesn't the 5% figure already account for that fact? Given that the 5% figure was presumably calculated directly from the data, wouldn't Bayes' Theorem effectively be contradicting the data in this case?","['statistics', 'conditional-probability', 'bayesian', 'bayes-theorem', 'probability']"
2958189,How can I name a geometrical entity similar to a plane but with finite length and width?,"I was considering just using the name rectangle for representing the set of points contained in a 3D plane for a given rectangular area. I would like to know whether there's a more appropriate name. If I understood correctly, by definition, that would not be a finite plane. UPDATE I know the difference between disk and circle: the disk's set of points includes points inside the circular region. Is there something equivalent for rectangles?",['geometry']
2958198,Distributing $15$ candies to $5$ children with restriction,"Suppose you want to distribute $15$ candies to $5$ different children. (a) In how many ways can this be done if no kid receives more than $6$ candies? (b) In how many ways can this be done if each child ends up with a
different number of candies? We already determined that the number of ways of distributing the candies to the $5$ children such that each child gets at least one piece is $1,001$ ways. How do we take care of the restriction of each child receiving no more than $6$ ? Can we take the complement and subtract the number of ways a child receives $7-15$ pieces? Or would this be a long, unnecessary attempt? My attempt: Consider the complement where one kid receives at least $7$ candies. Step $1$ : Choose the child to receive $7$ candies and give him/her the $7$ candies: $5$ choices Step $2$ : Distribute the remaining $15-7=8$ candies to the $5$ children. There are $\binom{8+4}{4}=\binom{12}{4}=495$ ways to do this. So there are $5\cdot 495=2,475$ ways to distribute the candies such that one child receives at least $7$ candies. There are $\binom{15+4}{4}=\binom{19}{4}=3,876$ ways to distribute the candies with no restriction. So there are $3,876-2,475=1,401$ ways to distribute the candies such that no child receives more than $6$ candies.",['combinatorics']
2958231,Eigenvalues of A are also eigenvalues of T,"Let $V$ be the set of all $n\times n$ matrices over a field $F$ . Let $A$ be a fixed element of $V$ . Define a linear operator $T$ on $V$ by $T(B)=AB$ . I am trying to show that if $\lambda$ is an eigenvalue of $A$ , then $\lambda$ is also an eigenvalue of $T$ . So suppose $Av=\lambda v$ for some $v\neq 0$ in $V$ and $\lambda\in F$ . So I'd like to prove the existence of a matrix $B$ such that $T(B)=AB=\lambda A$ , or equivalently, show that $T-\lambda I_V$ is not invertible (or injective or surjective). But I am not sure how to proceed from here. What can I do?","['matrices', 'linear-algebra', 'linear-transformations', 'eigenvalues-eigenvectors']"
2958240,How to prove that $\lim_{n\to\infty}\int_n^\infty f(x) dx = 0$ when $f$ is integrable.,"Suppose $f$ is a nonnegative integrable function. (Here, the integrals are Lebesgue integrals.) Is there an elementary way to prove that $$
\lim_{n\to\infty}\int_n^\infty f(x) dx = 0
$$ without using the dominated convergence theorem? Here is how I think you can prove it with the dominated convergence theorem:
Since $f(x)\chi_{[n, \infty)}(x) \to 0$ and $\lvert f(x)\chi_{[n, \infty)}(x) \rvert \leq f(x)$ , by the dominated convergence theorem, we have that $$
\lim_{n\to\infty}\int f(x) \chi_{[n, \infty)} = \lim_{n\to\infty}\int_n^\infty f(x) dx = 0.
$$ Perhaps there is a way to prove it with the monotone convergence theorem?",['measure-theory']
2958264,Constrained equilibrium for inflated sheet,"I am trying to figure out the shape that a disc of uniformly elastic material makes if you fix its circumference firmly to a flat table and slowly fill the interior with air. So far, I've determined that the answer should be some kind of constrained optimality problem, where there's an equilibrium between forces due to air pressure and elasticity, and the boundary is fixed. And based on symmetry, the solution ought to be rotationally symmetric. But I'm unsure how to model elastic forces or how to set up the problem more formally or if there's a quick insight to see the solution. Maybe I should set it up as a variational calculus problem, where the candidate shape of the inflated material determines the distribution of elastic forces? Any advice is appreciated. Edit: Maybe I can solve this equation in 2D instead of 3D, with a one-dimensional elastic string fixed at its endpoints and filled with air pressure? I imagine the solution to the 3D problem is something like a surface of revolution of the 2D solution. Is this problem equivalent to finding the shape of a meniscus between two fluids? Is the solution a catenary curve because it's like a hanging chain with air pressure replacing the force of gravity?","['physics', 'lagrange-multiplier', 'ordinary-differential-equations', 'calculus-of-variations']"
2958271,Gaussian curvature of the pseudosphere,"I am asked to show that the pseudosphere has Gaussian Curvature $-1$ at all points. So I parametrized the tractrix as: \begin{equation}
\alpha(t) = (\sin{t},\cos{t}+\log{\tan{\frac{t}{2}}})
\end{equation} So the surface of revolution would be: \begin{equation}
x(\theta,t)=(\sin{t}\cos{\theta},\sin{t}\sin{\theta},\cos{t}+\log{\tan{\frac{t}{2}}})
\end{equation} By calculating the first and second fundamental forms we have that $F=f=0$ and $E=(\sin{t})^2,G=(\cot{t})^2,e=(\cos{t})^2$ and $g=-(\cot{t})^2$ . So the Gaussian curvature is: \begin{equation}
K = \frac{eg}{EG} = -(\cot{t})^2
\end{equation} What am I doing wrong?",['differential-geometry']
2958276,Combinatorics question involving distributing identical candies to different children,"Suppose you want to distribute 15 identical candies to 5 different children. a) In how many ways can this be done if every kid receives at least one piece of candy? b) In how many ways can this be done if child A is to be given exactly 4 candies? c) In how many ways can this be done if child C and child D receive exactly 7 candies together? d) In how many ways can this be done if no kid receives more than 6 candies? e) In how many ways can this be done if every kid ends up with a different number of candies? (i.e. A = 2, B = 3, C = 4, D = 1, E = 5) \
\ My attempts: a) First I gave each of the kids one piece of candy so I was left with 10 dots and 4 slashes (separating the distinct children). So I ended up with ${10+4\choose 4}$ = ${14\choose 4}$ ways. b) Again I gave child A four candies and so I was left with 11 candies. In addition, I took child A way from consideration to ensure he received exactly four candies and thus I was left with 11 dots and 3 slashes, separating the 4 remaining distinct children. So I ended up with ${11+3\choose 3} = {14\choose 3}$ ways. c) Again I gave both child C and D 7 candies collectively and took them away from consideration. I was left with 8 candies and 2 slashes, separating the 3 distinct remaining children. So I ended up with ${8+2\choose 2} = {10\choose 2}$ ways. I am struggling with d and e quite a bit. For d I wanted to assign 7 candies to a child and them take them away from consideration, but if I were to do this another child could receive >6 candies since there would be 8 remaining candies with no restrictions to which of the four children would receive a specific amount of candies. For e I would assume we would play around assigning different children different amounts of candies and then work with the remaining amount of candies to distribute among the children but I'm really not even sure where to start with this one... Would someone mind checking my responses for a, b and c and then assisting with d and e?","['combinations', 'combinatorics']"
2958395,"If $M$ is symmetric posdef, then all diagonal band matrices derived from $M$ are also posdef?","Let $M$ be a positive definite and symmetric matrix: $$M = \left(\begin{array}{cccc}
  a_{11} & a_{12} & \cdots & a_{1 n}\\
  a_{21} & a_{22} & \cdots & a_{2 n}\\
  \vdots & \vdots & \ddots & \vdots\\
  a_{n 1} & a_{n 2} & \cdots & a_{n n}
\end{array}\right)$$ where $a_{ij} = a_{ji}$ . Consider the band matrices $B^{(d)}$ with components $b_{i j}^{(d)} = a_{i j}$ if $|i-j| < d$ and $b_{i j}^{(d)}=0$ otherwise. Thus $B^{(d)}$ is a band matrix of ""width"" $d$ . For example $$B^{(2)} = \left(\begin{array}{ccccccc}
  a_{11} & a_{12} & 0 & 0 & \cdots & 0 & 0\\
  a_{21} & a_{22} & a_{32} & 0 & \cdots & 0 & 0\\
  0 & a_{32} & a_{33} & a_{43} & \cdots & 0 & 0\\
  0 & 0 & a_{34} & a_{44} & \cdots & 0 & 0\\
  \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
  0 & 0 & 0 & 0 & \cdots & a_{n - 1, n - 1} & a_{n - 1, n}\\
  0 & 0 & 0 & 0 & \cdots & a_{n, n - 1} & a_{n n}
\end{array}\right)$$ Is $B^{(d)}$ positive definite? Motivation: I am trying to construct a preconditioner for an optimization problem. Computing the full Hessian is computationally expensive, but I can compute a few diagonals without problems. I want to use a band approximation like this as a preconditioner, but I need to be sure it will be posdef.","['matrices', 'linear-algebra', 'positive-definite']"
2958398,radially unbounded functions,"Is the following function radially unbounded or not? $$V(x) = \frac{x_{1}^2}{1 + x_{1}^2} + x_{2}^2$$ I know that if $x_{2} \to \infty$ in which case $||x|| \to \infty$ and $V(x) \to \infty$ but if $x_{1} \to \infty$ which makes $||x|| \to \infty$ but then $V(x)$ does not go to $\infty$ . What I would like to know is that is a function radially unbounded if only one or more of its variables makes it go to infinity or should it be true for every variable that that function depends on? Edit: I am not a student of math. In a engineering subject, this popped up. I have seen other answers but can't get my head around it. So can you kindly answer the question in simpler words? instead of down voting the question.","['lyapunov-functions', 'control-theory', 'real-analysis', 'functional-analysis', 'infinity']"
2958404,Seating students so that their numbers are different from both of the numbers on their chairs,"I read this question in a book without answer as an exercise. I read Mathematics by myself. This is the question: In a class, there are $10$ chairs for sitting. There are two numbers on each chair: $(1,2) (2,3) (3,4) (4,5) (5,6) (6,7) (7,8) (8,9) (9,10) (10,1)$ There are $10$ students with numbers from $1$ to $10$ . In how many ways we can sit the students so that their numbers are different from the numbers on the chair in which they sit? (the answer of question at most $6$ digits) Is it possible to help me?","['combinatorics', 'discrete-mathematics']"
2958405,A conjecture about the intersections of three hyperboles related to any triangle,"Given any triangle $\triangle ABC$ , we build the hyperbole with foci in $A$ and $B$ and passing through $C$ . Similarly, we can build other two hyperboles, one with foci in $A$ and $C$ and passing through $B$ (red), and one with foci in $B$ and $C$ and passing through $A$ (green). The first part of my conjecture is that the three hyperboles always intersect in two points $D$ and $E$ . Moreover, the ellipse with foci in these two points $D$ and $E$ , and passing through one of the three vertices of the triangle $\triangle ABC$ , pass also through the other two vertices. These are probably obvious results. However, is there an elementary proof for these conjectures? Thanks for your help! Sorry in case this is too trivial. EDIT: You might be interested also in this other post .","['triangles', 'conic-sections', 'geometry', 'geometric-construction']"
2958414,Question related to the trace and the commutator of matrices,"Let $K$ be any field and $n\in \mathbb N$ . For every $A\in M_n(K)$ , define a linear form $\lambda_A: M_n(K) \rightarrow K$ by sending the matrix $M$ to $\lambda_A(M):= \operatorname{Tr}(AM)$ . The map $\lambda: M_n(K) \rightarrow M_n(K)^*$ defined this way is an isomorphism, as can be seen from injectivity and equality of dimensions (which are finite). The question is the following. Let $A, B\in M_n(K)$ be two matrices such that for every $M\in M_n(K)$ that commutes with $A$ (that is, such that $AM=MA$ ), we have $\lambda_B(M)=0$ . Prove that $B=AC-CA$ for some matrix $C\in M_n(K)$ . The reciprocal being clearly true, the statement above in fact exactly characterizes all the matrices that can be written as $AC-CA$ for some matrix $C\in M_n(K)$ . As a preliminary question, I managed to show that $$\operatorname{Ker}(\operatorname{Tr})=\operatorname{Span}\{AB-BA\,|\,A,B\in M_n(K)\}$$ The inclusion of the right-hand space - which I call $E$ - inside the kernel is indeed clear. However, it may not be clear that the right-hand space is a hyperplane. If we assumed it were not, we may find another hyperplane $H$ such that $E\subset H$ , and $H\not = \operatorname{Ker}(\operatorname{Tr})$ . The hyperplane $H$ is the kernel of some linear form which is linearly independant on $\operatorname{Tr}$ . It can be uniquely written as $\lambda_X$ for some matrix $X\in M_n(K)$ which is not a multiple of the identity matrix $I_n$ . However, the condition that $\lambda_X$ vanishes on $E$ implies that $X$ is a multiple of the identity, so we get a contradiction. Hence, $E$ is a hyperplane and so by equality of dimensions, we must have the equality above (note that the kernel of $\operatorname{Tr}$ is never trivial). I have been trying to solve the problem using this result, however I can't progress further. Taking $M=I_n$ , we may express $B$ as $XY-YX$ for some matrices $X,Y \in M_n(K)$ . Considering $A$ , we may also express $BA$ as a commutator. But the problem is that we have no idea what these commutators would look like. In particular, I can't think of any way to prove that one of the matrices could be $A$ . Could you please give me any hint/advices that could lead me to the solution?","['matrices', 'linear-algebra']"
2958431,Writing cycles of a graph as a linear combination of fundamental cycles,"It is folklore that the fundamental cycles (corresponding to aparticular spanning tree) of a graph constitute a basis for its cycle space, while the proof uses the linear indepence of fundamental cycles as well as some orthogonality arguements (see chapter 1 of Diestel’s Text book for more context). But I could not find any constructive way for writing an arbitrary cycle as a linear combination of fundamental cycles in the litrature, and my personal effort also did not get to anywhere. Is there any such way of writing the precise linear combination, preferably a purely combinatorial one? All answers and comments are appreciated.","['graph-theory', 'algebraic-graph-theory', 'linear-algebra']"
2958452,Convergence in exponential moments implies convergence in distribution,"We say a sequence of random variables ${x_n}$ converge in exponential moments to $X$ if $\mathbb E(e^{zX_n})\to \mathbb E(e^{zX})$ uniformly in some neighborhood of $0\in \mathbb C$ . I want to show the convergence in exponential moments implies the convergence in distribution. To this end, it suffices to show that for any $t\in \mathbb R$ , we have $\mathbb E(e^{itX_n})\to \mathbb E(e^{itX_0})$ . The difficult part is that $U$ may not necessarily contain the whole imaginary axis (we only have the convergence on a neighborhood of $0$ ). Thanks for help!","['measure-theory', 'probability']"
2958475,Differential invariants of ODEs,"I have been reading 'Symmetry and Integration Methods for Differential Equations' by Bluman and Anco. I'm trying to make sense of differential invariants and ODEs... It is confusing. There is an ODE in a general form, for example $$F(x,y,y',y'',...,y^{(n)})=0$$ where $x$ is the independent variable, and $y$ is the dependent variable. Say, the ODE is invariant under the one-parameter Lie group $X_1$ . Say we know the prolongation $$X_1^{(n)}$$ and the ODE admits the group, i.e. is invariant w/respect to $X_1^{(n)}$ If I do $$X_1^{(n)}I=0$$ then $I$ is a ""differential invariant"" of the Lie group, right? What does it mean that $I$ is a ""differential invariant""? Does it mean it doesn't change under the transformation $X_1^{(n)}$ ? Or does it mean $I$ doesn't change under the transformations $X_1$ (without the prolongation)? It seems confusing... What are the properties of $I$ ?","['invariant-theory', 'lie-algebras', 'lie-groups', 'ordinary-differential-equations']"
2958501,$x=\sqrt[3]{\sqrt{5}+2}+\sqrt[3]{\sqrt{5}-2}$ is rational or irrational?,"The number $x$ defined below is rational or irrational? $$x=\sqrt[3]{\sqrt{5}+2}+\sqrt[3]{\sqrt{5}-2}$$ From: IMO 1973 - Longlist My attempt (my real question is at the end): the identity $a^3+b^3+c^3-3abc=(a+b+c)(a^2+b^2+c^3-ab-ac-bc)$ when $a+b+c=0$ , leads to $$a^3+b^3+c^3=3abc \tag{1}$$ Now considering $$a=\sqrt[3]{\sqrt{5}+2},b=\sqrt[3]{\sqrt{5}-2},c=-x$$ from (1) it is true that $$x^3-3x-2\sqrt{5}=0 \tag{2}$$ That is the number $x$ is a root from (2). Note: By trial and error I've found that answer is $x=\sqrt{5}$ (the other 2 roots are complex), that is irrational. But my question is more subtle. Question : Can I conclude just inspecting (2), judging by the coefficient $2\sqrt{5}$ , that $x$ is irrational, without actually solving the equation? In a math contest that might be helpful, if possible, as it would avoid extra steps.","['contest-math', 'algebra-precalculus']"
2958502,Ordered pair - why so complicated?,"In wikipedia is written that Kuratowski definition of ordered pair is now-accepted (I use down-right index ""K"" to mark Kuratowski formula): $p_K = (a,b)_K := \{ \{a\}, \{a,b\} \}$ My question is why people not use below simpler definition instead: $p = (a,b) := \{ \{a\}, \{b, \varnothing \} \}$ ? UPDATE After discussion on comments we get following Advantages of $(a,b)$ : for paris $(a,\varnothing)$ the result of $(a,\varnothing) = \{ \{a\}, \{  \varnothing \} \}$ is simpler than $(a,\varnothing)_K = \{ \{a\}, \{ a, \varnothing \} \}$ (we don't need to duplicate $a$ ) for case when $a=b$ formula $(a,a)=\{ \{a\}, \{a,\varnothing \} \}$ save property that we have two elements(pairs) in set, which is loose by formula $(a,a)_K=\{\{a\}\}$ Disatvantages of $(a,b)$ : for case $(\{\varnothing\},\varnothing) = \{ \{\{\varnothing\}\}, \{ \varnothing \}  \}$ we loose property that when $a\neq b$ the first element of pair  cardinality is $=1$ and second element cardinality is $=2$ which is saved for Kuratowski formula: $(\{\varnothing\},\varnothing)_K = \{ \{\{\varnothing\}\}, \{ \{\varnothing\}, \varnothing \}  \}$ . In this case both elements has cardinality $=1$ . for case $a=b$ formula $(a,a)_K=\{\{a\}\}$ is simpler than $(a,a)=\{ \{a\}, \{a,\varnothing \} \}$ for Kuratowski formula is easy to extract pair elements using union/intersection (which is not possible/easy for my formulat): $\pi_1( p_K ) = \bigcap (a,b)_K = \{a\}\cap \{a,b\} = \{a\}$ $\pi_2( p_K ) = \bigcup (a,b)_K = \{a\}\cup \{a,b\} = \{a, b\}$ I also realized that my definition is similar (but not the same) to Winner's definition",['elementary-set-theory']
2958541,Why the sheaf of abelian groups so fundamental?,"So in texts in algebraic geometry and sheaf cohomology I'm seeing sheaves of abelian groups most commonly mentioned. I'm wondering what is the reason for this, why not sheaves of commutative rings, or sets? Why is the abelian group so fundamental as the sheaf attachment? Thank you for your help.","['abelian-groups', 'algebraic-geometry', 'sheaf-cohomology']"
2958550,Law of large numbers and theoretical probability,"I didn't exactly know how to phrase the title of this question so a little more information.. I was conducting a small experiment with a class of secondary-school students to demonstrate the law of large numbers. Students were recording the result of independent dice rolls. The theoretical probability for rolling any value on a dice (1 to 6) is of course ${1\over6}$ or ~16.67%. I wanted to demonstrate that a small number of trials will yield results that may differ substantially from the theoretical probability but as we increase the number of trials the observed probability will converge towards the expected probability. Which I showed by collating all the students trials together. The following is a table with the collated results from independent dice rolls. with 1146 trials the observed probability is within about 10% of the theoretical probability (The most being P(1) which is 9.42% away. Is there a formula that determines how many trials must be done in order for the observed probability to be within a certain percentage of the expected probability? For example, how many trials are needed to be confident the observed probability will be within 1% of the expected probability? Is there a field of study for this? If so what is it called? The reason I want to know is because I want to talk about randomness and bias with these students. Say a dice used favours a specific number but only slightly which in an experiment with a small amount of trials may not be evident.","['law-of-large-numbers', 'probability-theory']"
2958554,Induction: How to prove that $ab^n+cn+d$ is divisible by $m$. [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question If $a+d$ , $(b-1)c$ , $ab-a+c$ are divisible by $m$ , prove that $ab^n+cn+d$ is also divisible by $m$ . I want to prove this by induction. For proving $ab^{k+1}+c(k+1)+d$ is divisible by $m$ , i want to prove that $ab^k(b-1)+c$ is divisible by $m$ and then add it to $ab^{k}+ck+d$ . Any idea how to prove $ab^k(b-1)+c$ is divisible by $m$ ? Or is there a better way to solve the problem? Thanks in advance.","['induction', 'discrete-mathematics']"
2958596,Evaluate $\lim_{x\to0}\frac{\sqrt{a^2+x}-|a|}{\sin^a2x\ln\cos x}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have just learned limits and series by myself, but I'm stuck with this limit: $$\lim_{x\to0}\frac{\sqrt{a^2+x}-|a|}{\sin^a2x\ln\cos x}$$ I would like to evaluate that limit with $a\in\mathbb R$ . I would also like to understand in detail the steps involved in order to solve that. Thanks.","['limits', 'calculus', 'real-analysis']"
2958600,Is there a non-trivial definite integral that values to $\frac{e}{\pi}$?,"I know that both $$\int_{-\infty}^\infty \frac{\cos(x)}{x^2+1} \, \text{dx} = \frac{\pi}{e}$$ and $$\int_{-\infty}^\infty \frac{x\sin(x)}{x^2+1} = \frac{\pi}{e}$$ But what about $\frac{e}{\pi}$ ? Is there a non-trivial definite integral which evaluates to that, but still where the integrand is composed of elementary functions like the two above? The only constraint on the integrand is that it is an elementary function . It does not need to have an elementary antiderivative. Also, the value of the integral should not be obvious at first glance. The integrals mentioned above are certainly not, which makes the answer so beautiful in my opinion. By trivial I mean integrals like $\int_{-\infty}^1 \frac{e^x}{\pi} \, \text{d}x = \frac{e}{\pi}$ , which quite obviously converges to the desired value. Edit: I have been asked to clarify my question as it is too broad. The two integrals mentioned above are, in my opinion, beautiful because they both converges to a fraction containing two such fundamental constants, even though it is not obvious at all that they should at first glance. Then, out of curiosity, I wondered if there are similar integrals which converges to $\frac{e}{\pi}$ . The answer @aleden has written is certainly interesting, but some of the magic disappears when both $\pi$ and $e$ are in the integrand. I have added some more information in the paragraphs above. I hope this cleared my question a bit, but it is broad by its nature.","['integration', 'constants', 'definite-integrals']"
2958612,Is my solution correct on this Groups/Rings,"So basically, I'm still learning about Groups/Rings and I was wondering if my solution for this exercise is correct. Also how can I solve the b part of the exercise (since i'm not sure). Exercise: On a set R, we have a defined $*$ operation as follows: for any $x,y \in R$ , we have $x*y = ax+by+c$ , for some $a,b,c \in R$ . a)Find a,b,c such that $(R,*)$ is an abelian group b)Find $c$ such that $(R, *, \times)$ is a ring, where ' $\times$ ' is normal multiplication over R. My Solution: a) To prove that it's abelian group. I created the following equation $$x*y = y*x \implies ax+by+c = ay+bx+c \implies ax+by = ay+bx \implies ax - ay = bx - by 
\implies a(x-y) = b(x-y) \implies a = b$$ Therefore in order for $(R, *)$ to be an abelian group $a$ must be equal to $b$ . 
Is this correct? Also for the b part this is my thinking (haven't really solved it) b) We have already proved that it's an abelian group if $a = b$ therefore now we need to prove closure with ' $\times$ ', association with ' $\times$ ' and distributive with ' $\times$ '.
Is my thinking correct if yes how would I finish this solution or is there a different approach to this? Thank you very much!","['ring-theory', 'group-theory', 'abstract-algebra']"
2958627,Determine the number of point $z \in \mathbb{C}$ such that $(2z+i\overline{z})^3=27i$,"I've just learned complex numbers in Mathematical Analysis 1, and I'm stuck in the following problem: I would like to determine the number of point $z \in \mathbb{C}$ such that $(2z+i\overline{z})^3=27i$ , and solve the following system of equations: $\begin{cases}\begin{matrix} (2z+i\overline{z})^3=27i \\ Re(z)\geq Im(z) \end{matrix}\end{cases}$ . Can someone help me explaining in detail the steps? Thank you very much!","['complex-analysis', 'complex-numbers', 'analysis', 'real-analysis']"
2958643,Is $\frac{\left(1+\sqrt{4n^2+1}\right)^n+\left(1-\sqrt{4n^2+1}\right)^n}{2^n}$ always an integer?,"Let $$a_n = \frac{\left(1+\sqrt{4n^2+1}\right)^n+\left(1-\sqrt{4n^2+1}\right)^n}{2^n}=2^{1-n} \sum _{k=0}^{\lfloor n/2\rfloor} \binom{n}{2 k} \left(4 n^2+1\right)^k,$$ then we have $$a_1=1,\quad a_2=9,\quad a_3=28,\quad a_4=577,\quad a_5=3251,\quad a_6=105193,\quad...$$ How can we prove that $a_n$ is an integer for all positive integer $n$ ?","['number-theory', 'sequences-and-series']"
2958649,How many numerical multiplications for the product of three matrices?,"For example you have the matrices $A, (5 \times 8), B, (8 \times 4), C (4 \times 10). $ The question wants you to find the number of multiplications if you were to multiply these matrices like $(A\times B)\times C$ . The answer is $5 \times 4 \times 8 + 5 \times 10 \times 4 = 160 + 200 = 360$ multiplications.","['matrices', 'discrete-mathematics']"
2958656,Bound on $\sum\limits_{n=0}^{x}{\sin{\sqrt{n}}}$,"Using Desmos and Mathematica, I was able to find a function $g(x)$ that seemingly estimated the function $$f(x)=\sum_{n=0}^{x}{\sin{\sqrt{n}}}$$ I found that $${g(x)=2\sqrt{x}*\sin{\left({{\sqrt{4x+{\pi}^2}-\pi}\over{2}}\right)}}\approx f(x)$$ Furthermore, I found that the difference between these two functions, or $|{f(x)-g(x)}|$ , never seemed to exceed some constant $C\approx 0.464568$ Is there some way to prove the conjecture below? $g(x)<f(x)<(g(x)+C)$ for all $x>14$ Or perhaps a weaker version considering I have no good definition for $C$ : $g(x)<f(x)<(g(x)+{1\over2})$ for all $x>14$ . Further questions: Is there a closed form for $C$ ? Is there a closed form for $f(x)$ ?","['sequences-and-series', 'upper-lower-bounds', 'real-analysis']"
2958675,$X(\omega)=\sum_{n=1}^{\infty} X_n(\omega)2^{-n}$ is a Uniform Random Variable?,"I am working on a problem on the infinite coin-tossing space and I'm having trouble making any meaningful progress. Let $(\Omega, \mathcal F, P)$ , where $\Omega=\{0,1\}^{\mathbb N}$ , $F$ is the $\sigma$ -field generated by the finite-dimensional sets, and $P$ is the coin(a fair coin) tossing probability. Finite dimensional sets are defined as sets $A=\{\omega\in\Omega\lvert(\omega_1,\omega_2,...,\omega_n)\in B\}$ for some $n\in\mathbb N$ , $B\subset\{0,1\}^n$ . Define $X_n(\omega)=\omega_n \in\{0,1\}$ is the
result of the nth toss and define $X(\omega)=\sum_{n=1}^{\infty} X_n(\omega)2^{-n}$ . Show that the above series converges for every $\omega \in \Omega$ and defines a random variable taking values in $[0,1]$ . Show that $X$ has uniform distribution on $[0,1]$ . My attempt : I believe the series converges (absolutely) by using the comparison test with $\sum_{i=1}^\infty 2^{-n}$ , as $X_n\leq 1$ , and $\lvert X_n2^{-n} \rvert\leq\lvert2^{-n}\rvert$ . Now I'm hoping to show that $$E=\{\omega :X(\omega)\in (a,b)\}\in\sigma(F)$$ as if that holds, I can conclude that any Borel set will be in $\sigma(F)$ as well, since $\{(a,b):a,b\in[0,1]\}$ generates all the Borel sets in $[0,1]$ . Now I can certainly find $\omega$ 's that will be in $E$ , since $X$ is a binary expansion of some real number in $[0,1]$ , but I'm a bit stuck at this point. I was given a hint to find $P(X\in[i2^{-m},(i+1)2^{-m}])$ for $i=0,...,2^{m}-1$ . I'm not sure how to find the probabilities, and how it can help to show that $X$ has a uniform distribution. Any help on both questions would be greatly appreciated. Thank you.","['measure-theory', 'probability-theory', 'probability', 'real-analysis']"
2958711,Are all derivatives vectors?,"Before reading this question, let me just state that prior to this year, I had allways thought vectors were arrows in space or lists of numbers, and I am still getting accustomed to their more formal definition of meeting a list of rules in their operations, having a magnitude, and some ""direction"" (although that is not necessarilly a direction in physical space). My question is this: Are all derivatives vectors? Even of things that aren't? Allow me to explain: I was reading in a physics book that although angular displacement in itself is not a vector, angular velocity and acceleration is. This makes sense: Consider 2 rotations on an object, X and Y. $X$ spins the object around the X-axis and $Y$ spins the object around the Y-axis. Saying that we are going to produce $X+Y$ means we are going to spin the object around the X-axis and then the Y-axis while saying we are going to do $Y+X$ means we are going to rotate the object around the Y-axis and then around the X-axis. As you can see from the picture below, $X+Y\neq Y+X$ However, as the change becomes infinitely small, the prospect of applying $Y$ after $X$ or $X$ after $Y$ becomes meaningless, since there is an infinitely small amount of time between the application of each tiny rotation, meaning that $dX/dt + dY/dt    =     dX/dt + dX/dt$ . It doesn't matter whether we apply an infinitely small rotation in the X direction first or in the Y direction first, the rotations are infinitely small, so applying one after the other kind of loses all meaning. They are being applied at the same time. We can also justify other vector properties via the same reasoning, such as $C(w_{X} + w_{y}) = Cw_{X} + Cw_{y}$ ( $w$ is the symbol for rotational velocity) . While with overall rotations, multiplying by the constant made a difference in the final position, since the velocities are over infinitely small time intervals and are therefore being applied at the same time, scaling each of the velocities by a constant is the same thing as applying each of the scaled velocities one after the other (which has lost all meaning when we are referring to derivatives) and scaling them separately: it doesn't make a difference their ""magnitude"" relative to the other angular velocity, since at the end of the day, since we are considering infinitely small time intervals, they will still be happenning at the same time, unlike overall rotations which will be happenning one after the other and making one of the rotations greater may affect the total effect after applying the other. We can even think of more examples where this applies, such as (and this is a completely random example just to get my point across) the effects of temperature change, let's say. While raising the temperature to 1000 degrees and then lowering it to -500 will leave you as frozen ashes, but lowering to -500 and then raising it to 1000 will leave you with ashes surrounded by steam, doing each of them in infinitely small intervals, it doesn't matter if you start by raising the temperature or start by lowering it, at the end of the day you will have the same result (if the temperature is being raised faster than it is being lowered, the room will get hotter, and if it's being lowered faster than its being raised, the room will get colder, and if they're happening at the same time you won't end up as ashes OR frozen since the temperature won't change at all). Anyways, are there other examples such as this, where the derivative is a vector but the change in itself is not? Are all derivatives vectors? Thanks!","['derivatives', 'vectors', 'rotations']"
2958717,How does the product construction of the Stone-Cech compactification work?,"Wikipedia's article on the Stone-Cech compactification gives several constructions of it, one which is this: One attempt to construct the Stone–Čech compactification of $X$ is to take the closure of the image of $X$ in $${\displaystyle \prod C}$$ where the product is over all maps from $X$ to compact Hausdorff spaces $C$ . This works intuitively but fails for the technical reason that the collection of all such maps is a proper class rather than a set. There are several ways to modify this idea to make it work; for example, one can restrict the compact Hausdorff spaces $C$ to have underlying set $P(P(X))$ (the power set of the power set of $X$ ), which is sufficiently large that it has cardinality at least equal to that of every compact Hausdorff set to which $X$ can be mapped with dense image. Forget the caveat about sets and proper classes, I'm just trying to understand the idea.  My question is, what does ${\displaystyle \prod C}$ mean?  I don't understand ""the product is over all maps from $X$ to compact Hausdorff spaces $C$ "".  What exactly are we taking a product of? Are we taking a Cartesian product of compact Hausdorff spaces, or are we taking some kind of product of maps, or what?","['general-topology', 'compactification', 'compactness']"
2958746,How do fractional tensor products work?,"In this blog post , Terry Tao discusses the $n$ -fold tensor product of a one-dimensional vector space $V^L$ ( $L$ is just a non-numeric label, not an exponent). He claims that With a bit of additional effort (and taking full advantage of the one-dimensionality of the vector spaces), one can also define spaces with fractional exponents; for instance, one can define $V^{L^{1/2}}$ as the space of formal signed square roots $\pm l^{1/2}$ of non-negative elements $l$ in $V^L$ , with a rather complicated but explicitly definable rule for addition and scalar multiplication. ... However, when working with vector-valued quantities in two and higher dimensions, there are representation-theoretic obstructions to taking arbitrary fractional powers of units. What is the ""rather complicated but explicitly definable rule for addition and scalar multiplication""? Is it easy to see why it doesn't work in higher than one dimension? Could one extend the construction to include irrational exponents? And what properties does the field need to satisfy in order for this construction to work? (Tao claims that the 1D vector space needs to be totally ordered. I'm not sure if this is exactly the same requirement as the field's being totally ordered. Presumably this construction doesn't work for arbitrary ordered fields, because you certainly can't define a square root function $\mathbb{Q} \to \mathbb{Q}$ . Does it only work for real vector spaces?)","['dimensional-analysis', 'linear-algebra', 'tensor-products']"
2958747,"Is every primitive element of a finite field of characteristic $2$, a generator of the multiplicative group?",Let $\alpha\in \overline {\mathbb F_2}$ (the algebraic closure of $\mathbb F_2$ ) be such that $\mathbb F_2[\alpha]$ is a field of order $2^n$ (where $n>1$ ). Then is it true that $\alpha \in \mathbb F_2[\alpha]^{\times}$ generates the multiplicative group $\mathbb F_2[\alpha]^{\times}$ i.e. is $2^n-1$ the multiplicative order of $\alpha$ ?,"['finite-fields', 'field-theory', 'galois-theory', 'abstract-algebra', 'group-theory']"
2958787,Confused when changing from Lebesgue Integral to Riemann Integral,"I'm currently studying Stochastic Calculus via Shreve II. I have a question about switching back and forth between Lebesgue and Riemann Integral. Suppose we have a non-negative random variable $X$ defined on a probability space $(\Omega, F, P)$ with exponential distribution: $$P(X<x) = 1-e^{-\lambda x}$$ Written in Lebesgue Integral, the expected value of $X$ can be written as: $$E[X] = \int_{\{{\omega \mid X(\omega) \geq 0}\}}^{ }X(\omega)dP(\omega)$$ Question: How exactly do we switch from $\omega$ in the Lebesgue Integral to $x$ in Riemann integral so that we get $$E[X] = \int_{0}^{\infty}x\lambda e^{-\lambda x}dx$$ Does this have to do with the fact that we should define our $\Omega$ to be the Borel $\sigma$ -algebra $B(\mathbb{R})$ and simply define $X(\omega) = \omega$ for non-negative $\omega$ 's? Any help is greatly appreciated!","['calculus', 'probability-theory', 'probability', 'measure-theory']"
2958814,Combinatorics question involving distributing DISTINCT candies to DISTINCT children,"Suppose you want to distribute 15 DISTINCT candies to 5 DISTINCT children: A, B, C, D and E. a) In how many ways can this be done (no restrictions)? b) In how many ways can this be done if each kid gets at least one candy? c) In how many ways can this be done if each kid gets exactly 3 candies? d) In how many ways can this be done if A and B collectively get at most four candies? e) In how many ways can this be done if exactly two kids get nothing? \ My attempts: \
\ a) Since we can assign the children to the candies there are $5^{15}$ ways to do this with no restrictions. b) I am very confused on this one...I would assume we would use the exclusion inclusion principle but I'm not sure exactly how to. First, I took the total number of ways to do this problem with no restrictions --> $5^{15}$ ways. Then I would subtract the bad cases, in which there are $5x4^{15}$ cases because if we take the case in which A has nothing then there are $4^{15}$ ways to distribute the rest of the kids to the candies and this stands for if B has nothing if C, if D and if E has nothing (so 5 times the $4^{15}$ ). I'm just unsure on how to figure out how much we overcounted by since there are essentially 5 circles in the venn diagrams... Can someone walk me through this one? c) I picked three candies for the first kid, three candies for the second all the way to the 5th kid. So there are ${15\choose 3}{12\choose 3}{9\choose 3}{6\choose 3}{3\choose 3}$ ways to do this. d) Let's give A and B 4 candies. First we need to choose the four candies for A and B so there are ${15\choose 4}$ ways to do this. Then we must assign the rest of the candies. Since there are 3 remaining children we can assign 3 kids to the remaining 11 candies in $3^11$ ways. So for this case there are ${15\choose 4}x3^{11}$ ways. You would continue this for the remaining cases in which A and B get 3 collectively, 2 collectively, 1 collectively and 0 collectively. So there would be ${15\choose 4}$ x $3^{11}$ + ${15\choose 3}$ x $3^{12}$ + ${15\choose 2}$ x $3^{13}$ + ${15\choose 1}$ x $3^{14}$ + ${15\choose 0}$ x $3^{15}$ ways in total. e) I'm not sure... \
\ Overall I'm really not sure about b, d and e, but if you could check a and c as well I would appreciate it! Thanks!","['combinations', 'combinatorics']"
2958916,Cohomology of Symmetric Group 3 using Lyndon-Hochschild-Serre spectral sequence,"For the symmetric group $S_{3}$ we have the short exact sequence $$0\rightarrow C_{3}\rightarrow S_{3}\rightarrow C_{2}\rightarrow 0,$$ where $C_{n}$ is the cyclic group of order $n$ . Using the Lyndon-Hochschild-Serre spectral sequence we obtain $$E_{2}^{p,q}=H^{p}(C_{2},H^{q}(C_{3},\mathbb{Z})),$$ where we would have $0$ for $q$ odd (right?). So my doubt is that I'm not sure of how to obtain the non-trivial action of $C_{2}$ on $\mathbb{Z}$ or $C_{3}$ when $q$ is even. A trivial action doesn't lead to the correct result, since after adding the diagonals in $E_{\infty}$ it should be the cohomology of $S_{3}$ , which is $$H^{n}(S_{3},\mathbb{Z})=\begin{cases}
\mathbb{Z} & n=0 \\
0 & n \hbox{ odd} \\
C_{2} & n\equiv2 \hbox{ mod 4} \\
C_{6} & n\equiv0 \hbox{ mod 4.}  \end{cases}$$","['symmetric-groups', 'group-theory', 'group-cohomology', 'spectral-sequences']"
2958923,What is the probablity of sitting next to my friend?,"Let's say you are at a table with $5$ others, everyone is seated randomly around a $6$ person table, and you only know $1$ person at this party. What is the likelihood you sit next to the individual that you know? What is the likelihood you are seated opposite to the person that you know? What is the likelihood that you sit next to two strangers? The table has $6$ seats so if you sit in any one seat then there are $5$ chairs left over. Since your friend can be seated on either side of you that leaves 3 chairs.  With that reasoning would it be $1/3$ ( $2/6$ )?",['probability']
