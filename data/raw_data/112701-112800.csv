question_id,title,body,tags
1635495,"Is $\nu(E) = \max\{\mu(E),\eta(E)\}$ a measure","Let $\mu$ and $\eta$ be measures on the measurable space $(X,\mathcal{M})$. For $E \in \mathcal{M}$, define $\nu(E) = \max\{\mu(E),\eta(E)\}$. Is $\nu$ a measure on $(X, \mathcal{M})$? My Try: I use the fact: $$\max(a,b)=\frac{a+b+|a-b|}{2}$$
then I tried to prove that $\nu$ is a measure and though I can prove or get to a contradiction. $$\nu(E)=\max\{ \mu(E), \eta(E) \} = \frac{\mu(E) + \eta(E) +|\mu(E)- \eta(E)|}{2}$$ $\nu(\emptyset)=\frac{\mu(\emptyset)+\eta(\emptyset)+|\mu(\emptyset) -\eta(\emptyset)|}{2}=0$ since $\mu$ and $\eta$ are measure. Consider now a countable disjoint collection $\{E_k\}_{k=1}^{\infty}$ of measurable sets \begin{align*}
\nu(\bigcup\limits_{k=1}^{\infty} E_k) &= \frac{1}{2} \Big[\mu(\bigcup\limits_{k=1}^{\infty} E_k) +\eta(\bigcup\limits_{k=1}^{\infty} E_k)+\Big|\mu(\bigcup\limits_{k=1}^{\infty} E_k)-\eta(\bigcup\limits_{k=1}^{\infty} E_k)\Big|\Big]\\
&=\frac{1}{2} \Big[\sum\limits_{k=1}^{\infty} \mu(E_k)+ \sum\limits_{k=1}^{\infty} \eta(E_k)+\Big|\sum\limits_{k=1}^{\infty} \mu(E_k)-\sum\limits_{k=1}^{\infty} \eta(E_k)\Big| \Big ]\\
&=\frac{1}{2} \Big[ \sum\limits_{k=1}^{\infty} \Big(\mu(E_k) +\eta(E_k)\Big) + \Big|\sum\limits_{k=1}^{\infty} \mu(E_k) - \eta(E_k)\Big|\Big]
\end{align*} Am I on the right track, because I am stuck? Any help? Thanks","['real-analysis', 'measure-theory']"
1635501,Geometric intuition for the Stein factorization theorem?,"What is the intuition behind the Stein Factorization Theorem ? I understand that it was originally a theorem in several complex variables, so I was wondering if there's some geometric explanation that isn't as opaque as the statement in EGA. In particular, why would one expect this theorem to be true? Are there any suggestive examples or heuristics? For example, when I think about the upper semi-continuity of dimension, I always have in the back of my head the picture of a blow up.","['schemes', 'several-complex-variables', 'algebraic-geometry']"
1635519,Is is possible to simplify the expression $\arctan(y)-\arctan(x)=c$,Is is possible to simplify the expression $\arctan(y)-\arctan(x)=c$. I tried writing the expression in the form $\frac{\arcsin(y)}{\arccos(y)}-\frac{\arcsin(x)}{\arccos(x)}=c$ but it does not lead to anyting. How do I reduce it I want to eliminate $\arctan$ from the expression.,['trigonometry']
1635520,How to prove this? $\lim_{h \to 0} \frac{\sin(\theta+h)-\sin(\theta)}{\cos(\theta+h)-\cos(\theta)} = -\frac{1}{\tan(\theta)}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How to prove this? $$\lim_{h \to 0} \frac{\sin(\theta+h)-\sin(\theta)}{\cos(\theta+h)-\cos(\theta)} = -\frac{1}{\tan(\theta)}$$","['trigonometry', 'limits']"
1635545,Chromatic Index in Graph,There is a graph $G$ with maximum degree that is greater than $0$. Suppose that $G$ contains a perfect matching $P$ and that $G-P$ (graph after removing all edges of $P$ in $G$) is bipartite. What is now the chromatic index(edge chromatic number) of $G$ in terms of it's maximum degree? I'm not really sure how to approach this problem. Any ideas?,"['graph-theory', 'coloring', 'discrete-mathematics']"
1635563,When is the rank of Jacobian constant?,"Suppose I've got a function $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$ which I know is bijective. Considering $\mathcal{J}$, the Jacobian of $\ f$, I want to understand what can be said about the rank of $\mathcal{J}(\mathbf{x})$. Let's say I evaluate $\mathcal{J}(\mathbf{0})$, and find that the rank of $\mathcal{J}(\mathbf{0})$ is $k$. Does this mean that the rank of $\mathcal{J}(\mathbf{x})$ is $k$ for all $\mathbf{x} \in \mathbb{R}^{n}$? Is there a theorem regarding this? EDIT: If this is not enough that $f$ is a bijection, what if $f$ is a homeomorphism? Or of class $C^{\infty}$?","['multivariable-calculus', 'matrix-rank']"
1635585,Uppercase E notation for sets?,"In Jónsson and Tarski's (1951) paper Boolean Algebras with Operators, Part I from the American Journal of Mathematics, they write formulae such as $L_i = \underset{u}{\mathbf{E}} \, [u \in At^m \text{ and } u \leq x^{(i)}]$ and $K = \underset{u}{\mathbf{E}} \, [y \geq u \in At^m]$, without explaining this $\underset{u}{\mathbf{E}}$ notation. From the context, I guess these define sets, i.e., they respectively mean $L_i = \{u \mid u \in At^m \text{ and } u \leq x^{(i)} \}$ and $K = \{u \mid y \geq u \in At^m \}$. Am I correct in this? Also, is this notation something common that mathematicians generally understand? What does E stand for, and where did this notation originate? Are there good books/articles/webpages where I can learn about this notation? I would greatly appreciate your help! Additional note. To give some context, on p. 900, following the first formula, they proceed to define $K = \bigcup_{i \in I} L_i$ (which has nothing to do with the $K$ in the second of the formulae above) and say that $u \in K$ if and only if $\sum_{i \in I} x^{(i)} \geq u \in At^m$. In order for this equivalence to hold, it seems to me that $L_i = \{u \mid u \in At^m \text{ and } u \leq x^{(i)} \}$. $At$ denotes the set consisting of $0$ and all the atoms of a Boolean algebra $A$. For my interpretation to make sense, however, I suppose that $At^m$ should be the set consisting of $0$ and all the atoms of $A^m$, and not the $m$-times product of $At$.","['boolean-algebra', 'notation', 'elementary-set-theory', 'universal-algebra']"
1635589,Why was $\aleph$ (aleph) chosen for infinities?,"Why did Cantor choose a letter from the Hebrew alphabet to represent infinities, rather than using some Greek letter?","['math-history', 'notation', 'infinity', 'elementary-set-theory', 'cardinals']"
1635619,Solve $\int_0^1 \int_0^{2\pi}\frac{ax-x^2\sin(\theta)}{\sqrt{a^2-2ax\sin(\theta)+x^2}}d\theta dx$,"Solve
$$\int_0^1 \int_0^{2\pi}\frac{ax-x^2\sin(\theta)}{\sqrt{a^2-2ax\sin(\theta)+x^2}}d\theta dx$$
This integral is from the following paper : Frictional coupling between sliding and spinning motion and can be understood as an integration of unit vector field parallel to the velocity of a spinning disk which is sliding with the speed of $a$ over a unit disk. The evaluation of the integral is claimed to be $$\frac{4}{3}\frac{(a^2+1)E(a)+(a^2-1)K(a)}{a}$$
for $a<1$, and $$\frac{4}{3}((a^2+1)E(1/a)+(a^2-1)K(1/a))$$
for $a\geq1$ in the paper, where $K$ and $E$ are the first and second kind of complete elliptic integrals respectively. But for me this integration is not clear since if I do the integration for $x$ first then it results in a term with natural log which is irrelevant to elliptic integral, and doing it for $\theta$ first results in the terms including $E(\frac{4ax}{(a-x)^2})$ and $K(\frac{4ax}{(a-x)^2})$ which seems hard to be reduced to $E(a)$ and $K(a)$. What approach would give the most clarified evaluation? Any advice or help will be appreciated.","['integration', 'definite-integrals']"
1635628,Tail estimate for $L^1$ functions.,Suppose $f\in{L^1(\mu)}$ for some probability measure $\mu$. Pick $\epsilon>0$ and let $A_n=\{x:|f(x)|>\epsilon{n}\}$. I want to show that $$\mu(A_1)+\mu(A_2)+\dots<\infty$$ My first thought was some sort of Chebyshev estimate but this doesn't immediately work due to divergence of the harmonic series. Anybody have any hints/ideas? I would like to work out the majority of this question by myself with a small hint if possible.,"['real-analysis', 'probability-theory']"
1635663,"Theorem 19.4 in Munkres' TOPOLOGY, 2nd ed: Does the converse also hold?","Let $J$ be an arbitrary (non-empty) index set, and let $\left\{ X_\alpha \right\}_{\alpha \in J}$ be an indexed family (or collection) of topological spaces, and let $\Pi_{\alpha \in J} X_\alpha$ denote their Cartesian product. Then here's Theorem 19.4 in the book Topology by James R. Munkres, 2nd edition: If each space $X_\alpha$ is a Hausdorff space, then $\Pi_{\alpha \in J} X_\alpha$ is a Hausdorff space in both the box and product topologies. So far so good! Now my question is, does the converse also hold? That is, if $\Pi_{\alpha \in J} X_\alpha$ is a Hausdorff space in either the box or the product topologies, then does it follow that each space $X_\alpha$ is Hausdorff also? Of course, if $\Pi_{\alpha \in J} X_\alpha$ is Hausdorff in the product topology, then it is Hausdorff in the box topology also.",['general-topology']
1635743,An Analogue of Chinese Remainder Theorem for Groups,"I am trying to prove the following analogue of Chinese remainder theorem for groups: Let $G$ be group and let $H_1, \dots, H_n$ be its normal subgroups such that their indices $[G : H_1], \dots, [G : H_n]$ are pairwise coprime. Then we have $$G/(H_1 \cap \cdots \cap H_n) \cong G/H_1 \times \cdots \times G/H_n.$$ I think that a good strategy would be to try to prove that the mapping $\phi$ defined by $$\phi(g(H_1 \cap \cdots \cap H_n)) = (gH_1, \dots, gH_n)$$ is an isomorphism, but I am not sure how to do this.","['chinese-remainder-theorem', 'group-theory']"
1635746,Are there any other solutions to this equation?,"Consider the equation $1-t = tx^{1-2t}$ for some complex number $t$ and real $x$.
Are there any other solutions to this equation besides $\Re(t) = \frac{1}{2}$ ? My attempt: The above equation can be written in the form               $\dfrac{x^t}{t} = \dfrac{x^{1-t}}{1-t}$ Which can be interpreted as $\int_0^x u^{-\alpha} \mathrm {d}u = \int_0^x u^{\alpha-1} \mathrm {d}u $, 
where $\alpha= \Re(t)$. Interpreting these integrals as areas under the respective curves, observe that the equality requires that $-\alpha = \alpha - 1$, which yields $\alpha= 1/2$, as required ?","['algebra-precalculus', 'complex-numbers']"
1635761,Proof verification for $\mathcal{P}(A) \cap \mathcal{P}(B) = \mathcal{P}(A \cap B)$,"I propose here my proof for: $$\mathcal{P}(A) \cap \mathcal{P}(B) = \mathcal{P}(A \cap B)$$ $\Longrightarrow$
$$x \in \mathcal{P}(A) \land x \in\mathcal{P}(B)$$
$$x \subseteq A \land x \subseteq B$$
$$x \subseteq A \cap B$$
$$x \in \mathcal{P}(A \cap B)$$ $\Longleftarrow$
$$x \in \mathcal{P}(A \cap B)$$
$$x \subseteq A \cap B$$
$$x \subseteq A \land x \subseteq B$$
$$x \in \mathcal{P}(A) \land x \in \mathcal{P}(B)$$ Is it correct?","['elementary-set-theory', 'proof-verification']"
1635775,Proving $x>\sin(x)$ without calculus for $x>0$,"The starting problem was to prove $$\sin 26^{\circ}\sin 58^{\circ}\sin 74^{\circ}\sin 82^{\circ}\sin 86^{\circ}\sin 88^{\circ} \sin 89^{\circ}>\frac{45\sqrt{2}}{64\pi}\\\cos 1^{\circ}\cos 2^{\circ}\cos 4^{\circ}\cos 8^{\circ}\cos 16^{\circ} \cos 32^{\circ}\cos 64^{\circ}>\frac{45\sqrt{2}}{64\pi}\\128\sin 1^{\circ}\cos 1^{\circ}\cos 2^{\circ}\cos 4^{\circ}\cos 8^{\circ}\cos 16^{\circ} \cos 32^{\circ}\cos 64^{\circ}>\frac{90\sqrt{2}}{\pi}\sin 1 \\\sin 128^\circ>\frac{90\sqrt{2}}{\pi}\sin\frac{\pi}{180}$$
Now if $\frac{\pi}{180}>\sin(\frac{\pi}{180})$ is true then the inequality is true but I'm not sure how to prove the inequality.","['algebra-precalculus', 'contest-math', 'inequality', 'trigonometry']"
1635779,In how many ways can 40 identical carrots be distributed among 8 different rabbits?,"In how many ways can 40 identical carrots be distributed among 8 different rabbits, while every rabbit needs to get a carrot, and no rabbit get more then 16 carrots. Thank you for the help!","['combinatorics', 'discrete-mathematics']"
1635813,Finding a matrix given eigenvalues and eigenvectors.,"I am asked to construct a $4 \times 4$ symmetric matrix, with given eigenvalues and eigenvectors. I understand how to actually get $A$ as a product of $P^T, D$ and $P$, when $D$ is the diagonal matrix, and $P$ is a matrix with the eigenvectors as columns. The problem is that there is only three given eigenvectors, along with three eigenvalues (one is repeated), so my question is, how do you construct a $4 \times 4$ matrix with three eigenvectors? For more information here is the actual question: Let $A$ be a symmetric $4 \times 4$ matrix with real entries whose eigenvalues
are $−1$ and $2$. If $(1, 0, 0, −1)$, $(0, 1, 1, 0)$ is a basis for the eigenspace of eigenvalue $-1$ and $(1, 0, 0, 1)$ is an eigenvector of $A$ with eigenvalue $2$, find the matrix $A$. Thank you.","['matrices', 'diagonalization', 'eigenvalues-eigenvectors', 'symmetry']"
1635823,Find a Markov chain transition kernel,"Let $f_{X}$ be a density we would like to sample from. For some reasons, $f_{X}$ may be analytically intractable or expensive to evaluate. A solution consists in considering a density $(x,y) \in X \times Y \, \mapsto \, f(x,y)$ on $X \times Y$ such that $f_{X}$ is a marginal density of $f$. Then, we define a Markov chain $(X_{n})_{n \geq 0}$ as follows : Given the current state of the chain $X_{n} = x$ : $ Y_n \sim f_{Y \mid X}(\cdot \mid x)$ $X_{n+1} \sim f_{X \mid Y}(\cdot \mid Y_n)$ where $f_{X \mid Y}(\cdot \mid y)$ (resp. $f_{Y \mid X}(\cdot \mid x)$) denotes the conditional density of $X$ given $Y=y$ (resp. of $Y$ given $X=x$). I would like to determine the transition kernel of the Markov chain $(X_{n})_{n \geq 0}$. For that, let $\varphi$ be a bounded measurable function on $X$. I would like to compute $\mathbb{E}\big[ \varphi(X_{n+1}) \mid \mathcal{F}_{n} \big]$ where $\mathcal{F}_{n} = \sigma(X_{0},\ldots,X_{n})$ is the natural filtration. Let $g(Y_n) = \mathbb{E}\big[ \varphi(X_{n+1}) \mid Y_n \big]$. Then : $$ g(y) = \int_{X} \varphi(u) f_{X \mid Y}(u \mid y) \, du. $$ Therefore : $$ \mathbb{E}\big[ \varphi(X_{n+1}) \mid \mathcal{F}_{n} \big] = \mathbb{E}\big[ g(Y_n) \mid \mathcal{F}_{n} \big] = \int_{Y} \Bigg( \int_{X} \varphi(u) f_{X \mid Y}(u \mid y) \, du \Bigg) f_{Y \mid X}(y \mid X_{n}) \, dy.$$ This can be rewritten : $$ \int_{X} \varphi(u) \Bigg( \int_{Y} f_{X \mid Y}(u \mid y) f_{Y \mid X}(y \mid X_{n}) \, dy \Bigg) \, du \tag{$\star$}$$ We know that $\mathbb{E}\big[ \varphi(X_{n+1}) \mid \mathcal{F}_{n} \big] = P\varphi(X_{n})$ where the transition kernel $P$ satisfies $\displaystyle P\varphi(x) = \int_{X} \varphi(u)P(x,du)$. So we can use $(\star)$ to identify the transition kernel. I am not sure this is correct and I am having trouble deriving the transition kernel from the last identity.","['markov-chains', 'probability-theory', 'probability-distributions']"
1635837,Is any smooth deformation of a metric in dimension 1 conformal?,"Consider $(S^1, g)$ where $S^1$ is the unit circle and g is a metric. Now consider the metric 
$$
\tilde g := f g
$$
where f is a smooth positive function. Since in 1 dimension this is the only smooth deformation that is possible it seems to me that any deformation would necessarily be conformal. Since I am asked to show a certain equality holds for deformations of a metric on $S^1$ that are not necessarily conformal I wonder where my reasoning is going wrong. Does the task make sense ?","['conformal-geometry', 'riemannian-geometry', 'differential-geometry']"
1635849,"For any $n^2+1$ closed intervals of $\mathbb R$, prove that $n+1$ of the intervals share a point or $n+1$ of the intervals are disjoint","Stuck on a question from 'Introduction to Combinatorics by Martin J. Erickson'. Q: For any $n^2+1$ closed intervals of $\mathbb R$, prove that $n+1$ of the intervals share a point or $n+1$ of the intervals are disjoint. I think we can use the Erdos-Szekeres Theorem: relating the usual series of integers to the closed intervals, and the decreasing/increasing monotonic sequences somehow to the two outcomes, but I am stuck on how to technically do this. Could we measure the 'distance' between the intervals? Creating a decreasing sequence ensuring they'd be close enough to share a point, and an increasing sequence that would ensure they are far enough away to be disjoint?","['combinatorics', 'sequences-and-series']"
1635888,Positivity of the alternating sum associated to at most five subspaces,"Let $V_1 , V_2 , \dots , V_n $ be vector subspaces of $ \mathbb{C}^m$ and let  $$\alpha = \sum_{r=1}^n (-1)^{r+1} \sum_{  \ i_1 < i_2 < \cdots < i_r } \dim(V_{i_1} \cap \cdots \cap V_{i_r})$$ For $n = 2$ we have the equality $ \alpha =  \dim(\sum_{i = 1}^{n} V_i) $; it's false for $n>2$, see this answer . For $n=3$, we have only the inequality $ \alpha \ge  \dim(\sum_{i = 1}^{n} V_i) $; it's false for $n>3$, see this post . For $n>5$, the inequality $\alpha  \ge 0$ is false in general, see the comment of Darij Grinberg below. Question : Is it true that $\alpha \ge 0$, in the case $n \le 5$? Remark : I think this question interesting for itself; it admits also applications in the interaction between representations theory and subgroups lattice.","['combinatorial-geometry', 'homological-algebra', 'combinatorics', 'linear-algebra', 'vector-spaces']"
1635900,"Continuous but not compact operator on $L^2(0,\infty)$","Define the following operator on $L^2(0,\infty)$: $$Tf(x)=\frac{1}{x} \int_0^xf(y)dy,\quad f\in L^2(0\infty).$$ I would like to see that it is continuous but not compact. So, this is an integral operator with kernel $k(x,y)=\frac{1}{x}\mathbf1_{(0,x)}(y)$. The problem is that $k$ is not even in $L^2(0,\infty)^2$. Thus, the usual bound $\|Tf\|_2\leq \|k\|_2\cdot \|f\|_2$ does not work. Hence I am not even sure why the operator is well-defined. I.e. why is $Tf$ even in $L^2(0,\infty)$? And how might we show continuity/non-compactness?","['functional-analysis', 'integration', 'operator-theory']"
1635949,Is there a chain rule for integration?,"I know the chain rule for derivatives. The way as I apply it, is to get rid of specific 'bits' of a complex equation in stages, i.e I will derive the $5$th root first in the equation $(2x+3)^5$ and continue with the rest. I wonder if there is something similar with integration. I tried to integrate that way $(2x+3)^5$ but it doesn't seem to work. Well, it works in the first stage, i.e it's fine to raise in the power of $6$ and divide with $6$ to get rid of the power $5$, but afterwards, if we would apply the chain rule, we should multiply by the integral of $2x+3$!, But it doesn't work like that, we just need to multiply by $1/2$ and that's it. So my question is, is there chain rule for integrals? I want to be able to calculate integrals of complex equations as easy as I do with chain rule for derivatives.","['integration', 'calculus']"
1635996,Cardinality of the set of all bijections,"Let $A$ be an infinite set and let $S$ be the set of all bijections $A \rightarrow A$. Then if $\mid A \mid = \kappa$, then $\mid S \mid = 2^\kappa$. I'm able to prove it for $A = \mathbb{N}$ by showing an injection $P(\mathbb{N}) 
\rightarrow S$, but how can I prove it for any set $A$?","['cardinals', 'elementary-set-theory']"
1636000,why is the limit as n goes to infinity of $(1+\frac{1}{n}+\frac{200}{n^2})^n = e$?,"I know that $$\lim_{n\to\infty}\left(1+\frac{1}n\right)^n = e .$$ But why does $$\lim_{n\to\infty}\left(1+\frac{1}n+\frac{a}{n^b}\right)^n = e ? \quad where\quad  b\gt1$$
  better yet, how can I conclude something like:
  $$\lim_{n\to\infty}\left(1+\frac{1}n+\sum_{k=2}^\infty \frac{700^k}{k!n^k}\right)^n = e $$
  Why do all the terms in the sigma not contribute anything to limit? This is from a statistics course where we have to evaluate a similar expression but I have studied and done most of the exercises of the chapter on sequences and series of real numbers in Rudin's principles of math. analysis","['real-analysis', 'exponential-function', 'limits']"
1636021,Rigorous proof that $dx dy=r\ dr\ d\theta$,"I get the graphic explanation, i.e. that the area $dA$ of the sector's increment can be looked upon as a polar ""rectangle"" as $dr$ and $d\theta$ are infinitesimal, but how do you prove this rigorously? 1. Geometrically, the exact area would be $$\frac{(r+dr)^2d\theta}{2} - \frac{r^2d\theta}{2}$$$$= (r + \frac{dr}{2}) dr d\theta  $$$$= r dr d\theta + \frac{dr^2 d\theta}{2}.$$ How do we get rid of $\frac{dr^2 d\theta}{2}$? Is it too insignificant in value compared to $r dr d\theta$ so that it vanishes? If so, is it the reason it gets ignored? And what's more, geometrically speaking, why would the two areas - $dx dy$ and $r dr d\theta$, be equal? 2. Approaching it algebraically, setting $$x = r\sin\theta$$$$y = r \cos\theta$$ gives $$\frac{dx}{d\theta} = r\cos\theta,  dx = r\cos{\theta} d\theta$$$$ \frac{dy}{dr} = \cos\theta,  dy = \cos\theta dr. $$ Multiplying both equations, side by side, gives $$dxdy = r\cos^2\theta dr d\theta.$$ Again I get an extra term, which is $\cos^2 \theta$. In both cases I am unable to derive that $dxdy = rdrd\theta$. What do I do wrong in my reasoning? It all makes me think that I'm getting something essential terribly wrong.","['multivariable-calculus', 'multiple-integral', 'area', 'polar-coordinates']"
1636023,Every variety is isomorphic to an intersection of a linear space and a Veronese surface,"""Deduce that any projective variety is isomorphic to an intersection of a Veronese variety with a linear space"" I've been trying to solve this exercise from Joe Harris book. I can see that if a variety $X\subset \mathbb{P}^n$ has only polynomials of degree $d$, then in the coordinates of $\nu_d(\mathbb{P}^n)$ it is, indeed, a linear space. The problem is when I have different degrees. If I take a family of polinomial of maximum degree $d$, and I have a polynomial $f_0$ in this family, with $m=deg ~f_0<d$, i can multiplicate $f_0$ by $X_0^{d-m}$, so I get $X_0^{d-m}f_0$ with degree d, and it is a linear space in $\nu_d(\mathbb{P}^n)$. The problem is that it may not be the same variety $X$ after this proceeding. There is a way to fix that? Or should I try in a different way? Thanks.",['algebraic-geometry']
1636058,"Is a relation, R, an Equivalence Relation of a Power Set?","Where $A = \{1,2,3,4,5,6\}$ and $S = P(A)$ is the power set, for $a,b \in S$ define a relation $R: (a,b) \in R$ where $a$ and $b$ have the same number of elements. Is $R$ an equivalence relation on $S$ and if so how many equivalence classes are there? I've defined my $R$ as being $\{(\{1,2\},\{3,4\}),(\{1,2\},\{5,6\}),(\{1,2\},\{1,2\}),(\{3,4\},\{1,2\}),(\{3,4\},\{3,4\}),(\{3,4\},\{5,6\}),(\{5,6\},\{1,2\}),(\{5,6\},\{3,4\}),(\{5,6\},\{5,6\})\}$ because of the part of the question mentioning $a$ and $b$ have the same number of elements. Was that wrong to do?",['elementary-set-theory']
1636073,Is $f(x)=x^{3}+3x^{2}+12x-2\sin x $ one-one and onto?,"For linear or simple quadratic equations, it is quite simple to check if the function is onto or not. But I often face questions like the one I posted above, to check whether they are one-one and onto. While ascertaining that it is one-one is a piece of cake (using first derivative) is there an algorithm to ascertain whether it is onto or into?","['algebra-precalculus', 'functions']"
1636098,Solving a Limit with L'Hospital's Rule with 2 Unknown Constants,"I am trying to find this limit: $$ \lim_{n \to \infty} \frac{(\log_{2}n)^k}{n^p} $$ where $k$ and $p$ are constants and $k \geq 1$, $p \gt 0$. The limit equates to $0$, but I need someone to help explain this derivation. Thank you.","['calculus', 'limits']"
1636113,Notation: $\mathbb{Z}[\sqrt{-5}]$,"Show that the elements 2,3, and $1 \pm \sqrt{-5}$ are irreducible elements of $\mathbb{Z}[\sqrt{-5}]$.  I have never seen this notation before.  From another post I am interpreting this to mean the following: $\mathbb{Z}[\sqrt{-5}] = \{a_{0}+a_{1}\sqrt{-5}+ \dots + a_{n}(\sqrt{-5})^{n} \colon a_{i} \in \mathbb{Z}     \}$. Am I correct or is it something else?  I do not need the proof, just verification of notation.","['abstract-algebra', 'notation']"
1636117,Monotonicity of the sum/product/max of two monotone functions,"Suppose two monotone functions $f$ and $g$ (both weakly increasing or both weakly decreasing) are given. How can it be shown that $f+g, f \cdot g, \max(f,g)$ is again monotone (either weakly increasing or weakly decreasing)? Is there a reference to a text book?","['monotone-functions', 'functions']"
1636128,Fermat like equation for meromorphic functions.,"I found this question in Conway, and really have no idea how to answer it.  Can anyone provide any hints? For each integer $n\geq 1$ determine all meromorphic functions on $\mathbb{C}$ $f$ and $g$ with poles at $\infty$ such that $f^n+g^n =1$.","['functional-analysis', 'complex-analysis', 'analysis']"
1636137,BMO2 2016 Number Theory Problem,"Suppose that $p$ is a prime number and that there are different positive
integers $u$ and $v$ such that $p^2$ is the mean of $u^2$ and $v^2$. Prove that
$2p−u−v$ is a square or twice a square. Can anyone find a proof. I can't really see any way to approach the problem.","['number-theory', 'prime-numbers']"
1636152,Double Integration over finite plane.,$$\phi(z)=\frac{\sigma}{4\pi\varepsilon_0}\int_{\frac{-a}{2}}^{\frac{a}{2}}\int_{\frac{-a}{2}}^{\frac{a}{2}}\frac{1}{\sqrt{x^2+y^2+z^2}}~dx~dy$$ I'm not sure how to do this integral. For the first integral w.r.t $x$ i tried to substitute $x=\sqrt{y^2+z^2}\sin{\theta}\implies dx=\sqrt{y^2+z^2}\cos\theta~d\theta$. The integral then becomes: $$\phi(z)=\frac{\sigma}{4\pi\varepsilon_0}\int_{\frac{-a}{2}}^{\frac{a}{2}}\int_{-\text{?}}^\text{?}1~d\theta~dy$$ But the bounds are $\text{?}=\arcsin{\frac{a}{2\sqrt{y^2+z^2}}}$ since arcsin is an odd function. However this just makes it even harder to solve. So what is the best way to do this integral? If it helps I will give the context of the question. I am asked to find the strength of the electric field at a height z above the centre of a square sheet with constant charge density $\sigma$ and side lengths $a$.,['integration']
1636169,"Question on a ""dual form"" of $F$ that caprures the singularity of a plane section of $F$","In an article I was reading the notion of dual form came up, which I write down below. I was interested in learning more about this and I have two questions regarding it. This is how it came up:
Let $F(x_1, x_2, x_3, x_4)$ be a non-singular homogeneous rational polynomial. Then there exists a non-zero ""dual form""
$
f(x_1, x_2, x_3, x_4) \in \mathbb{Q}[x_1, x_2, x_3, x_4]
$
such that 
we have
$$
f(\mathbf{t}) = 0
$$
whenever the plane section 
$$
F(\mathbf{x}) = \mathbf{t} \cdot \mathbf{x} = 0
$$
is a singular curve. 
Furthermore, $f$ is irreducible. My two questions are the following: Does this only hold for a form in $4$ variables? or is there a generalization for which works in more variables? (so that $f(\mathbf{t})$
vanishes whenever $F(\mathbf{x}) = \mathbf{t} \cdot \mathbf{x} = 0$ is singular) I would like to learn some ideas behind the proof. I would greatly appreciate if someone could explain me how one can obtain such a form.","['algebraic-curves', 'algebraic-geometry']"
1636173,‚Äé‚Äé‚Äé$‚Äé‚ÄéC^*$-algebra generated by ‚Äé$‚Äé‚Äéa$‚Äé,"Let ‚Äé$‚Äé‚ÄéA$ ‚Äébe a unital ‚Äé‚Äé‚Äé$‚Äé‚ÄéC^*$-algebra. ‚Äé‚Äé
Assume that ‚Äé$‚Äé‚Äéa\in A$ ‚Äéis a ‚Äé‚Äénormal ‚Äéand ‚Äéinvertible element ‚Äéi.e ‚Äé‚Äé$‚Äé‚Äéaa^*=a^*a$ ‚Äéand ‚Äé‚Äé$‚Äé‚Äéaa^{-1}=a^{-1}a=1$‚Äé.‚Äé ‚Äélet $‚Äé‚ÄéC^*({a}) $ be the ‚Äé‚Äé‚Äé$‚Äé‚ÄéC^*$-algebra generated by ‚Äé$‚Äé‚Äéa$‚Äé. I know that ‚Äé$‚Äé‚ÄéC^*({a}) $ ‚Äéis ‚Äéthe ‚Äéclosed ‚Äélinear ‚Äéspan ‚Äéof ‚Äé‚Äé$‚Äé‚Äéa^{m}a^{*{n}}$‚Äé‚Äé‚Äé such that $m,n\in N$. ‚Äé
‚Äé
I want to know ‚Äé$‚Äé1 , a^{-1} \in ‚Äé‚ÄéC^*({a}) ‚Äé‚Äé$‚Äé‚Äé
‚Äé Q: Is it true?""$‚Äé1 , a^{-1} \in ‚Äé‚ÄéC^*({a}) ‚Äé‚Äé$‚Äé‚Äé""‚Äé How can I prove it?
‚Äé‚Äé","['functional-analysis', 'c-star-algebras', 'banach-spaces', 'banach-algebras']"
1636186,Maximum Likelihood Estimation with 2 parameters for a Poisson distribution,"I have two observations from a Poisson distribution. The first one ($N_r$) come with a Poisson distribution with mean $k_1$. For the second one ($N_e$) I know that $N_e - M$ also come from the same Poisson distribution with mean $k_1$, but the parameter $M$ is unknown. My observations are $N_r$ and $N_e$. I want to do the MLE for $M$ and $k_1$. If I write the equations: 
$$ f(N_r, N_e|k_1, M) = \frac{e^{-k_1}k_1^{N_r}}{N_r!}\frac{e^{-k_1}k_1^{N_e-M}}{(N_e-M)!}=\frac{e^{-2k_1}k_1^{N_r+N_e-M}}{N_r!(N_e-M)!}$$ Taking logarithms:
$$ ln(f) = -2k_1 + (N_r+N_e-M)ln(k_1)-ln(N_r!(N_e-M)!)$$ I can estimate the mean parameter $k_1$ as
$$ k_1 = \frac{N_r+N_e-M}{2}$$ But I don't know how to estimate the $M$ parameter as I have to deal with the factorial in the derivative. Can anybody help me? Thank you very much!!","['maximum-likelihood', 'statistics', 'estimation', 'probability', 'poisson-distribution']"
1636192,"How many pairs are in $(B,C) \in P(A) \times P(A)$ such that $B \subseteq C$","I'm trying to solve this problem:
Let $A = \{1,2,3,\ldots,n \}$ How many pairs are in $(B,C) \in P(A) \times P(A)$ such that $B \subseteq C$ I want to solve this using combinatorics, Basically what I know is that for every $x \in A$ we have the possibilities: 1) $x \in B$ 2) $x \in C \setminus B$ 3) $x \notin B$ and $x \notin C$ And I know that for every $C$ I take, there could be $2^k$ possibilities for $B$ since $B$ is a subset of $C$, but I'm having trouble counting the possibilities for $C$. Some help?
Thanks","['combinatorics', 'discrete-mathematics']"
1636206,zeros of $p(z)=z^4+2$,"I want to find all zeros of $p(z)=z^4+2$ and I'm not sure if I've done everything correctly. Can you correct this if something is wrong?
$$x^4+2=0 \iff x^4=-2=2\cdot(-1)$$
$$\Rightarrow x_k= \sqrt[4]{2}e^{\frac{i(2k+1)\pi}{4}}$$with $k\in \{0,1,2,3\}$ are the zeros of $p$. Is it correct? Additional question: If I want to determine all zeros of $z^n+a$ with $a\in\mathbb{R}$ and $n\in\mathbb{N}$, are the zeros
$$x_k= \sqrt[n]{a}e^{\frac{i(2k+1)\pi}{n}}$$for $k=0,..,.n-1$ in general?","['complex-analysis', 'polynomials']"
1636207,"If $B= \{1, 2\}$ and $C = \{\{1,2\}\}$ what is $B \times C$?","I understand the basics of Cartesian products, but I'm not sure how to handle a set inside of a set like $C = \{\{1,2\}\}$.  Do I simply include the set as an element, or do I break it down? If I use it as an element I think it would be something like this: $$\{(1,\{1,2\}), (2,\{1,2\})\}$$ If I were to break $C = \{\{1,2\}\}$ further, I'm not sure how I would implement that, so I'm guessing what I did above is correct, but I want to make sure.",['discrete-mathematics']
1636216,Lebesgue measure has the Darboux property,"Let $A$ be a measurable Lebesgue set, with $\lambda(A)>0$($\lambda$ is the Lebesgue measure). Then, for every $b \in (0,\lambda(A))$, there exists a set $B$ measurable Lebesgue, $B\subset A$, with $\lambda(B)=b$. Can you give me a suggestion? I have no idea how to ""build"" the set $B$. It seems to me very likely to the Darboux property of functions..can it be used here?",['measure-theory']
1636246,why $|x|$ in $\frac{d}{dx}\sec^{-1}x=\frac{1}{|x|\sqrt{x^2-1}}$,"I derived $$\frac{d}{dx}\sec^{-1}x$$ as follows: Let $$z=\sec^{-1}x$$ Then $$x=\sec z$$ differentiating both sides w.r.t $x$ we get $$1=\sec z \tan z \frac{dz}{dx}$$ so we get $$1=x\sqrt{x^2-1}\frac{dz}{dx}$$ so $$\frac{dz}{dx}=\frac{1}{x\sqrt{x^2-1}}$$ But why we need to introduce modulus to $x$, is it because slope of the tangent should be unique?","['derivatives', 'calculus']"
1636313,Representation of roots of unity.,"How to represent solutions of $\sqrt[26]{1}$ with solutions of $\sqrt[26]{-1}$? I know that $$w_{k}=\cos\left(\frac{0+2k\pi}{26}\right)+i\sin\left(\frac{0+2k\pi}{26}\right), \; \; k=\overline{0,25}$$
and $$z_{k}=\cos\left(\frac{\pi+2k\pi}{26}\right)+i\sin\left(\frac{\pi+2k\pi}{26}\right), \; \; k=\overline{0,25}$$
But I cannot establish connection that would rewrite $\sqrt[26]{1}$ using $\sqrt[26]{-1}$ i.e. $w_k$ using $z_k$.","['algebra-precalculus', 'trigonometry', 'complex-numbers', 'calculus']"
1636318,Symmetric Difference Approximation of a Measurable Set [duplicate],"This question already has answers here : Approximating a $\sigma$-algebra by a generating algebra (3 answers) Closed 8 years ago . Let $(\Omega,\mathcal{F},P)$ be a probability space and $\mathcal{A}$ be an algebra of subsets of $\Omega$ such that 
$\sigma(\mathcal{A})=\mathcal{F}$. Prove that for all $B\in \mathcal{F}$  and for all $\varepsilon >0$ there is $A\in \mathcal{A}$ such that $P(B\bigtriangleup A)<\varepsilon,$ where $B\bigtriangleup A:=(A\setminus B)\cup (B\setminus A)$ is the symmetric difference of $A,~B.$ I have tried proof by contraposition, by assuming the existence of a set $B \in \mathcal{F}$ and an $\varepsilon>0$ such that $P(B\bigtriangleup A)\geq \varepsilon,$ but this doesn't seem to get me somewhere. On the other hand, for a straight proof, the construction of the desired set $A$ seems foggy, since I don't know how to start. Thanks a lot for the help!","['probability-theory', 'measure-theory']"
1636324,Proving number of partitions of $n$ to $3$ parts at most.,"I have an exercise, to prove that the number of partitions of $n$ to at most $3$ integers is $\frac{(n+3)^2}{12}$ rounded. I tried to prove by induction but I don't know how.",['combinatorics']
1636328,Powers of a prime as one more than the square of an integer...,"Given a fixed prime $p$, are there finitely many positive integers $k$ such that $p^k = n^2 +1$ for some $n$?",['number-theory']
1636373,Partial derivative of x - is quotient rule necessary?,"Let 
$$u(x,y)=\frac{x}{x^2+y^2}$$ I'm trying to determine if the given function is harmonic. I know that the 2nd partial derivative with respect to $x$ should, when added to the 2nd partial derivative of $y$, equal $0$. However, I'm kind of stuck. I'm using the quotient rule to solve for the partial derivative of $x$, but is this the right way to take a partial derivative of a quotient?","['multivariable-calculus', 'partial-derivative', 'derivatives']"
1636375,Artin approximation vs implicit function theorem in the class of analytic functions,"I am not an algebraist so my question might be stupid. I am doing mainly complex analysis and recently I was informed about the existence of Artin's theorem and it sounded like it could be of interest to me. I have found a survey on the subject and I started reading it. Here's the link . So to the actual theorem (section 1.1) Let $\mathbb{k}$ be a field of characteristic 0 and let $f(x,y)$ be a vector of convergent power series in two variables $x$ and $y$. Assume given a formal power series $\hat{y}(x)$ vanishing at 0,
  $$f(x,\hat{y}(x))=0.$$
  Then for any $c\in\mathbb{N}$, there exists a convergent power series solution $\tilde{y}(x)$
  $$f(x,\tilde{y}(x))=0$$
  which coincides with $\hat{y}(x)$ up to degree $c$,
  $$\hat{y}(x)\equiv \tilde{y}(x) \mbox{ mod }x^c. $$ I really care only for the case where $k=\mathbb{C}$. Using the implicit function theorem for some analytic $f$ we get the existence of an analytic solution as long as the Jacobian is invertible at 0. If on the other hand the Jacobian is not invertible then we generically get some king of branching and this means that there is no formal solution in powers of $x$ that solves the equation. So in that sense I don't see how Artin's theorem is stronger than the implicit function theorem in the analytic setting. Is this true or do I miss something? By the way I don't know what happens when we considering other fields and I don't imply that the theorem is trivial or useless.","['complex-analysis', 'algebraic-geometry']"
1636378,Solve $\sqrt[3]{7x+19}+\sqrt[3]{7x-19}=\sqrt[3]{2}$ by algebraic methods,"I was trying to solve this equation without using calculus.
Is it possible to be solved by elementary algebraic methods? $$\sqrt[3]{7x+19}+\sqrt[3]{7x-19}=\sqrt[3]{2}$$",['algebra-precalculus']
1636441,What angle is $\sin^{-1}(3/2)$?,"So i have this trigonometric equations: $$2\cos^2(x)+4\sin(x)+\cos(2x)=0$$ I have rewritten the expression and came up with $$(2\sin(x)-3)(1+2\sin(x))=0$$ Then i split the equation in two and got $$\sin(x)=3/2,\quad\text{and}\quad\sin(x)=-1/2$$ Since $\sin(x)=-1/2$ is a standard angle and with respect to the period I got $x=\pi - \sin^{-1}(11\pi/6)$ The problem is that I can not figure out what angle $\sin(x)=(3/2)$. I have tried using the Pythagorean theorem but since $\sin$ is $\text{opp}/\text{hyp}$, it tells me that something is wrong since the hypotenuse can not be shorter than the sides. Does anyone have an idea of what I should do?",['trigonometry']
1636494,Polynomial divides set of points,"Given a set of points in the plane with distinct $x$-coordinates, each point colored black or white. A polynomial $P(x)$ ""divides"" the set of points if no black point lies above $P(x)$ and no white point lies below $P(x)$, or vice versa. Points of any color can lie on $P(x)$. What is the least $k$ such that any valid set of $n$ points can be divided by a polynomial of degree at most $k$? We can do $k=n-2$ by having a polynomial pass through any $n-1$ points, thus trivially satisfying the condition of dividing the points.","['algebra-precalculus', 'polynomials']"
1636506,"Is there a difference between $f(x,y)$ $f(x;y)$ and $f(x\mid y)$?","While reading I have come across all three of these notations seemingly at random, and as far as I can tell they are all positional arguments to a function, but I can't tell if they mean different things, do they?","['notation', 'functions']"
1636532,Conditional Independence with infinite r.v.'s,"I read this property in ""Probability with Martingales"" from Williams(page 91-92): Suppose that $X_1 , ...,X_r $ are independent, each $X_k$ with law $\lambda_k$. If $h$ is bounded and $\mathcal{B}^r $-measurable ; and we define for $x_1 \in \mathbb{R} $ :
  $$ \gamma^h(x_1):=\mathbb{E}(h(x_1,X_2,X_3,...,X_r)) $$
  then $\gamma^h(X_1) $ is a version of the conditional expectation
  $$\mathbb{E}(h(X_1,X_2,X_3,...,X_r)|X_1) $$ I need to figure out if the property holds when I have infinite R.V.'s $(X_k)_{k \in \mathbb{Z}} \ $  instead of   $\  r  $ R.V.'s ; and I condition on, say, $(X_N)_{N\in \mathbb{N}}$. The proof for the property in the finite case uses Fubbini's Theorem; but I understand that it holds only for finite product measures.","['probability-theory', 'conditional-expectation']"
1636543,$\int_{-1}^1\frac1f=\infty$ iff $\int_{-1}^1(u_n')^2f\to0$,"Let $f$ be a continuous function on $[-1,1]$ such that $f(x\neq0)>0,f(0)=0$. How can I show that $\int_{-1}^1\frac{1}{f(t)}dt=\infty$ iff there exists a sequence of functions $u_n$, $C^1$ on $[-1,1]$ such that $\int_{-1}^1 (u_n'(t))^2f(t)dt\underset{{n\infty}}\to0$ and $(u_n)$ converges pointwise to the sign function on $[-1,1]$ ? The sign function is defined by $sgn(x<0)=-1,sgn(0)=0,sgn(x>0)=1$. By $C^1$ I mean derivable with a continuous derivative. Neither way of the equivalence seems easy, and other than having confirmed that the above works both ways on easy cases (such as $f$ being $|x|$), I haven't really made any progress. What is more since the convergence is only pointwise in the reverse way, it tells us very little on the derivative of $u_n$.","['integration', 'calculus', 'limits']"
1636549,"Cardinal equality: $\;\left|\{0,1\}^{\Bbb N}\right|=\left|\{0,1,2,3\}^{\Bbb N}\right|$","I need to prove the above equality without Cantor-Bernstein Theorem or cardinals arithmetic (i.e., a bijection must be found). I know that for example $\;S\to 1_S=\;$ the indicator function, gives a bijection $\;P(\Bbb N)\to \{0,1\}^{\Bbb N}\;$ , so if I can find a bijection $\;P(\Bbb N)\to\{01,2,3\}^{\Bbb N}\;$ then I can compose these two and that's all. Yet this last one is making problems to me, so any help will be appreciated.","['cardinals', 'elementary-set-theory']"
1636554,"How do we conclude that $f(x)=0, \forall x\in \mathbb{R}$ ?","Suppose that $f:\mathbb{R}\rightarrow \mathbb{R}$ is a periodic function such that $\displaystyle{\lim_{x\rightarrow +\infty}f(x)=0}$. I want to show that $f(x)=0$ for all $x\in \mathbb{R}$. $$$$ Let $T$ be the period of $f$, then $f(x)=f(x+T)$. Therefore, we have that $$0=\lim_{x\rightarrow +\infty}f(x)=\lim_{x\rightarrow +\infty}f(x+T)=\lim_{x\rightarrow +\infty}f(x+2T)=\dots =\lim_{x\rightarrow +\infty}f(x+nT), \ \forall n\in \mathbb{Z}$$ So, $$|f(x)|=|f(x+T)|=|f(x+2T)|=\dots =|f(x+nT)|\leq \epsilon$$ $$$$ But how exactly do we conclude that $f(x)=0, \forall x\in \mathbb{R}$ ?","['real-analysis', 'periodic-functions', 'analysis', 'limits']"
1636596,Positivity of a series of functions involving double poles constrained by certain inequalities,"This is a calculation I need for my statistics project Big edit: simplify the function $f(x)$ a lot. Define for $f(x)$ , $x\geq 0$ , $$
f(x):=\sum_{k=1}^\infty \frac{a_k^2\lambda_k^2x}{(1+x\lambda_k)^2} - \sum_{k=1}^\infty \frac{b_k^2\beta_k}{(1+x\beta_k)^3}
$$ where $a_k\in \mathbb R$ , $b_k\in\mathbb R$ , $\lambda_k>0$ , $\beta_k> 0$ , and $$
\sum_{k=1}^\infty b_k^2\leq \sum_{k=1}^\infty a_k^2<\infty\,\text{ and }\sum_{k=1}^\infty a_k^2 \lambda_k\leq \sum_{k=1}^\infty b_k^2 \beta_k<\infty.
$$ Additional assumption: we may think each $\beta_k$ is very large. You may take it as large as you want. I am trying to prove that $f(x)$ has following graph. That is, prove that there exists $x_0>0$ such that $f(x_0)=0$ , and for all $x<x_0$ , $f(x)<0$ , and for all $x>x_0$ , $f(x)>0$ . This question has already been solved in this link We only need to take $\beta:=\min(\beta_i)$ and the function $(1+\beta x)^3f(x)$ is increasing.","['derivatives', 'calculus']"
1636611,Deductive Proof - Justify each step with law or inference rule,"My Professor gave me the following: a) If $P \to Q, \neg R \to \neg Q$ , and $P$ then prove $R$ . b) If $P \to (Q\wedge R)$ and $\neg R\wedge Q$ then prove $\neg P$ . I understand how to do truth tables, but we've barely started on using laws and inference rules for proofs. I feel like this hw is a huge leap from what we covered in class, and I don't even know how to begin. For a, this is what I understand: Premise: $P \to Q$ Premise: $\neg R \to \neg Q$ Premise: $P$ Conclusion: $R$ Then for b: Premise: $P \to (Q\wedge R)$ Premise: $\neg R\wedge Q$ Conclusion: $\neg P$ Laws (provided by prof.): Inference Rules :","['propositional-calculus', 'proof-writing', 'logic', 'discrete-mathematics']"
1636632,Maximum likelihood estimator of $\lambda$ and verifying if the estimator is unbiased,"$(X_1,\ldots,X_n)$ is a random sample extracted from an exponential law of parameter $\lambda$ Calculate the  likelihood estimator $\nu$ of $\lambda$ . Then, if $n=2$ : establish if $\nu$ is a unbiased estimator $$L(\lambda: X_1,\ldots,X_n)=\prod_{i=1}^n \lambda \ e^{-\lambda \  x_i} \ \ 1_{(0,+\infty)} \  (x_i)=$$ $$=\prod_{i=1}^n (1_{(0,+\infty)} \  (x_i)) \ \ \lambda^n \ e^{-\lambda \sum_{i=1}^n  x_i} \ \ $$ $$\frac{\partial}{\partial \lambda} L(\lambda: X_1,...,X_n)=\prod_{i=1}^n (1_{(0,+\infty)} \  (x_i)) \ \ n \lambda^{n-1} \ e^{-\lambda \sum_{i=1}^n  x_i}-\sum_{1=1}^n x_i \ \lambda^n \ \ e^{-\lambda \sum_{i=1}^n  x_i}=  \ \ $$ $$=\prod_{i=1}^n (1_{(0,+\infty)} \  (x_i)) \ \ \lambda^{n-1} \ e^{-\lambda \sum_{i=1}^n x_i} \ \ (n- \lambda \sum_{i=1}^n x_i) $$ $$\frac{\partial}{\partial \lambda} L(\lambda: X_1,\ldots,X_n) \ge 0 \Longleftrightarrow \lambda \le \frac{1}{\overline{X}}$$ Maximum likelihood estimator of $\lambda$ is $\nu=\frac{1}{\overline{X}}$ If $n=2$ , I think that: $$\nu=\frac{2}{\sum_{i=1}^2 X_i}$$ and $$\sum_{i=1}^2 X_i \sim \Gamma(2, \lambda)$$ How can I establish if $\nu$ is a unbiased estimator? Thanks!","['statistical-inference', 'probability-distributions', 'maximum-likelihood', 'statistics', 'probability']"
1636651,What is this matrix notation and how is it solved?,"I've never taken a stats class, or linear algebra or much of anything that involves matrices.  In one of my books they give me this as part of an example and it states, $$\binom{6}{4} = 15 \text{ combinations}$$ I don't understand how the math is done for that statement. I don't get where $15$ comes from. I'm assuming it is some sort of a probability formula, but I am not sure.","['combinatorics', 'statistics', 'binomial-coefficients']"
1636748,Is there an easy criterion to determine whether given polynomials form a complete intersection?,"Suppose we have homogeneous polynomials in $s$ variables $F_1, ..., F_n$
with coefficients in integers. Let 
$X$ be a variety (or algebraic set) defined by the simultaneous equations
$$
F_1(\mathbf{x}) = ... = F_n(\mathbf{x}) = 0. 
$$ I was wondering under what conditions do $F_1, .., F_n$ form a complete intersection? Is there a relatively easy to check condition that polynomials have to satisfy to guarantee $X$ to be a complete intersection? I would greatly appreciate any commments/references! Thank you very much!",['algebraic-geometry']
1636797,Sole minimal element: Why not also the minimum?,"A minimal element (any number thereof) of a partially ordered set $S$ is an element that is not greater than any other element in $S$. The minimum (at most one) of a partially ordered set $S$ is an element that is less than or equal to any other element of $S$. Let's consider the power set $\mathcal P (\{x,y,z\})$ together with the binary relation $\subseteq$. The Hasse diagram shows what element(s) we're looking for: It's easy to see that: $\emptyset$ is a minimal element $\emptyset$ is the minimum Now if we remove $\emptyset$ and consider $\mathcal P (\{x,y,z\})\setminus \emptyset$ instead, we get the following: $\{x\}$, $\{y\}$ and $\{z\}$ are minimal elements there is no minimum (1) We know that a minimum is unique and it is always the only minimal element. (2) And from the example above, it seems that, if a sole minimal element exists, it is always the minimum. But I read that (2) is false. Why?","['relations', 'elementary-set-theory', 'order-theory']"
1636806,Why do the Existence and Uniqueness Theorem and The Principle of Superposition not contradict each other?,"I have a question regarding the Existence and Uniqueness Theorem and The Principle of Superposition, which my book (Elementary Differential Equations and Boundary Value Problems) defines in the following ways: Existence and Uniqueness Theorem Consider the initial value problem $$y'' + p(t)y' + q(t)y = g(t), \qquad y(t_0) = y_0, \qquad y'(t_0) = y'_0$$ where $p$ , $q$ , and $g$ are continuous on an open interval $I$ that contains the point $t_0$ . Then, there is exactly one solution $y = \phi(t)$ of this problem, and the solution exists throughout the interval $I$ . Principle of Superposition If $y_1$ and $y_2$ are two solutions of the differential equation $y'' + p(t)y' + q(t)y = 0$ , then the linear combination $c_1 y_1 + c_2 y_2$ is also a solution for any values of the constants $c_1$ and $c_2$ . From the existence and uniqueness theorem, we know there is only one equation $y = \phi(t)$ that satisfies the equation $y'' + p(t)y' + q(t)y = 0$ . From the principle of superposition, we know that $y = c\phi(t)$ is also a solution. How is this possible? Is it because the existence and uniqueness theorem is for particular solutions, where as the principle of superposition is for general solutions?","['ordinary-differential-equations', 'initial-value-problems']"
1636845,The concatenation of two independent normal vectors is multivariate normal.,"I've already read this question . By the definition I have, 
$$\mathbf{z} = \begin{bmatrix}
z_1 & z_2 & \cdots & z_n 
\end{bmatrix}^{T}$$
is a multivariate standard normal vector if each $z_i \sim \mathcal{N}(0, 1)$ is iid. Furthermore, $\mathbf{Az+b} \sim \mathcal{N}(\mathbf{b}, \mathbf{A}\mathbf{A}^{T})$ (assuming that $\mathbf{A}$, $\mathbf{b}$ are obviously conformable) is multivariate normal. Suppose that I have two vectors $\mathbf{a} \sim \mathcal{N}(\boldsymbol{\mu}_1, \mathbf{V}_1)$ and $\mathbf{b} \sim \mathcal{N}(\boldsymbol{\mu}_2, \mathbf{V}_2)$ which are both $n$-dimensional and independent. How do I show that
$$\begin{bmatrix}
\mathbf{a} \\
\mathbf{b}
\end{bmatrix} \sim \mathcal{N}\left(\begin{bmatrix}
 \boldsymbol{\mu}_1\\
\boldsymbol{\mu}_2 
\end{bmatrix}, \begin{bmatrix}
\mathbf{V}_1 & \mathbf{0}_{n \times n} \\
\mathbf{0}_{n \times n} & \mathbf{V}_2
\end{bmatrix} \right)\text{?}$$ The mean vector and variance-covariance matrix are easy to derive, but I am stuck as to how to show joint normality.","['statistics', 'probability', 'probability-distributions']"
1636847,"Homotopy equivalence between $X=\{0\}\cup \{\frac{1}{n},n\in \mathbb{N}\}$ and a discrete space","Consider the space  $X=\{0\}\cup \{\frac{1}{n},n\in \mathbb{N}\}$ with the topology induced by the real line. Is $X$ homotopy equivalent to some enumerable discrete space $Y$? My try was the following: Let $Y$ a enumerable discrete space. If $X$ and $Y$  are homotopy equivalent then there exists maps $f:X\to Y$ and $g:Y\to X$ s.t  $g\circ f:Y\to Y$ is homotopic to  $Id_Y$.  But $X$ is compact, so the image of $g\circ f$ must be finite. However I can't explore this ideia :(","['algebraic-topology', 'general-topology']"
1636881,An alternative derivation of radius of curvature (2D functions). How valid is it?,"I was wondering how radius of curvature was derived, and this is what I came up with. It turned out to be longer than expected. Then I looked at how it compares with other (presumably more mathematically accurate) derivations on the Internet. Everywhere I looked had the same approach, where they first looked at curvature by finding the rate of change of the angle between the tangent at the point of interest and the $x$ -axis, with respect to arc length, and then getting to the radius of curvature. What I've done seems more of a direct approach to me, maybe more geometric as well. What I started with: What I finished with: $$R = \left | \frac{(1+f'(a)^2))^{\frac{3}{2}}}{f''(a)} \right |$$ Before getting to that I made these statements: $$\left (  \lim_{h\rightarrow 0}f(a+h)=f(a)\right )$$ and \begin{align*}
\text{By definition:}\qquad
\lim_{h\rightarrow 0} \frac{f(a+h)-f(a)}{h} &= f'(a), \\
\text{and}\qquad
\lim_{h\rightarrow 0} \frac{f'(a+h)-f'(a)}{h} &= f''(a).
\end{align*} I'm not sure about the statement in the brackets on the fourth page. It feels right, but I'm not sure. That statement also seems like it's contradicting the ""By definition..."" statement on the third page? Any thoughts on this, or on the derivation in general? (Full derivation at https://drive.google.com/file/d/0B4LamLT1ywM1M1NzOHVUQnFvd2M/view?usp=sharing&resourcekey=0-0wvgtszqdJPteKUxY3Fakg ) Update: Changed link, also by ""It turned out to be longer than expected"" I mean 4 pages long...","['curvature', 'differential-geometry', 'calculus', 'geometry']"
1636897,Why is the solution to $\sqrt{6-5x}=x$ only $x=1$ and not $x=-6$? [duplicate],"This question already has answers here : Is $\sqrt{64}$ considered $8$? or is it $8,-8$? (11 answers) Closed 8 years ago . I solved the equation $\sqrt{6-5x}=x$ as follows:
$$(\sqrt{6-5x})^2=x^2$$
$$6-5x=x^2$$
$$0=x^2+5x-6=(x+6)(x-1)$$
$$x=-6 \quad \text{or} \quad x=1$$ If I plug in $x=-6$ into the original equation, I get $\sqrt{6+30}=\sqrt{36}=\pm 6$ and if I plug in $x=1$, I get $\sqrt{6-5}=\sqrt{1}=\pm 1$. To me it seems that both values satisfy the original equation. I am using an online education system for my class called MyMathLab and the solution is only $x=1$. Why is that? Thank you in advance.","['radicals', 'problem-solving', 'algebra-precalculus', 'arithmetic', 'quadratics']"
1636900,Show that every open set in second countable LCH space is $\sigma$-compact,"Let $(X,\tau)$ be a second countable, locally compact Hausdorff space. Theorem: If $S \in \tau$ , then $S$ is $\sigma$ -compact. How do I show this statement? The following is what I have tried: Let $\mathcal{E}$ be a countable base for $(X,\tau)$ . For each $x \in S$ there exists a compact neighbourhood $V(x), x \in V(x) \subseteq S$ .(This is a theorem that I'm allowed to use) Then $S = \bigcup_{x \in S} V(x)$ , but how do I show that there exists a finite cover?","['general-topology', 'compactness', 'separation-axioms']"
1636901,Finding an explicit entire function $g$ satisfying $g(n \log n) = n^{\pi}$,"I encountered the following problem in the lecture note in my complex analysis class: Problem. Find an explicit entire function $g$ satisfying $g(n \log n) = n^{\pi}$ for $n = 1, 2, \cdots$ . Hint. $|(1 − t)|e^t$ is decreasing for $0 < t < 1$ and increasing for $t > 1$ . Use this to compare the derivative of a product vanishing at $n \log n$ to one vanishing at $n$ , for integers $n$ . There is also an easier way using an elementary function that vanishes at the integers. This is an exercise in the chapter where the Mittag-Leffler theorem and the Weierstrass factorization theorem were introduced. I tried to follow the hint and was able to indirectly construct such a function (by mimicking the proof of Mittag-Leffler), but I was unable to find an explicit closed form. Would anyone help me find an explicit example? Thanks in advance!","['analyticity', 'complex-analysis']"
1636908,Fundamental Theorem of Algebra for Trigonometry,"The Fundamental Theorem of Algebra states that ""Any polynomial of degree $n$ ... has $n$ roots."" Is there anything analogous for trigonometric equations? I've been solving some trigonometric equations, and solving some of the slightly more complex ones involves, what seems like guess and check, applying certain trig identities (namely $sin  (\theta + 2\pi)$ or $sin(\pi - \theta) = sin (\theta)$) over and over again to check to see whether the resultants are within the given domain. So, my question is, if we cant determine the exact solutions from a method other than guess and check, can we at least determine the number of solutions of a trigonometric equation given its range using a more mathematical procedure (i.e. no guess and check whatsoever)?",['trigonometry']
1636918,Double integration over function with absolute values,"I have having difficulty in how to solve the following double integral problem involving absolute values and the assumption that $\alpha > 1$: $\iint_{-\infty}^{+\infty} \frac{1}{1+|x|^\alpha} \frac{1}{1+|y|^\alpha} \frac{1}{1+|x-y|^\alpha} \,dx\,dy$ Any tips on how to calculate the above integration is highly appreciated. Thank you in advance!","['multivariable-calculus', 'integration', 'absolute-value']"
1636947,What's the point of the fancy notation for surface integrals and line integrals?,"Most of the times you see line integrals of a vector field written as this $$
\int_C\mathbf{F\cdot ds}
$$ And surface integrals like $$\iint_\Sigma \mathbf{F\cdot n}\,\mathrm dS$$ My question is, what's the point of all this symbology? It seems like the $\mathbf {ds}$ and $\mathbf{n}\, \mathrm dS$ are there just to remind the reader ""hey! This is a little tiny piece of curve/surface!"" or whatever heuristic you have to explain this integrals. If you know that $C\subseteq \Bbb R^k$ is a curve, and $\Sigma\subseteq \Bbb R^k$ is a surface, why not just write $\int_C \bf F$ and $\iint_\Sigma \bf F$ (or even $\int_\Sigma \bf F$ (although the double integral sign makes more sense, because in the end you end up calculating a double integral))?","['notation', 'line-integrals', 'surface-integrals', 'multivariable-calculus', 'integration']"
1636950,Alternating group on infinite sets,"It is well known that the only normal subgroup of $S_n$ is $A_n$ when $n\geqslant 5$, and that $A_n$ is also simple. Furthermore, $A_{\infty}$, the even permutations on $\mathbb{N}$, is also simple. This lead me to wonder about the following: Take a general set $X$ with cardinality $\kappa>\aleph_0$ from which we can generate the group $\text{Sym}\,X$. Questions can we define an alternating group on $X?$ if so does it remain the only normal subgroup of $\text{Sym}\, X?$","['permutations', 'group-theory']"
1636956,Use Ito's Lemma to compute $d(\log S(t))$ and use this to find the closed form solution of S(t),I am having issues with this practice problem. If someone could help me solve it that would be greatly appreciated! Let $S(t)$ be the stock price that satisfies the BSM model in SDE form $$dS(t) = \mu S(t) dt + \sigma S(t) dW_t$$ where $\mu > 0$ and $\sigma > 0$ are two constants. Use Ito's Lemma to compute $d \log S(t)$ and use this result to find the closed form solution of $S(t)$ .,"['brownian-motion', 'ordinary-differential-equations', 'stochastic-calculus']"
1636978,derivative of arctan(u),"Im trying to find the derivative of $\arctan(x-\sqrt{x^2+1})$ here are my steps if someone could point out where I went wrong. $$\begin{align}
\frac{\mathrm d~\arctan(u)}{\mathrm d~x} \;& =\; {1\over{1+u^2}}\cdot \frac{\mathrm d~u}{\mathrm d~x}
\\[1ex]
& =\; {1-{x\over{\sqrt{x^2+1}}}\over{1+(x-\sqrt{x^2+1})^2}}
\end{align}$$
Everything after this turns into a huge mess I don't know how to simplify. Is there a trick I missed or something I don't see?","['derivatives', 'inverse-function']"
1636992,An example of a reversible but reducible Markov chain,"The reversibility of a Markov chain is defined in the following way with some basic propositions. Unfortunately all examples of reversible Markov chains shown in my textbook so far are irreducible, giving me an impression (I think it is false) that all reversible Markov chains are irreducible. I am curious about is there any example of reversible yet reducible Markov chains , so the initial distribution satisfying the detailed balance is not the only stationary distribution. Thank you!","['stochastic-processes', 'markov-chains', 'probability']"
1637120,Circle is similar to a polygon with infinite number of sides,"It is known from the time of Euclid, that a circle is similar to a polygon with infinite number of sides. But this ^^ is informal. Do you know any formalization where it appears that a circle is a polygon with infinite number of sides?","['math-history', 'infinity', 'calculus', 'geometry']"
1637131,Solve $y-x\frac{dy}{dx}=y^2\cos x(1-\sin x)$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Solve the given differential equation. $y-x\frac{dy}{dx}=y^2\cos x(1-\sin x)$ Can anyone give some hint as how to initiate the solution?","['ordinary-differential-equations', 'calculus']"
1637138,Example where integration by parts formula fails for a.e. differentiable functions,"I'm studying for a qual and found this problem. We were given two absolutely continuous functions $f,g$ on $[a,b]$. The first two parts of the problem involved proving the integration by parts formula: $$
\int_{[a,b]}fg' = f(b)g(b)-f(a)g(a) - \int_{[a,b]} f'g.
$$
This was fairly straight forward. However, the last part now asks to find an example where this formula need not hold if we only assume $f$ and $g$ are differentiable almost everywhere as opposed to absolutely continuous. Can someone provide an example/a hint for me to verify?","['real-analysis', 'measure-theory']"
1637154,"Prove that if $X$ is subgaussian, then ${\bf E}e^{tX}=1+\sum_{k=1}^{\infty}\frac{t^k}{k!}{\bf E}X^k$","Prove that if $X$ is subgaussian, then $${\bf E}e^{tX}=1+\sum_{k=1}^{\infty}\frac{t^k}{k!}{\bf E}X^k$$ So basically I just need to push the integral through the infinite sum
$${\bf E}e^{tX}=\int_{\bf R}e^{tx}d\mu_X=\int_{\bf R}\sum_{k=0}^{\infty}\frac{(tx)^k}{k!}d\mu_X$$
Thus I'll use the dominated convergence theorem, bounding the absolute value of the partial sums by the (hopefully) $\mu_X$-integrable function $e^{|tx|}$,
$$\Big|\sum_{k=0}^n\frac{(tx)^k}{k!}\Big|\leq e^{|tx|}$$
Now to show $e^{|tx|}$ is $\mu_X$-integrable, I have
$$\int_{\bf R}e^{|tx|}d\mu_X=\;\;?$$ So by the subgaussian property of $X$ I have that
$$P(|X|\geq\lambda)\leq\int_{\lambda}^{\infty}2cCxe^{-cx^2}dx=Ce^{-c\lambda^2}$$
for $c,C>0$ fixed and for any $\lambda>0$. Hence this integrand almost functions as my pdf for $X$, and if it did I could use the Radon-Nikodym theorem to solve this.  However even though any upper-tailed integral of it bounds that of the actual pdf , I can't quite see how to use it to bound the integral of $e^{|tx|}$.","['probability', 'measure-theory']"
1637158,Find the value of $\sum_{n =1}^\infty \frac 1 {5^{n+1}-5^n+1}$,"$$\sum_{n = 1}^\infty \dfrac 1 {5^{n+1}-5^n+1}$$ I can factorize denominator to $4\times5^n+1$ to confirm the series does not diverge, But how do I calculate its actual sum? The series is not a telescoping series nor I can partial factorise.
I get confused due to $+1$  in the denominator. Thanks a lot","['summation', 'sequences-and-series', 'calculus']"
1637160,Fourier distribution $\frac{e^{i|x|}}{|x|}$,"I need help to calculate Fourier transform in distribution sense of $\frac{e^{i|x|}}{|x|}$ in $D'(\mathbb{R}^3)$
we have $ \frac{e^{i|x|}}{|x|} \in L^1_{loc}(\mathbb{R}^3)$ edit, Let $E(x)=\frac{e^{i|x|}}{|x|}$, i have $\langle\hat E,\phi\rangle=\langle E,\hat \phi\rangle=\int_{R^3}\int_{R^3}\frac{e^{i|\xi|}}{|\xi|}e^{-i\xi.x}\phi(x)dx\,d\xi$ Thanks","['functional-analysis', 'distribution-theory', 'fourier-analysis', 'analysis']"
1637174,Prove that $a(x+y+z) = x(a+b+c)$,"If $(a^2+b^2 +c^2)(x^2+y^2 +z^2) = (ax+by+cz)^2$ Then prove that $a(x+y+z) = x(a+b+c)$ I did expansion on both sides and got:
$a^2y^2+a^2z^2+b^2x^2+b^2z^2+c^2x^2+c^2y^2=2(abxy+bcyz+cazx) $ 
but can't see any way to prove $a(x+y+z) = x(a+b+c)$. How should I proceed?","['algebra-precalculus', 'polynomials']"
1637175,how to prove an element is non-zero in a tensor-product,"I was studying the following example from Atiyah & MacDonald's Introduction to Commutative Algebra : Let $x$ be the non-zero element in $N := \mathbf{Z}/ 2\mathbf{Z}$, $M := \mathbf{Z}$, and $M' := 2 \mathbf{Z}$. The element $2 \otimes x$ is zero in $M \otimes N$, but non-zero in $M' \otimes N$. I suppose this can be seen by the fact that $2 \otimes x$ generates $M' \otimes N$, and the tensor-product $M' \otimes N$ is non-zero. However, I was wondering, what one would do if that element weren't a generator of $M' \otimes N$. So my question is: What are the methods to prove that an element is non-zero in a tensor-product of modules? Thanks a lot!","['abstract-algebra', 'modules', 'tensor-products', 'commutative-algebra']"
1637179,Is there a formula for the area under $\tanh(x)$?,I understand trigonometry but I've never used hyperbolic functions before. Is there a formula for the area under $\tanh(x)$? I've looked on Wikipedia and Wolfram but they don't say if there's a formula or not. I tried to work it out myself and I got this far: $\tanh(x) = {\sinh(x)\over\cosh(x)} = {1-e^{-2x}\over 1+e^{-2x}} = {e^{2x}-1\over e^{2x}+1} = {e^{2x}+1-2\over e^{2x}+1} = 1-{2\over e^{2x}+1}$ Now I'm stuck. I don't know if I'm on the right track or not.,"['hyperbolic-functions', 'area', 'calculus']"
1637210,Norm of the operator on Hilbert Space $l_2$,"Suppose $l_2 = \{x= (x_n) | \sum x_n^2 < \infty \}$ is a Hilbert Space and $T( (x_n))= (x_2 -x_1, x_3 - x_2, \dots , x_n-x_{n-1}, \cdots )$. Which of the followings are true a) $||T|| =1$ b) $||T|| > 1$ and bounded. c) $||T||$ is unbounded. I knnow that  $||T|| = sup \{ ||T(x)||_2 \ \ : \ \ ||x||_2 = 1\} =sup \{(x_2 -x_1, x_3 - x_2, \dots , x_n-x_{n-1}, \cdots ) : \sum x_n^2 = 1 \} = sup \{\sum (x_{n+1}-x_n)^2 : \sum x_n^2 =1\}$ but $\sum (x_{n+1} -x_n)^2  \leq \sum x_n^2$. thus $sup \{\sum (x_{n+1}-x_n)^2 : \sum x_n^2 =1\} \leq sup \{\sum (x_n)^2 : \sum x_n^2 =1\} = 1$
we get $||T|| \leq 1$ and take $x = (1,0,0,\cdots )$, then $||T(x)||_2 = 1$ thus $|| T|| = 1$ Please check my solution , if found any error , then correct me.","['functional-analysis', 'proof-verification']"
1637233,Numerical evidence of law of iterated logarithm (random walk),"The law of iterated logarithm states that for a random walk $$S_n = X_1 + X_2 + ... X_n$$ with $X_i$ independent random variables such that $P(X_i = 1) = P(X_i = -1) = 1/2$ , we have $$\limsup_{n \rightarrow \infty} S_n / \sqrt{2 n \log \log n} = 1, \qquad \rm{a.s.}$$ Here is Python code to test it: import numpy as np
import matplotlib.pyplot as plt

N = 10*1000*1000
B = 2 * np.random.binomial(1, 0.5, N) - 1       # N independent +1/-1 each of them with probability 1/2
B = np.cumsum(B)                                # random walk
plt.plot(B)
plt.show()

C = B / np.sqrt(2 * np.arange(N) * np.log(np.log(np.arange(N))))
M = np.maximum.accumulate(C[::-1])[::-1]        # limsup, see http://stackoverflow.com/questions/35149843/running-max-limsup-in-numpy-what-optimization
plt.plot(M)
plt.show() Question: I have done it lots of times , but the ratio is nearly always decreasing to 0, instead of having a limit 1. Where is the problem? Here's the kind of plot I have most often for the ratio (which should approach $1$ ):","['random-walk', 'probability-theory', 'simulation', 'random-variables']"
1637243,Quaternions: Why is the angle $\frac{\theta}{2}$? [duplicate],This question already has answers here : half sine and half cosine quaternions (3 answers) Closed 8 years ago . The equation for creating a quaternion from an axis-angle representation is $$x'= x \sin\left(\frac \theta 2\right)$$ $$y' = y \sin\left(\frac \theta 2\right)$$ $$z' = z \sin\left(\frac \theta 2\right)$$ $$w' = \cos\left(\frac \theta 2\right)$$ But why $\frac \theta 2$? Why not just $\theta$?,"['angle', 'quaternions', 'trigonometry', 'rotations']"
1637252,Calculate $\lim\limits_{x\to0} \frac{1}{\sin x} \cdot \ln \left(\frac{e^x -1}{x}\right)$,"I was trying to calculate the limit of the following function: $$ \lim_{x\to0} \frac{1}{\sin x} \cdot \ln \left(\frac{e^x -1}{x}\right) $$ My first thought was using L'Hopital's rule since $\Large \frac{e^x -1}{x}$ goes to 1 so the whole $\ln$ goes to 0. But then I get another complicated expression, and finally I end up using L'Hopital's rule at least 5 times before getting an actual result. Is there a wiser way for dealing this limit? (I mean, without using this rule?) Thanks.","['limits-without-lhopital', 'limits']"
1637293,Evaluation of $\lim_{x \rightarrow 0^+} x^{\frac{1}{x}}$,"$\lim_{x \rightarrow 0^+} x^{\frac{1}{x}}$ My workout: Let $y$ be the answer to the limit. \begin{align}y = \lim_{x \rightarrow 0^+} x^{\frac{1}{x}}&\implies \ln\ y = \lim_{x \rightarrow 0^+} \ln\ x^{\frac{1}{x}}\\&\implies\ln\ y = \lim_{x \rightarrow 0^+} \frac{1}{x} \ln\ x \\&\implies\ln\ y = \lim_{x \rightarrow 0^+} \frac{\ln\ x}{x}\end{align}
and by L'Hopital's Rule: $$\ln\ y = \lim_{x \rightarrow 0^+} \frac{1}{x} \implies y = e^{\lim_{x \rightarrow 0^+} \frac{1}{x}}$$ Therefore: $y = e^{\infty} = \infty$. Correct Answer: $0$ What is wrong with my answer? And why is the answer $0$?","['calculus', 'limits']"
1637315,Differential entropy of the product of Gaussian random variables,"Given two independent Gaussian random variables $X \sim \mathcal{N}(\mu_x,\sigma_x^2)$ and $Y \sim \mathcal{N}(\mu_y,\sigma_y^2)$. We look at the product distribution of these two random variables $Z=XY$. My question is, what is the differential entropy $h(Z)$? The differential entropy is defined as $h(Z)=\int_z f(z) \log (f(z)) dz$, where $f(z)$ is the probability density function of $Z$. The probability density function can be computed to be
$f(z)=\frac{1}{\pi\sigma_x\sigma_y} K_0(\frac{|z|}{\sigma_x\sigma_y})$,
where $K_0$ is the modified Bessel function second order.
(see: Wikipedia: product distribtuion , Wolfram: Normal product distribution .
) However, I couldn't find any information on the entropy of the modified bessel function. There seems to be a special case for $K_{\frac{1}{2}}$, but I could find a closed form solution for $K_0$. Does anybody know how to compute $h(Z)$? A good approximation of $K_0$ such that $h(Z)$ can be lower bounded, would be sufficient, in case an exact solution is known to be intractable. It seems that even the most basic lower bounds fail to yield some insights,
for example: $h(XY)\geq h(XY|Y) = \int f(y)H(Xy|Y=y) dy = \int f(y) (\log|y|+ h(X))dy=h(X)+\int f(y) \log |y|dy$, 
where the relation to $h(X)$ is shadowed by the last term. Thanks in advance, Rick","['information-theory', 'entropy', 'probability']"
1637317,Can you go from $\aleph_0$ to $\aleph_1$ with tetration or other higher order operators?,"The paradox of Hilbert's Hotel shows us that you can not get past the cardinality of the natural numbers ($\aleph_0$) by adding a finite number (one new guest), adding an infinite quantity (infinitely many new guests), or by multiplying by any infinite quantity (infinitely new buses with infinitely many guests each). Put simply, this shows that $\aleph_0 + n = \aleph_0 \cdot n = \aleph_0^n = \aleph_0$ for all $n$ in the natural numbers. I know that you can get from $\aleph_0$ to $\aleph_1$ by taking $2^{\aleph_0}$, but what I want to know is if it is possible for tetration, or any similar operators that are extensions of multiplication/exponentiation/tetration? I am almost certain the answer is no, since I don't believe any of those operators can surpass the speed of exponentiation as they tend towards infinity, but I haven't seen it explicitly discussed anywhere and I would like to be sure.","['cardinals', 'elementary-set-theory']"
1637318,Factorial of a matrix: what could be the use of it?,"Recently on this site, the question was raised how we might define the factorial operation $\mathsf{A}!$ on a square matrix $\mathsf{A}$. The answer , perhaps unsurprisingly, involves the Gamma function . What use might it be to take the factorial of a matrix?  Do any applications come to mind, or does this – for now* – seem to be restricted to the domain of recreational mathematics? (*Until e.g. theoretical physics turns out to have a use for this, as happened with Calabi–Yau manifolds and superstring theory ...)","['matrices', 'matrix-calculus', 'operator-theory', 'applications', 'factorial']"
1637343,Distribution of product of bernoulli random variable and poisson random variable,"There are random variable $Z=XY$ ($X$ is poisson and $Y$ is bernoulli) $$X(n;\lambda) = \frac{\lambda^n}{n!}e^{-\lambda}$$ $$Y=\begin{cases} & \beta \text{ with probability } \beta \\  & 0 \text{ probability } 1-\beta \end{cases}$$ I likes to know distribution of product of bernoulli random variable and poisson random variable. So i calculate MGF(Moment Generating Function) so i can get below expression. $$M_{z}(t)=\sum_{y}\sum_{x}e^{txy}P(x)P(y)=\sum_{x}(\beta e^{t\beta x}+1-\beta) P(x) 
=\beta e^{-\lambda}(e^{\lambda e^{t\beta}})+1-\beta = \beta(e^{\lambda(e^{t\beta -1})}+1-\beta)$$ I can`t obtain pmf(probability mass function) from this calcultaed MGF Is it impossible obtain pmf ? Or is there any technique obtaining probability in this case? Thank you","['probability', 'probability-distributions']"
1637351,Interesting properties of a mathematical number theory game,"The game, which is purely recreational, goes as follows: Starting out with 1, you can employ any of two different generation rules: You can multiply by 3 You can divide by two, rounding up (e.g. 3 divided by 2 is 2) Solving the game, i.e. showing that you can reach every number is another interesting problem, although trivial in comparison to the property I am trying to define: complexity. Say the complexity of an integer N is the shortest sequence of rules needed to reach N . As an example, the complexity of 9 is 2. The derivation has the following notation: 1,3,9. So we write C(9)=2. There are some numbers, namely all powers of three, which have a low complexity. But even small numbers such as 4 or 10 are already very complex. I need your help in researching the complexity of a number and its inherent properties (lower, upper bounds). The randomness seems to be insane, mostly stemming from the ""round-up"" rule, which yields unpredictable results. I leave you with some derivations so that you can get a feel for how you reach a number: (7): 1,3,9,27,14,7.
(5): 1,3,9,5.
(11): 1,3,9,27,14,7,21,11.
...´ I will be monitoring the comments for additional information requests.","['number-theory', 'algorithmic-game-theory']"
1637367,Classification of homomorphisms $\mathbb Q \to \mathbb C^\times$,Are there any textbooks which discuss/classify the injective group homomorphisms from $\mathbb Q$ (under addition) into $\mathbb C \setminus \{0\}$ (under multiplication)?,"['abstract-algebra', 'book-recommendation', 'reference-request', 'complex-numbers']"
1637405,"Does there exist a surjective homomorphism from $(\mathbb R,+)$ to $(\mathbb Q,+)$ ?","Does there exist a surjective homomorphism from $(\mathbb R,+)$ to $(\mathbb Q,+)$ ?  ( I know that there 'is' a 'surjection' , but I don't know whether any surjective homomrophism from $\mathbb R$ to $\mathbb Q$ exist or not . Please help . Thanks in advance )","['abelian-groups', 'group-homomorphism', 'infinite-groups', 'group-theory']"
1637409,What does the integral of position with respect to time mean?,"The integral of acceleration with respect to time is velocity.
The integral of velocity with respect to time is position. What is the integral of position with respect to time, and what does it mean? Please explain so that your answer is understandable by someone who took calculus I.","['intuition', 'physics', 'integration', 'calculus']"
1637412,Would this solution of the limit of the sequence be correct?,"Let's suppose that I have the sequence $a_n = \frac{1}{n^2} + \frac{2}{n^2} + \frac{3}{n^2} + \ldots + \frac{n}{n^2}, n \in \mathbb{N}$. And I have to find the limit of the sequence as $n \rightarrow \infty$. Would the below solution be correct? The sequence $a_n$ can be rewritten as
\begin{align}
a_n &= \sum_{k=1}^{n} \frac{k}{n^2} \\
&= \frac{1}{n^2} \sum_{k=1}^{n} k \\
&= \frac{1}{n^2} \cdot \frac{n(n+1)}{2} \\
&= \frac{n+1}{2n}.
\end{align} Thus we have
\begin{align}
\lim_{n \rightarrow \infty} a_n = \lim_{n \rightarrow \infty} \frac{n+1}{2n} = \lim_{n \rightarrow \infty} \frac{n}{2n} = \frac{1}{2}.
\end{align}","['sequences-and-series', 'proof-verification', 'limits']"
