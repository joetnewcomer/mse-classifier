question_id,title,body,tags
2607530,Morse index of minimal surfaces in $\mathbb{R}^3$,"I am wondering where can I find the Morse index of the most famous examples of minimal surfaces in $\mathbb{R}^3$, such as the cathenoid, the helicoid, etc. Is there any general standard technique to estimate the Morse index of a minimal hypersurface in $\mathbb{R}^n$? I have briefly checked the book A Course in Minimal Surfaces , by Colding and Minicozzi, but I couldn't find an answer to my questions. Any help or reference will be very much appreciated!","['minimal-surfaces', 'differential-geometry', 'morse-theory']"
2607531,"70% of the population drinks beer, 60% drinks vodka. What percent of the population drinks both?","Those are the only givens, no restriction as to what percentage of the population drinks neither beer nor vodka, but no given as to whether the whole population must drink either of the two. I'm no mathematician, but in my lay opinion, I visualized this as a ""slider"" i.e. BBBBBBB---
VVVVVV---- Where B is beer, '-' is nothing, and V is vodka. In the above configuration, 60% of the population drinks both. However, on the other extreme ---BBBBBBB
VVVVVV---- 30% of the population drinks both. Is it safe to say that 30-60% of the population drinks both beer and vodka? (Friends have explained this to me in a Venn diagram, and they insist that 30% is the only correct answer. I understand that their Venn diagram looks exactly like my ""other extreme"" visualization, but why is it the only correct answer ?). Edit: It's also been argued that it can't be answered because of ""not enough data"", but I propose that it can be answered, just not discretely (IDK if that's the correct term, but I mean ""definitely 30 or bust"").",['elementary-set-theory']
2607539,Contour integration with absolute value,"I am trying to use contour integration to evaluate the integral
$$
\int_\mathbb{R}\frac{|x|e^{i\omega x}}{x^2+ a}dx \,,
$$
where $a>0$ and $\omega$ can take any real value. I am worried about the existence of the absolute value and I am not sure how to proceed. Any help appreciated. EDIT: Contour integration might not be the way to go here (see comments). In any case, it is easy to see that the integral in question is also equal to
$$
2\int_0^\infty \frac{x \cos(\omega x)}{x^2+ a}dx \,,
$$
and I am wondering whether anyone can produce a solution regardless of the method used.","['complex-integration', 'complex-analysis', 'improper-integrals', 'integration', 'contour-integration']"
2607578,Number of binary sequences with no consecutive ones.,"So I've got a problem that says ""How many ways are there go give $k$-identical biscuits to $n$-different children if each child gets at least one biscuit?"" I figured I'd do it using binary sequence. My approach is using $1$'s as children (separators) and $0$'s as biscuits. Since every child must get at least one biscuit then the sequence can't start with one (there would be no $0$'s ""biscuits"" to the left) and there can't be $1$'s next to each other (some $1$ ""child"" would get no biscuit). Now I already know the answer to that problem and it is $\binom{k-1}{n-1}$ and my problem is understanding why it is $k-1$ and not $k$. Clearly $n-1$ is because we only need $n-1$ separators to have $n$ divisions but why is it not $k$ instead of $k-1$? I think we have $k$ places for $1$'s to place and not $k-1$.","['combinations', 'combinatorics', 'binary']"
2607634,To show $a$ is removable singularity of $f$,"Let $X,Y$ be two compact Riemann surfaces. Let $a \in X$ and $f : X-\{a\} \to Y$ be a injective holomorphic map. Prove that $a$ is removable singularity of $f$. I want to apply Riemann removable singularity theorem somehow but I am unable to get it. Any help will be appreciated.",['complex-analysis']
2607640,A universal bound on expectation $E[X^ke^{-X}]$,"My friend introduced me to association inequalities for expectation. Namely: If $f$ is monotonically increasing and $g$ is monotonically decreasing, then for any random variable $X$
$$E[f(X)g(X)] \leq E[f(X)]E[g(X)]$$ provided the expectations are well defined. As an example, we can show for $k\geq 1$
$$E[X^ke^{-X}] \leq E[X^k]E[e^{-X}]$$ The Cauchy Schwarz inequality would give a looser bound in this case. Now for the above example, I wondered if it was possible to prove the following stronger claim:
Does there exist a universal constant $C_k$ such that for $X$ having ANY distribution,
$$E[X^ke^{-X}] \leq C_kE[e^{-X}]$$
? My understanding as of now is that it does exist but not universally (i.e. not distribution independent). But I couldn't come up with a counter for this? I'd appreciate it if someone could throw some light on this. If necessary we may assume that $X$ is non negative. The reason I'm interested in such a bound is that there are cases where the kth moment of $X$ is infinity but $E[X^ke^{-X}]<\infty$. Update: The question has been successfully answered. However I wanted to make a note that $E[X^ke^{-X}] \leq k^ke^{-k}$ for $X\geq 0$.","['expectation', 'probability']"
2607641,Sum of distances of points,"Given 3 points on a plane, all 3 at the same semi-plane defined by a line (e), find a point P on the line (e) for which the sum of lengths of the 3 segments that are defined (by each of the 3 points and the point on the line), is minimal. I think that we must draw the projections of each of the 3 initial points A, B, C, say, A', B' and C' and then take the middle point M of A'B', then the middle point N of MC' and point N is the one we ask - or something like this.
But even if it is correct, I don't know how to prove it :(
Any ideas are much appreciated!",['geometry']
2607699,Group cohomology of $\mathrm{GL}(V)$,"Let $K$ be a field not isomorphic to $\mathbb F_2$, $V = K^n$ - vector space over $K$ on which $\mathrm{GL}(V)$ acts. How to compute cohomology groups $H^i(\mathrm{GL}(V),V)$? It is easy to see that $H^0(\mathrm{GL}(V),V) = 0$ but what about other cohomology groups?","['group-cohomology', 'homological-algebra', 'linear-algebra']"
2607704,Show that $g \circ f$ is n times differentiable,"Let $D,E \subseteq \mathbb{R}$ and $f: D \to E, g: E \to \mathbb{R}$ be two $n$ times differentiable functions. Then $g \circ f: D \to \mathbb{R}$ is $n$ times differentiable. I was thinking about using induction over $n$. I know the base case with $n=1$ is true and assume it is true for $n$. But I struggle to show the induction step $(g \circ f)^{(n+1)} := \Big( (g \circ f)^{(n)} \Big)' = \ldots$ If possible I want to avoid using the concrete formula for the n-th chain rule because it is quite complicated.","['derivatives', 'real-analysis', 'chain-rule']"
2607742,Solving the heat equation with robin boundary conditions,"I have a coupled non-dimensional diffusion system in $v(z,\tau)$ , formulated by the following equations \begin{align}
\frac{\partial v}{\partial \tau} &= \Delta\frac{\partial^2 v}{\partial z^2},
%
\qquad &\text{for}\ z\in[0,1],\ \tau>0 \\ 
%%%
\frac{\partial v}{\partial z} &= Ev,
%
\qquad &\text{for}\ z=0,\ \tau>0,\\ 
%%%
\frac{\partial v}{\partial z} &= -D v,
%
\qquad &\text{for}\ z=1,\ \tau>0
\end{align} where $\Delta,E,D>0$ We next proceed with separation of variables, let \begin{align}
v = Z(z)T(\tau)
\end{align} Substitution yields the following \begin{align}
\frac{1}{\Delta}\frac{\dot{T}}{T} &= \frac{Z''}{Z} = -\lambda^2
\end{align} Therefore we find \begin{align}
T &\propto \exp{\left(-\Delta\lambda^2\tau\right)},\\
Z &= a \cos(\lambda z) + b\sin(\lambda z),\\
Z' &= \lambda \left( b\cos(\lambda z) -a \sin(\lambda z) \right)
\end{align} WLOG we may set $a=1$ , as we will later take a linear superposition of these solution functions. Therefore we have \begin{align}
Z &= \cos(\lambda z) + b\sin(\lambda z),\\
Z' &= \lambda \left( b\cos(\lambda z) - \sin(\lambda z) \right)
\end{align} Therefore via our boundary condition at $z=0$ we find \begin{align}
\lambda b &= E
\quad\Rightarrow\quad
b = \frac{E}{\lambda}
\end{align} and via our second \begin{align}
\lambda \left( \frac{E}{\lambda}\cos(\lambda) - \sin(\lambda) \right) &= -D\left(\cos(\lambda) + \frac{E}{\lambda}\sin(\lambda)\right)\\
%%%
\Rightarrow\quad
E\lambda\cos(\lambda) - \lambda^2\sin(\lambda) &= -D\cos(\lambda) - ED\sin(\lambda)\\
%%%
\Rightarrow\quad
\left(E\lambda+D\right)\cos(\lambda)
&=
\left( \lambda^2- ED\right)\sin(\lambda)\\
%%%
\Rightarrow\quad
\tan(\lambda)
&=
\frac{E\lambda+D}{\lambda^2- ED}
\end{align} This has countably infinite solutions $\lambda_i$ for $i\in\mathbb{N}$ . Therefore we have the following solution for $v(z,\tau)$ \begin{align}
v(z,\tau) &=
\sum_{i=1}^\infty C_n
\left(
\cos(\lambda_i z)
+
\left(\frac{E}{\lambda_i}\right)\sin(\lambda_i z)
\right)
\exp{\left(-\Delta\lambda_i^2\tau\right)}
\end{align} Therefore at $\tau=0$ \begin{align}
v(z,0) &=
\sum_{i=1}^\infty C_i
\left(
\cos(\lambda_i z)
+
\left(\frac{E}{\lambda_i}\right)\sin(\lambda_i z)
\right)
= 1
\end{align} How can I find $C_i$ ?. EDIT: If we define $Z_i(z)$ as follows \begin{align}
Z_i(z) = 
\cos(\lambda_i z)
+
\left(\frac{E}{\lambda_i}\right)\sin(\lambda_i z)
\end{align} then am I correct in thinking we use the following relation to find $C_i$ ? \begin{align}
\int_0^1 Z_i(z)Z_j(z) \text{d}z = c_i\delta_{ij}
\end{align} I'm not sure if this is the case, see this link . Does this mean my spatial basis is not orthogonal?","['partial-differential-equations', 'heat-equation', 'linear-pde', 'mathematical-modeling', 'ordinary-differential-equations']"
2607773,"Do there exist distinct prime numbers $p,q,r$ such that $\frac{p+q+r}{3}; \frac{p+q}2; \frac{r+q}2; \frac{p+r}2$ are prime numbers.","Do there exist distinct prime numbers $p,q,r$ such that 
$$\frac{p+q+r}{3}; \frac{p+q}2; \frac{r+q}2; \frac{p+r}2$$
are prime numbers. My work: I found $(p,q,r)=(5,17,29)$. Are there any other examples?","['algebra-precalculus', 'prime-numbers']"
2607796,How to compute $\int_{0}^{1}\left (\frac{\arctan x}{1+(x+\frac{1}{x})\arctan x}\right )^2dx$,"I want to calculate the following integral $$\int_{0}^{1}\left(\frac{\arctan x}{1+(x+\frac{1}{x})\arctan x}\right)^2 \, dx$$ But I have no way to do it, can someone help me, thank you.","['integration', 'trigonometric-integrals', 'calculus']"
2607873,A problem in using theorem for finding limit,"Let $a_n = \arctan ( \ln n )$ , find $\lim_{ n \to \infty} a_n$ if any . I think we can't apply the theorem (i.e $\lim_{ n \to \infty} \arctan ( \ln n ) = \arctan (\lim_{ n \to \infty} \ln n) = \pi /2 )$ . The condition of theorem is $a_n \to L$  but here $a_n$ diverges to infinity . Although , we know that $\pi / 2$ is the right answer since $\lim_{x \to \infty} \arctan x = \pi /2  $ and instead of $a_n$ we can put any other sequence that diverges to infinity .","['sequences-and-series', 'limits-without-lhopital', 'limits']"
2607896,Cubic surfaces and 27 lines,It is well known that a smooth cubic surface will contain 27 lines. All the references I came across gives a proof of the fact for fields of Characteristic $\neq 2$. Is there a reference where the characteristic 2 case is proved?,"['reference-request', 'cubics', 'algebraic-geometry']"
2607922,"Find the extreme points of the function $g(x):=(x^4-2x^2+2)^{1/2}, x∈[-0.5,2]$","I need to find the extreme points of the function $$g(x):=(x^4-2x^2+2)^{1/2}, x∈[-0.5,2]$$
I first found $$f'(x)=\frac{{4x^3-4x}}{2\sqrt {x^4-2x^2+2}}$$ and made $f'(x)=0$ to find all the roots of the function, $x_1=0, x_2=1, x_3=-1$ but since $x_3$ is out of the domain I didn't consider it. Now I have $4$ candidates for the extreme points for this function, namely $x_1, x_2, r_1=-0.5, r_2=2$, where $r_1, r_2$ are the ends of the domain. I then put these candidates back into $f(x)$ and found that $$f(x_2)<f(r_1)<f(x_1)<f(r_2)$$ showing that $x_2$ is the global minimum and $r_2$ is the global maximum. But I can't seem to figure out the local maximum and local minimum of the function. I tried making a sign table for the function: But I have no idea how to determine that $x_1$ is the local maximum and $r_1$ is the local minimum. PS - Sorry for the terrible sign graph, I had to use an online graphing tool.","['derivatives', 'real-analysis', 'maxima-minima', 'calculus']"
2607924,"Cardinality of $\{\emptyset, \mathcal{P}(\emptyset), \{\emptyset\} \} $","This is an old exam question from ""Diskrete Mathematik"" at ETH Zurich $\mathcal{P}(A)$ denotes the Powerset, which is the set of all subsets of A: $\mathcal{P}(A) := \{S|S\subseteq A \}$. I believe that  $\{\emptyset, \mathcal{P}(\emptyset), \{\emptyset\} \} $ since
$\mathcal{P}(\emptyset) = \{\emptyset\} $ can be reduced to $\{\emptyset, \{\emptyset\} \} $ and thus the cardinality of said set would be 2. However, the inofficial solution is 3.",['elementary-set-theory']
2607930,Calculating limit $\lim\limits_{x\to\infty}\frac{3x^2-\frac{3}{x^2+1}-4f'(x)}{f(x)}$ for an unknown function.,"Given that $f(x)$ is a continuous function and satisfies $f'(x)>0$ on $(-\infty,\infty)$ and $f''(x)=2 \forall x \in(0,\infty)$. We need to find the limit $$\lim_{x\to\infty}\frac{3x^2-\frac{3}{x^2+1}-4f'(x)}{f(x)}$$ Now the numerator is tending to infinity so denominator must also go to infinity else limit won't exist.So I tried the L'Hospitals rule and it became$$\lim_{x\to\infty}\frac{6x+\frac{6x}{(x^2+1)^2}-4f''(x)}{f'(x)}$$The numerator is still infinity so once again applying L'Hospitals rule (assuming denominator must still be infinity) we get $$\lim_{x\to\infty}\frac{6+\frac{6(x^2+1)^2-6x×2(x^2+1)×2x}{(x^2+1)^4}+0}{f''(x)}$$ Now putting $f''(x)=2$ we get $$3+\lim_{x\to\infty}\frac{3(x^2+1)^2-12x^2(x^2+1)^2}{(x^2+1)^4}$$ Collecting the coefficients of $x^4$ from numerator and denominator we get the limit to be$3-9=-6$ but the answer is not -6. Is applying LHospital wrong?Help. Thanks.","['real-analysis', 'limits', 'functions', 'calculus', 'analysis']"
2607932,"Continuity of the function $f(x)=\lim\limits_{n \to \infty}[\lim\limits_{t\to 0}[\frac{\sin^2 (n!\pi x)}{\sin^2(n! \pi x)+t^2}]] $, $x \in \mathbb R$","Consider the Question.  I think that f(x)=1 $\forall x \in \mathbb {R}$. Reason: As t $\to$ 0 , $[\frac{\sin^2 (n!\pi x)}{\sin^2(n! \pi x)+t^2}]$   $\to[\frac{\sin^2 (n!\pi x)}{\sin^2(n! \pi x)}]= 1$. So it is differentiable in $\mathbb R$.
But the explanation given is the following. Now my questions are as follows: 1) Am I right? if not, in what way? 2) Whether the explanation given in the picture is correct? 3)Can I interchange Limits? if So under what conditions. Thanks in advance.","['derivatives', 'real-analysis', 'continuity', 'limits']"
2607945,"For a surjective function $f: A \rightarrow A$, if there exist $n$ such that $\ker f^n = \ker f^{n+1} = \dots$ Is f injective?","For a surjective function $f: A \rightarrow A$, where $A$ is a ring, if there exists $n$ such that $\ker f^n = \ker f^{n+1} = \dots$ Is f injective?
(A is a ring)","['ring-theory', 'functions']"
2607948,What is the second derivative of $f^{-1}(g(x))$?,"What is the second derivative of $f^{-1}(g(x))$? $f^{-1}$ is the inverse of $f$: The first derivative of $f^{-1}(x)$ is given by
$\frac{1}{f'(f^{-1}(x))}$ The second derivative of $f^{-1}(x)$ is given by
$-\frac{f''(f^{-1}(x))}{[f'(f^{-1}(x))]^3}$ The second derivative of $f(g(x))$ (by the chain rule) is given by $g'(x)^2\cdot f''(g(x))+g''(x) \cdot f'(g(x))$.
Substituting the derivatives into this equation, we have that the second derivative of $f^{-1}(g(x))$ is $$-\frac{g'(x)^2\cdot f''(f^{-1}(g(x)))}{[f'(f^{-1}(g(x)))]^3}+\frac{g''(x)}{f'(f^{-1}(g(x)))}$$ Which is the same as $$\frac{g''(x)*[f'(f^{-1}(g(x))]^2-g'(x)^2*f''(f^{-1}(g(x)))}{[f'(f^{-1}(g(x)))]^3}$$ Is this correct?","['functional-analysis', 'real-analysis', 'calculus', 'derivatives']"
2608022,Volume between a sphere and a cone,"Find the volume between $r=R\,$ (sphere) and $\theta=\alpha\,$ (cone) , for $\theta$ and $r $ constants such that -  $0 < \theta < \dfrac{\pi}{2}$ I am sorry for the question being basic, but i couldn't find similar questions on Math Exchange. I can't figure what my integration limits suppose to be in order to solve this in a spherical coordinate system - $\phi$ is obviously from $0$ to $2\pi$, $\theta$ from $0$ to $\alpha$ but how can i bound $r$ ? Thank you !","['multivariable-calculus', 'spherical-coordinates']"
2608032,"$\text{Spec}~\mathbb C[x,y]/(xy)$ is connected in Zariski topology","I want to prove that $\text{Spec}~\mathbb C[x,y]/(xy)$ is connected w.r.t. Zariski topology.(so that it is an example reducible space but connected under Zariski topology). Intuitively, $\text{Spec}~\mathbb C[x,y]/(xy)$ is the ""union of $x$-axis and $y$-axis"" and clearly ""connected"", but I wish to prove this rigorously in the Zariski topology. To begin with, we identify $\text{Spec}~\mathbb C[x,y]/(xy)$ with the subspace of $\mathbb A^2_\mathbb C$ containing $(xy)$ and write it as a disjoint union of $V(I)$ and $V(J)$. But I get stuck in here and don't know how to continue. I wish the solution doesn't use theorems that are too advanced.","['algebraic-geometry', 'commutative-algebra']"
2608036,"A question on Group of homeomorphism of $[0,1]$.","Consider $[0,1]$ with the usual euclidean topology. Now $G$ be the set of homeomorphisms from $[0,1]$ onto $[0,1]$. $G$ forms a group under composition. Now, Let $F=\{ f\in G | f(0)=0 \}\ $. Now $F$ is a normal subgroup of $G$. The problem is the following. Suppose $g\in F$ has exactly one fixed interior point. I need a homeomorphism $h \in F$ which is conjugate to $g$ in $F$ such that either of the following happens: $$  h(x)>g(x)  \hspace{4 cm} \forall x\in(0,1)$$ 
$$  h(x)>g^{-1}(x)  \hspace{3.5 cm} \forall x\in (0,1)$$ I have no idea how to come up with such a homeomorphism satifying either of the two properties and also being conjugate to $g$ in $F$. I suppose that $g$ having exactly one fixed interior point is crucial but I am not sure how to use it! Thanks in advance for any kind of help!","['general-topology', 'real-analysis', 'topological-groups', 'group-theory']"
2608048,Time derivative of a pullback of a time-dependent 2-form,"I've been reading McDuff's Introduction to symplectic topology book and I couldn't figure out how to justify the last line in this image(p108): Specifically, the identity
$$\frac{d}{dt}\psi^*_t \omega_t = \psi^*_t (\frac{d}{dt}\omega_t + \iota(X_t)d\omega_t + d\iota(X_t)\omega_t)$$ When $\omega_t = \omega$ is constant with respect to $t$, this reduces to the well-known Cartan's formula:
$$\frac{d}{dt}\psi^*_t \omega = \psi^*_t (\iota(X_t)d\omega + d\iota(X_t)\omega)$$ My question is this: how would one derive/justify the generalization?","['lie-derivative', 'riemannian-geometry', 'differential-forms', 'symplectic-geometry', 'differential-geometry']"
2608074,"$f_{n} \in L^{p}(X),$ such that $\lVert f_{n}-f_{n+1}\rVert_{p} \leq \frac{1}{n^2}$. Prove $f_{n}$ converges a.e.","Here's the problem at hand: Let $(X,\mu)$ be a measure space, let $f_{n} \in L^{p}(X)$ for each $n$ and some $p>1$ , such that $\lVert f_{n}-f_{n+1}\rVert_{p} \leq \frac{1}{n^2}$ . If $A_{n} = \left\lbrace x \in X : \lvert f_{n}(x)-f_{n+1}(x)\rvert \geq \frac{1}{n} \right\rbrace$ , prove that: $\mu(A_{n}) \leq \frac{1}{n^p};$ $\mu\mathopen{}\left(\overline{\lim}A_{n}\right)\mathclose{}=0;$ $f_{n}$ converges a.e. I've done 1. and 2, but I can't finish 3. Here's the proof of 1. and 2: follows from $\frac{1}{n^p} \mu(A_{n}) = \int_{A_{n}}\lvert f_{n}-f_{n+1}\rvert^p d\mu \leq \int_{X} \left\lvert f_{n}-f_{n+1}\right\rvert^p d\mu \leq \frac{1}{n^{2p}}$ . For 2, note that $\overline{\lim}A_{n} = \bigcap_{n=1}^{\infty} \bigcup_{k=n}^{\infty} A_{k} =: \bigcap_{n=1}^{\infty} B_{n}$ . $B_{n}$ is a decreasing sequence, and $\mu(B_{1}) \leq \sum_{n=1}^{\infty} \frac{1}{n^p} < +\infty$ , so by continuity from above, we get $ 0 \leq \mu\mathopen{}\left(\overline{\lim}A_{n}\right)\mathclose{} = \lim_{n\to \infty} \mu(B_{n}) \leq \lim_{n\to \infty} \sum_{k=n}^{\infty} \frac{1}{n^p} = 0$ . Now, I also note that $\lVert f_{m} - f_{n}\rVert_{p} \leq \sum_{k=n+1}^{m} \frac{1}{k^2} < \varepsilon$ , for sufficiently large $m, n$ , and arbitrary $\varepsilon$ , so $f_{n}$ is Cauchy in $L^p$ so it's convergent in $L^p$ ; so now, as a corollary, there exists an $f$ such that $f_{n} \xrightarrow{\mu} f$ , and so we have a subsequence $f_{n_{k}}$ which converges to $f$ a.e. How do I prove that the whole sequence has to converge to $f$ a.e?","['lp-spaces', 'measure-theory']"
2608117,Scheme theoretic definition of a vector bundle,"I have started reading the book ""Lectures on Vector Bundles"" by J. Le Potier, where at the very beginning he gives the following two definitions, which are very familiar in smooth or holomorphic settings: $X=$variety over $\mathbb{C}$ - separated finite type scheme over $\mathbb{C}$.
""Points"" in this book mean closed points. Definition 1 . Let $X$ be an algebraic variety. A (complex) linear fibration over X is given by a surjective morphism of algebraic varieties $p: E \to X$, where for each point $x\in X$, $p^{-1}(x)$ has the structure of complex vector space. Definition 2. An algebraic vector bundle of rank $r$ is a linear fibration $E \to X$ which is locally trivial, that is for each point $x\in X$ there exists an open neighbourhood $U$ with an isomorphism of fibrations $E|_U \to U \times \mathbb{C}^r$. However, in this algebraic case I have the following confusion.: What exactly does the structure of vector space mean in this case? Say if we take $\mathbb{C}^n$, scheme-theoretically this should be taken as $Spec \ \mathbb{C} [x_1,...,x_n]$, and on closed points we indeed have the vector space structure. But what about non-closed points, do we care about them at all? So is it correct to understand the first definition in the sense that each (scheme-theoretic) fiber over closed point $x$ is isomorphic to $Spec \ \mathbb{C}[x_1,...,x_n]$? Also what about fibers over non-closed points, do I require something from them as well? Another problem arises when the author starts treating associated bundles. Say for the Whitney sum, he takes $E \oplus F = \coprod_{x\in X} (E_x \oplus F_x)$ as sets, and then locally induces the structure of algebraic variety by considering bijection $(E \oplus F)|_{U_i} \to U_i \times (\mathbb{C}^r \oplus \mathbb{C}^s)$, given by $\phi_i \oplus \psi_i$ where the latter are corresponding trivializations for $E$ and $F$. But what does $\mathbb{C}^r \oplus \mathbb{C}^s$ mean rigorously? Is it just $\mathbb{C}^{r+s} = Spec \ \mathbb{C} [x_1,...,x_r, y_1,...,y_s]$? But then it is a well known fact, that $Spec \ \mathbb{C} [x_1,...,x_r, y_1,...,y_s]$ has more points than direct product as sets $Spec \ \mathbb{C}[x_1,..,x_r] \times  Spec \ \mathbb{C}[y_1,...,y_s]$ (thus more points than $E_x \oplus F_x$ for that matter). So $\phi_i \oplus \psi_i$ can only be a bijection if one restricts to closed points. So if I wanted to keep track of non-closed points, how should I modify the definition of $E \oplus F$ as sets? The only rigorous solution which comes to my mind is to  directly glue this scheme using the ""direct sum"" cocycle, starting from $U_i \times \mathbb{C}^{r+s}$ as constituent schemes. I am aware that this approach of describing vector bundles should ultimately be the same as using locally free sheaves. The proof given in the book isn't rigorous enough for my tastes because of the aforementioned complications. Is there a source describing this approach to vector bundles and related constructions more rigorously?","['vector-bundles', 'algebraic-geometry']"
2608149,Intuitive proof of Lyapunov's theorem,"I have recently come across Lyapunov's stability theorem, which states: Let $0$ be an equilibrium of a differential autonomous system $X'=f(X)$ where $f$ is smooth. Suppose $V$ is a Lyapunov function of the system in some neighborhood of $0$ ; that is, $V(X(t))$ is non-increasing on $t$ for every solution $X$ of the system, and furthermore suppose that $V$ is definite positive. Then $0$ is a stable equilibrium. Sadly, all the proofs I have seen feel rather obscure to me. Do you know of any good, intuitive proof of the theorem? (or would you be so kind to outline one yourself?)","['stability-in-odes', 'ordinary-differential-equations', 'stability-theory']"
2608201,Compute the limit without L'Hospital's rule,"I'm not sure how to handle the trig functions with different arguments when computing this limit using L'Hospital's rule. $$\lim_{x \rightarrow 0} \frac {x^2\cos(\frac {1} {x})} {\sin(x)}.$$ I have come up with the correct numerical answer via a different method, but am unsure if the logic would hold true for all cases (maybe I arrived at the correct answer by chance). Here is my working: Let $g(x)=x^2\cos(\frac 1 x)$ and $h(x) = \frac 1 {\sin x} = \csc x$. We know that the following holds true for all $x$: $$-1 \le \cos (\frac 1 x) \le 1$$
  Since $x^2 \ge 0$ for all x:
  $$-x^2 \le x^2\cos (\frac 1 x) \le x^2$$
  Taking limits as $x \rightarrow \infty$ gives:
  $$ \lim_{x\rightarrow 0}(-x^2)\le \lim_{x\rightarrow 0}(x^2\cos (\frac 1 x)) \le \lim_{x\rightarrow 0}(x^2)$$
  By the sandwich rule (or squeeze theorem):
  $$ 0 \le  \lim_{x\rightarrow 0}(x^2\cos (\frac 1 x)) \le 0$$
  $$ \Rightarrow \lim_{x\rightarrow 0}(x^2\cos (\frac 1 x)) $$
  And hence, due to the algebra of limits:
  $$\lim_{x \rightarrow 0} \frac {x^2\cos(\frac {1} {x})} {\sin(x)} = 0.$$","['limits-without-lhopital', 'limits']"
2608228,$SU(2)$ adjoint and fundamental transformations,"I have to demonstrate that the transformation from $\mathbf{\pi}=(\pi_1,\pi_2,\pi_3) \rightarrow U \mathbf{\pi}$ with $U= \exp(i \alpha^a \mathbb{T}^a)$ and $\mathbb{T}^a$ the adjoint representation of $SU(2)$ is equivalent to the transformation $U \frac{\pi^a\tau^a}{2}U^{-1}$ with $U = \exp(i\alpha^a\tau^a/2)$.
I can easiliy do it at infinitesimal level (show that they transform as vectors) but i don't know how at ""global"" level. The adjoint is equivalent to the fundamental of $SO(3)$ so is just a rotation in space, but with the other one?","['representation-theory', 'group-theory', 'lie-algebras', 'lie-groups']"
2608237,What is the connection between the invertibility of a matrix the kernel of the matrix,"If the kernel of a matrix A only has the zero vector (ker(A) = {0}) Then why is the matrix A considered invertible because of that? I know that if ker(A) = 0, then there is only one unique solution of the matrix( assuming that the matrix is made up of a system of linear equations) but what does this tell me about whether or not the matrix is singular? Can someone explain the logic behind this?","['matrices', 'linear-algebra']"
2608354,Spectral decomposition of some special matrix,"Let $A_{n\times n} = aI+bJ$, where $I$ is the identity matrix and $J$ is the matrix of all ones. Is it possible to find the expression of $A^{1/2}$ such that $A^{1/2}A^{1/2} = A$? In particular $A = I_{n\times n} - \frac{(1-\alpha)}{n+\alpha(2-n)}J_{n\times n}$, where $0<\alpha<1$.",['matrices']
2608368,What is $\lim\limits_{n \to 0} \frac{d}{dx} \frac{1}{n} x^n$?,"A limit that I find rather intriguing is $\lim\limits_{n \to 0} \frac{d}{dx} \frac{1}{n} x^n$. Following the usual rules for differentiation of polynomials, this would be  $\lim\limits_{n \to 0} \frac {nx^{n-1}}{n} = x^{-1}$. It seems unlikely that this is actually the limit because the derivative of $ln(x)$ is $1/x$. Is there a way to prove what this actually is?","['calculus', 'limits']"
2608371,Is the quotient of a finitely generated module finitely generated?,Is the quotient of a finitely generated module finitely generated ? I have difficulties to find a counterexample... or to prove that it is finitely generated.,"['abstract-algebra', 'modules']"
2608398,Jacobson radical of formal power series over an integral domain,How can I calculate the Jacobson radical of R[[x]] when R is an integral domain? It's easy to prove J(R[x])=0 when R is an integral domain.,"['abstract-algebra', 'ring-theory', 'formal-power-series', 'integral-domain']"
2608467,Curves of genus 0,"There are two properties of (projective, geometrically integral, and regular) curves of genus 0: If the curve has a $k$-point, then it is isomorphic to $\mathbb{P}_k^1$ where, if $P$ is a $k$-point, the line bundle $\mathscr{L}(P)$ defines a closed embedding into $\mathbb{P}_k^1$. All genus 0 curves can be described as conics in $\mathbb{P}_k^2$ where the degree 2 line bundle $\omega_C^{\vee}$ defines a closed embedding into $\mathbb{P}_k^2$. This is mentioned, for example, in Vakil's notes section 19.3. But $\mathbb{P}_k^1$ is not a conic in $\mathbb{P}_k^2$. What's going on here? Can you actually embed $\mathbb{P}_k^1$ as a conic in $\mathbb{P}_k^2$?","['algebraic-curves', 'algebraic-geometry']"
2608485,Approximate x+1 without addition and logarithms,"I am looking for an expression $f(x)$ involving only constants, $x$, multiplication, exponentiation, and division such that for in as large an interval [1,a) as possible, the function closely approximates $g(x)=x+1$. Or maybe even a recursive sequence of increasingly good approximations $f_i(x)$. $f(x)$ should not include any addition and logarithms. Allowed operators are: $$ x^{n}, nx, \frac {x}{n}$$ where $n$ can be $x$ or $const$.  Any composition of these operators is  allowed. Here is something I tried ( graph ): $$ f(x) = 1.8 (x^{\frac{1}{2}}) (x^{\frac{1}{2*3}})^{(x\frac{1}{2*3})} (x^{\frac{1}{2*3*5}})^{{(x\frac{1}{2*3*5})}^{(x\frac{1}{2*3*5})}}
$$ But continuing this pattern does not make $f(x)$ any closer to $g(x)$.","['algebra-precalculus', 'recurrence-relations', 'sequences-and-series', 'approximation']"
2608544,What is the physical interpretation of homogeneous?,"In ode's and pde's we pay great attention as to whether the equations are homogeneous or nonhomogeneous. I remember learning in my first ODE class that for the general linear ode $$a_n(x)\frac{d^ny}{dx^n}+a_{n-1}(x)\frac{d^{n-1}y}{dx^{n-1}}+\cdots+a_1(x)\frac{dy}{dx}+a_0(x)y=g(x),$$ that $g(x)$ takes on some very important physical meanings in engineering problems but I can't remember what they are. And in general, could someone provide an interpretation of the physical meaning of g(x) in odes and in pdes? If your examples are only from famous and specific equations then that's welcome too.","['homogeneous-equation', 'ordinary-differential-equations', 'soft-question', 'partial-differential-equations']"
2608566,"Is there an ""inverted"" dot product?","The dot product of vectors $\mathbf{a}$ and $\mathbf{b}$ is defined as: 
$$\mathbf{a} \cdot \mathbf{b} =\sum_{i=1}^{n}a_{i}b_{i}=a_{1}b_{1}+a_{2}b_{2}+\cdots +a_{n}b_{n}$$ What about the quantity?
$$\mathbf{a} \star \mathbf{b} = \prod_{i=1}^{n} (a_{i} + b_{i}) = (a_{1} +b_{1})\,(a_{2}+b_{2})\cdots \,(a_{n}+b_{n})$$ Does it have a name? ""Dot sum"" seems largely inappropriate. Come to think of it, I find it interesting that the dot product is named as such, given that it is, after all, a ""sum of products"" (although I am aware that properties of $\mathbf{a} \cdot{} \mathbf{b}$, in particular distributivity, make it a meaningful name). $\mathbf{a} \star \mathbf{b}$ is commutative and has the following property: $\mathbf{a} \star (\mathbf{b} + \mathbf{c}) = \mathbf{b} \star (\mathbf{a} + \mathbf{c}) = \mathbf{c} \star (\mathbf{a} + \mathbf{b})$","['abstract-algebra', 'terminology', 'products', 'linear-algebra', 'vectors']"
2608664,"Simple, yet evasive integral from zero to $\pi/2$","Q: Evaluate$\newcommand{\dx}{\mathrm dx}\newcommand{\du}{\mathrm du}\newcommand{\dv}{\mathrm dv}\newcommand{\dtheta}{\mathrm d\theta}\newcommand{\dw}{\mathrm dw}$$$I=\int\limits_0^{\pi/2}\dx\,\frac {\left(\log\sin x\log\cos x\right)^2}{\sin x\cos x}$$ I'm out of ideas on what to do. I tried using the astute limit identity for integration, but that lead to nowhere, because it's simply the same integral i.e adding the two together, doesn't yield a simplification whatsoever$$I=\int\limits_0^{\pi/2}\dx\,\frac {\log^2\sin x\log^2\cos x}{\sin x\cos x}=\int\limits_0^{\pi/2}\du\,\frac {\log^2\cos u\log^2\sin u}{\cos u\sin u}$$I've looked into making a u-substitution, but have no idea where to begin. I plan to use a trigonometric identity, namely$$\sec x\csc x=\cot x+\tan x$$but I'm not sure what to do afterwards because we get$$I=\int\limits_0^{\pi/2}\dx\,\cot x\log^2\sin x\log^2\cos x+\int\limits_0^{\pi/2}\dx\,\tan x\log^2\sin x\log^2\cos x$$I would appreciate it if you guys gave me an idea on where to begin! Also, as a side note, I've used \newcommand on \dx , \du , \dv , and \dw to automatically change to $\dx$, $\du$, $\dv$, and $\dw$ respectively, if you guys don't mind! Just to make it easier for the differential operator!","['integration', 'definite-integrals']"
2608688,Plot function y = tan(yx),"Could you help me to understand this expression y = tan(yx)?
Desmos.com renders it like image below: I am trying to write a function to do similar in JavaScript.
To do so, I have to rearrange it in this way: y = tan(xy)
tan-1(y) = xy
x = tan-1(y)/y But, this expression only draws the middle line?
What am I doing wrong? Need to understand how to rearrange y=tan(xy) expression to get something similar to the first illustration with JavaScript or any other programming language.","['trigonometry', 'functions', 'graphing-functions']"
2608728,A countable set has outer measure zero. Explanation of Royden example.,"This is an example from Royden's Real Analysis 4th edition that I am having a lot of trouble understanding. A countable set has outer measure zero.  Let $C$ be a countable set enumerated as $C=\{c_k\}_{k=1}^{\infty}$.  Let $\epsilon > 0$.  For each natural number $k$, define $I_k=(c_k-\epsilon/2^{k+1}, c_k+\epsilon/2^{k+1})$.  The countable collection of open intervals $\{I_k\}_{\infty}^{k=1}$ covers $C$.  Therefore: $$ 0 \leq m^*(C)\leq \sum^{\infty}_{k=1} \ell(I_k)=\sum^{\infty}_{k=1}\epsilon/2^k = \epsilon$$ This inequality holds for each $\epsilon > 0$.  Hence $m^*(E) = 0$. What I understand along with questions: A countable set is a set like the natural numbers. $C=\{c_k\}_{k=1}^{\infty}$ is just a way to describe said countable set using indexing for all of the elements in the set. $I_k=(c_k-\epsilon/2^{k+1}, c_k+\epsilon/2^{k+1})$ I kind of understand.  This is defining an open cover over the countable set (I think). Why the $\epsilon/2^{k+1}$ instead of just $\epsilon$? It looks like the interval is continually shrinking as n goes to infinity. The equation in the middle is summing up all of the intervals.  The outer measure is simply the sum of all of the open intervals in the set C.  $m^*(C)$ is countably subadditive, hence the $\leq$ sign here:  $m^*(C)\leq \sum^{\infty}_{k=1} \ell(I_k)$ How is the last summation producing epsilon? How is this summing to zero? Thank you for your help. -Idle","['outer-measure', 'real-analysis', 'measure-theory']"
2608730,Expected value of the number of teams,"Suppose there are 32 people, and we want to create a championship consisted of 16 teams (i.e., 2 people in each team). A system will pick these teams at random. Further, suppose 4 of these 32 people are sick, and any team containing any sick person can't play in the championship. Question is what is the expected number of teams remaining in the championship? My Approach I think we only have 3 possible outcomes here: 14 teams, 13 teams, and 12 teams based on how these sick people get distributed in the teams. So the expected value would be $$ (P(14)*14) + (P(13)*13) + (P(12)*12) $$
where P(n) is the probability that n teams can participate in the championship. My Question I'm not sure if my intuition is correct or not. Also, I got stuck with finding P(n)","['probability', 'discrete-mathematics']"
2608740,Understanding line bundles on $\mathbb{P}_k^1$ using transition functions,"Vakil defines $\mathcal{O}_{\mathbb{P}_k^1}(n)$ as follows: it is Spec $k[x_1/x_0]$ on $U_0 = D(x_0)$, and Spec $k[x_0/x_1]$ on $U_1 = D(x_1)$, and the transition function from $U_0$ to $U_1$ is multiplication by $(x_0/x_1)^n$.Then he claims that from this definition it's obvious that $\mathcal{O}(n) = \mathcal{O}(1)^{\otimes n}$. I'm not sure if I completely understand transition functions. Is it basically giving a glue along the open sets $D(x_0/x_1) \subset U_0$ and $D(x_1/x_0) \subset U_1$? I don't see how $\mathcal{O}(n) = \mathcal{O}(1)^{\otimes n}$ follows easily from this transition function definition. Tensor product of sheaves is pretty mysterious to me because you have to sheafify.","['line-bundles', 'projective-space', 'algebraic-geometry']"
2608748,"Isn't $\lim_{h \to 0} \frac{c-c}{h}$ indeterminate, as $\lim_{h \to 0} \frac{c-c}{h} = \frac{0}{0}$?","I have a question that asks me to differentiate 
$f(x) = e^5$. This looks like differentiating a constant. So the answer is 0. But I'm confused about the proof withthe definition of a deriative: Don't we have an indeterminate limit since $\lim_{h \to 0} \frac{c-c}{h} = \frac{0}{0}$? Or is the right way to think about this is that the numerator is 0 but the denominator is never really quite 0 and so the limit as a whole is 0?",['limits']
2608821,Forcing homeomorphism,"I don't know much about forcing, only the basics. But I know for instance that forcing can make two first-order structures isomorphic, by collapsing their cardinals to be $\aleph_0$. For this to work, it suffices that they were $\mathcal{L}_{\infty, \omega}$-elementarily equivalent in the ""first"" universe. I've read some author say that if two such structures weren't isomorphic to begin with, it was for ""a silly reason"", i.e. cardinality. But I wonder if this could work with topological spaces : can two topological spaces not be homeomorphic ""for silly reasons"" ? More precisely, in what way can forcing to change cardinals modify the answer to ""Are $X$ and $Y$ homemorphic ?"" ? More generally, how can forcing change the answer ? And what conditions on $X$ and $Y$ can we find so that they are homemorphic in a forcing extension ? (A condition like the one for first-order structures) The answers to these questions need not be extremely technical since, as I said, I only know the very basics of forcing theory.","['forcing', 'general-topology', 'set-theory']"
2608846,Limit of $\lim\limits_{n\to+\infty}\left(\cos\frac{x}{n} + \sin\frac{2x}{n}\right)^n $,"Given the limit:
  $$
\lim_{n \to \infty}\left[\cos\left(\frac{x}{n}\right) +
\sin\left(\frac{2x}{n}\right)\right]^{n} =
\alpha
$$ Find the value of $\alpha$. I suppose there is a special technique of how to solve such expressions. Sadly, I am not familiar with it. Could anyone put me on point ?.","['real-analysis', 'limits', 'trigonometry', 'calculus', 'algebra-precalculus']"
2608877,"If $A\in\mathcal{L}(E)$, why $\lim\limits_{n\to+\infty}\|A^n\|^{1/n}$ always exists?","Let $E$ be a complex Hilbert space, $\mathcal{L}(E)$ be the algebra of all bounded linear operators on $E$. If $A\in\mathcal{L}(E)$, why $\displaystyle\lim_{n\to+\infty}\|A^n\|^{1/n}$ always exists? Thank you.","['functional-analysis', 'operator-algebras', 'operator-theory', 'hilbert-spaces']"
2608890,"$\liminf, \limsup$ and continuous functions","If a function $f: \mathbb R \to \mathbb R$ is continuous in x, $f$ is sequentially continuous in x: $$ \lim_{n\rightarrow +\infty} x_n = x \Rightarrow \lim_{n\rightarrow +\infty} f(x_n)=f(x)$$
I'm wondering whether the same  is true with $\liminf$ or $\limsup$ instead of $\lim$ $$ \liminf_{n\rightarrow +\infty} x_n = x \Rightarrow \liminf_{n\rightarrow +\infty} f(x_n)=f(x)$$
I can't find a counterexample to prove that it is false.","['limsup-and-liminf', 'analysis']"
2608908,Integral of error-like function,"I have some trouble evaluating the following integral: $$\int_{v=0}^{\infty}\int_{u=0}^{\infty}\frac{\sqrt{uv}}{(u+v)^2}e^{-\frac{(u+v)}{2}}dudv.$$ I know that the value of this double integral is $\dfrac{\pi}{4}$. However, I tried to solve it by using the change of variables: $t=u+v$ and separate the variables by bringing $v$ out. Doing so yields $$\int_{v=0}^{\infty}\sqrt{v}\int_{t=v}^{\infty}\frac{\sqrt{t-v}}{t^2}e^{-\frac{t}{2}}dtdv,$$ but it does not seem to get into the form of Gaussian's error function.","['integration', 'error-function']"
2608946,Number theory equality related to Möbius function,"I proved this equality for $x>0$,
$$
2(e^x-1)e^{-2x} = \sum_{n=1}^\infty \frac{\mu(n)}{\sinh nx}
$$
where $\mu(n)$ is the Möbius function. The problem is my proof is not so elegant and I think I could improve it using a variation of Möbius inversion formula. I leave my proof. \begin{align*}
\sum_{n=1}^{∞} \frac{μ(n)}{\sinh (nx)} &= 2 \sum_{n=1}^{∞} \frac{μ(n)}{e^{nx} - e^{-nx}} = 2 \sum_{n=1}^{∞} \frac{μ(n)}{e^{nx}} \frac{1}{1-e^{-2nx}}\\
&= 2 \sum_{n=1}^{∞} μ(n) e^{-nx}(1+e^{-2nx}+e^{-4nx}+\dots)\\
& = 2 \sum_{n=1}^{∞} μ(n) \sum_{k=1}^{∞} e^{-(2k-1)nx}= 2 \sum_{n=1}^{∞} \sum_{k=1}^{∞} μ(n) e^{-(2k-1)nx}
\end{align*}
We know that series are absolutly convergent, so
$$ \sum_{n=1}^{∞} \frac{μ(n)}{\sinh (nx)} = 2 \sum_{N=1}^{∞} e^{-Nx} \left(\sum_{n|N \text{ y }N/n\text{ is odd}} μ(n) \right)$$
If $N$ is odd
$$ \sum_{\substack{n|N \\ N/n\text{ is odd}}} μ(n) = \begin{cases}
	1, &\text{ si }N = 1\\
	0, &\text{ si }N > 1
\end{cases}$$
If $N$ is even and $N=2^am$
$$ \sum_{2^ak|2^am} μ(n) = \sum_{k|m} μ(2^a k) = \sum_{k|m} μ(2^a) μ(k) = \begin{cases}
	0, &\text{ si }a ≥ 2\\
	-1 &\text{ si }m=1\\
	0 &\text{ c.c.}
\end{cases}$$
Finally
$$ \sum_{n=1}^{∞} \frac{μ(n)}{\sinh (nx)} = 2(e^{-x}-e^{-2x})$$","['number-theory', 'elementary-number-theory']"
2608973,Evaluation of Integral $\int \frac{x^2+1}{\sqrt{x^3+3}}dx$,Calculate  integral $\int \frac{x^2+1}{\sqrt{x^3+3}}dx$ This was my exam question. I've tried many online math solvers and math programs but none were able to solve. If anybody has an answer would be helpful. Thanks,"['indefinite-integrals', 'integration', 'elliptic-integrals', 'calculus']"
2608977,Rank of a Polynomial function over Finite Fields,"Consider a function $f:\mathbb{R}^k \to \mathbb{R}^n$ where $f=(f_1,f_2,\dots,f_n)$ with each $f_i$ being a polynomial function of variables $t_1,t_2,\dots,t_k$. Now if the polynomials $f_1,f_2,\dots,f_k$ were linear functions of $t_1,\dots,t_k$, then $f$ is a linear map from $\mathbb{R}^k$ to $\mathbb{R}^n$. In this case we can define a rank of the map, and we have the rank-nullity theorem relating the rank and the dimension of its kernel. What if the functions $f_1,f_2,\dots,f_n$ were of higher degree? What would be the natural generalization of the ""rank"" of the map $f$? In one context, I noticed that they defined the rank of $f$ to be the rank of the Jacobian matrix of $f$ (whose entries are treated as elements of the rational function field over $\mathbb{R}$. Is this a standard notion? It does generalize the notion of rank of a linear map. Do we have some analogous rank-nullity theorem in this case, relating the rank and the dimension of the variety $f=0$? I am aware that there are standard rigorous notions of dimension for varieties. I am especially interested in such a notion of rank when the underlying field is not $\mathbb{R}$ but a small prime field, say $\mathbb{F}_2$. In this case, the Jacobian seems a bit restrictive, since partial derivatives over $\mathbb{F}_2$ are themselves not very useful. Or maybe I am wrong here. 
Another notion I have seen is that of algebraic independence, which seems reasonable as well. Would that agree with a rank-nullity theorem? More importantly, would that agree with the rank of the Jacobian? So my essential question is: What would be a notion of rank for a polynomial function $f:\mathbb{F}_2^k \to \mathbb{F}_2^n$?","['finite-fields', 'polynomials', 'algebraic-geometry', 'abstract-algebra', 'linear-algebra']"
2608984,Range of Even Function,Is it possible for an even function to have the entire set of real numbers as the range? I thought much about it but I didn't find. Please explain if anybody knows.,['functions']
2609001,Combinatorics -subsets containing at least $9$ elements,"How many subsets of the set $\{1,2..., 17\}$ contain at least $9$ elements
Not in a form of sum. I know the answer is $2^{16}$, but I don't know why.","['combinatorics', 'discrete-mathematics']"
2609091,Time derivative of a line integral,"Question :
What is the derivative of $$ \frac{d}{dt} \int_{(x_0,y_0,z_0)}^{(x,y,z)} \vec{F}(x,y,z,t) \cdot d\vec{r}$$ Now in order to compute any single integral, I need to give you a path. So far, all we know is that we are integrating $\vec{F}$ from the point $(x_0, y_0, z_0)$ to the point $(x, y, z)$ along some path. Let me specify that path. What is the derivative of $$ \frac{d}{dt} \int_{t_0}^{t} \vec{F}(c(t),t) \cdot \vec{c}\;'(t)dt $$ where $c(t) = (x(t), y(t), z(t))$ is the path and $\vec{c}\;'(t) = (x'(t), y'(t), z'(t))$ is the tangent vector to the path. Thoughts : Derivatives require a path. You can't take a derivative unless you know what path you are taking the derivative over. Likewise, integrals/antiderivatives require a path. You can't create an antiderivative unless you know what path you are doing the integral over. If I created the antiderivative $$\int_{t_0}^{t} \vec{F}(c(t)) \cdot \vec{c}\;'(t)dt $$ This would be the antiderivative over the path $c(t)$. Therefore if I take the derivative $d/dt$, this indicates the derivative over the same path $c(t)$. And since we know derivatives and antiderivatives are inverse operations ( over the same path ), $$ \frac{d}{dt} \int_{t_0}^{t} \vec{F}(c(t)) \cdot \vec{c}\;'(t)dt = \vec{F}(c(t)) \cdot \vec{c}\;'(t)$$ That's what I think at least. My problem is that my integrand involves an extra $t$ (I'm asking this because in order to understand time-dependent forces and energy consequences in physics, I'd like to understand derivatives of line integrals). Useful break down of my question :
These would be useful to solve. Just to make the problem look simpler, what is
$$\frac{d}{dt} \int_{t_0}^{t} f(x(t),y(t))x'(t)dt $$ This is the antiderivative over the path $x(t)$. But the derivative $d/dt$ is with respect to the path $(x(t), y(t))$. So would this become, using the chain rule $$ \frac{\partial }{\partial x} \Bigg [\int_{t_0}^{t} f(x(t),y(t))x'(t)dt\Bigg]\frac{dx}{dt} + \frac{\partial}{\partial y}\Bigg[\int_{t_0}^{t} f(x(t),y(t))x'(t)dt\Bigg]\frac{dy}{dt}$$ Is the second bracket zero? Another useful, simpler looking, problem :
$$\frac{d}{dt}\int_{t_0}^t f(x(t),t)x'(t)dt $$ Which might become:
$$ \frac{\partial }{\partial x} \Bigg [\int_{t_0}^{t} f(x(t), t)x'(t)dt\Bigg]\frac{dx}{dt} + \frac{\partial}{\partial t}\Bigg[\int_{t_0}^{t} f(x(t), t)x'(t)dt\Bigg]\frac{dt}{dt}$$ Is the first bracket just $f(x(t),t)$? If I write this first bracket as 
$$ \int_{x_0}^x f(x, t)dx$$ Then a $\partial/\partial x$ of this antiderivative should return $f(x,t)$. However I'm getting confused by the abstraction of everything and the actual computation . This last integral seems to imply that the path is just the $x$-axis. But the real path is $x(t)$. You need to do the integral with all $t$'s then. But I still feel like
$$ \frac{\partial }{\partial x}\int_{t_0}^{t} f(x(t), t) x'(t)dt = f(x(t),t) ?$$ If you read all of this, thank you. Any help would be greatly appreciated","['multivariable-calculus', 'calculus']"
2609107,Good reference book for fibre bundles and principal bundles,"I want to read the theory of fibre bundle, vector bundle and principal G bundle. Also about classifying spaces. Can someone suggest me good references for this material.","['algebraic-topology', 'differential-geometry', 'differential-topology']"
2609113,Sample path of Brownian motion Hölder continuous?,"Let $t\mapsto X_t(\omega)$ be the sample path of a Brownian motion $X$. Certainly it is possible for the sample path to be Hölder continuous on a bounded interval $[0,K]$ but can it be Hölder continuous on all of $[0,\infty)$?","['probability-theory', 'brownian-motion']"
2609133,"Estimate $P(A_1|A_2 \cup A_3 \cup A_4...)$, given $P(A_i|A_j)$","This question is related to some undergraduate research on summary generation of documents of which I am a part of. I am trying to estimate $P(A_1|A_2 \cup A_3 \cup A_4...A_k)$, where I know the values $P(A_i|A_j)\ \forall i,j \in\{1,2,...,n\}$. I understand that it is not possible to evaluate this probability exactly. Are there methods that relate to the approximation of such an expression under certain assumptions? Eg: Assuming event $A_2,A_3$ are independent. I would be glad if someone could point me to such resources. (webpages,books,papers,etc)","['machine-learning', 'statistics', 'probability']"
2609154,Evaluating the integral $\int_0^{2\pi}e^{-\sqrt{a-b\cos t}}\mathrm dt$,"In the course of my research, I have stumbled upon the following integral, which I do not know how to compute: $$I(a,b)=\int_0^{2\pi}e^{-\sqrt{a-b\cos t}}\mathrm dt,\qquad a>b>0.$$ Help would be greatly appreciated. My thoughts on the problem: Using the substitution $z=e^{it}$ yields a contour integral of a function analytic in some neighborhood of the circle $|z|=1$, so techniques from complex analysis might help. For $a=b$, the function from the previous point has a singularity at $z=1$, but we have the following explicit result (found with some help from Mathematica): $$I(a,a)=\int_0^{2\pi}e^{-\sqrt{a-a\cos t}}\mathrm dt=2\pi(I_0(\sqrt{2a})-\mathbf{L}_0(\sqrt{2a})),$$ where $I_0$ and $\mathbf{L}_0$ are modified Bessel and Struve functions, respectively. Alternatively, using the substitution $s=\sqrt{a-b\cos t}$ and exploiting the fact that the original integrand is symmetric about $t=\pi$, we obtain the following expression for the integral: $$I(a,b)=\int_{\sqrt{a-b}}^{\sqrt{a+b}}\frac{4s e^{-s}\mathrm ds}{\sqrt{b^2-(a-s^2)^2}},$$ however I also do not see how to proceed from here.","['integration', 'definite-integrals', 'contour-integration']"
2609168,How to compute $\lim\limits_{n\to\infty}\int_0^\pi \frac{\sin x}{1+3\cos^2(nx)}\text dx$,"In one of our exam  class we were given the following limit to compute
  $$
\lim_{n \to \infty}\int_{0}^{\pi}
\frac{\sin\left(x\right)}{1 + 3\cos^{2}\left(nx\right)}\,\mathrm{d}x
$$ patently is not advisable to use the convergent dominate which I failles to apply here since the integrand does not even converges. The wonderful think with is that
$\lim\limits_{n \to \infty}\cos^{2}\left(nx\right)$ does not and this problem kept me awake for a while now. Does anyone has some tips how to attack this problem ?.","['real-analysis', 'limits', 'trigonometry', 'calculus', 'integration']"
2609216,Computing $\int_0^\pi \frac{dx}{1+a^2\cos^2(x)}$,"I am trying to compute the following integral
  $$\int_0^\pi \frac{dx}{1+a^2\cos^2(x)}$$ But I got stuck on my way. 
Indeed, enforcing the change of variables $t =\cos^2x$ leads to $$\int_0^\pi \frac{d x}{1+a^2\cos^2(x)}= \int_{-1}^1 \frac{d x}{(1+a^2t^2)(1-t^2)^{1/2}}dx =2\int_{0}^1 \frac{d x}{(1+a^2t^2)(1-t^2)^{1/2}}dx$$ then what next? I though this was related the beta and Gamma function but it seems not. Can some help me here?","['real-analysis', 'trigonometry', 'calculus', 'closed-form', 'integration']"
2609232,Discovery of the first Janko Group,"Recently, I was reading about Janko's discovery of $J_1$, the first “modern” sporadic simple group. Janko and others were trying to classify all finite simple groups with an involution centralizer isomorphic to $C_2 \times \mathrm{PSL}(2,q)$. At some point in that process, it seemed possible that there exist a new sporadic simple group with involution centralizer $C_2 \times \mathrm{PSL}(2,5)$ among them. People (mostly Janko himself) started to investigate this hypothetical simple group $J_1$. They computed its order (175560) and drew conclusions about its subgroup structure. In the end, Janko was able to give concrete generators of $J_1$ as a subgroup of $\mathrm{GL}(7,11)$ (the matrices can be found on Wikipedia ). At this point it was known that if that new sporadic group $J_1$ really exists, then it has to be a concrete subgroup of $\mathrm{GL}(7,11)$. Afterwards, the existence of $J_1$ was confirmed by a computer program. Can someone offer more insights about how it was possible to get from some structural knowledge of this hypothetical group to concrete generators as a matrix group? I am curious to see how to derive a linear action out of a hypothetical simple group (of course one cannot just simply pick an elementary abelian normal subgroup on which it acts naturally).","['abstract-algebra', 'math-history', 'group-theory', 'simple-groups']"
2609276,Prove subgaussian norm of sugaurssian random variables is a norm,I know how to prove the zero and scaling property of norm. However I'm stuck on proving triangle inequality. The definition of norm of sub-Gaussian random variable is. Sub-Gaussian random variable is such norm exists. $$\|X\|_{\psi_2}=\inf\{t>0:E e^{-\frac{X^2}{t^2}}\}$$,"['functional-analysis', 'real-analysis', 'probability']"
2609330,multivariable continuous,"I wanna show that for $\alpha+\beta-2\gamma>0$: $\lim_{(x,y)\rightarrow(0,0)}\frac{\vert x\vert^{\alpha}\cdot\vert y\vert^{\beta}}{(x^{2}+y^{2})^{\gamma}}=0$ I thought about proving it via Sandwhich theory but I have no idea how to simplify this expression. Thanks in advance!","['multivariable-calculus', 'continuity']"
2609386,$\sum_{n=1}^\infty\log\left (\frac{n^2}{1+n^2}\right)$,"I'm trying to evaluate the following series:
$$\sum_{n=1}^\infty \log \left(\dfrac{n^2}{1+n^2}\right)$$
- In this case the terms are negative $\lim\limits_{n\rightarrow \infty} \log  \left(\dfrac{n^2}{1+n^2}\right)=\log 1=0$ Now I'm not sure about the application of a test
$\lim\limits_{n\rightarrow \infty} \dfrac {\log \left(\frac{n^2}{1+n^2}\right)}{\left(\frac {1}{2}\right)^n}=0$ being $\sum_{n=1}^\infty \left(\frac {1}{2}\right)^n$ is a geometric series that converges.",['sequences-and-series']
2609436,Calculus book suggestion [duplicate],"This question already has answers here : What are the recommended textbooks for introductory calculus? (8 answers) Closed last year . I am a high schooler with no prior exposure to calculus. I want a calculus book to learn math for classical mechanics on my own, and perhaps learning math for math itself. I don't like memorizing formulas, I want some understanding but nothing too rigorous. A lot of people suggest Thomas and Stewart, but a lot people dislike them as well. Why do people dislike these books? Because they are too simplistic? And they came overly long to me (over 1000 pages). I think Lang, and perhaps Kline are nice, but I am not sure. And it came to me that these books are better in 'why's of formulas. And there is Simmons as well. I worked through Spivak a little, but it was too hard. Perhaps I may return to it after some exposure to calculus to refine my understanding of the concepts. Thanks for suggestions.","['book-recommendation', 'calculus']"
2609441,Name of a subgroup whose centralizer is equal to the center of the group,Let $G$ be a group. Is there a specific name for those subgroups $H<G$ such that the centralizer $C_G(H)<G$ is equal to the center $Z(G)<G$ ?,"['abstract-algebra', 'group-theory']"
2609454,Understanding theorem 9.12 in Rudin's PMA,"$9.11$ Definition Suppose $E$ is an open set in $R^n,$ f maps $E$ into $R^m,$ and x $\in E.$ If there exists a linear transformation $\mathbf{A}$ of $R^n$ into $R^m$ such that $$\lim_{h\to 0}\frac{\left|\mathbf{f(x +h)-f(x)-Ah}\right|}{|\mathbf{h}|}=0,\tag{14}$$ then we say $\mathbf{f}$ is differentiable at $\mathbf{x},$ and we write $\mathbf{f'(x)=A}$ $9.12$ Theorem Suppose $E$ and f are as in Definition $9.11,$ x $\in E,$ and $(14)$ holds with $\mathbf{A=A_1}$ and with $\mathbf{A=A_2}.$ Then $\mathbf{A_1=A_2}.$ So the idea is to consider $\mathbf{B= A_1-A_2}$ and show that $\left|\mathbf{Bh}\right|\le \epsilon$ for every $\epsilon >0.$ But I failed to understand Rudin's argument : If $\mathbf{B=A_1-A_2},$ the inequality $$\left|\mathbf{Bh}\right|\le\left|\mathbf{f(x+h)-f(x)-A_1h}\right|+\left|\mathbf{f(x+h)-f(x)-A_2h}\right|$$ shows that $\frac{|\mathbf{Bh}|}{|\mathbf{h}|}\to 0$ as $\mathbf{h}\to 0.$ For fixed $\mathbf{h\ne 0},$ it follows that $$\frac{|\mathbf{B(th)}|}{|\mathbf{th}|}\to 0 \text{ as } t\to 0.\tag{16}$$ The linearity of $\mathbf{B}$ shows that the left side of $(16)$ is independent of $t.$ Thus $\mathbf{Bh}=0$ for every $\mathbf{h}\in R^n.$ Hence $\mathbf{B}=0.$ I understood each and every step but I am not able to see the link showing $\left|\mathbf{Bh}\right|\le \epsilon$ for every $\epsilon >0.$ Can someone help me with this?","['real-analysis', 'analysis']"
2609474,Want to use Herbert Wilf's snake oil method to show $\sum_k \binom{2n+1}{2k}\binom{m+k}{2n} = \binom{2m+1}{2n}$,"I was trying to use the snake oil method to show that $$\sum_k \binom{2n+1}{2k}\binom{m+k}{2n} = \binom{2m+1}{2n}$$ tl;dr I wasn't able to and desperately need help My approach was to try to establish a more general identity regarding $$\sum_k \binom{2n+1}{2k}\binom{m+k}{s}$$
So I let $$F(x) = \sum_s \sum_k \binom{2n+1}{2k}\binom{m+k}{s} x^s$$
By changing the order of summation, I obtained
$$F(x) = \sum_k \sum_s \binom{2n+1}{2k}\binom{m+k}{s} x^s = \sum_k \binom{2n+1}{2k}(1+x)^{m+k}$$
After this, I noted that $$\sum_{k \geq 0} \binom{n}{2k}x^{2k} = \frac{(1+x)^n + (1-x)^n}{2}$$
Now, $$F(x) = \frac{(1+x)^m}{2}((1+\sqrt{1+x})^{2n+1} + (1-\sqrt{1+x})^{2n+1})$$
I'm having trouble finding the coefficient of $x^{2n}$ in this generating function. Alternate approach: I could have created a generating function where the summation was on $m$ but that proved to be fruitless.","['generating-functions', 'discrete-mathematics']"
2609501,Let $a_n$ be a decreasing sequence. Prove that the power series $\sum a_n x^n$ has no roots in $A=\{z\in C:|z|<1\}$,"Let $a_n$ be a decreasing, positive sequence, in the real space. Prove that the power series $\large p(z)=\sum\limits_{k=0}^{n} a_k z^k$ has no roots in $A=\{z\in C:|z|<1\}$. What I did so far $zp(z) =\sum\limits_{k=1}^{n+1} a_{k} z^k \Rightarrow |zp(z)| \leq \sum\limits_{k=1}^{n+1} a_{k} |z|^k \leq\sum\limits_{k=0}^{n} a_k |z|^k = |p(z)|=|-p(z)|$ (since $a_n$ non decreasing and $|z|<1$) using Rouche's principle $(z-1)p(z)$ has the same amount of roots with $-p(z)$ or $p(z)$. Am I going to the right direction ?","['complex-analysis', 'power-series']"
2609505,The Heegner Polynomials,"What is special about $x^3-	6 x^2 +	4 x	-2$ ? The 24th power of the real root - 24 is curiously close to two other numbers, one being the Ramanujan constant . There are more of these polynomials associated with the Heegner numbers . $poly_1 = x^2 -2$ .  The first root space has the Pythagoras constant , Silver ratio , vertices of an octagon , A4 paper, Ammann tile and the curiosity below, with numbers representing powers of $\sqrt2$ . $poly_2 = x^3 - 2x^2 +2x-2$ .  The second root space is part of tribonacci space $T = t^3 - x^2 -x-1$ , notable for the snub cube . Roots $T_n$ as ${T_n}^2 - T_n$ are the roots of $poly_2$ . The polynomial roots $1+2 T_n -{T_n}^2$ , also in the same root space, become de Weger's example , the second best known algebraic solution for the ABC Conjecture . $poly_3 = x^3 - 2x-2$ . This root space builds the 12 point Heilbronn solution and the 12 disk covering solution . With $r$ as the real root, the circles have radius $\sqrt{(1,r,r^2)\cdot(-3,0,1)}$ , with two centers on the $x$ -axis at $\sqrt{(1,r,r^2)\cdot(-7,4,0)}$ and $\sqrt{(1,r,r^2)\cdot(-1,2,-1)}$ . Can anyone find amazing properties for the root spaces of the last three polynomials? These are related to New Substitution Tilings Using 2, φ, ψ, χ, ρ .","['roots', 'polynomials', 'algebraic-geometry', 'recreational-mathematics']"
2609524,"$X^2 + X =A$ with $X, A\in \text{Mat}_{2,2} (\mathbb{R})$ . Show that there exists a solution $X$ for a given $A$","Prove the following statement: There exists an $\epsilon > 0$ such that the following holds: If $A = (a_{ij}) \in \text{Mat}_{2,2} (\mathbb{R})$ a matrix with $(|a_{ij}|) < \epsilon$ for $i,j \in \{1,2\}$, then the following equation $$X^2 + X = A$$ has a solution $X \in \text{Mat}_{2,2} (\mathbb{R})$ My Idea on how to solve this: Let $X =  \begin{bmatrix}
v& w \\ x & y 
\end{bmatrix}$.
Therefore $X^2 + X = \begin{bmatrix}
v^2 + v + w x& v w + w y + w\\ v x + x y + x & w x + y^2 + y
\end{bmatrix} = \begin{bmatrix}
a_0 & a_1 \\ a_2 & a_3\end{bmatrix}$ Lets now define the function $$ 0=h(v,w,y,x,a_0,a_1,a_2,a_3) =\begin{cases} v^2 + v + w x - a_0 \\ v w + w y + w - a_1\\v x + x y + x -a_2
\\w x + y^2 + y-a_3
\end{cases} $$ We can now calculate the derivative of $h$: $$dh = \begin{bmatrix}
 2v + 1 &  x & 0 & w & -1&0&0&0\\ 
w& v+y+1& w& 0& 0&-1&0&0\\
x & 0&x&v +1 & 0&0&-1&0
\\0&x&2y+1&w& 0&0&0&-1
\end{bmatrix}$$ The idea now would be to apply the implicit function theorem and show that there exists an $X$ which solves this equation. I am not sure though if this approach is correct. Last but not least.. this question comes from an analysis sheet, so I assume one should use the methods of analysis to solve it. Is my approach the correct way? And how does one proceed from here? Feel free to use another approach. Thank you for your time.","['implicit-function-theorem', 'linear-algebra', 'analysis']"
2609526,Evaluate $\lim_{ x\to \infty} x^2 \times \log \left(x \cot^{-1}x\right)$,Evaluate $$L=\lim_{ x\to \infty} x^2 \times \log \left(x  \cot^{-1}x\right)$$ My Try:we have by change of variable $y=\frac{1}{x}$ $$L=\lim_{ y\to 0}\frac{\log\left(\frac{\tan^{-1}y}{y}\right)}{y^2}=\lim_{ y\to 0}\frac{\log\left(\tan^{-1}y\right)-\log y}{y^2} $$ We have for $y$ very very small$$\tan^{-1}y=y-\frac{y^3}{6}$$ $$L=\lim_{ y\to 0}\frac{\log\left(y-\frac{y^3}{6}\right)-\log y}{y^2}$$ $\implies$ $$L=\lim_{ y\to 0}\frac{\log y+\log\left(1-\frac{y^2}{6}\right)-\log y}{y^2}$$ $\implies$ $$L=\lim_{ y\to 0}\frac{\log\left(1-\frac{y^2}{6}\right)}{y^2}$$ $\implies$ $$L=\frac{-1}{6}$$ Is this approach fine?,"['algebra-precalculus', 'derivatives', 'limits-without-lhopital', 'limits']"
2609572,"How to verify $(a,b) = (c,d) \implies a = c \wedge b = d$ naively","Given set $A, B$, $a\in A, b\in B$, we call $$(a, b) = \{a, \{a, b\}\}.$$ How do we show naively that
  $$(a, b) =(c, d) \Rightarrow a =c , b=d?$$ The proof that I have in mind is the following: Given $(a, b) = (c, d)$, so $$\{a, \{a, b\}\} = \{ c,  \{ c, d\}\}.$$ First of all $a \neq \{ a, b\}$, or this will give $a\in a$. Similarly $c\neq \{c, d\}$. Now $a\in \{c, \{c, d\}\}$ so either $a = c$ or $a= \{c, d\}$. If the latter is true, then we have $$ c = \{ a, b\} = \{ \{ c, d\} , b\}$$ This implies $$c \in \{ c, d\} \in c$$ and it is not allowed. Thus $a=c$. So $$\{a, b\} = \{c, d\} = \{ a, d\}$$ and from there we can show also $b=d$. Question : This results is stated in p.8 of this book , but they never mention that infinite chain is not allowed. So I was wondering if there is an even more naive way to prove the above assertion.","['elementary-set-theory', 'definition']"
2609653,The condition between $\chi(1)$ and $[G:H]$ which gives us a normal subgroup.,"$\textbf{The question is as follows:}$ Let $H \le G$ with $|G : H| = n$ and suppose that $\chi \in Char(G)$. $\rm (a)$ Show that $[\chi ; \chi] \ge \frac{[\chi_H; \chi_H]}{n}$. $\rm (b)$ Show that, if $H$ is Abelian and $\chi \in Irr(G)$ then $\chi(1) \le n$. $\rm (c)$ Show that, if the equality holds in part (b), then $H \vartriangleleft G$. I can show the first part as follows: $\rm (a)$  We have
$$|H| [\chi_H; \chi_H] = \sum_{h \in H}^{} |\chi(h)|^2 \le \sum_{g \in G}^{}|\chi(g)|^2 = |G|[\chi, \chi] $$ $\rm (b)$ For second part I can write $$\chi(1)=\chi|_H(1)\le [\chi|_H,\chi|_G]\le [G:H][\chi,\chi]=[G:H]$$
But I am not sure if it is correct? Can someone correct me please? $\rm (c)$  For the third part I still have no idea so far! Can someone help me to show this, please? Thanks!","['finite-groups', 'abstract-algebra', 'group-theory', 'characters']"
2609654,How many seats should be available such that the probability.. is less than $0.01$?,"Two railway companies respectively deploy one train (to get from city
  X to city Y). In total, $1000$ people randomly choose the train,
  respectively with probability $\frac{1}{2}$. How many seats should make one railway company available in the train,
  such that the probability, that at least one of their passengers needs
  to stand, is less than $0.01$? I think for these problem I need to use theorem of De Moivre Laplace. I call total amount of people $n = 1000$ Probability for choose train is $p = \frac{1}{2}$ But formula for it is strange and I'm not sure how use it correct: $$\lim_{n \rightarrow \infty}\mathbb{P}\left(\frac{X-np}{\sqrt{np(1-p)}}\leq x\right) = \Phi(x)$$ When insert everything in formula we have $$\lim_{n \rightarrow \infty} \left(\frac{1000-0.5n}{\sqrt{0.5n(1-0.5)}}\right) = \lim_{n \rightarrow \infty} \left(\frac{1000-0.5n}{\sqrt{0.25n}}\right) = \lim_{n \rightarrow \infty}\left(\frac{1000}{\sqrt{0.25n}} - \sqrt{n}\right) = 0-\infty = -\infty$$ But I see from solution something went wrong :( It was also hard find the correct formula on internet because in script there is strange thing. How solve this correct because I think similar question can asked in lesson and I want do it correct in test.","['probability-theory', 'probability-distributions', 'statistics', 'probability', 'discrete-mathematics']"
2609765,Is a Lipschitz function differentiable?,"Is a Lipschitz function differentiable? I have been wondering whether or not this property applies to all functions. I do not need a formal proof, just the concept behind it. Let $f: [a,b] \to [c,d]$ be a continuous function (What is more - it is uniformly continuous!) And let's assusme that it's also Lipschitz continuous on this interval. Does this set of assumptions imply that $f$ is differentiable  on $(a,b)$?","['derivatives', 'real-analysis', 'calculus', 'lipschitz-functions', 'analysis']"
2609826,Determinant of Block Tridiagonal Matrix,"I found in the following paper: Comments on ‘‘A note on a three-term recurrence
for a tridiagonal matrix’’ that we can compute the determinant of a block tridiagonal matrix A via a recursion. In my particular case A is $4n\times4n$, $$\textbf{A}=\begin{pmatrix}
\textbf{B}_L-h\textbf{R} & J\space\textbf{R} & \textbf{0} & \cdots & \textbf{0} \\
J\space \textbf{R} & -h\textbf{R} & J\space\textbf{R} &  & \textbf{0} \\
\textbf{0} & J\space \textbf{R} & -h\textbf{R} &\ddots &\vdots \\
\vdots &  &\ddots &\ddots & J\space\textbf{R}\\
\textbf{0} & \textbf{0} & \cdots & J \space\textbf{R} & \textbf{B}_R-h\textbf{R}
\end{pmatrix}$$ where 
$$\textbf{R}=\begin{pmatrix}
0&0&1&0\\
0&0&0&1\\
-1&0&0&0\\
0&-1&0&0\\ 
\end{pmatrix},\space
\textbf{B}_{L,R}=\begin{pmatrix}
0&\frac{i}{2}\Gamma_{+}^{\text{L,R}}&-\frac{i}{2}\Gamma_{-}^{\text{L,R}}&\frac{1}{2}\Gamma_{-}^{\text{L,R}}\\
-\frac{i}{2}\Gamma_{+}^{\text{L,R}}&0&\frac{1}{2}\Gamma_{-}^{\text{L,R}}&\frac{i}{2}\Gamma_{-}^{\text{L,R}}\\
\frac{i}{2}\Gamma_{-}^{\text{L,R}}&-\frac{1}{2}\Gamma_{-}^{\text{L,R}}&0&\frac{i}{2}\Gamma_{+}^{\text{L,R}}\\
-\frac{1}{2}\Gamma_{-}^{\text{L,R}}&-\frac{i}{2}\Gamma_{-}^{\text{L,R}}&-\frac{i}{2}\Gamma_{+}^{\text{L,R}}&0\\
\end{pmatrix}
$$
and $$J,h,\Gamma_{+}^{\text{L,R}},\Gamma_{-}^{\text{L,R}} \in \mathbb{R}$$ Now let me state the recursion mentioned in the above paper, $$\text{det}(\textbf{A})=\prod_{k=1}^{n}\text{det}(\Lambda_{k})\space\space\space\space(1)$$ where (in my case), $$
\Lambda_{1} = \textbf{B}_L-h\textbf{R}\\
\Lambda_{k} = -h\textbf{R}-J^{2}\textbf{R}\Lambda_{k-1}^{-1}\textbf{R}\\
\Lambda_{n}=\textbf{B}_R-h\textbf{R}-J^{2}\textbf{R}\Lambda_{n-1}^{-1}\textbf{R}
$$ 
Now according to (1), the set of eigenvalues of $\textbf{A}$ should contain the eigenvalues of $\Lambda_{1} = \textbf{B}_L-h\textbf{R}$ since applying (1) to $\textbf{A}-\lambda I_{4n}$ gives $\Lambda_{1}^{'} = \textbf{B}_L-h\textbf{R}-\lambda I_{4}$. Now here comes my issue . I computed the spectrum of A in Mathematica for $n=50$ for the values $h=1,J=1.5,\Gamma_{+}^{\text{L}}=1.6,\Gamma_{+}^{\text{R}}=1.3,\Gamma_{-}^{\text{L}}=-0.4,\Gamma_{-}^{\text{R}}=-0.7$ I found that the spectrum did not contain the eigenvalues of $\textbf{B}_{L}-h\textbf{R}$. My question is: Is this recursion not applicable to my case, or am I incorrect in stating that the set of eigenvalues of A should contain the eigenvalues of $\Lambda_{1}$?","['eigenvalues-eigenvectors', 'block-matrices', 'matrices', 'recursion', 'linear-algebra']"
2609840,I want to know every single bit there is to understand in this following proof,"I'm ashamed of myself for not answering my professor when he asked if I found the proof myself. I gave him the impression that I did which was still not true. So I want to make it right. It's about the proof of the product of the functions $f$ and $g$ is continuous. I found the proof here , it's on the 1st page on the bottom of the page. 
I'd like to know why he/she starts with setting prerequisites like $\epsilon \lt 1$ and $M = \max(f(a), |g(a)|,1)$. I certainly don't understand why to put $M$ like that. And the second sentence I understand because first of all it's defining what we already know about continuity and then setting an assumption,ok. But then the inequality $f(x) \lt f(a) + \dfrac{\epsilon}{3M} \le M + \dfrac{\epsilon}{3M}$ Why exactly did he/she do that? Thanks for making things right.","['continuity', 'epsilon-delta', 'calculus', 'proof-explanation']"
2609855,"What does ""x <= y or not"" mean?","What is the significance of the phrase ""for any two objects $x,y \in X$, the statement $x \leq_X y$ is either a true statement or a false statement.""? By the law of the excluded middle, I think that it's always the case that $x \leq_X y$ is either true or not true. Does that mean a ""partially ordered set"" is not more restrictive than simply a ""set""?","['real-analysis', 'elementary-set-theory']"
2609861,Can real non-symmetric matrices have real eigenvalues? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question From the spectrum theorem, we know real symmetric matrices have real eigenvalues. But can real non-symmetric matrix have real eigenvalues? What are the necessary and sufficient conditions for a real matrix to have real eigenvalues?","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2609873,Understanding extended Riemann integral in Munkres.,"I'm self-studying Analysis on Manifolds by Munkres.  I understood the theory of the Riemann integral over bounded rectangles and more general rectifiable sets in $\mathbb{R}^n$.  In the part on improper integrals, I am becoming confused. Munkres defines the extended or improper integral of a continuous function $f$ over an open set $A \subset \mathbf{R}^n$.  He chooses any sequence $C_N$ of compact rectifiable sets such that $A = \bigcup_N C_N$   and $C_N \subset \text{Int }C_{N+1}$ for all $N$ and states that the extended integral exists if and only if the sequence $\int_{C_N} |f|$ is bounded and $$\int_Af = \lim_{N \to \infty} \int_{C_N} f.$$ He states ""...if the ordinary integral exists, then so does the extended integral and the two integrals are equal"", but then "" ...the extended integral may exist when the ordinary integral does not."" This makes sense if you think about integrals over intervals in $\mathbf
{R}$.  If a function is unbounded or the interval is unbounded then the Riemann integral does not exist but the improper integral can. But Munkres claims this even if $A$ is a bounded, open set and $f:A \to \mathbf{R}$ is a bounded, continuous function. How is this possible?","['multivariable-calculus', 'real-analysis', 'improper-integrals', 'riemann-integration']"
2609884,Prove that the common chord passes through the origin.,"Consider the parabola $x^2=4ay$. Two focal chords are constructed, and
  the interval that the chord makes with the parabola are diameters of
  two circles. Prove that that the common chord of the two circles passes through the
  origin (vertex of the parabola). I am having trouble finding a purely geometric way of proving this result.","['circles', 'conic-sections', 'geometry']"
2609893,Closed-Form PDF of Sum of n Independent Rayleigh Random Variables,"I was wondering if there is a closed-form expression for the sum of n iid Rayleigh RVs. Here is what I tried:
An exponential density function  $$f_x(x)=\frac{1}{2 \sigma^2}exp(-\frac{x^2}{2\sigma^2})$$ can be transformed from a Rayleigh density $$f_z(z)=\frac{z}{\sigma^2}exp(-\frac{z^2}{2\sigma^2})$$ using  $f_x(x)=\frac{1}{(2 \sqrt{x})}f_z(\sqrt{x})$ because $x=z^2$. From this, the density of the sum of n iid values of x (exponentially distributed) can be determined analytically using convolution. There is a thread on this already. $$f_t(t)=\left(\frac{1}{2\sigma^2}\right)^n \frac{t^{n-1}}{(n-1)!}exp(-\frac{t^2}{2\sigma^2})$$ I thought I could then transform this density to arrive at the density of the sum of Rayleigh RVs using $y=\sqrt{t}$, because Rayleigh and exponential RVs are related this way. The resulting density is $$f_y(y)=2 y f_t(y^2)$$ Using Mathcad, if I compare the shape of the pdf $f_y(y)$ to the pdf obtained using n-fold convolution of $f_t(t)$, how I would normally do it, the results are not the same.  I suppose this could be a numerical methods problem. However, I have never seen a closed-form solution to this problem, so I expect I could have a problem with my reasoning here.  Any help on this is appreciated. Thanks!","['probability-theory', 'probability', 'statistics', 'probability-distributions']"
2609907,"Lindeberg condition fails, but a CLT still applies","I'm having difficulty with this old qualifying exam problem. Suppose we have a sequence of independent R.V's $\{X_n\}_{n\in\mathbb{N}}$ satisfying, $$ \mathbb{P}(X_n = \pm n^2) = \frac{1}{12n^2}, \;\;\;\; \mathbb{P}(X_n = \pm n) = \frac{1}{12}, \;\;\;\; \mathbb{P}(X_n = 0) = 1 - \frac{1}{6} - \frac{1}{6n^2}$$ I managed to show that the Lindeberg condition does not hold. However, the problem states that the sequence $\frac{S_n}{b_n}$ still converges in distribution to a standard normal, where, $$ S_n = \sum\limits_{k=1}^nX_k, \;\;\; b_n^2 = \frac{n(n+1)(2n+1)}{18} $$ Is there any cheeky way to show this? I tried using characteristic functions, but I feel like this method would be too inefficient on a timed exam. EDIT: I consulted with the graduate director and he told me that there was a typo in the question. The question was actually taken from Chung's ""A Course in Probability Theory"" text (Section 7.2, Exercise 10). The correct problem states: A CLT may well hold with a different sequence of constants $b_n$. Prove that Lindberg's condition is not satisfied. Nonetheless, if we take $b_n^2 = n^3/18$, then $S_n/b_n$ converges in dist. to the standard normal. The point is that abnormally large values may not count! Hint: Truncate out the abnormal value","['probability-theory', 'central-limit-theorem', 'normal-distribution']"
2609932,"Can this function be integrated and, if so, what is its value?","Say I have a piecewise function defined as: $f(x) =
\begin{cases}
0 & \text{if x is irrational} \\
1/q &\text{if x = $p/q$, where $p,q > 0$ are relatively prime integers} \\
\end{cases}$ over the interval $[0,1]$.  I can prove that $f$ is continuous over the irrational numbers and discontinuous at all rational numbers.  But, is the function Riemann integrable:
$\int_{0}^{1}f(x)dx$?  If so how would I prove it and what would be the value?  Intuition suggests that the value would be zero but I'm at a loss for how to proceed.  Assuming the function IS Riemann integrable could I split it up into multiple sections at every discontinuity?  But, there are infinitely many discontinuities... Thanks for your help.","['real-analysis', 'integration', 'measure-theory']"
2609939,Swapping row $n$ with row $m$ by using permutation matrix,"Let $A$ be matrix ( finite or infinite). and We want to swap ( exchange ) location of row $n$ with row $m$. I know we need to use permutation matrix to do this, but my question do exactly we know which permutation matrix need to be used to perform this swapping? or we need to know the entries of the matrix too.","['matrices', 'matrix-decomposition', 'linear-algebra', 'linear-transformations']"
2609979,(Number of perfect powers ≤ n) ∼ $\sqrt{n}$?,"The set $E$ of perfect powers is defined as follows: $$E:=\{x^y:\ x,y\in\ \mathbb{N}_{\ge 2} \}.$$ (Thus, a positive integer $n\in E$ iff the exponents in the prime factorization of $n$ have GCD $>1$.) I was curious what the counting function for $E$ would look like, i.e., 
$$\epsilon(n) := \mathrm{card}\{k\in E:\ k\le n \},$$
so I computed $\epsilon(n)$ for $0\le n\le 10^6$, obtaining the following plot: This picture is a superposition of $\epsilon(n)$ (black) and a fitted function of form $\ a\,n^b\ $ (red), with $a\approx 1.4,\ b\approx 0.48.$  Empirically, as $n$ increases, such fitted functions suggest that $\ \epsilon(n)\sim \sqrt{n}$. Question : Is it correct that $\ \epsilon(n)\sim \sqrt{n}\ ?\ $ How can the correct asymptotic form be derived? EDIT: Upon learning that these are called ""perfect powers"", I located several online articles proving $\ \epsilon(n)\sim \sqrt{n},\ $ including the following: M. A. Nyblom, A counting function for the sequence of perfect powers , Austral. Math. Soc. Gaz. 33 (2006), 338–343. R. Jakimczuk, On the distribution of perfect powers , J. Integer Seq. 14 (2011), Article 11.8.5.","['combinatorics', 'prime-factorization', 'elementary-number-theory']"
2610066,Function notation with set of mappings valid or not? I have never seen one,"I have seen notations in maths scripts similar to the following: $f: \{1,2\} \rightarrow \{3, 4\}$ $1 \mapsto 3$ $2 \mapsto 4$ but not once compact mappings similar to the relation in the set notation {(1,3), (2,4)} Would you say the following definition is ambiguous or even incorrect? $f: \{1,2\} \rightarrow \{3, 4\}, \{1 \mapsto 3, 2 \mapsto 4\}$","['elementary-set-theory', 'notation', 'functions']"
2610087,Numerically solve complex differential equation,"I'd like to find a numerical solution to a complex differential equation of the form $ \frac{dz(t)}{dt} = f(z,t)$, where $z$ and $t$ can both be complex, with $z(0)=0$. Specifically, I'd like to determine values of $z$ for some mesh of values of $(t_r,t_c)$, where $t = t_r + t_c i$, so that I can potentially interpolate later to find $z(t)$ at any complex $t$. One approach would be to numerically solve the ODE for real $t$, find a smooth approximation to the solution (e.g. a polynomial), and then analytically continue the smooth function into the complex plane. I'm worried about the accuracy of this approach, because I don't know if the smooth function being an accurate approximation on the real axis guarantees its accuracy in other parts of the complex plane.
I could also consider solving the ODE for imaginary $t$, or along any particular curve through the plane, and then analytically continue it similarly - which would at least provide consistency tests. But perhaps I should be thinking of it as a PDE rather than an ODE - I can write $\frac{\partial z}{\partial t_r} = -i \frac{\partial z}{\partial t_c} = f(z,t_r,t_c)$, but then it looks like I've got too many constraints for a usual PDE solver... Is there a more natural way to solve these types of equations numerically?","['partial-differential-equations', 'complex-analysis', 'analytic-continuation', 'numerical-methods', 'ordinary-differential-equations']"
2610128,Expected number of same-colour regions in a tiled floor,"Suppose we have a rectangular floor, $a$ units long by $b$ units wide, which we need to tile with black and white unit square tiles. We flip a coin to decide whether the first tile will be black or white, and lay it down in the top left corner. The second and all subsequent tiles will also have a 50-50 chance of being black and white. A 'region' is defined as a contiguous area of same-colour tiles, touching each other by their sides (just a corner is not enough). Thus, the maximum number of regions possible is $ab$ (checkerboard tiling), while the minimum number is 1 (the whole floor is either black or white). What is the expected number of regions on the board, as a function of $a$ and $b$?","['combinatorics', 'probability', 'geometry']"
2610132,What can the disk conformally cover?,"The Riemann mapping theorem states that there is conformal bijection between the open disk and any simply connected open subset of the complex number plane. My question is, for which subsets of the complex number plane is there a conformal covering map from the disk to that space? Example: In this image , a ring model of the hyperbolic plane is given. It is given by $w=e^{za}$, where $w$ is a point in the ring, and $z$ is a point in the band model of the hyperbolic plane, for appropriate $a$. It only works if the image in the hyperbolic plane has the appropriate symmetry, since some different points in the band model get mapped to the same point in the ring model. Essentially, it corresponds to a covering map from the hyperbolic plane to the ring. Since the hyperbolic plane is conformally equivalent to the open disk, that means there is also a conformal covering map from the disk to the ring. So, what other spaces besides the ring work?","['complex-analysis', 'conformal-geometry', 'complex-geometry']"
2610145,Why is $\kappa$ for a vertical line in 2-space not undefined?,"By the definition of curvature (in terms of $t$), 
$$\kappa(t)=\frac{\|r'(t)\times r''(t)\|}{\|r'(t)\|^3},$$
where $r(t)$ represents a linear vertical vector-valued function, such as $r(t)=<0,t>$. Since the derivative of a vertical line is undefined, (e.g. a vertical tangent) it would seem that that $r'(t)$ would similarly be undefined, meaning $\kappa(t)$ would be undefined. Yet at the same time, the textbook does point out that a linear line's curvature is constant (by nature of it not bending) -- that is, $$\left\|\frac{dT}{ds}\right\|,$$
is constant, or $$\kappa(s)=\left\|\frac{dT}{ds}\right\|=0.$$
Thus, my question is how I can reconcile the two answers, and see where I went wrong in my reasoning that $\kappa(t)$ was undefined.","['multivariable-calculus', 'curvature']"
2610186,Discrete points curvature analysis,"I have a set of discrete points in the x-y domain and I need to find the points of abrupt change and gradual change as well. Approach 1: I tried finding the curvature formed by 3 points using this equation: where a , b , c stands for distance between all three line segments. This worked fine in some cases with soft curve, but failed to detect some points with very less acute angle as attached in the sample output: I have selected a threshold which segregates the cases of a break point from a smooth curve. If I lower that threshold value from the current value then it may report some false cases as well. The threshold chosen is 5 and the value of curvature at the sharp turn is 2 , while for the blunt turns the curvature value is 11 . I have chosen the value 5 by hit and trial. Approach 2: I read about the double derivative of discrete points from this blog and tested it on my data-set, But it is also not detecting that sharp edge, instead it detects seemingly vertical points as break point as: Approach 3: I calculated the angle subtended by 3 points at the centre and then threshold it to detect the break points. It works fine in case of sharp edges but fails in case of blunt points. Proposed solution: From all of the above learning implementations I concluded that I can use Approach 3 to detect sharp edges and then use Approach 1 to detect blunt edges. But it would be appreciable if get this done more efficiently in a single pass. I am open to any other solution that you may think would work in this case. Sample cases: case 1 x_coords = [153, 168, 186, 202, 214, 234, 248, 270, 286, 311, 327, 350, 363, 386, 398, 413, 428, 415, 394, 378, 353, 335, 307, 290, 264, 248, 226, 211, 188, 174, 157, 145, 129, 116, 100, 90, 98, 114, 139, 152, 167, 178, 195, 207, 221, 234, 246, 266, 280, 290, 278, 266, 250, 232, 214, 196, 180, 166]
y_coords = [120, 121, 125, 128, 129, 131, 131, 131, 129, 128, 127, 128, 129, 131, 133, 137, 139, 137, 135, 133, 133, 133, 135, 137, 140, 142, 145, 147, 148, 147, 146, 144, 141, 138, 132, 125, 115, 111, 107, 103, 100, 96, 91, 87, 81, 75, 69, 58, 50, 41, 34, 30, 28, 27, 29, 34, 38, 41] case 2 x_coords = [387, 376, 355, 335, 321, 301, 288, 268, 255, 235, 222, 200, 187, 168, 156, 139, 127, 111, 99, 117, 130, 145, 158, 179, 193, 217, 235, 263, 283, 315, 336, 368, 389, 420, 440, 467, 483, 506, 520, 537, 549, 564, 566, 564, 562, 560, 558, 556, 554, 551, 540, 536, 532, 530, 528, 527, 526, 526, 527, 528, 531, 532, 534, 534, 534, 530, 524, 514, 503, 494, 486, 474, 465, 452, 444, 433, 423, 409, 396]
y_coords = [45, 41, 43, 51, 58, 71, 78, 89, 94, 101, 105, 109, 110, 112, 112, 112, 111, 111, 112, 122, 125, 127, 128, 129, 130, 130, 131, 131, 133, 137, 139, 144, 146, 150, 151, 151, 150, 148, 146, 143, 141, 137, 116, 101, 87, 71, 58, 42, 30, 15, 19, 30, 53, 68, 91, 105, 126, 138, 151, 166, 182, 196, 211, 224, 236, 256, 268, 276, 259, 240, 225, 203, 189, 171, 159, 147, 139, 130, 129]","['discrete-geometry', 'curvature', 'discrete-mathematics']"
2610205,Showing that a Severi-Brauer Variety with a point is trivial,"Let $X/k$ be a variety over a field such that $X_{\overline k} \cong \mathbb P^n$ over $\overline k$ for some $n$. Suppose moreover that $X$ has a rational $k$-point $P$. Then, I know that $X \cong \mathbb P^n$ over $k$. The argument I know goes as follows: $(X,P)$ is a twist of $(\mathbb P^n,(1,0,\dots,0))$ and the automorphism group of this object is an extension of $\mathrm{GL}_n(\overline k)$ by ${\overline k}^\times$ and therefore has trivial first cohomology and hence no non trivial twists. In the case of $n=1$ however, there is a more ""geometric"" proof that explicitly constructs the required isomorphism. First, embed $X$ in $\mathbb P^2$ over $k$ using the dual of the canonical divisor and then project onto a line in $\mathbb P^2$. This map is in fact an isomorphism. Is there a similar construction in the general case? Can we explicitly construct the required isomorphism? I would also be interested in 
other proofs of this fact.","['arithmetic-geometry', 'brauer-group', 'algebraic-geometry']"
2610228,Equivalence for a reversed Lipschitz-type condition,"Denote the real line by $\mathcal{R}$ and the non-negative real line by $\mathcal{R}^+$. Suppose that the function $f:\mathcal{R}\longrightarrow\mathcal{R}$ is continuous. Find necessary and sufficient conditions on $f$ for the following statement. There exist a strictly increasing continuous function $\sigma:\mathcal{R}^+\longrightarrow\mathcal{R}$ satisfying $\sigma(0)=0$ such that 
$$|x-y|\leq\sigma(|f(x)-f(y)|),$$
for all $x,y\in\mathcal{R}$.","['continuity', 'real-analysis', 'lipschitz-functions', 'functions']"
2610285,"Solving $y''+\dfrac{\varepsilon y'}{y^2} - y' = 0, \, y(-\infty)=1$ and $y(\infty) = \varepsilon$","$y''+\dfrac{\varepsilon y'}{y^2} - y' = 0, \, y(-\infty)=1$ and $y(\infty) = \varepsilon >0$. Above is an ODE that was in an asymptotic analysis exam. A part of the question asked us to solve it exactly on the domain $(-\infty, \infty)$, where I got into troubles. I wrote the equation as: $$\left(\dfrac{\varepsilon}{y}\right)' = y''-y'=(y'-y)'$$
So we have $\dfrac{\varepsilon}{y} - y'+y = C_0$ and then $\dfrac{ydy}{y^2-yC_0+\varepsilon} = dx$, which would give a closed formula for the general solution. But this is where the trouble is: the antiderivative of the LHS will be very different functions depending on the relationships between $\varepsilon$ and $C_0$; therefore, it would seem like we need to determine $C_0$ to begin with. But that is impossible because of the initial conditions not involving any $y'.$ How should I proceed from here?","['asymptotics', 'ordinary-differential-equations', 'initial-value-problems']"
2610315,How to prove $\int^\infty_{-\infty}x dx$ is divergent?,"Well, intuitively, I thought $\int^\infty_{-\infty}x dx$ is $0$
 ,but the answer is divergent. How to prove $\int^\infty_{-\infty}x dx$ is divergent?","['improper-integrals', 'integration', 'calculus']"
2610350,Versions of Lusin theorem,"I found this two version of Lusin theorem, without reference Can someone know in any book i can found it : Theorem 1: Let $(X,\beta,\mu)$ a measurable space satisfying: $\bullet$ $X$ is a metric space locally compact, $\bullet$  $\beta$ is a borelian  $\sigma-$algebra. $\bullet$ $\mu$ is a regular measure. Suppose that $A\in \beta$ such that $\mu(A)<+\infty.$ Then, given
  $\varepsilon>0$ there exists $g\in C_0(X)$ such that $\bullet$ $\mu(\{x\in X; g(x)\neq \chi_{A}(x)\})<\varepsilon;$ $\bullet$ $g(x)\in [0,1], x\in X.$ and the second: Theorem 2: $(i)$ $Y$ be a Hausdorff topological space with a countable  bases; $(ii)$ $(\Omega,\beta,\mu)$ a finite measurable space, where 
  $\Omega\subset Y$; $(iii)$ $f:\Omega\to \mathbb{R}$ a measurable  function.    Given
  $\varepsilon>0$, there exists a compact set $K\subset \Omega$  such
  that $(I)$ $f$ is continuous on $K$; $(II)$  $\mu(\Omega\setminus K)<\varepsilon.$","['borel-measures', 'reference-request', 'functional-analysis', 'measure-theory', 'analysis']"
2610393,Understanding Rudin's PMA Theorem 9.17,"$9.17$ Theorem Suppose f maps an open set $E\subset R^n$ into $R^m,$ and f is differentiable at a point $\mathbf{x}\in E.$ Then the partial derivatives $D_jf_i(\mathbf{x})$ exist, and 
\begin{align}
\mathbf{f'(x)e_j}=\sum\limits_{i=1}^m(D_jf_i)\mathbf{(x)u_i} &&(1\le j\le n)\tag{27}
\end{align} $\mathbf{e_i}'$s  and $\mathbf{u_j}'$s are standard bases of $R^n$ and $R^m$ respectively. Proof Fix $j.$ Since f is differentiable at $\mathbf{x},$ $$\mathbf{f(x+}t\mathbf{e_j)}-\mathbf{f(x)}=\mathbf{f'(x)(}t\mathbf{e_j)}+\mathbf{r(}t\mathbf{e_j)}\tag{28}$$ where $\left|\mathbf{r(}t\mathbf{e_j)}\right|/t\to 0$ as $t\to 0.$ The linearity of $\mathbf{f'(x)}$ shows therefore that $$\lim_{t\to 0}\sum\limits_{i=1}^m\frac{f_i(\mathbf{x}+t\mathbf{e_j})-f_i(\mathbf{x})}{t}\mathbf{u_i}=\mathbf{f'(x)e_j.}\tag{29}$$ It follows that each quotient in this sum has a limit, as $t\to 0$(see Theorem $4.10$), so that each $(D_jf_i)(\mathbf{x})$ exists, and then $(27)$ follows from $(29).$ link to theorem $4.10$ Question Suppose I  denote $g_i(t) =\frac{f_i(\mathbf{x}+t\mathbf{e_j})-f_i(\mathbf{x})}{t}\mathbf{u_i}.$ Then $(29)$ is just saying that $\lim_{t\to 0}\sum g_i(t) =L.$ Then why can't I just conclude $\lim_{t\to 0}g_i(t)=l$ for some $l\in R^m.$ Why do I need Theorem $4.10$ here?","['real-analysis', 'analysis']"
