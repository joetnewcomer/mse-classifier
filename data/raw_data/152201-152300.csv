question_id,title,body,tags
2548680,Prove the convergence of $\sum \frac{(-1)^n}{n+(-1)^n}$,"How would one go around proving the convergence of: $ \sum \frac{(-1)^n}{n+(-1)^n} $. I'm fairly certain, that this series converges, but it doesn't do so absolutely $\left( \frac{1}{n+(-1)^n} \approx \frac{1}{n}\right)$. Leibniz criterion of convergence can't be used, because  $\left( \frac{1}{n+(-1)^n} \right) $ is not nonincreasing. Note: I've noticed, that by shuffling terms of the series one would get $\sum (-1)^n a_n$, where $a_n$ is a nonincreasing sequence, but shuffling is not allowed since the series doesn't converge absolutely.","['sequences-and-series', 'convergence-divergence']"
2548740,Partial Derivatives : Show that $\frac{∂x}{∂y}\frac{∂y}{∂z}\frac{∂z}{∂x}=-1$ [duplicate],"This question already has an answer here : Prove that $\frac{\partial x}{\partial y} \frac{\partial y}{\partial z} \frac{\partial z}{\partial x} = -1$ and verify ideal gas law (1 answer) Closed 6 years ago . Let $f : \mathbb{R}^3 \rightarrow \mathbb{R}$. How can i show that If $f(x,y,z)=0$ then $$\frac{∂x}{∂y}\frac{∂y}{∂z}\frac{∂z}{∂x}=-1 $$ Any help will be appreciated.","['multivariable-calculus', 'partial-derivative', 'chain-rule', 'calculus']"
2548820,Asymptotic behaviour of the solution of $\ \ln(x)+e^x=c\ $ for large $c\ $?,"What is the asymptotic behaviour of the solution of the equation $$\ln(x)+e^x=c$$ for large $c$ ? It it clear that $\ln(c)$ is a good approximation. Experimenting with large numbers , I found out that $$\ln(c)-\frac{\ln(\ln(c))}{c}$$ is an excellent approximation. The difference between this number and the solution seems to be of order $O(\frac{1}{c^2})$. A series expansion of the solution for $c\rightarrow\infty$ in terms of $c$ would be vey nice. How can I find the first few terms , lets say upto order $\frac{1}{c^3}$ ?","['asymptotics', 'sequences-and-series', 'calculus']"
2548821,A supermodular game is superadditive. But is a superadditive game is supermodular?,"I came across a presentation by Mohammad T. Hajiaghayi from University of Maryland, where he talks about Coalition Game Theory, and he states that ""every super-additive game is a convex game"". He also states that, $ \lbrace \text{Additive games} \rbrace \subseteq \lbrace \text{Super-additive games} \rbrace \subseteq \lbrace \text{Convex games} \rbrace $. Now I don't quite get it. Where I can easily prove that a convex or supermodular game is super-additive, I can't prove the converse. Definitions: A game $G = (N, v)$ with $v(\varnothing) = 0$ is convex (supermodular) if for all $S,T \subseteq N$, $$v(S \cup T) + v(S \cap T) \geq v(S) + v(T)$$ Also is a superadditive game if for all $S,T \subseteq N$, if $S \cap T = \varnothing$, $$v (S \cup T) \geq v (S) + v (T)$$","['game-theory', 'elementary-set-theory']"
2548868,Compact subgroups have determinant $1$ or $-1$,"The following question is from 'Matrix Groups for Undergraduates' by Kristopher Tapp: Let $G \subset GL_n(\mathbb R)$ be a compact subgroup. (1) Prove that every element of $G$ has determinant $1$ or $-1$. (2) Must it be true that $G \subset O(n)?$ Hint: Consider conjugates of $O(n).$ Attempt: I know the relevant definition of closed (i.e. if a sequence of matrices in $G$ has a limit in $GL_n(\mathbb R)$, then that limit must lie in $G$). I also know a matrix element is bounded if (when it is regarded as an element of $\mathbb R^{n^2}$) it has finite norm. I just cannot really see how these give us any information about the determinant. As for $(2)$, following the hint and considering a conjugate of $O(n)$, when we consider $A \in O(n)$ and $B \in GL_n(\mathbb R)$ we see that 
\begin{equation} 
\begin{split} 
\det BAB^{-1} &= (\det B)(\det A)(\det B^{-1}) \\ 
&= (\pm 1)(\det B)(\det B^{-1}) \\ 
&= \pm 1 
\end{split} 
\end{equation} so we see that conjugates are in the group, implying it is a normal subgroup, but what does this tell us about G? I'm definitely missing a lot here, any help would be appreciated, thanks all.","['determinant', 'group-theory', 'compactness', 'differential-geometry', 'lie-groups']"
2548889,distributing distinguishable and indistinguishable balls(r) into bins(s) with some restrictions,"Sorry for asking so much lately but you guys are the only way to assure me that I'm understanding this homework and not teaching myself the wrong stuff. So my answers to the following question are the following, I would like to check if they're correct because this whole discrete mathematics makes me  unsure about my answers especially that the rules don't always apply: 1- Once a placement of balls has been made, then, since the balls are identical, all we can say is which boxes have
received a ball and which have not. In other words, placing the balls has the same effect as simply choosing k of the n
boxes.
Those boxes that receive a ball are the ones that are chosen, and those that do not receive a ball are not chosen.
Hence, in this case we are making unordered selections, that is, forming combinations of size k, taken from the set of n
boxes. 
${r \choose s} $ 2- since the balls are indistinguishable, we can only tell how many balls each box has received. This
means making a choice of k of the n boxes, but with the possibility that a box may be chosen more than once. Thus,
placing k balls into n boxes, in this case, corresponds to forming an unordered selection, or combination, of size k, taken
from the set of n boxes, but with unrestricted repetitions.${s+r-1  \choose s-1}$ 3- Putting k distinguishable balls into n boxes, with 1 at most in each bin, amounts to the same thing as making an ordered selection of k of the n boxes, where the balls
do the selecting for us. The ball labeled 1 selects the first box, the ball labeled 2 selects the second box, and so on. In other
words, distributing k distinguishable balls into n distinguishable boxes, with exclusion, is the same as forming a permutation
of size k, taken from the set of n boxes $$\prod_{i=0}^{r-1} (s-i)$$ 4- we can think in terms of selecting k of the n boxes. As
before, the balls do the selecting for us
but this time more than one ball may go into the same box, which means that the same box may be chosen more than
once.
Therefore, we are still dealing with ordered selections, or permutations, of the boxes, but now with unrestricted repetitions. $$r^s$$","['permutations', 'discrete-mathematics']"
2548926,How to prove this lemma related to Rolle's theorem,"For any function $f$ denote by $Z(f)$ and $Z_o(f)$ the cardinalities of $f^{-1}(0)\cap[0,1]$ and $f^{-1}(0)\cap(0,1)$, respectively. Let $H=\{f\in C^\infty(\mathbb{R}): \text{supp}(f) = [0,1]\}$ From this question we have Lemma 1 Let $q:x\mapsto(x-r)p(x)$ where $p\in H$ and $r\in\mathbb{R}$. Then $Z(q^{(n)})\geq Z(p^{(n-1)})+1$ for all $n\in\mathbb{N}$. Proof Note that $q^{(n)}(x) = n p^{(n-1)}(x) + (x-r)p^{(n)}(x)$. Hence $r$ is a root of $q^{(n)}$ if and only if it is a root of $p^{(n-1)}$. Moreover we have $$\underbrace{(x-r)^{n-1}q^{(n)}(x)}_{\text{LHS}} = n (x-r)^{n-1} p^{(n-1)}(x) + (x-r)^np^{(n)}(x) = \underbrace{\frac{d}{dx}(x-r)^n p^{(n-1)}(x)}_{\text{RHS}}$$ If $q^{(n)}(r) = 0$ then $Z(q^{(n)}) - 2 = Z_o(\text{LHS}) = Z_o(\text{RHS}) \geq Z(p^{(n-1)}) - 1$. If $q^{(n)}(r) \neq 0$ then $Z(q^{(n)}) - 1 = Z_o(\text{LHS}) = Z_o(\text{RHS}) \geq Z(p^{(n-1)})\quad\quad\quad\quad\text{q.e.d.}$ $\text{ }$ The following modification Lemma 2 Let $q:x\mapsto(x-r)(x-\bar{r})p(x)$ where $p\in H$ and $r\in\mathbb{C}\setminus\mathbb{R}$. Then $Z(q^{(n)})\geq Z(p^{(n-2)})+2$ for all $n\in\mathbb{N}\setminus\{1\}$. I tried to prove the same way by writing $$\frac{j(x)k(x)}{(x-r)(x-\bar{r})}q^{(n)}(x) = \frac{d}{dx}\left(j(x)^2 k(x)\frac{d}{dx}j(x)^{-1}p^{(n-2)}(x)\right)$$
where $\frac{d}{dx}$ is differentiation along the real axis and $j(x)=c_1(x-r)^{1-n}+c_2(x-\bar{r})^{1-n}$ $k(x) = c_3 ((x-r)(x-\bar{r}))^n$ $c_1,c_2,c_3\in\mathbb{C}$ The real and imaginary part of $j(x)^{-1}$ are proportional, and $j(x)k(x)\in \mathbb{R}$ when choosing
$c_1 = e^{i d_1}\\c_2=e^{i d_2}\\c_3=e^{-i\cdot 
 (d_1+d_2)/2}\\d_1,d_2\in\mathbb{R}$ but even then I couldn't get Rolle's theorem to work unless $j(x)$ has no roots on $(0,1)$. I have looked at many functions/different $n$s and even when choosing $r$ such that $j(x)$ has such roots, it seems impossible to find a counterexample to the lemma. How to prove the lemma? Or maybe this simpler version which remains when removing some of the assumptions: If $p$ is smooth with $p^{(n-2)}$ having $m$ roots on $[0,1]$, then how to prove that the $n$'th derivative of $x \mapsto p(x)(x-r)(x-\bar{r})$ has at least $m-2$ roots on $(0,1)$.","['derivatives', 'real-analysis', 'roots']"
2548989,Proof of Discontinuity Criterion for functions,"I would really appreciate a proof for the Discontinuity Criterion Theorem for functions. It is stated as such... Let $A$ be a subset of $\mathbb{R}$, let $f: A \to \mathbb{R}$ and let $c \in A$. Then $f$ is discontinuous at $c$ iff there exists a sequence $x_n \in\mathbb{R}$ such that $x_n$ converges to $c$ but the sequence $f(x_n)$ does not converge to $f(c)$. Thank you!","['continuity', 'real-analysis', 'functions']"
2549045,"Is this relation antisymmetric, connex?","Given is relation  $R=\left\{(a,b) \in S \times S: a+b \text{ is an
even number }\right\}$ over the set $S=\mathbb{N}$. Is this relation
  antisymmetric, connex? Justify your decision. A relation is antisymmetric if $\forall x,y \in A: xRy \wedge yRx \Rightarrow x=y$ A relation is connex, if $\forall x,y \in A: (x,y) \in R \vee (y,x) \in R $ or $x=y$ The sum of two numbers is even if and only if both summands are even or both summands are odd. antisymmetric: If we have $aRb$, then both summands are either even or odd, same applies for $bRa$. Buth now the implication says $x=y$, so we might end up with an odd number which means that this relation is not antisymmetric. connex: If we concentrate at the 3th condition of connexivity, i.e. $x=y$, then we will end up with an even number because both summands will either be even or uneven and thus the sum will be even. So the relation is connex. This task was confusing me a lot.. I'm pretty sure that this relation is reflexive, symmetric and transitive but I'm not sure about these two properties. Did I do it correctly?","['relations', 'elementary-set-theory', 'discrete-mathematics']"
2549140,Definition of meagre set,"While reading the section about Baire Spaces in the book ""Espaços Métricos"" by Elon Lages Limas I found the following statement. I shall translate from portuguese after giving some context. The author motivates the definition of a meagre set as something analogous to the notion of a set of measure zero. Therefore he refers to it as ""insignificant"" (in a topological sense of course). Firstly he points out that they should be preserved when one takes subsets and countable unions. Then he gives an example to show it wouldn't suffice to require such a set to have empty interior since $\text{int}( \mathbb{Q})=\emptyset$ and $\text{int}( \mathbb{R} \setminus \mathbb{Q})=\emptyset$ but $\mathbb{Q} \cup \mathbb{R} \setminus \mathbb{Q}=\mathbb{R}$. Translation: A better idea would be to consider a set $X\subset M$ whose closure $\overline X$ has empty interior in $M$. In fact, if $\text {int}( \overline X)=\emptyset $ and $\text {int}( \overline Y)=\emptyset $ then $$\text {int}( \overline {X\cup Y})=\text {int}( \overline X \cup \overline Y) (\color {red}\subset) \supset \text {int}( \overline X) \cup \text {int}( \overline Y)=\emptyset$$ I highlighted in red color where I think the author made a mistake. The author then says that it is not true that if $\text {int}( \overline {X_n})=\emptyset$ for all $n \in \mathbb {N}$ then $X=\cup X_n$ still has the property that $\text {int}( \overline X)=\emptyset$. (Simply take the rationals as an example). Actually, if what I quoted above was right it would violate this statement and that's why this caught my eye. My question if indeed I stumbled upon a mistake in the book. Also, I was enjoying the way the author was motivation the definition and I don't see why, intuitively (using the idea of ""insignificant"" topological set), we require that the property holds when we take countable unions. Why not uncountable?","['general-topology', 'metric-spaces', 'baire-category', 'definition']"
2549211,Definition of a quotient curve?,"If $C \subseteq \mathbb{P}^m(\mathbb{\overline{F_q}})$ is an irreducible projective curve and $H \leq Aut(C)$ is a finite subgroup of the group of the automorphisms of $C$, then I know that the set $C/H = \{ Orb_H(x) | x \in C\}$ is again a projective curve. However I don't know what exactly are its projective coordinates or even its function field. Can some one tell me, please? I've searched through the internet about this, but with no success...","['algebraic-curves', 'quotient-spaces', 'algebraic-geometry']"
2549296,$ y''\sin^2 x + y' \tan x + y \cos^2x = 0 $,"I have been going insane over the following differential equation over the past few days. $y''\sin^2(x) + y'\tan(x)  + y\cos^2(x) = 0 $ The assignment is: $a)$ Show that $x=0$ is a regular singular point of the differential equation. $b)$ Find the indicial roots. $c)$ Show that $\sin(\ln(\sin x))$ and $\cos(\ln(\sin x))$ are solutions of the differential equation for some $0<x<R$ and $R>0$ I have tried many times to show $a)$ : $p(x)=\tan x/\sin^2x$ and $q(x)=\cos^2x/\sin^2x$ are both not analytic at $x=0$ . $$xp(x)=\frac{x\tan x}{\sin^2x} = \frac{x}{\sin x \cos x}$$ Trying to compute this series directly will always yield a sine in the denumerator, making this fail at x=0. So I tried some tricks, the one I was most convinced of was determining the series of $\sin x\cos x$ and then dividing through by $x$ : $$\sin x\cos x=\frac{1}{2}\sum_0^k(-1)^k\frac{(2x)^{2k+1}}{(2k+1)!}$$ So then $$\frac{x}{\sin x \cos x}= \frac{x}{\frac{1}{2}\sum_0^k(-1)^k\frac{(2x)^{2k+1}}{(2k+1)!}}=\frac{2}{\sum_0^k(-1)^k\frac{2^{2k+1}(x)^{2k}}{(2k+1)!}}$$ Setting $x=0$ here yields $\frac22=1$ . So I thought this could work and that $xp(x)$ could be considered analytic at $0$ like this. A similar argument I tried for $x^2q(x)$ was: $$\frac{x^2\cos^2x}{\sin^2x}=\frac{x^2}{\tan^2x}$$ And then taking the series of $\tan x = x+\frac{x^3}{3}+\frac{2x^5}{15}+...$ So that $x^2q(x)$ becomes: $$\frac{x^2}{( x+\frac{x^3}{3}+\frac{2x^5}{15}+...)( x+\frac{x^3}{3}+\frac{2x^5}{15}+...)} = \frac{1}{1 + 2\frac{x^2}3+\frac{17x^4}{45}+...}$$ The series check out as far as I can tell, but I'm not convinced I can really say that $xp(x)$ and $x^2q(x)$ are analytic at the origin because of this. Any insight as to how this power series expansion conclusively shows that they are analytic is much appreciated. If the expansions are correct, it follows that $p_0=q_0=1$ , which will give indicial roots $+/-i$ . I have never seen complex indicial roots and they aren't covered by the course I'm taking. It feels like these are wrong and this is the main reason I started doubting my result for a). Showing the solutions in c) are true is simple (derive and plug in the equation). However I can't find the number R they want me to find. I thought they meant the interval of convergence of the series found in a), as this is directly related to the solution. I attempted to compute those: I used the denumerator only, because I didn't have a clue how to calculate a radius of convergence of the reciprocal of a power series. The expansion for $xp(x)$ gives: $$a_k= (-1)^k\frac{2^{2k+1}x^{2k}}{(2k+1)!}$$ and $$a_{k+1}=(-1)^{k+1}\frac{2^{2k+3}x^{2k+2}}{(2k+3)!}$$ $$\lim_{k->\infty}\frac{a_{k+1}}{a_{k}}=\lim_{k->\infty}\frac{(-1)^{k+1}\frac{2^{2k+3}x^{2k+2}}{(2k+3)!}}{(-1)^k\frac{2^{2k+1}x^{2k}}{(2k+1)!}}=\lim_{k->\infty}\frac{-4}{(2k+3)(2k+2)}x^2=0$$ So the whole thing doesn't even converge to begin with, which means my solution obtained in a) must be wrong. I don't know what or where exactly I am going into the wrong and any advice is welcome.","['frobenius-method', 'ordinary-differential-equations']"
2549315,Studying ODEs... is it worth it?,"I am a first year Master's student in applied mathematics. I'm currently taking a Mathematical Physics class and we have been studying techniques for solving ODEs. There is some overlap with undergrad ODEs. Solving these problems (ex: $x^2y''+xy'-16y=8x^4$) often takes lots of time and lots of algebraic manipulation, all to find a solution that I won't use. I understand that it is important to become familiar with these equations and have an idea about how they are solved. But I am failing to see the big picture. How will knowing myriad solution techniques for these ODEs help my future research in applied mathematics? Have you found this knowledge useful in your research?","['research', 'ordinary-differential-equations', 'learning']"
2549347,How to derive tangential divergence (and thus Laplace-Beltrami operator) from tangential gradient?,"If we define the tangential gradient for some appropriate function $f$ on a hypersurface $\Gamma$ \begin{align}
\nabla_\Gamma f = P \nabla f = (I - n n^T)\nabla f, 
\end{align} then we can derive its tangential divergence \begin{align}
\text{div}_\Gamma f = \nabla_\Gamma^T f = \nabla^T(f - n^T f n) = \mathrm{tr}(\nabla_\Gamma f)
\end{align} and thus the Laplace-Beltrami operator \begin{align}
\Delta_\Gamma f = \text{div}_\Gamma(\nabla_\Gamma f)
\end{align} A lecture note says we can derive but doesn't say how to derive . 
Can you show me step by step why it holds true? In addition, for some $g \in C^1$ the tangential divergence satisfies \begin{align}
\text{div}_\Gamma (gf) = g \, \text{div}_\Gamma f. 
\end{align} Why?","['manifolds', 'differential-geometry', 'field-theory', 'linear-algebra']"
2549355,Affine Morphism - Equivalent Formulation,"Let $f:X \to Y$ be a morphism of schemes. It is called affine if there exist a covering $\{U_{\alpha}\}_{\alpha}$ by open affine $U_{\alpha}$ such that every $f^{-1}(U_{\alpha}$ is affine. My question is why this property already indeces that for every open affine $U \subset Y$ the preimage $f^{-1}(U)$ is affine? Indeed thats clear to me that that's enough to consider the case that $Y:=Spec(R)$ is affine (because by def for every open affine $U$ the restricted map $f: f^{-1}(U) \to U$ is affine). Futhermore, we can find $r_1, ..., r_i \in R$ such that $Y= \bigcup D(r_j)$ where $D(r_j)$ are open affine, such that $X = \bigcup f^{-1}(D(r_j))=\bigcup D(f^*(r_j))$ is covered by open affine $ f^{-1}(D(r_j))=D(f^*(r_j))$. But that's not clear to me how I can deduce from this that $X$ is affine, therefore $X = Spec(\mathcal{O}_X(X)$.","['schemes', 'algebraic-geometry']"
2549380,"Is there a simple proof that if $(b-a)(b+a) = ab - 1$, then $a, b$ must be Fibonacci numbers? [duplicate]","This question already has answers here : Prove the inequality $\left|\frac{m}{n}-\frac{1+\sqrt{5}}{2}\right|<\frac{1}{mn}$ (2 answers) Closed 6 years ago . Consider the identity $(b-a)(b+a) = ab - 1$, where $a, b$ are nonnegative integers. We can also express this identity as $a^2 + ab - b^2 = 1$. This identity is clearly true when $a = F_{2i-1}$ and $b = F_{2i}$, where $F_i$ is the $i^{th}$ term of the Fibonacci sequence. This is equivalent to one case of Cassini's identity, $(F_{2i-1})(F_{2i+1}) = F_{2i}^2 + 1$, and is easily proven by induction or several other simple elementary means. My question is this: Is there a simple elementary proof that these Fibonacci numbers are the only solutions of this identity? By simple elementary proof, ideally I mean a proof using methods and steps that a mathematically gifted high school student could follow and understand. Alternatively, I could define it as a proof using methods that would have been known to mathematicians in Cassini's time, in the late 17th century. In other words, I am looking for a proof that does not rely on more advanced methods such as quadratic number fields or generalized solutions of Pell equations.","['number-theory', 'fibonacci-numbers', 'elementary-number-theory']"
2549426,Expected value of $X$ given $X > 15$,"If $X$ is a poisson random variable with $\lambda =\frac{1}{15}$, what is the expected value of $X$ given $X > 15$ ? I should know this but it's been a while since my intro to probability class.",['probability']
2549455,Is every path a reparametrization of a path with non zero velocity?,"Let $\alpha:[0,1] \to \mathbb{R}^n$ be a non-constant smooth path, and suppose $\alpha'(0)=0$. Do there exist a smooth path $\beta:[a,b]\to \mathbb{R}^n$ and a smooth increasing function $h:[0,1] \to [a,b]$, such that $\alpha=\beta \circ h,h(0)=0$ and $\beta'(0) \neq 0$. Note $h$ must satisfy $h'(0)=0$. Example: $n=1,\alpha(t)=t^2$, take $h(t)=\alpha(t)$, $\beta=\text{Id}$. It's easy to shows that if the answer is positive for $n=1$, then it is positive for every dimension $n$. So we are reduced to the one-dimensional case.","['multivariable-calculus', 'parametrization', 'calculus', 'derivatives']"
2549538,"Does $\Re(\operatorname{arccosh}(z))$ vanish with $\Im(z)=0, -1<\Re(z)<1$?,","This may seem like a homework question to you but I'm a software engineer trying to get an argument for the improvement of arb, a complex ball open source computation package. I tried Wolfram and Sage but both give me complicated expressions for $\Re(\operatorname{arccosh}(z))$ that do not reveal the stated simple conjecture that it vanishes for $-1<z<1$. At the moment the arb package returns an (epsilon) nonzero real part for $\operatorname{arccosh}(z)$, with $-1<z<1$ but if there were a proof this could be improved. Can you please help?",['complex-analysis']
2549586,Showing a that the derivative exists on every $x$ but is not continuous at $x=0$,"So the question goes as following: Let $f(x)=x^2\sin(1/x)$ if $x\not=0$ show that $f'(x)$ exist at every $x$ but $f'$ is not continous at $x=0$. I'm not really sure how to show it. Usually I would show that when x approaches 0 from either side it will have different limits, but this is clearly not the case here. All help is welcomed!","['derivatives', 'continuity', 'calculus']"
2549630,What is meant by homogeneous boundary conditions?,"I am sorry if this is basic knowledge for differential equations but it has been a long time since I took the class, I probably learnt it and forgot about it. I would appreciate the explanation. Thank you!","['ordinary-differential-equations', 'analysis']"
2549651,Finding $\lim_{x \to \infty} x(\ln(1+x) - \ln(x))$ without l'Hopital,I solved the limit $$\lim_{x \to \infty} x(\ln(1+x) - \ln(x))$$ by writing it as $\lim_{x \to \infty} \frac{\ln(\frac{1+x}{x})}{\frac{1}{x}}$ and applying l'Hopital rule but is it possible to solve it without using l'Hopital rule?,"['limits-without-lhopital', 'calculus', 'limits']"
2549684,The étalé space associated to the sheaf of continuous functions over $\mathbb{R} $?,"We consider the sheaf $C$ of continuous functions over the reals, that's it, for any open subset $U$ of the reals we put $C(U) $ as the set of all continuous functions from $U$ to $\mathbb {R} $. There is an equivalence between the category of sheaves over a topological space $X$ and the category of étalé space over $X$. So for this sheaf, there must be an étalé space $p:E\longrightarrow \mathbb {R} $ such that the sheaf of sections is isomorphic to this sheaf. My question is there a description of the space $E$? What properties does $E$ have? $E$ is Haussdorf, compact, separable, connected,...?","['general-topology', 'sheaf-theory']"
2549731,Is it correct to say that the second derivative at the maximum likelihood estimator can never be positive?,"The maximum likelihood estimator is found to be the critical value of the likelihood function, that is also the local maximum. I understand that if the function is differentiable then you can analyse the second derivative at this point and if it is negative then it is indeed the local maximum. I was wondering if there were any cases that contradict this? I understand that some cases the function isn't differentiable or you need to calculate the hessian matrix to see that the function isn't just a saddle point. I am talking just in the simple single variable case. So is it okay to say: If $\frac{d^2L(\hat{\theta)}}{d\theta^2} > 0$ then $\hat{\theta}$ CANNOT be the MLE of $L(\theta)$?","['maxima-minima', 'fixed-point-theorems', 'statistics', 'maximum-likelihood']"
2549753,Sums of Powers as Sums of Stirling Numbers and Falling Factorials,"I am able to prove the following identity:
$$\sum_{k = 0}^n k^p = \sum_{j = 0}^p {p \brace j} \frac{(n+1)^\underline{j+1}}{j+1},$$
where $p$ and $n$ are non-negative integers, ${p \brace j} = S(p, j)$ is a Stirling number of the second kind, and $x^\underline{m} = x(x-1)\cdots (x - m + 1)$ is the falling factorial function. My question: Is this identity commonly known in combinatorics?","['stirling-numbers', 'combinatorics']"
2549760,"counting the number of antichains in $P_{m,n}$","Define m and n be integers. $P_{m,n}$ denote the set of pairs of integers
$(i,j)$ with $1\leq i\leq m$ and $1\leq j\leq n$. Define a partial order on $P_{m,n}$ as:
\begin{equation}
(i,j)\leq (i^{'},j^{'}) \iff (i\leq i^{'} and \  j\leq j^{'})
\end{equation} 
I've done the first part as ""count the number of maximal chains in $P_{m,n}$"", but I don't know how to count the number of the antichains in $P_{m,n}$(As it needs to count every size!).
Thank you for your help!","['combinatorics', 'graph-theory', 'order-theory']"
2549784,Calculate expected value and variance of normally distributed..,"$X_1,.., X_n$ are independent, identical distributed random variables.
  They are continuous, too. Let $$\bar{X}= \frac{1}{n} \cdot
(X_1+..+X_n)$$ Determine the expected value and variance of $\bar{X}$ if $$X_i \text{
is normally distributed, } X_i \sim N(\mu, \sigma^2)$$ I don't know how do it good? I check expected value of normal distribution on internet. This is $$E(X) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}xe^{\frac{-x^2}{2}}dx$$ Now I try form it so I have good solutin $$-\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{\frac{-x^2}{2}}d\left(-\frac{x^2}{2}\right)= -\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}= 0$$ It work like this? But no idea about variance..?","['probability-theory', 'probability-distributions', 'probability', 'discrete-mathematics']"
2549850,Calculus Derivative—Finding unknown constants [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Find all values of $k$ and $l$ such that
  $$\lim_{x \to 0} \frac{k+\cos(lx)}{x^2}=-4.$$ Any help on how to do this would be greatly appreciated.","['derivatives', 'constants', 'calculus']"
2549930,"Integrating with respect to arc length (ds) vs. dx, or dy","I'm learning line integrals. And I understand the concept of integration with respect to $ds$, but I don't know what $dx$ and $dy$ actually mean. In my calc textbook, it just says that $dx$ is the same thing as $x^{'}(t)dt$ and likewise for dy being equal to $y^{'}(t)dt$. But what do they mean physically. Intuitively I know that ds is the arc length, or a certain curtain in a sense. And I also know that if ds is present I just use the arc length formula, but for dx and dy I don't have to. I looked at the other questions but I never understood that well.","['multivariable-calculus', 'integration', 'calculus', 'vector-spaces']"
2549933,Pointwise Convergence and Convergence in the mean of Riemann-integrable functions,"Let $(f_n)_{n\in\mathbb{N}}$ be a sequence of Riemann-integrable functions on $[a, b]$.
  Assume that $(f_n)_{n\in\mathbb{N}}$ converges pointwise to $f$ on $[a, b]$ and converges in the mean to $g$ on $[a, b]$, where $f$ and $g$ are both continuous. Show that $f = g$. ($(f_n)_{n\in\mathbb{N}}$ converges in the mean to g on $[a , b]$ means that $\lim_{n\to\infty} d(f_n, g) = 0$, where $d$ is the $L_2$ metric.) I am studying real analysis now and want to prove this statement.
But I don't know how to use condition ""$(f_n)_{n\in\mathbb{N}}$ converges in the mean to g on $[a , b]$."" How can I prove this?","['riemann-integration', 'convergence-divergence', 'analysis', 'limits']"
2549935,Implications of the fact that anti-derivatives form an equivalence class?,"I just started learning about equivalence classes in the context of modular arithmetic, and learned that the union of the $n$ equivalence classes modulo $n$ is $\mathbb{Z}$. My professor then applied this to the fact that we've seen equivalence classes before when talking about anti-derivatives - how $\int f(x) dx + c$
is the equivalence class of anti-derivatives of $f$. I apologize if this is not quite correct, and I'd welcome a correction, because I'm not too sure of the precise language used when speaking of relations. I'm wondering if because the union of equivalence classes is the whole set, that any differentiable function must have a class of anti-derivatives $g(x)$ such that $\frac{d}{dx} g(x) = f(x)$? I was reading the response to this question, which is similar to mine, but I'm wondering if due to the fact that we can form an equivalence relation, there must be an anti-derivative for any differentiable function?","['elementary-set-theory', 'equivalence-relations', 'relations', 'calculus']"
2549962,Deriving the cartesian del operator from cylindrical del operator,"I'm having trouble going from the cylindrical form of the del operator to the cartesian form. Here is my attempt so far: $\rho = \sqrt{x^2 + y^2}$ $\theta = \arctan(\frac{y}{x})$ $\nabla = \frac{\partial}{\partial \rho} \hat{\rho} + \frac{1}{\rho} \frac{\partial}{\partial \theta} \hat{\theta} + \frac{\partial}{\partial z} \hat{z} $ $\frac{\partial}{\partial \rho} = \frac{\partial x}{\partial \rho}\frac{\partial}{\partial x} + \frac{\partial y}{\partial \rho}\frac{\partial}{\partial y}$ $\frac{\partial x}{\partial \rho} = (\frac{\partial \rho}{\partial x})^{-1} = (\frac{x}{\rho})^{-1} = \frac{\rho}{x} = \frac{1}{\cos(\theta)}$ $\frac{\partial y}{\partial \rho} = (\frac{\partial \rho}{\partial y})^{-1} = (\frac{y}{\rho})^{-1} = \frac{\rho}{y} = \frac{1}{\sin(\theta)}$ thus, $\frac{\partial}{\partial \rho} = \frac{1}{\cos(\theta)}\frac{\partial}{\partial x} + \frac{1}{\sin(\theta)}\frac{\partial}{\partial y}$ Also: $\hat{\rho} = \cos(\theta) \hat{x} + \sin(\theta) \hat{y}$ next, $\frac{\partial}{\partial \theta} = \frac{\partial x}{\partial \theta}\frac{\partial}{\partial x} + \frac{\partial y}{\partial \theta}\frac{\partial}{\partial y}$ $\frac{\partial x}{\partial \theta} = (\frac{\partial \theta}{\partial x})^{-1} = (\frac{x^2}{y^2 + x^2} \frac{-y}{x^2})^{-1} = -\frac{\rho^2}{y} = -\frac{\rho}{\sin(\theta)}$ $\frac{\partial y}{\partial \theta} = (\frac{\partial \theta}{\partial y})^{-1} = (\frac{x^2}{y^2 + x^2} \frac{1}{x})^{-1} = \frac{\rho^2}{x} = \frac{\rho}{\cos(\theta)}$ thus, $\frac{\partial}{\partial \theta} = -\frac{\rho}{\sin(\theta)}\frac{\partial}{\partial x} + \frac{\rho}{\cos(\theta)}\frac{\partial}{\partial y}$ also: $\hat{\theta} = -\sin(\theta) \hat{x} + \cos(\theta) \hat{y}$ so: $\nabla = \frac{\partial}{\partial \rho} \hat{\rho} + \frac{1}{\rho}\frac{\partial}{\partial \theta} \hat{\theta} + \frac{\partial}{\partial z} \hat{z} = (\frac{1}{\cos(\theta)}\frac{\partial}{\partial x} + \frac{1}{\sin(\theta)}\frac{\partial}{\partial y}) (\cos(\theta) \hat{x} + \sin(\theta) \hat{y}) + (-\frac{1}{\sin(\theta)}\frac{\partial}{\partial x} + \frac{1}{\cos(\theta)}\frac{\partial}{\partial y}) (-\sin(\theta) \hat{x} + \cos(\theta) \hat{y}) + \frac{\partial}{\partial z} \hat{z}$ Simplifying this: $\nabla = \frac{\partial}{\partial x} \hat{x} + \tan(\theta)\frac{\partial}{\partial x} \hat{y} + \cot(\theta)\frac{\partial}{\partial y} \hat{x} + \frac{\partial}{\partial y} \hat{y} + \frac{\partial}{\partial x} \hat{x} - \cot(\theta)\frac{\partial}{\partial x} \hat{y} - \tan(\theta)\frac{\partial}{\partial y} \hat{x} + \frac{\partial}{\partial y} \hat{y} + \frac{\partial}{\partial z} \hat{z}$ $ = 2\frac{\partial}{\partial x} \hat{x} + 2\frac{\partial}{\partial y} \hat{y} + (\tan(\theta) - \cot(\theta))\frac{\partial}{\partial x} \hat{y}
+ (\cot(\theta) - \tan(\theta))\frac{\partial}{\partial y} \hat{x} + \frac{\partial}{\partial z} \hat{z}$ but this isn't the cartesian del operator. Where did I go wrong?","['derivatives', 'vector-analysis']"
2549965,What is the exponential map?,"I am well versed in Calculus but I have never taken a differential geometry course. I am trying to understand what the exponential map is, with the main purpose to approximate a geodesic line on an elliptical manifold. I have read a couple of sites that try to describe it, but doing depth first search on every single term I encounter that I can't recognize is not very efficient. Can someone give an intuitive explanation of what the exponential map is?","['differential-geometry', 'calculus', 'geometry']"
2549971,Rudin's Principle of Mathematical Analysis Theorem 2.14 Question,"$\mathbf{Theorem 2.14:}$
Let $A$ be the set of all sequences whose elements are the digits $0$ and $1$. Then A is uncountable, meaning there does not exist a one-to-one mapping of A onto $\mathbb{Z}$. For reference, elements of $A$ have this form $(0,1,0,1,0,0,0,1,1,1,1,1,\cdots)$ $\mathbf{Question:}$ Please bear with me. I know I'm wrong I don't know why. What is wrong with this logic? Let $E_1$ be the set containing all the sequences with just one $1$ in the sequence. i.e. $E_1 = \left\{(1,0,0,\cdots),(0,1,0,0,\cdots),(0,0,1,0,0,\cdots),\cdots\right\}$. $E_1$ is countable. Let $E_{2k}$ be the set containing all the sequences with only two $1$'s in the sequence where the next $1$ in the sequence is $k$ units next to the first $1$. $E_{21} = \left\{(1,1,0,\cdots),(0,1,1,0,0,\cdots),(0,0,1,1,0,0,\cdots),\cdots \right\}$
$E_{22} = \left\{(1,0,1,0,\cdots),(0,1,0,1,0,\cdots),(0,0,1,0,1,0,\cdots),\cdots \right\}$ Let the union of the sets $E_{2k}$ for ($k=1,2,3,\cdots$) be called $E_2$. $E_2$ is countable. Let $E_{3ij}$ be the set of sequences with only three $1$'s where the second $1$ is $i$ units away from the first $1$, and the third $1$ is $j$ units away from the second. For example: $E_{311} = \left\{(1,1,1,0,\cdots),(0,1,1,1,0,\cdots),(0,0,1,1,1,0,\cdots),\cdots\right\}$
$E_{312} = \left\{(1,1,0,1,\cdots),(0,1,1,0,1\cdots),(0,0,1,1,0,1,0,\cdots)\cdots\right\}$
$E_{322} = \left\{(1,0,1,0,1,\cdots),(0,1,0,1,0,1,0\cdots),(0,0,1,0,1,0,1\cdots)\cdots\right\}$
$E_{333} = \left\{(1,0,0,1,0,0,1\cdots),(0,1,0,0,1,0,0,1,\cdots),(0,0,1,0,0,1,0,0,1,\cdots)\cdots\right\}$ Let $E_3$ be the union of all the sets $E_{3ij}$ with ($i,j=1,2,3,\cdots$).
Since each $E_{3ij}$ is countable the union of them is countable, so $E_3$ is countable. The zero sequence is just one sequence, $E_1$ is countable, $E_2$ is countable, $E_3$ is countable. Now if we continue defining the sets in this manner then each set $E_k$ (where $k$ is the number of $1$'s in the sequences) will be a countable set. So, the union of all these sets will be countable and it will represent all the sequences in A. Thank you in advance to anyone who reads all of this and answers!","['real-analysis', 'sequences-and-series', 'elementary-set-theory']"
2549979,Pi series that converges arbitrarily fast.,"The old series for $\pi$ is this alternating series: $$\pi = 4 \sum_{i=0}^{\infty}\frac{(-1)^i}{2i+1}$$ Now, as already noticed, the series is alternating: adding one term overshoots $\pi$ every time. The partial sums evolve like this: Now, I thought: let's average the partial sum K and partial sum K+1. This will converge faster, right? The result is this: Where the original series is still visible in light grey. Indeed this is a success! It converges faster. But wait! It again is an alternating series. So we can repeat this trick over again. I did all the maths and it turns out you can keep repeating this trick for ever. Which basically causes you to be able to construct a series expansion for $\pi$ with arbitrary convergence speed. I plotted the $log(abs(\pi-s_n(k)))$ functions for some of these series: In the figure above, the selected graph is the Euler Transformed variant: $$\pi = \sum_{n=0}^{K}{\frac{\left( n! \right)^{2}2^{n+1}}{\left( 2n+1 \right)!}}$$ The fastest converging one on this figure is the one obtained after applying this trick 10 times (I wrote some Haskell to calculate this): $$s_{10}\left( k \right)= 4\sum_{i=0}^{k}{\frac{\left( -1 \right)^{i}}{2i+1}} +\; \left( -1 \right)^{k}\; \left( \; -\; \frac{\left( 3.99609375 \right)}{\left( 2k+3 \right)}\; +\; \frac{\left( 3.95703125 \right)}{\left( 2k+5 \right)}\; -\; \frac{\left( 3.78125 \right)}{\left( 2k+7 \right)}\; +\; \frac{\left( 3.3125 \right)}{\left( 2k+9 \right)}\; -\; \frac{\left( 2.4921875 \right)}{\left( 2k+11 \right)}\; +\; \frac{\left( 1.5078125 \right)}{\left( 2k+13 \right)}\; -\; \frac{\left( 0.6875 \right)}{\left( 2k+15 \right)}\; +\; \frac{\left( 0.21875 \right)}{\left( 2k+17 \right)}\; -\; \frac{\left( 4.296875\times10^{-2} \right)}{\left( 2k+19 \right)}\; +\; \frac{\left( 3.90625\times10^{-3} \right)}{\left( 2k+21 \right)} \right)$$ The nice thing about this is that it requires only 10 additional terms, for significant faster convergence and that the rest remains the same. Now this was a lot of fun, but: Does this technique have a name? Is this result of any significance, as I seem to be able to reach arbitrary rate of convergence, at virtually the same cost? Other series are still faster, I think, because the $log(abs(∆))$ plots get flat after a while, in comparison to the Euler Transformation variant.","['sequences-and-series', 'convergence-divergence', 'pi']"
2549986,Executing Dijkstra's algorithm starting at 1 to this graph,"I tried to apply Dijkstra's algorithm to this graph; however, the end result after completing the table and updating the shortest path consistently was $(1,2,5,6)$. But when I went to the tutor a week ago, we concluded that the correct answer was $(1,2,3,5,6)$; I didn't quite understand how he got that result. Therefore, which one of the answers is correct, and most importantly why?","['graph-theory', 'algorithms', 'discrete-mathematics']"
2550043,How to convert a summation into a closed form,"I stumbled upon two problems that I solved in an exam a while ago; however, the only reason why I solved them was through trial and error by picking each choice from the choices available, I set an upper limit and compared the two values from the closed form and the summation, and when they matched I knew that that closed form was the right answer. But this isn't very beneficial since I don't know the logic behind converting the summation to a closed form. 
So my question is, what are the steps that converted these two summations to its respective closed forms",['discrete-mathematics']
2550050,Bound on the absolute difference between mean and median of a random variable,"Let $X$ be a real-valued random variable such that, for all $t>0$,
$$\mathbb{P}\{\lvert X-M\rvert >t\} \leq a e^{-t^2/b} $$ where $M$ is the median and $a > 0,b >0$. How can I show that $|M-\mathbb{E}[X]\rvert \leq \sqrt{ab}$?","['probability-theory', 'inequality', 'upper-lower-bounds', 'random-variables']"
2550064,Proof of Generalized Cayley's formula,"I'd like to prove the equation following: $$x_1x_2x_3...x_n(x_1 + x_2 + ... x_n)^{n-2} = \sum_Tx_1^{d_{T(1)}}x_2^{d_{T(2)}}...x_n^{d_{T(n)}}\tag 1$$ where the sum is over all spanning trees $T$ in $K_n$ and $d_{T(i)}$ is the degree of $i$ in $T$ I heard this's called Cayley's generalized formula, which is the number of $trees$ in $n$ vertices: $n^{n-2} \tag 2$ Only the morphological similarity between $(1)$ and $(2)$ is the power $n-2$. I think in this case induction might work well, but honestly $induction$ itself is not a productive way of proving something to reveal one's mathematical identity to me including this case, proof of $(1)$. Any hint to prove this equation in combinatoric and algebraic way?","['combinatorics', 'graph-theory']"
2550068,Summary of my understanding of sequences and series' convergence and divergence?,"I'm trying to summarise my understanding of infinite sequences, series, and their relationships with respect to convergence at the fundamental level. Here is what I know. How much of this is correct? First off, here's a table of the notations that I use, and their corresponding meaning. My understanding is that: The sequence $\lbrace a_n \rbrace _{n=0}^{\infty}$ converges if
$$\lim\limits_{n\to\infty}a_n=L_{a}.$$ The infinite series $\sum\limits_{n=0}^{\infty}a_n$ converges if its
sequence of partial sums,  $\lbrace s_n \rbrace _{n=0}^{\infty}$, has
a limit, i.e.$$\lim\limits_{n\to\infty}s_n=L_{s}.$$ If the infinite series $\sum\limits_{n=0}^{\infty}a_n$ converges,
then the limit of the sequence $\lbrace a_n \rbrace _{n=0}^{\infty}$
is $0$, i.e. $$\sum\limits_{n=0}^{\infty}a_n \: converges \rightarrow
   \lim\limits_{n\to\infty}a_n=0.$$ The divergence test: If the the limit of the sequence $\lbrace a_n \rbrace
   _{n=0}^{\infty}$ is NOT $0$ or does not exist, then the infinite series diverges, i.e. $$\lim\limits_{n\to\infty}a_n\neq0 \rightarrow \sum\limits_{n=0}^{\infty}a_n \: diverges$$ Would seriously appreciate it if anyone could verify whether the above is accurate or incorrect in any way. EDIT: I've modified the two limits notation that were mentioned in the comments and answers below, as well as adding the additional condition (limit does not exist or does not equal zero) for the divergence test. I appreciate all the answers/comments.","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'limits']"
2550096,Matrix determinant contradicts corresponding box volume – how is it possible?,"I'm taking an online linear algebra course and got stuck with a problem (it's not for credit)... Since I don't know anyone qualified in person, this is last resort. Pretty sure I've made some trivial error, but can't find it... If you can spot it, please let me know. Thank you in advance. As we know, absolute value of determinant of a square matrix is equal to the volume of the parallelepiped with edges corresponding to the matrix's columns (or rows). Consider matrix $A$ with columns $(1,1,0)^T,(0,1,1)^T,(1,0,1)^T$. Its determinant is 2. But it seems to me that the volume of the cube with edges corresponding to A's columns, is not 2… I mean, all three column vectors clearly have the same length, and it is $\sqrt2$, therefore volume of the cube with edges corresponding to these vectors must be $\sqrt2^3=2\sqrt2$. What am I doing wrong?","['matrices', 'volume', 'determinant']"
2550101,Divide an equilateral triangle into three similar parts?,"Is it possible to divide an equilateral triangle into three similar parts, in which two are identical but the third one is of different size?","['triangles', 'geometry']"
2550106,Creating a composite function that is injective with an injective and a non-injective part,"Find an example of functions $f:A\to B$ and $g:B\to C$ such that $f$
and $g\circ f$ are both injective, but $g$ is not injective. So If I understand this correctly, Need a function $f$ that is injective and that will also make $g$ injective when plugged in during $g\circ f$. Need a function $g$ that is not injective on its own The range of $f$ must be a subset of the domain of $g$ I tried thinking along the lines of using variations of $f(x)=x$ and $g(x)=x^2$ but all those leave my composite function as non injective I've also been using $x\in\Bbb R$ so as to keep the range of $f$ and domain of $g$ the same. Any suggestions of where to go with this? Thank you Edit:
Thank you everyone the answers were very helpful in understanding the problem and concepts better","['function-and-relation-composition', 'functions']"
2550128,Convergent sequences and accumulation points [duplicate],"This question already has an answer here : If a Cauchy Sequence has an accumulation point, then it converges to said accumulation point (1 answer) Closed 1 year ago . Definitions: Let $a$ be an accumulation point of $A$ . Then $\forall \ \epsilon >0$ , $B_{\epsilon}(a) \setminus \{a\}$ contains an element of $A $ . Question: I have two questions: if $(a_n)_{n\in N}$ is a convergent sequence in $\mathbb{R}$ then, Does the set $\{a_n\}$ have exactly one accumulation point? Or, could it have more than one? If so, does $(a_n)_{n\in N}$ necessarily converge to the said accumulation point? I'm tempted to say no to (1), but I'm afraid that I'm missing something. My counter-example to (1) is $\{a_n\} = \{ 4, 3, 2, 1, 0,0,0,...\}$ (i.e. inserting $0$ s after the 4th element). Then the set has no accumulation point and it converges to 0. Is that correct?","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2550139,$\det(A^4+I)=29$ is not solvable by any $A\in M_4(\mathbb Z)$,"I recently encountered the following problem. Given any $A \in M_4(\mathbb Z)$, show that $\det(A^4+I)\ne29$, where $I$ denotes the identity matrix. LHS can be written as the product of $1+{\lambda _i}^4$ where $\lambda _i$ denotes the eigenvalues of A. By using AM-GM inequality, I found that A is either invertible in $M_4(\mathbb Z)$ or has a zero determinant. I cannot go further. Can anyone help me?","['matrices', 'linear-algebra']"
2550144,Injectiveness of an infinite triangular matrix,"Let $A$ be an infinite upper triangular matrix with complex entries and all diagonal entries nonzero, i.e., \begin{align}
A=\left(\begin{matrix}
a_{11}&a_{12}&a_{13}&\cdots\\
a_{21}&a_{22}&a_{23}&\cdots\\
a_{31}&a_{32}&a_{33}&\cdots\\
\vdots&\vdots&\vdots&\ddots
\end{matrix}\right)
\end{align} where $a_{ij}=0(j<i)$ and $a_{ii}\neq 0(\forall i)$. Moreover, assume that $A$ represents a bounded linear operator on an $\ell^p$-space, and for convenience, say, from $\ell^2(\mathbb N)$ to itself. Though any finite triangular matrix with nonzero diagonal entries must be invertible because of its nonvanishing determinant, it seems that for such an infinite matrix $A$, only its range can be easily seen. What I am wondering are: Is $A$ always injective (and if not is there any counterexample, and is there any condition under which $A$ is injective)? If $A$ is not necessarily injective, is there any example that $A$ is injective and there are infinitely many subdiagonals of $A$ that are not eventually zero? Any help is greatly appreciated.","['functional-analysis', 'linear-algebra', 'hilbert-spaces', 'operator-theory']"
2550145,"For complex numbers $a,b,c$, explain why $a^{b\cdot c}=(a^b)^c$ is not necessarily true.","For complex numbers $a,b,c$, explain why $a^{b\cdot c}=(a^b)^c$ is not
  necessarily true. I know that complex powers are really sets of complex numbers.  But coming from real analysis the above seems confusing.  Can anyone give a simple explanation of what is going on.",['complex-analysis']
2550165,"Show a set in a metric space is dense, if and only if, the metric space is the union of open balls.","Show that if $(X, d)$ is a metric space, $S \subseteq X$ is dense if and only if , $\space \forall r > 0$, $X = \bigcup_{x\in S}B(x; r)$. This problem has been confusing me, as I've tried to justify that, starting from the left, since
$\bar S  = X$ I could show $\bar S = \bigcup_{x\in S}B(x; r)$ and conclude that $X = \bigcup_{x\in S}B(x; r)$. I don't know what to do since I'm mostly confused by how the closure of a set could be the union of all of the open balls in $X$, especially when the union of all of the open balls would be open. I've included some definitions to show the framework I'm working in. Any input would be greatly appreciated! Definitions: Given a metric space $(X, d)$: A set $S \subseteq X$ is dense if $\bar S  = X$. Given a set $S \subseteq X$, the closure of $S$ is $\bar S  =  
   \{\space x\in X \space | \space \forall r>0, B(x;r)\cap S\neq   
   \emptyset \}$. The open ball is defined as $B(x;r)= \{a\in X \space | \space d(x,a)<r\}$.","['general-topology', 'real-analysis', 'metric-spaces']"
2550188,Expected time of sequence getting typed when the letters are typed randomly,"This question has been asked in an examination of the Indian  Statistical Institute, Kolkata for second year Master of Statistics students in the subject Martingale Theory. Q. Mr.Trump  decides to post a random message on the  Facebook   and
  starts typing a random sequence $\{U_k\}_{k\geq1}$ of letters such that
  they are chosen independently and uniformly  from the $26$ English
  alphabets. Find out the expected time of the first appearance of the
  word ""COVFEFE"". We may assume that Trump has his caps lock on so
  that only upper case letters are typed. Assume further that the letters
  are typed at the rate of one letter per second. I have no idea as to how to proceed. I will be grateful for any help.","['probability-theory', 'martingales', 'statistics']"
2550220,Eigenvalues bounded $\implies$ Matrix entries bounded?,Suppose $H$ is a Hermitian matrix with all its eigenvalues be bounded (below and above) say between $-n_0$ and $n_0$. Is it true that all members of the matrix $H$ is bounded (below and above) by some constant which does not depend on the order of the matrix?,"['matrices', 'eigenvalues-eigenvectors']"
2550226,Proof of An Inequality on $\Bbb S^n$,"I have no idea for proving the following inequality: Let $x=(x_1,\cdots,x_n), y= (y_1,\cdots,y_n)\in\Bbb S^n$. then 
  $$(x_1y_2-x_2y_1)^2\leq 2(1-\left<x,y\right>).$$ Any help would be great. Thanks.","['inequality', 'calculus', 'geometry']"
2550259,convergence sequence and continuous functions,"I have the following question: assume $C$ is a subset (we can assume it is convex and compact) of a Banach space $(X,\|\cdot\|)$, $f:C\longrightarrow C$ continuous and $x_{0}\in C$ a fixed point of $f$. Also, let $(x_{n})_{n\geq 1}\subset C$ be a sequence having a subsequence converging to $x_{0}$, and such that $$\Big| \|x_{n+1}-x_{0}\|-\|f(x_{n})-f(x_{0})\| \Big|\leq c_{n},$$ for each $n\geq 1$, where $c_{n}$ is a positive sequence of numbers with $c_{n}\rightarrow 0$. From the above conditions, can we derive that $x_{n}\rightarrow x_{0}$? I know that if $f$ is nonexpansive (i.e. $\|f(x)-f(y)\|\leq \|x-y\|$) the assett is true under some additional conditions on the sequence $(c_{n})_{n\geq 1}$,but assuming only the continuity of $f$, it is not clear. What do you think? Many thanks in advance for your comments!","['functional-analysis', 'continuity', 'sequences-and-series']"
2550293,"$G=\{(x_n)\in l_1 : x_1 - 3x_2 = 0\}$, $f$ on G is defined by $f(x) = x_1$. Show $g(x) = (3/4)(x_1+x_2)$ is the unique Hahn-Banach extension for $f$.","Let $G=\{(x_n)\in l_1 : x_1 - 3x_2 = 0\}$ and $f : G\to \mathbb{R}$ be defined by $f(x) = x_1$. Prove that $g(x) = (3/4)(x_1+x_2)$ is the unique Hahn-Banach extension for $f$. (I found $g$ is an extension for $f$, $||g||=3/4$. But how to prove $g$ is unique. By Hahn-Banach theorem, for a bounded linear functional $f$ defined on a subspace $G$ there exists an extension $g$ with $||f||=||g||$.)","['functional-analysis', 'linear-transformations', 'dual-spaces']"
2550319,What is the value of $\arctan(1/2)+\arctan(1/5)+\arctan(1/8)$?,"What is the value of :
  $$\arctan(1/2)+\arctan(1/5)+\arctan(1/8)?$$ I tried to do geometric solution:: Where in the angles we are looking for are shown, but I can't solve it.  Can we use it with this kind of approach? Can someone also post a solution using trigonometric identities?","['contest-math', 'trigonometry']"
2550320,Prove that $\pi:S^3\rightarrow \mathbb{P}^1(\mathbb{C})\cong S^2$ is a submersion,"I have to solve this one: Let us consider the sphere $S^3\subset \mathbb{R}^4\cong\mathbb{C}^2$, and the map $\pi:S^3\rightarrow \mathbb{P}^1(\mathbb{C})\cong S^2$ defined as the projection's restriction $(z_1,z_2)\mapsto [z_1:z_2]$. Prove that $\pi$ is a submersion. Solution: I identify $\mathbb{P}^1(\mathbb{C})$ and $S^2$ through a diffeomorphism \begin{array}{crcl}
f:& \mathbb{P}^1(\mathbb{C}) & \longrightarrow & S^2 \\
& [z_1,z_2] & \longmapsto & \frac{1}{|z_1|^2+|z_2|^2}\big(|z_2|^2-|z_1|^2,2z_1\overline{z_2} \big)
\end{array} This way we have \begin{array}{crcl}
f\circ\pi:& S^3 & \longrightarrow & S^2 \\
& (z_1,z_2) & \longmapsto & \big(|z_2|^2-|z_1|^2,2z_1\overline{z_2} \big)
\end{array} because $|z_1|^2+|z_2|^2=1$. Now we compute the Jacobian matrix of $f\circ\pi$ \begin{equation}
J_{f\circ\pi}=\left(
\begin{array}{cc}
2\overline{z_2} & 2z_1\frac{2|z_2|z_2-|z_2|^2}{z_2^2} \\
2|z_1| & -2|z_2|
\end{array}
\right)
\end{equation}
that has full rank, so $f\circ\pi$ is a submersion. Now the thing is: how can I conclude that $\pi$ is a submersion? Any other idea to solve it? Thanks a lot","['spheres', 'projective-space', 'hopf-fibration', 'differential-geometry']"
2550386,The 'Factorialth Root',"I was dealing with the following question, given by my friend: Let $\xi(x)=\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{\cdots}}}}$ Define the series $X$ as $\xi(1),\xi(2),\xi(3),\dots$ Find $n$ for which $\xi(n)$ is the 51st Whole Number in the series. I solved it, of course, [and interestingly $\xi(1)={{1+\sqrt5}\over2}$, the Golden Ratio] but that led us on a competition in which we would try to find out the value of increasingly convoluted expressions. Some time later, I made an expression, which I called 'The Factorialth Root', written as $\sqrt[!]{x}$. For some $x$, $\sqrt[!]{x}=\sqrt{x\sqrt{(x-1)\sqrt{(x-2)\sqrt{\ddots\sqrt{2\sqrt1}}}}}$ My friend thought that $(x>y)\to(\sqrt[!]{x}<\sqrt[!]{y})$, while I thought the opposite, that $(x>y)\to(\sqrt[!]{x}>\sqrt[!]{y})$. I showed by example that mine was correct, but couldn't prove it. My attempt: If $[(x>y)\to(\sqrt[!]{x}>\sqrt[!]{y})]$ is true, then $\sqrt[!]{x}>\sqrt[!]{x-1}$. This is possible only when $x>\sqrt[!]{x-1}$. It follows that $\sqrt[!]{2}>\sqrt[!]{1},\sqrt[!]{3}>\sqrt[!]{2}$, and so on. So, I thought I could prove it by induction, but can't seem to find any way to apply it. Can anyone help?","['algebra-precalculus', 'radicals', 'proof-writing']"
2550420,Proof of inequality in Weierstrass approximation theorem proof through probability,"I found this exercise in my probability theory book. The problem guides you through a proof of the Weierstrass Approximation Theorem through probability theory. My question is only about part b, so any help or hint is much appreciated. I can make the exercise, except for part $b$, and I show all my workings below. I understand that you might want to see my workings for $b$, but I have really no idea. First I quote Chebyshev's inequality, because apparently we need this. Thus: my question is only about part b, my work for the other parts can be found below. $\bf{Chebyshev's \ inequality}$ If $Y$ is a random variable and $\mathbb{E}(Y^2)<\infty$, then $$\mathbb{P}(|Y|\geq t)\leq \frac{\mathbb{E}(Y^2)}{t^2}, \quad \mathrm{for \ } t>0.$$ I quote the exercise: Let ($X_i$) be a sequence of i.i.d. Bernoulli$(p)$ variables, thus $\mathbb{P}(X_i=0)=1-p$ and $\mathbb{P}(X_i=1)=p$, for all $i\in\mathbb{N}$. a) Let $f$ be a continuous function of $[0,1]$ and prove that $$B_n(p)=\mathbb{E}\left(f\left(\frac{\sum_{i=1}^nX_i}{n}\right)\right)$$ is a polynomial in $p$ of degree at most $n$. b) Use Chebyshev's inequality to prove that for all $p$ such that $0\leq p\leq1$ and for any $\epsilon>0$, $$\sum_{k\in K}\binom{n}{k}p^k(1-p)^{n-k}\leq\frac{1}{4n\epsilon^2},$$ where $K=\{k:0\leq k\leq n, |k/n-p|>\epsilon\}$. c) Using this and the fact that $f$ is bounded and uniformly continuous on $[0,1]$, prove the following version of the Weierstrass approximation theorem: $$\lim_{n\to\infty}\sup_{0\leq p\leq1}|f(p)-B_n(p)|=0.$$ For part $a$, if we let $Y_n:=\sum_{i=1}^nX_i$, then $Y_n\sim \mathrm{Bin}(n,p)$. Thus $$B_n(p)=\mathbb{E}\left(f\left(\frac{\sum_{i=1}^nX_i}{n}\right)\right)=\mathbb{E}\left(f\left(\frac{Y_n}{n}\right)\right)=\sum_{k=0}^n f\left(\frac{k}{n}\right)\binom{n}{k}p^k(1-p)^k$$ is a polynomial in $p$ of degree at most $n$. This follows directly from just writing out the expectation of the binomial distribution. For part $c$, $f$ is continuous on the closed interval $[0,1]$, so $f$ is uniformly continuous and bounded on $[0,1]$, and assumes it's minimum and maximum on this interval. These are all basic real analysis results. Since $f$ is bounded on $[0,1]$, there exists an $M\in\mathbb{R}$ such that $|f(p)|\leq M$, for all $p\in[0,1]$. From part $b$, we know that $0\leq p\leq1$ and for any $\epsilon>0$, $$\sum_{k\in K}\binom{n}{k}p^k(1-p)^{n-k}=\mathbb{P}(|k/n-p|\geq \epsilon)\leq\frac{1}{4n\epsilon^2},$$ where $K=\{k:0\leq k\leq n, |k/n-p|>\epsilon\}$. Thus we obtain the following: In equation 1, we take the limit of the result of part b; equation 2 follows because of uniform continuity of $f$; equation 4 follows because of linearity of expectation, part $a$ and because $f(p)$ is fixed; equation 5 follows from the partition theorem for expectations:  $$\begin{align} 
&\lim_{n\to\infty}\mathbb{P}(|\frac{k}{n}-p|\geq\epsilon) = 0 & (1)\\ 
&\lim_{n\to\infty}\mathbb{P}\left(|f\left(\frac{k}{n}\right)-f(p)|\geq\epsilon\right) = 0 & (2)\\ 
&\lim_{n\to\infty}\mathbb{E}\left(|f\left(\frac{k}{n}\right)-f(p)|\right)  = & (3)\\
&\lim_{n\to\infty}|B_n(p)-f(p)|=& (4)\\  
&\lim_{n\to\infty}\mathbb{E}\left(|f\left(k/n\right)-f(p)|\Bigg|k/n-p|\geq\epsilon\right)\cdot\mathbb{P}(|k/n-p|\geq\epsilon)+& \\
&\lim_{n\to\infty}\mathbb{E}\left(|f\left(k/n\right)-f(p)|\Bigg|\frac{k}{n}-p|<\epsilon\right)\cdot\mathbb{P}(|k/n-p|<\epsilon)=& (5)\\ 
& 0+0=0.& (6)\\
\end{align}$$ The first term is zero because $\mathbb{P}(|\frac{k}{n}-p|\geq\epsilon)$ goes to zero as $n$ goes to $\infty$, as shown in part $b$, and because the boundedness of $f$. The second term goes to zero because: (i) $f(k/n)-f(p)$ goes to zero as $n$ goes to infinity because $f$ is uniformly continuous, because $\lim_{n\to\infty}k/n=p$ because of the law of large numbers (remember that $k$ denotes the number of successes in a $\mathrm{Bin}(n,p)$ experiment); and (ii) because $\mathbb{P}(|k/n-p|<\epsilon)$ is bounded by definition of a probability measure. Thus $\lim_{n\to\infty}|B_n(p)-f(p)|=0$, for any $p\in[0,1]$, which is the interval where $f$ is uniformly continuous, and thus $\lim_{n\to\infty}\sup_{0\leq p\leq1}|f(p)-B_n(p)|=0$. Thus any continuous function on $[0,1]$ can be uniformly approximated by polyinomials on $[0,1]$. Obvious rescaling proves the Weierstrass Approximation Theorem. EDIT : Should I use for part $b$ that $\sum_{k\in K}\binom{n}{k}p^k(1-p)^{n-k}$, where $K=\{k:0\leq k\leq n, |k/n-p|>\epsilon\}$ equals $$\begin{eqnarray}\mathbb{P}(|k/n-p|\geq \epsilon) & = & \mathbb{P}(|k-np|\geq \epsilon n)\\&=& \mathbb{P}(|k-\mathbb{E}(Y_n)\tag{with $Y_n\sim\mathrm{Bin}(n,p)$}|\geq \epsilon n)\\ & \leq & \frac{1}{\epsilon^2n^2}\mathbb{E}((k-np)^2)\tag{Chebyshev's inequality}\\ &\leq& \frac{1}{\epsilon^2n^2}(\frac12\cdot\frac12\cdot n) \tag{variance is maximized if $p=\frac12$}\\&=& \frac{1}{4n\epsilon^2}\end{eqnarray}$$ I think this is correct, after posting I came a lot further through reading on wikipedia and the like.","['real-analysis', 'probability', 'weierstrass-approximation']"
2550437,Iterated Euler's totient function,"Let $\phi(n)$ be the Euler totient function : 
$$
\phi(2)=1 \;,\; \phi(11)=10 \;,\; \phi(12)=4\;,$$
etc.
Define $\Phi(n)$ to be the number of iterations $k$ so that $\phi^k(n)$
reaches $1$.
For example,
$\Phi(25)=5$ because $\phi(25)=20$ and continuing, it takes $5$ applications
to reach $1$:
$$25,20,8,4,2,1 \;.$$
Another example: $\Phi(113)=7$:
$$113,112,48,16,8,4,2,1 \;.$$
Here is a plot of $\Phi(n)$: Red curve: $0.43 + 1.22 \ln( n )$. $\Phi(n)$ is fit quite well (and well beyond what's shown above) by $c \ln(n)$. Two questions: Q1 . What explains the logarithmic growth, at a high-level? Q2 . What explains the constant $c \approx 1.22$? Likely both of these questions are answered in the literature.","['number-theory', 'totient-function']"
2550461,Quotient of a quotient ring,"I'm trying to understand the quotient $\Bbb Z[\sqrt{47}]/(2, 1 +\sqrt{47})$, in order to find out whether or not $(2, 1 + \sqrt{47})$ is a prime ideal in $\Bbb Z[\sqrt{47}]$. I think it is but my calculations seem to be giving me something that doesn't agree with this, so either it isn't a prime ideal or I'm doing something very wrong. Since $\Bbb Z[\sqrt{47}] \cong \Bbb Z[X]/(X^2 - 47)$, I'm writing $$ (\Bbb Z[X]/(X^2 - 47))/(2, X^2 - 2X - 46) $$ where $X^2 - 2X - 46$ is a monic irreducible polynomial with $1 + \sqrt{47}$ as a root. Is this step correct? If so, then I think it follows that $$ (\Bbb F_2[X]/(X^2))/(\overline{X^2 - 2X - 46}) $$ where $\overline{.}$ denotes the reduction map. The problem is, this is the zero-ideal in $\Bbb F_2[X]/(X^2)$, and this finite ring is not an integral domain, so the conclusion is that $(2, 1 + \sqrt{47})$ is not a prime ideal. Which steps here (if any) are correct? Is any body able to show me how they might do it if it is not correct?","['abstract-algebra', 'ring-theory', 'polynomials', 'ideals']"
2550505,"Continuous function is constant $\mu$-almost everywhere, then it has to be constant on the topological support of $\mu$","Let $(X,\tau)$ be a topological space with Borel-$\sigma$-Algebra $\mathcal{B}$ and $\mu$ a measure on $\mathcal{B}$ with topological support $\text{supp}(\mu) := \{x\in X \ |\   x \in U \in \tau \Rightarrow \mu (U) > 0\}$. Let $\varphi :  X \rightarrow \Bbb{R}$ be a continuous function with $\mu$-almost everywhere $\varphi = 1$. The claim is: $\varphi = 1$ on $\text{supp}(\mu)$. It seems easy, but I don't know where to start. What I know is: There exists a set $\Omega \in \mathcal{B}$, such that $\mu(\Omega^c) = 0$ and $\varphi(\omega)=1 \; \forall \omega \in \Omega$. If $U$ is a neighborhood of $x \in \text{supp}(\mu)$, then $U\cap \Omega \ne \emptyset$.","['borel-sets', 'functions', 'continuity', 'measure-theory', 'general-topology']"
2550534,Godement Resolution don't see $\mathcal{O}_X$-module structure,"Let $\mathcal{F}$ a sheaf on $X$. We can define the Godement sheaf $\mathcal{F^+}$ by $\Gamma(U, \mathcal{F^+}) := \prod _{a \in U} \mathcal{F_a}$ for every open subset $U$. Then we have the canonical inclusion $\mathcal{F} \subset \mathcal{F^+}$ by $s_U \to (s_a)_{a \in U}$. Set $\mathcal{F^0} := \mathcal{F^+}$. Recursively we can define $\mathcal{F^{r+1}}:= (\mathcal{F^r}/\mathcal{F^{r-1}})^+$. This provides the Godement resolution $0 \to \mathcal{F} \to \mathcal{F^0} \to \mathcal{F^1} \to ...$ Their cohomology is defined by $H^r(X, \mathcal{F}):= \frac{Ker(\Gamma(X, \mathcal{F^r}) \to \Gamma(X, \mathcal{F^{r+1}})}{Im(\Gamma(X, \mathcal{F^{r-1}}) \to \Gamma(X, \mathcal{F^r}))}$. My question is: If $\mathcal{F} $ has a $\mathcal{O}_X$-module structure, why $H^r(X, \mathcal{F})$ is only depending on $ \mathcal{F}$ as abelian sheaf, therefore $H^r(X, \mathcal{F})$ ""forgets"" the $\mathcal{O}_X$-module structure?","['homology-cohomology', 'algebraic-geometry']"
2550578,Question about finding the inverse of matrices,"My professor gave us this formula for finding the inverse of 3x3 matrices; [matrix given|identity] then row reduce matrix given to the identity and you end up with [identity|inverse of matrix given] So basically you take the matrix you are trying the find the inverse of, augment it with the identity, row reduce it until you have the identity for the first 3 columns, and then your last 3 columns will be the inverse. (I hope my wording makes sense.) For example: to find the inverse of \begin{pmatrix}0&1&2\\ 1&0&3\\ 4&-3&8\end{pmatrix} you first do this: 
\begin{pmatrix}0&1&2&1&0&0\\ 1&0&3&0&1&0\\ 4&-3&8&0&0&1\end{pmatrix}
then row reduce it until you have the identity at the beginning and the last 3 columns will be the inverse: \begin{pmatrix}1&0&0&-\frac{9}{2}&7&-\frac{3}{2}\\ 0&1&0&-2&4&-1\\ 0&0&1&-\frac{3}{2}&-2&\frac{1}{2}\end{pmatrix} so the inverse is: 
\begin{pmatrix}-\frac{9}{2}&7&-\frac{3}{2}\\ -2&4&-1\\ \frac{3}{2}&-2&\frac{1}{2}\end{pmatrix} My question is: does this work for other matrices bigger than a 3x3?
Thanks!","['matrices', 'linear-algebra', 'inverse']"
2550598,Need A help in understanding a step in matrix representation of bounded linear operators.,"Israel Gohberg book that is called ""Basic operator theory"" said that: Suppose $A \in L(H),$ where $H$ is a Hilbert space with orthonormal basis $\phi_{1}, \phi_{2},....$, then for $x \in H, x = \sum_{j} <x,\phi_{j}> \phi_{j}.$ It follows from the linearity and continuity of $A$, applied to the sequence of partial sums of this series, that $$Ax = \sum_{j}<x,\phi_{j}> A\phi_{j}......(1)$$ But I did not understand the last statement, could anyone clarify this for me please? Thanks!","['real-analysis', 'operator-theory', 'functional-analysis', 'proof-explanation', 'analysis']"
2550621,"Calculating a random ""blob"" in a 10 x 10 grid","The problem: You have a 10 x 10 grid where each cell can be either occupied (1) or unoccupied (0). All occupied cells in the grid are part of a single ""blob"": a single shape defined by a set of occupied coordinates for which each occupied coordinate has at least one occupied coordinate to either its top, left, bottom, and/or right. (From this definition it seems the blob must occupy at least two coordinates.) How can you quickly generate a blob that is chosen randomly from the set of all possible such blobs? If it's not possible to generate such a blob quickly, why is it not possible? The (probably-wrong) method I'm using right now: I choose a random cell from the 10 x 10 grid to be the first cell of the blob. I then choose randomly from the set of cells neighboring the first cell to create the second cell of the blob (to make sure every blob is valid). I then iterate through an updated-as-it's-added-to list of all of the current cells of the blob, considering each neighboring cell once (and only once). A certain percentage of the time, I add this neighboring candidate cell to the blob. Right now I'm using 50% as the percentage. If this method of generating the blob can work (i.e. it doesn't have any fundamental problems with it), I would need to know what this certain percentage should be. My gut feelings of what the solution may look like: It seems like the blob is generally going to occupy most of the 10 x 10 grid. It seems like it might be better to first choose the size of the blob, and only then choose which cells are occupied. I suspect that the ideal way of creating the blob if I already know how big it should be would be to choose a random cell and then to repeatedly choose a random neighbor to the existing blob and make it part of the blob. I suspect it may be helpful to first create an algorithm for a simpler one-dimensional grid before trying to create one for a two-dimensional grid. Intermediate conclusions I'm coming to: It looks like for the simpler case of a one-dimensional grid, the number of possible blobs is equal to the $(n-1)th$ triangular number . See the pictures below that show a one-dimensional grid. The pattern of possible shapes as you move from $n=2$ to $n=3$ to $n=4$ is a triangular-number pattern. Clarifications: Two disconnected areas of occupied cells do not count as a single blob. Each possible blob needs to have an equal chance of being picked. Two blobs of the exact same size, shape, and orientation, but different location are considered different blobs. There are answers below which show how to generate a randomly-picked blob but where the randomly-selected blobs can't be generated quickly. I'm looking for a way to do it quickly or a proof that it's not possible to do it quickly. Some pictures I drew to wrap my head around the issue: Disclaimer: I'm dealing with an interview question where I'm allowed to use third party help as long as I disclose it. The interview question isn't to generate the blob, but by being able to correctly generate a randomly-chosen blob I can test several different possible solutions to the interview question. Some clarifications of Misha's answer: Q: Why do you say ""this method is definitely biased"" ? A: Because we already know that the unbiased method works exactly the same way except you only accept the outcome of the random selection when the rules for a blob are applied ""strictly"" (i.e. you only have a blob when there's a single connected group of occupied spaces), and so if you instead just pick an occupied space and use its connected occupied spaces as the blob, you'll at the very least be biased towards smaller blobs. Q: What do the vertical bars represent? A: They represent size / cardinality. So $B$ is the blob itself, and $|B|$ is how many cells it contains. Q: Why use the $∂$ character? A: In topology it refers to the boundary of a subset of a space. Q: How did you figure out the probability for the biased method? A: Well, $\frac{|B|}{100}$ is just the probability that you choose one of the cells of the blob, assuming it exists. So presumably the other part is the probability that it was created. To make it easier to understand, note that in the first formula you see that the odds of getting the strict blob you want (strict because you're not tolerating occupied cells not part of your blob) on any particular attempt is $2^{-100}$, because you need all 100 of those coin-flips to go exactly the way you want them. So in this less-strict situation, you need the coin-flips for your desired occupied spaces to go the way you want, and you also need the coin-flips for the unoccupied neighboring spaces to go the way you want, so that there's no chance you'll have a neighbor end up occupied and change the blob to a different one. Q: How did you figure out the lower bound? A: He just took some constraints he knew to be true and then used calculus(?) to combine those functions to determine what the lowest value could be. But to rationalize it: if you look at the formula for $p(B)$ and say ""I want to make $p(B)$ as small as possible"", you can see that you want to make the $(\frac 12)^{|B| + |∂B|}$ term as small as possible, and the way to do that is to make $|B|+|∂B|$ as big as possible. And the biggest that sum can be is 100, where the blob and its neighboring cells take up the entire grid. Then you want to make the $\frac {|B|}{100}$ term as small as possible, and the way to do that is to figure out the smallest blob that where the blob and its neighbors fill the entire grid. But since he didn't try to find an actual blob, the number he ended up with (33) may not actually correspond to a real blob (I tried creating a blob of size 33 that occupied or bordered every cell and I couldn't do it). I think the effect of that is to make it take longer for the program to run than if the number were more accurate. If the number was perfectly accurate, then you'd have a 100% chance of accepting the rarest blobs, but with the number less accurate, you have less than a 100% chance of accepting the rarest blobs, but it should still be unbiased. Q: If the goal of multiplying by the acceptance probability is to get rid of the $p(B)$ term, why use $p^*$ in the numerator? Why not use ""1""? A: Using a numerator greater than the probability of getting the rarest blobs (which is roughly $p^*$) will result in a non-uniform distribution, and using a numerator that's smaller will make the program take longer to run. The reason using a numerator greater than $p^*$ will result in a non-uniform distribution is that it will lead us to accept some not-rarest blobs with the same probability as the rarest blobs. If we have the numerator as $p^*$, then when we actually come across one of those rarest blobs, the $p(B)$ will equal $p^*$, and so the acceptance probability will be ""1"" (we'll always accept that blob). But if the numerator is larger, not only will we always accept the rarest blobs, but we will also always accept the less-rare blobs whose $p(B)$ is such that dividing the larger numerator by $p(B)$ will result in an acceptance probability of at least 1.","['combinatorics', 'algorithms', 'computational-mathematics', 'probability']"
2550627,If $f(g(x))=x$ is $f$ an injective function?,"Let $ f:\mathbb R\to \mathbb R $ . If $f(g(x))=x$ then is $f$  an injective function? Well, I proved it to be true. But honestly I have a strong feeling that my proof is wrong.
Here's my proof: Assume $f(a_1)=f(a_2) = x$ and we want to prove that $a_1 = a_2$ . if $f(a_1)=f(a_2) = x,$  then $a_1 = g(x)$  and   $a_2 = g(x) $. meaning $a_1 = a_2$. Therefor f is injective. Is my proof wrong? I assume that not only that my proof is wrong but also that this isn't true at all. But I'm having a hard time to find a contradicting example.","['algebra-precalculus', 'calculus', 'functions']"
2550655,Does integrating over different curves give different results?,"When we're integrating a one variable function, there is only one path to follow between points $a$ and $b$. $F(b)=F(a)+h(f(a)+f(a+h)+f(a+2h)+........)$ where $h$ is very small. So, we approach from $a$ to $b$ in steps of $h$ along the line joining unique path joining $a$ and $b$. I was wondering what if we have a multi-variable function $f(x,y)$. What does it mean to integrate $f(x,y)$ from $(a,b)$ to $(c,d)$? Clearly, there isn't a unique path joining $(a,b)$ and $(c,d)$. We have partial integration: $$\int_{(a,b)}^{(c,d)} f(x,y)dxdy=\int_{a}^{c}f(x,b)dx+\int_{b}^{d}f(c,y)dy$$ But that's basically doing it in two steps, first keeping $y$ constant then $x$ constant. How can we vary both $x$ and $y$ simultaneously? I thought of finding a curve which contains both the points $(a,b)$ and $(c,d)$ and varying both variables simultaneously along that path. Suppose $g(x)$ is a curve such that $g(a)=b$ and $g(c)=d$. Then we can replace $y$ with $g(x)$ to get: $$\int_{(a,b)}^{(c,d)} f(x,y)dxdy=\int_a^b f(x,g(x))g'(x)dx$$ Does this integration of $f(x,y)$ from $(a,b)$ to $(c,d)$ depend on the choice of curve $g(x)$?","['integration', 'definite-integrals', 'calculus', 'functions']"
2550757,"finding a matrix $A_2$ such that $A_2(\mbox{vect}(e_2,e_3)) \subseteq \mbox{vect}(e_2,e_3)$","Let us consider
$$A_1 = \left[\begin{array}{ccc} 2&-1&0 \\  0&1&0\\  0&0&0\end{array}\right].$$ I want to find a matrix $A_2$ such that $A_1A_2=A_2A_1$ and $A_2F\subseteq F$ with $F=\mbox{vect}(e_2,e_3)$ Thank you.","['matrices', 'functional-analysis', 'linear-algebra']"
2550818,Quadrilateral of maximum perimeter of given area inside a square,"Find the quadrilateral of maximal perimeter of fixed area $15$
  enclosed in a square of side $4$. One (trivial) remark: we cannot take a concave quadrilateral because any concave quadrilateral is contained in a triangle, which must in turn be contained in the square by convexity, and a triangle contained in a square of side $4$ has maximum area $8<15$. Therefore the quadrilateral must be convex. Hence the perimeter cannot be larger than that of the square ($16$). Also, we might thus decompose the quadrilateral as the union of $4$ triangles by drawing the diagonals. See the square as $[-2,2]^2\subset \mathbb{R}^2$. My idea: the sought quadrilateral is a rhombus having two vertices $A=(-2,-2)$, $C=(2,2)$ (on one diagonal of the square) and the other two $D=(a,-a)$ and $B=(-a,a)$ (on the other diagonal of the square, equally distant from the center), with the parameter $a\in (0,2)$ chosen so that the area is actually $15$ (actually $a=15/8$, which yields the perimeter $\sqrt{1+31^2}/2\approx 15.51$). I would say that because without the condition of being enclosed in a square, we can arbitrarily increase the perimeter of a quadrilateral with a given area by making the angles on two opposite vertices $A,C$ vanish (while the others tend to $\pi$), and their distance increase to infinity. Inside the square, we can maximize the distance between $A$ and $C$ by taking them to be opposite vertices of the square, i.e. $A=(-2,-2)$ and $C=(2,2)$. With this assumption, the situation is simpler. But I still need another assumption: the two triangles $ABC$ and $ACD$ have same area . Then for the area to be fixed, $B$ and $D$ must lie on a specific line parallel to the diagonal. At this point, it is easy  (one-variable calculus) to show that the perimeter of $ABC$ is maximized if $B$ lies on the other diagonal of the square, and the same goes for $D$. EDIT: This is wrong and lead me to wrong conclusions. The isosceles triangle minimizes, not maximizes, the perimeter! As a result, the point on the line which maximizes the perimeter is the furthest away point of intersection with the border of the square. In particular, all vertices of the quadrilateral must lie on the border of the square. Since the areas must be the same, the distances of $B$ and $D$ from the center must also be the same. Either way, I have no idea how to make my assumptions more formal. I'm not even sure that my solution is correct.","['quadrilateral', 'euclidean-geometry', 'geometry']"
2550842,Which counterexamples to keep in mind when studying (Lebesgue) integral convergence theorems,"When studying the classical theorems regarding the convergence of a sequence of integrals, to the integral of the limit function, e.g. Beppo Levi Dominated convergence Vitalli's theorem which counterexamples should one keep in mind?","['examples-counterexamples', 'integration', 'measure-theory']"
2550852,Nested integral: volume of a right simplex,"Consider the integral
$$I_n \equiv \int_{t_1=0}^T dt_1 \int_{t_2 = t_1}^T dt_2 \cdots \int_{t_n = t_{n-1}}^T dt_n \, .$$
I suspect the result should be $I_n = T^n/n!$ but would like to prove it.
Doing the $n^\text{th}$ integral gives
\begin{align}
I_n &= \int_{t_1=0}^T dt_1 \cdots \int_{t_{n-1}=t_{n-2}}^T dt_{n-1} \, (T-t_{n-1}) \\
&= T I_{n-1} - \int_{t_1=0}^T dt_1 \dots \int_{t_{n-1}=t_{n-2}}^T dt_{n-1} \, t_{n-1} \, .
\end{align}
Is there some way to evaluate the second term or to come up with a useful recurrence relation or induction step?",['integration']
2550879,"$C^1$ diffeomorphism, but not $C^\infty$","Can you think of a $C^1$ bijection $f:\mathbb{R}\to\mathbb{R}$ with $f^{-1}$ also of class $C^1$ but $f$ is not of class $C^\infty$? I have been trying to construct one from some usual examples of $C^1$ functions that are not $C^\infty$ (like $x\mapsto \mid x\mid^3$), but they are not even bijective or defined on all $\mathbb{R}$. And also some usual examples of smooth homeomorphisms that are not diffeomorphisms (like $x\mapsto x^3$) do not do the job here because their inverse is not even differentiable.","['differential-geometry', 'calculus']"
2550911,Find a lower bound on $a^2(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{y})^2-(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{x})^2$,"I wonder if there is a good to find the lower bound of the following term:
\begin{equation}
\min_{\boldsymbol{x}}\,a^2(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{y})^2-(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{x})^2
\end{equation}
where $a>0$ is some constant; $\boldsymbol{x},\boldsymbol{y}\in\mathbb{R}^n$ have unit norm; and $\boldsymbol{A}\in\mathbb{R}^{n\times n}$ is a symmetric matrix. $\boldsymbol{A}$ and $\boldsymbol{y}$ are known. It can also be written as:
\begin{equation}
\min_{\boldsymbol{x}}\,(a\boldsymbol{y}-\boldsymbol{x})^\textrm{T}\boldsymbol{A}\boldsymbol{x}\boldsymbol{x}^\textrm{T}\boldsymbol{A}(a\boldsymbol{y}+\boldsymbol{x})
\end{equation}
or 
\begin{equation}
\min_{\boldsymbol{x}}\,\boldsymbol{x}^\textrm{T}\boldsymbol{A}(a^2\boldsymbol{y}\boldsymbol{y}^\textrm{T}-\boldsymbol{x}\boldsymbol{x}^\textrm{T})\boldsymbol{A}\boldsymbol{x}
\end{equation}
This is related to an earlier trivial question I asked, any help is greatly appreciated, thanks so much! Edit : In other words, is there an $a$ that could make the above term greater than $0$?","['matrices', 'upper-lower-bounds', 'linear-algebra', 'optimization']"
2550916,How can the following PDE be solved?,"I am working on this problem on PDE's : $f(x, y)\frac{{\partial}f}{{\partial}x} + \frac{{\partial}f}{{\partial}y} = 1$ with $f(u,u) = \frac{u}{2} , 0< u < 1$. I tried to solve this PDE with the method of characteristics but since $f(x, y)$ is not given, I did not know how to continue. Any ideas?","['ordinary-differential-equations', 'partial-differential-equations']"
2550929,"Let $a,b \in \mathbb Z$. Prove that if $3 \mid (a+2b)$ then $3 \mid (2a+b)$","Let $a,b \in \mathbb Z$. Prove that if $3 \mid (a+2b)$ then $3 \mid (2a+b)$ This is how I solved this: $3m = a+2b \iff a = 3m-2b$ $2a+b = 2(3m-2b)+b = 6m -3b = 3(2m-b)$
 And now, my solution seems to work. However, when - instead of solving for $a$, I solve for $b$, then I get this: $2a+b = 3 \frac{a+m}{2}$ How can I even be sure that this number is an integer? Is there a way to fix the second solution? If my first solution is correct. the second one should work as well.","['number-theory', 'linear-algebra', 'elementary-number-theory']"
2550936,Prove $\sum_{k= 0}^{n} k \binom{n}{k} = n \cdot 2^{n-1}$ using the binomial theorem for $n\geq1$ [duplicate],"This question already has answers here : How to prove this binomial identity $\sum_{r=0}^n {r {n \choose r}} = n2^{n-1}$? (10 answers) Closed 3 years ago . I do as follows $(1+x)^n= \binom{n}{0}+\binom{n}{1}x+\binom{n}{2}x^2+...+\binom{n}{n}x^n$ differentiating in both sides $n(1+x)^{n-1}= \binom{n}{1}+2\binom{n}{2}x+3\binom{n}{3}x^2...+n\binom{n}{n}x^{n-1}$ then, for $x=1$ $n \cdot2^{n-1} = \sum_{k=1}^{n}k\binom{n}{k}$ From here, I don't know how to proceed, I have tried to multiply both sides for $(1+x)$ to make $n \cdot2^{n}$ but then the summatory changes, I don't know how to prove the equality.","['binomial-theorem', 'discrete-mathematics']"
2550970,Proving subgroup of $SO(2)$ with $n$ elements is unique?,"I was thinking of taking two subgroups with $n$ elements and showing that they both are the same subgroup. I know that if $M$ ($2\times2$ matrix)  is any element of $SO(2)$, $M^{-1} = M^t$. And $\det M =1$. I don't know how to proceed. Any help would be appreciated.","['abstract-algebra', 'orthogonal-matrices', 'group-theory']"
2550987,On the cruelty of really intersecting Bézier curves,"Today (4 December 2017) is my 19th birthday, and I realised I hadn't asked a question here in over a year. So this is one on an old problem: computing the intersections of two Bézier curves. In my Kinross SVG library , a Bézier curve object contains two equivalent representations of its coordinate polynomials ($x(t),y(t)$): a control point form (Bernstein basis) and a polynomial form (power basis). The control point form is used for interfacing with the SVG format; the polynomial form is used for explicit calculations, and will be the form used below. The SVG standard includes line segments (linear Béziers), quadratic Béziers and cubic Béziers, which have two, three and four control points respectively. Intersecting two Bézier curves is easy if they are not both cubic: If one of the curves is linear or has collinear control points, aligning that curve with the $x$-axis and solving a cubic equation will do the trick. If one of the curves is quadratic, the same idea can be used: perform an affine transformation that maps that curve to the graph of $y=x^2$ with $0\le x\le 1$, then solve a sextic equation derived from the (transformed) other curve. This is curve implicitisation, and NumPy (the external library Kinross uses) has both (univariate) polynomial objects and a good polynomial root-finder for the high-degree polynomials that arise, so these are rather boring cases. The interesting part is cubic–cubic intersection, where this simple implicitisation no longer works. Several techniques are available, but many are unsuitable for my purposes: Bézier subdivision/clipping, in my eyes, is an inelegant approach that also fails to be reasonably quick in refining solutions. I like my answers to be as exact as possible, notwithstanding necessary precision compromises. Homotopy solvers are overly complicated, involving a differential equation at every step, and have a bewildering array of divergent cases to handle. I once considered optimisation (minimising $(p_x(t)-q_x(u))+(p_y(t)-q_y(u))^2$, where the polynomials are defined below), but this was also seen as overly complicated compared to direct solving. A resultant computation is currently a little too taxing for NumPy, since it involves polynomials in matrices (which is just not possible). I could sidestep these problems in the future by working at a lower level though. The groebner-basis has its own tag here, and certain places like this question and this Scholarpedia article describe Buchberger's algorithm in a fair amount of detail. It is more complicated than the resultant algorithm, however, and my own tests indicate that the numbers involved blow up even for polynomials with small coefficients. This leaves the rational univariate representation (RUR). Its final stage involves univariate root-finding, which again is a piece of cake for NumPy, but so far I have not been able to fully understand how it goes from the initial polynomials to the solutions (more on this later). So I came up with my own algorithm based on the multivariate Newton's method. Suppose the two curves to be intersected are $(p_x(t),p_y(t))$ and $(q_x(u),q_y(u))$. An intersection occurs when
$$p_x(t)-q_x(u)=p_y(t)-q_y(u)=0\tag1$$
The Newton iteration (from $t,u$ to $t',u'$) is thus
$$\begin{bmatrix}t'\\u'\end{bmatrix}=\begin{bmatrix}t\\u\end{bmatrix}+\begin{bmatrix}\Delta_t\\\Delta_u\end{bmatrix}$$
where the last term solves
$$\begin{bmatrix}p'_x(t)&-q'_x(u)\\p'_y(t)&-q'_y(u)\end{bmatrix}\begin{bmatrix}\Delta_t\\\Delta_u\end{bmatrix}=\begin{bmatrix}q_x(u)-p_x(t)\\q_y(u)-p_y(t)\end{bmatrix}$$
To get enough starting points to catch all the roots, I take for each curve: its endpoints its points of extremal curvature its inflection points and then perform a Cartesian product of the two sets obtained. With a little fuzzing to handle cases where the linear system in the Newton iteration fails to have a unique solution, my algorithm has performed very well against the test cases I have considered. (I do not need to report tangencies or other multiple root cases, since the vector images I handle in practice have ""nice enough"" curves.) In contrast, there are few articles on RUR around. Thus my question here is: What are the exact algorithmic details of solving via the RUR, going from $(1)$ to the solutions? Bézier curve-specific optimisations may exist. I am looking for an explanation that can be implemented from the ground up using NumPy. The ultimate objective I have in mind is not just to see whether RUR is better than my Newton-based algorithm (in time or reliability), but to understand some fundamental concepts of algebraic geometry. If you want, you can explain RUR using this pair of Bézier curves I tested my algorithm against:
$$p_x(t)=2+147t-342t^2+221t^3\qquad q_x(u)=1+87u-84u^2+11u^3\\
p_y(t)=17-90t+183t^2-104t^3\qquad q_y(u)=10+42u-165u^2+124u^3$$
$$1+147t-342t^2+221t^3-87u+84u^2-11u^3=0\\
7-90t+183t^2-104t^3-42u+165u^2-124u^3=0$$
In Kinross these are represented as p = bezier(2+17j, 51-13j, -14+18j, 28+6j)
q = bezier(1+10j, 30+24j, 31-17j, 15+11j) They intersect in five points ($0\le t,u\le1$) whose corresponding $(t,u)$ parameters are
$$(0.053611563083829826, 0.10086541719622877)\\
(0.15646174074706343, 0.94523150924142163)\\
(0.41077722343039424, 0.88035078651205612)\\
(0.86571544618164098, 0.97134323668380174)\\
(0.96472594025254754, 0.43951554821497935)$$","['bezier-curve', 'polynomials', 'computational-geometry', 'algebraic-geometry']"
2550996,Does the sequence of operators $(A_nx)(t) = x(t^{1+\frac{1}{n}}) $converge by norm?,"I have a simple question: The question is as follows: Consider the sequence of operators $A_n:C[0,1] \rightarrow C[0,1]$ as follows: $$ (A_nx)(t) = x(t^{1+\frac{1}{n}})$$
Prove that for each $n\in \mathbb{N}$, $A_n$ is a linear bounded operator. Show that $A_n \rightarrow I$ strongly. Does it converge by norm? Justify. $\textbf{Notation and information}$ I refer you to the answer by ""user438666"", for to get information about the notations in the question. I really cannot make connection with the defined operators! Can someone help me to understand it? Thanks!","['real-analysis', 'uniform-convergence', 'limits', 'functional-analysis', 'sequences-and-series']"
2551022,Show that $(x_n)_n $ is bounded,"Let $(x_n)_n, (y_n)_n $ two sequences s.t. $x_1,y_1>0$; $x_1 <y_1$ $x_{n+1}=x_n (1+\frac {1}{y_n}) $ $y_{n+1}=y_n (1+\frac {1}{x_n}) $ Show that $(x_n)_n $ is bounded. I noticed that $x_n <y_n $ for any $n\in \mathbb {N} $. Also, $(y_n)_n $ has limit $\infty $.","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2551043,Simulating truncated normal distribution,"The truncated normal distribution has pdf:
$$f(x;\mu,\sigma,a,b) = \frac{\varphi\left(\frac{x-\mu}{\sigma}\right)}{\sigma\left(\Phi\left(\frac{b-\mu}{\sigma}\right)-\Phi\left(\frac{a-\mu}{\sigma}\right)\right)}.$$ And $f=0$ otherwise.
Here $\varphi(x) = \frac{\exp\left(-\frac{x^2}{2}\right)}{\sqrt{2\pi}}$, $\Phi(x) = \frac{1}{2}(1+\operatorname{erf}(x/\sqrt{2}))$ and $a<b$, $\sigma>0$. I want to draw samples using only the normal distribution (using Python 2.7), and i do this by taking a value from the normal distribution with the same mean $\mu$ and sd $\sigma$ over and over again until this value lies between $a,b$. I have made a histogram using a sample size of 100.000 and plotted it with this pdf and it looks similar. My question is: are these distributions the same?","['statistics', 'probability', 'density-function', 'normal-distribution']"
2551082,Linearity of expectation,"I understand the algebraic proof of the linearity of expectation, but I cannot grasp the intuition, especially when the random variables are not independent. It is even counterintuitive as you cannot simply add the probabilities of dependent random variables, but it does not affect on the expected value. Can anyone explain the intuition behind the algebraic formula? How to think about it?","['expectation', 'probability']"
2551099,"If $a+b\mid a^4+b^4$ then $a+b\mid a^2+b^2$; $a,b,$ are positive integers.","Is it true: $a+b\mid a^4+b^4$ then $a+b\mid a^2+b^2$? Somehow I can't find counterexample nor to prove it. I try to write it $a=gx$ and $b=gy$ where $g=\gcd(a,b)$ but didn't help. It seems that there is no $a\ne b$ such that $a+b\mid a^4+b^4$. Of course, if we prove this stronger statement we are done. Any idea?","['number-theory', 'divisibility']"
2551129,"Homotopy of homeomorphisms implies homeomorphism $X \times [0,1] \to X \times [0,1]$?","Let $h_0$ and $h_1$ be self-homeomorphisms of a topological space $X$. Let us say that $h_0$ is homotopic to $h_1$, and write $h_0 \sim h_1$ if there exists a 1-parameter family $h_t$, $t \in [0,1]$ of self-homeomorphisms of $X$ such that $(x,t) \mapsto h_t(x) :X \times [0,1] \to X$ is continuous. This is just the usual definition of homotopy, except that I am requiring the intermediate maps to also be homeomorphisms. It is natural to want the following thing, which would be true, for example, if we had succeeded in topologizing $\mathrm{Homeo}(X)$ so as to get a topological group in which path-equivalence coincides with homotopy equivalence. Hoped for statement: $h_0 \sim h_1$ implies $h_0^{-1} \sim h_1^{-1}$. My first idea to approach this was to consider $H:X \times [0,1] \to X \times [0,1]$ given by $H(x,t) = (h_t(x),t)$. This is a continuous bijection restricting to a homeomorphism $X \times \{t\} \to X \times \{t\}$ for each $t$. If $H$ were a homeomorphism, one could use the inverse mapping $H^{-1} : X \times [0,1] \to X \times [0,1]$ to obtain the desired homotopy. However, I do not see any reason for $H$ to be an open map. So, my question for you is: Question: Is $H$ a homeomorphism? Note that the hoped for statement does not actually turn on the question having a positive answer. Indeed, my friend noticed a cute argument which circumvents the issue. Simply consider the map $X \times [0,1] \to X$ defined as the composition 
$$ X \times [0,1] \overset{h_0^{-1} \times \mathrm{id}}{\longrightarrow} X \times [0,1] \overset{(x,t) \mapsto h_t(x)}{\longrightarrow} X \overset{h_1^{-1}}{\longrightarrow} X.$$
This gives a homotopy from $h_1^{-1}$ to $h_0^{-1}$ through homeomorphisms, witnessing $h_0^{-1} \sim h_1^{-1}$.","['general-topology', 'homotopy-theory']"
2551133,"Let $\angle BAC =90 ^{\circ} AB=15 ,CD=10 ,AD=5$ Then $OA=?$","Let $\angle BAC =90 ^{\circ} AB=15 ,CD=10 ,AD=5$ Then $OA=?$",['geometry']
2551181,Boundary of the cone of positive polynomials,"Assume that $P^d(\mathbb{R})$ is the cone of polynomials $f\in\mathbb{R}[x_1,...,x_n]$ such that $\deg f\leq d$ and $f(x)\geq 0$ for all $x\in\mathbb{R}^n$. Is it true that the boundary of this cone is the polynomials with at least one root? At the first glance, I thought the answer was yes but then I saw polynomials like $f=(xy-1)^2+x^2$ which is strictly positive on $\mathbb{R}^2$ but there exists no $a\in\mathbb{R}$ such that $a>0$ and $f(x)\geq a$ for all $x\in\mathbb{R}^2$. If the answer is yes, what is the situation of this polynomial?","['real-algebraic-geometry', 'polynomials', 'algebraic-geometry']"
2551183,Proving if a certain matrix exists or not,"The question is: Is there a natural $n$ and $A\in M_{n}(\mathbb C)$ (complex $n\times n$ matrices) such that the conditions \begin{align}
\operatorname{rank}(\,A\,) &= 10\\
\operatorname{rank}(A^2) &= 7\\
\operatorname{rank}(A^3) &= 2
\end{align} are satisfied? Either provide an example or prove it does not exist. It's clear that $10\le n$ and because $A$ is not invertible it means $11\le n$.
I've tried finding an example using Jordan Blocks: there are two block with the size of $4$ or one block with the size of $5$, in order to answer on the third term. Then I tried filling the matrix with block of 3 but it doesn't work out. 
I also don't have any idea how to prove such a matrix doesn't exist.","['matrices', 'jordan-normal-form', 'linear-algebra']"
2551189,Given are two structures $X$ and $Y$. Is $X$ isomorphic to $Y$?,"Let $\sigma = \left\{f,R,S,c\right\}$ be a signature with a 1-digit function symbol $f$, a 2-digit relation symbol $R$, a 3-digit relation symbol $S$ and a constant symbol $c$. Given are two $\sigma$-structures $X = (A, f^X, R^X,S^X,c^X), \space Y= (B, f^Y, R^Y, S^Y, c^Y)$ where $A=\left\{q,r,s,t,u\right\}, \space\space R^X=\left\{(q,q),(r,t),(t,r),(u,q)\right\}, \space\space S^X=\left\{(q,s,q),(u,t,r)\right\}, \space\space c^X = s$ $B=\left\{1,2,3,4,5\right\}, \space R^Y = \left\{(5,5), (4,1), (2,5), (1,4)\right\}, \, S^Y = \left\{(2,1,4),(5,3,5)\right\}, \,\, c^Y= 3$ and the functions $f^X: A \rightarrow A, \space\space f^Y: B \rightarrow B$ defined by: a∈A     | q  r  s  t  u               a∈B      | 1  2  3  4  5
-----------------------              --------------------------
f^X (a) | t  s  t  u  q               f^Y (a)   |2  5  1  3  1 Is $X$ isomorphic to $Y$? I'm curious how this is solved correctly? We have several different things given inside the structures.. So I think we need to do several different checks and all need to be satisfied. So first of all we need to make sure that $A \rightarrow B$ is bijective. From the defined functions we can see that they are bijective. $q \rightarrow 5, r \rightarrow 4, s \rightarrow 3, t \rightarrow 1, u \rightarrow 2$ Now we need to show that $R^X \Leftrightarrow R^Y$, I think. This is satisfied as well because let $q=5, u=2, r=4, t=1$. We see that we have same pairs if we insert and check. We also need to make sure that all constant symbols $c$ are equal. This is satisfied as well $s=3$ Finally, all function symbols $f \in \sigma$ need to be equal. Well, they are but I cannot really say why :p Thus $X$ is isomorphic to $Y$ Can you tell me if my attempt is correct and if not, how to do it correctly?","['elementary-set-theory', 'general-topology', 'group-theory', 'linear-algebra', 'discrete-mathematics']"
2551193,$M$'s entries are known up to $(1\pm \varepsilon)$. How well do we know $M$'s determinant?,"$M$ is a $n\times n$ positive definite matrix and we would like to compute its determinant. We observe another positive definite matrix $\hat{M}$ whose components are within some factor of those of $M$. In particular $\hat{M}_{ij}=c_{ij}\cdot M_{ij}$ and each $c_{ij}\in (1-\varepsilon, 1+\varepsilon).$ How is $|\hat{M}|$ related to $|M|$? In particular how small does $\varepsilon$ have to be in terms of $M$'s entries for strong bounds to exist? When each $c_{ij}=c_{i}$ for each $i$, it's immediate that $|\hat{M}|=\prod_ic_i\cdot |M|.$ There must be some correspondence, since the determinant is just a polynomial of a matrix's entries so it's continuous.","['numerical-methods', 'linear-algebra', 'determinant']"
2551198,Why Quotient group is not a subgroup? [duplicate],"This question already has answers here : Is a quotient group a subgroup? [closed] (2 answers) Closed 6 years ago . $G/N = \{ gN  | g \in G \}$ is factor group where $N$ is normal subgroup of $G$. My doubt is that is it subgroup of $G$ ? Though I found it is not a subset of $G$. But how, my claim is that if we take $x \in  G/N$ so $x= an$ for some $a \in G$ and $n \in N$. We know $n$ belongs to $G$ as it is element of normal subgroup $N$. So $x= an$ should belongs to $G$.","['normal-subgroups', 'group-theory', 'quotient-group']"
2551216,Does positive Yamabe invariant imply every metric in that conformal class has positive scalar curvature?,"Here I'm taking the Yamabe invariant of $(M,g)$ to mean the conformal invariant $Y(g)$ as given here , not the supremum $\sigma(M)$ over all metrics $g$ (which is also sometimes called the Yamabe invariant). I feel like my lack of visualisation of conformal changes of metric is stopping me from answering what might be a very basic question: Given a Riemannian manifold $(M,g)$ of dimension $\geq 3$, with $Y(g)>0$, does every metric in the conformal class of $g$ (i.e. those metrics $e^{2f}g$ for smooth functions $f$) have everywhere positive scalar curvature? $(*)$ In other words (by the Yamabe problem), if $(M,g)$ admits a conformal metric of constant positive scalar curvature, does every conformal metric have positive scalar curvature? As a potential counterexample, could one find a metric conformal to the standard metric on the sphere which has non-positive scalar curvature somewhere? If the answer to $(*)$ is no, could there exist a metric in the conformal class of $g$ with everywhere negative scalar curvature?","['conformal-geometry', 'riemannian-geometry', 'differential-geometry', 'curvature']"
2551255,Residue of Pole $s=1$ of $\zeta$ function,I have trouble to understand Why the residue of the riemann $\zeta$ function is 1. I can just find that One can see this because $\lim_{s\to 1} (s-1)\zeta(s)=1$. But I do not understand how to get the 1 by using the Series representation $\zeta(s)=\sum_{n=1}^\infty \frac{1}{n^s}$,"['complex-analysis', 'riemann-zeta']"
2551276,Triangle Sides/orthocenter/vertices problem,"I have to prove that: The sum of the squares of the distances of the vertices
of a triangle from the orthocenter is equal to twelve times the square of the
circumradius diminished by the sum of the squares of the sides of the
triangle. Specifically, denote $A,B,C,H$ be the vertices of a triangle, and the orthocenter respectively. And let $R$ be the circumradius of $\triangle ABC$, prove: $$AH^2+BH^2+CH^2 = 12R^2 - AB^2 - BC^2 - AC^2$$ Any hints/tips? Kinda lost here.","['triangles', 'geometry']"
2551286,Prove no field exists if $b+d\neq 0$,"I need to prove no field $K$ exists (with minimum 3 elements) which holds: $\forall a,c\in K, b,d\in K\setminus\{0\}: b+d\neq 0 \implies\frac a b + \frac c d = \frac{a+c}{b+d}$ I know this can't be true, but I don't know where to start to prove it. Probably my problem is with proving things false in general. Thanks in advance.","['field-theory', 'linear-algebra']"
2551297,Fourier transform of $\frac{1}{\sqrt{1 + x^2}}$,"I have a question about Fourier transforms: What does the following question means? "" Does the Fourier transform of the function $f(x)= \frac{1}{\sqrt{1 + x^2}}$ belongs to $L_2(\mathbb{R})$?"" We know that $L_2(\mathbb{R})= \{ f: \mathbb{R} \rightarrow \mathbb{C} \mid \int_{- \infty}^{+ \infty} |f(t)|^2 dt < \infty \} $ and we know that $\hat{f}(t) = \int_{- \infty}^{+ \infty} e^{-itx} f(x) dx$. Now for to show that $\hat{f}(t) \in L_2(\mathbb{R})$, what do we need to prove? $\textbf{(1)}$ $~$ Is it enough to show that $f(x) \in L_1(\mathbb{R}) \cap L_2(\mathbb{R})$, then we have some results that shows $\hat{f}(t) \in L_2(\mathbb{R})!$ $\textbf{(2)}$ $~$ Or we have to prove $\int_{- \infty}^{+ \infty} |\hat{f}(t)|^2 dt < \infty$? If $\textbf{(2)}$ is true? Can you please help me to show that? Here linked-to result someone has found its Fourier transform, but I cannot understand it! I mostly prefer to find it by usual integration ways such as variable changes or substitution or even by inverse transform theorem! Please let me know if I am wrong about using $\textbf{(1)}$ for to prove that $\hat{f}(t) \in L_2(\mathbb{R})$? Thanks!","['functional-analysis', 'real-analysis', 'fourier-analysis', 'fourier-transform']"
2551300,Quotient group members $\mathbb{Z}_3[X]/N$ with $N=1+X^2$,"What are the members of the quotient group $\mathbb{Z}_3[X]/N$ with $N=1+X^2$? I do not know how to write this out. Which operation do you use? 
Are these all possible polynomials in $\mathbb{Z}_3[X]$ multiplied or added with $N$?","['normal-subgroups', 'quotient-group', 'discrete-mathematics']"
2551359,Method of Characteristics(Advection equation with initial and boundary condition),"Solve, using the Method of Characteristics, the equation
$\frac{\partial \rho}{\partial t} + \frac{\partial \rho}{\partial x}=-\mu\rho$ for $x,t>0$ with the conditions $\rho(x,0)=f(x)$ and $\rho(0,t)=g(t)$. I'm currently trying to solve above problem using method of characteristics. I used initial condition $\rho(x,0)=f(x)$ and obtained $|\rho(x,t)|=|f(x-t)|e^{- \mu t}$. But I need to find $\rho(x,t)$ and also I don't know how to use the boundary condition $\rho(0,t)=g(t)$. I'm posting this problem here to get a hint.Thanks in advance.","['fluid-dynamics', 'partial-differential-equations', 'characteristics', 'mathematical-modeling', 'ordinary-differential-equations']"
2551384,Solve $(xz)^2+(yz)^2=(xy)^2$ over the integers,"I'm attempting to solve $(xz)^2+(yz)^2=(xy)^2$ over the integers. So far, I have that $x^2+y^2=\Big(\frac{xy}{z}\Big)^2$, and I can represent the triple $(x,y,xy/z)$ as a Pythagorean triple ($m,n\in \mathbb{Z})$: $$x=m^2-n^2$$
$$y=2mn$$
$$\Big(\frac{xy}{z}\Big)=m^2+n^2$$ But, I'm not sure how to proceed from here. Solving for $z$ in the last equation gives $z = \frac{xy}{m^2+n^2}$, and as $z$ is supposed to be an integer, I'm not sure where to go from here. A hint would be appreciated.","['number-theory', 'elementary-number-theory']"
2551441,Second order Terms from the Laplacian and Lie Bracket,"The Laplacian measures the second order average difference of u in a neighbourhood around a point, i.e. $$ \frac{1}{v(B_r)} \int_{B_r(x)} [f(y) - f(x)] = C \Delta u(x) r^2 + o(r^2) $$ Where the constant $C$ depends on the dimension. On the other hand, in a Lie group, the second order terms of the conjugate $ghg^{-1}h^{-1}$ forms the Lie bracket structure on the induced Lie algebra, which is a sort of 'difference measurement'. Is there a connection between these two 'second order' operators in mathematics?","['lie-groups', 'differential-geometry', 'lie-algebras', 'partial-differential-equations']"
2551453,Help explaining formula to calc students,This is a stupid question but what is the proper name of this formula? Been trying to look for a proper definition for this but can't find it online. It's been a long time since I had to use these that my brain is drawing a blank. $$||a||=\sqrt{\left(\frac{{\rm d}^{2}y}{{\rm d}t^{2}}\right)^{2}+\left(\frac{{\rm d}^{2}x}{{\rm d}t^{2}}\right)^{2}}$$ Edit: just found it used in this question: Acceleration of a particle described by parametric equations So it's the magnitude of the vector's speed?,"['derivatives', 'parametric', 'calculus']"
2551471,What is $\aleph_0!$?,"What is $\aleph_0!$ ? I know that in the original definition the factorial is defined for natural numbers but, what if we extend this concept to cardinal numbers? This concept has been extended to the real numbers by the $\Gamma$ function but I never see this kind of extension before. This is a proof that I made by myself and can be incorrect but still interesting for me. $\aleph_0\times(\aleph_0 - 1)\times(\aleph_0 - 2)\times ...$ We can rewrite this as $$\aleph_0! = \prod_{i = 1}^{\infty}(\aleph_0 - i) = \prod_{i = 1}^{\infty}(\aleph_0)$$ But, is this equal to: $$\aleph_0^{\aleph_0}$$ Also, if we assume the continumm hypothesis $2^{\aleph_0} = \mathfrak{c} \leq \aleph_0^{\aleph_0} \leq \mathfrak{c}$ Hence, $\aleph_0! = \mathfrak{c}$","['infinite-product', 'infinity', 'elementary-set-theory', 'factorial', 'cardinals']"
