question_id,title,body,tags
4787169,"A ""perfect"" (chess) rating system","Assume we want to have a player rating system with the following conditions: For simplicity, no draws. If A wins against B with ratings $a,b$ , their new ratings are $a'=f(a,b),b'=g(a,b)$ . Most important: Order of games shall play no role. Say, A wins twice (rest analogous): $a''=f(f(a,b),c)=f(f(a,c),b)$ for all $a,b,c$ . If possible, win=lose: $a+b=f(a,b)+g(a,b)$ . I'm quite sure condition(s) 3 is easily to fulfil and has been researched already; can condition 4 and 5 also be fulfilled? At least one trivial solution exists: $a'=a+k,b'=b-k$ with constant $k$ . But I'd rather have an Elo-like $f$ that considers winning is easier the larger $|a-b|$ gets, so $k$ should get smaller.","['puzzle', 'applications', 'functions', 'scoring-algorithm', 'probability']"
4787237,Set of natural numbers that sum to 2023 having the maximum product.,"Suppose I have a multiset of $n$ positive integers $S$ such that $\sum_{s \in S}s = 2023$ and $\prod_{s\in S}s$ is maximum. Then I need to determine the maximum value of this product. My work : Denote by $s_r$ the $r$ th element of $S$ . First, I use the AM-GM inequality and hence obtain that: $
\frac{2023}{n} \geq {\left(s_1\cdot s_2\cdot s_3 \cdots s_n\right)}^{\frac{1}{n}}.$ For the maximum to be achieved, $s_r = \frac{2023}{n} \ \ \ \forall \ r \leq n$ , each of which I call $s$ . Hence, $n = \frac{2023}{s}$ . So, I need to maximise $\prod_{s \in S} s = s^{n} = s^{\frac{2023}{s}}.$ Therefore, $s^n$ is maximised when: $\ln(s) = 1$ Hence, $s^n$ is maximised when $s = e$ . So I suppose that we can use the integer $3$ which is closest to $e$ and use it fill up the set $S$ . But $2023$ is not divisible by $3$ , so all of $s \in S$ can not be $3$ . Clearly, $674$ $3$ s can be used and $1$ $1$ s can be used, hence giving the optimum product to be $3^{674}$ . My questions are: Is my argument for this problem correct? Is the usage of $3$ in order to maximise the product justified by the argument that it is closest to $e$ ? Can the use of derivatives be avoided? Any help and insight will be appreciated. Edit : $S$ was supposed to be a multiset — not a set. I apologize for any confusions caused. Nevertheless, those answers which worked with $S$ as a set are still very helpful to me and other users for they answer a similar question. So I encourage not deleting them.","['optimization', 'calculus', 'inequality']"
4787297,How to integrate the above integrand? Please help me out.,"How to integrate $$\int_0^1 \frac{x^{49}}{1+x+x^2+\cdots+ x^{100}}\;dx$$ I am trying by breaking the summation in the denominator. $$\sum_{i=0}^{100} x^i=\frac{1-x^{101}}{1-x}$$ Therefore the above integrand will become $$\frac{x^{49}(1-x)}{1-x^{101}}$$ But how to approach further? As far as I can guess  the only way to proceed after this is by applying the Maclaurian series expansion of $1/(1-x)$ ; which is $$\sum_{i=0}^n x^i$$ . But the Series expansion of $1/(1-x)$ is applicable only when $-1 \le x \lt 1$ . In the integration, the limit is from $0$ to $1$ , i.e. we have to find the area in the interval $[0,1]$ . So, how to proceed further? Please help me out.","['calculus', 'definite-integrals']"
4787351,Leavitt Path Algebras and Direct Sum,"Let $A$ be an abelian group and $B, C$ be two subgroups of $A$ such that $A=B\oplus C$ . Consider the canonical inclusions and projections $p_C,p_B,\pi_C,\pi_B$ . We have the following relations: $\pi_Bp_B=1_B$ , $\pi_Cp_C=1_C$ , $\pi_Bp_C=0$ , $\pi_Cp_B=0$ and $p_B\pi_B+p_C\pi_C=1_A$ . If we identify all the ""ones"" and the ""zeros"" as maps we end up exactly with the CK1 CK2 relations of the Leavitt Path Algebra of the graph with one vertex and two petals. In this case $\pi_B,\pi_C$ are the ghost edges respectively of $p_B,p_C$ . Hence in a certain sense, this decomposition of $A$ induces a Leavitt Path Algebra, in particular, the Leavitt algebra $L_{1,2}(\mathbb{K})$ . More in general, the direct sum of $k$ subgroups will induce the algebra $L_{1,k}(\mathbb{K})$ . I'm wondering if there is some sense behind this or if it is only a coincidence. Is there a similar construction for other types of decompositions of an abelian group? (A similar thing appears when we consider an extension of groups $0\rightarrow M\xrightarrow{i} E\xrightarrow{\pi} G\rightarrow 0$ . If we choose a section $s:G\rightarrow E$ we can naturally induce a map $\phi:E\rightarrow M$ such that $i,\pi,s,\phi$ verify similar relations as above).","['graph-theory', 'ring-theory', 'abstract-algebra', 'group-theory', 'abelian-groups']"
4787368,Is $F_xF_{zy}=F_yF_{zx} $ true for any $C^{\infty}$ function $F:\mathbb{R}^3\to\mathbb{R}$?,"Is $F_xF_{zy}=F_yF_{zx} $ true for any $C^{\infty}$ function $F:\mathbb{R}^3\to\mathbb{R}$ ? What if we add the assumption $F_z\ne 0$ ? Motivation: If $F:\mathbb{R}^3\to\mathbb{R}$ is $C^\infty$ , and at $(x_0,y_0,z_0)$ , $F_z\ne 0$ , then in a neighbourhood of $(x_0,y_0,z_0)$ , $F(x,y,z)=F(x_0,y_0,z_0)$ gives a function $z=z(x,y)$ s.t. $F(x,y,z(x,y))=F(x_0,y_0,z_0)$ . We have $$z_{yx}=(z_x)_y\ \  \underbrace{=}_{?}\ \  \left(-\frac{F_x}{F_z}\right)_y=-\frac{F_{yx}F_z-F_xF_{yz}}{F_z^2}$$ ( Edit : The underbraced equality is incorrect, since $z_y=-\frac{F_y}{F_z}$ is not true for all $(x,y,z)\in\mathbb{R}$ .) Symmetrically we have $$z_{xy}=-\frac{F_{xy}F_z-F_yF_{xz}}{F_z^2} $$ Since $F_{xy}=F_{yx}$ and $z_{yx}=z_{xy}$ we yield $$F_xF_{zy}=F_yF_{zx}. $$ However, the above argument assumes $F'z\ne 0$ , and it is very indirect. Is $F_xF_{zy}=F_yF_{zx} $ true for any $C^{\infty}$ function? Do we have a direct proof?","['multivariable-calculus', 'calculus']"
4787480,"If a matrix contains a differential operator, can it have eigenvalues? If yes, how can I determine them?","If a matrix contains a differential operator, can it have eigenvalues? If yes, how can I determine them? I will assume that the answer is yes and try to solve the following problem. Consider the matrix $A$ defined as: $$ A = \begin{bmatrix}0 & {d \over dx} \\1 & 0
 \end{bmatrix} \tag 1$$ The eigenvectors can be found by solving a system of equations given by: $$ A\vec v - \lambda \vec v =0 \tag 2$$ where $\vec v$ is the eigenvector and $\lambda$ is the eigenvalue. Let $f(x)$ and $g(x)$ be components of $\vec v$ : $$ \vec v = \begin{bmatrix}f(x)  \\g(x) 
 \end{bmatrix} \tag 3 $$ Now equation $(2)$ becomes: $$ \begin{bmatrix}0 & {d \over dx} \\1 & 0
 \end{bmatrix} \begin{bmatrix}f(x)  \\g(x) 
 \end{bmatrix} = \lambda \begin{bmatrix}f(x)  \\g(x) 
 \end{bmatrix}\tag 4$$ which is essentially the following system of equations: $$ {d g(x) \over dx} - \lambda f(x)=0 \tag 5$$ $$ f(x) - \lambda g(x)=0 \tag 6$$ The solutions of this system of equations are: $$ f(x) = C \lambda e^{\lambda ^2 x} \tag 7$$ $$ g(x) = C e^{\lambda ^2 x} \tag 8$$ The constant $C$ should be determined from the boundary condition. However, I don't know how to actually determine the value of $\lambda$ (assuming that it actually exists). If $A$ was a linear operator, the eigenvalues would be calculated from its determinant. But now $A$ contains a differential operator. So my question is, how can the eigenvalues of $A$ be determined?","['systems-of-equations', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'matrices', 'eigenfunctions']"
4787485,Evaluating $\lim_{n\to\infty}\sum_{k=0}^{n/2-1} \frac{k}{(n-k)^2} $,"I am trying produce a closed form for $\lim_{n\to\infty}S_n$ where: $$ S_n = \sum_{k=0}^{n/2-1} \frac{k}{(n-k)^2} $$ For example, if $n = 10$ : $$S_n = \sum_{k=0}^{4} \frac{k}{(10-k)^2}  = \frac{0}{10^2}+\frac{1}{9^2}+\frac{2}{8^2}+\frac{3}{7^2}+\frac{4}{6^2} = 0.2159 $$ If $n = 1000$ : $S_n = 0.3059$ If $n = 10000$ : $S_n = 0.3068$ Though simulation, as $n$ increases, this series converges to $\approx 0.3069$ . However, I would like to know how to calculate this value without resorting to plugging in large $n$ . Even an approximation of the summation as a function of $n$ would be fine. However, my knowledge of sum calculation is limited to geometric series and harmonic sequences. What I've tried: Series Analysis I tried to find some patterns in the sequence, and know that the first term is always $0$ , and the last term is always $\frac{n/2 -1}{(n/2 + 1)^2}$ , which approaches $0$ as $n$ increases, but I'm not sure how to generate a closed form solution for $S_n$ . I also thought of breaking the series up into two parts as shown: $$ \sum_{k=0}^{n/2-1} k \cdot \frac{1}{(n-k)^2} $$ So there would be an arithmetic progression ( $S_{n+1} = S_{n} + 1 $ ), with each term multiplied by $1/{(n-k)}^2$ , but this still has lead me nowhere. I have noticed that if the series was: $$ \sum_{k=0}^{n/2-1} \frac{n}{(n-k)^2} $$ ( $k$ in numerator replaced with $n$ ), the sum converges to $1$ . I did this because this series is strictly larger than the original, and I wanted to use the direct comparison test to formally prove convergence. Of course, I don't know how to prove that this series converges either, so this wasn't much help. Integration The last thing I tried was to plot the values of each term in the summation, and then plot them for different $n$ . If I could get an equation for these curves as a function of $n$ , I should be able to integrate them and get the summation value, right? The curves look to have the general form: $$ f(k) = y \cdot (e^{k\cdot x} - 1)  $$ where $x$ and $y$ are two real numbers. Does this idea hold merit? How could I find these curves analytically? MATLAB code used to calculate sum: n = 1000;
sn = 0;
for k = 0:n/2-1
    sn(k+1) = k / (n-k)^2;
end
sum(sn)","['riemann-sum', 'convergence-divergence', 'sequences-and-series']"
4787487,Show that $\mathbb{P}(X>u)\leq\mathbb{E}(g(X)\mathbf{1}_{X>u})$ for all $u\in\mathbb{R}$ where $g(x)$ is nonnegative and weakly increasing.,"I am trying to solve the following exercise: Let $X$ be an absolutely continuous random variable with density $f$ . Let $g$ be a non-negative non-decreasing function with $\mathbb{E}(g(X))=1$ . Show that $$\mathbb{P}(X>u)\leq \mathbb{E}(g(X)\mathbf{1}_{X>u}),\ \ \forall u\in\mathbb{R}.$$ I have the following attempt: since $\mathbb{E}g(X)=1$ , we have that $$1=\int_{-\infty}^{u}g(x)f(x)dx+\mathbb{E}(g(X)\mathbf{1}_{X>u})\leq g(u)\int_{-\infty}^{u}f(x)dx+\mathbb{E}(g(X)\mathbf{1}_{X>u})=g(u)\mathbb{P}(X\leq u)+\mathbb{E}(g(X)\mathbf{1}_{X>u}).$$ Write $\mathbb{P}(X\leq u)=1-\mathbb{P}(X>u)$ , we have that $$g(u)\mathbb{P}(X>u)\leq g(u)-1+\mathbb{E}(g(X)\mathbf{1}_{X>u}),$$ and thus $$\mathbb{P}(X>u)\leq 1-\dfrac{1}{g(u)}+\dfrac{\mathbb{E}(g(X)\mathbf{1}_{X>u})}{g(u)}\leq 1+\dfrac{\mathbb{E}(g(X)\mathbf{1}_{X>u})}{g(u)}.$$ I do not know how to push it further. Another attempt is to use Markov inequality. Since $X$ is absolutely continuous and $g$ is non-decreasing and nonnegative, we have $$\mathbb{P}(X>u)=\mathbb{P}(X\geq u)=\mathbb{P}(g(X)\geq g(u))\leq\dfrac{\mathbb{E}g(X)}{g(u)}=\dfrac{1}{g(u)}.$$ What have I missed?","['analysis', 'expected-value', 'probability-theory', 'probability', 'random-variables']"
4787493,Convergence theorem for bounded continuous martingale in $L^2$,"I would like to be sure to understand the proof this theorem. Consider the set $\mathcal{M}^2$ of continuous martingale bounded in $L^2$ . If $(X_t)_{t\geq0}\in\mathcal{M}^2$ , there exists $X_{\infty}\in\mathcal{M}^2$ such that $(X_t)_{t\geq0}$ converges almost surely and in $L^2$ to $X_{\infty}$ . Moreover $\lVert X\rVert_{L^2} = \lVert X_{\infty}\rVert_{L^2}$ and for all $t\geq 0$ we have $\mathbb{E}(X_{\infty} | \mathcal{F}_t) = X_t$ Proof : Consider $X_t$ in $L_2$ a continuous martingale, to prove the convergence in $L^2$ norm it suffices to use the sequential characterization of the continuity (of the $L^2$ norm) and show that for all increasing sequence $t_n$ of positive numbers that goes to $\infty$ we have $X_{t_n}\to X_{\infty}$ . Consider such a sequence, since $X_{t_n}$ is a martingale and we are in $L^2$ we have for $m\geq n$ $$
\lVert X_{t_m} - X_{t_n}\rVert_{L^2}^{2} = \lVert X_{t_n}\rVert_{L^2}^{2} - \lVert X_{t_m}\rVert_{L^2}^{2}
$$ The $L^2$ norm of a martingale is increasing and here it is bounded, so as $n, m$ go to $\infty$ we have that $\lVert X_{t_m} - X_{t_n}\rVert_{L^2}^{2}\to 0$ . $L^2$ being complete and $X_{t_n}$ being a Cauchy sequence we conclude it has a limit $X_{\infty}$ . This limit is in $\mathcal{M}^2$ using the continuity of the $L^2$ norm. By continuity of the conditional expectation we also have $$
\mathbb{E}(X_{\infty} | \mathcal{F}) = \lim_{s\to\infty}\mathbb{E}(X_s | \mathcal{F}_t) = X_t
$$ Next we want to prove the almost sure convergence : from $X_{t_n}$ we can extract a subsequence $X_{\phi(t_n)}$ that converges almost surely to $X_{\infty}$ , we denote $A$ the set of probability $1$ on which we have this convergence. We would like to use that for all $\omega\in A$ we have $$
\lvert X_t - X_{\infty} \rvert\leq \lvert X_t - X_{\phi(t_n)} \rvert + \lvert X_{\phi(t_n)} - X_{\infty} \rvert
$$ However at the stage we have not upper bound for $\lvert X_t - X_{\phi(t_n)} \rvert$ . For this, we consider for all $n$ the martingale $Y_{n,s} = X_{\phi(t_n)+s} - X_{\phi(t_n)}$ . It is in $L^2$ so by Doob’s inequality and the first result we prove earlier for convergence in $L^2$ of martingale in $L^2$ we have $$
\lVert \sup_{s\geq 0}\lvert Y_{n,s}\rvert\rVert_{L^2} \leq 2\lVert Y_n \rVert_{L^2}=2\lVert Y_{\infty}\rVert_{L^2} = 2\lVert X_{\infty} - X_{\phi(t_n)}\rVert_{L^2}\to 0
$$ When $t_n\to\infty$ . This proves that $ \sup_{s\geq 0}\lvert Y_{n,s}\rvert$ converges to $0$ in $L^2$ . By extracting a sub sequence $\left(Y_{\psi(n),s}= X_{\psi(\phi(t_n))+s} - X_{\psi(\phi(t_n))}\right)$ we get a convergence almost surely to $0$ , denote $B$ the set where take place such convergence. Now consider $C=B\cap A$ , we have $\mathbb{P}(C)=1$ . Hence, consider $\epsilon>0$ , $\exists N\left(\frac{\epsilon}{2}\right)$ such that $\psi(\phi(t_n))> N\left(\frac{\epsilon}{2}\right) $ implies $$
\sup_{s\geq 0}\lvert Y_{\psi(n),s}\rvert + \lvert X_{\psi(\phi(t_n))} - X_{\infty} \rvert\leq \frac{\epsilon}{2} + \frac{\epsilon}{2}
$$ Now take $t$ big enough such that $s = t-\psi(\phi(t_n))>0$ , then we have $$
\lvert X_t - X_{\psi(\phi(t_n))} \rvert = \lvert X_{\psi(\phi(t_n)) + s} - X_{\psi(\phi(t_n))} \rvert\leq \sup_{s\geq 0}\lvert Y_{\psi(n),s}\rvert
$$ Thus $$
\lvert X_t - X_{\infty}\rvert\leq \lvert X_t - X_{\psi(\phi(t_n))} \rvert + \lvert X_{\psi(\phi(t_n))} - X_{\infty} \rvert\leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
$$ This holds for all $\omega\in C$ so we conclude for the convergence almost surely. Is this seems correct please ? I try to make clear every step in order to show my understanding of all notions that are used here, I would like to know if my understanding (and hence my proof) is correct please. Thank you !","['stochastic-processes', 'probability-theory', 'martingales']"
4787505,"How to prove that $f(x,y)=xy(x+y)$ isn't surjective as $f:\mathbb{Q}^2\to\mathbb{Q}$.","I would like to show that the rational polynomial function $f:\mathbb{Q}^2\to\mathbb{Q}$ defined as $f(x,y)=xy(x+y)$ isn't surjective. To do this, I tried proving that the algebraic plane curve $$K:xy(x+y)=1$$ has no rational points so that $1\not\in f[\mathbb{Q}^2]$ . I then considered the following elliptic curve $$E:v^2-v=u^3$$ which, according to the LMFDB , has the three rational points $E(\mathbb{Q})=\{\mathcal{O},(0,0),(0,1)\}\cong\mathbb{Z}/3\mathbb{Z}$ . Now, the function $\psi:K(\mathbb{Q})\to E(\mathbb{Q})$ defined as $\psi(x,y)=\left(-
\frac{1
}{x+y},\frac{y}{x+y}\right)$ is well defined since $x+y$ isn't annihilated on $K$ and $$v^2-v=\left(\frac{y}{x+y}\right)^2-\frac{y}{x+y}=\frac{y^2-y(x+y)}{(x+y)^2}=-\frac{xy}{(x+y)^2}=-\frac{1}{(x+y)^3}=u^3$$ with $(u,v)=\psi(x,y)$ . But this means that $K$ has no rational points since $-\frac{1}{x+y}\neq0\Rightarrow \psi(x,y)\not\in E(\mathbb{Q}).\ \square$ However, this seems too much as it relies on theory of elliptic curves computing the Mordell-Weil group of $E$ . Maybe there's a non-constructive approach which proves that some $q\in\mathbb{Q}$ isn't in the image of $f$ without specifying it. Or with elementary number theory employing, for example, modular arithmetic and power residues. So my question then is: Q: Is there an easier approach on proving that $f$ is not surjective?","['elliptic-curves', 'number-theory', 'alternative-proof', 'functions', 'polynomials']"
4787529,Dugundji: cofinite topology on countable infinite set is totally path-disconnected,"In James Dugundji's Topology (1966), chapter V, are the following problems: 1.3 Let $X$ be a[n infinite] countable set, with topology $\cal T = \{\emptyset\} \cup \{ A \space \vert \space {\complement}A$ is finite $\}$ . Show that $X$ is connected. 5.3 Let $X$ be the connected set in 1, Problem 3. Show that $X$ is totally pathwise disconnected. (By definition, $X$ is called totally pathwise disconnected if the only continuous maps $f: I \to X$ are the constant maps.) The first one is easy: in the cofinite topology, any two nonempty open sets meet, so there's no disconnection of $X$ . The second one was a challenge for me. A path in $X$ must be compact and connected, but all subsets of $X$ are compact and connected, so that doesn't help. Eventually I realized that a map $f: A \to X$ is continuous precisely when all fibers $f^{-1}(x)$ are closed. That led me to Proving a Sierpiński result on partitions of the unit interval into closed sets . So that's an answer, but it's hard for me to imagine many beginning topology students getting it. At this point in the text, we haven't even seen compactness, or identification maps, or separation axioms. Here's someone on Math Overflow who also found it not trivial: Why are the integers with the cofinite topology not path-connected? . Possibly there's a mistake in the text that makes this problem unusually hard? Certainly it has many small mistakes (you can see where I had to apply a correction above to Problem 1.3). Question. Is there any simpler approach to Problem V.5.3? I got interested in this question because of a resemblance to the pseudo-arc , which is another space with a homogeneous feel that's connected but totally path-disconnected. The pseudo-arc is Hausdorff, though, which makes it pretty different. So, a bonus question : are there other interesting examples of such spaces?","['general-topology', 'path-connected', 'real-analysis']"
4787545,Why doesn't simplifying this limit work?,"I tried to find the following limit: $$\lim _{x \rightarrow 0}\left(\frac{\sin x^2}{\sin x^2+x^2 \cos x^2}\right)$$ I simplified it by dividing by $\sin x$ , and I got $$\lim _{x \rightarrow 0}\left(\frac{1}{1+x^2 \tan \left(x^2\right)}\right)$$ which is equal to $1$ . However, my book states that the answer is $\frac{1}{2}$ , and L'Hopital's theorem is used to find the limit. What is the mistake in my method?","['limits', 'calculus']"
4787556,How is summation defined for two measurable functions with different measurable sets?,"I have a question about the definition of sums of RVs and I can't find the answer to it. Assume that $A$ and $B$ are $\sigma$ -algebras and $A \subset B$ , if $X$ is $A$ -measurable and $Y$ is $B$ -measurable then how is $X+Y$ is defined?
My guess is that it's defined as $X + Y$ as a $A$ -measurable function, but I'm not sure if that's the case or even whether I'm asking the correct question. In case you wonder about the context, it comes up when I wanted to reason about the sums of conditional expectations in a filtration. For instance assume $X_t$ is adapted to $F_t$ , then how would you define $E[X_{t-1}|F_{t-2}]$ + $E[X_t|F_{t-1}]$ ? Is it $E[X_t + X_{t-1} | F_{t-1}]$ ?","['measure-theory', 'conditional-probability', 'conditional-expectation', 'measurable-functions', 'probability-theory']"
4787587,Is it possible to simplify $\frac{n + 1}2\sum_{k=1}^n\sin^2\frac{k\pi}{n+1}\left(1+2\cos\frac{k\pi}{n+1}\right)^s$ for integers $n\geq2$ and $s\geq1$?,"Is it possible to simplify this trigonometric sum? $$ \frac{n + 1}{2} \sum\limits_{k=1}^{n} \sin^2 \frac{k \pi}{n + 1}\left(1 + 2 \cos \frac{k \pi}{n + 1}\right)^s $$ $ n \ge 2 \in \mathbb{Z} $ , $ s \ge 1 \in \mathbb{Z} $ . The sum should always be an integer, would love to be able to calculate it in just the integers. Wonder if we can interpret this as some kind of sum of roots for some Chebyshev polynomials. Context - if that matters: This come up from an attempt to compute an analytic solution for this coding challenge. https://leetcode.com/problems/number-of-ways-to-stay-in-the-same-place-after-some-steps/ In particular, this formula comes calculating the matrix power after diagonalizing the tridiagonal Toeplitz matrix. The floating point numbers involved in the formula make it impractical to use, but if we find a route that doesn't involve them, then that would be enough. Of course, we have to keep the complexity advantage in order to be useful.","['algebra-precalculus', 'trigonometry']"
4787620,Uncountable set of different sized infinities,"Using Cantor's Theorem is is easy to see that $s=$ { $ \mathbb{N}, \mathcal{P}(\mathbb{N}), \mathcal{P}(\mathcal{P}(\mathbb{N})), \mathcal{P}(\mathcal{P}(\mathcal{P}(\mathbb{N}))), ...$ } is a countable set of ""different sized infinities."" However, I am interested in constructing an uncountable set of different sized infinities. More precisely, I want to find a set $S$ such that $$\text{card}(\mathbb{N})<\text{card}(S)$$ and for all $s_1, s_2 \in S$ such that $s_1 \neq s_2$ $$\text{card}(s_1)<\text{card}(s_2) \text{ or } \text{card}(s_2)<\text{card}(s_1).$$ Is this possible to construct $S$ with ZFC?",['elementary-set-theory']
4787685,Prove function is a PDF,"I'm trying to figure out how to prove $\theta y^{\theta} x^{-\theta -1}$ where $y>0, \theta>1, x\geq y$ is a PDF. So far I have $\int_{-\infty}^\infty \theta y^{\theta} x^{-\theta -1}dx=-\frac{y^\theta}{x^\theta}|_{-\infty}^\infty$ but don't know where to go from there. I know given the dependencies that the fraction will result in something at most 1, but don't how to make the integral equal to 1. Edit: I think I figured it out by changing the bounds. Since $x\geq y >0$ , then the bounds are from $y$ to $\infty$ . Thus $-\frac{y^\theta}{x^\theta}|_{y}^\infty=1$ . Any confirmation would be great. It is also trivial that the function is $\geq0$ , so it satisfies both properties to be a PDF.","['statistics', 'probability', 'density-function']"
4787735,$\lim\limits_{n\to\infty}\frac{1}{n} \ln\big(P\big(\sum_{i=1}^n X_i\geq a\big)\big)$,"I try to solve the following task, but I have no idea. Do you have a small hint?
Let $X_n$ be some iid sequence of random variables with $P(X_1=0)=P(X_1=1)=0.5$ . Let $a\in (\frac{1}{2},1)$ . Determine $\lim\limits_{n\to\infty}\frac{1}{n} \ln\big(P\big(\sum_{i=1}^n X_i\geq a\big)\big)$","['limits', 'probability-theory', 'probability', 'random-variables']"
4787756,Problem regarding coloring balls drawn from a bin,"I'm not sure if this has been asked before, but here I have the following problem: There are $n$ indistinguishable red balls in a bin. Each ""round,"" $k$ balls are randomly chosen from the bin with equal probability. All $k$ balls are colored blue and put back into the bin (this means that balls that are already blue are not changed). What is the expected number of rounds needed to color all $n$ balls blue? Is there an explicit formula for this problem? I've thought about this problem for a bit and quickly got stumped, but I realized that unless there was some insight for the problem, a recursive formula of some sort is probably needed as each round relies on the results of the previous rounds.  If the formula turns out to be recursive, is there some relatively ""simple"" heuristic that can estimate the expected number of rounds? There's also the tricky case of infinitely drawing blue balls after initially drawing some red balls, which would result in an infinite number of rounds. But my gut feeling says that the probability of this happening are small enough that the expected number of rounds should be finite. After searching a bit online, this seems to be (correct me if I'm wrong) the Coupon collector's problem but multiple coupons are drawn at once  instead of just one at a time, and that each of the coupons drawn are all distinct.  If that's the case, can any of the insights from that problem be used in the problem I have currently?","['expected-value', 'coupon-collector', 'balls-in-bins', 'recreational-mathematics', 'probability']"
4787861,Limit of an integral involving a general $f(x)$,"Let $f:[0,1]\to\Bbb R$ be a continuous function. Calculate $$\lim_{{n\to\infty}} \frac{\sqrt{n}}{\ln{n}} \int_0^1 \left( \sqrt{n+1+x+x^2+x^3+\ldots+x^{n-1}} - \sqrt{n} \right) f(x) \, dx.$$ I have absolutely no clue how to solve this kind of limit; I tried rationalization, simplifying it using the GP formula and then trying to break it into different integrals, and using Newton Leibniz rule (and L'Hopital's rule) to differentiate the inner term wrt $n.$ Can anyone please help me out with this?","['limits', 'calculus', 'definite-integrals', 'real-analysis']"
4787862,"Derive a closed-form expression for $\int\limits_0^\infty \frac{u \operatorname{J}_0(xu)}{\sqrt{1+u^2} \sqrt{1+\beta u^2}} \, \mathrm{d} u$","My objective is to derive analytically a closed-form expression for the following integral: $$
\int\limits_0^\infty \frac{u \operatorname{J}_0(xu)}{\sqrt{1+u^2} \sqrt{1+\beta u^2}} \, \mathrm{d} u, \tag{1}
$$ Here, $\operatorname{J}_0$ represents the zeroth-order Bessel function of the first kind, and both $x$ and $\beta$ are positive real numbers. It is worth noting that obtaining a closed-form expression for integral (1) is straightforward when $\beta$ takes on the values of either $0$ or $1$ . This question is inspired from this question in which Mr Zacky suggested using this question to obtain a solution of the problem.
Any help with this is highly appreciated. Thank you!","['integration', 'improper-integrals', 'real-analysis', 'calculus', 'bessel-functions']"
4787864,Constrained geometry on manifolds,"I learn some Einstein's notation and apply it to my knowledge on Differential Geometry. For instance, we can use his notation on the geodesic curve second order differential equality $\ddot{c}^k + \Gamma^k_{ij} \dot{c}^i \dot{c}^j=0$ such that it becomes format $\ddot{c} + \Gamma(c, \dot{c}) = 0$ on manifold $\mathcal{X}$ by use of below identities: \begin{align*}
(\dot{c} \dot{c}^\intercal)_{ij} &= \dot{c}^i \, \dot{c}^j \\
(\Gamma^k)_{ij} &= \Gamma^k_{ij} \\
\Gamma^k_{ij} x^i x^j &= \sum\limits_{i, j} (\Gamma^k \circ \dot{c} \dot{c}^\intercal)_{ij} \\
& = 1^\intercal (\Gamma^k \circ \dot{c} \dot{c}^\intercal)  1 \\
& = \mbox{tr}(\Gamma^k \dot{c} \dot{c}^\intercal)
\end{align*} The $k$ -rowise element $\Gamma^k(c, \dot{c})$ of here-called Christoffel vector $\Gamma(c, \dot{c})$ is given by trace $\mbox{tr}(\Gamma^k \, \dot{c} \dot{c}^\intercal)$ of product $\Gamma^k \, \dot{c} \dot{c}^\intercal$ . My question begins the geodesic equation constrained to some tangent subspace $U \subseteq T_{c(t)} \mathcal{X}$ such that velocity $\dot{c} \in T_{c(t)} \mathcal{X}$ and spanned by linear product $B_\ell p^\ell$ , for anew velocities $p \in U$ . By naive substitution, we get the equality: $B^k_\ell \dot{p}^\ell + \dot{B}^k_\ell p^\ell + \Gamma^k(c, B \, p)=0$ . Since tensor $B$ may not be square, usually with more rows than columns, we multiply by metric tensor $g_{m k}$ and also dual-space tensor $(B^\intercal)_m^n$ , which brings to design: $$(B^\intercal)_m^n \, g_{m k} B^k_\ell \dot{p}^\ell + (B^\intercal)_m^n \, g_{m k} \dot{B}^k_\ell p^\ell + (B^\intercal)_m^n \, g_{m k} \Gamma^k(c, B \, p)=0$$ Let us call the resulting constrained metric $(B^\intercal)_m^n \, g_{m k} B^k_\ell$ as metric $h_{n\ell}$ and constrained Christoffel vector element $I^n(c, p)$ as map $h^{n\ell} \left((B^\intercal)_m^n \, g_{m k} \dot{B}^k_\ell p^\ell + (B^\intercal)_m^n \, g_{m k} \Gamma^k(c, B \, p)\right)$ . I am sure, there is some more compact form, which I am not investigating in this text. Are my algebraic correct according to Differential Geometry formalism? If so, is there some canonical notation to $(B^\intercal)_m^n$ .","['geodesic', 'geometry']"
4787890,Does there exist $( x_n)_{n\in\mathbb{N}}\subset \mathbb{R}$ such that $\sum_{k=1}^{\infty} x_{kn} =1$ for every $n\in\mathbb{N}?$,"Does there exist $( x_n)_{n\in\mathbb{N}}\subset \mathbb{R}$ such that $\displaystyle\sum_{k=1}^{\infty} x_{kn} =1$ for every $n\in\mathbb{N}?$ I suspect so, but I am struggling to actually construct such a sequence. All I can say so far is that there must be infinitely many members of $( x_n)_{n\in\mathbb{N}}$ that are positive and infinitely many members of $( x_n)_{n\in\mathbb{N}}$ that are negative. Other than this, I have not made much progress. Can someone sketch a construction or provide any hints of a disproof please?","['sequences-and-series', 'examples-counterexamples', 'real-analysis']"
4787911,Monodromy Properties of Heun Functions in the Singularity Parameter,"Hello I'm studying a physical problem involving Heun equation and the local Heun functions.
Heun equation is the Fuchsian differential equation \begin{equation}
\frac {d^2w}{dz^2} + 
\left[\frac{\gamma}{z}+ \frac{\delta}{z-1} + \frac{\epsilon}{z-a} \right] 
\frac {dw}{dz} 
+ \frac {\alpha \beta z -q} {z(z-1)(z-a)} w = 0
\end{equation} with the constraint \begin{equation}
\epsilon=\alpha+\beta-\gamma-\delta+1
\end{equation} The solution which is holomorphic and equals $1$ at the singular point $z = 0$ is called the local Heun function denoted as $H\ell(a, q, \alpha, \beta, \gamma, \delta, z)$ . I am interested in the properties of this function as a function of the ""singularity parameter"" $a$ , in particular branch points and monodromies. Is there any paper or reference I should read?",['ordinary-differential-equations']
4787919,Solving $\int_0^1\frac{\log^4(1-x^2)}{1+x}dx$,"This question is similar to my last question . It's also from the same book, Problem 1.8(iii). $$I=\int_0^1\dfrac{\log^4(1-x^2)}{1+x}dx$$ Wolfram Alpha can't give the closed form, it is only giving a numerical expression. According to the book $$I=\dfrac{16}{5}\log^52-16\log^32\zeta(2)+48\log^22\zeta(3)-54\log2\zeta(4)-24\zeta(2)\zeta(3)+72\zeta(5)$$ I tried taking substitutions like $y=1-x^2$ . It didn't take me anywhere. Is there some way to solve this integral?","['integration', 'calculus', 'definite-integrals']"
4788041,How large does a sample need to be in order to be very representative with high probability?,"Suppose we have a set $X$ of $n$ objects that we want to sample uniformly at random. Let $\ell \colon X \rightarrow Y$ be a total, surjective function assigning a label to each object. For a subset $X'$ of $X$ and a label $y \in Y$ let $f_{X'}(y)$ denote the relative frequency of $y$ in $X'$ , i.e. $\frac{|\{ x \in X' \mid \ell(x) = y  \}|}{|X'|}$ . Given a subset $X'$ of $X$ , there are two simple ways to measure how representative it is: $$r_0(X') := \min \left\{ r \geq 0 \mid \forall y \in Y \colon (1-r)\cdot f_X(y) \leq f_{X'}(y) \leq (1+r)\cdot f_X(y) \right\}  $$ $$r_1(X') := \min \left\{ r  \geq 0 \mid \forall y \in Y \colon f_X(y) - r \leq f_{X'}(y) \leq f_X(y) + r \right\}  $$ If $r_0(X') = r$ , this means the relative frequencies in $X'$ differ by at most $(100\cdot r)$ % from their counterparts in $X$ . The function $r_1$ refers to the absolute difference. In both cases 0 denotes the highest degree of representativity, i.e. all relative frequencies in $X'$ coincide with their respective ones in $X$ . Assume we only know the number of objects $n$ , the number of labels $l =|Y|$ and the least amount of times a certain label occurs $k = \min \left\{ |\ell^{-1}(y)| \mid y \in Y \right\}$ . I would like to know the smallest sample size $m=|X'|$ such that the probability that $r_i(X') \leq r$ is at least $p$ for some given $r \geq 0$ , $p \in [0,1]$ and $i \in \{1,2\}$ . Stated differently, I'm looking for (bounds on) the function $f_i(n,l,k,r,p)=m$ .","['statistics', 'probability']"
4788085,"If $(X_{i})$ are i.i.d nonnegative with $\mathbb{P}(X_{1}=0)>0$, show that $\lim \sqrt[n]{X_{1}\dots X_{n}}=0$ (surely).","I am working on an exercise as follows. Given i.i.d non-negative random variables $(X_{i})_{i=1}^{\infty}$ with $\mathbb{P}(X_{1}=0)>0$ . Show that $$\lim_{n\rightarrow\infty}\sqrt[n]{X_{1}\dots X_{n}}=0.$$ I can show the almost surely convergence but the exercise asks for ""surely"" convergence. For the almost surely convergence, it is easy to see that $\sum_{n=1}^{\infty}\mathbb{P}(X_{n}=0)=\infty$ , and thus using independence and Borel-Cantelli lemma, we have $\mathbb{P}(X_{n}=0\ \text{i.o.})=1$ . Hence, with probability $1$ , for every $n\geq 1$ , there exists $k\geq n$ such that $X_{k}=0$ , which implies that $X_{1}\dots X_{k}\dots X_{n}=0$ for all $n\geq k$ . However, how to extend this result to all $\omega\in\Omega$ instead of just almost surely convergence? In other words, I also have to show that $Y_{n}\rightarrow 0$ for all $\omega$ such that $X_{n}(\omega)\neq 0$ eventually. I've been trying to show that in fact $\{X_{n}\neq 0\ e.v.\}=\varnothing$ , but I do not see a doable way to achieve this. Can this ""surely convergence"" be true? Is there an example where such a ""weighted product"" converges almost surely but not convergence surely?","['independence', 'borel-cantelli-lemmas', 'convergence-divergence', 'probability-theory', 'probability']"
4788093,Fourier transform of an $L^p$ function need not be a function,"Fourier transforms can be defined for any $L^p$ function for $1\leq p \leq \infty$ by treating them as a tempered distribution. I am looking for an explicit example of an $L^p$ function whose Fourier transform is a tempered distribution which is not generated by any function.
Explicit counter example, with a justification as to why its Fourier transform is not of function type will be greatly appreciated.
Thanks in advance.","['fourier-transform', 'functional-analysis', 'analysis']"
4788100,Solving $\nabla f=g$,"For $f: \Omega \subset \mathbb{R}^n \rightarrow \mathbb{R}$ , let $\nabla f=(\partial_{x_1}f,\ldots,\partial_{x_n}f)$ denote its gradient.
Under what conditions on $\boldsymbol{g}=(g_1,\ldots,g_n)$ , the equation $\nabla f=\boldsymbol{g}$ admits a solution? For $n=1$ , this problem is simple and for any continuous function $g:\Omega \rightarrow \mathbb{R}$ , the function $f(x):=\int_0^xf(s) d s$ does the job. The case $n>1$ , does not seem trivial to me. A precise necessary and sufficient conditon will be greatly appreciated.","['multivariable-calculus', 'calculus', 'analysis', 'real-analysis']"
4788133,An intriguing integral representation of $\zeta^2(3)$,"In a short presentation on RG, that is A Mesmerizing Integral Representation of $ζ^2(3)$ by C. I. Valean , we have, if I'm allowed, the following intriguing integral representation of $\zeta^2(3)$ , $$\zeta^2(3)$$ $$\small =\frac{32}{5} \int_0^1\left(2 \Re\left\{\text{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right)\right\}-\text{Li}_3\left(\frac{1}{1+x^2}\right)\right)\frac{\log (1-x) \log (1+x)}{x} \textrm{d}x.$$ The proof flows as follows: using that $\displaystyle  \int_0^1 \frac{y\log(1-y)\log(1+y)}{x^2+y^2}\textrm{d}y=-\frac{3}{8}\zeta(3)+\frac{1}{4}\operatorname{Li}_3\left(\frac{1}{1+x^2}\right)-\frac{1}{2}\Re\biggr\{\operatorname{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{2x}{1+x^2}\right)\biggr\}=-\frac{3}{8}\zeta(3)+\frac{1}{4}\operatorname{Li}_3(\cos^2(\theta))-\frac{1}{2}\sum_{n=1}^{\infty} \frac{\cos(2n\theta)}{n^3}, \ x=\tan(\theta), \theta \in \left(-\frac{\pi}{2},\frac{\pi}{2}\right)$ , given in More (Almost) Impossible Integrals, Sums, and Series (2023) , page $4$ , together with the fact that $\displaystyle \int_0^1 \frac{\log (1-x) \log (1+x)}{x}\textrm{d}x=-\frac{5}{8}\zeta(3)$ , also found (Almost) Impossible Integrals, Sums, and Series (2019) , we have $$\frac{32}{5} \int_0^1\left(2 \Re\left\{\text{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right)\right\}-\text{Li}_3\left(\frac{1}{1+x^2}\right)\right)\frac{\log (1-x) \log (1+x)}{x} \textrm{d}x$$ $$=-\frac{32}{5} \int_0^1\left(\frac{3}{2}\zeta(3)+4\int_0^1 \frac{y\log(1-y)\log(1+y)}{x^2+y^2}\textrm{d}y\right)\frac{\log (1-x) \log (1+x)}{x} \textrm{d}x$$ $$=6\zeta^2(3)-\frac{128}{5} \int_0^1 \left( \int_0^1 \frac{y \overbrace{\log (1-x) \log (1+x)}^{\displaystyle  f(x)} \overbrace{\log(1-y)\log(1+y)}^{\displaystyle  f(y)}}{x(x^2+y^2)}\textrm{d}y\right)\textrm{d}x$$ $$\small=6\zeta^2(3)-\frac{128}{5} \int_0^1 \left( \int_0^1 \frac{y f(x) f(y)}{x(x^2+y^2)}\textrm{d}y\right)\textrm{d}x\overset{\substack{\text{use  } \\ \text{the symmetry}}}{=}6\zeta^2(3)-\frac{128}{5} \int_0^1 \left( \int_0^1 \frac{x f(x) f(y)}{y(x^2+y^2)}\textrm{d}y\right)\textrm{d}x$$ $$\small=6\zeta^2(3)-\frac{64}{5} \int_0^1 \left( \int_0^1 \left(\frac{y}{x}+\frac{x}{y}\right) \frac{ f(x) f(y)}{x^2+y^2}\textrm{d}y\right)\textrm{d}x=6\zeta^2(3)-\frac{64}{5}\int_0^1 \frac{f(x)}{x}\left( \int_0^1 \frac{f(y) }{y}\textrm{d}y\right)\textrm{d}x$$ $$\small=6\zeta^2(3)-\frac{64}{5}\left(\int_0^1 \frac{\log(1-x)\log(1+x)}{x} \textrm{d}x\right)^2=\zeta^2(3),$$ which gives an end to the present proof. Question $1$ : Do we have in the mathematical literature challenging integrals with a similar structure of the integrand involving parts like $\displaystyle \text{Li}_n\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right), n\ge 2$ ? Any links or references would be highly appreciated. Question $2$ : I would love to see different approaches to this integral, this time not for the sake of seeing more proofs, but out of the curiosity of possible interesting and subtle connections with integrals and series that are out of my sight at the moment. Therefore, even some unfinished solutions could be very interesting. Update $1$ : Another very nice version presented by the same author is $$\zeta^2(3)$$ $$=\frac{32}{19} \int_0^1 \biggr(\Re\left\{\operatorname{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right)\right\}\left(\log ^2(1-x)+\log ^2(1+x)\right)$$ $$-\operatorname{Li}_3\left(\frac{1}{1+x^2}\right) \log (1-x) \log (1+x)\biggr)\frac{1}{x} \textrm{d}x,$$ which combines the previous result and the particular case of a result given in More (Almost) Impossible Integrals, Sums, and Series (2023) .","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'sequences-and-series']"
4788163,Exact differential in the setting of logarithmic integral,"The question is in trying to understand the ""Argument Principle"" Gamelin's Complex Analysis textbook (pages 224-225). Suppose $f(z)$ is analytic on a domain $D$ . For a curve $\gamma$ in $D$ such that $f(z) \neq 0$ on $\gamma$ , we refer to \begin{equation}
\frac{1}{2 \pi i} \int_{\gamma} \frac{f'(z)}{f(z)} \, dz = \frac{1}{2 \pi i} \int_{\gamma} d (\log f(z)) 
\end{equation} as the logarithmic integral of $f(z)$ along $\gamma$ . Thus the logarithmic integral measures the change of $\log f(z)$ along the curve $\gamma$ . I couldn't understand what the author meant by ""exact"" and ""closed but not exact"". Any help in understanding these two concepts are much appreciated.","['complex-analysis', 'logarithms']"
4788174,Sum of two sinusoids,"Let $A_1 , A_2  , w_1, w_2 $ are positives numbers  If for all real t , $s_1(t)=A_1\cos(w_1t+\phi_1)$ , $s_2(t)=A_2\cos(w_2t+\phi_2)$ and $s=s_1 +s_2$ We know if $w_1 = w_2$ , we can write $s$ as $s(t)=A\cos(wt+\phi), \forall t\in \mathbb R$ . If $w_1\neq w_2$ ,  I'm trying to prove  that, we can't write $s$ as $s(t)=A\cos(wt+\phi), \forall t\in \mathbb R$ . My attempt was deleted (any comment)","['trigonometry', 'laplace-transform', 'real-analysis']"
4788189,Sets closed on addition,"Consider a set of positive real numbers $S$ which is closed on addition. Also if we have a set of positive real numbers $S_2$ such that each element in $S$ can be expressed as the sum of elements in $S_2$ , allowing an element to be used multiple times, then we say $S_2$ generates $S$ . For any set $S$ (of positive real numbers) closed on addition, we say element is special if it cannot be expressed as the finite sum of other elements (obviously, you can't use the element itself). Then, we call a set good if the set of special elements of $S$ generates $S$ . The problem in question was that if you are given an infinite sequence of positive reals $a_n$ that is decreasing and $a_i$ is in $S$ for all $i$ and $a_i - a_{i+1}$ is also in $S$ for all $i$ , can we find a subset of S closed on addition that is bad? (By the way, S is closed under addition as well) I think the answer is yes but couldn't prove it. Some things i tried to prove was that the set generated by numbers of the form $a_i$ and $a_{i}-a_{i+1}$ for all $i$ is a bad set, and obviously this is a subset of S, but this seemed to fail since i dont know how to express $a_i-a_{i+1}$ as the sum of other elements, but $a_i$ is easy as $a_i = (a_i -a_{i+1})+a_{i+1}$ Also something cool i found was that $a_i-a_{i+1}$ is eventually less than $c$ for any positive real number $c$ , which gives that for any open interval $(a,b)$ with $a<b$ , we have an infinite number of elements in that range","['semigroups', 'sequences-and-series']"
4788212,Is there any relation between an Ehresmann connection/connection 1-form and an affine connection?,"We can define a connection 1-form which picks out the vertical part of any vector in the tangent bundle of some principal bundle. This allows us to define parallel transport so that we have a way to connect nearby fibers. As a map, it maps vectors in the tangent bundle to the Lie algebra associated with the Lie group of the principal bundle In contrast, an affine connection is defined for any manifold and maps the cartesian product of two vector fields to another vector field. I am confused on the relationship between these two kinds of connections. On one hand they both capture a similar idea of connecting nearby fibers to each other, but as maps they seem completely different with one have values in the space of vector fields and the other having values in a Lie algebra. Furthermore affine connections seem to have nothing to do with horizontal or vertical tangent spaces, which seem to be at the heart of connection 1-forms/Ehresmann connections. Is there any relationship between these two objects? I took a look at some older posts, such as the one below, but I am still confused. Levi-Civita connection as an Ehresmann connection","['principal-bundles', 'connections', 'fiber-bundles', 'differential-geometry']"
4788236,Linear Operators on Hilbert Spaces,"Here is a question that I cannot figure out myself: Let $H$ and $K$ be separable Hilbert spaces (possibly infinite-dimensional), and $\lbrace e_i\rbrace_{i\in \mathbb{N}}$ be the orthonormal basis for $H$ . $T: H \to K$ a $\textbf{linear operator (not necessarily bounded)}$ such that $$\sum_{i\in\mathbb{N}}\vert\vert T(e_i)\vert\vert_{K}^2< \infty$$ Where $\vert \vert \cdot\vert\vert_{K}$ is the norm of $K$ induced by inner product. $\textbf{Prove or disprove}$ that $T$ is a bounded linear operator. The key problem here is that I cannot passage the limit, i.e., $T(\sum\limits_{i}e_i) = \sum\limits_{i}T(e_i)$ may not hold. Thanks in advance for any help!","['hilbert-spaces', 'functional-analysis', 'real-analysis']"
4788237,"Find $P(\sum_{i=1}^{10} X_i>50)$ if $X_i\sim U\{\pm 1,\dots,\pm 6\}$.","A stock is currently worth \$ $100$ . each day a coin is flipped and a dice rolled. If the coin lands heads the stock price increases by the rolled value of the dice in $. If the coin lands tails, the stock price decreases by rolled value of the dice. What is probability that after 10 days the stock is worth more than 150? What I've tried is that 50 is such a large number that if you don't roll a 6, you have to roll 5*5 for the dice every day. And then to discuss case by case of rolling one 6,two 6 and so on.But that's a lot of work and seems tedious for 10 cases. Anyone can give me some insights on this?","['simulation', 'stochastic-processes', 'generating-functions', 'probability', 'random-variables']"
4788243,The probability that ${x_1}^{1/k_1}+{x_2}^{1/k_2}+\dotsb+{x_n}^{1/k_n}$ is less than $1$ - combinatorial proof?,"A friend and I were playing around with Beta integrals and we noticed the following fact. Choose some positive integers $k_1,k_2,\dots,k_n$ , and let $x_1,x_2,\dots,x_n$ be independent uniform random variables between $0$ and $1$ . Then the probability that ${x_1}^{1/k_1}+{x_2}^{1/k_2}+\dotsb+{x_n}^{1/k_n}\le1$ is $$\frac{k_1!k_2!\dotsm k_n!}{(k_1+k_2+\dotsb+k_n)!}.$$ (Sidenote: In fact, using the Gamma extension of the factorial, this works for noninteger $k_i$ as well. This has the nice corollary that the volume of a sphere is $2^3\frac{\frac12!^3}{\frac32!}r^3=\frac43\pi r^3$ . In fact, this yields the volume formulas for $n$ -balls in any dimension.) This result is particularly clean in that it can be expressed as a multinomial coefficient : it's $\dfrac1{\binom{k_1+k_2+\dotsb+k_n}{k_1,k_2,\dots,k_n}}$ . (The multinomial coefficient describes the number of ways of distributing $k_1+k_2+\dotsb+k_n$ objects into bins of sizes $k_1,k_2,\dots,k_n$ .) In particular, it is always the reciprocal of an integer. Given the combinatorial nature of the solution, we should expect a combinatorial proof. Is there one? (One can prove this using Beta integrals, as I noted above, but I wouldn't call this combinatorial.) In other words: is there an elementary proof of the identity $P({x_1}^{1/k_1}+{x_2}^{1/k_2}+\dotsb+{x_n}^{1/k_n}\le1)=\frac{k_1!k_2!\dotsm k_n!}{(k_1+k_2+\dotsb+k_n)!}$ ? (P.S. The case where $k_1=\dotsb=k_n=1$ is well-known.)","['multinomial-coefficients', 'probability']"
4788291,"How to get $\frac{d}{dt}(D_yx(t,y))=[D_xf(t,x(t,y))](D_yx(t,y)$?","Consider the nonlinear DE $x'=f(t,x)$ where $f\in C^1$ . For a fixed $\tau$ let $x(t,y)$ be the solution of the IVP $$
x'=f(t,x), x(\tau)=y
$$ How to get the equation of variation for the $n\times n$ Jacobian matrix $D_yx$ is $$
\frac{d}{dt}(D_yx(t,y))=[D_xf(t,x(t,y))](D_yx(t,y))
$$ And thus $$
\frac{d}{dt}(det(D_yx(t,y)))=tr([D_xf(t,x(t,y))])det((D_yx(t,y)))
$$ I am confused about the derivative. If we take derivative of $D_y x$ w.r.t. $t$ , why we have $D_x f$ ? Is that we exchange the derivative? $$
\frac{d}{dt}(D_yx(t,y))=D_y[\frac{d}{dt}(x(t,y))]=?
$$","['analysis', 'ordinary-differential-equations']"
4788341,"Show that $\sum_{n\leq x}\frac{f(n)}{\sqrt n}=2L(1,\chi)\sqrt{x}+O(1)$, where $f(n)=\sum_{d\vert n}\chi(n)$.","Exercise 2.4.4 from M. Ram Murty's Problems in Analytic Number Theory asks us to show that for $\chi$ a nontrivial Dirichlet character $(\operatorname{mod} q)$ , we have the estimate $$\sum_{n\leq x}\frac{f(n)}{\sqrt n}=2L(1,\chi)\sqrt{x}+O(1)$$ where $f(n):=\sum_{d\vert n}\chi(n)$ . I'm having trouble getting the correct bound on the error term. Using Abel's summation formula and Dirichlet's hyperbola method , I found that $$\sum_{n\leq x}\frac{f(n)}{\sqrt n}=\frac{A(x)}{\sqrt x}+\frac{1}{2}\int_1^x\frac{A(t)}{t^{\frac{3}{2}}}dt$$ where $$A(x)=xL(1,\chi)-x\sum_{n> y}\frac{\chi(n)}{n}+O(y)+O\left(\frac{x}{y}\right)\text{ for any }y>0$$ The optimal choice of $y$ for minimizing the terms $O(y),O(x/y)$ would seem to be $y=\sqrt x$ , but this yields $$A(x)=xL(1,\chi)-x\sum_{n> \sqrt x}\frac{\chi(n)}{n}+O(x^{1/2})$$ and the $O(x^{1/2})$ term, when integrated in $\frac{1}{2}\int_1^x\frac{A(t)}{t^{3/2}}dt$ , yields a $O(\log x)$ term and thus poses a problem for $\sum_{n\leq x}f(n)/\sqrt{n}$ since all the terms that are not $2L(1,\chi)\sqrt{x}$ should be bounded. Am I missing something? Is there a better choice for $y$ ?","['analytic-number-theory', 'number-theory', 'l-functions', 'dirichlet-character']"
4788367,Continuous extension of a function which retains the modulus of continuity,"Question Let $S \subset \mathbb R^d$ be compact and $f : S \to \mathbb R$ be continuous (hence uniformly continuous) on $S$ . A function $\omega : (0,\infty) \to (0,\infty)$ is called a modulus of continuity for $f$ if, for all $\epsilon>0$ , $$
\|x-y\| < \omega(\epsilon) \implies |f(x)-f(y)| < \epsilon, \text{for all } x,y \in S.
$$ In other words, $\delta = \omega(\epsilon)$ satisfies the $\epsilon-\delta$ of uniform continuity for $f$ . Furthermore, as illustrated by a counterexample in the comments, we insist that $\lim_{\epsilon \to 0} \omega(\epsilon) =0$ . Suppose that $S \subset \mathbb R^d$ is compact and $f : S \to \mathbb R$ admits a modulus of continuity $\omega$ on $S$ . Does there exist a function $g : \mathbb R^d \to \mathbb R$ such that (a) $g(x) = f(x)$ on $S$ , and (b) $g$ is uniformly continuous on $\mathbb R^d$ with modulus of continuity $\omega$ ? We can possibly relax the modulus of continuity of $g$ to be of the form $C \times \omega$ for any $C>0$ positive, but not more than that. Motivation In order to motivate this question, it helps to look at other extension theorems. Given $f : O \to \mathbb R$ uniformly continuous on an open set $O$ , it admits a unique extension to $\overline{O}$ which is also uniformly continuous. The proof technique is to define $\tilde{f}(x) = \lim_{y \to x, y \in O} f(y)$ and show that this works. Given any $f : S \to \mathbb R$ continuous where $S$ is closed, it admits a continuous extension to $\mathbb R^d$ . This follows directly from the Tietze extension theorem. If we assume that the modulus of continuity has a specific structure, then some results are known. See e.g. the Kirszbraun Theorem for Lipschitz moduli of continuity i.e. $\omega(\epsilon) = C\epsilon$ for some $C>0$ . Here's a nice proof of that result. What makes this particularly difficult to approach is the control of the modulus of continuity of $g$ outside the set $S$ . For example, suppose that we naively try to force $g$ to be constant outside an open set $O$ which contains $S$ , so that its modulus of continuity is controlled on $O^c$ . Certainly we can find a function $g$ such that $g \equiv f$ on $S$ and $g \equiv 0$ on $O$ , by a variation of Tietze's theorem. However, what we can't do using that scheme is control $|g(x)-g(y)|$ in the region between $S$ and $O$ . That's what makes this question interesting.","['uniform-continuity', 'analysis', 'real-analysis', 'continuity', 'general-topology']"
4788370,The fundamental group of closed orientable surface of genus 2 contains a free group on two generators,"Let $S$ be the closed orientable surface of genus $2$ . It is well known that its fundamental group is given by $$ \pi_1(S)=\langle a,b,c,d:[a,b][c,d]=1\rangle.$$ How can we show that this group has a subgroup isomorphic to a free group on two generators? Let $F=\langle x,y\rangle$ be a free group on two generators $x,y$ . Intuitively, it seems that the map $F\to \pi_1(S)$ determined by $x\mapsto a$ , $y\mapsto b$ is injective, but I can't see how to prove that this map has trivial kernel. Or can we use covering space theory? If there is a covering map $S'\to S$ such that $\pi_1(S')\cong F$ then we'll be done. Any hints?","['fundamental-groups', 'free-groups', 'covering-spaces', 'group-theory', 'algebraic-topology']"
4788390,"Where did the author use the condition that $C$ contains $x$? (Michael Spivak's ""Calculus on Manifolds"")","For $\delta>0$ let $$M(a,f,\delta)=\sup\{f(x):x\in A\text{ and }|x-a|<\delta\},\\
m(a,f,\delta)=\inf\{f(x):x\in A\text{ and }|x-a|<\delta\}.$$ The oscillation $o(f,a)$ of $f$ at $a$ is defined by $o(f,a)=\lim_{\delta\to 0} [M(a,f,\delta)-m(a,f,\delta)]$ . This limit always exists, since $M(a,f,\delta)-m(a,f,\delta)$ decreases as $\delta$ decreases. In the proof of Theorem 1-11 in ""Calculus on Manifolds"" by Michael Spivak, the author wrote as follows: Let $C$ be an open rectangle containing $x$ such that ... Where did the author use the condition that $C$ contains $x$ ? Is the condition really necessary? 1-11 Theorem. Let $A\subset\mathbb{R}^n$ be closed. If $f:A\to\mathbb{R}$ is any bounded function, and $\varepsilon>0$ , then $\{x\in A:o(f,x)\geq\varepsilon\}$ is closed. Proof. Let $B=\{x\in A:o(f,x)\geq\varepsilon\}$ . We wish to show that $\mathbb{R}^n-B$ is open. If $x\in\mathbb{R}^n-B$ , then either $x\notin A$ or else $x\in A$ and $o(f,x)<\varepsilon$ . In the first case, since $A$ is closed, there is an open rectangle $C$ containing $x$ such that $C\subset\mathbb{R}^n-A\subset\mathbb{R}^n-B$ . In the second case there is a $\delta>0$ such that $M(x,f,\delta)-m(x,f,\delta)<\varepsilon$ . Let $C$ be an open rectangle containing $x$ such that $|x-y|<\delta$ for all $y\in C$ . Then if $y\in C$ there is a $\delta_1$ such that $|x-z|<\delta$ for all $z$ satisfying $|z-y|<\delta_1$ . Thus $M(y,f,\delta_1)-m(y,f,\delta_1)<\varepsilon$ , and consequently $o(y,f)<\varepsilon$ . Therefore $C\subset\mathbb{R}^n-B$ .","['proof-explanation', 'multivariable-calculus']"
4788412,Proving that function proportional to derivative is monotonic indirectly,The solution to the differential equation $f'(x)=Cf(x)$ is $f(x)=Ae^{Cx}$ The function $f(x)$ is monotonic. $$f(x)=Ae^{Cx}$$ $$f'(x)=ACe^{Cx}$$ $$\operatorname{sgn}(f'(x))=\operatorname{sgn}(AC)\operatorname{sgn}(e^{Cx})$$ $$\operatorname{sgn}(f'(x))=\operatorname{sgn}(AC)\cdot1$$ $$\operatorname{sgn}(f'(x))=\operatorname{sgn}(AC)=constant$$ This approach to proving the monotonicity depends on solving the diff eq. Is there a way to prove that any function that satisfies $f'(x)=Cf(x)$ is monotonic without solving for $f(x)$ ?,"['calculus', 'derivatives', 'ordinary-differential-equations']"
4788443,How to prove that $P(A) P(B)=P(A \cap B) P(A \cup B)+P\left(A \cap B^{\prime}\right) P\left(B \cap A^{\prime}\right)$,"What I tried to do so far is replace $P(B)$ with $P(B) = P(A \cap B) + P(B \cap A^c)$ . With which I get, $P(A)P(A \cap B) + P(A)P(B \cap A^c)$ . After that I also tried doing the same trick for $P(A)$ but it did not work out. Any help would be greatly appreciated.","['elementary-set-theory', 'probability']"
4788453,What is the Rademacher complexity of kernelbased hypotheses with offset?,"Let $\mathcal{H}=\{ x\rightarrow \langle \mathbf{w},\Phi(x)\rangle+b : \|  \mathbf{w}\|_{\mathbb{H}} \le \Lambda, b\in \mathbb{R}\}$ be a function family, where $\Phi$ is a feature mapping, and $\mathbb{H}$ denotes the feature space, i.e., Hilber space. How can we bound the Rademacher complexity of $\mathcal{H}$ ? I know that when $b=0$ , the Rademacher complexity of $\mathcal{H}$ is bounded by $\sqrt{\frac{r^2\Lambda^2}{m}}$ , where $K(x,x) \le r^2$ and $m$ is the number of samples.
But when the offset $b$ exists, how can we bound this Rademacher complexity?","['machine-learning', 'statistics', 'linear-algebra', 'real-analysis']"
4788473,An example of an open ball whose closure is strictly between it and the corresponding closed ball,"Does there exist a metric space $M$ , such that there exists a point $x$ and a positive real number $r$ , for which the open ball of radius $r$ centered at $x$ is a proper subset of its closure, and the closure in turn is a proper subset of the closed ball of radius $r$ centered at $x$ ?","['general-topology', 'metric-spaces']"
4788498,Sufficient conditions for $E(G\eta|Z=1)\geq 0$,"Consider three random variables $G,Z,\eta$ . $G$ and $Z$ are   binary. We assume $$
\begin{aligned}
&(1) \quad E(\eta|Z=1)=0,\\
&(2) \quad \Pr(G=1| Z=1)=1.
\end{aligned}
$$ In turn, $$
\begin{aligned}
E(G\eta|Z=1)& =E(G \eta|G=1, Z=1)\Pr(G=1|Z=1)= E(\eta| G=1, Z=1).
\end{aligned}
$$ Further, observe that $$
\begin{aligned}
E(\eta|Z=1)&=E( \eta|G=1, Z=1)\Pr(G=1|Z=1)+E(\eta|G=0, Z=1)\Pr(G=0|Z=1)=E( \eta|G=1, Z=1).
\end{aligned}
$$ Therefore, $$
(3) \quad E(G\eta|Z=1)=0.
$$ Question : Instead of (3), I'd like to write down some sufficient conditions to get the weaker conclusion $$
(3') \quad E(G\eta|Z=1)\geq 0.
$$ To this end, I would like to replace (2) with ""some"" inequality condition, such as $$
\begin{aligned}
& E(\eta| Z=1, G=1)\geq E(\eta| Z=1, G=0)\\
& \Pr(G=1|Z=1)\geq \Pr(G=0|Z=1)\\
& \Pr(G=1|Z=1)\geq \Pr(G=1)\\
\end{aligned}
$$ or similar. I can keep (1). Any idea?","['conditional-probability', 'conditional-expectation', 'expected-value', 'probability', 'random-variables']"
4788518,How to prove the following number theory puzzle,"Consider all the integers from 1-13. Without reusing any integers, find a set of three integers for which all numbers 1-13 may be created using only addition and subtraction. Prove that for all such sets, it is not possible to create a sum of 14. For this question, I've identified that 1,3,9 is a possible set of integers that fulfill the requirements as follows: $1 = 1 \\ 2 = 3 - 1 \\ 3 = 3 \\ 4 = 1 + 3 \\ 5 = 9 - 1 - 3 \\ 6 = 9 - 3 \\ 7 = 9 + 1 - 3 \\ 10 = 9 + 1 \\ 11 = 9 + 3 - 1 \\ 12 = 9 + 3 \\ 13 = 1 + 3 + 9$ The obvious reason that this set of integers will not add up to 14 is that their sum is 13, smaller than 14. Hence, we cannot get a sum of 14. But I'm not sure if this is the only set of integers that fulfill the requirements and therefore cannot prove that all of these sets of integers will not create a sum of 14. Is there a rule or property I may be overlooking? Any assistance on this would be much appreciated!","['algebra-precalculus', 'puzzle']"
4788544,How to calculate the excpected Number if ec primes were random?,"Ec primes are primes of the form: $(2^n-1)\cdot 10^d+2^{n-1}-1$ , where d Is the Number of decimal digits of $2^{n-1}$ . The Vector containing all the numbers n leading to a prime or probable prime Is: [2,3,4,7,8,12,19,22,36,46,51,67,79,215,359,394,451,1323,2131,3336,3371,6231,19179,39699,51456,56238,69660,75894,79798,92020,174968,176006,181015,285019,331259,360787,366770,541456] I dont know if there are other ec probable primes between n=400000 and n=541456 maybe there could be one or two.
The thing that strikes me Is the abundance of multiples of $43$ in the vector,infact there are four n's (215,69660,92020,541456) out of 38 which are multiple of $43$ .
If the primes of this type were random, how many n's leading to a prime would be excpected multiple of $43$ out of 38 n's leading to a prime? How to calculate the excpected Number of multiple of $43$ ?","['number-theory', 'statistics']"
4788547,Different surfaces with the same Gaussian and mean curvature everywhere,"I'm trying to teach myself the classical differential geometry of 2D surfaces in 3D Euclidean space but I'm struggling to understand exactly how much information the Gaussian and mean curvature provide. If I've understood the Bonnet theorem correctly, the Gauss–Codazzi equations are both necessary and sufficient conditions on the first and second fundamental forms to determine a surface (up to rigid motions). As elaborated on in this question Gaussian curvature and mean curvature sufficient to characterize a surface? , this implies that the Gaussian curvature and mean curvature alone should generally be insufficient to determine a surface. Can someone give me an explicit example of two different surfaces, parameterised over the same patch, which have the same Gaussian and mean curvature everywhere?","['surfaces', 'curvature', 'differential-geometry']"
4788567,Deriving a polylog bound,"Reading through a paper and it is stated that $k(1-1/d)^{k-1} < 1$ implies that $k < (1+o(1))d \log d$ without proof (assume $k,d > 1$ ). How is this fact derived? I assume that the $d \log d$ term comes from $\log(1-1/d) < \log \log d$ but apart from that it is not obvious to me.","['inequality', 'discrete-mathematics', 'logarithms']"
4788589,Inequality involving minors of a hermitian matrix,"Let $A$ be an $n \times n$ hermitian matrix with $n \geq 3$ . I am trying to prove the following inequality involving its minors $$\left| \sum_{k=3}^n A_{3k} A_{[12k],[123]} \right| \leq \sqrt{\sum_{i < j} |A_{[13],[ij]}|^2}\sqrt{\sum_{i < j} |A_{[23],[ij]}|^2},$$ where the square brackets in subscripts contain the numbers of rows and columns involved in the given minor. The inequality holds trivially (and saturates) for diagonal matrices and passes all numerical tests with flying colours, but I am unable to crack it in the general case. It looks Cauchy-Schwarz-esque, but that resemblance did not lead me anywhere. Let me add that the above inequality can be written equivalently as $$|\langle u_1 \otimes u_2 \otimes A u_3, A u_1 \wedge A u_2 \wedge A u_3 \rangle | \leq \tfrac{1}{2} \|A u_1 \wedge A u_3 \| \|A u_2 \wedge A u_3 \|$$ for any hermitian $A \in B(\mathbb{C}^n)$ and any pairwise orthonormal $u_1,u_2,u_3 \in \mathbb{C}^n$ , where the wedge products are not normalized, i.e. $u \wedge v = u \otimes v - v \otimes u$ and similarly for the triple wedge product. The inner product on $(\mathbb{C}^n)^{\otimes 3}$ is of course the natural one. Any hint would be much appreciated!","['cauchy-schwarz-inequality', 'hermitian-matrices', 'linear-algebra', 'tensor-products', 'inequality']"
4788602,Continuity of $L^p$ norm in terms of p [duplicate],"This question already has answers here : Limit of $L^p$ norm (4 answers) Closed 8 months ago . The community reviewed whether to reopen this question 22 hours ago and left it closed: Original close reason(s) were not resolved Can it be shown that $\lim_{p \rightarrow q} \lVert f \rVert_{L_p} = \lVert f \rVert_{L_q}$ ? I don't know where to even begin. I have thought of using the dominated convergence theorem, but I don't know how. I want to prove this because it seems like many proofs involving the $L_p$ norm will be much easier, e.g. it would often be enough to prove that something holds for $p>1$ and it follows easily that it also holds for $p=1$ . Instead the textbooks I have read seem to proof many things separately for $p>1$ and $p=1$ , which seems clumsy to me. Edit: My question is different from this question which only considers the case for $p \rightarrow \infty$ , while my question condiders $p \rightarrow q$ for any $q$ . I was not able to generalize this other proof for a general $q$ , but if this is possible, please help me out.","['limits', 'measure-theory', 'lp-spaces', 'lebesgue-integral']"
4788656,Divergence of Mehler's Hermite polynomial series,"According to Mehler's formula, $$
\sum\limits_{n=0}^{\infty}\left(\cfrac{\rho}{2}\right)^n\cfrac{H_n(x)H_n(y)}{n!}=\cfrac{1}{\sqrt{1-\rho^2}}\exp\left(-\cfrac{\rho^2(x^2+y^2)-2\rho x y}{1-\rho^2}\right),
$$ when $|\rho|<1$ , where $H_n(x)$ are Hermite polynomials. My questions is about what happens when $|\rho|> 1$ . Is it possible to show that the series on the left-hand goes to $+\infty$ for all $x$ and $y$ ? What happens when $|\rho|=1?$ .","['orthogonal-polynomials', 'divergent-series', 'hermite-polynomials', 'sequences-and-series']"
4788658,"$ \int_{-1/2}^{3/2} xf(3x^2 - 2x^3) \,dx = 2 \int_{0}^{1} xf(3x^2 -2x^3) \,dx $ proof","I have faced a confusing maths problem that goes : $f : [\frac{-1}{2}, \frac{3}{2}] \to\mathbb R$ is a continuous function Prove that : $${\int_{-1/2}^{3/2} x f(3x^2 -2x^3) \,dx =2 \int_0^1 xf(3x^2 - 2x^3) dx}$$ I thought about using the formula : $${\int_a^b\ f(x) dx = \frac{a+b}2\int_a^b f(x)dx 
}$$ and the formula : $${\int_a^b\ f(x) dx =\int_a^b f(a+ b -x)dx 
}$$ Nevertheless, they both seem not to help.
Even trying the hints mentioned in the comments is fruitless.","['integration', 'calculus', 'functions', 'definite-integrals']"
4788684,Having trouble finding the Laurent Series Expansion of $f(z)=\frac{ze^{z}}{(z-2)^{2}}$ on the region $0<|z-2|<R$,"I'm just not sure how to finish out this problem. This is how the problem was given: Find the Laurent Series Expansion for $\displaystyle f(z)=\frac{ze^{z}}{(z-2)^{2}}$ on the region $0<|z-2|<R$ . I can see it has a pole at $z_{0}=2$ of order $n=2$ . I understand we can write the Laurent Series for $0<|z-z_{0}|<R$ of a pole of order $n$ as the following: $\displaystyle \frac{a_{-n}}{(z-z_{0})^{n}} + \frac{a_{-(n-1)}}{(z-z_{0})^{n-1}} + \cdots +\frac{a_{-1}}{(z-z_{0})} + a_{0} + a_{1}(z-z_{0}) + a_{2}(z-z_{0})^{2} + a_{3}(z-z_{0})^{3} +\cdots$ With $z_{0}=2\text{, and }n=2$ we would find the following Laurent Series: $\displaystyle \frac{a_{-2}}{(z-2)^{2}} + \frac{a_{-1}}{(z-2)} + a_{0} + a_{1}(z-2) + a_{2}(z-2)^{2} + a_{3}(z-2)^{3}\cdots$ If I understand correctly we stop at $-2$ on the negative side because all $n<-2$ returns a zero for that coefficient.  However, on the positive side we take $n\to \infty$ so we can rewrite the series with $z_{0}=2\text{, and }n=2$ plugged in and get the following: $$\displaystyle \frac{a_{-2}}{(z-2)^{2}} + \frac{a_{-1}}{(z-2)} + a_{0} + a_{1}(z-2) + a_{2}(z-2)^{2} + a_{3}(z-2)^{3}\cdots$$ $$=\sum_{n=-2}^{-1}a_{n}(z-2)^{n} + \sum_{n=0}^{\infty}a_{n}(z-2)^{n}$$ This is where I get confused.  I see in my text that for a simple pole we can find: using $\displaystyle f(z)=\frac{ze^{z}}{(z-2)^{2}}$ (I included the simple pole because the book explicitly states this is $a_{-1}$ , but I know this does not apply to a pole of order $n$ ) $\displaystyle a_{-1}=\text{Res}(f(z),z_{0})=\lim\limits_{z\to z_{0}}(z-z_{0})f(z)$ and for the Residue at a Pole of order $n$ we have $\displaystyle \text{Res}(f(z),z_{0})=\frac{1}{(n-1)!}\lim\limits_{z\to z_{0}}\left(\frac{d^{n-1}}{dz^{n-1}}\left[(z-z_{0})f(z)\right]\right)$ But what coefficient is the Residue of a Pole of order $n$ ? $a_{-2}\text{ or }a_{-1}$ ? and what if the pole had a greater order, $n>2$ , resulting in many more $a_{-n}$ terms? Then, how do I find $a_{n}\text{ for }n\geq 0$ ? EDIT:
The textbook I reference throughout this post is Complex Analysis A First Course with Applications by Dennis Zill & Patrick Shanahan, 3ed EDIT 2 using information from @user317176's response: I found $a_{-1}$ using the residue theorem $$a_{-1}=\text{Res}\left(\frac{ze^{z}}{(z-2)^{2}},z_{0}=2\right)$$ $$a_{-1}=\frac{1}{(2-1)!}\lim\limits_{z\to 2}\left(\frac{d^{2-1}}{dz^{2-1}}\left[(z-2)^{2}\frac{ze^{z}}{(z-2)^{2}}\right]\right)$$ $$a_{-1}=\frac{1}{1!}\lim\limits_{z\to 2}\left(\frac{d^{1}}{dz^{1}}\left[(z-2)^{2}\frac{ze^{z}}{(z-2)^{2}}\right]\right)$$ $$a_{-1}=\lim\limits_{z\to 2}\left(\frac{d}{dz}\left[ze^{z}\right]\right)$$ $$a_{-1}=\lim\limits_{z\to 2}\left(e^{z}+ze^{z}\right)$$ $$a_{-1}=e^{2}+2e^{2}$$ $$a_{-1}=3e^{2}$$ still trying to figure out how to work out $a_{-2}$ and $a_{n\geq 0}$","['complex-analysis', 'laurent-series']"
4788702,What constitutes a direct proof?,"Suppose I want to prove the following elementary theorem: The perpendicular bisector of the base of an isosceles triangle is the angular bisector of its vertex angle. To do so, I would start by drawing the angular bisector of the vertex angle, and show that it must be a perpendicular bisector of the base. Since there exists a unique perpendicular bisector of the base, if we draw a perpendicular bisector of the base, we will inevitably draw what we just drew, the angular bisector of the vertex angle, hence proving the proposition. This method of proof, which in Asia (and by Asia I mean Japan and Korea) is introduced as an example of an indirect proof (the translation of which I could not find, hence the question), is described as follows: Suppose I want to prove that B is A. I then prove that A is B instead, and then prove that B is unique. Thus this unique B must be A also, completing the proof. So here are the questions: (1) Is the first proof given above (about the angular bisector of the vertex angle) an indirect proof? (2) Is there an English term for the method of proof described in the following paragraph? Logicians, help please!","['proof-writing', 'solution-verification', 'geometry']"
4788703,Number of teams that can be formed subject to the condition that A will work on the team only if B also works on the team,"In an organization there are 11 people A, B, C, D, E, F, G, H, I, J, K.  Four members from these 11 people are supposed to work on a project. How many distinct four persons team can be chosen such that the following condition is met ""A will work on the team only if B is also working on the team"". If we divide it into cases A and B are fixed, so 9c2 A is not in the team but B is in team so 9c3
so 9c2+9c3=120 Is it correct? Please verify.","['combinations', 'combinatorics', 'discrete-mathematics']"
4788704,Find UMVUE for $1/\lambda^3$ in gamma distribution.,"I was taking a look at old probability qualifiers and this is from one of them: Suppouse $\alpha$ is known in $X\sim Gamma(\alpha, \lambda)$ . Find the UMVUE for $1/\lambda^3$ . It is well known that: $$\mathbb{E}[X^k]=\frac{1}{\lambda^k}\frac{\Gamma(\alpha+k)}{\Gamma(\alpha)}$$ From this, we may find an unbiased estimator given by $X_1^3 \frac{\Gamma(\alpha)}{\Gamma(\alpha+3)} $ . By Lehman-Scheffe we need only find an unbiased estimator which is a function of $\sum X_i$ , because this comes from the exponential family. Rao-Blacwell teaches us how to find such an estimator, namely: $$\mathbb{E}\left[\left.X_1^3 \frac{\Gamma(\alpha)}{\Gamma(\alpha+3)}\right|\sum X_i\right]$$ Let me compute the conditional density function: $$f_{X_1|\sum X_i=y}(x)= \frac{f_{X_1}(x)f_{\sum_{i>1} X_i}(y-x)}{f_{\sum X_i}(y)}=\frac{(\lambda^\alpha/\Gamma(\alpha))  e^{-\lambda x}x^{\alpha-1}(\lambda^{(n-1)\alpha}/\Gamma((n-1)\alpha))  e^{-\lambda (y-x)}(y-x)^{(n-1)\alpha-1}} {(\lambda^{n\alpha}/\Gamma(n\alpha))  e^{-\lambda y}y^{n\alpha-1}}$$ $$\therefore f_{X_1| \sum X_i=y}(x)=\frac{\Gamma(n \alpha)}{\Gamma(\alpha)\Gamma((n-1)\alpha)}\frac{x^{\alpha-1}(y-x)^{(n-1)\alpha-1}}{y^{n\alpha-1}}$$ I believe, but I am not completely sure this enables us to write: $$ \mathbb{E}\left[\left.X_1^3 \frac{\Gamma(\alpha)}{\Gamma(\alpha+3)}\right|\sum X_i\right]=\int_0^\infty x^3 \frac{\Gamma(\alpha)}{\Gamma(\alpha+3)} f_{X_1| \sum X_i=y}(x)dx =$$ $$ \frac{\Gamma(\alpha)}{\Gamma(\alpha+3)}\frac{\Gamma(n \alpha)}{\Gamma(\alpha)\Gamma((n-1)\alpha)}\int_0^y\frac{x^{\alpha+2}(y-x)^{(n-1)\alpha-1}}{y^{n\alpha-1}}dx=$$ $$\frac{1}{\Gamma(\alpha+3)}\frac{\Gamma(n \alpha)}{\Gamma((n-1)\alpha)}\int_0^1\frac{{(y\tilde{x})}^{\alpha+2}(y-(y\tilde{x}))^{(n-1)\alpha-1}}{y^{n\alpha-1}}yd\tilde{x}=$$ $$\frac{(\sum X_i)^3}{\Gamma(\alpha+3)}\frac{\Gamma(n \alpha)}{\Gamma((n-1)\alpha)}\int_0^{1} \tilde{x}^{\alpha+2}(1-\tilde{x})^{(n-1)\alpha-1}d\tilde{x}$$ I am having trouble computing this integral and I am not really not sure if I am on the right track.","['statistics', 'conditional-expectation', 'umvue', 'probability-theory', 'probability']"
4788719,"How to solve this absolute value equation: $|z-3|-|z+6|=0,z\in\mathbb C$","So I was bored, and decided to solve some absolute value equations. After a while, I came up with this $$|z-3|-|z+6|=0,z\in\mathbb C$$ which I thought that I might be able to solve. Here is my attempt at doing so: For any absolute value of a complex number, we have $$|a|\implies\sqrt{a^2}\\|a+bi|\implies\sqrt{a^2+b^2}\quad a,b\in\mathbb R\\\implies|a+bi+c|=\sqrt{a^2+2ac+c^2+b^2}\quad a,b,c\in\mathbb R$$ So we now have the equation $$\sqrt{a^2-6a+9+b^2}-\sqrt{a^2+12a+36+b^2}=0\\\implies a^2-a^2+b^2-b^2-6a-12a+9-36=0\\\implies-27-18a=0\implies18a+27=0\implies a=-\dfrac{27}{18}$$ Now to solve for $b$ . So we have $$\sqrt{\dfrac{729}{324}-\dfrac{162}{18}+9+b^2}=\sqrt{\dfrac{729}{324}+54+b^2}\\\implies\sqrt{\dfrac94+b^2}=\sqrt{\dfrac{225}4+b^2}\\\implies\dfrac94+b^2=\dfrac{225}4+b^2$$ meaning that for no $bi$ will make this statement true, meaning that our solution is $$z=-\dfrac{27}{18}+0i$$ which we can check in Desmos to confirm our answer. My question Is it true that the only solution to $$|z-3|-|z+6|=0,z\in\mathbb C$$ is $z=-\dfrac{27}{18}+0i$ , or what might I have done wrong?","['algebra-precalculus', 'absolute-value', 'complex-numbers']"
4788740,What are the chances that the Enemy/Defender game has a stable solution?,"There is a game called Enemy/Defender that you might play with kids. The setup is as follows: Everyone stands in a circle. You say, ""Look around the circle and select someone (at random) to be your enemy . Now pick a different person to be your defender . When I say 'Go!', you have to position yourself so that your defender is between you and your enemy... Go!"" The players will run around frantically until you stop the game. On rare occasions though, there is a stable configuration that occurs where everyone is standing in a line and doesn't need to move at all. The question is: For a game with n players with enemies and defenders chosen uniformly at random, what is the probability that a stable configuration exists? I'm finding it surprisingly difficult to solve this or even simulate it efficiently. Here's some basic insights: Even if there is a solution where everyone is not standing in a line, we can collapse it to a line without breaking the order, so we only need to consider permutations of the players as solutions. The game needs at least 3 players, but no solution exists for 3 players. This is easy to see because the middle player in any proposed solution must be adjacent to their enemy. For 4 or more players, there is a nonzero chance a solution exists. For any solution, the players at either end of the line cannot be anyone's defenders If everyone chooses the same defender, then a stable solution exists iff you can partition the remaining players into two sets such that everyone's enemy is in the opposite set (and the common defender's enemy and defender are in the same set). Some results from brute force simulation: 4 players :  8.3366% of 10000000 games 5 players : 12.2495% of 1000000 games 6 players : 16.6140% of 100000 games 7 players : 21.5500% of 10000 games 8 players : 28.1000% of 1000 games 9 players : 39.0000% of 100 games I would be very surprised if a succinct closed form solution exists, but even some insight on how to simulate it faster would be appreciated.","['order-theory', 'combinatorics', 'algorithms', 'recreational-mathematics', 'computer-science']"
4788843,"What's more likely to show up first when rolling a die, 556 or 234?","What's more likely to show up first when rolling a $6$ -sided die, $556$ or $234$ ? The expected number of rolls of getting $556$ is solving for $a$ in the following equations: $$a = {5\over6}(a + 1) + {1\over6}(b + 1)$$ $$b = {5\over6}(a + 1) + {1\over6}(c + 1)$$ $$c = {2\over3}(a + 1) + {1\over6}(c + 1) + {1\over6}$$ Solving, we get $a = 216$ , $b = 210$ , $c = 174$ , so the expected number of rolls it takes to get $556$ is $216$ . The expected number of rolls is getting $234$ is solving for $x$ in the following equations: $$x = {5\over6}(x + 1) + {1\over6}(y + 1)$$ $$y = {2\over3}(x + 1) + {1\over6}(y + 1) + {1\over6}(z + 1)$$ $$z = {2\over3}(x + 1) + {1\over6}(y + 1) + {1\over6}$$ Solving, we get $x = 216$ , $y = 210$ , $z = 180$ , so the expected number of rolls it takes to get $234$ is also $216$ . So I have two questions: Given that the expected number of rolls for getting $556$ and $234$ is $216$ for both, does it follow that they're equally likely to show up first when rolling a $6$ -sided die? If not, which one is more likely to show up first? And what's the calculation that would demonstrate this? Without writing out the Markov chain and calculating, on an intuitive level, how do we see which one is more likely to show up first (or that both of them are equally likely to show up first)?","['markov-chains', 'expected-value', 'dice', 'combinatorics', 'probability']"
4788870,How to define length geometrically,"My boss claims that distance in $\Bbb{R}^n$ between two vectors, $a=[a_1, a_2, \dots a_n]$ and $b=[b_1,b_2,\dots b_n]$ is defined as: $$d = \sqrt{\sum (b_i-a_i)^2}$$ I claim that no, under a geometric treatment one only needs to define length in one dimension. From there, one can apply Pythagoras theorem to get derive the Euclidean distance in $\Bbb{R}^2$ and inductively from there to $\Bbb{R}^n$ . My question is two-fold: Is it true that Euclidean distance in $\Bbb{R}^n$ must be defined and can't be derived once we define it in $\Bbb{R}^1$ and just apply Pythagoras theorem repeatedly (and the definition of $\Bbb{R}^n$ as the space spanned by orthogonal vectors)? In modern geometry, how is distance/ length in $\Bbb{R}^1$ defined? I understand everything is built on Euclid's five postulates. The first postulate talks about being able to draw a line segment between two points. But I see nothing defining length in $\Bbb{R}^1$ . Also, is there a textbook on Geometry that covers these concepts?","['euclidean-geometry', 'geometry']"
4788874,Showing uniqueness of solution to a non-linear Poisson problem,"I'm trying to prove that a non-linear Poisson problem has a unique solution. The context is the following: Let $\Omega \subset \mathbb{R}^n$ be a bounded open subset of class $C^2$ . Consider the following non-linear problem: $$\begin{cases} &(\Delta u)(x_0) = f(x_0, u(x_0)), \text{ for every } x_0 \in \Omega \\ &u(x_0) = \varphi(x_0), \text{ for every $x_0 \in \partial \Omega$}\end{cases}$$ where $f(\bullet, u) \in C^{0, \gamma}(\overline{\Omega})$ , $f(x, \bullet) \in C^1(\mathbb{R})$ and $f$ is non decreasing in $u$ , i.e $\displaystyle{\frac{\partial f}{\partial u}(x) \geq 0}$ for every $x \in \overline{\Omega}$ . Prove that this problem has at most one solution in $C^2(\Omega) \cap C(\overline{\Omega})$ . If $u, v$ are both solutions of the problem, ideally, I'd like to use the maximum principle for $w = u -v$ . Since the problem is non-linear, the usual maximum principle doesn't apply, so I need to come up with another linear problem that $w$ satisfies on which the maximum principle can be used. I think the non-negativity of the derivative of $f$ will play an essential role, but I haven't been able to come up with anything helpful. Any help will be greatly appreciated. Thanks in advance!","['elliptic-equations', 'poissons-equation', 'analysis', 'maximum-principle', 'partial-differential-equations']"
4788880,Injective function from $\mathcal{P}(\mathbb{N})$ to $\operatorname{Eq}(\mathbb{N})$,"I am asked to show that $|\mathcal{P}(\mathbb{N})| \leq |\operatorname{Eq}(\mathbb{N})|$ by showing there exists an injective function $f:\mathcal{P}(\mathbb{N})\to \operatorname{Eq}(\mathbb{N})$ , where $\operatorname{Eq}(\mathbb{N})$ are the equivalence relations on $\mathbb{N}$ , or equivalently the partitions of $\mathbb{N}$ . My intuition says $f:\mathcal{P}(\mathbb{N})\to \operatorname{Eq}(\mathbb{N}):A\mapsto \{A \cup(\mathbb{N} \setminus A)\}$ should work but I don’t know how even start a proof. If I’m correct, how can I prove it? If not, what function would work?","['elementary-set-theory', 'discrete-mathematics']"
4788881,"Arranging 5 red balls, 5 green balls, 5 blue balls so that no two balls of the same color are adjacent","There are 5 red balls, 5 green balls, and 5 blue balls.
How many ways are there to arrange them in a straight line so that no two balls of the same color are adjacent? Here is what I have tried so far:
I ignored the colors of the blue and the green balls and just treated them as the same colored balls (say black).
Then, for the red balls to be apart from each other, there are 11C5 ways. After that, I am stuck on what I should do for the green and blue balls.",['combinatorics']
4788893,the probability of that the winning combinations have at least two consecutive numbers,"suppose that we have a game in which there are 40 balls numbered 1 to 40 and six are drawn without replacement to determine the winning
combination.we want to find the probability of  that the winning combinations have at least two consecutive numbers my attempt: using numbers from 1 to 40,the number of  the possible pairs are ${40 \choose 2}$ ,from those pairs there is 39 pairs of consecutive numbers. now let's take $B$ the set of all pairs of  consecutive numbers ,i.e B is the union of those 39 pairs. $B=A_{1,2} \cup A_{2,3}\cup ......\cup A_{38,39} \cup A_{39,40}$ ( $A_{i,j}$ is the pair $(i,j)$ where i and j are consecutive numbers) let's calculate the probability of the event $B$ using ""The Union of a Finite Number of Events"" theorem $P(B=A_{1,2} \cup A_{2,3}\cup ......\cup A_{38,39} \cup A_{39,40})=$ $\sum_{i=1}^{39}P(A_{i,i+1})-\sum_{}^{}P(A_{i,i+1} \cap A_{j,j+1})+\sum_{}^{}P(A_{i,i+1} \cap A_{j,j+1}\cap A_{w,w+1})+_-.....-P(A_{1,2} \cap A_{2,3}\cap .....\cap A_{39,40})$ the first term $\sum_{i=1}^{39}P(A_{i,i+1})$ :the probability of each pair is just $\frac{1}{{40 \choose 2}}$ ,and we have 39 pairs so the first term equal $\frac{39}{{40 \choose 2}}$ the Second term $\sum_{}^{}P(A_{i,i+1} \cap A_{j,j+1})=P(A_{1,2} \cap A_{2,3})+...$ : here in each term the first pair have the propability of $\frac{1}{{40 \choose 2}}$ and the second pair have the propability of $\frac{1}{{40 \choose 2}-1}$ and we have ${39 \choose 2}$ terms,so the Second term equal $\frac{{39 \choose 2}}{({40 \choose 2}-1){40 \choose 2}}$ . i know that the attempt is not applicable because of such large amount of terms to deal with .anyway i have two questions:does  my approach is Right?,what is other approach to deal with this problem?.(i  started studying probability  in this  week  so please don't use advanced stuff )","['statistics', 'combinatorics', 'probability-theory', 'probability']"
4788901,Verification of a proof for supremum and infimum of the set $A=${$(n^3+1)/(n^4+16):n\in\mathbb{N}$},"I came across an example while studying which asks us to solve (and prove by definition), the supremum and infimum of the following set: A = { $\frac{n^3+1}{n^4+16}$ : $n = 1,2,3...$ } Here is my approach. I would like to know if this works or if I'm getting lost in the objective to prove. I start by looking for the $\inf(A)$ . Clearly the infimum is $0$ due to the dominating exponent in the denominator. So by the definition, I need to show; $0$ is a lower bound for $A$ $\forall $ $ \epsilon>0$ , $0 + \epsilon$ is not a lower bound for A I begin with a rough proof to find the logic I need to follow for 1. So, show: \begin{align}
0 \leq \frac{n^3+1}{n^4+16} \\
0 \leq n^3+1\\
0 \leq n+1 \leq n^3+1\\
-1 \leq n
\end{align} Which then lets me start the actual proof: By the Archimedean property, $\exists$ $n \in \mathbb{N}$ s.t. $n \geq -1$ \begin{align}
n+1 \geq 0\\
n^3+1 \geq n+1 \geq 0\\
\frac{n^3+1}{n^3+16} \geq 0
\end{align} So $0$ is a lower bound for A Secondly, we need to show that $\epsilon + 0$ is not a lower bound for A. So, starting with a rough proof, I need to show that there exists some $n$ s.t \begin{align}
\frac{n^3+1}{n^4+16} \leq \epsilon\\
\frac{n^3+1}{n^4+16} \leq \frac{n^3}{n^4+16}+1 \leq \epsilon\\
\frac{n^3}{n^4+16} \leq \epsilon -1\\
\frac{n^3}{n^4+16} \leq \frac{n^3}{n^4} \leq \epsilon -1\\
\frac{1}{n} \leq \epsilon -1\\
n \geq \frac{1}{\epsilon -1}
\end{align} And with this, I worked backward to get the implication I needed (starting with the A.P as stated above). I won't write out my entire work for the supA, but it follows the same sort of method I used here. I want to make sure these step are coherent, and is there anything I can do to get to the desired values quicker as this is quite a tedious process to do explicitly. Thanks! Edit: In my supremum proof I've found an inequality I can't seem to manipulate. After proving 28/97 is an upper bound (through inspection of the sequence, n=3 yields the largest value). In the second step of showing that $\frac{28}{97} -\epsilon$ is not an upper bound, I begin like usual (showing that there exists an n s.t.): \begin{align}
\frac{n^3+1}{n^4+16} \geq \frac{28}{97} - \epsilon\\
\frac{n^3+1}{n^4+16} \geq \frac{n^3}{n^4+16} \geq \frac{28}{97} - \epsilon
\end{align} And I seem to be stuck. No matter the manipulation I can think of, I can't get myself to a form of $n \geq f(\epsilon)$ to use the Archimedean property. Any hints?","['solution-verification', 'supremum-and-infimum', 'analysis', 'real-analysis']"
4788970,"The continuity of $g\circ h$ and $h$, and $h$ surjective, imply the continuity of $g$?","Structural assumptions: $X$ is a nontrivial connected separable topological space Given: $f(x)=g(h(x))$ . $f:X\to\mathbb R$ is continuous $h:X\to\mathbb R$ is continuous and $h(X)=\mathbb R$ . $g:\mathbb R\to\mathbb R$ . Question: Can we say $g$ is also continuous? Motivation: Composition of continuous functions is also continuous. Does an ""inverse"" argument holds? After doing some research the argument seems to be correct for metric spaces with additional assumptions of one of the function being homeomorphism: If the composition of two functions is continuous and one of those functions is continuous, is the other function continuous as well? Note: superscripts in this proof are indexes not exponents. My test: I think if we let $X$ be path-connected, it is easy to prove that $g$ is continuous. Let $y_1<y_2$ be two real numbers and $h(x_i)=y_i$ , for any real number $y\in (y_1,y_2)$ , consider a sequence $y^n\in (y_1,y_2)$ converging to $y$ . We will show that $g$ is continuous at arbitrary real number $y$ . Let $T$ be the path between $x_1,x_2$ . It follows from the continuity of $h$ that for each $n$ there exists $x^n\in T$ such that $h(x^n)=y$ . Because of path-connectedness, path $T$ is compact. So sequence $x^n$ converges to $x$ where $f(x)=y$ . By the continuity of $f$ , it follows that $f(x^n)$ converges to $f(x)$ and $g(h(x^n))$ converges to $g(h(x))$ . Finally, we conclude that $h(x^n)\to h(x)$ $\implies g(h(x^n))\to g(h(x))$ . However, I think this method only works for path-connected $X$ not connected $X$ . Could you please give me some hints or a counterexample?","['connectedness', 'function-and-relation-composition', 'real-analysis', 'continuity', 'general-topology']"
4788980,Triviality of the Euler totient of a non-cyclic Abelian group,"Recall that the Euler totient $\phi(n)$ satisfies the identity $$\phi(n) = \sum_{d|n} \frac{n}{d}\mu(d) = n\sum_{d|n}\frac{\mu(d)}{d}.$$ This computation can be interpreted as computing a certain invariant of the cyclic group $\mathbb{Z}/n$ as follows. Given a finite group $G$ , let $\mu(G)$ be the Mobius function defined inductively by $\mu(1) = 1$ and for $G\neq 1$ $$\mu(G) = -\sum_{H<G} \mu(H).$$ Then we can define a generalized Euler totient $$\phi(G) := \sum_{H\leqslant G} [G:H]\mu(H) = |G|\sum_{H\leqslant G}\frac{\mu(H)}{|H|},$$ so that $\phi(\mathbb{Z}/n) = \phi(n)$ . This function came up while I was doing some computations in the incidence algebra of the lattice of subgroups of $G$ . I've seen a few papers that consider generalized Euler totients defined on finite groups, but they all seem to consider definitions that are not equivalent to the one given above. Which is actually pretty reasonable, since it turns out the above definition is trivial! Proposition. For any non-cyclic finite Abelian group $G$ , $\phi(G) = 0$ . It's not terribly difficult to prove this assertion. By multiplicativity it suffices to consider the case where $G$ is a non-cyclic $p$ -group. By standard properties of the Mobius function, $\mu(H) = 0$ unless $H$ is contained in the socle of $G$ . So it suffices to show the result for $G = (C_p)^n$ with $n>1$ . Using Hall's formula $$\mu((C_p)^n) = (-1)^n p^{\binom{n}{2}}$$ and basic counting we obtain the formula $$\phi((C_p)^n) = p^n \sum_{k=0}^n (-1)^{k}p^{\binom{k}{2}}p^{-k} \binom{n}{k}_p$$ where $\binom{n}{k}_p$ is the number of $k$ -dimensional subspaces in $\mathbb{F}_p^n$ . Then applying the $q$ -binomial theorem we can simplify this expression to $$\phi((C_p)^n) = p^n\prod_{k=0}^{n-1} (1-p^{k-1})$$ which clearly vanishes if $n>1$ due to the $k=1$ term. Question: The computational proof outlined above doesn't really explain why the proposition is true, but presumably the result should be a reflection of some simple structural property of non-cyclic Abelian groups. Is there a more conceptual proof of this result, or at least some intuition that shows how one might have predicted the result a priori?","['abelian-groups', 'combinatorics']"
4789039,"Odd-sounding ""everyday"" applications of the Handshaking Lemma","First of all, a convention. Let $(V,E)$ be a graph with degree function $\operatorname{deg}:V\to\mathbb{N}_0$ . There are three results which get referred to as the Handshaking Lemma $\sum_{v\in V}\operatorname{deg}(v)=2|E|$ , $\sum_{v\in V}\operatorname{deg}(v)$ is even, the number of nodes of odd degree is even. I am using the third one (of course we can get to the third from the other two, readily). I am teaching some graph theory and I had it in my head that in my undergraduate there was an odd-sounding ""everyday"" application of the Handshaking lemma. In class I gave the following. Let $V$ be a set of people and draw an edge between siblings ( $x$ and $y$ are siblings if $x\neq y$ and the set of parents of $x$ is equal to the set of parents of $y$ ). The handshaking lemma implies that the number of people in $V$ who have an odd number of siblings in $V$ is even. This was a bit underwhelming in class because the people in $V$ with an odd number of siblings, well, look at $x\in V$ and its set of neighbours $N(x)$ ... well $|\{x\}\cup N(x)|$ must be even and the thing falls out that way too... maybe I undersold this, I don't know. Well anyway, I rustled up my old undergraduate notes and lo and behold the great confounding example that made my eyes light up was just this one... (there was another example but it was actually a non-example: Must the number of families with an odd number of children be even? ... this is not true... unless there is some weird definition of family). So now I am wondering are there any particularly odd or ""cool"" ""everyday"" applications of the handshaking lemma to win back these students? Question: What are some odd/weird/cool/strong/counter-intuitive applications of the handshaking lemma that could be stated to the person on the street? Are there any?","['graph-theory', 'big-list', 'intuition', 'discrete-mathematics', 'recreational-mathematics']"
4789072,Minimizing surface area of a fixed volume pyramid,"I assume you're familiar with the classic ""find the dimensions of an <X> containing 1 liter with minimum surface area"". I remember solving this in school at least for a <cylinder> and a <square prism> and found the rule ""make square cross sections and it becomes minimal surface area"" to be satisfying but kind of boring. Now that I'm teaching the stuff myself, I took up the challenge to solve for the cases of <cone> and <rectangular pyramid> as they don't usually have square (orthogonal) cross sections – and I got myself stuck on the pyramid. This is how I started: rectangular base area: $B = 2a \cdot 2b$ (note that I defined $a$ and $b$ as half of the respective base dimensions) pyramid height: $h$ slanted heights: $h_a = \sqrt{b^2 + h^2}$ , $h_b = \sqrt{a^2 + h^2}$ slanted areas: $S = 2 \cdot (a \cdot h_a + b \cdot h_b)$ let the unit be 1 cm , so volume $V = \frac{1}{3} \cdot B \cdot h = 1000$ surface area $A = B + S = 4ab + 2 \cdot (a \cdot h_a + b \cdot h_b)$ I then defined $r := \frac{b}{a} \Rightarrow b = ra$ and $k := \frac{750}{r}$ , so: $V = \frac{1}{3} \cdot 4ra^2 \cdot h = 1000 \iff h = \frac{750}{ra^2} = ka^{-2}$ $\Rightarrow h' = -2 \cdot ka^{-3}$ ; $h^2 = k^2a^{-4}$ , $(h^2)' = -4 \cdot k^2a^{-5}$ $h_a = \sqrt{r^2a^2 + h^2}$ , $h_b = \sqrt{a^2 + h^2}$ So I'm pretty sure I arrived at a correct representation of the surface area: $$A(a) = 4ra^2 + 2a \cdot \left( \sqrt{r^2a^2 + h^2} + r \cdot \sqrt{a^2 + h^2} \right)$$ $$A'(a) = 8ra + 2 \cdot \left( \sqrt{r^2a^2 + h^2} + r \cdot \sqrt{a^2 + h^2} \right) + 2a \cdot \left( \frac{2r^2a + (h^2)'}{2\sqrt{r^2a^2 + h^2}} + r \cdot \frac{2a + (h^2)'}{2\sqrt{a^2 + h^2}} \right) \stackrel{!}{=} 0$$ $$4ra + \sqrt{r^2a^2 + h^2} + r \cdot \sqrt{a^2 + h^2} + \frac{2r^2a^2 + a(h^2)'}{2\sqrt{r^2a^2 + h^2}} + r \cdot \frac{2a^2 + a(h^2)'}{2\sqrt{a^2 + h^2}} = 0$$ $$\dots$$ A few lines later, I derived this: $$ 4ra^3 + \frac{2r^2a^6 - k^2}{\sqrt{r^2a^6 + k^2}} + r \cdot \frac{2a^6 - k^2}{\sqrt{a^6 + k^2}}= 0 \text{, with constants } r, k \in \mathbb{R}^{\gt 0}$$ And frankly, the only step forward I can see is to choose a concrete value for the ""aspect ratio"" $r$ and continue with a fixed shape that way. Is there anything else I could do to solve this monstrosity for $a$ ? ~LDer","['optimization', 'calculus', 'geometry']"
4789130,$n+1$ and $n \phi (n) + 1$ are both perfect squares if and only if $n$ is a product of twin primes?,"I'm trying to prove the following conjecture concerning twin primes and Euler's totient function, which I have verified for $n$ up to 1 billion. For all $n \in \mathbb{N}$ , $n+1$ and $n \phi (n) + 1$ are both
perfect squares if and only if $n$ is a product of twin primes. If is easy. Suppose $n = pq$ for primes $p$ and $q$ with $q=p+2$ . Then \begin{eqnarray}
n+1 &=& pq + 1\\
&=& p(p+2) + 1\\
&=& (p+1)^2 
\end{eqnarray} and \begin{eqnarray}
n \phi (n) + 1 &=& pq(p-1)(q-1) + 1\\
&=& p(p+2)(p-1)(p+1) + 1\\
&=& p^4 + 2p^3 - p^2 - 2p + 1\\
&=& (p^2 + p -1)^2
\end{eqnarray} I'm struggling with only if. We know from $n = x^2 -1 = (x+1)(x-1)$ that $n$ must have a factor pair separated by 2. Based on the  nature of the totient function I tried considering different cases based on the structure of the prime factorization. In the case that $n$ is a semiprime we are done as the two factors must be twin primes, however, in the the case $n=pqr$ , I get stuck.  We have wlog that $pq = r +2$ and we need to show that \begin{eqnarray}
n \phi (n) + 1 &=& pqr(p-1)(q-1)(r-1) + 1\\
&=& pq(pq-2)(p-1)(q-1)(pq-3) + 1 \\
&=& p^{4}q^{4}-p^{4}q^{3}-p^{3}q^{4}-4p^{3}q^{3}+5p^{3}q^{2}+5p^{2}q^{3}+p^{2}q^{2}-6p^{2}q-6pq^{2}+6pq
\end{eqnarray} cannot be a square. I tried evaluating the expression modulo small primes but it didn't appear useful. Is there a better approach to solving this case? Is there a better way to approach the conjecture in general?","['number-theory', 'twin-primes', 'totient-function']"
4789132,James's theorem in incomplete normed spaces,"One version of James's theorem in functional analysis states the following. A Banach space is reflexive if, and only if, every bounded linear functional attains its norm on the unit sphere. For example if $X$ is a reflexive Banach space, then by Banach Alaoglu the unit sphere is weakly compact, so every bounded linear functional attains its supremum. If $X$ is irreflexive, then James's theorem states there exists a bounded linear functional that does not attain its norm on the unit sphere, which is significantly harder to show. My question is whether this theorem still holds when we drop the completeness requirement. So is a general normed space reflexive if, and only if, every bounded linear functional attains its norm on the unit sphere? Since incomplete spaces are never reflexive, the question is really whether every incomplete normed space admits a bounded linear functional that does not attain its norm on the unit sphere. My first thought is to look at the completion $\overline{X}$ of an incomplete normed space $X$ . If the completion is irreflexive then James's theorem already suffices, so we may assume $\overline{X}$ to be reflexive. Then for any vector $x$ on the unit sphere of $\overline{X}$ that is not in $X$ the Hahn Banach separation theorem gives a bounded linear functional $\phi$ which attains its norm at $x$ . If $\overline{X}$ is strictly convex, then $\phi$ can not attain its norm anywhere else, so it does not attain its norm on the unit sphere of $X$ . However I am lost at what to do if $\overline{X}$ is reflexive but not strictly convex. I have tried to use Krein Milman to ensure $x$ is an extreme point of the unit sphere, but then I can not ensure that $x$ is not in $X$ and it does not even really solve the strict convexity problem, because an extreme point can still be an endpoint of a line segment in the unit sphere. Side note: If $X$ is strictly convex, does it follow that $\overline{X}$ is also strictly convex?","['normed-spaces', 'functional-analysis', 'reflexive-space']"
4789136,Probability of $\lim \sup_{n \rightarrow \infty}$ of function sequence.,"I might just be misunderstanding the notation here but I‘m supposed to show that for $(X_n)$ a sequence of i.i.d. uniformly distributed random variables on $[0,1]$ we have that: $$
\lim \sup_{n \rightarrow \infty} \frac{1}{\ln(n)}\ln(\frac{1}{X_n}) = 1
$$ almost surely. For me this means that I need to show: $$
\mathbb{P}(\{\omega \in \Omega : \lim \sup_{n \rightarrow \infty} \frac{1}{\ln(n)}\ln(\frac{1}{X_n(\omega)}) = 1\}) = 1
$$ Where the $\lim \sup_{n \rightarrow \infty}$ is taken pointwise. I‘m supposed to show this using the Borel-Cantelli lemmas but these lemmas only allow me to make statements about the following probability: $$ \mathbb{P}(\lim \sup_{n \rightarrow \infty} \{\omega \in \Omega : \frac{1}{\ln(n)}\ln(\frac{1}{X_n(\omega)}) = 1\}) = 1
$$ But these two sets aren’t equal, are they? Am I just understanding the exercise incorrectly and do I actually need to show the second statement or am I misunderstanding something?","['measure-theory', 'probability-theory', 'probability']"
4789143,Conjecture: If $f(x)=\prod\limits_{k=1}^n(x-k)$ then $\lim\limits_{n\to\infty}\frac1n [\text{largest root of $f(x)=f'(x)$}]=\frac{e}{e-1}$.,"I was thinking about the polynomial $f(x)=\prod\limits_{k=1}^n(x-k)$ . I noticed that if we draw the graphs of $y=f(x)$ and $y=f'(x)$ together, the $x$ -coordinate of the rightmost intersection seems always to be approximately $1.6n$ . Further numerical investigation suggests that the largest root of $f(x)=f'(x)$ approaches approximately $1.58198n$ as $n\to\infty$ . I conjecture that $\lim\limits_{n\to\infty}\frac1n [\text{largest root of $f(x)=f'(x)$}]=\frac{e}{e-1}=1.5819767...$ . Is my conjecture true? The coefficients in the expansion of $f(x)$ , in decreasing powers of $x$ , are found in the sequence of Stirling numbers of the first kind . The coefficients in the expansion of $f'(x)$ , in decreasing power of $x$ , are given by A196837 . Other than that, I don't know how to approach approximating the largest root of $f(x)=f'(x)$ .","['conjectures', 'roots', 'polynomials', 'derivatives', 'stirling-numbers']"
4789167,Over which rings is matrix rank defined?,"I'm investigating $m \times n$ matrices over $R$ , where $R$ is a finite commutative unitary ring, and more specifically a finite chain ring or a principal ideal ring. Define the row rank as the maximal number of lin. independent rows, the column rank in a similar manner, and the inner rank. My question is, are there necessary and sufficient conditions for $R$ such that the row rank is equal to the column rank, so that the term matrix rank is well-defined? I know that the two ranks coincide, for example, over a PID, and in this case they also coincide with other notions of rank I have encountered, such as the McCoy rank and the determinantal rank, but in general I don't know when I have the right to speak about the rank of a matrix. A reference would be welcome.","['matrix-rank', 'matrices', 'ring-theory', 'abstract-algebra', 'linear-algebra']"
4789175,Terminology for equivalence between a multivariable function and a function that maps to a function,"There is a bijection between the functions that map to functions, i.e. of the form $f: A \to M$ where $M$ is the set of all functions $g:B \to C$ , and the functions of the Cartesian product, i.e. $h: A \times B \to C$ . Concretely, for any such $h$ , there exists $f$ such that: $$(f(a))(b) = h(a, b).$$ In other words, every function that maps to functions can be reduced to a simple function from a Cartesian product, which is a function of multiple variables and every function of multiple variables can be reduced into a chain of functions of just one variable. This property is often implicitly used, but I have never seen someone writing it down like this. Does this property have a name and if yes, what is it?","['functions', 'terminology']"
4789200,Asymptotic formula for ratio of double factorials,"I am interested in getting an asymptotic formula for $$
\frac{(2n-1)!!} {(2n)!!}.
$$ After some trial and error, I think the asymptotic can be of the form $\frac{1}{\sqrt{\alpha n + \beta}}.$ Use Stirling's formula, I got $\alpha = \pi$ Using software, I was able to get $\beta = \frac{\pi}{4}$ , but I have no idea how to prove this. So my question is: how do I show that $$
\lim_{n \to \infty} \left(\left(\frac{(2n)!!}{(2n-1)!!}\right)^{2} - \pi n\right) = \frac{\pi}{4}
$$ ?","['limits', 'calculus', 'factorial']"
4789202,"Notation $S\twoheadrightarrow \,\,\,\stackrel{S}{}\!\!\unicode{x2215}_{\!\unicode{x202f}\sim} \hookrightarrow T$ in Group Theory","I am learning group theory now. My professor wrote down some notation that I was not able to understand. Can you tell me the meanings? $$S\twoheadrightarrow 
\,\,\,\stackrel{S}{}\!\!\unicode{x2215}_{\!\unicode{x202f}\sim}
\hookrightarrow T$$ $$a\mapsto [a]\mapsto f(a)$$","['notation', 'group-theory', 'abstract-algebra']"
4789233,Volume of unit sphere using a single integral,"I was just doing this problem for fun, but I seem to be incorrect, and I cannot wrap my head around why. Consider the unit sphere centered at $(0, 0, 0)$ . When trying to compute the volume, we can get the volume of the top half, $z \in (0, 1)$ , then double it due to symmetry. For each $z \in (0, 1)$ , volume of a slice of the sphere is $\pi r^2 dz$ where $r$ is the radius at that slice. We compute $r = 1 - z$ . For example, at $z = 1$ , the radius $r = 0$ , at $z = 0.5, r = 0.5$ . By this, we can write an integral for the top half of the sphere. $$V = 2\int_0^1(1 - z)^2\pi dz = \frac{2\pi}{3} \neq \frac{4\pi}{3}$$ EDIT: I actually think $r = 1 - z$ is incorrect. Would it be $r = \sqrt{(1 - z)^2}$ ?","['calculus', 'geometry']"
4789253,When Is it possible to define a ''Integral surface''?,"An integral curve $c(t)$ of a vector field $V$ on a manifold $M$ is defined as the solution of $\frac{dc(t)}{t}=V(c(t))$ , here we require that $t\in \mathbb{R}$ . Is it possible to define an integral surface $s$ of two vector fields $V_1,V_2$ ,  i.e.  a map $\mathbb{R}^2 \supset \Delta \longrightarrow M$ , $t=(t_1,t_2)\mapsto s(t_1,t_2)$ , such that $\frac{\partial s}{\partial t_1}=V_1(s)$ and $\frac{\partial s}{\partial t_2}=V_2(s)$ ? I tried to define $s(t_1,t_2)$ by firstly fixing $t_2$ ,  we go from the initial point $p=s(0,0)$ through the integral curve of $V_1$ for time $t_1$ , the $s(t_1,0)$ is defined as the point where we are now, then fix $t_1$ and go through the integral curve of $V_2$ for time $t_2$ and define $s(t_1,t_2)$ as the point where we are now. But I know this is wrong since if we instead go through integral curve of $V_2$ first, we will get another point if the Lie bracket $[V_1,V_2]\neq 0$ . So when is this system of PDE ( $\frac{\partial s}{\partial t_1}=V_1(s)$ , $\frac{\partial s}{\partial t_2}=V_2(s)$ ) solvable, is it solvable only if $[V_1,V_2]=0$ ? I don't much knowledge in PDE theory so I apologize if this question is too naive.","['partial-differential-equations', 'differential-geometry']"
4789257,Prove that $\lim_{x\to 0^+} 1/x^a \int_0^x t^{a-1} f(t) dt$ exists if and only if $\lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt$,"Suppose $f:[0,1]\to\mathbb{R}$ is a function that is continuous on $(0,1].$ Let $a,b>0$ . Prove that $\lim_{x\to 0^+} 1/x^a \int_0^x t^{a-1} f(t) dt$ exists if and only if $\lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt$ exists. It seems useful to define $F(x) = \int_0^x t^{a-1} f(t) dt$ . Then $t^{b-1} f(t) = t^{b-a} F'(t)$ . The idea is now to integrate by parts. Clearly by symmetry it is enough to show that if $\lim_{x\to 0^+} 1/x^a F(x)$ exists, then $\lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt$ exists. We see that $$\lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt = \lim_{x\to 0^+} 1/x^b \int_0^x t^{b-a} F'(t) dt = \lim_{x\to 0^+} x^{-b} \left[(t^{b-a} F(t))_0^x - (b-a) \int_0^x t^{b-a-1} F(t) dt\right] = \left[\lim_{x\to0^+} 1/x^a F(x) - 1/x^b (b-a) \int_0^x t^{b-a-1} F(t) dt\right].$$ Of course, even though $f$ is continuous on $(0,1]$ , it may not be bounded; take $f(x)=1/x$ for $x>0$ and $f(x)=1$ for $x=0$ . I'm not sure if one can place a bound on $F(x)$ . From above, we just need to show that $- 1/x^b (b-a)\int_0^x t^{b-a-1} F(t) dt$ exists to conclude the proof.","['integration', 'continuity', 'calculus', 'contest-math']"
4789268,Seeking a More Efficient Solution for Knights and Knaves Logic Puzzle,"I am working on a Knights and Knaves logic puzzle, and I have formulated a solution using a truth table. However, I'm wondering if there's a more efficient or faster way to arrive at the solution without having to manually create a large truth table, especially in cases with many statements. The puzzle I'm working on involves five locals, A, B, C, D, and E, each making specific statements. The rules are as follows: Knights always tell the truth.
Knaves always lie.
The statements from the locals are as follows: A: ""D is a knight, or E is a knave."" B: ""Me and E are different."" C: ""I am a knight or E is a knave."" D: ""A is a knight or E is a knight."" E: ""B is a knave or I am a knight."" My current solution involved creating a truth table with 32 combinations, but I'm looking for a more efficient approach to determine which locals are knights and which are knaves. After my lengthy process, I concluded that everyone except E is a knight. Is there a more efficient way to solve this puzzle without the need for a truth table? If so, I would appreciate any guidance or insights on how to approach such problems more effectively.","['propositional-calculus', 'puzzle', 'proof-writing', 'logic', 'discrete-mathematics']"
4789274,Prove that $N_n = n(n-1)/2$.,"Let $n\ge 2, n\in\mathbb{Z}$ and define $N_n$ to be the number of $n$ -tuples $(a_1,\cdots, a_n)$ of positive integers such that $(a_1 ! - 1)\cdots (a_n ! - 1)-16$ is a perfect square.  Prove that $N_n = n(n-1)/2$ . Let $(a_1,\cdots, a_n)$ be an n-tuple of positive integers such that $e_n := (a_1!-1)\cdots (a_n! - 1)-16$ is a perfect square. I think it might be possible to obtain some upper bound on the $a_i$ 's, but I'm not sure how to do so. For example, perhaps each $a_i < 10$ , since otherwise $(a_1 ! - 1)\cdots (a_n ! - 1)-16$ might be congruent to some residue modulo some modulus that is known to not be a residue of a perfect square (we'll call the latter a quadratic residue for convenience, even though some definitions of quadratic residues exclude 0). The quadratic residues modulo 5 for instance are $0,1,4$ . The quadratic residues modulo 8 are $0,1,4$ . Obviously each $a_i > 1$ as $e_{n}$ must be positive. Any $a_i \ge 4$ satisfies that $a_i! - 1\equiv 7\mod 8$ . Thus the possible residues modulo 8 of $e_n$ are $7,1,3,5$ . In order to achieve the residue 1, the number of indices i with $a_i\ge 4$ must differ by an even number from the number of indices $i$ with $a_i= 3$ . It could be useful to consider other moduli such as 16, 12, 9, etc.","['contest-math', 'modular-arithmetic', 'elementary-number-theory', 'square-numbers', 'combinatorics']"
4789313,"2 right-angled Triangles with sides $a,b,c$ and $1/a,1/b,1/c$. The first has twice the area of the second, what is the perimeter of the first?","The question in full: The triangle T has side lengths $a$ , $b$ and $c$ , whilst triangle U has side lengths $1/a$ , $1/b$ , and $1/c$ . Both triangles are right-angled and T has twice the area of U . What is the perimeter of T ? I can calculate what $b$ is, but cannot get any further. Any help would be appreciated. How I calculate $b$ : If we assume that $a$ is the shortest side of T , and that $b$ is the other side of the right-angle. Then $c$ must be the hypotenuse, and therefore longest side. Therefore, on U , $\dfrac{1}{c}$ must now be the shortest side, $\dfrac{1}{a}$ is the longest and $\dfrac{1}{b}$ is the ""middle"". Lets call A the opposite angle of $a$ and F the opposite of $\dfrac{1}{c}$ . Using sohcahtoa we can then deduce that $$\sin{A} = \dfrac{a}{c}$$ and $$\sin {F} = \frac{\dfrac{1}{c}}{\dfrac{1}{a}} = \frac{a}{c}$$ therefore the triangles are similar. Also because we are given that $$A(T) = 2\cdot A(U)$$ we know that $a=\frac{1}{c}\sqrt{2}$ and $b=\frac{1}{b}\sqrt{2}$ . the latter then allows us to calculate $b$ , which is is the 4th root of 2. Other worthy things to note: Using $$\text{Area of triangle} = \dfrac{1}{2} \:(\text{base} \times \text{height})$$ we get $$A(T)= \frac{1}{2}(ab)$$ and $$A(U) = \frac{1}{2bc}$$ therefore $$b^2 = \frac{2}{ac}$$ Also, since the triangles are similar, the ratio of $a/b$ must be the same as $(1/c) / (1/b)$ , thus $b^2 = ac$ .  Combining these gives $ac = 2 / ac$ . I have a feeling that I need to use the sine rule or cosine rule to get further, but can't quite get something that is solvable (other than $(a^4)/2 + (a^2)/\sqrt{2} = 1$ )","['triangles', 'trigonometry', 'geometry']"
4789416,Derivation or intuition on the covariant derivative for higher rank tensors,So the derivation in my textbook for the covariant derivative of a vector field $\vec{u}$ in curvilinear coordinates $\xi^k$ is the following: $$\frac{\partial \vec{u}}{\partial\xi^j}=\frac{\partial (u^i\vec{a}_i)}{\partial\xi^j}=\frac{\partial u^i}{\partial\xi^j}\vec{a}_i+\frac{\partial \vec{a}_i}{\partial\xi^j}u^i=\vec{a}_i\left(\frac{\partial u^i}{\partial\xi^j}+u^k\Gamma_{jk}^i\right)$$ So the covariant derivative is defined as: $$\nabla_ku^i=\left(\frac{\partial u^i}{\partial\xi^k}+u^j\Gamma_{jk}^i\right)$$ And the same way is derived for covector field: $$\nabla_ku_i=\left(\frac{\partial u_i}{\partial\xi^k}-u_j\Gamma_{ik}^j\right)$$ And then out of nowhere it says that for higher rank tensors it must be defined as: $$\nabla_k\Phi^{i_1 i_2 ...i_p}_{j_1j_2...j_q}=\frac{\partial \Phi^{i_1 i_2 ...i_p}_{j_1j_2...j_q}}{\partial\xi^k}+\Phi^{s i_2 ...i_p}_{j_1j_2...j_q}\Gamma_{sk}^{i_1}+...+\Phi^{i_1 i_2 ...s}_{j_1j_2...j_q}\Gamma_{sk}^{i_p}-\Phi^{i_1 i_2 ...i_p}_{sj_2...j_q}\Gamma_{j_1k}^{s}-...-\Phi^{i_1 i_2 ...i_p}_{j_1j_2...s}\Gamma_{j_qk}^{s}$$ And I just can't understand what makes us think that it must be the way the higher rank tensor field covariant derivatives should look like without any intuition of proof that follows from the first two examples. Can you please help me understand it?,"['tensors', 'vector-analysis', 'partial-derivative', 'derivatives', 'differential-geometry']"
4789423,"Is there a closed form for $b_{n,m}(x)=\sum_{i=1}^m \frac{b_{n-1,i}(1)}{4i^2-1}x^i$?","Define $b_{n,m}(x)=\sum_{i=1}^m \frac{b_{n-1,i}(1)}{4i^2-1}x^i$ , and let $b_{1,m}(x)=\sum_{i=1}^m \frac{x^i}{4i^2-1}$ . Does we have a closed form for $b_{n,m}(1)$ ? I just know this from these functions: $$f(x)=\prod_{n=1}^\infty(1+\frac{x}{4n^2-1})=1+b_{1,\infty}(1)x+(b_{1,\infty}(1)^2-b_{2,\infty}(1))x^2+(b_{1,\infty}(1)^3-2b_{1,\infty}(1)b_{2,\infty}(1)+b_{3,\infty}(1))x^3+...$$ $$f(1)=\frac{2\times 2 \times 4 \times 4 \times 6 \times 6 ...}{1\times 3\times 3 \times 5 \times 5 \times 7 ...}=\frac{\pi}{2}$$ EDIT: Since if we have a function that satisfies the conditions $f(0)=1, f(a_n)=0$ , for $n \in [1,\infty)$ , then we have that the function f is equal to a infinite product and a infinite sum: $$f(x)=\prod_{n=1}^\infty (1-\frac{x}{a_n})=1-x\sum_{n=1}^\infty \frac{1}{a_n}+x^2\sum_{n=1}^\infty \sum_{k=n+1}^\infty \frac{1}{a_n a_k}-x^3\sum_{n=1}^\infty \sum_{k=n+1}^\infty \sum_{j=k+1}^\infty\frac{1}{a_n a_k a_j}+...$$ So if we have that $a_n=1-4n^2$ , then: $$f(x)=\prod_{n=1}^\infty \frac{4n^2-1+x}{4n^2-1}=1+x\sum_{n=1}^\infty \frac{1}{4n^2-1}+...=1+xb_{1,\infty}(1)+...$$","['infinite-product', 'sequences-and-series']"
4789428,Characterizations of divisors of the order of an element.,"For prime numbers $p$ consider the two conditions $({\rm A})\ \ \forall\:\! k\geq0\!:\ (-2)^k+1\not\equiv0\,\pmod{\!p}$ $({\rm B})\ \ \exists\:\! k\geq0\!:\ \ 2^{2k+1}+1\equiv0\,\pmod{\!p}$ Experimentation seems to show that the odd primes satisfying $\rm(A)$ are precisely the odd primes satisfying $\rm(B)$ . Is this correct?","['modular-arithmetic', 'elementary-number-theory', 'cyclic-groups', 'ideals', 'group-theory']"
4789502,"$\iota_{[X,Y]}\omega = \mathcal{L}_X\iota_Y\omega - \iota_Y\mathcal{L}_X\omega$ for every $X, Y \in \mathfrak{X}(M)$ and $\omega \in \Omega^k(M)$.","Prove that $\iota_{[X,Y]}\omega = \mathcal{L}_X\iota_Y\omega - \iota_Y\mathcal{L}_X\omega$ for every $X, Y \in \mathfrak{X}(M)$ and $\omega \in \Omega^k(M)$ . I know that $\mathcal{L}_X\iota_Y\omega - \iota_Y\mathcal{L}_X\omega=(\iota_X d\iota_Y\omega+d\iota_X\iota_Y\omega)-(\iota_Y d\iota_X\omega+\iota_Y\iota_X d\omega)$ For the another side $i_{[X,Y]}\omega = i_{L_X(Y)-L_Y(X)}\omega =i_{L_{X(Y)}}\omega-i_{L_{Y(X)}}\omega$ So, how much can I do?","['lie-derivative', 'smooth-manifolds', 'manifolds', 'lie-groups', 'differential-geometry']"
4789526,What is known about merely-orthogonal matrices?,"I'm interested in square matrices whose columns are orthogonal, but not necessarily orthonormal, non-zero vectors. Answers to other questions on this topic have noted that such matrices do not have an agreed name and that the natural name of ""orthogonal matrix"" means a matrix with orthonormal columns. So I'm going to use the name ""merely-orthogonal"" for these matrices and ""orthonormal"" for those matrices where $Q^TQ=I$ , avoiding ""orthogonal matrix"" entirely. The answer to one question usefully notes that if $M$ is merely-orthogonal then there exists an invertible diagonal matrix $D$ and an orthonormal matrix $Q$ such that $M=QD$ . The elements of $D$ are easily found as the norms of each column. This also immediately implies that $$M^TM = (QD)^TQD = D^TQ^TQD = D^T D = D^2$$ which is diagonal and has non-zeros on the diagonal. And hence, $M$ is invertible with $$M^{-1} = D^{-2}M^T$$ It seems like merely-orthogonal matrices should form a group under multiplication and that there should be an analog to the $QR$ decomposition (call it the $MR$ decomposition) where for any matrix $A$ , $A = MR$ with $M$ merely-orthogonal and $R$ upper triangular. It seems to me that the $MR$ decomposition would not require the underlying field to be algebraically closed, like the $QR$ decomposition does but would be computable over the rational numbers. So my questions are: Has anyone studied merely-orthogonal matrices and established these or other results? Am I right that merely-orthogonal matrices form a group? Am I right that $MR$ decompositions can be done over the rationals?","['orthogonal-matrices', 'linear-algebra']"
4789572,Difference between Axiom and Definition? [duplicate],"This question already has answers here : What is exactly the difference between a definition and an axiom? (13 answers) Closed 8 months ago . The community reviewed whether to reopen this question 8 months ago and left it closed: Original close reason(s) were not resolved Question: What is the difference between an Axiom and a Definition ? Example Terence Tao's Analysis I, which is intentionally and carefully written to develop theory in small incremental steps from basic axioms has Set Union as an Axiom (3.5 in the 4th ed book) Set Intersection as a Definition (3.1.22 in the 4th ed book) I can't understand why one if an axiom and one is a definition. Why are they both not axioms? Discussion I (think) I understand there is a hierarchy of definitions, with axioms being the most fundamental and which have no supporting definitions. And as such they can't be proved, they are the basic for other proofs. In my mind a definition is for convenience. That is, a definition uses notions established in axioms to define a shortcut for a new concept. And then propositions , lemmas and theorems are ""taller"" constructions based on axioms and definitions, with the only difference between these three being their ""height"" above the basic axioms.","['elementary-set-theory', 'terminology']"
4789630,Prove that XY is a chi-squared random variable if X and Y are independent normal random variables.,"According to this answer , $X+Y$ and $X−Y$ are Gaussian random variables, so that $(X+Y)^2$ and $(X−Y)^2$ are Chi-square distributed with 1 degree of freedom, where $X\sim N(a,b)$ and $Y\sim N(c,d)$ I'm confused since while I understand that $(X+Y)$ ~Normal $(a + c, b + d)$ and $(X-Y)$ ~Normal $(a + c, b + d)$ , I do not understand why $(X+Y)^2$ and $(X-Y)^2$ are chi-squared random variables. As far as I know, only squares of standard normal random variables can be chi-squared distributed, and $(X+Y)^2$ and $(X-Y)^2$ are, in general, not standard normal random variables. Am I misunderstanding / missing something here?","['chi-squared', 'statistics', 'probability-distributions', 'normal-distribution']"
4789682,Does $|\vec x_{n+m}|\leq |\vec x_n+\vec x_m|$ imply the convergence of $\{\vec x_n/n\}$?,"Let $d\geq 1$ be a positive integer. If $\{\vec x_n\}_{n=1}^\infty$ is a sequence of $d$ -dimensional vectors satisfying $$|\vec x_{n+m}|\leq |\vec x_n+\vec x_m|\qquad \text{for all }n,m\in\mathbb N_{\geq 1},\tag{$*$}$$ where $|\vec x|=[(x^{(1)})^2+\cdots+(x^{(d)})^2]^{1/2}$ for $\vec x=(x^{(1)}, \cdots, x^{(d)})$ . Then does the limit $\lim\limits_{n\to\infty}\frac{\vec x_n}{n}$ exist? Ideas. It follows from $(*)$ and the induction that $|\vec x_n|\leq n|\vec x_1|$ for all $n\in\mathbb N_{\geq 1}$ , hence $\{\vec x_n/n\}_{n\geq 1}$ is a bounded sequence. On the other hand, $(*)$ also implies that $|\vec x_{n+m}|\le |\vec x_n|+|\vec x_m|$ for all $n,m\in\mathbb N_{\geq 1}$ , hence by Fekete's subadditive lemma we get the convergence of $\{|\vec x_n|/n\}_{n\geq 1}$ with $$\lim_{n\to\infty}\frac{|\vec x_n|}n=\inf_{n\geq 1}\frac{|\vec x_n|}n.$$ If $d=1$ , then the answer is yes , which is proved in this post . For $d\geq2$ , I tried to use the same ideas as in that post. If $\lim\limits_{n\to\infty}\frac{|\vec x_n|}n=L$ , then for all $\epsilon>0$ there exists $N\gg L$ such that $\left|\frac{|\vec x_n|}n-L\right|<\varepsilon$ for all $n>N$ . Take $\varepsilon>0$ small enough. Fix $n_0>N$ , then there exists $\vec z_1$ with $|\vec z_1|=1$ such that $\left|\frac{\vec x_{n_0}}{n_0}-L\vec z_1\right|<\varepsilon$ . Let $n$ be the largest integer such that $\left|\frac{\vec x_{k}}{k}-L\vec z_1\right|<\varepsilon$ holds for all $n_0\leq k\leq n$ . Then $\left|\frac{\vec x_{n}}{n}-L\vec z_1\right|<\varepsilon$ and $\left|\frac{\vec x_{n+1}}{n+1}-L\vec z_1\right|\geq\varepsilon$ , hence $\left|\frac{\vec x_{n+1}}{n+1}-L\vec z_2\right|\geq\varepsilon$ for some $\vec z_2$ with $|\vec z_2|=1$ and $\vec z_2$ is far away from $\vec z_1$ . However, they are not as far away as in that post, where $\vec z_1=1, \vec z_2=-1$ for $d=1$ . I'm not sure how to continue. In fact, I suspect that the answer for $d\geq 2$ is negative. But I cannot find a counterexpamle.","['limits', 'multivariable-calculus', 'sequences-and-series', 'real-analysis']"
4789716,Integral Representation of $\zeta^2(3)$ based on $\int_{0}^{1}\frac{\ln^ms\cdot\ln^n(1+s)}{1+s}ds$,"Using Integer Relation Algorithms, I was able to arrive at the following : $$12\zeta^2(3)=-40\int_{0}^{1}\frac{\ln s\ln^{4}(1+s)}{1+s}ds+40\int_{0}^{1}\frac{\ln^{2}s\ln^{3}(1+s)}{1+s}ds-44\int_{0}^{1}\frac{\ln^{3}s\ln^{2}(1+s)}{1+s}ds+23\int_{0}^{1}\frac{\ln^{4}s\ln(1+s)}{1+s}ds$$ Is it possible to simplify the Right Hand Side so that we have a nice Integral Representation for $\zeta^2(3)?$ Maybe we can apply some Powerful Substitution. The Coefficients do have Alternating Signs. I thought to make a pattern appear by removing the powers of $2$ as : $$5\cdot2^3,\ 10\cdot2^2,\ 22\cdot2^1,\ 23\cdot2^0$$ But this does not appear to be binomial. Also, I do not expect it to be feasible considering the Coefficients do not exhibit any Binomial Pattern.","['integration', 'calculus', 'definite-integrals']"
4789717,Precise statement of Poincaré duality,"Let $(M,g)$ be a (not-necessarily compact) oriented, connected Riemannian manifold. Lets consider the pairing $$\Omega^{k}(M)\times\Omega^{d-k}_{c}(M)\to\mathbb{R}, (\alpha,\beta)\mapsto\int_{M}\alpha\wedge\beta$$ It is well-known that this pairing induces a well-defined pairing on cohomology $$H^{k}(M)\times H^{d-k}_{c}(M)\to\mathbb{R}, ([\alpha],[\beta])\mapsto\int_{M}\alpha\wedge\beta\quad\quad\quad (\ast)$$ Poincaré duality states that this pairing is non-degenerate (in both entries I suppose (?)). Now, my question is, what are the precise assumption for this to hold? In the book Manifolds and Differential Geometry by J. M. Lee, only non-degenaracy in the first entry is proven, i.e. that ( $\ast$ ) viewed as a map $H^{k}(M)\to (H^{d-k}_{c}(M))^{\ast}$ is an isomorphism provided $(M,g)$ admits a finite good cover . On the other hand, in Connections, Curvature, and Cohomology 1 by W. Grueb, it is mentioned that the finite good cover assumption is actually only needed for the other side, i.e. to show that ( $\ast$ ) viewed as a map $H^{d-k}_{c}(M)\to (H^{k}(M))^{\ast}$ is an isomorphism... Does anyone know the precise statement or has some idea where to find it?","['riemannian-geometry', 'reference-request', 'duality-theorems', 'algebraic-topology', 'differential-geometry']"
4789720,"$\int_0^\infty \frac{u}{u^2+\sigma^2} \, J_{4m} (u) \, \mathrm{d} u = K_{4m}(\sigma) + ?$","As I delved into the intricacies of a fluid mechanics problem that revolved around Green's functions in porous media, I stumbled upon the following intriguing infinite integral: $$
\phi_m (\sigma) = \int_0^\infty \frac{u}{u^2+\sigma^2} \, J_{4m} (u) \, \mathrm{d} u \, , 
$$ wherein $\sigma \ge 0$ and $m \in \mathbb{N}$ .
The first four terms can be obtained as \begin{align}
\phi_0  &= K_0(\sigma) \, , \\
\phi_1  &= K_4(\sigma) + \frac{4}{\sigma^4} \left( \sigma^2-12\right) \, , \\
\phi_2  &= K_8(\sigma) + \frac{8}{\sigma^8}
\left( \sigma^6-60\sigma^4+2\,880\sigma^2-80\,640 \right) \, , \\
\phi_3 &= K_{12} (\sigma)
+ \frac{12}{\sigma^{12}}
\left( \sigma^{10}-140\sigma^8 + 17\,920\sigma^6 - 1\,935\,360\sigma^4 + 154\,828\,800\sigma^2-6\,812\,467\,200 \right) \, .
\end{align} It appears that the overall expression will begin with the first term, denoted as $K_{4m}(\sigma)$ , accompanied by a generalized hypergeometric function for which I have yet to discover a general expression. Any assistance in this matter would be greatly valued. Thank you.","['integration', 'improper-integrals', 'calculus', 'indefinite-integrals', 'bessel-functions']"
4789759,"Embedding of $\mathcal{D}(\Omega)$ in $W^{k,p}_0(\Omega)$","$W^{k,p}_0(\Omega)$ is defined as the  closure of the set of all $C_c^{\infty}(\Omega)$ under the topology generated by the norm $W^{k,p}(\Omega)$ .
So clearly the identity map from $\mathcal{D}(\Omega) \rightarrow W^{k,p}_0(\Omega)$ defines an injection. Is this injection continuous? If so how to prove it. P.S: $\mathcal{D}(\Omega)=(C_c^{\infty}(\Omega),\tau_{LF})$ , i.e., $C_c^{\infty}(\Omega)$ endowed with its canonical LF topology. $W^{k,p}_0(\Omega)=\big(\mathcal{D}(\Omega), \|\cdot\|_{W^{k,p}({\Omega})}\big)$ , i.e., $C_c^{\infty}(\Omega)$ endowed with the topology generated by $W^{k,p}{\Omega}$ norm. A clean proof will be greatly appreciated. Thanks in advance.","['sobolev-spaces', 'functional-analysis', 'analysis', 'weak-topology']"
4789782,How does this definition give mutual independence of events?,"I am not understanding how this definition gives mutual independence: I am not seeing this because It seems the LHS of the equation will always be the null set, as $B_i$ assumes both $A_i$ and $\bar{A_i} $ , and the intersection of these is null, making the entire intersection whose probability we are computing on the LHS null. How am I misinterpreting this definition?","['elementary-set-theory', 'discrete-mathematics', 'probability']"
4789795,Surface Area of an Ellipsoid $AX^2 + BY^2+ CZ^2 + DXY + EXZ + FYZ - R_0^2= 0$,"I am attempting to determine the surface area of an ellipsoid described by the equation: $$AX^2 + BY^2 + CZ^2 + DXY + EXZ + FYZ - R_{0}^2 = 0 \; .$$ When it comes to an ellipse aligned with its axes at the origin of a Cartesian system, as given by $\frac{x^2}{a^2} + \frac{y^2}{b^2}=1$ , the process is straightforward, and we swiftly find that $A = \pi ab$ , where $a$ and $b$ represent the semi-major and semi-minor axes respectively. However, for an ellipse not meeting these conditions, like the example mentioned here , the task becomes more challenging but remains achievable through algebraic manipulation. I attempted to apply a similar approach to the equation $AX^2 + BY^2 + CZ^2 + DXY + EXZ + FYZ - R_{0}^2= 0$ representing an ellipsoid, but I have not been able to solve it. Can someone provide assistance with this question, please? Edit: I do not have any other information such as eccentricity, length of the axes, etc... I only have the equation above.","['conic-sections', 'geometry']"
4789874,Single line coupled 2nd order differential equation,"In the paper 2111.05151 that I'm reading, there is a single coupled second-order differential equation (equation 71 in the paper), \begin{equation}
r \ddot{z}_1 = z_1 \ddot{r}, \qquad z_1(v_0) = z_1(v_f) = 0
\end{equation} where the dot is derivative with respect to $v$ , and $v_0$ & $v_f$ are the initial and final points respectively. The paper wrote that the solution that satisfies the above equation including the boundary condition on $z_1$ is given by, \begin{equation}
z_1 = c r
\end{equation} for some constant $c$ . Typically, we would need two equations in order to solve for $z_1$ and $r$ as a function of $v$ . However, in this case, I just need to get $z_1$ as an expression in terms of $r$ as is done in the paper. How is the solution obtained? I'm applying the method in the paper to a situation that I'm studying for which I got a single coupled second-order differential equation given by, \begin{equation}
2 r \ddot{z}_1 + 4 \dot{r} \dot{z_1} + 3 \ddot{r} z_1 = 0, \qquad z_1(v_0) = z_1(v_f) = 0
\end{equation} Again, how do you solve for $z_1$ in terms of $r$ ? EDIT: I'm not sure if rewriting the expression in the same way as @user10354138 will be helpful, \begin{equation}
2 r \ddot{z}_1 + 4 \dot{r} \dot{z_1} + 3 \ddot{r} z_1 = 0\\
2 r \ddot{z}_1 + 2 \dot{r} \dot{z_1} + 2 \dot{r} \dot{z_1} + 2 \ddot{r} z_1 + \ddot{r} z_1 = 0\\
2\left( \frac{d}{dv}(r \dot{z}_1) + \frac{d}{dv}(\dot{r} z_1) \right) + \ddot{r} z_1 = 0\\
\frac{d}{dv} \left( r \dot{z}_1 + \dot{r} z_1 \right) = -\frac{1}{2} \ddot{r} z_1
\end{equation} Of course, we could rewrite the LHS and RHS further, but this is one version.","['calculus', 'systems-of-equations', 'ordinary-differential-equations', 'partial-differential-equations']"
