question_id,title,body,tags
4663702,"Elements of the coset $G/H$, where $G=\text{GL}^+(2)$ and $H=\text{SO}(2)$","In this paper , in section $2$ , a method to write the elements of the coset of $G/H$ is provided for $\text{GL}(4)$ , but I am interested in $\text{GL}^+(2)$ . My matrix representation of $\mathfrak{gl}(2)$ is $$
\begin{bmatrix}
a+x& -b+y\\
b+y & a-x
\end{bmatrix}
$$ My matrix representation of $\mathfrak{so}(2)$ is $$
\begin{bmatrix}
0& -b\\
b & 0
\end{bmatrix}
$$ Reading the paper, it states that the element $g$ of $G$ can be decomposed as $g=\gamma h$ , where $$
\gamma = \exp \left( \begin{bmatrix}
a+x& +y\\
y & a-x
\end{bmatrix} \right) \in G/H
$$ and where $$
h = \exp \left( \begin{bmatrix}
0& -b\\
b & 0
\end{bmatrix} \right) \in H
$$ Is this correct, or no? I am suspicious of the argument, because to me $$
\exp \left( \begin{bmatrix}
a+x& y\\
y & a-x
\end{bmatrix} \right)\exp \left( \begin{bmatrix}
0& -b\\
b & 0
\end{bmatrix} \right) \neq \exp \left( \begin{bmatrix}
a+x& -b+y\\
b+y & a-x
\end{bmatrix} \right)
$$ Thus, $\gamma h$ does not appear to realize all elements of $G$ . Or do we not care about some missing elements for cosets?","['group-theory', 'lie-algebras', 'lie-groups']"
4663708,Can superset and subset be used interchangeably?,"Is it true that $A \subset B \iff B \supset A$ ? I have been using both interchangeably in order to save time when Latexing some notes, but this question gave me some pause.",['elementary-set-theory']
4663715,closed-form Newton flow of tanh(ln(1+x^2)),"The differential equation for the Newton flow $z (t)$ of $f (t)$ is given by \begin{equation}
  \dot{z} (t) = - \frac{f (z (t))}{\frac{d}{d t} f (z (t))} = -
  \frac{f (z (t))}{\dot{f} (z (t))}
\end{equation} If we let \begin{equation}
  b (a) = 2 (1 + a^2) + a^4
\end{equation} then define \begin{equation}
  g (t, a) = e^t - 1 + \frac{2}{b (a)}
\end{equation} and \begin{equation}
  h (t, a) = \sqrt{e^{2 t} - \frac{a^4 (2 + a^2)^2}{b (a)^2}}
\end{equation} then there are 4 solutions of $z (t)$ for the Newton flow given by \begin{equation}
  z (t,a) = \pm \sqrt{\pm \frac{g (t, a) + h (t, a)}{g (t, a)}}
\end{equation} where lim_(t→∞)z(t,a)∈{0,±i√2}∀a∈ℂ and S(lim_(t→∞)z(t,a))=0","['gradient-flows', 'ordinary-differential-equations', 'newton-raphson', 'roots', 'derivatives']"
4663723,How to determine if a subspace of $ \mathbb{R}^n $ has an integer basis,"Let $ W $ be a sub vector space of $ \mathbb{R}^n $ . How can we determine if $ W $ admits an integer basis? This is equivalent to asking how to determine if $ W \cap \mathbb{Z}^n $ spans $ W $ . Obviously if $ W=\mathbb{R}^n $ then there is always an integer basis. For example the standard basis $ e_1,\dots e_n $ , just the columns of the identity matrix. $ W $ a $ 1 $ dimensional subspace of $ \mathbb{R}^2 $ is the first nontrivial case. In this case $$
W=Span \{ (a,b) \} 
$$ and $ W $ has an integer basis if and only if $ a/b $ or $ b/a $ (check both in case $ a $ or $ b $ is $ 0 $ ) is rational. Is there some general criterion that allows us to look at certain spanning sets for $ W $ and perhaps look at ratios of coefficients like this to solve the problem? Update: Just to reiterate the correct answer given below, to determine if an integer basis exists just take any set of $ k $ vectors spanning $ W $ , put them as the rows of a matrix, then find RREF of that matrix and then $ W $ has an integer basis if and only if that RREF has all rational entries.",['linear-algebra']
4663725,What is the relationship between measurable or continuos cross-sections?,"Let $G$ be a locally compact Polish (or compact) group acting continuously on a locally compact Polish (or compact) space $X$ , and $\mu$ a Borel measure on $X$ . To be sure, continuity of the action means that the map $(g, x) \in G \times X \mapsto g \cdot x \in X$ is continuous with respect to the product topology on $G \times X$ . Let $X/G$ denote the orbit space endowed with the quotient topology, and $\pi : X \rightarrow X/G$ denote the orbit map. A cross-section to the orbit map is a map $s: X/G \rightarrow X$ satisfying $s \circ \pi = 1_{X}$ . If $s$ and $t$ are measurable or continuous cross-sections to the orbit map, then it is known that their images $s(X/G)$ and $t(X/G)$ are measurable (closed in the case of a continuous cross-section with compact $G$ and $X$ ). What is the relationship between $\mu(s(X/G)$ and $\mu(t(X/G))$ ? Is it reasonable to expect $\mu(s(X/G)) = \mu(t(X/G))$ ? PS: It is enough for me to consider the case of $X = G$ , that is, $X$ is the underlying topological space of $G$ , and the action of conjugation, and $\mu$ Haar measure on $G$ . Thanks.","['measure-theory', 'group-actions', 'haar-measure', 'topological-groups']"
4663748,"Bounds on the maximum real root of a polynomial with coefficients $-1,0,1$","Suppose I have a polynomial that is given a form $$
f(x)=x^n - a_{n-1}x^{n-1} - \ldots - a_1x - 1
$$ where each $a_k$ can be either $0,1$ . I've tried a bunch of examples and found that the maximum real root seems to be between $1,2$ , but as for specifics of a polynomial of this structure I am not aware. Using IVT, we can see pretty simply that $f(1)\leq0$ and $f(2)> 0$ so there has to be a root on this interval, but thats a pretty wide range was wondering if this was previously studied","['roots', 'calculus', 'polynomials', 'upper-lower-bounds', 'algebra-precalculus']"
4663777,When does the product of random matrices diverge?,"Suppose $A_i$ are IID samples of a random matrix-valued variable. I'm interested in determining whether the following infinite product is likely to diverge $$A_1 A_2 A_3\cdots$$ Finding necessary+sufficient condition is hard (Theorem 2). There are cheap to compute sufficient conditions for the product to converge. Are there cheap to compute sufficient conditions for the product to diverge? Of particular interest is case of $A_i=(I-x_i x_i^T)$ where $x_i$ are IID Gaussian. The case for isotropic Gaussian is solved here , I'm looking for insight into non-isotropic case","['ergodic-theory', 'stochastic-processes', 'stability-theory', 'probability', 'dynamical-systems']"
4663778,Show that divisor in $\mathbb P^2\times \mathbb P^2$ is very ample.,"Let $H$ denote a quadric hypersurface in $\mathbb P^2\times \mathbb P^2$ . In the Chow ring of $\mathbb P^2\times \mathbb P^2$ , we have $H\equiv 2H_1+2H_2$ , where $H_1,H_2$ are the classes of linear forms in each copy of $\mathbb P^2$ . I want to prove that $H$ is very ample as a divisor. This should mean that there are ''enough'' global sections $s$ in $\Gamma(\mathbb P^2\times \mathbb P^2,\mathcal O(H))$ . By this I mean that there should exist global sections $s_1,\ldots,s_n$ that do not vanish simultaneously, and the map $[s_1:\cdots:s_n]$ from $\mathbb P^2\times \mathbb P^2$ to $\mathbb P^{n-1}$ is isomorphic onto its image. But I can't see how to proceed from there.","['divisors-algebraic-geometry', 'algebraic-geometry', 'line-bundles']"
4663812,Can I infer the hypotenuse given only $a > b$?,"Today, I came up with a problem. The problem is this: Let $a$ , $b$ and $c$ be the sides of a right-angled triangle, such that $a > b$ . The nature of $c$ is unknown. Can I infer the hypotenuse given only this data? Resolution: The sides of all right triangles should follow: $$ hypt \gt adj1 \ge adj2 $$ Where $adj1$ and $adj2$ are the remaining, adjacent sides.
I then do a simple permutation of the hypotenuse within the above to uncover all the possibilities: Possibility 1. $$ a (hypt) \gt b(adj1, adj2) \ge c (adj1, adj2) $$ Possibility 2. $$ a (adj1, adj2) \lt b (hypt) $$ $$ b (hypt) \gt c (adj1, adj2) $$ Possibility 3. $$ a (adj1, adj2) \le b(adj1, adj2) \lt c(hypt) $$ Is this correct?","['algebra-precalculus', 'pythagorean-triples', 'trigonometry']"
4663814,"The Iwasawa decomposition of $\text{GL}(2,\mathbf R)$","The Iwasawa decomposition of $\text{SL}(2,\mathbf R)$ is $$
\begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix} \begin{bmatrix} r & 0 \\ 0 & 1/r \end{bmatrix} \begin{bmatrix}1&x\\ 0 & 1 \end{bmatrix}
$$ where $\begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix} \in SO(2)$ , where $\begin{bmatrix} r & 0 \\ 0 & 1/r \end{bmatrix}$ is a squeeze matrix and where $\begin{bmatrix}1&x\\ 0 & 1 \end{bmatrix}$ is a shear matrix. I wish to extend this to $\text{GL}(2,\mathbf R)$ . Can I multiply with $$
\begin{bmatrix} \sqrt{a} & 0 \\ 0 & \sqrt{a} \end{bmatrix}
$$ Perhaps this gives $\text{GL}^+(2,\mathbf R)$ ? If so, I am also okay with that. Finally, I was wondering what the lie algebra of $\begin{bmatrix} r & 0 \\ 0 & 1/r \end{bmatrix}$ is. And also for $\begin{bmatrix}1&x\\ 0 & 1 \end{bmatrix}$ .",['group-theory']
4663849,The definition of smooth sections given in Introduction to smooth manifolds by John M. Lee,"In p. 176 of John M. Lee's Introduction to smooth manifolds, 2nd edition, Lee defines vector fields along subsets $A \subseteq M$ of a smooth manifold with boundary $M$ .
The part that intrigues me is the fact that he requires a smooth local extension at each point by vector fields , not just any smooth maps.
That makes you think there is a separate definition for smooth vector fields along arbitrary subsets.
I've been thinking about it and I think the requirement of local extension at each point by vector fields is unnecessary. I'll write briefly about that here in the more general context of vector bundles.
I want to know (preferably from @ Jack Lee ) if I'm doing this correctly. This is the definition that Lee suggest of smooth sections along arbitrary subsets. Definition. Let $M$ be a smooth manifold with or without boundary, $\pi: E \to M$ a vector bundle of rank $k$ , $A \subseteq M$ an arbitrary subset and $\sigma: A \to E$ a rough section, i.e. a map (not necessarily continuous) such that $\pi\circ\sigma = Id_A$ .
We say $\sigma$ is smooth if $\sigma$ extends to a smooth local section of E in a neighborhood of each point. See Definition of smoothness to have some context about the definition of smoothness along arbitrary subsets. And this is what I'm thinking. Theorem 1. Let $M$ be a smooth manifold with or without boundary, $A \subseteq M$ an arbitrary subset and $k \in \mathbb{N}$ .
If $f: A \to \mathbb{R}^k$ is smooth then there exist an open neighborhood $U$ of $A$ and a smooth extension $\overline{f}: U \to \mathbb{R}^k$ of $f$ . Proof. Use partitions of unity. Theorem 2. Let $M$ be a smooth manifold with or without boundary, $\pi: E \to M$ a vector bundle of rank $k$ , $(\sigma_1, \dots, \sigma_k)$ a local frame for $E$ over the open subset $U \subseteq M$ , $A \subseteq U$ an arbitrary subset and $\sigma: A \to E$ a rough section.
Write $\sigma = \tau^i\sigma_i$ for some component functions $\tau^i: A \to \mathbb{R}$ .
Then $\sigma$ is smooth if and only if all $\tau^i$ are smooth.
Furthermore, if $\sigma$ is smooth then there exists a neighborhood $W$ of $A$ and a smooth  extension $\overline{\sigma}: W \to E$ of $\sigma$ which is a section such that $W \subseteq U$ . Proof. The first part is just following the definitions (but being careful of using the right definition of smoothness for arbitrary subsets of smooth manifolds with boundary).
For the second part, assume $\tau = (\tau^1, \dots, \tau^k): A \to \mathbb{R}^k$ is smooth. By Theorem 1. , there exists  a neighborhood $W$ of $A$ and a smooth extension $\overline{\tau}: W \to \mathbb{R}^k$ of $\tau$ such that $W \subseteq U$ .
Now simply define $\overline{\sigma} = \overline{\tau}^i\sigma_i$ .
This is a smooth section and extends $\sigma$ . Theorem 3. Let $M$ be a smooth manifold with or without boundary, $\pi: E \to M$ a vector bundle of rank $k$ , $A \subseteq M$ an arbitrary subset and $\sigma: A \to E$ a rough section.
If $\sigma$ is smooth then there exists an open neighborhood $U$ of $A$ and a smooth extension $\overline{\sigma}: U \to E$ of $\sigma$ which is a section. Proof. By Theorem 2. , we can pick for each $p \in A$ , a neighborhood $U_p$ of $p$ and a smooth extension $\overline{\sigma}_p: U_p \to E$ of $\sigma|_{A \cap U_p}$ which is a section.
Now pick a smooth partitions of unity $(\psi_p)_{p \in A}$ subordinated to the open cover $(U_p)_{p \in A}$ of $U = \bigcup_{p \in A}U_p$ .
Now define \begin{equation*}
  \overline{\sigma}(x) = \sum_{p \in A}\psi_p(x)\overline{\sigma}_p(x),
\end{equation*} for $x \in U$ .
This is a smooth section and extends $\sigma$ .","['vector-bundles', 'smooth-manifolds', 'differential-geometry']"
4663850,Haar measures are decomposable,"In the real analysis book by Folland, section $11.1$ exercise $9$ have been come that: if $G$ is a locally compact topological group with Haar measure $\mu$ , then $\mu$ is decomposable. A measure space $(X,\mathfrak{M},\mu)$ is decomposable if: (i) $X$ is a disjoint union of measurable subsets, $X=\bigcup_{i\in I}X_{i}$ , with $\mu(X_{i})<\infty$ for all $i$ . (ii) $\mu(A)=\sum_{i\in I}\mu(A\cap X_i)$ for every measurable set $A$ of finite measure. (iii) if $A\subseteq X$ and $A\cap X_i\in \frak{M}$ for all $i$ , then $A\in\frak{M}$ . $\textbf{EDITED:}$ $\color{red}{\textbf{I succeeded that prove (i) and (ii). }}$ $\color{red}{\textbf{can someone help me to prove (iii)? }}$ $\color{blue}{\textbf{proof of (i):}}$ if $U\subseteq G$ be a compact simetric ( $U^{-1}=U$ ) neighbourhood of $e$ , then if $U_n=\underbrace{U...U}_{\text{n times}}$ ,  the sets $U_n$ for every $n$ , are compact ( because action of group is countinuous) and  the set $H=\underset{n\in \mathbb{N}}{\bigcup}U_n$ is an open subgroup generated by $U$ ( $H$ is open because, any set $U_n$ , have a neighbourhood $U_{n+1}\subseteq H$ ) and we have $G=\underset{g\in C}{\bigcup}gH=\underset{\underset{n\in \mathbb{N}}{g\in C}}{\bigcup}gU_n$ , where $C$ by axiom of choice is a set that contains one point from every coset $gH$ . sets $gU_n$ are compact  and hence $\mu(gU_n)<\infty$ . now (i) is proved if we consider sets of the form $gU'_n=gU_n\setminus\underset{i=1}{\overset{n-1}{\bigcup}}gU_i$ instead of $gU_n$ . It causes the sets $\{gU'_n| {n\in \mathbb{N}}\}$ for every $g$ , be disjoint and since $gH=\underset{n\in C}{\bigcup}gU'_n$ and the cosets $\{gH\}_{g\in C}$ are disjoint, then we have $G=\underset{\underset{n\in \mathbb{N}}{g\in C}}{\bigcup}gU'_n$ where $\mu(gU'_n)<\infty$ and the sets $\{gU'_n|n\in \mathbb{N}, g\in C\}$ are disjoint. $\color{blue}{\textbf{proof of (ii):}}$ if we substitute $\{gU'_n|g\in C, n\in\mathbb{N}\}$ with $\{X_i\}_{i\in I}$ , since $\underset{i\in F}{\sum}\mu(A\cap X_i)=\mu\Bigg(A\bigcap\bigg[\underset{i\in F}{\bigcup}X_i\bigg]\Bigg)\leq\mu(A)$ for finite set $F\subseteq I$ , then $\underset{i\in I}{\sum}\mu(A\cap X_i )=\underset{\underset{\text{F is finite}}{F\subset I}}{\sup}\left(\underset{i\in F}{\sum}\mu(A\cap X_i)\right)\leq \mu(A)$ and hence $$\underset{i\in I}{\sum}\mu(A\cap X_i)\leq\mu(A) \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(1) $$ now if $K\subseteq G$ be compact, then since $K\subseteq \underset{g\in C}{\bigcup}gH$ and the sets $gH$ are open, then $K\subseteq \underset{i=1}{\overset{m}{\bigcup}}g_iH$ where $g_i\in C$ . now if $\mu(A)<\infty$ and $\epsilon>0$ , then by inner regularity of haar measure, there exist a compact set $K\subseteq A$ such that $\mu(A)-\epsilon<\mu(K)$ . also we have $\mu(K)=\mu\Big[K\bigcap\Bigg(\underset{i=1}{\overset{m}{\bigcup}}g_iH\Bigg)\Big]=\underset{i=1}{\overset{m}{\sum}}\mu(K\cap g_iH)=\underset{i=1}{\overset{m}{\sum}}\underset{n\in\mathbb{N}}{\sum}\mu(K\cap g_iU'_n)\leq\underset{i=1}{\overset{m}{\sum}}\underset{n\in\mathbb{N}}{\sum}\mu(A\cap g_iU'_n)\leq\underset{i\in I}{\sum}\mu(A\cap X_i)$ then we have $\mu(A)-\epsilon<\mu(K)\leq\underset{i\in I}{\sum}\mu(A\cap X_i)$ and since $\epsilon$ is arbitrary, it follows $$\mu(A)\leq\underset{i\in I}{\sum}\mu(A\cap X_i) \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(2)$$ and by $(1)$ and $(2)$ , (ii) also is proved.","['measure-theory', 'harmonic-analysis', 'topological-groups', 'haar-measure', 'locally-compact-groups']"
4663897,Evaluating the sum $\sum_{k=1}^\infty (-1)^k / k^2$ via a contour integral,"I'm evaluating the sum: \begin{align*}
\sum_{k=1}^\infty (-1)^k \frac{1}{k^2}
\end{align*} I expressed the sum via a complex contour integration. But I'm not getting out the right answer. My method: It employs the fact that the function \begin{align*}
\dfrac{1}{\tan \pi z}
\end{align*} has simple poles at all integers $n$ . This allows us to write \begin{align*}
\sum_{k=1}^\infty (-1)^k \frac{1}{k^2} = \frac{1}{2 i} \oint_C e^{i \pi z} \dfrac{1}{z^2 \tan \pi z} dz
\end{align*} where the contour $C$ is defined in fig (a). We have \begin{align*}
\sum_{k=1}^\infty (-1)^k \frac{1}{k^2} & = \frac{1}{2} \sum_{k=1}^\infty (-1)^k \frac{1}{k^2} + \frac{1}{2} \sum_{k=-1}^{-\infty} (-1)^k\frac{1}{k^2}
= \frac{1}{4 i} \oint_{C+C'} e^{i \pi z} \dfrac{1}{z^2 \tan \pi z} dz
\end{align*} where the contour $C'$ is defined in fig (a). We complete the path of integration along semicircles at infinity (see fig (b)) since the integrand vanishes there. Since the resulting enclosed area contains no singularities except at $z=0$ , we can shrink this contour down to an infinitesimal circle $C_0$ around the origin (see fig (c)). So that \begin{align}
\sum_{k=1}^\infty (-1)^k \frac{1}{k^2} = \frac{1}{4 i} \oint_{C_0} e^{i \pi z} \dfrac{1}{z^2 \tan \pi z} dz
\end{align} Expanding the integrand around $z=0$ , we get \begin{align}
e^{i\pi z} \dfrac{1}{z^2 \tan \pi z} = (1 + i \pi z - \frac{1}{2!} \pi^2 z^2 + \cdots ) \dfrac{1}{z^2 (\pi z + \frac{1}{3} \pi^3 z^3 + \cdots)}
\end{align} \begin{align}
= (1 + i \pi z - \frac{1}{2!} \pi^2 z^2 + \cdots ) \dfrac{1}{z^3 \pi (1 + \frac{1}{3} \pi^2 z^2 + \cdots)}
\end{align} \begin{align}
= \cdots + (1 - \frac{1}{2!} \pi^2 z^2 + \cdots ) \dfrac{1}{z^3 \pi} (1 - \frac{1}{3} \pi^2 z^2 + \cdots)
\end{align} \begin{align}
= \cdots - \dfrac{5\pi}{6} \frac{1}{z} + \cdots
\end{align} So that \begin{align*}
\sum_{k=1}^\infty (-1)^k \frac{1}{k^2} = \frac{1}{4 i} \oint_{
C_0} e^{i\pi z} \dfrac{1}{z^2 \tan \pi z} dz
= \frac{1}{4 i} (-2 \pi i) (-\dfrac{5\pi}{6})
= \frac{5 \pi^2}{12}
\end{align*} Which is the wrong answer, it should be $- \pi^2 /12$ . What have I done wrong?","['complex-analysis', 'contour-integration', 'sequences-and-series']"
4663900,"Let $T$ be the following set of ordered triplets,$T=\{(a,b,c):a,b,c\in N\}$. Find the number of elements in $T$ such that $L.C.M(a,b,c)=72$.","Let $T$ be the following set of ordered triplets, $T=\{(a,b,c):a,b,c\in N\}$ . Find the number of elements in $T$ such that $L.C.M(a,b,c)=72$ . My Attempt Let $a=2^{x_1}3^{y_1}$ , $b=2^{x_2}3^{y_2}$ and $c=2^{x_3}3^{y_3}$ The $x_i's$ have 4 choices each where at least one of them should have the value $3$ and the rest can take any value from $0$ to $4$ . This can be done in $3\times 4\times 4=48$ ways Similarly for $y_i's$ we have $3$ choices for each. This can be done in $27$ ways . So required number of ways = $48\times 27=1245$ ways. Is this correct","['permutations', 'combinations', 'elementary-number-theory', 'combinatorics', 'algebra-precalculus']"
4663902,Total placement number of battleship game,"The board is 4x4 and there are three types of battleships: 3x1, 2x1, 1x1. One for each type . How many total placements are possible? Notice, the ships cannot overlap and we must use all ships . (there is one previous thread about this topic but seems no one actually got the right answer). Here is my approach: Calculate the total number (sample space) which is 11 (1 * 3 ship) * 24 (1 * 2 ship) * 16 (1 x 1 ship) Subtract the overlap
-> which is quite challenging to do since the discussing all cases using inclusion-exclusion principle is very time consuming and although I do believe one can get the correct solution, it is more suitable to use computer simulation. How can we do it by hand?","['combinatorics', 'packing-problem']"
4663908,"$f$ is holomorphic in $U(z_0,\delta)$ $\implies\displaystyle{\lim_{n\to\infty}\frac{f(z_n)-f(w_n)}{z_n-w_n}}=f'(z_0)$.","Suppose $f$ is holomorphic in $U(z_0,\delta)$ and $z_n\to z_0,w_n\to z_0\ (n\to\infty), z_n\neq w_n$ ,
then $$\lim_{n\to\infty}\frac{f(z_n)-f(w_n)}{z_n-w_n}=f'(z_0).$$ Unlike in real analysis, we can't use MVT (MVT is not ture for holomorphic functions).
I can prove the result by using power series expansion as follows: WLG, we can suppose $z_0=0$ ,
and then $$f(z)=\sum_{k=0}^{\infty}a_kz^k,\quad z\in U(0,\delta).$$ $$f(z_n)-f(w_n)=\sum_{k=1}^{\infty}a_k\left(z^k_n-w^k_n\right)
=a_1(z_n-w_n)+\sum_{k=2}^{\infty}a_k\left(z^k_n-w^k_n\right).$$ We need only to prove $$\lim_{n\to\infty}\frac{\sum\limits_{k=2}^{\infty}a_k\left(z^k_n-w^k_n\right)}{z_n-w_n}=0,$$ which is not difficult by using the umiform convergence. If there is another method such as $\epsilon-N$ language. Any help or hints will be welcomed.","['complex-analysis', 'sequences-and-series']"
4663927,Mathematics Behind Coincidence of Probabilities (Magic the Gathering Example),"In Magic the Gathering, a player must decide how many land cards to include in his or her deck.  Drawing too few at the beginning of a game is called mana screw.  Drawing too many at the beginning of a game is called mana flood.  Both make winning the game an uphill battle.  The ordinary course of action would be to add land cards to your deck if you experience mana screw more frequently, and remove land cards from your deck if you experience mana flood more frequently, in other words, equalize the frequencies of the two outcomes to the extent possible. However, I wondered if this might be the wrong goal.  Why should it matter if the probabilities of the two bad outcomes are the same?  What really matters is minimizing the probability that either outcome obtains, that is, maximizing the probability that an acceptable number of land cards gets drawn.  I tested this in Mathematica... As can be seen, these quantities exactly coincide.  As I change the parameters defining how many land cards constitute ""too few"" or ""too many,"" the minimum of the first graph moves in lockstep with the zero of the second graph.  This lends support to the ordinary course of action. Why does this happen mathematically?  It is not the case that any $f(x) + g(x)$ has extrema where $f(x) - g(x)$ has zeros.  Minimizing isn't a linear operator, so it's not clear that minimizing the sum relates to minimizing the summands.  Is there a clear reason that this happens here?  Is it an attribute of probability distributions?  The hypergeometric distribution in particular?","['optimization', 'probability-theory', 'probability']"
4663974,Order Statistic as a Consistent Estimator,"Problem. Let $X_{1},\ldots,X_{n}$ denote a random sample from the distribution with common pdf $$ f(x;\theta) = e^{-(x-\theta)} 1_{(\theta,+\infty)}(x), \;\; \theta \in \mathbb{R} $$ Let $Y_{n} = \min \{X_{1},\ldots,X_{n}\}$ . Is $Y_{n}$ a consistent estimator of $\theta$ ? Attempt. Note that if $ \displaystyle \lim_{n \to \infty} \mathbb{P}[|Y_n - \theta| < \epsilon]=1$ , then $Y_n$ is a consistent estimator for $\theta$ . The CDF of $X_{1},\ldots,X_{n}$ is \begin{align*}
F(x) &= \int_{0}^{x} e^{-(t-\theta)} \, dt \\
&= e^{\theta-x}(e^x-1) && x \in (\theta, \infty)
\end{align*} So $F_{Y_n}(x) = [e^{\theta-x}(e^x-1)]^n$ . Moreover, \begin{align*}
\lim_{n \to \infty} \mathbb{P}[|Y_n - \theta| < \epsilon] &= \lim_{n \to \infty} \mathbb{P}[\theta - \epsilon < Y_n < \epsilon + \theta] \\
&= \lim_{n \to \infty}  [F_{Y_n}(\theta + \epsilon) - F_{Y_n}(\theta - \epsilon)] \\
&= \lim_{n \to \infty} (e^{-\epsilon}(\epsilon^{\theta+\epsilon}-1))^{n} \\
&= \infty
\end{align*} So $Y_n$ is not a consistent estimator for $\theta$ . Is this correct?","['statistics', 'solution-verification']"
4663985,"$\mu,\nu$ are Borel pr. measures s.t. $|\int fd\nu-\int fd\mu|<\epsilon$ for some Lipschitz $f$ then the inequality holds for some bounded $g\in C(X)$","Let $(X, d)$ be a metric space and $\mu,\nu$ be two Borel probability measures over $X$ . Let $\epsilon > 0$ be fixed and suppose that $$\left|\int_X fd\nu - \int_Xfd\mu\right| < \epsilon$$ for some non-zero Lipschitz function $f:X\to \mathbb{R}$ . Does there then exists a bounded non-zero continuous function $g:X\to\mathbb{R}$ that $$\left|\int_X gd\nu - \int_Xgd\mu\right| < \epsilon$$ A book I am reading says essentially that yes, because Lipschitz functions are continuous. My initial idea was to cut $f$ with some thresholds, e.g. $g(x) = \chi_{\{t_1\leq f(x)\leq t_2\}}(x)f(x) + \chi_{\{f(x) < t_1\}}(x)t_1 + \chi_{\{f(x) > t_2\}}(x)t_2$ , but it is all but clear to me how I can choose $t_1,t_2$ such that $$\left|\int_X gd\nu - \int_Xgd\mu\right| < \epsilon$$ holds.","['measure-theory', 'metric-spaces', 'real-analysis']"
4664057,Doubts in $\lim_{x \to +\infty} \frac{\int_{-x}^x \frac{1}{y^2}dy}{x}$,"Consider the limit: $$\lim_{x \to +\infty} \frac{\int_{-x}^x \frac{1}{y^2}dy}{x}$$ Since $\int_{-x}^x \frac{1}{y^2}dy=\int_0^x \frac{1}{y^2}dy-\int_0^{-x} \frac{1}{y^2}dy$ , using Hopital's rule: $$\lim_{x \to +\infty} \frac{\int_{-x}^x \frac{1}{y^2}dy}{x}=\lim_{x \to +\infty} \frac{\frac{2}{x^2}}{1}=0$$ However, since $1/y^2$ is unbounded around $y=0$ , using the definition of improper integral: $$\int_{-x}^x \frac{1}{y^2}dy=\int_{-x}^0\frac{1}{y^2}dy+\int_0^x \frac{1}{y^2}dy=\lim_{a\to 0^+}\left(\int_{-x}^{-a}\frac{1}{y^2}dy+\int_a^{x}\frac{1}{y^2}dy\right)=2\lim_{a\to 0^+} \left[\frac{1}{a}-\frac{1}{x}\right]$$ And, for each $x \in\mathbb{R}\setminus\{0\}$ , the limit as $a\to 0^+$ is $+\infty$ . Some questions: (i) What's happening with the definition? After evaluating the integral with the definition I have to take another limit as $x\to+\infty$ , but the integral already diverges to $+\infty$ . Can I conclude that $\lim_{x \to +\infty}\int_{-x}^x \frac{1}{y^2}dy=+\infty$ because (very unprecisely) ""I am taking the limit as $x\to+\infty$ of something that already tends to $+\infty$ ""? More precisely: since the integral is $+\infty$ for each $x \in \mathbb{R}\setminus\{0\}$ , I would say that in particular it remains arbitrary large if $x$ is arbitrarily large. However, the $\delta$ such that $|a|<\delta \implies \int_{-x}^x \frac{1}{y^2}dy>M$ of the limit definition I must choose depends on $x$ too, because it must be $\frac{1}{a}-\frac{1}{x}>M$ . Is this dependence troublesome? (ii) Is the approach with Hopital's rule correct?","['integration', 'limits', 'improper-integrals']"
4664064,$g$ attains both maximum and minimum over $\mathbb{R}$. $f$ is continuous on range of $g$. Does $f\circ g$ necessarily attain maximum on $\mathbb{R}$?,"Let $g:\mathbb{R}\to\mathbb{R}$ be a function (not necessarily continuous) which attains both its maximum and minimum on $\mathbb{R}$ . Let $f:\mathbb{R} \to \mathbb{R}$ is a function which is continuous on range of $g$ . Does $f\circ g$ necessarily have a maximum on $\mathbb{R}$ ? Prove or provide a counterexample. My Solution: I don't think that it necessarily attains its maximum on $\mathbb{R}$ . I had thought of the following example: Let $$g(x) := \begin{cases}
x^2 &\text{if}\; 0<x\leq 10 \\\; 1&\text{otherwise}\\
\end{cases}$$ The range of $g$ is $(0,100]$ . let $f(x) = \frac{1}{\sqrt x}$ . Note that $f$ is continuous on the range of $g$ . $$(f \circ g )(x)=
\begin{cases}
\frac{1}{x}&\text{if}\;0<x\leq 10\\ 1&\text{otherwise}\\
\end{cases}$$ $(f \circ g )(x)$ does not attain its maximum on $\mathbb{R}$ . This is the example that I could come up with, however, $g(x)$ doesn't attain its minimum on $\mathbb{R}$ . I am unable to tweak it in a way so as to ensure that it does and the rest of it holds as well. Another example that I could come up with was the following: Let $$g(x) := \begin{cases}
1+x&\text{if}\; 0<x\leq100\\ 
-1&\text{otherwise}
\end{cases}$$ The range of $g$ is $(1,101] \cup \{-1\}$ . Let $f(x) := \frac1x$ for all $x\in \mathbb{R}\backslash\{0\}$ . $$(f\circ g)(x)=\begin{cases}
\frac1{1+x}&\text{if}\;0<x\leq 100\\ -1&\text{otherwise}
\end{cases}$$ $g$ attains both maximum and minimum and $f\circ g$ does not attain its maximum. But I am unsure if $f$ is continuous on range of $g$ or not. $\frac1x$ is continuous everywhere except at $x=0$ . Is that justification enough?","['optimization', 'functions', 'continuity', 'real-analysis']"
4664079,Convergence in the formulation of the spectral theorem,"Let $\mathcal H$ be a complex (separable, if needed) Hilbert space and $B(\mathcal H)$ the ring of bounded operators on $\mathcal H$ . I am interested in understanding the formulation of the spectral theorem (for self-adjoint, possibly unbounded operators) in terms of resolutions of the identity. A function $E:\mathbb R\rightarrow B(\mathcal H)$ is a resolution of the identity if for each $\lambda\in\mathbb R$ , $E(\lambda)$ is an orthogonal projection; for each $\lambda_0<\lambda_1$ , $E(\lambda_0)\le E(\lambda_1)$ ; the function $E(\lambda)$ is right-continuous; $\lim_{\lambda\rightarrow-\infty} E(\lambda)=0$ and $\lim_{\lambda\rightarrow\infty}E(\lambda)=I$ where $I$ is the identity operator. One then defines a self-adjoint (possibly unbounded) operator $A$ through $$ A=\int_{-\infty}^{+\infty}\lambda\, \mathrm dE(\lambda). \qquad(\ast)$$ The spectral theorem then states that corresponding to every densely defined self-adjoint  operator on $\mathcal H$ there is a unique resolution of the identity such that $(\ast)$ is true. I do have access to a number of references that treat the spectral theorem this way in a reasonably rigorous manner but nonetheless I am a bit confused about the various definitions of convergence that appear in the statements above. So first, as I understand, if $A_n$ is a sequence of bounded operators on $\mathcal H$ , we have the strong convergence $A_{n}\overset{s}{\longrightarrow}A$ if for each $x\in\mathcal H$ , $A_nx\longrightarrow Ax$ and the weak convergence $ A_{n}\overset{w}{\longrightarrow}A $ if for each $x,y\in\mathcal H$ we have $ \langle x,A_n y\rangle\rightarrow \langle x,Ay\rangle $ (I am a physicist so for me the inner product is linear in the second variable). Now I assume that since each operator $A$ is uniquely determined by the sesquilinear form $\langle x,Ax\rangle$ through the polarization identity, it is true that if $\langle x,A_nx\rangle\rightarrow\langle x,Ax\rangle$ , then $A_n\overset{w}{\longrightarrow A}$ is that correct ? Then the first question is Q1 : In property 3. and 4. of the resolution of identity, the right continuity of the family $E(\lambda)$ and the limits $E(\lambda)$ as $\lambda\rightarrow\pm\infty$ are meant in the sense of weak or strong convergence, and why? I think it is irrelevant, because from what I can tell, for orthogonal projections the weak and strong convergence coincides, but I often find functional analysis so counterintuitive I am not sure I trust my proof. Basically suppose that $P_n$ is a sequence of orthogonal projections converging weakly to $P$ , i.e. $\langle x,P_ny\rangle\rightarrow\langle x,Py\rangle$ , then we have $$ \Vert P_nx-Px\Vert^2=\Vert P_nx\Vert^2+\Vert Px\Vert^2-2\mathrm{Re}\langle P_nx,Px\rangle, $$ but $\langle x,P_nx\rangle=\langle P_nx,P_nx\rangle=\Vert P_nx\Vert^2$ , hence $\Vert P_nx\Vert\rightarrow \Vert Px\Vert$ and thus $$ \lim_{n\rightarrow\infty}\Vert P_nx-Px\Vert^2=2\Vert Px\Vert^2-2\langle Px,Px\rangle=0, $$ thus $P_n\overset{s}{\longrightarrow} P$ as well. Is this correct? I have more problems with interpreting the integral $(\ast)$ defining $A$ . I have often seen it being meant that for any $x\in \mathcal H$ , we have $$ \langle x,Ax\rangle=\int_{-\infty}^{+\infty}\lambda\,\mathrm d\langle x,E(\lambda)x\rangle, \qquad(\ast\ast)$$ where the RHS is an ordinary Riemann-Stieltjes integral with respect to the function $P_x(\lambda):=\langle x,E(\lambda)x\rangle$ . Due to the polarization identity, this does determine $A$ uniquely and I guess $x$ belongs to the domain of $A$ if and only if the above Stieltjes integral converges. Q2 : Is there any direct expression for the action $Ax$ ? I would intuitively think that the formula for $\langle x,Ax\rangle$ implies that for any $x\in\mathcal H$ (or at least in the domain of $A$ ) we have $$ Ax=\int_{-\infty}^{+\infty}\lambda\,\mathrm d(E(\lambda)x), \quad(\ast\ast\ast)$$ and as far as I am aware, Stieltjes integrals with values in a Hilbert space do make sense (hence "" $\mathrm d(E(\lambda)x)$ "" can be interpreted), but the fact that weak convergence does not coincide with strong convergence in general makes me think that $(\ast\ast)$ does not imply $(\ast\ast\ast)$ . So does $(\ast\ast\ast)$ makes sense and if so when?","['operator-theory', 'spectral-theory', 'functional-analysis']"
4664082,Is there a way to show that this ode system is asymptotically stable?,"Suppose we have $$\dot{x} = -\frac{x}{y+a} $$ $$\dot{y} = -y$$ for $a>0$ , Is the above system asymptotically stable? Now, I know that we can solve for $y$ as $$y = y(0) e^{-t}$$ and we can choose for the $x$ part of the system, a Lyapunov function as $V(x) = 0.5 x^2$ and thus obtain the derivative: $$\dot{V} = -\frac{x^2}{y(0) e^{-t} + a}$$ which $\lim\limits_{t \rightarrow \infty} \dot{V} =  -\frac{x^2}{a}$ . But, I don't think this is correct. Is there a way to prove the $x,y$ system is stable, just by finding a positive definite Lyapunov function and proving that its derivative is negative definite?","['stability-in-odes', 'lyapunov-functions', 'ordinary-differential-equations']"
4664121,Rotating $y=x-2\sqrt{x}+1$ by $45^\circ$ counter-clockwise [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I tried to rotate the equations $$y=x-2\sqrt{x}+1 \quad\text{and}\quad y=x+2\sqrt{x}+1$$ (because of the two possible answers) $45^\circ$ counter-clockwise by using $$y \cos(\theta)- x \sin(\theta)=(f(y \sin(\theta))+x \cos(\theta))$$ and ended up getting $$2x^2=\sqrt{8y^2}-2(4 \sqrt{8y^2})+1$$ This is wrong because the vertex of the parabola should be on the $y$ axis at the $\sqrt{\dfrac{1}{8}}$ . Where did I go wrong? Was it with $$y \cos(\theta)-x \sin(\theta)=(f(y \sin(\theta))+x \cos(\theta))$$ or did I mess something else up along the way? How can I prevent from doing that mistake in the future?","['trigonometry', 'geometry']"
4664156,Solution of system of nonlinear equations with trigonometric terms,"Issue: I am trying to solve the following system of nonlinear equations for the unknown variables: $x$ , $z$ and $\beta$ . The remaining variables are known values. $$a=u(s^2+(x\cos\beta\ )^2+(z\sin\beta\ )^2\ ) \tag{eq. 1}\label{eq1}$$ $$b=u(y^2+(x\sin\beta\ )^2+(z\cos\beta\ )^2\ ) \tag{eq. 2}\label{eq2}$$ $$c=y \cdot s+(x^2-z^2\ )\ \ 1/2\ \sin2\beta \tag{eq. 3}\label{eq3}$$ It can be assumed that all variables are real numbers and the following may also be assumed positive: $a$ , $b$ and $u$ . Any guidance on how to best approach this question would be highly appreciated. What I have tried so far: I have tried solving it with substitution, simplifying wherever I can, but the equations expand drastically and become too complex to handle for me. An alternate route I tried was using a symbolic solver in python (sympy), but this also struggles with coming up with a solution. Moreover, I have tried adding eq. 1 & 2 together, which produces the following more ""simple"" equation utilizing Pythagorean identity, but issues with complex expansion still persists: $$a+b=u\left(y^2\ +s^2+x^2+z^2\ \right) \tag{eq. 4}\label{eq4}$$","['nonlinear-system', 'trigonometry', 'systems-of-equations']"
4664170,What is the area of intersection between two circular sectors? (Where can I find more information?),"I am trying to find an expression for the area of the intersection of two circular sectors.
There are some obvious and trivial solutions for some special cases,  but I'm trying to generalize. Here is an example of the kind of problem I'm envisioning: I have tried to work out ways of dissecting the shapes and calculating areas with trigonometry or integration.
At the moment, I'm playing with polar coordinates and integrating, treating the centre of A as the origin, then generating expressions for each line of B's perimeter. I've also been trying to find examples of where this might have been solved before. So far, though, I haven't found anything useful. Are there any formulae that I should be looking at? Does this kind of problem crop up anywhere else that I could research? Even better, does anyone have a good starting point so I can derive it myself?","['area', 'geometry']"
4664200,When is the tangent space of a fiber product the fiber product of the tangent spaces?,"If $X,Y,Z$ are schemes locally of finite type over an algebraically closed field of characteristic 0, and $X\to Z$ and $Y\to Z$ are morphisms, then is it true that $T_{(x,y)}(X\times_Z Y)\simeq T_x(X)\times_{T_z(Z)}T_y(Y)$ (where $z$ is the image of $x$ and $y$ in $Z$ )? By the universal property of fiber products we have a natural map $T_{(x,y)}(X\times_Z Y)\to T_x(X)\times_{T_z(Z)}T_y(Y)$ . Moreover, seeing each element of $T_x(X)$ , for example, as a morphism from the dual numbers to $X$ with set-theoretic image $x$ , then we get, again by the universal property, a map in the other direction. Are these inverses of each other? It seems they should be but I'm nervous about missing some subtlety here.","['algebraic-geometry', 'schemes']"
4664212,Find $P(X=k)$ where $X$ is the random variable that represents the number of draws required to obtain a white ball,"Consider a box containing one black ball and one white ball. Every time a black ball is drawn, a die is rolled and a number of black balls equal to the result of the die are added to the box. We want to determine $P(X=k)$ where $X$ is the random variable that represents the number of draws required to obtain a white ball. Examples I found: $$\begin{split}
P(X=1)&=\frac 12\\
P(X=2)&=\frac 1{24}\\
P(X=3)&=\dfrac 1{2.6^2}\displaystyle \sum_{k=1}^6\dfrac {k+1}{k+2}\sum_{i=1}^{6}\dfrac 1{i+k+1}= 0.0472490588115\dots$
\end{split}$$ it appears that there is no elegant formula for $P(X=k)$ .","['conditional-probability', 'combinatorics', 'probability']"
4664227,"$f(x, y, z)$ with both $\leq$ and $=$ constraints. General questions.","I need to ask you for this question, which is a rather general one, in order to understand how to behave when studying maxima and minima with constraints, in many variables. The specific question is the following: suppose I have some $f(x, y, z)$ (in this case I'm specifically asking for three variables) subject to two constraint: $$ \begin{cases} g(x, y, z) \leq 0 \\\\ h(x, y, z) = 0\end{cases}$$ Question: I use Langrange multipliers in the usual way, by building the Lagrangian $$L(x, y, z, \lambda, \mu) = f(x, y, z) - \lambda g(x, y, z) - \mu h(x, y, z)$$ (then I know how to proceed). Say I will be able to reduce the problem to a two dimensiona one, and I cannot go further than this. Then I study $\nabla f(x, y) = (0, 0)$ and I hopefully find some points. What now? Should I use them to find the third point $z$ by using the constraints, or should I study $f(x, y)$ with the Hessian matrix? Thank you! AN EXAMPLE Just to make things more concrete, I thought to write here an example for what I meant. Say $$f(x, y, z) = xyz$$ Subject to $$\begin{cases} x^2+y^2+z^2 \leq 1 \\\\ x+y+z = 1 \end{cases}$$ So a sphere and a plane, which intersect to form a cicumference. $$L(x, y, z, \lambda, \mu) = xyz - \lambda(x^2+y^2+z^2-1) - \mu(x+y+z-1)$$ And the derived equations read $$
\begin{cases}
yz = 2\lambda x + \mu \\
xz = 2\lambda y + \mu \\
xy = 2\lambda z + \mu \\
\text{the two constraints}
\end{cases}
$$ I was able to write down: $$\mu = zy - 2\lambda x$$ hence from arranging the second with this one: $$\lambda = \frac{-z}{2}$$ And then subtituting all in the third: $$xy = -z^2 + zy + zx$$ Using the second constraint for $z \to z = 1-x-y$ I conclude with $$f(x, y) = 2x^2+ 2y^2-3x-3y+xy +1$$ So at this point: $\nabla f(x, y) = (0, 0)$ gives the points $$x = 0 \qquad \qquad y = 0$$ $$x = \frac{3}{5} \qquad \qquad y = \frac{3}{5}$$ and fron this, question two: should I find $z$ and then simply evaluate $f$ on this points, or should I go on with $f(x, y)$ with eh Hessian? Bonus question: am I reasoning right or wrong? Am I missing something? EDIT I understood that with mixed constraints I have to set up Kuhn Tucker conditions. So I did it and the result reads: $$\begin{cases}
x^2+y^2+z^2 -1 \leq 0 \\
x+y+z-1 = 0 \\
\lambda(x^2+y^2+z^2-1) = 0 \\
\lambda \geq 0 \\
\nabla L = 0
\end{cases}
$$ And the last one, creating $$L(x, y, z, \lambda, \mu) = xyz - \lambda(x^2+y^2+z^2-1) -\mu(x+y+z-1) $$ then reads $$\begin{cases}
yz - 2\lambda x-\mu = 0 \\
xz - 2\lambda y - \mu =0 \\
xy - 2\lambda z - \mu = 0
\end{cases}
$$ Now, for the case $\lambda = 0$ , all beautiful, I solve and obtain the point $\left(\frac{1}{3}, \frac{1}{3}, \frac{1}{3}\right)$ Which shoudl be a max (?) When $\lambda \neq 0$ the mess starts. Arranging, I obtain $\lambda = -\frac{z}{2}$ , which results in $$z^2 - z(x+y) + xy = 0$$ And using th constraints I can reduce it to $$3xy - x - y+1 = 0$$ And I am stuck. If I try the gradient, I get the same points I got above. W. Mathematica says $(\text{ at } x,y,z) \left(\min  \left\{x y z\left|x^2+y^2+z^2\leq 1\land x+y+z=1\right.\right\}=-\frac{4}{27}\right)=\frac{2}{3},\frac{2}{3},-\frac{1}{3}$ And at two other very similar points, but $z$ is not negative in those ones...","['lagrange-multiplier', 'maxima-minima', 'multivariable-calculus', 'optimization', 'constraints']"
4664232,Limit of the sum of cosine,"I want to find the value of $$\sum_{k=0}^\infty \cos\left[\left(k+\frac{1}{2}\right)\pi x\right]\cos\left[\left(k+\frac{1}{2}\right)\pi t\right].$$ I think it should relate to delta function because we can use a trig identity to get: $$\sum_{k=0}^\infty \frac{1}{2}\left(\cos\left[\left(k+\frac{1}{2}\right)\pi(t+x)\right]-\cos\left[\left(k+\frac{1}{2}\right)\pi(t-x)\right]\right)$$ , but how to deal with the $\frac{1}{2}\pi (t+x)$ ?","['complex-analysis', 'limits', 'dirac-delta', 'real-analysis']"
4664258,height of regular sequence modulo minimal prime,"Liu Qing's book Algebraic Geometry and Arithmetic Curves has the following statement on 6.3.11: Let $Y$ be a locally Noetherian scheme and $i \colon X \to Y$ a regular immersion of codimension $n$ . Then for every irreducible component $Y_i$ of $Y$ , then $\mathrm{codim}(X\cap Y_i, Y_i) = n$ provided $X \cap Y_i \ne \emptyset $ . After passage to localization, the statement may be reformulated as follows: Let $A$ be a Noetherian local ring with regular sequence $x_1,\dots, x_n$ . For any minimal prime ideal $\mathfrak{q}$ of $A$ , we have $\operatorname{ht}((x_1,\dots,x_n) \mod \mathfrak{q}) = n$ in the ring $A/\mathfrak{q}$ . For $n = 1$ this is exactly Krull's principal ideal theorem. The book claims that one can prove the general statement by induction on $n$ but I can't quite see the issue.","['algebraic-geometry', 'commutative-algebra']"
4664266,"$(\beta_n)_{n \geq 0}$ converges uniformly to the solution of $x' = F(t,x)$, variation of Picard iteration?","This is exercise 2.7. from Differential Equations: A Dynamical Systems Approach to Theory and Practice by Marcelo Viana and José Espinar. Let $F \colon \mathcal{U} \to \mathbb{R}^n$ be continuous and locally
Lipschitz in $x$ in an open subset $\mathcal{U}$ of $\mathbb{R}\times
> \mathbb{R}^n$ . Let $K$ be a compact subset of $\mathcal{U}$ which is
also convex in the second variable, and let $(t_0, x_0)$ be a point in $K$ . Let $I \subseteq \mathbb{R}$ be any open interval containing $t_0$ and suppose that there exist $x_n \in \mathbb{R}^n, n \geq 0,$ converging to $x_0$ and curves $\beta_n \colon I \to \mathbb{R}^n, n
> \geq 0$ such that $$ (t, \beta_n(t)) \in K \quad \text{and}\quad
\beta_{n+1}(t) = x_n + \int_{t_0}^t  F(s, \beta_n(s) \,ds 
$$ for every $t \in I$ and all $n \geq 0$ . (1) Show that there exists $\varepsilon > 0$ such that, restricted to $(t_0 - \varepsilon, t_0 + \varepsilon)$ , the sequence $(\beta_n)_n$ converges uniformly to the solution of $x' = F(t,x)$ with initial
condition $x(t_0) = x_0.$ (2) Deduce that $(\beta_n)_n$ converges uniformly to the solution of $x' = F(t, x)$ with initial condition $x(t_0) = x_0$ in any compact
subinterval. I am already struggling with the first item. Note that this is Homework, so I am not looking for a full solution (at this time), but only hints. Picard's Theorem, or the proof of it, should be useful for this. I tried to proceed as in the proof but there are several points where I got stuck. What I have: If I define the space $$
Y = \{\gamma\colon (t_0 - \varepsilon, t_0 + \varepsilon) \to K' \text{ continuous} \mid \sup_{t \in (t_0 - \varepsilon, t_0 + \varepsilon)} |\gamma(t) - x_0| \leq \alpha\},
$$ where $K'$ is the projection of $K$ to $\mathbb{R}^n$ , I can show that the operator defined as $$
\mathcal{L}(\gamma)(t) = x_0 + \int_{t_0}^t F(s, \gamma(s))\, ds
$$ is well defined and a contraction for suitably chosen $\varepsilon$ and $\alpha$ . (This is basically just the proof of Picard's Theorem.) The unique fixed point of $\mathcal{L}$ will be the solution of $x' = F(t,x)$ . I was also able to show that for $n$ large enough, $\beta_n \in Y$ . This means in particular that taking such $\beta_n$ and applying the Picard operator, I get something that converges to the solution uniformly, i.e., $$
\mathcal{L}^k(\beta_n) \to \beta
$$ as $k \to \infty$ , where I denote by $\beta$ the unique solution of the ODE. However, this is not quite the convergence I want since I would like to have $\beta_n \to \beta$ . I don't see a way how to salvage this. I also tried to define a slightly different operator, namely $$
\mathcal{T}(\gamma)(t) = \gamma(t_0) + \int_{t_0}^t F(s, \gamma(s))\, ds,
$$ which would seem to work nicer with the $\beta_n$ since $$
\mathcal{T}(\beta_n)(t) = \beta_n(t_0) + \int_{t_0}^t F(s, \beta_n(s))\, ds = x_{n-1} + \int_{t_0}^t F(s, \beta_n(s))\, ds,
$$ however, this last expression is not quite $\beta_{n+1}(t)$ . Further, I can't show (and don't know if it is true) that this is a contraction. I am looking for either a hint how to continue with one of my approaches, or a hint where else to start. Thanks!","['fixed-point-theorems', 'analysis', 'uniform-convergence', 'ordinary-differential-equations']"
4664282,"What does $\exists x\left(L\left(x,x\right)\wedge \forall z\left(L\left(x,z\right)\rightarrow \left(z=x\right)\right)\right)$ mean?","I translated ""There is someone who loves no one besides himself or herself"" as $$\exists x\left(L\left(x,x\right)\wedge \forall z\left(L\left(x,z\right)\rightarrow \left(z=x\right)\right)\right)$$ but the textbook gave $$\exists x \forall y \Bigl(L(x,y) \leftrightarrow (x=y)\Bigr)$$ Where $L(x, y)$ means "" $x$ loves $y.$ "" As I understand the first one says : ""There exists a person such that, he loves himself and for all people, if this person loves someone then this someone is himself"" The second one says : ""There is a person for all people such that this person loves someone if and only if they are both the same(it's himself)"" Now I wonder are these two formulas equivalent?","['quantifiers', 'predicate-logic', 'discrete-mathematics', 'logic-translation']"
4664286,On the convergence of approximate units for C*-algebras.,"Let $A$ be a non-unital C*-algebra and let $\pi : A \to \mathcal{B}(H)$ be a non-degenerate representation of $A$ (that is, $\mathrm{ span }\{\pi(a)h : a \in A, h \in H\}$ is a dense subset of $H$ ). Now let $(u_\lambda)_{\lambda \in \Lambda}$ be an approximate unit for $A$ . A standard argument, using non-degeneracy, shows that $(\pi(u_\lambda))_{\lambda \in \Lambda}$ converges strongly to $1_H$ , the identity operator on $H$ . There are several examples that show that $(\pi(u_\lambda))_{\lambda \in \Lambda}$ is not norm convergent. However, below I will present an argument that ""shows"" that $(\pi(u_\lambda))_{\lambda \in \Lambda}$ converges in norm to $1_H$ . This is certainly at fault, but I want to make sure I understand exactly where the proof goes wrong. Argument: Let $\epsilon >0$ and choose $h \in H$ with $\|h\|=1$ such that $$
\| 1_H - \pi(u_\lambda) \| < \epsilon + \| h - \pi(u_\lambda)h \|.
$$ Then, since strong convergence of $(\pi(u_\lambda))_{\lambda \in \Lambda}$ implies that $(\| h - \pi(u_\lambda)h \|)_{\lambda \in \Lambda}$ converges to $0$ , we find $\lambda' \in \Lambda$ such that for all $\lambda \geq \lambda'$ , $$
\| h - \pi(u_\lambda)h \| < \epsilon.
$$ Thus, for any $\lambda \geq \lambda'$ , we have shown $$
\| 1_H - \pi(u_\lambda) \| < 2\epsilon,
$$ which proves that $\pi(u_\lambda)$ converges in norm to $1_H$ . End of argument. I am pretty sure that the problem in the above argument is that $h$ depends
on $\epsilon$ and therefore also $\lambda'$ . Thus, the expression $\| h - \pi(u_\lambda)h \| < \epsilon$ for $\lambda \geq \lambda'$ doesn't make much sense to me. In any case, I am still not quite convinced about this and I would love to hear someone else's thoughts. Thanks in advance!","['hilbert-spaces', 'c-star-algebras', 'functional-analysis', 'operator-algebras']"
4664293,Use of Fubini's Theorem in Papa Rudin's Holomorphic Fourier Transforms,"I am starting to read on chapter 19, Holomorphic Fourier Transforms from Real and Complex Analysis by Walter Rudin. In the first page of that chapter I came across the function $$f(z) = \int_0^\infty F(t)e^{itz} dt$$ where $z\in \mathbb{C}$ is in the upper half plane, $F\in L^2(\mathbb{R})$ and $F$ vanishes on $(-\infty, 0)$ . I want to prove that $f$ is analytic. I proved that $f$ is continuous in $\mathbb{C}$ . To prove that it is holomorphic, the author hints to use Morera's theorem, for which I have to show that for any closed path $\gamma$ in $\mathbb{C}$ , the integral $$\int_\gamma f(z) dz =0$$ My course of proof is to show \begin{align*}
    \int_\gamma f(z)\ dz &= \int_\gamma \int_0^\infty F(t) e^{2\pi i t z} \ dt \ dz \\
    & = \int_0^\infty F(t) \int_\gamma e^{2\pi itz} \ dz \ dt \\
    & = \int_0^\infty F(t) \cdot 0 \ dt \\
    & = 0
  \end{align*} But I can't see how the use of Fubini's theorem is justified in the change of order of integration.","['measure-theory', 'fourier-analysis', 'fourier-transform', 'real-analysis', 'complex-analysis']"
4664346,How I can solve this nonlinear second order ODE?,"I am trying to solve a non-linear ODE. Let $g(x)$ is an arbitrary smooth real function, I want to solve the following equation $2f(x)f''(x) +4(f'(x))^2=g(x)$ for $f(x)$ .
Multiplying $\dfrac32 f(x)$ , I obtain $(f^3(x))''=\dfrac32f(x)g(x)$ . In the following case I have solved the Eq. $\bullet$ If $g(x)$ identically be zero, then $f(x)=\sqrt[3]{ax+b}$ , for $a,b\in \mathbb{R}$ . $\bullet$ If $g(x)=\alpha f^2(x)$ for some constant $\alpha$ , then by taking $f^3(x)=F(x)$ , the equation can be solved. In fact, we have $F''(x)=\dfrac32 \alpha F(x)$ . But, in general case of $g(x)$ , I do not know how solve the equation. Any hint is highly appreciated.",['ordinary-differential-equations']
4664356,Isometric invariance of riemannian distance function,"I need proof verification on the following theorem (Lee intro to Riemannian Manifolds Prop 2.51): Let $\varphi : (M,g) \longrightarrow (\tilde{M},\tilde{g})$ be an isometry between connected riemannian manifolds, then for all $x,y \in M$ we have $$d_g(x,y)=d_{\tilde{g}}(\varphi(x),\varphi(y)),$$ $d_g$ and $d_{\tilde{g}}$ being the Riemann distance function on $M$ and $\tilde{M}$ respectively. I start by proving that if $\varphi$ is a local isometry, then $$d_g(x,y)\geq d_{\tilde{g}}(\varphi(x),\varphi(y)) \quad \forall x,y\in M.$$ If then $\varphi$ is an isometry, then both $\varphi$ and its inverse are local isometry and I can conclude that equality holds. Let $x,y \in M$ and $\gamma : [a,b] \longrightarrow M$ be an admissible curve (piecewise regular) joining $x$ and $y$ in $M$ , and here I'm assuming $\gamma$ is injective, and let $\Gamma = \gamma([a,b])$ . Observe that $\Gamma$ is compact by continuity of $\gamma$ . Since $\varphi$ is a local isometry, for each $p\in \Gamma$ we find a neighborhood $U_p$ in $M$ such that $\varphi\big|_{U_p}$ is an isometry. Since $\{U_p\}_{p\in \Gamma}$ is an open cover of $\Gamma$ we find a finite subcover $\{U_0,\ldots,U_n\}$ .
Define $\Gamma_i = U_i \cap \Gamma$ . What I have in mind here is that, after suitable reordering of the indexes and refinement, that $\Gamma_i$ is a partition of $\Gamma$ , meaning $[t_i,t_{i+1}]=\gamma^{-1}(\overline{\Gamma_i})$ and $a=t_0<\ldots<t_{n+1}=b$ is a partition of $[a,b]$ . I can now write \begin{align}
L_g(\gamma)&=\sum_{i=0}^{n} L_g(\gamma|_{[t_i,t_{i+1}]})=\sum_{i=0}^{n} L_{\tilde{g}}((\varphi \circ \gamma)|_{[t_i,t_{i+1}]}) \tag{1}\\
&\geq \sum_{i=0}^{n} d_{\tilde{g}}(\varphi(x_i),\varphi(x_{i+1})) \geq d_{\tilde{g}}(\varphi(x),\varphi(y)) \tag{2}
\end{align} where $x_i=\gamma(t_i), \quad i=1,\ldots,n$ . The first equality follows by definition of local isometry and the second inequality from the triangle inequality. Since $\gamma$ is arbitrary I conclude that $d_g(x,y)\geq d_{\tilde{g}}(\varphi(x),\varphi(y)).$ In the exercise Lee asks for a counterexample showing that local isometries don't preserve distances, I thought of $$\varphi : [0,2\pi] \longrightarrow \mathbb{S}^1, \quad \varphi(t)=(\cos t,\sin t),$$ which is clearly a local isometry but not injective.","['isometry', 'riemannian-geometry', 'differential-geometry']"
4664401,Probability and Random Variables.,"Hi, I was trying to understand this example in the book. In the first part of the question, We've to find p.d.f ( probability density function ). For that, we take the derivative of the given distribution function F(x) and it is done.👍 But in another part of the question we've to find P(|X| < 1.5) and we have to integrate for p.d.f (Probability distribution function), Now I am confused about the ranges of those integrals. Why we must divide the range into these four pieces i.e, (-∞, -1.5), (-1.5 , 0), (0 , 1), (1 , 1.5) ? for integration. My question is, if We're trying to find P(|X| < 1.5) then why do we must divide the range of integration into these four pieces? Why is the range is going from -∞ to -1.5 (in the first integral) and not to 0?
I'm confused about where did -1.5 come from? I mean, why we are including -1.5 in the range of integrals? I am assuming, it is because of the || (mod) thing. Kindly correct me if I'm wrong.
Thank you.","['statistics', 'definite-integrals', 'probability-theory', 'probability', 'random-variables']"
4664407,Pettis integral on locally convex space and seminorms,"Let $E$ be a locally convex Hausdorff space, and $X$ be a locally compact Hausdorff space which we fix a positive Radon measure $\mu$ . Assume that $f: X \to E$ is a function such that the Pettis-integral $$\int_X f d \mu$$ exists. Let $p$ be a continuous seminorm on $E$ . Is it true that the inequality $$p\left(\int_X f d\mu\right)\le \int_X (p\circ f) d\mu$$ holds? Context: This seems to be implicitly used in the proof of lemma 2.4 chapter VI ""Left Hilbert Algebras"" in Takesaki's second book ""Theory of operator algebras"".","['integration', 'measure-theory', 'operator-algebras', 'topological-vector-spaces', 'locally-convex-spaces']"
4664468,Why does the semigroup of matrices form an epigroup?,"An epigroup is a semigroup $S$ in which every element has a (positive) power that lies in a subgroup of $S$ . (The subgroup may depend on the element). Note that if $x^n\in G$ , where $G$ is a subgroup of $S$ , then $x^m\in G$ for all $m\ge n$ . It is stated on the wiki article on epigroups that: The semigroup of all matrices over a division ring is an epigroup. It seems to be taken directly from their source, The Concise Handbook of Algebra (Shevrin 2002), though I think what they mean must be the following (otherwise we can't talk of a semigroup at all): The semigroup of all square matrices of a given size over a division ring is an epigroup. Why is this true? I believe I have stumbled my way to a proof that works when the matrices are over an algebraically closed field, since this lets us use Jordan normal forms, but I don't know if it extends to even the case of general fields. Going from general fields to skew fields seems even harder, since this leaves the realm of standard linear algebra with which I'm familiar. I do suspect that there are nicer, higher-level approaches that don't use a concrete structure of the matrices like Jordan matrices. I would appreciate a more abstract proof. In my proof, we assume without loss of generality that $X$ is a Jordan matrix. This of course requires the existence of a Jordan normal form for any matrix, which is why I think I need an algebraically closed field. Let $n$ be the size of the largest Jordan block with eigenvalue $0$ . Then the nilpotent Jordan blocks of $X$ have vanished in $X^n$ , so we get a block matrix of the form $$
X^n = \begin{pmatrix}J_{\lambda_1}^n\\&\ddots\\&&J_{\lambda_k}^n\\&&&0\end{pmatrix},
$$ where $J_{\lambda_j}$ is a Jordan block of eigenvalue $\lambda_j\ne0$ . Let $E$ to be the diagonal matrix with $1$ 's at all the non-zero diagonal places of $X^n$ . Then $E$ is an idempotent, and $X^nE=X^n=EX^n$ . Furthermore, Jordan blocks are invertible (by the formula in this answer ), so if we take $$
Y=\begin{pmatrix}J_{\lambda_1}^{-n}\\&\ddots\\&&J_{\lambda_k}^{-n}\\&&&0\end{pmatrix},
$$ then $YX^n=E=X^nY$ , which shows that $X^n$ is included in a subgroup with identity $E$ . Translating the results to general (non-Jordan) $X$ is easy from here.","['jordan-normal-form', 'matrices', 'abstract-algebra', 'linear-algebra', 'semigroups']"
4664542,Why is studying centralizers the/a key to classifying finite groups?,"In this MO thread https://mathoverflow.net/questions/38161/heuristic-argument-that-finite-simple-groups-ought-to-be-classifiable , Borcherds says One problem, as least with the current methods of classification via centralizers of involutions, is that every simple group has to be tested to see if it leads to new simple groups containing it in the centralizer of an involution. For example, when the baby monster was discovered, it had a double cover, which was a potential centralizer of an involution in a larger simple group, which turned out to be the monster. The basic theorem about centralizers and involutions is https://en.wikipedia.org/wiki/Brauer%E2%80%93Fowler_theorem , whose Wikipedia page states Perhaps more important is another result that the authors derive from the same count of involutions, namely that up to isomorphism there are only a finite number of finite simple groups with a given centralizer of an involution. This suggested that finite simple groups could be classified by studying their centralizers of involutions, and it led to the discovery of several sporadic groups. Later it motivated a part of the classification of finite simple groups. Here are some neat write-ups on Brauer-Fowler that I thought were nice: http://thomasbloom.org/notes/brauer.html , https://terrytao.wordpress.com/2013/05/02/quasirandom-groups-and-a-cheap-version-of-the-brauer-fowler-theorem/ . Looking at https://en.wikipedia.org/wiki/Classification_of_finite_simple_groups#Timeline_of_the_proof , we see that 1955  The Brauer–Fowler theorem implies that the number of finite simple groups with given centralizer of involution is finite, suggesting an attack on the classification using centralizers of involutions. ... 1957  Suzuki shows that all finite simple CA groups of odd order are cyclic. ... 1960  Feit, Marshall Hall, and Thompson show that all finite simple CN groups of odd order are cyclic. ... 1963  Feit and Thompson prove the odd order theorem. Where CA and CN stand for ""all centralizers are abelian"" and ""all centralizers are nilpotent"". This article https://www.mat.uniroma2.it/~eal/Suzukithm.pdf points out the relation between the CA, CN, and Feit-Thompson theorems: The CA theorem tells us that groups whose non-identity elements have abelian centralizers are solvable. The CN theorem tells us that groups whose non-identity elements have nilpotent centralizers are solvable. The F-T theorem tells us/is equivalent to the statement that odd-order groups whose non-identity elements have solvable centralizers are solvable. That article also says that In the odd order
paper Feit and Thompson proceed by analyzing the structure of the centralizers inside a
hypothetical minimal counterexample to their theorem. In this blogpost , Terry Tao describes a ""ladder"" to the CFSG , with rungs in particular going from CA theorem (to the CN theorem) to the F-T theorem. In summary : going up this ladder we are considering groups with (progressively less and less) nice properties regarding the centralizers of elements and still trying to prove that it is solvable (resulting in the CA, CN, and F-T theorems); and in the separate (but still related of course) subject of finding new simple groups, as Borcherds said we again focus on centralizers (this time of just involutions). So it appears that at many different stages of proving the CFSG --- from the very ""beginning"" in "" 1955 The Brauer–Fowler theorem implies that the number of finite simple groups with given centralizer of involution is finite, suggesting an attack on the classification using centralizers of involutions."", to the late stage of discovering the Monster group --- people relied on this philosophy/idea of focusing on/studying centralizers. Question: why is this? Why should having some information on what elements commute with each other in a group, and/or properties about that set of commuting elements (e.g. that it is abelian, nilpotent, etc.), tell you things about the group as a whole, and moreover give classification results about whole families of groups? P.S. in the above-linked Terry blogpost on Brauer-Fowler , Terry does say ...the actual proof of the CFSG does not quite proceed along these lines. However, a significant portion of the argument is based on a generalisation of this strategy, in which the concept of a centraliser of an involution is replaced by the more general notion of a normaliser of a {p}-group, and one studies not just a single normaliser but rather the entire family of such normalisers and how they interact with each other (and in particular, which normalisers of {p}-groups commute with each other), motivated in part by the theory of Tits buildings for Lie groups... So perhaps I am giving the centralizer-strategy a little too much credit. But Terry's blogpost on the Frobenius and Suzuki theorems linked this very detailed (and a bit technical) survey by Solomon , and Ctrl-F ""CG"" finds occurences of centralizers $C_G(\bullet)$ all over the place (along with of course other fancier subgroups: Fitting subgroup, Thompson subgroup, maximal normal semisimple subgroups, generalized Fitting subgroups, p-layer subgroups, etc.). Regardless, I think the main body of my post makes it clear that the centralizer-strategy was quite important and pursued/utilized in many different areas, and I would appreciate any insight as to why.","['group-theory', 'simple-groups', 'finite-groups', 'big-picture']"
4664570,Five men and seven women stand in a line in random order. What is the probability that each man stands next to at least one woman?,"Source: Purple Comet Spring 2009 Problem 20 Five men and seven women stand in a line in random order. Let $m$ and $n$ be relatively prime positive integers so that $\frac{m}{n}$ is the probability that each man stands next to at least one woman. Find $m + n$ I've gotten an answer but for some reason it is incorrect. The correct answer is $287$ . Here's what I've done:
First, let us use complementary counting to find the number of ways to arrange the men and women such that there exists one man who is not adjacent to any women. Then, there must be a section of MMM (an M is a male), MMMM, or MMMMM. Now, let's count with PIE. There are $10$ places to put the MMM in a string of 12 letters, and for the remaining men and women, there are $\binom{9}{2}$ ways to arrange them. So, we have $10 \cdot \binom{9}{2}$ . However, this overcounts MMMM twice, so we subtract each MMMM once. There are $9$ places to put an  MMMM and $\binom{8}{1}$ ways to place the remaining men and women, so we have $10 \cdot \binom{9}{2} - 9 \cdot \binom{8}{1} = 360 - 72 = 288$ . MMMMM is counted correctly because we count it 3 - 2 = 1 time. There are $\binom{12}{5} = 792$ ways to arrange the men and women in total, so we have $1 - 288/792 = 504/792 = 7/11$ . So, we get $7 + 11 = 18$ , which is wrong.","['contest-math', 'combinatorics']"
4664616,Is there a 'simple' function that flips the order of positive numbers without making them negative?,"If I want to flip the order of some numbers, I can just multiply them with -1. But is there a not too complicated way to do it such that the numbers remain positive? Here's my attempt to word the question a bit more formally if you prefer that: Is there a simple* function that maps from $\mathbb{R} \to S$ where $S \subseteq \mathbb{R} \ $ s.t.: $$ \ \forall x \colon \forall y \colon (x \in \mathbb{R}) \wedge (y \in \mathbb{R}) \wedge (x < y)  \rightarrow (f(x) > f(y)) \wedge (f(x) > 0) \wedge (f(y) > 0)$$ Its fine even its from $\mathbb{N} \to S$ or something. *I know this is vague, but I just mean like obviously I know there is some function but I want one I can use. Thank you!","['real-numbers', 'monotone-functions', 'real-analysis', 'functions', 'algebra-precalculus']"
4664619,"What does the equation $x^2+y^2=r^2$ represent when $x, y, r$ are complex numbers?","I know this question is vague or maybe broad and subjective. But, I am interested in studying the equation $x^2+y^2=r^2$ when $x,y,r$ are complex numbers. What are a few directions that I can follow to study this equation? Is it even interesting (subjective, I know)? Are there already results for this equation?","['systems-of-equations', 'research', 'complex-analysis', 'functions', 'complex-numbers']"
4664648,"Consider a quadratic equation $az^2+bz+c=0$, where $a, b, c$ are complex number. Then Find condition for one purely imaginary root.","Consider a quadratic equation $az^2+bz+c=0$ , where $a, b, c$ are complex number.
Then condition that above equation has one purely imaginary root (A) $(a\bar b+\bar ab)(b\bar c+ \bar b c )+(c \bar a -\bar c a)^2=0$ (B) $(a\bar b-\bar ab)(b\bar c+ \bar b c )+(c \bar a -\bar c a)^2=0$ (c) $(a\bar b-\bar ab)(b\bar c- \bar b c )+(c \bar a -\bar c a)^2=0$ (D) None of these My solution Note: $i=\sqrt{-1}$ and $k$ is real number. $a=a_{1}+i\cdot a_{2},\;b=b_{1}+i\cdot b_{2},\;c=c_{1}+i\cdot c_{2}$ Let purely imaginary root $z=k\cdot i\;$ since $z$ is root of above equation it must satisfy the equation so equation turns into $(a_{1}+i\cdot a_{2})(ki)^2+(b_{1}+i\cdot b_{2})(ki)+(c_{1}+i\cdot c_{2})=0$ Comparing real and imaginary part of left hand side with real and imaginary part of right hand side. I obtained the equations $$\begin{align}
a_{1}k^2+b_{2}k-c_{1}&=0\qquad(1)\\
\text{and }\quad a_2k^2-b_1k-c_{2}&=0 \qquad(2)
\end{align}
$$ Equations $(1)$ and $(2)$ must have both roots common because we want only one value of $k$ . After applying condition for both roots common I obtained $\dfrac{a{_1}}{a_{2}}=\dfrac{-b_2}{b_1}=\dfrac{c_1}{c_2} \qquad(3)$ Using $(3)$ I obtained $a_1b_1+a_2b_2=b_1c_1+b_2c_2=a_2c_1-a_1c_2=0$ Now using $(a\bar b+\bar a b)=2\Re(a\bar b)=2(a_1b_1+a_2b_2),\;$ similary $(b\bar c+ \bar b c )=2\Re(b\bar c)=2(b_1c_1+b_2c_2),\;(c \bar a -\bar c a)=2i\;(\Im (c \bar a))=2i\;(a_2c_1-a_1c_2)$ . Now $0\cdot 0=0^2\implies (a_1b_1+a_2b_2)(b_1c_1+b_2c_2)=(a_2c_1-a_1c_2)^2$ $\implies \dfrac{(a\bar b+\bar a b)}{2}\dfrac{(b\bar c+ \bar b c )}{2}=\bigg(\dfrac{c \bar a -\bar c a}{2i}\bigg)^2$ Hence $$(a\bar b+\bar ab)(b\bar c+ \bar b c )+(c \bar a -\bar c a)^2=0$$ Can anyone Verify me If I made mistake anywhere or I did some mathematical blunder I know one other method to solve this problem. I just want to know If there anything mathematically wrong in my solution. Because My teacher said this problem cannot be solved using $z=ki$ . He said we must take conjugate of original equation. Related Question Consider a quadratic equation $az^2+bz+c=0$ where a,b,c are complex numbers. Prove that the equation has one purely imaginary root is given ...","['algebra-precalculus', 'quadratics', 'roots', 'complex-numbers']"
4664665,Solve ${a_n} = \sum\limits_{r = 0}^n {\frac{1}{{{}^n{C_r}}}} = \frac{1}{{{}^n{C_0}}} + \frac{1}{{{}^n{C_1}}} +\ldots+ \frac{1}{{{}^n{C_n}}}$,"If $${a_n} = \sum\limits_{r = 0}^n {\frac{1}{{{}^n{C_r}}}}  = \frac{1}{{{}^n{C_0}}} + \frac{1}{{{}^n{C_1}}} + \frac{1}{{{}^n{C_2}}} + \ldots + \frac{1}{{{}^n{C_n}}},$$ then find the value of $$\sum\limits_{r = 0}^n {\frac{r}{{{}^n{C_r}}}}$$ in terms of $a_n$ . (Where ${{}^n{C_r}}$ is $\frac{{n!}}{{r!\left( {n - r} \right)!}}$ .) Not able to solve it as ${{}^n{C_r}}$ comes in the denominator.","['binomial-theorem', 'combinatorics']"
4664707,System of linear equations with a free variable.,"The following matrix $A_n$ is an $(n-1) \times n$ matrix. I need to solve $A_nX = 0$ , where $X = (x_1, \cdots, x_n)$ . \begin{equation}
A_n = \begin{bmatrix}
b_2 & a_2 & b_3 & 0 & 0 & 0 & 0 & \cdots & 0 & 0 \\
0 & b_3 & a_3 & b_4 & 0 & 0 & 0 & \cdots & 0 & 0\\
0 & 0 & b_4 & a_4 & b_5 & 0 & 0 & \cdots  & 0 & 0\\
0 & 0 & 0 & b_5 & a_5 & b_6 & 0 & \cdots & 0 & 0\\
0 & 0 & 0 & 0 & b_6 & a_6 & b_7 & \cdots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \ddots & \ddots & \ddots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \ddots & \ddots & \ddots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \ddots & \ddots & b_{n-1} & a_{n-1} & b_n\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & b_n & a_n\\
\end{bmatrix}
\end{equation} Since $$
-\dfrac{b_n}{a_n}x_{n-1} = x_n,
$$ so $x_n$ is a free variable and thus there is always a solution to this system of equations. Let us denote $x_m \sim x_j$ if we can write $x_j$ in terms of $x_m$ . Therefore $x_{n-1} \sim x_n$ . By substituting $x_n$ by $x_{n-1}$ in $(n-1)$ th row, it implies $x_{n-2} \sim x_{n-1}$ . Therefore we will end up with $x_1 \sim x_2$ . My problem is to find the formula, say $\phi$ , so that $x_2 = \phi x_1$ . Now I need a hint to propose a method that can help to solve this problem. Cramer's rule seems doesn't work because the right hand side is zero vector.","['matrices', 'systems-of-equations', 'linear-algebra']"
4664717,"If $B$ is a Brownian motion independent of $X$, then $B$ is still a Brownian motion under the regular conditional probability given $X$","Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $B$ be a one-dimensional Brownian motion. Let $X, Y:\Omega\to \mathbb R$ be random variables. Let $\nu: \mathbb R \times \mathcal F \to [0, 1]$ be a regular conditional probability of $\mathcal F$ given $X$ , i.e., for every $x \in \mathbb R$ , we have $\nu(x, \cdot)$ is a probability measure on $(\Omega, \mathcal F)$ , for every $A \in \mathcal F$ , we have $\nu(\cdot, A)$ is $\mathcal B(\mathbb R)$ -measurable. for every $A \in \mathcal F$ and $C \in \mathcal B(\mathbb R)$ , $$
\mathbb P [A \cap \{X \in C\}] = \int_C \nu(x, A) \ \mathrm d (X_\sharp \mathbb P) (x),
$$ where $X_\sharp \mathbb P$ is the push-forward of $\mathbb P$ by $X$ . Then we have a straightforward result, i.e., Theorem If $X,Y,B$ are pairwise independent under $\mathbb P$ , then $Y, B$ are conditionally independent given $X$ . Proof Let $E := \mathcal C([0, \infty); \mathbb R)$ be the space of paths of $B$ . Let $A \in \mathcal B (\mathbb R)$ and $C \in\mathcal B (E)$ . By this lemma, $$
\begin{align}
\nu(X, \{Y\in A\} \cap \{B \in C\}) &= \mathbb P [\{Y\in A\} \cap \{B \in C\} |X] 
\quad \text{a.s.} \\
\nu(X, \{Y\in A\}) &= \mathbb P [Y\in A | X]  \quad \text{a.s.},\\
\nu(X, \{B\in C\}) &= \mathbb P [B\in C | X]  \quad \text{a.s.}
\end{align}
$$ By pairwise independence of $X,Y,B$ , $$
\begin{align}
\mathbb P [\{Y\in A\} \cap \{B \in C\} |X] &= \mathbb P [Y\in A ] \cdot \mathbb P [ B \in C] \quad \text{a.s.}, \\
\mathbb P [Y\in A | X] &=  \mathbb P [Y\in A] \quad \text{a.s.},\\
\mathbb P [B\in C | X] &=  \mathbb P [B\in C] \quad \text{a.s.}
\end{align}
$$ The claim then follows. It is mentioned in this answer that Here independence of $B$ from $X$ guarantees that $B$ is still an independent collection of Brownian motions under each $\nu (x, \cdot)$ . Could you explain how to prove above statement?","['conditional-probability', 'independence', 'brownian-motion', 'probability-theory']"
4664736,Why do variances add when summing independent random variables?,"I understand intuitively why spread would be additive, but not precisely why variances add rather than, say, the standard deviations. And how does this relate to the pythagorean theorem/euclidean distance? It appears the SDs can be treated as vectors perpendicular to (i.e. independent from) each other and the length of the vector when you sum them is equal to the SD when you sum the distributions. (This seems similar to how the SD itself can be seen as the euclidean distance of summed perpendicular deviations, divided by $\sqrt{n}$ ). (To be clear, I am not asking why variances can be added but not SDs, I am asking why variances are added rather than SDs (or anything else), so this is not a duplicate question) Thanks so much! Edit: I've gotten a great simple algebraic answer to my question, which is probably as far as you can go, but if anyone has some insight into the intuition behind it, that would be greatly appreciated too. It's probably like the pythagorean theorem and doesn't have a satisfying intuition, but I'd be more than happy with an explanation for why the standard deviations act like the sides of a right-angled triangle!","['statistics', 'normed-spaces', 'variance', 'standard-deviation']"
4664772,"Prove triangle inequality on $\hat{N}(x,y) = \sqrt{x^2 + xy + y^2}$","I'm having problems to demonstrate triangle inequality on the above function. So far I've tried: \begin{align}
\hat{N}(u+v) &= \hat{N}((u_1, u_2)+ (v_1,v_2))\\
&= \hat{N}((u_1+v_1, u_2+v_2))\\
&= \sqrt{(u_1+v_1)^2+(u_1+v_1)(u_2+v_2)+(u_2+v_2)^2}\\
&= \sqrt{(u_1^2+u_1u_2+u_2^2) + (v_1^2+v_1v_2+v_2^2) + 2(u_1v_1+u_2v_2) + u_1v_2 + u_2v_1}
\end{align} For the restant terms inside the square root, using Cauchy-Schwarz inequality we have that: \begin{align}
|2(u_1v_1 + u_2v_2)| &\leq 2(u_1^2+u_2^2)(v_1^2+v_2^2)\\
|u_1v_2 + u_2v_1| &\leq (u_1^2+u_2^2)(v_1^2+v_2^2)
\end{align} Summing up the inequalities: \begin{align}
|2(u_1v_1 + u_2v_2)| + |u_1v_2 + u_2v_1| &\leq 3(u_1^2+u_2^2)(v_1^2+v_2^2)
\end{align} Using that $ |a+b| \leq |a| + |b|$ : \begin{align}
|2(u_1v_1 + u_2v_2) + u_1v_2 + u_2v_1| &\leq 3(u_1^2+u_2^2)(v_1^2+v_2^2)
\end{align} Maybe it could be of some help, but I'm stuck from this point.","['multivariable-calculus', 'normed-spaces', 'vector-spaces']"
4664815,"In how many ways can you divide the set of eight numbers $\{2,3,...,9\}$ into 4 pairs such that no pair of numbers has g.c.d equal to $2$?","In how many ways can you divide the set of eight numbers $\{2,3,...,9\}$ into $4$ pairs such that no pair of numbers has g.c.d equal to $2$ ? My solution goes like this: First, we observe, that a pair can have gcd $d=2$ if both the numbers are even. It is easy to see that, any two even numbers paired up, has $d=2$ (We consider the gcd to be $d$ ), except the pair, $(4,8)$ which has $d=4.$ Now, the odd numbers in the given set are $\{3,5,7,9\}$ and the even numbers in the given set are $\{2,4,6,8\}.$ Now, if we choose any two odd numbers to form a pair, it is possible as all the conditions are satisfied. Now, the thing is, if we want to have all the pairs constituted of only odd numbers, now,this is absurd as, we have only $4$ odd numbers and the maximum possible pairs that can be made with them is $2.$ Now, if we have these maximum number of odd pairs,i.e $2$ odd pairs then, we have to build another two pairs with only the even numbers. This is where, the problem is, since, we can have $(4,8)$ as the only remaining choice and for the $4th$ pair, with the remaining even numbers, left, if we try to pair up, them, in any way we end up, with $d=2$ for the 4th pair. Thus, having $2$ odd pairs, is not feasible. Thus, we can have, atmost one odd pair. So now we have two cases: Case 1: There are no odd pairs in the 4 pairs and Case 2: One odd pair is there. In the former case, we can build the pairs,  either with an odd and even number i.e $(odd,even)$ and  we might take the pair $(4,8)$ as one of the 4 pairs. Here, we have, two subcases. Subcase 1: We have all the 4 pairs of the form $(odd,even).$ The number of ways, this can be done is: We first, choose an odd number and an even number and then continue the process with the remaining numbers i.e $\frac{\binom{4}{1}\binom{4}{1}\binom{3}{1}\binom{3}{1}\binom{2}{1}\binom{2}{1}\binom{1}{1}\binom{1}{1}}{4!}$ ways. Next, subcase is the case, when some pairs are of the form $(odd,even)$ and we have $(4,8)$ as one pair. Thus, we need to choose 3 pairs, of the form $(odd,even)$ . This case is not possible as we won't have  any even number left, to create the 4th pair. Now, this completes case 1. For case 2, we have two subcases as well. Subcase 1: Out of the to be chosen 3 pairs, all are of the form $(odd,even)$ and Subcase 2: We have $(4,8)$ as one pair decided and the rest are of the form $(odd,even).$ Now, Subcase 1, is not possible, since, we dont have any odd number left to form 4th pair. As for subcase 2, we have to choose 2 pairs. This can be done in $\frac{\binom{2}{1}\binom{2}{1}\binom{1}{1}\binom{1}{1}}{2!}$ ways. Thus, the total number of ways is : $\frac{\binom{4}{1}\binom{4}{1}\binom{3}{1}\binom{3}{1}\binom{2}{1}\binom{2}{1}\binom{1}{1}\binom{1}{1}}{4!}$$+\frac{\binom{2}{1}\binom{2}{1}\binom{1}{1}\binom{1}{1}}{2!}.$ Is the above solution correct? If not, where is it going wrong? I know there might be thousands of posts concerning the same topic, but I just want to verify, whether this process is valid or not.","['solution-verification', 'combinatorics']"
4664907,Intuition for expression of most likely trajectory of an SDE,"Consider a stochastic differential equation evolving on $\mathbb R$ \begin{equation}
dx_t = f(x_t)dt + c dw_t ,\quad x_0 = y \in \mathbb R
\end{equation} where $f: \mathbb R \to \mathbb R, c \in \mathbb R$ and $w_t$ is an $1$ -dimensional Wiener process/Brownian motion. In [eq. 8.6, 1] Dürr and Bach showed that when $f\in C^2(\mathbb R)$ the most likely trajectory taken by $x_t$ on the time interval $[0,T]$ is the solution to \begin{equation}
\ddot z_t = f(z_t) f'(z_t) + \frac {c^2} 2 f''(z_t), \quad
z_0 = y, \quad \dot z_T = f(z_T)
\end{equation} Note: In [1] the most likely trajectory is defined as the differentiable path starting at $y$ whose surrounding tube of radius $\epsilon$ in the uniform norm has maximal probability under $x_t$ . They show that this is well-defined provided that $\epsilon$ is below some threshold. I am struggling to get an intuition for this result. Intuitively, one can interpret the SDE as repeatedly adding Gaussian noise to the ODE $dx_t = f(x_t)dt$ . (For instance, this is what happens as one integrates the SDE using the Euler-Maruyama scheme). This leads one to think that the most likely path would be given by $\dot z_t = f(z_t), z_0=y$ . (I know that this mental picture is inaccurate since it ignores the contribution of the noise to the derivative of the process and hence its influence on the motion.) Question: Is there some intuition as to why the expression for the most likely path of the process makes sense? Bonus question: How about if instead of considering Brownian noise, we considered the same SDE driven by smooth noise (e.g. Brownian motion convolved with a mollifier), would the most likely path still be very different from $\dot z_t = f(z_t)$ ? I am guessing that, in virtue of the Wong-Zakai theorems, the situation will be analogous to the Brownian case. [1] Dürr, Bach (1978). The Onsager-Machlup function as Lagrangian for the most probable path of a diffusion process. Communications in Mathematical Physics. Addendum: According to the Stratonovich path integral formalism as presented in [2], the action (ie negative log probability) of a path $x= \left\{x_t, t\in [0,T]\right\}$ is (up to an additive constant): \begin{equation}
-\log p(x \mid x_0=y ) =\int_0^T \frac 1 {2c^2}(\dot{x}_t-f(x_t))^2 +f'(x_t) d t
\end{equation} If $f'$ is a constant then we can see that the most likely path, ie the path of least action, is simply $\dot z_t = f(z_t), z_0=y$ ; this is also what we find by solving the equation of Dürr and Bach in this case. In the non-linear case, the non-constant derivative term $f'$ leads to the more complex expression for the most likely path. In particular, if we apply the Euler-Lagrange equations to the action, we recover the second-order ODE of Dürr and Bach (this is how they obtain the most likely path in their paper). Interestingly, we can see that in the limit of small $c$ , the contribution of $f'$ to the action will be small wrt $(\dot{x}_t-f(x_t))^2$ and so the most likely path will tend to $\dot z_t = f(z_t), z_0=y$ . [2] Seifert (2012). Stochastic thermodynamics, fluctuation theorems and molecular machines. Reports on Progress in Physics.","['ordinary-differential-equations', 'euler-lagrange-equation', 'stochastic-processes', 'stochastic-differential-equations', 'soft-question']"
4664985,Real world example of an equation with no solution? [closed],Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question I have just started reading basic algebra and I have this curiosity that came up when solving basic linear equations. Some equations have no solutions. Are there any real world example of equations with no solutions? For example consider the equation: $4x -24 = 4x - 32$ . It has no solution. Do we find such cases in real world? Please ignore any naiveness in the question as I have just started off in mathematics and do not have a deep understanding of it. Thanks.,"['algebra-precalculus', 'applications']"
4665009,Trigonometry problem: $\sin^4 \frac {\pi}{7} + \sin^4 \frac {3\pi}{7} + \sin^4 \frac {5\pi}{7} = \frac {a}{b}$,"The question: $$\sin^4 \frac {\pi}{7} + \sin^4 \frac {3\pi}{7} + \sin^4 \frac {5\pi}{7} = \frac {a}{b} $$ Find $a+b$ . My attempt: $$\sin^4 \theta = (\sin^2 \theta)^2 = \left(\frac{1-\cos2\theta}{2}\right)^2 \\ = \frac {\cos4\theta - 4\cos2\theta+3}{8}$$ So, using this result, the question simplifies to: $$\frac {\cos \frac{4\pi}{7} + \cos \frac{12\pi}{7} + \cos \frac{20\pi}{7} - 4 (\cos \frac{2\pi}{7} + \cos \frac{6\pi}{7} + \cos \frac{10\pi}{7})+9}{8}$$ I tried using the identity $\cos A+\cos B=2\cos\frac{A+B}{2}\cos\frac{A-B}{2}$ but it just doesn't get shorter. I assume that this question is easily solvable using trigonometric identities, but I would also really appreciate alternate solutions not using pure trigonometry.",['trigonometry']
4665041,Joint distribution given marginals and correlation,"I am trying to understand whether it is possible and if so then how to find a joint distribution given marginals and a correlation matrix. In particular, suppose $X_{1},\ldots,X_{n}$ are discrete random variables. I am given their marginals $p_{X_{1}},\ldots,p_{X_{n}}$ and their correlation matrix. Can I construct (uniquely?) the joint $p_{X_{1},\ldots,X_{n}}$ ? I tried this for $n=2$ , assuming each of the two random variables has two possible outcomes $a$ and $b>a$ , and assuming that the marginals of $X_{1}$ and $X_{2}$ are the same given by $\mathbb{P}[X_{1}=a]=c$ . Let $r$ be the correlation of $X_{1}$ and $X_{2}$ . My Mathematica readily calculates unique joint distribution provided $c(1-r)<1$ and claims there is no joint otherwise. How would I generalize to more random variables with more than two possible outcomes? What I do in Mathematica (calculate correlation from joint distribution and then solve for the joint distribution that gives $r$ ) does not generalize nicely. Also, are there some results that would tell me what I am doing can/cannot be done? Uniquely? I tried reading StackExchange questions/answers here , here , here , here , and here . But none of the answers help me forward. Also, I know that this question is trivial when the random variables are independent. So let's disregard that special case.","['probability-distributions', 'probability-theory', 'probability', 'random-variables']"
4665064,periodic function is constant,"Let $g:\mathbb{R} \rightarrow \mathbb{R} $ be a function with intermediate value property, such that $\lim_{x\to\infty} g(x) = \infty$ . Prove that if $f:\mathbb{R} \rightarrow \mathbb{R} $ is a periodic function such that $\lim_{x\to\infty} f(g(x))$ exists, then $f$ is constant. This is what I have tried:
As $g$ is a function with IVP such that $\lim_{x\to\infty} g(x) = \infty$ , there exists an interval $[a, \infty) \subset Im(g)$ . Let $x_0$ be a real number and let $T>0$ be one of the periods of $f$ , and define $x_n=x_0+ nT$ . Then, because $\lim_{n\to\infty} x_n =\infty$ , there exists a positive integer $n_0$ such that $x_n \in Im(g)$ for all $n \ge n_0$ . Therefore there exists a sequence $(y_n)_{n\ge1}$ such that $g(y_n)=x_n$ for all $n\ge n_0$ . Then $f(g(y_n))=f(x_0)$ for all $n \ge n_0$ . If I can get that $(y_n)_{n\ge1}$ is unbounded I would be done, because then $f(x_0)=l$ , and since $x_0$ was chosen arbitrarily, $f$ would be constant. I can't find a reason why or why not this is not true. I would be glad to receive some help!","['limits', 'functions', 'continuity']"
4665088,Regular Polygon Inscribed In Circle - a question I found with no solution given. I made some progress but don't know how to proceed.,"Here is the full question: A regular $48$ -gon is inscribed in a circle with radius $1$ . Let $X$ be the set of distances (not necessarily distinct) from the center of
the circle to each side of the $48$ -gon, and $Y$ be the set of
distances (not necessarily distinct) from the center of the circle to
each diagonal of the $48$ -gon. Let $S$ be the union of $X$ and $Y$ .
What is the sum of the squares of all of the elements in $S$ ? No solution was given, but I gave it a shot anyway. Here's my answer so far: First, we find the elements of set $X$ - the distances from the centre of the circle to each side of the $48$ -gon. Let $x$ be the distance from the centre of the circle to one side of the polygon. We draw this diagram (not to scale) and find out the following values. A line from the centre of the $48$ -gon, $C$ , to one of its vertex is equivalent to the radius of the circle, which is $1$ . This is the hypotenuse of the right-angled triangle. That line is also an angle bisector of one of its interior angles. Each interior angle of the $48$ -gon is $\frac{180°(48-2)}{48}=172.5°$ , so $\frac{172.5°}{2}=86.25°$ . Now, we find that $x = \frac{\sin(86.25°)}{1} = \sin(\frac{23\pi}{48})$ in terms of radians. As such, there are $48$ elements in set $X$ , each element with the value of $\sin(\frac{23\pi}{48})$ . Second, we try to find the elements of set $Y$ - the distances from the centre of the circle to each diagonal of the $48$ -gon. Here's where I get stuck. We know that there are $\frac{48(48-3)}{2}=1080$ diagonals in the $48$ -gon. Since the polygon has an even number of sides ( $48$ is even), there will be $\frac{48}{2}=24$ diagonals passing through the centre of the circle, meaning their distances from the centre will be $0$ . This leaves us with $1080-24=1056$ diagonals unaccounted for. Now... how do I find every distance of the rest of the diagonals from the centre of the circle? Is there anything wrong with my working so far? What suggestions do you guys have? How would you go about solving this question? Thanks in advance!","['circles', 'geometry', 'complex-numbers', 'polygons']"
4665111,Rudin's RCA $5.15$ Theorem.,"Premise. These are the definitions which we need for the proof of the theorem: $1.39$ Theorem: suppose $f$ $\in$ $L^1(\mu)$ and $\int_E f d\mu$ $=$ $0$ for every $E$ $\in$ $\mathfrak M$ ( $\sigma$ - algebra ). Then $f$ $=$ a.e. on $X$ . Lusin's theorem with its corollary : $4.25$ Theorem : If $f$ $\in$ $C(T)$ and $\epsilon$ $\gt$ $0$ , there is a trigonometric polynomial $P$ such that $|f(t) - p(t)|$ $\lt$ $\epsilon$ for every real $t$ . For any $f$ $\in$ $L^1(T)$ , we define the Fourier coefficients of $f$ by the formula $\hat f(n)$ $=$ $\frac {1} {2\pi}$ $\int_{-\pi}^{\pi}$ $f(t) e^{-int} dt $ ( $n$ $\in$ $Z$ ). $\hat f(n)$ $\to$ $0$ as $|n|$ $\to$ $\infty$ , for every $f$ $\in$ $L^1$ . Let $c_0$ be the space of all complex functions $\varphi$ on $Z$ such that $\varphi(n)$ $\to$ $0$ as $n$ $\to$ $+-$$\infty$ , with the supremum norm || $\varphi||_\infty$ $=$ sup { $|\varphi(n)|$ : $n$ $\in$ $Z$ }. There is the theorem : Theorem. The mapping $f$ $\to$ $\hat f $ is a one-to-one bounded linear
transformation of $L^1(T)$ into (but not onto) $c_0$ . There is the proof: Define $\Lambda$ by $\Lambda f $ $=$ $\hat f$ . It is clear that $\Lambda$ is linear. We have already known that $\Lambda$ maps $L^1(T)$ into $c_0$ , and formula $\hat f(n)$ $=$ $\frac {1} {2\pi}$ $\int_{-\pi}^{\pi}$ $f(t) e^{-int} dt $ ( $n$ $\in$ $Z$ ) shows that $|\hat f(n) |$ $\leq$ $||f||_1$ , so that $||\Lambda||$ $\leq$ $1$ . ( Actually, $||\Lambda||$ $=$ $1$ , to see this, take $f$ $=$ $1$ .) Let us now prove that $\Lambda$ is one-to-one. suppose $f$ $\in$ $L^1(T)$ and $\hat f(n)$ $=$ $0$ for every $n$ $\in$ $Z$ . Then $\int_{-\pi}^{\pi}$ $f(t)g(t)$ $dt$ $=$ $0$ if g is any trigonometric polynomial. By Theorem $4.25$ and the dominated convergence theorem, $\int_{-\pi}^{\pi}$ $f(t)g(t)$ $dt$ $=$ $0$ holds for every $g$ $\in$ $C(T)$ . Apply the dominated convergence theorem once more, in conjunction with the Corollary to Lusin's theorem, to conclude that $(1)$ holds if g is the characteristic function of any measurable set  in $T$ .
Now Theorem $1.39$ shows that $f$ $=$ $0$ a.e. . I have a few questions : how does the $\int_{-\pi}^{\pi}$ $f(t)g(t)$ $dt$ $=$ $0$ holds for every $g$ $\in$ $C(T)$ by using $4.25$ and the dominated convergence theorem. How do we conclude that $\int_{-\pi}^{\pi}$ $f(t)g(t)$ $dt$ $=$ $0$ holds if $g$ is the characteristic function of any measurable set in $T$ ? Why do we have the right to apply $1.39$ theorem on this ? ( Integral in this one depends on $\mu$ measure ) . Any help would be appreciated.","['fourier-analysis', 'analysis', 'functional-analysis', 'linear-transformations', 'fourier-series']"
4665125,Sequence of quadratic surds over nonnegative integers without having to delete or sort?,"I am trying find an strictly increasing iterative sequence that gives this set sorted: $$[a+\sqrt{b}: a,b \in \mathbb{N_0}].$$ These are a subset of constructable numbers. When I look at it, there are obvious patterns, but I cannot seem to find an efficient algorithm to find them without deleting or sorting.
I don't know if this is even possible. Any help is appreciated. I can generate the first 20 using Mathematica: Take[Sort[Flatten[Table[Select[DeleteDuplicates[Total[Tuples@{Table[i,{i,0,k}], Table[Sqrt[i],{i,0,k}]}, {2}]],# <= k&],{k,60,60}]],Less],20] $$\left\{0,1,\sqrt{2},\sqrt{3},2,\sqrt{5},1+\sqrt{2},\sqrt{6},\sqrt{7},1+\sqrt{3},2\sqrt{2},3,\sqrt{10},1+\sqrt{5},\sqrt{11},2+\sqrt{2},1+\sqrt{6},2\sqrt{3},\sqrt{13},1+\sqrt{7}\right\}$$","['well-orders', 'quadratics', 'algorithms', 'sequences-and-series']"
4665252,Character table guarantees certain property of groups,"How to show there exist three conjugacy classes in a certain group $G$ such that the products $xyz, x\in C_1, y\in C_2, z\in C_3$ divide equally over all elements of $G$ by looking at the character table? For example, take $G=A_5$ , $x\in [(12)(34)]=C_1,y\in [(123)]=C_2,z\in [(12345)]=C_3$ , then each element $a\in A_5$ can be written in the same number of ways as $a=xyz$ . How to deduce this by means of properties of the character table? Thanks.","['group-theory', 'abstract-algebra', 'representation-theory', 'characters']"
4665321,Apostol: How to calculate work by force field along intersection of sphere and cylinder.,"This question is about line integrals in cartesian and cylindrical coordinates. It is based on the following problem from Apostol's Calculus , Volume II, chapter 10 ""Line Integrals"", section 10.9 Calculate the work done by the force field $$f(x,y,z)=y^2\hat{i}+z^2\hat{j}+x^2\hat{k}\tag{1}$$ along the curve of
intersection of the sphere $x^2+y^2+z^2=a^2$ and the cylinder $$x^2+y^2=ax\tag{2}$$ where $$z\geq 0\tag{3}$$ and $$a>0\tag{4}$$ The path is traversed in a
direction that appears clockwise when viewed from high above the $xy$ -plane. The answer from Apostol's book is $\pi a^3/4$ . Initial Attempt At first I did everything in cartesian coordinates. From (2) we have $$y=\pm\sqrt{ax-x^2}\tag{5}$$ Subbing (2) into (1) and solving for $z$ we have $$z=\sqrt{a^2-ax}\tag{6}$$ From (5) and (6) we have the condition $x\in [0,a]$ . If we let $x$ be our parameter, a parametrization of the intersection of the sphere and the cylinder is given by the two equations in (7) (notice the $\pm$ on the $\hat{j}$ component) $$\vec{r}(t)=t\hat{i}\pm\sqrt{at-t^2}\hat{j}+\sqrt{a^2-at}\hat{k}\tag{7}, t\in [0,a]$$ Here is a plot of the two curves that give the full intersection (for $a=2$ ) Hence $$d\vec{r}=(\hat{i}\pm\frac{a-2t}{2\sqrt{at-t^2}}\hat{j}+\frac{-a}{2\sqrt{a^2-at}}\hat{k})dt\tag{8}$$ and $$f(\vec{r}(t))=(at-t^2)\hat{i}+(a^2-at)\hat{j}+t^2\hat{k}\tag{9}$$ And the work done by the force field on this curve is $$\int_C \vec{f}\cdot d\vec{r}=\int_0^a \vec{f}(\vec{r}(t))\cdot d\vec{r}(t)dt\tag{10}$$ $$=\int_0^a \left(at-t^2+\frac{(a^2-at)(a-2t)}{2\sqrt{at-t^2}}-\frac{at^2}{2\sqrt{a^2-at}}\right )dt \tag{11}$$ My first question is if this approach is correct so far. This integral does not seem easy, and using Maple to solve it doesn't yield an answer. Second Attempt Then I tried with cylindrical coordinates, a topic that still eludes me. The sphere is $$r^2+z^2=a^2\tag{12}$$ and the cylinder is $$r^2=ar\cos{\theta} \implies r=a\cos{\theta}\tag{13}$$ From these two equations we obtain $$z=\sqrt{a^2-r^2}=\sqrt{a^2-a^2\cos^2{\theta}}=a\sin{\theta}\tag{14}$$ So at this point we have a parametrization of the intersection. $$r=a\cos{\theta}$$ $$\theta=\theta$$ $$z=a\sin{\theta}$$ And if we plot this, we get something that seems to make sense as well except that we have to remember that our intersection has $z\geq 0$ , so it our intersection is the top part of the plot above, and so coincides with the plot we obtained when we used cartesian coordinates. Now is the part that is more murky for me. We have $$\vec{f}(\vec{r}(\theta))=a^2\cos^2{\theta}\sin^2{\theta}\hat{i}+a^2\sin^2{\theta}\hat{j}+a^2\cos^4{\theta}\hat{k}\tag{15}$$ $$\vec{r}(\theta)=r(\theta)\hat{r}(\theta)+z(\theta)\hat{k}\tag{16}$$ $$=a\cos{\theta}\hat{r}(\theta)+a\sin{\theta}\hat{k}\tag{17}$$ $$=a\cos^2{\theta}\hat{i}+a\cos{\theta}\sin{\theta}\hat{j}+a\sin{\theta}\hat{k}\tag{18}$$ Then $$\frac{d\vec{r}(\theta)}{d\theta}=-2a\cos{\theta}\sin{\theta}\hat{i}+(-a\sin^2{\theta}+a^2\cos^2{\theta})\hat{j}+a\cos{\theta}\hat{k}\tag{18}$$ Thus, if this is correct, then the line integral is $$\int_C \vec{f}\cdot d\vec{r}=\int_{\pi}^0\left ( -2a^3\sin^3{\theta}\cos^3{\theta}-a^3\sin^4{\theta}+a^4\sin^2{\theta}\cos^2{\theta}+a^3\cos^5{\theta} \right )d\theta\tag{19}$$ Note that I chose the integration limits by looking at the parameterized equations and figuring out the values of $\theta$ that trace out the trajectory we want. For example, consider $a=2$ , as in the plots above. If we start at $\theta=0$ which corresponds to $(r,\theta,z)=(2,0,0)$ and move to $\theta=\pi/2$ which is $(r,\theta,z)=(0,\pi/2,2)$ . This is halfway. Then we go to $\theta=\pi$ which takes us back to the starting point, but now with coordinates $(-2,\pi,0)$ . However, this is counterclockwise, and we want clockwise, as per the problem statement. Hence the limits go from $\pi$ to $0$ . Using Maple, (19) evaluates to $$\frac{\pi a^3}{4}\tag{2}$$ So my questions are the following is the approach in Cartesian coordinates just destined to fail because that integral is too difficult to solve? is there some easier way than what I did above with the cylindrical coordinates? Final Note: As sometimes happens, in the course of carefully writing out my question and making sure it is well organized and clear for the reader, I sometimes end up learning. And it seems that I did reach the correct result after all with the cylindrical coordinates. However, my questions remain about if this was all way too cumbersome and if there is a smarter way.","['multivariable-calculus', 'solution-verification', 'vectors', 'line-integrals']"
4665323,Mystery integral in the first issue of Annals of Mathematics,"Vol 1. No. 1 of Annals of Mathematics has an ""Exercises"" section with this unusual integral sent in by a Professor Lewis Green Barbour: $$\int_{\frac \pi 2}^\pi \sqrt{1-\frac 1 2 \cos^2\vartheta + \sin\vartheta\sin2\vartheta}\,{\rm d}\vartheta$$ The other exercises are answered in later issues but this one seems to have been quietly forgotten. I've tried modern software and it's stumped on solving the indefinite integral. Numerical methods yield a value: $$\approx 0.8277600029391442$$ However, this doesn't seem to be a closed-form number in any obvious way. Searches of various number databases turn up nothing. Does this integral have a closed-form solution? If not, what methods would a reader in 1884 have used to solve it? Given how famous the Annals became, does this unsolved exercise from the very first issue have any lore attached to it?","['calculus', 'math-history', 'definite-integrals']"
4665337,Is there any method other than Feynman’s trick which can deal further with powers higher than 2?,"Background When I met the integral $$\int_0^1 \frac{\left(x^\phi-1\right)^2}{\ln ^2 x} d x\\$$ where $\phi$ is the golden ratio: $\phi^2= \phi+1, $ I was surprised by its simple and decent  value  though it is hard to tackle. I had tried some methods such as  substitutions, integration by parts etc. and failed. Then I tried Feynman’s trick by introducing the integral parametrized by $t$ $$I(t)=
\int_0^1 \frac{\left(x^t-1\right)^2}{\ln ^2 x} d x\\
$$ As usual differentiating $I(t) $ w.r.t. $t$ once and twice yields $$
I^{\prime}(t)=\int_0^1 \frac{2\left(x^t-1\right) x^t}{\ln x}dx
$$ and $$
\begin{aligned}
I^{\prime \prime}(t) & =\int_0^1 \left(4x^{2 t}-2x^t\right) d x \\
& =\frac{4}{2 t+1}-\frac{2}{t+1}
\end{aligned}
$$ Noticing that $I(0)=I^{\prime}(0)=0$ , we can easily integrating back to $I(t)$ in two steps. $$
I^{\prime}(t)-I^{\prime}(0)=\int_0^t I^{\prime \prime}(u) d u=\int_0^t\left(\frac{4}{2 u+1}-\frac{2}{u+1}\right) du
$$ $$
I^{\prime}(t)=2\ln (2 t+1)-2 \ln (t+1)
$$ Similarly, $$
\begin{aligned}
I(t)-I(0) & =\int_0^t I^{\prime}(u) d u =\int_0^t[2\ln (2 u+1)-2 \ln (u+1)] d u
\end{aligned}
$$ Using the result $\int \ln x d x=x \ln x-x+C$ , we have $$
\boxed{\int_0^1 \frac{\left(x^t-1\right)^2}{\ln ^2 x} d x =(2 t+1) \ln (2 t+1)-2(t+1) \ln (t+1)}
$$ Using $\phi^2= \phi+1 $ gives $$I=I(\phi)= (2 \phi+1) \ln \left(\phi^3\right)-2(\phi+1) \ln \left(\phi^2\right)= (2 \phi-1) \ln \phi =(2 \phi-1) \ln \phi =\boxed{\sqrt 5 \ln \phi }$$ My Questions: Is there any method other than Feynman’s trick ? Can we go further with the powers higher than 2?","['integration', 'golden-ratio', 'improper-integrals', 'definite-integrals', 'calculus']"
4665369,Find a closed form for the integral $\int_{0}^{1}\int_{0}^{1}\frac{1}{\sqrt{x(1-x)}\sqrt{y(1-y)}\sqrt{1-xy}}dxdy$,"I found this double integral in this group $$\displaystyle\int_{0}^{1}\int_{0}^{1}\frac{1}{\sqrt{x(1-x)}\sqrt{y(1-y)}\sqrt{1-xy}}dxdy$$ After integrating with respects to $x$ (or $y$ , the order is not important because of the symmetry), I get this integral $$\displaystyle2\int_{0}^{1}\frac{K(y)}{\sqrt{y(1-y)}}dy$$ Where $K(y)$ is the complete elliptic integral of the first kind. WolframAlpha says its closed form is $$\frac{256\pi^3}{\Gamma^4{\left(\frac{-1}{4}\right)}}$$ or it is equavalent $$\frac{\pi^3}{\Gamma^4{\left(\frac{3}{4}\right)}}$$ I also try to use series expansion of $K(y)$ but the sum is more complicated. Any another approach is welcome, and thank you for reading.","['integration', 'multivariable-calculus', 'calculus']"
4665382,"If $\lim_{n\to \infty} a_n = 4$, can we say $\lim_{n\to \infty} a_{n-2} = 4$?","I was doing this exponential tower equation: $$2^{x^{2^{x^{2^{...}}}}} = 4$$ $$\text {(each new exponent is the  power of the last exponent)}$$ The popular method is to break the tower at the first exponent and evaluate that what's left in the exponential tower is still the LHS if the tower length goes into infinity: $$2^{x^{2^{x^{2^{...}}}}} =  2^{x^{4}} = 4$$ Consider sequence $a_n$ with $a_1 = 2^x$ , $a_2 = 2^{x^{2}}$ , $a_3 = 2^{x^{2^{x}}}$ , and so on, then: $$LHS = \lim_{n\to ∞} a_n = 4$$ The method above seems to be reasoning that: $$\text{if}\;\;\;\;\;\;\;\;\;\;\lim_{n\to ∞} a_n = 4$$ $$\text{then}\;\;\;\;\;\;\;\;\;\lim_{n\to ∞} a_{n-2} = 4$$ (1) Is this something that we need to prove? I know this is intuitive enough, but is there a way to rigorously demonstrate this? (2) I was assuming that this is the reasoning behind the method above. If you think otherwise, please advise! (3) Unrelated, but is there a formula to define the sequence above with $a_n, a_{n+1}, a_{n+2}$ ? (I assume we cannot write a general case for $a_n$ alone.) Thank you!","['calculus', 'power-towers', 'sequences-and-series', 'limits', 'convergence-divergence']"
4665450,Generalization of the result of $\int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x$.,"When I came across the integral $$\int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x,$$ I didn’t know how to deal with it. After struggling, I thought of Feynman’s trick and Euler formula and tried by letting $$I(a)=\int_0^{\infty} \frac{e^{-x^2} \sin \left(a x^2\right)}{x^2} d x$$ with $I(0)=0.$ Differentiating $I(a)$ w.r.t. $a$ yields $$
I^{\prime}(a)=\int_0^{\infty} e^{-x^2} \cos \left(a x^2\right) d x
$$ Inevitably, I used the Euler formula $e^{i x}=\cos x+i \sin x$ to group the functions in terms of exponential function. $$
\begin{aligned}
I^{\prime}(a) & =\int_0^{\infty} e^{-x^2} \cos \left(a x^2\right) d x \\
& =\operatorname{Re} \int_0^{\infty} e^{-x^2} e^{i a x^2} d x \\
& =\operatorname{Re} \int_0^{\infty} e^{-(1-i a) x^2} d x
\end{aligned}
$$ which is a Gaussian integral : $\int_{-\infty}^{\infty} e^{-a(x+b)^2} d x=\sqrt{\frac{\pi}{a}}$ for any $Re(a)>0$ . $$
\begin{aligned}
I’(a) &=\operatorname{Re}\left(\frac{\sqrt{\pi}}{2 \sqrt{1-i a}}\right)\\ I(a)&=\frac{\sqrt{\pi}}{2} \operatorname{Re}\left[\frac{(1-i a)^{\frac{1}{2}}}{-\frac{1}{2} i}\right]\\ \therefore \int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x&= \sqrt{\pi}\operatorname{Re}\left[i \sqrt{\sqrt{2} e^{-\frac{\pi i}{4}}}\right]\\&   = \sqrt{\sqrt{2} \pi} \operatorname{Re}\left[i\left(\cos \frac{\pi}{8}-i \sin \frac{\pi}{8}\right)\right]\\&= \sqrt{\sqrt{2} \pi} \sin \frac{\pi}{8}
\end{aligned}
$$ As a bonus, $$\int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x = \sqrt{\sqrt{2} \pi} \cos \frac{\pi}{8} $$ My Question :Can we generalise the method to the integral $$
I_n=\int_0^{\infty} \frac{e^{-x^n} \sin \left(x^n\right)}{x^n} d x?
$$ Fortunately, the answer is positive and decent as: $$
\int_0^{\infty} \frac{e^{-x^n} \sin \left(x^n\right)}{x^n} d x= \frac{2^{\frac{n-1}{2 n}}}{n-1} \Gamma\left(\frac{1}{n}\right) \sin \left(\frac{\pi(n-1)}{4 n}\right)
$$ where $n$ is any real number greater than $1$ . Proof:
Replacing the number 2 in the original integral by $n$ gives $$
I_n^{\prime}(a)=\operatorname{Re} \int_0^{\infty} e^{-(1-i a) x^n} d x
$$ Letting $(1-i a) x^n \mapsto x$ transforms the derivative into $$
\begin{aligned}
I_n ^{\prime}(a) & =\operatorname{Re}\left[\frac{1}{n(1-i a)^{\frac{1}{n}}} \int_0^{\infty} x^{\frac{1}{n}-1} e^{-x} d x\right] \\
& =\frac{1}{n} \operatorname{Re}\left[\frac{1}{(1-i a)^{\frac{1}{2}}}\Gamma\left(\frac{1}{n}\right)\right] \\
& =\frac{1}{n} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[\frac{1}{(1-i a)^{\frac{1}{n}}}\right]
\end{aligned}
$$ Integrating back yields $$
\begin{aligned}
I_n(1)-I_n(0) & =\frac{1}{n} \Gamma\left(\frac{1}{n}\right) \operatorname{Re} \int_0^1(1-i a)^{-\frac{1}{n}} d a \\
I(1)& =\frac{1}{n} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[\frac{(1-i a)^{-\frac{1}{n}+1}}{-i\left(\frac{1}{n}+1\right)}\right]_0^1 \\
& =\frac{1}{n-1} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left(i(1-i)^{\frac{n-1}{n}}\right)
\end{aligned}
$$ Now we can conclude that $$
\begin{aligned}
I& =\frac{1}{n-1} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[i\left(\sqrt{2} e^{-\frac{\pi}{4}}\right)^{\frac{n-1}{n}}\right] \\
& =\frac{1}{n-1} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[i 2^{\frac{n-1}{2 n}} e^{-\frac{\pi(n-1)}{4 n}}\right] \\
& =\frac{2^{\frac{n-1}{2 n}}}{n-1} \Gamma\left(\frac{1}{n}\right) \sin \left(\frac{\pi(n-1)}{4 n}\right)
\end{aligned}
$$ Any comments and alternative methods are highly appreciated.","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'gaussian-integral']"
4665482,Evaluate $\int_{0}^{1} \frac{K(k)E(k)^2-\frac{\pi^3}{8} }{k} \text{d}k$ and $\int_{0}^{1} \frac{E(k)^3-\frac{\pi^3}{8} }{k} \text{d}k$,"Let $K(k),E(k)$ be the complete elliptic integral of the first kind and second kind respectively, where $k$ is the elliptic modulus. Consider four integrals, $$\begin{aligned}
&I_1=\int_{0}^{1} \frac{K(k)^3-\frac{\pi^3}{8} }{k}
\text{d}k,\\
&I_2=\int_{0}^{1} \frac{K(k)^2E(k)-\frac{\pi^3}{8} }{k}
\text{d}k,\\
&I_3=\int_{0}^{1} \frac{K(k)E(k)^2-\frac{\pi^3}{8} }{k}
\text{d}k,\\
&I_4=\int_{0}^{1} \frac{E(k)^3-\frac{\pi^3}{8} }{k}
\text{d}k.
\end{aligned}$$ $I_1$ is computed to be $$
\int_{0}^{1} \frac{K(k)^3-\frac{\pi^3}{8} }{k}
\text{d}k=\frac{\Gamma\left ( \frac{1}{4}  \right )^8}{3200\pi^2} 
-\frac{12}{5}\beta(4)+\frac{\pi^3}{4}\ln(2),
$$ where we utilize $$
\int_{0}^{1}\left ( \frac{K^\prime}{K}  \right )^{s-1}
\left[ \frac{K(k)\left ( K(k)^2-\frac{\pi^2}{4}  \right ) }{k} 
-\frac{k}{5}K(k)^3  \right]\text{d}k
=\frac{\pi^{4-s}}{20}\Gamma(s)\zeta(s)\left [ \beta(s-4)+5\beta(s-2) \right ].
$$ By differentiatng $(1-k^2)K(k)^3$ , we have $$
\int_{0}^{1} \left ( kK(k)^3-\frac{3K(k)^2\left ( K(k)-E(k) \right ) }{k}  \right )
\text{d} k=\left [ (1-k^2)K(k)^3 \right ] \Big|^{1}_0=-\frac{\pi^3}{8}.
$$ Hence the evaluation, $I_2$ is $$
\int_{0}^{1} \frac{K(k)^2E(k)-\frac{\pi^3}{8} }{k}
\text{d}k=-\frac{\Gamma\left ( \frac{1}{4}  \right )^8}{4800\pi^2} 
-\frac{12}{5}\beta(4)+\frac{\pi^3}{4}\ln(2)-\frac{\pi^3}{24}.
$$ While I have met the tedious part, differentiating $k^2(1-k^2)K(k)^3,E(k)^3,(1-k^2)K(k)^2E(k)$ and $(1-k^2)K(k)E(k)$ . They generate the following relations: $$\begin{aligned}
&(1)3kK^2E-k^3K^3\equiv0,\\
&(2)E^3/k-KE^2/k\equiv0,\\
&(3)E^3/k+KE^2/k+2kK^2E-3kKE^2\equiv0,\\
&(4)2KE^2/k-kK^2E\equiv0,
\end{aligned}$$ in which we say $f(k)$ is ""equal"" to $0$ if the regularized value of $\int_{0}^{1}f(k)\text{d}k$ has known explicit closed-forms. These four equations have five unknowns and it seems that the fifth can be constructed normally, though, I was stuck here. Question. Whether we can find the fifth relation or the closed-forms of integrals the title comprised?","['integration', 'definite-integrals', 'calculus', 'closed-form', 'elliptic-integrals']"
4665496,"Finding non-trivial real numbers which satisfy all three of: $\sum_{i=1}^{n}a_i=0,\sum_{i=1}^{n}{a_i}^3=0$ and $\sum_{i=1}^{n}\lvert a_i\rvert=1.$","A question was asked recently on this site: Find bounds on $\ \displaystyle\sum_{i=1}^{n} {a_i} ^5\ $ if $\
 \displaystyle\sum_{i=1}^{n} a_i = 0,\ \sum_{i=1}^{n} {a_i}^3 = 0\ $ and $\
 \displaystyle\sum_{i=1}^{n} \lvert a_i \rvert = 1.$ I then asked for one example of $a_i$ that satisfy the three conditions in the question, and the user correctly gave the example: $a_1 = -0.5,\ a_2 = 0.5 $ and then he deleted the question. Now, the only other examples I can think of which satisfy all three conditions are $\left( \underbrace{\frac{-1}{2n},\ \frac{-1}{2n},\ \ldots,\ \frac{-1}{2n}}_{n\ \text{times}},\ \underbrace{\frac{1}{2n},\ \frac{1}{2n},\ \ldots,\ \frac{1}{2n}}_{n\ \text{times}} \right).\ $ This works for any $\ n\in\mathbb{N}.$ My question is: Are there any other examples satisfying all three conditions? I found some examples that convince me there are numbers that satisfy $\displaystyle\sum_{i=1}^{n} {a_i}^3 = 0\ $ and $\
 \displaystyle\sum_{i=1}^{n} \lvert a_i \rvert = 1,\ $ for example $\ 0.40,\ -0.375, -0.225\ $ is close enough to convince me. However, these don't come close to satisfying the condition $\
 \displaystyle\sum_{i=1}^{n} a_i = 0.$ Similarly, there are many numbers satisfying $\
 \displaystyle\sum_{i=1}^{n} a_i = 0\ $ and $\
 \displaystyle\sum_{i=1}^{n} \lvert a_i \rvert = 1:\ $ The positive terms must sum to $0.5$ and the negative terms must sum to $-0.5,$ but other than this there are no restrictions. And as an example where $\ \displaystyle\sum_{i=1}^{n} {a_i}^3 = 0\ $ and $\
 \displaystyle\sum_{i=1}^{n} a_i = 0,\ $ there's $\left( \underbrace{\frac{-1}{2n},\ \frac{-1}{2n},\ \ldots,\ \frac{-1}{2n}}_{n\ \text{times}},\ \underbrace{\frac{1}{2n},\ \frac{1}{2n},\ \ldots,\ \frac{1}{2n}}_{n\ \text{times}} \right),$ but I'm sure there's other examples also. For example, $(-0.2,-0.3, 0.5)$ doesn't work because ${0.5}^3 > \vert {-0.2}^3 + {-0.3}^3\vert,\ $ however we can split the $0.5$ up into three (or more) positive numbers $a_i, i=3,\ldots, n,\ $ such that $ \displaystyle\sum_{i=3}^{n} a_i = 0.5\ $ where $n\geq 5.$ Since $\ \displaystyle\sum_{i=3}^{n} {a_i}^3\ $ has min value $\ n \left( \frac{0.5}{n} \right)^3\ $ (I think), and due to continuity of the multi-variable function $\ \displaystyle\sum_{i=3}^{n} {a_i}^3,\ $ there are clearly examples such that $ \vert {-0.2}^3 + {-0.3}^3\vert = \displaystyle\sum_{i=3}^{n} {a_i}^3.$","['a.m.-g.m.-inequality', 'real-analysis', 'multivariable-calculus', 'inequality', 'holder-inequality']"
4665545,Asymptotics of a nonautonomous linear dynamical system.,"I'm interested in the limits $$\lim_{t\rightarrow\infty}A\left(t\right),\lim_{t\rightarrow\infty}B\left(t\right),$$ of the system $$\begin{cases}
\frac{d}{dt}A\left(t\right)=p_{A}\left(1-A-B\right)\\
\frac{d}{dt}B\left(t\right)=\left(p_{B}+q_{B}\right)\left(1-A-B\right)-q_{B}\left(1-B\right)e^{-\left(p_{A}+p_{B}\right)t}
\end{cases}$$ with initial conditions $A(0)=B(0)=0$ , where $p_{A},p_{B},q_{B}>0$ are constants. Since most of what I know about critical points of dynamical systems concerns autonomous ones, I thought about investigating the following autonomous system: $$\begin{cases}
\frac{d}{dt}A\left(t\right)=p_{A}\left(1-A-B\right)\\
\frac{d}{dt}B\left(t\right)=\left(p_{B}+q_{B}\right)\left(1-A-B\right)-q_{B}\left(1-B\right)C\left(t\right)\\
\frac{d}{dt}C\left(t\right)=-\left(p_{A}+p_{B}\right)C\left(t\right)
\end{cases}$$ where $C(0)=1$ . Setting the left hand sides to zero results in the conditions $$A+B	=1,C	=0,$$ which means there's an infinite number of critical points, which is the set $$\left\{ \left(a,1-a,0\right)|a\in\mathbb{R}\right\}.$$ My question is, is there some way to find the critical point towards the solution converges to? My attempts at solving said system have failed (other than when $q_{B}=0$ , which is a special but simpler case), and I'm hoping there's some other method of finding the limits, that don't require completely solving the system. Any thoughts would be appreciated on the matter, Thank you in advance!","['ordinary-differential-equations', 'dynamical-systems']"
4665548,Ray problem (geometry),"Problem: Two plane mirrors $OP$ and $OQ$ are inclined at an acute angle (diagram is not to scale). A ray of light $XY$ parallel to $QO$ strikes mirror $OP$ at $Y$ . The ray is reflected and hits mirror $OQ$ , is reflected again and hits mirror $OP$ and is reflected for a third time and strikes mirror Magic Square Association $OQ$ at right angles at R, as shown. The distance $OR$ is $5$ cm. The ray XY is d cm from the  mirror $OQ$ . What is the value of $d?$ My thoughts: the answer says its 5, but I am not sure how to specifically construct the triangles or angles and sides to be able to solve the question. I did write a bunch of angles but Im not sure how to proceed. Can anyone help me solve this problem?","['physics', 'geometry']"
4665554,Analytic solution to coupled nonlinear first order ODEs with quadratic terms on the right hand side,"I am trying to find the analytic solution to the following nonlinear, coupled, first order ODE: $$\frac{dx}{dt} = -2ax^2-bxy$$ $$\frac{dy}{dt} = ax^2-bxy$$ I started with calculating $\frac{dx}{dy}$ and $\frac{dy}{dx}$ , but I found $\frac{dx}{dy} \neq (\frac{dy}{dx})^{-1}$ , which means we can't change the order of derivative. I failed to decoupled the variables, and I am also not sure if such an analytical solution can be found. I looked at two paper that discussed similar cases: https://doi.org/10.1007/s11040-021-09400-7 This paper discusses a subset of the ODE system with 2nd order polynomial on the right-hand side. Unfortunately, my system is not in that set. https://doi.org/10.1063/5.0011257 This paper is in the citation of the previous one. It only focuses on ODE systems with quadratic term on the RHS, but I haven't fully understood the method yet. If someone knows how to approach the problem and is willing to give some hints, it will be much appreciated!! Thank you!! -----------------------------------------------------Update------------------------------------------------------ Inspired by one of my friends, I do the following transformation: Multiply $\frac{y}{x^2}$ to the first equation and multiply $\frac{1}{x}$ second one, I get: $$\frac{yx'}{x^2} = -2ay-b\frac{y^2}{x}$$ $$\frac{xy'}{x^2} = ax-by$$ let $u = \frac{y}{x}$ , $$\frac{yx'}{x^2} = -2aux-bu^2x$$ $$\frac{xy'}{x^2} = ax-bux$$ Then, subtract these two equations: $$\frac{yx'-xy'}{x^2} = \frac{du}{dt} = x(-bu^2-(2a-b)u-a)$$ Similarly, we can also write $$\frac{dx}{dt} = x^2(-2a-u)$$ Then we have $$\frac{du}{dx} = \frac{1}{x}\frac{-bu^2-(2a-b)u-a}{-2a-u}$$ $$\frac{1}{x} dx= \frac{2a+u}{bu^2+(2a-b)u+a}du$$ $$\int \frac{1}{x} dx = \log{x} = \int \frac{2a+u}{bu^2+(2a-b)u+a}du$$ According to Wolfram, The integration equals: Let's call it $A(u)$ . Then we know $x = e^{A(u)}$ . Plug x in the ODE, then: $$\frac{du}{dt} = e^{A(u)}(-bu^2-(2a-b)u-a)$$ However, analytic form of u can not be calculated by Wolfram... Is there any way to get around it? -----------------------------------------------------update----------------------------------------------------
The numerical solution for $a=b=1$ is, If solving the equation analytically is not possible, is there any way to compute the asymptotic behavior directly? (Here, $x$ approaches $0$ and $y$ approaches $0.16303362$ ) -----------------------------------------------------Update------------------------------------------------------ I have successfully calculated the equilibrium state, and I do agree with the answer below that there's no analytical form. Thank you for the help guys!",['ordinary-differential-equations']
4665572,does different domain for inverse trigonometric function give different definite integral?,"$\int_{-1}^1 {\frac{\tan^{-1}x}{1+x^2}} \; dx$ becomes $\int_{\tan^{-1}-1}^{\tan^{-1}1} {\theta} \; d\theta$ by substituting $x = \tan(\theta)$ different domain $\left(-\frac\pi2, \frac\pi2\right)$ or $\left(-\frac\pi2 + \pi, \frac\pi2 + \pi\right)$ for the function tan would give different answer. I think they must be equal, and I am missing something.. I can't see which statement is causing the problem. when I set the domain of than to be $\left(-\frac\pi2 + \pi, \frac\pi2 + \pi\right)$ $x=\tan\theta$ $dx = \sec^2(\theta) \ d \theta$ $\tan^{-1}1 = \pi + \frac\pi4$","['trigonometry', 'trigonometric-integrals']"
4665579,About the definition of the morphisms of varieties in Hartshorne's algebraic geometry.,"I am reading Hartshorne's algebraic geometry. According to the definition about morphism of varieties in Hartshorne's book, a morphism $\varphi:X \to Y$ should not only be continuous but also preserve all local regular functions on Y. I am naturally thinking about when a continuous map $\varphi:X \to Y$ is not a morphism of varieties?(Examples?) At the same times, I assume that if the continuous map $\varphi:X \to Y$ is in 'the fraction form of polynomials', i.e. $\varphi(x_1,\cdots,x_m)=(\frac{f_1(x_1,\cdots,x_m)}{g_1(x_1,\cdots,x_m)},\cdots,\frac{f_n(x_1,\cdots,x_m)}{g_n(x_1,\cdots,x_m)})$ if $X\subset \mathbb{A}^m$ and $Y\subset\mathbb{A}^n$ $\varphi(x_1,\cdots,x_m)=[\frac{f_0(x_1,\cdots,x_m)}{g_0(x_1,\cdots,x_m)},\cdots,\frac{f_n(x_1,\cdots,x_m)}{g_n(x_1,\cdots,x_m)}]$ if $X\subset \mathbb{A}^m$ and $Y\subset\mathbb{P}^n$ $\varphi[x_0,\cdots,x_m]=[\frac{f_0(x_0,\cdots,x_m)}{g_0(x_0,\cdots,x_m)},\cdots,\frac{f_n(x_1,\cdots,x_m)}{g_n(x_1,\cdots,x_m)}]$ if $X\subset \mathbb{P}^m$ and $Y\subset\mathbb{P}^n$ $\varphi[x_0,\cdots,x_m]=[\frac{f_1(x_0,\cdots,x_m)}{g_1(x_0,\cdots,x_m)},\cdots,\frac{f_n(x_1,\cdots,x_m)}{g_n(x_1,\cdots,x_m)}]$ if $X\subset \mathbb{P}^m$ and $Y\subset\mathbb{A}^n$ (obviously there are some conditions on polynomials $f_i,g_i$ especially when When $X$ and $Y$ are involved in projective varieties, but they do not matter here) then $\varphi:X\to Y$ is a morphism of varieties, since for each locally regular function $\frac{p}{q}:U\subset Y\to \mathbb{K}$ , the composition $\frac{p}{q}\circ \varphi:\varphi^{-1}(U)\to \mathbb{K}$ could also be represented by the form $\frac{p'}{q'}$ of polynomials ( $q'$ is obviously nowhere zero on $\varphi^{-1}(U)$ ). So does the second condition 'preserve all local regular functions' just mean the continuous map should be in 'the fraction form of polynomials'?","['affine-varieties', 'algebraic-geometry', 'commutative-algebra', 'projective-varieties']"
4665631,"Normal Family of Entire Functions, Convergence","I am looking for hints for this old prelim exam question. Let $\mathcal{F}$ be a normal family of entire functions, and let $$A_n=\sup\left\{|a_n|:f(z)=\sum_{j=0}^{\infty}a_jz^j,f\in\mathcal{F}\right\}$$ for $n=0,1,2,\ldots.$ Show that $\sum_{n=0}^{\infty}A_nr^n<\infty$ for all $r>0.$ My first step is to recognize that $A_n=\frac{1}{n!}\sup_{f\in\mathcal{F}}|f^{(n)}(0)|.$ For each $n,$ there exists a sequence of functions $(f_{n,k})_{k=1}^{\infty}$ in $\mathcal{F}$ such that $$\lim_{k\to\infty}\frac{\left|f_{n,k}^{(n)}(0)\right|}{n!}=A_n.$$ Now since $\mathcal{F}$ is a normal family, the sequence $(f_{n,k})_{k=1}^{\infty}$ admits a subsequence $\left(f_{n,k_j}\right)_{j=1}^{\infty}$ such that there exists a function $g_n\colon \mathbb{C}\to\mathbb{C}$ such that $f_{n,k_j}\xrightarrow[j\to\infty]{}g_n$ uniformly on compact sets. Then it can be shown that $g_n$ is entire for each $n,$ and $f_{n,k_j}^{(m)}\xrightarrow[j\to\infty]{}g_n^{(m)}$ uniformly on compact sets for all orders $m\in \mathbb{N},$ in particular $f_{n,k_j}^{(n)}\xrightarrow[j\to\infty]{}g_n^{(n)}$ uniformly on compact sets. It follows that $\frac{\left|g_n^{(n)}(0)\right|}{n!}=A_n.$ My hope is to use the functions $g_n$ find an entire function $h$ with the property that $h^{(n)}=g_n^{(n)}$ for all orders $n,$ but don't see how to do this. Does this seem to be a good strategy, or is there a better approach I am missing? I feel that I am not making enough use of the fact that $\mathcal{F}$ consists of entire functions - is there a way to use this fact here?",['complex-analysis']
4665676,Convergence of generalized hypergeometric function for the case $p=q+1$,"it is known that the generalized hypergeometric function is defined by: \begin{equation}
	\label{e:pFq}
	{}_{p}F_q(\textbf{a};\textbf{b};z)=\sum_{n=0}^{\infty}\frac{(a_1)_n\cdots (a_p)_n}{(b_1)_n\cdots(b_q)_n}\frac{z^n}{n!},
\end{equation} where \begin{equation}
	\label{relposchgamma}
	(z)_n = \frac{\Gamma(z+n)}{\Gamma(z)}
\end{equation} is the Pochammer symbol, $\textbf{a}\in\mathbb{C}^p$ , $\textbf{b}\in\mathbb{C}^q$ and $z\in\mathbb{C}$ . I am studying the case $p=q+1$ . On the one hand, I know by comparison limit test that it is absolutely convergent if $\alpha=\Re\left(\sum_{j=0}^{q}b_j-\sum_{j=0}^{q+1}a_j\right)>0$ . On the other hand, I read in the literature that if $\alpha\leq -1$ it is divergent; and, if $-1<\alpha\leq 0$ and $z\neq 1$ , the series converges, but I don't know how to prove it, so I would like you to give me some help to try it out. Thanks a lot,
Ivan.","['analysis', 'complex-analysis', 'absolute-convergence', 'convergence-divergence', 'hypergeometric-function']"
4665849,Does set-wise convergence of probability measures imply convergence of moments?,"Let $(S, d)$ be a complete separable metric space and consider the corresponding Borel $\sigma$ -algebra $\mathcal{B}(S)$ . Let $\mu$ and $\mu_n$ , $n \in \mathbb{N}$ , be probability measures on $\mathcal{B}(S)$ with finite first moments such that $$
\lim_{n \rightarrow \infty} \mu_n(A) = \mu(A) \quad \forall A \in \mathcal{B}(S).
$$ According to this post , we have $$
\lim_{n \rightarrow \infty} \int_S f \, d\mu_n = \int_S f \, d\mu
$$ for all bounded and Borel-measurable functions $f : S \rightarrow \mathbb{R}$ . Since the latter is true for bounded and Lipschitz continuous functions, it further follows that $\mu_n$ converges weakly to $\mu$ . I am further interested in the following question: Is it true that $$
\lim_{n \rightarrow \infty} \int_S d ( x, x_0 ) \mu_n (dx) = \int_S d (x, x_0) \mu (dx), \tag{1}
$$ for some $x_0 \in S$ . In other words, do we have the convergence of the corresponding moments? Moreover, it is known that weak convergence together with $(1)$ is equivalent to the convergence with respect to the Wasserstein metric. Hence the question could be formulated as: Does set-wise convergence of probability measures imply convergence with respect to the Wasserstein metric?","['measure-theory', 'optimal-transport', 'probability-theory', 'real-analysis']"
4665886,Is there a way to compute this nested sum efficiently?,"I came across this problem while coding. I was wondering if there is an easy way to compute the following expression: for a set of $n$ values $a_1 \le a_2 \le ... \le a_n \le A$ , calculate $$\sum_{k_1=0} ^{a_1} \sum_{k_2=k_1} ^{a_2}\dots\sum_{k_n=k_{n-1}} ^{a_n} 1$$ Now this looks quite intimidating but I was thinking that there might be a relatively fast way of calculating it. Specifically I'd need something for $n = 7$ and $A = 52$","['summation', 'combinatorics']"
4665913,"Does the operator have any eigenvalues, also is it compact?","Question:
Let $H=L^2([0,1],m)$ , where $m$ is the Lebesgue measure, and consider th    e operators $M,S\in L(H,H)$ given by $$ Mf(t)=tf(t), \; Sf(t)=f(1-t), f\in H, t\in [0,1]$$ You are not asked to verify that $M$ and $S$ defined bounded linear operators on $H$ .\ Justify that $M$ is self-adjoint, but not compact. Show that $\lVert Sf\rVert_2 = \lVert f \rVert_2$ , for all $f\in H$ . Does $S$ have any eigenvalues? Is $S$ compact? My Attempt:
Note for $f_1\in H$ $$\langle M_t f,f_1\rangle=\int_{[0,1]} tf(t)\overline{f_1(t)}dm(t)=\int_{[0,1]} f(t)\overline{tf_1(t)}dm(t)=\langle f_1, M_{t}f\rangle$$ where we used $t\in [0,1]\subset \mathbb{R}$ . Thus it is self-adjoint. Moreoever by (something we proved in class) it follows that $M$ has no eigenvalues, yet $\sigma(M)=[0,1]$ , and so suppose for the sake of contradiction that $M$ is indeed compact. Then each non-zero $\lambda    \in [0,1]$ would also be an eigenvalue of $M$ , contradictory to the fact that $M$ has no eigen-values.
Note $$ \langle Sf,f_1\rangle=\int_{[0,1]}f(1-t)\overline{f_1(t)}dm(t)=\int_{[0,1]}f(u)\overline{f(1-u)}dm(u)=\langle f,Sf\rangle$$ 0 Where we simply made a change of variables. To see $\lVert Sf\rVert_2=\lVert f \rVert_2$ note; $$\lVert Sf\rVert=\int_[0,1] f(1-t)^2 dm(t)=\int_[0,1] f(u)^2dm(u)=\lVert f\rVert
 $$ where we made a simple change of variables. So we have shown $S$ is an isometry. Since we have isometry it follows that $$\langle f,f \rangle=\langle Sf, Sf\rangle=\langle \lambda f, \lambda f\rangle =\vert \lambda\vert^2 \langle f,f\rangle$$ from which we clearly see the only possible eigenvalues are $-1,1$ . Suppose for the sake of contradiction that $S$ is compact. Then we must have $\sigma(S)\setminus{\{0\}}=EV(S)\setminus{\{0\}}$ . But $\sigma(S)\setminus{\{0\}}\neq \{-1,1\}=EV(S)\setminus{\{0\}}$ and hence we have reached a contradiction and we conclude that $S$ is not compact. My questions:
Does the proof look right?","['operator-theory', 'spectral-theory', 'compact-operators', 'functional-analysis']"
4665924,Questions about Stokes's Theorem on Manifolds from Prof Shifrin's Lectures,"I've been watching Professor Ted Shifrin's brilliant lectures on Stokes's Theorem, and I had a few questions that I don't think were answered: Rectangles in the plane are not manifolds with boundary as the lectures define them, so I don't think Stokes's Theorem (as discussed in the lectures) would apply to them. But Professor Shifrin said that Stokes's Theorem was a generalization of Green's Theorem, which was grounded in rectangles in the lectures. How do these two ideas hold at once? Professor Shifrin provided an example of a region on which the integral of a differential form on which wasn't parametrized ""exactly,"" but the integral over the small segment canceled out. Then, he asserted that a similar cancellation would not occur when integrating over a Möbius strip. (The relevant portion of the lecture starts here: https://youtu.be/5k13cowATAw?t=644 ) Why doesn't a similar cancellation occur when integrating over a Möbius strip? I would think that the segment would have opposite orientation when parametrized each time. Why does the integral on that segment even matter? (I know this is probably a stupid question.) On first glance, my thought was that this boundary region had 2d volume zero, so it wouldn't matter in the grand scheme of integration. Professor Shifrin said that the Möbius strip could be parametrized locally but not globally. Why don't we define the integral over the whole manifold to be the sum of the integrals along each coordinate patch, similar to what we do for piecewise smooth manifolds? Thank you so much for your help!!","['multivariable-calculus', 'differential-forms', 'differential-geometry']"
4665979,How to extend this Taylor series formulation to second order ODE?,"Let $y^{\prime}=f\left(  x,y\right) $ be first order ode with expansion around $x_{0}$ with
initial conditions $y\left(  x_{0}\right)  =y_{0}$ , and where $f\left(
x,y\right)  $ is analytic at $x_{0}$ then the solution to the ode in series expansion around $x_0$ by Taylor series is given by $$
y=y_{0}+\sum_{n=1}^{\infty}\frac{x^{n}}{n!}\left.  F_{n}\left(  x,y\right)
\right\vert _{x=x_{0},y=y_{0}}
$$ Where \begin{align*}
F_{1}\left(  x,y\right)    & =f\left(  x,y\right)  \\
F_{n+1}\left(  x,y\right)    & =\frac{\partial F_{n}}{\partial x}+\left(
\frac{\partial F_{n}}{\partial y}\right)  F_{1}
\end{align*} What would be the equivalent formulation (if one exists) for a second order
ode $y^{\prime\prime}=f\left(  x,y,y^{\prime}\right)  $ with initial
conditions $y\left(  x_{0}\right)  =y_{0},y^{\prime}\left(  x_{0}\right)
=y_{0}^{\prime}$ where it is assumed also that $f\left(  x,y,y^{\prime}\right)  $ is analytic at $x_{0}$ ? The above was taken from sympy ode solver here with reference for the above formula given as Travis W. Walker, Analytic power series technique for solving
first-order differential equations, p.p 17, 18 But I could not find such book searching.  I could only find this page which references paper Walker, T.W.  “Analytic Power Series Technique for Solving First-Order
Ordinary Differential Equations.”  MAA Rocky Mountain Section 2008
Meeting, Spearfish, South Dakota.  25-26 Apr. 2008. Where the above formulation is given. But all links from the above page are dead now. One advantage of this formulation over standard power series, is that is it easier to automate and to program. (no need to find recurrence relation for the $a_n$ for example). The $F_n$ expressions above do this job already. Is it possible to extend the above for second order ode by use of Taylor series expansion? We would need additional term for the partial derivative of $f(x,y,y')$ w.r.t. $y'$ ofcourse. Does any one knows of a reference where it is given in the above form? (compared to the standard power series form, for ordinary point).","['power-series', 'taylor-expansion', 'ordinary-differential-equations']"
4665985,to find Infimum of set $ \{\frac{1}{2^n} : n \in \mathbb{N}\}$,"to find infimum  of set $ \{\frac{1}{2^n} : n \in \mathbb{N}\}$ Clearly $0$ is lower bound of set. Let $l$ be another lower bound such that $ l > 0 $ .Now by Archimedian property we have $\frac{1}{n} < l $ for some $n$ . Also we have for all $n \in \mathbb{N}$ , $2^n> n$ . so $\frac{1}{2^n} < \frac{1}{n}$ . In particular we have , $\frac{1}{2^n} < l$ . hence a contradiction I am not entirely sure about my proof and wondering what other ways can be used to prove this ?",['real-analysis']
4665991,Confusion regarding independent events,"Two unbiased dice are rolled. Let A be the event that sum is $6$ and $B$ be the event that sum is $9$ . Are these events independent? Intuitively it seems that these events are independent. But, $P(A)=5/36$ and $P(B) =4/36$ and $A \cap B = \phi$ so $P(A \cap B) =P(A) P(B) $ is not satisfied. What am i missing here?","['probability-theory', 'probability']"
4665994,Is this $1$-form holomorphic?,"Example .
Consider the smooth affine plane curve $X=\{(x,y)\in \mathbb{C}^2\ \vert \ y^2=x^3-1\}$ . Is the $1$ -form defined as $\omega=dx/y$ where $y\neq 0$ a holomorphic $1$ -form on a Riemann surface? I tried writing out the transition maps but had to choose branches of the square/cube root, and am not sure that I am on the right track. More generally . Let $f(x,y)$ be a non-singular bivariate polynomial over $\mathbb{C}$ and look at $X=\{f(x,y)=0\}$ . How do I decide whether a differential form on $X$ given locally by a single formula $gdx$ (with $g$ a holomorphic function) is holomorphic? My idea. We know that $\frac{\partial}{\partial x}fdx+\frac{\partial}{\partial y}fdy=0$ . So in the above example $3x^2dx=2ydy$ . Therefore where $x\neq 0$ and $y\neq 0$ we know that $\frac{dx}{y}=\frac{2}{3x^2}dy$ . We conclude that $dx/y$ gives a holomorphic $1$ -form on $X\setminus\{(\zeta_1,0),(\zeta_2,0),(\zeta_3,0),(0,i),(0,-i)\}$ , where $\zeta_1, \zeta_2, \zeta_3$ are the three third complex roots of unity. Am I right?","['algebraic-curves', 'riemann-surfaces', 'complex-analysis', 'algebraic-geometry', 'differential-forms']"
4666016,Residue of the pullback,"Suppose $p\colon Y \rightarrow X$ is a holomorphic mapping of Riemann surfaces, $a\in X$ , $b\in p^{-1}(a)$ and $k$ is the multiplicity of $p$ at $b$ . Then I am trying to show the claim: Given any holomorphic $1$ -form $\omega$ on $X\setminus\{a\}$ we have $$\operatorname{Res}_b(p^*\omega)= k \operatorname{Res}_a(\omega).$$ The residue of $\omega$ at $a$ is defined to be the coefficient of the exponent $z^{-1}$ in the Laurent expansion (with respect to a chart $z$ of $X$ centered at $a$ ) of the coefficient function $f$ of $\omega$ with respect to the chart $z$ (i.e. $\omega=fdz$ ). The residue of $p^*\omega$ at $b$ is defined similarly. I tried to somehow use the local normal form of $p$ to show above claim, but I can't quite figure out how. Any hints?","['complex-analysis', 'riemann-surfaces', 'pullback', 'differential-forms']"
4666040,How to know if a Numerical method gives an exact solution?,"Consider the following 2 step numerical method $y_{n+1}-y_n=\frac{h}{2}(3f_n -f_{n-1})$ I proved that this method is convergent but the problem asked Show that for any step size h, the method gives exact solutions to the initial value problems $y'=ax+b$ , with $y(x_0)=y_0$ ( $a,b \in \mathbb{R}$ ) provided that we choose $y(x_1)=y_1$ Well the exact solution is $y=\frac{a}{2}x²+bx+c$ . How to show that the numerical method gives an exact result? I tried finding $y_2=y(x_0+2h)$ in the numerical and the exact method and they both gave me the same answer $y_2= \frac{a}{2}x_0²+ bx_0+ 2ahx_0+ 2bh +2ah²+c$ Is this enough?
Or how do I solve such a question?","['numerical-methods', 'ordinary-differential-equations']"
4666073,Find second derivative of $\gamma (t) = A^{-1}(t)$ where $A: \mathbb{R} \to GL(\mathbb{R}^l)$ is smooth curve.,"Find second derivative of $\gamma (t) = A^{-1}(t)$ where $A: \mathbb{R} \to GL(\mathbb{R}^l)$ is smooth curve. What I should get is following $$\gamma '' = 2 \gamma ' A \gamma ' - A^{-1} \frac{d^2 A}{dt^2} A^{-1}.$$ I have proven that if $f:GL(X) \to GL(X)$ given as $f(L) = L^{-1} $ , where $GL(X) := \{ L \in \mathcal{L} (X; X) \mid \exists L^{-1} \in \mathcal{L} (X; X) \} $ , first derivative of such a map is $$Df(A)h = -A^{-1} h A^{-1}.$$ I've tried to do something with that but I was unable to get exactly what is required.","['derivatives', 'linear-transformations', 'real-analysis']"
4666147,bounded normal operator and spectrum,"Problem: If A is a bounded normal operator, the spectrum $\sigma(A)=\{s+it:s \in \sigma(B),t \in \sigma(C)\}$ , where B, C are bounded self adjoint operators which commute. Fact: A bounded normal operator A can be written $A=B+iC$ , where B,C are bounded self adjoint operators which commute. Fact: Let H be a complex Hilbert space and let $A:H \rightarrow H$ be a bounded complex linear operator, then A is normal if only if $\Vert A^*x \Vert=\Vert Ax \Vert$ for all $x \in H$ . Also every self-adjoint operator is normal. I was told there is a mistake in the problem, but have not spotted it. Thanks in advance.",['functional-analysis']
4666170,"2D Laplacian with 1 Non-Homogeneous Dirichlet, 1 Homogeneous Dirichlet and 2 Homogeneous Neumann BCs, Rectangular Domain","To preface, I am very new to PDEs and not overly familiar with hyperbolic trig so forgive me if there are obvious mistakes - I have tried my best to research this problem before asking this question. Additionally, I am not an applied mathematician so intuition and applications don't really help my understanding. The question: $$
u_{xx} + u_{yy} = 0, \;\; 0<x<1,\;\; 0<y<1 \\
u(x,0) = f(x), \;\; u(x,1) = 0 \\
u_x(0,y) = u_x(1,y) = 0
$$ Where $f(x)$ is an integrable function defined on $0\leq x\leq 1$ that has vanishing derivative at $x=0$ and $x=1$ . My Solution: Seek a solution of the form $u(x,y) = X(x)Y(y)$ , then the PDE becomes: $$
X''Y + XY'' = 0 \implies \frac{X''}{X}=-\frac{Y''}{Y} = \lambda
$$ Implement homogeneous boundary conditions: $$
u_x(0,y) = X'(0)Y(y) = 0 \implies X'(0) = 0 \\
u_x(1,y) = X'(1)Y(y) = 0 \implies X'(1) = 0 \\
u_x(x,1) = X'(x)Y(1) = 0 \implies Y'(1) = 0 
$$ Now, consider the ODE for $X(x)$ : $$
X'' - \lambda X = 0
$$ We require $\lambda < 0$ so introduce $\omega$ such that $\lambda = - \omega^2$ . Thus, we get general solution, $$\begin{align}
X &= A\sin(\omega x) + B\cos(\omega x)\\
\implies X' &= A\omega \cos(\omega x) - B\omega \sin(\omega x)\\
\\
X'(0) &= A\omega \implies A = 0, \;\; X'(1) = -B\omega \sin(\omega) = 0 \implies \sin(\omega) = 0 \\
\end{align}$$ Thus, we get the solution for $X(x)$ : $$
X_n(x) = B_n \cos(\omega_n x), \;\; \lambda_n = -\omega_{n}^2, \;\; \omega_n = \pi n,\;\; n = 1, 2, 3,...
$$ Now, the ODE for $Y(y)$ becomes: $$
Y_n'' - \omega_{n}^2 Y_n = 0
$$ Which has general solution, $$
Y_n = C_n \sinh(\pi n y) + D_n \cosh(\pi n y)
$$ And, this is where I am stuck... Implementing the homogeneous boundary condition on $Y_n$ , we get $$
Y_n(1) = C_n \sinh(\pi n) + D_n \cosh(\pi n) = 0
$$ But, it seems to me that the only way to satisfy this is with the trivial solution $C_n = D_n = 0$ . So, my question is, am I missing something with how to implement this boundary condition? Or, have I done something wrong earlier in the question?","['harmonic-functions', 'hyperbolic-functions', 'multivariable-calculus', 'partial-differential-equations', 'boundary-value-problem']"
4666233,"Solution for $x$ of $\,y=(a+bx)^x$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Solution for $x$ of $\,y=(a+bx)^x$ Is there an algebraic solution for $x$ to this equation? I've tried using the Lambert $W$ function and approximating it, but I've had no luck with that. Even an approximation of the solution would be appreciated.","['algebra-precalculus', 'approximation', 'approximation-theory']"
4666236,How can I compute the $n$-th derivative of $e^{a e^{-x}}$,"I'm trying to compute the $n$ -th derivative of the function $e^{a e^{-x}}$ , but I'm stuck. I found that I could use Faa di Bruno's formula but maybe I'm worsening the problem to the computation of the Exponential Bell Polynomial $B_{n, k} (-a e^{-x}, ae^{-x}, -ae^{-x}, ae^{-x}, ...)$ and I'm not sure how to go on. Actually it will be much welcomed even just a way to compute the derivative in the case $x=0$ . Do you think there is a way to compute this derivative in a more or less closed form following this path or another?","['derivatives', 'polynomials', 'real-analysis']"
4666280,Solving the Fractional Differential Equation $\alpha D^{2v}y(x) + \beta D^{v}y(x) + \gamma y(x) = f(x)$,"Introduction I'm very interested in fractional calculus, especially in Fractional Differential Equations (FDEs). The question arises how to solve the FDE $\alpha \cdot \operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + \gamma \cdot y\left( x \right) = f\left( x \right)$ where $\left\{ \alpha,\, \beta,\, \gamma,\, y\left( x \right),\, f\left( x \right),\, v,\, x \right\} \in \mathbb{C}$ and where $\operatorname{D}^{v} \left( y\left( x \right) \right)$ is the $v$ -th derivative of $y\left( x \right)$ . The FDE is simple, but I don't see a trivial solution. My attempts at a solution so far have only led to a lesson or restricted the FDE to an ODE with $v \in \mathbb{R}$ .
Hence the question: How to solve the FDE $\alpha \cdot \operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + \gamma \cdot y\left( x \right) = f\left( x \right)$ where $\left\{ \alpha,\, \beta,\, \gamma,\, y\left( x \right),\, f\left( x \right),\, v,\, x \right\} \in \mathbb{C}$ ? Similar FDE Of course there are similar FDEs, like the special case $f\left( x \right) = 0$ : $$
\begin{align*}
\alpha \cdot \operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + \gamma \cdot y\left( x \right) &= f\left( x \right)\\
\alpha \cdot \operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + \gamma \cdot y\left( x \right) &= 0\\
\end{align*}
$$ $$
\begin{align*}
\alpha \cdot \operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + \gamma \cdot y\left( x \right) &= f\left( x \right)\\
\operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \frac{\beta}{\alpha} \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + \frac{\gamma}{\alpha} \cdot y\left( x \right) &= \frac{1}{\alpha} \cdot f\left( x \right)\\
\operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + a \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + b \cdot y\left( x \right) &= \frac{1}{\alpha} \cdot f\left( x \right)\\
\end{align*}
$$ $$
\begin{align*}
\operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \frac{\beta}{\alpha} \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + \frac{\gamma}{\alpha} \cdot y\left( x \right) &= 0\\
\end{align*}
$$ This FDE has e.g. given a solution by: $$
\begin{align*}
y\left( x \right) &= \begin{cases}
\sum\limits_{k = 1}^{\frac{1}{v} - 1}\left( a^{\frac{1}{v} - k - 1} \cdot \operatorname{E}_{x}\left( -k \cdot v;\, a^{\frac{1}{v}} \right) - b^{\frac{1}{v} - k - 1} \cdot \operatorname{E}_{x}\left( -k \cdot v;\, b^{\frac{1}{v}} \right) \right), &\text{if } a \ne b\\
x \cdot \exp\left( a \cdot x \right) \cdot \sum\limits_{k = -\frac{1}{v} + 1}^{\frac{1}{v} - 1}\left(  a^{k} \cdot \left( \frac{1}{v} - \left| k \right| \right) \cdot \exp\left( a^{\frac{1}{v}} \cdot x \right) \right), &\text{if } a = b \ne 0\\
\frac{1}{\Gamma\left( 2 \cdot v \right)} \cdot x^{2 \cdot v - 1 }, &\text{if } a = b = 0\\
\end{cases}\\
\end{align*}
$$ where $\Gamma\left( \cdot \right)$ is the Gamma Function and $\operatorname{E}_{t}\left( \cdot;\, \cdot \right)$ is the $\operatorname{E}_{t}$ Function . My Best Attempts At Finding A Solution Frobenius Method Without Substitution I would assume that $y\left( x \right)$ has a Maclaurin Series given by $y\left( x \right) = \sum\limits_{k = 0}^{\infty}\left( y_{k} \cdot x^{k} \right)$ . Then we could use Euler's approach for the Fractional Derivative of power functions $\operatorname{D}^{n}\left( x^{m} \right) = \frac{\Gamma\left( m + 1 \right)}{\Gamma\left( m - n + 1 \right)} \cdot x^{m - n}$ and get this: $$
\begin{align*}
\operatorname{D}^{n} \left( y\left( x \right) \right) &= \operatorname{D}^{n} \left( \sum\limits_{k = 0}^{\infty}\left( y_{k} \cdot x^{k} \right) \right)\\
\operatorname{D}^{n} \left( y\left( x \right) \right) &= \sum\limits_{k = 0}^{\infty}\left( y_{k} \cdot \frac{\Gamma\left( k + 1 \right)}{\Gamma\left( k - n + 1 \right)} \cdot x^{k - n} \right)\\
\end{align*}
$$ If we plug $\operatorname{D}^{n} \left( y\left( x \right) \right) = \sum\limits_{k = 0}^{\infty}\left( y_{k} \cdot \frac{\Gamma\left( k + 1 \right)}{\Gamma\left( k - n + 1 \right)} \cdot x^{k - n} \right)$ in, we get: $$
\begin{align*}
\operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + a \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + b \cdot y\left( x \right) &= \frac{1}{\alpha} \cdot f\left( x \right)\\
\sum\limits_{k = 0}^{\infty}\left( y_{k} \cdot \Gamma\left( k + 1 \right) \cdot \left( \frac{1}{\Gamma\left( k - 2 \cdot v + 1 \right)} \cdot x^{k - 2 \cdot v} + a \cdot \frac{1}{\Gamma\left( k - v + 1 \right)} \cdot x^{k - v} + \frac{1}{\Gamma\left( k + 1 \right)} \cdot x^{k} \right) \right) &= \frac{1}{\alpha} \cdot f\left( x \right)\\
\sum\limits_{k = 0}^{\infty}\left( y_{k} \cdot \Gamma\left( k + 1 \right) \cdot \left( \frac{1}{\left( k - 2 \cdot v - 1 \right) \cdot \Gamma\left( k - 2 \cdot v \right)} \cdot x^{k - 2 \cdot v} + a \cdot \frac{1}{\left( k - v - 1 \right) \cdot \Gamma\left( k - v \right)} \cdot x^{k - v} + \frac{1}{\Gamma\left( k + 1 \right)} \cdot x^{k} \right) \right) &= \frac{1}{\alpha} \cdot f\left( x \right)\\
\end{align*}
$$ Where I have no idea how to continue in a meaningful way. Of course, I could substitute the exponents of $x$ the same, then do a index-shift to put it back together, hoping that helps, but I'd rather give up on that.
However, that would only allow one solution for $v \in \mathbb{Z}$ , but then it is no longer an FDE, but an ODE. With Substitution (for $\gamma = 0$ ) The first sensible substitution that comes to mind is $z\left( x \right) ~{:=}~ \operatorname{D}^{v} \left( y\left( x \right) \right)$ . Which we can rearrange with something and apply the condition $\operatorname{D}^{a} \left( \operatorname{D}^{b} \left( y\left( x \right) \right) \right) = \operatorname{D}^{a + b} \left( y\left( x \right) \right)$ so we get this: $$
\begin{align*}
\alpha \cdot \operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + 0 \cdot y\left( x \right) &= f\left( x \right)\\
\alpha \cdot \operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + 0 &= f\left( x \right)\\
\alpha \cdot \operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) &= f\left( x \right)\\
\alpha \cdot \operatorname{D}^{v + v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) &= f\left( x \right)\\
\alpha \cdot \operatorname{D}^{v} \left( \operatorname{D}^{v} \left( y\left( x \right) \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) &= f\left( x \right)\\
\alpha \cdot \operatorname{D}^{v} \left( z\left( x \right) \right) + \beta \cdot z\left( x \right) &= f\left( x \right)\\
\end{align*}
$$ I would assume that $z\left( x \right)$ has a Maclaurin Series given by $z\left( x \right) = \sum\limits_{k = 0}^{\infty}\left( z_{k} \cdot x^{k} \right)$ . Then we could use Euler's approach for the Fractional Derivative of power functions $\operatorname{D}^{n}\left( x^{m} \right) = \frac{\Gamma\left( m + 1 \right)}{\Gamma\left( m - n + 1 \right)} \cdot x^{m - n}$ and get this: $$
\begin{align*}
\alpha \cdot \operatorname{D}^{v} \left( z\left( x \right) \right) + \beta \cdot z\left( x \right) &= f\left( x \right)\\
\alpha \cdot \sum\limits_{k = 0}^{\infty}\left( z_{k} \cdot \frac{\Gamma\left( k + 1 \right)}{\Gamma\left( k - v + 1 \right)} \cdot x^{k - v} \right) + \beta \cdot \sum\limits_{k = 0}^{\infty}\left( z_{k} \cdot x^{k} \right) &= f\left( x \right)\\
\alpha \cdot \sum\limits_{k = -v}^{\infty}\left( z_{k + v} \cdot \frac{\Gamma\left( k + 1 \right)}{\Gamma\left( k + 1 \right)} \cdot x^{k} \right) + \beta \cdot \sum\limits_{k = 0}^{\infty}\left( z_{k} \cdot x^{k} \right) &= f\left( x \right)\\
\alpha \cdot \sum\limits_{k = -v}^{0}\left( z_{k + v} \cdot \frac{\Gamma\left( k + 1 \right)}{\Gamma\left( k + 1 \right)} \cdot x^{k} \right) + \alpha \cdot \sum\limits_{k = 0}^{\infty}\left( z_{k + v} \cdot \frac{\Gamma\left( k + 1 \right)}{\Gamma\left( k + 1 \right)} \cdot x^{k} \right) + \beta \cdot \sum\limits_{k = 0}^{\infty}\left( z_{k} \cdot x^{k} \right) &= f\left( x \right)\\
\alpha \cdot \sum\limits_{k = -v}^{0}\left( z_{k + v} \cdot \frac{\Gamma\left( k + 1 \right)}{\Gamma\left( k + 1 \right)} \cdot x^{k} \right) + \sum\limits_{k = 0}^{\infty}\left( \left( \alpha \cdot z_{k + v} \cdot \frac{\Gamma\left( k + 1 \right)}{\Gamma\left( k + 1 \right)} + \beta \cdot z_{k} \right) \cdot x^{k} \right) &= f\left( x \right)\\
\end{align*}
$$ However, that would only allow one solution for $v \in \mathbb{Z}$ , but then it is no longer an FDE, but an ODE. Solving For The Homogeneous Equation With Substitution (for $\gamma = 0$ ) The first sensible substitution that comes to mind is $z\left( x \right) ~{:=}~ \operatorname{D}^{v} \left( y\left( x \right) \right)$ . Which we can rearrange with something and apply the condition $\operatorname{D}^{a} \left( \operatorname{D}^{b} \left( y\left( x \right) \right) \right) = \operatorname{D}^{a + b} \left( y\left( x \right) \right)$ so we get this: $$
\begin{align*}
\alpha \cdot \operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + 0 \cdot y\left( x \right) &= f\left( x \right)\\
\alpha \cdot \operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) + 0 &= f\left( x \right)\\
\alpha \cdot \operatorname{D}^{2 \cdot v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) &= f\left( x \right)\\
\alpha \cdot \operatorname{D}^{v + v} \left( y\left( x \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) &= f\left( x \right)\\
\alpha \cdot \operatorname{D}^{v} \left( \operatorname{D}^{v} \left( y\left( x \right) \right) \right) + \beta \cdot \operatorname{D}^{v} \left( y\left( x \right) \right) &= f\left( x \right)\\
\alpha \cdot \operatorname{D}^{v} \left( z\left( x \right) \right) + \beta \cdot z\left( x \right) &= f\left( x \right)\\
\end{align*}
$$ Composing the characteristic equation gives: $$
\begin{align*}
\alpha \cdot \operatorname{D}^{v} \left( z\left( x \right) \right) + \beta \cdot z\left( x \right) &= f\left( x \right)\\
\alpha \cdot \lambda^{v} + \beta \cdot 1 &= 0\\
\alpha \cdot \lambda^{v} + \beta &= 0\\
\lambda^{v} &= -\frac{\beta}{\alpha}\\
\lambda &= \left( -\frac{\beta}{\alpha} \right)^{\frac{1}{v}}\\
\lambda_{k} &= \sqrt[v]{\left| \frac{\beta}{\alpha} \right|} \cdot \operatorname{cis}\left( \operatorname{Arg}\left( -\frac{\beta}{\alpha} \right) + 2 \cdot k \cdot \pi \right)^{\frac{1}{v}}\\
\lambda_{k} &= \sqrt[v]{\left| \frac{\beta}{\alpha} \right|} \cdot \operatorname{cis}\left( \frac{\operatorname{Arg}\left( -\frac{\beta}{\alpha} \right) + 2 \cdot k \cdot \pi}{v} \right)\\
\lambda_{k} &= \sqrt[v]{\left| \frac{\beta}{\alpha} \right|} \cdot \operatorname{cis}\left( \frac{\operatorname{Arg}\left( -\frac{\beta}{\alpha} \right)}{v} + \frac{2}{v} \cdot k \cdot \pi \right)\\
\end{align*}
$$ where $\operatorname{Arg}\left( z \right)$ is the argument of z wich is the closest to $0$ . This means that  the roots of the equation are $\lambda_{k} = \sqrt[v]{\left| \frac{\beta}{\alpha} \right|} \cdot \cos\left( \frac{\operatorname{Arg}\left( -\frac{\beta}{\alpha} \right)}{v} + \frac{2}{v} \cdot k \cdot \pi\right) + \sqrt[v]{ \left| \frac{\beta}{\alpha} \right|} \cdot \sin\left( \frac{\operatorname{Arg}\left( -\frac{\beta}{\alpha} \right)}{v} + \frac{2}{v} \cdot k \cdot \pi\right) \cdot i$ aka $\lambda^{v} + \frac{\beta}{\alpha} = \prod\limits_{k = 1}^{\left| \mathbb{S} \right|}\left( \lambda - \sqrt[v]{\left| \frac{\beta}{\alpha} \right|} \cdot \operatorname{cis}\left( \frac{\operatorname{Arg}\left( -\frac{\beta}{\alpha} \right)}{v} + \frac{2}{v} \cdot k \cdot \pi\right) \right)$ where $\mathbb{S}$ is the set of all roots of the equation. This gives the multiplicity of the all roots $= 1$ for $\alpha \ne 0 \ne \beta$ . With $z_{h}\left( x \right) = Q_{0}\left( x \right) \cdot \exp\left( \Re\left( \lambda_{k} \right) \cdot x \right) \cdot \cos\left( \Im\left( \lambda_{k} \right) \cdot x \right) + P_{0}\left( x \right) \cdot \exp\left( \Re\left( \lambda_{k} \right) \cdot x \right) \cdot \sin\left( \Im\left( \lambda_{k} \right) \cdot x \right)$ we'll get $$
\begin{align*}
z_{h}\left( x \right) &= c_{2 \cdot k} \cdot \exp\left( \Re\left( \lambda_{k} \right) \cdot x \right) \cdot \cos\left( \Im\left( \lambda_{k} \right) \cdot x \right) + c_{2 \cdot k + 1} \cdot \exp\left( \Re\left( \lambda_{k} \right) \cdot x \right) \cdot \sin\left( \Im\left( \lambda_{k} \right) \cdot x \right)\\
z_{h}\left( x \right) &= \sum\limits_{k = 1}^{\left| S \right|}\left( c_{2 \cdot k} \cdot \exp\left( \sqrt[v]{\left| \frac{\beta}{\alpha} \right|} \cdot \cos\left( \frac{\operatorname{Arg}\left( -\frac{\beta}{\alpha} \right)}{v} + \frac{2}{v} \cdot k \cdot \pi\right) \cdot x \right) \cdot \cos\left( \sqrt[v]{\left| \frac{\beta}{\alpha} \right|} \cdot \sin\left( \frac{\operatorname{Arg}\left( -\frac{\beta}{\alpha} \right)}{v} + \frac{2}{v} \cdot k \cdot \pi\right) \cdot x \right) + c_{2 \cdot k + 1} \cdot \exp\left( \sqrt[v]{\left| \frac{\beta}{\alpha} \right|} \cdot \cos\left( \frac{\operatorname{Arg}\left( -\frac{\beta}{\alpha} \right)}{v} + \frac{2}{v} \cdot k \cdot \pi\right) \cdot x \right) \cdot \sin\left( \sqrt[v]{\left| \frac{\beta}{\alpha} \right|} \cdot \sin\left( \frac{\operatorname{Arg}\left( -\frac{\beta}{\alpha} \right)}{v} + \frac{2}{v} \cdot k \cdot \pi\right) \cdot x \right) \right)
\end{align*}
$$ where $c$ is a constant. Aka if $v \in \mathbb{N} \Rightarrow \left| \mathbb{S} \right| = v$ . Via using $z\left( x \right) = z_{h}\left( x \right) + z_{p}\left( x \right)$ we would get: $$\fbox{
$y\left( x \right) = \operatorname{D}^{v} \left( z_{h}\left( x \right) + z_{p}\left( x \right) \right)$
}$$ But I'm not sure how to determine $z_{p}$ (Particular Solution), whether I have correctly determined $z_{h}$ (Homogeneous Solution) and how to proceed after further determination.","['fractional-differential-equations', 'fractional-calculus', 'ordinary-differential-equations']"
4666371,Prove that the equation $x^4-4x^3-14x^2-4x+1=0$ has a root $x = \tan 9^\circ$,Prove that the equation $x^4-4x^3-14x^2-4x+1=0$ has a root $x = \tan 9^\circ$ . I have tried calculating $\tan 9^\circ$ and substituting it in the equation but it is pretty tough to calculate that. Can anyone help me with a hint that leads to a better approach to this problem?,"['algebra-precalculus', 'trigonometry']"
4666422,Different proofs of the Pythagorean theorem?,"The Pythagorean Theorem is one of the most popular to prove by mathematicians, and there are many proofs available (including one from James Garfield ). What's are some of the most elegant proofs? My favorite is this graphical one: According to cut-the-knot: Loomis (pp. 49-50) mentions that the
proof ""was devised by Maurice Laisnez,
a high school boy, in the
Junior-Senior High School of South
Bend, Ind., and sent to me, May 16,
1939, by his class teacher, Wilson
Thornton."" The proof has been published by Rufus
Isaac in Mathematics Magazine, Vol. 48
(1975), p. 198.","['euclidean-geometry', 'big-list', 'geometry', 'triangles', 'algebra-precalculus']"
4666432,"Finding the coefficients of $x^0,x^1,x^2, \dots ,x^{10}$ in $(1+x^2+x^5)^{100}$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I have been able to find $x_0: 1,$ $x_1: 0,$ $x_2: {100\choose 1} = 100,$ $x_3: 0,$ $x_4: {100\choose 2} = 4950$ , and $x_5: {100 \choose 1 }= 100$ . I am having trouble with figuring out how to get the rest up to $x_{10}$ . Any suggestions?","['combinatorics', 'discrete-mathematics', 'generating-functions']"
4666472,Prove that $I + A^{T}A$ is invertible,"I'm completely stuck with this seemingly simple problem. I'm trying to prove that matrix $I + A^TA$ is invertible, where $A \in \mathbb{R^{m \times n}}$ . The book where this is from hasn't yet introduced determinants or eigenvalues or other such ""fancier"" results so I should do without those. I've tried to prove that $rank(A) = n$ or that $(I + A^TA)x=0 \implies x=0$ , but none of my approaches seem to lead to any breakthroughs. Any hints? Edit: Based on the replies from Will Jagy and Martin Argerami I was able to see the solution (without the use of eigenvalues). So, here's my formulation for the solution (I think): Suppose that $(I + A^TA)x=0$ where $x$ is a nonzero vector. Then, we also have $x^T(I + A^TA)x=0$ . Because $x^T(I + A^TA)x=x^TIx+x^T(A^TA)x=x^Tx+(Ax)^TAx$ and $x^Tx=\sum_{k = 1}^{n} x^2_k>0$ (for $x \ne 0$ ) and $(Ax)^TAx=\sum_{h=1}^n (\sum_{k=1}^n a_{hk}x_k)^2 \ge 0$ , then we have $x^T(I + A^TA)x=x^Tx+(Ax)^TAx>0$ which is a contradiction. Thus, $x$ cannot be a nonzero vector and $(I + A^TA)x=0 \implies x=0$ which means that $I + A^TA$ is invertible.",['matrices']
4666524,Evaluate the following series having finite terms,Evaluate the following series $$\sum_{n=0}^{2023}\frac{2^n}{1+2023^{2n}}$$ My first thoughts were that this series didn't seem to telescope so there must be some other way of solving it. I did $$S=\sum_{n=0}^{2023}\frac{2^n}{1+2023^{2n}}$$ $$\implies S=\sum_{n=0}^{2023}\frac{2^{2023-n}}{1+2023^{2(2023-n)}}$$ $$\implies S=\sum_{n=0}^{2023}\frac{2^{2023}\cdot2023^{2n}}{2^n(2023^{2n}+2023^{4046})}$$ Now when we add them or subtract both expressions nothing happens. I use this technique in integration so I thought that maybe this is useful here too. Any help is greatly appreciated.,"['algebra-precalculus', 'sequences-and-series']"
4666536,Show that this function is $C^1$ class.,"Let $f:[-1,1]\to\mathbb R$ be $C^1$ class. Define $g:[-1,1]\to\mathbb R$ by $$g(x)=\begin{cases}f(0) &\mathrm{if}\ x=0 \\ \frac{1}{x}\int_0^x f(t)dt &\mathrm{otherwise} \end{cases}$$ Then, show that $g$ is $C^1$ . If $x\neq 0$ , $g$ is differentiable and $g'(x)=-\frac{1}{x^2}\int_0^x f(t)dt+\frac{1}{x}f(x)$ and this is continuous by Fundamental theorem of Calculus. Thus what I have to show is $g'(0)$ exists and $\displaystyle\lim_{x\to 0}g'(x)=g'(0)$ . Expectation Before calculating $g'(0)$ , I expect $g'(0)$ is $\frac{f'(0)}{2}$ because if I assume $g'$ is continuous at $0$ , i.e., $\displaystyle\lim_{x\to 0}g'(x)=g'(0)$ , I have \begin{align}
\lim_{x\to 0}g'(x)
&=\lim_{x\to 0}\left[-\frac{1}{x^2}\int_0^x f(t)dt+\frac{1}{x}f(x)\right]\\
&\underset{\mathrm{l'Hôpital}}=\dfrac{f'(0)}{2},
\end{align} and thus $g'(0)=\dfrac{f'(0)}{2}$ . Anyway, I calculate $g'(0)$ by definition. I have to evaluate $$\lim_{h\to 0}\frac{g(h)-g(0)}{h}.$$ For $h>0$ , I have $$\dfrac{g(h)-g(0)}{h}=\dfrac{\tfrac{1}{h}\int_0^h [f(t)-f(0)] dt}{h}$$ From the mean value theorem for integral, there is $c_h\in(0,h)$ s.t. $$\frac{1}{h}\int_0^h[f(t)-f(0)]dt=f(c_h)-f(0)$$ Thus $$\dfrac{g(h)-g(0)}{h}=\dfrac{f(c_h)-f(0)}{h}=\dfrac{c_h}{h}\dfrac{f(c_h)-f(0)}{c_h}$$ I have $\displaystyle\lim_{h\to 0^+}\dfrac{f(c_h)-f(0)}{c_h}=f'(0)$ , but what is $\displaystyle\lim_{h\to 0^+}\dfrac{c_h}{h}$ ? By the expectation above, $\displaystyle\lim_{h\to 0^+}\dfrac{c_h}{h}$ seems to be $\frac{1}{2}$ , but I don't know how I should conclude that. Perhaps my solution doesn't work. Thanks for the help.","['calculus', 'solution-verification', 'derivatives', 'real-analysis']"
4666629,"Since $C\ne(C∪(A∪B)),$ how does $C−R = (C∪(A∪B))−R?$","This question is from a course, Introduction to Probability, Statistics and Random Processes : Let $A$ , $B$ , and $C$ be three events in the sample space $S$ .
Suppose we know that $A∪B∪C=S$ , $P(A)=\frac{1}{2}$ , $P(B)=\frac{2}{3}$ , $P(A∪B)=\frac{5}{6}.$ Find $P(C−(A∪B))$ . The given solution: We can write $C−(A∪B) = (C∪(A∪B))−(A∪B)$ Which then becomes "" $S−(A∪B)$ "" and finally "" $(A∪B)^\text{c}$ "". How does $C$ become ( $C∪(A∪B))$ ?","['elementary-set-theory', 'discrete-mathematics', 'probability']"
4666657,Average Duration Normal Distribution,"The average duration of Alzheimer’s disease is $8$ years and the standard deviation is $4$ years. For a clinical study $30$ patients, who have been determined to be at the very
beginning stage of the disease, are randomly selected.
(i) What is the probability that the average duration of the disease among the sampled
patients will be less than $7$ years? I believe if I let $Y$ denote the time it is normally distributed with parameters $8$ and $4^2$ . I worked out $P(Y<7) = 0.4013$ . However, I need to work out the probability of the average duration being less than $7$ years. How would I do this ? I know there are $30$ patients in total so the total sum must add to a number less than $210$ years.","['statistics', 'normal-distribution', 'probability']"
4666671,Identifying an action on an arbitrary set with an action on corresponding factor group,"My textbook (Szekeres's A Course in Mathematical Physics ) asks me the following set of questions (NB I am self-studying): If $H$ is any subgroup of a group $G$ , define the action of $G$ on $G/H$ by $\psi: G \to$ Sym( $G/H$ ) with $\psi(g)(g'H) = gg'H$ . a) Show that this is always a transitive action of $G$ on $G/H$ b) Let $G$ have a transitive left action on a set $X$ , and set $H = G_x$ to be the isotropy (stabilizer) group of any $x \in X$ . Show that the map $i: G/H \to X$ defined by $i(gH) = gx$ is well-defined and bijective.
[I note here that the transitive action alluded to is tacit in the notation $gx$ . Call it $\varphi$ as required.] c) Show that the left action of $G$ on $X$ can be identified with the action of $G$ on $G/H$ defined in (a). Now I've done (a) and (b), but am a bit confused by (c). I assume it's alluding to there being some commutative diagram-type argument that I can make, saying something like ""for every transitive action $\varphi$ of $G$ on $X$ , we can think of it instead as $\varphi = i \circ \psi$ , but I'm not sure if I'm missing something big. I really can't quite see how to start since the ""identified with"" is so vague here.","['group-theory', 'group-actions']"
