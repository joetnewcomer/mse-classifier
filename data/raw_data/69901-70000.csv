question_id,title,body,tags
850502,How many matrices with integer eigenvalues are there?,"Let $m,n \in \mathbb N$. How many $m \times m$ matrices with integer entries from $-n$ to $n$ have the property that all eigenvalues (possibly multiple) are integers? The following table calculated with PARI shows the values for $m = 2$ and
$n = 1,\dots,20$: 1  55

2  317

3  963

4  2301

5  4315

6  7793

7  12047

8  18449

9  26527

10  37325

11  48683

12  66149

13  82547

14  104713

15  131247

16  162297

17  191599

18  233813

19  270939

20  324045 For $m = 3$, I only know the values for $n = 1,2$: 1 6417  
2 260353  
3 2570569 Brute force method is soon not feasible. A formula depending on $m$ and $n$ would be nice.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
850554,rank of quadrics,"Consider the quadric $xw-yz$ in $\mathbf{P}^3$ (all over $\mathbf{C}$), and the Klein quadric $x_0 x_5+x_1 x_4+x_2 x_3$ in $\mathbf{P}^5$. I want to determine the rank of these quadrics. For the first I see that this quadric can be written as $s^T A s$ with $s=(x,y,z,w)$ and $A$ having entries $a_{14}=-a_{23}=-a_{32}=a_{41}=1/2$ and all other zero. From this I can conclude, since $\mathrm{rank} \ A=4$, that the first quadric has rank $4$ and similarily that the second one has rank $6$. So is this correct, both the result and the approach? Talking from experience, what would you say is the shortest way to determine the rank of a quadric? (I should note that I am mostly interested in a computational, i.e. linear algebra approach.)","['quadratic-forms', 'linear-algebra', 'algebraic-geometry']"
850557,Cantor's diagonal argument meets logic,"(I'm not normally dealing much with set theory and logic, so excuse me if my choice of words below seems a bit off the traditional terminology.) Where does the following Cantor-inspired argument go wrong? Let $\Sigma = \{A_0,A_1,A_2,\ldots\}$ be the countable collection of statements with one free variable such that for all $n\ge 0$,
$$
\exists!x_n\in [0,1):A_n(x),
$$
where by $[0,1)$ I mean the interval . ($\Sigma$ is countable, right? Statements must consist of finitely many characters after all.)
Now write $x_n$ in decimals as
$$
x_n =\sum_{k=1}^\infty x_n^{(k)}10^{-k}
$$
where $x_n^{(k)}\in\{0,1,\ldots,9\}$ for all $k\ge 1$ (we make sure never to write e.g. $0.1$ as $0.099999\ldots$). Now put
$$
y^{(k)}=\begin{cases}
1 & \text{if $x_k^{(k)}\neq 1$}\\
2 & \text{else}.
\end{cases}
$$
Unsurprisingly, we put
$$
y:=\sum_{k=1}^\infty y^{(k)}10^{-k}.
$$
Now $y\neq x_n$ for all $n$, just like in Cantor's classical argument. But the above definition of $y\in [0,1)$ should (I think---but this could be where I'm wrong) be possible to formalise in first-order logic. In other words, there exists a statement $A$ such that $y$ is the one and only number in $[0,1)$ such that $A(y)$ is true. Thus $A\in\Sigma$, which is obviously a contradiction.",['elementary-set-theory']
850558,Properties of Markov chains,"We covered Markov chains in class and after going through the details, I still have a few questions. (I encourage you to give short answers to the question, as this may become very cumbersome otherwise) 1.) There are many theorems like: If $(X_n)$ is an irreducible( with a state space $S$), positive recurrent markov chain, then...(for example there is a unique stationary distribution $\pi>0$). Now, assuming that $(X_n)$ is not irreducible, but contains a positive recurrent class $A \subset S$. Does this mean that we can say that there is a unique stationary distribution $\pi|_A>0$ and $\pi=0$ elsewhere? So my question is: If we want to talk about one positive recurrent class instead of one irreducible chain, does this method of transferring the results ( by restricting the result to the subset $A$) always work? When I thought about it, I considered this to be a trivial conclusion, but most books don't even talk about this simple generalisation, so I wanted to ask this here. 2.) When you want to identify communication classes of a Markov chain on a finite set you can do this by drawing a picture and look whether there is a pathway between two states. But is there also a way of calculating the classes directly from the transition matrix?
If my guess in question (1) holds, then there should be a one-one correspondence between eigenvectors $\pi^T = \pi^T P $ with $\pi_A >0$ (and $\pi_{A^C}=0$) and positive recurrent classes $A$. In that case the only question would be how to find the remaining classes. But probably, the remaining transient states are just the states that always have a zero component in every stationary distirbution we get from the eigenvalue problem $P^T \pi = \pi$. 3.) The same as question 2) for the period. Is there a faster method of calculating the period of a state than calculating the powers of the transition matrix? Or at least a fast way to determine whether a state is periodic or aperiodic? 4.) I am struggeling with closed states. My intuition tells me that:
Every finite closed class is recurrent and every recurrent class is closed. So for finite chains the terms closed and (positive) recurrent should be $1:1$. Is this true? 
It obviously diverges over infinite sets as the randomn walk in $\mathbb{Z}^3$ is transient, but the whole state space is closed. 5.) In case that we regard the continuous case of a homogenous markov chain and $P(X_{n+1} \in A |X_n =x) = \int_A q(x,y) dy$. How do we get $P(X_{n+k} \in A |X_n = x)
$? 6.) For a transient states $i,j$ in one class we have: $\sum_{n = 0}^{\infty} p^{(n)}_{ij} < \infty$. For a (positive) recurrent states $i,j$ in one class we have: $\sum_{n = 0}^{\infty} p^{(n)}_{ij} = \infty$. This is in my opinion a consequence of the fact that transience/recurrence is a class property, but I just wanted to be sure about this. Unfortunately, this questions seems to be very unpopular. Any suggestion how I could improve my question is highly appreciated.","['probability-theory', 'stochastic-processes', 'markov-chains', 'probability']"
850568,What is $\operatorname{Pic}(\mathbb{P}^n_{\mathbb{Z}})$?,"I would like to know the Picard group of the projective spaces over the integers $\mathbb{Z}$. I know that the projective space over a field $k$ has $\operatorname{Pic}(\mathbb{P}^n_{\mathbb{k}}) \cong \mathbb{Z}$, but what in the case of the integers (or even arbitrary rings)? Are there any results?",['algebraic-geometry']
850572,Finding the best possible $\delta$ for a continuous function.,"I am trying to understand the following problem... I understand half of it, but I get confused with something. First of all, I was wondering if there is a relation between $\delta$ and $\epsilon$ other than the one stated in the question (I think it was $\delta < 1/\epsilon$ or something like that, but I can't find it in my notes). Thanks a lot in advance! Question: Consider $f:\mathbb{R}\to\mathbb{R}$ given by $f(x)=x^2$. We know that it is continuous at $a$ for all $a\in\mathbb{R}$. So, for every $a$, for every $\epsilon>0$ there is a $\delta>0$ such that $|x-a|<\delta$ implies $|f(x)-f(a)|<\epsilon$. For $a>1$ and $\epsilon=1$ find the best possible $\delta$. Is this best possible $\delta$ independent of $a>1$. Solution: Our $\delta$ will be the best one such that $x\in(a-\delta,a+\delta)$ implies that $x^2\in(a^2-1,a^2+1)$, that is, $x\in((a^2-1)^{1/2},(a^2+1)^{1/2})$. Clearly very small $\delta$'s are fine, until either $a-\delta$ hits $(a^2-1)^{1/2}$ or $a+\delta$ hits $(a^2+1)^{1/2}$. So we have to work out $a-(a^2-1)^{1/2}$ and $(a^2+1)^{1/2}-a$ and our best $\delta$ will be whichever is the smaller of these two numbers. So I understand everything up to here, but I get lost now. How do you get these 2 values? The first is $\frac{1}{a+(a^2-1)^{1/2}}$ and the second is $\frac{1}{(a^2+1)^{1/2}+a}$, of which $\frac{1}{(a^2+1)^{1/2}+a}$ is the smaller. And I also don't see how one works and the other won't work :/ So any $\delta\le\frac{1}{(a^2+1)^{1/2}+a}$ will work, and any $\delta\ge\frac{1}{(a^2+1)^{1/2}+a}$ won't work. Thus we clearly cannot find a $\delta$ which works uniformly for all $a>1$. The best $\delta$ goes to $0$ when $a$ gets larger and larger.","['epsilon-delta', 'functions']"
850580,Proving result on measure's atoms,"I have been told that a measure's atoms are at most a countable set. This has not been proved to me, and my book leaves it as an exercise. The only possible way I can think of is to try by contradiction, so suppose there is a measure whose atoms are more than countably many. So what? How do I go on? How do I prove this?",['measure-theory']
850583,Herstein Question: $G^{i}$ normal in $G$?,"I just wanted to ask a quick question. I'm going over the second edition of I.N. Herstein's topics in algebra and one of his exercises asks the reader to prove that each $G^{i} $ is a normal subgroup of G where $G^{i}$ is the $i^{th}$ commutator group. Now I think I was able to prove $G^{i}$ is normal in $G^{i+1}$, where $G^{i}$ is the commutator group of $G^{i+1}$: Let $g \in G^{i+1}, h \in G^{i}$. Every $h \in G^{i+1}$ since $h=g_{1}^{-1}g_{2}^{-1}g_{1}g_{2} \in G^{i+1}$ since $G^{i+1}$ is a group. So $(gh_{1}g^{-1}h_{1}^{-1} \in G^{i}) \Rightarrow (gh_{1}g^{-1}h_{1}^{-1}=h_{2}) \Rightarrow (gh_{1}=h_{2}h_{1}g) \Rightarrow (gh_{1}=h_{3}g)$ So $G^{i}$ is normal in $G^{i+1}$. I'm seeing how this relates to solvable groups through composition series. However it doesn't seem necessary to have every $G^{i}$ normal to the whole group G. We only really need $G^{i}$ normal in $G^{i+1}$. Is it absolutely necessary I prove the additional property of its normality in G? I suppose for the sake of curiosity I might come back to prove the proposition but, for now, can I move on without being delayed by it? I'd like to get to the more exciting stuff. Also is my proof correct? Thanks in advance!!","['proof-verification', 'group-theory', 'abstract-algebra']"
850610,A question on the unit tangent bundle of the sphere and $SO(3)$,"Let the unit tangent bundle be defined as follows: $$T^1S^2=\{(p,v)\in \mathbb R^3 \times \mathbb R^3 | |p|=|v|=1 \text{ and } p \bot v \}$$ Let $SO(3)$ be the group of rotations of $\mathbb R^3$. Apparently, $SO(3)$ is in bijection with $T^1S^2$. My question is: If $N$ is a point on $S^2$, say the north pole, does the
  rotation in $SO(3)$ moving $N$ to $p$ along $v$ correspond to $(p,v)$
  in $T^1 S^2$? Put the other way around: Does the matrix $(p,v, p \times v)$ corresponding to $(p,v)$ represent
  the rotation around the axis $p$? And if so, is the angle somehow
  represented by $v$? Later added The reason why I think there should be geometric meaning to this bijection or at least some insight to be gained is that finding the bijection was an exercise in a book I am reading. If there was no insight to be gained the exercise would be more or less purely computational and not very insightful.","['lie-groups', 'differential-geometry']"
850622,How do I simplify $\log (1/\sqrt{1000})$?,"How do I simplify $\log \left(\displaystyle\frac{1}{\sqrt{1000}}\right)$? What I have done so far: 1) Used the difference property of logarithms 
$$\log \left(\displaystyle\frac{1}{\sqrt{1000}}\right) = \log(1) - \log(\sqrt{1000}) $$ 2) Used the exponent rule for logarithm $$\log (1) - \frac{1}{2}\log (1000) $$ I'm stuck at this point. Can someone explain why and what I must do to solve this equation?","['logarithms', 'algebra-precalculus']"
850641,Rearrange columns and rows of a matrix such that it can be split in half,"I am not a mathematician, so please excuse me if this question turns out to be trivial. I need this at work, but I could not figure out how to solve this efficiently, though it looks like it might be a very common, and perhaps simple (however I have a bad feeling about this) problem. Anyway, given is a matrix of boolean values, i.e. zeros and ones. Allowed operations are: Rows can be swapped Columns can be swapped The problem: using only these two operations, is it possible to convert the input matrix to a kind of a block diagonal matrix, except that the blocks should not necessarily be square matrices, but of restricted size, i.e. the number of rows (or columns) in each block should be equal to a given number. For example: Let the input matrix be: a b c d e f
--+------------
w | 1 0 0 0 1 0
x | 0 0 0 1 0 1
y | 1 0 1 0 0 0
z | 0 1 0 1 0 0 And the blocks should have the size (2,3), then one possible solution would be: a e c d b f
--+------------
w | 1 1 0 0 0 0
y | 1 0 1 0 0 0
x | 0 0 0 1 0 1
z | 0 0 0 1 1 0 Here, the rows x and y and the columns b and e have been swapped. The problem can also be relaxed such that only the number of columns (or rows, but not both) of the sub-matrices is fixed. I need this in order to fit a rather big but sparse matrix on a sheet of paper of limited width, and the idea was to rearrange it like described above and split it in half. I know that it is not always possible, but in cases where it is, I would proceed by displaying the result matrix like this: a e c
--+------
w | 1 1 0
y | 1 0 1

    d b f
--+------
x | 1 0 1
z | 1 1 0 So my question is: what is this problem called generally? Is there an alternative formulation? And, most importantly, is there an (efficient) algorithm to solve it (I mean, there must be one, right)? Is there a text or something on the web where I can look it up? Thank you very much in advance. UPDATE I did some progress on this and - if I didn't make a mistake - it turns out that it reduces to PARTITION. Basically, it is relatively easy to group columns in equivalence classes by checking whether there are any rows which have 1s in both of the columns. I came up with this algorithm: pick some cell with a 1 strike through it vertically and horizontally for each cell containing a 1 which gets struck through, perform the steps 2-3. remove the sub-matrix containing struck out rows and columns and repeat steps 1-4 till there are no 1s left That way we get the set of smaller sub-matrices which should then be combined to form two matrices of equal width (the goal was to split the input matrix in half). Actually, in case where there are all-0-columns, these can be used to pad other matrices to a required size, but this just complicates things. Also, in the second phase (combining the sub-matrices), only the width of the sub-matrices is relevant, therefore we can consider a multi-set of positive integers representing the width of each sub-matrix. Requiring that this set is to be partitioned into two sub-sets which sum up to equal size, well, that's exactly PARTITION (I knew there was something bad about this problem ಠ_ಠ). By the way, splitting the matrix into more than two chunks of more or less equal size reduces to BIN PACKING.","['matrices', 'combinatorics']"
850645,How to explain this quirk of the chain rule?,"Assume I have a function $f = f(y, \phi(y,x))$ and I want to calculate $\frac{\partial f}{\partial y}$, I use the chain rule to get \begin{equation}
\frac{\partial f}{\partial y} = \frac{\partial f}{\partial y} + \frac{\partial f}{\partial \phi}\frac{\partial \phi}{\partial y} 
\end{equation} but obviously the $\frac{\partial f}{\partial y}$ represent different things on each side of equality. How do I explain this? I'm guessing it is a notational issue. Edit: Just to give some context why this troubles me. Here $x_i$ refers to the ith component of the vector $\mathbf{x}$ in euclidean space. In an acoustic textbook the Lighthill stress tensor $T_{ij}$ is involved in the following identity: \begin{equation}
\frac{\partial}{\partial x_i} \frac{T_{ij}(\mathbf{y},t-|\mathbf{x}-\mathbf{y}|/c)}{|\mathbf{x}-\mathbf{y}|} = \frac{\frac{\partial T_{ij}}{\partial y_i}}{|\mathbf{x}-\mathbf{y}|} - \frac{\partial}{\partial y_i} \frac{T_{ij}(\mathbf{y},t-|\mathbf{x}-\mathbf{y}|/c)}{|\mathbf{x}-\mathbf{y}|}  \end{equation} This can only be resolved if the numerator in the term $\frac{\frac{\partial T_{ij}}{\partial y_i}}{|\mathbf{x}-\mathbf{y}|}$ is given a different interpretation...Just try showing this: Let $t-|\mathbf{x}-\mathbf{y}|/c = \phi(t,\mathbf{x}, \mathbf{y})$ \begin{array}{lcl} 
\frac{\partial}{\partial x_i} \frac{T_{ij}(\mathbf{y},\phi)}{|\mathbf{x}-\mathbf{y}|} & = & \frac{1}{|\mathbf{x}-\mathbf{y}|} \frac{\partial}{\partial x_i}T_{ij}(\mathbf{y},\phi) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}  \\  
& = &  \frac{1}{|\mathbf{x}-\mathbf{y}|} (\frac{\partial T_{ij}}{\partial \phi}\frac{\partial \phi}{\partial x_i}) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}\\ 
& = &  -\frac{1}{|\mathbf{x}-\mathbf{y}|} (\frac{\partial T_{ij}}{\partial \phi}\frac{\partial \phi}{\partial y_i}) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|} 
\end{array} \begin{array}{lcl} 
\frac{\partial}{\partial y_i} \frac{T_{ij}(\mathbf{y},\phi)}{|\mathbf{x}-\mathbf{y}|} & = & \frac{1}{|\mathbf{x}-\mathbf{y}|} \frac{\partial}{\partial y_i}T_{ij}(\mathbf{y},\phi) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial y_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}  \\  
& = &  \frac{1}{|\mathbf{x}-\mathbf{y}|} ( \frac{\partial}{\partial y_i}T_{ij} +\frac{\partial T_{ij}}{\partial \phi}\frac{\partial \phi}{\partial y_i}) - T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|} 
\end{array} Adding up the last line from each expression gives the result.","['notation', 'multivariable-calculus']"
850658,Definition of Trace of Linear Operator,The trace of a linear operator $f$ can be defined as the trace of the matrix $A$ representing $f$ with respect to some basis $B$.  However the trace does not depend on the basis chosen.  This suggests to me that there is some definition of the trace of $f$ independent of matrices (and thus coordinate-independent).  Any suggestions as to how I could define $\mathrm {tr}(f) $ without defining it as $\mathrm {tr}([f]_B)$?,['linear-algebra']
850690,Prove that the a modified Cantor Set is not Jordan-Measurable,"Let $C_0 = [0,1]$ and if $C_n$ is given as a disjoint union of intervals, construct $C_{n+1}$  by removing from each interval $I$ an open interval of length $(n+2)^{-2}|I|$ in the middle of each interval, and then define $C$ as the intersection of all those intervals. I want to show that this set is not Jordan-measurable. Note that contrary to the ""standard"" Cantor set, where we remove $1/3 \cdot |I|$ in the middle of each interval, here we remove $(n+2)^{-2} \cdot |I|$ in each stage. Using some Theory about Lebesgue-measurable sets, it is easy to show this, because counting what we remove we have
$$
 1 - \sum_{n=0}^{\infty} \frac{1}{(n+2)^2} = 1 - \left( \sum_{n=1}^{\infty} \frac{1}{n^2} - 1 - 1/2 \right) = \frac{5}{2} - \frac{\pi^2}{6} > 0
$$
and so it has positive Lebesgue-meassure. Also if it is Jordan-measurable, then its Jordan-measure equals its Lebesgue-measure, and so it would have positive Jordan-measure. But because it equals its boundary, its boundary has positive Jordan-measure, which implies it is not Jordan-measurable, and by this contradiction it cannot be Jordan-measurable. But I want to proof this fact without using the Lebesgue-measure , so do you know a proof? Some facts about Jordan-measurable (J-measurable for short) sets I know: a set is J-measurable iff its inner and outer J-measure coincide (definition) a set is $M$ J-measureable iff for each $\varepsilon > 0$ there exists sets $S,T$ which could be written as a union of a finite number of intervals such that
$$
 S \subseteq M \subseteq T, \quad |T| - |S| < \varepsilon
$$ a set $M$ is J-measureable iff $|\partial M| = 0$. and also a fact about approximations if we successively partition $\mathbb R$ in intervals of length $1/2^k, k = 1,2,3,\ldots$. But I do not see a way to use this facts in any useful way here.","['general-topology', 'real-analysis', 'analysis']"
850699,I need help on the process of solving this derivative.,"How do I go about solving this derivative. $$f(x)=\ln\left(\frac{7x}{x+4}\right)$$
I go from this to $$1. \quad f(x)=\ln(7)+\ln(x)-\ln(x+4)$$ and then $$2. \quad f'(x)=\frac{1}{x}-\frac{1}{x+4}$$ then $$3. \quad f'(x)=\frac{x+4-x}{x(x+4)}$$ and end up with $$4. \quad f'(x)=\frac{4}{x(x+4)}$$ which is the correct answer. This procedure is what we did in class, but I do not get how I went from step 2 to 3 which is what has me confused. A step by step explanation would be greatly appreciated. Thanks!","['natural-numbers', 'logarithms', 'calculus', 'derivatives']"
850723,Rules of inference: The Rules of Disjunctive Syllogism and Double Negation,"I have a question about the use of Double Negation in relation to this problem I found in my textbook examples. Problem: $\;¬(r \land t) \lor u$ $\;r \land t$ Therefore, $u$. In my textbook it says it used the double negation and then followed by the Rule of Disjunctive Syllogism. I understand that we have to turn premises 1 and 2 into premises that can be used with disjunctive syllogism, but I don't know the STEPS taken using double negation to get it into the form to be used with disjunctive syllogism. Help is much needed! I really can't figure out how double negation was employed here.","['logic', 'propositional-calculus', 'discrete-mathematics']"
850724,Binomial Congruence,How can we show that $\dbinom{pm}{pn}\equiv\dbinom{m}{n}\pmod {p^3}$ where $m$ and $n$ are nonnegative integers and $p$ is a prime such that $p \geq 5$ ? I can do it for $\mod p^2$ but I am stuck here.,"['binomial-coefficients', 'number-theory', 'combinatorics']"
850732,Uniform continuity and translation invariance,"Consider the function $a(s)=\dfrac{1}{1+s^2}$ and the space  $X=\{f:\mathbb{R}\to \mathbb{R}$ such that $t\mapsto a(t)f(t)$ is bounded uniformly continuous$ \}$. I want to show that $X$ is translation invariant, i.e. if $f\in X$ then $f_t\in X$ for all $t\in \mathbb{R}$, where $f_t$ is the $t$-translation of $f$ defined by $f_t(s)=f(t+s)$. I showed that if the function $s\mapsto a(s)f(s)$ is bounded then the function $s\mapsto a(s)f_t(s)$ is also bounded. In fact
$$\sup_{s\in \mathbb{R}}\left\lvert a(s)f_t(s)\right\rvert=\sup_{s\in \mathbb{R}}\left\lvert a(s)f(t+s)\right\rvert=\sup_{s\in \mathbb{R}}\left\lvert a(s-t)f(s)\right\rvert\leq M_t\sup_{s\in \mathbb{R}} \left\lvert a(s)f(s)\right\rvert,$$
because we know that there's a constant $M_t$  such that for all $s\in \mathbb{R} $ 
$$a(s-t)=\dfrac{1}{1+(s-t)^2}\leq M_t \dfrac{1}{1+s^2}=M_t a(s).$$ Now I just need to prove that if the function $s\mapsto a(s)f(s)$ is uniformly continuous,  then the function $s\mapsto a(s)f_t(s)$ is also uniformly continuous.","['uniform-continuity', 'real-analysis', 'analysis']"
850735,How to make an order isomorphism,"Two linear orders $A$ and $B$ have starting points $a_0$ and $b_0$, and have cofinalities $\omega_1$. Let $(a_\alpha )_{\alpha<\omega_1}$ and $(b_\alpha )_{\alpha<\omega_1}$ be cofinal sequences.  Suppose we also know that for every $\alpha<\omega_1$ there is an order isomorphism $[a_\alpha ,a_{\alpha+1}]\simeq [b_\alpha ,b_{\alpha+1}]$ that maps $a_\alpha$ to $b_\alpha$ and $a_{\alpha+1}$ to $b_{\alpha+1}$. Can we conclude that $A$ is isomorphic to $B$? How would you write this down? Thank you for any help. Please let me know if there is anything I can clarify.","['general-topology', 'set-theory', 'order-theory']"
850812,Is a finite group always a element-wise product of Sylow subgroups?,"Let $G $ be a finite group, and let $ p_1, \ldots, p_n $ be the distinct primes dividing $|G|$. For each $i $, let $ P_i $ be a Sylow $ p_i $-subgroup of $ G $. I seem to recall a theorem saying $ G=P_1\cdots P_n $. Is this true?  It's immediate via a simple counting argument when $ n=2$, but I haven't been able to prove/disprove the general case.","['sylow-theory', 'finite-groups', 'group-theory']"
850833,Prove $\frac{\pi}{4} = \sum_{n = 0}^{\infty}\frac{(-1)^{n}}{2n + 1}$ using Dominated or Monotone Convergence,Is there a way to prove that $$\frac{\pi}{4} = \sum_{n = 0}^{\infty}\frac{(-1)^{n}}{2n + 1}$$ via the Dominated or Monotone Convergence Theorem?,"['sequences-and-series', 'real-analysis', 'analysis']"
850835,Positive Operators: Definition?,"Definitions Given an operator algebra $\mathcal{A}\subseteq\mathcal{B}(\mathcal{H})$ with $1\in\mathcal{A}$ Consider selfadjoint operators $A=A^*\in\mathcal{A}$. Define positive elements by:
$$A\geq0:\iff\sigma(A)\geq0$$
and positive operators by:
$$A\geq0:\iff\mathcal{W}(A)\geq0$$ Problem Do the numerical range and spectrum coincide:
  $$A=A^*:\quad\langle\sigma(A)\rangle=\overline{\mathcal{W}(A)}$$ Attempt For bounded operators one has at least:
$$\|A\|<\infty:\quad\sigma(A)\subseteq\overline{\mathcal{W}(A)}$$
So any positive operator is a positive element; but what about the converse?","['banach-algebras', 'operator-theory', 'operator-algebras', 'hilbert-spaces', 'functional-analysis']"
850847,The set of $x$ where a sequence convergences in terms of set operations,"I'm befuddled by this. Suppose $f:\mathbb{R}\to\mathbb{R}$, $f_n:\mathbb{R}\to\mathbb{R}$, $n=1,2,\dots$, and consider the set $$\bigcap_{k\geq 1}\bigcup_{p\geq 1}\bigcap_{m\geq p}\{x\in\mathbb{R} \ | \ |f(x)-f_m(x)| <1/k\}.$$ Is this the set of $x$ where $f_n(x)$ converges to $f(x)$, or the set of $x$ where it converges uniformly ? What would need to be modified to change from one type of convergence to the other? This comes from an effort to understand the last step in the proof of Egorov's theorem. Thank you.","['measure-theory', 'calculus', 'elementary-set-theory']"
850868,I am having problems figuring out how to derive this.,"I have the function $$\tag{1} f(x)=\ln\sqrt{8+\cos^2x}$$ So we derive it as follows: $$\tag{2} f(x)=\ln(8+\cos^2x)^\frac{1}{2}$$ $$\tag{3} f(x)=\frac{1}{2}\ln(8+\cos^2x)$$ $$\tag{4} f'(x)=\frac{1}{2}\left[\frac{-2 \cos x{\sin x}}{8+\cos^2x}\right]$$ $$\tag{5} f'(x)=\left[\frac{-\cos x{\sin x}}{8+\cos ^2x}\right]$$ I'm having problems with everything after step 2, a step by step explanation would be greatly appreciated. Thanks. For example where does the sin come from and why is there a cos in the numerator now. A detailed process  of beginning to end of solving this (what laws are used and why etc..) would be greatly appreciated. Here is the image of the website and how it goes about doing the steps, I thought it's be better for me to try and type it out unless anybody is curious. !","['trigonometry', 'calculus', 'derivatives']"
850869,Linear Algebra without Matrices,"How far could one get in linear algebra without matrices?  It seems like the more I learn, the less I actually use them, but most of the basic theorems and invariants that learned first -- and still use -- were defined via matrices.  Could linear algebra be done without matrices at all?  Is there a book that takes this approach?  I'm just curious how one would go about it.","['linear-algebra', 'soft-question']"
850879,Evaluating differential forms.,"Can someone please check my work? It's an exercise from Barret O'Neill's Elementary Differential Geometry. I want to be really sure that my understanding of this is right. I see that the forms $\mathrm{dx, dy, dz}$ work as projections of the vector part, and the functions multiplying these forms are evaluated in the point in question.
I'm given the tangent vector $v_p = (1,2,-3)_p$, where $p = (0,-2,1)$ 
I must evaluate some forms on $v_p$. 
My work: a) $y^2~ \mathrm{dx}$. $$\begin{align} (y^2 ~\mathrm{dx})(v_p) &= (-2)^2 \cdot \mathrm{dx}(1,2,-3) \\ &= 4 \cdot 1 \\ &= 4 \end{align}$$ b) $z~ \mathrm{dy} - y~ \mathrm{dz}$. $$\begin{align} (z~ \mathrm{dy} - y~ \mathrm{dz})(v_p) &= 1 \cdot \mathrm{dy}(1,2,-3) - (-2) \cdot \mathrm{dz}(1,2,-3) \\ &= 2 + 2 \cdot (-3) \\ &= -4 \end{align}$$ c) $ (z^2 - 1) ~\mathrm{dx} - \mathrm{dy} + x^2~ \mathrm{dz}$. $$\begin{align} \left((z^2 - 1)~ \mathrm{dx} - \mathrm{dy} + x^2 ~ \mathrm{dz} \right)(v_p) &= (1^2 - 1) \cdot \mathrm{dx}(1, 2,-3) - \mathrm{dy}(1,2,-3) + 0^2 \cdot \mathrm{dz}(1,2,-3) \\ &= - 2 \end{align}$$ Thank you.","['multivariable-calculus', 'differential-forms']"
850903,Finding the smallest $x$ such that $ax\equiv b\mod m$,"I'm looking to solve for $x$ in the equation $ax\equiv b\mod m$ and I wish to find the smallest $x$ which satisfies this. How would I go about doing this, in the general case? (This is for a programming problem). What I've done so far is try to compute the multiplicative inverse of $a$ in the ring $\mathbb{Z}_m$, then multiply this inverse with $b$ which effectively does modular division $b/a$. But the problem is that sometimes $a$ doesn't have a multiplicative inverse in the ring, while the equation can still be solved. For instance, $2x\equiv 2\mod 6$ trivially has $x=1$ as the solution. But 2 has no multiplicative inverse in $\mathbb{Z}_6$. So I'm a bit stuck. Does anyone have any suggestions?",['number-theory']
850906,$V = \operatorname{Im} T + \ker T $ then $ \operatorname{Im} T \cap \ker T = \{0\}$,"Let $F$ be a field, let $V$ be a vector space with finite dimension over $F$ and let $T$ be a linear operator on $V$. Prove that: a) If $V = \operatorname{Im} T + \ker T $ then $\operatorname{Im} T \cap \ker T = \{0\}$; b) If $\operatorname{Im} T \cap \ker T = \{0\}$ then $V = \operatorname{Im} T \bigoplus \ker T $ I'm really stuck with this problem, some help to solve this please.","['linear-algebra', 'transformation']"
850924,Characterization of differentiable functions from $\mathbb{R}^m$ to $\mathbb{R}^n$.,"Let $U\subset\mathbb{R}^m$ be an open set. Consider a function $f:U\to\mathbb{R}^n$ and a point $a\in U$. I need help to prove that the following sentences are equivalents. (a) There exists a linear map $f'(a):U\to\mathbb{R}^n$ such that
  $$f(a+v)-f(a)=f'(a)\cdot v+r(v),$$
  where the ""remainder"" $r(v)$ satisfies $$\lim_{v\to 0}\frac{\|r(v)\|}{\|v\|}=0.$$ (b) For each $h\in\mathbb{R}^m$ such that $a+h\in U$, there exists a linear map $A(h):\mathbb{R}^m\to\mathbb{R}^n$ such that $A(h)\cdot h=f(a+h)-f(a)$ and $h\mapsto A(h)$ is continuous at $h=0$. The first sentence is the common definition of differentiability at a point. Could someone give me some ideias to solve it? Thanks.","['multivariable-calculus', 'derivatives', 'real-analysis', 'analysis']"
850949,Which is the correct way to calculate the expected value of a shared lottery jackpot?,"I want to calculate the expected value of a ticket in a lottery game which gives players a probability $p$ of winning a jackpot prize of $j$ dollars. The total number of tickets in play is $t$. If every winning ticket gets the full prize amount, the expected value for a ticket is given by $jp$. However, if winners must evenly split the prize in case of multiple winners, then the expected value depends on the number of winners $W$. The expected number of winners is $tp$. The probability that the number of winners $W$ is $w = 0, 1, 2, \dotsc$, follows a Poisson distribution with the expected number of winners as its parameter: $$P(W=w) \sim Pois(tp) = \frac{tp^we^{-tp}}{w!}$$ I don't know how to get from there to calculating an accurate expected value for the ticket as a function of the number of tickets in play. In reading online, I've found two different methods each used by several sources. If I'm following them correctly, then they give different results. My question is 1) which one is correct? 2) what is the error in reasoning (or in my understanding/implementation) in the incorrect method? Method 1: Number of Winners The first method calculates the probability that the number of winners $W$ will be $w = 0, 1, \dotsc, t$, given that there is at least one winner: $$P(W=w | W>0) = \frac{P(W>0|W=w)P(W=w)}{P(W>0)}$$ Where, $P(W>0|W=w)$ is $\left\{
     \begin{array}{lr}
       0 & : w = 0\\
       1 & : w > 0
     \end{array}
   \right.$ $P(W=w)$ is the probability of $w$ winners: $\frac{tp^we^{-tp}}{w!}$ $P(W>0)$ is the probability of more than one winner: $1 - P(W=0)$ So the expected value of the ticket is given by: $$p\sum_{w=1}^{t} \frac{j}{w}\frac{P(W=w)}{1-P(W=0)}$$ For a numerical example, we'll tabulate the first few values of $P(W=w)$ for a lottery with a 1/34,220 chance of winning \$100,000 jackpot, with 6,000 tickets in play, so $p = 1/34,220; j = 100,000; \text{and } t = 6,000$ $$\begin{array}{c|c|c|c|c|} 
\text{Winners} & \text{Probability} & \text{Conditional Probability} & \text{Share} & \text{Contribution } \\
w & P(W=w) & P(W=w|W>0) & j/w & (j/w)P(W=w|W>0) \\ \hline
0 & 0.839 & 0 & \text{\$0} & \text{\$0} \\ \hline
1 & 0.147 & 0.913 & \text{\$100,000} & \text{\$91,300} \\ \hline
2 & 0.013 & 0.081 & \text{\$50,000} & \text{\$4,050} \\ \hline
\end{array}$$ Summing the contribution column and multiplying by $p$ gives an expected value of $2.79 . Online resources which use Method 1 ""Powerball Odds"" by Durango Bill - see the section titled ""Sample Calculation to Find the Expected Shared Jackpot Amount When a Large Number of Tickets are in Play"" ""I Am A Statistician and I Buy Lottery Tickets"" by DC Woods. ""Is it Ever Worth it to play Mega Millions?"" by David Torbert Method 2: Number of Other Winners The second method calculates the probability that the number of total winners $W$ is $w = 0, 1, \dotsc, t$, given that our ticket is a winner: $$P(W=w|Winner) = \frac{P(Winner|W=w)P(W=w)}{P(Winner)}$$ Where, $P(Winner)$ is the probability that our ticket is a winner: $p$ $P(Winner|W=w)$ is the probability that our ticket is a winner given $w$ winning tickets: $w/t$ $P(W=w)$ is the probability of $w$ winners: $\frac{tp^we^{-tp}}{w!}$ Plugging those figures in shows that $P(W=w|Winner)$ reduces to $P(W=w-1)$: $$\frac{w}{t}\frac{P(W=w)}{p} = \frac{tp^{w-1}e^{-tp}}{(w-1)!} =  P(W=w-1)$$ So the expected value is given by: $$p\sum_{w=1}^{t}\frac{j}{w}\frac{tp^{w-1}e^{-tp}}{(w-1)!}$$ Using the same lottery numbers as above, the first few values of $w$ are given in the following table. $$\begin{array}{c|c|c|c|c|} 
\text{Winners} & \text{Probability} & \text{Conditional Probability} & \text{Share} & \text{Contribution } \\
w & P(W=w) & P(W=w|Winner) & j/w & (j/w)P(W=w|Winner) \\ \hline
0 & 0.839 & 0  & \text{n/a} & \text{\$0} \\ \hline
1 & 0.147 & 0.839  & \text{\$100,000} & \text{\$83,900} \\ \hline
2 & 0.013 & 0.147 & \text{\$50,000} & \text{\$7,350} \\ \hline
\end{array}$$ Summing the contribution column and multiplying by $p$ gives an expected value of $2.67 . Online Resources Which Use Method 2 ""Mega Millions and Powerball Odds: Can You Ever Expect a Ticket to be Profitable?"" by Jeremy Elson. See especially his ""Computing the Expected Jackpot: The Gory Details"" . The accepted answer for the math.stackoverflow question, ""What's the expected value of a lottery ticket?"" , gives a nice formula which is equivalent to Method 2: $\dfrac j t (1-(1-p)^t)$ Mark Adler's answer to the math.stackexchange question ""Is Mega Millions Positive Expected Value?"" Clearly the expected payout for the example lottery above cannot be both \$2.79 and \$2.67, but I'm having a difficult time reasoning my way to the correct method. Any hints will be appreciated!",['probability']
850951,feature selection for continuous variables,"I wonder how exactly ""feature selection"" should be performed in case of continuous feature values. When feature values are discrete it is very straitforward to apply feature selection, but what to do when you have term-document matrix with (tf*idf|tf|idf) as feature values for text classification task. I don't think that it's correct to take ~20% highest tf values, because it's biased towards features that appear in long documents. In short, what's simple way to evaluate feature selection for continuous feature values.","['statistics', 'artificial-intelligence', 'machine-learning']"
850977,Conditions for a quotient module to be Noetherian,"I'm solving this problem from ""Introduction to Commutative Algebra"" of Atiyah and Macdonald. Here is the problem: Let $M$ be an $A$-module and let $N_1, N_2$ be submodules of $M$. If $M/N_1, M/N_2$ are Noetherian, so is $M/(N_1 \cap N_2)$. I found a solution which states that we have the exact sequence
$$0 \rightarrow M/N_1 \rightarrow M/(N_1 \cap N_2) \rightarrow M/N_2 \rightarrow 0$$ so $M/(N_1 \cap N_2)$ is Noetherian if and only if $M/N_1$ and $M/N_2$ are Noetherian. I can't prove that this sequence is exact but can't find any counter-example for it. Can anyone help me? Thanks so much.","['modules', 'commutative-algebra', 'abstract-algebra', 'noetherian']"
850992,Show that $\sum\limits_n1/x_{n}^{2} = 1/10$ where $x_{n}$ is the $n^{\text{th}}$ positive root of $\tan x = x$ [duplicate],This question already has answers here : Sum of the squares of the reciprocals of the fixed points of the tangent function (6 answers) Closed 10 years ago . Recently I encountered this problem Show that $$\sum_{n = 1}^{\infty}\frac{1}{x_{n}^{2}} = \frac{1}{10}$$ where $x_{n}$ is the $n^{\text{th}}$ positive root of $\tan x = x$. I found many threads on MSE regarding solutions to $\tan x = x$ like this and this which suggest that the series $\sum 1/x_{n}^{2}$ is convergent if we compare it with $\sum 1/n^{2}$. But at the same time I have no idea how we can sum this up. Please help with suggestions or an answer. Update : I further tried to relate it with the sum $\sum 1/n^{2} = \pi^{2}/6$ and we can rewrite it as $\sum 1/(n^{2}\pi^{2}) = 1/6$ so that if $y_{n}$ is $n^{\text{th}}$ positive root of $\sin x = 0$ then $\sum 1/y_{n}^{2} = 1/6$. I believe this has got to do with the product $$\frac{\sin x}{x} = \prod_{n = 1}^{\infty}\left(1 - \frac{x^{2}}{n^{2}\pi^{2}}\right)$$ where we can see the factors corresponding to actual roots. We can now compare the coefficients of $x^{2}$ on both sides to get $\sum 1/y_{n}^{2} = 1/6$. I don't see any product related to equation $\tan x = x$.,['sequences-and-series']
851007,"Interesting dilemma, answer not matching with stewart, My work is Included","Question : Compute flux through the upper hemisphere of $x^2+y^2+z^2 = 1$ . Where $$\textbf{F} =  \left( z^2x\right)\textbf{ i }+\left[\dfrac{1}{3}y^3+ \tan z\right]\textbf{ j } + \left(x^2z+y^2 \right)\textbf{ k }$$ ANSWER GIVEN AT BACK OF STEWART : $\dfrac{13\pi}{20}$ MY WORK $\textbf{Divergence theorem }$ $$\textbf{Flux} = \int\int_S \textbf{F}\cdot d\textbf{S} = \int\int\int_E \text{div }\textbf{F} \hspace{2mm} dV$$ Where $S$ is a closed surface. And $E$ is the region inside that surface. In this problem, instead of computing the surface is not closed But we want to use divergence theorem, because divergence of the given vector field is cute. We will over come this problem by attaching a disk at the bottom of the hemisphere, we call this closed this closed surface $S_2$ and the disk as $S_1$ We can use divergence theorem for $S_2$ We will then find flux through $S_1$. Then the  flux through $S$ =  Flux through $S_2$  $-S_1$ $$\begin{align} & \text{div }\textbf{F} = \dfrac{\partial \left( z^2x\right)}{\partial x}+\dfrac{\partial }{\partial y}\left[\dfrac{1}{3}y^3+ \tan z\right] + \dfrac{\partial \left(x^2z+y^2 \right)}{\partial z} \\
& \text{div }\textbf{F} =  z^2+y^2 +x^2 \end{align}$$ $$\textbf{Compute Flux through $S_2$ using divergence theorem }$$ $$\int\int\int_E \text{div }\textbf{F} \hspace{2mm} dV= \int\int\int_E x^2+y^2+z^2 \hspace{2mm} dV$$ $ $ We can define $E$ in spherical as follows : $$\begin{align} & \left(\rho, \theta, \phi \right)\in E \hspace{1mm} |  \hspace{2mm} 0< \rho < 1, \hspace{2mm}  0< \phi < \dfrac{\pi}{2}, \hspace{2mm} 0< \theta < 2\pi \\
& \text{Therefore,} \quad   \int_0^{\pi/2}\int_0^{2\pi}\int_0^{1}  \rho^2  \quad (\rho) d\rho d\theta d \phi  \\ 
& =\int_0^{\pi/2}\int_0^{2\pi}\int_0^{1}  \rho^3  \hspace{2mm} d\rho d\theta d\phi \\
& =\int_0^{\pi/2}\int_0^{2\pi}\left[ \dfrac{\rho^4}{4}  \right]_0^{1}  \hspace{2mm}  d\theta d\phi \\
 & =\dfrac{1}{4}\int_0^{\pi/2}\int_0^{2\pi}  d\theta d\phi  = \dfrac{1}{4}\times \dfrac{\pi}{2}\times 2\pi = \dfrac{\pi^2}{4}  \end{align} $$ $$\textbf{Compute Flux through $S_1$ }$$ $ $ Note that $$\int\int_{S_1} \textbf{F}\cdot d\textbf{S} = \int\int_{S_1} \textbf{F}\cdot \textbf{n}\hspace{1mm}dS$$ Note that $S_1$ is part of the plane $z=0$ $$ = \int\int_{D} \textbf{F}\cdot \textbf{n}\sqrt{\left( \dfrac{\partial z}{\partial x}\right)^2+\left( \dfrac{\partial z}{\partial y}\right)^2+1}\hspace{1mm}dA $$ Where $\textbf{n} = \textbf{k}$ [because $\textbf{n}$ is  normal unit vector to $S_1$ ] And $D$ is the region inside the circle $x^2+y^2=1$ [ Because $D$ is projection of $S_1$ on $xy$ plane ] $$ \begin{align} & = \int\int_{D} \textbf{F}\cdot \textbf{k}\sqrt{\left( 0\right)^2+\left( 0\right)^2+1}\hspace{1mm}dA \\
& = \int\int_{D} x^2z+y^2 \hspace{1mm}dA \end{align} $$ Substitute $z=0$, since $S_1$ is part of the plane $z=0$ $$ = \int\int_{D} y^2 \hspace{1mm}dA $$ In polar coordinates the Integral becomes $$ \begin{align} & = \int_0^{2\pi}\int_{0}^1 r^2\sin^2\theta \hspace{1mm}(r)drd\theta \\
&  =\left( \int_0^{2\pi}\sin^2\theta \hspace{1mm}d\theta \right) \left(\int_{0}^1 r^3 \hspace{1mm}dr \right) \\
& =\dfrac{1}{2}\left[  \theta-\dfrac{\sin2\theta}{2}\right]_0^{2\pi} \left[ \dfrac{r^4}{4} \right]_{0}^1 \\
& =\dfrac{1}{2}\times 2\pi \times \dfrac{1}{4} = \dfrac{\pi}{4} \\ \end{align}$$ Therefore, answer should be $$\dfrac{\pi^2-\pi}{4}$$","['multivariable-calculus', 'calculus', 'integration', 'definite-integrals', 'vector-analysis']"
851030,How prove this $|A||M|=A_{11}A_{nn}-A_{1n}A_{n1}$ [duplicate],"This question already has answers here : Determinant identity: $\det M \det N = \det M_{ii} \det M_{jj} - \det M_{ij}\det M_{ji}$ (2 answers) Closed 9 years ago . Question: let the matrix $A=(a_{ij})_{n\times n},i=1,2,\cdots,n,j=1,2,\cdots,n$,  and the matrix $M=(a_{ij})_{(n-2)\times (n-2)},$ mean  that $$A=\begin{bmatrix}
a_{11}&\cdots&a_{1n}\\
\vdots& M&\vdots\\
a_{n1}&\cdots&a_{nn}
\end{bmatrix}$$
show that $$\det|A|\cdot \det |M|=A_{11}A_{nn}-A_{1n}A_{n1}$$
where $A_{ij}$ is cofactor with the matrix $A$. This problem is from linear problem book ,and this problem I can't deal it. because this value
$$|A||M|$$ I can't choose something to  deal it?","['matrices', 'matrix-calculus', 'determinant']"
851038,On decomposition of subgroups a finite abelian groups,"Let $G$ be an   finite abelian group  and $H$ is a subgroup of $G$. 
Then do can find a decomposition of $G$ as a direct sum of cyclic groups such that the intersections of the summands with $H$ give a direct sum decomposition of $H$?","['finite-groups', 'group-theory', 'abelian-groups']"
851054,Prove that a function is symmetric in its three variables.,"Let $U=\{1, 2,\ldots, 2014\}$. For positive integers $a$, $b$, and $c$, we denote by $f(a, b, c)$ the number of ordered $6$-tuples of sets $(X_1,X_2,X_3,Y_1,Y_2,Y_3)$ satisfying the following conditions: (i) $Y_1 \subseteq X_1 \subseteq U$ and $|X_1|=a$; (ii) $Y_2    \subseteq X_2 \subseteq U\setminus Y_1$ and $|X_2|=b$; (iii) $Y_3    \subseteq X_3 \subseteq U\setminus (Y_1\cup Y_2)$ and $|X_3|=c$. Prove that $f(a,b,c)$ does not change when $a$, $b$, and $c$ are rearranged. I tried drawing the venn diagram of the sets but I couldn't go anywhere from there. This is a problem from International Zhautykov Olympiad 2014. I hope someone could help me.","['contest-math', 'elementary-set-theory', 'functions', 'symmetry', 'combinatorics']"
851128,"Topological counterexample: compact, Hausdorff, separable space which is not first-countable","I need an example for a compact, Hausdorff, separable space which is not first-countable.
I tried to look for it for some time without success...","['first-countable', 'separable-spaces', 'examples-counterexamples', 'general-topology', 'compactness']"
851175,Intersection of 2 Indicator Functions,"Let $E$ and $F$ be events. Let $I_E(\omega)= \left\{\begin{array}{cc} 1, & \omega\in E, \\ 0, &\omega\in E^C. \end{array}\right.$ Show that $I_{E\cap F}(\omega)=I_EI_F$ I found the answer on this Link as shown below However, I not satisfy with this answer, I hope I can show this question by using only plus, minus, multiply or divide, just like what I do to in this problem (but I not able to solve the intersection problem by using the similar method) Is it possible? Please show me. I appreciate your help. (I know my request is quite strange and hope you can understand what am I asking about)","['probability', 'functions']"
851182,"If $\{M_i\}_{i \in I}$ is a family of $R$-modules free, then the product $\prod_{i \in I}M_i$ is free?","If $\{M_i\}_{i \in I}$ is a family of free $R$-modules, then $\bigoplus_{i \in I}M_i$ is free. Is this  true for the product $\prod_{i \in I}M_i$ too?","['modules', 'commutative-algebra', 'abstract-algebra']"
851185,How fat is a triangle?,"The slimness factor of a geometric shape in 2 dimensions is the ratio between the side-length of its smallest containing square and its largest contained square. This is an important factor in computational geometry. So the slimness of a square is 1, the slimness of a circle is $\sqrt 2$, but what is the slimness factor of a given triangle? After trying unsuccessfully to solve this geometrically, I decided to try a purely analytical solution. Here it is: The triangle The fatness obviously does not depend on scale. Hence two parameters are sufficient to define the triangle. I choose as parameters an angle $\theta$ adjacent to the longest side and the ratio $D$ between the side next to that angle and the longest side.  We have to find the fatness as a function of $\theta$ and $D$. Normalize the triangle so that its longest side lies on the x axis, between $(0,0)$ and $(1,0)$. Define $\theta$ as the angle at the origin. Rotate the triangle such that $\theta$ is between $0^\circ$ and $90^\circ$. For brevity, let $C=\cos \theta$ and $S=\sin \theta$. $D$ is the length of the side starting at the origin, so that the 3rd vertex of the triangle is at $(DC,DS)$, where $0<D<1$, $0\leq C<1$ and $0\leq S<1$. The slimness factor should be calculated as a function of the parameters $D$, $C$ and $S$. The containing square Consider the unit square $[0,1]\times[0,1]$. We want to transform it so that it contains the triangle. A general transformation of a point $(x,y)$ is: $(rcx+rsy+h, rsx-rcy+v)$ where: $r$ is the dilation factor, $s$ and $c$ are sine and cosine of the rotation angle (we can assume they are both in $[0,1]$ because of the rotational symmetry of the square), $h$ is horizontal translation and $v$ is vertical translation. So a general transformation of the unit square has the following corners: $(h,v)$ $(rc+h,rs+v)$ $(rs+h,-rc+v)$ $(rc+rs+h, rs-rc+v)$ and the following sides: $s(x-h)=c(y-v)$ $s(x-h-rs)=c(y-v+rc)$ which is the same as $s(x-h)=c(y-v)+r$ $c(x-h)=-s(y-v)$ $c(x-h-rc)=-s(y-v-rs)$ which is the same as $c(x-h)=-s(y-v)+r$ In order to contain the triangle, each vertex $(x,y)$ of the triangle must be in the 4 half-planes defined by the 4 sides of the square: $c(y-v)\leq s(x-h)\leq c(y-v)+r$ $-s(y-v)\leq c(x-h)\leq -s(y-v)+r$ Now, substitute each of the 3 vertices of the triangle and get, for $(0,0)$, $(1,0)$ and $(DC,DS)$ respectively: $-cv\leq -sh\leq -cv+r$ $sv\leq -ch\leq sv+r$ $-cv\leq s-sh\leq -cv+r$ $sv\leq c-ch\leq sv+r$ $c(DS-v)\leq s(DC-h)\leq c(DS-v)+r$ $-s(DS-v)\leq c(DC-h)\leq -s(DS-v)+r$ There are a total of 12 inequalities here. Our goal is to find the smallest $r$ such that there are $c,s,h,v$ satisfying all these 12 inequalities. Removing some redundant inequalities and ordering, we get: $r\geq s+cv-sh$ $cv-sh\geq 0$ $r\geq c-ch-sv$ $-ch-sv\geq 0$ $r\geq s(DC-h)-c(DS-v)=sDC-cDS+cv-sh$ $r\geq c(DC-h)+s(DS-v)=cDC+sDS-ch-sv$ To make $r$ as small as possible, we should select $h$ and $v$ such that inequalities 2 and 4 become zero. Then we remain with the following inequalities which $r$ must satisfy: $r\geq s$ $r\geq c$ $r\geq cDC+sDS$ Here I am stuck: how can I solve this optimization problem? The contained square Again consider the unit square $[0,1]\times[0,1]$. Now we want to transform it so that it is contained in the triangle. The 4 corners of the square must satisfy the 3 inequalities dictated by the sides of the triangle, which are: $y\geq 0$ $xDS\geq yDC$ $(1-x)DS\geq (1-DC)y$ Substituting the 4 corners of the square gives the following 12 inequalities which r must satisfy, some of which are redundant: $-rc+v\geq 0$ $v\geq 0$ (redundant) $rs+v\geq 0$ (redundant) $rs-rc+v\geq 0$ (redundant) $hDS\geq vDC$ $(rc+h)DS\geq (rs+v)DC$ $(rs+h)DS\geq (-rc+v)DC$ (redundant) $(rc+rs+h)DS\geq (rs-rc+v)DC$ (redundant) $(1-rc-h)DS\geq (1-DC)(rs+v)$ $(1-h)DS\geq (1-DC)v$ (redundant) $(1-rc-rs-h)DS\geq (1-DC)(rs-rc+v)$ $(1-rs-h)DS\geq (1-DC)(-rc+v)$ (redundant) These imply the following 5 inequalities (note that this time the direction of the inequalities is inversed because we are looking to maximize $r$ subject to the inequalities): $cr \leq v$ $0\leq hDS-vDC$ $(sDC-cDS)r  \leq hDS-vDC$ $(s-sDC+cDS)r\leq DS-v-hDS+vDC$ $(s-c-sDC+cDC+sDS+cDS)r\leq DS-v-hDS+vDC$ Here, again, I am stuck... The last step is just to divide the two $r$'s, but, how do I find each $r$? Note I am looking for a formula that gives fatness as a function of $\theta$ and $D$. However, if you think there is another pair of parameters by which it is more convenient to represent the fatness (e.g. the two smaller angles), then this is also welcome.","['optimization', 'geometry']"
851197,Uncountable family of random variables,"Let $\{ \xi _a \}_{a \in [0;1]}$ be a family of independent uniformly distributed on $[0;1]$ random variables on some probability space 
$(\Omega, \mathscr{F},P)$, indexed by a continuous parameter. Let $u$ be an independent of $\{ \xi _a \}_{a \in [0;1]}$ uniformly distributed on $[0;1]$ random variable. For $\omega \in \Omega$, define the map $$
\alpha : \Omega \to \mathbb{R}, \ \ \\   \ \alpha (\omega) = \xi_{u(\omega)} (\omega).
$$ Is $\alpha$ a random variable? I think the answer is negative, since the family $\{ \xi _a \}_{a \in [0;1]}$ is uncountable. How could I prove this?","['uniform-distribution', 'independence', 'measure-theory', 'random-variables', 'probability-theory']"
851222,How to calculate the integral $\int_{-1}^{1}\frac{dz}{\sqrt[3]{(1-z)(1+z)^2}}$?,"The integral is $I=\displaystyle\int_{-1}^{1}\dfrac{dz}{\sqrt[3]{(1-z)(1+z)^2}}$. I used Mathematica to calculate, the result was $\dfrac{2\pi}{\sqrt{3}}$, I think it may help.","['definite-integrals', 'improper-integrals', 'calculus', 'integration']"
851227,Help to evaluate this limit $\lim_{x \to \infty}x^{\frac{1}{x}}$,"What is the value of this limit?
$$
\lim_{x \to \infty}x^{\frac{1}{x}}
$$ I have never encountered such a limit before, so any help or advice would be much appreciated.","['radicals', 'limits']"
851263,Proving Two sets have same cardinality,"I'm studying on my own over the summer and I'm having a bit of trouble with this question. Also, I don't have any background in this, I haven't taken any classes in this yet so, if you choose to help, please go slow. Thanks. Anyway, here is the problem: For any set A, finite or infinite, let B be the set of all functions mapping A into the set {0,1}. Show that the cardinality of B is the same as the cardinality of P(A) (power set of A). Here's what progress I've made. Let f be a mapping from B to P(A). Now I have to show it's injective. Let g,h be elements of B and let x be an element of A: f(g(x)) = f(h(x)) g, h are elements of B and B maps x to {0,1}. If g(x) = h(x) = 1 or g(x) = h(x) = 0 then we are done. But what about the possibility of g(x) = 1 and h(x) = 0 or vice-versa. How do I deal with that possibility?",['elementary-set-theory']
851283,Category theory for graph theory research,"I am doing research in algebraic graph theory, focusing on the relation between graphs and groups (especially the representing groups as graphs) for my Ph.D. In particular, one of the ideas is to study groups using their endomorphisms and define related graphs. Are ideas from category theory likely to be helpful in this field? If so, which book would be a best introduction to category theory from the point of view of someone interested in algebra and graph theory? I looked at other questions about book recommendations for category theory, but they ask for books related to set theory and foundations, or programming, or ask for a general introduction. Also, the first question is more important than the second , and I would really like to hear some views from category theory experts about possible applications in graph theory and algebra research. Please let me know if I should add more details.","['book-recommendation', 'abstract-algebra', 'graph-theory', 'category-theory', 'group-theory']"
851285,Find the minimum of $\displaystyle \frac{1}{\sin^2(\angle A)} + \frac{1}{\sin^2(\angle B)} + \frac{1}{\sin^2(\angle C)}$,"Is it possible to find the minimum value of $E$ where
$$E = \frac{1}{\sin^2(\angle A)} + \frac{1}{\sin^2(\angle B)} + \frac{1}{\sin^2(\angle C)}$$for any $\triangle ABC$. I've got the feeling that $\min(E) = 4$ and that the critical value occurs when $ABC$ is equilateral.",['trigonometry']
851289,Second-order non-linear ODE,"$2tx'-x=lnx'$ I differentiated both sides with respect to x: $x'+2tx''=\frac {x''}{x'}$ Substituting $p=x'$, $p+2tp'=\frac{p'}{p}$ But I have no clue what can I do from here on. EDIT: $t$ is the non-dependent variable.",['ordinary-differential-equations']
851290,Relation between the integral of geodesic curvature and Gaussian Curvature,"I need help with an exam question: Let $S$ be a regular oriented surface such that for any simple, closed, and positively oriented curve in $S$ the value of the integral of the geodesic curvature along this curve is always the same( that is, independent of the curve). What can be said about the Gaussian curvature of $S$? I thought about using the Gauss-Bonnet Theorem: $$\int_C \kappa_g(s) ds + \iint_R K d\sigma + \sum_{l=1}^p\theta_l = 2\pi\chi(R)$$ Since the $\int_C \kappa_g(s) ds$ is always the same, for any curve. Then for a geodesic, the curvature is zero, so $\int_C \kappa_g(s) ds = 0$. So, I can assume that if the integral of the geodesic curvature is always the same, then it must be zero. Is it correct to assume that? If that is correct, where do I go from there?",['differential-geometry']
851302,How to prove $\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=+\infty$,"How to prove $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=+\infty.$$ I try to do like $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=\sum_{N=1}^\infty \sum_{n+m=N}^\infty  \frac{1}{m^2+n^2}=\sum_{N=1}^\infty \sum_{m=1}^{N-1}  \frac{1}{m^2+(N-m)^2}$$
 $$\frac{1}{m^2+(N-m)^2}\leq \frac{2}{N^2}$$
but it doesn't work.","['sequences-and-series', 'real-analysis']"
851327,Show that it is possible that the limit $\displaystyle{\lim_{x \rightarrow +\infty} f'(x)} $ does not exist.,Let $f: \mathbb{R} \rightarrow \mathbb{R}$ a differentiable function with continuous derivative and the limit $\displaystyle{\lim_{x \rightarrow +\infty} f(x) }$ exists. Show  with an example that it is possible that the limit $\displaystyle{\lim_{x \rightarrow +\infty} f'(x)} $ does not exist. My attempt: $$f(x)=\int_{-\infty}^{x} e^{t^2}dt$$ $$\lim_{x \to +\infty} f(x)=\lim_{x \to +\infty} \int_{-\infty}^{+\infty} e^{t^2}dt=\frac{\sqrt{\pi}}{2}$$ $$\lim_{x \to +\infty} f'(x) =\lim_{x \to +\infty} e^{x^2}= +\infty \notin \mathbb{R}$$ Is my attempt right?,"['calculus', 'integration', 'analysis', 'derivatives', 'limits']"
851328,"$\text{lcm}(1,2,3,\ldots,n)\geq 2^n$ for $n\geq 7$","I can prove that $\text{lcm}(1,2,3,\ldots,n)\geq 2^{n-1}$. Newly, i read in a paper that  for $n\geq 7$ we have: $$\text{lcm}(1,2,3,\ldots,n)\geq 2^n$$ Can you prove it? (This inequality is an interesting inequality. For example with this inequality can find a lower bound for the number of primes less than $n$.)","['prime-numbers', 'inequality', 'least-common-multiple', 'number-theory']"
851380,Birthday problem & primes,"Let $\pi_k(n)$ be the almost prime counting function, then $\pi_k(2^kn)$ reaches a max value, since $\pi_k(2^kn)=\pi_{k+1}(2^{k+1}n)$ for large enough $k$. (eg, $\pi_{5}(272)=\pi_{6}(2\times272)=\pi_{7}(4\times272)=\pi_{8}(8\times272)\dots$) Is it true that, for fixed $n$, that $\dfrac{\pi_k(2^kn)}{max.\pi_k(2^kn)}\sim 1-e^{-k^2/2\log(n)}$ for any $k$? ($n=10,n=10^3,n=10^6$ respectively.) AlmostPrimePi[k_Integer, n_] := 
Module[{a, i}, a[0] = 1; If[k == 1, PrimePi[n], Sum[PrimePi[n/Times @@ 
Prime[Array[a, k - 1]]] - a[k - 1] + 1, Evaluate[Sequence @@ Table[{a[i], 
a[i - 1], PrimePi[(n/Times @@ Prime[Array[a, i - 1]])^(1/(k - i + 1))]}, 
{i, k - 1}]]]]]

n = 100000; range1 = 15; range2 = 35;
m = AlmostPrimePi[range2, 2^range2 n];
Show[Plot[1 - 1/(E^((k^2)/(2 Log[n]))), {k, 0, range1}], 
ListLinePlot[Join[{0}, Table[AlmostPrimePi[k, 2^k n], {k, 1, range1}]/m], 
DataRange -> {0, range1}, PlotStyle -> Red]] ($k=35$ works up to $n=10^6$, since $2^{35}\times10^6<3^{35}$. range2 should be increased for $n<10^6$ to satisfy $2^{k}n<3^{k}$.) If this is the case, what does this have to do with the birthday problem ?","['prime-numbers', 'number-theory']"
851387,Find the directional derivative of the scalar field,"Find the directional derivative of the scalar field: $f(x,y,z)=\log(x^2+y^2+z^2)$ at $P_0(1,1,1)$ in the direction of the straight line $\ P_0P $  where $P=(3,2,1)$ What I have done: $\nabla(f)=(2/3,2/3,2/3)$ at $P_0$ and I know the eqation of $P_0P$ is $(x-1)/2=(y-1)/1=(z-1)/0=t(say)$ Now the required unit vector is $\frac{(3-1,2-1,1-1)}{\sqrt 5}$ taking dot product with $\nabla f$ my final result comes out to be $\frac{2}{\sqrt{5}}$ . But the answer does not match. Also my confusion arises seeing this article . According to the article, answer comes out as $\frac{9}{\sqrt{14}}$ . But the answer given in my exercisebook is $\frac{8}{3\sqrt5}$. Please help.","['multivariable-calculus', 'real-analysis']"
851392,Partial integration for lebesgue integrable functions,"I want to show the following: Let be two Lebesgue integrable functions given: $f,g:[a,b] \rightarrow \mathbb R$. We define the functions: $$F,G: [a,b] \rightarrow \mathbb R : F(x)=\int_{[a,x]}^ \! f(t) \, dt, G(x)=\int_{[a,x]}^ \! g(t) \, dt. $$ Show that the rule for the partial integration holds also in this case, i.e. It holds: $$\int_{[a,b]}^ \! F(x)g(x) \, dx =F(b)G(b)-\int_{[a,b]}^ \! f(x)G(x) \, dx$$ Hint: Use the theorem of Fubini. I tried a bit , but I didn't come to any solution.","['lebesgue-integral', 'integration']"
851410,"Show that $f^{(n)}(0)=0$ for $n=0,1,2, \dots$","Let $f: \mathbb{R} \rightarrow \mathbb{R}$ an infinitely many times differentiable function and $f(\frac{1}{n})=0$ for each $n \in \mathbb{N}$. Show that $f^{(n)}(0)=0$ for $n=0,1,2, \dots$ $$$$ Could you give me some hint what I could do?? I got stuck right now..",['calculus']
851420,Prove that the function $F ( z ) = \sum_{n = 0}^\infty \frac{c_n z^n}{n!}$ is analytic on the whole $\mathbb{C}$,"Let $f : \mathbb{D} → \mathbb{D}$ be an analytic function with Taylor series $f(z) = \sum_{n = 0}^\infty c_nz^n.$ Prove that the function
$F ( z ) = \sum_{n = 0}^\infty \frac{c_n z^n}{n!}$
is analytic on the whole $\mathbb{C}$ (i.e., is entire) and satisfies the inequality $|F (z)| ≤ e^{|z|}$
for all $z ∈ \mathbb{C}.$ This is an old qual problem in Complex Analysis.  I am at a loss on how to begin.  I would appreciate some help.  Thank you.",['complex-analysis']
851424,Finding the range and domain of $f(x)=\tan (x)$,"I am attempting to find the range and domain of $f(x)=\tan(x)$ and show why this is the case. I can seem to find the domain relatively well, however I run into problems with the range. Here's what I have done so far. Finding the domain of $f(x)=\tan(x)$ Consider $f(x)=\tan(x)$ is defined as $f(x)=\tan(x)=\frac{\sin(x)}{\cos(x)}$, it is clear the domain of $f(x)$ is undefined when $\cos(x)=0$. $\cos(x)=0$ whenever $x=\frac{\pi}{2}+\pi k$ for integers $k$, so the domain of $f(x) =\tan (x)$ can be stated as $x\in\mathbb{R}, x \ne \frac{\pi}{2}+\pi k\text{  for integers k}$ Finding the range of $f(x)=\tan(x)$ To find the range of $f(x)=\tan(x)$ we must refer to the definition of $f(x)=\tan(x)=\frac{\sin(x)}{\cos(x)}$. From this we can see that $f(x)$ is undefined when $\cos(x)=0$, as the interval of $\cos(x)$ is $[-1,1]$ we will now need to split this into two cases: $-1\leqslant\cos(x)<0$ and $0<\cos(x)\leqslant1$. Considering the first interval; $-1\leqslant\cos(x)<0$, as $\cos(x)\to0^-$: $\sin(x)\to1$ and $\tan(x)=\frac{\sin(x)}{\cos(x)}\approx\frac{1}{\text{very small(negative)}}\approx\text{very big(negative)}$. In other words as $\cos(x)\to0^-$, $\tan(x)\to-\infty$ Considering the second interval; $0<\cos(x)\leqslant1$, as $\cos(x)\to0^+$: $\sin(x)\to1$ and $\tan(x)=\frac{\sin(x)}{\cos(x)}\approx\frac{1}{\text{very small(positive)}}\approx\text{very big(positive)}$. In other words as $\cos(x)\to0^+$, $\tan(x)\to+\infty$. This is where I am up to. What I want to know is how I can show definitively that $tan(x)$ can take all the values within the interval $[-\infty,\infty]$. Some people would call it a day here, and say that this shows that the range  of $f(x)$ is $-\infty<\tan(x)<\infty$. However I nearly ran into a similar error when I was finding the range of $\sec(x)$, only to discover that although it does tend to positive and negative infinity it doesnt take any values in the interval $(-1,1)$. Where do I proceed from here? EDIT: I am not looking for answers that use differentiation. Answers should be pre-calculus level.","['trigonometry', 'algebra-precalculus', 'functions']"
851442,$rk(A)=n$ implies $rk(AB)=rk(B)$,"Let $A \in Mat_{m\times n}(\mathbb{R})$ and $B \in Mat_{n\times p}(\mathbb{R})$ . Assume $rk(A)=n$ . Prove that $rk(AB)=rk(B)$ . Lets start by proving $rk(B) \ge rk(AB)$ . Indeed, since the rows of $AB$ are linear combination of $B$ 's rows, $AB \subseteq B$ and therefore, $rk(B) \ge rk(AB)$ . Now, I wish to show the other part which is: $rk(AB) \ge rk(B)$ . How should I do that? Or maybe I should prove it with another approach.","['vector-spaces', 'matrices', 'linear-algebra']"
851443,Klein's 4 subgroups,"I have just started learning about group theory. And, I learnt about The Klein's 4 group. I tried proving that two distinct Klein's 4 subgroup of a group intersect only at Identity. But I can't. So please help me.",['group-theory']
851470,What to study from Eisenbud's Commutative Algebra to prepare for Hartshorne's Algebraic Geometry?,"I surveyed commutative algebra texts and found Eisenbud's ""Commutative Algebra: With a View Toward Algebraic Geometry"" to be the most accessible for me. The book outlines a first course in commutative algebra in the introduction. The course uses most of the material in chapters 1 to 14. Is this course sufficient to prepare my self for Hartshorne's Algebraic Geometry? Or do I need to study more chapters? You can find the book's table of contents here . Also, the first chapter in the book ""Roots of Commutative Algebra"" is a survey of a wide range of topics. Can I safely skip most of it as indicated by the chapter's intro? Thank you","['commutative-algebra', 'algebraic-geometry', 'reference-request']"
851479,Sobolev spaces and using monotone convergence theorem (don't understand a paper),"I'm reading this paper . In it there the following argument (see page 240). Firstly, what precisely does the author mean by the displayed equation after 66? The PDE in (65) only holds weakly.. $\frac{\partial b(\overline{v}_n)}{\partial t}$ is only in $H^1(0,T;H^{-1})\cap L^\infty(0,T;L^s)$. He just integrates it in time.. I don't follow. Secondly, I am lost with the spaces when he says that $\int_0^T \overline{v}_n(s)\;ds$ is a Cauchy sequence in the space $L^\infty(0,T;W^{1,r}_0)$. And then he uses the monotone convergence theorem to say that $\overline{v}_n$ convreges strongly in $L^1(\Omega \times (0,T))$ to some $\overline{v}$. Again a different space. Any explanation appreciated.","['convergence-divergence', 'sobolev-spaces', 'partial-differential-equations', 'functional-analysis', 'bochner-spaces']"
851498,Proof that the limit of $\frac{1}{x}$ as $x$ approaches $0$ does not exist,"Hello I was hoping that someone might be able to verify that the following proof that $\lim_{x\to 0} {1\over x}$ does not exist is correct. First assume that $\lim_{x\to 0} {1\over x} = L$. This means that for every $\epsilon > 0$ there is a $\delta > 0$ such that for all $x$ if $0 < \lvert x\rvert < \delta$ then $\lvert{1\over x} - L \rvert <\epsilon.$  Now let $\epsilon=1$ and choose $x < \min(1,\delta)$.  Therefore there is a $\delta>0$ such that if $0 < \lvert x\rvert < \delta$ then $\lvert{1\over x} - L \rvert <\epsilon$.  It follows that $\lvert{1\over x}\rvert < 1+ \lvert L\rvert$.  However clearly $\lvert{1\over x}\rvert>1$ and therefore a contradiction has been reached and there is no number $L$ such that $\lim_{x\to 0} {1\over x} = L$.","['epsilon-delta', 'proof-verification', 'real-analysis', 'limits']"
851504,"If $f(x) = \frac{x}{x+1}$ and $g(x) = 2x-1$, find $(g\circ f) (x)$.","If $f(x) = \frac{x}{x+1}$ and $g(x) = 2x-1$, find $(g\circ f)(x)$. My answer is $\frac{x-1}{x+1}$. However, the answer key in the book states $\frac{2x}{x+1}$. How is that? Is the book wrong?",['algebra-precalculus']
851538,"$x^5+x-1=0\;,\;x\in\mathbb{R}$","Given the following equation $$x^5+x-1=0\;,\;x\in\mathbb{R}$$ How to prove that (unevaluated) $$x=\dfrac13\left(-1+\sqrt[3]{\dfrac{25}2-\dfrac{3\sqrt{69}}2}+\sqrt[3]{\dfrac12\left(25+3\sqrt{69}\right)}\right).$$ $x^5+x-1=0\;,\;x\in\mathbb{R}$ Any hint would be appreciated.",['algebra-precalculus']
851551,How to find the integer part of big number?,"How to calculate the integer part
$$\left \lfloor10^{10^{10^{10^{10^{-10^{10}}}}}} \right \rfloor ?$$
Does this equal $$10^{10^{10}}? $$
Both Maple and Mathematica fail with it. PS. Unmotivated votes down are not a constructive answer.","['arithmetic', 'algebra-precalculus', 'numerical-methods']"
851584,Is $\sigma$-finiteness really a necessary condition for this problem?,"Question: Let $(X, \mathcal A, \mu)$ be a measure space and suppose $\mu$ is $\sigma$-finite. Suppose $f$ is integrable. Prove that given any $\varepsilon$, there exists a $\delta >0$ such that $$\int_A |f| d\mu < \varepsilon$$
whenever $\mu(A) < \delta$. My Solution: We use the fact that if $f \geq 0$ is measurable, then $\lim_{n\to\infty}\int (f\wedge n) \to \int f$, proved in Exercise 6.3. Since $|f|  = \max(f, 0) + \max(-f,0)\geq 0$ is a sum of measurable functions, it is itself measurable, and we can apply the result of Exercise 6.3. For simplicity, let $f_n = |f| \wedge n$. ( $f \wedge n = \min(f,n)$ ) Let $\varepsilon> 0$ be given and $A\in \mathcal A$. Then there exists an $N \in \mathbb N$ such that for all $n > N$, we have 
$$ \int_A |f|- \int_A f_N d\mu < \varepsilon/2.$$
Since $f_n \leq n$ for all $n$, we know that
$$\int_A f_N d\mu \leq \int_A N d\mu = N\cdot \mu(A).$$
Choose $\delta = \varepsilon / (2N)$ and $A \in \mathcal A$ such that $\mu(A) < \delta$. Then,
\begin{align*}
\int_A |f| d\mu &= \int_A f_N d\mu  + \int_A |f|d\mu - \int_A f_Nd\mu\\
&< N\cdot \mu(A) + \varepsilon/2\\
&< N\cdot \delta + \varepsilon/2\\
&= \varepsilon.
\end{align*} Is $\sigma$-finiteness really a necessary condition on $\mu$?","['measure-theory', 'integration', 'real-analysis']"
851591,$A\cup B$ is connected when $A$ is connected in $X$ and $B$ clopen in $X-A$,"Let $A$ be connected subset of a connected space $X$, and $B\subset X-A$ be an open and closed set in the topology of the subspace $X-A$ of the space $X$. Prove that $A\cup B$ is connected. I think the idea is to show limit point of $A$ is in $B$ or vice-versa.But I have no idea how to show it. I am preparing for qualifying so please do for me...
Thanks for help....","['general-topology', 'connectedness']"
851596,Show $\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A+B)$ [duplicate],"This question already has answers here : Show that $\operatorname{rank}(A+B) \leq \operatorname{rank}(A) + \operatorname{rank}(B)$ (6 answers) Closed 2 years ago . Show $\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A+B)$, where $A,B \in M_{m\times n}(\mathbb{F})$. I'm trying to think in terms of linear transformations. We can define $T_a, T_b:\mathbb{F}^n\rightarrow \mathbb{F}^m$. I know that $\dim_{\mathbb F}\operatorname{Im} T_a, \dim_{\mathbb F}\operatorname{Im} T_b \le m$. What should I do next?","['matrices', 'linear-algebra', 'inequality', 'matrix-rank']"
851616,How to calculate this integral using Rodrigues' formula?,"I'm trying to get practice using Rodrigues' formula for Legendre Polynomials, but it's being quite confusing to manipulate that $n$-th derivative. Basically, I'm trying to calculate: $$\int_{-1}^1 x^n P_n(x) dx$$ using Rodrigues' formula. Substituing $P_n$ we have $$\int_{-1}^1 x^n P_n(x)dx = \dfrac{1}{2^n n!} \int_{-1}^1 x^n \dfrac{d^n}{dx^n}(x^2-1)^ndx$$ Now integrating by parts I have the following: $$\int_{-1}^1 x^nP_n(x)dx=\dfrac{1}{2^n n!}\left(x^n \dfrac{d^{n-1}}{dx^{n-1}}(x^2-1)^n \bigg|_{-1}^1 - n\int_{-1}^1 x^{n-1} \dfrac{d^{n-1}}{dx^{n-1}}(x^2-1)^ndx\right)$$ Now I don't know how to evaluate that boundary term, also I would need to do integration by parts again, but then what? I imagine I would keep doing it until the derivative disappears. In the process I would get a lot of boundary terms and then in the end $$\int_{-1}^1x^nP_n(x)dx = \dfrac{1}{2^n n!}\left([\text{Boundary terms}] + (-1)^n n!\int_{-1}^1(x^2 -1)^n dx\right)$$ Where the $(-1)^n$ comes because at each step the new integral is multiplied by the sign of the last one and the $n!$ appears because the $n$ on the first integral multiplies $(n-1)$ on the second and so forth. But still, this is something I see intuitively, how can I really calculate that? Also, how to deal with all those boundary terms? How to make this computation in a good way?","['legendre-polynomials', 'integration']"
851619,"If I bet half of my money each round in a fair gamble, what's the probability...","that I can make 10 times of what I initially have? Here's the formal description. In a fair gamble, I lose or double my wager each with probability 1/2. No matter how much money I have, I always gamble half of my money (the money is infinitely divisible, so that I'm never ruined). If I start with 1 dollar, and I win once I own 10 dollars or more, what's the probability of winning? Let $X_n$ be the money I have after the nth round. Thus $X_{n+1} = 1.5X_n$ or $0.5X_n$ each with p = 1/2, and the process is a martingale. I tried to use the optional stopping theorem, by setting $\tau$ as the time that my money goes over 10 or goes below $\epsilon$ for the first time, and then let $\epsilon$ approach 0. The problem is, $X_{\tau}$ never actually equals 10. In fact it lies in the interval [10,15). So the theorem would only give a bound on the desired probability: $E[X_{\tau}] = E[X_{\tau}|X_{\tau} \geq 10] \Pr (X_{\tau} \geq 10) + E[X_{\tau}|X_{\tau} \leq \epsilon] \Pr (X_{\tau} \leq \epsilon) $ Next I take $y$ as the natural log of money and let $f(y)$ be the probability I can win with initial log-money $y$. The following formulas looked promising: $f(y) = 1, y \geq \ln 10$ $f(y) = 1/2 + f(y - \ln 2), \ln(20/3) \leq y < \ln 10$ $f(y) = f(y - \ln 2)/2 + f(y + \ln (3/2))/2, y < \ln(20/3)$ but still I can't find the right analytic expression of $f(y)$. I'm not even sure if $f$ is continuous at $y=\ln 10$ or $y = \ln (20/3)$. Any help is appreciated!","['martingales', 'probability', 'gambling']"
851650,Dot product notation,"Let $\mathbf{A=(a_1,a_2,\ldots, a_n)}$ and $\mathbf{B=(b_1,b_2,\ldots,b_n)}$. Many linear algebra books and texts define the dot product as
$$
\mathbf{A\cdot B^T=a_1b_1+a_2b_2+\cdots+a_nb_n}
$$
where $\mathbf{B^T}$ is the transpose of row vector $\mathbf{B}$. But Serge Lang in Linear Algebra define as
$$
\mathbf{A\cdot B=a_1b_1+a_2b_2+\cdots+a_nb_n}
$$
There is no diference between the results. But  which is more correct and why? EDIT As Cameron Williams said, $\mathbf{A\cdot B^T}$ is implied by matrix multiplication. So I correct a part of my question: Many linear algebra books and texts define the dot product as matrix multiplication
$$
\mathbf{A B^T=a_1b_1+a_2b_2+\cdots+a_nb_n}
$$","['notation', 'linear-algebra']"
851667,Converting a set to a tuple?,"Okay, so, let's say I have a set: $\{0,1,2,3\}$ And I want to convert it to a tuple: $(0,1,2,3)$ How would I do this? Would it be as simple as: $f(\{0,1,2,3\}) = (0,1,2,3)$ ??","['elementary-set-theory', 'functions']"
851683,solving double integrals,"I'm trying to solve a double integral: $\displaystyle \int_{0}^{\frac{1}{2}}\int_{0}^{\frac{1}{2}-y}24xy\; dx \;dy$ I first solved in respect to $y$, making the $x$ a constant and plugged in the $y$ values then took the derivative with respect to $x$, making the $y$ values a constant. 24 x2/2(1/2-y)2 - x2/2(1/2-0)2 = ?  this is where I am stuck","['multivariable-calculus', 'calculus', 'integration']"
851685,"Problem with a pushforward of vector field formula (Michael Taylor, ""Partial Differential Equations"")","Let $X$ denote a vector field and let $\mathcal F^t_X$ denote its flow. If $X$ and $Y$ are two vector fields we denote by $\mathcal F^t_{X\#}Y$ the vector field satisfying
$$
    \mathcal F^s_{\mathcal F^t_{X\#}Y} = \mathcal F^{-t}_X \circ \mathcal F^s_Y \circ \mathcal F^t_X,
$$
for small $|t|$, $|s|$. Then $\mathcal F^{t+s}_X = \mathcal F^t_X \mathcal F^s_X$ and hence $\mathcal F^{t+s}_{X\#} = \mathcal F^t_{X\#} \mathcal F^s_{X\#} = \mathcal F^s_{X\#}\mathcal F^t_{X\#}$. Taking into account that $\mathcal L_X Y = [X,Y]$ and differentiating the latter inequality we obtain:
$$
    \frac{d}{dt} \mathcal F^t_{X\#} Y = [X,\mathcal F^t_{X\#}Y], \\
    \frac{d}{dt} \mathcal F^t_{X\#} Y = (D\mathcal F^t_{X\#})(Y) \cdot [X,Y].
$$
But at page 41 of M. Taylor's ""PDEs I"" formula (8.6) states that
$$
   \frac{d}{dt} \mathcal F^t_{X\#} Y = \mathcal F^t_{X\#} [X,Y].
$$
Please, help me, where is the problem?","['dynamical-systems', 'ordinary-differential-equations', 'differential-geometry']"
851687,"Reference Request: Prereqs for Lecture Notes on ""Abstract Linear Algebra""","I just found this set of lecture notes on linear algebra which seems to go over several things I've been wondering about as I study linear algebra.  Unfortunately there are very few exercises in the text and I'm not even positive I have the skill to prove the first few lemmas myself.  I have read the first couple of sections so far, and I think I understand the material as well I as can while not having any exercises to practice on -- which is not well enough .  So my question is: Is there a book/ reference which goes over this same material, but can be more easily self-studied?  Or else can anyone recommend a text which will at least prepare me enough to be able to figure these lecture notes out on my own?","['linear-algebra', 'reference-request', 'soft-question']"
851698,Can this be simplified?,"$$
e^{-i\frac43\pi n} - e^{-i\frac23\pi n}, n\in \mathbb{N}
$$
I am trying to simplify this but cant. Any ideas appreciated.","['trigonometry', 'fourier-analysis', 'calculus']"
851716,"If $a+\sqrt{b}$ is a root of a polynomial equation with integer coefficients, so is $a-\sqrt{b}$","I tried to use the Briot-Ruffini method but it didn't work. The question I need help is:
""Prove that, if a polynomial equation with integer coefficients has the irrational number $a+\sqrt{b}$ as a root, with $a,b \in \mathbb{Z} $, $b$ a prime number, so is $a-\sqrt{b}$.""",['abstract-algebra']
851728,"What does ""extension"" mean in the Axiom of extension","I am learning Set Theory from the book Naive Set Theory by Halmos as part of my course. The first chapter is on the Axiom of Extension. I understand what it is but what I don't understand is why it has the word ""extension"" in the title for this axiom. I can't understand how equality has the same meaning as extension. Please can someone clarify this.","['terminology', 'axioms', 'elementary-set-theory']"
851736,What does the secant value represent?,"What does the secant value represent? I know that $$\sec = 1/\cos(\theta)$$ but really I do not know what this value represents, so I need your help. A clear example with images would be appreciated.","['geometry', 'calculus']"
851742,Calculate coordinate of any point on triangle in 3D plane,"I am really stuck and can't find right way to write a formula(s) that will calculate Z coordinate of point on triangle plane in 3D plane. I know all coordinates of triangle points ( Ax, Ay, Az, Bx, By, Bz, Cx, Cy, Cz ), and I know x and y of point, but I need z where it's touching triangle plane. I am pretty sure I am going wrong way, but I want to tell you what I have done so far...
I figured out that if point is closer to triangle angle it's more effected by it, so I calculated how far is it from each angle, and by that it's clear, each triangle point is ""pulling"" point to his Z coordinate, and the closer the point the more strength it has. I went over and over, and couldn't find a way to put this in formula. I used 3D program as simulation and I know right missing Z coordinate, but I can't find formula that can calculate it... Here is the info: x    y   z
A ( 0,   0,  0 )
B ( 80, 50, 20 )
C ( 10, 60, 10 )

X ( 50, 45, 14.418* ) Missing coordinate that needs to be calculated approximately from 3D program... Here is the image from the program, if needed, just coordinates are switched here, but you can get the clue how it looks like and what needs to be calculated... If anyone know formula for this that will be a life saving! Thanks! Found answer here: Finding the missing coordinate of a point within a 3D triangle","['trigonometry', '3d']"
851749,"Finding an area of a triangle inside of a triangle, given certain areas of other triangles, and area ratios.","I'm studying for the Waterloo Math Contest (Galois, Gr. 10) taking place in April of 2015 and I am preparing by looking at previous problems and solving them. This is question 4(c) on the 2010 Galois Contest : In the diagram, points $X$, $Y$ and $Z$ are on the sides of $\triangle UVW$, as shown. Line segments $UY$, $VZ$ and $WX$ intersect at $P$. Point $Y$ is on $VW$ such that $VY:YW=4:3$. If $\triangle PYW$ has an area of $30$ and $\triangle PZW$ has an area of $35$, determine the area of $\triangle UXP$. $$\\$$ I came across this problem which unfortunately, I could not solve. I've got a bit of it done, but I simply don't understand what ""connections"" and relations to make with the given information in the question which are the areas of the triangles, and the provided ratio of 4:3. What I've done so far: I've written down that, Triangle PYW's Area = 30
PZW's Area = 35 There is a ratio in terms of line length such that VY:YW = 4:3. Given this information, because I know that PYW's area is 30, and the ratio is 4:3, I think that Triangle PVY's area is equal to 40. Due to line PY being the height of triangles PVY and PYW, this means that their height is the same. It is only the base length that differs, which means that PYW which has the shorter line segment will have an area of 30, while PVY will have an area of 40. I just don't really know where to go on from here. I can't see any other correlations that may lead to finding out the area of triangle UXP. Could someone help me with this question, explain their thought process going into this, as well as tips on how to do well in these types of geometry questions? Is there anything that you immediately look for?","['relations', 'geometry', 'triangles', 'trigonometry']"
851759,Series $\sum \frac{1}{n^2\sin^3n}$,Question : Show that series $\sum \cfrac{1}{n^{2}\sin^{3}n}$ is divergent. Hint: Show that $$\sum \frac{1}{n|\sin(n)|}$$ is divergent. I am interested in other possible proofs for this question.,"['summation', 'sequences-and-series', 'real-analysis']"
851761,Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^m $ be differentiable and $K \subset \mathbb{R}^n$ be compact and convex. Show $f$ is Lipschitz on $K$.,"The Assignment: Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^m $ be continuously partial differentiable and let $K \subset \mathbb{R}^n$ be compact and convex. Show $f$ is Lipschitz on $K$. A hint my tutor gave me is to use the MVT, which is why I'm trying to get an expression I can use it on. Since $ K $is compact and $f$ is continuously differentiable, the derivative will be bounded since we're in $\mathbb{R}$. I have several problems, firstly I still have no clue how/where to use the MVT and don't know where the fact that K is convex becomes important. (I do know the definition of convex, though.) I'd appreciate any help.","['multivariable-calculus', 'ordinary-differential-equations', 'real-analysis']"
851769,Any non-zero homomorphism of holomorphic vector bundles over a compact Riemann surface factors through a maximal rank homomorphism,"I was reading the paper ""Stable and Unitary vector bundles on a compact surface"" by Narashiman and Seshadri. I quote from the paper: Let $W_1$ and $W_2$ be two vector bundles of rank $n$ on the compact Riemann surface $X$. A (holomorphic) homomorphism $f:W_1\to W_2$ is said to be of maximal rank if the canonical extension $\bigwedge^nf:\bigwedge^nW_1\to\bigwedge^nW_2$ is a non-zero homomorphism. If $f:W_1\to W_2$ is a homomorphism of maximal rank we have $d(W_1)\le d(W_2)$, and if $d(W_1)=d(W_2)$, $f$ is an isomorphism. (These statements follow from the corresponding statements for line bundles.) Let $V$ and $W$ be two vector bundles on $X$, not necessarily of the same rank. Let $f:V\to W$ be a non-zero homomorphism. Since the structure sheaf $\mathbf O_x$ is a sheaf of principal ideal domains, we see that $f$ has the following canonical factorisation
  $$\begin{array}\\
0&\to&V_1&\to&V&\overset\eta\to&V_2&\to&0\\
&&&&&&\downarrow\small g\\
0&\gets&W_2&\gets&W&\underset i\gets&W_1&\gets&0
\end{array}$$
  where $V_1,V_2,W_1,W_2$ are vector bundles, each row is exact, $f=i\circ g\circ\eta$ and $g$ is of maximal rank. We call $W_1$ the subbundle of $W$ generated by the image of $f$. Can someone please explain how does any non-zero homomorphism of vector bundles can be factored through a maximal rank homomorphisms? It will be helpful if someone provides with an simple to read reference.","['riemann-surfaces', 'algebraic-geometry', 'vector-bundles']"
851774,What is the quickest way to show that the integral equals zero?,Let $a=x_0<\ldots<x_n=b$ be equidistant sampling points with $n$ being an even number. How can one show that $$\int_a^b \prod_{k=0}^n(x-x_k) \ \text{d}x=0$$ in a fast way? I showed it by proving that $\prod_{k=0}^n(x-x_k)$ is an odd function w.r.t. $\frac{a+b}{2}$ and then substituting to shift the boundaries of integration. This was quite tedious and I thought maybe there is a quick and smart argument to show it's zero.,"['calculus', 'integration']"
851777,Prove that These Families of Level Curves are Orthogonal,"From p. 79 in Brown's and Churchill's ""Complex Variable and Application"": Let the function $f(z) = u(x, y)+iv(x, y)$ be analytic in a domain $D$, and consider the family of level curves $u(x, y) = c_1$ and $v(x, y) = c_2$.  Prove that these families are orthogonal. Specifically, show that if $z_0 = (x_0, y_0)$ is a point in $D$ which is common to two particular curves $u(x, y) = c_1$ and $v(x, y) = c_2$, and if $f'(z_0) \ne 0$, then the lines tangent to the curves at $(x_0, y_0)$ are perpendicular to each other.  Then, the question gave a clue that: $\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}\frac{dy}{dx} = 0$ and  $\frac{\partial v}{\partial x}+\frac{\partial v}{\partial y}\frac{dy}{dx} = 0$ (*) At first, I thought that the authors meant $\frac{\partial u}{\partial x} = \frac{\partial u}{\partial x}\frac{dx}{dx}+\frac{\partial u}{\partial y}\frac{dy}{dx} = 0$, $\frac{\partial v}{\partial x} = \frac{\partial v}{\partial x}\frac{dx}{dx}+\frac{\partial v}{\partial y}\frac{dy}{dx} = 0$.  However, that cannot be the case, since we were told that $f'(z_0) = u_x(x_0, y_0)+iv_x(x_0, y_0) \ne 0$.  Furthermore, $\frac{\partial u}{\partial x} = 0$ implies that $u(x, y)$ is constant in a direction parallel to the $x$-axis, but that certainly is not the case in general.  So, how do you get the two equalities in (*)? Furthermore, what is the significance in $f'(z_0) \ne 0$?  In the next question, we are asked to show that with $f(z) = z^2$, the level curves $u(x, y) = x^2-y^2=0$ and $v(x,y)= 2xy = 0$ are not orthogonal.  But a straight-forward computation $\nabla u · \nabla v = u_xv_x + u_yv_y = 4xy - 4xy = 0$, a constant zero.  What did I gloss over?","['multivariable-calculus', 'complex-analysis']"
851781,Expected Value of a Determinant,"Suppose that I construct an $n \times n$ matrix $A$ such that each entry of $A$ is a random integer in the range $[1, \, n]$. I'd like to calculate the expected value of $\det(A)$. My conjecture is that the answer is zero, though I could very well be incorrect. Running some numerical experiments with different values for $n$ and a large number of trials, it seems that $\mathbb{E}[\det(A)]$ is normally in the range $[0.25, \, 0.7]$, so I'm starting to lose faith in my intuition that it is zero. Could anyone lend some advice on how to approach this problem and what strategies I may want to consider applying?","['matrices', 'linear-algebra', 'probability', 'determinant']"
851786,Clarification on tetration,"So far when I looked at tetration I noticed it had a recursive relation. It's $t_2=2^{(t_1)}.$ For example if we start at point $(0,1)$, we can take the x-value of $0$, and $2^0=1$, then we take $1$ and get $2^1=2$, and so on, for $^{x} 2$. $$\begin{align} & (0,1)\\
& (1,2) \\
& (2,4) \\ 
& (4,16) \\
& (16,65536) \\ \end{align} $$ If you take this relation you basically get all the integers for $^{x} 2$. If it is all raised by $2$. We can find $1/2$ between each integer by using $x^{x^{(t_1)}}=(t_2) $, which will give the intermediate values. However, unlike exponents, and addition, there is no similarity to the ""x's"" between each integer. For example, if we take the function $2^x$, and take $1.5$, you would get $2\sqrt{2}$, and if you multiply by $\sqrt{2}$ you get $4$. For each $1/2$ between each integer the factor was by $\sqrt{2}$. However the (x's) for tetrations are different. Also if you divide by $1/4^{th}$  you would $x^{x^{x^{x^{(t_1)}}}}=2^{(t_1)} $. However if you take $x^{x^{(2^{(t_1)})}} $, it's not equal to the half intervals. Also if you take the inverse of $2^{(t_1)}$ you would get $\log_2(t_1)$ , so lets take $\log_2(t_1)=1 $, we get $0$, but if we take $\log_2(t_1)= 0$ , it would be undefined, (some would argue it's $-\infty$). So what are all the requirements for a real continuous tetration function? What are the problems with Kneser's method, if any? Do you personally think there is a solution to a ""tetration function""?","['complex-analysis', 'tetration', 'real-analysis']"
851789,Why is $\lim_{x \to 0} \frac{\sin(2x)}{8x} = \frac{1}{4}$,"If $\lim_{x \to 0} \sin(2x) = 0$ and $\lim_{x \to 0} 8x = 0$, then isn't $\lim_{x \to 0} \frac{\sin(2x)}{8x} = \frac{0}{0}$?","['calculus', 'limits']"
851791,Covariant Derivative of a vector field - Parallel Vector Field,"I'm having trouble to understand the concept of Covariant Derivative of a vector field. The definition from doCarmo's book states that the Covariant Derivative $(\frac{Dw}{dt})(t), t \in I$ is defined as the orthogonal projection of $\frac{dw}{dt}$ in the tangent plane. Does that mean that if $w_0 \in T_pS$ is a vector in the tangent plane at point $p$, then its covariant derivative $Dw/dt$ is always zero? Since $dw_0/dt$ will be parallel to the normal $N$ at point $p$. Is that correct? If so, then for a vector field to be parallel, then every vector must be in the tangent plane. Is that also correct? Could you explain without using tensors and Riemannian Manifolds? Thank you",['differential-geometry']
851792,How do the dependent sets of a matroid characterize the matroid?,Wikipedia says: The dependent sets of a matroid characterize the matroid completely. The collection of dependent sets has simple properties that may be taken as axioms for a matroid. So I wonder what properties a collection of subsets need to satisfy so that it is the collection of dependent sets for a matroid? Thanks.,"['computer-science', 'matroids', 'discrete-mathematics', 'combinatorics']"
851809,Maximizing Area of Triangle in Circle,"I was playing around with another example that I made up where I am trying to maximize the area of a triangle inscribed in a circle of radius. I want to do the problem using the method of Lagrange Multipliers. Attempt: Consider the circle of radius 2 given by $x^2+y^2=4$ . The function we are trying to maximize is $A(x,y) = \frac{1}{2}xy$ . Let $h(x,y)=4-x^2-y^2$ . Then, $\nabla h(x,y) = [-2x \ \ -2y ]$ and $\nabla A(x,y) = [\frac{y}{2} \ \ \frac{x}{2}]$ . We want, $ [\frac{y}{2} \ \ \frac{x}{2}]= \lambda[-2x \ \ -2y ] $ . So, $[x \ \ y]=[-4\lambda y \ \ -4\lambda x]$ . Hence, $(-4\lambda y)^2+(-4\lambda x)^2 = 4 \Rightarrow 4\lambda^2(y^2+x^2)=1 \Rightarrow x^2+y^2 = \frac{1}{4\lambda^2} \Rightarrow \lambda = \frac{1}{4}$ . It then follows that $[x \ \ y] = [-0.5y \ \ -0.5x]$ . And, $h(-0.5y,y) = 4-\frac{5}{4}y^2 = 0 \Rightarrow [x \ \ y] = [\frac{-4}{\sqrt{5}} \ \ \frac{4}{\sqrt{5}}]$ . But then $A(x,y)<0$ . Question: What am I doing wrong?","['multivariable-calculus', 'lagrange-multiplier']"
851812,"$f(x,y)=(x^2-y^2,2xy)$ is one to one on the set $A$ consisting of all $(x,y)$ with $x>0$. What is the set $f(A)$","Let $f:\mathbb R^2\to\mathbb R^2$ be defined by the equation $$f(x,y)=(x^2-y^2,2xy).$$ Show that $f$ is one to one on the set $A$ consisting of all $(x,y)$ with $x>0$.
 What is the set $f(A)?$","['multivariable-calculus', 'functions', 'real-analysis', 'complex-numbers', 'complex-analysis']"
851833,Completeness of the set of convergent sequences,"It's a problem from the book ""Topology of Metric Spaces"", written by Kumaresan: ""Show that the set $\textbf{c}$ of convergent sequences in the Normed Linear Space of all bounded real sequences under the sup norm $\|\|_\infty$ is complete. Hint: Enough to show that $\textbf{c}$ is closed. If x = $(x_n)$ is a limit point of $\textbf{c}$, it suffices to show that $x$ is Cauchy."" I got a little bit confused here... What I tried to do: I know that to show that a metric space is complete, I have to show that every Cauchy sequence is convergent there. About the hint: I don't understand why it is enought to show that $\textbf{c}$ is closed, and what does it mean to say that $(x_n)$ is a limit point of $\textbf{c}$? Does that mean that there is a squence $(y_k) \in \textbf{c}$ such that $(y_k) \to (x_n)$? If $(y_k) \to (x_n)$, given $\epsilon > 0$, there are $n_0, k_0 \in \mathbb{N}$ such that
$$\|y_k-x_n\|_\infty<\epsilon \forall k,n \geq \max\{n_0, k_0\}=m_0$$ As we have that $(y_n) \in \textbf{c}$, we have that $(y_n) \to y \in \mathbb{R}$. Because of the uniqueness of the limit, $(x_n) \to y$, so $(x_n) \in \textbf{c}$, therefore, $\textbf{c}$ is closed. Now, let $(y_{n_k}) \in \textbf{c}$ be a Cauchy sequence. Well, $(y_{n_k})=(y_{n_1},y_{n_2},\dots,y_{n_j},\dots)$ is a sequence of sequences. Knowing that $(y_{n_k})$ is Cauchy's, we know that, given $\epsilon > 0, \exists j_0 \in \mathbb{N}$ such that $\forall m, p \geq j_0$ $$\|y_{n_m}-y_{n_p}\|_\infty < \epsilon$$ I don't know what to do from here. My idea is to show that, since every term $y_{n_j}$, of the sequence $(y_{n_k})$, belongs to $\textbf{c}$, therefore, every term $y_{n_j}$ is convergent, we have that $(y_{n_k})$ converges for a convergent sequence $(y_n)$. So, since $(y_n)$ is convergent, $(y_n) \in \textbf{c}$, we have that every Cauchy sequence in $\textbf{c}$ is convergent, meaning that $\textbf{c}$ is complete. But I don't know how to do that, and I don't know if I understood the problem correctly, so I don't know if what I did before is correct... I'd be really grateful if someone could give a hint, a solution, a coment about anything I wrote here! :)","['sequences-and-series', 'cauchy-sequences', 'complete-spaces', 'metric-spaces', 'functional-analysis']"
851835,Definite integrals and möbius transformations,"In examples I have seen for solving an infinite integral from $-\infty$ to $\infty$ using contour integration, the real axis becomes part of the contour of integration in the complex plane, and the residue method is used. Posted here is an example of transforming the real line to a unit circle at the complex origin using Möbius transformations. The residue method is used.  I have not seen this method before. Can someone point me to the literature on this method?","['integration', 'contour-integration']"
851838,Kadison's Inequality,"Let $\mathcal{A}$ be a C*-algebra and $\omega$ a positive linear functional. Is there a simple proof for Kadison's inequality:
$$|\omega(A)|^2\leq\|\omega\|\cdot\omega(A^*A)$$","['operator-theory', 'operator-algebras', 'functional-analysis']"
851839,Help solving this related rates problem.,"The question: A car leaves an intersection traveling east. Its position t sec later is given by 
$x = t^2 + t$
ft. At the same time, another car leaves the same intersection heading north, traveling 
$y = t^2 + 5t$ ft
 in t sec. Find the rate at which the distance between the two cars will be changing 5 sec later. (Round your answer to one decimal place.)
So what I have from this is $$x=30,y=50,\frac{dx}{dt}=10,\frac{dy}{dt}=15,z=\sqrt{3,400}$$
I found $\frac{dx}{dt}$and $\frac{dy}{dt}$ by differentiating $x = t^2 + t$ and $y = t^2 + 5t$ and then plugging in 5 for $t$. (Not sure if right) So I have $x^2+y^2=z^2$ so $$2x\frac{dx}{dt}+2y\frac{dy}{dt}=2z\frac{dz}{dt}$$
and after plugging in the values I THINK I know I end up with $$\frac{60(10)+100(15)}{2\sqrt{3,400}}$$ which is around 18.007 but the answer is 18.5 rounded. So where am I going wrong? All help is appreciated!","['implicit-differentiation', 'calculus', 'derivatives']"
851849,Evaluation of $ \lim_{x\rightarrow \infty}\left\{2x-\left(\sqrt[3]{x^3+x^2+1}+\sqrt[3]{x^3-x^2+1}\right)\right\}$,"Evaluate the limit $$
\lim_{x\rightarrow \infty}\left(2x-\left(\sqrt[3]{x^3+x^2+1}+\sqrt[3]{x^3-x^2+1}\right)\right)
$$ My Attempt: To simplify notation, let $A = \left(\sqrt[3]{x^3+x^2+1}\right)$ and $B = \left(\sqrt[3]{x^3-x^2+1}\right)$. Now $$
\begin{align}
2x^2 &= A^3-B^3\\
x &= \sqrt{\frac{A^3-B^3}{2}}
\end{align}
$$ So the limit becomes $$\lim_{x\rightarrow \infty}\left(\sqrt{\frac{A^3-B^3}{2}}-A-B\right)$$ How can I complete the solution from this point?","['calculus', 'limits']"
851850,Differentiability of the Cantor Function,"I know that the Cantor function is differentiable a.e. but I want to prove it without using the theorem about monotonic functions. I have already proved that $f'(x) = 0$ for all $x \in [0,1] \backslash \mathbb{C}$ where $\mathbb{C}$ is the Cantor set. But I'm not sure how to go about proving that if $x \in \mathbb{C}$ then $f$ is not differentiable at $x$. Actually, upon reflection, I think I have already proved differentiability a.e. but I would still like to know how to finish this part. Also, the definition I am using for the function: 
 $$f:[0,1] \to [0,1]$$
Let $x \in [0,1]$ with ternary expansion $0.a_1a_2...$ Let $N$ be the first $n \in \mathbb{N}$ such that $a_n = 1$. If for all $n \in \mathbb{N}$, $a_n \in \{0,2\}$, let $N = \infty$. Now define $b_n = \frac{a_n}{2}$ for all $n < N$ and $b_N = 1$.
Then $$f(x) = \sum_{i=1}^{N} \frac{b_n}{2^n}.$$","['derivatives', 'real-analysis']"
