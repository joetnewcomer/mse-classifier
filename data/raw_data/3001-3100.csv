question_id,title,body,tags
21175,Convergence of the series $\sum\limits_{n=2}^{\infty} \frac{ (-1)^n} { \ln(n) +\cos(n)}$,$$\sum_{n=2}^{\infty} \frac{ (-1)^n} { \ln(n) +\cos(n)}$$,"['sequences-and-series', 'real-analysis']"
21182,What is the simplification of $\frac{\sin^2 x}{(1+ \sin^2 x +\sin^4 x +\sin^6 x + \cdots)}$?,What is the simplification of $$\frac{\sin^2 x}{(1+ \sin^2 x +\sin^4 x +\sin^6 x + \cdots)} \space \text{?}$$,"['trigonometry', 'sequences-and-series']"
21191,"Is the complex exponential function injective, surjective and/or bijective - and why?","I was just reading about the e -function in the complex plane and was trying to understand the differences between the real and the complex case. Part of the problem is that the mapping of a 2-D plane to another 2-D plane is hard to visualize. My question What are the properties of the complex exponential function in terms of being injective, surjective and/or bijective - and how is this different from the real case?
How can you proof these attributes in the real and in the complex case?",['complex-analysis']
21196,Convergence in probability versus convergence in distribution,Suppose $\bar{X}_n$ is the mean of a random sample of size ${n}$ from an exponential distribution with $\lambda$ > 0. Then what does the following statement about convergence mean (how does this converge)? $$\text{exp} \left(-\frac{1}{\bar{X}_n} \right) \xrightarrow{\rm{P}} \text{exp}(-\lambda) $$ More specifically what does the $\rm{P}$ on top of the arrow mean? I understand it's probability but what is the difference between some expression converging to some value in probability versus in distribution?,"['probability-distributions', 'probability']"
21199,Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio?,"In the book Thomas's Calculus (11th edition) it is mentioned (Section 3.8 pg 225) that the derivative $\frac{\textrm{d}y}{\textrm{d}x}$ is not a ratio. Couldn't it be interpreted as a ratio, because according to the formula $\textrm{d}y = f'(x)\textrm{d}x$ we are able to plug in values for $\textrm{d}x$ and calculate a $\textrm{d}y$ (differential). Then if we rearrange we get $\frac{\textrm{d}y}{\textrm{d}x}$ which could be seen as a ratio. I wonder if the author say this because $\mbox{d}x$ is an independent
variable, and $\textrm{d}y$ is a dependent variable, for $\frac{\textrm{d}y}{\textrm{d}x}$ to be a ratio both variables need to be independent.. maybe?","['nonstandard-analysis', 'math-history', 'calculus', 'analysis']"
21203,What makes Torus Special,"For the past couple of days I have been encountering the word ""Torus"" quite often. I would like to know what special properties does the Torus possess that it is studied very much in Mathematics. A recent article on Toral Automorphisms was given out to students by one of our Professors, which i have posted here. http://chandrumath.wordpress.com/2011/01/31/toral-automorphisms/ This is one example which illustrates what properties ""Torus"" possesses. I am looking for more exciting properties of the Torus which makes it ubiquitous in Mathematics. Another question : Are Torus and Doughnuts both same? Or is there any topological difference between them.","['general-topology', 'geometry']"
21243,Knowledge about Graph Spectral Theory and correlation between a graph Weighted Adjacency matrix and its eigenvalues,"I know that this question is some sort of bridge between Informatics and Mathematics, not knowing the best place where to post this question, I opted for this place because of the type of answer I want (which concerns Math more than anything). Consider to have a graph represented by a collection of nodes and connections. The best way to describe the graph is through the adjacency matrix (AM): a matrix that has 0 or 1 if one node is connected to another (we consider non-directed graphs, so we have all bidirectional connections). Does anyone know whether the eigenvalues of the matrix implies something for the graph??? properties, topology....
I ask this question because I've almost finished studying Markov Chains. In a chain, the matrix of transition probabilities P (for discrete time markov chains), or the matrix of transition frequencies Q (for cont-time markov chains), can be evaluated (their eigenvalues) in order to inspect whether the chain is ergodic or not (with a parallelism to Control Theory: eigenvalues in the unitary circle or in the negative half-plane). I am trying to find something similar for graphs. Thank you","['matrices', 'eigenvalues-eigenvectors', 'graph-theory']"
21250,Pasting Together Fibers of a Vector Bundle,"Everyone:
  Please forgive that I do not yet know LaTex, bro, and my English ( I am from UCV in Venezuela). I think I understand  concept of bundles almost well, and that, once a vector bundle with a fiber is known/given, that we can define a new fiber pointwise, in manipulating each of the fibers, e.g., we may change the fiber from being R (over itself) to being R(+)R, or from R to R(x)R ; RxR*, (dual)etc.
    What I  not too clear on, is on how one put together all the new fibers coherently into a bundle, i.e., how one construct new trivializations and transition functions to turn the space with altered fiber into a new bundle. I am particularly interest in the quotient bundle, if someone  knows. I imagine we use initial charts, trivialization to construct the altered bundle, but I don't see fully how, other than I pretty sure we use multilinear algebra and functoriality somehow.   Would be great if someone knew about how to do this for general fiber bundles. Thanks You from Caracas.","['general-topology', 'linear-algebra', 'vector-bundles']"
21256,"Equivalence relation $(a,b) R (c,d) \Leftrightarrow a + d = b + c$","Suppose $A$ is the set composed of all ordered pairs of positive integers. Let $R$ be the relation defined on $A$ where $(a,b) R (c,d)$ means that $a + d = b + c$. (a) Prove that $R$ is an equivalence relation. Here is what I have so far. Is this correct? Reflexive:
$a \sim a$
$\implies$ $a+b=a+b$; $(a,b) R (c,d)$ Symmetric:
if $a \sim b$ then $b \sim a$
$\implies$ if $a+d=b+c$ then $c+b=d+a$ Transitive:
if $a \sim b$ and $b \sim c$ then $a \sim c$
$\implies$ if $a+d=b+c$ and $c+f=d+e$ then $a+d=d+e$ (b) Find $[(1,1)]$. I'm not sure how to approach this.","['relations', 'equivalence-relations', 'discrete-mathematics']"
21257,On Some Properties of HÃ¶lder Continuous Functions,"The function space $H^{\alpha} (\Omega)$ for $0 < \alpha \le 1$,  is the set of functions: $$\{ f \in C^0(\Omega) : \sup_{x \neq y} \dfrac{|f(x) - f(y)|}{|x-y|^{\alpha}} < \infty \}$$ with the metric $d_{H^{\alpha}} = || f - g ||_{H^{\alpha}}$, where 
$$||f||_{H^{\alpha}} = ||f||_{sup} + [f]_{H^{\alpha}} \text{ ,        } [f]_{H^{\alpha}} = \sup_{x \neq y}  \dfrac{|f(x) - f(y)|}{|x-y|^{\alpha}} $$ Now, if $0 < \alpha < \beta \le 1$, then $$[f]_{H^{\alpha}} \le 2 ||f||_{sup}^{1-\frac{\alpha}{\beta}}  [f]_{H^{\beta}}^{\frac{\alpha}{\beta}} \space \forall f \in H^{\beta}$$ And also, there is some constant $M$ so that: $$||f||_{H^{\alpha}} \le M ||f||_{sup}^{1-\frac{\alpha}{\beta}}  ||f||_{H^{\beta}}^{\frac{\alpha}{\beta}} \space \forall f \in H^{\beta}$$ These were some questions on a problem set: I have checked that $d_{H^{\alpha}}$ is a metric, and proved the two properties (in the second I found that $M = 2$ is sufficient). However, rather blindly. It's easy to show from the first that if $0 < \alpha < \beta \le 1$, then $H^{\beta} \subset H^{\alpha}$. What else do these formulas mean? Are they just some useful inequalities, or do they establish some connection between $H^{\beta}$ and $H^{\alpha}$? Thanks.","['holder-spaces', 'functional-analysis', 'real-analysis']"
21263,Sufficient conditions on subsequences for convergence of a sequence,"Given a sequence $a_n$, I know that if I can find a divergent subsequence of $a_n$, or two subsequences of $a_n$ that converge to different values, $a_n$ diverges, since, if I have understood correctly, a sequence $a_n$ converges to a limit $L$ if and only if every subsequence of $a_n$ converges to that value $L$. I've been wondering if this last condition was equivalent to showing that some subsequences converge to $L$, picking the subsequences such that every element of the original sequence is in at least one of the subsequences. Is it? I would guess that the terms ""partition"" or ""covering"" fit this description. Thanks.","['sequences-and-series', 'calculus']"
21292,Morphisms of finite type are stable under base change,"I am trying to prove that morphisms of finite type are stable under base change, but I am having some trouble moving from the case where everything is affine to the general case.  Suppose $f:X \rightarrow Y$ is a morphism of finite type and $Y'$ is a $Y$-scheme.  I want to show that the morphism $g: X \times_Y Y' \rightarrow Y'$ is of finite type.  In the case that $X$, $Y$, and $Y'$ are affine, I understand why this is true.  For the general case, by a lemma in Liu's book, it is enough to show that there is an affine open cover $\{V_i\}_i$ of $Y'$ such that for each $i$, $g^{-1}(V_i)$ is a finite union of affine open subsets $U_{ij}$ such that for each $i$ and $j$, $O_X(U_{ij})$ is a finitely generated algebra over $O_Y(V_i)$. Here is my attempt at proving this. Choose an affine open cover $\{V_i\}_i$ of $Y'$.  Is it true that $g^{-1}(V_i)=X \times_Y V_i$?  I think this should follow from how we constructed the fibered product by gluing.  Since $f:X \rightarrow Y$ is of finite type, we may choose an affine open cover $\{Y_j\}$ of $Y$ such that $f^{-1}(Y_j)$ is covered by a finite number of affine opens $W_{jk}$.  Now, $W_{jk} \times_Y V_i$ are open subschemes that cover $X \times_Y V_i$, but since $Y$ is not necessarily affine, these schemes are not necessarily affine, right?  Furthermore, if we are using the $W_{jk}$ to cover all of $X$, there could be infinitely many of them.  To make the $W_{jk} \times_Y V_i$ affine, we could further cover them with $W_{jk} \times_{Y_k} V_i$, but we are not guaranteed finitely many $Y_k$ either, so while these schemes will be affine, there will not necessarily be finitely many.  I have been having some trouble with these sorts of arguments where one can immediately reduce to the affine case, and some help here would be greatly appreciated.",['algebraic-geometry']
21301,Does zero covariance imply independence of random variables here?,"I have two random variables $X$ and $Y$. Both are distributed according to $N(0,1)$. If their covariance is 0, are they independent? I know that this is not true for other distributions, say the Wikipedia example: $X$ chosen uniformly in $[-1,1]$ and $Y=X^2$.",['probability-theory']
21307,"If $A$ and $B$ are row-stochastic matrices, what can we say about $AB$?","Let $A \in \mathbb{R}^{n \times n}$ be a symmetric row-stochastic matrix (that is, for each row, the sum over the columns is $1$) with all elements positive. Let $B \in \mathbb{R}^{n \times m}$ be a matrix with all elements positive and the sum over columns is $1$. What can we say about the product $A \times B$? What if we continuously apply $B_{t+1} \leftarrow A \times B_t$ where $B_1 = B$?","['stochastic-processes', 'linear-algebra']"
21308,Trig Question - $\arcsin(\sqrt{2}/2)$ and arc trig functions in general,"I know $\arcsin(\sqrt{2}/2)$ is equal to $\pi/4$. However I don't understand why, I've done some searching on google about arc trig functions and I haven't found any webpages that explain it very well. Can any of you help? Thanks!",['trigonometry']
21313,What is the difference between topological and metric spaces?,What is the difference between a topological and a metric space?,"['general-topology', 'metric-spaces']"
21320,"Why is the identity representation of $SL(2,k)$ isomorphic to its dual representation?","By identity representation I mean the representation sending each element of $SL(2,k)$ to itself. Is there a simple way to see this isomorphism? I feel like I am missing something incredibly basic here.","['linear-algebra', 'representation-theory']"
21330,"""Closed"" form for $\sum \frac{1}{n^n}$","Earlier today, I was talking with my friend about some ""cool"" infinite series and the value they converge to like the Basel problem, Madhava-Leibniz formula for $\pi/4, \log 2$ and similar alternating series etc. One series that popped into our discussion was $\sum\limits_{n=1}^{\infty} \frac{1}{n^n}$. Proving the convergence of this series is trivial but finding the value to which converges has defied me so far. Mathematica says this series converges to $\approx 1.29129$. I tried Googling about this series and found very little information about this series (which is actually surprising since the series looks cool enough to arise in some context). We were joking that it should have something to do with $\pi,e,\phi,\gamma$ or at the least it must be a transcendental number :-). My questions are: What does this series converge to? Does this series arise in any context and are there interesting trivia to be known about this series? I am actually slightly puzzled that I have not been able to find much about this series on the Internet. (At least my Google search did not yield any interesting results).",['sequences-and-series']
21339,Showing consistency of an estimator,I have $\log X \sim Exp(\vartheta - 1)$ and I would like to show $$ P \Big [ \Big |\frac{1}{\frac{1}{n} \sum_{i=1}^n \log X_i}Â - (\vartheta - 1) \Big | > \varepsilon \Big ] \rightarrow 0  \hspace{5 mm} \forall \vartheta (n \rightarrow \infty)$$ In the answer to this question it states that all moments of the exponential distribution that are necessary for the strong law of large numbers exist. Therefore $\frac{1}{\frac{1}{n} \sum_{i=1}^n \log X_i}$ converges almost surely towards $\frac{1}{\vartheta - 1}$. Can someone explain to me why the moments need to exist for the law of large numbers? And how this proof works? Many thanks for your help!,"['statistics', 'probability']"
21341,places and primes,what does it means that a place divide a prime on an algebraic number field?,"['arithmetic-geometry', 'algebraic-geometry', 'algebraic-number-theory', 'number-theory']"
21361,integration of a function,"I found this explanation in a journal paper but I could not understand it. Can someone give me an explanation or possibly a proof that: If
$$\frac{\mathrm{d}V(t)}{\mathrm{d}t}=\sqrt{2}\sum_{h=1}^{H}h\omega V_{h}\cos\left(h\omega t+\frac{\pi }{2}\right),$$
then why integration over whole period is:
$$\frac{1}{T}\int_{0}^{T}  \left( \frac{\mathrm{d} V(t)}{\mathrm{d} t}  \right)^{2}dt=\omega \sum_{h=1}^{H}h^{2}V_{h}^{2}.$$ I have problem with the power of $\omega$; my solution returns $\omega^2$, while the power of $\omega$ in answer is one. Here is my solution:
$$\frac{1}{T}\int_{0}^{T}\ \left( \frac{dV}{dt} \right)^{2}dt=\frac{2\omega ^{2}}{T}\int_{0}^{T}\sum_{h=1}^{H}h^{2}V_{h}^{2}\sin^{2}(h\omega t)dt$$
and over whole period:
$$\frac{1}{T}\int_{0}^{T}\sin^{2}(h\omega t)dt=\frac{1}{2}$$
then we will have 
$$\omega ^{2}\sum h^{2}V_{h}^{2} $$
not $$\omega \sum h^{2}V_{h}^{2}$$ Why?","['definite-integrals', 'calculus', 'integration']"
21372,"Finding $\min \{(x + 6), (4  â  x) \}$","Let $ y = \min \{ (x + 6), (4 â x) \}$, then find $y$. How to solve this problem?",['algebra-precalculus']
21380,Proving that there exists a unique f(x) given Area and arc-length of f(x) on a given interval,"I've been suggested to this site by some nice people at mathoverflow.net Before I get started, let me tell you a little about myself. Iâm a fourth year Mechanical Engineering student at the University of Michigan, Dearborn Campus. This said, I am not a mathematician, however I do know a bit about math, and I also enjoy doing math. Please excuse any redundancies I may make and my âlack of rigorâ. The problem Iâm working on now is an interesting one, and it goes as follows: Suppose we have some unknown function of some real variable x, namely f(x), which is âwell-behavedâ on some known, finite interval, {a,b}. What is meant by âwell-behavedâ is that it is at least once differentiable, once integrate-able, smooth, and continuous on {a,b}. Suppose further that we are given some finite area-under-the-curve A, and some finite arc-length S. Suppose finally that we are given f(x=a) and f(x=b). The problem is this: Given the above restraints on f(x), prove that there always exists some unique function f(x) that âfits the billâ. For example: Say f(x) = Sin(x), a = pi/2, and b = pi. We know that: f(a) = 1, f(b) = 0, A = 1, S = Sqrt[2]*EllipticE[1/2] â 1.9101. The problem in this case would be to prove that f(x) = Sin(x) is the only function that has the known area A, and known arc-length S. Side Note: f(x) in this case could also be Cos(x - pi/2), however, this would not constitute two unique functions, f(x). A corollary question could be posed if a proof to the first problem was given, and is as follows: Given the same criterion for f(x) as before, devise a method of determining what f(x) would have to be to âfit the billâ. That is, like in our previous example, given: A = 1, S = 1.9101; devise a method of determining that f(x) must equal Sin(x). Another Side Note: I have realized (with help from mathoverflow.net) that f(x)=(x-1)^2(x+1) and f(-x) have the same Area and arc-length on [-1,1]. Making a potentially erroneous proposition, let us say that for all polynomials of nth order P(x), P(-x) will have the same Area and arc-length on any [-c,c]. This would prove difficult to prove analytically due to the ""ugly"" expression for arc-length. While in general f(x) and f(-x) are unique functions, there must be some way of thinking about them as similar enough to ""lump"" them into one unique function... Just a thought Thank you all. I look forward to any insight you can shed on the problem.","['calculus', 'integration']"
21393,Number of equivalence relations on a set,If a set has $n$ elements then what are maximum number of equivalence classes and equivalence relations possible on it?,"['discrete-mathematics', 'algebra-precalculus', 'elementary-set-theory', 'combinatorics']"
21400,"Cardinality of all lines on $\mathbb R^{2}$ which do not contain point $(x,y)\in l$ where $x, y \in \mathbb Q$","Problem: What is the cardinality of all lines $l$ on $\mathbb R^{2}$ which do not contain a point $(x,y)\in l$ where $x, y \in \mathbb Q$ (call it $A$). My solution: I was thinking of using CB theorem for this problem. It's easy to show that the cardinality of all lines in $\mathbb R^{2}$ is $2^{\aleph_0}$, so it's obvious that  $|A|\le 2^{\aleph_0}$, but I'm having trouble of showing that the opposite direction ($|A|\ge 2^{\aleph_0}$). I thought about this injective function ($f:\mathbb R \rightarrow A$) $\forall r \in \mathbb R$ 
$f(r)=\left\{\begin{matrix}
(r,0), r \in \mathbb R-\mathbb Q & \\
(g(r),0), r \in \mathbb Q& 
\end{matrix}\right.
$ where $g(r) = min{(x\in\mathbb R-\mathbb Q, x \lt r)}$ Is that injective correct? Thanks!",['elementary-set-theory']
21401,Find valuations of $f(x)=x^4+5x^3+1$,"consider $f(x)=x^4+5x^3+1$. Let $\alpha$ be a root of this polynomial and consider $K=\mathbb{Q}(\alpha)$. Which are the absolute values on $K$ extending the usual absolute values on $\mathbb{Q}$? Are these finitely many? In general, if $K$ is a number field how to find them?
Thanks","['arithmetic-geometry', 'algebraic-geometry', 'number-theory']"
21409,Proof that $\frac{S_n}{n}$ converges almost surely to $\mu$,"I'm trying to show that given $(X_i)$ i.i.d., $E[X_i^2] < \infty$, $\mu := E[X_i]$ then $P\Big [  \lim_{n \rightarrow \infty} \frac{S_n}{n} = \mu \Big ] = 1$ where $ S_n := \sum_{k=1}^n X_k$. So far, I have rewritten  $P\Big [  \lim_{n \rightarrow \infty} \frac{S_n}{n} = \mu \Big ]$ as $$ \lim_{k \rightarrow \infty} P \Big [ \cap_{n \geq k} \{ \omega \Big | |\frac{S_n}{n} - \mu| < \varepsilon \}  \Big ]$$ But I'm not sure how to proceed from here. I have
$$ \sum_{n} P\Big [ |X_n - X|Â  > \varepsilon \Big] < \infty  \Rightarrow P\Big [ \lim_n X_n = X \Big ] = 1$$ which I think I should apply but I don't see how. Can anyone help me with this? Also, I don't see where $E[X_i^2] < \infty$ comes in. Many thanks for your help!",['probability']
21411,Injective maps from $B^A$ to $(C^B)^{C^A}$,"Let $A$, $B$, and $C$ be nonempty sets. I need to find conditions to guarantee that there is an injective function from $B^A$, the set of all functions from $A$ to $B$, to $\displaystyle (C^B)^{C^A}$. Afterwards, I need to show this kind of a function and to prove that it is injective one. I tried to do that with a power equation.
Any suggestions? Thanks
(and if you may try to write it in simple as you can due to my math-English barriers :) ) -Nir","['elementary-set-theory', 'functions']"
21429,Linear transformations and norm,"I am studying Linear Algebra II, and I came across several questions in which, for a certain linear transformation ($T\colon\mathbf{V}\to\mathbf{V}$) I was told that:
$$||T(a)|| \leq ||a||.$$ I am not completely certain how to use this information. For instance, consider the following question (please forgive my translation, it's the first time I write math in English): For a linear transformation $T\colon\mathbf{V}\to\mathbf{V}$in a unitary space [i.e., complex inner product space], such that $|c|=1$ for every eigenvalue $c$ of $T$; $||T(a)|| \leq ||a||$ for every vector $a$ in $\mathbf{V}$;
  prove that T is a unitary operator. How does the fact that $||T(a)|| \leq ||a||$ help me? Thanks.",['linear-algebra']
21437,Upper bound for zeros of holomorphic function,"I'd appreciate some help with the following problem form Conway's book on functions of one complex variable: Let $f$ be analytic in $\overline B (0;R)$ with $|f(z)|\le M$ for $|z|\le R$ and $|f(0)|=a>0$. Show that the number of zeros of f in $B(0;R/3)$ is less than or equal to $$\frac{1}{\log(2)}\log\left(\frac M a\right)$$ I know that the number of zeros is given by 
$$n = \frac 1 {2\pi i}\int_{|z|=R/3} \frac{f'}{f} \, dz
$$ And there is a hint to look at $g(z) = f(z) \prod_{k=1}^n (1-z/z_k)^{-1}$, where the $z_k$ are the zeros of $f$. I have given it some time now, but don't seem to get anywhere. In particular I don't see how the logarithm, $M, a$ come into play.
The problem is in the chapter on the maximum modulus theorem, if that's of any help. Might someone maybe give me a hint? Cheers, S.L.",['complex-analysis']
21440,Defining the Product of Ideals,"Where does the ""naive definition"" of the product of two ideals $I$ and $J$ , $IJ = \{ ij \mid i \in I, j \in J \}$ fall apart? (The product of two ideals $I$ and $J$ is defined to be $IJ := \sum_i a_ib_i$ , where each $a_i$ is in $I$ and each $b_i$ is in $J$ .) Note: This is a question from a problem set I'm working on. I am not expecting a full solution for the answers, but I'd like some help knowing where to look. It seems to work fine in $\mathbb{Z}$ and $\mathbb{C}[x]$ .","['ring-theory', 'abstract-algebra']"
21449,How to evaluate the following limit $\lim_{x\rightarrow \infty} \frac{x^x - (x-1)^x}{x^x}$?,title says everything. How do I evaluate the limit given ?,['limits']
21454,Prove that the eigenvalues of a block matrix are the combined eigenvalues of its blocks,"Let $A$ be a block upper triangular matrix: $$A = \begin{pmatrix} A_{1,1}&A_{1,2}\\ 0&A_{2,2} \end{pmatrix}$$ where $A_{1,1} â C^{p \times p}$ , $A_{2,2} â C^{(n-p) \times (n-p)}$ . Show that the eigenvalues of $A$ are the combined eigenvalues of $A_{1,1}$ and $A_{2,2}$ I've been pretty much stuck looking at this for a good hour and a half, so any help would be much appreciated. Thanks.","['matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
21459,Bourbaki exercise on connected sets,"This is Exercise I.11.4 of Bourbaki's General Topology . Let $X$ be a connected space. a) Let $A$ be a connected subset of $X$, $B$ a subset of $\complement A$ which is both open and closed in $\complement A$. Show that $A\cup B$ is connected. b) Let $A$ be a connected subset of $X$ and $B$ a component of the set $\complement A$. Show that $\complement B$ is connected (use a)). I have managed to show a). My attempts to show b): Let $C$ be a nonempty clopen subset of $\complement B$. By a), $B\cup C$ is connected. Since $B$ is a component of $\complement A$, $B\cup C$ cannot be a subset of $\complement A$. So $C$ has to contain an element of $A$. Since $A$ is connected, $A\subset C$. But I'm stuck here. I can't see how this implies $C=\complement B$. Can someone point me in the right direction?","['general-topology', 'connectedness']"
21460,How to show that $L^p$ spaces are nested?,"Suppose $1<p_1<p_2<\infty$ , then show $L^{p_1}[a,b] \supset L^{p_2}[a,b]$ . I was able to show $\|f\|_{p_1} \le \|f\|_{p_2} (b-a)^{1/p_1 - 1/p_2}$ but I'm not sure how to proceed from here.",['measure-theory']
21463,Determine which of the following mappings F are linear,"I'm having a really hard time understanding how to figure out if a mapping is linear or not. Here is my homework question: Determine which of the following mappings F are linear. (a) $F: \mathbb{R}^3 \to \mathbb{R}^2$ defined by $F(x,y,z) = (x, z)$ (b) $F: \mathbb{R}^4 \to \mathbb{R}^4$ defined by $F(X) = -X$ (c) $F: \mathbb{R}^3 \to \mathbb{R}^3$ defined by $F(X) = X + (0, -1, 0)$ Sorry about my formatting. I'm not sure how to write exponents and the arrow showing that the mapping is from R^n to R^m. Any help is greatly appreciated!!",['linear-algebra']
21475,What are some examples of classes that are not sets?,"After reading about Russell's paradox, I see that the set of all sets does not exist, so instead it is called a class. What other commonly known classes exist that are not sets? I know the class of all singleton sets is not a set, because you can unite that class to get the class of all sets. This seems like the class of all sets of a fixed finite size is not a set either, correct? What are some examples of other 'things' that can't be gathered up and put into a set?",['elementary-set-theory']
21483,"Is the book wrong about this left-hand limit with absolute value? (But, my delta depends on x.)","The book says that $$\lim_{x \rightarrow 0^{-}} \left( \frac{1}{x} - \frac{1}{|x|} \right) \mbox{does not exist}$$ But, given any $M \lt 0$ of large magnitude, if I choose $\delta = \frac{-x^{2}M}{2}$ then any value of x where $|x-0|< \delta$ and $x <0$ (as we are coming from the left) will lead to $\left( \frac{1}{x} - \frac{1}{|x|} \right) < M$. To me, that says that my text book is incorrect in saying that this limit ""d.n.e."" I'm a little bothered that my $\delta$ depends on $x$, but I tried a few numerical examples and it worked fine. Perhaps the function is not uniformly continuous when $x \lt 0$? I have not done enough work to answer that question yet. Maybe the book meant to say $$\lim_{x \rightarrow 0} \left( \frac{1}{x} - \frac{1}{|x|} \right) \mbox{does not exist?}$$
Or maybe I have missed something elementary.","['absolute-value', 'calculus', 'limits']"
21484,solving an initial value ODE with a twist,"The goal of this problem is to solve the initial value problem $$y' = -f(x)y;\quad  y(0) = 1;$$ where
$$f(x)=
\begin{cases}
1,&\text{if }x\leq 2\\
\frac{3}{2},& \text {if } x>2.
\end{cases}
$$
Since $f$ is discontinuous, it is necessary to solve the above ODE separately in each of
the intervals where $f$ is continuous. (a) Determine the intervals where $f$ is continuous. (b) Solve the equation in each of these intervals. Note that each of the solutions
obtained will have a different constant of integration. (c) Match the solutions at the points where $f$ is discontinuous, in order to make the solution $y$ continuous on $\Bbb R$. Note that in this case it is impossible to make $y'$ continuous at the points where $f$ is discontinuous. I just don't understand what part (c) is asking, if anyone could help me with the concepts I would really appreciate it.",['ordinary-differential-equations']
21490,"For a polygon on complex plane, when are the vertex 'Fourier coefficients' non-zero","Consider an $n$-sided convex polygon $P$ that contains the origin in the complex plane. Let the $j$-th vertex be denoted $z_j = r_j e^{i\theta_j}$ ($0 \leq \theta_j < 2 \pi$) for $j= 1 \dots n$. I'm interested in non-zero values of $$ a_k(P)= \sum_{j=1}^{n} \frac{z_{j}^{k}}{|z_{j}|^{k-1}}=\sum_{j=1}^{n} r_j e^{ik\theta_j} \textrm{ for } k \geq 1.$$ Lemma: Given a integer $m \geq 2$, if, for every $k \geq 1$ where $m$ does not divide $k$, $a_k(P)=0$, then the polygon $P$ is $m$-fold rotationally symmetric, that is, a rotation of $e^{i\frac{2 \pi}{m}}$ rotates the polygon into itself. Pseudo-Proof: Re-imagine the $n$-sided polygon $P$ as a $2 \pi$-periodic function $f(\theta)$ of the angle $\theta$ where each vertex $z_j$ is represented as a Dirac delta function at $\theta_j$ with integral $r_j$, that is,
$$f(\theta)=\sum_{j=1}^{n} r_j \delta (\theta - \theta_j).$$ The calculation $a_k(P)$ is then just $2 \pi$ times the $k$-th Fourier coefficient for $f(\theta)$. If, for all $k$ where $m$ does not divide $k$, $a_k(P)=0$, then the corresponding Fourier coefficients of $f(\theta)$ are all zero, implying that $f(\theta)$ is $\frac{2 \pi}{m}$-periodic. Hence the polygon will be $m$-fold rotationally symmetric. $\square$ Firstly, is there a good way to prove this lemma without resorting to non-converging Fourier series? Then, in the same vein, the lemma implies that, if $P$ is not rotationally symmetric, then for every $m$, there are values of $k$, that are not multiples of $m$ for which $a_k(P) \neq 0$. But I believe much more is true, namely, that for 'almost' all $k$, $a_k(P) \neq 0$. In particular, if $k$ is the smallest so that $a_k(P) \neq 0$, I'd like to show that there is a $k'$ relatively prime to $k$ so that $a_{k'}(P) \neq 0$, but I'm not sure how to approach the issue. Any thoughts?","['fourier-series', 'geometry', 'complex-numbers']"
21491,"If a matrix is invertible, is its multiplication commutative?","The question is prompted by change of basis problems -- the book keeps multiplying the bases by matrix $S$ from the left in order to keep subscripts nice and obviously matching, but in examples bases are multiplied by $S$ (the change of basis matrix) from whatever side. So is matrix multiplication commutative if at least one matrix is invertible?",['linear-algebra']
21504,What's the probability of the max segment is equal to k?,"There are $m$ integer points on a line of length $m-1$ (including the end point of the line). We randomly mark $n$ points of the $m$ points ($n\leq m$), dividing the line into several segments, so each segment have an integer length. What is the probability the max segment's length is equal to $k$?",['probability']
21514,Linear Algebra exercises available online?,Is there a good source of undergraduates linear algebra problems available on line? Do you have any better idea of how to resharpen my linear algebra skills?,"['online-resources', 'linear-algebra']"
21516,"Is there a closed form for $\int x^n e^{cx}\,\mathrm dx$?","Wikipedia gives this evaluation: $$ \int x^ne^{cx}\,\mathrm dx=\frac1cx^ne^{cx}-\frac nc\int x^{n-1}e^{cx}\,\mathrm dx=\left(\frac{\partial}{\partial c}\right)^n\frac{e^{cx}}{c}$$ But I have no idea how I should exactly understand the partial part: $\left(\frac{\partial}{\partial c}\right)\frac{e^{cx}}{c}$ EDIT Thanks for your responses so far. I should add that $n$ is not necessarily an integer. Can be for example $n = 1.2$.
I'll see how far I get on learning about fractional derivatives.",['integration']
21519,power of a sets,"I need to figure out what is the power of the group of functions from R to R that for each x that is not from Q, it's f(x) belongs to {x,x+1} I can make a mapping for each function to be Q-->R * (R/Q)--> {x,x+1}
Does it ok? Thanks","['exponentiation', 'functions']"
21528,"one more question with cardinality: $(\{1,2,3\}^{\mathbb{N}} - \{1,2\}^{\mathbb{N}})\cap\mathcal{P}(\mathbb{N}\times\mathbb{N}).$","How can I calculate the cardinality of
$$\left(\{1,2,3\}^{\mathbb{N}} - \{1,2\}^{\mathbb{N}}\right)\cap\mathcal{P}(\mathbb{N}\times\mathbb{N}).$$
where $A^B$ is the set of all functions $f\colon B\to A$. thanks","['cardinals', 'elementary-set-theory']"
21533,shortcut for finding a inverse  of matrix,"I need tricks or shortcuts to find the inverse of  $2 \times 2$ and $3 \times 3$ matrices. I have to take a time-based exam, in which I have to find the inverse of square matrices.","['matrices', 'linear-algebra']"
21552,$\omega$ - space of all sequences with FrÃ©chet metric,"I'm working on to prove the following:
Show that the convergence in the space $\omega$ (space of all sequences with respect to the FrÃ©chet metric) is the coordinate convergence.
Any hint is appreciated, thanks.","['topological-vector-spaces', 'functional-analysis']"
21570,Projective but not free (exercise from Adkins - Weintraub),"This is exercise 38 from Chapter 3 (Modules and Vector Spaces) in Algebra by Adkins and Weintraub (GTM). How do you solve this problem? Let 
\begin{equation*}
R = \lbrace f : [0, 1] \to \Re : f \;\text{ is continuous and} \; f (0) = f (1) \rbrace  
\end{equation*}
and let
\begin{equation*}
M = \lbrace f : [0, 1]\to \Re : f \;\text{is continuous and} \; f (0) = - f (1) \rbrace.
\end{equation*}
Then $R$ is a ring under addition and multiplication of functions, and $M$ is an $R$-module. Show that $M$ is a projective $R$-module that is not free.","['modules', 'projective-module', 'abstract-algebra']"
21579,Powers of the Laplacian matrix,"Given a graph with an adjacency matrix $\bf A$, powers of this matrix give the number of walks from vertices. That is, $({\bf A}^k)_{ij}$ gives the number of walks from nodes $i$ to $j$ in $k$ steps. Is there any nice physical iterpertation for the powers of the Laplacian matrix?","['graph-theory', 'linear-algebra']"
21581,How does one actually show from associativity that one can drop parentheses?,"I've always heard this reasoning, and it makes obvious sense, but how do you actually show it for some arbitrary product? Would it be something like this? $$(a(b(cd)))e=((ab)(cd))e=(((ab)c)d)e=abcde?$$ Do you just say that the grouping of the parentheses now corresponds to just multiplying straight through? Thanks.","['induction', 'algebra-precalculus', 'abstract-algebra']"
21589,Evaluate $\int \cos^3 x\;\sin^2 xdx$,"Is this correct? I thought it would be but when I entered it into wolfram alpha, I got a different answer. $$\int (\cos^3x)(\sin^2x)dx  = \int(\cos x)(\cos^2x)(\sin^2x)dx

= \int (\cos x)(1-\sin^2x)(\sin^2x)dx.$$ let $u = \sin x$, $du = \cos xdx$ $$\int(1-u^2)u^2du = \int(u^2-u^4)du

= \frac{u^3}{3} - \frac{u^5}{5} +C$$ Plugging in back $u$, we get $\displaystyle\frac{\sin^3 x}{3} - \frac{\sin^5 x}{5}$ + C","['calculus', 'integration']"
21593,Strange use of differentials - is $d{\bf x} \cdot d{\bf x}$ a dot product?,"If ${\bf x}(s)$ is a curve in $\mathbb{R}^3$ on a surface parameterized by its arc length $s$, and ${\bf N}$ is the surface normal at ${\bf x}$, consider the following equality (with ""$\cdot$"" being the dot product): \begin{align*}
\frac{d{\bf x}}{ds} \cdot \frac{d{\bf N}}{ds} = \frac{d{\bf x} \cdot d{\bf N}}{d{\bf x} \cdot d{\bf x}}
\end{align*} This comes from a book on differential geometry. Now, the LHS is a perfectly fine dot product between two vectors. But to me, the RHS does not make any sense at all. I do understand that this result can easily be arrived at if you treat $d{\bf x}$ and $d{\bf N}$ as real vectors (e.g. the part where $d{\bf x} \cdot d{\bf x}=ds^2$). But this kind of manipulation seems rather sloppy... I would really like to understand this from first principles. A derivative like $\frac{d{\bf x}}{ds}$ is defined in terms of limits, \begin{align*}
\frac{d{\bf x}}{ds}(s) = \lim_{\Delta s \rightarrow \infty} \frac{{\bf x}(s + \Delta s) - {\bf x}(s)}{\Delta s}
\end{align*} The extension from scalar $x$ to a vector-valued $\bf x$ is obvious. But how does this definition apply to the RHS of the equation given above? Does a dot product of differentials like $d{\bf x} \cdot d{\bf x}$ even mean anything? How do you set up the limits so that the expression can be evaluated (after all, the RHS should evaluate to a real number)?",['differential-geometry']
21596,What does it mean for a sequence of self-homeomorphism of $\mathbb{R}^n$ to converge to a point?,"Let $\{f_j\}$ be a family of self-homeomorphisms of $\overline{\mathbb{R}}^n$ and $x,y\in\overline{\mathbb{R}}^n$, where $\overline{\mathbb{R}}^n$ is the one-point compactification of $\mathbb{R}^n$.  What does it mean for $\{f_j\}$ to converge c-uniformly to y in $\overline{\mathbb{R}}^n\setminus \{x\}$? That is, what does ""$\lim_{j\rightarrow\infty} f_j =y$, c-uniformly in $\overline{\mathbb{R}}^n\setminus {x}$"" mean?  I'm only confused about what function the point $y$ represents.  I would have assumed it meant the function that sends everything to the point $y$, but this is not a bijection on $\overline{\mathbb{R}}^n$. The source for this question is the paper ""Discrete Quasiconformal Groups I"" by Gehring and Martin, Proc. London Math. Soc. (3) 55, 1987.  Definition 3.3 and Theorem 3.7.  My interest here is in understanding convergence groups, in particular their use in defining relatively hyperbolic groups.","['analytic-geometry', 'group-theory', 'analysis']"
21605,Cardinal arithmetic: $b\ge x>1$ and $b^2=b$ implies $x^b=2^b$,"Let $x$ and $b$ represent cardinals. Assume that  $b\geq x > 1$ and $b^2=b$.
Prove that $x^b=2^b$. Thanks!","['cardinals', 'elementary-set-theory']"
21614,Is there a definition of determinants that does not rely on how they are calculated?,"In the few linear algebra texts I have read, the determinant is introduced in the following manner; âHere is a formula for what we call $\det A$ . Here are some other formulas. And finally, here are some nice properties of the determinant.â For example, in very elementary textbooks it is introduced by giving the co-factor expansion formula. In Axlerâs âLinear Algebra Done Rightâ it is defined, for $T\in L(V)$ to be ${(-1)}^{\dim V}$ times the constant term in the characteristic polynomial of $T$ . However I find this somewhat unsatisfactory. Itâs like the real definition of the determinant is hidden. Ideally, wouldnât the determinant be defined in the following manner: âGiven a matrix $A$ , let $\det A$ be an element of $\mathbb{F}$ such that $x$ , $y$ and $z$ .â Then one would proceed to prove that this element is unique, and derive the familiar formulae. So my question is: Does a definition of the latter type exist, is there some minimal set of properties sufficient to define what a determinant is? If not, can you explain why?",['linear-algebra']
21618,Intermediate Text in Combinatorics?,"I'm currently attending a somewhat disorganized seminar on combinatorics that follows no textbook. So far we have covered the orbit-stabilizer theorem, some recursion, and we're heading into the MÃ¶bius inversion formula. Can anyone suggest a text that approaches combinatorics at this level for a 2nd-3rd year undergrad who already knows some algebra and the more basic combinatorics like combinations, permutations, stars-and-bars, generating functions? Most introductory combinatorics books I've found are more suited to a discrete math class and cover stuff which I already know. I'm looking for something to supplement this lecture. Thank you.","['book-recommendation', 'reference-request', 'combinatorics']"
21645,Restricted Compositions,Number Composition studies the number ways of compositing a number. I wanna know the number of compositions of $m$ with $n$ parts with the size of the max part equal to or less than $k$. Is there a closed form for this problem?,['combinatorics']
21655,bound of the number of the primes on an interval of length n,"I made this observation and it seems reasonable to me to ask :if $n$ is a natural number then the number of the primes less than or equal to $n$ is denoted by $Ï(n)$ . is that true that in any interval of length $n$ there are at most $Ï(n)+1$ primes?(the $+1$ is needed for the trivial occasion where $n=p-1$ and the interval of length $n$ is $[2,p]$)
Alternative we can say that in any interval of length $n-1$ there are at most $Ï(n)$ primes.","['prime-numbers', 'number-theory']"
21668,Orthogonal Projection,"Seems like I still don't get it , I think I am missing something important. Let $V$ be an $n$ dimensional inner product space ($n \geq 1$), and $T\colon\mathbf{V}\to\mathbf{V}$ be a linear transformation such that: $T^2 = T$ $||T(a)|| \leq ||a||$ for every vector $a$ in $\mathbf{V}$; Prove that a subspace $U \subseteq V$ exist, such that $T$ is the orthogonal projection on $U$. Now, I know these things: The fact hat $T^2 = T$ guarantees that $T$ is indeed a projection, so I need to prove that T is an orthogonal projection (I guess this is where $||T(a)|| \leq ||a||$ kicks in). To do this I can prove that: For every $v$ in $ImT^{\perp}$, $T(v) = 0$ Alternatively, I can prove that for every $v$ in $ImT$ and $u$ in $KerT$, $(v,u)=0$. $T$ is self-adjoint (according to Wikipedia) The matrix $A = [T]_{E}$ when $E$ is an orthonormal basis, is hermitian (this is equivalent to the previous point). What else? I've been thinking about it for quite some time now, and I'm pretty sure there is something big I'm missing, again . I just don't know how to use the data to prove any of these things. Thanks!","['linear-transformations', 'linear-algebra', 'projection']"
21669,Lattice of Gauss and Eisenstein Integers,"Z is a 1D lattice Gaussian and Eisenstein integers are 2D lattices But the golden integers (for example) are dense on the real line. Are there rings of integers which have 3D, 4D, ... lattices? Here is a plot of $(a + \tfrac{1}{2}(1+\sqrt{5})b,a + \tfrac{1}{2}(1-\sqrt{5})b)$ for $-10\le a,b \le 10$. which is the lattice corresponding to the golden integers, if I understand correctly. The green points represent rational integers and the blue points represent multiples of $\varphi$.","['algebraic-number-theory', 'number-theory']"
21688,A question about the Inclusion-Exclusion principle,"Grandma has 8 grandchildren, and 4 different types of popsicles: 6 Vanilla popsicles 6 Strawberry popsicles 5 Banana popsicles 3 Chokolate popsicles This morning, all of her grandchildren came together and asked for one popsicle each (every grandchild asked for a particular flavor). What is the total number of different sets of requests that Grandma can fulfill? I think this is related to the Inclusion-Exclusion principle because it was taught in the same class. Can you help me solve it? I did reach the following sum, but I imagine the question's author had something simpler in mind... $ E(0) = W(0)-W(1) = 3^8 - 4\cdot C(8,8)-4\cdot 3\cdot C(7, 8)-2\cdot C(6, 8)\cdot 3^2-C(5, 8)\cdot 3^3 - C(4, 8)\cdot 3^4$","['inclusion-exclusion', 'combinatorics']"
21689,Angular Velocity around a given axis,"Consider the problem of a point p rotating around a given axis.
The axis of rotation is Ï , |Ï| = 1 , and q is a point on the axis.
Assuming that the point rotates around the axis with unit angular velocity, then the velocity of the point p(t) is p'(t) = Ï x ( p(t) - q ) I'm looking for a more detailed explanation for this equation, as it is not the same with the classic equation of angular velocity. I'm confused with the term p(t) - q in the right hand side. Thanks",['multivariable-calculus']
21690,How can I systematically find a solution to this problem?,"While doing some self-study, a friend posed this problem to me: Let $a$ be a sequence, defined as following: $$a_0 = 0,\quad a_1= 1, \qquad a_{n+2}=\frac{a_n+a_{n+1}}2$$ Figure out, whether $a$ converges, and if yes to which value. Find a closed form for $a_n$ . If you want to, you may also have a look at the common case with $a_0= \alpha$ and $a_1 = \beta$ . It's easy to see, that this converges to $\frac2 3$ , and the other parts are also easy to solve with quessing a result and afterwards proving it correct, but I dislike this approach, as it is not productive if the solution isn't obvious. Is there a systematically approach for such problems? I would be really happy if somebody can explain me, how to systematically solve problems like this.",['sequences-and-series']
21692,Does monotonicity and derivability of a function $f\colon \mathbb{R}\to\mathbb{R}$ imply bijectiveness?,"I have to prove that $f \colon x \mapsto e^{4x} + x^5 + 2$ ($f\colon \mathbb{R}\to\mathbb{R}$) is bijective. The argument given in the solution is that since the first two summands of the image is a bijective function of $x$, then so is $f$. Nonetheless, this ""proof"" doesn't seem at all rigorous to me, since there are many counterexamples to this argument. So I proved that $f$ is strictly increasing by looking at its derivative, thus injective, and that every $c \in \mathbb{R}$ has at least one preimage, applying Bolzano's theorem to $f(x) - c$ and evaluating its limit at $-\infty$ and $+\infty$, and so, demonstrating $f$ to be surjective. In consequence, $f$ is bijective. I am much more pleased about this proof than the one given in the solutions, but I want to know if I missed something, or if my hypothesis are insufficient. I gave an example to illustrate my argument, but the question I want to ask in the general form is in the title. Also, I would like to know whether the converse holds as well. Thanks. Update: I've been thinking about this, and realized that only monotonicity and continuity (together with unboundedness, as Mariano pointed out) are necessary for bijectiveness, and derivability only helps to prove monotonicity. Is this correct?",['calculus']
21694,"A complete guide to solving questions of the form ""how many integer solutions does this sum have""?","In combinatorics, there are several different of the form ""How many integer solutions does the equation $\Sigma_{1\le i\le k}X_i=n$ have?"" Some variations include: $X_i\in\{0, 1\}$ $0\le X_i$ $1\le X_i$ $0\le X_i\le m$ $m_1\le X_i \le m_2$ ... Is there a complete guide to solving these kinds of questions?",['combinatorics']
21696,Local complete intersections and the cotangent module,"Let $A$ be a (commutative) noetherian ring, and let $I \subseteq A$ be an ideal. It is not hard to show that if $I$ is generated by a length $n$ regular sequence, then the $A/I$-module $I/I^2$ is a free module of rank $n$. Is the converse true? if we know that $I/I^2$ is free, does it follows that $I$ is generated by a regular sequence? Thanks!","['commutative-algebra', 'algebraic-geometry']"
21730,Weekend birthdays,"My birthday this year (2011) is on a Friday. In most years, one's birthday the following year is on the subsequent day of the week, and in that pattern, my birthday next year (2012) it is on a Saturday. However, due to 2012 being a leap year, my birthday in 2013 will be on Monday - I miss out on having a Sunday birthday. I quite like having weekend birthdays, so that disappoints me a bit. However it leads me to a question: Over a reasonable-length life-time, does any given person's number of weekend birthdays even out to an average, or are some people significantly more blessed than others? If some people do have an advantage in this respect, is there any way mathematical way to work out whether being born on any given day of any given year will confer an advantage? And further, what are the probabilities of being 'lucky' in your number of weekend birthdays. My guess is that for any given year, on each side of Feb 29th, birthdays on any given day of the week throughout the year will have the same score. To make it a bit easier, lets say for a 'reasonable length life-time', we mean a fixed length of 80 years (though I for one intend to beat that!). That said, I would also be interested in how the maths change as we vary the life-span.","['calendar-computations', 'combinatorics']"
21736,Explain why a set of mutually orthogonal non-zero vectors is linearly independent given a clause,"""Given $\vec{u}_1,\ldots ,\vec{u}_n$ mutually orthogonal non-zero vectors, explain why for $\vec{v}=c_1\vec{u}_1+\ldots +c_n\vec{u}_n$ $c_k=\frac{\vec{v} \cdot \vec{u}_k}{\vec{u}_k \cdot \vec{u}_k}$"" This I explained by dotting both sides with $\vec{u}_k$ and simplifying everything. However, the question I now have is, how using this achieved result, can I show that $\vec{u}_1,\ldots ,\vec{u}_n$ are linearly independent? I was thinking of saying that in accordance to $c_k=\frac{\vec{v} \cdot \vec{u}_k}{\vec{u}_k \cdot \vec{u}_k}$, every coefficient can be of only one fixed value, so there is no room for changing one at the expense of another (as one could with coefficients of linearly dependent vectors), but I am not sure if this is right, and whether I am phrasing this correctly. Thanks for your help!",['linear-algebra']
21752,Why is the 'change-of-basis matrix' called such?,"""Let $P$ be the change-of-basis matrix
  from a basis $S$ to a basis $S'$ in a
  vector space $V$. Then, for any vector
  $v \in V$, we have $$P[v]_{S'}=[v]_{S}
 \text{ and hence, }  P^{-1}[v]_{S} =
 [v]_{S'}$$ Namely, if we multiply the coordinates
  of $v$ in the original basis $S$ by
  $P^{-1}$, we get the coordinates of
  $v$ in the new basis $S'$."" - Schaum's
  Outlines: Linear Algebra. 4th Ed. I am having a lot of difficulty keeping these matrices straight. Could someone please help me understand the reasoning behind (what appears to me as) the counter-intuitive naming of $P$ as the change of basis matrix from $S$ to $S'$? It seems like $P^{-1}$ is the matrix which actually changes a coordinate vector in terms of the 'old' basis $S$ to a coordinate vector in terms of the 'new' basis $S'$... Added: ""Consider a basis $S =
 \{u_1,u_2,...,u_n\}$ of a vector space
  $V$ over a field $K$. For any vector
  $v\in V$, suppose $v = a_1u_1
 +a_2u_2+...+a_nu_n$ Then the coordinate vector of $v$
  relative to the basis $S$, which we
  assume to be a column vector (unless
  otherwise stated or implied), is
  denoted and defined by $[v]_S =
 [a_1,a_2,...,a_n]^{T}$. "" ""Let $S = \{ u_1,u_2,...,u_n\}$ be a
  basis of a vector space $V$, and let
  $S'=\{v_1,v_2,...,v_n\}$ be another
  basis. (For reference, we will call
  $S$ the 'old' basis and $S'$ the 'new'
  basis.) Because $S$ is a basis, each
  vector in the 'new' basis $S'$ can be
  written uniquely as a linear
  combination of the vectors in S; say, $\begin{array}{c} v_1 = a_{11}u_1 +
 a_{12}u_2 + \cdots +a_{1n}u_n \\ v_2 =
 a_{21}u_1 + a_{22}u_2 + \cdots
 +a_{2n}u_n \\ \cdots \cdots \cdots \\ v_n = a_{n1}u_1 + a_{n2}u_2 + \cdots
 +a_{nn}u_n \end{array}$ Let $P$ be the transpose of the above
  matrix of coefficients; that is, let
  $P = [p_{ij}]$, where $p_{ij} =
 a_{ij}$. Then $P$ is called the
  \textit{change-of-basis matrix} from
  the 'old' basis $S$ to the 'new' basis
  $S'$."" - Schaum's Outline: Linear Algebra 4th Ed. I am trying to understand the above definitions with this example: Basis vectors of $\mathbb{R}^{2}: S= \{u_1,u_2\}=\{(1,-2),(3,-4)\}$ and $S' = \{v_1,v_2\}= \{(1,3), (3,8)\}$ the change of basis matrix from $S$ to $S'$ is $P = \left( \begin{array}{cc} -\frac{13}{2} & -18 \\ \frac{5}{2} & 7 \end{array} \right)$. My current understanding is the following: normally vectors such as $u_1, u_2$ are written under the assumption of the usual basis that is $u_1 = (1,-2) = e_1 - 2e_2 = [u_1]_E$. So actually $[u_1]_S = (1,0)$ and I guess this would be true in general... But I am not really understanding what effect if any $P$ is supposed to have on the basis vectors themselves (I think I understand the effect on the coordinates relative to a basis). I guess I could calculate a matrix $P'$ which has the effect  $P'u_1, P'u_2,...,P'u_n = v_1, v_2,..., v_n$ but would this be anything?","['matrices', 'linear-algebra', 'change-of-basis']"
21755,Cyclic Extensions of $\mathbb{R}(t)$,"Let $\mathbb{R}(t)$ be the field of rational functions over $\mathbb{R}$ (the fraction field of $\mathbb{R}[x]$). I am looking for elements in the Brauer group of the field, and the current idea I have to follow on is to find infinitely many cyclic field extensions, and use those to create cyclic division algebras. My Galois theory experience is not very rich with transcendental extensions of $\mathbb{R}$ and I'm a bit lost. Am I even on the right path towards the Brauer group? Any ideas on how to prove there are many cyclic extensions?","['galois-theory', 'abstract-algebra', 'field-theory']"
21769,Variation on euler totient/phi function,"Is there any efficient way , to find for a particular n, the cardinality of set consisting of all numbers coprimes to n, but bigger than m(assuming i know the prime factorisation of n and m) I am looking for the implementation which is simple+fast (like the euler totient/phi function, which given the factorisation of n will just need O(logn) steps).",['number-theory']
21780,Why is this not an isomorphism?,"Let $T(f(t))$=
$\begin{bmatrix}
f(0) & f(1)\\ 
f(2) & f(3)
\end{bmatrix}$ from $P_2$ to $\mathbb{R}^{2\times 2}$. To show that it is not an isomorphism, I need to show that either kernel of the transformation is not equal to the zero element only, or that the image is not the whole target space. I am struggling in showing that either of these is false, dealing with polynomials in transformations is very counter-intuitive. Thanks for help!",['linear-algebra']
21785,what is the expected maximum number of balls in the same box,"If I have $m$ distinct boxes, and $n$ distinct balls. I put all of these balls into the boxes with one box possibly containing more than one balls. What is the expected maximum number of balls in one box? Appreciate your thoughts on solving this problem.",['probability']
21792,Norms Induced by Inner Products and the Parallelogram Law,"Let $ V $ be a normed vector space (over $\mathbb{R}$, say, for simplicity) with norm $ \lVert\cdot\rVert$. It's not hard to show that if $\lVert \cdot \rVert = \sqrt{\langle \cdot, \cdot \rangle}$ for some (real) inner product $\langle \cdot, \cdot \rangle$, then the parallelogram equality
$$ 2\lVert u\rVert^2 + 2\lVert v\rVert^2 = \lVert u + v\rVert^2 + \lVert u - v\rVert^2 $$
holds for all pairs $u, v \in V$. I'm having difficulty with the converse.  Assuming the parallelogram identity, I'm able to convince myself that the inner product should be
$$ \langle u, v \rangle = \frac{\lVert u\rVert^2 + \lVert v\rVert^2 - \lVert u - v\rVert^2}{2} = \frac{\lVert u + v\rVert^2 - \lVert u\rVert^2 - \lVert v\rVert^2}{2} = \frac{\lVert u + v\rVert^2 - \lVert u - v\rVert^2}{4} $$ I cannot seem to get that $\langle \lambda u,v \rangle = \lambda \langle u,v \rangle$ for $\lambda \in \mathbb{R}$.  How would one go about proving this?","['normed-spaces', 'linear-algebra', 'inner-products', 'functional-analysis']"
21796,"Finding $\int_0^{\pi/2} \sin x\,dx$","I'm interested in why $$\int_0^{\pi/2} \sin x\,dx = 1.$$ I know how to do the integral the conventional way but am more interested in what makes radians special for this problem. If we instead compute $$\int_{0}^{90} \sin x^\circ\,dx,$$ we won't get $1$ as the answer. What about the definition of radians makes this integral evaluate to $1$? I'm looking for an intuitive (presumably geometric) explanation.","['definite-integrals', 'trigonometry', 'calculus', 'integration']"
21803,How to find a basis for this sub-space?,"I am given a subspace of all polynomials $f(t)$ in $\mathbf{P}_2$ such that $f(1)=0$. I know that a basis for this space is $1-t$, $1-t^{2}$, and when I look at it, it makes perfect sense as to why. I was just wondering what is a systematic way of finding it, without eyeballing. Thanks!",['linear-algebra']
21807,Find matrices $A$ and $B$ given $AB$ and $BA$,"Given that:
$$AB= \left[ {\matrix{
   3 & 1 \cr 
   2 & 1 \cr 

 } } \right]$$
and
$$BA= \left[ {\matrix{
   5 & 3 \cr 
   -2 & -1 \cr 

 } } \right]$$
find $A$ and $B$.",['matrices']
21812,Can we construct a function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that it has intermediate value property and discontinuous everywhere?,"Can we construct a function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that it has intermediate value property and discontinuous everywhere? I think it is probable because we can consider 
$$  y = 
\begin{cases}
\sin \left( \frac{1}{x} \right), & \text{if } x \neq 0, \\
0, & \text{if } x=0.
\end{cases}
$$
This function has intermediate value property but is discontinuous on $x=0$. Inspired by this example, let $r_n$ denote the rational number,and define 
$$  y = 
\begin{cases}
\sum_{n=1}^{\infty} \frac{1}{2^n} \left| \sin \left( \frac{1}{x-r_n} \right) \right|, & \text{if } x \notin \mathbb{Q}, \\
0, & \mbox{if }x \in \mathbb{Q}.
\end{cases}
$$ It is easy to see this function is discontinuons if $x$ is not a rational number. But I can't verify its intermediate value property.","['examples-counterexamples', 'real-analysis', 'analysis']"
21816,A geometric look at $\frac{1}{a}+\frac{1}{b}=\frac{1}{c}$?,"Is there a geometric way of looking at the relationship between the positive real numbers $a$, $b$ and $c$ if $\frac{1}{a}+\frac{1}{b}=\frac{1}{c}$?","['geometry', 'algebra-precalculus']"
21825,Estimating $\#\{\{\alpha k\} < 1/\sqrt{k} : k \leq n\}$ for irrational $\alpha$,"Suppose $\{\alpha n\}$ is the fractional part of $\alpha n$. Put
$$A_{\alpha}(n) = \#\{\{\alpha k\} < 1/\sqrt{k} : k \leq n\}.$$
If $\alpha$ is irrational, can I find some constant $K$ such that $A_{\alpha}(n) < K \sqrt{n}$ for all $n$? Does the order of $A_{\alpha}(n)$ depend on $\alpha$? Suppose $1/\sqrt{k}$ is replaced by some function $f(k)$. What can I say about the number of $\{\alpha n\}$ less than $f(n)$ as $n$ tends to infinity?","['ergodic-theory', 'diophantine-approximation', 'irrational-numbers', 'analysis']"
21851,"Connecting a $n, n$ point grid","I stumbled across the problem of connecting the points on a $n, n$ grid with a minimal amount of straight lines without lifting the pen. For $n=1, n=2$ it is trivial. For $n=3$ you can find the solution with a bit trial and error (I will leave this to the reader as it is a fun to do, you can do it with 4 lines). I found one possible solution for a $4,4$ grid and animated it, it uses 6 lines and is probably optimal (will hopefully help you to understand the problem better, the path doesn't have to be closed like in the animation, open ends are allowed!): Now my question is, for higher $n$ , is there a way to get the amount of minimal lines to use and does an algorithm exist to find a actual solution? I think its quite hard to model the ""straight lines"" with graph theory. Edit:
Reading Erics excellent answer I found the following website: http://www.mathpuzzle.com/dots.html that also gives an algorithm to connect the points in $2n-2$ steps, solutions up to $10,10$ and mentions: Toshi Kato conjectures: On $(2N+1)x(2N+1)$ grid, $N \geq 2$ , Using $4N$ continuous lines, and not lifting your
pencil from the paper, can go through
all the dots of a $(2N+1)x(2N+1)$ grid,
ending at the same place started. But
must visit at least one dot twice in
the route. On $(2N)x(2N)$ grid, $N \geq 2$ , Using $4N-2$ continuous lines, and not lifting your
pencil from the paper, can go through
all the dots of a $(2N)x(2N)$ grid,
ending at the same place started. And
can visit each dots just once. It seems to be an open problem to show that $2n-2$ is optimal. Also I found the following page with a proof that in the $3,3$ grid there cannot be $2$ parallel lines: http://fahim-patel.blogspot.com/2011/01/proof.html I think it might be interesting for coming up with a proof that $2n-2$ is optimal (however maybe there is no such proof, as we only saw solutions for very small $n$ , for bigger $n$ there might be some developments we don't know about).","['geometry', 'puzzle', 'graph-theory']"
21852,Techniques to compute complex integrals over infinite contours,"In asymptotic analysis and analytic number theory, one often has to deal with complex integrals over infinite contours in the complex plane, and the required techniques to do so often go beyond the standard courses of complex analysis in one variable. In particular, I am interested in the following type of argument which I donÂ´t fully understand and which I will try to illustrate by an example (taken from Paris and Kaminski, ""Asymptotics and Mellin-Barnes Integrals""): Consider the so called Cahen-Mellin integral
$$e^{-z}=\frac{1}{2\pi i}\int_{(c)}\Gamma(s)z^{-s}ds,\ \arg(z)<\frac{\pi}{2}, z\neq 0,$$
which is an integral representation of the exponential function by taking the inverse Mellin transform of the Gamma function, where the integration contour $(c)$ stands for the vertical line $\{\Re(s)=c\}$ with some $c>0$. It can be shown that the integrand has ""the controlling behavior"" $|z|^{-\sigma}O(|t|^{\sigma-{1\over 2}}e^{t\arg(z)-{1\over 2}\pi|t|})$ as $|t|\to\infty$, where $s=\sigma + it$. Now, aside from the obvious way to show the validity of the above integral representation, Paris and Kaminski argue that because of the aforementioned exponential decay of the integrand we are allowed to move the contour of integration over the poles of the Gamma function and use the residues of the latter to obtain the exponential series. This is precisely the argument I want to understand, so I will try to break it down into few smaller questions: (Q1) How does the exponential decay of the integrand allow us to displace the contour of intagration over singularities? Is there a more general setup where the asymptotic behavior of the integrand allows for moving the contour of integration through and over singularities? (Q2) After the displacement of the integration contour (still a vertical line), what kind of a residue theorem allows for considering all of the infinitely many singularities of the gamma function? The version of the residue theorem I know uses bounded interior of a (simply) closed contour in the complex plane and only finitely many residues contained in there. Remark 1: In order to compute the above integral in a classical way, I would take a finite line segments of the vertical line, symmetric with respect to the positive real axis, i.e. $\{Re(s)=c, -r_n\leq\Im(s)\leq r_n\}$, construct circle segments with radii $r_n$ encompassing each of the poles, with $r_n\uparrow \infty$ suitably chosen such that no poles lie on the segment contours, and then show that the integrals over the half-circles tend to zero as $n\to\infty$, thus obtaining on the one hand the integral over the infinite vertical line and on the other hand the infinite sum of the residues. However, I havenÂ´t really checked whether the exponential decay of the integrand would suffice for the half-circle integrals to vanish in the limit. Remark 2: I could imagine that there might be a version of the residue theorem suitably formulated for the Riemann sphere resp. $\bar{\mathbb{C}}:=\mathbb{C}\cup\{\infty\}$, where basically infinite contours from the complex plane correspond to closed ones on the sphere. (Q3) Where could one find a more systematic treatment of integrals over infinite contours in the complex plane, including contour shifting over poles, other types of contour modifications, usage of infinitely many residues, as well as other techniques for the exact computation of such integrals? I understand that such techniques are often to be applied ""individually"", thus such literature would ideally contain a few good examples. Thanks in advance for your attention and sorry if I appear to sound too confused :-), I am only trying to fill in certain ""gaps"" in my knowledge of complex analysis. PS: I am not interested in numerical computations or general asymptotic expansions for contour integrals (even though in the above example the residues ""expansion"" appears as a special case thereof).","['reference-request', 'complex-analysis']"
21864,Solving short trigo equation with sine - need some help!,"From the relation $M=E-\epsilon\cdot\sin(E)$, I need to find the value of E, knowing the two other parameters . How should I go about this? This is part of a computation which will be done quite a number of times per second. I hope there's a quick way to get E out of this equation. Thank you very much, MJ","['trigonometry', 'numerical-methods']"
21868,Curves and Sums-of-Powers Representations,"Jacobi first noticed the connection between the functions that bear his name and counting the representations of sums-of-squares , 
\begin{eqnarray}
\theta_{3}^{n}(q) = \left( \sum_{k \in \mathbb{Z}} q^{k^{2}} \right)^{n} = \sum_{k \geq 0} r_{2,n}(k) \ q^{k},
\end{eqnarray}
where $q = e^{\pi i \tau}$ (or $e^{2 \pi i \tau}$ depending on which text you consult) and $r_{2,n}(k)$ is the number of ways to represent $k$ as a sum of $n$ squares, i.e. , the integral solutions of the equation $k = x_{1}^{2} + \dots + x_{n}^{2}$. The Jacobi theta functions , including $\theta_{3}$, are known to satisfy a myriad of symmetries and identities. Question: Are these identities related in any way to the family of curves $\{ X^{2} + Y^{2} = k \}_{k \in \mathbb{N}}$? If so, how? Suppose, more generally, I'd like to count $m$-powers instead of squares. The corresponding $q$-series is then
\begin{eqnarray}
\left( \sum_{k \in \mathbb{Z}} q^{k^{m}} \right)^{n} =  \sum_{k \geq 0} r_{m,n}(k) \ q^{k},
\end{eqnarray}
If the answer to the first question is yes, then this function (known?) and its symmetries should then be intimately connected to $\{ X_{1}^{m} + X_{2}^{m} + \dots + X_{n}^{m} = k \}_{k \in \mathbb{N}}$, a family of deformations of the homogeneous hypersurface $F = \sum_{i = 1}^{n} X_{i}^{m} = 0$. Generalizing further still, let $F = F(X_1, \dots, X_n)$ be a $\mathbb{Z}$-polynomial in $n$ indeterminates. Let $a(k)$ count the number of ways that an integer $k$ can be represented by $F$ over the integers. What general characteristics are known about the function $\sum_{k \geq 0} a(k) \ q^{k}$? Can one determine its symmetries by the symmetries of $\{ F(X_1, \dots, X_n) = k \}_{k \in \mathbb{N}}$ as a family of deformations of the hypersurface $F = 0$? Can one define a related $L$-function and presuppose its functional identities similarly? Any clarification is certainly appreciated!","['algebraic-geometry', 'number-theory']"
21869,"If $\frac{dy}{dt}dt$ doesn't cancel, then what do you call it?","I have $y$ is a function of $t$. I have reached a situation here where I need to evaluate $$\displaystyle \int_0^b{\frac{dy}{dt}dt}$$ Now clearly $y$ has dependence on $t$, otherwise $\displaystyle \frac{dy}{dt}$ should be 0. So now I write $$\displaystyle \int_{y(0)}^{y(b)}{dy} = y \rvert_{y(0)}^{y(b)} = y(b) - y(0)$$ I know that dt's don't cancel , but what do you call it, then?  Just a ""change of variables""?  How do we justify where $dt$ went?","['notation', 'calculus']"
21877,Minimal polynomials of $\sqrt[4]{2}i$ over $\mathbb{Q}$ and $\mathbb{R}$,"Can someone tell me if this is right: I would like to find the minimal polynomial of (i) $\sqrt[4]{2}i$ over $\mathbb{Q}$ (ii) $\sqrt[4]{2}i$ over $\mathbb{R}$ (i): $\sqrt[4]{2}i$ is a root of $f(x) = x^4-2$. This is already monic, so to show that this is a minimal polynomial I only need to show that it is irreducible. Edit: To do that, I can use the Eisenstein: $p=2$ does not divide $a_1 = 1$ and $p^2$ does not divide $a_0 = -2$, therefore it is irreducible over $\mathbb{Q}$. (ii): This time, $\sqrt[4]{2}i$ is a root of $f(x) = x^2+\sqrt{2}$. For polynomials of degree two it's enough to check if they have a root. The only roots this one has are complex therefore it is irreducible over $\mathbb{R}$ and therefore the minimal polynomial. So my more general question is: is the way to find a minimal polynomial of an element $a$ in general to find a polynomial $f$ such that $f(a) = 0$ and then to norm $f$ and then to show that $f$ is irreducible? Many thanks for your help!","['minimal-polynomials', 'abstract-algebra', 'field-theory']"
21878,"Examples of function sequences in C[0,1] that are Cauchy but not convergent","To better train my intuition, what are some illustrative examples of function sequences in C[0,1] that are Cauchy but do not converge under the integral norm?","['metric-spaces', 'convergence-divergence', 'analysis']"
21883,Does Fermat's Last Theorem hold for cyclotomic integers in $\mathbb{Q(\zeta_{37})}$?,"The first irregular prime is 37. Does FLT(37) $$x^{37} + y^{37} = z^{37}$$ have any solutions in the ring of integers of $\mathbb Q(\zeta_{37})$, where $\zeta_{37}$ is a primitive 37th root of unity? Maybe it's not true, but how could I go about finding a counter-example? (for any cyclotomic ring, not necessarily 37)",['number-theory']
21915,Chain rule for multi-variable functions,"So I have been studying the multi-variable chain rule.  Most importantly, and this is what I must have overlooked, is it's not always clear to me how to see which variables are functions of other variables, so that you know when to use the chain rule.  For example, if you have: $$ x^2+y^2-z^2+2xy=1 $$
$$ x^3+y^3-5y=8 $$ In general, say we want to find $\frac{dz}{dt}$ but $z$ is a function of $x$, then we get: $$ \frac{dz}{dt} = \frac{dz}{dx} \frac{dx}{dt} .$$ And if $z$ is a function of both $y$ and $t$, we get: $$ \frac{dz}{dt} = \frac{dz}{dx} \frac{dx}{dt} + \frac{dz}{dy} \frac{dy}{dt}$$ In this case, we have two equations.  One involving all three variables $x,y,z$ and one involving just $x,y$.  Say we want to find $\frac{dz}{dx}$. What does this mean for this case?  How should we interpret this rule in general?","['multivariable-calculus', 'calculus']"
21916,Finding an Inverse function with multiple occurences of $y$,For some reason I cannot figure out how the book is finding the solution to this problem. find the inverse function of $f(x)=3x\sqrt{x}$. My steps seem to lead to a dead end: step 1. switch $f(x)$ with $y$: $y = 3x\sqrt{x}$ step 2. swap $x$ and $y$: $x = 3y\sqrt{y}$ step 3. solve for $y$: step 3.1: $\displaystyle \frac{x}{3y} = \sqrt{y}$. step 3.2: $\displaystyle \left(\frac{x}{3y}\right)^2 = \left(\sqrt{y}\right)^2$ step 3.3: $\displaystyle \frac{x^2}{9y^2} = y$ ... uhhh? The book answer is $\displaystyle y = \left(\frac{x}{3}\right)^{2/3}$.,"['algebra-precalculus', 'functions', 'inverse-function']"
21928,How to find $\int_0^{\infty}\frac{dx}{(1+x^2)^4}$,How would you compute for the definite integral of $$\int_0^{\infty}\frac{dx}{(1+x^2)^4}$$ I know that integral of $\displaystyle \frac1{(1+x^2)}$ equals $\tan^{-1}x$. I tried using integration by parts without much luck.  My teacher pointed me to special functions by which I found out about the hypergeometric distribution. Although I don't know how to apply it to this problem. Anybody know how to use special functions or how to go about this problem?,"['calculus', 'integration']"
21931,Refining the central limit theorem on discrete random vars,"Let $x_i$ be iid nonnegative discrete random variables $E[x_i]=N/M$ for some integers $N, M$, variance $\sigma^2$ and higher moments known (finite). Then, the sum $\displaystyle S = \sum_{i=1}^M x_i$ will have $E[S]=N$. I'm interested in the probability that 
$S$ takes that precise value: $A=P\left(S=E[S]\right)$. Applying the central limit theorem, I can write $\displaystyle A \approx \frac{1}{\sqrt{ 2 \pi M \sigma^2}}$ My question is: can this approximation be refined? ADDED: To add some example-context-motivation: Lets consider $X$ as a sum of $N$ Bernoullis (0/1) with prob=$p$, such that $E(X)=E(N p)$ is an integer. We can compute exactly the probability that $X$ attains its expected value, it's a Binomial: $\displaystyle P = P(X= N p) = {N \choose N p} p^{N p} q^{N q} \hspace{2cm}$  [1a] We might also get an approximate value of that probability using the CTL (Central Limit Theorem) $\displaystyle P \approx \frac{1}{\sqrt{2 \pi N p q}} \hspace{2cm} $  [2a] If we take [1a] and use the Stirling approximation, with $K \approx (K/e)^K \sqrt{2 \pi K}$, we get the same value. Fine. Now, we may try to refine the approximation, both from [1a] and [2a]. Plugging the next orden Stirling approximation in [1a], we get (I am not mistaken) $\displaystyle P \approx \frac{1}{\sqrt{2 \pi N p q}} \left(1 - \frac{1- p q}{12 N p q} \right) \hspace{2cm} $  [1b] To refine the CTL, one can think of use some ""continuity correction"" to evaluate more precisely the (hipothetical) gaussian integral add some terms from the Edgeworth expansions do nothing of the above - because the CLT does not justify those procedures in this scenario (just one value of a discrete variable) I'm not sure which is the correct way. But let's try the first one:  the next order approximation of the integral gives me (again, if I'm not mistaken) $\displaystyle P \approx \frac{1}{\sqrt{2 \pi N p q}} \left(1 - \frac{1}{24 N p q} \right) \hspace{2cm} $  [2b] This is not the same as [1b], but it's close. Is this just casual? Was it a reasonable thing to do? Should I look (also/instead) after the Edgeworth expansions?",['probability']
21932,"What does it mean to say a map ""factors through"" a set?","Consider the following diagram: What does it mean precisely to say ""$f$ factors through $G/\text{ker}(f)$""? Does it mean $f = \tilde{f} \circ \pi$, for some $\tilde{f}$? I've seen texts use the phrase, but never a definition of this notion.",['abstract-algebra']
21933,Does algorithmic unsolvability imply unsolvability in general?,"I recently found out that there is no algorithm which, given an arbitrary group presentation, will determine in finite time if it represents the trivial group*.  Additionally, in a lecture I recently attended, it was portrayed that given any desired property P of a group, there is no algorithm that will determine if an arbitrary group (presentation) has this property.  (For additional info, see the second answer to this question on MathOverflow). My question is about the scope of this notion of ""algorithm.""  With regard to group presentations, my intuition tells me that there must be some way to classify or otherwise look at these arbitrary group presentations that would enable us to determine if they are the trivial group (or have some property P).  However, I am not very well versed on what it means for something to be solved via an algorithm vs. other mathematical methods. In general, if something has been shown to be unsolvable via algorithm, does it mean that it is entirely unsolvable (and thus futile to pursue in research)?  Or, are there cases where we have shown that despite being unable to use an algorithm to determine if a property exists, we have used other mathematical methods with success? *Clarification by Arturo.","['logic', 'group-theory']"
21934,Nonmeasurable set with positive outer measure,"It is well-known that any set $E \subseteq \mathbb{R}$ with positive outer measure contains a nonmeasurable subset $V$. I know that $0 < m^*(V) \le m^*(E)$. Nevertheless, my question is the following: given $r \in \mathbb{R}$ such that $r>0$, is there a nonmeasurable subset of $\mathbb{R}$ whose outer measure is exactly $r$? Thank you in advance.","['measure-theory', 'examples-counterexamples']"
21942,How do I complete this square?,"I need to simplify the following equation, by completing the square:
$$\frac{\left(x-z\right)^{2}}{2(u-s)}+\frac{(z-y)^{2}}{2(t-u)}$$ As $\displaystyle\frac{\left(x-y\right)^{2}}{2(t-s)}+C$. How can I do this? I  can't seem to be able to deal with the fractions effectively when completing this square...",['algebra-precalculus']
21945,Infinite product of measurable spaces,"Suppose there is a family (can be
infinite) of measurable spaces. What
are the usual ways to define a sigma
algebra on their Cartesian product? There is one way in the context of
defining product measure on planetmath . Let $(E_i, B_i)$ be
measurable spaces, where $i \in I$ is an index set, possibly infinite.
We define their product as follows: let $E= \prod_{i \in I} E_i$ , the    Cartesian product of $E_i$ , let $B=\sigma((B_i)i \in I)$ , the    smallest sigma algebra
containing    subsets of $E$ of the
form $\prod_{i    \in I}B_i$ where $B_i=E_i$ for all    but a finite
number of $i \in I$ . I was wondering why it is required
that "" $B_i=E_i$ for all but a finite
number of $i \in I$ ""? Thanks and regards! ADDED: I was wondering if the product sigma algebra defined in 2 is the smallest sigma algebra such that any tuple composed of one measurable set from each individual sigma algebra is measurable?","['product-space', 'measure-theory', 'elementary-set-theory']"
21946,Integral of measurable spaces,"If for each $t\in I=[0,1]$ I have a measurable space $(X_t,\Sigma_t)$, is there a standard notion which will give a measurable space deserving to be called the integral $\int_I X_t\,\mathrm d t$? Motivated by this question and curiosity...",['measure-theory']
21949,On problems of coins totaling to a given amount,"I don't know the proper terms to type into Google, so please pardon me for asking here first. While jingling around a few coins, I realized that one nice puzzle might be to figure out which $n$ or so coins of a currency (let us, say, use the coins of the American dollar as an example) can be used to total to a given amount. For instance, one can have five coins totaling to forty cents: three dimes and two nickels. 1) How does one find other $n$ combinations of coins that can total to a set amount? To use my example, are there other five coins that can total to forty cents? How might one algorithmically prove that those are the only solutions? 2) Given an amount not equal to a denomination, what is the minimum number of coins needed to be equivalent to the given amount? For instance, one can have forty cents in three coins: a quarter, a dime, and a nickel. How does one algorithmically show that the ""magic number"" for forty cents is three (i.e., one cannot find two coins whose amounts total to forty cents)? As mentioned already, this isn't homework; just idle curiosity. Any pointers to algorithms would be appreciated!","['optimization', 'integer-partitions', 'algorithms', 'combinatorics']"
