question_id,title,body,tags
4124684,Use generating functions to calculate the sum $\sum_{k=0}^n (k+1)(n-k+1)$,"Use generating functions to calculate the sum $\sum_{k=0}^n (k+1)(n-k+1)$ And Prove it by induction. Attempt: $$A(x)= \sum_{k=0}^n (k+1)(n-k+1)$$ $$\sum_{n=0}^\infty(\sum_{k=0}^n (k+1)(n-k+1))x^n$$ The series of differences is : $\{(n+1),2(n),3(n-1),4(n-2)…,1,…\}$ The generating functions of this series is : $(1-x)A(x)$ The series of differences is : $\{(n+1),(n-1),(n-3),(n-5),…,2-n,…\}$ The generating functions of this series is : $(1-x)^2A(x)$ The series of differences is : $\{(n+1),-2,-2,-2,…,-2,…\}$ The generating functions of this series is : $(1-x)A^3(x)$ The series of differences is : $\{(n+1),-(n+3),0,0,…,0,…\}$ The generating functions of this series is : $(1-x)^4A(x)$ On the other hand, the generating functions $(n+1)+(-n-3)x$ , Therefore: $(1-x)^4A(x)=(n+1)+(-n-3)x$ $$A( x) =\frac{( n+1) +( -n-3) x}{( 1-x)^{4}} =(( n+1) +( -n-3) x) \cdot \sum _{n=0}^{\infty }\binom{n+4-1}{4-1} x^{n}$$ $$\displaystyle \sum _{k=0}^{n}( k+1)( n-k+1) =\sum _{n=0}^{\infty }( n+1) \cdot \binom{n+4-1}{4-1} x^{n} +\sum _{n=0}^{\infty }( -n-3) \cdot \binom{n+4-1}{4-1} x^{n+1}$$ Prove in with induction : \begin{aligned}
( n+1) \cdot \binom{n+3}{3} +( -n-3)\binom{n+2}{3} & =\frac{( n+3)( n+2)( n+1)^{2} n}{3!} +\frac{( n+2)( n+1)( n-1) n( -n-3)}{3!}\\
 & =\frac{( n+3)( n+2)( n+1)^{2} -( n+2)( n+1)( n+3) n}{6}\\
 & =\frac{( n+3)( n+2)( n+1)}{6}\\
 & =\frac{1}{6}\left( n^{3} +6n^{2} +11n+6\right)
\end{aligned} $$\displaystyle \sum _{k=0}^{n}( k+1)( n-k+1) =\frac{1}{6}\left( n^{3} +6n^{2} +11n+6\right)$$ induction basis: $n=0$ $$\displaystyle \sum _{k=0}^{n}( 0+1)( 0-0+1) =1=\frac{1}{6}\left( 0^{3} +6\cdotp 0^{2} +11\cdotp 0+6\right)$$ For $n+1$ : \begin{aligned}
\sum _{k=0}^{n+1}( k+1)( n-k+1) & =\sum _{k=0}^{n}( k+1)( n-k+1) +( n+2)(( n+1) -( n+1) +1)\\
 & =\frac{1}{6}\left( n^{3} +6n^{2} +11n+6\right) +( n+2)\\
\end{aligned} If anyone can enlighten me, and find my mistakes I appreciate that. Unfortunately, I did not succeed.","['discrete-mathematics', 'generating-functions']"
4124686,Determine the limit using Taylor expansions:,"I struggle with this one—maybe someone could point me in the right direction. $$\lim_{x\to 0} \frac{5^{(1+\tan^2x)} -5}{1-\cos^2x}$$ Getting the Taylor series expansion for $\tan^2x$ and $\sin^2x$ is no problem, but I struggle with getting further along at this step: $$\frac{5^1 \cdot 5^{x^2} \cdot 5^{(2/3)x^4} -5}{x^2 - \frac{x^4}3}$$ Any help would be greatly appreciated!","['limits', 'taylor-expansion']"
4124728,Abstract symmetric definition of duality in linear algebra?,"In his Linear Algebra , 4th ed. from 1975, Greub presents (p. 65) an abstract, symmetric definition of duality, in which two vector spaces $E^*,E$ over a field $\Gamma$ are said to be dual if there is a non-degenerate bilinear form $\langle-,-\rangle:E^*\times E\to\Gamma$ defined between them. This differs from most standard linear algebra books I've seen which simply define ""the"" dual space of a vector space $E$ to be the space $L(E)$ of linear functionals on $E$ . Of course, with Greub's definition, there is an embedding $E^*\to L(E)$ defined by $x^*\mapsto\langle x^*,-\rangle$ , which is an isomorphism when $E$ is finite-dimensional, but the definition itself is more general. I prefer Greub's approach to the standard approach, and see it akin to many other uses of abstract definitions in mathematics -- like using the abstract definition of a group even though any group is isomorphic to a group of permutations (Cayley's Theorem), etc. I am curious who first used this approach in linear algebra. I've found the following sources so far: Greub used it as early as his first German edition in 1958. In Fundamental Concepts of Algebra (1956), Chevalley uses it (p. 106) when defining ""vector spaces in duality"", although he also defines ""the"" dual module of a module earlier in the book (p. 67). In Lectures in Abstract Algebra, Vol. II: Linear Algebra (1953), Jacobson uses it (p. 141, 253). I was expecting to find this approach used in Bourbaki, but I did not. Does anyone have an earlier source?","['reference-request', 'linear-algebra', 'dual-spaces', 'math-history', 'duality-theorems']"
4124752,Difference between $\mathbb{R}^1$ and $\mathbb{R}$,"We write $\mathbb{R}^n$ to be the set $\{(x_1,\dots,x_n): x_i \in \mathbb{R} \mbox{ for } i=1,\dots,n\}$ , so $\mathbb{R}^n$ is the set of $n$ -tuples of real numbers. Is then $\mathbb{R}^1$ a set of 1-tuples but still ""somehow the same"" as $\mathbb{R}$ ? Is there any subtle difference between the two?","['elementary-set-theory', 'notation']"
4124781,"For a projective scheme over a base, when do cones and fibers commute?","Definitions : Let $R$ be a ring and suppose $X\subset \Bbb P^n_R$ is a closed subscheme cut out by the ideal sheaf $\mathcal{I}$ on $\Bbb P^n_R$ . Then the largest homogeneous ideal of $X$ in $R[x_0,\cdots,x_n]$ is $I=\Gamma_*(\mathcal{I})=\bigoplus \pi_*(\mathcal{I}(d))$ (this can be written as the saturation of any other ideal cutting out $X$ ), and we can write $X=\operatorname{Proj}(R[x_0,\cdots,x_n]/I)$ . Define $C(X)$ , the cone on $X$ , as $\operatorname{Proj}(R[x_0,\cdots,x_n,x_{n+1}]/(I))$ . The fiber of $X$ over a point $r\in \operatorname{Spec} R$ is defined to be $X_r=\operatorname{Spec} \kappa(r)\times_{\operatorname{Spec} R} X$ , which can be seen to be $\operatorname{Proj}((R[x_0,\cdots,x_n]/I)\otimes_R \kappa(r))$ . Question: When is $C(X_r)\cong C(X)_r$ as subschemes of $\Bbb P^n_{\kappa(r)}$ ? Motivation : This seems like something that ought to be intuitively true with no conditions if you think about the geometric constructions these two operations are supposed to represent. But one has to verify this from the algebraic definitions, and it wouldn't surprise me if some assumptions were required (flatness, for instance). Unfortunately, I've made this claim in an answer here without having a full proof in hand and the OP there has rightly pointed out my lack of rigor. After several days of thinking about it, I don't have a good fix, and I've decided to ask for help. Progress : Define $J\subset \kappa(r)[x_0,\cdots,x_n]$ to be the kernel of $$R[x_0,\cdots,x_n]\otimes_R \kappa(r) =  \kappa(r)[x_0,\cdots,x_n] \to (R[x_0,\cdots,x_n]/I)\otimes_R \kappa(r)$$ (equivalently the image of $I\otimes_R \kappa(r)\to R[x_0,\cdots,x_n]\otimes_R \kappa(r)$ ). Then $X_r=\operatorname{Proj}(\kappa(r)[x_0,\cdots,x_n]/J^{sat})$ , and $$C(X_r)=\operatorname{Proj}(\kappa(r)[x_0,\cdots,x_{n+1}]/(J^{sat})).$$ On the other hand, $C(X)_r=\operatorname{Proj}((R[x_0,\cdots,x_{n+1}]/(I))\otimes_R \kappa(r))$ , and writing $R[x_0,\cdots,x_{n+1}]/(I)=\bigoplus x_{n+1}^d R[x_0,\cdots,x_{n}]/I$ , we see that $(R[x_0,\cdots,x_{n+1}]/(I))\otimes_R \kappa(r)\cong \kappa(r)[x_0,\cdots,x_{n+1}]/(J)$ . Since two homogeneous ideals cut out the same subscheme of $\Bbb P^{n+1}_{\kappa(r)}$ if and only if they have the same saturation, we need to show that $(J^{sat})^{sat} = (J)^{sat}$ . One way to attack this would be to show that if $I\subset R[x_0,\cdots,x_n]$ is saturated, then $S^{-1}I\subset S^{-1}R[x_0,\cdots,x_n]$ is saturated for $S\subset R$ multiplciatively closed and the image of $I$ in $(R/P)[x_0,\cdots,x_n]$ is saturated for $P\subset R$ prime. The first is claim doable, but I'm stumped on the second claim.","['algebraic-geometry', 'commutative-algebra']"
4124789,Improvement of Jensen inequality for random variables,"Jensen inequality implies that for every real random variable $X$ and every integer $n\in \mathbb N$ $$ (\mathbb E[X^2])^n \,\leq\, \mathbb E[X^{2n}]$$ by convexity of the function $x\mapsto x^n$ for $x\geq0$ . Now if we apply the previous inequality to a Gaussian random variable $X$ of mean $0$ and variance $\sigma^2$ we find: $$ \sigma^{2n} \,\leq\, \sigma^{2n}\, (2n-1)!!$$ which of course is not very good for large $n$ since $(n-1)!!$ grows faster than exponential. Is there an alternative / improvement of Jensen inequality that bounds $(\mathbb E)^n$ with a linear operator defined on a suitable space of random variables functions of $X^2$ and preserving exponential growth for large $n$ ? Edit. I would be happy enough if I could do that for all $X$ having a strong log-concave distribution on $\mathbb R$ , namely $$\mathbb E [\varphi(X)] \,=\, \int_{-\infty}^{\infty}\, \varphi(x)\,e^{-U(x)} \,dx $$ with $U''(x)\geq\lambda>0$ for all $x\in\mathbb R\,$ , for all $\varphi:\mathbb R\to\mathbb R$ having polynomial growth.
Notice that the Gaussian case corresponds to $U(x)=(x-\mu)^2/(2\sigma^2) + \frac{1}{2}\log(2\pi\sigma^2)$ . Edit 2. A possible modification of this question could be: for which family of random variables it holds true $$ (\mathbb E[X^2])^n \,\leq\, \frac{c^n}{(2n-1)!!}\,\mathbb E[X^{2n}]$$ for some $c>0$ and all $n\in\mathbb N\,$ ?
Centred Gaussian random variables for example satisfy the inequality with $c=1$ .","['measure-theory', 'functional-analysis', 'gaussian-integral', 'convex-analysis', 'probability-theory']"
4124798,Sketching phase portraits using Hamiltonians,"Consider the system \begin{align*}
    x' &= 2y^{3} - y \\
    y' &= x^{3} - x \\
\end{align*} The problem I am working on asks to find the Hamiltonian for the system, find the equilibria, and then sketch the phase portrait. Finding equilibria is easy, and I believe the Hamiltonian is $$
H(x,y) = \frac{1}{2}(x^{2} + y^{4} - y^{2}) - \frac{1}{4}x^{4}.
$$ I understand Hamiltonians give conserved quantities and therefore level curves in the phase plane. I understand how to sketch phase portraits from a system in polar coordinates. However, I do not at all understand how the Hamiltonian is supposed to help me visualize the phase portrait. I am studying for a test with questions like this so I can not use any mathematical software. How do you use Hamiltonians to visualize level curves and sketch phase portraits? Thanks.","['ordinary-differential-equations', 'dynamical-systems']"
4124845,Finding the $n^{\text{th}}$ derivative of a beautiful function,"I have to find the $n^{\text{th}}$ derivative of the following function $$y= \frac{x+2}{\sqrt[3]{1-x}}$$ I tried taking the derivative for a couple of times to find some patterns but it didn't help.
I feel like I have to use some formulas for common function's $n^{\text{th}}$ derivative, but in my function-as you can see- there are 2 types of functions so I don't know what i should do. Also I don't know any series yet, I don't know if that was necessary, just in case. Could you please help me?
Thanks.","['calculus', 'derivatives', 'analysis', 'real-analysis']"
4124915,Find a bijection $g:\mathbb{R} \longrightarrow \mathbb{R}\setminus \{0\}$,"Find an explicit bijection $g:\mathbb{R} \longrightarrow \mathbb{R}\setminus \{0\}$ . I'm sure I'm close to figuring this one out, but not quite there yet. My first idea was $g(x) = 
        \begin{cases}
        x+1, \text{if } x\geq0 \\
        x, \text{if } x<0\\
        \end{cases}$ , but this is not surjective (eg. 1/2) I think I'm getting mixed up when creating this map because since I can't map 0 to itself, I have to map it to something else, which means that I can't map that thing to itself either lest the function be noninjective, and so on. To fix this problem I was also thinking along the lines of $g(x) = 
        \begin{cases}
        x+1, \text{ if } x \in \mathbb{N} \\
        x, \text{if } x \in \mathbb{R} \setminus \mathbb{N}\\
        \end{cases}$ where $\mathbb{N}$ denotes the non-negative integers.","['calculus', 'functions']"
4124948,Do random subsequences of converging random variables converge as well?,"I am trying to understand the convergence of the sequence of random variables $(X_{N_n})_n$ where $(X_n)_n$ is a sequence of random variables and $(N_n)_n$ is a sequence of random positive integers that ""increase to $\infty$ "" (since this is a sequence of random variables there is different notions of how this $N_n \to \infty$ ). There are two scenarios that I am trying to analyze. Suppose $X_n\xrightarrow{P}0$ and $N_n\xrightarrow{a.s.}\infty$ . Is it neccesarily true that $X_{N_n}\xrightarrow{P}0$ ?. Similarly if $X_n\xrightarrow{a.s.} 0$ and $N_n\xrightarrow{P}\infty$ (This just means $P(|N_n|\le K)\to 0$ for all $K>0$ ) is it necessary that $X_{N_n}\xrightarrow{a.s.} 0$ ?","['probability-theory', 'random-variables']"
4125002,Regular surface that is homeomorphic to a plane,"I'm trying to solve the following exercise. Let $\gamma: \mathbb{R} \rightarrow \mathbb{R}^{2}$ be a regular plane curve without self-intersections and consider the set $S \subset \mathbb{R}^{3}$ , given by $S=\left\{(x, y, z) \in \mathbb{R}^{3}:(x, y) \in \gamma(\mathbb{R})\right\}$ . Show that $S \subset \mathbb{R}^{3}$ is a regular surface that is homeomorphic to a plane. I have managed to show that such a surface is indeed a regular surface, but am struggling to come up with a suitable homeomorphism, or see why it is a homeomorphism. The official solutions state that ""it is clear that the surface is homeomorphic to a plane"", but I cannot understand why this is. Can anyone help me understand this?","['surfaces', 'geometry', 'multivariable-calculus', 'general-topology', 'differential-geometry']"
4125054,Show torus homeomorphic to $S^1 \times S^1$,"Prove that $\mathbb{T}^2$ is homeomorphic to $S^1 \times S^1$ . Here is my attempt on this. Is this correct? Let $C^*$ be the torus as an identification space of $[0,1] \times [0,1]$ under the quotient map $g:[0,1] \times [0,1] \rightarrow C^{*}$ . Then $C^*=\{\{(x,0),(x,1)\}|0 < x < 1\} \cup\{\{(0,y),(1,y)\}|0 < y < 1\} \cup \{\{(x,y)\}|0 < x < 1, \ 0<y<1\} \cup\{\{(0,0),(0,1),(1,0),(1,1)\}\}$ Define the function $f:C^* \rightarrow S^1 \times S^1$ by $f(\{(x,y)\})=((\cos 2 \pi x,\sin 2 \pi x),(\cos 2 \pi y, \sin 2 \pi y)),0<x<1,0<y<1,f(\{(x,0),(x,1)\})=((\cos 2 \pi x,\sin 2 \pi x),(1, 0)),f(\{(0,y),(1,y)\})=((1,0),(\cos 2 \pi y, \sin 2 \pi y)),f(\{(0,0),(0,1),(1,0),(1,1)\})=((1,0),(1,0))$ $C^*$ is compact since it is the surjective image of the continuous function $g$ . Similarly, $S^1 \times S^1$ is Hausdorff, since it is a subspace of the Hausdorff space $\mathbb{R}^2 \times \mathbb{R}^2$ . Because of this it suffices to show $f$ is a continuous bijection, in order to prove it is a homeomorphism. $f(\{(x,y)\}=f(\{(w,z)\} \implies  ((\cos 2 \pi x,\sin 2 \pi x),(\cos 2 \pi y, \sin 2 \pi y))=((\cos 2 \pi w,\sin 2 \pi w),(\cos 2 \pi z, \sin 2 \pi z)) \implies x=w,y=z \implies \{(x,y)\}=\{(w,z)\}$ , similarly $f(\{(x,0),(x,1)\})=f(\{(w,0),(w,1)\}) \implies \{(x,0),(x,1)\}=\{(w,0),(w,1)\}$ and $f(\{(0,y),(1,y)\})=f(\{(0,z),(1,z)\}) \implies \{(0,y),(1,y)\}=\{(0,z),(1,z)\}$ so that $g$ is injective. Let $((\cos 2 \pi x,\sin 2 \pi x),(\cos 2 \pi y, \sin 2 \pi y)) \in S^1 \times S^1$ . If $x=0,y \neq 0$ then $f(\{(0,y),(1,y)\})=((1,0),(\cos 2 \pi y, \sin 2 \pi y))$ . If $x \neq 0,y=0$ then $f(\{(x,0),(x,1)\})=((\cos 2 \pi x,\sin 2 \pi x),(1, 0))$ . If $x=0,y=0$ then $f(\{(0,0),(0,1),(1,0),(1,1)\})=((1,0),(1,0))$ . Otherwise $f(\{(x,y)\}=((\cos 2 \pi x,\sin 2 \pi x),(\cos 2 \pi y, \sin 2 \pi y))$ . So that the function is surjective. Finally consider the quotient map $g:[0,1] \times [0,1] \rightarrow C^*$ . $f$ is continuous if and only if $f \circ g$ is continuous by one of the well known theorems. Now for each $1 \leq i \leq 2$ , the projection function $\pi_i \circ f \circ g:[0,1] \times [0,1] \rightarrow \mathbb{R}^2$ is continuous since $(\pi_1 \circ f \circ g)(x,y)=(\cos 2 \pi x,\sin 2 \pi x), (\pi_2 \circ f \circ g)(x,y)=(\cos 2 \pi y, \sin 2 \pi y)$ are continuous. Similarly $(1,0),(0,1)$ are continuous. So $f$ is continuous by one of the well known theorems and $f$ is a homeomorphism being a continuous bijection from a compact space to a Hausdorff space. .",['general-topology']
4125062,How can we state Sylvester's law of inertia without referring to a particular basis?,"In elementary linear algebra, we talk about matrices, i.e. rectangular arrays of numbers. In advanced linear algebra, we prefer whenever possible to talk about abstract tensors, such as linear operators or bilinear forms, without going into any particular basis. This framing is considered more elegant, and also allows for easier generalization to vector spaces more abstract than the simple case of $\mathbb{R}^n$ for finite $n$ . In particular, I've found that almost all theorems about matrices generalize fairly easily to more abstract theorems about linear operators and/or bilinear forms. (One strong hint as to which one is that matrix similarity is the natural expression of ""equivalence"" for linear operators, and matrix congruence is the natural expression of ""equivalence"" for bilinear forms. For an inner product space, the distinction is often blurred, because there's a natural isomorphism between linear operators and bilinear forms.) But I'm not quite how this works for Sylvester's law of inertia , because it seems to combine elements of both notions without apparently requiring any inner product on the vector space. Part 1 of Sylvester's law of inertia says that every real symmetric square matrix is congruent to exactly one diagonal matrix with entries 1, -1, and 0 (up to permutation). This pretty clearly seems to generalize to the Wikipedia article's basis-independent statement about real quadratic forms: For any real quadratic form $Q$ , every maximal subspace on which the restriction of $Q$ is positive definite (respectively, negative definite or totally isotropic) has the same dimension. But I'm confused about part 2 of Sylvester's law of inertia, which says that two symmetric square matrices are congruent iff they have the same numbers of positive, negative, and zero eigenvalues (which equal the numbers of entries 1, -1, and 0 defined in part 1). I'm not sure how to state this in a basis-independent way, because matrix congruence is a equivalence relation on representations of bilinear or quadratic forms , but the notion of eigenvalues only makes sense for linear operators . Moreover, the statement of the theorem doesn't seem to require any inner product that would allow you to naturally convert between the two types of tensors. Is part 2 of Sylvester's law of inertia a statement about forms or about linear operators, and how can we generalize it to tensors in a basis-independent way? The only solution I can think of, which seems to me to be very much a hack, is to introduce an arbitrary inner product - something like the following: Let $Q$ be an arbitrary quadratic form on a real vector space $V$ , and $B$ be the naturally associated symmetric bilinear form. Define an arbitrary inner product $\langle \rangle$ on V. Consider the unique linear operator $L_{B,\langle \rangle}$ on $V$ associated with $B$ , which maps any vector $v$ to the vector $u$ such that $B(v, w) = \langle u, w \rangle$ for all $w \in V$ . Then regardless of the choice of inner product $\langle \rangle$ , the numbers of positive, negative, and zero eigenvalues of $L_{B, \langle \rangle}$ equal the indices of inertia of $Q$ defined in part 1. To me, this proposition seems ... rather convoluted. Is there a simpler version?","['change-of-basis', 'linear-algebra', 'linear-transformations', 'symmetric-matrices', 'quadratic-forms']"
4125087,How to evaluate the definite integral of a derivative?,"I am looking through some old material on first order differential equations and realized I didn't really understand how the bounds on the integral are found. I found this question from a couple years ago which gives an answer but doesn't really go into much detail. I'm currently just thinking about first order linear diff eq's of the form: $$\frac{dy}{dx}+P(x)y=f(x)$$ Finding the integrating factor is pretty straightforward: $$\mu(x)=e^{\int{P(x)dx}}$$ Which is multiplied to both sides: $$\mu(x)\left[\frac{dy}{dx}+P(x)y\right]=\mu(x)f(x)$$ And this simplifies to: $$\frac{d}{dx}(\mu(x)y)=\mu(x)f(x)$$ And this is where the trouble starts. This equation should be integrated on both sides over the same bounds to maintain equality: $$\int_a^b{\frac{d}{dx}(\mu(x)y)dx}=\int_a^b{\mu(x)f(x)dx}$$ The RHS is just a straghtforward definite integral. The LHS however is a little more complicated. The question I linked above has a couple responses suggesting an integration by substitution so doing something like $u=\mu(x)y$ and I guess turning the integral into $\int_{u(a)}^{u(b)}{u}du$ but I'm not sure if that is right. This question was inspired partially by a practical issue I'm trying to solve involving an op-amp integrator and following the u substitution route leads to some things that don't make sense like the initial value of the output growing to infinity which shouldn't happen since I'm modeling a circuit that already was made and I know works as intended in real life. The LHS in my model looks like this: $$\int_0^T{\frac{d}{dt}\left[e^{-\frac{t}{R_2C}}V_{out}(t)\right]dt}$$ And applying $$u=e^{-\frac{t}{R_2C}}V_{out}(t)$$ and the bounds: $$a=u(0)=V_{out}(0)\quad\text{and}\quad b=u(t)=e^{-\frac{t}{R_2C}}V_{out}(t)$$ gives: $$\int_{u(0)}^{u(t)}{du}=u(t)-u(0)=e^{-\frac{t}{R_2C}}V_{out}(t) - V_{out}(0)$$ This is problematic because when $V_{out}(0)$ is added over to the RHS and the exponential term is multiplied across it will then have a positive exponent and grow this initial condition to infinity. I don't believe I made a mistake up to this point but regardless of whether my derivation in this particular problem is correct or not the question still remains as to what the proper way to deal with the limits on integrals of this type is, both determining what appropriate limits are and how to evaluate them. Any help here is appreciated. Edit: I realized I did make a mistake and lost a negative sign earlier causing that weird behavior. Corrected the model now has a decaying initial value which is in line with what I've seen from the actual circuit. I've left this up however because I am still interested in whether there is a more formal way of doing this integration since I'm aware that a lot of the methods taught in differential equations play fast and loose. If there is any nuances that I'm missing in this post or any corrections to anything I said I would still like to hear. Thanks.","['definite-integrals', 'ordinary-differential-equations']"
4125118,Sufficient conditions for being a harmonic function,"Let $f(x,y)$ be a real-valued function defined on an open subset $U \subset \mathbb R^2$ . Suppose that $f$ is twice differentiable separately in each variable and satisfies Laplace's equation $f_{xx} + f_{yy} = 0$ . Using the given information, we cannot conclude that $f$ is even continuous, let alone twice continuously differentiable. Therefore, we cannot guarantee that $f$ is harmonic. My questions are: Suppose we know that $f$ is continuous. Is this sufficient to guarantee that $f$ is harmonic? Suppose we know that $f$ is differentiable. Is this sufficient to guarantee that $f$ is harmonic? Suppose we know that $f$ is continuously differentiable. Is this sufficient to guarantee that $f$ is harmonic? Suppose we know that $f_{xy}$ and $f_{yx}$ exist. Is this sufficient to guarantee that $f$ is harmonic? Suppose we know that $f_{xx}$ and $f_{yy}$ are continuous. Is this sufficient to guarantee that $f$ is harmonic? Suppose we know that $f$ is twice differentiable (but not continuously so!). Is this sufficient to guarantee that $f$ is harmonic?","['partial-derivative', 'partial-differential-equations', 'harmonic-functions', 'real-analysis']"
4125125,Cylindrical Mirror Anamorphosis – Development of a System of Equations,"I have a personal project to develop an application in Python to generate a Cylindrical Mirror Anamorphosis from a bitmap.  I’ve developed working code that involves iteration but is slow as a result.  I’m trying to develop some equations to replace my iteration routines. With reference to Figure 1, I’m trying to find the coordinates of $p$ given $v$ , $a$ , $o$ and $r$ . For my project $x_o$ , $y_o$ and $x_v$ equal zero. The gradients of $A$ , $B$ and $C$ are therefore: $$gradA = \frac{y_p-y_a}{x_p-x_a} \; \; \; \; \; gradB = \frac{y_p}{x_p} \; \; \; \; \; gradC = \frac{y_p-y_v}{x_p}$$ Given the angle of incidence and reflection Ø are equal then: $$\frac{gradB-gradA}{1+gradB.gradA}=\frac{gradC-gradB}{1+gradC.gradB}$$ Gives: $$\frac{\frac{y_p}{x_p} - \frac{y_p-y_a}{x_p-x_a}}{1+\frac{y_p}{x_p}.\frac{y_p-y_a}{x_p-x_a}}=\frac{\frac{y_p-y_v}{x_p}-\frac{y_p}{x_p}}{1+\frac{y_p-y_v}{x_p}.\frac{y_p}{x_p}}$$ Which condenses down to: $$ x_p^3(y_a+y_v)-x_p^2(y_px_a+y_vx_a)-x_p2y_vy_py_a+y_p^2(x_py_a+y_vx_a+y_vx_p)-y_p^3x_a=0$$ Given $p$ is on a circle of radius $r$ : $$y_p=\sqrt{r^2-x_p^2}=\sqrt{(r+x_p)(r-x_p)} = \sqrt{r+x_p}\sqrt{r-x_p}$$ I now want to substitute $y_p$ into the above equation and develop an expression for $x_p$ and this is where I could do with some guidance from this community.  How, if at all possible, do I free $x_p$ from the square root term? I’ll apologies now if my question appears naive.","['trigonometry', 'geometry']"
4125142,Prove that $3$ is the smallest prime dividing the order of the group.,"Let $G$ be a finite group where $p$ is the smallest prime dividing the order of the group. Now, let $x$ be an element of order $p$ in $G$ such that $hxh^{-1}=x^{10}$ for some $h \in G$ . Then we need to show that $p=3$ My approach to the solution goes like this: let $H$ be a subgroup generated by $h$ and $K$ be the subgroup generated by $x$ . Now we consider the group action of conjugation from, $H \times K \to \text{Aut}(K)$ . We define a map $\phi$ such that, $\phi:H \to \text{Aut}(K)$ where $\phi(h^k):x \mapsto h^k x h^{-k}$ . I am kind of stuck after this. I can understand that order of Aut( $K$ ) is $p-1$ . I have also been able to show using some calculations that order of the element( $p$ ) will be either $3$ or $11$ . However I am not sure of how to continue after this. This is a practice problem to revise automorphism of groups and I have absolutely forgotten all the theory. I need some help with the basic.","['automorphism-group', 'group-theory', 'abstract-algebra']"
4125165,Does the limit $\lim _{x \rightarrow 0} \frac{\int_{0}^{x^{2}} \sin \sqrt{t} d t}{x^{3}}$ exists?,"Evaluate $$L=\lim _{x \rightarrow 0} \frac{\int_{0}^{x^{2}} \sin \sqrt{t} d t}{x^{3}}$$ Since the limit is $\frac{0}{0}$ form, Using L'Hopital's rule and Leibniz rule we get $$L=\lim_{x \to 0}\frac{2x\sin |x|}{3x^2}$$ $\implies$ $$L=\frac{2}{3}\lim_{x \to 0}\frac{\sin|x|}{x}$$ Now we know that limit exists only when right and left hand limits are finite and equal. $$L^+=\lim_{h \to 0^+}\frac{\sin h}{h}=1$$ $$L^-=\lim_{h \to 0^+}\frac{\sin|-h|}{-h}=-1$$ Since $$L^+ \ne L^-$$ we can say limit Does not exists.
But book answer is $\frac{2}{3}$","['indeterminate-forms', 'limits', 'derivatives', 'leibniz-integral-rule']"
4125344,Split all subsets of $\mathcal{P}(\mathbb{N})$ into 2 groups $A \dot\cup B =\mathcal{P}(\mathbb{N})$ s.t. no 2 neighboring sets are in the same group,"So I'm trying to come up with a proof of the above action being possible or not, using the compactness theorem from logics (a set of first-order sentences has a model if and only if every finite subset of it has a model) on this one but I'm not quite sure how to split up the finite subsets of $\mathbb{N}$ to make the condition true. (Note: Neighboring here means that for instance $X,Y \subseteq \mathbb{N}$ are neighboring if you can get $X$ by adding an element to Y, i.e. $X=Y \cup \{c\}$ for a c $\not\in Y$ or the other way around. $\{2,41\}$ and $\{0,2,41\}$ would be neighbors but not $\{3,4\}$ and $\{3,5\}$ Note 2: $\dot\cup$ is the disjoint union Note 3: Referring to all possible subsets of $\mathcal{P}(\mathbb{N})$ . @Gae. S. provided a very nice answer using the axiom of choice, but I am looking for a way to prove it with the compactness theorem of logics which is in a certain way similar to the axiom of choice.
) I'd really appreciate some help on this :) Thanks so much!","['first-order-logic', 'logic', 'discrete-mathematics']"
4125401,Sufficient conditions for partial derivative reciprocal,"I would like to know the sufficient conditions for the following to hold, $$\frac{\partial y}{\partial x}= \frac{1}{\frac{\partial x}{\partial y}}$$ I have intentionally been vague here because I don't want to restrict any answers to specific cases and I don't know enough about multivariable calculus to appreciate what information I need to tell you in order for this to be a straightforward question. I'm hoping this situation is common enough for you to get the jist, but please let me know otherwise. There a bunch of similar questions on this site, and I haven't found one that quite addersses this. This question tells us that it is not always true, but not why. This question also shows it isn't true in general, and apparently we can form some kind of matrix to answer this question, but there is no reference how to actually do this in any other example or what it is called. Also, we have to make sure ""the same variables are being held constant in each partial derivative."", what does this mean? Why is it sufficient? Because the lecturer said so? Before marking this as a duplicate, please dig deep as to whether the question posed here has really been answered.","['partial-derivative', 'multivariable-calculus']"
4125411,"Evaluating integral $\int_0^{\infty } \left(\coth x-\frac{1}{x}\right) \text{csch} \>x\, dx$","I am having trouble evaluating the following integral: $$\int_0^{\infty } \left(\coth (x)-\frac{1}{x}\right) \text{csch}(x) \, dx\tag{1}$$ Numerically the integral appears to evaluate to $\log 2$ . The Weierstrass substitution doesn't help me other than allow a series approximation to be calculated using Mathematica. The Weierstrass substitution results in $$\frac{1}{2} \int_0^1 \left(\frac{1}{t^2}-\frac{1}{t \tanh ^{-1}(t)}+1\right) \, dt\tag{2}$$ Any ideas? Incidentally there seem to be a sequence of such integrals with closed forms: $$I_n=\int_0^{\infty } \left(\coth^n (x)-\frac{1}{x^n}\right) x^{n-1}\text{csch}(x) \, dx\tag{3}$$","['integration', 'trigonometric-integrals', 'improper-integrals']"
4125416,Integral of square root of a quartic polynomial,"I have the following definite integral to solve $$\int_{a_1}^{a_2} dx \sqrt{(a_1-x)(x-a_2)(x-c)(x-\bar{c})},$$ where $a_1$ is real and negative, $a_2$ is real and positive and $c$ is a complex number and $\bar{c}$ its complex conjugate. I am sure that the result involves the eliptic functions and since the boundaries are fixed it can probably be reduced to only having complete eliptic integrals. I tried to use Formulas like 259.03 together with 341.05 and (for $R_1$ needed) 361.54. in the book https://link.springer.com/book/10.1007%2F978-3-642-65138-0 by Byrd and Freeman.
In order to use this formula I rewrote the integral as $$\int_{a_1}^{a_2} dx \frac{P(x)}{\sqrt{P(x)}}, \quad\text{where}\, P(x) = (a_1-x)(x-a_2)(x-c)(x-\bar{c}).$$ However, I was unable to find a pleasing form and I was wondering whether there is a simpler approach than starting with the quite general formula in 259.03.
For example I found Integrals of the square root of a cubic polynomial which unfortunately only covers the cubic case. Any ideas are highly appreciated!","['integration', 'elliptic-integrals']"
4125425,Limit Problem using L'Hospital's Rule Transformation,"Find $\lim _{x \rightarrow 0}\left[x /\left(e^{x}-1\right)\right]^{1 / x}$ It seems obvious that the initial step should be taken using an exponential function, where the new form yields $\frac{\ln(\frac{x}{e^x-1})}{x}$ . However, L'Hospital's rule does not work in this case. Are there any possible alternatives to transform the given expression?","['limits', 'derivatives']"
4125466,"Find the values of $\sinh 2x$ and $\cosh 2x$, if $\sinh 3x = 3/4$","First, use $\sinh 3x = 3/4$ to find the value of $\sinh x$ . Then, we know that $$\cosh^2 x - \sinh^2 x = 1$$ By substituting the value for $\sinh x$ in this equation we can find the value for $\cosh x$ . Then use the $\cosh x$ and $\sinh x$ to find $\sinh 2x$ and $\cosh 2x$ by substituting in $ \sinh 2x = 2 \sinh x \cosh x$ $ \cosh 2x = 2 \cosh^2x - 1 $ Right now I can't seem to find a way to obtain the $\sinh x$ value from the given $\sinh 3x$ .",['trigonometry']
4125468,Manipulation of Variance and quick way to show $\operatorname{Var}(X\mid X=x) = 0 $,"It is quite intuitive that $\operatorname{Var}(X\mid X=x) = 0 $ as the variable is no longer ""random"". It is also fairly simple to do this from the definition of Variance showing that $\mathbb{E}[X^2] = \mathbb{E}[X]^2 = x^2$ But I was wondering how in general we could manipulate conditioned variance and if there is another way to show this :) Thanks","['statistics', 'variance', 'probability']"
4125472,"Let $g\in G$ such that $o(g)=n<\infty$ (of a finite order). Show that $o(g^r)=\frac{n}{(n,r)}, 0<r<n$","Let $g\in G$ such that $o(g)=n<\infty$ (of a finite order). Show that $o(g^r)=\frac{n}{(n,r)}, 0<r<n$ where $(n,r)$ is gcd $(n,r)$ . First of all, it is quite easy to verify that $(g^r)^{\dfrac{n}{(n,r)}}=e$ . Then, let $s>0$ such that $g^{rs}=e$ . To show that $o(g^r)=s$ , we have to show that $s$ is the smallest integer such that $g^{rs}=e$ . Clearly, $n$ must divide $rs$ . My problem is how to show that $s=\frac{n}{(n,r)}$ . I was told to use the prime number decomposition, but I don't really see how to get the desired result. If someone could help, I would really appreciate it. Thank you in advance.","['group-theory', 'prime-numbers']"
4125487,How to use the Backwards Euler Method,"Given the ODE $\frac{dy}{dt} = f(t,y)$ and the function $f(y) = -y^3$ , with the initial condition $y(0)=1$ , I want to use the backward Euler Method with $h = \frac{1}{2}$ , combined with the Newton-Rapson method to approximate $y$ at $t = 2$ . This ODE is seperable and an exact solution can be found, which I found to be $y = \sqrt{\frac{1}{2t+1}}.$ Therefore at $t = 2$ the exact solution would be $\sqrt{\frac{1}{5}}$ . However, I want to know how to use these two numerical methods to approximate a solution. So far, I used the Backward Euler Method as follows: $$y_{n+1} \approx y_n + hf(t_{n+1}, y_{n+1})$$ $$y_{n+1} \approx 1 - \frac{1}{2}y_{n+1}^3$$ This forms the cubic equation $y_{n+1}^3 + 2y_{n+1} - 2 \approx 0$ which can be solved via Newton-Raphson, and this gives me a value of $y_{n+1} = 0.7709$ . But where can I go from here? Have I made a mistake? Any guidance would be much appreciated.","['numerical-methods', 'approximation', 'ordinary-differential-equations']"
4125528,$\sum_{n=1}^{\infty}\frac{(2^n + 3^n)\sin(n)}{2^n + n^2\cdot3^n}$,Proving the convergence of a series by Weierstrass M-test. $$\sum_{n=1}^{\infty}\frac{(2^n + 3^n)\sin(n)}{2^n + n^2\cdot3^n}.$$ $$\frac{(2^n + 3^n)\sin(n)}{2^n + n^2\cdot3^n} \leq \frac{2^n + 3^n}{2^n + n^2 \cdot 3^n} = \frac{1 + \left(\frac{3}{2}\right)^n}{1 + n^2 \left(\frac{3}{2}\right)^n}.$$ Further I got stuck how to evaluate and get convergence.,"['convergence-divergence', 'analysis', 'sequences-and-series']"
4125547,Interchange of limit and derivative [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose we have two sequences of functions, $f_n(x)$ , $g_n(x)$ ., where the following relation holds $$
\forall n, \hspace{5mm} \frac{d}{dx}f_n(x) = g_n(x)
$$ Also $$
\lim_{n \rightarrow \infty} f_n(x) = f(x)
$$ We know that $f_n$ and $f$ are continuous functions. Under what conditions we are allowed to say $$
\lim_{n \rightarrow \infty} g_n(x) = \frac{d}{dx}f(x)
$$","['limits', 'derivatives', 'analysis', 'real-analysis']"
4125556,How to plot a complex function?,"We cannot plot graph of a complex function $f:\mathbb {C\to C}$ as it requires $4$ dimensions.But we can show how the mapping transforms the domain plane into image plane.We can draw grid lines parallel and perpendicular to $x$ -axis and see how the grid lines are modified.But often it becomes tedious task to plot these kind of diagrams.Is there any systematic procedure to draw such figures without help of any software? For example , $z^2,z^3,\sin(z),\log(z),\exp(z)$ etc. I want a method to visualize any given function.Is there a way out?","['visualization', 'graphing-functions', 'complex-analysis', 'intuition', 'complex-numbers']"
4125568,Galois group of the polynomial $x^n+x^{n-1} +⋯+x^2+x−1$,"As we already know, the following polynomial is irreducible over $\mathbb Q[x]$ : $$x^n + x^{n-1} + \cdots + x^2 + x - 1 = \frac{x^{n+1} - 2x + 1 }{ x-1}.$$ By Descartes' rule of signs, it has only 1 positive real root.
It seems, though I do not know how to prove it yet, that there is only 1 real root if $n$ is odd and 2 real roots of opposite signs if $n$ is even. Computational tests, conducted with GAP, suggest that the Galois group
of this polynomial is $S_n$ . How we can actually prove it? Any help is greatly appreciated.","['galois-theory', 'group-theory', 'irreducible-polynomials', 'polynomials']"
4125721,Partial likelihood in Cox's proportional hazards model,"I'm reading about Cox's proportional hazards approach to (continuous) survival analysis and I'm finding it difficult to understand his argument for the derivation of the partial likelihood in his 1972 paper . He states on page 6 of the pdf in the link that the probability to observe a failure at time $t_{(i)}$ on the individual $i$ (given that there is exactly one failure at $t_{(i)}$ ) is equal to $$\frac{\exp(z_{(i)}\beta)}{\sum_{l\in R(t_{(l)})}\exp(z_{(l)}\beta)}.$$ Now I understand that the baseline hazard in the denominator is supposed to cancel against the baseline hazardin the numerator. However, the denominator itself surprises me, because it seems that this denominator is associated to the expected number of failures, rather than the probability to observe exactly one failure (which is what I would expect from the conditional probability that this partial likelihood is supposed to describe). I'm wondering if, instead, I should view this partial likelihood as some approximation to the exact problem (and if so, why this approximation is justified). Also, I'm curious if it's still necessary with modern computers to actually split the full likelihood into partial likelihoods or if this is something that was mostly useful in the 70s.","['statistical-inference', 'statistics', 'maximum-likelihood']"
4125791,"Prove that for any real values of $c$, the equation $x(x^2-1)(x^2-10)=c$ can't have $5$ integer solutions.","Prove that for any real values of $c$ , the equation $x(x^2-1)(x^2-10)=c$ can't have $5$ integer solutions. My teacher gave this question at the end of the class as a ""challenging"" one. I have no idea how to approach it. He also gave a hint that opening the brackets might help, so I did so and got the following: $$ x^5-11x^3+10x-c=0$$ I don't see how this is helping, and finding roots of this equation in terms of $c$ is just tedious. Any hints or help would be most appreciated! The question is to be done using Descarte's rule of signs. Other methods are welcome too.","['contest-math', 'algebra-precalculus', 'polynomials']"
4125797,Seeking an example of continuous function that has no integrable derivative,"I am seeking for a function $[0,1]\to \mathbb{R}$ that is continuous and has derivative almost everywhere, but this derivative is not integrable niether on sense of improper integrals. For instance, $f(x)=\sqrt{x}$ is s.t. $f'(x)=\dfrac{1}{2\sqrt{x}}$ and $f'$ is Lebesgue integrable. I am seeking an example s.t. $f'(x)$ is not Lebesgue integrable. This is the difference on the question What is an example that a function is differentiable but derivative is not Riemann integrable Thank you in advance.","['continuity', 'measure-theory', 'derivatives', 'examples-counterexamples']"
4125874,Lawvere theories with two generic objects,"What is the simplest example of a category with finite products and two objects $X,Y$ such that $X$ and $Y$ are not isomorphic, $X$ is isomorphic to $Y^n$ for some $n \in \mathbb{N}$ , $Y$ is isomorphic to $X^m$ for some $m \in \mathbb{N}$ ? This should demonstrate that the generic object of a Lawvere theory is not uniquely determined. Rather, it shows that the correct definition of a Lawvere theory is a pair $(\mathcal{L},X)$ consisting of a category with finite products and an object $X \in \mathcal{L}$ such that every object of $ \mathcal{L}$ is isomorphic to some $X^n$ . It is not enough to state the existence of such an object as a property. I am not sure if this answers the question (does it?), but in any case there must be simpler examples (since here any category can be chosen). Edit. In fact, a theorem of Ketonen implies that there are boolean algebras $A$ such that $A \cong A^4$ and $A \not\cong A^2$ , and then $X := A$ and $Y := X^2$ are a counterexample. I am looking for an answer which either explains this algebra $A$ in a self-contained way and gives a proof of $A \not\cong A^2$ and $A \cong A^4$ , or (perhaps even) better gives an example in a more elementary category.","['abstract-algebra', 'examples-counterexamples', 'category-theory']"
4125895,Is positive definite function (dynamical systems) always convex?,"Let $f : \mathbb{R}^n \to \mathbb{R}$ , $n \in \mathbb{N}$ , be such that $f \in C^1(\mathbb{R}^n)$ , $f(0) = 0$ , $f(x) > 0$ for all $x \in \mathbb{R}^n \setminus\{0\}$ . Is it true that such function must be convex on a neighborhood of the origin? If not, can you come up with a counterexample? Considering the simplest scenario when $n = 1$ , I expect that a smooth function with infinite amount of minima approaching the origin should serve as a counterexample. But I am unable to construct an explicit formula for the function.","['multivariable-calculus', 'convex-analysis', 'real-analysis']"
4125903,Separability of unit ball in strong operator topology,Let $X$ be a separable Banach space.  Let $B_1(X)$ be the set of all bounded linear operators $X \to X$ with operator norm $\leq 1$ .  Does $B_1(X)$ have to be separable in the strong operator topology?,"['banach-spaces', 'operator-theory', 'functional-analysis']"
4125913,Whether a group containing a free group is also a free group,"Suppose $G$ is a group generated by two elements $s,t$ . Suppose $H$ is a subgroup of $G$ such that $H= \langle s^k,t^k \rangle$ is a free group where $k$ is some integer not equal to $1$ . Does it imply that $G$ is also a free group? Note: I know there are examples of non free group with a free subgroup. But in those examples, the subgroup had different generators.
But I want to know about this particular case where the generators of the subgroup has some power of the generators of the mother group.","['combinatorial-group-theory', 'finitely-generated', 'group-theory', 'free-groups']"
4125936,Finding $\mathbb{E}\left[\hat{\sigma}^2\right] = \frac{\sum_{i=1}^{n}{\left(\ln(x)-\mu\right)^2}}{n}$ where $X \sim \mathrm{LogNorm}$,"I'm tasked with finding $\mathbb{E}\left(\hat{\sigma}^2\right)$ where $\hat{\sigma}^2 = \frac{\sum_{i=1}^{n}{\left(\ln(x)-\mu\right)^2}}{n}$ , $x \sim \mathrm{Lognorm}(\mu, \sigma^{2}$ ) . So far I get, $$\frac{1}{n}\sum_{i=1}^{n}{ \mathbb{E}\left((\ln(x)-\mu)^2\right)}$$ and can simplify to $$\frac{1}{n}\sum_{i=1}^{n}{\sigma^2} \\ \frac{n\sigma^2}{n} = \sigma^2$$ but I know this isn't right because the MLE estimate of $\sigma^2$ shouldn't be unbiased. Can anyone assist on what I did wrong?","['statistical-inference', 'statistics', 'probability-distributions', 'probability-theory', 'probability']"
4125940,"Let $(X, \mathcal{M})$ be a measurable space and $\mu : \mathcal{M} → [0, \infty]$. Show that $\mu$ is a measure.","Let $(X, \mathcal{M})$ be a measurable space and $\mu : \mathcal{M} → [0, \infty]$ such that (a) $µ (A) <\infty$ for some $A \in \mathcal{M}$ (b) $µ(A \cup B) = \mu(A) + \mu(B)$ if $A, B \in \mathcal{M}$ and $A \cap B = \emptyset$ , and (c) $\mu (\bigcup_{n=1}^{\infty} A_{n}) \leq \sum_{n=1}^{\infty}\mu(A_n) $ if $A_n \in \mathcal{M}$ for $n=1,2,3,...$ Show that $\mu$ is a measure. In my context, we define a measure as follows: Definition: A function µ is called a positive measure, defined in a $\sigma$ -algebra $\mathcal{M}$ with values in $[0, \infty]$ and which is countably additive. This means that if $\{E_i\}$ is a disjoint countable collection of elements of $\mathcal{M}$ , then $\mu (\bigcup_{i=1}^{\infty} E_{i}) = \sum_{i=1}^{\infty}\mu(E_i)$ . To avoid trivial cases, we will further assume that $\mu (A) < \infty $ for at least one $A \in \mathcal{M}$ My attempt: From the definition, it only remains for me to prove that $\mu$ is countably additive. To do this, let $\{A_n\}$ be a disjoint countable collection of elements of $\mathcal{M}$ . Then, $$ \bigcup_{n=1}^{\infty}A_n  \supset \bigcup_{n=1}^{N}A_n$$ And so $$\mu \left(\bigcup_{n=1}^{\infty}A_n \right)  \geq \mu \left(\bigcup_{n=1}^{N}A_n \right) = \sum_{n=1}^{N}\mu(A_n)$$ Since the left side of this inequality is independent of $N$ , we have $$\mu \left(\bigcup_{n=1}^{\infty}A_n \right)  \geq \sum_{n=1}^{\infty}\mu(A_n)$$ For part (c) we have the equality. First of all I want to know if there are any errors in my reasoning and, on the other hand, I don't see where I used the hypothesis of part (b) in my proof; This really does have me worried. I appreciate any help you can give me.","['measurable-sets', 'measure-theory', 'solution-verification']"
4125957,Is there a name for this ordinal set mapping?,"I have an integer $N$ . I am working with the set $\{1,...N\}$ and it's powerset $\mathcal{P}(\{1,...N\})$ . I want to use define a function, $f$ , on the sets $p\in\mathcal{P}(\{1,...N\})$ which represents what seems to me to be maybe the most ``natural way"" of ordering the sets. But I am unsure how to define it rigorously and if it already has a name (it feels like it probably does). As an example, if $N=3$ , my function $f$ should map the subsets of $\{1,2,3\}$ to integers as follows: $$f(\emptyset)=0$$ $$f(\{1\})=1,f(\{2\})=2,f(\{3\})=3,$$ $$f(\{1,2\})=4,f(\{1,3\})=5,f(\{2,3\})=6,$$ $$f(\{1,2,3\})=7$$ Is there a name for an ordering of this sort. i.e. can I just say something like ``let $f$ be the {BLANK} ordering on $\mathcal{P}(\{1,...N\})$ "" to define the $f$ above or something similar?","['elementary-set-theory', 'order-theory', 'terminology']"
4125959,On Duhamel's principle for the inhomogeneous heat equation with irregular data and regularity of solution,"Consider the following inhomogenous initial value problem for the heat equation, that is $\begin{align}
\dot{u}-u^{\prime \prime}&=f(u) &&\quad x \in \mathbb R, \;t>0, \\
u(\cdot,0)&=u_0(\cdot) &&\quad x \in \mathbb R,
\end{align}$ where $f(u) \in L^{\infty}(\mathbb R \times (0,\infty))$ and $u_0$ denotes a non negative finite measure. QUESTIONS: Can I apply Duhamel's principle in order to obtain the solution of the above problem by means of the heat kernel despite the fact that $f$ belongs only in $L^\infty(\mathbb R)$ and $u_0$ is just a measure? The only times I 've seen this principle is when deriving classical solutions for the heat or the wave equation [see Evans, PDEs]. However, in these cases much more regularity on both $f$ and $u_0$ is assumed. Hence, I wonder if Duhamel's principle is still valid assuming less regularity. Is there any good reference in order to obtain more insight on Duhamel's principle? Suppose that the answer in the first item is true. Then we can write $\begin{equation}
u(x,t)=K\star u_0 +\int_0^t K(t-s)\star f(u(s)) \;ds,
\end{equation}$ where $K$ stands for the heat kernel. What would the regularity of the solution be now? I can see that the integral term will provide some continuity in time since the kernel is extremely smooth (besides $t=0$ ) but I do not know how to interpret the other term... I apologize if these questions are silly or elementary but only now I realized that I have never seen a derivation of Duhamel's principle and thus I am unsure on when I can actually use it or not. Any help will be much appreciated! Many thanks in advance.","['measure-theory', 'convolution', 'heat-equation', 'partial-differential-equations', 'regularity-theory-of-pdes']"
4125966,Solving $P_y\frac{\sin{\theta_1}}{\sqrt{1-\sin^2{\theta_1}}}+F_y\frac{m\sin{\theta_1}}{\sqrt{1-m^2\sin^2{\theta_1}}}=F_x - P_x$ for $\sin\theta_1$,"I am simulating the phenomenon of refraction, ruled by Snell's law: $$\sin\theta_1\cdot n_1 = \sin\theta_2\cdot n_2$$ For this question, is enough to know that relationship between both angles. $n_1$ and $n_2$ are given constants. The thick black line is called ""interface"". What I am trying to achieve: Based on user input, I know the coordinates of the point $P$ and point $F$ is fixed. Now, to draw the lines shown above, I need to determine where is the point $I$ . Specifically, I need to know its $x$ coordinate: the horizontal distance between $P$ and $I$ . How I tried to do it: Since I know the horizontal distance between $P$ and $F$ , I can equate it to the sum of the opposing legs of two right triangles, formed by angles $\theta_1$ , $\theta_2$ and the vertical thin line, as shown below: $$P_y\cdot\tan{\theta_1}+F_y\cdot\tan{\theta_2}=F_x - P_x$$ (Assuming absolute values and the interface is located at $y=0$ ). Using Snell's law, I found that $\sin{\theta_2}=\frac{n_1}{n_2}\sin{\theta_1}$ . Then, I tried to substitute it in the equation above. To simplify the expression, let $m=\frac{n_1}{n_2}$ . The issue In terms of $\sin{\theta_1}$ , the equation becomes: $$P_y\frac{\sin{\theta_1}}{\sqrt{1-\sin^2{\theta_1}}}+F_y\frac{m\sin{\theta_1}}{\sqrt{1-m^2\sin^2{\theta_1}}}=F_x - P_x$$ And I'm stuck here. WolframAlpha could not help, and I don't know any numerical method good enough to determine $\sin{\theta_1}$ . How can I solve that equation for $\sin{\theta_1}$ ?","['computational-mathematics', 'algebra-precalculus', 'trigonometry', 'numerical-methods']"
4126002,Volume form on compact Lie group,"Suppose $G$ is a compact Lie group, $\omega \in \Omega^n(G)$ is a left-invariant volume form and let $i:G \rightarrow G$ be the inversion $i(g)=g^{-1}$ . I have previously shown that $r^*_g \omega$ is left-invariant as well, where $r_g$ is multiplication on the right and so $r^*_g \omega=f(g)\omega$ . Moreover, since this $f$ is a homomorphism, by compactness, it follows that $f(g)=1$ . I also need to prove that $i^* \omega = \omega$ or $-\omega$ . I think I need to use the fact that $i^* \omega$ is also left-invariant. I can see this, because $$r^*_h i^* \omega=(i \circ r_h)^* \omega=(l_{h^{-1}}\circ i)^* \omega=i^* l_{h^{-1}}^* \omega=i^* \omega$$ since $\omega$ is left-invariant.
How can I use this to show $i^* \omega = \omega$ or $-\omega$ ?","['smooth-manifolds', 'pullback', 'lie-groups', 'differential-forms', 'differential-geometry']"
4126007,"Evaluating $\int_{0}^{\infty} \left( \text{coth} (x) - x \text{csch}^2 (x) \right) \left( \ln \left( \frac{4 \pi^2}{x^2} + 1 \right) \right) \, dx$","How can the following improper integral be evaluated? $$\int_{0}^{\infty} \left( \text{coth} (x) - x \text{csch}^2 (x) \right) \left( \ln \left( \frac{4 \pi^2}{x^2} + 1 \right) \right) \, dx$$ or alternatively: $$\int_{0}^{\infty} \frac{x \text{coth} (x) - 1}{x^2 (2 \pi + i x)} \, dx$$ Note: I am only really interested in the imaginary component of the second integral. I've attempted multiple methods, all of which seeming unsuccessful, however, I believe contour integration may be the solution to the second integral above, which would also easily allow me to get the integral I'm interested in.","['integration', 'improper-integrals', 'calculus', 'contour-integration', 'closed-form']"
4126052,Computing degree of a map,"Define $f:\mathbb{CP}^1 \rightarrow \mathbb{CP}^1$ by $$f([z_0,z_1])=[p(z_0,z_1),z_1^n],$$ where $p(z_0,z_1)=z_0^n+c_{n-1}z_0^{n-1}z_1+ \dots c_1z_0 z_1^{n-1}+c_0 z_1^n$ is an arbitrary homogeneous polynomial. I would like to compute the degree of $f$ . I think I need to use the following: for a regular point $x \in \mathbb{CP}^1$ , the differential $df_x:T_x\mathbb{CP}^1 \rightarrow T_{f(x)}\mathbb{CP}^1$ is orientation preserving. I am not entirely sure how to show this. I know that $df_x$ is orientation preserving if its Jacobian is positive. However, can I compute it in charts? For instance, in the chart $U_0=\{[z_0,z_1] \mid z_0 \neq 0\}=\{[1,z] \mid z\in \mathbb{C}\}$ , we have $$f([1,z])=[p(1,z),z^n],$$ whose Jacobian is $0$ .","['projective-geometry', 'differential-geometry', 'smooth-manifolds', 'manifolds', 'projective-space']"
4126066,Proving that $x(x^2-1)(x^2-10)=c$ cannot have five integer solutions for any real $c$,"I found this question that caught my attention at MSE and  I did a solution, but I suspect something is wrong with the solution. Original problem says: Prove that for any real values of $c$ , the equation $x(x^2-1)(x^2-10)=c$ can't have $5$ integer solutions. The things I have done: I find stronger (?) results in this answer: If $c=0$ , then we have $3$ integer solutions. If $c>0$ or $c<0$ , then we have only $1$ integer solution. This means, The number of integer solutions is always less than $4$ . Let, $$f(x)=x(x^2-1)(x^2-10)$$ and $$x(x^2-1)(x^2-10)=c$$ where $c\in\mathbb R, x\in\mathbb Z$ . In fact we need only $c\in\mathbb Z$ . Because, if $x\in\mathbb Z$ then $c\in\mathbb Z$ . We see that $c=0$ is trivial. $\underline{\text{Case}-1:~c>0}$ $$\begin{align}&x(x^2-1)(x^2-10)>0 ,x\in\mathbb Z \\ &\iff x\in \left\{-3, -2\right\}∪[4,+\infty)\end{align}$$ Then suppose that, $x_1≥4, x_1\in\mathbb Z $ is a solution. If $x_2>x_1≥4$ then $f(x)$ is strictly increasing and if $4≤x_2<x_1$ , then $f(x)$ is strictly decreasing. This means, if $x_1≥4$ , then we have one positive integer solution. Then, we see that $f(-3)<f(-2)<f(4)$ and $f(x)$ is strictly increasing for $x≥4$ . This implies, if $c>0$ , then we have only one integer solution. $\underline{\text{Case}-2:~c<0}$ Let's multiply both sides of the equation by $(-1).$ $$-x(x^2-1)(x^2-10)=-c, c<0$$ Let, $-x=t$ then $$t(t^2-1)(t^2-10)=-c,-c>0$$ This means, for $c<0$ we have also one integer solution. Thus, we conclude the number of integer solutions is always less than $4$ . Does my solution contain any errors? Please, don't post the correct solution. Thank you for reviewing.","['elementary-number-theory', 'proof-writing', 'solution-verification', 'polynomials', 'algebra-precalculus']"
4126090,$f\in L^1\implies \lim_{n\rightarrow\infty}f(n^2 x)=0$ a.e. $x\in\mathbb{R}$?,"Can someone provide a hint for the following: $f\in L^1(\mathbb{R})\implies \lim_{n\rightarrow\infty}f(n^2 x)=0$ a.e. $x\in\mathbb{R}$ I can't really make heads or tails of it. Something makes me think convergence in measure; or even a duality argument, but the quantification on $x$ for which this is satisfied is throwing me. I am not sure how it arises. This is from an old qualifying exam.","['measure-theory', 'lp-spaces', 'analysis', 'real-analysis']"
4126183,Where is my mistake $\int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx$,"Edit As commented bellow by @Donald Splutterwit and @ Elliot Yu, it seems that my computation is numerically correct and the post is wrong! I also added a corollary from this computation. I saw the following statement here $$\boxed{\int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx=\frac{7}{4}\zeta(3)}$$ And I wanted to proof it. My approach was the following $$I=\int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx=\int_{0}^{1}\frac{\log(1-x)\log[(1-x)(1+x)]}{x}dx$$ $$=\int_{0}^{1}\frac{\log(1-x)\log(1-x)}{x}dx+\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx$$ $$=\underbrace{\int_{0}^{1}\frac{\log^2(1-x)}{x}dx}_{I_{1}}+\underbrace{\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx}_{I_{2}}$$ $$I_{1}=\int_{0}^{1}\frac{\log^2(1-x)}{x}dx$$ let $1-x=t$ $$I_{1}=\int_{0}^{1}\frac{\log^2(t)}{1-t} dt$$ $$=\int_{0}^{1}\log^2(t)\sum_{k=0}^{\infty}t^kdt$$ $$=\sum_{k=0}^{\infty}\int_{0}^{1}t^k\log^2(t)dt$$ Now use the fact that $$\boxed{\int_{0}^{1}x^m \log^n(x)dx=\frac{(-1)^{n}n! }{(m+1)^{n+1}}}$$ for $$n=2 \,\,\text{and} \,\, m=k $$ $$I_{1}=\sum_{k=0}^{\infty}\frac{(-1)^{2}2! }{(k+1)^{3}}$$ $$\boxed{\int_{0}^{1}\frac{\log^2(1-x)}{x}dx=2\zeta(3)}$$ The second integral is a little trickier $$I_{2}=\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx$$ Observe the following $$\Big(\log(1-x)+\log(1+x) \Big)^2=\log^2(1-x)+2\log(1-x)\log(1+x)+\log^2(1+x)$$ $$\log(1-x)\log(1+x)=\frac{\Big(\log[(1-x)(1+x)] \Big)^2-\log^2(1-x)-\log^2(1+x)}{2}$$ $$\log(1-x)\log(1+x)=\frac{\log^2(1-x^2)-\log^2(1-x)-\log^2(1+x)}{2}$$ dividing both sides by $x$ and integrating from $0$ to $1$ $$\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=\frac{1}{2}\int_{0}^{1}\frac{\log^2(1-x^2)}{x}dx-\frac{1}{2}\int_{0}^{1}\frac{\log^2(1-x)}{x}dx-\frac{1}{2}\int_{0}^{1}\frac{\log^2(1+x)}{x}dx$$ Now we have to evaluate the three integrals on the RHS and we are done. I´ll state only the value of each  integral since their proof is easy to find on this forum. $$\int_{0}^{1}\frac{\log^2(1-x^2)}{x}dx=\zeta(3)$$ $$\int_{0}^{1}\frac{\log^2(1-x)}{x}dx=2\zeta(3)$$ $$\int_{0}^{1}\frac{\log^2(1+x)}{x}dx=\frac{1}{4}\zeta(3)$$ Putting all together we get $$I_{2}=\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=-\frac{5}{8}\zeta(3)$$ Now summing the results of $I_{1}$ and $I_{2}$ we get the final result which differs form the above statement! $$\int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx=2\zeta(3)-\frac{5}{8}\zeta(3)=\frac{11}{8}\zeta(3)$$ A Corollary We just computed the integral $$\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=-\frac{5}{8}\zeta(3)$$ On the other hand we can show that this integral equals $$\sum_{n=1}^{\infty}\frac{(-1)^n H_{n}}{n^2}$$ and conclude that $$\sum_{n=1}^{\infty}\frac{(-1)^{n-1} H_{n}}{n^2}=\frac{5}{8}\zeta(3)$$ Proof: Expanding $\log(1+x)$ in Taylor series we get $$I=\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\int_{0}^{1}x^{n-1}\log(1-x)dx$$ Integrating by parts we get: $$I=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\bigg\{\frac{\log(1-x)(x^n-1)}{n}\Big|_{0}^{1}+\frac{1}{n}\int_{0}^{1}\frac{x^n-1}{1-x}dx \bigg\}$$ $$I=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\bigg\{-\frac{1}{n}\int_{0}^{1}\frac{1-x^n}{1-x}dx \bigg\}$$ $$I=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\bigg\{-\frac{1}{n}H_{n} \bigg\}$$ $$\boxed{\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=-\sum_{n=1}^{\infty}\frac{(-1)^{n-1}H_{n} }{n^2}}$$ and therefore $$\boxed{\sum_{n=1}^{\infty}\frac{(-1)^{n-1} H_{n}}{n^2}=\frac{5}{8}\zeta(3)}$$","['integration', 'harmonic-numbers', 'zeta-functions', 'sequences-and-series']"
4126189,Proof of Cayley-Hamilton using Krylov subspaces,"I came up with another proof of the Cayley-Hamilton Theorem.  Is this new?  The proof is by induction over the dimension of the underlying vector space. Let $v \in \mathbb F^n \setminus \{0\}$ .  Consider the Krylov subspaces $$ K_j = \text{span} \{v, Av, \dots, A^{j-1} v\} .$$ Let $$j_0 = \min\{j \ge 1 : K_j = K_{j+1}\} .$$ Case 1: $j_0 < n$ .  Then $K_{j_0}$ is an invariant subspace for $A$ , so with respect to a basis whose first $j_0$ elements are in $K_{j_0}$ , the matrix is a block upper triangular matrix.  Now the result follows by the inductive hypothesis on each of the diagonal blocks. Case 2: $j_0 = n$ .  Then $K_n = \mathbb F^n$ , and $\{v, Av,\dots,A^{n-1}v\}$ is a basis of $\mathbb F^n$ .  It follows that there exists $a_0, a_1, \dots, a_{n-1} \in \mathbb F$ such that $$ A^n v = -a_0 v - a_1 Av - a_2 A^2 v - \cdots - a_{n-1} A^{n-1} v .$$ That is, setting $$p(\lambda) = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots + a_0,$$ we have $$ p(A) v = 0 .$$ For any vector $w \in \mathbb F^n$ , we have that $w = q(A) v$ for some polynomial $q$ .  Thus $$ p(A) w = p(A) q(A) v = q(A) p(A) v = 0 .$$ Hence $$ p(A) = 0 .$$ Finally with respect to the basis $\{v, Av,\dots,A^{n-1}v\}$ , the matrix $A$ has the form of the companion matrix: $$ \begin{bmatrix} 0 & 0 & 0 & \cdots & 0 & 0 & -a_0 \\
                   1 & 0 & 0 & \cdots & 0 & 0 & -a_1 \\
                   0 & 1 & 0 & \cdots & 0 & 0 & -a_2 \\
                   \vdots & \vdots & \vdots & & \vdots & \vdots & \vdots \\
                   0 & 0 & 0 & \cdots & 0 & 0 & -a_{n-3} \\
                   0 & 0 & 0 & \cdots & 1 & 0 & -a_{n-2} \\
                   0 & 0 & 0 & \cdots & 0 & 1 & -a_{n-1}
\end{bmatrix} ,$$ and it is well known that the characteristic polynomial of the companion matrix is given by $$ p(\lambda) = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots + a_0. $$","['cayley-hamilton', 'linear-algebra', 'reference-request']"
4126191,How can you show that a tilted parabola is not the graph of a function?,"Consider the parabola that is the graph of $f(x) = x^2$ , and suppose we rotate this parabola by an angle $\theta$ . Unless this $\theta$ is a multiple of $\pi$ radians, the result will not be the graph of some function. How can you show this? Or a slightly tougher question: in terms of a variable $\theta$ how do you exhibit two points on the rotated parabola with the same $x$ -coordinate? I can figure this out, but I want to know how other people would tackle this problem and the minimal machinery necessary to do so. This would make a good challenge question for a motivated student, and I would like to get ideas for how they might approach it or how I could guide them towards a fruitful approach. (I'll update the tags in response to answers)","['algebra-precalculus', 'graphing-functions', 'rotations']"
4126194,Inverse of Definite Integrals?,"We know that the inverse of an integral is the derivative, but what happens if the integral is definite (meaning: it has bounds/limits from-to)? For example we have the function $g(x) = \int f(x) dx$ , then to find $g^{-1}(x)$ we take the inverse of integral which is derivative and about this part im unsure, we probably take the inverse of $f(x)$ i guess (meanwhile $f^{-1}(x)$ , also correct me if im wrong about this part)... In total, we have $g^{-1}(x) = \frac{d(f^{-1}(x))}{dx}$ , or if im wrong, then it is $g^{-1}(x) = \frac{d(f(x))}{dx}$ But what if $g(x) = \int_a^b f(x) dx$ , and then what is $g^{-1}(x)$ ? If there is a missing information in my question that is necessary to be put, just tell me so i add it.","['integration', 'functions', 'derivatives', 'inverse']"
4126196,The ODE $y''(x)=\sinh(x)-3y'(x)-2y(x)$,"I am trying to solve the differential equation that is in the title as a System of first order ode. My Approach: $\frac{d}{dx} \left(\begin{array}{c} y \\ y' \end{array}\right)=\left(\begin{array}{c} y' \\ \sinh(x)-3y'-2y \end{array}\right)=$ $\left( \begin{array}{rrr}
0 & 1 \\ 
-2 & -3 \\
\end{array}\right)\left(\begin{array}{c} y \\ y' \end{array}\right)+\left(\begin{array}{c} 0 \\ \sinh(x) \end{array}\right)$ Then I calculate the characteristic polynomial of the coefficient matrix, which leads to the eigenvalues $\lambda_1=-2,\lambda_2=-1$ . Let A denote the matrix above.
Calculating the matrix exponential, I get $e^{Ax}$ = $\frac{1}{e^{2x}} \left( \begin{array}{rrr}
2e^x-1 & e^x-1 \\ 
-2e^x+2 & -e^x+2 \\
\end{array}\right)$ Now I am variating the parameters and get $y(x)=\frac{1}{e^{2x}} \left( \begin{array}{rrr}
2e^x-1 & e^x-1 \\ 
-2e^x+2 & -e^x+2 \\
\end{array}\right)y_0+\frac{1}{e^{2x}} \left( \begin{array}{rrr}
2e^x-1 & e^x-1 \\ 
-2e^x+2 & -e^x+2 \\
\end{array}\right) \int_0^s  \left(\begin{array}{c} \sinh(x)(e^{-s}-1) \\ \sinh(x)(-e^{-s}+2) \end{array}\right)ds$ = $y(x)=\frac{1}{e^{2x}} \left( \begin{array}{rrr}
2e^x-1 & e^x-1 \\ 
-2e^x+2 & -e^x+2 \\
\end{array}\right)y_0+\frac{1}{e^{2x}} \left( \begin{array}{rrr}
2e^x-1 & e^x-1 \\ 
-2e^x+2 & -e^x+2 \\
\end{array}\right)  \left(\begin{array}{c} \frac{1}{4}e^{-2x}(e^{2x}(2x+3)-2e^x-2e^{3x}+1+C_1) \\ \frac{-x}{2} -\frac{e^{-2x}}{4}+e^{-x}+e^{x} +C_2\end{array}\right)$ My Questions are: In the exercise description there was no value for $y_0$ , is there a way to find the value for it?
Is my calculation correct (does it seem correct) or are there any mistakes?","['analysis', 'ordinary-differential-equations']"
4126238,$f$ is uniformly continuous $\implies |f(x)| < C + D|x|$,"Prove $f$ is uniformly continuous $\implies$ there exist $C, D$ such that $|f(x)| < C + D|x|$ . Proof below.  Please verify or critique. By definition of uniform continuity, there exists $\delta > 0$ such that $|x_a - x_b| \leq \delta \implies |f(x_a)- f(x_b)| < 1$ .  Choose $D > 1/\delta$ and $C > |f(0)| + D + 1$ . For any $x$ , $|f(x)| - |f(0)| \leq  |f(x) - f(0)| \leq \sum_{0 < j \leq |x/\delta|+1}|f(j\delta) - f((j-1)\delta)| \leq |x/\delta|+1$ , so $|f(x)| \leq |x/\delta| + 1 + |f(0)| < C + D|x|$ .","['solution-verification', 'uniform-continuity', 'real-analysis']"
4126272,"$f(x|\theta)=\frac{\theta}{x^2}I_{(\theta,\infty)}(x)\hspace{0.5cm}\theta\in\mathbb{R}^+$ UMVUE for $h(\theta)=\theta$","Let  X a random sample with density function $$f(x|\theta)=\frac{\theta}{x^2}I_{(\theta,\infty)}(x)\hspace{0.5cm}\theta\in\mathbb{R}^+$$ Find the UMVUE for $h(\theta)=\theta$ $$f_\theta(\vec{x})=\theta^nI_{(0,x_{(1)})}(\theta)\prod_{i=1}^n\frac
{1}{x_i^2}$$ Then $$T(X)=X_{(1)}$$ is sufficient for $\theta$ I am trying to prove that  T is a complete statistic $$F_\theta(x)\,dx= \int_\theta^x f_\theta(x)\,dx = \int_\theta^x \frac{\theta}{x^2}\,dx=\left[-\frac{\theta}{x}\right]_\theta^x=\frac{x-\theta}{x}$$ Then $$F_{X_{(1)}}(y)=1-[1-F_\theta(y)]^n=1-\left[\frac{\theta}{y}\right]^n$$ Then $$f_{X_{(1)}}(y)=n\theta^ny^{-n-1}$$ Let $g(t)$ $$E[g(t)]=\int_\theta^{\infty}g(t)f_{X_{(1)}}=\int_\theta^{\infty}g(t)n\theta^nt^{-n-1}\,dt=n\theta^n\int_\theta^\infty g(t)t^{-n-1}\,dt $$ Then $$0=n\theta^n\int_\theta^\infty g(t)t^{-n-1}\,dt$$ $$0={n^2\theta^{n-1}\int_\theta^\infty g(t)y^{-n-1}\,dt}+n\theta^n\frac{d}{d\theta}\left[\int_\theta^\infty g(t)t^{-n-1}\,dt\right]$$ It follows that $\int_0^\theta g(t)t^{{-n-1}}\;dt=0$ Then $$n\theta^n\frac{d}{d\theta}\left[\int_\theta^\infty g(t)t^{-n-1}\,dt\right]=0$$ I am stuck in this point ... I thougt T was complete but now I am not sure, can you help me please
Thanks","['statistical-inference', 'statistics']"
4126285,Does $\mathbb{N} \times \mathbb{R}$ make sense?,"I know that it is possible to have a Cartesian product for two sets of different cardinalities if the cardinality of both sets is finite. Taking $A = \{1,2\}$ and $ B = \{3,4,5\}$ , $A \times B = \{(1,3),(1,4),(1,5),(2,3),(2,4),(2,5)\}$ . But what if we have one set where it has uncountably infinite elements and one with countably infinite elements? Does $\mathbb{N} \times \mathbb{R}$ make sense? I can understand it geometrically as the graph shown below but I'm not well versed enough with set theory to know if rigorously speaking this creates any problems/is allowed.",['elementary-set-theory']
4126368,Hausdorff Measure of a Compact segment in $\mathbb R$ has infinite hausdorff measure for $s < 1$,"This is probably obvious, but I'm having trouble with it. It is an exercise in Falconer: Show that $\mathcal{H}^s([0,1]) = \infty$ if $s \in [0,1)$ I can see that this should loosely be the case: consider a dyadic partition of $[0,1]$ , then $\sum_i 2^i \left(\frac 1 {2^i}\right)^s$ doesn't converge for $s < 1$ . This however gets me an upper bound on Hausdorff measure. How does one proceed?","['measure-theory', 'hausdorff-measure', 'geometric-measure-theory']"
4126377,Time spent in an interval of Brownian motion,"Consider the standard one-dimensional Brownian motion $B_t$ . Set $$X_t = \int_0^t\mathbf{1}_{[0, 1]}(B_s)ds.$$ Here $\mathbf{1}_{[0, 1]}(\cdot)$ is the indicator function of the interval $[0, 1]$ . $X_t$ denotes the total time spent by Brownian motion in $[0, 1]$ before time $t$ , which can also be written using the Brownian local time as $$X_t = \int_0^1L^a(t)da.$$ Question: Can we compute the distribution of $X_t$ ? If we can't get the specific expression of its density, is there any method to get the decay estimate (lower and upper bound) of its density?","['stochastic-calculus', 'stochastic-processes', 'functional-analysis', 'brownian-motion', 'probability']"
4126387,Integral inequality in proof of irrationality of $\pi$,"I'm trying to follow along with my textbook's proof of $\pi$ being irrational. One step in the proof (which my book takes for granted) confuses me. It states that, for each positive integer $n$ , $$0 < \int\limits_0^\pi \frac{(px-qx^2)^n}{n!}\hspace{3pt}\sin(x) \hspace{3pt}dx < \pi \cdot \frac{p^n}{n!}$$ where $\pi = p/q$ (this is a proof by contradiction). I was able to manipulate the integral, using simple algebra, as follows: $$\frac{q^n}{n!}\int\limits_0^\pi x^n(\pi-x)^n\hspace{3pt}\sin(x) \hspace{3pt}dx$$ Any hints as to how I might prove this inequality from here?","['integration', 'number-theory', 'proof-explanation', 'definite-integrals']"
4126404,composition of homemomerphism and continuous function and limit at $\infty$,"Let $h\colon \Bbb R\to \Bbb R$ be a continuous function and $\lim_{x\rightarrow\infty}h(x)=0$ and $\lim_{x\rightarrow -\infty}h(x)=0.$ Let $f\colon (a,b)\to\Bbb R$ be a homeomoerphism. I do not see why $$\lim_{x\rightarrow a^{+}}(h\circ f)(x)=0 \ \  \text{and} \ \  \lim_{x\rightarrow b^{-}}(h\circ f)(x)=0.$$ Does that correct? What does this tell me in terms of cluster point of $h\circ g$ , that is, it says $0$ is cluster point of $h\circ f$ at $a$ and $b$ ? Is that right? Any help will be appreciated greatly.","['limits', 'calculus', 'limsup-and-liminf', 'real-analysis']"
4126429,Find $E(X_{(1)}\mid T)$ where $T=\sum_{i=1}^n X_i$,"Let $X_1,X_2,\ldots,X_n$ be a random sample with $n\geq 2$ from an exponential distribution. $X_{(1)}=\min(X_1,X_2,\ldots,X_n)$ . Find $E(X_{(1)}\mid T)$ where $T=\sum_{i=1}^n X_i$ . I was able to find the Cdf of $X_{(1)}$ which is \begin{align}
F_{X_{(1)}}(x)&=
\begin{cases}
0, & x<0
\\
1-e^{-n \lambda x}, & x\geq 0
\end{cases}\label{6}
\end{align} But I am unable to understand how to proceed from here. Please help.","['probability-distributions', 'conditional-expectation', 'exponential-distribution', 'order-statistics', 'probability-theory']"
4126458,Can a discrete random variable be normally distributed?,"This question may seem quite strange, but I am wondering whether the above is true? In several russian books I saw the notion of ""somehow"" normally distributed discrete r.v. Can this even be possible? Thank you in advance!","['statistics', 'probability-distributions', 'normal-distribution', 'random-variables']"
4126466,How to convert an integral equation to a boundary value problem.,"I have the integral equation $$f(x)=\int_0^x (x-t)f(t)dt$$ and want to turn it into an boundary value problem, so I differentiate both sides with respect to $x$ and I get $$f'(x)=\int_0^x f(t)dt$$ Differentiate again and I get $$f''(x)=f(x)$$ However I don't know how to obtain the boundary values for $$f''(x)=f(x)$$ , there must be two boundary values of the form $$f(a)+f'(a)=A$$ and $$f(b)+f'(b)=B$$ , can you explain how to get them?","['boundary-value-problem', 'integral-equations', 'ordinary-differential-equations']"
4126468,Let $\alpha$ be a root of $x^3+x+1$ and $\beta$ be a root of $x^3+x+3$. Show that it is not possible that $\alpha\in\mathbb Q(\beta)$,"Question : Let $\alpha$ be a root of $x^3+x+1$ and $\beta$ be a root of $x^3+x+3$ . Show that it is not possible that $\alpha\in \mathbb Q(\beta)$ . My proof : Given $\beta$ is a root of $x^3+x+3$ . Thus $\beta^3+\beta+3 = 0$ . Then $(\beta^3+\beta+2 ) + 1 = 0$ . If $\alpha\in\mathbb Q(\beta)$ , then $\alpha = r_1 + r_2\beta$ for some rationals $r_1$ and $r_2$ . Also $(r_1+r_2\beta)^3+r_1+r_2\beta+1 = 0$ . Therefore for some rationals $r_1$ and $r_2$ we have $(r_1+r_2\beta)^3+r_1 + r_2\beta = \beta^3+\beta+2$ . But equating and solving such $r_1$ and $r_2$ doesn't exist. Thus, $\alpha$ doesn't belong to $\mathbb Q(\beta)$ . Do you think my proof is right? If not correct me or provide a better easier  proof","['field-theory', 'ring-theory', 'abstract-algebra', 'irreducible-polynomials']"
4126516,Approximate value of Series,"Consider the series $$
\sum_{k=2}^\infty \frac{\ln(k)}{k^p},
$$ which is easily seen to converge if $p>1$ . Numerical computations seems to reveal that, if $n\in\mathbb N$ $$
\left\lceil\sum_{k=2}^\infty \frac{\ln(k)}{k^{1+10^{-n}}}\right\rceil=10^{2n}.
$$ Is there an easy way to prove this?",['sequences-and-series']
4126519,Find $a_1$ and $a_2$ such that estimator is unbiased,"Let us consider the sample of independent random variables $X_1,\ldots,X_n$ with $E[X_1]=E[X_2]\ldots=E[X_n]=\mu$ . We examine the following estimator of $\mu$ : $$a_1(X_1+X_2+\ldots+X_n)+a_2.$$ Find constants $a_1$ and $a_2$ such that the estimator is unbiased. I proceed as follows: $$E[a_1(X_1+X_2+\ldots+X_n)+a_2]=a_1E[X_1+X_2+\ldots+X_n]+a_2=a_1n\mu+a_2.$$ So estimator is unbiased for $a_1=1/n$ and $a_2=0$ since the above expected value is equal to $\mu$ . I wonder if I should separately consider the case when $a_2\neq 0$ and find $a_1$ such that the expected value is equal to $\mu$ .","['statistics', 'parameter-estimation', 'estimation']"
4126535,Euler Equation $ax^2y''+bxy’+cy=0$,"I heard that one can solve the equation above by substituting $x$ for $e^z$ .
The only way I could solve this equation was by assuming that $y(x)$ has the form $x^p$ I am kinda stuck with that new approach. Hope someone could show me how to solve it. Thanks in advance","['analysis', 'ordinary-differential-equations']"
4126560,"Given a covariant derivative, is there a metric tensor that has the covariant derivative as the metric connection?","In differential geometry or general relativity, we usually think of the metric tensor $g_{\mu \nu}$ first and then introduce the metric connection. However, I wonder if we can go reverse. That is, let $M$ be a smooth manifold equipped with some nontrivial affine connection $\nabla$ . Then does there always exist a metric tensor $g_{\mu \nu}$ on $M$ that has $\nabla$ as its metric connection? I tried to figure this out myself but it is more confusing than expected. Could anyone help me?",['differential-geometry']
4126571,Will there be arbitrage if we don't have the $dB_t$ term?,"Assume you have a filtered probability space $(\Omega,\mathcal{F},P,\mathcal{F}_t)$ . Lets say you have the financial market with the bank process: $$dR_0(t,\omega)=\rho(\omega,t)R_0(t,\omega)dt,$$ where $\rho$ is a progressively measurable process and it is assumed that $P(\int_0^T|\rho(\omega,t)R(\omega,t)|dt<\infty)=1$ . And you have a risky asset which is modelled by $$dR_1(\omega,t)=\mu(\omega,t)dt+\sigma(\omega,t)dB_t,$$ where $\mu, \sigma$ are progressively measurable and $P(\int_0^T|\mu(\omega,t)|dt<\infty)=1$ and $P(\int_0^T|\sigma(\omega,t)|^2dt<\infty)=1$ . Now assume that $\sigma(\omega,t)=0$ . Then the risky asset is just modelled by $$dR_1(\omega,t)=\mu(\omega,t)dt.$$ Can we show that we have arbitrage in this case? The reason I suspect this to be true is that if we we instead had a time-discrete market with $t_0=0<t_1<t_2<\cdots<t_n=T$ and we modelled it by $$\Delta R_1(\omega,t_i)=R_1(\omega,t_{i+1})-R_1(\omega,t_{i})=\mu(\omega,t_i)\cdot \Delta t_t,$$ then we would have had arbitrage. Because since $\mu$ is adapted, it is known at time $t_i$ hence at time $t_i$ we would know what the value of the risky asset would be at time $t_{i+1}$ . We would also know what the value of the bank process would be at time $t_{i+1}$ at time $t_i$ . So we could either borrow money and buy the stock or short the stock and put money in the bank, depending on what gave highest return. However, can we show that it will lead to arbitrage in the time-continuous case as well? Update I think I have a solution. It goes like this: Look at the dsicounted process $R_1/R_0$ , this is an Itö-process by looking at the function $F(x,y)=x/y$ and noting that $R_1/R_0=F(R_1,R_0)$ . F satisfies the hypothesis in the Itö-formula since $R_0\ne 0$ . Since $dR_1$ and $dR_0$ don't have any $dB$ terms it follows from Itös formula that $dF(R_1,R_0)$ don't have any $dB$ terms. Hence $$\frac{R_1(t)}{R_0(t)}=\frac{R_1(0)}{R_0(0)}+\int_0^tK(\omega,s)ds,$$ where we find $K$ from Itö's formula. Since Itös formula tells us that $R_1/R_0$ is an Itö-process we have that $$P\left(\int_0^T|K(\omega,t)|dt<\infty\right)=1.$$ The fundamental theorem of asset-pricing tells us that there is no arbitrage if and only if there exist an equivalent local martingale measure $Q$ where $R_1/R_0$ is a local martingale. Assume for contradiction that $Q$ exists. Now we have that $$R_1/R_0$$ is defined $\omega$ -wise $P$ -a.s. And since $Q$ is equivalent to $P$ we must also have $$Q\left(\int_0^T|K(\omega,t)|dt<\infty\right)=1.$$ Now look at $K$ , $\omega$ -wise. Then it is an $L^1[0,T]$ -function. And it is known that $F(s)=\int_0^t f(s)ds$ has bounded variation for $f \in L^1[0,T]$ . Hence under $Q$ we have that $R_1/R_0$ is a continuous local martingale with finite variation. This means that it is constant. Hence $$Q(\{\omega: R_1(\omega,\cdot)/R_0(\omega,\cdot)=\text{Constant}\})=1.$$ But since $P$ and $Q$ are equivalent we have $$P(\{\omega: R_1(\omega,\cdot)/R_0(\omega,\cdot)=\text{Constant}\})=1.$$ This means that $$R_1(\omega,t)=R_0(\omega,t)\cdot M(\omega).$$ But this also holds for $t=0$ hence $$R_1(\omega,t)=\frac{R_1(0)}{R_0(0)}\cdot R_0(\omega,t),$$ Hence we are limited to the trivial case where the risky asset is a multiple of the bank-process.","['stochastic-analysis', 'finance', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4126589,"Why does $df\wedge dg$ represent the ""density of curves"" of constant $f$ and $g$?","While reading the answers to Geometric understanding of differential forms. , I stumbled upon this one , which, among other things, makes the case that $df\wedge dg$ can be interpreted as representing a ""density of curves"" of constant $f$ and $g$ . Here $f,g\in C^\infty(M)$ for some manifold $M$ . More generally, they state that $\alpha\wedge\beta$ , for generic 1-forms $\alpha,\beta$ , represents the density of curves formed by the intersection of the surfaces represented by $\alpha$ and $\beta$ . I don't quite understand how this picture works. I can understand a 1-form $\alpha$ as characterising surfaces, in that at every point $p\in M$ , $\alpha_p$ is a linear functional, and thus represents a plane via (the dual of) its normal vector. Similarly, $df_p$ represents a plane orthogonal to the direction of maximum variation of $f$ , and thus I understand $df$ as representing surfaces of constant $f$ . But how does this picture work when we take exterior products? For example, say $M=\mathbb R^3$ and $\alpha=xdx+ydy$ and $\beta=xdx$ . Then at every point I can picture $\alpha$ as the plane orthogonal to the vector $(x,y,0)$ , and $\beta$ as the plane orthogonal to the vector $(x,0,0)$ . Now, $$\alpha\wedge\beta = yx \,dy\wedge dx.$$ This is now a 2-form, which takes two tangent vectors as input. How do I visualise it geometrically as a ""density of curves""? Returning to the specific case with $df\wedge dg$ , a toy example could be $$f(x,y,z) = x+y^2, \qquad g(x,y,z) = yz, \\
df = dx + 2y \, dy, \qquad dg= z\, dy + y \, dz, \\
df\wedge dg = z \, dx\wedge dy + y \, dx\wedge dz + 2y^2 \, dy\wedge dz.$$","['exterior-algebra', 'differential-forms', 'differential-geometry']"
4126592,How can I represent a fraction as a finite sum of reciprocal squares?,"I've found this result : $$ \frac{1}2 = \frac1{2^2}+\frac1{3^2}+\frac1{4^2}+\frac1{5^2}+\frac1{6^2}+\frac1{15^2}+\frac1{18^2}+\frac1{36^2}+\frac1{60^2}+\frac1{180^2}$$ I've tried my best to make a program to find that result but i can't find a solution.
I found this solution to an easier problem : the representation of a fraction as the sum of distinct reciprocals. I tried to expend this algorithm to solve my problem but when i execute my program the numbers given get too large: def pgcd(a,b):
    return a if b==0 else pgcd(b,a%b)
def subFrac(p1,q1,p2,q2):
    u,v=p1*q2-p2*q1,q1*q2
    d=pgcd(u,v)
    return (u//d,v//d)
def dec(p,q):
    print(p,q)
    frac=[]
    k=2
    while p>0:
        while 1/(k**2)>p/q:
            k+=1
        p,q=subFrac(p,q,1,k**2)
        print(k,p,q)
        frac.append(k)
        k+=1
    return frac Gives : $$ \frac{1}2 = \frac1{2^2}+\frac1{3^2}+\frac1{4^2}+\frac1{5^2}+\frac1{6^2}+\frac1{11^2}+\frac1{54^2}+\frac1{519^2}+\frac1{59429^2}+...$$ In general, I'd like to make a program that gives a representation with reasonable number like the one given or this one from proofWiki : $$ \frac{1}2 = \frac1{2^2}+\frac1{3^2}+\frac1{4^2}+\frac1{5^2}+\frac1{7^2}+\frac1{12^2}+\frac1{15^2}+\frac1{20^2}+\frac1{28^2}+\frac1{35^2}$$ Thanks in advance! EDIT: Thanks for your answers.
I looked into the problem 152 of Euler project and I saw that they mostly brute force to find solutions and try to reduce the initial list size using math. I will probably try this later. Here is a link to a solution (not mine) written in C++ and here in python. I've tried James Arathoon solution and it works well. Here is the full python code if you are interested ( to find combinations i used a method which works but i think using a recursive function like they do in most project Euler 152 solutions is better ): import numpy.polynomial.polynomial as nppol # To make the variable you called list
# Useful function, i could use already existing module
def Divisors(k):
    l=[]
    for ii in range(1,k+1):
        if k%ii==0:
            l.append(ii)
    return l
def pgcd(a,b):
    return a if b==0 else pgcd(b,a%b)
def addFrac(p1,q1,p2,q2):
    assert q1!=0 and q2!=0
    u,v=p1*q2+p2*q1,q1*q2
    d=pgcd(u,v)
    return (u//d,v//d)
def subFrac(p1,q1,p2,q2):
    u,v=p1*q2-p2*q1,q1*q2
    d=pgcd(u,v)
    return (u//d,v//d)
# The interesting part
def test(alist, frac):
    for i in alist:
        testDivisors=Divisors(i)
        testDivisorsSq=[k*k for k in testDivisors]

        prod=[1] # Its a polynomial
        for j in testDivisorsSq:
            prod=nppol.polymul(prod,[1]+(j-1)*[0]+[1])
        llist=[k for k in range(len(prod)) if prod[k]!=0]

        totalInvTestDivisorsSq=(0,1)
        for k in testDivisorsSq[1:]:    # Remove 1 which is always the first element of TestDivisorsSq
            totalInvTestDivisorsSq=addFrac(*totalInvTestDivisorsSq,1,k)
        totalInvTestDivisorsSq=subFrac(*totalInvTestDivisorsSq,*frac)
        
        testnum=testDivisorsSq[-1]/totalInvTestDivisorsSq[1]*totalInvTestDivisorsSq[0]
        if testnum in llist:
            print(""Possible Solutions : "")
            print(i,""  "",testDivisorsSq,""  "",testnum) # Max(Divisors(i))=i always   and no need to reverse the list
            # Find all the correct combination that add up to testnum
            l=len(testDivisorsSq)
            for j in range(1,2**l):
                auth=(""0""*l+bin(j)[2:])[-l:]
                s=0
                k=0
                while s<=testnum and k<l:
                    if auth[k]==""1"":
                        s+=testDivisorsSq[k]
                    k+=1
                if s==testnum:
                    print(""Find a combination : "",[testDivisorsSq[k] for k in range(1,l) if auth[l-1-k]==""0""])
# its l-1-k because its reversed
test([1,2,6,12,18,30,36,60,90,120,150,180],(1,2))
# It took a few seconds to run this program It gives : Possible Solutions : 
120    [1, 4, 9, 16, 25, 36, 64, 100, 144, 225, 400, 576, 900, 1600, 3600, 14400]    500.0
Find a combination :  [4, 9, 16, 25, 64, 100, 225, 400, 576, 900, 1600, 3600, 14400]
Find a combination :  [4, 9, 16, 25, 64, 100, 144, 576, 900, 1600, 3600, 14400]

Possible Solutions : 
180    [1, 4, 9, 16, 25, 36, 81, 100, 144, 225, 324, 400, 900, 1296, 2025, 3600, 8100, 32400]    1086.0
Find a combination :  [4, 9, 16, 25, 81, 100, 144, 225, 400, 8100, 32400]
Find a combination :  [4, 9, 16, 25, 36, 225, 324, 1296, 3600, 32400]
Find a combination :  [4, 9, 16, 25, 36, 225, 400, 1296, 2025, 3600, 8100]
Find a combination :  [4, 9, 16, 25, 36, 144, 1296, 2025, 3600, 8100]
Find a combination :  [4, 9, 16, 25, 81, 100, 144, 324, 400, 900, 3600, 8100]
Find a combination :  [4, 9, 16, 25, 81, 100, 144, 225, 900, 1296, 2025, 3600]","['egyptian-fractions', 'sequences-and-series']"
4126627,How to calculate the integral $\int_0^{+\infty}\frac{\ln(\cos^2x)}{1+e^{2x}}dx$,"Calculate the integral $$\int_0^{+\infty}\frac{\ln(\cos^2x)}{1+e^{2x}}\,dx.$$ I tried $$\displaystyle\ln(\cos^2x)=\ln\left(\frac{\cos2x+1}{2}\right)=\ln(1+\cos2x)-\ln2.$$ It's easy to get the result of $$\displaystyle\int_0^{+\infty}\frac{-\ln2}{1+e^{2x}}\,dx=-\ln 2$$ and using $2x=t$ for another part $$\displaystyle \int_0^{+\infty}\frac{\ln(1+\cos2x)}{1+e^{2x}}\,dx$$ I got $$\int_0^{+\infty}\frac{\ln(\cos^2x)}{1+e^{2x}}\,dx=\frac12\int_0^{+\infty}\frac{\ln(1+\cos t)}{1+e^{t}}\,dt-\ln2.$$ I don't know how to calculate the first integral. Could someone help me? Thanks!","['integration', 'complex-analysis', 'definite-integrals']"
4126662,"How to solve $a_{n} = \frac{2a_{n-1}^2}{a_{n-2}}$, where $a_0 = 1$ and $a_1 = 2$","I'm trying to solve this reccurence relation as a part of the ""linear recurrence relations with constant coefficients"" chapter, with the hint that the next step is using logarithms. The problem I'm facing is what to do after the equation is logarithmized (since it's not linear anymore? and the coefficients aren't constant?) The usual way of solving $a_{n+3} - 2a_{n+2} -4a_{n+1} + 8a_n = 0$ for example, would be to find the characteristic polynomial, which would be $x^3 - 2x^2 - 4x + 8 = 0$ and using it's solutions to predict the form of $a_n$ and solving it using the starting conditions $a_0, a_1, a_2$ .
I just don't see how a similair way of solving would be relevant here.","['recurrence-relations', 'discrete-mathematics', 'generating-functions']"
4126698,"$G$ is simple, connected and planar graph on $22$ vertices then $G$ has at most $30$ edges.","Let $G$ be a graph on $22$ vertices. $G$ is simple, connected and planar. It does not have a circuit of length $4$ . Furthermore, the dual of $G$ has an Euler circuit. To prove that $G$ has at most $30$ edges. I am not getting an idea to proceed with the problem. From the condition of the planar graph, we have $e \le 3v-6$ . I am not able to use the fact that the dual of $G$ has an Euler circuit.","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4126714,"if $\lim_{x \to a}(f \circ g) (x)=L$, then we cannot generally conclude that $ \lim_{x \to g(a)}f(x)=L$","I've been playing around with the limits of compositions...e.g. $\displaystyle \lim_{x \to a}(f \circ g) (x)$ ...and wanted to confirm my intuition that just because $\displaystyle \lim_{x \to a}(f \circ g) (x)=L$ , it is not necessarily the case that $\displaystyle \lim_{x \to g(a)}f(x)=L$ . Said differently, if $\displaystyle \lim_{x \to a}(f \circ g) (x)=L$ , then we cannot generally conclude that $\displaystyle \lim_{x \to g(a)}f(x)=L$ . The pictures (hand drawn, sorry) I had in mind are the following: where the function $g$ maintains its value for all $x \geq a$ . The definition of $\displaystyle \lim_{x \to a}(f \circ g) (x)=L$ , which is $\forall \epsilon \gt 0 \exists \delta \gt 0 \forall x \in \mathbb R \big [ 0 \lt \lvert x -a \rvert \lt \delta \rightarrow \lvert (f \circ g)(x) - L \rvert \lt \epsilon \big ] $ for $L = f(g(a))$ , seems to be satisfied. From the graph of $f$ , clearly $\displaystyle \lim_{x \to g(a)}f(x)\neq f(g(a))$ . Is this the correct intuition?","['limits', 'calculus']"
4126771,Evaluate $\int_0^1 \sqrt\frac{1-x}{x}dx$ with the dogbone contour,"I'm currently working on the integral $I = \int_0^1 \sqrt\frac{1-x}{x}dx$ . I would like to evaluate it with the dogbone contour specifically . Here's what I did so far: Denote $f(z) = \sqrt\frac{1-z}{z}$ the integrand function (with complex variable). Then I observe that $$f(z) = (1 - z)^{\frac{1}{2}}z^{-\frac{1}{2}} = \frac{|1 - z|^{\frac{1}{2}}}{|z|^{\frac{1}{2}}}e^{\frac{1}{2}i(\arg(z) - \arg(1 - z))} (*)$$ Since $\ln(z) = \ln(|z|) + i\arg(z)$ (and similarly $\ln(1 - z) = \ln(|1 - z|) + i\arg(1 - z)$ ). Denote $Arg = \frac{1}{2}(\arg(z) - \arg(1 - z))$ for the step to come. Next I defined $- \pi < \arg(z) \leq \pi$ and $0 \leq \arg(z) < 2\pi$ , to observe the behaviour of Arg in the intervals $(- \infty, 0)$ and $(0,1)$ . In $(- \infty, 0)$ , from above, we have: $\arg(z) \to \pi$ , $\arg(1 - z) \to 2\pi \implies Arg \to \frac{1}{2}(\pi - 2\pi) = - \frac{\pi}{2}$ In $(- \infty, 0)$ , from below, we have: $\arg(z) \to - \pi$ , $\arg(1 - z) \to 0 \implies Arg \to \frac{1}{2}(- \pi - 0) = - \frac{\pi}{2}$ Hence we deduce that our function is continuous everywhere on this interval. However: In $(0,1)$ , from above, we have: $\arg(z) \to 0$ , $\arg(1 - z) \to 2\pi \implies Arg \to \frac{1}{2}(0 - 2\pi) = - \pi$ In $(0,1)$ , from below, we have: $\arg(z) \to 0$ , $\arg(1 - z) \to 0 \implies Arg \to 0$ And in this interval our function is discontinuous since $e^{- i\pi} \ne e^0 = 1$ . Thus I introduced a dogbone contour, namely $C$ , aroud the branch $[0,1]$ such that: $$C = \psi_1 \cup \psi_2 \cup c_0 \cup c_1$$ Where $\psi_1$ is the blue segment from $1$ to $0$ and $\psi_2$ is the blue segment from $0$ to $1$ . (Thanks to @Zaid Alyafeai for the picture). Then we have the equality: $\oint_C f(z)dz = \int_{\psi_1} f(z)dz + \int_{\psi_2} f(z)dz + \int_{c_1} f(z)dz + \int_{c_2} f(z)dz$ ; and pumping up $C$ we also have that $\oint_C = - 2\pi i (res_{z = 0}f(z)dz + res_{z = \infty}f(z)dz)$ by Cauchy Residue Theorem since $0,\infty$ are the only singular points of $f(z)$ . Then I computed them: For $\int_{\psi_1} f(z)dz$ and $\int_{\psi_2} f(z)dz$ , I used $z = t + i\epsilon$ with $t \in [0,1] $ and $dz = dt$ : $$\int_{\psi_1}f(z)dz = - \int_0^1\frac{(1 - t - i\epsilon)^{\frac{1}{2}}}{(t + i\epsilon)^{\frac{1}{2}}}dt = - \int_0^1 \frac{|1 - t - i\epsilon|^{\frac{1}{2}}}{|t + i\epsilon|^{\frac{1}{2}}}e^{i\pi}dt$$ (I deduced the last integral from $(*)$ and from the arguments I computed above) Then taking the limt when $\epsilon \to 0$ I finally obtained that $\lim\limits_{\epsilon \to 0} \int_{\psi_1}f(z)dz = I$ . In a very similar way I found that $\lim\limits_{\epsilon \to 0} \int_{\psi_2}f(z)dz = I$ as well. Now for $\int_{c_0} f(z)dz$ and $\int_{c_1} f(z)dz$ , I used $z = \epsilon e^{i\theta}$ with $\theta \in [0,2\pi]$ and $dz = i\epsilon e^{i\theta}$ , and my goal was to find upper bounds of the absolute value of these integrals: $|\int_{c_0} f(z)dz| = |i\epsilon^{\frac{1}{2}}\int_0^{2\pi}(1 - \epsilon e^{i\theta})^{\frac{1}{2}}e^{\frac{1}{2}i\theta}dt| \leq i\epsilon^{\frac{1}{2}}\int_0^{2\pi}|1 - \epsilon e^{i\theta}|^{\frac{1}{2}}|e^{\frac{1}{2}i\theta}|dt \leq i\epsilon^{\frac{1}{2}}(1 + \epsilon)^{\frac{1}{2}}2\pi$ Thus I had: $0 \leq \lim\limits_{\epsilon \to 0}|\int_{c_0}f(z)dz[ \leq 0$ which implies that $\lim\limits_{\epsilon \to 0}\int_{c_0}f(z)dz = 0$ . In a very similar way again, I found that $\lim\limits_{\epsilon \to 0}\int_{c_1}f(z)dz = 0$ . Hence in the end I had: $\oint_Cf(z)dz = I + I + 0 + 0 = 2I$ , and since I also had that $\oint_Cf(z)dz = - 2\pi i (res_{z = 0}f(z)dz + res_{z = \infty}f(z)dz)$ , it follows that: $$2I = - 2\pi i (res_{z = 0}f(z)dz + res_{z = \infty}f(z)dz) (***)$$ And that's where my problem starts (or not !): I computed both residues and found they were both equal to 0, but this implies that $I = 0$ and I feel like this is wrong, because first I don't understand why they would be 0 when looking at their shape, and second this means the ""number"" of $I$ on the LHS of $(***)$ does not influence our result. So I hope you can help me either finding my mistake, either to show me that those residues are not 0 because I did not achieve to find another result (either to prove me that $I = 0$ because I don't think so). I already appreciated your time, but any help would be welcome!","['complex-analysis', 'contour-integration', 'solution-verification', 'residue-calculus']"
4126789,Injective continuous real function that is not strictly monotone,"I'm trying to find an injective continuous real function that is not strictly monotone. I can't seem to find an example for that. When it is continuous and not monotone it can't be injective, since the y values are not unique for every x and the other way around. I already proved, that it is not possible on an interval and somone proposed to me, that there might be a possibility if I use an union of intervals, but I can't find any. Does anyone have an example or a hint to find one?","['functions', 'real-analysis']"
4126802,How to check that the Poisson bracket of traces of matrix powers is zero?,"The canonical Lie-Poisson bracket on functions on the space $\mathfrak g$ of n×n square matrices is given by: $$\{f_1,f_2\}(a)=\langle a, [df_1(a),df_2(a)] \rangle,$$ where $a \in {\mathfrak g}^*$ , [,] is the commutator and $\langle, \rangle$ is the canonical linear pairing of $\mathfrak g$ and $\mathfrak g^*$ . A mathematical paper I am reading claims that it is obvious that the Poisson bracket of the two functions $f_k(X)= trace(X^k)$ and $f_m(X)= trace(X^m)$ is zero but I have trouble seeing this for myself. Could you help me check that $\{f_k, f_m\}=0$ ? Thank you very much!","['matrices', 'poisson-geometry']"
4126805,Variation of fat Cantor Sets -- Help needed with the sum of a sequence with relationship between $a_n$ and $S_{n-1}$,"I am having trouble with finding the infinite sum of the sequence $\{a_n\}$ with the relationship $$ a_1 = r, \; a_n = r^n(1-S_{n-1})$$ where $$ S_{n-1} = \sum_{k=1}^{n-1} a_k, \; 0<r<1$$ I tried to subtract $$S_{n-1} = 1- \frac{a_n}{r^n}$$ from $$S_{n} = 1- \frac{a_{n+1}}{r^{n+1}}$$ and after some simplification I got $$\frac{a_{n+1}}{a_n} = r(1-r^n)$$ Could anyone help me to calculate $\sum_{n=1}^\infty a_n$ based on the relationships above? Many thanks!! Edit: I thought this is a rather technical question so I didn't provide the background of why I am doing this in the first place. Now I see why it might help. I am considering a variation of 'fat Cantor sets' which is obtained by varying the ratio removed at each step. To be precise, starting from the unit interval $K_0 = [0,1]$ , we remove the middle open subinterval $G^1_1$ such that $\ell(G^1_1)/\ell(K_0) = r$ , for some $r \in (0,1)$ . Call the remaining intervals $K_1$ . Next, we remove the middle open subintervals $G^2_1$ and $G^2_2$ such that $\mu(G^2_1 \cup G^2_2)/\mu(K_1) = r^2$ . Continuing in this fashion such that the measure of intervals removed at step $n$ is of proportion $r^n$ to the measure of the original intervals. Denote $\mu(\bigcup_{m=1}^{2^{n-1}}G_m^n) = a_n$ the length of intervals removed at step $n$ , $n \geq 1$ . We therefore have the relation $a_n = r^n(1-S_{n-1})$ . In other words, the length of the subintervals removed at each step is of proportion $r^n$ to the original interval, which equals r^n*(1 - sum of the length of intervals removed in previous steps). I am interested in finding $\sum_{n=1}^\infty a_n$ , which is the total length removed in this construction.","['general-topology', 'sequences-and-series', 'real-analysis']"
4126851,Gamma-like distribution integral,"Here I have an integral which is similar to gamma distribution, $$
\int_{0}^{\infty} \frac{e^{-\lambda x} \lambda^n x}{(n-1)!}dx
$$ and how to compute its integral?","['calculus', 'statistics', 'probability']"
4126934,Pushforward from Flag variety,"Consider a vector space $V$ and some $n,n+1\leqslant\dim V$ .
Let $F\subset G(n,V)\times G(n+1,V)$ be the Flag variety (where $G(n,V)$ is the Grassmannian of $n$ -dimensional subspaces of $V$ ),
i.e. the subvariety of couples $(W,W')$ such that $W\subset W'$ .
We have projections from $F$ to both $G(n,V)$ and $G(n+1,V)$ . I can show that the projection to $G(n,V)$ is just the projectivization $P(Q)$ , where $Q$ is the universal quotient bundle on $G(n,V)$ . Is there a way to describe
the second projection, i.e. $\pi:F\rightarrow G(n+1,V)$ , and more specifically, what is the pushforward $\pi_{\ast}(\mathcal{O})$ of the structure sheaf?
Is $\pi$ just the projectivization $P(S)$ , where $S$ is the universal subbundle on $G(n+1,V)$ so $\pi_{\ast}(O)=O$ ?","['grassmannian', 'algebraic-geometry', 'differential-geometry']"
4126960,"Integral $\int_0^\infty\frac{\log(1+\cos x)}{1+e^x}\,dx=0$","In this question , the result $$\int_0^\infty\frac{\log\cos^2x}{1+e^{2x}}\,dx=-\frac{\log^22}2$$ was shown by writing $\log\cos^2x=2\log\cos x$ . The OP of the linked question attempted the integral as follows $$\int_0^\infty\frac{\log\cos^2x}{1+e^{2x}}\,dx=\frac12\int_0^\infty\frac{\log(1+\cos x)}{1+e^x}\,dx-\frac{\log^22}2$$ by writing $\log\cos^2x=\log(1+\cos2x)-\log2$ . Equating these two expressions yields $$\int_0^\infty\frac{\log(1+\cos x)}{1+e^x}\,dx=0.$$ Is there a direct way to prove this result (without writing $\log\cos^2x$ in two different ways to get there)?","['integration', 'complex-analysis', 'definite-integrals', 'real-analysis']"
4126973,"Is there a mathematical name for a ""pillow"" shape?","Is there a mathematical name for the shape you get when stitching two squares or rectangles together along all four edges, and then ""inflating"" it? I've been calling this a ""pillow"" shape, but I'm sure it must have a real name. There's a similar-ish problem called a ""Mylar Balloon"", but involves two circles stitched together instead of squares or rectangles: Mylar Balloon My friend also stumbled on the term ""squinch vault"" that looks about right: However, this seems to be an architectural term and not a mathematical one.","['geometry', 'terminology']"
4127012,Part of Birkhoff's HSP theorem for varieties of groups.,"This is part of Exercise 2.3.7 of Robinson's ""A Course in the Theory of Groups (Second Edition)"" . According to Approach0 , it is new to MSE; well, except this , which is not what I'm looking for as it uses universal algebra, a topic I haven't studied in a long time. The first part of the exercise is this: Show that ${\rm Var}\, \mathfrak{X}$ is a variety. A previous exercise in the set in Robinson's book, relevant to this exercise, is this: Part of Birkhoff's theorem for varieties of groups in Robinson's book. The Details: Since definitions vary, on page 15, ibid. , paraphrased, it states that A subgroup $N$ of $G$ is normal in $G$ if one of the following equivalent statements is satisfied: (i) $xN=Nx$ for all $x\in G$ . (ii) $x^{-1}Nx=N$ for all $x\in G$ . (iii) $x^{-1}nx\in N$ for all $x\in G, n\in N$ . The definition of a direct product is on pages 20 to 21, ibid. Let $\{G_\lambda\mid \lambda\in\Lambda\}$ be a given set of groups. The cartesian (or unrestricted direct ) product , $$C=\underset{\lambda\in\Lambda}{{\rm Cr}}\, G_\lambda,$$ is the group whose underlying set is the set product of the $G_\lambda$ s [. . .] and whose group operation is multiplication of components: thus $$(g_\lambda)(h_\lambda)=(g_\lambda h_\lambda),$$ $g_\lambda, h_\lambda\in G_\lambda$ . [. . .] The subset of all $(g_\lambda)$ such that $g_\lambda=1_\lambda$ for almost all $\lambda$ [. . .] is called the external direct product , $$D=\underset{\lambda\in\Lambda}{{\rm Dr}}\, G_\lambda,$$ [. . .] In case $\Lambda=\{\lambda_1,\dots,\lambda_n\}$ , a finite set, we write $$D=G_{\lambda_1}\times\dots\times G_{\lambda_n}.$$ Of course $C=D$ in this case. On page 56, ibid. , Let $F$ be a free group on a countably infinite set $\{x_1,x_2,\dots\}$ and let $W$ be a nonempty subset of $F$ . If $w=x_{i_1}^{l_1}\dots x_{i_r}^{l_r}\in W$ and $g_1,\dots, g_r$ are elements of a group $G$ , we define the value of the word $w$ at $(g_1,\dots,g_r)$ to be $w(g_1,\dots,g_r)=g_1^{l_1}\dots g_{r}^{l_r}$ . The subgroup of $G$ generated by all values in $G$ of words in $W$ is called the verbal subgroup of $G$ determined by $W$ , $$W(G)=\langle w(g_1,g_2,\dots) \mid g_i\in G, w\in W\rangle.$$ On page 57, ibid. , If $W$ is a set of words in $x_1, x_2, \dots$ and $G$ is any group, a normal subgroup $N$ is said to be $W$ -marginal in $G$ if $$w(g_1,\dots, g_{i-1}, g_ia, g_{i+1},\dots, g_r)=w(g_1,\dots, g_{i-1}, g_i, g_{i+1},\dots, g_r)$$ for all $g_i\in G, a\in N$ and all $w(x_1,x_2,\dots,x_r)$ in $W$ . This is equivalent to the requirement: $g_i\equiv h_i \mod N, (1\le i\le r)$ , always implies that $w(g_1,\dots, g_r)=w(h_1,\dots, h_r)$ . [The] $W$ -marginal subgroups of $G$ generate a normal subgroup which is also $W$ -marginal. This is called the $W$ -marginal of $G$ and is written $$W^*(G).$$ On page 57, ibid. , A [. . .] class of groups $\mathfrak{X}$ is a class - not a set - whose members are groups and which enjoys the following properties: (i) $\mathfrak{X}$ contains a group of order $1$ ; and (ii) $G_1\cong G\in\mathfrak{X}$ always implies $G_1\in\mathfrak{X}$ . [. . .] A group in a class $\mathfrak X$ is called an $\mathfrak X$ -group. On page 58, ibid. , If $W$ is a set of words in $x_1, x_2, \dots $ , the class of all groups $G$ such that $W(G)=1$ , or equivalently $W^*(G)=G$ , is called the variety $\mathfrak{B}(W)$ determined by $W$ . The Question: If $\mathfrak X$ is any class of groups, define ${\rm Var}\,\mathfrak X$ to be the intersection of all varieties that contain $\mathfrak X$ . Prove that ${\rm Var}\,\mathfrak X$ [. . .] consists of all images of subgroups of cartesian products of $\mathfrak X$ -groups. Thoughts: By the first part of the exercise , ${\rm Var}\,\mathfrak X$ is a variety; by the earlier exercise , then, ${\rm Var}\,\mathfrak{X}$ is closed with respect to forming subgroups, images, and subcartesian products; so - I guess - we can conclude $$\mathbf{H}(\mathbf{S}(\mathbf{P}(\mathfrak X)))\subseteq {\rm Var}\,\mathfrak X,\tag{1}$$ where: $\mathbf{H}(\mathfrak{Y})$ means ""homomorphic images of the groups in the class $\mathfrak{Y}$ of groups""; $\mathbf{S}(\mathfrak{Y})$ means ""subgroups of the groups in the class $\mathfrak{Y}$ of groups""; and $\mathbf{P}(\mathfrak{Y})$ means ""products of the groups in the class $\mathfrak{Y}$ of groups"". This notation, I gather, is from universal-algebra . One problem I have here is in ensuring that $\mathbf{K}(\mathfrak Y)$ is a variety for each (relevant) $\mathfrak Y$ and for each (relevant) $\mathbf{K}$ ; another problem is that, wouldn't it be $\mathbf{H}(\mathbf{S}(\mathbf{P}({\rm Var}\,\mathfrak X)))\subseteq {\rm Var}\,\mathfrak X$ ? To prove the reverse inclusion of $(1)$ , I guess I could proceed as I would with a set theoretic inclusion: by supposing $G\in {\rm Var}\,\mathfrak X$ with the aim of proving $G\in \mathbf{H}(\mathbf{S}(\mathbf{P}(\mathfrak X)))$ . Nothing springs out at me regarding how to do this, although I suppose I'd have to find a homomorphism $\varphi: G\to S$ (or is it $\psi: S\to G$ ?) for some subgroup $S$ of a product $P$ of $\mathfrak X$ -groups. Please help :)","['group-homomorphism', 'direct-product', 'group-theory']"
4127013,An immediate alternative to a trigonometry problem (high school),"I have a right triangle, $AH\perp BC$ , where $\cos \beta=\sqrt 5/5$ and $\overline{AH}+\overline{CH}+\overline{HB}=7$ . I have to found the area. My steps (or solution) : $$\mathcal A(\triangle ABC)=\frac 12\overline{AB}\cdot \overline{BC} \sin \beta$$ with $0<\beta<\pi$ . Hence $\sin \beta=\sqrt{1-5/25}=2\sqrt5/5$ . But $\overline{AB}=\overline{BC}\cos \beta$ and the area $$\mathcal A(\triangle ABC)=\frac12(\overline{BC}\cos \beta)\cdot (\overline{BC}\sin \beta)=\frac12\overline{BC}^2 \tag 1$$ We know that: $$\overline{AH}+\overline{CH}+\overline{HB}=7, \quad \text{with}\quad  \overline{CH}+\overline{HB}=\overline{BC}$$ Hence $$\overline{AH}+\overline{BC}=7 \iff \overline{BC}=7-\overline{AH}$$ and $\overline{AH}=\overline{AB}\cos\gamma=\overline{BC}\cos\gamma\cos\beta$ . Putting this last identity to the condition $\overline{BC}=7-\overline{AH}$ I will have: $$\overline{BC}=7-\overline{BC}\cos\gamma\cos\beta \iff \overline{BC}(1+\cos\gamma\cos\beta)=7$$ After, $$\overline{BC}=\frac{7}{1+\cos\gamma\cos\beta}$$ with $\cos \gamma=\cos(\pi/2-\cos\beta)=\sin\beta=2\sqrt5/5$ .
Definitively $$\overline{BC}^2=\frac{49}{\left(1+\frac 25\right)^2}=\frac{49}{\frac{49}{25}}$$ and $$\mathcal{A}(\triangle ABC)=\frac15\overline{BC}^2=5 \tag 2$$ I'm very tired and often don't find immediate solutions to problems. I had thought of using Euclid's second theorem by placing $\overline{AH}=x$ and $\overline{BC}=y$ , and $x+y=7$ . After I will have $x^2=\overline{BH}\cdot \overline{CH}$ . But I have left this path. Can this possible alternative be used or is there another more immediate way?","['alternative-proof', 'trigonometry', 'education']"
4127065,Simplest expression for this angle?,"We have a unit circle, a point $R = (\sin\theta,\cos\theta)$ on the circumference, and a point $P=(a,b)$ contained in the circle ( $a^2+b^2<1$ ). We know $a,b$ and $\theta$ , and we want to find the angle $\phi$ between the line segment $\overline{PR}$ and the $y$ -axis. I've summarized this in the image below: I suppose I could use cosine law but it seems to get real messy real fast. I'm pretty terrible at trigonometry so I wanted to ask here if there are any really nice/elementary solutions I'm just not seeing.",['trigonometry']
4127142,Tensor product of quadratic number field with itself,"If $F=\mathbb{Q}(\sqrt{D})$ , is there a nice structure to $F\otimes_{\mathbb{Q}} F$ ? A spanning set of that tensor product is $1\otimes 1$ , $1\otimes \sqrt{D}$ , $\sqrt{D}\otimes 1$ , and $\sqrt{D}\otimes\sqrt{D}$ . I think these are linearly independent, since $\sqrt{D}\notin\mathbb{Q}$ . In fact, if I write them as matrices (with respect to the previous basis, in order), I get that $$\begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1\end{pmatrix},
\begin{pmatrix} 0 & D & 0 & 0 \\ 1 & 0 & 0 & 0\\ 0 & 0 & 0 & D\\ 0 & 0 & 1 & 0\end{pmatrix},
\begin{pmatrix} 0 & 0 & D & 0 \\ 0 & 0 & 0 & D\\ 1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\end{pmatrix},
\begin{pmatrix} 0 & 0 & 0 & D^2 \\ 0 & 0 & D & 0\\ 0 & D & 0 & 0\\ 1 & 0 & 0 & 0\end{pmatrix}$$ All of these are full-rank, actually. I tried checking whether an arbitrary linear combination of these can have determinant 0, but I couldn't solve the resulting equation. Based on another question, I can map $F\otimes_{\mathbb{Q}} F\rightarrow F$ by $x\otimes y\mapsto xy$ , and by dimensionality this has a kernel of dimension 2. In fact I can find a basis of this kernel: $1\otimes\sqrt{D}-\sqrt{D}\otimes 1$ and $\sqrt{D}\otimes\sqrt{D} - D\otimes 1$ . So this has a non-trivial ideal, so it is not a field, but it seems to be an integral domain. Is there much more I can say about it?","['extension-field', 'number-theory', 'tensor-products']"
4127258,Checking $( \binom{n}{k} - \binom{n-k}{k}) / \binom{n}{k} < \frac{k^2}{n}$,"Let $n$ and $k$ be integers with $2\leq k \leq n$ . I want to check the inequality $$\left( \binom{n}{k} - \binom{n-k}{k} \right) / \binom{n}{k} < \frac{k^2}{n}$$ which occurs in a paper I am currently reading. For fixed $k$ and $n\to \infty$ one should get asymptotic equality. There should be a ""elementary"" proof. Here is my question: Why does this inequality hold? Direct calculation didn't succeed so far. It might be useful to observe that the inequality is equivalent to $$ 1- \binom{n-k}{k}/\binom{n}{k} < \frac{k^2}{n} .$$ The asymptotic equality will probably follow directly from the proof.","['inequality', 'binomial-coefficients', 'combinatorics', 'asymptotics']"
4127267,"Continuous $f$ on $[0,1]$ is not one-to-one. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question We have continuous function $f$ on $[0,1]$ with $f([0,1])=[0,1]\times[0,1]$ . Prove $f$ is never one-to-one? I know I should show what I tried but I unable to think how to start on this one. I would appreciate some hints so I can update what I tried.","['analysis', 'real-analysis']"
4127303,$\int\limits_{-\infty}^\infty \left(f_T(\frac{x-\mu}{1+\Psi/2})-f_T(\frac{x+\mu}{1-\Psi/2})\right)\frac{x\gamma}{(x-x_{0})^{2}+\gamma^2/4}dx$,"EDIT: I realized from numerical implementation that the step from \begin{align}
\mathcal{I}_2=&\frac{\gamma}{2}\int\limits_{-\infty}^\infty \left(f_T(\frac{x-\mu}{1+\Psi/2})-f_T(\frac{x+\mu}{1-\Psi/2})\right)\left[\frac{1}{x-\left(x_{0}+ \mathrm{i}\frac{\gamma}{2}\right)}+\frac{1}{x-\left(x_{0}- \mathrm{i}\frac{\gamma}{2}\right)}\right]dx\nonumber
\end{align} is not the same to the 'equivalent expression' \begin{gather}
\mathcal{I}_2=\mathcal{I}^A+\mathcal{I}^R\\
\mathcal{I}^{A/R}=\frac{\gamma}{2}\int\limits_{-\infty}^\infty f_T(z)\left[\frac{1}{z-\frac{x_0-\mu\pm\mathrm{i}\frac{\gamma}{2}}{1+\Psi/2}}-\frac{1}{z-\frac{x_0+\mu\pm\mathrm{i}\frac{\gamma}{2}}{1-\Psi/2}}\right]dz
\end{gather} (in fact the first expression gives the black line of the plot and the second gives the red line of the plot), Can someone see the difference? ORIGINAL QUESTION: I want to solve the following integral \begin{align}
W=\int\limits_{-\infty}^\infty \left(f_T(\frac{x-\mu}{1+\Psi/2})-f_T(\frac{x+\mu}{1-\Psi/2})\right)\frac{x\gamma}{(x-x_{0})^{2}+\frac{\gamma^{2}}{4}}dx
\end{align} with $f_{T}(x)=\frac{1}{1+e^{\frac{x}{T}}}$ the Fermi function and $x_{0},\mu,\gamma, T\in\Bbb{R}_{\ge0}$ and $\Psi\in[-2,2]$ . I have calculated the analytical solution for the integral in terms of the digammas $\psi(z)$ functions (see derivation below), but when comparing the derived expression with the numerical solution using the quadrature method from the scipy routine in python for the set of parameters $x_{0}=\mu=0,\gamma= T=1$ and $\Psi\in[-2,2]$ there is an evident discrepancy between both approaches, plot at the end. I suspect that my analytical derivation is not completely correct, although when I set $\Psi=0$ the agreement between both approaches is perfect. Can anybody help me to understand where I'm wrong? The required integral is derived after an auxiliary integral $\mathcal{I}_1$ (correctly checked). Problems may start from $\mathcal{I}_2$ . \begin{align}
\mathcal{I}_1=\int\limits_{-\infty}^\infty f_T(x-\mu)\frac{\gamma}{(x-x_{0})^{2}+\frac{\gamma^{2}}{4}}dx
\end{align} Using the substitution $x = \mu + T u$ and the abbreviations $u_0 = \frac{x_0 - \mu}{T}$ and $\beta = \frac{\gamma}{2T}$ , we can write the integral as \begin{align}
\mathcal{I}_{1} = 2 \beta \int \limits_{-\infty}^\infty \frac{\mathrm{d} u}{(\mathrm{e}^{u} + 1)[(u-u_0)^2 + \beta^2]}  \equiv 2 \beta \int \limits_{-\infty}^\infty g_{\beta,u_0}(u) \, \mathrm{d} u \, . 
 \end{align} $g_{\beta,u_0}$ has simple poles at $u_0 \pm \mathrm{i} \beta$ with residues \begin{align}
 \operatorname{Res}(g_{\beta,u_0},u_0 \pm \mathrm{i} \beta) = \pm \frac{1}{2 \mathrm{i} \beta (\mathrm{e}^{u_0 \pm \mathrm{i} \beta} + 1)}
 \end{align} and at $\pm (2n+1) \pi \mathrm{i}$ (so only at the odd-integer multiples of $\pi \mathrm{i}$ !) with residues \begin{align}
 \operatorname{Res}(g_{\beta,u_0},\pm (2n+1) \pi \mathrm{i}) = - \frac{1}{[(2n+1) \pi \mathrm{i} \mp u_0]^2 + \beta^2}
 \end{align} for $n \in \mathbb{N}_0$ . The integrals of $g_{\beta,u_0}$ along semi-circles (avoiding the poles on the imaginary axis) vanish in the limit of large radii, so we can use the residue theorem to evaluate the integral. Closing the contour in the upper half-plane (the lower half-plane works just as well) yields \begin{align}
\mathcal{I}_{1} =& 2 \beta \, 2 \pi \mathrm{i} \left[\operatorname{Res}(g_{\beta,u_0},u_0 + \mathrm{i} \beta) + \sum \limits_{n=0}^\infty \operatorname{Res}(g_{\beta,u_0},(2n+1) \pi \mathrm{i}) \right] \\
=&4 \pi \beta\mathrm{i} \left[ \frac{1}{2 \mathrm{i} \beta (\mathrm{e}^{u_0 + \mathrm{i} \beta} + 1)}- \sum \limits_{n=0}^\infty \frac{1}{[(2n+1) \pi \mathrm{i} - u_0]^2 + \beta^2} \right]\\
=&4 \pi \beta\mathrm{i} \left[ \frac{1}{2 \mathrm{i} \beta (\mathrm{e}^{u_0 + \mathrm{i} \beta} + 1)}+ \sum \limits_{n=0}^\infty\frac{\frac{1}{4\pi\beta}}{\left(n+\left[\frac{1}{2}+\frac{\mathrm{i}u_0+\beta}{2\pi}\right]\right)}-\sum \limits_{n=0}^\infty\frac{\frac{1}{4\pi\beta}}{\left(n+\left[\frac{1}{2}+\frac{\mathrm{i}u_0-\beta}{2\pi}\right]\right)} \right]
\end{align} Now we use the series formula for the digamma function $\psi$ \begin{align}
\psi(z)=\sum \limits_{n=0}^\infty\left(\frac{1}{n+1}-\frac{1}{n+z}\right)-\gamma
\end{align} to find \begin{align}
\mathcal{I} = 2 \pi \left[\frac{1}{\mathrm{e}^{u_0 + \mathrm{i} \beta} + 1} - \frac{1}{2 \pi \mathrm{i}} \left(\psi \left(\frac{1}{2} + \frac{\beta + \mathrm{i} u_0}{2 \pi}\right) - \psi \left(\frac{1}{2} + \frac{-\beta + \mathrm{i} u_0}{2 \pi}\right) \right)\right]\end{align} Finally, we apply the reflection formula \begin{align}
\psi(1-z)=\psi(z)+\pi\cot{\left(\pi z\right)}
\end{align} to the second digamma function together with the properties \begin{gather}
 \operatorname{Im} \left[\psi(x+ \mathrm{i}y)\right]=\frac{ \mathrm{i}}{2}\left(\psi(x- \mathrm{i}y)-\psi(x+ \mathrm{i}y)\right)\\
  \cot{\left(\frac{\pi}{2}+z\right)}=-\tan(z)\\
 \frac{1}{e^{z}+1}+\frac{\mathrm{i}}{2}\tan\left(\frac{-\mathrm{i}z}{2}\right)=\frac{1}{2}
\end{gather} and simplify the result: \begin{align}
\mathcal{I}&=  2 \pi \left[\frac{1}{\mathrm{e}^{u_0 + \mathrm{i} \beta} + 1} - \frac{1}{2 \mathrm{i}} \tan\left(\frac{\beta - \mathrm{i} u_0}{2}\right) -\frac{1}{2 \pi \mathrm{i}} \left(\psi \left(\frac{1}{2} + \frac{\beta + \mathrm{i} u_0}{2 \pi}\right) - \psi \left(\frac{1}{2} + \frac{\beta - \mathrm{i} u_0}{2 \pi}\right) \right)\right] \\
&= 2 \pi \left[\frac{1}{2} - \frac{1}{\pi} \operatorname{Im} \left(\psi \left(\frac{1}{2} + \frac{\beta + \mathrm{i} u_0}{2 \pi}\right) \right) \right] = \pi - 2 \operatorname{Im} \left[\psi \left(\frac{1}{2} + \frac{\beta + \mathrm{i} u_0}{2 \pi}\right) \right] \, .
\end{align} Returning to the original parameters, we end up with \begin{align}
\boxed{\mathcal{I}_{1}(\gamma,x_{0}, \mu,T) =\int \limits_{-\infty}^\infty \frac{1}{\mathrm{e}^{(x - \mu)/T} + 1} \frac{\gamma}{(x-x_0)^2 + \gamma^2/4} \, \mathrm{d} x = \pi - 2 \operatorname{Im} \left[\psi \left(\frac{1}{2} + \frac{\frac{\gamma}{2} + \mathrm{i} (x_0 - \mu)}{2 \pi T}\right) \right] }\, .
\end{align} Note that the dependence on the parameters is explicit and in the special case $x_0 = \mu$ the result is simply $\pi$ . This can also be shown using elementary methods, so the implicit assumption $u_0 + \mathrm{i} \beta \not\in (2 \mathbb{Z} + 1) \pi \mathrm{i}$ used in the computation of the residues is justified. The integral we are interested in is \begin{align}
\mathcal{I}_2=\int\limits_{-\infty}^\infty \left(f_T(\frac{x-\mu}{1+\Psi/2})-f_T(\frac{x+\mu}{1-\Psi/2})\right)\frac{x\gamma}{(x-x_{0})^{2}+\frac{\gamma^{2}}{4}}dx
\label{eq:2}
\end{align} We can rewrite the integral $I_{2}$ decomposing the second factor as the sum of two complex parts. This is related to the advanced and retarded Green's functions \begin{align}
\frac{x\gamma}{(x-x_{0})^{2}+\frac{\gamma^{2}}{4}}  = &\frac{\gamma}{2}\left[\frac{1}{x-\left(x_{0}+ \mathrm{i}\frac{\gamma}{2}\right)}+\frac{1}{x-\left(x_{0}- \mathrm{i}\frac{\gamma}{2}\right)}\right]
- \mathrm{i}x_{0}\left[\frac{1}{x-\left(x_{0}+ \mathrm{i}\frac{\gamma}{2}\right)}-\frac{1}{x-\left(x_{0}- \mathrm{i}\frac{\gamma}{2}\right)}\right]\\
=&\frac{\gamma}{2}\left[G^{A}+G^{R}\right]- \mathrm{i}x_{0}\left[G^{A}-G^{R}\right]
\end{align} On the other hand, we can rewrite the Cauchy distribution used in the previous integral as function of the Green's functions too
and using that the second contribution is related to the previously calculated integral $\mathcal{I}_{1}$ \begin{align}
\frac{\gamma}{(x-x_{0})^{2}+\frac{\gamma^{2}}{4}} =- \mathrm{i}\left[\frac{1}{x-\left(x_{0}+ \mathrm{i}\frac{\gamma}{2}\right)}-\frac{1}{x-\left(x_{0}- \mathrm{i}\frac{\gamma}{2}\right)}\right] =- \mathrm{i}\left[G^{A}-G^{R}\right].
\end{align} So \begin{align}
\mathcal{I}_2=&\frac{\gamma}{2}\int\limits_{-\infty}^\infty \left(f_T(\frac{x-\mu}{1+\Psi/2})-f_T(\frac{x+\mu}{1-\Psi/2})\right)\left[\frac{1}{x-\left(x_{0}+ \mathrm{i}\frac{\gamma}{2}\right)}+\frac{1}{x-\left(x_{0}- \mathrm{i}\frac{\gamma}{2}\right)}\right]dx\nonumber\\&+x_{0}\left(I_{1}\left(\gamma,x_{0}, \mu,T(1+\Psi/2)\right)-I_{1}(\gamma,x_{0}, -\mu,T(1-\Psi/2))\right)\\
=&\frac{\gamma}{2}\int\limits_{-\infty}^\infty f_T(z)\left[\left(\frac{1}{z-\frac{x_0-\mu+\mathrm{i}\frac{\gamma}{2}}{1+\Psi/2}}+\frac{1}{z-\frac{x_0-\mu-\mathrm{i}\frac{\gamma}{2}}{1+\Psi/2}}\right)-\left(\frac{1}{z-\frac{x_0+\mu+\mathrm{i}\frac{\gamma}{2}}{1-\Psi/2}}+\frac{1}{z-\frac{x_0+\mu-\mathrm{i}\frac{\gamma}{2}}{1-\Psi/2}}\right)\right]dz\nonumber\\&+x_{0}\left(\mathcal{I}_{1}\left(\gamma,x_{0}, \mu,T_L\right)-\mathcal{I}_{1}(\gamma,x_{0}, -\mu,T_R)\right)
\end{align} Note that the integral $\mathcal{I}_2$ is convergent because the difference of the Fermi functions contributes asymptotically as $x^{-1}$ and the Green's function contributes another $x^{-1}$ . The integral of a single Green's function with a single Fermi function would be logarithmically divergent.
In the second equality the Fermi functions arguments have been shifted to the Green's functions terms
we can split the integral in two parts such that \begin{gather}
\mathcal{I}_2=\mathcal{I}^A+\mathcal{I}^R+x_{0}\left(\mathcal{I}_{1}\left(\gamma,x_{0}, \mu,T_L\right)-\mathcal{I}_{1}(\gamma,x_{0}, -\mu,T_R)\right)\\
\mathcal{I}^{A/R}=\frac{\gamma}{2}\int\limits_{-\infty}^\infty f_T(z)\left[\frac{1}{z-\frac{x_0-\mu\pm\mathrm{i}\frac{\gamma}{2}}{1+\Psi/2}}-\frac{1}{z-\frac{x_0+\mu\pm\mathrm{i}\frac{\gamma}{2}}{1-\Psi/2}}\right]dz=\frac{\gamma}{2}\int\limits_{-\infty}^\infty g^{A/R}(z)dz
\end{gather} closing the integration contour in the upper half and using now the series relation for the Fermi function \begin{align}
f_{T}(z)=\frac{1}{1+e^{\frac{z}{T}}}  = \frac{-\mathrm{i}}{2\pi}\sum \limits_{n=-\infty}^\infty \frac{1}{n+\frac{1}{2} +\mathrm{i}\frac{z}{2\pi T}}
\end{align} we have \begin{gather}
\mathcal{I}^{A}=\frac{\gamma}{2}2\pi\mathrm{i}\left( \operatorname{Res}(g^{A},\frac{x_0-\mu+\mathrm{i}\frac{\gamma}{2}}{1+\Psi/2})-\operatorname{Res}(g^{A},\frac{x_0+\mu+\mathrm{i}\frac{\gamma}{2}}{1-\Psi/2})+\sum\limits_{n=0}^\infty \operatorname{Res}(g^{A},(2n+1)\pi\mathrm{i}T)\right)\\
=\gamma\pi\mathrm{i}\left(f_{T}(\frac{x_0-\mu+\mathrm{i}\frac{\gamma}{2}}{1+\Psi/2})-f_{T}(\frac{x_0+\mu+\mathrm{i}\frac{\gamma}{2}}{1-\Psi/2})-T\sum\limits_{n=0}^\infty\frac{1}{(2n+1)i\pi T-\frac{x_{0}-\mu+i\gamma/2}{1+\Psi/2}}+T\sum\limits_{n=0}^\infty\frac{1}{(2n+1)i\pi T-\frac{x_{0}+\mu+i\gamma/2}{1-\Psi/2}}\right)\\
=\gamma\pi\mathrm{i}\left(f_{T}(\frac{x_0-\mu+\mathrm{i}\frac{\gamma}{2}}{1+\Psi/2})-f_{T}(\frac{x_0+\mu+\mathrm{i}\frac{\gamma}{2}}{1-\Psi/2})+\frac{i}{2\pi}\sum\limits_{n=0}^\infty\frac{1}{n+\frac{1}{2}+\frac{-\gamma/2+i(x_{0}-\mu)}{2\pi T(1+\Psi/2)}}-\frac{i}{2\pi}\sum\limits_{n=0}^\infty\frac{1}{n+\frac{1}{2}+\frac{-\gamma/2+i(x_{0}+\mu)}{2\pi T(1-\Psi/2)}}\right)\\
=\gamma\pi\mathrm{i}\frac{i}{2\pi}\left(-\sum \limits_{n=-\infty}^\infty \frac{1}{n+\frac{1}{2} +\mathrm{i}\frac{\frac{x_0-\mu+\mathrm{i}\frac{\gamma}{2}}{1+\Psi/2}}{2\pi T}}+\sum \limits_{n=-\infty}^\infty \frac{1}{n+\frac{1}{2} +\mathrm{i}\frac{\frac{x_0+\mu+\mathrm{i}\frac{\gamma}{2}}{1+\Psi/2}}{2\pi T}}+\sum\limits_{n=0}^\infty\frac{1}{n+\frac{1}{2}+\frac{-\gamma/2+i(x_{0}-\mu)}{2\pi T(1+\Psi/2)}}-\sum\limits_{n=0}^\infty\frac{1}{n+\frac{1}{2}+\frac{-\gamma/2+i(x_{0}+\mu)}{2\pi T(1-\Psi/2)}}\right)\\
=\frac{\gamma}{2\pi}\left(\sum \limits_{n=-\infty}^\infty \frac{1}{n+\frac{1}{2} +\frac{-\gamma/2+i(x_{0}-\mu)}{2\pi T_L}}-\sum \limits_{n=-\infty}^\infty \frac{1}{n+\frac{1}{2} +\frac{-\gamma/2+i(x_{0}+\mu)}{2\pi T_R}}-\sum\limits_{n=0}^\infty\frac{1}{n+\frac{1}{2}+\frac{-\gamma/2+i(x_{0}-\mu)}{2\pi T_L}}+\sum\limits_{n=0}^\infty\frac{1}{n+\frac{1}{2}+\frac{-\gamma/2+i(x_{0}+\mu)}{2\pi T_R}}\right)\\
=\frac{\gamma}{2}\left(\sum \limits_{n=-\infty}^{-1} \frac{1}{n+\frac{1}{2} +\frac{-\gamma/2+i(x_{0}-\mu)}{2\pi T_L}}-\sum \limits_{n=-\infty}^{-1} \frac{1}{n+\frac{1}{2} +\frac{-\gamma/2+i(x_{0}+\mu)}{2\pi T_R}}\right)\\
=\frac{\gamma}{2}\left(\sum \limits_{n=1}^{\infty} \frac{-1}{n-\frac{1}{2} +\frac{\gamma/2-i(x_{0}-\mu)}{2\pi T_L}}+\sum \limits_{n=1}^{\infty} \frac{1}{n-\frac{1}{2} +\frac{\gamma/2-i(x_{0}+\mu)}{2\pi T_R}}\right)\\
=\frac{\gamma}{2}\left(\psi(\frac{1}{2} +\frac{\gamma/2-i(x_{0}-\mu)}{2\pi T_L})-\psi(\frac{1}{2} +\frac{\gamma/2-i(x_{0}+\mu)}{2\pi T_R})\right)
\end{gather} with $T_{L/R}=T(1\pm\Psi/2)$ . In the last step we have used \begin{align}
\psi(z+1)=-\gamma+\sum\limits_{n=1}^\infty\left(\frac{1}{n}-\frac{1}{n+z}\right)
\end{align} for the second contribution \begin{gather}
\mathcal{I}^{R}=\frac{\gamma}{2}2\pi\mathrm{i}\left(\sum\limits_{n=0}^\infty \operatorname{Res}(g^{R},(2n+1)\pi\mathrm{i}T)\right)\\
=\gamma\pi\mathrm{i}\left(\frac{i}{2\pi}\sum\limits_{n=0}^\infty\frac{1}{n+\frac{1}{2}+\frac{\gamma/2+i(x_{0}-\mu)}{2\pi T(1+\Psi/2)}}-\frac{i}{2\pi}\sum\limits_{n=0}^\infty\frac{1}{n+\frac{1}{2}+\frac{\gamma/2+i(x_{0}+\mu)}{2\pi T)}}\right)\\
=\frac{\gamma}{2}\left(\psi(\frac{1}{2}+\frac{\gamma/2+i(x_{0}-\mu)}{2\pi T_L})-\psi(\frac{1}{2}+\frac{\gamma/2+i(x_{0}+\mu)}{2\pi T_R})\right)
\end{gather} So \begin{gather}
\mathcal{I}^{A}+\mathcal{I}^{R}=\frac{\gamma}{2}\left(\psi(\frac{1}{2}+\frac{\gamma/2-i(x_{0}-\mu)}{2\pi T_L})-\psi(\frac{1}{2}+\frac{\gamma/2-i(x_{0}+\mu)}{2\pi T_R})+\psi(\frac{1}{2}+\frac{\gamma/2+i(x_{0}-\mu)}{2\pi T_L})-\psi(\frac{1}{2}+\frac{\gamma/2+i(x_{0}+\mu)}{2\pi T_R})\right)\\
=\gamma\operatorname{Re} \left(\psi(\frac{1}{2}+\frac{\gamma/2+i(x_{0}-\mu)}{2\pi T_L})\right)-\gamma\operatorname{Re} \left( \psi(\frac{1}{2}+\frac{\gamma/2+i(x_{0}+\mu)}{2\pi T_R}\right)
\end{gather} Finally \begin{gather}
\mathcal{I}_2=\gamma\operatorname{Re} \left(\psi(\frac{1}{2}+\frac{\gamma/2+i(x_{0}-\mu)}{2\pi T_L})\right)-\gamma\operatorname{Re} \left( \psi(\frac{1}{2}+\frac{\gamma/2+i(x_{0}+\mu)}{2\pi T_{R}})\right)+x_{0}\left(\mathcal{I}_{1}\left(\gamma,x_{0}, \mu,T_L\right)-\mathcal{I}_{1}(\gamma,x_{0}, -\mu,T_R)\right)
\end{gather} So \begin{gather}
\boxed{W=\gamma\operatorname{Re} \left(\psi(\frac{1}{2}+\frac{\gamma/2+i(x_{0}-\mu)}{2\pi T_L})\right)-\gamma\operatorname{Re} \left( \psi(\frac{1}{2}+\frac{\gamma/2+i(x_{0}+\mu)}{2\pi T_{R}})\right)+x_{0}\left(\mathcal{I}_{1}\left(\gamma,x_{0}, \mu,T_L\right)-\mathcal{I}_{1}(\gamma,x_{0}, -\mu,T_R)\right)}
\end{gather} (The motivation is related to a quantum problem, but people downvoted previous question arguing that this is related to physics, so if somebody wants me to explain the physical interpretation of the parameters please do not hesitate.)","['integration', 'complex-integration', 'cauchy-integral-formula']"
4127316,Gauss Bonnet Theorem for half a cone,"Let $S \subset \mathbb{R}^3$ be given by $$ S = \{ (x,y,z) \in \mathbb{R}^3 \, : \, x^2+y^2=z^2 , \quad y \ge0, 0\le z \le 1 \}. $$ Verify the Gauss Bonnet Theorem, by computing $\int_S K\, dA$ and $\int_{\partial S} \kappa_g \, ds $ . Computation of $\int_S K \, dA$ : The surface (minus the pointy end) is a local isometry of a plane so the Gauss curvature $K$ is everywhere $0$ , so this integral contributes nothing. I think this can also be found easily via $K = (LN-M^2)/(EG-F^2) = 0 $ via parametrizing the half-cone. Computation of $\int_{\partial S} \kappa_g \, ds $ : The meridians of the cone are geodesics and have zero geodesic curvature, so the meridians don't contribute. The other boundary of the half-cone is the semi-circle parametrized by $\alpha(t) = (\cos(t), \sin(t), 1)$ for $ t \in (0, \pi)$ . To find the geodesic curvature $\kappa_g$ on the cone, one computes the Gauss map $N \circ \alpha(t) = (\cos(t), \sin(t), -1)/\sqrt{2}$ and then use the formula $$ \kappa_g = \frac{1}{|| \alpha'||^3} (\alpha' \times \alpha'') \cdot (N \circ(\alpha)) = \frac{-1}{\sqrt{2}} $$ Note: I am aware more careful analysis of tangent direction is required to determine the sign - but I'm quite sure it's $1/\sqrt{2}$ up to $\pm$ . The length of the semi-circle is $\pi$ , making this integral $\pm \pi/\sqrt{2}$ . And then finally the Euler Characteristic of the half-cone is $1$ . (Homeomorphic to a disc). But then it doesn't add up with the Gauss Bonnet theorem: $$ \int_S K \, dA + \int_{\partial S} \kappa_g \, ds + \sum_{i=1}^n \theta_i = 2 \pi \chi(S) $$ $$ \implies 0  \pm \frac{\pi}{\sqrt{2}} +  \sum_{i=1}^n \theta_i = 2 \pi$$ And I can't see any way the exterior angles (given by $\sum_{i=1}^n \theta_i$ ) can make this work. By my computation, we have three right-angles, giving this sum to be $3 \pi /2$ which can't cancel the $1/\sqrt{2}$ . There must be an error somewhere. Any insight or help is greatly appreciated!","['geodesic', 'surfaces', 'riemannian-geometry', 'differential-geometry']"
4127332,"What does $\int_CF=\int_a^bF(C(t))\cdot\frac{dC}{dt}\,dt$ actually mean?","Let $F$ be a continuous vector field on an open set $U$ and $C$ is a continuously differentiable curve on $U$ . We define the integral of $F$ along $C$ to be $$\int_CF=\int_a^bF(C(t))\cdot\frac{dC}{dt}\,dt$$ using the chain rule $$\int_CF=\int_{C(a)}^{C(b)}F(C)dC$$ But what does this all mean? I mean is there any geometric interpretation to the integral curve? or can you just describe what does the integral curve do? and the reason why we decompose a field with a curve?","['multivariable-calculus', 'calculus', 'vector-fields', 'intuition']"
4127339,convexity proof of a function including ln and sums,"$$f(x_1,\dots,x_n)=\sum\limits_{i=1}^nx_i\ln x_i-\left(\sum\limits_{i=1}^nx_i\right)\ln\left(\sum\limits_{i=1}^nx_i\right)\rightarrow R_{++}^n$$ How can I prove this is convex on $R_{++}^n$? I tried using the Hessian and couldn't prove it. There is a solution using the gradient and Jensen but very long and complicated.",['convex-analysis']
4127357,Pulling random times out of conditional expectation,"Problem Let $G$ be a positive random variable (a random time) that is a.s. finite, $(X)_{t \geq 0}$ be a càdlàg process taking values in $\mathbb{R}^d$ and $g$ is some sufficiently nice real-valued function that takes as input a part of the path of $X$ (i.e. $g( (X_s)_{s \in [a,b]} )$ is a real valued random variable for fixed $0 \leq a \leq b \leq \infty$ ). I want to have the following ""pull out"" property $$\mathbb{E}\left[g( (X_s)_{s \geq G} ) \mid (X_s)_{s \leq G} \right] = \mathbb{E}\left[g( (X_s)_{s \geq t} ) \mid (X_s)_{s \leq t} \right] \mid _{t = G}$$ where $\mid_{t=G}$ means evaluating the function of $t$ in $G$ . It is somewhat similar to the question here , but there, one assumes that densities exsist. Here, $G$ is $\sigma( (X_s)_{s \leq G } )$ -measurable, but is not a $\sigma( (X_s)_{s \leq t})$ stopping time (It is unobserved from only observing $X$ ). Also, \begin{equation}
\sigma( (X_{s})_{s \geq t} ) \perp \!\!\! \perp  \sigma(G 1_{G \leq t} )\; \mid \;  \sigma( (X_{s})_{s \leq t} )
\label{eq:independence}
\end{equation} My attempt : By the tower property: $$\mathbb{E}[g( (X_s)_{s \geq G} ) \mid (X_s)_{s \leq G} ] = \mathbb{E}\Big[ \mathbb{E}[ g( (X_s)_{s \geq G} ) \mid (X_s)_{s \leq G}, G ] \mid (X_s)_{s \leq G} \Big] $$ Now by disintegration (Kallenberg 1997, Theorem 5.4) $$\mathbb{E}\Big[ \mathbb{E}[ g( (X_s)_{s \geq G} ) \mid (X_s)_{s \leq G}, G ] \mid (X_s)_{s \leq G} \Big] = \int \mathbb{E}[ g( (X_s)_{s \geq G} ) \mid (X_s)_{s \leq G}, G=t ] \; \mu((X_s)_{s \leq G} , dt) $$ where $\mu(x,\cdot)$ is the regular conditional distribution of $G$ given $(X_s)_{s \leq G}=x$ (it will be a one-point measure, since $G$ is $(X_s)_{s \leq G}$ measurable). Now by a little dubious subtitution, we get \begin{align*} 
\mathbb{E}[ g( (X_s)_{s \geq G} ) \mid (X_s)_{s \leq G}, G=t ] &= \mathbb{E}[ g( (X_s)_{s \geq t} ) \mid (X_s)_{s \leq t}, G=t ] \\
&= \mathbb{E}[ g( (X_s)_{s \geq t} ) \mid (X_s)_{s \leq t}, G 1_{G \leq t}=t ] \\
&= \mathbb{E}[ g( (X_s)_{s \geq t} ) \mid (X_s)_{s \leq t} ]
\end{align*} where the last equality followed by the conditional independence. Finally, using that $\mu((X_s)_{s \leq G} , dt) = \delta_{G}(dt)$ (the dirach measure in $G$ ), we get the desired result $$\mathbb{E}[g( (X_s)_{s \geq G} ) \mid (X_s)_{s \leq G} ] = \mathbb{E}[ g( (X_s)_{s \geq t} ) \mid (X_s)_{s \leq t} ] \mid_{t=G}$$ My problem here is that i am aware of the problems and paradoxes that occur when using heuristical arguments such as the above ""dubious substitutions"" inside regular conditional expectations. So is there a way to formalize the argument above, or give a different argument for the same result? If there is additional regularity required, what is then needed? This seems related to the difference between the usual Markov property and the strong Markov property (since if the ""pull out"" property always held, it seems like the Markov property would imply the strong Markov property), but $G$ is not a stopping time here, and some additional conditional independence is known. Maybe some of the arguments from there can be used e.g. approximating the random time by a sequence of discrete valued random times?","['measure-theory', 'conditional-probability', 'independence', 'conditional-expectation', 'stochastic-processes']"
4127400,Relating the traditional definition of a vector field in terms of function with the differential geometry definition involving fibers and bundles,"My first exposure to the concept of vector fields was in highschool physics courses, which had a simple intuitive idea of being a function which aassociates a point in a given region in space (domain where function is defined) with an arrow pointing in some direction.  For example, I can give the famous electric field of point charge centered at origin as vector field: $$ \vec{E}(r) = \hat{r} \frac{kq}{r^2} $$ This is fine. Recently, I came with a more mathematically sophisiticated definition when going through some lectures on the mathematical side of general relativity. The following definition is given: A smooth vector field $\chi$ is a smooth map that is a section of the map $TM \xrightarrow{\pi} M$ where $M$ is a topological manifold and $TM$ is the tangent bundle of that manifold. Satifying the law that: $\pi \circ  \chi = id_M$ Source 44:45 , By prof. Frederic D Schuller After some few seconds, the professor explains the above definition using the following picture: He draws on the circle what I know traditionally as a vector field, and then according to length of the arrow of the tangent vector he associates points on the tangent bundle. For example, the bottom most point on the circle has a vector of zero length attached to it, so, on the tangent bundle he choses the point with a height zero above that point. Note: The prof said that it doesn't matter how we depict the tangent bundle i.e: orient the tangent lines because the tangent bundle in itself has only a structure of a set and nothing else (Maybe I am misinterpretting this point he had mentioned here , please correct me if I am wrong) This leads me to the following questions: How exactly does this 'smooth' vector field formalism coincide with the idea of vector fields as function? What exactly is the significance of this formalism?","['vector-fields', 'vector-bundles', 'differential-geometry']"
4127405,Confused about how we scale graph axis' to make the axis' dimensionless.,"I am trying to understand the solution to part $\mathrm{(iii)}$ . But, for the question I'm asking to make sense I need to include the solutions to parts $\mathrm{(i)}$ and $\mathrm{(ii)}$ also: Consider a triangular lattice where the sides of the triangles have length $d$ . The figure gives a choice of unit cells (dashed lines). $\mathrm{(i)}$ Use the sides of the unit cells as the primitive lattice vectors, $\boldsymbol{a}_1$ and $\boldsymbol{a}_2$ . Write down these vectors in Cartesian coordinates. $\mathrm{(ii)}$ Write down a pair of reciprocal space vectors $b_{1,2}$ satisfying the condition that $a_i\cdot b_j = 2\pi\delta_{ij}$ .
(If you want to use the explicit formula in three dimensions given in the lectures,
then you should pick as $\boldsymbol{a}_3$ the unit vector in the direction out of the page.) $\mathrm{(iii)}$ The reciprocal lattice vectors $\boldsymbol{G}$ are defined by $\boldsymbol{G} = h_1b_1 + h_2b_2$ where $h_{1,2}$ are integers and $\boldsymbol{b}_1$ and $\boldsymbol{b}_2$ . Sketch the lattice that is formed by the reciprocal lattice vectors $\boldsymbol{G}$ of the triangular lattice. Solutions: $\mathrm{(i)}$ The primitive lattice vectors are $\boldsymbol{a}_1 = (d, 0)$ and $\boldsymbol{a}_2 = \left(\dfrac{d}{2},\dfrac{\sqrt{3}d}{2}\right)$ . $\mathrm{(ii)}$ A choice of the primitive lattice vectors (bold arrows in
diagram) for the reciprocal lattice is $\boldsymbol{b}_1=\left(\dfrac{2\pi}{d},-\dfrac{2\pi}{\sqrt{3}d}\right)$ and $\boldsymbol{b}_2=\left(0,\dfrac{4\pi}{\sqrt{3}d}\right)$ . Other choices are possible, such as $−\boldsymbol{b}_1$ and $−\boldsymbol{b}_2$ . $\mathrm{(iii)}$ $\boldsymbol{G} = h_1\boldsymbol{b}_1 + h_2\boldsymbol{b}_2$ with integers $h_{1,2}$ .
The diagram shows all the $\boldsymbol{G}$ vectors plotted as points in $\boldsymbol{k}$ -space.
All the $\boldsymbol{G}$ vectors form a periodic array in reciprocal space. This ‘reciprocal lattice’ for a triangular lattice in real space is itself a triangular lattice in $\boldsymbol{k}$ -space. When I asked my lecturer about this scaling on the $x$ and $y$ axis he just said (something like) that it is to ""avoid having factors of $\dfrac{2\pi}{d}$ on each increment of the $x$ and $y$ axis"". This makes sense since having a dimensionless $x$ -axis looks clearer than this: and similarly for the $y$ axis. So I will first factor out $\dfrac{2\pi}{d}$ then the reciprocal lattice vectors are $\boldsymbol{b}_1=\left(\dfrac{2\pi}{d},-\dfrac{2\pi}{\sqrt{3}d}\right)=\dfrac{2\pi}{d}\left(1,-\dfrac{1}{\sqrt{3}}\right)$ and $\boldsymbol{b}_2=\left(0,\dfrac{4\pi}{\sqrt{3}d}\right)=\dfrac{2\pi}{d}\left(0,\dfrac{2}{\sqrt{3}}\right)$ . Writing it this way, I thought the graphs $x$ -axis should look like this: and similarly for the $y$ -axis. The reason I think the graph axis label should read $\dfrac{2\pi k_x}{d}$ and not $\dfrac{k_x d}{2\pi}$ (in the solution) is simply because I have factored out the $\dfrac{2\pi}{d}$ above so that what is plotted does not depend on $\dfrac{2\pi}{d}$ . Math is not my strong point and I just cannot figure out why the axis reads $\dfrac{k_x d}{2\pi}$ instead of $\dfrac{2\pi k_x}{d}$ (which is what it looks like it should be). Can anyone please explain what is going on here? Thanks in advance!","['vectors', 'graphing-functions', 'dimensional-analysis', 'fractions', 'algebra-precalculus']"
4127421,Understanding the role of Hilbert spaces in signal processing,"I'm taking a signal processing class and I've become quite confused about the nature of the subject. We began the course by elaborately developing the theory of function spaces (infinite dimensional Hilbert spaces), but only after this did I realize that signals are finite discrete functions. So it's always necessarily to embed these finite discrete-time signals in the function space in ways that seem far-fetched and rather pointless. Take the DFT for example. We have a signal $x[n]$ of finite length, $0\leq n < N-1$ and we embed this in $L^2([0,2 \pi])$ by interpreting the components as sample values for some periodic function on $[0,2\pi]$ . Then we apply the Fourier transform to get the coefficients of the Fourier series. This eventually leads to defining the space of $P_N$ of N-periodic functions with basis of the sampled complex exponentials. The story is similar for the Haar wavelet basis. The decomposition is defined on the entire continuous space $L^2(\mathbb{R})$ , and after all the effort of embedding we get a discrete transform. My question is: why not just define the discrete transforms independently, without this continuous Hilbert space machinery?","['signal-processing', 'functional-analysis']"
4127444,Motivation for quantum cohomology rings,"I can't seem to find a good source for the motivation for defining the big quantum cohomology ring with its quantum product. Collecting the Gromov-Witten invariants in a generating function seems like a sensible thing to do (and is supposedly related to free energy in the topological $A$ -model), but I don't see where the quantum product comes from. I vaguely understand that this is supposed to come from quantum field theory/string theory, but the only connection to that effect that I'm aware of is the equivalence between 2D TQFTs and Frobenius algebras and the identity $\langle a*b, c\rangle = \langle a,b,c\rangle$ (which comes from diffeomorphism invariance); setting the three point functions to be Gromov-Witten invariants for (as an example) $\mathbb{P}^r$ produces a TQFT with quantum product $\ast$ defined by the above identity and linearity: $$h^i * h^j = \sum_{e+f =r} \langle h^i,h^j,h^e \rangle h^f$$ where $h \in H^\ast(\mathbb{P}^r)$ is the hyperplane class. Examining the nonvanishing GW invariants, this produces the ring structure $\mathbb{Z}[h]/(h^{r+1}-1)$ which is the $q=1$ limit of the small quantum cohomology ring, or, equivalently, the $\mathbf{x} =0$ limit of the big quantum cohomology ring. Is the leap from this to either of the quantum cohomology rings ad-hoc, or is there an underlying explanation? Is there a physical model underlying these constructions that is more complicated than a TQFT? Edit: $\mathbb{Z}[h]/(h^{r+1} -1)$ is precisely the deformation of the ordinary cohomology ring described in Witten's paper 2D Gravity and Intersection Theory on Moduli Space (up to choice of coefficients) as the ""quantum cohomology ring"", and he says that the big quantum cohomology ring generalizes this, but it's opaque to me how one would come up with the definition $\mathcal{O}_\alpha \mathcal{O}_\beta = f_{\alpha \beta}^\gamma \mathcal{O}_\gamma$ where $f_{\alpha \beta \gamma} = \frac{\partial^3 F}{\partial y^\alpha \partial y^\beta \partial y^\gamma}$ and where $F$ is the Gromov-Witten potential (this is precisely the quantum product). He writes that the above equality is equivalent to a certain overdetermined set of PDEs, which might be related to the boundary divisor identities (e.g the three boundary points on $\overline{M}_{0,4} = \mathbb{P}^1$ are linearly equivalent, and pulling these boundary divisors back produces nontrivial equivalences on the spaces $\overline{M}_{0,n}(\mathbb{P}^r,d)$ ).","['topological-quantum-field-theory', 'algebraic-geometry', 'quantum-field-theory', 'intersection-theory', 'mathematical-physics']"
4127447,"St. Basil's cathedral, Moscow steeple shape","Onion-shaped dome cathedral architecture seen here appears to include in its lower part a geometry of positive, and in upper (steeple) part negative Gauss curvature. The corresponding elliptic and hyperbolic geodesics transition at an inflection cone radius. They typically have continuous contrasting curved lines as seen on the middle yellow tessellated surface. It has been modeled approximately now like: What intrigues me is the artful way its architect produced a variety of cupolas on the same onion meridian ( upto scale ) by changing : Spacing between the (two combined) geodesics, 2) Flatness /Roughness appearance by choosing tessellation heights  , 3) even Blending the tessatations into a  smooth continuous spiral ""garlic"" tubular form shown sectioned at right, 3) Coloring  schemes... among others. How might have been the lines drawn and built? Was differential geometry developed enough during Ivan's time in architecture for computations? Are there such shapes built as domes/steeples elsewhere in the world?","['geodesic', 'art', 'applications', 'soft-question', 'differential-geometry']"
4127448,"Problem of Analysis about Matrix Exponential, Infimum and Limit","Let $f: \mathbb{R}^{n^2} \times \mathbb{Z}^{n} \longrightarrow \mathbb{R}$ defined by $$
f(X,z) = \prod_{i=1}^{n} |x_i z|,
$$ where $x_i$ is the $i$ -th row of $X$ and $x_iz$ is a dot product of $x_i$ and $z$ .
My question is:
Is it true that $$
\inf_{z \neq 0} \lim_{X \rightarrow 0} f(e^{X+C}, z) = \lim_{X \rightarrow 0} \inf_{z \neq 0} f(e^{X+C}, z) ?
$$ Some observations that I deduce about this: $e^{X} = \displaystyle\sum_{k=0}^{\infty} \dfrac{X^{k}}{k!}$ is infinitely differentiable and therefore $e^{X+C}$ is continuous. I think that you don´t need to know about the constant $C$ , but $C = \log b$ and the definition is that $e^{C}=b$ . $C$ is a matrix, i.e., $C \in \mathbb{R}^{n^2}$ . Actually, still I believe that we don´t need to know about the function $f$ the same way that I wrote above. I think that the problem can be formulated as follows: Given $f: \mathbb{R}^{n^2} \times \mathbb{Z}^{n} \longrightarrow \mathbb{R}$ continuous, is it true that $\inf_{z \neq 0} \lim_{X \rightarrow 0} f(X, z) = \lim_{X \rightarrow 0} \inf_{z \neq 0} f(X, z)$ ? So I can´t do how prove it. I tested with some examples in Matlab and Wolfram Mathematica and the equality has always been valid. One of the ways which I thought was to ""open"" the expressions of both sides of equality and try to reach the same result. To reach the infimum, I thought that $z$ should have all coordinates equal to zero, except 1 coordinate. In this case, I have $z \neq 0$ and maybe this is a vector that works to prove it. But I didn´t get anywhere.","['multivariable-calculus', 'analysis']"
