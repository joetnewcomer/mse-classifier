question_id,title,body,tags
4528675,First order description of connected components?,"Let $X\subset\mathbb{R}^{n}$ , and let $\psi=(x\in X)$ , a first order formula in the language of the real field, enriched by the single predicate $(x\in X)$ . Is it possible to define the connected components using $\psi$ (and the field operations, so everything in ""epsilon-delta language"" is allowed)? More specifically, given $x\in X$ , is there an ""operator"" $\mathcal{L}(\psi,x)$ on pairs of first order formulae and points in $\mathbb{R}^{n}$ , which doesn't depend on $X$ (!), so that the formula $\mathcal{L}(\psi,x)$ defines the connected component of $x$ in $X$ ? For example, the operator $\psi\mapsto\forall\epsilon\exists y\;\psi(y)\wedge\;|y-x|<\epsilon$ defines the closure of $X$ , and is independent of $X$ . I truly suspect that the answer is no. However, I absolutely know nothing about logic, in fact, I'm not sure my question is even well phrased, much less to prove that one can't define the connected components of $X$ using first order formulas. Edit: In the comments, Alex Kruckman suggested the question of whether connected components are uniformly definable. While this seems to be a more formal question, it is not completly equivalent to my question. In fact, I am asking this in regard to o-minimality, where connected components are uniformly definable - given a definable family, one can consider a cellular decomposition of its total space, and then the connected components of each fiber is a union of projections of cells from the decomposition. However, while this shows connected components are uniformly definable, it still doesn't answer my question - each family has a different cellular decomposition - so the ""operator"" $\mathcal{L}$ will not be universal, it will depend on the family.","['general-topology', 'logic', 'model-theory']"
4528687,Dimension of the space of derivations for finitely differentiable manifolds,"The $C^\infty$ -case: If $M$ is a (paracompact Hausdorff) real $C^\infty$ -manifold and $p \in M$ then the tangent space $T_pM$ can be defined in several different but equivalent ways. One possibility is to define a tangent vector via its action on smooth functions $M \to \mathbb{R}$ and set $T_pM := \mathrm{Der}_p(C_p^\infty(M))$ . Here $C_p^\infty(M)$ is the algebra of germs at $p$ of smooth real valued functions, and $\mathrm{Der}_p(C^\infty(M))$ is the space of derivations of $C_p^\infty(M)$ relative to evaluation at $p$ , i.e. the space linear maps $\varphi: C_p^\infty(M) \to \mathbb{R}$ satisfying $\varphi(fg) = \varphi(f)g(p) + f(p)\varphi(g)$ for $f,g \in C_p^\infty(M)$ . Equivalently one can also use $\mathrm{Der}_p(C^\infty(U))$ for an arbitrary open neighbourhood $U$ of $p$ . To proof the finite-dimensionality of $T_pM$ we can assume without loss of generality that $M=\mathbb{R}^n$ , $p=0$ and that $U$ is a convex set containing $0$ . One can then write down a basis for $T_pM$ , namely the derivations $\frac{\partial}{\partial x^i}|_0$ where $i \in \{1,\ldots,n\}$ . To see that this is a spanning set one first writes any smooth function $g$ on $U$ as $g = g(0) + \sum_i g_ix^i$ , where $g_i(x) = \int_{0}^1\frac{\partial g}{\partial x^i}(tx)dt$ (using the fundamantal theorem of calculus) and then applies a derivation $\varphi$ to obtain $\varphi(g) = \sum_i \varphi(x^i)\frac{\partial}{\partial x^i}|_0(g)$ . For linear independence one evaluates at the coordinate functions $x^i$ . The $C^r$ -case: In contrast, for a $C^r$ -manifold where $r$ is a positive integer the situations looks different. Namely here $\mathrm{Der}_p(C^r(U))$ is infinite dimensional. A proof outline of this can for example be found in J.M.Lee's book ""Manifolds and differential geometry"", Chapter 2, Problem (18) on page 124. First it is established that as vector spaces $\mathrm{Der}_p(C^r(U)) \cong (\mathfrak{m}_r/\mathfrak{m}_r²)^*$ , where $\mathfrak{m}_r$ is the maximal ideal of $\mathrm{Der}_p(C^r(U))$ given by functions $f$ that vanish at $p$ , $f(p)=0$ . In the case $M=\mathbb{R}$ and $p=0$ one then defines functions $g^r_\varepsilon(x) := \begin{cases}x^{r+\varepsilon} & x \geq 0 \\ 0 & x \leq 0\end{cases}$ for $\varepsilon \in (0,1)$ and shows that their equivalence classes are all linearly independent in $\mathfrak{m}_r/\mathfrak{m}_r²$ . The general case follows from this as well. Question: Going through the proof of finite-dimensionality in the $C^\infty$ -case I can't see where we need the assumption of smoothness, it all seems to work verbatim for the $C^r$ -case. That we can choose an arbitrary open neighbourhood $U$ for the proof follows from the existence of cutoff functions. These are smooth and in particular $C^r$ , so this also works in the latter case. In the argument that the $\frac{\partial}{\partial x^i}|_0$ form a basis we only seem to assume $C^1$ really. This of course stands in contradiction to the fact that $\mathrm{Der}_p(C^r(U))$ is infinite-dimensional. So where does the proof go wrong?","['differential-topology', 'differential-geometry', 'real-analysis']"
4528700,Walter Rudin's proof: countable union of countable sets is countable,"The capture is from Rudin's Principles of Mathematical Analysis , and I've seen similar proof for this theorem but with a different technique. It uses a single arrow to throw all the elements, the arrow can wiggle and turn around, this kind of proof I could understand. Q1 : When I see Rudin's proof, I was confused about how those arrows have bijections with $\mathbb{N}$ . It said arrange in sequence, it seems that the sequence's terms are increasing(e.g $2$ nd term is a two-element tuple, $3$ rd term is a three-element tuple, etc.) can a sequence have terms for different types?","['proof-explanation', 'analysis', 'real-analysis']"
4528730,Does every finitely generated group have finitely many retracts up to isomorphism?,"The infinite dihedral group $D_\infty = \langle a,b \mid a^2 = b^2 = \text{Id}\rangle $ is a finitely generated group with infinitely many cyclic subgroups of order 2, every one of which is a retract. For the group $\mathbb{Z}\oplus\mathbb{Z}$ ‎, ‎take $H_n=\langle (1,n)\rangle$ for any integer $n$ (with $K=\langle (0,1)\rangle$ )‎. ‎Then we have $\mathbb{Z}\oplus \mathbb{Z}=H_n \oplus K$ which shows that $\mathbb{Z}\oplus\mathbb{Z}$ (hence every finitely generated abelian group) has infinitely many different retracts‎. Every free group $F_n$ of finite rank $n$ has infinitely many retracts. In fact, each free factor of $F_n$ is a retract and there are infinitely many free factors. These are examples of finitely generated groups with infinitely many retracts. If we look at them, we'll find that they have finitely many retracts up to isomorphism. My question is that does every finitely generated group have finitely many retracts up to isomorphism?","['finitely-generated', 'group-theory', 'retraction']"
4528736,Tensoring an abelian group with a divisible abelian group,"$\newcommand{\Tors}{{\rm Tors}}
\newcommand{\tf}{{\rm t.f.}}$ Let $A$ be an abelian group. We denote by $A_\Tors$ the torsion subgroup of $A$ .
We set $A_\tf=A/A_\Tors$ ,, which is a torsion free abelian group. We say that an abelian group $B$ is divisible , if for any element $b\in B$ and any natural $n\in{\Bbb Z}_{>0}$ there exists an element $b'\in B$ (maybe non-unique) such that $n b'=b$ . I am looking for a reference for the following well-known lemma: Lemma. Let $A$ be an abelian group and $B$ be a divisible abelian group.
Consider the canonical homomorphism $$\tf\colon A\to A_\tf:= A/A_\Tors\,.$$ Then the induced homomorphism $$(\tf)_*\colon A\otimes B\to A_\tf\otimes B$$ is an isomorphism. I know a proof of this lemma; I am looking for a reference.","['reference-request', 'tensor-products', 'group-theory', 'abelian-groups', 'commutative-algebra']"
4528746,"Find the all possible values of $a$, such that $4x^2-2ax+a^2-5a+4>0$ holds $\forall x\in (0,2)$","Problem: Find the all possible values of $a$ , such that $$4x^2-2ax+a^2-5a+4>0$$ holds $\forall x\in (0,2)$ . My work: First, I rewrote the given inequality as follows: $$
\begin{aligned}f(x)&=\left(2x-\frac a2\right)^2+\frac {3a^2}{4}-5a+4>0\end{aligned}
$$ Then, we have $$
\begin{aligned}
0<x<2\\
\implies -\frac a2<2x-\frac a2<4-\frac a2\end{aligned}
$$ Case $-1:\,\,\,a≥0 \wedge 4-\frac a2≤0$ . This leads, $$
\begin{aligned}\frac {a^2}{4}>\left(2x-\frac a2\right)^2>\left(4-\frac a2\right)^2\\
\implies \frac {a^2}{4}+\frac {3a^2}{4}-5a+4>f(x)>\left(4-\frac a2\right)^2+\frac {3a^2}{4}-5a+4\end{aligned}
$$ For $f(x)>0$ , it is enough to take $\left(4-\frac a2\right)^2+\frac {3a^2}{4}-5a+4>0$ with the restriction $a≥0\wedge 4-\frac a2≤0$ . Case $-2:\,\,\,a≤0 \wedge 4-\frac a2≥0$ . We have: $$
\begin{aligned}\frac {a^2}{4}<\left(2x-\frac a2\right)^2<\left(4-\frac a2\right)^2\\
\implies \frac {a^2}{4}+\frac {3a^2}{4}-5a+4<f(x)<\left(4-\frac a2\right)^2+\frac {3a^2}{4}-5a+4\end{aligned}
$$ Similarly, for $f(x)>0$ , it is enough to take $\frac{a^2}{4}+\frac {3a^2}{4}-5a+4>0$ with the restriction $a≤0\wedge 4-\frac a2≥0$ . Case $-3:\,\,\,a≥0 \wedge 4-\frac a2≥0$ . This case implies, $\left(2x-\frac a2\right)^2≥0$ .  This means, $f(x)≥\frac {3a^2}{4}-5a+4$ Thus, for $f(x)>0$ , it is enough to take $\frac {3a^2}{4}-5a+4>0$ with the restriction $a≥0\wedge 4-\frac a2≥0$ . Finally, we have to combine all the solution sets we get. I haven't done the calculation, because I want to make sure that the method I use is correct. Do you see any flaws in the method?","['inequality', 'solution-verification', 'polynomials', 'algebra-precalculus', 'quadratics']"
4528762,Why is the sample mean a random variable?,"I know that a random variable is a measurable function from some measurable space $(\Omega, \mathscr{F})$ to some borel space $(R, \mathscr{B})$ . Suppose I am given some random sample $X_1, ..., X_n$ . From the definition of a random sample, each $X_i:\Omega \rightarrow R$ are a measurable function with common domain the sample space $\Omega$ . I know that the sample mean is a random variable. Hence it is a measurable function. So what type of function is the sample mean exactly? From which measurable space to where? And what exactly does the notation $\bar{X} = n^-1 \sum _i^n X_i$ mean given that it does not mean that $\bar{X} (s)= n^{-1} \sum _i^n X_i(s)$ ? I ask because usually defining a function $f$ by $f = v+g$ where $g:A \rightarrow R, v:A \rightarrow R$ implies that $f(x) = v(x) + g(x) \forall x \in A$ holds.","['statistical-inference', 'statistics']"
4528767,Is exercise 1.10d from Hartshorne just entirely wrong or is it salvageable in any non-trivial way?,"The question is (d) If $Y$ is a closed subset of an irreducible finite-dimensional topological space $X$ , and if $\dim Y=\dim X$ , then $Y=X$ . Here we are using this definition for dimension Definition. If $X$ is a topological space, we define the dimension of $X$ (denoted $\dim X$ ) to be the supremum of all integers $n$ such that there exists a chain $Z_0 \subsetneq Z_1 \subsetneq \ldots \subsetneq Z_n \subsetneq X$ of distinct irreducible closed subsets of $X$ . Letting $X = \{1,2,3\}$ and $Y= \{1,2\}$ where the closed sets are $\emptyset, \{1\}, \{2\}, \{1,2\}, \{1,2,3\}$ . You can check that $X$ is indeed irreducible, $Y$ is a closed subset, $\dim X = \dim Y = 0$ , but clearly $Y \neq X$ . You can salvage the statement by requiring $Y$ to also be irreducible but then the exercise is almost trivial. I am wondering if I am missing something here or if there's some non-trivial way you can salvage this exercise. (Proof that $\dim X = \dim Y = 0$ )
The only irreducible closed subsets of $Y$ are $\{1\}$ and $\{2\}$ , these sets are mutually disjoint so clearly, the only maximal chains of irreducible closed subsets of $Y$ are $$\{1\} \subsetneq Y \quad \text{and} \quad \{2\} \subsetneq Y.$$ Thus $\dim Y = 0$ , note that since $Y$ itself is NOT irreducible we also have that the only irreducible closed subsets of $X$ are $\{1\}$ and $\{2\}$ , so again the only maximal chains of irreducible closed subsets of $X$ are $$\{1\} \subsetneq X \quad \text{and} \quad \{2\} \subsetneq X.$$ Thus $\dim X = 0$ .","['general-topology', 'algebraic-geometry']"
4528771,Is a spherically symmetric space-time isometric to a warped product?,"A spherically symmetric spacetime is a Lorentian 4-dimensional manifold $(M, g)$ whose isometry group contains a subgroup $G$ isomorphic to $\text{SO}(3)$ and whose orbits are 2-spheres. Here I am already confused. How can the orbits of $G$ , which are submanifolds of $M$ , be 2-spheres? Am I missing some general definition of 2-spheres that does not require being in Euclidean space? Next, how can we rigorously decompose the metric of this spacetime into one of the form $$
g = A(r, t) \text d r ^2 + B(r, t) \text d r \text d t + C(r, t) \text d t^2 + C(r, t) \text d \Omega ^2 \ ? 
$$ I am willing to take for granted that $\text{Iso}(M)$ is a Lie group and thus $G$ is diffeomorphic to $\text{SO}(3)$ . In particular, the Lie algebra of $G$ is isomorphic to that of $\text{SO}(3)$ which is isomorphic to $(\Bbb R^3, \times)$ . Because the Lie algebra of the isometry group consists of Killing fields, this means we have 3 Killing Fields $V_i$ s.t. $$
[V_i, V_j] = \epsilon_{ijk} V_k.
$$ By Frobenius theorem, these vector fields generate  foliation of $M$ , and around every point of $M$ we can find a coordinate chart $(U, x^i)$ s.t. each leaf of our foliation corresponds to slices of constant $x^i$ with, say, $i = 0, 1$ . Now I somehow need to find a coordinate transformation that ensures the vector fields $\partial_0$ and $\partial_1$ are orthogonal to every leaf of the foliation. To show that the inner products of the coordinate vector fields are independent of their location on each leaf I imagine I have to use the isometry conditions, but I do not know how the coordinate vector fields relate to the isometries themselves. Is there a canonical way to do this?","['foliations', 'riemannian-geometry', 'differential-geometry']"
4528782,The coefficients of the power series $\frac{1}{1-z-z^2}$ centered at $0$ are the numbers in the Fibonacci sequence [duplicate],"This question already has answers here : Summation of Fibonacci numbers. (3 answers) Closed 1 year ago . So I need to show the coefficients of the power series centered about $z=0$ of $\frac{1}{1-z-z^2}$ are $1,1,2,3,5,8,13,21,\dots$ Here's what I have thus far but I am unable to show all coefficients are equal to the coefficients of the series. Let $F(z):= \sum_{n=0}^\infty a_nz^n$ Then \begin{align}
\sum_{n=0}^\infty a_nz^n &= a_0 + a_1z + z^2 \sum_{n=0}^\infty (a_{n+2})z^n \\
&= 1+z+ z^2 \sum_{n=0}^\infty (a_{n+1}+a_n)z^n\\
&= 1+ z+ z \sum_{n=0}^\infty a_nz^n - 1 + z^2(\sum_{n=0}^\infty a_nz^n)\\
&= 1+ z(\sum_{n=0}^\infty a_nz^n)+z^2(\sum_{n=0}^\infty a_nz^n)
\end{align} Then setting $F(z)= \sum_{n=0}^\infty a_nz^n$ , bringing everything to the other side and dividing we obtain $$F(z) = \frac{1}{1-z-z^2}$$ But I am stuck here. I saw another version of this asked but it uses the Cauchy Integral formula which we have not yet covered in the course. The hint was to write $\sum_{n=0}^\infty a_nz^n$ and use the recursive formula to rewrite the power series.","['complex-analysis', 'fibonacci-numbers', 'generating-functions']"
4528820,How to interpret the (possible) relationship between Jacobian and Covariance matrix,"I'm researching a problem which suggests that progress could be achieved if the Jacobian of a vector function might be in some way considered in the manner of a covariance matrix. Specifically, and to take the case in $\mathbb{R}^2$ , let $f_1 := f_1(x,y)$ and $f_2 := f_2(x,y)$ for $x,y \in \mathbb{R}$ and $f_1,f_2: \mathbb{R}^2 \rightarrow \mathbb{R}$ , the quadratic form of the Jacobian is $$
\mathbf{J}^{\intercal}\mathbf{J} = 
\left[
\begin{array}{cc}
\left(\frac{df_1}{dx}\right)^2+\left(\frac{df_2}{dx}\right)^2 & \frac{df_1}{dx}\frac{df_1}{dy}+\frac{df_2}{dx}\frac{df_2}{dy} \\
\frac{df_1}{dx}\frac{df_1}{dy}+\frac{df_2}{dx}\frac{df_2}{dy} & \left(\frac{df_1}{dy}\right)^2+\left(\frac{df_2}{dy}\right)^2
\end{array}
\right].
$$ Suppose we were to compare with the form of covariance matrix $\mathbb{\Sigma}$ expressed as $$
\mathbb{\Sigma} = 
\left[
\begin{array}{cc}
\sigma_x^2 & \rho \sigma_x \sigma_y \\
\rho\sigma_x \sigma_y & \sigma_y^2
\end{array}
\right],
$$ it would impose that the correlation coefficient is given by $$
\rho = \frac{\frac{df_1}{dx}\frac{df_1}{dy}+\frac{df_2}{dx}\frac{df_2}{dy}}
{\sqrt{\left(\frac{df_1}{dx}\right)^2+\left(\frac{df_2}{dx}\right)^2}
\sqrt{\left(\frac{df_1}{dy}\right)^2+\left(\frac{df_2}{dy}\right)^2}
}.
$$ My questions are therefore: Is $\rho$ admissible as a correlation coefficient in the sense that $|\rho| \leq 1$ ? Are there conditions on $f_1,f_2$ under which (1) is satisfied if not satisfied always? What is the geometric/functional interpretation of the two terms in the denominator?  Clearly they represent norms or 'distances' but is there an intuitive characterisation in terms of $f_1,f_2$ ? Are there any immediate implications for finding the eigenvalues of $\mathbf{J}^{\intercal}\mathbf{J}$ ? What branch of mathematics have I happened upon? Any suggestions for reading?","['derivatives', 'statistics', 'linear-algebra', 'real-analysis']"
4528843,"""Functional Analysis"" by Kesavan, Exercise 2.32 (b)","""Functional Analysis"" by Kesavan, Exercise 2.32 (b) Define $T, S\colon \ell_2 \to \ell_2$ by \begin{align*}T(x) &= (0, x_1, x_2, \dots) \\ S(x) &= (x_2, x_3, \dots)\end{align*} where $x = (x_1, x_2, \dots) \in \ell_2$ . If $A$ is a continous linear operator on $\ell_2$ such that $\Vert A - T\Vert < 1$ , show that $A$ is also not invertible.
Deduce that $\mathcal{G}$ , the set of invertible linear operators
in $\mathcal{L}(V)$ , is not dense in $\mathcal{L}(V)$ , if $V$ is
infinite dimensional. Part (a) of the question was to show that $T, S$ are continuous linear operators, and that $ST = I$ while $TS \neq I$ , which I could do. This shows that $T$ and $S$ are not invertible. For the second statement, I have an intuition that if $\mathcal{G}$ were dense in $\mathcal{L}(V)$ , then there would exist a sequence of invertible linear operators converging to $T$ , which woud mean that some $A_n \in \mathcal{G}$ must satisfy $\Vert A_n - T\Vert < 1$ , contradicting the first statement.","['operator-theory', 'functional-analysis']"
4528853,"Showing $\sum_{i=1}^n\prod_{j\neq i}x_j^2\prod_{q\neq p,p\neq i}x_q-x_p=(\sum_{i=1}^n\prod_{j\neq i}x_j)(\prod_{q\neq p}(x_q-x_p))$","Let $(x_i)_{i=1}^n$ be a sequence of distinct nonzero real numbers (the ""distinct nonzero"" assumption is likely not necessary, but we may assume it. If you can discuss the case without this assumption, I'll likely accept the answer). I'd like to show $$\sum_{i=1}^n\left(\prod_{j \neq i} x_j^2 \prod_{q \neq p, p \neq i} (x_q-x_p)\right) =\left( \sum_{i=1}^n \prod_{j \neq i} x_j\right)\prod_{q \neq p} (x_q - x_p).$$ For instance, with $n=2$ this reads $x_2^2(x_1-x_2) + x_1^2(x_2-x_1) = (x_1 + x_2)(x_1-x_2)(x_2-x_1)$ , which is true: $x_2^2(x_1-x_2) + x_1^2(x_2-x_1) = (x_1^2 - x_2^2)(x_2-x_1)=(x_1+x_2)(x_1-x_2)(x_2-x_1)$ . We can verify the $n=3$ case on Wolfram here . I tried solving it with brute force algebra, but haven't had luck with that.  Of course, for $n>2$ , there isn't algebraic trick as simple as difference of squares, which we can do with $n=2$ . Another idea: the problem gives Lagrange interpolation vibes. The ""distinct nonzero"" assumption could be useful for dividing by $x_j - x_i$ at some point, which is relevant for Lagrange interpolation. Second idea: identifying both sides as the determinant of some matrix? The expressions look not too dissimilar from the determinant of the Vandermonde matrix.","['algebra-precalculus', 'polynomials']"
4528855,Average value from summed uniform rolls with threshold,"Suppose you roll an 6-sided dice and sum the values until a threshold is reached. When that threshold is reached any overflowing amount is discarded. What is the average dice roll value, depending on that threshold value? For a threshold of 1 , the average roll would also be 1 , since any dice roll is capped to a value of 1 . For an infinitely large threshold, the average roll would be 3.5 . What would be the formula to calculate the average value of a roll for a N -sided dice and a T threshold? Simulations I made using python :","['dice', 'stopping-times', 'discrete-mathematics', 'probability']"
4528868,Can a function satisfy two different elliptic PDEs?,"Let $D\subset \mathbb{R}^2$ be a simply connected finite domain and consider two different 2nd order elliptic operators $\nabla^2$ and $\widetilde{\nabla}^2$ . We set some continuous arbitrary function $\Omega_{\mathsf{B}}$ defined on the boundary of $D$ . Apart from the case of $\widetilde{\nabla}^2$ and ${\nabla}^2$ being linearly dependent, is there an $\Omega$ such that $$
\begin{align}
\nabla^2\Omega=0\:\:\:\text{with}\:\:\:\Omega\Big|_{\partial D}=\Omega_{\mathsf{B}}\\
\widetilde{\nabla}^2\Omega=0\:\:\:\text{with}\:\:\:\Omega\Big|_{\partial D}=\Omega_{\mathsf{B}}
\end{align} 
$$ By different I mean, for instance, that writing $\nabla^2=a_{ij}(x,y)\:\partial^2_{ij}$ and $\widetilde{\nabla}^2=\tilde{a}_{ij}(x,y)\:\partial^2_{ij}$ we have that $a_{ij}\neq\tilde{a}_{ij}$ for at least one point in the interior of $D$ . (As a special case I wish to take $\nabla^2$ to be the Laplacian, so the question would be rephrased as: given an elliptic $\widetilde{\nabla}^2$ , can we find a harmonic function $f$ with fixed boundary value such that $\widetilde{\nabla}^2f=0$ ?)","['elliptic-equations', 'analysis', 'real-analysis', 'functional-analysis', 'partial-differential-equations']"
4528878,Unclear on distributional derivative,"Suppose we have a distribution $\phi \in D'(\mathbb{R}^n)$ . The distributional derivative of $\phi$ is defined as $$\phi'(g) = -\phi(g'), \quad \forall g \in C^\infty_C(\mathbb{R}^n).$$ I am clear on this definition and how it is motivated. What has been confusing me is perhaps to due with terminology. Suppose we look at the example of the Heaviside function: $$H(x) \begin{cases} 1, \quad x>0\\ 0, \quad x\leq 0.\end{cases}$$ We can define a distribution $\phi_H$ as $$\phi_H(g) =\langle H, g\rangle = \int H g dx.$$ In textbooks you see that the derivative of the Heaviside function is calculated as $$\langle H', g \rangle = \int H' g dx = -\int Hg'dx = -\int_0^\infty g' dx = g'(0) = \langle\delta, g'\rangle$$ and so we conclude that $H' = \delta$ , where $\delta$ is the Dirac-delta function. My question is do we say that the function $H$ has derivative $\delta$ , or that the distribution induced by $H$ (what I had called $\phi_H$ ) has derivative $\delta$ ? I assume it is the latter, but I see a lot of sources say statements such as ""the derivative of the Heaviside function is the Dirac-delta function"". Is this merely abuse of notation, or do they mean to say ""the distribution induced by the Heaviside function is equal to the Dirac-delta function/distribution"". Further, is there some other connection where one implies the other? From my reading it seems that if a function $f$ induces a distribution $\phi_f$ , and $f$ is differentiable, then the distributional derivative $\phi'_f$ is equal to the distribution induced by $f'$ , $\phi_{f'}$ . Can we say anything more?","['dual-spaces', 'functional-analysis', 'distribution-theory']"
4528893,Show the special unitary group is connected,"Let $SU(n)$ be the group of $n \times n $ special unitary complex matrices. Show that $SU(n)$ is connected. Here are some thoughts: By definition $SU(n) = \{ A \in GL(n): A^*A=I=AA^*, det (A)=I \}$ . I know that $det: SU(n) \rightarrow \{1\}$ is a continuous surjective map. I also know that continuous image of a connected set is connected. But in this case it's not really helping. I showed that the group $SU(n)$ is compact. But from here I'm not sure how to proceed to show connectedness. Any help will be appreciated!","['group-theory', 'analysis', 'real-analysis']"
4529002,"What is a good explanation for ""$y = f(x)$ has the same meaning as $(x, y) ∈ f$""?","From Pinter's, A Book of Set Theory, the statement "" $y = f(x)$ has the same meaning as $(x, y) ∈ f$ "" is emboldened below Corollary 2.4. The justification given for the same meaning between the two statements is that it is customary. But I don't know how to convince myself that in all applications, that these statements are equivalent. The best justification I have is that we can be sure for a function $f: A\rightarrow B$ , for every $x\in A$ , we are given a unique element $y\in B$ , so this symbolic leap is justified in the sense that it is unique and comparable to its notational change; that is we will always have a unique pair of statements for each $x\in A$ . But my justification feels like I'm handwaving. I have some doubts because we are dealing in different predicates, namely ' $\in$ ' and ' $=$ ', and just because I can't think of a situation where these alternative statements may not work does not alleviate my doubt over the potential for error. Right now I'm operating on some faith that it just always works, but if someone were to be so kind to provide a proper explanation or some texts that might point me in the right direction, I would appreciate that.",['elementary-set-theory']
4529036,Why is the subspace topology defined as it is?,"Given a topological space $(X, \tau)$ , and $Y \subset X$ , the subspace topology on $Y$ is defined to be $$\tau_Y = \{Y \cap U : U \in \tau\}.$$ The definition itself is clear and self-explanatory, what I am wondering here is why is it defined this why? Is it so that the collection of open sets within $Y$ do indeed form a topology?",['general-topology']
4529047,"Why is the definition of the monotonicity of a function an ""if"" statement and not an ""iff"" statement? [duplicate]","This question already has answers here : Are ""if"" and ""iff"" interchangeable in definitions? (15 answers) Alternative ways to say ""if and only if""? (5 answers) Closed 1 year ago . For the sake of simplicity, I will only consider the definition of a strictly increasing function: ""The function $f(x)$ is strictly increasing on an interval $K$ if $$\forall x_1, x_2 \in K, x_1 < x_2 \implies f(x_1)<f(x_2)."" (1)$$ But then I came across this statement in my textbook: ""The function $f(x)$ is strictly increasing on an interval $K$ if and only if $$\forall x_1, x_2 \in K, x_1 \neq x_2 \implies \frac{f(x_2)-f(x_1)}{x_2-x_1}>0."" (2)$$ I've tried to prove $(2)$ using definition $(1)$ but I could only prove the ""if"". I think we need more than just definition $(1)$ to prove the ""only if"", which is weird. Right? And based on that statement in my textbook, I can prove that: ""The function $f(x)$ is strictly increasing on an interval $K$ if and only if $$\forall x_1, x_2 \in K, x_1 < x_2 \iff f(x_1)<f(x_2)."" (3)$$ Statement $(3)$ is obviously more detailed than $(1)$ . So to sum up, I have two questions: Do I really need more than definition $(1)$ to prove $(2)$ ? And why don't we make definition $(1)$ more detailed, like statement $(3)$ ? Sorry it's kinda chaotic in here but I can't find a better way to organize my ideas.
Any help is appreciated.","['logic', 'monotone-functions', 'calculus', 'functions', 'definition']"
4529145,Extending the tangent bundle of a submanifold to a subbundle of the manifold,"Given an $m$ -dimensional manifold $M$ , and an $n$ -dimensional submanifold $N$ , with $n<m$ , the tangent bundle of $N$ is a smooth $n$ -dimensional subbundle of $TM|_N$ . When can $TN$ be extended to an $n$ -dimensional subbundle of $TM$ ? There are obvious examples where it's easy to extend $TN$ to a subbundle of $TM$ , but for a spiral in $\mathbb{R}^2$ , or the equator in a sphere it is not possible. Edit - It's interesting to note that a sphere doesn't admit any $1$ -dimensional subbundle. On the other hand, $\mathbb{R}^2$ obviously admits a $1$ -dimensional subbundle, but still the tangent bundle of the spiral can't be extended to a full subbundle. Similar ""spiral""-like submanifolds of any dimension $1 \le n < m$ can be found for every $m$ -dimensional manifold. Cross posted to MO","['vector-bundles', 'smooth-manifolds', 'differential-geometry']"
4529185,$\int_{0}^{\infty}\tan^{-1}\left(\frac{2x}{1+x^2}\right)\frac{x}{x^2+4}dx$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved Improve this question $$\int_{0}^{\infty}\tan^{-1}\left(\frac{2x}{1+x^2}\right)\frac{x}{x^2+4}dx$$ I am given solution for this definite integral is $\frac{\pi}{2}\left(\ln\frac{\sqrt2+3}{\sqrt2+1}\right)$ . Any idea or approach you would use to solve this?","['integration', 'calculus', 'definite-integrals']"
4529289,Has anyone seen this formula that I proved? $\lim_{n\to\infty} \frac{2n \choose n}{2n - 2m \choose n - m} = 4^m$,"I'm a high-schooler from the Netherlands. I developed the following formula, and I proved it: $$\lim_{n\to\infty} \frac{2n \choose n}{2n - 2m \choose n - m} = 4^m$$ Has anyone already seen my formula or does somebody know whether it already has been proven or not? I would like to call this 'Groen's first law' :) if it's still unknown. Thank you and have a nice day!
Jari","['limits', 'combinatorics']"
4529373,Why does Euclid need the point D in the proof of proposition 3 of the book 1?,"I'm reading Euclids Elements Book 1 and get stuck at the proof of proposition 3. There he wants to prove that one can cut of a line $C$ of a line $AB$ assuming that $C$ is smaller then $AB$ . In the proof he gave us an arbitrary line $AB$ and a shorter one $C$ . Then he could place the straight line $C$ at the point $A$ and got the line $AD$ (this can be done using proposition 2 of book 1). Then he draw the circle $DEF$ with center $A$ and radius $AD$ . Now using the definition of a circle $AD=AE$ and since $C=AD$ he got that $AE=C$ . Now my question is, why does we need the point $D$ . Wouldn't it be enough to copy the line $C$ with the compass and then get to the point E? Thanks for your help.","['euclidean-geometry', 'geometry']"
4529393,Prove that $\sum_{1\leq i\leq n}\prod_{j\neq i} \frac{1-x_ix_j}{x_i-x_j} = 0$ if $n$ is even and $1$ otherwise,"Let $x_1,\cdots, x_n$ be different real numbers. Prove that $$\sum_{1\leq i\leq n} \prod_{j\neq i} \frac{1-x_ix_j}{x_i-x_j} = \begin{cases} 0,&\text{if }n\text{ is even}\\
1,&\text{if }n\text{ is odd}\end{cases}.$$ Let $f(t) = \prod_{i=1}^n (1-x_i t)$ . Note that $f(x_i) = (1-x_i^2)\prod_{j\neq i} (1-x_ix_j)$ . First, I think one can ignore the case where $\{x_1,\dots, x_n\}\cap \{-1,1\}\neq \emptyset$ , but I'm not sure how to justify this, which is the point of this question. I think I should use a similar approach to below. For instance, if one assumes that $x_i = 1$ and $x_j=-1$ for some $i,j$ , Lagrange interpolation gives the following expression: $\sum_{1\leq k\leq n, k\neq i,j} f(x_k) \dfrac{(x-1)(x+1)}{(x_k - 1)(x_k + 1)} \prod_{a\neq j,i,k} \dfrac{x-x_a}{x_k - x_a} + f(1) \dfrac{x+1}{1+1}\prod_{1\leq k\leq n, k\neq j,i} \dfrac{x-x_k}{1-x_k} + f(-1)\dfrac{x-1}{-1-1}\prod_{1\leq k\leq n,k\neq i,j}\dfrac{x-x_k}{-1-x_k}$ . The issue is that we need one more interpolation point to uniquely determine $f$ in this case; the above expression clearly does not have to equal $f$ . Now assuming that we can ignore the above case, one can use Lagrange interpolation to get the following expression for $f$ : $\sum_{i=1}^n f(x_i) \dfrac{(x-1)(x+1)}{(x_i - 1)(x_i + 1)} \prod_{j\neq i} \dfrac{x-x_j}{x_i - x_j} + f(1) \dfrac{x+1}{1+1}\prod_{1\leq i\leq n} \dfrac{x-x_i}{1-x_i} + f(-1)\dfrac{x-1}{-1-1}\prod_{1\leq i\leq n}\dfrac{x-x_i}{-1-x_i}$ . Setting the coefficient of $x^{n+1}$ on both sides equal to zero, we eventually get that if $H(x_1,\cdots, x_n)$ denotes the desired rational function, then $H(x_1,\cdots, x_n) = \dfrac{1}2 (1 +(-1)^{n+1})$ . Also there may be other methods involving multivariate polynomials.","['contest-math', 'algebra-precalculus', 'polynomials', 'lagrange-interpolation']"
4529477,Derived function slope to the given function [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question Preface: I'm just a musician, trying to understand some math. Don't blame me, please ;) Synopsis: Given next 5 images, calculate slope of the proportion function. Make so the highest proportion is determined by the input n Image 1 , Image 2 , Image 3 , Image 4 , Image 5 Description: You can see 5 screenshots from the program FabFilter C-2. I need to do exactly the same mathematically with changing max number (max right position of the knob). My thoughts: I tried 2 fixed number - 10 and 200 (+1 is because of starting from 1; 200 is input n ): 1: f1(x) = 9x + 1
2: f2(x) = 199x + 1
3: f3(x) = f2 / f1
4: f4(x) = x * (9.95 * f3) + 1 This example works well for fixed inputs (coefficients). But for any other coefficient in f2 it's a mess. Plus the slope is more linear than it should be. Edit: I made a mistake. I started to describe it as 1 to 10 linear function. But it should be 0 through 1 or 1 through 2. So, for each given 0<=x<=1 we have to construct a non-linear function so that f(0)=1, f(0.5)=3, f(1)=100. But the last number (100) can be changed, so that f(0.5) has to change somehow too.. Edit 2: I found the solution for the domain [0,1] and the f(1)=200 on this site . I choose an EXPONENTIAL USING CURVE FITTING option. It is f(x)≈2.32033e^(4.46319x)−1.32027 There should be an explanation) P.S.: I appreciate every answer! Thanks","['functions', 'slope']"
4529534,Evaluating the limit of a polynomial fraction as the base goes to infinity where the polynomial orders are related,"I am attempting to evaluate the following limit: $$\lim_{x \rightarrow \infty} \frac{x^a}{x^b}$$ Where $a$ and $b$ are positive real numbers such that $a < b$ . It seems intuitive that this limit should evaluate to $0$ , as: $$\lim_{x \rightarrow \infty} a\log x - b \log x \leq 0$$ Because $a < b$ . I worry-- because it's been a while since I've proven anything mathematically-- that this isn't a rigorous/correct proof, and wonder if anyone would be kind enough to point me in the right direction if I am on the wrong track? Thank you!","['logarithms', 'proof-writing', 'asymptotics', 'solution-verification', 'limits']"
4529542,2nd order homogenous differential equation solution,Suppose we have this differential equation : $-\frac{2}{x^2}y(x)+\frac{d^{2}y(x)}{dx^2}=0$ One obvious function which satisfies the equation is $y(x)=x^2$ We take $x^{2}$ to be a partial solution and from it we try to find the general solution. Let $f_{o}(x) = u(x)\cdot x^{2} $ be the full solution of the differential equation: they by differentiating $f_{o}(x)$ we get $f_{o}'(x) = u'(x)x^{2}+u(x)2x$ and $f_{o}''(x) = u''(x)x^{2}+2xu'(x)+2xu'(x)+2u(x)$ we substitute into the previous equation and we get $-\frac{2}{x^{2}}u(x)x^{2}+u''(x)x^{2}+2xu'(x)+2xu'(x)+2u(x)=0\rightarrow u''(x)x^{2}+4xu'(x)=0$ Now I cannot find $u''(x)$ I tried doing the same procedure to find u(x) but the equation I get is the same with the one I have already found so I cannot continue.Help appreciated.,"['calculus', 'derivatives', 'ordinary-differential-equations']"
4529560,Expectation of $e^{i\alpha H}$,"I have $3\times 3$ a matrix $A$ defined as $$A=e^{i\alpha H}, $$ with $H$ a $3\times 3$ random Hermitian matrix, and $\alpha\in[0,\infty]$ . I am trying to determine two things: Can we say anything on the expectation of $A$ in terms of the statistics of $H$ ? What conditions do $H$ need to satisfy so that the expectation of $A$ is diagonal? Numerically I find that if the elements of $H$ have zero mean then the expectation of $A$ is diagonal. But I would like to know the general conditions for this (and why), and an explicit expression in terms of the statistics of $H$ , if it exists!","['unitary-matrices', 'matrix-exponential', 'matrices', 'hermitian-matrices', 'random-matrices']"
4529564,"Asymptotics on root $x = x_n \in [2\pi n, 2\pi n + \pi/2]$ of $\sin{x}=(\log{x})^{-1}$ [duplicate]","This question already has answers here : Asymptotic behavior of the solutions to $\sin x = (\log x)^{-1}$ (2 answers) Closed 1 year ago . Problem Description I am currently self-studying Buijn's book on asymptotic analysis ""Asymptotic Methods in Analysis"". I am stuck at the first exercise of chapter 2: I have completed the first part of the question . My problem is with proving the specific form of the $x_n$ root. My Attempt In said chapter of the book, three methods are demonstrated to tackle implicit-function problems in asymptotic analysis. Firstly , the method where one converts the equation to the form $z/f(z) = w$ with $w$ being $z$ -independent and $f(0) \ne 0$ . Then close enough to $z=0$ and for small enough $w$ , $z$ can be expressed as a power series in $w$ which satisfies the given equation. Secondly , the method where one derives a relation which can be used to get better and better approximations of the solution iteratively. Thirdly , a special case of the previous method where the relation is produced using Newton's method. ie: $$x_n = x_{n-1} - f(x_{n-1})/f'(x_{n-1})$$ The first thing I noticed is that for $x \rightarrow \infty$ , $(\ln{x})^{-1} \rightarrow 0$ therefore $\sin{x} \rightarrow 0$ . This means that $x_n \rightarrow 2 \pi n$ since $x_n \in [2 \pi n, 2 \pi n + \pi/2]$ . Setting $$x_n = 2 \pi n + z$$ I attempted to convert $\sin{x}=(\ln{x})^{-1}$ in the form of the first method descirbed above. This failed as I arrive at $$ (\sin z)^{-1} = \log{(2\pi n)} +  \log {\Big(1 + \frac{z}{2\pi n}\Big)}$$ I am not able to construct a $w$ which decreases as $n$ increases and is z-independant, while keeping $f(z)$ $n$ -independant (is this necessary?).
I got a bit further by writting $$ \log {\Big(1 + \frac{z}{2\pi n}\Big)} = O(\frac{z}{2\pi n}) = O(\frac{1}{2\pi n}), \quad (n \rightarrow \infty, z \rightarrow 0) $$ and then $$(\sin z)^{-1} = \log (2\pi n) + O(\frac{1}{2\pi n})$$ I then tried applying the first method to $(\sin z)^{-1} = \log (2\pi n)$ but unfortunately the $f(0) \ne 0$ assumption is broken. The next thing I attempted was the third method. Using Newton's method for $f(x) = \log{x}\sin{x} - 1$ , we arrive at the following iterative formula for recursive approximations $\phi_k(n)$ for $x$ : $$\phi_{k+1} = \phi_k - \frac{\log{\phi_k}\sin{\phi_k}-1}{\frac{\sin{\phi_k}}{\phi_k}+\log{\phi_k}\cos{\phi_k}}$$ If we start with $\phi_0(n) = 2\pi n$ , $$\phi_1 = 2\pi n + (\log {2 \pi n})^{-1}$$ which looks promising. But then this nice pattern breaks and we do not get the result we are looking for. So, my final attempt was to use $\phi_1$ by writting $x = \phi_1 + z$ and trying to show that $$z = O((\log{2\pi n})^{-3}), \quad (n \rightarrow \infty)$$ by using $\sin{x} \log{x} = 1$ . However this also failed Any kind of help is very much appreciated!","['numerical-methods', 'functional-analysis', 'asymptotics']"
4529566,Do equivalence classes partition all of the elements of the set that are in the relation or does it partition the entire set?,"My discrete math textbook says that ""The equivalence classes associated with an equivalence relation on a set A form a partition of A."" However, I am not sure how this would hold for something like this:
A relation R on the set of all integers not including 0 by aRb when ab > 0. This relation does not hold for a lot of integers like a = -1 and b = 5 (which would make ab = -5), so I don't see how the equivalence classes for this relation would form a partition of Z. On a side note, I am also a bit confused about the fact that we are given two constants, a and b, but the relation is defined on Z / {0} (all integers except 0) instead of Z^2 / {0,0}. I think this might be because we are considering if ab > 0, but I'm not sure.","['set-partition', 'equivalence-relations', 'relations', 'discrete-mathematics', 'elementary-set-theory']"
4529613,Showing $\int_1^\infty\left(\sqrt{\sqrt{x}-\sqrt{x-1}}-\sqrt{\sqrt{x+1}-\sqrt{x}}\right)dx=\frac4{15}\left(\sqrt{26\sqrt2-14}-2\right)$,"A Putnam problem asked to show that some improper  integral is convergent, but I was curious to see if it can be computed in closed form and Mathematica came up with this: $$\int_1^{\infty} \left(\sqrt{\sqrt{x}-\sqrt{x-1}} -\sqrt{\sqrt{x+1}-\sqrt{x}      }     \right)dx=
\frac{4}{15} \left(\sqrt{26\sqrt{2}-14 }-2\right) \approx 0.739132$$ I did a few substitutions but it didn't  turn out as an easy calculation.  I remember that eliminating square roots required some Euler type substitutions. Any ideas of how one can arrive at such a surprising result?","['calculus', 'algebra-precalculus']"
4529618,Can a smooth manifold be embedded into its tangent bundle?,"Given a smooth $n$ -dimensional manifold $M$ , one can always find an immersion into its tangent bundle $TM$ by looking at the zero-section, i.e. the map that sends $p\in M$ to $(p,0)\in TM$ . One can see this is an immersion for example by writing everything in local coordinates and checking that this is locally an inclusion. Can it be proved that this is also an embedding? (i.e. a homeomorphism onto its image). It seems intuitively clear to me that this should be true because ""the tangent bundle has a copy of $M$ inside"" but I can't find a way to prove or disprove it. Could you help me?","['submanifold', 'tangent-bundle', 'smooth-manifolds', 'differential-geometry']"
4529645,Minimum Problem,"Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be given by $$f(m) = \frac{-1}{2}m^2 + \frac{1}{\beta}\int^m_{0} g^{-1}(s)ds.$$ where $\beta$ is a positive constant, $g: \mathbb{R} \rightarrow \mathbb{R}$ is a function which positive derivative. In particular, it is strictly increasing and globally lipschitz with $g(0) = 0$ , that is, there is $k_1 > 0$ such that $|g(x) - g(y)| \leq k_1|x - y|$ , $\forall \ x, y \in \mathbb{R}$ . In particular, $|g(x)| \leq k_1|x|$ , $\forall \ x \in \mathbb{R}$ .
Assume $k_1\beta > 1$ . Affirmation: $f$ has the a global minimum. Attempt: I want to show initially that $f$ does have a minimum. If $f$ has a global minimum, then we must have $f'' > 0$ . Thus, deriving $f$ , we have $$f''(m) = -1 + \frac{1}{\beta}(g^{-1})'(m) = -1 + \frac{1}{\beta g'(m)}.$$ Now, since $g$ is globally Lipschitz, we must have $|g'(m)| \leq k_1$ , from which we conclude that $\frac{1}{\beta k_1} \leq \frac{1}{\beta g'(m)}$ . Thus, $$f''(m) = -1 + \frac{1}{\beta g'(m)} \geq -1 + \frac{1}{k_1 \beta}.$$ From here, we can't conclude anything, since $k_1\beta > 1$ , by hypothesis.","['derivatives', 'real-analysis']"
4529657,"Why is Itô integration not a ""pathwise solution theory"" to SDEs, but rough path theory is?","I am studying the rough path theory of Lyons and am struggling to understand a heuristic claim made in several books and articles. Let $(\Omega,\mathcal{F},\mathcal{F}_t, P, B_t)$ be a filtered probability space on which a standard Brownian motion $B_t$ is defined. Let $f:\mathbb{R}\to\mathbb{R}$ be a function (say, Lipschitz). Suppose $X_t$ is an $\mathcal{F}_t$ -adapted process which solves the SDE $dX_t = f(X_t)dB_t$ , that is, such that $X_t$ satisfies $$
X_t - X_0 = \int_0^tf(X_s)dB_s\quad\text{a.s.},
$$ where the the integral is in the sense of Itô. The Itô integral can then be obtained as the limit in $L^2$ of the Riemann-Stieltjes sum: $$
\sum_{0=t_0<t_1<\cdots<t_n=t}f(X_{t_i})(B_{t_{i+1}}-B_{t_i})\xrightarrow{\sup_i|t_i-t_{i+1}|\to 0}_{L^2(\Omega,\mathcal{F},P)}\int_0^tf(X_s)dB_s.
$$ Many authors claim that rough path theory constitutes a ""pathwise SDE solution theory"": given a realization $B_t(\omega)$ of the driving Brownian motion, one can solve the corresponding ""rough differential equation"" driven by the corresponding realization of the Brownian rough path. In particular, the analogous statement is claimed to not hold for the Itô integral, because the above limit is only in $L^2$ (indeed, the above limit cannot be almost sure because Brownian motion has unbounded variation on any compact interval). On the other hand, a routine application of the Borel-Cantelli lemma shows the existence of a sequence of partitions along which the above limit is almost sure. My question is: in what sense does the realization of the Itô integrals $\int_0^tf(X_s)dB_s$ as an a.s. limit of Riemann-Stieljtes sums fail to constitute a ""pathwise solution theory?"" What defect of this ""pathwise"" construction does rough path theory mitigate?","['stochastic-integrals', 'stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory']"
4529674,Product of areas in a circle: Why does it approach $1$?,"In a circle of radius $a$ , point $P$ is located $pa$ from the centre, where $0<p<1$ . Draw $n$ line segments from $P$ to the circle, such that neighboring line segments have equal angles between them. (There is more than one many way to do this, because all the line segments can be rotated together; any of these ways will do.) Here is an example with $p=\frac{1}{3}$ and $n=12$ . I am trying to prove (or disprove) the following conjecture (which I worked out by playing with desmos). For constant $p$ , if the mean area of the regions is always $\dfrac{1}{1-p^2}$ , then $L=\lim\limits_{n\to\infty}{(\text{product of areas})}=1$ . My attempt: The cartesian equation of the circle, with $P$ at the origin, is $$(x-pa)^2+y^2=a^2$$ Changing this to a polar equation, and using $\dfrac{\pi a^2}{n}=\dfrac{1}{1-p^2}\implies a=\sqrt{\dfrac{n}{\pi(1-p^2)}}$ , we get $$\left(r\cos{\theta}-p\sqrt{\dfrac{n}{\pi(1-p^2)}}\right)^2+(r\sin{\theta})^2=\dfrac{n}{\pi(1-p^2)}$$ Using the formula for area in polar coordinates, we have $$L=\lim\limits_{n\to\infty}{\prod_{k=1}^n{\dfrac{1}{2}\int_{2(k-1)\pi/n+t}^{2k\pi/n+t}r^2{\text{d}\theta}}}$$ ( $t$ is there because all the line segments can be rotated together.) Extracting $r$ from the polar equation of the circle and putting it into the integral, and simplifying, we get $$L=\lim_{n\to\infty}{\prod_{k=1}^n{\dfrac{n}{2\pi(1-p^2)}\int_{2(k-1)\pi/n+t}^{2k\pi/n+t}\left(p\cos{\theta}+\sqrt{1-(p\sin{\theta})^2}\right)^2{\text{d}\theta}}}$$ Desmos suggests that $L=1$ (for $0<p<1$ and $t\in \mathbb{R}$ ), but I have not been able to prove this. Wolfram evaluates the indefinite integral, but it seems hopelessly intractable. (Context: I am interested in questions like this about the product of areas in a circle. Here is another one.)","['integration', 'area', 'circles', 'geometry', 'infinite-product']"
4529688,A fair coin is flipped $100000$ times and you get $100000$ Heads in a row. What is the probability that you get Heads on $100001$th flip?,"My Answer: $1/2$ My Reasoning: Each coin flip is an independent event, outcome of which doesn't depend on preceding flips. So, even though the probability of $100000$ heads in a row is very low, the probability of next coin to be a head is $1/2$ itself. An answer by a peer: $1$ Their reasoning: Its very naïve to say that the probability will be $1/2$ without considering the Bayesian approach which considers the information of both data as well as the prior distribution of parameter, here one can easily check the posterior probability to be $1$ with simple Bayesian analysis. So, what is the correct acceptable answer? What is the correct reasoning here? If their answer is correct, please explain me how Bayesian approach is used here.","['independence', 'bayesian', 'probability']"
4529706,Question on proof that two cyclic groups of the same order are isomorphic,"Say $G = \langle x \rangle$ and $H = \langle y \rangle$ are cyclic groups of order $n$ . These are isomorphic, and the isomorphism has the form $f(x^i) = y^i$ . My question is as follows. I don't want to define this map only for $0 \leq i < n$ because then if I want to prove that $f(x^i y^j) = f(x^i) f(y^j)$ , I'd have to construct $x^{i+j}$ , and it may be the case that $i + j > n$ . It's clear to me every element of, say $G$ , can be written uniquely as $x^i$ if $0 \leq i < n$ , but if I allow $i$ to range over all of $\mathbb{Z}$ , it is not unique. Would this require me, for a fully rigorous proof, to prove that the map $f$ is well-defined? That is, would I need to show that if $x^i = x^j \in G$ , then $y^i = y^j$ ?","['group-theory', 'cyclic-groups', 'group-isomorphism']"
4529767,Prove/disprove $\mathbb{E}[\ln(\det(\mathbf{A}))] \rightarrow 0$ when bounded $\mathbf{A}\rightarrow \mathbf{I}$ in probability,"Problem: Given $\mathbf{A}_D\in [0,1]^{N\times N}$ ( $D,N\in\mathbb{Z}^+$ and $D\ge N$ ) converging to the identity matrix $\mathbf{I}_N$ in probability, i.e. , for any $\epsilon>0$ and choice of norm $\|\cdot\|$ , there is: $$
\mathbb{P}[\|\mathbf{A}_D-\mathbf{I}_N\|\geq\epsilon]\to0~~(D\rightarrow \infty).
$$ Can we say that $\mathbb{E}[\ln(\det(\mathbf{A}_D))] \rightarrow 0$ ? How to prove/disprove this? Can we directly calculate the value of $\mathbb{E}[\ln(\det(\mathbf{A}_D))]$ ? (Please see the Update part for more details about how $\mathbf{A}_D$ is generated in my task.) Background: I post a previous problem at Here , which is resolved by the answer from @JacobManaker. Now I am confused by how to show if the convergence of expectation holds. I first try to learn something from Here . However, the above problem is still too difficult for me. Intuitively, I guess that $\mathbf{A}_D\rightarrow \mathbf{I}_N$ , $\det(\mathbf{A}_D)\rightarrow 1$ and $\ln(\det(\mathbf{A}_D))\rightarrow 0$ . One key thing is that all elements of $\mathbf{A}_D$ are bounded in $[0,1]$ . But how to exactly analyse this? Update 1 (The Generation Method of $\mathbf{A}_D$ ): Here I supplement more details about how the matrix $\mathbf{A}_D$ is generated ( the previous problem ): Given $\alpha\in\mathbb{R}^+$ , $N\in \mathbb{Z}^+$ and $D\in \{N, N+1, N+2, \cdots\}$ , a random matrix $\mathbf{A}_D$ is generated by the following steps: $(1)$ Randomly select $N$ numbers from $\{1,2,\cdots,D\}$ to form a sequence $p=\{p_i\}_{i=1}^N$ . $(2)$ Then calculate $\mathbf{A}_D=[a_{ij}]_{N\times N}$ , where $a_{ij}=e^{-\alpha |p_i - p_j|}$ . Update 2 (Some of My Efforts): I am confused by how to start. I may know that the diagonal elements of $\mathbf{A}$ will be all ones, since $|p_i-p_i|=0$ . And I may know that all elements of $\mathbf{A}$ are in $[0,1]$ and $\mathbf{A}$ is symmetric. Intuitively, I guess that when $D$ increases, the absolute distances between each two $p_i$ s may become larger and larger, so $a_{ij}$ is expected to be smaller and smaller. I also write the following Python program for numerical validation: import numpy as np
import random
from scipy import spatial

alpha = 1
N = 10
I = np.eye(N)
for D in range(N, 10000):
    MSE = 0.0
    for i in range(100):
        p = np.array(random.sample(range(1, D + 1), N)).reshape(N, 1)
        A = np.exp(-alpha * spatial.distance.cdist(p, p))
        MSE += np.sum((A - I) ** 2.0)
    MSE /= (100 * N * N)
    print(MSE) I can see that when $D$ increases, the mean squared error between $\mathbf{A}$ and $\mathbf{I}_N$ converges to zero. 0.027683220252563596
0.02508590350202309
0.02317795057344325
...
0.0001934704436327538
0.00032059290537374806
0.0003270223508894337
...
5.786435956425624e-05
1.1065792791574203e-05
5.786469182583059e-05","['measure-theory', 'expected-value', 'sequences-and-series', 'convergence-divergence', 'probability-theory']"
4529788,Sum of non i.i.d. Bernoulli and Le Cam's theorem,"Consider a (deterministic) sequence $(p_i)_i$ such that $p_i \in (0, 1)$ and $\sum_{I}^n p_i = T_n \rightarrow T < +\infty$ . Then define a sequence of independent Bernoulli random variables $B_i \sim \text{Be}(p_i)$ . From le Cam's theorem, I know that $S_n = B_1 + \cdots + B_n$ is approximately Poisson distributed with parameter $T_n$ . Is there a result that shows that $S_{\infty} = \lim_{n \rightarrow \infty} S_n$ is Poisson distributed with parameter $T$ ?
I know of similar limit results when $T_n$ is constant across $n$ (see Chen, 1974 at https://www.jstor.org/stable/2959300 ), but I'm missing something with this more general setting. EDIT: I have found some places where the result I'm looking for is stated (e.g., https://d-nb.info/1197139125/34 , https://stats.stackexchange.com/questions/391461/limit-behavior-of-weighted-poisson-binomial-distribution ) but no proof is given. Looking at Le Cam's statement, it seems that the upper bound of the total variation distance between the law of $S_n$ and the Poisson distribution could be large if some of the $p_i$ 's do not tend to zero.","['probability-limit-theorems', 'poisson-distribution', 'bernoulli-distribution', 'probability-theory', 'probability']"
4529813,"Flajolet & Sedgewick: Asymptotic behavior of ""super-necklaces""","In Flajolet and Sedgewick's Analytic Combinatorics pp. 261 (Chapter IV Note IV.28), the authors state that \begin{align}
[z^n]\ln \left(\frac{1}{1- \ln \frac{1}{1-z}}\right) \sim \frac{1}{n} (1-e^{-1})^{-n} \tag{1}
\end{align} where $S(z) = \ln \left(\frac{1}{1-\ln \frac{1}{1-z}}\right)$ is the EGF of the ""super-necklaces"", which is defined as \begin{align}
\mathcal{S} = CYC \circ CYC(\mathcal{Z})
\end{align} I can't reach $(1)$ as $S(z)$ cannot be expressed as a rational function $\frac{f(z)}{g(z)}$ .  Also, the derivative of $S(z)$ still consists of the factor $\left(1-\ln \frac{1}{1-z}\right)$ , which is $0$ at the pole $1-e^{-1}$ .  (A usual method to solve this kind of problems is to find the derivative $g'(z_0)$ , where $z_0$ is the pole of $S(z)$ .  But it looks not applicable in this problem.) Is there any method to solve the problem?","['logarithms', 'analytic-combinatorics', 'asymptotics', 'complex-analysis', 'combinatorics']"
4529834,Elliptic partial differential equations with robin boundary condition and domain of fractional power of Robin Laplacian operator,"When I read the paper ""On the attractor for a semilinear wave equation with critical exponent and nonlinear boundary dissipation"" by Igor Chueshov,Matthias Eller and Irena Lasiecka,I encounter a difficulty:
Suppose that $\Omega$ is a simple connected domain with smooth boundary, the authors introduced a Robin Laplacian operator $$\Delta_{R}:L^{2}(\Omega)\to L^{2}(\Omega).$$ This is an unbounded operator with the domain $$D(\Delta_{R})=\{u\in H^{2}(\Omega):\partial_{\nu}u+u=0\ on\ \partial\Omega\}.$$ Moreover, the Robin Laplacian can be extended to a continuous operator $\Delta_{R}:H^{1}(\Omega)\to H^{1}(\Omega)'$ by $$(-\Delta_{R}u,v)_{L^{2}(\Omega)}=(\nabla u,\nabla v)_{L^{2}(\Omega)}+<u,v>_{L^{2}(\partial\Omega)}.$$ Then the authors say ""this extension is the duality map $H^{1}(\Omega)$ into $(H^{1}(\Omega))'$ "" when we equip $H^{1}(\Omega)$ the norm $$\|u\|=\sqrt{(\nabla u,\nabla v)_{L^{2}(\Omega)}+<u,v>_{L^{2}(\partial\Omega)}}.$$ So, firstly, I want to know if this norm is equivalent to the usual Sobolev norm $$\|u\|=\sqrt{(\nabla u,\nabla v)_{L^{2}(\Omega)}+(u,v)_{L^{2}(\Omega)}}$$ when $u$ satisfies the robin boundary conditon? Next, the authors said $$D((-\Delta_{R}))^{\frac{1}{2}}\sim H^{1}(\Omega),$$ why? the authors offered an reference but it is french, but I don't understand french. the reference is
""Grisvard,P. Characterisation de Quelques Esoaces d'interpolation. Archives Rational Mechanics and Analysis 1967,26,40-63"" Any comments and hints are welcome, thank you very much!!!","['functional-analysis', 'partial-differential-equations']"
4529868,Independent random variables with infinite expectation and central limit theorem,"I’m trying to construct a sequence of independent random variables $X_1, X_2, \ldots$ with $\mathbb E[|X_n|] = \infty$ for every $n$ , and for which we have $S_n^* := \frac{X_1 + \cdots + X_n}{\sqrt n}$ converges in distribution to the standard normal distribution with density $e^{-x^2/2}/\sqrt{2\pi}$ . The proof of the Central Limit Theorem with which I’m most familiar involves taking the characteristic functions $\varphi_{S_n^* }(t)$ of $S_n^* $ , showing they converge pointwise to $e^{-t^2/2}$ as $n \to \infty$ , and using Lévy’s Continuity Theorem to show that the distributions of $S_n^* $ converge weakly to the standard normal distribution $\mathcal N_{0,1}$ . The only way I know to do this involves using the Taylor expansion of $\varphi_{S_n^* }$ , which requires that $\mathbb E[|X_n|] <\infty$ for all $n$ . (More specifically, the Taylor expansion requires that $\varphi_{S_n^* }(t)$ be at least twice differentiable, which is equivalent to $\mathbb E[|S_n^* |^2]<\infty$ , which we don’t have if $\mathbb E[|X_n|] = \infty$ for all $n$ ). The other ideas I’ve tried have involved either (a) taking a distribution of the form $\mathbb P_{X_n} = \sum_{k \in \mathbb Z} p_{n,k} \delta_k^* $ , where $p_{n,k} = \mathbb P[X_n = k]$ and $\delta_k$ is the Dirac mass at $k \in \mathbb Z$ , or (b) taking $X_n$ with a continuous density $f_n$ with respect to Lebesgue measure. In both cases, we want $\mathbb E[|X_n|] = \infty$ . We know $\varphi_{S_n^* }(t) = \prod_{i = 1}^n \varphi_{X_i}(t/\sqrt{n})$ by independence. In the discrete case (a), $$
\varphi_{X_i}(t) = \sum_{k \in \mathbb Z} p_{i,k} \cos(kt),
$$ and in the continuous case (b), $$
\varphi_{X_i}(t) = \int_{\mathbb R} \cos(xt) f_i(x) dx.
$$ In both cases I’m not sure how to find a nice expression for $\prod_i\varphi_{X_i}(t/\sqrt{n})$ , and using a Taylor approximation for $\varphi_{X_i}$ is essentially a non-starter if $\mathbb E[|X_i|] = \infty$ for all $i$ . Any suggestions for how I might proceed?","['characteristic-functions', 'central-limit-theorem', 'probability-theory', 'weak-convergence']"
4530011,A question in measure theory.,"I found a question in a measure theory book which is as follows: If $\mu$ is a measure on $X$ and $A\subset X$ ,then $\lambda:X\to [0,\infty]$ defined by $\lambda(E)=\mu(E\cap A)$ is a measure on $X$ . But I doubt whether this question is correct.First of all there is no specification of the underlying $\sigma$ -algebra and $\lambda$ is given from $X\to [0,\infty]$ although $\lambda$ is a set function and its domain should be a subset of $\mathcal P(X)$ and domain of $\mu$ is not given ,so we do not know whether $E\cap A$ is $\mu$ -measurable so that the right hand side makes sense.So,can someone correct the question and tell me what should have been the right question?","['measure-theory', 'soft-question']"
4530024,Evaluate : $\lim_{n\to \infty}e^{-2n}\left(1+{2\over n}\right)^{n^2}$,My attempt: $$\lim_{n\to \infty}e^{-2n}\left(1+{2\over n}\right)^{n^2}\\ =\lim_{n\to \infty}e^{-2n}. \lim_{n\to \infty}\left(1+{2\over n}\right)^{n^2}\\=\lim_{n\to \infty}e^{-2n}.e^{2n}=1$$ Where did I go wrong? Please do help me to solve the problem. Thanks in advance.,"['limits', 'calculus']"
4530060,Is the smash-hom adjunction a homeomorphism?,"$\newcommand{\top}{\mathsf{Top}}\newcommand{\cg}{\mathsf{CG}}$ I will provide references for specific terms and constructions that I'm using, if people ask. In general category theory, we like exponential objects. Unfortunately, the category of spaces, $\top$ , does not possess all exponentials. In some instances, however, we can make things work - in $\top$ . The Tl;Dr of this question is - although things ""work"" in the pointed space category $\top_{\ast}$ too, do they work as nicely as they do in $\top$ ? Specifically, are the natural bijections also homeomorphisms? Let $X$ be a locally compact Hausdorff (LCH) space and $Y,Z$ any spaces whatsoever. Then it is indeed true that there is a bijection: $$\top(X\times Y,Z)\cong\top(Y,Z^X)$$ Where $Z^X$ is the space with point-set $\top(X,Z)$ and the classical compact-open topology. Beautifully and surprisingly, if $Y$ is also an LCH space then, topologising $\top(\cdots)$ with the compact-open topology, the above (natural) bijection is also a homeomorphism. Let $\cg$ denote the category of all compactly generated spaces, where the following definition is used: A space $X$ is compactly generated iff. "" $A\subseteq X$ is open iff. for all compact Hausdorff spaces $K$ , and $u\in\top(K,X)$ , $u^{-1}(A)$ is open"" Let $k:\top\to\cg$ be the $k$ -ification functor (it is left adjoint to the forgetful $\cg\hookrightarrow\top$ ). For spaces $X,Y$ understood in $\cg$ , their categorical product in $\cg$ will be $X\times_k Y:=k(X\times Y)$ . If we let $C_0(X,Y):=\top(X,Y)$ with the compact open topology amended as follows: A subbasis $(K,U)$ for $C_0(X,Y)$ will be replaced by triplets $(u,K,U)$ which represent: $$\{f\in C_0(X,Y):f(u(K))\subseteq U\}$$ For $U\subseteq Y$ open and $u\in\top(K,X)$ , for $K$ a compact Hausdorff space. We then define $C(X,Y)$ as $k(C_0(X,Y))$ . It turns out that the following adjunction always works (for $Z\in\cg$ ) and is a homeomorphism : $$C(X\times Y,Z)\cong C(Y,C(X,Z))$$ Which is one reason why $\cg$ is a preferred category. Now if we decide to do some algebraic topology and work with pointed spaces, some things change. We instead consider the smash-hom adjunction (except in full generality it isn't really an adjunction). To keep the post as brief as possible I'll just say (and provide reference if asked) that: Assume $a_0$ is the basepoint for a pointed space referred to as $A$ . If $X\wedge Y$ denotes the smash product of pointed spaces, then it follows from the above and a quick check that - if $X$ is a LCH space: $$\top_{\ast}(X\wedge Y,Z)\cong\top_{\ast}(Y,Z^X)$$ Is a bijection of sets. Likewise, if working in $\cg$ one can define a smash product in the usual way, except that one $k$ -ifies the product before the quotient is taken. In that context the above bijection also holds, but $\top_{\ast}$ has to be seen with $k$ -ified topology also. Moreover if $Y$ is also LCH (or everything is $\cg$ ) then there is a homeomorphism: $$\mathcal{M}\cong\top_{\ast}(Y,C(X,Z))$$ Of pointed spaces, where: $$\mathcal{M}:=\{f\in\top(X\times Y,Z):f(\{x_0\}\times Y\cup X\times\{y_0\})=\{z_0\}\}\cong\top_{\ast}(X\wedge Y,Z)$$ But is the last bijection a homeomorphism? That would ""complete the picture"" and show that everything nice about the non-pointed categories falls into nice properties of the pointed categories. I did attempt this, but it quickly felt far too hard - more of a professional matter, than one for a student new to algebraic / categorical topology. The fundamental difficulty was, given a compact-open subbasic set $(K,U)$ (or $(u,K,U)$ ) of $\top_{\ast}(X\wedge Y,Z)$ , it's not clear how to find a corresponding neighbourhood in $\mathcal{M}$ since $K\subseteq X\wedge Y=X\times Y/(X\vee Y)$ and the preimage of a compact set, out of a quotient space, isn't necessarily compact. This might be a basic stone to stumble over, but I'm not sure how to proceed. I don't even know if it's true . I would really appreciate references that confirm/deny (under certain conditions) whether or not the smash-hom adjunction is a homeomorphism. Or perhaps it's an open (uninteresting?) problem. Either way, I think I'm in a little out of my depth here, so I'm asking for help!","['general-topology', 'category-theory', 'algebraic-topology', 'reference-request']"
4530094,Is it possible to use a residue calculation to solve the complex integral $ C_{MRB} = \int_0^\infty \frac{\Im(1+it)^{\frac1{1+it}}}{\sinh(\pi t)}dt $?,"For some insight, I looked at comments by @Brevan Ellefsen, but still wonder if this integral can be expressed more elementarily than just calling it $ C_{MRB},$ which is short for the MRB constant which I first described as a sum at the end of the last millennium. There is no known closed-form expression of the MRB constant. With as slow as the series is to converge and as hard the integral is to calculate, a finite number of standard operations for the constant would be just glorious! Here is a summary of nearly a quarter century of evaluating it. @Dark Malthorp had the insight to prove my suspicion as shown:
"" Here are the details on how I used the Abel-Plana formula: Let $f(x) =  1-(1+x)^{\frac1{1+x}}$ . Then we have $$ C_{MRB} = \sum_{n=1}^\infty
 (-1)^n \left(n^{\frac1n}-1\right) =\sum_{n=0}^\infty (-1)^n \left(1
> (n+1)^{\frac1{n+1}}\right)= \sum_{n=0}^\infty (-1)^nf(n) $$ To apply
Abel-Plana, we need bounds on $f(z)$ . It doesn't quite satisfy
Wikipedia's assumptions, but actually, it's pretty nice anyway, as $\lim_{x\rightarrow \infty} f(x+yi) = 0$ for all fixed $y$ , and it is
bounded in the right half-plane. Then we use the alternating series
formulation of Abel-Plana: \begin{eqnarray}
  C_{MRB}&=&\sum_{n=0}^n(-1)^nf(n) =
  \frac12f(0)+i\int_0^\infty\frac{f(it)-f(-it)}{2\sinh(\pi t)}dt\\
  &=&\frac12 \cdot 0 + i\int_0^\infty \frac{ 1-(1+i t)^{\frac1{1+it}} 
  -1+(1-i t)^{\frac1{1-it}} }{2\sinh(\pi t)}dt\\ &=&i\int_0^\infty \frac{ -(1+i t)^{\frac1{1+it}} +(1-i t)^{\frac1{1-it}} }{2\sinh(\pi
  t)}dt \end{eqnarray} Because $(1+z)^{\frac1{1+z}}$ is holomorphic for $\Re z\ge 0$ and real-valued for real $z$ we know that $f(\overline z)  = \overline {f(z)}$ . Because $\overline z - z= -2i(\Im z)$ , this implies $$ C_{MRB} = \int_0^\infty
 \frac{\Im(1+it)^{\frac1{1+it}}}{\sinh(\pi t)}dt $$ Strictly speaking,
we cannot pull the imaginary part out of the integral, as the function $\frac{(1+it)^{\frac1{1+it}}}{\sinh(\pi t)}$ has a pole at $0$ and
thus its integral doesn't converge. The imaginary part, however, is
bounded for $t\in(0,\infty)$ . "" Is it possible to use a residue calculation to find a closed form for $$
C_{MRB} = \int_0^\infty \frac{\Im(1+it)^{\frac1{1+it}}}{\sinh(\pi t)}dt?
$$ I'm not sure if I'm on the right track, but $\frac{(1+it)^{\frac1{1+it}}}{\sinh(\pi t)}$ having a pole at $0,$ can we consider the possibility of evaluating $$ 2C_{MRB} = \int_{-\infty}^\infty
 \frac{\Im(1+it)^{\frac1{1+it}}}{\sinh(\pi t)}dt? $$ I found out Mathematica gives the following. In[70]:= Limit[Im[(1 + I t)^(1/(1 + I t)) Csch[\[Pi] t]], t -> 0]

Out[70]= 1/\[Pi]

In[181]:= Residue[(1 + I t)^((1/(1 + I t))) /Sinh[\[Pi] t], {t, 0}]

Out[181]= 1/\[Pi]

In[236]:= NIntegrate[
 Im[(1 + I t)^(1/(1 + I t))/Sinh[Pi t]], {t, 0, Infinity}]

Out[236]= 0.18786

In[237]:= 1/2 - 1./Pi

Out[237]= 0.18169 This line of reasoning gives a nice set of approximations for CMRB, but nothing exact. In[527]:= CMRB=NSum[(-1)^n (n^(1/n)-1),

{n,1,Infinity},WorkingPrecision->30,Method->""AlternatingSigns""]

Out[527]= 0.18785964246206712024857897184

Let p be the following partial approximation

In[544]:= p=((1/2-1/\[Pi])+1/(2 \[Pi]-1));

In[545]:= CMRB - 1/2 p

Out[545]= 0.00237470999999980600500193334

In[546]:= (-279/(485 \[Pi]) + p) - CMRB

Out[546]= -7.2407186775943961640*10^-10

In[547]:= (237471/50000000 + p)/2 - CMRB

Out[547]= 1.9399499806666*10^-16

In[548]:= (Pi^2 Sqrt[4187/10993830] + p)/3 - CMRB

Out[548]= 3.1221252470091*10^-16 A different line of reasoning follows, but its analysis, or how to improve its approximation (or even to determine whether that approximation is fully related to it) is beyond my power. Here is a little more detail showing some symmetry, but I don't see anything exactly equaling CMRB here. In[827]:= CMRB/2 + NIntegrate[
  E/Pi t - Im[(1 + I t)^(1/(1 + I t))/Sinh[Pi t]]/
    Re[(1 + I t)^(1/(1 + I t))/Sinh[Pi t]], {t, 0, a}, 
  WorkingPrecision -> 20] - (3078 p)/(7769 p + 3850) Out[827]= 1.714*10^-19","['integration', 'complex-analysis', 'residue-calculus']"
4530225,Deriving $\gamma \approx H(n)-\ln(n+1)+\frac{1}{2(n+1)}+\frac{1}{12(n+1)^2}$,"The Euler-Mascheroni constant can be represented geometrically by the infinite sum of the areas in blue in the following picture, which is the area between the curve $y=1/x$ and the harmonic numbers. Thus, the total area can be approximated by taking a finite sum of the first $n$ areas, such that $$\gamma \approx H(n)-\ln(n+1)$$ This approximation can be improved by noting that, as $n$ increases, these areas approach a triangle with base $\frac{1}{n}-\frac{1}{n+1}$ and height $1$ . Therefore, the sum of the remaining areas is $$A \approx\sum_{k=n+1}^{\infty}\frac{1}{2} \left(\frac{1}{n}-\frac{1}{n+1} \right)=\frac{1}{2(n+1)}$$ Thus, $$\gamma \approx H(n)-\ln(n+1)+\frac{1}{2(n+1)}$$ Is there also a relatively simple way to derive the following even better approximation? $$\gamma \approx H(n)-\ln(n+1)+\frac{1}{2(n+1)}+\frac{1}{12(n+1)^2}$$ I saw an almost identical formula in the harmonic numbers Wolfram MathWorld page but it seems that it comes from the rather complex Euler-Maclaurin formula.","['approximation', 'approximation-theory', 'harmonic-numbers', 'euler-mascheroni-constant', 'sequences-and-series']"
4530247,Why are absolutely continuous measures called that way?,"Let $X\subseteq \Bbb{R}^n$ and $\mathcal{B}(X)$ the borelian set for $X$ . Let $\mu$ and $\nu$ be two measures on $(X,\mathcal{B})$ . We say that $\mu$ is absolutely continuous with respecto to $\nu$ (denoted by $\mu\ll \nu$ ) if $\forall A\in\mathcal
{B}(X),\; \mu(A)=0\implies \nu(A)=0$ . Generally, continuity refers to some sort of smoothness of a function: small variations on domain gives an small variation on co-domain. I don't see how this definition fits this category which leads me to wonder, why are absolutely continuous measures called that way? I'm looking for maybe an historical answer (the reason why it started being called that way) or an answer that appeals to the definition itself (something on the definition which makes it reasonable to be called that way).","['absolute-continuity', 'measure-theory', 'math-history', 'soft-question']"
4530278,What is the name of this multiset-like object?,"My understanding is that a multiset (roughly, a set where we care about multiplicity) can be modeled as a function $V\to\Bbb{Card}$ with set-sized support, where $V$ is the class of all sets, $\Bbb{Card}$ is the class of all cardinalities, and set-sized support means the class of sets on which the function is nonzero is a set. (These in turn can be modeled within ZFC as sets of input-output pairs where you simply ignore all the inputs whose outputs are zero.) A similar and natural object would be a function $V\to\Bbb Z$ with set-sized support. (Again, $V$ is the class of all sets.) This would be like a multiset except that we can no longer have infinitely many copies of an element, but we may have negatively many copies of it. These form a group and an algebra, which makes them particularly natural to work with. Adjoining the constant function $1$ to our algebra makes things especially nice (though it raises some issues in representing these within ZFC), since it allows us to say things like, ""if the multiplicities of the elements of $X$ and $Y$ are all $1$ (i.e. they're pure sets), then $X\cap Y=XY$ and $X\cup Y=1-(1-X)(1-Y)$ "". The principle of inclusion-exclusion is essentially the equation \begin{align}X\cup Y\cup Z&=1-(1-X)(1-Y)(1-Z)\\&=X+Y+Z-XY-XZ-YZ+XYZ.\end{align} In any case, my question is: do these natural objects (functions $V\to\Bbb Z$ with set-sized support, interpreted as variations on the multiset concept) have a name? If so, what is it? To be clear, the sort of answer I'm expecting is a piece of terminology . I want a word that can fill the blank in the following piece of dialogue: ""The right way to think about this puzzle is to frame it in terms of _____s instead of just sets."" (If there is no name, as I'm starting to worry is the case, I'd appreciate your help in inventing one - though I'd appreciate if you didn't just open a dictionary and choose a word at random.)","['elementary-set-theory', 'multisets', 'set-theory']"
4530307,What is the probability that exactly 2 repairers are called out of 4 repairers?,"A town contains 4 people who repair televisions. If
4 sets break down, what is the probability that exactly 2
of the repairers are called? I know that the solution is $$\frac{6 * (2^4-2)}{4^4}$$ But I don't understand why we do $-2$ the the numerator. My understanding: 6 is the amount of ways you can combine 4 workers along 2 choices. (4 choose 2). $2^4$ is the amount of times a house can choose 2 repairers. So, in my mind, the solution is: $$\frac{6 * (2^4)}{4^4}$$",['probability']
4530471,"""Second binomial formula"" for p.d. matrices","Suppose $a,b$ are positive numbers, $a\neq b$ . Then, the relationship $$
\frac{1}{4}\left(\frac{1}{a}+\frac{1}{b}\right)>\frac{1}{a+b}
$$ is true because we can rewrite it as $$
a+b>4\frac{ab}{a+b}
$$ or $$
(a-b)^2>0
$$ My question: Is there a similar relationship for p.d. matrices (of suitable dimension so that addition and multiplication work), i.e., can one establish that $$
\frac{1}{4}\left(A^{-1}+B^{-1}\right)-(A+B)^{-1}>0?
$$ With similar steps to the scalar case, I write the claim as $$
A+B-4B(A+B)^{-1}A>0,
$$ from which the analogous steps to the scalar case do not directly go through anymore. Using, e.g., Inverse of the sum of matrices did not help me proceed, either. For context, the assertion would help me establish the general case here (I believe/hope that specific application has no properties that I do not mention in my question here): https://stats.stackexchange.com/questions/588398/help-partitioned-samples-efficiency-in-ols-compared-to-one-sample-regression","['matrices', 'positive-definite']"
4530510,Picard-Lindelöf and the choice of the metric,"So the theorem of Picard-Lindelöf ensures that an ordinary differential equation $\frac{dy}{dt}=f(y)$ has a unique solution if $f$ is Lipschitz-continuous. The general form of Lipschitz continuity is given if $\forall x_1,x_2 \in X : d_Y(f(x_1),f(x_2)) \le L \cdot d_X(x_1,x_2)$ with any two metric spaces $(X,d_X)$ and $(Y,d_Y)$ . But what if we have a function $f$ that is Lipschitz-continuous for some metrics $d_X,d_Y$ but not for others? I currently can neither prove nor disprove that such a function might exist, so I am assuming that PL should be applicable even in that case. I could imagine the following situations: This is no problem at all because PL only provides a sufficient condition for existence and convergence. So essentially providing any pair of metrics for which $f$ is Lipschitz is enough. The solution of the ODE actually somehow depends on the metric and I currently fail to see how. Picard-Lindelöff actually depends on some specific subset of metrics for which $f$ must be Lipschitz, but this is sometimes omitted when talking about PL. This situation is impossible, that is there is some proof that a function that is Lipschitz under one pair of metrics is Lipschitz under all pairs of metrics. Which of these four situations is the correct one? I am currently tending towards 1, because this would imply very weak conditions (just find any pair of metrics that make it work) for PL.","['lipschitz-functions', 'metric-spaces', 'ordinary-differential-equations']"
4530512,Hessian Calculation in Higher Dimensions,"In this paper https://arxiv.org/pdf/2003.00307.pdf , section $3$ , author has written the hessian in compact form which is not clear to me. He considers an over-parametrisation system $\mathcal{F}(\mathbf{w}):\mathbb{R}^m\rightarrow\mathbb{R}^n$ , where $m>n$ , with the square loss function $\mathcal{L}(F(\mathbf{w}),\mathbf{y})=\frac{1}{2}\|\mathcal{F}(\mathbf{w})-\mathbf{y}\|^2$ . Then the hessian matrix of the loss function takes the form $$H_{\mathcal{L}}(\mathbf{w})=D\mathcal{F}(\mathbf{w})^T\frac{\partial^2\mathcal{L}}{\partial\mathcal{F}^2}D\mathcal{F}(\mathbf{w}) +\sum_{i=1}^n(\mathcal{F}(\mathbf{w})-\mathbf{y})_iH_{\mathcal{F}_i}(\mathbf{w})$$ . My Approach: I tried to apply chain rule as follows: $$\frac{\partial\mathcal{L}(F(\mathbf{w}),\mathbf{y})}{\partial\mathbf{w}}=\frac{\partial\mathcal{L}(F(\mathbf{w}),\mathbf{y})}{\partial\mathcal{F}(\mathbf{w})}\times\frac{\partial\mathcal{F}(\mathbf{w})}{\partial\mathbf{w}}\\ =(\mathcal{F}(\mathbf{w})-\mathbf{y})D\mathcal{F}(\mathbf{w})$$ Again, we differentiate with $\mathbf{w}$ as follows: $$\frac{\partial^2\mathcal{L}(F(\mathbf{w}),\mathbf{y})}{\partial\mathbf{w}^2}=\frac{\partial(\mathcal{F}(\mathbf{w})-\mathbf{y})}{\partial\mathbf{w}}D\mathcal{F}(\mathbf{w})+(\mathcal{F}(\mathbf{w})-\mathbf{y})D^2\mathcal{F}(\mathbf{w})$$ please explain me in detail if possible.","['machine-learning', 'multivariable-calculus', 'hessian-matrix', 'non-convex-optimization']"
4530540,Convergence of Random Power series,"Q) Let $X_1,X_2,..$ be i.i.d. and not $\equiv 0$ . Show that the radius of convergence of the power series $\sum_{n\geq 1}X_nz^n$ is $1$ a.s. or $0$ a.s. according as $E\text{ log}^+|X_1|<\infty \text{ or } = \infty$ , where $\text{log}^+x = \text{max(log }x,0)$ . I am trying to understand the proof of the question given here here but I fail to understand the portion when $E[\log_{+}(|X_{1}|)]<\infty$ . Btw I am posting this as a new question because this is too long for a comment and it is most likely to be ignored as it is a fairly old post. It says that $\sum_{n=1}^{\infty}P(\log_{+}(|X_{n}|)\geq \epsilon n)=\infty$ for any $\epsilon>0$ and hence $|X_{n}|< e^{\epsilon n} $ for large $n$ with probability $1$ . But why is that the case? .  If the above holds , then we have by BC lemma 2 that $\log_{+}(|X_{n}|)\geq \epsilon n$ for infinitely many $n$ with probability $1$ . I think since we need $|X_{n}|<e^{\epsilon n}$ for large $n$ , i.e. for all but finitely mane $n$ , we should consider $\sum_{n=1}^{\infty}P(\log_{+}|X_{n}|\geq \epsilon n)$ and show that it converges and hence by BC lemma 1 we would have $|X_{n}|\geq e^{\epsilon n} $ occurs infinitely often with probability $0$ , Or in other words $|X_{n}|\leq e^{\epsilon n}$ occurs for all but finitely many $n$ with probability $1$ . Explicitly , I mean that if $A_{n}= \{|X_{n}|< e^{\epsilon n}\}$ then $P(\lim\inf A_{n})=1-P(\lim\sup A_{n}^{c})$ . But then again, I run into a problem of how to at all show $\sum_{n=1}^{\infty}P(\log_{+}|X_{n}|>\epsilon n)<\infty$ . I would want to use Markov's inequality, but that would require something like $n^{1+\epsilon}$ . That is I can atmost show $$\sum_{n=1}^{\infty}P(\log_{+}|X_{n}|>\epsilon n^{1+\epsilon}) \leq \sum_{n=1}^{\infty}\frac{E[\log_{+}X_{1}]}{\epsilon n^{\epsilon+1}}\leq M\sum_{n=1}^{\infty}\frac{1}{n^{\epsilon+1}}<\infty$$ for all $\epsilon>0$ . This would allow me to conclude $\large |X_{n}|<e^{\epsilon n^{1+\epsilon}}$ for all large $n$ . But this does not really help with the problem as @KaviRamaMurthy 's answer uses $|X_nz^{n}| \leq e^{n\epsilon} |z|^{n}=(e^{\epsilon}|z|)^{n}$ as a term for the geometric series to show convergence. However for any $\epsilon$ , we have $\large e^{n^{1+\epsilon}}$ would outgrow $e^{n}$ . In any case I am not using the independence with this approach so there must be something wrong . Question : Can anybody tell me how to show the required $|X_{n}|<e^{\epsilon n}$ for large $n$ with probability $1$ in this or some other way? .","['expected-value', 'borel-cantelli-lemmas', 'convergence-divergence', 'probability-theory', 'probability']"
4530554,Finding a vector field given its curl.,"This is the question I'm working on Find a vector field with twice differentiable components whose curl is $\langle x,y,z \rangle$ or prove that no such field exists. My Solution Let $\textbf{F}=\langle M,N,P \rangle$ be the vector field which we have to find.
Given $$
\nabla \times \textbf{F} = \langle x,y,z \rangle
$$ Using the determinant expansion of the curl formula, we get $$
\frac{\partial P}{\partial y}-\frac{\partial N}{\partial z} = x \\
\frac{\partial P}{\partial x}-\frac{\partial M}{\partial z} = y \\
\frac{\partial N}{\partial x}-\frac{\partial M}{\partial y} = z
$$ Differentiating first equation w.r.t $x$ , second w.r.t $y$ and the third w.r.t $z$ , we get $$
P_{xy}-N_{xz}=1 \\
P_{xy}-M_{yz}=1 \\
N_{xz}-M_{yz}=1 \\
$$ Subtracting the second from first, third from second and then adding the first and third, we get $$
M_{zy}-N_{xz}=0 \\
P_{xy}-N_{xz}=0 \\
P_{xy}-M_{yz}=0 \\
$$ Comparing the last two sets of equations yields $1=0$ , a contradiction. Hence the initial assumption that such a vector field exists is false.","['vector-fields', 'multivariable-calculus', 'solution-verification', 'vector-analysis']"
4530576,For every polynomial $P$ of degree not greater than 2012 $\underset{3\leq t \leq 4}{\max}|P(t)|\leq C \underset{0\leq t \leq 1}{\max}|P(t)|$.,"Can you help me with the following? Prove that there exists constant $C$ such that for every polynomial $P$ of degree not greater than 2012 $\underset{3\leq t \leq 4}{\max}|P(t)|\leq C \underset{0\leq t \leq 1}{\max}|P(t)|$ . I know that set of polynomials on $[a,b]$ is subset of $C[a,b]$ and that $C[a,b]$ is isometric isomorph to $C[0,1]$ for every $a,b$ . So, $$ \underset{3\leq t \leq 4}{\max}|P(t)| = ||P||_{C[3,4]} = ||P||_{C[0,1]} = \underset{0\leq t \leq 1}{\max}|P(t)|,$$ so $C=1$ . I am very suspicious of this solution, can you tell me am I wrong and what should I do instead?
Thank you!","['normed-spaces', 'continuity', 'maxima-minima', 'polynomials', 'functional-analysis']"
4530580,What is Chow's lemma really about?,"By Chow's lemma, I mean any variant of the following basic result in algebraic geometry relating complete varieties to projective varieties: Lemma. For any complete variety $X$ , there exist a projective variety $\tilde{X}$ and a surjective birational map $\tilde{X} \to X$ . For example, Stacks gives this generalisation : Lemma. For any noetherian scheme $S$ and any separated $S$ -scheme $X$ of finite type, there exist an $S$ -scheme $\tilde{X}$ and a surjective proper morphism $\pi : \tilde{X} \to X$ such that $\tilde{X}$ admits an immersion into $\mathbb{P}^n_S$ (for some $n$ ) and there is a dense open $U \subseteq X$ such that $\pi : \pi^{-1} U \to U$ is an isomorphism. I am interested in the history of this result. Question. What is the original version of Chow's lemma, when and where was it proved/published, and what was it used for? In Weil's Foundations of algebraic geometry , the very definition of completeness seems to build in the core of the proof of Chow's lemma.
The resemblance is enough to make me wonder whether Chow's lemma came first and Weil turned it upside-down to get the definition of completeness.
Unfortunately I was not able to trace the result further back than EGA II, and I did not find anything resembling it in Chow's famous paper On compact complex analytic varieties either, so I am a bit at a loss.","['algebraic-geometry', 'math-history', 'projective-varieties']"
4530605,Centralizer of $g$ has finite index $\iff$ $g$ belongs a finite normal subgroup,"Let $G$ an arbitrary group and $g \in G$ . I would like to prove that $|g|< +\infty$ and $|\{g^x: x \in G\}| =|G : C_G(g)|<+\infty$ if, and only if, exists a normal subgroup $N\trianglelefteq G$ such that $g\in N$ and $|N|<+\infty$ . The converse is cleary trivial. For the first side, I'm trying to prove that $x \in G'$ and $|G:Z(G)|$ is finite (or the set of commutators is finite) and thus by Schur's Theorem (or the Neumann's Theorem), $|G'|$ is finite, but I'm not able to do this. I appreciate any help.","['normal-subgroups', 'group-theory', 'abstract-algebra']"
4530664,Rationalize the denominator of $\frac{1}{1+\sqrt[3]{3}-\sqrt[3]{9}}$,Rationalize the denominator of $$\dfrac{1}{1+\sqrt[3]{3}-\sqrt[3]{9}}$$ Usually we are supposed to use one of the formulas $$x^3\pm y^3=(x\pm y)(x^2\mp xy+y^2)$$ I don't think they will work here. We can say $\sqrt[3]{3}=t\Rightarrow t^3=3$ and the given expression is then $$\dfrac{1}{1+t-t^2}$$ I don't see anything else. What are the available approaches?,['algebra-precalculus']
4530741,How to solve this non-linear differential equation.?,"$$y''=-(4x+e^{2y})\cdot(y')^3$$ If there is no $x$ term or no $e^{2y}$ term, then it is easy to solve. But here it includes both of them. Is there a smart substitution to solve it?",['ordinary-differential-equations']
4530751,Partition of a group by proper subgroups with trivial intersection where no two subgroups are isomorphic to each other?,"I was wondering if there exists a group $G$ and a partition of $G$ by proper subgroups $H_1,\cdots,H_n$ , meaning $G = \cup_{i=1}^{n} H_i$ and $H_i \cap H_j = \{1_G\}$ for every $i,j$ , such that $H_i \not\cong H_j$ for $i\neq j$ . I have already tried to check groups of small order and to use the sylow theorems with no success.","['group-theory', 'abstract-algebra', 'group-isomorphism']"
4530776,Recommendations about the exponential function,"I am studying differential equations and I am very surprised by how omnipresent the exponential function is. It pops up everywhere, but there isn't usually a lot of detail provided in introductory textbooks about how the solutions come up and why the exponential function has so much importance. One thing I find fascinating is how the exponential function has the characteristic that it is the same as its slope. I'd like to know more about it so that I can understand why it's so important. I guess the main problem is that this would probably require going into some real analysis. As I said, I have some knowledge of ODEs, calculus and linear algebra (and I have worked a little bit with proofs). So I would be very grateful if I could get some recommendations from you to get deeper into the topic of exponential functions, perhaps analysis more broadly if that is required.","['ordinary-differential-equations', 'real-analysis', 'calculus', 'soft-question', 'exponential-function']"
4530780,Collision time of 2 circles with friction,"I have $2$ moving objects (see picture) both with the same size and acceleration (which is negative because of friction but I'll keep it positive and change the formula instead) $$2R = 1, a = 1$$ I need to find time t when these 2 objects collide with each other knowing: initial position $(x,y)$ initial speed $v^2 = v_x^2+v_y^2$ (the orange arrow, different value for each object: $v_1, v_2$ ) and based on this, each object can travel a specific distance $d_m=\frac{v^2}{2a}$ (with green) but at some point they might collide with each other. That means: $$(2R)^2 = \Delta d_x^2+\Delta d_y^2$$ Where $(d_x,d_y)$ is the position of an object at impact $$d_x = x + d*\frac{v_x}{v}$$ and from physics: $d = t * (v - t*\frac{a}{2})$ and $d^2 = (d_x-x)^2 + (d_y-y)^2$ but this one might not be needed. results in: $d_x = x + t * (v - t*\frac{a}{2})*\frac{v_x}{v}$ After all these I end up with a very long formula which simplified looks like this $$(2R)^2 = (c_x+b_x*t-a_x*t^2)^2 + (c_y+b_y*t-a_y*t^2)^2$$ where for e.g. $\Delta d_x = (x_2 - x_1) + (v_{2x} - v_{1x})*t - \frac{a}{2} * (\frac{v_{2x}}{v_2} + \frac{v_{1x}}{v_1}) * t^2$ so: $c_x = (x_2-x_1)$ Apparently this is a $4$ th degree equation but we know that $2$ objects cannot collide more than once so there must be a much easier solution or some special case. I probably picked the wrong formulas or made some bad assumptions. I need to find the most simplified formula for t Picture with initial state","['physics', 'trigonometry', 'linear-algebra']"
4530801,Where is the flaw in my calculation of $\lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)}$,"The limit at 0 may be correctly calculated this way: $$
\lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)} = \lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)}\cdot\frac{1+\cos(x)}{1+\cos(x)}=\lim\limits_{x \to 0} \frac{x^2(1+\cos(x))}{\sin^2(x)}=\\\lim\limits_{x \to 0} \left(\frac{x^2}{\sin^2(x)}\cdot(1+\cos(x)\right)=\lim\limits_{x \to 0} \frac{x^2}{\sin^2(x)}\cdot\lim\limits_{x \to 0}(1+\cos(x)=1\cdot2=2
$$ Where is the flaw in the following? $$\lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)} = \lim\limits_{x \to 0} \frac{x}{1-\cos(x)}\cdot\lim\limits_{x \to 0}\,x$$ Thank you.","['limits', 'calculus', 'solution-verification']"
4530816,What does it mean for a distribution to not have a power law tail?,"In this paper , while generating a random data set (on origin spacing), it is written ""All we require is to define an average separation between adjacent origins and the existence
of an associated standard deviation (i.e. we assume the distribution has no power law tail)"" What does this mean, exactly?","['random', 'statistics', 'probability-distributions', 'probability']"
4530872,Is this formula new and useful? [duplicate],"This question already has answers here : Infinite Integration by Parts (2 answers) Closed 1 year ago . Recently, I have been doing some work relating to the Taylor Series on this site. I decided to integrate $f'(x)$ with integration by parts instead of using the obvious definition: $$\int f'(x)dx=xf'(x)-\int xf''(x)dx=xf'(x)-\frac{x^2}{2}f''(x)+\int\frac{x^2}{2}f'''(x)dx=\sum^N_{k=1}\frac{(-1)^{k+1}x^kf^{(k)}(x)}{k!}+\int\frac{(-1)^{N+1}x^Nf^{(N)}(x)}{N!}dx$$ Now this looks a lot like the Maclaurin series, but there is just that factor of $(-1)^{k+1}$ . I have a few questions about this formula. Can it be used alternatively to Taylor's Series? Is it novel? And finally, what are the limits for $f$ ?","['integration', 'calculus', 'functions', 'taylor-expansion', 'power-series']"
4530875,Prove that $\left|\frac{x}{1+x^2}\right| \leq \frac{1}{2}$,"Prove that $$\left|\frac{x}{1+x^2}\right| \leq \frac{1}{2}$$ for any number $x$ . My attempt: $$\left|\frac{x}{1+x^2}\right| \leq \frac{1}{2} \\ \iff \frac{x}{1+x^2} \geq -\frac{1}{2} \land \frac{x}{1+x^2} \leq \frac{1}{2}$$ $$\iff (x+1)^2 \geq 0 \land (x-1)^2 \geq 0$$ the last two inequalities are obviously true, which concludes my proof attempt. Not sure if this is a correct way to prove the inequality, also it's clearly not very elegant. Could someone please verify my solution, and maybe suggest a more elegant or efficient approach?","['proof-writing', 'inequality', 'solution-verification', 'analysis']"
4530942,Functions in Sobolev Spaces that are NOT continuous,"For $\beta\in N_{+}$ , $d\in N_{+}$ define the Sobolev space $\mathcal{W}^{\beta,\infty}([-1,1]^d)$ of functions $f:[-1,1]^d \to \mathbb{R}$ such that $f$ and its weak partial derivatives up to order $|\beta|$ are elements of $L^{\infty}$ -the space of essentially bounded functions. Sobolev Imbedding Theorems (such as Thrm 4.12 of Adams and Fournier, 2003 ) seem to indicate that functions in this Sobolev space are continuous, $\mathcal{W}^{\beta,\infty}([-1,1]^d) \subset C^0([-1,1]^d)$ . Question: I have ran across what seems to be a counterexample to this result. Let $\mathbb{Q}$ denote the rational numbers, and consider the function $\boldsymbol{1}_{\mathbb{Q}}(x)$ which is equal to one if $x\in \mathbb{Q}\cap [-1,1]$ and is equal to zero if $x\in [-1,1] \setminus \mathbb{Q}$ . Clearly this function is bounded so it is in $L^{\infty}$ and it is well known that this function has a weak derivative $v(x)=0$ . It follows that this function should be an element of $\mathcal{W}^{1,\infty}([-1,1])$ , however it is not continuous which seems to be a violation of $\mathcal{W}^{\beta,\infty}([-1,1]^d) \subset C^0([-1,1]^d)$ . Can someone explain to me what is going on here? Is this a counterexample or is one of my conclusions wrong? (Additional references on this topic would also be much appreciated)","['lp-spaces', 'sobolev-spaces', 'functional-analysis', 'weak-derivatives']"
4530949,standard error of sample mean,"I am asked to calculate the standard error of the sample mean using bootstrapping for this data set y = c(4.9, 3.3, 2.2, 2.3, 1.6, 2.4, 4.7, 1.4, 1.7, 5.1) The solutions are as follows: set.seed(12345)

nsim = 10^6 
ybar.sim = numeric(nsim)

for (i in 1:nsim){
  y.sim = sample(y, replace=T)
  ybar.sim[i] = mean(y.sim) 
}

se.boot = sd(ybar.sim); se.boot

[1] 0.4322378 whereas I thought it would be this way: mean(replicate(1000000, sd(sample(
  y, replace=T))/sqrt(length(y)))) which gives [1] 0.4264217 and using the boot lbrary gives: bootmean <- function(d, i) mean(d[i])
bs <- boot(y, bootmean, R=1000000, stype=""i"")
print(bs)


Bootstrap Statistics :
    original     bias    std. error
t1*     2.96 0.00021043   0.4318501 which is similar to the first answer. I do not understand why the first answer is correct given the formula for standard error is the standard deviation divided by the length of the vector. Why is the second answer wrong?",['statistics']
4531020,"Cauchy's Integral Theorem. Does it have to be a ""curved"" closed contour?","My question is (e). I know f is analytic by the Cauchy-Riemann condition for (a). For the results of (b), (c), and (d), I get 4/3, 8/3 + 2i, 1/3 + i5/3. So, to make the closed loop, I did (b)+(c)-(d), but it does not give me 0. Cauchy's integral formula says it should be 0 tho. What did I wrong?",['complex-analysis']
4531195,Calculating the maximum value of a quadratic polynomial on several variables with some restrictions,"Consider the following function on seven variables: $$f(x_1,\dots,x_7)=-x_1^2-2x_2^2-5x_3^2-4x_4^2-2x_5^2-71x_6^2-2x_7^2+2x_1x_2+2x_1x_3+2x_1x_4+2x_1x_6+2x_4x_5+2x_6x_7. $$ In another form, we have $$ f=3x_1^2-x_2^2-4x_3^2-2x_4^2-x_5^2-69x_6^2-x_7^2-(x_1-x_2)^2-(x_1-x_3)^2-(x_1-x_4)^2-(x_1-x_6)^2-(x_4-x_5)^2-(x_6-x_7)^2.$$ Can we find the maximum value $M$ of $f(x_1,\dots,x_n)$ with the following restrictions? The $x_i$ 's are integers with $x_1,x_3,x_6$ odd and $x_2,x_4,x_5,x_7$ even. What I know is that $(0,\dots,0)$ is the unique global maximum point of $f$ (as a function on real variables) with $f(0,\dots,0)=0$ . Also $f(1,0,1,0,0,1,0)=-73$ , so $0>M>-73$ . P.S. The result that I am hoping is $M<-7$ .","['number-theory', 'maxima-minima', 'quadratic-programming', 'optimization']"
4531215,"On representability of $\underline{Hom}_k(\mathbb A^1_k, \mathbb A^1_k)$","Let $F:k\text{-Sch} \rightarrow \text{Set}$ be the following functor: $$F(X)=\text{Hom}_{X-\text{sch}}(\mathbb A^1_X, \mathbb A^1_X).$$ I would like to show that such functor is not representable. I am able to show that is not representable by an affine scheme. Indeed, suppose $X=\text{Spec }R$ represents such functor and let $p(x)\in \text{Hom}_{X-\text{sch}}(\mathbb A^1_X, \mathbb A^1_X)=R[x]$ the universal object. If $A$ is a $k$ -algebra and $a(x)\in A[x]$ , then it should exist a morphism of $k$ -algebras $R\rightarrow A$ , such that the induced morphism $R[x]\rightarrow A[x]$ maps $p(x)$ to $a(x)$ . If the degree of $p(x)$ is smaller than the one of $a(x)$ , then it is impossible. Such argument could be extented to prove that such functor it is not representable by a quasi-compact scheme, but I'm not able to do the general case. Any hints or ideas?","['algebraic-geometry', 'schemes', 'category-theory', 'representable-functor']"
4531224,Is $\mathcal P(A \cap B) \subseteq \mathcal P(A) \cap \mathcal P(B)$ for all sets A and B?,"My question is: Is $\mathcal P(A \cap B) \subseteq \mathcal P(A) \cap \mathcal P(B)$ for all sets A and B? I'm pretty lost on this but I have tried to solve it and I need some help. Let's say $A=\{1, 2, 3\}$ and $B=\{2, 3, 4\}$ Then $\mathcal P(A \cap B) = \mathcal P(\{2, 3\})=\{ \{\emptyset\}, \{2, 3\}, \{2\}, \{3\} \}$ And $\mathcal P(A) \cap \mathcal P(B)=\mathcal P(\{ \{\emptyset\}, \{1, 2, 3\}, \{1, 2\}, \{2, 3\}, \{1, 3\}, \{1\}, \{2\}, \{3\} \})\cap\mathcal P(\{ \{\emptyset\}, \{2, 3, 4\}, \{2, 3\}, \{2, 4\}, \{3, 4\}, \{2\}, \{3\}, \{4\} \})=\{ \{\emptyset\},\{2, 3\}, \{2\}, \{3\} \}$ Does this mean that the original statement is true? If so, how can we prove that it's true? Thanks in advance.",['elementary-set-theory']
4531289,Papa Rudin abstract integration exercise $1$,"There is the exercise: Does there exist an infinite $\sigma$ -algebra which has only countably many members? Intuitively I think that the answer is no but I've really trouble to proof it. This is one proof But I have few questions about it.
I don't understand this part in this proof: "" If not, then every other measurable set of $X$ must intersect both $E$ and $E^c$ nontrivially."" I also don't understand why is $F_n$ a set of size $n$ measurable sets? for example $F_2$ has three members . by this logic $F_n$ should have $n+1$ members.
why do we need to make $n$ sufficiently large?
Any help would be appreciated.","['measurable-sets', 'measure-theory', 'analysis', 'real-analysis']"
4531337,Is there a reflexive Banach space whose ball is not the convex hull of its extreme points?,"Let $X$ be a reflexive Banach space. Then the convex hull of the extreme points of the unit ball is weakly dense by the Krein-Milman theorem and Kakutani's theorem. My question is, if there is an example of a reflexive Banach space $X$ whose unit ball does not equal the convex hull of its extreme points? Such an example $X$ must be infinite-dimensional and can't be strictly convex.","['banach-spaces', 'functional-analysis']"
4531347,Evaluate $\sin^2x = \frac{2+ \sqrt{3}}{4}$,Evaluate $\sin^2x = \frac{2+ \sqrt{3}}{4}$ Find value of $2x$ I worked it out as such: $\sin^2x = \frac{2+ \sqrt{3}}{4} \implies \frac{1- \cos 2x}{2} = \frac{2+ \sqrt{3}}{4}$ $2 - 2 \cos 2x = 2 + \sqrt{3}$ $- 2 \cos 2x = + \sqrt{3}$ $ \cos 2x = \frac{- \sqrt{3}}{2}$ And $ 2x = \frac{5\pi}{6}$ I was told that the angle $2x$ has another value: $2x = 2\pi - \frac{5\pi}{6} = \frac{7\pi}{6}$ Why is there another value and what is this value for or mean in this question context.,['trigonometry']
4531460,"Is $\{\{1\}\}\subseteq A$ true or false when $A=\{ 2, 3, 4, 5, \{1\}, 6 \}$","Can someone please explain how you are supposed to think when you encounter questions like this: $A=\{ 2, 3, 4, 5, \{1\}, 6 \}$ Is the following statement true or false: $\{1\}\subseteq A$ $\{\{1\}\}\subseteq A$ $\{\{\{1\}\}\}\subseteq A$ I would say that the first is true and the other two are false. But I don't know for certain that it is correct and I also don't know how to explain why. Please help, thanks.",['elementary-set-theory']
4531495,Probability of a painted cube with at least one white face,"A large solid cube with edge length of $4$ cm. is made from white dice with edge length of $1$ cm., and and the whole surface of the cube is painted black. These smaller cubes are shuffled randomly. A blind man (who also cannot feel the paint) reassembles the small cubes into a large one. What is the probability that there is a at least one white face outside this large cube? I find it very difficult to solve but perhaps I am missing something. This is what I got to: There are $24$ dice with 1 painted face, $24$ dice with 2 painted faces, $8$ dice with 3 painted faces, and $8$ dice with no painted faces. Calling them type 1, 2, 3 and 4 respectively, the probabilities of choosing a dice of each type are: $P(T_1) = 3/8$ , $P(T_2) = 3/8$ , $P(T_3) = 1/8$ , $P(T_4) = 1/8$ . By Total probability theorem the probability of choosing a white face in a dice is: $P(B_d)=P(B_d/T_1)P(T_1)+P(B_d/T_2)P(T_2)+P(B_d/T_3)P(T_3)+P(B_d/T_4)P(T_4)=(5/6)(3/8)+(4/6)(3/8)+(3/6)(1/8)+(6/6)(1/8)=3/4$ I think it is not easy to find the probability of having at least a white face in the cube from this. The probability of having a white face in a dice outside the cube is dependent on other faces. For instance, a dice with $3$ painted faces could make that upto $3$ faces on the cube are not white. I tried an approach similar to Paul Sinclair's but I am not convinced it is a good one. The probability for a face to be white would be: $P(B)= (P(B_d))^{16}=(3/4)^{16}=(3^{16}/4^{16})$ . Consequently, the probability that it is not completely white is $P(B^c)=1-P(B)=1-3^{16}/4^{16}=(4^{16}-3^{16})/4^{16}$ . If we consider for $i \in \{1, \ldots, 6 \}$ the events B_i = ""the face i is white"" and $B_i^c$ its complement, then: $P(B_i) = 3^{16}/4^{16}$ and $P(B_i^c)=(4^{16}-3^{16})/4^{16}$ . Thus: $P(\text{At least 1 face is white})=1-P(\text{0 white faces})=1-P(B_1^c \cap B_2^c \cap B_3^c \cap B_4^c \cap B_5^c \cap B_6^c)=1-P(B_1^c)P(B_2^c | B_1^c)P(B_3^c | B_1^c \cap B_2^c) P(B_4^c | B_1^c \cap B_2^c \cap B_3^c) P(B_5^c | B_1^c \cap B_2^c \cap B_3^c \cap B_4^c) P(B_6^c | B_1^c \cap B_2^c \cap B_3^c \cap B_4^c \cap B_5^c) \approx 1-(99/100)^6$ Consequently: $P(\text{At least 1 face is white})=1-P(\text{0 white faces}) \approx 1- (P(B_i))^6 = 1-((4^{16}-3^{16})/4^{16})^6 \approx 1-(99/100)^6 \approx (6/100)=0.06$","['combinatorics', 'geometry', 'probability']"
4531498,Computing the bias of the estimator $e^{-t/\theta}$,"I have a random variable with exponential distribution, X~ Exp( $\frac{1}{\theta}$ ) and I need to estimate $\zeta=\mathbb{P}(x>3)$ . I know that $e^{-3/\hat{\theta}}$ is the maximum likehood estimator of $\zeta$ , with $\hat{\theta}=\bar{x}=\sum_{i=1}^{n}x_{i}/n$ . Now I have to compute the MSE of $e^{-3/\hat{\theta}}$ but I don't know  how to start. I know that I need the bias and variance and but how can I compute the mean of $e^{-3/\hat{\theta}}$ ? How can I find its density function? My theacher said that we could use a symbolic calculator to compute an integral in this exercise so I think the answer will be related to compute $\int\theta\cdot g(\theta)d\theta$ with $g$ the density function of $\theta$ .","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4531501,Solution to diffusion/Smoluchowski equation using an inverse Laplace transform,"I am studying a new formula that extracts solution to the diffusion-Smoluchowski equation and is rooted on the theory of complex calculus. Namely, the formula looks like \begin{equation}
P(t) = \mathcal{N} \oint_{\text{Br}}  J \cfrac{{\rm e}^{\tfrac{Ht}{4D}} {\rm e}^{-\tfrac{2 \Delta V + 2 \int_{x_0}^{x_1} \sqrt{H + V'(x)^2} \mathop{dx}}{4D}}}{\sqrt{\sqrt{H+ V'(x_0)^2} \sqrt{H + V'(x_1)^2}}} \cfrac{\mathop{dH}}{4D}.
\end{equation} where our $H$ is the Laplace parameter, $\mathcal{N}$ is a normalisation constant, $J$ is a Jacobian, $V(x)$ is the potential, $\Delta V = V(x_1) - V(x_0)$ and we are doing a Bromwich integral. I have done it for the zero potential and linear potential case. However, I am struggling with the quadratic case $V(x) = \tfrac{1}{2} x^2$ (Ornstein-Uhlenbech solution). It is analytically tractable for quadratic potentials but I would need two cuts and this is what is confusing me. I will show what I have done for the other cases, and then any guidance to extend this to quadratic potentials would help me out. For instance, consider when the potential is zero (typical diffusion with no mechanical forcing): At first, let’s take the potential $V(x) = 0$ , i.e.,\ assume there is no external force. In this case, one expects to recover the fundamental solution to the heat equation, and indeed we do. So, \begin{equation}
P_{V = 0}(t) = \oint_{\text{Br}} \cfrac{{\rm e}^{\tfrac{Ht}{4D}} {\rm e}^{-\tfrac{2 \sqrt{H}(x_1 - x_0) }{4D}}}{4D\sqrt{H}}  \mathop{dH}.
\end{equation} The normalisation constant is unity for this case. I approach solving this integral by using complex analysis and the theory of branch points. Let \begin{equation}
\oint_{\Gamma} \cfrac{{\rm e}^{\tfrac{zt}{4D}} {\rm e}^{-\tfrac{2 \sqrt{z}(x_1 - x_0) }{4D}}}{4D\sqrt{z}}  \mathop{dz} = \oint_{\Gamma} f(z) \mathop{dz} \hspace{2mm} \text{where}  \hspace{2mm} f(z) = \cfrac{{\rm e}^{\tfrac{zt}{4D}} {\rm e}^{-\tfrac{2 \sqrt{z}(x_1 - x_0) }{4D}}}{4D\sqrt{z}}. 
\end{equation} This has a branch point at $z = 0$ qnd I choose the branch cut to span to negative infinity (principle branch). We use the keyhole contour. It has 6 components. $\Gamma_1$ spans the imaginary axis and is shifted to the right of the point at zero. $\Gamma_2, \Gamma_4$ are semi-circles with radius $R$ , $\Gamma_4$ is a circle of radius $\epsilon$ about the branch point and $\Gamma_3, \Gamma_5$ are the lines just above/below the cut from $-\infty$ to 0 and $0$ to $-\infty$ respectively. It is straightforward to show that the arced segments contribute nothing to the overall integral in the limiting process $R \rightarrow \infty, \epsilon \rightarrow 0$ . The semicircular arcs vanish by Jordan’s lemma and then the contribution from $\Gamma_4$ vanishes by using the parameterisation $z = \epsilon \exp(i \theta)$ . By Cauchy's residue theorem, we know that \begin{equation}
\left[\oint_{\Gamma_1} + \oint_{\Gamma_3} + \oint_{\Gamma_5} \right] f(z) \mathop{dz} = 0
\end{equation} since the branch point is excluded from the interior of the contour.
The integral we want to compute is $\cfrac{1}{2 \pi i} \oint_{\Gamma_1} f(z) \mathop{dz}$ . On $\Gamma_3$ , let’s take $z = p {\rm e}^{i \pi} = - p$ . Then, $\mathop{dz} = - \mathop{dp}$ and $\sqrt{z} = i \sqrt{p}$ . Since the integral is from $- \infty$ to 0 on this segment, we have \begin{equation}
\oint_{\Gamma_3} f = - \int_{\infty}^{0} \cfrac{{\rm e}^{-\tfrac{pt}{4D}} {\rm e}^{-\tfrac{i\sqrt{p}(x_1 - x_0) }{2D}}}{4Di\sqrt{p}}  \mathop{dp} = \int_{0}^{\infty} \cfrac{{\rm e}^{-\tfrac{pt}{4D}} {\rm e}^{-\tfrac{i\sqrt{p}(x_1 - x_0) }{2D}}}{4Di\sqrt{p}}  \mathop{dp}.
\end{equation} Similarly, on $\Gamma_5$ lets follow a similar parameterisation and take $z = p {\rm e}^{- i \pi}$ . Note, the minus sign in the exponent represents moving below the branch cut along the line. Now, $\sqrt{z} = - i \sqrt{p}$ . Since the integral is from 0 to $- \infty$ on this segment, we have \begin{equation}
\oint_{\Gamma_5} f = - \int_{0}^{\infty} \cfrac{{\rm e}^{-\tfrac{pt}{4D}} {\rm e}^{\tfrac{i\sqrt{p}(x_1 - x_0) }{2D}}}{-4Di\sqrt{p}}  \mathop{dp} = \int_{0}^{\infty} \cfrac{{\rm e}^{-\tfrac{pt}{4D}} \cdot {\rm e}^{\tfrac{i\sqrt{p}(x_1 - x_0) }{2D}}}{4Di\sqrt{p}}  \mathop{dp}.
\end{equation} Hence, from Cauchy’s residue formula, we have that our inverse Laplace transform is given by \begin{equation}
\cfrac{1}{2 \pi i} \oint_{\Gamma_1} f(z) \mathop{dz} = - \cfrac{1}{2 \pi i } \left[\oint_{\Gamma_3} + \oint_{\Gamma_5} \right] f(z) \mathop{dz}.
\end{equation} From here, we can solve the integral explicitly. We shall use the substitution $p = u^2$ to transform our exponent into a quadratic, making it a Gaussian integral, enabling the use of the result \begin{equation}
\int_{-\infty}^{\infty} \exp( - \alpha x^2) \mathop{dx} = \sqrt{\cfrac{\pi}{\alpha}}.
\end{equation} So, \begin{align}
\cfrac{1}{2 \pi i} \oint_{\gamma_1} f(z) \mathop{dz} & =   \cfrac{1}{ \pi }  \int_{0}^{\infty} \cfrac{{\rm e}^{- \tfrac{pt}{4D}}}{4D \sqrt{p}} \cos \left(\tfrac{\sqrt{p}}{2D} (x_1 - x_0) \right) \mathop{dp} \\
& = \cfrac{1}{\pi} \Re \left\{ \int_{0}^{\infty} \cfrac{{\rm e}^{- \tfrac{pt}{4D}}}{4D \sqrt{p}} \cdot {\rm e}^{\tfrac{i \sqrt{p}(x_1 - x_0)}{2D}}  \mathop{dp}\right\} \\
& = \cfrac{1}{4 \pi D} \Re \left\{ \int_{-\infty}^{\infty} {\rm e}^{- \tfrac{u^2 t}{4D} + \tfrac{iu}{2D}(x_1-x_0)} du \right\} \\
& = \cfrac{1}{4 \pi D} \Re \left\{\int_{-\infty}^{\infty} {\rm e}^{-\tfrac{t}{4D}[(u - \tfrac{i}{t}(x_1 - x_0))^2 + \tfrac{1}{t^2}(x_1-x_0)^2]} \mathop{du} \right\} \\
& = \cfrac{1}{\sqrt{4 \pi D t}} {\rm e}^{- \tfrac{(x_1-x_0)^2}{4Dt}}, 
\end{align} which is exactly what we hoped. The case $V(x) = \alpha x$ follows very similarly. The case $V(x) = \tfrac{1}{2}x^2$ causes me some trouble. For simplicity, I think it is best if we take $x_0 = 0$ for now. In such a case, one has an $\operatorname{arcsinh}(x_1/\sqrt(z))$ term appearing from the integral, and the Jacobain (which gives a $\tfrac{1}{2} \operatorname{arsinh}$ factor) as well, and I do not know how to handle it and it becomes quite a formidable looking integral. In this case, the integral becomes \begin{equation}
P(t) = \mathcal{N} \oint_{\text{Br}}  \cfrac{{\rm e}^{\tfrac{Ht}{4D}} {\rm e}^{-\tfrac{x_1^2 +  (H-2D) \operatorname{arcsinh}(x_1/\sqrt{H})+ x_1 \sqrt{H+ x_1^2}}{4D}}}{\sqrt{\sqrt{H} \sqrt{H + x_1^2}}} \cfrac{\mathop{dH}}{4D}.
\end{equation} As a little comment, I am quite sure this is the right formula and what is fascinating is how the branch points of the nasty hyperbolic function align perfectly with the other exponent term and the denominators; this must be why it will offer some analytic tractability for this quadratic case.","['integration', 'inverse-laplace', 'cauchy-integral-formula', 'complex-analysis', 'contour-integration']"
4531507,Find the area of $ABCD$,"Find the area of $ABCD$ if $PSC$ is a semicircle. I have used the tangent properties of circles. I also assumed many variables to get a relationship between product of sides of the rectangle $ABCD$ but eventually and unfortunately, I proved $$1=1$$ Any help is greatly appreciated.","['euclidean-geometry', 'trigonometry', 'geometry']"
4531530,Conjecture $\lim\limits_{x\to 0}\prod\limits_{n=1}^{\infty}\left(\arctan\left(x^{n}\right)+1\right)^{\frac{1}{x}}=e$,"I cannot figure why but let me propose  it: $$\lim_{x \to 0}\prod_{n=1}^{\infty}\left(\arctan\left(x^{n}\right)+1\right)^{\frac{1}{x}}\overset?=e$$ We have a more obvious statement: $$\lim_{x\to 0}\prod_{n=1}^{\infty}\left(\tanh\left(x^{n}\right)+1\right)^{\frac{1}{x}}=e$$ If we replace a $\arctan$ by a similar function, for example $\operatorname{erf}$ , we don't have the same result. If the conjecture is true I think we may need complex numbers to prove it but it's my supposition. I have checked the conjecture up to $n=100$ with Desmos. How should I prove or disprove it?","['examples-counterexamples', 'constants', 'products', 'limits', 'trigonometry']"
4531601,growth of coefficients of half-integral weight modular forms,What is the order of growth of coefficients of half-integral weight modular forms on congruence subgroup with character ? There is the usual Hecke trick to compute bounds on the coefficients of integral weights on $SL_2(\mathbb{Z})$ but I don't know how to adapt the argument. I've looked online at many articles but didn't find anything. My guess is that it should be $O(n^{k/2})$ where $k$ is the half-integral weight.,"['complex-analysis', 'number-theory', 'modular-forms']"
4531627,How to prove that the sum of reciprocals of one plus perfect powers is $\frac{\pi^{2}}{3}-\frac{5}{2}$,"Let $S$ be Set of perfect powers without duplicates $1,4,8,9,\dots$ ( http://oeis.org/A001597 ) How to prove the following? $$\sum_{s\in S}\frac{1}{s+1}=\frac{\pi^{2}}{3}-\frac{5}{2}$$ (starting with $s=4$ ) I found this formula in the book ""Mathematical Constants"" by Steven R. Finch on page 113.","['number-theory', 'analysis']"
4531629,How do you calculate $\mathcal P(A) \times \mathcal P(B)$?,"How do you calculate $\mathcal P(A) \times \mathcal P(B)$ ? Let's say we have this example: $A=\{2\}$ $B=\{7\}$ Then this should be the case: $\mathcal P(A) \times \mathcal P(B)$ $\mathcal P(A)=\{ \emptyset , \{2\} \}$ $\mathcal P(B)=\{ \emptyset , \{7\} \}$ How do you continue from here?",['elementary-set-theory']
4531651,Showing that $\mathbb{RP}^n$ is a manifold,"I would like to show that the real projective space $\mathbb{RP}^n$ is a manifold. This seems like a standard problem, and the explanation can be found in Lee's book on smooth manifolds as well as numerous proofs being available on this site. For the most part I understand the proof, but I am missing some intuition. The idea, as I understand it, is to construct coordinate domains $U_i$ whose union cover our set, where $U_i = \pi(\tilde{U}_i)$ , $\pi: \mathbb{R}^{n+1}\backslash\{0\} \rightarrow \mathbb{RP}^n$ is the quotient map, and each $\tilde{U_i} \subset \mathbb{R}^{n+1}$ is defined to be the set of all points such that $x_i \neq 0$ . The union of all such $U_i$ clearly cover $\mathbb{RP}^n$ , so all that is left is to construct the coordinate maps $\varphi_i$ such that $\varphi_i(U_i) \subset \mathbb{R}^n$ and $\varphi_i$ is a homeomorphism. Lee defines this map as $$\varphi_i[x^1,\ldots, x^{n+1}] = \Big(\frac{x^1}{x^i}, \ldots, \frac{x^{i-1}}{x^i}, \frac{x^{i+1}}{x^i}, \ldots, \frac{x^{n+1}}{x^i}\Big)$$ and so the corresponding inverse is given by $$\varphi_i^{-1}(u_1,u_2,.....,u_n)=[u_1,u_2,.....,u_{i-1},1,u_{i+1},.....,u_n].$$ What is the motivation behind $\varphi_i$ (and its inverse), and why is the coordinate $x^i$ omitted in its image? From what I have gathered it has to do with some kind of slope of a hyperplane, but I do not see this or why we are scaling by $x^i$ . Also, why is the inverse necessarily continuous? My guess is that it has to do with some property of quotient maps, but I am not sure. As a side question, is the argument of $\varphi_i$ a single equivalence class and each $x^i$ is a component of $[x]$ ? If so, how come there are $n+1$ entries and not $n$ ?","['manifolds', 'general-topology', 'smooth-manifolds', 'differential-geometry']"
4531652,"Proving that $x^n=a$, for $n>0$ an odd natural number, has exactly one real root","In my school book, I read this theorem Let $n>0$ is an odd natural number (or an odd positive integer), then the equation $$x^n=a$$ has exactly one real root. But, the book doesn't provide a proof, only tells $x=\sqrt [n]a$ .
How can I prove this theorem? I tried to prove some special cases $$x^3=8$$ $$(x-2)(x^2+2x+4)=0$$ $$x=2 \vee x^2+2x+4=0$$ But the Discriminant of $x^2+2x+4=0$ equals to $2^2-4×4=-12<0$ . So $x=2$ is an only root. But for $x^5=32$ , I got $x=2$ and $x^4+2x^3+4x^2+8x+16=0$ . I don't know how I can proceed.","['algebra-precalculus', 'roots']"
4531694,non negativity of Fourier coefficients of modular forms,"I would like to know if there exists some criteria to test whether a given modular form, of level N with integral or half-integral weight, has non negative coefficients. I am also interested in results saying when certain coefficients are necessarily non negative. Any research paper or reference is welcome !","['complex-analysis', 'number-theory', 'modular-forms']"
4531735,Proving Chern forms are closed via Bianchi identity,"Let $M$ be a smooth real manifold and $E \rightarrow M$ an Hermitean vector bundle over it. Define Chern classes as $$c(M)=\sum_{i=0}^m c_i(E)t^i=\det \left( \textrm{Id} +\frac{i}{2\pi}F \right) = \prod_i (1 + \lambda_i t),$$ Letting $F \triangleq D^2$ denote the curvature two-form associated with the connection $D$ on $M$ - a ( $\mathfrak{u}(E)$ -valued two-form?), and $\lambda_i$ denote the eigenvalues of the normalization $i/2\pi F$ . I have seen it asserted that one may use the Bianchi identity to show that the Chern forms are closed and hence represent cohomology classes, i.e. $$ DF = 0 \implies dc_i(E) = 0 $$ How does one see this from the definition of the Chern classes as above?","['complex-geometry', 'curvature', 'vector-bundles', 'characteristic-classes', 'differential-geometry']"
4531779,"Given the real numbers $x_1,y_1,z_1,x_2,y_2,z_2,x_3,y_3$ and $z_3$, prove this determinant equality: [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question $$(x_1+y_1+z_1)\begin{vmatrix}x_1 & y_1 & z_1 \newline x_2 & y_2 &z_2 \newline x_3 & y_3 & z_3\end{vmatrix} =  \begin{vmatrix} 1 & 1 &1 \newline \begin{vmatrix}
y_1 & z_1 \newline y_2 & z_2
\end{vmatrix} & -\begin{vmatrix}
x_1 & z_1 \newline x_2 & z_2
\end{vmatrix} & \begin{vmatrix}
x_1 & y_1 \newline x_2 & y_2
\end{vmatrix} \newline \begin{vmatrix}
y_1 & z_1 \newline y_3 & z_3
\end{vmatrix} & -\begin{vmatrix}
x_1 & z_1 \newline x_3 & z_3
\end{vmatrix} & \begin{vmatrix}
x_1 & y_1 \newline x_3 & y_3
\end{vmatrix} \end{vmatrix}$$ I found this equality playing with barycentric coordinates. It took me very long to figure that it is the case and I wonder if someone can come up with a simple proof for it. I did expand it all with wolframalpha and concluded it was true, I just can't figure the logic behind it","['matrices', 'algebra-precalculus', 'determinant', 'linear-algebra']"
4531799,Bug jumping problem,"In the real axis, there is a bug standing at coordinate $x=1$ . At each step, from the position $x=a$ , the bug can jump to either $x=a+2$ or $x=\frac{a}{2}$ . How many positions in total are there (including the initial position) that the bug can jump to by at most $n$ steps? Isn't the answer trivially $2^n+1$ ? Am I missing something here? EDIT : For examples, I tested the following: $n=1$ : Trivially, the answer is $2+1=3$ . $n=2$ : The possible sequences are $1,3,5$ and $1,3,3/2$ and $1,1/2,7/2$ and $1,1/2,1/4$ , which gives us a total of $7$ positions, but this is not equal to $2^2+1$ . So, my hypothesis is incorrect. Does anyone have any other idea in mind to solve this problem? We want to avoid collisions, where two possibilities land on the same position at some point, and we end up double-counting it.","['recreational-mathematics', 'combinatorics']"
4531805,Proving using Mathematical Induction,"I'm Stuck on the last step, this is proving using mathematical induction, a lecture from my Elementary number theory class. The question goes to, Prove that $\sum_{k=1}^n \frac{1}{k^2}=\frac{1}{1^2}+\frac{1}{2^2}+\cdots+\frac{1}{n^2}\le 2-\frac{1}{n}$ whenever $n$ is a positive integer. This is my Attempt, Step 1: Base Case ( $n=1$ ) $$\sum_{k=1}^n \frac{1}{k^2}=\frac{1}{1^2}\le2-\frac{1}{1}$$ $$=1\le1$$ , Therefore Base case is true. Step 2: Induction Hypothesis Suppose $\sum_{k=1}^n \frac{1}{k^2}=\frac{1}{n^2}\le2-\frac{1}{n}$ is true for $n=m$ $\implies$ $\sum_{k=1}^m \frac{1}{k^2}=\frac{1}{m^2}\le2-\frac{1}{m}$ $\forall m \in \mathbb{N}$ Step 3: $n = m+1$ $\sum_{k=1}^{m+1} \frac{1}{k^2}=\sum_{k=1}^m \frac{1}{k^2}+\frac{1}{(m+1)^2}\le2-\frac{1}{m}+\frac{1}{(m+1)^2}$ I'm Stuck on this step","['induction', 'discrete-mathematics']"
4531844,Prove $A^c \cap (A \cup B) = (A \cup B^c)^c $,Prove $A^c \cap (A \cup B)  = (A \cup B^c)^c $ Start with the left side and try to make it match with the right side: $A^c \cap (A \cup B)$ $ (A^c \cap A) \cup (A^c \cap B) $ $A^c \cap B$ And I don't know how to keep going after this. How do I match it up with : $ (A \cup B^c)^c $ I tried using De Morgan's laws but I can't make the left side the same as the right side.,['elementary-set-theory']
4531859,Is ideal sheaf left exact,"Let $P$ be a zero dimensional scheme on a projective surface $X$ . One can consider its ideal sheaf $I_P$ in $X$ to be the kernel of the morphism between $\mathcal O_X$ and pushforward of $\mathcal O_P$ . Is the following true : the injection $\mathcal O_X \to \mathcal O_X(1)$ also stays an injection when we tensor it with $I_P$ , i.e. is $I_P \to I_P(1)$ also injective? Any argument or counterexample is welcome",['algebraic-geometry']
4531900,Upper bound on hitting probability for a non-simple one-dimensional random walk,"Consider the random walk $S_n=\sum_{k=1}^n X_k$ where $(X_n)$ is an i.i.d. sequence of variables with distribution $P(X_n=1)=P(X_n=-2)=\frac{1}{2}$ . I am trying to evaluate the probability $P(E)$ where $E$ is the event $E=\lbrace \exists n, S_n \geq 1 \rbrace$ . If we define the stopping time $\tau={\sf min}(n\ | \ S_n \geq 1)$ (or $\infty$ if the sums never reach $1$ ), it is easy to see that $P(\tau=k)$ is nonzero iff $k$ is congruent to $1$ modulo $3$ . I have computed the first values of $P(\tau=k)$ and $s_k=\sum_{j=1}^k P(\tau=j)$ : $$
\begin{array}{|c|c|c|c|}
\hline
k & 1 & 4 & 7 & 10 & 13 & 16 \\
\hline
P(\tau=k) & \frac{1}{2} & \frac{1}{16} & \frac{3}{2^7} & \frac{3}{2^8} &  \frac{55}{2^{13}} & \frac{273}{2^{16}} \\
\hline
s_k & 0.5 & 0.56 & 0.59 & 0.6 & 0.6 & 0.61 \\
\hline
\end{array}
$$ Question. Is it true that $P(E)\leq 0.7$ ? My thoughts : one can introduce $f(k)=P(\lbrace \exists n, S_n \geq k)$ . Then for $k\geq 1$ one has the linear recurrence $f(k)=\frac{f(k-1)+f(k+2)}{2}$ , with $f(0)=1$ and $f(1)=P(E)$ . The characteristic polynomial associated to this recurrence is $X^3-2X+1=(X-1)(X^2+X-1)$ , and its root are $1$ and $\frac{-1\pm\sqrt{5}}{2}$ . So there are constants $c_1,c_2,c_3$ such that $f(k)=c_1+c_2\big(\frac{-1+\sqrt{5}}{2}\big)^k+c_3\big(\frac{-1-\sqrt{5}}{2}\big)^k$ . Because of $0\leq f(k)\leq 1$ and $\big|\frac{1+\sqrt{5}}{2}\big| \gt 1$ it follows that $c_3=0$ .  I am stuck after this however because the only relation I can see is $f(0)=1$ , which leaves one degree of freedom.","['stochastic-processes', 'random-walk', 'probability']"
4531920,Proving a random walk with a drift is recurrent,"Given the following set of independent random variables $\mathbb{P}(X_n=1)=\frac{1+n^{-\beta}}{2}, \mathbb{P}(X_n=-1)=\frac{1-n^{-\beta}}{2}$ and the sum $S_n=\sum^n_{i=1}X_n$ I managed to prove that if $\beta<0.5$ then $S_n$ is transient by using Hoeffding's bound and that if $\beta > 1$ $S_n$ is recurrent by treating the random variables as a simple random walk with a drift - which I proved is finite almost surely. I am now trying to prove the case of $0.5\leq\beta\leq1$ . I am pretty sure intuitively that it is recurrent however I am not successful in proving the case. Any help will be appreciated.","['probability-theory', 'random-walk', 'random-variables']"
4531936,Motivation for differential Manifolds,"I've an epistemological question about Manifolds. According to the text I'm reading the motivation for introducing differential manifolds is the following: ""In general terms, smooth manifolds are objects that locally look like $\mathbb{R}^n$ , and on which it is possible to define operations that extend those ones used in classic Calculus. The most familiar examples are spheres, ellipsoid, etc"". So to extend Calculus to curved surfaces. Considering for example the sphere as the subset of points in $\mathbb{R}^3$ for which $x^2 + y^2 + z^2 = r_0$ holds, and which can be parametrized by the vector function $$(\theta,\phi) ⟼ r_0 \cos \theta \cos \phi \hat{i} + r_0 \sin \theta \cos \phi \hat{j} + r_0 \sin φ \hat{k} \quad(1)$$ in the parameter domain, the rectangle $D = [0, \pi] \times [0, 2\pi].$ If the motivation for Manifold is doing calculus as in $\mathbb{R}^n$ , I don't see why I cannot consider the sphere as an object in $\mathbb{R}^3$ , for example mapping each point on the surface (with the function (1) defined above) of the sphere with a 3D vector in $\mathbb{R}^3$ and do calculus in $\mathbb{R}^3$ ; Instead of considering the manifold $M$ a set of points with a continuous 1-1 map from each open neighborhood onto an open set of $\mathbb{R}^n$ , which associates with each point $P$ of $M$ an $n$ -tupel $(x_1(P) , ..., x_n(P))$ . It seems there exists already a natural mapping (1) which maps points on the surface of the sphere to 3-tuples, i.e. vectors in $\mathbb{R}^3$ .","['soft-question', 'surfaces', 'differential-geometry']"
4531954,Where is symmetry condition used in the proof of Banach fixed point theorem?,"I was learning the proof of Banach fixed point theorem from here . I am wondering where is the symmetry condition $d(x,y)=d(y,x)$ used in the proof?","['proof-explanation', 'metric-spaces', 'analysis', 'real-analysis', 'general-topology']"
4531964,Why does $\mu$ and $\sigma$ minimise the product of normal distributions?,"First of all, there might be a better way to phrase the question, this is in a course about real analysis so I'm not sure if there's a better name for this function that is clearly related to probability somehow. Let $x_1,\ldots,x_N$ be fixed real numbers. I am asked to find values of $\mu$ and $\sigma$ which minimize the function: $$\begin{pmatrix} \mu \\ \sigma \end{pmatrix} \mapsto \prod_{i=1}^N \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2\sigma^2}(x_i - \mu)^2\right).$$ Obviously the expression in the the product is the probability density function of a normal distribution with mean $\mu$ and standard deviation $\sigma$ , so I expect the punchline to be: $$\mu = \frac{\sum x_i}{N}, \qquad \sigma = \sqrt{\frac{\sum (x_i - \mu)^2}{N}}.$$ Simplifying the product I get: $$ \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^N \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^N(x_i - \mu)^2\right),$$ from which I can accept that choosing $\mu$ to be the mean is the ""best"" way to minimise this expression (although rigorously proving this would be beyond me). If I now assume that $\mu$ is the mean and $\sigma$ is the standard deviation, my expression becomes: $$ \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^N \exp\left(-\frac{1}{2}\frac{N\sum (x_i - \mu)^2}{\sum (x_i - \mu)^2}\right),$$ or: $$ \left(\frac{1}{\sqrt{\frac{2\pi}{N}\sum (x_i - \mu)^2}}\right)^N\exp\left(-\frac{N}{2}\right).$$ I have no idea how to prove that this is minimal. It certainly makes the exponent a lot simpler to look at, but beyond that I am unsure how to answer.","['probability-distributions', 'real-analysis', 'maxima-minima', 'functions', 'exponential-function']"
