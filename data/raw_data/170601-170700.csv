question_id,title,body,tags
3015160,Constant Rank Theorem for Manifolds with Boundary,"I'm trying to answer problem 4-3 from Lee's Introduction to Smooth Manifolds , 2nd edition.  The problem says: Formulate and prove a version of the rank theorem for a map of constant rank whose domain is a smooth manifold with boundary. Lee himself gave a hint in another question . Here is what I have so far: I'm assuming I have a smooth map of constant rank $F: \mathbb{H}^m\rightarrow\mathbb{R}^n$ . Let's say $\mathrm{rank}(F)=k$ . Then I extend $F$ to a smooth map $\tilde{F}:\mathbb{R}^m\rightarrow\mathbb{R}^n$ .  I can shrink the domain to a small enough neighborhood of $\mathbf{0}$ , $U$ , such that there is a projection $\pi:\mathbb{R}^n\rightarrow\mathbb{R}^k$ with $\pi\tilde{F}_{|U}$ a submersion. The actual constant-rank theorem then says we have charts $(A,\alpha)$ and $(B,\beta)$ with $$ \beta\circ\pi\tilde{F}\circ\alpha^{-1}:\alpha(A)\rightarrow\beta(B)$$ If I write the coordinates of $\mathbb{H}^m$ as $(x,y)$ and the coordinates of of $\alpha(A)$ as $(a,t)$ , then this map looks like $$ \beta\circ\pi\tilde{F}\circ\alpha^{-1}(a,t) = a$$ I can switch the order so the projection is the last map applied: $$ \pi\circ(\beta\times\mathrm{id})\circ\tilde{F}\circ\alpha^{-1}\equiv\beta\circ\pi\tilde{F}\circ\alpha^{-1}$$ Which finally means $$(\beta\times\mathrm{id})\circ\tilde{F}\circ\alpha^{-1}(a,t)=(a,S(a,t))$$ Where $S:\alpha(A)\rightarrow\mathbb{R}^{(n-k)}$ . This is pretty similar to how the constant-rank theorem is proved, but now I'm stuck.  The map above, restricted to $\alpha(A\cap\mathbb{H}^m)$ , has rank $k$ . But that does not mean $S$ is independent of $y$ . That's because $\alpha$ is not necessarily a boundary chart on $A\cap\mathbb{H}^m$ . I also don't see any way to make $\alpha$ a boundary chart (something similar is proved in Lee's book, but crucially uses that $F$ is an immersion). Can someone give me a hint as to how to finish this? Ideally I'd get to a place where (with possibly different charts) $$ \tilde{\beta}\circ F\circ\tilde{\alpha}^{-1}(a,t) = (a,0)$$ and $\tilde{\alpha}$ is a boundary chart. Bonus question: where do I use Lee's assumption (from the hint) that $\ker dF_p\not\subseteq T_p\partial\mathbb{H}^m$ ?","['manifolds-with-boundary', 'smooth-functions', 'smooth-manifolds', 'differential-geometry']"
3015176,Locality and Hilbert Curve,"I have a hilbert curve index based on this algorithm. I take two to four values (latitude, longitude, time in unix format and an id code) and create a 1-d hilbert curve. I'm looking for a way to use this data to create a bounding box query (i.e. ""find all ids within this rectangle). I'm looking for a way to do so without decoding the 1d Hilbert code back into its constituent parts. My question is: if I created a 2d hilbert curve range (i.e. I converted the range of the box into a hilbert curve so x1y1-> hilbert value1 and x2y2-> hilbertvalue2) would all values of corresponding 2d hilbert values fall within their range? E.g. If I converted (1,2) and (20,30) into Hilbert values and then searched for all values between hilbertvalue1 and hilbertvalue2, would all the values I get decode to fall within (1,2) and (20, 30), or would I have to perform additional transformations? When I set all my values to 2^a* X and 2^a * y (a being a positive integer multiplier) it seems to be true. However, is there a way to use a range search on the 4d hilbert curve? I.e., if I have a Hilbert Curve made of 4 values and I have a bounding box query, can I see which hilbert values fall within that bounding box without decoding the entire Hilbert curve and checking? Thanks.",['analysis']
3015185,"Is the ""immersed proper"" hypothesis necessary in Half-space Theorem?","I'm using the following version of Half-space Theorem: $\textbf{Theorem}$ (Half-space) A connected, immersed proper, nonplanar minimal surface $M$ in $\mathbb{R}^3$ is not contained in a halfspace. I supposedly proved this theorem using the closed and complete hypothesis instead of immersed proper. Is the ""immersed proper"" hypothesis necessary? The original statement is found in the paper ""The strong halfspace theorem for minimal surfaces"" by D. Hoffman and W. H. Meeks, III, 1990.
In the statement they use the hypothesis of immersed proper and allow the surface to be ""possibly branched"". Does the fact that the surface is ""possibly branched"" need the proper immersed hypothesis? Can someone help me understand this better? Follow the link in the paper below http://www.math.jhu.edu/~js/Math748/hoffman-meeks.halfspace.pdf","['surfaces', 'riemannian-geometry', 'minimal-surfaces', 'analysis', 'differential-geometry']"
3015207,Proof of Feynman-Kac theorem,"I am reading a proof of Feynman-Kac theorem as done here , where I do not follow one step. Specifically, after the author derived: $$dY_s = u_x(t-s, W_s)e^{-R(s)}dW_s$$ they seem to have concluded $$Y_t =u(0,W_t)e^{-R(t)}$$ in the next line. But the partial derivative in $x$ magically seemed to have disappeared. Can anyone point out if this is a mistake or this is how it is supposed to be?","['stochastic-processes', 'brownian-motion', 'probability-theory']"
3015332,Given that derivative of a function is bounded. Prove surjectivity,"Given a differentiable function $f:\mathbf{R} \to \mathbf{R},$ such that $|f'(x)| < c < 1$ . Consider a function $g:\mathbf{R}^2 \to \mathbf{R}^2$ , such that $g(x,y) = (x+f(y),y+f(x))$ . Prove that g is surjective. My attempt: determinant of derivative of g is always non-zero. Consider a point $(a,b)$ which we want to show will have a preimage, by implicit function theorem, there's a neighbourhood around the point and also a neighbourhood around the point $(a+f(b),b+f(a))$ , such that function is a bijection. Now I wanted to make sure that the distance between $(a,b)$ and $(a+f(b),b+f(a))$ is small so that $(a,b)$ is attainable.","['multivariable-calculus', 'diffeomorphism', 'inverse-function-theorem']"
3015411,Choosing $\delta$ for a given $\varepsilon$ when computing $\lim_{x\to 2}x^2=4$,"How does the author come to the conclusion that $\delta$ has to be the $ \min\left\{1,\frac{\epsilon}5\right\}$ . Why that $1$ ? Can it be any number and we get accordingly the value of $\epsilon$ ? In general, finding $\delta$ for any $\epsilon$ is purely by some guesswork? I can understand why these values work but the ‘why’ question is troubling me. I’m not very comfortable and unable to convince myself perfectly.","['limits', 'epsilon-delta', 'real-analysis']"
3015453,"If $\vec{\nabla} \cdot \vec{V} \neq 0$ at only one point, will this prevent us from saying that $\vec{V}=\vec{\nabla} \times \vec{U}$?","This question has an answer in the language of high level mathematics. Can somebody explain this in the language of vector calculus. Part I: Let us consider Cartesian coordinate system with origin $O$ and axes $x,y,z$ . Let: $$r=\sqrt{x^2+y^2+z^2}$$ $$\text{and }\vec{V}=0\ (\hat{i})
+\dfrac{\partial}{\partial z} \left( \dfrac{1}{r}  \right) (\hat{j})
-\dfrac{\partial}{\partial y} \left( \dfrac{1}{r}  \right) (\hat{k})$$ It is obvious that $\dfrac{1}{r}$ is defined everywhere except at the origin. Now let us take the divergence of $\vec{V}$ : $$\vec{\nabla} \cdot \vec{V}=0$$ Since $\dfrac{1}{r}$ is not defined at the origin, $\vec{\nabla} \cdot \vec{V}=0$ is true everywhere except at the origin. Since $\vec{\nabla} \cdot \vec{V} \neq 0$ at a point $P (0,0,0)$ , will this prevent us from concluding $\vec{V}=\vec{\nabla} \times \vec{U}$ at points other than $P$ ? Why? Why not? Part II: If $\vec{\nabla} \cdot \vec{V} \neq 0$ at points on a one dimensional arbitrary curve in space, will this prevent us from concluding $\vec{V}=\vec{\nabla} \times \vec{U}$ at other points not on the curve? Why? Why not? NOTE - For both Part I and Part II: If (Why/Why not) is beyond the scope of vector (multivariable) calculus, just reply as yes/no . However please try your best to explain (Why/Why not) in the language of vector (multivariable) calculus. SEMI ANSWER: Please point out the limitations I have stumbled upon a derivation in the language of elementary vector calculus. Please point out if there are any limitations in my derivation. In the context of advanced mathematics (de Rham cohomology or Poincare lemma), it seems to me that there are limitations. Derivation: To prove: At all points where $\vec{B}$ is defined (whatever be the domain of $\vec{B}$ ), if $\vec{\nabla} \cdot \vec{B}=0$ , then $\vec{B}=\vec{\nabla} \times \vec{A}$ Proof: At all points where $\vec{B}$ is defined (whatever be the domain of $\vec{B}$ ): \begin{align}
\vec{B} &= B_x\ (\hat{i}) + B_y\ (\hat{j}) + B_z\ (\hat{k})\\
&= B_x\ (\hat{i}) + B_y\ (\hat{j}) + \int^{(x,y,z)}_{(x,y,\infty)}
\dfrac{\partial B_z}{\partial z}\ dz\ (\hat{k})\\
&= B_x\ (\hat{i}) + B_y\ (\hat{j}) - \int^{(x,y,z)}_{(x,y,\infty)}
\left(  \vec{\nabla} \cdot \vec{B} - \dfrac{\partial B_z}{\partial z}\   \right) dz\ (\hat{k})\\
&\text{{Since $\vec{\nabla} \cdot \vec{B}=0$}}\\
&= B_x\ (\hat{i}) + B_y\ (\hat{j}) - \int^{(x,y,z)}_{(x,y,\infty)}
\left(   \dfrac{\partial B_x}{\partial x} + \dfrac{\partial B_y}{\partial y}\   \right) dz\ (\hat{k})\\
&= B_x\ (\hat{i}) + B_y\ (\hat{j})\
+ \left[
 \dfrac{\partial}{\partial x}
\left(- \int^{(x,y,z)}_{(x,y,\infty)}B_x\ dz \right) 
-\dfrac{\partial}{\partial y}
\left(  \int^{(x,y,z)}_{(x,y,\infty)}B_y\ dz \right)
\right]
(\hat{k})\\
&\text{{By changing the order of integration and differentiation}}\\
\end{align} At all points where $\vec{B}$ is defined (whatever be the domain of $\vec{B}$ ), let's define: $\displaystyle \vec{A}=\int^{(x,y,z)}_{(x,y,\infty)}B_y\ dz\ (\hat{i}) - \int^{(x,y,z)}_{(x,y,\infty)}B_x\ dz\ (\hat{j}) + 0\ (\hat{k}) + \vec{\nabla}f$ where $f$ is an arbitrary function of $(x,y,z)$ Therefore at all points where $\vec{B}$ is defined (whatever be the domain of $\vec{B}$ ): \begin{align}
\vec{B} &= \left(
\dfrac{\partial A_z}{\partial y}-\dfrac{\partial A_y}{\partial z}
\right) (\hat{i})
+\left(
\dfrac{\partial A_x}{\partial z}-\dfrac{\partial A_z}{\partial x}
\right) (\hat{j})
+\left(
\dfrac{\partial A_y}{\partial x}-\dfrac{\partial A_x}{\partial y}
\right) (\hat{k})\\
&= \vec{\nabla} \times \vec{A}
\end{align}","['divergence-operator', 'vector-fields', 'curl', 'multivariable-calculus', 'vector-analysis']"
3015459,solving Differential Equation $y''+x^2 y'+(2x+1)y=0$,"I tried to solve this problem with power series method,
but it became so complicated.
like this: $na_{n-2}+n(n+1)a_{n+1}+a_{n-1}=0 $ for $n>=2$ And I cannot solve this a_{n} How can I get this? $y''+x^2 y'+(2x+1)y=0$",['ordinary-differential-equations']
3015464,Second derivative of a matrix quartic form,"I need to compute the second derivative of the following quartic expression: $$x^H A^H x x^H A x$$ where is Hermitian. I have tried to compute the first derivative, and if I am not wrong, it should be: $$(A+A^H) x x^H (A+A^H) x$$ But then, I do not know how to proceed to calculate the second derivative. Could someone sketch the steps I need to follow? Thank you.","['matrices', 'derivatives', 'linear-algebra', 'optimization']"
3015465,"What's behind Tesla's (attributed) claim that ""3-6-9 is the key to the universe""? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 5 years ago . Improve this question If you only knew the magnificence of the 3, 6 and 9, then you would have a key to the universe. — Nikola Tesla (attributed) There are a number of videos on youtube showing some interesting arithmetic patterns regarding digital roots (call it $T(•)$ ). For instance, $\forall n \in \mathbb{N}$ : $T(2^n)=x_{mod(n,6)}, x=(1,2,4,8,7,5)$ $T(5^n)=x_{mod(n,6)}, x=(1,5,7,8,4,2)$ $T(4^n)=x_{mod(n,3)}, x=(1,4,7)$ $T(7^n)=x_{mod(n,3)}, x=(1,7,4)$ $T(8^n)=x_{mod(n,2)}, x=(1,8)$ And $\forall n>1$ : $T(3^n)=T(6^n)=T(9^n)=9$ Also, $\forall n \in \mathbb{N}$ : $T(3(n+1))=x_{mod(n,3)}, x=(3,6,9)$ These sequences are represented in the following diagram by Randy Powell : Another version was drawn by Math teacher Joey Grether (who attributed it to Tesla as a hoax, after unsuccessfully trying to divulge it): So the question is: is there a mathematical structure behind all this, or is it just a simple number game?","['number-theory', 'modular-arithmetic']"
3015467,Real solution of $(\cos x -\sin x)\cdot \bigg(2\tan x+\frac{1}{\cos x}\bigg)+2=0.$,"Real solution of equation $$(\cos x -\sin x)\cdot \bigg(2\tan x+\frac{1}{\cos x}\bigg)+2=0.$$ Try: Using Half angle formula $\displaystyle \cos x=\frac{1-\tan^2x/2}{1+\tan^2 x/2}$ and $\displaystyle \sin x=\frac{2\tan^2 x/2}{1+\tan^2 x/2}$ Substuting These values in equation we have an polynomial equation in terms of $t=\tan x/2$ So our equation $$3t^{4}+6t^{3}+8t^{2}-2t-3=0$$ Could Some Help me how to Factorise it. OR is there is any easiest way How to solve it, Thanks","['trigonometry', 'roots', 'polynomials']"
3015499,Convergence of $a_n$ given $a_{\lfloor{x^n}\rfloor}$ converges to $0$ [duplicate],"This question already has an answer here : Convergence of a sequence with assumption that exponential subsequences converge? (1 answer) Closed 5 years ago . Sequence $a_n$ has a property that for every real $x > 1$ , sequence $a_{\lfloor{x^n}\rfloor}$ converges to $0$ . Does that mean that $a_n$ converges to $0$ ? I have tried to find a counterexample, but failed at it, so I think it might be true, but I don't know how to prove it.","['limits', 'convergence-divergence', 'real-analysis']"
3015527,"Example of function which is twice differentiable with $f,f''$ strictly increasing but $\lim_{x\to \infty}f(x)\neq \infty$","I wanted to find Example of function which is twice differentiable with $f,f''$ strictly increasing but $\lim_{x\to \infty}f(x)\neq \infty$ . My usual notion fails for above statement .
As I thought if $f$ is strictly increasing and $f''$ strictly incresing means $f'$ should also in increase. Where is my mistake in my thinking ? Any help will be appreciated","['derivatives', 'real-analysis']"
3015570,Find the integral $\int _{1}^{e} (x \ln x)^2 dx$.,"Find : $$\int _{1}^{e} (x \ln x)^2 \;dx.$$ My answer: I have tried integration by parts with $u = x^2$ and $dv = (\ln x)^2$ but I end up having the same integration another time! I reversed the role of $u$ & $v$ , but it also did not work? Do you have any suggestions ?","['integration', 'calculus', 'analysis']"
3015623,Denseness of algebraic points within a variety,"I want to use the following statement concerning varieties, but I do not know why it is true. Claim . Let $V \subset \mathbb{C}^n$ be a variety defined over $\mathbb{Q}$ . Then the set $V \cap \bar{\mathbb{Q}}$ is dense in $V$ with respect to the Hausdorff topology (not the Zariski topology). Here, $\bar{\mathbb{Q}}$ denotes the algebraic closure of the rationals $\mathbb{Q}$ . It was pointed out to me that one can show this using the so-called Tarski-Seidenberg Principle, in particular using Proposition 5.3.5 of Real Algebraic Geometry by Bochnak, Coste, Roy. Let $R$ be a real closed field, $A\subset R^m$ and $B\subset R^n$ semialgebraic sets, and $f:A\to B$ a semialgebraic map with graph $G\subset A\times B$ . Let $K$ be a real closed extension of $R$ , and denote the extension of a semialgebraic set $S$ defined over $R$ to $K$ as $S_K$ . Proposition 5.3.5 i) The semialgebraic set $A$ is open (resp. closed) in $R^m$ iff $A_K$ is open (resp. closed) in $K^m$ . More generally, $clos(A_K)=(clos(A))_K$ . ii) The semialgebraic mapping $f$ is continuous iff $f_k$ is continuous. I do not see how my statement follows. Does anyone have experience with applying this principle to this kind of situation? Or can you think of another approach leading to a proof of the claim?","['algebraic-geometry', 'real-algebraic-geometry']"
3015673,A question on Frobenius groups,"This is a follow up to this question Let $p$ and $q$ be distinct primes and $G\cong(\underbrace{\mathbb{Z}_{q}\times\mathbb{Z}_{q}\times\dots\times\mathbb{Z}_{q}}_{n\,\,times})\rtimes\mathbb{Z}_{p}$ , where a subgroup of order $p$ acts irreducibly on the kernel( means $G$ has no proper subgroup of order $pq^{i}$ , for $1\leqslant i\leqslant n-1$ ). How can we show that $p\nmid (q^{i}-1)$ for each $1\leqslant i\leqslant n-1$ js21 answered the question: Assume that $p \neq q$ , and let $r$ be the order of $q$ modulo $p$ . The set of irreducible representations of $G = \mathbb{F}_p$ over $\mathbb{F}_q$ is in bijection with Frobenius-orbits of irreducible representations of $G$ over $\overline{\mathbb{F}_q}$ . Here, the non-trivial irreps over $\overline{\mathbb{F}_q}$ are $1$ -dimensional (corresponding to $p$ -roots of unity in $\overline{\mathbb{F}_q}$ ), and their Frobenius-orbit has length $r$ , so that the irrep over $\mathbb{F}_q$ corresponding to such an orbit has dimension $r$ . Thus there are exactly $1 + \frac{p-1}{r}$ irreps of $G$ over $\mathbb{F}_q$ : the trivial one, and $\frac{p-1}{r}$ of dimension $r$ . So in your question, $n >1$ implies $n =r$ , and thus $p$ does not divide $q^i -1$ for $0 < i < n$ . I have a few question about the answer and I would be very thankful if someone kindly help me about them. Could someone introduce me a reference for the first statement: ""The set of irreducible representations of $G = \mathbb{F}_p$ over $\mathbb{F}_q$ is in bijection with Frobenius-orbits of irreducible representations of $G$ over $\overline{\mathbb{F}_q}$ .""  . Also I don't understand what he mean of ""Frobenius-orbit"" and also what's the meaning of $\overline{\mathbb{F}_q}$ . I apologize if my questions are elementary. Thank you in advance.","['group-theory', 'finite-groups']"
3015697,How to make it formally correct?,"Can someone help me formalizing this statement: $$
z= x^0 +ix^1
$$ And therefore $$
\frac{\partial}{\partial z} = \frac{\partial}{\partial (x^0 +ix^1)} = \frac{\partial}{\partial x^0} + \frac{1}{i} \frac{\partial}{\partial x^1}
$$ My problem is with the last equality, I see it's right, but I'm not sure I'm allowed to do it in that way. Is there a way to procede more formally?","['partial-derivative', 'multivariable-calculus', 'calculus', 'analysis']"
3015813,Picking marbles,"We have 15 urns each of them having a different number of marbles, from 1 to 15. We start by picking the same number of marbles from each of the urns we choose. We repeat the process until we have picked all marbles. What is the minimum number of days we can finish picking all marbles? Just to clarify that it is not necessary to pick marbles from EVERY urn. I don't think I can make it in less than 5 moves (start by picking 6, then 4, then 3, then 2 and 1) but I am fairly sure it can be done in 4 or maybe less. Any ideas?",['combinatorics']
3015816,Polynomials $P(x)\in k[x]$ satisfying condition $P(x^2)=P(-x)P(x)$,"Fix a field $k$ .  Let $P(x)\in k[x]$ be such that $$(1)\ \ \ \ \ P(x^2)=P(-x)P(x).$$ Let $T(k,d)\subseteq k[x]$ denote the set of solutions to $(1)$ of degree $d$ , and $t(k,d)=\big|T(k,d)\big|$ .  Constant solutions are obviously $P\equiv 0$ and $P\equiv 1$ .  So $T(k,0)=\{0,1\}$ and $t(k,0)=2$ .  We assume from now on that $d>0$ , so $P(x)$ is non-constant. Questions: (a) Is it possible to list all elements of $T(k,d)$ ?   (b) How to calculate $t(k,d)$ ?  (If this is too broad, we can take $k$ to be $\Bbb R$ or $\Bbb C$ .) Here is my attempt.  If $z\in \overline{k}$ is a root of $P(x)$ , then $z^{2^n}$ is a root of $P(x)$ for every $n$ .  Since $P$ has finitely many roots, we must have $z=0$ or $z^{2^n-1}=1$ for some positive integer $n$ .  Suppose that $0$ is a root of $P(x)$ of multiplicity $m_0$ .  Then, $P(x)=x^{m_0}P_1(x)$ for some $P_1(x)\in k[x]$ such that $P_1(0)\neq 0$ .   Using $(1)$ we get $$(2)\ \ \ \ \ P_1(x^2)=(-1)^{m_0}P_1(-x)P_1(x).$$ Let $m_1$ be the smallest positive integer such that $P_1(x)$ has a root $w_1\in\overline k$ such that $w_1^{2^{m_1}-1}=1$ .  Because $m_1$ is the smallest possible, $w_1$ , $w_1^2$ , $w_1^{2^2}$ , $\ldots$ , $w_1^{2^{m_1-1}}$ are pairwise distinct roots of $P_1(x)$ .  This shows that $$P_1(x)=\prod_{r=0}^{m_1-1}\left(x-w_1^{2^r}\right)P_2(x),$$ where $P_2(x)\in\overline{k}[x]$ satisfies $$P_2(x^2)=(-1)^{m_0+m_1}P_2(-x)P_2(x).$$ In the end, we can use induction to show that there exist integers $m_0\geq 0$ , $m_1>0$ , $m_2>0$ , $\dots$ , $m_l>0$ such that $$(3)\ \ \ \ \ P(x)=(-1)^d\,x^{m_0}\,W_1(x)\,W_2(x)\,\cdots\,W_l(x)\,,$$ where $m_0+m_1+m_2+\ldots+m_l=d$ and each $W_j(x)\in \overline{k}[x]$ is of the form $$\prod_{r=0}^{m_j-1}\,\left(x-w_j^{2^{r}}\right)\,,$$ where each $w_j\in\overline{k}$ is a root of $z^{2^{m_j}-1}-1$ , but not a root of $z^{2^r-1}=1$ for any positive integer $r<m_j$ .  If $k$ is algebraically closed then $(3)$ is as good a description as you can get, I guess.  But what happens when $k$ is not algebraically closed? In particular, if $d=4$ and $k=\Bbb C$ , there are the following possible choices: $P(x)=x^4$ $P(x)=x^3\,(x-1)$ $P(x)=x^2\,(x-1)^2$ $P(x)=x^2\,(x^2+x+1)$ $P(x)=x\,(x-1)^3$ $P(x)=x\,(x-1)\,(x^2+x+1)$ $P(x)=x\,\prod\limits_{r=0}^2\,\Bigg(x-\exp\left(\frac{2\pi {2^r} i}{7}\right)\Bigg)$ $P(x)=x\,\prod\limits_{r=0}^2\,\Bigg(x-\exp\left(-\frac{2\pi {2^r} i}{7}\right)\Bigg)$ $P(x)=(x-1)^4$ $P(x)=(x-1)^2\,(x^2+x+1)$ $P(x)=(x-1)\,\prod\limits_{r=0}^2\,\Bigg(x-\exp\left(\frac{2\pi {2^r} i}{7}\right)\Bigg)$ $P(x)=(x-1)\,\prod\limits_{r=0}^2\,\Bigg(x-\exp\left(-\frac{2\pi {2^r} i}{7}\right)\Bigg)$ $P(x)=(x^2+x+1)^2$ $P(x)=\prod\limits_{r=0}^3\,\Bigg(x-\exp\left(\frac{2\pi {2^r} i}{15}\right)\Bigg)$ $P(x)=\prod\limits_{r=0}^3\,\Bigg(x-\exp\left(-\frac{2\pi {2^r} i}{15}\right)\Bigg)$ $P(x)=x^4+x^3+x^2+x+1$ . Unless I omitted something, this gives $t(\Bbb C,4)=16$ .  Furthermore, only polynomials 1-6, 9-10, 13, 16 have real coefficients (in fact rationals), $t(\Bbb R,4)= t(\Bbb Q,4)=10$ .","['functional-equations', 'field-theory', 'combinatorics', 'polynomials', 'extension-field']"
3015826,Is there a combinatorial proof that the Catalan number $C_n$ satisfies $(n+1)C_n={2n \choose n}$?,"I saw this question and thought that may be it is possible to prove that the $n^{\text{th}}$ Catalan number $C_n$ equals $\frac{1}{n+1}{2n\choose n}$ by taking a set $A$ of size $n+1$ and another set $B$ of size $C_n$ such that there exists a $1$ - $1$ correspondence $A\times B\to T$ , where $T$ is the set of subsets of $\{1,2,\ldots,2n\}$ of size $n$ .  I made my attempt but failed, but I am curious if there is a known bijection $A\times B\to T$ for some $A$ , $B$ . I know that there are combinatorial proofs that show $C_n={2n\choose n}-{2n\choose{n-1}}$ , but I want a specific proof that shows $(n+1)C_n={2n\choose n}$ .  Below is my attempt. Write $[k]=\{1,2,\ldots,k\}$ .  Furthermore, $\binom{X}{k}$ is the set of all subsets of cardinality $k$ of a given set $X$ . Let there be $2n$ people, named, $1$ , $2$ , $\ldots$ , $2n$ .  The $2n$ people are seated around a round table in the counterclockwise order.  Let $\mathcal{P}$ denote the set of all pairings $$\big\{\{x_1,y_1\},\{x_2,y_2\},\ldots,\{x_n,y_n\}\big\}$$ of $[2n]$ in such a way that, when $x_i$ shakes hand with $y_i$ simultaneously for every $i\in[n]$ , there are no crossing arms.  Wlog, we assume that $x_i<y_i$ for each $i\in[n]$ and that $x_1<x_2<\ldots<x_n$ . Define $f:[n+1]\times\mathcal{P}\to\binom{[2n]}{n}$ as follows: $$f\Big(k,\big\{\{x_1,y_1\},\{x_2,y_2\},\ldots,\{x_n,y_n\}\big\}\Big)=\{y_1,y_2,\ldots,y_{k-1},x_k,x_{k+1},\ldots,x_n\}$$ for each $k\in[n+1]$ and $\big\{\{x_1,y_1\},\{x_2,y_2\},\ldots,\{x_n,y_n\}\big\}\in\mathcal{P}$ .  Well, this is where my idea fails.  I was hoping that $f$ will be a bijection, but it isn't even injective.","['catalan-numbers', 'combinatorial-proofs', 'reference-request', 'functions', 'combinatorics']"
3015835,5x5 Digit Matrix Puzzle,"Is it possible to construct a 5x5 matrix of decimal digits, such that each of the numbers 0-99 are present as individual or adjacent digits.  Adjacent can mean horizontal, vertical or diagonal and reversed in all three cases.  For example, the matrix below contains 18 as the top left digits are 1 and 8, and 60 is diagonal at the bottom left. \begin{array}{ccc}
  1&8&7&9&4\\
  5&1&8&3&5\\
  3&0&6&2&5\\
  9&9&2&6&7\\
  3&1&4&4&0
\end{array} The above matrix isn't a solution as 33,48,77 and 84 are missing.  I would be interested to see a successful solution, or a proof that none exists.",['combinatorics']
3015862,Show that $E(X|\mathcal{F}_\tau)=\sum\limits_{n\in\mathbb{N}}E(X|\mathcal{F}_n)\mathbf{1}_{\{\tau=n\}}$,"If $\mathbf{E}X<\infty$ and $\tau$ is a stopping time, then $$\mathbf{E}(X|\mathcal{F}_\tau)=\sum_{n\in\mathbb{N}}\mathbf{E}(X|\mathcal{F}_n)\mathbf{1}_{\{\tau=n\}}.$$ My attempt: First assume that $X$ is nonnegative. The general case will follow directly from nonnegative case. Let $A\in \mathcal{F}_\tau.$ Then $A\cap \{\tau=n\}\in \mathcal{F}_n$ for every $n\in\mathbb{N}.$ Therefore, \begin{align*}
    \int_{A}\sum_{n\in\mathbb{N}}\mathbf{E}(X|\mathcal{F}_n)\mathbf{1}_{\{\tau=n\}}d\mathbf{P}&=\sum_{n\in\mathbb{N}}\int_{A}\mathbf{E}(X|\mathcal{F}_n)\mathbf{1}_{\{\tau=n\}}d\mathbf{P}=\sum_{n\in\mathbb{N}}\int_{A\cap \{\tau=n\}}\mathbf{E}(X|\mathcal{F}_n)d\mathbf{P}\\&=\sum_{n\in\mathbb{N}}\int_{A\cap \{\tau=n\}}Xd\mathbf{P}=\int_A Xd\mathbf{P}
\end{align*} where the first equality follows from monotone convergence theorem, third follows from the definition of the conditional expectation. Question: (1) To prove that $\sum_{n\in\mathbb{N}}\mathbf{E}(X|\mathcal{F}_n)\mathbf{1}_{\{\tau=n\}}$ is indeed the conditional expectation of $X$ wrt $\mathcal{F}_\tau$ , I have to prove that it is $\mathcal{F}_\tau$ -measurable. How should I do so? (2) Since the stopping time can take infinite value, my calculation of the integration above holds only when $\tau<\infty$ almost surely. Is the condition $\tau<\infty$ a.s. necessary here? Thanks in advance!","['conditional-expectation', 'stochastic-processes', 'stopping-times', 'probability-theory']"
3015909,Bounded function with bounded second derivative imply bounded first derivative,"Let $f$ be a $C^2$ function from $(t_1,t_2)$ to $R^n$ such that $\Vert f(t)\Vert\leq A$ and $\Vert f''(t)\Vert \leq B$ for all $t\in (t_1,t_2)$ , where $A$ and $B$ are nonnegative reals. Let $t_0\in (t_1,t_2)$ and $\alpha>0$ be such that $(t_0-\alpha,t_0+\alpha)\subset (t_1,t_2)$ . I would like to prove that $\Vert f'(t_0)\Vert\leq 2A/\alpha + B\alpha/2$ . I thought of using a Taylor expansion of $f$ but cannot manage to get the inequality.","['derivatives', 'taylor-expansion', 'real-analysis']"
3016018,Advantage of multi steps ODE methods over single step methods,"I wanna know what's the advantage of multi step ODE methods such as Adams-Bashforth over ordinary single step methods such Runge–Kutta, accuracy/time wise.","['runge-kutta-methods', 'numerical-methods', 'ordinary-differential-equations']"
3016123,The last digit of pi (in terms of Banach limits),"Let $\phi : l^\infty \to \mathbb C$ be a Banach limit , and define the sequence $\{x_k\}_{k\geq 0}$ to be the digits in the 10-base decimal expansion of $\pi$ . Note that $$\{x_k\}_{k\geq 0} \in l^\infty$$ and so we can talk about $\phi(\{x_k\}_{k\geq 0})$ . What it is? Note that Banach limits don't have to be unique. Now consider the real number $\sqrt 2$ . What is its last number? Finally, consider any element $x\in \mathbb R$ . Can we say about the last digit of $x$ , in the sense of Banach limits as considered above?","['limits', 'banach-spaces', 'functional-analysis', 'sequences-and-series']"
3016145,Schrödinger Equation in Spherical Coordinates,"I am trying to learn how to solve three dimensional Schrödinger Equation in Spherical Coordinates. I was reading a text book and I found that there is a missed step in the solution, seen below: The $\theta$ equation, $$\sin\theta\frac{\mathrm d}{\mathrm d\theta}\left(\sin\theta\frac{\mathrm d\Theta}{\mathrm d\theta}\right)+\left[\ell\left(\ell+1\right)\sin^2\theta-m^2\right]\Theta=0.\tag{4.25}$$ is not so simple. The solution is $$\Theta(\theta)=AP_\ell^m\left(\cos\theta\right).\tag{4.26}$$ where $P^m_\ell$ is the associated Legendre function , defined by $$P_\ell^m\left(x\right)\equiv\left(1-x^2\right)^{\vert m\vert/2}\left(\frac{\mathrm d}{\mathrm dx}\right)^{\vert m\vert}P_\ell(x).\tag{4.27}$$ It says that the solution of equation (4.25) is not simple and gives directly as equation (4.26). Can you help me to learn how to solve such differential equations?","['coordinate-systems', 'ordinary-differential-equations', 'mathematical-physics']"
3016149,"Given a book with $100$ pages and a $100$ lemmas, prove that there is some lemma written on the same page as its index","A book consists of 100 pages and contains 100 lemmas and some images. Each lemma is at most one page long and can't be split into two pages (it has to fit in one page). The lemmas are numbered from 1 to 100 and are written in ascending order. Prove that there must be at least one lemma written on a page with the same number as the lemma's number. If lemma no. $1$ is written on page $1$ , then it is proved. Let's assume lemma no. $1$ is written on page $k, k>1$ . Then on at least one page there must be $2$ lemmas. Let's assume that always on page $k+i$ we have lemma no. $i+1$ and so on. Then the last $100-k-i$ lemmas must fit on the last page, which means that there will be at least one lemma (number $100$ ) on page $100$ . But I don't know how to express it in a more mathematical way! Any help?",['combinatorics']
3016161,Moduli space of vector bundles open in moduli of sheaves,"Let $X$ be a smooth projective variety with polarization $\mathcal{O}_X(1)$ . For fixed Chern class $r,c_1,c_2....$ , do we have an OPEN embedding $$M^{ss}(X,r,c_1...)_{vec}\subset M^{ss}(X,r,c_1...)$$ of moduli space (Gieseker)-semistable vector bundles into the moduli space of semistable sheaves with the above Chern classes? ( I guess I should assume there is such a vector bundle to start with) Also, since $\mu$ -stable implies stable, is the moduli space of $\mu$ -stable vector bundles with the above Chern character openly embedded in $M^{ss}(X,r,c_1...)_{vec}$ ? Thanks!","['algebraic-geometry', 'moduli-space']"
3016162,Proof that $\exp(x+y) = \exp(x)*\exp(y)$ using limit definition of $\exp(x)$,"I want to prove: $\exp(x+y) = \exp(x)\cdot \exp(y)$ using the definition: $\exp(x) = \lim_{n\to\infty} (1+\frac{x}{n})^n$ I am having trouble completing the proof, but here is my idea so far: $$\lim_{n\to\infty} \left(1+\frac{x+y}{n}\right)^n = \lim_{n\to\infty} \left(1+\frac{x}{n}\right)^n \cdot \lim_{n\to\infty} \left(1+\frac{y}{n}\right)^n =   
 \lim_{n\to\infty} \left(\left(1+\frac{x}{n}\right)^n \cdot \left(1+\frac{y}{n}\right)^n  \right) $$ Now I rearrange the last expression: $$\lim_{n\to\infty} \left(1+\frac{x+y+\frac{xy}{n}}{n}\right)^n $$ From here my idea is to somehow show that this limit is equal to $$\lim_{n\to\infty} \left(1+\frac{x+y}{n}\right)^n = \exp(x+y)$$ using the Squeeze Theorem and perhaps Bernoulli's Inequality, but I am at a loss as to how exactly to do it. I'd appreciate your help.","['limits', 'exponential-function', 'real-analysis']"
3016186,"Functional Analysis by Rudin, Chapter $10$, Exercise $20$","This is the exercise: Suppose $x\in A$ , $x_n\in A$ , and $\lim x_n=x$ . Suppose $\Omega$ is
  an open set in $\mathbb C$ that contains a component of $\sigma(x)$ .
  Prove that $\sigma(x_n)$ intersects $\Omega$ for all sufficiently
  large $n$ . (This strengthens Theorem 10.20.) Hint: If $\sigma(x)\subset\Omega\cup\Omega_0$ , where $\Omega_0$ is an open
  set disjoint from $\Omega$ , consider the function $f$ that is $1$ in $\Omega$ , and $0$ in $\Omega_0$ . According to Rudin's hint, we can define $f\in
H(\Omega\cup\Omega_0)$ as follows: $$
f(\lambda) = \left \{\begin{array}{lll}
  1 &  \ \ \ \ \lambda\in \Omega\\
  0  & \ \ \ \ \lambda\in \Omega_0\\
  \end{array}\right.
$$ And then by symbolic calculus we have $$
\tilde{f}(x) = \left \{\begin{array}{lll}
  e &  \ \ \ \ \sigma(x)\subset \Omega\\
  0  & \ \ \ \ \sigma(x)\subset \Omega_0\\
  \end{array}\right.
$$ For the $x\in A$ that $x_n\rightarrow x$ , we have $$\tilde{f}(x)=\tilde{f}(\lim_{n\to\infty}x_n)=\lim_{n\to\infty}\tilde{f}(x_n),$$ meaning $\{\tilde{f}(x_n)\}$ as a sequence in $A$ converges to $\tilde{f}(x)$ . So for a large enough $n$ ; $$\sigma\big(\tilde{f}(x_n)\big)\cap\sigma\big(\tilde{f}(x)\big)\neq\emptyset.$$ And by spectral mapping theorem; $$f\big(\sigma(x_n)\big)\cap f\big(\sigma(x)\big)\neq\emptyset,$$ $$f\big(\sigma(x_n)\big)\cap \{0,1\}\neq\emptyset.$$ So we are forced to have only one of these three possibilities; 1) $f\big(\sigma(x_n)\big)=\{0,1\}$ 2) $f\big(\sigma(x_n)\big)=\{1\}$ 3) $f\big(\sigma(x_n)\big)=\{0\}$ The third case never happens. Because if it happens by the
definition of $f$ , we can conclude for any large $n$ that $$\sigma(x_n)\subset\Omega_0,$$ in which by tending $n$ to infinity, we get the following
contradiction; $$\sigma(x)\subset\Omega_0.$$ In the first and second cases, there will be at least one $\lambda\in \sigma(x_n)$ such that $f(\lambda)=1$ which by
definition simply means $\sigma(x_n)$ intersects $\Omega$ . Is this true? Thank you for your help.","['spectral-theory', 'proof-verification', 'functional-analysis']"
3016196,Editions of Niven-Zuckerman book on number theory,There are several editions of this popular introduction to the theory of numbers. Are they substantially different from one another? Do you think the edition in which Hugh Montgomery appears as co-author is the one  I should definitely acquire? Thanks a bunch for your replies!,"['number-theory', 'reference-works', 'elementary-number-theory', 'reference-request']"
3016198,Is there a generalization of Pfaffians?,"For an skew-symmetric matrix $A$ (meaning $A^T=-A$ ), the Pfaffian is defined by the equation $(\text{Pf}\,A)^2=\det A$ . It is my understanding that this is defined for anti-symmetric matrices because it is known that the determinant of an anti-symmetric matrix is always a square of a polynomial in the entries of the matrix. Now, skew-symmetry is sufficient to prove that the determinant is a square of a polynomial, but it is not necessary. The simplest example is the $2n\times 2n$ matrix $A=a I_{2n}$ with $a\in\mathbb{C}$ and $I_k$ the $k\times k$ identity matrix. The determinant is $\det A = a^{2n} = (a^n)^2$ . Of course, for $a\neq 0$ , $A$ is not skew-symmetric. I have a few questions about this. Is there a generalization of a Pfaffian for any matrix whose determinant is a square of a polynomial? Is there a characterization (or some known set of properties) of matrices  whose determinants are squares of polynomials? (Edit) Are there any known necessary and sufficient conditions for a matrix to have its determinant be the square of a polynomial (aside from skew-symmetry being sufficient)? (Edit 2) For those who are curious, these questions arise from a problem from physics I am working on. I have a certain class of matrices  whose characteristic polynomials (which arise as the determinant of a non-skew-symmetric matrix) appear to be the squares of Chebyshev polynomials. If I could prove that these characteristic polynomials must be squares of polynomials (using properties of the matrix) then I may be able to use some of the properties attributed to Pfaffians (or the proper generalization to non-skew-symmetric matrices) to confirm that they are indeed squared Chebyshev polynomials. (Edit 3) To be as concrete as possible, I am looking for any information (e.g., answers to questions 1-3) on the set $$\{A\in\mathcal{M}_n(\mathbb{C}): \det A = p(\{a_{ij}\})^2\text{ with }p\text{ a polynomial} \}$$ where $\mathcal{M}_n(\mathbb{C})$ is the set of $n\times n$ complex matrices and $a_{ij}$ is the $i,j$ 'th entry of $A$ .","['determinant', 'linear-algebra', 'pfaffian']"
3016199,Conditional probability (urns and balls),"From an urn, containing 6 white and 12 black balls, one takes balls randomly one by one until the second white ball appears. 
What is the probability that:
1) second white ball appears on the second step
2) second white ball appears on the third step
3) second white ball appears on the k-th step My solution is the following.
The probability of the second white ball is $$Pr(X=2)=\frac{6}{18}\frac{5}{17}$$ The probability of the third white ball is $$Pr(X=3)=\frac{6}{18}\frac{12}{17}\frac{5}{16}+ \frac{12}{18}\frac{6}{17}\frac{5}{16} = \frac{2⋅6⋅5⋅12}{18⋅17⋅16} $$ Therefore, the probability $$Pr(X=k)=\frac{(k-1)⋅6⋅5⋅(18-k)!⋅12!}{18!⋅(12-k+2)!}$$ Could somebody, please, check my solution, especially the third part. Thank you.","['balls-in-bins', 'probability']"
3016259,$\iint_D f = 0$ implies $f(p)=0$ for all $p$.,"If $D$ is open, and if $f$ is continuous, bounded, and obeys $f(p) \ge 0$ for all $p \in D$ , then $\iint_D f = 0$ implies $f(p)=0$ for all $p$ . The hint in the back of my book says that There's a neighborhood where $f(p) \ge \delta$ . From the definition in my book, it states that The double integral $\iint_R f$ exists and has value $v$ if and only if for any $\epsilon > 0$ there is a $\delta > 0$ such that $|S(N,f,\{p_{ij}\})-v| < \delta$ . There's another theorem in the book that states that If $f(p) \ge 0$ for all $p\in D$ , $\iint_D f \ge 0$ . So far, from the information I've been given, that would mean that I could use the contrapositive to prove that if $f(p) \ne 0$ for all $p$ , then $\iint_D f \ne 0$ . Since $f(p) \ne 0$ I could use cases. Case 1: If $f(p) > 0$ , then I could use the aforementioned theorem to show that $\iint_D f > 0$ which is not equal to $0$ . Case 2: If $f(p) < 0$ , then $-f(p)=g(p)>0$ , so that $\iint_D g(p) > 0$ . Then $\iint_D g(p) > 0$ is equivalent to - $\iint_D f(p) > 0$ which is not equal to $0$ . I would like to know if I'm approaching this proof the correct way? Also, would it be difficult to instead try a direct proof using the hint and the definition that I stated above?","['multivariable-calculus', 'calculus', 'analysis']"
3016262,expectation of absolute value of cauchy distribution & general questions about expectation of absolute value of distributions,"The Cauchy distribution has PDF: $$f_X(x) = \frac{1}{\pi(1 + x^2)}$$ Its expectation does not exist. If we wanted to compute the expectation of the absolute value of this distribution, would it be correct to do the following: $$E(|X|) = \int_{-\infty}^{\infty} |x| \cdot \frac{1}{\pi(1 + x^2)} dx = \int_{-\infty}^{0} -x \cdot \frac{1}{\pi(1 + x^2)} dx + \int_{0}^{\infty} x \cdot \frac{1}{\pi(1 + x^2)} dx$$ If I'm not mistaken, neither of the integrals in the above sum of integrals converges, so does this mean that the expectation of the absolute value also does not exist? Furthermore, is this a general result (that if the expectation of the original distribution does not exist, the expectation of the absolute value of the distribution also does not exist), or is it specific to just this distribution? And what about the converse: if the expectation of the absolute value of the distribution does not exist, what can we conclude (if anything) about the expectation of the original distribution?","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
3016331,Modulo and probability,How can I prove that 4 modulo 5 is 4? My though is floor of (4 / 5) is 0 then the remaining is = to the modulo.Am I right?,"['elementary-number-theory', 'discrete-mathematics']"
3016455,Ring with infinitely many totally ordered prime ideals,"I was looking for a specific ring with infinitely many prime ideals such that they are totally ordered by inclusion.
A valuation ring with rank $\mathbb{N} \cup \infty$ should work, or something like that, but I don't know any.","['algebraic-geometry', 'ring-theory', 'abstract-algebra', 'maximal-and-prime-ideals']"
3016461,Proving v1+v2 is not an eigenvector of A,"Let $\lambda_1$ and $\lambda_2$ be two distinct eigenvalues of an $n \times n$ matrix $A$ , $v_1$ and $v_2$ are the corresponding eigenvectors. Prove that $v_1 + v_2$ is not an eigenvector of $A$ . Is this how you set this up? Unsure where to begin. $A(v_1+v_2) = Av_1  + Av_2$ $A(v_1+v_2) = \lambda_1v_1 + \lambda_2v_2$ ...","['matrices', 'linear-algebra']"
3016671,When are quadratic rings of integers unique factorization domains?,"Let $D$ be a square free integer. Let $R_D$ be the integral closure of $\mathbb{Z}$ in the field $\mathbb{Q}(\sqrt{D})$ . For some values $D$ , the ring $R_D$ is a $UFD$ , but not for all. For example, the Gaussian integers $R_{-1}$ are a $UFD$ whereas the ring $R_{-5}$ is not. There are several ways to show this, including computing the class number of $R_D$ . However, all the proofs I've seen feel ad hoc and unintuitive. According to the Stark-Heegner theorem, for $D<0$ , the ring $R_D$ is a $UFD$ if and only if $$D \in \{-1,-2,-3,-7,-11,-19,-43,-67,-163\}.$$ Is there any intuitive reason why this should be a complete list? Ideally there would be a structural reason - coming up with a separate proof for each $D$ in the list is deeply unsatisfying to me.","['number-theory', 'integers', 'unique-factorization-domains', 'commutative-algebra']"
3016674,Functors that aren't continuous/smooth,"Let $\mathbf{FinVec}$ denote the category of finite-dimensional real vector spaces and linear maps, let $F\colon\mathbf{FinVec}\rightarrow\mathbf{FinVec}$ be a functor. For each pair of objects $V,W$ , the Hom-set $\operatorname{Hom}(V,W)$ is itself a finite-dimensional vector space and we equip it with the unique structure of a topological space/of a smooth manifold that is compatible with its vector space structure. The composition maps are then continuous/smooth. In this way, we view $\mathbf{FinVec}$ as enriched over the category $\mathbf{Top}$ / $\mathbf{Diff}$ respectively. It then makes sense to ask whether the functor $F$ is enriched over $\mathbf{Top}$ / $\mathbf{Diff}$ . That is just to ask whether the induced map $F\colon\operatorname{Hom}(V,W)\rightarrow\operatorname{Hom}(FV,FW)$ is continuous/smooth for all $V,W$ . (Irrelevant to the rest of the question, but the reason this is a useful notion is that it is the natural hypothesis assuring that the functor induces another functor on the category of topological/smooth vector bundles, applying the original functor fiberwise.) In the case $V=W$ , by functoriality, we can restrict $F$ to a group homomorphism $\operatorname{GL}(V)\rightarrow\operatorname{GL}(V)$ . If the functor is continuous/smooth, so is this map. Note that $\operatorname{GL}(V)$ is a Lie group. It is a general result that a measurable group homomorphism between Lie groups is automatically smooth. This begs the following questions: What's an argument for the existence of functors that aren't continuous? Can we construct one? Given the above, it either happens that the restrictions to $\operatorname{GL}(V)\rightarrow\operatorname{GL}(V)$ are continuous and continuity fails elsewhere, or the induced maps will be non-measurable, so that the construction will necessarily have to involve choice to some extent. Are there continuous functors that aren't smooth? Given the above, a continuous functor will have smooth restrictions to $\operatorname{GL}(V)\rightarrow\operatorname{GL}(V)$ , but smoothness may fail elsewhere.",['category-theory']
3016693,"$f''+pf'+qf=0$ where $q\leq0$ and $f(0)=f(1)=0$ prove $f=0$ ($f$, $p$, $q$ defined on $[0,1]$)","For $f$ defined on $[0,1]$ twice differentiable, and two continuous function defined on $[0,1]$ named $p(x)$ , $q(x)$ , satisfying $f''+pf'+qf=0$ , where $q\leq0$ and $f(0)=f(1)=0$ , prove $f=0$ . I haven't got an idea about the question. Any hint will be appreciated. Edited: for $q(x_0)<0$ where $f'(x_0)=0$ the problem can easily be solved.
So now I'm mainly concerned about $q(x_0)=0$ . It appears on my textbook, so i guess it's probably true. Counter examples are also welcomed.","['ordinary-differential-equations', 'real-analysis']"
3016708,Different forms of 2nd Bianchi's Identity,"I have been struggling with relating two different forms of Bianchi's 2nd Identity: $d\Omega = \Omega\wedge\omega-\omega\wedge\Omega$ and $\mathfrak{S}\left\{(\nabla_{Z}R)(X,Y,Z)+R(T(X,Y),Z)W)\right\}=0$ here $R$ and $T$ represent curvature and torsion tensor of a connection $\nabla$ $\Omega$ and $\omega$ represent curvature and connection form of $\nabla$ How do we relate the exterior derivative of curvature form to the covariant derivative of curvature and torsion tensors? I am able to prove them independently but I am not able to prove the equivalence. I am pretty new to differential geometry and I apologize if this question is very easy. Here are some of my thoughts: I started with curvature form. Suppose $X_1,X_2,X_3,...X_n$ represent the moving frames and $\theta^1, \theta^2,...,\theta_n$ represent the dual co frame. In the index notation the first form of bianchi's 2nd identity can be given as $d\Omega^{i}_{j} = \Omega^i_k\wedge\omega^k_j-\omega^i_k\wedge\Omega^k_j$ (using einstien's summation convention) Relation between cuvature tensor and curvature form can be given as $R(X_k,X_l)X_j = \Omega^i_j(X_k,X_l)X_i$ in the index notation curvature tensor can be given as $R(X_k,X_l)X_j = R^i_{jkl}X_i$ this gives us $R^i_{jkl}X_i = \Omega^i_j(X_k,X_l)X_i$ or $\Omega^i_{j} = \frac{1}{2}\sum_\limits{k,l}R^i_{jkl}\theta^k\wedge\theta^l$ this implies $d\Omega^{i}_{j} =\frac{1}{2}\sum_\limits{m,k,l} \frac{\partial{R^i_{j,k,l}}}{\partial{X_m}}\theta^m\wedge\theta^k\wedge\theta^l$ the covariant derivative of R in index notation can be given as: $\nabla R = \sum_\limits{h,j,k,l,i}R^i_{jkl;h}\theta^h\oplus\theta^j\oplus\theta^k\oplus\theta^l\oplus X_i$ where $R^i_{jkl;h}$ can be given as $R^i_{jkl;h} = \frac{\partial{R^i_{j,k,l}}}{\partial{X_h}} + \sum\limits_{v=1}^n R^v_{j,k,l}\Gamma_{hv}^{i}-\sum_\limits{v=1}^{n}(R^i_{vkl}\Gamma^{v}_{hj}+R^i_{kvl}\Gamma^{v}_{hk}+R^i_{jkv}\Gamma^{v}_{hl})$ where $\Gamma$ can be given as $\nabla_{X_i}{X_j} = \Gamma_{ij}^K X_k$ Also connection 1-form $\omega^i_{j}$ can be given as: $\omega^i_j = \Gamma^i_{kj}\theta^k$ on one hand I have tensor product and on the other hand I have wedge product. At this point I am stuck and wonder if there is an alternate approach which doesn't involve coordinate representations. I am also not sure about my use of $X_i$ 's as coordinate vector fields.",['differential-geometry']
3016718,A way to find this shaded area without calculus?,"This is a popular problem spreading around. Solve for the shaded reddish/orange area. (more precisely: the area in hex color #FF5600 ) $ABCD$ is a square with a side of $10$ , $APD$ and $CPD$ are semicircles, and $ADQB$ is a quarter circle. The problem is to find the shaded area $DPQ$ . I was able to solve it with coordinate geometry and calculus, and I verified the exact answer against a numerical calculation on Desmos . Ultimately the result is 4 terms and not very complicated. So I was wondering: Is there was a way to solve this using trigonometry? Perhaps there is a way to decompose the shapes I am not seeing. A couple of years ago there was a similar ""Find the shaded area"" problem for Chinese students . I was able to solve that without calculus, even though it was quite an involved calculation. Disclosure: I run the YouTube channel MindYourDecisions. I plan to post a video on this topic. I'm okay posting only the calculus solution, but it would be nice to post one using only trigonometry as many have not taken calculus. I will give proper credit to anyone that helps, thanks! Update : Thanks for everyone's help! I prepared a video for this and presented 3 methods of solving it (the short way like Achille Hui's answer, a slightly longer way like David K and Seyed's answer, and a third way using calculus). I thanked those people in the video on screen, see around 1:30 in this link: https://youtu.be/cPNdvdYn05c .","['trigonometry', 'area', 'geometry']"
3016737,Evaluating $\liminf_{n\to\infty}n\{n\sqrt2\}$,"How can we evaluate $$\liminf_{n\to\infty}n\{n\sqrt2\},$$ where $\{\cdot\}$ denotes the fractional part of $\cdot$ ? The first thing came to my mind is Pell's equation $x^2-2y^2=1$ . Knowing that $\sqrt2$ has a continued fraction $[1;2,2,2\cdots]$ , I tried to estimate the limit by using $\sqrt2$ 's convergents' denominators. It seems like the limit approximately equals to $0.36$ .","['number-theory', 'pell-type-equations', 'fractional-part', 'limsup-and-liminf']"
3016802,Reducible and Irreducible polynomials are confusing me,"The definition claims that a polynomial in a field of positive degree is a reducible polynomial when it can be written as the product of $2$ polynomials in the field with positive degrees. Other wise it is irreducible. So if a polynomial $f(x)$ can be written as the product of say $41(x^2 + x)$ , is that considered not reducible because $41$ is really $41x^0$ , and $0$ isn't technically positive, but by the definition of a polynomial in a field it is a polynomial if $a_n$ isn't $0$ for the highest degree $n$ where $n \geq 0$ . So would the example of the polynomial example I gave be reducible or irreducible?",['linear-algebra']
3016825,$\lim E[f^2(X_n)]\neq E[f^2(X)]$ even if $X_n \rightarrow ^d X$,"Let $X_n\sim N(0,1/n)$ . Is there a continuous function $f$ such that $E[f^2(X_n)]<\infty$ $\lim_{n\rightarrow \infty} E[f^2(X_n)] \neq f^2(0)$ ? Also, what would happen if I add the condition $E[|f(X)f(Y)|]<\infty$ for all jointly normal $X,Y$ such that $EX=EY=0$ I know that there is no such $f$ if we additionally require $f$ to be bounded since $X_n \stackrel{d}{\rightarrow}\delta_0$ . However, I am totally clueless when it comes to proving (or disproving) the existence of such $f$ if we drop out boundedness condition. I appreciate every hint!","['normal-distribution', 'real-analysis', 'functional-analysis', 'convergence-divergence', 'probability-theory']"
3016844,Calculating $S=\sum\limits_{n=1}^\infty\left(\frac{1}{\Gamma^2(n+1)}\right)^{{1}/{n}}$,"I tried to find the answer for the question: Numerical evaluation of $\sum_{N=1}^\infty\left(\frac{1}{\Gamma(N+1)^2}\right)^{\frac{1}{N}}$ .
I think my result is $4$ times than the expected value. Is this accidental and my solution is not correct? Does my solution have theoretical or some inattention fault? The solution is: $S=\sum\limits_{n=1}^\infty\left(\frac{1}{\Gamma^2(n+1)}\right)^{{1}/{n}}=\sum\limits_{n=1}^\infty\left(\frac{1}{n!^2)}\right)^{{1}/{n}}=\sum\limits_{n=1}^\infty \prod\limits_{k=1}^n k^{-2/n}$ $S=\sum\limits_{n=1}^\infty e^{\frac{2}{n}\sum\limits_{k=1}^n \ln(\frac{1}{k})} $ From the exponent we get: ${\frac{2}{n}\sum\limits_{k=1}^n \ln(\frac{1}{k})}={\frac{2}{n}\sum\limits_{k=1}^n \ln(\frac{n}{k} \frac{1}{n})}=\frac{2}{n}\sum\limits_{k=1}^n \ln\frac{n}{k}+\frac{2}{n}\sum\limits_{k=1}^n \ln \frac{1}{n}$ The first term of the exponent is Riemann sum so we get: $2\int\limits _0^1\ln\frac{1}{x} dx= 2$ Put back into the sum: $S=\sum\limits_{n=1}^\infty e^{2-\frac{2}{n} n \ln{n}}=\sum\limits_{n=1}^\infty \frac {e^{2}}{n^2}=\zeta(2)e^2 $","['summation', 'gamma-function', 'sequences-and-series', 'zeta-functions', 'riemann-integration']"
3016897,"Reason for the term ""smooth""","A normed space $X$ is said to be smooth if for $x \in X$ with $||x||=1$ there exists a unique bounded linear functional $f$ such that $||f||=1$ and $f(x)=||x||$ . Why the term ""smooth"" comes?","['banach-spaces', 'normed-spaces', 'functional-analysis']"
3016927,Solving for $y'' - 4y' - 5y - 2 = 0$,"I am looking to solve for the above nonhomogeneous ODE. I know how to find the general solution for the reduced equation of the homogeneous form, that is, $$y'' - 4y' - 5y = 0.$$ The characteristic equation is $r^{2} - 4r - 5 = 0$ , which gives two real and distinct roots $r=-1,5$ . So the complementary solution is $y_{c}=c_{1}e^{5x} + c_{2}e^{-x}$ . Now I am looking to guess the particular on the right-hand side but I am not sure about how to do that in order to find the general solution of the above nonhomogenous ODE.",['ordinary-differential-equations']
3016974,Equivalence of definitions of ergodic action,"Let $G$ be a group acting on a probability measure space $(X, \mu)$ by measure-preserving transformations. I have read the two following definitions of ergodicity of such an action: For every measurable set $A$ such that $\mu(gA \Delta A) = 0$ for all $g \in G$ , we have $\mu(A) = 0$ or $1$ . For every measurable set $A$ such that $gA = A$ for all $g \in G$ , we have $\mu(A) = 0$ or $1$ . It is clear that 1 implies 2. It is not hard to show that 2 implies 1 if the group is countable. But what about the more general case? What if, for instance, $G$ is a separable group acting continuously on a topological, or even metric space $X$ ? Would that be enough? Also, it is clear that a transitive action satisfies 2. But when does it satisfy 1 in the general case? Would the hypotheses above help?","['group-theory', 'group-actions', 'ergodic-theory', 'measure-theory']"
3016990,What are the commutative rings $R$ for which $A \otimes _{\Bbb Z} B = A \otimes _R B$ as abelian groups?,"This is a follow up . What are the commutative rings $R$ , for which given $R$ -modules $A$ and $B$ , $A \otimes _{\Bbb Z} B = A \otimes _R B$ as abelian groups? This is true when $R= \Bbb Q$ , or $\Bbb Z_m$ . We can give an $R$ -module structure $A \otimes _{\Bbb Z} B$ satisfying $r (a \otimes b) = ra \otimes b$ . When $R= \Bbb Q$ , we get the additional fact that $a \otimes rb=ra \otimes b$ . To see this,  note that when $r \in \Bbb N$ , we have $$ r a \otimes b = \sum a \otimes b = a \otimes rb $$ by bilinearity  - which extends $r$ to $\Bbb Z$ too. When $r=1/m$ , $m \in \Bbb Z$ , $$ \frac{1}{m}a \otimes b = \frac{1}{m} (a \otimes b) = \frac{1}{m} ( \sum (a \otimes \frac{1}{m} b)) = \frac{1}{m} (ma \otimes \frac{1}{m} b ) = a \otimes \frac{1}{m} b $$ Thus, we have equality for all $r \in \Bbb Q$ . I think generalizing to $\Bbb Q$ is as far as we can get for this naive strategy. I wonder if there exists better method for the classification.","['abstract-algebra', 'tensor-products']"
3017005,"Given $n$ integers, is it always possible to choose $m$ from them so that their sum is a multiple of $m$?","The original question: given $6666$ integers, (positive, negative or $0$ ) is it always possible to choose $2018$ from them so that the chosen numbers add to a multiple of $2018$ ? (positive multiple, negative multiple or $0$ ) Prove or disprove. More generally, for what $n$ and $m$ , given $n$ integers, it is always possible to choose $m$ from them so that their sum is a multiple of $m$ ? I tried for small $m$ s to find a minimum $n$ that fits, and my research shows (in $n$ for $m$ form): $1$ for $1$ , $3$ for $2$ , $5$ for $3$ , $7$ for $4$ , $9$ for $5$ , $11$ for $6$ , $13$ for $7$ , $15$ for $8$ . My program takes already quite long for the last case ( $15$ for $8$ ), so I terminated it. My guess from these cases is that $2m - 1$ is the least fit for $m$ , and therefore the original question is true since $6666 > 4035 = 2 \cdot 2018 - 1$ , but I apparently have no idea about the proof. Disclaimer: this is not homework. I fail to do it cuz I'm bad.",['algebra-precalculus']
3017051,Prove that the function is bijective,"A function $f:\mathbb{R}\to\mathbb{R}$ is defined by, $$f(x)={x+\arctan(x)}.$$ Prove that this function is bijective. Here's my attempt:-
Since $x$ and $\arctan(x)$ are continuous, $f(x)={x+\arctan(x)}$ is also continuous. And further, $$ f'(x) = 1+ \frac1{1+x^2}\gt 0 \quad\forall x \in\mathbb{R}$$ Therefore the function is strictly increasing and hence, is strictly monotonic. Therefore this function is bijective. Would this be correct? Can you show me a better way (that doesn't involve derivative) of proving this? Thank you!","['calculus', 'functions']"
3017125,"How to prove $f\equiv 0$ $\forall x\in [a,b]$?$\quad$($f''＋pf'＋qf＝0$ with $f(a)＝f(b)＝0$)","Define $f \in C^{2}[a,b]$ satisfying $f''＋pf'＋qf＝0$ with $f(a)＝f(b)＝0$ , where $p\in C^{0}[a,b]$ and $q\in C^{0}[a,b]$ are two functions. If $q\leq0$ , can we prove $f\equiv 0$ $\forall x\in [a,b]$ ? My try: If $f\not\equiv 0$ , without loss of generality, we assume that the maximum of $f$ on $[a,b]$ is greater than zero, while notating $f(x_0)＝\displaystyle\max_{[a,b]} f$ . Then we have $f(x_0) > 0$ , $f'(x_0) ＝ 0$ and $f''(x_0) \leq 0$ . I figured out that if we alter the condition $q\leq0$ into $q(x)<0$ there evidently exists contradiction. But how to analyze further with the condition $q\leq0$ ? Can we still find contradiction if $q(x_0)＝0$ and $f''(x_0)＝0$ ? Any ideas would be highy appreciated!","['continuity', 'ordinary-differential-equations', 'real-analysis']"
3017150,Group of orientation preserving homeomorphisms of circle $S^1$ acts transitively on the set of closed intervals of $S^1.$,"The closed intervals here mean the arcs including endpoints on the circle. I tried to do it by taking the inverse image of those two closed intervals from $S^1$ to its covering space $\mathbb{R}$ and then constructed a homeomorphism of $\mathbb{R}$ that takes these closed intervals to each other. But the issue is that while projecting it back to $S^1$ , I am unable to ensure that this will result in a homeomorphism. Any other approach is also welcome.","['geometric-topology', 'general-topology', 'differential-geometry']"
3017238,"If $F(x,y)=\int_{f(x,y)}^{g(x,y)}h(x,y,t)\, dt$, then find $\partial F/\partial x$.","Let $f,g:\mathbb{R}^2\to \mathbb{R}$ and $h:\mathbb{R}^3\to\mathbb{R}$ be $C^1$ , define $F(x,y)=\int_{f(x,y)}^{g(x,y)}h(x,y,t)\, dt$ . Find $\frac{\partial F}{\partial x}$ . So I split $$F(x,y)=\int_0^{g(x,y)}h(x,y,t)\, dt-\int_0^{f(x,y)}h(x,y,t)\, dt.$$ And then using FTC I believe I have $$F(x,y)=H(x,y,g(x,y))-H(x,y,0)-H(x,y,f(x,y))+H(x,y,0)=H(x,y,g(x,y))-H(x,y,f(x,y))$$ Then differentiating this new thing with respect to x. $$\frac{\partial F}{\partial x}=\frac{\partial H}{\partial x}+\frac{\partial H}{\partial y}\frac{\partial y}{\partial x}+\frac{\partial H}{\partial g(x,y)}\frac{\partial g(x,y)}{\partial x}-\frac{\partial H}{\partial x}-\frac{\partial H}{\partial y}\frac{\partial y}{\partial x}-\frac{\partial H}{\partial f(x,y)}\frac{\partial f(x,y)}{\partial x}$$ Is this correct? From here I would simplify the expression and expand out the partials. But I'm not sure that the FTC works in this way for $H$ , since it's a function of 3 variables.","['multivariable-calculus', 'proof-verification']"
3017296,Intuition of a Function as an Infinite Dimensional vector,"Functions as infinite dimensional vectors make sense intuitively after studying much linear algebra, but I only recently realized I've been taught two ways of thinking about them. For a one dimensional function $f(x)$ you can expand in the basis of polynomials (a taylor series) and you get an infinite dimensional vector where the $i$ th component is attached to the basis $x^{i}$ , or you can think of the function as a list of all its values over its domain e.g. $ f = ...f(-0.01),f(0),f(0.01)... $ where 0.01 goes to 0. In the former case, differentiation and integration become matrix multiplication, which is a nice property, and the latter case makes things like the inner product of functions feel a lot more natural. But in the former case the length of the vector is the cardinality of the natural numbers, as opposed to the latter where it has the size of the reals, so these clearly aren't equivalent representations. Is there any relationship between these two ways of thinking of a function as an infinite dimensional vector?","['functions', 'vectors']"
3017322,"How to select distribution? — Binomial, Poisson, ...","How do I go about finding which distribution I need to use for my exercise? I have the following exercise: Compute the probability that within a group of 5 students exactly two
  are born on a Sunday. What gives me a hint on what probability distribution that is?","['probability-distributions', 'probability-theory', 'probability']"
3017324,Strictly increasing function from reals to reals which is never an algebraic number,"Let $f:\Bbb R\rightarrow\Bbb R$ have the properties $\forall x,y\in\Bbb R,\space x<y\implies f(x)<f(y)$ and $\forall x\in\Bbb R,\space f(x)\notin\Bbb A$ where $\Bbb A$ is the set of algebraic numbers; i.e. $f$ is strictly increasing, but nowhere is $f(x)$ algebraic. Does such a function exist? And if so, can one be explicitly constructed? My thoughts are that such a function should exist, since the algebraic numbers are ""small"" compared to the reals; we can show that a bijection (or more weakly an injection) must exist from $\Bbb R$ to $\Bbb R\backslash\Bbb A$ because they have the same cardinality, but I'm not entirely sure how to show rigorously that a strictly increasing function exists, even if in principle this is just a special type of injection. Replacing $\Bbb A$ by a set such as $\Bbb Z$ in the definition makes the question trivial, and these sets have the same cardinality, so clearly the difficulty arises because $\Bbb A$ is dense in the reals - any hints or answers would be appreciated.","['order-theory', 'algebraic-numbers', 'real-analysis']"
3017340,"If $f$ is a complex polynomial such that $f(z) \in \mathbb{R}$ whenever $|z| = 1$, then $f$ is constant. [duplicate]","This question already has answers here : If a function $f$ is holomorphic on the closed unit disk centered at the origin and is real valued whenever $|z| = 1$, then $f$ is constant. (2 answers) Closed 4 years ago . Let $f \in \mathbb{C}[z]$ a complex polynomial such that $f(S^1) \subset \mathbb{R},$ where $$
S^1 = \{ z \in \mathbb{C} : |z| = 1 \}.
$$ Show that $f$ is constant. My attempt is: define a function $g(z) = e^{if(z)}$ and plug some roots of unity in $g,$ but how can I conclude from this?
I know that if $\Omega$ is open, connected and $f(\Omega) \subset \mathbb{R},$ then $f$ is constant.","['complex-analysis', 'polynomials']"
3017422,Damped vibrations of a membrane stretched over a circular frame,"I am given this following PDE with the initial and boundary conditions with $0 < r < 1$ , $t > 0$ , and $v_0$ being a constant: $u,_t,_t + 2bu,_t = u,_r,_r + \frac{1}{r} u,_r$ $u(t,r=0) = 0, \vert u(t,r=0)\vert < \infty$ $u(t=0,r)=0, u,_t(t=0,r) = v_0$ Given this information I am suppose to obtain the following solution:: $$u(t,r) = 2v_0e^{-bt}\sum_{i=0}^\infty \frac{J_0(z_{0n}r)}{z_{0n}J_1(z_{0n})} \frac{\sin(t\sqrt{z_{0n}^2 -b^2})}{z_{0n}^2 -b^2}$$ I am at a loss on how to start this problem. My best guess would be use to separation of variables in which case I would get the following: $u(t,r)=h(t)*f(r)$ $\frac{h''}{h'} + \frac{2bh'}{h} = \frac{f''}{f} + \frac{f'}{rf} = -\lambda$ With the separated ODEs: $h'' + 2bh' +h\lambda = 0$ $f'' + f' + fr\lambda$ = 0 Question: Am I going in the right direction to obtain the given solution and if not what am I missing?","['orthogonality', 'derivatives', 'bessel-functions', 'partial-differential-equations']"
3017475,Is there a closed form for the sum of the cubes of the binomial coefficients?,"We know that $$
\sum_{k=0}^n \binom{n}{k} = 2^n\;\; \text{ and }\;\;
\sum_{k=0}^n \binom{n}{k}^2 = \binom{2n}{n}
$$ hold for all $n\in \mathbb{N}_0$ .
Now I tried to find a similar expression for $$
\sum_{k=0}^n \binom{n}{k}^3
$$ but didn't get anywhere at all. What I found were only asymptotic estimates (see Sum of cubes of binomial coefficients or Asymptotics of $\sum_{k=0}^{n} {\binom n k}^a$ ). Now is there a closed form for this sum or, what would be even better, for $\sum_{k=0}^n \binom{n}{k}^\alpha$ with any $\alpha \in \mathbb{N}_0$ ?","['binomial-coefficients', 'combinatorics', 'closed-form']"
3017481,Can every orthogonal matrix be written as a product of Givens rotations?,"I'd like to know whether every orthogonal matrix $$ A \in \mathcal{O}_n(\mathbb{R})$$ can be written as a product of givens-rotations. I know that when we do QR-decomposition of matrix $A$ we get $$ A = Q R $$ So my idea was to prove that $R$ must be the identity $I_n$ , however I'm stuck at that. Can somebody give me a hint on how I could prove this?","['matrices', 'orthogonal-matrices', 'matrix-decomposition']"
3017493,Help:For what $ \alpha $ do the integrals exist?,"Let $ K_R (y) \subset \mathbb{R}^n $ be  an n-dimensional Ball with radius $R>0$ and midpoint $y \in \mathbb{R}^n $ Far what $ \alpha \in \mathbb{R} $ and $ n \in \mathbb{N}\{0\} $ do following integrals exist: $ \int_{K{_R(y)}} |x-y|^{\alpha}dx $ $ \int_{ \mathbb{R}^n \setminus K{_R(y)}} |x-y|^{\alpha}dx $ $ \int_{ \mathbb{R}^n } |x-y|^{\alpha}dx $ with what criteria can i solve this task?
I don't know how to begin.
Thanks for any help!! Let's take $ \int_{K{_R(y)}} |x-y|^{\alpha}dx $ , $y=0, n=2 $ If I replace by polarcoordinates $ x= rsin \phi $ : $ \int_0^{2 \pi} \int_0^R | r|^{\alpha} r dr d\phi $ ..is this the right way?","['integration', 'multivariable-calculus', 'real-analysis']"
3017507,"Why is $\operatorname{Tr}([A,B]^{m}) = \operatorname{Tr}([AB, [A,B]^{m-1}])$, if [A,[A,B]]=0$?","Why is $\operatorname{Tr}([A,B]^{m}) = \operatorname{Tr}([AB, [A,B]^{m-1}])$ , where $[A,B] = AB-BA$ for two quadratic matrices $A,B$ with $ [A,[A,B]]=0$ and $Tr$ is the trace of a matrix? I tried to rewrite this and reduce it to $\operatorname{Tr}([A,B]^{m-1} AB) = \operatorname{Tr}(BA [A,B]^{m-1} ) $ the following way: $$[A,B]^m = (AB-BA) [A,B]^{m-1} $$ $$[AB,[A,B]^{m-1}] = AB [A,B]^{m-1} - [A,B]^{m-1} AB$$ But now I do not see how to use $[A,[A,B]] = 0$ . Does anyone has hints for this or a hint how to advance?","['matrices', 'linear-algebra']"
3017511,Find the orthogonal projection of a function on the set of $L^2$ functions whose integrals are $0$,"Given $u\in L^2(0, 1)$ , find its orthogonal projection on $V =\{v∈L^2(0,1):\int_0^1 v(x)\ dx=0\}$ . For Hilbert spaces it holds a theorem about projections on a closed convex set which states that given a subset $C$ of an Hilber space $H$ , $u\in H$ , $v\in C$ , then $\langle u-v,g-v\rangle\le0,\ \forall\ g\in C$ , and $v$ is the orthogonal projecition of $u$ in $C$ . $V$ is closed if every sequence $\{v_n\}\subset V$ converges to an element of $V$ , i.e. if $\forall\ \phi(x)\in L^2(0,1):\lim_{n\to+\infty}\int_0^1v_n\phi=\int_0^1v\phi$ (weak convergence). Take $\phi=\chi_{(0,1)}$ , then $\int_0^1v_n\phi=\int_0^1v_n=0$ and $\lim_{n\to+\infty}\int_0^1v_n=\int_0^1v=0$ . Then $V$ is closed. $V$ is convex if every convex combination of two elements of $V$ is still an element of $V$ , but I don't know how to check this. Another theorem about Hilbert spaces says that every Hilbert space splits into the direct sum of any closed subspace and its orthogonal. Statement: given a Hilbert space $X$ , a closed subspace $Y$ and the map $p$ which sends any element $x\in X$ to its closest element in $Y$ , then $p$ is linear, continuous, and $x-p(x)$ is orthogonal to $Y$ . Moreover, if $\{e_1,...,e_n\}$ is an orthonormal basis of $Y$ , then $p(x)=\sum_1^n\langle x,e_i\rangle e_i$ . I read that examples of orthonormal basis for $L^2[0,1]$ are $\{e^{2πinx}\}$ , for $n$ from $-\infty$ to $+\infty$ , and the Legendre polynomials. But these two basis have infinite dimension and so cannot be used to compute $p(x)$ , right? Any hint on how to start the exercise?","['hilbert-spaces', 'projection', 'functional-analysis']"
3017514,derivative of function and fundamental theorem of calculus,"Let $f\colon[a,b]\to\mathbb{R}$ be differentiable on $(a,b)$ . Suppose that the limits $f(a+)=\lim_{x\to a+}f(x)$ and $f(b-)=\lim_{x\to b-}f(x)$ exist and are finite. My question is: Do we have $$\int_{a}^{b}f'(x)dx=f(b-)-f(a+)$$ without further assumption on $f$ ? If yes, what would be a reference for this result? If no, is there a counterexample for this? Any help is highly appreciated.","['integration', 'calculus', 'derivatives', 'real-analysis']"
3017623,Can a noncompact metric space have a maximal metrizable Hausdorff compactification?,"We know the Stone-Čech compactification $(h, \beta X)$ of a Tychonoff space $X$ is its largest (in particular, a maximal) Hausdorff compactification, in the sense that if $(k,\gamma X$ ) is any other Hausdorff compactification, then there is a continuous map $f:\beta X 
 \rightarrow \gamma X$ such that $fh=k$ . What about if I restrict to considering just all metrizable Hausdorff compactifications? To fix ideas, say $X$ is a noncompact metric space. (In particular $\beta X$ will not be metrizable.)
Is there a maximal metrizable Hausdorff compactification of $X$ ? I am guessing no: if we're given any metrizable Hausdorff compactification of noncompact metric space $X$ , we can make another metrizable Hausdorff compactification that is larger (in the sense of the first paragraph above).","['general-topology', 'compactification', 'metric-spaces', 'compactness']"
3017625,Let $R$ be a finitely generated subring of a number field. Is $R/I$ finite for every non-zero ideal of $R$?,"Given any finitely generated subring $R$ of a number field (finite extension of $\mathbb{Q}$ ) or a global function field (finite extension of $\mathbb{F}_p(T)$ ), does $R$ have the property that $R/I$ is a finite ring for every non-zero ideal $I$ of $R$ ? It is sufficient to show that $R$ has Krull dimension $1$ . Indeed, then $R/I$ is a finitely generated artinian ring, which is finite.","['number-theory', 'finite-rings', 'krull-dimension', 'finitely-generated', 'commutative-algebra']"
3017629,Variance in linear combinations,"I am having a problem understanding why the below is calculating the variance when the exercise is asking about the standard deviation. The standard deviation of the time you take for each statistics homework problem is 1.5 minutes, and it is 2 minutes for each chemistry problem. What is the standard deviation of the time you expect to spend on statistics and physics homework for the week if you have 5 statistics and 4 chemistry homework problems assigned? Variability of a linear combination of two independent random variables: V(aX + bY) = $a^2$ ×V(X) + $b^2$ ×V(Y) The standard deviation of the linear combination is the square root of the variance. So we will use the above formula to calculate the exercise: V(5S + 4C) = $5^2$ × V(S) + $4^2$ × V(C) = 25 × $1.5^2$ + 16 × $2^2$ = 56.25 + 64 = 120.25 In my mind we are clearly finding the variance here, not the standard deviation, as that would be the sqrt of the 120.25, still the answer is 120.25, why? Edit: Also, the formula says that $a^2$ and $b^2$ but still they do power of two on both 5 and 4 and V(S) and V(C).","['statistics', 'variance', 'standard-deviation']"
3017644,"Weak analogues of gradient, divergence, and curl (collecting examples)","This question is mostly to help me understand the idea behind the ""weak curl"", but I also hope to accomplish other objectives with this question/post as well, partially inspired from some of the ""proof collecting"" posts I've seen. $1)$ I want to better understand the notion of ""weak curl"" with some examples. $2)$ To hopefully discuss theorems surrounding when the weak versions of gradient, divergence, and curl (if possible), are equal to their strong counterparts and what this implies for solutions for PDEs. $3)$ Collect illustrative examples of weak gradient, weak curl, and weak divergence in any number of dimensions or subsets. Maybe we can consider compact vs. non-compact subsets, upper/lower bounds on these quantities, disconnected spaces, and any related topic of interest. WEAK GRADIENT : let $\Omega\subset \mathbb{R}^n$ , and let $u\in L^1_{loc}(\Omega)$ and $\phi\in C^{\infty}_c(\Omega)$ . The function $v$ is called the ""weak gradient"" of $u$ if $\int_{\Omega}u\phi' d\mu=-\int_{\Omega}v\phi d\mu$ . The ""a-th"" weak gradient is just $\int_{\Omega}uD^{a}\phi d\mu=-(1)^a\int_{\Omega}v\phi d\mu$ $\forall \phi \in C^{\infty}_c(\Omega)$ WEAK DIVERGENCE: v is called the ""weak divergence"" for $u\in L^2(\Omega)$ if we have $\int_{\Omega}u\phi d\mu=-\int_{\Omega}\langle v, \nabla \phi \rangle$ $\forall \phi \in C^{\infty}_c(\Omega)$ EDIT : I also realized I need clarification on the notation $\int_{\Omega}(v,\nabla \phi)$ . I think this means we integrate w.r.t. each vector component of $\nabla \phi$ , so for $\mathbb{R}^2$ we have $\int_{\mathbb{R}}\int_{\mathbb{R}}|v_1 \nabla \phi_1 v_2 \nabla \phi_2|^2 d\mu d\lambda$ . WEAK CURL: This is a bit more delicate, and I am only aware of the definition where $\Omega\subset \mathbb{R}^3$ . First, we need to define $u:\mathbb{R}^2\rightarrow \mathbb{R}^2$ with $u=(u_0, u_1)$ where $u_0\in L^2(\Omega)$ , and $u_1\in L^2(\partial \Omega)$ . Let $n$ be a normal vector to the boundary $\partial \Omega$ . $v$ is called the ""weak curl"" if we have: $v=curl(u)=\langle u_0, (\nabla \times \phi) \rangle + \langle u_1 \times n, \phi \rangle $ $\forall \phi \in C^{\infty}_c(\Omega)$ where the inner product here is the standard $L^2$ inner product. Any interesting remarks/theorems are also welcome.","['vector-fields', 'real-analysis']"
3017649,Positivity of Currents,"I already asked about this a couple of weeks ago but had introduced some rather annoying notation. I decided to reformulate the question in a more compact format. (edit: old post taken down as it is basically a duplicate now) Suppose $\psi$ is the complex differential $(1,1)$ -form $$\psi = i \sum_{j,k=1}^n h_{j,k} ~dz_j \wedge d\bar{z}_k$$ and that the matrix $H$ with entries $h_{j,k}$ is hermitian (i.e. $\psi$ is a real form). In a lecture of mine, it was given as an exercise that $\psi$ is positive (i.e. $\psi(v,Jv) > 0$ for all $v \ne 0$ , where $J$ is the canonical almost complex structure) if and only if $H$ is positive-definite. This differential form induces a current by integration, $$T_{\psi} ~\colon ~ \Omega^{n-1,n-1}_c(\mathbb{C^n}) \rightarrow \mathbb{C}, ~ \phi \mapsto \int_{\mathbb{C^n}} \psi \wedge \phi.$$ In Harris and Griffiths' textbook ""Principles of Algebraic Geometry"" (on page 386 with $p=1$ ), a real $(1,1)$ -current is defined to be positive if for every $(n-1,0)$ -form $\eta$ we have that $T(\eta \wedge \bar{\eta})$ is a non-negative real number. Take such a test form $$\eta = \sum_{j=1}^n \phi_j ~dz_J,$$ where $dz_J$ denotes $dz_1 \wedge \dots \wedge \hat{dz_j} \wedge \dots \wedge dz_n$ . Then we have $$\eta \wedge \bar{\eta} = \sum_{j,k} \phi_j \overline{\phi_k} ~dz_J \wedge d\bar{z}_K$$ and \begin{align*}
T_{\psi}(\eta \wedge \bar{\eta}) &= i \sum_{j,k} \int h_{j,k} \phi_j \overline{\phi_k} ~dz_j \wedge d\bar{z}_k \wedge dz_J \wedge d\bar{z}_K \\
&= i \sum_{j,k} \int h_{j,k} \phi_j \overline{\phi_k} ~\sigma(j,k) ~dz_1 \wedge d\bar{z}_1 \wedge \dots \wedge dz_n \wedge d\bar{z}_n,
\end{align*} where $\sigma(j,k)$ denotes the sign coming from $$dz_j \wedge d\bar{z}_k \wedge dz_J \wedge d\bar{z}_K = \sigma(j,k) ~dz_1 \wedge d\bar{z_1} \wedge \dots \wedge dz_n \wedge d\bar{z}_n.$$ Thus, the current $T_{\psi}$ is positive not if $H$ is positive definite but if the matrix with entries $\sigma(j,k) h_{j,k}$ is positive definite. Is it true that positivity of currents and positivity of differential forms do not give rise to the same notion? Can this be fixed?","['complex-analysis', 'complex-geometry', 'differential-topology', 'algebraic-geometry']"
3017691,Quadratic Formula With Independent and Dependent Variables,"Given the differential equation $dy/dt = (y + t)^2$ , we can apply the u-substitution $u = y + t$ to arrive at the separable differential equation $du/dt = u^2 + 1$ .  This separates to $1/(u^2 + 1)\ du = dt$ which integrates to (EDIT:  As LutzL has pointed out, I integrated incorrectly.  However, correcting it would eclipse potentially interesting part of the question, so I'll leave the mistake) $u^2 + 1 - Ce^t = 0$ .  Reverting the substitution yields $y^2 + 2ty + t^2 + 1 - Ce^t = 0$ .  Note that $y$ is a dependent variable, $t$ is the independent variable, and $C$ is an arbitrary constant. Is it legal to proceed via the quadratic formula, using the appropriate expressions in terms of $t$ as coefficients?  This would look like $y = (-(2t) ± \sqrt{(2t)^2 - 4(1)(t^2 + 1 - Ce^t)})\ /\ 2(1)$ , which works out to $y = -t ± \sqrt{Ce^t - 1}$ .  However, this practice feels a bit suspect, since in other instances of applying the quadratic formula, there is no dependency between the variable and its coefficients, whereas here there is.  Is this a legal and correct approach to the problem? Secondly, suppose a similar problem yielded $y^2 + 2ty + t^2 + 1 − Ce^y = 0$ , where $y$ is still a dependent variable, $t$ is still the independent variable, and $C$ is still an arbitrary constant.  Would it be legal to solve for $t$ using the quadratic formula using the appropriate expressions in terms of $y$ as coefficients?","['constants', 'independence', 'ordinary-differential-equations']"
3017733,Find probability of $P(X Y < 1)$,"Let $X$ and $Y$ be two independent $\mathrm{Uniform}(0,2)$ random variables. Find $P(XY < 1)$ I started off by finding the pdf $f_X(x)=\frac{1}{2} $ when $0<x<2$ Same for $Y$ . I then found their joint PDF via independence: $f_{XY}(xy)=\frac{1}{4} $ when $0\leq y \leq 2 $ and $0 \leq x \leq 2   $ Otherwise $0$ $\int_0^2 \int_0^{1/y} \frac{1}{4} dx dy$ But this cannot be solved, so where did i go wrong?","['probability-distributions', 'uniform-distribution', 'probability-theory', 'probability']"
3017744,"Difference between backslash and minus on domain, range","When declaring domain or range, is "" $R\setminus\{0\}$ "" any different than "" $R-\{0\}$ ""?","['elementary-set-theory', 'notation']"
3017814,$L^p$-space is a Hilbert space if and only if $p=2$,"Inspired by $\ell_p$ is Hilbert if and only if $p=2$ , I try to prove that a $L^p$ -space (provided with the standard norm) is a Hilbert space if and only if $p=2$ . I already know that every $L^p$ -space is a Banach space with respect to the standard norm. This is what I got so far. To prove Let $(S, \Sigma, \mu)$ be a measure space and assume that $\mu$ is a positive, $\sigma$ -finite measure that is not the trivial measure. Let $p\in [1, +\infty]$ . The normed space $(L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} )$ is a Hilbert space if and only if $p=2$ . Proof Case 1: $p=2$ The standard inner product $\langle \cdot , \cdot \rangle _{L^2}$ induces the standard norm $\| \cdot \| _{L^2}$ , so $(L^2 (S, \Sigma , \mu), \| \cdot \| _{L^2})$ is a Hilbert space. Case 2: $p\in [1, \infty) \backslash \{ 2 \}$ Assume $(L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} ) $ is a Hilbert space. Then it is also an inner product space, so it must satisfy the parallelogram rule: \begin{align}
\forall f,g \in L^p(S,\Sigma , \mu): \ \ \| f + g \| ^2 + \| f -g \| ^2 = 2 ( \| f \| ^2 + \| g \| ^2 ). 
\end{align} Let $A, B$ be disjoint measurable sets such that $0 < \mu (A), \ \mu (B) < \infty$ . Define $f_p, g_p \in 
\mathcal{L} ^p (S, \Sigma, \mu)$ by \begin{align*} f_p  := \frac{1}{(\mu (A) ) ^{1/p}} 1 _A \geq 0 \ \ \ \text{ and } \ \ \ g_p  := \frac{1}{(\mu (B) ) ^{1/p}} 1 _B  \geq 0.\end{align*} Doing some calculations gives us \begin{align*} 2 \left ( \| f_p \| _{L^p} ^2  + \| g_p \| _{L^p} ^2 \right ) = 4 \ \ \text{ and } \ \ \| f_p + g_p \| _{L^p} ^2 + \| f_p - g_p \| _{L^p} ^2 = 2 \cdot 2 ^{2/p}. \end{align*} The functions $f_p$ and $g_p$ do not satisfy the parallelogram rule, since $4\neq 2 \cdot 2 ^{2/p}$ (remember that $p\neq 2$ ). But this contradicts our earlier conclusion that all $f,g\in L^p (S, \Sigma , \mu)$ must satisfy the parallelogram rule. Therefore, our assumption that $(L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} ) $ is a Hilbert space is wrong. Hence, $(L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} ) $ is not a Hilbert space. Case 3: $p=\infty$ Assume $(L^{\infty} (S,\Sigma , \mu), \| \cdot \| _{L^{\infty}} ) $ is a Hilbert space. Then it is also an inner product space, so it must satisfy the parallelogram rule. Let $A$ and $B$ be disjoint measurable sets which are not null sets and consider $F=1 _A$ and $G=1_B$ . Then, we have \begin{align*}
\| F + G \| _{L^{\infty}} ^2 + \| F - G \| _{L^{\infty}} ^2 = 1 + 1 = 2 \neq 4 = 2(1+1) = 2( \| F \| _{L^{\infty}} ^2 + \| G \| _{L^{\infty}} ^2 ).
\end{align*} But this contradicts our earlier conclusion that all $f,g\in L^{\infty} (S, \Sigma , \mu)$ must satisfy the parallelogram rule. Therefore, our assumption that $(L^{\infty} (S,\Sigma , \mu), \| \cdot \| _{L^{\infty}} ) $ is a Hilbert space is wrong. Hence, $(L^{\infty} (S,\Sigma , \mu), \| \cdot \| _{L^{\infty}} ) $ is not a Hilbert space. Question I am not completely sure about the following statements ""Let $A, B$ be disjoint measurable sets such that $0 < \mu (A), \ \mu (B) < \infty$ "" and ""Let $A$ and $B$ be disjoint measurable sets which are not null sets"". How do I know for sure that such measurable sets $A,B$ actually exist? Should I make more assumptions about the measure space in order to make these arguments work?","['banach-spaces', 'measure-theory', 'real-analysis', 'hilbert-spaces', 'functional-analysis']"
3017872,Integral identity involving Bernoulli polynomials,"I found the following identity on Wikipedia, and I am having a difficult time proving it. For $m,n\in\Bbb N$ , $$I(m,n):=\int_0^1B_n(x)B_m(x)\mathrm{d}x=(-1)^{n-1}\frac{m!n!}{(m+n)!}b_{n+m}$$ Where $B_n(x)$ is the $n$ -th Bernoulli polynomial, and $b_n=B_n(0)$ is the $n$ -th Bernoulli number. Notably, when one uses $x!=\Gamma(x+1)$ on the RHS and then uses the Beta function, we arrive at $$I(m,n)=(-1)^{n-1}b_{n+m}\int_0^1t^n(1-t)^m\mathrm{d}t$$ Here's what I've tried. Because $B_n(x)$ satisfies $$B_n'(x)=nB_{n-1}(x)$$ We can integrate by parts with $\mathrm{d}v=B_n(x)\mathrm{d}x$ : $$I(m,n)=\frac1{n+1}B_{n+1}(x)B_m(x)\bigg|_0^1-\frac m{n+1}\int_0^1B_{n+1}(x)B_{m-1}(x)\mathrm{d}x$$ $$I(m,n)=\frac1{n+1}\bigg(B_{n+1}(1)B_m(1)-b_{n+1}b_m\bigg)-\frac{m}{n+1}I(m-1,n+1)$$ Which is a start, but I don't know how to proceed. I would think that the integral should be easy given the fact that Bernoulli polynomials have so many identities, yet I can't seem to find the right one to use. Could you either point me in the right direction, or show me a complete proof? Thanks.","['integration', 'number-theory', 'bernoulli-polynomials', 'combinatorics', 'polynomials']"
3017916,Bias Variance Decomposition for KL Divergence,"While studying a slide deck, I encountered the following exercise: We are to give a bias variance decomposition when the prediction is given as a probability distribution over $C$ classes.
Let $P = [P_1, . . . , P_C ]$ be the ground truth class distribution associated to a particular input pattern. Assume the random estimator of class probabilities $$\bar{{P}} = [\bar{{P}}_1, . . . , \bar{{P}}_C ] $$ for the same input pattern. The error function is given by
the KL-divergence between the ground truth and the estimated probability distribution: $$\text{Error} = E[D_{KL}(P||\bar{{P}})]$$ First, we would like to determine the mean of the class distribution estimator $\bar{{P}}$ . We define the mean as the distribution
that minimizes its expected KL divergence from the class distribution estimator, that is, the distribution $R$ that optimizes $$\begin{matrix}\min\\R \end{matrix} \space E[ D_{KL}(R||\bar{{P}})]$$ I have found a way to proof that this is: $$ R = [R_1, . . . , R_C ] $$ $$ \text{where} \space\space R_i = \frac{{\exp\space E[\log \bar{P_i} ]}}{{\sum_j\exp\space E[\log \bar{P_j}]}}  \space \space \space ∀ \space 1 ≤ i ≤ C.$$ We are now asked to proof that: $$ Error(\hat{P}) = Bias(\hat{P}) + Var(\hat{P}) $$ where $$Error(\hat{P}) = E[D_{KL}(P || \hat{P})$$ $$Bias(\hat{P}) = D_{KL}(P || R)$$ $$Var(\hat{P}) = E[D_{KL}(R || \hat{P})$$ I started by writing out the KL Divergence: $$Bias(\hat{P}) + Var(\hat{P}) = \sum_{i=1}^{C} \left (P_i \log \left (\frac{P_i}{R_i} \right ) \right) + E \left [ \sum_{i=1}^{c} \left (R_i \log \left (\frac{R_i}{\hat{P}_i} \right ) \right ) \right ]$$ But I don't know how to continue from here on.","['statistics', 'variance']"
3017918,$\lim_{x \to \infty} e^x - \frac{e^x}{x+1}$ Application of L'Hopital's Rule,"I want to know if I can ""use"" a limit after I've used L'hopital's rule on it? I'm not sure how to better word it, but I can show you what I tried, maybe you can tell me if it is right or why it is wrong. $$\lim_{x \to \infty} e^x - \frac{e^x}{x+1}$$ We can split this into two limits $$\lim_{x \to \infty} e^x - \lim_{x \to \infty} \frac{e^x}{x+1}$$ Now since the limit on the right side is infinity over infinity, we can apply L'Hopital's rule $$\lim_{x \to \infty} e^x - \lim_{x \to \infty} \frac{e^x}{1}$$ Now we can join the two limits back (I am ""reusing"" the limit after applying L'hopital...is this allowed?) $$\lim_{x \to \infty} e^x - e^x$$ Subtracting we have $$\lim_{x \to \infty} 0 = 0$$","['limits', 'calculus', 'derivatives']"
3017962,Why Can L'Hôpital's Rule Not be Applied to the Sum or Difference of Limits?,Consider $$\lim_{x\to\infty}\frac{f(x)}{g(x)} + \lim_{x\to\infty}\frac{h(x)}{i(x)}$$ I was told that I cannot apply L'Hôpital's rule to each individual limit and then join the limits as $$\lim_{x\to\infty}\frac{f(x)}{g(x)} +\frac{h(x)}{i(x)}$$ Why is this incorrect?,"['limits', 'calculus']"
3017979,"About homeomorphisms on $[0,1]$","I need some help with the following: Suppose that $T:[0,1] \rightarrow [0,1]$ is an homemorphism, which satisfies that $T(0)=0$ . It is really intuitive that $$\int_0^1 |T(y) - y| dy = \int_0^1 |T^{-1}(y) - y| dy$$ since $T$ and $T^{-1}$ are symmetric with respect to the identity, and so the area between $T$ and identity will be the same as $T^{-1}$ and identity, which is the equality posted above. For me, it's intuitive but I can't get a proof of that. Also, what happend if we change the absolute value by square (change the $L_1$ norm for $L_2$ norm). Thanks a lot!","['measure-theory', 'lebesgue-integral', 'analysis', 'real-analysis', 'calculus']"
3018055,Analizing the stability of the equilibrium points of the system $\ddot{x}=(x-a)(x^2-a)$,"$\require{amsmath}$ $\DeclareMathOperator{\Tr}{Tr}$ $\DeclareMathOperator{\Det}{Det}$ Investigate the stability of the equilibrium points of the system $\ddot{x}=(x-a)(x^2-a)$ for all real values of the parameter $a$ . (Hints: It might help to
  graph the right-hand side. An alternative is to rewrite the equation as $\ddot{x}=−V′(x)$ for a suitable potential energy function $V$ and then use your intuition about particles
  moving in potentials.) I am not really sure on how to approach the problem with the given hints, since it would require plotting the graph for different critical values for $a$ , which I am not really sure how to find. Thus, I am wondering if the following is correct. The system $\ddot{x}=(x-a)(x^2-a)$ can be re-written as $$\begin{cases}
\dot{x}=y\\
\dot{y}=(x-a)(x^2-a)
\end{cases}$$ with fixed points $P_1(a,0),\,P_2(\sqrt{a},0)$ and $P_3(\sqrt{a},0)$ . The Jacobian is $$J(x,y)=\begin{bmatrix}
0 &1\\
3x^2-2ax-a &0
\end{bmatrix}$$ and thus $$J(a,0)=\begin{bmatrix}
0 &1\\
a^2-a &0
\end{bmatrix};\quad J(\sqrt{a},0)=\begin{bmatrix}
0 &1\\
2a-2a^{3/2} &0
\end{bmatrix}; \quad J(-\sqrt{a},0)=\begin{bmatrix}
0 &1\\
2a+2a^{3/2} &0
\end{bmatrix}.$$ It can be noticed that the $\Tr\left[J(x,y)\right]=0$ and that $$\begin{aligned}
&1.\,\Det\left[J(a,0)\right]=a(1-a)\implies \text{Saddle for }a<0 \wedge a>1, \text{Center for }0<a<1.\\
&2.\,\Det\left[J(\sqrt{a},0)\right]=2a(\sqrt{a}-1)\implies \text{Saddle for }0<a<1, \text{Center for }a>1.\\
&3.\,\Det\left[J(-\sqrt{a},0)\right]=-2a(\sqrt{a}+1)\implies \text{Saddle for }a>0.
\end{aligned}$$ If $a=0$ the system reduces to $\ddot{x}=x^3$ where the only fixed point is at $(0,0)$ and thus it is unstable. On the other hand, if $a=1$ then the system reduces to $\ddot{x}=x^3-x^2-x+1$ with fixed points at $(1,0),(-1,0)$ , both being unstable. Is my work correct?","['analysis', 'ordinary-differential-equations', 'dynamical-systems']"
3018140,Proving a Lipschitz function is continuous,A function $f:D\subset \mathbb R \to \mathbb R$ is lipschitz given that there exists a $L\gt0$ such that $|f(x)-f(y)|\le L|x-y|$ I need to prove this function is then continuous. Is there a best definition of continuous functions to use for this proof given the definition of Lipschitz? Is the epsilon delta way the only way to do it? can someone help me set it up? I really struggle with implementing that definition.,"['continuity', 'calculus', 'lipschitz-functions', 'real-analysis']"
3018164,Explain this derivative identity: $ \frac{1}{2^n} \frac{d^n}{dy^n} \frac{(1+y)^{2n+3}(1-y)}{((1+y)^2 -2yx)^2} \bigg|_{y=0} = (n+1)! x^n $,"I have the following result that I believe to be true: $$
\frac{1}{2^n} \frac{d^n}{dy^n} \frac{(1+y)^{2n+3}(1-y)}{((1+y)^2 -2yx)^2} \bigg|_{y=0} = (n+1)! x^n
$$ The LHS is something that arose in physics research. The RHS has been inferred by checking with Mathematica for n from 0 to 100. However, proving this result has evaded me. It's not surprising that the RHS is a polynomial in x with coefficient $(n+1)!$ (the only way to get an $x^n$ term is to operate the derivative on the denominator repeatedly). What is surprising is that the coefficients vanish for all but the leading term. This looks like it might be amenable to induction or recursion, but I was unable to make any meaningful headway with those techniques. Good luck!","['induction', 'derivatives', 'recursion']"
3018184,Condition for a morphism of schemes to factor through a closed subscheme,"Suppose $f\colon Y\to X$ is a morphism of schemes. Suppose $Z\hookrightarrow X$ is a closed subscheme defined by the quasi-coherent ideal $\mathcal J\subset \mathcal O_X$ . The Stacks Project claims that $f$ factors through the inclusion $Z\hookrightarrow X$ precisely if the morphism $f^\ast \mathcal J\to f^\ast \mathcal O_X=\mathcal O_Y$ is the zero morphism. As far as I can tell, this is equivalent to the condition that $\mathcal J\hookrightarrow \mathcal O_X\to f_\ast \mathcal O_Y$ is the zero morphism, since the adjunction should preserve the zero morphisms. However, this is never stated anywhere, which makes me suspicious, since I find my condition more straightforward than the one on the Stacks Project, so I don't know why it is not mentioned. I would very much appreciate if anybody is willing to verify (or falsify) my thoughts.","['algebraic-geometry', 'schemes']"
3018235,Infinitely differentiable functions with compact support are dense in $L^p$,"This problem is in Stein. In $L^p$ , $1\leq p<\infty$ on $\mathbb{R}^d$ with Lebesgue measure. (a) Continuous function with compact support are dense in $L^p$ .
I have already proves this :) (b) Infinitely differentiable functions with compact support are dense in $L^p$ . How proves (b)? I read that this hard...","['measure-theory', 'lebesgue-integral', 'real-analysis', 'lp-spaces', 'functional-analysis']"
3018238,Optimality of Kantorovich potentials for the squared distance,"This question comes from Villani's book, Optimal Transport: Old and New. Consider the cost function $c(x, y) = |x - y|^2$ on $X  \times Y$ , where $X$ is the right half of the unit ball, and $Y$ is the right half of the unit ball, shifted one unit to the right. Let $\mu$ be the uniform distribution on $X$ and $\nu$ the uniform distribution on $Y$ . The optimal transport map $T$ is given by $(x, y)  \mapsto (x + 1, y)$ ,  and by a theorem from chapter 10 we know that $T(x, y)  =  (x,y) + \nabla \psi(x, y)$ for some $c$ -convex function $\psi$ on $X$ . Since this  means that $\psi(x, y) = (1, 0)$ we have $\psi(x, y) = x$ . My question: Is this $\psi$ equal to the optimal $\psi$ in the Kantorovich dual problem? That is, is the following inequality  true: $$
\int_X c(x, Tx)\, d\mu(x) = \int_Y \psi^c \, d\nu - \int_X \psi\, d\mu,
$$ where $\psi^c(y) := \inf_x \psi(x) + c(x, y)$ .
I've tried computing the $c$ -transform and as far as I can tell the answer to my question is negative, since I am getting that $\partial_c \psi(x)$ is not the translate by 1, which it should be to guarantee optimality by Kantorovich duality. Any help is much appreciated!","['optimization', 'optimal-transport', 'analysis']"
3018294,Prove that $MN-NM$ is singular. [duplicate],"This question already has answers here : $A^2+B^2=AB$ and $BA-AB$ is non-singular [duplicate] (2 answers) Closed 5 years ago . Let $M$ and $N$ be square matrices such that $M^2+N^2=MN$ . Then prove that $MN-NM$ is singular. So basically I have to prove: $\det(MN-NM)=0$ . I tried to prove this by multiplying the given condition by the inverse of matrices $M$ and $N$ , but could not come to the answer. Could anyone please give a hint?",['matrices']
3018333,Find the limit of the expression $\lim_{x\to 0}\left(\frac{\sin x}{\arcsin x}\right)^{1/\ln(1+x^2)}$,"Limit: $\lim_{x\to 0}\left(\dfrac{\sin x}{\arcsin x}\right)^{1/\ln(1+x^2)}$ I have tried to do this: it is equal to $e^{\lim\frac{\log{\frac{\sin x}{\arcsin x}}}{\log(1+x^2)}}$ , but I can't calculate this with the help of l'Hopital rule or using Taylor series, because there is very complex and big derivatives, so I wish to find more easier way. $$\lim_{x\rightarrow 0}{\frac{\log{\frac{\sin x}{\arcsin x}}}{\log(1+x^2)}} = \lim_{x\rightarrow 0}\frac{\log1 + \frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)}}{\log(1+x^2)} = \lim_{x\rightarrow0}\frac{\frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)} + o(\frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)})}{x^2+o(x^2)}$$ using Taylor series. Now I think that it's not clear for me how to simplify $o\left(\frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)}\right)$ .","['limits', 'taylor-expansion', 'real-analysis']"
3018394,"linear isometric embedding from $(\mathbb{R}^2, \| \|_2)$ to $(l^1, \| \|_1)$","I would like to prove the following : There isn't a linear isometric embedding from $(\mathbb{R}^2, \| \cdot \|_2)$ to $(l^1, \| \cdot \|_1)$ I don't know how to prove this. So far I am able to prove this result only in the case where the vector $(1,0)$ and $(0,1)$ are sent to sequences that have all positive, or all negative value. In this case I use the fact that the $2$ norm is not linear whereas the $1$ norm is (ie $\| xa + yb \| = xa + yb$ , $x, y, a, b > 0$ ). The problem is that when the sequences have different signs i's hard for me to conclude. Thank you.","['normed-spaces', 'real-analysis', 'linear-algebra', 'functional-analysis', 'general-topology']"
3018414,Is $\Bbb Q / \Bbb Z$ discrete?,"I would like to say that $\Bbb Q / \Bbb Z$ is not discrete (when $\Bbb Q$ has euclidean topology), since $\Bbb Z \subset \Bbb Q$ is not open. But OTOH we have $$\Bbb Q / \Bbb Z \cong \bigoplus_p \Bbb Q_p / \Bbb Z_p$$ which is a direct sum of discrete groups, so it should be a discrete group. Maybe the issue is that the above isomorphism is only as abstract groups, but not as topological groups. Could anyone confirm/elaborate on this?","['general-topology', 'topological-groups']"
3018445,Composition with Lipschitz map is Lipschitz on Sobolev spaces,"Suppose that $F: \mathbb{R}^d \rightarrow \mathbb{R}^d$ is Lipschitz with some constant $L$ and that $F(0)=0$ . Then it is clear that $F$ defines a Lipschitz continuous map $L^2(\mathbb{R}^d) \rightarrow L^2(\mathbb{R}^d)$ by $u \mapsto F(u)$ with the same Lipschitz constant $L$ . We can extend this to Sobolev spaces $H^k(\mathbb{R}^d)$ when $k \in \mathbb{N}$ , by requiring in addition that $F \in C^k$ with the first $k$ derivatives of $F$ Lipschitz and that the first $k$ derivatives vanish at $0$ . For instance, if $k=1$ , we can compute \begin{align}
\| \partial_i(F(u)-F(\tilde{u})) \|_{L^2(\mathbb{R}^d)} &= \| \nabla F(u) \partial_i u - \nabla F(\tilde{u}) \partial_i \tilde{u} \|_{L^2} \\
&\leq \| \nabla F(u) \|_{L^2} \|\partial_i u - \partial_i \tilde{u} \|_{L^2} +  \|\partial_i \tilde{u} \|_{L^2} \|\nabla F(u) - \nabla F(\tilde{u}) \|_{L^2} \\
&\leq C \left( \|u\|_{L^2} \|u-\tilde{u} \|_{H^1}+\| \tilde{u} \|_{H^1} \| u- \tilde{u}\|_{L^2} \right) \\
&\leq C (\|u\|_{H^1} + \| \tilde{u} \|_{H^1}) \| u - \tilde{u} \|_{H^1}
\end{align} where we have used that $\nabla F$ is again a Lipschitz map (and I denoted all constants by $C$ ). Thus we obtain that the map $u \mapsto F(u)$ is (at least) locally Lipschitz in $H^1$ . Similarly of course for $H^k$ , when $k \in \mathbb{N}$ . Now here is my question: (how) is it possible to extend this result to Sobolev spaces $H^s$ with real index $s \in \mathbb{R}$ ? Is the map $u \mapsto F(u)$ well-defined and locally Lipschitz continuous on the space $H^s(\mathbb{R}^d)$ ? Is there some interpolation argument that easily accomplishes this? Finally, did I miss something, and is the map perhaps even globally Lipschitz?","['lipschitz-functions', 'analysis', 'fractional-sobolev-spaces', 'sobolev-spaces', 'functional-analysis']"
3018453,"Showing $\lim_{n \to \infty} L(f,P_n,[a,b]) = L(f,[a,b])$, where $P_n$ is partition of $[a,b]$ into $2^n$ subintervals","Suppose $f : [a,b] \to \mathbb{R}$ is bounded. With a partition $P$ of the form $a = x_0, \dots ,x_n = b$ of $[a,b]$ , the lower Riemann sum is $L(f,P,[a,b]) := \sum_{i=1}^{n} (x_i - x_{i-1}) \inf_{[x_{i-1},x_i]} f$ . Then the lower Riemann integral is $L(f,[a,b]) := \sup_{P} L(f,P,[a,b])$ ; that is, the lower Riemann integral is the supremum over all the lower Riemann sums. Define the sequence $L(f,P_n,[a,b])$ , where $P_n$ is the partition of $[a,b]$ obtained by splitting $[a,b]$ up into $2^n$ intervals of equal size. I want to prove that $\lim_{n \to \infty} L(f,P_n,[a,b]) = L(f,[a,b])$ . Here's my approach so far: Since the list defining partition $P_{n-1}$ is a sublist of the list defining the partition $P_n$ , we have $L(f, P_{n-1}, [a,b]) \leq L(f,P_n, [a,b])$ . That is, the sequence $L(f,P_n,[a,b])$ is monotone non-decreasing. Since it also has an upper bound of $L(f,[a,b])$ , it follows from the monotone convergence theorem that $L(f,P_n,[a,b])$ is a convergent sequence, and it converges to the least upper bound of its terms. I need to prove that this limit is equal to $L(f,[a,b])$ . This is where I am getting stuck. Let the limit of $L(f,P_n,[a,b])$ be $l$ . It is immediate that $l \leq L(f,[a,b])$ , because the supremum of a subset is at most the supremum of the original set. So I need just to show that $L(f,[a,b]) \leq l$ to complete the proof. It is equivalent to show that for all $\epsilon >0$ , we have $L(f,[a,b]) > l - \epsilon$ . To this end, let $\epsilon > 0$ . Since $L(f,[a,b])$ is the supremum of the lower Riemann sums, there exists a partition $P$ of $[a,b]$ such that $L(f,P,[a,b]) > L(f,[a,b]) - \frac{\epsilon}{2}$ . If I can show that there exists $N \in \mathbb{N}$ such that $L(f,P_N, [a,b]) \geq L(f,[a,b])$ and $|L(f,P_N,[a,b]) - l| < \frac{\epsilon}{2}$ , then I am done, since I will have as a consequence that $|l - L(f,[a,b])| < \epsilon$ , by the triangle inequality. Intuitively, I want to use the triangle inequality to show a connection between four 'things'. Firstly, I have the sequence $L(f,P_n,[a,b])$ , which is increasing (or at least non-decreasing) to $L(f,[a,b])$ as $n$ gets big. I know I can approximate $L(f,[a,b])$ , the second thing, with a margin $\epsilon$ of error, by $L(f,P,[a,b])$ , the third thing, for some partition $P$ . Then I just want to show that if I go far enough into the sequence $L(f,P_n,[a,b])$ , the terms eventually get at least as big as $L(f,P,[a,b])$ . Once they are at that threshold, the terms are within an $\epsilon$ margin of error to $L(f,P,[a,b])$ , and using the triangle inequality to get an upper bound on the distance between $l$ , the fourth thing, and $L(f,[a,b])$ , I would be finished. But how do I do this?","['riemann-integration', 'real-analysis']"
3018466,Does this polynomial have a rational value which is the square of a rational number?,"I have the following polynomial: $$P(x,y,z):=9y^2z^2-30x^2z+90xyz+54yz-270x+81\in\mathbb Q[x].$$ It came up in a larger proof, and I would need in order to complete the proof to prove the following result: Does there exist $(x,y,z,r)\in\mathbb Q^4$ such that $x\ne 0$ and $$P(x,y,z)=r^2.$$ We can reformulate the problem in the following way: Does the algebraic variety defined by $$9Y^2Z^2-30X^2Z+90XYZ+54YZ-270X+81-T^2$$ have a rational point with $X\ne 0$ ? I have no idea how to tackle this problem, I have looked up several articles, but nothing seems to apply to this particular question. Any hints or references would be greatly appreciated.","['number-theory', 'diophantine-equations', 'algebraic-geometry', 'polynomials', 'rational-numbers']"
3018513,Limit almost everywhere of averages of uniformly bounded and integrable functions .,"Let $f_n :[0,1] \to \Bbb{R}$ a sequence of uniformly bounded measurable functions with the property: $$\int_0^1f_n(x)f_m(x)dx=0,\forall m \neq n$$ Prove that $\frac{1}{N}\sum_{n=1}^Nf_n(x) \to 0$ almost everywhere in $[0,1]$ . I have already proven that: $(1)$ $\frac{1}{N^2}\sum_{n=1}^{N^2}f_n(x) \to 0$ almost everywhere. $(2)$ $\frac{1}{N+N^2}\sum_{n=1}^{N+N^2}f_n(x) \to 0$ almost everywhere. $(3)$ $\frac{1}{N}\sum_{n=1}^{N}f_n(x)-\frac{1}{N}\sum_{n=1}^{N}f_{n+m}(x)  \to 0$ almost everywhere, $\forall m \in \Bbb{N}$ Because of the fact that we have already a subsequence converging to $0$ almost everywhere i tried to show that $\frac{1}{N}\sum_{n=1}^{N}f_n(x)$ is a Cauchy sequence for almost every $x$ .
But i did not manage anything. Can someone give me a hint to solve this? I do not want a full solution. Thank you in advance.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3018518,Probability of sum of N dice being above certain value X,"In Dnd, sometimes you have to roll n, m-sided dice (say 5, d20s) and have the sum be greater than or equal to a certain value, x (say 90). This is easy for me to calculate by brute force, for most typical examples. I simply take the total number of possibilities that meet the criteria, and divide by the total number of possibilities, by running through each combination of dice rolls.  My result for the above example is 3003 dice combinations that sum to > 90, out of 3200000 combinations in total p = 0.009384375 chance of getting 90 or over. Is there a way (e.g. an equation) to reach this value directly?","['dice', 'probability']"
3018660,Equational laws holding in the symmetric group $S_3$,"I'm engaged in group theory (at least I am trying to get better) and so I found a problem dealing with the symmetric group $S_3$ . The first question is to find an ( equational ) law $\gamma$ , which holds in $S_3$ , but doesn't hold in some other groups. Furthermore I should find a group $G$ with $ |G| > |S_3|  $ , which satisfies all laws from $S_3$ . Sadly I'm even struggling at the first point.
I started listing all members of $ S_3 = \{e, (12), (13), (23), (123), (132)  \} $ and tried to find out which of the well known laws hold: Due to $(12)\circ(13) \neq (13)\circ(12)$ it can be seen that $S_3$ isn't an abelian group. $S_3$ is a group, so associativity must hold. Furthermore (same reason) there has to exist a neutral element $e$ . Does anyone have an idea which law is meant and how I can find out how many laws hold in $S_3$ overall? Thanks a lot!","['symmetric-groups', 'group-theory', 'abstract-algebra']"
3018684,How to compute associated graded algebras?,"I am trying understand associated graded algebras. Let $A=k[x,y]$ and $I=\langle xy^2, x^3-y^2 \rangle$ . Let $R=A/I$ . In the webpage , it is said that $\mathrm{gr}(R) = A/(x^4, y^2)$ . Let $A=k[x_1, \ldots, x_n]$ and `` $>$ '' a monomial order on $A$ . For a polynomial $p$ , denote by $in_>(p)$ the ideal the term of $p$ with the largest monomial. For an ideal $I$ of $A$ , the initial ideal of $I$ is defined as $in_{>}(I) = \langle in_{>}(p) \mid p \in I \rangle$ . I think that $gr(R)$ is isomorphic to $A/in(I)$ as a vector space, where $in(I)$ is the initial ideal of $I$ . Is this correct? I obtain that $gr(R)$ is isomorphic to $k[x,y]/(xy^2, x^3, y^4)$ as a vector space. Is this correct? Thank you very much.","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
3018712,Is this zeta-type function meromorphic?,"In An older question I asked : ( See A Thue-Morse Zeta function (Generalized Riemann Zeta function and new GRH) ) —— Consider $t_n$ as the Thue-Morse sequence . Let $m$ be a positive integer and $s$ a complex number, and recall that the Odiuos numbers are the indices of nonzero entries in the Thue-Morse sequence. Now consider the sequence of functions below: $$f(1,s)=1+2^{-s}+3^{-s}+4^{-s}+\dotsb$$ This is the zeta function valid for $\mathrm{Real}(s)>1$ . $$f(2,s)=1-2^{-s}+3^{-s}-4^{-s}+\dotsb$$ This is the alternating zeta function valid for $\mathrm{Real}(s)>0$ . $$f(3,s)=1-2^{-s}-3^{-s}+4^{-s}+5^{-s}-6^{-s}-7^{-s}+8^{-s}+\dotsb = 4^{-s} (\zeta(s,1/4) - \zeta(s,2/4) - \zeta(s,3/4) + \zeta(s,4/4) ) $$ ( $\zeta(s,a)$ is Hurwitz zeta ) I'm not sure if this has an official name yet but it clear that it is valid for $\mathrm{Real}(s)>-1$ . This sequence of functions is constructed in the similar way the Thue-Morse sequence is constructed. $$\begin{align}
&\vdots\\
f(\infty,s)&= \sum (-1)^{t_n} n^{-s}
\end{align}$$ This is a nice generalization/variant of the Riemann Zeta function and the Dirichlet eta or Dirichlet $L$ -functions. It follows that $f(m, s)$ is valid for $\mathrm{Real}(s)>-m+2$ . Now there are two logical questions analogue to the questions about the Riemann Zeta function: What are the functional equations for $f(m,s)$ ? Call the $N^\text{th}$ zero $Z_n(m)$ . Are all the zero's of $f(m,s)$ for any $m$ with $0<\mathrm{Real}(s)<1$ on the critical line $(\mathrm{Real}(Z_N(m))=1/2)$ ? Is clearly a generalizations of the Riemann Hypothesis. And I think it might be true! (I made some plots that were convincing but the accuracy was low.) I wonder if these functions have a name yet and what the answers to the 2 logical questions are. I also invite the readers to make more conjectures and variants with this. —— Some additional questions : let $T(s) = f(\infty,s) $ . 1) Is $T(s)$ meromorphic on The entire complex plane ? 2) how Many poles does $T(s)$ have ? Is it one ? 3) assuming 1) : What is The infinite product representation for $T(s)$ ? ( hadamard type product ) 4) assuming 1),2) how fast is this function growing on The complex plane ? As fast as Riemann zeta ?? I assume so. I think all of these are true.
Maybe 2) can Be shown by induction from $f(n,z) $ To $f(n+1,z) $ ?? But infinity is no integer , so maybe not.","['complex-analysis', 'zeta-functions', 'infinite-product', 'analytic-number-theory']"
3018808,limit of $\left(1+\frac{1}{n!}\right)^n$,"I'm having trouble resolving the following limit: $$ \lim_{n \to \infty} \left(1+\frac{1}{n!}\right)^n $$ Intuituvely the limit is equal to 1, but the exercises requires me to resolve via calculation and I have no idea how I can accomplish this. Can someone please explain it to me?","['limits', 'factorial']"
