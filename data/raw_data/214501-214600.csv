question_id,title,body,tags
4346522,Why an initial ring is a domain?,"It is a well known fact that $\mathbb{Z}$ , the ring of integers, is a domain. On the other hand, $\mathbb{Z}$ is also the initial object in the category Ring. If one defines $\mathbb{Z}$ as the initial object in Ring, is it possible to prove it is a domain without an explicit construction?","['abstract-algebra', 'soft-question', 'category-theory']"
4346542,How to show that this set is finite?,"Let $m \in \mathbb{N}$ For $\alpha = (\alpha_{1},...,\alpha_{m}) \in \mathbb{N}_{0}^{m}$ , let $|\alpha|:= \alpha_{1}+...+\alpha_{m}$ Is the set $\{\alpha \in \mathbb{N}_{0}^{m}: |\alpha|\leq k\}$ finite for any $k \in \mathbb{N}$ ? I'm almost sure that it is but I have no idea how to prove it or even give a convincing explanation as to why that's the case. I thought maybe use proof by induction but the argument seems to be very complicated. Is there a contradiction you can reach by assuming the set to be infinite? Apologies for tags, I don't know which section to post this question in.","['elementary-set-theory', 'combinatorics', 'natural-numbers']"
4346566,$e^{-nx}\cdot\sum_{k=0}^\infty\frac{(nx)^k}{k!}f\left(\frac{k}{n}\right)\to f(x)$ for $f$ continuous and bounded,"Let $f:\mathbb R\to\mathbb R$ be continuous and bounded. Prove that for each $x>0$ we have $$f(x)=\lim_{n\to\infty}\left(e^{-nx}\cdot\sum_{k=0}^\infty\frac{(nx)^k}{k!}f\left(\frac{k}{n}\right)\right).$$ When $x$ is an integer we have $$e^{-nx}\cdot\sum_{k=0}^\infty\frac{(nx)^k}{k!}f\left(\frac{k}{n}\right)=\frac{e^{-nx}}{(nx)!}\cdot\sum_{k=0}^\infty\binom{nx}{k}(nx-k)!(nx)^k.$$ I then substitute the gamma function integral formula for the factorial, swap the order of $\sum$ and $\int$ and apply the binomial theorem to get that this is equal to $$\frac{e^{-nx}}{(nx)!}\int_0^\infty(nx+y)^{nx}e^{-y}\,dy.$$ This doesn't look easy to solve. When $x$ is not an integer, I am having even more difficulties. I would appreciate any help.","['analysis', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
4346587,"The incircle of $\triangle ABC$ has centre $I$ and touches $BC$ at $D$. Prove that $M,I,N$ are collinear","The incircle of $\triangle ABC$ has centre $I$ and touches $BC$ at $D$ . Let the midpoints of $AD$ and $BC$ be $M$ and $N$ respectively. Prove that $M,I,N$ are collinear My book proceeds like this- Let $NI$ meet $AD$ at $M'$ . Join $A$ and $I$ and let $AI$ meet $BC$ at $L$ . For $\triangle ADL$ with $NIM'$ as transversal, Menelaus gives- $$\frac {DN}{NL}\frac {LI}{IA}\frac {AM'}{M'D}=-1$$ $$\frac {LI}{IA}=\frac {LC}{CA}=ab/(b+c)b$$ And, $$DN/LN=\to\frac{BN-BD}{BN-BL}=\frac{a/2-(s-b)}{a/2-ac/(b+c)}\leftarrow=\frac{b+c}a$$ I tried but could not figure out how the highlighted portions were derived. Please help.","['contest-math', 'euclidean-geometry', 'geometry']"
4346665,Find the probability of a face of a loaded die.,"I have a die, for which all faces have the same probability, except for one face. I know in advance which face is different, let's say it's the 6. My goal is to find the probability of that face by tossing the die $n$ times and counting how many times it lands on the loaded face. Let's suppose I rolled the die $n = 1000$ times and it landed $k = 250$ times on the 6. The obvious answer would be that the probability of landing on the 6 is $\frac{k}{n} = \frac{1}{4}$ . But the real probability ( $\frac{k}{n}$ after infinitely many tosses) could be different. It could also be only $\frac{1}{8}$ but I got extremely lucky in this case and the die landed 250 out of 1000 times on the 6. However, this would be extremely unlikely, but how unlikely? How to calculate the probability, that x% is the real probability of the die? I'd like to have a function f(x) = probability that x is the real probability of the die , with the parameters $n$ and $k$ . I think the maximum of this function should be at $\frac{k}{n}$ , but I'm not sure exactly how the function would look. Also, I'd like to give upper and lower bounds on the probability after $n$ tosses. I know that it's technically impossible since the die could land $k$ times on the 6 no matter what the real probability is (except if it's 0% or 100%). However, I could set the bounds in a way that they ignore extremely improbable events. So, I want to say that the real probability of the die is between a% and b%, with a probability of c%. I believe this can be solved using integrals, but how?","['statistics', 'probability-distributions', 'probability']"
4346668,Dimension of fibre products of $k$-schemes for an arbitrary field $k$,"I am trying to get some intuition concerning fibre products of schemes and thus was looking for examples of the following: Suppose we have schemes $X$ and $Y$ with morphisms $f$ and $g$ from each of them to $S=\operatorname{Spec}(k)$ for an arbitrary field $k$ (I believe these are sometimes called $k$ -schemes). If $f$ and $g$ are of finite type, then the dimension of their fibre product over $k$ is the sum of the dimensions (Prop. 5.37 in Görtz and Wedhorn Algebraic Geometry I). Otherwise, what can we say about the dimensions of their fibre product over $k$ ? Can we obtain a bigger dimension than the sum of both? Could this dimension be even infinite if $X$ and $Y$ had both dimension $0$ ? I would appreciate examples where possible.","['algebraic-geometry', 'schemes', 'fibre-product']"
4346676,Can someone help me with the intuition behind this combinatorics problem from Blitzstein's Stats 110?,"Premise: ""A jar contains r red balls and g green balls, where r and g are fixed positive integers. A ball is drawn from the jar randomly (with all probabilities equally likely), and then a second ball is drawn randomly"" Question: ""Suppose that there are 16 balls in total, and that the probability that the two balls are the same color is the same as the probability that they are different colors. What are r and g?"" I understand that the only way the two probabilities equal each other are if they're both 1/2 - that's obvious. I also understand that the denominator in terms of how many combinations that can be selected is (g+r) (g+r-1). But I don't understand why in the solutions, they state the numerator as 2gr. I understand the algebra after setting (2gr)/((g+r) (g+r-1)) equal to 1/2 and solving the quadratic equation. I just don't get the intuition of why the numerator is 2gr in the first place.","['statistics', 'combinatorics', 'probability']"
4346685,"What is the principal branch of the complex arcsine, in simple terms?","By simple, I mean something akin to $\{z: -\pi < \mathrm{Im}\, z \le  \pi\}$ for the logarithm or $\{z: -\pi/2 < \mathrm{Arg}\, z \le \pi/2\}$ for the square root. I am aware of the answer, ""the range of $z \mapsto -i\,\mathrm{Log}(iz + \sqrt{1-z^2})$ "", where one uses the principal branch of the square root, but I have having trouble sketching that region.  Clearly, it is not the $-i$ times the principal branch of the logarithm ( i.e. , $\{z: -\pi < \mathrm{Re}\, z \le \pi\}$ ), as sine is not one-to-one on that region.  The trouble lies in figuring out what the principal branch of $z \mapsto iz + \sqrt{1-z^2}$ is. Before looking at other sources, I tried determining a branch of the arcsine from the relation, $\sin(x + iy) = \sin x \cosh y + i \cos x \sinh y$ .  From this, I got $\{z: |\mathrm{Re} \, z| < \pi/2\} \cup \{z: \mathrm{Re}\, z=\pm \pi/2, \mathrm{Im}\,z \ge 0\}$ as a natural candidate.  (I think that's a branch, isn't it?  That is, I've convinced myself that sine provides a bijection from this region to the entire complex plane, but I still doubt myself a little.)  But this turns out not to be the principal branch, for $\mathrm{Arcsin}(2) = \pi/2 - i\ln(2+\sqrt{3})$ , with a negative imaginary part.  ( $\sin[\pi/2 + i\ln(2+\sqrt{3})]$ is indeed 2.) It turns out that $\mathrm{sgn \, Im\, Arcsin}\, a = -\mathrm{sgn}\,a$ when $a$ is a real number whose magnitude exceeds unity.  This maintains $\mathrm{Arcsin}(-z)= -\mathrm{Arcsin}\,z$ .  So now I hypothesize that the principal branch is almost the one I suggested as my natural candidate above.  Just replace the positive imaginary parts with their negatives when the real part is $\pi/2$ .  Haven't proved this, though.  (Somehow, it would feel more natural to me if the signs of the real and imaginary parts matched on these boundaries.) This comes up because, as a teacher and tutor of precalculus, I often remark in passing that arcsin(2) is undefined until you get to complex analysis (which is never for almost all of my students), where things gets really hairy. I haven't done complex analysis in years, though I was assigned to teach it to undergraduates once. Just got me wondering.  I've tried very hard, but, surprisingly, I have been unable to find an answer to my question on the 'net.","['complex-analysis', 'trigonometry', 'inverse-function']"
4346700,Prove the following rules for formulas with bounded quantifiers,"I have a question that goes like this: Prove the following rules for formulas with bounded quantifiers (derive them from suitable logical rules for unbounded quantifiers): a) [(∀x)α(x) φ(x) ∧ (∀x)α(x) ψ(x)]⇔(∀x)α(x) (φ(x)∧ψ(x)) b)∼(∃x)α(x) φ(x) ⇔ (∀x)α(x) ∼φ(x) My attempt to solve it: a) I started with the LHS to derive the RHS: [(∀x)α(x) φ(x) ∧ (∀x)α(x) ψ(x)] ⇔ [(∀x)α(x) ⇒ φ(x)] ∧ [(∀x)α(x) ⇒ ψ(x)] ⇔ (∀x)[α(x) ⇒ φ(x) ∧ α(x) ⇒ ψ(x)] ⇔ (∀x)α(x)[α(x) ∧ ψ(x)] b) The same approach, I started with LHS: ∼(∃x)α(x) φ(x) ⇔ ∼(∃x)[α(x) ∧ φ(x)] ⇔ (∃x)[∼α(x) ∨ ∼φ(x)] ⇔ ∼(∃x)[∼α(x) ⇒ φ(x)] ⇔ (∀x)∼[∼α(x) ⇒ φ(x)] ⇔ (∀x)[α(x) ∧ ∼φ(x)] ⇔ (∀x)α(x) ∼φ(x). I am not sure if what I did are correct especially for b)! Any help may be appreciated!","['logic', 'discrete-mathematics', 'computer-science']"
4346719,"Why is $\frac{d}{dx}\ln(x) = \frac1x$ when the domain of $\frac1x$ includes $(-\infty,0)$?","On Khan Academy , they factor the derivative: $$
\frac{d}{dx}\frac{x^2 + x - 2}{x-1} = \frac{d}{dx}(x+2) = 1 \text { where $x \ne 1$} 
$$ Khan says: Recall the derivative is equal to $1$ for all $x$ values where the function is defined. Since the function is undefined for $x=1$ , so is the derivative. I did some Google searching to see if this was really a rule, and I found this Quora question .  One of the answers provides a seeming counterexample: $$
\ln(x) \text{ has a domain } (0,∞) \\
\frac{d}{dx}\ln(x)=\frac{1}{x} \\
\frac{1}{x} \text { has the domain} (-∞,0) \text{ and } (0,∞) \\
$$ So is Khan stating an incorrect rule?  Obviously it works in the case of his problem, but I'd like to understand what the accurate rule is here.  Maybe the key question is what type of discontinuity we're dealing with?","['continuity', 'calculus', 'functions', 'derivatives']"
4346720,Is the kernel of a surjective morphism $\mathscr{F} \to \mathscr{G}$ of locally free sheaves locally free?,"This condition seems rather strong so I am not sure if this is true. We assume $X$ is (locally) noetherian and $\mathscr{F}, \mathscr{G}$ are locally free of finite rank. Now, I claim that $\mathscr{H} = \ker(\mathscr{F} \to \mathscr{G})$ is locally free. Hartshorne's exercise II.5.7 states that a coherent sheaf $\mathscr{H}$ on $X$ is locally free iff $\mathscr{H}_x$ is free over $\mathcal{O}_x$ for all $x \in X$ . Now, let $x \in X$ be arbitrary. Since $\mathscr{H}_x$ fits into the exact sequence below $$0 \to \mathscr{H}_x \to \mathcal{O}_x^{\oplus n} \to \mathcal{O}_x^{\oplus m} \to 0.$$ Now, since $\mathcal{O}_x^{\oplus n}, \mathcal{O}_x^{\oplus m}$ are projective $\mathcal{O}_x$ -modules, this exact sequence splits so that $\mathscr{H}_x \oplus \mathcal{O}_x^{\oplus m} \cong \mathcal{O}_x^{\oplus n}$ so that $\mathscr{H}_x$ is a direct summand of a free module and hence projective. Now, $\mathscr{H}_x$ is projective over a local ring $\mathcal{O}_x$ and is hence free. The above seems fine. I am mostly concerned with the case that $X = \operatorname{Spec} A$ , where this would imply that the kernel of a surjective map of free modules of finite rank is free. Based on this answer , this does not appear to be true. Am I making a mistake above or am I mistaken in assuming a locally free sheaf on an affine scheme is free?","['algebraic-geometry', 'solution-verification', 'commutative-algebra']"
4346736,Dimension of maximal totally isotropic space over finite field with standard bilinear form,"Let $p$ be a prime and consider the vector space $\mathbb{F}_p^n$ , equipped with the standard bilinear form $\langle x, y \rangle = \sum_{i=1}^n x_iy_i$ . A subspace $U$ is said to be totally isotropic if $\langle u, v \rangle = 0$ for every two (not necessarily distinct) $u,v \in U$ . What can we say about the dimension of a maximal totally isotropic subspace? A very well known upper bound is $\dim U \leq \lfloor \frac{n}{2} \rfloor$ and it follows by $\dim U + \dim U^{\perp} = n$ and $U \subseteq U^{\perp}$ . But how really is this bound attainable for $p>2$ ? For $p=2$ there is an easy example - take the subspace with basis $(1,1,0,\ldots,0)$ , $(0,0,1,1,0,\ldots,0)$ , $(0,0,0,0,1,1,0,\ldots,0)$ , etc. If you mimic this for other $p$ (by taking chunks of $p$ consecutive $1$ -s) the result would be of dimension $\lfloor \frac{n}{p} \rfloor$ . So is the truth closer to $\lfloor \frac{n}{p} \rfloor$ or to $\lfloor \frac{n}{2} \rfloor$ ? If it is too hard to answer for all $p$ , perhaps focusing on $p=3$ only could give good insight. Any help appreciated!","['orthogonality', 'finite-fields', 'linear-algebra', 'bilinear-form']"
4346761,"If $F$ is differentiable at $x_0$, $f$ is continuous at $x_0$","just looking for a hint on how to proceed here. In Tao's Analysis I , exercise 11.9.3 is as follows: Let $a<b$ be real numbers, and let $f:[a,b]\rightarrow\mathbb{R}$ be a monotone increasing function. Let $F:[a,b]\rightarrow\mathbb{R}$ be the function $F(x):=\int_{[a,x]}f$ . Let $x_0\in(a,b)$ be given. Show that $F$ is differentiable at $x_0$ iff $f$ is continuous at $x_0$ . (Hint: one direction is taken care of by one of the Fundamental Theorems of Calculus. For the other, consider the left and right limits of $f$ and argue by contradiction.) From this section, we are also given the two Fundamental Theorems of Calculus as follows: (First Fundamental Theorem). Let $a<b$ be real numbers, and let $f:[a,b]\rightarrow\mathbb{R}$ be a Riemann integrable function. Let $F:[a,b]\rightarrow\mathbb{R}$ be the function $F(x):=\int_{[a,x]}f$ . Then $F$ is continuous. Furthermore, if $x_0\in[a,b]$ and $f$ is continuous, then $F$ is differentiable at $x_0$ , and $F'(x_0)=f(x_0)$ . (Second Fundamental Theorem). Let $a<b$ be real numbers, and let $f:[a,b]\rightarrow\mathbb{R}$ be a Riemann integrable function. If $F:[a,b]\rightarrow\mathbb{R}$ is an antiderivative of $f$ , then $\int_{[a,b]}f=F(b)-F(a)$ . It was proven in an earlier section that if $f$ is monotone on a closed interval, it is integrable on that interval. Following the hint given, the First Fundamental Theorem tells us if $f$ is continuous at $x_0$ , then $F(x):=\int_{[a,x]}f$ is differentiable at $x_0$ , so we are done in that respect. My trouble here is with the other direction. Following the hint again, suppose for the sake of contradiction that $F$ is differentiable at $x_0$ , but $f$ is not continuous at $x_0$ . Since $f$ is a function on $[a,b]$ , it is defined at $x_0$ . Since $f$ is monotone increasing, $f(x_0)\geq f(x)$ for all $x<x_0$ and $f(x_0)\leq f(x)$ for all $x>x_0$ . Therefore, $\lim\limits_{x\rightarrow x_0^-}f(x)\leq f(x_0)\leq\lim\limits_{x\rightarrow x_0^+}f(x)$ , and since $f$ is not continuous, $\lim\limits_{x\rightarrow x_0^-}f(x)<\lim\limits_{x\rightarrow x_0^+}f(x)$ . This is where I seem to be stuck. My intuition is that now I should be able to show that $$\lim\limits_{x\rightarrow x_0^-}\frac{F(x)-F(x_0)}{x-x_0}<\lim\limits_{x\rightarrow x_0^+} \frac{F(x)-F(x_0)}{x-x_0},$$ but I can't quite piece together the connection. I've simplified the expression for which we're taking a limit to $$\frac{F(x)-F(x_0)}{x-x_0}=\frac{\int_{[x,x_0]}f}{x-x_0},$$ but this hasn't made anything clear for me, either. Any hints on how to proceed would be much appreciated. Thanks!","['integration', 'derivatives', 'monotone-functions', 'real-analysis']"
4346764,Limit using Itô isometry,"I have a silly problem I don't understand. if $\chi$ denotes, the characteristic function and $B_s$ a Browninan motion, I need to show $\int_0^t\frac{B_s}{\epsilon}\chi_{B_s\in(-\epsilon,\epsilon)}dB_s\to 0$ in $L^2(P)$ as $\epsilon\searrow0$ . However, using Itô's identity I obtain $$E\left[\left(\int_0^t\frac{B_s}{\epsilon}\chi_{B_s\in(-\epsilon,\epsilon)}dB_s\right)^2\right]=E\left[\int_0^t \left(\frac{B_s}{\epsilon}\right)^2\chi_{B_s\in(-\epsilon,\epsilon)}ds\right]\le\int_0^tds=t\not\to0$$","['stochastic-processes', 'martingales', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4346765,4th order linear ordinary differential equation,"While I was solving an integral using Feynman Integration, I came across the following differential equation: $$y’’’’-y’’+y=0$$ I tried substituting $y$ with an exponential function which failed. Can someone else show me how to solve it?",['ordinary-differential-equations']
4346766,Double integral substitution where the region of integration becomes a point,"I want to evaluate a double integral of some function over the region between the graphs $y=x^2$ and $y=\frac{x^2}{2}+1$ . $D=\{(x,y):-\sqrt2\leq x \leq \sqrt2, x^2\leq y \leq \frac{x^2}{2}+1\}$ The subsitution I am trying to make is $$x^2=u$$ $$y=v$$ The way I visualise this substitution is folding around the $y$ axis from left to right and making the curves straight lines. $$y=x^2 \rightarrow v=u$$ $$y=\frac{x^2}{2}+1 \rightarrow v=\frac{u}{2}+1$$ From there I get confused. The Jacobian is always zero $\pm \frac{1}{2\sqrt{u}}$ and $D$ became a single point $(2,2)$ . Doesn't this make the integral zero?","['integration', 'multivariable-calculus']"
4346780,Rigorous definitions of probabilistic statements in Machine Learning,"In a supervised machine learning setup, one usually considers an underlying measurable space $(\Omega, \mathcal{F}, \Bbb P)$ and random vectors/variables $X:\Omega \rightarrow \Bbb R^n, Y: \Omega \rightarrow \Bbb R.$ We can then consider the probability distribution of $(X,Y),$ denoted as $\Bbb P_{X,Y}.$ For a loss function $\ell: \Bbb R \times \Bbb R \rightarrow \Bbb R,$ the corresponding risk of a measurable functional $f: \Bbb R^n \rightarrow \Bbb R$ is then defined as $$R(f): = E_{\Bbb P_{X,Y}}\left[\ell(f(X), Y)\right],$$ where $E_{\Bbb P_{X,Y}}$ denotes the expectation with respect to the probability measure $P_{X,Y}.$ The Bayes risk is defined as $$R^* := \inf \{R(f) \mid f: \Bbb R^n \rightarrow \Bbb R \textrm{ measurable}\}$$ and any measurable $f^*$ for which $R(f^*) = R^*$ is called a target function. I many textbooks and courses on the topic, one can find the following statements: a) if $(X,Y)$ is absolutely continuous and $\ell(y, \hat{y}): = (y - \hat{y})^2,$ then $$f^*(x) = E_{\Bbb P}[Y \mid X = x].$$ b) if $Y$ is discrete, say $Y \in\{1, \ldots, K\}$ with probability one, and $\ell(y, \hat{y}):= \left\{
\begin{array}{ll}
      0, & \textrm{if }y = \hat{y} \\
      1, & \textrm{otherwise, } \\
\end{array}\right.$ then $$f^*(x) = \textrm{argmax}\{\Bbb P(Y = k\mid X = x) \mid k \in \{1, \ldots, K\}\}.$$ I have the following questions: In a), I am aware of a correct definition of $f^*$ coming from the (measure- theoretic) concept of conditional expectation. Specifically, using Radon-Nikodym's theorem (and some additional assumptions), one can show that there is a measurable $f^*$ (unique a.s) for which the risk $R$ is minimised, and then by definition $E_{\Bbb P}[Y \mid X = x] := f^* \circ X$ However, in all of these books/courses there is no mention of this proper definition, nor they give a satisfactory alternative as a definition. How is it possible to work in a (computer science) class with these constructions then? Is there some kind of non-spoken truth among computer scientists that I am not aware of? Am I alone in this feeling?  This makes me believe that they have a way to look at these constructions that I am just not familiar with. How should I look at these things then? Reading computer science literature on the topic is a pain as I just can't trust what I am reading. In b), $f^*$ is simply not well defined if $X$ is absolutely continuous (in this case, the event (X = x) has probability zero and thus conditional probability is not defined). Again, nobody asks these kind of questions in the lectures. How should I look at it? Can you please provide a (simple) reference treating these topics in a rigorous fashion? My background is in optimization, so I am not very familiar with the prob/stats literature.","['conditional-probability', 'conditional-expectation', 'machine-learning', 'probability-theory', 'radon-nikodym']"
4346782,The Mukai pairing,"I am reading the book “Fourier-Mukai transforms in algebraic geometry” by Daniel Huybrechts. at the end of the page 132 and the beginning of the page 133, he introduced the Mukai pairing as follows: Definition: Let $\nu=\Sigma \nu_j\in \oplus H^j(X, \mathbb{C})$ . Then one defines the dual of $\nu$ by $$\nu^{\vee}:=\Sigma \sqrt{-1}^j\nu_j\in H^*(X,\mathbb{C})$$ and the Mukai pairing on $H^*(X,\mathbb{C})$ is the quadratic form $$\langle\nu,\nu^{\vee}\rangle_X:=\int_X exp(c_1(X)/2).(\nu^{\vee}.\nu’)$$ Now, my question is that is there any reference that has some computations of the Mukai pairing? Or do you know any specific easy example of it?","['derived-categories', 'algebraic-geometry']"
4346784,Comparison of bird diet at two different nests,"I have a list with the different prey types and quantities that birds at Nest $1$ gave to their fledglings. I also have the same information for another nest of the same bird species. Suppose I have NEST 1
Mammals: 500
Arthropods: 200
Birds: 20

NEST 2
Mammals: 180
Arthropods: 50
Birds: 3 Can I use the Chi-squared test to demonstrate the diets are actually very similar (or maybe not similar)? Of course, my actual data is more complex, with more classes (to family level).","['biology', 'chi-squared', 'statistics']"
4346813,"It is possible to find a solution to $y''+\sqrt{|y|}\operatorname{sgn}(y)+\sqrt{|y'|}\operatorname{sgn}(y')=0,$ $\,y'(0)=0,\,y(0)= 1/4$?","It is possible to find an exact solution (hopefully in ""close form"") to $$y''+\sqrt{|y|}\operatorname{sgn}(y)+\sqrt{|y'|}\operatorname{sgn}(y')=0, \,y'(0)=0,\,y(0)= 1/4$$ ? How?... There exist a value of $t^*=\,?$ from which $y(t)=0,\,\forall t\geq t^*$ ? PS: If I multiply the equation by $i$ the terms looks alike Hilbert Transforms... I wasn´t able to do something with this, but maybe it helps someone else. **** 3rd Added later**** The main things I would like to know (in case that the solution is to hard to find): i) How it behaves for $t<0$ ? ii) Does it have a ending time $t_f$ from where $y(t)=0,\, t>t_f$ ? iii) Does the solution $y(t)$ been a compact-supported function? iv) Does the solution $y(t)$ been a bump function $\in C_c^{\infty}$ ?, v) Does the solution $y(t)$ starts with a discontinuity? vi) Does the solution $y(t)$ lives always in the reals $y(t) \in \mathbb{R}\,\forall t$ ? vi) Is the solution unique? This how the solution looks on Wolfram-Alpha . **** 2nd Added later**** Following a comment and since I am interested in a solution with $y\geq 0\Rightarrow |y|\equiv y$ , following the graph of Wolfram-Alpha it also happen than $y'\leq 0\Rightarrow |y'|\equiv -y'$ , and also using that $\text{sgn}(y)\sqrt{|y|} \equiv \frac{y}{|y|}\sqrt{|y|}\equiv \frac{y}{\sqrt{|y|}}$ , I could change the equation into another equation that I believe are not ""formally"" equivalent, but will give the same solution to the initial value problem: $$y'' +\sqrt{y}+i \sqrt{y'}=0, \,y'(0)=0,\,y(0)= 1/4$$ and in Wolfram-Alpha the plot looks similar, but I also get stack this time, maybe some knows how to solve this other differential equation. Plese note this analysis is only valid for this case $y(0)=1/4$ , since for higher initial values it can be seen that the solution do oscillations around $y=0$ so the assumption $y>0$ is not always hold. ***** 1st Added later ***** Since someone close the question because of lack of background, I will explain why I think is an interesting question. Recently I have learned here on MSE that no non-constant real-valued and continuous ANALYTIC function can be of finite-duration (since its compact-supported in the time variable), and also that no finite-duration function could be a solution to a linear ordinary differential equation (Linear ODE) - because of uniqueness-related-issues of solutions at the beginning and at the end which becomes zero outside them, so, since everything I have seen in engineering, which is been modeled or through Linear ODE or through solutions than can be described as a Power Series (analytical), are only approximations, since NO FINITE-DURATION solution to a system could be described by them. I have been looking for a Theory that described continuous-time finite-duration systems and its solutions (which now I know must be non-linear), but I can´t find yet any related theory.... All I have found yet on the web are abstract things I don´t understand (modern differential topology/geometry), but all they starts with assumptions of, or, with Linear Operators or Analytical Manifolds (which I already know can´t model a finite-duration solution), or with Smooth Manifolds or Smooth diffeomorphism, which only can be of finite-duration if at their support in the time-axis the function is zero, which is too restrictive (because, if it is of finite-duration, is of compact support in the time variable, and the only way a smooth function could be of compact support is through been a bump-function in the time axis, requiring that their values and also all their derivatives are zero at the beginning and at the end, to keep smoothness). As example, the simple model of a particle that experience a elastic collision with a wall, if the position vs time function is continuous (so no ""teleporting"" is allowed), the derivative/speed will show a bounded ""jump discontinuity"", which will become a singularity in the acceleration profile, so smoothness is no a desirable requirement (is too restrictive). Since finite-duration non-smooth phenomena is the most abundant kind of system in daily life, I was expecting to find a lot of math related on internet, but disappointingly, I have only found a few (four) papers working with continuous-time finite-duration differential equations (I take the question from one of them): 1 , 2 , 3 and 4 . From my side, I believe I have found a way to find solutions to segments of functions that are already solutions of initial value problems Linear ODE, this by taking their finite-duration Fourier Transform, without needing to take the convolution with the rectangular function that made the ""cut"" of the full-time-solution, and avoiding the problem of the discontinuities at the edges of the compact-supported domain, keeping true the Parseval's relation - and this scheme is also useful for finding the finite-duration Fourier transform of non-linear functions if I know beforehand the finite-duration Fourier transform of its first or its second derivative, jointly with the border conditions - I explained it here .... but unfortunately, It doesn´t work with finite-duration systems which are necessarily non-linear (but is an easy alternative to being taking convolutions). I have already know that if the solution start and end at zero, if smooth, it is a bump function (for which I am not really interested, since it implies a self-emerging system), but they also are quite complicated, and the only differential equation I found from them is in here , which is non-linear and also time-delayed, but is defined from all $t \in \mathbb{R}$ even where its solution lives only in a compact-support, so it fulfill what I have learned so far. From the mentioned papers, I understand now that there exists continuous non-linear systems with finite-duration solutions, which solutions are not unique, so I am trying to understand If every continuous non-linear system could support finite-duration solutions or not, and if not, How you can identify systems with finite-duration solutions?, and also, If there are restrictions imposed to the solutions because of been of finite duration, like example, if they can have unbounded derivative or not because of it (my intuition says they will have more restricted conditions because of causality issues since an infinite speed violate every possible constant-causality-speed model, but is just speculation).... And many other questions rise, since non-linear systems don´t preserve the superposition principle, How is possible that Maxwell's wave equations solutions to be ""right"" if finite-duration phenomena are non-linear? Which spurious effects could be introducing modeling them with infinite-duration signals?, speaking in ""physicists terms"", I quite shocked with the ""so little amount"" of papers about finite-duration physics that could be found published on Google (maybe I am using wrong words, so any help with finding these theory will be of great help), and maybe huge restrictions could rise: as example, if a continuous function is of finite-duration, just because of these two restrictions, the function is also bounded, and it Fourier Transform is Analytic, which are a huge restrictions just because of been of finite-duration... maybe other issues will rise because of it finite extension. But before, I am trying to understand these papers, which are a bit advanced for me (but not unattainable as the differential topology topics), and for this, I am trying to find the solution of the presented equation, which is supposed to be a continuous-time finite-duration differential equation, and when plotted, it looks like the half of a bumped-function similar to a Gaussian kernel, but I don´t know if its really becoming zero for every time that is bigger than an specific $t^*$ , or if it behaves as vanishing at infinity as the Gaussian function does, so, not been a truly finite-duration solution. An also, I don´t know what is happening at the beginning: it is just starting at zero previous the beginning (implying a jump discontinuity at the start)? or have non-zero values outside its finite-duration (maybe, becoming a complex function)?, What is happening with the derivative at the discontinuity at the beginning?... all these question requiring to know the specific solution. Hope its clear now, and hope you get interested as I am into these continuous-time finite-duration systems.","['ordinary-differential-equations', 'real-analysis', 'singular-solution', 'finite-duration', 'dynamical-systems']"
4346850,Sobolev space counterexample,"Is there an example of a smooth function $u:\mathbb{R}\to\mathbb{R}$ with compact support such that $|u|$ fails to lie in the Sobolev space $H^s(\mathbb{R})$ for every $s>1$ ? Clearly $|u|$ can fail to lie in $W^{s,\infty}$ , but it isn't clear to me that the singularities that arise in the $L^{\infty}$ case can't be ""integrated"" away in the $H^s$ case. Am I missing an obvious example?","['sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4346901,"Use sandwich theorem to find $\lim_{x \to 0} f(x)$, when $|f(x)-1| \le x^2$","My approach: Using the definition of absolute function we know that, $$ 0 \le |f(x)-1| \le x^2$$ Applying $\displaystyle \lim_{x \to 0}$ on lower and upper bound, we get $$ \lim_{x \to 0}\,(0)=0 \qquad\text{ and }\qquad \displaystyle \lim_{x \to 0} \, (x^2)=0$$ So, by sandwich theorem, $$  \lim_{x \to 0} |f(x)-1|=0$$ Case I: $$\lim_{x \to 0} f(x)-1=0$$ $$\implies \displaystyle \lim_{x \to 0} f(x)=1$$ Case II: $$\displaystyle \lim_{x \to 0} 1-f(x)=0$$ $$\implies \displaystyle \lim_{x \to 0} f(x)=1$$ In either case, $\displaystyle \lim_{x \to 0} f(x)=1$ . This was a 5 mark question on a test and my calculus professor gave me 2 marks for this solution. He says that he deducted marks because I wrote $0 \le |f(x)-1| \le x^2$ instead of $-x^2 \le f(x)-1 \le x^2$ and then applying sandwich theorem. How is my approach wrong? Will my approach yield a wrong answer in a different scenario? Please explain.","['real-analysis', 'continuity', 'calculus', 'limits', 'derivatives']"
4346902,Expectation of the product between two dependent Bernoulli random variables,"I have two dependent Bernoulli random variables $X$ and $Y$ , and I know that: $$
\begin{align}
P(X=0)&=P(X=1)=1/2 \\
P(Y=1)&=11/24 \\
P(Y=0|X=0)&=1/3 \\
P(Y=1|X=1)&=1/4 \\
\end{align}
$$ Is it possible to get the expectation $\mathbb{E}[XY]$ ?","['expected-value', 'statistics', 'probability', 'random-variables']"
4346907,Why are local rings called local?,"I gather that rings of germs of functions at a point $p$ on a manifold/variety/etc. are local with the maximal ideal containing exactly the germs of functions which vanish at $p$ . So in some sense, these rings, which happen to be local, describe the local behavior of functions. But what about other local rings? $\mathbb Z/p^n\mathbb Z$ is local for primes $p$ and $n\geq1$ . Can we interpret it as a ring of germs of functions on some space? I found a way to do so for fields $F$ , at least. They can be seen as the ring of functions (or germs thereof, makes no difference in this case) on a one-element topological space $\{p\}$ , where $x(p):=x$ for all $x\in F$ . Which seems like it would make sense in a context where local rings actually are rings of germs: local rings with trivial maximal ideal (fields) are germs of functions on a trivial space. But how to generalize this?","['local-rings', 'ring-theory', 'abstract-algebra', 'germs']"
4346932,How to find the probability that the sample variance (given in the question below) is this low/lower when given the true population variance?,"I'm not sure how to answer the question below. I have gone through worked examples and I have the answer for this question as well. However, the answer doesn't really provide any working. There are similar questions to this that are basically structured in the same way: 'what is the probability that a sample variance this high/low would be found if the population variance is x'. I'm assuming the same process is used to solve them. Some context: I am an undergraduate student and I was working through my book when I came across this question. Relating to sample variance, I know that: Given a random sample of 'n' observations from a normally distributed population whose population variance is $\sigma^2$ and whose resulting sample variance is $s^2$ , it can be shown that: $$\chi^{2} = \frac{(n-1)s^2}{\sigma^2}$$ has a chi-square distribution with n-1 degrees of freedom. I know for the below question I may have to find values for the cumulative distribution of $\chi^{2}$ , which could be either the upper or lower tail of the distribution: P( $\chi^{2}$ <K) = 0.05 for example or P( $\chi^{2}$ >K) = 0.05 [where K can be any critical value based on the (n-1) degrees of freedom, for example: 3.94 for the lower distribution, when (n-1)=10]. I'm not sure if I have to use an upper or lower tail for the below question and what value to find from the above expression. It would be great if you could explain the process behind solving this question: Q. A manufacturer has been purchasing raw materials from a supplier whose consignments have a variance of 15.4 (in squared pounds) in impurity levels. A rival supplier claims that she can supply consignments of this raw material with the same mean impurity level but with lower variance. For a random sample of 25 consignments from the second supplier, the variance in impurity levels was found to be 12.2 . What is the probability of observing a value this low or lower for the sample variance if, in fact, the true population variance is 15.4 ? Assume that the population distribution is normal. [if this helps: Supplier 1: variance: 15.4 ; Supplier 2: n= 25 , variance: 12.2 ; true population variance: 15.4 ] Also, I apologise in advance if this isn't the right stack exchange to ask this question. Let me know if it's more appropriate in another stack exchange (as I posted it here since it part of my statistics course in economics). Any help you could provide would be much appreciated. Thanks a lot!","['statistics', 'variance', 'sampling']"
4346933,"Proving using definition that $f=(f_1,f_2,...,f_m)$ is differentiable at $c$ iff $f_i$'s are differentiable at $c$.","I want to prove the following statement:
Suppose that $S$ is a non-empty subset of $\mathbb R^n$ . Suppose that $f:S\to \mathbb R^m$ is differentiable at an interior point $c$ of $S$ . Suppose that $f=(f_1,...,f_m)$ , where $f_i:S\to \mathbb R$ for every $i\in \{1,2,...,m\}$ . Then $f$ is differentiable at $c\in S$ if and only if $f_i$ is differentiable at $c$ for every $i\in \{1,2,...,m\}$ . Definition of differentiability: $f$ is said to be differentiable at an interior point $c$ of $S$ if there exists $h>0$ and a linear transformation $T_c:\mathbb R^n\to \mathbb R^m$ such that for all $||v||<h$ the following holds: $f(c+v)-f(c)=T_c(v)+o(||v||), v\to 0$ . $T_c$ is also denoted by $f'(c)$ . I tried to prove it as follows: ( $\Leftarrow$ ) Suppose that $f_i$ 'are differentiable at $c$ . It follows that \begin{align*}
f_i(c+v)-f_i(c)-f_i'(c)v=o(||v||)
&\implies f_i(c+v)-f_i(c)-\nabla f_i(c).v=o(||v||)\\&\implies \sum_{i=1}^m\{f_i(c+v)-f_i(c)-\nabla f_i(c).v\}e_i=\sum e_i o(||v||)\\&\implies f(c+v)-f(c)-(\nabla f_1 (c).v, \nabla f_2(c).v,\cdots, \nabla f_m(c).v)=\color{red}{\sum e_i o(||v||)=o(||v||}\\&\color{blue}{\implies }f'(c) \text{ exists.}
\end{align*} The red part is due to the fact that: $\frac{\sum o(||v||)}{||v||}=(\sum e_i)o(1)\to 0$ as $v\to 0$ .
The implication highlighted in blue is true because $(\nabla f_1 (c).v, \nabla f_2(c).v,\cdots, \nabla f_m(c).v)=\color{green}{\begin {pmatrix} \nabla f_1 (c)\\\nabla f_2 (c)\\...\\ \nabla f_m (c)\end{pmatrix}}v$ . The matrix in green is an $m\times n$ matrix and hence satisfies differentiability definition. Now for the other direction: ( $\Rightarrow$ ) Suppose that $f$ is differentiable at $c$ so $\sum_{i=1}^m\{f_i(c+v)-f_i(c)-\color{purple}{\nabla f_i(c)}.v\}e_i=o(||v||)$ . For any $j\in \{1,2,...,m\}$ , the following holds: $|f_j(c+v)-f_j(c)-\nabla f_j(c).v|\leq \sqrt{\sum_{i=1}^m|f_i(c+v)-f_i(c)-\nabla f_i(c).v|^2}$ . It follows that: $\frac{|f_j(c+v)-f_j(c)-\nabla f_j(c).v|}{||v||}\leq o(1)$ . So taking $v\to 0$ , it follows that $f_j(c+v)-f_j(c)-\nabla f_j(c).v=o(||v||)$ . It follows by definition of differentiablity that $f_j$ is differentiable for every $j\in \{1,2,...,n\}$ . Is my proof correct? Thanks.","['multivariable-calculus', 'solution-verification', 'real-analysis']"
4346947,Cubic expectation of peanuts on squares,"There are 9 squares on the table, and we drop 9 peanuts to the table one at a time. Each peanut fall into every square with equal probability 1/9. After dropping all peanuts, we denote $x_{i}$ as the number of peanuts on the i-th square, i=1,2,3...9, and define Y = $x_{1}^{3}+x_{2}^{3}+...+x_{9}^{3}$ . Calculate expectation of Y. Updates: Based on linearity of expectation, permutation symmetry of $x_{i}$ and $x_{i}$ obeys Binomial distribution $$E[Y]=\sum_{i=1}^{9}{E[X_{i}^3]}=\sum_{i=1}^{9}{n(n-1)(n-2)p^3+3n(n-1)p^2+np}$$ Given n=9 and p=1/9 for binomial distribution, we can derive: $$E[Y]=9*(4+29/81)=39\frac{2}{9}$$ This answer should lie between $Y_{min}=9$ and $Y_{max}=9^3=729$ . For third order moment of binomial distribution, refer to The 3rd raw moment of a binomial distribution","['statistics', 'probability']"
4346971,"$\lim_{n\to\infty} n^2 \int_{0}^{1} \frac{x\sin{x}}{1+(nx)^3} \, \mathrm{d}x$","I have to calculate $$ \lim_{n\to\infty} n^2 \int_{0}^{1} \frac{x\sin{x}}{1+(nx)^3} \, \mathrm{d}x $$ I'm thinking of using the dominated convergence theorem. So I define $$f_n(x)=\frac{n^2 x \sin{x}}{1+(nx)^3}.$$ $f_n$ are measurable owing to the fact that they are continuous. Now, I have to calculate its limit. If I'm not wrong: $$\lim_{n\to\infty} \frac{n^2 x \sin{x}}{1+(nx)^3}
= (x\sin{x}) \lim_{n\to\infty} \frac{n^2}{1+(nx)^3}
= (x\sin{x}) \cdot 0
=0.$$ So $|f_n(x)|\le0$ and using the dominated convergence theorem: \begin{align*}
\lim_{n\to\infty} n^2 \int_{0}^{1} \frac{x\sin{x}}{1+(nx)^3} \, \mathrm{d}x
&=\lim_{n\to\infty} \int_{0}^{1} n^2 \frac{x\sin{x}}{1+(nx)^3} \, \mathrm{d}x \\
&= \int_{0}^{1} \lim_{n\to\infty} n^2 \frac{x\sin{x}}{1+(nx)^3} \, \mathrm{d}x \\
&= \int_{0}^{1} 0 \, \mathrm{d}x =0
\end{align*} Is it ok?","['lebesgue-measure', 'lebesgue-integral', 'functions', 'limits', 'pointwise-convergence']"
4346978,"Let $Y_n,X_n$ be a sequence of r.v. Does $\sup_z \Big|P(Y_n<z\ |\ X_n) - \Phi(z)\Big|\rightarrow_p0 \implies \{Y_n|X_n=x_n\}\xrightarrow[]{d}N(0,1)?$","Let $Y_n,X_n$ be a sequence of random variables. Does $\sup_z \Big|P(Y_n<z\ |\ X_n) - \Phi(z)\Big|\xrightarrow[]{p}0$ implies $\{Y_n|X_n=x_n\}\xrightarrow[]{d}N(0,1)?$ where $x_n$ is any sequence of real numbers within the support of $X_n$ . Note that $P(Y_n<z\ |\ X_n)$ is a random variable. By asymptotic equivalence, the above implies for all value of $z$ , $P(Y_n<z\ |\ X_n)\xrightarrow[]{d}\Phi(z)$ which implies $P(Y_n<z\ |\ X_n)\xrightarrow[]{p}\Phi(z)$ for all $z$ . This is as far as I could go. This question is closely related to this other question .","['statistics', 'probability-distributions', 'real-analysis', 'probability-theory', 'probability']"
4346986,Is there any website/software that let's me compare two mathematical statements and let me know if both are equivalent?,"Sometimes when I am doing my maths homework I sometimes get confused and then decide to make two mathematical statements and then plug some numbers in them to see if the results match.
For example : $$\frac{\sqrt{x}}x = \frac{1}{\sqrt{x}}$$ Is there any website/software that lets me type the left hand side and the right hand side of the above equation and then tells me if these two are equivalent statements or not(like telling me in the form of true and false)
I searched but was unable to find any, so if you know it then the answer would be appreciated.
Thnx in advance.
Note: Since I am an Idiot and Newbie I was unable to find the relevant tags for the question.","['calculus', 'algebra-precalculus']"
4347000,"How can we adapt the Ito's formula if $g \in C^{2}(\mathbb R\setminus \{ x_{1},...,x_{n}\})$ and $g^{''} \leq M$","Let $B$ be Brownian motion. Consider $g:\mathbb R \to \mathbb R$ that is $C^{2}$ except for some exceptional set $\{ x_{1},...,x_{n}\}\subseteq \mathbb R$ . How can we adapt the Ito's formula if $g^{''} \leq M$ , for $M > 0$ , to show that $$g(B_{t}) = g(B_{0}) + \int_{0}^{t} g^{\prime}(B_{s})dB_{s}+\frac{1}{2}\int_{0}^{t}g^{\prime\prime}(B_{s})ds \; (*)$$ Due to density arguments we can find $(f_{k})_{k \in \mathbb N} \subseteq C ^{2}$ such that: $f_{k} \to g, \; (f_{k})^{\prime} \to g^{'},\; \text{ and }(f_{k})^{\prime\prime} \to g^{''} $ uniformly such that $(f_{k})^{\prime\prime}\leq M$ My attempt: Note that by Ito's formula: $$f_{k}(B_{t}) = f_{k}(B_{0}) + \int_{0}^{t} f_{k}^{\prime}(B_{s})dB_{s}+\frac{1}{2}\int_{0}^{t}f_{k}^{\prime\prime}(B_{s})ds \; $$ Clearly a.s. for all $t \geq 0$ , $\;f_{k}(B_{t})\to g(B_{t})$ and $f_{k}(B_{0})\to g(B_{0})$ Now we need to show that a.s. $\int_{0}^{t} f_{k}^{\prime}(B_{s})dB_{s}\to \int_{0}^{t}g^{\prime}(B_{s})dB_{s}$ in $L^{2}$ ( I thought it would be a.s. be the stochastic integral lacks pathwise interpretability) $$ \mathbb E \left[\left(\int_{0}^{t} f_{k}^{\prime}(B_{s})dB_{s}- \int_{0}^{t} g^{\prime}(B_{s})dB_{s}\right)^{2}\right]= \mathbb E \left[\int_{0}^{t} \left(f_{k}^{\prime}(B_{s})- g^{\prime}(B_{s})\right)^{2}ds\right]$$ and then use dominated convergence to let it go to zero. Now onto proving $\int_{0}^{t}f_{k}^{\prime\prime}(B_{s})ds\to\int_{0}^{t}g^{\prime\prime}(B_{s})ds$ . Here I am not sure in what convergence sense we should prove (convergence in $L^{2}$ or convergence almost surely, since the Lebesgue-Stieltjes integral does have pathwise interpretation.) I suggest a.s. convergence: $\rvert\int_{0}^{t}f_{k}^{\prime\prime}(B_{s})ds-\int_{0}^{t}g^{\prime\prime}(B_{s})ds\lvert \leq \int_{0}^{t} \lvert f_{k}^{\prime\prime}(B_{s})-g^{\prime\prime}(B_{s})\rvert ds $ using the fact that $\lvert f_{k}^{\prime\prime} \lvert ,\; \rvert g^{\prime\prime}\rvert\leq M$ , we use dominated convergence again to get the result. Questions: If the above is correct, we have one term in $(*)$ as a limit in $L^{2}$ while the other terms are a.s. limits. How can we include all of these ""different"" limits in the equality?","['stochastic-integrals', 'stochastic-analysis', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4347001,Removal of an arbitrary point of the boundary of a closed and connected $A\subseteq\Bbb R^2$ so the new set remains connected,"Prove the following statement or find a counterexample: Let $A\subseteq\Bbb R^2$ be a closed and connected set. Then, $\exists c\in\partial A$ s. t. $A\setminus\{c\}$ is still connected. I think I found some counterexamples: $x$ or $y$ axis or any other line in $\Bbb R^2,$ as well as graphs of unbounded continuous functions defined on an open interval $I\subseteq\Bbb R$ or graphs of continuous functions defined on the whole $\Bbb R.$ Question : Is the unboundedness necessary for the statement not to hold?","['connectedness', 'examples-counterexamples', 'real-analysis']"
4347037,Proving that equivalent atlases form an equivalence relation?,"I know that this question was already asked here , but I'm afraid I cannot really follow the answers (which are given by the OP her- or himself). So, here is a definition of equivalent atlases: Two atlases $\mathcal A$ and $\mathcal B$ on $M$ are called equivalent if $\mathcal A \cup \mathcal B$ is an atlas on $\mathcal M$ . And here a definition of atlas : Let $M$ be a second countable Hausdorff topological space. An $n$ - dimensional smooth atlas on $M$ is a collection of maps $$\mathcal A = \left\{ \left(\varphi_i, U_i\right) \mid i\in A\right\}, \quad \varphi_i: U_i\rightarrow \varphi_i(U_i)\subset \mathbb R^n,$$ such that all $U_i \subset M$ are open, all $\varphi_i$ are homeomorphisms, and $\{U_i, i\in I\}$ is an open covering of $\mathcal M$ $\varphi_i\circ \varphi_j^{-1}: \varphi_j\left(U_i\cap U_j\right)\rightarrow \varphi_i\left( U_i\cap U_j\right)$ are smooth for all $i, j\in I$ . Let $\mathcal A = \{(\varphi_i, U_i)\mid i\in A\}$ , $\mathcal B = \{(\psi_i, V_i)\mid i\in B\}$ and $\mathcal C = \{(\chi_i, W_i)\mid i\in C\}$ be atlases on the same set $M$ . Now, if $\mathcal A$ is an atlas, then obviously, $\mathcal A\cup \mathcal A = \mathcal A$ is also an atlas. $\mathcal A\cup \mathcal B$ is an atlas if and only if $\mathcal B \cup \mathcal A$ is an atlas. The tricky part is proving transitivity. Let $\mathcal A \cup \mathcal B$ be an atlas (i.e. the transition functions $\varphi_i\circ \psi_j^{-1}: \psi_j\left(U_i \cap V_j\right)\rightarrow\varphi_i\left(U_i\cap V_j\right)$ are smooth $\forall i\in A$ , $j\in B$ ), and let $\mathcal B\cup \mathcal C$ be an atlas (i.e. the transition functions $\psi_j\circ \chi_k^{-1}: \chi_k\left(V_j \cap W_k\right)\rightarrow\psi_j\left(V_j\cap W_k\right)$ are smooth $\forall j\in B$ , $k\in C$ ). For $\mathcal A\cup \mathcal C$ to be an atlas, we need to show that $$\varphi_i\circ \chi_k^{-1}: \chi_k\left(U_i \cap W_k\right)\rightarrow \varphi_i\left(U_i \cap W_k\right)\quad \text{smooth}\ \forall i\in A, k\in C.$$ My idea to prove this was sth like this: $$\varphi_i\circ \chi_{k}^{-1} = (\varphi_i\circ\psi_j^{-1})\circ \left(\psi_j \circ \chi_k^{-1}\right) \forall i\in A, j\in B, k\in C.$$ However, this raises the question of the well-definedness of the RHS. The codomain of $\chi_k^{-1}$ is $W_k$ , whereas the domain of $\psi_j$ is $V_j$ . How can we resolve this?","['manifolds', 'differential-geometry']"
4347045,$ \frac{(1+o_p(1) )A_n}{(1+o(1) )E[A_n]}=\frac{A_n}{E[A_n]}+ o_p(\frac{A_n}{E[A_n]}) ??$,"I was reading a paper and in it, they stated for a random sequence $ A_n=\Theta(1)$ , $$ \frac{(1+o_p(1) )A_n}{(1+o(1) )E[A_n]}=\frac{A_n}{E[A_n]}+ o_p(\frac{A_n}{E[A_n]}) ??$$ without any proof. Although seemingly intuitive, I couldnt come up with a proof, and this actually is a crucial step, so I wanted to make sure. Also, if possible, can you tell me if there is a easy way to estimate the precise order of $ o_p(\frac{A_n}{E[A_n]}) $ ,other than actually calculating $ \frac{(1+o_p(1) )A_n}{(1+o(1) )E[A_n]}-\frac{A_n}{E[A_n]}$ ,which is quite tedious in this case. Thanks beforehand.","['notation', 'statistics', 'probability-limit-theorems', 'probability']"
4347048,Irreducible plane curve of degree $d>2$ with a point $P$ of multiplicity $d-1$ is rational,"We have to prove that an irreducible plane curve $X$ of degree $d>2$ with a point $P$ of multiplicity $d-1$ is rational and we need to find a resolution of this $X$ . To prove that $X$ is rational, we can construct a birational map $\mathbb{P}^1 \mapsto X$ , using the point $P$ . However I have no clue on how to find this birational map. Also to find a resolution of this curve, I have no idea on how to start.
Does someone know to proceed?",['algebraic-geometry']
4347085,Number of ways in which letters of the word ENGINEER can be arranged so that no two alike letters are together,"Number of ways in which letters of the word ENGINEER can be arranged so that no two alike letters are together is ________ My solution is as follows: As per the figure number Es=3;G=1;I=1;Ns=2 and R=1, total word is 8 Number of arrangement of the word ""ENGINEER"" is $3360$ where $\frac{{8!}}{{3!.2!}} = 3360$ Now let's take the case that our focus is on E and neglect N initially. Remove Case of the type EE,E,G,I,N,N,R is $\frac{{7!}}{{2!}} = 2520$ Now add cases when EEE,G,I,N,N,R is $\frac{{6!}}{{2!}} = 360$ Hence the number of ways Es are segregated is $3360-2520+360=1200$ . There are number of cases when E are segregated. Now do we segregate N also?",['combinatorics']
4347120,Condition for martingale,"What are the conditions for the function that is two times differentiable and all partial derivatives existing $f$ : $$f(B(t)^2,t)$$ to be a martingale? I am able to show the conditions for $f(B(t),t)$ to be a martingale: this requires that $1/2f_{xx}(B(t),t)$ and $f_t(B(t),t))$ have to be zero, to ensure we don't have $dt$ terms. But how does it work for $f(B(t)^2,t)$ ?","['stochastic-processes', 'martingales', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4347127,Maximally unique beach volleyball games,"There are 24 players, 12 men and 12 women. A team is a set of 1 man and 1 woman. A (beach volleyball mixed) game is a set of 2 different teams, i.e. players are unique in those teams. Let's denote men from 1 to 12 and women from A to L. 1A2B is a game example, man 1 with woman A play versus man 2 with woman B. A round is a set of 6 unique games, i.e. players are unique in those games. For example the first round can be: 1A2B
3C4D
5E6F
7G8H
9I10J
11K12L We'd like to have as unique games as possible, i.e. maximize the unique partners and enemies a player has during games. How to generate 5 more rounds? If it's not possible to generate 5 more rounds with totally unique games, we give more priority to the unique partners than enemies: a penalty for repeated partner is 2, a penalty for repeated opponent is 1. Here is the scoring script and exemplary invocation . For example the next 5 rounds could be the same as the first one, but the uniqueness score would be very low, -480, since 24 players would repeat partners (-2) and both opponents (-1 and -1) in 5 rounds, so 5 times -96.","['graph-theory', 'optimization', 'combinatorics', 'combinatorial-designs']"
4347133,"Do these ""hyper-discontinuous"" functions exist?","Suppose $\ X\subset \mathbb{R}\ $ and $\ Y\subset \mathbb{R}.$ Definition: A function $\ f: X \to Y\ $ is hyper-discontinuous if for every $\ x\in X,\ \ \exists\ \delta>0,\ \varepsilon>0\ $ such that $\ y\in X \setminus \{x\},\  \vert y-x \vert < \delta,\ \implies \vert f(x) - f(y) \vert \geq \varepsilon.$ Hyper-discontinuous is a term I just made up, but it seems appropriate, because $\ f\ $ is hyper-discontinuous implies $\ f\ $ is nowhere continuous, whereas the converse must be false, and nowhere continuous is the concept most closely relating to ""most discontinuous function"" that I am aware of. My questions are the following: Is there a hyper-discontinuous function $\ f:[0,1] \to [0,1]\ ?$ Is there a hyper-discontinuous function $\ f:[0,1]\cap\mathbb{Q} \to [0,1]\cap\mathbb{Q}\ ?$ I've spent a while on these questions, but keep getting confused. I think Blumberg's theorem might be related to this, but I'm not sure.","['continuity', 'general-topology', 'recreational-mathematics', 'real-analysis']"
4347182,Why can I square both sides?,"I am not used to English. I ask for your understanding in advance. There is the equation: $ x= 2^\frac{1}{2}$ we can square both side like this: $ x^2= 2$ But I don't understand why that it's okay to square both sides. What I learned is that adding, subtracting, multiplying, or dividing both sides by the same thing is okay. For example: $ x = 1 $ $ x-1 = 1-1 $ $ x-1 = 0 $ $ x \times 2 = 1 \times 2 $ $ 2x = 2 $ like this. But how come squaring both sides is okay too? $ x = 2 $ $ x \times 2 = 2 \times 2 $ $ 2x = 4 $ $ 2x \times x = 4 \times x $ This does not induce it. Can you answer this silly question?",['algebra-precalculus']
4347222,Is the set of positive-definite symmetric matrices open in the set of all matrices?,"So if $M_{n\times n}(\Bbb R)$ is the set of square real matrix of order $n$ then it is well know that the position $$
\langle A,B\rangle:=\text{tr}(AB^T)
$$ defines an inner product on $M_{n\times n}(\Bbb R)$ so that this set is a topological vector space. Now I'd like to prove that the subset $$
\text{Sym}_{n\times n}^+(\Bbb R):=\big\{A\in M_{n\times n}(\Bbb R):A=A^T\wedge \vec v\cdot A\vec v>0\,\,\text{for any }\vec v\in\Bbb R^{n}\setminus\{0\}\big\}
$$ is open. So first of all I observed that if $\text{Sym}_{n\times n}(\Bbb R)$ and $M^+_{n\times n}(\Bbb R)$ and are the sets of symmetric matrices and the set of positive definite matrices, that is $$
\text{Sym}_{n\times n}(\Bbb R):=\big\{A\in M_{n\times n}(\Bbb R):A=A^T\big\}\\
M^+_{n\times n}(\Bbb R):=\big\{A\in M_{n\times n}(\Bbb R):\vec v\cdot A\vec v>0\,\,\text{for any }\vec v\in \Bbb R^n\setminus\{0\}\big\}
$$ then $$
\text{Sym}_{n\times n}^+(\Bbb R)=\text{Sym}_{n\times n}(\Bbb R)\cap M_{n\times n}^+(\Bbb R)
$$ so that the result would follow immediately showing that $M_{n\times n}^+(\Bbb R)$ is an open set disjoint form the boundary of $\text{Sym}_{n\times n}(\Bbb R)$ because this last is a linear subspace and any linear subspace of a finite dimensional t.v.s is closed so that in this case the above intersection is exactly equal to the intersection $$
\operatorname{int}\Big(\text{Sym}_{n\times n}(\Bbb R)\Big)\cap M_{n\times n}^+(\Bbb R)
$$ Now let be $\varphi:M_{n\times n}(\Bbb R)\rightarrow\Bbb R^{n\cdot n}$ the function defined through the equation $$
\varphi(X):=(X_1,\dots ,X_n)=\big((x_{1,1},\dots,x_{1,n}),\dots,(x_{n,1},\dots,x_{n,n})\big)
$$ for any $X\in M_{n\times n}(\Bbb R)$ and thus we observe that it is an isomorphism (moreover it is not hard to show that it is an isometry) and thus a homeomorphism becuase any ismorphism between finite dimensional t.v.s is an homeomorphism. So the statement can be proved showing that $\varphi\Big[M_{n\times n}^+(\Bbb R)\Big]$ is open and disjoint form the boundary of $\varphi\Big[\text{Sym}_{3\times 3}(\Bbb R)\Big]$ but unfortunately I was not able to prove this; alternatively the statement can be proved aslo showing that $\varphi\Big[\text{Sym}_{n\times n}^+(\Bbb R)\Big]$ but I yet was not be able to do this. Finally I have to point out that here I found a question where is exatly ask to prove what I ask now but unfortunately it seems it is only proved that the set of positive matrices is open: indeed no answers there was accepted. Anyway I would not mind now that even here it proves that the set of positive matrices is open: indeed in the Ovi's answer there is something is not clear. So could someone help me, please?","['positive-semidefinite', 'topological-vector-spaces', 'functional-analysis', 'symmetric-matrices', 'general-topology']"
4347286,"Computing the expected size of the largest connected component in a ""hitomezashi graph"" (described in the question body)","A while ago there was a numberphile video about a certain graph you can build based on Hitomezashi Sashiko , a kind of decorative mending. Intuitively, we alternate putting walls (originally stitches) in each column/row, which separate the integer lattice into components. $s_1$ governs the positions of the vertical walls and $s_2$ governs the positions of the horizontal walls. A $0$ tells us to start ""at the edge"" and a $1$ tells us to start ""off the edge"". For example, choosing $s_1 = 01101$ and $s_2 = 10110$ gives the following stitch pattern. If we think of each unit square as a vertex, with adjacent cells connected whenever there isn't a wall separating them, we get a graph, $H(s_1,s_2)$ : If it's still unclear how the walls work, it's made clear in the first few minutes of the linked numberphile video. For those looking for a precise description, formally, we build a graph $H(s_1, s_2)$ given two binary strings (say of length $n_1$ and $n_2$ ) as follows $[n_1] \times [n_2]$ is the vertex set ( $0$ indexed) $(x,y) \sim (x+1,y)$ whenever $y \not \equiv s_1[x+1] \pmod{2}$ $(x,y) \sim (x,y+1)$ whenever $x \not \equiv s_2[y+1] \pmod{2}$ I also have a demo on my blog where you can input binary strings and it will output a picture of the graph. Now, if we do this with longer binary strings, we get some quite intricate pictures: and there are some natural questions to ask. I've put a fair amount of my thoughts about these problems in a different blog post , but here is the one I'm primarily interested in: Say we (uniformly) randomly choose two binary strings of a fixed length $n$ . What is the expected size of the largest connected component of $H(s_1, s_2)$ as a function of $n$ ? I'm not much of a probabilistic combinatorialist, so I pretty quickly exhausted my personal bag of tricks for attacking these kinds of problems. But this feels like something that somebody knows how to answer (and ideally, something somebody would enjoy answering ^_^). I recognize this is probably hard to answer, so I'm open to partial progress. In particular, I wrote some sage code to get some data, and here's a graph of the average maximum region size (across a few hundred samples) as a function of $n$ : The blue curve is the polynomial of best fit, which turns out to be $\approx 1.95 n^{1.38}$ This brings us to some (hopefully easier) problems: Can we show that the expected size of the largest component is $o(n^2)$ ? What about $o(n^{1.5})$ ? Is it possible to pin down the exponent exactly? Can we get lower bounds too? You can find more of my thoughts, as well as some code for simulating these things yourself if you're interested, in my blog post here . I'm open to hearing any thoughts that people have about this, because I really have no idea how to proceed. Edit (Jan 5): Based on the new data from Daniel Mathias, it seems a good conjecture is $\frac{8}{3} n^{4/3}$ . I've added a $50$ rep bounty as thanks, and afterwards I'll add a $100$ rep bounty for a proof of this conjecture (or some substantial progress). I need to wait $24$ hours before I can post that second bounty, though. Thanks in advance! ^_^","['graph-theory', 'combinatorics', 'probability']"
4347288,The Intuition of the Construction of a Non-Measurable Set (Vitali Set) on the Real Line,"I suppose the question could be stated another way: if you were asked to construct a non-measurable set the first time in the history, what would motivate the construction to a Vitali set? For a construction of the Vitali set, I followed the post posted here: The construction of a Vitali set .","['measure-theory', 'lebesgue-measure', 'intuition', 'real-analysis']"
4347298,The Lebesgue $\sigma$-algebra $L(\mathbb{R}^n)$ is the completion of the Borel $\sigma$-algebra $B(\mathbb{R}^n)$,"I came across the following proof of the completion of Borel $\sigma$ -algebra to $\sigma$ -algebra comprised of Lebesgue measurable sets which I cannot understand quite clearly. Can anyone elaborate this proof in a detailed manner and what is the whole point of the proof? Theorem 2.28. The Lebesgue $\sigma$ -algebra $L(\mathbb{R}^n)$ is the completion of the Borel $\sigma$ -algebra $B(\mathbb{R}^n)$ . Proof . Lebesgue measure is complete. If $A \subset \mathbb{R}^n$ is Lebesgue measurable, then there is an $F_\sigma$ set $F \subset A$ such that $M = A \setminus F$ has Lebesgue measure zero. It follows by the approximation theorem that there is a Borel set $N \in G_\delta$ with $\mu (N) = 0$ and $M \subset N$ . Thus, $A = F\cup M$ where $F \in B$ and $M \subset N \in B$ with $\mu (N ) = 0$ , which proves that $L(\mathbb{R}^n)$ is the completion of $B(\mathbb{R}^n)$ .","['measure-theory', 'borel-measures']"
4347305,Show that $X_{t} = \sqrt{c}e^{-\lambda t}\beta_{e^{2\lambda t}}$ solves $dX_{t}=\sqrt{2c\lambda}dB_{t}-\lambda X_{t}dt$,"Let $(\beta_{t})$ and $(B_{t})$ be standard Brownian motions. How can I show that show that $X_{t} = \sqrt{c}e^{-\lambda t}\beta_{e^{2\lambda t}}$ solves $dX_{t}=\sqrt{2c\lambda}dB_{t}-\lambda X_{t}dt$ ? My attempt: $$ dX_{t}=d\left(\sqrt{c}e^{-\lambda t}\beta_{e^{2\lambda t}}\right)=\sqrt{c}d(e^{-\lambda t}\beta_{e^{2\lambda t}})=\sqrt{c}\left( e^{-\lambda t}d(\beta_{e^{2\lambda t}})+\beta_{e^{2\lambda t}}d(e^{-\lambda t})\right)$$ Now clearly $d(e^{-\lambda t})=-\lambda e^{-\lambda t }dt$ . But I do not know how to compute $d(\beta_{e^{2 \lambda t}})$ .
Personally, I want to think of $ \beta_{e^{2 \lambda t}}$ and $\beta_{g(t)}=f(t,\beta_{t})$ which is $C^{2}$ , but it seems very suspicious.","['stochastic-integrals', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4347326,"if $\cos^{10}x + \sin^{10}x=11/36$, find $\cos^{12}x+\sin^{12}x$","If $\cos^{10}x+ \sin^{10}x=11/36$ , find $\cos^{12}x+\sin^{12}x$ . I've tried solving using index manipulation but no way. I think we're to use $\sin^2 x+\cos^2 x=1$ . But I don't know how. Need help please.",['trigonometry']
4347360,"If $(f_n')$ converges uniformly on an interval, does $(f_n)$ converge?","Let $(f_n)$ be a sequence of functions that are all differentiable on an interval A, and suppose the sequence of derivatives $(f_n')$ converges uniformly on A to a limit function $g$ . Does it follow that $(f_n)$ converges to a limit function f on A? What I tried: As $(f_n')$ converges uniformly to $g$ , we may write that the limit of integral of $(f_n')$ is the integral of the limit of $(f_n')$ . Hence, $(f_n)$ converges point wise to the integral of $g$ . How does this sound?","['sequence-of-function', 'pointwise-convergence', 'uniform-convergence', 'real-analysis']"
4347383,Finite coverings and Tangent bundles,"Motivation: Apparently the tangent bundle of real projective space is trivial if and only if the tangent bundle of it's universal cover (the sphere) is trivial. That is, $ n=1,3,7 $ . Does that follow from a general fact trivial tangent bundle of universal cover implies triviality if the covering is finite? Let $ M $ be a manifold and $ M' $ it's universal cover. Then what is the relationship between the tangent bundle $ T(M) $ and the tangent bundle $ T(M') $ ? First it seems that $ T(M') $ should be the universal cover of $ T(M) $ . Indeed the tangent bundle is homotopy equivalent to the base space so $ T(M') $ must be simply connected. What else can we say? Is there any relationship between the characteristic classes of $ T(M) $ and $ T(M') $ ? And how about triviality? Every flat or hyperbolic manifold has a contractible universal cover and thus $ T(M')$ is the trivial bundle. Yet many flat and hyperbolic manifolds are not even orientable let alone parallellizable.","['smooth-manifolds', 'covering-spaces', 'differential-topology', 'characteristic-classes', 'differential-geometry']"
4347395,Midpoints and other constraints in triangle,"I'm trying to solve this problem: I can intuitively observe that $\angle IFH = 2\angle NMH$ , but don't know how to prove it. I extended segment $NM$ until it intersects the (extended) segment $IF.$ Let's call the intersection point $P.$ If I could prove that newly created triangle FMP is isosceles $(FM = FP)$ , then this would prove that $\angle IFH = 2\angle NMH. $ I even did a simulation in Fusion 360 ( video here ) and it's interesting to see how the constraints work together to keep $FM = FP.$","['euclidean-geometry', 'triangles', 'geometry']"
4347433,Why $f(x) = x^2$ has variable derivative but its tangent has constant slope?,"I'm taking Brilliant.org's calculus course, and I'm on the section called The Derivative . My (mis)understanding: A tangent line is a linear function that grazes a point, $a$ , on the graph of a different function. The slope of that tangent line is the instantaneous rate of change at $a$ . The way you find this is by taking the difference ratio, giving you the slope, and plugging in equal values for the input points. So, for the function $f(x) = x^2$ , the slope would be: $$\require{cancel} \frac{b^2 - a^2}{b-a} = \frac{\cancel{(b-a)}(b+a)}{\cancel{b-a}} = b+a$$ Now, considering the case as the difference between $b$ and $a$ gets smaller, or equivalently, as the secant line's two points become one and the same point, thus turning it to a tangent line: $$\lim_{b \rightarrow a} b +a = 2b = 2a$$ Thus, the derivative of the square function is found, $f'(x) = 2x$ . The slope of the tangent is $2x$ , but apparently, the derivative isn't the slope of the tangent, it is only so the other way around (see this Math.SE answer ). I could define any number of functions of the form $g(x) = ax$ , which would be tangent to or intersect the function $f(x)=x^2$ , when $x=a$ . For any of these functions, $g'(x) = a$ , as per the difference ratio. The slopes of these functions would be any arbitrary number $a$ , not $2x$ . So, it seems we're dealing with the slope of one particular tangent. However, I can't see how this tangent line is linear. If it is linear, its slope is a constant, but $2x$ contains a variable. All of this leads me to think that we're talking about the tangent line in a very different sense than the one of ""a linear line that grazes the function's graph"". But I've only been made familiar with the latter sense, and this mysterious sense is very ill-defined in my head. What is it and what does it have to do with tangent lines? The trick with morphing a secant line into a tangent line doesn't really explain this, because one can do that anywhere on the graph and wind up with tangent lines that have differing slopes, none of which are equal to $2x$ .","['tangent-line', 'calculus', 'slope', 'derivatives', 'terminology']"
4347447,"Given an ergodic property that guarantees convergence of sample means to an expectation, how can I bound the Cesàro Mean of expectation of terms?","I have a sequence of (not iid) random variables $\{E_{i}\}$ that converge in distribution (actually, in total variation) as well as in $\mathcal{L}^{2}$ to $E_{\infty}$ . For all nonnegative functions $f$ with $\mathbb{E}[f(E_{\infty})]<\infty$ , it's established that \begin{align}
\lim_{T\rightarrow\infty}\frac{1}{T}\sum_{n=1}^{T}f(E_{n}) \overset{\mathrm{a.s.}}{=}\mathbb{E}[f(E_{\infty})].
\end{align} I would like to bound (again for a nonnegative $f$ ) \begin{align}
\underset{T\rightarrow\infty}{\lim\sup}\text{ }\frac{1}{T}\sum_{n=1}^{T}\mathbb{E}[f(E_{n})].
\end{align} In my particular case, I've been unable to uniformly bound the random variables $f(E_{i})$ , so I cannot use, for example, the reverse Fatou's Lemma to interchange the limit and expectation to show something like $\underset{T\rightarrow\infty}{\lim\sup}\text{ }\mathbb{E}[\frac{1}{T}\sum_{n=1}^{T}f(E_{n})] \le \mathbb{E}[\underset{T\rightarrow\infty}{\lim\sup}\text{ }\frac{1}{T}\sum_{n=1}^{T}f(E_{n})]$ . Does anyone have any advice for how I could try to proceed-- in particular ideas for how to interchange the limit and expectation without some kind of uniform bound? The thing is that I don't much about the function $f$ . I do know some potentially relevant facts. I can prove that $\mathbb{E}[f(E_{n})]$ is always finite.","['measure-theory', 'probability-limit-theorems', 'ergodic-theory', 'probability-theory', 'probability']"
4347505,Evaluate $\lim_{n \to \infty} \left(\int_0^1 e^{-x^2/n} dx\right)^n$,"Evaluate $\lim_{n \to \infty} \left(\int_0^1 e^{-x^2/n} dx\right)^n$ . I've tried this: from Taylor's expansion at $t_0=0$ of the function $e^t$ , I get that for any $t \in \mathbb{R}$ it is $e^t \ge 1+t$ . Moreover, for the Lagrange's remainder of the Taylor's expansion there exists $c(t)$ on the segment of endpoints $0$ and $t$ such that $$e^t=1+t+\frac{t^2}{2}+\frac{t^3}{6}e^{c(t)}$$ When $t \le 0$ , it is $\frac{t^3}{6}e^{c(t)}\le0$ ; hence for $t \le 0$ it is $e^t\le1+t+\frac{t^2}{2}$ . Since $-\frac{x^2}{n} \le 0$ for any $x\in[0,1]$ and for any $n\in\mathbb{N}$ , it is $$1-\frac{x^2}{n}\le e^{-x^2/n} \le 1-\frac{x^2}{n}+\frac{x^4}{2n^2}$$ For monotonicity of integral, integrating in the inequality in the interval $[0,1]$ it is $$1-\frac{1}{3n} \le \int_0^1 e^{-x^2/n} dx\le1-\frac{1}{3n}+\frac{1}{10n^2}$$ Since $1-\frac{1}{3n} \ge 0$ for any $n\in\mathbb{N}$ , for the monotonicity of the $n$ -th power it is $$\left(1-\frac{1}{3n}\right)^n \le \left(\int_0^1 e^{-x^2/n} dx\right)^n \le \left(1-\frac{1}{3n}+\frac{1}{10n^2}\right)^n$$ Since the limit preserves non strict inequalities, it is $$\lim_{n\to\infty} \left(1-\frac{1}{3n}\right)^n\le \lim_{n \to \infty} \left(\int_0^1 e^{-x^2/n} dx\right)^n \le \lim_{n\to\infty} \left(1-\frac{1}{3n}+\frac{1}{10n^2}\right)^n$$ So, since for $n\to\infty$ it is $\left(1-\frac{1}{3n}\right)^n \to e^{-1/3}$ and $\left(1-\frac{1}{3n}+\frac{1}{10n^2}\right)^n \to e^{-1/3}$ , for the squeeze theorem it follows that $$\lim_{n \to \infty} \left(\int_0^1 e^{-x^2/n} dx\right)^n=e^{-1/3}$$ Could this work? I am unsure about the use of the Taylor formula for $t \le 0$ and the various monotonicity arguments I made.","['integration', 'limits', 'solution-verification', 'analysis']"
4347509,Can A function grow quicker than another but never catch up to it,"I know this is a bit of a strange question, but it is one that has been on my mind for a little bit now. If we have two real valued functions f(x) and g(x) and they are both everywhere continuous and differentiable, could we choose f(x) and g(x) such that three conditions are satisfied f(x) > g(x) for all x in R g'(x) > f'(x) for all x in R f(x) and g(x) both diverge as x tends to infinity Essentially, I'm asking if g(x) can grow infinitely but never catch up to f(x)? I know this problem would be simple if f(x) and g(x) converged, but I am curious about if these criterion could be met while maintaining divergence. My initial thought is that no such functions exist, but I am not sure how I could prove that.  If such functions did exist, I suspect g(x) would need to be ln(x) or something of the like, but I am not convinced it is possible for these criterion to all be met. If anyone has insight either way, that would be extremely helpful.","['calculus', 'real-analysis']"
4347510,$\int_{-1}^{1} \log\left(\frac{1+x}{1-x} \right) \frac{1}{1-ax} dx.$,"Definite Integral: $$\int_{-1}^{1} \log\left(\frac{1+x}{1-x} \right) \frac{1}{1-ax} dx,$$ where $0 < a <1$ . I tried with integration by parts taking $\log\left(\frac{1+x}{1-x} \right) $ as the first function and $\frac{1}{1-ax} $ as the second function, but it did not work. Need some help to compute it.","['integration', 'calculus', 'real-analysis']"
4347599,Intersection of two families of sets,"I got stuck while reading through some proof involving set operations. Define $P = \bigcup_{j \in A} S_j\cup (S_1 - \{x\})$ and $Q = \bigcup_{j \in B} S_j\cup (S_1 - \{y\})$ , where $x,y \in S_1$ and $A,B \subseteq \{2,3,...,m\}$ . Then $$|P \cap Q| \geq \left| \bigcup_{j \in A \cap B} S_j \right|.$$ I started with $$|P \cap Q| \geq \left| \left( \bigcup_{j \in A}S_j \right) \cap \left( 
\bigcup_{j\in B} S_j \right)\right|$$ from the distributive property of set intersection. I don't know how to proceed from here...","['elementary-set-theory', 'discrete-mathematics']"
4347625,How to calculate or estimate the area of such implicit region about function $x^{\frac{1}{x+\frac{1}{x}}}$,"How to estimate or calculate the area enclosed by the implicit equation. $$x^{\frac{1}{x+\frac{1}{x}}}+y^{\frac{1}{y+\frac{1}{y}}}=\mathrm{e}$$ It is possible to prove that the area is between $13$ and $15$ ? I draw the curve by Asymptote( http://asymptote.ualberta.ca/ ). import graph;
import contour;
size(200);
real f(real x){return x^(1/(1/x + x));};
real g(real x, real y) {return f(x)+f(y);}
draw(contour(g,(.1,.1),(8,8),new real[]{exp(1)},operator ..),blue+2);
xaxis(""$x$"",BottomTop,LeftTicks);
yaxis(""$y$"",LeftRight,RightTicks);","['integration', 'multivariable-calculus', 'area']"
4347666,How do I prove that this function is a measure?,"I have the following problem: Let $(\Omega, \mathfrak{A}, \mu)$ be a measurespace and let $f:\Omega \rightarrow [0,+\infty]$ be a nonegative simple function, i.e. $f(\Omega)=\{b_1,...b_s\}$ . We define for $a\in \mathfrak{A}$ $$\int_A fd\mu=\int_\Omega \chi_A f d\mu$$ . Show that the function $$A\mapsto \int_A fd\mu$$ is a measure. My idea was the following: Proof We need to check the following two points. First it is easy to see that $$\emptyset \mapsto \int_\emptyset f d\mu=0$$ But now let $I$ be a countable index set and let $\{A_i\}_{i\in I}\in \mathfrak{A}$ such that $A_i \cap A_j=\emptyset$ if $i\neq j$ . Now consider $$\bigcup_{i=1}^\infty A_i \in \mathfrak{A}$$ and look at $$\int_{\bigcup_{i=1}^\infty A_i} f d\mu=\int_\Omega f \chi_{\bigcup_{i=1}^\infty A_i} d\mu\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(1)$$ Let us remark that $$\chi_{\bigcup_{i=1}^\infty A_i}(x)\leq \sum_{i=1}^\infty \chi_{A_i} (x)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(2)$$ but since the $A_i$ 's are disjoint we have indeed $$\chi_{\bigcup_{i=1}^\infty A_i}=1 \Leftrightarrow x\in A_i$$ for some $i\in I$ . Therefore from this fact we have equality in $(2)$ .Then we have $$\int_\Omega f \chi_{\bigcup_{i=1}^\infty A_i} d\mu=\int_\Omega f \cdot \sum_{i=1}^\infty \chi_{A_i}d\mu=\int_\Omega \left(\sum_{i=1}^\infty f\chi_{A_i}\right) d\mu$$ . But now since we can swap the integrals and the sum we get $$\int_\Omega \left(\sum_{i=1}^\infty f\chi_{A_i}\right) d\mu=\sum_{i=1}^\infty \int_\Omega f\chi_{A_i} d\mu=\sum_{i=1}^\infty \int_{A_i} f d\mu\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(3)$$ Now putting $(1)$ and $(3)$ together gives the sigma additivity. I'm not sure if this works like this. could someone have a look maybe? Thanks for your help","['integration', 'measure-theory', 'solution-verification']"
4347709,How to compute the ring of all $f\in K[X]$ with $f(\mathcal{O}_K)\subseteq \mathcal{O}_K$?,"Let $K$ be a number field, $\mathcal{O}_K$ the ring of integers of $K$ , and $A\subseteq K[X]$ the ring of all polynomials $f\in K[X]$ with $f(\mathcal{O}_K)\subseteq\mathcal{O}_K$ . It is obvious that $\mathcal{O}_K[X]\subseteq A\subseteq K[X]$ , but we can find better bounds. Define a sequence of polynomials $\{f_n\}_{n\ge 0}$ in $\mathbb{Z}[X]$ by $f_0=1$ and $f_{n+1}=(X-n-1)f_n$ . For each prime $\mathfrak{p}\mid (p)$ of $\mathcal{O}_K$ , define an ideal $I_n:=\langle f_n(\mathcal{O}_K)\rangle$ For all $n$ , let $\lambda_n\in\mathcal{O}_K$ with $I_n\subseteq (\lambda_n)$ , then if I'm not mistaken $$
\bigoplus_{n\ge 0}\frac{f_n}{\lambda_n}\cdot \mathcal{O}_K\subseteq A\subseteq \bigoplus_{n\ge 0}\frac{f_n}{n!}\cdot \mathcal{O}_K.
$$ Moreover, define $$
m(\mathfrak{p},n):=\min_{\alpha\in\mathcal{O}_K}\operatorname{ord}_\mathfrak{p}(f_n(\alpha)).
$$ It is easy see that $I_n=\prod_{\mathfrak{p}}\mathfrak{p}^{m(\mathfrak{p},n)}$ . let $f(\mathfrak{p})$ be the residue class degree and $e(\mathfrak{p})$ be the ramification index, then I believe that $$
m(\mathfrak{p},n) = \begin{cases}
\operatorname{ord}_p(n!)\quad&\text{if $f(\mathfrak{p})=e(\mathfrak{p})=1$}\\
\left\lfloor\frac np\right\rfloor&\text{if $f(\mathfrak{p})=1$ and $e(\mathfrak{p})>1$}\\
0&\text{otherwise.}
\end{cases}
$$ Which makes the lower bound fairly concrete. The lower and upper bounds are equal only when $K=\mathbb{Q}$ . In this case, we find that $A = \bigoplus_{n\ge 0}{X\choose n}\cdot \mathbb{Z}$ Questions: Can you compute $A$ for a few number fields other than $\mathbb{Q}$ ? Is there an algorithm to do it in general? Is $A$ free as a $\mathcal{O}_K$ -module? Can you compute a minimal generating set (so a basis if the answer to the previous question is affirmative) for $A$ for a few number fields other than $\mathbb{Q}$ ? Is there an algorithm to do it in general? Proof of the upper bound Define $\Delta:A\to A$ by $\Delta f:=f(X+1)-f(X)$ . Note that all constant polynomials in $A$ lie in $\mathcal{O}_K$ . Let $f\in A$ have degree $d$ and leading coefficient $a_d\in K$ , then $\Delta^{(d)}f=d!a_d\in\mathcal{O}_K$ , and $f-d!a_d\cdot \frac{f_d}{d!}=f-d!a_d{X\choose d}$ has degree strictly less than $f$ . Induction on the $\deg(f)$ finishes the job.","['number-theory', 'abstract-algebra', 'algebraic-number-theory', 'polynomials']"
4347738,Show that the minimal polynomial of $T:\mathbb{K}^n \mapsto \mathbb{K}^n$ remains the same over field extension,"Show that the minimal polynomial of linear transformation $T:\mathbb{K}^n \mapsto \mathbb{K}^n$ remains the same over field extension using the cyclic decomposition theorem. Let $\mathbb{C}$ be a field and let $\mathbb{K}$ be a subfield of $\mathbb{C}$ . Then we see that the minimal polynomial of $T$ over $\mathbb{K}$ (say $p_K(x)$ ) divides the minimal polynomial over $\mathbb{C}$ (say $p_C(x)$ ) Now assume that $p_C(x)= (x-c_1)p_K(x)$ I don't understand where do I use the cyclic decomposition theorem.Some hints rather than the  complete answer would be helpful Edit 1:
Here's my attempt to the outline given in the answer: We try to show that $p_{\mathbb{C}}(x) | p_{\mathbb{K}}(x)$ . Now $p_{\mathbb{K}}(T)(e_i) = 0, 1 \le i \le n$ . [ As $e_i \in \mathbb{K}^n , \forall i   s.t. 1 \le i \le n]$ Now $v \in \mathbb{C}^n$ then $v = d_1.e_1 + \cdots + d_n.e_n$ where $d_i \in \mathbb{C}$ then $p_{\mathbb{K}}(T)(d_1.e_1 + \cdots d_n.e_n) = p_{\mathbb{K}}(T)(d_1.e_1) + \cdots + p_{\mathbb{K}}(T)(d_n.e_n) $ then $p_{\mathbb{K}}(T)(v) = 0$ for all $v \in \mathbb{C}$ . Then by the definition of minimal polynomial we see that $p_{\mathbb{C}}(x) | p_{\mathbb{K}}(x)$ . Now to show the converse we know that by the cyclic decomposition theorem we can write : $\mathbb{K}^n = Z(\alpha_1;T) \bigoplus \cdots \bigoplus Z(\alpha_k;T)$ Then $T_{Z(\alpha_i;T)} = T|_{Z(\alpha_i;T)}$ then as $Z(\alpha_i;T)$ is a cyclic subspace then we know that the minimal polynomial of $T_{Z(\alpha_i;T)}$ is the same as the c.p. of $T_{Z(\alpha_i;T)}$ . Now the c.p.( $T$ ) = m.p.( $T_{Z(\alpha_1;T)}$ ).mp.( $T_{Z(\alpha_i;T)}$ ).... m.p.( $T_{Z(\alpha_j;T)}$ Also the minimal polynomial of $T$ over $\mathbb{K}$ is the l.c.m.( m.p.( $T_{Z(\alpha_1;T)}$ ),mp.( $T_{Z(\alpha_i;T)}$ ),...., m.p.( $T_{Z(\alpha_j;T)})$ Thus $p_{\mathbb{K}}(x) = $ l.c.m.( m.p.( $T_{Z(\alpha_1;T)}$ ),mp.( $T_{Z(\alpha_i;T)}$ ),...., m.p.( $T_{Z(\alpha_j;T)})$ Now by the definition of the characteristic polynomial of $T$ it remains the same over any field. Now $p_{\mathbb{C}}(x) | $ c.p. $(x)$ then by the property of the l.c.m. we can claim that $p_{\mathbb{K}}(x) | p_{\mathbb{C}}(x)$ Hence $p_{\mathbb{K}}(x) = p_{\mathbb{C}}(x)$ .","['minimal-polynomials', 'linear-algebra', 'linear-transformations']"
4347748,Inequality for descents after applying Cauchy-Schwarz,"Let $S_i$ be the symmetric group defined over i elements. Let $W_i$ be defined as $$W_i=D_i-\frac{i-1}{2}.$$ $D_i$ is the random variable counting the number of descents of a random permutation in $S_i$ (so we know that $E[D_i]=\frac{i-1}{2},$ $\operatorname{Var}(D_i)=\frac{i+1}{12}$ and $E[D_n^3]=\frac{(n^2-n+2)(n-1)}{8}$ ) Then one has \begin{align*}
\frac{144}{n^4(n+1)^2}\left(  \sum_{1\le i, j \le n-1} E[W_{i-1}^2 W_{j-1}^2] \right) &\stackrel{Cauchy}{\le} \frac{144}{n^4(n+1)^2} \left( \sum_{1 \le i, j \le n-1} \sqrt{E[W_{i-1}^4]E[W_{j-1}^4]} \right) \\
&\stackrel{?}{\le} \frac{C}{n^4(n+1)^2} \left( \sum_{1 \le i, j\le n-1}ij \right)
\end{align*} I'm having problems understanding how to get the inequality that I marked as $?$ . While I understand that I can choose a big enough constant instead of $144$ , but I don't get how the terms in the root are bounded by $ij$ .","['inequality', 'probability-theory']"
4347798,Name of invariance for ODE,"If I have a first order ODE $\dot{x} = f(x)$ with $x = (x_1, x_2, \cdots,x_n) \in \mathbb{R}^n$ . What is the property called that if $x$ is a solution then a solution vector given by a permutation of the components of $x$ is also a solution?","['ordinary-differential-equations', 'dynamical-systems']"
4347801,$f:\mathbb{R} \to \mathbb{R}$ such that $f(x+y^2+f(y)) = f(x-y^2-f(y))$,"Find all functions $f:\mathbb{R} \to \mathbb{R}$ such that $f(x+y^2+f(y)) = f(x-y^2-f(y))$ . There is actually a proof that $f\equiv c$ and $f(x) = -x^2$ are the only ones - it goes through the not too natural substitution $g(x) = f(x) + x^2$ and using that in the resulting equation every real number can be written as a difference of the form $g(y) - g(z)$ . I am looking for a different and a bit more natural approach, e.g. that $f$ must be periodic (if it is different from the abovementioned solutions). Any help appreciated!","['functional-equations', 'substitution', 'functions']"
4347815,Link between Poisson distribution and Exponential distribution,"Let $(\sigma_i)_{i \geq 1}$ be i.i.d. random variables with Exponential distribution of parameter $\lambda$ , representing the waiting time between consecutive events . The arrival time of these events (from a $t=0$ origin) is: $$\tau_n = \sigma_1 + \sigma_2 + ... + \sigma_n$$ The number of those events happening between time $t_0=k$ and $t_1=k+1$ is: $$M_k :=\sum_{\quad j \geq 1 \\ k \leq \tau_j < k+1} 1$$ Is it true that $(M_k)$ , counting the number of events happening in a time-window $[k, k+1[$ , has a Poisson distribution? It seems true intuitively, but I would like to find a source / proof. NB: I have aleady read Link between Poisson and Exponential distribution and Relationship between Poisson and exponential distribution which may be linked, but it's not the same question. I wonder if the reciprocal is also true: let's say we have consecutive events with arrival times of $(\tau_i)_{i \geq 1}$ , such that the number $M_k$ of events happening between $[k, k+1]$ is a Poisson distribution of parameter $\lambda$ , for each integer $k$ . Can we conclude that $\sigma_i = \tau_i - \tau_{i-1}$ has an exponential distribution?","['poisson-distribution', 'probability-distributions', 'exponential-distribution', 'probability', 'random-variables']"
4347824,"Restriction of fractional Sobolev ""function"" of negative order to subset","Assume $ U\subset V\subset \mathbb{R}^n$ are bounded open subsets with smooth boundary. We define $H^{-s}(\Omega)=(H_0^{s}(\Omega))'$ for $s>0$ . It is straightforward to show that $\left. v\right|_{U}\in H^s(U)$ for all $v\in H^s(V)$ when $s\geq 0$ and that this is continuous. However, it is not clear to me how this should work for negative orders. I think I would need a continuous extension map $\iota: H_0^s(U)\rightarrow H_0^s(V)$ , then I could set $\left. v\right|_{U}=v\circ \iota $ for $v\in H^{-s}(V).$ Intuitively I would define $\iota$ as the extension by $0$ , but when looking this up  in ""Non-Homogeneous Boundary Value Problems and Applications"" by Lions and Magenes I saw, that extension by $0$ , $H_0^s(\Omega)\rightarrow H^s(\mathbb{R}^n)$ is only continuous for $s \not = \text{integer}+\frac{1}{2}.$ This is probably due to the fact $H_0^{\frac{1}{2}}(\Omega)=H^{\frac{1}{2}}(\Omega)$ for Lipschitz domains, e.g. $1\in H_0^{\frac{1}{2}}(\Omega)$ , but indicator functions are not in $H^{\frac{1}{2}}$ . So is there any other obvious way this works?","['fractional-sobolev-spaces', 'interpolation-theory', 'sobolev-spaces', 'functional-analysis']"
4347831,Prove: $P(X>a)\ge\frac{(1-a)^2}{b}$ for every $0<a<1$,"I found this question and I have no idea how to solve it. I'll be glad if you can help me! $$$$ let $X>0$ be a random variable, such that: $E(X)=1,\quad E(X^2)=b$ Then, for every $0<a<1$ , prove that: $P(X>a)\ge\frac{(1-a)^2}{b}$ $$$$ If we were talking about upper bound, i would've start thinking about Markov or Chebyshev inequality, but it's not... So I'm out of ideas.","['inequality', 'probability-distributions', 'probability', 'random-variables']"
4347849,$E$ is a Banach space for the norm $\| \cdot \|_1$ where $\|x\|_{1} := \|x\|_{E}+\|T x\|_{F}$,"I'm reading a proof of closed graph theorem in textbook Functional Analysis, Sobolev Spaces and Partial Differential Equations . Let $E$ and $F$ be two Banach spaces. Let $T$ be a linear operator from $E$ into $F .$ Assume that the graph of $T, G(T)$ , is closed in $E \times F$ . Then $T$ is continuous. Proof: Consider, on $E$ , the two norms $$
\|x\|_{1}=\|x\|_{E}+\|T x\|_{F} \quad \text { and } \quad\|x\|_{2}=\|x\|_{E}
$$ It is easy to check, using the assumption that $G(T)$ is closed, that $E$ is a Banach space for the norm $\| \cdot \|_1$ . Could you verify if my proof, that $E$ is a Banach space for the norm $\| \cdot \|_1$ , is correct? My attempt: Let $(x_n)$ be a Cauchy sequence w.r.t. $\| \cdot \|_1$ , i.e., for every $\varepsilon >0$ , there is $N \in \mathbb N$ such that $$\|x_n -x_m \|_1 = \|x_n - x_m\|_{E}+\|T x_n -Tx_m\|_{F} < \varepsilon, \quad \forall m,n \ge N.$$ This implies $$\|x_n - x_m\|_{E} < \varepsilon \quad \text{and} \quad\|T x_n -Tx_m\|_{F} < \varepsilon, \quad \forall m,n \ge N.$$ Hence $(x_n)$ is a Cauchy sequence w.r.t. $\| \cdot \|_E$ , and $(Tx_n)$ a Cauchy sequence w.r.t. $\| \cdot \|_F$ . Because $E, F$ are complete. there are $x \in E, y\in F$ such that $x_n \to x$ w.r.t. $\| \cdot \|_E$ and $Tx_n \to y$ w.r.t. $\| \cdot \|_F$ . On the other hand, $(x_n, Tx_n)$ is a sequence in $G(T)$ such that $(x_n, Tx_n) \to (x, y)$ . This is because convergence in product topology is equivalent to component-wise convergence. Because $G(T)$ is closed, $(x, y) \in G(T)$ and thus $Tx = y$ . It follows that $Tx_n \to Tx$ w.r.t. $\| \cdot \|_F$ . Hence $x_n \to x$ w.r.t. $\| \cdot \|_1$ .","['banach-spaces', 'solution-verification', 'functional-analysis']"
4347881,Prove that a function is $\mathcal{C}^n$ using the reccurence,"Let $n \in \mathbb{N}^{*}$ and $f:\left[0,1\left[\rightarrow \mathbb{R}\right.\right.$ a $\mathrm{C}^{n} $ function. Let $g$ the application $\left[0,1\left[\rightarrow \mathbb{R}\right.\right.$ such that $g(x)=\frac{f(x)-f(0)}{x}$ for each $x \neq 0$ and $g(0)=f^{\prime}(0)$ .
Let $\mathrm{t}:\left[0,1\left[\rightarrow \mathbb{R}\right.\right.$ defined by $\mathrm{t}(x)=f(x)-\sum_{\ell=0}^{n} \frac{f^{(\ell)}(0)}{\ell !} x^{\ell}$ , and $h$ defined by $h(x)=\frac{\mathrm{t}(x)}{x}$ if $x \neq 0$ and $h(0)=0 .$ Using L'Hôpital's rule , prove that $h$ is a $\mathrm{C}^{n-1}$ function  on $[0,1[$ , and $\forall 0 \leq k \leq n-1$ we have $h^{(k)}(0)=0$ . In $]0,1[$ , it's okay. I still have a problem with $0$ , I tried using L'Hôpital's rule, and the recurrence, but I didn't get the result. Any help! I appreciate it.","['derivatives', 'real-analysis']"
4347912,Under which probabilistic assumptions is this estimation approach correct?,"trying to tackle a problem, I ended up having built a model $ p(\boldsymbol{x}_{i}|\boldsymbol{y}_{i,j}, \boldsymbol{x}_{j}) $ which gives me for different $ \boldsymbol{y}_{i,j} $ observations made by $ \boldsymbol{x}_{j} $ , a new PDF of $ \boldsymbol{x}_{i} $ . My ultimate goal is to obtain the best estimate for it, $ \hat{\boldsymbol{x}_{i}} $ , given my $ \boldsymbol{y}_{i,j} $ observations. The most typical approach I have seen being used (from others who faced the same problem and made other models) is the: \begin{equation}
\widehat{\boldsymbol{x}_{i}}=\arg \max_{\boldsymbol{x}_{i}} f(\boldsymbol{x}_{i}) \prod_{j \in \mathcal{O}} f(\boldsymbol{y}_{i,j} \mid \boldsymbol{x}_{i}, \boldsymbol{x}_{j})
\end{equation} However, I tried using instead simply the following and it worked just fine: \begin{equation}
\widehat{\boldsymbol{x}_{i}}=\arg \max_{\boldsymbol{x}_{i}} \prod_{j \in \mathcal{O}} p(\boldsymbol{x}_{i} \mid \boldsymbol{y}_{i,j}, \boldsymbol{x}_{j})
\end{equation} I would like to ask why is that happening? What does the fact that both appear to work, suggest about their relationship? I did not mention the underlying assumptions (because I am not quite sure), therefore, I would like some help in identifying mathematically those. Thank you for your time. *Some further information regarding $ \boldsymbol{x}_{i} $ , $ \boldsymbol{y}_{i,j} $ , $ \boldsymbol{x}_j $ and their relation: $ \boldsymbol{x}_{i} $ is the position of some node in 3D space. Assume that this node is emitting one single impulse signal. $ \boldsymbol{y}_{i,j} $ is the measurement of that signal from another node (namely node $ j $ ), whose position is ""known"" to us. A measurement depends only on the distance between the transmitting and the receiving node (the model of this dependency is the same for all receiving nodes). Therefore, given some measurement $ \boldsymbol{y}_{i,j} $ and the position of the receiver $ \boldsymbol{x}_j $ , there is a distribution about where $ \boldsymbol{x}_{i} $ is. The following graph depicts my system where my actual goal is to find the best position estimation for all nodes $ \boldsymbol{X} $ (their positions are independent to each other) because, in fact, I do not have any prior knowledge about them (the knowledge gets built iteratively via my optimization process). So in fact, after optimizing $ \boldsymbol{x}_{i} $ , I continue optimizing iteratively one by one all the rest of $ \boldsymbol{x}_{j} $ 's (which become the new $ \boldsymbol{x}_{i} $ on each step) using the previous estimations of the $ \boldsymbol{x}_{j} $ 's when available (else I am using a random position). I have practically seen that this method converges to a correct solution where nodes are in a relative reference system (since my initial positions are random) placed.","['conditional-probability', 'probability-distributions', 'bayesian', 'probability-theory', 'probability']"
4347916,Same distribution with different probability density function,"Suppose $X$ is a random variable with probability density function: $$f_a(x)=\frac{1}{a}$$ With $0<x<a$ . I must find the Moment-generating function of $Y = -(\ln X - \ln a)$ . After calculation I've found that $M_Y(t)=\frac{1}{1-t}$ . So no matter what is the value of $a$ . For every $a$ , Moment-generating function is independent from $a$ . So every $Y$ with different values of $a$ have the same distribution? Even if their probability density functions are different? Or I've found the Moment-generating function wrong? $$M_Y(t)=E(e^{t(-(\ln X - \ln a)})=a^t\int_{0}^{a}x^{-t}\frac{1}{a}dx=\frac{1}{1-t}$$ So for every $t\in (-1,1)$ , we have $E(e^{tY})<\infty$ and so the Moment-generating function exists.","['moment-generating-functions', 'probability-distributions', 'probability']"
4347979,Dynkin index of algebra embedding,"While studying the following paper An $\mathcal{N}=1$ Lagrangian for the rank 1 $E_6$ superconformal theory , on page 10 a stumbled upon the definition of embedding index Consider an embedding of $H$ into $G$ and choose a representation $R_G$ of $G$ that decomposes to $\sum R_{H_i}$ representations of H under the embedding. Then the embedding index is defined as $$I_{I \hookrightarrow G} = \frac{\sum_i T_{R_{H_i}}}{T_{R_G}}$$ where $T_R$ stands for the Dynkin index of the representation $R$ . This definition is borrowed from another paper S-duality in $\mathcal{N}=2$ supersymmetric gauge theories where the authors do even some examples in appendix C. The definition is clear to me, but what does not follow is the result in equation $(18)$ of the first paper where they evaluate $I_{U(1)\hookrightarrow SU(6)}$ from the embedding $\mathfrak{su}(5)\otimes\mathfrak{u}(1) \subset \mathfrak{su}(6)$ decomposing the fundamental of $SU(6)$ using, I imagine, the branching rule $6\rightarrow (1,-5)\oplus(5,1)$ where the notation stands for $(R_{SU(5)},R_{U(1)})$ . This branching rule was found using the LieART package for mathematica. What the authors find is the following $$ I_{U(1)\hookrightarrow SU(6)} = \frac{5\times (2/3)^2+(-10/3)^2}{1/2}$$ which I really do not understand where it came from! In particular the squares.","['physics', 'group-theory', 'lie-algebras', 'lie-groups']"
4347986,Number of zeros of $f(x)= \frac{1}{2} E\left[ \tanh \left( \frac{x+Z}{2} \right) \right]-\tanh(x)+\frac{x}{2}$ where $Z$ is standard normal,"Consider the following function: \begin{align}
f(x)= \frac{1}{2} E\left[ \tanh \left( \frac{x+Z}{2} \right) \right]-\tanh(x)+\frac{x}{2},
\end{align} where $Z$ is standard normal. Question : How to show that this function has only three zeros? Note, that we are not interested in the locations just the number of zeros. By using that $\tanh(x)$ is an odd function, it is not difficult to show that $f(0)=0$ .
However, I am not sure how to show the existence of the other two zeros. I know that two more zeros exist from the numerical simulation (see the attached figure). Edit: The current answer shows that there are at least 3 zeros.  Now we need to show that there can be no more than 3 zeros. Edit 2 Idea for a proof.  Consider only positive $x$ .  I think  if we can show the following: $f(x)>0$ for  all $x>x_1$ , $f(x)$ is convex for $x \in (0,x_2)$ , and $x_2>x_1$ . Then this will imply that the function is convex in the regime while it changes a sign. Therefore, it can only have at most one sign change.","['hyperbolic-functions', 'roots', 'probability', 'real-analysis']"
4347989,"Let $f,g \in \mathbb{R}[x]$ , $f(x)+g(x)=5$ and $f(g(x))=8-4x $ find $g(2)$","Let $f,g$ are two polynomials $f,g \in \mathbb{R}[x]$ such that $f(x)+g(x)=5$ and $f(g(x))=8-4x $ . Find set of all possible values of $g(2)$ . Let $\deg f=n , \deg g=m$ so $\deg f(g(x)) \le mn$ so $mn=1$ then $m=n=1$ then $f(x)=ax+b$ and $g(x)=cx+d$ We have $f(g(2))=0$","['analysis', 'real-analysis', 'calculus', 'functions', 'algebra-precalculus']"
4348013,Finding the covariance matrix of $Y=AX$ where $X$ is a known multivariate gaussian random variable,"For $X=(X_1,X_2,X_3)$ with Gaussian distribution, covariance matrix $2I_3$ ( $2$ multiplied by the identity matrix), and mean vector $\mu$ = $(3,3,3)^T$ , I want to find the covariance matrix of $Y=(Y_1,Y_2,Y_3)^T$ where $$Y=\begin{pmatrix}1/\sqrt 2 & 0 & -1/\sqrt 2 \\1/\sqrt 3 & 1/\sqrt 3 & 1/\sqrt 3\\ 1/\sqrt 6 & -2/\sqrt 6 & 1/\sqrt 6\\
\end{pmatrix}X$$ I know that when I use the definition of the covariance matrix in terms of expectations, my answer is $2I$ (which is the correct answer). However when I substitute in $Y=AX=A(2Z+\mu)=2AZ+A\mu$ , the covariance matrix of Y is $(2A)(2A)^T$ which simplifies to $4I_3$ . However, this contradicts the correct answer $2I_3$ . I hope someone could clarify this for me.","['statistics', 'covariance', 'normal-distribution', 'probability', 'multivariate-statistical-analysis']"
4348019,How to solve this nonlinear recurrence relation?,"I'm trying to solve the following recurrence relation (all values are real): $$g(n+1) = \frac{a_n + g(n)}{k\cdot g(n)}$$ With: $a_n,g(n) > 0$ for all n, and $g(0)$ is arbitrary. $k \in \mathbb{N}\setminus\{0,1\} $ , (At this moment you can assume that $k=2$ but it'll be interesting to see results with a general $k$ ). This comes from my research, which I can't elaborate on at this point. I don't have much experience with solving these kind of relations. For the past days I've tried reading about generating functions and other techniques but couldn't manage to solve it. Any help will be much appreciated.","['recursion', 'recurrence-relations', 'discrete-mathematics', 'generating-functions']"
4348022,Local form of a fundamental vector field,"Let $M$ be a smooth manifold of dimension $2l$ on which acts a lie group $G$ . Let $X$ be a element in the lie algebra $\mathfrak{g}$ of $G$ , we associate to it the vector field $X_M$ defined by $X_M(m)= \frac{d}{dt}\biggr\vert_{t=0} e^{-tX}.m$ . Let $p$ be a zero of the vector field $X_M$ . In the page 6 of the article [1], the authors say that we can find  some local coordinates $x_1,...,x_{2l}$ around $p$ such that the vector field $X_M$ is linearized: $$X_M= a_1 \left( x_2 \frac{\partial}{\partial x_1} -x_1 \frac{\partial}{\partial x_2} \right)+\ldots+a_l\left(x_{2l} \frac{\partial}{\partial x_{2l-1}}-x_{2l-1}\frac{\partial}{\partial x_{2l}}\right),$$ $a_1,..,a_l \in \mathbb{R}.$ How can we prove that $X_M$ has the above form locally (around p)? References [1] Michèle Vergne, ""Cohomologie équivariante et théorème de Stokes"" (rédigé par Sylvie Paycha) (French) Analysis on Lie groups and representation theory. Proceedings of the summer school, Kénitra, France, 1999 , Séminaires et Congrès 7, Paris: Société Mathématique de France (ISBN 2-85629-142-2/pbk), pp. 1-43 (2003), MR2038647 , Zbl 1045.57021 .","['group-actions', 'differential-geometry']"
4348028,Cutting a galette and hitting the fève,"We have in France a tradition of eating in January countless galettes des rois (*) . Hidden inside is a fève , a small figurine (it was originally a coin). The one who gets the fève without breaking a tooth is crowned queen or king. To give some context, the home-made galette we ate today, together with its fève As I was cutting the galette , my son asked I wonder what the probability to hit the fève when making a cut is? Now I wonder as well. In the tradition of spherical cows in a vacuum, a galette with its fève can be simplified as where $r_g$ and $r_f$ are the radii of, respectively, the galette and the fève . $d_f$ is the distance of the center of the fève from the center of the galette . EDIT: the placement of the fève is random. Asking for a full calculation of the probability would be too much :), so my question is: how should I approach this calculation , especially since it will be dependent on $d_f$ (which will probably have a squared distribution). Any hints and warnings are welcome (**) . (*) We are of course talking about the only proper one - the northern one (in case someone has doubts from Wikipedia). The proper drink for a galette des rois is cidre , of course from Brittany (**) The prize could be a part of the galette but it is already gone.","['recreational-mathematics', 'probability']"
4348137,Evaluate $\lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k}$,"Problem: evaluate $\lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k}$ . My work: since $n+1 \le k \le 2n$ , it is $\frac{1}{2n} \le \frac{1}{k} \le \frac{1}{n+1}$ . Since $k \ge 1$ , the exponential with base $k$ is increasing and so, from $\frac{1}{2n} \le \frac{1}{k} \le \frac{1}{n+1}$ , it follows that $k^{1/2n} \le k^{1/k} \le k^{1/(n+1)}$ . Since the $\alpha$ -th ( $\alpha$ real) power is increasing when its base is nonnegative and $k \ge 1$ , it follows that $(n+1)^{1/2n} \le k^{1/2n}$ and $k^{1/(n+1)} \le (2n)^{1/(n+1)}$ ; in conclusion, it is $(n+1)^{1/2n} \le k^{1/k}\le(2n)^{1/(n+1)}$ for any $n+1 \le k \le 2n$ and for any $n\in\mathbb{N}$ . Hence $$\lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k} \ge \lim_{n \to \infty} \prod_{k=n+1}^{2n} (n+1)^{1/2n}=\lim_{n\to\infty}\sqrt{n+1}=\infty$$ $$\implies \lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k}=\infty$$ Is this correct? I'm not sure that what I've written about the estimation I get using the fact that the exponential and the real power are increasing.","['infinite-product', 'limits', 'solution-verification', 'real-analysis']"
4348166,A group product,"If $G$ and $H$ are two groups, and $\triangleright$ and $\triangleleft$ are a left action and a right action
of $H$ on $G$ by group automorphisms such that $$h\triangleright(g\triangleleft h')=(h\triangleright g)\triangleleft h',$$ then we can construct a group $G\bowtie H$ with underlying set $G\times H$ and multiplication such that $$(g,h)\cdot(g',h')=((g\triangleleft
h')(h\triangleright g'),hh').$$ This group can also be described as a semidirect product but … does this particular construction have a name and/or appears in the literature?","['group-theory', 'group-actions', 'terminology']"
4348172,What is the meaning of the ideal class group?,"When I first learned about the ideal class group, I learned that it measures the failure of unique factorization in a number ring. The main justification for this is that a number ring has unique factorization if and only if it has class number $1$ . This is very unsatisfying though because the exact size of the class group is not used, and neither is the entire group structure of the class group. Furthermore, the dichotomy of ""UFD / not UFD"", while an important first step, doesn't measure the extent to which unique factorization fails, only if it fails or not. So my questions are: In what way does the exact size of the class group measure the extent to which a number ring fails to have unique factorization? (Beyond the dichotomy of class number $1$ vs. not $1$ .) In what way does the group structure of the class group measure the extent to which a number ring fails to have unique factorization? This I have basically no feeling for: if the class group is $\mathbb{Z}/2 \times \mathbb{Z}/2$ versus $\mathbb{Z}/4$ , is that difference measuring anything related to unique factorization? What is it measuring at all? This is a question that has been asked on SE a few times before (see here and here ) but the answers weren't exactly what I was looking for, so I wanted to ask it again. Thanks for the help!","['ideal-class-group', 'number-theory', 'algebraic-number-theory']"
4348312,How does the fundamental theorem of calculus help to simplify difficult exponential functions.,"I am very confused by the value of the fundamental theorem of calculus to this initial value problem from Logan, D. (2017). A first course in differential equations. Springer. Sure we have found the value of the arbitrary integration constant but now we have an indefinite integral with a free parameter.  I'm not clear on what this is meant to illustrate.  It has left me confused about why the FTC is so important since solving $x'=e^{-t^2}$ was what motivated discussing the FTC earlier in the chapter.","['initial-value-problems', 'calculus', 'ordinary-differential-equations']"
4348418,Proof for volume of n-ball with radius 1,"I'm trying to prove this formula from here , that the volume of a n-ball with radius 1 (let's call it $B_n$ ) is: $$\frac{\pi^{n/2}}{\Gamma(\frac{n}{2}+1)}$$ However, I come to the wrong result and I cannot find the mistake. The intersection of an n-ball with a hyperplane is an $(n − 1)$ -ball. Therefore: $$\text{vol}(B_{n+1}) = \int_{-1}^1 \text{vol}((B_{n} \,\,| \,\,\text{Radius = }\sqrt{1-x^2})) \,\,dx =$$ $$2 \int_{0}^1 \text{vol}((B_{n} \,\,| \,\,\text{Radius = }\sqrt{1-x^2})) \,\,dx$$ Now I want to find $\text{vol}((B_{n} \,\,| \,\,\text{Radius = }\sqrt{1-x^2}))$ . That is per definition $\int_{-1}^1 \,\,1_K$ where $K = \{(x_1, ..., x_{n}) \,\,|\,\, x_1^2 + ...+ x_n^2 \leq \sqrt{1-x^2} \}$ . It holds: $
x_1^2 + ...+ x_n^2 \leq \sqrt{1-x^2} \Leftrightarrow \sum \frac{x_i^2}{\sqrt{1-x^2}} \leq 1
$ Now I do the transformation $x_i \mapsto x_i \cdot (\sqrt{1-x^2})^{1/2}$ .
The determinant of the Jacobian is $(\sqrt{1-x^2})^{n/2}$ . So I get: $$2 \int_{0}^1 \text{vol}((B_{n} \,\,| \,\,\text{Radius = }\sqrt{1-x^2})) \,\,dx = $$ $$2 \int_{0}^1 \text{vol}(B_{n}) \cdot (\sqrt{1-x^2})^{n/2} \,\,dx = $$ $$2 \int_{0}^1 \text{vol}(B_{n}) \cdot (1-x^2)^{n/4} \,\,dx$$ Here I would use induction now. But above formula isn't correct. For $n=2$ it's correct, but for $n=3$ it would state that $$\text{vol}(B_{3}) = 2 \int_{0}^1 \text{vol}(B_{2}) \cdot (1-x^2)^{3/4} \,\,dx = 2 \pi \cdot \int_{0}^1 (1-x^2)^{3/4} \,\,dx$$ which according to Wolfram Alpha doesn't equal $\frac{3}{4}\pi$ , which would be the correct answer.","['integration', 'lebesgue-integral', 'volume', 'real-analysis']"
4348488,Why doesn't the Borel-Kolmogorov paradox cause problems in practice?,"The Borel-Kolmogorov paradox shows that the usual formula for conditional density $f_{X|Y}(x|y) = f_{X,Y}(x, y)/f_Y(y)$ can lead to inconsistent results depending on the coordinate system that is used to describe the problem [1]. The paradox stems from conditioning on zero probability events being undefined. One explanation comes from considering this conditioning operation, as a limit of distributions that are conditioned on events with non-zero probability. Different limits lead to different conditional densities. This leaves me with two questions that worry me: When simply applying the ""ratio of densities"" formula, it's not clear which limit is implied. So two people solving the same problem in different coordinate frames can do the calculation without needing to think about when they can be expected to end up with the same result. Is there a procedure that makes it clear when two results will end up being different? How often does this cause a problem in practice? In statistics, we condition on real-valued outcomes all the time! Why do seemingly irrelevant details like the parameterisation not end up causing problems more often? Or is this universally used formula in fact on incredibly shaky ground? To what extent is statistics or Bayesian inference parameterisation independent? In short, to what extent does this paradox matter in practice? From my current (limited) understanding, there is an answer to Question 1. Namely, if you follow the measure-theoretic definition of conditional probability, the choice of $\sigma$ -algebra influences the answer. This at least makes it clear where a choice appears, in contrast to following the ratio of densities rule. [1] See YouTube video or chapter 15 of Jaynes.","['statistics', 'probability']"
4348491,"Approach to counting the number of sub-graphs of a given graph $G = (V,E)$","I have just currently started reading about Graph theory in computer science and while reading the concept of graphs I came across the concept of sub-graphs. As a sub-graph is essentially similar to a subset in set theory I became interested in the question ""How many sub-graphs exist for a given graph G = (V,E)"" While trying to count the number of sub-graphs I got a little confused as to what should be a generic approach in order to try figure out the number of sub-graphs quickly as it is highly dependent on the edge set the given graph possesses and this question confirmed my intuition : How many subgraphs of this simple graph? . But even this question doesn't answer the generic question as to what should be the general approach to figure out the number of sub-graphs for a given graph quickly. It will be great if someone could throw some light on the approach and explain it like they were explaining it to a 5 year old.","['elementary-set-theory', 'graph-theory', 'combinatorics', 'discrete-mathematics']"
4348496,Prove that no graph has exactly $2$ spanning trees.,"This question is why I must go second time to the final exam in discrete mathematics . The original question was: For which $n$ exists a graph with $n$ spanning trees? I know, that for $n > 2$ there exists a graph with $n$ spanning trees, because if we take $C_3$ , which is a cycle with $3$ vertices, it has exactly $3$ spanning trees. Cycle on $4$ vertices has $4$ spanning trees and so on.
I know that if a graph is not connected, than it has $0$ spanning trees, and if I have a graph on $1$ vertex, it has exactly $1$ spanning tree. So the question remains, how do I prove, that no graph exists, which has exactly $2$ spanning trees.","['graph-theory', 'discrete-mathematics']"
4348499,"Completeness of $\{ f_n : n \in \mathbb N \} \subset C[0,1]$ in $L^1[0,1]$","Suppose that $\{ f_n : n \in \mathbb N \} \subset C[0,1]$ is a system of continuous functions which is complete in $L^1[0,1]$ , i.e. each $g \in L^1[0,1]$ can be approximated arbitrarily well by a finite linear combination of the $f_n$ 's. Is it true that if $g \in L^1[0,1]$ is an arbitrary $L^1$ -functions such that $$
\int_0^1g(t)f_n(t) \, dt = 0 \ \ \ \ \forall n \in \mathbb N
$$ then $g=0$ almost everywhere? I know that this is true if $L^1[0,1]$ gets replaced by $L^2[0,1]$ since then we're in a Hilbert space setting. Thanks for any help!","['measure-theory', 'lp-spaces', 'functional-analysis', 'real-analysis']"
4348569,An alternating product/sum Fibonacci-like sequence,"Starting from reals $a_1, a_2$ , define a sequence $S(a_1, a_2)$ $$a_1, a_2, a_3, a_4, \ldots, a_{k-1}, a_{k}, a_{k+1}, a_{k+2}, \ldots \;,$$ so that, given the sequence up to its $k$ -th element, $k \ge 2$ even,
the next two elements are defined by \begin{eqnarray}
a_{k+1} &=& a_{k-1} &\times& a_k \\
a_{k+2} &=& a_k &+& a_{k+1}
\end{eqnarray} For example, the first ten elements of $S(\frac{1}{2},-\frac{1}{2})$ are: $$
\frac{1}{2},-\frac{1}{2}
   ,-\frac{1}{4},-\frac{3}{4}
   ,\frac{3}{16},-\frac{9}{16}
   ,-\frac{27}{256},-\frac{171}{256}
   ,\frac{4617}{65536},-\frac{39159}{65536},\ldots \;.
$$ I wanted to explore mixing multiplication and addition.
So this is like the Fibonacci sequence, except that the steps
alternate between  multiplication and addition. Two questions, one specific, one general. Q1 . Numerically, the odd elements of $S(\frac{1}{2},-\frac{1}{2})$ head to $0$ , and the even elements head to $\approx -0.622748$ .
You can see in the list of rationals above that the
denominators are $2^m$ where $m = 2^ {\lfloor (k-1)/2 \rfloor }$ .
But the pattern in the numerators seems difficult to see.
How can the even and odd limits be proven,
either by explicit calculation of the numerators, or by identifying
dominate terms that lead to proofs of convergence? Q2 . For which $a_1, a_2$ do both the even and odd elements of $S(a_1, a_2)$ converge? It seems the convergence region in the $(a_1, a_2)$ plane includes the two
diagonally opposite squares \begin{eqnarray}
a_1 \in (0,1) &\;\;\mathrm{and}\;& a_2 \in (-1,0)\\
a_1 \in (-1,0) &\;\;\mathrm{and}\;& a_2 \in (0,1)
\end{eqnarray} but is significantly broader.
For example, $S(2, -\frac{1}{3})$ converges.","['limits', 'sequences-and-series']"
4348587,Number of points in line segment,"What is the number of points in a line segment? In schools we are taught that a line segment is made up of infinitely many points. Let us suppose that there is a line segment $AB$ of length $a$ units and another line segment $CD$ of length $b$ units where $a>b$ . According to the schools, both the line segments are made up of $\infty$ points  but since $a>b$ won't it mean that $AB$ contains more points than $CD$ ? Or both have infinite points but some one has larger infinity than other? I have taken reference from here: How many points in a line segment? but I couldn't get a satisfying answer. Any comment will be greatly appreciated.","['euclidean-geometry', 'fixed-points', 'geometry']"
4348588,How to solve $\lfloor x \rfloor + \lfloor \frac{1}{x} \rfloor = 1$?,"I am stuck with this equation. All I could do is this: $\lfloor x \rfloor$ = $\lfloor n + m \rfloor$ such that $n \in N$ and $m<1$ .
We get: $\lfloor x \rfloor + \lfloor \frac{1}{x} \rfloor = 1$ $\lfloor n + m \rfloor + \lfloor \frac{1}{n+m} \rfloor = 1$ $n + \lfloor m \rfloor + \lfloor \frac{1}{n+m} \rfloor = 1$ $n + 0 + \lfloor \frac{1}{n+m} \rfloor = 1$ $n + \lfloor \frac{1}{n+m} \rfloor = 1$ From here on I have no idea what to do! Edit: It is easy to see that any value $1<x<2$ satisfies the equation, but can I find all the solutions?","['real-numbers', 'ceiling-and-floor-functions', 'discrete-mathematics', 'real-analysis']"
4348593,Domain vs Co-domain vs Support of a random variable,"As a sequel of my previous question . We say that $X$ is a random variable defined over $Y$ , does it mean that $Y$ is the domain of the random variable? Also we say that $X$ takes values in some set $Z$ , is $Z$ again the domain or the co-domain Referring to the support of a random variable, that is the union of all the sets that have strictly positive (ie non zero) probability with respect to the distribution of the random variable, do we refer to the domain or the co-domain of $X$ . Namely, if $R_X$ is the support of $X$ are the following equivalent: "" $X$ takes values in $R_X$ "" $\Leftrightarrow$ "" $R_X$ is the support of $X$ ""? P.S. If anyone would like to provide an example to explain the question above, I would appreciate this.","['measure-theory', 'probability-distributions', 'probability-theory', 'probability', 'random-variables']"
4348646,How do you know when trying to find a proof/counterexample isn't worth it?,"This is quite a general question... apologies if it is too general; I'm just wondering if anyone has any useful tips or advice. As part my final year university project I've made an algebriac/combinatorial conjecture (I won't say exactly what as I'm not asking for homework help) about something that looks like it might be true... but I also wouldn't be entirely surprised if it was false. I'd like to disprove or prove it, so I've split my time between trying very hard on paper to come up with a proof, while simultaneously computationally searching for counterexamples (the search space is quite large and awkward). On both fronts I seem to be going nowhere... I haven't identified a single counterexample yet (I have lots that do satisfy the conjecture though), but equally a proof always seems to be just out of reach (there always seems to be one logical step in the way). Basically this is the first time I've really had to do my ""own"" maths, so to speak, so I don't really have enough experience or gut instinct to know inuitively whether this is worth pursuing. I fear it's very possible that one of the counterexample is computationally out of reach; the proof is beyond my mathematical abilities; or one them is actually within my reach and I'm about to give up prematurely. So I'm wondering if people with more experience might be able to share any strategies they use when they find themselves in this situation. Are there indicators/tell-tale-signs I should be looking for to help decide what's more likely to be the case? And does anyone have any advice on how ""urn the problem around and start looking at in a different light? Many thanks.","['proof-writing', 'abstract-algebra', 'combinatorics', 'research']"
4348658,Can someone help me with this statistic question?,"A firm decided to have a sales event that they take name cards from 1000 of their customers, and randomly distribute the cards back to those customers. And the customers who receive their own card will get 50% sales for all of the firms products. When X is a random variable of number of people who gets back their own card, what is the variance of X I tried by first let $A_i$ be the event that $i^{th}$ person getting back his own card. which so $P(A_i) = 1/1000$ and $P(A_i \bigcap A_j)=1/(1000)(999)$ from here I am kind of lost what to do",['statistics']
4348665,Takesaki lemma 4.5,"Consider the following fragment from Takesaki's book ""Theory of operator algebra I"" (p82 and previous pages): The notation $\mathscr{L}_G$ means all normal operators with spectrum contained in $G$ and similarly $\mathscr{L}_{\mathbb{C}}$ denotes the normal operators. Why is the boxed equality true? The right hand side is the functional calculus on two elements. I suppose it makes intuitive sense. It reminds me of the fact that composition respects classical functional calculus but I can't justify it in this case. In any case, I also think it is relevant that $u(a), v(a) \ne 1$ . Can anybody formally justify why the boxed equality is true and resolve this technicality?","['von-neumann-algebras', 'c-star-algebras', 'operator-algebras', 'operator-theory', 'functional-analysis']"
4348683,Is a measure measurable?,"This could totally be a stupid question but I'm unsure: is a measure (ie positive, countable additive on a $\sigma$ algebra, 0 for the empty set) actually a measurable function (wrt to the Borel-sigma algebra on $\mathbb{R}$ )?","['measure-theory', 'borel-measures']"
4348694,"Show that $f:(0,\infty) \to (0,\infty)$ differentiable and such that $f'(x) \le \frac{1}{2f(x)}$ implies $f(x) \le \sqrt{x}$","Problem: let $f:(0,\infty) \to (0,\infty)$ be a differentiable function such that $$\lim_{x \to 0^+} f(x)=0 \\ f'(x) \le \frac{1}{2f(x)}, \ \forall x>0$$ Show that $f(x) \le \sqrt{x}$ for any $x>0$ . My textbook solves this problem using the derivative of a product, but I've tried this other approach: since $f$ has codomain $(0,\infty)$ , it is $f(x)>0$ for any $x>0$ and so, being by hypothesis $f'(x) \le \frac{1}{2f(x)}$ for any $x>0$ , it is $$f'(x) \le \frac{1}{2f(x)} \iff f(x)f'(x) \le \frac{1}{2}$$ By hypothesis $f$ is differentiable in $(0,\infty)$ , hence it is continuous in $(0,\infty)$ and so, for $a>0$ , it is integrable in any interval $[a,x] \subset (0,\infty)$ . So $$f(x)f'(x) \le \frac{1}{2}, \ \forall x>0 \implies \int_a^x f(t)f'(t)dt\le\int_a^x \frac{dt}{2}$$ $$\iff f(x)^2 \le x+f(a)^2-a$$ Letting $a \to 0^+$ in the inequality and using the hypothesis that $f(x) \to 0$ when $x \to 0^+$ , it follows that $f(x)^2 \le x$ . But $f(x)>0$ and the domain of $f$ is $(0,\infty)$ , hence $x>0$ as well and so the inequality $f(x)^2 \le x$ is equivalent to the one obtained taking the square root both sides and hence $f(x) \le \sqrt{x}$ . Is this correct? In particular, I am not quite sure about the fact that I integrated in a subset $[a,x]$ and then let $a \to 0^+$ .","['integration', 'limits', 'solution-verification', 'analysis']"
4348707,Is projection of locally connected compact subset locally connected?,"Let $(X, \mathcal{T}_X)$ and $(Y, \mathcal{T}_Y)$ be topological spaces, $Z = X \times Y$ , $\mathcal{T}_Z$ be the product topology on $Z$ , $f : Z \to X$ be defined by $f(x, y) = x$ , and $C \subset Z$ be compact and locally connected. Is $f[C]$ locally connected? Background A space $(Z, \mathcal{T}_Z)$ , where $\mathcal{T}_Z$ is a topology on $Z$ , is locally connected , if for each $z \in U \in \mathcal{T}_Z$ there exists a connected $V \in \mathcal{T}_Z$ such that $z \in V \subseteq U$ . Locally connected subset whose image is not locally connected The following shows that some restrictions are necessary for the subset $C$ . Let $X = Y = \mathbb{R}$ , and $Z' = \{(0, 1)\} \cup \{(1/n, 0) : n \in \mathbb{N}^{> 0}\}$ . Then $Z'$ is locally connected, but not compact, and $f[Z'] = \{0\} \cup \{1/n : n \in \mathbb{N}^{> 0}\}$ is not locally connected. Holds when $f\restriction C$ is a quotient map Suppose $f\restriction C$ is a quotient map. Quotient maps preserve local connectedness . Therefore $f[C]$ is locally connected. This question provides conditions for $f\restriction C$ being a quotient map. However, as shown there, $f\restriction C$ is not always a quotient map. Non-quotient strategy There exist maps which are continuous, surjective, and preserve local connectedness, but are not quotient; in the linked example $X$ and $Y$ are both locally connected. If the claim does hold, then a general solution to this problem may need a stronger theorem for preservation of locally connectivity which includes these maps. Edit:
Since there were no answers, I asked this question also in Mathoverflow: https://mathoverflow.net/questions/416561/is-projection-of-locally-connected-compact-subset-locally-connected","['general-topology', 'projection', 'connectedness']"
4348711,Mapping torus of orientation reversing isometry of the sphere,"$\DeclareMathOperator\SL{SL}\DeclareMathOperator\SO{SO}\DeclareMathOperator\SU{SU}\DeclareMathOperator\O{O}\DeclareMathOperator\Iso{Iso}$ Let $ f_n $ be an orientation reversing isometry of the round sphere $ S^n $ . Let $ M_n $ be the mapping torus of $ f_n $ . What can we say about $ M_n $ ? Here are the things I think I know: $ M_n $ is a compact manifold of dimension $ n+1 $ $ M_n $ is an $ S^n $ bundle over $ S^1 $ Applying LES homotopy to the fiber bundle be have $$
1 \to \pi_1(S^n) \to \pi_1(M_n) \to \pi_1(S^1) \to \pi_0(S^n) \to \pi_0(M_n) \to 1
$$ For $ n=0 $ , $ M_0 $ is the circle and the bundle map $ M_0 \to S^1 $ is just the circle double covering itself. For $ n \geq 1 $ the sphere is connected so the LES of homotopy simplifies to $$
1 \to \pi_1(S^n) \to \pi_1(M_n) \to \pi_1(S^1) \to 1
$$ For $ n\geq 1 $ the sphere is connected so $ f_n $ orientation reversing implies $ M_n $ must be nonorientable $ M_1 $ is the Klein bottle For $ n \geq 2 $ then $ S^n $ is connected simply connected so the LES homotopy simplifies to $$
\pi_1(M_n) \cong \pi_1(S^1) \cong \mathbb{Z}
$$ $ M_2 $ is a non orientable 3-manifold admitting $ S^2 \times R $ geometry. $ M_2 $ is the quotient of its orientable double cover $ S^2 \times S^1 $ by the free $ C_2 $ action given by $ (-x,-z) $ see this answer $ S^2 \times R $ geometry . I am interested in the geometry of this mapping torus $ M_n $ . In particular, $ M_n $ always admits a Riemannian metric with respect to which it is locally isometric to the geometry of the universal cover of the trivial bundle $ S^n \times S^1 $ . This geometry $$
\widetilde{S^n \times S^1} 
$$ is the product of a round geometry with a one dimensional flat $$
S^n \times R
$$ for $ n \geq 2 $ . For $ n=0,1 $ the geometry is just flat with universal cover $ \mathbb{R},\mathbb{R}^2 $ respectively. We can verify this in some examples by observing that $ S^1 $ and the Klein bottle both admit flat metrics. And $ M_2 $ is well known from Thurston geometrization as one of the exactly four compact 3-manifolds that admits $ S^2 \times R $ geometry. Now to the question. Recall that $ M_n $ is the mapping torus of an orientation reversing isometry of $ S^n $ . Let $$
G_n:=\Iso(S^n \times R) \cong \O_{n+1} \times \mathbb{R} \times C_2
$$ For which $ n $ does there exists a transitive action of $ G_n $ on $ M_n $ ? I'm also curious for which $ n $ the action factors through the compact group $ \O_{n+1} \times \mathbb{R}/\mathbb{Z} \times C_2 $ . Because then a transitive action by a compact group implies $ M_n $ admits the structure of a Riemannian homogeneous manifold. For example, there is a transitive action of $ G_n $ for both $ n=0,1 $ . But that action can only factor though the action of a compact group in the case $ n=0 $ , not in the case $ n=1 $ . And I'm also curious how $ M_n $ might differ for odd and even $ n $ , since odd and even orthogonal groups are significantly different. I'm especially curious about all this for the low dimensional examples $ M_2 $ and $ M_3 $ .","['riemannian-geometry', 'homogeneous-spaces', 'differential-topology', 'lie-groups', 'differential-geometry']"
4348718,Are finite state irreducible continuous Markov chains identifiable in general?,"Let $S=\{1,...,h\}$ be a finite state space and $X(t)$ an irreducible Markov chain fully described by a generator matrix $Q$ with a transition probability matrix $P(t)=e^{Qt}$ on time horizon $[0,T]$ . I have noticed in literature that identifiability of $Q$ is always assumed; however, I can hardly imagine that this cannot be proven. Unfortunately, I get stuck on where to look using the definition of identifiability \begin{equation}
P_{Q_{1}}=P_{Q_{2}}\implies Q_{1}=Q_{2}
\end{equation} and the MLE \begin{equation}
q_{ij}=\frac{N_{ij}(T)}{R_{i}(T)}
\end{equation} with $N_{ij}(t)$ the number of $i\rightarrow j$ transition up to time and $R_{i}(t)=\int_{0}^{t}1_{\{X(u)=i\}}du$ for $t\in[0,T]$ and any $i,j\in S$ with $j\neq i$ and \begin{equation}
q_{ii}=-\sum_{j=1,j\neq i}q_{ij}
\end{equation} Does anyone have some tips on where to start on or literature proving identifiability?","['statistics', 'transition-matrix', 'markov-chains', 'maximum-likelihood']"
4348822,Verifying Bertini's theorem for the hypersurface $V = \mathbb{V}(x^5 + y^5 + z^5 + t^5)$ in $\mathbb P^3$.,"I want to verify the theorem of Bertini in the specific case of $V = \mathbb V(x^5 + y^5 + z^5 + t^5) \subset \mathbb P^3$ . Because there are many different versions of this theorem, below the one I use. Let $V \subset \mathbb P^n$ be a smooth irreducible projective variety. Consider the set $W = \{ H \in \check{\mathbb P}^n \mid H \cap V \text{ is smooth}\}$ of all hyperplanes in $\mathbb P^n$ intersecting $V$ in a smooth variety.
Then W is an open subvariety of $\check{\mathbb P}^n$ .
That is, the set of hyperplanes having singular intersection with $V$ forms a Zariski-closed subset of $\check{\mathbb P}^n$ . (Here $\check{\mathbb P}^n$ denotes the dual of $\mathbb P^n$ and a hyperplane $H = \mathbb V\left(\sum_{i=0}^n a_i x_i\right) \subset \mathbb P^n$ corresponds to the point $[a_0:a_1:\cdots:a_n] \in \check{\mathbb P}^n$ .) An invitation to algebraic geometry by Smith, Kahanpää, Kekäläinen and Traves In other words, I want to prove that $\{ H \in \check{\mathbb P}^3 \mid H \cap V \text{ is singular} \}$ is closed. I want to do this by finding the equations that describe this set. My attempt below, inspired by the following statement in this post , Let $X \subset \mathbb P^n$ be a smooth irreducible projective variety.
Let $H \subset \mathbb P^n$ be a hyperplane.
The hyperplane section $H \cap X$ is smooth at $p$ if and only if the hyperplane $H$ is not tangent to $X$ at $p$ . So if $H = \mathbb V(ax + by + cz + dt)$ is a hyperplane, then $H \cap V$ is singular if and only if there exists some $p \in H \cap V$ such that $H$ is tangent to $X$ at $p$ .
Since $H$ and $V$ are smooth, their tangent spaces are two-dimensional, and so $H$ is tangent to $X$ if these tangent spaces coincide.
If $p = [p_0:p_1:p_2:p_3] \in H \cap V$ , then $$
T_pH = H = \mathbb V(ax + by + cz + dt) \qquad T_pV = \mathbb V\left(p_0^4 x + p_1^4 y + p_2^4 z + p_3^4 t\right).
$$ Now $T_pH = T_pX$ if and only if there exists some $\lambda \in \mathbb C \setminus \{0\}$ such that $$
\lambda (ax + by + cz + dt) = p_0^4 x + p_1^4 y + p_2^4 z + p_3^4 t.
$$ This is equivalent with requiring $$
\lambda a = p_0^4, \quad\lambda b = p_1^4,\quad \lambda c = p_2^4 \quad\text{ and }\quad \lambda d = p_3^4.
$$ To summarize what I have found: a hyperplane $\mathbb V(ax + by + cz + dt)$ belongs to $\{ H \in \check{\mathbb P}^3 \mid H \cap V \text{ is singular} \}$ if and only if there exist $p=[p_0:p_1:p_2:p_3] \in \mathbb P^3$ and $\lambda \in \mathbb C \setminus \{0\}$ satisfying the following system of equations: $$
\begin{align}
p_0^5 + p_1^5 + p_2^5 + p_3^5 &= 0 \\
ap_0 + bp_1 + cp_2 + dp_3 &= 0 \\
\lambda a - p_0^4 &= 0 \\
\lambda b - p_1^4 &= 0 \\
\lambda c - p_2^4 &= 0 \\
\lambda d - p_3^4 &= 0
\end{align}
$$ From this, I would like to obtain some condition that $[a:b:c:d]$ has to satisfy, but I do not see how I can proceed.
The main problem is that these equations still contain the point $p$ and the parameter $\lambda$ , so I thought of using the multipolynomial resultant somehow, but the polynomials above are not homogeneous when viewed as polynomials in the variables $p_0,p_1,p_2,p_3,\lambda$ . How can I go from these equations to a condition on the coefficients $[a:b:c:d]$ ?",['algebraic-geometry']
4348852,"For $a, b, c$ with $a+b+c=0$ prove $\frac15\sum a^5=\frac13\sum a^3\cdot\frac12\sum a^2$ and $\frac17\sum a^7=\frac15\sum a^5\cdot\frac12\sum a^2$","Consider the following problem: Problem. Suppose that real numbers $a$ , $b$ and $c$ satisfy the condition $a+b+c=0$ . Prove the following identities: $$
\frac{a^5+b^5+c^5}{5}=\frac{a^3+b^3+c^3}{3}\cdot\frac{a^2+b^2+c^2}{2},
\\
\frac{a^7+b^7+c^7}{7}=\frac{a^5+b^5+c^5}{5}\cdot\frac{a^2+b^2+c^2}{2}.
$$ Perhaps, the shortest solution I can think of is as follows: plug $c=-a-b$ into the equation but instead of expanding everything use the following identities $$
(a+b)^3-a^3-b^3=3ab(a+b),
\\
(a+b)^5-a^5-b^5=5ab(a+b)(a^2+ab+b^2),
\\
(a+b)^7-a^7-b^7=7ab(a+b)(a^2+ab+b^2)^2.
$$ However, these identies are coming out of nowhere and moreover, in order to prove them one still needs to do some computations. Question. Is it possible to solve this problem in a ""smart"" way (i.e. avoiding computations and preferrably elementary since it is almost a high school problem)? Any other solutions are also welcome. Comment. It should be also noted that it is unclear (at least for me) how those identies were invented. It seems there is no nice similar identities for $\frac{a^p+b^p+c^p}{p}$ for $p$ other than 2, 3, 5, 7.","['contest-math', 'algebra-precalculus', 'polynomials']"
4348859,Are the following sets of coordinates affinely equivalent?,"Given the coordinates $\{(0,t^{k}),(t^{k},0)\}$ for $k\in\{1,...,n\}$ and for $t>1$ , and $\{(\alpha s^{k-1},0),(0,\alpha s^{k-1})\}$ for $k\in \{1,...,n\}$ and for $s>1$ and $\alpha>0$ , are the two sets of coordinates affinely equivalent to one another? I know that affine equivalence must preserve the parallelity / collinearity of lines, which both sets of points seem to do with respect to the other set. Is there anything additional that I should also take into consideration?","['euclidean-geometry', 'geometry', 'discrete-mathematics', 'affine-geometry']"
