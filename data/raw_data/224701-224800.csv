question_id,title,body,tags
4626242,How do you find the area of a triangle given two $2D$ vectors?,"Question: The absolute value of the determinant of a matrix computes the volume of the shape
defined by its row vectors. In $2D$ , this is the area of the parallelogram. What is the area of a triangle with vectors $u = (3, 6)$ , $v = (8, 10)$ as sides? I found a lot of resources of finding the area with $3D$ vectors, but none for $2D$ . In this case, I'm not able to follow the steps of finding the cross product first, then magnitude and finally, the area of the triangle. How do I solve this?","['matrices', 'area', 'linear-algebra', 'geometry']"
4626243,Will my robot go everywhere? It takes steps of length $1$ and turns left $k$ radians before its $k$th step.,"A robot takes steps of length $1$ and turns left $k$ radians before its $k$ th step. Will it get arbitrarily close to any chosen point in the plane of it motion? Suppose it starts at $(0,0)$ facing in the positive $x$ direction. It turns left $1$ radian, then steps forward $1$ . Its coordinates, after $n$ steps, can be expressed as: $$x=\sum_{k=1}^n \cos{\left(\frac{k(k+1)}{2}\right)}$$ $$y=\sum_{k=1}^n \sin{\left(\frac{k(k+1)}{2}\right)}$$ Here is a desmos animation showing its movement. It seems plausible that it will get arbitrarily close to any point, if given enough steps. My attempt I considered the facts that $$\sum\limits_{k=1}^n \cos k = \frac{\cos\left(\frac{n+1}{2}\right)\sin\left(\frac{n}{2} \right)}{\sin\left(\frac{1}{2}\right)}$$ $$\sum\limits_{k=1}^n \sin k = \frac{\sin\left(\frac{n+1}{2}\right)\sin\left(\frac{n}{2} \right)}{\sin\left(\frac{1}{2}\right)}$$ I tried to derive similar formulas for the robot's coordinates. I got $$x=\text{Re}(e^{i}+e^{3i}+e^{6i}+...)$$ $$y=\text{Im}(e^{i}+e^{3i}+e^{6i}+...)$$ But this doesn't seem to help. EDIT : For what it's worth, $\sum\limits_{k=1}^{n} \sin(k^2)$ is unbounded . EDIT2 Clarification: When I ask whether the robot will get arbitrarily close to any chosen point in the plane of its motion, I am considering only its locations at the end of each step, not the line segments of length $1$ between steps.","['harmonic-analysis', 'trigonometry', 'complex-numbers', 'sequences-and-series']"
4626262,Expected Euclidean norm of a random vector,"Let $\mathbf{v} = (v_1,\ldots,v_n) \in \mathbb{R}^n$ be a vector, whose coordinates are i.i.d random variables with zero mean and standard deviation $\sigma$ . Using Jensen's inequality, we obtain $$
{\mathbb{E}[\|\mathbf{v}\|]} \leq \sqrt{ \mathbb{E}[\|\mathbf{v}\|^2 } = \sqrt{ \sum_{i=1}^n \mathbb{E}[v_i^2] } = \sigma \sqrt{n}.
$$ Question: Is this bound asymptotically sharp, i.e., does it hold that $\lim_{n \to \infty} \frac{\mathbb{E}[\|\mathbf{v}\|]}{{\sqrt{n}}} = \sigma  $ ? According to this answer , the strong law of large numbers ""suggests"" that this is the case. I am not sure, whether ""suggests"" is meant here in a heursitic sense, or whether it is supposed to mean ""implies"". I am by no means an expert in probability theory, and therefore unfortunately fail to see, how the statement $\lim_{n \to \infty} \frac{\mathbb{E}[\|\mathbf{v}\|]}{{\sqrt{n}}} = \sigma$ would follow from the law of large numbers. As far as I understand it, the strong law of large numbers (together with the continuous mapping theorem) only implies that $$
\sqrt{\frac{v_1^2+\ldots+v_n^2}{n}} \xrightarrow[]{a.s.} \sigma.
$$ But as shown in this answer , almost sure convergence does not necessarily imply convergence of the mean.","['expected-value', 'probability-theory', 'asymptotics']"
4626285,How to find Zadeh's extension of a function like this?,"I'm learning fuzzy logic and i don't find many examples explaining Zadeh's extension principle I found this one but i don't know how to solve it. Can you help me ? Let us consider two fuzzy subsets $A$ and $B$ defined by their membership functions $\mu_{A},\mu_{B}:\{1,2,3,4,5\}\rightarrow\{0, α, β, 1\}$ , where $0 < \alpha < \beta < 1$ and $A = \sum_{i=1}^{5}\mu_{A}(x_{i})/x_{i}=0/1 + \alpha/2 + 1 / 3 + 1 / 4 + α / 5$ $B = \sum_{i=1}^{5}\mu_{B}(x_{i})/x_{i}=0/ 1 + 0 /2 + \beta /3 + 1 / 4 + \beta/ 5$ (This is the standard notation to represent a fuzzy set with countable support). Using the principle extension of Zadeh :
suppose $f$ is a function with $n$ arguments that maps a point in $\omega$ to point in $V$ , determine the membership function of $A + B$ and $\min\{A,B\}.$ Thanks.","['fuzzy-logic', 'fuzzy-set', 'logic', 'discrete-mathematics']"
4626348,Area of Two Inequalities in the $XY$ Plane,"This was question B1 on the 2014 CMI (Chennai Mathematical Institute) Entrance Examination for Undergraduate Courses. We were asked to find the area of the region in the $XY$ plane consisting of all points in the set $S=\{(x,y):x^2+y^2\leq144 \text{ and} \sin(2x+3y)\leq0\}.$ While this question is interesting, I do not know how to even begin. To me, it isn't readily obvious why this set has to have an area at all. What if only finitely many values of $x$ and $y$ satisfy the above conditions? I mean, given that the question is asking for the area, I can guess that there are infinitely many such pairs $(x,y),$ but apart from that, I don't know what to do. I suppose we could begin by writing $S=S_1\cap S_2,$ where $S_1=\{(x,y):x^2+y^2\leq144\},$ and $S_2=\{(x,y):\sin(2x+3y)\leq0\}.$ What if we look for a more general answer by putting $S_1=\{(x,y):x^2+y^2=r^2,r\in\mathbb{R}\setminus\{0\}\},$ and $S_2=\{(x,y):\sin(ax+by),a,b\in\mathbb{R}\setminus\{0\}\}?$","['contest-math', 'coordinate-systems', 'trigonometry']"
4626368,Proving: $\partial_v f(a)$ exists if and only if so does $\partial_v f_k(a)$ $(\forall k)$ (in Normed Vector Spaces).,"I'm rereading some sections of Spivak's Calculus on Manifolds and attempting to generalize some results (usually to Normed Vector Spaces). I'm struggling to prove the (generelized version of) the theorem below, so I thought of asking for a hand. Throughout the post let $V$ and $W$ be NVSs, with $V'\subseteq V$ open and $f$ a function $V'\to W$ . Definition: given $a\in V'$ and a vector $v\in V$ , we define (assuming the limit exists) $$\frac{\partial f}{\partial v}(a) := \lim_{t\rightarrow 0}\frac{f(a+tv)-f(a)}{t},$$ and, in the case $v=e_i$ for a basis vector $e_i$ , $$\frac{\partial f}{\partial x_i}(a) := \frac{\partial f}{\partial e_i}(a).$$ Theorem: let $\dim W = m$ so that $f(v)=(f_1(v), \ldots ,f_m(v))$ . Then $\partial_v f(a)$ exists if and only if so does $\partial_v f_k(a)$ $(\forall k)$ , in which case $$\frac{\partial f}{\partial v}(a) = \sum_{k=1}^me_k\frac{\partial f_k}{\partial v}(a).$$ Proof: \begin{equation}
\begin{split}
\frac{\partial f}{\partial v}(a) & = \lim_{h\to 0}\frac{f(a+hv)-f(a)}{h}\\
& = \lim_{h\to 0}\left(\sum_{k=1}^me_k\frac{f_k(a+hv)-f_k(a)}{h}\right)\\
& \stackrel{*}{=} \sum_{k=1}^me_k\left(\lim_{h\to 0}\frac{f_k(a+hv)-f_k(a)}{h}\right)\\
& = \sum_{k=1}^me_k\frac{\partial f_k}{\partial v}(a).\\
\end{split}
\end{equation} In my attempt to justify the equality $\stackrel{*}{=}$ , I've come up with -what I believe is- a lemma which if true would legitimize the equality. However, now I find myself struggling to prove said lemma. Is the lemma below true? If so, how could one prove it? If not, how else could one justify the equality $\stackrel{*}{=}$ ? Lemma (?): Let $X$ be an arbitrary topological space with $p$ a limit point of $X'\subseteq X$ . Let $W$ be a topological vector space with $\{e_\alpha\}_{\alpha\in A}$ as a basis of $W$ , and let $f:X'\to W$ be a function. Then $$\lim_{x\to p} f(x) = L \iff \lim_{x\to p} f_\alpha(x) = L_\alpha \ (\forall \alpha\in A)$$ where by $f_\alpha$ (or $L_\alpha$ ) I mean the $\alpha$ component of $f$ (or $L$ ).","['multivariable-calculus', 'solution-verification', 'normed-spaces', 'analysis']"
4626395,Can I apply power rule to the derivative of constant function?,"I just saw someone trying to show $\frac{d f(x)}{d x}=\frac{d k}{d x}=0$ by arguing that $$f(x)=k=k\cdot x^0;$$ thus, according to the power rule, $f'(x)=0\cdot k x^{-1}=0.$ I wonder if this is mathematically valid in terms of general application of the power rule. In other words, can I apply the power rule even to constant functions?",['derivatives']
4626400,Effects of the fact that the idèles have a finer topology than the adèles,"The adelic ring (of, say, $\mathbb{Q})$ has a topology given by a basis of open sets of the form $$U_S \times \prod_{p \notin S} \mathbb{Z}_p$$ where $U_S$ is a finite product of open sets $U_v$ of $\mathbb{Q}_v$ . The group of idèles has a topology given by a basis of open sets of the form $$U_S \times \prod_{p \notin S} \mathbb{Z}_p^\times$$ where $U_S$ is a finite product of open sets $U_v$ of $\mathbb{Q}_v^\times$ . I do not understand why is the idèle topology finer than the adèle topology? (is there an explicit set which is open for one but not for the other? we also need to show that all the open sets for one is open for the other) how important this distinction is in practice? (so we have examples or counter examples of what it would be that they have the same topology, to develop an intuition of the difference between both?) What is the relation with the adelic norm? I think that none of these is given by the adelic norm, what is the point of this norm then? And if these topologies are so different, how can it be that we embed the idèles inside the adèles?","['number-theory', 'general-topology', 'adeles']"
4626405,How do you determine the number of tuples such that $x+y+z= 30$ and any integer cannot be greater than 15?,"How do you determine the number of tuples such that $x+y+z= 30$ and any integer cannot be greater than 15? First I calculated all the the possible tuples by $\binom{32}{2}$ or 496. Then I tried to figure out how many cases there were where any integer has is greater than 15. I found it to be $\binom{16}{2}\times 3$ which is equal to $360$ . Then I subtracted to obtain $136$ but apparently this is incorrect? Can someone let me know what I did wrong and how to approach the problem? Thanks. Edit : The full question is jocko, wocko, and docko plan to buy a new toilet for their bathroom. The toilet costs 300
and each of them has saved 200 dollars in 10 dollar bills. They decide that each of them will start with contributing 50 dollars  in 10 dollar bills(everyone contributes at least 50 dollars to the purchase.) Then, each of them will contribute some amount more money to reach the required 300 dollars to buy the toilet. For each $m \in \{\text{jocko, wocko, docko} \}$ , we denote by $a_m$ the number of 10 dollar bills that each person m is left with after they buy the toilet. Count the number of possible triples of $\{jocko_m, wocko_m, docko_m \}$ . Note that any person can contribute 0 dollars towards the purchase.","['combinations', 'combinatorics']"
4626510,A proof that the null linear mapping is the only one whose matrix representation does not depend on the basis [FALSE],"I would like to show the fact that the linear mapping $$
L : E\to E
$$ $$
x\mapsto0_{E}
$$ is the unique linear mapping whose matrix representation does not depend on the choice of the basis. My attempt : Consider $\mathcal{V}$ and $\mathcal{\hat{V}}$ two basis of $E$ . First we show that $A$ , the matrix representation of $L$ , does not depend on the basis. To do so just consider the image of any vector of the basis $\mathcal{V}$ : $$
L(v_i) = 0
$$ clearly its coordinates in the basis $\mathcal{V}$ are all zeros. This gives us the null matrix of size $n\times n$ . Now consider the coordinate of $L(v_i)$ in the basis $\mathcal{\hat{V}}$ , by linear independance of the $\hat{v}_i$ 's its coordinates are also zeros, this yields the null matrix. An analogous reasoning with the base $\mathcal{\hat{V}}$ as starting point allows to conclude. Now to prove the uniqueness, consider $F$ a linear mapping which is not the null linear mapping and does not depend on the choice of the basis $\mathcal{V}$ and $\mathcal{\hat{V}}$ . So what happens if I consider the image $F(v_i)$ and $F(\hat{v_i})$ ? No reason to be the same at first since it is the coordinates of the image $F(v_i)$ and $F(\hat{v}_i)$ which should be the same no matter which base is chosen. But if it does not depend on the choice of the basis, I should have the same matrix wether I consider the basis $\mathcal{V}$ in the space of arrival or the basis $\mathcal{\hat{V}}$ , but if I do so and decide to take the same basis on the space of arrival, then if their coordinates coincide, they are the same vector. Thus, we should have $$
F(v_i) = F(\hat{v}_i),\quad\forall i \in\{1,...,n\}
$$ which implies $$
F(v_i) - F(\hat{v}_i) = 0_{E}\implies F(v_i-\hat{v}_i) = 0_{E}\implies v_i = \hat{v}_i,\quad\forall i \in\{1,...,n\}
$$ but the bases $\mathcal{V}$ and $\mathcal{\hat{V}}$ are not the same, which yields the contradiction. I would like to know if the proof is correct please. Thank you a lot ! EDIT This is false , thanks to Federico Fallucca for his answer. My mistake is at the last implication.","['matrices', 'solution-verification', 'linear-algebra', 'linear-transformations', 'hamel-basis']"
4626571,Can we axiomatize the complex numbers without directly defining the reals?,"I've decided to attempt the entire Rudin sequence in a single 6 month period, because I'm insane. Rudin spends very little time on foundational matters, and that bothers me, it makes the subject of analysis feel less philosophically sound. So I've been adding the missing axioms and proofs as I go. I've defined the real numbers as the unique model (up to isomorphism) of the second-order theory over the language $\langle+,\cdot,<\rangle$ or $\langle+,<,\cdot,0,1\rangle$ given by the axioms: $\forall x\forall y\forall z[(x+y)+z=x+(y+z)]$ $\forall x\forall y \forall z [(x\cdot y)\cdot z=x\cdot (y\cdot z)]$ $\forall x\forall y(x+y=y+x)$ $\forall x\forall y(x\cdot y=y\cdot x)$ $\forall x\forall y\forall z(x\cdot(y+z)=(x\cdot y)+(x\cdot z))$ $\exists x\forall y(x+y=y)$ or $\forall x(0+x=x)$ $\forall x\exists y\forall z[x+y=z\implies\forall w(w+z=w)]$ or $\forall x \exists y(x+y=0)$ $\exists x\forall y(x\cdot y=y)$ or $\forall x(1\cdot x=x)$ $\forall x\forall y\forall z[(x+y=x\land x\cdot z=x)\implies y\ne z]$ or $0\ne 1$ (not sure if this one is necessary) $\forall x[\exists y(x+y\ne y)\implies\exists y\forall z((x\cdot y)\cdot z=z)]$ or $\forall x[x\ne0\implies\exists y (x\cdot y=1)]$ $\forall x\forall y\forall z(x<y\implies x+z<y+z)$ $\forall x\forall y\forall z((x<y\land y<z)\implies x<z)$ $\forall x\forall y(x<y\lor y<x\lor x=y)$ $\forall x\forall y\forall z[(x<y\land\exists w(w+z\ne w))\implies x\cdot z<y\cdot z]$ or $\forall x\forall y\forall z((x<y\land z>0)\implies x\cdot z<y\cdot z)$ $\forall P[(\exists xPx\land\exists y\forall x(Px\implies x<y))\implies\exists y\forall x((Px\implies x\le y)\land\forall z(z<y\implies\exists x_1(Px_1\land z<x_1)))]$ I picked these axioms over say, the Tarski axioms, because the standard first-order characterization can be easily recovered by replacing the second-order completeness axiom with its first-order equivalent. That way, I can keep pretending that I'm not using SOL as long as I'm subtle with my wording (technically, this is called ""lying,"" but the belief that analysis is somehow a first-order endeavor, and that first-order logic is somehow the ""true"" logic, is so deeply ingrained in the subject that I've given up correcting people.) Now I want to define the complex numbers. I can think of several ways to do this, but all of them make explicit reference to the real numbers, and that bugs me. You shouldn't need to define the very specific Dedekind complete ordered field of real numbers to define the complex numbers; it should be possible to recover it from an adequate single-sorted characterization of the complex numbers as a field (up to isomorphism.) Yet every set of axioms I can find or invent ends up directly referencing either $\Bbb R^2$ , by way of needing a predicate for ""real"" or implicitly defined projection functions (e.g. $\forall x\exists!y\exists!z\cdots$ := "" $x=y+iz$ ""), or the topology on $\Bbb C$ (the second-order characterization of the Euclidean topology on $\Bbb C$ suffices to characterize all analysis-relevant properties.) We don't usually define the real numbers to by a two-sorted theory describing such-and-such ordered field containing $\Bbb Q$ . Instead, all of the expected properties just arise from the ordering on the field as though by miraculous coincidence. Why should the complex numbers be different? How can I ""properly"" axiomatize the complex numbers (over $\langle+,\cdot\rangle$ , $\langle+,\cdot,0,1\rangle$ , or $\langle+,\cdot,0,1,i\rangle$ ) so that the associated properties (the ordering of the reals, conjugation, the standard topology, etc.) are implied by the axioms without being stated outright? And how can I do it in a way that let's me pretend that SOL exist in different universe from analysis?","['axioms', 'foundations', 'analysis', 'second-order-logic']"
4626583,What is wrong with this proof of the derivative product rule?,"I was trying to prove the derivative product rule, but I got a wrong result and nothing seems wrong with my proof. If someone could help me, I'd really appreciate it. Thanks  in advance $f(x) = g(x)h(x)$ Prove $f^\prime (x) = g(x)^\prime h(x) + g(x)h(x)^\prime$ My proof: \begin{align}
f^\prime (x)
&= \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} \\
&= \lim_{h \to 0} \frac{g(x+h)h(x+h) - g(x)h(x)}{h}\\ 
&=\lim_{h \to 0} \left(\frac{g(x+h)}{h} \cdot h(x+h)\right)-\lim_{h \to 0} \left(\frac{g(x)}{h} \cdot h(x) \right)\\
&= \lim_{h \to 0} \frac{g(x+h)}{h} \cdot \lim_{h \to 0} h(x+h) - \lim_{h \to 0} \frac{g(x)}{h} \cdot \lim_{h \to 0} h(x)\\
&= h(x) \cdot \lim_{h \to 0} \frac{g(x+h)}{h} - h(x) \cdot \lim_{h \to 0} \frac{g(x)}{h} \\
&= h(x) \left( \lim_{h \to 0} \frac{g(x+h)}{h} - \lim_{h \to 0} \frac{g(x)}{h} \right)\\
&= h(x) \cdot \lim_{h \to 0} \frac{g(x+h)-g(x)}{h} \\
&= h(x) \cdot g^\prime (x)
\end{align}","['limits', 'calculus', 'solution-verification', 'derivatives']"
4626594,Is it valid to prove this trigonometric identity by substituting this?,"I wish to prove that $\sin{(\pi/2 -x)} = \cos{(x)}$ . I have substituted $x+\pi/2$ for $x$ to eventually show equality between the right and left hand sides of this expression using the double angle formula.
My question is: does the fact that the expression I want to prove is true for $x+\pi/2$ as $x$ prove that it is true for all values of $x$ ? One the one hand, my thought is that the range of values which can be the output of the function $f(x)= x+\pi/2$ is exactly the same as the set of values which can be the output of the function $f(x)=x$ ; however, I feel that this equality was only able to be shown due to the $\pi/2$ being added to $x$ , which isn't identical to $x$ itself.
for context, I am not very well versed in proofs yet, as I am a second year computer science student.",['trigonometry']
4626596,Prove that $\angle AED = 2 \cdot \angle BEC$,"Let $ABCDE$ be a pentagon such that $AE = ED$ , $BC = DC + AB$ and $\angle BAE + \angle CDE = 180°$ . Prove that $\angle AED = 2 \cdot \angle BEC$ . So, by constructing it in Geogebra, I noticed that if I mark a point $F$ in $BC$ such that $CF = CD$ and $BF = AB$ , then $EF = ED = AE$ , and the triangles $\triangle CEF$ and $\triangle CDE$ are congruent. Same thing about $\triangle BEF$ and $\triangle BAE$ . Then $\angle AEB = \angle BEF$ and $\angle CEF = \angle CED$ and then the problem is basically done. But how can I prove $EF = ED = AE$ ? I wasn't able to prove it, so I wasn't able to progress any further. I tried proving $\angle ECF = \angle ECD$ , but still, I got stuck.","['contest-math', 'euclidean-geometry', 'congruences-geometry', 'geometry', 'triangles']"
4626612,Solving $\sin x=\cos 2x$ by expressing left side as $\cos (\pi/2-x)$ doesn't give correct solution,"I can solve the equation in different ways but I'm not getting the right answer when solving as below: $$
\sin x=\cos 2x \\
\cos(\pi/2-x)=\cos 2x \\
\pi/2-x = \pm2x+2\pi k$$ $$x = -\pi /2+2\pi k\\
x = \pi /6-(2\pi k)/3$$ However the second solution should be $x = \pi /6+(2\pi k)/3$ . Have checked my math multiple times. Where I'm going wrong?",['trigonometry']
4626633,Rigorous introduction to statistics for self study,"I want to start learning statistics.
I have taken proof-based Calculus 1 and 2, as well as two proof-based courses in Linear Algebra (1 and 2). What is a good introductory-level (but rigorous enough) book to start self-learning statistics? Also, if you know about any other resources (other than books) to help my self study, you can recommend them. Thanks in advance!","['self-learning', 'statistics', 'book-recommendation', 'reference-request', 'soft-question']"
4626656,Find The Locus of the Triangle When the Difference of Its Base Angles are Given,"This problem is found in S.L. Loney's book on The Elements of Coordinate Geometry. The problem is pronounced in full as follows: The base BC (=2a) of a triangle ABC is fixed; the axes being BC and a perpendicular to it through its middle point, find the locus of the vertex A, when the difference of the base angles is given (=a) The way I attempted at it was: I realised that once the tangent of one of the angles was $=$$y\over x + a$ then the other would be equal to $=$$y\over x — a$ . Hence let the tangent of one of the base angles, B (say), be $y\over x + a$ ; and let the tangent of C then be $y\over x — a$ . And it is given that $B — C = a$ . Taking the tangent of both sides of the last equation, we get: $\tan (B — C) =$$\tan B — \tan C \over 1 + \tan B \tan C$ . Since we know the values of all these expressions we can substitute to get: $\tan a =$$—2ay \over x^2 + y^2 — a^2$ Algebraically rearranging this equation we get $x^2\tan a + y^2\tan a + 2ay — a^2\tan a = 0$ or much more concisely: $\tan a(x^2 + y^2 — a^2) + 2ay = 0$ I am still knew to the grand picture of Coordinate Geometry, hence I seek affirmation from the more profesional of the answer I have provided to satisfy the problem. If I have made any fallacy in reasoning, reader, please say so and show me how its properly done. Thank you in advance.","['analytic-geometry', 'coordinate-systems', 'geometry', 'solution-verification', 'problem-solving']"
4626699,On the integral: $\int_{0}^{\frac{\pi}{4}} \ln(1+\tan(x)) dx$,"This semester, I take a compulsory subject in advanced calculus. One of beautiful topic is integration. However, I am not sure enough that my solution (step) are correct. $$\int_{0}^{\frac{\pi}{4}} \ln(1+\tan(x)) dx$$ We are going to use this 2 properties.
First, $$\int_{0}^{a} f(x) dx = \int_{0}^{a} f(a-x) dx.$$ Second, $$1+ \tan\left (\frac{\pi}{4}-x\right) = \frac{2}{1+\tan(x)}.$$ We obtain $$\int_{0}^{\frac{\pi}{4}} \ln(1+\tan(x)) dx = \int_{0}^{\frac{\pi}{4}} \ln(2) dx - \int_{0}^{\frac{\pi}{4}} \ln(1+\tan(x)) dx = \frac{\pi}{8}\ln(2).$$ My Question I am going to do this integration: $$\int_{0}^{1} \frac{\ln(x+1)}{x^2+1} dx.$$ By substituting $x = \tan(\alpha)$ , we obtain $$\int_{0}^{\frac{\pi}{4}} \ln(1+\tan(\alpha)) d\alpha = \frac{\pi}{8}\ln(2).$$ Are my step correct? I am doubt, just by substituting, and then we get the answer.","['integration', 'calculus', 'definite-integrals']"
4626716,Converting angles to the range $ -180$ deg through $180$ deg,"Is there a direct formula to transform a set of angles to $-180$ through $180$ deg? For instance, $181$ deg and $541$ deg should be translated to $-179$ , while $-182$ deg and $-542$ deg should be translated to $178$ . I know that for an angle $X$ which is not on any axis ( $0, 90, 270, 360, ...$ ), when $\text{mod}(a,b)$ is the remainder of division of $a$ by $b$ , we can write use $k = \lfloor(\frac{\text{mod}(X,360)}{90})\rfloor$ to determine the quadrant where angle $X$ resides since $k = 0, 1, 2, 3$ corresponds to quadrants $1, 2, 3, 4$ respectively. So for angle $X$ : $k = 0$ or $1$ : $\text{mod}(X,360)$ gives the $0$ through $180$ range. $k = 2$ or $3$ : $\text{mod}(X,360)-360$ gives $-180$ through $0$ range. Is there a way to make this simpler?","['trigonometry', 'angle']"
4626718,Parametric Equations for Line(s) Tangent to a Sphere That Pass Through Point Outside of Sphere,"Question asks for a the parametric equations for the set of all lines that pass through the point $(4,0,0)$ and are tangent to the curve: $$x^2+y^2+z^2=2$$ My issue here is that I am not really sure how to approach the problem. The previous question asked for a similar thing but it wasn't as complicated as it was just the equation of a circle and not a sphere. Nevertheless, it was still the same point and it was still outside of the equation of the circle. I am assuming there will be a general solution for this question since the question implies that there will be more than one line that passes through the point and is tangent to the sphere. Am I supposed to utilize gradients to solve this? Or maybe the 3D tangent line equation? I am genuinely not sure how to approach this problem. Any help would be appreciated.","['multivariable-calculus', 'linear-algebra']"
4626751,Integral and inequality,"Let $p(u,x):=(4 \pi u)^{-1/2}e^{-\frac{x^2}{4u}},u>0,x \in \mathbb{R}.$ Let $\phi \in C_c^{\infty}(\mathbb{R}),\text{supp}(\phi) \subset B(0,1),||\phi||_{\infty} \leq 1.$ Prove that for all $U>0,\beta>1/2,$ there exist $\epsilon>0,C>0$ such that for all $u\in [0,U],\lambda \in \left]0,1\right],$ $$\int_0^{u} \int_{\mathbb{R}} \left(\int_{\mathbb{R}} \phi^\lambda(y_1)p(r,y_1-y_2) dy_1 \right)^2 dy_2 dr\leq Cu^\varepsilon \lambda^{1-2\beta},$$ where $\phi^\lambda(y) = \lambda^{-1} \phi(\lambda^{-1}y).$ I tried, using a change of variable, replacing $\phi^{\lambda}$ with $\phi.$ also $\lambda(B(0,1))<\infty$ might be useful. How can we prove this inequality?","['integration', 'measure-theory', 'lebesgue-integral', 'analysis', 'real-analysis']"
4626789,Polish Olympiad Problem,"Given the above problem, the solution seems trivial. Shouldn't it just be $$\dfrac{180^\circ-40^\circ-90^\circ }{2}=25^\circ ?$$ However because this is an olympiad problem, I think I might have gotten the answer wrong. How do you work this problem out? What am I doing wrong?","['contest-math', 'angle', 'geometry', 'triangles', 'problem-solving']"
4626899,Finding the maximum cycle of a given set,"Problem: Given $4$ circles, we define the following set of rules: i) Any circle which contains $\ge 3 $ elements transfers exactly one of its elements to each of other $3$ circles. ii) Circles which contain $<3$ number of element do not transfer any of its elements to the other three circles. In one operation all rules are applied simultaneously. There are two states which can be achieved under these rules: This is a stable state reached. This is an oscillating state. Question: I am curious to find what is the maximum value of $k$ such that some initial configuration cycles through $k$ distinct configurations and returns to the original configuration (for the first time) after $k$ operations? $k$ is also called the least period of the transformation.","['periodic-functions', 'combinatorics', 'pattern-recognition']"
4626929,Show that the sequence $a_n=1-\frac13+\frac{1}{3^2}-...+(-1)^n\frac{1}{3^n}$ is bounded.,"Show that the sequence ${a_n}$ where $$a_n=1-\dfrac13+\dfrac{1}{3^2}-...+(-1)^n\dfrac{1}{3^n}$$ is bounded. The first thing that came to my mind was to see if the sequence is monotone. If I am right, the $(n+1)th$ term should look like this: $$a_{n+1}=1-\dfrac13+\dfrac{1}{3^2}-...+(-1)^{n+1}\dfrac{1}{3^{n+1}}$$ and then the difference $a_{n+1}-a_n$ is $$a_{n+1}-a_n=\dfrac{(-1)^{n+1}}{3^{n+1}},$$ the sign of which depends on $n$ . If we write the first terms $$a_1=1;a_2=\dfrac{2}{3}=\dfrac{6}{9},a_3=\dfrac{7}{9},$$ we can see that the sequence isn't monotone. Another thing that I noted is that we actually have the sum of a geometric sequence with first term $b_1=1$ and common ratio $-\dfrac13$ . That is for the general term, the sum is $$S_n=\dfrac{\left(-\frac13\right)^n-1}{-\frac43}$$",['sequences-and-series']
4626936,"$f$ non-constant and holomorphic, then for each $z_0$ there is a neighborhood where $f$ must take a different value from $f(z_0)$","Let $f:\Omega \to \mathbb{C}$ be non-constant and holomorphic function and $\Omega\subseteq\mathbb{C}$ be an open region. We will show that for any $z_0\in\Omega$ , there is a neighborhood of $z_0$ such that for all $z\not=z_0$ in this neighborhood, $f(z)\not=f(z_0)$ . To do this, we need the result that If $f,g$ holomorphic on region $\Omega$ agree on a sequence of points with limit point in $\Omega$ , they are identical on $\Omega$ . Proof : We prove the claim in the title by contradiction. Suppose that for each neighborhood of $z_0$ in $\Omega$ , there is a $z$ in that neighborhood with $f(z)=f(z_0)$ . Since $\Omega$ is open, there is a $\varepsilon>0$ such that $D_\varepsilon(z_0)\subseteq\Omega$ . Consider the monotonically decreasing sequence $(r_n=\varepsilon/n)_n$ with $r_n\to 0$ . By assumption, for each $n\in\mathbb{N}_0$ there is a $z_n\in D_{r_n}(z_0)\setminus\{z_0\}$ such that $f(z_n)=f(z_0)$ . Then $z_n\to z_0$ . By the result above this implies that $f=f(z_0)$ on all of $\Omega$ , which is a contradiction. QED Question : I've only now stumbled on this fact about holomorphic functions, even though I've been studying the complex analysis course for an entire semester. It's not mentioned in the book. Is it an obvious property that is much more easily seen than my proof above, perhaps?",['complex-analysis']
4626946,Does $X\times \mathbb{R}$ characterize $X$ as topological space?,"Let $X,Y$ be topological spaces s.t. $X\times\mathbb{R}\simeq Y\times\mathbb{R}$ . I want to know if $X\simeq Y$ ?",['general-topology']
4626981,Evaluation of a multi-variable integral,"I was reading this answer: Fourier transform of the indicator of the unit ball and couldn't really follow the computations in the answer. Specifically the equality $$\int_{\|x\| \leq 1} e^{-ix_n\rho} dx_1 \dots dx_n = \int_{-1}^1 (1−x_n^2)^{(n−1)/2} \alpha_{n−1} e^{-ix_n\rho} dx_n$$ where $\rho >0$ and $\alpha_{n-1}$ is the volume of the unit ball in $\mathbb{R}^{n-1}$ . The computation looks a lot like the ones in https://en.wikipedia.org/wiki/Volume_of_an_n-ball#The_one-dimension_recursion_formula where a recursion formula for the volume of the unit ball is derived, but I can't really wrap my head around it. Could anyone help me understand what is going on here?","['integration', 'multivariable-calculus', 'definite-integrals']"
4627046,Why variance is only defined for square integrable random variables?,"I'm learning probability theory alone so I'm having basic questions. Every book I read defined variance only for square integrable random variable. So my question is: why they exclude the case when a random variable is only integrable? If $X$ is an integrable random variable, then $(X-\mathbb{E}[X])^2$ is a non-negative measurable function and, therefore, it's Lebesgue integrable. Hence, we could define the variance of an integrable random variable $X$ as $\text{var(X)}:=\mathbb{E}[(X-\mathbb{E}[X])^2]$ . In this case we still have properties such as $\text{Var}(aX+b)=a^2\text{Var}(X)$ $\mathbb{P}(|X-\mathbb{E}[X]|\geq t)\leq \frac{\text{Var}(X)}{t^2}$ for all $t\in(0,\infty)$ using the ""generalized"" Chebyshev's Inequality (see the section "" Measure-theoretic statement "" in Wikipedia's article about the Chebyshev's Inequality).","['variance', 'probability-theory', 'probability', 'random-variables']"
4627082,Bound for the gradient (or laplacian) of a strictly convex function from above,"Let $V \in C^2(\mathbb{R}^d; \mathbb{R})$ a (strictly) convex function with $ \int_{\mathbb{R}^d} \mathrm{e}^{-V(x)} \, dx = 1.$ I am trying to show that $$ \int_{\mathbb{R}^d} |\nabla_x V(x) |^2\mathrm{e}^{-V(x)} \, dx < +\infty.$$ My ideas:
(Due to strict convexity and $V$ is not constant and smooth on $\mathbb{R}^d$ we should have $ \lim_{|x| \to \infty} V(x) = + \infty.$ Perhabs this could be useful...) $$ \int_{\mathbb{R}^d} |\nabla_x V(x) |^2\mathrm{e}^{-V(x)} \, dx = - \int_{\mathbb{R}^d} \nabla_x V(x) \cdot \nabla_x\mathrm{e}^{-V(x)} \, dx 
= \int_{\mathbb{R}^d} \Delta_x V(x) \cdot \mathrm{e}^{-V(x)} \, dx.$$ If I could estimate $\Delta_x V(x)$ in a certain way like: $$ \Delta_x V(x) \leq \alpha + \beta |\nabla_x V(x) |^2$$ with $\alpha > 0$ and $\beta \in [0;1) $ it would work. Does this hold for all (strictly) convex functions with $ \int_{\mathbb{R}^d} \mathrm{e}^{-V(x)} \, dx = 1$ ? The inequality does not hold for all strictly convex functions like for $-\mathrm{ln}(x)$ on $(0, +\infty).$ So I suppose we will need an additional condition. Or are there any other ideas? Maybe something like $$ |\nabla_x V(x) |^2 \leq a + b |V(x)|^2$$ would work too... Since V is convex and differentiable it fulfills: $$ V(x) \geq V(y) + \nabla^{\top} V(y) \cdot (x-y)$$ for all $x,y \in \mathbb{R}^d.$ That should be helpful.","['multivariable-calculus', 'convex-analysis', 'analysis']"
4627128,"What if the partial derivates don't equal to zero for any value, how to find the critical point?","Given the function: $f(x,y) = \cos y\cdot e^x$ Question is to determine the nature of its critical points.
I've calculated the partial derivatives as following : $\frac{\partial f}{\partial x} (x,y)=\cos y\cdot e^x$ $\frac{\partial f}{\partial y} (x,y)=-\sin y\cdot e^x$ None of these can equal to zero does that mean that the function doesn't have a critical point or is there another method?","['maxima-minima', 'multivariable-calculus', 'calculus', 'partial-derivative', 'optimization']"
4627208,More details in the proof of the equivalence of categories between vector bundles over a smooth manifold and locally free sheaves,"$\def\VB{\mathsf{VB}}
\def\sO{\mathcal{O}}
\def\Mod{\mathsf{Mod}}
\def\LFMod{\mathsf{LFMod}}$ For a ringed space $(X,\sO_X)$ and a ring $R$ , denote $\Mod(\sO_X)$ and $\Mod(R)$ to the categories of $\sO_X$ -modules and of $R$ -modules. Let $M$ be a smooth manifold and denote $\sO_M$ to the sheaf of smooth functions on $M$ . Denote $\VB(M)$ to the category of smooth vector bundles over $M$ with vector bundle homomorphisms. For a vector bundle $\pi:E\to M$ over $M$ , denote $\Sigma_E$ to the sheaf of sections of $\pi$ . There is a functor \begin{align*}
\Sigma:\VB(M)&\to\Mod(\sO_M)\\
E&\mapsto\Sigma_E\\
F:E\to E'&\mapsto F_*:\Sigma_E\to\Sigma_{E'},
\end{align*} where $F_*$ is induced by postcomposition by $F$ . (Actually, postcomposition induces a morphism of hom-sheaves $\mathcal{H}om_{\VB(M)}(E,E')\to\mathcal{H}om_{\sO_M}(\Sigma_E,\Sigma_{E'})$ ). Working in a local frame, one can see that $\Sigma$ is faithful by seeing that $\Gamma\circ\Sigma$ is faithful, where $\Gamma:\Mod(\sO_M)\to\Mod(\underbrace{\Gamma(M,\sO_M)}_{C^\infty(M)})$ denotes the global sections functor. Less trivial is that the functor $\Sigma$ is also full. For this, one first checks that the functor $\Gamma\circ \Sigma$ is full: this is Lemma 10.29 of Lee's Introduction to Smooth Manifolds . Leveraging Lee's proof, one can show after fullness of $\Sigma$ : Given vector bundles $E$ and $E'$ over $M$ and an $\sO_M$ -linear map $\varphi:\Sigma_E\to\Sigma_{E'}$ , denote $F^U:E|_U\to E'|_U$ to the unique vector bundle homomorphism such that $\Gamma\circ\Sigma(F^U)=\Gamma(F^U_*)=\varphi_U$ . Using the explicit definition of $F^U$ given in Lee's proof of Lemma 10.29, one can check that $F^U|_{U\cap V}=F^V|_{U\cap V}:E|_{U\cap V}\to E'|_{U\cap V}$ . Therefore, the $F^U$ 's glue to a vector bundle homomorphism $F:E\to E'$ such that $F|_U=F^U$ . Thus, since $F_*|_U=(F|_U)_*=F_*^U$ , one has $F_{*,U}=\Gamma(F^U_*)=\varphi_U$ , i.e., $F_*=\varphi$ . This shows that $\Sigma$ is a fully faithful functor and hence an equivalence of categories into its essential image. It is clear that the image of $\Sigma$ is contained in the full subcategory of locally free $\sO_M$ -modules. What I am interested is on finding a detailed proof that locally free $\sO_M$ -modules are in fact the essential image of $\Sigma$ . The only reference I am aware of is Ramanan's Global Calculus . He discusses this essential surjectivity in Chapter 2, between definitions 2.8 and 2.9. However, I find the argument sketched on his discussion a little bit unsatisfactory. So my questions are: Do you know any reference which proves the essential surjectivity of $\Sigma:\VB(M)\to\LFMod(\sO_M)$ with more detail than Ramanan does? Do you know yourself a more detailed proof of this fact than Ramanan's one? I was trying to combine the Vector Bundle Chart Lemma of Lee's book (Lemma 10.6 of the 2nd edition) with Ramanan sketch, but I wasn't sure how to actually do it.","['vector-bundles', 'equivalence-of-categories', 'smooth-manifolds', 'differential-geometry']"
4627220,Verifying $\frac{\cot\theta-1}{\cot\theta+3}=\frac{\csc^2\theta-6\cot\theta+4}{\csc^2\theta-2\cot\theta-16}$,"I need to verify the following identity: $$\dfrac{\cot(\theta) - 1}{\cot(\theta) + 3} = \dfrac{\csc^2(\theta) - 6 \cot(\theta) + 4}{\csc^2(\theta) - 2 \cot(\theta) - 16}$$ using only the Pythagorean identities. I'm not allowed to modify the other side in any way (if I pick the left, I can't change the right, and vice versa), which means I can't just cross-multiply and simplify to verify that both sides are equal. I've tried to convert the cotangents and cosecants to sines and cosines: $$\dfrac{\cot(\theta) - 1}{\cot(\theta) + 3} = \dfrac{\dfrac{1}{\sin^2(\theta)} - \dfrac{6 \cos(\theta)}{\sin(\theta)} + 4}{\dfrac{1}{\sin^2(\theta)} - \dfrac{2 \cos(\theta)}{\sin(\theta)} - 16}$$ but I don't see anywhere I could simplify by applying the reciprocal or Pythagorean identities. Is this even the right step towards proving this identity? I notice that the right side is pretty close to this variation of the Pythagorean identities: $$ \csc^2(\theta) - \cot^2(\theta) = 1 $$ but I can't use it because the cotangent is not squared. I'm guessing that the solution would use this in some way? I know this is true since they have the same graph (and Wolfram Alpha agrees).","['algebra-precalculus', 'trigonometry']"
4627262,"Is there a name for the result that ""a sum of uncountably many positive numbers cannot be finite""?","There is a result that is fairly fundamental in measure theory, and is also quite nice in its own right, and so I think it would be nice for it to have a reasonably standard name. A set $S$ is uncountable if and only if for every $M>0$ and every function $f \colon S \to (0,\infty)$ there is a finite subset $C \subset S$ such that $\sum_{x \in C} f(x) > M$ . Does this result have a name? [A roughly equivalent formulation can be found at https://math.stackexchange.com/q/994999/222867.]","['elementary-set-theory', 'measure-theory', 'terminology', 'reference-request']"
4627275,How can I approximate this characteristic function?,"Let $F\subset \Bbb{R}^d$ be a closed subset. Let us define $f(x)=\Bbb{1}_F(x)$ . I want to approximate this function by a continuous function. If $d=1$ I know how to solve this problem, but if $d$ is arbitrary I have some problems.
In internet I found someone who defined $f_m(x)=\max\{0,1-m\cdot d(x,F)\}$ and claimed that $f_m$ approximates $f$ . The problem is that I don't see how it works. Is there some intuition behind this? It would be nice if someone could explain me the inuition.","['approximation', 'functional-analysis', 'analysis']"
4627284,Finding $\int_{0}^\infty f(u)du$ where $f$ solves $\frac{\pi}{2}f(u) = \frac{1}{1+u^2} - \int_{0}^\infty \frac{f(v) dv}{4+(u-v)^2}$,"I'm looking for $I=\int_{0}^\infty f(u)du$ where $f$ is the solution to the integral equation $$\frac{\pi}{2}f(u) = \frac{1}{1+u^2} - \int_{0}^\infty \frac{f(v) dv}{4+(u-v)^2}$$ for $u$ between $0$ and $\infty$ . While knowing $f(u)$ exactly would be nice, I'd be quite happy with just $I=\int_{0}^\infty f(u)du$ or an approximation to $I$ ! My goal would be an approximation within $1$ %. I have two attempts below - one with an ansatz for $f(u)$ and the other a perturbative series for $I$ . I'm hopeful that either direction will be fruitful. Here's a sketch of my attempt that roughly estimates $I \approx .6$ . First, note that a problem with different bounds and with $u$ between $\infty$ and $\infty$ , $$\frac{\pi}{2}g(u) = \frac{1}{1+u^2} - \int_{\color{red}{-\infty}}^\infty \frac{g(v) dv}{4+(u-v)^2}$$ can be solved via Fourier transform to find $g(u) = \frac{1}{e^{\frac{\pi}{2}u}+e^{-\frac{\pi}{2}u}}$ and $\int_{0}^\infty g(u) du = \frac{1}{2}$ . I anticipate that $I$ for the problem of interest will be a little more than $.5$ , given that it appears we're subtracting less from the right hand side when we use smaller bounds. Consider the ansatz $$f(u) = \frac{c_1}{e^{\frac{c_2 \pi}{2}u}+e^{-\frac{c_2\pi}{2}u}}$$ Changing the parameters $c_1$ and $c_2$ by hand, I find with numerical integration that $c_1=1.11$ and $c_2=.93$ make the left hand side and right hand side of $$\frac{\pi}{2}f(u) = \frac{1}{1+u^2} - \int_{0}^\infty \frac{f(v) dv}{4+(u-v)^2}$$ nearly equal as a function of $u$ . I anticipate this ansatz won't include the exact solution, but it gives what I think is a reasonable estimate for $I$ of about $.6$ . Here is another attempt. Note that these linear problems can in principle be solved through perturbative solutions; $$f(u) = h(u)+\lambda \int_0^\infty K(u,v) f(v)$$ is solved by $$f(u) = \sum_{n=0}^\infty \lambda^n \int_0^\infty du_2 ... \int_0^\infty du_{n+1} K(u, u_2)...K(u_n, u_{n+1}) h(u_{n+1})$$ so $$I = \sum_{n=0}^\infty \lambda^n \int_0^\infty du_1 \int_0^\infty du_2 ... \int_0^\infty du_{n+1} K(u_1, u_2)...K(u_n, u_{n+1}) h(u_{n+1}).$$ That is, we have $$I = \sum_{n=0}^\infty \left(-\frac{2}{\pi}\right)^n \frac{2}{\pi} \int_0^\infty du_1 ... \int_0^\infty du_{n+1} \frac{1}{4+(u_1-u_2)^2}...\frac{1}{4+(u_n-u_{n+1})^2}\frac{1}{1+u_{n+1}^2}$$ The first few partial sums are $1, 0.318, 0.866$ . Assuming the terms are decreasing in the series, the alternating sum is then bounded between $0.318<I<0.866$ . This is consistent with $I\approx.6$ , but the bounds are quite large, and the integrals rapidly become both analytically and computationally challenging to evaluate.","['integration', 'definite-integrals', 'approximation', 'sequences-and-series']"
4627321,Are Functions Smooth Sections?,"In differential geometry we often identify $\Omega^0(M)$ with the smooth functions on
a smooth manifold $M$ . But for every $i>0$ we know that: $$\Omega^i(M)=\Gamma(\Lambda^i(T^*M))$$ I imagine with $i=0$ then $\Lambda^i(T^*M)$ would be the trivial line bundle since constant functions exist on $M$ . So can we think about functions as smooth sections of this trivial line bundle? If this is true, does that mean for every smooth function $f$ there is a projection such that: $$\pi\circ f=\text{Id}_M$$ If so, it would be very helpful to a problem I am currently working on...","['smooth-functions', 'smooth-manifolds', 'vector-bundles', 'differential-forms', 'differential-geometry']"
4627361,"Using 4 by 4 Lorentz boost matrices to verify the tensor transformation law, $T^{\mu'\nu'}=\Lambda^{\mu'}_\alpha\Lambda^{\nu'}_\beta T^{\alpha\beta}$","In the following question, $K^{\prime}$ is a frame moving in the positive $x$ -direction with speed $v$ relative to frame $K$ : A tensor of type $(2,0)$ is $16$ numbers, $T^{\mu\nu}$ with the transformation property $$T^{\mu^\prime\nu^\prime}=\Lambda^{\mu^{\prime}}_\alpha\Lambda^{\nu^\prime}_\beta T^{\alpha\beta}\tag{1}$$ under the Lorentz transformation $x^{\mu^\prime}=\Lambda^{\mu^{\prime}}_\nu x^\nu$ Suppose that in frame $K$ , $T^{00} = \alpha$ , where $\alpha$ is a constant and the other $15$ components of $T^{\mu\nu}$ are zero. Determine the components $T^{\mu^\prime\nu^\prime}$ in $K^\prime$ . I want to try to answer this using $4\times 4$ Lorentz boost matrix multiplication, even though there is a much faster and simpler method given by the author which will be shown at the end, though I do not understand that method given by the author. Since the $4\times 4$ Lorentz boost matrix is $\Lambda^{\mu^{\prime}}_\alpha=\begin{pmatrix}\gamma & -\beta\gamma & 0 & 0\\-\beta\gamma & \gamma & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix}$ along the $x$ -direction. Then multiplying two such boosts as in $(1)$ , $\Big(\Lambda^{\mu^{\prime}}_\alpha$ and $\Lambda^{\nu^\prime}_\beta\Big)$ in $4\times 4$ matrix format eqn $(1)$ should read, $$\begin{align}T^{\mu^\prime\nu^\prime}&=\begin{pmatrix}\gamma & -\beta\gamma & 0 & 0\\-\beta\gamma & \gamma & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix}\begin{pmatrix}\gamma & -\beta\gamma & 0 & 0\\-\beta\gamma & \gamma & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix}\begin{pmatrix}\alpha & 0 & 0 & 0\\0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\\&=\alpha\begin{pmatrix}\gamma^2(1+\beta^2) & -2\beta\gamma^2 & 0 & 0\\-2\beta\gamma^2 & \gamma^2(1+\beta^2) & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix}\begin{pmatrix}1 & 0 & 0 & 0\\0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\\&=\alpha\gamma^2\begin{pmatrix}1+\beta^2 & 0& 0 & 0\\-2\beta & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\tag{2}\end{align}$$ The calculation was performed using wolframalpha.com , where $\beta=\dfrac{v}{c}$ and $\gamma=\left(1-v^2/c^2\right)^{-1/2}$ Addressing the question, the only non-zero components of $T^{\mu^\prime\nu^\prime}$ are $$T^{0^\prime 0^\prime}=\alpha\gamma^2(1+\beta^2)=\alpha\gamma^2\left(1+\frac{v^2}{c^2}\right)$$ and $$T^{0^\prime 1^\prime}=-2\beta\alpha\gamma^2=-2\frac{\gamma^2 v \alpha}{c}$$ Now the problem is that according to the authors solution my answer above is wrong and the correct solution is As the only non-zero component of $T^{\mu\nu}$ is $T^{00}=\alpha$ we have $$T^{\mu^\prime\nu^\prime}=\Lambda^{\mu^{\prime}}_0\Lambda^{\nu^\prime}_0 T^{00}=\Lambda^{\mu^{\prime}}_0\Lambda^{\nu^\prime}_0 \alpha\tag{3}$$ Accordingly, $$T^{0^\prime 0^\prime}=\Big(\Lambda^{0^{\prime}}_0\Big)^2\alpha=\gamma^2\alpha,$$ $$T^{0^\prime 1^\prime}=T^{1^\prime 0^\prime}=\Lambda^{0^{\prime}}_0\Lambda^{1^{\prime}}_0\alpha=-\frac{\gamma^2 v \alpha}{c},$$ $$T^{1^\prime 1^\prime}=\Big(\Lambda^{1^{\prime}}_0\Big)^2\alpha=\frac{\gamma^2 v^2\alpha}{c^2}$$ Looking at the authors' solution, it's clear that he/she is explicitly writing the matrix components of $\Lambda^{\mu{^\prime}}_{\alpha}$ such that $$\Lambda^{\mu^\prime}_\alpha=\Lambda^{\nu^\prime}_\beta=\begin{pmatrix}\Lambda^{0^\prime}_0 & \Lambda^{0^\prime}_1 & \Lambda^{0^\prime}_2 & \Lambda^{0^\prime}_3 \\ \Lambda^{1^\prime}_0 & \Lambda^{1^\prime}_1 & \Lambda^{1^\prime}_2 & \Lambda^{1^\prime}_3 \\ \Lambda^{2^\prime}_0 & \Lambda^{2^\prime}_1 & \Lambda^{2^\prime}_2 & \Lambda^{2^\prime}_3 \\ \Lambda^{3^\prime}_0 & \Lambda^{3^\prime}_1 & \Lambda^{3^\prime}_2 & \Lambda^{3^\prime}_3\end{pmatrix}$$ In fact, just by inspection, the correct matrix that will give the same components for $T^{\mu^\prime\nu^\prime}$ as the authors' is $$T^{\mu^\prime\nu^\prime}=\alpha\gamma^2\begin{pmatrix}1 & -\beta & 0 & 0\\-\beta & \beta^2 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\tag{4}$$ but how to obtain this matrix is beyond my comprehension. So my questions are, Why is the logic I used in eqn $(2)$ not giving me the same answer as the author (I have only $2$ non-zero components, whereas the author has $4$ ) and how can I obtain the correct matrix form for the components, shown in eqn $(4)$ ? How did the author 'know in advance' that $T^{\mu^\prime\nu^\prime}=\Lambda^{\mu^{\prime}}_0\Lambda^{\nu^\prime}_0 T^{00}$ (from eqn $(3)$ ) would give non-zero components? Put another way, I'm asking how the author knew the lower indices on the $\Lambda$ matrices are zero even though there is a contribution to $T^{0^\prime 1^\prime}=T^{1^\prime 0^\prime}$ ?","['special-relativity', 'tensors', 'matrices', 'intuition', 'mathematical-physics']"
4627418,equivalent condition of an element being positive in a $C^*$-algebra,"Let $\mathcal{A}$ be a $C^*$ -algebra. Suppose $a,b\in\mathcal{A}$ with $a,b\geq 0$ and $\Vert a\Vert\leq\Vert b\Vert$ . Does it imply $a\leq b$ ? Comments: I could not be able to either prove it or find a counter-example. Any comment is highly appreciated. Thanks in advance.","['operator-theory', 'functional-analysis', 'analysis', 'operator-algebras']"
4627424,What is the probability that the sum of 6 four-sided dice is less than or equal to 14?,"The solution given is shown below. My question is how did they count the numerator like that?
What is the explanation for it please? $$\begin{align}
\frac{C_6^{14}-C_1^6\times C_6^{10}+C_2^6 \times C_6^6}{4^6}&=\frac{3003-6\times 210+15\times 1}{4^6}\\
&= \frac{1758}{4^6}\\
&= \frac{879}{2048}
\end{align}$$ I understand  the denominator namely because each of the four sided dice has four choices and six of them so, all possible outcomes will be $4^6$ . I believe that the 4-sided dice here has 1, 2, 3, 4 printed on its faces. Any help is appreciated.","['contest-math', 'dice', 'combinatorics', 'recreational-mathematics', 'probability']"
4627428,Are there any ways to convert inverse trigonometric values to radicals?,"When we solve a cubic equation $ax^3+bx^2+cx+d=0$ , the roots are supposed to be in the form of radicals in real numbers or complex realm. However,  if the discriminant is less than 0, the solution is ended up with roots represented by inverse trigonometric function in most cases. For example, the three roots for $x^3−4x+1=0$ are all in trigonometric form. And the equation $x^3−2x+1=0$ has 1 rational root, and two other roots that could be in radical form if solved by factorization method or inverse trigonometric values if solved by Cardano's solution and trigonometric method. By comparing their decimals, the roots obtained by two different methods are equal. My question is - are there any general ways to convert these inverse trigonometric values to radicals?","['cubics', 'trigonometry', 'inverse-function', 'radicals']"
4627432,How to handle measurements and Lebesgue integrals? (with concrete example),"Question Define the measure $\mu$ on the measurable space $(\mathbb{R}, \mathcal{B})$ as follows. $$
\mu\left( (a, b] \right) = \int_a^b \frac{1}{1+x^2}dx\ \ \mathrm{for\ all\ interval\ (a,b]}.
$$ Then, I want to calculate the following: $$
\int_{(-1, 1)} gd\mu,\ (g:=|x|)
$$ What I know I'm getting the flow of the equation transformation, but I don't understand the transformation of the question mark part. $$
\int_{(-1, 1)} |x|d\mu(x) = \int_\mathbb{R} |x|\chi_{(-1, 1)} d\mu \overset{?}{=} |x|\mu((-1, 1)) \overset{?}{=} \int_{-1}^1 |x|\frac{1}{1+x^2}dx = \log 2.
$$ The definition of measurement is understood as follows.
setup: $(X, m, \mu); \textrm{measure spaces}, s = \sum_{i=1}^n \alpha_i \chi_{A_i}, A_i = \{ x\in X; s(x) = \alpha_i \}, \alpha_i \geq 0, E \in m$ . $$
\int_E s d\mu := \sum_{i=1}^n \alpha_i \mu(A_i \cap E)
$$ In other words, the integral can be calculated for measurable single functions, but I believe the others are undefined...","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'analysis', 'real-analysis']"
4627438,Is the following function even or odd?,"Let $\frac{dx}{dw} = w^2x^4+1$ Show that $x(w)$ is an odd function. While trying to solve the problem I came across the following theorem: If $dy/dx=f(x,y)$ which is a function which is even then the solution $y_0$ is a odd function However, before trying to prove the theorem, I am confused about the fact that whether $f(x,y)$ is even in x, y or both. Can someone help me to see the intuition behind the problem?",['ordinary-differential-equations']
4627456,Evaluate $\int_{0}^{\infty}\text{sech}^2(x+\tan(x))dx$,"Evaluate the Integral: $$\int_{0}^{\infty}\text{sech}^2(x+\tan(x))dx$$ Source: MIT Integration Bee My Try: Applying Glasser's Master Theorem, the value of improper integral doesn't change. Substituting $x$ in place of $x+\tan(x)$ we have $$\int_{0}^{\infty}\text{sech}^2(x)dx=\left[ \tanh (x) \right]_{0}^{\infty}=\lim_{x \to \infty} \tanh(x)=\lim_{x \to \infty} \frac{e^{2x}-1}{e^{2x}+1}= 1$$ I don't know whether the solution is  correct; can anyone tell me please?  Also Is there any other method of solving it? Any help would be appreciated .","['integration', 'improper-integrals', 'calculus', 'infinity', 'trigonometry']"
4627483,Factor $1-64m^6$ under $\mathbb{Q}$,"Factor $1-64m^6$ under $\mathbb{Q}$ . We can employ the well known formula: $a^3-b^3 = (a-b)\cdot(a^2+ab+b^2)$ . $1-64m^6 = (1)^3-(4m^2)^3 = (1-4m^2)(1+4m^2+16m^4)=(1+2m)(1-2m)(1+4m^2+16m^4)$ . However, the answer is provided as $(1+2m)(1-2m)(1-2m+4m^2)\cdot(1+2m+4m^2)$ . I cannot understand how it is possible to simplify $(1+4m^2+16m^4)$ to $(1-2m+4m^2)(1+2m+4m^2)$ ?","['algebra-precalculus', 'factoring']"
4627494,Hausdorff dimension of $A \times A$ (Folland's exercise 11.2.15),"The next question is regarding Hausdorff measure, it was taken from ""Real Analysis"" by Folland (question 11.15). If $A \subset \mathbb{R}^n$ has Hausdorff dimension of $p$ , then $A \times A \subset \mathbb{R}^{2n}$ has Hausdorff dimension $\ge 2p$ . My attempt: We can see that for any $B \subset \mathbb{R}^n$ , $\operatorname{diam}(B \times B) \le \sqrt{2\operatorname{diam}(B)}$ , and if we assume that $H_p(A)< \infty$ , then for any $\delta > 0$ ,
We can find $\{B_i\}_{i=1}^{\infty} \subset R^n, \operatorname{diam}(B_i) < \delta , A \subset \bigcup_{i=1}^{\infty}{B_i}, s.t$ $$\sum_{i=1}^{\infty}{\operatorname{diam}(B_i)^p} \le H_p(A) < \infty$$ So by choosing $\{ B_i \times B_i \}_{i=1}^{\infty}$ we will get that $\operatorname{diam}(B_i \times B_i) \le \sqrt{2\delta}, A \times A \subset \bigcup_{i=1}^{\infty}{B_i \times B_i}$ $$ \sum_{i=1}^{\infty}{\operatorname{diam}(B_i \times B_i)^{2p}} \le \sum_{i=1}^{\infty}{\sqrt{2\operatorname{diam}(B_i)}^{2p}} = 2\sum_{i=1}^{\infty}{\operatorname{diam}(B_i)^p} \le 2H_p(A) < \infty
$$ I believe that from this we can conclude that the dimension of $A \times A \ge 2p$ but I'm not sure why, and there is still the case where $H_p(A) = \infty$ . Thanks.","['measure-theory', 'real-analysis']"
4627535,Proof that $(x+y)^n = x^n + y^n$ iff. $x = 0 \lor y = 0 \lor x = -y$ for $n$ odd and $\geq 3$,"I was going through Calculus by Spivak when in the first chapter I encountered problem 16, which in the end recites you should know make a good guess as to when $\left(x+y\right)^n = x^n + y^n$ quoting the results of previous exercises with a fixed $n$ , which had
this result iff $x = 0 \lor y = 0 \lor x = -y$ I started going on with the proof and immediately thought of using induction, but I was later discouraged about it when, seeing how I ""proved"" the cases with fixed $n$ , I wasn't using an ""induction-like"" procedure, and I found myself lost (I saw Spivak proof and he uses Rolle's theorem on the function $f\left(x\right) = x^n + y^n - \left(x+y\right)^n$ , but it isn't a proof that I find satisfying, as I think there's a proof that doesn't use such methods. Here is a look into the procedure I used for $n=5$ : $$\left(x+y\right)^n = x^5 + 5 x^4 y + 10 x^3 y^2 + 10 x^2 y^3 + 5 x y^4 + y^5 = x^5 + y^5$$ $$5\left(x^4y + 2x^3y^2 + 2x^2y^3 + xy^4\right) = 0$$ $$xy\left(x^3 + 2x^2y + 2xy^2 + y^3\right) = 0$$ So either $x = 0 \lor y = 0$ gives us a solution, so now we consider the part inside parenthesis $$x^3 + 2x^2y + 2xy^2 + y^3= x^3 + 3x^2y + 3xy^2 + y^3 - x^2y - xy^2 = 0$$ $$\left(x+y\right)^3 - xy\left(x + y\right) = 0$$ So either $$xy = \left(x+y\right)^2 \iff 0 = x^2 + xy + y^2 \iff x = -y $$ or $$\left(x+y\right)^3 = 0 \land \left(x+y\right) = 0 \iff x = -y$$ For a generic $n$ , I went with: $$\left(x+y\right)^n = \sum_{k = 0}^n \binom{n}{k}x^{n-k}y^k = x^n + y^n$$ Unpacking the first and last term of the sum that gives us $$\sum_{k=1}^{n-1} \binom{n}{k}x^{n-k}y^k = 0$$ $$xy\left(\sum_{k=1}^{n-1} \binom{n}{k}x^{n-1-k}y^{k-1}\right) = 0$$ Which proves the condition $x = 0 \lor y = 0 \implies \left(x+y\right)^n = x^n + y^n$ Now I tried to find a way to transform the inner sum into the form $$\left(x+y\right)^{n-2} - z\left(x+y\right)$$ Where $z$ is any possible polynomial that uses $x$ or $y$ or both
This way I should resolve $$\left(x+y\right)^{n-2} = \left(x+y\right) = 0$$ which yields $x = -y$ as the only result excluding $x = y = 0$ --- Update --- Following Mark Bennet advise, I went on to prove that, with $|x| > |y|, z = y/x$ (I also tried with $z = x/y$ ), the sum $$\sum_{i=1}^{n-1}\binom{n}{i}z^i = 0 \iff z = -1 \lor z = 0$$ which for $z < 0$ has the same number of positive and negative terms. Due to the properties of the binomial, we also know that the $i$ -th and the $\left(n-i\right)$ -th term of the sum have the same coefficient $\binom{n}{i}$ . So we can write it as $$\sum_{i=1}^{\left(n-1\right)/2}\binom{n}{i} \cdot z^i\left(1+z^{n-2i}\right)$$ My idea was to prove that such sum is never equal to $0$ except when $z = 0 \lor z = -1$ . Do to prove such a thing, I had 3 main ideas: -prove that every element of the sum differs by values whose sum $\neq 0$ -prove that the sum has only 2 real solutions, so that they can only be $z = 0 \lor z = 1$ -prove that the sum is a parabola: this implies that it only has 2 real solutions But again, I cannot find a way to prove such things --- Update 2 --- Thinking about the problem, I tried to pose it as $$\left(1+z\right)^n < 1 + z^n$$ with $z = y/x \land -1 < z < 0$ (We already proved for $z$ > 0 that the claim is false)
So, listing down a couple of things: $$1 + z < 1 \land z^n < z \implies \left(1+z\right)^n < 1 +z < 1 + z^n \forall n$$ This means that $$\forall z \in \left(-1, 0\right), \left(1+z\right)^n\neq 1 + z^n$$ As this is the only left range, it means that $z$ must either be $-1 \lor 0$ , which implies $y = 0 \lor x = -y$ , which proves our claim","['exponentiation', 'alternative-proof', 'binomial-coefficients', 'discrete-mathematics', 'induction']"
4627567,How to calculate the Cramer-Rao lower bound,"I am having trouble calculating the Cramer-Rao lower bound for an unbiased estimator $\theta$ for the following function: $$f(x|\theta) = \theta^{2} x e^{-\theta x} , x>0$$ I have already calculated the maximum likelihood estimation which is $$\widehat{\theta} = \frac{2}{\overline{X}}.$$ I have been using the following equation to calculate the Fisher Information: $$I_{n}(\theta)=E[(\dfrac{\partial }{\partial \theta}\ln f(X|\theta))^{2}]$$ However, I cannot move forward past this point, since whenever I tried to calculate the Fisher Information $I(\theta)$ . $$\ln f(X|\theta) =2\ln (\theta)+\ln(x)-(\theta x)$$ $$\dfrac{\partial}{\partial \theta} \ln f(X|\theta)= \frac{2}{\theta}-x $$ $$(\dfrac{\partial}{\partial \theta} \ln f(X|\theta))^{2}= \frac{4}{\theta^{2}}-\frac{4x}{\theta}+x^{2}$$ When I calculate the expected value of this equation it does not equal to the solution that I was given for the problem, which is $\frac{\theta^{2}}{2n}$ .","['fisher-information', 'statistics', 'parameter-estimation']"
4627621,"Find maximum value of $\sin(A) \sin(B) \sin(C)$, where $A, B,$ and $C$ are angles of a triangle [duplicate]","This question already has answers here : If $A,B$ and $C$ are the interior angles of a triangle, then what is the maximum value for $\sin(A)\cdot\sin(B)\cdot\sin(C)$? (3 answers) Closed last year . I was doing this problem that requires to prove that $P≤\frac{3\sqrt{3}R^2}{4},$ where $P$ is an area of a triangle and $R$ is the radius of its circumscribed circle. I started with the law of sines $\frac{a}{\sin(A)}=\frac{b}{\sin(B)}=\frac{c}{\sin(C)}=$ 2R, and $P=\frac{abc}{4R},$ so I got $4P=8R^2\sin(A)\sin(B)\sin(C).$ so the problem comes down to proving that $\sin(A)\sin(B)\sin(C)≤\frac{3\sqrt{3}}{8}.$","['trigonometry', 'geometry']"
4627679,Compute the matrix representation of a linear map with respect to a basis,"I'm trying to solve this exercise but I'm stuck: Let $V$ be a vector space over $K$ and let $\mathcal{B} = \{v_1, v_2, v_3, v_4\}$ be a basis of $V$ . Let $f: V \rightarrow V$ be a linear map such that: $$f(v_1) = v_2 - v_3, \quad f(v_2) = 3v_1 - 2v_4, \quad f(v_3) = v_3, \quad f(v_4) = v_1 + v_4.$$ Calculate the matrix $A$ which represents $f$ in the basis $\mathcal{B}$ . (Extra: is $A$ nilpotent?) I tried multiple things. Intuitively, I thought the answer would look something like this: \begin{bmatrix}\mid&\mid&\mid&\mid\\\ v_2-v_3&3v_1-2v_4&v_3&v_1+v_4\\\ \mid&\mid&\mid&\mid\end{bmatrix} Which of course works for our usual basis vectors $e_1, e_2, e_3, e_4$ , but unfortunately not for other basis vectors. What I tried next was to calculate the matrix by hand in the case of a $2 \times 2$ matrix and then a specific example with some made-up numbers for a $3 \times 3$ matrix but I wasn't able to see any easy patterns. Surely the intention here is not to solve multiple, big systems of equations to figure out all the entries in the matrix one by one, right? What am I missing here? Oh and for the extra, my guess is that $A$ is not nilpotent, since no matter how many times we apply $A$ , we will always get $v_3$ for $v_3$ and never the zero vector. Is this correct? Thanks to a comment I came to this solution: $A = \begin{bmatrix}
0 & 3 & 0 & 1 \\
1 & 0 & 0 & 0 \\
-1 & 0 & 1 & 0 \\
0 & -2 & 0 & 1
\end{bmatrix}$ , and I see that this works with the basis $e_1, e_2, e_3, e_4$ . But if I try it with other vectors, obviously it doesn't work. For example, let $v_1 = \begin{pmatrix} 1\\\ 1\\\ 1\\\ 1\end{pmatrix}, \quad v_2 = \begin{pmatrix} 1\\\ 1\\\ 1\\\ 0\end{pmatrix}, \quad v_3 = \begin{pmatrix} 1\\\ 1\\\ 0\\\ 0\end{pmatrix}, \quad v_4 = \begin{pmatrix} 1\\\ 0\\\ 0\\\ 0\end{pmatrix}$ But then $Av_1 = \begin{pmatrix} 4\\\ 1\\\ 0\\\ -1\end{pmatrix} \neq v_2 - v_3$ What am I not understanding here? Shouldn't the solution have to be in terms of the given vectors? Doesn't the solution change if you have different specific vectors?","['matrices', 'linear-algebra']"
4627693,"Estimator for $\theta$ on $U[0,\theta]$","Consider $\theta^\ast = X_{(1)}+X_{(n)}$ . I have the following questions: Is $\theta^\ast$ unbiased? I would say so. I need to show that $\mathbb E[\theta^\ast] = \mathbb E[X_{(1)}+X_{(n)}] = \theta$ . The expectation of $\mathbb E[X_{(n)}]$ is \begin{align*}
F_{X_{n}}(x) &= \mathbb P(X_{(n)}\leqslant x)\\
&= \mathbb P(X_{(1)}\leqslant x,\dots, X_{(n)}\leqslant x)\\
&= \left(\frac x\theta\right)^n\\
f_{X_{(n)}}(x) &= F'_{X_{(n)}}(x)\\
&=n\theta^{n-1}x^{n-1}\\
\mathbb E[X_{(n)}] &= \int\limits_0^\theta xf_{X_{(n)}}(x)\\
&=\frac {n}{n+1}\theta.
\end{align*} The expectation of $\mathbb E[X_{(1)}]$ is \begin{align*}
F_{X_{(1)}}(x) &= 1-[1-F_{X}]^n\\
&=1-[1-\frac x\theta ]^n\\
f_{X_{(1)}}(x)&= F'_{X_{(1)}}(x)\\
&=n\theta^{-1} \left(1-\frac x \theta\right)^{n-1}\\
\mathbb E[X_{(1)}] &= \int\limits_{0}^\theta xf_{X_{(1)}}(x)\\
&=\frac 1{n+1}\theta.
\end{align*} And therefore we have $$\mathbb E[X_{(1)}+X_{(n)}] = \frac{n}{n+1}\theta + \frac1 {n+1}\theta = \theta.$$ Is $\theta^\ast$ unbiased consistent? I need to show that $\mathbb P(|\theta-\theta^\ast|\geqslant \varepsilon)\to0$ for all $\varepsilon >0, n\to\infty$ . I am not sure how to do this or even if it is true. I believe I have to use some central limit theorem .","['statistics', 'parameter-estimation', 'probability-theory']"
4627695,"What is the value of $\lim\limits_{n\to\infty}2^{n}x_{n}$ if $x_1\in(0,1)$ and $x_{n+1}=\frac{\sqrt{1+x_n}-\sqrt{1-x_n}}2$ for every $n\ge1$?","We have the sequence $(x_{n})_{n\geq1}$ with $x_{1}\in(0,1)$ and $x_{n+1}=\dfrac{\sqrt{1+x_{n}}-\sqrt{1-x_{n}}}{2}\;$ for every $n\geq1$ . What is the value of $$\lim_{n \rightarrow \infty}2^{n}x_{n}=\;?$$ We can easily find that $\lim\limits_{n \rightarrow\infty}x_{n}=0$ and $\lim\limits_{n \rightarrow\infty}\dfrac{x_{n+1}}{x_{n}}=\dfrac{1}{2}$ . I tried using Stolz-Cèsaro once, twice but it does not work. I tried using the ratio test but again nothing. I tried taking the natural logarithm of the $z_{n}=2^{n}x_{n}$ and try calculating $\lim\limits_{n \rightarrow\infty}\big(n\ln 2+\ln x_{n}\big)$ but nothing.","['limits', 'sequences-and-series']"
4627726,Combinatorial identity: $2^N=\sum_{m=0}^N\sum_{r=0}^n\sum_{s=0}^m(-1)^{n+m}(-2)^{r+s}\binom nr\binom ms\binom{N-r}m\binom{N-s}n$ for all $0\le n\le N$,"Given an integer $N>1$ , is it true that $$2^N=\sum_{m=0}^N\sum_{r=0}^n\sum_{s=0}^m(-1)^{n+m}(-2)^{r+s}\binom nr\binom ms\binom{N-r}m\binom{N-s}n$$ for all $0\le n\le N$ ? This identity means that elements on the main diagonal of $T^2$ are all equal to $2^n$ , where $T$ is an $(n+1)\times(n+1)$ matrix with elements $$T_{ij}=(-1)^i\sum_{k=0}^i\binom ik\binom{n-k}j(-2)^k.$$ It is the last step to resolving the problem Matrix performing local differintegral analysis being its own inverse. Coincidence? in an arbitrary dimension, and so far I have not been able to proceed. I wonder if there is a combinatorial interpretation of the summand, since it cycles through $r,s$ in a symmetrical manner.","['matrices', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4627804,"$F$ is Lipschitz $\Longrightarrow$ $F(x)=F(a)+\int_{a}^{x}f(t)\,dt$","According to the Fundamental Theorem of Lebesgue Integral Calculus, the following statement holds: Let $F:[a,b]\to\mathbb{R}$ . Then $F$ is absolutely continuous iff there exists a function $f:[a,b]\to\mathbb{R}$ which is Lebesgue integrable such that: $F(x)=F(a)+\int_{a}^{x}f(t)\,dt$ I am wondering whether the following statement of similar nature holds as well: Let $F:[a,b]\to\mathbb{R}$ . Then $F$ is Lipschitz continuous iff there exists a function $f:[a,b]\to\mathbb{R}$ which is Riemann integrable such that: $F(x)=F(a)+\int_{a}^{x}f(t)\,dt$ Note that the implication "" $\Leftarrow$ "" is already well known, and is usually mentioned as a lemma in preparation for the Fundamental Theorem of Calculus. Is it indeed true that for every Lipschitz continuous function $F:[a,b]\to\mathbb{R}$ there exists a function $f:[a,b]\to\mathbb{R}$ which is Riemann integrable such that: $F(x)=F(a)+\int_{a}^{x}f(t)\,dt$ ? My Attempt: It is already well known that a Lipschitz continuous function is differentiable almost everywhere (and the derivative is clearly bounded). Thus if we only show that this derivative is Riemann integrable, the problem can be solved at once with an existing geralized version of the Newton-Leibniz Theorem. So far, I was not successful in showing the Riemann integrability of that derivative...","['integration', 'lebesgue-integral', 'lipschitz-functions', 'calculus', 'derivatives']"
4627844,The projective plane as a smooth surface in a 4-dimensional space,"For $(a, b, c)$ pairwise distinct: $$
\begin{align}
f: & S^2       & \mapsto & \quad \mathbb{R}^4 \\
   & (x, y, z) & \mapsto & \quad (X, Y, Z, W) = (y \cdot z, x \cdot z, x \cdot y, a \cdot x^2 + b \cdot y^2 + c \cdot z^2)
\end{align}
$$ is a smooth double covering map of the real unit sphere $S^2$ to a
surface in the real 4-dimensional space $\mathbb{R}^4$ , and opposite
points have the same image, such that it can also be interpreted as an
injective embedding of the real projective plane. In particular, the
image of $f()$ is a surface, that is, each point has a non-degenerate
tangential plane. The image of $f()$ can be defined in the $(X, Y, Z, W)$ coordinates as follows. If $X \cdot Y \cdot Z \neq 0$ : $$
    (Y \cdot Z)^2 + (X \cdot Z)^2 + (X \cdot Y)^2 = X \cdot Y \cdot Z \\
    a \cdot (Y \cdot Z)^2 + b \cdot (X \cdot Z)^2 + c \cdot (X \cdot Y)^2 = X \cdot Y \cdot Z \cdot W
$$ If $X = Y = 0$ and $Z \neq 0$ : $$
    (a - b)^2 \cdot (1 - 4 \cdot Z^2) = (2 \cdot W - (a + b))^2
$$ If $X = Z = 0$ and $Y \neq 0$ : $$
    (a - c)^2 \cdot (1 - 4 \cdot Z^2) = (2 \cdot W - (a + c))^2
$$ If $Y = Z = 0$ and $X \neq 0$ : $$ 
    (b - c)^2 \cdot (1 - 4 \cdot Z^2) = (2 \cdot W - (b + c))^2
$$ If $X = Y = Z = 0$ : $$ 
    (W - a) \cdot (W - b) \cdot (W - c) = 0
$$ Question 1 : Is the image of $f()$ an algebraic set in the $(X, Y, Z, W)$ coordinates ? Can
it be defined by a single ideal, without case analysis ? If yes, what is the ideal in question, explicitly ? If no, how to prove it ? Furthermore, the image of f() consists then of five
algebraic pieces fitting together smoothly, without being, as a whole,
algebraic; I find it surprising, what happens where the pieces meet ? Question 2 : If $X \cdot Y \cdot Z \neq 0$ , or, equivalently, $x \cdot y \cdot z \neq 0$ ,
then $$
    [x : y : z] = [Y \cdot Z : X \cdot Z : X \cdot Y]
$$ Is there a rational formula for each of the other pieces ? If yes, what are the respective formulæ, explicitly ? If no, how to prove it ?","['birational-geometry', 'algebraic-geometry', 'projective-space', 'differential-geometry']"
4627918,Alternating partial sums of binomial coefficients,"I am interested in a generalization of the partial alternating sum $$\sum_{k=0}^{m} (-1)^k{n\choose k}=(-1)^m{n-1\choose m},\quad m<n$$ Including a factor $k^{\ell}$ for $0\leq\ell\leq n-1$ $$\sum_{k=0}^{m} k^{\ell}(-1)^k{n\choose k}$$ Mathematica can evaluate these sums, for instance the first couple are $$\sum_{k=0}^{m} k(-1)^k{n\choose k}=(-1)^m\frac{nm}{n-1}{n-1\choose m}$$ $$\sum_{k=0}^{m} k^2(-1)^k{n\choose k}=(-1)^m\frac{nm(m(n-1)-1)}{(n-1)(n-2)}{n-1\choose m}$$ I am struggling to find a formula for general $\ell$ , any help would be appreciated!","['summation', 'binomial-coefficients', 'combinatorics']"
4628066,Relationship between measure theory and quantification,"In a 1978 paper published by David P. Ellerman and Gian-Carlo Rota, the duo discuss the relationship(s) exhibited by measure theory, probability theory, and logic paying special attention to how these concepts apply to describing the nature of quantification via an algebraic perspective. I was hoping that someone who is familiar with the paper (or for someone whose research area would permit them an understanding) to elucidate its underlying concepts. In particular, in paragraph two on page 3 (of 21) in the attached pdf , the authors state ""It has long been known (e.g., Wright [12]) that there is an analogy between averaging operators such as the conditional expectation operators of probability theory and the algebraic (existential) quantifiers in Halmos"" theory of polyadic algebras (Halmos [5]) or the cylindrifications used by Tarski and his co-workers in the theory of cylindric algebras (Henkin, Monk, and Tarski [7]). All these operators satisfy an averaging condition which has the general form: $A(f) \cdot A(g) = A \cdot (f \cdot A(g))$ . We will provide some theoretical underpinning for this analogy by constructing the logical quantifiers using an abstract rendition of conditional expectation operators on a ring of simple random variables."" Can someone please elaborate upon how the logical quantifiers are related to conditional expectation operators as the term is used here. Additionally, can anyone point me towards some good resources for pursuing this line of inquiry further (lectures, videos, books, etc.)?","['measure-theory', 'logic', 'algebraic-logic', 'abstract-algebra', 'probability']"
4628074,Semifinite measure induced by a measure,"Define $\mu_0: \mathcal{M} \longrightarrow [0,\infty]$ by $$
\mu_0(E) = \sup\{ \mu(F) \mid F \subseteq E, \mu(F) < \infty \},
$$ prove that $\mu_0$ is a measure on $\mathcal{M}$ . My attempt: Obviously $\mu_0(\emptyset)=0$ . Then let $E_i \in \mathcal{M}, \, i \in \mathbb{N}^+$ be pairwisely
disjoint. $$
\begin{aligned}
\mu_0(\bigsqcup_{i=1}^{\infty}E_i) =&  
\sup \{ \mu(F) \mid F \subseteq \bigsqcup_{i=1}^{\infty}E_i,
\mu(F) < \infty \} \\
%%%%%%%%%%%%%%
%%%%%%%%%%%
=& \sup \{ \mu(\bigsqcup_{i=1}^{\infty}(F \cap E_i)) \mid F \subseteq \bigsqcup_{i=1}^{\infty}E_i,
\mu(F) < \infty \} \\
%%%%%%%%%%%%%%
%%%%%%%%%%%%
=& \sup \{ \sum_{i=1}^{\infty}\mu(F \cap E_i) \mid F \subseteq \bigsqcup_{i=1}^{\infty}E_i,
\mu(F) < \infty \} \\
%%%%%%%%%%%%
%%%%%%%%%%%%%%%
\leq& \sup \{ \sum_{i=1}^{\infty}\mu(F_i) \mid F_i \subseteq E_i,
\mu(F_i) < \infty, \forall i \in \mathbb{N}^+ \}  \\
%%%%%%%%%%%%
%%%%%%%%%%%%%
=& \sum_{i=1}^{\infty} \sup \{\mu(F_i) \mid F_i \subseteq E_i,
\mu(F_i) < \infty \}  \\
%%%%%%%%%%
%%%%%%%%%%%
=& \sum_{i=1}^{\infty}  \mu_0(E_i)  \; .
\end{aligned}
$$ On the other hand, $$
\begin{aligned}
\sum_{i=1}^{\infty}  \mu_0(E_i) =&  
\sum_{i=1}^{\infty} \sup \{\mu(F_i) \mid F_i \subseteq E_i,
\mu(F_i) < \infty \}  \\
%%%%%%%%%%%%%%
%%%%%%%%%%%
=& \sup \{ \sum_{i=1}^{\infty}\mu(F_i) \mid F_i \subseteq E_i,
\mu(F_i) < \infty, \forall i \in \mathbb{N}^+ \} \\
%%%%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%%%%%
=& \sup \{ \mu(\bigsqcup_{i=1}^{\infty}F_i) \mid F_i \subseteq E_i,
\mu(F_i) < \infty, \forall i \in \mathbb{N}^+ \} \;.
\end{aligned}
$$ But we can't get $$
\begin{aligned}
&  \sup \{ \mu(\bigsqcup_{i=1}^{\infty}F_i) \mid F_i \subseteq E_i,
    \mu(F_i) < \infty, \forall i \in \mathbb{N}^+ \}   
 \\
%%%%%%%%%%%%%%
%%%%%%%%%%%
\leq& \sup \{ \mu(F) \mid F \subseteq \bigsqcup_{i=1}^{\infty}E_i,
\mu(F) < \infty \} \;, 
\end{aligned}
$$ because $\mu(\bigsqcup_{i=1}^{\infty}F_i)$ might be $\infty$ .",['measure-theory']
4628144,"If a function $f$ is analytic and $f(x,y)=f(x,-y)=f(x+y,y)$ then the function depends on $y$ alone","Let $f:\mathbb{R^2}\rightarrow\mathbb{R}$ a function such that $f(x,y)=f(x,-y)=f(x+y,y)$ . I need to show that if $f$ is an analytic function, then $f$ depends only on $y$ What I did was this: Given that $f$ is analytic, the Taylor series at the point $(x+h, y+k)$ is equal to $f(x+h,y+k)$ , so: $f(x+h,y+k)=f(x,y)+hf_x(x,y)+kf_y(x,y)+\frac12(h^2f_{xx}+2hkf_{xy}+k^2f_{yy})+...$ for $k=0$ and $h=y$ we'll get: $f(x+y,y)=f(x,y)+yf_x(x,y)+\frac12(y^2f_{xx}(x+y,y)+...$ I'll define $g:\mathbb{R^2}\rightarrow\mathbb{R}$ by $g(x,y)=f(x+y,y)-f(x,y)$ . Based on the initial data $g(x,y)\equiv0$ , and so: $0\equiv f(x+y,y)-f(x,y)=yf_x(x,y)+\frac12y^2f_{xx}(x+y,y)+...$ It follows then that $f_x \equiv f_{xx} \equiv f_{xxx} \equiv ... \equiv0$ What I want now is to show that every mixed derivative, i.e $f_{xy},f_{xyy}$ , etc is also equivalent to $0$ , but I don't how to do this, or really why this would be true. Edit: Because $f_{xy}=f_{yx}$ , all the mixed derivatives would be taking the derivative of a constant function (where the constant is just $0$ ), And so they'll all be equivalent to the $0$ function. And then I'll get that $f(x+h,y+k)=f(x,y)+kf_y+\frac12k^2f_{yy}+...$ is an expression that doesn't depend on $h$ and then $f$ is supposedly a function that only depends on $y$ . But I don't really understand why this means that $f$ is only dependent on $y$ . I can still have $x$ 's in the partial derivatives of $f$ according to $y$ , no? Presumably because $f(x,y)=f(x,-y)$ it'll then mean that $f$ only depends on $|y|$","['multivariable-calculus', 'taylor-expansion', 'analytic-functions']"
4628157,"Suppose $(x_n)_n$ is a non-decreasing sequence of natural numbers. Then, $\sum\frac{1}{x_{x_n}-x_n}$ diverges $\iff\sum\frac{1}{x_n}$ diverges.","Is the following proposition true, and if so, how can it be proven? I cannot find a counter-example. Proposition: Suppose $\ (x_n)_n\ $ is a non-decreasing sequence of natural numbers, and $\ x_{x_n} \neq x_n\ \forall
 n\in\mathbb{N}.\ $ Then, $\ \displaystyle\sum_n \frac{1}{x_{x_n} -
 x_n}\ $ diverges $\ \iff \displaystyle\sum_n \frac{1}{x_n}\ $ diverges. Note: if we did not require $\ (x_n)_n\ $ to be non-decreasing, then $\ 2, 10, 4, 100, 6, 1000, 8, 10000, 12, 100000,\ldots\ $ would be a counterexample.","['recreational-mathematics', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'problem-solving']"
4628158,Please explain how to solve limit. I know the answer but how to explain it?,"Problem: $a@b = \frac{a+b}{ab+1}$ . Solve limit: $\lim_{n \to \infty}(2@3@...@n)$ . I've tried to solve this problem by just calculating: $$2 @ 3 = 0.714$$ $$2 @ 3 @ 4 = 1.222$$ $$2 @ 3 @ 4 @ 5 = 0.875$$ $$2 @ 3 @ 4 @ 5 @ 6 = 1.1$$ I found the pattern. The first number is less than 1, then the next is greater than 1, the next is less than 1, and so on. So the limit must be 1. But how to explain it mathematically? I've tried to transform this: $$(n-1)@n = \frac{2n-1}{n^2-n+1}$$ $$n@(n+1) = \frac{2n + 1}{n^2+n+1}$$ But it didn't help me to understand the method how to solve it. I think there should be a simple idea, which I don't see. I appreciate all hints.",['limits']
4628173,Signs of entries of Kravchuk matrices asymptotically produce a large circular region with hyperbolic sinks. Why?,"Crossposted on MathOverflow to investigate the sinks (Observation 2). Edit 2: I have now found the following paper Wolf and Krötzsch (2007): Geometry and dynamics in the fractional discrete Fourier transform where Figure 7 apparently shows the circle pattern. However, I am unsure whether the author explains why a circle is formed, since the mathematics/physics involved is currently beyond my understanding. If there is indeed a proof or exposition of this behaviour, I will accept any answer that clearly explains what is happening. Edit: I just learned that these are known as the Kravchuk matrices , which are heavily applied in quantum physics and signal processing. So it won't be a total surprise if the pattern I describe below is intrinsically linked to discrete Fourier transforms, for instance. However, I worked on the matrix for a different reason:
it was developed due to this question on an involution for estimating $n$ th order derivatives. So I'm also interested in the deeper mathematical link between high-order derivative approximation and these matrices. Consider the square matrix $T^{(n+1)}$ such that its elements follow the formula below \begin{align}T^{(n+1)}_{i,j}&=(-1)^i\sum_{k=0}^i\binom ik\binom{n-k}j(-2)^k\quad\forall0\le i,j\le n.\end{align} This can be rewritten as $T^{(n+1)}_{i,j}=[x^j](1+x)^{n-i}(1-x)^i$ where $[x^j]$ denotes the coefficient of $x^j$ . By evaluating at $x=1$ , this means the sum of each row is zero, except the first row with sum $2^n$ . We describe an asymptotic plot of the sign of $T^{(n+1)}_{i,j}$ as odd $n\to\infty$ , where $i$ is the vertical axis and $j$ horizontal. Positive entries are filled orange and negative entries are filled white. Below is when $n=999$ . For reference, these are the plots for $n=499$ and $n=99$ . The pattern was unexpected to me. We observe the following: Asymptotically, there is a large circular region centred at $i=j=(n+1)/2$ whose pattern noticeably differs from the rest of the plot. Within this circle are numerous hyperbolic sinks of varying sizes. It appears that the largest sinks lie on the line $i=j$ , and the number of sinks tends to infinity asymptotically. Outside the circle, in the top-left quadrant, all elements of $T$ are positive. Outside the circle, in the top-right and bottom-left quadrants, the sign alternates between full rows and columns. Outside the circle, in the bottom-right quadrant, the sign appears to alternate by consecutive entries. Question: How do we explain Observations 1, 2, 3, 4, 5? Note: When $n$ is even, the plot is the same, with the addition of vertical and horizontal lines for zero entries which sometimes lie on the circumference of the circle as well. So we need to resolve the odd case first before fully describing the scenario when $n$ is even.","['asymptotics', 'matrices', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4628182,Why is the derivative of a function at a point is a linear functional on the tangent space?,"My apologies if this question has been asked before but I haven't been able to find a satisfying answer. Whenever I look into the definition of tangent spaces, it's always in the context of manifolds or differential geometry which are two topics I do not know about a lot. The reason I am asking about it is because I have seen in some definition, that the derivative of a function at a point $p$ , let's say $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is actually a linear functional acting on the tangent space of $\mathbb{R}^n$ at that point. I find this definition very interesting but I am not sure I am grasping the intuition geometrically because I do not think I understand what the tangent space represents. If I try to visualize it, how would it relate to the tangent plane of a surface at a point in this?","['manifolds', 'surfaces', 'tangent-spaces', 'differential-geometry']"
4628206,Differentiate $|\sin x|$,"Could you differentiate $|\sin x|$ and give the points where the derivative doesn't exist? I have only so far tried to analyze it using the graph, from 0 to $\pi$ is $\cos x$ . But from $\pi$ to $2\pi$ $\sin x$ will be on the positive side and it'd be given by the derivative of $\cos x$ too. Anyways I am confused and need help. Is there a better way of doing this?","['calculus', 'derivatives', 'algebra-precalculus', 'absolute-value']"
4628226,"Please tell me why my method of solving $\sin\left(\frac{\pi}{3}-2\,x\right)-\sin\left(6\,x\right)=2$ is not correct.","$\sin\left(\frac{\pi}{3}-2\,x\right)-\sin\left(6\,x\right)=2$ Since $\left| \sin\left(\frac{\pi}{3}-2\,x\right)\right|\leqslant 1$ and $\left| \sin\left(6x\right)\right|\leqslant 1$ , the two sides of the equation can be equal when and only when $\sin\left(6\,x\right)=-1$ and $\sin\left(\frac{\pi}{3}-2\,x\right)=1$ : $\left\{\begin{matrix}\sin\left(\frac{\pi}{3}-2\,x\right)=1\\\sin\left(6\,x\right)=-1\end{matrix}\right.$ $\left\{\begin{matrix}\frac{\pi}{3}-2\,x=\frac{\pi}{2}+2\,\pi\,n\\6\,x=-\frac{\pi}{2}+2\,\pi\,k\end{matrix}\right.$ $\left\{\begin{matrix}x=-\frac{\pi}{12}-\pi\,n\\x=-\frac{\pi}{12}+\frac{\pi\,k}{3}\end{matrix}\right.$ And then I equate the found values: $-\frac{\pi}{12}-\pi\,n=-\frac{\pi}{12}+\frac{\pi\,k}{3}$ Getting $n=-\frac{k}{3}$ And then I just substitute $n=-\frac{k}{3}$ into $x=-\frac{\pi}{12}-\pi\,n = -\frac{\pi}{12}+\frac{\pi\,k}{3}$ . And this solution is wrong . I am very curious why, because I solved $\sin\left(3\,x\right)\,\cos\left(4\,x\right)=1$ using this method and the solution was correct. But it doesn't work for this equation.","['trigonometry', 'systems-of-equations']"
4628263,Double covariant derivative in coordinates: Why does this work?,"Lets take a vector field $X$ on some Riemannian manifold $(\mathcal{M},g)$ with Levi-Civita connection $\nabla$ . Then, the components of its covariant derivative in coordinates are $$\nabla_{\alpha}X^{\beta}=\partial_{\alpha}X^{\beta}+\Gamma_{\alpha\gamma}^{\beta}X^{\gamma}$$ Now, since $\nabla X$ is again tensor field, we can apply a second covariant derivative, which in coordinates yields $$\nabla_{\gamma}\nabla_{\alpha}X^{\beta}:=\nabla_{\gamma}(\nabla_{\alpha}X^{\beta})=\partial_{\gamma}(\nabla_{\alpha}X^{\beta})-\Gamma_{\gamma\alpha}^{\delta}\nabla_{\delta}X^{\beta}+\Gamma_{\gamma\delta}^{\beta}\nabla_{\alpha}X^{\delta}=...$$ Now, instead of writing it like this, let us formally change the order in which the covariant detivative act, i.e. let us write $$\nabla_{\gamma}\nabla_{\alpha}X^{\beta}=\nabla_{\gamma}(\nabla_{\alpha}X^{\beta})=\nabla_{\gamma}(\partial_{\alpha}X^{\beta}+\Gamma_{\alpha\gamma}^{\beta}X^{\gamma})=\nabla_{\gamma}(\partial_{\alpha}X^{\beta})+\nabla_{\gamma}(\Gamma_{\alpha\delta}^{\beta}X^{\delta})=...$$ Now, mathematically speaking, the two terms on the right-hand side are ill-defined and do not make sense, since $\nabla$ is an operation acting on tensor fields and neither $\partial_{\alpha}X^{\beta}$ nor $\Gamma_{\alpha\delta}^{\beta}X^{\delta}$ are the components of a tensor field. However, if we treat these two terms as if they were rank (1,1) tensor fields, i.e. elements of $\Gamma^{\infty}(T\mathcal{M}\otimes T^{\ast}\mathcal{M})$ , and use the standard formula for the connections, we will find the same and correct result for the components of $\nabla^{2}X$ . Now, this seem to work in general, i.e. for arbitrary rank tensors and an arbitrary amount of covariant derivatives (at least I never have seen a counter example). My question, or lets say, my curiosity, is: Why does the second ""approach"", in which we produce ill-defined terms in the steps in-between, work? Is there any mathematical reason? For example, maybe one can extend the covariant derivative to more general ""objects with indices"" in a unique way, such that the steps in between become well-defined. For example, one can extend the covariant derivative to a map acting on tensor densities (i.e. sections of tensor product of a tensor bundle and a density bundle). Maybe there is a similar and more general notion which also includes objects like the partial derivative and the Christoffel symbols.","['riemannian-geometry', 'connections', 'tensors', 'vector-bundles', 'differential-geometry']"
4628289,Derivative of a pointwise limit of a sequence of functions,"It is easy to construct a sequence of differentiable functions $f_n(x)$ converging pointwise to a function that is not differentiable (a simple example is $\tan^{-1}(nx)$ ). And it is a theorem that if in addition the derivatives $f_n'(x)$ converge uniformly (on an interval $[a,b]$ for example), then  the limit function is differentiable with derivative the limit of the derivatives. I am looking for an example (if any exist)
of a sequence
of differentiable functions $f_n(x)$ that converges pointwise to
a differentiable function $f(x)$ , and such that the sequence of derivatives $f_n'(x)$ also converges pointwise to a differentiable function $g(x)$ , but $f'\neq g$ .
If such an example does not exist, I would like to see the proof of the corresponding theorem.","['sequence-of-function', 'derivatives', 'pointwise-convergence', 'uniform-convergence']"
4628338,Satisfiability of Divisible Group Axioms,"I'm struggling with this problem: Let $\mathcal{G}=(G,+,-,0)$ be an abelian group. Work in the language $L=\{+,-,c_0\}$ and expand it with a constant symbol $c_g$ , for each $g\in G\setminus \{0\}$ . Let $\Sigma$ be the set of divisble group axioms and let: $$\Sigma' = \Sigma \cup\{c_{g_1}+c_{g_2}=c_h: g_1,g_2,h\in G \text{ and } g_1+g_2=h\} \cup \{c_g\neq c_h: g,h\in G \text{ and } g\neq h\}$$ (a) Prove that $\Sigma'$ is satisfiable; (b) Conclude that, up to isomorphism, every abelian group $\mathcal{G}$ embeds into some divisible abelian group $\mathcal{H}$ . (c) Show that, if $\mathcal{G}$ is infinite, there exists $\mathcal{H}$ as in (b) such that $|G|=|H|$ . Now I have problems with point (a), I can't manage to prove the satisfiability of $\Sigma'$ . I posted also points (b), (c) because I get the idea but I'm struggling to formalize it. Any help is very much appreciated :)","['group-theory', 'logic', 'first-order-logic', 'model-theory']"
4628378,In which point is the function not differentiable?,"$$s(x)=\begin{cases}
x^2+1 \quad\quad x<0 \\
x+2 \,\,\quad\quad0\leq x<2\\
x^2 \,\,\,\quad\quad\quad x \ge2
\end{cases} $$ In which points is the function differentiable? It is my understanding that a function is differentiable if the point
is continuous, so I am asking if I need to check if s(x) is continuous
when x=0 and x=2? But I know that the function is continuous for all x's otherwise
except these two points. To check if x=0 is continuous I need to make
sure this applies: $\lim_{x \to 0} x^2 + 1 = s(0)$ But I don't know how to simplify $x^2 + 1$ more, do I just put x=0 and
check? In that case I get $1 \neq s(0)=2$ ? I did same thing for x=2 and found out that $\lim_{x \to 2} x + 2 = 2+2= 4 = s(0)= 2^2=4$ ? My answer was then that x was differentiable for all $x \neq 0$ . But
the answer is that x is differentiable for $x \neq 0$ and $x \neq 2$","['calculus', 'functions', 'education', 'algebra-precalculus', 'derivatives']"
4628379,"Finding an injective function between an infinite set $A$ and the complement of $B$ in A, where B is a finite subset of A.","Basically if $C=A-B$ , I want to find a one-to-one correspondence $f:A\rightarrow C$ where A is an infinite set and $B\subset A$ is finite. I thought of something like $$f=
\begin{cases}
i(a), \; &a\in C,\\
??,\; &a\notin C
\end{cases}
$$ where $i$ is the identity $i:A\rightarrow A$ . But I am not sure what I should replace the questionmarks with, or if it is even possible to construct like this. Could it be possible to replace the with a so called ""empty function""? Help appreciated!","['elementary-set-theory', 'functions']"
4628384,A (maybe) trivial question on trivial vector bundles: alternative definition of trivial vector bundle,"I am studying Loring W. Tu's Differential geometry, Connections, Curvature and Characteristic Classes and I am having a doubt (the same doubt I had when studying the same topic in the author's An Introduction to Manifolds ). The following definition of a vector budle is given Definition 7.1. A $C^{\infty}$ surjection $\pi : E \to M$ is a $C^{\infty}$ vector bundle of rank $r$ if For every $p \in M$ , the set $E_p:=\pi^{−1}(p)$ is a real vector space of dimension $r$ ; every point $p \in M$ has an open neighborhood $U$ such that there is a fiber-preserving diffeomorphism $\phi_U:\pi^{−1}(U) \to U\times \mathbb{R}^r$ that restricts to a linear isomorphism $E_p \to {p}\times \mathbb{R}^r$ on each fiber. The following definition for bundle map is then given: Definition 7.5. Let $\pi_{E}: E \to M$ and $π_F: F \to N$ be $C^{\infty}$ vector bundles. A $C^{\infty}$ bundle map from $E$ to $F$ is a pair of $C^{\infty}$ maps $(\phi: E \to F,  \underline{\phi}: M \to N)$ such that the diagram $$
\newcommand{\ra}[1]{\!\!\!\!\!\xrightarrow{\quad#1\quad}\!\!\!\!\!}
\newcommand{\da}[1]{\left\downarrow{\scriptstyle#1}\vphantom{\displaystyle\int_0^1}\right.}
%
\begin{array}{lllllll}
E & \ra{\phi} & F \\
\da{\pi_E} & & \da{\pi_F} \\
M & \ra{\underline{\phi}} & N  \\
\end{array}
$$ commutes. ${\phi}$ restricts to a linear map $\phi_p: E_{p} \to F_{\underline{\phi(p)}}$ of fibers for each $p \in M$ . A bundle map over $M$ is when $\underline{\phi}$ is the identity map.
The author then says that If there is a bundle map $\psi: F \to E$ over $M$ such that $\psi \circ \phi = \mathbb{1}_E$ and $\phi \circ \psi= \mathbb{1}_F$ , then $\phi$ is called a bundle isomorphism over $M$ , and the vector bundles $E$ and $F$ are said to be isomorphic over $M$ . Finally, here is the definition of trivial bundle: Definition 7.6. A vector bundle $\phi: E \to M$ is said to be trivial if it is isomorphic to a product bundle $M \times \mathbb{R}^r \to M$ over $M$ . Here are my questions: Can we say, equivalently, that a trivial bundle is a bundle in the sense of Definition 7.1 where there exists a open $U = M$ , i.e. when there is a fiber-preserving diffeomorphism with the same properties $\phi_M: \pi^{-1}(M) \to M \times \mathbb{R}^r$ ? To me $\psi \circ \phi = \mathbb{1}_E$ and $\phi \circ \psi= \mathbb{1}_F$ is equivalent to say that $\phi$ is a diffeomorphism, i.e. a bijective $C^{\infty}$ map with $C^{\infty}$ inverse. Can we say that that any manifold with a single chart has trivial tangent bundle? Apologies in advance if my question is obvious or if, on the contrary, I am missing some macroscopic obstruction to my idea. I was wondering why the definition of trivial bundle has been given after the one of bundle isomorphism, while right after definition 7.1 the author defines trivializing open subset ( $U$ ), trivialization ( $\phi_U$ is a trivialization for $\phi^{-1}(U)$ ) and trivializing open cover. thanks","['differential-topology', 'tangent-bundle', 'vector-bundles', 'differential-geometry']"
4628485,"Why is $\langle a,b,c,d\mid abcda^{-1}b^{-1}c^{-1}d^{-1}\rangle\cong \langle a,b,c,d\mid aba^{-1}b^{-1}cdc^{-1}d^{-1}\rangle$?","We have two ways to present a surface of genus 2: (see Identifying the two-hole torus with an octagon ) Now, if one were to calculate the fundamental groups of the above surfaces using van Kampen's theorem, we would get that $$\langle a,b,c,d\mid abcda^{-1}b^{-1}c^{-1}d^{-1}\rangle\cong \langle a,b,c,d\mid aba^{-1}b^{-1}cdc^{-1}d^{-1}\rangle.$$ While I do understand that this clearly constitutes a proof of the highlighted fact, for a while now, I have been struggling to find a purely algebraic method of finding an isomorphism between the two presentations above. I've tried manipulating the letters of the free groups, as one usually does, to find such an isomorphism to no avail. How may I approach this differently? Any help at all will be much appreciated.","['group-presentation', 'combinatorial-group-theory', 'abstract-algebra', 'group-theory', 'algebraic-topology']"
4628532,Find the probability that a person is healthy given Covid 19?,"A test is (covid-19) $(1)95 \% \text{ reliable on infected patients.}\\(2)99 \% \text{ reliable on healthy people.}\\(3) 4\% \text{ of population has virus.}\\ \text{define event: V: patient has virus}\\\qquad  \qquad S: \text{Test indicates a positive result.} \\ \Rightarrow{\text{We know: }}P(S|V) =.95, P(S^c|V^c) =.99,P(V)=.04\\\text{This test is given to a person with positive result. What is the probability }\\\text{ that this person is infected?}\\P(V|S)=\frac{P(V\cap{S})}{P(s)}=\frac{P(V)P(S|V)}{P(V)P(S|V)+P(V^c)P(S|V^c)}\\=\frac{P(V)P(S|V)}{P(V)P(S|V)+(1-P(V)(1-P(S^c|V^c)}=\frac{.04\star.95}{.04\star.95+(1-.04)(1-.99)}=.8636$ Find the probability that a person is healthy given the test says this person has a negative result. How do I solve this part any tips?","['conditional-probability', 'statistics', 'bayes-theorem', 'probability']"
4628535,Is the space of compact linear operators on a separable Banach space separable?,"let $K(X)$ denote the space of compact linear operators on a separable Banach space $X$ to itself. It is a known result that $K(X)$ is a closed subspace of $L(X)$ , the space of linear operators on $X$ to itself, endowed with the operator norm. Is it also known whether $K(X)$ is separable?","['compact-operators', 'functional-analysis']"
4628554,Why is the constant assumed to be zero when solving second order differential equation?,"Using the reduction of order method to solve a differential equation, let $y_1(t) = t^{-1}$ and $y_2(t) = v(t) \times t^{-1}$ I got $2tv''(t) - 3v'(t) =0$ Let $w(t) = v'(t)$ and I will get $w(t) = ct^{\frac{3}{2}}$ And I can integrate $w(t)$ to find $v(t)$ , $$v(t) = \int w(t)dt = Ct^{\frac{5}{2}} + k$$ So, here's my problem, my maths teacher told me that the constant in $v(t)$ should be neglected, I need to assume $C = \frac{2}{5}$ and $k = 0$ . But I don't understand why should I do this.","['integration', 'calculus', 'ordinary-differential-equations']"
4628561,Relation between connection on vector bundle and Ehresmann connection on frame bundle,"I'll be using the conventions, notation and (numbered) results from Tu's Differential Geometry book. Let $M$ be a smooth manifold, $E\to M$ a (real) vector bundle of rank $r$ with a connection $\nabla:\Gamma(E)\to\Omega^1(M)\otimes\Gamma(E)$ . There is a way to obtain a connection on $E$ via the Consider the frame bundle $\pi:\text{Fr}(E)\to M$ of $E$ , which has an induced Ehresmann-connection $\omega\in\Omega^1(\text{Fr}(E),\text{Fr}(E)\times\mathfrak{gl}_r\mathbb R)$ , and whose corresponding associated vector bundle $\text{Fr}(E)\times_1\mathbb R^k$ with respect to the identity representation $1:\text{GL}(r,\mathbb R)\to\text{GL}(r,\mathbb R)$ is canonically isomorphic to $E$ . Let's denote by $\Omega_1^k(\text{Fr}(E),\text{Fr}(E)\times\mathbb R^r)$ the $\mathbb R^r$ -valued $k$ -forms on the frame bundle which are right-invariant and horizontal, meaning that $r_g^*\omega=\omega$ and that $\omega$ vanishes as soon as an input vector is vertical. There is a covariant derivative $$
D:\Omega_1^k(\text{Fr}(E),\text{Fr}(E)\times\mathbb R^r)\to \Omega_1^{k+1}(\text{Fr}(E),\text{Fr}(E)\times\mathbb R^r),\omega\mapsto (d\omega)^h,
$$ where $(d\omega)^h(v_1,\dots,v_{k+1})=dw(hv_1,\dots,hv_{k+1})$ , where by $hv$ we mean the horizontal component of $v$ . By Theorem 31.9 we have an identification $$
\Omega_1^k(\text{Fr}(E),\text{Fr}(E)\times\mathfrak{gl}_r\mathbb R))\cong\Omega^k(M,\text{Fr}(E)\times_1\mathbb R^r),
$$ which means that the covariant derivative $D$ can be considered as a map $$
\Omega^k(M)\otimes\Gamma(E)\to\Omega^{k+1}(M)\otimes\Gamma(E).
$$ My question is as follows: I want to show that for $k=0$ this map corresponds to $\nabla$ , but I'm not sure how to show this rigourously; due to the identifications made it is difficult for me to imagine how $D$ looks like as a map from $\Gamma(E)$ to $\Omega^1(M)\otimes\Gamma(E)$ . By Theorem 31.19 I know that for $\phi\in\Omega^0_1(\text{Fr}(E),\text{Fr}(E)\times\mathbb R^r)\cong\Gamma(\text{Fr}(E)\times\mathbb R^r)$ we have $$
D\phi=d\phi+\omega\cdot\phi,
$$ where the action $\omega\cdot\phi$ is the natural action of $\omega\in\Omega^1(\text{Fr}(E))\otimes\Gamma(\text{Fr}(E)\times\mathfrak{gl}_r\mathbb R))$ on $\phi$ . Note that the formula above looks a lot like the formula $$
\nabla(f^i\sigma_i)=df^i\otimes\sigma_i+f^i\nabla\sigma_i=df^i\otimes\sigma+f^i\omega^j_i\otimes\sigma_j,
$$ where $\omega_\sigma=(\omega^i_j)$ is the connection matrix corresponding to the local frame $(\sigma_i)\subset\Gamma(E\vert_U)$ . If we think of the local frame $(\sigma_i)$ as a section $\sigma\in\Gamma(\text{Fr}E\vert_U)$ , then by Theorem 29.10 we have $\omega_\sigma=\sigma^*\omega$ . So I feel like I've got most of the ingredients, but I can't make it into a complete and formal argument. Could someone help me out?","['connections', 'differential-geometry']"
4628589,$g(x)=c$ solves the ODE $y'=f(y)$ iff $c$ is a critical point of the ODE,"Consider the autonomous ODE $y'=f(y)$ where $f$ is continuously
differentiable. $g(x)=c$ (where $c$ is a constant) solves this ODE if
and only if $c$ is a critical point of the ODE. Remember to prove in
both directions. My attempt at the solution: First, I need to show that if $g(x)=c$ is a solution of the ODE then $c$ is a critical point of the ODE. So if $g(x)=c$ is a solution of $y'=f(y)$ then we have $y'=f(y)=f(c)$ by definition for all $x$ . Since $f(c)$ is a constant, then $y=c$ is also a solution of the ODE. If we do $\frac{d}{dy} f(y) = f'(y)$ at the point $y=c$ , then we have $f'(c)=0$ because the derivative of a constant function is always zero. Thus $c$ is a critical point of the ODE. Second, I need to show the converse is true. Namely, if $c$ is a critical point of the ODE, then $g(x)=c$ is a solution of the ODE. So if $c$ is a critical point, that means $f(c)=0$ . To show that $g(x)=c$ is a solution of the ODE, we need to show that $y'=f(y)=f(c)$ for all $x$ . Since $f(c)=0$ then $f(y)$ is constant at $y=c$ so when $y=c$ , $y'=f(y)=f(c)$ for all $x$ . Is this a correct proof?",['ordinary-differential-equations']
4628670,Proof of additive counting principle?,"In my ""Discrete Structures"" textbook, the Additive Counting Principle is defined as: If $\{S1, S2, . . . , Sm\}$ is a partition of a finite set S, then $|S| = |S1| + |S2| + · · · + |Sm|$ . However, it goes on to say that A formal proof would require diving into the foundations of mathematics (defining what we mean by cardinality, addition, etc) and mentions that some of this sort of stuff was covered in an Introduction to Analysis class I already took. Out of curiosity, what would this sort of proof look like?",['discrete-mathematics']
4628710,Real solution of Trigonometric and inverse Trigonometric equation,"The number of real solution of the $\displaystyle \sin^{-1}(|\cos(x)|)=\cos^{-1}(|\sin(x)|)$ in $[0,4\pi]$ is What I have done as using $\displaystyle \sin^{-1}(x)+\cos^{-1}(x)=\frac{\pi}{2}$ Then $\displaystyle \sin^{-1}(|\cos(x)|)=\frac{\pi}{2}-\sin^{-1}(|\sin(x)|)$ $\displaystyle \sin^{-1}(|\sin(x)|)+\sin^{-1}(|\cos(x)|)=\frac{\pi}{2}$ Now I did not know how do i go ahead Please look on that problem",['trigonometry']
4628777,Bonferroni-like inequality,"I am trying to prove this interesting tail bound for probabilities. Suppose that $A_1,\dots,A_n$ are independent events. I am interested in showing that $$
P(\bigcup_{I=1}^n A_i)\ge \sup_{m\ge 1}(1-e^{-m})\min\left\{1,\frac{1}{m}\sum_{i=1}^n P(A_i)\right\}.
$$ Not sure what can be done here, I tried using inclusion exclusion but that leads to no avail. Furthermore I have tried to just show directly this holds for just one of the $m$ but that doesnt really lead anywhere as well. Even just taking $m=1$ , it's not quite clear why $(1-e^{-m})$ is the subadditivity gap.","['probability-theory', 'probability']"
4628829,Two nice (challenging) binoharmonic series,"I've recently seen a nice binoharmonic series from Ali's book (page 309), but that post, for some reason, vanished ( Update : that post was undeleted and now it may be found here Evaluating $\int_0^{\frac{\pi}{2}}x^2 \cot x\ln(1-\sin x)\mathrm{d}x$ ). It is an excellent addition to MSE, that is, $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_{2n}}{n^3}.$$ since finding elegant ways is not easy to do. I would also add the following variant, which seems to be a new one (in case it is known, any reference will be highly appreciated), $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_{2n}}{n^4}.$$ Can we find very elegant ways to go for these two series? Lots of other variants we could consider, like $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_{2n}^2}{n^2}.$$ More curious binoharmonic structures (only a few examples in the list of possible items alike, keeping the weight $\le5$ ), $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{n^2};$$ $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{n^3};$$ $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{(2n+1)^2};$$ $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{(2n+1)^3}.$$ Let me also make a little update ( February 03, 2023 ): $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}H_{4n}}{n^2};$$ $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}H_{4n}}{(2n+1)^2}.$$ Update : Yes , that's the answer to the question above concerning the first series, and we can get brilliant proofs , and a paper is under work, which will be available soon.","['integration', 'real-analysis', 'harmonic-numbers', 'calculus', 'sequences-and-series']"
4628867,Cantor's Diagonalization Process(CDP) and North-east to South-west Diagonalization Process(NSDP),"While looking at the function $f:\mathbb{N}\rightarrow \mathbb{N} \times \mathbb{N}$ , I accidentally made an inversion in my labelling for $f(x)$ What I mean is, we go with a zig-zag path in CDP(either $f(2)=(1,2)$ or $f(2)=(2,1)$ , I am going with the former), but while writing the value for each of the members of $\mathbb{N}$ , I went in the north-east(NE) to south-west(SW) manner. Elaboration: $$(1,1),\ (1,2),\ (1,3),\ ...\\
(2,1),\ (2,2),\ (2,3),\ ... \\
(3,1),\ (3,2),\ (3,3),\ ... \\
  .\\
.\\
.\\$$ $\big($ Also, regardless of CDP or NSDP, we can observe below, that the last element in each row $n(n\geq2)$ is of the form ${n+1 \choose 2} \ \big( 3={2+1 \choose 2}$ , $6={3+1 \choose 2}$ , $10={4+1 \choose 2}\big)\big)$ In CDP we have: $f(1)=(1,1)$ $f(2)=(1,2), \ f(3)=(2,1)$ ( $\leftarrow$ NE to SW) $f(4)=(3,1), \ f(5)=(2,2),\ f(6)=(1,3)$ ( $\rightarrow$ SW to NE) $f(7)=(1,4),\ f(8)=(2,3),\ f(9)=(3,2)\ f(10)=(4,1)$ ( $\leftarrow$ NE to SW) $f(11)=(5,1),\ f(12)=(4,2),\ f(13)=(3,3), \ f(14)=(2,4),\ f(15)=(1,5)$ ( $\rightarrow$ SW to NE) ... But, I did: $f(1)=(1,1)$ $f(2)=(1,2), \ f(3)=(2,1)$ ( $\leftarrow$ NE to SW) $f(4)=(1,3), \ f(5)=(2,2),\ f(6)=(3,1)$ ( $\leftarrow$ NE to SW) $f(7)=(1,4),\ f(8)=(2,3),\ f(9)=(3,2)\ f(10)=(4,1)$ ( $\leftarrow$ NE to SW) $f(11)=(1,5),\ f(12)=(2,4),\ f(13)=(3,3), \ f(14)=(4,2),\ f(15)=(5,1)$ ( $\leftarrow$ NE to SW) ... Now, in doing so I saw a pattern:
Take the first column.
Let's label the set of the domains' elements in the first column. $$\begin{array}\{X_1&:=\{1,2,4,7,11,16,22,29,37,...\} \\
                       &=\{x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,...\}\\ \end{array}$$ and for the range: $$\begin{array}\{Y_1&:=\{(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(1,7),(1,8),(1,9),...\} \\
                       &=\{(1,y_1),(1,y_2),(1,y_3,(1,y_4),(1,y_5),(1,y_6),(1,y_7),(1,y_8),(1,y_9),...\}\\ \end{array}$$ We can see that the first co-ordinate is constant, and the second one increases at a linear pace. Moreover, $x_i+y_i=x_{i+1}$ Also, if we observe the $x_r's$ , $x_r=\frac{(r-1)(r)}{2}+1, \ \forall r \in \mathbb{N}$ In case of the second column: $$\begin{array}\{X_2&:=\{3,5,8,12,17,23,30,38,47,...\} \\
                       &=\{x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,...\}\\ \end{array}$$ $$\begin{array}\{Y_2&:=\{(2,1),(2,2),(2,3),(2,4),(2,5),(2,6),(2,7),(2,8),(2,9),...\} \\
                       &=\{(2,y_1),(2,y_2),(2,y_3,(2,y_4),(2,y_5),(2,y_6),(2,y_7),(2,y_8),(2,y_9),...\}\\ \end{array}$$ Observing the $x_s's$ , $x_s=\frac{(s)(s+1)}{2}+2, \ \forall s \in \mathbb{N}$ In case of the third column: $$\begin{array}\{X_3&:=\{6,9,13,18,24,31,39,48,58,...\} \\
                       &=\{x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,...\}\\ \end{array}$$ $$\begin{array}\{Y_3&:=\{(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(3,7),(3,8),(3,9),...\} \\
                       &=\{(3,y_1),(3,y_2),(3,y_3,(3,y_4),(3,y_5),(3,y_6),(3,y_7),(3,y_8),(3,y_9),...\}\\ \end{array}$$ Observing the $x_p's$ , $x_p=\frac{(p+1)(p+2)}{2}+3, \ \forall p \in \mathbb{N}$ By the first principle of mathematical induction, we have for the $n^{th}$ column:
for sets $X_n,Y_n$ : $\big(x_q=\frac{(q+(n-1))(q+(n-2))}{2}+n, \forall q \in \mathbb{N} \big)\forall n \in \mathbb{N}$ Thus, in general, the function looks like $f(x)=(\alpha,\beta)$ , where $x=\frac{(\beta+(\alpha-1))(\beta+(\alpha-2))}{2}+\alpha$ . Hence there is a dependence of at least two variables that we know of, on x. Now, my question is: (A) In NSDP, can I make a proper explicit form of the function?(I just know $x$ drives $\alpha,\beta$ , but I am unable to arbitrarily locate the value, other than knowing the column type slection, which is good while dealing with one column at a time, but in a general manner, I am not able to make the function's definition explicit.) (B) In CDP, how do I proceed with the logic used in NSDP, or is there a better way out?
(I know that the last element of each row $n$ is of the form ${n+1 \choose 2}$ and the sum of co-ordinates of each elements is equal in case of each row(eg: 1+3=2+2=3+1).)","['analysis', 'real-analysis', 'sequences-and-series', 'elementary-set-theory', 'set-theory']"
4628902,"Proof that $(x,\omega)\mapsto X^n(x,\omega)$ measurable and $X^n(x,\cdot)$ convergence in probabiliity gives a uniform limit in $(x,\omega)$","This is  part of the proof of Lemma 16.12 from Schilling's Brownian Motion. Let $(E,\mathscr{E})$ be a measurable space and $(\Omega, \mathscr{A},P)$ be a probability space.
Assume that $(X^n(x,\cdot))_{n\ge 1}$ is a sequence of random variables depending on a parameter $x$ . Also let $(x,\omega)\mapsto X^n(x,\omega)$ be $\mathscr{E}\otimes \mathscr{A}$ measurable and for every $x\in E$ , the sequence $X^n(x,\omega)$ converges in probability. The goal is then to show that there is a version of the limit $X(x,\omega)$ such that $(x,\omega)\mapsto X(x,\omega)$ is $\mathscr{E}\otimes \mathscr{A}$ measurable. The proof proceeds as follows. By assumption, $\lim_{m,n\to \infty}P(|X^n(x,\omega)-X^m(x,\omega)|>2^{-k})=0$ for all $k\ge 0$ and $x\in E$ . Next we define $$n_k(x):=\inf\{m>n_{k-1}(x):\sup_{i,j\ge m} P(|X^i(x,\cdot)-X^j(x,\cdot)|>2^{-k})\le 2^{-k}\}.$$ The text then says obviously, $x\mapsto n_k(x)$ is $\mathscr{E}$ -measurable and we see that $$P(\sup_{x\in E} |X^{n_k(x)}(x,\cdot)-X^{n_l(x)}(x,\cdot)|>2^{-k})\le 2^{-k}$$ for all $l>k$ .
And then using the completeness of the convergence in probability, this shows that $X^{n_k(x)}(x,\omega)$ converges in probability, uniformly in $x\in E$ and we can set this limit as $X(x,\omega)$ . My questions are $1$ . Why is $x\mapsto n_k(x)$ $\mathscr{E}$ measurable? I can't see why this is obvious. $2$ . From the definition of $n_k$ , how can we get the boundedness in probability, uniformly in $x$ of the form $$P(\sup_{x\in E} |X^{n_k(x)}(x,\cdot)-X^{n_l(x)}(x,\cdot)|>2^{-k})\le 2^{-k}$$ for all $l>k$ ? $3$ .Finally, how can we show that this implies convergence in probability for $X^{n_k(x)}(x,\omega)$ uniformly in $x\in E$ ? I would greatly appreciate if anyone could provide the details here.","['real-analysis', 'stochastic-processes', 'uniform-convergence', 'probability-theory', 'stochastic-calculus']"
4628978,"Hyperbolic sums $S(n,k)=\sum_{m=1}^{\infty} \frac{1}{\cosh(\pi m)^{n} \sinh(\pi m)^{k}}$","It can be verified that $$
\sum_{n=1}^{\infty}\frac{1}{\cosh(\pi n)^3\sinh(\pi n)^2}
=\frac{11}{12}-\frac{3K}{2\pi}+\frac{K^2}{2\pi^2}-\frac{K^3}{\pi^3}$$ where $K=\frac{\Gamma\left ( \frac14 \right )^2 }{4\sqrt{\pi}}$ (here and below) and $\Gamma(z)$ is the Euler Gamma function. Proof. The proof relies on complex analysis.  Let us directly consider the function $$
f(z)=\frac{\operatorname{cs}\left(z,\frac{1}{\sqrt{2}}\right)}{\cosh(\frac{\pi z}{2K})^3\sinh(\frac{\pi z}{2K})^2}.
$$ Where $\operatorname{cs}(z,k)$ denotes one of Jacobi's elliptic functions. Integrate the function along a rectangular contour with vertices $-L,L,L+2Ki,-L+2Ki$ counterclockwise, where $L$ is a positive real number such that there is no pole on the left(or right) edge of the contour. To avoid poles on the upper and lower edges, construct infinite semicircles around the poles inside the rectangle with radius $r$ . Since $\operatorname{cs}$ has double periodicity and poles at $z=\{ (2m+2ni)K\mid (m,n)\in\mathbb{Z}^2\} $ , we have $$
\mathcal{P.V.}\int_{-L}^{L}[f(z)-f(z+2Ki)]\text{d}z+
\sum_{L_n\in\text{semicircles}}\int_{L_n}f(z)\text{d}z
+\int_{L}^{L+2Ki}f(z)\text{d}z+\int_{-L+2Ki}^{-L}f(z)\text{d}z
=2\pi i\sum_{z_k\in\text{poles inside the contour}}\operatorname{Res}(f(z),z_k).
$$ As $L\rightarrow+\infty$ , the first, third and fourth integrals obviously vanish because $f(z)$ is odd. So we have $$
\sum_{L_n\in\text{semicircles}}\int_{L_n}f(z)\text{d}z=2\pi i\sum_{z_k\in\text{poles inside the contour}}\operatorname{Res}(f(z),z_k).
$$ The rest is easy. Note that $$
\sum_{L_n\in\text{semicircles}}\int_{L_n}f(z)\text{d}z=
-\pi i\sum_{z_k\in2K\mathbb{Z},2K\mathbb{Z}+2Ki}\operatorname{Res}(f,z_k)=-\pi i\left ( 4\sum_{n=1}^{\infty}\frac{1}{\cosh(\pi n)^3\sinh(\pi n)^2}+2\operatorname{Res}(f(z),0)\right).
$$ Then we get $$
\sum_{n=1}^{\infty}\frac{1}{\cosh(\pi n)^3\sinh(\pi n)^2}=\frac12\left(-\operatorname{Res}(f,0)-\operatorname{Res}(f,iK)\right).
$$ The residues are easy to compute, then we prove the sum. We can compute the values of $S(2n+1,2k)=\sum_{m=1}^{\infty} \frac{1}{\cosh(\pi m)^{2n+1}
\sinh(\pi m)^{2k}},(n,k)\in\mathbb{Z}^2$ if the sum exists. For example, $$
S(3,4)=-\frac{943}{720}+\frac{5K}{2\pi}
-\frac{13K^2}{12\pi^2}+\frac{K^3}{\pi^3}+\frac{K^4}{20\pi^4},$$ $$
\sum_{n=1}^{\infty} \frac{1}{\cosh(\pi n)^{11}}
=-\frac{1}{2}+\frac{63K^{}}{256\pi^{}}
+\frac{117469K^{3}}{201600\pi^{3}}+
\frac{17281K^{5}}{10080\pi^{5}}+\frac{3553K^{7}}{1200\pi^{7}}
+\frac{869K^{9}}{240\pi^{9}}+\frac{1381K^{11}}{600\pi^{11}}.
$$ Question 1 : Can we prove the sum in an alternative way, or is there a simpler method to compute these $S(n,k)$ ? Question 2 : Is there a complex method to compute $S(2n,2k)$ for integers $n,k$ ?","['hyperbolic-functions', 'complex-analysis', 'contour-integration', 'elliptic-functions', 'sequences-and-series']"
4629030,Finding the generating function.,"Find the generating function for the number of solutions for the equation $x_1+x_2+x_3+x_4 = n$ , where $x_1,x_2,x_3,x_4\geq1$ , and $x_1 < x_2$ . My attempt so far: I have tried putting a $y$ value in my equation, where $y$ represents the difference between $x_2$ and $x_1$ , $ y $ must be greater than $0$ and less than or equal to $n-4$ ,because we have to take something for $x_1,x_2,x_3,x_4$ . Then, after solving it, I get that the number is ${n-1\choose n-5}$ , which works for $n = 5$ , but for nothing else. Any help would be appreciated, thanks!","['integer-partitions', 'discrete-mathematics', 'generating-functions']"
4629046,Is there a category-theoretic explanation for this analogy between $\mathbf{Set}$ and $\mathbf{Vect}_k$?,"You can tell whether there exists an injective, surjective, or bijective map $A\to B$ by comparing the cardinalities of $A$ and $B$ . Similarly, you can tell whether there exists an injective, surjective, or bijective linear map $U\to V$ by comparing the dimensions of $U$ and $V$ (i.e., the cardinalities of their Hamel bases). Is there a category-theoretic explanation for this analogy? I'm inclined to consider the functor $\mathbf{Set}\to\mathbf{Vect}_k$ given by formal linear combinations on objects and linear extensions on arrows, but I'm not sure where to go from there.","['linear-algebra', 'vector-spaces', 'category-theory']"
4629048,Why does this description of vector spaces include so many tedious/ seemingly obvious criteria?,"I'm a CS student. I commonly notice that when I'm learning math-heavy topics (like machine learning) that the descriptions often seem overly tedious and unintuitive, like the one below. Vector spaces are the basic setting in which linear algebra happens. A vector space $V$ is a set (the elements of which are called vectors) on which two operations are defined: vectors can be added together, and vectors can be multiplied by real numbers called scalars. $V$ must satisfy There exists an additive identity (written $0$ ) in $V$ such that $x + 0 = x$ for all $x\in V$ . For each $x\in V$ , there exists an additive inverse (written $−x$ ) such that $x + (−x) = 0$ . There exists a multiplicative identity (written $1$ ) in $\mathbb{R}$ such that $1x = x$ for all $x\in V$ . Commutativity: $x + y = y + x$ for all $x,y\in V$ . Associativity: $(x + y) + z = x + (y + z)$ and $\alpha(\beta x) = (\alpha \beta)x$ for all $x,y,z\in V$ and $\alpha,\beta\in\mathbb{R}$ . Distributivity: $\alpha(x + y) = \alpha x + \alpha y$ and $(\alpha+\beta)x = \alpha x + \beta x$ for all $x,y\in V$ and $\alpha,\beta\in\mathbb{R}$ . Why do we even need to specify these rules? It seems fairly obvious that in 1., $x +0 =x$ . If fact, I can't think of a case where that wouldn't be true.","['machine-learning', 'linear-algebra', 'vectors', 'axioms']"
4629074,Solution of non-homogeneous second-order differential equation with time dependent coefficients [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I am having difficulty understanding how to solve this differential equation: $$ \ddot x+\frac{\dot a(t)}{a(t)}\dot x+b^{2}(t)x=-\frac{1}{a(t)}k$$ where $a(t)$ and $b(t)$ are time-dependent coefficients and $k$ is a constant. Could you give me some advice? I think I have to use Wronskian but I can't solve it anyway. I hope I have expressed myself clearly and thank you in advance for your help.",['ordinary-differential-equations']
4629088,Existence in ZF of a set with countable power set,"Is it consistent with ZF for there to exist a set $S$ such that the power set $P(S)$ is countable? If so, what is the weakest form of the axiom of choice needed to prove that no such set exists?","['axiom-of-choice', 'set-theory']"
4629140,"Why does a wheel need seven spokes to hold it rigid? (an ""inverse problem"")","In the biography ""King of infinite space: Donald Coxeter, the man who saved geometry"" by Siobhan Roberts, the following passage describes an aspect of the subject's relationship with Buckminster Fuller: Coxeter was not the first to be frustrated by Fuller.  Fuller was neither an architect nor engineer nor mathematician by training, and he became a controversial figure among experts in all those fields.  He made awkward geometric mistakes, such as how many spokes are needed on a wheel to hold it rigid (Fuller said twelve instead of seven). I was intrigued by the claim that seven spokes are needed on a wheel to hold it rigid.  Searching Google does reveal that Fuller seems to have thought the answer was twelve, but I cannot find any other claim that it should be seven. I cannot figure out a (mathematical) model in which seven spokes is the answer (nor twelve, for what it's worth).  It certainly seems that the spokes are assumed to be ""tension"" only, as of course there are three-spoke bicycle wheels made of solid (rigid) spokes.  In other words, I assume the subject under discussion is wire wheels , where the spokes ""function mechanically the same as tensioned flexible wires"". I have constructed a few models, definitely unrealistic, where the minimal number of spokes is three. Can anyone give a mathematical model of tension-spoked wheels where the minimum number of spokes is seven (or twelve, or perhaps any finite number greater than three)? [The ""inverse problem"" in the title refers to the fact that in this situation, we know the answer but not the question!]","['physics', 'mathematical-modeling', 'rigid-transformation', 'geometry']"
4629170,Unsure about basic discrete math [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Question 1 Seeing that set $B$ is a member of set $A$ , and set $C$ is by extension a member of set $A$ by being a member of set $B$ , I would like to think that both $B$ and $C$ are empty sets since their requirements are based on values that don't exist in set $A. (A = {7})$ It's also difficult to imagine that all most of these answers would simply be "" $\{ \}$ "", so I am assuming that I don't understand this correctly. Thanks,
CS","['elementary-set-theory', 'discrete-mathematics']"
4629173,"Find the value of the $5$th decile, $D_5$.","For the data set: $$18,15,12,6,8,2,3,5,20,10$$ Find the value of the $5$ th decile, $D_5$ . I computed this 2 ways and each time I got a different answer. If the sample size is $n$ , then the rank of the $m$ th decile is $\frac{mn}{10}$ . So in our case it would be $\frac{5(10)}{10}=5$ . So it would be the fifth value in the data after we arrange them from the smallest to the largest. $$2,3,5,6,8,10,12,15,18,20$$ So it would be $D_5=8$ . We know that $D_5=Q_2$ , where $Q_2$ is the second quartile. $Q_2$ is the value with $50%$ of the data below it, so it is the mean of the data. In this case the mean is $9$ So is it $8$ or $9$ ? Any help would be appreciated. Thanks!","['statistics', 'quantile']"
4629211,Can a vector field on a non-intersecting smooth curve assign multiple vectors to a point?,"I'm reading Semi-Riemannian Geometry by Newman - currently about vector fields on curves. Let $\gamma(t)$ be an injective smooth curve from $(a,b)\subset \mathbb{R}$ to a smooth manifold $M$ . I know that for any $t_0\in(a,b)$ , $\frac{d\gamma}{dt}(t_0)$ is an operator (or a tangent vector in $T_{\gamma(t_0)}(M)$ ), where $$\frac{d\gamma}{dt}(t_0)(f)\equiv\frac{d(f\circ\gamma)}{dt}(t_0)$$ The book states that a vector field on $\gamma$ is defined as a map $J_{\gamma}$ that assigns each $t\in(a,b)$ to a vector $J_{\gamma}(t)$ in $T_{\gamma(t)}(M)$ . The set of smooth vector fields on $\gamma$ is denoted by $\mathfrak{X}_M(\gamma)$ . Then there is a theorem stating that $d\lambda/dt$ is a vector field in $\mathfrak{X}_M(\gamma)$ . I'm confused at this stage: corresponding to a particular curve $\gamma$ and at a particular point $p=\gamma(t_0)\in M$ , by definition we identify a single vector, right? Which is $d\gamma/dt$ evaluated at $t=t_0$ . So then we should have only one vector field on $\gamma$ ? Because if there were several, then we could identify at least two fields in $\mathfrak{X}_M(\gamma)$ that pick out different values at some $p$ lying in $\gamma$ 's image, which contradicts the previous paragraph. My understanding is: even if there are obviously several vectors in $T_{\gamma(t_0)}(M)$ , there's only one that is tangential to $\gamma$ . Would appreciate any help since I seem to be missing something. Maybe I'm wrong in my assumption that the vectors in the vector field need to be tangential to the curve?",['differential-geometry']
4629225,Essential singularity of $e^{1/z}$,"$e^{1/z}$ has an essential singularity at $z=0$ , which by the Casorati–Weierstrass theorem implies that for every complex number $W$ there is a sequence $z_k\to 0$ with $e^{1/z_k}\to W$ . Is there some elegant way to see this directly without proving Casorati–Weierstrass first? (I know that the proof of Casorati–Weierstrass is not hard, I'm just curious.)","['complex-analysis', 'singularity']"
4629239,Show that $\det(xA+yB+zI_{n})=\det(yA+xB+zI_{n})$,"We have $A$ and $B$ $(n×n)$ matrices with complex entries. We know that $A-B=AB-BA$ . Show that $$\det(xA+yB+zI_{n})=\det(yA+xB+zI_{n})$$ for every $x,y,z$ complex numbers with $x+y≠0$ . We can see that $\operatorname{Tr}(A)=\operatorname{Tr}(B)$ . I tried to suppose $A$ or $B$ are invertible and try changing $A-B=AB-BA$ somehow, but I dont know if that helps. We can also see that $$\operatorname{Tr}(xA+yB)=\operatorname{Tr}(yA+xB)=(x+y)\operatorname{Tr}(A).$$ The conclusion looks like we need to show that $f(x,y)=f(y,x)$ . Maybe some calculus and we can use the continuity of polynomial functions? I also minded calculating the determinants using polynomial forms.","['contest-math', 'determinant', 'eigenvalues-eigenvectors', 'matrices', 'characteristic-polynomial']"
4629243,Is it true the set of functions with an infinite or undefined expected value form a prevalent subset of the set of all functions?,"Suppose for $n\in\mathbb{N}$ , we have set $A\subseteq\mathbb{R}^{n}$ which is measurable w.r.t to to some arbitrary measure the radon-nikodym derivative of the uniform probability measure on the $\sigma$ -algebra of caratheodory measurable sets along with measurable function $f:A\to\mathbb{R}$ . According to here and here , “almost all” measurable functions in a function space can be defined without a measure on the set of measurable functions. Such a set of functions are known as prevelant set in a function space. Question: Is it true the set of all functions with infinite or undefined expected value forms a prevalent set in the subset of the set of all measurable and non-measurable functions? This is a sequel to question, "" Is it true the set of Lebesgue-measurable functions that are non-integrable are prevalent in the set of measurable functions? "" I assume if the question in that link can be answered then the question here could be answered. (Infact, a statisticians' answer in that link was only part of the full answer stated here ): We can follow the argument presented in example 3.6 of this paper Because a function can always be represented as $f=f^{+}-f^{-}$ we only consider whether positive functions have a mean value. We consider the case of set $A$ with a finite positive measure. In this context having a mean means having a finite integral, and not not being integrable means having an infinite integral. Take $X:=L^{0}(A)$ (measurable functions over $A$ ), let $P$ denote the
one-dimensional sub-space of $A$ consisting of constant functions
(assuming the Lebesgue measure on $A$ ) and let $F:=L^{0}(A)\setminus
 L^{1}(A)$ (measurable functions over $A$ with no finite integral). Let $\lambda_{P}$ denotes the Lebesgue measure over $P$ , for any fixed $f\in F$ : $$\lambda_{P}\left(\left\{\alpha\in\mathbb{R}\left| \int_{A}\left(f+\alpha\right) d\mu<\infty\right.\right\}\right)=0 $$ Meaning $P$ is a one-dimensional probe of $f$ , so $f$ is a 1-prevalent set. Is this true? If so, then it seems my hypothesis is incorrect. Is this true? If so, this still does not completely prove my hypothesis since we are taking the subset of all Lebesgue-measurable functions rather than the set subset of all measurable and non-measurable functions . What do you think and what are your suggestions to proving this my hypothesis? since measurable functions with a undefined expected value is prevelant in the set of all measurable functions; and the expected value of functions defined on non-measurable sets have an undefined expected value, the set of all functions (measurable/non-measurable) with an undefined (or infinite) expected value should be prevelant in the set of all functions .","['measure-theory', 'functions', 'functional-analysis']"
4629268,Confusion while factoring quadratics.,"I recently factored the quadratic $3x^2+4x-4$ , the results where $x = -2$ ; $x = 2/3$ . My question is why is it incorrect to write the factors of the quadratic as $(x+2)(x-2/3)$ ? The correct way to write it is $(x+2)(3x-2)$ , but why? What is the difference in $(x-2/3)$ and $(3x-2)$ , both when $x$ is $2/3$ result in the y being zero.","['algebra-precalculus', 'quadratics', 'factoring']"
4629333,"In a Clifford algebra over an $n$-dimensional non-degenerate space, is a product of any number of vectors a product of at most $n$ vectors?","In a Clifford algebra over an $n$ -dimensional vector space $V$ with a non-degenerate quadratic/bilinear form, can any product of vectors $a_1,a_2,\cdots,a_m\in V$ be written as $$a_1a_2\cdots a_m=a_1'a_2'\cdots a_k'$$ for some $k\leq n$ and $a_1',a_2',\cdots,a_k'\in V$ ? Due to the grading on the algebra, necessarily $k\equiv m\bmod2$ (unless the product vanishes). It suffices to prove that a product of $n+1$ vectors can be reduced to a product of $n-1$ vectors. If the vectors are all invertible ( $a^{-1}=\frac{a}{a\cdot a}$ ), then they represent reflections ( $v\mapsto-ava^{-1}=v-2\frac{v\cdot a}{a\cdot a}a$ ), and the product represents an isometry of $V$ . According to the Cartan-Dieudonne theorem, any isometry is the composition of at most $n$ reflections, so we get the desired result. (I could make the correspondence between multivectors and isometries more explicit, but that's beside the point of this question.) So, suppose some of the vectors are null ( $a\cdot a=0$ ), and thus don't represent isometries. Non-degeneracy means, for $a\neq0$ , there's some vector $v\in V$ such that $a\cdot v\neq0$ . To see that this condition is necessary, consider $V=\mathbb R^3$ with the degenerate quadratic form $v=(v_1e_1+v_2e_2+v_3e_3)\mapsto v\cdot v=v_1^2-v_2^2+0v_3^2$ . (This space may be denoted $\mathbb R^{1,1,1}$ .) The product of four unit vectors $$e_1(e_1+e_3)e_2(-e_2+e_3)=1+(e_1+e_2)e_3$$ cannot be written as a product of two vectors; their wedge product would have to be $(e_1+e_2)e_3$ , so they'd be in the span of the null vectors $e_1+e_2$ and $e_3$ , but then their dot product would be $0$ , not $1$ as required. I've proven this for $n=2,3,4$ ( $n=1$ requires a scalar in front of the empty product). See the Degenerate Cartan-Dieudonne chat . I also thought through a proof for $n=5$ , but I didn't write it down, and it's a bit complicated. Anyway, the methods I used won't work for $n\geq6$ . I relied on the fact that the product of vectors, $P=a_1a_2\cdots a_{n+1}$ , has only grades $\equiv n+1\bmod2$ , and has $P\tilde P$ and $\tilde PP$ being scalars. These latter properties alone imply that $P$ is a product of vectors, only in low dimensions. Over $\mathbb R^6$ , the odd multivector $P=e_1e_2e_3+e_4e_5e_6$ has $P\tilde P=\tilde PP=2$ , but $P$ is not a product of vectors, because $Pe_1\tilde P$ is not a vector (it's a $5$ -vector).","['clifford-algebras', 'linear-algebra', 'quadratic-forms']"
4629353,Can we define the fractional derivative by mapping a function to a sinusoidal?,"For integer $n$ we have that: $$\frac{d^n}{dx^n} \sin(x) = \sin\left(x+\frac{n \pi}{2}\right)$$ For any function $f(x)$ (ignoring domain restrictions for the time being), let: $$f(x) = \sin(u) \ \ [1]$$ $$u = \arcsin\left(f(x)\right)  \ \ [2]$$ Then, take the $n$ th derivative wrt $u$ of both sides of $[1]$ : $$\frac{d^n}{du^n} f(x) = \frac{d^n}{du^n} \sin(u)$$ $$\frac{d^n}{du^n} f(x) = \sin\left(u+\frac{n \pi}{2}\right)$$ Now we can solve for $du$ and substitute: $$f'(x) \ dx = \cos(u) \ du$$ $$du = \frac{f'(x)}{\cos(u)} \ dx  = \frac{f'(x)}{\cos\left(\arcsin(f(x)\right)} \ dx = \frac{f'(x)}{\sqrt{1-f(x)^2}} \ dx $$ Then: $$\frac{d^n}{du^n} f(x) = \frac{d^n}{\left(\frac{f'(x)}{\sqrt{1-f(x)^2}} \ dx \right)^n} \ f(x) = \left(\frac{f'(x)}{\sqrt{1-f(x)^2}}\right)^{-n} \frac{d^n}{dx^n} f(x)$$ Finally we can solve for the $n$ th derivative in terms of $x$ and $n$ alone, using $[2]$ : $$\frac{d^n}{dx^n} f(x) = \sin\left(\arcsin\left(f(x)\right)+\frac{n \pi}{2}\right) \left(\frac{f'(x)}{\sqrt{1-f(x)^2}}\right)^{n}$$ And now there is no issue with letting $n$ vary continuously. Now from this formula we have that $|f(x)| < 1$ , so it obviously doesn't work everywhere for all functions. I made a Desmos graph , try inputting different choices of $f$ and see how it interpolates between the function and its first derivative. My questions are the following: Why is there sometimes not a smooth transition between the function and its derivative? Try letting $f = x^{x}$ and see what happens when you let $n$ go from $0$ to $1$ , the left half flips back and forth. Is this due to the exponentiation not being defined? How do we extend the domain? We can just make the transformation $f(x) \to f(x - c)$ to center the derivative around a point other than 0, but it's not clear how to make this accept functions with a magnitude greater than $1$ . Is this always exact for integer n so long as $|f(x)| < 1$ ? It seems like there might be a bit of error, but can't tell if that is numerical or due to the domain of the functions involved. Has anything like this been done before? I'd like to see if there is any existing literature that uses this sort of idea.","['complex-analysis', 'fractional-calculus', 'real-analysis']"
4629411,Possibility of arranging $1$ and $-1$ in a grid such that the sum of the products is $0$,"Consider an $11$ x $11$ grid, where in each square, the number $1$ or $-1$ is written. One multiplies the numbers in each row and column, and then sums up these $22$ products. Is it possible for this sum to be equal to $0$ ? My approach was to realize that the product in every row or column is either $1$ or $-1$ . This means, since we have $22$ products, that $11$ of them must be equal to $1$ and the other $11$ to $-1$ . If there is an odd number of $-1$ :s in a row or column, the product will be $-1$ , while if there is an even number of $-1$ :s, the product will be $1$ . Then, I considered coloring each square in the following way: black if the cell contains a $-1$ , uncolored if the cell contains a $1$ . Thus, the question becomes: can one color grid cells in an $11$ x $11$ grid in such a way that an odd number of cells is colored in $11$ rows/columns, and an even number of cells is colored in the remaining $11$ rows/columns? However, from here, I couldn't make much progress. Does anyone have any ideas?","['combinatorics', 'parity', 'discrete-mathematics']"
4629439,Proving some rational numbers are integers,"Let $r_1$ , $\ldots$ , $r_k$ distinct (non-zero) rational numbers, $P_1$ , $\ldots$ , $P_k \in \mathbb{Q}[x]$ non-zero polynomials, such that for every integer $n\ge 0$ the number $$\sum_{s=1}^k P_s(n) r_s^n$$ is an integer. Show that all of the numbers $r_1$ , $\ldots$ , $r_k$ are integers. $\bf{Notes:}$ This is a particular case of a question I posted  before. The solution I managed to come up with of the general case could be  understood here, even by people not familiar with the rudiments of algebraic number theory. Thus the posting.  I would like to see whether there are any other distinct solutions in this case.  Thank you for your attention!  Any feedback would be appreciated!","['elementary-number-theory', 'divisibility', 'sequences-and-series']"
