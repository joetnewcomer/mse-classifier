question_id,title,body,tags
2535824,"$P(X_{(2)}>2)$ given $f_X(x) =\frac{2}{x^3}I_{(1,\infty)}(x)$","$X_1$, $X_2$, and $X_3$ are independent random variables having pdf $$f_X(x) =\frac{2}{x^3}I_{(1,\infty)}(x)$$ Give the value of $P(X_{(2)}>2)$ Would this just be the same as finding the probability that at least two of $X_1$, $X_2$, and $X_3$ are greater than $2$? Because if only $1$ were greater than $2$ then that would be $X_{(n)}$, the sample maximum. If that's true, then I have for an individual $X$ that $$\begin{align*}
P(X>2)
&= 1-\int_1^2 \frac{2}{x^3}dx\\\\
&= 1-\left({-1\over{x^2}}|_1^2\right)\\\\
&= 1-\left(\frac{-1}{4}-\frac{-1}{1}\right) \\\\
&= \frac{1}{4}
\end{align*}$$ Then $$\begin{align*}
P(X_{(2)}>2)
&= {3\choose{2}}\cdot\frac{1}{4}^2 \cdot\frac{3}{4}+{3\choose{3}}\cdot\frac{1}{4}^3\\\\
&= \frac{5}{32}=.15625\\\\
\end{align*}$$ Is this correct?","['statistics', 'probability', 'probability-distributions']"
2535857,Probability of the existence of a specific pattern in one million coin flips,"I came across this question while preparing for an interview. Given a coin with head-up probability p, flip it $N = 1,000,000$ times. What is the probability that a string of ""HHHHHHTTTTTT"" (i.e. 6 heads followed by 6 tails) exists? Can this pattern be generalized? Any idea on the question? Thanks in advance! This question has two related questions: Average number of the pattern. Average number of the pattern of at least 6 tails in a row. For Question 1, one can solve it using the linearity of expectation. Specifically, let $X_i$ be a binary variable indicating whether there is one such pattern starting at the ith flip. Then the average number of the pattern across all one million flips is just
\begin{align*}
E \sum_{i=1}^{N-11} X_i &= \sum_{i=1}^{N-11} E X_i \\
&= (N-11) p^6(1-p)^6.
\end{align*} Question 2 can be solved in a similar way. The only difference is that this time $X_i=1$ requires that the $(i-1)$th flip is a head. Hence
\begin{align*}
E \sum_{i=1}^{N-5} X_i &= E X_1 + \sum_{i=2}^{N-5} E X_i \\
&= (1-p)^6+(N-6) p(1-p)^6 \\
&=(1+Np-6p)(1-p)^6.
\end{align*}","['probability-theory', 'probability', 'statistics']"
2535875,"Prove the existence of $X \subset \mathcal{P}(\mathbb{N})$ with continuum cardinality, such that each intersection of two of its elements is finite. [duplicate]","This question already has answers here : Can a countable set contain uncountably many infinite subsets such that the intersection of any two such distinct subsets is finite? (6 answers) Closed 6 years ago . Prove that there exists a set $X \subset \mathcal{P}(\mathbb{N})$ so
  that $|X| = \mathfrak{c}$ and for every $A,B\in X$, the set $A \cap B$
  is finite. Since this statement feels unintuitive, it's difficult to come up with ideas on how to prove it. Surely if the set X has continuum cardinality, some intersections of it's elements are bound to be infinite as well? My thought then went to assembling $X$ so that all elements in $X$ contain the number 1 and no other common elements. However, that seems quite difficult, when we have to make a continuum cardinality of these sets where the only common element is 1. So basically, I'm stuck and I'd like help.","['infinity', 'elementary-set-theory']"
2535877,"Property of $(p-3)! + 1$, where $p$ is a prime.","I am currently working on a take home exam for undergraduate number theory so I can only accept hints. The problem I am working on is: Let $p$ be an odd prime such that $p \equiv 3,5$ mod $8$. Prove that $(p-3)! + 1$ is not a perfect square. My approach so far is: If $(p-3)! + 1$ is a perfect square, then it would a perfect square mod $p$. By Wilson's theorem, $(p-3)! + 1 \equiv (-1)(p-1)^{-1}(p-2)^{-1} + 1 \text{ mod } p$. Since $(p-1)^{-1} \equiv_{p} (p-1)$ and $(-1) \equiv_{p} (p-1)$, we have $(-1)(p-1)^{-1}(p-2)^{-1} + 1 \equiv ((p-2)^{-1} + 1) \text{ mod } p$. We know $(p-2)^{-1} \equiv_{p} (-2)^{-1} = \frac{p-1}{2}$ so, $((p-2)^{-1} + 1) \equiv_{p} \frac{p-1}{2} + 1 = \frac{p+1}{2}.$ At this point, I am quite stuck and would appreciate any feed back or hints as to another approach I can use.","['number-theory', 'elementary-number-theory']"
2535880,Computing: $\int_\Bbb R\frac{dt}{\cosh (\pi t )+\cos (\pi x )}~~0<x<1 $,"We are given the following problem: prove the existence and compute integrals $$I(x)=\int_\Bbb R\frac{dt}{\cosh (\pi t )+\cos (\pi x )} $$ $$J(x)=\int_\Bbb R\frac{dt}{\cosh (\pi t )-\cos (\pi x )} $$ where, $0<x<1.$ I have already proved the existence. Also I remarked that $J(1-x) =I(x)$ Now I am struggling to compute $I(x)$ any help?","['real-analysis', 'integration', 'calculus']"
2535881,Justifying exchanging limit and integral arising in a contour integration with a branch point,"I'd like to solve $\displaystyle \int_0^\infty \frac{x^\alpha}{x(x+1)}dx,$ where $\alpha \in (0,1).$ The answer is  $\frac{\pi}{\sin (\alpha \pi)}.$ Typically, to solve it, we use contour integral and residue theorem as follows. Let $C=C_{r}^R-I_{r,R}^- -C_r+I_{r,R}^+$ denote the simple closed positively oriented contour, where $-C_r$ and $C_{r}^R$ are the portions of the circles $C_r(0)$ and $C_R(0)$, respectively, and $-I_{r,R}^-$ and $I_{r,R}^+$ the horizontal segments joining them. We select a small value of $r$ and a large value of $R$ so that the nonzero poles $-1$ of $\displaystyle f(z):=\frac{z^\alpha}{z (z+1)}$ lie inside $C.$  We use the branch of $z^\alpha$ corresponding to the branch of the logarithm $\log_0$ as follows: $\displaystyle z^\alpha=e^{\alpha \log_0 (z)} =|z|^\alpha e^{i \alpha\theta}$ for $z=re^{i\theta}\neq 0$ and $\theta \in (0,2\pi]$ Then $\displaystyle \int_C f(z)dz=\int_{C_{r}^R}f(z)dz-\int_{I_{r,R}^-}f(z)dz -\int_{C_r}f(z)dz+\int_{I_{r,R}^+}f(z)dz.$ Using the residue theorem,
    \begin{equation}\label{8-29}
\int_C f(z)dz=2\pi i \textrm{Res}[f,-1].
\end{equation}
 On the other hand, $\displaystyle \lim_{r\to 0^+}\int_{C_r^R}f(z)dz=\int_{C_R^+(0)}f(z)dz$, and it can be shown that $\displaystyle\lim_{r \to 0^+} \int_{C_r}f(z)=\lim_{R\to \infty}\int_{C_R^+(0)}f(z)dz=0,$ by M-L inequlity. My question arises in the next part (most textbooks don't explain it in detail): Because of the branch we chose for $z^\alpha,$  $\displaystyle \lim_{r\to 0^+} \int_{I_{r,R}^+}f(z)dz=\int_0^R\frac{x^\alpha}{x(x+1)}$ and $\displaystyle\lim_{r\to 0^+} \int_{I_{r,R}^-}f(z)dz=\int_0^R\frac{x^\alpha e^{i\alpha 2\pi}}{x(x+1)}$. Note that since $Q$ has a zero of order at most $1$ at the origin, the above two integrals converge. I understand that the integrand $f(z)$ on the upper horizontal line approaches $\frac{x^\alpha}{x(x+1)}$, where $x$ is a real number. Similarly, $f(z)$ on the lower horizontal line approaches $\frac{x^\alpha e^{i\alpha 2\pi}}{x(x+1)}$. I'd like to prove $\displaystyle \lim_{r\to 0^+} \int_{I_{r,R}^+}f(z)dz=\int_0^R\frac{x^\alpha P(x)}{Q(x)}$ in detail. What theorem do I use for this equality. Do I use Lebesgue dominated convergence theorem? I appreciate if you give any comments about it. Thanks in advance.","['complex-analysis', 'improper-integrals', 'contour-integration', 'branch-points']"
2535891,Asymptotic behavior of solutions to laguerre's differential equation,"The Frobenius solution to the homogenous DE (Laguerre's): $\\$ $$xg'' + (2n+2 -x)g' + (\lambda - n -1)g = 0$$ is given by $$g(x) = \sum_{k = 0}^\infty a_k \,x^{n+k}$$ where the coefficients are generated by the recurrence: $$a_0 = 1, \qquad \frac{a_{k+1}}{a_k} = \frac{n+k+1 - \lambda}{2(n+k+1)(k+1)}$$ $\\$ The series truncates for certain integer values of $\lambda$. Multiple texts mention that for non-integer values of $\lambda$ the solution grows on the order of $e^x$ at infinity. Unfortunately, this statement is always prefaced by ""it can be shown that .."". I can't seem to find a source that demonstrates this. I was hoping someone could point me in the right direction.","['analyticity', 'asymptotics', 'ordinary-differential-equations']"
2535924,Finding limit of a continuous function,"I have difficulty in calculating the following limit. Let $f$ be a continuous function on $[-1,1]$, it is desired to find the following limit, $\lim_{n\rightarrow \infty}n \int_{-\frac{1}{n}}^{\frac{1}{n}} f(x)(1-n|x|)dx$ Thank you very much in advance.","['real-analysis', 'analysis']"
2535927,Every Irreducible Representation of a Compact Group is Finite-Dimensional,"I know it's true that every irreducible representation of a compact Lie group is finite-dimensional, and I've seen it mentioned that this result is a corollary of the Peter-Weyl Theorem. However, I don't really see how that's possible. The Peter Weyl theorem gives us the following isomorphism of $G \times G$-representations for a compact Lie group $G$: $$\widehat{\bigoplus_{\rho \in \widehat{G}}}\,\, \rho^* \boxtimes \rho \simeq L^2(G),$$ where $\widehat{G}$ denotes the space of finite-dimensional irreducible representations of $G$. Is there any way to use this statement to deduce that every irreducible representation is finite-dimensional? Here is what I have thus far. The Schur orthogonality relations, which state that the images of two different irreducible representations under the Peter-Weyl map are orthogonal, can be proven in the case where only one of the two representations is finite-dimensional. So for an infinite-dimensional $\rho$, the image of $\rho^* \boxtimes \rho$ in $L^2(G)$ is orthogonal to the images of all the finite-dimensional representations. If the Peter-Weyl map embeds $\rho^* \boxtimes \rho$ as a subrepresentation of $L^2(G)$, then we would have a contradiction because the image of the direct sum of all the finite-dimensional representations is dense in $L^2(G)$. But I don't know how to show that the Peter-Weyl map is injective without first proving that it's an isometry, and this uses finite-dimensionality. Clarification: I am not putting any restrictions on what sort of infinite-dimensional vector space we're representing $G$ on. I just want to use the Peter-Weyl Theorem to show that every infinite-dimensional representation of a compact Lie group has a proper invariant subspace, with no additional assumptions on the vector space.","['representation-theory', 'compact-operators', 'group-theory', 'lie-groups']"
2535932,If $\operatorname{var}(X) =0$ then $p( X = E (X)) = 1.$,"Let $X$ be a random variable.
Then $\operatorname{var}(X) =0$ implies $p( X = E (X)) = 1.$ Actually i need both directions. But i was able to show that  $p( X = E (X)) = 1$ implies $\operatorname{var}(X) =0.$ So all i need is the other direction. I think we have to use the definition of the variance and set the $\operatorname{var}(X) = 0.$ But why is $p(X = E(X)) = 1$?","['probability-theory', 'probability', 'variance']"
2535961,Number of divisors of a number $30^{30}$ which have 30 divisors,"The task is to calculate the number of divisors of a number $30^{30}$, but only of those which have 30 divisors themselves. Basically, I can count how many divisors given number has, since they are all in a form $2^\alpha3^\beta5^\gamma$, such that $\alpha,\beta,\gamma\leqslant30$. What bothers me is how to approach to the other part of the problem. Any hints would be appreciated.","['combinatorics', 'discrete-mathematics']"
2535989,How to parametrically represent an ARBITRARY circle in polar coordinates?,"So, we know that any circle centered at the origin $(0,0)$ can be written in polar coordinates as: $r=f(\theta)=C \iff \langle\theta(t), r(t)\rangle=\langle t,C\rangle$ Where $C\in \mathbb{R}$ is the radius of the circle. However, let's say I move my circle to any point in the polar plane, if this was cartesian coordinates, one can simpy state that the original circle was parametrized as: $\langle x,y\rangle=\langle C\cos(t),C\sin(t)\rangle$ So shifting it is simply adding an offset vector, giving us: $\langle x,y\rangle=\langle C\cos(t)+v_x, C\sin(t)+v_y\rangle$ Is it correct to say that the parametric function: $\langle \theta,r\rangle=\langle t+v_\theta,C+v_r\rangle$ is moving the circle $v_\theta$ radians in the counter-clockwise direction and $v_r$ length units away from the origin?","['polar-coordinates', 'linear-algebra', 'vectors', 'geometry']"
2536008,"What is the ""submersion principle?"" Showing that $SL_n(R)$ is a submanifold of $GL_n(R)$.","I'm watching the following series of video lectures on Lie groups .  In the last couple of minutes of the first lecture, he states his strategy to show that $\textrm{SL}_n(\mathbb{R})$ is a Lie subgroup of $\textrm{GL}_n(\mathbb{R})$ : 1 .  Show that $\textrm{SL}_n(\mathbb{R}) = \textrm{det}^{-1}\{1\}$ is a submanifold of $\textrm{GL}_n(\mathbb{R})$ at $I_n$ , by showing that the map on tangent spaces at $I_n$ is surjective, and then using the ""submersion principle."" 2 .  Use homogeneity to show that $\textrm{SL}_n(\mathbb{R})$ is a submanifold everywhere. The second principle is clear to me, as well as the fact that the tangent space map at the identity is surjective.  But I don't understand what is the ""submersion principle"" or how it is used.  I tried googling submersion principle but nothing useful came up.  This seems to have something to do with a smooth map having constant rank in a neighborhood of a point. Edit: Not a duplicate of the previous question of why SL is a submanifold, because I am asking about a specific approach to showing it is a submanifold.","['smooth-manifolds', 'manifolds', 'submanifold', 'differential-geometry', 'lie-groups']"
2536048,Circle-Line Intersection,"Problem Statement Given radius $r$ of a circle centered at the origin and a line on which two points $(x1,y1)$ and $(x2,y2)$ lie, determine whether the line intersects the circle at any point. I'm having trouble understanding the intuition behind MathWorld's formula for this problem. For example, how is the determinant of the column matrix $D$ relevant in finding the answer?","['discriminant', 'determinant', 'geometry']"
2536068,"Calculate $\iiint_B x \text{ d}V$, where $B$ is tetrahedron with vertices $(0,0,0),(0,1,0),(0,0,1),(1,0,0)$","Calculate $\displaystyle \iiint_B x\text{ d}V$, let $dV=dx\, dy\, dz$ (in that order) where $B$ is tetrahedron with vertices $(0,0,0),(0,1,0),(0,0,1),(1,0,0)$ Here is a picture: Lets fix $z$. Then $0\leq z\leq 1$, and we want to consider the projection on the $xy$ plane. We can see that $0\leq x\leq 1-y$, but what about $y$? $y$ must be a function of $z$ and all I see is that $y$ goes from $0$ to $1$?","['multivariable-calculus', 'integration', 'volume', 'calculus']"
2536075,Factor 43361 knowing $\phi(43361)$,"It is given that the number $43361$ can be written as product of two distinct prime numbers $p_{1}$ and $p_{2}$. Further, assume that there are $42900$ numbers which are less than $43361$ and co-prime to it. Then find $p_{1}+p_{2}$. A simple search on Google yielded $p_{1}$ and $p_{2}$ to be $131$ and $331$. But what would be the proper way to find it?","['number-theory', 'prime-numbers']"
2536096,Convolution of L^p and L^q function is uniformly continuous or not?,"This is a homework question (the due date has passed) and I have been thinking of it for a while. We are asked to prove or disprove the following statement: $f \in L^p(\mathbb{R}), g \in L^p(\mathbb{R}), 1 < p, q < \infty, \frac{1}{p} + \frac{1}{q} = 1$. $f*g(x) = \int f(x-y)g(y)dy$ is uniformly continuous or not? I know the convolution of a $L^1$ and a $L^{\infty}$ function is uniformly continuous. Below is my attempt, yet I feel I probably did not use Holder's inequality correctly: Consider a fixed $x \in \mathbb{R}$ and any $a \in \mathbb{R}$, then 
$$|f*g(x-a) - f*g(x)| = | \int f(x-a-y)g(y)dy - \int f(x-y)g(y)dy| \\ 
= | \int f(x-y)g(y-a)dy - \int f(x-y)g(y)dy| \\ 
= | \int f(x-y)[g(y-a) - g(y)]dy| \\ 
\leq \int | f(x-y)[g(y-a) - g(y)] | dy \\ 
\leq ||f||_p ||h||_q
$$
where $h(y) = g(y-a) - g(y), 1 < p, q < \infty, \frac{1}{p} + \frac{1}{q} = 1$. Since the difference between $f*g(x-a)$ and $f*g(x)$ for any fixed $x$ and any $a$ is bounded, I can choose $a$ such that $||h||_q < \frac{\varepsilon}{|| f ||_p}$, so $|f*g(u) - f*g(x)| < \varepsilon$ for any arbitrary $\varepsilon > 0$ and any $u \in (x-a, x+a)$. I thought I used Holder's inequality correctly, since I was considering some fixed $x$, meaning that I can consider $f(x-y)$ as a function in $y$. Could anyone check my solution? I am really unsure about my use of Holder's inequality.","['uniform-continuity', 'real-analysis', 'lp-spaces', 'holder-inequality']"
2536130,"$\mathbb{C}[x,y]/\langle y^2-g(x) \rangle$ is integrally closed in its fraction field. [duplicate]","This question already has answers here : Why is $\mathbb{C}[x,y]/(y^2 - x^3 + 1)$ normal? (1 answer) Normalization of a quotient ring of polynomial rings (Reid, Exercise 4.6) (1 answer) Closed 6 years ago . Let $g(x)=(x-a)(x-b)(x-c)$ with $a\neq b\neq c$. Then $\mathbb{C}[x,y]/\langle y^2-g(x) \rangle$ is integrally closed in its fraction field. I manage to do with the following theorem: A smooth function $f$ is non-singular if and only if $\mathbb{C}[x,y]/\langle f(x,y) \rangle$ is integrally closed in its fraction field. But how will I do without using this? Like an example has done here .","['algebraic-geometry', 'commutative-algebra']"
2536167,Two derivatives on a ring with two metrics?,"In real analysis, the derivative can be defined by the limit formula: $$D[f](x)v := \lim_{t\to 0} \frac{f(x+tv) - f(x)}{t}$$ and we ask that as a function, $D[f] : \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ is continuous. Sometimes, one wants to do analysis over a more general commutative ring with
a metric, or even more generally, a topological ring.  This is done for example by Bertram et al in ""Differential calculus of general base fields and rings"". Here a continuous function $f : R \to R$ has a derivative when there is a 
continuous function $f^{[1]} : R \times R \times R \to R$ such that $$f(x+tv)-f(x) = t\cdot f^{[1]}(x,v,t) \qquad (1)$$ The $f^{[1]}$ can be classically (over $\mathbb{R}$) described as $$
  f^{[1]}(x,v,t) = 
    \begin{cases}
      \frac{f(x+tv) - f(x)}{t} & t \ne 0\\
      D[f](x)v & t = 0
    \end{cases}
  $$ But in general, as $R$ is not a field, an $f^{[1]}$ satisfying $(1)$ will not be defined as above. When a continuous $f$ has an associated $f^{[1]}$ that
satisfies $(1)$, and that $f^{[1]}$ is linear in $v$, we may 
define its differential as $$D[f](x)v := f^{[1]}(x,v,0)$$ Bertram makes an assumption that leads to $f^{[1]}$ being unique, and thus, is a ""Fermat ring"" (in the sense of Dubuc and Kock).  One aspect of uniqueness is that it makes the derivative well defined.  It also ensures that the collection of differentiable functions satisfies the chain rule, and other usual properties of differentiation. But I am curious -- what if one changes the metric or the topology on the ring, but leaves the operations unchanged.  Is it possible for some fixed CRing $R$ to have two metrics and where the induced derivatives are different? Link to Bertram et al: http://www.sciencedirect.com/science/article/pii/S0723086904800069 Dubuc and Kock: ""On 1-form classifiers""  E. Dubuc and A. Kock.  Communications in Algebra.  Volume 12, Issue 12, 1984. pp 1471-1531. -- Edits to clarify --","['derivatives', 'ring-theory', 'metric-spaces', 'calculus']"
2536171,"""Point-wise"" value of Bochner integral of $L^2$ functions","Let $T:L^2(\mathbb{R})\rightarrow L^2(\mathbb{R})$ be a linear operator and $\phi:[a,b]\rightarrow L^2(\mathbb{R})$ be a parameterised family of $L^2$ functions. I sometimes see an integral (which seems to be called a Bochner integral) like $$\int_a^b T\phi(t)\,dt,$$ which is supposed to give an element of $L^2(\mathbb{R})$, call it $f$. Question: How does one interpret $f$ as an actual function (up to a set of measure $0$)? For instance, it is possible to make a statement like: the point-wise value of $f$ at $x$ (up to a set of measure $0$) is the same as the value $$\int_a^b (T\phi(t))(x)\,dt\in\mathbb{R}$$ where $T\phi(t)(x)$ is the value at $x$ of some representative of $T\phi(t)\in L^2(\mathbb{R})$? I'm not sure if the last question makes sense in general, but it certainly should if $\phi(t)$ was point-wise well-defined, for example if for all $t$ we have $\phi(t)\in C_c^\infty(\mathbb{R})\subseteq L^2(\mathbb{R})$.","['real-analysis', 'functional-analysis', 'integration', 'measure-theory', 'analysis']"
2536176,A conjecture regarding prime numbers,"For $n,m  \geq 3$, define $ P_n = \{ p : p$ is a prime such that $ p\leq n$ and  $ p \nmid n \}$ . For example :
$P_3= \{ 2 \}$
$P_4= \{  3 \}$
$P_5= \{ 2, 3 \}$,
$P_6= \{  5 \}$ and so on. Claim: $P_n \neq P_m$ for $m\neq n$. While working on prime numbers I formulated this problem and it has eluded me for a while so I decided to post it here. I am not sure if this is an open problem or solved one. I couldn't find anything that looks like it.
My attempts haven't come to fruition though I have been trying to prove it for a while. If $m$ and $n$ are different primes then it's clear. If $m \geq 2n$, I think we can find a prime in between so that case is also taken care of. My opinion is that it eventually boils down to proving this statement for integers that share the same prime factors. My coding is kind of rusty so would appreciate anybody checking if there is a counterexample to this claim. Any ideas if this might be true or false? Thanks. Update: Lucia has provided a conditional proof of the claim for large integers on mathoverflow. Please see https://mathoverflow.net/questions/287011/a-conjecture-regarding-prime-numbers/287039#287039","['conjectures', 'prime-factorization', 'number-theory', 'prime-gaps', 'prime-numbers']"
2536178,Spectral decomposition of hilbert modules,"Let $A$ be a $C^*$-algebra, $E$ a Hilbert $A$-module and $T:E\rightarrow E$ a bounded self-adjoint regular operator. Is there something like a spectral theorem for Hilbert modules that gives a decomposition of $E$ in relation to the spectrum of $T$? For example, is there an analogue of the spectral theorem for compact self-adjoint operators? Edit: clarified question with Martin's comments below.","['c-star-algebras', 'operator-theory', 'functional-analysis', 'hilbert-modules', 'operator-algebras']"
2536181,"How to calculate the floor integral $\int_0^{\pi}\lfloor\pi^2\cos^3x\rfloor\sin x\,dx$?","$$\int_0^{\pi}\lfloor\pi^2\cos^3x\rfloor\sin x\,dx$$ (where $\lfloor x \rfloor $ is the floor of $x$) I thought of breaking into required bounds but its too lengthy. Moreover I had to take cube root and then $\cos$ inverse. Please give a hint.","['trigonometry', 'bounds-of-integration', 'integration', 'trigonometric-integrals', 'ceiling-and-floor-functions']"
2536197,Relation between the Curvature of a Connection and the Curvature of the Induced Connection on the Frame Bundle,"Let $\nabla$ be a connection on a vector bundle $\pi:E\to M$. The curvature of $(E, \nabla)$ is the tensor $R:\mathcal X(M)\times \mathcal X(M)\times \Gamma(M, E)\to \Gamma(M, E)$ defined as
$$R(X, Y, \sigma) = \nabla_X\nabla_Y\sigma - \nabla_Y\nabla_X\sigma - \nabla_{[X, Y]}\sigma$$ (Here $\mathcal X(M)$ is the set of all smooth vector fields on $M$ and $\Gamma(M, E)$ denotes the set of all the smooth sections of $E$). Now the connection $\nabla$ on $E$ defined a connection $1$-form $\phi$ on the frame bundle $FE\to M$. And the curvature of $\phi$ is defined as the $\mathfrak g\mathfrak l_n(\mathbf R)$-valued $2$-form $\Omega$ on $FE$ as
$$\Omega(X, Y) = d\phi(X, Y)+[\phi(X), \phi(Y)]$$
for vector fields $X$ and $Y$ on $FE$. I am wondering what is the relation (connection?) between the curvature $R$ coming from $\nabla$ and the curvature $\Omega$ coming from $\phi$. Is there a way to get $R$ from $\Omega$? Any further interesting comments relevant to the above scenario are very welcome.","['curvature', 'principal-bundles', 'differential-geometry', 'vector-bundles', 'connections']"
2536206,Definition of an integral curve,"Let $M$ be a smooth manifold of dimension $n$, and let $T(M)$ be the tangent bundle of $M$.  Let $F$ be a smooth vector field, which is to say a smooth section of the canonical map $T(M) \rightarrow M$.  If I understand the manifold structure on $T(M)$ correctly, smoothness means that for any chart $(U,\phi)$ of $M$, we can use $\phi$ to identify $T_p(M)$ with $\mathbb{R}^n$ for all $p \in U$, and under these simultaneous identifications, $F$ becomes a map $\phi(U) \rightarrow \mathbb{R}^n$ which is smooth. Let $p_0 \in M$ and $t_0 \in \mathbb{R}$.  Wikipedia defines an integral curve for the vector field $F$, passing through $p_0$ at time $t_0$, to be an open neighborhood $J$ of $t_0$, together with a smooth morphism $\alpha: J \rightarrow M$, such that $\alpha(t_0) = p_0$ and $$\alpha'(t) = F(\alpha(t))$$ for all $t \in J$.  I am confused on what this equality is saying.  First, I do not understand what $\alpha'$ means as a map from $J$ to the manifold $M$.  Maybe a chart $(U,\phi)$ containing the image of $J$ must be chosen, and then we can talk about the derivative of the composition $\phi \circ \alpha: J \rightarrow \mathbb{R}^n$.  Even so, that derivative is a priori a collection of linear maps $\mathbb{R} \rightarrow \mathbb{R}^n$, so for each $t \in J$, $\alpha'(t)$ can be thought of as a linear map $\mathbb{R} \rightarrow \mathbb{R}^n$.  On the other hand, $F(\alpha(t))$ identifies as an element of $\mathbb{R}^n$.  I don't see in what sense these things are supposed to be equal.  Are we also using the identification $\textrm{Hom}_{\mathbb{R}}(\mathbb{R},V) = V$ for any vector space $V$?","['vector-fields', 'smooth-manifolds', 'differential-geometry']"
2536208,The boundary of a manifold is a closed subset.,"We want to show that the boundary $\partial M$ of an $n-$manifold M is a closed subset of the manifold. We show that its complement $M\setminus\partial M$ is open in $M$. Indeed,  each point $x \in M\setminus\partial M$ has an open neighborhood $V_x\subseteq M$ homeomorphic to $\mathbb R^n$. It remains only to show that $V_x$ lies entirely in $M\setminus \partial M$ which means that $V_x\cap \partial M=\emptyset$. Thank you for your help! EDIT: In a manifold with boundary each point has an open neighborhood that is homeomorphic to $\mathbb R^n$ or to $\mathbb R^n_+=\{(x_1,\cdots,x_n)\in\mathbb R^n\;|\; x_n\ge 0\}$, the points who have open neighborhoods homeomorphic to $\mathbb R^n_+$ form the boundary of the manifold.","['manifolds', 'general-topology', 'manifolds-with-boundary']"
2536232,"Does $\varphi(x)\le\int_0^x\varphi(t)dt$ for all $x\in[0,\infty)$ imply $\varphi\equiv0$.","Let $\varphi$ be a nonnegative and continuous function on $[0,\infty)$ and such that $$\varphi(x)\le\int_0^x\varphi(t)dt$$ for all $x\in[0,\infty)$. Can we infer from here that $\varphi\equiv0$. By taking limit on both sides, I got $\varphi(0)\le 0$. As $\varphi$ is nonnegative, I can say $\varphi(0)=0$. But what then. I could have progressed if I could show that $\varphi(x)\le \varphi'(x)$, but it is not quite evident from the given condition.","['derivatives', 'real-analysis', 'integration', 'calculus']"
2536235,What are the branches of the square root function?,"I am studying branches of logarithm. I came to know that there are infinitely many branches of logarithm where $\log z = \log |z| + i (\arg z +2k\pi)$ , $k \in \mathbb Z$ and $z \neq 0$ . Now for each $\alpha \in [0,2\pi)$ if we restrict $\arg z$ to lie inside $(\alpha , \alpha + 2\pi)$ this will yield a branch of logarithm having branch cut $\theta = \alpha$ which is analytic in the cut plane $D_{\alpha} = \mathbb C \setminus \{z \in \mathbb C : z \leq 0 \}$ . For each such branch there exists a principal logarithmic function where $k=0$ i.e. $\log z =\log |z| + i \arg_{\alpha} z$ where $z \neq 0$ and $\arg_{\alpha}$ is the restriction of the argument function on $(\alpha,\alpha+2\pi)$ for some $\alpha \in [0,2\pi)$ . The principal branch of logarithm corresponds to $k=0$ and $\arg=\arg_{\pi}$ as the argument function which is known as principal argument function. Now my question is : ""Is the same true for square root?"" As we know that $z^{\frac {1} {2}} = \exp (\frac {1} {2} \log z)$ . As we know that logarithm has infinitely many branches, each of which is analytic in some certain cut plane. So we can say that $z^{\frac {1} {2}}$ is analytic on a certain cut plane of the corresponding logarithmic branch. But I don't know whether it is analytic on any point on the cut plane of the corresponding logarithmic branch or not!! If it is not so then clearly there are infinitely many branches of square root function. Corresponding to each branch there are two square root functions. One is $z \mapsto |z|^{\frac {1} {2}} e^{\frac {i\arg_{\alpha} z} {2}}$ and the other is $z \mapsto -|z|^{\frac {1} {2}} e^{\frac {i\arg_{\alpha} z} {2}}$ for each $\alpha \in [0,2\pi)$ . But for that I need the answer to the question whether $z^{\frac {1} {2}}$ is analytic on the points of the cut plane of the corresponding logarithmic branch or not. If the answer to that question is ""no"" then only we can extend the concept of logarithmic function to the square root function. I only know that the principal square root function is not continuous on $\mathbb C \setminus \{0 \}$ . Is it true or not? I am in a fix. Please help me. Thank you in advance.","['logarithms', 'complex-analysis', 'branch-cuts']"
2536242,A subspace of Hilbert space,"How can I  prove that a finite dimensional subspace of Hilbert space is closed?
I assume that there is a sequence in this subspace which is convergent and I want to show that this limit is actually in the same subspace any help?","['functional-analysis', 'hilbert-spaces']"
2536304,"Show a non-constant, continuous function $f:\bar{D}\rightarrow \bar{D}$ is such that $f(\partial D)=\partial D$","Suppose $\bar{D}= \{z:|z|\leq 1\}$ and assume we have a non-constant, continuous function $f:\bar{D}\rightarrow \bar{D}$, that is holomorphic on the interior of $\bar{D}$ and such that $f(\partial D)\subset \partial D$. Show that $f(\partial D)=\partial D$. Since $f$ is non-constant on $\bar{D}$, it is also non-constant on $\partial D$, because $\partial D$ is the closure of $\bar{D}$ and I think I have to use the fact that $|f|$ attains the max or min on the boundary $\partial D$. Could someone please help me to understand how to do that?","['complex-analysis', 'open-map']"
2536382,Solve $ (x^2+y^2)^3=(x^2-y^2)^2$ in rational numbers,"Everyone talks about Pythagoras equations but here is something similar: $$ (x^2+y^2)^3=(x^2-y^2)^2$$ This is the shape of a rose curve . It does have a peculiar 8-fold intersection at the origin. Two self tangencies at the origin: $y = x$ and $y = -x$. Wikipedia says this curve is genus 0, meaning there should be a ""map"" to projective space. We can find a point on the curve e.g. $(x,y)=(0,0)$ or $(1,0)$ and intersect the curve with various lines of rational slope $y=mx+b$ with $m\in \mathbb{Q}$. There could also be an integer equation by writing the equation in homogenous coordinates $[x:y:z]\in P^1(\mathbb{Q})$. $$  (x^2+y^2)^3=(x^2-y^2)^2 \, z^2$$ The $z$ coordinate looks extra, but in this part $x,y,z \in \mathbb{Z}$.","['number-theory', 'diophantine-equations', 'algebraic-geometry']"
2536386,Set of left cosets and set of right cosets of a not normal subgroup could be equal?,"Question 1: Let $H$ be a subgroup of a finite group $G$. I would like to show an example for H, which is not normal, but the partition of $G$ into the left cosets of H is equal to the partition of $G$ into the right cosets of H. Example: A definition of the Dihedral group of order 6 is $D_3=gp\{c,b\}\ ,\ c^3=b^2=(bc)^2=e$. It follows from the definition, that $D_3=\{e,c,c^2,b,bc,bc^2\}$, and $bc=c^2b\ ,\ cb=bc^2$. $H=\{e,b\}$ a subset of $D_3$.
H is not normal because $\exists c \in D_3: \  cH=\{c,bc^2\} \neq \{c,bc\}=Hc$ But the set of left cosets and the set of right cosets is the same: Set of left cosets: $L=\{gH\ |\  \forall g \in D_3\}=\Big\{\{e,b\},\{c,bc^2\},\{c^2,bc\}\Big\}$ Set of right cosets: $R=\{Hg\ |\  \forall g \in D_3\}=\Big\{\{e,b\},\{c,bc\},\{c^2,bc^2\}\Big\}$ So $R\neq L$ Refer to ""coffeemath"" 's answer a missed the calculation at first time, now i correct it. So it's not a counter example for Question 2. Question 2: Is the next statement is true? H is not normal $\ \Rightarrow \ \exists a \in G : aH\neq Hb \quad \forall b \in G $ I only see, that (from the negate of the definition of a normal subgroup) H is not normal $\ \Rightarrow \ \exists a \in G : aH\neq Ha $","['finite-groups', 'normal-subgroups', 'group-theory']"
2536387,"Show that $T(X) = (X_{(1)}, X_{(n)})$ is sufficient.","Exercise: Let $\phi$ be a positive (Borel) function on $\mathbb{R}$ such that $\int\limits_{a}^b\phi(x)dx < \infty$ for a pair $\theta = (a,b)$, with $-\infty<a<b<\infty$. Let $\Omega = \{\theta \in \mathbb{R^2}:a<b\}$. Define $$f(x|\theta) = c(\theta)\phi(x)\mathbb{1}{(a,b)},$$ with $c(\theta)$ such that $\int f(x|\theta)dx = 1$. Then $\{f(\cdot|\theta), \theta\in\Omega\}$ is called a truncation family. Suppose that $X_1,...,X_n \stackrel{iid}{\sim} f(\cdot|\theta)$. Let $X = (X_1,...,X_n)$. Show that $T(X) = (X_{(1)},X_{(n)})$ is a sufficient statistic. Question: How do I show that $T(X)$ is a sufficient statistic? I know that according to the mathematical definition $T(X)$ is sufficient if the distribution of $X$ given $T$ is known (does not depend on $\theta$). However, I think it's quite hard to find the conditional distribution and show that it's not dependent on $\theta$. I also know that $T(X)$ is sufficient if no other statistic that can be calculated from the same sample provides more information regarding $\theta$. So I need to show that $T(X)$ gives us just as much about $\theta$ as $S(X) = (X_1,...,X_n)$ does. Intuitively I feel this can be done by looking at $f(x_1|\theta)$ and $f(x_2|\theta)$ and then inspecting what happens with $\mathbb{1}_{(a,b)}(x)$, but I'm not sure how. Thanks in advance!","['statistics', 'statistical-inference', 'probability-distributions']"
2536438,What is the period of $f(2x)$ if $f(2x+6) = f(2x)$ and that of $f(x)$?,"The question is in the title. Also note that $f(x)$ is non-constant function . This is not same as other question asked from similar title. I understand that if I put $x+3$ in place of $x$ in $f(2x)$, I get the same function. So the period must be $3$.
And that the period of $f(x)$ must be $6$. But 'The book' says period of $f(2x)$ is $6$ and of $f(x)$ is $12$.
I cannot understand this.","['algebra-precalculus', 'periodic-functions', 'functional-analysis', 'functions']"
2536469,Perturbing a dynamical system,"Let
$$x'=1+y-x^2-y^2+af_1(x,y)$$
$$y'=1-x-x^2-y^2+af_2(x,y)$$
where $a>0$ is very small. What is a perturbation $af(x,y)$ that preserves periodic orbits but makes it asymptotically stable? What I've done: So we perturb the system
$$x'=1+y-x^2-y^2$$
$$y'=1-x-x^2-y^2$$
A periodic orbit of this is given by $(x,y)=(\cos t,-\sin t)$ (found this earlier), so we must have
$$-\sin t=-\sin t+af_1(\cos t,-\sin t)$$
$$-\cos t=-\cos t+af_2(\cos t,-\sin t)$$
So we want
$$f_1(\cos t,-\sin t)=f_2(\cos t,-\sin t)=0$$
However, how do I find such $f_1,f_2$ s.t. we obtain asymptotic stability? I am supposed to determine asymptotic stability by finding the Floquet exponents. Edit: Following LutzL's answer below I want to compute the Floquet multipliers. For that we have to compute the Jacobian of $F_1+\lambda(F_2-F_1)$, then plugging in our periodic solution $(\cos t,-\sin t)$ in the Jacobian $J$, we find the multiplier to be
$$m=\exp\left(\int_0^{2\pi}\text{tr }J dt\right)$$
I found
$$J_{11}=\frac{\partial x'}{\partial x}=-2x+\lambda(1-x^2-y^2)-2\lambda x(x-1)$$
$$J_{22}=\frac{\partial y'}{\partial y}=-2y+\lambda(1-x^2-y^2)-2\lambda y(y-1)$$
But plugging in our periodic solution $(\cos t,-\sin t)$ and integrating from $0$ to $2\pi$ gives multiplier 1, wherea I need $<1$ for asymptotic stability.","['dynamical-systems', 'stability-theory', 'perturbation-theory', 'stability-in-odes', 'ordinary-differential-equations']"
2536507,"Find $f:(1,\infty) \to \mathbb{R}$","Find $f:(1,\infty) \to \mathbb{R}$, where
  \begin{align*}
&f(x) \leq \frac{x-2}{\ln 2}, \: \forall x>1 \\
&f(x^3+1) \leq 3f(x+1), \: \forall x>0 \\
&f(x)+f\left(\frac{x}{x-1} \right) \geq 0, \: \forall x>1
\end{align*} All I got, which is also obvious, is the fact that $f(2)=0$. I tried to at least guess the function, without success. Also, substituting $x \to \frac{x}{x-1}$ in the last inequality makes it unchanged, thus this doesn't help either... My last thought was that the $3$ in the second inequality getting out of the function must somehow be related to $\log$s. As a side note, I found this as a real analysis/calculus problem .","['real-analysis', 'calculus', 'functions']"
2536508,Proving every compact set on $\mathbb{R}$ is measurable,"Before starting proof, my knowledge includes followings; (m represents the measurement function, $\lambda_*$ and $\lambda^*$ represent Lebesgue inner and outer measure) i) Let $U$ is an $d_E$-open set then $U$ can be written by using disjoint intervals $\{I_n\}$ which are finite or countable sets and   $U= \bigcup_n I_n$ $\Rightarrow$ $m(U)=\sum_n m(I_n)$. and ii) Let $U$ and $V$ are $d_E$-open sets on $\mathbb{R}$ $m(U)+m(V)=m(U \cup V) + m(U \cap V)$ iii) $E \subset \mathbb{R}$ is a bounded set. $E$ can be measured iff $\lambda^*(E)=\lambda_*(E)$ So I have started proof by determining my aim for measuring the compact set. So I think that because of measuring only open sets on $\mathbb{R}$ I have to write the compact set into combination of open sets then I can measure the compact set. Let K is a compact set, U is an arbitrary open set and $K \subset U$. Because of $K \subset U$ we can re-write K as $K= U \setminus (U\setminus K)$ and we have open and disjoint sets $U$ and $(U \setminus K)$  Now $m(K)=m(U \setminus(U \setminus K))=........$ which I got for now. I don't know what am I going to do because I have no information about measuring the set difference. Also I didn't use the information iii) because while determining the Lebesgue inner measure, $\lambda_*(E)=sup\{ m(K) : K \subset E, K compact\}$ I have to calculate a compact set's measure. Thanks for any help","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2536533,A complex-valued harmonic function of which the absolute value has a maximum point is constant,"Let $\Omega \subseteq \mathbb R^n$ be an open, connected domain, and let $u: \Omega \to \mathbb C$ be (complex-)harmonic on $\Omega$. Furthermore, let $|u|$ have a (global) maximum in some point $x_0 \in \Omega$, i.e. we have $u(x_0) = \sup_\Omega u$. I now want to show that $u$ is constant on all of $\Omega$. Now my approach was to somehow apply the (strong) maximum principle which tells me that any subharmonic function $\Omega \to \mathbb R$ on a connected open set $\Omega \subseteq \mathbb R^n$ that has a maximum within $\Omega$ is constant. My problem is that in the above preliminaries, I have a harmonic function $\Omega \to \mathbb C$, so I think I have to apply the maximum principle for its absolute value $|u|: \Omega \to \mathbb R$ instead, but do I know in this given context that $|u|$ is subharmonic,  and if so, how could I see/deduce that?","['complex-analysis', 'harmonic-functions', 'partial-differential-equations']"
2536550,Finding function $f(x)$ which satisfy given functional equation,"Find all function $f:\mathbb{R}-\{0,1\}$ in $$f(x)+2f\left(\frac{1}{x}\right)+3f\left(\frac{x}{x-1}\right)=x$$ Attempt: put $\displaystyle x = \frac{1}{x}$, then $$f\left(\frac{1}{x}\right)+2f(x)+3f\left(\frac{1}{1-x}\right) = \frac{1}{x}$$ could some help me how to solve it , thanks",['functions']
2536598,Computing the gradient of scalar wrt a vector,"Let 
$$
\alpha = x^TAx \enspace x\in \mathbf{R}^{nx1}, A \in \mathbf{R}^{nxn}
$$ How do I compute the derivative $\Large \frac{\partial\alpha}{\partial x}$
without using the coordinate way i.e, writing A in terms of $\large A = (a_{ij})$ My attempt is as follows (using the product rule): $$
\begin{align}
\frac{\partial\alpha}{\partial x} = &\frac{\partial(x^TAx)}{\partial x}\\
=&\frac{\partial(x^TA)x}{\partial x} + x^TA\frac{\partial x}{\partial x}\\
=&\frac{\partial(A^Tx)^Tx}{\partial x} + x^TAI\\
=&\left(\frac{\partial A^Tx}{\partial x}\right)^Tx+ x^TA\\
=&Ax+x^TA
\end{align}
$$ This is different from the answer that I find in wikipedia which says is to be: $$
\frac{\partial\alpha}{\partial x} = x^TA+x^TA^T
$$ Where am I going wrong, please present your answer in the product rule form.","['matrices', 'matrix-calculus', 'derivatives']"
2536667,Given that $f'(x)g(x) = g'(x)f(x)$ show that g has a root,"I'm given a question which reads: ""suppose $f(x)g'(x) = f'(x)g(x)$ for all $x \in (a,b)$. Let $r_{1}, r_{2} \in (a,b)$ where $r_{1} < r_{2}$ be two consecutive roots of $f$. Also, $f(x) \ne 0$ for any $x \in (r1, r2)$. Furthermore assume that $g(r_{1}) \ne 0$ and $g(r_{2}) \ne 0$. Show that g must have a root in $(r_{1}, r_{2})$. My attempt: We know that $f(r_{1}) = f(r_{2}) = 0$. So $0 = g(r_{1})f'(r_{1})$. Since $g(r_{1}) \ne 0$, it follows that $f'(r_{1}) = 0$. A similar argument can be made for $r_{2}$. Now, since $f(r_{1}) = 0$ and $f(r_{2}) = 0$, then there must be a $x_{1} \in(r_{1}, r_{2})$ such that $f'(x_{1}) = 0$. So, $f(x_{1})g'(x_{1}) = g(x_{1})f'(x_{1}) \to f(x_{1})g'(x_{1}) = 0 \to g'(x_{1}) = 0$, which means that g has an extremeum at that point. That's about as far as I can get before getting stuck.","['derivatives', 'proof-writing']"
2536681,Calculate the triangle index of the parent triangle.,"I am not a mathematician so I try to explain the topic with an image.
Given is a subdivided triangle. I count the smallest triangles using an index starting at 1. I need a formula that calculates the index of the parent triangle.$$pindex = f(index)$$ $$f(1) = 1$$
$$f(2) = 1$$
$$f(3) = 1$$
$$f(4) = 1$$
$$f(5) = 2$$
$$f(6) = 3$$
This leads to the following integer sequence:
$$1,1,1,1,2,3,3,3,4,2,2,2,3,4,4,4,5,6,6,6,..$$ I did not find a formula at OEIS for this sequence. I am also curious about how to improve my question that it is more clear. Update Let's call the horizontal alignment of triangles a row . I can calculate the row index by the triangle index using this formula A000196 :
$$r(i) = round(1 + 0.5 * (-3 + sqrt(i) + sqrt(1 + i)))$$
Let's call the offset of a triangle in a row the row offset . I can calculate this offset by the triangle index using this formula A071797 :
$$o(i) =i - (floor(sqrt(i))^2$$ I have got the row and the offset in this row for triangle indices. I think I am close to a solution with this. Any ideas?",['geometry']
2536687,Does a relation on a set that partitions it an equivalence relation?,"I know that if a relation is an equivalence relation on a set, it partitions the set. However, I do not know whether the inverse is also true. Namely, if a relation partitions a set, is it an equivalence relation on the set?","['equivalence-relations', 'elementary-set-theory']"
2536724,"I know that any two left/right cosets are either equal or disjoint, but what about the sets of cosets?","I know that the set of left cosets $G/H$ and the set of right cosets $H\backslash G$ contain elements (cosets) which are either equal or disjoint between themselves. Can the same be said for these two sets? If $H \trianglelefteq G$, I know that they are equal. If $H \ntrianglelefteq G$, are they disjoint?",['group-theory']
2536785,What do I do wrong solving this differential equation?,"$$x^2 y''-2xy'+2y=2+x$$
With $$t=\ln(x), x=et$$ I have $$y_H= c_1e^t +c_2e^{2t}$$ . Going on: $y_p$ should be rather easy to calculate, yet I fail to do it correctly. I'm calculating the determinant of the following matrices
 $$W=\begin{bmatrix} e^t & e^{2t} \\
e^t & 2e^{2t}\end{bmatrix} $$ $$ W_1=\begin{bmatrix} 0 & e2t \\
2+et & 2e2t\end{bmatrix}$$
$$ W_2=\begin{bmatrix} e^t & 0\\
e^t & 2+e^t\end{bmatrix} $$
$$y_p=u_1e^t +u_2e^{2t}$$ , where $$u_i=\frac{|W_i|}{|W|}$$
I integrate them, substitute it and get the wrong answer (should be -$x\ln(x)+1$, but I get $ -x\ln(x)+x+3$). Can you please help me figure it out, how to do it correctly? Thanks!",['ordinary-differential-equations']
2536827,Question arising from combinatorial proof of Fermat's little theorem,"The proof in question can be found here: Combinatorial Proof Of A Number Theory Theorem--Confusion In cases where we attempt the process described in this proof with some non-prime $p$ for the necklace length, the proof fails to work because some cyclic permutations of a necklace of composite length may be identical to each other. For example, with $p=6$, and with the colors $B$ and $W$, we have the necklace: $BWBBWB$, which does not correspond to six distinct permutations, but only to three. Thus, we do not end up showing that $6|(2^6-2)$, and indeed it does not. Great. In a discussion at a related question ( Why need prime number in Fermat's Little Theorem ), a question arose. In the non-prime case, is there a nice way to quantify the extent to which the proof breaks down? If $p$ is composite, can we write down a nice formula that counts the number of necklaces that appear fewer than $p$ times, and thus write $a^p-a$ as the sum of a multiple of $p$ and some other terms that depend on $p$ and $a$? Does the resulting error term reveal some number theoretic property of $p$?","['combinatorics', 'elementary-number-theory']"
2536870,Is $ \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} a_n x^n$ a norm on the space of sequences $\mathbb{R}^\mathbb{Z}$?,"The set of sequences of integers real numbers $\{ a_n \}_{n \in \mathbb{N}}$ with $a_n \in \mathbb{R}$ is not a Hilbert space but it is a Banach space vector space.  However, it can be a Hilbert space if I give it a norm such as : $$ \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} |a_n|^2 $$ and require that the norm be less than infinity.  What happens if I use a different equation. $$ \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} a_n x^n$$ Could this constitute a norm on the Banach space of sequence of integers (I forget the name in the textbook).  Maybe we need to say $0 < x < 1$ and / or put absolute value signs. ""Norm"" on an infinite dimensional space means we have to strict to convergent subsequences (or we could accept the case $||v|| = \infty$ as an outcome.  The vector $\vec{1} = (1,1,1,\dots)$ is has infinite norm in the first case. In terms of sequences spaces the norm I have written is $\ell^2(\mathbb{N})$.  Certainly there is an $\ell^1(\mathbb{N})$, even though it can diverge for many sequences. $$ \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} |a_n| $$ Then I am asking about a weighted version of $\ell^1$.  with $0 < x < 1$.  Is that still a norm? $$\ell^1(\mathbb{N}) \subseteq\left\{  \{ a_n \}_{n \in \mathbb{N}} :  \sum_{n \in N} |a_n| x^n < \infty \right\} $$ I believe the left side is strictly smaller than the right side.  There could also be an analogue of $\ell^2$: $$\ell^2(\mathbb{N}) \subseteq\left\{  \{ a_n \}_{n \in \mathbb{N}} :  \sum_{n \in N} |a_n|^2 x^n < \infty \right\} $$","['banach-spaces', 'hilbert-spaces', 'normed-spaces', 'functional-analysis', 'sequences-and-series']"
2536889,"What does ""If three real numbers $a, x, y$ satisfy the inequalities $a≤x≤a+\frac{y}{n}$ for every integer $n≥1$, then $x=a$"" mean?","If three real numbers $a, x, y$ satisfy the inequalities $a≤x≤a+\frac{y}{n}$ for every integer $n≥1$, then $x=a$ I saw this theorem in Apostol's Calculus, and even though I understand the proof, I quite don't get what it means, why it's a consequence of the Archimedean property of real numbers (even though I know it's used to prove it), and why it's important to Calculus, as he states. I suppose it has something to do with the inexistence of infinitesimal and infinite numbers that the Archimedean property states, but I still quite don't get it. Thanks in advance.","['inequality', 'real-numbers', 'calculus', 'algebra-precalculus', 'integers']"
2536892,Restriction of a morphism of varieties,"For me, an affine variety is an irreducible closed subset of some $A_k^n$. A quasi-affine variety is a non-empty open subset of an affine variety. A projective variety is an irreducible closed subset of some $P_K^n$. A quasi-projective variety is a non-empty open subset of a projective variety. A variety is everyone of the above: an affine variety, or a quasi-affine variety, a projective variety, or a quasi projective variety. If $X\subseteq A_K^n$ is an affine variety, then the quotient $A(X)=K[x_1,\dots,x_n]/I(X)$ is the affine coordinate ring of $X$ , where $I(X)$ is the ideal of the polynomial that vanish on $X$. Let $U$ be a non empty open subset of $X$, let $f\colon U\to K$ be a function, where $K$ is an algebraically closed field. I say that $f$ is regular in $a\in U$ if there exists an open subset $V$ of $U$ such that $a\in V$ and exist $h,k \in A(X)$ such that $f_{|V}=\frac{h}{k}_{|V}$ and $k(y)\neq 0$ for all $y\in V$. I say that $f$ is regular in $U$ if $f$ is regular in all point of $U$. Then, $O_X(U)$ is the ring ($K$-algebra) of the regular functions in $U$. If $X\subseteq P_K^n$ is a projective variety, then the quotient $R(X)=K[x_0,\dots,x_n]/I(X)$ is the projective coordinate ring of $X$ , where $I(X)$ is the ideal generated by the homogeneous polynomial that vanish on $X$. Let $U$ be a non empty open subset of $X$, let $f\colon U\to K$ be a function, where $K$ is an algebraically closed field. I say that $f$ is regular in $a\in U$ if there exists an open subset $V$ of $U$ such that $a\in V$ and exist $h,k \in R(X)$, homogeneous of the same degree, such that $f_{|V}=\frac{h}{k}_{|V}$ and $k(y)\neq 0$ for all $y\in V$. I say that $f$ is regular in $U$ if $f$ is regular in all point of $U$. Then, $O_X(U)$ is the ring ($K$-algebra) of the regular functions in $U$. [Note that $R(X)$ is also a graduated ring because $K[x_0,\dots,x_n]$ is a graduated ring and $I(X)$ is an homogeneous ideal] If $Y$ is a quasi affine variety and $X$ is the affine variety of which $Y$ is an open subset, and $U$ is an open subset of $Y$, then $U$ is open in $X$ and I define $O_Y(U):=O_X(U)$. (I think this is the case, but my book doesn't say it explicitly, so I am not sure). If $Y$ is a quasi projective variety and $X$ is the projective variety of which $Y$ is an open subset, and $U$ is an open subset of $Y$, then $U$ is open in $X$ and I define $O_Y(U):=O_X(U)$. (I think this is the case, but my book doesn't say it explicitly, so I am not sure). Finally, if $X$ and $Y$ are varieties , $f\colon X\to Y$ is a morphism of varieties if $f$ is continuous and for every open subset $V$ of $Y$, $$f^*\colon O_Y(V)\to O_X(f^{-1}(V))$$ is well- defined, i.e. $g\circ f$ is in  $O_X(f^{-1}(V))$ for every $g$ in $O_Y(V)$. My question is: Let $f\colon X\to Y$ be a morphism between the varieties $X$ and $Y$. Let $Z:= \overline{f(X)}$ be the closure of $f(X)$ in $Y$. Then, why $f\colon X\to Z$ is  morphism according to my definitions above? I know $f\colon X\to Z$ is continuous. Then, i have to show that $Z$ is also a variety. I have distinguished two cases (i know that the image of an irreducible space by a continuous function is also irreducible, and that the closure of an irreducible subset is also irreducible): If $Y$ is affine variety, then $Z$ is an affine variety. If $Z$ is projective variety, than $Z$ is also projective vaeriety If $Y$ is quasi affine, then let $W$ be the affine variety of wich $Y$ is an open subset. Then the closure of $Z$ in $W$ is affine variety, and $Z$ is open in the closure of $Z$ in $W$, so $Z$ is quasi affine. The same if $Y$ is quasi projective. My problem is to show the condition for every open subset $V$ of $Z$, $$f^*\colon O_Z(V)\to O_X(f^{-1}(V))$$ is well- defined, i.e. $g\circ f$ is in  $O_X(f^{-1}(V))$ for every $g$ in $O_Z(V)$, knowing that this is true for the open subset of $Y$, i.e. for every open subset $V$ of $Y$, $$f^*\colon O_Y(V)\to O_X(f^{-1}(V))$$ is well- defined, i.e. $g\circ f$ is in  $O_X(f^{-1}(V))$ for every $g$ in $O_Y(V)$. So, what am I missing? I'm struggling on this. EDIT 26/11/2017 My strategy was: take an open subset $V$ of $Z$ and a regular function $g\colon V\to K \in O_Z(V)$. Then, $V=Z\cap V'$ where $V'$ is an open subset of $Y$. I want to find a regular function $g':V' \to K\in O_Y(V')$ such that $g'_{|f(f^{-1}(V))}=g$. If I do that, then $g'\circ f$ is in $O_X(f^{-1}(V'))$, so $(g'\circ f)_{|f^{-1}(V)}$ is in $O_X(f^{-1}(V))$, so $g\circ f=(g'\circ f)_{|f^{-1}(V)}$ is in $O_X(f^{-1}(V))$ Q.E.D. So the question is : how can I find a regular function $g':V' \to K\in O_Y(V')$ such that $g'_{|f(f^{-1}(V))}=g$? EDIT 26/11/2017, n°2 Another idea was to try with a ""local approach"". Take an open subset $V$ of $Z$ and a regular function $g\colon V\to K \in O_Z(V)$. I want to show that $(g\circ f)\colon f^{-1}(V)\to K$ is in $O_X(f^{-1}(V))$. Let $y$ be in $f^{-1}(V)$. I want to show that $g\circ f$ is a quotient of polynomials (or a quotient of homogeneous polynomials of the same degree if $X$ is a (quasi) projective variety) in an open neighborhood of $y$. So $f(y)\in V$. But $g$ is regular in $f(y)$, so there exists an open subset $W$ of $V$ with $f(y)\in W$, and exist $h,k$ ""polynomials"" such that $k\ne 0 $ in $W$ and $$g_{|W}=\frac{h}{k}_{|W}$$
But then $f^{-1}(W)$ is open in $X$ and $y \in f^{-1}(W)$ and we have that $$(g\circ f)_{|f^{-1}(W)}=\frac{h\circ f}{k \circ f}_{|f^{-1}(W)}$$ The problem is that now I don't know if $h,k$ are regular functions on $Y$, i.e. elements of $O_Y(Y)$ (for example if $Y$ is a projective variety, than $h$ and $k$ are  homogeneous polynomials of the same degree, but they don't define regular functions, not even functions). In other words , I want to say that $h\circ f$ and $k \circ f$ are regular functions on $X$ (or, on an open subset $U$ of $X$ containing $y$), such that they are both quotient of polynomials and so $g\circ f$ is quotient of polynomials in an open neighborhood of $y$, but I don't know how to say that .","['abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
2536952,Sumset that covers $\mathbb{Z}/p\mathbb{Z}$.,"Let $p$ be a prime. Let $S$ be a set of residues modulo $p$. Define 
$$S^2 = \{a \cdot b \mid a \in S, b \in S\}.$$ Question: How small can we make $|S|$ such that $\{0, 1, \cdots, p-2, p-1\} \in S^2$ ? It seems that the optimal bound should be around $\sqrt{2p}$. This can either be seen from a probabilistic argument or by recognizing that if $|S| = k$ then $S^2$ can cover $\dbinom{k}2$ elements. I am able to get a bound of $ \sim 2 \sqrt{p}$ as follows. Let $g$ be a generator of $(\mathbb{Z}/p\mathbb{Z})^{\times}.$ Then we can have the following powers of $g$ in $S:$ $$S = \{ g, g^2, g^3, \cdots, g^{\sqrt{p}}, g^{2 \sqrt{p}}, g^{3 \sqrt{p}}, \cdots, g^{(\sqrt{p}-1) \sqrt{p}} \}.$$ Can we do better ?","['number-theory', 'combinatorics', 'prime-numbers', 'additive-combinatorics']"
2536965,Compute a limit without L'Hopital's rule $\lim_{x \to a} \frac{a^x-x^a}{x-a}$ [duplicate],"This question already has answers here : Finding limits $l=\lim_{x \rightarrow a}\frac{x^x-a^x}{x-a} $ & $m= \lim_{x \rightarrow a}\frac{a^x-x^a}{x-a} $. (3 answers) Closed 4 years ago . I would want to know how we can compute the following limit by using only fundamental limits. $$\lim_{x \to a} \dfrac{a^x-x^a}{x-a},$$ where $a$ is a positive real number. My idea was to use a substitution: $y=x-a$. We get
$$\lim\limits_{y \to 0} \dfrac{a^aa^y-(y+a)^a}{y}
=a^a\left[ \lim\limits_{y \to 0} \dfrac{a^y-1+1-(\frac{y+a}{a})^a}{y} \right]
=a^a\left[ \ln a+\lim\limits_{y \to 0}\frac{1-(\frac{y+a}{a})^a}{y} \right].
$$ I am looking forward to read any tips on how I can continue from this point. Any help is appreciated.","['real-analysis', 'limits-without-lhopital', 'calculus', 'limits']"
2537002,Number which is simultaneously sum of 2 and 3 squares [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Is there positive integer $m$ such $m=x_1^2+x_2^2$ and $m=y_1^2+y_2^2+y_3^2$ where $x_i, y_j$ are nonzero integers. I have tried by hand for the ten natural numbers but I was not able to find such $m$. Would be thankful for help.",['number-theory']
2537011,Is it possible to form a square using sheets of $A4$ sized paper (without them overlapping)?,Any sheet of A series paper has an irrational number as its aspect ratio; $\sqrt2$. My intuition tells me that there is no way to combine these sheets of paper into a square without them overlapping — but I can't find any way on how I would go about proving this. Anybody got any idea?,"['analysis', 'geometry']"
2537092,Determining all real numbers $a$ for which the limit $\lim\limits_{n \to \infty} a^nA^n$ exists and is non-zero for a $3 \times 3$ matrix $A$.,"Q. Let $A$ be a $3 \times 3$ matrix $$A=
    \left(\begin{matrix}
    1 & -1 & 0 \\
    -1 & 2 &-1 \\
    0 & -1 & 1 \\
    \end{matrix}\right)
$$ Determine all real numbers $a$ for which the limit  $\lim\limits_{n \to \infty} a^nA^n$ exists and is non-zero.[For a sequence of $3 \times 3$ matrices $\{B_n\}$ and a $3 \times 3$ matrix $B$, $\lim\limits_{n \to \infty} B_n = B$ means that, for all vectors $x \in \Bbb R^3$, we have $\lim\limits_{n \to \infty} B_nx=Bx$ in $\Bbb R^3$.] My approach : I found eigenvalues of $A$ to be $0,1,3$ with eigenvectors $(1,1,1)$,$(1,0,-1)$ and $(1,-2,1)$ respectively. Let $x \in \Bbb R^3$, then $x=b(1,1,1)+c(1,0,-1)+d(1,-2,1)$ for some unique scalars $b,c,d$ since the eigenvectors form a basis of $\Bbb R^3$. Thus, $\lim\limits_{n \to \infty} a^n A^n x=\lim\limits_{n \to \infty} a^n A^n[b(1,1,1)+c(1,0,-1)+d(1,-2,1)]=\lim\limits_{n \to \infty} [a^n\;b\; A^n(1,1,1)+a^n\;c\;A^n(1,0,-1)+a^n\;d\;A^n(1,-2,1)]=\lim\limits_{n \to \infty} [a^n\;b\; 0^n(1,1,1)+a^n\;c\;1^n(1,0,-1)+a^n\;d\;3^n(1,-2,1)]=(0,0,0)+c \lim\limits_{n \to \infty} a^n(1,0,-1)+d\lim\limits_{n \to \infty} (3a)^n(1,-2,1)$ which exists. Since $\lim\limits_{n \to \infty} a^n A^n$ exists, so from second term of the last expression, $|a| \le 1$ and from third term of the last expression $|3a| \ \le 1 \Rightarrow |a| \le \frac 13$. This implies that $|a| \le \frac 13$. But if $|a| \lt \frac 13$, then $\lim\limits_{n \to \infty} a^n A^nx=(0,0,0) \; \forall \; x \in \Bbb R^3.$ $\therefore a=\frac 13$. Is this approach correct? Particularly I am concerned with the basis I used in the beginning. Since everything in the question is in the standard basis, did I do right by writing $x$ in the eigenbasis? Another approach could be finding modal matrix $P$ and using $A^n=PD^nP^{-1}$ where $D$ is the diagonal matrix $
    \left(\begin{matrix}
    0 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 3 \\
    \end{matrix}\right)
$ which also yields the same answer $a=\frac 13$.","['limits', 'matrices', 'proof-verification', 'contest-math', 'linear-algebra']"
2537102,Expectation of indicator variable squared,"Let $X$ be an indicator random variable with $P(X=1) = p$. My understanding is that $E(X) = p$, but why is it true that $E(X^2)=p$ as well?","['probability', 'random-variables']"
2537143,A density proof of the Dominated Convergence theorem,"Consider the following corollary of the DCT, sometimes called the Bounded Convergence theorem: If $f_n$ is a uniformly bounded sequence of functions converging pointwise a.e. to $f$ on a finite measure space, then $f_n$ converges to $f$ in $L^1$. If we assume that theorem, can we prove the full DCT like this? Theorem. Let $f_n\to f$ pointwise a.e. on a measure space $(\Omega, \mu)$, and suppose $|f_n|\leq F$ a.e. for all $n$, where $F$ is some integrable positive function. Then $f_n\to f$ in $L^1$. Proof. Since $F$ is positive and integrable, we may use it as a density to obtain a finite measure $\mu_F$. The functions $g_n:=\frac {f_n} F$ are well defined $\mu_F$ a.e. and are uniformly bounded by $1$. Therefore by the Bounded Convergence theorem, $g_n\to g:=\frac f F$ in $L^1(\mu_F)$. But $\int |g_n-g| d\mu_F=\int |f_n-f|d\mu$, thus $f_n\to f$ in $L^1(\mu)$. $\blacksquare$","['real-analysis', 'measure-theory', 'proof-verification']"
2537170,Showing the Clayton Copula is $2-$increasing,"This is the definition of a bivariate ($2$-dimensional) copula: $C(\mathbf{u}):[0,1]^2 \mapsto [0,1]$ is a bivariate copula if $C(u_{1},0) = 0$ and $C(0,u_{2})=0$; i.e., $C = 0$ if one argument is $0$. $C(u_{1},1) = u_{1}$ and $C(1,u_{2}) = u_{2}$; i.e., the copula reduces to $u_{i}$ if all arguments are $1$ except the $i$th one. $C(\mathbf{u})$ is $2$-increasing - i.e., for each hyperrectangle $B = \prod_{i=1}^{k}[x_{i},y_{i}]$ in $[0,1]^{2}$, the $C$-volume:
  $$ \int_{B}dC = \sum_{\mathbf{z} \in \{x_{1},y_{1}\}\times\{x_{2},y_{2}\}} (-1)^{N(\mathbf{z})} C(\mathbf{z}) \geq 0 $$
  where $N(\mathbf{z}) = \text{the number of}\,z_{i}=x_{i}$ for $\mathbf{z} \in \{x_{1},y_{1}\}\times \{x_{2},y_{2}\}$ I need to prove that the Clayton Copula, $C(u,v) = \left[\max\{u^{-\theta} + v^{-\theta}-1,0 \}\right]^{-1/\theta}$ for $u,v \in (0,1)$ and $\theta > 0$, is a bonafide bivariate copula. So, far, the only part I am still having trouble with is showing property #3 - namely, that $C$ is what is known as $2-$increasing. For #3 , I have that $\displaystyle \int_{B}dC = C(x_{2},y_{2})-C(x_{2},y_{1})-C(x_{1},y_{2})+C(x_{1},y_{1})$, which after many, many steps of algebra, I got to look like $$ = \frac{x_{2}y_{2}}{\left(y_{2}^{\theta}+x_{2}^{\theta}-x_{2}^{\theta}y_{2}^{\theta} \right)^{1/\theta}} - \frac{x_{2}y_{1}}{\left( y_{1}^{\theta}+x_{2}^{\theta}-x_{2}^{\theta}y_{1}^{\theta} \right)^{1/\theta}} - \frac{x_{1}y_{2}}{\left(y_{2}^{\theta}+x_{1}^{\theta}-x_{1}^{\theta}y_{2}^{\theta} \right)^{1/\theta}}  + \frac{x_{1}y_{1}}{\left( y_{1}^{\theta}+x_{1}^{\theta}-x_{1}^{\theta}y_{1}^{\theta} \right)^{1/\theta}} $$ But, how do I show that this must be $\geq 0$? I thank you ahead of time for your help!","['probability-theory', 'probability', 'statistics']"
2537203,What is the third (matroid) flat axiom?,"Given: a finite set $E$ and a set family $\mathscr{F} \subseteq 2^E = \mathscr{P}(E)$ . Every source known to me gives these as the first two  axioms for $\mathscr{F}$ to be a family of matroid flats: $E \in \mathscr{F}$ . $F_1, F_2 \in \mathscr{F} \implies F_1 \cap F_2 \in \mathscr{F}$ . Question: (a) What is the correct form for the third axiom? Can it be stated precisely? (b) What is the conclusion of the third axiom when applied to $E \in \mathscr{F}$ ? E.g. how should one understand ""the empty set partitions the empty set""? Isn't the empty union equal to $\emptyset$ , not to $E$ ? And if the third axiom makes no sense when applied to $E$ , and $E \in \mathscr{F}$ is only included so that $\mathscr{F}$ isn't empty, then why not say $E \not\in \mathscr{F}, \mathscr{F} \not= \emptyset \subseteq 2^E$ ? Wikipedia says: 3. If $F \in \mathscr{F}$ , then the set of all $G \in \mathscr{F}$ which cover $F$ (meaning that $G \supsetneq F$ but there is no $H \in \mathscr{F}$ between $F$ and $G$ ) partition the elements of $E \setminus F$ . Encyclopedia of Mathematics gives two supposedly equivalent forms: 3.(a) If $G_1, \dots, G_k \in \mathscr{F}$ is the family of flats which cover $F \in \mathscr{F}$ (i.e. each $G_i$ contains $F$ properly with no $H \in \mathscr{F}$ in between), then $G_1 \setminus F, \dots, G_k \setminus F$ partition $E \setminus F$ . 3.(b) For all $F \in \mathscr{F}$ , $\bigcup \{ F' \in \mathscr{F}: F' \succ F  \} = E$ . Oxley says: If $F \in \mathscr{F}$ and $\{ G_1, G_2, \dots, G_k\}$ is the set of minimal members of $\mathscr{F}$ that properly contain $F$ , then the sets $G_1 \setminus F, G_2 \setminus F, \dots, G_k \setminus F$ partition $E \setminus F$ . These notes parrot this. Gordon, McNulty say the same thing as the first axiom given by the Encyclopedia of Mathematics, except that they don't even bother to try to explain what ""cover"" means. Attempt: Are either of these definitions of the ""flat cover"" correct? (a) $cover: \mathscr{F} \to 2^{2^E}$ , $cover: F \mapsto \{G \in \mathscr{F}: G \supsetneq F, \forall H \in \mathscr{F}, F \subseteq H \subseteq G \implies H = F \lor H = G  \}$ (b) $cover: \mathscr{F} \to 2^{2^E}$ , $cover: F \mapsto \{G \in \mathscr{F}: G \supsetneq F, \forall X \in 2^E, F \subsetneq X \subsetneq G \implies X \not\in \mathscr{F} \}$ EDIT: What I had here before was totally wrong (among other things, it should be $G \setminus F$ , not $E \setminus G$ ). Also the definitions of cover incorrectly said $G \supseteq F$ instead of $G \supsetneq F$ . Then define $[cover(F) \setminus F] \in 2^{2^E}$ by $[cover(F) \setminus F] := \{ G \setminus F: G \in cover(F) \}$ . My understanding of ""partition"" is the following: $[cover(F) \setminus F]$ partitions $E \setminus F$ $\iff$ $\bigsqcup\limits_{G \in cover(F)} G \setminus F = E \setminus F$ , where $\bigsqcup$ denotes disjoint union. EDIT: What follows is also totally incorrect and based on a false premise. We're interested in $G \setminus E$ (which does not exist because $cover(E) = \emptyset$ ), not in $E \setminus E$ , which does exist but is the empty set. I.e. using the right definition $cover(F) \setminus F$ it is an empty union, using the wrong definition $E \setminus cover(F)$ it is a non-empty union of empty sets. When $F = E$ , we should have that $E \setminus F = \emptyset$ , so $$ \bigsqcup\limits_{X \in cover(F)} E \setminus X = \emptyset \,,$$ thus this is supposed to be either an empty union, or a non-empty union of empty set(s). If it is supposed to be an empty union, then we should have that $cover(E) = \emptyset$ . If it is supposed to be a non-empty union of empty set(s), we should have that $cover(E) = \{ E \}$ . In particular, should we have that $cover(E) = \{ E \}$ or $cover(E) = \emptyset$ ? Using definition (a), $cover(E) = \{E\}$ , (b) is supposed to be equivalent to (a), so to get $cover(E) = \emptyset$ one needs to change $H = F \lor H = G$ in the definition of (a) to $H = G$ or something else entirely.","['matroids', 'axioms', 'elementary-set-theory', 'discrete-mathematics']"
2537224,Prove $(\oplus_{n=1}^{\infty}\ell_1^n)_{\ell_2}$ cannot admit a norm which is uniformly convex.,"Just as the title, prove $(\oplus_{n=1}^{\infty}\ell_1^n)_{\ell_2}$ cannot admit a norm which is uniformly convex. I find in the answer of A reflexive space which does not have an equivalent uniformly convex norm , a user noticed using Enflo's theorem, but he did not mention what this theorem says, and I cannot find a version of this theorem on the internet.","['functional-analysis', 'convex-geometry', 'real-analysis', 'banach-spaces']"
2537234,Prove that the Extension Theorem is Complete.,"Prove that the probability triple $(\Omega, \mathcal{M}, \mathbb{P^*}) $ constructed from the Extension Theorem is complete . Specifically, this problem asks us to show that, given any $A \subseteq \Omega$, such that $A \in \mathcal{M}$ and $ \mathbb{P^*} (A)=0$, and any $B \subseteq A$, then $$ \mathbb{P^*}(B \cap E) + \mathbb{P^*}(B^c \cap E) = \mathbb{P^*}(E).$$ The proof of this theorem posted below seemed fairly straightforward, but there are two issues (one of which has been previously asked by another user, without answer), which make me doubt it. The text I am reading writes that, from this result , it follows $B$ has outer measure $0$, i.e.:
$$B \in \mathcal{M} \implies \mathbb{P^*}(B)=0,$$ but doesn’t that result follow regardless of whether $B$ is in $\mathcal{M}$ or not? The other issue is that the following proof does not seem to make use of the fact that $$A \in \mathcal{M}.$$ In fact, it seems we could rewrite the definition of a complete probability triple as follows: Definition: If $A \subseteq \Omega$, such that $$\mathbb{P^*}(A)=0,$$ then $$S(A) \in \mathcal{M},$$ where $S(A)$ is the set of all subsets of $A$. Proof: By the monitonicity of outer measure, for any $E \subseteq \Omega$, $$\mathbb{P^*}(B \cap E) \leq \mathbb{P^*}(A \cap E).$$ Further, since $(A \cap E) \subseteq A$, $$\mathbb{P^*}(B \cap E)=0. \qquad (1)$$ Clearly, $(B^c \cap E) \subseteq E$, which implies $$\mathbb{P^*}(B^c \cap E) \leq \mathbb{P^*}(E).\qquad (2)$$ Putting $(1)$ and $(2)$ together $$\mathbb{P^*}(B \cap E) + \mathbb{P^*}(B^c \cap E) \leq \mathbb{P^*}(E).$$ Since outer measure is also subadditive: $$ \mathbb{P^*}(E) \leq  
\mathbb{P^*}(B \cap E) + \mathbb{P^*}(B^c \cap E).$$ Thus $\mathbb{P^*}$ is additive on the union $B \cap E$ and $B^c \cap E$, for all subsets $E$ of $\Omega$, from which we conclude $B \in \mathcal{M}$. $\square$","['probability-theory', 'measure-theory', 'proof-verification']"
2537262,How to integrate a total derivative?,"Suppose $f(x,y)=x y$ Then, its total derivative is
$$
\begin{align}
\mathbb{d}f&=x \mathbb{d}y+y \mathbb{d}x \\
\int\mathbb{d}f&= \int x \mathbb{d}y+ \int y \mathbb{d}x \tag{Integration}\\
f&=xy+yx +c\\
f&=2xy + c \\
f&=2xy \tag{suppose $c=0$} \\
\end{align}
$$ Why are we not getting back the same value of $f$?","['derivatives', 'integration']"
2537277,"How to find the center of a circle from a point on it, and a tangent line.","I am clearly not typing this in to search engines correctly, if it's possible. The teacher likes to ask trick questions, so it might actually not be possible. I can't see anything like this in my notes, nor does anything come up when I Google. When I turned in the work, I assumed the point on the circle was opposite to the tangent line. I was told this wasn't a valid assumption to make. I'm also not getting any answers from him. I can't think of any way to solve this, so I turn to you guys. Consider a circle $C$ that is tangent to $3x+4y-12=0$ at $(0,3)$ and contains $(2,-1)$. 
      Set up equations that would determine the center $(h,k)$ and radius $r$ from the circle. 
      DO NOT SOLVE THESE EQUATIONS.","['euclidean-geometry', 'linear-algebra', 'analytic-geometry']"
2537350,How can group theory explain movement on a hexagonal tiling?,"(As a prelude, I have no formal math training other than high school. I am a beginner with group theory and have just recently begun picking it up and seeing its potential uses.) Imagine an infinite game board whose spaces make a hexagonal tiling. Pieces on this board can move directly to adjacent spaces from their current space, or choose not to move at all (denoted as e ). The six directions they can move could be called N , NE , SE , S , SW , and NW . This forms three distinct axes, where positive movement on axis a would correspond with N , positive movement on axis b would correspond with SE , and positive movement on axis c would correspond with SW . Describing movement on this board meets the criteria of an abellian group: Closure: Any combination of movements will still land the piece on this infinite game board. Associativity : (a • b) • c will result in the same location as a • (b • c) , although they will take different paths. Identity element: The zero movement (not moving at all) combined with any other movement results in just the other movement. ( e • a = a , a • e = a ) Inverse element: Moving one space towards N and then one space towards S results in zero net movement. ( a • (-a) = e ) Commutativity: Moving one space on the a axis and then one space on the b axis results in the same position as moving one space on the b axis and then one space on the a axis. ( a • b = b • a ) The ways that movements combine is interesting, though. The three axes are not orthogonal to each other. For instance, a • b • c = e and a • b = -c for all axes. I'm looking for a deeper understanding of the patterns that emerge from this. Does this group have a name? How can one use group theory to describe, for instance, an algorithm that can determine if two long lists of movements on the board result in the same net movement? ( a • a • b • c • (-a) = e )","['abelian-groups', 'infinite-groups', 'tessellations', 'group-theory']"
2537360,Elementary integrals and Riemann surfaces,"An article of Brian Conrad "" Impossibility theorems for elementary integration "" says that a way of proving the following elliptic integral If $P(x)$ is a monic polynomial of degree $\ge 3$ without repeated roots, then $$\int \frac{dx}{\sqrt{ P(x)}}$$ 
  is not an elementary integral. After that, Conrad says that this theorem is a consequence of general facts about compact Riemann surfaces, because the formula obtained by the Liouville theorem for elementary integrals is equivalent to the equality for meromorphic $1$-forms $$\left({dy}/{y}\right)=\sum c_j (dg_j)/(g_j)+dh$$ on the compact Riemann surface associated to $y^2=P(x)$, and for degree of $P>2$ the left side of the formula is a holomorphic $1$-form on $C$, and a non-zero holomorphic 1-form on a compact Riemann surface never admits an expression like the above expression. I don't know anything about Riemann surfaces, and my question is: where can I find the information to understand this reasoning? I don´t know if this it's very difficult or it's very elementary in the theory of Riemann surfaces. I appreciate any answer. Thanks","['riemann-surfaces', 'complex-analysis']"
2537380,Is Side-Side-Angle a valid congruency pattern for triangles sharing that first side?,"The image below shows two triangles. I am convinced that this is a Side-Side-Angle situation, which is not enough to prove the two triangles  congruent. Is this correct? What is throwing me off is that the triangles share a common side.",['geometry']
2537393,"$x^n+y^n=z^3$ What is Darmon-Merel's objection to using Conrad, Diamond, and Taylor?","In ""Winding quotients and some variants of
Fermat’s Last Theorem"" Darmon and Merel 2007 on page 4 They discuss the proof that Assume that every elliptic curve over $Q$ is modular. Then $x^n+y^n = z^3$ has no non-trivial primitive solution when $n >= 3$. They state: In this case, our proof of part 3 of the Main Theorem still requires the
  hypothesis that the elliptic curves involved in the study of the equation $x^n+y^n = z^3$ are modular. This requirement is not a consequence of the results of Wiles, not even of the strengthenings due to Conrad, Diamond, and Taylor, since the conductor of these elliptic curves is divisible by $27$. The problem of showing that a cube cannot be expressed as a sum of two relatively prime nth powers ($n >= 3$) gives a Diophantine incentive for proving the entire
  Shimura-Taniyama conjecture [...] My question is how on earth does the conductor being divisible by $27$ conflict with the modularity theorem?  And how do you calculate it being divisible by $27$ in this case?","['diophantine-equations', 'algebraic-number-theory', 'number-theory', 'elliptic-curves', 'modular-forms']"
2537404,Second Steklov operator and Dirichet-to-Neumann operator for a disk,"Let $D: C^{\infty}(\partial K) \rightarrow C^{\infty}(\partial K)$ be the Steklov operator (or sometimes so-called Dirichlet-to-Neumann operator) defined via the relation: $\psi \mapsto u|_{\partial K}$. Here $u$ solves the following boundary value problem: $$ \Delta u = 0$$
$$ \frac{\partial u}{\partial \vec{n}}|_{\partial K} = \psi|_{\partial K}$$ Therefore, we can calculate the spectrum of the Steklov operator for a disk and thus obtain some general propetries of the spectrum via Riemann Mapping Theorem up to some conformal equivalence. Let also $u := u_{\psi}$, $w := v_{\varphi}$ be smooth functions that solve the boundary value problem in the preceding setting. Then we define so-called second Steklov operator $S_{2}$ as 
$$\int_{\partial K}{(S_{2} \psi) \varphi dx } = \int_{K} {\langle \nabla^{2} u, \nabla^{2} w \rangle dx}$$ By differentiating the Bochner-Lichnerowicz-Weitzenbock formula, we get that $S_{2}$ can be expressed as $$S_{2}(f) = - \nabla_{\partial K}{Df} - D(\nabla_{\partial K}{f}) - H_{\partial K}{f} + D \nabla_{\partial K} \cdot II_{\partial K} \nabla_{\partial K} D(f)$$
Here $II_{\partial K}$ stands for the second fundamental form, $H_{\partial K}$ is the mean curvature, D is the Dirichlet-to-Neumann operator on $K$. My questions are: (1) Are there any implicitly stated relationships between the Dirichlet-to-Neumann operator and 2nd Steklov operator apart from those that can be directly derived from the definition? What can we say about some propetries that the spectrums of these two operators share? (2) Is it possible to treat the 2nd Steklov operator as a ""connecting"" operator for some mixed boundary value problem, perhaps, such as Dirichlet-to-Neumann is related to the Diriclet problem and, correspondingly, Neumann problem? Any piece of advice or references provided are much appreciated!","['functional-analysis', 'spectral-theory', 'riemannian-geometry', 'partial-differential-equations']"
2537421,Answered: With what probability do $4$ points placed uniformly randomly in the unit square of $\mathbb{R}^2$ form a convex/concave quadrilateral?,"I have this problem that I've struggled with for a while. If you place $4$ points randomly into a unit square (uniform distribution in both $x$ and $y$), with what probability will this shape be convex if the $4$ points are connected in some order? Equivalently, with what probability will there be a point inside the triangle with the largest area with vertices at the other $3$ points. In particular I am interested in the answer for when this area of support is $\mathbb{R}^2$ and is uniform. I ran a simulation and found that on a unit square the answer is about $71\%$ concave. On a unit circle picking polar co-ordinates r and theta from uniform random distributions results in a a probability of concavity of $68\%$. When the distribution for r is altered so that each point in the circle is equally likely then this falls to $51\%$. Any advice or links for a possible answer or whether this is even possible would be appreciated. EDIT: It turns out this problem is the same as Sylvester's 4 point problem. Alas I am 150 years too late. Thanks to all who helped. Only one person gave an answer, not quite correct but I award the bounty to them anyway for their efforts.","['uniform-distribution', 'probability']"
2537427,$A$ self adjoint matrix. Prove there is a real $c$ such that $cI+A$ is positive,"This was partially asked before (1. There exists a real number $c$ such that $A+cI$ is positive when $A$ is symmetric , 2. Proving $aI+A$ is Positive Definite ) but I'd like revision of this proof using forms. Let $A$ be a self-adjoint $n \times n$  matrix. Prove that there is a real number $c$ such that the matrix $cI+A$ is positive This is my attempt: Let be $f$ the form with matrix $A$ in the canonical basis. Since $A$ is self adjoint there exists a unitary matrix $Q$ such that
$$Q^*AQ = D$$
where $D$ is a diagonal matrix. But this is just the matrix of the form $f$ in the basis composed of the column vectors of $Q$. Moreover, since $A$ is self adjoint it follows that every entry of the diagonal matrix D is real. So, consider the matrix $A+cI$ wich is also self adjoint and the consider the form $f'$ associated to this matrix. Again, there exists $P$ unitary such that $P^*(A+cI)P = D'$ is diagonal. But $$P^*(A+cI)P = P^*AP + cP^*P = P^*AP + cI = D'$$
Choose $c$ so the entries of $D'$ are positive (for example, $c = 2\max \left\{
 |(P^*AP)_{ii}|: i \in \left\{1,\ldots,n\right\}\right\}$) and then obviously 
$$ X^*D'X > 0 \quad\forall X \in V$$
so the form $f'$ is positive and then $A+cI$ is positive (using the fact that if a form $f$ is positive then its matrix in every ordered basis is a positive matrix) Thanks a lot for your feedback!","['matrices', 'linear-algebra', 'proof-verification']"
2537445,Evaluating iterated integral with 3 variables,"I am asked to evaluate the integral by hand. I do not know how to start $$\int_0^4\int_x^4\int_0^y\frac{6}{1 + 48z - z^3}\, dz\, dy\, dx$$ Note: This is a homework question. Explicit permission to seek help from others is given. I do not see a suitable $u$-sub nor can I factor out $z$ for an easier integral. Changing the order of integration does not seem to help either (i.e., to $dy\, dz\, dx$). Even WolframAlpha can only show it is approximately equal to $4.859\,81$. Symbolab says steps are not supported for this type of question.","['multivariable-calculus', '3d', 'integration', 'definite-integrals']"
2537450,Relationship between the roots of derivatives and their functions,"Given that $f$ is differentiable on $\mathbb{R}$, I know that ""between the zeroes of $f$, there is a zero of $f'$. But given that $f'$ has $k$ roots, then is it true that $f$ has at most $k+1$ roots.","['derivatives', 'roots', 'calculus']"
2537457,Prove $\mathbb{R}$ is connected,"Prove that $\mathbb{R}$ is connected. PLease i have found other ways to prove it but i want to make this way work. Proof: 1) Strategy : If i show that a arbitrary interval is connected then  i can take the colection of intervals around zero that make up $\mathbb{R}$ and have a common point So that the union is connected. TO show that $(a,b)$  is connected ill use the fact that if it is not connected then There is an clopen in $(a,b)$ that makes up a separation with its complement in $(a,b)$ and there is not limit point of one another in any of the 2.Then ill use Least upper bound of property of that clopen set and come to a contradiction. 2) Let $(a,b)$ arbitrary open interval in $\mathbb{R}$. Suppose  $\mathbb{R}$ is not connected. And also $(a,b)$ is not connected.Since it is not connected there is a clopen set $V \subseteq (a,b) $ such that it makes a separation  $V \cup V^{c} =(a,b) $ . NOw since $V$ is open in $(a,b)$ $$V=K \cap (a,b)$$ and $$U=C \cap (a,b) $$ where $U=V^{c}$  and $K,C$ open in $\mathbb{R}$. Now i have $V \cap U= \emptyset $ $$(K \cap (a,b)) \cap  (C \cap (a,b)=\emptyset $$ so $$(K \cap C) \cap (a,b) =\emptyset  $$ Now for the last to be true and also that my $U$ and $V$ are non empty  and by drawing and trying to figure it out the $ K \cap C$ must be empty otherwise  $V$ or $U$ will be empty. I can see that with drawing intervals but i cant prove it . Hence i need to prove that $K \cap C = \emptyset $  .After i know that  the specific intersection is empty  things are much clear and  i can take the supremum of $U$ and it will be  a limit point of $U$ and be in $U$ but it will be also limit point of $V$ since every open area of it will have a non empty intersection. So it cant be a separation of $(a,b)$ and that means the only  clopen sets of $(a,b)$ are itself and the empty hence it is connected and complete the proof. Is what im trying to prove even a necessary setp? Or i can just go on with the supremum argument straight away.But  i really wanna prove that intersection is empty i draw all the possibilities and it has to be empty.It is so frustrating something so easily seen not being able to write it down??? I dont want proofs  using other arguments. I could easily show $R$ is path connected or myabe some other proofs. Im just stuck trying to do this one so you kinda feel  my pain.","['general-topology', 'real-analysis', 'analysis', 'connectedness']"
2537458,Question about $\int_{0}^{\infty} \sin x dx = 1$,"I saw that physicists use the following integral 
$$
\int_{0}^{\infty} \sin x dx = 1
$$
in a distribution sense, i.e. 
$$
\int_{0}^{\infty} \sin x dx := \lim_{a\to 0} \int_{0}^{\infty} e^{-ax}\sin x dx = \lim_{a\to 0} \frac{1}{a^{2}+1} = 1
$$
However, I wonder we get the same value when we change the distribution part. For example, we have the following result (according to Wolfram alpha)
$$
\lim_{a\to 0} \int_{0}^{\infty} e^{-ax^{2}}\sin x dx = \lim_{a\to 0}\frac{F\left(\frac{1}{2\sqrt{a}}\right)}{\sqrt{a}} = \lim_{a\to 0 }\frac{e^{-1/4a}}{\sqrt{a}}\int_{0}^{\frac{1}{2\sqrt{a}}}e^{y^{2}}dy =1 
$$
where $F(x):=e^{-x^{2}}\int_{0}^{x}e^{y^{2}}dy$ is a Dawson's integral. To show this, we have to show the following : suppose a function $f:\mathbb{R}_{\geq 0}\times \mathbb{R}_{\geq 0}\to \mathbb{R}$ is continuous and the integral 
$$
\int_{0}^{\infty} f(a, x)\sin x dx
$$
converges for any $a\geq 0$. If $f(0, x)=0$ for all $x\geq 0$, we have
$$
\lim_{a\to 0} \int_{0}^{\infty} f(a, x)\sin x dx = 0
$$
It seems that we cannot use dominated convergence theorem since $\sin x\not\in L^{1}$. If the above statement is true, is there any other function than $\sin x$ which satisfies the property and can define the improper integral in this way?","['distribution-theory', 'improper-integrals', 'integration']"
2537474,The practical usage of Arnold Matrix Trace Theorem,"I would like to ask about the Arnold's Matrix Trace theorem:
$$\textrm{tr}\big(A^{p^k}\big)\equiv\textrm{tr}\big(A^{p^{k-1}}\big)\ (\!\!\bmod  {p^k}).$$
This theorem looks fantastic to me. But is there any practical usage of it or maybe some algorithm based on this? EDITED:
Adding some reference: https://rjlipton.wordpress.com/2009/08/07/fermats-little-theorem-for-matrices/","['matrices', 'number-theory', 'algorithms', 'trace', 'prime-numbers']"
2537477,Looking for an elementary solution to this angle problem,"I came across this when I was trying to prove some properties of a particular parallelogram. It looked pretty trivial at first, but it looks like I've sneezed up the wrong tree with this one.","['trigonometry', 'euclidean-geometry', 'triangles', 'geometry', 'contest-math']"
2537538,Possible Generalizations of The Heine-Borel Theorem,"I'm quite new to general topology, so I apologize if this question is trivial. The famed Heine-Borel theorem states that in $\mathbb{R}^n$, a set is compact if and only if it is closed and bounded. Clearly this result doesn't hold over all metric spaces, (non-complete ones are the usual go-to examples). There's another theorem that states that a metric space is compact if and only if it is complete and totally bounded. I have a couple questions about this. 1) As the second theorem requires completeness and total boundedness, could you show me an example of a metric space that is bounded, complete, but not compact? 2) Second, (a softer question), what kinds of spaces satisfy what I'll call the ""Heine-Borel property"" (i.e. subspaces of them are compact if and only if they are closed and bounded). Is $\mathbb{R}^n$ the only example of such a space? If not, what other things can we say about that class of spaces?","['general-topology', 'compactness', 'soft-question']"
2537557,Sum of integer reciprocals,"The following problem results in a cubic, but is there a way to simplify it easier?: The sum of the reciprocals of three consecutive integers is 47/60. What is the sum of these integers? I ended up with $47{ x }^{ 3 }-180{ x }^{ 2 }-47x+60 =0$. However, unlike other methods where I've learned to ""Let ${ x }^{ 2 }$ = k, $\therefore  { x }^{ 4 }={ k }^{ 2 }$"", ${x}^{3}$ is not a multiple of 2, so I cannot apply this same method. How should I proceed? (Other than just substitute, which the answers from this & this website did).","['number-theory', 'cubics', 'integers', 'elementary-number-theory']"
2537583,Is there any way to solve this without using a graphing calculator?,"Solve for $x$: $$x=2^{8-2x}$$ Whenever I've seen solutions to this question, they have always been through plotting the two graphs and finding their intersection point. But, is there any other way to solve this (perhaps in a more algebraic way)?","['exponential-function', 'functions']"
2537596,"Is $W_0(A_1^*,A_2^*)=\overline{ W_0(A_1,A_2)}$?","Let $E$ be a complex Hilbert space. Let $A_1,A_2\in \mathcal{L}(E)$. Let
\begin{eqnarray*}
W_0(A_1,A_2)
&=&\{(\lambda_1,\lambda_2)\in \mathbb{C}^2;\;\exists\,(x_n)_n;\;\|x_n\|=1,\;(\langle A_1 x_n\; ,\;x_n\rangle,\,\langle A_2 x_n\; ,\;x_n\rangle)\to (\lambda_1,\lambda_2),\\
&&\phantom{++++++++++}\;\hbox{and}\;\displaystyle\lim_{n\rightarrow+\infty}(\|A_1x_n\|^2+\|A_2x_n\|^2)=\|A_1\|^2+\|A_2\|^2\;\}.
\end{eqnarray*} How to show that 
  $$W_0(A_1^*,A_2^*)=\overline{ W_0(A_1,A_2)}:=\{(\overline{\lambda_1},\overline{\lambda_2});\;(\lambda_1,\lambda_2)\in W_0(A_1,A_2)\,\}?$$ I try as follows: $(\lambda_1,\lambda_2)\in W_0(A_1^*,A_2^*)$ if and only if there exists $(y_n)_n$ such that $\|y_n\|=1$, $(\langle A_1 y_n\; ,\;y_n\rangle,\,\langle A_2 y_n\; ,\;y_n\rangle)\to (\overline{\lambda_1},\overline{\lambda_2})$ and $\displaystyle\lim_{n\rightarrow+\infty}(\|A_1^*y_n\|^2+\|A_2^*y_n\|^2)\rightarrow \|A_1\|^2+\|A_2\|^2$ I stuck here, because I think that $\|A_1^*y_n\|$ is not in general equal to  $\|A_1y_n\|$. If the result is false, I want to construct a counter-example. I think it is true only for normal operators. Thank you!!","['functional-analysis', 'linear-algebra']"
2537614,Intuition for why Costas arrays of order $n$ fail to undergo combinatorial explosion.,"A Costas array can be regarded geometrically as a set of $n$ points lying on the squares of a $n \times n$ checkerboard, such that each row or column contains only one point, and that all of the $\frac{n(n − 1)}{2}$ displacement vectors between each pair of dots are distinct. — Costas array, Wikipedia OEIS sequence A001441 counts the ""number of inequivalent Costas arrays of order $n$ under [the action of the] dihedral group [of order 8]"": 1, 1, 1, 2, 6, 17, 30, 60, 100, 277, 555, 990, 1616, 2168, 2467, 2648, 2294, 1892, 1283, 810, 446, 259, 114, 25, 12, 8, 29, 89, 23 Usually combinatorial things undergo ""combinatorial explosions""—it's surprising that this one grows for a bit, and then decreases again. What is the intuition for the local (global?) maximum at $n=16$? Are asymptotic bounds known for this sequence? In particular, should I expect a value like $a(10^{10})$ to be small?","['combinatorics', 'combinatorial-geometry', 'asymptotics']"
2537620,Do inflection points of $f(x)$ give $f'(x)=0$?,I understand that the values of $x$ that allow $f'(x)=0$ are stationary points and therefore potential local maximums and minimums of $f(x)$. When would a stationary point NOT be a local maximum or minimum? Do inflection points also yield $f'(x)=0$?,"['derivatives', 'stationary-point', 'calculus']"
2537687,"Checking if a given $d(x,y)$ is a metric","For $x,y$ $\in$ $[0,1]$, let
  $$d(x,y)= \begin{cases}
|x-y|,  & \text{if $x-y \in \mathbb{Q} $} \\[2ex]
2, & \text{if $x-y \notin \mathbb{Q}$}
\end{cases}$$
  where $\mathbb{Q}$ is a set of rational numbers.
  Find if $d$ is a metric in $[0,1]$. So what I've already done is: 1) $d(x,y)=0$ iff $x=y \Rightarrow$ $d(x,x)=|x-x|=0$ because $x-x=0 \in \mathbb{Q} $ 2) $d(x,y)=d(y,x) \Rightarrow$ 2.1) if $x-y \in \mathbb{Q} $ and $y-x \in \mathbb{Q}$ then $|x-y|=|y-x|$, 2.2) if $x-y \notin \mathbb{Q} $ and $y-x \notin \mathbb{Q} $ then $2=2$ - ok 2.3) if $x-y \in \mathbb{Q} $ and $y-x \notin \mathbb{Q} $ then $|x-y|=2$ $\Rightarrow x=y+2$ or $x=y-2$ 2.4) if $x-y \notin \mathbb{Q} $ and $y-x \in \mathbb{Q} $ then $2=|y-x|$ $\Rightarrow x=y+2$ or $x=y-2$ 3) $d(x,y) \le d(x,z) + d(z,y) \,,\forall x,y,z\in \mathbb{Q}  \Rightarrow$ So do I have to write all of these and prove inequalities like: $|x-y| \le |x-z| + |z-y|$ or is there any other option? If anyone can help me with the 3rd condition and check all the previous ones, if I didn't make any mistakes (or is it wrong?), I would be really thankful.","['general-topology', 'metric-spaces']"
2537708,Hypergeometric integral $\int_0^1 dx ~e^{-(\frac{1}{x}+\frac{1}{1-x})}(x)^{b-a-1}(1-x)^{a-1}$.,"I have an this integral $$\int_0^1 dx ~e^{-(\frac{1}{x}+\frac{1}{1-x})}(x)^{b-a-1}(1-x)^{a-1}$$ how can I calculate the integral in form of Bessel function or Hypergeometric function. As mentioned in Arfken (special function-chapter)
$$M(a,c;x)=\frac{\Gamma(c)}{\Gamma(a)\Gamma(c-a)}\int_0^1 e^{xt}t^{a-1}(1-t)^{c-a-1}dt$$
and 
$M(a,c;x)= e^x M(c-a,c;-x) $ But I couldn't find  useful variable to simplify the integral","['special-functions', 'integration', 'definite-integrals']"
2537729,What does it mean for a vector function to be twice differentiable?,"Let $I \subseteq \mathbb{R}$ be an open interval and $\vec{f}: \mathbb{R}^2 \to \mathbb{R}^2, \vec{y}: I \to \mathbb{R}^2$ be functions such that the system $\vec{y}' = \vec{f} \circ \vec{y}$ has an isolated critical point at $\vec{0}$. Then we say the above system is locally linear at $\vec{0}$ if there is a linear map $A: \mathbb{R}^2 \to \mathbb{R}^2$ and a function $\vec{g}: \mathbb{R}^2 \to \mathbb{R}^2$ such that $$\vec{y}' = A\vec{y} + \vec{g} \circ \vec{y}$$ and $$\lim_{\vec{\epsilon} \to \vec{0}} \frac{||\vec{g}(\vec{\epsilon})||}{||\vec{\epsilon}||} = 0$$ Now in the lecture notes, it is stated that if $\vec{f}$ is a twice continuously differentiable vector function, then the system is locally linear at $\vec{0}$. While in the textbook, it is stated that suppose we write $\vec{f} = \begin{pmatrix}
f_1 \\
f_2
\end{pmatrix}
$,
then the system is locally linear at $\vec{0}$ if both $f_1$ and $f_2$ has continuous second order partial derivatives. Can anyone explain what does it mean for a vector function to be twice differentiable and what is the relationship between the condition in the lecture notes and the condition in the textbook?","['multivariable-calculus', 'ordinary-differential-equations', 'definition']"
2537742,Evaluating double integral.,"Find the double integral $$I=\iint_D y dy dx$$ where $D$ is area bounded by $$D= \{(x,y): x^2+y^2\leq 1, x^2+y^2\leq2x, y\leq0 \}$$ First off, function I am evaluating is $z=y$ and it's just one plane in $\mathbb{R}^3$ . Now, this is the $D$ I am looking for: Now, I should integrate over the intersection of these two circles where $y<0$ . Obviously, I should use polar coordinates so $$x=r\cos(\theta)\\ y=r\sin(\theta)$$ , Jacobian is $r$ so it remains now to find the bounds of integration, however I am not quite sure how to do that, since the intersection point of two circles is $$(x,y)=\left(\frac{1}{2}, -\frac{\sqrt3}{2}\right)$$ I suppose that angle should go from $-\frac{\pi}{6}$ to $0$ but then, I could write out the equations of circles to get bounds for $r$ , where $r$ should go from the blue circle to red circle, which means $$r \in [2\cos\theta, 1]$$ , but I am not quite sure is that correct way to do it because when i calculate it this way I get the positive result $$I=\frac{5}{12}$$ , which is not what I expected since I am integrating over an area where $x$ is positive and $y$ is negative which means that I am actually finding a volume of body that's either in fourth or eighth octant, but, when I take a look at the function I am integrating I see that $z=y$ meaning that sign of $y$ will determine the sign of $z$ so I am finding a volume of a body that's located in eighth octant, so I expected a negative value here. Obviously, something is wrong, it might be $D$ or bounds of integration or calculations or ,the worst case probably, my reasoning. Any help is appreciated.","['multivariable-calculus', 'integration']"
2537749,"{$x_1, x_2, ...,x_n$} is linearly independent in normed space $X$. Show for any scalars $a_1, ..., a_n$ there is $f$ in dual $X'$ with $f(x_i) = a_i$","Let $X$ be a normed space, $n\in N$ and {$x_1, x_2, ...,x_n$} be a linearly independent set in $X$. Prove that for any scalars $\alpha_1, \alpha_2, ..., \alpha_n$ there exists $f$ in the dual space $X'$ such that $f(x_i) = \alpha_i$; $ i = 1, 2, ..., n$. (I tried it by using the theorem:
For any non-zero $x_0$ in a normed space $X$ we have a linear functional $f$ such that $||f||=1 $ and $f(x_0)=||x_0||$.
But I could not succeed. )","['functional-analysis', 'linear-transformations', 'dual-spaces']"
2537775,Product of Trigonometric function,"The value of $$\prod^{10}_{r=1}\bigg(1+\tan r^\circ\bigg)\cdot \prod^{55}_{r=46}\bigg(1+\cot r^\circ\bigg)$$ Attempt: $\displaystyle \prod^{10}_{r=1}\bigg(1+\tan r^\circ\bigg)=(1+\tan 1^\circ)(1+\tan 9^\circ)\cdots \cdots (1+\tan 4^\circ)(1+\tan 6^\circ)\tan 5^\circ$ from $\tan(A+B) = \tan 10^\circ\Rightarrow \frac{\tan A+\tan B}{1-\tan A\tan B} = \tan 10^\circ$ could some help me to solve it, thanks",['trigonometry']
2537798,$\ell^\infty(\mathbb{N})$ is not separable,"Show, that $\ell^\infty(\mathbb{N})$ is not separable. Use, that $\mathcal{P}(\mathbb{N})$ is uncountable and observe the set $\{1_A\colon A\in\mathcal{P}(\mathbb{N})\}\subseteq\ell^\infty(\mathbb{N})$. Suppose $\ell^\infty(\mathbb{N})$ is separable. Then it exists a countable, dense set $D=\{d_n\colon n\in\mathbb{N}\}\subseteq\ell^\infty(\mathbb{N})$. Since $D$ is dense in $\ell^\infty(\mathbb{N})$ exists for every $a_k\in\ell^\infty(\mathbb{N})$ and $\varepsilon>0$ a sequence $(d_k^{(n)})\subseteq D$ with $\|d_k^{(n)}-a_k\|_{\infty}=\|d_k(n)-a_k(n)\|_{\infty}<\varepsilon$ Now I want to construct an $(a_k)$ such that it can not be the limit of any $d_k\in D$. I am trying to follow the hint in the task: $a_k=\begin{cases} 0,~~ \text{if}~~ |d_k(k)|>1\\ 1,~~ \text{else}\end{cases}$ Now we have: $\vert|d_k(n)-a_k(n)\|_\infty=\sup_{n\in\mathbb{N}} |d_k(n)-a_k(n)|\geq 1$ for all $n\in\mathbb{N}$ This is a contradiction. Hence $\ell^\infty(\mathbb{N})$ is separable. Is this correct?
Thanks in advance for your help.","['functional-analysis', 'separable-spaces']"
2537828,Find all matrices which commute with a given matrix,"I wonder how to find all matrices $B$ which satisfy $AB=BA$, where $A=\left( {\begin{array}{*{20}{c}}
  1&{}&{}&{}&{} \\ 
  1&1&{}&{}&{} \\ 
  1&1&1&{}&{} \\ 
   \vdots & \vdots & \vdots & \ddots &{} \\ 
  1&1&1&1&1 
\end{array}} \right)$ (the coefficients above the diagonal of the matrix $A$ are zero). I tried Jordan form of $A$, but still couldn't see what to do next... Any help will be appreciated :)","['matrices', 'matrix-equations', 'linear-algebra']"
2537835,Trigonometric sum of a ratio of two sine functions,"Evaluate $$\sum^{13}_{k=1}\frac{\sin (30^\circ k +45^\circ)}{\sin(30^\circ(k-1)+45^\circ)}.$$ Put $30^\circ = \alpha, 45^\circ = \beta$. Then
$$\begin{align}
S:=&\sum^{13}_{k=1}\frac{\sin (\alpha k+\beta)}{\sin(\alpha(k-1)+\beta)} = \sum^{13}_{k=1}\frac{\sin(\alpha(k-1)+\beta+\alpha)}{\sin(\alpha(k-1)+\beta)}\\
&= \sum^{13}_{k=1}\frac{\sin(\alpha(k-1)+\beta)\cos \alpha+\cos(\alpha(k-1)+\beta)\sin \alpha}{\sin(\alpha(k-1)+\beta)}\\
&= \cos \alpha\sum^{13}_{k=1}1+\sin \alpha\sum^{13}_{k=1}\cot(\alpha(k-1)+\beta) \\
&= \frac{\sqrt{3}}{2}\cdot 13+\frac{1}{2}\sum^{13}_{k=1}\cot(\alpha(k-1)+\beta).
\end{align}
$$
Could some help me to solve it, thanks.",['trigonometry']
2537864,Isomorphism between Endomorphisms of $R^n$ and $M_n(R^\mathrm{op})$,"My professor said in class that if we consider $R^n$ as an $R$-module, we get a bijection from the $\operatorname{End}(R^n)$ to the set of $n\times n$ matrices since we can regard any such homomorphism as a linear map from the vector space defined by $R^n$ onto itself. But what I don't get is the following, he said that this bijection is not a ring homomorphism, but there is a ring isomorphism between $\operatorname{End}(R^n)$ to $M_n(R^\mathrm{op})$ where $R^\mathrm{op}$ is the opposite ring. I know why this is true for a trivial module $M$ over $R$ where $M = R$. However I fail to see how this is the case for $R^n$ since I can simply identify a homomorphism from $R^n$ onto itself with a matrix, can anyone explain the technical details for how the opposite ring comes into the picture? Thanks.","['abstract-algebra', 'ring-theory', 'modules']"
2537878,Converging trigonometric result,"Consider the sequence $f_n(x)$ defined by $f_0(x) = \sin x$, $f_1(x) = \sin\cos x$, $f_2(x) = \sin\cos\sin x$, and so on. One can check that $f_n(x)$ converges to a constant, say $N_p$. My question is what might be the value of $N_p$? By brute force I got $N_p=0.69481969\ldots.$","['functions', 'trigonometry', 'calculus', 'limits']"
2537880,Kähler form on the Blow up of a Kähler manifold,"Given a Kähler manifold $X$, the blow up is a Kähler manifold aswell (see Hodge theory and CAG by Voision Prop. 3.24).
The idea is of course to use the pull-back the Kähler form $\pi^*\omega_X$. It says that this form is not positive , but only semi-positive and I fail to see why exactly. Positivity of $\omega_x$ means that locally we can write $\omega_x= \sum_i \alpha_i dz_i\wedge d\bar z_i$ with $\alpha_i$ real and non-negative.
Now $\pi^*\omega_X= \sum_i (\alpha_i\circ\pi) d(z_i\circ\pi)\wedge d(\bar z_i\circ\pi)$.
Obviously the coefficients $(\alpha_i\circ\pi)$ still remain positive, so somehow the differential forms $ d(z_i\circ\pi)\wedge d(\bar z_i\circ\pi)$ must vanish on some special tangent vectors in a neighborhood around the blow up? I am also not sure, if my notion of positivity is correct or appropriate here.","['complex-geometry', 'differential-forms', 'pullback', 'kahler-manifolds', 'differential-geometry']"
2537883,Transforming quadratic parametric curve to implicit form,"I have an algebraic parametric curve $$ \mathbf{p}(t) = (x(t), y(t)) $$ where $x$ and $y$ are both polynomials of degree $\leq p$ . Now, I want to find the implicit form $f(x, y) = 0$ . A document I'm reading claims without argumentation that there is always an equivalent implicit formulation of the form $f(x, y) = 0 $ , where $f$ is a polynomial of degree $\deg(f) \leq p$ . This is not obvious to me. I found some documents stating similar results. For example, in [1]: Algebraic geometry provides us with the following key facts about algebraic curves: e.g. [2]: Every plane parametric curve can be expressed as an implicit curve. Some, but not all, implicit curves can be expressed as parametric curves. Unfortunately, I can't get my hands on a copy of [2]. [This question][3] considers the case of a quadratic parametric curve. I was wondering if there is a simple proof for the statement every algebraic parametric curve on the plane can be expressed as an implicit curve . Moreover, I am curious if $x$ and $y$ being polynomials of degree $\leq p$ implies that $\deg(f) \leq p$ as well. Since I don't know the nature of the answer, pointing me to a resource might be more appropriate. [1]: Conversion methods between parametric and implicit curves and surfaces, Christoph M. Hoffmann, from http://graphics.stanford.edu/courses/cs348a-17-winter/Handouts/a228715.pdf [2]: R.J. Walker, Algebraic Curves, Springer Verlag, New York, 1978.
[3]: Parabola in parametric form","['plane-curves', 'algebraic-geometry', 'parametric', 'implicit-function', 'algebraic-curves']"
2537894,Expected value of sum of a random number of i.i.d. random variables,"The question: $X_1$, $X_2$, etc. are independent and identically distributed non-negative
integer valued random variables. $N$ is a non-negative integer valued random variable which is independent of $X_1$, $X_2$ etc.., and $Y$ = $X_1 + X_2 + X_3 + … + X_N$ . (We take $Y = 0$ if $N = 0$). Prove that $\mathbb{E}[Y] = \mathbb{E}[X_1]\mathbb{E}[N]$. My attempt: I know that the probability generating $G_Y(s)$ of $Y$ is equal to $G_N(G_X(s))$... but I'm not sure how that's helpful here. My intuition leads me in this direction: $\mathbb{E}[Y] = \sum\limits_{n=0}^{\infty} \mathbb{E}[Y|N=n]\mathbb{P}(N=n)$ $= \sum\limits_{n=0}^{\infty} \mathbb{E}[nX_1|N = n]\mathbb{P}(N=n)$ $= \sum\limits_{n=0}^{\infty} n \mathbb{E}[X_1|N = n]\mathbb{P}(N=n)$ (is this step valid??) But I don't know where to go from here.","['probability', 'random-variables']"
2537897,Is an injective function always monomorphic,"Is an injective function monomorphic (in category of sets)? How do I prove this? Conversely,
I could prove that a monomorphic function is injective in the following manner: If f is monic, then 
$f\circ g = f\circ h\Rightarrow  g= h$ Given the mapping $C\to A\to B$, where $g,h\colon C\to A$ and $f\colon A\to B$
Assume $f$ is non-injective then there are some $a$, $b$ in $A$ and $c$ in $B$ such that $f(a) = f(b) = c$ There are also $x$ and $y$ in $C$ such that $g(x) = a$ and $h(y) = b$
Thus $f(g(x) = f(h(y)) = c$, thus $f\circ g = f\circ h$, but $g \neq h$
but $g = h$
Thus $f$ cannot be non-injective I could not prove it other way round, so is an injective function monic?","['category-theory', 'monomorphisms', 'elementary-set-theory']"
2537971,Induction : Prove $a^k-b^k \le ka^{k-1}(a-b)$,"I am doing revision for my discrete mathematics and I am having a really hard time with this question. Suppose that a,b are real numbers with $0<b<a$. Prove by mathematical induction that if $k$ is a positive integer then $a^k-b^k \le ka^{k-1}(a-b)$ I know this is a duplicate question that has been answered , but I can't wrap my head around the given solutions. I especially do not understand how : $a^{k+1} - b^{k+1} = a^k(a-b) + b(a^k-b^k)$ I also tried using the hint : $a^k - b^k = (a-b)(a^{k-1} + a^{k-2}b + ... + b^{k-1}) $ I am really lost with this , could someone point me in the right direction please ?","['induction', 'discrete-mathematics']"
2537980,To prove $\int_0^1dx\int_x^{1/x}\frac{ydy}{(1+xy)^2(1+y^2)}=\frac{\pi-1}{4}$,"Prove by changing order of integration
$$\int_0^1dx\int_x^{1/x}\frac{ydy}{(1+xy)^2(1+y^2)}=\frac{\pi-1}{4}$$ When I tried to draw the region of integration for this,I'm not getting a closed boundary.I double checked and I'm not getting where I went wrong","['multivariable-calculus', 'multiple-integral']"
2538022,Convergence of $ \sum_{k=1}^{\infty} \frac {3^k}{5^k + 1}$,"Show the convergence of the following series: $$\sum_{k=1}^{\infty}\frac {3^k}{5^k + 1}$$ a) Show the monotony of the partial sums b) estimate upwards c) remember the geometric series (I do not know how to use that here.) The following is what I have done so far: To show by induction: $a_{k+1} < a_k \forall k \in \mathbb N_0$ Induction start: $n=1$ $a_2= \frac{3^2}{5^2+1}=\frac{9}{25+1}=\frac{9}{26}=\frac{18}{52} < \frac{26}{52}=\frac{1}{2}=\frac{3}{6}=\frac{3^1}{5^1+1}=a_1$ Induction step: $$\begin{align}
a_{k+1}&<a_k \\
\equiv \frac{3^{k+1}}{5^{k+1}+1} &< \frac{3^k}{5^k+1} \\
\equiv \frac{3^{k+1}}{5^{k+1}} &< \frac{3^k}{5^k} \\
\equiv \frac{3^{k+2}}{5^{k+1}} &< \frac{3^{k+1}}{5^k} \\
\equiv \frac{3^{k+2}}{5^{k+2}} &< \frac{3^{k+1}}{5^{k+1}} \\
\equiv \frac{3^{k+2}}{5^{k+2}+1} &< \frac{3^k+1}{5^{k+1}+1} \\
\equiv a_{k+2} &< a_{k+1} \\
\end{align}$$ To Show: $|a_k|= a_k$ $$
\begin{align}
|a_k| &= |\frac{3^k}{5^k+1}| \\
      &= \frac{|3^k|}{|5^k+1|} \\
      &= \frac{3^{|k|}}{5^{|k|}+|1|} \\
      &= \frac{3^k}{5^k+1} \\
      &= a_k
\end{align}$$ Because of the induction I can conclude that the sequence $\lim_{k \to \infty}a_k$ becomes smaller and smaller. 
And because of $|a_k|=a_k$ are all values $\forall k \in \mathbb N$ positive. $\Rightarrow $ The sequence  $a_k$ is monotically decreasing. 
$\Rightarrow $ The series $\sum_{k=1}^{\infty}a_k$ is monotically increasing. $$
\begin{align}
a_k = \frac{3^k}{5^k+1} &< \frac{3^k}{5^k}  \\
                        &< \frac{3^k}{3^k}  \\
                        &= 1 \\
\end{align}$$ $$\lim_{k \to \infty} 1 = 1$$ And thus: $\exists N \in \mathbb N$, such that $$|a_k| \le 1, \forall \quad k \ge N$$ Using the direct comparison test there can be concluded that $\sum_{k=1}^{\infty}{a_k}$ converges. Question : Is my proof correct?","['real-analysis', 'convergence-divergence', 'sequences-and-series', 'proof-verification']"
2538049,If $T^2$ is a compact operator on a Banach space then is $T$ also compact?,"If $T^2$ is a compact operator on a Banach space $X$ then is it true that $T$ is also compact operator on X? Here $T:X\to X.$ First I was trying to prove(assuming it is true). Since $X$ is Banach the set of all compact operators is  closed  in $\beta(X;X)$ ($\beta(X;X)$ is the set of all bounded operators in X). So I was trying to find a sequence $\{T_n\}$ in $\beta_0(X;X)$ (the set of all compact operators on $X$) which would converge to $T$ . Then using the given condition that $T^2\in\beta_0(X;X)$, we could say that $T\in \beta(X;X)$. So the main point is to find a suitable sequence $\{T_n\}$ from $\beta_0(X;X)$ that will converge to $T.$ Please someone help . Can we find such sequence or the statement is false? Thank you..","['functional-analysis', 'compact-operators', 'banach-spaces']"
