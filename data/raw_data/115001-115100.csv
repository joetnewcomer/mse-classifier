question_id,title,body,tags
1683082,Does every Sierpinski number have a finite congruence covering?,"The following link notes that $k = 78557$ is a Sierpinski number and the answer provides a congruence covering to prove that no integer of the form $k2^n + 1$ is prime: pew ( https://math.stackexchange.com/users/139000/pew ), How was 78557 originally suspected to be a Sierpinski number?, URL (version: 2014-12-30): How was 78557 originally suspected to be a Sierpinski number? There are six values of $k$ remaining less than $78557$ that PrimeGrid is trying to show are not Sierpinski numbers by finding a prime of each form. Maybe an alternate way of approaching the problem would be to attempt to find a congruence covering for those six values of $k$ implying that no prime can be found.  However, that would be a feasible approach only if one could expect the number of congruences involved in the covering to be finite. Hence the question: Could one expect a congruence covering of the set of integers represented by a Sierpinski number to be finite?","['number-theory', 'prime-numbers', 'elementary-number-theory']"
1683096,Distribution of Coefficients in Karhunen-Loeve Expansion,"Recall that the Karhunen-Loeve expansion of a second order stochastic process $X(t)$ is an orthogonal series expansion of the form $$
X(t) = \sum_{n=1}^\infty Z_n \phi_n(t)
$$ where $\phi_n(t)$ are the eigenfunctions of the covariance operator $$
T_K\phi = \int_a^b K(\cdot,s)\phi(s)ds
$$ Since $K(t,s)$ is positive semi-definite, we know that $\{\phi_n\}$ form an orthonormal basis, so the series converges in the $L^2$ sense for each path $X(t)$.  Furthermore the coefficients $Z_n$ are ""orthogonal in probability"" i.e. uncorrelated: $$
E[Z_n] = 0,\quad E[Z_nZ_m] = \lambda_n\delta_{nm}
$$
If the process is Gaussian, then $Z_n$ are independent Gaussians.  What if $X(t)$ is not Gaussian?  Is there a convenient/efficient way to determine the distribution of $Z_n$?  I realize that I can compute higher moments, form the MGF (if it exists!) and perform inverse Laplace transform, but that seems slightly inconvenient/inefficient (or maybe I'm just lazy).","['functional-analysis', 'probability-theory', 'stochastic-processes']"
1683097,Prove that the inequality $\sin^8(x) + \cos^8(x) \geq \frac{1}{8}$ is true for every real number. [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Prove that the inequality $\sin^8(x) + \cos^8(x) \geq \frac{1}{8}$ is true for every real number.","['algebra-precalculus', 'inequality', 'trigonometry']"
1683143,Likelihood function and MLE,Please let me know how to find the likelihood function and MLE for the function $$f(x;θ) = (θ+1)(x^θ)$$ I have tried using the general formula for likelihood function $L(θ)$ however not sure how to proceed further. Please assist.,['statistics']
1683150,Showing that $xy \leq \frac{x^p}{p} + \frac{y^q}{q}$,"Question: Let $x \geq 0$ , $y \geq 0$ and $p > 0$, $q>0$ with $\frac{1}{p} + \frac{1}{q} = 1$. Show that  $$xy \leq \frac{x^p}{p} + \frac{y^q}{q} $$ 
  [Suggestion: Without loss of generality suppose $xy = 1$]. Attempt: Let $f, \varphi : U \to \mathbb R$, $U = \{(x,y) \in \mathbb R^2; x > 0 , y > 0 \}$ given by $f(x,y) = \frac{x^p}{p} + \frac{y^q}{q}$ and $\varphi (x,y) = xy$. Then we have $$\mathrm {grad}\, f(x,y) = (x^{p-1}, y^{q-1}) \,\,\,\text{and}\,\,\, \mathrm {grad} \,\varphi (x,y) = (y,x)$$ Then $1$ is a regular value of $\varphi$. Consider $M = \varphi^{-1} (1)$, the hyperbola $xy =1$. Now $(x,y) \in M$ is a critical point of $f|_M$ iff $$\mathrm {grad}\, f(x,y) = \lambda\, \mathrm{grad} \, \varphi (x,y) \,\,\,\text{and}\,\,\, \varphi (x,y) = 1$$ As $x> 0 $ and $y>0$ we have $$x^{p-1} = \lambda y \,\,\, , y ^{q-1} = \lambda x \,\,\,\text{and} \,\,\,xy = 1$$ Then $$\frac{x}{y} = \frac{y ^{q-1}}{x ^{p-1}} \implies x^p = y^q$$ This gives us $$\begin{align}\frac{x^p}{p} + \frac{y^q}{q} &= \frac{qy^q + py^q}{pq } = y^q \frac{p + q}{pq}\\&= y^q = y ^{1 + \frac{q}{p}}\\&=y^{\frac{q}{p}}\cdot y = x \cdot y\end{align}$$ Now $f$ is of class $C^{\infty}$ and its Hessian is given by $$Hf(x,y) = \begin{pmatrix} (p-1)x^{p-2} & 0 \\ 0 & (q-1)y^{q-2}\end{pmatrix}  $$ and it is positive, therefore $xy$ is a local minimum. It follows then $$xy \leq \frac{x^p}{p} + \frac{y^q}{q}$$ as we wanted. The cases $x = y = 0$, $x = 0 $ and $y> 0$ were considered trivially true. Note: This inequality is used to prove Hölder's Inequality .","['young-inequality', 'real-analysis', 'lagrange-multiplier', 'proof-verification']"
1683151,Calculating $\int_0^{\pi/2} (x \sin x)^n dx$,"Define $$I_n = \int_0^{\pi/2} (x \sin x)^n dx$$ for $n \ge 0$ . I calculate the value for $n = 0, 1$ and $2$ . $$I_0 = \frac{\pi}{2}, \>\>\>I_1 = 1, \>\>\>I_2 = \frac{{\pi}^3 + 6 \pi}{48} .$$ In general, what's the value of $I_n$ ? P.S. WolframAlpha produces $$I_3 = \frac{7{\pi}^2}{12} - \frac{122}{27},$$ $$ I_4 = \frac{6{\pi}^5 + 170 {\pi}^3 -975 \pi}{2560},$$ $$ I_5 = \frac{149{\pi}^4}{720} - \frac{31841{\pi}^2}{3375} + \frac{56992552}{759375}\\$$ n = 3 ( http://www.wolframalpha.com/input/?i=integrate+%5B(x+sin(x))%5E3,+%7Bx,+0,+PI%2F2%7D%5D ), n = 4 ( http://www.wolframalpha.com/input/?i=integrate+%5B(x+sin(x))%5E4,+%7Bx,+0,+PI%2F2%7D%5D ), n = 5 ( http://www.wolframalpha.com/input/?i=integrate+%5B(x+sin(x))%5E5,+%7Bx,+0,+PI%2F2%7D%5D )","['integration', 'definite-integrals', 'trigonometric-integrals']"
1683202,Sum modulo of two random variables with one uniformly distributed,"I have to use the following proposition, but since I'm not that into statistics, I don't know how to prove it formally. If there are two independent random variables $A$ and $B$ over $\{0,1,...,m-1\}$, with $A$ uniformly distributed, the random variable $C = A + B \text{ mod }m$ is also uniformly distributed (the distribution of $B$ is arbitrary). I think you can argue that if $B$ has a certain value $b$, then $A + b \text{ mod }m$ is uniformly distributed. Can anyone help me to write this down correctly?","['probability-theory', 'probability', 'statistics']"
1683205,"Infinite intersection of compact, path connected, nested sets is path connected?","I showed that given $A_1\supseteq A_2, ...$ compact, connected sets, $\bigcap_{i=1}^n A_n$ is  connected but is the statement true if we replace connected with path connected? Is there an counterexample of compact, path connected sets whose arbitrary intersection is not path connected?","['general-topology', 'metric-spaces', 'analysis']"
1683236,"Evaluate the following complex integral $\frac{1}{2\pi i}\int_{|z|=1}\frac{\overline{f(z)}}{z-a}\,dz$","Let , $f$ be analytic for $|z|<2$. Show that $$\frac{1}{2\pi i}\int_{|z|=1}\frac{\overline{f(z)}}{z-a}\,dz=\begin{cases}\overline{f(0)} &\text{ if } |a|<1\\\overline{f(0)}-\overline{f(1/a)}&\text{ if }|a|>1\end{cases}$$ By putting $z=e^{i\theta}$ , $\displaystyle\int_{|z|=1}\frac{\overline{f(z)}}{z-a}\,dz=\int_0^{2\pi}\frac{\overline{f(e^{i\theta})}}{e^{i\theta}-a}ie^{i\theta}\,d\theta=i\int_0^{2\pi}\frac{\overline{f\left(\overline{e^{-i\theta}}\right)}}{1-ae^{-i\theta}}\,d\theta$. Now putting , $e^{-i\theta}=t$ , it becomes $\displaystyle =-\int_{|t|=1}\frac{\overline{f(\bar t)}}{1-at}\,dt$. As $f(t)$ is analytic so $\overline{f(\bar t)}$ is also analytic. Now we can apply the residue theorem and finally I got the desire integral $=-\overline{f(0)}$ when $|a|<1$. My problem is about the sign . I got an extra negative sign. Please verify my proof & detect where I made mistake ...",['complex-analysis']
1683238,"Let $0<\alpha<1$. Prove that $\exists \, 0<x<1$ s.t. for all $n \in \Bbb{N}$, $\alpha^n < [nx]$","Let $[x]$ denote the fractional part of x. I'm quite lost about how to solve this problem. I suspect the solution is elementary, but all I can determine is that $x\notin\Bbb{Q}$.","['discrete-mathematics', 'real-analysis', 'limits']"
1683247,Is the Hölder random constant of the Brownian Motion Integrable?,"Let $\{B_t:t\in [0,1]\}$ be the standard one-dimensional Brownian motion on the closed unit interval. Fix $\gamma\in (0,1/2)$. It is well known that there is a positive random variable $K\equiv K(\gamma)$ such that for any pair $s,t\in [0,1]$ 
we have 
$$
|B_t-B_s|\leq K|t-s|^{\gamma} \qquad \text{a.s.}
$$
I would like to know if $K$ can be chosen so that $\mathbb{E}[K]<+\infty$.","['probability-theory', 'brownian-motion']"
1683309,Is this proof of the four color theorem for infinite graphs legit?,"So you got an infinite planar graph $G$. I will prove that it is four colorable. So, construct an infinite number of statements about graphs: The first is ""is four colorable"" Next, for each vertex $v$ the statement ""contains $v$"" Next, for each edge $e$, the statement ""contains $e$"" Every finite set of these statements is satisfiable. A finite set of those statements is satisfied by a finite subgraph of $G$ (which will be planar). Therefore, we can use the saturation principle to say there is a hypergraph $H$ that satisfies all of the above statements (a hypergraph is to a graph what a hyperreal number is to a real number). Color $H$ using four colors. Now, for each vertex $v$ in $G$, we color it with the color used for $v$ in $H$. Now for every $v_1$ and $v_2$ in $G$, $v_1$ and $v_2$ will be adjacent and different colors in $H$. Therefore, the coloring is valid for $G$. Here is the problem I have. We know by the transfer principle that all $v_1$ and $v_2$ that are adjacent in $G$ are adjacent in $H$. The problem is, we need to use an infinite number of such statements. This would seem to imply that the proof would need to be infinitely long, sense it refers to an infinite number of individual statements. For example, for an infinite hyperreal $H$, we know that $H > 1$, $H > 1 + 1$, $H > 1 + 1 + 1$, and so on, but we can't conclude that is bigger than every number, since that would be contradiction. Can you use the transfer principle an infinite number of times but still have a finite proof? Note: A hypergraph is a graph in a nonstandard model .","['graph-theory', 'nonstandard-analysis', 'infinite-graphs', 'coloring', 'discrete-mathematics']"
1683322,Wedge product of closed form each with integral periods has integral period?,"Suppose $\alpha$ and $\beta$ are closed forms on $M$ which have integral periods, i.e. for all $[A] \in H_*(M, \mathbb{Z})$ represented by a smooth cycle $A$, we have $\int_A \alpha \in \mathbb{Z}$, and similarly for $\beta$. Does $\alpha \wedge \beta$ have integral period?","['homology-cohomology', 'differential-topology', 'manifolds', 'algebraic-topology', 'differential-geometry']"
1683354,Methods for Integrating $\int \frac{\cos(x)}{\sin^2(x) +\sin(x)}dx$,"So I've found that there's the Weierstrass Substitution that can be used on this problem but I just want to check I can use a normal substitution method to solve the equation: $$\int \frac{\cos(x)}{\sin^2(x) +\sin (x)}dx$$ Let $u = \sin(x)$ $du = \cos(x)\, dx$ $dx = \frac {1}{\cos(x)\,} du$ Which becomes: $$\int \frac{\cos(x)}{u^2 + u} \frac{1}{\cos(x)}du$$ $$\int \frac{1}{u^2 + u}du$$ Factor out u from denominator: $$\int \frac{1}{u(u + 1)}du$$ Integrate as a partial fraction: $$\int \frac{1}{u} - \frac{1}{(u + 1)}du$$ Which integrates as: $$\ln|u| - \ln|(u + 1)| + C$$ Subtitute $u = \sin(x)$ back in and simplifies to: $$\ln \left|\frac{\sin(x)}{\sin(x)+1} \right| + C$$ Is this correct? From the Weierstrass Substitution, one gets: $$\ln \left|\tan \left(\frac{x}{2}\right)\right|-2\ln \left|\tan \left(\frac{x}{2}\right)+1\right| +C $$","['substitution', 'integration', 'trigonometry', 'calculus']"
1683399,Why define norm in $L_p$ in that way?,"Who first defined the norm in $L_p$ space as $$\left(\int{\lvert f(x) \rvert^p}\right)^{1/p}$$ Is there any reference for this? Is it just an simple extension from $L_2$? $L_p$ space has some really nice properties. I think it relies on the definition of this norm, but this definition does not seem to come naturally or maybe I just can't see how it comes naturally. I understand this definition gives a norm. I just want to know the underlying thought of it. Are those nice properties just by chance or does it actually have a relationship with this definition?","['reference-request', 'lp-spaces', 'measure-theory']"
1683426,"Is there a continuous function such that $\int_0^{\infty} f(x)dx$ converges, yet $\lim_{x\rightarrow \infty}f(x) \ne 0$? [duplicate]","This question already has answers here : Improper integral from 1 to infinity $\Rightarrow$ integrated function converges towards zero? (3 answers) Closed 5 years ago . Is there a continuous function such that $\int_0^{\infty} f(x)dx$ converges, yet $\lim_{x\rightarrow \infty}f(x) \ne 0$? I know there are such functions, but I just can't think of any example.","['real-analysis', 'examples-counterexamples', 'calculus']"
1683440,Prove that $(fg)^{(n)} = \sum_{k=0}^n \binom{n}{k}f^{(k)}(x)g^{(n-k)}(x)$,Assume $f$ and $g$ are differentiable at $x$. Prove that $(fg)^{(n)} = \sum_{k=0}^n \binom{n}{k}f^{(k)}(x)g^{(n-k)}(x)$ I am assuming here $fg = f(x) g(x)$. Then we can prove this via induction. If $n = 0$ we have $1 = 1$ which is true. Now assume it is true for some $m$ we need to show it is possible for $m+1$. We have $(fg)^m = \sum_{k=0}^m f^{(k)}(x)g^{m-k}(x)$. How do we show that?,['calculus']
1683462,Why would the category of topological spaces be a balanced category (i.e. monic epimorphisms are isomorphisms)?,"I've just read on this page that For example, $\mathsf {Set}$ (the cateogry of sets), $\mathsf {Grp}$ (the category of groups), and $\mathsf {Top}$ (the category of topological spaces) are all balanced. (Balanced means that all the monic epimorphisms are isomorphisms). I clearly understand for $\mathsf{Set}$ and $\mathsf{Grp}$, but isn't this wrong for $\mathsf{Top}$? For instance, $$f:[0,1[ \longrightarrow S^1 \qquad t \longmapsto e^{2πit}$$
is continuous and bijective but is not an isomorphism in $\mathsf{Top}$. Am I missing something there? Thank you for your comments!","['category-theory', 'general-topology']"
1683475,"How to prove $\forall x\in[0,\pi/2]: \sin{x}+\cos{x} \ge 1?$","Let $x\in [0,\frac{\pi}{2}]$ Let $x_0\in [0,\frac{\pi}{2}]$ Then $\sin{x_0}\ge 0$ and $\cos{x_0}\ge 0$ Then $2\sin{x_0}\cos{x_0}\ge 0$ and $\sin{x_0}+\cos{x_0}\ge 0$ Also $(\sin{x_0})^2+(\cos{x_0})^2=1$ Then $(\sin{x_0})^2+(\cos{x_0})^2+2\sin{x_0}\cos{x_0}\ge1$ Then $(\sin{x_0}+\cos{x_0})^2\ge1$ Then $(\sin{x_0}+\cos{x_0})\ge1$ Then $(\sin{x}+\cos{x})\ge1$ Therefore $\forall x\in[0,\pi/2]: \sin{x}+\cos{x} \ge 1.$ I am not sure if it misses anything. Could someone check?","['inequality', 'trigonometry']"
1683478,expectation of product of two independent normal random variables,"Say I have two normal independent random variables with nonzero means - X distributed as N(a,b) and Y as N(c,d). What is the mean of the product XY? I see that the distribution has been computed ( Is the product of two Gaussian random variables also a Gaussian? ) as a bessel function in an earlier question if it is 0 mean- but if the mean is not zero, is there a simple expression for E[XY] ?","['statistics', 'probability', 'normal-distribution']"
1683492,$f(x)$ is an analytic function in $\mathbb{R}$ such that $f(-x)f(x)=1$. What else can we find out about $f(x)$?,"Well, I know that there are some easy things we can say immediately: $f(0)= \pm 1$, follows immediately $f(x)=\pm 1$ is the obvious solution, so let's look for other solutions. Moreover, let's consider only the case $f(0)=1$ for now The obvious identities, such as $$f(x)^2=\frac{f(x)}{f(-x)}$$ And now for the series: $$f(x)=a_0+a_1 x+ a_2 x^2+a_3 x^3+\dots$$ We can immediately see by multiplying the series for $f(x)$ and $f(-x)$ that (for the case $f(0)>0$): $$a_0=1$$ $$a_2=\frac{a_1^2}{2}$$ $$a_4=a_1 a_3-\frac{a_1^4}{8}$$ $$a_6=a_1 a_5+\frac{a_3(a_3-a_1^3)}{2}+\frac{a_1^6}{16}$$ And so on. The coefficients for the even powers will be related to the ones for the odd powers. But that's the extent of what we can really say, or so I think. What else can we say about $f(x)$ based on these two restrictions only? And what is the weakest restriction we need to get $f(x)=c^x$?","['exponential-function', 'calculus', 'functional-equations']"
1683531,Prove $\sum\left| f\left(\frac 1 n\right)\right|^r$ converges for all $r>1$.,"I just got out of an introductory real analysis exam. I had this question which I tried to solve for more than $1.5$ hours and yet I accomplished nothing. Let $f:\Bbb R \to\Bbb R$ be a differentiable function with $f(0)=f'(0)=0$. Prove that 
  $$
\sum_{n=1}^\infty \left|f\left(\frac 1 n \right)\right|^r
$$
  Converges for all $r>1$. I tried to use the ratio test, root test, Lagrange's theorem, and a few more things I know but I couldn't crack this. Could someone explain how to solve this? (We only studied basic things about series)","['real-analysis', 'sequences-and-series']"
1683539,Implicit Function Theorem Application to 2 Equations,"So I think I understand how to use the Implicit Function theorem to find partial derivatives given one function but I am confused as to how to do this for 2 functions. I'm trying to find $\frac{\partial x}{\partial u}, \frac{\partial x}{\partial v}$ around the point $(1,-1,-1,2)$ given the equations $$x^2+2y^2+u^2+v=6$$$$2x^3+4y^2+u+v^2=9$$ and I thus calculated that $$Df(1,-1,-1,2)=\begin{bmatrix} 2 & -4 & -2 & 1\\6 & -8 & 1 & 4\end{bmatrix}$$ but I'm not sure where to go from here since the equation for the partial of the implicit function in my textbook is only for cases where we have one equation? Thanks in advance for any help! EDIT: So I've looked at the problem a bit further and I'm still confused as to how the derivative for an implicit function works in the case of $2+$ equations? Could anyone give an example with simpler equations so I could then apply it to this system? Thanks again.","['multivariable-calculus', 'implicit-function-theorem', 'implicit-differentiation']"
1683558,Probability that sum of independent uniform variables is less than 1,"I would like to determine the probability $\mathbb{P}(X_1+\dots+X_n\leq 1)$, where $X=(X_i)_{1\leq i\leq n}$ is a family of independent uniform random variables on $[0,1]$. My first idea is to do this by induction. The first three base cases are straightforward to determine and give us $\mathbb{P}(X_1\leq 1)=1$, $\mathbb{P}(X_1+X_2\leq 1)=\frac{1}{2}$ and $\mathbb{P}(X_1+X_2+X_3\leq 1)=\frac{1}{6}$, which suggests that $\mathbb{P}(X_1+\dots+X_n\leq 1)=\frac{1}{n!}$. Supposing this is true for a certain arbitrary integer $n$, I am having difficulties establishing the result for $n+1$, i.e. $\mathbb{P}(X_1+\dots+X_n+X_{n+1}\leq 1)=\frac{1}{(n+1)!}$. I believe the starting point should be:
$$\mathbb{P}(X_1+\dots+X_n+X_{n+1}\leq 1)=\mathbb{P}(X_1+\dots+X_n\leq 1-X_{n+1}),$$
and then somehow condition on $X_{n+1}$, but I am stuck at this point of the calculation. Any ideas of references to literature or even an alternative direct proof would be greatly appreciated.","['uniform-distribution', 'probability-theory', 'probability', 'probability-distributions']"
1683624,"Counterexample: Continuous, but not uniformly continuous functions do not preserve Cauchy Sequences","I want to prove this: There exists a continuous function $f:\mathbb{Q}\to\mathbb{Q}$, but not uniformly continuous, and a Cauchy sequence $\{x_n\}_{n\in\mathbb{N}}$ of rational numbers such that $\{f(x_n)\}_{n\in\mathbb{N}}$ is not a Cauchy sequence. More particular:
Does there exist a Cauchy sequence $\{x_n\}_{n\in\mathbb{N}}$ of rational numbers such that $\{x_n^2\}$ is not Cauchy? I think that would be weird, and the counterexample should be with some function that is continuous in $\mathbb{Q}$ but not in $\mathbb{R}$. Am I right? Which would be some example of that?","['uniform-continuity', 'real-analysis', 'examples-counterexamples', 'cauchy-sequences']"
1683631,Does there exist a $1$-form $\alpha$ with $d\alpha = \omega$?,"Let $\omega := dx \wedge dy$ denote the standard area form on $\mathbb{R}^2$. As the question title suggests, does there exist a $1$-form $\alpha$ with $d\alpha = \omega$?","['differential-topology', 'differential-forms', 'multivariable-calculus', 'manifolds', 'differential-geometry']"
1683638,Exercise $2.1.1$ - Differential Topology by Guillemin and Pollack,"If $U \subset \mathbb{R}^k$ and $V \subset H^k$ are neighborhoods of $0$, prove that there exists no diffeomorphism of V with U. Here, $H^k$ is simply the upper half-space. I tried to solve this problem with the continuity, but it is a dead-end. I have difficulty to solve this problem. Anyone could give me a hint to complete this problem?",['differential-topology']
1683658,The frog puzzle,"So here's the puzzle. You're poisoned in the jungle and the only way to save yourself is to lick a special kind of frog. To make matters worse, only the female of that species will do. Licking the male frog doesn't do anything.
The male and female frogs look identical. The only difference is that the male frog makes a sound and the female is silent. So you run through the jungle and spot a frog in front of you. Before you could start running towards it you hear a sound behind you. You turn around and spot two frogs there. There's only time to run to one side. Now, the best course of action is to run towards the two frogs and lick both. The reasoning is that there are 4 possible combinations of two frogs and knowing that one of them is male eliminates only one of those possibilities. Of the remaining three, two of them have at least one female frog. This gives you a $\frac 2 3$ chance of survival as opposed to a $\frac 1 2$ with the single frog. Now here's my problem. The reason this works is because you don't know which frog made the sound. If you did, you'd have a $50\%$ chance with the other one. But wouldn't that imply that, if you for some reason turned around earlier to see which one made the sound, you would decrease your chances of survival? What's the explanation here?","['intuition', 'puzzle', 'probability']"
1683662,Intuition behind definition of spinor,"Some time ago I searched for the definition of spinors and found the wikipedia page on the subject. Although highly detailed the page tries to talk about many different constructions and IMHO doesn't give the intuition behind any of them. As far as I know physicists prefer to define spinors based on transformation laws (as with vectors and tensors), but all due respect, I find these kind of definitions quite unpleasant. Vectors and tensors can be defined in much more intuitive ways and I believe the same happens with spinors. In that case, how does one really define spinors without resorting to transformation properties and what is the underlying intuition behind the definition? How the definition relates to the idea of spin from Quantum Mechanics? In Wikipedia's page we have two definitions. One based on spin groups and another based on Clifford Algebras. I couldn't understand the intuition behind neither of them, so I'd like really to get not just the definition but the intuition behind it.","['spin-geometry', 'abstract-algebra', 'mathematical-physics', 'representation-theory', 'clifford-algebras']"
1683696,"Diffeomorphism group $\text{Diff}_\omega(D^2, \partial D^2)$, exact differential form.","Let $D^2$ denote the closed unit disk in $\mathbb{R}^2$. Let $\omega = dx \wedge dy$ denote the standard area form on $\mathbb{R}^2$ (and on $D^2$ by restriction). Let $\phi$ be a diffeomorphism of $D^2$ which is equal to the identity in a neighborhood of $\partial D^2$, and which preserves area; i.e. $\phi^*\omega = \omega$. We denote the group of such diffeomorphisms by $\text{Diff}_\omega(D^2, \partial D^2)$. I know from here that there is a $1$-form $\alpha$ with $d\alpha = \omega$. I have two questions. Is $\phi^* \alpha - \alpha$ exact? Is $\phi^*\alpha - \alpha$ equal to $df$ for some smooth function $f$?","['multivariable-calculus', 'differential-forms', 'symplectic-geometry']"
1683712,Probability that binomial random variable is greater than another,"Let $X$ and $Y$ be two independent random variables with respective distributions $B(n+1,\frac{1}{2})$ and $B(n,\frac{1}{2})$. I am trying to determine $\mathbb{P}(X>Y)$. So far, I have written that: $$\mathbb{P}(X>Y)=\sum_{k=0}^n \mathbb{P}(X>k)\mathbb{P}(Y=k)=\frac{1}{2^{2n+1}}\sum_{k=0}^n {n \choose k}\sum_{j=k+1}^{n+1}{n+1 \choose j}.$$
However, at this point, I am not sure how to compute the double sum involving the binomial coefficients. Is there any way we can re-arrange the terms to make use of the identity $\sum_{k=0}^n {n \choose k}=2^n$?","['probability-theory', 'binomial-coefficients', 'probability', 'probability-distributions']"
1683735,Is proving a theorem the same as showing that a logical expression is a tautology?,"The question I am trying to answer is this: Prove that $$((\neg r \lor \neg f) \to (s \land l)) \land (s \to t) \land (\neg t) \to r $$ is a theorem using a deductive proof method. Can someone help me with this? Do I have to show that this is a tautology in order to prove that it is a theorem? If that is the case, I only know how to do that using truth tables and I do not think that is what I am supposed to do. This is what I have done so far: $$(\neg (r \land \neg f) \to (s \land l)) \land  (\neg s \lor t) \land (\neg t) \to r $$ $$(\neg (r \land \neg f) \to (s \land l)) \land  (\neg s \lor \neg t) \land (t \lor \neg t) \to r $$ $$(\neg (r \land \neg f) \to (s \land l)) \land  (\neg s \lor \neg t) \land T \to r $$ $$((r \land \neg f) \lor (s \land l)) \land  (\neg s \lor \neg t) \land T \to r $$ Could someone please help? Thanks.","['proof-explanation', 'logic', 'discrete-mathematics']"
1683743,why is sigma field important when a sample space is uncountable?,I am a undergraduate student studying mathematical statistics. I am learning about sigma field and I dont understand why sigma field is so important when sample space is uncountable. Can anyone tell me what properties of sigma field make sigma field so important?,"['elementary-set-theory', 'probability-theory', 'statistics']"
1683753,"Smooth representative $f: S^{2n - 1} \to S^n$, do we have $f^*\omega = d\alpha$?",Let $[f] \in \pi_{2n - 1}(S^n)$. Choose a smooth representative $f: S^{2n - 1} \to S^n$. Let $\omega$ be a smooth $n$-form on $S^n$ with$$\int_{S^n} \omega = 1.$$Do we have that$$f^*\omega = d\alpha$$for some $(n -1 )$-form $\alpha$ on $S^{2n - 1}$?,"['differential-topology', 'manifolds', 'differential-forms', 'algebraic-topology', 'differential-geometry']"
1683795,Variation of argument of a complex function,"Variation of Argument : Definition( Collect from my book ) : Let $f$ be analytic inside and on a simple closed contour $C$ except possibly for poles inside $C$ and $f(z)\not=0$ on $C$ . As $z$ describes $C$ once in the positive direction in the $z$ -plane , the image point $w=f(z)$ describes a closed curve $\Gamma=f(C)$ in the $w$ -plane in a particular direction which determines the orientation of the image curve $\Gamma$ . Since $f(z)\not=0$ on $C$ , $\Gamma$ never passes through the origin in the $w$ -plane. Let $w_0$ be the arbitrary fixed point on $\Gamma$ and let $\phi_0$ be the argument of $w_0$ . Then let , $\arg z$ run continuously from $\phi_0$ , as the point begins at $w_0$ and traverses $\Gamma$ once in the direction of orientation assigned to it by $w=f(z)$ . If $w$ returns to the staring point $w_0$ , then $\arg w$ assume a particular value of $\arg w_0$ which we denote by $\phi_1$ . We define , $$\Delta_C\arg f(z)=\phi_1-\phi_0.$$ I am unable to understand the meaning of bold sentences. Can anyone explain these sentences with a proper example or by a rough figure such that I can realize what actually the variation of argument $\Delta_C f(z)$ over $C$ ? If you can provide an example of a function and a simple closed contour $C$ explaining the value of $\Delta_C f(z)$ over $C$ then it is better to me.","['complex-geometry', 'complex-analysis', 'complex-numbers', 'complex-integration']"
1683804,Tighter tail bounds for subgaussian random variables,"Let $X$ be a random variable on $\mathbb{R}$ satisfying $\mathbb{E}\left[e^{tX}\right] \leq e^{t^2/2}$ for all $t \in \mathbb{R}$ . What is the best explicit upper bound we can give on $\mathbb{P}[X \geq x]$ for $x > 0$ ? A well-known upper bound can be obtained by applying Markov's inequality to the moment generating function of X: For $t>0$ , $$\mathbb{P}[X \geq x] = \mathbb{P}\left[e^{tX}\geq e^{tx}\right] \leq \frac{\mathbb{E}\left[e^{tX}\right]}{e^{tx}} \leq e^{t^2/2-tx}.$$ Setting $t=x$ , yields $$\mathbb{P}[X \geq x] \leq e^{-x^2/2}.$$ How tight is this? Examples of such $X$ include the standard Gaussian and Rademacher distributions. Both satisfy stronger tail bounds, as illustrated by the following plot. This makes me wonder whether stronger bounds hold for all such $X$ . Some more precise questions: Define $$f(x) = \sup \left\{ \mathbb{P}[X\geq x] : X \text{ a random variable satisfying } \mathbb{E}\left[e^{tX}\right] \leq e^{t^2/2} \text{ for all } t \in \mathbb{R} \right\}.$$ What is $f(1)$ ? (We know $f(1) \in \left[\frac12,e^{-1/2}\right]$ , by the Rademacher example and the above tail bound.) What is $\limsup_{x \to \infty} f(x) \cdot e^{x^2/2}$ ? (We know it is $\leq 1$ , by the above tail bound.) What is $\limsup_{x \to \infty} f(x) \cdot e^{x^2/2} \cdot {x}$ ? (We know it is $\geq 1/\sqrt{2\pi}$ , by the Gaussian example [link] .) Any improved bounds on $f$ would interest me. Is there a closed form expression for $f$ ? or some way to numerically approximate $f$ ? A follow-up question would be: Are there are any natural assumptions that in addition to subgaussianity would imply $\mathbb{P}[X \ge x] \le O\left(\frac{e^{-x^2/2}}{x}\right)$ .","['gaussian-integral', 'distribution-tails', 'probability', 'orlicz-spaces']"
1683809,"Is it possible to ""depress"" any term in a polynomial with a suitable substitution?","If we have a degree $n$ polynomial $$p(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots + a_1x+a_0$$with coefficients in a field, say $\Bbb C$, for concreteness, it is well known that the substitution $y= x + \frac{a_{n-1}}{na_n}$ ""kills"" the $a_{n-1}x^{n-1}$ term, that is, we get something of the form $$b_ny^n +b_{n-2}y^{n-2}+\cdots+ b_1y+b_0.$$
I think of that substitution as ""translating"" the polynomial such that the mean of the roots ""in the new variable $y$"" is zero. Everything seems to work fine since the arithmetic mean of $n$ numbers is a linear function of them, in some sense. Question: is there a suitable substitution that eliminates an arbitrary term $a_kx^k$, $0\leq k < n $, of our choice? Working with $n=3$ around, it seems that there are two possibilities for ""depressing"" the $a_1x$ term (given by the quadratic formula), and these expressions are very close to the closed expressions for the critical points of a third degree polynomial. I don't know if this is a coincidence or not, but I digress. Also, thinking of Vieta's formulae doesn't seem to lead anywhere, because of the non-linearity of the expressions involved. I googled around a bit, and this might be related to Tschirnhaus transformations, but my background in algebra is weak and I don't know enough Galois theory to digest some of that information (and that's probably overkill). I would like some intuition behind such substitutions instead of the obvious ""write $y = x+t$, plug in, put the desired coefficient equal to zero and solve for $t$"", if said substitutions do exist - I'd like some intuition that avoids computations as most as possible (something geometric, maybe?) Thanks, and sorry for the rant. (You can edit in tags that might be also relevant, I'm a bit lost here)","['abstract-algebra', 'polynomials', 'symmetric-polynomials']"
1683820,"Text recommendations for linear algebra (tensors, jordan forms)","I'm having extreme difficulty trying to understand to topic of tensor products, freespaces, and jordan forms. Are there any text books that take an elementary approach to these topics that you may recommend? I am currently using advanced linear algebra by roman, but other sources would be appreciated. Thanks!","['tensor-products', 'jordan-normal-form', 'book-recommendation', 'linear-algebra']"
1683845,How to prove this topology equality?,"Suppose $(A,\tau_A)$ is the subspace of $(X,\tau)$, show that for all $B\in 2^A$ the following relationship holds:
$$\text{int}B=\text{int}_A B\cap \text{int} A.$$
Here subtopology $\tau_A$ is defined as follows: $V\in\tau_A$ if there exists some $U\in\tau$ such that $V=U\cap A$.  And $\text{int}$ denotes the interior of a set, which is defined as the union of every open subset it contains. So here I go. From definitions
$$\text{RHS}=\text{int}A\cap (\bigcup_{C\subset B,C\in\tau_A}C)=\text{int}A\cap (\bigcup_{C^*\cap A\subset B,C^*\in\tau}(C^*\cap A))=(\bigcup_{C^*\cap A\subset B,C^*\in\tau}(C^*\cap \text{int}A)).$$
Don't know what to do next. Most probably I'm on the wrong track. So I really need help from you guys now. Clues and complete answers are both appreciated. Thanks in advance.","['general-topology', 'elementary-set-theory']"
1683849,Why do Clifford bivectors represent the orthogonal Lie algebra?,"It takes a long, painful, but straightforward calculation to see that the commutators of grade one elements $[e_i, e_j]$ of a Clifford algebra $\mathrm{Cl}(p,q)$ have exactly the same commutation relations as the rotations in the $(ij)$ plane that form the basis of $\mathfrak{so}(p,q)$. There is a different path to this result that I have yet to understand thoroughly, but on the surface it seems to involve a similarly unilluminating calculation to prove that $\mathrm{ad}_{[e_i, e_j]}$ maps vectors into vectors and (linear combinations of) bivectors into bivectors. Is there a way to understand why it couldn’t be otherwise? (Not necessarily a complete proof, but at least some motivation to carry out the calculation knowing what to expect.)","['clifford-algebras', 'linear-algebra', 'lie-algebras']"
1683869,$Tr(A^2)=Tr(A^3)=Tr(A^4)$ then find $Tr(A)$,"Let $A$ be a non singular $n\times n$ matrix  with all eigenvalues real and 
$$Tr(A^2)=Tr(A^3)=Tr(A^4).$$Find $Tr(A)$. I considered $2\times 2$ matrix $\begin{bmatrix}a&b\\c&d\end{bmatrix}$ and tried computing traces of $A^2,A^3,A^4$ and ended up with following $Tr(A^2)=Tr(A)^2-2\det(A)$ $Tr(A^3)=Tr(A)^3-3Tr(A)\det(A)$ $Tr(A^4)=Tr(A)^4-4Tr(A)^2\det(A)+2\det(A)$ I have no idea how to proceed from here...","['abstract-algebra', 'trace', 'linear-algebra', 'determinant']"
1683878,How can trigonometric functions be negative?,"I cannot understand why $\cos(180-\theta)$  say is $-\cos\theta$. This is probably because my teacher first introduced trigonometry in triangles. I do not understand it for obtuse angles because I cannot think of them in a right triangle. I realised that I couldn't feel what I had read ""in my spleen"" when I was looking at the proof for the law of cosines in an obtuse-angled triangle. I have spent quite some time thinking about how the ""$-\cos\theta$"" entered the derivation. I cannot fully understand, why the negatives which work in the $XY$-plane work in triangles. For instance, since in a triangle, all the sides are positive while taking the ratio of sides we do not get any negative values but how then does $\cos 120^{\circ}=-0.5$. My brain is in a mess right now. I would appreciate it if someone could help me out or suggest something that I can do. Let me illustrate what I can't get around. It is given that in the triangle $\angle BAC=120$ degrees,$|AC|=3$ and that D is the foot of the perpendicular from C to BD. Then $\cos\angle BAC=-0.5=\dfrac{AD}{AC} \implies AD=-1.5$ ?",['trigonometry']
1683905,"How to find the shortest path between opposite vertices of a cube, traveling on its surface?","I am stuck with the following problem that says: Let $A,B$ be the ends of the longest diagonal of the unit cube . The length of 
  the shortest path from $A$ to $B$ along the surface is : $\sqrt{3}\,\,$  2.$\,\,1+\sqrt{2}\,\,$  3.$\,\,\sqrt{5}\,\,$ 4.$\,\,3$ My Try: So, the length of the longest diagonal $AB=\sqrt{3}$. If I reach from $A$ to $B$ along the surface line $AC+CD+BD$, then it gives $3$ units. But the answer is given to be option 3. Can someone explain? Thanks in advance for your time.",['geometry']
1683908,Solving linear recursive equation $a_n = a_{n-1} + 2 a_{n-2} + 2^n$.,"I wish to solve the linear recursive equation: $a_n = a_{n-1} + 2a_{n-2} + 2^n$, where $a_0 = 2$, $a_1 = 1$. I have tried using the Ansatz method and the generating function method in the following way: Ansatz method First, for the homogenous part, $a_n = a_{n-1} + 2a_{n-2}$, I guess $a_n = \lambda^n$ as the solution, and substituting and solving for the quadratic, I get $\lambda = -1, 2$. So, $a_n = \alpha (-1)^n + \beta 2^n$. Then, for the inhomogenous part, I guess $a_n = \gamma 2^n$, to get $\gamma 2^n = \gamma 2^{n-1} + 2\gamma 2^{n-2} + 2^n$, whence $2^n=0$, which means, I suppose, that this guess is not valid. These are the kind of guesses that usually work, so I don't know why it fails in this particular case, and what to do otherwise, so I tried the generating function method. Generating function method Let
$$
A(z) = \sum_{i=0}^{\infty} a_k z^k
$$
be the generating function for the sequence $\{ a_n \}_{n \in \mathbb{N} \cup {0}}$. Then, I try to write down the recursive relation in terms of $A(z)$:
$$
A(z) = zA(z) + 2z^2 A(z) + \frac{1}{1-2z} + (1 - 2z),
$$
where the last term in the brackets arises because of the given initial conditions. Then, solving for $A(z)$,
$$
\begin{align}
A(z) &= \frac{1}{(1+z)(1-2z)^2} + \frac{1}{1+z}\\
&= \frac{2}{9}\frac{1}{1-2z} + \frac{2}{3}\frac{1}{(1-2z)^2} + \frac{10}{9}\frac{1}{1+z}\\
&=\frac{2}{9} \sum_{k=0}^{\infty} 2^k z^k + \frac{2}{3} \sum_{k=0}^{\infty} (k+1)2^k z^k + \frac{10}{9} \sum_{k=0}^{\infty} (-1)^k z^k\\
&= \sum_{k=0}^\infty \frac{(3k+4)2^{k+1} + (-1)^k 10}{9} z^k.
\end{align}
$$
So,
$$
a_k = \frac{(3k+4)2^{k+1} + (-1)^k 10}{9}.
$$
But then, $a_1 = 2$, whereas we started out with $a_1 = 1$. At first, I thought that maybe the generating function method did not work because some of the series on the right hand side were not converging, but they all look like they're converging for $|z| < 1/2$. I rechecked my calculations several times, so I don't think there is any simple mistake like that. It would be great if someone could explain to me what exactly is going wrong here.",['sequences-and-series']
1683929,Uniform lower bound of sequence of linear maps on a Banach space,"Suppose a sequence of $T_j:X\to\mathbb{R}$, with $X$ Banach, has the following property: 
$$\forall j:\|T_j\|\geq c>0$$
Then we have for all $n$, 
$$\exists x\in X\setminus \{0\}:\forall j\leq n:|T_jx|\geq\frac12c\|x\|$$
Does anybody know how to prove or disprove this? It seems like a reverse of the uniform boundedness principle somehow, but perhaps this analogy is incorrect.","['inequality', 'banach-spaces', 'normed-spaces', 'functional-analysis', 'sequences-and-series']"
1683984,Difference operator: Proof by induction that $\Delta^k (X_t)= k!a_k+\Delta^k (Y_t)$,"Hello I am having issues with the following exercise. I have to prove that $$\Delta^k (X_t)= k!a_k+\Delta^k (Y_t)$$ where $X_t = m_t +Y_t=\sum_{j=0}^ka_jt^j+Y_t$ for $t \in \mathbb {Z}$. Note: $\Delta$ is the backward difference operator: $\Delta X_t = X_t - X_{t-1}$,  $\Delta^{j} X_t = \Delta(\Delta^{j-1} X_tX_t)$, $\Delta^{0} X_t = X_t$ etc. Basis step: 
I have managed to show this for k=1. Induction step:
I assume that it holds  for k and show that it holds for k+1. 
However, for some reason I just cannot figure out how to write it up for $k+1$ and show that it also holds. I can show it if I choose $k$ to be a certain number say 2 or 3 etc. but not for $k+1$. What I have so far $$\Delta^{k+1} (X_t)=\Delta(\Delta^{k} (X_t) ) = \Delta \left(k!a_k+\Delta^k (Y_t)\right)$$ where I used the assumption that it holds for k. But how do I continue from here?  What do I do with $k!a_k$ if I multiply with $\Delta$? UPDATE: I also tried the following 
$$\Delta^{k+1} (X_t)=\Delta^{k}(\Delta (X_t) ) = \Delta^{k}(X_{t}- X_{t-1} )= \Delta^{k}X_t - \Delta^{k}X_{t-1} = \Delta^{k}\left((m_t + Y_t) - (m_{t-1} + Y_{t-1})\right) = \Delta^{k+1}m_t + \Delta^{k+1}Y_t $$ But what about $$\Delta^{k+1}m_t$$ I order to finish the prove I have to show that $$\Delta^{k+1}m_t= (k+1)!a_{k+1}$$ Again I managed to do this for the basis step and I assume it holds for k. But same problem as before: I cannot figure out how to do it for k+1. Hope some one can help.  I am stuck. Best Husky","['time-series', 'induction', 'statistics', 'proof-writing', 'finite-differences']"
1684029,Why is being onto necessary for a function to have inverse?,"I know that a function needs to be one-to-one so that it can have an inverse but could someone please explain why a function (in addition to being one-to-one) needs to be onto so that it can have inverse? We define the function $f:A\rightarrow B$ as a rule that assigns for
  each $a\in A$, one specific member $f(a)\in B$. The range is denoted
  by $f(A)$ and is the set: $$f(A)=\{f(x)|x\in A\}$$ Generally, $f(A)\subset B$ and if
  $f(A)\subseteq B$, then $f$ is said to be onto $B$.",['calculus']
1684052,"If $A=\frac{1}{\frac{1}{1980}+\frac{1}{1981}+\frac{1}{1982}+........+\frac{1}{2012}}\;,$ Then $\lfloor A \rfloor\;\;,$","If $$A=\frac{1}{\frac{1}{1980}+\frac{1}{1981}+\frac{1}{1982}+........+\frac{1}{2012}}\;,$$ Then $\lfloor A \rfloor\;\;,$ Where $\lfloor x \rfloor $ Represent floor fiunction of $x$ My Try:: Using $\bf{A.M\geq H.M\;,}$ We get $$\frac{1980+1891+1982+....+2012}{33}>\frac{33}{\frac{1}{1980}+\frac{1}{1981}+\frac{1}{1982}+........+\frac{1}{2012}}$$ So $$\frac{1}{\frac{1}{1980}+\frac{1}{1981}+\frac{1}{1982}+........+\frac{1}{2012}}<\frac{1980+1981+....+2012}{(33)^2}=\frac{1996}{33}\approx 60.5<61$$ Now how can i prove that the above expression $A$ is $>60$ Help me, Thanks",['sequences-and-series']
1684077,why we use uniform distribution on accept reject method?,"the accept-reject method have the following algorithm: Given known random number
generators $U \sim Unif(0,1)$ and $X \sim g$, we can generate $Y
\sim f$ by the following algorithm. Let $c$ be a constant such
that $f(x)
\leq cg(x)$ for all $x$. Step 1. Generate $X \sim g$, $U \sim Unif(0,1)$. Step 2. Accept $Y = X$ if $U \leq \frac{f(X)}{cg(X)}$ otherwise go
to Step 1. I don't understand why we generate $U \sim \mathcal{U}(0,1)$ Can anybody help me? Thanks.","['monte-carlo', 'probability-theory', 'probability', 'statistics']"
1684121,Contractive Operators on Compact Spaces,"Suppose that $T: M \to M$ is a compact contractive Operator on a nonempty compact subset $M$ of a complete metric space $X$. Show that $T$ has a unique fixed point. Further show that the sequence defined by $x_{n+1}=Tx_n$ converges to the fixed point from an arbitrary point $x_0 \in M$. By a contractive operator I mean there exists a $1 \gt k \ge 0$ such that $d(Tx,Ty) \le k d(x,y)$. My try: For the first part Let $S=\{(x,y): 0 \lt a \le d(x,y) \le b\}$. Let $f: M \times M \to K$ such that $f(x,y)=\frac{d(Tx,Ty)}{d(x,y)}$. $f$ is continuous on $S$ and $S$ being compact, $f$ attains its maximum say $K(a,b) \lt 1$ . Then by generalized fixed point theorem ( Generalized Fixed Point Theorem ) $T$ has a unique fixed point. I have trouble showing the second part. Since $M$ is compact, every such $x_n$ will have a convergent subsequence say $x_{n_{k}}$ which goes to say $x'$. I need to show that all convergent subsequences go to $x$ which is the fixed point and somewhere use $x_{n+1}=Tx_n$. I am unable to do so. Thanks for the help!!","['banach-spaces', 'functional-analysis', 'nonlinear-analysis', 'fixed-point-theorems', 'banach-fixed-point']"
1684123,Applications of the Dedekind-Hasse criterion,"It is a fact that an integral domain $R$ is a principal ideal domain if and only if there is a Dedekind-Hasse function $|R|\setminus\{0\}\xrightarrow{\ \ \delta\ \ }\mathbb{N}$ on $R$, i.e. a function such that for $0\not=a\in R$ and $b\in R$ arbitrary either $a\mid b$ or $\delta(c)<\delta(a)$ for some $0\not=c\in(a,b)$. Proof. Let $\delta$ be a Dedekind-Hasse function, $0\not=I\trianglelefteq R$ an ideal. Chose $0\not=a\in I$ of minimal degree. Clearly, $(a)\le I$. Conversely suppose $b\in I$, $b\notin (a)$. Then there exists $0\not=c\in(a,b)\le I$ of degree less then $a$, a contradiction. If $R$ is a principal ideal domain, then it is factorial, so the definition
$$a\longmapsto\text{Number of prime factors of }a$$
is meaningful. If $0\not=a,b\in R$ and $a\nmid b$, then $\gcd(a,b)$ properly divides $a$, hence has strictly less prime factors. But $\gcd(a,b)\in (a,b)$ by Bézout's Lemma. $\square$ Now it can be shown with some bit of commutative algebra that $\mathbb{R}[X,Y]/(X^2+Y^2+1)$ is a principal ideal domain, see for example here . Is it possible to give a concrete description of a Dedekind-Hasse function on the ring $R=\mathbb{R}[X,Y]/(X^2+Y^2+1)$? The most well-known application of the Dedekind-Hasse criterion is probably to some rings of integers of quadratic number fields, e.g. the algebraic norm is a Dedekind-Hasse function on $\mathcal{O}_{\mathbb{Q}(\sqrt{-19})}
$. Are there other applications of the Dedekind-Hasse criterion to find some non-obvious examples of principal ideal domains?","['principal-ideal-domains', 'abstract-algebra', 'ring-theory', 'commutative-algebra']"
1684143,Is there a null set that is not a Borel set? [duplicate],"This question already has answers here : Lebesgue measurable but not Borel measurable (2 answers) Closed 8 years ago . In my module notes, if $A$ is a Borel and $m(A)=0$, then it is not necessarily true that any subset $B$ of $A$ (with $m(B=0)$) is Borel. So I am wondering if there is a null set that is not a Borel set?",['measure-theory']
1684178,Solution of $y'' - (k+\pi ^2)y=0$,"Give this D-E:
$$y'' - (k+\pi ^2)y=0$$ $$k>0$$
$$y(0)=0$$
$$y(1)=1$$ How can I get to this solution: 
$$y=\frac{ \sinh \sqrt{k+\pi ^2}x}{\sinh \sqrt{k+\pi ^2}} $$ What I did: ***$$r^2-(k+\pi ^2)=0 $$
$$r^2=(k+\pi ^2) $$
$$r=+-\sqrt{(k+\pi ^2}) $$
The solution then is:
$$y=Ae^{\sqrt{(k+\pi ^2})x} +Be^{-\sqrt{(k+\pi ^2})x} $$ Managed also to get : $$A=-B$$
$$B=\frac{1}{-e^{\sqrt{(k+\pi ^2}}+e^{-\sqrt{(k+\pi ^2}}}$$
And what next?***",['ordinary-differential-equations']
1684193,Verifying the Jacobi identity for the semidirect product of Lie algebras,"Given Lie algebras $S$ and $I$ and a Lie homomorphism $\theta \colon S\to \operatorname{Der} I$ , we have the semidirect product to be the vector space $S\oplus I$ with operation $$
  (s_{1},x_{1})(s_{2}x_{2})
  :=
  ([s_{1},s_{2}],[x_{1},x_{2}]+\theta(s_{1})x_{2}-\theta(s_{2})x_{1}).
$$ Show that this is a Lie algebra. So I can easily verify the skew-symmetric but I can't seem to work out a nice way of proving the Jacobi identity. Am I missing a simple trick or must you perform the tedious calculation to show this? Thanks.","['abstract-algebra', 'semidirect-product', 'lie-algebras']"
1684228,Integral $\int \frac{1}{\left(x-2\right)^3\sqrt{3x^2-8x+5}}dx$,"How to solve the integral $$\int \frac{1}{\left(x-2\right)^3\sqrt{3x^2-8x+5}}dx$$ I am pretty sure that a certain substitution should work.
I tried using $x-2=\frac{1}{t}$ and got $$\int \:\frac{-\frac{dt}{t^2}}{\frac{1}{t^3}\sqrt{3\left(\frac{1}{t}+2\right)^2-8\left(\frac{1}{t}+2\right)^2+5}}$$ After some simplification: $$\int \:\frac{tdt}{\sqrt{3\left(\frac{1}{t}+2\right)^2-8\left(\frac{1}{t}+2\right)^2+5}}$$ Any this point, I'm lost.","['indefinite-integrals', 'integration', 'calculus']"
1684238,"closure of inverse image is subset of inverse image of closure, given that $f$ is continuous","Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be a continuous function. Prove then that $$ \overline{f^{-1}(X)} \subset f^{-1} (\overline{X}) $$  for every $X \subset \mathbb{R}$. Attempt at proof: Let $a \in \overline{f^{-1}(X)}$ be arbitrary. Then by definition we have $\forall \delta > 0$ that $$ ] a - \delta, a + \delta [ \cap f^{-1}(X) \neq \emptyset. $$ Let $x$ be an element in this intersection. Thus $x \in ]a - \delta, a + \delta [ $ and $x \in f^{-1}(X)$. It follows that $f(x) \in X$. Because $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous $a$, we can find $\forall \epsilon > 0$ a $\delta > 0$ such that $\forall x \in \mathbb{R}$  it holds that $$ | f(x) - f(a) | < \epsilon $$ if $| x - a | < \delta$. Now we have $$f^{-1} (\overline{X})  = \left\{a \in \overline{X} \mid f(a) \in f(\overline{X}) \right\}. $$ This means I have to show that $a \in \overline{X}$ and then show that $f(a) \in f(\overline{X})$. This is the part where I'm stuck. Help would be appreciated.","['general-topology', 'real-analysis', 'continuity']"
1684258,Solve the Integral $\int \:\frac{3x+1}{\left(x^2-x-6\right)\sqrt{3x^2+4x+7}}dx$,"$$\int \:\frac{3x+1}{\left(x^2-x-6\right)\sqrt{3x^2+4x+7}}dx$$ Can someone tell me what kind of substitution would work here and if this type of integral belongs to a certain group, that can be solved with a certain type of substitution, also a link to that type would be greatly appreciated. P.S. There is no need to solve the problem for me, getting the right substitution and some explanation behind it is really plenty, thanks in advance.","['integration', 'calculus']"
1684265,"If $\,f''(x) \ge f(x)$, for all $x\in[0,\infty),$ and $\,f(0)=f'(0)=1$, then is $\,f(x)>0$?","Let $f:[0,\infty) \to \mathbb R$ be a twice differentiable function, such that 
$\,f''(x) \ge f(x)$, for all $x\in\ [0,\infty)$, and
$ f(0)=f'(0)=1$. Can we deduce that $f$ is increasing? I feel like it is, but I cannot see it. I can only show that to get it increasing it is enough to show that $f$ is non negative .","['derivatives', 'real-analysis', 'ordinary-differential-equations', 'calculus']"
1684343,What is the probability that the Golden State Warriors will break the NBA regular season record of wins?,"There are $82$ games in a regular season, and the current record is held by the Chicago Bulls, at 72-10. As of yesterday (March 4th 2016), the GSW season performance stood at 55-5. Assuming they maintain this record or do better,they need to win at least 18 of their next 22 games.  I calculated the probability of them breaking the Bulls' record as ~7.4%, since each game's outcome is a binomial probability, and the probability of them winning so far is 55/60. I used the following code in R: p = 11/12 #55/60, their current record
q = 1-p
i = c(0:22)
(choose(22,18)*(p^18)*(q^4))/sum(choose(22,i)*(p^i)*(q^(22-i))) But if they keep winning, the probability p of their winning a game will keep changing. How can we take that into consideration while calculating the overall probability?",['probability']
1684390,Two group structures on the cotangent bundle of a Lie group. Are they related?,"Let $G$ be a compact connected Lie group, and consider its cotangent bundle $T^*G$. There are two ways of viewing this space. Using left translation, we can trivialize $T^*G\cong G\times{\frak g}^*$. (Where ${\frak g}$ is the Lie algebra of $G$.) Moreover, $G$ acts on ${\frak g}^*$ via the coadjoint action, so we can consider the semi-direct product
$$G\ltimes_{{\rm Ad}} {\frak g}^*.$$
This gives a Lie group structure on $T^*G$. On the other hand, there is a diffeomorphism $T^*G\cong G_{\Bbb C}$, where $G_{\Bbb C}$ is the complexification of $G$. (It comes from the map $G\times{\frak g}\to G_{\Bbb C},(g,X)\mapsto ge^{iX}$.) This gives a second Lie group structure on $T^*G$. Does these two group structures have any interesting relation? I don't have a counterexample, but I don't suspect the group structures to be the same. However can we still say something about their relation? In particular: Can we construct the group structure of $G_{\Bbb C}$ in a natural way from the one on $G\ltimes_{{\rm Ad}} {\frak g}^*$? Is there an interesting subset of $T^*G$ on which they coincide?","['differential-geometry', 'group-theory', 'lie-groups']"
1684412,Find sum of $1+\frac{1}{3}+\frac{1}{5}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}+\frac{1}{7}+\frac{1}{9}+\frac{1}{11}-\cdots$,"Find the sum of the following series : 
$$1-\frac{1}{2}-\frac{1}{4}+\frac{1}{3}-\frac{1}{6}-\frac{1}{8}+\frac{1}{5}-\frac{1}{10}-\frac{1}{12}+\cdots$$ and 
$$1+\frac{1}{3}+\frac{1}{5}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}+\frac{1}{7}+\frac{1}{9}+\frac{1}{11}-\cdots$$ For the first series , we have , $$(1-\frac{1}{2})-\frac{1}{4}+(\frac{1}{3}-\frac{1}{6})-\frac{1}{8}+(\frac{1}{5}-\frac{1}{10})-\frac{1}{12}+\cdots$$
$$=\frac{1}{2}-\frac{1}{4}+\frac{1}{6}-\frac{1}{8}+\cdots=\frac{1}{2}\log 2$$ But I am unable to set the second series to find its sum..Please help.","['real-analysis', 'sequences-and-series', 'analysis']"
1684421,When adding zero really counts ...,"Note: Although adding zero has usually no effect, there is sometimes a situation where it is the essence of a calculation which drives the development into a surprisingly fruitful direction. Here is one example of what I mean. The Goulden-Jackson Cluster Method counts words built from a finite alphabet which are not allowed to contain so-called bad words . This method nicely presented (and something to chuckling about) by J. Noonan and D. Zeilberger is very efficient and the reason for it's efficiency is due to a clever addition of zeros. Let's denote the alphabet $V$, the language $\mathcal{L}$ and let $B$ be the set of bad words . Since we want to work with generating functions, we introduce weights on words $$weight(w):=s^{length(w)}$$ The generating function $f(s)$ is the weight enumerator of the set of valid words $\mathcal{L}(B)$ that avoids the members of $B$ as factors (i.e. substrings). We obtain
  \begin{align*}
f(s)=\sum_{w\in\mathcal{L}(B)}weight(w)
\end{align*} It turns out according to the first section in the referred paper that counting these words is a cumbersome job. In fact we can do it much better and the trick is to add $0$ to both sides and rewrite this expression as
  \begin{align*}
f(s)=\sum_{w\in V^*}weight(w)0^{[\text{number of factors of }w\text{ that belong to }B]}
\end{align*}
  and then use the following deep facts (wording from the paper :-) )
  \begin{align*}
0&=1+(-1)\\
0^r&=
\begin{cases}
1,&\text{if }r=0\\
0,&\text{if }r>0
\end{cases}
\end{align*}
  and for any finite set $A$,
  \begin{align*}
\prod_{a\in A}0=\prod_{a\in A}(1+(-1))=\sum_{S\subset A}(-1)^{|S|}
\end{align*}
  where $|S|$ denotes the cardinality of $S$. We now have
\begin{align*}
f(s)&=\sum_{w\in V^*}weight(w)0^{[\text{number of factors of }w\text{ that belong to }B]}\\
&=\sum_{w\in V^*}weight(w)(1+(-1))^{[\text{number of factors of }w\text{ that belong to }B]}\\
&=\sum_{w\in V^*}\sum_{S\subset\text{Bad}(w)}(-1)^{|S|}s^{\text{length}(w)}
\end{align*}
where Bad$(w)$ is the set of factors of $w$ that belong to $B$. This clever usage of the Inclusion-exclusion principle is a much more superior approach to calculate the valid words not containing any bad factors and the essence was to add zero in order to introduce the IEP. So, my question is: Do you know from other situations where cleverly adding $0$ or multiplying with $1$ opens up a door to solve a problem.","['combinatorics', 'big-list', 'analysis']"
1684447,Nonnegative determinant of a symmetric matrix,"Consider the following matrix with nonnegative entries:
$$
M=\begin{pmatrix} a & b & c & d \\ b & c & d & e \\ c & d & e & f \\ d & e & f & g \\ \end{pmatrix}.
$$ Can we prove that, if each minor $2\times 2$ has nonnegative determinant, then the determinant of $M$ itself is nonnegative?","['matrices', 'determinant']"
1684506,Rationalising factor of $a+b \sqrt{2}+c \sqrt{3} + d \sqrt{6}$,"I am trying to express the inverse of $a+b \sqrt{2}+c \sqrt{3} + d \sqrt{6}$ (given $a, b, c, d \in \mathbb{Q}$) in the form $e+f \sqrt{2}+g\sqrt{3}+h\sqrt{6}$ (where $e, f, g, h \in \mathbb{Q}$). I could come up with a long way to do this by solving 4 linear equations to find $e, f, g, h$ which we get by expading the following, $(a+b \sqrt{2}+c \sqrt{3} + d \sqrt{6})(e+f \sqrt{2}+g\sqrt{3}+h\sqrt{6})=1$ and equating the rational term equal to 1 and the rest of the terms equal to $0$. But, I am now trying to find a more creative (elegant, generalisable, insightful) way to do this. For this I tried a few things which did not work (unecessary detail?). I guessed, $(a+b \sqrt{2}+c\sqrt{3}+d\sqrt{6})(a+b \sqrt{2}+c\sqrt{3}-d\sqrt{6})(a+b \sqrt{2}-c\sqrt{3}+d\sqrt{6})(a-b \sqrt{2}+c\sqrt{3}+d\sqrt{6}) \cdots$ (all the terms with all possible signs), but this did not work as I checked with a special case. I guessed that since a similar thing works for the rationalising factor of $\sqrt{a}+\sqrt{b}+\sqrt{c}$ which has $(\sqrt{a}+\sqrt{b}-\sqrt{c})(-\sqrt{a}+\sqrt{b}+\sqrt{c})(\sqrt{a}-\sqrt{b}+\sqrt{c})$ (I was motivated to this from  heron's formula for area of triangle). So, can anyone help me? Please. 
By the  way I am in high school so I don't know  anything about field splittings and so on.","['algebra-precalculus', 'irrational-numbers']"
1684515,Working with the $\frac{d}{dx}$ operator,"I have a fundamental query about the way derivatives can be used in algebraic manipulations. Say $\dfrac{d(\ln x)}{dx}=\dfrac{1}{x}$ Apparently, this can be manipulated to $d(\ln x)=\dfrac{dx}{x}$. I understand that this can be integrated back to the first equation. But, the reason this was done was to depict $\dfrac{dx}{x}$ as a percentage change in $x$. The definition of $\dfrac{df(x)}{dx}=\lim_\limits{h\to 0}\dfrac{f(x+h)-f(x)}{h}$. So, isn't $\dfrac{d}{dx}$ more like a function than a ratio of two quantities ? If so, how is moving $dx$ to RHS possible as done above ? Please advise.","['derivatives', 'calculus']"
1684525,"Automorphism of elliptic curve, Vakil 19.10.E","There are many other proofs of finding all the possible automorphism groups of elliptic curves, but I am interested in the $Hint$ and the corresponding proof in the following exercise from Vakil's book. Ex 19.10.E. Suppose $(E,e)$ is an elliptic curve over an algebraically closed field $k$ of characteristic not 2. Show that the automorphism group of $(E,e)$ is isomorphic to $\mathbb{Z}/2,\mathbb{Z}/4$, or $\mathbb{Z}/6$. Hint: reduce to the question of automorphisms of $\mathbb{P}^1$ fixing the point $\infty$ and a set of distinct three points $\{p_1,p_2,p_3\} \in \mathbb{P}^1- \{\infty\}$. So the proof is to use the fact that when $k=\bar{k}$ and $\text{char}~k \neq 2$, the elliptic curve is a double cover of $\mathbb{P}^1$ branched over $\{\infty,p_1,p_2,p_3\}$. So my question is how to show the 1-1 bijection between the automorphisms of $(E,e)$ and the automorphisms of $\mathbb{P}^1$ that fix $\{\infty\}$ and $\{p_1,p_2,p_3\}$, and what are they?","['algebraic-curves', 'elliptic-curves', 'algebraic-geometry']"
1684535,Geometrical proof of the limit $\lim_{(x \to 0)}\left(\frac{e^x-1}{x}\right)=1$ using sandwich theorem.,"I am studying about sandwich theorem and its applications by deriving some well-known limits such as this-
$$\lim_{(x \to 0)}\left(\frac{e^x-1}{x}\right)=1$$ while I found some proofs of this result by first defining $e$ and then using that definition such as here( Proof of $ f(x) = (e^x-1)/x = 1 \text{ as } x\to 0$ using epsilon-delta definition of a limit )( which I agree sounds a lot easier because if one is using the Taylor series expansion of $e^x$ then it becomes very easy )but my book tries to do this in a different manner by using this inequality $$\frac{1}{1+|x|}≤\left(\frac{e^x-1}{x}\right)≤ 1 + (e – 2) |x|$$( holds for all $x$ in $[–1, 1]-[0]$) and then just using sandwich theorem the limit is easily calculated , but the book does not explain as to from where this inequality came from.And I am not able to get it by myself ,as I am not able to see how can this result be so obvious, and even though the graph does make it a bit clear( which I have attached below ) still I am not able to get the given inequality(any hints there?) I tried to search it on this site but couldn't find it,but still I found some very neat applications of sandwich theorem such as here ( How to prove that $\lim\limits_{x\to0}\frac{\sin x}x=1$? ) so which makes me think that maybe this inequality can be derived easily by looking at it's geometrical interpretation such as in the link given. So can someone please help me in understanding the geometrical meaning of this inequality ( just like in the limit given in the link above ) or help to derive it using its geometrical implications? 1)And even if that is not possible can someone help me in understanding the inequality intuitively because I honestly haven't got any idea as to how such a weird looking inequality can be related to the given limit, 2)And how that inequality is derived ? 3)Also what can possibly be the motivation behind this complicated inequality for deriving this limit, are there other such wierd inequalities also for finding this limit?","['calculus', 'limits']"
1684589,Property of the conjugate transpose matrix with inner product,"I'm trying to prove that for a certain matrix $A$, and its conjugate transpose $A^*$, we have $⟨Ax,y⟩=⟨x,A^*y⟩$, where $⟨⟩$ represent the inner product. So here it's simply the dot product in $R^n$. Does anyone have an idea of how to prove this? Thank you!","['matrices', 'transpose', 'linear-algebra', 'inner-products']"
1684594,"If $f$ and $g$ are analytic functions, does $f \circ g - g \circ f = g- f$ implies $f = g$?","The question is in the title. There are actually two variants, one for real analytic functions and the other one for complex analytic functions.
It came up as an attempt to solve this question by analytic means, but I am not sure it really helps. I am afraid it is just a more difficult problem, unless there is a counterexample.","['complex-analysis', 'real-analysis', 'functions']"
1684603,How is the $p$-adic Tate module of a formal group defined?,"I am familiar with the definition of the $p$-adic Tate module of an elliptic curve defined over a $p$-adic field $k$ (a finite extension of $\mathbb{Q}_p$). But I have also seen some instances where the $p$-adic Tate module of a formal group is talked about. I wanted to know what the precise definition of $T_p(F)$ is, where $F$ is any formal group, like $T_p(\mathbb{G}_m)$. I could not find a satisfactory definition anywhere. Thank you.","['algebraic-number-theory', 'algebraic-geometry']"
1684611,How can I solve this ODE $ \frac {d^2x}{dt^2} + 8\frac {dx}{dt} + 25x = 10u(t)$?,"The problem its that I don't know how to treat the $ 10u(t) $ to obtain the particular solution since I don't know what the $ u(t) $ function represents. I've already have the complementary solution of the homogeneous equation associated which is: $$ X_c(t)=C_1e^{-4t}\cos {3t} + C_2e^{-4t}\sin {3t} . $$ Here again, I leave the equation to solve. $$ \frac {d^2x}{dt^2} + 8\frac {dx}{dt} + 25x = 10u(t)$$ Thank you. EDIT: Thanks SplitInfinity, Yes, the $ u(t) $ was the Step Function and something I forgot to mention that the problem says: consider initial conditions in $ 0 $ . So the step function states: $$ u(t)=
\begin{cases} 
0 & t<0 \\ 
1 & t \ge 0 
\end{cases} $$ in this case $ t=1 $ , therefore the equation ends up like this $$ \begin{align}
x''+8x'+25x&=10(1) \\
x''+8x'+25x&=10
\end{align}$$ because the non-homogeneous part its a linear polynomial the $X_p$ (proposal solution) need to be in this way: $$ X_p=A \\
X'_p=0 \\
X''_p=0 $$ So the equation ends up like: $$ \begin{align}
x_p''+8x_p'+25x_p & =10 \\
0+8(0)+25(A) & = 10 \\
A & = \frac {10}{25} \\
A & = \frac 25
\end{align}$$ Now the General Solution is given by $ X=X_c+X_p $ which is: $$ X=C_1e^{-4t}\cos {3t} + C_2e^{-4t}\sin {3t}+\frac 25 $$ To find out the $C_1$ and $C_2$ values we need to apply the initial values, which are: $$ x(0)=0,~~
x'(0)=0 $$ in order to do so, we differentiate $X$ $$ X'=C_1e^{-4t}(-3\sin{3t}-4\cos{3t})+C_2e^{-4t}(3\cos{3t}-4\sin{3t}) $$ Now, applying initial values to X: $\require{cancel}$ \begin{align} 
X(0) & = C_1\cancelto{1}{e^{-4(0)}}\cancelto{1}{\cos{3(0)}}+\cancelto{0}{C_2e^{-4(0)}\sin{3(0)}}+\frac25 \\
0 &= C_1 + \frac 25 \\
C_1 &= -\frac25
\end{align} Now to $ X' $ : \begin{align}
X'(0) &= C_1\cancelto{1}{e^{-4(0)}}(\cancelto{0}{-3\sin{3(0)}})+C_2\cancelto{1}{e^{-4(0)}}(3\cancelto{1}{\cos{3(0)}}-\cancelto{0}{4\sin{3(0)}}) \\
0 &= C_1(-4)+C_2(3) \\
0 &= -4C_1 + 3C_2 \\
3C_2 &= 4C_1 \\
\text{Replacing the $C_1$ value} \\
C_2 &= \frac{4(-\frac25)}{3} \\
C_2 &= -\frac8{15}
\end{align} Replacing the constants $ C_1 $ and $ C_2 $ in $X$ we found the solution: \begin{align}
X &= -\frac25 e^{-4t}\cos{3t}-\frac8{15}e^{-4t}\sin{3t}+\frac25 \\
X &= \frac1{15} \left[ e^{-4t}(-6\cos{3t}-8\sin{3t})+6 \right]
\end{align}",['ordinary-differential-equations']
1684658,At what rate does the entropy of shuffled cards converge?,"Consider a somewhat primitive method of shuffling a stack of $n$ cards: In every step, take the top card and insert it at a uniformly randomly selected one of the $n$ possible positions above, between or below the remaining $n-1$ cards. Start with a well-defined configuration, and then track the entropy of the distribution over the possible permutations of the stack as these shuffling steps are applied. It starts off at $0$. Initially most moves will lead to unique permutations, so we should have roughly $n^k$ equiprobable states after $k$ steps, so the entropy should initially increase as $k\log n$. For $k\to\infty$ it should converge to the entropy corresponding to perfect shuffling, $\log n!\approx n(\log n-1)$. What I'd like to know is how this convergence takes place. I have no idea how to approximate the distribution as it approaches perfect shuffling. I computed the entropy for $n=8$ for $k$ up to $50$; here's a plot of the natural logarithm of the deviation from the perfect shuffling entropy $\log n!$: The red crosses show the computed entropy; the green line is a linear fit to the last $30$ crosses, with slope about $-0.57$. So the entropy converges to its maximal value roughly as $\exp (-0.57k)$. For $n=7$, the slope is about $-0.67$, and for $n=9$ it's about $-0.50$. How can we derive this behaviour?","['permutations', 'entropy', 'probability', 'card-games']"
1684664,Understanding Meyers-Serrin theorem: about the use of mollifiers.,"I have read through the Meyers-Serrin theorem, and would like to understand why a simpler argument would not work. The theorem states that $C^{\infty}(\Omega)$ is dense in $W^{k,p}(\Omega), 1 \le p < +\infty.$ In the following we assume $k = 1$ and $\rho_{\epsilon} $a sequence of mollifiers. For $u \in W^{1,p}(\Omega),$ we consider $u, \nabla u \in L^p(\mathbb{R}^n),$ through natural extension through zero. Then we know: $u*\rho_{\epsilon} \rightarrow u$ $\nabla u*\rho_{\epsilon} \rightarrow \nabla u$ where both convergences are in $L^p(\mathbb{R}^n)$. From here we find: $u*\rho_{\epsilon} |_{\Omega} \rightarrow u|_{\Omega} $ $\nabla u*\rho_{\epsilon}|_{\Omega}  \rightarrow \nabla u|_{\Omega} $ here both convergences are in $L^p(\Omega).$ So we have almoast proven convergence in $W^{1,p}(\Omega),$ as soon as we know that $\nabla u*\rho_{\epsilon}|_{\Omega} = \nabla (u*\rho_{\epsilon}|_{\Omega})$ as a distribution on $\Omega.$ Is this last statement false? Because it seems true to me, and it should follow from the general results on derivatives of a convolution with a distribution. Thanks for the help in making my trhoughts clearer! :)","['functional-analysis', 'sobolev-spaces', 'analysis']"
1684694,Could the Monty-Hall Problem be applied to multiple choice tests?,"Given a multiple choice test where each question contains 4 possible answers, what would happen if before beginning the test (before reading the questions), someone were to make a random selection for each question? At this point it seems logical that for a given question the student has a 1/4 chance of their choice being correct and a 3/4 chance of one of the other choices being correct. Let's say that they now begin to read the questions and in some cases they can deduce that one of the provided answers which was not the one that they picked is not correct (let's assume that there is no error in this deduction). In the scenario with the Monty Hall Problem, the probabilities did not change once the door was opened, they just shifted. By applying the same logic, the original selected answer has a 1/4 chance of being correct and the other three have a 3/4 chance of being correct, except that since one was deduced to be incorrect, the two remaining options have a 3/4 chance of being correct and so switching answers would increase the odds of being correct to $\frac{1}{2} * \frac{3}{4}$. Is this an accurate assumption or are there pitfalls in doing this? If this is the case, then what happens if another deduction is made such that their original answer was determined to be incorrect? It seems that there would be no change in the odds, but that seems unlikely.","['monty-hall', 'probability']"
1684713,How to show a function can or cannot be extended to a compactification?,"This comes from Munkres 38.2. Let $Y$ be the compactification of $(0,1)$ induced by $h(x) = (x,\sin(1/x))$. Show that $g(x) = \cos(1/x)$ cannot be extended to this compactification $Y$. Also I wonder in general how to show a function can or cannot be extended to a compactification induced by some other function. Here is my attempted solution under the hint of Brian Scott: Let $Y$ denote the compactification of $(0,1)$ induced by $h(x) = (x,\sin (1/x))$. Let $Y_0$ be $H(Y)$ where $H$ is the extension of $h$. Suppose we can extend $g$ to a continuous function $G: Y\to \mathbb{R}$, then the function $G\circ H^{-1}: Y_0 \to \mathbb{R}$ is also continuous. Consider the sequence $\{h(\frac{1}{k\pi})\}_{k\in\mathbb{Z}_+} = \{(\frac{1}{k\pi}, 0)\}_{k\in\mathbb{Z}_+}$ in $Y_0$, then $h(\frac{1}{k\pi}) \to (0,0)$ in $Y_0$, by continuity of $G\circ H^{-1}$ we must have $\lim_{k\to\infty}G(H^{-1}(h(\frac{1}{k\pi}))) = \lim_{k\to\infty} G(\frac{1}{k\pi}) = G(0)$. However, $G(H^{-1}(h(\frac{1}{k\pi}))) = g(\frac{1}{k\pi}) = (-1)^k$ does not converge, which is a contradiction. Hence $g$ cannot be extended to a continuous map on $Y$.",['general-topology']
1684747,"Is $(T,\omega) \mapsto \int_0^T f(t,\omega)\ dt$ measurable?","Let $(\Omega, \mathcal{A}, P)$ be a probability space. Denote by $\mathcal{B}$ the Borel space on the real line and denote by $\mathcal{B}_{[0, \infty)}$ the Borel space on the interval $[0, \infty)$. Denote by $\mu$ the Lebesgue measure on $\mathcal{B}_{[0,\infty)}$. Let $f:[0,\infty)\times\Omega\rightarrow\mathbb{R}$ be a $(\mathcal{B}_{[0,\infty)}\otimes\mathcal{A})/\mathcal{B}$-measurable function such that for every $T \in [0,\infty)$, $f\mathbb{1}_{[0, T]\times\Omega} \in L_1(\mu \otimes P)$. For every $T \in [0,\infty)$ define $X_T : \Omega \rightarrow \mathbb{R}$ as follows:
$$
X_T(\omega) := \int_0^T f(t, \omega)\ dt
$$
as long as the (Lebesgue) integral on the right is defined and finite. Otherwise, set $X_T(\omega) := 0$. From Fubini's theorem we know that, w.l.g., we may assume that $X_T$ is $\mathcal{A}/\mathcal{B}$-measurable. Consider the stochastic process $X := (X_T)_{T \in [0,\infty)}$. As a function from $[0,\infty)\times \Omega$ to $\mathbb{R}$, is $X$ $(\mathcal{B}_{[0,\infty)}\otimes \mathcal{A})/\mathcal{B}$-measurable?","['stochastic-processes', 'probability-theory', 'lebesgue-integral', 'measure-theory']"
1684781,"$E$ be a domain, define $E^*=\{z \in C: \overline{z}\in E\}$ $f: E \to C$ is analytic, then $f^*(z)=\overline{f(\overline{z})}$ analytic on $E*$. [duplicate]","This question already has answers here : How do I rigorously show $f(z)$ is analytic if and only if $\overline{f(\bar{z})}$ is? (5 answers) Closed 4 years ago . We know that since $f$ is analytic on $E$ we have $$u_x=v_y \quad u_y=-v_x$$ We have $f^*(z)=u(x,-y)-iv(x,-y)$ Essentially we are going from $$E^* \stackrel{\overline{z}}{\rightarrow} E \stackrel{f}{\rightarrow} \mathbb{C} \stackrel{\overline{z}}{\rightarrow} \mathbb{C}$$ and $f$ is analytic on $E$. While $z \mapsto \overline{z}$ is not analytic we are doing that mapping twice so it may not disturb analyticity? I know I need to consider the Cauchy-Riemann equations on $f^*$ but I'm not sure how to get there. Thanks for the help!",['complex-analysis']
1684782,Normalized partial sums of normal random variables are dense in $\mathbb{R}$,"I came across an interesting result appearing as an exercise in some lecture notes I'm reading. Suppose $X_{1},X_{2},...$
  are IID $N\left(0,1\right)$
  RVs all defined on $\left(\Omega,\mathcal{F},\mathbb{P}\right)$
  and let $S_{n}=\sum_{i=1}^{n}X_{i}$
 . Then with probability 1
  (w.r.t to $\mathbb{P}$
 ) the sequence $\frac{S_{n}\left(\omega\right)}{\sqrt{n}}$
  is dense in $\mathbb{R}$
 . That is, with probability 1 for all $x\in\mathbb{R}$
  there is a subsequence $\frac{S_{n_{k}}}{\sqrt{n_{k}}}$
  converging pointwise to $x$. I'm curious about what kind of proof approach would be appropriate here. If some cares to reference or write a proof that would be great.","['probability-theory', 'normal-distribution', 'random-variables']"
1684790,Why is $\mathbb{Q}(\sqrt[3]{2})$ not a stem field for the polynomial $X^3-2 \in \mathbb{Q}[X]$?,"I'm trying to learn Galois theory on Coursera. The lecturer gave the following definition of a stem field: Let $P$ be an irreducible monic polynomial in $K[X]$ with a root $\alpha$. A stem field is an extension E such that $\alpha \in E$ and $E=K(\alpha)$. A textbook I have says $\mathbb{Q}(\sqrt[3]{2})$ is not a stem field for the polynomial $X^3-2 \in \mathbb{Q}[X]$, but to me it seems to match the definition. Can someone help me understand why it isn't?","['abstract-algebra', 'extension-field']"
1684833,What is the geometric relationship between $A$ and $A^T$?,Posed a more specific way: Let $A \in \mathbb R ^ {m \times n}$ and $S_k$ be the unit $k$-sphere. What is the exact geometric relationship between $E_m = \{ A\vec x \mid \vec x \in S_m \}$ and $E_n = \{ A^T\vec y \mid \vec y \in S_n \}$?,"['matrices', 'linear-algebra', 'geometry']"
1684843,Pushforward on principal divisors,"I am having much difficulty proving the following fact. Let $\phi:C_1\to C_2$ be a Galois cover of curves over an algebraically closed field $K$. Let $G=\mathrm{Gal}(K(C_1)/\phi^*K(C_2))$. Let $N: K(C_1)\to \phi^*K(C_2)$ be the field norm. Let $\phi_*: \mathrm{Div}(C_1)\to \mathrm{Div}(C_2)$ and $\phi^*: \mathrm{Div}(C_2)\to \mathrm{Div}(C_1)$ be the pushforward and pullback maps induced by $f$ respectively. For $f\in K(C_1)$, I wish to show that $\phi_*(\mathrm{div}(f)) = \mathrm{div}(N(f))$. I was given the hint that since for $P\in C_1$, $\phi^*\circ \phi_*([P]) = \sum_{\sigma\in G} [\phi(\sigma(P))]$ and $\phi^*$ is injective on divisors it suffices to prove that $f^*\circ f_*(\mathrm{div}(g)) = f^*(\mathrm{div}(N(g)))$. But I have no idea how to show this. Any help would be greatly appreciated.","['algebraic-curves', 'algebraic-geometry']"
1684851,Clarification on random variables?,"If $X$ and $Y$ are dependent random variables, then it is possible that $Var(X+Y) > Var(X) + Var(Y)$. I only know that the two are equal for independent random variables; for dependent variables, would this be the case? According to forecasts, the end-of-year value in dollars of IBM stock has variance 10. If an investor holds a portfolio containing 5 shares of IBM stock and 240 dollars of idle cash, what is the variance in the end-of-year value in dollars of his portfolio? I would assume it's 50, as variance is additive? If X and Y are random variables such that $P(X=0)=0.5$ and $P(Y=0)=0.1$, then is $P((X+Y)/2=0)$ equal to $P(X=0)/2 + P(Y=0)/2 = 0.3$? I don't believe that it's possible to add probabilities like this, would I multiply instead?","['statistics', 'random-variables']"
1684869,A Noetherian integral domain is a UFD iff $(f):(g)$ is principal,"Let $R$ be a Noetherian integral domain. For $f, g \in R$, define $(f):(g)=\{h \in R \mid hg \in (f) \}$. Sow that $R$ is a UFD if and only if $(f):(g)$ is principal for all $f,g \in R$. It is easy to show that $(f):(g)$ is an ideal. For the forward direction, I suspect that I need to use the fact that $R$ is Noetherian to show that $(f):(g)$ is principal. For the reverse direction, I know that if every $(f):(g)$ is principal then the ring $R$ is a PID. But I don't know how to proceed. Any ideas?","['abstract-algebra', 'ring-theory', 'unique-factorization-domains', 'noetherian']"
1684898,Norm of matrix and its maximum eigenvalue,I've seen in some inequalities in the theory of ODEs that $\lVert Q \lVert \le \lambda_{max}(Q)$. What theorem from Linear Algebra is relevant here?,"['dynamical-systems', 'ordinary-differential-equations', 'linear-algebra']"
1684925,A complex analysis problem to prove an inequality,"$\textbf{Problem.}$ Suppose $f$ is a holomorphic function on $\{z\in\mathbb{C}:|z|<1\}$, the open unit disk, with the property that Re$f(z)>0$ for every point $z$ in the disk. Prove that $|f'(0)|\leq 2\text{Re}f(0)$. This is a problem that appeared on a qualifying exam in some graduate school. What I tried is, define $\displaystyle\varphi(z)=\frac{z-1}{z+1}$, then this sends the open right half plane onto the open unit disk, so let $\hat{f}=\varphi\circ f(z)$, then its image is contained in the open unit disk. And I wanted to use the Schwarz lemma, but then the origin had to be fixed, so define $\displaystyle\psi(z)=\frac{\hat{f}(0)-z}{1-\overline{\hat{f}(0)}z}$ and let $\tilde{f}=\psi\circ\hat{f}$, then the image of this is still contained in the open unit disk and it fixes the origin so the Schwarz lemma $|(\tilde{f})'(0)|\leq 1$ could be applied. But after all the calculation, I got
$$|f'(0)|\leq\frac{|f(0)+\overline{f(0)}+2|f(0)|^{2}|^{2}}{2|f(0)+1|^{2}|f(0)|}$$
but, for example, if the magnitude of the imaginary part of $f(0)$ is quite bigger than the magnitude of the real part of $f(0)$, then this doesn't result the desired inequality but actually means that the desired inequality is wrong instead. I also thought about composing some different function to the RHS of $\hat{f}$ and use the Schwarz lemma but it didn't seem to work. Maybe I should try something else, but what could be tried instead?","['complex-analysis', 'inequality']"
1684973,Differentiability of $\int_0^tx^2f(x)dt$,"If $f(x)$ is continuous, how can I prove that $\int_0^tx^2f(x)dt$ is differentiable? This is what I thought of: Since $\int_0^tx^2f(x)dt=F(t)-F(0)$ for some function $F$ which is the antiderivative of $x^2f(x)$ so $F$ is differentiable. Thus $\int_0^tx^2f(x)dt$ is differentiable. Am I right?","['derivatives', 'calculus']"
1684988,Finding Maximum Area of a Rectangle in an Ellipse [duplicate],"This question already has answers here : Find the area of largest rectangle that can be inscribed in an ellipse (7 answers) Closed 8 years ago . Question: A rectangle and an ellipse are both centred at $(0,0)$.
       The vertices of the rectangle are concurrent with the ellipse as shown Prove that the maximum possible area of the rectangle occurs when the x coordinate of
  point $P$ is $x = \frac{a}{\sqrt{2}} $ What I have done Let equation of ellipse be $$ \frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$$ Solving for y $$ y = \sqrt{ b^2 - \frac{b^2x^2}{a^2}} $$ Let area of a rectangle be $4xy$ $$ A = 4xy $$ $$ A = 4x(\sqrt{ b^2 - \frac{b^2x^2}{a^2}}) $$ $$ A'(x) = 4(\sqrt{ b^2 - \frac{b^2x^2}{a^2}}) + 4x\left( (b^2 - \frac{b^2x^2}{a^2})^{\frac{-1}{2}} \times \frac{-2b^2x}{a^2} \right) $$ $$ A'(x) = 4\sqrt{ b^2 - \frac{b^2x^2}{a^2}} + \frac{-8x^2b^2}{\sqrt{ b^2 - \frac{b^2x^2}{a^2}}a^2} = 0 $$ $$ 4a^2\left(b^2 - \frac{b^2x^2}{a^2} \right) - 8x^2b^2 = 0 , \sqrt{ b^2 - \frac{b^2x^2}{a^2}a^2} \neq 0 $$ $$ 4a^2\left(b^2 - \frac{b^2x^2}{a^2} \right) - 8x^2b^2 = 0 $$ $$ 4a^2b^2 - 4b^2x^2 - 8x^2b^2 = 0  $$ $$ 4a^2b^2 - 12x^2b^2 = 0  $$ $$ 12x^2b^2 = 4a^2b^2 $$ $$ x^2 = \frac{a^2}{3} $$ $$ x = \frac{a}{\sqrt{3}} , x>0 $$ Where did I go wrong? edit:The duplicate question is the same but both posts have different approaches on how to solve it so I don't think it should be marked as a duplicate..","['derivatives', 'optimization', 'calculus', 'conic-sections', 'area']"
1685000,one-step-ahead forecast in out-of-sample estimation,"Suppose I have a sample of $n+m$ observations of $\{x_i,y_i\}$ and i use the first $n$ observations to estimate the parameters in my model and save the last $m$ observations for forecasting. Let $\hat{f}_{n+h} $ be the one-step-ahead forecast of $y_{n+h+1}$ for $h=0,1,..m-1$. How do i estimate  $y_{n+h+2}$ ? Do I use the $n+1$ observations of both $x$ and $y$ to estimate $\hat{f}_{n+h+1} $ or use $n+1$ observations from $x$ and $n$ from $y$ and $\hat{f}_{n+h} $  to estimate $\hat{f}_{n+h+1} $? Thanks","['time-series', 'statistics']"
1685008,Some Trouble Understanding set theory,"I'm currently in a discrete mathematics class and we've recently been discussing set theory. I feel like I have basic understanding of how to actually prove set relations when a question asks to do so. However I am having a lot of trouble when initially presented with questions, where I am asked to determine if a statement is true or false. The approach we were taught was to set up ven diagrams in order to help us. However I find that I get very lost in certain types of questions, especially ones where one set contains another. Here is an example of a problem I struggled with: One of these is true and one is false, provide a proof for both: (1) For all sets A, B and C, if A-B is a subset of A-C then C is a subset of B (2) For all sets A, B and C, if C is a subset of B then A-B is a subset of A-C. I understand that the first one is false and the second one is true. However when first presented with the problems the only way I was able to solve it was by plugging in sets, until one was false. I was hoping that someone would be able to provide me with a better approach to making sense of these type of problems, and possibly how I could represent these questions with a diagram. Thanks!","['elementary-set-theory', 'discrete-mathematics']"
1685015,Show that $G$ is abelian.,"Let $G$ be a finite group such that every Sylow subgroup of $G$ is normal and abelian.Show that $G$ is abelian. Let $x,y\in G$ . Case 1:If $x,y$ are in the same Sylow subgroup and as it is given  to be abelian so $xy=yx$. Case 2:If $x,y$ are not in the same Sylow subgroup, so suppose that $x$ is in a Sylow $p$ subgroup of $G$ say $P$ and $y$ is in a Sylow $q$ subgroup of $G$,say $Q$ where $p,q$ are distinct primes. Now $P,Q$ are normal and hence $xyx^{-1}y^{-1}\in P\cap Q=\{1\}\implies xy=yx$. Thus $G$ is abelian. Is the proof correct?Please suggest required edits.","['finite-groups', 'abstract-algebra', 'group-theory', 'sylow-theory']"
1685030,T-Statistics and s Calculations,How can I calculate s in T statistics? Example: John H. takes one sample of size 20 and finds that the sample mean in 32.8. Calculate a 95% confidence interval for John. (Assume John knows the true standard deviation.)??? I know what formula to use but I want to understand how can I calculate s without knowing the sample #s.,"['statistics', 'probability']"
1685054,Is it good to use mean value theorem in $\epsilon-\delta$ continuity proofs?,"I wanted to prove $f(x) = \cos(x)$ is continuous using $\epsilon-\delta$ proof Couple of posts on MSE appealed to MVT to resolve this problem. Namely: $\exists c \in [x,x_o]$ s.t. $|\cos(x)-\cos(x_o)| = |\sin(c)||x-x_o|$ Tada! Problem here is that we are appealing to the fact $\sin(x)$ is the derivative of $\cos(x)$
...which necessarily implies that $\cos(x)$ is continuous. Is it ""good"" to use MVT in proving a function is continuous?","['real-analysis', 'calculus', 'continuity', 'proof-writing', 'soft-question']"
1685057,Picard group of the grassmannian,"I would like some help in finishing this argument, as sketched in Dolgachev's book ""Lectures in Invariant Theory"" (page 165). Everything here is done over $\mathbb{C}$. The claim is that $Pic(Gr_{k,n}) \cong \mathbb{Z}$. The argument goes as follows: Consider $Gr_{k,n}$ embedded into $\mathbb{P}^{n}$ via Plucker coordinates $p_{i_{1}..i_{k}}$. Given a line bundle $L$ on $Gr_{k,n}$, one may trivialize the line bundle over the open set (plucker coordinate) $p_{1...k} \neq 0$ which is isomorphic to an affine space. Thus, one may produce a global section $s$ of $L$ such that the corresponding divisor $Z(s)$ is supported in the hyperplane section $p_{1..k} = 0$. If this hyperplane section were smooth or irreducible, then we may conclude that $Z(s)$ is a multiple of this hyperplane section and then we would be finished. However, Dolgachev does not address the fact that the hyperplane section may not be irreducible, and I think that without this fact the argument may fall apart. 
I am aware of Bertini's theorem, which tells me that there will exist smooth hyperplane sections.. however, the complement of an arbitrary hyperplane section may not be isomorphic to affine space. Could anybody shed some light on this? Thanks!",['algebraic-geometry']
1685076,Can someone give me an idea of finding the distribution of $\frac{\sum_{i=1}^N (X_i-E(X))^2}{\sum_{i=1}^M (Y_i-E(Y))^2}$,"X~N(4, $\sigma^2$) and  Y~N(1, $\sigma^2$) are independent. $$A=\frac{\sum_{i=1}^N (X_i-E(X))^2}{\sum_{i=1}^M (Y_i-E(Y))^2}$$ Find the distribution of A? I tried this way. $$\frac{M-1}{N-1}A=\frac{\frac{1}{N-1}\sum_{i=1}^N (X_i-E(X))^2}{\frac{1}{M-1}\sum_{i=1}^M (Y_i-E(Y))^2}=\frac{S_x^2}{S_y^2}=\frac{(\frac{N-1}{\sigma^2})S_x^2/(N-1)}{(\frac{M-1}{\sigma^2})S_y^2/(M-1)}$$ So I concluded that $\frac{M-1}{N-1}A$ ~$F_N-_1,_M-_1$ But the question is about A, not about $\frac{M-1}{N-1}A$ I need some help! Please give me an advice.",['statistics']
1685100,Are these two sequences the same?,"I was browsing OEIS and came across the largely composite numbers , A067128 , defined as the natural numbers that have at least as many divisors as all smaller natural numbers. (They are of course related to the highly composite numbers .) A comment on the OEIS page asks whether the largely composite numbers are the same as A034287 , the numbers $n$ such that the product of the divisors of $n$ is larger than for all smaller natural numbers. In reply, another comment says that the two sequences are the same for all terms less than $10^{150}$, of which there are 105834. My questions are: Are these two sequences the same, or do they differ at some point after the 105834th term? If they do differ, is there a nice way to see why the two sequences should be the same for such a large range of initial values?","['number-theory', 'sequences-and-series']"
1685143,"Binary string of length $2^n + n - 1$ that contains, as substrings, all strings of length $n$ [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question There is a famous formula, used in the context of Sanskrit prosody, that is used to give names to all the binary strings of length 3. It goes something like yamaataaraajabaanasalagaa .
Read a as short syllable (1) and aa as long syllable (2). Then the names we get by taking three consecutive syllables at a time are as follows:
$$y=(1,2,2), m=(2,2,2), t=(2,2,1), r=(2,1,2),$$
$$j=(1,2,1), b=(2,1,1), n=(1,1,1), s=(1,1,2).$$
These names are used to describe meters in compact notation.
I want to get a generalized method for generating strings like these, i.e., instead of $3$, consider strings of length $n$ and find all strings of length $2^n + n - 1$, each of which contains, as substrings, all strings of length $n$.",['combinatorics']
1685148,How is $\frac{1}{1^4}+\frac{1}{2^4}+\frac{1}{3^4}\cdots=\frac{\pi^4}{90}$,"If $\frac{1}{1^4}+\frac{1}{2^4}+\frac{1}{3^4}\cdots=\frac{\pi^4}{90}$, then find the value of $\frac{1}{1^4}+\frac{1}{3^4}+\frac{1}{5^4}\cdots$ Firstly how is $\frac{1}{1^4}+\frac{1}{2^4}+\frac{1}{3^4}\cdots=\frac{\pi^4}{90}$? Secondly, I thought $$\frac{1}{1^4}+\frac{1}{3^4}+\frac{1}{5^4}\cdots=\frac{1}{2^4}+\frac{1}{4^4}+\frac{1}{6^4}\cdots=\frac{S}{2}$$
But answer given is $\frac{\pi^4}{96}$. Whats the mistake in this? Edit: I found a way to get the answer.
$$\frac{1}{1^4}+\frac{1}{2^4}+\frac{1}{3^4}\cdots=\frac{1}{1^4}+\frac{1}{3^4}+\frac{1}{5^4}\cdots+\frac{1}{2^4}\left(\frac{1}{1^4}+\frac{1}{2^4}+\frac{1}{3^4}\cdots\right)$$
$$S=S_1+\frac{1}{2^4}S$$","['algebra-precalculus', 'sequences-and-series']"
1685153,Trouble understanding how this identity is derived: $\sum_{j=0}^{\infty}\binom{a+j}{j}x^j=(1-x)^{-a-1}$,"$$\sum_{j=0}^{\infty}\binom{a+j}{j}x^j=(1-x)^{-a-1}$$ The $-a-1$ is throwing me off. Can anyone help me understand this identity. I have tried letting $m=-a-1$ and then applying the binomial theorem, and letting the sum run up to $\infty$ since anything past $m$ will be $0$. But I didn't get anywhere because we'll still have $(-1)^i$ in the sum. Since there is the $\infty$ in the sum, I have also tried thinking about it in terms of generating functions but can't get anywhere.","['combinatorics', 'binomial-coefficients', 'sequences-and-series', 'discrete-mathematics']"
1685169,Expected value of $\log(\det(AA^T))$,"Consider uniform random $n$ by $n$ matrix $A$ where $A_{i,j} \in \{-1,1\}$.  We know that with high probability $A$ is non-singular. Are there known estimates or bounds for $$\mathbb{E}(\log(\det(AA^T)))\;?$$","['random-matrices', 'probability', 'linear-algebra']"
1685186,"Understanding and modifing a theorem from Fulton's ""algebraic curves"" [duplicate]","This question already has an answer here : Show that $\dim_k(\mathfrak m^n / \mathfrak m^{n+1})=n+1$ whenever $0\leq n<m_P$, where $(\mathcal O,\mathfrak m)$ is the local ring of $P$ (1 answer) Closed 12 months ago . I am reading Theorem 2 in chapter 3.2 in Fulton. It says that $mult_p(F)=\dim_k (m_p(F)^n/m_p(F)^{n+1})$ for a sufficiently large $n$. Reading the proof I understood that we eventually play a game of exact sequences, I want to understand what happens if $n$ is not sufficiently large and what does sufficiently large mean. Say we call $mult_p(F)=k$ (the book uses m, but I am using k to differentiate with the ideals $m_p$)
 So we want to find the dimension of $\mathbb{C}[x,y]/(F,I^n)$. Then the book proceeds with saying that if $F$ has multiplicity k then the polynomial can be written as $F=F_k+ F_{k+1}+\cdots$ thus if we have $ F * G \in I^n$ then $G \in I ^{n-m}$. That is great and we can make the exact sequence
$$0 \rightarrow \mathbb{C}[x,y]/I^{n-m} \rightarrow \mathbb{C}[x,y]/I^n \rightarrow \mathbb{C}[x,y]/(F,I^n) \rightarrow 0$$
Then from there their proof is very straight. What I observed is that for ""n"" to be sufficiently large it must be larger than k ie the multiplicity of the point. ( I think I am correct here but feel free to correct me)
My question is if we say that n is smaller than the k. In that case I think the thing that will change is that we do not need a G anymore thus we will get that
$F \in I ^n$ because the smallest term of F is $F_k$ but k is larger than n so we have the inclusion. Thus the exact sequence that we will obtain is
$$0  \rightarrow \mathbb{C}[x,y]/I^n \rightarrow \mathbb{C}[x,y]/(F,I^n) \rightarrow 0$$ To compute the dimension of $\mathbb{C}[x,y]/(F,I^n) $ we get 
$\dim \mathbb{C}[x,y]/(F,I^n) = \dim \mathbb{C}[x,y]/I^n +1$ but this does not seem to add up. If someone is familiar with Fulton and the argument can possibly guide me I would appreciate it. Thank you","['algebraic-geometry', 'commutative-algebra']"
