question_id,title,body,tags
3228266,Proof of familiarity between 9 people,How can we try to prove that among any 9  people thare are 3 people who are familiar with each other or 4 who are not familiar with each other? My approach: I try to convert this to graph theory. So I have a graph with 9 vertices which is clique. Now we try to point the graph edges into 2 colors. When we show that there always exist 3-clique or 4-clique in the same color in this graph then the statement will be proved. But how to start considering the graph?,"['graph-theory', 'ramsey-theory', 'combinatorics', 'discrete-mathematics']"
3228296,What is the value of $\lim_{x\to 0} \frac{\cos(1/x)}{\cos(1/x)}$?,"$$\lim_{x\to 0} \frac{\cos\left(\frac{1}{x}\right)}{\cos\left(\frac{1}{x}\right)}$$ I think the answer should be $1$ , I understand that the value of $\cos(1/x)$ would be oscillating quickly as $x$ approaches $0$ . But , wouldn't both the numerator and denominator get cancelled ? My teacher said that the limit does not exist in this question , I dont understand.",['limits']
3228412,Related rates of change: what am I doing wrong?,"I'm new to derivatives, and I try to solve the following exercise: A ground camera is being used to film the lift-off of a rocket from its launch pad. The rocket rises vertically so that its vertical distance $y$ , in meters, is related to time $t$ , in seconds from launch, by $y=10t^2$ . Determine the rate of change of the angle of elevation of the camera $10s$ after the launch if the camera is situated horizontally $600m$ from the launch pad. So my way of thinking is like this:
The angle of the camera is related to the rocket's height by the following formula: $$tan(\alpha)=\frac{y(t)}{600meters}=\frac{10t^2}{600}=\frac{1}{60}t^2$$ so $$\alpha=arctan(\frac{1}{60}t^2)$$ So the rate of change of the angle is: $$\frac{d\alpha}{dt}=\frac{d}{dt}arctan(\frac{1}{60}t^2)=\frac{\frac{1}{60}2t}{1+(\frac{1}{60}t^2)^2}=\frac{120t}{t^4 + 3600}$$ So the rate of change of the angle $10s$ after the launch is: $$\alpha'(10seconds)=\frac{120*10}{10^4+3600}=0.088 rad/s$$ But the answer on my solution sheet is $0.9 rad/s$ , it's a different order of magnitude. Is there an error in the solution sheet or am I doing something wrong? Thank you in advance for your help","['calculus', 'trigonometry']"
3228416,Topological vector spaces with a unique trivial nonempty convex set which is algebraically open,"Let $X$ be a topological vector space. We recall that a set $A\subseteq X$ is algebraically if, for each $x,y \in X$ , the set $\{t \in \mathbf{R}: x+ty \in A\}$ is open. Moreover, it is well known (and easy to prove) that every open set is algebraically open. Also, there are non-open sets which are algebraically open (see e.g. here ). In addition, if $A$ is convex, then the converse holds in $X=\mathbf{R}^n$ . This is still false in an arbitrary topological vector space $X$ (see e.g. here ). Question . Does there exist a topological vector space $X$ such that the only convex algebraically open non-empty subset is $X$ itself? Ps. Replacing ""algebraically open"" with the the stronger ""open"" the answer is positive, setting $X=L_p([0,1])$ , for some $p \in (0,1)$ .","['convex-geometry', 'general-topology', 'topological-vector-spaces']"
3228503,Local system associated to monodromy representation,"How can I associate a local system to a representation $\rho: \pi_1(X) \to \mathbb C^*$ ? I have seen some construction, but it doesn't click for me. I know that the idea is to use a diagonal action of $\pi_1(X)$ on $\tilde X \times \mathbb C$ , where $P: \tilde X \to X$ is the universal cover. But why does one obtain a local system in this way? For example, I have seen the construction (with $L_{\tilde X}$ the trivial sheaf on $\tilde X$ ): $$\Gamma(U,\mathcal L):= \{s \in \Gamma(P^{-1}(U,L_{\tilde X})):~ \forall u \in P^{-1}(U), \forall \gamma \in \pi_1(X,v),~ s(\gamma.u)=\rho(\gamma).s(u) \}. $$ I guess the sheaf axioms are inherited from $L_{\tilde X}$ . But why is this locally constant? My approach: $P$ is a covering map, so we automatically get nice neighbourhoods to work with. But then how do I proceed?","['complex-geometry', 'representation-theory', 'fundamental-groups', 'algebraic-geometry', 'algebraic-topology']"
3228505,Understanding i.i.d. random variables from product measure space perspective,"I have a very weak background in measure theory, and I am having some troubles understanding i.i.d. random variables from a measure theoretic perspective. Let $X$ be a random variable defined on a probability triple $(\Omega, \mathcal{F},P)$ . I have seen that some authors write "" $X_1, \ldots, X_n$ are $n$ independent copies of $X$ "", which I understand as $X_1,\ldots, X_n$ are i.i.d. random variables having the same distribution as that of $X$ . 
I found that the most intuitive way to construct an i.i.d. sequence to me is to define $X_1,\ldots,X_n$ on a product measure space $(\Omega^n, \mathcal{F}^n,P^n)$ . Here is how I understand this construction.
When we say something like $(x_1,\ldots, x_n) = (X_1(\omega),\ldots, X_n(\omega))$ , this $\omega$ belongs to $\Omega^n$ , whereas when we consider $X(\tau)$ , this $\tau$ is in $\Omega$ . So $\omega$ and $\tau$ belong to different spaces unless $n=1$ . Thus, $X_i$ and $X$ are completely different functions (defined on different domains $\Omega^n$ and $\Omega$ ). So, $X_i$ is not a copy of $X$ as a function, but they just have the same distribution. Moreover, as $n$ changes, the probability triple $(\Omega^n, \mathcal{F}^n,P^n)$ also changes accordingly. Hence, for example, if we consider $(X_1(\gamma), X_2(\gamma))$ and $(X_1(\sigma),X_2(\sigma),X_3(\sigma))$ , $\gamma$ and $\sigma$ live in different spaces, $\Omega^2$ and $\Omega^3$ . So, the random variable $X_1$ in the pair $(X_1, X_2)$ and $X_1$ in the triple $(X_1,X_2,X_3)$ are also different functions (defined on different domains), but having the same distribution, and denoted by the same notation. Is this understanding correct? I am confused since the dependency on $n$ does not seem to be explicitly specified in most cases, and they seem to use the same notations $\omega$ , $\Omega$ , $X_i$ , etc. everywhere. Many thanks.","['measure-theory', 'probability-theory', 'probability', 'random-variables']"
3228507,Calculate a limit: $\lim_{t \to 0+} {1\over 2}\cdot({\pi \over t}\cdot\frac{1+e^{-2\pi t}}{1 - e^{-2\pi t}} - {1\over t^2}) $,"Please calculate the limit $\lim\limits_{t \to 0+} {1\over 2}\cdot \left({\pi \over t}\cdot\frac{1+e^{-2\pi t}}{1 - e^{-2\pi t}} - {1\over t^2}\right)$ and provide the corresponding procedure. The answer is $\pi^2 \over 6$ . I tried L'Hospital's Rule but I failed. The following is not necessary to read: The background of this problem: this problem arose from a problem related to the Fourier Transform. I tried to use Poisson's summation formula to get $\sum\limits_{n = -\infty}^\infty \frac{1}{n^2 + t^2} = {\pi \over t}\cdot\frac{1+e^{-2\pi t}}{1 - e^{-2\pi t}}$ . (This can be proved by let $f(x) = \frac{1}{x^2+t^2}$ , where $t>0$ is a parameter. Then $\hat{f}(n) = {\pi \over t}\cdot e^{-2\pi |n| t}$ .) Then $\sum\limits_{n = 1}^\infty {1\over n^2} = \frac{\pi^2}{6} $ should be a corollary.","['limits', 'calculus']"
3228508,How do you find the probability that the sample mean is between 52 and 56?,"I was attempting to help a friend with a question and I am not sure if I am overthinking it, or simply missing an assumption I can make.  It goes like this: The average life of a battery is 50 hours with a standard deviation of 4 hours.  What is the probability that the mean of 16 samples is between 52 and 56? I thought I had this, but I am getting caught up on the fact that it is a small sample size and it is not stated that it is normally distributed. Question:  What method do you use to solve this? Other people suggested: $$Z = \frac{\bar{X}-\mu}{\sigma}$$ My problem here is we are assuming it is normal and we are not utilizing the given info about the sample size of 16. I suggested: $$Z = \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}$$ My problem here is we are again assuming it is normal and I am getting $2<Z<6$ , which gives about $2\%$ . Any clarifications will be greatlyappreciated!","['statistics', 'normal-distribution', 'probability', 'sampling']"
3228522,Solving ODE - Space curve Frame,"I'm trying to calculate the parallel frame $\{T, U, V\}$ of a space curve $\alpha : I \mapsto \mathbb{R}^3$ . It's similar to Frenet frame, except we have instead the projection of torsion $\tau$ on $U$ : $\tau_U = 0$ It's described by the three equations, $$T' = \kappa_U U + \kappa_V V \quad \quad \quad \quad \ \ (1)$$ $$U' = -\kappa_U T = -\langle \kappa,U \rangle \ T \quad \quad (2)$$ $$V' = -\kappa_V T = -\langle \kappa,V \rangle \ T \quad \quad (3)$$ where $\kappa = T'$ is the curvature vector and $\kappa_U, \kappa_V$ are its components.
I know $T$ , $\kappa$ and the initial conditions $\{T(0), U(0), V(0)\}$ at $\alpha(0)$ but I don't know how to solve the ODE $(2)$ and $(3)$ in order to get $U$ and $V$ . Any suggestion or a note on what to search for would be great!","['ordinary-differential-equations', 'differential-geometry']"
3228526,Help trying to find the coefficient in a generating function expansion,"Find the coefficient of ${x^{20}}$ in the expansion of the generating function $g(x) = \frac{5{(1-x^5)^7}}{(1-x)^{2}}$ I broke the function into two components: $5{(1-x^5)^7}$ and $\frac{1}{(1-x)^2}$ Because I'm looking for the ${x^{20}}$ coefficient, I have 5 terms: $(a_0 \cdot b_{20}) + (a_5 \cdot b_{15}) + (a_{10} \cdot b_{10}) + (a_{15} \cdot b_5) + (a_{20} \cdot b_0)$ which gives me: $20+2-1 \choose 20$ $-$ $7 \choose 1$ $15+2-1 \choose 15$$ $ + $ $$7 \choose 2$$10+2-1 \choose 10$$ $ - $ $$7 \choose 3$$45+2-1 \choose 5$$ $ + $ $$7 \choose 4$$0+2-1 \choose 0 $ I believe that I multiply the $5$ in the first polynomial to the coefficient I find, but my answer comes out to be $(5 \cdot (-35))=-175$ which I don't believe is possible. Is this the correct answer?","['combinatorics', 'generating-functions']"
3228543,Why does the comparison test fail here?,"Someone recently stumped me here. Consider the series $\sum_3^{\infty} \dfrac{1}{x^{1.0001}}$ and the series $\sum_3^{\infty} \dfrac{1}{x\ln{x}}$ . The first series in convergent, and the second series is divergent. Yet, for all $x \geq 3$ , $\dfrac{1}{x^{1.0001}} \geq \dfrac{1}{x\ln{x}}$ . How can this be? My conclusion is that the proposed inequality is actually false and so the direct comparison test cannot be applied, but the $x$ must be very large, I haven't been able to convince myself of this.","['limits', 'convergence-divergence', 'paradoxes', 'sequences-and-series']"
3228550,Probability of population parameter to be contained in confidence interval,"First of all, I have found this question: Interpretation of confidence interval which might seem as a duplicate. As well as this explanation http://onlinestatbook.com/2/estimation/confidence.html within it -- but I find it hard to understand. In the second link it is stated that: Confidence intervals for means are intervals constructed using a
  procedure that will contain the population mean a specified proportion of the time, typically either
  95% or 99% of the time. An example of a 95% confidence
  interval is shown below: 72.85 < Î¼ < 107.15 There is good reason to believe that the population mean lies between
  these two bounds of 72.85 and 107.15 since 95% of the time confidence
  intervals contain the true mean. But also: It is natural to interpret a 95% confidence interval as an interval
  with a 0.95 probability of containing the population mean. However,
  the proper interpretation is not that simple. One problem is that the
  computation of a confidence interval does not take into account any
  other information you might have about the value of the population
  mean. For example, if numerous prior studies had all found sample
  means above 110, it would not make sense to conclude that there is a
  0.95 probability that the population mean is between 72.85 and 107.15. What about situations in which there is no prior information about the
  value of the population mean? Even here the interpretation is complex.
  The problem is that there can be more than one procedure that produces
  intervals that contain the population parameter 95% of the time. Which
  procedure produces the ""true"" 95% confidence interval? For the sake of the discussion I rather refer to a situation in which one can have huge amount of data, and the confidence level can be extremely high. My questions are: If one can't say it is as likely for the population parameter to be inside the interval as the confidence level, why does the author write ""There is good reason to believe that the population mean lies between these two bounds (...)"" in the first paragraph? In the second quoted paragraph the author refers to a situation in which there was a prior information. I don't see how that is relevant. If your confidence level is, say $1-2^{-30}$ , it is indeed very unlikely you get an interval which contradicts previous studies. If it indeed happens, one must conclude that one of you had a mistake. You, or the previous studies. Where am I wrong? Also in the second paragraph the author writes: ... there can be more than one procedure that produces intervals that contain the population parameter 95% of the time. Which procedure produces the ""true"" 95% confidence interval? I didn't understand this line, what is he trying to say? To summarize, I'll try to compare it to null hypothesis rejecting, which I understand better: If I randomly pick a confidence interval from a set of confidence intervals, of which $1-2^{-30}$ contain the true population parameter, why can't I say it is as likely I have picked a good interval, as it is likely the null hypothesis should be rejected when $p\leq2^{-30}$ ? Note: I am a beginning math student. I have taken with passion some basic math classes such as Linear Algebra 101, and Calculus, but nothing more.
In statistics I have a reasonable understanding of basic hypothesis testing (null hypothesis, statistical significance, p-values).","['statistics', 'confidence-interval']"
3228573,"I need some help with the integral $\int_0^\infty \frac{x^3}{e^x+1} \, dx$","I've been stuck on an integral for days now and would love to get some help with it: $$\int_0^\infty \frac{x^3}{e^x+1} \, dx$$ My teacher was also ""kind"" enough to give me the answer to another similiar integral: $$\int_0^\infty \frac{x^3}{e^x-1}dx=\frac{\pi^4}{15}$$ So that I should (based on that answer) calculate the integral: $$\int_0^\infty \frac{x^3}{e^x+1} \, dx$$ I can't see how we can simplify/substitute anything so that it'll match the ""help integral"" so that we can use it's value to calculate the actual integral. First post on this page, so please be kind! :)","['integration', 'calculus', 'improper-integrals']"
3228617,Evaluate $\sum_{r=1}^{502} \lfloor \frac{305r}{503} \rfloor$.,"The question is to find the value of â€” $$\sum_{r=1}^{502} \Big \lfloor \frac{305r}{503}\Big \rfloor$$ The answer is pretty big, so I don't think trial and error will work here. I seriously can't come up with a solution to this problem. I have asked plenty of floor function questions (you can check my profile), but this one is quiet different. Can someone please help me out? Even a hint is greatly appreciated.","['number-theory', 'summation', 'elementary-number-theory', 'ceiling-and-floor-functions']"
3228619,Find the value of $\lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^n\vert f(a_{k+1})-f(a_k) \vert$,Define sequence $a_n=\cos(2n) \quad (n\geq1)$ and $f(x)=\frac{x}{\vert x \vert}$ . Find the value of $$\lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^n\vert f(a_{k+1})-f(a_k) \vert$$ I don't know how to approach. I couldn't find any regulation of numerator (I mean the sum part.).,"['limits', 'calculus', 'sequences-and-series']"
3228651,In each cell of $n$x$n$ is a number so that in every $3$x$3$ subtable the sum of numbers is negative and the sum of all numbers is positive.,"In each unit cell of a $n\times n$ table we have a number so that in every $3\times 3$ subtable the sum of numbers is negative and the sum of all numbers is positive.
  For which $n\geq 4$ we can have such an arangement? Clearly we don't have such an arangement if $3\mid n$ . Now suppose $3\nmid n$ . Then if $n=7$ (or $3$ or $10$ or $13$ ) we see that a configuration \begin{array} {|r|r|r|r|r|r|r|}
\hline
7 & -1 & -1& 7&-1&-1&7 \\
\hline
-1 & -1 & -1& -1&-1&-1&-1 \\
\hline
-1 & -1 & -1& -1&-1&-1&-1 \\
\hline
7 & -1 & -1& 7&-1&-1&7 \\
\hline
-1 & -1 & -1& -1&-1&-1&-1 \\
\hline
-1 & -1 & -1& -1&-1&-1&-1 \\
\hline
7 & -1 & -1& 7&-1&-1&7 \\
\hline
\end{array} works. So I tried to generalise this for an arbitrary $3n+1$ , instead of $7$ we put a positive $a$ and instead of $-1$ we put a negative $b$ . So I have to prove that there are such $a,b$ that satisfies $a+8b<0$ and $$(n+1)^2a+(8n^2+4n)b>0$$ for an arbitrary $n$ , but I fail to do that. Any idea how to solve this.","['combinatorics', 'discrete-mathematics']"
3228704,Confusion in Intersection of Images of sets.,"I am somehow unable to understand the following relation. if $f:X\rightarrow Y$ and $S,T\subseteq X$ then $$f(S\cap T)\subseteq f(S)\cap f(T)$$ The main problem is I always end up in showing the equality rather than the subset-ness. Example $f:X=\{x_1,x_2,x_3\}\rightarrow Y=\{y_1,y_2,y_3\}$ where $f$ is defined by $f(x_1)=f(x_2)=y_1$ , $f(x_3)=y_3$ and $S=\{x_1\},T=\{x_2,x_3\}$ Then clearly $f(S)\cap f(T)\subseteq f(S\cap T)$ does not hold,thus the equality is not possible. But My question is where I am making mistake in following proof : $$y\in f(S) \cap f(T)$$ $$\implies y\in f(S) \land y\in f(T)$$ $$\implies \exists x\in S \land \exists x\in T$$ such that $y=f(x)$ $$\implies x\in S\cap T$$ $$\implies f(x)=y\in f(S\cap T)$$ Hence $$f(S)\cap f(T)\subseteq f(S\cap T)$$ Can you please correct me,?",['elementary-set-theory']
3228738,Draw multiple elements from set: overlap,"Assume I have a finite set of $N$ elements. Now assume that $k$ elements will be drawn randomly from the set (without replacement). These $k$ draws will in total be repeated $I$ times, but between each draw $i\in I$ , there is replacement. How many unique elements $n\in N$ can I expect to draw as a function of $I$ , $N$ , and $k$ if I assume that each $n$ is equally likely to be chosen? What if each $n$ is not equally likely (because e.g. the likelihood approximates a normal distribution)? I believe that for the case of equal probability to be chosen, $I\cdot k-\frac{I(I-1)}{2}\cdot\left(\frac{k}{N}\right)^{2}\cdot k$ should do the trick. Am I right?","['statistics', 'probability']"
3228808,Show that $(I âˆ’ P)^2 = I âˆ’ P$ if $P=P^2$,Let $P $ be an $n \times n$ matrix and $I$ be the $n \times n$ identity matrix. Show that $$ (I âˆ’ P)^2 = I âˆ’ P $$ is valid if $P = P^2$ . I did the following. $$(I - P)^2 = I^2 - IP - PI + P^2 = I - P$$ where $I^2 = I $ because it is the identity matrix. Is this enough to show or did I miss something?,"['matrices', 'projection-matrices', 'linear-algebra', 'idempotents']"
3228829,"Determine whether the following expression is a tautology, a contingency, or a contradiction by using the logical equivalences","As a practice problem, I'm asked to determine whether the following proposition is a tautology, contradiction, or contingency through the use of logical equivalences. I get how to determine what truth value the proposition is and I understand most of the logic rules, but I'm stumped on what the first step would be on solving this one. I can't use distribution on ( ( ð‘ âˆ¨ ð‘ž ) âˆ§ ( Â¬ ð‘ âˆ¨ ð‘Ÿ ) ) because of the negation symbol on p. If I use de morgan's by moving that outer negation inside, I don't see how it would make a difference because all it'd do is just flip the signs. If I used association and moved the bracket to ( ( ð‘ž âˆ¨ ð‘Ÿ ) âˆ¨ ( ð‘ âˆ¨ ð‘ž )) I still can't use distribution because all the symbols are a âˆ¨. Â¬ ( ( ð‘ž âˆ¨ ð‘Ÿ ) âˆ¨ ( ( ð‘ âˆ¨ ð‘ž ) âˆ§ ( Â¬ ð‘ âˆ¨ ð‘Ÿ ) ) ) Any help or push in the right direction would be appreciated.",['discrete-mathematics']
3228928,What is the coefficient of $x^5$ in $(1+x+x^2+x^3+x^4+x^5)^{17}$?,I figured that $(1+x+x^2+x^3+x^4)^{17} = (1-x^6)^{17}*(1-x)^{-17}$ but don't know what else to do. I would really appreciate any help,"['binomial-coefficients', 'discrete-mathematics']"
3228950,Question about notation: The order notation,"For example, when we Taylor expand say $e^x$ about $x=0$ , we would write $$e^x = 1+x+\frac 12 x^2 + \frac 16 x^3 + \mathcal O(x^4) \qquad \qquad \text{as } x \rightarrow 0$$ with the use of the ""Big-O"" notation $\mathcal O$ . I was just wondering, is there a generalisation for this to ""multiple variables""? For example, if I Taylor expand $e^{x+y}$ about $(x,y) = (0,0)$ , we get $$e^{x+y} = 1 + x + y + \frac 12 x^2 + xy + \frac 12 y^2 + \text{higher order terms} \qquad \qquad \text{as } (x,y) \rightarrow (0,0)$$ where ""higher order terms"" in this case would refer to cubic or higher terms in $x,y$ , namely $x^3$ , $x^2y$ , $xy^2$ , $y^3$ or beyond. You can't just write something like $\mathcal O(x^3)$ or $\mathcal O((xy)^3)$ because there are terms like $x^2y$ that don't contain the $x^3$ . So...what's the correct way to express this?","['notation', 'multivariable-calculus', 'taylor-expansion', 'asymptotics']"
3228969,Role of $d^2 = 0$ in chain complex,"What is the motivation for requiring that the square of a differential be $0$ for a complex, aside from enabling us to speak of the homology of a complex? Other homological notions like chain maps, homotopic maps, homotopy equivalences seem to be meaningful without any restriction on the differential (of course, no longer do homotopic maps induce isomorphisms on Homology, for Homology no longer is meaningful). If we ignore any connections to Homology, is there some other moral reason to want that the differential square to $0$ ?","['homological-algebra', 'abstract-algebra', 'homology-cohomology', 'differential-forms']"
3229002,Covering a compact set with balls whose centers do not belong to other balls.,"Let $K\subset \Bbb R^n$ be a compact set such that each $x\in K$ is associated with a positive number $r_x>0$ . Claim : $K$ can be covered by a family of balls $$
\mathcal B = \{ B(x_i,r_i) : i=1,\dots,k\ \},
$$ where $r_i := r_{x_i}$ , such that for any distinct $i,j \le k$ , we have $$
x_i\notin B(x_j,r_j) \quad\text{and}\quad x_j\notin B(x_i,r_i).
$$ Is the claim true without any additional assumptions on $r_x$ 's? At first I thought of using Zorn's lemma to extract a maximal subfamily of balls from $\mathcal F = \{B(x,r_x/2) : x\in K \}$ such that any pair of balls is disjoint. However, enlarging the radii by a factor of $2$ may not be a cover of $K$ so this approach may not work.","['compactness', 'general-topology', 'measure-theory', 'real-analysis']"
3229044,Show that the characteristic polynomial is the same as the minimal polynomial,"Let $$A =\begin{pmatrix}0 & 0 & c \\1 & 0 & b \\ 0& 1 & a\end{pmatrix}$$ Show that the characteristic and minimal polynomials of $A$ are the same. I have already computated the characteristic polynomial $$p_A(x)=x^3-ax^2-bx-c$$ and I know from here that if I could show that the eigenspaces of $A$ all have dimension $1$ , I would be done. The problem is that solving for the eigenvalues of this (very general) cubic equation is difficult (albeit possible), meaning it would be difficult to find bases for the eigenspaces. A hint would be appreciated.","['jordan-normal-form', 'eigenvalues-eigenvectors', 'matrices', 'minimal-polynomials', 'linear-algebra']"
3229051,Check my work: General solution of a PDE,"I have been asked to find the ""Most general solution"" for $u(x,y)$ of the PDE $$\frac{\partial u}{\partial x} = \frac{x}{\sqrt{x^2+y^2}} + 3y\cos(3xy) + 3x^2y^2$$ I know you must take the integral of both sides to ""undo"" the partial derivative. Is it reasonable to then split the right hand side into three separate integrals as below? $$u(x, y) = \int\frac{x}{\sqrt{x^2+y^2}}dx + \int3y\cos(3xy)dx + \int3x^2y^2dx.$$ From here I got: $$\int\frac{x}{\sqrt{x^2+y^2}}dx = \frac{1}{2}\int\frac{2x}{\sqrt{x^2+y^2}}dx = \sqrt{x^2+y^2} + C(y)$$ $$\int3y\cos(3xy)dx = \sin(3xy) + C(y)$$ $$\int3x^2y^2dx = y^2x^3+C(y)$$ The final answer is: $$u(x, y) = \sqrt{x^2+y^2} + \sin(3xy) + y^2x^3+C(y)$$ Is this correct?","['integration', 'indefinite-integrals', 'partial-derivative', 'partial-differential-equations']"
3229068,Show that $\sin(x+y)$ is differentiable.,"Show that $f(x,y)=\sin (x+y)$ is differentiable in its domain by the definition i.e. prove $\lim_{(x,y) \rightarrow (x_0, y_0)}\frac{|\sin(x+y)-\sin{(x_0+y_0)}-\cos(x_0+y_0)(x-x_0)-\cos(x_0+y_0)(y-y_0)|}{\|(x,y)-(x_0,y_0)\|} = 0$ I can not find a way to compare $|f(x,y)-z|$ with $\|(x,y)-(x_0,y_0)\|$",['multivariable-calculus']
3229085,Local behavior of sheaf of ideals given by a closed immersion,"I know that if $Y \hookrightarrow X$ is a closed embedding i of schemes, then the sheaf of ideals $I_Y(U) = $ { $f \in \mathcal{O}_X(U)\text{ } | i^*(f) = 0$ } is quasi coherent. I sort of understand the proof that Hartshorne gives for this (Proposition 5.9, Chapter 2), but I'm having trouble with one thing conceptually. By the definition, $I_Y$ being quasi coherent means that there is an affine cover { $U_\alpha$ } of X such that $I_Y|_{U_\alpha} \cong \widetilde M$ for some $\mathcal{O}_X(U_\alpha)$ module M. What exactly is $\widetilde M$ and the isomorphism between $I_Y|_{U_\alpha}$ and $\widetilde M$ ? I don't see understand this structure based on the proof given. I'm not really sure intuitively what it should be either. (Still trying to get aquainted with algebraic geometry). I'd appreciate any help.","['quasicoherent-sheaves', 'algebraic-geometry', 'sheaf-theory', 'modules']"
3229088,Raising to the power of $i$,"We all know the usual exponentiation $a^i$ in the complex setting; one can ask is there such a map in a given group; more specifically is there a criterion in the general sense such that given an abelian group $G$ one can determine with the criterion whether there is a homomorphism $\phi_{i}$ such that for any $x \in G$ we have that $\phi_{i}(\phi_i(x)) = x^{-1}$ (because we need $(x^{i})^{i} = x^{-1}$ this is ofcourse very informal). Note that for groups $Z/(p)$ there is such a criterion. If there is a map $\phi_{i} $ in $Z/(p)$ then given a primitive element $q \in Z/(p)$ one has that $\phi_i(q) = q^{a}$ for some $a \in \mathbb{N}$ hence $\phi_i(q^{a}) \equiv q^{a^2} \equiv q^{-1}$ hence $a^2+1 \equiv 0$ $mod$ $(p-1)$ which is iff there is no prime $r$ that is $3 $ mod $4$ dividing $p-1$ . One automatically realizes that $Z/(2)$ has such a homomorphism, for $p \geq 3$ we also need $p \equiv 3$ mod $(4)$ as for $p \equiv 1$ mod $(4)$ we have that $p-1 \equiv 0 mod 4$ and hence if $\phi_i(q^{a}) \equiv q^{a^2} \equiv q^{-1}$ mod $(p)$ we have that $a^2+1 \equiv 0 mod 4$ which is impossible. After working through the details one arrives at the conclusion that if there is a homomorphism $\phi_{i}$ in $Z/(p)$ then $p = 2$ or $p \equiv 3 $ mod $(4)$ and $p-1$ has no prime $q$ which is $3$ mod $(4)$ (we need that any prime $q$ which is $3$ mod $(4)$ does not divide $p-1$ ). This criterion is only for a small class of finite groups.","['number-theory', 'group-theory']"
3229114,How to define pivot columns?,"When you use Gaussian elimination to solve a homogeneous system of linear equations, you end up with ""pivot variables"" and ""non-pivot variables"".  The non-pivot variables have the property that they can each be chosen freely, and once specified, they uniquely determine a solution to the equation. I'm looking for a characterization of these free variables that depends on the operator $A$ and the choice of basis $\{b_i\}$ but does not refer to the Gaussian elimination process itself. For example, you could look at the equation $x_1 + x_2 + x_3 = 0$ and determine that any two of the variables can be chosen freely and uniquely determine the third, whereas one variable is insufficient and three variables is too many to be chosen freely. For another example, take $$x_0 + x_1 + x_2 = 0\\ x_0 + x_3 + x_4 = 0.$$ The set $\{x_0, x_2,x_3\}$ consists of variables that can be freely chosen and uniquely determine a solution. In contrast, $\{x_0,x_1,x_2\}$ cannot be chosen freely, despite having three variables. My goal is to find a test that can identify which sets of variables uniquely and freely determine a solution. My starting point is Gaussian elimination, where the non-pivot rows show you one such subset of variables. I would like to be able to characterize all such sets of variables, without reference to Gaussian elimination. Here's my attempt. Let $A$ be a matrix with basis $B=\{x_1,\ldots,x_m\}$ . Reduce $A$ using Gaussian elimination. Let $N$ be the submatrix of $\text{rref}(A)$ consisting of the non-pivot columns. I believe $N$ is equivalent to the kernel map for $A$ expressed in our basis, in which case it can be defined without referring to the elimination processâ€”is that right? Consider a subset of variables $E\equiv \{e_1,\ldots, e_d\}\subseteq \{x_1,\ldots,x_m\}$ . These variables might have the desired property, or not. To determine whether $E$ has the desired property (i.e. the variables of $E$ may be chosen freely, and when they are chosen, they uniquely determine a solution to the homogeneous equation.), consider the linear map $Q:\mathbb{R}^d \hookrightarrow \mathbb{R}^m$ induced by the inclusion of $E$ into $B$ .  The requirement is that $QN$ is the identity map $I_{d\times d}$ . (Or maybe just that $QN$ is invertible.) I got this definition by trying to formalize the idea that each basis vector in $E$ meets (has a nonzero dot product with) the columns of the null matrix in exactly one unique place. That is, it forms a kind of identity sub matrix. Because it meets each vector exactly once, we know that the variables can be chosen freely and uniquely determine a solution. Is this right? Is there a better formulation? Thanks for your help. P.S. For example of how to apply this method, consider the following separate problems $A_1 = [1,1]$ , versus $A_2 = [1,0]$ .  Each of these problems is a system with two variables and one equation $(A : \mathbb{R}^m\rightarrow \mathbb{R}^n$ with $m=2$ , $n=1$ ). Each has a one-dimensional space of solutions (the nullity of $A$ is $d=1$ ). Our basis of variables $B$ consists of the standard basis vectors. Any set of $d=1$ vectors (i.e. the singleton sets $\{e_1\}$ , $\{e_2\}$ ) is a candidate for being a complete set of free variables. To test them, 
we consider the inclusions of $e_1$ or $e_2$ from $\mathbb{R}^m \hookrightarrow \mathbb{R}^d$ : $$E_1 = \begin{bmatrix}1 & 0\end{bmatrix}$$ $$E_2 = \begin{bmatrix}0 & 1\end{bmatrix}$$ We will need the kernel maps of $A_1$ and $A_2$ . These are maps $\mathbb{R}^d\rightarrow \mathbb{R}^m$ . The kernel of the matrices $A_1$ and $A_2$ are, respectively: $$K_1 = \begin{bmatrix}1\\ -1\end{bmatrix}\qquad u\mapsto \langle u,-u\rangle$$ $$K_2 = \begin{bmatrix}0 \\ 1\end{bmatrix}\qquad v \mapsto \langle 0, v\rangle$$ When we test whether $E_1$ and $E_2$ are free variables for the first matrix, we find: $$E_1K_1 = [1]\\ E_2K_1 = [-1]$$ Whereas for the second matrix, we find: $$E_1K_2 = [0]\\ E_2K_2 = [1]$$ By examining which of these is an invertible transformation $\mathbb{R}^d\rightarrow \mathbb{R}^d$ , we have determined that $\{e_1\}$ and $\{e_2\}$ are complete unique variable sets for the first system $A_1$ , but only $\{e_2\}$ is a complete unique variable set for the second system $A_2$ .","['systems-of-equations', 'category-theory', 'linear-algebra', 'linear-transformations', 'gaussian-elimination']"
3229119,Seeking intuition on hat-check probability depending on parity of $n$,"In (one version of) the hat-check problem, https://proofwiki.org/wiki/Hat-Check_Problem the question is to find the probability that for $n$ hat-checkers, nobody gets their own hat. If this is called $p_n,$ the solution is to obtain the $n$ th partial sum of the series for $1/e.$ So we have, for $n=1,2,3,\cdots,$ that $$p_n=\frac{1}{0!}-\frac{1}{1!}+\frac{1}{2!}-\frac{1}{3!}+\cdots+\frac{(-1)^n}{n!}.$$ For odd $n,$ the likelihood increases as $n$ does. This seems peculiar (to me) in that for more people checking their hats, it would become more likely that at least one person got their own hat, and so less likely that nobody did. For even $n$ as $n$ goes up the likelihood $p_n$ decreases as expected by the vague (incorrect) intuition proposed above. I'm looking for some reasonable intuitive reason why one would expect this even/odd $n$ behavior for this problem. Also from the answer, for any $m,n$ where $m$ even and $n$ odd, $p_m>p_n,$ which puzzles my intuition more. Any intuitions (not proofs, which can be done and are known anyway) appreciated.",['probability']
3229134,"How to find $\frac{âˆ‚f}{âˆ‚y}$ for Functions of the form $f(x,y,z(y))$?","This is the first time I'm using math.stackexchange. Please excuse me and correct me if I'm not doing things in the right format. So my question is this: given a function of the form $f(x,y,z(y))$ , and suppose we want to find $\frac{âˆ‚}{âˆ‚y} f(x,y,z(y))$ . Then by the Chain Rule, we would have something like this $$\frac{âˆ‚f}{âˆ‚y} = \frac{âˆ‚f}{âˆ‚y} + \frac{âˆ‚f}{âˆ‚z}\frac{dz}{dy}.$$ But this notation is really confusing since the two $\frac{âˆ‚f}{âˆ‚y}$ do not mean the same thing. Am I doing this correctly, or is there any better notation to clarify this expression? I would be much appreciated if someone could shed some light on me.","['notation', 'multivariable-calculus', 'calculus', 'chain-rule']"
3229146,Formation of commissions,"Of a group of seven women, Mary is one of them, and of four men, John is one of them. How many commissions can be formed with any number of people, provided there are the same number of men and women? I did so: 4 men and 4 women: $${7 \choose 4}  { {4} \choose {4} }  = 35 $$ 3 men and 3 women: $${7 \choose 3}  { {4} \choose {3} }  = 140 $$ 2 men and 2 women: $${7 \choose 2}  { {4} \choose {2} }  = 126 $$ 1 man and 1 woman: $${6 \choose 1}  { {4} \choose {1} }  = 24 $$ By the additive principle, it comes: $$35 + 140 + 126 + 24 = 325$$ The answer is 329. Have I forgotten a case or addressed the problem in the wrong way?","['combinations', 'combinatorics', 'discrete-mathematics']"
3229175,What is the probability that two cards drawn from a deck are both face cards and at least one is red?,"Two cards are drawn from a deck of cards. What is the probability that they are both face cards and at least one is red? Assume that there are $52$ cards and without replacement. I have two different methods, and they got different solutions. The second method below is the same as the teacher's answer: $0.0385$ , but the first is not. What is wrong with my first solution? Method 1: $\frac{12\cdot11}{52\cdot51}\cdot\frac{3}{4}=0.0373$ . I got this because I calculated the probability that I get two face cards, and then I multiplied by $\frac {3}{4}$ because there is $\frac {3}{4}$ chance that I get at least $1$ red. Method 2: $\frac{\binom{12}{2}}{\binom{52}{2}}-\frac{\binom{6}{2}}{\binom{52}{2}}=0.0385$ . The first fraction is the probability that both are face cards. The second fraction is the probability that they are both face cards and both black.","['card-games', 'probability']"
3229240,Can $\frac{\csc \alpha +\cos \alpha}{\cos \alpha - \tan \alpha - \sec \alpha}$ be simplified?,"I am trying to simplify the following but I cannot. $$
\frac{\csc \alpha +\cos \alpha}{\cos \alpha - \tan \alpha - \sec \alpha}
$$ Can it be simplified? Edit My last result is $$
- \frac{\cos \alpha \left( 1 + \sin \alpha \cos \alpha\right)}
{\sin^2 \alpha \left(1+\sin \alpha\right)}
$$ I am wondering it might be a wrong question given by my student's teacher.",['trigonometry']
3229256,Proving that $\sum_{k=0}^{2n} {2k \choose k } {2n \choose k}\left( \frac{-1}{2} \right)^k=4^{-n}~{2n \choose n}.$,"I have happened to have  proved this sum while attempting to prove another summation. Let $$S_n=\sum_{k=0}^{2n}  {2k \choose k }  {2n \choose k}\left( \frac{-1}{2} \right)^k$$ ${2k \choose k} $ is the coefficient of $x^0$ in $(x+1/x)^{2k}$ .
Consequently, $S_n$ is the coefficient of $x^0$ in $$f(x)= \sum_{k=0}^{2n} {2n \choose k}~\left (x+\frac{1}{x}\right)^{2k}~\left ( \frac{-1}{2}\right)^{k}=\left(1-\left(\frac{x+1/x}{\sqrt{2}}\right)^2 \right)^{2n} = 4^{-n} ~ \left(x^2+\frac{1}{x^2}\right)^{2n}.$$ Finally, the coefficient of $x^0$ in $f(x)$ is $$4^{-n}~{2n \choose n}=S_n.$$ I hope that you will find it interesting and prove it in some other way. Do try!","['binomial-coefficients', 'binomial-theorem', 'sequences-and-series']"
3229473,"Linear bounded operator from $L^p[0,1]$ to itself whose range consists of continuous functions.","Let $T\colon \mathbb L^p[0,1]\to \mathbb L^p[0,1]$ , $1<p<+\infty$ , be a linear bounded operator such that $\operatorname{Im}(T)$ is contained in the space of continuous functions. It was shown in this thread that $T$ is a compact operator. Some non trivial example of such $T$ can be given, for example kernel operators with appropriated conditions on the kernel. The question is 
whether there exists other example (maybe asking for a characterisation is too ambitious).","['compact-operators', 'operator-theory', 'examples-counterexamples', 'lp-spaces', 'functional-analysis']"
3229538,Counting the Number of Lattice Points in an $n$-Dimensional Sphere,"Let $S_n(R)$ denote the number of lattice points in an $n$ -dimensional ""sphere"" with radius $R$ . For clarification, I am interested in lattice points found both strictly inside the sphere, and on its surface. I want to count exactly how many such points there are. This count corresponds to the amount of sets of integers that are solutions to the equation $$a_1^2+a_2^2+a_3^2+...+a_{n-1}^2+a_n^2= R^2$$ where the $a_i$ 's are required to be integers, not necessarily positive, and sets that differ just by the order of the summands are also considered distinct , i.e for the case $n=3, R=5$ , the following are both counted: $3^2+(-4)^2+0^2$ and $(-4)^2+0^2+3^2$ . This can also be interpreted as the sum of squares function - $r_d(k)$ denotes in how many such ways I can write $k$ as the sum of $d$ squares. This means that $$S_n(R) = \sum_{k=1}^{R^2}r_n(k)$$ I am asking this question as a general one, but I am mostly concerned about the cases of $n=2, n=3,n=4$ . In order not to be too lengthy and remain focused, I won't explain why but summing this way requires to factor each $k$ first. This is very time consuming, so I searched for better ways. For the case of $n=2$ , which is essentially just the count of lattice points in a circle of radius $R$ , I have managed to find that $$S_2(R)= 1+\sum_{i=1}^\infty\biggl(\biggl \lfloor\frac{R^2}{4i+1}\biggl \rfloor-\biggl \lfloor\frac{R^2}{4i+3}\biggl \rfloor\biggl )$$ For the case of $n=4$ , I found: $$S_4(R)= 1+8\sum_{k=1}^{R^2}\sum_{d|k \atop {4\nmid d}}d$$ Unfortunately, for the case of a sphere ( $n=3$ ), I have found no formula, neither for the count of lattice points inside nor on the surface of the sphere. Although these formulas do indeed provide the exact count of lattice points, they are very slow in computational terms, as their running time complexity is $O(R^2)$ . I was wondering if a more efficient way exists to count the number of such lattice points, perhaps $O(R)$ or $O(R^{1/2+\epsilon})$ , so that I can work with radii as big as $10^9$ or even more. I suspect there is a way, but I can't find it. Not necessarily a different approach to the problem, but rather just a more clever way to reduce the order of the sum. Also, any insight about $S_3(R)$ will be appreciated.","['number-theory', 'geometry', 'linear-algebra', 'combinatorics', 'computer-science']"
3229549,Evaluate the indefinite integral of multiplication of two functions,"Let $f,g:[a,+\infty]\to\mathbb{R}$ be continuous and $K>0$ s.t. $$
\left|\int_c^df(x)\,dx\right|\leq K \quad \forall c,d\in [a,\infty).
$$ If $g\in C^1$ is decreasing with $\displaystyle\lim_{x\to \infty}g(x)=0,$ prove that the limit $$\int_a^\infty f(x)g(x)\,dx=\lim_{x\to\infty} \int_a^x f(t)g(t)\,dt$$ exists. I tried to use the Cauchy criterion, that is $\forall \epsilon>0, \exists A>a$ s.t. $A<c<d$ implies that $\left|\int_c^d f(t)g(t)\,dt\right|<\epsilon.$ However I could not get anywhere. I appreciate any suggestions. P.S. I already posted this question and as its expression was not completely right, I deleted it and asked it again here.","['integration', 'riemann-integration', 'real-analysis']"
3229568,"$U$~$N(3,16)$ $V$ ~$\chi_{9}^{2}$ U and V are independent random variables. Find $P(U-3<4.33\sqrt{V})$","$U$ ~ $N(3,16)$ $V$ ~ $\chi_{9}^{2}$ U and V are independent random variables.
Find $P(U-3<4.33\sqrt{V})$ (The notes I'm working through don't seem to approach this rigorously...) The answer is $P(t_{9} <3\times\frac{4.33}{4})$ and I'd appreciate a rigorous solution. where $t_{9}$ is the t distribution with 9 degrees of freedom.","['statistical-inference', 'statistics', 'probability-distributions', 'sampling']"
3229580,Using chain rule to differentiate $f(x)=a(x)b(x)$?,Why can I not apply the chain rule to a product in the following way. If we have some product: $$f(x)=a(x)b(x)$$ Consider the multiplication of b by a as anotherâ€™s function so that: $$f(b(x))=ab$$ So that $\frac{df}{dx} = fâ€™(b)bâ€™(x)$ Something feels very wrong. But I canâ€™t put my finger on it.,"['derivatives', 'chain-rule']"
3229676,Showing $E(X) = \sum_{i}E(X\mid A_i)P(A_i)$,"Following is my proof. Suppose $X$ is a discrete-type random variable ranging in the set $S$ and $\{A_i : i=1,2,3,\dots\}$ is a finite or
countably infinite partition of a sample space $\Omega$ . We have $$P(X=x) = \sum_i{P(X=x\mid A_i)P(A_i)}$$ So $$xP(X=x) = \sum_i{xP(X=x\mid A_i)P(A_i)}$$ Thus $$
\begin{aligned}
E(X) &= \sum_{x\in{S}}{xP(X=x)} \\
&= \sum_{x\in{S}}{\sum_i{xP(X=x\mid A_i)P(A_i)}} \\
&= \sum_i{\sum_{x\in{S}}{xP(X=x\mid A_i)P(A_i)}} \\
&= \sum_i{P(A_i)E(X\mid A_i)}
\end{aligned}
$$ Is this right?","['expected-value', 'probability-theory', 'probability']"
3229677,What sets can we define continuity and differentiability on?,"I am an undergraduate Physics student (completing my first year shortly) who has had a (first) course on Calculus, and another on Linear Algebra. When working with differential equations (in physical problems), I decided to look with mathematical rigour what I was dealing with. So I decided to start with the beginning. The following questions popped up. Let $\Bbb{F=R}$ or $\Bbb{C}$ . Generalising Intervals What is the concept parallel to an interval in $\Bbb{R}$ , in $\Bbb{C}$ ? (Note: Iâ€™m not looking for contours, rather something which generalises the â€œrectanglularâ€ and â€œcircularâ€ domains.) Can the answer be open or closed subsets of $\Bbb{C}$ ? Can it be generalised to $\Bbb{F}^n$ ? Generalising Continuity and Differentiability Suppose we have a subset $S\subseteq \Bbb{F}$ . What properties must $S$ have so that some form of continuity and differentiability can be defined on some subset $\mathcal{S} \subseteq S$ ?
(See this post (later in the post) and this .)
(Iâ€™d appreciate if your answer is generalisable to $\Bbb{F}^n$ .) Extending Normal Calculus For such a â€œcontinuous and differentiableâ€ $\mathcal{S} \subseteq S$ , what theorems of normal calculus will apply? Also, how will the integral, if needed, be redefined? Finally, ODEâ€™s Now turning to ODEâ€™s, I found a very satisfying definition of an ODE here . But unfortunately, it is restricted to intervals of $\Bbb{R}$ . How can you generalise this definition to $\Bbb{C}$ ? (Thatâ€™s why I was asking for generalisation of interval.) Very importantly, can we have solutions to ODEâ€™s not as â€œintervalsâ€, but sets like $\mathcal{S}$ above?","['continuity', 'calculus', 'ordinary-differential-equations']"
3229722,Prove statement about Minkowski functional,"I have this following to prove: Let $X$ be normed space and $C \subset X$ be an open, convex subset of $X$ with $0\in C$ . Show that: $C= \{x \in X: p_C(x) < 1\}$ With $p_C$ defined as the Minkowski functional of $C$ : $p_C : X \to [0,\infty]$ , $x \to \mathbb{inf}\{\alpha > 0: x \in \alpha C\}$ I could show that $\{x \in X: p_C(x) < 1\} \subset C$ but don't know how to show the other way. I suppose I need to use the openness of $C$ . Any hint would be appreciated.",['functional-analysis']
3229773,Find the algebraic dimension of space of matrices,"From A. Gathmann's Algebraic Geometry (2014): Exercise 2.31. Let $X$ be the set of all $2 \times 3$ matrices over a field $K$ that have rank at most $1$ , considered as a subset of $\mathbb A^6 = \mathrm{Mat}(2\times 3,K)$ . Show that $X$ is an affine variery. Is it irreducible? What is its dimension? My take on this is the following: Call $M_1$ , $M_2$ and $M_3$ the three $2 \times 2$ minors of any $2 \times 3$ matrix. Then $$X = V(\det M_1, \det M_2, \det M_3).$$ Trivially, this shows $X$ is an affine variety.
Moreover, the ideal generated by the three determinants (call it $I$ ) looks prime (my guess is that $\det M_i$ and $\det M_j$ are coprime when $i \neq j$ ), thus the above expression is actually an irreducible decomposition of $X$ (which shows, in turn, that $X$ is not reducible). Finally, I guess $(\det M_i, \det M_j)$ to be the only prime ideals under $I$ , so that the (Krull) dimension of the latter is $3$ . Hence I guess the dimension of $X$ is $3$ as well. As you see, there a lot of guesses: are they correct? Do they need more discussion?","['algebraic-geometry', 'proof-verification', 'commutative-algebra']"
3229776,Equivalence of possible definitions of quasi-coherent sheaves on a prescheme,"this is my first question on stackexchange. So, please don't be too hard to me. I am listening to a master course in Algebraic Geometry. My question belongs to Proposition II.5.4 in Hartshorne's ""Algebraic Geometry"". I want to show that the following statements are equivalent: Proposition : Let $(X, \mathcal{O}_X)$ be a prescheme and $\mathcal{F}$ be an $\mathcal{O}_X$ -module. Then the following statements are equivalent: 1) For all open affine $U \subseteq X$ there is an $\Gamma(U, \mathcal{O}_X)$ -module M, sucht that $F_{|U} \cong \tilde{M}$ . 2) There is an open affine covering $\{U_i\}_{i\in I}$ of $X$ and some $\Gamma(U_i, \mathcal{O}_X)$ -modules $M_i$ , such that $\mathcal{F}_{|U_i} \cong \tilde{M_i}$ .} The implication from 1) to 2) is clear. To prove the other direction, I oriented at the proof of Hartshorne. I hope, you can fill the gaps: First, I proved the following lemma: Lemma : Let $M$ be an $R$ -module and $\mathcal{F}=\tilde{M}$ . Then $\mathcal{F}_{|D(f)} \cong \tilde{M_f}$ for any $f \in R$ . Proof of the proposition : Let $U =SpecR \subseteq X$ be open and affine. Assume, the second statement holds. We denote the corresponding ring of $U_i$ by $R_i$ . Then $\{U \cap U_i\}_{i \in I}$ is an open, not necessarily affine covering of $U$ . Step 1: We want to show, that $\mathcal{F}{|U}$ satisfies the second statement. Since $U \cap U_i \subseteq U_i$ is open, \begin{align*}
U \cap U_i = \bigcup_{j \in J_i} D(g_{ij})
\end{align*} for some $g_{ij} \in R_i$ . Therefore, $\{D(g_{ij}) ~|~ i \in I, j \in J_i\}$ is an open affine coveriung of $U$ . Using the lemma and the assumption, we have: \begin{align*}
\mathcal{F}_{|D(g_{ij})} = {\mathcal{F}_{|U_i}}_{|D(g_{ij})} \cong \widetilde{{(M_i)}_{g_{ij}}}
\end{align*} Step 2: We want to finish the proof. We define $M = \Gamma(U, \mathcal{F}_{|U})$ . So we have the identity map \begin{align*}
M \to \Gamma(U, \mathcal{F}_{|U}).
\end{align*} Since $\widetilde{}$ and $\Gamma$ are adjoint functors, this leads to a map \begin{align*}
\alpha \colon \tilde{M} \to \mathcal{F}_{|U}.
\end{align*} Now, I want to use that $\alpha_{|D(g_{ij})}$ is the isomorphism, which we received in step 1. Because then, since the $D(g_{ij})$ 's are an open covering, $\alpha$ has to be an isomorphism. But why is this the case? Thank you in advance for your help. And please tell me, if I am abusing notation. I want to be as formal as possible. Felix",['algebraic-geometry']
3229781,"show $\mathbb{C} [x,y]/(xy-1)$ is a principal ideal domain","I've got to show that $A:=\mathbb{C} [x,y]/(xy-1)$ is a principal ideal domain I know that $A$ is isomorphic to $\mathbb{C}[t,t^{-1}]$ and that this a subfield
of $\mathbb{C}[t]$ which is a PID. So then $A$ is also a PID. But however I've got to work with the canonical inclusion map $i: \mathbb{C}[x]  \hookrightarrow A$ in order to show then $I \cap \operatorname{Im}(i) \neq \{0\}$ for every ideal $I \neq \{0\}$ . I have no idea why I need this.","['principal-ideal-domains', 'algebraic-geometry', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
3229789,using trigonometric identities,"For proving $$\frac {16}{\cos (4x)+7} =\frac{1}{\sin^4x +\cos^2x} +\frac{1}{\sin^2x +\cos^4x} $$ I tried to use that: \begin{align}
\sin^4 x +\cos^4 x&=\sin^4 x +2\sin^2x\cos^2 x+\cos^4 x - 2\sin^2x\cos^2 x\\
&=(\sin^2x+\cos^2 x)^2-2\sin^2x\cos^2 x\\
&=1^2-\frac{1}{2}(2\sin x\cos x)^2\\
&=1-\frac{1}{2}\sin^2 (2x)\\
&=1-\frac{1}{2}\left(\frac{1-\cos 4x}{2}\right)\\
&=\frac{3}{4}+\frac{1}{4}\cos 4x
\end{align} but i can't try more",['trigonometry']
3229790,Limit of outer measure of set intersection,"This is a question on Axler's book Measure, Integration & Real Analysis available here: http://measure.axler.net/ Prove that $|A|=\lim_{k\to\infty} |A\cap [-k,k]|$ for all $|A|\subset \mathbb{R}$ . where $|\cdot|$ is the outer measure. Essentially I think that by the definition given in that book, we have $$\lim_{k\to\infty}|A\cap[-k,k]|=\lim_{k\to\infty}\inf\left(\sum_{j=1}^\infty \ell(I_j):I_1,\text{... are open sets s.t. } A\cap[-k,k] \subset\bigcup_{j=1}^\infty I_j\right)$$ And I feel like we can just say the $\lim \inf$ of this is $|A|$ , but I can't really justify why.",['measure-theory']
3229799,"Eliminate $\alpha,\beta,\gamma$ from the system of equations","Eliminate $\alpha,\beta,\gamma$ from the following system of equations. $$a\cos(\alpha)+b\cos(\beta)+c\cos(\gamma)=0$$ $$a\sin(\alpha)+b\sin(\beta)+c\sin(\gamma)=0$$ $$a\sec(\alpha)+b\sec(\beta)+c\sec(\gamma)=0$$ My try: Squaring and adding first two equations, we get: $$\cos(\alpha-\beta)=\frac{c^2-a^2-b^2}{2ab}$$ $$\cos(\gamma-\beta)=\frac{a^2-c^2-b^2}{2cb}$$ $$\cos(\alpha-\gamma)=\frac{b^2-a^2-c^2}{2ac}$$ Now the RHS of above all looks like negative cosines of triangle $\Delta ABC$ . But I am not sure whether it will help. This question is taken from plane trigonometry part 1 by SL Loney book. Page number 264, question number 176.","['algebra-precalculus', 'trigonometry']"
3229823,Resolving $\lim\limits_{x \to \infty} (x-{\sqrt x})$,"I'm relearning limits and I'm stuck at an exercise. I have to resolve the following limit $$\lim\limits_{x \to \infty} (x-{\sqrt x})$$ If I use $\infty$ instead of $x$ , it'll be $\infty - \infty$ and it gets undefined. I tried rewriting the square root to $x^{1/2}$ but then it remains undefined. How can I resolve this kind of problem?","['limits', 'calculus']"
3229874,Factoring Constants out of Matrix Products,"This seems like a rather trivial question, but I just want to know first if I am correct that this step is valid and, if so, if there's a name to it. It surely isn't linearity, unless I am mistaken. Suppose we have matrices A and B. Further, B is a scalar multiple of some other matrix, so perhaps we have $B = 3C$ . My question is, would it be valid in multiplying $A$ and $B$ to pull out the constant $3$ and then multiply it by the product of $A$ and $B$ ? In  other words: $$AB = A(3C) = 3(AC)$$ In other words, it is necessarily the case that the above must hold? Is there a name for such a property? Thanks. I apologize for how elementary this surely is.",['matrices']
3229905,Finding $\lim\limits _{x\to-\infty} \frac{\sqrt[3]{x^3+x}}{x}$,"$$\lim\limits _{x\to-\infty} \frac{\sqrt[3]{x^3+x}}{x}$$ I have to resolve this limit, I tried factoring out x, I tried rewriting $x^3+x$ as $x^3(1+ \frac{x}{x^3})$ and it doesn't seem to cancel. What should I do?","['limits', 'calculus']"
3229913,In how many ways can we partition a set into smaller subsets so the sum of the numbers in each subset is equal?,"Let $A = \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\}$ .How many ways are there to partition this set into at least 2 subsets so the sum of the numbers in each subset is equal? Here's what I've tried:
Let $n$ be the number of subsets and $s$ the sum of the numbers in a subset(Where all the sets are equal). Since $n\cdot{}s=45$ , $n$ has to divide $45$ and on the other hand, $s$ can't be smaller than $9$ so the only options for $n$ are $3$ and $5$ . Now I know I can just count how many possible solutions there are given the fact that $n$ has only $2$ possible values but I was wondering is there a more elegant solution? P.S: This actually came in a math contest and I didn't have time to count all the possible solutions so if there aren't any ""better"" solution, what do you think I should have done since I only had around $5$ minutes for this question.","['combinations', 'combinatorics']"
3229928,Evaluate $\iint_E xy\ dx\ dy$ over the region $E$ common to the circles $x^2+y^2=x$ and $x^2+y^2=y$ using change of variable,"I have solved the problem just by taking limits in $y$ first, then in $x$ and got the answer ${1\over 96}$ i.e. $$\iint_E xy\ dy\ dx=\int_{x=0}^{1/2}\int_{y=\left(1-\sqrt{1-4x^2}\right)/{2}}^{\sqrt{x-x^2}}xy\ dy\ dx={1\over 96}.$$ But the author asks to use the change of variable- $${x^2+y^2\over x}=u$$ and $${x^2+y^2\over y}=v$$ But I cannot get the limits of $u$ and $v$ , I am also facing problem to find the Jacobian as we need to find $x$ and $y$ explicitly. Can anybody solve the problem using the change of variable? Thanks for the assistance in advance.","['integration', 'multivariable-calculus', 'calculus']"
3229953,Number of permutation matrices within a distance of a given matrix,"Given a matrix $M$ , and a permutation $P_0$ , is it possible to easily count, or easily approximately count, the number of permutation matrices $P$ that satisfy $\|P - M\| = \|P_0 - M\|$ ? What about the permutations that satisfy $\|P - M\| \leq \|P_0 - M\|$ ? What about list all the permutations $P$ that satisfy either the equality or inequality constraint? If someone knows how to solve this under some useful matrix norm, or with some restriction on $M$ , or some other restriction, please do share.","['graph-theory', 'combinatorics', 'discrete-mathematics']"
3230019,Semiperimeter of isosceles Heronian triangles.,"A Heronian triangle is a triangle with integer sides and area, named after Heron's formula which states that the area of a triangle with sides $a$ , $b$ , and $c$ is $$
  A = \sqrt{s(s-a)(s-b)(s-c)}
$$ where $s = (a + b + c)/2$ is the semiperimeter of the triangle. Example The five smallest isosceles Heronian triangles (by semiperimeter) are 8: (5,5,6)
9: (5,5,8)
16: (10,10,12)
18: (10,10,16), (10,13,13) with areas $12, 12, 48, 48$ , and $60$ respectively. Question The possible semiperimeters of isosceles Heronian triangles is given by the list 8, 9, 16, 18, 24, 25, 27, 32, 36, 40, 45, 48, 49, 50, 54, 56, 63, 64, 72, 75, ... which agrees with OEIS sequence A046790 as far as I've checked. A046790: Positive numbers divisible by 8 or by the square of an odd prime. How do you prove (or disprove) that a number is a semiperimeter of isosceles Heronian triangle if and only if it is in A046790?","['triangles', 'experimental-mathematics', 'geometry']"
3230021,How to make Laguerre's equation into Sturm-Liouville form,"how do I put Laguerre's diff equation into Sturm-Liouville form? $$xy'' + (1 - x)y' + Î»y = 0$$ I have to use the integrating factor method, for which I obtained $e^{-x}$ Is the integrating factor correct? How do I go about completing it? Many thanks.","['sturm-liouville', 'ordinary-differential-equations']"
3230029,"""Easy"" way to derive Black-Scholes delta","I was always amazed that the Black-Scholes delta i.e. the following expression: $$\frac{\partial}{\partial S}\left[ S\cdot \Phi\left(\frac{log \left(\frac{S}{K}\right)+(r+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}} \right)-Ke^{-rT} \cdot \Phi\left(\frac{log \left(\frac{S}{K}\right)-(r+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}} \right)\right]$$ Is just equal to the value of the CDF on the left: $$\Phi\left(\frac{log \left(\frac{S}{K}\right)+(r+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}} \right)$$ And similarly for derivative with respect to $K$ despite the fact that $S$ and $K$ appear in the middle of both of them. Recently I've stumbled upon a paper which states that this fact follows from Euler's theorem as the expression is homogeneous of degree $1$ as a function of $S$ and $K$ and therefore can be written as a linear combination of its partial derivatives: $$f(S,K)=S \cdot \frac{\partial f}{\partial S}+K \cdot \frac{\partial f}{\partial K}$$ I see how this makes the above result much more reasonable but I don't see how it actually proves this fact. Imagine for example the function $f(S,K)=S+K$ . We have: $$f(S,K)=S\cdot \frac{K}{S}+K\cdot \frac{S}{K}$$ And yet $\frac{\partial f}{\partial S}=1 \neq \frac{K}{S}$ as well as $\frac{\partial f}{\partial K}=1 \neq \frac{S}{K}$ . Can this reasoning be fixed somehow, so that it is possible to compute this derivative without angaging in messy calculations?","['partial-derivative', 'calculus', 'derivatives', 'analysis']"
3230060,Why is the interchange of a sum and an integral not justified here?,"This problem arose when I was studying the analytic continuation of the $\Gamma$ function. Consider $$f(z) = \int_{0}^1 e^{-t}t^{z-1} dt$$ This integral converges only for $\Re(z)>0$ . However, if we expand $e^{-t}$ into power series and interchange the summation and integration, we get that $$f(z) = \sum_{n=0}^{\infty} \dfrac {(-1)^n}{n!(n+z)}$$ which converges for every $z \not = -n$ . This of course means that the interchange of summation and integration was not justified. However, I attempted to come up with a justification anyway, and I cannot find the mistake. Can you help me find it please? Justification We will use the Weierstrass $M$ -test. First, if we replace $e^{-t}$ by a power seires and simplify, we get $$f(z) = \int_0^1 \left[\sum_{n=0}^{\infty} \dfrac {(-1)^n}{n!}t^{n+z-1} \right]dt$$ We will now show that for $0< \delta < 1$ , the series converges uniformly on $[\delta, 1]$ . We have $$\left|\dfrac {(-1)^n}{n!}t^{n+z-1}  \right| = \dfrac {1}{n!}t^{\Re(z)+n-1}$$ For $n$ large enough (greater than some $N$ ), we have $\Re(z)+n-1 > 0$ , and as a result, $$\dfrac {1}{n!}t^{\Re(z)+n-1} < \dfrac {1}{n!} = M_n, \text{ and } \sum_{n=N}^{\infty} M_n \text{converges}$$ Therefore $$\sum_{n=N}^{\infty} \dfrac {(-1)^n}{n!}t^{n+z-1}$$ converges uniformly on $[\delta, 1]$ , and as a result, $$\sum_{n=0}^{N-1} \dfrac {(-1)^n}{n!}t^{n+z-1}  + \sum_{n=N}^{\infty} \dfrac {(-1)^n}{n!}t^{n+z-1} = \sum_{n=0}^{\infty} \dfrac {(-1)^n}{n!}t^{n+z-1} $$ converges uniformly on $[\delta, 1].$ Therefore $$\int_{\delta}^1 \left[\sum_{n=0}^{\infty} \dfrac {(-1)^n}{n!}t^{n+z-1} \right]dt = \sum_{n=0}^{\infty} \left[ \int_{\delta}^1  \dfrac {(-1)^n}{n!}t^{n+z-1} dt \right] = \sum_{n=0}^{\infty} \left[ \dfrac {(-1)^n}{n!(n+z)}- \dfrac {(-1)^n}{n!(n+z)} \delta^{n+z} \right]$$ $$= \sum_{n=0}^{\infty}  \dfrac {(-1)^n}{n!(n+z)} - \sum_{n=0}^{\infty} \dfrac {(-1)^n}{n!(n+z)} \delta^{n+z}$$ Now looking at the sum on the right as a function of $\delta$ , we can argue that the series converges uniformly by splitting up the series as we did above. Thus if we take the limit as $\delta \to 0$ , that sum vanishes and we get that $$\int_{0}^1 \left[\sum_{n=0}^{\infty} \dfrac {(-1)^n}{n!}t^{n+z-1} \right]dt = \sum_{n=0}^{\infty}  \dfrac {(-1)^n}{n!(n+z)} $$ , as desired.","['complex-analysis', 'gamma-function', 'analysis', 'real-analysis']"
3230061,Do i need to find the elements of all powerset to answer such questions?,"Let $M = \{\{2\},2,5,7,0,\{23\}\}$ . Q1. $\{7, \{23\}\} \in \mathcal{P}(M)$ ?
A1. Since they are elements in m then they are true. Q2. $\{\{2\},\{5,7\}\} \subseteq \mathcal{P}(M)$ ? However here I am confused? What should I do? A2. To solve it I usually turn the symbol back to $\in$ , and then if the answer was true I just flip it to false? is this approach right?","['elementary-set-theory', 'discrete-mathematics']"
3230085,Oscillations of an Energy Eigenstate,"Energy eigenstates of a 1-dimensional particle are given by solutions to differential equations of the form $$
\left(-\frac{\hbar^2}{2m} \frac{d^2}{dx^2} + V(x) \right) \psi(x) = E\psi(x)
$$ where $V$ is the potential, $\psi$ is the energy eigenstate, and $E$ is the energy eigenvalue. In introductory (and even fairly advanced) texts on quantum mechanics, the following facts are asserted without proof: The set $\{E \in \mathbb{R} : E \text{ is an energy eigenvalue and } E < \sup_{x \in \mathbb{R}} V(x) \}$ is a discrete subset of the reals bounded below by $\inf_{x \in \mathbb{R}} V(x)$ . If the discrete energy eigenvalues above are listed in ascending order as $E_0, \ldots, E_n$ and have corresponding energy eigenstates $\psi_0, \ldots, \psi_n$ , then $\psi_n$ will have $n + 1$ local maxima. The above results are often stated with less precision, and sometimes even conflict with each other. What statements like 1 and 2 above are actually true, and how does one go about proving them?","['quantum-mechanics', 'mathematical-physics', 'ordinary-differential-equations']"
3230137,Does anyone recognize the function from this picture?,"I was playing with the exterior algebra, and stumbled on this interesting function from $\Bbb N^2 \to \Bbb N$ , which I'll call $f(x,y)$ . This is plotted from $1 \leq x,y \leq 100$ : In this picture, yellow values are higher, blue values are lower. So you can see the function is at its lowest when $x:y$ is closest to a simple ratio. For all $x$ , we have $f(x,x) = 2$ . However, for sufficiently large $x$ , we also have $f(x,x+1) = 3$ , $f(x,x+2)=4$ , etc, so this function is also in some sense measuring the distance to the nearest low-slope line. In comparison, here's the gcd(x,y) function: So you can see that while both functions feature prominent rational slopes, that the first one seems to be ""smoothed"" relative to the GCD, and also reaches minima at rational slopes rather than maxima . Is there some simple function that this resembles? Here is a CSV, if anyone wants to play with it, but due to the way I calculated this function, there may be some sporadic errors (though should be good for $x,y < 50$ or so: https://pastebin.com/raw/vEWAdBVM Just to say how I got this function: So for each $x,y$ pair, I generated the vector $(x,y,1)$ . Then, I wanted to find the ""shortest"" integer bivector that is the wedge product of $(x,y,1) \wedge v$ for some $v$ with integer coordinates. This is a plot of the norm of the resulting shortest bivector for each $(x,y)$ . I was using the $\ell_1$ norm to measure shortest for the thing I'm doing, but you get the same basic plot with the $\ell_2$ norm, as well as any $\ell_p$ norm. For each $(x,y)$ pair above, I ran a Monte Carlo search testing 10000 randomly generated bivectors and took the best one; after a few tries everything converged to the plot above. If you start with $(x,y,0)$ instead, you seem to get the basic GCD function instead, but $(x,y,1)$ gives the ""smooth nega-GCD"" pattern above. I was surprised to find something so simple and beautiful from such a strange starting point!","['elementary-number-theory', 'functions', 'special-functions', 'graphing-functions']"
3230153,Solve ODE $y'' = y^3 -y y'$,"I want to solve $y'' = y^3 -y y'$ with boundary conditions $y(1) = 1/2$ and $y(2) = 1/3$ but not sure how to start. If someone could give me a complete step by step explanation, it would be greatly appreciated because I want to fully understand it. Edit: By the hint $(y'+\frac{y^2}{2})'=y^3$ $y' + \frac{y^2}{2} = \frac{y^4}{4} + c1$ $\frac{dy}{dx} = \frac{y^4}{4} - \frac{y^2}{2} + c1$ Integrate $\frac{dy}{\frac{y^4}{4} - \frac{y^2}{2} + c1} = dx$ Integration looks very messy.
I know that the answer is $y = \frac{1}{(x+1)}$",['ordinary-differential-equations']
3230167,Calculating an average speed (simple),"The other day I had to cover a distance of 241 km. The first 99 km I covered within 33 hours, so my speed was 3 km/h. The next 120 km I covered within 30 hours, so the speed was 4 km/h. Then it was 21 km within 3 hours at the speed of 7 km/h. And the very last kilometer I finished within one hour. Thus, I covered the whole distance within 67 hours (33+30+3+1=67). And my average speed was 3.59 km/h (241/67 = 3.59) However, if I sum up all the four above-mentioned speeds and divide them by four, I will get a different value: (3+4+7+1)/4 = 15/4 = 3.75 Why is it so? Why is this second average speed higher then the first one?
What does it reflect?","['calculus', 'statistics']"
3230193,Is there any matrix representation for the second order derivation of a determinant?,"$\newcommand{\piff}[2]{\frac{\partial{#1}}{\partial{#2}}}\newcommand{\ppiff}[3]{\frac{\partial^2 {#1}}{\partial{#2}\partial{#3}}}\newcommand{\pa}[2]{\ppiff{|A|}{#1}{#2}}$ Suppose matrix $$A = \begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1n}\\
x_{21} & x_{22} & \cdots & x_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{n2} & \cdots & x_{nn}
\end{pmatrix}$$ Consider $\det{A}$ , denoted $|A|$ , as a $n^2$ -variable function of variable $x_{ij}(1\le i,j \le n)$ . I've found that $$\piff{|A|}{A} = \begin{pmatrix}
\piff{|A|}{x_{11}} & \piff{|A|}{x_{12}} & \cdots & \piff{|A|}{x_{1n}}\\
\piff{|A|}{x_{21}} & \piff{|A|}{x_{22}} & \cdots & \piff{|A|}{x_{2n}}\\
\vdots & \vdots & \ddots & \vdots\\
\piff{|A|}{x_{n1}} & \piff{|A|}{x_{n2}} & \cdots & \piff{|A|}{x_{nn}}
\end{pmatrix} = C$$ where $C$ is the cofactor matrix . Is there any matrix representation for follwing $n^2$ by $n^2$ matrix? $$
\begin{pmatrix}
\pa{x_{11}}{x_{11}} & \pa{x_{11}}{x_{12}} & \cdots\cdots & \pa{x_{11}}{x_{nn}} \\
\pa{x_{12}}{x_{11}} & \pa{x_{12}}{x_{12}} & \cdots\cdots & \pa{x_{12}}{x_{nn}} \\
\vdots & \vdots & \ddots & \vdots \\
\pa{x_{nn}}{x_{11}} & \pa{x_{nn}}{x_{12}} & \cdots\cdots & \pa{x_{nn}}{x_{nn}}
\end{pmatrix}
$$","['matrices', 'multivariable-calculus', 'matrix-calculus', 'linear-algebra']"
3230195,If $X\sim \mathrm{lognormal}$ then $Y:=(X-d|x\geq d)$ has approximately a Generalized Pareto distribution.,"Let $X$ be a random variable with lognormal distribution. Show that when sufficiently large then $Y:=(X-d|x\geq d)$ is approximately a random variable with generalized Pareto distribution. Hint: Use the fact that $\mathrm{erf}(x)\approx 1-\frac{1}{\sqrt{x}}e^{-\frac{x^2}{2}}$ for large values of $x$ . My attempt: We recall that the density function for the lognormal distribution is given by $$
f(x)=\frac{1}{x\sigma\sqrt{2\pi}}e^{\frac{-(\log x-\mu)^2}{\sigma}}\:\:\mbox{ for }x>0.
$$ The comulative distribution function for a generalized Pareto random variable is given by $$
G(x)=1-\left(1+\frac{\gamma x}{\theta}\right)^{\frac{-1}{\gamma}}.
$$ The objective is to find parameters $\gamma$ and $\theta$ such that $\mathbb{P}(Y\leq y)\approx G(y)$ , it is clear that $\gamma$ and $\theta$ will be expressed in terms of $\sigma$ , $\mu$ and $d$ . My attempt is: \begin{align}
\mathbb{P}(Y\leq y) & =1- \frac{1-\int_{0}^{d+y}\frac{1}{x\sigma\sqrt{2\pi}}e^{\frac{-(\log x-\mu)^2}{\sigma}} dx}{1-\int_{0}^{d}\frac{1}{x\sigma\sqrt{2\pi}}e^{\frac{-(\log x-\mu)^2}{\sigma}} dx} 
 \end{align} We consider the changge of variable given by $t=\frac{\log x -\mu}{\sqrt{2}\sigma}$ , then $dt=\frac{1}{\sqrt{2}\sigma x}dx$ , so, $dx=\sqrt{2}\sigma x dt$ . Therefores, we have \begin{align}
\mathbb{P}(Y\leq y) & =1- \frac{1-\frac{1}{\sqrt{\pi}}\int_{-\infty}^{\frac{\log(d+y) -\mu}{\sqrt{2}\sigma}}e^{-t^{2}} dt }{1-\frac{1}{\sqrt{\pi}}\int_{-\infty}^{\frac{\log(d) -\mu}{\sqrt{2}\sigma}}e^{-t^{2}} dt} \\
&= 1- \frac{1-\frac{1}{\sqrt{\pi}}\int_{-\infty}^{0}e^{-t^{2}} dy- \frac{1}{\sqrt{\pi}}\int_{0}^{\frac{\log(d+y) -\mu}{\sqrt{2}\sigma}}e^{-t^{2}} dt }{1-\frac{1}{\sqrt{\pi}}\int_{-\infty}^{0}e^{-t^{2}} dy-\frac{1}{\sqrt{\pi}}\int_{0}^{\frac{\log(d) -\mu}{\sqrt{2}\sigma}}e^{-t^{2}} dt} \\
&= 1- \frac{1-\frac{1}{2}- \frac{1}{\sqrt{\pi}}\int_{0}^{\frac{\log(d+y) -\mu}{\sqrt{2}\sigma}}e^{-t^{2}} dt }{1-\frac{1}{2}-\frac{1}{\sqrt{\pi}}\int_{0}^{\frac{\log(d) -\mu}{\sqrt{2}\sigma}}e^{-t^{2}} dt} \\
&= 1- \frac{\frac{1}{2}- \frac{1}{2}\mathrm{erf}\left(\frac{\log(d+y) -\mu}{\sqrt{2}\sigma}\right) }{\frac{1}{2}- \frac{1}{2}\mathrm{erf}\left(\frac{\log(d) -\mu}{\sqrt{2}\sigma}\right) } \\
&= 1- \frac{1- \mathrm{erf}\left(\frac{\log(d+y) -\mu}{\sqrt{2}\sigma}\right) }{1- \mathrm{erf}\left(\frac{\log(d) -\mu}{\sqrt{2}\sigma}\right) } \\
&\approx 1- \frac{\frac{1}{\sqrt{\frac{\log(d+y) -\mu}{\sqrt{2}\sigma}}}e^{-\frac{(\log(d+y)-\mu)^2}{4\sigma^2}} }{\frac{1}{\sqrt{\frac{\log(d) -\mu}{\sqrt{2}\sigma}}}e^{-\frac{(\log(d)-\mu)^2}{4\sigma^2}}  } \leftarrow \mbox{by hint.}\\
&= 1- \sqrt{\frac{\log(d)-\mu}{\log(d+y)-\mu}}e^{-\frac{(\log(d+y)-\mu)^2}{4\sigma^2}+\frac{(\log(d)-\mu)^2}{4\sigma^2}}\\
&=1- \sqrt{\frac{\log(d)-\mu}{\log(d+y)-\mu}}e^{\frac{1}{4\sigma^2}(\log(d)-\log(d+y))(\log(y+d)+\log(d)-2\mu) }\\
&=1- \sqrt{\frac{\log(d)-\mu}{\log(d+y)-\mu}}e^{\frac{1}{4\sigma^2}\log\left(\frac{d+y}{d}\right)\left(2\mu-\log(dy+d^2)\right) }\\
 \end{align} I do not know how to continue, algebraically I have not been able calibrate the parameters to get what I need. I ask for your help with this problem, any solution or suggestion will be well received.","['conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
3230348,"Conceptual doubt on ""Limits""","Suppose I have a function say $f(x)= x^2$ . Now we know that graph is parabola, and it passes through the origin. Now I write $x^2$ as $e^{2 ln(x)}$ . I plug in the value $0$ . I know that $ln 0$ approaches $-\infty$ . So my answer should be $\frac{1}{e^{2\infty}} $ which is approaching zero . However in the situation above,  I am getting zero. It seems, I am having some problem in understanding the concept. Any help would be greatly appreciated.  Approaching zero and zero, I guess are not same.",['limits']
3230371,"Is there any $k$ , for which we can prove that $n^n+k$ is never prime?","Is there any positive integer $k$ , such that we can prove that $n^n+k$ is not prime for any positive integer $n$ ? $$n^n+1805$$ has a prime factor not exceeding $43$ up to $n=1805$ . However, for the multiples of $1806$ this is in general no more the case. I think that for no $k$ , $n^n+k$ must have a ""small"" factor for all $n$ . Is this true, and if yes, does this destroy any hope for a proof ?","['number-theory', 'perfect-powers', 'elementary-number-theory', 'prime-numbers']"
3230376,Interpolate between 4 points on a 2D plane,"I'm trying to 'morph' between 4 values, which I have mapped on 4 corners of a square plane in my user interface (A, B, C, D, see image below). By selecting a point (P) within the boundaries of this plane I want to determine the interpolated value of P. I've tried naively interpolating the X and Y coordinates and using pythagoras to calculate the distances from each of the corners, but none of my calculations end up correct. What would be the mathematically correct method for determining the value of P, based on the values of A, B, C and D? 4 values on the corners of a 2D plane","['interpolation', 'geometry']"
3230391,Show that a triangle is equilateral,"A circle crosses the sides of a triangle, dividing each of them into
  three equal parts. Prove that the triangle is equilateral. I think that the best way is to show that $\angle BAC = \angle ABC$ , and then $\angle ABC = \angle ACB$ . Something like that and I would be very grateful if you could help me because I don't see how this can be done.","['euclidean-geometry', 'triangles', 'circles', 'geometry']"
3230394,differentiation of fractional part of $x$,What is the differentiation of fractional part of $x$ ? Since the slope of $\{x\}$ is $1$ so that derivative of $\{x\}$ should be $1$ . Is it correct or not,"['calculus', 'derivatives']"
3230408,If $f_n \to f$ in $L^p$ and $g_n \xrightarrow{a.e.} g$ in $L^\infty$ then $f_ng_n \to fg$ in $L^p$,"Exercise : Let $\Omega \subseteq \mathbb R^n$ be open and bounded, $\{f_n\}_{n \geq 1} \subseteq L^p(\Omega)$ with $1<p< \infty$ and $\{g_n\}_{n \geq 1} \subseteq L^\infty(\Omega)$ . If it is $f_n \to f$ in $L^p(\Omega)$ and $g_n \xrightarrow{a.e.} g$ in $L^\infty(\Omega)$ while $\{g_n\}_{n \geq 1}$ is also bounded then show $f_ng_n \to fg$ in $L^p(\Omega)$ . Attempt : Since $\{g_n\}_{n \geq 1}$ is bounded, then it would be $\|g_n\|_\infty \leq M$ for some $M>0$ and thus for the $p$ -norm it would be $\|g_n\|_p \leq M$ as well. Now, it is : \begin{align*}
\|fg - f_ng_n\|_p &= \|fg - fg_n + fg_n - f_ng_n\|_p \\ &\leq \|fg-fg_n\|_p + \|g_n(f-f_n)\|_p \\ &\leq \|fg-fg_n\|_p + M\|f-f_n\|_p 
\end{align*} The second term goes to $0$ as $f_n \to f$ in $L^p(\Omega)$ . Now, since $g_n \xrightarrow{a.e.} g$ we can also deduce that $\|g_n\|_\infty \xrightarrow{a.e.} \|g\|_\infty$ . For the first term : \begin{align*}
\|fg-fg_n\|_p &= \left(\int_\Omega|fg-fg_n|^p\mathrm{d}x \right)^{1/p} \\ &\leq \left[ \int_\Omega \left(|fg| + |fg_n|\right)^p\mathrm{d}x\right]^{1/p} \\ &\leq \left[ \int_\Omega \left(|fg| + M|f|\right)^p\mathrm{d}x\right]^{1/p}
\end{align*} I can't see how to continue on though to prove that this term can become arbitrarily small, thus that $\|fg-f_ng_n\|_p \to 0$ and thus the desired convergence. Any hints or elaborations will be greatly appreciated ! Edit : I worked it around myself as such : \begin{align*}
\|f(g_n-g)\|_p &= \left(\int_\Omega |(g_n-g)f|^p\mathrm{d}x\right)^{1/p} \\ &\leq \left(\int_\Omega ||g_n-g\|_\infty^p|f|^p\mathrm{d}x\right)^p \\ &= \|g_n-g\|_\infty\left(\int_\Omega |f|^p\mathrm{d}x \right)^p \to 0
\end{align*} So finally we get $\|fg-f_ng_n\|_p \to 0 \Leftrightarrow f_ng_n \to fg$ ÏƒÏ„Î¿Î½ $L^p(\Omega)$ .","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'functional-analysis', 'convergence-divergence']"
3230444,Difference between 'Only' and 'Every' Keyword in Mathematical logic,"Represent these two statement in first order logic: A)
 Only Alligators eat humans B)
 Every Alligator eats humans Is Every represents â‰¡âˆƒ and Only represents â‰¡âˆ€
 ?? Can we differentiate it with verb â€˜eatâ€™ and â€˜eatsâ€™??","['first-order-logic', 'discrete-mathematics']"
3230472,Find the value of $\sin({-11\pi}/{3})$?,"In the image the rotation starts from point 1 and rotates clockwise until 7 comes at point 3 Now in this question I know how to solve it by using $\sin(2n\pi-\theta)$ but if I try to solve it in this way $$\sin(\frac{-11\pi}{3})=
\sin(\frac{-11\pi}{3}\cdot\frac{180}{\pi})=
-\sin(660^\circ)=
-\sin(90^\circ\cdot7+30^\circ)=
-\cos(30^\circ)= 
-\frac{\sqrt3}{2}$$ Now as there is negative outside the bracket we should move clockwise in the Cartesian plane, right? If I start from the $+x$ axis then after moving seven times I would arrive at $-x$ axis. As it is $+30^\circ$ the angle should lie in II quadrant and as it is sin the value should be +ve. So the answer should be $-\frac{\sqrt3}{2}$ but the answer is $+\frac{\sqrt3}{2}$ . So my doubt is which axis should I start from in the Cartesian plane if the movement is clockwise and also if the movement is anti clockwise or counter clockwise? Which axis should my point 1 be in?Sorry I went through the question again and I saw that I had made a mistake in one of the steps.Thank you all for your time and effort.",['trigonometry']
3230496,How to solve $\sqrt{x+2}\geq x$?,"How do you solve the inequality $$\sqrt{x+2}\geq{x}?$$ Now since ${x+2}$ is under the radical sign, it must be greater than or equal to ${0}$ to be defined. So, ${x+2}\geq{0}$ Thus ${x}\geq{-2}$ Now keeping this in mind, we can solve the inequality by squaring both the sides: ${x+2}\geq{x^2}$ So ${x^2-x-2}\leq{0}$ Solving, ${(x-2)(x+1)}\leq{0}$ Therefore ${x}$ belongs to the interval ${[-1,2]}$ . As ${x}\geq{-2}$ , the function is also defined. Why does the answer say that ${x}$ belongs to ${[-2,2]}$ , then? Please feel free to point out the mistakes.","['algebra-precalculus', 'inequality']"
3230540,"If $x = \frac{\sqrt{111}-1}{2}$, calculate $(2x^{5} + 2x^{4} - 53x^{3} - 57x + 54)^{2004}$.","I already have two solutions for this problem, it is for high school students with an advanced level. I would like to know if there are better or more creative approaches on the problem. Here are my solutions: (1st solution): Notice that $$x^{2} = (\frac{\sqrt{111}-1}{2})^{2} =  28 - \frac{\sqrt{111}}{2}$$ $$x^{3} = x \cdot x^{2} = \left( \frac{\sqrt{111}}{2} - \frac{1}{2} \right) \left( 28 - \frac{\sqrt{111}}{2} \right) =  14 \sqrt{111} - 111/4 - 14 + \frac{\sqrt{111}}{4} = \frac{57 \sqrt{111}}{4} - \frac{167}{4} $$ $$x^{4} =  (x^{2})^{2} =   \frac{ 111 + 4 \cdot 784}{4} - 28 \sqrt{111}  $$ $$x^{5} = x^{4} \cdot x = \left( \frac{ 111 + 4 \cdot 784}{4} - 28 \sqrt{111} \right)\left(\frac{\sqrt{111}-1}{2} \right) =  \left( \frac{ 3247 }{4} - 28 \sqrt{111} \right) \left(\frac{\sqrt{111}-1}{2} \right) $$ $$ = \sqrt{111}\frac{3359}{8}  - \frac{15679}{8}  $$ So we have $$ 2x^{5} + 2x^{4} - 53x^{3} - 57x + 54 = $$ $$(\sqrt{111}\frac{3359}{4}  - \frac{15679}{4}) + (\frac{ 111 + 4 \cdot 784}{2} - 56 \sqrt{111}) - 53 (\frac{57 \sqrt{111}}{4} - \frac{167}{4}) - 57 (\frac{\sqrt{111}-1}{2})  + 54 $$ $$ = \sqrt{111} (\frac{3359}{4}  - \frac{3359}{4})  - \frac{15679}{4} + \frac{ 111 + 4 \cdot 784}{2} + \frac{53 \cdot 167}{4} + \frac{57}{2} + 54 $$ $$ = - \frac{15679}{4} + \frac{ 222 + 8 \cdot 784}{4} + \frac{53 \cdot 167}{4} + \frac{114}{4} + \frac{216}{4}  $$ $$ = - \frac{15679}{4} + \frac{ 5600 + 894 }{4} + \frac{5300 + 3180 + 371 }{4} + \frac{114}{4} + \frac{216}{4}  $$ $$ = - \frac{15679}{4} + \frac{ 6494 }{4} + \frac{8851}{4} + \frac{114}{4} + \frac{216}{4} $$ $$ = -4/4 = -1, $$ and the answer is $ (-1)^{2004} = 1.$ The above solution requires quite tedious calculation. Below is an alternative solution. (2nd solution): Notice that $x = \frac{\sqrt{111}-1}{2}$ is equivalent with $$ (2x + 1)^{2} = 111$$ $$ 4x^{2} + 4x + 1 = 111 $$ $$ 4x^{2} + 4x - 110 = 0$$ $$ (2x^{2} + 2x - 55) = 0 \:\: ........ \:\: (1)$$ Multiply $(1)$ with $x^{3}$ we get $$ (2x^{5} + 2x^{4} - 55x^{3}) = 0 $$ Multiply $(1)$ with $x$ we get $$ (2x^{3} + 2x^{2} - 55x) = 0 $$ Sum both of them and we get: $$ 2x^{5} + 2x^{4} - 53 x^{3} + 2x^{2} - 55x = 0  \:\: ........ \:\: (2)$$ and now we have the 1st 3 terms of the form that we want to calculate. Substract $(2)$ with $(1)$ to get: $$ 2x^{5} + 2x^{4} - 53 x^{3} - 57x + 55 = 0 $$ $$ 2x^{5} + 2x^{4} - 53 x^{3} - 57x + 54 = -1$$ So the answer is $(-1)^{2004} = 1.$","['contest-math', 'algebra-precalculus', 'radicals', 'polynomials']"
3230545,"Find limit of modified geometric sum $\sum_{k=1}^{\infty} k^3 q^k,$ where $\left| q\right| < 1$","I've been trying to find the limit of the infinite series $\displaystyle\sum_{k=1}^{\infty} k^3 q^k, \quad \left| q\right| < 1$ I already determined that it converges using the ratio test, but I am at a loss when it comes to actually determine the limit. I know how to calculate the limit of $\displaystyle\sum_{k=1}^{\infty} k q^k, \quad \left| q\right| < 1$ This is simple by derivation with respect to q applied to $1 + q + q^2 + \ldots q^n = \frac{1-q^{n+1}}{1-q}$ and then multiplying with $q$ and taking the limit. But this gets way too complicated when doing it three times. Is there a better way?","['limits', 'convergence-divergence', 'sequences-and-series']"
3230604,Show that $\int_{-\infty}^{\infty}|f(x)|^2dx = \frac{1}{2\pi}\int_{-\infty}^{\infty}|\hat{f}(\mu)|^2d\mu$,"Given: Let $a_1 \lt b_1 \le a_2 \lt b_2 \le ... \le a_{n-1} \lt b_{n-1} \le a_n \lt b_n$ and let $$f(x) = \sum_{j=1}^nc_jf_{a_jb_j}(x).$$ Show that , $$(*)\int_{-\infty}^{\infty}|f(x)|^2dx = \frac{1}{2\pi}\int_{-\infty}^{\infty}|\hat{f}(\mu)|^2d\mu$$ My intuition is first using the fact that (I proved that): $$(1)\int_{-\infty}^{\infty}\frac{1-costx}{x^2}dx = |t|\pi \quad,\forall t\in \mathbb R$$ In order to show that: $$(2)\int_{-\infty}^{\infty}\hat{f}_{a_jb_j}(\mu)\overline{\hat{f}_{a_jb_j}(\mu)}d\mu = 0 \quad for\;i \neq j$$ and then use $(2)$ to show what is required. I'm still not sure how to get from $(1)$ to $(2)$ and from $(2)$ to show $(*)$ My intuition for showing $(2)$ is using: $$g(\mu) = \hat{f}_{cd}(\mu)\overline{\hat{f}_{ab}(\mu)} = \{\frac{e^{i\mu d}-e^{i\mu c}}{i\mu}\}\{\frac{\overline{e^{i\mu b}-e^{i\mu a}}}{i\mu}\} = \frac{e^{i\mu (d-b)}-e^{i\mu (c-b)}-e^{i\mu (d-a)} + e^{i\mu(c-a)}}{\mu^2}$$ And somehow and exploit the fact that $g(\lambda)$ is holomorphic in $\mathbb C$ and the coefficients of $i\mu$ in the exponential terms in $g(\mu)$ are all nonnegative. Also, another intuition is showing (still don't know how - would highly appreciate if someone can solve this too) that: $$(*)(*)\lim\limits_{R \uparrow \infty}\frac{1}{2\pi}\int_{-R}^{R}e^{-i\mu x}\hat{f}_{ab}(\mu)d\mu = f_{ab}(x)$$ and from that to show $(*)$ . where $f_{ab}(x)$ and $\hat{f}_{ab}(\mu)$ are: Let the Fourier transform $\hat{f}(\mu)$ of a function $f(x)$ specified on $\mathbb R$ is defined (often) by the formula: $\hat{f}(\mu) = \int_{-\infty}^{\infty}e^{i\mu x}f(x)dx$ for $\mu \in \mathbb C$ whenever the integral makes sense. Let $f_{ab}(x) = 1$ for $a \le x \le b$ and $f_{ab}(x) = 0$ for $x \neq [a,b]$ . the points $x$ are $\in \mathbb R$ except for $a, b$ .","['integration', 'fourier-analysis', 'fourier-transform', 'linear-algebra', 'functional-analysis']"
3230651,Proving that weird topology is compact,"The following question appeared on my topology exam. I couldn't solve it then, and now I am trying it once again. Could someone check my proof to see if it is correct, or maybe suggest an easier way? Let $(X,\mathcal{T})$ a topological Hausdorff space. Let $p\not\in X$ and define $\widehat{X}=X\cup\{p\}$ with the topology $\widehat{\mathcal{T}}$ (you may assume that this indeed defines a topology): $$U\in\widehat{\mathcal{T}}\iff\begin{cases}p\in U \text{ and }X\setminus U\text{ is a compact subset of }X,\text{ or} \\ p\not\in U\text{ and }U\in\mathcal{T}. \end{cases}$$ Prove that $\widehat{X}$ is compact. I drew a picture, where the red space is $X$ and the green circles are the open sets in the covering. My approach: Let $\{A_\alpha \}_{\alpha\in I}$ an open covering of $\widehat{X}$ . Let $J\subset I$ such that $p\in A_\alpha\iff \alpha\in J$ . Take an $\alpha'\in J$ . We know that $X\setminus A_{\alpha'}$ is a compact subset of $X$ . For all $\beta\in J$ , $X\setminus A_\beta$ is compact. Since $X$ is Hausdorff, we now that $X\setminus A_\beta$ is closed, so $A_\beta\cap X$ ( $=A_\beta\setminus\{p\}$ ) is open in $X$ . Now $\{A_\beta\setminus\{p\} \}_{\beta\in J}\cup \{A_\gamma \}_{\gamma\in I\setminus J}$ is an open covering of $X$ using open sets in $X$ , so it also covers $X\setminus A_{\alpha'}$ . This implies that there exist $M\subset J$ and $N\subset I\setminus J$ finite such that $\{A_\beta\setminus\{p\} \}_{\beta\in M}\cup \{A_\gamma \}_{\gamma\in N}$ covers $X\setminus A_{\alpha'}$ , or that $$\{A_\alpha\}_{\alpha\in\{\alpha' \}\cup M\cup N}$$ is a finite subcovering of $\{A_\alpha \}_{\alpha\in I}$ of $\widehat{X}$ .","['general-topology', 'proof-verification']"
3230674,Limit of $\sum_{r=0}^{n}\frac{{n\choose r}}{n^r\cdot(r+3)}$ as $n\rightarrow\infty$ [duplicate],"This question already has an answer here : Infinite series with the binomial coefficient [duplicate] (1 answer) Closed 5 years ago . Evaluate $$\lim_{n\rightarrow\infty}\sum_{r=0}^{n}\dfrac{{n\choose r}}{n^r\cdot(r+3)}$$ This form forcing me to use integrals, I tried expanding $${n\choose r}=\dfrac{n(n-1)\cdots (n-r+1)}{r!}$$ then dirtributing that $n^r$ to each braket to generate $$\dfrac{1(1-\frac{1}{n})\cdots (1-\frac{r-1}{n})}{r!}$$ then I don't know what to do with $r!$ and $r+3$ . How to proceed further?","['riemann-sum', 'calculus', 'binomial-coefficients', 'limits', 'algebra-precalculus']"
3230694,Finding Upper Bound in a Set,"I'm trying to solve this question Let S be a non-empty set and consider the partially ordered set(P(S
  ), âŠ†). Show that every subset of P(S ) has a least upper bound. I'm not sure I agree with the premise of the question. If I take S = N, then I won't have an upper bound for the set of all 
 natural numbers (since it isn't closed). For example, if S = N, then I could consider the chain: { {1}, {1,2}, {1,2,3}, ...} This doesn't have an upper bound since the union of this chain isn't closed Am I thinking correctly?","['elementary-set-theory', 'upper-lower-bounds']"
3230730,"If $\sin(18^\circ)=\frac{a + \sqrt{b}}{c}$, then what is $a+b+c$? [duplicate]","This question already has answers here : How to prove $\cos \frac{2\pi }{5}=\frac{-1+\sqrt{5}}{4}$? (11 answers) Closed 5 years ago . If $\sin(18)=\frac{a + \sqrt{b}}{c}$ in the simplest form, then what is $a+b+c$ ? $$ $$ Attempt: $\sin(18)$ in a right triangle with sides $x$ (in front of corner with angle $18$ degrees), $y$ , and hypotenuse $z$ , is actually just $\frac{x}{z}$ , then $x = a + \sqrt{b}, z = c$ . We can find $y$ as $$ y = \sqrt{c^{2}- (a + \sqrt{b})^{2}} $$ so we have $$ \cos(18) = \frac{y}{z} = \frac{\sqrt{c^{2}- (a + \sqrt{b})^{2}}}{c}$$ I also found out that $$b = (c \sin(18) - a)^{2} = c^{2} \sin^{2}(18) - 2ac \sin(18) + a^{2}$$ I got no clue after this. The solution says that $$ \sin(18) = \frac{-1 + \sqrt{5}}{4} $$ I gotta intuition that we must find $A,B,C$ such that $$ A \sin(18)^{2} + B \sin(18) + C = 0 $$ then $\sin(18)$ is a root iof $Ax^{2} + Bx + C$ , and $a = -B, b = B^{2} - 4AC, c = 2A$ . Totally different. This question is not asking to prove that $sin(18)=(-1+\sqrt{5})/4$ , that is just part of the solution.","['contest-math', 'euclidean-geometry', 'algebra-precalculus', 'trigonometry']"
3230820,How is $\sum_{k=0}^{5} {}^5C_k\sin(kx)\cos(5-k)x=16\sin(5x)$?,The original question was if $\sum_{k=0}^{5} {}^5C_k\sin(kx)\cos(5-k)x=N\sin(5x)$ then what is the value of N? I plugged $\frac{\pi}{2}$ in place of x and N came out to be 16. But is there any algebraic way to prove this. I tried expanding $(\sin(x)+i\cos(x))^5$ but eventually got stuck.,"['trigonometry', 'binomial-coefficients', 'binomial-theorem', 'complex-numbers']"
3230865,Rao-Blackwell and Cramer-Rao LB comparison,"Let $X_1, X_2, \dots, X_n$ be a random sample following the Geometric distribution . $$
\prod\limits_{i=1}^{n} f(x_i|p) = (1-p)^{\sum\limits_{i=1}^n x_i-n}p^n
$$ Since the pmf of the Geometric distribution is exponential family, the factorization theorem yields that the statistic $$
T = \sum\limits_{i=1}^n x_i
$$ is sufficient and complete. Then, $$
E[T] = E\left[ \sum\limits_{i=1}^n x_i \right] = \sum\limits_{i=1}^n E[x_i] = \frac{n}{p}
$$ Therefore, according to Rao-Blackwell , the Minimum Variance Unbiased Estimator of $\frac{1}{p}$ is $$
W = \frac{1}{n} \sum\limits_{i=1}^n x_i
$$ Now, Cramer-Rao's Lower Bound : $$
LB = \frac{\left[\left(\frac{1}{p}\right)'\right]^2}{nI(p)} = \frac{1}{n}\frac{1-p}{p} 
$$ Question: Is $V[W] = LB$ in this specific example? If so, is there a reason why $W$ has the lowest possible variance that has something to do with the geometric distribution?","['statistics', 'probability-distributions', 'variance', 'parameter-estimation']"
3230893,Can $1/\log(2)$ be represented as a period?,"In this article by Zagier-Kontsevich, period is defined as values of integral of a rational function over a domain in $\mathbb{R}^{n}$ defined by polynomial inequalities with rational coefficients. For example, $\pi, \log(2), \zeta(3)$ are periods, and $e$ is conjecturally not a period. The set of periods form a ring (period ring), which is not a field. My question is: is $$
\frac{1}{\log(2)}
$$ also a period? More generally, how about $$
\frac{1}{\log\alpha}
$$ for $\alpha \in \overline{\mathbb{Q}}$ ? Note that $\log\alpha$ is a period for $\alpha\in \overline{\mathbb{Q}}$ . I tried to find integral representations of $1/\log(x)$ , but I can't find a suitable one. Also, if it is not, it would be super hard to prove that it is actually not a period.","['number-theory', 'definite-integrals']"
3230912,Why does an injection from a set to a countable set imply that set is countable?,"I'm reading a proof, and it concludes that a set $A$ is countable after finding an injection from $A$ to a countable set. Why is this true? I thought that we need to find a bijection from $A$ to a countable set to prove $A$ is countable. Shouldn't $A$ be at most countable?","['elementary-set-theory', 'functions']"
3230924,"fixpoint iteration to solve $y'(t)=y(t), y(0)=1$","Solve the initial value problem $y'(t)=y(t)$ , $y(0)=1$ on the interval $[0,1]$ with a fixpoint iteration of the operator $T: Y\to Y, (Ty)(t):=y_0+\int_0^t f(s,y(s))\, ds$ . Begin with $y_0(t)=0$ and give the function series $(y_k)$ . The operator $T$ is supposed to be taken from the proof of the theorem of Picard-LindelÃ¶f. But how do I do the fixpoint iteration here? 
What is $f(s,y(s))$ ? In the proof of Picard-LindelÃ¶f it is $y'(t)=f(t,y(t))$ . 
Since we want to solve $y'(t)=y(t)$ can we set $f(t,y(t))=y(t)$ ? So, I set that all together and start the iteration: We have $y(0)=1$ and $y_0(t)=0$ . $y_1(t)=y(0)+\int_0^t y_0(s)\, ds=1$ $y_2(t)=y(0)+\int_0^t y_1(s)\, ds=t+1$ $y_3(t)=y(0)+\int_0^t y_2(s)\, ds=\frac{1}{2}t^2+t+1$ $y_4(t)=y(0)+\int_0^t y_3(s)\, ds=\frac{1}{6}t^3+\frac12t^2+t+1$ And so on. We see, that this indeed gives the sum: $y_n(t)=\sum_{k=0}^n \frac{t^k}{k!}$ Which would give $e^t$ eventually. Is this done correctly?
How comes the interval $[0,1]$ into account here? Thanks in advance.","['fixed-point-theorems', 'numerical-methods', 'ordinary-differential-equations']"
3230941,Differential of a one-form eating a vector?,"Let $(M,g)$ be a Riemannian manifold with Levi-Civita connection $\nabla$ , and consider a one-form $\alpha\in\Omega^1(M;\mathbb{R})$ and a vector field $X\in\Gamma(TM)$ . Since $\alpha(X)\in C^\infty(M;\mathbb{R})$ we can consider it's differential $d(\alpha(X))$ . Is there a coordinate invariant way to computer this quantity? Maybe something like $d(\alpha(X))(Y) = d\alpha(X,Y) + \alpha(\nabla_{Y}X)$ ? How does this generalize if we consider the differential of the smooth function $\beta(X,Y)$ where $\beta$ is now a 2-form and $X,Y$ are both vector fields?","['manifolds', 'differential-forms', 'differential-geometry']"
3230968,Centralizer of one element on a compact connected Lie group,"Exercise 16.2 from Daniel Bump - Lie Groups . Let $G$ be a compact connected Lie group and let $g\in G$ . Show that
the centralizer $C_G(g)$ of $g$ is connected. I have some problems verifying this, I have tried to use that in that case the exponential map is surjective, and think of the maximal torus, but I have not achieved it, I would appreciate some answer.","['representation-theory', 'lie-groups', 'differential-geometry']"
3230974,Categorical general topology -- reference request,"I am looking for literature describing general topology in terms of category theory. I would prefer literature which does not assume too much familiarity with category theory, but would appreciate any coherent refrences.","['general-topology', 'category-theory', 'reference-request']"
3230986,$n$-derivative of $m$-power of function,"There is well-known Leibniz rule generalization  for the $n$ -th derivative of product of $m$ functions $f_1, f_2, \ldots, f_m$ , namely: $$
D^n(f_1 f_2 \cdots f_m)=\sum_{k_1+k_2+\cdots+k_m=n} \binom{n}{k_1 \,k_2 \, \cdots k_m} D^{k_1}(f_1)D^{k_2}(f_2)\cdots D^{k_m}(f_m).
$$ Is there any simplification of the formula for the case $f_1=f_2=\cdots=f_m=f$ ? I hope there is a formula without the multinomial coefficients.","['derivatives', 'combinatorics', 'algebraic-combinatorics']"
3231012,Finding an inverse function (sum of non-integer powers),"I have a function: $$f(x)=x^{2.2} + (1-x)^{2.2}$$ It is defined on the interval $[0,1]$ . Minimum: $x=0.5, y=2*0.5^{2.2} = 2^{-1.2}$ . I want to find an inverse for it. Since the function has two ""wings"", inverse will be a family of two functions. After some tinkering, I crafted something that looks like a very good approximation of an inverse function: $$ g(x)=\frac{1}{2} \left( 1 \pm \left(\frac{x-2^{-1.2}}{1-2^{-1.2}}\right)^{0.504288} \right) $$ The number $0.504288 \approx 1 / 1.9829939 $ was found experimentally by substituting $g(x)$ into $f(x)$ and tweaking it to make it look as straight as possible: $$ p(x) = f(g(x)) \approx x $$ Illustration: https://www.geogebra.org/graphing/zgzafsk4 (Might be a bit slow. Image substitute just in case.) And now it bothers me if I'm just one step away from the exact solution. So the question is: is it possible to express the exact power in $g(x)$ to get the equality $p(x) = x$ and what that value will be? Update: OK, people seem to focus on using usual numeric tools to get an arbitrarily close approximation. But this is not what the question was about. I have an approximation that is good enough for my purposes. The question is about this particular special case. There is a power function added to reversed and shifted copy of itself. Inverse function for a power function $y = x^{2.2}$ will be just the power reversed $x = y^{1/2.2}$ . Since we adding an increasing and a decreasing function, the resulting curvature has changed. And it raises the suspicion that there might even be an exact power value, smaller than the original 2.2... After writing this, I realized that the problem can be expressed in a different way. What I actually did is that I made an inverse function for an approximation of $f(x)$ : $$f_{approx}(x) = 2^{-1.2}+ (1-2^{-1.2}) (2x-1)^{1.983}$$ Now I made a different illustration: https://www.geogebra.org/graphing/msfzaqah ( image ). There is also $h(x) = \frac{f_{approx}(x)}{f(x)}$ on the illustration. It clearly has some extremes, and changing the power just pushes them around. So the answer to the original question must be: this approximation doesn't fit the function exactly, so there is no exact number to put in there. Now the question is: can the original function be expressed as something invertible? Same shape functions with an integer power are invertible. What stands in the way for a function with non-integer (fractional) power to be invertible too? Note: For powers 2 and 3, similar functions can be expressed in a clearly invertible form: $$x^2+(1-x)^2 = \frac{1}{2} + \frac{1}{2}(2x - 1)^2$$ $$x^3+(1-x)^3 = \frac{1}{4} + \frac{3}{4}(2x - 1)^2$$ For the power of 4 and above WolframAlpha doesn't provide a form like this (single power), but still able to construct inverse functions, albeit more and more complicated. Interesting that for powers of 2 and 3 the resulting function has the power of 2. And this fact seems to persist for higher powers - a sum of (2n+1) power functions will be a (2n) power function. But that's a digression. Update 2: I really appreciate the answers about Tailor series expansion. But I'm still concerned: is it the best we can do?","['exponentiation', 'functions', 'inverse', 'inverse-function']"
3231030,Check whether $f \mapsto f+ \frac{df}{dx}$ is injective or surjective,Consider maps $C^{\infty} \to C^{\infty}$ s.t $f \mapsto f+ \frac{df}{dx}$ . We have to check whether this map is injective or surjective. My try: The map is clearly not injective as $x$ and $x+e^{-x}$ maps to $x+1$ . Now to check whether the map is surjective. Consider $g \in C^{\infty}$ . Then I was thinking in this way that considering $\int_0^xg$ then $f=g-\int_0^xg$ now $f+\frac{df}{dx}=g-\int_0^xg+\frac{dg}{dx}-g=-\int_0^xg+\frac{dg}{dx}$ still I am not getting a proof whether it is surjective or not.,"['integration', 'analysis', 'real-analysis', 'calculus', 'derivatives']"
3231060,What would be the number of inequivalent $6$-colourings of the faces of a cube?,"Consider the different ways to colour a cube with $6$ given colours such that each face will be given a single colour and all the six colours will be used. Define two such colourings to be equivalent if one will get from another just by rotation. Then what would be the number of inequivalent colourings? The answer is given $30$ . But I was trying to prove it by choosing the colour in the two opposite sides. For example, first let us choose the colours on the two opposite sides in $^6C_2$ ways, then another two opposite sides in $^4C_2$ ways, and the leftover sides in $^2C_2$ ways. So, in total, we get $15 \times 6=90$ , but I think I am triple-counting the situation or thinking it in a wrong way. Please help.","['permutations', 'combinatorics', 'discrete-mathematics']"
3231073,Kronecker Product Interpretation,The algebraic expression for a Kronecker product is simple enough.  Is there some way to understand what this product is? The expression for matrix-vector multiplication is easy enough to understand.  But realizing that the multiplication yields a linear combination of the columns of the matrix is a useful insight. Is there some analogous insight for the Kronecker product?,"['linear-algebra', 'tensor-products', 'kronecker-product']"
3231122,Find $n$ integers from $3n$ ones,"$n$ is a positive integer. Is the following statement true? For any $3n$ integers, saying $\{b_1,..,b_{3n}\}$ . There exists $n$ of them, saying $\{a_1,..,a_n\}$ ,so that $\forall$ $1\leq i,j,k\leq n$ we have $a_i+a_j\neq a_k$ . Here $i,j,k$ is not necessary to be different. I tried to mod them by $3$ and use Principle of tolerance, but it does not help when most of them divided by $3$ . I also get some easy inequality by considering some possible trivial condition, for example if $b_l$ are ranked from small to big, if $2b_m\geq b_{m+n}$ for some $m$ . I can take $\{b_m,..,b_{m+n-1}\}$ as desired. Any help will be appreciated.","['integers', 'combinatorics']"
3231173,"Let $f:[a,\infty)\rightarrow \mathbb{R}$ be a uniformly continuous function. $\int_{a}^{\infty} f$ converges.Prove that $\lim_{x\to\infty} f(x)=0$","Let $f:[a,\infty)\rightarrow \mathbb{R}$ be a uniformly continuous function in that range. $\int_{a}^{\infty} f$ converges. Prove that $\lim_{x\to\infty} f(x)=0$ Hint: Use the sequence $F_n(x)=n\int_{x}^{x+\frac{1}{n}} f$ . Honestly I have been trying to solve this one for some time but the hint really confuses me. I have tried to mess around with $F_n(x)$ a bit, for example by using the fundamental theorem but it still seems like such a random choice and I can't make anything out of it. Any guidance/explanations will be appreciated. Please use the hint in the question.","['improper-integrals', 'definite-integrals', 'sequences-and-series']"
3231252,"What is the difference between, a ""square"" and a ""perfect-square"", number?","Is, ""36"", a perfect square? I know that, ""4"" is a perfect square. Similarly, ""1"",""9"",""25"", are ""perfect-square""s.","['number-theory', 'square-numbers', 'elementary-number-theory', 'terminology']"
3231253,"Differentiation applied in Physics, need to clear a little doubt.","So, again my over-curiosity arose a problem for me, this time in Physics class. Being able to solve the questions sir gave me before the time limit, I was told to solve another, a little tougher. This is the exact words of my Physics Teacher : ""A particle of unit mass undergoes $1$ -dimensional motion such that its velocity expressed as a function of its position is: $$V(x) = bx^{-2n}$$ where, $b$ and $n$ are constants, and $x$ denotes the position of the particle.
Prove that, Acceleration $a$ varies with $x$ ."" Now, I spent a lot of time and came up with this : $$a = \frac{dv}{dt} = \left(\frac{dv}{dx}\right)\left(\frac{dx}{dt}\right)$$ By definition, $\frac{dx}{dt} = V(t)$ . But Velocity is expressed as a function of position, not time. Despite that, I went ahead and put $V(x)$ . That gave me $$a = \frac{dv}{dt} = \left(\frac{dv}{dx}\right)\left(\frac{dx}{dt}\right)$$ $$= \frac{d}{dx}(bx^{-2n}) * V(x)$$ $$= -2nbx^{-2n - 1} * bx^{-2n}$$ $$= -2nb^2x^{-4n - 1}$$ My teacher said I was correct. But what about $V(t)$ ? The function I put into calculation is $V(x)$ , not the $V(t)$ we're supposed to put normally. Is $V(x) = V(t)$ ? If it is, How can we prove it? How can we proceed with this?","['calculus', 'derivatives']"
