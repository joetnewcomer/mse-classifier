question_id,title,body,tags
4121453,Do we have some function $f(n)$that converges to the series $\ln n!$ ?,"I have tried $f(n) = \int_1^n \ln x dx$, but it fails. The difference is 
$$\lim_{n\to \infty}(\ln n! - \int_1^n \ln x dx)$$ $\int_1^n \ln x dx = n\ln n -n +1 \\
\ln n! - \int_1^n \ln x dx = \ln n! - n\ln n +n -1 = \ln(\frac{n!e^n}{n^n})-1$ where $\frac{n!e^n}{n^n}\to \infty$, so the limit diverges. Do we have some function $f(n)$that converges to the series $\ln n!$ ?",['sequences-and-series']
4121461,hard conditional probability question,"It is known that a woman has $7$ children and $2$ of them are girls , the rest is unknown. a-) What is the probability that the intermediate child is boy (intemediate boy means it is in amid when they ordered as to their ages) b-)What is the probability that woman has  three boys I know that it is conditional probability question but i stuck in it.I do not even how to calculate ""a woman has $7$ children and $2$ of them are girls , the rest is unknown."" Can you help me to handle this question.","['discrete-mathematics', 'conditional-probability', 'combinatorics', 'probability']"
4121521,Do strict henselizations satisfy Going-Up?,"Let $(A,\mathfrak{m})$ be a local ring. (I'd be ok assuming it's noetherian, if that's helpful, although I'd also be surprised if the answer really depended on this.) Let $A^{sh}$ be a strict henselization of $A$ . My question is this: Does the structure map $A\rightarrow A^{sh}$ satisfy Going-Up ? Intuitively, I think it should, for reasons I'll mention in a moment; on the other hand, in standard lists of basic properties of the strict henselization such as those at Stacks Project and EGA IV 18.8.12 and 18.8.13, I don't recall seeing this mentioned. The (quite hand-wavy) reasons I believe that $A\rightarrow A^{sh}$ ""ought"" to satisfy Going-Up are as follows: Fixing a map $\phi: A\rightarrow k^{sep}$ from $A$ to a separable closure $k^{sep}$ of its residue field, $A\rightarrow A^{sh}$ is the limit over diagrams of the form $A \xrightarrow{f} A' \xrightarrow{\psi} k^{sep}$ , where $f$ is étale and $\phi = \psi\circ f$ . (This is the definition of $A^{sh}$ given in Milne's book on étale cohomology, see p. 38.) In general, étale extensions don't satisfy Going-Up. However, this is only because they allow a ""certain amount of localization"": by this I mean that any étale extension is, up to isomorphism, standard étale, i.e., of the form $A[x]_h/(g)$ , where $g,h \in A[x]$ and $g$ is monic (there's also a condition how $g,h$ relate); it is a localization of the integral extension $A \rightarrow A[x]/(g)$ , which does satisfy Going-Up. Going-Up can thus only fail if the ""localization step"" of inverting $h$ destroys the primes of $A[x]/(g)$ that lie over a target prime of $A$ in the right way. Furthermore, the étale extensions $A'$ used in constructing $A^{sh}$ are not arbitrary but precisely those having a maximal ideal that pulls back to $\mathfrak{m}$ in $A$ ; this is what makes it possible for them to fit into the diagram $A \xrightarrow{f} A' \xrightarrow{\psi} k^{sep}$ . Thus $\mathfrak{m}$ , and therefore any prime of $A$ (as they are all contained in $\mathfrak{m}$ ), ""survives the localization step"" in the construction of all the étale extensions used to build $A^{sh}$ . Now it seems to me that, given a prime $\mathfrak{p}$ of $A$ , some of the primes lying over it in the intermediate étale extensions $A'=A[x]_h/(g)$ will be contained in other maximals of $A[x]/(g)$ than the one that ends up being the kernel of $\psi:A'\rightarrow k^{sep}$ , which could therefore get destroyed by the localization at $h$ ; so the intermediate $A'$ 's probably won't satisfy Going-Up. However, any such primes will be destroyed by a later localization, since $A^{sh}$ is local. Thus it seems to me that in each intermediate $A'$ , Going-Up is going to hold for the primes inside $\ker \psi$ (I mean if $\mathfrak{q}\subset \ker \psi$ pulls back to $\mathfrak{p}$ in $A$ , and $\mathfrak{p}_1\subset A$ contains $\mathfrak{p}$ , then there exists $\mathfrak{q}_1\subset\ker \psi$ pulling back to $\mathfrak{p}_1$ ); and then in $A^{sh}$ , a similar situation will exist with respect to the kernel of $A^{sh}\rightarrow k^{sep}$ , except that these will be all the primes. This train of thought is, of course, rather soft (or I wouldn't be asking the question). It is a standard property of $A\rightarrow A^{sh}$ that it is faithfully flat and of relative dimension zero. Thus Going-Down , Lying-Over , and Incomparability obtain for $A\rightarrow A^{sh}$ . It is not the case that every faithfully flat map of relative dimension zero satisfies Going-Up (see here ); still, it renders it at least plausible in this case? This is insanely vague, but: while I haven't seen Going-Up listed as a standard property of $A\rightarrow A^{sh}$ , other standard properties do suggest that it ""behaves well"" with respect to chains of primes. E.g., $A^{sh}$ is universally catenary if and only if $A$ is universally catenary (EGA IV 18.8.17). This is perhaps even more insanely vague. I understand $\operatorname{Spec}A^{sh}$ geometrically as a kind of ""universal cover of a very small neighborhood of [the closed point of] $\operatorname{Spec} A$ "". Ascending chains of primes in $A$ are, geometrically, descending flags of irreducible subschemes each of which hits the closed point of $\operatorname{Spec}A$ . Thinking by analogy with covering space theory in, e.g., the category of smooth manifolds, any such flag ought to be able to be seen in a very small neighborhood, and ought to be able to be lifted to a universal cover. If I start with a lift of some irreducible subscheme of $\operatorname{Spec}A$ to $\operatorname{Spec}A^{sh}$ , I ought (again purely by analogy with topological covering space theory) be able to lift any irreducible sub-subscheme downstairs to a sub-subscheme upstairs as well. This would be Going-Up if the analogy actually holds. I'm looking forward to your thoughts. Thanks in advance.","['algebraic-geometry', 'commutative-algebra']"
4121546,Intuition on how $L^2$ difer from $L^1$ Lebesgue function spaces,"I have read the Wikipedia page on the topic, and the answers on this post focusing solely on $L^1$ . The integral definitions are very clear, and the fact that $L^2$ has a natural norm induced by the scalar product, which is important in Fourier tranforms, and other bits of knowledge around these definitions, still leave me wanting for a couple of plots of functions, one for each space, as well as a short definition in English, even if only approximate. To some degree, and leaving aside measure theory, which would likely have pre-empted this post to begin with, the absolute value of the functions may seem to serve a similar purpose to squaring. Yet, these are completely separate spaces. If it wasn't clear enough from the tone of my answer, or my self-deprecating profile, I have no formal training in mathematics, so I'm looking for answers along the vein of the notable Terry Tao's explanation here: I’ll start today with my article on “Function spaces“. Just as the
analysis of numerical quantities relies heavily on the concept of
magnitude or absolute value to measure the size of such quantities, or
the extent to which two such quantities are close to each other, the
analysis of functions relies on the concept of a norm to measure
various “sizes” of such functions, as well as the extent to which two
functions resemble to each other. But while numbers mainly have just
one notion of magnitude (not counting the $p$ -adic valuations, which are
of importance in number theory), functions have a wide variety of such
magnitudes, such as “height” ( $L^\infty$ or $C^0$ norm), “mass” ( $L^1$ norm), “mean square” or “energy” ( $L^2$ or $H^1$ norms), “slope”
(Lipschitz or $C^1$ norms), and so forth. In modern mathematics, we use
the framework of function spaces to understand the properties of
functions and their magnitudes; they provide a precise and rigorous
way to formalise such “fuzzy” notions as a function being tall, thin,
flat, smooth, oscillating, etc. Evidently my question here is a bit more concrete, but not far off from what Professor Tao is addressing in the most natural, uncondescending and didactic way to capture as broad a segment of his blog's readers as possible. Clearly he is not talking at the audience, seeking acclaim from the initiated, but rather communicating effectively. The first $1/2$ of the answer I am looking for would be this motivation for $L^2$ : Roughly speaking, $L^2$ space is the only functional space among $L^p$ spaces which is a Hilbert space, i.e. it has an inner product (and also complete)! One can imagine this spaces as a generalization of $\mathbb R^n$ to infinite dimensional cases. So many trends like finding minimum/maximum of function from $\mathbb R^n$ to $\mathbb R$ can be generalized to these spaces in a similar way... But I am looking for more motivation behind the comment, "" $L^1$ and $L^2$ are not ""completely separate"", in that $L^1 \cap L^2$ is a ""very large"" set."" Is $\{L^1\} > \{L^2\}$ ? How much bigger (or smaller)? What does it mean that the former measures the mean, while them latter, the energy? Etc. There is this post in the Physics SE , also delving into $L^2,$ but again, not making an intuitive comparison with $L^1.$","['normed-spaces', 'vector-spaces', 'real-analysis', 'functional-analysis', 'soft-question']"
4121548,"Direct limit and amalgamation (Serre's ""Trees"")","At the very beginning of Serre's Trees , it's taken that the groups $G_i$ (indexed over some set $I$ , with no additional specifications) are equipped with homomorphisms $f_{ij}:G_i\to G_j$ , collected in $F_{ij}$ defined for all $i,j\in I$ . From this, he constructs the direct limit $G$ with maps $f_i:G_i\to G$ , satisfying $f_j\circ f_{ij}=f_i$ and a universal property. Then, he gives the example $I=[2]$ , with the only specifications on the groups $G_1$ and $G_2$ being the existence of homomorphisms $f_i:A\to G_i$ for some other group $A$ , and claims that $G$ in this setting is the amalgamated product $G_1*_AG_2$ . My question is exactly what implicit information about $F_{12}$ and $F_{21}$ is given in that example. Here is my best guess. Say $G_1$ is generated by (fixed) $S$ , and $T\subset A$ has the set bijection $\phi:T\overset{\sim}{\to}S$ where $\phi=f_1|_T$ . Then $f_T=f_2\circ\phi^{-1}$ , which extends to all of $G_1$ , and $F_{12}=\{f_T:T\subset A,f_1|_T:T\overset{\sim}{\to}S\}$ . $F_{21}$ arises analogously. I think this is equivalent to the notion of amalgamation that I've seen elsewhere of handling $f_1(x)f_2(x)^{-1}$ , but wanted to be certain by phrasing it explicitly in the notation Serre introduces for the direct limit. Thanks in advance.","['group-theory', 'abstract-algebra', 'limits-colimits']"
4121600,The trees $T_i$ and $T_j$ have a vertex in common. Show that $T$ has a vertex which is in all of the $T_i$.,"Let $T_1, \ldots, T_k$ be subtrees of a tree $T$ such that for all $1 \leq i < j \leq k$ , the trees $T_i$ and $T_j$ have a vertex in common. Show that $T$ has a vertex which is in all of the $T_i$ . My demonstration . By induction on $n$ , the order of $T$ . First, if $T$ has only one vertex, i.e., $n = 1$ , the result is obvious: each of the $T_1, \ldots , T_k$ are in fact the only vertex of $T$ . So the base case of induction is true. Suppose the statement is true for $m$ . Let us consider a terminal vertex $v\in T$ . Let us define $T':=T\setminus \{v\}$ which is a tree with $m$ vertices, and let us define $T_i':=T_i\setminus \{v\}$ which are also subtrees of $T'$ . Let us check that for each $i\neq j$ , $T_i'\cap T_j'\neq \emptyset$ . We know by hypothesis that $T_i\cap T_j\neq \emptyset$ ; if this common vertex is not $v$ , then it is a common vertex for $T_i'$ and $T_j'$ . If the common vertex for $T_i$ and $T_j$ is $v$ , then the vertex adjacent to $v$ , say $u$ , is a common vertex for $T_i'$ and $T_j'$ . So the subtrees $T_1', \ldots , T_k'$ have a common vertex. The induction hypothesis tells me that $T'$ has a vertex that is in all $T_i'$ , and this is what must be concluded for $T$ . I must see how to conclude from the information I already have, the existence of such a vertex in $T'$ and what I know by hypothesis for the $T_i$ and the $T_i'$ .","['graph-theory', 'trees', 'discrete-mathematics']"
4121606,Is the function surjective?,"Is the function $f:\mathbb Z^2 \to \mathbb Z$ ; $f(m,n)=m^2-2n^2$ surjective? Failed showing that $m^2=5+2n^2 \implies m$ is not an integer with induction...","['functions', 'discrete-mathematics']"
4121655,How to apply Borel-Cantelli Lemma to find if something converges almost surely to $0$.,"Suppose that $Z_1,Z_2,...$ are random variables with $Z_n\sim\mathrm{Exp}(1)$ .  (We do not assume that these random variables are independent.)  Show that $Z_n/\big(\ln^2(n)\big)$ converges to $0$ almost surely. To be honest I am not really sure where to start with this problem , I believe I should start with the lim(sup) of the given function summed up. But after that I have no idea. Help would be appreciated, thanks.","['borel-cantelli-lemmas', 'convergence-divergence', 'probability-limit-theorems', 'probability-theory']"
4121696,$\delta$- hausdorff measure of open ball,"Let U open ball in $\mathbb{R}^n$ , $n \ge 2$ , such that diameter $d(U)=\delta$ . Let $ 0 \le s \le 1$ , we need to prove that $H_{\delta}^s(U)= H_{\delta}^s(\partial U)= H_{\delta}^s(\bar{U}) $ . Here $H_{\delta}^s(A)= \inf \{ \sum_{i} d(E_i)^s : A \subset \cup E_i, d(E_i) \le \delta \}, $ and $d(B)$ is just the diameter of $B$ . I am thinking since $\partial{U} \subset \bar{U} $ and $ U \subset \bar{U}$ , we have $H_{\delta}^s(\partial U) \le H_{\delta}^s(\bar{U}) $ and $H_{\delta}^s(U) \le H_{\delta}^s(\bar{U}) $ . I also know that $H_{\delta}^s(U) \le \delta^s$ . Any hint how to continue?","['measure-theory', 'geometric-measure-theory', 'hausdorff-measure', 'analysis']"
4121734,When should I stop playing this dice game?,"The rules are as follows: You start with \$1 and roll a six-sided die. If you roll anything but a 1, you double your money (so \$2 for the first roll, $4 for the second, and so on). If you roll a 1, you lose all your money. What is the optimal number of times you should roll the die to make the most money? My initial theory: The probability of rolling 2-6 consecutively $n$ times is $(\frac{5}{6})^n$ ; and the probability of rolling a 1 is just $\frac{1}{6}$ . So, the expected value is going to be: $$
E = (\frac{5}{6})^n2^n-\frac{1}{6}2^n \\
=2^n(\frac{5}{6}^n-\frac{1}{6})
$$ Now, if I solve for $n_{L}$ where $E=0$ : $$
0=2^{n_{L}}(\frac{5}{6}^{n_{L}}-\frac{1}{6}) \\
\frac{1}{6}=\frac{5}{6}^{n_{L}} \\
n_{L}=log_{5/6}\frac{1}{6}\\
\approx9.8
$$ Which means I should roll around 9 times to maximize my profit (or maybe 10 if I'm feeling lucky). However, I tried running a quick simulation and got $n_{L}=5.01 \pm 0.05$ after 10,000 games. Where did I go wrong? Thank you! For reference: import random
import numpy as np

def dice():                                                                    
    return random.randint(1,6)
 
ns=np.array([])
for i in range(10000):
    n=0 
    while dice() != 1:
        n+=1
    ns=np.append(ns,[n])
print(np.average(ns))
print(np.std(ns)/100) Output 5.0093
0.05511008393207182 Edit: Here's a more accurate simulation that reflects Ross Millikan's answer for my own reference. import random
import numpy as np
                                                                               
def dice():
    return random.randint(1,6)
    
trials=np.array([[]])
for i in range(30):
    moneys=np.array([])
    for j in range(100):
        money=1
        numrolls=0
        while numrolls <= i:
            numrolls+=1
            if dice() != 1:
                money=2*money
            else:
                money=0
                break
        moneys = np.append(moneys,[money])
    print(""i: ""+str(i))
    trial = np.array([i,np.average(moneys)])
    print(""money: ""+str(np.average(moneys)))
    trials = np.append(trials,trial) Output i: 0
money: 1.74
i: 1
money: 2.48
i: 2
money: 4.88
i: 3
money: 6.56
i: 4
money: 14.08
i: 5
money: 28.8
i: 6
money: 32.0
i: 7
money: 40.96
i: 8
money: 81.92
i: 9
money: 112.64
i: 10
money: 245.76
i: 11
money: 737.28
i: 12
money: 1146.88
i: 13
money: 327.68
i: 14
money: 2293.76
i: 15
money: 3276.8
i: 16
money: 6553.6
i: 17
money: 13107.2
i: 18
money: 26214.4
i: 19
money: 52428.8
i: 20
money: 20971.52
i: 21
money: 0.0
i: 22
money: 83886.08
i: 23
money: 335544.32
i: 24
money: 335544.32
i: 25
money: 671088.64
i: 26
money: 1342177.28
i: 27
money: 8053063.68
i: 28
money: 0.0
i: 29
money: 0.0","['expected-value', 'dice', 'probability', 'gambling']"
4121750,How to find minimal generating sets efficiently?,"Let $G$ be a group. A minimal generating set of $G$ is a subset of $R$ of $G$ that generates $G$ such that no proper subset of $R$ also generates $G$ . For arbitrary groups is its not true (unlike vector space bases) that two minimal generating sets have the same cardinality. The minimal cardinality of the minimal generating sets of $G$ is denoted by $d(G)$ and the maximal cardinality of the minimal generating sets of $G$ is denoted by $m(G)$ (or sometimes $\mu(G))$ . The standard example for groups that have $d(G)\not=m(G)$ is the symmetric groups. In this case Whiston proved that $m(Sym(n))=n-1$ and it is easily shown that $d(Sym(n))=2$ . In the case when $d(G)=m(G)$ then the following procedure can be used to generate a minimal generating set (all of which have the same number of elements). Randomly choose an element $x\in G$ and let $H=\langle x\rangle$ . Now choose any element in $y\in G$ . If $y\not\in H$ then add $y$ to $H$ . If $y\in H$ then choose another element from $G$ . Continue until $H=G$ . The process will be efficient if there is an efficient way to determine if an element from $G$ is contained within the subgroup $H$ . Here efficient means that the number of operations needed is $O(r(\log(|G|)))$ for some polynomial $r$ when $|G|$ finite and some thing else that is appropriate when $G$ is infinite. If this condition is to difficult then we can substitute a weaker definition. If $G$ satisfies the condition $d(G)=m(G)$ then $G$ is called a $\mathcal{B}$ -group and is said to have the weak basis property. A group $G$ is said to have the basis property if all its subgroups are $\mathcal{B}$ -groups. Apisa and Klopsch proved that if $G$ is finite then $G$ is a $\mathcal{B}$ -group if, and only if, one of the following holds (1) $G$ is a $p$ -group for some prime $p$ ; (2) $G=P\rtimes Q$ where $P$ is a $p$ -group and $Q$ is a cyclic $q$ -group where $p\not=q$ and such that $C_Q(P)\not=Q$ and then $\mathbb{F}_p[Q/C_Q(P)]$ -module $P/\Phi(P)$ is isotypical. A similar result is true for $G$ that have the basis property. These results generalize Burnside's basis theorem which is usually used to show that $p$ -groups have $d(G)=m(G)$ . This means that we need only find an efficient membership test for $p$ -groups and semidirect products $p-$ groups and a cyclic $p$ -group (whether the extra structure needed in the generalization of Burnside's theorem is needed is not clear to me) in order to efficiently find minimal generating sets of finite $\mathcal{B}$ -groups. Question 1: Is there an effective membership algorithm for $\mathcal{B}$ -groups? I assume that there is from some comments but I as of yet I seem to not see it. Computing packages like GAP will find minimal generating sets of minimal cardinality (i.e. $d(G)$ ) when possible). GAP's manual (as pointed out below) states that there are only efficient methods known for computing minimal generating sets of finite solvable groups and of finitely generated nilpotent groups - but does not provide references. Now finite $\mathcal{B}$ -groups are solvable and so they fit into the first category. Question 2: What are references for these methods (finding minimal generating sets of minimal cardinality for finite solvable groups and of finitely generated nilpotent groups). If no references are possible how different are these methods since finite nilpotent groups are solvable? Question 3: When $G$ is not a $\mathcal{B}$ -group (so $d(G)< m(G)$ ) and when $G$ is still a finite solvable group or is a finitely generated nilpotent group is there a known relationship between $d(G)$ and $m(G)$ beyond $d(G)<m(G)$ ? Such algorithms can be used to help find groups that satisfy properties like: $d(H)<d(K)\leq d(G)$ for all subgroups $e\neq H\lneq K\leq G$ (Which appear to have a similar structure for $\mathcal{B}$ -groups or for groups that almost satisfy this property like $n$ -qubit Pauli groups. But in general these types of questions are just interesting.","['gap', 'computational-algebra', 'nilpotent-groups', 'p-groups', 'group-theory']"
4121790,"Show that $3^{2008} + 4^{2009}$ can be written as product of two positive integers, each of which is larger than $2009^{182}$.","Show that $3^{2008} + 4^{2009}$ can be written as product of two positive integers, each of which is larger than $2009^{182}$ . To show that $3^{2008} + 4^{2009}$ is composite, I used the Sophie-Germain identity: $$[a^4 + 4b^4 = (a^2 + 2ab + 2b^2)(a^2 − 2ab + 2b^2)]$$ So, I got: $$(3^{502})^4 + 4(4^{502})^4$$ Which gives: $$(3^{1004}+2(3^{502} \cdot 4^{502}) + 2\cdot 4^{1004})\cdot(3^{1004}-2(3^{502} \cdot 4^{502}) + 2\cdot4^{1004})$$ I don't know how to simplify this further and compare it with $2009^{182}$ and check whether my method is correct or not. If the factorization is such that both the factors are greater than $2009^{182}$ , then how to prove it? If the factors are not greater than $2009^{182}$ , then we're back to square $1$ . How to proceed further?","['contest-math', 'algebra-precalculus', 'solution-verification', 'factoring']"
4121833,Bound in the Hille-Yosida Theorem,"I saw the following version of the Hille-Yosida theorem in a book: However, from the proof that I have seen, they only proof the bound $$\left|\left|\frac{\partial Y}{\partial s}(s)\right|\right|=\left|\left|AY(s)\right|\right| \le \left|\left|AY_0\right|\right|$$ I know that the proof of the theorem follows from studying the Yosida approximations, but I am unable to prove $\left|\left|AY(s)\right|\right| \le \frac 1s\left|\left|Y_0\right|\right|$ from that.","['functional-analysis', 'unbounded-operators', 'partial-differential-equations']"
4121839,Flows commute if and only if poisson bracket is identically zero,"I tried the following symplectic geometry exercise and wanted to make sure it was correct Let (M,ω) be a closed symplectic manifold, $f,g ∈ C∞(M)$ two smooth functions and $\phi_t,\psi_t ∈ Ham(M,ω)$ the (autonomous) Hamiltonian flows generated by $X_f$ and $X_g$ . Then  the flows commute, ∀t ∈ [0,1], if and only of the Poisson bracket ${f, g} ≡ ω(Xf , Xg) ∈ C^∞(M)$ is identically zero. Let's first suppose that the poisson bracket is identically zero. Then we will have that $H:=\omega(X_g,X_f)=-\omega(X_f,X_g)=0$ . Then using the fact that $ i([X_f,X_g]) \omega = dH = 0$ we get that $i([X_f,X_g]) \omega =0$ and since $\omega$ is non-degenerate we will get that $[X_f,X_g]=0$ , and now using a standard differential geometry fact we get that the flows commute Now let's assume that $\phi_t \circ \psi_t = \psi_t \circ \phi_t, \forall t\in [0,1]$ .It's a well know fact of geometry that this implies that $[X_f,X_g]=0$ . Then we know that $i([X_f,X_g]) \omega = dH$ where $H=\omega(X_g,X_f)$ . And so this tells us that $d\omega(X_f,X_g)=0$ . Now another way to see this is that in each connected component $U$ the function will be constant,i.e. , $\omega(X_f,X_g)=a_U$ for some $a_U\in \mathbb{R}$ and so this tells us that $df(X_g)=a_U$ and so we get that for any $p\in U, \frac{d}{dt}f(\psi_t(p))=a_U*t+b$ for some $b\in \mathbb{R}$ . Now if we assume that $a_U \neq 0$ we will get a contradiction with the fact that $M$ is compact. Take the sequence $\{\psi_{n}(p)\}_{n=-\infty}^{n=\infty}$ , since $M$ is compact we can take a convergent subsequence $\psi_{n_k}(p)\rightarrow a$ with $n_k\rightarrow \infty$ .Now since the functions involved are continuous we must have that $f(\psi_{n_k}(p))=a_U*(n_k)+b\rightarrow f(a)$ , but the left side is going to infinity while the right side is a finite number and so we obtain a contradiction,and we must have that $a_U=0$ . Since the choice of $U$ was arbitrary we get the desired result. Any input is appreciated. Thanks in advance.","['solution-verification', 'symplectic-geometry', 'differential-geometry']"
4121841,How to plot the graph in polar coordinates of $r^2< \cos(2θ)$,"$z$ is a complex number, $|z^2-1|<1$ .
Question is, to verify whether the above set is a region? So i tried to plot the graph of the above set, to get idea of the set. First i tried converting to, cartesian coordinates, it became, $(x^2+y^2)^2<2(x^2-y^2)$ . But for me,this seemed no good. Using polar coordinate, the inequality transformed to, $r^2< 2\cos(2θ)$ , it looks a lot simpler now. Now , i don't know how to plot such  graphs, in polar coordinates.","['complex-analysis', 'complex-geometry', 'polar-coordinates', 'graphing-functions']"
4121851,"If every bounded sequence has a weakly convergent subsequence, then the inner product space is Hilbert","We can prove that in Hilbert space, bounded space has a weakly convergent subsequence, as we can see here or there . Does the converse hold? That is, in an inner product space $H$ , if every bounded sequence has a weakly convergent subsequence, is $H$ complete? My work: I thought it is a good start to look at a bounded sequence in an incomplete inner product space. As we see here , the space of the sequence which has finitely non-zero element is incomplete. Then I tried to make a bounded (but not weakly convergent) sequence in this space, but I am failing to do so. How can I proceed from here?","['hilbert-spaces', 'inner-products', 'functional-analysis', 'weak-convergence']"
4121860,How to find the limit that involves the factorials?,"$$ \lim_{n\to\infty}\left(\int_a^b((x-a)(b-x))^n\mathrm{d}x\right)^{\frac{1}{n}}$$ I have found the form of the integrand to be $$\frac{(n!)^2}{(2n+1)!}(b-a)^{2n+1}$$ Now, splitting the limit into two different parts i would need to solve $$\lim_{n\to\infty}(\frac{(n!)^2}{(2n+1)!} )^{\frac{1}{n}}$$ My mind went to Stirling's approximation formula but i don't think it could help.","['integration', 'limits', 'calculus', 'factorial']"
4121879,Concatenation of truncated Brownian motion and increment from the stopped Brownian motion is again a Brownian motion,"Consider the concatenation  mapping $\Phi: C_{(0)}\times [0, \infty) \times C_{(0)}$ , where $C_{(0)}:=\{f \in C[0, \infty): f(0) = 0 \}$ , $$
\Phi(f, t, g) := \begin{cases}
f(s) &\text{ if } 0 \le s < t, \\
f(t) + g(s-t) &\text{ if } t \le s < \infty
\end{cases}
$$ ( $\Phi$ ""glues"" $f$ and $g$ together at the point $t$ ). Take as facts the strong Markov property and that $\Phi$ is continuous and hence measurable. I would like to show that given a Brownian motion $B$ , the concatenation of the truncated Brownian motion $B(t \wedge \tau)$ and the increment $B(\cdot + \tau) - B(\tau)$ is a Brownian motion, that is, that $$
\Phi(B(\cdot \wedge \tau), \tau, B(\cdot + \tau) - B(\tau))
$$ is a Brownian motion. My idea is to verify the axioms (for Brownian motion) directly. The strong Markov property  means that the probability of a measurable rectangle of the form $$
\left \{ ( B( \cdot \wedge \tau ), \tau ) \in A \right\} \times \left \{ B (\cdot + \tau) - B (\tau) \right \}
$$ equals the product of the probabilities of the two rectangles. I would need help with the axioms for independent increments  and that the increments are normally distributed with zero mean and variance equal to the length of the increment. That is,  1) that given $0 \le t_0 < t_1 < t_2 < t_3$ , $$
\Phi(B(\cdot \wedge \tau), \tau, B(\cdot + \tau) - B(\tau))(t_3) -\Phi(B(\cdot \wedge \tau), \tau, B(\cdot + \tau) - B(\tau))(t_2)
$$ and $$
\Phi(B(\cdot \wedge \tau), \tau, B(\cdot + \tau) - B(\tau))(t_1) -\Phi(B(\cdot \wedge \tau), \tau, B(\cdot + \tau) - B(\tau))(t_0)
$$ are independent and 2) that given any $0 \le s<t$ , $$
\Phi(B(\cdot \wedge \tau), \tau, B(\cdot + \tau) - B(\tau)) -\Phi(B(\cdot \wedge \tau), \tau, B(\cdot + \tau) - B(\tau))(s)
$$ is distributed according to $N(0, t-s)$ .","['stochastic-processes', 'brownian-motion', 'probability-theory']"
4121884,Why does a spiral structure appear for this function?,"Consider the following 2D vector field on the $xy$ -plane $$\vec{V}=\begin{pmatrix}
-(m^2-md+x^2)\cos{2d\,t}+xy\sin{2d\,t} \\
-xy\cos{2d\,t}+(m^2-md+y^2)\sin{2d\,t}
\end{pmatrix}$$ where $d=\sqrt{x^2+y^2+m^2}$ and a constant $m\geq0$ .
When plotting the vector's angle $\arctan(V_x,V_y)\in[0,2\pi]$ by color on the $xy$ -plane, it always clearly shows a spiral pattern ( $t=15,m=0.3$ in the plot below). How can I understand the appearance of the spiral?","['vectors', 'vector-fields', 'visualization', 'functions', 'trigonometry']"
4121951,Another question on reciprocals of partial derivatives,"Consider the function $f(\mathbf{x},g(\mathbf{x}))$ , where $f:\mathbb{R}^n\times\mathbb{R}^n\to \mathbb{R}$ , $\mathbf{x}\in\mathbb{R}^n$ and $g:\mathbb{R}^n\to\mathbb{R}^n$ . I want to take the gradient $\nabla_{\mathbf{x}}f(\mathbf{x},g(\mathbf{x}))$ , which I think should give me this... $$
\nabla_{\mathbf{x}}f(\mathbf{x},g(\mathbf{x})) = \left[\frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial g}\frac{\partial g}{\partial x_1},\cdots,\frac{\partial f}{\partial x_n} + \frac{\partial f}{\partial g}\frac{\partial g}{\partial x_n}\right]^{T}
$$ Finding the derivatives $\frac{\partial g}{\partial x_i}$ is a little tricky.  Naively I thought I could do something like $$
\frac{\partial g}{\partial x_i} = \frac{\partial g}{\partial f}\frac{\partial f}{\partial x_i}
$$ under the assumption that $\frac{\partial g}{\partial f} = \left(\frac{\partial f}{\partial g}\right)^{-1}$ , yet substituting such an expression into the equation for the gradient would give me the following form $$
\nabla_{\mathbf{x}}f(\mathbf{x},g(\mathbf{x})) = \left[\cdots,2\frac{\partial f}{\partial x_i},\cdots\right]
$$ Clearly the reciprocal argument used to find $\frac{\partial g}{\partial f}$ has been abused here, but I'm not clear why.  I was under the impression that such an argument could be used as long as the same variables are being held constant, and I think I am doing that.  I guess the remaining weak spot is that I have fundamentally misunderstood the application if inverse function theorem in this context?","['partial-derivative', 'multivariable-calculus', 'calculus']"
4121977,Pythagorean triples conditions,"Pythagorian triple is every triple of natural numbers $(x, y, z)$ such that $x, y, z$ are sides of a right triangle, where $z$ is the hypotenuse. Now, Pythagorean theorem says: $$x^2 + y^2 = z^2 \tag1$$ If we look just natural solutions to the equation $(1)$ , without geometrical condition that a right triangle with sides $x, y, z$ do exist, do we have more solutions? And if yes, in which pattern? Of course, condition that for natural triple satisfying $x^2 + y^2 = z^2$ there must exist right triangle with sides $x, y, z$ , which is equivalent to add 3 conditions to the $(1)$ : (triangle inequality) $x^2 + y^2 = z^2$ and $x + y > z$ $x + z > y$ $y + z > x$ (Because if 1., 2., 3. holds, we can construct right triangle with sides $x, y, z$ .)","['elementary-number-theory', 'algebra-precalculus', 'pythagorean-triples']"
4121985,"$\frac{d}{ds} \langle T, T \rangle = 2\langle \nabla_S T, T \rangle$?","Our setting is $(M, g)$ , a Riemannian manifold. Let $\Gamma(s,t) \subset M$ be a variation about curve $\gamma(t) = \Gamma(0, t)$ (Let us say that our domain of $\Gamma$ is $(a_0, a_1) \times (b_0, b_1) \subset \mathbb R^2$ , and $(a_0, a_1)$ contains $0$ .) Define $$T = \partial_t \Gamma; S = \partial_s \Gamma.$$ My textbook says: \begin{equation*}
\frac{d}{ds} \langle T, T \rangle = 2\langle \nabla_S T, T \rangle.\end{equation*} If I treat $\frac{\partial}{\partial s}$ as a tangent vector $S$ (or a vector field), then everything makes sense. However, I have a trouble understanding why $\frac{d}{ds}$ is a tangent vector at $T_p M$ , where $p = \Gamma(s_0,t_0)$ for some $s_0, t_0$ . Note that $\langle T, T \rangle$ is a function $(a_0, a_1) \times (b_0, b_1) \rightarrow \mathbb R$ , so it can be treated it as a function from $\mathbb R^2$ to $\mathbb R$ . I am merely taking a partial differentiation w.r.t. $s$ , and it has nothing to do with tangent vector at $T_p M$ . How do I resolve this?",['differential-geometry']
4122130,How do I prove that $\frac{1}{45}<\sin^{2020}(\frac{\pi}4)<\frac{2}{45}?$,"How do I prove that $$\frac{1}{45}<\sin^{2020}\left(\frac{\pi}4\right)<\frac{2}{45},$$ where $\sin^n$ denotes the composition of the sine function with itself $n$ times.  For example, $$\sin^3(x) = \sin(\sin(\sin(x))).$$ Are there any relations between $45$ and $\sin x?$ or is there some way to calculate an approximationďĽź","['trigonometry', 'inequality']"
4122146,"Suppose A and B are sets, finite or infinite. Prove relationship of cardinality and power set","Question: Suppose A and B are sets, finite or infinite. Prove: If $|A| \le |B|$ then $|P(A)| \le |P(B)|$ where $P$ denotes a power set and $|A|$ denotes the cardinality of $A$ . Approach: Since $|A| \le |B|$ , there exists an injection $f:A \to B$ . We can use this to construct an injection $F:P(A) \to P(B)$ . Define $F(X)$ as follows, $F(X)=\{t \in T| t=f(s) \text{ for some } s \in X\}$ . This is clearly a function from $P(A)$ to $P(B)$ , It remains to show that it is injective. Injective Suppose there were two sets $X,Y \subseteq T$ , $X \not=Y$ and $F(X)=F(Y)$ . Consider an $s$ that is in one of these sets and not the other. Then there must be another element $s'$ in $Y$ such that $f(s)=f(s')$ , since otherwise $f(s)$ would not be in $F(Y)$ . But, $f$ was a injection, so this cannot be the case. Does my answer look good? Please give me some advice! Thank you so much!",['elementary-set-theory']
4122155,Can we apply relations in a group presentation one by one?,"Consider a group presentation $\langle x,y \mid xy=yx, x^7=y^3 \rangle$ . By definition, this is $F(\{x,y \})/N(xyx^{-1}y^{-1},x^7y^{-3})$ where $F(S)$ denotes the free group on the set $S$ and $N(R)$ denotes the normal subgroup generated by $R$ . Intuitively, when identifying this group from its presentation, I first apply the relation $xy=yx$ , which gives the group $\mathbb{Z}^2$ , and then i further apply the relation $x^7=y^3$ to the groups $\mathbb{Z}^2$ , which gives $\mathbb{Z}$ as a final answer. As a sanity check, I'm trying to prove that this approach is valid in general, but I can't quite seem Claim: Let $G$ be a group and $a,b\in G$ and $\pi: G \to G/N(a)$ be the quotient map. Then $G/N(a,b) \cong \big (G/N(a) \big )/N(\pi(b))$ Attempted proof: We know that $N(\pi(b))= \pi^{-1}(N(\pi(b)))/N(a)$ and then by the third isomorphism theorem we have that $\big (G/N(a)\big )/N(\pi(b)) \cong G/\pi^{-1}(N(\pi(b)))$ so all we need to prove is that $N(a,b) = \pi^{-1}(N(\pi(b)))$ .
We have $z\in\pi^{-1}(N(\pi(b))) \Leftrightarrow \pi(z) \in N(\pi(b)) \Leftrightarrow zN(a) =N(bN(a)) \Leftrightarrow zN(a) = \big( \prod_{g_i \in G} g_i^{-1}b^{\epsilon_i}g_i \big)N(a)$ for some $\epsilon_i$ all $\pm 1$ . This is equivalent to $z = \big( \prod_{g_i \in G} g_i^{-1}b^{\epsilon_i}g_i \big)\big( \prod_{h_i \in G} h_i^{-1}a^{f_i}h_i \big)$ . Now, if $G$ were Abelian I could claim that this was equivalent with $z \in N(a,b)$ , but without that assumption I'm not sure how to proceed. Is the claim incorrect? Or do I just need to do a little more work? Not a duplicate of Why is $\langle S\mid R\cup R'\rangle $ a presentation for $G/N(R')$, where $G$ is a group with presentation $\langle S\mid R\rangle?$ because I'm asking about specifically the part of the proof which the only answer there omits.","['combinatorial-group-theory', 'group-presentation', 'group-theory']"
4122219,Positive integer solutions to $y^2=a(1+xy-x^2)$,"Let $a>3$ be an integer. Define a sequence $X$ as : \begin{equation}
  \begin{aligned}
    x_1 & = 1\\
    x_2  & = a-1\\
      x_n & = (a-2)x_{n-1}-x_{n-2}, \ \ n\ge3
  \end{aligned}
\end{equation} if $a$ is not a perfect square and \begin{equation}
  \begin{aligned}
    x_1 & = 1\\
    x_2  & = \sqrt{a} > 0\\
    x_3 & =a-1\\
    x_4 & =(a-2)\sqrt{a} > 0\\
      x_{2n+1} & = (a-2)x_{2n-1}-x_{2n-3} , \ \ \ \ \ \ \  n\ge 2\\
 x_{2n} & = (a-2)x_{2n-2}-x_{2n-4}, \ \ \ \ \ \ \  n\ge 3
  \end{aligned}
\end{equation} if $a$ is a perfect sqaure. From a maple output, it appears the terms of this sequence form a complete solution for $x$ in positive integers for the given diophantine equation $y^2=a(1+xy-x^2)$ . How do we go about proving this i.e each term of sequence $X$ is a solution and that these are the only positive solutions. I tried mathematical induction but got stuck.","['elementary-number-theory', 'diophantine-equations', 'real-analysis', 'algebraic-geometry', 'sequences-and-series']"
4122288,Solving $7^x\bmod {29} = 23 $,I have $$7^x\bmod {29} = 23 $$ It is possible to get $x$ by trying out different numbers but that will not be possible if $x$ is actually big. Are there any other solutions for this equation? Kind regards,"['modular-arithmetic', 'discrete-mathematics']"
4122350,Properties of the stochastic integral $\int_0^t\frac{|B_s|}{s}\Bbb dB_s$,"For a Brownian motion $(B_t)_{t\geq0}$ we define the process $$I_t:=\int_0^t\frac{|B_s|}{s}\Bbb dB_s,\qquad (t>0).$$ But what can we say about this process? Is it even possible to define this process? If yes, it is for sure a local martingale, but is it also a (true) martingale? As $$\Bbb E[I]_t=\Bbb E\int_0^t\frac{B_s^2}{s^2}\Bbb ds=\int_0^t\frac{1}{s}\Bbb ds=\infty,
$$ where $[.]_t$ denotes the quadratic variation, the second moment can't exist if it is a martingale. Also, what can we say about the limit $$I_0:=\lim\limits_{t\searrow0}I_t,$$ does it exist in any sense?
I thought about scaling property, Hölder continuity and the law of the iterated logarithm, but notthing helped. As the limit $$\lim\limits_{t\searrow0}\frac{B_t^2}{t}$$ doesn't exist, I can't just use the Itô formula. All hints about which properties this process possesses are welcome.","['stochastic-integrals', 'stochastic-processes', 'martingales', 'probability']"
4122369,Why is $\prod_{a\in G}a = e_G$ if there exist several elements with $\operatorname{ord}(u) = 2$?,"I'm currently reading Algebra by Karpfinger, Christian, and Meyberg, Kurt . There I found the following exercise: Prove that for any abelian finite group $G$ it holds true that: If $G$ has more than one element $u$ with $\operatorname{ord}(u) = 2$ then: $$\prod_{a\in G}a = e_G$$ where $e_G$ is the neutral element in $G$ . Idea : Since $G$ is abelian we can rewrite the product $\prod_{a\in G}a = a_1\cdot a_2 \cdot \ldots$ by placing any $a_i$ next to its inverse and they'll cancel out. Then we are only left with all selfinverse elements $u$ . Further I know that there must be an odd number of those elements $u$ , because: the number of elements with order greater than $2$ is even there is one neutral element the order of $G$ is even, because of Lagrange's theorem Question: I'd like to get a hint on how to solve this. Is my progress until now correct or leading to the solution? In particular, I don't understand how we get the neutral element through the multiplication of the selfinverse elements, because I have no intuition on how they are connected.","['group-theory', 'abstract-algebra', 'abelian-groups']"
4122408,"Prove $\int\int_{[0,\pi]\times [0,\pi]} |\cos(x+y)| d(x,y) = 2\pi$","I have come to an exercise in a multivariate calculus book that I am having trouble with. The problem is : Show that $\int\int_{[0,\pi]\times[0,\pi]} |\cos(x+y)| d(x,y) = 2 \pi$ I have attempted solving the problem but I'm not getting the right answer. I know there are probably other ways to solve this problem, but here I am seeking help from others to see where I am going wrong in my solution. My solution is below : Let : \begin{equation}
R = [0,\pi] \times [0,\pi]
\end{equation} We see : \begin{equation}
\{ x + y \; : \; (x,y) \in R \} = [0,2\pi] = T
\end{equation} We see : \begin{equation}
| \cos(x+y) | = 
	\begin{cases}
	\cos(x+y)  \; & \forall \; x + y \in \left[ 0 , \frac{\pi}{2} \right] \bigcup \left[ \frac{3\pi}{2} , 2 \pi \right] \\
	-\cos(x+y) \; & \forall \; x + y \in \left[ \frac{\pi}{2} , \frac{3\pi}{2} \right]
	\end{cases}
\end{equation} Define : \begin{align}
D_{1} 	& = \left\{ (x,y) \in R \; : \; x + y \in \left[ 0 , \frac{\pi}{2} \right] \right\} 	\\
D_{2} 	& = \left\{ (x,y) \in R \; : \; x + y \in \left[ \frac{3\pi}{2} , 2\pi \right] \right\} \\
D_{3} 	& = \left\{ (x,y) \in R \; : \; x + y \in \left[ \frac{\pi}{2} , \frac{3\pi}{2} \right] \right\} 
\end{align} We see : \begin{equation}
T = D_{1} \bigcup D_{2} \bigcup D_{3}
\end{equation} and : \begin{align}
D_{1} \bigcap D_{2}	& = \emptyset \\
D_{1} \bigcap D_{3}	& = \left\{ (x,y) \in R \; : \; x + y = \frac{\pi}{2} \right\}\\
D_{2} \bigcap D_{3}	& = \left\{ (x,y) \in R \; : \; x + y = \frac{3\pi}{2} \right\}
\end{align} We see : \begin{align}
(x,y) \in D_{1} \bigcap D_{3} 	& \Rightarrow \cos(x+y) = 0 \\
(x,y) \in D_{2} \bigcap D_{3} 	& \Rightarrow \cos(x+y) = 0
\end{align} So  : \begin{equation}
\int\int_{R} |\cos(x+y)| d(x,y) = \int\int_{D_{1}} \cos(x+y)d(x,y) + \int\int_{D_{2}} \cos(x+y)d(x,y) - \int\int_{D_{3}} \cos(x+y)d(x,y)
\end{equation} We see : \begin{align}
(x,y) \in D_{1} & \Rightarrow x \in \left[ 0 , \frac{\pi}{2} \right] \text{ and } y \in \left[ 0 , \frac{\pi}{2} - x \right] \\
(x,y) \in D_{2} & \Rightarrow x \in \left[ \frac{\pi}{2} , \pi \right] \text{ and } y \in \left[ \frac{3\pi}{2} - x , \pi \right]
\end{align} Let's say : \begin{equation}
D_{3} = D_{3}^{(a)} \bigcup D_{3}^{(b)}
\end{equation} where : \begin{align}
D_{3}^{(a)} 	& = \left\{ (x,y) \in D_{3} \; : \; x \in \left[ 0 , \frac{\pi}{2} \right] \right\}\\
D_{3}^{(b)} 	& = \left\{ (x,y) \in D_{3} \; : \; x \in \left[ \frac{\pi}{2} , \pi \right] \right\}
\end{align} So : \begin{align}
(x,y) \in D_{3}^{(a)} & \Rightarrow x \in \left[ 0 , \frac{\pi}{2} \right] \text{ and } y \in \left[ \frac{\pi}{2} - x , \pi \right] \\
(x,y) \in D_{3}^{(b)} & \Rightarrow x \in \left[ \frac{\pi}{2} , \pi \right] \text{ and } y \in \left[ 0 , \frac{3\pi}{2} - x \right]
\end{align} and : \begin{equation}
(x,y) \in D_{3}^{(a)} \bigcap D_{3}^{(b)} \Leftrightarrow x = \frac{\pi}{2} \text{ and } y \in \left[ 0, \pi \right]
\end{equation} So : \begin{equation}
\int\int_{D_{3}} \cos(x+y)d(x,y) = \int\int_{D_{3}^{(a)}} \cos(x+y)d(x,y) + \int\int_{D_{3}^{(b)}} \cos(x+y)d(x,y) - \int\int_{D_{3}^{(a)}\bigcap D_{3}^{(b)}} \cos(x+y)d(x,y)
\end{equation} We can see that $D_{1},D_{2},D_{3}^{(a)},D_{3}^{(b)}$ , and $D_{3}^{(a)} \bigcap D_{3}^{(b)}$ are all elementary regions. So we can use
Fubini's theorm to evaluate each. Let : \begin{align}
\phi_{1}(x) & = 0 \\
\phi_{2}(x) & = \frac{\pi}{2} - x
\end{align} So : \begin{align}
\require{cancel}
\int\int_{D_{1}} \cos(x+y)d(x,y) 	
	& = \int_{0}^{\frac{\pi}{2}} \left[ \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right] dx\\
	& = \int_{0}^{\frac{\pi}{2}} \left[ \int_{0}^{\frac{\pi}{2} - x} \cos(x+y) dy \right] dx\\
	& = \int_{0}^{\frac{\pi}{2}} \left( \sin(x+y) \Bigr|_{0}^{\frac{\pi}{2} - x} \right) dx \\
	& = \int_{0}^{\frac{\pi}{2}} \left( \sin\left( \cancel{x} + \frac{\pi}{2} - \cancel{x} \right) - \sin(x) \right) dx\\
	& = \int_{0}^{\frac{\pi}{2}} \sin\left( \frac{\pi}{2} \right) dx - \int_{0}^{\frac{\pi}{2}} \sin(x) dx \\
	& = \int_{0}^{\frac{\pi}{2}} 1 dx + \left( \cos(x) \Bigr|_{0}^{\frac{\pi}{2}} \right)\\
	& = \frac{\pi}{2} + \cos\left( \frac{\pi}{2} \right) - \cos(0) \\
	& = \frac{\pi}{2} + 0 - 1 \\
	& = \frac{\pi}{2} - 1
\end{align} Let : \begin{align}
\phi_{1} & = \frac{3\pi}{2} - x \\
\phi_{2} & = \pi
\end{align} So : \begin{align}
\int\int_{D_{2}} \cos(x+y)d(x,y) 
	& = \int_{\frac{\pi}{2}}^{\pi} \left( \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right) dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \left( \sin(x+y)\Bigr|_{\frac{3\pi}{2} - x}^{\pi} \right) dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \left( \sin(x+\pi) - \sin\left( \cancel{x} + \frac{3\pi}{2} - \cancel{x} \right) \right)dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \sin(x+\pi) dx - \int_{\frac{\pi}{2}}^{\pi} \sin\left( \frac{3\pi}{2} \right) dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \left[ \sin(x)\cos(\pi) + \cancel{\sin(\pi)\cos(x)} \right] dx + \int_{\frac{\pi}{2}}^{\pi} dx\\
	& = \left( - \int_{\frac{\pi}{2}}^{\pi} \sin(x) dx \right) + \frac{\pi}{2} \\
	& = \cos(x) \Bigr|_{\frac{\pi}{2}}^{\pi} + \frac{\pi}{2} \\
	& = \cos(\pi) - \cancel{\cos\left( \frac{\pi}{2} \right)} + \frac{\pi}{2} \\
	& = -1 + \frac{\pi}{2} \\
	& = \frac{\pi}{2} - 1 
\end{align} Now let : \begin{align}
\phi_{1}(x) & = \frac{\pi}{2} - x \\
\phi_{2}(x) & = \pi
\end{align} So : \begin{align}
\int\int_{D_{3}^{(a)}} \cos(x+y) d(x,y) 
	& = \int_{0}^{\frac{\pi}{2}} \left[ \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right] dx\\
	& = \int_{0}^{\frac{\pi}{2}} \left( \sin(x+y) \Bigr|_{\frac{\pi}{2} - x}^{\pi} \right) dx\\
	& = \int_{0}^{\frac{\pi}{2}} \sin(x+\pi) dx - \int_{0}^{\frac{\pi}{2}} \sin\left(\cancel{x} + \frac{\pi}{2} - \cancel{x} \right) dx\\
	& = \int_{0}^{\frac{\pi}{2}} \left( \sin(x)\cos(\pi) + \cancel{\sin(\pi)\cos(x)} \right) dx - \int_{0}^{\frac{\pi}{2}} \sin\left( \frac{\pi}{2} \right) dx\\
	& = -\int_{0}^{\frac{\pi}{2}} \sin(x) dx + 0 - \frac{\pi}{2}\\
	& = \cos(x) \Bigr|_{0}^{\frac{\pi}{2}} - \frac{\pi}{2} \\
	& = \cancel{\cos\left( \frac{\pi}{2} \right)} - \cos(0) - \frac{\pi}{2}\\
	& = -1 - \frac{\pi}{2} 
\end{align} Now let : \begin{align}
\phi_{1}(x) & = 0\\
\phi_{2}(x) & = \frac{3\pi}{2} - x
\end{align} So : \begin{align}
\int\int_{D_{3}^{(b)}} \cos(x+y)d(x,y) 
	& = \int_{\frac{\pi}{2}}^{\pi} \left( \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right) dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \left( \sin(x+y) \Bigr|_{0}^{\frac{3\pi}{2} - x} \right) dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \left[ \sin\left( \cancel{x} + \frac{3\pi}{2} - \cancel{x} \right) - \sin(x) \right] dx\\
	& = - \int_{\frac{\pi}{2}}^{\pi} 1 dx - \int_{\frac{\pi}{2}}^{\pi} \sin(x) dx \\
	& = -\frac{\pi}{2} + \left( \cos(\pi) - \cos\left( \frac{\pi}{2} \right) \right)\\
	& = -\frac{\pi}{2} - 1 
\end{align} We see : \begin{align}
\int\int_{D_{3}^{(a)} \bigcap D_{3}^{(b)}} \cos(x+y) d(x,y) 
	& = \int_{0}^{\pi} \cos\left( \frac{\pi}{2} + y \right) dy \\
	& = \int_{0}^{\pi} \left[ \cancel{\cos\left(\frac{\pi}{2}\right)\cos(y)} - \sin\left(\frac{\pi}{2}\right) \sin(y) \right] dy\\
	& = -\int_{0}^{\pi} \sin(y) dy\\
	& = \cos(y) \Bigr|_{0}^{\pi}\\
	& = \cos(\pi) - \cos(0)\\
	& = -1 - 1 = -2
\end{align} So : \begin{align}
\int\int_{D_{3}} \cos(x+y)d(x,y) 	
	& = \left( -1 - \frac{\pi}{2} \right) + \left( -\frac{\pi}{2} - 1 \right) - (-2)\\
	& = \cancel{-2} - \pi + \cancel{2} \\
	& = -\pi
\end{align} So : \begin{align}
\int\int_{R} |\cos(x+y)|d(x,y) 	& = \left( \frac{\pi}{2} - 1 \right) + \left( \frac{\pi}{2} - 1 \right) - (-\pi)\\
				& = \pi - 2 + \pi\\
				& = 2 \pi - 2  \neq 2\pi
\end{align} So I made a mistake somewhere. Can anyone help me see where I made the mistakes that lead to the incorrect result.",['multivariable-calculus']
4122451,How to show that this matrix is symmetric definite positive,"I need help.
We want to show that the matrix $A$ is symmetric definite positive for all $n$ where $n$ is the size of the matrix $A$ . Here is $A$ : $$
A = \begin{bmatrix}
 1&  -1&  0& ...& 0\\ 
 -1&  2&  -1& \ddots & \vdots\\ 
 0&  -1&  2& \ddots &0\\ 
 \vdots &  \ddots &  \ddots&  \ddots &-1 \\
0 & ...  &0 & -1& 2
\end{bmatrix} 
$$ I tried brute forcing by calculating for $x$ a vector of size $n$ , $x^TAx$ , but it seems tricky. Thanks for your help","['matrices', 'linear-algebra', 'positive-definite', 'eigenvalues-eigenvectors']"
4122498,"What rational numbers are expressible as ""multiple-of-$12$"" continued fractions?","I need to use a special subclass of continued fraction expansions for a problem I'm studying. Namely, expansions of the form $$[a_0;a_1;a_2;\dotsc; a_{n}] = a_0+\displaystyle\frac{1}{a_1+\displaystyle\frac{1}{a_2+\displaystyle\frac{1}{\ddots\displaystyle\frac{1}{a_n}}}}$$ where $a_0,\dotsc, a_{n-1}\in 12\mathbb{Z}$ and $a_n\in S\subset\mathbb{Q}\cup\{\infty\}$ , a finite subset of the extended rationals. This problem is in analogue to the ""even continued fraction expansions"", which have $a_0,\dotsc, a_{n-1}\in 2\mathbb{Z}$ and $a_n\in\{0,1,\infty\}$ . It turns out that the set of even continued fraction expansions is just all of $\mathbb{Q}\cup\{\infty\}$ . The proof uses the projectivized congruence level $2$ subgroup ${\rm P}\Gamma(2)$ , which is generated by the Möbius transformations $$A: z\mapsto \frac{z}{-2z+1}\text{ and }B: z\mapsto z+2.$$ I've seen it claimed in various places (but never with proof) that the (left) action of ${\rm P\Gamma}(2) = \langle A, B\rangle$ on $\mathbb{Q}\cup\{\infty\}$ has orbit transversal $\{0,1,\infty\}$ , meaning that for each $r/s\in\mathbb{Q}\cup\{\infty\}$ there is a word $w\in \langle A,B\rangle$ such that $r/s = w.x$ for some $x\in\{0,1,\infty\}$ . Determining an even continued fraction expansion for $r/s$ from there is then simply a matter of decoding the corresponding word $w$ . Now, my questions: is there any hope of a similar result holding in the ""multiple-of- $12$ "" expansion case? Namely, could there be some finite set $S$ such that expansions of that form describe all of $\mathbb{Q}\cup\{\infty\}$ ? Equivalently, if $G$ is the group generated by the transformations $$a: z\mapsto z+12, \quad b:z\mapsto -z,\quad c:z\mapsto 1/z,$$ does the action of $G$ on $\mathbb{Q}\cup\{\infty\}$ have a finite orbit transversal? And how would such an orbit transversal be calculated, finite or not? This seems like a pretty tough/niche problem, so even just pointing me in the direction of some references would be greatly appreciated. Edit: I gave both $b$ and $c$ as valid transformations since I happen to have those symmetries in the context I'm working in, but if one needs to restrict this to just $z\mapsto -1/z$ so that $G$ is a subset of ${\rm PSL}(2,\mathbb{Z})$ , then that is fine.","['continued-fractions', 'number-theory', 'group-theory']"
4122535,Random Walk Markov Process -- Probability of Return to the Origin,"I'm struggling with a problem I was asked by a friend a few days ago. It goes as follows: You start at the origin. On the first iteration you walk right with probability $p$ and left with probability $q = 1 - p$ . On each subsequent iteration you walk in the same direction as the previous iteration with probability $p$ and in the opposite direction with probability $q$ . What is the probability that you return to the origin in $n$ steps or fewer? The issue is that it's difficult to account for the Markov process in the calculation. Since I want something that works for arbitrary $n$ , I can't use matrices. Say $X_i$ is the the random variable such that on the $i$ -th step $$
X_i = \begin{cases} 1 &\text{if}\ i = 0\ \text{and went right}\\
-1 &\text{if}\ i = 0\ \text{and went left}\\
1 &\text{if}\ i > 0\ \text{and went in same direction}\\ 
-1 &\text{if}\ i > 0\ \text{and went in opposite direction.}\end{cases}
$$ Then the probability of returning to the origin is \begin{align*}
P(\text{return}) 
&= q P(\text{return}|X_0 = -1) + p P(\text{return}|X_0 = 1)\\
&= P(\text{return}|X_0 = 1)\\
&= q + p P(\text{return}|X_1 = 1).
\end{align*} I don't know where to go from here. When constructing a Monte Carlo simulation, I first simulate $\left(X_i\right)_{i = 0}^{n-1}$ . Then the cumulative product tells you whether you went left or right on the $i$ -th step. From there, you can take the cumulative sum to find position. If that's ever 0, then you add to the count. It's not clear to me how one would map this onto a mathematically rigorous calculation. This is my Python code. import numpy as np

# Define n
n = 4

# Define p
p = 0.75

# Calculate q
q = 1 - p

# X can be either 1 or -1
vals = [1, -1]

# How many Monte Carlo simulations
sims = 10**6

# Whether same or opposite
X = np.random.choice(vals, size = (sims, n), replace = True, p = [p, q])

# Whether left or right
move = np.cumprod(X, axis = 1)

# Sum change to calculate position
position = np.cumsum(move, axis = 1)

# Which rows go through the origin
success = np.sum(position == 0, axis = 1) > 0

# Calculate fraction that go through the origin
solution = np.mean(success)

solution","['markov-process', 'random-walk', 'probability']"
4122549,"Show that the ""distinguished power"" of a continuous complex-valued function is well-defined","Let $E$ be a normed $\mathbb R$ -vector space $^1$ and $\ln$ denote the principal branch of the complex logarithm. We can show the following result: Theorem : Let $\varphi:E\to\mathbb C\setminus\{0\}$ with $\varphi(0)=1$ . If $\left.\varphi\right|_{\overline B_r(0)}$ is uniformly continuous and $$\inf_{x\in\overline B_r(0)}|\varphi(x)|>0\tag1$$ for all $r>0$ , then there is a unique $f\in C(E,\mathbb C)$ with $f(0)=0$ and $e^f=\varphi$ . Moreover, for every $n\in\mathbb N$ , there is a unique $g_n\in C(E,\mathbb C\setminus\{0\})$ with $g_n(0)=1$ and $g_n^n=\varphi$ ; in fact, $$g_n=e^{\frac fn}\tag2.$$ By this result, the notation $$\varphi^{\frac1n}:=g_n\;\;\;\text{for }n\in\mathbb N$$ is well-defined. Question : For $m,n\in\mathbb N$ , it is tempting to write $\varphi^{\frac mn}$ instead of $g_n^m$ . Are we able to justify this notation by showing that whenver $m,m',n,n'\in\mathbb N$ satisfy $\frac mn=\frac{m'}{n'}$ , then $g_n^m=g_{n'}^{m'}$ ? I wasn't able to show this result, but I'm really sure that it holds. My main problem is that it is intuitively so trivial that it's easy to make a stupid mistake. I wasn't able to utilize this, but we may note $$\left(g_{mn}^m\right)=\varphi\tag3$$ and hence $$g_n=g_{mn}^m\tag4.$$ $^1$ If this generality is preventing you from providing an answer, feel free to assume $E=\mathbb R^d$ (in which case we may further assumpe that $\varphi$ is uniformly continuous (on the whole space) and hence $(1)$ is trivially satisfied by compactness of closed balls in $\mathbb R^d$ ).","['levy-processes', 'logarithms', 'complex-analysis', 'radicals', 'complex-numbers']"
4122564,Density of open sets in finer topology,"Let $X$ be any topological space endowed with two topologies $\mathcal{T} \subset \mathcal{T}'$ (the later means that $\mathcal{T}'$ refines $\mathcal{T}$ , that is every open subset $U \subset X$ with respect $\mathcal{T}$ (ie $U \in \mathcal{T})$ is
already open with respect $\mathcal{T}'$ . Assume that $X$ is irreducible (or say more weaker connected;
irreducible implies connected) with respect $\mathcal{T}$ . Connected means that if there exist
two $U,V$ which are open & cloled for $\mathcal{T}$ and $X= U \dot{\cup}V $ then either $U$ or $V$ is empty. And irreducible
that every two non empty open $U, V \in \mathcal{T}$ intersect properly: $U \cap V \neq \emptyset$ . Question: Are there any interesting sufficient or neccessary conditions
on $X, \mathcal{T}$ and $\mathcal{T}'$ known such that following holds every open non empty $U \in \mathcal{T}$ is 'dense' in $X$ with respect
finer topology $\mathcal{T}'$ What I mean by 'interesting'? Well, everything non trivial/boring like $\mathcal{T}= \mathcal{T}'$ or $\mathcal{T}= \{X, \emptyset\}$ etc. Note that this question generalizes this Question where $X \subset \mathbb{C}^n$ is a complex irreducible variety, $\mathcal{T}$ the Zariski topology and $\mathcal{T}'$ analytical topology on $X$ induced
by $\mathbb{C}^n$","['zariski-topology', 'general-topology']"
4122583,When the sum of the first $n$ consecutive even ($2k \gt 0$) powers is a prime number?,"When $n=2$ , we know 4 Fermat primes (among the 5 which are known) satisfying this condition: $$ 1^2+2^2 =5$$ $$ 1^4+2^4 =17$$ $$ 1^8+2^8 = 257$$ $$ 1^{16}+2^{16} = 65537.$$ One may wonder for what other values of $(n,k)$ , it holds that $\sum_{i=1}^n i^{2k}$ is a prime number. Apparently, when $n=5$ and $2k= 1440$ , \begin{align*} \sum_{i=1}^5 i^{1440}&=32870494973745590489672618520147226309471918340144695384282781773232645934767374502515598230671247791137527930976112524088008871699095171820372590939626862942685233205917502232958206871013324680536478277482222732171441694215728644181003593159822253865262588786239180084214018426223815623365507773026858373360823443708140883611306592330902932994635589466928339291445175747183614303780853567119243706761062506654051875550472877972022914140455264123266196392819668763698772234814545138902251784786312525104750087996929330185562414784278046396853711290578794116661635417485831969776614488207040722193777994839584756032261597855392326049625939136446894123910001325456787924317732389996265611844035119303216257160839794599676231991399494717047707966247305837634999211674025286390932779274786302608986436901299610556101262201055350271221229232970792516169435984584890424504786508517368097919942459145192988140070892281836785524154439529178872700448867894407573012070924416033111999815571623367458593539025033893379 \end{align*} is a $1007$ digits prime number. Anyone knows another one? (with $n \gt 2$ and $k\gt0$ ) I am also interested in any necessary conditions on $n$ and $k$ for $\sum_{i=1}^n i^{2k}$ to be prime. Clearly neither $n$ nor $n+1$ can be a multiple of $4$ . More generally, if $q$ is an odd prime number such that $q-1$ divides $2k$ , $n$ cannot be $ 0\bmod {q^2} $ otherwise $$\sum_{i=1}^n i^{2k}\equiv \sum_{i\le n ,\mathrm {gcd}(i,q)=1} 1 = n-\lfloor {\frac{n}{q}}\rfloor \equiv 0 \pmod q $$ and cannot be a prime number. Then $n$ cannot be a multiple of $9$ . Also if $q$ is an odd prime number such that $q-1$ divides $2k$ , $n+1$ cannot be $0  \bmod {q^2} $ either, otherwise $$\sum_{i=1}^n i^{2k}=\sum_{i=1}^{n+1} i^{2k}-(n+1)^{2k}\equiv \sum_{i\le n+1 ,\mathrm {gcd}(i,q)=1} 1 = n+1-\lfloor {\frac{n+1}{q}}\rfloor \equiv 0 \pmod q $$ and cannot be a prime number. Then $n+1$ cannot be a multiple of $9$ either. And also if $q$ is an odd prime number such that $q-1$ divides $2k$ , $2n+1$ cannot be $0 \bmod {q^2} $ either, otherwise $$\sum_{i=1}^{n} i^{2k}=\sum_{i=1}^{2n+1} i^{2k}-\sum_{i=n+1}^{2n+1} i^{2k}=\sum_{i=1}^{2n+1} i^{2k}-\sum_{i=0}^{n} (2n+1-i)^{2k}$$ $$\sum_{i=1}^{n} i^{2k} \equiv \sum_{\underset{i\le 2n+1 }{\mathrm {gcd}(i,q)=1}} 1- \sum_{i=1}^{n} i^{2k}\equiv 2n+1-\lfloor {\frac{2n+1}{q}}\rfloor - n +\lfloor {\frac{n}{q}}\rfloor \equiv - n +\lfloor {\frac{n}{q}}\rfloor\pmod q.$$ But neither $n$ nor $n+1$ is multiple of $q$ , otherwise both would be (since $q$ divides $2n+1$ ) which is impossible since $n$ and $n+1$ are coprime. Then $\lfloor {\frac{n}{q}}\rfloor=\lfloor {\frac{n+1}{q}}\rfloor$ , then $$\sum_{i=1}^{n} i^{2k} \equiv  - n +\lfloor {\frac{n}{q}} \rfloor\equiv 2n+1-n+\lfloor {\frac{n+1}{q}} \rfloor\equiv n+1+\lfloor {\frac{n+1}{q}} \rfloor\pmod q,$$ then $$2\sum_{i=1}^{n} i^{2k} \equiv  1 +\lfloor {\frac{n}{q}} \rfloor+\lfloor {\frac{n+1}{q}} \rfloor\pmod q.$$ But $$\frac{2n+1}{q}-1=\lfloor {\frac{2n+1}{q}} \rfloor-1 \le \lfloor {\frac{n}{q}} \rfloor+\lfloor {\frac{n+1}{q}} \rfloor \le \lfloor {\frac{2n+1}{q}} \rfloor= \frac{2n+1}{q} $$ and $$\lfloor {\frac{n}{q}} \rfloor+\lfloor {\frac{n+1}{q}}\rfloor \neq \frac{2n+1}{q} $$ then $$\frac{2n+1}{q}-1= \lfloor {\frac{n}{q}} \rfloor+\lfloor {\frac{n+1}{q}} \rfloor $$ $$2\sum_{i=1}^{n} i^{2k} \equiv  \frac{2n+1}{q}\equiv 0 \pmod q$$ $$\sum_{i=1}^{n} i^{2k} \equiv 0 \pmod q$$ and cannot be a prime number. Then $2n+1$ cannot be a multiple of $9$ either. So we have shown that $n(n+1)(2n+1)$ cannot be divisible by $4$ nor by $9$ and  I can show that a more general necessary condition is that $n(n+1)(2n+1)$ is squarefree. The proof is hereafter in the Appendix. I also wonder whether another necessary condition is that the prime divisors of $2k$ are smaller than or equal to those of $n$ . But we only have $1440=2^5\cdot3^2\cdot 5$ and the Fermat primes to support this. Appendix Let $\mathrm{rad}(n)$ be the product of the distinct prime factors of $n$ .
The purpose of this appendix is to show that when $k\ge1$ , we have $$\sum_{i=1}^n j^{2k} \equiv 0 \bmod {\frac{n(n+1)(2n+1)}{\mathrm{rad} \big(n(n+1)(2n+1)\big)}}$$ whence if $n(n+1)(2n+1)$ is not squarefree, then $\sum_{i=1}^n j^{2k}$ is composite. We first observe that it suffices to show that $S_{2k}(n):=\sum_{j=1}^nj^{2k}$ is divisible by $\frac{n}{\mathrm{rad}(n)}$ . Indeed $n$ and $n+1$ are coprime and $S_{2k}(n)= S_{2k}(n+1)-(n+1)^{2k}$ is then also divisible by $\frac{n+1}{\mathrm{rad}(n+1)}$ , since $(n+1)^{2k}$ is divisible by $(n+1)^{2}$ , hence by $\frac{n+1)}{\mathrm{rad}(n+1)}$ . Also $n$ and $2n+1$ are coprime and $S_{2k}(n)= S_{2k}(2n+1)-\sum_{i=n+1}^{2n+1}i^{2k}=S_{2k}(2n+1)-\sum_{i=0}^{n}(2n+1-i)^{2k}$ , then $$S_{2k}(n)\equiv -\sum_{i=1}^{n}i^{2k}= -S_{2k}(n)\bmod {\frac{2n+1}{\mathrm{rad}(2n+1)}} $$ then $$2S_{2k}(n)\equiv 0\bmod {\frac{2n+1}{\mathrm{rad}(2n+1)}} $$ then $$S_{2k}(n)\equiv 0\bmod {\frac{2n+1}{\mathrm{rad}(2n+1)}} $$ Let $q$ be a prime divisor of $n$ . By @reuns remark in the comment, $\sum_{j=1}^{n} j^{2k}\equiv \frac{n}{q}\sum_{j=1}^{q-1}j^{2k} \bmod q$ and this clearly shows that $\sum_{j=1}^{n} j^{2k}\equiv 0 \bmod q$ when $q^2$ divides $n$ . $\square$","['number-theory', 'elementary-number-theory', 'prime-numbers']"
4122617,How can you change the counts on vertices of a tetrahedron if you increase/decrease the counts on all vertices of a face the same amount?,"Hard question to ask, but here's the idea: I'm creating a game to teach invariants. I want to make a game where each vertex of a platonic solid (we'll start with a tetrahedron) has a counter - all initially set to 0. If you click a face, the counter on each vertex of that face increases by 1. If you right-click a face, the counters on each vertex of that face all decrease by 1. I want to describe all possible counter values attainable in this game. Clearly a necessary condition is that the sum of the counters on the tetrahedron be a multiple of 3, but I don't think that's sufficient. For example, I don't think (0,0,1,2) is possible. What possibilities exist? And what if we start with a different platonic solid? Edit: I think I was wrong in asserting that (0,0,1,2) was impossible. So maybe my necessary condition is sufficient? Is the same true for an octahedron and an icosahedron?","['geometry', 'invariant-theory']"
4122657,A problem of trigonometry (high school),"In a quadrilateral $ABCD$ are known $\overline{AB}=6a'\sqrt3$ , $\overline{AD}=15a'\sqrt2$ and the angles $D\hat{C}A=\pi/4$ , $A\hat{B}C=2\pi/3$ , $A\hat{C}B=\pi/6$ . Calculate the measure of the diagonal $AC$ and the perimeter of the quadrilateral. The solutions of my textbook are: $\overline{AC}=18a'$ and the perimeter is $2p=(12\sqrt 3+36\sqrt 2)a'$ . I put the original Italian question: My synthetic solution : I have drawn the image: We have: $$\frac{b}{\sin \beta}=\frac{c}{\sin \gamma} \implies b=18 a', \quad \frac{a}{\sin \alpha}=\frac{b}{\sin \beta} \implies a=6a'\sqrt 3$$ Hence $c=a=6a'\sqrt 3$ . I know that $\eta=\pi/3$ and $\delta=\pi/4$ . Thus $\psi=5\pi/12=15°$ . Now $$\frac{15a'\sqrt 2}{\sin \delta}=\frac{\overline{DC}}{\sin \psi} \implies \overline{DC}=30a'\cdot \sin(15°)$$ and $$\sin(15°)=\sqrt{\frac{1-\cos(\pi/6)}{2}}=\frac{\sqrt 6 -\sqrt 2}{4}$$ Possibly I will have made some mistake or there is an error in the textbook of an high school but I will never find that perimeter of the solution. Lastly, my female student has forwarded me her solution and she asks me how her could derive from the arcosine $\text{arcsin}=\sin^{-1}$ a non-approximate relation to derive the correct misure of the angle when it is: $$\sin^{-1}\left(\frac 35\right)$$ (see the red rectangles).","['trigonometry', 'solution-verification', 'soft-question', 'education']"
4122708,Infinite product of infinite sums of formal power series: proof?,"Teaching a course on algebraic combinatorics has made me aware of a technical fact about formal power series that is used throughout the subject, but that I have never seen formally stated, let alone proved. Here is a representative particular case of the fact (see below for the general case): Infinite distributive law (positive integers version). An infinite product of infinite sums of formal power series -- e.g., of the form $\prod\limits_{i=1}^\infty \left( p_{i,0} + p_{i,1} + p_{i,2} + \cdots \right)$ , where each $p_{i,j}$ is a formal power series -- can be expanded into an infinite sum as long as certain reasonable conditions hold. Namely, if we assume that $p_{i,0} = 1$ for each $i$ , and if we assume that the family $\left(p_{i,k}\right)_{i\geq 1,\ k\geq 1}$ is summable (i.e., each monomial appears in only finitely many entries of this family), then the product $\prod\limits_{i=1}^\infty \left( p_{i,0} + p_{i,1} + p_{i,2} + \cdots \right)$ converges (in the coefficientwise topology on our ring of formal power series) and equals the sum of the products $p_{1, k_1} p_{2, k_2} p_{3, k_3} \cdots$ over all essentially finite sequences $\left(k_1, k_2, k_3, \ldots\right)$ of nonnegative integers. (""Essentially finite"" means that all but finitely many $i$ satisfy $k_i = 0$ . This is used for proving various standard generating function identities, such as \begin{align}
\prod_{i=1}^\infty \dfrac{1}{1-x^i} = \prod_{i=1}^\infty \left(1 + x^i + x^{2i} + x^{3i} + \cdots\right)
= \sum_{\lambda\text{ is a partition}} x^{\left|\lambda\right|}
\end{align} or \begin{align}
\prod_{i=1}^\infty \dfrac{1}{1-x_it} = \prod_{i=1}^\infty \left(1 + x_it + x_i^2t^2 + x_i^3t^3 + \cdots\right)
= \sum_{n \in \mathbb{N}} h_n t^n
\end{align} (where the latter equality is playing out in the ring of formal power series over the ring of symmetric functions in $x_1, x_2, x_3, \ldots$ , with $h_n$ standing for the $n$ -th complete homogeneous symmetric function). In most simple situations (including the two I just mentioned), it is not hard to forego the use of the infinite distributive law for a limiting argument that reduces the problem to finite products (for which a distributive law isn't too hard to show). However, as one dives deeper into partitions and symmetric functions, these epicycles start getting exhausting. It is clear that the infinite distributive law belongs into the textbooks; yet I have never seen it there. Thus I started writing out a proof for my lecture notes , but so far I have not had much success. Question. Is there a citeable source or a teachable proof for the infinite distributive law? Note that I am looking for a proof that isn't specific to single-variable power series; at least it needs to cover the case of symmetric functions. Ideally, the proof shouldn't be spread over multiple chapters of a treatise or use heavy topological lingo. That said, so far I haven't even found such a proof. Another complication is the fact that not all infinite products are indexed by positive integers. For example, the Cauchy formula in symmetric functions theory is about expanding $\prod\limits_{\left(i,j\right)\in\left\{1,2,3,\ldots\right\}^2} \dfrac{1}{1-x_iy_j}$ . While convergence of nets can make this general case not much harder than the integer-indexed one, I'd prefer not to rely on it too much. Here is the general version of infinite distributivity that I'm really aiming for (of course, there are even more general facts, but this one seems to suffice for my combinatorial needs): Infinite distributive law (general version). Let $K$ be a commutative ring. Let $L$ be the ring of formal power series over $K$ in some set of variables. We equip $L$ with the usual coefficientwise topology. A family $\left(f_j\right)_{j \in J}$ is said to be multipliable if the product $\prod\limits_{j\in J} f_j$ is well-defined, i.e., if for each monomial $\mathfrak{m}$ , there exists a finite subset $K$ of $J$ such that the $\mathfrak{m}$ -coefficient of $\prod\limits_{j\in K} f_j$ does not change if we increase $K$ (that is, the $\mathfrak{m}$ -coefficient of $\prod\limits_{j\in K} f_j$ equals the $\mathfrak{m}$ -coefficient of $\prod\limits_{j\in K'} f_j$ for any finite subset $K'$ of $J$ with $K \subseteq K'$ ). The notion of a summable family is defined similarly (but can also be characterized in a simpler way: a family $\left(f_j\right)_{j \in J}$ is summable if and only if each monomial $\mathfrak{m}$ occurs in only finitely many of its entries). Let $I$ be a set. For any $i\in I$ , let $S_i$ be a set that contains the number $0$ . Set \begin{align}
\overline{S} = \left\{  \left(  i,k\right)  \ \mid\ i\in I\text{ and }k\in S_i\text{ and }k\neq0\right\}  .
\end{align} For any $i\in I$ and any $k\in S_i$ , let $p_{i,k}$ be an element of $L$ . Assume that \begin{equation}
p_{i,0}=1\ \ \ \ \ \ \ \ \ \ \text{for any }i\in I.
\end{equation} Assume further that the family $\left(  p_{i,k}\right)  _{\left(  i,k\right) \in\overline{S}}$ is summable. Then, the product $\prod\limits_{i\in I}\ \ \sum\limits_{k\in S_i}p_{i,k}$ is well-defined (i.e., the family $\left(  p_{i,k}\right)_{k\in S_i}$ is summable for each $i\in I$ , and the family $\left( \sum\limits_{k\in S_i}p_{i,k}\right)  _{i\in I}$ is multipliable), and we have \begin{equation}
\prod\limits_{i\in I}\ \ \sum\limits_{k\in S_i}p_{i,k}=\sum\limits_{\substack{\left(
k_{i}\right)  _{i\in I}\in\prod\limits_{i\in I}S_i\\\text{is essentially finite}
}}\ \ \prod\limits_{i\in I}p_{i,k_{i}}.
\end{equation} Here, a family $\left(k_{i}\right)  _{i\in I}\in\prod\limits_{i\in I}S_i$ is said to be essentially finite if all but finitely many $i \in I$ satisfy $k_i = 0$ . In particular, the family $\left(  \prod\limits_{i\in I}p_{i,k_{i}}\right)  _{\left(
k_{i}\right)  _{i\in I}\in\prod\limits_{i\in I}S_i\text{ is essentially finite}}$ is summable.","['formal-power-series', 'topological-vector-spaces', 'algebraic-combinatorics', 'sequences-and-series']"
4122737,"If $\int_0^1 x^n d\mu = 0$ for $n = 0, 1, 2, 3, ...$ then $\mu = 0$","I am trying to solve this problem: Let $\mu$ be a finite, Borel measure on $[0, 1]$ and suppose that $$\int_0^1 x^n d\mu = 0$$ for $n = 0, 1, 2, 3, ...$ . Show that $\mu = 0.$ I assume that the question allows $\mu$ to be signed, otherwise the problem is trivial. One potential strategy is clear: show that $\mu(I) = 0$ for any open interval $I \subseteq [0, 1],$ which implies that the measure of open set is zero. Then by outer regularity, we get that $\mu = 0.$ However, I haven't been able to show that $\mu(I) = 0.$ I tried using the Jordan Decomposition Theorem to write $\mu = \rho_+ - \rho_-$ with $\rho_+ \perp \rho_-$ , but no luck. Thank you very much for any help.","['integration', 'measure-theory', 'analysis']"
4122739,Problems solving the SDE $dX_t = aX_tdt +\sigma dB_t$. Why don't Ito's lemma work?,"I am trying to solve the SDE below and I am running into some problems. We're given that $X_0=1$ , $a \in R$ , $\sigma > 0.$ $$dX_t = aX_tdt +\sigma dB_t$$ I tried to solve it the following manner. Assuming that $X_t=f(t, B_t)$ and applying Ito's lemma yielded: $$(\frac{\partial f}{\partial t} + \frac{1}{2}\frac{\partial f^2}{\partial^2 x})dt + \frac{\partial f}{\partial x}dB_t$$ Now here is where I want to match ""coefficients"". That is saying $$\frac{\partial f}{\partial x} = \sigma  \implies f=\sigma x + g(t)$$ Entering this into the other ""coefficent"" yields: $$\frac{\partial f}{\partial t} + \frac{1}{2}\frac{\partial f^2}{\partial^2 x} = g'(t) = aX_t = af=a(\sigma x+g(t))$$ Now we've arrived at a seperable ODE for $g(t)$ which when solved would yield the final solution $f(t, B_t)$ . However, this doesn't work, and looking at the solution set I am completely off target. My question is why is this? Why can't I solve this SDE using this method? Where is the flaw in the solution method? The suggested solution method was to study $Y_t= e^{-at}X_t$ . How on earth would you come up with this anstaz by looking purely at the SDE? EDIT (completed tried solution): After having said that $f= \sigma x + g(t)$ I used this fact to obtain that: $$\frac{\partial f}{\partial t} + \frac{1}{2}\frac{\partial f^2}{\partial^2 x}=\frac{\partial f}{\partial t} =g'(t)=a(\sigma x  +g(t))$$ Now I tried to solve that ODE by the follwing processes: $$g'(t)=a(\sigma x+g(t))\iff \int \frac{dg}{a(\sigma x + g(t))} = t + C$$ This yielded further that $g(t)=Ce^{at}-\sigma x$ and so $f=\sigma x + g(t) = Ce^{at}$ . This clearly isn't the solution however.","['stochastic-integrals', 'probability', 'stochastic-processes', 'stochastic-differential-equations', 'stochastic-calculus']"
4122747,Need condition which will make functions both true,"Let $f:X\mapsto Y$ Let $A \subset X, B \subset  Y$ What condition on $f$ can make $A=f^{-1}(f[A])$ . and $B=f(f^{-1}[B])$ both true? For the first ,its $f$ being 1-1, the second
it’s $f$ being onto. But both I am somewhat
perplexed . I know both can be solved separately. The info is on MSE. The only other condition I can think to make it work on both if $f$ is a bijection. If I assume $f$ is a bijection,I have to show $f$ is invertible for both ,correct? Please don’t Downvote me if it’s a duplicate.I checked and
couldn’t find it Help?","['proof-explanation', 'advice', 'functions']"
4122803,how to prove that $1^x+2^x+3^x+4^x+\cdots+N^x$ will never sum to a prime number except $1^x+2^x$?,"I am a a web developer programming in PhP which is limited to large calculations, but running a quick script shows that $1^x+2^x+3^x+4^x+\cdots+N^x$ can never sum to a prime number unless in the case of $1^x+2^x$ , such as in the cases of $x=1$ and $x=2$ where $1^1+2^1=3$ and $1^2+2^2=5$ . As a self learner, I am currently refreshing my learning in Algebra 2 (before moving on), and sometimes my mind wonders into questions that I just can't find the answers to (mostly because I am not familiar with the concerned topics). I tried finding an answer but if this is a duplicate with a relevant answer, please close and refer me to it. How to prove (if possible) that $1^x+2^x+3^x+4^x+\cdots+N^x$ will never be the sum of a prime number, unless in the case of $1^x+2^x$ , such as in the cases of $x=1$ and $x=2$ where $1^1+2^1=3$ and $1^2+2^2=5$ ? Edit: $x$ and $N$ are positive integers I appreciate any answers even if it is just a hint or a reference.","['proof-explanation', 'bernoulli-numbers', 'algebra-precalculus', 'stirling-numbers', 'prime-numbers']"
4122814,"Bijections $f: \mathcal{P}(S) \to \{0,1\}^S$","I'm trying to understand my professor's lecture notes, but I can't tell whether he's defining two bijective functions in both directions or two injective maps and invoking Shroder Bernstein. Let $S$ be a set, $\mathcal{P}(S)$ its power set, and $\{0,1\}^S$ the set of functions from $S \to \{0,1\}$ . Define two maps: \begin{align*}
& f \colon \mathcal{P}(S) \to \{0,1\}^S, \; A \mapsto I_A \\
& g \colon \{0,1\}^S \to \mathcal{P}(S), \; f \to f^{-1} (1),
\end{align*} where $I_A$ is the indicator function that sends $x \in A$ to $1$ and $x \not \in A$ to $0$ . I believe both $f$ and $g$ are bijective. We show $f$ is bijective. Given a function, call it $h: S \to \{0,1\}$ . define $A = h^{-1} (1)$ . It's clear that $h = I_A$ ; indeed, for $x \in S$ , we have $h(x) = I_A (x) = 1$ and for $x \not \in S$ , we have $h(x) = I_A (x) = 0$ , so $h(x) = I_A (x)$ for all $x \in S$ , so $f$ is surjective. Second, given $A,B$ for which $f(A) = f(B)$ , we have $I_A = I_B$ . I claim $A = B$ . If $x \in A$ , then we have $I_A (x) = I_B (x) = 1$ , so $x \in B$ . Similarly, if $x \in B$ , we have $I_A (x) = I_B (x) = 1$ , so $x \in A$ , so $A = B$ . So $f$ is injective, and hence bijective. Now for $g$ . We show that $g$ is bijective. Let $A \in \mathcal{P}(S)$ . Then $g\left(I_A\right) = A$ , so $g$ is surjective. If $f,g \in \{0,1\}^S$ have the property that $f^{-1} (1) = g^{-1} (1)$ , then $f^{-1} (0) = g^{-1} (0)$ since the domain of $f,g$ is $\{0,1\}$ , so $f = g$ , so $g$ is bijective. I can't tell if $f$ and $g$ are inverses of each other, though they both seem to be bijective.","['functions', 'solution-verification']"
4122851,Defining curvature via osculating circles,"I am trying to figure out a geometrically accessible definition for the curvature of a smooth plane curve $c:I \to \mathbb{R}^2$ where $I$ is an interval and $c' \neq 0$ everywhere. My plan is to define the curvature via the osculating circle so that the definition might take the following form: if the osculating circle at a point $c(t_0)$ exists and has radius $r$ , define the curvature as $\kappa(t_0):=1/r$ . If the circle doesn't exist (for example, if $c$ is a straight line), define $\kappa(t_0):=0$ . Moreover, I want to do all this without assuming that $c$ is a unit-speed parametrization. Question: Does anyone know how such a definition of the curvature could look like in mathematically precise terms? It is important that the definition is mathematically precise. I also plan to compare the above definition with the more common definition which uses the Frenet frame and show that both definitions yield the same function $\kappa:I \to \mathbb{R}$ up to sign. So, a robust definition is wanted. My attempts and problems: First of all, we need a precise definition of the osculating circle and a precise and useful criterion when it exists. Here is where I already stuck. I tried to work with something like this: For $h>0$ , let $M(t_0,h)$ the center of the circle through the three points $c(t_0-h)$ , $c(t_0)$ and $c(t_0+h)$ , presupposed they are not located on a straight line. The osculating circle at $c(t_0)$ could then be ""defined"" as the circle with center $M$ and radius $r$ where $$M:= \lim_{h \to 0} M(t_0,h),\phantom{aaa}r:=\Vert M-c(t_0)\Vert$$ In order for this definition to work, we must ensure that for sufficiently small $h>0$ , the three points $c(t_0 \pm h), c(t_0)$ are not collinear. Otherwise, the points $M(t_0,h)$ do not exist as $h \to 0$ and it makes no sense to talk about their limit. So, we may at least require some condition like this: $\phantom{aa}$ There is a sequence $(h_n)_{n\in \mathbb{N}}$ with $h_n \to 0$ and all $h_n >0$ such that for each $n$ , the points $\phantom{aai}$$c(t_0 \pm h_n), c(t_0)$ are not collinear. But using only this condition, I have no idea how to prove that the limit $\lim_{h \to 0} M(t_0,h)$ exists which is also necessary in order to show that the definition makes sense. Needless to say that all this should be done without referring to the curvature since I want to use the osculating circles to define the curvature and avoid circularity. Remark: During the past days, I already posted some related questions. However, they might have been unclear or to narrowly focused on particular details so that answers didn't really help me. (However, thank you for all the answers.) Some users suggested that I formulate a new question to put all this into a broader context so that the people here get a better idea of what I am trying to achieve and why I have all the questions. I hope this post clarifies the things a bit.","['curves', 'osculating-circle', 'curvature', 'differential-geometry']"
4122881,"Show that there exists a unique weak solution $(u, p)$ such that $-\Delta u + p = f$","Let $\Omega\subseteq\mathbb{R}^n$ be a bounded domain and $f\in L^2(\Omega)$ . I would like to show that there exists a unique pair $(u, p)$ with $u \in H_0^1(\Omega)$ and $p\in \mathbb{R}$ such that $$
\int_\Omega \nabla u\cdot \nabla v + p\int_\Omega v = \int_\Omega fv \qquad\text{and}\quad\quad \int_\Omega u = 0
$$ for all $v\in C_c^\infty(\Omega)$ . If we fix $p$ and do not require that $\int_\Omega u = 0$ , then this problem is a straightforward problem that can be solved using well-known methods in PDE. My idea was therefore to show that we must have $p = \int_\Omega f$ . This would be easy to show by setting $v=1$ if it were not for the requirement $v\in C_c^\infty(\Omega)$ . Then, for this choice of $p$ I can find a solution $u\in H_0^1(\Omega)$ such that $$
\int_\Omega \nabla u\cdot \nabla v + p\int_\Omega v = \int_\Omega fv
$$ for all $v\in C_c^\infty(\Omega)$ .
I was then hoping to conclude that the condition $\int_\Omega u=0$ must also be satisfied. One possible approach is to consider $H_0^1(\Omega)\times \mathbb{R}$ as a Hilbert space and apply the Lax-Milgram theorem to an appropriate choice of bilinear map and bounded linear functional. More specifically, I consider the bilinear form $$
B((u,p), (v,q)) = \int_\Omega fv + p\int_\Omega v - q\int_\Omega u + pq
$$ and the linear function $$
L((v,q)) = \int_\Omega f(v-q).
$$ Here, $u,v\in H_0^1(\Omega)$ and $p,q\in \mathbb{R}$ . I can then conclude that there exists a unique pair $(u, p)$ satisfying $$
\int_\Omega fv + p\int_\Omega v - q\int_\Omega u + pq
=\int_\Omega f(v-q)
$$ for all $v\in H_0^1(\Omega)$ and $q\in \mathbb{R}$ .  In particular, setting $q=0$ we see that $$
\int_\Omega fv + p\int_\Omega v
=\int_\Omega fv
$$ as desired. Furthermore, ranging $q\in \mathbb{R}$ I can conclude that $$
\int_\Omega u = p - \int_\Omega f.
$$ However, I cannot show that $\int_\Omega u = 0$ .","['sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4122937,"Express $\mathrm{Tr}(X)$ in terms of $A$, given that $X=A^TX(I+X)^{-1}A$","Given real non-singular $n\times n$ matrix $A$ , with all eigenvalues larger that $1$ . Express $\mathrm{Tr}(X)$ in terms of $A$ , given that $X=A^TX(I+X)^{-1}A$ . $\quad$ ( $X$ is sym. pos. def.) It is allowed to assume that $A$ is in any special form that can be obtained using similarity transformation, i.e. $A$ can be changed by $\hat{A}$ , if $A=P^{-1}\hat{A}P$ , for some nonsingular $P$ . My attempt: For symmetric $A$ case, WLOG we can assume $A$ is diagonal, then \begin{align}
X&=AX(I+X)^{-1}A\\
AX^{-1}AX&=I+X\\
AYA&=Y+I\\
(A\otimes A-I)\mathrm{vec}(Y)&=\mathrm{vec}(I)\\
\mathrm{vec}(Y)&=(A\otimes A-I)^{-1}\mathrm{vec}(I),
\end{align} where $Y=X^{-1}$ and $\otimes$ denotes Kronecker product . If $\mathrm{vec}(Y)=(A\otimes A-I)^{-1}\mathrm{vec}(I)$ , then $\mathrm{vec}(X)=(A\otimes A-I)\mathrm{vec}(I)$ , using this result . From last eq. it is easy to see that $\mathrm{Tr}(X)=\sum_{i=1}^na^2_i-n$ , where $a_i$ are diagonal elements of $A$ . Assume that all eigenvalues of $(A)$ are equal to $\lambda$ and that Jordan canonical form of $A$ consists of single Jordan block $J$ . Then \begin{align}
\mathrm{vec}(Y)&=(A\otimes A-I)^{-1}\mathrm{vec}(I)\\
&=\begin{bmatrix} 
\lambda J-I & J &  &&  \\ 
&\lambda J-I&J& &\\
 &  & \ddots & \ddots& \\
&  &  & \lambda J-I & J \\
& &  &  & \lambda J-I
\end{bmatrix}^{-1}\mathrm{vec}(I)\\
&=\begin{bmatrix} 
M & -MJM & MJMJM & \dots & (-1)^{n-1}MJM\cdots M \\ 
 & \ddots & \ddots & \ddots & \ddots\\
  &  & M & -MJM & MJMJM\\
&  &  & M& -MJM\\
&&&&M
\end{bmatrix}\mathrm{vec}(I)\\
&=\begin{bmatrix} 
TJ^{-1} & -T^2J^{-1} & T^3J^{-1} & \dots & (-1)^{n-1}T^nJ^{-1} \\ 
 & \ddots & \ddots & \ddots & \ddots\\
  &  & TJ^{-1} & -T^2J^{-1} & T^3J^{-1}\\
&  &  & TJ^{-1}& -T^2J^{-1}\\
&&&&TJ^{-1}
\end{bmatrix}\mathrm{vec}(I)
\end{align} where $M=(\lambda J-I)^{-1}$ and $T=MJ=(\lambda I-J^{-1})^{-1}$ . Second last equation is obtained using Block matrix inversion formula . I am not sure what to do next, the expression looks too complicated. Maybe there is an easier way to do it? EDIT: $\qquad$ For $A=\begin{bmatrix} 
a_1 & 1\\ 
 0&a_2
\end{bmatrix}$ , we will get $\mathrm{Tr}(X)=(a_1^2+a_2^2-2)+\frac{(a_1^2-1)(a_2^2-1)}{(a_1a_2-1)^2+1}$ . $\qquad$ While For $A=\begin{bmatrix} 
a_1 & 0\\ 
 0&a_2
\end{bmatrix}$ , we will get $\mathrm{Tr}(X)=(a_1^2+a_2^2-2)$ . The original problem is equivalent to the following problem: Find $\mathrm{Tr}(Z)$ , where $A'Z^{-1}A+Z=A'A+I$ and $Z-I>0.$",['linear-algebra']
4122950,Sequence of Nested Projections in an arbitrary Normed Linear Space Converges to the Identity,"I have seen similar questions on this site, most notably this one: Convergence of projections onto a nested sequence of subspaces of a Hilbert space , but they all include the Hilbert space assumption. The following, from Cheney's Analysis for Applied Mathematics , is more general: Let $P_1, P_2, \cdots$ be a sequence of projections on a normed space $X$ . Suppose that $P_{n+1} P_n = P_n$ for all $n$ and that the union
of the ranges of these projections is dense in $X$ . Suppose further
that $\sup_n \|P_n \| < \infty$ . Prove that $P_n x \rightarrow x,
> \forall x \in X.$ [Problem 4.4.3, p.197.] Thus, $X$ is not even assumed to be a Banach space. These are my thoughts on this: First, let $V_n$ be the range of each $P_n$ . Thus, $X = \overline{ \bigcup _{n=1}^\infty V_n}.$ It is easy to demonstrate that $V_i \subset V_j, j>i:$ $$Let \, \, v_i \in V_i \Rightarrow P_iv_i =v_i.\,\, Then\,\, P_{i+1}(P_i(v_i))=P_iv_i = v_i \Rightarrow v_i \in R(P_{i+1})=V_{i+1}.$$ It is also easy to show that if $x \in V_n \Rightarrow, \exists N,\,s.t.\, P_{N \geq n}x =x.$ Thus, if $x\in \bigcup _{n=1}^\infty V_n \subset X$ , then as $n \rightarrow \infty$ , $\exists N$ such that $P_nx=x, \forall n \geq N.$ But what if $x \in X \setminus \bigcup _{n=1}^\infty V_n $ ? I understand that in that case, $x$ would be a limit point of the union of the ranges, due to $ \bigcup _{n=1}^\infty V_n$ being dense in $X$ . I cannot push it much further from here though. I'd appreciate any help or suggestions for better approaches.","['alternative-proof', 'proof-explanation', 'projection', 'functional-analysis']"
4122998,"Given inradius and half of one side, need to find the area of Triangle","△ABC is a traingle, where O is the incentre and OD is perpendicular to AB. By definition, <OAC = < OAD = θ and < OBC = < OBD =  α. Given OD = 3 and BD = 4. The problem is to find the area of △ABC. My approach : We know the area of a triangle is inradius (r) multiplied with the semicentre (s). So, need to find s. Better to get hold of the sides. Construct the necessary lines to get the following diagram. We already have r = 3. Now, s would be s = {(a+b)+(a+4)+(b+4)}/2 = a+b+4. Now, tan(α) = 3/4. Also, a = 3 tan(θ) and b = 3 tan(π/2-α-θ) = 3 cot(α+θ) = 3/tan(α+θ). Then, s = a+b+4 = 3 tan(θ) + 3/tan(arctan(3/4)+θ) No progress furthermore. Can anybody suggest any lead? Or maybe some other way to solve the problem.","['triangles', 'trigonometry', 'angle', 'geometry']"
4123029,Finding polynomial to the power of 2020,"I am trying to solve a homework problem, but I am stuck at a point where I don't know what am I suppose to do next. We're given a $3 \times 3$ matrix $$A =
\begin{pmatrix}1& 2& 2\\
2& 1& 2\\
2& 2& 1\end{pmatrix}$$ And we have to find a polynomial $p(x)$ such that deg( $p(x)$ ) = 2020, and $p(A) = 0$ (as 0 matrix 3x3) What I did was finding the characteristic polynomial which is $p(x) = (x-5)(x+1)^2$ And I know that if I use Cayley Hamilton I can place A in the characteristic polynomial and get the zero matrix. but what is that part with the degree 2020, I don't understand how do I do that or what do I rely on? This might seem easy but I really can't see it. any help is appreciated :) Thank you","['matrices', 'cayley-hamilton', 'linear-algebra', 'characteristic-polynomial']"
4123040,Ambiguity in deciding upper and lower limits of definite integration while doing substitutions,"$
I = \int_{0}^{\infty} \frac{x}{({x^2 + 1)(1+x)}} \,\mathrm{d}x
$ Let $x=\tan(\theta)$ gives $\mathrm{d}\theta= \frac{\mathrm{d}x}{({x^2 + 1)}}$ $I= \int_0^{\pi/2} \frac{\tan \theta}{1 + \tan \theta} \,\mathrm{d}\theta 
$ $= \int_0^{\pi/2} \frac{\tan (\pi/2-\theta)}{1 + \tan(\pi/2-\theta)} \,\mathrm{d}\theta$ $ =\int_0^{\pi/2} \frac{1}{1 + \tan \theta} \,\mathrm{d}\theta$ $1^{st} $ and $3^{rd}$ gives $2I= \int_0^{\pi/2}  \,\mathrm{d}\theta \rightarrow I=\frac{\pi}{4}
$ Main doubt: Why can't we take $ \{ x \rightarrow \infty \} $ in upper limit of integration as $\{ \tan(\theta) \rightarrow 3 \pi / 2 \} $ ? That is $I= \int_0^{3\pi/2} \frac{\tan \theta}{1 + \tan \theta} \,\mathrm{d}\theta $ $= \int_0^{3\pi/2} \frac{\tan (3\pi/2-\theta)}{1 + \tan(3\pi/2-\theta)} \,\mathrm{d}\theta$ $ =\int_0^{3\pi/2} \frac{1}{1 + \tan \theta} \,\mathrm{d}\theta$ $1^{st} $ and $3^{rd}$ gives $2I= \int_0^{3\pi/2}  \,\mathrm{d}\theta \rightarrow I=\frac{3\pi}{4}
$ which yeilds different answer. Above Question is just an example for the Main doubt I have asked. The below Question is closely related to and subset of my Question (which I do not knew while posting my question). So I am linking it for future readers. Why doesn't trig substitution work for definite integrals?","['integration', 'definite-integrals', 'fake-proofs', 'calculus', 'trigonometry']"
4123043,About the stability of cycles for the logistic map $x_{n+1}=rx_n(1-x_n)$,"Please indicate a reference (if there exists any) proving that when a cycle appears (i.e. for the minimal value of the logistic parameter $r$ for which a $k$ -cycle exists) we also have, ""immediately"", stability of this/these cycle(s). Whenever I read about this, it appears a ""known"" fact, or it is noticed as being ""expected"". Also, is there a theoretical result which proves that if there exist several $k$ -cycles for a given $r$ there is at most one that is stable?","['bifurcation', 'chaos-theory', 'recurrence-relations', 'discrete-mathematics']"
4123074,Pushforward of a $\sigma$-finite measure,Let $\phi: X \rightarrow Y$ be a measurable function between measurable spaces. Let $\mu$ be a $\sigma$ - finite measure on $X$ . In general I have seen that the push forward measure $\phi_*\mu$ need not be $\sigma$ - finite. What are assumptions we need to make $\phi_*\mu$ also $\sigma$ - finite? I saw a remark that if $\phi$ is injective and $\mu$ is non atomic then $\phi_*\mu$ is $\sigma$ - finite. I couldn't prove it. Do we need any more assumption?,['measure-theory']
4123091,Bit strings and probability,"Given a bit string of length $n$ , I should develop a probabilistic algorithm that answers one of the following questions: Does the bit string have more zeros than ones? Does the bit string have more ones than zeros? Does the number of zeros (/ones respectively) lies between $0.4n$ and $0.6n$ ? The probability that the answer is correct should be at least $0.99$ . Notice that the algorithm has to answer only one of the three questions for a given bit string, and it does not always have to be the same question it answers. The algorithm should run in $O(1)$ . Generally, my method would be to choose some sample set of $k$ bits of the bit string  at random and then approximate the ratio with this sample set, which yields the desired probability when choosing the size accordingly. But since the runtime constraint is that tight, I don't really think that I could proceed with this method. If I could somehow get a bound for the ratio that doesn't depend on $n$ , I could get a constant runtime, but I don't really know how to do this. I thought about letting the ratio be $0.5$ which would be the expected ratio for a randomly chosen bit string, but I don't think that this approach would be valid. Any ideas how one could tackle this problem? Edit: I am not allowed to use the normal distribution, the problem is solvable without using it.","['bit-strings', 'probability-theory', 'probability', 'algorithms']"
4123102,Let $0<a<b<1$ and $E=\log_{a}{\frac{2ab}{a+b}} + \log_{b}{\frac{2ab}{a+b}}$ Prove that $E>2$,"Let $0<a<b<1$ and $E=\log_{a}{\frac{2ab}{a+b}} + \log_{b}{\frac{2ab}{a+b}}$ Prove that $E>2$ I tried to change both log bases to $\frac{2ab}{a+b}$ and I got $E=\frac{1}{\log_{\frac{2ab}{a+b}}{a}} + \frac{1}{\log_{\frac{2ab}{a+b}}{b}}$ . Also I noticed that $\frac{2ab}{a+b} < 1$ because $\frac{2}{\frac{1}{a} + \frac{1}{b}} = \frac{2ab}{a+b}$ which is the harmonic mean of $a$ and $b$ and since harmonic mean is always smaller than $max(a,b)$ , $\frac{2ab}{a+b}$ will be smaller than 1. Since both the base and the argument of $\log_{\frac{2ab}{a+b}}{a}$ and $\log_{\frac{2ab}{a+b}}{b}$ are smaller than 1, the logarithms will be positive, but I do not know if they will be greater than 1 or smaller than 1. Any tips ? Thanks in  advance !","['algebra-precalculus', 'logarithms', 'a.m.-g.m.-inequality', 'inequality']"
4123138,Proving monotonicity of a function $g$,"Let $r\ge 1$ . For $r-1\leq x \le r+1$ we define $f(x)=\arccos\left(\frac{x^2 + r^2 - 1}{2 r x}\right)$ . Now let $g:[1,\infty)\to\mathbb{R}$ be given by $$g(r)=\frac{\int_{r-1}^{r+1}{r(f(x))^2\,dx}}{\int_{r-1}^{r+1}{ f(x)\,dx}}.$$ I want to show that $g$ is a monotonically decreasing function. Notes Numerically, this looks to be almost certainly true. I have already proved that $g$ has some interesting properties; for instance $g(1)=\pi-2$ and $\lim_{r\to \infty} g(r)=\frac{8}{3\pi}$ . But ideally I'd like to show that the function decreases monotonically between these two values. I've tried looking at the derivative - but it seems too ghastly to be useful!","['calculus', 'monotone-functions', 'real-analysis']"
4123278,"Power set cardinality exercise, where am I wrong?","Exercise. Find the cardinality. $|P(P(P(A\times\varnothing)))|$ My solution 1: $|P(P(P(A\times\varnothing)))|=2^{|P(P(A\times\varnothing))|}=2^{2^{|P(A\times \varnothing)|}}=2^{2^{2^{|A\times\varnothing|}}}=2^{2^{2^{|\varnothing|}}}= 2^{2^{2^0}}=2^2=4$ My solution 2: $|P(P(P(A\times\varnothing)))|=|P(P(P(\varnothing)))|=|P(P(\{\varnothing \}))|=|P(\{\varnothing,\{\varnothing\} \})|=|\{\varnothing, \{\varnothing\},\{\{\varnothing\} \}, \{\varnothing, \{\varnothing\} \}    \}|=4$ But I got it wrong in my homework. Where am I wrong?",['elementary-set-theory']
4123285,Verification of my solution of the functional equation $ f \left( x + y ^ 2 \right) = f ( x ) +f ( y ) ^ 2 $,"Find all functions $ f : \mathbb R \to \mathbb R $ satisfying $$ f \left( x + y ^ 2 \right) = f ( x ) +f ( y ) ^ 2 $$ for all $ x , y \in \mathbb R $ . Attemp: $ f $ is non-decreasing: In fact, $ f ( y ) ^ 2 \ge 0 $ , so $ f ( x + \epsilon ) \ge f ( x ) $ for all $ \epsilon > 0 $ . $ f ( 0 ) = 0 $ : Just set $ x = y = 0 $ . $ f \left( y ^ 2 \right) = f ( y ) ^ 2 $ , for all real $ y $ : Just set $ x = 0 $ . $ f $ is odd: Setting $ x = - y ^ 2 $ , we have $ f \left( - y ^ 2 \right) + f ( y ) ^ 2 = f \left( y ^ 2 \right) + f \left( - y ^ 2 \right) = 0 $ . So $ f ( r ) = - f ( - r ) $ for every real $ r $ . $ f $ satisfies the Cauchy functional equation: From the previous items, it's easy to see that $ f ( x + a ) = f ( x ) + f ( a ) $ and $ f ( x - a ) = f ( x ) + f ( - a ) $ for all non-negative reals $ a $ and all reals $ x $ . $ f $ is increasing or $ f \equiv 0 $ : Suppose that $ 0 $ is not the only solution of $ f ( x ) = 0 $ and thus $ f ( \epsilon ) = 0 $ for some $ \epsilon > 0 $ . from $ f $ satisfying the Cauchy equation we have that $ f ( q ) = f ( 1 ) q $ for any rational $ q $ . In addition, $ f \left( 1 ^ 2 \right) = f ( 1 ) ^ 2 $ , so that $ f ( 1 ) = 1 $ or $ f ( 1 ) = 0 $ . If $ f ( 1 ) = 0 $ we have $ f ( q ) = 0 $ for all rational $ q $ , and this with the condition of being non-decreasing would guarantee that $ f ( x ) = 0 $ for all real $ x $ . If $ f ( 1 ) = 1 $ take rationals $ \alpha $ and $ \beta $ with $ \alpha < \epsilon < \beta $ . We have $ 0 < \alpha \le f ( \epsilon ) \le \beta $ , which is absurd, so $ f (x) = 0 $ if and only if $ x = 0 $ . In conclusion, if $ f \ne 0 $ , $ f ( y ) ^ 2 = f \left( y ^ 2 \right) > 0 $ holds for all non-zero $ y $ and $ f ( x + \epsilon ) > f ( x ) $ holds for all real $ x $ and $ \epsilon > 0 $ . These conditions together with the Cauchy equation guarantee that in the non-zero case the solution is $ f ( x ) = x $ for all real $ x $ . Answer: $ f = 0 $ or $ f ( x ) = x $ . Am I right?","['functional-equations', 'functions', 'solution-verification']"
4123421,"Complex analysis - Finding $\int_0^\infty \frac{x\cos\left(\frac{1}{x^2}\right)}{x^4 + 4}\,dx$","I want to use complex analysis to solve this integral: $$ I = \int_0^\infty \frac{x\cos\left(\frac{1}{x^2}\right)}{x^4 + 4}\,dx$$ I'm having trouble because I get two different results with two different methods. First of all, I applied the change of variables $\displaystyle y = \frac{2}{x^2}$ to get that $\displaystyle I = \frac{1}{4} \int_0^\infty \frac{\cos(\frac{y}{2})}{y^2 + 1}dy = \frac{1}{4} \Re \left( \int_0^\infty \frac{e^{iy/2}}{y^2 + 1}dy \right)$ which is a well known integral and is equal to $\displaystyle \frac{\pi}{2\sqrt e}$ . So $\displaystyle I = \frac{\pi}{8\sqrt e}$ . This is the correct result. (For those who don't know, the integral can be solved using Feynman's Trick or Jordan's Lemma with a suitable contour in the upper half plane.) Now, let's say that I want to solve the integral without making any change of variables or others tricks. I just want to solve it using the residue theorem with a quarter of circle of radius $\displaystyle R \to \infty$ and center $\displaystyle C = (0,0)$ in the first quadrant as a contour. Let's call this contour $\displaystyle \gamma$ . So I want to find $\displaystyle J =\oint_\gamma \frac{z\cos(\frac{1}{z^2})}{z^4 + 4}dz$ and how it is related to I. Let's find J. $\displaystyle J =\int_0^\infty \frac{z\cos(\frac{1}{z^2})}{z^4 + 4}dz + \int_{Arc} \frac{z\cos(\frac{1}{z^2})}{z^4 + 4}dz + \int_{i\infty}^{i0} \frac{z\cos(\frac{1}{z^2})}{z^4 + 4}dz$ , where Arc is parametrized by $\displaystyle z(\theta) = R e^{i\theta}$ for $\displaystyle \theta \in \left [0,\frac{\pi}{2} \right]$ . The first integral is obviously I and so is the third ( use the change of variables $\displaystyle z \to -iz$ ). Let's call the second integral $\displaystyle I_{Arc}$ . EDIT: After a comment I decided to work on the third piece: $\displaystyle Q = \int_{i\infty}^{i0} \frac{z\cos(\frac{1}{z^2})}{z^4 + 4}dz$ using the change of variables $\displaystyle q = -iz$ $\displaystyle Q = \int_{\infty}^{0} \frac{iq\cos\left(\frac{1}{(iq)^2}\right)}{(iq)^4 + 4}idq = -\int_{\infty}^{0} \frac{q\cos\left(\frac{-1}{q^2}\right)}{(iq)^4 + 4}dq = \int_{0}^{\infty} \frac{q\cos\left(\frac{1}{q^2}\right)}{q^4 + 4}dq = I $ Now, I would like to show that $\displaystyle I_{Arc} \to 0$ as $\displaystyle R \to \infty$ . $\displaystyle \mid I_{Arc} \mid = \left| \int_0^{\frac{\pi}{2}} \frac{R e^{i\theta} \cos\left(\frac{1}{R^2 e^{i2\theta}}\right)}{R^4 e^{i4\theta} + 4} R e^{i\theta} i d\theta \right| \leq
\int_0^{\frac{\pi}{2}} \left|  \frac{R^2 \cos\left(\frac{1}{R^2 e^{i2\theta}}\right)}{R^4 e^{i4\theta} + 4} \right| d\theta \leq \frac{3R^2}{R^3} \int_0^{\frac{\pi}{2}} d\theta = \frac{1}{R}\frac{3\pi}{2} \to 0$ as $\displaystyle R \to \infty$ Where I used: $\displaystyle \left| \cos\left(\frac{1}{R^2 e^{i2\theta}}\right) \right| = 
\left| \frac{ \exp\left(\frac{i\cos(2\theta) + \sin(2\theta)}{R^2} \right) + \exp\left(\frac{-i\cos(2\theta) - \sin(2\theta)}{R^2} \right) }{2} \right| \leq
\left| \frac{ \exp\left(\frac{\sin(2\theta)}{R^2} \right)}{2} \right| + \left| \frac{ \exp\left(\frac{-\sin(2\theta)}{R^2} \right)}{2} \right| \leq \exp\left(\frac{\sin(2\theta)}{R^2} \right) \leq 3$ since R is definitely bigger than 1. Also $\displaystyle \mid R^4e^{i4\theta} + 4 \mid = \sqrt{(R^4\cos4\theta + 4)^2 + R^4\sin^24\theta} = \sqrt{R^8 + 16 + 8R^4\cos4\theta} \ge \sqrt{R^8 + 8R^4\cos4\theta} \ge \sqrt{R^8 - 8R^4} \ge R^3$ . Which is true if $\displaystyle R >> 1$ . So I changed how I bounded $\displaystyle  \mid I_{Arc} \mid$ So, it stands to reason that $\displaystyle J = 2I$ and $\displaystyle J = 2\pi i \sum\operatorname{Res}(f,z_k)$ , but the only pole inside the region bounded by $\displaystyle \gamma $ is $\displaystyle z = 1 + i$ , so after some algebra, one can find that $\displaystyle J = \frac{\pi}{8}\cosh\frac{1}{2}$ . So, $\displaystyle I = \frac{\pi}{8} \left(\sqrt{e} + \frac{1}{\sqrt{e}} \right)$ . What I am doing wrong? (I know that I may have written too much, but please don't bash me too harshly, this is my first time writing a question). EDIT: For those who don't want to read the comments. $\displaystyle z = 0$ is an essential singularity, so as the singularity is on the contour I need to make a detour around $\displaystyle z = 0$ . For example I can use the path parametrized by $\displaystyle \sigma(\theta) = \epsilon e^{i\theta}$ for $\displaystyle \theta \in \left[0,\frac{\pi}{2}\right]$ and $\displaystyle \epsilon << 1$ which contributes to the value of $\displaystyle J$ . But, now the curve is oriented clock-wise. So, $\displaystyle J = 2I + $ this contribution. After I calculate this contribution I will edit the question again. Final EDIT ( I suppose ): Calculating this contribution is just like calculating manually $I_{Arc}$ but in the limit of $R \to 0$ , I also suppose that this is not analytically doable, I guess that a simple Monte Carlo Integration technique could help. Anyway, this contribution should make things work. The moral of the story is to trust in the power of analysis and to do things in a smart way just like I did in the first part of this question.","['integration', 'complex-analysis', 'contour-integration']"
4123438,"For an integrable function, does there exist a sequence of partitions with this property.","Assume you have the measurespace $([a,b],\mathcal{B}([a,b],l)$ , where $l$ is the Lebesgue measure. Assume also you have an integrable function $f [a,b]\rightarrow \mathbb{R}$ on the measure space. For a partition of $[a,b]$ , $\Pi=\{t_0=a<t_2,\cdots,t_n=b\}$ , define $$I(\Pi)=\sum\limits_{i=0}^{n-1}f(t_i)\cdot1_{\{x: |f(x)|<\infty\}}(t_i)(t_{i+1}-t_i).$$ For an arbitrary sequence of partitions, $\Pi_n$ , with $\max_{i \in \{0,\cdots n-1\}}(t_{i+1}-t_i)\rightarrow 0$ , I am quite sure it is not the case that $$\limsup\limits_{n \rightarrow \infty}I(\Pi_n)=\liminf\limits_{n \rightarrow \infty}I(\Pi_n)=\int_{[a,b]}f(x)dx.$$ But I am wondering what happens if we may choose a special sequence so that it holds, so what I am wondering is: Case 1: Does there exist a sequence of partitions where the above equality holds? Case 2: If for a partition $\Pi$ we define $$f_{\Pi}(t)=\sum\limits_{i=0}^{n-1}f(t_i)\cdot1_{\{x: |f(x)|<\infty\}}(t_i)1_{x \in [t_i,t_{i+1})}(t),$$ and we still assume that $f$ is integrable, does there exist a sequence of partitions $\{\Pi_n\}$ so that $$\int_{[a,b)}|f(t)-f_{\Pi_n}(t)|dt\rightarrow0?$$ Case 3 If we also assume that $f \in L^2([a,b])$ , and let $f_\Pi$ be as above, does there exist a sequence of partitions $\{\Pi_n\}$ such that $$\int_{[a,b)}|f(t)-f_{\Pi_n}(t)|^2dt\rightarrow0?$$","['measure-theory', 'lebesgue-measure', 'lebesgue-integral']"
4123446,How to evaluate $\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4}$?,"Is it possible to evaluate the sum: $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4}$$ I expect it may be related to $\zeta^{\prime} (2)$ : $$\zeta^{\prime} (2) = - \sum_{k=2}^{\infty} \frac{\ln(k)}{k^2}$$ Is there an identity that works for my series, involving the natural logarithm, that is similar to the identity that: $$\sum_{n=0}^{\infty} \frac{1}{(n+a)(n+b)} = \frac{\psi(a) - \psi(b)}{a-b}$$ Also potentially related, the Lüroth analogue of Khintchine’s constant can be defined as the following: $$\sum_{n=1}^{\infty} \frac{\ln (n)}{n(n+1)}$$ as mentioned here . After some work, the following can be shown: $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \frac{5\ln(2) + 4\ln(3)}{16} + \frac{1}{2} \sum_{k=3}^{\infty} \frac{1}{k} \text{tanh}^{-1} \left( \frac{2}{k} \right)$$ and furthermore: $$\sum_{k=3}^{\infty} \frac{1}{k} \text{tanh}^{-1} \left( \frac{2}{k} \right) = \int_{0}^{2} \left( \frac{\left(1-\pi x \cot(\pi x) \right)}{2x^2} + \frac{1}{x^2 - 1} + \frac{1}{x^2 -4} \right) \, dx$$ EDIT I have derived yet another form for my sum of interest, however, I found this one interesting as it seems like it could potentially be solvable? $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \int_{0}^{\infty} \left( \frac{\psi^{(0)} (s+3) + \gamma}{(s+2)(s-2)} - \frac{25}{16 (s-2)(s+1)} \right) \, ds$$ From this, it is possible to obtain the following: $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \frac{\pi \gamma}{4} i + \frac{25}{48} (\ln (2) - i \pi) - \frac{1}{8} + \frac{1}{16} i \pi + \frac{1}{4} \int_{0}^{i \pi} \psi^{(0)} \left( \frac{4}{1+ e^{u}} \right)  \, du$$ $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \frac{\pi \gamma}{4}i+\frac{25}{48} (\ln (2)-i \pi )+\frac{7 i \pi }{48}-\frac{1}{8}-\frac{\ln (2)}{3} -2 \int_0^{\infty } \frac{t \ln (\Gamma (1-i t))}{\left(t^2+4\right)^2} \, dt$$ $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = -\frac{1}{8}-\frac{i \pi }{4}+\frac{i \gamma  \pi }{4}-\frac{\ln (2)}{16} - 2 \int_{0}^{\infty} \frac{t \ln (\Gamma (-i t)) }{(4+t^2)^2} \, dt$$ $$\implies \sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} =\frac{25}{48} \ln (2) -\frac{1}{8} + \int_{1}^{\infty} \frac{\ln (v-1) \text{li} (v^2)}{v^5} \, dv$$ Where $\text{li}$ is the logarithmic integral function. $$\sum_{k=3}^{\infty} \frac{\ln(k)}{k^2-4} = \frac{3 \ln (2)}{16} - \frac{\pi^2+1}{8} - \frac{\pi}{2} \int_{0}^{\infty} \sin(4\pi x) (\psi (x) - \ln (x)) \, dx$$","['definite-integrals', 'analysis', 'calculus', 'closed-form', 'sequences-and-series']"
4123460,determine the coefficient of $x^2yz$ in the expansion of $(2x-y+z+1)^7$,"determine the coefficient of $x^2yz$ in the expansion of $(2x-y+z+1)^7$ I found this question in my textbook , i seem very easy in the beginning but my answer is wrong according to answerkey. I said that the coefficient is equal to $\frac{7!}{2! \times 1 \times 1 \times 3!} \times 2^2 \times (-1)$ However ,the answer is $-280$ . What am i missing ?","['binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4123520,Uniform convergence of integral function,"Let $f \in \mathcal{C}^{\infty}_{K}(\mathbb{R})$ be a smooth function with compact support and $t \geq0$ . Then $
g_{t}(x) =\frac{1}{t}\int_{\mathbb{R}}(f(x+\sqrt{t}z)-f(x)-\frac{t}{2}f^{''}(x))e^{-\frac{1}{2}z^2}dz \rightarrow 0
$ uniformly
for $t \rightarrow 0$ holds. I am really struggling to show this and would be grateful for hints or a solution.","['limits-without-lhopital', 'analysis', 'real-analysis', 'functional-analysis', 'limits']"
4123526,How to know if a logic formula is equivalent to a Horn clause?,"Take for instance $X \lor Y$ . It is commonly known that this formula is not a horn clause since it does not have a minimal model. Now, does it suffice to say $\exists Minimal \ model \equiv horn \ clause$ ? (note: only equivalence! I know that it doesn't imply that it is a horn clause) If not, is there another way to find out?  Do I have to rewrite a formula into the shape of a horn clause possibly?","['logic', 'discrete-mathematics']"
4123530,"If $f: X \to Y$ diffeomorphism and $X,Y$ manifolds with boundary then $f(\partial X) = \partial Y$","Well, the exercise is the following: If $f: X \to Y$ diffeomorphism and $X,Y$ manifolds with boundary then $f(\partial X) = \partial Y$ My idea to prove this is the following: By the definition of $\partial X$ I use that if $x \in \partial X$ then $\exists \ \phi : U \subset H^k \to U' \subset X $ with $\phi$ a diffeomorphism. Also, $x \in U' \cap \partial X = \partial U'$ and $\phi^{-1}(x) = u \in \partial U$ .  So as I know that $f$ is also a diffeomorophism I used that $f(U') = V$ with $f(x) = y \in V \subset Y$ .  And also $f : U' \to V$ is a diffeomorphism. So if I compone $f \circ \phi : U \subset H^k \to V \subset Y$ we can see that $f \circ \phi (u) = y \in V \subset Y$ . So by definition of boundary of a manifold we have that $f(x) = y \in \partial V \subset \partial Y $ . So right now I have prove that $f(\partial X) \subset \partial Y$ . To prove $\partial Y \subset f(\partial X)$ we prove $f^{-1}(\partial Y) \subset \partial X$ as we have prove $f(\partial X) \subset \partial Y$ . I don't know if my idea is correct or not.","['diffeomorphism', 'manifolds-with-boundary', 'differential-geometry']"
4123537,Prove or disprove $SO(4n) \supseteq \frac{(Sp(1)\times Sp(n))}{\mathbb{Z}_2}$?,"I suspect that this is true $$
\boxed{SO(4n) \supseteq \frac{(Sp(1)\times Sp(n))}{\mathbb{Z}_2}.}
$$ How do we prove it? When $n=1$ , we have $$
SO(4) \supseteq \frac{(SU(2)\times Sp(1))}{\mathbb{Z}_2}=  \frac{(SU(2)\times SU(2))}{\mathbb{Z}_2}=\frac{Spin(3) \times Spin(3)}{\mathbb{Z}_2}
=\frac{Spin(4)}{\mathbb{Z}_2}.
$$ which is true by isomorphism. When $n=2$ , we have $$
SO(8) \supseteq \frac{(SU(2)\times Sp(2))}{\mathbb{Z}_2}=  \frac{(SU(2)\times Spin(5))}{\mathbb{Z}_2}=\frac{Spin(3) \times Spin(5)}{\mathbb{Z}_2}.
$$ But in contrast only $$
{Spin(8)} \supseteq \frac{Spin(3) \times Spin(5)}{\mathbb{Z}_2},
$$ which is true. How to prove that general $n$ , it is all true? It is better even to show the explicit embedding. p.s. Here I use the symplectic group $Sp(1)=SU(2)$ and $Sp(2)=Spin(5)$ . We can see that a generic $SU(2)$ group can be represented by a rank-2 unitary matrix satisfies $$V^\dagger V =\mathbb{I}.$$ Then we can write such a complex $V = \begin{pmatrix} a &  b\\ - b^* & a~* 
\end{pmatrix}$ . It can be checked that it obeys $Sp(1)$ condition $$
V^T \begin{pmatrix} 0&  1\\ -1 & 0 \end{pmatrix} V = \begin{pmatrix} 0&  1\\ -1 & 0 \end{pmatrix}.
$$","['symplectic-geometry', 'spin-geometry', 'group-theory', 'lie-groups', 'differential-geometry']"
4123545,Book recommendation on Dedekind's Cuts,"Is there any book (except Baby Rudin) where 'Dedekind's cuts' and it's consequences are explained in detailed manner with pictures (in intuitive way)? EDIT Seems that Fikhtengol'ts's ""The Fundamental of Mathematical Analysis"" (Vol-1) has a little part dedicated to the Dedekind's cuts (which is in a more brain freidnly way).","['real-numbers', 'calculus', 'real-analysis']"
4123554,What are all three 2-fold coverings of the Klein bottle?,"Let $K$ be a Klein bottle. I know that $$\pi_1(K)\cong \langle a,b | abab^{-1}\rangle.$$ All 2-fold coverings of $K$ will correspond to index $2$ subgroups of $\pi_1(K)$ . Thus up to isomorphism, 2-fold coverings of $K$ correspond to surjective homomorphisms $\pi_1(K)\rightarrow \mathbb{Z}/2\mathbb{Z}$ . There are three such homomorphisms: $a\mapsto 0$ , $b\mapsto 1$ . This one is easy to find by splitting the Klein bottle in two. $a\mapsto 1$ , $b\mapsto 0$ . This one is also easy to find, similar to the one above but with a torus. $a\mapsto 1$ , $b\mapsto 1$ . This third one I can't seem to find the corresponding covering space. From the question I got it from I know that it should be another Klein bottle. It's also clear that the paths corresponding to $a$ and $b$ should not be loops in this covering space. I have been stuck on this for a bit, and also didn't find any clear answer online. I have tried many sorts of shapes and ways to partition the Klein bottle but can't seem to find it. I'd appreciate any help!","['general-topology', 'fundamental-groups', 'algebraic-topology', 'covering-spaces']"
4123571,How exactly does it make sense to differentiate a function whose input is a point on a manifold?,"In this lecture , prof Frederic Scheuller introduces the concept of the tangent vector space defined the following way by introducing the concept of velocity at a point: Let $(M,\theta,A)$ be a smooth manifold, and a curve $\gamma: R \to M$ at least $C^1$ (One differentiable and derivative is continous). Suppose $\gamma(\lambda_o)=p$ , then the velocity at $p$ is defined as: $v_{\gamma,p} : C^{\infty} (M) \to R$ Where $C^{\infty}(M) := \{ f: M \to R |  f   \text{ is a smooth function} \}$ equipped with an addition of the form $$(f+g)(p)= f(p) + g(p)$$ and s multiplication of the form $$( \lambda \cdot g)(p) = \lambda g(p)$$ where the addition and scaling is in the same sense as operations we define on the real numbers From 1:23 of the video , time stamped link The above is fine and more or less intuitive, but what bothers me is what happens at 52:51 , where he takes the derivative of the following object: $$[ \partial_i( f \circ x^{-1})] x (p)$$ I am not sure what the object of $\partial$ is precisely meant to mean but I can say that (this is one of my doubts) : $p$ is the geometric point on the manifold $x$ is the function which takes us from the manifold to the point on the chart corresponding to it $x^{-1}$ is the function which inverts the point on the chart to the point on the manifold $f$ is a function from a point on the manifold to real number line This is where I get confused, if $x^{-1}$ maps from points on the chart to geometric points on the manifolds and f takes that as an input, so what would it mean to take derivative of such a thing? The professor even points this out at around 55:40 but doesn't explain what exactly he has written. Thanks in advance.","['tangent-spaces', 'smooth-manifolds', 'differential-geometry']"
4123593,What is the precise formula for this curve?,"Here's about what I think the curve looks like: In working through a theory of mine, I have come across a curve I cannot identify. Along with $x$ and $y$ , this curve needs an additional input to complete the curve. Things I know about this curve: The curve is identical when reflected around the line $y=x$ . The curve always contains the points $(0,10), (10,0), (5,a),$ and $(a,5)$ . The area under the curve from $0$ to $10$ is always $10a$ . At $a=5$ , the curve matches perfectly with the line $10-x$ . I am nearly certain that the curve is hyperbolical in nature. Knowing this, what is the equation of this curve?",['functions']
4123679,"A book misprinted ""$\sin x\cos x$"" as ""$\sin\cos x$"" in an equation. Can the erroneous equation still be solved by elementary or numerical methods?","In my textbook of trigonometry there is, for an exercise, a misprint (fix typo) on this equation $$3\sin ^2\left(x\right)-\sqrt{3}\color{red}{\sin \left(\cos \left(x\right)\right)}+3\sin \left(2x\right)-2\sqrt{3}\cos ^2\left(x\right)=0 \tag1$$ because it is missed the $x$ for the operator $\sin$ . Here there is the original photo: I believe that with derivatives or numerical methods can be solved the $(1)$ . Is there no hope of being able to find the solution in case there is a misprint like this?","['trigonometry', 'soft-question', 'education']"
4123774,"Breaking a real valued increasing function into absolutely continuous, continuous and jump function","Suppose $F$ is an increasing function on $[a, b]$ .
(a) Prove that we can write $$
F=F_{A}+F_{C}+F_{J}
$$ where each of the functions $F_{A}, F_{C}$ , and $F_{J}$ is increasing and: (i) $F_{A}$ is absolutely continuous. (ii) $F_{C}$ is continuous, but $F_{C}^{\prime}(x)=0$ for a.e. $x$ . (iii) $F_{j}$ is a jump function. If we let $$
j_{n}(x)=\left\{\begin{array}{cl}
0 & \text { if } x<x_{n} \\
\theta_{n} & \text { if } x=x_{n} \\
1 & \text { if } x>x_{n}
\end{array}\right.
$$ then we define the jump function associated to $F$ by $$
J_{F}(x)=\sum_{n=1}^{\infty} \alpha_{n} j_{n}(x) .
$$ I am kind of sure that I have to use the property that an increasing function can have only countably many jump discontinuity. Given an increasing order of discontinuities $\{x_n\}$ of $F$ i.e., $x_n\le x_{n+1}$ and $f$ has discontinuities at $x_n$ then we can represent the function $$
F(x)=f_n(x); x_n\le x\le x_{n+1}
$$ and $f_n$ is continuous on $(x_n,x_{n+1})$ . Now we can define a function $$G(x)=\left\{\begin{array}{cl}
g_1(x):=f_1(x) & \text { if } a=x_1\leq x<x_{2} \\
g_{n}(x) & \text { if } x_n\le x\le x_{n+1}
\end{array}\right.
$$ where $g_n(x)$ is a function defined on $(x_n,x_{n+1})$ having the same slope of $f_n(x)$ on $(x_n,x_{n+1})$ so that $G(x)$ is continuous basically I am removing the jump discontinuities of $F$ . So we can write $F(x)=G(x)+J(x)$ where $$J(x)=f_n(\frac{x_n+ x_{n+1}}{2})-g_n(\frac{x_n+ x_{n+1}}{2}); x_n\le x\le x_{n+1}
$$ then $J(x)$ is a step function. I am wondering if I can at all transform this thing into the solution I want. Let me know if there is any other way. Please help!","['contest-math', 'analysis', 'real-analysis', 'continuity', 'absolute-continuity']"
4123794,Folland proof of proposition 4.1,"screenshot of proposition from folland here I am having trouble understanding the last part of the proof. Since $x \not\in A \cup acc(A) $ , then there exists a open $U$ containing $x$ such that $U \cap A = \emptyset$ . I do not understand how this implies $\overline{A} \subset U^c$ . My question arises since $A \subset \overline{A}$ , how can we know that $\overline{A} \cap U = \emptyset$ ? EDIT: I have been thinking about this a bit more and I think I have the solution: Let $x \in \overline{A}\backslash A$ and assume towards contradiction that $x\not\in acc(A)$ . Then there exists an open set $U$ such that $U \cap A = \emptyset$ . Then $\overline{A} \cap U^c$ is a closed set containing $A$ . Since $\overline{A}$ is the smallest set containing $A$ , we have a contradiction unless $\overline{A}\subset U^c$ which requires $\overline{A}\cap U = \emptyset$ . Is this correct? Also it seems to be a lot to leave out in a proof that doesn't seem to be a sketch. Is there a more concise way of thinking about this?","['general-topology', 'analysis']"
4123868,Math genius needed for combinatorics calculation of my card game (Shannon number-like),"My name is Lych, I am a game developer from Germany.
My card game is a quite complex one and I am very curious about the question, what amount of different matches are possible in this game. Just like the Shannon number in chess. However, this will most likely be bigger than the Shannon number.
In the following, I will describe everything you need to know. Some of the instructions are already simplified to not make it harder than necessary.
After all, it is not important to get an exact result, just a rough calculation.
Please keep in mind that I am not especially gifted when it comes to math. Therefore, I do hope that my instructions are clear. If they are not, please ask me. Thank you really much in advance! I am grateful to have the opportunity to ask people for help who can do this better than me. 4 players in total. After each turn, the next player clock-wise is next with their turn. Everyone has 5 cards in their hands in the beginning.
75 cards are available in total, all different, no equal ones.
(First interposed calculation that could help later: How many possible combinations of starter hand cards are possible?) Per turn, the player can set 2 cards from his hand to the field (choose freely from the 5 hand cards). Every single card on the field can do 21 different actions. In ANY order.
(That means that the order of options ""1, 2, 3, 4, 5 ... 21"" is a different one than ""1, 3, 2, 4, 5, ... 21"" and therefore another match obviously).
Also, the 2 cards can take turns in their actions as the order is completely free again.
(Therefore, the amount of possible orders with those two cards has to be an incredible high number already). However, the players will not always add 2 more cards per turn. In average (for this calculation), they will keep having 2 cards on the field to use, constantly. Every turn, they are destroyed and 2 new cards will be placed from the hand again, freely. In the beginning of each new turn, the player draws 3 more cards (don't mind because only
2 are used in the previous point. One card of them is always directly destroyed, but do not take
this into the calculation. Just act like the player only gets 2 cards per turn, but 3 are removed from the total card stack).
(Still keep in mind that every card of those 75 is unique and all possibilities of card drawings from the stack that are still available, and also all card settings from the hand to the field must be taken into the calculation). In total, there are 20 turns, but only 18 of them the players draw cards because then, the card stack
is empty (55/3 = 18 roughly. 55 because in the beginning, 20 cards are already drawn).
In the last 2 turns, they don't draw but still play as before with 2 cards on the field. -> How many different matches are possible in this game? Closing words: As I said, thank you very much for your effort to everyone who helps me with this. If anything should be not clear, please ask. After all, it is enough if the result is an estimation and not exact. ~ Lych","['combinations', 'probability-distributions', 'combinatorics', 'card-games', 'probability']"
4123979,Markov inequality for monotonically increasing function,"The Markov inequality is given as $$ P(X \geq a) \leq \frac{\mathbb{E}(X)}{a} $$ The extended version for monotonically increasing function follows, if $\varphi$ is a monotonically increasing nonnegative function for the nonnegative reals $$ P(|X| \geq a) \leq \frac{\mathbb{E}(\varphi(|X|))}{\varphi(a)} $$ The proof for this is given in Wikipedia article as $$ P(|X| \geq a) = P(\varphi(|X|) \geq \varphi(a)) \leq \frac{\mathbb{E}(\varphi(|X|))}{\varphi(a)} $$ Why this statement $ P(|X| \geq a) = P(\varphi(|X|) \geq \varphi(a)) $ is true? For e.g. let's say $\varphi$ is $e^x$ , why is it that for any non-negative random variable $X$ , then $P(X \geq a) = P(e^X \geq e^a) $ , it doesn't seem obvious to me why this is true.","['inequality', 'probability-theory']"
4124007,Example of Riemannian metric on sphere such that it become non strictly convex.,"My understanding of strictly convexity of the compact set in Euclidean space is that if we take any straight line joining any two boundary points then the line must be in a compact set with out intersecting any other boundary point. But for compact Riemannian manifold $(M,g)$ , the definition of strict convexity is that the second fundamental form of boundary must be positive definite. Which depends upon metric. To better understand this concept in the Riemannian setup, I was trying to find an example of metric on Sphere such that in that case, it becomes non strictly convex. But I could not able to find it. Can anyone please help to relate the positive definiteness of the second fundamental form with strictly convexity in the Riemannian manifold? Any Help or hint will be greatly appreciated.","['riemann-surfaces', 'examples-counterexamples', 'riemannian-geometry', 'differential-geometry']"
4124008,"$f(x)=\cos x-x$. If $\lim_{n\rightarrow \infty }|f(a_{n})|=0$, then $\lim_{n\rightarrow \infty }a_{n}$ converges?","Here, we have a very well-known function $f(x)=\cos x-x$ . I know there is only one solution for $f(x)=0$ . Let it be $a\approx 0.739$ . Now, I'm trying to prove that if we have $a_{n+1}=\cos a_{n}$ for all $n$ , then $a_{n}$ converges to $a$ . (Just with MVT) As a result, I got the fact "" $\left | a_{n+2}-a_{n+1} \right |\leq \left | a_{n+1}-a_{n} \right |$ for all $n$ ."" So, I broke it into two cases s.t $\square $ :"" $\left | a_{n+2}-a_{n+1} \right |<  \left | a_{n+1}-a_{n} \right |$ for any $n$ ,"" and $\bigcirc $ :"" $\left | a_{n+2}-a_{n+1} \right |=   \left | a_{n+1}-a_{n} \right |$ for any $n$ ."" In case $\square $ , we can get $\lim_{n\rightarrow \infty }|f(a_{n})|=0$ . But, I'm not certain if it is safe to say that ""since $f(x)=0$ has a unique solution $x=a$ , we have $\lim a_{n}=a$ "" So, here's my general question: For any continuous & differentiable function $f(x)$ that has a unique solution $x=a$ for $f(x)=0$ , if $\lim_{n\rightarrow \infty }|f(a_{n})|=0$ , then is $\lim a_{n}=a$ ?","['limits', 'convergence-divergence', 'recursion', 'real-analysis']"
4124010,$\sigma$-fields generated by multiples of natural numbers,"Let $\mathbb{N} = \{1,2,3,\dots\}$ and $k\mathbb{N} =\{k,2k,3k,\dots\}$ . What is the $\sigma$ -field generated by the following collections $\mathcal{B_1} = \{k\mathbb{N} : k \in \mathbb{N} \}$ $\mathcal{B_2} = \{k\mathbb{N} : k \ \ is \ a \ prime \}$ I can prove if the description of $\sigma$ -field is given that it'll be generated by some given collection but I can't guess the other way. Any hints is appreciated.","['measure-theory', 'probability-theory', 'borel-measures']"
4124014,Geometry problem about proving midpoint using complex numbers,"The original question is as follows, Let O be the center of a circle passing through points A, B, C and let
AD be a diameter of the circle. Let the tangent line at D to the
circle intersect line BC at P. Let line OP intersect line AC and AB at
M and N respectively. Prove that O is the midpoint of MN. This is what I have come up with the picture, I have the equation of line of BC is $Z+BC\cdot\overline{\rm Z}=-BC$ . And the equation of the tangent line at D is $Z+D^{2}\overline{\rm Z}=2D$ After assuming it is a unit circle, I expressed point B and C as $e^{i\alpha}$ and $e^{i\beta}$ respectively. But I am stuck afterward. I would appreciate if anyone can help.","['euclidean-geometry', 'circles', 'geometry', 'plane-geometry', 'complex-numbers']"
4124031,Solve the following system of equations:$|x|+|y|=2$ and $y=x+1$.,"Solve the following system of equations: $|x|+|y|=2$ and $y=x+1$ , where $x$ is a real number. Approach: I substituted $y$ in equation $1$ , so: $$ \ \ \ \ \ \ \ \ \ \ |x|+|x+1|=2$$ $$1 \ \ \ \ \ \ \ \ \ \ 1$$ $$2 \ \ \ \ \ \ \ \ \ \ 0$$ $$0 \ \ \ \ \ \ \ \ \ \ 2$$ These are the total possibilities I think, because $|x|, |x+1| \ge 0$ . Then I made cases, Case $1$ : $|x|=1$ , so, $x = \pm 1$ and $|x+1| = 1$ , so $x+1=\pm 1$ so, $x = 0,-2$ . But no value of $x$ is matching, so this case gets rejected. Case $2$ : $|x|=2$ , so, $x = \pm 2$ and $|x+1| = 0$ , so $x=-1$ . But no value of $x$ is matching, so this case also gets rejected. Case $3$ : $|x|=0$ , so, $x = 0$ and $|x+1| = 2$ , so $x+1 = \pm 2$ and so, $x=1,-3$ . Here also, no value of $x$ is matching. So no solution exists. Is this solution correct? Please confirm. If there is a shorter method to approach the question, please share it.","['contest-math', 'algebra-precalculus', 'systems-of-equations', 'absolute-value']"
4124066,"If $x_n + A = A$ and $x_n \to 0$, then Lebesgue measure of $A$ is $0$","If $A$ is a measurable set, $x_n$ is a sequence converging to $0$ (all $x_n$ different from $0$ ), and $x_n +A = A$ for all $n$ , then $\lambda(A) = 0$ or $\lambda(A^c) = 0$ where $\lambda$ is the Lebesgue measure. I have been trying this problem for quite some time but made no progress. Any hints?","['measure-theory', 'lebesgue-measure']"
4124086,Assume n people are sitting around a circular table. In how many ways we can re-arrange them so each person has a different person on his right?,"So I have this question.
n dancers are dancing in a circle, and then spread out and dance solo.
Now they come back together for another circular dance but now each dancer can't be standing in a way that he'll have the same person on his right from the previous circle.
How many options for the second circle are there? *the order of the dancers in the first circle is given, the questions is only about the order in the second circle.",['combinatorics']
4124125,Estimating the number of infected people,"Given a set $U$ of people, there is a subset $S\subset U$ of infected people. You only know the size of $U$ but want to estimate $\dfrac{|S|}{|U|}$ . However, you don't have enough time to test every single person $p\in U$ on the virus so you decide to only test a fraction $K\subset U$ of all the people and thus try to approximate the ratio $\dfrac{|S|}{|U|}\approx \dfrac{|\{p\in K\mid p \text{ is infected}\}|}{|K|}$ . Given that you obtain a ratio $r\in [0.4\cdot |U|, 0.6\cdot |U|]$ , you want calculate the probability that the actual ratio $\dfrac{|S|}{|U|}$ lies in the interval $[0.4\cdot |U|, 0.6 \cdot |U|].$ I read some procedures about this often involving the normal distribution, however, I'm not allowed to use the normal distribution for this task. How could I attempt to solve this problem?","['approximation', 'probability']"
4124155,Asymptotic analysis: difference between big O and big Omega limits?,"So I've started university again after taking a break and my first assignment in algorithms is to prove whether $f(n)$ is in Big O / Big Omega of $g(n)$ .
These are the limit rules I have: $$\lim\limits_{n\to\infty} \frac{f(n)}{g(n)} < \infty \Longrightarrow f \in O(g) \\
\lim\limits_{n\to\infty} \frac{f(n)}{g(n)} = 0 \Longrightarrow f \in o(g) \\
\lim\limits_{n\to\infty} \frac{f(n)}{g(n)} > 0 \Longrightarrow f \in \Omega(g) \\
\lim\limits_{n\to\infty} \frac{f(n)}{g(n)} = \infty \Longrightarrow f \in ω(g)$$ My question is: What is the difference between Big O and Big Omega in terms of limits ? Isn't every value bigger than $0$ also smaller than infinity? Also the following problem: $$\lim\limits_{n\to\infty} \frac{n+\pi}{n-e^{2021}} \text{ with l'Hopital} = \frac{1}{1} = 1$$ So it would be $>0$ , so Big Omega, but if I do l'Hopital again, it comes down to zero, meaning small $o$ ...?
Am I missing something here?
My brain is still hazy from the long break, so maybe I'm confusing simple things here ;)","['limits', 'proof-explanation', 'asymptotics']"
4124174,Expectation of a composite function.,"My thoughts on the solution: I noticed that the ln function ""stretches"" the intervals of X like: (-1; 0) to (-inf; 0), and (0; 1) to (0; ln2). Intuitively, it becomes clear that due to the long interval of negative values of ln(1+x), the expectation will also be negative. But I haven't figured out how to strictly prove it. In general, it is probably worth considering the integral: ∫1−1ln(x+1)f(x)dx, where f(x) is probability density function. Thanks!","['expected-value', 'statistics', 'probability-distributions', 'probability']"
4124180,Inequality $f_{n}(x)>1$ for $0<x<1$,"Let $0<x< 1$ and $n\geq 1$ a positive natural number then we have : $$f_{n}(x)=\left(x\left(\frac{1-x}{1+x}\right)^n\right)^{\left(\frac{x}{x+1}\left(\frac{1-x}{1+x}\right)^n\right)}-x\left(\frac{1-x}{1+x}\right)^n+\sqrt{x\left(\frac{1-x}{1+x}\right)^n}>1$$ I  can show a related result wich is a limit because we have : $$\lim_{n\to\infty}\left(x\left(\frac{1-x}{1+x}\right)^n\right)^{\left(\frac{x}{x+1}\left(\frac{1-x}{1+x}\right)^n\right)}-x\left(\frac{1-x}{1+x}\right)^n+\sqrt{x\left(\frac{1-x}{1+x}\right)^n}=1$$ But it doesn't help so much to solve the inequality .It just show that the inequality is sharper and sharper as $n\to \infty$ . The base case : Using Wolfram alpha we have : $$f_{1}(x)=1+\frac{\sqrt{1-x}}{\sqrt{2}}+\frac{1}{4}(x-1)(-\log(1-x)+2+\ln(2))+O((x-1)^{1.5})
$$ Unfortunately it's not sufficient to show the base case . Edit :For the base case a better approximation is here Also we can use Newton's expansion (so without derivative) to kill the exponent for the general case.Remains to replace $a$ by $x$ .It's around $1$ and we can also use an expansion around $0$ . Question : How to show that $f_{n}(x)>1$ ? I'm thanksful to anyone who have interest for the question and want to share something with me and other person on this subject.","['limits', 'inequality']"
4124249,Prove that : $|P(\mathbb{R}) \setminus P(\mathbb{R}\setminus \mathbb{Z})| = 2^{2^{\aleph_0}}$,Prove that: $|P(\mathbb{R}) \setminus P(\mathbb{R}\setminus \mathbb{Z})| = 2^{2^{\aleph_0}}$ It is obvious that: $P(\mathbb{R}) \setminus P(\mathbb{R}\setminus \mathbb{Z}) \subseteq P(\mathbb{R})$ so: $|P(\mathbb{R}) \setminus P(\mathbb{R}\setminus \mathbb{Z})| \le  2^{2^{\aleph_0}}$ But I don't know how to prove that it is also $\ge 2^{2^{\aleph_0}}$ .,"['elementary-set-theory', 'cardinals']"
4124268,$\lim_{x\to0}2^{\cot x}$,$$\lim_{x\to0}2^{\cot x}$$ So it turns out that the limit does not exist but here's the thing. I can't compute this limit $$\lim_{x\to0}{\cot x}$$ this is all I did $$\lim_{x\to0}{\cot x}=\lim_{x\to0}\frac{\cos x}{\sin x}=\lim_{x\to0}\frac{x\cos x }{x\sin x}=\lim_{x\to0}\frac{\cos x}{x}$$ then nothing comes to mind. I feel like this is a really easy problem but I've been trying to solve thins for over an hour and I'm stuck. Could you please help me? Thanks in advance.,"['limits', 'calculus', 'trigonometry']"
4124270,Misunderstanding of an exercise in Görtz-Wedhorn: When do the nilradical and Jacobson radical coincide?,"Exercise 2.3 of Görtz-Wedhorn, Algebraic Geometry I , states that the nilradical of $A$ is equal to the Jacobson of $A$ if and only if every non-empty open subset of $\operatorname{Spec}A$ contains a closed point. My issue is as follows: if every non-empty open subset of $\operatorname{Spec}A$ contains a closed point, in particular for every $f \neq 0$ the open subset $D(f)$ contains a closed point, i.e. a maximal ideal $\mathfrak{m}$ . In other words, there is a maximal ideal that does not contain $f$ , so in particular $f \notin J(A)$ , where $J(A)$ is Jacobson of $A$ . This means that $J(A) = \{0\}$ , so that the nilradical and Jacobson coincide because they are both zero.
This seems false, though - there seem to be rings $A$ for which the nilradical and the Jacobson coincide, but they are non-zero (for example, $\mathbb{Q}[x]/(x^2))$ .","['algebraic-geometry', 'commutative-algebra']"
4124321,Approximating the average of a list consisting of real numbers,"Given a list of real number $(a_1, a_2, \ldots,a_n)$ we are interested in the average $\mu = \dfrac{1}{n}\displaystyle\sum_{i = 1}^{n}a_n.$ You are given a simple probabilist algorithm that approximates $\mu:$ Choose a parameter $k$ (is a fixed parameter you determine and thus not choosen at random) $s := 0$ repeat $k$ times: $i\leftarrow$ uniformly at random choosen index $1\leq i\leq n$ $s \leftarrow s + a_i$ $X = \dfrac{s}{k}$ return $X$ I'm asked to show that $\text{Var}[X] \leq \dfrac{1}{4k}.$ So far I noticed that clearly $\mathbb{E}[X] = \mu$ and for estimating the variance one probably wants to make use of the formula $\text{Var}[X] = \mathbb{E}[X^2]-\mathbb{E}[X]$ but I don't know how to determine $X^2.$ Could someone give me an advice how to proceed?","['average', 'variance', 'probability-theory', 'algorithms']"
4124335,"How many different ways to distribute 30 different books to 3 people, such that the number of books given to each of them create an arithmetic series?","How many different ways to distribute $30$ different books to $3$ people, such that the number of books given to each of them create an arithmetic series? So, I thought one of them should have exactly $10$ books, and the others can partition rest of the books however they like. Example: $1 - 10 - 19$ Choose $10$ books, $\binom{30}{10}$ , any of them can have exactly $10$ books, so multiply it by $3$ , also others can partition the books however they like, which is $2^{20}$ .
In total $\binom{30}{10} \times 3 \times 2^{20}$ . Then, I guess I should be done here. Except, when all of them have $10$ books, so $10-10-10 $ right? Therefore I should discard $2 \times \binom{30}{10} \times \binom{20}{10}$ . In conclusion, $\ 3 \times 2^{20} \times \binom{30}{10} - (2 \times \binom{30}{10} \times \binom{20}{10})$ Is my answer true? If you have an another approach, can you share? Thank you in advance.",['combinatorics']
4124439,Global minimum of convex function,"Let $E \subset \mathbb R^n$ a closed set, convex and not bounded. Let $f \in C^0
(E, \mathbb R)$ strictly convex, it means that $$\forall (x, y) \in E^2, \forall \alpha \in ]0, 1[, x \neq y \implies f(\alpha x + (1 - \alpha)y) < \alpha f(x) + (1 − \alpha)f(y)$$ such that $$\lim_{\lVert 𝑥 \rVert \to +\infty} f(x) = +\infty ; i.e. ∀𝑐 ∈ ]0, +∞[, ∃𝑀 ∈ ]0, +∞[, \forall x \in 𝐸, \lVert x \rVert \geq M \implies f(x) \geq c$$ Show that $f$ has a global minimum and at a unique point. Since the function is defined on $\mathbb R^n$ I cannot use one dimensional mean value theorem to show that there exists a point $a$ on which $\triangledown f(a)=0$ and I also must show that the hessian matrix is defined positive to show that it is a global minimum but I am running out of ideas to do so.","['global-optimization', 'real-analysis', 'maxima-minima', 'multivariable-calculus', 'convex-analysis']"
4124452,Number of ways to arrange pairs of integers with distance constraint,"Given a sequence 0,1,1,2,2,3,3,...,n,n, find the number of ways to arrange the elements in the sequence so that there's exactly one digit between the two 1's, two digits between the two 2's, three digits between the two 3's, so on and so forth, and there are exactly n digits between the two n's. For n=2, a possible solution would be 12102. For n=5, a possible solution would be 53141352402. I have calculated that when n=2, the answer is 2; when n=3, the answer is 6; when n=4, the answer is 10; and when n=5, the answer is 22. But when n=6, my raw permutation function looks over 13! values and takes over an hour to calculate on my computer. I cannot think of any combinatorial enumeration or dynamic programming methods for this either. The best I can manage at the moment is backtracking with symmetry. So my question is, what is the fastest way to compute the answer to this question for a large n and what are the mathematical intuitions for this problem? For example, is there a nontrivial upper / lower bound for the computational complexity for this problem? Thanks for the answers in advance. I first came across this problem in a Chinese coder forum.","['computational-complexity', 'discrete-mathematics']"
4124488,Prove that $\sum a_n$ converges $\implies \sum \frac {a_n} {2^n}$ converges.,"Prove that $\sum a_n$ converges $\implies \sum a_n/2^n$ converges. My proof is below.  Please verify or improve it. $\lim a_n = 0$ , as $\sum a_n$ converges.  Therefore, there exists $N$ such that for all $n > N$ , $|a_n| < 1$ .  For $n > N, 0 \leq |a_n/2^n| < 1/2^n$ .  For any series $p_n, q_n$ ,  if $\sum q_n$ converges, and for all $n > N$ , $0 \leq p_n \leq q_n$ , then $p_n$ converges; consequently, $\sum |a_n/2^n|$ converges.  And since for any series $p_n$ , if $\sum |p_n|$ converges, then $\sum p_n$ converges, therefore $\sum a_n/2^n$ converges.  QED. Update: Weaker Conditions The condition $\sum a_n$ converges is unnecessarily strong. The proof works as long as $a_n$ eventually becomes bounded.","['convergence-divergence', 'solution-verification', 'sequences-and-series', 'real-analysis']"
4124522,Approximate $\sin 29^\circ$ using differentials,"Using differential find approximate value of $\sqrt[3]{1.02}$ I did this.
We know $f(x_0+\Delta x)-f(x_0)=\frac{df}{dx}(x_0)\cdot\Delta x$ $$f(x)=\sqrt[3]{1+x}$$ $$f(0.02)-f(0)=\frac{df}{dx}(0)\cdot0.02$$ And after calculation I solved. But the problem I don't know how to solve is approximate using differential $\sin29^\circ$ .","['approximation', 'analysis', 'real-analysis', 'calculus', 'derivatives']"
4124545,Stokes' Theorem - cylindrical coordinates,"I'm currently having an issue with verifying the validity of Stokes' Theorem on a particular problem. I can solve the problem by using Stokes' theorem to turn a surface integral of the curl of a vector into a line integral of a vector. However, when I try to check the validity of this by explicitly computing the surface integral of the curl, I am apparently faced with integrating a zero vector. We have the following where we use the cylindrical coordinate system $(\rho,\varphi,z)$ and $R,\Gamma$ are constants: \begin{align*} 
\boldsymbol{u} &= u_\varphi(\rho)\hat{\boldsymbol{\varphi}} 
\end{align*} \begin{align*}
&u_\varphi(\rho) = \begin{cases}
\frac{\Gamma \rho}{2 \pi R^2} &\rho \leq R \\
\frac{\Gamma}{2 \pi \rho} & \rho > R \\
\end{cases}
\end{align*} \begin{align}
\boldsymbol{\omega} &= \nabla \times \boldsymbol{u} = (0,0,\omega(\rho))
\end{align} \begin{align*} 
&\omega(\rho) = \begin{cases}
\frac{\Gamma}{2 \pi \rho} &\rho \leq R \\
0 & \rho > R \\
\end{cases}
\end{align*} We are given \begin{align}
\Omega(\rho) = \int_{D(0,\rho)} \boldsymbol{\omega} \cdot \ dS
\end{align} where $D(0,\rho)$ is a disc of radius $\rho$ centered at the origin. We are asked to show that: \begin{align*} 
&\Omega(\rho) = \begin{cases}
\Gamma(\frac{\rho}{R})^2 &\rho \leq R \\
\Gamma & \rho > R \\
\end{cases}
\end{align*} where $D(0,\rho)$ is a disc of radius $\rho$ centered at the origin. While I can find $\Omega$ for $\rho \leq R$ , I'm having an issue directly computing it when $\rho > R$ . I have used Stokes' theorem to change the integral to $\int_{0}^{2 \pi} \boldsymbol{u} \cdot d\boldsymbol{x}$ which then gives me: \begin{align*}
\Omega(\rho) &= \int_{0}^{2 \pi} (0, \frac{\Gamma }{2 \pi \rho},0) \cdot d \boldsymbol{x} \\
 &= \int_{0}^{2 \pi} \frac{\Gamma }{2 \pi \rho} \rho d \varphi \\
&= \frac{\Gamma }{2 \pi } [ \varphi ]_{0}^{2 \pi} = \Gamma
\end{align*} However, when I try to compute this directly using: \begin{align*}
\Omega(\rho) = \int_{D(0,\rho)} \boldsymbol{\omega} \cdot \ dS = \int_{D(0,\rho)} (\nabla \times\boldsymbol{u}) \cdot \ dS
\end{align*} I get $(\nabla \times \boldsymbol{u})=(0,0,0)$ which would give $\Omega(\rho)= 0 \quad \forall \ \rho > R$ We are also provided with a formula for computing the curl of a vector field expressed in cylindral polar coordiantes $(\rho, \varphi, z)$ : \begin{align*} 
 \nabla \times \boldsymbol{A} = \frac{1}{\rho} \begin{vmatrix}
\hat{\boldsymbol{\rho}} & \rho \hat{\boldsymbol{\varphi}} & \hat{\boldsymbol{z}} \\
\partial_{\rho} & \partial_{\varphi} & \partial_{z} \\
A_{\rho} & \rho A_{\varphi} & A_{z}
\end{vmatrix}
\end{align*} Any help on how I can get the two methods to agree or observations on where I have gone wrong would be much appreciated, thank you!","['cylindrical-coordinates', 'multivariable-calculus', 'stokes-theorem', 'vector-analysis']"
4124547,How to solve for $X$ given $A = X + Y$ and $B = 1 + YX$?,"So I have the following system of equations: $A = X + Y$ $B = 1 + YX$ And I'm asked to solve for $X$ and $Y$ .
My question is how do I do so? I was taught that I should first try to isolate $X$ and get it in terms of $A$ and $B$ . So my attempt was the following: $A = X + Y$ $B = 1 + YX$ into $-A = -X-Y$ $B = 1 + YX$ into $-AX = -x^2-YX$ $B = 1 + YX$ so adding $-AX + B$ together I get $B-Ax=-x^2+1$ which does not isolate $X$ in terms of $A$ and $B$ as there's still an $X$ on the LHS So this is where I get stuck. Can someone please help me figure out the necessary algebra? Is my way of going at this completely wrong? I have the same issue isolating $Y$ . Any help is greatly appreciated.","['algebra-precalculus', 'quadratics']"
4124582,"Finding the MLE's when $X_i \sim N(\mu,\sigma^2)$","We have a random sample of size $n$ from a normal distribution, $X_i \sim N(\mu,\sigma^2)$ . I have to find the MLE of the following: $P[X > c]$ for arbitrary $c$ The $95^\text{th}$ percentile of $X$ Could someone give me tips how to handle these kind of questions? I have solved MLE questions before, so I understand the idea behind it, however I couldn't answer these two unfortunately. So far, I have standardized the $P[X>c]$ and I know have $P[X>c] = 1 - F_X[\frac{c-\mu}{\sigma}]$ . I guess I can now conclude something, but I do not know what.","['statistics', 'normal-distribution', 'probability', 'maximum-likelihood']"
4124583,An IMO training problem: Living in an overpopulated apartment,"Here is a problem from the book 102 Combinatorial Problems: From the Training of the USA IMO Team by Titu Andreescu and Zuming Feng : Original problem: A total of $119$ residents live in a building with $120$ apartments. We call an apartment overpopulated if there are at least $15$ people living there. Every day the inhabitants of an overpopulated apartment have a quarrel and each goes off to a different apartment in the building (so they can avoid each other). Is it true that this process will necessarily be completed someday? Extension (Self-made): If after $n$ days the process is completed, what is the maximum value of $n$ ? Solution from the book Let $p_1,p_2,\cdots,p_{120}$ denote the $120$ apartments, and let $a_i$ denote
the number of residents in apartment $p_i$ . We consider the quantity $$\color{red}{\textrm{S=$\frac{a_1(a_1-1)}{2}+\frac{a_2(a_2-1)}{2}+\cdots+\frac{a_{120}(a_{120}-1)}{2}$}}$$ ( Assume that all the residents in an apartment shake hand with each other at the beginning of the day, then quantity $S$ denotes the number of the handshakes in that day .) If all $a_i < 15$ , then the process is completed and we are done. If not, without loss of generality, we assume that $a_1\ge15$ and that
the inhabitants in $p_i$ go off to different apartments in the building. Assume that they go to apartments $p_{i_{1}},p_{i_2},\cdots,p_{i_{a_1}}$ . $\color{blue}{\textrm{On the next day, the quantity is changed by an amount of  }}$ $$\color{blue}{\textrm{$a_{i_1}+a_{i_2}+\cdots+a_{i_{a_1}}-\frac{a_1(a_1-1)}{2}$}}$$ which is positive as $$a_{i_1}+a_{i_2}+\cdots+a_{i_{a_1}}\le119-a_1\le119-15=104$$ and $$\frac{a_1(a_1-1)}{2}\ge\frac{15\cdot14}{2}=105$$ Hence the quantity is decreasing during this process. On the other hand, $S$ starts as a certain finite number and $S$ is nonnegative. Therefore this process
has to be completed someday. $\blacksquare$ My question I don't understand the solution (especially the texts which are in colored), even after trying to understand it for almost two days. I have the following questions: Why is $S$ needed to be considered? Shouldn't the quantity be same all the time, as there is no change in the number of residents? (text in $\color{blue}{\textrm{blue}}$ ) And how to solve the extension of the problem? Can someone explain the solution or provide a different and easier solution? Thanks in advance.","['contest-math', 'recreational-mathematics', 'combinatorics', 'discrete-mathematics']"
4124584,How to show graphically that $\sum_i(1/3)^i$ goes to $1/2$,"As everybody knows, it is very easy to show: $$\sum_{i=1}^\infty \frac{1}{2^i} = 1$$ As follows: The coloured parts always show $\frac{1}{2^i}$ and it's easy to see they all come together to fill the entire square. Does anybody know a similar drawing to show that: $$\sum_{i=1}^\infty \frac{1}{3^i} = \frac{1}{2}$$ Thanks in advance","['visualization', 'geometric-series', 'geometry', 'sequences-and-series']"
