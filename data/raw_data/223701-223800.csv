question_id,title,body,tags
4601317,Non-trivial intersection of two cyclic groups,"Consider the additive group $(\mathbb{Q},+)$ and let $p,q\in\mathbb{Q}\setminus\{0\}$ . Show that for the cyclic groups $\langle p\rangle$ and $\langle q\rangle$ , we have that $$
\langle p\rangle\cap\langle q\rangle\neq\{0\}.\tag{1}
$$ I think I am only missing the very last step. I know that there exist $k,\ell\in\mathbb{Z}$ with $k,\ell\neq 0$ such that $$
kp=\ell q.\tag{2}
$$ What I am missing is the reason why this implies $(1)$ . I only see that $(2)$ implies $p=\frac{\ell}{k}q$ and $q=\frac{k}{\ell}p$ , respectively.
Hence, I get $$
\langle p\rangle = \{p^n: n\in\mathbb{Z}\}=\left\{\left(\frac{\ell}{k}\right)^n q^n: n\in\mathbb{Z}\right\},\qquad\langle q\rangle=\{q^n: n\in\mathbb{Z}\}=\left\{\left(\frac{k}{\ell}\right)^n p^n: n\in\mathbb{Z}\right\}
$$","['group-theory', 'abstract-algebra', 'cyclic-groups', 'rational-numbers']"
4601370,"If an invertible function approaches infinity at infinity, does its inverse approach infinity at infinity?","Prove or disprove: if $f(x)$ is defined on $ℝ$ (not necessarily continuous) and has an inverse and $\lim_{x→∞} f(x)=∞ $ then $\lim_{x→∞}f^{-1}(x)=∞ $ . I think it's true. I tried using Heine's theorem:
We know that $\lim_{x→∞}f(x)=∞ $ , this means that for every sequence $x_n→∞$ we have $f(x_n)→∞$ . So if by way of contradiction we assume that $\lim_{x→∞}f^{-1}(x)≠∞ $ , this means that there exists a  sequence $y_n→∞$ such that $\lim_{n→∞}f^{-1}(y_n)≠∞ $ but now I'm stuck because it doesn't give me any information about $f(x)$ .","['limits', 'calculus', 'examples-counterexamples', 'real-analysis']"
4601372,Is it true that power series with infinite radius of convergence are at most exponential?,"Suppose I have a real power series $P(x)=\sum_{n=0}^\infty x^n/a_n$ with $a_n>0$ which has infinite radius of convergence (ROC). Is it true that I can find constant $c>0$ such that $|P(x)|\leq e^{c(1+|x|)}$ for every $x\in\mathbb R$ ? The answer is obviously true if we consider the exponential power series. In a sense, is the exponential power series the ""worst possible"" power series with infinite ROC? All I have been able to say till now is that $a_n^{1/n}\to 0$ if the ROC is infinite. So given any $x$ , I can say there exists $N_x\in\mathbb N$ such that $a_n>(2|x|)^n$ for all $n>N_x$ . Hence $$\sum_{n=0}^\infty \dfrac{|x|^n}{a_n}\leq\sum_{n=0}^{N_x}\dfrac{|x|^n}{a_n}+\dfrac{1}{2^{N_x}}\leq \sum_{n=0}^{N_x}\dfrac{|x|^n}{a_n}+1$$ Now I need to know (probably) smething about $a_n$ and $N_x$ . I am stuck at this point, and any help will be appreciated. Thanks in advance!","['power-series', 'sequences-and-series', 'real-analysis']"
4601394,"Understanding a Math ""Meme""?","I recently saw this ""Math Meme"": Loosely speaking, I think I understand the humor - even though some functions may appear to look continuous on paper, they are not mathematically continuous as such. As an example, I think the ""Step Function"" ( https://en.wikipedia.org/wiki/Step_function ) is an example, as I can draw this without lifting my pen/pencil, but is clearly not defined at certain points. Is this a valid example with regards to this math ""meme""? If not, can someone please help me come up with one (e.g. perhaps ""Absolute Value Function""?) In general: Is there any particular relevance of the function that they have drawn in this picture (i.e. function plotted in red)? Is this an example of a discontinuous function? In general, are functions like $$f(x) = x \cdot \sin{\left(\frac{1}{x}\right)}$$ considered discontinuous since at $x=0$ this function is ""not defined"" (i.e. infinity) - but nonetheless, I can still draw it without lifting my pen? Thanks! Note: R Simulation # Define function
f <- function(x) x*sin(1/x)

# Plot function
plot(f, xlim = c(-5, 5), ylim = c(-5, 5), xlab = ""x"", ylab = ""y"")

# Add vertical lines at x = 0 and x = (2*k+1)*pi
abline(v = c(0, pi, -pi, 3*pi, -3*pi), lty = 2) Warning message:
In sin(1/x) : NaNs produced","['calculus', 'functions']"
4601405,Comparing the inequalities of Azuma and Chernoff,"Let $n$ be a positive integer and let $p = p(n) \in (0, 1)$ . Let $X$ be the
sum of the i.i.d. random variables $Y_1,\ldots, Y_n$ , which are $1$ with probability $p$ and $0$ with probability $1-p$ . Define a martingale $X_0,\ldots, X_n$ that satisfies $X_0 = \mathbb{E}[X]$ and $X_n = X$ . Compare the bound resulting from Azuma's inequality $$\mathbb{P}[X > \mathbb{E}[X] + t]$$ with the bounds that our two versions of Chernoff's inequality give us. Which one is better? Does the answer depend on the choice of $p(n)$ and $t$ ? I will write out the inequalities as we did them in the lecture: Azuma: Let $(X_0, \ldots, X_n)$ be a martingale with $X_0 = 0$ and $\lvert X_i - X_{i-1} \rvert \le 1 \quad (\forall 1 \le i \le n)$ . Then for any $t > 0$ it holds $$\mathbb{P}[X_n \ge \mathbb{E}[X]+t] \le \exp\bigg(-\frac{t^2}{2n} \bigg)$$ Chernoff 1: Let $X \sim Bin(n,p)$ . Then for any $t > 0$ it holds $$\mathbb{P}[X \ge \mathbb{E}[X]+t] \le \exp\bigg(-\frac{t^2}{2(\mathbb{E}[X]+t/3} \bigg) = \exp\bigg(-\frac{t^2}{2(np+t/3)} \bigg)$$ Chernoff 2: Let $X \sim Bin(n,p)$ with $\sigma^2 := \mathbb{V}[X]$ . Then for any $t > 0$ it holds $$\mathbb{P}[X \ge \mathbb{E}[X]+t] \le \exp\bigg(-\frac{t^2}{2(\sigma^2+t/3} \bigg) = \exp\bigg(-\frac{t^2}{2(p(1-p)+t/3)} \bigg)$$ So we need to compare the terms $2n, 2(np+t/3)$ and $2(p(1-p)+t/3)$ .  Since $n \ge 1$ it is clear that $$2(np+t/3) \ge 2(p(1-p)+t/3).$$ On the other hand I can see that for $p(n) \rightarrow 1$ and , $6n(1-p) \le t$ , which I obtained by reordering $2n \le 2np +t/3$ , we have $$2(np+t/3) \ge 2n$$ Similarly I got that for either $6n(1-p) \ge t$ or $p(n) \rightarrow 0$ together with $t \le 3n$ we have $$2(np+t/3) \le 2n.$$ However, I am wondering if a more sophisticated comparsion is possible.","['martingales', 'probability-theory', 'probability', 'upper-lower-bounds']"
4601431,How to evaluate $\int_0^{\pi/2} x^2 \ln^2(2\cos{x}) \mathrm{d}x$,"In this post , It is mentioned that $$ \int_0^{\pi/2} x^2 \ln^2(2\cos{x}) \mathrm{d}x = \frac{11 \pi}{16} \zeta(4) $$ is easy to evaluate. I thought it by doing integration by parts but if I assume $x^2$ as $1st$ function and $\ln^2(2\cos{x})$ as $2nd$ function but problem with this is that, We can't integrate $\ln^2(2\cos{x})$ directly. I also tried to use Fourier series of $\ln(2\cos{x})$ $$ \ln(2 \cos(x)) = \sum_{n=1}^\infty \frac{(-1)^{n-1}}{n} \cos(2 n x)$$ But it didn't work as well. Please guide me how to solve it ?
Thank you very much!!","['integration', 'calculus', 'zeta-functions', 'logarithms']"
4601452,Martingale and linear function,"Let $B$ be a $q$ -dimensional Brownian motion, $f:\mathbb{R^q} \to \mathbb{R}$ be continuous . We suppose that $(f(B_u))_{u \in \mathbb{R}_+}$ is a martingale. Prove or disprove: $f$ is linear, that is there exist $c_0,c_1,..,c_q \in \mathbb{R}$ such that for all $(x_1,...,x_q) \in \mathbb{R}^q,f(x_1,...,x_q)=c_0+\sum_{k=1}^qc_kx_k.$ I know how how to prove the case $q=1$ using stopping times: Show that $f(B_t)$ is a martingale iff $f(x)=a+bx$. What about $q>1$ ?","['stochastic-analysis', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4601477,Efficient half-angle formula for both sine and cosine when both are available,"Given a unit vector $\begin{bmatrix}\cos\theta\\\sin\theta\end{bmatrix}$ I seek to efficiently find any vector parallel (or anti-parallel) to $\begin{bmatrix}\cos{\frac{\theta}{2}}\\\sin\frac{\theta}{2}\end{bmatrix}$ . One way could be to use the half-angle identities to calculate the components of the new vector. However this would require calculating two square roots. An alternative I considered is to simply add $\begin{bmatrix}1\\0\end{bmatrix}$ to the original vector $\begin{bmatrix}\cos\theta\\\sin\theta\end{bmatrix}$ . While this is efficient and works in almost all cases, it runs into numerical problems when $\theta$ is equal to or near to $\pi$ . When both $\cos\theta$ and $\sin\theta$ are available, is there an identity that could efficiently provide the required vector (e.g. by calculating only one square root)?","['numerical-linear-algebra', 'trigonometry', 'vectors']"
4601493,How can I solve this Cauchy equation $(x - 4)^{2}y'' - 5(x - 4)y' + 9y = 4 - x$?,"I am trying to solve the following Cauchy-Euler equation $$(x - 4)^{2}y'' - 5(x - 4)y' + 9y = 4 - x$$ My first step is substituting $x - 4$ by $t$ and the equation becomes $$t^{2}y'' - 5ty' + 9y = -t$$ before finding the value of $m$ I need to use the chain rule so the equation becomes $\mathrm{d}x/\mathrm{d}t$ .  So I need help in this step. I know the chain rule but i don't know how to apply it and use it in this type of equations, so any suggestions?","['calculus', 'derivatives', 'ordinary-differential-equations']"
4601510,"The number of placement of $n$ pieces on the chessboard of $n\times n$, such that the piece $k$ cannot be placed in the $k$-th row nor $k$-th column","Considering the chessboard of $n\times n$ , we now want to put n pieces $1,2,\cdots, n$ on this chessboard, and satisfy that the piece $k$ cannot be placed in the $k$ -th row nor $k$ -th column, and each grid can only place one piece at most. How many kinds of placement are there? I calculate the answer with inclusion-exclusion principle the Mathematica code is: glist = RecurrenceTable[{g[
       i] == (2 n - 1) g[i - 1] + (-2 i + 2) g[i - 2], 
     g[2] == 4 n^2 - 4 n - 1, g[1] == 2 n - 1}, g, {i, 1, 10}] // 
   FullSimplify // Expand
answer[n_] := 
 n!*Binomial[n^2, n] + 
  Sum[(-1)^i*Binomial[n, i]*(g /. {g -> glist[[i]]})*
    Binomial[(n^2 - i), (n - i)]*(n - i)!, {i, 1, n}]
Table[answer[i] /. {n -> i}, {i, 1, 10}] $$
\{0,1,52,4737,718656,162509785,51209676372,21445634148225,11519808468594976,7721569549966334481\}
$$ Someone says this question can also be solved with generating function. $$
n ! \cdot\left[x^n\right](1+x)^{n^2} \cdot \exp \left(\frac{-2 n x}{1+x}+\frac{x}{(1+x)^2}\right)
$$ the following mma code verified the correctness of the formula. Flatten@Table[
  i!*SeriesCoefficient[(1 + x)^{n^2}*
      Exp[-2 n*x/(1 + x) + x/((1 + x)^2)], {x, 0, i}] /. {n -> i}, {i,
    1, 10}] The key word of this approach is probably: the inclusion-exclusion principle, the Mobius equation/Mobius inversion of partition lattice , Weisner's method . My question is: what are the detailed steps of this approach?","['chessboard', 'inclusion-exclusion', 'combinatorics', 'generating-functions']"
4601532,A pen-and-paper proof for a matrix implication.,"Suppose $A = \begin{bmatrix} x & 1\\ y & 0\end{bmatrix}, B = \begin{bmatrix} z & 1\\ w & 0\end{bmatrix}$ , for $x,y,z,w \in \Bbb{R}$ . I have observed by considering many  examples of $x,y,z,w$ that: If all the eigen values of $A^2B$ and $AB^2$ are less than one in absolute value $\implies$ $\det(AB+A+I)<0$ and $\det(BA+B+I)<0$ is not possible. OR alternatively, If all the eigen values of $A^2B$ and $AB^2$ are less than one in absolute value $\implies$ $\det(AB+A+I)\ge 0$ OR $\det(BA+B+I)\ge 0$ I wonder how to prove it actually? A computational proof using computer package was shown in https://mathoverflow.net/questions/435267/proof-of-a-matrix-implication/435689#435689 But I am wondering about formal or analytical proof for this question which can be done using pen paper. EDIT The case of $y=x$ or $z=w$ is covered by Andreas as an answer below. The only case that remains to show is whether the conjecture still holds for $y\neq x$ or $w \neq z$ .","['contest-math', 'inequality', 'determinant', 'linear-algebra']"
4601538,Determine if a point is inside a polygon made of perfect arcs,"I was wondering if there is a simplified way to determine if a point lies inside a polygon shape if the shape is represented only by circular arcs (not bezier or other complicated curves). My current approach is dividing the curves into small line segments and checking the winding number - which is a common approach although requires thousands of segments for very large curves and high accuracy. But given i have arcs I am wondering if i can simplify this checking. I provided a sample image of a shape made of arcs (each arc alternating in color). Some current observations I found when testing but doesn't succeed for all cases: If the arcs are ordered for an overall CCW direction of the complete polygon, then you know you're in the shape if you overlap an arc sector and the given arc is also CCW. You're also not in the shape if you overlap an arc that is going CW (in the case of the concave area of the shape). However this doesn't quite cover everything after some further testing because you can overlap a CCW and a CW arc area at the same time (see image below): Wondering if any one has some insights on how this might be solvable to avoid using thousands of line segments to get a winding number which is an expensive calculation to do.","['geometry', 'polygons', 'algorithms']"
4601558,Azuma's inequality for a simple case of Polya's urn,"Suppose that an urn contains one red ball and one blue ball. A ball
is drawn from the urn uniformly at random. After that, the ball is put back into the
urn and another ball of the same colour is added to the urn. This process is repeated $n$ times. Denote by $X_n$ the proportion of red balls in the urn after these $n$ steps (i.e.
number of red balls divided by total number of balls). Use Azuma’s inequality to prove that $$\mathbb{P}\bigg[ \bigg\lvert X_n - \frac{1}{2} \bigg\rvert\ge \varepsilon \bigg] \le 2\exp\bigg(-\frac{6\varepsilon^2}{2\pi^2-15} \bigg)$$ Remark: In class I learned the following version of Azuma's inequality: Let $(X_0, \ldots, X_n)$ be a martingale with $X_0 = 0$ and $\lvert X_i - X_{i-1} \rvert \le 1 \quad (\forall 1 \le i \le n)$ . Then for any $t > 0$ it holds $$\mathbb{P}\bigg[\lvert X_n - \mathbb{E}[X_n]\rvert \ge \varepsilon \bigg]   \le 2\exp\bigg(-\frac{\varepsilon^2}{2n} \bigg)$$ I already know that $(X_n)_n$ is a martingale that fullfills the conditions for Azuma's inequality and has the property that $\mathbb{E}[X_n] = 1/2$ .  However I do not see where the term $-\frac{6\varepsilon^2}{2\pi^2-15} $ should come from. Could you please give me a hint?","['polya-urn-model', 'upper-lower-bounds', 'martingales', 'probability-theory', 'probability']"
4601559,Is there another way of solving this binomial problem?,"The problem is as follows:  in $$(1+x)^n$$ find $n$ such that the fifth coefficient is $70$ and the seventh coefficient equals $28$ .
I put $$ \binom{n}{4}=70 $$ $$\binom{n}{6}=28 $$ I ended up with two polynomial equations: $$ n^4-6n^3+11n^2-6n-1680=0  $$ $$ n^6-15n^5+85n^4-225n^3+274n^2-120n-20160=0 $$ and the solution to this binomial is $n=8$ .
Is there another (simpler) way to solve this problem?","['binomial-coefficients', 'combinatorics']"
4601570,"Why Does The ""Bootstrap Method"" Work?","Consider the ""Bootstrap Method"" ( https://en.wikipedia.org/wiki/Bootstrapping_(statistics) ) in Probability and Statistics. As I understand, the Bootstrap Method is a useful procedure that can be used to estimate the ""empirical distribution"" of some ""statistic"" (e.g. mean) for some observed data. In the Bootstrap Method: First, we take a random sample (e.g. 70%) of the collected data and calculate the ""statistic"" from this random sample. Next, repeat this above step many times. Each time, you will have a ""version"" of this ""statistic"" corresponding to each random resample. Finally, rank all these ""versions"" from smallest to largest - by taking the ""version"" corresponding to the 5th percentile and the 95th percentile from this ranked list, you can effectively place ""Confidence Intervals"" on this ""statistic"". The Bootstrap Method is said to be particularly advantageous as it allegedly ""works"" in many otherwise difficult circumstances where a closed-form distribution for the ""statistic"" of interest might not be readily known or available. Our professor demonstrated that the Bootstrap Method does in fact work, and showed us some examples with randomly simulated data  where the closed-form distributions for the ""statistic"" of interest is known - and it is easy to compare the solutions generated from the Bootstrap Simulations and the analytical answer. But in the back of my mind, I always play ""Devils Advocate"" and wonder - how do I know that the Bootstrap ""just happens"" to work in this example, and perhaps in the next example, we might not be as lucky. I tried asking one of my professors as to why exactly the Bootstrap Method works - but the professor replied that its because of the Law of Large Numbers ( https://en.wikipedia.org/wiki/Law_of_large_numbers ). While this is probably true, I was hoping to find a more ""detailed reason"" as to why the Bootstrap Method works. As an example, (my understanding of) the Law of Large Numbers applies in situations where you have access to the entire population and can you can resample this population an infinite number of times - whereas in situations where the Bootstrap is used, you have a (possibly imperfect) sample from the original population, and can only resample this sample of the population. This makes me a bit unsure if extending the use of the Law of Large Numbers to justify the correctness of the Bootstrap is legitimate. I found what seems to be a very informative University Lecture on this subject ( https://www.stat.cmu.edu/~larry/=sml/Boot.pdf ) in which proofs are even provided - but I don't think my knowledge of mathematics is currently adequate enough to understand this proof by myself. I was hoping that perhaps someone here might be able to walk me through a simplified version of this proof - or perhaps provide another simplified version of a similar proof which demonstrates why the Bootstrap Method ""works"". Thanks!","['statistics', 'probability']"
4601578,Decomposition of a random variable into independent random variables,"Let us consider two random variables $X$ and $Y$ that are really random: $$
H(X) > 0,\\
H(Y) > 0,
$$ where $H(\cdot)$ denotes the Shannon entropy. Suppose that a realization of $X$ only has a part of information of $Y$ , as follows: $$
H(X\mid Y) = 0.
$$ Then, is there a random variable $Z$ responsible for only holding the rest of information of $Y$ ? In particular, I would like to know whether there is a random variable $Z$ satisfying the following properties. The total amount of information of $(X,Z)$ 's realization is equivalent to $Y$ 's realization: $$H(Y)=H(Z)+H(X);$$ Once realizations of $X$ and $Z$ are observed, a observer knows all about $Y$ 's realization: $$H(Y\mid X,Z) = 0,$$ and vice versa: $$H(X,Z\mid Y) = 0.$$ (added 2022/12/20)
The above-stated conditions can be visually rephrased into the following I-diagram . Here, $X$ and $Y$ share the whole regions covered by the smaller and larger circle. The region of $Z$ is the larger circle not covered by the smaller circle. If this is a famous fact, I'd also like to know the name of this fact like the name of theorem in order to learn more by googling. Example Suppose $Y$ is a two-bit random variable and $X$ is the bit of either side of those bits. Then, the above properties are clearly satisfied by $Z$ that is the other side of $Y$ 's bit. Concretely, assume that we have two coins, say, 1 cent and 5 cent coins. $Y$ represents an event of tossing those two coins, and $X$ expresses a result of tossing the 1 cent coin. if $Z$ represents the tossing result of the 10 cent coin, $Z$ only holds the rest of the $Y$ 's information not covered by $X$ .","['information-theory', 'probability-theory', 'probability', 'random-variables']"
4601656,Rotating a vector around an axis so that it falls onto the specified plane,"I'm trying to refactor/solve the following equation for theta if $ \vec{v} , \vec{r} $ and $ \vec{n} $ are known vectors: $ \left(\vec{v} \cdot \cos\left(\theta\right) + \left(\vec{r} \times \vec{v}\right) \cdot \sin\left(\theta\right) + \vec{r} \cdot \left(\vec{r} \cdot \vec{v}\right) \cdot \left(1 - \cos\left(\theta\right)\right)\right) \cdot \vec{n} = 0 $ If this looks familiar, it's probably because it is - it's a combination of the Rodrigues equation for rotating a vector (in this case, $ \vec{v} \,$ ) around an axis $ \left(\vec{r} \right)$ and a simple dot operation to determine the distance of the resulting vector to a given plane defined by a normal $ \left(\vec{n}\right) $ . In essence, I'm trying to solve for $ \theta $ such that the vector $ \vec{v} $ will be rotated onto plane $ \vec{n} $ if possible. Near as I can tell, there should be either $ 0, 1 $ or $ 2 $ solutions to this problem - making me think that it might be possible to refactor this into something that can be solved by a quadratic equation. Alternatively (and I have no idea how this would work) - I'm guessing it might also be possible to somehow ""project"" $ \vec{v} $ around the axis $ \vec{r} $ so that it falls on the plane with a normal defined by $ \vec{n} $ and then calculate the angle between the original vector and the projected vector $ \left(\vec{s}\right) $ , but I'm not really sure how you would do that. How can I solve this problem?","['vectors', 'geometry', 'rotations']"
4601669,What is the difference between smooth manifolds and differential geometry?,"I am planning to study geometry (I know this is very vague, but I can't exactly explain what I mean, because I don't fully understand the distinction between various terms, which is the reason for this question). I'm confused by titles of books. As I understand, there are books which have ""Smooth Manifolds"" in their title, such as Lee's book Introduction to Smooth Manifolds . On the other hand, there are books which contain ""Differential Geometry"" in their title, such as Spivak's book A Comprehensive Introduction to Differential Geometry .
Looking at the table of contents, I don't really understand the difference between  books on ""smooth manifolds"" and books on ""differential geometry"". Could someone explain what the distinction is, and perhaps suggest what is the usual order of studying them? In other words, would you first study something which has ""differential geometry"" in the title, or ""smooth manifolds""? Similar question about difference between Riemannian geometry and differential geometry","['smooth-manifolds', 'differential-geometry']"
4601671,$(a^{-1})^{-1} = a$ in a groupoid: proof review,"Exercise on groupoids article : Show that in a groupoid, $$
  (a^{-1})^{-1} = a \,.
$$ Proof: Every element must have an inverse, and likewise each element’s inverse must itself have an inverse (e.g., $a^{-1}$ must have an inverse $(a^{-1})^{-1})$ , so $((a^{-1})^{-1} a^{-1}) a = a$ , hence $(a^{-1})^{-1} ((a^{-1}) a) = a$ and thus $(a^{-1})^{-1} = a$ . Is my proof correct?","['group-theory', 'solution-verification', 'groupoids']"
4601697,Topological manifold and a subset,"Let $A,B$ be two $n$ -dimensional topological manifolds with $\varnothing \neq A \subset B$ . Assume that $A,B$ are closed manifolds. Is it possible that $A \cong B$ and $A \neq B$ ?","['general-topology', 'differential-topology', 'algebraic-topology']"
4601705,"""Derivative"" of function on set of measures","Across my studies I came across an object which could be thought of as a ""derivative"" of a function defined on a set of measures and I was looking for references to the matter (if any). Specifically, let $\mathscr{M}$ be the set of (finite) measures on $\mathbb{R}^n$ and let $P:\mathscr{M}\to\mathbb{R}$ be an arbitrary function. For every $\eta\in\mathscr{M}$ and $x\in \mathrm{Supp}(\eta)$ we can consider the limit $$
\lim\limits_{\varepsilon\to 0}\frac{P(\eta_{\varepsilon})-P(\eta)}{\eta(B_{\varepsilon}(x))}
$$ where $B_{\varepsilon}(x)$ is the open ball of radius $\varepsilon$ centered in $x$ and $\eta_{\varepsilon}$ is the measure defined by $\eta_{\varepsilon}(A)=\eta(A)+\eta(A\cap B_{\varepsilon}(x))$ . I am looking for references on the study of the previous limit (properties, conditions for its existence, etcetera) or another way of looking at it. I understand one could simply re-express the limit through a function $f:[0,\infty)\to \mathbb{R}$ given by $f(\varepsilon)=P(\eta_{\varepsilon})$ and then multiply/divide by the Lebesgue measure of $ B_{\varepsilon}(x)$ to sort of try to see this as a normal derivative multiplied by a Radon-Nikodym derivative, however I am not aware of the details necessary for carrying this out and it seems to me that work like this has already be done. Thanks for any help you might provide.","['measure-theory', 'geometric-measure-theory', 'reference-request']"
4601734,Help with existence & uniqueness proof involving sets.,"Im currently reading Velleman's How To Prove It. Exercise 6. (a) Asks to prove the following: Let $U$ be any set. Prove there is a unique $A \in \mathcal{P}(U)$ such that for every $B \in \mathcal{P}(U)$ , $A \cup B = B$ Im a little puzzled with the uniqueness solution that the book proposes. It goes like this: Proof Let $A = \varnothing \in \mathcal{P}(U)$ . Then clearly for any $B \in \mathcal{P}(U)$ , $A \cup B = \varnothing \cup B = B$ To see that $A$ is unique, suppose $A' \in \mathcal{P}(U) $ & for all $B \in \mathcal{P}(U)$ , $A' \cup B = B $ . Then in particular taking $B = \varnothing$ , we can conclude that $A' \cup \varnothing = \varnothing$ . But clearly $A' \cup \varnothing = A'$ so we have, $A' = \varnothing = A$ $\square$ What's bugging me is that I don't seem to get why is it valid to set $B = \varnothing$ . It seems to me that we're only showing that $A' = \varnothing$ only when $B = \varnothing$ , but $B$ is supposed to be arbitrary. Why is the proof still valid?","['elementary-set-theory', 'proof-explanation', 'first-order-logic']"
4601772,Why piecewise functions have open curly braces and no closing bracket? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question For example, the mod function $$|x|=\begin{cases}-x,&{\text{if }}x<0,\\+x,&{\text{if }}x\geq 0.\end{cases}$$ Why is a closing bracket not there? And who started this notation? Also, please provide some kind of logic and evidence. Thank you for your effort and time!","['notation', 'continuity', 'functions']"
4601778,Real analysis: Dyadic squares and rigorous proof writing,"I have been working my way through an exercise from Pugh's ""Real Mathematical Analysis"" which asks me to prove that two dyadic squares in $\mathbb{R}^2$ of same size either intersect along a common edge, have a common vertex, are the same or are disjoint. It seems intuitively obvious to me (and I have made some progress in the proof as well), but I struggle with rigorously writing it down. Here's how far I got. Intuition/Explanation Dyadic cubes are fairly easy to explain: a dyadic cube in $\Bbb R^m$ is the cartesian product of $m$ intervals of same length, with the catch that the interval can be written as $[\frac{a}{2^k}, \frac{a+1}{2^k}]$ , with $a\in\mathbb{Z}$ and $k\in\mathbb{N}$ including $k = 0$ . So for example, $[\frac{a}{2^k}, \frac{a+1}{2^k}]\times[\frac{b}{2^k}, \frac{b+1}{2^k}]$ would be an example of a planar dyadic cube (dyadic square) as the length of both intervals is the same, namely $\frac{1}{2^k}$ . My idea has been that if there was a common point $x$ in any 2 different dyadic squares which is neither on a common edge nor vertex, then I could show that one of the squares would have their edge along a ""coordinate line"" which is not a dyadic number. To show what I mean, I attached the image below: Suppose the green square was an ordinary dyadic square. My assumption is that if there was a square such as the red one intersecting the green one, then there's no way coordinate line A (I don't know the right term I could use to describe the line) has a natural number as its value (again I don't know how to say this in mathematical terms, as that would imply that there was a integer between two adjacent integers. My question is: How could I put this into words? My progress so far Suppose there are 2 dyadic squares A, B of same size such that they are not the same, do not intersect along a common line or vertex. Let $M = \{x\in \mathbb{R}|x\in A \land x\in B\}$ . Since they intersect, they can intersect only in one of 2 possible ways (both drawn out below): Let $x$ be any point in $M$ . Consider the closest square side lying above $x$ . This side is between two dyadic rationals, namely $\frac{a}{2^k}$ and $\frac{a+1}{2^k}$ . This side lying between these two dyadic rationals would also be (by hypothesis) a dyadic rational (described in picture as $\frac{c}{2^k}$ , implying that $\frac{a}{2^k} < \frac{c}{2^k} < \frac{a+1}{2^k}$ , from which $a < c < a+1$ would follow, which is not possible for any $c\in \mathbb{Z}$ It is clear to me that there are some large gaps in this proof: How could I show that the only way the two squares could intersect is in the ways I've drawn out below? How could I show that it follows that there is a line lying above any point in the set M? Overall, how do I make this proof more ""mathematical""? Lastly, I ask for some advice - I often catch myself having an idea such as this one but then greatly struggle with putting the proof together. How do I get better at this / practice this skill in a more targeted fashion?","['proof-writing', 'solution-verification', 'geometry', 'real-analysis']"
4601853,Differentiating The Law of Cosines,"Alright, I've got a stupid question (maybe). I took the equation given by the law of cosines and I differentiated it with respect to some other parameter. Right, so we begin be considering a triangle with vertices $A,B,$ and $C$ . In $\triangle ABC,$ we have $|AB|=a,|BC|=b,$ and $|CA|=c.$ The angle opposite to $a$ is $\alpha,$ the angle opposite to $b$ is $\beta,$ and the angle opposite to $c$ is $\gamma.$ I begin by considering the cosine rule. We have that $a^2+b^2=c^2+2ab\cos{(\gamma)}.$ Now, this is the weird-ish part. I differentiated with respect to some other parameter, say $t$ . So, we get $2[a\frac{da}{dt}+b\frac{db}{dt}]=2[c\frac{dc}{dt}-ab\sin{(\gamma)}\frac{d\gamma}{dt}+a\cos{(\gamma)}\frac{db}{dt}+b\cos{(\gamma)}\frac{da}{dt}].$ Cumbersome, I know. But, consider what happens when the sides $a,b,$ and $c$ don't change as $t$ changes. Most of the derivatives vanish, and we get $-ab\sin{(\gamma)}\frac{d\gamma}{dt}=0.$ Now, for a triangle, $a,b,$ and $\sin{(\gamma)}$ cannot be $0.$ This means that the angle $\gamma$ remains constant as $t$ varies. Similar arguments can be used for $\alpha,$ and $\beta.$ I know that I simply could've used $SSS$ congruency or something like that to prove that if all $3$ sides of a triangle remain constant, then so do the angles, but is this line of reasoning correct? Further, if we do something similar with the law of sines, we don't get a neat argument that shows that the angles remain constant as the sides do. Is there any way to rectify that? In particular, we get $\tan{(\beta)}\frac{d\alpha}{dt}=\tan{(\alpha)}\frac{d\beta}{dt},$ when the sine rule is applied to $a,b,\alpha,$ and $\beta,$ and differentiated. Nothing about this tells me that either of the derivatives are $0$ .","['geometry', 'calculus', 'triangles', 'trigonometry', 'derivatives']"
4601860,"If $f$ is bounded away from $0$, are the functions that are close to $f$ in $L^2$ bounded away from zero as well?","Consider a probability space $(\Omega,\mathcal F,\mathbb P)$ , and let $X$ be a $\mathbb R^d$ -valued random variable defined on that space with distribution $\rho$ (i.e. $\rho(A) = \mathbb P(X\in A)$ for all $A\in\mathcal B(\mathbb R^d)$ ). Furthermore, assume that $X\in \mathcal X$ with probability $1$ , where $\mathcal X$ is a compact subset of $\mathbb R^d$ . Now let $f^*:\mathcal X\to\mathbb R$ be a real-valued and continuous function such that $$\mathbb P\left(|f^*(X)|>\delta\right) = 1 $$ For some constant $\delta > 0$ . In words, this means that $f^*$ is such that $f^*(X)$ is almost surely a margin of $\delta$ away from zero. My question is the following : assuming that I can find a continuous function $\hat f:\mathcal X\to\mathbb R$ such that $\|\hat f - f^*\|_{L^2} \le \varepsilon$ for a small $\varepsilon>0$ (here, $\|\cdot\|_{L^2}$ refers to the $L^2(\rho)$ norm), is it possible to get a lower bound of the form $$\mathbb P\left(|\hat f (X)|>\alpha(\delta,\varepsilon)\right) \ge 1-\beta(\delta,\varepsilon)$$ For some positive $\alpha$ and non-negative $\beta$ which will depend on $\delta$ and $\varepsilon$ ? The intuition is that, if $\hat f$ and $f^*$ are ""very close"", then with high-probability $\hat f$ should be bounded away from $0$ , for some (perhaps smaller) margin $\alpha$ . I do not know however how to formalize that intuition, and the fact that the approximation is in the $L^2$ sense rather than $L^\infty$ makes the problem even more challenging. Alternatively, could there be some non-trivial assumptions I could make on $\rho$ to guarantee that $\mathbb P\left(|\hat f (X)|>\alpha(\delta,\varepsilon)\right) = 1 $ ?","['measure-theory', 'approximation-theory', 'real-analysis', 'inequality', 'probability-theory']"
4601886,To Evaluate $\sum_{r=1}^\infty\frac{1}{r^2}$ using limit as a sum method,"I have recently learnt to find some limits and values of some series using integration. This is called as evaluating series and limits using limit as a sum in my textbook. Now, I tried to derive this series using what I learnt. (It is not part of my textbook. I was just doing this for my own fun): $$\sum_{r=1}^\infty\frac{1}{r^2}= \frac{π^2}{6}$$ Here is what I did : Let $S = \lim_{n\to\infty}\sum_{r=1}^n\frac{1}{r^2}$ Using change of variable of $n\to n^2$ and $r\to r^2$ , call $n^2 = x$ and $r^2 = y$ , we have : $$S=\lim_{x\to\infty}\frac{1}{x}\sum_{y=1}^x\frac{x}{y}$$ Now using limit as a sum formula, we have : $$S = \int_0^1\frac{1}{x}dx=\infty$$ Of course this is wrong. Where did I went wrong? Can you provide a correct proof of this series using this limit as a sum integration method ? (There is a famous question of this series on this site but on the answers to that, I didn't find a solution involving limit as a sum method. Hence my question is different.)","['integration', 'limits', 'calculus', 'sequences-and-series']"
4601942,Solve $y''+5y'+6y=e^{-3x}$,"so I took the auxiliary equation and solved it: $$m^2+5m+6=0$$ gave me $$ 
m=-2,-3$$ and gives the complementary/general solution $$c_1e^{-2x}+c_2e^{-3x}$$ so I took $$axe^{-3x}$$ as the guess
and got the particular solution as $$−xe^{−3x}$$ and adding that to the general solution gives $$c_1e^{-2x}+c_2e^{-3x}−xe^{−3x}$$",['ordinary-differential-equations']
4601986,Uniform tightness of sequence of stochastic integral wrt Brownian motion,"Given a sequence defined by $$
 X_{n,\varepsilon}(\theta) = \int_0^1 \varepsilon f(Y_{n,s},\theta)\chi_{\{s \leq \tau\}} dW_s
$$ where $W_s$ is a standard Brownian motion and $\tau$ is the time such that the process exceeds some $M >0$ and $Y_n \to Y$ as $n \to \infty, \varepsilon \to 0$ , what $Y$ is specifically isn't important to the question other than it is a deterministic process. Additionally, $f$ has a linear growth condition. So for fixed $M$ can show point-wise convergence to $0$ as $n \to \infty$ and $\varepsilon \to 0$ . Proof uses Chebyshev's inquality. Now, consider $$ 
\mathbb{P}(|X_{n,\varepsilon}(\theta)|\ge a) = \mathbb{P}(|X_{n,\varepsilon}(\theta)|^2\ge a^2) \leq \frac{1}{a^2}\mathbb{E}(|X_{n,\varepsilon}(\theta)|^2)
$$ So for sufficiently large $a$ can make this small, say $a >> M$ . This means I can conclude the sequence $(X_{n,\varepsilon}(\theta)$ is Uniformly Tight, correct? Where uniformly tight means
For every $\delta >0$ , there exists $K$ such that $$
\sup_{n,\varepsilon}\mathbb{P}(|X_{n,\varepsilon}(\theta)|\ge K) \leq \delta.
$$ This seems correct to me but I want to insure I am indeed correct.","['stochastic-integrals', 'solution-verification', 'probability-theory', 'stochastic-calculus']"
4601987,Find the value of $\lim_{n\rightarrow\infty}\int_{-\infty}^{\infty}e^{-x^{2n}}\:\:dx$,"Find the value of $$\lim_{n\rightarrow\infty}\int_{-\infty}^{\infty}e^{-x^{2n}}\:\:dx$$ I just searched on the internet and learned that the given integral given is a special integral. If we consider, $f(x)=e^{-x^{2n}}$ then we seen that $f(x)$ is an even function. Maybe it can help $?$ When I put $n=\infty$ in the $2n$ I got $e^{-x^{\infty}}$ How do I evaluate it $?$ As it's value depends on the valuenof $x$ . Any help is greatly appreciated. Apparently this is a MIT Integration Bee problem. But I ain't sure.","['integration', 'limits', 'calculus']"
4602005,Borel measure $\mu$ with $f'(a)$ an integral of $f$ w.r.t $\mu$,"I am learning measure theory, and I noticed that if we integrate a function $f : \mathbb{R} \to \mathbb{R}$ with respect to a dirac measure at $a \in \mathbb{R}$ , call it $\mu$ , then $$ \int f d\mu = f(a)$$ I am wondering how this generalizes, in particular I was wondering if there exits a borel measure (Possibly signed) $\mu$ such that $$f'(a) = \int f d\mu$$ for $f \in C^\infty(\mathbb{R}) $ Thank you in advance! My thoughts: I am honestly a little clueless, I am learning from Rudin's RCA and I was thinking that maybe we could use the Riez representation theorem, maybe if we instead take $f$ so that $f \in C^\infty(\mathbb{R}) $ and it is compactly supported we can use it, but I am not sure where the linear functional would come from.","['measure-theory', 'borel-measures', 'real-analysis']"
4602030,Expression for $ \left( \frac{d^2}{dx^2} + \frac{c}{x} \frac{d}{dx} \right)^k \phi(x)$,"Consider the following differential operator \begin{align}
D_c= \frac{d^2}{dx^2} + \frac{c}{x}  \frac{d}{dx}
\end{align} defined on $x>0$ for some given non-zero constant $c$ . Let $\phi(x)=\exp(-x^2/2)$ be the Gaussian function. For every positive integer $k$ , we are interested in characterizing \begin{align}
D^k_c \phi(x)
\end{align} i.e., repeated application of this transform. What I did First, note that \begin{align}
 \frac{d^k}{dx^k} \phi(x)= (-1)^k \operatorname{He}_k(x) \phi(x)
\end{align} where $\operatorname{He}_k(x)$ is the Hermite polynomial. Here is the expression for the first few expressions \begin{align}
D_c \phi(x)&=  \operatorname{He}_2(x) \phi(x)- \frac{c}{x}  \operatorname{He}_1(x) \phi(x)\\
&= \left(\operatorname{He}_2(x) - c \right) \phi(x)
\end{align} and (if correct) \begin{align}
D_c^2 \phi(x)= \left( \operatorname{He}_2(x)^2-c \operatorname{He}_2(x)-4  \operatorname{He}_1^2(x)+2 + c(2-( \operatorname{He}_2(x)-c))  \right) \phi(x)
\end{align} So the expression appears to be of the form \begin{align}
D_c^k \phi(x)= \text{Poly}(2k) \phi(x)
\end{align} where $\text{Poly}(2k)$ is a polynomial of degree $2k$ .  But I couldn't really find the expression for the polynomial. One more thing this operator looks like Bessel operator but is slightly different.","['hermite-polynomials', 'derivatives', 'ordinary-differential-equations']"
4602039,Solve the ODE $y'=(yy')'-y$,"Solve, or make any progress in reducing the ODE \begin{equation}
y' = (yy')'-y
\end{equation} on $x \in [0,1]$ . I have tried reducing the order of this ODE by introducing the coordinate $y'(x)=p(y)$ , so that $y''(x)=pp'(y)$ and I obtain the first order ODE \begin{equation}
p'(y) = \frac{p-p^2+y}{yp}
\end{equation} but can't seem to find a solution to this either. Thanks.",['ordinary-differential-equations']
4602106,Question 3 from Exercises 2.5.1 from F. Mary Hart - Guide to Analysis,The question tells you that $(a_{3n})^{\infty}_{n=1}$ $(a_{3n+1})^{\infty}_{n=0}$ $(a_{3n+2})^{\infty}_{n=0}$ all converge to a. The question asks you to prove $(a_n)^{\infty}_{n=1}$ converges. Intuitively this makes sense because the sub-sequence of every 3rd term converges. I know how to get from a sequence converging to its sub-sequence converging but I am not sure about the converse.,"['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4602119,Is every simple continuous curve on a plane a sub-curve of a simple continuous closed curve?,"Let $P:[0,1]\to \mathbb{R}^2$ be a simple continuous curve on the plane, i.e. $P$ is continuous and injective. Can we always extend it so that it is a sub-curve of a simple closed curve? It's equivalent to showing that $\mathbb{R}^2\backslash P((0,1))$ is path-connected. I am only interested in the two dimensional case, but feel free to present a proof with $\mathbb{R}^n,n\geq 2$ replacing $\mathbb{R^2}$ . The fact that $\mathbb{R}^2\backslash P([0,1])$ is path-connected should be useful, see The complement of a simple non closed curve is path connected? .","['connectedness', 'path-connected', 'real-analysis', 'general-topology', 'low-dimensional-topology']"
4602120,Deriving the equation of a parabola?,"Imagine you wanted to derive the equation of a parabola by imposing that all parallel rays that bounce on the parabola, end up in the foci of the curve. By using geometry and vectors, I arrived to something like: $$
F = (a, b) \\
-\sqrt{(a - x)^2 + (b - y(x))^2} = (a - x)y'(x) + y(x) + b \Rightarrow \\
(a - x)^2 y'(x)^2 + 2(a - x)y'(x)y(x) + 2(a - x)y'(x)b = a^2 + x^2 +2ax
$$ I've just imposing that the $\cos{\theta}$ has to be equal when the ray bounces (using dot products): This is far from what I was expecting: $$
y''(x) = k
$$ For $a = b = 0$ we have the following: $$
xy'(x)^2 - 2y'(x)y(x) = x
$$ This is a pretty intimidating equation. My question is: Is this line of reasoning correct? Is this strange looking equation right? Is there any other way of doing this more efficiently?",['ordinary-differential-equations']
4602183,"find local maximum/minimum of function $f(x,y)=x+y+4\sin x \sin y$.","I have this task: find local maximum/minimum of function $f(x,y)=x+y+4\sin x \sin y$ . Now I have found partial derivatives: $\frac{\partial f}{\partial x} = 1+4\cos x \sin y$ $\frac{\partial f}{\partial y} = 1+4\cos y \sin x$ $\cos x \sin y=-\frac{1}{4}$ and $\cos y \sin x =-\frac{1}{4}$ But from here I am having trouble finding stationary points, if anyone could help I would be grateful!","['multivariable-calculus', 'calculus']"
4602194,$\omega$-limit set in $\mathbb S^2$,"Let $X(x,y,z)=(-y+xz^2,x+yz^2,-zx^2-zy^2)$ be a vectorial field in $\mathbb S^2$ , computationally it is easy to see that the $\omega-$ limit set of any point other than the poles is the equator, but I don't know how to prove this mathematically, any ideas please?","['ordinary-differential-equations', 'dynamical-systems']"
4602208,Does the product rule imply the chain rule?,"Let $\mathbb{F}$ be a field, and consider $\mathbb{F}^\mathbb{F}$ as an algebra over $\mathbb{F}$ with the standard function multiplication. Let $D$ be a derivation on a subalgebra of $\mathbb{F}^\mathbb{F}$ closed under function composition that takes the identity function to the function that sends everything to $1,$ as in the answers to these two questions. Does $D$ necessarily satisfy the chain rule for arbitrary $\mathbb{F}$ ?","['vector-spaces', 'field-theory', 'abstract-algebra', 'linear-algebra', 'algebras']"
4602226,Solution of a second order non linear ordinary differential equation,"Consider the ordinary differential equation $$ \frac{d^{2}y}{dx^{2}} = \alpha \sinh{y},~~ y = \beta~~\text{along}~x=-\frac{1}{2},x=\frac{1}{2}~~\text{and}~~\frac{dy}{dx}=0~~\text{along}~x=0.$$ Here $\alpha, \beta$ are constants. Is there any standard method to solve these types of ODEs or we need to solve it by hit and trial method. Any help to solve this ODE will be appreciated.",['ordinary-differential-equations']
4602227,How does rerolling one of two dice affect the expected value?,"This problem came to my head this morning and now I've just been curious since.
Say you have two fair dice and you roll both of them after you must reroll the lowest die, if the dice are equal one gets chosen at random to be rerolled. What would be the expected outcome of this? Example: I roll two dice and get $ \left\{2,3\right\} $ because $ 2 \lt 3 $ I reroll the first die and then I get $ \left\{4,3\right\} $ . The expected value of two die rolls is $ 7 $ because you add all the outcomes then divide by $ 12 $ . However in this case there the data is negatively skewed therefore we have to times the outcomes by their chance of appearing. In this case the chances for each number are: Roll Outcome Chance $ 2 $ $ 1/216 $ $ 3 $ $ 4/216 $ $ 4 $ $ 9/216 $ $ 5 $ $ 16/216 $ $ 6 $ $ 25/216 $ $ 7 $ $ 36/216 $ $ 8 $ $ 35/216 $ $ 9 $ $ 32/216 $ $ 10 $ $ 27/216 $ $ 11 $ $ 20/216 $ $ 12 $ $ 11/216 $ The $ x $ in $ 216 $ comes from $ 6 $ cubed as we are effectively rolling three six-sided dice. Multiplying the rows together and then adding them we get $ 7.9\overline{2} $ as the expected value. Is this number correct? If not, how can I get the value? If so, is there a more mathematical way to prove this?","['statistics', 'dice', 'probability']"
4602252,Finding the radius of a triangle's incircle: confusions on scale factors,"The radius of the incircle of a triangle whose sides are $18$ , $24$ , and $30$ cms is : (a) $2$ (b) $4$ (c) $6$ (d) $8 $ Note that the radius of the incircle of the triangle is given by $\text{area}/\text{semi-perimeter}$ $P$ is the half of the perimeter or $(a+b+c)/2$ , so $P = (18+24+30)/2 = 36$ $A= \sqrt{(36(36−18)(36−24)(36−30)}= \sqrt{36×18×12×6} =216 \text{ cm.}^{2}$ Therefore, the radius of incircle of the triangle is $216/36$ , or 6cm. I simplified the lengths to $3$ $4$ $5$ instead of $18$ $24$ $30$ . So after solving I am getting $1$ cm. $^2$ as answer. I am not getting why I have to multiply it with $6$ at the end when I am finding both area and semi perimeter w.r.t. the ratios.","['trigonometry', 'triangles']"
4602290,Proving representability of algebraic group centralizer over a field,"$\newcommand{\ul}[1]{\underline{#1}}$ I'm stuck on a detail when trying to prove centralizers/normalizers/transporters of algebraic groups over a separably closed field are representable. Let $k = k_s$ be a separably closed field and $G$ a $k$ -group of finite type acting on a finite type $k$ -scheme $X$ . Let $W \subset X$ a (geometrically) reduced separated $G$ -stable closed subscheme and $\alpha_w : G \to W$ be the orbit map for each $w \in W(k)$ . We would like to show that the functorial centralizer $$
\ul{Z}_G(W) : S \longmapsto \{g \in G(S) : g.w = w \;\; \forall w \in W(S'),\,S'/S\}
$$ is represented by the candidate subgroup $$
Z'_G(W) := \bigcap_{w \in W(k)} \alpha_w^{-1}(w) : S \longmapsto \{g \in G(S) : g.w_S = w_S \;\; \forall w\in W(k)\}.
$$ Clearly (1) we have a subfunctor $\ul{Z}_G(W) \subseteq Z'_G(W)$ , and (2) they are equal on $k$ -points: since $W$ is generically smooth $k$ -points are dense, and since it is separated the morphisms $g$ and $\operatorname{id}_W : W \to W$ which agree on a Zariski-dense subset are equal. How do I show that $\ul{Z}_G(W)(S) = Z'_G(W)(S)$ for an arbitrary $S$ ? For general $S$ , the argument above gives two morphisms $g, \operatorname{id}_{W_S} : W_S \to W_S$ which agree on those $S$ -points which come from $k$ -points of $W$ , but I don't see how to carry over the density argument. It would also be enough to show $\ul{Z}_G$ is a sheaf for an appropriate topology, and then show sheaf-surjectivity. I don't see how this makes things any easier.","['algebraic-groups', 'algebraic-geometry', 'abstract-algebra', 'group-theory', 'schemes']"
4602374,Draw an area on the complex plane,"On the complex plane draw the area: $$ 
\begin{equation}
    \begin{cases}
       |z+4i| < 3 \\
       |\arg(z-5-5i)|<\frac{\pi}{3}
    \end{cases}
\end{equation}
$$ Where $ \arg(z) \in (-\pi, \pi ]$ I can draw $|z+4i| < 3$ : $|x + iy + 4i|<3 \Rightarrow \sqrt{x^2 + (y + 4)^2}<3 \Rightarrow x^2 + (y + 4)^2<9$ : However, I have no idea how to draw and intersect with $|arg(z-5-5i)|<\frac{\pi}{3}$","['complex-analysis', 'graphing-functions', 'linear-algebra', 'complex-numbers']"
4602388,Finding maximum value of a trigonometric functions,"Let there be three distinct points on a unit circle whose polar coordinates are given as $P(\theta_1)$ , $P(\theta_2)$ , $P(\theta_3)$ . Now, let the function $f(\theta_1,\theta_2, \theta_3 )=cos(\theta_1−\theta_2)+cos(\theta_2−\theta_3)+cos(\theta_3−\theta_1)$ . I want to find the maximum value of function but doing it straightforward is not a way since changing one value of input would change the values of at least two cosine functions. Then, how to find maximum values of such trigonometric functions?","['maxima-minima', 'trigonometry']"
4602391,$n$-th roots in a group,"Background: I am trying to define rational powers of elements in an arbitrary group. For this, I first define $n$ -th powers for $n\in\mathbb Z\setminus\{0\}$ : I call $y$ an $n$ -th root of $x$ iff $y^n = x$ . Now, while trying to come up with a definition for rational exponentiation, and then then trying to deduct the various laws of exponents, I am stuck on the following problem : Let $G$ be a group and $x, y\in G$ . Let $m, n\in\mathbb Z\setminus\{0\}$ such that $\gcd(m, n) = 1$ and $x^m = y^n$ . Do $x$ and $y$ respectively have $n$ -th and $m$ -th roots? I don't think it should be true in general, but my knowledge of groups is limited. Can you give some counterexample? Any insights in special cases (for instance, take $G$ to be abelian and/or ordered) will be appreciated a lot too!","['exponentiation', 'group-theory', 'radicals', 'examples-counterexamples']"
4602406,Random walk of a drunk man trying to find his way home,"The problem is stated as the following: Suppose we have a man walking along a pavement, whose width is 5. For each step forward, the drunk man can either move to the left, or to the right. Both outcomes are equally as likely. However, when the drunk man encounters the 5th position, he must return to the 4th, and when the drunk man encounters the 1st position, he can either go left or right. If he chooses to go right, the drunk man falls off, and the game ends. The question is now: How far on average will the walker walk? What is the probability of the walker returning home after moving K positions. I'll try to solve the latter question, since I believe it's easier for me. First of all, we have to form our transition matrix T. $$ T = \begin{pmatrix}
1 & 0 & 0 & 0 & 0 & 0 \\
1/2 & 0 & 1/2 &  0& 0 & 0 \\
0 & 1/2 & 0 & 1/2 & 0 & 0 \\
0 & 0 & 1/2 & 0 & 1/2 & 0 \\
0 & 0 & 0 & 1/2 & 0 & 1/2 \\
0 & 0 & 0 & 0 & 1 & 0 \\
\end{pmatrix} $$ Where the rows are indexed as E,1,2,...,5 where E stands for ""end"", and thus the random walk terminates if the drunk man falls into this position. To answer what the probability for the walker to arrive at home after K positions, we can note one obivous fact If the K moves are odd, the probability of returning home is $0$ . We might need to use this later to check whether our answer is reasonable. More generally, we can write the probability of returning home after $K$ positions, which we can denote as the event $H_K$ , as: $$ P(H_K) = \begin{pmatrix}
 0& 0 & 0 & 1 & 0 & 0
\end{pmatrix} T^k \cdot \begin{pmatrix}
 0& 0 & 0 & 1 & 0 & 0
\end{pmatrix}
 $$ However, I don't really know if there's any way to create a closed form expression for this probability given this. It's not really that easy to calculate $T^k$ for some general $k$ too. I tried to plot the probabilities of the positions after 30 steps in Python, and got this graph: Notice, position $1$ is what I previously labeled as position $E$ . Position $2$ in the graph represents position $1$ in my previous definition, and so forth. I also tried to plot the probabilities after 100 steps, and it really looks some exponential decay regarding the probabilites for positions. So I assume there might be some easy expression to derive from this. For the question regarding how far the walker will get on average, I've really no idea how to start. I'm thinking that I want to find the expected value of arriving at position $E$ given that our starting position is $3$ . However, I don't really know how to express this in terms of our transition matrix $T$ . I'd be glad if anyone could share their ideas for this problem. Thanks in advance.","['random-walk', 'markov-chains', 'expected-value', 'markov-process', 'probability']"
4602461,Do 1-forms return scalars or co-vectors?,"I've been browsing questions regarding 1-forms and their difference with co-vectors, and I have stumbled upon what follows. @magma, here , said: 1-forms are simply the linear operators that take a vector and give
out a number So, thinking of a function, the range of a 1-form is a real value. On the other hand, @Silly Goose, here , said ... if you have a 1-form α on some manifold M and you pick any x∈M,
the value of α at x is a co-vector. Doesn't the latter quote assert that a 1-form, functionally, returns a co-vector, thereby contradicting the former quote?",['differential-geometry']
4602476,A detail about multivariable calculus concerning $\int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta$ whether equal to $0$,"In the book of ""Classical Fourier Analysis"" by Loukas Grafakos, I am confused by the following detail, the author says that $$\int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta$$ is $0$ when at least one $\alpha_{j}$ is odd, why? My attemption: suppose that $\alpha_{1}$ is odd then we can split the integral $\int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta$ into two parts according the sign of $\theta_{1}$ , we denote $\mathbb{S}^{n-1}_{+}$ the positive $\theta_{1}$ part and $\mathbb{S}^{n-1}_{-}$ the negtive $\theta_{1}$ part, and so we write $$\int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta=\int_{\mathbb{S}_{+}^{n-1}}\theta^{\alpha}d\theta+\int_{\mathbb{S}_{-}^{n-1}}\theta^{\alpha}d\theta$$ We make the change of variable $\theta_{1}$ to $-\theta_{1}$ in the integral $$\int_{\mathbb{S}_{-}^{n-1}}\theta^{\alpha}d\theta$$ can we get $$\int_{\mathbb{S}_{-}^{n-1}}\theta^{\alpha}d\theta=-\int_{\mathbb{S}_{+}^{n-1}}\theta^{\alpha}d\theta$$ ? Is sphere measure $d\theta$ invariant under the reflection $\theta_{1}\to-\theta_{1}$ transformation? Notation: $\mathbb{S}^{n-1}$ is the sphere in $n$ dimension Euclidean space and $d\theta$ is surface measure on sphere, $\alpha=(\alpha_{1},\dots,\alpha_{n})\in\mathbb{Z}^{n}_{+}$ , $\theta=(\theta_{1},\dots,\theta_{n})\in\mathbb{S}^{n-1}$ , $\theta^{\alpha}=\theta_{1}^{\alpha_{1}}\theta_{2}^{\alpha_{2}}\cdots\theta_{n}^{\alpha_{n}}$","['harmonic-analysis', 'multivariable-calculus', 'calculus', 'vector-analysis']"
4602502,How to optimally label three boxes. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question The problem: I have three infinite boxes. One contains both Apples and Bananas, one contains Apples and Cherries and the other contains Bananas and Cherries. All fruit in all boxes are in equal proportion, that is the probability of receiving one of the two available fruit is $\frac{1}{2}$ . You do not know which of the boxes are which but to correctly label them you may randomly draw fruit from them in order order you wish to. What is the optimal (with respected to expected draws) way to do this. What is the expected number of draws? My Result: I only managed to solve this nuts and bolts by manual analysis of combinations. I get $4.75$ . Is there a nice and clean way to solve this. Do you get a different answer?","['expected-value', 'statistics', 'soft-question', 'probability']"
4602506,From $\sqrt{2\pi}$ to Glaisher-Kinkelin to what?,"I was reading about Glaisher-Kinkelin Constant and came across the following formulas $$
\sqrt{2\pi} = \lim_{n\rightarrow\infty} \frac{n!}{n^{\frac{2n+1}{2}}e^{-n}}
$$ $$
A = \lim_{n\rightarrow\infty} \frac{H(n)}{n^{\frac{6n^2+6n+1}{12}}e^{-\frac{n^2}{4}}}
$$ I came across the Stirling numbers of the second kind $S(p,k)$ in research, in the form of $a_{p,k} = k! S(p,k)$ . Now in the formulas above, we see that there are the polynomials $2n+1$ and $6n^2+6n+1$ , whose coefficients are $a_{2,\cdot}$ and $a_{3,\cdot}$ respectively. Is there a known constant that uses the polynomial $24n^3+36n^2+14n+1$ , the coefficients for $a_{4,\cdot}\ ?$ . My guess is that the numerator in the limit would have something like $H_2(n) = \prod k^{k^k}$ .",['limits']
4602601,Find the value of ${a_n^2}+{b_n^2}+{c_n^2}-a_nb_n-b_nc_n-c_na_n$,"Let $$a_n=\binom{n}{0}+\binom{n}{3}+\binom{n}{6}+\cdots$$ $$b_n=\binom{n}{1}+\binom{n}{4}+\binom{n}{7}+\cdots$$ $$c_n=\binom{n}{2}+\binom{n}{5}+\binom{n}{8}+\cdots$$ Then find the value of $${a_n^2}+{b_n^2}+{c_n^2}-a_nb_n-b_nc_n-c_na_n$$ I wrote all expressions in a summation form but couldn't find a closed form. Surprisingly, wolframaplha shows that all of $a_n,b_n$ and $c_n$ converges. And it also gives the closed form, but that is in the form of trigonometric expressions. Like for example, $$\sum_{i=0}^{\infty}\binom{n}{3i}=\frac13\left(2\cos\left(\frac{n\pi}{3}\right)+2^n\right)$$ Till now I just knew the methods of integration, differentiation, sequences series and putting values to find sum of particular binomial coefficients. How did trigonometry come into play $?$ Also, we need to find the above asked expression and squaring these trigonometric expressions is not a good idea imo so there must be some other method. Any help is greatly appreciated.","['binomial-coefficients', 'recurrence-relations', 'sequences-and-series']"
4602614,Combinatorics with a set number of coins,"I came upon this question yesterday and was wondering about it: Sydney started the day with $15$ coins in her pocket which totaled $2.00$ dollars. At the end of the day, after a number of transactions, she had $16$ coins which totaled $3.00$ dollars. She had only quarters, dimes, nickels, and pennies, and ended the day with a different number of each type of coin than she had started with (one or the other of which could have been zero). If she started with $m$ quarters and ended with $n$ quarters, what is $m + n$ ? After a long, brute force process, I determined that $m + n = 18$ , with her original coins being $7$ quarters, $1$ dime, $2$ nickels, and $5$ pennies, and her coins at the end of the day being $11$ quarters, $0$ dimes, $5$ nickels, and $0$ pennies. I achieved this answer by completely brute-forcing my way through all of the possible coin combinations and figuring it out after about an hour and a half. Is there a better way to do this (without a calculator) that I could have done? For reference : quarter = $0.25$ dollar dime = $0.10$ dollar nickel = $0.05$ dollar penny = $0.01$ dollar","['combinatorics', 'diophantine-equations']"
4602683,Does the chain rule imply the product rule?,"Let $\mathbb{F}$ be a field, and consider $\mathbb{F}^\mathbb{F}$ as an algebra over $\mathbb{F}$ with the standard function multiplication. Let $D$ be a linear transformation on a subalgebra of $\mathbb{F}^\mathbb{F}$ closed under function composition that satisfies the chain rule.  Does $D$ necessarily satisfy the product rule for arbitrary $\mathbb{F}$ ? (Inspired by a comment on this question. ) What if the subalgebra must be unital?","['vector-spaces', 'field-theory', 'abstract-algebra', 'linear-algebra', 'derivatives']"
4602697,Deflation of eigenvalue,"Could you explain to me the deflation? For example if we have the matrix $A=\begin{pmatrix}1 & -0.5 & -1.5\\ -15 & -2.5 & 4.5\\ -15 & -4.5 & 2.5\end{pmatrix}$ how do we apply the deflation of the eigenvalue $\lambda_1=4$ ? $$$$ EDIT : I have done the following : We have the eigenvalue $\lambda_1=4$ . The corresponding eigenvector is : $v_1=\begin{pmatrix}-1\\ 3 \\ 1\end{pmatrix}$ , right? Since $v_1$ does not have $1$ as the component of largest modulus, we multiply $v_1$ by a permutation matrix $P$ which interchanges the largest element and the first element: We interchange rows $1$ and $2$ of $A$ : $v_1'=\begin{pmatrix}3 \\-1\\  1\end{pmatrix}$ $$\begin{pmatrix}-15 & -2.5 & 4.5\\1 & -0.5 & -1.5\\  -15 & -4.5 & 2.5\end{pmatrix}$$ We want to transform the vector into the vector $e_1$ , so we divide the first row by $3$ , add it to the second row and subtract it from the third row : $v_1''=\begin{pmatrix}1 \\0\\  0\end{pmatrix}$ $$\begin{pmatrix}-5 & -5/6 & 1.5\\-4 & -4/3 & 0\\  -10 & -11/3 & 1\end{pmatrix}$$ Since we have interchanged the rows $1$ and $2$ , now we have to interchange the columns $1$ and $2$ and so we get the matrix $$\begin{pmatrix} -5/6 & -5 &1.5\\-4/3 & -4 & 0\\   -11/3 & -10 &1\end{pmatrix}$$ Then $\lambda_1=4$ should be an eigenvalue of this matrix with $e_1$ the corresponding eigenvector, but this is not like that. What am I doing wrong? Is the deflated matrix equal to $$\begin{pmatrix} -4 & 0\\   -10 &1\end{pmatrix}$$ ?","['matrices', 'numerical-methods', 'eigenvalues-eigenvectors']"
4602702,How to solve the following polynomial recursion $p_n(x)= p_{n-1}(x)(x^2-a)+\left( \frac{b}{x}-2x \right) p_{n-1}'(x)+p_{n-1}''(x)$,"How to find the solution to the following polynomial recursion \begin{align}
p_n(x)= p_{n-1}(x)(x^2-a)+\left( \frac{b}{x}-2x \right) p_{n-1}'(x)+p_{n-1}''(x)
\end{align} with $p_0(x)=1$ for some fixed constant $a$ and $b$ . The first two terms of this are given by \begin{align}
p_1(x)&=x^2-a\\
p_2(x)&= x^4 - (2 a+4) x^2 + 2 b+a^2   + 2\\
p_3(x)&= x^6 +(2a+ 6b+ 3a^2+ 30)x^2  - (3a+12)x^4    - 8- 8b - 6ab - 6a- a^3
\end{align} This question arose as part of another question here .","['recurrence-relations', 'ordinary-differential-equations']"
4602727,Can we always assume that a function is differentiable?,"I know how to solve the following problem assuming a supplementary condition(that a function is differentiable). The problem: Let $a$ and $b$ be real numbers in the interval $(0, +\infty)$ and $f : [a, b] \to [0, b]$ a bijective and continuous function and $f(0)=0$ . Show that $$\int_0^a f(x)dx+\int_0^b f^{-1}(x)dx=ab$$ where $f^{-1}$ is the inverse function of $f$ . This is my approach. $f$ is a continuous function therefore it has the intermediate value property. The function is bijective and it is continuous so, in particular, it is injective. Therefore, the function is monotone. The function is increasing because the codomain has positive values. What I mean is that if we take $x \in [0, a] \implies f(x) \in [0, b] \implies 0 \le f(x)$ . Since $0 \le x \implies f(0) \le f(x)$ it means that f has to be increasing. $f$ is continuous and increasing therefore, the extreme value theorem says that $f(a)=b$ . Now, I would assume that $f$ is differentiable, so I can use the substitution $u=f^{-1}(x)$ on the second integral. Therefore, $x=f(u) \implies dx=f'(u)du$ . So, $$\int_0^b f^{-1}(x)dx=\int_0^a uf'(u)du=uf(u)\biggr \rvert_{0}^{a}-\int_0^a f(u)du$$ Therefore, the sum of the two integrals is $ab$ . However, I don't know how to deal with the case where $f$ is not differentiable. Can we assume this? Because I have found counterexamples, like $f(x)=x$ for $x \in [0, 1]$ and $f(x)=2x-1$ for $x \in (1, 2]$ . This function is not differentiable at $x=1$ , but is both continuous and bijective. And with the same reasoning we can create a function that has a countable set of points where the function is not differentiable, like $f(x)=\frac{1}{n}x+c_n$ for $x \in [\frac{1}{n}, \frac{1}{n+1}]$ and we find $c_n$ such that the function is continuous. Is it safe to assume that the function is differentiable, and if yes why? Edit: I wrote some ambiguous things and I corrected them","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'derivatives']"
4602746,$(a^n-1)(b^n-1)$ can't be a square for all $n$ unless $ab$ is a square,"Let $a,b$ be positive integers $>1$ such that $(a^n-1)(b^n-1)$ is a square for all $n\ge 1$ . Prove that $ab$ is a perfect square. I'm not asking for a solution to this problem because I already know one. What I'm asking is would the following approach work? or rather I want a particular limit to equal zero. Assume $(a^n-1)(b^n-1)$ is a perfect square $\forall n\ge1$ and let $x_n=\sqrt{(a^n-1)(b^n-1)}$ . Note that $x_n\in \mathbb N $ Now $$x_{n+2}=\sqrt{(a^{n+2}-1)(b^{n+2}-1)}=ab\sqrt{\left(a^{n}-\frac{1}{a^2}\right)\left(b^{n}-\frac{1}{b^2}\right)}$$ and $ab\cdot x_n=ab\sqrt{(a^n-1)(b^n-1)}$ . Define $d_n=abx_n-x_{n+2}$ and let us assume for the moment $$\lim_{n\to \infty}d_n=\lim_{n\to \infty}abx_n-x_{n+2}=0$$ Therefore $(d_n)$ converges to $0$ , but $d_n\in \mathbb N $ hence $d_n$ is eventually $0$ . Meaning for big enough $N$ , we have $\forall n\ge N$ $$d_n=abx_n-x_{n+2}=0$$ in other words $abx_n=x_{n+2}$ . From this I think there is a lot of ways to get a contradiction but this is what I did, $$\forall n\ge N' \quad  ab\mid x_{n}\mid x_n^2=(a^n-1)(b^n-1)=(ab)^n-a^n-b^n+1$$ which means $a\mid ab\mid a^n+b^n-1\implies a\mid b^n-1$ . In particular $$\forall p\ge N' \quad b^p\equiv  1\pmod a$$ Where $p$ is a prime. But this implies $\operatorname{ord}_ab\mid p$ hence $r=\operatorname{ord}_ab=1$ or $p$ . Using the very same method we can see $s=\operatorname{ord}_ba=1$ or $p$ both gives a contradiction. If one of $r$ or $s$ is $p$ the contradiction is immediate since $p\mid \phi(a)$ or $\phi(b)$ but clearly this not okay for large enough $p$ . Thus both of $r$ and $s$ is $1$ . But this means $$a\mid b-1 \text { and } b\mid a-1$$ Hence $a+b\le a+b-2\implies 0\le -2$ a contradiction. I think that the solution doesn't contain errors (If it did tell me). The only thing that I'm not sure of is $\lim d_n=0$ because I just counldn't evaluate $$\lim_{n\to \infty}\sqrt{(a^n-1)(b^n-1)}-\sqrt{\left(a^{n}-\frac{1}{a^2}\right)\left(b^{n}-\frac{1}{b^2}\right)}$$ You can clearly multiply by the conjugate but after expanding you get $-\infty /\infty$ .","['limits', 'elementary-number-theory', 'perfect-powers']"
4602844,What notions of independence exist for these random variables?,"I think I've gotten a bit confused about the notions of independence that the following random variables satisfy. Any help would be greatly appreciated. Let $x_1, ..., x_k \in \mathbb{Z}_q^n$ , each distinct (not random, I want this result to hold in general for any choice of these $x$ 's). Let $A_j \in \mathbb{Z}_q^{m \times n}, m < n, b_j \in \mathbb{Z}_q^m$ both uniformly distributed, and independently.
Then define: \begin{equation}
y_{i,j} = A_jx_i + b_j
\end{equation} What can I say about the independence of the $y_{i,j}$ , i.e., pairwise independence, etc,. I think for each $j$ $y_{i,j}$ are independently and uniformly distributed over $\mathbb{Z}_q^m$ , (since they must differ in at least one coordinate), but I am generally confused. Thanks","['hash-function', 'probability-theory', 'probability']"
4602889,Area of circle inscribed in two quarter circles,"So our math teacher gave us this puzzle: We need to find the area of the circle. The equation for the two quarter circles are: $x^2+y^2=1$ , $(x-1)^2+y^2=1$ . I found that the equation for the circle is $(x-0.5)^2+(y-a)^2=a^2$ . I want to find out $a$ , as the circle is tangent to both the two quarter circles. Thanks!","['circles', 'geometry']"
4602923,Probability on Number Theory,"Problem: Suppose that $a,b,c \in \{1,2,3,\cdots,1000\}$ are randomly selected with replacement. Find the probability that $abc+ab+2a$ is divisible by $5$ . Answer given from the worksheet: $33/125$ My answer: $\frac{641}{3125}$ Attempt : Since $abc+ab+2a = a(bc+b+2)$ , either $a \equiv 0 \pmod{5}$ or $bc+b+2 \equiv 0 \pmod{5}$ . The first case, which is $a \equiv 0 \pmod{5}$ , has a probability of $1/5$ . Now on the other case, $$bc+b+2 \equiv 0 \pmod{5} \Rightarrow b(c+1) \equiv 3 \pmod{5}$$ happens when $b\equiv 1$ and $c \equiv 2$ , $b \equiv 2$ and $c \equiv 3$ , $b \equiv 3$ and $c \equiv 0$ , or $b \equiv 4$ and $c \equiv 1$ , total of $4$ solutions. So, this case has a probability of $4 \cdot\frac{1}{5^4} = \frac{4}{625}$ . By Inclusion-exclusion principle, the final probability should be $\frac{1}{5} + \frac{4}{625} - \frac{1}{5} \cdot \frac{4}{625} = \frac{641}{3125}.$ This is apparently not the same from the given answer in the worksheet. Where did I go wrong?","['elementary-number-theory', 'probability']"
4602972,analytic discription of $\mathrm{Ext}^1$,"We know that for any pair of holomorphic vector bundles $\mathcal{E}_1$ and $\mathcal{E}_2$ over complex surface $X$ (i.e. complex dimension 2), $\mathrm{Ext}^1(\mathcal{E}_1, \mathcal{E}_2)$ can be discribed as $\mathrm{H^{0,1}}(X, \mathrm{Hom}((\mathcal{E}_1, \mathcal{E}_2))$ , the $\mathrm{Hom}^1(\mathcal{E}_1, \mathcal{E}_2)$ -valued holomorphic 1-forms on $X$ . I wonder what will happen if we replace $\mathrm{Ext}^1(\mathcal{E}_1, \mathcal{E}_2)$ to line bundle $\mathcal{L}$ and ideal sheaf $\mathcal{I}_Z$ for some subvariety $Z$ with dimension $0$ ? I mean, can we give an analytic describption of $\mathrm{Ext}^1_{\mathcal{O}_X}(\mathcal{I}_Z, \mathcal{L})$ ? I know the geometric description of $\mathrm{Ext}^1(\mathcal{E}_1, \mathcal{E}_2)$ can be understood as extending $\mathcal{E}_1$ to a higher holomorphic vector bundle by $\mathcal{E}_2$ , i.e. finding all isomorphism class of short exact sequence: $0\rightarrow \mathcal{E}_2\rightarrow \mathcal{E}\rightarrow \mathcal{E}_1\rightarrow 0$ , where the differential $\bar{\partial}_E$ of $\mathcal{E}$ is given by uppertriangle matrix: $$
\begin{bmatrix}
\bar{\partial}_1 & \psi\\
0&\bar{\partial}_2
\end{bmatrix}
$$ Where $\psi\in\mathrm{H^{0,1}}(X, \mathrm{Hom}(\mathcal{E}_1 , \mathcal{E}_2))$ by integrability $\bar{\partial}_E\cdot\bar{\partial}_E=0$ and $\bar{\partial}_1$ the differential on $\mathcal{E}_1$ , $\bar{\partial}_2$ the differential on $\mathcal{E}_2$ . So for the case $\mathrm{Ext}^1_{\mathcal{O}_X}(\mathcal{I}_Z, \mathcal{L})$ , as $\mathcal{I}_Z|_{X-Z}=\mathcal{O}_{X-Z}$ , which is the trivial line bundle over $X-Z$ , I think we can obtain similar $\bar{\partial}$ matrix: $$
\begin{bmatrix}
\bar{\partial} & \phi\\
0&\bar{\partial}_L
\end{bmatrix}
$$ on $X-Z$ , here $\bar{\partial}$ is the standard one on $\mathcal{O}_{X-Z}$ , and $\bar{\partial}_L$ the differential on $\mathcal{L}$ . Since $\phi\in\mathrm{H^{0,1}}(X-Z, \mathrm{Hom}(\mathcal{O}_{X-Z}, \mathcal{L}))$ , then $\bar{\partial}\phi=0$ on $X-Z$ and $\bar{\partial}\phi$ should be some 2-form valued functions surpported on $Z$ , But do all 2-form valued functions surpported on $Z$ represent some elements in $\mathrm{Ext}^1_{\mathcal{O}_X}(\mathcal{I}_Z, \mathcal{L})$ ? Answers and comments are both welcomed!","['complex-geometry', 'vector-bundles', 'algebraic-geometry', 'sheaf-theory']"
4602985,find vanishing line (horizon) in image without parallel lines,"I have the image of a vase from which I would like to extract the horizon of the plane on which the vase lays. How could I achieve this supposing the vase is a right cone and I can extract the two ellipses corresponding to two circular cross sections of the cone? Usually to find the horizon in a scene you have to find two pairs of parallel lines and compute their intersection, finding two vanishing points and computing the line through them. But in this scene we have no parallel lines :(","['projective-geometry', 'conic-sections', 'geometry']"
4603015,Euler number of symmetric cube of the tautological bundle,"Let $E$ -is a tautological two-dimensional bundle (rank $n=2$ ) over complex Grassmannian $\operatorname{Gr}(2, 4)$ ( $2$ -dimensional planes in $C^4$ ). I'm trying to compute the Euler number $\oint_{\operatorname{Gr}(2, 4)}{e(S^3E)}$ . Where $S^3E$ -is a symmetric cube of the bundle $E$ . My thoughts about this question are the following: I know that the Euler class of a complex vector bundle is always equal to the top Chern class. For tautological vector bundle $E$ the top Chern class is $c_4(\operatorname{Gr}(2, 4)) = 6c_2(Q)^2$ where $c_2(Q)^2$ is the generator of $H^8(\operatorname{Gr}(2, 4); \mathbb{Z})$ . Also we have the splitting principle $c(S^{p} E)=\prod_{1 \leq i_1 \leq i_2 \leq \ldots \leq i_p \leq n} (1+x_{i_1}+\ldots+x_{i_p})$ (possibly it may be useful in this situation). I don't know how to continue computations for symmetric cube. Please, can you explain these computations in more details?","['vector-bundles', 'algebraic-geometry', 'differential-topology', 'characteristic-classes', 'differential-geometry']"
4603048,Behaviour of the function satisfying $f(x)+f(\frac{x-1}{x})=x+1$,"Question: Let $f:\mathbb R-\{0,1\}\to \mathbb R$ be a function satisfying $f(x)+f(\frac{x-1}{x})=x+1$ , then both $f$ and $f'$ are not injective. (True or False) I tried forming patterns and deduced that $f(x)=\frac{x^3-x^2-1}{2x(x-1)},x\ne0,1.$ To comment on whether $f,f'$ are injective, we have that $f'(x)=\frac{x^4-2x^3+x^2+2x-1}{2(x-1)^2x^2}$ , need not be strictly increasing unless $x^4-2x^3+x^2+2x-1\geq0,x\in\mathbb R-\{0,1\}$ . Let $h(x)=x^4-2x^3+x^2+2x-1$ , using Descartes' rule of sign $h(x)$ can have atmost $3$ positive real roots and $1$ negative real root. Observe that $h(0)=-1,h(1)=1$ although they are not in the domain but they will help determine in the roots, so this particular $h(x)$ must have atleast $1$ root in $(0,1)$ . Also $h'(x)=4x^3-6x^2+2x+2$ such that $h'(0)=2,h'(1)=2$ here the condition of Rolle's Theorem is applicable on $(0,1)$ as the assumed $h(x)$ is continuous on $[0,1]$ then $\exists c\in(0,1)$ such that $h''(c)=0\implies12c^2-12c+2=0$ . This will give the existence of $2$ real $c\in(0,1)$ . I lost my track after this, since if $f$ vanishes twice in the domain $\implies$ $f'$ vanishes atleast once, but $f'$ will not make us comment anything on the behaviour of $f$ as the converse of Rolle's need not help. Any hint would be appreciated The thing that created the doubt is as: $h'(x)$ is a cubic polynomial so it need not guarantee the existence of $3$ real roots, I used $h''$ to comment on the same and $h'$ has no root in $(0,1)$ but it is not one-one there as ell. I request not to use the graphing calculators to depict the behaviour of this function. I am looking for a simpler way to do this question . Thanks.","['functions', 'roots']"
4603050,Formula to check if a point is inside a a right triangle,"I am developing an app that show some geology diagrams as Streckeisen: I need that when the user clicks in a field the corresponding rock sheet opens. The approach I am using is to store the coordinates inside the image that has been clicked. I think the better approach is to divide the fields in squares and/or right triangles. I have no troubles to check with code if the user has clicked in the squares, but I need to add a formula for the right triangle shapes. Can you help me with the formula needed to show if the coordinates clicked are inside the right triangle?",['trigonometry']
4603090,Evaluation of $~\int_{0}^{2\pi}{\cos(\theta)^2-\sin(\theta)^2\over\sin(\theta)^4+\cos(\theta)^4}\mathrm d\theta$,"$$
I:=\int_{0}^{2\pi}{\cos(\theta)^2-\sin(\theta)^2\over\sin(\theta)^4+\cos(\theta)^4}\mathrm d\theta
$$ My tries $$\begin{align}
s&:=\sin\theta\\
c&:=\cos\theta\\
I&=\int_{0}^{2\pi}{\cos(\theta)^2-\sin(\theta)^2\over\sin(\theta)^4+\cos(\theta)^4}\mathrm d\theta\\
&=\int_{0}^{2\pi}{c^2-s^2\over s^4+c^4}\mathrm d\theta\\
&=\int_{0}^{2\pi}{(1-s^2)-s^2\over s^4+(c^2)^2}\mathrm d\theta\\
&=\int_{0}^{2\pi}{1-2s^2\over s^4+(1-s^2)^2}\mathrm d\theta\\
&=\int_{0}^{2\pi}{1-2s^2\over s^4+(s^2-1)^2}\mathrm d\theta\\
&=\int_{0}^{2\pi}{1-2s^2\over s^4+s^4-2s^2+1}\mathrm d\theta\\
&=\int_{0}^{2\pi}\underbrace{\color{red}{\left({1-2s^2\over 2s^4-2s^2+1}\right)}}_{\text{I got stuck here}}\mathrm d\theta\\
\end{align}$$ I need your help.","['integration', 'systems-of-equations', 'definite-integrals', 'trigonometric-integrals', 'trigonometry']"
4603130,Greatest common divisor of some binomial coefficients,"Note that this is now cross-posted on mathoverflow . While making some computation, I stumbled upon a curious relation among some binomial coefficients. Consider the sequence of binomial coefficients $a(k,n)$ , $0 \le k \le n-1$ , $n \ge 2$ defined as follows: $$a(k,n) = \binom{\frac{n(n-1)}{2}+3+k}{\lfloor\frac{n-1}{2}\rfloor+1+k}$$ and the sequence ( $n \ge 2$ ): $$b_n=\gcd(a(0,n),a(1,n),\ldots,a(n-2,n),a(n-1,n))$$ I conjecture that: $$b_n \gt 1 \space\space\space\space \forall n \ge 2$$ The first few values of $b_n$ starting from $b_2$ are $2,5,6,143,102,253,899,703$ . I didn't check for $n \gt 9$ . Any hint for proving or disproving the conjecture? Also, can we say something about the values of $b_n$ ?","['elementary-number-theory', 'gcd-and-lcm', 'binomial-coefficients', 'combinatorics']"
4603140,Calculating $\mathbb E\left[\sum_{k=1}^TX_k\right]$ where $(X_k)$ where $T$ is a stopping time,"Let $(X_k)_{k\in\mathbb N^*}$ be integrable random variables and $(\mathcal F_k)_{k\in\mathbb N^*}$ a filtration such that $(X_k)$ is $(\mathcal F_k)$ -adapted. Let $T$ be an integrable $(\mathcal F_k)$ -stopping time. Suppose that for $k\ge1$ , $\mathbb E[X_{k+1}\mid\mathcal F_k]=\mathbb E[X_1]$ . Show that : $$\mathbb E\left[\sum_{k=1}^TX_k\right]=\mathbb E[T]\mathbb E[X_1]$$ The case where $T$ and $(X_k)$ is very simple but I can't quite slove this version of the problem. The natural way to solve this problem seems to be taking : $$Y_n=\sum_{k=1}^nX_k,\quad n\ge1$$ Such that the quantity we wish to determine is $\mathbb E[Y_T]$ . We have $\mathbb E[Y_{n+1}\mid \mathcal F_n]=Y_n+\mathbb E[X_1]$ and so depending on the value of $\mathbb E[X_1]$ , $(Y_n)$ is either a martingale, submartingale or supermartingale. Next I looked at the decomposition : $$Y_{T\wedge n}=Y_{T\wedge n}\mathbf1_{\{T<+\infty\}}+Y_{T\wedge n}\mathbf 1_{\{T=+\infty\}}\implies \mathbb E[Y_{T\wedge n}]=\mathbb E[Y_{T\wedge n}\mathbf1_{\{T<+\infty\}}]+\mathbb E[Y_{T\wedge n}\mathbf 1_{\{T=+\infty\}}]$$ where $x\wedge y=\min(x,y)$ . Since we supposed that $T$ was integrable, we get that $\mathbb P(T=+\infty)=0$ because $T$ is positive. Therefore : $$\mathbb E[Y_{T\wedge n}\mathbf 1_{\{T=+\infty\}}]=\int_{\{\omega,T(\omega=+\infty\}}Y_{T(\omega)\wedge n}(\omega)\mathbb P(d\omega)=0$$ Hence : $$\mathbb E[Y_{T\wedge n}]=\mathbb E[Y_{T\wedge n}\mathbf1_{\{T<+\infty\}}]$$ Then I would like to let $n\rightarrow\infty$ and use a result like dominated convergence to get $\mathbb E[Y_T]$ but since there isn't really any link between $T$ and the $(Y_n)$ I don't really know how to continue from there. Any help would be greatly appreciated !","['expected-value', 'martingales', 'stopping-times', 'probability-theory', 'probability']"
4603314,Prove that the curve given by $f(t)=\cos(t) u_1 + \sin(t)u_2$ is an ellipse.,"Let $u_1$ , $u_2 \in \mathbb{R}^2$ be two linearly independent vectors. Prove that the curve given by $f(t)=\cos(t) u_1 + \sin(t)u_2$ is an ellipse. My attempt Let $u_1=(a_1,a_2)$ , $u_2=(b_1,b_2)$ , $x=a_1 \cos(t)+b_1 \sin(t)$ , $y=a_2 \cos(t)+b_2 \sin(t)$ Then: $x^2=a_1^2 \cos ^2(t) + a_1 b_1 \sin(2t) + b_1^2 \sin^2(t)$ $y^2=a_2^2 \cos ^2(t) + a_2 b_2 \sin(2t) + b_2^2 \sin^2(t)$ I'm trying to figure out what would be the two constants $\alpha, \beta$ such that $\dfrac{x^2}{\alpha^2}+ \dfrac{y^2}{\beta^2}=1$ , but I'm not sure what to do next. I'm aware that the fact that $u_1$ and $u_2$ are linearly independent implies that $f(t) \neq 0$ ...","['analytic-geometry', 'multivariable-calculus', 'linear-algebra', 'vector-spaces']"
4603345,"Generating set of a ring and the ""empty product""","Given some ring $R$ , a subset of elements $S$ is a ""generating set"" $R$ if every element in the ring can be written as a sum or difference of products of elements of $S$ . Or, borrowing the LaTeX from this answer , we have that a ring $R$ is generated by $S=\{s_1,\dots, s_n\}$ iff every element of $R$ can be written in the form $$
\pm(s_{i_{1,1}}s_{i_{1,2}}\dots s_{i_{2,1}})\pm(s_{i_{2,2}}\dots s_{i_{2,p_1}})\pm\dots\pm(s_{i_{n,1}}\dots s_{i_{n,p_n}})
$$ The question, which is also talked about in the comments of the above answer, is if the ""empty product"" of $1$ is allowed. If so, this would seem to suggest that $\Bbb Z$ is generated by the empty set, since everything is a sum or difference of empty products. This is kind of the ring-theoretic version of the idea that the trivial group is generated by the empty set. Similarly, the rings $\Bbb Z/n\Bbb Z$ would also appear to be generated by the empty set (as a subset of those rings). The ring $\Bbb Z[x]$ would be generated by only $\{x\}$ , as $1$ would still basically be generated as an empty product of no $x$ 's (i.e. as $x^0$ ). It all seems to make sense, and agrees with the alternative definition that $S$ generates $R$ iff $R$ is the smallest subring of itself (preserving $1$ ) that has all the elements of $S$ , which is (vacuously) true of the empty set in $\Bbb Z$ . Questions : Is this standard use of terminology when we talk about generating sets of rings? If we look at ""rng""s without $1$ , is it then standard to omit the empty product (equivalent to looking at the smallest ""subrng"" containing $S$ )?","['ring-theory', 'abstract-algebra', 'terminology']"
4603350,Can any $n$ real numbers be part of a geometric sequence?,"Ok so, this might be a really silly question, but quite honestly I couldn't find any info about it. So it all begins with a little rather simple question, so I wrote a simple python code, which essentially brute forces step-by-step solutions. The question is Can $x, y, z$ be a part of a geometric sequence? I've noticed a pattern however, for most of the inputs I gave it, it usually took the smallest value from the input, and found a common ratio approaching $1$ , ie. $1.001.$ Which effectively approached y and z after multiplying it a lot of times. Here's an example for input $2,  24$ $24 = 2*12^n$ where $2$ is a, $12$ is r, when we plug it into $ar^n$ we get that $2$ is the first term and $24$ is the second one. However, aside from that, it spit out many other approximations, such as $2*1,0001^n$ , which also is a very good approximation, and the more time it took to compute the more 0's it added essentially. In here $2$ is the first term, and $24$ is the $24851$ st term. When I let it compute it for an hour with very high approximation it came up with $2*1,0000001^n$ where $2$ is the first term, and $24$ being $24849068$ th term, you get the point. So now my question is, can any given set of Real positive numbers, with n terms, be expressed as a geometric sequence of $ns*1.0000...1^n$ ? ns being the smallest nonnegative number?","['sequences-and-series', 'geometric-series', 'real-analysis']"
4603371,"Question about the proof in ""Understanding Analysis"" by Stephen Abbot","My question is rather simple and involves a step that is taken when proving that there is a number squared that equals two. He uses the least upper bound theorem. The first few steps make sense, but how he gets between these two highlighted steps confuses me. I do not know how he derives that second inequality that I have highlighted. Thank you in advance for your help.","['analysis', 'real-analysis']"
4603380,In how many ways can we distribute $k$ identical balls into $n$ different boxes,"In how many ways can we distribute $k$ identical balls into $n$ different boxes
so that each box contains atmost one ball and no two consecutive boxes
are empty. My Approach: First I filled the $k$ boxes out of $n$ box with $k$ identical balls. Then I arranged remaining $(n-k)$ boxes between $(k+1)$ gaps formed by $k$ boxes and this can be done in ${k+1 \choose n-k}$ . and this match with the given answer. My Doubt: According to me I must select $k$ boxes out of $n$ boxes to put $k$ identical balls. So according to me answer must be ${n\choose k}{k+1\choose n-k}$ . What I am thinking wrong in second Approach? Is there any other way to solve this problem?","['permutations', 'combinations', 'combinatorics', 'balls-in-bins']"
4603387,Proving that certain integral is positive,"Given a compact set $K\subset \mathbb{R}^3$ , we consider $f:K^3\subset\mathbb{R^9}\to  \mathbb{R}_0^+$ such that $f(x_1,x_2,x_3)=f(x_{\tau(1)},x_{\tau(2)},x_{\tau(3)})$ for every permutation $\tau$ and $\int_{K^3} f=1$ and we define $g:K^3\to  \mathbb{R}_0^+,\;g(x_1,x_2,x_3)=\int_{K^2}f(x_1,x_2,x_3)dx_2dx_3\int_{K^2}f(x_1,x_2,x_3)dx_1dx_3\int_{K^2}f(x_1,x_2,x_3)dx_1dx_2$ . Today during my statistical physics class the teacher said that it is easy to show that $\int_{K^3}f(x_1,x_2,x_3)\ln\frac{f(x_1,x_2,x_3)}{g(x_1,x_2,x_3)}dx_1dx_2dx_3\geq0 $ . He said that we just need to add $g-f$ inside the integral. I know that this doesn't change the integral because $\int g= \int f=1$ , but I don't know how to see that $\int_{K^3}f(x_1,x_2,x_3)\ln\frac{f(x_1,x_2,x_3)}{g(x_1,x_2,x_3)}+g(x_1,x_2,x_3)-f(x_1,x_2,x_3)dx_1dx_2dx_3\geq0 $ .","['real-analysis', 'multivariable-calculus', 'functional-analysis', 'statistical-mechanics', 'probability-theory']"
4603402,Limit of distribution function of Poisson distributed random variables,"Let $\left\{X_n\right\}_{n \geq 1}$ be i.i.d. random variables with Poisson $(\lambda)$ distribution, $\lambda>0$ . For each $n \geq 1$ , let $F_n$ denote the distribution function of $$
\frac{1}{\sqrt{n}} \sum_{i=1}^n\left(X_{2 i-1}-X_{2 i}\right) .
$$ How can I calculate the limit $\lim _{n \rightarrow \infty} F_n(x), x \in \mathbb{R}$ ? I thought I could apply the weak law of large numbers? Let $\left\{X_{n}\right\}$ be a sequence of random variables with finite expected values. We will say that $\left\{X_{n}\right\}$ satisfies the weak law of large numbers if $$
\frac{1}{n} \sum_{i=1}^{n} X_{i}-\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}\left[X_{i}\right] \stackrel{P}{\rightarrow} 0.
$$ The problem here is that this converges in probability but above I just take the limit. Any hints?","['poisson-distribution', 'probability-limit-theorems', 'probability']"
4603471,"Is every permutation of naturals that is convergent-invariant, a bounded permutation?","Let $C$ be the set of sequences of real numbers whose associated series is convergent. I define a permutation $f$ of the naturals to be convergent-invariant if, for every sequence $a_n$ in $C$ , the series of the sequence $a_{f(n)}$ converges, and to the same sum. Certainly, any permutation that moves only finitely many natural numbers is convergent-invariant. I was surprised to learn, however, that there are other convergent-invariant permutations. So, now, I define a bounded permutation to be a permutation that does not move natural numbers arbitrarily far. For example, the permutation that moves $0$ to $1$ , $1$ to $0$ , $2$ to $3$ , $3$ to $2$ , $4$ to $5$ , $5$ to $4$ , etc is bounded, because it does not move any natural number greater than a distance of $1$ . So, my question now is, is being a bounded permutation equivalent to being a convergent-invariant permutation? If not, does either direction of the implication hold?","['permutations', 'sequences-and-series']"
4603543,"If every element of a field is a sum of squares, then how many squares are required?","Let $\mathbb{F}$ be a field such that every element is a sum of squares.  For $\mathbb{F = Q}(\sqrt{-7}),$ four squares might be required. Is there any field such that there are contains elements that aren't the sum of four squares? What is the smallest $n$ that is at most the Pythagorean number of such a field?","['field-theory', 'abstract-algebra']"
4603566,"Prove that if $y'=f(x,y),y(x_0)=y_0$ is invariant under the transformation $(x,y) \mapsto (-x,-y)$, then $y(-x)=-y(x)$ provided the existence","How to make a clear justification for the proposition: If $y'=f(x,y),y(x_0)=y_0$ is invariant under the transformation $(x,y) \mapsto  (-x,-y)$ , then $y(-x)=-y(x)$ provided the existence of solution. For instance, $u(x)=-y(-x)$ can also satisfy the ODE $z'(x)=z(x)^2,~z(x_0)=z_0$ , but how it is necessary whenever the ODE is invariant under $(x,y) \mapsto  (-x,-y)$ ?",['ordinary-differential-equations']
4603584,Terminology: Subsets that are contained in a compact subset,"I am looking at a subsets of a particular topological space $\mathbf{X}$ , and I care about subsets $A$ of $\mathbf{X}$ which are a subset of a compact subset $B$ of $\mathbf{X}$ . Since I am not restricting myself to Hausdorff $\mathbf{X}$ here, this notion is not the same as being relatively compact . My question is whether the notion I am looking at has a name or some ""further reading"" attached to it.","['general-topology', 'terminology', 'reference-request']"
4603634,Expectation of X increases when conditioning on X being greater than another independent random variable Y?,"I'm doing research and for a proof I need the smaller result that for X, Y random, independent (but not identical) variables we have $$\mathbb{E}\left[X|X>Y\right] \geq \mathbb{E} \left[X\right]$$ This seems very intuitive, but I've had no luck proving it so far. I've managed to derive the pdf of X|X>Y and use it to calculate the expectation: $$\mathbb{E}\left[X|X>Y\right] = \int_{-\infty}^\infty\frac{f_X(x)F_Y(x)}{1-\int_{-\infty}^\infty F_X(y)f_Y(y)dy}xdx$$ but I find it very hard to bound this. Does someone have any ideas on how to proceed? Thanks!","['expected-value', 'conditional-expectation', 'conditional-probability', 'probability-theory']"
4603689,Expectation of $\cos(X+Y)$ given $X$.,"Let $X$ , $Y$ be independent and exponentially distributed with mean $1$ . Find $\mathbb E(\cos(X+Y)\mid X)$ . What I did: \begin{align*}
\mathbb E(\cos(X+Y)\mid X) &= \mathbb E(\cos X \cos Y-\sin X\sin Y|X) \\&= \mathbb E(\cos X \cos Y|X)-\mathbb E(\sin X \sin Y|X) \\&= \cos X \mathbb E(\cos Y|X)-\sin X \mathbb E(\sin Y|X)
\end{align*} Is this correct? I am not sure we can open it like this or not. Does this imply that $\cos X \cos Y$ and $\sin X \sin Y$ are independent?","['conditional-expectation', 'trigonometry', 'probability-theory', 'exponential-distribution']"
4603739,Existence of integrating factor for a complex 1-form,"Let $M$ be a smooth $n$ -manifold and $\theta\in\Omega^1(M)$ a (smooth) $1$ -form. A clear consequence of Frobenius' theorem is that the necessary and sufficient condition for the local existence of functions $\phi,\psi$ such that $\theta=\psi d\phi$ is $d\theta\wedge\theta=0$ . In particular this implies that every $1$ -form on a $2$ -manifold has an integrating factor. Suppose now that $\theta=\theta_1+i\theta_2$ is a complex-valued differential $1$ -form on $M$ . What is the necessary and sufficient condition for the existence of a pair $\phi,\psi$ of complex-valued functions such that $$ \theta=\psi d\phi? $$ The real Frobenius theorem does not seem to help here. The context of this question is the existence of isothermal coordinates on two dimensional Riemannian manifolds. When $(M,g)$ is a two dimensional Lorentzian manifold, the Frobenius theorem provides an easy proof since we may write $ds^2=\theta_1^2-\theta_0^2$ for some orthonormal coframe $\theta_0,\theta_1$ , which can be factored as $\theta_1^2-\theta_0^2=(\theta_1+\theta_0)(\theta_1-\theta_0)=\theta_+\theta_-$ and since each $1$ -form has an integrating factor, this gives $ds^2=\psi_+\psi_- d\phi_+d\phi_-=\psi_+\psi_-(dx^2-dt^2)$ where $\phi_+=x+t$ and $\phi_-=x-t$ . The same proof does not work for positive definite metrics since then we have $ds^2=\theta_1^2+\theta_2^2$ and this can be factored as $ds^2=(\theta_1+i\theta_2)(\theta_1-i\theta_2)=\omega\bar\omega$ . If it is also true that on a two dimensional space every complex $1$ -form has an integrating factor, we are done. There is a proof of the existence of isothermal coordinates this way in a paper by Chern , however it seems to me Chern wants to prove this theorem under minimal regularity assumptions and there are lots of analytical ""nonsense"" in the proof which makes it quite difficult for me to understand. What I am hoping if one is satisfied with a proof in the $C^r$ ( $r\ge 1$ ) or even $C^\infty$ category, there is a proof that is similar to the Frobenius theorem in that it does not rely on any heavy-duty analytic or PDE-theoretic results.","['partial-differential-equations', 'differential-geometry']"
4603767,How come area of all discs in $W=\left( 1-\dfrac{\pi}{4} \right) (1-\epsilon) (a-\epsilon)$?,"I am reading the answer here by Hagen von Eitzen . Reading only the first paragraph. His answer Let $a\le 1$ be supremum of all finite disc packing areas in the unit square. Consider the square minus an inscribed disc. Because its boundary is a zero-measure set (or whatever argument also worked for the first part of the problem), it can be exhausted arbitrarily well by finitely many dyadic squares, that is for $\epsilon>0$ we find a finite set of squares such that the squares  fill a proportion $1-\epsilon$ of this shape. For each small square find a finite disc filling that fills a proportion of $a-\epsilon$ of their respective area. Then the total area filled by all discs is $$\tag1\frac\pi4+\left(1-\frac\pi 4\right)(1-\epsilon)(a-\epsilon). $$ As $\epsilon\to 0$ , the expression in $(1)$ goes $\to a+\frac\pi4(1-a)$ which must be $\le a$ . Hence $a=1$ . My understanding: Consider unit square minus inscribed disc. Call it $W$ . By similiar argument that worked for the first part of the problem: $\color{blue}{\forall\ \epsilon >0, \exists\ \text{partition}\ P\ \text{such that:}}$ $($ area of all square tiles in $W)>\left(    1-\dfrac{\pi}{4}    \right)- \epsilon$ From here how shall I reach? $\implies\ ($ area of all discs in $W)=\left(    1-\dfrac{\pi}{4}    \right)  (1-\epsilon)  (a-\epsilon)$ $\implies\ ($ area of all discs in square $)\ =\dfrac{\pi}{4} + \left(    1-\dfrac{\pi}{4}    \right)  (1-\epsilon)  (a-\epsilon)<a$ Applying limit $1 \leq a$ Also $a \leq 1$ Therefore $a=1$ EDIT Comment by Hagen: I use a scaled down (almost) optimal packing only for those tiny squares that are not coverd by the big disc enscribed ot the big square.","['circles', 'geometry', 'real-analysis']"
4603777,"Proving $\int_0^{\pi/2-\phi_x} \chi^*\cos\psi\sin\psi\,d\psi = \frac\pi4(1-\sin\phi_x)$, where $\cos\chi^*=\tan \phi_x \tan \psi$","I want to prove the following result $$
I_1(\phi_x)
=\int_{0}^{\pi/2-\phi_x}
\chi^* \cos{\psi} \sin{\psi} d\psi
= \frac{\pi}{4}[1 - \sin \phi_x ]
$$ where $\cos \chi^* = \tan \phi_x \tan \psi$ . For $\phi_x=0$ ,it holds $$I_1(0)
=\frac{\pi}{2}
\int_{0}^{\pi/2}
 \cos{\psi} \sin{\psi} d\psi
=\frac{\pi}{4} \int_{0}^{\pi/2} \sin(2\psi) d\psi=
\frac{\pi}{4} [-\frac12 \cos(2\psi) ]^{\pi/2}_0 =
\frac{\pi}{4}
$$ I have done experiments in Matlab and this is true but I do not know how to proceed in the general case. Thank you in advance for your help","['integration', 'trigonometry', 'trigonometric-integrals']"
4603792,"If $f$ is continuous and $f'(x)\ge 0$, outside of a countable set, then $f$ is increasing","PROBLEM. Let $f:[a,b]\to\mathbb R$ be a continuous function, such that $f'(x)\ge 0$ , for all $x\in [a,b]\setminus A$ , where $A\subset [a,b]$ is a countable set. Show that $f$ is increasing. Attention. In this problem, we DO NOT assume that $f$ is differentiable in the whole $[a,b]$ . Notes. (1) If we assume that $f$ is differentiable in the whole interval, then we can easily show that $f'(x)\ge 0$ , everywhere. For otherwise, if $f'(x_0)=c<0$ , for some $x_0\in [a,b]$ , then by virtue of Darboux's Theorem , $(c,0)\subset f'([a,b])$ , and hence, $f'(x)<0$ , for uncountably many $x$ 's. (2) The conclusion of the problem does not hold if we replace the assumption $A$ is countable with $A$ is a set of measure zero . Take for example the Devil's staircase , with a negative sign in front. (3) If the hypothesis $f'(x)\ge 0$ , is replaced by $f'(x)=0$ , then the conclusion becomes f is constant .","['monotone-functions', 'real-analysis', 'continuity', 'calculus', 'derivatives']"
4603796,about the definition of connected sets in $\mathbb R^n$,"(Edited to make reference to the topological space $X$ precise.) A subset $A$ of a topological space $X$ is connected if there are no two open subsets $O_1$ and $O_2$ of $X$ such that (a) $A \subseteq O_1 \cup O_2$ , (b) $A \cap O_1 \neq \emptyset$ , (c) $A \cap O_2 \neq \emptyset$ ,
and (d) $A \cap O_1$ and $A\cap O_2$ are disjoint.
Call the subset $A$ brown if in the definition above, we replace (d) by (d') $O_1$ and $O_2$ are disjoint. The definitions imply that every connected set is brown.
The opposite is not true: take the topological space $X = \{1,2,3\}$ , where a subset of $X$ is open if it contains 1 or if it is the empty set. The set $A = \{2,3\}$ is not connected yet it is brown. My question is whether for sets in the Euclidean space $X = \mathbb R^n$ (with the standard Euclidean topology), connectedness and being brown are equivalent.",['general-topology']
4603836,On generalizing $\frac{17-\sqrt{17}}8 =\sin^2(t)+\sin^2(2t)+\sin^2(4t)+\sin^2(8t)$?,"Given $\color{blue}{t = 2\pi/p}$ for the appropriate prime $p=4m+1$ . I. Sine $$\begin{align}
\frac{5+\sqrt{5}}8 &=\sin^2(t)\\ 
\frac{13+\sqrt{13}}8 &=\sin^2(t)+\sin^2(3t)+\sin^2(4t)\\
\frac{17+\sqrt{17}}8 &=\sin^2(3t)+\sin^2(5t)+\sin^2(6t)+\sin^2(7t)\\
\frac{29+\sqrt{29}}8 &=\sum_{k=1}^7\sin^2(a_k\, t)
\end{align}$$ with the seven $a_k = 1,4,5,6,7,9,13.$ And so on for other prime $p=4m+1.$ For the opposite sign, one uses the remaining integers $b_k \leq \frac{p-1}2.$ For example, $$\frac{17-\sqrt{17}}8 =\sin^2(t)+\sin^2(2t)+\sin^2(4t)+\sin^2(8t)$$ where the $a_k$ are simply $2^n$ . (The next Fermat prime $p=257$ isn't so nice since it has $256/4 = 64$ sine terms.) II. Cosine This uses the same set of multipliers $a_k$ . $$\begin{align}
\frac{3-\sqrt{5}}8 &=\cos^2(t)\\ 
\frac{11-\sqrt{13}}8 &=\cos^2(t)+\cos^2(3t)+\cos^2(4t)\\
\frac{15-\sqrt{17}}8 &=\cos^2(3t)+\cos^2(5t)+\cos^2(6t)+\cos^2(7t)\\
\frac{27-\sqrt{29}}8 &=\sum_{k=1}^7\cos^2(a_k\, t)
\end{align}$$ with the same seven $a_k = 1,4,5,6,7,9,13.$ For the opposite sign, $$\frac{15+\sqrt{17}}8 =\cos^2(t)+\cos^2(2t)+\cos^2(4t)+\cos^2(8t)$$ III. Conclusion Given prime $p=4m+1$ and $t = 2\pi/p.$ The pattern clearly is, $$\frac{p\pm\sqrt{p}}8 = \sum_{k=1}^m \sin^2(a_k\, t)$$ $$\frac{(p-2)\mp\sqrt{p}}8 = \sum_{k=1}^m \cos^2(a_k\, t)$$ Question: I used Mathematica's integer relations to find the above examples. But, for any prime $p=4m+1$ , what is a clever and faster algorithm to derive the correct set of $a_k$ ?","['galois-theory', 'trigonometry', 'radicals', 'sequences-and-series']"
4603842,Solving $x f'(x) - f'(x)f(x) + \alpha x^{\alpha - 1} = 0$.,"I have been trying to solve the differential equation $$x f'(x) - f'(x)f(x) + \alpha x^{\alpha - 1} = 0 $$ where $\alpha > 0$ is a fixed parameter, $f(0) = 0$ is the boundary condition, and the domain is $\mathbb{R}^+$ . In case of $\alpha = 1$ , Wolfram Alpha suggests that $$ f(x) = W(-e^{-x- 1}) + x+ 1$$ where $W$ is the product log function. However, Wolfram Alpha won't give me a solution for the general case. Is it possible to get a 'nice' expression for $f$ here? If not, can one still gain some qualitative insight into its properties?",['ordinary-differential-equations']
4603867,The angle between corresponding tangent lines on two Bertrand curves is constant,"The angle between corresponding tangent lines on two Bertrand curves is constant and torsions of the two associate Bertrand curves have the same sign and their product is constant Two distinct parametrized curves $\boldsymbol{x}$ and $\boldsymbol{y}$ are called Bertrand mates if for each $t$ , the normal line to $\boldsymbol{x}$ at $\boldsymbol{x}(t)$ equals the normal line to $\boldsymbol{y}$ at $\boldsymbol{y}(t)$ . So, we have $$\boldsymbol{y}(s_1)=\boldsymbol{x}(s)+a(s)\boldsymbol{n}(s)$$ Now we have to calculate $\boldsymbol T_x$ and $\boldsymbol T_y$ : $$
\begin{align}
\boldsymbol T_y&=\boldsymbol y'(s_1)\\
&=\boldsymbol x'(s)+a'(s)\boldsymbol n(s)+a(s)\boldsymbol n'(s)\\
&= \boldsymbol x'(s)+a'(s)\boldsymbol n(s)+a(s)(-\kappa \boldsymbol  t(s)+\tau\boldsymbol  b(s))\\
&=\boldsymbol  x'(s)-\kappa a(s)\boldsymbol  t(s)+a'(s)\boldsymbol n(s)+\tau a(s)\boldsymbol  b(s)\\\\
\boldsymbol T_x&=\boldsymbol x'(s)
\end{align}
$$ Now, consider, $\boldsymbol T_x . \boldsymbol T_y=\|\boldsymbol T_x\|\|\boldsymbol T_y\|\cos\theta$ . But from this, I couldn't see the angle $\theta$ is constant. Any help will be appreciated.","['curves', 'frenet-frame', 'differential-geometry']"
4603881,Causal system giving a non-causal output?,"I have just written a Python code ploting DFT's using the convolution product : $$y[t] = u[k] * h[k] = \sum_{k=-\infty}^{+\infty} u[k] h[t-k]$$ I'll take a high resolution so the graph is more precise. Here is a simple example,for $h(t) = \delta(t)$ and $u(t) = \sin(t)\nu(t)$ : So far, so good. The causal system gives a causal answer. Yet, for $h(t) = \nu(t) - \nu(t-6)$ , I obtain this graph: The causal signal gives a non-causal answer. Indeed, $y(t)$ is not equal to $0$ for $t<0$ How is it possible ? Maybe is it a mistake from my code ?","['convolution', 'fourier-transform', 'discrete-mathematics', 'planar-graphs', 'signal-processing']"
4603975,"The ideal $I=(3,1+\sqrt{-23})\subseteq \mathcal O_{\mathbb{Q}(\sqrt{-23})}$","Intro: $K=\mathbb Q(\sqrt{-23})$ be a number field with obvious minimum polynomial. $\mathcal O_K$ be its ring of integers which is determined as $$\mathcal O_K=\mathbb Z\left[\frac{1+\sqrt{-23}}{2}\right]$$ since $-23\equiv 1 \mod 4$ I want to determine if $I,I^2,I^3$ are principal or not. I was able to calculate norm of $I$ as: $N(I)=3$ . Then using basic ideas I can show that $I$ is not principal. $N(I^2)=9=\left(x+y/2\right)^2+\frac{23}4y^2=3^2$ Only solution is $x=\pm 3,y=0$ Why we can't say that $I^2$ is generated by the ideal $(3)$ ? I want to show $I^3$ is principal ideal. Norm is multiplicative so $N(I^3)=27$ So $N(I^3)=27=\left(x+y/2\right)^2+\frac{23}4y^2=3^3$ Has following integer solutions $(x,y)=(-3,2),(-1,2),(1,2),(3,-2)$ And I am stuct to find which one is appropriate candidate, which kind of theorem says/guarantees that?","['algebraic-number-theory', 'abstract-algebra', 'modular-arithmetic', 'ideals']"
4603976,Double integral $\int_0^\infty \int_0^\infty \frac{e^{-(x+y)/2}x^j y^k}{ax+by+c} dx dy$,"In an ideal scenario I would like to have an analytic expression for the integral $$
\operatorname{I}\left(\,{a,b,c,j,k}\,\right) =
\int_{0}^{\infty} \int_{0}^{\infty}
\dfrac{\operatorname{e}^{-\left(\,{x + y}\,\right)/2}\,\,x^{j}\, y^{k}}{ax + by + c}\,{\rm d}x\,{\rm d}y
$$ for positive real $a,b,c$ and non-negative integers $j,k$ , which can then be evaluated for very large values of $c$ . The reason I mention the latter circumstance is because while Mathematica ( $13.1$ for sure) is able to take this integral for small values of $j$ and $k$ , the resulting expression is numerically unstable for large values of $c$ (meaning that it's a product of terms exponentially large and small in $c$ ). For example, in the case $j = k = 0$ one gets: $$
\frac{2 e^{\frac{c}{2 b}} \operatorname{Ei}\left(-\frac{c}{2 b}\right)-2 e^{\frac{c}{2 a}} \operatorname{Ei}\left(-\frac{c}{2 a}\right)}{a-b}
$$ Would appreciate any advice on how to: Take the original integral analytically, Efficiently evaluate $\operatorname{e}^\alpha \operatorname{Ei}(-\alpha)$ for large values of $\alpha$ . Given that Mathematica can take this integral for small $j$ and $k$ , I would guess that there exists a recursive relation.","['integration', 'numerical-methods', 'definite-integrals']"
4603991,"Dual spaces isomorphic, implies vector spaces itself are isomorphic?","When I have two vector spaces $W, V$ over $k$ a field. And I know that the algebraic dual spaces of $V$ and $W$ are isomorphic. Can I conclude, (in the infinite dimensional case) that $V$ and $W$ are isomorphic? I am saying algebraic dual, because i don't want to be confused with the dual of continuous linear functionals. But I am just using the definition of dual space, that everyone uses in linear algebra. Say I know, that I have a linear map $\varphi: V \to W$ such that its dual map is an isomorphism, can I then conclude it was an isomorphism all along? I know this is true if $\varphi$ is assumed to be injective, because then the injectivity of the dual map, implies surjectivity of the original map. Can I say something, if my map $\varphi$ was assumed to be surjective and the dual map is an isomorphism?",['linear-algebra']
4604019,Understanding Central Limit Theorem vs. Law of Large Numbers,"I am trying to clarify these two concepts - and understand the differences between the Central Limit Theorem ( https://en.wikipedia.org/wiki/Central_limit_theorem ) and the Weak Law of Large Numbers ( https://en.wikipedia.org/wiki/Law_of_large_numbers ). As an example, suppose I have a coin and I don't know the true probability of Heads or Tails - I start to flip the coin again and again: The Law of Large Numbers states that if I flip this coin enough times, I will get an estimate of the true probability of getting a Heads The Central Limit Theorem states that as I flip the coin again and again, the distribution  for the probability of getting a Heads will follow a Standard Normal Distribution Is my understanding of this correct? Thanks!","['statistics', 'central-limit-theorem', 'law-of-large-numbers', 'probability']"
4604053,Alternative proofs of this Inequality,"So I was reading a paper which made the claim ""It is easy to see that $\frac{1-e^{-\alpha}}{\alpha} > 1-\frac{\alpha}{2} > \frac{1}{1+\alpha}$ when $0 < \alpha < 1$ ."" Verifying that $1-\frac{\alpha}{2} > \frac{1}{1+\alpha}$ only involves some simple algebra, but in order to prove that $\frac{1-e^{-\alpha}}{\alpha} > 1-\frac{\alpha}{2}$ I had to use what I have only seen referred to as the ""racetrack theorem\principle"" from elementary calculus twice. Namely, if $f(0) = g(0)$ and $f'(x) \geq g'(x)$ for all $x \geq 0$ , then $f(x) \geq g(x)$ for all $x \geq 0$ . Taking derivatives involving the quotient rule is somewhat of a pain in the ass, so I was wondering if there were any slick ways of proving this inequality? Or maybe I missed a far simpler way of proving this inequality?","['analysis', 'alternative-proof', 'functions', 'inequality', 'exponential-function']"
4604055,Algebra structure of $\mathbb{R}[Q_8]$ where $Q_8$ is the quaternion group of order $8$.,"Let $Q_8$ be the quaternion group of order $8$ . I would like to determine the algebra structure for $\mathbb{R}[Q_8]$ . I think $\mathbb{R}[Q_8] \cong \mathbb{R}^4 \oplus \mathbb{H}$ . Maybe a simpler question to all of this is: why is $\mathbb{R}[Q_8] \not\cong \mathbb{R}^4 \oplus M_2(\mathbb{R})$ ? My work so far: By Maschke's Theorem, $\mathbb{R}[Q_8]$ is semisimple.
By Artin-Wedderburn Theorem, since $\mathbb{R}[Q_8]$ is a finite-dimensional semisimple $\mathbb{R}$ -algebra, it follows that $$\mathbb{R}[Q_8] \cong M_{n_1}(D_1) \oplus \dots \oplus M_{n_k}(D_k)$$ where each $n_i$ is a positive integer and $D_i$ is a division ring over $\mathbb{R}$ . By Frobenius theorem (of real division algebras), it follows that each $D_i$ is isomorphic to either $\mathbb{R}$ (1-dimensional), $\mathbb{C}$ (2-dimensional), or the quaternions $\mathbb{H}$ (4-dimensional). Thus, we begin a combinatorial argument: $$\mathbb{R}[Q_8] \cong \mathbb{R}^a \oplus M_2(\mathbb{R})^b \oplus \mathbb{C}^c \oplus M_2(\mathbb{C})^d \oplus \mathbb{H}^e$$ (Note that $\mathbb{R}[Q_8]$ is an $8$ -dimensional group algebra which is why $M_n(\mathbb{R})$ terms don't exist for $n>2$ and similar reasoning for $M_n(\mathbb{C})$ and $M_n(\mathbb{H})$ .) My argument will rely on the following facts: (i) $Q_8$ has five conjugacy classes so $k=5=a+b+c+d+e$ . (ii) $\operatorname{dim}(\mathbb{R}[Q_8]) = 8 = a + 4b + 2c + 8d + 4e$ (iii) $\mathbb{R}[Q_8]$ is non-commutative because $Q_8$ is non-abelian so we must have at least one of $b$ , $d$ , or $e$ to be nonzero. [Claim 1: d = 0] First of all, to satisfy (ii), $d$ must be either $0$ or $1$ . If $d=1$ , then we immediately get $a=b=c=e=0$ which contradicts (i) so $d=0$ . [Claim 2: c = 0] Similarly, if $c>1$ then we cannot simultaneously satisfy (i) and (ii) so $c$ is either $0$ or $1$ . To satisfy both conditions with $c=1$ , we are forced to have $a=4$ and $b=d=e=0$ which contradicts (iii). Thus, $c=0$ . [Claim 3: a = 4] Condition (ii) is now simplified to $8=a+4b+4e$ . This equation can only be satisfied when $a\in \{ 0 ,4,8 \}$ . However, $a=8$ contradicts (iii) and $a=0$ contradicts (i) so we must have that $a=4$ . At this point, I now have that $\mathbb{R}[Q_8] \cong \mathbb{R}^4 \oplus M_2(\mathbb{R})$ OR $\mathbb{R}[Q_8] \cong \mathbb{R}^4 \oplus \mathbb{H}$ . Both possibilities satisfy (i)-(iii). Intuition tells me that I should get a copy of $\mathbb{H}$ and Wikipedia suggests this too. However, their reasoning seems to rely on more advanced machinery on irreducible characters/representations that I do not have. I thought about possibly arguing by nilpotent elements or the center of the algebra but I don't know how to rigorously argue that it should be isomorphic to the latter. Ideally, I am hoping there would be a fundamental property about these group algebras that I am overlooking to provide a simple argument rather than going into heavier machinary.","['abstract-algebra', 'quaternions']"
4604095,An indecomposable abelian group $G$ is either torsion or torsion-free,"I've been self-studying Rotman's An Introduction to the Theory of Groups , and Corollary 10.44 is that ""an indecomposable [abelian] group $G$ is either torsion or torsion-free"". The proof given is as follows: Assume that $0 < tG < G$ . Now $tG$ is not divisible, lest it be a summand of $G$ , so that Corollary 10.43 shows that $G$ has a (cyclic) summand, contradicting indecomposability. The referenced Corollary 10.43 states: A torsion [abelian] group $G$ that is not divisible has a $p$ -primary cyclic direct summand (for some prime $p$ ). Now, in the last step of this proof, I agree that $tG$ has this direct summand (it is the torsion group in the hypothesis of Corollary 10.43), but I am not sure why this implies it is also a summand of the whole group $G$ . I am looking for either an explanation of why this is the case, a different proof of the theorem, or a disproof (since I see from this question that the corresponding statement is not true for general modules over integral domains).","['proof-explanation', 'alternative-proof', 'abstract-algebra', 'group-theory', 'abelian-groups']"
4604110,My proof of the theorem : finitely many disjoint discs can be inscribed in a unit square with total area approaching 1,"This question has the answer here by Hagen von Eitzen . I consider the proof as unwantedly lengthy. I am looking to simplify the proof. My proof: Consider a convex shape $S$ of positive area $A$ inside the unit square. Let $a\le 1$ be the supremum of all subsets of the unit square that can be obtained as disjoint union of finitely many scaled and translated copies of $S$ . Partition the square into $n\times n$ smaller squares (see picture).
There are three types of such small squares: $e$ exterior squares (white in the picture), $i$ interior squares (light red in the image) and $b$ boudary squares (blue/purple). Of course $e+b+i=n^2$ $$\dfrac{i}{n^2} < A$$ $$\implies\dfrac{i}{n^2} + \dfrac{b}{n^2} < A + \dfrac{b}{n^2} \tag1$$ Picking a finite packing that covers $\ge a-\epsilon$ , for some $\epsilon$ , we can put a scaled-down copy of this packing into each of the $e$ ""white"" squares and, together with the original shape $S$ , obtain a finite packing of the unit square that covers $e \dfrac{a- \epsilon}{n^2}  + A$ \begin{align}
\text{Area of finite disc packing}
&= e \dfrac{a- \epsilon}{n^2}  + A  \tag2
\end{align} \begin{align}
a>\text{Area of finite disc packing}
&= e \dfrac{a- \epsilon}{n^2}  + A \tag {by 2}\\
&= \dfrac{e}{n^2} (a- \epsilon) + A \\
&= \left[   1- \left(   \dfrac{i}{n^2} + \dfrac{b}{n^2}  \right)  \right]   (a- \epsilon) + A\\
&\geq \left[   1- \left(   A + \dfrac{b}{n^2}  \right)  \right]   (a- \epsilon) + A \tag {by 1}\\
&=\left[   1 - A - \dfrac{b}{n^2}    \right]   (a- \epsilon) + A
\end{align} $$\implies \left[   1 - A - \dfrac{b}{n^2}    \right]   (a- \epsilon) + A<a$$ As $n\to\infty$ and $\epsilon\to 0$ the LHS converges to $a+(1-a)A$ . According to a limit theorem, this limit must be $\le a$ . Thus we conclude $a=1$ . My question: Is my proof correct and sufficient?","['solution-verification', 'circles', 'geometry', 'real-analysis']"
4604121,Property of Antiderivatives,"Consider two differentiable functions $f:\mathbb R \rightarrow \mathbb R$ and $g:\mathbb R \rightarrow \mathbb R$ . Suppose that there exists a $\delta$ such that for $x>\delta$ , $f'(x)>g'(x)$ . I am trying to show that this implies there exists a $\delta_2$ such that for $x>\delta_2$ , $f(x)>g(x)$ . So far, I have used $f(x)=\int_{\delta}^x f'(t)dt + f(\delta)$ and similarly for $g(x)$ to show that this obviously holds if $f(\delta)>g(\delta)$ . I am now trying to construct the case where $f(\delta)<g(\delta)$ .","['calculus', 'derivatives']"
