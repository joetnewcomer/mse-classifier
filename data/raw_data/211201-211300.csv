question_id,title,body,tags
4248517,Weaking the path test for multivariable limits,"The multivariable limit $\lim_{(x,y)\to (x_0,y_0)} f(x,y)$ exists and is equal to some scalar $L$ if and only if the limit $\lim_{t\to 1} f(r(t))$ exists and is equal to $L$ for all functions $r:\mathbb{R}\to \mathbb{R}^2$ such that $\lim_{t\to 1} r(t) = (x_0, y_0)$ . It is common to use this fact as a way to conclude that a certain limit does not exist, by checking that taking the limit along different paths leads to different limits or by showing that the limit along a certain path does not exist. I wonder if this characterizarion can be weakened in the following way: The multivariable limit $\lim_{(x,y)\to (x_0,y_0)} f(x,y)$ exists and is equal to some scalar $L$ if and only if $\lim_{y\to y_0} f(x_0,y)=L$ and $\lim_{x\to x_0} f(x,g(x)) = L$ for all functions $g:\mathbb{R}\to \mathbb{R}$ such that $\lim_{x\to x_0} g(x) = y_0$ . That is, it the limit exists and is the same going through all graphs of functions $g(x)$ and through the $y$ axis, can I conclude the existence of the multivariable limit? EDIT: I substituted my initial condition of $\lim_{y\to y_0} \lim_{x\to x_0} f(x,y)=L$ by $\lim_{x\to x_0} f(x,g(x)) = L$ since, as José Carlos Santos noted in the comments, formalizes better the idea of approaching the limit vertically.","['multivariable-calculus', 'limits', 'calculus', 'real-analysis']"
4248598,Convergence in bounded Lipschitz metric from convergence of bounded Lipschitz expectations,"Let $(X_n)_{n \in \mathbb{N}}$ and $X$ be random variables and assume that $$
E(f(X_n)) \to E(f(X))
$$ for all bounded, Lipschitz functions $f$ . Prove that $$
\sup_{f \in \textrm{BL}_1} |E(f(X_n)) - E(f(X))| \to 0
$$ where $\textrm{BL}_1$ consists of all $f$ with $\| f \|_\infty \leq 1$ and Lipschitz constant less than $1$ . Attempt I think the idea is to choose a compact set $K$ such that both $\sup_n \mathbb{P}(X_n \not\in K) < \varepsilon$ and $\mathbb{P}(X \not\in K) < \varepsilon$ letting us write $$
\sup_{f \in \textrm{BL}_1} |E(f(X_n)) - E(f(X))| \leq \sup_{f \in \textrm{BL}_1} |E(f(X_n)\mathbf{1}_{(X_n \not\in K)}) - E(f(X)\mathbf{1}_{(X \not\in K)})|  + \sup_{f \in \textrm{BL}_1} |E(f(X_n)\mathbf{1}_{(X_n \in K)}) - E(f(X)\mathbf{1}_{(X \in K)})|.
$$ Dealing with the first term is straight forward and we get $$
\sup_{f \in \textrm{BL}_1} |E(f(X_n)\mathbf{1}_{(X_n \not\in K)}) - E(f(X)\mathbf{1}_{(X \not\in K)})| \leq 4 \varepsilon.
$$ I'm not sure how to deal with the second term though. It is clear that the inside of the second supremum goes to $0$ for each $f$ but I presume there's some way of relating this to the Lipschitz constant? Any ideas? EDIT: Perhaps I need to apply Arzela-Ascoli somehow?","['probability-theory', 'weak-convergence']"
4248618,Issue with getting the derivative of an integral,"I'm trying to find the derivative of the following integral: $\int_{\cos(x)}^{1} \sqrt{1-t^2} dt$ My steps are: $\frac{d}{dx} \left( \int_{\cos(x)}^{1} \sqrt{1-t^2} dt \right) = -\frac{d}{dx} \left( \int_{1}^{\cos(x)} \sqrt{1-t^2} dt \right) = -\sqrt{1-\cos^2(x)} \frac{d}{dx}\left( \cos(x) \right) = -\sqrt{1-\cos^2(x)} \cdot -\sin(x) = \sin^2(x) $ However, the book says the right answer is $ \lvert \sin(x) \rvert \sin(x) $ What am I missing here?","['calculus', 'derivatives', 'trigonometry']"
4248665,Combinatorics using averages,"How many solutions exist for $$x+2y+4z=100$$ in non-negative integers? The author, Martin Erickson, in his book, Aha! Solutions , published by MAA, gives the following brief solution : There are $26$ choices for $z$ , namely, all integers from $0$ to $25$ . Among these choices, the average value of $4z$ is $50$ . So, on average, $x+2y=50$ . In this equation, there are $26$ choices for $y$ , namely, all integers from $0$ to $25$ . The value of $x$ is determined by the value of $y$ . Hence, altogether there are $26^2=676$ solutions to the original equation. Can somebody explain to me why this mindblowing solution works using averages?","['proof-explanation', 'average', 'combinatorics']"
4248704,Lagrange Multiplier in higher codimension,"Consider the Lagrange Theorem: Let $U \subset \mathbb{R}^{n+k}$ be an open set, $f:U \rightarrow \mathbb{R}$ of class $C^1$ , $c \in \mathbb{R}^k$ be a regular value of $g$ and $M = g^{-1}(c)$ . Then $$x \ \text{is a critical point of } \ f|_M \Longleftrightarrow \exists\ \lambda \in \mathbb{R}^k  \ \text{with}\ \nabla f(x) = \lambda^T \nabla g'(x_0).$$ First: Does anyone has books on the subject? I search for it on Spivak's book and Rudin, but did't find anything. i would appreciate any help! Second: Consider the function $$\mathcal{L}(x,\lambda) = f(x) -\lambda^T (g(x)-c)$$ i must show that $(x_0,\lambda_0)$ is a critical point of $\mathcal{L}(x,y)$ if, and only if, $x_0$ is a critical point for $f|_{g(x)=c}$ .  I'm having some troubles to show this. Any hint on this exercise will be very helpful!","['lagrange-multiplier', 'analysis', 'real-analysis']"
4248746,Deriving the equation for radial wave function,"I'm trying to solve Schrodinger's equation of an exciton using the separation of variables method: $\psi = RY$ . Here's the equation I've already derived: $$
\frac{2\mu r^2}{\hbar^2}(E+\frac{e^2}{\epsilon r})+\frac{r^2}{R}\frac{\partial^2 R}{\partial r^2}+\frac{r}{R}\frac{\partial R}{\partial r} = -\frac{1}{Y}\frac{\partial^2Y}{\partial\theta^2}
$$ Where the radial wave function is $R = R(r)$ and angular part is $Y = Y(\theta)$ . Since the two sides of the equation only depend on a single variable, they are both constants. I've found the angular part is $$
Y_m(\theta) = \frac{1}{\sqrt{2\pi}}e^{im\theta}
$$ I'm having trouble solving the left part to obtain the radial equation. How can I do that? Thanks!","['wave-equation', 'derivatives', 'mathematical-physics', 'partial-differential-equations']"
4248776,"Lower central series of the Unitriangular group $UT(n, \mathbb{Z}_p)$","This is Exercise 5.44 from Rotman's book "" An Introduction to the theory of Groups (4th Ed) "". Specificaly, the exercise asks us to prove that the $i$ -eth term in the lower central series is the set of all unitriangular matrices with "" $0$ "" in the $i$ super-diagonals. In particular, this means the group is nilpotent of class $n-1$ . While I believe I could do it in a rather straightforward way by calculating the commutators explicitly (by hand), Rotman provides a hint: ""Given $A \in UT(n, \mathbb{Z}_p)$ , consider the powers of the matrix $A -I_n$ "". What I don't get is how to use this hint... I get that $A - I_n$ is nilpotent of class $n - z$ , where "" $z$ "" denotes the number of super-diagonals with only $0$ . So, for instance, if $n = 2$ and $$A = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}$$ then $$A - I_2 = \begin{bmatrix} 0 & 2 \\ 0 & 0 \end{bmatrix}$$ which is nilpotent of class $2$ ( $(A-I_2)^2 = 0$ ). However, how can this be used to solve the problem at hand, in a way to (hopefully) avoid too much explicit calculation? Please provide only a hint (if possible). Thanks in advance!","['matrices', 'nilpotence', 'group-theory', 'nilpotent-groups']"
4248793,"Prove that there exists a natural number $n$ that has more than $2017$ divisors $d$ satisfying $\sqrt{n} \le d < 1{,}01\sqrt{n}$","Prove that there exists a natural number $n$ that has more than $2017$ divisors $d$ satisfying $$\tag 1 \sqrt n \le d < 1.01\sqrt n$$ My reasoning was that $\tag 2 1.01\sqrt n-\sqrt n\ge2019$ must occur. Otherwise, there won't be enough space to fit in enough numbers in $(1)$ . $(2)$ gives us that $n\geq 201900^2$ . We also have that $201900^2<201900!$ so we can pick any factorial greater than or equal to $201900!$ and that satisfies the desired condition. Is this solution correct?","['contest-math', 'divisibility', 'number-theory', 'elementary-number-theory', 'solution-verification']"
4248843,Integral from Ramanujan to Hardy,"I came across the interesting family of integrals that was studied by Ramanujan and was one of subjects of his first letter to Hardy. $$
\phi(n):=\int_{0}^{\infty} \frac{\cos n x}{e^{2 \pi \sqrt{x}}-1} d x
$$ He offers a functional equation $$
\int_{0}^{\infty} \frac{\sin n x}{e^{2 \pi \sqrt{x}}-1} d x=\phi(n)-\frac{1}{2 n}+\phi\left(\frac{\pi^{2}}{n}\right) \sqrt{\frac{2 \pi^{3}}{n^{3}}}
$$ And give some special cases, $$
\phi(0)=\frac{1}{12} ; \quad \phi\left(\frac{\pi}{2}\right)=\frac{1}{4 \pi} ; \quad \phi(\pi)=\frac{2-\sqrt{2}}{8} ; \quad \phi(2 \pi)=\frac{1}{16}
$$ $$
\begin{gathered}
\phi\left(\frac{2 \pi}{5}\right)=\frac{8-3 \sqrt{5}}{16} ; \quad \phi\left(\frac{\pi}{5}\right)=\frac{6+\sqrt{5}}{4}-\frac{5 \sqrt{10}}{8}  \\
\phi\left(\frac{2 \pi}{3}\right)=\frac{1}{3}-\sqrt{3}\left(\frac{3}{16}-\frac{1}{8 \pi}\right)
\end{gathered}
$$ The particular case $\phi(0)=\frac{1}{12}$ is easy to proof: $$
\begin{aligned}
\phi(0)&=\int_{0}^{\infty} \frac{1}{e^{2 \pi \sqrt{x}}-1} d x \qquad (x \mapsto x^2)\\
&=2 \int_{0}^{\infty} \frac{x}{e^{2 \pi x}-1} d x \qquad (2\pi x \mapsto x)\\
&=\frac{1}{2 \pi^2} \int_{0}^{\infty} \frac{x}{e^{ x}-1} d x\\
&=\frac{1}{2 \pi^2} \zeta(2)\\
&=\frac{1}{12} \qquad \blacksquare
\end{aligned}
$$ I am interesting in computing $\phi(\pi)=\frac{2-\sqrt{2}}{8}$ $$
\phi(n)=\int_{0}^{\infty} \frac{\cos n x}{e^{2 \pi \sqrt{x}}-1} d x
$$ Letting $x \mapsto x^2$ we obtain $$
\begin{aligned}
\phi(n)&=2\int_{0}^{\infty} \frac{x\cos n x^2}{e^{2 \pi x}-1} d x \qquad (2 \pi x \mapsto x)\\
&=\frac{1}{2 \pi^2}\int_{0}^{\infty} \frac{x\cos \frac{n x^2}{4 \pi^2} }{e^{ x}-1} d x
\end{aligned}
$$ Recall the Bernoulli polynomials generating function $$\sum_{k=0}^{\infty} \frac{B_{k} \, x^k}{k!} = \frac{x}{e^{x} - 1}$$ Then, letting $ n \rightarrow \pi$ we get $$
\begin{aligned}
\phi(\pi)&=\frac{1}{2 \pi^2}\sum_{k=0}^{\infty} \frac{B_{k} }{k!}\int_{0}^{\infty} x^k\cos \frac{ x^2}{4 \pi}  d x \qquad (\frac{x^2}{4 \pi}=w)\\
&=\frac{1}{2 \pi^2}\sum_{k=0}^{\infty} \frac{B_{k} }{k!}(2 \sqrt{\pi})^{k+1}\int_{0}^{\infty} w^{\frac{k-1}{2}}\cos w  d w\\
&=\frac{1}{2 \pi^2}\sum_{k=0}^{\infty} \frac{B_{k} }{k!}(2 \sqrt{\pi})^{k+1}\Gamma \left(\frac{k+1}{2} \right)\cos\left(\frac{(k+1) \pi}{4} \right)
\end{aligned}
$$ Which does not seem very promising! Any idea how to proceed?",['integration']
4248885,Can $\int\limits_0^\infty e^{ix^ x}dx$ be written without a limit?,"We know about the Fresnel Integrals : $$C(x)=\int \cos x^2 \, dx,\quad S(x)=\int \sin x^2 \, dx$$ which can also be written as: $$\int e^{ix^2}dx=C(x)+i\,S(x)$$ To make a more interesting and tetration based integral with a rapidly oscillating part of $\mathrm{Re(x)\ge0}$ integral converges. To make a more general result possible, let one consider the goal integral: $$\int e^{ix^ x}dx=\int \cos\ x^x dx+i\,\int \sin\ x^x dx$$ Here is the graph over the real line of the integrand: Then the exponential Taylor Series comes to mind. The series converges for all x just like the expansion for $e^x$ . We can integrate by each term to switch the sum and integral. Note the logarithm may also affect convergence. It uses the Exponential Integral function and Regularized Incomplete Gamma function : $$\int e^{ix^x}dx=\int \sum_{n=0}^\infty\frac{i^n x^{nx}}{n!}dx=\sum_{n=0}^\infty \frac{i^n}{n!}\sum_{k=0}^\infty \frac{n^k}{k!}\int x^k \ln^k(x)dx\mathop=^{y=\ln(x)} \sum_{n=0}^\infty \frac{i^n}{n!}\sum_{k=0}^\infty \frac{n^k}{k!}\int e^{ky+1} y^k dy =C- \sum_{n=0}^\infty\sum_{k=0}^\infty \frac{i^nn^k\ln^{k+1}(x) \text E_{-k}(-(k+1)\ln(x))}{n!k!} = C+\sum_{n=0}^\infty\sum_{k=1}^\infty \frac{i^n(-n)^{k-1} Q(k, -k \ln(x)) \ln^{k-1}(x) \ln^{-(k+1)}(x) }{n! k^k}\mathop=^{\small{\ln^{k-1}(x) \ln^{-(k+1)}(x)=1}} C-\sum_{n=0}^\infty\sum_{k=1}^\infty \frac{i^n(-1)^k n^{k-1} Q(k, -k \ln(x)) }{n! k^k} $$ If this is correct, then how can I find the following? Note that when evaluating the bounds, the $”-\ln(0)”$ in the second argument Regularized Incomplete Gamma function will grow faster than the index of the summation, so it can be ignored: $$\int_0^\infty e^{i x^x} dx= \int_0^\infty \cos\ x^x dx +i\,\int_0^\infty \sin\ x^xdx= -\lim_{x\to-\infty}\sum_{n=0}^\infty\sum_{k=1}^\infty \frac{(-1)^{k+\frac n2} n^{k-1} Q(k, kx) }{n! k^k} $$ Related question: Can $$\int_0^\infty x^{-x} dx$$ be written without a limit? Related answer which has an experimental solution: Improper integral inequality including the golden ratio and $$\int_0^\infty \sin\left(x^{-x}\right)$$ Is there a good way to integrate this function and find this value? The limit makes the expression more complicated and perhaps we can write the answer without it. Please correct me and give me feedback!","['improper-integrals', 'power-towers', 'sequences-and-series', 'tetration', 'complex-numbers']"
4248984,"Let $a_n=(1+\frac{1}{n})^n$, show that $\lim_{n\to\infty}n(e-a_n)\geq \frac e2$.","Let $a_n=(1+\frac{1}{n})^n$ , show that $$\lim_{n\to\infty}n(e-a_n)\geq \frac e2.$$ We know easily by calculus that $\lim_{n\to\infty}n(e-a_n)=\frac e2$ . However, can we give an easier proof without using functional limit, but only the following prosition. if $a_n>b_n$ for large $n$ , then $\lim a_n\geq \lim b_n$ . What I should is just consider $n(a_{n^2}-a_n)$ , and show it is $\geq \frac e2$ . But is seems not easy to compare $a_{n^2}$ and $a_n$ .","['limits', 'calculus', 'exponential-function', 'real-analysis']"
4249000,Resolve: $4\sin(2x)+4\cos(x)-5=0$,"The first thing that comes to mind is to substitute $\sin(2x)=2\sin(x)\cos(x)$ and so we have: \begin{align*}
8\sin(x)\cos(x)+4\cos(x)-5=0
\end{align*} But after that I can't see what other identity to apply, I've been checking several times, if it is necessary to add something or multiply properly, but I can't find anything.","['cubics', 'trigonometry', 'quartics']"
4249045,Show that $E[e^{(2(m-1)X^2)}] \leq m$,Can someone help me with the following exercise: Let $X$ be random variable satisfying $P(X \geq \epsilon ) \leq \exp(-2m\epsilon^2)$ . Show that $E[e^{(2(m-1)X^2)}] \leq m$ . Since $Z = e^{(2(m-1)X^2)}$ is nonnegative I wanted to solve this by using $E[Z] = \int_0^{\infty} P(Z \geq z) dz$ . I started with $P(Z \geq \epsilon) = P(e^{(2(m-1)X^2)} \geq \epsilon) = P(X \geq \sqrt \frac{\log(\epsilon)}{2(m-1)} \;) \leq \exp(-2m\frac{\log(\epsilon)}{2(m-1)}) = \epsilon^{\frac{-m}{m-1}}.$ This implies: $E[Z] \leq \int_0^{\infty} z^{\frac{-m}{m-1}}dz$ . But this integral turns out to be divergent and I have no other idea. I thought about using Markov's inequality but I don't quite know how to apply it in this situation. Any input would be appreciated.,"['expected-value', 'probability-theory']"
4249066,Reducing download time using prime numbers,"An issue has cropped up recently in programming with which I could greatly benefit from the expertise of proper mathematicians. The real-world problem is that apps often need to download huge chunks of data from a server, like videos and images, and users might face the issue of not having great connectivity (say 3G) or they might be on an expensive data plan. Instead of downloading a whole file though, I've been trying to prove that it's possible to instead just download a kind of 'reflection' of it and then using the powerful computing of the smartphone accurately reconstruct the file locally using probability. The way it works is like this: A file is broken into its bits (1,0,0,1 etc) and laid out in a predetermined pattern in the shape of a cube. Like going from front-to-back, left-to-right and then down a row, until complete. The pattern doesn't matter, as long as it can be reversed afterwards. To reduce file size, instead of requesting the whole cube of data (the equivalent of downloading the whole file), we only download 3 x 2D sides instead. These sides I'm calling the reflection for want of a better term. They have the contents of the cube mapped onto them. The reflection is then used to reconstruct the 3D cube of data using probability - kind of like asking a computer to do a huge three-dimensional Sudoku. Creating the reflection and reconstructing the data from it are computationally heavy, and as much as computers love doing math, I'd like to lighten their load a bit. The way I'm picturing is like a 10x10 transparent Rubik's cube. On three of the sides, light is shone through each row. Each cell it travels through has a predetermined value and is either on or off (either binary 1 or 0). If it's on, it magnifies the light by its value. If it is off, it does nothing. All we see is the final amount of light exiting each row, after travelling through the cube from the other side. Using the three sides, the computer needs to determine what values (either 1 or 0) are inside the cube. At the moment I'm using normal prime numbers as the cell values to reduce computing time, but I'm curious to know if there is another type of prime (or another type of number completely) that might be more effective. I'm looking for a series of values that has the lowest possible combination of components from within that series. Here is a rough diagram: It might help to imagine that light shines in at the green arrows, and exits with some value at the red arrows. This happens for each row, in each of the three directions. We're left with only three 2D sides of numbers, which are used to reconstruct what's inside the cube. If you look where the 14 exits on the left, it can have two possible combinations, (3 + 11 and 2 + 5 + 7). If for arguments sake we were to assume it were 3 and 11 (coloured green), then we could say at the coordinate where 3 and 11 exist, there are active cells (magnifying the light by their value). In terms of data, we would say this is on (binary 1). In practice we can rarely say for certain (for 2 and 3 we could) what an inside value has based on its reflection on the surface, so a probability for each is assigned to that coordinate or cell. Some numbers will never be reflected on the surface, like 1, 4 or 6, since they can't be composed of only primes. The same happens in the vertical direction, where the output is 30, which has multiple possibilities of which two correspond to the possibility shown in the horizontal direction with an exit of 14, coloured blue and pink (since they hit the 23, the same as 3 in the horizontal direction). This probability is also added to that coordinate and we repeat in the front-to-back direction, doing the same a final time. After this is done for each cell in the whole cube, we have a set of three probabilities that a cell is either on or off. That is then used as the starting point to see if we can complete the cube. If it doesn't 'unlock' we try a different combination of probabilities and so forth until we have solved the 3D Sudoku. The final step of the method is once the cube is solved, the binary information is pulled out of it and arranged in the reverse pattern to how it was laid out on the server. This can then be cut up (say for multiple images) or used to create a video or something. You could cough in theory cough download something like Avatar 3D (280GB) in around 3 minutes on decent wifi. Solving it (nevermind building the pixel output) would take a while though, and this is where I'm curious about using an alternative to prime numbers. You might have guessed that my maths ability goes off a very steep cliff beyond routine programming stuff. There are three areas of concern / drawbacks to this method: it is rubbish at low levels of data transfer. A 10 x 10 x 10 cube for instance has a larger 'surface area' than volume. That's because while each cell can hold one bit (either 1 or 0), each surface cell needs to be a minimum of 8 bits (one character is one byte, or 8 bits). We can't even have 'nothing', since we need null to behave as a type of placeholder to keep the structure intact. This also accounts for why in the above diagram, a 1000x1000x1000-cell cube has its surface areas multiplied by 4 characters (the thousandth prime is 7919 - 4 characters) and the 10'000(cubed)-cell cube has its surface areas multiplied by 6 characters (10'000th prime is 104729, six characters). The aim is to keep total character length on the 2D side to a minimum. Using letters could work, as we could go from a-Z with 52 symbols, before paying double bubble for the next character (the equivalent to ""10""). There are 256 unique ASCII characters, so that's the upper limit there. the factorials are still too high using prime numbers. Is there a series of numbers that are both short in character length (to avoid the problem above) and have very few possible parents? I'm leaning towards some subset of primes, but lack the maths to know which - some sort of inverted Fibonacci? The fewest possible combinations, the faster the computer will solve the cube. I haven't tested yet if its possible to use a third, fourth or nth side to increase either the capacity of the cube or the accuracy of the reflection. Using a say octahedron (yellow below) instead of a cube might be better, just hurts the brain a little to picture how it might work. I'm guessing it would tend towards a sphere, but that's beyond my ability. EDIT: Thank you for your helpful input. Many answers have referred to the Pigeonhole principle and the issue of too many possible combinations. I should also correct that a 1000 x cube would require 7 digits not 4 as I stated above, since the sum of the first 1000 primes is 3682913. I should also emphasise the idea isn't to compress in the common sense of the word, as in taking pixels out of an image, but more like sending blueprints on how to build something, and relying only on the computation and knowledge at the receiving end to fill in the blanks. There is more than one correct answer, and will mark correct the one with the most votes. Many thanks for the detailed and patient explanations.","['factorial', 'probability', 'prime-numbers']"
4249181,Unpacking a proof that the degree of the field extension $[L:L^H]$ is equal to $|H|$.,"I'm trying to understand the proof provided in this link . It goes as follows: Theorem : Let $L$ be a field and let $H$ be a finite subgroup of $\text{Aut}(L)$ . If $L^H$ is the fixed field of $H$ , then $$[L:L^H]=|H|$$ Proof : let $H=\{\sigma_1,\ldots ,\sigma_r\}$ . Every $\alpha \in L$ has a finite orbit $\{\sigma_1(\alpha),\ldots ,\sigma_r(\alpha)\}$ under $H$ , so the polynomial $$p(x)=\prod_i \big(x-\sigma_i(\alpha)\big)$$ is in $L^H[x]$ . This shows the extension is separable, normal, and that every element $\alpha\in L$ has degree lower or equal than the cardinality of $H$ . So $[L:L^H]$ is finite, and given that $H\leq \text{Gal}(L:L^H)$ we have that $[L:L^H]=|H|$ . There are a few things I do not understand about this proof: Why is the extension separable? Is it not possible that $\sigma_i(\alpha)=\sigma_j(\alpha)$ for some $i\neq j$ ? Why is $[L:L^H]$ finite? Just because every element is algebraic over $L^H$ it does not follow that the degree of the extension is finite. Given the above, I can understand why $|H|\leq[L:L^H]$ , but how does the proof establish equality?","['proof-explanation', 'field-theory', 'galois-theory', 'abstract-algebra', 'group-theory']"
4249215,Computing a limit of a sum mixed with product.,Given $d\ge 2$ integer and $m_0>0$ define $$m_k= m_0\left(\frac{d}{d-1}\right)^k\quad \text{and}\quad\sigma_n= \frac{1}{m_n+d}$$ I would like to compute $$\lim_{n\to \infty}\prod_{j=1}^{n}(1-\sigma_j)$$ and $$\lim_{n\to \infty}\sum_{k=1}^{n-2} \sigma_k\prod_{j=k+1}^{n-1}(1-\sigma_j)$$ I expect the following results $\frac{m_0}{m_0+d}$ for the first and $\frac{1}{m_0+d}$ for the second.,"['real-analysis', 'sequences-and-series', 'infinite-product', 'limits', 'algebra-precalculus']"
4249231,Set question which I believe relates to Fibonacci numbers.,"For a subset $S$ of $\mathbb{N}$ , let us define $S+1=\{x+1: x \in S\}$ . How many subsets $S$ of the set $\{1,2,...,n\}$ satisfy the condition $S\cup \left(S+1\right)=\{1,2,...,n+1\}$ ? I think that the amount of subsets is the $nth$ Fibonacci number. I looked at the cases for $n=1,2,3,4,5$ and I find $1,1,2,3,5$ subsets respectively. My professor suggested a proof by induction, but I don't see how to invoke the inductive hypothesis here. Examining $n=3$ . We want to show the subsets which give $S\cup \left( S+1\right)=\{1,2,3,4\}$ . Subsets $S$ include: $\{1\}, \{2\}, \{3\}, \{1,2\}, \{1,3\}, \{2,3\}, \{1, 2, 3\}$ .
Subsets $\left( S+1\right)$ include: $\{2\}, \{3\}, \{4\}, \{2, 3\}, \{2, 4\}, \{3, 4\}, \{2, 3, 4\}$ . So, the only cases in which $S\cup \left( S+1\right)=\{1,2,3,4\}$ is when $S=\{1, 3\}$ and $S=\{1, 2, 3\}$ . In other words, we have two subsets which satisfy the relationship when $n=3$ . As mentioned, I did this for $n=1,2,3,4,5$ to find a pattern, and it appears to be the Fibonacci sequence, but again, I am completely unsure how to show this through induction. Any help would be appreciated.","['fibonacci-numbers', 'combinatorics']"
4249244,Proving inequality from the fields of probability theory using stochastic calculus,"Let $\left(X,Y\right)$ be a two dimensional normal random variable, where $X\overset{d}{=}N\left(0,1\right)$ and $Y\overset{d}{=}N\left(0,1\right)$ with $\operatorname{cov}\left(X,Y\right)=\rho$ . How can we prove $$\operatorname{cov}\left(f\left(X\right),g\left(Y\right)\right)\leq\left|\rho\right|\mathbb{D}\left(f\left(X\right)\right)\mathbb{D}\left(g\left(Y\right)\right),$$ where $f\left(X\right),g\left(Y\right)\in L^{2}\left(\Omega\right)$ ? This is a homework and we got the following hint: Let $\left(X_{t},Y_{t}\right)_{t}$ a two dimensional Wiener-process, with constant $\rho$ correlation. We should write somehow $f\left(X_{1}\right)-\mathbb{E}\left(f\left(X_{1}\right)\right)$ and $g\left(Y_{1}\right)-\mathbb{E}\left(g\left(Y_{1}\right)\right)$ into integral form, and than we should use the Itô isometry. Here is what I did so far: I used Itô's formula for $f\left(X_{t}\right)-\mathbb{E}\left(X_{t}\right)$ , and I got $$f\left(X_{1}\right)-\mathbb{E}\left(f\left(X_{1}\right)\right)=\underbrace{f\left(X_{0}\right)-\mathbb{E}\left(f\left(X_{0}\right)\right)}_{=0}+\int_{0}^{1}f'\left(X_{s}\right)dX_{s}+\frac{1}{2}\int_{0}^{1}f''\left(X_{s}\right)ds.$$ I also did it for the other expression, and I got: $$g\left(Y_{1}\right)-\mathbb{E}\left(g\left(Y_{1}\right)\right)=\underbrace{g\left(Y_{0}\right)-\mathbb{E}\left(g\left(Y_{0}\right)\right)}_{=0}+\int_{0}^{1}g'\left(Y_{s}\right)\rho dX_{s}+\int_{0}^{1}g'\left(Y_{s}\right)\sqrt{1-\rho^{2}}dX_{s}^{*}+\frac{1}{2}\int_{0}^{1}g''\left(Y_{s}\right)ds,$$ where $\left(X_{s}\right)_{s}$ and $\left(X_{s}^{*}\right)_{s}$ are independent Wiener processes.
By definition, I got the following for covariance: $$\operatorname{cov}\left(f\left(X_{1}\right),g\left(Y_{1}\right)\right)	=\mathbb{E}\left(\left[f\left(X_{1}\right)-\mathbb{E}\left(f\left(X_{1}\right)\right)\right]\cdot\left[g\left(Y_{1}\right)-\mathbb{E}\left(g\left(Y_{1}\right)\right)\right]\right)=
	\mathbb{E}\left(\left[\int_{0}^{1}f'\left(X_{s}\right)dX_{s}+\frac{1}{2}\int_{0}^{1}f''\left(X_{s}\right)ds\right]\cdot\left[\int_{0}^{1}g'\left(Y_{s}\right)\rho dX_{s}+\int_{0}^{1}g'\left(Y_{s}\right)\sqrt{1-\rho^{2}}dX_{s}^{*}+\frac{1}{2}\int_{0}^{1}g''\left(Y_{s}\right)ds\right]\right).$$ Here I want to use somehow the Cauchy–Schwarz inequality: $$\mathbb{E}\left(\left[f\left(X_{1}\right)-\mathbb{E}\left(f\left(X_{1}\right)\right)\right]\cdot\left[g\left(Y_{1}\right)-\mathbb{E}\left(g\left(Y_{1}\right)\right)\right]\right)\leq\underbrace{\sqrt{\mathbb{E}\left(f\left(X_{1}\right)-\mathbb{E}\left(f\left(X_{1}\right)\right)^{2}\right)}}_{\overset{\circ}{=}\mathbb{D}\left(f\left(X_{1}\right)\right)}\cdot\sqrt{\mathbb{E}\left(\left(g\left(Y_{1}\right)-\mathbb{E}\left(g\left(Y_{1}\right)\right)\right)^{2}\right)},$$ and for $\mathbb{E}\left(\left(g\left(Y_{1}\right)-\mathbb{E}\left(g\left(Y_{1}\right)\right)\right)^{2}\right)$ using the Itô isometry: $$\mathbb{E}\left(\left(g\left(Y_{1}\right)-\mathbb{E}\left(g\left(Y_{1}\right)\right)\right)^{2}\right)	=\mathbb{E}\left(\int_{0}^{1}\left(g'\left(Y_{s}\right)\right)^{2}\rho^{2}ds\right)+\mathbb{E}\left(\int_{0}^{1}\left(g'\left(Y_{s}\right)\right)^{2}\left(1-\rho^{2}\right)ds\right)+0
	+2\cdot0+2\mathbb{E}\left(\int_{0}^{1}g'\left(Y_{s}\right)\rho dX_{s}\cdot\frac{1}{2}\int_{0}^{1}g''\left(Y_{s}\right)ds\right)+2\mathbb{E}\left(g'\left(Y_{s}\right)\sqrt{1-\rho^{2}}dX_{s}^{*}\cdot\frac{1}{2}\int_{0}^{1}g''\left(Y_{s}\right)ds\right).$$ From this point I don't know how to continue... Anyway, is it a right direction to prove this statement with the hints?","['stochastic-processes', 'inequality', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4249249,Variation on Ricci tensor about metric,"Picture below is from the ""Three manifolds with positive Ricci flow"".  I try to calculate the red line. First, in normal coordinate, I have $$
E(g_{ij})=-2R_{ij} 
=
\frac{\partial}{\partial x^i}(g^{kl}\frac{\partial}{\partial x^j}g_{kl})
-
\frac{\partial}{\partial x^k}[
g^{kl}(
\frac{\partial}{\partial x^i}g_{jl}
+
\frac{\partial}{\partial x^j}g_{il}
-
\frac{\partial}{\partial x^l}g_{ij}
)
]
+(\text{lower order term})
$$ now $$
DE(g_{ij})\tilde g_{ij} = \frac{d}{ds}\Big|_{s=0} E(g_{ij}+s\tilde g_{ij})
$$ I don't know  what is the inverse  of $g_{ij}+s\tilde g_{ij}$ . For example, to calculate $$
 \frac{d}{ds}\Big|_{s=0}
\frac{\partial}{\partial x^i}[(g_{kl}+s\tilde g_{kl})^{-1}\frac{\partial}{\partial x^j}(g_{kl}+s\tilde g_{kl})]
$$ how should I get the elements of $(g_{kl}+s\tilde g_{kl})^{-1}$ ?  But seemingly, since it is normal coordinate, $\frac{\partial}{\partial x^j}g_{kl}=0$ , so there must be $$
\frac{\partial^2}{\partial x^i \partial x^j}g_{kl}
$$ therefore, $(g_{kl}+s\tilde g_{kl})^{-1}$ become $g^{kl}$ . But why $$
DE(g_{jk})\tilde g_{jk}=-2\tilde R_{jk}  ~~~?
$$ After all, the lower order terms are omitted. How to see that it is equal to $-2\tilde R_{jk}$ ?  Seemingly, it is a directly calculation!","['ricci-flow', 'riemannian-geometry', 'differential-geometry']"
4249266,Does $(n^p-1)/(n-1)=m^2$ have any positive integer solutions for $p\ge 2$ prime?,"I was working on my partial answer to this question, which led me to consider the diophantine equation $$
\frac{n^k-1}{n-1}=m^2
$$ with $k>1$ odd. It's not too hard to prove that if $(n,k)$ is a solution, then if $k$ is a perfect square, $(n,p^2)$ is a solution for all $p\mid k$ , and otherwise $(n,p)$ is a solution for $p\mid k$ the largest prime factor of $k$ with $\operatorname{ord}_p(k)$ odd. Therefore, it is natural to ask whether there are any solutions with $k=p$ prime. I have absolutely no idea to start. Perhaps note that $$(m^2)=\prod_{i=1}^{p-1}\left(n-\zeta_p^i\right)\subseteq\mathbb{Z}[\zeta_p],$$ and that all the ideals in this product are pairwise coprime. Therefore, they must all be squares of ideals in $\mathbb{Z}[\zeta_p]$ . Also, if $q\mid m$ is prime, then $q\equiv 1\pmod p$ and the decomposition of $(q)$ in $\mathbb{Z}[\zeta_p]$ is $$
(q) = \prod_{i=1}^{p-1}\left(q,n-\zeta_p^i\right),
$$ whence $\left(q,n-\zeta_p\right)^2\mid \left(n-\zeta_p\right)$ . I have no idea to continue. Perhaps if the class group of $\mathbb{Q}(\zeta_p)/\mathbb{Q}$ has odd order, we can use that $(n-\zeta_p)$ must be the square of a principal ideal? Descending solutions: Suppose $(n,k)$ is a solution and $p^2\mid k$ for some prime $p$ , then $$
\frac{n^k-1}{n^{k/p^2}-1}\cdot \frac{n^{k/p^2}-1}{n-1} = m^2.
$$ Let $q\mid n^{k/{p^2}}-1$ be prime, then $\frac{n^k-1}{n^{k/p^2}-1}=\sum_{j=0}^{p^2}(n^{k/p^2})^j\equiv p^2\pmod q$ . Hence, the only prime that can possibly divide both $\frac{n^k-1}{n^{k/p^2}-1}$ and $\frac{n^{k/p^2}-1}{n-1}$ is $p$ . However, if $p$ divides both, let $t:=\operatorname{ord}_p(n^{k/p^2}-1)$ and let $g$ be a primitive root mod $p^{t+3}$ . Write $n^{k/p^2}\equiv g^a\pmod {p^{t+3}}$ , then $\operatorname{ord}_p(a)=t-1$ , whence $\operatorname{ord}_p(p^2a)=t+1$ , so $\operatorname{ord}_p(n^k-1)=t+2$ . We conclude that $\frac{n^k-1}{n^{k/p^2}-1}$ and $\frac{n^{k/p^2}-1}{n-1}$ are both perfect squares. In particular, $(n,k/p^2)$ is a solution. This way, we can remove the $p^2$ factors from $n$ one by one, until we're either left with an exponent which is itself a square of a prime (if $k$ is a perfect square) or with a square-free exponent. Assume we end up with a solution $(n,k)$ with $k$ square-free. Let $p\mid k$ be the smallest prime factor of $k$ . Note that the product of $(n^k-1)/(n^{k/p}-1)$ and $(n^{k/p}-1)/(n-1)$ is a perfect square, and that $p$ is the only prime that can possibly divide both. However, $(p-1,k/p)=1$ , so if $p\mid n^{k/p}-1$ then $p\mid n-1$ and $p\nmid (n^{k/p}-1)/(n-1)$ . Therefore, $(n^{k/p}-1)/(n-1)$ is a perfect square, and $(n,k/p)$ a new solution. This is how we can remove the primes from $k$ one by one, until only the largest remains.","['algebraic-number-theory', 'number-theory', 'elementary-number-theory', 'diophantine-equations', 'galois-theory']"
4249279,power series expansion of holomorphic function self-composes to the identity,"I struggle with the following problem that I came across dealing with discontinuous group actions on Riemann surfaces. Let $D\subseteq\mathbb{C}$ be a domain that contains $0$ and $f:D\to\mathbb{C}$ a holomorphic function with the power series expansion $f(z)=\sum_{i=0}^\infty a_iz^i$ at $z=0$ . The function satifies $f(0)=0$ , i.e. $a_0 = 0$ , and there exists a $n\in\mathbb{N}$ such that $f_k:=\underbrace{f\circ...\circ f}_{k\text{-times}}\neq\operatorname{Id}$ for $k=1,...,n-1$ and $f_n=\operatorname{Id}$ . Prove or disprove the following : $a_k=0$ for all $k>1$ and $a_1$ is a $n$ -th root of unity. Surely $a_1$ is a primitive root of unity. If $f$ was a polynomial, the rest would be easy too (although I have a feeling that it is easy either way and I'm just blind): if $f(z)=\sum_{i=0}^m a_iz^i$ , then the leading coefficient of $f_n$ would be a power of $a_m$ and equal to zero, so $a_m=0$ , and by induction the same follows for all coefficients. With $f$ not being a polynomial, I thought I could start computing the second coefficient of $f_m$ and show that $a_2=0$ and so on. However, the second coefficients is, if I'm correct, $a_2\sum_{j=m-1}^{2m-2}a^j$ . Since $a_1$ must be a primitve $n$ -th root of unity, the sum is equal to the sum of all $n$ -th roots of unity and hence 0. Thus the second coefficient of $f_n$ is zero independent of what $a_2$ is. Same for the third coefficient. I feel like computing coefficients won't help here and some other (maybe geometric) tool is needed, but I can't think of one that helps.","['complex-analysis', 'power-series', 'analysis']"
4249327,Is this statement related to trigonometry valid to write?,Is this statement related to trigonometry valid to write? $$\sin\left(\frac{\pi}{6}-2x\right)=\sin4x$$ $$\implies \frac{\pi}{6}-2x=\pm 2n\pi+4x \tag{i}$$ where $n$ is any whole number. Can I write (i)?,['trigonometry']
4249329,"“No man is weak, unless their name is Bob.”","Here is the question I am attempting to express in propositional logic: No man is weak, unless their name is Bob. From this expression I see that every man who is weak must be called Bob (W => B), but the expression does not equate to all Bob's are weak (B => W), as being named Bob is simply the criteria for allocating everyone who isn't called Bob to the set of ""all men who are not weak"", meaning that the set of ""all weak men"" is a subset within the set of ""all men named Bob"". I just want to make sure that my reasoning for why it can't be (B => W) is correct, I know that this might be obvious but I just found this confusing initially.","['propositional-calculus', 'logic', 'discrete-mathematics']"
4249342,A question on the directional derivative in a real Banach space,"Let $K$ be a convex and compact set in the real Banach space $(\mathbb{R}^n, |\cdot|_{\infty})$ . Here, the max-norm is defined as $|v|_{\infty}=\max_{i}|v_{i}|$ and the distance function $d_{K}(\cdot)$ is defined to be $d_{K}(x):=\inf_{y \in K}|x-y|_{\infty}$ . Let us use the notation $D_{K}(x;v)$ to denote the directional derivative of $d_{K}(x)$ with respect to the direction $v \in \mathbb{R}^{n}$ , i.e., $$D_{K}(x;v):=\lim_{h \to 0^{+}}\frac{d_{K}(x+hv)-d_{K}(x)}{h}$$ Then the question that I want to ask is: If $x$ is a point in $\mathbb{R}^{n}-K$ with $D_{K}(x;v) \leq 0$ for some $v \in \mathbb{R}^{n}$ , then is it possible that there exists a sequence $\{ x_{n} \}$ in $\mathbb{R}^{n}-K$ and $\epsilon>0$ such that $x_{n}$ converges to $x$ and $D_{K}(x_{n};v) \geq \epsilon$ for all $n \in \mathbb{N}$ ? I provide some remarks and my opinions related to this question. If the ambient space was just a Euclidean space, i.e., $(\mathbb{R}^{n},|\cdot|_{2})$ , then the answer of this question is obviously NO. The reason is that the directional derivative $D_{K}(x;v)$ is continuous with respect to both $x$ and $v$ since $d_{K}(\cdot)$ is continuously differentiable ( $C^{1}$ ). I checked that this question also fails if $K$ is convex polytope. So, I guess that this statement is false for general convex and compact set $K$ in the real Banach space $(\mathbb{R}^{n}, |\cdot|_{\infty})$ but I don't have an idea to derive a contradiction for such a general set. I really appreciate that you read and answer my question.","['convex-geometry', 'real-analysis', 'set-valued-analysis', 'functional-analysis', 'convex-analysis']"
4249348,Interchange Summation Indexes - Möbius Inversion Formula,"I'm reading a book that prove the first part of Möbius Inversion Formula exactly like this: ( $\Rightarrow$ )
Suppose that $$f(n)=\sum_{d|n}g(d).$$ Then, $$\begin{align}\sum_{d|n}\mu(d)f\left(\frac{n}{d}\right)&=\sum_{dd'=n}\mu(d)f\left(d'\right)\\
&=\sum_{dd'=n}\mu(d)\sum_{m|d'}g(m)\\
&=\sum_{dmh=n}\mu(d)g(m)\tag{1.1}\\
&=\sum_{mh'=n}g(m)\sum_{d|h'}\mu(d).\tag{1.2}\end{align}$$ But $\sum_{d|h'}\mu(d)=0$ for $h'>1$ . Hence $$\sum_{d|n}\mu(d)f\left(\frac{n}{d}\right)=g(n).$$ I understood everything except why we can go from 1.1 to 1.2. I found similar questions on the internet, but I couldn't understand the answers either. I'm realizing that this is not the first time that I have not understood how to interchange summation indexes.  It might be interesting for me to do a more careful study of summations, so in addition to the proof explanation, I'm looking for books and videos that better explain how summation works and, in particular, how index interchange works. Feel free to point out mistakes in my writing as well (I'm not used to writing in English). Thanks in advance.","['number-theory', 'summation', 'arithmetic-functions']"
4249365,Monodromy associated to a meromorphic function,"I ask for some hint/help about finding the monodromy homomorphism of a holomorphic map $g: \Bbb{P}^1_{\mathbb{C}}\rightarrow \Bbb{P}^1_{\mathbb{C}}$ locally defined by $$
f(z) = \frac{27z^2(z-1)^2}{4(z^2-z+1)^3}
$$ I see that $deg(g) = 6$ and that we have two ramification points $\infty$ and $0$ with three points in the fiber of $0$ , each of multiplicity 2 and two point in the fiber of $\infty$ , each of multiplicity 3. Call this branch locus $B$ and consider $p\notin B$ . Hence, by Riemann Existence theorem, we get a monodromy homomorphism $$
\rho:\pi_{1}(\mathbb{P}^1\smallsetminus B, p)\simeq \Bbb{Z}*\Bbb{Z}*\Bbb{Z}*\Bbb{Z}\rightarrow \mathcal{S}_6
$$ with transitive image. I know that the loop around multiplicity 2 -points should be 2-cycle, and similarly loops around multiplicity 3-points should be mapped to 3-cycle.
But how can I determine the exact permutation for each homotopy class generator? Thank you for any help you can provide.","['riemann-surfaces', 'complex-geometry', 'complex-analysis', 'algebraic-geometry', 'covering-spaces']"
4249469,Inequality between binomial sums,"I want to prove that the following inequality holds whenever $k\leq n-1$ and $1\leq i\leq \lfloor\frac{k}{2}\rfloor$ . $$\frac{2}{C}\binom{n}{k}\binom{k}{i+1} \leq \frac{k}{i}\sum_{j=i}^{k-1}
 (k-j)\binom{j-1}{i-1}\binom{n-k+j-1}{j}$$ where $C = \max(k,n-k)+1$ . Two identities I suspect might be very useful here are $$\binom{k}{i+1} = \sum_{j=i}^{k-1} (k-j)\binom{j-1}{i-1}$$ $$\binom{n-1}{k-1}=\sum_{j=0}^{k-1} \binom{n-k+j-1}{j}$$ I tried to use these and the Chebyshev's inequality (which states that if $a_1\leq \cdots \leq a_n$ and $b_1\leq \cdots \leq b_n$ , then $\sum a_i \cdot \sum b_i \leq n \sum a_ib_i$ ) but it didn't work. I think that this inequality is somehow ""tight"". Also, I tried splitting into two cases $k\leq n-k$ and $k\geq n-k$ to have a more concrete expression for $C$ . Any help would be highly appreciated. EDIT: I have tried approaching the case $i=1$ , and many identities can be used. The inequality in this case is very tight, and I think that proceeding via induction on $i$ does not help, as the inequality seems to get tighter at every step. EDIT2: The problem with the Chebyshev approach is that the sequence $(k-j)\binom{j-1}{i-1}$ is not increasing (hence it is not possible to use the Chebyshev's inequality as stated). EDIT3: I have noticed using Zeilberger's algorithm that $$\sum_{j=i}^{k-1} j\binom{j-1}{i-1}\binom{n-k+j-1}{j} = \frac{i(i+1)(n-k)}{n(n+i-k)} \binom{n}{k}\binom{k}{i+1}$$ However, no closed expression seems to exist for $$\sum_{j=i}^{k-1}\binom{j-1}{i-1}\binom{n-k+j-1}{j}$$","['inequality', 'binomial-coefficients', 'combinatorics', 'hypergeometric-function']"
4249470,Structure of the space of holomorphic structure on a vector bundle,"It is well known that the space of connections on a vector bundle $E\rightarrow X$ is an affine space modeled on $\Omega^1(X,End(E))$ . Let $Dol(E)$ denote the space of holomorphic structures $\bar{\partial}_E$ on $E$ . We easily see  from the Leibniz rule $$\bar{\partial}_E(fs)=\bar{\partial}f\cdot s + f\bar{\partial}_Es$$ that the difference between two holomorphic structures $\bar{\partial}^1_E-\bar{\partial}^2_E$ is a tensor that belongs to $\Omega^{0,1}(X,End(E))$ . Moreover, there is a flatness condition $$\bar{\partial}_E\circ \bar{\partial}_E=0$$ which implies that $Dol(E)$ is not affine. What can we say anyway on the tangent space of $Dol(E)$ as a subspace of $\Omega^{0,1}(X,End(E))$ ?","['holomorphic-bundles', 'complex-geometry', 'connections', 'vector-bundles', 'differential-geometry']"
4249549,Do these “ultraweak” one-sided group axioms guarantee a group?,"This post shows that the “left” group axioms, which only guarantee a left-identity and left-inverses, are sufficient to guarantee that a semigroup is a group. The same idea could be used to show that the “right” group axioms are also sufficient. These sets of axioms might be considered “weak” group axioms, but I am curious whether we can get weaker. Consider the following “ultraweak” axioms: Let $G$ be a set and $*$ be a binary operation on $G$ satisfying: $*$ is associative. There exists an ultraweak identity element $e\in G$ such that for all $x\in G,$ either $e*x = x$ or $x*e=x$ (that is, the “sidedness” of $e$ may differ for each element of $G$ ). For all $x \in G$ there exists an ultraweak inverse $x^{-1}\in G$ such that either $x^{-1} * x = e$ or $x*x^{-1}=e$ (that is, each element of $G$ has at least a one-sided inverse, where the side may differ for each element). Do these axioms guarantee that $(G,*)$ is a group? And if not, how much closer to these axioms can we get, starting from just the “weak” left or right axioms? [For example, maybe assuming an ultraweak identity element with left (or right) inverses is sufficient.] REVISED UPDATE: In the comments to the accepted answer by Vincent, @Yakk asks whether the following condition is sufficient to guarantee a group (assuming associativity of $*$ ): There exists an $e\in G$ such that for all $x\in G$ , either (1) $e*x=x$ and there exists an $x'\in G$ such that $x'*x=e$ , or (2) $x*e=x$ and there exists an $x'\in G$ such that $x*x'=e$ . At first I thought this was true due to the standard ""left identity + left inverses"" and ""right identity + right inverses"" cases applying element by element, but now I realize this reasoning is flawed (these proofs also require the one-sided inverse to have their own one-sided inverse with the same sidedness). So the question remains: Does the above condition, proposed by @Yakk, guarantee a group? Please provide a proof or counterexample. The answer to the revised update is “yes;” see here . There remains a further question about even weaker conditions, where the left and right identities can be different elements. I've asked that here .","['group-theory', 'abstract-algebra', 'semigroups', 'axioms']"
4249594,The distance between power of 3 and the largest power of 2,"For some positive integer $k \gg 1$ , the largest bit index of $3^k$ is given by $$m \equiv \lfloor k\log_2 3 \rfloor$$ The distance between $3^k$ and $2^m$ can be written as $$ 3^k - 2^m \equiv a_k \cdot 2^m$$ where $0 \lt a_k \lt 1$ . My questions are: Is it true that $a_k \gtrsim 10^{-c}$ , where $c$ is a positive constant. How do I prove, or disprove, this? If bullet 1 is true, how do I estimate the constant $c$ ? As a side note, I have computed numerically for $k$ up to 10,000. It appears that the value of $a_k$ kept in a stable range in $(0, 1)$ , and it could be as small as $\sim \mathcal{O}(10^{-6})$ .","['exponentiation', 'approximation', 'number-theory', 'integers', 'asymptotics']"
4249626,"Using Lagrange multiplier , find minimum value of $xy(x^2 + y^2) +4$ , given that $x^2 + y^2 +xy -1 = 0 , x,y \in \mathbb R$","Using Lagrange multiplier , find minimum value of $$f(x,y)=xy(x^2  + y^2) +4$$ , Given that $g(x,y)=x^2 + y^2 +xy -1 = 0 , $ for all values of $  x,y \in \mathbb R$ . My attempt So i formed a function $$F(x,y , \lambda ) = f(x,y) + {\lambda}g(x,y)  $$ , where $\lambda$ is the Lagrange multiplier . I then solved the partial derivatives w.r.t $x,y$ and $\lambda$ . $${{\partial {F} } \over {\partial x}} = 3x^2 y + y^3 + 2x{ \lambda} + y{\lambda} = 0 
\space ....(1)$$ $${{\partial {F} } \over {\partial y}} = 3y^2 x + x^3 + 2y{ \lambda} + x{\lambda} = 0 
\space ....(2)$$ $${{\partial {F} } \over {\partial \lambda}} = x^2 + y^2 +xy -1 = 0  \space ....(3).$$ Solving equations (1) ,(2) and (3) , I got $$xy = \frac12$$ Therefore , putting value of $xy$ in $x^2 + y^2 + xy -1 = 0$ I got $$x^2 + y^2 = \frac12$$ . And hence ,putting these value in $f(x,y)$ I got the value $$f_{min} (x,y) =\frac{17}{4}$$ However , this is not the correct answer , the correct answer is 2 . Please help me out and tell where am I going wrong  in the method . I know a trigonometric solution to this question , but I want a solution using Lagrange multipliers .
Any help is appreciated .","['optimization', 'calculus', 'functions', 'lagrange-multiplier']"
4249630,"Random ants, need help with error in my thought process","I am trying to solve this problem: Expected falling time of all $500$ random ants I need help with understanding why my thought process is flawed. (highlighted below) Question: 500 ants are randomly put on a 1-foot string (independent
uniform distribution for each ant between 0 and 1). Each ant randomly
moves toward on end of the string (equal probability to the left or
the right) at constant speed of 1 foot/minute until it falls of a t
one end of the string. Also assume that the size of the ant is
infinitely small. When two ants collide head-on, they both immediately
change directions and keep on moving at 1 foot/min. What is the
expected time for all ants to fall off the string? I totally get that when ants collide they exchange labels, so we just consider that ants never collide and keep on walking till one end of the string. My flawed thought process : When an ant lands on a point ""x"" foot away it can go either left or right (equal probability). So expected time taken by it is 0.5 x + 0.5 (1-x) = 0.5, which is independent of x. So all ants regardless where they land will take an expected time of 0.5min. However the solution says that the time taken is x or 1-x and we can just solve for expected value of maximum value of 500 X's. ( because x , 1-x are symmetric.) . What am I missing here ?","['expected-value', 'probability-theory']"
4249639,Do you lose solutions when differentiating to solve an integral equation?,"The question is more general but here's the problem that motivated it: I want to find all solutions to the integral equation $$f(x) + \int_0^x (x-y)f(y)dy = x^3.$$ Differentiating twice with respect to x this yield the second order differential equation $$f''(x) + f(x) = 6x.$$ The solution to this last equation is something of the form $f(x) = A \cos(x + \phi) + f_p$ where $f_p$ can be found by variation of parameters or other tools and $A, \phi$ are determined by initial values. However, what I want to know is whether I'm accounting for all solutions here. Do I not ""lose information"" when differentiating the original equation? If so, how can I construct all solutions from the ones I just found?","['integral-equations', 'ordinary-differential-equations']"
4249669,How can I find the limit of the following?,What is the limit of this $$\lim_{x\to+\infty}\left(1+\frac{4}{2x+3}\right)^x$$ I know that $$\lim_{x\to+\infty}\left(1+\frac{4}{2x}\right)^x$$ will give me $$e^2$$ but the I dont know what to do with the 3. I have tried bringing them to a common denominator so I got $$\lim_{x\to+\infty}\left(\frac{2x+7}{2x+3}\right)^x=\lim_{x\to+\infty}\left(e^{x\ln{(\frac{2x+7}{2x+3})}}\right)$$ And then Im stuck again,['limits']
4249680,how to obtain the strong convexity inequality,"I was playing around with the strong convexity definition and got stuck at some point. I was wondering if someone could kindly help me out. We say that function $f$ is strongly convex if $1) f(x) \geq f(y) + \xi^T(x- y) + \frac{\mu}{2}||x-y||^2$ for $\mu >0$ and $\forall x, y \in dom f$ Now, suppose $f$ is convex and let $h(x) = f(x) -\frac{\mu}{2}||x||^2$ which is also convex. If $\xi \in \partial f(y)$ , then $\xi - \mu y \in \partial h(y)$ . Then, I can use the definition of convexity for function $h$ . $h(x) \geq h(y) + (\xi - \mu y)^T (x-y)$ . Now, let us replace $h$ by $ f -\frac{\mu}{2}||x||^2$ . $f(x) -\frac{\mu}{2}||x||^2 \geq f(y)  -\frac{\mu}{2}||y||^2 + (\xi - \mu y)^T (x-y)$ $f(x) \geq f(y)  + \frac{\mu}{2}||x||^2 -\frac{\mu}{2}||y||^2 + \xi^T(x-y) -  \mu y^Tx + \mu ||y||^2$ This is where I got stuck. I was expecting to obtain the inequality (1), however, I am making a mistake somewhere.","['calculus', 'functions', 'inequality', 'convex-analysis', 'convexity-inequality']"
4249689,Six spheres in a flying saucer?,"I take a sphere of diameter d and remove two ends to create two bowls each having a depth of d/4. If I bring these two bowls together it forms a 3D, flying saucer shaped, Vesica Piscis whereby the saucer's diameter around (x), divided by its height/depth (d/2) equals Root3.
I drop three smaller spheres into one bowl & they bunch together around its central vertical axis...I sit another three into the first to form a six sphered octahedron...and then I place the second bowl over...the upper three spheres sit as perfectly in the top bowl as the lower sit in the bottom....a perfect fit of the six within the flying saucer..
How do I determine the diameter of each of the six smaller spheres please? I say how but in all honesty I probably wouldn't understand any process offered...personally, with little knowledge of math, I just assume there to be some constant appertaining to the diameter of the original sphere and the diameter of the smaller? Eg: Six regular 40mm ping-pong balls require a Vesica/Saucer formed from a sphere of diameter 40 x some Constant? Thanks/Gill",['geometry']
4249716,Recurrence of number of derangement of $[n]$ with $k$ cycles,"Let $d(n,k)$ be the number of derangement of $[n]$ with $k$ cycles. Derive a recurrence for $d(n,k)$ . I know the formula for $d(n,k)$ (which involves unsigned Stirling number of the first kid) but I am not sure how to derive a recurrence combinatorially for $d(n,k)$ Update:
I have tried to derive a recurrence for $d(n,k)$ but not sure if it is right or not. Any help/ advice/ correction would be appreciated. Here is my attempt:
We count the ways to partition $[n]$ into $k$ cycles of length at least $2$ . There are two cases: 1st Case: If the cycle containing the element $n$ has length $2$ , then there are $n-1$ ways to pick its other element, and there are $d(n-2,k-1)$ ways to derange the remaining $n-2$ elements into $k-1$ cycles. 2nd Case: If the cycle containing the element $n$ has length $\geq$ 3, then skipping element $n$ in its cycle still leaves cycle of length at least $2$ , and we can produce a derangement of $[n-1]$ into $k$ cycles, which gives us $d(n-1, k)$ ways. On the other hand, every derangement of $[n]$ with $n$ into $k$ cycles arises from a derangement of $[n-1]$ of $k$ cycle by inserting $n$ immediately following some $x\in[n-1]$ on the cycle containing $x$ . So, there are $(n-1)d(n-1,k)$ of this type. So, the derived recurrence is $d(n,k)= (n-1)(d(n-2,k-1)+ d(n-1,k))$ Any help/ advice/ correction would be appreciated.","['derangements', 'solution-verification', 'combinatorics', 'combinatorial-proofs']"
4249747,Divergence of a radial vector field,"I am reading Modern Electrodynamics by Zangwill and cannot verify equation (1.61) [page 7]: \begin{equation}
\nabla \cdot  \textbf{g}(r)=\textbf{g}^{\prime}\cdot \mathbf{\hat{r}},
\end{equation} where the vector field $\textbf{g}(r)$ is only nonzero in the radial direction. By using the divergence formula in Spherical coordinates, I get: \begin{align}
\nabla \cdot  \textbf{g}(r)&=\frac{1}{r^2} \partial_r (r^2 g_r) + \frac{1}{r \sin \theta} \partial_{\theta} (g_{\theta} \sin \theta) + \frac{1}{r \sin \theta} \partial_{\phi} g_{\phi}\\
&=\frac{2}{r}g_r + \frac{d}{dr}g_r\\
&= \frac{2}{r}\textbf{g}\cdot \mathbf{\hat{r}}+\textbf{g}^{\prime}\cdot \mathbf{\hat{r}}
\end{align} What is going wrong?","['divergence-operator', 'vector-fields', 'electromagnetism', 'multivariable-calculus', 'spherical-coordinates']"
4249769,De Shalit's lemma in R = T.,"In Wiles' celebrated paper where any semi-stable elliptic curve $E$ over ${\Bbb Q}$ is modular, Theorem $0.3.$ therein assumes that either $E$ is good or multiplicative reduction at $3$ . This condition seems to restrict the Hecke algebra ${\Bbb T}_m(N)$ to cases where either $p \nmid N$ or ${\Bbb T}_m(N)$ is an ordinary Hecke algebra, i.e., $U_p \notin {\frak m}$ . In either cases in the above, Haruzo Hida had already established De Shalit's lemma which asserts that Hecke algebra ${\Bbb T}_{\frak m}(Nq_1 \cdots q_n)$ is free over ${\Bbb Z}_p[\Delta]$ with $\Delta$ generated by Diamond operators which are generators of $({\Bbb Z}/q)^{\times}$ . Q. Can Hida's result substitute for De Shalit's result which includes $p \mid N$ and non-ordinary case?","['hecke-algebras', 'galois-representations', 'number-theory', 'algebraic-geometry', 'modular-forms']"
4249774,If $y=f(x)$ is a concave upward function and $y=g(x)$ is a function such that $f'(x)g(x)-g'(x)f(x)=x^4+2x^2+10$ then prove that...,"If $y=f(x)$ is a concave upward function and $y=g(x)$ is a function
such that $f'(x)g(x)-g'(x)f(x)=x^4+2x^2+10$ then prove that $g(x)$ has at least one root between consecutive roots of $f(x)=0$ $f'(x)g(x)-g'(x)f(x)=(x^2+1)^2+9\implies f'(x)g(x)-g'(x)f(x)>0$ If $\alpha,\beta$ are consecutive roots of $f(x)=0$ , $f'(\alpha)g(\alpha)>0$ and $f'(\beta)g(\beta)>0$ Now if we prove that: ""The slope of a concave upward function is positive at the first root and negative at the second, given that the roots in question are consecutive and the function has two or more roots"" , we will be able to reach the result. When I draw a graph, the above statement seems obvious, but I don't know how to prove it mathematically Because $g(\alpha)g(\beta)<0 \implies g(x)$ has at least one root in $\alpha,\beta$","['calculus', 'functions']"
4249781,Example of a Hausdorff topology that is homeomorphic to $\mathbb R^n$ but isn't second countable?,"The definition of a topological manifold $M$ I have is: $M$ is Hausdorff. Each point of $M$ has a neighborhood
that is homeomorphic to an open subset of $\mathbb{R}^n$ . $M$ is second countable. That last one confuses me a little since it seems to me the second condition should imply the third. $\mathbb{R}^n$ is second countable, thus an open subset $S$ of it is also second countable through the base: $B_S = \{U \cap S : U \in B; S \subseteq \mathbb{R}^n\}$ where $B$ is the base of $\mathbb{R}^n$ , In English, grab the base for $\mathbb{R}^n$ , for each set in it, intersect it with the open space, append all the results to a new set. By construction this new set is both countable and any open set in $S$ is a union of sets in $B_S$ . And now use the homeomorphism $\phi$ to map the base sets onto $M$ , Since we used a homeomorphism the resulting set $\phi(B_S)$ should be a countable base for $M$ . Where is my logic failing? What is an example of a topology that ahs the first 2 properties without the third?","['second-countable', 'examples-counterexamples', 'manifolds', 'general-topology', 'differential-geometry']"
4249820,How to pronounce Stirling Numbers of Second Kind ${n\brace k}$?,"The Stirling Numbers of the Second Kind, ${n\brace k}$ , count the number of ways to partition an $n$ -element set into $k$ unlabeled non-empty parts and are rather useful for several introductory questions in combinatorics alongside the other earlier taught binomial coefficients $\binom{n}{k}$ and factorials, and so on. The binomial coefficients have a standardized way of reading them aloud in English, being "" $n$ choose $k$ .""  Is there anything similar for Stirling Numbers of the Second Kind?  Or for the Stirling Numbers of the First Kind? In my head, when typing them out or thinking of them, I often read them with the TeX commands as ""n brace k""... but if I were to try to use a more suggestive phrase that helps imply the meaning of the notation I might prefer "" $n$ partition $k$ "" or ""Second Stirling $n$ $k$ ."" I am curious how other people read this aloud in a classroom setting or in their own head.","['notation', 'soft-question', 'combinatorics', 'stirling-numbers']"
4249832,Symmetric and Reflexive relation,"Let R be a relation on the set of ordered pairs of positive integers such that ((p,q),(r,s))∈R if and only if p−s=q−r. Which one of the following is true about R? Both reflexive and symmetric
Reflexive but not symmetric
Not reflexive but symmetric
Neither reflexive nor symmetric Solution: The key trick here is to realize that the relation is of the form : {ordered pair, ordered pair} and not simply ordered pair. Ok, so for reflexive $\forall_{a,b}\, if((a,b),(a,b)) \in \mathrel{R} \rightarrow \text{reflexive}$ $((a,b),(a,b))∈R↔(a−b=b−a)$ (not possible for any postive integers b and a) But that is a contradiction hence it is not reflexive. Now, for symmetric $((a,b),(c,d))∈R→((c,d),(a,b))∈R$ $((a,b),(c,d))∈R→(a−d=b−c)$ $((c,d),(a,b))∈R$ $∵(c−b=d−a)↔(d−a=c−b)↔(−(a−d)=−(b−c))↔(a−d=b−c)$ This is the solution given in from trustworthy source of technical examiniation. But my doubt is If we are saying that $((a,b),(c,d))∈R→((c,d),(a,b))∈R$ to be symmetric right? Why are we checking symmetric relation on ordered pair and reflexive relation on only two pairs $((a,b),(a,b))∈R$ I know Reflexive relation is said to be reflexive if $xRx  \forall_{x}\ $ in $R$","['elementary-set-theory', 'relations']"
4249836,Tips for forming countability proofs?,"I'm new to writing proofs, and I'm having a really hard time enumerating sets. I'm struggling to construct functions which convert positive integers into the desired set and which is bijective. For instance, one of these sets is the list of all real numbers with decimal representations consisting of 1's. I have no idea how to approach this question. I'm not even sure if it's countable or not. Another example which illustrates something which I am having trouble with is whether the set of all integers divisible by 5 but not divisible by 7 is countable. Obviously it is countable, but I don't know how to prove that it is countable. I got f: $\mathbb{N} \rightarrow \mathit{G}$ f(x) = $\frac{5x}{5}$ where x is divisble by 5 $\frac{-5(x-1)}{5}$ where (x - 1) is divisble by 5 $\frac{35(n-2)}{5} - \frac{35(n-2)}{5}$ where (x-2) is divisible by 5 At which point I felt as if I wasn't going in the right direction and stopped. Any and all advice would be appreciated.","['cardinals', 'proof-writing', 'discrete-mathematics']"
4249849,On the definition of the Atiyah class,"I am reading the book Complex Geometry: an Introduction by Daniel Huybrechts. On page 180 the author defined the Atiyah class as follows: The Atiyah class $$A(E)\in H^1(X,\Omega_X\otimes \mathrm{End}(E))$$ of the holomorphic vector bundle E is given by the $\mathrm{\check{C}}$ ech cocycle $$A(E)=\{U_{ij},\psi_j^{-1}\circ(\psi_{ij}^{-1}d\psi_{ij})\circ\psi_j\}.$$ Here $X$ is a complex manifold and $X=\cup U_i$ is an open covering such that $\psi_i\colon E|_{U_i}\cong U_i\times\mathbb{C}^r$ are holomorphic trivializations, and $\Omega_X$ is the sheaf of holomorphic 1-forms on $X$ . Moreover, $U_{ij}:=U_i\cap U_j.$ I cannot see why these $\psi_j^{-1}\circ(\psi_{ij}^{-1}d\psi_{ij})\circ\psi_j$ are in sections $\Gamma(U_{ij}, \Omega_X\otimes \mathrm{End}(E)).$ If this is true, take some $x\in U_{ij},$ $v\in T_xX$ and $e\in\Gamma(U_{ij},E),$ then $v\otimes e(x)$ should be mapped by $\psi_j^{-1}\circ(\psi_{ij}^{-1}d\psi_{ij})\circ\psi_j(x)$ to an element in $\mathbb{C}\otimes E.$ But I think the result should still be in $T_xX\otimes E,$ as $d\psi_{ij}$ is from $T(U_{ij}\times\mathbb{C}^r)$ to $T(U_{ij}\times\mathbb{C}^r)$ . Where am I wrong? Could someone help me?","['complex-geometry', 'vector-bundles', 'algebraic-geometry', 'characteristic-classes']"
4249873,Number of invertible matrix of order 3 using only 0 and 1,"Let A be a 3 × 3 matrix whose each entry is either 0 or 1. If the probability that A is invertible is $\frac{3n}{64}$ , then is equal to_______ My approach is as follow $A = \left[ {\begin{array}{*{20}{c}}
{{a_{11}}}&{{a_{12}}}&{{a_{13}}}\\
{{a_{21}}}&{{a_{22}}}&{{a_{23}}}\\
{{a_{31}}}&{{a_{32}}}&{{a_{33}}}
\end{array}} \right] \Rightarrow \left| A \right| = {a_{11}}\left( {{a_{22}}.{a_{33}} - {a_{23}}.{a_{32}}} \right) - {a_{12}}\left( {{a_{21}}.{a_{33}} - {a_{23}}.{a_{31}}} \right) + {a_{13}}\left( {{a_{21}}.{a_{32}} - {a_{22}}.{a_{31}}} \right)$ The total number of Matrices are $2^9$ and the number of favourable cases is $X$ Hence the probability is $\frac{X}{2^9}$ , using $0$ and $1$ how do I find that the matrix is invertible","['matrices', 'linear-algebra']"
4249897,Does the second order non-linear ODE $yy''=f(x)f(x)''$ have a general solution?,"I was working on this problem and came across an equation of the form \begin{align}
yy''=f(x)f(x)''.
\end{align} The equation $yy''=f(x)$ does not have a general solution (that I or WA can find), but this equation has the particular solutions $y_{0,1}=\pm f(x)$ , which gives me a bit of hope. I also tried a solution of the form $y=\sqrt{f(x)^2+g(x)}$ for some $g(x)$ to be determined, but I believe only $g(x)\equiv0$ works. Non-linear equations don't always have general solutions or transformations to simpler equations given particular solutions, but perhaps someone is aware of one for such an equation? Edit I've channeled my inner Claude Leibovici for this idea, taking $x$ to be the dependent variable and $y$ to be the independent variable: \begin{align}
y''=-\frac{x''}{(x')^3},\quad\text{and}\quad
f''=-\frac{x''}{(x')^3}f'+\frac{1}{(x')^2}f''.
\end{align} So then \begin{align}
-y\frac{x''}{(x')^3}=-\frac{x''}{(x')^3}ff'+\frac{1}{(x')^2}ff'\quad\longrightarrow\quad
x''+\frac{ff''}{y-ff'}x'=0.
\end{align} Using the integrating factor \begin{align}
\ln(\mathrm E)&=\int\frac{ff''\mathrm dy}{y-ff'},\\
\left(\mathrm Ex'\right)'=0&\quad\longrightarrow\quad
x'=\frac{k_1}{\mathrm E},\\
x(y)&=k_1\int\frac{\mathrm dy}{\mathrm E}+k_2.
\end{align} This strikes me as a solution at first, but after thinking a little I'm not so certain. Evaluating the integral for $\mathrm E$ requires knowing the first and second derivative of $f$ with respect to $y$ , which I'm not sure is possible without knowing the solution to the original ODE.",['ordinary-differential-equations']
4249919,"$f:[1,4]\rightarrow[7,14]$ is a concave surjective function then prove that $(f'(x))^2=49/9$ has at least one and at most two roots in $[1,4]$","$f:[1,4]\rightarrow[7,14]$ is a strictly concave surjective function then prove that $(f'(x))^2=49/9$ has at least one and at most two roots in $[1,4]$ $\displaystyle f'(x)=\pm\frac{7}{3}$ , so if we need to prove that a line with slope $   \displaystyle\pm\frac{7}{3}$ is tangent to the function at least once and not more than two times. The graph is enclosed in a rectangle with coordinates $(1,7),(1,14),(4,7)$ and $(4,14)$ Case 1 (the function is concave increasing): Since it's surjective and increasing $(1,7)$ and $(4,14)$ lie on the function. We can prove $(f'(x))^2=49/9$ has one root  by Lagranges mean value theorem (since the slope of the diagonal of the rectangle is $7/3$ ). Case 2 (the function is concave decreasing): Same logic as above, only this time the diagonal with slope $-7/3$ will be a tangent. Case 3 (function is first increasing, then becomes decreasing or starts as decreasing and becomes increasing): This also case seems correct when I draw a graph, but I can't think of a rigorous proof. Can anyone solve this? It's fine if you use another method. Thanks","['functions', 'convex-analysis', 'real-analysis']"
4249966,Finding the value of $\displaystyle\int_{x_1+x_2}^{3x_2-x_1}\big\{\frac x4\big\}\left(1+\left[\tan\left(\frac{\{x\}}{1+\{x\}}\right)\right]\right)dx$,"If $x_1$ and $x_2$ ( $x_1\lt x_2$ ) are two values of $x$ satisfying the equation $\left|2\left(x^2+\frac1{x^2}\right)+|1-x^2|\right|=4\left(\frac32-2^{x^2-3}-\frac1{2^{x^2+1}}\right)$ then find the value of $\displaystyle\int_{x_1+x_2}^{3x_2-x_1}\big\{\frac x4\big\}\left(1+\left[\tan\left(\frac{\{x\}}{1+\{x\}}\right)\right]\right)dx$ (where $\{*\}$ and $[*]$ denotes fractional part function & greatest integer function respectively.) RHS of the given equation can be written as $6-2^{x^2-1}-\frac1{2^{x^2-1}}=6-(y+\frac1y),$ where $y=2^{x^2-1}$ Since $y\gt0\implies y+\frac1y\ge2\implies  RHS\ge4$ LHS $=\left|2\left(x-\frac1x\right)^2+4+|1-x^2|\right|\ge4\implies 2\left(x-\frac1x\right)^2+4+|1-x^2|\ge4$ (taking only one case as of now) Thus, $2\left(x-\frac1x\right)^2+|1-x^2|\ge0$ , which is true always. Does the question imply there are only two values of $x$ satisfying the given equation? In any case, how to proceed next?","['integration', 'calculus', 'functions', 'definite-integrals']"
4250008,"Weierstrass Preparation Theorem, simple exercise","I am working with a specific version of the Weierstrass preparation theorem, trying to understand it with an example. I'll cite the theorem before stating the question: Theorem If $f:U\subset \mathbb{C}^{n+1}\rightarrow \mathbb{C}$ is a holomorphic function in the variables $(z,w_1,\dots,w_n)$ such that $f(z,0,\dots,0)$ is not identically zero near $z=0$ , then there exist a holomorphic function $h:B \subset \mathbb{C}^{n+1}\rightarrow \mathbb{C}$ with $h(0,\dots,0)\neq 0$ , a Weierstrass polynomial of the form $$
p(z,w_1,\dots,w_n) = z^d + \alpha_1(w_1,\dots,w_n) z^{d-1} + \dots + \alpha_d(w_1,\dots,w_n)
$$ where the coefficients are holomorphic functions in $(w_1,\dots,w_n)$ vanishing at the origin, such that locally near the origin in $\mathbb{C}^{n+1}$ we have $f=h\cdot p$ . The polynomial $p$ is unique. Question : I am trying to get the Weierstrass  polynomial for the following function $f:\mathbb{C}\rightarrow \mathbb{C}$ given by $$
f(z_1,z_2) = z_1^3z_2 + z_1 z_2 + z_1^2z_2² + z_2^2 + z_1 z_2^3
$$ Obviously $f(z_1,0)$ vanishes identically, so the variable that I need to work on is $z_2$ . By differentiating $f=h\cdot p$ with respect to $z_2$ I have been able to show that the degree $d$ is necessarily 2, i.e. $$
p(z_1,z_2) = z_2^2+\alpha(z_1)z_2 + \beta(z_1),
$$ for holomorphic coefficients $\alpha,\beta$ vanishing at $z_1=0$ . However, I am stuck since I have too many degrees of freedom : $h,\alpha,\beta$ . How can I construct a method for finding this coefficients?","['complex-analysis', 'several-complex-variables', 'complex-numbers']"
4250074,Possible reverse triangle inequality,"I'm looking at the convergence (when blowing up the metric) of the spectrum of a self-adjoint operator $P$ that acts on differential forms of a 3-dimensional closed manifold M.
Let $\lambda$ be a complex (not real) number. I need to prove the following inequality $$\|(\lambda-P)\alpha\|_0^2\geq |Im(\lambda)|^2\|\alpha\|^2_0+\|P\alpha\|_0^2$$ with respect to the $L^2$ -norm. In some sense, the real part of $\lambda$ eats the double inner product, i.e. $$|Re(\lambda)|^2\|\alpha\|^2_0\geq 2(P_\epsilon\alpha,\lambda\alpha)_0.$$ Have you seen this happening somewhere else? Is there a general property of the spectrum of self-adjoint operators?","['spectral-theory', 'functional-analysis', 'geometric-functional-analysis', 'differential-geometry']"
4250079,Can we determine a number of objects in a category?,"Maybe my question is too idealistic, but I try it: Is there any way to look at a category and be able to tell whether this category has zero, one, countable, infinite or any particular number of objects? If not generally, is it possible in particular cases of categories? And what about determining if the category has/hasnt initial or terminal object? The example that motivated me for this question : If we have a topological space and take all its compactifications, which form a category - are we able to determine (using category theory tools or whatever else), how many objects are there in the category?","['general-topology', 'compactification', 'category-theory', 'compactness']"
4250097,Show that $|f'(z)|≤\frac{1}{1 − |z|^2}$,"Suppose $f : \Bbb D \longrightarrow \Bbb D$ is holomorphic on $ \Bbb D = D (0, 1).$ Then $$|f'(z)|≤\frac{1}{1 − |z|^2}.$$ I know how to show this for $|f'(z)|≤\frac{1}{1 − |z|}$ . Fix $z_0 ∈ D(0,1)$ and choose $δ$ such that $|z_0| < δ < 1$ . Then if $z ∈ \overline {D(z_0,δ −|z_0|)}$ ,  we
have $$|z|= |z −z_0 + z_0|≤|z −z_0|+ |z_0|≤δ −|z_0|+ |z_0|= δ < 1.$$ That is, $D(z_0,δ −|z_0|) ⊆ D(0,1)$ and hence $|f| ≤ 1$ in this closed disk. Thus by
Cauchy's estimates, we get $$|f'(z_0)|≤ \frac{1}{δ −|z_0|}.$$ This is true for every $0 < δ < 1$ . Thus sending $δ →1$ , we get $$|f'(z_0)|≤ \frac{1}{1 −|z_0|}.$$ How do I show it for $|f'(z)|≤\frac{1}{1 − |z|^2}$ ? I tried to do it using
the Cauchy Integral formula. For $|z|<1$ and for all $0<r<1−|z|^2$ , $$f'(z)=\frac{1}{2πi}∫_{|z-z_0|=r}\frac{f(z)}{(z-z_0)^2}dz.$$ But I still could not get anywhere. I always end up with the result $|f'(z_0)|≤ \frac{1}{1 −|z_0|}$ . Any help would be appreciated and please don't close this question. I really did try in many ways.",['complex-analysis']
4250103,Verify exponential integral $\int_0^\infty e^{-\frac{1}{2}(x+\frac{a}{x})^2}dx=\sqrt{\frac{\pi}{2}}e^{-a-|a|}$,"I need to verify a result $$\int_0^\infty\exp\left[-\frac{1}{2}\left(x+\frac{a}{x}\right)^2\right]dx=\sqrt{\frac{\pi}{2}}e^{-a-|a|}$$ for $a\in\mathbb{R}$ . What I tried so far is to use residue calculus , but the problem is here $z=0$ would become an essential singularity and it can't be removed by using an inverse sbstitution that is usually made due to the symmetrical $\displaystyle z+\frac{1}{z}$ factor in the integrand . So I have no idea to proceed . Is there any other approach than residue calculus ? If not , what could make the residue approach a bit more easier ? Thanks in advance .","['integration', 'complex-analysis', 'real-analysis']"
4250194,"If every elements of a matrix convergence in probability, do this matrices converge in probability?","Let $\widehat{\sum}=\left(a_{i,j}^{N}\right)_{1\leq i\leq j\leq N}$ and $\sum=\left(a_{i,j}\right)_{1\leq i\leq j\leq N}$ . If $a_{i,j}^{N}\overset{P}{\to} a_{i,j}$ for any $1\leq i\leq j\leq N$ , then $\widehat{\sum}\overset{P}{\to} \sum$ ? Is this true or are there additional conditions required?","['random-matrices', 'probability-theory', 'weak-convergence']"
4250209,How do I define my function?,"I have this function here. $$f(x)=\frac{\frac{x-14}{x-2}-1}{7+ \frac{4}{x-2} }$$ I can see that when $x=2$ and when $$x=\frac{10}{7}$$ it's undefined. But when I simplify this into this: $$ f(x)=\frac{-12}{7x-10}$$ $x=10/7$ is still not defined, however, $x=2$ is defined. So my question is, what does this mean? Is the original expression defined for $x=2$ or not? Or is there something else I'm missing here? I'm wondering this because my teacher asked me to calculate $f(x)=-3$ and then double-check it by plugging it into the original expression. When I calculate $f(x)=-3$ , I get that $x=2$ on the simplified expression. However, in the original expression $f(x)=-3$ is undefined. So I'm a bit confused by this. Can I say that $f(2)$ is defined or not?",['algebra-precalculus']
4250217,Geometric Brownian motion is a martingale,"Why is the geometric Brownian motion, given by $$ \alpha \exp \left( \sigma W_t - \frac{\sigma^2}{2} t    \right)$$ a martingale? I just have problems to show the point: $\mathbb{E} [X_t \mid \mathcal{F}_s] = X_s \ \ \ \ \mathbb{P}$ -a.s for all $t > s$ .","['martingales', 'brownian-motion', 'probability-theory']"
4250249,Is finite the limit of $(x_n)_n$?,"Let $ (x_n)_n$ and $ (y_n)_n$ two sequences such that $x_1 =1$ , $ y_1 =4$ and $ x_{n+1}= \frac{ 4+ 2 x_n + x_n y_n}{y_n}$ , $ y_{n+1}= \frac{ 4+ 2 y_n + x_n y_n}{x_n}$ . Find the limits of $ (x_n)_n$ and $ (y_n)_n$ . I seen that the sequences are increasing and that the limit of $(y_n)_n$ is infinite. In the same time $ y_n \geq x_n $ . I can't to decide if the limit of $(x_n)_n$ is finite or infinite.","['analysis', 'real-analysis']"
4250263,Is there a non empty set X such that $X \subseteq X\times X$?,"I came up with this idea but I can't seem to prove it. Maybe I have to prove that X is the set of every set (such that X doesn't exist), or that X is empty, but I am not getting anything done and I would like a bit of help. Just for reference, I am just starting first year of undergrad, so I would prefer an answer that doesn't require much knowledge beyond set theory. Thanks a lot.","['elementary-set-theory', 'set-theory']"
4250291,Are most finite posets large?,"I'm currently experimenting with an algorithm that can be found here to asymptotically uniformly generate posets on a finite set $[n]$ . This paper describes a Markov chain that can be used to generate directed acyclic graphs that represent partial orders and suggests that running this chain for $n^2$ steps generates results that are nearly uniformly distributed. I tried generating large numbers of partial orders for larger $n$ (up to $n = 40$ ) and noticed that all posets I generated with this algorithm contain most of the tuples they could contain (about ~740 tuples on average; partial orders on $[40]$ can contain at most 820 tuples). Assuming this algorithm is still accurate with $n^2$ steps for larger $n$ , this would suggest that there are vastly more posets with a large number of tuples than there are posets with a small number of them. Since for each $n$ , there is exactly one smallest but $n!$ largest posets on $[n]$ , this intuitively makes sense to me, but are there any more exact results on how many ""large"" posets there are compared to ""small"" ones?","['order-theory', 'discrete-mathematics']"
4250321,"Graph of quadratic $f(x)=ax^2+bx+c$ when $a$ is fixed and $b,c$ are varied","I noticed a small thing while playing with the graph of quadratic . $$ax^2+bx+c = a\left(x+\frac{b}{2a}\right)^2 + c - a\left(\frac{b}{2a}\right)^2$$ Clearly $b,c$ only determine how the vertex of the graph changes, not the shape of the graph; that is, as $b,c$ are varied, the graph just translates without changing its shape. This means adding a linear function $bx+c$ to a quadratic doesn't change its shape! This makes sense from above crude manipulation of the equation but I'm wondering if there is a more satisfying way to see this, perhaps geometry/calculus ?","['algebra-precalculus', 'quadratics']"
4250331,"If $f$ is bounded, differentiable and satisfies the inequality, does there exist $\lim_{x\to\infty}f(x)$?","Let $f:[0,\infty)\to\mathbb{R}$ be a function. Suppose that $f$ is bounded, differentiable and satisfies the following inequality: $$f(x)f'(x)\geq \sin(x)\text{ for all }x\in[0,\infty).$$ Does there exist $\lim\limits_{x\to\infty}f(x)$ ? I suppose that the statement is false since I don't see how the inequality will help us prove the limit existence. However, I have not found out any counterexample yet.","['limits', 'calculus', 'real-analysis']"
4250414,Proof of spatial Markov property,"I have to prove the spatial Markov property. Here is what I mean by this. Let $G$ be a graph, and let $I$ be an independent set in $G$ , chosen from the hard-core model (that is, the probability of choosing any independent set $I$ is proportional to $\lambda^{|I|}$ for some activity parameter $\lambda$ ). Let $U\subseteq V(G)$ be a set of vertices in $G$ and let $S_U\subseteq U$ be a subset of $U$ . Then I want to prove that $$ 
   \mathbb{P}(I\cap U=S_U \mid I\cap U^c) = \mathbb{P}(I\cap U=S_U \mid I\cap N(U)).
$$ This is shorthand for saying that for any independent set $I_0$ disjoint from $U$ , $$ 
   \mathbb{P}(I\cap U=S_U \mid I\cap U^c = I_0) = \mathbb{P}(I\cap U=S_U \mid I\cap N(U) = I_0 \cap N(U)).
$$ The case in which $U$ is a vertex, i.e., $U=\{v\}$ , for $v\in V(G)$ is proved as follows. We want to show that $\mathbb{P}(v\in I\,|\,I\setminus\{v\})=\mathbb{P}(v\in I\,|\,I\cap N(v))$ . $$\mathbb{P}(v\in I\,|\,I\setminus\{v\}=J)=\frac{\lambda^{|J|+1}}{\lambda^{|J|+1}+\lambda^{|J|}}=\frac{\lambda}{\lambda+1},$$ since conditioned on $I\setminus\{v\}=J$ , there are two possibilities for the independent set $I$ , either $I=J$ or $I=J\cup\{v\}$ On the other hand, if we know the set $I\cap N(v)$ , the only case in which the right-hand side probability is not zero is when $I\cap N(v)=\varnothing$ , therefore $$\mathbb{P}(v\in I\,|\,I\cap N(v)=\varnothing)=\frac{\lambda}{1+\lambda},$$ which gives the desired equality. The problem is that I don't know how to show the general case, i.e., for a general vertex set $U$ . Any help would be very much appreciated. Thanks!","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4250447,Looking for function which satisfies $f(n)=f(2n)+f(2n+1)$,"I am looking for a function from $\Bbb N^*$ to $\Bbb R^{+*}$ such that $f(n)=f(2n)+f(2n+1)$ (for any $n$ in $\Bbb N^*$ ). I also am looking for something smooth, where $f(n+1)-f(n)$ would be strictly decreasing with $n$ . A solution like $f(n) = 1/2^{\lfloor \log_2(n)\rfloor}$ , for example, isn't ideal as is doesn't decrease smoothly. My goal is to be able to ponderate values in a specific way such that smaller ones weight more. The whole thing is a bit too complicated to explain here. I though maybe some kind of exponential could work, but by assuming there are $a$ and $b$ such that $a^nb = a^{2n}b + a^{2n+1}b$ for any $n$ quickly fails. What else could I try?","['functional-equations', 'functions']"
4250457,Smooth sequence of functions converging pointwise to a smooth function and limit of derivatives,"In the wikipedia page of uniform convergence, it says that given a sequence $\{ f_n \}$ of differentiable real functions (say, over the reals) with the property that it converges pointwise to some function $f$ , the limit of $\{f_{n}' \}$ need not be equal to $f'$ . It then gives an example where $\{ f_n \} $ converges uniformly to a differentiable $f$ , but $\{f_n '\}$ does not converge even pointwise. My question is, what if we assume that each $f_n$ and their limit $f$ are, say, $C^\infty$ , and $\{ f_n '\}$ converges pointwise to some $C^\infty$ function $g$ as well. Is it now enough to show that $f' = g$ ? Or do we further need to assume uniform convergence? Is there a classic counterexample to this question as well?","['derivatives', 'uniform-convergence', 'real-analysis']"
4250534,"Financial Mathematics Force of Interest, Discount, Accumulation Functions","Mr. Valdez has $10000 to invest at time t=0, and three ways to invest it. Investment account I is governed by compound interest with an annual effective discount rate of 3%. Investment account II has force of interest equal to :
0.04/(1+0.05t^(2)) Investment account III is governed by the accumulation function:
aIII(t)= 1/(1-0.005t^(2)) Mr. Valdez can transfer his money between the three investments at any time. What is the maximum amount he can accumulate at time t=5? The correct answer is 12140.26 dollars, but the highest investment I got was 11643.4772 dollars. I know that I have to compare first delta t values and any given time compare whichever account providing greatest return in given time use that account for that t however I suppose my calculations for delta t values slightly off. delta t account 1 = -ln(1-d)
delta t account 2 given
delta t account 3= a`(t)/a(t) which gives function 2t/(200-t^2) using this values I obtained above answer which is very wrong. Would you point me out where am I doing wrong. Thanks","['economics', 'finance', 'discrete-mathematics']"
4250550,Extension of functional calculus of continuous functions,"On Reed & Simon's book , we can find the following theorem, which is called the continuous functional calculus . Notation: $\sigma(A)$ is the spectrum of the operator $A$ , $\mathscr{L}(\mathscr{H})$ is the space of all bounded linear maps on a Hilbert space $\mathscr{H}$ and $C(\sigma(A))$ is the space of all continuous functions defined on $\sigma(A)$ . Theorem: Let $A$ be a bounded self-adjoint operator on a Hilbert space $\mathscr{H}$ . Then there exists a unique map $\phi: C(\sigma(A)) \to \mathscr{L}(\mathscr{H})$ with the following properties: (a) $\phi$ is an algebraic $*$ -homomorphism, that is: $$\phi(fg) = \phi(f)\phi(g) \quad \phi(\lambda f) = \lambda \phi(f) \quad \phi(1) = I \quad \mbox{and} \quad \phi(\bar{f}) = \phi(f)^{*}$$ (b) $\phi$ is continuous, that is, $||\phi(f)||_{\mathscr{L}(\mathscr{H})} \le C||f||_{\infty}$ for some $C>0$ . (c) If $f(x) = x$ then $\phi(A) = A$ . (d) If $A\psi = \lambda \psi$ , then $\phi(f)\psi = f(\lambda)\psi$ , $\psi \in \mathscr{H}$ , (e) $\sigma[\phi(f)] = \{f(\lambda): \lambda \in \sigma(A)\}$ (f) If $f \ge 0$ then $\phi(f) \ge 0$ (g) $||\phi(f)|| = ||f||_{\infty}$ . The proof of this theorem is given by the authors and I have no problem with it. Later in the book, the authors discuss the extension of this theorem for bounded Borel measurable functions. In this latter case, $\phi$ is now a map $\hat{\phi}: B(\mathbb{R}) \to \mathscr{L}(\mathscr{H})$ , where $B(\mathbb{R})$ denotes the space of all bounded Borel measurable functions on $\mathbb{R}$ . All properties (a) - (g) remain unchanged for $\hat{\phi}$ . The discussion uses spectral measures to prove this Borel functional calculus. However, the authors state that the Borel functional calculus can be proven directly by extending the above continuous functional calculus . Question: How do I extend the above stated continuous functional calculus to prove the Borel functional calculus? The authors give no hint on how to do it and I have no clue on how to approach such extension. By their comment, I'd expect that some natural extension exists but I couldn't even sketch one by myself.","['self-adjoint-operators', 'operator-theory', 'functional-analysis', 'mathematical-physics']"
4250562,"is $S(n,k) = S^{*}(n,k)$?","Egoroff's Theorem : If $\mu(X)<\infty$ , if $\{f_n\}$ is a sequence of complex measurable functions which converges pointwise on $X$ , and if $\epsilon >0$ , then there is some measurable $E\subset X$ with $\mu(E-X)<\epsilon$ such that $\{f_n\}$ converges uniformly on $E$ . I'm studying the proof of Egoroff's theorem and I have seen that the definition of the auxiliary set has different versions. In one of the tests they defined it as follows: $$S(n,k) = \bigcap_{i>n}\{x \in X \colon |f_{i}(x)-f(x)|<\frac{1}{k} \}$$ On the other hand, in another test they define it as follows: $$S^{*}(n,k) = \{x \in X \colon |f_{i}(x)-f(x)|<\frac{1}{k},\hspace{0.2cm} \text{for all} \hspace{0.2cm} i > n \}$$ My question is, are these two sets the same? On the other hand, in the proof of this theorem, where is the fact that $ \mu(X) <\infty $ used? Proof of Egoroff's theorem: Also, I would like to know how to show that the theorem does extend, with essentially the same proof, to the situation in which the sequence $\{f_n\}$ is replaced by a family $\{f_t\}$ , where $t$ ranges over the positive reals; the assumptions are now that, for all $x\in X$ , (i) $\hspace{0.2cm}$ $\lim_{t \to \infty}f_{t}(x) = f(x)$ and (ii) $\hspace{0.2cm}$ $t \to f_{t}(x)$ is continuous Any answer is appreciated and valued","['elementary-set-theory', 'proof-explanation', 'measure-theory']"
4250574,"Prove if in a group $a$ commutes with $b$, it also commutes with $b^k$ for any integer $k$","Prove if in a group $a$ commutes with $b$ , it also commutes with $b^k$ for any integer $k$ (based on Gallian's Algebra text). This site has similar questions for individual integers; I'd like to prove it for all integers.  My proof is below. Is my proof correct? Can writing be improved? Is my use of induction appropriate? Is there a more direct proof than using induction? Proof: Since $ab=ba$ , $aba^{-1}=b$ and similarly $a^{-1}ba=b$ . We now show that for any integer $k$ , $a^kba^{-k}=b$ .  If $k>0$ , we have $a^kba^{-k} = a^{k-1}(aba^{-1})a^{-(k-1)} = a^{k-1}ba^{-(k-1)}$ .  By induction, we get $a^kba^{-k}=b$ .  A similar induction can be used for $k<0$ .  And for $k=0$ , $a^kba^{-k}=b$ is trivial. Consequently, $a^kb=ba^k$ , QED.","['group-theory', 'abstract-algebra', 'solution-verification', 'proof-writing']"
4250587,Finding all solutions to quartic Diophantine equation,"Consider the Diophantine equation $6x^2 = y^2(2y-1)(y-1)$ . I am interested in finding all solutions to this such that $y$ is a positive integer -- or at the very least knowing whether there are infinitely many. Certainly there are some; the smallest being $(x,y) = (0,1)$ , and then (less trivially) $(x,y) = (350,25)$ . Generating more is possible. This Diophantine equation arises in my research on certain determinants; I am not fluent enough in this area to see any immediate ways this question might be resolved. I had a look in Mordell, but could not see anything that could be shaped to give any direct answer (though perhaps I am wrong!).","['number-theory', 'diophantine-equations']"
4250612,"Why $C^{\infty}[0,1]$ with natural topology can not be normalized.","Consider $C^{\infty}[0,1]$ space, where $f_n \to f$ iff all its derivatives $f_n^{(k)}(x)$ uniformly converge to $f^{(k)}(x)$ . The question is prove that this space is metrizable, bot can not be normalized. My attempt: it's not hard to see that using $\{\sup_{[0,1]} |f^{(k)}(x) - g^{(k)}(x)|\}_{k\ge0}$ we can construct a metric $\rho$ , which replicate convergence in it's natural way. So now we need to find a sequence of function $\{f_n\}$ that converges in $\rho$ sense, but there is no norm for which this sequence also converges? The first idea is to consider something easily differentiable $\{\exp(nx)\}_n$ . This sequence looks reasonable, but I don't know why this sequence doesn't converge in any norm. Actually, it looks curious, that there is no appropriate norm to normalize this space.","['banach-spaces', 'vector-spaces', 'functional-analysis', 'examples-counterexamples']"
4250632,What's the difference between $-81^{3/2}$ & $(-81)^{3/2}$?,"Calculating $81^{3/2}$ , I got $729$ (not saying it is correct, but I am trying :) ).  Would $-81^{3/2}$ just be the opposite ( $-729$ ) and does it make a difference if $-81$ was placed inside a pair of parentheses $(-81)^{3/2}$ ?","['exponentiation', 'radicals', 'notation', 'associativity', 'algebra-precalculus']"
4250648,Standard Monty Hall problem: proof that switching is optimal?,"I have a model of the Monty Hall problem that as far as I know is standard: three doors, the contestant chooses one at random, then if Monty has a choice (i.e., the contestant has chosen the door with a car) he chooses his door to open at random, and events are generally independent. For the contestant, a strategy of always switching doors gives a $2/3$ probability of winning, and a strategy of never switching doors has a $1/3$ probability of winning. Intuitively the switching strategy is better because in order to win, you need to have choosen a door with a goat behind it; for the non-switching strategy, in order to win you need to have choosen the door with a car behind it. Since you're more likely to choose a goat door than the car door, you're better off switching. I have two further questions about this: I think it's clear that the contestant's ""gain"" from adopting a switching strategy is 2, in the sense that over many trials, the contestant is twice as likely to win by using the switching strategy. However, is there a way to compute a ""gain"" given that the contestant will only play the game once? How would we prove that the strategy of switching is optimal? I have searched but not found anything. Obviously if you only consider the two strategies, then computing the $2/3$ and $1/3$ probabilities above constitutes the proof. However, you could open up the model to allow for weird strategies like ""Always switch when selecting door 1 and Monty opens door 3, but switch only half the time when Monty selects door 2"", and so on. I am not sure how to consider these strategies given my model of the problem.","['monty-hall', 'probability']"
4250702,How to deduce this Fourier cosine transform of the product of modified Bessel function,"I want to know how to deduce $$
\int_0^\infty K_\nu(ax)I_\nu(bx)\cos cxdx=\frac{1}{2\sqrt{ab}}Q_{\nu-\frac12}(\frac{a^2+b^2+c^2}{2ab})
$$ My attempt: I have evaluated $$
\int_0^\infty J_\nu(ax)J_\nu(bx)e^{-cx}dx=\frac{1}{2\sqrt{ab}}Q_{\nu-\frac12}(\frac{a^2+b^2+c^2}{2ab})
$$ Then I want to prove $$
\int_0^\infty K_\nu(ax)I_\nu(bx)\cos cxdx= \int_0^\infty J_\nu(ax)J_\nu(bx)e^{-cx}dx
$$ I have tried Fourier transform, Mellin transform and series to prove the equation, but failed. How to deduce the equation? $$
\int_0^\infty K_\nu(ax)I_\nu(bx)\cos cxdx=\frac{1}{2\sqrt{ab}}Q_{\nu-\frac12}(\frac{a^2+b^2+c^2}{2ab})
$$ or $$
\int_0^\infty K_\nu(ax)I_\nu(bx)\cos cxdx= \int_0^\infty J_\nu(ax)J_\nu(bx)e^{-cx}dx
$$ Thank you for your time.","['integration', 'legendre-functions', 'bessel-functions']"
4250703,"Are there only two fields that are subrings of the $M_{2,2}(\mathbb{R})$, up to isomorphism? (Or subfields of them)","Inspired by $\operatorname{Mat}_2(\mathbb{R})$ as a field ,  it made me curious if, up to isomorphism, we only get two fields as subrings of the ring of two by two matrices under usual matrix addition and subtraction, $\mathbb{R}$ from $<I>$ and $\mathbb{C}$ (via the usual way): Complex number isomorphic to certain $2\times 2$ matrices? . My instinct is yes, since we need invertible matrices that stay invertible under linear combinations,  I don't see a way of doing that without forcing the patterns like we do in the above constructions, but I'm a bit shy of a proof.  (This is idle curiosity) Edit: As was pointed out in the comments, one can easily have any subfield of $\mathbb{R}$ or $\mathbb{C}$ constructed this way.   To get at the heart of what I meant,  can we get any fields that aren't isomorphic to a subfield of $\mathbb{C}$ this way?","['matrices', 'abstract-algebra', 'linear-algebra', 'field-theory']"
4250791,What functions with source and target the rational numbers satisfy the intermediate value theorem?,"I am curious to find if there is a characterization of the set $S$ of all functions $f: \mathbb{Q} \to \mathbb{Q}$ where for all intervals $[a,b] \subset \mathbb{Q}$ , there exists $x \in [a,b]$ for every $y$ between $f(a)$ and $f(b)$ such that $f(x) = y$ . We see $f(x) = x$ has $f \in S$ since for any $y \in [f(a),f(b)] \subset \mathbb{Q}$ we can choose $x = y$ to have $f(x) = y$ . For a non example, take $f(x) = x^2$ and $[f(0),f(2)] = [0,4]$ . We have $2 \in [0,4]$ , but $f(x) = 2$ has no solutions. Similarly, $f(x) = x^n \not \in S$ for $n > 1$ . From the lemmas I've found so far, I conjecture $S$ is made up of piecewise defined functions of the form $a(x-b)^n+c$ for $a,b,c \in \mathbb{Q}$ and $n \in \{-1,0,1\}$ with appropreate care taken to deal with discontinuities. Here are my questions Do we miss anything restricting to piecewise rational functions $p(x)/q(x)$ ? Supposing $f(x), g(x) \not \in S$ , can we show $f(x) + g(x) \not \in S$ Is this well known appearing in a book somewhere? Is the corresponding question for number fields well known?","['real-analysis', 'continuity', 'functions', 'rational-functions', 'rational-numbers']"
4250823,Is there any reason to consider the *bounded* derived category?,"When derived categories were invented, people had to consider bounded variants to ensure that the ""classical"" resolutions worked in this new context. However, we now know that the derived category of a Grothendieck abelian category has K-injective resolutions (tag 079P on the Stacks Project). Also, in many (all?) interesting cases we also have K-flat or even K-projective resolutions allowing us to derive functors of all sorts. (I would also love to know if there's a general result about these resolutions akin to the one cited on the Stacks Project.) This seems to imply that there's no more reason to consider bounded derived categories. Is there something that I'm missing? (P.S.: While I'm interested in all sorts of reasons, I have one example that may be useful. In Neeman's approach to the Grothendieck duality (via Brown representability), which fundamentally needs the unbounded derived category, people still define the functor $f^!$ on a bounded derived category. I imagine that there's more than history to this.)","['homological-algebra', 'algebraic-geometry']"
4250883,Question on tangent lines and the center of an ellipse,"Let $\frac{x^2}{a^2}+\frac{y^2}{b^2} =1$ be an ellipse. Let $A = (x_0,y_0)$ be a point outside of an ellipse. Draw two lines that touch an ellipse and denote two tangent points by $D_1 = (x_1,y_1),D_2 = (x_2,y_2)$ . My question is that if I draw a line passing through $A$ and the middle point of $D_1$ and $D_2$ i.e., $(\frac{x_1+x_2}{2},\frac{y_1+y_2}{2})$ then it passes the origin. Here's the image I found on google. I think it's correct but I don't know how to prove this. I already know the equation of a line passing $D_1$ and $D_2$ is $\frac{xx_0}{a^2}+\frac{yy_0}{b^2} =1$ . So what I need to show is $$\frac{y_0}{x_0}\left(\frac{x_1+x_2}{2}\right) = \frac{y_1+y_2}{2}$$ But I don't know how to get further. How can I do this? Is there any simple geometric proof of this?","['conic-sections', 'geometry']"
4250911,"Proving $\int_{\mathbb R^2}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)=c\phi(0,0)$ for $\phi$ Schwartz","I'm trying to prove that $$\int_{\mathbb R^2}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)=c\phi(0,0)$$ for $\phi\in\mathcal S(\mathbb R^2)$ for some constant $c$ . I can see that $(x+iy)^{-1}$ is  smooth and bounded away from the origin, while it has finite integral over the unit ball. Hence by dominated convergence (using that $\phi$ is Schwartz), we can write $$\int_{\mathbb R^2}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)=\lim_{\epsilon\to0}\int_{\mathbb R^2-B_\epsilon(0)}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)$$ Away from $(0,0)$ we have $(\partial_x+i\partial_y)(x+iy)=0$ . After this, to manipulate the integral, I'd like to use some divergence formula/ integration by parts formula, and was wondering if we could make sense of something like the following: \begin{align*}&\phantom=\int_{\mathbb R^2-B_\epsilon(0)}\frac{(\partial_ x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)+\int_{\mathbb R^2-B_\epsilon(0)}(\partial_x+i\partial_y)\left(\frac{1}{x+iy}\right)\phi(x,y)\ \mathrm d(x,y)\\&=\int_{\partial B_\epsilon(0)}\frac1{x+iy}\phi(x,y)\cdot \mathbf n\ \mathrm d(x,y)\\&=\int_0^{2\pi}\frac1{\epsilon e^{i\theta}}\phi(\epsilon,\theta)e^{i\theta}\epsilon\ \mathrm d\theta,\end{align*} which would be $\approx2\pi\phi(0,0)$ for $\epsilon$ small, by continuity of $\phi$ . Then the second integral in the first line of the previous display equals zero since on the domain $(\partial_ x+i\partial_y)(x+iy)=0$ . Is such a divergence type formula justified? If not, how to calculate this integral?","['multivariable-calculus', 'real-analysis']"
4250998,Counterexample for a differentiable structure,"I'm trying to understand the definition of a differentiable structure.
Is it correct that $x\rightarrow x$ and $x \rightarrow x^3$ doesn't form a diffeomorphism, since $x\rightarrow x^{1/3}$ isn't differentiable in $0$ ?",['analysis']
4251034,"Is $f:(-1,1)\to \{y\in\mathbb{C}\mid |y| =1\}\setminus\{-1\}:x\mapsto e^{i\pi x} $ a homeomorphism?","Consider $f:(-1,1)\to \{y\in\mathbb{C}\mid |y| =1\}\setminus\{-1\}:x\mapsto e^{i\pi x} $ .
I think $f$ is a homeomorphism but I still need to show that the inverse $f^{-1}$ is continuous.
For the inverse I calculated if $0\leq\theta<\pi$ that $f^{-1}(e^{i\theta})= \frac{\theta}{\pi} $ and if $\pi<\theta<2\pi$ then $\frac{\theta-2\pi}{\pi}$ .
How do I go on for the continuity?",['analysis']
4251039,$L^1(G)$ is a $C^*$-algebra $\iff$ $G$ is trivial,"Let $L^1(G)$ be the group Banach $*$ -algebra of a locally compact Hausdorff group $G$ . Then it is known that $L^1(G)$ is a $C^*$ -algebra if and only if the group $G$ is trivial. The proof of this fact can be found here and it is not elementary. To my surprise, I came across a seemingly much simpler and completely elementary proof in Vahid Shirbisheh's Lectures on $C^*$ -algebras , Proposition 2.2.21., Page 24. However, upon more careful reading, I think that the proof is wrong. It goes like this: Assume that $G$ is nontrivial and pick $s \in G\setminus \{1\}$ . Pick an open relatively compact neighbourhood $U$ of $1$ in $G$ such that $U \cap sU = \emptyset$ . Then by properties of left Haar measure we have $0 < \mu(U) < +\infty$ . If we define $$f : G \to \Bbb{C}, \quad f(x) := \frac1{\sqrt{\Delta(x)}}(\mathbf{1}_{U} - i \mathbf{1}_{sU})(x), \quad x \in G$$ it is easy to show that $f \in L^1(G)$ , where $\Delta : G \to \langle0,+\infty\rangle$ is the modular function. We have \begin{align}
\|f * f^*\|_{L^1(G)} &= \int_G |(f * f^*)(x)|\,d\mu(x)\\
&=\int_G \left|\int_G f(y)\Delta(x^{-1}y)\overline{f(x^{-1}y)}\,d\mu(y)\right|d\mu(x)\\
&= \int_G \left|\int_G \Delta(x^{-1}y)\cdot \frac{\mathbf{1}_{U}(y) - i \mathbf{1}_{sU}(y)}{\sqrt{\Delta(y)}}\cdot \frac{\mathbf{1}_{U}(x^{-1}y) + i \mathbf{1}_{sU}(x^{-1}y)}{\sqrt{\Delta(x^{-1}y)}}\,d\mu(y)\right|d\mu(x)\\
&= \int_G \frac1{\sqrt{\Delta(x)}} \left|\int_G (\mathbf{1}_U(y)\mathbf{1}_{U}(x^{-1}y) + i\mathbf{1}_U(y)\mathbf{1}_{sU}(x^{-1}y) - i\mathbf{1}_U(x^{-1}y)\mathbf{1}_{sU}(y) + \mathbf{1}_{sU}(y)\mathbf{1}_{sU}(x^{-1}y))\,d\mu(y)\right|d\mu(x)\\
&= \int_G \frac1{\sqrt{\Delta(x)}} \left|\mu(U \cap x U) + i \mu(U \cap xsU) - i\mu(xU \cap sU) + \mu(sU \cap xsU)\right|d\mu(x).
\end{align} Now comes the dubious step. The author writes (paraphrased): ""In the last integral each term of the integrand is non-zero at least for some values of $x\in G$ and when one of the terms is non-zero the other three terms are zero. This feature justifies the following steps of our argument:"" \begin{align}
\|f * f^*\|_{L^1(G)} &< \int_G \frac1{\sqrt{\Delta(x)}} (\mu(U \cap x U) +  \mu(U \cap xsU) +\mu(xU \cap sU) + \mu(sU \cap xsU))\,d\mu(x)\\
&= \int_G \frac1{\sqrt{\Delta(x)}} \left|\int_G (\mathbf{1}_U(y)\mathbf{1}_{U}(x^{-1}y) + \mathbf{1}_U(y)\mathbf{1}_{sU}(x^{-1}y) + \mathbf{1}_U(x^{-1}y)\mathbf{1}_{sU}(y) + \mathbf{1}_{sU}(y)\mathbf{1}_{sU}(x^{-1}y))\,d\mu(y)\right|d\mu(x)\\
&= \int_G \left(\int_G \Delta(x^{-1}y)\cdot \frac{\mathbf{1}_{U}(y) + \mathbf{1}_{sU}(y)}{\sqrt{\Delta(y)}}\cdot \frac{\mathbf{1}_{U}(x^{-1}y) + \mathbf{1}_{sU}(x^{-1}y)}{\sqrt{\Delta(x^{-1}y)}}\,d\mu(y)\right)d\mu(x)\\
&= \int_G \left(\int_G \Delta(x^{-1}y)\cdot \left|\frac{\mathbf{1}_{U}(y) -i \mathbf{1}_{sU}(y)}{\sqrt{\Delta(y)}}\right|\cdot \left|\frac{\mathbf{1}_{U}(x^{-1}y) -i \mathbf{1}_{sU}(x^{-1}y)}{\sqrt{\Delta(x^{-1}y)}}\right|\,d\mu(y)\right)d\mu(x)\\
&= \int_G \left(\int_G \Delta(x^{-1}y)|f(y)||f(x^{-1}y)|\,d\mu(y)\right)d\mu(x)\\
&= \int_G \Delta(y)|f(y)|\left(\int_G |f(xy)|\,d\mu(x)\right)\,d\mu(y)\\
&= \int_G \Delta(y)|f(y)|\Delta(y^{-1})\left(\int_G |f(x)|\,d\mu(x)\right)\,d\mu(y)\\
&= \|f\|_{L^1(G)}^2.
\end{align} So, the idea seems to be to notice that the supports of the functions $$x \mapsto \mu(U \cap x U), \quad x \mapsto \mu(U \cap xsU), \quad x \mapsto \mu(xU \cap sU), \quad x \mapsto \mu(sU \cap xsU)$$ are pairwise disjoint and nonempty. Hence we somehow get the strict inequality, but I argue that it should in fact be equality. Precisely because the supports are pairwise disjoint we have $$\left|\mu(U \cap x U) + i \mu(U \cap xsU) - i\mu(xU \cap sU) + \mu(sU \cap xsU)\right|=\mu(U \cap x U) +  \mu(U \cap xsU) +\mu(xU \cap sU) + \mu(sU \cap xsU)$$ for all $x \in G$ . It is irrelevant that the supports are nonempty. My questions are: Is the proof indeed wrong? If yes, can it be easily fixed?","['c-star-algebras', 'measure-theory', 'representation-theory', 'real-analysis', 'functional-analysis']"
4251189,Interpretation of standard deviation,"I've come across many articles that describe standard deviation to be a measure of how much spread out our distribution is. In other words, it is a measure of how far away from the mean, we expect a data point to land. Now, imagine we have a distribution of random variables, like in the case of a symmetric random walk. The mean of the distribution is obviously $0$ . So we can write $\langle x_i \rangle = 0 $ . In this case, let $\sigma$ be the standard deviation. Then we have the following : $$\sigma^2 = \langle x_i^2 \rangle - \langle x_i\rangle^2$$ In our example, we can say : $$\sigma = \sqrt{\langle x_i^2\rangle}$$ The term on the right is often referred to as root mean squared distance in random walk and other problems of the sort. It is often interpreted as, if we repeat the experiment many many times, and take the squares of the final outcome, and average it, and then take the square root, our average outcome is $\sigma.$ What I know is, the likelihood of getting a data point is maximum between $+\sigma$ and $-\sigma$ i.e. within one standard deviation of the mean with a likelihood of around $70 $ percent. However, I still don't understand why many authors, interpret the root mean squared distance to be the average distance where we find the final answer. To me, the root mean squared distance is actually the boundary of the region (strip) where we are most likely to obtain our results. However, even though we are likely to obtain our data within the root mean squared distance, or the standard deviation, we do find many data points outside of this range. If we take an average of all these distances where we happen to find the range, it does lie around the standard deviation. Is this alternate interpretation of standard deviation correct? Normally standard deviation marks the boundary of the region in which, we are most likely to obtain the outcomes. However, it is also the average distance from the mean, where we expect our outcomes to be. This is usually the description of the root mean squared position, but in this instance, it is equivalent to the standard deviation. In random walk, most of our walkers, about $70$ percent of them, are found within the region bounded by $\pm \sigma.$ These walkers have taken more steps in one direction compared to the other. Since $\sigma \approx  \sqrt{n}$ , the difference in steps for 70 percent of walkers is less than $\sqrt{n}$ . However, for the rest of the walkers, about 30 percent of them, this difference is more than $\sqrt{n}$ and can be enormously large. So, we have 70 percent of walkers with a small difference in steps and a small no. of walkers with a large difference in steps. The average difference thus happens to lie around the standard deviation, and so it is often interpreted as the average distance where we happen to find our walker. Is this reasoning correct?","['statistics', 'standard-deviation', 'random-walk', 'probability-distributions', 'probability']"
4251191,"$P_n$ is the maximum prime factor of $1 + P_1P_2P_3…P_{n-1}$ for any integer n ≥ 2, where $P_1$ = 2. Is there a $P_n =11$?","The definition of a sequence ${P_n}$ is as following: $P_1$ = 2, $P_n$ is the maximum prime factor of $1 + P_1P_2P_3…P_{n-1}$ for any integer n ≥ 2. Is there one term equal to 11 in this sequence? I tried the following We know that $P_1$ = 2, $P_2$ = 3, and $P_3$ = 7. If there is a $P_n$ = 11, then $1 + P_1P_2P_3…P_{n-1}$ has a maximum prime factor of 11. Since two consecutive integers will not have common prime factors, $1 + P_1P_2P_3…P_{n-1}$ does not have prime factors of 2, 3, and 7. Therefore, the only possible prime factorization for $1 + P_1P_2P_3…P_{n-1}$ is $5^a11^b$ .How should I start from here? Thanks.","['modular-arithmetic', 'prime-numbers', 'sequences-and-series']"
4251193,When is the ring homomorphism $\mathbb{Z} \to R$ an epimorphism?,"For every ring $R$ there exists a unique ring homomorphism $f: \mathbb{Z} \to R$ .  For which rings is this an epimorphism? The only examples I know are the subrings of $\mathbb{Q}$ and the quotient rings of $\mathbb{Z}$ , namely the rings $\mathbb{Z}/\langle n \rangle$ .  Are these all, or are there more? Three characterizations of epimorphisms of commutative rings are listed here: Epimorphisms of rings , The Stacks Project. Maybe one will help. (Why I'm interested: an object in a category is subterminal if its unique morphism to the terminal object is a monomorphism.  Since $\mathbb{Z}$ is initial in $\mathrm{Ring}$ , here I am asking what are the subterminal objects in $\mathrm{Ring}^{\rm op}$ .)","['ring-theory', 'abstract-algebra']"
4251257,Why are dense set useful in analysis,"Consider the following theorem from Rudin's Real-and complex analysis. 3.14 Theorem For $1 \leq p < \infty, C_c(X)$ is dense in $L^p(\mu)$ . What I don't understand is why this result is useful in proofs. I heard few examples of where in order to define functionals in $L^p(\mu)$ you can firstly define them in $C_c(X)$ and then extend them using Hahn Banach theorem. Is this the main application? definition of maps and functionals? Update : If anyone could point to the proofs of results that actually use density of $C_c(X)$ in $L^p(\mathbb{\mu})$ that would be useful.","['functional-analysis', 'real-analysis']"
4251261,How to solve $\dot{\mathbf{r}}(\theta)^2 + \mathbf{r}(\theta)^2 = 1$?,"I'm trying to determine all smooth functions $\mathbf{r}(\theta)$ such that the curve $\gamma(\theta) = (\mathbf{r}(\theta)\cos(\theta), \mathbf{r}(\theta)\sin(\theta))$ is unit-speed curve, i.e. that $\|\dot{\gamma}(\theta)\| = 1 \implies $$\dot{\mathbf{r}}(\theta)^2 + \mathbf{r}(\theta)^2 = 1$ . According to my source, which omits intermediate steps, the solution ends up being $\pm\sin(\theta + \alpha)$ for some constant $\alpha$ . But how can I show this myself?","['curves', 'ordinary-differential-equations', 'differential-geometry']"
4251262,"If $f:{\Bbb R}^m \to {\Bbb R}^n$ is Lipschitz on all compact sets, under what conditions is $f$ a $C^1$ map?","If $f:{\Bbb R}^m \to {\Bbb R}^n$ is Lipschitz on all compact sets, under what conditions is $f$ a $C^1$ map? Background : I have proved the following result, and I am looking for a converse . Proposition. Let $f: {\Bbb R}^m \to {\Bbb R}^n$ be a $C^1$ mapping and let $K \subset {\Bbb R}^m$ be compact. Then the restriction $f|_K$ of $f$ to $K$ is Lipschitz continuous. The proof of this result, and some special cases, can be found here: Post 1 , Post 2 , Post 3 , and Post 4 . In general, differentiability is a stronger condition than continuity, so there is no reason to expect the ""Lipschitz on all compact sets"" assumption above to imply differentiability. What other additional assumptions on $f$ would be required to ensure that it is a $C^1$ mapping? Rademacher's Theorem is a related result; but I am clearly looking for something stronger. The goal is to characterize $C^1$ maps in terms of Lipschitz continuity on compact sets, and other additional conditions if required. Thank you, and I am excited to see where this goes!","['multivariable-calculus', 'lipschitz-functions', 'compactness', 'real-analysis']"
4251270,Can all squares in a free group be made from squares in the free monoid?,"Here's a question I thought of, that I don't know the answer to. Let $F_2$ be the free group on $\{a,b\}$ , and $F_2^+$ be the subset where all the exponents are positive. For a set $S$ , let $S^2=\{g^2:g\in S\}$ . Is $\langle (F_2^+)^2\rangle$ (the group generated by $(F_2^+)^2$ ) the same as $\langle (F_2)^2\rangle$ ? For example, can $(a^{-1}b)^2$ be written as a product of squares, each with all-positive or all-negative exponents? I've thought a bit about Cayley diagrams, but I'm not sure how to make progress.","['monoid', 'group-theory', 'free-groups']"
4251315,$C^k$ extension of a function,"Let $\Omega$ be a bounded set in $\mathbb{R}^n.$ Suppose $f$ is continuous on $\Omega \subset \mathbb{R}^n$ then continuous extension does not exist in general. For example $f(x)=1/x$ and $\Omega=(0,1)$ does not admit continuous extension on $\mathbb{R}$ . On the other hand, if $f$ is uniformly continuous on $\Omega$ then there exists a uniformly continuous function $g \in C(\mathbb{R}^n)$ such that $f=g$ on $\Omega.$ In other words uniformly continuous function admits uniformly continuous extensions on $\mathbb{R}^n.$ Are there any analogous results for higher derivatives? i.e. Under what conditions on $\Omega$ and $f,$ we get an extension of $f$ which is $C^k(\mathbb{R}^n)$ (k-times continuously differentiable)? Do we need regularity assumption on boundary( $\partial \Omega$ ) of $\Omega?$ Rigorous proof/references will be appreciated.","['multivariable-calculus', 'derivatives', 'analysis', 'real-analysis']"
4251321,extension Rolle's theorem for limit values,"Rolle's theorem states that: If a real-valued function $f$ is continuous on a proper closed interval $[a, b]$ , differentiable on the open interval $(a, b)$ , and $f (a) = f (b)$ , then there exists at least one $c$ in the open interval $(a, b)$ such that ${\displaystyle f'(c)=0}$ . Exercise: Show that Rolle's theorem is true in case $f$ is defined and differentiable in the open interval  ] $a, b\left[\right.$ , and $\lim\limits_{x\to a^+}f(x)=\lim\limits_{x\to b^-}f(x)$ . Note that $a$ could be $-\infty$ and $b$ could be $+\infty$ . Furthermore the two limits could also be infinite. My attempt: Let choose $a_1, b_1$ so that $a<a_1<b_1<b$ . Now we have the following cases: $f(a_1)=f(b_1)$ , $f(a_1)<f(b_1)$ or $f(a_1)>f(b_1)$ . When $f(a_1)=f(b_1)$ we can directly apply Rolle's theorem on $]a_1,b_1[$ so $ \exists c \in ]a_1,b_1[$ such that $f′(c)=0$ . If $f(a_1)<f(b_1)$ then there is a real number $z$ such that $f(a_1)<z<f(b_1)$ and by the intermediate value theorem we have $a_2 \in ]a_1,b_1[$ which $f(a_2)=z$ . Now, again we can apply Rolle's theorem to the restriction of $f$ to $[a_2,b_1]$ . The case when $f(a_1)>f(b_1)$ can be done similarly. My question: Is this proof correct or I am missing something? Thank you in advance.","['calculus', 'functions', 'derivatives', 'real-analysis']"
4251368,Correct notation for the set of composite numbers,"The set of all prime numbers is usually denoted by $\mathbb{P}$. The set of all composite numbers, however is not denoted by $\mathbb{C}$, given the ambiguity with the set of complex numbers. What is the correct (usual) way of denoting the set of composite numbers (with a single symbol)? EDIT - an example : Given a function $f$ that has a ""prime version"" and a ""composite version"", one may denote the ""prime $f$"" function by $f_{\mathbb{P}}$, but the ""composite $f$"" function cannot be denoted by $f_{\mathbb{C}}$ since it creates ambiguity with the set of complex numbers. What symbol would one use in this case to denote the ""composite $f$"" function?","['elementary-set-theory', 'notation']"
4251371,"Why do particular solutions of ordinary, non-homogeneous differential equations not contains arbitrary constants, but homogeneous solutions do?","I've been solving ordinary linear differential equations for a while. First order and second order mostly, utilizing a slew of methods and theorems to find particular and complementary solutions. I still don't quite understand the intuition behind the fact that when I find a solution to a non-homogeneous equation it doesn't contain arbitrary constants like what happens when I find a general solution to the associated homogeneous equation. In the latter case, I can show that every solution to the homogeneous equation is a linear combination of n linearly independent solutions to the same equation, where n is the order of the (differential) equation. One of such linear combinations is a particular solution to the homogeneous equation. What is it about adding an input term that makes it so that the same result (n l.i. solutions form a basis for all other solutions) doesn't apply to particular solutions to the non-homogeneous equation? In other words, why do particular solutions of non-homogeneous equations not contain arbitrary constants?",['ordinary-differential-equations']
4251381,"When is the product of two injective functions, also injective?","I'm currently, by curiosity, investigating when the product of two injective functions, is also injective. My condition is that this is true if and only if there $\nexists x_1, x_2 \in X$ such that $f(x_1) = g(x_2)$ and $g(x_1) = f(x_2)$ . Here's my reasoning, I hope someone can confirm / disconfirm it, or leave some response. Proof. Let's suppose we got two functions: $f: X \mapsto Y $ , and $g: X \mapsto Y $ , where the function $h(x)$ is defined as the product between the two: $$h(x) = f(x)g(x)$$ The definition of an injective function, states that for some $x_1,x_2 \in X$ where $x_1 \neq x_2 \Rightarrow f(x_1) \neq f(x_2)$ . Which then also holds for $g(x)$ . Let's use this fact for our newly defined function: $$\left\{\begin{matrix}
h(x_1) = f(x_1)g(x_1)\\ 
h(x_2) = f(x_2)g(x_2)
\end{matrix}\right.$$ If $h(x)$ is injective, then: $x_1 \neq x_2 \Rightarrow h(x_1) \neq h(x_2)$ . Because $f(x)$ and $g(x)$ are injective functions, the statement is true only and only if $f(x_1) \neq g(x_2)$ and $g(x_1) \neq f(x_2)$ . $\ \ \ \ \ \ \ \ \ \square$ Can this be interpreted graphically, where the functions can't have any common y - values if I haven't misunderstood myself? Is there any other way to interpret the given result? Thanks for your help!","['functions', 'real-analysis']"
4251401,Sudoku on a countably infinite board,"Consider the game of Sudoku played on an infinite board where the subsquares are also infinite, i.e. our board is indexed by $\mathbb{N}^2 \times \mathbb{N}^2$ . Let's call a solution to such a game a function $f(a, b, m, n)$ which assigns a natural number to each space $(m,n)$ in each subsquare $(a,b)$ , such that each row, column, and subsquare contains every natural number exactly once . It is clear that such a solution exists, as for any finite board state, given any natural number and any row, column, or subsquare, there are always at most a finite number of ""collision"" squares, and so with infinite spaces at our disposal we can always pick a space to put this number in, and continue to do this infinitely until we have filled the board. However, I'm having trouble constructing an explicit example of such a solution, which does not rely on this choice-like magic. My initial thought was to use products of primes  to guarantee that you don't have a collision, but while I can get plenty of solutions with no repetitions, guaranteeing that every row, column, and subsquare contains every label seems like a lot harder of a challenge. But, I suspect I'm missing  a very elegant / basic solution. Any ideas / hints?","['number-theory', 'puzzle']"
4251459,The main idea behind Big O notation,"Well, when we use Big O notation we never know even approximate number of steps of a given algorithm, right? For example, if we have $O(n)$ algorithm, then we don't know how fast this algorithm itself is: its exact number of steps may be $10n$ or even $10^{100}n$ . Thus we just put this algorithm in particular ""class of functions"" $O(n)$ . The reason for doing that is for us to be able to compare algorithms from different ""classes"". For example, we always know that $O(n)$ is going to be slower than $O(\log_2 n)$ for large $n$ , because $\lim_{n \to +\infty} \frac{\log_2 n}{n} = 0$ . But if we are told that there are two different $O(n)$ algorithms, then we can't say which one is going to be faster, right? Moreover, if we are given a single $O(n)$ algorithm, we can't say how fast it is (because it may take $10n$ or even $10^{100}n$ steps and still be $O(n)$ ). Is it the main and the only idea behind Big O notation? Or is there something else that I missed? I've already googled it million times and still have this confusion in my head. So, please, check my understanding and say if anything is wrong. Thanks in advance!","['big-picture', 'asymptotics', 'real-analysis']"
4251469,Lemma 2.3. of Riemannian Geometry by do Carmo; Existence/Uniqueness of the Geodesic field $G$ on $TM$,"Lemma 2.3 states: There exists a unique vector field $G$ on $TM$ (+) whose trajectories are of the form $t\mapsto (\gamma(t),\gamma'(t))$ , where $\gamma$ is a geodesic on $M$ . Uniqueness: Suppose that there exists a vector field $G$ on $TM$ with (+). Choose $q\in TM$ . By assumption, we then know that the trajectory $\alpha:(-\delta,\delta)\to M$ with $\alpha'(t)=G(\alpha(t))$ and $\alpha(0)=q$ is of the form $\alpha(t)=(\gamma(t),\gamma'(t))$ , where $\gamma$ is a geodesic of $M$ . Now $\gamma$ , in local coordinates, has to satisfy a first order system with initial conditions given by $q$ , for which there is a unique solution. Since $\alpha'(0)=G(\alpha(0))=G(q)$ and $\alpha$ is completly determined by this unique solution $\gamma$ , we conclude that $G(q)$ is also uniquely determined. Since this holds for every $q\in TM$ , $G$ has to be unique. In order to show existence, do Carmo locally defines $G$ by the system mentioned above. It follows that the trajectories of $G$ are of the form $(\gamma,\gamma')$ . He finishes the proof by stating: Using the uniqueness, we conclude that $G$ is well-defined on $TM$ . My idea: In the uniqueness argument, we have shown that $G(q)$ is uniquely determined for every $q\in TM$ . It follows that for every open subset $O\subset TM$ , there also exists at most one vector field with the property (+). Now let $x:U\subset \mathbb{R}^n \to M$ and $y:V\subset \mathbb{R}^n \to M$ be two parametrizations with $x(U)\cap y(V)\neq \phi$ and denote by $G_x$ and $G_y$ their corresponding vector fields on $x(U)$ and $y(V)$ which satisfy (+) (which we know exist by the construction above). Then $G_x$ and $G_y$ also satisfy (+) on the open set $x(U)\cap y(V)$ . It follows that $G_x\bigg|_{x(U)\cap y(V)}=G_y\bigg|_{x(U)\cap y(V)},$ which shows that $G$ is well-defined. I am not sure if this argument is correct. It also seems a little too complicated to me/not what do Carmo intended. I would be grateful for any other (easier) proof of this fact which uses the uniqueness-part. Without relying on the uniqueness, the well-definedness is shown in this answer https://math.stackexchange.com/a/1359927/776794e by a direct calculation using the formula for the coordinate change. In Riemannian Manifolds, Lee proves that $$Gf(p,v)=\frac{d}{dt}\bigg|_{t=0} f(\gamma_v(t),\dot{\gamma}_v(t)),$$ where $\gamma_v$ denotes the unique geodesic with $\gamma(0)=p, \dot{\gamma}(0)=v$ , which also implies that $G$ is well-defined.","['vector-fields', 'smooth-manifolds', 'alternative-proof', 'tangent-bundle', 'differential-geometry']"
4251495,Prove that this function is increasing,"Let $g : \mathbb{N} \rightarrow \mathbb{R}_+$ with $g(0) = 0$ . Let's start by assuming that there exists positive constants $\Gamma_-\leq 1 \leq \Gamma_+$ such that $$\Gamma_- \leq g(k)-g(k-1)\leq \Gamma_+.$$ For $\phi \in \mathbb{R_+}$ , consider a random variable $X_\phi$ that assumes natural values ​​with the following distribution: $$ \mathbb{P}( X_\phi =k) = \frac{1}{Z(\phi)}\frac{\phi^k}{g(k)!} \text{, for all } k \in \mathbb{N}, $$ where $g(k)!=g(k)g(k-1)\cdot \cdot \cdot g(1)$ , $g(0)!=1$ and $Z(\phi)$ is a normalizing constant given by \begin{equation}
Z(\phi)=\sum_{k=0}^{\infty}\frac{\phi^k}{g(k)!}.
\end{equation} I need to show that the expectation of this random variable is a increasing function of $\phi$ . Denoting this expectation by $R(\phi)$ , a simple calculation shows that $$R(\phi)= \frac{1}{Z(\phi)}\sum_{k=0}^{\infty}\frac{k\phi^k}{g(k)!} = \phi \frac{Z^\prime(\phi)}{Z(\phi)}.$$ By deriving this expression, we get $$R'(\phi)= \frac{Z'(\phi)Z(\phi)+\phi Z''(\phi)Z(\phi)-\phi Z'(\phi)^2}{Z(\phi)^2}.$$ So we just need to show that the numerator of the above expression is positive. But I fail to show this. We can also see that $R(\phi)= \phi \frac{d}{d \phi} \ln Z(\phi)$ , but I don't know if it helps either. Any help in solving this exercise will be appreciated. Thank you!","['expected-value', 'probability', 'real-analysis']"
4251496,Solution containing Riemann Zeta function for an integral involving the EGF of the Bernoulli/Euler polynomials,"In this post , the first of the following integrals is questioned. I added the second one. $$
\begin{align}
&2\int_{0}^{\infty}\left(\sum_{k=0}^{n}\frac{\left(-1\right)^{k}B_{k}(1)}{k!}x^{k-n-1}-\frac{1}{x^{n}\left(e^{x}-1\right)}\right)dx\\\\
=&\ \frac{1}{1-2^{n}}\int_{0}^{\infty}\left(\sum_{k=0}^{n-1}\frac{\left(-1\right)^{k}E_k(1)}{k!}x^{k-n}-\frac{2}{x^{n}\left(e^{x}+1\right)}\right)dx\\\\
=&\ \frac{\zeta (n)}{(2\pi)^{n-1}},\quad\text{for odd } n.
\end{align}
$$ The result is conjectured. How can we prove it? This problem is derived from integrating the EGF of the Bernoulli & Euler polynomials after dividing it by a power of $x$ . See my previous post , which outlines the far more generalized problem (and a generalized conjecture).","['definite-integrals', 'bernoulli-polynomials', 'generating-functions', 'riemann-zeta', 'limits']"
4251527,Finding a basis for the canonical line bundle of $f=y^3-x^5+1$,"Consider the canonical line bundle $K_C$ for $C$ defined by the compactification of $f=y^3-x^5+1$ . $K_C$ is the as a set the set of holomorphic $1$ -forms on $C$ . How does one go about finding the basis for $K_C$ ? I have seen an example for hyperellptic curves that gives the basis as $z^adz/w$ , where $w$ is the product of rational functions having divisors precisely at ramification points and infinity. This does not even seem holomorphic to me, and I am thus confused in general.","['curves', 'algebraic-geometry', 'riemann-surfaces', 'differential-geometry']"
