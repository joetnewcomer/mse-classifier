question_id,title,body,tags
1088732,Find a and b of $x^3+ax^2+bx−26=0$,"I am doing practise papers and one of the questions is: The cubic equation $x^3+ax^2+bx−26=0$ has $3$ positive, distinct,
  integer roots. Find the values of $a$ and $b$ The mark scheme says: $3$ roots are $1, 2, 13$. Equation is $x^3-16x^2+41x-26=0$ I'm guessing this has something to do with the factor theorem thing where $x-a$ is a factor if $f(a)=0$ but with 3 unknowns I don't get how you find any of these. I tried just putting $x$ as $1$: $$
1^3+a+b-26=0
$$
$$
a+b=25
$$
then doing the same with $2$:
$$
2^3+4a+2b-26=0
$$
$$
4a+2b=18
$$
and after subsituting the two equations I got the answer of a $a=-16$ and $b=41$. However, I realised this was just by chance that these numbers were factors (it didn't work with $x=3$ for example). What is the proper way to solve this?","['factoring', 'cubics', 'algebra-precalculus']"
1088742,Are reference angles also coterminal angles?,"It would seem reference angles end up on the same terminal ray, so wouldn't it be safe to assume that reference angles are also coterminal angles by classification? Thank You","['trigonometry', 'algebra-precalculus']"
1088758,Defining a sheaf of differential operators,"I have two questions related to the coordinate-free definition of $\mathcal{D}$ -modules provided in Ginzburg's notes (see pages 24-25): Exercise 2.1.13 says that Diff( $M,M$ ) is almost-commutative, i.e. that the associated graded ring $A$ is commutative. But the zeroth graded piece is simply $A$ -linear homomorphisms from $M$ to $M$ , in which composition $f\circ g$ need not be commutative in general. What am I missing? We define $\mathcal{D}(\mathcal{M},\mathcal{N})$ (for $\mathcal{M},\mathcal{N}$ coherent) on $X$ by defining it on the basis of open affines of $X$ by $$\Gamma(U,\mathcal{D}(\mathcal{M},\mathcal{N}))=\text{Diff}_{\mathcal{O}(U)}(\mathcal{M}(U),\mathcal{N}(U)).$$ How do the details work out? I'm confused, in particular, about how to construct the restriction maps $$\rho_{VU}:\text{Diff}_{\mathcal{O}(U)}(\mathcal{M}(U),\mathcal{N}(U))\to \text{Diff}_{\mathcal{O}(V)}(\mathcal{M}(V),\mathcal{N}(V)).$$ Is the coherence hypothesis on $\mathcal{M},\mathcal{N}$ needed/used here? Thanks.","['modules', 'algebraic-geometry', 'd-modules']"
1088780,Proving that the solutions of a system of ODE $\frac{dx}{dt}=-\nabla V(x)$ are defined for all positive time.,"Let $V:\mathbb{R}^n \to \mathbb{R}$ s.t $V\in C^2$ and $\lim_{x\to \infty} V(x)=\infty$. Prove that the system: 
$$\frac{dx}{dt}=-\nabla V(x)$$ 
has defined solutions for all positive time. My questions: is the hypothesis right? How can I use the limit? Also, thanks for any suggestions.","['ordinary-differential-equations', 'real-analysis']"
1088782,Covariant derivative and geodesic,"Let $f: U \subset \mathbb{R}^2 \rightarrow \mathbb{R}^3$ be a surface patch. Then if we have two vector fields $$X = \sum_i \xi^i \frac{\partial f}{\partial u^i}$$ and $$Y = \sum_i \eta^i \frac{\partial f}{\partial u^i}$$ the covariant derivative is given by $$\nabla_X Y:= \sum_{i,k} \xi^{i} \left( \frac{\partial \eta^k}{\partial u^i} + \sum_j \Gamma_{i,j}^k \eta^j\right)\frac{\partial f}{\partial u^k}$$ Now my question is: If I have a curve $\gamma: I \rightarrow U$ and $c:=f \circ \gamma,$ then my textbook claims that $$\nabla_{c'}c' = \sum_{i,k} \dot{u}^{i}(t) \left( \frac{\partial \dot{u}^{k}(t)}{\partial u^i} + \sum_j \Gamma_{i,j}^k \dot{u}^{j}(t)\right)\frac{\partial f}{\partial u^k}$$ Although I don't know what this function $u$ should actually denote, this equation looks somehow wrong, as $\dot{u}$ is a function of $t$ but is differentiated with respect to $u^{i}$, which does not make sense to me. Could anybody explain this to me? So I have essentially two questions: 1.)  How is $u$ defined? 2.) How does this equation follow from the definition of the covariant derivative and why is the derivative of $\dot{u}$ with respect to $u^{i}$ not zero?","['calculus', 'differential-geometry', 'geodesic', 'real-analysis', 'analysis']"
1088801,Weak formulation for nonhomogeneous problem $-\Delta u = 0$,"I am wondering about the definition of weak solution to the nonhomogeneous problem
$$-\Delta u = 0 \text{ in }\Omega$$
$$u = g \text{ in }\partial\Omega$$
given $g \in H^{\frac 12}(\partial\Omega)$. It should be something like: $u \in H^1(\Omega)$ satisfies
$$\int_\Omega \nabla u \nabla v - \int_{\partial\Omega} v \partial_\nu u = 0\quad\text{for all $v \in H^1(\Omega)$}\tag{1}$$
and $\gamma(u) = g$ where $\gamma$ is the trace operator. Edit : as suggested by Tomas, the normal derivative is not defined for $H^1$ functions. So I am not sure what the natural weak formulation should be for this problem. I could test with $H^1_0$ functions instead, but this seems unnatural and we obtain a mixed bilinear form. Let $G$ extend $g$, and consider the homogeneous problem
$$-\Delta w = \Delta G \text{ in }\Omega$$
$$w = 0 \text{ in }\partial\Omega$$
This is well-posed via Lax-Milgram and we have $w \in H^1_0(\Omega)$ such that
$$\int_\Omega \nabla w \nabla \varphi = -\int_\Omega \nabla G \nabla \varphi$$
for all $\varphi \in H^1_0(\Omega)$.
So then set $u=w+G \in H^1(\Omega)$, then $\gamma(u) = g$ and $u$ satisfies
$$\int_\Omega \nabla u\nabla \varphi = 0\quad\text{for all $\varphi \in H^1_0(\Omega)$.}$$ How do I reconcile this with what I think should be the weak form $\text{(1)}$? Notice the different spaces the test functions lie in.","['sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
1088803,A better resource for vector calculus than Stewart?,"I took the Math GRE Subject Test last October and tried to relearn vector calculus via the current edition of Stewart's text. I thought, to put it lightly, that the exposition was atrocious and unmotivated, with too much of a focus on memorizing the equations needed to solve problems. I did do well on the calculus questions on the Subject Test [I think], but if I need to teach myself vector calculus again, I can't use Stewart to do it. What do you all recommend for a good rigorous, motivational text on vector calculus? I need the book to cover Green's Theorem [which I've noticed, for example, Kline's text does not cover]. [This is one of those times that I wished Spivak covered vector calculus.]","['multivariable-calculus', 'reference-request', 'vectors']"
1088826,Picard's theorem applied to $f^n + g^n =1$,"So I have the following problem. Part 1 is just to state Picard's theorem, so for that we have that any entire holomorphic function takes on every value with possibly one exception. Part 2 is to show that for $n\geq 2$ there are no nowhere vanishing and nonconstant entire functions $f, g$ such that $f^n+g^n=1$ The proof of that goes as follows. $f^n$ is entire, and $f^n=1-g^n$. Since $g\neq 0$, $g^n\neq 0$ hence $f^n \neq 1$ hence $f^n$ is an entire function omitting both $0$ and $1$, contradiction. The final part is assume that $n>2$. Find all the solutions $f$ and $g$ to $f^n+g^n=1$ in the ring of entire functions. The hint is to transform the problem to the setting of meromorophic functions  on the complex line, but I don't know how to proceed. Thanks for any help!","['complex-numbers', 'complex-analysis']"
1088848,Multiplication Operation,"I am a father of two young boys and I looks forward to exploring mathematics with them for as long as they will let me :-). I would really like for them to have a deeper understanding of mathematics than what I had when I was a young student. As I think about how I might approach some of the topics, there is one that remains particularly unclear to me to this day - the multiplication operation. Now I do not have a strong background in mathematics (e.g. never had a course in abstract algebra), so please forgive me if some things that I say are off - maybe  even way off. I have seen that there have been debates online as to what multiplication is, and how to teach it to students.  Often the discussion turns into interpretations of multiplication (e.g. repeated addition, scaling, etc.) but the discussions/debate from this approach seem to be fruitless. Other times properties of multiplication are discussed, but often the properties are the same as those found under different types of operations. Integer multiplication may be associative, but so is integer addition - leaving me no more informed about the unique and universal thread for the concept of multiplication. From my perspective, I am most confused by the many definitions for the multiplication operation depending on the type of objects of interest (real numbers, complex numbers, matrices, etc). I always think to myself, ""why would mathematics allow the same name to be associated with multiple definitions?"". It seems like there must be something that all the definitions must have in common. Surely, not just any binary operation on a set of objects can be labeled multiplication on a whim...or can it? So this is my question, is there a characterization of the multiplication operation that holds true for all operations labeled multiplication, that it is agreed on within the academic community, and is unique enough to be able to distinguish it from other operations (namely addition)? If so, please do share. And if not, how would you explain why the same term has various definitions in mathematics to students learning about operations like multiplication? From my limited mathematical knowledge, it appears that the only thing in common with different definitions of multiplication on different objects is that that they all rely on the use of the addition operation in their construction. So perhaps the term addition is used to reference an operation for a set of objects that is considered to be the simplest method for combining/connecting two objects in a set, and multiplication is a more complex method for  doing so (perhaps based on the use of simpler operations, like addition, already defined for the set). But, I would prefer that my discussion with my sons not rely on my experience. Hence, the reason for the post. Many thanks for taking the time to review my write up and I look forward to any insight that may be offered.","['soft-question', 'abstract-algebra', 'definition']"
1088853,Prove that a presheaf is a sheaf,"Let $X$ be a variety. Show that if $X$ is irreducible, then the constant abelian presheaf $\mathcal{F}$ with $\mathcal{F}(U)=\mathbb{Z}$ for every nonempty open subset $U\subseteq X$ and $\mathcal{F}(\emptyset)=0$ is a sheaf. Any leads? What does the word ""constant"" means here?",['algebraic-geometry']
1088936,Prove that if a set A of natural numbers contains $n_0$ and whenever A contains k it also contains k+1.,"Prove that if a set A of natural numbers contains $n_0$ and that whenever A contains k it also contains k+1. Prove that A contains all natural numbers $ \geq n_0 $ This is rather similar to a question already on this site but its not quite the same and the only answer to that question is incomplete. So if someone could tell me if this proof is OK i would appreciate it. I could not for the life of me figure out how to turn this into an induction proof i think it may be possible to use strong induction on how i set this up to prove it or prove it by induction in a different way but i couldn't figure it out for the life of me even with the hint from the similar problem. Case 1) In Case 1 set A=D but in D we shall define $n_0 =1 $ Now since $ 1 \in D $ and k+1 is $\in D$  we know that $D= \mathbb{N}$ technically this is the only part of the proof that relies on induction and i couldn't even figure out how to prove that since $ 1 \in D$  that $ D= \mathbb{N}$ Case2) now we know that $n_0 > 1$ so i will define a set $A_1$ such that $ a\in A_1 \forall a \in A$ such that $a \geq n_0$  and a set B where $ 1\in B$ and $(k+1) \in B$ whenever $(k+1) < n_0$ Since $ A_1 \cup B = D $ Where D is the set from Case 1 and that this is true $\forall n_0 \in \mathbb{N} $ thus we know that $A_1 = \mathbb{N} - B$
Which by definition defines $A_1$ as the set of natural numbers $ \geq n_0$ lastly since $ A_1 \subset A$ ( as A could contain numbers smaller than $n_0$ ) We know that A contains all natural numbers $ \geq n_0 $","['induction', 'elementary-set-theory']"
1088938,Find this limit: $ \lim_{n \to \infty}{(e^{\frac{1}{n}} - \frac{2}{n})^n}$,"Can anyone help me with this limit: $$ \lim_{n \to \infty}{\left(e^{\frac{1}{n}} - \frac{2}{n}\right)^n}$$ I think that I should expand $e^{\frac{1}{n}}$ as the sequence $1 + \frac{1}{n} + \frac{1}{n^{2}2!} + \frac{1}{n^{3}3!} + \dots $.
But I can't see how it helps me. Edit: or maybe use logarithm?","['sequences-and-series', 'real-analysis', 'limits']"
1088944,"$E$ measurable set and $m(E\cap I)\le \frac{1}{2}m(I)$ for any open interval, prove $m(E) =0$","Ran across this problem and need some help. Let $E$ be a measurable subset of the real numbers and suppose that for any open interval $I$ one has $m(E\cap I)\le \frac{1}{2}m(I)$, where $m$ is the Lebesgue measure. Prove that $m(E)=0$.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
1088964,An Additional Rule for Calculus,"Background The rules for differentiating elementary functions (arithmetic, exponential, trigonometric, etc.) together with the chain rule for differentiating compositions of functions are often considered the only basic inference tools needed to explicitly compute the derivatives of typical functions thrown at a beginning calculus student. Yet, an easy counterexample is $f(x) = x^x$ , exploiting the fact that exponentiation is really a function of two variables. Then implicit / logarithmic differentiation techniques are typically introduced. I had been curious in my calculus days about an explicit method to compute the derivative of $f$ ; what I came up with I haphazardly dubbed the independence trick : group all appearances of the independent variable into several partitions. Then, the derivative of the function is a sum, each term being a ""partial"" derivative of the function with respect to one partition, treating each instance outside of the partition as a constant. Rigorously, this is an easy consequence of the multivariate chain rule: $$ \left.\frac{d}{dt} f(x_1,\ldots,x_n)\right|_{(x_1,\ldots,x_n) = (x,\ldots,x)} = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(x,\ldots,x) \frac{\partial x_i}{\partial t}~~, $$ yielding the mentioned rule because each $\partial x_i / \partial t = 1$ , as the variables are identical. There are a few results nontrivial to a student without multivariate calculus: Directly, $\frac{d}{dx} x^x = x x^{x-1} + x^x \ln x = x^x (\ln x + 1)$ . A one-line proof extending the Leibniz integral rule to its general form with variable limits. Questions Are there other nice applications for the ""independence trick"" in single-variable calculus? Does anybody know if this method has been written / used / taught specifically as a trick for undergrads, and if so, where?","['calculus', 'reference-request']"
1088976,"$y=\frac{1}{x}$, show that $\frac{dy}{dt}=\frac{-1}{x^2}\frac{dx}{dt}$ and find $\frac{d^2y}{dt^2}$","If $y=\frac{1}{x}$, where $x$ is a function of $t$ show that $\frac{dy}{dt}=\frac{-1}{x^2}\frac{dx}{dt}$ and find an expression for $\frac{d^2y}{dt^2}$. For $\frac{d^2y}{dt^2}$ I keep getting $$\frac{d^2y}{dt^2}=\frac{-1}{x^2}\frac{d^2x}{dt^2}+\frac{2}{x^3}\frac{dx}{dt}$$ but the correct answer is apparently $$\frac{d^2y}{dt^2}=\frac{-1}{x^2}\frac{d^2x}{dt^2}+\frac{2}{x^3}\left({\frac{dx}{dt}}\right)^2$$",['ordinary-differential-equations']
1089001,Area of a circle inside a quarter circle,I'm trying to figure out a couple things. The main question I have is how to find the area of a circle inscribed inside a quarter circle with a radius of x. The secondary question to that is if the radius of the inner circle drawn to touch the tangent lines of the sides of the quarter circle bisects the sides of the quarter circle. I hope that makes sense...,"['geometry', 'tangent-line', 'circles', 'area']"
1089004,Expectation of product of two random variables,"Let $X,Y$, two random variables which are indicators. Lets assume $P(X=1)=p$ and $P(Y=1)=q$ for some $0 \le p,q \le 1$. I've understood that: $E[XY] = P(X=1, Y=1)$. How to show it? $$E[XY] = \sum_{i=0}^1 XY\cdot Pr(?)$$ I guess it should be $Pr(X=i, Y=i)$ and of course, when $i=0$ the all expression equals to $0$. But, why is it $Pr(X=i, Y=i)$? I'd be glad for both algebraic and intuitive explanation. Thanks!","['probability', 'random-variables', 'expectation']"
1089015,Axes-intersections of normal tangents to an ellipse,"Question: What values can $x_T$,$y_T$,$x_N$, and $y_N$ take on? Let $T$ and $N$ be the tangent and normal lines to the ellipse $\frac{x^2}{9} + \frac{y^2}{4} = 1$ at any point on the ellipse in the first quadrant. Let $x_T$ and $y_T$ be the $x-$ and $y$-intercepts of $T$ and $x_N$ and $y_N$ be the intercepts of $N$ As $P$ moves along the ellipse in the first quadrant(but not on the axes), what values can $x_T$,$y_T$,$x_N$, and $y_N$ take on? What I've managed thus far Here is a graph of one of the scenarios, Slopes of lines T and N: Slope of $T$ is equal to $y\prime = -\frac{4x}{9y}$ Slope of $N$ is equal to $-\frac{1}{y\prime} = \frac{9y}{4x}$ Line equations for lines $T$ and $N$: Line $T:\quad y - y_T = -\frac{4x}{9y}\Big(x-x_T\Big) \Leftrightarrow y = -\frac{4x^2}{9y} + \frac{4xx_T}{9y} + y_T$ Line $N:\quad y - y_N = \frac{9y}{4x}\Big(x - x_N\Big) \Leftrightarrow y = \frac{9y}{4} - \frac{9yx_N}{4x} + y_N$ Now I will take a few limits on the line equations of $T$ and $N$: Limits on $T$ $\lim_{x \to 0} (y) = \lim_{x \to 0}\Big(-\frac{4x^2}{9y} + \frac{4xx_T}{9y} + y_T\Big)$ That gives me $$\lim_{x\to 0} y_T = 2$$ Multiplying line $T$'s equation by $9y$, I obtain the following: $$\lim_{x\to 3} x_T = 3$$ Limits on $N$ $\lim_{x \to 3} (y)  = \lim_{x \to 3} \Big(\frac{9y}{4} - \frac{9yx_N}{4x} + y_N\Big)$ That gives me, $$\lim_{x \to 3} y_N = 0$$ Multiplying line $N$'s equation by $4x$, I obtain the following: $$\lim_{x\to 0} x_N = 0$$ That's where I'm stuck, I still need to find $\lim_{x\to 0} x_T$, $\lim_{x\to 0} y_N$, $\lim_{x\to 3} y_T$, and $\lim_{x\to 3} x_N$. That is, at the moment I have partial ranges for $x_T$, $y_T$, $x_N$, and$y_N$, as follows $$x_T(3, \lim_{x\to 0} x_T)$$ $$y_T(2, \lim_{x\to 3} y_T)$$ $$x_N(0, \lim_{x\to 3} x_N)$$ $$y_N(0, \lim_{x\to 0} y_N)$$ Does anybody have any hints, suggestions, or alternative approaches?","['calculus', 'derivatives', 'limits']"
1089043,Convergence of $\int_0^\infty $sin$ (x^p) dx$,"Consider the $\displaystyle \int_0^\infty $sin$ (x^p) dx$. Does it converge when $p<0$? Does it converge when $p>1$? My Work: Let $x^p=y$, then $\displaystyle \int_0^\infty $sin$ \displaystyle (x^p) dx=\frac{1}{p}\sum_{n=1}^\infty \int_{(n-1)\pi}^{n\pi} \frac{\text{sin}  \; y}{y^r} dy$ , where $r=\frac{p-1}{p}>1 $when $p<0$ and $0<r=\frac{p-1}{p}\leq 1 $when $p>1$. I know that $\displaystyle \int_0^\infty $sin$ (\frac{1}{x}) dx$ diverges which is a special case of first case. But failed to show it generally. Can anyone please give me a hint to preceed?","['improper-integrals', 'convergence-divergence', 'integration', 'real-analysis']"
1089060,Equality case in elementary form of Holder's Inequality,"A well known elementary formulation of Holder's Inequality can be stated as follows: Let $a_{ij}$ for $i = 1, 2, \dots, k; j = 1, 2, \dots, n$ be positive real numbers, and let $p_1, p_2, \dots, p_k$ be positive real numbers such that $\sum_{i=1}^{k} \frac{1}{p_i} = 1$.  Then we have $$\sum_{j = 1}^{n} \prod_{i = 1}^{k} a_{ij} \leq \prod_{i=1}^{k} \left ( \sum_{j = 1}^{n} a_{ij}^{p_i} \right )^{\frac{1}{p_i}}.$$ Does anyone know when equality occurs in this inequality?  Any insights on the equality case and/or a proof would be appreciated. I have seen several proofs of the two sequence case.  Here is one, from Cvetkovski's Inequalities : Perhaps this argument can be generalized to prove the above result.  Thanks in advance!","['inequality', 'calculus', 'algebra-precalculus', 'optimization']"
1089070,"The ""find my car"" problem: proper interpretation and solution?","This has been asked at least twice here, but both questions have accepted answers which are wrong. I don't really know any good way to draw attention to the question besides asking again and being a bit more careful about not accepting incorrect or incomplete answers, so here goes. Say I am standing at the origin of the real line, and I know my car is somewhere on the real line. The PDF is a normal curve. I want to search for it optimally walking at a fixed pace. The problem is, if I look right and it really was left, I'm going to, at some point, have to back track to find it. So let's see if we can model this more formally. The set of times is $\mathbb R^+$, so I want a continuous, rigid (so that one ""second"" becomes one ""unit"") transformation $f$ with $f(0) = 0$. A few definitions: Let $S_f(t)$ (success of $f$) be the probability that the car is found by time $t$. It equals $\int_{\min f}^{\max f}N(\mu, \sigma)(x)dx$, Where the $\min$ and $\max$ are taken on $[0,t]$. So it's the amount of area under the normal curve that is covered. For a fixed $x \in \mathbb R$ where the car might be, let $T_f(x)$ be the amount of time it takes to find $c$. This is $\min \{t: f(t) = c\}$. Define $P_f(p)$ for $0 < p < 1$ to be the amount of time it will take to have found the car with probability $p$. That is, $P_f(p) := \min \{t \in \mathbb R^+ : S_f(t) \geq p \}$. I think there are a few valid interpretations of this problem: For a fixed amount of time $t$, find $f$ to maximize $S_f$. I don't think $f$ is independent of $t$, because backtracking takes time, and taking a function that backtracks and telling it, ""you don't have time to backtrack all the way back,"" would tell it not to backtrack at all. Find an $f$ whose expected value of $T_f$ where $x$ is normally distributed is smallest. I think this is closest to the real world problem. For a fixed probability $p$, find $f$ such that $P_f(p)$, which returns an amount of time, is minimal. Let's put these together. Say you're willing to abort once you have reached a $p$ chance of finding the car. This models reality in the Bayesian sense that once I should have found my car with probability $99.99\%$ by now, then maybe my model needs to be revised and I should look on another street. Consider $J_f(x) := \min(T_f(x), P_f(p))$. Now find $f$ such that the expected value of $J_f$ is smallest over normally distributed $x$. So the first question is whether I interpreted this question correctly. The second is how to solve any of them.","['searching', 'measure-theory', 'probability']"
1089078,Encode order of playing cards (data compression),"Suppose we have a deck of cards, shuffled in a random configuration. We would like to find a $k$-bit code in which we explain the current order of the cards. This would be easy to do for $k=51 \cdot 6=306$, since we could encode our deck card-by-card, using $2$ bits for the coloring and $4$ bits for the number on each card. We would like to optimalise this code. There exist $52!$ ways to arrange our deck, and so $k$ will have to be at least: $\lceil\log(52!)\rceil=226$. I'm asked to find a code for a $k$-value halfway between $306$ and $226$. I understand that my code cannot work card-by-card, since there exist $52$ different cards and $\lceil \log(52)\rceil=6$. Therefore any card-by-card codation will lead to $k\geq306$. Therefore I concluded my strategy should encode blocks of cards, another idea I had was to encode the colouring first and the numbers second. Could anyone give me a hint about where to go from here?","['card-games', 'combinatorics']"
1089080,"If a differentiable function approaches $-\infty$ as a limit from the positive side, must its derivative simultaneously approach $\infty$?","Can we say that if $g: (0, \infty)\rightarrow\Bbb{R}$ is a differentiable function and $\lim \limits_{x \to 0+}g(x)= -\infty$, then  $\lim \limits_{x \to 0+}g'(x)= +\infty$ is always true? I think the statement is true, I tried many functions, one of them is $-1/x$ for example, and I always get a true result. if its not true then I need a negative example, and if not, i really need some help to prove this formally.","['calculus', 'derivatives', 'real-analysis']"
1089098,"if a point is a root of all linear functionals on a normed space, then it's zero","I've been trying to do some work as literal and detailed as possible in order to see I know my analysis for every detail. I tried to explain my self why if there is $x_0\in X$ ($X$ is a normed space) such that $\forall\varphi\in X^*:\:\varphi(x_0)=0$ then $x_0=0$. I could justify it using some high theorems, like consequences of the hahn-banach theorem, but I really don't feel like this is necessary and I would love a farily easy explanation if anyone is familiar with. If you feel like all the reasoning for this claim do require these kind of theorem please let me know that. Thanks a lot","['normed-spaces', 'functional-analysis', 'real-analysis']"
1089124,Elements of the zero-th Čech cohomology group versus global holomorphic sections,"Something that is confusing (well, to me) has come up in the course of asking other questions. Let $\pi:V\to X$ be a holomorphic vector bundle of rank $r$ on a complex manifold $X$, such that $V$ is defined by an open cover $\{U_\alpha\}$ and holomorphic transition functions $g_{\alpha\beta}:U_\alpha\cap U_\beta\to\mbox{GL}(r,\mathbb C)$. If $s:X\to V$ is a global holomorphic section of $V$, then the local data $s_\alpha:U_\alpha\to\mathbb C^r$ of $s$ must glue together as $s_\beta=g_{\alpha\beta}s_\alpha$ over the overlaps $U_\alpha\cap U_\beta$. Global holomorphic sections are also said to be elements of $H^0(X;\mathcal O(V))$, where $H^i$ is Čech cohomology and $\mathcal O(V)$ is the sheaf of holomorphic sections of $V$. I'm going to assume that the cover $\{U_\alpha\}$ is a ""good"" cover, for Čech purposes. Literally, $H^0(X;\mathcal O(V))=\mbox{ker}[C^0(\mathcal O(V))\stackrel{\delta}{\longrightarrow}C^1(\mathcal O(V))]$, where $\delta$ is the Čech coboundary operator, and $C^p(\mathcal O(V))$ is the vector space of $0$-cochains for $\mathcal O(V)$ with respect to $\{U_\alpha\}$. Hence, a section $s$ of $V$ is in $H^0(X;\mathcal O(V))$ if and only if $(\delta s)_{\alpha\beta}=s_\beta-s_\alpha=0$, which says that $s_\alpha$ and $s_\beta$ must agree on the nose over $U_\alpha\cap U_\beta$. This is different than saying that $s_\beta=g_{\alpha\beta}s_\alpha$ on the overlap. Having $s_\alpha=s_\beta$ seems to be a very strong restriction on a section.  The existence of such a section suggests that $g_{\alpha\beta}=I_r$ on each overlap, and hence $V$ would be trivial. But for instance: the holomorphic line bundle $\mathcal O(1)$ (i.e. hyperplane bundle) on $\mathbb{CP}^1$ is non-trivial, and at the same time is supposed to have $H^0(\mathbb{CP}^1;\mathcal O(1))\cong\mathbb{C}^2$. Something is amiss somewhere in my thinking.  Can someone please help?","['vector-bundles', 'complex-geometry', 'algebraic-geometry', 'holomorphic-bundles', 'sheaf-cohomology']"
1089145,Prove the existence of an entire function $f$ such that $f(z)=\sin^2(\sqrt{z})$,"Prove there is an entire function $f$ such that for any branch $g$ of $\sqrt{z}$, $$\sin^2(g(z))=f(z)$$
  for all $z$ in the domain of definition of $g$. I don't know how to overcome the fact that $g$ is not defined everywhere on the complex plane. Thanks for any help.",['complex-analysis']
1089180,"Where f is a function, does $f ab = f(a)b~ \text{or}~ f(ab)$?","I know $\sin ab = \sin(ab)$, but does this apply to other functions?","['notation', 'functions']"
1089185,Topological covering + local diffeomorphism gives smooth covering,"I got stuck at some point while working on this part of an exercise from Lee's Introduction to Smooth Manifolds, 2nd edition. The part which I am stuck on is to prove (one of the directions of proposition 4.33(c)): A topological covering map is a smooth covering map if it is a local diffeomorphism. What I have is as follows. Let $\pi : E \to M$ be a topological covering map. Let $q\in M$. By assumption, we have some open subset $V \subset M$ and collection of disjoint open subsets $\{ U_\alpha \}$ of $E$ such that each $U_\alpha$ is homeomorphic to $V$ under $\pi$. Further, let $p_\alpha \in U_\alpha$ be such that $\pi(p_\alpha)=q$. Now, for each $\alpha$, I can find open neighbourhoods $\hat{U}_\alpha$ of $p_\alpha$ and $\hat{V}_\alpha$ of $q$ such that $\hat{U}_\alpha$ and $\hat{V}_\alpha$ are diffeomorphic. We may demand that $\hat{U}_\alpha \subset U_\alpha$. The problem is I need one single neighbourhood of $q$ that lifts diffeomorphically, but all I have is the collection $\hat{V}_\alpha$ and taking their intersection does not necessarily give an open set. How should I correctly approach the problem? An alternative idea I have: Repeat the first three lines of the first proof. Now for some $\alpha '$, define $\hat{U}_{\alpha '}$ and $\hat{V}_{\alpha '}$ as above. Then lift $\hat{V}_{\alpha '}$ using $\pi^{-1}$ to all other $U_\alpha$ to obtain $\hat{U}_\alpha$. This time I know that each $\hat{U}_\alpha$ is smoothly homeomorphic to $\hat{V}_{\alpha '}$.","['covering-spaces', 'smooth-manifolds', 'differential-geometry']"
1089191,How to prove Cauchy-Schwarz integral inequality?,"The Cauchy-Schwarz integral inequality is as follows: $$
\displaystyle \left({\int_a^b f \left({t}\right) g \left({t}\right) \ \mathrm d t}\right)^2 \le \int_a^b \left({f \left({t}\right)}\right)^2 \mathrm d t \int_a^b \left({g \left({t}\right)}\right)^2 \mathrm d t
$$ How do I prove this using multivariable calculus methods, preferably with double integrals?","['multivariable-calculus', 'integral-inequality']"
1089193,Every reflection is an isometry proof,"The theorem is that every reflection $R_{S}$ in an affine subspace $S$ of $\mathbb{E}^{n}$ is an isometry: $R_S:\ \mathbb{E}^{n} \rightarrow \mathbb{E}^{n}:\ x \mapsto R_{S}(x) = x + 2 \overrightarrow{x\pi_{S}(x)}$ I'm horrendously stuck with the proof.
I get that I'm trying to prove that $R_{S}$ preserves the distance between the two points, but I'm lost as to how. EDIT: $\pi_{S}(x)$ is defined as the intersection of $S$ and the euclidean subspace $T_x$ through $x$, perpendicular to $S$.","['affine-geometry', 'geometry', 'reflection', 'euclidean-geometry']"
1089224,Is $B = \{x:x \notin B\}$ a valid paradox in Naive Set Theory?,"The version of Cantor's notion of sets that I've come across goes something like this: ""...collection of well defined, distinguishable objects of our intuition or of our thought to be conceived of as a whole. The objects are called the members of the set..."" With Russell's paradox $B = \{x:x \notin x\}$, I understand the mistake is assuming the collection $B$ is a set (i.e. if by sets we mean a collection which has the membership relation with its elements). The paradox shows not all collections are sets. So far I haven't seen any paradox phrased like this: $B = \{x:x \notin B\}$? It seems slightly different from Russell's paradox in that the question isn't so much about whether $B$ is a collection which is also a set, but whether $B$ is a collection at all. Is this formulation allowed in Cantor's notion of sets, where it must satisfy the criterion of being  well defined? Thanks!","['elementary-set-theory', 'paradoxes']"
1089229,Is $123456788910111121314\cdots$ a $p$-adic integer?,"On the back of this question comes the natural question of whether the string
$$1234567891011121314\!\cdots$$
is even a number at all. While that sort of question is vague, given the lack of generic definition for the word ""number"", I would feel comfortable answering this question in the affirmative if we knew that it were a $p$-adic number. Now, my first impressions are that this number is $10$-adic integer (although it is not quite as easy to show this as I initially thought). However, it seems rather unlikely that it is $p$-adic for any prime $p$. Does anyone know how to show that it is or isn't $p$-adic, or if there are similar questions which have been answered— I mean, the $p$-adicity of strings like $2481632\!\cdots$ or $23571113\!\cdots$? (Sidebar: I believe that if it is a $p$-adic number, it has to be a $p$-adic integer, but I admit that I could be mistaken here.) EDIT: mixedmath's answer shows that if the question is interpreted with $1$ as the ""leftmost digit"" then the question makes no sense. However, KCd points out that this notation is frequently used when we intend $1$ to be the ""rightmost digit"". In this case, the question becomes formalizable (probably), and almost surely much more interesting. Therefore, the question becomes whether or not the following $10$-adic integer is $p$-adic for some prime $p$: $$\sum_{k=0}^\infty k\exp_{10}\left(k+\sum_{i=1}^k\lfloor\log(i)\rfloor\right)$$ where $\exp_{10}(x)$ is a [formal] power $10^x$. This probably still doesn't make perfect sense formally, but it is at least easy to imagine formalizing it. We might reasonably interpret it to be a statement about the existence of sequences $b_m\in\Bbb Z_p$ and $e_m,\, f_m,\, g_m\to\infty$ such that the finite sums agree up to a point: $$\sum_{m=0}^{f_N} b_m p^m \equiv \sum_{k=0}^{g_N} k\exp_{10}\left(k+\textstyle\sum\lfloor\log(i)\rfloor\right) \qquad \text{mod} \exp_{10}(e_N)$$ for all $N\in\Bbb N$. Perhaps something regarding rearrangements would work a little bit better (for this formalism I worry about some inessential objections regarding instability of the ones digit). If anyone would like to answer this other interpretation of the question, I would give a bounty for it.","['p-adic-number-theory', 'number-theory']"
1089274,How prove this $\{a\}\cdot\{b\}\cdot\{c\}=0$ if $\lfloor na\rfloor+\lfloor nb\rfloor=\lfloor nc\rfloor$,"Interesting problem Let $a,b,c$ be real numbers such that
  $$\lfloor na\rfloor+\lfloor nb\rfloor=\lfloor nc\rfloor$$
  for all postive integers $n$.
  Show that:
  $$\{a\}\cdot\{b\}\cdot\{c\}=0$$
  where $\{x\}=x-\lfloor x\rfloor$ My partial work: since for any postive intger $n$,have
$$\lfloor na\rfloor+\lfloor nb\rfloor=\lfloor nc\rfloor$$
then we have
$$a+b=c$$
because consider
$$\Longrightarrow \lim_{n\to\infty}\dfrac{\lfloor na\rfloor+\lfloor nb\rfloor}{\lfloor nc\rfloor}=1$$
since
$x-1<\lfloor x\rfloor \le x$,so we have
$$na+nb-2<\lfloor na\rfloor+\lfloor nb\rfloor\le na+nb$$
so
$$1=\lim_{n\to\infty}\dfrac{\lfloor na\rfloor+\lfloor nb\rfloor}{\lfloor nc\rfloor}=\dfrac{a+b}{c}$$
I guess we can use by contradiction prove this. Assume that
$\{a\}\cdot\{b\}\cdot\{c\}\neq0$, then $a,b,c$ all not integer.","['ceiling-and-floor-functions', 'number-theory']"
1089286,An nth-order ODE has n linearly independent solutions,"MathWorld states: ""In general, an $n^\text{th}$-order ODE has $n$ linearly independent solutions"". Are they referring to linear ODEs? I only know why it should be true for ODEs with constant coefficients, by the following observations: The solutions to the differential equation $a_0f+\dots +a_nf^{(n)}=0$, where $a_n\ne 0$ form a vector space $V$ (check). Let $f\in C^n(\mathbb{R})$ s.t. $a_0f+\dots +a_nf^{(n)}=0$, where $a_n\ne 0$. Let $\vec{a}=(a_0,a_1,\dots,a_{n-1})$ and $\vec{f}=(f,f^{(1)},\dots,f^{(n-1)})$. $f^{(n)}=-a_n^{-1}(a_0f+\dots +a_{n-1}f^{(n-1)})=-a_n^{-1}\vec{a}\cdot\vec{f}$ is differentiable, and the $m^\text{th}$ derivative of $\vec{b}\cdot\vec{f}$ is: $$\vec b\left(\matrix{\vec{e_2}\\\vdots\\\vec{e_n}\\-a_n^{-1}\vec{a}}\right)^m\cdot \vec{f}$$ Hence $f$ is infinitely differentiable. Moreover, the coefficients above are bounded above by an exponential in $m$. For any closed interval $[-d,d]$, $\vec{f}$ is continuous and therefore bounded. This means that the Taylor series for $f$ converges to $f$ in the interval by Taylor's theorem for the expansion about $x=0$ (using the Lagrange form of the remainder on the whole interval). Hence the Taylor series for $f$ about $x=0$ converges to $f$ for all $\mathbb{R}$, i.e. $f$ is analytic. Now consider the linear transformation $L:V\to\mathbb{R}^n,f\mapsto (f(0),f^{(1)}(0),\dots,f^{(n-1)}(0))$. To prove surjectivity, use the differential equation to produce a Taylor series and show that it is a solution. Injectivity is proven by the below: If $L(f)=L(g)$ for some solutions $f,g$, then $\forall k=0,1,\dots,n-1, f^{(k)}(0)=g^{(k)}(0)$, and by the differential equation, this also holds for all $k\in\mathbb{N}$. $f$ and $g$ are analytic and since the Taylor series is unique, $f=g$. Hence, $V$ has dimension $n$. Is my proof correct? Is the theorem for general linear ODEs true, and how do I prove it?","['linear-algebra', 'ordinary-differential-equations', 'proof-verification']"
1089289,Finding a value from 5 systems of equations of 5 variables(CHMMC 2014),"$$\text{For } a_1\cdots a_5\in \mathbb{R},$$
$$\frac{a_1}{k^2+1}+\cdots+\frac{a_5}{k^2+5}=\frac{1}{k^2}$$
$$\forall k=\{2,3,4,5,6\}$$
$$\text{Find }\frac{a_1}2+\cdots+\frac{a_5}6$$
The Provided explanation/official solution was
$$\text{Solution }1:\large{\frac{65}{72}}$$
Please don't mark this as off-topic, I have no clue where to start. Clearly it would not be feasible to compute $a_1\cdots a_5$.","['linear-algebra', 'systems-of-equations', 'contest-math']"
1089291,Another Monty Hall Question,"I still do not believe the ""correct"" solution to the Monty Hall Problem. Here is my reasoning:
The player can pick from $1$ of $3$ doors.
The prize can be behind $1$ of $3$ doors.
Monty will open $1$ of $3$ doors. $3 \times 3 \times 3 = 27$ possible sequences of events. In $15$ of those possible events, Monty either opens the door the player picked or the door with the prize.  Since Monty will not do either of those, these $15$ events are removed from the possibilities. Of the remaining $12$ possibilities, $6$ times the player wins if he stays with his original pick and $6$ times he wins if he switches.  It looks to me like the player has the same chance of winning whether he stays or switches. Could someone please explain the flaw in my reasoning.","['monty-hall', 'probability']"
1089301,"Prove: If $a^2+b^2=1$ and $c^2+d^2=1$, then $ac+bd\le1$","Prove: If $a^2+b^2=1$ and $c^2+d^2=1$, then $ac+bd\le1$ I seem to struggle with this simple proof. All I managed to find is that ac+bd=-4 (which might not even be correct).","['inequality', 'algebra-precalculus']"
1089318,Do we have $-1\bmod 2 \equiv -1$ or $+1$?,"As far I can calculate $-1 \bmod 2 \equiv -1$, but the software I am using (R) is telling that $-1 \bmod 2 \equiv +1$. This is the R code: -1%%2
[1]1 Which is correct?","['modular-arithmetic', 'self-learning', 'algebra-precalculus']"
1089351,Is $f(x) = \dfrac{x^2 -4x + 4}{x - 2}$ the same as $g(x) = x - 2 $?,"Is $$f(x) = \dfrac{x^2 -4x + 4}{x - 2}$$ the same as $$g(x) = x - 2?$$
Why yes? Why not?","['rational-functions', 'algebra-precalculus']"
1089383,How to solve given expression?,"We know that the derivative  $f'(1)=3$. $$
 \lim_{h \to 0} \frac{f(1-5h^2)-f(1+3h^2)}{h^2(h+1)}=?
$$ I try to solve it by applying L'Hôpital's rule, but answer was incorrect. Since $f$ is differentiable at $x=1$, it has Continuity, so its right and left limit at $x=1$ are equal. So the numerator and denominator are zero. I think we can use L'Hôpital's rule. With given $f'(1)=3$
$$
 \lim_{h \to 0} \frac{f(1-5h^2)-f(1+3h^2)}{h^2(h+1)}=\frac {0}{0},
$$ $$
 \lim_{h \to 0} \frac{-10hf(1-5h^2)-6hf(1+3h^2)}{3h^2+1}=\frac {-16 h f'(1)}{3 h ^ 2 +1}=\frac {0}{1}=0,
$$
$$
{h \to 0} \implies f'(1-5h^2)=f'(1+3h^2).
$$ What is wrong with this answer?","['derivatives', 'functions']"
1089458,How can I prove the convergence of a power-tower? [duplicate],"This question already has answers here : Convergence of tetration sequence. (2 answers) Closed 9 years ago . In here, I saw that $$x^{x^{x^{x^{x^{x^{x^{.{^{.^{.}}}}}}}}}}$$ exists as a real number (convergent) if and only if $$x\in[e^{-e}, e^\frac{1}{e}].$$ How can I prove this??","['convergence-divergence', 'calculus']"
1089474,Eigenvalue of a matrix that the sum of each column is equal?,"Let $A$ be an $n \times n$ matrix. i) Prove that if the sum of each row of $A$ equals $s$, then $s$ is an eigenvalue of $A$. ii) Prove that if the sum of each column of $A$ equals $s$, then $s$ is an eigenvalue of $A$. The first question isn't a problem, but I've been banging my head against the wall for an hour trying to find the answer to the second one. I'd really appreciate some help.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
1089479,"A question on Mumford's drawing of $\text{Spec}\,\mathbb{Z}[x]$","This might seem like a really silly question, but what are those weird curves connecting $(x^2 + 1)$ and $(5, x+2)$ in Mumford's picture of $\text{Spec}\,\mathbb{Z}[x]$?","['geometry', 'ring-theory', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
1089494,Application of Stoke's Theorem: Wrong parametrization?,"Show that the path $\mathbf{x}(t)=( \cos t, \sin t, \sin 2t )$ lies on the surface $z=2xy$. Evaluate $$\oint_C (y^3+ \cos x)\, dx+(\sin y+z^2)\, dy+x\,dz$$ where $C$ is the closed curve parametrized and oriented by the path
  $\mathbf{x}$ in 1. Question $1$ is trivial. It is pure substitution and double-angle formula. I am stuck with question $2$. I think it should be an application of the Stoke's Theorem: $$ \iint_S \nabla \times \mathbf{F}\cdot d\mathbf{S}=\oint_C \mathbf{F}\cdot d\mathbf{s}$$ where $S$ is the surface bounded and $C$ is the corresponding boundary. Now, $$\nabla \times \mathbf{F}=(2z, -1, -3y^2)$$ and then I parametrize the bounded area: $\mathbf{X}(s,t)=(s \cos t, s \sin t, 2s^2 \sin t \cos t)$ where $0 \leq s \leq 1, 0 \leq t \leq 2\pi$. But it leads me to a messy looking vector surface integral. I suppose it is a wrong way to tackle this question. I also try to solve this vector surface integral without parametrizing the surface. I then find $$ \iint_S \frac{-8xy^2 + 2x -3y^2}{\sqrt{4y^2+4x^2+1}}\,dS.$$ Again, messy! Any help will be appreciated!","['multivariable-calculus', 'integration']"
1089593,How to solve $\frac{dy}{dx}=\cos(x-y)$?,How to solve $\dfrac{dy}{dx}=\cos(x-y)$ ? How do I separate x and y here ? Please advise.,"['ordinary-differential-equations', 'calculus']"
1089601,What are other methods to Evaluate $\int_0^{\infty} \frac{y^{m-1}}{1+y} dy$?,"I am looking for an alternative method to what I have used below. The method that I know makes a substitution to the Beta function to make it equivalent to the Integral I am evaluating. Usually we start off with the integral itself that we are evaluating (IMO, these are better methods)  and I would love to know such a method for this. Also, I would be glad to know methods which uses other techniques that I am not aware, which does not necessarily follow (1) $$\Large{\color{#66f}{B(m,n)=\int_0^1 x^{m-1} (1-x)^{n-1} dx}}$$ $$\bbox[8pt,border: 2pt solid crimson]{x=\frac{y}{1+y}\implies dx=\frac{dy}{(1+y)^2}}$$ $$\int_0^{\infty} \left(\frac{y}{1+y}\right)^{m-1} \left(\frac{1}{1+y}\right)^{n-1} \frac{dy}{(1+y)^2}=\int_0^{\infty} y^{m-1} (1-y)^{-m-n} dy$$ $$\Large{n=1-m}$$ $$\Large{\color{crimson}{\int_0^{\infty} \frac{y^{m-1}}{1+y} dy=B(m,1-m)=\Gamma(m)\Gamma(m-1)}}$$ Thanks in advance for helping me expand my current knowledge.","['definite-integrals', 'calculus', 'integration']"
1089617,The group $E(\mathbb{F}_p)$ has exactly $p+1$ elements,"Let $E/\mathbb{F}_p$ the elliptic curve $y^2=x^3+Ax$. We suppose that $p \geq 7$ and $p \equiv 3 \pmod {4}$. I want to show that the group $E(\mathbb{F}_p)$ has exactly $p+1$ elements. I was wondering if we could use the rank of the group.. Do you have an idea? EDIT : There are the following possibilities: If the point $(x,y)$ is in $E(\mathbb{F}_p)$, then the point $(x,-y)$ is also in $E(\mathbb{F}_p)$. If the point $(-x,y)$ is in $E(\mathbb{F}_p)$, then the point $(-x,-y)$ is also in $E(\mathbb{F}_p)$. $y^2=f(x)$ $f(x)=x^3+Ax \Rightarrow f(-x)=-f(x)$ Let $(x,  y)$ be a point in $E(\mathbb{F}_p)$. Then $f(x)$ is a square. If $(-x,  y)$ would also be a point in $E(\mathbb{F}_p)$, then $f(-x)$ would also be a square, and then $-f(x)$ would be a square, so $-1$ would be a square. That cannot be true, since $p \equiv 3 \pmod 4$. Is this correct? But I still don't understand how we can count the solutions. Could you explain it to me?","['algebraic-geometry', 'group-theory', 'abstract-algebra', 'elliptic-curves']"
1089632,Regarding proof of converse to Girsanovs theorem,"This is regarding an argument from Arbitrage Theory by Thomas Björk - Theorem 11.6, but is attempted self contained. Consider a Wiener process W on probability space $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in[0,T]},P)$. Assume that the filtration is generated by W, and that $\mathcal{F}=\mathcal{F}_T$. Assume Q is a probability measure absolutely continuous with respect to P on  $\mathcal{F}_T$. Denote $L_T$ the Radon-Nikodym derivative and define $L_t = E[L_T \lvert \mathcal{F}_t]$ a martingale. By the martingale representation theorem we can find $g_t$ such that 
$$
dL_t = g_t dW_t
$$
If we define $\phi_t = \frac{1}{L_t} g_t$ this should suffice as a Girsanov transformation, showing that in the filtration of the Wiener process any absolutely continuous transformation is of a Girsanov type. The text says: ""There remains a small problem namely when $L_t=0$ but also this can be handled"". How does one handle that problem?","['stochastic-processes', 'radon-nikodym', 'probability-theory', 'stochastic-calculus', 'brownian-motion']"
1089635,How to show result in terms of $\pi$ in Mathcad?,"Is it possible to display a result in Mathcad as a function of $\pi$? I'm studying physics and I need to show exact results at the exams. I know I can set Mathcad to give me the result in decimals or in fractions, but none of them are good enough. Example: calculating 400/200 $\pi$ What mathcad can give me: $(400/200) \cdot  \pi  = 6.283$  or $(400/200) \cdot \pi  = 10838702/1725033 $ What I want: $(400/200) \cdot \pi  = 2 \pi$","['mathcad', 'discrete-mathematics', 'math-software']"
1089636,Exponential map is surjective for compact connected Lie group,"How do I show that for every compact connected group $G$, the exponential map $\exp \colon\mathfrak{g} \rightarrow G$ is surjective? I tried to find the proof on the internet but most of them are either just a short note or ""left as an exercise for reader"" with some hints like: use invariant inner product and existence of geodesic but I don't really understand. So if someone could point out where to find a complete proof of this or give me a more extensive hints on how to start the proof that would be great. Thank you!","['topological-groups', 'lie-algebras', 'lie-groups', 'differential-geometry']"
1089638,Harmonic Analysis on the Affine Group,"In my previous question , I asked about harmonic analysis on the group $\operatorname{SL}(3, \mathbb{R})$. The representation theory of this group appears to be quite complicated, so I am now looking at another group that is of interest in my application: the affine group $\operatorname{Aff}(2, \mathbb{R})$ consisting of matrices
\begin{equation}
  A = (g, t) =
 \begin{bmatrix}
   g_{11} & g_{12} & t_1 \\
   g_{21} & g_{22} & t_2 \\
   0 & 0 & 1
 \end{bmatrix}
\end{equation}
where $g \in \operatorname{GL}(2, \mathbb{R})$ and $t \in \mathbb{R}^2$. If it simplifies things, I would also be interested in the restriction to $\operatorname{GL}^+(2, \mathbb{R})$ of $2 \times 2$ real matrices with strictly positive determinant (unlike $\operatorname{GL}(2,\mathbb{R})$, this group has only 1 connected component). I would like to build an algorithm that performs the Fourier transform on this group, which is defined as 
$$
\hat{f}(\lambda) = \int_G f(g) U^\lambda(g^{-1}) d\mu(g)
$$
where $U^{\lambda}$ is an irreducible unitary representation of $\operatorname{Aff}(2, \mathbb{R})$, and $\mu$ is a left-invariant Haar measure on this group (which is not unimodular). Since we can only work with finitely sampled functions $f$, the way to go will be to assume that $f$ is ""band limited"", i.e. it is a linear combination of a finite number of matrix elements of irreducible representations. By increasing the number of matrix coefficients (the ""resolution"" of the algorithm), we can transform an increasingly rich class of functions. Questions As in my previous question, I'd like to know: Which IURs do I need in order to decompose a function in $L^2(G)$? I'm not set on this particular function space, so if another one is easier feel free to modify the question. I think this question amounts to finding a Plancherel theorem for this group. Are there any explicit formulas known for the matrix elements of the relevant IURs? What about the basis functions of irreducible representation spaces in $L^2(G)$? In both cases, integral representations are fine. Is anything known about the asymptotics of these functions? This is important because the IURs of $\operatorname{Aff}(2, \mathbb{R})$ are infinite-dimensional matrices, so an algorithm will have to choose some cutoff. If functions $U^\lambda_{mn}$ do not decay sufficiently fast (or at all) with $m,n$, that may be a showstopper. Strategy In response to my previous question, the user named guest mentioned that to deal with semidirect product groups (such as $\operatorname{Aff}(2,\mathbb{R}) = \mathbb{R}^2 \rtimes \operatorname{GL}(2, \mathbb{R})$), I should look at the concept of a system of imprimitivity. I have read a little bit on this topic, but do not yet understand how to apply it exactly. Intuitively I would think that Fourier analysis on the affine group should somehow ""decompose"" into Fourier analysis on its two factors, but I don't know if that is true or how to make this precise. Some guidance on how the system of imprimitivity concept helps to understand the representation theory of a semidirect product group would be very useful. Provided that we can indeed understand the representation theory of $\operatorname{Aff}(2, \mathbb{R})$ by understanding the representation theory of $\mathbb{R}^2$ and that of $\operatorname{GL}(2, \mathbb{R})$ and then assembling them, the main problem is then to understand $\operatorname{GL}(2, \mathbb{R})$. A lot is known concretely about the representation theory of $\operatorname{SL}(2, \mathbb{R})$, but I could not find much on $\operatorname{GL}(2, \mathbb{R})$. I think that $\operatorname{GL}(2, \mathbb{R}) = \operatorname{SL}(2, \mathbb{R}) \times \mathbb{R}^*$, so maybe there is an easy way to extend the representation theory of $\operatorname{SL}(2, \mathbb{R})$ to $\operatorname{GL}(2, \mathbb{R})$? I did find a paper by D. A. Vogan, ""The unitary dual of GL(n) over an archimedean field."", but (without having read the paper) it seems a bit abstract and does not treat the Plancherel formula. Thanks in advance!","['topological-groups', 'representation-theory', 'harmonic-analysis', 'lie-groups', 'group-theory']"
1089639,Tools to bound the singular values of a finite sum of random matrices from below?,"Matrix Chernoff bounds (see also this arXiv paper ) are usually used to give upper bounds on the largest eigenvalue of a finite sum of random matrices. Sometimes it can also be used to give a lower bound on the smallest eigenvalue of a finite sum of random positive semidefinite (PSD) matrices. For PSD matrices, the eigenvalues are the same as the singular values. My question is: Is there any tool, or any other special cases (apart from the PSD case) for which there is a tool, to obtain lower bounds on the singular values of a finite sum of random matrices, or random symmetrical matrices?","['inequality', 'calculus', 'concentration-of-measure', 'matrices', 'linear-algebra']"
1089648,Is sum and product of a infinite number of continuous functions are also continuous functions?,"Whether in Real Analysis or by Open Set Def of Continuity in Topology, it is easy to show that the sum and product of a FINITE number of continuous functions are also continuous functions. That is, assuming that $f_1, ..., f_m:\Bbb R\rightarrow\Bbb R$ are continuous, then $S:\Bbb R\rightarrow\Bbb R$ and $P:\Bbb R\rightarrow\Bbb R$, defined by $S(x) = f_1(x) + ... + f_m(x)$ and $P(x) = f_1(x) \times  ... \times f_m(x)$, are continuous. But many analytic functions that are continuous can be written in their expanded form (by Taylor Series), which are the sum and product of INFINITE functions. 
My question is, EVEN if there is another way to show that some/all analytic functions are continuous (which I don't know that way), still we should prove from ""the sum and product of infinite functions"" way. Would you please help me regarding the question? I think one of Topology or Analysis way of proof should be enough, because, as we can prove, the topological definition of continuity is equivalent to the $\epsilon - \delta$ definition for functions that map $\Bbb R$ to $\Bbb R$. EDIT: Let me rephrase it: limit of sum of two functions exists if limit of each of the two functions exists. If sum of in finite number of functions is a function that has limit in some point, is it mean that we are allowed to say that for this type of function, sum of limit infinite number of functions exists since limit of sum of those infinite number of functions exists?","['general-topology', 'real-analysis']"
1089712,"A problem is almost similar with the famous ""Ceva's theorem""","Let $\bigodot  O_{1},\bigodot O_{2}$ and $\bigodot O_{3}$ be internal tangents to $\bigodot O$ at $A, B$ and $C$ , respectively, and mutually intersecting at $D,E,F$ respectively. (As shown in Figure) Assume that $GH,IJ,KL$ are external common tangents of $\bigodot  O_{1},\bigodot O_{2}$ and $\bigodot O_{3}$ respectively. show that $$\dfrac{S_{FLA}}{S_{AGE}}\cdot\dfrac{S_{EHC}}{S_{CID}}\cdot\dfrac{S_{DJB}}{S_{BKF}}=1$$ This result was found by my student, and this result fell nice, because it is almost similar to Ceva's theorem , only difference in this problem it is area, or can I say it is Ceva's theorem generalization!. I try by Ceva's theorem and area, Sine theorem, and so on, and can't solve it",['geometry']
1089741,Does $\sum\limits_n \log\left(1+{1\over n}\right)$ diverge or converge?,"How do I find out if $\sum\limits_n\log(1+{1\over n})$ diverges or converges? Wolfram recommends me to use comparison test, but I do not know series which diverges and less than this.","['divergent-series', 'convergence-divergence', 'sequences-and-series']"
1089742,Let $G$ be a group of order 315 with a normal 3-Sylow subgroup. Prove $G$ is abelian.,"I know this it a prevalent question, I really do. It's just that every proof requires using Automorphisms groups about which we were barely taught. I can't start learning everything about Automorphisms by myself (Nor can I tell how far to learn about it) for I don't know how necessary it is if we weren't taught about it. I would appreciate that if you could help me circumvent those proofs that use the autumorphisms. Thank you.","['sylow-theory', 'group-theory', 'abstract-algebra', 'abelian-groups']"
1089761,Evaluating the sum : $\;\frac{1}{3}+\frac{1}{4}.\frac{1}{2!}+\frac{1}{5}.\frac{1}{3!}+\ldots$,How to evaluate this sum? $$\frac{1}{3}+\frac{1}{4}.\frac{1}{2!}+\frac{1}{5}.\frac{1}{3!}+\ldots$$ Please give some technique. Binomial not working.,['sequences-and-series']
1089789,Is $\text{Trace}(e^{XA+A^TX})$ a convex function of $X$?,"Is $\text{Trace}(e^{XA+A^TX})$ a convex function of $X$?
$X$ is diagonal and positive definite, $A$ is symmetric negative definite definite. And by the way, what is the best way to solve a problem of the form: $\text{min}.$ $\text{Trace}(e^{XA+A^TX})$ s.t. $X$ diagonal, $X_{ii}>0$ and $1^T X 1<x$ where $X$ is the variable and $A$ is symmetric negative definite, $x>0$, $x \in R$","['optimization', 'matrices', 'linear-algebra', 'convex-optimization']"
1089794,Radioactive decay of an element A into an element B,"It's well known that the rate of decay of an element is proportional to its amount. Suppose we have a radioactive element $A$ which decays into a radioactive element $B$ ($B$ also decays). If the initial amount of $A$ is $A_0$ and there is no element $B$ at the beginning find a formula for the amount of $B$ at time $t$, $B(t)$. I figured that the amount of $A$ that turned into $B$ in time $t$ is $t\dfrac{dA(t)}{dt}$. So the rate of change of $B$ is $\dfrac{d}{dt}\left(t\dfrac{dA(t)}{dt}\right)=\dfrac{dA(t)}{dt}+t\dfrac{d^2A(t)}{dt^2}=\dfrac{dB(t)}{dt}=bB(t)$ and therefore $$bB(t)=aA(t)+ta\dfrac{dA(t)}{dt}$$ where $a,b$ are the decay constants.I don't know if my thinking is correct, probably not because when I solve this DE I get a wrong answer which says that at time $t=0$ $B(t)\not=0$.",['ordinary-differential-equations']
1089797,"What is the least number $n$, such that $n^{2015}+2015$ is prime?","What is the least number $n$, such that $n^{2015}+2015$ is prime ? According to my calculations, there is no prime for $n\le 6000$. It is clear, that $n$ must be even, since $n^{2015}+2015$ must be odd.","['prime-numbers', 'number-theory']"
1089800,Cumulative distribution function - why is it so important?,"Why CDF is so imporant that it's defined before probability density function in most textbooks? What makes it so important? Was it defined in that way just for practical reasons, or there is a deeper, mathematical reason of how it's defined?","['probability-theory', 'probability']"
1089831,Evaluate $\int_{-\infty}^{\infty} \frac{\log(1+x^2) dx}{1+x^2}$ Using Complex Analysis,"Evaluate: $$\int_{-\infty}^{\infty} \frac{\log(1+x^2) dx}{1+x^2}$$ Using complex analysis, contour integration. This function has no poles at all. Try the contour $C$ Obviously, $$\oint_{C} f(z) dz = 0$$ $$\oint_{C} f(z) dz = \int_{A} f(z) dz + \int_{-R}^{R} f(x) dx$$ $$\int_{-R}^{R} f(x) dx = -\int_{A} f(z) dz$$ Along the semi circle $A$ contour-part the parametrization is: $$z = Re^{i\theta}$$ $$\int_{A} f(z) dz = \int_{0}^{\pi} (iRe^{i\theta})\cdot \frac{\log(Re^{i\theta} + i) + \log(Re^{i\theta} - i)}{(Re^{i\theta} + i)(Re^{i\theta} - i)} d\theta$$ $$\int_{-R}^{R} f(x) dx = (-)\cdot\int_{0}^{\pi} (iRe^{i\theta})\cdot \frac{\log(Re^{i\theta} + i) + \log(Re^{i\theta} - i)}{(Re^{i\theta} + i)(Re^{i\theta} - i)} d\theta$$ $$\int_{-\infty}^{\infty} f(x) dx = (-)\cdot \lim_{R \to \infty} \int_{0}^{\pi} (iRe^{i\theta})\cdot \frac{\log(Re^{i\theta} + i) + \log(Re^{i\theta} - i)}{(Re^{i\theta} + i)(Re^{i\theta} - i)} d\theta$$ Lets say we can apply the dominated convergence theorem.  We can then take the limit INSIDE the integral on the RHS. The problem becomes: $$\int_{-\infty}^{\infty} f(x) dx = (-i)\cdot \int_{0}^{\pi} \lim_{R \to \infty}  (Re^{i\theta})\cdot \frac{\log(Re^{i\theta} + i) + \log(Re^{i\theta} - i)}{(Re^{i\theta} + i)(Re^{i\theta} - i)} d\theta$$ Wolframalpha returns it as $0$ which is wrong. What is the issue? The answer is: $$I = -\pi\log(\sqrt{2})$$ Please tell me what is wrong with this, do not suggest something else until the end. Thank you!","['convergence-divergence', 'calculus', 'integration', 'analysis', 'complex-analysis']"
1089868,What is the fastest growing primitive-recursive-function?,"Fast growing functions tend to be not primitive-recursive. So I wonder if there is a limit how fast a function can grow, if it is known that it is primitive recursive. What is the fastest growing primitive-recursive function ? A function $f$ is said to grow faster than a function $g$, if $f$ eventually surpasses $g$ no matter how big the head start, which is given to $g$, is. In other words, for every natural number $k$ there is natural number $n_0$, such that $f(n)>g(n+k)$ for all $n\geqslant n_0$.","['proof-theory', 'number-theory']"
1089870,"Show that $T^n(x,y)=\left(x+n\alpha \mod 1, y+nx+\frac{n(n-1)}{2}\alpha \mod 1 \right)$","Let $\mathbb{T}^2=\mathbb{R}^2 / \mathbb{Z}^2$. Let $T: \mathbb{T}^2 \rightarrow \mathbb{T}^2$ be the transformation. Let $\alpha \in \mathbb{R}$. $$T(x,y)=\left(x+\alpha \mod 1, x+y \mod 1 \right) $$. Show that $T^n(x,y)=\left(x+n\alpha \mod 1, y+nx+\frac{n(n-1)}{2}\alpha \mod 1 \right)$. I get that $T^n(x,y)=\left(x+n\alpha \mod 1, y+nx \mod 1 \right)$. I cannot see you could obtain $\frac{n(n-1)}{2}\alpha$. Clearly $\frac{n(n-1)}{2}$ is an integer as $n(n-1)$ will be even, but $\alpha$ could be anything???","['dynamical-systems', 'modular-arithmetic', 'transformation', 'functions']"
1089872,Cardinality of set of groups,"After analyzing this question I started wondering. Thoughts . Everyone can give a simple example of a countable group $G, |G| = \aleph_0$ which has uncountable number $2^{\aleph_0}$ subgroups, for instance $G = \bigoplus_{n=1}^{\infty}\mathbb{Z}_2$ but if we are going to assume only non-isomorphic subgroups it turns out that $G$ has only countable number of them. We can modify this example to $G = \bigoplus_{p_n}\mathbb{Z}_{p_n}$ ($p_n$ are primes) and it will satisfy the condition. It is obvious that number of countable groups that have $2^{\aleph_0}$ non-isomorphic subgroups is uncountable since we can construct $2^{\aleph_0}$ of them as follows. Denote $\Pi_1 = \{p_1, p_3, p_5, \dots\}, \Pi_2 = \{p_2,p_4, p_6, \dots\}$. Now our set will be $\{(\bigoplus_{p_k \in \Pi_1}\mathbb{Z}_{p_k})\oplus (\bigoplus_{p'_k \in A}\mathbb{Z}_{p'_k}) \mid\forall A \subset \Pi_2\}$. Obvious that this set is uncountable and all groups from this set are countable and has $2^{\aleph_0}$ non-isomorphic subgroups. Problem . But what is about cardinality of the set $X$ ($X_A$ for abelian) of all countable groups that have $\aleph_0$ non-isomorphic subgroups? Solution (for finitely generated abelian case) If we will assume only abelian groups then $|X_A| = \aleph_0$ since we can decompose each group into direct sum of prime ($\mathbb{Z}$ or $\mathbb{Z}_p$) and if number of non-isomorphic summands will be infinite then number of non-isomorphic subgroups is uncountable. Hence every $G \in X_A$ has finite number of non-isomorphic summands. And there are countably many different prime summands. So finally we got that $|X_A| = \aleph_0$. But can situation be changed using all abelian or non-abelian groups? P.S. Where I'm saying ""set of groups"" one should understand ""set of isomorphism classes"".","['elementary-set-theory', 'group-theory']"
1089877,How to show that $ \int^{\infty}_{0} \frac{\ln (1+x)}{x(x^2+1)} \ dx = \frac{5{\pi}^2}{48} $ without complex analysis?,"The Problem I am trying to show that $ \displaystyle \int^{\infty}_{0} \frac{\ln (1+x)}{x(x^2+1)} \ dx = \frac{5{\pi}^2}{48}$ My attempt I've tried substituting $x=\tan\theta$, and then using the substitution $u=1 + \tan \theta $ which gives: $ \displaystyle \int^{\infty}_{1} \frac{\ln u}{(u-1)(u^2-2u+2)} \ du $ , however I am unable to evaluate this.","['definite-integrals', 'integration']"
1089887,Integration limits and probability density,"So I've got the density function for the $2$-dimensional random variable $(X,Y)$: $$p(x,y) = \frac{4}{3}xe^{-x-y} $$ when $ 0 < y < x$. Otherwise, it's $0$. I am now interested in the density of the random variable $W = X + Y$. This is given by: $$\int_{-\infty}^\infty p(x,w-x)dx$$ Fine enough, but now I run into a problem I always have... what are all the limits concerning this problem? Not just for the integral, but for $W$ as a random variable as well; i.e. when does its density function equal $0$, and when does it equal a function $d(w)$?",['statistics']
1089888,Miranda's Exercise on the Jacobian of a complex torus,"I have to prove that the Jacobian of a complex torus $X=\mathbb{C}/L$ is isomorphic to $X$ by explicity showing that the subgroups of periods $\Lambda \subset \mathbb{C}$ is a lattice which is homotethic to the defining lattice $L$ for $X$, i.e. there is a nonzero complex number $\mu$ such that $\mu \Lambda=L$. My idea, that I can't formalize, is the following: the first homology group of the torus is the free group of rank $2$, i.e. $Z^2$, so the set $\Lambda =\{ \int_c \omega, \, \, c \in H_1(X,\mathbb{Z}) \}$ is of the form $\{n_1 \int_{\gamma_1} \omega+n_2 \int_{\gamma_2} \omega, \, \, n_1, n_2 \in \mathbb{Z} \}$. This implies that $\Lambda$ is a lattice and $Jac(X)=\mathbb{C}/\Lambda$ is a complex torus. Now I have to prove that there is a nonzero complex number $\mu$ such that $\mu \Lambda=L$. How can I solve this exercise?","['abelian-varieties', 'algebraic-geometry']"
1089957,"Show that sets of real numbers $A, B$ are adjacent iff $\sup A = \inf B$","If $A,B \subset \mathbb{R}$ satisfy :
  $$\begin{cases}\forall\ a \in A,\ \forall\ b \in B,\ a \le b \cr
  \forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{
    such that }\quad  b-a \le \epsilon\end{cases}$$
  then we say that  $A$ and $B$ are adjacent . Show that $A$ and $B$ are adjacent if and only if : $\sup(A) = \inf(B)$. My thoughts : note that : $$\sup A =: \begin{cases}\forall\ a \in A,\  ,\ a \le \sup A \cr
  \forall\ \epsilon > 0,\ \exists\ a \in A,\ \text{
    such that }\quad \epsilon-\sup A  <  a\le \epsilon\end{cases}$$ $$\inf B =: \begin{cases}\forall\ b \in B,\  ,\ \inf B \le b \cr
  \forall\ \epsilon > 0,\ \exists\ b \in B,\ \text{
    such that }\quad \inf B \le b  <  \inf B+\epsilon\end{cases}$$ To show the first implication : Assume that $A$ and $B$ are adjacent and let's show that $\sup(A)$, and $\inf(B)$ exists such that $\sup(A) = \inf(B)$. Show first that $\sup(A)$, and $\inf(B)$ exists : Let $b\in B$, we have $$\forall a\in A,\quad a \le b$$
then b is  upper bound, $A \neq  \emptyset, A  \subseteq   \mathbb{R}$ then $\sup(A)$ exist. Let $a\in A$, we have $$\forall b\in B,\quad a \le b$$
then a is  Lower bound, $B \neq  \emptyset, B  \subseteq   \mathbb{R}$ then $\inf(B)$ exist. Show first that $\sup(A)=\inf(B)$: we have :$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{
    such that }\quad  b-a \le \epsilon$$ or $$\forall\ \epsilon > 0,\ \exists\ a,b \in A\times B,\ \text{
    such that }$$
$$ \begin{cases}\epsilon-\sup A  <  a\le \epsilon \cr
   \inf B \le b  <  \inf B+\epsilon\end{cases}$$
$$\iff \begin{cases}-\sup A  <  a\le \sup A-\epsilon\cr
   \inf B \le b  <  \inf B+\epsilon\end{cases}$$
$$\iff \inf B-\sup A  < b-a < \sup A+\inf B $$
$$\iff \inf B-\sup A  < \epsilon \quad \forall \epsilon > 0 $$
i'm stuk here or we can say : since $\forall a,b \in A\times B \quad  a\leq  b $ then $\forall b\in B,\quad  \sup A \leq b$ then $$\sup A \leq \inf B **(1)** $$ we have :$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{
    such that }\quad  b-a \le \epsilon$$
then 
:$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{
    such that }\quad  b<a+\epsilon $$
then 
:$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{
    such that }\quad  \inf B \le \sup A + \epsilon $$ Then  $$\inf B \le \sup A **(2)**$$ From  (1) and (2) we have $$  \inf B=\sup A $$ To show the second implication : Assume that  $\sup(A)$, and $\inf(B)$ exists such that $\sup(A) = \inf(B)$ and let's show that $A$ and $B$ are adjacent To show : $$\forall\ a \in A,\ \forall\ b \in B,\ a \le b $$ from the defintion of $\sup A$ and $\inf B$ we have:
$$ \forall\ a \in A,\  ,\ a \le \sup A \text{ and } \forall\ b \in B,\  ,\ \inf B \le b$$
or we know that $\sup(A) = \inf(B)$
then $$\forall\ a \in A,\ \forall\ b \in B,\ a \le \sup A =\inf B\le b $$ To show : $$  \forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{
    such that }\quad  b-a \le \epsilon $$ from the defintion of $\sup A$ and $\inf B$ we have:
$$ \ \forall\ \epsilon > 0,\ \exists\ a \in A,\ \text{ such that }\quad \epsilon-\sup A  <  a\le \epsilon \text{ and } \forall\ \epsilon > 0,\ \exists\ b \in B,\ \text{ such that }\quad \inf B \le b  <  \inf B+\epsilon$$ we can also say that : $$\forall\ \epsilon > 0,\ \exists\ a,b \in A\times B,\ \text{
    tell que } \begin{cases}\frac{ \epsilon }{2}-\sup A  <  a\le \frac{ \epsilon }{2}\cr
   \inf B \le b  <  \inf B+\frac{ \epsilon }{2}\end{cases}$$
any help would be appreciated !!","['calculus', 'real-analysis', 'supremum-and-infimum']"
1089966,Why is a norm a continuous function? (Question about existing proof),"I'm trying to follow the proof given in this answer: https://math.stackexchange.com/a/265595/188401 I understand the proof in general, but I have a question. It's mentioned that ""In this case it suffices to take δ=ε, for the following reason"", but I don't see a reason provided. So why does this suffice? What happens if delta is bigger than epsilon? Note that I don't have enough reputation to post a comment in the existing thread, hence the new question.","['continuity', 'real-analysis', 'analysis']"
1090000,How to measure the radius of a sphere?,"It is easy enough to determine the radius of a circle, using a minimal number of ""tools"". For instance, one could take any two points on the circle, build a right angle triangle and, by Thales's theorem, the hypotenuse of said triangle would also be the diameter of the circle. However, I would like to know if there are any simple methods of accurately measuring the diameter/radius of a sphere, without cutting it or altering it in any other way, and using a minimum number of tools and intermediate measurements (to reduce the final error). One method that comes immediately to mind is to pin down the sphere between two flat surfaces (blocks) and measure the separation between these blocks; but that method introduces many possible sources of measurement errors, and requires the use of additional equipment (the blocks themselves). tl;dr : Is there a simple way to determine the radius of a sphere without using anything else than a (bendy) ruler?","['spheres', 'geometry']"
1090014,Partial order relations,"Please help solve following: Let $P$ be the set of all people who have ever lived and define a
relation $R$ on $P$ as follows: $\forall\; r,s \in P,\; r\,R\,s \iff r$ is an ancestor of $s$ or $r=s$ . Is $R$ a partial order relation? Prove or give a counter-
example. I think it is partial order based on following calculation, not sure if its correct though: Reflexive: $(r,s)$ belong to $P, r\,R\,s$ because $r=r$ and $s=s$ Antisymetric: Then either $r=s$ or $r < s$ ; or $s=r$ 0r $s < r$ ; Thus $r\leq s$ or $s\leq r$ .
Hence $r=s$ is antisymetric Transitive: Suppose $r$ , $s$ , $e$ are ordered such that $r\,R\,s$ and $s\,R\,e$ ; Then either $r=s$ or $r < s$ and either $s=e$ or $s < e$ . It follows that: $(r < s, s < e)$ by transitivity, $r < e$ and $r\,R\,e$ ; $(r < s, s=e)$ by substitution, $r < r$ and $r\,R\,e$ ; $(r=s, s < e)$ by substitution, $r < e$ and $r\,R\,e$ ; $(r=s, s=e)$ by definition of $R$ , $r=s$ , $s=e$ , then $r=e$ and $r\,R\,e$ ; In each case $r\,R\,e$ , therefore $R$ is transitive. $R$ is partial order.","['relations', 'discrete-mathematics']"
1090027,How derivative relates to roots of original function,"Assume $f$ is differentiable on $\mathbb{R}$. Show that for any $ k \in \mathbb{R}$, $f' + kf$ has a root between any two distinct roots of $f$. I am completely stumped on this. What are some good ways to show existence of zeros? Thanks.","['derivatives', 'analysis']"
1090056,Prove that $\int_{-\infty}^{+\infty} \frac{1}{x^4+x^2+1}dx = \frac{\pi}{\sqrt{3}}$,"Good evening everyone, how can I prove that $$\int_{-\infty}^{+\infty} \frac{1}{x^4+x^2+1}dx = \frac{\pi}{\sqrt{3}}\;?$$ Well, I know that $\displaystyle\frac{1}{x^4+x^2+1} $ is an even function and the interval $(-\infty,+\infty)$ is symmetric about zero, so $$\int_{-\infty}^{+\infty} \frac{1}{x^4+x^2+1}dx = 2\int_0^\infty \frac{1}{x^4+x^2+1}dx.$$
Then I use the partial fraction:
$$2\int_0^\infty \frac{1}{x^4+x^2+1}dx= 2\int_0^\infty \left( \frac{1-x}{2(x^2-x+1)} + \frac{x+1}{2(x^2+x+1)} \right)dx.$$
So that's all. What's next step?","['definite-integrals', 'improper-integrals', 'calculus', 'integration']"
1090061,Manipulating Partial Derivatives of Inverse Function,"In lectures we're told: $$\dfrac {\partial y} {\partial x} = \dfrac 1 {\dfrac {\partial x} {\partial y}}$$ as long as the same variables are being held constant in each partial derivative. The course is 'applied maths', i.e non-rigorous, so don't confuse me. But anyway, if we have: $$\xi = x - y \qquad \eta = x$$ Then $\dfrac {\partial x} {\partial \xi} = 0$ and $\dfrac {\partial \xi} {\partial x} = 1$. The rule presumably fails because one of the partial derivatives is $0$. But then isn't this rule useless? Since I cannot use it without checking that the partial derivative isn't in fact $0$, but then I've worked out the partial derivative manually anyway.","['multivariable-calculus', 'partial-derivative']"
1090066,Is the Riemann sphere conformal equivalent to the 2-sphere?,"Today I stumbled across the calculation (mentioned in this post ) of the transition maps of the stereographic projections from the 2-sphere to the plane. And I wondered about the result that the last transition map is $1/z^*$ (I guess that the $^*$ stands here for complex conjugation) and not $1/z$.
That means one can only show (with these projections) that the 2-sphere $\mathbb{S}^2$ and the Riemann sphere $\mathbb{\bar C}$ are ""just"" diffeomorphic. But is there any
biholomorphic map between $\mathbb{S}^2$ and $\mathbb{\bar C}$?","['stereographic-projections', 'riemann-surfaces', 'complex-analysis']"
1090078,$2\times 2$ matrix $C=AB-BA$,"Let $C=\begin{pmatrix}c_{11}& c_{12} \\ c_{21} & c_{22}\end{pmatrix}$ be a $2 \times 2$ matrix. Show that there exist matrices $A$ and $B$ such that $C=AB-BA$ if and only if $c_{11}+c_{22}=0$. I could show that if such matrices exist, then the trace of the matrix $C$ is equal to zero, I did it by hand, just adding up the entries of the diagonal of the matrix $AB-BA$. I could not prove the other implication. Suppose that $c_{11}+c_{22}=0$, then $c_{22}=-c_{11}$ so $C$ is of the form $C=\begin{pmatrix}c_{11}& c_{12} \\ c_{21} & -c_{11}\end{pmatrix}$. I've tried to find matrices $A$ and $B$ using this condition. If $$A=\begin{pmatrix}a& b \\ c & d\end{pmatrix},B=\begin{pmatrix}e& f\\ g & h\end{pmatrix},$$ then $$AB-BA=\begin{pmatrix}bg-cf& b(h-e)+f(a-d) \\ c(e-h)+g(d-a) & cf-bg\end{pmatrix}$$ I got stuck trying to find the values of each entry, any help would be appreciated.","['matrices', 'linear-algebra']"
1090082,Modern algebra and set theory: ZFC vs. NBG,"This may be somewhat of a philosophical question and is probably nitpicking, but it is also one that has always bothered me a little: Is it not more natural consider NBG set theory as the foundation for modern algebra as opposed to traditional ZFC? To me, ZF has always seemed sort of hacky, for lack of a better word, as if it has been patched and patched over the years; kind of how windows vista would look today if it were still in use. It is no doubt an extremely powerful theory, but the point is that in modern application ZF tends to be somewhat inadequate, seemingly always requiring a work around; thus, hacky. On the other hand, NBG deals with classes directly, and is just for all intents and purposes more accessible from the algebraic viewpoint, especially from the point of view of lattice and order theory, all the way to class field theory. NBG is just better equipped for the job. I guess an easier way to say all of this is that while ZF is more concerned with objects, NBG is designed to exploit the relationships between objects, which, in my opinion is more fundamental to not only mathematics, but to logic itself. NBG is implemented naturally to exhibit the abilities of comparison and deduction, which can be argued to form the basis for the concept of logic, in and of itself. Am I crazy, or has anyone else ever felt this way?","['set-theory', 'soft-question', 'foundations', 'abstract-algebra']"
1090093,Are these graphs all bipartite?,"Given a number $D >0$, define a graph $G_D$ as follows. The vertices of $G_D$ correspond to points in the two-dimensional integer lattice $\mathbb{Z} \times \mathbb{Z}$. A pair of vertices $\{ p,q \}$ is defined to be adjacent if $d(p,q)=D$. My question: is the graph $G_D$ bipartite for every choice of $D$?","['graph-theory', 'metric-spaces', 'combinatorics']"
1090199,Tensor Calculus Second Order Derivatives,"I'm learning tensor calculus by myself through lectures and texts, and I'm presented with the problem of finding the first and second order derivatives of a scalar function of three variables that vary with two parameters. I label the function $f(x^i(\mu^{\alpha}))$ where $i$ runs from one to three and $\alpha$ runs from one to two, setting the $x^i$ as the variables that change with the parameters $\mu^{\alpha}$. Geometrically this represents a surface in $\mathbb{R}^3$. I also set forth the convention for this problem will be to take Latin indices to run from one to three ( coordinates ) and Greek indicies to run from one to two ( parameters ). I first begin by finding the general form of the first derivatives of $f$ with respect to arbitrary coordinates. Brute forcing them with multivariate calculus I get: $$ \frac{\partial f}{\partial \mu^1} = \frac{\partial f}{\partial x^1} \frac{\partial x^1}{\partial \mu^1} + \frac{\partial f}{\partial x^2} \frac{\partial x^2}{\partial \mu^1} + \frac{\partial f}{\partial x^3} \frac{\partial x^3}{\partial \mu^1} $$ The same follows for differentiation w.r.t. the second parameter as well. The pattern makes itself clear and allows me to combine both of those derivatives into the single statement:
$$\frac{\partial f}{\partial \mu^{\alpha}} = \frac{\partial f}{\partial x^i} \frac{\partial x^i}{\partial \mu^{\alpha}}$$ following the Einstein Summation Convention. As for the second derivative, I attempted to carry on the tensor notation and not derive results from multivariable calculus. Beginning with a first rank, covariant tensor, I take the derivative of the previous expression with respect to the parameters again, but using a different index to include all possible second order derivative combos. In symbols: $$
\frac{\partial}{\partial \mu^{\beta}}( \frac{\partial f}{\partial \mu^{\alpha}}) = \frac{\partial}{\partial \mu^{\beta}}(\frac{\partial f}{\partial x^i} \frac{\partial x^i}{\partial \mu^{\alpha}}) $$ Applying the chain rule--if done correctly--I get $$ \frac{\partial^2 f}{\partial \mu^{\alpha} \partial \mu^{\beta}} = \frac{\partial^2 f}{\partial x^i \partial x^j}\frac{\partial x^i}{\partial \mu^{\beta}}\frac{\partial x^j}{\partial \mu^{\alpha}} + \frac{\partial f}{\partial x^i} \frac{\partial^2 x^i}{\partial \mu^{\alpha} \partial \mu^{\beta}} $$ a second order covariant tensor. Is this correct?","['multivariable-calculus', 'tensors']"
1090210,Birational proper morphism and global sections,"Let $f:X \to Y$ be a proper, surjective, birational morphism of noetherian (connected) projective schemes. Assume that $X$ is non-singular. Let $D$ be a Cartier divisor on $Y$ and $L$ the line bundle $\mathcal{O}_Y(D)$. Is it true that the natural morphism $H^0(Y,L) \to H^0(X,f^*L)$ surjective? (I would imagine this to be true. In particular, take any non-constant global section of $f^*L$, $Z$ be the zero locus of the section. It seems to follows from Fulton that $\mathcal{O}_Y(f(Z)) \cong f_*f^*L \cong L$. But I am not entirely sure of this argument)","['algebraic-geometry', 'birational-geometry']"
1090211,How to prove equality between two sets contained in union of another two disjoint sets?,"I need help completing the following proof: Let $A,B$ be two disjoint sets, and $X,Y\subseteq A\cup B$. I want to prove that if $X\setminus A=Y\setminus A$ and $X\setminus B=Y\setminus B$ then $X=Y$. So suppose $X\setminus A=Y\setminus A$, and let's prove $X=Y$. Let $x\in X$. Since $X\subseteq A\cup B$, it holds that $x\in A \vee x\in B$. Now two cases are possible: Case 1: Suppose $x\notin A$. This means $x\in X \wedge x\notin A$ so $x\in X\setminus A$, according to the given premise, we get $x\in Y\setminus A$, so $x\in Y\wedge x\notin A$ and we conclude $x\in Y$. Case 2: Suppose $x\in A$... This is where I'm stuck... I can't use the given premise that $X\setminus A=Y\setminus A$, how can I proceed? Also, I understand why it is necessary for $A$ and $B$ to be disjoint in order for this to be true, and I can easily find a counter example in this case, but where in the proof am I using  the fact that $A$ and $B$ are disjoint? How does that help me? Thanks in advance!",['elementary-set-theory']
1090217,Borel $\sigma$-Algebra on Real Numbers and interval elements [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question I am confused when learning about Borel $\sigma$-Algebra , the source of confusion is that the Borel $\sigma$-Algebra is on the Real numbers... Confusion arises from the fact that i began learning about $\sigma$-Algebra's and how it can work with sets like the ""Power Set"", ie discrete cases. So how can I come to understand that [2,6] or (π,9) is a Borel Set ?","['probability-theory', 'measure-theory', 'probability']"
1090233,Question concerning the proof of the equality of $x\sqrt{y}$ and $y\sqrt{\frac{x^2}{y}}$,"I want to prove that $x\sqrt{y}=y\sqrt{\dfrac{x^2}{y}}$; I've proven it to myself via calculator (brute forcing it) when $x,y>0$, and this is my proof:
$$\begin{align*}
x\sqrt{y} &= y\sqrt{\dfrac{x^2}{y}}&&\text{Conjecture}\\
          &= y\dfrac{\sqrt{x^2}}{\sqrt{y}}&&\text{By Theorem 2.2}\\
          &= y\dfrac{x}{\sqrt{y}}&&\because\sqrt{x^2}=x\\
          &= \dfrac{xy}{\sqrt{y}}&&\text{Simplification}\\
x\sqrt{y}\cdot\dfrac{1}{x} &= \dfrac{xy}{\sqrt{y}}\cdot\dfrac{1}{x}&&\text{Multiplication}\\
\sqrt{y}  &= \dfrac{y}{\sqrt{y}}&&\blacksquare
\end{align*}$$ Questions In a proof, does it suffice to end with a truism as a result of the conjecture to prove the conjecture? Is my proof correct (the steps I took to get there)?","['logic', 'algebra-precalculus', 'proof-verification']"
1090243,Calculation of second order partial derivatives,"Let $$F\left( \frac{x}{z}, \frac{y}{z} \right)=0$$ determine a function $z=z(x,y)$. Show that $$\frac{\partial^2z}{\partial x^2} \cdot \frac{\partial^2z}{\partial y^2}-\left( \frac{\partial^2z}{\partial x\partial y} \right)^2=0.$$ I was frustrated in simplifying doing the second partial derivatives...","['multivariable-calculus', 'partial-derivative']"
1090301,"Are logarithms the only continuous function on $(0, \infty)$ such that $f(xy) = f(x) + f(y)$?","Are logarithms the only continuous function on $(0, \infty)$ that has this property? $$
f(xy) = f(x) + f(y)
$$ If so, how would we show that? If not, what else would we need to show that a function $f$ that satisfies this property is some function $\log_a$ for some $a$?","['logarithms', 'functions', 'real-numbers']"
1090326,How to graph this sin equation?,I have the following sin equation which I am supposed to graph: sin(3x) = -1 and also find how many solutions it contains between 0 and 2π . Seeing this I am a bit confused as I don't understand what I am supposed to do with the -1 . Usually when I graphed these they just equaled y . This said this is what I tried: A (amplitude): 1 B (the stretch/compression amount): 3 P (the period): $\frac{2\pi}{3}$ Note: I found the period by doing $\frac{2\pi}{B}$ Should I bring the -1 on the other side of the equation and let it act as the axis of the wave / mid-line? Basically my question is: what should I do with the -1 ? How does it affect the graph?,"['trigonometry', 'algebra-precalculus', 'graphing-functions']"
1090330,Find Cardinality of the set $X$ defined below,"I did this question by crude counting. Since $|S|$ was only $4$, counting didn't took much time but there should be a way to do it for bigger sets. Let $S = \{a, b, c, d\}$ and  $X = \{f : S \to S \mid f\text{ is bijective and }f(x)\ne x\text{ for each }x \in S\}$ Then
$|X| =\text{ ?}$",['combinatorics']
1090362,"If every vector is an eigenvector, the operator must be a scalar multiple of the identity operator? [duplicate]","This question already has answers here : Demonstration: If all vectors of $V$ are eigenvectors of $T$, then there is one $\lambda$ such that $T(v) = \lambda v$ for all $v \in V$. (3 answers) Closed 5 years ago . I am posed with the following question: Suppose that $T\in \mathcal{L}(V)$, where $V$ is a finite-dimensional vector space,  is such that every vector in $V$ is an eigenvector of $T$. Prove that T is a scalar multiple of the identity function. My attempt goes as follows: If $T$ is a scalar multiple of the identity function, then $Tv=av$ for all $v\in V$, where $a$ doesn't depend on $v$. We will start off assuming that that may not necessarily be true, and work our way to the result. From the problem statement, we know that $Tv=a_j v$, where $a_j$ may depend on the choice of $v$. To show that it doesn't, consider two non-zero vectors $v_1$ and $v_2$ both in $V$ (It would be pointless to consider zero vectors). Consider $T(v_1+v_2)$. Also, let us first consider the case where $v_2$ is not a scalar multiple of $v_1$ (so $v_1$ & $v_2$ form a linearly independent set). On one hand we have $$\begin{align*}
T(v_1+v_2)&=\alpha (v_1+v_2)\\
&= \alpha v_1 + \alpha v_2
\end{align*}$$ Which is true because of the the assumption that every vector is an eigenvector. On the other hand, we have $$\begin{align*}
T(v_1+v_2)&=T(v_1)+T(v_2)\\
&=a_1 v_1 + a_2 v_2
\end{align*}$$ So we are left with the following equality $$a_1 v_1 + a_2 v_2=\alpha v_1 +  \alpha v_2$$
$$\implies (a_1 - \alpha)v_1 + (a_2 - \alpha)v_2 = 0$$ Because $v_1$ & $v_2$ are linearly independent, $a_1=a_2=\alpha$. Now consider $\beta v$, a scalar multiple of $v$. $$\begin{align*}
T(\beta v)&= \beta T(v)\\
&= \beta (a v)\\
&= a (\beta v)
\end{align*}
$$ Because every vector in $V$ that is not equal to an arbitrary vector $v$ is either a scalar multiple of it or the sum of $v$ and some other vector in $V$, $Tv=av$ for all $v\in V$. Is my prove valid? Please criticize my proof holistically. P.S: Is the same proof valid for infinite-dimensional vector spaces? I ask because such truth would lie in the validity of the last paragraph, and I don't know if it is true in infinite-dimensional vector spaces.",['linear-algebra']
1090374,Is there a vector field that is equal to its own curl?,I was wondering if there is a vector field that satisfies the following condition: $$\vec F=\nabla \times \vec F$$,"['multivariable-calculus', 'vector-fields', 'partial-differential-equations', 'curl', 'vector-analysis']"
1090383,Change the order of conditional expectation of integration,"I encountered this problem when learning SDE: $g(t,\omega)$ is a adapted process then
$$\mathbb E\left(\int_a^b |g(t)|^2 \, dt \mid \mathcal F_a\right)=\int_a^b\mathbb E\left(|g(t)|^2\mid \mathcal F_a\right) \, dt$$ I don't know whether I can change the order of conditional expectation of integration. So I try to prove it using the definition of conditional expectation but failed, does it need additional condition?","['probability-theory', 'stochastic-processes', 'conditional-expectation', 'integration']"
1090421,"Questions about matrix rank, trace, and invertibility","(a) Prove that a square matrix $T$ of rank one has $\text{tr}(T)=0$ if and only if $T^2=0$. (b) Consider a matrix $A$ of the form $A=aI+T$, where $a\ne0$, $I$ is the identity matrix,
  and $T$ has rank one and zero trace. Find the inverse and the determinant of $A$. (c) Find the inverse of $A$ as above when $T$ has rank one but nonzero trace $\text{tr}(T)=b$. For which value of $b$ is $A$ not invertible? I'm still stuck on part (a), but campus buildings are closing soon, so I'll be working from home but would love to get some hints / comments on this question.  I'll have limited access to this site - on my phone. For part (a), I've been trying to look at the SVD of matrix $A$, since one can read off the rank very easily - by looking at the number of non-zero singular values of $A$.  Then I am trying some block matrix multiplication to see whether $T^2 = 0$, from assuming that $\text{tr}(T) =0$.  So far, no luck.  Do you think I should stick with this SVD approach, or is it better to play around with the definition and properties of nilpotent operators? Any other hints for the other parts of the question would be greatly appreciated. Thanks!","['trace', 'matrix-rank', 'matrices', 'linear-algebra', 'inverse']"
1090455,Roadmap to reach Arithmetic Geometry for a Physics Major,"I am a physics major but I self-study mathematics. my interests are number theory and geometry. it seems that due to the works of Grothendieck , algebraic geometry have to be used to study deepest problems of number theory which culminates in Arithmetic Geometry or Arithmetic Algebraic Geometry ( Please correct me if this isn't true! ). I would really appreciate it if someone can provide me an elementary roadmap to reach Arithmetic Geometry. I have read Knapp's "" Basic Real Analysis "" and "" Basic Algebra "" and my motivation for choosing these books was that in the second volume of Knapp's Algebra, one will become familiar with the language of number theory and also methods of algebraic geometry which is used to study number theory. any comment about Knapp's books also would be really helpful. Any comment would greatly be appreciated! Thank YOU!",['algebraic-geometry']
1090498,How to calculate the expected maximum tree size in a pseudoforest,"I would like to calculate the expected maximum tree size in a randomly generated pseudoforest of $N$ labelled nodes where self-loops are not permitted. Empty and single-node trees are also not permitted. For example, if we have $4$ labelled nodes, we can generate $3$ pseudoforests with a largest tree size of $2$, and $78$ pseudoforests with a maximum tree size of $4$. There are a total of $(n-1)^n$ possible pseudoforests, thus for $N = 4$ there are $81$. The expected maximum tree size for $N = 4$ would therefore be:
$$
E(x) = \sum_{i=1}^{n}i\cdot p(i) = 2 \cdot \frac{3}{81} + 4\cdot\frac{78}{81} = 3.925...
$$ Some observations: There will never be a pseudoforest where the maximum tree size is $n-1$. The number of pseudoforests of $N$ nodes containing only one connected tree (therefore maximum tree size of $N$) can be calculated using sequences $A000435$ or $A001864 / n$. For $N = 4$, this gives us $78$, ie. when $i = n$ in the summation. The minimum tree size is $2$ if $N$ is even, and $3$ if $N$ is odd. The sum of the numerators of $p(i)$ is equal to $(n-1)^n$ When $N = 5$, the summation is:
$$
3 \cdot \frac{80}{1024} + 5\cdot\frac{944}{1024} = 4.84375
$$ When $N = 6$, the summation is:
$$
2 \cdot \frac{15}{15625} + 3\cdot\frac{640}{15625} + 4\cdot\frac{1170}{15625} + 6\cdot\frac{13800}{15625} = 5.72352
$$ How can I calculate the numerators of $p(i)$ when $i < n$?","['graph-theory', 'trees', 'sequences-and-series', 'probability']"
1090502,$|A\cap C|$ and $|B\cap C|$ are both even $\Rightarrow$ $|(A\bigtriangleup B)\cap C|$ is even?,Let $A$ and $B$ be two infinite sets such that $|A\cap C|$ and $|B\cap C|$ are both even for set $C$. Does it imply that $|(A\bigtriangleup B)\cap C|$ is even?,['elementary-set-theory']
1090514,How to prove the both identity (matrix),"I read a paper, and the paper use the following identities (that hold true in any ring) $(I+AB)^{-1}A = A(I+BA)^{-1}$ $(I+AB)^{-1} = I - A(I+BA)^{-1}B$ Any way to prove this? How to open the term $(I+AB)^{-1}$ ?","['control-theory', 'matrices', 'linear-algebra']"
1090525,$\left(n^n\right)_b = \left(n\right)_b\left(n\right)_b\ldots\left(n\right)_b$,"A friend of mine asked me this today and I was not able to give him an answer. Given a base $b$, find (or show the absence of) an integer $n>1$ s.t.
$$\left(n^n\right)_b=\left(n\right)_b\left(n\right)_b\ldots \left(n\right)_b.$$ The notation $\left(n\right)_b \left(m\right)_b$ means concatenating the representation of the number $n$ in base $b$ with the representation of the number $m$ in base $b$. The $\ldots$ in the above should be taken to mean ""any number of"" consecutive $\left(n\right)_b$. Edit: @alex.jordan points out the interesting relaxation (see comments): $$\left(n^n\right)_b=\left(\underbrace{0...0}_k\,n\right)_b\left(\underbrace{0...0}_k\,n\right)_b\ldots \left(\underbrace{0...0}_k\,n\right)_b.$$ where $\underbrace{0...0}_k$ should be understood to mean $k\geq0$ leading zeros. An answer to either is acceptable. The construction of such an $n$ giving an affirmative answer for the first question will (naturally) give an affirmative one to the second with $k=0$. Edit 2 : The general problem is prohibitively difficult. I will accept an answer for $b=10$.",['number-theory']
1090556,finding an angle without any segment lengths,"Said best with a picture. Given angles a and b, solve for angle x. ( Note that the top right vertex is also the center of the circle ) What I've tried Unable to find a simple method to get to x, I decided to draw all chords, and extend all segments to the edges of the triangle: I've managed to get just about every angle except the few I need to find x, so I'm not sure if this method of extending all lines has helped. Here's how far I got (excuse the rearrangement, I needed room to write): I think I'm overlooking something simple here. I'm not asking for the solution necessarily, just how I should get there. Also if someone could help me with a more technically descriptive title, I would love an edit.",['geometry']
1090560,What is a necessary and sufficient condition for a Taylor series to exist?,"I am trying to decide when a function can be written as a Taylor series.
I think it exists if the following condition is met: For a Taylor series of $f(x)$ about the point $a$ In the region $R$ containing both $x$ and $a$, the function $f(x)$ is single-valued with an infinite number of continuous derivatives that all exist. Is this both a necessary and sufficient condition? If not then what is?",['sequences-and-series']
1090567,How to calculate value of an analytic function in a closed disk.,"I just have answer of this question which is 6, but I don't know how to arrive at this answer. Please anyone help me solve this. How does one calculate the value of this function?",['complex-analysis']
