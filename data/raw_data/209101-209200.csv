question_id,title,body,tags
4196228,Does $(1 + f(x))^x \to e^k$ if $xf(x) \to k$?,"It's well known that $\displaystyle{\lim_{x \to \infty} \left(1 + \frac{k}{x}\right)^x = e^k}$ . Suppose $xf(x) \to k$ as $x \to \infty$ . Do we necessarily have $(1 + f(x))^x \to e^k$ ? If $f$ happens to be differentiable, and if we also have $\displaystyle{\frac{f'(x)}{-x^{-2}} \to k}$ , then the answer to the above question is yes, as can be seen by an application of L'Hopital's rule. But without assuming this second limit exists, I don't think L'Hopital's rule is relevant. If it's any easier, I'd also be interested in the answer for the special case when $k = 1$ . This is a problem I thought of as I was solving problem 3.2.1(b) (page 72) from ""Problems in Mathematical Analysis I"" by Kaczor and Nowak.","['limits', 'real-analysis']"
4196312,Same degree maps of spheres are homotopic,"I'd like to prove that if $f,g : \mathbb{S}^n \longmapsto \mathbb{S}^n$ are pointed maps with $deg(f) = deg(g)$ , then $f,g$ are homotopic. The degree is defined in the following way : given $f : \mathbb{S}^n \longmapsto \mathbb{S}^n$ continuos, this induces $f_* : \tilde{H}_n(\mathbb{S}^n) \longmapsto \tilde{H}_n(\mathbb{S}^n)$ , so exists $d \in \mathbb{Z}$ such that $f_*(x) = dx$ (this is because $\tilde{H}_n(\mathbb{S}^n) \simeq \mathbb{Z}$ ), and we define $d$ as the degree of $f$ . We will use the Hurewicz theorem in the following form : Theorem : There's an homomorphism $h_n : \pi_n(X,x_0) \longmapsto H_n(X)$ which is functorial. If $X$ is $(n-1)$ -connected $h_n$ is an
isomorphism (or in the limit case where $n=1$ is isomorphic to the
quotient with respect to the commutator's subgroup). I'd like to understand if my reasoning is correct, and in the case is not, improve it if possible : If $f,g$ have same degree,let's say $d$ , we have $f_*(x) = dx = g_*(x)$ , so $f_* = g_*$ , i.e they induce the same map in homology. Now if $n>1$ take the diagram $\require{AMScd}$ \begin{CD}
    \pi_n(\mathbb{S}^n,x_0) @>{f}>> \pi_n(\mathbb{S}^n,x_0) \\
    @VVV @VVV\\
    \tilde{H_n}(\mathbb{S}^n) @>{f_*}>> \tilde{H_n}(\mathbb{S}^n)
\end{CD} Where the vertical maps are given by $h_n$ and f is the induced map on $\pi_n(\mathbb{S}^n,x_0)$ . I don't the following, but if $h_n$ is natural in $X$ , we have induced commuting diagram  taking continuos maps $f : X \longmapsto Y$ , $\require{AMScd}$ \begin{CD}
\pi_n(\mathbb{S}^n,x_0) @>{f}>> \pi_n(\mathbb{S}^n,x_0) \\
    @VVV @VVV\\
    \tilde{H_n}(\mathbb{S}^n) @>{f_* = g_*}>> \tilde{H_n}(\mathbb{S}^n) \\
@VVV @VVV\\
\pi_n(\mathbb{S}^n,x_0) @>{g}>> \pi_n(\mathbb{S}^n,x_0)
\end{CD} Hence the diagram above commutes and this should be sufficient to show that $[f] = [g] \in \pi_n(X,x_0)$ , that implies that exists a pointed $H : \mathbb{S}^n \times I \longmapsto \mathbb{S}^n$ which is the desidered homotopy. I am not convinced of the follownigs : Is $h_n$ natural in $X?$ The statement is true only with pointed maps and so pointed homotopy? (Which would be the case of this proof) What about $n = 1?$ If the proof is correct, what smoky part that can be improved? Any help would be appreciated, thanks in advance.","['homotopy-theory', 'manifolds', 'general-topology', 'homology-cohomology', 'algebraic-topology']"
4196314,Hamiltonian vector field in the definition of model Dehn twists,"I am trying to understand Dehn twists along Lagrangian spheres, specifically in the context of section 2.1 in this paper. In order to define the model Dehn twist in $T^*S^n$ , which we identify with $$
\{(p,q) \in \mathbb{R}^{n+1}\times \mathbb{R}^{n+1} \mid |q| = 1, \langle p, q \rangle = 0\},
$$ one first considers the Hamiltonian $$
\mu: T^*S^n \setminus S_0 \to \mathbb{R}, \qquad (p,q) \mapsto |p|.
$$ $S_0$ denotes the zero section. It is then stated that the corresponding Hamiltonian vector field is $$
X_\mu = |p|^{-1} \sum_{j=1}^{n+1}p_j \partial_{q_j} - |p| \sum_{j=1}^{n+1} q_j \partial_{p_j}.
$$ One then defines the model Dehn twist using the flow of this vector field. My question now concerns how to obtain this Hamiltonian vector field - It is required to satisfy $$
\imath_{X_\mu}\omega_{can} = -d\mu,
$$ where we can write $\omega_{can} = dp \wedge dq$ . Then we have $$
\imath_{X_\mu}\omega_{can} = dp(X_\mu)dq - dq(X_\mu)dp \stackrel{!}{=} -\partial_p(\mu)dp - \partial_q(\mu)dq = -d\mu.
$$ Computing the partial derivatives, we have $\partial_{p_j}(\mu) = \frac{p_j}{|p|}$ and $\partial_{q_j}(\mu) = 0$ , hence the Hamiltonian vector field should only be $$
X_\mu = |p|^{-1}\sum_{j=1}^{n+1} p_j \partial_{q_j}!
$$ Am I making some basic mistake? How does the second term arise? Thank you in advance.","['geometric-topology', 'symplectic-geometry', 'differential-geometry']"
4196329,Is continuity characteristic required for two equations to be equal,"The function below is discontinuous : $$
f(x)=\frac{x^2-9}{x-3}
$$ But when it is further simplified, it becomes continuous : $$
g(x)=x+3
$$ First question, is $f(x) = g(x)$ ? Second question, if presented with $f(x)$ , is it mathematically incorrect to further simplify it?","['calculus', 'algebra-precalculus', 'real-analysis']"
4196354,What is the truly rigorous framework to model the success probability of three person shooting gun?,"Note : This is not a question about a senior high school book problem. Let the success probability of shooting a gun toward a target of three
people Adam, Bob, Charlie are $0.5$ , $0.7$ and $0.9$ respectively. And
suppose these three people do not affect each other. Compute the
probability that these three people success simultaneously. The answer is simply $0.5\times0.7\times0.9$ , of course. However, I wonder what is the true underlying framework that we indeed use to solve this problem. In particular, what are the sample space $\Omega$ , Borel sets $\Sigma$ and probability measure $P$ (and even random variables) being assumed or used in such context? There are at least three possible framework that come to my mind: One sample space $\Omega$ , Borel set $\Sigma$ and probability measure $P$ : in this case, $\Omega$ should be $\{(i,j,k)\mid i,~j,~k\in\{\text{Hit},\text{Fail}\}\}$ , and the event (measurable set) of Adam being hit is $A=\{\{\text{Hit},j,k\}\mid j,~k\in\{\text{Hit},\text{Fail}\}\}$ , and $P(A)=0.5$ . Follow this construction, we know what we want to solve is $P(A\cap B\cap C)$ , which is $P(A)P(B)P(C)$ . Interpreting as three independent random variables $X_1,~X_2,~X_3$ . In this case, $X_1,~X_2,~X_3$ should be inevitable to be defined on different probability space, since the probability of three people is varing, so using only one measure $P$ for three random variables appears wrong. However according to wikipedia, it seems that we cannot talk about the ""independent random variables"" on different probability space. Am I wrong? Or can we? Interpreting as the product of probability spaces : Let $(\Omega,\Sigma,P_1)$ , $(\Omega,\Sigma,P_2)$ and $(\Omega,\Sigma,P_3)$ be three different measure space, such that $\Omega=\{\text{Hit},\text{Fail}\}$ , $\Sigma=\{\phi,\Omega,\{\text{Hit}\},\{\text{Fail}\},\{\text{Hit},\text{Fail}\}\}$ , $P_1(\{\text{Hit}\})=0.5,~P_1(\{\text{Fail}\})=0.5,~P_1(S)=0$ otherwise. $P_2(\{\text{Hit}\})=0.7,~P_2(\{\text{Fail}\})=0.3,~P_2(S)=0$ otherwise. Finally, $P_3(\{\text{Hit}\})=0.9,~P_3(\{\text{Fail}\})=0.1,~P_3(S)=0$ otherwise. And then we construct the product measure $P$ , computing $P((\text{Hit},\text{Hit},\text{Hit}))$ . Which of the above is/are correct? Which is better or worse? What is the most standard way to formalize them? Are there other ways to formalize it?","['measure-theory', 'probability-theory', 'probability', 'real-analysis']"
4196362,Taylor Expansions with sum of vectors,"I have a function $f:\mathbb{R}^n\to\mathbb{R}$ with gradient $\nabla_x f$ and Hessian $H_f$ . Suppose $x, y, z, w\in\mathbb{R}^n$ and $\delta, \epsilon\in\mathbb{R}$ . If I have $x = y + \delta z$ then I could Taylor Expand $f$ around $y$ as $$
f(x) = f(y) + \delta \nabla_xf(x)\bigg|_y z + \frac{\delta^2}{2}z^\top H_f\bigg|_y z + \mathcal{O}(\delta^3)
$$ Now I have multiple vectors summed up to $y$ . For instance $x = y + \delta z + \epsilon w$ . How can I Taylor Expand $f$ around $y$ now?","['multivariable-calculus', 'calculus', 'taylor-expansion', 'real-analysis']"
4196424,"Is the ""Its transpose is its inverse"" definition of an orthogonal matrix equivalent to the ""It preserves the dot product"" definition?","A long time ago, I was taught that a real $n\times n$ matrix $A$ is called orthogonal if $AA^t=I$ . But recently I learned from a DG book that $A$ is said to be orthogonal if it preserves the dot product: $$(Ax)\cdot(Ay)=x\cdot y\quad\text{for all }x,y\in\mathbb{R}^n.$$ Are these two definitions equivalent? It is easy to see that the former implies the latter: $$(Ax)\cdot(Ay)=(Ay)^t(Ax)=(y^t A^t)(Ax)=y^t x=x\cdot y$$ But I have a hard time going from the latter to the former. Is it possible? Thank you for your time.","['matrices', 'orthogonal-matrices', 'linear-algebra']"
4196432,Arctan with minus sign in the exponents,"Is there an easy or quick way to see that $$
-\int\frac{4\lambda e^{-(\mu+\lambda x+\frac{t}{\lambda})}}{1+e^{-2(\mu+\lambda x+\frac{t}{\lambda})}}\, dx = -4\arctan(e^{\mu+\lambda x+\frac{t}{\lambda}})\tag{1}?
$$ Okay, when I know that $\frac{d}{dx}\arctan(x)=\frac{1}{1+x^2}$ , then by the chain rule, I can differentiate the right hand side of (1) to get $$
-4\frac{d}{dx}\arctan(e^{\mu+\lambda x+\frac{t}{\lambda}})=-4\lambda e^{\mu+\lambda x+\frac{t}{\lambda}}\frac{1}{1+e^{2(\mu+\lambda x+\frac{t}{\lambda})}}
$$ But what about the minus signs in the exponents appearing in the integrand?","['integration', 'trigonometry', 'real-analysis']"
4196467,Valid arguments and truth tables,"I'm trying to understand validity of arguments and using truth tables. This concerns an example on a discrete math course on Linkedin Learning which I reproduce here. Essentially, trying to make sentences out of the truth table is not making much sense except for the first row. We have some sentence propositions: Today is Monday ( $p$ ) If today is Monday, then I will have a salad for lunch ( $p \rightarrow q$ ) Therefore, I will have a salad for lunch ( $q$ ) Which, symbolically, looks like $$
p \\
p \rightarrow q \\
\therefore q
$$ This is then used to construct the following table $$
\begin{array}{|c|c|c|}
\hline
p& p \rightarrow q & q 
\\ \hline
T &\ \ T & T
\\ \hline
T & F & F
\\ \hline
F & T & T
\\ \hline
F & T & F
\\ \hline
\end{array}
$$ I get how the columns for $p$ and $q$ are laid out but I don't understand how the true/false values for $p \rightarrow q$ for the 3rd, and 4th rows play out (and I'm not entirely sure about the second either): Second row: ""Today is Monday. If it is a Monday, then I will not have a salad for lunch. Therefore I am not having a salad for lunch"". Third row: ""Today is not Monday. If it is Monday, then I will have a salad for lunch. Therefore I will have a salad for lunch"" ? Fourth row: ""Today is not Monday. If it is Monday, then I will have a salad for lunch. Therefore I will not have a salad for lunch"" ?","['logic', 'discrete-mathematics']"
4196502,Question on the evaluation functional in $L^p$,"I know that for $1 \leq p < \infty$ we have $C_c(X)$ dense in $L^p(X,\mu)$ . In $L^p(X,\mu)$ the evaluation functional is not bounded, but in $C_c(X)$ it is. So if I define $T_x : C_c(X) \to \mathbb{R}$ as $T_x f = f(x)$ for $x \in X$ I can extend this functional to $L^p(X,\mu)$ using the Hahn Banach (HB) theorem. To make sure I understand how to possibly apply HB I wonder: Is my application correct? Am I right when I say that the extension if $T_x$ is bounded? Thank you.",['functional-analysis']
4196511,If $EF$ meets $AD$ at $Y$ prove that $\frac{AY}{YD} = \frac{\lambda}{(\lambda+1)}$.,"$ABCD$ is a trapezium with $AD \parallel BC$ , $X$ lies on $AD$ such that $\frac{AX}{XD} = \lambda$ . The straight lines $AB$ and $CD$ meet at $E$ , and the lines $BX$ and $AC$ meet at $F$ . If $EF$ meets $AD$ at $Y$ prove that $\frac{AY}{YD} = \frac{\lambda}{(\lambda+1)}$ . What I Tried : Here is a Picture :- Let $BX$ meet $CE$ at $G$ . I was able to apply Menelaus Theorem on $5$ triangles and got $5$ equations. These are :- \begin{align}
\frac{DG}{GE} * \frac{EB}{AB} * \frac{AX}{DX} = 1.  && \text{} \tag 1\\ 
\\
\frac{DX}{AX} * \frac{AF}{FC} * \frac{CG}{DG} = 1. && \text{} \tag 2\\
\\
\frac{XY}{AY} * \frac{AE}{BE} * \frac{BF}{FX} = 1. && \text{} \tag 3\\
\\
\frac{DG}{GE} * \frac{EF}{YF} * \frac{YX}{DX} = 1. && \text{} \tag 4\\
\\
\frac{DY}{AY} * \frac{AF}{FC} * \frac{CE}{DE} = 1. && \text{} \tag 5\\
\end{align} I can notice there are some common ratios in some of these lines, but after these I am not able to get anything useful after here. I know I need to reach $\frac{AY}{YD}$ (in Eqn. $(5)$ ) from $\frac{AX}{XD}$ (in Eqn. $(2)$ ), but I do not know how. Can someone help me? Thank You. Edit :- I wrote some equations wrong, I have corrected them now.","['euclidean-geometry', 'triangles', 'geometry']"
4196531,Semidirect product in GAP (Wallpaper groups),"I am fairly new to GAP and I am having difficulties using the semidirect product method. I am trying to use this as I am working with wallpaper groups. In my example, I am trying to figure out : $D_6 \ltimes (Z_2 \times Z_2)$ with $D_6$ the dihedral group of order 12 and $Z_2$ isomorphic to $C_2$ , the cyclic group of order 2. The semidirect product is defined by the group operation (Golubitsky, 1988): $(\sigma_1, p_1)(\sigma_2, p_2) = (\sigma_1\sigma_2, \sigma_1p_2+p_1)$ . With $\sigma_1 \in D_6$ , $\sigma_2 \in D_6$ and $(p_1, p_2) \in Z_2 \times Z_2$ . I have created my groups in GAP using permutation groups, I have constructed the direct product, the automorphism group of $Z_2 \times Z_2$ but now I am stuck on the homomorphism from $D6$ onto the automorphism of $Z_2 \times Z_2$ . I don't understand how to create it for in order to use it for the direct product. I hope this is clear enough, please tell me if you need me to rephrase my question or if anything is missing. Thank you so much for your time.","['gap', 'group-theory', 'semidirect-product', 'finite-groups']"
4196551,Equivalent group presentation,"I have been trying to determine whether the following groups are isomorphic for a while now with no significant progress. $G_1 = \langle x, y \mid xyx = yx^2y\rangle$ $G_2 = \langle x, y \mid xyx = yxy\rangle$ I would bet they are not, yet I cannot think of any characteristic that might set them apart. I know that the order of the groups must be the same and that the order of elements must match, but I cannot see how to use this here. In the comments it was suggested that I compute the abelianisations. However, the abelianisation of $G_1$ is $\langle x, y\mid 1=y\rangle$ and the abelianisation of $G_2$ is $\langle x, y\mid x=y\rangle$ , which are both infinite cyclic. So abelianisations do not help either. To show isomorphism I would have to show that there are elements in the other group that obey the same relation and which generate the entire group. I cannot see any obvious candidates. Any help would be much appreciated.","['group-presentation', 'group-theory']"
4196559,Pushforward: from measure theory to differential geometry?,"I was wondering if there is a connection between the pushforward from measure theory, and the pushforward from differential geometry? In measure theory : let $X:(\Omega, \mathcal{A}, \mu) \to \mathbb{R}$ a random variable. The pushforward measure is then defined as $$
\nu = X_\#\mu = \mu \circ P^{-1}
$$ and defines a measure on $\mathbb{R}$ . Hence, we push the measure $\mu$ to $\mathbb{R}$ . In differential geometry : let $\phi:M \to N$ be a smooth map between two manifolds. The pushforward differential is a map $$
d\phi(x): T_xM \to T_{\phi(x)}N.
$$ Hence, we push a tangent vector of $M$ to a tangent vector of $N$ . I understand both definitions. But I am not sure I understand the relation correctly. Is there even a relation? Can we interpret the pushforward differential as a measure? Is the relation given by the Radon-Nikdoym theorem? Any idea is welcome.","['pushforward', 'measure-theory', 'category-theory', 'differential-geometry']"
4196583,What subsets of $\Bbb R$ are closed under countable sums?,"More precisely: Definition. A subset $S \subset \Bbb R$ is called good if the following hold: if $x, y \in S$ , then $x + y \in S,$ and if $(x_n)_{n = 1}^\infty \subset S$ is a sequence in $S$ and $\sum_{n = 1}^\infty x_n$ converges, then $\sum_{n = 1}^\infty x_n \in S$ . In other words, a good subset is closed under finite sums and countable sums whenever the sum does exist. Question: What are all the good subsets of $\Bbb R$ ? Origin This question was asked recently and Conifold had commented how the only subsets of $\Bbb R$ closed under countable summations are $\varnothing$ and $\{0\}$ . It was then natural to ask ""closed under countable summation, assuming it exists"". My thoughts Here are some examples of familiar sets which are good: $\varnothing$ , $\{0\}$ , $\Bbb Z_{\geq 0}$ , $\Bbb Z_{> 0}$ , $\Bbb Z$ , $n\Bbb Z$ , $\Bbb R$ . We even have the following: $$r \Bbb Z := \{rn : n \in \Bbb Z\},$$ where $r$ is any real number. But the examples apart from $\Bbb R$ are good for a trivial reason: Those are sets that are closed under finite summation and have the property that they are discrete enough so that the only convergent sums are those where the terms are eventually $0$ . (In the case of $\Bbb Z_{> 0}$ , there is no such sum.) On the same note, intervals of the form $[a, \infty)$ and $(a, \infty)$ are good for $a > 0$ . In general, suppose that $S$ satisfies the following: $S$ is closed under finite sums and there exists $\epsilon > 0$ such that $|s| > \epsilon$ for all $s \in S$ . Then, $S$ and $S \cup \{0\}$ are good. Another example: $[0, \infty)$ and $(0, \infty)$ are good and do not follow the criteria above. The following are some examples of not good sets: $\Bbb Q$ , $\Bbb R \setminus \Bbb Q$ , $\Bbb R \setminus \Bbb Z$ , a proper cofinite subset of $\Bbb R$ , any bounded set apart from $\{0\}$ and $\varnothing$ . In fact, excluding $\Bbb Q$ , the other ones are not even closed under finite sums. In the same vein as $\Bbb Q$ , we also have the set of real algebraic numbers which is not good (but is indeed closed under finite sums). Here's a nontrivial one: Consider the set $$B = \left\{\frac{1}{2^k} : k \in \Bbb Z_{> 0}\right\}.$$ Then, any countable subset of $\Bbb R$ that contains $B$ is not good. Proof. $(0, 1]$ is uncountable and every element in it can be written as a sum of elements of $B$ . (Binary expansions.) $\Box$ A bit more thought actually shows that more is true: Since $(0, 1]$ is contained in the set of all possible sums, any good set containing $B$ must contain all of $(0, \infty)$ . Another one: let $(a_n)_{n \ge 1}$ be any real sequence such that $\sum a_n$ converges conditionally. Then, the only good subset containing $\{a_n\}_{n \ge 1}$ is $\Bbb R$ , by the Riemann rearrangement theorem. Additional comments There are some variants that come to mind. Not sure if any of them are any more interesting. But I'd be happy with an answer that answers only one of the following variants as well. What if I exclude point 1. from my definition? Let's call such a set nice. In that case, the set $\{1\}$ is nice but not good. (Of course, if $0 \in S$ , then nice is equivalent to good.) What are the nice subsets of $\Bbb R$ ? What are the nice subsets which are not good? What if I consider only those sums which have converge absolutely? More observations (These are edits, which I'm adding later) Here are some additional observations: Arbitrary intersection of good sets is good and $\Bbb R$ is good. Thus, it makes sense to talk about the smallest good set containing a given subset of $\Bbb R$ . So, given a subset $A \subset \Bbb R$ , let us call this smallest good set to be the good set generated by $A$ and notationally denote it as $\langle A \rangle$ . (In particular, $A$ is good iff $A = \langle A \rangle$ .) $A \subset B \implies \langle A \rangle \subset \langle B \rangle$ . $\langle (0, \epsilon) \rangle = (0, \infty)$ and similar symmetric results. Suppose $S \subset (0, \infty)$ is dense in $(0, \infty)$ , then $\langle S \rangle = (0, \infty)$ . Indeed, pick $a_0 \in (0, \infty)$ . Then, there exists $s_1 \in (a_0/2, a_0)$ . Put $a_1 := a - s_1$ . Then, $a_1 \in (0, a_0/2)$ . Now pick $s_2 \in (a_1/2, a_1)$ and so on. Then, $\sum s_n = a_0$ . Symmetric results apply. In particular, the only dense good subset of $\Bbb R$ (or $\Bbb R^+$ or $\Bbb R^-$ ) is the whole set. Suppose $S \subset (0, \infty)$ contains arbitrarily small elements (i.e., $S \cap (0, \epsilon) \neq \varnothing$ for all $\epsilon > 0)$ ), then $\langle S \rangle = (0, \infty)$ . To see this, let $a > 0$ be arbitrary. Pick $s_1 \in (0, a)$ . Let $n_1$ be the largest positive integer such that $n_1 s_1 < a$ . Then pick $s_2 \in (0, a - n_1s_1)$ . Let $n_2$ be the largest such $n_2s_2 < a - n_1s_1$ and so on. Then, $$\underbrace{s_1 + \cdots + s_1}_{n_1} + \underbrace{s_2 + \cdots + s_2}_{n_2} + \cdots = a.$$","['sequences-and-series', 'analysis', 'real-analysis']"
4196606,Hartshorne Proposition II.7.13a,"Let $X$ be a Noetherian scheme, $\mathscr{I}$ be a coherent sheaf of ideals and let $\pi:\operatorname{Bl}_Y X \rightarrow X$ be the blow up of $X$ along the closed subscheme $Y$ corresponding to $\mathscr{I}$ . In the proof of the book of Hartshorne ""Algebraic Geometry"", Proposition II.7.13a, it is written: $$\mathscr{S}(U)(1) = \bigoplus_{d \geq 0} \mathscr{I}^{d+1}(U)$$ where $\mathscr{S}$ is the sheaf of graded algebras defined by $\mathscr{S} = \bigoplus_{d \geq 0} \mathscr{I}^d$ , and $U$ is an open affine subset of $X$ . From the above equality it follows that $\mathscr{S}(U)(1) = \mathscr{I} \cdot \mathscr{S}(U)$ , the ideal generated by $\mathscr{I}$ in $\mathscr{S}(U)$ , and so the inverse image ideal sheaf $\pi^{-1}\mathscr{I} \cdot \mathcal{O}_X$ is equal to $\mathcal{O}_{\operatorname{Bl}_YX}(1)$ . My question is, why it is $\mathscr{S}(U)(1) = \bigoplus_{d \geq 0} \mathscr{I}^{d+1}(U)$ ? Shouldn't it be $\mathscr{S}(U)(1) = \bigoplus_{d \geq -1} \mathscr{I}^{d+1}(U)$ , since $\mathscr{S}(U)(1)_{-1} = \mathscr{S}(U)_0 = \mathcal{O}_X(U) \neq 0$ ? In that case, is the equality $\mathscr{S}(U)(1) = \mathscr{I} \cdot \mathscr{S}(U)$ still true?","['algebraic-geometry', 'blowup', 'coherent-sheaves', 'sheaf-theory']"
4196658,$1$D mass-spring chain leading to a 'discrete ODE' in space,"Background (Not strictly necessary, you can jump to 'Question') Problem statement I'm studying a $1$ D system of $N$ masses, of equal mass $m$ , connected between them by springs, of equal stiffness $k$ . Masses are at starting locations $x_n\equiv x(n)=na$ where $n\in[0,..,N]$ and $a\in\mathbb{R}$ , the initial distance between masses, is a known constant parameter. No external forces are applied. System then, is modeled by $N$ constant-coefficient differential equations of the kind $$\qquad\qquad m\,\ddot u(x_n,t)+2k\,u(x_n,t)-k\,[u(x_{n+1},t)+u(x_{n-1},t)]=0\qquad n=1,..,N\qquad (1)$$ where $u(x_n,t)$ is the dispacement of mass $n$ at time $t$ , relative to its initial position $x_n$ . This set of equations account for inertia force of mass $n$ and spring force contributions of left and right-side springs, that connect mass $n$ respectively to masses $n-1$ and $n+1$ . Note : for $a\to0$ system reduces to the so-called wave equation $$u_{tt}-c^{2}\,u_{xx}=0$$ with $c=\sqrt{E/\rho}$ . To do this, just consider $m\equiv \rho Aa$ and $k\equiv EA/a$ , where $\rho$ is the volume density of the continuous rod (whose discretization creates the $1$ D system), $A$ is the cross-sectional area and $E$ is the elasticity. Finally, the fact that $\lim_{a\to 0}\,[u(x+a,t)-2\,u(x,t)+u(x-a,t)]/a^2=u_{xx}$ by definition of $2$ nd derivative. Note : Last 'Note' justify intuetively why following steps are taken, although, i'm posting this question because i want to justify them rigorously, being this, at the end, a purely mathematical problem. Solution - Step I Impose an harmonic solution: $$u(x_n,t)=u_n(t)=u_n(\omega)\,e^{-i\omega t}$$ No further justification given, i explained this to myself in the following way: If we define $u_n(t)\equiv u(x_n,t)$ and develop the system $(1)$ in matrix form we have \begin{align}
\underset{M}{\underbrace{\begin{bmatrix}
m&0&0&-&0\\
0&m&0&-&0\\
0&0&m&-&0\\
|&|&|& &|\\
0&0&0&-&m
\end{bmatrix}}}
\underset{\ddot{\mathbf{u}}(t) }{\underbrace{\begin{Bmatrix}
\ddot u_1(t)\\
\ddot u_2(t)\\
\ddot u_3(t)\\
|\\
\ddot u_n(t)
\end{Bmatrix}}}
+
\underset{K}{\underbrace{\begin{bmatrix}
2k&-k&0&-&0\\
-k&2k&-k&-&0\\
0&-k&2k&-&0\\
|&|&|& &|\\
0&0&0&-&2k
\end{bmatrix}}}
\underset{\mathbf{u}(t) }{\underbrace{\begin{Bmatrix}
u_1(t)\\
u_2(t)\\
u_3(t)\\
|\\
u_n(t)
\end{Bmatrix}}}
=
\mathbf{0}
\end{align} Then, being $det(M)\neq 0$ and thus matrix $M$ invertible, we can write \begin{align}
\left\{\begin{matrix}
\dot{\mathbf{u}}(t)=\dot{\mathbf{u}}(t)\qquad\qquad\\
\ddot{\mathbf{u}}(t)=-M^{-1}K\,\mathbf{u}(t)
\end{matrix}\right.
\end{align} Next, defining $\mathbf{z}(t)\equiv \begin{Bmatrix}\mathbf{u}(t)\\\dot{\mathbf{u}}(t)\end{Bmatrix}$ , we can compact last system in the form $$\dot{\mathbf{z}}(t)=
\underset{A}{\underbrace{\begin{bmatrix}
0&\mathcal{I_{n\times n}}\\
M^{-1}K&0
\end{bmatrix}}}\mathbf{z}(t)$$ where $\mathcal{I}_{n\times n}$ is the Identity matrix of order $n$ Eventually, we have achieved the manageable form of the constant-coefficient linear ODE system $$\dot{\mathbf{z}}(t)=A\,\mathbf{z}(t)$$ which has solution $$\mathbf{z}(t)=e^{A(t-t_0)}\,\mathbf{z}(t_0)$$ This solution, under the condition of $A$ being diagonalizable * (see this stack question about ), can be reduced in the form $$\mathbf{z}(t)=\sum_{i=1}^{N}c_i\,\mathbf{z}_0\,(\omega_i)\,e^{\omega_i t}$$ From this form, finally, for a specific $\omega_i$ , can be extracted the type of harmonic solution used in the quote $$\boxed{u_{n,\,\omega_i}(t)=u_n(\omega_i)\,e^{-i\omega_i t}}$$ * Note : Regarding $A$ being diagonalizable, for this case, i couldn't actually managed to carry out a proof, maybe it will be the object of a specific stack question about it Solution - Step II Impose a wave solution: $$u_n(\omega)=u_0[\kappa(\omega)]\,e^{-i\kappa(\omega) x_n}$$ My explanation: Imposing what obtained for 'Solution - Step I' , system $(1)$ reduces to \begin{align}
\{-\omega_i^2\,m\,u_{n}(\omega_i)+2k\,u_{n}(\omega_i)-k\,[u_{n+1}(\omega_i)+u_{n-1}(\omega_i)]\}\,e^{-i\omega_i t}&=0\\
& n=1,..,N,\; i=1,..,N
\end{align} Eventually, remembering that $u_n=u(x_n)$ , dropping $i,n$ notation and simplifying $e^{-i\omega_i t}$ as $\neq 0$ , we end up with $$-\omega^2\,m\,u(x,\omega)+2k\,u(x,\omega)-k\,[u(x+a,\omega)+u(x-a,\omega)]=0$$ Eventually, this equation should be solved with the solution provided in the second quote. Now, this is not a differential equation, although its solution is imposed to be so. Neither is an usual form i know how to deal with. From this non-understanding arises my question, which i will formulate better in the next section (there i will omit $\omega$ dependency as $\omega$ is set, i.e. a different equation is meant to exist for each $\omega$ ). Question Consider an equation of the form $$-\omega^2a^2\,u(x)+2\,c^2\,[u(x)-u(x+a)-u(x-a)]=0$$ with $u(x)\in C^2:\mathbb{R}\to\mathbb{R}$ and $a,c,\omega\in\mathbb{R}$ known parameters. So, i ask Which kind of equation is this? Does it fall in some particular category? How to find the complete set of solution for $u(x)$ ? Hint1 : Some solutions are in the form $u(x)=u_0(\kappa)\,e^{\kappa x}$ Hint2 : If we divide by $a^2$ and take the limit for $a\to 0$ , equation transforms in the $2$ nd order constant-coefficient linear ODE $\omega^2u-c^2\,u_{xx}=0$ (but $a$ is actually meant to stay not null) Hint3 : In equation there are many parameters which i just used to stay coherent with theory where equation is from, if these parameters bother you, feel free to rename them or group them My solution attempt (incomplete) Due to the presence of arguments like $x\pm a$ , and the fact that solution can have exponential form, my idea is to apply Fourier Transform at right and left hand-side, so to obtain \begin{align}
-\omega^2 a^2\,\hat u(\kappa)+2 c^2\,[\hat u(\kappa)-e^{\kappa a}\,\hat u(\kappa)-e^{-\kappa a}\,\hat u(\kappa)]&=0\\
[-\omega^2 a^2+2 c^2(1-e^{\kappa a}-e^{-\kappa a})]\,\hat u(\kappa)&=0
\end{align} Eventually, looking for non-zero solutions and expanding exponential with Euler formula, we obtain $$-\omega^2 a^2+2 c^2\,[1-cos(\kappa a)]=0$$ Solving for $\kappa$ $$\kappa=\frac{1}{a}\,{cos}^{-1}\bigg(1-\frac{\omega^2 a^2}{2\,c^2}\bigg)$$ Then, $\kappa$ can have either two distinct solutions or one solution, based on parameters value. To find back $u(x)$ , we can compute anti-Fourier transform, so to have \begin{align}
u(x)&=\frac 1 {2\pi}\,\int_{-\infty}^{+\infty}\hat u(\kappa)\,e^{i\kappa x}\,d\kappa\\
&=\kappa\in[\kappa_1,\kappa_2] \text{ or } \kappa=\kappa_1\text{ (trouble fomalizing this step in the integration)}\\
&=
\left\{\begin{matrix}
\frac 1 {2\pi}\,\hat u(\kappa_1)\,e^{i\kappa_1 x}+\frac 1 {2\pi}\,\hat u(\kappa_2)\,e^{i\kappa_2 x}\qquad\qquad\qquad\qquad
\\
[\frac 1 {2\pi}\,\hat u(\kappa_1)+\frac 1 {2\pi}\,\hat u(\kappa_1)\,x]\,e^{i\kappa_1 x}\text{(just a sound guess)}
\end{matrix}\right.\\
&=
\left\{\begin{matrix}
u_0(\kappa_1)\,e^{i\kappa_1 x}+u_0(\kappa_2)\,e^{i\kappa_2 x}\qquad\qquad\qquad\qquad
\\
[u_0(\kappa_1)+u_0(\kappa_1)\,x]\,e^{i\kappa_1 x}\text{(just a sound guess)}
\end{matrix}\right.
\end{align} Which, should be the complete set of solution Update : Googling 'Discrete ODE' i just found out that an equation like the one i'm looking solutions for might be called 'difference equation', and so it shall be rewritten like $$-\omega^2 a^2\,u_n+2c^2\,[u_n-u_{n+1}-u_{n-1}]=0$$","['ordinary-differential-equations', 'fourier-transform', 'recurrence-relations', 'discrete-mathematics', 'partial-differential-equations']"
4196680,Is “implies” the best symbol when rewriting equations?,"In my mathematical homework, I usually indicate algebraic rewrites of equations using implication, and the symbol "" $\implies$ "" (LaTeX \implies ).  For instance, I might write $$ 3 x - y = 0 \implies 3 x = y \implies x = \frac{y}{3} $$ to mean that, since $3 x - y = 0$ , the equivalent equation $3 x = y$ is also true, which then indicates that $x = y/3$ is true.  Is logical implication the correct facility to express rewriting an equation into an equivalent form?  If not, what other concept and symbol would be correct here?","['notation', 'algebra-precalculus', 'logic']"
4196688,"Writing $G/[G, G]$ as a direct product of cyclic groups","Let $G$ be the group given by the presentation $\langle x , y , z : x^2 , y^3 , (xyz)^4 \rangle$ . I would like to write $G/[G , G]$ as a direct product of cyclic groups, where $[G , G]$ is the commutator subgroup of $G$ . I am familiar with writing (finitely generated) abelian groups as a product of cyclic groups using Smith normal forms, but the free group aspect of this problem is throwing me off (which I believe is the point). I am aware that the free group on a set $S$ modulo its commutator subgroup is isomorphic to $\mathbb{Z}^{|S|}$ , but I am unsure of how the additional relations (such as those in the problem in question) impact things. Obviously $G/[G , G]$ is a finitely generated abelian group, but is $G/[G , G]$ isomorphic to the abelian group, $A$ , (written additively) generated by elements $a , b, c \in A$ subject to the relations $2a = 3b = 4(a + b + c) = 0$ ? If this is the case, then I am completely comfortable finishing off the problem and actually writing $G/[G , G]$ as a direct product of cyclic groups, but I would like some more justification as to why. If this is not the case, then what is the correct way to go about this problem?","['group-presentation', 'modules', 'combinatorial-group-theory', 'group-theory', 'abelian-groups']"
4196709,Question about using Sard's theorem in Whitney's weak embedding theorem,"I'm trying to study about Whitney's embedding theorem (the case for $2k+1$ ). Studying the proof of the theorem: http://staff.ustc.edu.cn/~wangzuoq/Courses/18F-Manifolds/Notes/Lec09.pdf I encountered a few things I'm not sure about. At the end of the third page, it's stated that the image of the map $\alpha:M\times M\setminus \Delta\rightarrow\Bbb{RP}^{K-1}$ $($ where $\Delta=\{(p,p):p\in M\})$ is of measure zero following Sard's theorem (because $M\times M$ is an open submanifold of dimension $2k<K-1$ ), but Sard's theorem speaks about critical points, so why does it imply that? Same question goes about the map $\beta$ mentioned right after it.","['submanifold', 'proof-explanation', 'smooth-manifolds', 'differential-topology', 'differential-geometry']"
4196715,Flow of a Vector Field Using Sheaves,"I am wondering whether the concept of the flow of a vector field can be described in the following sheaf-theoretic way: $\newcommand{\Flow}{\mathrm{Flow}}
\newcommand{\pre}{\mathrm{pre}}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}
\newcommand{\from}{\colon}
\newcommand{\blank}{{-}}$ Let $M$ be a smooth manifold. Let me define the presheaf $\Flow^\pre$ on $M$ given as follows: If $U \subset M$ is an open subset, then an element of $\Flow^\pre(U)$ should be given by a pair $(\epsilon, \phi)$ where $\epsilon$ is some positive real number and $\phi \from U \times (-\epsilon, \epsilon) \to M$ is a smooth map satisfying some properties, modulo some equivalence relation. The properties should be: $\phi(x,0) = x$ for all $x \in U$ . $\phi( \phi(x,t), s) = \phi(x, t+s)$ whenever $s,t \in (-\epsilon,\epsilon)$ , $x \in U$ such that $t + s \in (-\epsilon, \epsilon)$ and $\phi(x,t) \in U$ . The equivalence relation should be that $(\epsilon, \phi) \sim (\epsilon', \phi')$ if there exists some $\epsilon'' \le \min(\epsilon, \epsilon')$ such that $\phi(x,t) = \phi'(x,t)$ for all $x \in U$ and $t \le \epsilon''$ . I think that the above defines a separated presheaf on $M$ . There is a morphism of presheaves from $\Flow^\pre$ to the sheaf of smooth vector fields on $M$ . If $U \subset M$ is an open subset and $[\epsilon, \phi] \in \Flow^\pre(U)$ then a vector field $X$ is given by stipulating that $X_x = d_0\phi(x, \blank) (\partial/\partial t)$ . Here $d_0\phi(x,\blank)$ denotes the differential at $0$ of the smooth map $\phi(x, \blank) \from (-\epsilon, \epsilon) \to M$ . Denote by $\Flow$ the sheafification of $\Flow^\pre$ . By the universal property of sheafification, there is an induced morphism of sheaves from $\Flow$ to the sheaf of smooth vector fields. I think that the usual construction of a flow of a vector field (proved using the Cauchy-Lipschitz theorem) says that this morphism is an isomorphism of sheaves. Is that correct?","['sheaf-theory', 'vector-fields', 'smooth-manifolds', 'differential-geometry']"
4196744,Fubini's theorem on locally compact Hausdorff spaces without measure theory,"Suppose that $X$ is a locally compact Hausdorff space. For a Radon measure $\mu$ on $X$ , let $I_{\mu}\colon C_{c}(X)\to\mathbb{C}$ be the positive linear functional defined by $I_{\mu}(f):=\int_{X}f \ \text{d}\mu$ . The Riesz representation theorem imples that the assignment $\mu\mapsto I_{\mu}$ implements a one-to-one correspondence between (positive) Radon measures on $X$ and positive linear functionals on $C_{c}(X)$ . So one could define an integral on $X$ as a positive linear functional $I\colon C_{c}(X)\to\mathbb{C}$ . This definition does not rely on measure theory. I was wondering whether we could prove Fubini's theorem in this setting, i.e. without refering to measure theory. More precisely, does anyone know a proof or reference of the following statement without measure theory? Let $I$ and $J$ be positive linear functionals (i.e. integrals) on locally compact Hausdorff spaces $X$ and $Y$ , respectively. For $f\in C_{c}(X\times Y)$ and $y\in Y$ we define $f^{y}\colon X\to\mathbb{C}$ via $f^{y}(x):=f(x,y)$ . For $x\in X$ we define $f_{x}\colon Y\to\mathbb{C}$ similarly. The functions $x\mapsto J(f_{x})$ and $y\mapsto I(f^{y})$ are compactly supported and $$I(x\mapsto J(f_{x}))=J(y\mapsto I(f^{y})).$$","['measure-theory', 'geometric-measure-theory', 'real-analysis', 'functional-analysis', 'fubini-tonelli-theorems']"
4196749,Flux as a mapping,"I'm reading the proof of Theorem 5.1 in this paper . I have a few questions about flux. They say that for a vector field $\xi \in C^\infty_0(\mathbb{R}^n)$ , the corresponding flux is $\{\Phi_\tau\}_{\tau \in \mathbb{R}}$ satisfying $$ \partial_\tau \Phi_\tau = \xi \circ \Phi_\tau \;\;\;\;\; \text{ for all }\tau \in \mathbb{R}.$$ Then, they say that $\Phi_\tau$ defines a pushforward mapping on measures, i.e. for a given measure $\rho^k(y)$ , its pushforward under $\Phi_\tau$ is the measure $\rho_\tau (y)$ with $$ \int_{\mathbb{R}^n} \rho_\tau(y) \zeta(y)dy = \int_{\mathbb{R}^n} \rho^k (y) \zeta(\Phi_\tau(y))dy \forall \zeta \in C^0_0(\mathbb{R}^n).$$ I don't understand the first equation (definition of the flux). I thought that flux was a surface integral. Here, since the vector field has bounded support, I would guess that the corresponding flux would be the flux out of the boundary.
I don't know what $\tau$ represents here, either. Furthermore, how can $\Phi_\tau$ be a mapping on measures?","['integration', 'multivariable-calculus', 'measure-theory', 'physics']"
4196771,What is the most general distribution for which E[1/x] = 1/E[x]?,"What is the most general distribution for which the expected value of the multiplicative inverse equals the multiplicative inverse of the expected value? Motivation: I'm into modelling dynamics on graphs and I found a problem which is easily solvable in cases where the degree distribution of the vertices is a distribution where $E[1/k] = 1/E[k]$ . ( $k_i$ is the degree of the $i$ th vertex)
From this solution I may gain an insight into how to unify multiple models. So particularly I'm looking for a distribution which consists of non-negative, finite integers . But I'm also interested in continuous solutions. Distributions where $E[1/k^n]=1/E[k^n]$ may also help unifying the models. What I do know so far , that $k_i=1$ is a particular solution.
In the continuous case every function where $f(x)=f(1/x)$ and $E[x]=1$ is a solution.
I know what momentum generating functions are and they seem like a good direction to try in, but I failed so far. What is the most general form of this distribution? Does it have a name? It sounds like something trivial, like a ""famous"" distribution, but I can't find it.","['statistics', 'discrete-mathematics']"
4196811,Infinitely many primes with $2$ and $3$ generating the same set of residues,"Prove that there are sets $S$ and $T$ of infinitely many primes such that: For every $p\in S$ there exists a positive integer $n$ such that $p\mid 2^{n} - 3$ . For every $p \in T$ the remainders mod $p$ of $\{2,2^2,2^3,\ldots\}$ and $\{3,3^2,3^3,\ldots\}$ are the same as sets. (Hint: Consider divisors of $2^{3^n} - 3$ for suitably chosen $n$ .) I was able to do the first part - assume they are finitely many, say $p_1,\ldots,p_k$ , and observe that they are odd and that none of them divides $2^{(p_1-1)(p_2-1)\cdots (p_k-1)} - 3$ by Fermat's/Euler's theorem. Any idea how to approach the (stronger) second part? Any help appreciated! UPDATE: As Hagen von Etizen suggested, it suffices to consider $p$ such that $p\mid 2^n - 3$ and $p\mid 3^m - 2$ for some integers $m$ and $n$ . It is then tempting to take all primes $p_1,p_2,\ldots$ which divide some $2^n - 3$ and suppose only $q_1,\ldots,q_k$ divide some $A = 3^n - 2$ -- then $n=(q_1-1)\cdots(q_k-1) \geq 2$ could again help, but even though $A$ must have a prime factor, this factor may not be among $p_1,p_2,\ldots$ and this is troubling! Any other ideas?","['prime-factorization', 'elementary-number-theory', 'totient-function', 'group-theory', 'prime-numbers']"
4196818,Cyclic Vector and Diagonalizable operator,"I've been working on this exercise: Prove that a diagonalizable operator $T$ on an $n$ -dimensional vector space has a cyclic vector iff it has $n$ -distinct eigenvalues. I tried to do the following: Let $v$ be the cyclic vector of $T$ . Then $C=\{v, Tv, \dots, T^{n-1}v\}$ is basis of $V$ and since $T$ is diagonalizable there is a basis of eigenvectors, say $B=\{b_1, \dots, b_n\}$ . Now, you can write $v$ in terms of basis $B$ . And hence the other vectors of basis $C$ , that is $Tv, T^2v, \dots, T^{n-1}v$ . In the end, I believe that the result will follow of the linear independence of $\{v, Tv, \dots, T^{n-1}v\}$ , however I couldn't show how. It seems to me that contradiction could be used, assuming that there are distinct eigenvalues ​​and somehow have that the set $\{v, Tv, \dots, T^{n-1}v\}$ will not be linearly independent. Is my idea correct? Any tips to continue? If there is a simple way to solve, I would be grateful for the help.","['linear-algebra', 'eigenvalues-eigenvectors']"
4196822,"Solution integral $\;\displaystyle \iint \sqrt{\cos^2(x \pi)+\sin^2(y \pi)} \ dx\,dy$","Working on a hobby project: ""Circle from (2D) random walk"" [SE] and came across this integral: $$\bar{R}=\int_0^1 \int_0^1 \sqrt{\cos^2(X  \pi)+\sin^2(Y  \pi)} \ dX\,dY$$ My intention is to have the mean vector length of every vector (starting in origin) in a square: $x \in [0,1]$ and $y \in [0,1]$ where: $x=\cos(X  \pi)$ and $y=\sin(Y  \pi)$ . Initial I solved numerical with Python (taking sample of vectors): import numpy as np

x=np.linspace(-np.pi/2,0,1001)
y=np.linspace(0,np.pi/2,1001)

X,Y =np.meshgrid(x,y)

def radius(x,y):
    return np.sqrt((np.cos(x))**2+(np.sin(y))**2)

z=np.array([radius(x,y) for (x,y) in zip(np.ravel(X), np.ravel(Y))])

print(np.mean(z)) Giving: $$\bar{R}=0.95802...$$ Solving integral with Wolfram Alpha (online) gives: integral \sqrt(cos^2(x*pi)+sin^2(y*pi)) dxdy from x=0 to 1 and y=0 to 1 $$\bar{R}=0.958091\ldots$$ Values seems to match and looks like I am taking the mean vector length within square. $X$ and $Y$ are random values between $[0,2]$ in original problem. Is this integral known? And how to solve for it? I noticed that I can replace $sin^{2}$ for $cos^{2}$ giving: $$\bar{R}=\int_0^1 \int_0^1 \sqrt{\cos^2(X\pi) + \cos^2(Y\pi)} \ dX\,dY$$ or: $$\bar{R}=\int_0^1 \int_0^1 \sqrt{\sin^2(X\pi) + \sin^2(Y\pi)} \ dX\,dY$$ Does not help me gain more feeling. I would like to learn more about this integral where to start? And how do solutions (without intervals) look like? EDIT: original formula without $\cos$ and $\sin$ looks like: $\;\displaystyle \bar{R}=\frac{1}{a^2} \int_0^a \int_0^a \sqrt{x^2+y^2} \ dx\,dy$ . Here Wolfram Alpha (online) gives complicated overwhelming formula. Not sure if nice compact solution exists.",['integration']
4196860,Is $\mu$ a measure,"a) $\mu(E)=\lim_{n\rightarrow\infty} \frac{\#[\{2,4,\dots\}\cap \{1,\dots,n\}]}{n}=\lim_{n\rightarrow\infty}\frac{n/2}{n}=1/2$ and $\mu(O)=1/2$ by same argument. $$\mu(S)=\lim_{n\rightarrow\infty}\frac{\#[ \{1,4,9,\dots\}\cap \{1,\dots,n\}]}{n}=\lim_{n\rightarrow\infty}\frac{\sqrt n}{n}=0$$ b) Let $A,B$ be disjoint sets in $\mathcal A$ . $$\begin{split}\mu(A\cup B)&=\lim_{n\rightarrow\infty} \frac{\#[(A\cup B)\cap \{1,\dots,n\}]}{n}&&\text{definition}\\
&=\lim_{n\rightarrow\infty} \frac{\#[A\cap \{1,\dots,n\} \cup B \cap \{1,\dots,n\}]}{n}&&\text{DeMorgan's Law}\\
&=\lim_{n\rightarrow\infty} \frac{\#[A\cap \{1,\dots,n\}] +\#[ B \cap \{1,\dots,n\}]}{n}&&\text{$A$ and $B$ are disjoint}\\
&=\mu(A)+\mu(B)\end{split}$$ c) The two requirements for a measure are $\mu:\mathcal A\rightarrow[0,\infty]$ . This is satisfied since $\mu(B)$ is between 0 and 1. If $B_1,B_2,\dots$ are disjoint elements of $\mathcal A$ then $\mu\left(\bigcup_{i=1}^\infty B_i\right)=\sum_{i=1}^\infty \mu(B_i)$ . This follows inductively from part 2. I recognize this is a pretty simple exercise but have I made any big oversights?","['elementary-set-theory', 'measure-theory']"
4196861,Proof involving weak convergence: where to us compactness,"I have the following claim to prove as homework: Consider a continuous random variable $W$ with PDF $f_W(\cdot)$ and probability measure $\mathbb{P}_W$ . Let $B$ be a compact  subset of $\mathbb{R}\times \mathbb{R}^+$ . Show that, if $B$ is large enough, then there exists a probability measure $\pi(\cdot)$ on $B$ such that $f_W(\cdot)$ . can be approximated as \begin{equation}
\label{main}
f_W(w)\approx \int_{B} g(w; \mu, \sigma^2) d\pi(\mu, \sigma^2) \quad \text{ for each $w\in \mathbb{R}$},
\end{equation} where $g(\cdot; \mu, \sigma^2)$ is the PDF of a Normally distributed r.v. with mean $\mu$ and variance $\sigma^2$ . I have written a proof (reported below). However, there are 2 things which I do not understand and I would like your help on: (1) Where should I use compactness of $B$ (if needed at all)? (2) Suppose the $f_W(\cdot)$ has very fat tails . Then, I expect the approximation to be worse than for another PDF with thin tails. Where does this show up in my derivations? My proof: Step 1: Let $\mathbb{P}_W(\cdot)$ be the probability measure associated with $f_W(\cdot)$ . Consider a sequence of  finite and strictly positive numbers $\{s^2_n\}_n$ such that $\lim_{n\rightarrow \infty}s^2_n=0$ . Consider a sequence of r.v. $\{Y_n\}_n$ such that each $Y_n\sim \mathcal{N}(0,s^2_n)$ and $Y_n\perp W$ . Let $Z_n\equiv W+Y_n$ . Then, by Slutsky's Theorem, it holds that: $$
 Z_n\rightarrow_d W \quad \text{as $n\rightarrow \infty$},
 $$ that is: $$
(1) \quad \lim_{n\rightarrow \infty} \int_{x\in \mathbb{R}} h (x) d \mathbb{P}_{Z_n}(x) = \int_{x\in \mathbb{R}} h (x) d \mathbb{P}_{W}(x) \quad \text{for any bounded continuous function $h:\mathbb{R}\rightarrow \mathbb{R}$},
$$ where $\mathbb{P}_{Z_n}(\cdot)$ is the probability measure associated with the PDF $f_{Z_n}(\cdot)$ of $Z_n$ . Note also that $$
f_{Z_n}(z)=\int_{\mu \in \mathbb{R}} g(z; \mu, s^2_n) d \mathbb{P}_W(\mu) \quad \text{ for each $z\in \mathbb{R}$},
$$ where $g(\cdot; \mu, s^2_n)$ is the PDF of a Normally distributed r.v. with mean $\mu$ and variance $s^2_n$ .
In turn, (1) can be rewritten as $$
\lim_{n\rightarrow \infty}\int_{\mu \in \mathbb{R}} \Big( \int_{x\in \mathbb{R}} h (x)  d \mathbb{G}(x; \mu, s^2_n)\Big) d \mathbb{P}_{W}(\mu)= \int_{x\in \mathbb{R}} h (x) d \mathbb{P}_{W}(x)  \quad \text{for any bounded continuous function $h:\mathbb{R}\rightarrow \mathbb{R}$},
$$ where $\mathbb{G}(\cdot; \mu,s^2_n)$ is the probability measure associated with $g(\cdot; \mu, s^2_n)$ . Step 2: Consider a sequence of finite and strictly positive numbers $\{\tau_n\}_n$ such that $\lim_{n\rightarrow \infty} \tau_n=0$ . Consider a sequence of  sets $\{\mathcal{A}_{ n}\}_{n}\subset \mathbb{R}$ such that $\mathbb{P}_W(\mathcal{A}_{ n})>1-\tau_n$ . Note that, as $\tau_n\rightarrow 0$ , we have that the set $\mathcal{A}_{ n}$ enlarges. Let $\tilde{Z}_n$ be a r.v. with PDF given by the following Gaussian mixture: $$
 f_{\tilde{Z}_n}(z)= \int_{\mu \in \mathcal{A}_{ n}}  g(z; \mu, s^2_n) d \mathbb{P}_W(\mu) \quad \text{for each $z\in \mathbb{R}$}.
 $$ In what follows we show that $$
 \tilde{Z}_n\rightarrow_d W \quad \text{as $n\rightarrow \infty$}.
 $$ Step 3 : Consider  any bounded continuous function $h:\mathbb{R}\rightarrow \mathbb{R}$ and let $M\equiv \sup_{x\in \mathbb{R}}|h(x)|<\infty$ .   Then, $$
 \Big| \int_{\mu \in \mathbb{R}} \Big( \int_{x\in \mathbb{R}} h (x)  d \mathbb{G}(x; \mu, s^2_n)\Big) d \mathbb{P}_{W}(\mu)  - \int_{\mu\in \mathcal{A}_{ n}}  \Big(\int_{x\in \mathbb{R}} h (x)  d \mathbb{G}(x; \mu, s^2_n)\Big) d \mathbb{P}_{W}(\mu)  \Big|\\
\leq    \int_{\mu \in \mathbb{R}\setminus \mathcal{A}_{ n}}  \Big( \int_{x\in \mathbb{R}} |h (x)|  d \mathbb{G}(x; \mu, s^2_n)\Big) d \mathbb{P}_{W}(\mu)\\
\leq   M\times \mathbb{P}_W(\mathbb{R}\setminus \mathcal{A}_{ n})\leq M\times \tau_n. 
$$ Further, $$
  \lim_{n\rightarrow \infty} M\times \tau_n=0.
$$ Therefore: $$
\lim_{n\rightarrow \infty}  \int_{\mu \in \mathcal{A}_{ n}}  \Big(\int_{x\in \mathbb{R}} h (x)  d \mathbb{G}(x; \mu, s^2_n)\Big) d \mathbb{P}_{W}(\mu) =\int_{x\in \mathbb{R}} h (x) d \mathbb{P}_{W}(x) \quad \text{for any bounded continuous function $h:\mathbb{R}\rightarrow \mathbb{R}$}. 
$$ By Fubini's Theorem, $$
\int_{\mu \in \mathcal{A}_{ n}}  \Big(\int_{x\in \mathbb{R}} h (x)  d \mathbb{G}(x; \mu, s^2_n)\Big) d \mathbb{P}_{W}(\mu) = \int_{x\in \mathbb{R}} h(x) d\mathbb{P}_{\tilde{Z}_n}(x). 
 $$ Therefore: $$
\lim_{n\rightarrow \infty} \int_{x\in \mathbb{R}} h(x) d\mathbb{P}_{\tilde{Z}_n}(x) =\int_{x\in \mathbb{R}} h (x) d \mathbb{P}_{W}(x) \quad \text{for any bounded continuous function $h:\mathbb{R}\rightarrow \mathbb{R}$},
$$ that is $$
 \tilde{Z}_n \rightarrow_d W \text{ as $n\rightarrow \infty$}. 
 $$ Step 4: Let the set $\mathcal{B}_n$ be large enough to contain $\mathcal{A}_{ n}\times (0,\epsilon_n)$ . We have that $$
f_{\tilde{Z}_n}(z)= \int_{\mu \in \mathcal{A}_{ n}}  g(z; \mu, s^2_n) d \mathbb{P}_W(\mu)=\int_{\mu \in B_{ n}}  g(z; \mu, s^2_n) d \underbrace{(\mathbb{P}_W(\mu)\times \ {1}\{\mu\in \mathcal{A}_{n}, \sigma^2=s^2_n\})}_{\equiv \pi(\mu, \sigma^2)} \quad \text{for each $z\in \mathbb{R}$}.
$$ Therefore, for large $n$ , we can approximate $f_W(\cdot)$ with $f_{\tilde{Z}_n}(\cdot)$ provided that $\mathcal{B}_n$ is large enough.","['sequences-and-series', 'limits', 'probability-theory', 'probability', 'compactness']"
4196862,generalization of Cauchy-Riemann conditions,"Esteemed experts, Please excuse the ignorance and language of a poor physicist. As we know, the real $u(x,y)$ and imaginary $v(x,y)$ parts of an analytic function (in some domain) satisfy the Cauchy-Riemann (CR) conditions $\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y},$ $\frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x}.$ In contrast, in the problem that I encountered, I am looking at the following equations $\frac{\partial u}{\partial x} = \sigma(x,y)\,\frac{\partial v}{\partial y},$ $\frac{\partial u}{\partial y} = -\sigma(x,y)\,\frac{\partial v}{\partial x},$ where $\sigma(x,y)$ is a sufficiently smooth function. The latter equations look like  the CR conditions but are not equivalent to them because $\sigma(x,y)$ depends on $(x,y)$ . My question is whether it is possible to generalize the complex analysis to have the generalized Cauchy-Riemann conditions with position-dependent $\sigma(x,y)$ . Perhaps someone can point me to the relevant references. Thanks in advance,
Serge","['complex-analysis', 'cauchy-riemann-equations']"
4196883,"12-letter words with four X's, four Y's, and four Z's","Okay, per the title, we're constructing 12-letter words with four X's, four Y's, and four Z's. How many such words have no X's in the first 4 letters and no Y's in the next 4 letters? Let me show my work with what I got. Let's do casework based on the number of X's in the last 4 letters. YYYY ZZZZ XXXX: 1 possibility YYYZ ZZZX XXXY: 4(4)(4) = 64 possibilities YYYY ZZZX XXXZ: 4(4) = 16 possibilities YYYZ XXZZ XXYZ: 4(6)(12) = 288 possibilities YYZZ XXZZ XXYY: 6(6)(6) = 216 possibilities YYYY XXZZ XXZZ: 6(6) = 216 possibilities YYYY XXXZ XZZZ: 4(4) = 16 possibilities ZZZY XXXZ XYYY: 4(4)(4) = 64 possibilities YYZZ XXXZ XZYY: 6(4)(12) = 288 possibilities YYYZ XXXZ XYZZ: 4(4)(12) = 192 possibilities ZZZZ XXXX YYYY: 1 possibility YYYY XXXX ZZZZ: 1 possibility Adding up, we have 1363 possibilities. (1) Is this correct? (2) That was a pain to do all the casework. Is there a cleaner way to solve this problem?",['combinatorics']
4196910,evaluate $\int_{0}^{1}\frac{\text{Li}_2(\frac{x^2-1}{4})}{1-x^2}dx$,I came across this integral: $$\int_{0}^{1}\frac{\text{Li}_2(\frac{x^2-1}{4})}{1-x^2}dx=\frac{1}{2}\int_{-1}^{1}\frac{\text{Li}_2(\frac{x^2-1}{4})}{1-x^2}dx$$ One way to evaluate is to start with the subsitituion $x=2(v-0.5)$ to get: $$\int_{0}^{1}\frac{\text{Li}_2((v-1)v)}{v(1-v)}dv$$ We can use the series expansion of $\text{Li}_2((v-1)v)$ & beta function to get the A.A Markov series which equals $-\frac{4}{5}\zeta(3)$ which is: $$\sum_{n=1}^{\infty} \frac{2(-1)^{n}}{n^3\begin{pmatrix}2n\\ n\end{pmatrix}}$$ We can then prove that this sum equals $-\frac{4}{5}\zeta(3)$ by elementary means. My question: is there anther approach to evaluate this integral besides using A.A Markov series? Like an identity of the dilogarithm function or integral manipulations?,"['integration', 'calculus', 'polylogarithm', 'definite-integrals']"
4196933,Let $G$ be a group of order $16$ with an element $g$ of order $4$. Prove that the subgroup of $G$ generated by $g^2$ is normal in $G$.,"Question: Let $G$ be a group of order $16$ with an element $g$ of order $4$ .  Prove that the subgroup of $G$ generated by $g^2$ is normal in $G$ . Thoughts: I keep getting stuck.  My most recent path is that since $G$ is solvable, then there is a subnormal series... so maybe I can get something to come out there?  But I just feel like a counterexample is throwing a wrench into everything I try.  Any help would be greatly appreciated.","['normal-subgroups', 'group-theory', 'abstract-algebra', 'finite-groups']"
4196944,Number of zeroes of $f'$ given number of zeroes of $f$.,"Suppose that $f$ is a non-constant, smooth, complex valued function on $\mathbb{C}$ with $\Gamma=\{z\in \mathbb{C}: \lvert f(z)\rvert=7\}$ a smooth, closed, simple curve. Let $G$ be the enclosed domain and suppose that $f$ is analytic on $G$ . It is easy to show using the maximum principle that $f$ must have at least one zero in $G$ . Now suppose also that $f'\equiv \frac{\partial f}{\partial z}$ has no zeroes on $\Gamma$ and that $f$ has $m$ zeroes in $G$ counting multiplicity. How many zeroes must $f'$ have? I think that the answer has to be $m-1$ .  My approach was to try to use the argument principle. I don't exactly see how to get a handle on $\int_{\Gamma} \frac{f''}{f'}dz$ . Next I thought about parameterizing $\Gamma$ as $\gamma(t)$ for $a\leq t\leq b$ with $\lvert \gamma'(t)\rvert =1$ and trying to use $f'(\gamma(t))=\frac{\frac{d}{dt}f(\gamma(t))}{\gamma'(t)}$ , then computing the change in argument around $\Gamma$ . This sort of made sense to me as I'm pretty convinced that the change in argument of $\frac{d}{dt}f(\gamma(t))$ is the same as the change in argument of $f(\gamma(t))$ , but I couldn't prove that and even with this fact the proof is quite hand wavy. Anyway if someone has a more elegant approach, please share!",['complex-analysis']
4196984,Solve $y''-2y = 4x^2e^{x^2}$,"I want to use variation of parameters: $y_h = C_1e^{\sqrt{2}x} + C_2e^{-\sqrt{2}x}$ . The wronskian I get to be $w = -2\sqrt{2}$ , $w_1 = -4x^2e^{x^2}e^{-\sqrt{2}x}$ and $w_2 = 4x^2e^{x^2}e^{\sqrt{2}x}$ . Solving for $$u_1' = \frac{w_1}{w} \rightarrow \frac{\sqrt{2}}{2}x^2e^{x^2-2\sqrt{x}}$$ $$u_1 = \frac{\sqrt{2}}{2}\int x^2e^{x^2-\sqrt{2}x}dx$$ This integral seems like a headache. Is this right approach? Is there a trick to solving this with VOP?",['ordinary-differential-equations']
4197045,Faster method for solving the inequality $\sqrt{λ_1^2+ λ_2^2+ λ_3^2}$ $\leq$ $\sqrt{1949}$ for a 3x3 matrix,"λ 1 , λ 2 , λ 3 are the eigen values of the matrix \begin{bmatrix}26&-2&2\\2&21&4\\4&2&28\end{bmatrix} Show that $\sqrt{λ_1^2+ λ_2^2+ λ_3^2}$ $\leq$ $\sqrt{1949}$ Here's my approach at solving this problem - Performing Single Value Decomposition of $A$ , we get $$A^TA = \begin{bmatrix}696&-2&172\\-2&449&136\\172&136&804\end{bmatrix}$$ $$\Sigma^T\Sigma = \begin{bmatrix}389.78&0&0\\0&604.83&0\\0&0&954.38\end{bmatrix}$$ Therefore, $$\Sigma = \begin{bmatrix}\sqrt{389.78}&0&0\\0&\sqrt{604.83}&0\\0&0&\sqrt{954.38}\end{bmatrix}$$ Now, $$\sum_{i=1}^n|\lambda_i| \leq \sum_{i=1}^n|\sigma_{ii}|$$ Squaring on both sides, $$\sum_{i=1}^n|\lambda_i|^2 \leq \sum_{i=1}^n|\sigma_{ii}|^2$$ We get $$\lambda_1^2+\lambda_3^2+\lambda_3^2 \leq 1948.99\approx1949$$ Taking square root, we get $$\sqrt{\lambda_1^2+\lambda_3^2+\lambda_3^2} \leq \sqrt{1949}$$ Is the step where I square the summation inequality correct? I have never come across taking squares of summation so I have my doubts whether it's correct or not. Also, is there any faster way of solving this problem. Computing $A^TA$ and then finding it's eigenvalues takes too much time. EDIT : $$(\sum_{i=1}^n|\lambda_i|)^2 \leq (\sum_{i=1}^n|\sigma_{ii}|)^2$$ $$\sum_{i=1}^n|\lambda_i|^2 + 2\sum_{j=1}^{n-1}\sum_{i=j+1}^{n}|\lambda_i||\lambda_j| \leq tr(A^TA)$$ Since $2\sum_{j=1}^{n-1}\sum_{i=j+1}^{n}|\lambda_i||\lambda_j| > 0$ , $$|\lambda_1|^2+|\lambda_3|^2+|\lambda_3|^2 \leq 1949$$ $$\sqrt{|\lambda_1|^2+|\lambda_3|^2+|\lambda_3|^2} \leq \sqrt{1949}$$",['linear-algebra']
4197099,$X=\operatorname{grad} f$ if and only if $X f \equiv|X|_{g}^{2}$ and $X$ is orthogonal to the level sets of $f$ at all regular points of $f .$,"I was reading Lee's IRM book,there is a problem 2-10 which needs to prove the following result: Suppose $(M, g)$ is a Riemannian manifold, $f \in C^{\infty}(M)$ , and $X \in \mathfrak{X}(M)$ is a nowhere-vanishing vector field. Prove that $X=\operatorname{grad} f$ if and only if $X f \equiv|X|_{g}^{2}$ and $X$ is orthogonal to the level sets of $f$ at all regular points of $f .$ My attempt,the only if part is obvious,for the ""if"" part first since for regular level set $f^{-1}(c) = S$ (such that $c$ is regular value) such that $\dim(S) = \dim(M) - 1$ hence there exist a dimension 1 subspace orthogonal to $S$ (we know both $\operatorname{grad} f$ and $X$ orthogonal to $S$ ),hence for each $p \in S$ we have $kX=\operatorname{grad} f$ for some function $k$ since $Xf = |X|_g^2$ we have $ k = 1$ . I have no idea how to prove the final step,which needs to prove for each $p \in M$ (not only for those $p$ lies on some regular level set),do we need to use sard theorem ?I have no idea how to make it precise.","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4197166,Interesting combinatorics problem which can be connected to the graph theory,"For given positive integers $a>b$ , find minimum value of positive integer $n$ , such that there exist real numbers $a_1,a_2,\ldots,a_n$ , with $a_i>0$ ; There are nonempty Disjoint multisets $P_1,P_2,\ldots,P_a$ such that $S(P_i)=b$ $\forall 1\leq i \leq a$ ; There are nonempty Disjoint multisets $Q_1,Q_2,\ldots,Q_b$ such that $S(Q_i)=a$ $\forall 1\leq i \leq b$ . Comment: $P_i\subset\{a_1,\ldots,a_n\}$ and $Q_i\subset\{a_1,\ldots,a_n\}$ , also $S(A)$ is sum of all elements in $A$ . My attempt: lets $P_1,\ldots,P_a,Q_1,\ldots,Q_b$ be vertices of graph. Each vertices are connected if their corresponding sets have common element. Obviously graph is bipartite ( $P_1,\ldots,P_a$ on a left side and $Q_1,\ldots,Q_b$ on a right side). Using Hall's Theorem we have matching that covers right side. After this I tried several things but I got nothing. Please don't hesitate and suggest your own ides. Thanks!","['graph-theory', 'combinatorics', 'bipartite-graphs']"
4197209,Understanding the (Inverse) Flow of an ODE,"Background The problem is taken from section 2.5 of this paper , by Raphael Danchin. We are concerned with the following Transport Equation: $\begin{cases} \partial_t a + v\cdot\nabla a + \lambda a = f, \ \text{in } \mathbb{R^+} \times \mathbb{R^d}, \\
a|_{t=0} = a_0, \ \text{in } \mathbb{R^d}.
\end{cases}$ Here, the initial term $a_0 = a_0(x)$ , the source term $f=f(t,x)$ , the coeffictient $\lambda \geq 0$ , and the transport field $v=v(t,x)$ are all given. We assume $a\in X$ , for some Banach space $X$ , $f \in L^1_{\text{loc}}(\mathbb{R^+} ; X)$ , and $v$ is in a nice enough set (depending on choice of $X$ ) for there to be a unique solution. In particular, $v$ is at least integrable-in-time with $v(t)$ Lipschitz for all $t > 0$ , which ensures the existence of a flow, $\psi$ , with respect to $v$ . That is, $\psi$ satisfies \begin{align} \psi_0(x)  = x, \quad &  \forall x \in \mathbb{R^d} \\
\partial_t \psi_t(x)  = v(t, \psi_t(x)), \quad & \forall x \in \mathbb{R^d}, t\geq 0.
\end{align} A flow is meant to satisfy other properties as well, namely being a bijection and forming a semigroup with respect to time. These properties however will not be assumed for this question, as they are part of what I want to prove. Finally, we claim that the flow, $\psi$ , allows us to write the following explicit formula for the solution, $a$ : $$ a(t,x) = e^{-\lambda t} a_0(\psi^{-1}_t (x)) + \int_0^t e^{-\lambda (t-\tau)} f(\tau, \psi_\tau (\psi^{-1}_t (x))) \text{d}\tau. $$ My Problem I have managed to derive the formula above for $a$ , by converting from Euclidean to Lagarangian coordinates (i.e. substituting $x \to \psi_t(x)$ ), which changes the material derivative into a simple time derivative: $$ \Big{(}\partial_t + v(t,x)\cdot\nabla \Big{)} a(t,x) = \frac{\text{d}}{\text{d}t} a(t,\psi_t(x)). $$ The problem is thus reduced to an ODE, which after solving gives us the formula above when we undo our substitution to return back to Euclidean coordinates. My issue is when I try to confirm the result in Euclidean coordinates. That is, I want to explicitly write out $$ \partial_t a(t,x) = \partial_t \Bigg{(}e^{-\lambda t} a_0(\psi^{-1}_t (x)) + \int_0^t e^{-\lambda (t-\tau)} f(\tau, \psi_\tau (\psi^{-1}_t (x))) \text{d}\tau \Bigg{)}, $$ etc, and get equality on both sides of the equation that way. To do this, I need all of the terms on the left hand side to cancel out nicely. In particular, it seems that I need the following identity for the time derivative of $\psi^{-1}$ : $$ \partial_t \Big{(} \psi^{-1}_t(x) \Big{)} = -v(t,x).$$ This seems natural enough, and I think I recall proving a result like this years ago, but I seem to be running into difficulty with it now. My Attempt Our first step is to see if we can write an explicit formula for $\psi_t(x)$ when $t$ is positive. I believe the following formula should work: $$ \psi_t(x) := x + \int^t_0 v(\tau, \psi_\tau (x)) \text{d}\tau, $$ as it satisfies the two properties laid out in the Background. We then need to try and derive an inverse function from this. We note that any inverse should have the property that $\psi^{-1}_t(\psi_t(x)) = x$ , for all $x\in\mathbb{R^d}$ . Then, by our above formula for $\psi_t$ : $$ x = \psi^{-1}_t(\psi_t(x)) = \psi_t(x) - \int^t_0 v(\tau, \psi_\tau (x)) \text{d}\tau. $$ Now we replace $\psi_t(x)$ with a generic Euclidean coordinate $y$ , to get $$ \psi^{-1}_t(y) = y - \int^t_0 v(\tau, \psi_\tau ( \psi^{-1}_t(y) )) \text{d}\tau. $$ Taking the time derivative and using Leibniz's rule, however, leaves us with a messy unwanted term: $$ \partial_t \psi^{-1}_t(y) = - v(t,y) - \int^t_0 \partial_t v(\tau, \psi_\tau ( \psi^{-1}_t(y) )) \text{d}\tau. $$ My question thus is whether there is a way to show that the messy integral above turns out to be $0,$ or if I've messed up somewhere Edit 01 Just as I was writing this question out, I realised that the property I'm looking for on $\psi$ is satisfied if we replace the $\tau$ in the integrand with $t$ . That is, if we set $$ \psi_t(x) := x + \int^t_0 v(\tau , \psi_t(x)) \text{d}\tau.$$ These two possible definitions of $\psi$ don't seem to be equivalent, unless perhaps $v(t,x)$ is constant wrt $x$ along flowlines. That is, $$ v(t,x) = v(t,x_0), \ \forall t\geq 0, \ x \in \{ x \in \mathbb{R^d} \ | \ x = \psi_\tau(x_0) , \ \text{some } \tau \geq 0 \}.$$ So now my question becomes: how do I settle on a definition of $\psi$ ? Are the properties I've listed so far insufficient to find a unique formula?","['integration', 'ordinary-differential-equations', 'real-analysis', 'partial-differential-equations', 'derivatives']"
4197228,We can convert one apple to mango or vice versa in one move with equal probability. Expected moves by which only one kind of fruit will be left?,"Given $a$ apples and $b$ mangoes, where $a$ and $b$ can be non negative numbers, we can convert one apple to mango or vice versa in one move. What is the expected number of moves after which only one kind of fruit will be left, i.e, one of the following condition satisfies: #apples = $a$ + $b$ and #mangoes = 0 #apples = 0 and #mangoes = $a$ + $b$ My attempt: I thought of recurrence relation $E(a,b) = 1 + \tfrac12 E(a-1,b+1) + \tfrac12 E(a+1,b-1)$ (thanks @henry for correcting it) but could not moved further from this. Also, I thought that expected number of moves will only depend on max( $a$ , $b$ ). I am not sure but I have strong intuition that this is correct. Question Link: https://my.newtonschool.co/playground/code/crm33y2jcf/","['expected-value', 'probability']"
4197252,"Derivative of a function divided by its norm, i.e., $\phi(x) = \frac{f(x)}{\|f(x)\|}$","Setting $f:\mathbb{R}^n\to\mathbb{R}^n$ and $\|\cdot \|$ be the usual Euclidean norm. I would like to compute the derivative with respect to $x$ of $$
\phi(x)  = \frac{f(x)}{\|f(x)\|}
$$ My Attempt at a Solution $$
\nabla_x \frac{f(x)}{||f(x)||} = \frac{\nabla_x f(x)}{||f(x)||} + f(x)\nabla_x\left(f(x)^\top f(x)\right)^{-\frac{1}{2}} = \frac{\nabla_x f(x)}{||f(x)||}-\frac{1}{2}\frac{f(x)}{||f(x)||^3} 2f(x)^\top \nabla_x f(x) = \left(I - \frac{f(x)f(x)^\top}{||f(x)||^2}\right)\frac{\nabla_x f(x)}{||f(x)||}
$$ However I am very unsure about this. In particular, I have a feeling that the second term would be $f(x)^\top \nabla_x f(x)$ and hence lead to $$
\nabla_x \frac{f(x)}{||f(x)||}  = \frac{\nabla_x f(x)}{||f(x)||} - \frac{\nabla_x f(x)}{||f(x)||} = 0
$$ However this wouldn't make much sense cause surely the gradient is not $0$ for every function.","['multivariable-calculus', 'matrix-calculus', 'vector-analysis', 'partial-derivative', 'derivatives']"
4197312,"Consider all one-to-one and onto functions $f:\{1,2,3,4\} \rightarrow \{1,2,3,4\}$ which satisfy: if $f(k)$ is odd then $f(k+1)$ is even, k = 1, 2, 3","Consider all functions $f:\{1,2,3,4\} \rightarrow \{1,2,3,4\}$ which are one-one, onto and satisfy if $f(k)$ is odd then $f(k+1)$ is even, $k = 1, 2, 3$ What is the total number of such functions? The first thing that came to mind was to consider $f(k) = x^k$ . (Clearly, if $f(k)$ is odd, $f(k+1)$ is even) This function is obviously, a particular case. Any conclusions made using this approach did not help. This answer given is 12.
One of the approaches that I was suggested was to list all possible functions $f$ and then pick the ones which satisfied the condition. This was very easy to do in hindsight but is there some systematic way of doing this. Is there an approach other than brute-force?","['permutations', 'functions', 'combinatorics']"
4197341,"let $a_1=1$ and $a_n=\sin(a_{n-1})$, $n>1$ and $n \in N$ The Calculate the following Limit","Let $a_1=1$ and $a_n=\sin(a_{n-1})$ , $n>1$ and $n \in N$ If $$\lim_{n\to\infty} \frac{2^{2{a_n}}-2^{1+a_n} \cdot 3^{a_n}+3^{2a_n}}{\cos(a_n)+1-e^{a_n}-e^{-a_n}}=l\quad$$ Then calculate $l$ How to solve it without Using L'Hopital's Rule. My Approach: $$\begin{align}
a_2 &=\sin(a_1)\\
\implies a_2 &=\sin(1)\\
\implies a_3 &=\sin(\sin1)\\
\end{align}
$$ Similarly $a_n=\sin(\sin(\sin\sin(....(sin1))))$ Which will Approach to Zero for $n \to \ \infty$ $\implies$ $\lim_{n\to\infty} a_n=0$ Later I Replaced $a_n$ with $x$ So question changed to $$\lim_{x\to 0} \frac{2^{x}-2^{1+x} \cdot 3^{x}+3^{2x}}{\cos(x)+1-e^{x}-e^{-x}}=l \implies \lim_{x\to 0} \frac{(2^x-3^x)^2}{1+\cos(x)-e^x-e^{-x}}$$ How to solve it further without using L'Hopital's Rule.","['limits', 'calculus', 'limits-without-lhopital']"
4197353,Erroneous functional limit involving floor function,"I am working through Understanding Analysis 2nd Ed. and I am having some difficulty working with functional limits involving floor functions. Exercise 4.2.4: Consider the reasonable but erroneous claim that $\lim_{x \to 10} \frac{1}{\lfloor x \rfloor} = \frac{1}{10}$ (a) Find the largest $\delta$ that represents a proper response to the challenge $\epsilon = \frac{1}{2}$ . Note: $x-1 < \lfloor x \rfloor \leq x$ Rough work: $$0 < \lvert x - 10 \rvert < \delta \rightarrow \lvert \frac{1}{ \lfloor x \rfloor} - \frac{1}{10} \rvert < \epsilon$$ $$\frac{1}{10} - \epsilon < \frac{1}{ \lfloor x \rfloor} < \epsilon + \frac{1}{10}$$ $$-\frac{2}{5} < \frac{1}{ \lfloor x \rfloor} < \frac{3}{5}$$ $$ \lfloor x \rfloor < -\frac{5}{2} \quad and \quad \lfloor x \rfloor > \frac{5}{3}$$ $$ x < -3 \quad and \quad x > 2$$ I'm looking for some way to formulate $0 < \lvert x - 10 \rvert < \delta$ again in order to determine a $\delta$ . Intuitively looking at this I don't seen how a delta-ball exists because of the disjoint-ness of the inequality. This makes me think that I either 1) incorrectly applied the inversion of $\frac{1}{ \lfloor x \rfloor}$ to the inequality, or 2) this is not a good approach to solving the problem. I've been able to solve some floor function limits intuitively (just looking at the behavior in general), but I'd like to be more rigorous in my explanations. Any guidance here would be greatly appreciated!","['limits', 'real-analysis']"
4197441,"$d\langle \alpha,\beta\rangle$ for two $k$ forms $\alpha,\beta$ on a manifold","I want to write an expression for $d\langle\alpha,\beta\rangle$ where $\alpha,\beta$ are two $k$ forms on a closed Riemannian manifold say $M$ . Now I would want to write something like \begin{align*}
d\langle\alpha,\beta\rangle=\langle d\alpha,\beta\rangle+\langle\alpha,d\beta\rangle
\end{align*} But it won't be right as $d\alpha$ and $d\beta$ would become $k+1$ forms and also the end product should be a $1$ -form. What should be the correct expression and the interpretation for that? Here the inner product is the usual inner product on forms using Hodge-star operator.","['differential-forms', 'riemannian-geometry', 'differential-geometry']"
4197447,"countable subsets of [0,1]","Let $A=\{D \subset [0,1] : |D|\leq |\mathbb{N}| \}$ . is $|A| \leq \mathbb{R} ?$ . I tried finding an injection of $A$ into $P(\mathbb{Q})$ , but couldn't finish it.",['elementary-set-theory']
4197452,"Solving overdetermined, well posed, linear system of PDEs","Let $f=f(u,v)$ be a (given) solution of the following PDE, $$
\begin{equation}
\frac{\partial^2 f}{\partial u\partial v}=f,\label{1}\tag{$*$}
\end{equation}
$$ and consider the overdetermined system(s) of PDEs $$
\begin{cases}
\dfrac{\partial x}{\partial u}=f\cos\left(u-v\right)\\
\\
\dfrac{\partial x}{\partial v}=\dfrac{\partial f}{\partial v}\sin\left(u-v\right)\\
\\
\dfrac{\partial y}{\partial u}=f\sin\left(u-v\right)\\
\\
\dfrac{\partial y}{\partial v}=-\dfrac{\partial f}{\partial v}\cos\left(u-v\right).
\end{cases}
$$ Since $f$ solves \eqref{1} it is guaranteed that $$
\frac{\partial^2 x}{\partial u\partial v}=\frac{\partial^2 x}{\partial v\partial u}
$$ and similarly for $y$ , so the system is well posed. The question is, is it possible to write down a general solution for $x(u,v)$ and $y(u,v)$ in terms of integrals of $f$ ? If I try, for instance, to integrate the first equation to get $$
x=\int \mathrm{d}u\;f\cos(u-v)+g(v),
$$ with $g(v)$ some unknown function of $v$ , and then plug in the second equation for $x$ , I cannot solve for $g(v)$ , so I need to try some other ansatz.","['systems-of-equations', 'analysis', 'real-analysis', 'multivariable-calculus', 'partial-differential-equations']"
4197455,Is satisfying Lagrange's theorem sufficient to show a loop is a group? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Consider a finite loop $L$ , that is a set with an operation that has closure, identity, and invertibility (but not necessarily associativity). Consider a subloop of $L$ , that is a set $H$ contained in $L$ that is itself a loop (with the same identity element). If the order of all subloops of $L$ divide the order of $L$ (""satisfies Lagrange's theorem""), is the loop $L$ necessarily a group? Can this be proven or is there an easy counterexample of a finite loop satisfying Lagrange's theorem that is not a group?","['elementary-number-theory', 'group-theory', 'abstract-algebra']"
4197489,"How to find the ""elbow"" of a graph?","I have a function of one variable.  In this graph, we can see that there are a couple of places where the graph ""bends"" a lot -- a local maximum of ""bending"", if you will.  The ordinary second derivative measures ""bending"" in the $y$ direction, which will be greatest near the singularity (which is at $-1$ ).  However, if we measure ""bending"" orthogonal to the tangent line to the graph at a point, then the ""bending"" is maximized at the two ""elbows"" near $x=-2$ and $x=0.75$ or so.  Given a function $y=f(x)$ , how might I go about finding these elbows?  Presumably, I will set the third derivative of something to 0 and solve for $x$ , but that something is not $f$ ...",['derivatives']
4197519,The frog puzzle: simple probability isn't so simple; intuition and generalization,"There have been numerous discussions of the frog puzzle. Below is a puzzle followed by some solutions: ""You're poisoned in the jungle and the only way to save yourself is to lick a special kind of frog. To make matters worse, only the female of that species will do. Licking the male frog doesn't do anything. The male and female frogs look identical and appear with equal probabilities. The only difference is that the male frogs sometimes emit a distinctive croak. You spot a frog in front of you, but then you hear a croaking sound behind you. You turn around and spot two frogs there. There's only time to run to one side. Which way should you run?"" (1) The original solution to the puzzle gives the probability 2/3 of survival if you run to two frogs, which is wrong. The original solution assumes that the problem is equivalent to finding a female in a reduced sample space $MM$ $MF$ $FM$ and doesn't take into account the fact that you heard a croaking. (2) Another solution is that since you hear a croaking, you may represent a male frog as $M_c$ if it croaked and $M_n$ if it didn't croak, so the new sample space is $M_cM_n$ $M_nM_c$ $FM_c$ $M_cF$ , which gives a 1/2 probability that there is a female among the two frogs. (3) The correct solution (I believe) is that if the probability that a male frog croaks during a short time interval that you were listening for croaks is $p$ , then the probability that there is a female among the two frogs is $$
\begin{align*}
&P(FM\mbox{ or }MF | \mbox{one croak})\\
&=\frac{P(\mbox{one croak}|FM\mbox{ or }MF) P(FM\mbox{ or }MF)}{ P(\mbox{one croak}|FM) P(FM) +  P(\mbox{one croak}|MF) P(MF) + P(\mbox{one croak}|MM)P(MM)}\\
&=\frac{p\cdot0.5}{0.25p+0.25p+0.25p(1-p) + 0.25(1-p)p}\\
&=\frac1{2-p}
\end{align*}
$$ Here we're using Bayes' Theorem . If we look at the single frog that didn't croak, the probability that it is female is $$
\frac{ P(F)}{ P(M_n) + P(F)} = \frac{0.5}{0.5+0.5(1-p)} = \frac1{2-p}
$$ So it doesn't matter which direction you run! Does anyone have a good intuition as to why it doesn't matter which way you run? Does there exist an intuitive way to arrive at the answer in a general case: ""
There are $n$ frogs and $m$ of them croak. What is the probability that one of the frogs is female?
"" If $m=0$ , we can use the Binomial Theorem to get $$
1-\frac{(1-p)^n}{{n\choose0} + {n\choose1}(1-p) + {n\choose2}(1-p)^2+\ldots + {n\choose n}(1-p)^n}
= 1-\left(\frac{1-p}{2-p}\right)^n
$$ I suspect that the answer to the general case is $\displaystyle 1-\left(\frac{1-p}{2-p}\right)^{n-m}$ because we can just ""ignore"" the croaking frogs. I'm looking for a good intuitive explanation as to why that's true (if it is true, of course). A further generalization would be skewing the probability of a male vs. female frog. Let's say that the probability that any given frog is male is $x$ . Then the above formula for the case $m=0$ becomes $$
1-\frac{x^n(1-p)^n}{{n\choose0}(1-x)^n + {n\choose1}(1-x)^{n-1}x(1-p) + {n\choose2}(1-x)^{n-2}x^2(1-p)^2+\ldots + {n\choose n}x^n(1+p)^n}
= 1-\left(\frac{x(1-p)}{1-xp}\right)^n
$$ Is there a good answer for this more general case with $n$ frogs, $m$ of which are croaking?","['conditional-probability', 'probability']"
4197522,Example where cut locus is not the separating set,"For a Riemannian manifold $M$ the cut locus of any point $p$ , denoted by $\mathrm{Cu}(p)$ , consists of all points $q\in M$ such that if there exists a distance minimal geodesic joining $p$ to $q$ , then any extension of it beyond $q$ is not minimal. For example, the cut locus of any point in a sphere will be the corresponding antipodal point. The separating set of a point $p$ , denoted by $\mathrm{Se}(p)$ , is the collection of all points $q$ such that there exists at least two minimal geodesic joining $p$ to $q$ . By distance minimal , I mean the length of the geodesic is same as the distance between the points. A theorem (see Lemma 2) of Wolter says that the closure of the set $\mathrm{Se}(p)$ is the cut locus $\mathrm{Cu}(p)$ . My question: What are some examples where $\mathrm{Se}(p) \subsetneq \mathrm{Cu}(p)$ . In most of the classical examples both of the sets are same.","['geodesic', 'riemannian-geometry', 'differential-geometry']"
4197548,Why is it natural to define reflections in terms of its hyperplane?,There are two ways to reflect a vector $v$ by another vector $u$ . The first is to 'directly' reflect $v$ through $u$ : where $v'=2P_uv-v$ with $P_u=\frac{uu^T}{|u|^2}$ . The second way is to reflect $v$ through the normal plane spanned by $u$ like this: This time the reflection is $-1$ times the other convention i.e. $v'=v-2P_uv$ . To me the first way seems more intuitive but if I look at this article https://en.wikipedia.org/wiki/Geometric_algebra#Reflection they use the second convention. Is it common in math to use the second convention? Are there any benefits compared to the first convention?,"['geometric-algebras', 'reflection', 'convention', 'geometry']"
4197622,"In the definition of a complete statistic, what is the expectation taken over?","I have read lots of answers here attempting (and in my case, failing) to convey the idea of a ""complete statistic."" Even the idea of $E_{\theta}[g(T(x))] = 0$ is hard to parse.  What is the expectation taken over?  Different data?  Different values of $T$ ?  Different functions?  Different values of a given function? Different values of $\theta$ ? The binomial example would suggest it is values of $g(T(x))$ and their probabilities that are being summed over.  Is that correct? If so, is it still a function of $\theta$ ? I have Casella and am scrutinizing it along with lecture notes from a stats class online.  Any pointers to good references or explanations are welcome. EDIT to add detail . So I'm getting the impression that the space is some sort of space of functions that includes every distribution under consideration, I think.  So what else is in that space?  What isn't?  All functions?  All continuous functions? What are we trying to accomplish in that space?  What's the inner product? EDIT to add more detail . I created an example. Suppose I have two dice: one fair, and the other has even numbers twice as likely to come up as odd numbers.  So $\Theta$ consists of two possible values, call them $a$ and $b$ . Suppose $T(X)=1$ if $X$ is even, and $0$ otherwise. Suppose $U(i) = i$ for $i \leq 4, U(5)=3$ and $U(6) = 4$ . I have computed that both $T(X)$ and $U(X)$ are sufficient statistics, and $T(X)$ is a minimal sufficient statistic. So in addition to the desired general explanation, I would also like answered a more specific question, that I would like to see expressly calculated, is, ""Are either $T$ or $U$ complete?""",['statistics']
4197628,Pulling back divisors and example 7.6.3 in Hartshorne,"Example 7.6.3 in Hartshorne is about the following setup: Take the nonsingular cubic $X$ defined by $y^2z=x^3-xz^2$ in $\mathbb{P}_k^2$ and $P_0=(0:1:0)$ . Hartshorne claims that $\mathcal{O}_X(1)\cong L(3P_0)$ , but I don't really see why this is true. Very ample line bundle on a projective curve (also about this question) explains that the hyperplane defined by $z=0$ restricts to the divisor $3P_0$ , and we also know that $z=0$ (along with any hyperplane) corresponds to $O(1)$ (in $\mathbb{P}_k^2$ ). Now, to conclude, I want to say that the pullback of a line bundle associated to the divisor of some rational section is the same as the line bundle associated to the divisor of the pullback of this rational section, but I don't know in what generality this holds. Should I be working with Cartier or Weil divisors---it doesn't matter in this case (they are the same), but I'm interested in what one is allowed to do in a more general situation. Hartshorne also doesn't seem to address pullbacks of Weil or Cartier divisors for some reason. In any case, it seems like we need some condition like the image of the morphism isn't contained in the support (of say a Weil divisor), since otherwise trying to define the pullback just gives rise to the source scheme (which is not a Weil divisor). Is something like this also sufficient/what hypotheses do we need to have on the schemes?","['divisors-algebraic-geometry', 'algebraic-geometry', 'line-bundles']"
4197635,Guessing number of colors of beads in an urn,"Motivation from cocktail bar Every time when I order the cocktail “Latex and Prejudice” (“Латекс и предубеждение”) in the Tesla bar in Saint Petersburg (Russia) the barkeeper selects by random a small interesting photo $^1$ and attaches it with a clamp to the cocktail glass. At the beginning I got always different pictures and started to collect them. The more cocktails I ordered the more often the motifs repeated. Finally, I had drunken so much that almost every time I had to ask for a different photo. I was wondering how many different pictures there are but due to drunkenness couldn't solve the problem by myself. Mathematical form using urn model Given is an urn with an unknown number of balls that have an unknown number of colors. It is assumed that every color has equal probability. From this urn in total $n$ balls with $m$ different colors were drawn, $k_i$ balls for color $i$ were sampled, i.e. $n=\sum_{i=1}^m k_i$ . How many different colors $M$ are in the urn? Sampling with and without replacement is of interest. Open questions Some answers were already given. Now I am looking for either alternative answers or/and answers to the following more specific questions: Let's only for the first question assume that we do not know if the balls were drawn with or without replacement. Is the maximum likelihood for drawing with replacement always higher than the maximum likelihood for drawing without replacement? Is this answer helpful? If yes: Can we consider the calculated likelihoods as discrete probability distributions if they were be normalized (with support on integers in the range $(m,\infty)$ )? What can we say about variance, standard error? What can we say about variance, standard error of another answer ? Related problems In this SE post the number of colors in the urn is also unknown but in the problem given here it is assumed that the colors have equal probability. Another SE post deals with lending books from a library that were already lent at an earlier time. Annotation 2023 The bar was visited a lot of times before the war. $\small{^1 \text{Because the site is accessible to minors,$\\$ the content of the photos is not discussed here.}}$","['statistics', 'combinatorics', 'recreational-mathematics', 'multinomial-distribution', 'probability']"
4197681,"What is the variance of the product of a Bernoulli (0,1) and a normal random variable?","I want to find the variance of the product of a Bernoulli random variable and a normal random variable. I only have an introductory probability background...and here is what I would do normally. If I were dealing with the product of a discrete random variable, I would just list all the cases and find their probabilities and get $E[X], E[X^2],$ and $Var(X)$ from that. If I were dealing with a continuous random variable, I would find the CDF, differentiate to get the pdf, and find $E[X], E[X^2],$ and $Var(X)$ from that. But I'm not sure where to start with this case, where I have a piecewise continuous pdf. The answer to this question seems very relevant, except how would I work with a piecewise cdf like this? I appreciate any insight!","['expected-value', 'variance', 'normal-distribution', 'probability']"
4197691,"How many of the first $100$ terms are the same in the arithmetic sequences $2,9,16,\ldots$ and $5,11,17,\ldots$?","If $\{a_n\}$ is an arithmetic sequence with 100 terms where $a_1=2$ and $a_2=9$ , and $\{b_n\}$ is an arithmetic sequence with 100 terms where $b_1=5$ and $b_2=11$ , how many terms are the same in each sequence? I think the answer is 17, but how I got it seemed too easy and I just want someone to verify my answer. Here is how I found my solution: The sequence for $$a_n= 2, 9, 16, 23, 30, 37, 44, 51, 58, 65, 72,79,86,93,100,107,114,121,\ldots$$ The sequence for $$b_n= 5, 11,17, 23, 29, 35, 41, 47, 53, 59, 65, 71,77,83,89,95,101,107, \ldots$$ Ignoring the 1st four terms, I noticed that after the 4th term, the same terms appear in every 6th term for $a_n$ and 7th term for $b_n$ . Therefore, $100-4=96/6=16$ . Then I added $1$ to include $23$ . I hope this makes sense and thank you for your help.","['algebra-precalculus', 'sequences-and-series']"
4197698,Integration by parts on a manifold,"$M$ is a closed Riemannian manifold, I want to break the following integration on $M$ using integration by parts techniques which I am not so familiar with. \begin{align*}
\int_M\langle d\beta,\omega\rangle|\phi|^2 dV
\end{align*} Here $\phi$ is a section of a complex line bundle with a compatible metric on it, $\beta$ is a $3$ -form and $\omega$ is a Harmonic four form on the manifold.","['integration', 'differential-forms', 'differential-geometry']"
4197712,Proving the Bijectiveness of a Discrete Binomial Function (multivariable),"I am looking for a way to prove that this function $(f: \mathbb{N}^k \to \mathbb{N})$ is bijective \begin{align*}
    f((x_1, \dots, x_k)) &=  2+ \sum^k_{i=1} \binom{x_{k-i+1} + i -2}{i}
\end{align*} If conditioned by the fact that $x_1 \geq x_2 \geq \dots \geq x_k$ (also $k, x_j \in \mathbb{N}$ for all $j$ ). I will add here a picture of the $k=4$ case so that you can visualize what I mean by this. (the function $f$ gives the ranking of the trees in the sequence. I already know the function works but I am unable to prove it. Sequence of 4-furcating trees using the Colijin-Plazzota ranking I've tried induction but it doesn't seem to exactly work (this is my attempt at backwards induction, I got very confused in the end):
Let us prove this by backwards induction. Note that for our base case, when $x_k \ne y_k$ (and $x_1 = y+1, \dots, x_{k-1}=y_{k-1}$ ), the two expressions $2+ \sum^k_{i=1} \binom{x_{k-i+1} + i -2}{i}$ are identical, except for the term $i=1$ that yields \begin{align*}
    \binom{x_k - 1}{1} &= x_k - 1\\
    &\ne y_k - 1\\
    &\ne \binom{y_k - 1}{1}
\end{align*} Thus our base case holds and $f(x) \ne f(y)$ . Now assume that $f(x) \ne f(y)$ for $x_j \ne y_j$ and that for any $x_i,y_i$ where $i < j$ , $x_i = y_i$ . Now let us prove that  for $x_{j-1} \ne y_{j-1}$ (and that for any $x_i,y_i$ where $i < j-1$ , $x_i = y_i$ ), $f(x) \ne f(y)$ . \begin{align*}
    f((x_1, \dots, x_k)) &=  2+ \binom{x_{k} + 1 -2}{1} + \binom{x_{k-1} + 2 -2}{2} + \dots \binom{x_{k-k+1} + i -k}{k}
\end{align*} $$=2+ \binom{x_{k} + 1 -2}{1} + \dots  + \binom{x_{j} + (k+1-j) -2}{(k+1-j)} + \binom{x_{j-1} + (k+2-j) -2}{(k+2-j)} + \dots  + \binom{x_{k-k+1} + i -k}{k}$$","['trees', 'functions', 'combinatorics', 'discrete-mathematics']"
4197720,What matrix functionals are invariant under change of basis?,"Fix some integer $n$ , and consider the linear space $M(n,\mathbb F)$ of square $n\times n$ matrices in some field $\mathbb F$ . Let $f:M(n,\mathbb F)\to\mathbb F$ be a functional that is invariant under change of basis, that is, such that $f(PAP^{-1})=f(A)$ for any $A,P\in M(n,\mathbb F)$ with $P$ invertible. Standard examples are $f(A)=\det(A)$ and $f(A)=\operatorname{tr}(A)$ . More generally, any function defined via the eigenvalues of $A$ is another example of this.
Are these the only possible such examples? In other words, can we characterise the set of possible functionals $M(n,\mathbb F)\to\mathbb F$ that are invariant under change of basis as being all and only those functions that can be defined from the eigenvalues of the matrix? I'm mostly interested to the cases $\mathbb F=\mathbb R,\mathbb C$ , but I'm leaving this question general because I don't know if this assumption is relevant to the discussion.","['determinant', 'trace', 'matrices', 'linear-algebra', 'similar-matrices']"
4197734,Show that in general $A \cup (B \times C) \neq (A \cup B) \times (A \cup C)$.,"I have been reading the real analysis textbook ""Modern Real Analysis"" by William Ziemer.
I have come to an exercise that seemed strange to me. I was able to answer it I believe, but it seemed to me that there may be an error in the book here. The question is : Show that $A \times (B \cup C) = (A \times B) \cup (A \times C)$ .
Also, show that in general, $A \cup (B \times C) \neq (A \cup B) \times (A \cup C)$ . I was able to do the first part easily, but I was confused by the second. It seems easy to construct an example where the equality doesn't hold. For instance : \begin{align}
A & = \{0,1\}\\
B & = \{2,3\}\\
C & = \{4,5\}
\end{align} So : \begin{align}
A \cup (B \times C) 
  & = \{0,1\} \cup \{ (2,4) , (2,5) , (3,4) , (3,5) \}\\
  & = \{0,1,(2,4),(2,5),(3,4),(3,5) \}
\end{align} and : \begin{align}
A \cup B & = \{0,1,2,3\}\\
A \cup C & = \{0,1,4,5\}
\end{align} So : \begin{equation}
0 \not \in (A \cup B) \times (A \cup C)
\end{equation} and therefore : \begin{equation}
A \cup (B \times C) \neq (A \cup B) \times (A \cup C) \; \checkmark
\end{equation} But this second part of the problem seems strange. Is this a likely error in the book, or is there something that I am not seeing ?",['elementary-set-theory']
4197763,Wave Equation Via d'Alambert,"$$u_{tt} = 4u_{xx},\ u(x,\ 0) = \sin(\pi x),\ u_t (x,\ 0) = 0.$$ I factored the differential operators and used undetermined coefficients (can go into more detail on this part but I think it matters not) to find the general solution $$u = h(2t + x) + g(2t - x)$$ for arbitrary functions $h$ and $g$ .  Applying the initial conditions yields $$\begin{cases}\sin(\pi x) = h(x) + g(-x) \\ 0 = \frac{\partial}{\partial t}[h(2t + x) + g(2t - x)]_{t = 0}\end{cases}$$ which will require the second equation to be antidifferentiated with respect to $t$ in order for the system to be solved simultaneously. Thus, I end up with $$\begin{cases}\sin(\pi x) = h(x) + g(-x) \\ f(x) = \int \frac{\partial}{\partial t}[h(2t + x) + g(2t - x)]_{t = 0}\,\mathrm dt\end{cases}$$ for an arbitrary function $f$ . I'm not really sure how to finish solving this. I could make the substitutions $y = 2t + x$ and $z = 2t - x$ , leading to $$f(x) = 2 \int \left[\frac{\partial h(y)}{\partial y} + \frac{\partial g(z)}{\partial z}\right]_{t = 0}\,\mathrm dt,$$ but other than getting the $2$ from the chain rule derivatives pulled out, I'm not sure how to continue. I don't think I can just cancel the partial differentiation with the antidifferentiation, because of the $t = 0$ evaluation operation. How can I finish solving these equations?","['systems-of-equations', 'multivariable-calculus', 'wave-equation', 'partial-differential-equations', 'chain-rule']"
4197764,Complemented set in X***,"Let X be a normed space, then prove that the image of $J:X^* \rightarrow X^{***}$ is a complemented subset of $X^{***}$ My attempt: I know that the image of the canonical embedding will be closed in $X^{***}$ but I can't see how to show it's a complemented subspace, though of proving that the codimension of $Im(J)$ has finite dimension.","['banach-spaces', 'normed-spaces', 'functional-analysis']"
4197777,Stopping time random walk in a circle,"I've been trying to solve this question for some time now, and I could not figure out the second part of the exercise. Here it is: Consider a circle with five markings 0, 1, 2, 3, 4, in that order. Suppose that a particle starts at state 0 and moves to state 1 or 4 with probability 1/2 each. Subsequent movements are to one of its neighbors with equal probability. Let $X_{n}$ be the state of the particle at time $n \geq 0$ . Let $$ 
\tau = \min \{n \geq 1: X_{n}=0 \} \text{ and } V_{3} = \sum_{n=1}^{\tau}I(X_{n}=3)
$$ a) Compute $E(\tau)$ b) Compute $E(V_{3})$ For part (a) I'm using that $X_{n}$ is an irreducible Markov chain with finite number of states $S = \{0,1,2,3,4\}$ . It follows this chain is positive-recurrent and has a unique stationary distribution, which is $$
\pi = \left( \frac{1}{|S|},\dots,\frac{1}{|S|} \right)
$$ The mean recurrence time for any of the states is then $$
\mu_{i} = \frac{1}{\frac{1}{|S|}} = |S|, i \in S
$$ From this I got $E[\tau] = |S| = 5$ since $\tau$ is the time it takes to go back to state 0 after the chain started there. I wonder how can I solve this using recurrence, like in similar Markov chain problems. For part (b) I thought about using total expectation but I'm not sure how to move forward. This is what I have so far $$
E[V_{3}] = E[E[V_{3}|\tau=t]] = E \left[E \left[ \left. \sum_{n=1}^{\tau}I(X_{n}=3) \right| \tau=t\right] \right] =  E\left[ \sum_{n=1}^{\tau}P(X_{n} = 3) \right]
$$ Any suggestions will be appreciated. EDIT: I added my attempt for an answer using @Joe suggestions. Let me know if you have any comments.","['circles', 'markov-chains', 'stopping-times', 'probability-theory', 'probability']"
4197794,Characterzation of $I$-adic topologies,"Let $I,J$ be two ideals of a ring $A$ . We can define the $I$ -adic topology on $A$ by declaring $$\{x+ I^n \}_{x \in A, n \ge 0 }$$ be a basis of open neighboord for $A$ . (similarly for $J$ . It seems to me that: the $I$ -adic topology aand $J$ -adic topology on $A$ coincide $\Leftrightarrow Spec A/I $ is equal as sets to $Spec A/J$ . I can show $\Rightarrow$ . : This follows aas exists $n$ $I^n \subset J, J^n \subset I$ . Can one deduce the converse?","['ring-theory', 'general-topology', 'algebraic-geometry', 'commutative-algebra']"
4197812,"Exercise 20 in the book ""generatingfunctionology""","This will be a quick one: In the book ""generatingfunctionology"" there is an exercise that is formulated as follows: ""Let $f(n,m,k)$ be the number of strings of $n$ $0$ ’s and $1$ ’s that contain exactly $m$ $1$ ’s, no $k$ of which are consecutive"". Now, I might just be slightly mathematically illiterate (or maybe he just wrote it in a clunky way), but I have a hard time understanding exactly what he means. Is it: We got a string that in total contain $n$ $0$ 's and $1$ 's, $m$ of which are $1$ 's (so $n-m$ are $0$ 's and $f(n,m,k)=0$ for $n<m$ ) and of those $m$ $1$ 's we do not want $k$ of them to be in succession so $(f,m,0)$ and $f(n,m,1)$ are both equal to $0$ when $m$ is not equal to $0$ (since otherwise we cannot even have one $1$ next to anything?) I am not sure how to read ""no $k$ of which are consecutive"" otherwise. So for example $f(3,2,2) = 1$ as we got $1\,0\,1$ and $f(3,2,3) = 3$ as we got $1\,1\,0$ , $\,0\,1\,1$ , and $1\,0\,1$ . I just need to make sure before I start the exercise for real. Thanks.","['combinatorics', 'discrete-mathematics', 'generating-functions']"
4197828,A function with cubic variation,"Let $W(\omega,x):\Omega\times\mathbb{R}\rightarrow\mathbb{R}$ be a Brownian motion where $\Omega$ is the sample space. Recall that the quadratic variation of $W$ over the interval $[a,b]$ equals $b-a$ almost surely, that is to say: $$\langle W \rangle = \lim_{n\rightarrow\infty}\sum_{i=1}^{n} \left| W\left(\omega,a+\frac{i}{n}(b-a)\right)-W \left(\omega,a+\frac{i-1}{n}(b-a)\right) \right|^2 = b-a$$ almost surely. Now consider some continuous function (this time not a random variable) $f:\mathbb{R}\rightarrow\mathbb{R}$ and consider the more general limit: $$\langle f \rangle = \lim_{n\rightarrow\infty}\sum_{i=1}^{n} \left| f\left(a+\frac{i}{n}(b-a)\right)-f \left(a+\frac{i-1}{n}(b-a)\right) \right|^p $$ I would like to know if there exists a continuous function $f$ for which $\langle f \rangle$ is finite but non zero for a value of $p$ greater than 2. For example, does there exist a function $f$ of finite but non zero 'cubic variation' ( $p=3$ ). More generally, for which values of $p$ do such functions $f$ exist.","['p-variation', 'quadratic-variation', 'real-analysis', 'brownian-motion', 'probability-theory']"
4197860,Curvature and Torsion for a Space Curve,"Calculate the curvature and torsion of x= $\theta - \sin \theta, y = 1 - \cos \theta, z = 4 \sin (\theta/2)$ $\vec r = (θ - \sin θ)i+(1-\cos θ)j + 4 \sin (θ/2) \ k$ $\vec dr/dt = (1- \cos θ)i + \sin θ \ j+2 \cos (θ/2) \ k$ $d^2\vec r/dt^2 = \sin θ \ i + \cos θ \ j - sin (θ/2) \ k$ Since $k = |\vec dr/dt \times d^2\vec r/dt^2| \  / \ |\vec dr/dt|^3$ I calculated $\vec dr/dt \times d^2\vec r/dt^2$ , which upon simplification gave, $\vec dr/dt \times d^2\vec r/dt^2 = (\sin θ \sin (θ/2) - 2 \cosθ \cos(θ/2)) \ i + (3 \cos θ + 1) \sin (θ/2) \ j + (\cos θ-1) \ k$ In calculating | $\vec dr/dt$ x $d^2$$\vec r/d$$t^2$ | is where I'm starting to face complications. I'm not sure if I am doing this correctly or not. But I understand the application of the formulas of torsion and curvature. Kindly guide me for the same.","['multivariable-calculus', 'curvature']"
4197863,"1 out of 1000 boats get hit by lightning each year, if you boat an average amount each year for 20 years what is the probability of getting struck?",Having a discussion with my father over this. We found a article from an insurance company that says 1 out of 1000 boats get struck by lightning each year. He says that the probability of getting struck over a 20 year period is 1/50 but I say it is not. Could someone settle our debate and include the math to back it up?,"['statistics', 'probability-theory', 'probability']"
4197969,Evaluating $\int_{-\infty}^{\infty} \frac{z\ln(1+e^z) }{(1+z^2)^2}dz = \frac{\pi}{4}$ - how to handle branch cuts?,"My aim is to evaluate the integral $$I = \int_{-\infty}^{\infty} \frac{z\ln(1+e^z) }{(1+z^2)^2}dz = \frac{\pi}{4}$$ directly with contour integration. How can I do this? In particular, how should I handle the many branch points associated with $z=(2n+1)\pi i$ for each integer $n$ ? On a naive attempt, I equated $I$ with a semi-circle contour integral of the same function in the upper half plane (the function should be decreasing in $|z|$ fast enough so that the arc won't contribute), which would pick up the residue from $z=i$ . Naively, that would be $2 \pi i \frac{d}{dz} \frac{z\ln(1+e^z)}{(1+z^2)^2}$ evaluated at $z=i$ , which would yield $\frac{\pi}{2(1+e^{-i})}$ . Unfortunately, there shouldn't be an imaginary part, but the real part looks ok. Normally with such occurrences, I would just drop the imaginary part and continue. However, I want to understand what's going wrong, and how to draw an appropriate contour or identify additional contributions.","['complex-analysis', 'contour-integration', 'definite-integrals', 'branch-cuts']"
4197990,Trying to find a NICE form of : $\sum_{m=1}^{n}\lfloor\log_2m\rfloor$ [ Mathematical Gazette 2002 ],"$Q.$ Find a NICE form of : $$\sum_{m=1}^{p}\lfloor\log_2m\rfloor$$ APPROACH : We have , $$\lfloor\log_21\rfloor⠀\color{red}{\lfloor\log_22\rfloor}⠀\lfloor\log_23\rfloor⠀\color{red}{\lfloor\log_24\rfloor}⠀\lfloor\log_25\rfloor⠀\lfloor\log_26\rfloor⠀\lfloor\log_27\rfloor⠀\color{red}{\lfloor\log_28\rfloor}⠀........⠀\lfloor\log_2n\rfloor$$ $$0⠀⠀⠀⠀⠀⠀\color{red}1⠀⠀⠀⠀⠀⠀1⠀⠀⠀⠀⠀⠀\color{red}2⠀⠀⠀⠀⠀⠀2⠀⠀⠀⠀⠀2⠀⠀⠀⠀⠀⠀2⠀⠀⠀⠀⠀⠀\color{red}3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀$$ We know that : $$\lfloor\log_22^k\rfloor=k$$ STEP 1 : Sum of terms b/w $\lfloor\log_22^{k-1}\rfloor\to\lfloor\log_22^k\rfloor$ . Number of terms b/w $2^{k-1}\to2^k$ is $2^{k-1}$ . Hence the  sum upto $\underline{\lfloor\log_22^{k-1}\rfloor\to\lfloor\log_22^k\rfloor}$ will be : $$S_{2^{k-1}\to2^k}=\lfloor\log_22^{k-1}\rfloor2^{k-1}+\lfloor\log_22^{k}\rfloor$$ $$S_{2^{k-1}\to2^k}=(k-1)2^{k-1}+k$$ Now sum upto $\underline{\lfloor\log_21\rfloor\to\lfloor\log_22^k\rfloor}$ : $$S_{ \lfloor\log_22^k\rfloor}=k+\underbrace{\sum_{r=1}^{k}(k-1)2^{k-1}}_\phi$$ $\Rightarrow$ Now we'll solve for $\phi$ , $$\phi=0.2^0+1.2^1+2.2^2+3.2^3+.......+(k-1)2^{k-1}$$ $$\phi=2+2.2^2+3.2^3+.......+(k-1)2^{k-1}⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀(1)$$ $$\frac{\phi}{2}=1+2.2+3.2^2+.......+(k-1)2^{k-2}⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀(2)$$ Substracting $eq^n(1)$ from $eq^n(2)$ $$-\frac{\phi}{2}=\underbrace{1+2+2^2+2^3+.......+2^{k-2}}_{n-1⠀terms⠀(GP)}-(k-1)2^{k-1}$$ $$-\frac{\phi}{2}=(2^{k-1}-1)-(k-1)2^{k-1}$$ $$\phi=2((k-1)2^{k-1}-2^{k-1}+1)$$ $$\phi=2^k(k-2)+2$$ $\Rightarrow$ Using the value of $\phi$ in $S_{ \lfloor\log_22^k\rfloor}$ : $$S_{ \lfloor\log_22^k\rfloor}=k+2^k(k-2)+2$$ STEP 2 : Let there be a term $\lfloor\log_2p\rfloor$ b/w $\lfloor\log_22^{k-1}\rfloor\to\lfloor\log_22^k\rfloor$ : $$S_{\lfloor\log_2p\rfloor}=S_{ \lfloor\log_22^k\rfloor}-\lfloor\log_22^k\rfloor-\delta$$ Here $\delta$ is the sum of terms b/w $\lfloor\log_2p\rfloor$ and $\lfloor\log_22^k\rfloor$ . Number of terms b/w $\lfloor\log_2p\rfloor$ and $\lfloor\log_22^k\rfloor$ is $(2^k-p)-1$ Hence $\delta$ : $$\delta=(2^k-p-1)\lfloor\log_2p\rfloor$$ Now , $$S_{\lfloor\log_2p\rfloor}=k+2^k(k-2)+2-\lfloor\log_22^k\rfloor-(2^k-p-1)\lfloor\log_2p\rfloor$$ Here are some important conditions : $$ \lfloor\log_22^k\rfloor=k$$ $$\lfloor\log_22^{k-1}\rfloor=\lfloor\log_2p\rfloor$$ $$\lfloor\log_22^k\rfloor=\lfloor\log_22^{k-1}\rfloor+1=\lfloor\log_2p\rfloor+1$$ FINALLY : after further simplifying , I got  : $$\sum_{m=1}^{p}\lfloor\log_2m\rfloor=\lfloor\log_2p\rfloor(p+1)-2^{\lfloor\log_2p\rfloor+1}+2$$ DOUBT : Is there a simpler OR better approach than this ?","['ceiling-and-floor-functions', 'summation', 'logarithms', 'sequences-and-series', 'algebra-precalculus']"
4198011,"Functoriality of associating to a group algebra $kG$ its center $Z(kG)$, where $G$ is finite","It's well known that associating a group $G$ to its center $Z(G)$ is not functorial (read: doesn't extend to give us a functor $\mathsf{Grp} \to \mathsf{Ab}$ ). A simple counterexample is given by considering the inclusion of $C_2 \hookrightarrow S_3$ , which has a retract given by $S_3 \to S_3^{\text{ab}} = S_3/A_3 \cong C_2$ . You arrive at a contradiction for the functoriality of $Z(-)$ because $Z(S_3)$ is trivial. You can find more on this here https://math.stackexchange.com/a/3380460/395669 . Along these lines, one can construct a counterexample to show the failure of functoriality of $Z(-)$ for $k$ -algebras (read: doesn't extend to give us a functor ${}_k\mathsf{Alg} \to {}_k\mathsf{CommAlg}$ ), where $k$ is a unitcal commutative ring, by considering the algebras $k[F_1]$ and $k[F_2]$ . Here, $F_1$ is the free group with one generator $x$ seen as a subgroup of $F_2$ , the free group with two generators $x$ and $y$ . Then, as above, the inclusion $k[F_1] \hookrightarrow k[F_2]$ has a retract given by $k[F_2] \to k[F_2]/(y-x) \cong k[F_1]$ . You arrive at a contradiction for the functoriality of $Z(-)$ because $Z(k[F_2])$ is trivial, while $k[F_1]$ is a commutative $k$ -algebra and hence its center is itself. All this fine, what I've been unable to do is come up with a counterexample that shows the failure of $Z(-)$ to be a functor in the case of group algebras, when the group is finite. Here, the center of a group algebra has a different flavour, since it's a free $k$ -module. Any ideas? Addendum: A recent reading gave me a very partial answer to this question, which may or may not have a connection with Hochschild Cohomology. For a $k$ -algebra $A$ , we can talk about its commutator space $[A,A]$ defined as the $k$ -submodule generated by $[a,b] = ab - ba$ . Then for obvious reasons given an algebra map $A \to B$ , we get an induced map $A/[A,A] \to B/[B,B]$ . So we have a functorial construction here. Now, in the case of a group algebra $kG$ for a finite group $G$ , we define the $k$ -dual of $Z(kG)$ obviously as $Z(kG)^* = \mathrm{Hom}_k(Z(kG),k)$ . Then, we have a $k$ -module isomorphism $kG/[kG,kG] \cong Z(kG)^*$ . And so, given an algebra map $kG \to kH$ , we do get a map $Z(kH) \to Z(kG)$ , which is $k$ -linear but not necessarily multiplicative. So, the center gives a contravariant functor but not quite straightforward.","['group-theory', 'representation-theory', 'category-theory']"
4198025,Suspension homomorphism,"Let $(X,x_0)$ be a pointed space. Consider the suspension map $\Sigma : \pi_i(X,x_0) \longmapsto \pi_{i+1}(SX,x_0)$ , where $SX \simeq X \times I/(X \times \partial I \cup x_0 \cup I)$ . In order to prove that the suspension map is an homomorphism we need the following : The topological cone $CX$ is contractible $CX/X \simeq SX$ The following diagramm commutes : $\require{AMScd}$ \begin{CD}
    0=\pi(CX,\star)  \\
    @AAA\\
     \pi_i(X,\star)@>{\Sigma}>> \pi_{i+1}(SX,\star) \\
     @AA\partial A @VVV \\
 \pi_{i+1}(CX,X,\star) @>{q_{*}}>> \pi_{i+1}(CX/X,\star) \\
@AAA \\
0=\pi_{i+1}(CX,\star)
\end{CD} My question concerns understanding those three points better. It thinks the idea behind 1. is to deformation retract onto the base point, but I think the question has already an answer here the cone is contractible , so this point is okay. I don't understand if the commutativity of the diagram follows from some naturality of long exact sequences in homotopy. I recognise the sequence of the pair $(X,CX)$ on the vertical left arrow but I don't see how to fit the vertical arrow on the right properly in order to have a commutative diagram. Any help or hint on this and also of point 2. (which I don't even visualize) would be appreciated.","['exact-sequence', 'retraction', 'homotopy-theory', 'general-topology', 'algebraic-topology']"
4198049,"Is the following ""generalized version of distributive law of sets"" true?","Consider a collection of sets $\mathcal{T}_{i, s_i} \subseteq \mathbb{R}^n$ with $i \in \{0,\ldots,k\} =: \mathcal{I}$ and $s_i \in \mathcal{S}_i$ , where $\mathcal{I}$ is a finite set and $\mathcal{S}_i$ is an uncountable set. Then, does the following hold? \begin{gather*}
\bigcap_{i=0}^k \bigcup_{s_i \in \mathcal{S}_i} \mathcal{T}_{i,s_i} = \bigcup_{(s_0,\ldots,s_k) \in \mathcal{S}_0\times\cdots\times\mathcal{S}_k}\bigcap_{i=0}^k \mathcal{T}_{i,s_i}\tag{1}
\end{gather*} Thanks very much! PS: I have provide a proof (using mathematical induction) here, but not sure if it is rigorous enough. Base case: For $k = 0$ , $\text{LHS of}\ (1) = \bigcup_{s_0 \in \mathcal{S}_0} \mathcal{T}_{0,s_0}$ and $\text{RHS of}\ (1) = \bigcup_{s_0 \in \mathcal{S}_0} \mathcal{T}_{0,s_0}$ . Thus, (1) holds for $k = 0$ . Inductive step: Assume $(1)$ holds for $k = j \geq 0$ . For $k = j+1$ , we have \begin{align*}
\bigcap_{i=0}^{j+1} \bigcup_{s_i\in\mathcal{S}_i} \mathcal{T}_{i,s_i} &= \left(\bigcap_{i=0}^{j} \bigcup_{s_i\in\mathcal{S}_i} \mathcal{T}_{i,s_i}\right) \cap \left(\bigcup_{s_{j+1}\in\mathcal{S}_{j+1}} \mathcal{T}_{j+1,s_{j+1}}\right)\\
&\stackrel{(a)}{=} \left(\bigcup_{(s_0,\ldots,s_{j}) \in \mathcal{S}_0\times\cdots\times\mathcal{S}_{j}}\bigcap_{i=0}^{j} \mathcal{T}_{i,s_i}\right) \cap \left(\bigcup_{s_{j+1}\in\mathcal{S}_{j+1}} \mathcal{T}_{j+1,s_{j+1}}\right)\\
&\stackrel{(b)}{=} \bigcup_{(s_0,\ldots,s_{j}) \in \mathcal{S}_0\times\cdots\times\mathcal{S}_{j}} \left[\left(\bigcap_{i=0}^{j} \mathcal{T}_{i,s_i}\right) \cap \left(\bigcup_{s_{j+1}\in\mathcal{S}_{j+1}} \mathcal{T}_{j+1,s_{j+1}}\right)\right]\\
&\stackrel{(c)}{=} \bigcup_{(s_0,\ldots,s_{j}) \in \mathcal{S}_0\times\cdots\times\mathcal{S}_{j}} \left[\bigcup_{s_{j+1}\in\mathcal{S}_{j+1}}\left(\left(\bigcap_{i=0}^{j} \mathcal{T}_{i,s_i}\right) \cap \mathcal{T}_{j+1,s_{j+1}}\right)\right]\\
&= \bigcup_{(s_0,\ldots,s_{j}) \in \mathcal{S}_0\times\cdots\times\mathcal{S}_{j}} \left[\bigcup_{s_{j+1}\in\mathcal{S}_{j+1}}\left(\bigcap_{i=0}^{j+1} \mathcal{T}_{i,s_i}\right)\right]\\
&= \bigcup_{(s_0,\ldots,s_{j+1}) \in \mathcal{S}_0\times\cdots\times\mathcal{S}_{j+1}}\bigcap_{i=0}^{j+1} \mathcal{T}_{i,s_i},
\end{align*} where $(a)$ follows from $(1)$ ; $(b)$ and $(c)$ are established by the distributive law of sets that $$(\mathcal{A}_1\cup\cdots\cup\mathcal{A}_m) \cap \mathcal{B} = (\mathcal{A}_1\cap\mathcal{B}) \cup \cdots \cup (\mathcal{A}_m\cap\mathcal{B}).$$ Thus, $(1)$ holds for $k = j+1$ .",['elementary-set-theory']
4198055,Is a commutative monoid with a $\mathbb Z$-action an abelian group?,"Let $M$ be a commutative monoid and let there be an action of $\mathbb Z$ , i.e., a function $\mathbb Z\times M\to M$ such that for $r,s\in\mathbb Z$ and $m,n\in M$ we have: \begin{equation}
\left(r+s\right)\cdot m=r\cdot m+s\cdot m\\
r\cdot\left(m+n\right)=r\cdot m+r\cdot n\\
r\cdot\left(s\cdot m\right)=\left(rs\right)\cdot m\\
1\cdot m=m
\end{equation} Then is $M$ actually an abelian group with inverses given by $\left(-1\right)\cdot m$ ? Note that such an action would be extension of the natural action of $\mathbb N$ on $M$ .","['monoid', 'modules', 'ring-theory', 'abstract-algebra', 'abelian-groups']"
4198074,Space of compact operators is the only proper closed two sided ideal of the space of all bounded operators.,"Let $\mathcal H$ be a Hilbert space. Then $\mathcal K(\mathcal H)$ is the only proper closed two sided ideal of $\mathcal B(\mathcal H).$ I am following Rajendra Bhatia's notes on Functional Analysis. The following hint has been given in the book. Let $\mathcal I$ be a proper closed two sided ideal of $\mathcal B(\mathcal H).$ If $\mathcal I$ contains a positive operator $A$ that is not compact then there exists $\varepsilon \gt 0$ such that $P(\varepsilon, \infty)$ is infinite-dimensional, where $P$ is the projection valued measure associated with $A$ . Let $\mathcal M$ be it's range and let $V$ be the unitary operator from $\mathcal H$ onto $\mathcal M.$ Since $A(\mathcal M) = \mathcal M,$ we have $$V^*AV (\mathcal H) = V^*A(\mathcal M) = V^*(\mathcal M) = \mathcal H.$$ Then for every $x \in \mathcal H$ we have $$\|V^*AV x\| \geq \varepsilon \|x\|.$$ Hence $V^*AV$ is invertible and $V^*AV \in \mathcal I.$ So it follows that $\mathcal I = \mathcal B (\mathcal H).$ In the above proof I don't understand few things. Here they are $:$ $(1)$ Why does there exist $\varepsilon \gt 0$ such that the range of the projection $P(\varepsilon, \infty)$ is infinite-dimensional? $(2)$ How to guarantee the existence of an unitary operator $V : \mathcal H \longrightarrow \mathcal M\ $ ? $(3)$ Why do we have $A(\mathcal M) = \mathcal M\ $ ? I have understood the rest of the part in the proof quite clearly. Could anyone please help me understanding the proof? Thanks for your time. EDIT $:$ I have managed to answer the first question. Here it is $:$ Suppose for every $\varepsilon \gt 0,$ the range of $P(\varepsilon,\infty)$ is finite dimensional. Then for every $n \in \mathbb N,$ $AP\left (\frac {1} {n}, \infty \right )$ is a finite rank operator. Now we have $$AP \left (\frac {1} {n},\infty \right ) = \left (\int t\ dP \right ) \left (\int \chi_{\left (\frac {1} {n}, \infty \right )}\ dP \right ) = \int t\ \chi_{\left (\frac {1} {n} , \infty \right )}\ dP.$$ Since the integrand $t\ \chi_{\left (\frac {1} {n} , \infty \right )}$ increases to $t,$ by MCT we have $$A P \left (\frac {1} {n}, \infty \right ) \xrightarrow {\text {NORM}} A.$$ But then $A$ becomes compact as it is the norm limit of a sequence of finite rank operators, a contradiction. Hence there should exist some $\varepsilon \gt 0$ such that the range of $P(\varepsilon, \infty)$ is infinite-dimensional. For $(3)$ we first note that $$\begin{align*}AP \left (\varepsilon,\infty \right ) & = \left (\int t\ dP \right ) \left (\int \chi_{\left (\varepsilon, \infty \right )}\ dP \right ) \\ & = \int t\ \chi_{\left (\varepsilon , \infty \right )}\ dP \\ & = \int \chi_{\left (\varepsilon , \infty \right )}\ t\ dP \\ & = \left (\int \chi_{\left (\varepsilon, \infty \right )}\ dP \right ) \left (\int t\ dP \right ) \\ & = P(\varepsilon, \infty) A \end{align*}$$ Now for any $x \in \mathcal M = \text {ran} \left (P(\varepsilon,  \infty) \right )$ we have $$Ax  = A P(\varepsilon, \infty) x = P (\varepsilon,  \infty) A x \in \mathcal M$$ proving that $A(\mathcal M) \subseteq \mathcal M.$ But I don't know how to show the reverse inclusion i.e. how to show that $\mathcal M \subseteq A(\mathcal M).$ RE-EDIT $:$ For the other part of the inclusion in $(3)$ we first note that the map $t \mapsto \frac {1} {t}$ is a bounded measurable function on $(\varepsilon,  \infty)$ and hence we have $$P(\varepsilon,  \infty) = \displaystyle {\int_{(\varepsilon, \infty)} dP = \left (\int_{(\varepsilon, \infty)} t\ dP \right ) \left (\int_{(\varepsilon, \infty)} t^{-1}\ dP \right ) = A E(\varepsilon, \infty) B}$$ where $B = \displaystyle {\int_{(\varepsilon, \infty)} t^{-1}\ dP.}$ This shows that $\mathcal M \subseteq A(\mathcal M).$","['measure-theory', 'compact-operators', 'proof-explanation', 'ideals', 'spectral-theory']"
4198104,"Maximum possible value of a positive integer $n$, such that for any choice of seven distinct elements from ${1, 2, .., n},$ there will exist","What is the maximum possible value of a positive integer $n$ , such that for any choice of seven distinct elements from ${1, 2, ..., n},$ there will exist two numbers $x$ & $y$ satisfying $1 < x/y \leq 2$ What seems unclear to me are the following points, A) Is $2y$ supposed to be $\leq$ n ? B) Is there some standard approach to find the maximum possible value? Moreover, usually, I try to take a small set to try out the given conditions. In this case, is it possible to take a smaller set and use it to draw conclusions for the larger set? The answer is $2^7 - 2$","['algebra-precalculus', 'functions', 'combinatorics']"
4198143,Solve this equation: $3^{3x} - 3^x = (3x)!$,I have this equation: $$3^{3x} - 3^x = (3x)!$$ We have to solve for $x$ integer. I did try to attempt but to no avail. I can't manipulate any side of this equation. I took common $3^x$ in the LHS of the equation and got a product: $(3^x) (3^{2x}-1)$ but I have no idea what to do in the RHS of the equation (which is a factorial). It looks like the answer is $x=2$ but I want to solve it algebraically. Any hints/solution would be greatly appreciated.,['algebra-precalculus']
4198146,Computing the eigenvalues of the precision operator $C_0^{-1}=\eta(-\triangle)^p+KI$,"Consider $L_2(\mathbb{T})$ with the basis $$\phi_{2k}(x)=\sqrt{2}\cos(2\pi k x)\\ \phi_{2k-1}(x)=\sqrt{2}\sin(2\pi k x)$$ for $k\in\mathbb{N}$ . The functions $\phi_k$ belong to the domain $H^{2p}$ of the operator $C_0^{-1}=\eta(-\triangle)^p+KI$ where $\triangle$ is the Lapalcian and I the identity,K and $\eta$ are constants.
A simple application of the operator on the basis above yields: $$C_0^{-1}\phi_{2k-1}=\eta((4\pi^2k^2)^p+k)\phi_{2k}\\C_0^{-1}\phi_{2k}=\eta((4\pi^2k^2)^p+K)\phi_{2k-1} $$ It follows that $C_0$ is the operator on $L_2(\mathbb{T})$ which is diagonalized by the basis of $\phi_k$ , with eigenvalues: $$\lambda_k=\eta \left(\left( 4\pi^2\left[\frac{k}{2}\right]^2\right)^p +K\right)^{-1} $$ I am not understanding how the eigenvalues are computed in order to diagonalize the operator. I cannot visualize a matrix that contains the operator. Question: How do I compute the eigenvalues $\lambda_k$ ? Thanks in advance!","['linear-algebra', 'functional-analysis', 'eigenfunctions']"
4198175,Maximum number of members in the math club,"Problem: Professor Liyung wants to make a math club consisting of his $40$ students. But there is a problem. Every student is enemies with two other students. And no one wills to be a member of the club if any of his enemies is already a member of the club. Let $M$ be the maximum number of members the club can have. Find the sum of all possible values of $M$ . (If student $A$ is an enemy with student $B$ , then student $B$ is an enemy with student $A$ .) This is a problem from a local math contest in my city held on last month. Here is the solution I think of : My thoughts: For every student chosen to be in the club, two students can't be a member of the club. So, the maximum number of members of the club is $20$ . I'm confused of the statement ""Find the sum of all possible values of $M$ "" as there is only one value of $M$ in my solution. And I also believe that the problem would not be so easy as this was the second last problem in the contest and most of the problems there were not easy. But I couldn't think of other solution to the problem. So, I want to know if my solution is correct or not. If not, what is the correct solution? Update: A generalization to this problem has been discussed here .","['contest-math', 'graph-theory', 'extremal-combinatorics', 'combinatorics', 'extremal-graph-theory']"
4198195,Uniqueness of a constrained system of linear equations,"I would like to determine whether there exists a solution (and if so, check uniqueness) to the following system of linear equations (with respect to $\eta = (\eta_1,...,\eta_J))$ : $$\begin{aligned} \sum_{j=1}^J (\psi_i -y_j)a_{ij}\eta_j &= 0, \hspace{1em} \forall i=1,...,I,  \\ \sum_{j=1}^J \eta_j &= 1, \\ \eta_j &\geq 0, \hspace{1em} \forall j=1,...,J \end{aligned},$$ where there are the following constraints: $a_{ij} \in [0,1] \hspace{1em} \forall i,j$ if we rearrange y's in an increasing order $y_1 < y_2 < ... < y_J$ , then the following inequalities hold $y_1 \leq \psi_i \leq y_J, \hspace{1em} \forall i=1,...,I$ . Is there a way to determine whether solution exists in general case for any $I$ and $J$ ? And if there are solutions - how to determine uniqueness? I solved the most simple case for $I=J=2$ . I ""deleted"" the $I$ th equation and solved the following system: $$\begin{aligned}(\psi_1 - y_1)a_{11}\eta_1 + (\psi_1-y_2)a_{12}\eta_2 &= 0, \\ \eta_1+\eta_2 &= 1, \\ \eta_1 &\geq 0, \\ \eta_2 &\geq 0.      \end{aligned}$$ Assuming without loss of generality that $y_1 < y_2$ we obtain the following: $$\begin{aligned}\eta_1 &= \frac{a_{12}(y_2-\psi_1)}{a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1)}, \\ \eta_2 &= \frac{a_{11}(\psi_1-y_1)}{a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1)}.     \end{aligned}$$ $\eta$ 's must be nonnegative, so this implies that either of the following two conditions must hold: $a_{12}(y_2-\psi_1) \geq 0$ , $a_{11}(\psi_1-y_1) \geq 0$ , and $a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1) \geq 0 ;$ $a_{12}(y_2-\psi_1) \leq 0$ , $a_{11}(\psi_1-y_1) \leq 0$ , and $a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1) \leq 0 ;$ Which due to the constraints on $a_{ij}$ 's lead to the equivalent conditions: $y_1 \leq \psi_1 \leq y_2$ and $a_{11}(\psi_1 - y_1) + a_{12}(y_2-\psi_1) \geq 0$ ; $y_2 \leq \psi_1 \leq y_1$ and $a_{11}(\psi_1 - y_1) + a_{12}(y_2-\psi_1) \leq 0$ . And now it is easily seen that the first case occurs when $y_1 \leq \psi_1 \leq y_2$ and the second case cannot occur, since $y_2 > y_1$ . If this solution does not satisfy the ""deleted"" equation, then there is no solution. But if it does, then the solution is unique.","['algebra-precalculus', 'systems-of-equations', 'linear-algebra']"
4198206,Is there a direct proof of the intermediate value theorem?,"I am interested in proving a weak version of the intermediate value theorem, specifically: Suppose that $f$ is continuous on $[a,b]$ , and $f(a)<0<f(b)$ . Then, there exists a number $x\in(a,b)$ such that $f(x)=0$ . The usual proof of this uses contradiction. I am curious as to whether there is a proof that doesn't use contradiction, and also doesn't require too much machinery. For reference, here is the argument presented in Michael Spivak's Calculus . We need the following lemma, which can be easily proven using the epsilon-delta definition of the limit: Suppose that $\lim_{x \to \lambda}f(x)=f(\lambda)$ , and $f(\lambda)<0$ . Then, there is a $\delta>0$ such that if $|x-\lambda|<\delta$ , then $f(x)<0$ . Similarly, if $\lim_{x\to\lambda}f(x)=f(\lambda)$ and $f(\lambda)>0$ , then there is an open interval containing $\lambda$ for which $f$ is positive. This lemma also applies to ""one-sided continuity"": if $\lim_{x \to a^+}f(x)=f(a)$ and $f(a)<0$ , then there is a $\delta>0$ such that if $0\le x-a<\delta$ , then $f(x)<0$ ; likewise, if $\lim_{x \to b^-}f(x)=f(b)$ and $f(b)>0$ , then there is a $\delta>0$ such that if $0\le b-x<\delta$ , then $f(x)>0$ . Consider the set $$E=\{x\in[a,b]:\text{$f$ is negative on $[a,x]$}\} \, .$$ From the above lemma we know that $E$ contains values greater than $a$ , and that all points sufficiently close to $b$ are not in $E$ . Since the real numbers satisfy the least upper bound property, we know that $\sup E$ exists, and we will denote it as $\alpha$ . Note that $\alpha$ satisfies $a<\alpha<b$ . We can prove that $f(\alpha)=0$ by contradiction. If $f(\alpha)$ were smaller than $0$ , then because $f$ is continuous at $\alpha$ , there would be a $\delta>0$ such that if $|x-\alpha|<\delta$ then $f(x)<0$ . This means that there would be numbers $x>\alpha$ such that $x\in A$ , contradicting the fact that $\alpha$ is an upper bound of $E$ . Similarly, if $f(\alpha)$ were greater than $0$ , then there would be a $\delta>0$ such that if $|x-\alpha|<\delta$ , then $f(x)>0$ . This means that there would be numbers $x<\alpha$ that are not in $E$ , contradicting the fact that $\alpha$ is the least upper bound of $E$ . So $f(\alpha)$ must be equal to $0$ , and since $a<\alpha<b$ , the theorem is proven. As you can see, this proof functions by ruling out the possibilities $f(\alpha)>0$ and $f(\alpha)<0$ using contradiction. Is there a way of directly showing that $f(\alpha)=0$ instead?","['continuity', 'calculus', 'complete-spaces', 'real-analysis']"
4198225,Calculating the distribution functions from two random variables X and Y,"I'm currently trying to catch up on Stochastics for university and I'm really stuck on this one although I feel like its not as difficult as I may think: Let X, Y be random variables. The random variable X takes the values 1, 2 and 3 with probabilities
P(X = 1) = 0.5, P(X = 2) = 0.3 and P(X = 3) = 0.2 and Y takes the values 1 and 2 with
probabilities P(Y = 1) = 0.7 and P(Y = 2) = 0.3.
Moreover, it is known that P(X = 1, Y = 1) = 0.35 and P(X = 3, Y = 1) = 0.2. a) Calculate the distribution functions of X and Y . b) Compute the remaining probabilities P(X = i, Y = j) for i ∈ {1, 2, 3} and j ∈ {1, 2}. Regarding a: I think I just need to see how its done once and than I'll probably get it but my University is not publishing the solutions or the script so my only options are googling and hoping to get it but I really dont :/ Regarding b: I get why P(X = 1, Y = 1) = 0.35 ( because 0.5*0.7 ) but why is P(X = 3, Y = 1) = 0.2  ( 0.2 * 0.7 ? )? I know its alot and I'm not expecting full on solutions but maybe someone can explain the basic approach to solving the exercise? I would greatly appreciate it :)","['statistics', 'probability-distributions', 'stochastic-calculus', 'probability-theory', 'probability']"
4198229,Find a sequence of measures and a measurable function,I am new to measure theory and I'm have a problem finding an example for the problem below: If $C$ is a middle-thirds Cantor set; find a sequence $\{ \mu_{n}\}_{n \geq 1 }$ of measures on Borel $\sigma -$ algebra $C$ and a measurable function $f:C \to R$ such that $\lim_{n \to +\infty} \mu_n(x) = 0$ and $\int f d\mu_{m} = +\infty$ for every $m \geq 1$ . I think that this is the Cantor measure problem but I'm not sure.,"['measure-theory', 'borel-measures', 'analysis', 'measurable-functions', 'functional-analysis']"
4198241,Is F countable or uncountable?,"Let $F$ be the set of all functions $f: \mathbb{Z^+} \to \mathbb{Z^+}$ . Is $F$ countable or uncountable? Define $K: \mathcal{P}(\mathbb{Z^+}) \to \{0,1\}^{\mathbb{Z^+}}$ for $U \subseteq \mathbb{Z^+}$ by letting $K(U)=\chi_{U}:\mathbb{Z^+} \to \{0,1\}$ , where $\chi_{U}(a)$ = $\left\{
\begin{array}{lr}
       1 & if  \hspace{2mm} a \in U\\
        0 & if \hspace{2mm} a \notin U \\
     \end{array}
   \right.$ Suppose $U,V \subseteq \mathbb{Z^+}$ s.t. $K(U)=K(V)$ . Then $\chi_{U}=\chi_{V}$ . Because $U=V \iff \chi_{U}=\chi_{V}$ , we have $K(U)=K(V) \Rightarrow \chi_{U}=\chi_{V} \Rightarrow U=V$ . This means that $K$ is injective and thus $|\mathcal{P}(\mathbb{Z^+})| \le |\{0,1\}^{\mathbb{Z^+}}|$ . By Cantor's Theorem, $\mathcal{P}(\mathbb{Z^+})$ is uncountable, and since $\exists$ injection $K: \mathcal{P}(\mathbb{Z^+}) \to \{0,1\}^{\mathbb{Z^+}} \Rightarrow \{0,1\}^{\mathbb{Z^+}}$ is uncountable. As $|\mathbb{Z^+}^\mathbb{Z^+}| \ge |\{0,1\}^{\mathbb{Z^+}}| \ge |\mathcal{P}(\mathbb{Z^+})|=2^{|\mathbb{Z^+}|} \Rightarrow$ $F$ is uncountable. I'm not sure whether this would be an appropriate method to prove this.","['elementary-set-theory', 'cardinals', 'functions', 'infinity']"
4198258,"How many solutions the equation $(x-2)(x+1)(x+6)(x+9)+108=0$ has in the interval $(-10,-1)$?","How many solutions the equation $(x-2)(x+1)(x+6)(x+9)+108=0$ has in the interval $(-10,-1)$ ? Here is my work: By expanding the expression we get, $$(x^2-x-2)(x^2+15x+54)+108=x^4+14x^3+37x^2-84x$$ So I got $x(x^3+14x^2+37x-84)=0$ . One root is zero which doesn't lie in the interval $(-10,-1)$ . But I don't know how many roots the cubic equation has in that interval.","['algebra-precalculus', 'polynomials']"
4198292,Prove that $\epsilon$ is an elementary family of sets,"My knowledge of this topic is only based on Folland's Real Analysis and Modern Techniques, Chapter2. I would appreciated some help. Prove $$\cal{E} = \big\{ \{a_1\} \times \{a_2 \} \times ... \times \{a_n\} \times \{0,1\} \times \{0,1\} \times ... : n \geq 1, a_1,..., a_n \in \{0,1\} \big\} \bigcup \{\emptyset\}$$ is an elementary family on {0,1}^N and the $\sigma$ -algebra generated by it is the Borel $\sigma$ -algebra on $\{0,1\}^N$ is based on the metric $$d((x_n)_{n \geq 1}, (y_n)_{n \geq 1})  = \sum_{n \geq 1} \frac{\lvert x_n - y_n \rvert}{2^{n}}$$","['measurable-sets', 'measure-theory', 'real-analysis']"
4198297,"Given $3$ points on a unit circle, figure out something about them.","Question : Given three points $(a, b), (c, d)$ and $(x, y)$ on the unit circle in a rectangular coordinate plane, find the maximum possible value of the expression $(ax + by - c)^2 + (bx - ay + d)^2 + (cx + dy + a)^2 + (dx - cy - b)^2. $ Answer : We will prove that the only value for this expression is $4$ . Without loss of generality, assume that the unit circle is the graph $x^2 + y^2 = 1$ . This means that $b^2 = 1 - a^2, d^2 = 1 - c^2,$ and $y^2 = 1 - x^2$ . Expanding the expression, we get $a^2x^2 + b^2y^2 + c^2 + 2axby - 2acx - 2byc + b^2x^2 + a^2y^2 + d^2 - 2bxay - 2ayd + 2bxd + c^2x^2 + d^2y^2 + a^2 + 2cxdy + 2ady + 2acx + d^2x^2 + c^2y^2 + b^2 - 2dxcy + 2bcy - 2dxb$ . This was a long expression! Fortunately, we see that most of the terms cancel out and we are left with $a^2x^2 + a^2y^2 + b^2x^2 + b^2y^2 + c^2x^2 + c^2y^2 + d^2x^2 + d^2y^2 + a^2 + b^2 + c^2 + d^2$ . This can be factored to $(a^2 + b^2 + c^2 + d^2)(x^2 + y^2 + 1)$ . Using the fact from above that $b^2 = 1 - a^2, d^2 = 1 - c^2,$ and $y^2 = 1 - x^2$ , this expression simplifies to $(a^2 + 1 - a^2 + c^2 + 1 - c^2)(x^2 + 1 - x^2 + 1) = 2(2) = 4$ . So, the answer to our original question is $\boxed{4}$ . Please verify my proof to see if there are any flaws or mistakes with the proof. Thanks in advance!","['algebra-precalculus', 'solution-verification', 'geometry']"
4198311,Why is it called pushforward measure?,"I'm trying to get an intuitive sense of the pushforward measure. Let $\Phi: \mathbb{R}^n \rightarrow \mathbb{R}^n$ be a vector field. We can define a corresponding pushforward measure: for an initial measure $\mu$ , the pushforward induced by $\Phi$ would be $\nu$ such that $$ \int_{\mathbb{R}^n} \nu(x) f(x) dx = \int_{\mathbb{R}^n} \mu(y) f(\Phi(x)) dx \;\;\;\; \text{ for any } f\in C^0_0(\mathbb{R}^n).$$ This says that $\mathbb{E}_\nu[f(x)] = \mathbb{E}_\mu [f(\Phi(x))]$ . I've also seen pushforward measure defined as: for any Borel set $B\in \mathbb{R}^n$ , $$ \nu(\Phi^{-1}(B)) = \mu(B).$$ This says that $\mu$ and $\nu$ are equivalent measures under a transformation of the state space by $\Phi$ . Are these definitions equivalent? If we pick $f$ to be any continuous function with support on $B \in {\mathbb{R}^n}$ , I can see how the first definition implies the second. Is my intuitive explanation that "" $\mu$ and $\nu$ are equivalent under a transformation of the state space"" correct? Why is it called pushforward measure? It seems more like a ""pushbackward measure"" since the original meausure of $B$ is $\mu(B)$ and the resulting measure $\nu$ takes $\Phi^{-1}(B)$ as its argument.","['vector-fields', 'measure-theory', 'functional-analysis', 'probability']"
4198324,Is there a probability measure on $2^{\mathfrak{c}}$?,"Is there a probability measure on a set whose cardinality is $2^{\mathfrak{c}}$ ? I considered a real-valued stochastic process $S(t),t\in[0,1]$ . If $t_0$ is a constant, $S(t_0)$ is an ordinary random variable. But suppose the set of all sample curves of $S(t)$ is $SC$ . We have $Card(SC)=2^{\mathfrak{c}}$ . Let $SP = \{f(t_0)| f\in SC\}$ . I think the probability distribution of random variable $S(t_0)$ should be consistent with the measure of different values in the set $SP$ . But $Card(SP)=2^{\mathfrak{c}}$ and the probability measure I know is defined on the set of real numbers. How to understand the meaning of the set $SP$ and the relationship between $SP$ and $S(t_0)$ ?","['elementary-set-theory', 'measure-theory', 'cardinals', 'probability']"
4198342,Show that countable/co-countable sigma-algebra is strictly smaller than Powerset,"I have a question regarding the countable/co-countable $\sigma$ -algebra. I will write first its definition: Example. Let $X$ be an uncountable infinite set. Then $$ \mathcal{A} = \{A \subseteq X\mid A\text{ is at most countable or } A^c\text{ is at most countable}\} $$ is a $\sigma$ -algebra, which is strictly smaller than $\mathcal{P}(X) = 2^X$ . (stated in Wikipedia ) Now to prove that $\mathcal{A}$ is a $\sigma$ -algebra is quite easy, but how can we prove that $\mathcal{A} \subsetneq \mathcal{P}(X)$ ? With $X=\mathbb{R}$ we can write $B=(-\infty,0]$ with $B^c=(0,+\infty)$ , both are uncountable, with $$ B\in \mathcal{P}(X) \quad \text{ and }\quad B\notin \mathcal{A} $$ So $\mathcal{A} \subsetneq\mathcal{P}(X)$ .
But with a general $X$ how can we do it?","['elementary-set-theory', 'measure-theory']"
4198351,Proof for the formula of the $n^\text{th}$ term of a linear and homogeneous second-order recurrence,"This morning I decided to give a try on some math tests from the final of a textbook for $9^\text{th}$ graders, just ""for fun"" (this year I'm passing to the $10^\text{th}$ grade). It seems like it wasn't fun at all. I stumbled upon another problem that I believe can be solved using Newton's sums / Vieta's formulas and since these topics pop up so often in my problems I decided to go full documentation mode on them (I didn't understand a thing from the class). I got to the conclusion that an equation is actually a recurrence generator so I thought of the reverse processing - looking at a recurrence through an equation. The formula I arrived at is close to the formula explained very friendly here , but in my deduction it only works for a particularization of the initial terms. I wrote this document that I wish to add to my math portfolio, but it's incomplete (I apologize if the translations are inaccurate). The next subsection would be 3.3 Proof of the generalized formula. But I don't know how to continue from there. Everywhere on the internet I see something like: There's a general theorem that says $f(n)=Ar_1^n+Br_2^n$ where $r_1,r_2$ are roots of the equation [...] and $A,B$ can be calculated through a system of linear equations [...] But I want the proof. And I don't even want to think of induction. I think induction is an excuse to not actually think the problem the way it is meant to be thought. It would be nice if people could timetravel in the future, find the formula, then go back in time and say ""I've found the formula but hey let's prove it by induction because who cares about the logic."" I only agree with induction when the formula is intuitive and you want to make sure that it's correct, but here's not the case. I think I am really close to proving the formula. Basically the problem turns to: Knowing the formula for $f(n)=c_1f(n-1)+c_2f(n-2)$ with $f(0)=2,f(1)=c_1$ , find the formula for $f(n)$ for any given $f(0),f(1)$ . Again, I can't find anything on the internet and I don't want induction, nor calculus (I read that solving recurrence relations is similar to solving differential equations but I don't know calculus). Thanks in advance!","['algebra-precalculus', 'recurrence-relations']"
4198392,Inequality involving matrix inverse elements,"Let $A$ be an $N \times N$ matrix with all nonnegative entries and row sums strictly
less than one, let $v$ be an $N$ dimensional vector with all nonnegative entries and weakly lower than one, let $B\equiv\left(I-A\mathrm{diag}\left(v\right)\right)^{-1}$ and let $B^*\equiv\left(I-A\right)^{-1}$ , where $\mathrm{diag}\left(v\right)$ is the diagonal matrix formed from vector $v$ . I want to show that for any $i,j=1,...,N$ the following inequality holds: $$
v_{j}b_{ji}^2+\sum_{k}\left(1-v_{k}\right) b_{jk}b_{ki}^{2}\leq b_{ji} b_{ii}^{*}.
$$ Simulations suggest that this is true. The case in which $v$ is the vector of all ones follows from the fact that $b_{ji} \leq b_{ii}^{*}$ , which is shown here . The case with $A$ diagonal is trivial for $i \neq j$ , whereas for $i=j$ it boils down to showing $$v_{i}b_{ii}+(1-v_{i})b_{ii}^{2}\leq b_{ii}^{*},$$ which can be shown by plugging in for $$b_{ii}=\frac{1}{1-v_{i}a_{ii}},\quad b_{ii}^{*}=\frac{1}{1-a_{ii}},$$ and basic algebra. Apart from these simple cases, I have been able to show the result for the case in which $j=i$ , but it is an arduous induction proof that does not extend to the case in which $i \neq j$ . We would appreciate hints for approaches that could be useful to prove the claim. The problem above comes from a more general problem in matrix algebra, which is to show that $ii$ of the following matrix is less than $b_{ii}^*$ , $$J \equiv \left(\mathrm{diag}\left\{ B^{T}x\right\} \right)^{-1}B^{T}\left[ \mathrm{diag}\left\{ v\right\}\mathrm{diag}\left\{ x\right\} +\mathrm{diag}\left\{ 1-v\right\} \mathrm{diag}\left\{ B^{T}x\right\} \right]B,$$ with $x$ being an $N$ dimensional vector in the simplex, i.e., $x_j \geq 0,\sum_j x_j=1$ . It can be shown that the diagonal elements will be maximized with respect to $x$ when $x$ is at a corner of the simplex, and that if $x_j = 1$ then $$
J_{ii} = v_{j}b_{ji}+\sum_{k}\left(1-v_{k}\right) \frac{b_{jk}b_{ki}^{2}}{b_{ji}}\leq b_{ii}^{*}.
$$ Multiplying by $b_{ji}$ on both sides leads to the inequality postulated above. A closely related problem is to show that the spectral radius of $J\mathrm{diag}(\iota-A \iota)$ is lower than one (where $\iota$ is the vector of all ones), see here . (Note that if $v=\iota$ then $J\mathrm{diag}(\iota-A \iota)\iota = J(I-A)\iota = \iota$ and so the spectral radius of $J\mathrm{diag}(\iota-A \iota)$ is one.)","['matrices', 'inequality', 'linear-algebra']"
4198430,Prove that the Fredholm Integral Equation is a contraction,"As a part of an exercise I have to prove that the Fredholm Integral Equation is a contraction, I have the following definitions and theorem: Definition . Let $(X,d)$ be a metric space and $G:X \rightarrow X$ . The mapping G is a contraction if $ $ $\exists $ $ 0 \le\theta < 1$ s.t: $$
 d(G(x),G(y))\le \theta d(x,y),  \forall  x,y \in X.
$$ Definition . Let $ $ $C([a,b])$ be the space of bounded continuos funcions on [a,b]. Theorem (Banch Contraction-Mapping). $ $ Let $(X,d)$ be a complete metric space and $G$ a contraction map of $X$ . Then $\exists!$ fixed point of $G$ in $X$ . Note . We use the supremun norm. Exercise Show that the Fredholm Integral Equation $$
f(x)= \psi(x)+ \lambda \int_{a}^{b}K(x,y)f(y)dy
$$ has a unique solution $F \in C([a,b])$ , with $\lambda$ small enough, $\psi \in C([a,b])$ and $K \in (C([a,b]) \times C([a,b]))$ . My attempt: I want to use the Theorem, I know that I have a complete metric space, I still have to prove that it is a contraction. So i define the map $Tf:=f$ , then I have to prove that $ $ $ \exists $ $ 0 \le \theta < 1 $ $ s.t $ $ $ $ d(Tf,T \tilde{f})\le 
 \theta $ $ d(f, \tilde{f}), \forall f, \tilde{f} \in C([a,b]) $ . $$
d(Tf,T \tilde{f})= \underset{a \le x \le b}{\sup}  | \psi(x)+ \lambda \int_{a}^{b}K(x,y)f(y)dy - \psi(x)- \lambda \int_{a}^{b}K(x,y) \tilde{f}(y)dy | \\ = \lambda \underset{a \le x \le b}{\sup}  | \int_{a}^{b}K(x,y)f(y)dy - \int_{a}^{b}K(x,y) \tilde{f}(y)dy |= \lambda \underset{a \le x \le b}{\sup}  | \int_{a}^{b}K(x,y)(f(y)-\tilde{f}(y))dy | \\ \le \lambda \underset{a \le x \le b}{\sup}  \int_{a}^{b} | K(x,y)|  |f(y)-\tilde{f}(y)|dy= \lambda \int_{a}^{b} \underset{a \le x \le b}{\sup} (| K(x,y)|)  \underset{a \le x \le b}{\sup}(|f(y)-\tilde{f}(y)|)dy \\ = \lambda \int_{a}^{b} \underset{a \le x \le b}{\sup} | K(x,y)| \   d(f(y),\tilde{f}(y))dy.
$$ There I'm stuck, I know that $K, f, \tilde{f}$ are bounded but I don't know how to extract $f, \tilde{f}$ from the intregal, since we integrating in function of $y$ . Here a similar question Understanding Fredholm integral equation and to proof it is a contraction on C[a,b] , but the answer does not go through the details Any help or hint will we really appreciated, thank you in advance.","['fixed-point-theorems', 'banach-spaces', 'functional-analysis', 'analysis']"
4198444,Calculating $\lim_{x \to 0} \frac{\sin(x^2)-\sin^2(x)}{x^2\ln(\cos x)}$ without L'Hospital's rule,"this is my first post here so excuse the lack of knowledge about how things usually go. My question revolves around calculating the limit as $x$ approaches $0$ of the following function: $$\lim_{x \to 0} \frac{\sin(x^2)-\sin^2(x)}{x^2\ln(\cos x)}$$ The question came up in a test about a month ago and while I couldn't solve it in the test I've been working on it since then but I can't seem to get it. I know the limit is supposed to be $\frac{-2}{3}$ from some online calculators which abused l'hopital rule over and over again. I've tried playing around with it in so many ways but I always seem to get 0 over 0 or the so called indeterminate form. I've even tried calculating it by substituting in the Taylor series for the functions given but no luck. If anyone could show me a method of calculating this without using l'hopital rule or better yet, give me a hint as to how I should proceed I would be grateful.","['limits', 'calculus', 'functions', 'limits-without-lhopital']"
4198466,Is this decomposition of convex combination always feasible?,"Given $\mu$ , $\lambda_1$ , $\lambda_2$ , $\gamma_1$ , ..., $\gamma_{n}$ , which are all scalars. Suppose $\mu$ , $\lambda_1$ , $\lambda_2$ $\in \left(0,1\right)$ , $0\leqslant \lambda_1<\mu<\lambda_2 \leqslant 1$ , $\gamma_j \in \left[0,\lambda_1\right]\cup \left[\lambda_2,1\right], \forall j$ .
We have unique $p_1$ and $p_2$ such that $p_i \in [0,1],\forall i$ , $p_1+p_2=1$ , and $\mu=p_1 \lambda_1+p_{2}\lambda_{2}$ . Define several sets as follows. $Q=\left\{(q_1,...,q_{n})|q_j \in [0,1],\forall j,\sum_{j=1}^{n}q_j=1,\mu=q_1 \gamma_1+...+q_{n}\gamma_{n}\right\}$ . $R_i=\left\{(r_{i1},...,r_{in})|r_{ij} \in [0,1],\forall j,\sum_{j=1}^{n}r_{ij}=1,\lambda_i=r_{i1} \gamma_1+...+r_{in}\gamma_{n}\right\}$ $S=\left\{p_1(r_{11},...,r_{1n})+p_2(r_{21},...,r_{2n})|(r_{i1},...,r_{in})\in R_i\right\}$ The question is whether $Q=S$ is true? ( $S \subseteq Q$ is straightforward) Note $\gamma_1$ ,..., $\gamma_{n}$ are not necessarily affine independent, so both $Q$ and $R_i$ may contain more than one element. The intuition of this question is when we write $\mu$ as a convex combination of $\left(\gamma_1, ...,\gamma_n\right)$ , is it always feasible to replicate this sequentially by first writing $\mu$ as a convex combination of $\lambda_1$ and $\lambda_2$ and then write $\lambda_i$ as a convex combination of $\left(\gamma_1, ...,\gamma_n\right)$ . I really appreciate your help, thank you very much!","['convex-geometry', 'probability-theory', 'linear-algebra', 'geometry']"
4198468,Finding Option Probability Density Using Local Volatility from Dupire Model,"This question is different than https://quant.stackexchange.com/questions/31050/pricing-using-dupire-local-volatility-model I am reading about the Dupire local volatility model. I have found ways to calculate the local volatility so for my question we can assume that it is known. More specifically, it is considered piecewise constant between strikes and tenors so I have a local volatility surface that should be defined for all times and strikes. From here I am wondering how I can think about solving the Dupire equation and recover risk neutral probability densities. I am not super familiar with stochastic differential equations so I am hoping that I can receive help reasoning through the problem. My attempt The Dupire equation takes the form $dS_t = \mu_t S_t dt + \sigma(S_t,t) S_t dW_t$ where $S_t$ is the stock price at time $t$ , $\mu_t$ is the drift term, $\sigma$ is the local volatility and $W_t$ is a Wiener process. Additionally, $S_t|_{t=0} = S_0$ . For simplicity, take $\mu_t=0$ . It is at this point that I have been getting confused about how to define the necessary constraints to solve the Ito integral. First, I take it that $S_0$ is the spot price of the underlying of the derivative. If we solve this SDE, do we then find the how the spot price evolves over time? How does that help with option pricing? If $S_0$ is not the spot price, what instead is it? Is it the price of an option with a given strike at $t=0$ and we are then solving for the option prices at that strike across time? If the latter is the case, do I solve this SDE for each forward price I want to query and simply have my volatility function be a function of time? Once I have priced my options, I simply plan on taking the second derivative of price with respect to strike at each tenor to find the risk neutral probability density. Assume for this problem that tenors and strikes are dense enough that differentiation makes sense. I found all of this information in Lecture 1: Stochastic Volatility and Local Volatility by Jim Gatheral, http://web.math.ku.dk/~rolf/teaching/ctff03/Gatheral.1.pdf","['stochastic-integrals', 'finance', 'stochastic-differential-equations', 'brownian-motion', 'probability-theory']"
4198493,Find $\int_{0}^{\pi/2}\frac{\ln(1+3\sin^2x)}{\sin^2x}dx$,"I was not able to find the value of the integral $$\int_{0}^{\pi/2}\frac{\ln(1+3\sin^2x)}{\sin^2x}dx$$ So, I refered to the solution provided as follows. $$f(t)=\int_{0}^{\pi/2}\frac{\ln(1+t\sin^2x)}{\sin^2x}dx$$ $$\begin{align*}
f'(t)&=\int_{0}^{\pi/2}\frac{dx}{1+t\sin^2x}\\
&=\int_{0}^{\pi/2}\frac{\text{cosec}^2x}{\cot^2x+1+t}dx\\
&=\int_{0}^{\infty}\frac{dp}{p^2+1+t}\\
&=\left[\frac 1{\sqrt{1+t}}\tan^{-1}{\frac p{\sqrt{1+t}}}\right]_0^{\infty}=\frac{\pi}{2{\sqrt{1+t}}}\end{align*} $$ $$f(t)=\int f'(t)dt=\int \frac{\pi}{2{\sqrt{1+t}}}dt
=\pi{\sqrt{1+t}}+C$$ $$f(0)=0
\implies C=-\pi$$ $$\implies f(t)=\pi(\sqrt{1+t}-1)$$ Therefore, the original integral is $$\int_{0}^{\pi/2}\frac{\ln(1+3\sin^2x)}{\sin^2x}dx=f(3)=(\sqrt 4-1)\pi=\pi$$ This method is not obvious to me. Can someone provide an alternative solution? Thanks in advance.","['integration', 'calculus', 'definite-integrals']"
4198547,Find Maximum-Likelihood-Estimator (MLE) for $\alpha$,"Consider the following PDF: $$w_{\alpha,\beta}(x):=\alpha \beta x^{\beta-1}e^{-\alpha x^{\beta}} \mathbf{1}_{(0,\infty)}(x)$$ This is the Weibull distribution often used in material science. Assume we know $\beta$ and we want to estimate $\alpha$ . Let $X_1,\ldots X_n$ be i.i.d weibull-distributed. Find the MLE $\hat{\alpha}$ for the parameter $\alpha$ . Find a $c \in \mathbb R$ such that $c \cdot \alpha $ is an unbiased estimator. Question: The result I am getting for the MLE doesn't look correct but I don't know what I am doing wrong. For Part 2, do I just have to show that $\operatorname{E}(\hat{\alpha}-\alpha)=0$ ? My attempt: Step 1: Write down the ML function: $$L(\alpha)=\prod_{i=1}^n \alpha \beta x_i^{\beta-1}e^{-\alpha x_i^{\beta}}$$ Step 2: Take the natural log: $$ \begin{aligned}\ln(L(\alpha))&=\sum_{i=1}^n \ln\left(\alpha \beta x_i^{\beta-1}e^{-\alpha x_i^{\beta}} \right)
\\[5pt] &=\sum_{i=1}^n \ln\left(\alpha\beta x_i^{\beta-1}\right)+\ln \left( e^{-\alpha x_i^{\beta}}\right) 
\\[5pt] &=\sum_{i=1}^n\ln\left(\alpha \right)+\ln(\beta)+(\beta-1)\ln\left(x_i \right)-\alpha x_i^{\beta} \end{aligned}$$ Step 3: Differentiate and set equal to zero: $$\begin{aligned}&\frac{\partial }{\partial \alpha}\ln(L(\alpha))=\sum_{i=1}^n \frac{1}{\alpha}-x_i^{\beta}=0 
\\[5pt] &\iff \sum_{i=1}^n\frac{1}{\alpha}=\sum_{i=1}^n x_i^{\beta}
\\[5pt] & \iff \frac{n}{\alpha}=\sum_{i=1}^nx_i^{\beta} \iff \alpha=\frac{n}{\sum_{i=1}^nx_i^{\beta}}\end{aligned}$$ For part 2 I was thinking of setting $c=\alpha \bar{X}$ , where $\bar{X}$ is the average of the $x_i^{\beta}$ 's. Then it would follow that: $$E\left[\alpha \cdot \frac{1}{n} \cdot \sum_{i=1}^n x_i^{\beta} \cdot \frac{n}{\sum_{i=1}^n x_i^{\beta}}\right]=\alpha \\ \iff \alpha =\alpha $$ But I am not sure if am allowed to set $c$ equal to that.","['statistics', 'parameter-estimation', 'maximum-likelihood']"
4198573,Why doesn't nonexistence of $\lim_{x \to \infty^+}$ and $\lim_{x \to -\infty^-}$ cause limits at infinity to be undefined?,"One criterion for checking existence of limits is to check that and one-sided limits from left and right exist and agree: (Theorem) Let $f$ be a real-valued function. One-sided limits of $f$ as $x$ approaches $a$ from the left and right exist and equal $L$ : $$
\lim_{x \to a^-} f(x) = \lim_{x \to a^+} f(x) = L
$$ if and only if the two-sided limit of $f$ at $a$ exists and also equals $L$ , $$\lim_{x \to a} f(x) = L.$$ The contrapositive of this statement can be used to conclude that a limit does not exist: (Contrapositive) Let $f$ be a real-valued function. One-sided limits of $f$ as $x$ approaches $a$ from the left and right do not exist or do not agree if and only if the limit of $f$ at $a$ does not exist. When applying this to a variety of contexts, you can come up with some pretty weird examples and weird results. In the above figure, it seems to me that $\lim_{x \to 3} f(x)$ does not exist since $\lim_{x \to 3^+} f(x)$ does not exist. $\lim_{x \to 4} f(x)$ does not exist since $\lim_{x \to 4^-} f(x)$ and $\lim_{x \to 4^-} f(x)$ do not exist. $\lim_{x \to 5} f(x)$ does not exist since $\lim_{x \to 5^-} f(x)$ does not exist. A bit more controversial is if you apply the same to the limits at infinity, which would stand to reason that $\lim_{x \to \infty} f(x)$ does not exist since $\lim_{x \to \infty^+} f(x)$ does not exist. $\lim_{x \to -\infty} f(x)$ does not exist since $\lim_{x \to \infty^-} f(x)$ does not exist. However to contradict the above, many people would write $\lim_{x \to \infty} f(x)=2$ and $\lim_{x \to -\infty} f(x)=-1$ . TLDR; why does the nonexistence of left and right limits not cause limits at infinity to be undefined but does cause the limit at $x=3$ , $x=4$ , and $x=5$ to not exist? Real analysis answers are welcome. Edit Thanks to answers from Troposphere and Joe, I worked out some more careful definitions and theorems: Definition (Limit) Let $f$ be a function whose domain is a subset of $\mathbb{R}$ , and let $a, L \in \mathbb{R}$ . We say that the limit of $f$ as $x$ approaches $a$ is $L$ , \begin{equation*} \lim_{x \to a} f(x)=L \end{equation*} to mean that $a$ is an accumulation point of $\textrm{dom}(f)$ and for any $\epsilon > 0$ , there exists $\delta >0$ such that if $x \in \textrm{dom}(f)$ is within $\delta$ of $a$ (with $x \ne a$ ), then $f(x)$ is within $\epsilon$ of $L$ : \begin{equation*} 0 < |x-a| < \delta \quad \rightarrow \quad 0 < |f(x)-L| < \epsilon.
\end{equation*} Definition (One-Sided Limits) Let $f$ be a function whose domain is a subset of $\mathbb{R}$ , and let $a, L \in \mathbb{R}$ .: We say the limit of $f$ as $x$ approaches $a$ from the left is $L$ , $$\lim_{x \to a^-} f(x)=L,$$ to mean that $a$ is an accumulation point of $\textrm{dom}(f) \cap (-\infty,a]$ and for any $\epsilon > 0$ , there exists $\delta >0$ such that if $x$ is within $\delta$ of $a$ (for $x < a$ ), then $f(x)$ is within $\epsilon$ of $L$ : \begin{equation*} 0 < a-x < \delta \quad \rightarrow \quad 0 < |f(x)-L| < \epsilon. \end{equation*} We say the limit of $f$ as $x$ approaches $a$ from the right is $L$ , \begin{equation*} \lim_{x \to a^+} f(x)=L, \end{equation*} to mean that $a$ is an accumulation point of $\textrm{dom}(f) \cap [a,\infty)$ and for any $\epsilon > 0$ , there exists $\delta >0$ such that if $x$ is within $\delta$ of $a$ (for $x > a$ ), then $f(x)$ is within $\epsilon$ of $L$ : \begin{equation*} 0 < x-a < \delta \quad \rightarrow \quad 0 < |f(x)-L| < \epsilon. \end{equation*} Theorem (One-Sided and Two-Sided Limits Relationship) Let $f$ be a function whose domain is a subset of $\mathbb{R}$ , and let $a, L \in \mathbb{R}$ . Suppose a is an accumulation point of dom(f) and that $f$ is defined everywhere in some punctured neighborhood of $a$ . Then One sided limits of $f$ from left and right at $a$ exist and equal $L$ , $$\lim_{x \to a^-} f(x) = L \textrm{ and }\lim_{x \to a^+} f(x) = L,$$ if and only if the two-sided limit of $f$ at $a$ exists and also equals $L$ : $$\lim_{x \to a} f(x) = L.$$ Contrapositive (One-Sided and Two-Sided Limits Relationship) Let $f$ be a function whose domain is a subset of $\mathbb{R}$ , and let $a, L \in \mathbb{R}$ . Suppose a is an accumulation point of dom(f) and that $f$ is defined everywhere in some punctured neighborhood of $a$ . Then At least one of the one sided limits of $f$ from left and right at $a$ does not exist or do not equal $L$ : $$\lim_{x \to a^-} f(x) \ne L \textrm{ or }\lim_{x \to a^+} f(x) \ne L,$$ if and only if the two-sided limit of $f$ at $a$ does not exist or does not equal $L$ : $$\lim_{x \to a} f(x) \ne L.$$ Under these definitions and theorems, I think (hope) we get the conclusions we expect: $\lim_{x \to 3} f(x)$ exists $\lim_{x \to 5} f(x)$ exists. and $\lim_{x \to 3^+} f(x)$ DNE. $\lim_{x \to 4} f(x)$ DNE. $\lim_{x \to 5^-} f(x)$ DNE.","['real-analysis', 'calculus', 'definition', 'infinity', 'limits']"
