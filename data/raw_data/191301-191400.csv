question_id,title,body,tags
3623967,If $f \in L^1$ $\mu(A_n)\to 0$ then $\int_{A_n}f\to 0$,"In this quesiton , what happens if $p=1$ ? Does the statement remain true? That is, if $(X,\Sigma,\mu)$ is a measure space, $f\in L^1$ and if $\{A_n\}$ be a sequence in $\Sigma$ such that $\mu(A_n)\to 0$ , is it true that $\int_{A_n}fd\mu\to 0$ ? Following the answer of that question, we'd get $$\left|\int_{A_n} f\, d\mu\right| \le \int_X \chi_{A_n}|f|\, d\mu \le \|\chi_{A_n}\|_{\infty} \|f\|_1.$$ Which is true, but what's next? Do we have that $\|\chi_{A_n}\|_{\infty} \to 0$ when $\mu(A_n)\to 0$ ? How to prove that?","['integration', 'measure-theory']"
3623969,A weird matrix property,"I encountered the following weird matrix property. Consider any general matrix $M_{n\times n}$ with the property that the sum of each column vanishes, that is \begin{align} \sum^n_{j} M_{ji} =0 \end{align} Denoting $M_{(1)}$ : the matrix obtained from $M$ by removing the first column and row, $M_{(2)}$ : the matrix obtained from $M$ by removing the second column and row, $M_{(1,2)}$ : the matrix obtained from $M$ by removing the first and second column and row, $u_{(k)}=(1,\dots,1)$ : the $(k)$ -row vector with all elements being $1$ , $C_{(n-1)\times (n-2)}=\begin{pmatrix} 0 \dots  0  \\ 1_{(n-2)\times (n-2)}
  \end{pmatrix} $ , where $ 1_{(n-2)\times (n-2)}$ is the identity matrix Define $p_1$ and $p_2$ as $$ p_1 = u_{(n-1)} \cdot\big(M_{(1)}\big)^{-1}\cdot\begin{pmatrix} 1  \\ 0 \\ \vdots\\0
  \end{pmatrix}_{ (n-1)\times1}, \quad  p_2 = u_{(n-1)} \cdot\big(M_{(2)}\big)^{-1}\cdot\begin{pmatrix} 1  \\ 0 \\ \vdots\\0
  \end{pmatrix}_{ (n-1)\times1}. $$ Prove that all the elements of the row vector $$ u_{(n-1)} \cdot \left(p_2\big(M_{(1)}\big)^{-1}+p_1\big(M_{(2)}\big)^{-1}\right)\cdot C - (p_1+p_2)u_{(n-2)}\cdot \big(M_{(1,2)}\big)^{-1}   $$ are identical. This property comes from some intuition of the problem that I have been playing with. I have also tested it by evaluating it with a large set of matrix $M$ satisfying the first requirement. ( I thank user1551 for spotting an important typo, corrected now!) I have tried writing the inverse using minors but does not seem to help as it is not easy to implement the requirement that $\sum_{j} M_{ji}=0$ . 
Any comment/suggestion is greatly appreciated. Answers are of course the best! Thank you so much!","['matrices', 'linear-algebra']"
3624002,Prove or disprove: $(A\setminus B) \times C = (A\times B)\setminus (B \times C)$,"Prove or disprove: $$(A\setminus B) \times C = (A\times B)\setminus (B \times C).$$ If  we look at the left side of the statment, $(A\setminus B) \times C$ , we can say that we have $\{a,b\}$ where $a$ belongs to $A$ only and $b$ belong to $C$ (by the defenition of Cartesian product). But if we look at the right side, $(A\times B)\setminus (B \times C)$ , we can say that we have $\{a,b\}$ where $a$ belong to $A$ but not $B$ and $b$ belong to $B$ but not $C$ . the problem is that $y$ cant be belong to $C$ on the one hand and on the other hand not belong to $C$ ? Or am i mistaken ? Please help. If it's really not true I need a counterexample.","['elementary-set-theory', 'logic', 'discrete-mathematics']"
3624004,"Why is there no ""weakly version"" of locally path-connectedness?","As usual in topology, there are many different definition for terms like ""locally connectedness"": Let $(X,\mathcal{T})$ be  a topological space. Note that I use the following definition of neighbourhood: A set $U\subset X$ is a neighbourhood of $x\in X$ , if there is an open set $\mathcal{O}\subset X$ , such that $x\in\mathcal{O}\subset X$ . Therefore, neighbourhoods can also be closed. (1) $(X,\mathcal{T})$ is called ""weakly locally connected"" at $x\in X$ , if for every neighbourhood $U\subset X$ of $x$ there is a connected neighbourhood $V$ of $x$ , such that $x\in V\subset U$ . In othere words $x$ admits a neighbourhood basis of connected sets. If $(X,\mathcal{T})$ is weakly locally connected at every $x\in X$ , then it is called ""weakly locally connected"". (2) $(X,\mathcal{T})$ is called ""locally connected"" at $x\in X$ , if for every open neighbourhood $U\subset X$ of $x$ there is an open connected neighbourhood $V$ of $x$ , such that $x\in V\subset U$ . In othere words $x$ admits an open neighbourhood basis of connected sets. If $(X,\mathcal{T})$ is locally connected at every $x\in X$ , then it is called ""locally connected"". The two definition differ just by the word open . Obviously, if $(X,\mathcal{T})$ is locally connected at $x\in X$ , it is also weakly locally connected at $x$ . The reverse is in general not true. However, we can show that every weakly locally connected space is also locally connected. Therefore, the two definition are globally equivalent. (For a proof see https://proofwiki.org/wiki/Equivalence_of_Definitions_of_Locally_Connected_Space ) I am wondering, why there is no such thing as a weakly locally path-connected space. I have found both definition of locally path-connectedness in textbooks: every neighbourhood contains a path-connected neighbourhood and the second one with open neighbourhoods....are they in this case already equivalent locally? Also Wikipedia doesn't make a distinction: They define weakly locally connected and locally connected, but they define only locally path-connected without a weakly version... ( https://en.wikipedia.org/wiki/Locally_connected_space ) In other words: Is the following statement true: Let $x\in X$ be fixed. Every neighbourhood $U$ of x has a path-connected neighbourhood $V$ ,
  such that $x\in V\subset U$ $$\Rightarrow$$ Every open neighbourhood $U$ of x has an open path-connected neighbourhood $V$ , such that $x\in V\subset U$ That the statement is again globally (if we assume that both sides holds for all x) true is obvious, by a similar proof as for locally conectedness..... But I guess that is also true at a certain point, because otherwise there would also ba such a thing as weakly locally path-connectedness....","['connectedness', 'path-connected', 'definition', 'general-topology', 'locally-connected']"
3624023,"Is $f(x,y)=\sqrt{|xy|}$ differentiable? [duplicate]","This question already has an answer here : Check differentiability of multivariable functions (1 answer) Closed 4 years ago . I know that $f(x,y)=\sqrt{|xy|}$ when $(x,y)\neq{(0,0)}$ is differentiable since partial derivatives exist and they're continous.
When I do by definition $f_x (0,0)$ and $f_y (0,0)$ , both are equal to $0$ .
Due to ""Sufficient Condition for the Differentiability of Functions"", can I conclude that it is differentiable? 
My problem is that I think it isn't diff. because of an analogy to single-variable $f(x)=|x|$ , but not sure.
Thanks.","['multivariable-calculus', 'real-analysis']"
3624047,"Prove that $\lim\limits_{x\to\infty}xf(x)=0$ where $f$ is an integrable function over $(0,\infty)$.","Please check if the solution to the following problem is correct or not. If there is any flaw then please help me to correct it. Problem: Let $f:(0,\infty)\to(0,\infty)$ be a decreasing function, satisfying $$I=\int\limits_{0}^{\infty}f(x) \, \mathrm{d} x<\infty.$$ Prove that $\lim\limits_{x\to\infty}xf(x)=0$ . Solution: Let us assume that $f$ be a decreasing function such that $\lim\limits_{x\to\infty}xf(x)=l \, ,l>0$ . Since $xf(x)>0$ for $x>0$ , $\lim\limits_{x\to\infty}xf(x)$ cannot be negative. Case 1. Let $l=\infty$ . Then given any $M>0 \, \exists a=\frac{M}{f(a)}>0$ such that for any $x>a$ , $xf(x)>M$ . Then $$g(t)=\int\limits_{0}^{t}f(x) \, \mathrm{d} x=\int\limits_{a}^{t}f(x) \, \mathrm{d} x+\int\limits_{0}^{a}f(x) \, \mathrm{d} x>\int\limits_{0}^{a}f(a)\, \mathrm{d} x=af(a)=M.$$ Hence given any $M>0$ $\exists a=a(M)>0$ such that for any $t>a$ , $g(t)>M$ . Hence $\lim\limits_{t\to\infty}g(t)=\infty$ . Thus $I=\infty$ . Case 2. Let $0<l<\infty$ . Then given any $\epsilon>0 \, \exists \delta=\frac{\epsilon}{f(\delta)}>0$ such that for any $x>a$ , $|xf(x)-l|<\epsilon$ . Then $$g(t)=\int\limits_{0}^{t}f(x) \, \mathrm{d} x=\int\limits_{\delta}^{t}f(x) \, \mathrm{d} x+\int\limits_{0}^{\delta}f(x) \, \mathrm{d} x>\int\limits_{0}^{\delta}f(\delta) \, \mathrm{d} x=\delta f(\delta)=\epsilon.$$ Hence given any $\epsilon>0$ $\exists \delta=\delta(\epsilon)>0$ such that for any $t>\delta$ , $g(t)>\epsilon$ . Hence $\lim\limits_{t\to\infty}g(t)=\infty$ . Thus $I=\infty$ . Hence in both the cases we get a contradiction to the fact that $I<\infty$ . Thus $\lim\limits_{x\to\infty}xf(x)=0$ . Thank you.","['limits', 'solution-verification', 'improper-integrals', 'real-analysis']"
3624052,Finding the MLE of $\theta$,"Working through this given problem on maximum likelihood estimation (MLE). The density is given as $$f(x \mid \theta) = \frac{1}{2\theta} e^{-|x|/\theta}, -\infty < x <\infty $$ I get $$L(\theta)= \frac{1}{(2\theta)^{n}} e^{- \frac{1}{2\theta} \sum\limits_{i=1}^{n}|X_i|}$$ Applying the ln step $$\ln L(\theta)= -n\ln(2\theta) - \frac{1}{\theta}\sum\limits_{i=1}^{n}|X_i|$$ Taking the derivative $$\frac{d}{d\theta} = \frac{-n}{\theta} \frac{\sum\limits_{i=1}^{n}|X_i|}{\theta^{2}}$$ Setting the derivative to 0 and solving for $\theta$ $$\frac{-n}{\theta} \frac{\sum\limits_{i=1}^{n}|X_i|}{\theta^{2}} = 0 $$ $$\hat{\theta}= \frac{\sum\limits_{i=1}^{n}|X_i|}{n}$$ Is this correct or have i done a something wrong and im getting the wrong mle","['statistical-inference', 'statistics', 'maximum-likelihood']"
3624061,Are epimorphisms in the category of Stone spaces surjective?,"Clearly, if a map between Stone spaces is surjective on points it is an epimorphism. In the category of topological spaces, surjections coincide with epimorphisms. In the category of Hausdorff spaces, epimorphisms are precisely the continuous functions with dense image: in one direction, dense maps are epis since equalizers are closed and $hf=kf$ iff $\mathrm{im}(f)$ is contained in the equalizer of $h$ and $k$ ; in the other direction, if $f:X\to Y$ doesn't have dense image, then it coequalizes the two distinct maps $i,j: X\to (X+X)/\sim$ where $\sim$ identifies the two copies of the closure of the image. I think if the construction of $(X+X)/\sim$ could be performed in the category of Stone spaces it would show that all epimorphisms in this category are surjective, since $\mathrm{im}(f)$ is always closed whenever $X$ is compact Hausdorff. However, I don't know if Stone spaces are closed under the necessary pushouts as a subcategory of topological spaces.","['general-topology', 'category-theory']"
3624066,Munkres Topology Supplementary exercises chapter 1 question 2 (a). Showing two definitions are the same,"Let $J$ and $E$ be well-ordered sets; let $h: J\rightarrow E$ . Show the following two statements are equivalent. $h$ is order preserving and its image is $E$ or a section of $E$ $h(\alpha)= smallest(E-h(S_{\alpha}))$ for all $\alpha$ [Hint: Show that each of these conditions implies that $h(S_\alpha$ ) is a section of $E$ ; conclude that it must be a section by $h(\alpha)$ ] To clarify notation, $S_\alpha$ means the section by $\alpha$ . I got as far as proving the hint suggestion. How do I proceed from there?","['elementary-set-theory', 'well-orders']"
3624070,"Solving Differential Equation $(1+xy) y + (1-xy) x \frac{\,dy}{\,dx} = 0$","I was solving this question on differential equations: Solve the differential equation: $$(1+xy) y + (1-xy) x \frac{\,dy}{\,dx} = 0$$ I tried the problem by the following: $$(1+xy)y\,dx + (1-xy)\,dy=0$$ $$y\,dx+x\,dy = x^2y\,dy - y^2x\,dx$$ $$2\,d(xy) = x^2{\,d \left(\frac{y^2}{x^2}\right)}$$ $$2\frac {\,d(xy)}{xy} = \frac{\,d \left(\frac{y^2}{x^2}\right)}{\sqrt{\frac{y^2}{x^2}}}$$ $$2ln(xy) = 2\frac{y}{x} +C$$ $$$$ But the answer given was : $ln \left( \frac{x}{y} \right) - \frac{1}{xy} = C $ I don't see anything I did wrong...... Could anyone share how to do this problem or where I went wrong? Thanks EDIT: I found out what I did wrong thanks to an answer by @J.G. NOw I just wish to know the various methods to this question.",['ordinary-differential-equations']
3624125,"Exact determination of the probability distribution of a nonlinear function of two normally distributed variables, or of its standard deviation.","As a part of a problem in the design of an electronics apparatus, I am trying to analyze the probability distribution of the following quantity $$
\bar{g}_m=\frac{g_1g_2}{g_1+g_2}\label{1}\tag{1}
$$ from the point of view of its probabilistic behavior: parameters $g_1$ and $g_2$ (which are part of the small signal model of a semiconductor device) have normally distributed values (around their ""nominal"" one), and their correlation is $0$ . I do not know the exact value of the standard deviation $\sigma_i$ , $i=1,2$ of their value but I know their so called ""matching"" i.e. I know the value $$
\frac{\Delta g_i}{g_i}=k\sigma_i>0\qquad i=1,2\label{2}\tag{2}
$$ where $k$ is an integer $\ge 6$ (these devices are produced in millions of units, so the devices which do not satisfy \eqref{2} and must be rejected during the test phase should be less than one part per million) and can be assumed constant for both $g_1$ and $g_2$ : for the sake of precision, I can say that $\frac{\Delta g_1}{g_1}\simeq\frac{\Delta g_2}{g_2}\simeq 10\%$ , even if this is not very useful from the point of view of the problem I am posing. So my question is Is it possible to determine explicitly the probability distribution of $\bar{g}_m$ , or at least a precise estimate for the matching $\frac{\Delta g_m}{g_m}$ from values of the matching of $g_1$ and $g_2$ expressed by \eqref{2}? As it can bee seen, the question is equivalent to ask if it is possible to determine explicitly (or at least sharply estimate) the standard deviation $\sigma_m$ of $\bar{g}_m$ from the knowledge of $\sigma_1$ and $\sigma_2$ . Notes What I know : V. K. Rohatgi has developed a way of determine the probability distribution of the product of two random variables by using the Mellin transform of their distributions. However, \eqref{1} is not a simple product of random variables, but is a nonlinear algebraic function of two random variables , therefore a knowledge deeper than mine of the applicable probabilistic techniques may be required (read as: I am not an expert in applied probability). What I customarily do in common designs and why I cannot proceed in the same way for this one . The basis of the two methods I use (and, in my opinion, many other engineers customarily do) is the standard technique inherited from the theory of error propagation $$
\mathrm{d}\bar{g}_m =\frac{\partial\bar{g}_m}{\partial g_1}\mathrm{d}g_1+\frac{\partial\bar{g}_m}{\partial g_2}\mathrm{d}g_2\implies
\begin{align}
\Delta\bar{g}_m &\simeq\frac{\partial\bar{g}_m}{\partial g_1}\Delta g_1+\frac{\partial\bar{g}_m}{\partial g_2}\Delta g_2\\
\frac{\Delta\bar{g}_m}{\bar{g}_m }&\simeq\frac{{g}_1}{\bar{g}_m }\frac{\partial\bar{g}_m}{\partial g_1}\frac{\Delta g_1}{g_1 }+\frac{{g}_2}{\bar{g}_m }\frac{\partial\bar{g}_m}{\partial g_2}\frac{\Delta g_2}{g_2}\\
&=\alpha_1\frac{\Delta g_1}{g_1}+\alpha_2 \frac{\Delta g_2}{g_2}
\end{align}\label{3}\tag{3}
$$ Assuming \eqref{3}, the I use one of the following two estimates: The ""standard"" error propagation theoretical estimate $$
\left\vert\frac{\Delta\bar{g}_m}{\bar{g}_m }\right\vert\le|\alpha_1|\frac{\Delta g_1}{g_1}+|\alpha_2| \frac{\Delta g_2}{g_2}\label{I}\tag{I}
$$ A more refined estimates, that is an equality for the sum of normally distributed variables $$
\left\vert\frac{\Delta\bar{g}_m}{\bar{g}_m}\right\vert\le\sqrt{\left(\alpha_1\frac{\Delta g_1}{g_1}\right)^{\!2}+\left(\alpha_2 \frac{\Delta g_2}{g_2}\right)^{\!2}}\label{II}\tag{II}
$$ I use almost always \eqref{I}. However, despite being optimal (from the value-to-cost ratio point of view) for medium/small production batches (from 100 to few thousands units per month), this estimates is too pessimistic and it would rise excessively the costs for large production batches, if I use it to chose the matching of $g_1$ and $g_2$ in order to get the desired matching on $\bar{g}_m$ . On the other hand, \eqref{II} is a little bit more optimistic, but how much is it more optimistic ? A note after the comment of Nap D. Lover . The parameters $g_1$ and $g_2$ are explicitly independent: de facto , they are associated to two different devices, even technologically very different.","['statistics', 'probability-distributions', 'probability-theory']"
3624178,"Existence of Sigma algebra $\mathbb{G}$ such that $M = L^2(\Omega, \mathbb{G}, \mathbb{P})$","Let $(\Omega, \mathbb{F}, \mathbb{P})$ be a probability space and $M$ be a closed linear subspace of $L^2(\Omega, \mathbb{F}, \mathbb{P})$ . My question is : Does there exists a sigma algebra $\mathbb{G} \subset \mathbb{F}$ such that $M = L^2(\Omega, \mathbb{G}, \mathbb{P})$ ? My attempt: Define $G = \sigma\{X : X \in M\}$ then $ L^2(\Omega, \mathbb{G}, \mathbb{P})$ is a closed subspace and we have that $M \subset  L^2(\Omega, \mathbb{G}, \mathbb{P})$ . But I´m having trouble proving that $L^2(\Omega, \mathbb{G}, \mathbb{P}) \subset M$ I´m currently studying conditional Expectation, and I know that given a  sigma algebra $\mathbb{G} \subset \mathbb{F}$ and $X\in L^2(\Omega, \mathbb{G}, \mathbb{P})$ then $E[X | \mathbb{G}]$ is the orthogonal projection  of X to the closed linear subspace $ L^2(\Omega, \mathbb{G}, \mathbb{P})$ On the other hand given a closed subspace $M $ of $L^2(\Omega, \mathbb{F}, \mathbb{P})$ and $X \in  L^2(\Omega, \mathbb{F}, \mathbb{P})$ , by general theory of Hilbert Spaces there exists a unique $Z$ such that it is the orthogonal projection of $X$ to the closed subspace $M$ , that is $Z = P_M(X)$ and in some books $P_M(X)$ (for example in the book Time Series by Peter and Brockwell) is the definition of the conditional expectation of $E[X|M]$ . The problem I have is that by general probability theory, conditional expectation is defined to be with respect to a sigma algebra $\mathbb{G} \subset \mathbb{F}$ . So if we want that $E[X|M]$ concides with the probability definition then necessarily $M = L^2(\Omega, \mathbb{G}, \mathbb{P})$ for some sigma algebra $\mathbb{G} \subset \mathbb{F}$ and then we can define $E[X|M]$ as $E[X| \mathbb{G}]$ I would really appreciate any suggestion or comment with this problem. Edit 1 : as triple_sec pointed out, there is a counterexample if $M$ does not have constant functions, So what if we allow $M$ to be a closed linear subspace that also contains the constant functions. Can we guarantee the existence of the sigma algebra $\mathbb{G}$ ?","['measure-theory', 'orthogonality', 'conditional-expectation', 'hilbert-spaces', 'probability-theory']"
3624188,Why are these two inequalities not the same even though they use the same equation?,"I just don't get it, like at all. $U_{n}$ is an iteration defined on $\mathbb{N}$ , BTW. The question was: $$\begin{align}
U_{n+1} &=
\frac{8U_n - 8}{U_{n} + 2} = 8 - \frac{24}{U_{n} + 2}\\
U_0 &= 3
\end{align}
$$ ""Prove that $3 \leqslant U_n \leqslant 4$ by using mathematical induction"" Step 1: Test $n = 0$ , yes, it's correct. Step 2: Let's say the inequality is correct and test it for $n+1$ I used the first equation BTW. The result(for me) was: $\tfrac{16}{6} \leqslant U_{n+1} \leqslant \tfrac{24}{5}$ Now, the exercise solved it by using the second equation, and its result was: $\tfrac{16}{5} \leqslant U_{n+1} \leqslant 4 $ And since $16/5$ is bigger than 3 then it's correct and works. How are there two different results and inequalities when you use each one of them? Isn't $\tfrac{8U_{n} - 8}{U_{n} + 2}$ supposed to be equal to $8 - \tfrac{24}{U_{n} + 2}$ which means no matter which one you use, and there will be the same result? I just don't get it. Which one is correct? How are there two different results and inequalities when you use each one of them? My solution: An inequality for the numerator: \begin{align}
3 &\leqslant U_n \leqslant 4\\
24 &\leqslant U_n \leqslant 32\\
16 &\leqslant U_n \leqslant 24
\end{align} Now the equality for the dominator: \begin{align}
3 &\leqslant U_n \leqslant 4\\
5 &\leqslant U_n + 2 \leqslant 6\\
\tfrac{1}{6} &\leqslant \tfrac{1}{U_{n} + 2} \leqslant \tfrac{1}{5}
\end{align} By multiplying each side of the inequality then: $\tfrac{16}{6} \leqslant U_{n+1} \leqslant \tfrac{24}{5}$","['calculus', 'iterated-integrals', 'algebra-precalculus', 'inequality']"
3624194,"Two ""different"" definitions of $\sqrt{2}$","In Walter Rudin's Principles of Mathematical Analysis (3rd edition) (page 10), it is proved that for every $x>0$ and every integer $n>0$ there is one and only one positive real $y$ such that $y^n=x$ . (This is number $y$ is then written $\sqrt[n]{x}$ .) In particular, this implies the existence of $\sqrt{2}$ . On the other hand, if one considers the polynomial $f(x)=x^2-2$ as an element in the ring $\mathbf{Q}[x]$ , one can adjoin a root of $f$ to $\mathbf{Q}$ . The procedure (see, for instance, Michael Artin's Algebra (2nd edition) page 456) is to form the quotient ring $K = \mathbf{Q}[x]/(f)$ of the polynomial ring $\mathbf{Q}[x]$ . This construction yields a ring $K$ and a homomorphism $F\to K$ , such that the residue $\overline{x}$ of $x$ satisfies the relation $f(\overline{x})=0$ . In the real analysis case, $\sqrt{2}$ can be approximated (or defined, depending on how one constructs the real numbers) by a Cauchy sequence of rational numbers: $$
1, 1.4, 1.41, 1.414, 1.4142, \cdots
$$ In the abstract algebra case, the set of real numbers is absent; one does not even need to define it. And there is no way to ""approximate"" $\overline{x}$ . These two ways to define the object $\sqrt{2}$ seems to be somewhat different in that the defined object has rather different properties. How should one understand the ""discrepancy"" here? Are there other relations/connections between these two definitions besides being a root of $f(x)=x^2-2$ ?","['real-numbers', 'abstract-algebra', 'soft-question', 'real-analysis']"
3624255,Invertible sheaf associated to a Cartier divisor,"Why do some authors define the invertible sheaf $\mathcal{L}(D)$ , associated to a Cartier divisor $D = (U_{i}, f_{i})_{i\in I}$ , by setting $\mathcal{L}(D)_{U_{i}} = \mathcal{O}_{X|U_{i}}\cdot f_{i}^{-1}$ , instead of $\mathcal{L}(D)_{U_{i}} = \mathcal{O}_{X|U_{i}}\cdot f_{i}$ . Both $\mathcal{O}_{X|U_{i}}$ -modules, $\mathcal{O}_{X|U_{i}}\cdot f_{i}^{-1}$ and $\mathcal{O}_{X|U_{i}}\cdot f_{i}$ are isomorphic, but is there a deep reason to invert $f_{i}$ ?","['divisors-algebraic-geometry', 'algebraic-geometry', 'sheaf-theory']"
3624285,Continuity of $\sqrt{x}\sin{\frac{1}{x}}$,"I want to determine what values of f are continuous then show it, where $f(x) = \begin{cases} 
      \sqrt{x}\sin{\frac{1}{x}} & x \ne 0 \\
      0 & x=0 
   \end{cases}
$ Given that $\sqrt{x}$ is not defined for values of $x \lt 0$ , f is not continuous in that range. Also, at $x = 0$ , there has to be a value arbitrarily close to x, a such that $a \lt 0$ hence $f(a)$ is undefined so $f$ is not continuous at $x = 0$ I see no reason for $f$ to not be continuous at $x \gt 0$ , so then I want to show that that is true. To prove this, I started by fixing $a \in \Bbb{R}$ then letting $\epsilon \gt 0$ and then choose $\delta$ based on how the rest of proof finishes. Either way, I can suppose that $0 \lt |x-a| \lt \delta$ and since $x, a \gt 0$ , we can get $x + a \lt \delta$ or something similar, again, based on later parts of the proof. Then I've tried a variety of things to manipulate $|f(x)-f(a)|$ , the most hopeful being $|f(x)-f(a)|^2 \le x + a + 2\sqrt{a}\sqrt{x} \lt \delta + 2\sqrt{a}\sqrt{x}$ from before. My issue is I do not know how to finish this proof or even if this is a good method. Thanks in advance! edit: This is my idea based on a comment. Its known that $\sqrt{x}, \sin{\left(x\right)}, and \frac{1}{x}$ are continuous. so $ \sin{\left(\frac{1}{x}\right)}$ is continuous and hence $\sqrt{x}\sin{\frac{1}{x}}$ is continuous as it's the product of continous functions. Is this correct?","['epsilon-delta', 'proof-writing', 'real-analysis', 'continuity', 'functions']"
3624308,"Solving Differential Equation : $(1+\tan y)(\,dx - \,dy) + 2x\,dy = 0$","The Question is: $$(1+\tan y)(\,dx - \,dy) + 2x\,dy = 0$$ I tried the problem by: $$(\tan y+1)\left( 1 - \frac{\,dy}{\,dx} \right) +2x\frac{dy}{dx} = 0$$ Then I put $\tan y = u $ $\frac{du}{dx}.\frac{1}{1+u^2} = \frac{dy}{dx}$ However going ahead doesn't yield any useful expressions. Also I don't see any clear product rule expressions. How would you solve this ?",['ordinary-differential-equations']
3624357,"What does ""$f: \mathbb{R} \to[0,\infty), f(x)=x^2$"" mean? What is the co-domain, domain, or range?","What does this mean? $$f: \mathbb{R} \to[0,\infty), f(x)=x^2$$ What is the co-domain, domain, or range here? I'm lost. This might be a dumb question but I'm not able to read many questions properly due to this format.","['elementary-set-theory', 'algebra-precalculus', 'functions']"
3624369,"Asymptotic distribution of sum of independent, not identically distributed, bernoulli distribution with poisson tail probability","I'm trying to find an asymptotic distribution of the follwing random variable $$Z_n=\sum_{i=1}^n Y_i$$ where $Y_i = I[T_i<t]$ with $T_i \text{~} Gamma(i, \lambda)$ . Here $t$ is a fixed number. My initial trial was using the Lyapunov CLT ( or Linderberg CLT). First I noticed that by using integral by parts, $$E[Y_i]=P[Y_i=1]=P[T_i<t]=\sum_{k=i}^\infty \frac{(t/\lambda)^ie^{-t/\lambda}}{i!}$$ which is the tail probabilty of the poisson random variable with mean $t/\lambda$ . Let $p_i:=E[Y_i], R_i:=Y_i-p_i, S_n:=\sum_{i=1}^nR_i, \sigma_n^2:=Var(S_n)=\sum_{i=1}^np_i(1-p_i)$ . I'm trying to show that $R_i$ satisfies the Lyapunov condition where $$ \sigma_n^{-(2+\delta)}\sum_{i=1}^nE[|R_i|^{(2+\delta)}]\to0\ \text{as $n\to \infty$ }$$ . With $\delta=1$ , 
 I get $$E[|R_i|^3]=(1-p_i)^3p_i + p_i^3(1-p_i)^3\le p_i(1-p_i)$$ therefore $$\sigma_n^{-3}\sum_{i=1}^nE[|R_i|^{3}] \le \sigma_n^{-1}.$$ As long as I can show that $$\sigma_n \to \infty \ \text{as $n\to \infty$ }$$ I'm done with the proof. But I'm stuck with showing the divergence of $\sigma_n$ . In other words, I need to show that $$ \sum_{i=1}^n p_i(1-p_i) \to \infty \ \text{as $n\to \infty$ }.$$ I sincerely ask to help me with the proof of the last statement. Or Am i doing something wrong? Thank you.","['statistics', 'central-limit-theorem', 'poisson-distribution', 'asymptotics', 'probability']"
3624453,Infinite product Lebesgue measure as the pushforward of 1-Lebesgue measure,"I want to construct a Borel map from the unit interval to the Hilbert cube $f: [0,1] \to [0, 1]^\mathbb N$ so that \begin{equation}
\lambda \left(f^{-1}\left( \prod_{i \in \mathbb N} E_i\right)\right)= \prod_{i \in \mathbb N} \lambda(E_i)
\end{equation} for $\lambda$ the Lebesgue measure on the interval, $E_i \subseteq [0, 1]$ Borel, and $E_i = [0, 1]$ for all but finitely many indices. This gives the construction of the product measure without appealing to the Kolmogorov extension theorem (c.f. Tao's An Introduction to Measure Theory for the Kolmogorov approach to infinite product spaces). In the general case, I want to find a Borel map $f: [0, 1] \to \mathbb R^{\mathbb N}$ so that \begin{equation}
\lambda \left(f^{-1}\left( \prod_{i \in \mathbb N} E_i\right)\right)= \prod_{i \in \mathbb N} \mu_i(E_i)
\end{equation} for Radon probability measures $\mu_i$ on $\mathbb R$ . My initial thought was to try to encode the Hilbert cube into the dyadic intervals $[1/2^{n + 1}, 1/2^n]$ , e.g. map these into the edges of the Hilbert cube, and try to construct measure preserving maps \begin{equation*}
[0, 1] \to \bigsqcup_{n \in \mathbb N} [0, 1] \to [0, 1]^{\mathbb N}.
\end{equation*} The first map isn't too bad, however the second is more nebulous. The thought was that this has something to do with independent events in $[0, 1]$ representing a rectangle in the Hilbert cube, e.g. $A \times B \times [0, 1] \times \cdots$ gets pulled back to $A \cap B$ . My second thought was to construct a space-filling curve in spirit of showing the $d$ -dimensional Lebesgue measure $\lambda_d$ can be realised as the pushforward of $\lambda$ (c.f. the discussion here for existence of a space filling curve and here which states the Hilbert and Peano curves are measure preserving space filling curves). The second seems a bit unwieldy but an approach that can work. The first seems more succinct but I can't get the details right. Moreover, the first seems more easy to generalize, i.e. if we replace $\lambda$ on the right hand side of our initial equation with Radon probability measures $\mu_i$ on $\mathbb R$ and the map into the Hilbert cube with a map $f: [0, 1] \to \mathbb R^{\mathbb N}$ by considering the cumulative distribution functions of $\mu_i$ .","['measure-theory', 'lebesgue-measure', 'probability-theory', 'probability']"
3624478,Picard group in different settings,"Picard group is a recurring notion that I've been seeing very often recently recently. For projective varieties, Picard group is the divisor class group. For Dedekind domains, it is the ideal class group. For rings, Picard group can be defined on its algebraic line bundles. In the Wikepedia page, it says it can be defined for any ringed space. Or even more generally , it can be defined for any symmetric monoidal category. But I have yet to find a good exposition to link these together. So my questions are: How does one see that the Picard groups in the cases of divisor class groups, ideal class groups and line bundles are related? Follow up to (1.), what are some examples (using the relation of Picard groups in different settings) that allow us to understand a number theoretic problem geometrically, or a geometry problem arithmetically? Are there results or properties of Picard groups in the general settings (ringed spaces or symmetric monoidal categories) that turn out to be significant when we examine it in the more specific settings?","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
3624507,"Given two functions $\phi(k)$ and $E(k)$, how can I determine the functional relationship $\phi(E)$?","If I have two functions $\phi(k)$ and $E(k)$ expressed explicitly as (complicated) functions of $k,$ obtained using computer algebra software. I have reason to suspect that $\phi$ could be written purely as a function of $E,$ that is, $\phi(E),$ but not $\phi(E(k),k).$ Is there a way to test if such a functional relation exists, given the information I have? If such a relation exists, how might I go about finding the functional form of $\phi(E)$ ? Is there a general procedure? I have done some exploration using the chain rule. For example, I can study the function $$\frac{d\phi}{dE}=\frac{\frac{d\phi}{dk}}{\frac{dE}{dk}},$$ (which will be expressed as a function of $k$ ) and any higher order derivatives I like. However, what I really want is $\phi(E)$ . Thanks!","['calculus', 'functions']"
3624538,Hilbert's Hotel Paradox: Guests moving to new room every day?,"Suppose there are infinitely many coaches with infinitely many members in each coach. They stay at the hotel for infinitely many days. I know that guests can be accommodated using various methods like the prime powers method, but there's a slight variation in the question which is that the guests have to change their room every day such that one guest can't occupy the same room again (i.e., they have to occupy unique rooms every day). How can we achieve that? I tried solving the problem using the following method: I allotted rooms using the prime powers method. The next day, guests move from their current room $x$ to the new room $x+c$ . I'm struggling after this step. Can someone please help me out?","['paradoxes', 'infinity', 'discrete-mathematics', 'elementary-set-theory', 'prime-numbers']"
3624543,Fibre Product of $\operatorname{Proj} S$,"I have problem in doing Hartshorne's exercise II.5.11. It says Let $S$ and $T$ be two graded rings with $S_{0}=T_{0}=A .$ We define the Cartesian product $S \times_{A} T$ to be the graded ring $\oplus_{d> 0} S_{d} \otimes_{A} T_{d}$ . If $X= \operatorname{Proj} S$ and $Y=\operatorname{Proj} T,$ show that $\operatorname{Proj}(S \times_{A} T) \cong X \times_{A} Y$ , and show that the sheaf $\mathscr{O}(1)$ on $\operatorname{Proj}(S \times_{A} T)$ is isomorphic to the sheaf $p_{1}^{*}\left(\mathscr{O}_{X}(1)\right) \otimes p_{2}^{*}\left(\mathscr{O}_{Y}(1)\right)$ on $X \times Y$ . I can show $\operatorname{Proj} (S \times_{A} T) \cong X \times_{A} Y$ , by checking in affine neighborhoods. However, I'm stuck in the second statement. My questions are: Question 1. Does $\mathscr{O}(1)$ on $\operatorname{Proj}(S \times_{A} T)$ means $\mathscr{O}_{X\times_A Y}(1)$ , when seeing $X\times_A Y$ as $\operatorname{Proj}(S \times_{A} T)$ , under the twisting sheaf of Serre meaning? (I guess it's right.) Question 2. If $\mathscr{O}(1)$ means the twisting sheaf of Serre, I still have no idea solving it. Can anyone give me a solution? Thank you very much!",['algebraic-geometry']
3624594,"Prove or disprove that, if $a*b$ has an inverse with respect to $*$, an associative binary operation with identity $e$, then so do $a$ and $b$.","This question had two preceding parts. First, to prove that the inverse of an element $a$ is unique and second, to prove the converse of this statement. I.e. if $a$ and $b$ have inverses, then so does $a*b$ . I didn't have much of a problem with both of these but this third part has me stumped. At first, I believed, it was true and tried to work from the definition of an inverse: $(a*b)*c=c*(a*b)=e$ Using the fact that $*$ is associative, we have $a*(b*c)=e$ and $(c*a)*b=e$ which is half of the way to showing $a$ and $b$ have identities. However, I then became completely stuck on trying to show that $b*c*a=e$ (brackets omitted due to associativity) which doesn't seem to be true without commutativity. I then assumed the statement was false and went looking for counterexamples. I tried the composition of functions and matrix multiplication as both are associative but not commutative. In the case of functions, a function, $g•f$ , must be bijective to have an inverse. This combined with other theorems implies that $g$ and $f$ are bijective. Thus, the statement holds here. A similar thing arises when looking at matrix multiplication where multiplying a non-invertible matrix $A$ by any other matrix $B$ gives a non-invertible matrix $AB$ .
As both counterexamples failed, I am well and truly stuck and have no idea how to continue with this problem. Thanks for any help.","['elementary-set-theory', 'binary-operations']"
3624600,Birkhoff averages convergence,"Let be $(X,\mathcal{A},\mu)$ a probability space, $T:X\to X$ a measurable tranformation preserving $\mu$ and $f: X \rightarrow \mathbb C$ a measurable function. Show that for almost every $x \in X$ either: \begin{equation}
\lim_{N \rightarrow \infty} \frac{1}{N} \sum_{n=0}^{N-1} |f(T^n(x))| = \infty
\end{equation} or \begin{equation}
\lim_{N \rightarrow \infty} \frac{1}{N} \sum_{n=0}^{N-1} f(T^n(x)) \quad\text{exists and it's finite.}
\end{equation} My attempt :
Since $f$ isn't necessarily integrable, i tried to applicate Birkhoff ergodic theorem to $f_m = f 1_{|f| \leq m} + m 1_{|f| > m}$ which is an integrable function. The problem that i have is that i can't seem to get an upper bound for $\int_E f $ , where $E = \{ x | \sup_N S_N^{|f|}(x) < + \infty \}$ where $S_N^f$ are the Birkhoff averages of $f$ . I have shown that $E$ an invariant set. Does $S_N^f(x)$ converge to $\int_E f d\mu$ or i am on a bad track?","['measure-theory', 'ergodic-theory']"
3624625,"If $A$ and $B$ be two non-singular matrices of order $3$ such that $2A+3BB^{T}=I$ and $B^{-1}=A^{T}$, then determine the value of the following:","If $A$ and $B$ be two non-singular matrices of order $3$ such that $2A+3BB^{T}=I$ and $B^{-1}=A^{T}$ , then determine the value of the following: (i) $\det\left(B^{T}-2B+3B^{3}+3BA\right)$ (ii) $\det\left(A^{-1}-3B^{3}\right)$ (iii) $\text{trace}\left(A^{-1}+I-AB-3B^{3}\right)$ My Attempt: Given $2A+3BB^{T}=I$ Taking transpose $2A^{T}+3BB^{T}=I$ So on subtracting we get $A=A^{T}$ i.e. $A$ is symmetric Given that, $B^{-1}=A^{T}=A$ and thus $AB=BA=I$ Now, all I need to get the answers to the above questions is to somehow prove that $B$ is also symmetric i.e. $B^{T}=B$ $AB=I$ $B^TA^T=I$ $B^TB^{-1}=I$ $B^T=B$ Given that $2A+3BB^T=I$ $2B^{-1}+3B^2=I$ $B-3B^3=2I$ Thus, $B^T-2B+3B^3+3BA=B-2B+3B^3+3I=3B^3-B+3I=-2I+3I=I$ Also, $A^{-1}+I-AB-3B^3=A^{-1}-3B^3=B-3B^3=2I$","['matrix-rank', 'matrices', 'orthogonal-matrices', 'linear-algebra', 'symmetric-matrices']"
3624697,Can weak convergence of probability measures be characterized by countably many functions without having a limit a priori?,"Let $(\mu_n)_{n \geq 1}$ be a sequence of Borel probability measures on $\mathbb{R}^d$ . I'd like to know the following: Does there exist a countable family $(f_k)_{k \geq 1}$ of continuous, bounded real-valued functions with the following property: If $\text{lim}_{n}\int f_k d\mu_n$ exists in $\mathbb{R}$ for each $k \geq 1$ , then there exists a unique Borel probability measure $\mu$ such that $\mu_n \underset{n \to \infty}{\longrightarrow} \mu$ weakly? Clearly, it'd be sufficient to take a countable, dense subset of $C_b(\mathbb{R}^d)$ - only problem, such set doesn't exist ;-). On the other hand, the Riesz-Markov-representation theorem shows that a dense countable subset of $C_0(\mathbb{R}^d)$ (the continuous functions vanishing at infinity) [which exists - $C_0$ is separable] is ""too small"" in the sense that it allows mass to spread out at infinity, which yields that the limit measure $\mu$ is in general only a sub-probability measure. Next, I was thinking about the uniformly continuous bounded functions - but again: not separable. Next thought: Consider the vector space spanned by $C_0$ and $1$ . But for this vector lattice, the positive, linear, normalized functional $J: f \mapsto \text{lim}_n\int f d\mu_n$ is not continuous (also called $\sigma$ -continuous), meaning it does not hold $f_l \to 0$ pointwise decreasing from above $\implies$ $J(f) \to 0$ (which is, however, true for the vector lattice $C_0$ , which is essential for the proof of Riesz-Markov representation). Hence the classical Daniell-Stone theory does not apply, so we cannot obtain the desired limit measure (at least not by this method). Any comment or help on this is much appreciated!","['weak-convergence', 'probability-theory', 'functional-analysis', 'measure-theory']"
3624705,"How interpret the notation $f:\{0,\dots, N-1\} \rightarrow \{0,\dots, N-1\}$, $N$ is a number of the form $2^n$?","I need help how to interpret the following notation for $f$ : Zeroes and ones form a binary number which can be converted to
  decimal notation. Thus, we may think of the computer as calculating a function $$
f:\{0,\dots, N-1\} \rightarrow \{0,\dots, N-1\}, 
$$ where $N$ is a number of the form $2^n$ , and $n$ is the number of bits in the computer memory. In this description, $f$ must be a function because the computer cannot generate two or more different outputs from the same input. We assume without loss of generality that the domain and codomain of $f$ are of the same size. In other words, we assume that both the input and the output of the computer have the same number of bits. Update: I understand the function notation \begin{align}
f&:\mathbb R \rightarrow \mathbb R_+ \\
x& \mapsto f(x)
\end{align} so if $x\in \mathbb R$ we have $f(x)\in\mathbb R_+$ .
So far so good. However I don't follow the meaning (mapping) of $\{0, \dots, N-1\}$ in this case. Attempt: Say I have the decimal number $5$ , so $N= 5$ . I guess ""of the form $2^n$ "" means a binary number, i.e. $5_{10}=(0101)_2$ and thus $n=4$ . So I have the function $$
f: \{0, 1, 2, 3, 4\} \rightarrow \{0, 1, 2, 3, 4\}
$$ Is this correct? Or is the domain and codomain of $f$ a binary number?  I.e. no commas in the sets $$
f: \{0101\} \rightarrow \{0101\}
$$ Thanks in advance!","['boolean-algebra', 'notation', 'functions', 'discrete-mathematics', 'elementary-set-theory']"
3624724,Explanation on a proof of a lemma on the Adams-Bashforth method,"Lemma: For the two-step Adams-Bashforth (AB) method $$T(t)=\frac{5}{12} y'''(t)h^2+\mathcal{O}(h^3),$$ where $T(t)$ denotes the local truncation error. By definition, $$T(t)=\frac{y(t+h)-y(t)}{h}-f(t,y(t)).$$ For the AB method, $$T(t)=\frac{y(t+h)-y(t)}{h}-\sum_{j=0}^{r}b_jf(t-jh,y(t-jh)).$$ The lemma is the result for $r=1$ .  By Taylor's theorem, \begin{align}
\frac{y(t+h)-y(t)}{h}&=\frac{\left(y(t)+y'(t)h+y''(t)h^2/2+y'''(t)h^3/6+\mathcal{O}(h^4)\right)-y(t)}{h} \tag{1}\\
&=y'(t)+\frac{y''(t)}{2}h+\frac{y'''(t)}{6}h^2+\mathcal{O}(h^3) \tag{*}
\end{align} For $r=1$ : \begin{align}
\sum_{j=0}^{1}b_jf(t-jh,y(t-jh))&=b_0f(t,y(t))+b_1f(t-h,y(t-h)) \\
&=\frac{3}{2}f(t,y(t))-\frac{1}{2}f(t-h,y(t-h))  \ \ \ (b_0=3/2, \ b_1=-1/2)\\
&=\frac{3}{2}y'(t)-\frac{1}{2}y'(t-h) \ \ \ (y'(t)=f(t,y(t)) ) \\
&=\frac{3}{2}y'(t)-\frac{1}{2}\left(y'(t)-y''(t)h+\frac{1}{2}y'''(t)h^2+\mathcal{O}(h^3)\right) \tag{2} \\
&=y'(t)+\frac{y''(t)}{2}h-\frac{y'''(t)}{4}h^2+\mathcal{O}(h^3) \tag{**}
\end{align} Now the result follows by subtracting $(**)$ from $(*)$ , but I don't understand how Taylor's theorem was used in steps $(1)$ and $(2)$ , i.e. the expansions of $y(t+h)$ and $y'(t-h)$ .","['proof-explanation', 'numerical-methods', 'taylor-expansion', 'ordinary-differential-equations']"
3624834,Abstract Algebra ( tutoring suggestions ),"I'm looking for some good Abstract Algebra courses on youtube but i couldn't find any , so if someone has any suggestions please help me ( or a book with some nice problems would be good also ).","['group-homomorphism', 'algebraic-groups', 'field-theory', 'abstract-algebra', 'elementary-set-theory']"
3624888,What are the integer solutions to $5x^3=y^2+1$?,"I want to find the integer solutions to this Diophantine equation: $$5x^3=y^2+1$$ I have seen a lot of problems with monic variables, but not with a constant on the $x^3$ such as this. I know I can factorise the right hand side and get $5x^3=(y-i)(y+i)$ , so I can work in $\mathbb Z[i]$ .  But I am unsure where to proceed from here, and how the $5$ comes into the problem.","['number-theory', 'abstract-algebra', 'elementary-number-theory', 'diophantine-equations']"
3624895,Why we can draw a random number $s$ in a normal distribution when we are taught that $p(s)=0$?,"In various programming languages such as Matlab and Python, we can draw a random number s from virtually any continuous distribution, such as the normal and uniform. But we're also taught that $p(s)=0$ for any specific value $s$ in the support of a continuous distribution. So, my question is, how can we explain this paradox? Thank you very much!","['programming', 'statistics', 'probability-distributions', 'probability']"
3624915,"Prove: $\frac{\sin{nx}}{\sin{x}}\geqslant{n-\frac{n(n^2-1)x^2}{6}},n\in{\mathbb{N}\setminus\{0\}, x\in{\mathbb{R}, x\neq k\pi}} $","Prove : $\frac{\sin{nx}}{\sin{x}}\geqslant{n-\frac{n(n^2-1)x^2}{6}},n\in{\mathbb{N\setminus\{0\}}}, x\in{\mathbb{R}} $ I proved this relationship by incident. I tried to directly prove this afterwards, but failed. I would love to see another proof to this Problem. - A proof -: we know that $\displaystyle\sum_{i=1}^{n}\cos{a_i}= \sum_{k=1}^{n}\cos{(a+(k-1)x)}=\frac{\cos{\frac{a+a_n}{2}}\sin{\frac{nx}{2}}}{\sin{\frac{x}{2}}}=T(x)\quad(1)$ ( Here's the source for $(1)$ ).
Also, $\displaystyle\sum_{i=1}^{n}a^2_i=\sum_{k=1}^{n}(a+(k-1)x)^2\\=\displaystyle\sum_{k=1}^{n}(a^2+2ax(k-1)+x^2(k-1)^2)\\=\displaystyle\sum_{k=1}^{n}a^2+2ax\sum_{k=1}^{n}(k-1)+x^2\sum_{k=1}^{n}(k-1)^2\\=na^2+2ax\frac{n(n-1)}{2}+x^2\frac{n(n-1)(2n-1)}{6}\quad(2)$ We consider the function $f(x)=\frac{x^2}{2}+\cos{x}\implies f''(x)=1-\cos{x}\geq{0}$ therefore $f(x)$ is a concave function. From Jensens inequality for concave functions: $ \displaystyle\sum_{i=1}^{n}f(a_i)\geq{nf\left(\frac{\sum_{i=1}^{n}a_i}{n}\right)}\iff\displaystyle\sum_{i=1}^{n}\left(\frac{a^2_i}{2}+\cos{a_i}\right)\geq n\Big(\frac{1}{2}\left(\frac{\frac{n}{2}(a+a_n)}{n}\right)^2+\cos{\frac{\frac{n}{2}(a+a_n)}{n}}\Big)\iff\frac{1}{2}\sum_{i=1}^{n}a^2_i+\sum_{i=1}^{n}\cos{a_i}\geq \frac{n}{2}\left(\frac{a+a_n}{2}\right)^2+n\cos{\frac{a+a_n}{2}}\overset{(1),(2)}{\iff}\frac{1}{2}\left(na^2+2ax\frac{n(n-1)}{2}+x^2\frac{n(n-1)(2n-1)}{6}\right)+T(x)\geqslant \frac{n\left(2a+(n-1)x\right)^2}{8}+n\cos{\frac{a+a_n}{2}}\overset{\ldots}{\iff} \frac{x^2n(n^2-1)}{3}\geqslant n\cos{\frac{a+a_n}{2}}-T(x)\overset{(1)}{\iff} \frac{x^2n(n^2-1)}{3}\geq \cos{\frac{a+a_n}{2}}\left(n-\frac{\sin{\frac{nx}{2}}}{\sin{\frac{x}{2}}}\right)\iff \frac{x^2n(n^2-1)}{3}\geqslant \cos{\left(a+\frac{(n-1)x}{2}\right)}\left(n-\frac{\sin{\frac{nx}{2}}}{\sin{\frac{x}{2}}}\right)\quad(3) $ $Lemma.$ For every dinstict $x,x\in\mathbb{R}_{\neq kπ}$ there exists at least one value of $a$ such that $\cos{(a+\frac{(n-1)x}{2}})=1\quad(5)$ Proof of the lemma: $(5)\iff (n-1)x=2-2a+4kπ\iff a=2kπ+1-\frac{(n-1)x}{2}$ . Hence, for every $x,x\in\mathbb{R}_{\neq k\pi}$ we have $\frac{x^2n(n^2-1)}{3}\geqslant \cos{\left(a+\frac{(n-1)x}{2}\right)}\left(n-\frac{\sin{\frac{nx}{2}}}{\sin{\frac{x}{2}}}\right)\overset{(4,x\to 2x)}{\iff}\frac{\sin{nx}}{\sin{x}}\geqslant{n-\frac{n(n^2-1)x^2}{6}} \square$ To me it looks like $n-\frac{x^2n(n^2-1)}{6}$ is a good approximation function of $\frac{\sin{nx}}{\sin{x}}$ for small values of $x$ (or for small values of $x+ z\pi$ , $z\in\mathbb{Z}$ , it depends on $n$ , because the second one is periodic with period multiple of $\pi$ ).","['inequality', 'real-analysis', 'sequences-and-series', 'power-series', 'trigonometry']"
3624988,A problem about orthocenter and circumscribed circle,"The orthocenter of $\triangle ABC$ is $H$ . $E,F$ are on $BC, AC$ such that $\angle EHF = \angle C$ . $G$ is on the circumscribed circle of $\triangle ABC$ such that $AG \parallel HE$ . Prove that $EF$ bisects $HG$ I found the condition $\angle EHF = \angle C$ rather awkward but important. Not sure how to use it..","['contest-math', 'euclidean-geometry', 'geometry', 'triangles', 'plane-geometry']"
3625094,USAMO 2011 problem #5,"Let $P$ be a given point inside quadrilateral $ABCD$ . Points $Q_1$ and $Q_2$ are located within $ABCD$ such that $\angle Q_1 BC = \angle ABP$ , $\angle Q_1 CB = \angle DCP$ , $\angle Q_2 AD = \angle BAP$ , $\angle Q_2 DA = \angle CDP$ . Prove that $\overline{Q_1 Q_2} \parallel \overline{AB}$ if and only if $\overline{Q_1 Q_2} \parallel \overline{CD}$ I saw its solution. It involves isogonal conjugates.I know that isogonal conjugates is the reflection of a point along each angular bisector,actually they coincide but I think this is not the case with a point outside the triangle.Here P' is the reflection of P across bisector of $\angle A$ but it is not the same point as reflection across other bisectors?? Solution states that points $P$ and $Q_1$ are isogonal conjugates with respect to triangle BCX.I don't think they are reflections.I don't  understand how are they conjugates.Is there any flaw in my understanding?? Correct me? Here is my picture of diagram","['contest-math', 'euclidean-geometry', 'quadrilateral', 'geometry', 'plane-geometry']"
3625137,Three way duel: which gun to choose?,"Three shooters compete in three way duel game. Game 1 Rules: Shooters take turns to shoot. If it's your turn, you have to choose one other person to shoot, and cannot pass your turn or shoot in air, etc.. For the sake of fairness, shooters draw lots to decide who shoots first, second and third. They then fire in this order repeatedly until only one survives. Everyone is rational and calculates to maximize his survival probability. Before the game starts, there're three guns available to choose from, whose hitting probabilities are not revealed, but are known to have been drawn from $U[0,1]$ independently. The gun with the highest hitting probability is labeled ""1"", the one with the 2nd highest is labeled ""2"", and worst one is labeled ""3"". Shooters understand what the labels mean. After each chose his gun, the guns' exact hitting probabilities $g_1,g_2,g_3$ are reveal to all, and the game starts (aka Players draw lots and start shooting). Question: If you're the first one to choose a gun, which one should you choose to maximize your surviving probability? Which gun gives you the least surviving probability? Game 2 Rules: Each turn, a fair dice is flipped to decide who should shoot in this turn. If it's your turn, you have to choose one other person to shoot, and cannot pass your turn or shoot in air, etc.. Step 1 and 2 are repeated until only one survives. Everyone is rational and calculates to maximize his survival probability. Guns have to be chosen before the game starts as in Game 1. Question: If you're the first one to choose a gun, which one should you choose to maximize your surviving probability? Which gun gives you the least surviving probability? Game 0 This is an update. It just occurred to me that allowing the shooter with the worst gun to hold fire in Rule 2 Game 1 will not add much to the computation complexity. This is also more consistent with the spirit of the classical truel game, and is perhaps more reasonable. So while we're at game 1, might as well think about this case. Rules: Same as game 1 but with rule 2 changed, so that the shooter with the worst gun is allowed to hold fire/pass turns. Analysis for game 0: Holding fire can only happen when all 3 shooters are alive. If he
should choose to hold fire, the worst shooter (call him #3) is
essentially waiting to duel with the winner of the duel between #1 and
#2. This gives $$P_{hold}(3,3\vert 3,2,1)=P(2,2\vert 2,1)P(3,3\vert 3,2)+P(1,2\vert 2,1)P(3,3\vert 3,1)$$ $$=\frac{g_2}{g_2+g_1-g_2g_1}\frac{g_3}{g_2+g_3-g_2g_3}+\frac{g_1(1-g_2)}{g_2+g_1-g_2g_1}\frac{g_3}{g_1+g_3-g_1g_3}$$ $$P_{hold}(3,3\vert 3,1,2)=P(1,1\vert 1,2)P(3,3\vert
 3,1)+P(2,1\vert 1,2)P(3,3\vert 3,2)$$ $$=\frac{g_1}{g_2+g_1-g_2g_1}\frac{g_3}{g_1+g_3-g_1g_3}+\frac{g_2(1-g_1)}{g_2+g_1-g_2g_1}\frac{g_3}{g_2+g_3-g_2g_3}$$ where the notation $P(1,2\vert 2,1)$ means #1's survival probability
when its #2's turn to shoot, given the current set of shooters are
ordered in $\vert 2,1)$ , for instance.  To decide whether to hold or
not, #3 only needs to compare $P_{hold}(3,3\vert 3,1,2)$ with $P_{shoot}(3,3\vert 3,1,2)$ , and $P_{hold}(3,3\vert 3,2,1)$ with $P_{shoot}(3,3\vert 3,2,1)$ , where $P_{shoot}$ is computed by game 1.
This is the only additional computation you need to perform for game
0. Some motivations for formulating the games as such: In simpler versions of the classic three way duel game, hitting probabilities are given and you're asked to solve for surviving probabilities for the players. In the above games that goal is in some sense reversed, because I want to know how important is your accuracy (or hit probability) in a somewhat fair setting. Conclusions drawn from just one set of hit probabilities and one set of firing order don't tell much, because they are highly sensitive to those parameters. So you can think of the games as a kind of framework to answering the big picture question: overall, does a better shooter generally have higher survival rate? Unlike solving for instances of the game, questions like this are meta questions for the game, and actually give you more insights about the nature and structure of the game itself. (Meta questions are generally more interesting and challenging, I think. Think of the halting problem as a meta question about algorithms and Godel's incompleteness Theorems as meta questions about arithmetics! I'd better stop before I'm carried too far away by this :-p). The same question can even be asked for cases more than 3 players. For more than 3 players a closed form solution may be impractical to obtain, although simulations could always help. For game 1 for example, Simulation for 4 shooters with guns' hit probabilities $g_1\gt g_2\gt g_3\gt g_4$ randomly chosen shows that $P_{g_3}\gt P_{g_1}\gt P_{g_4}\gt P_{g_2}$ . For 5 shooters, $P_{g_4}\gt P_{g_3}\gt P_{g_1}\gt P_{g_5}\gt P_{g_2}$ . Not intuitive at all. Effective simulation of 6 shooters would take hours. So it seems small teens may be the most you can manage (if you have a super computer at hand). This means you can't go meta on the meta question again. Questions like ""If many shooters play game 1, choosing top notch guns never give you highest survival probability"" just rest safely beyond the ceiling of your computation power.","['integration', 'algorithms', 'game-theory', 'recreational-mathematics', 'probability']"
3625209,Section of a presheaf can be viewed as functions,"I am reading the section on 'Presheaves and sheaves', i.e., Section 6.3 from Bosch's Algebraic Geometry and Commutative Algebra , and I couldn't understand the following : After defining presheaf of sets (or any other category), the author says: ""Sometimes it is convenient to imagine the elements of $\mathcal{F}(U)$ as functions on $U$ ....."" (here $\mathcal{F}$ is a presheaf and $U$ is an open subseet of topological space $X$ ). I can't seem to get my head around seeing how exactly this identification is being made. Please help me with this.",['algebraic-geometry']
3625250,density of numbers that have relatively the same output of euler phi as input,"Let $\phi_a(n)= 1$ if $\phi(n) \geq \frac{n}{a}$ and $0 $ otherwise. where $\phi(n)$ is Euler phi function. Let $S_a(x) = \sum \limits_{n \leq x} \phi_a(n)$ and $M_a = \lim \limits_{x \to \infty}\frac{S_a(x)}{x} $ if the limit exists. Trivially we have $M_a \leq 1$ for  all $a \in \mathbb{R_{+}}$ . The lower bound trivially is $M_a \geq 0$ , with a little work we have $ \sum \limits_{n \leq x} \frac{\phi(n)}{n} \approx \frac{6}{\pi^2}x$ so we can say that $\sum \limits_{n \leq x , \phi_a(n)=1} \frac{\phi(n)}{n}  + \sum \limits_{n \leq x , \phi_a(n)=0} \frac{\phi(n)}{n} \leq \sum \limits_{n \leq x , \phi_a(n)=1} 1  + \sum \limits_{n \leq x , \phi_a(n)=0} \frac{1}{a}  = S_a(x) +\frac{x-S_a(x)}{a} $ So we have that $S_a(x)+\frac{x-S_a(x)}{a} \geq \frac{6}{\pi^2}x $ and so $M_a (1-\frac{1}{a}) \geq \frac{6}{\pi^2}-\frac{1}{a} $ so for $a\geq 2$ we have that $M_a \geq \frac{\frac{6}{\pi^2}-\frac{1}{a}}{1-\frac{1}{a}}$ For instance $a=2$ gives that $M_2 \geq  0.11773 $ Its better than the trivial bound but still not quite good, is there a more cleaver way to find $M_a$ ? For all $a> 1$ since for $0< a\leq 1$ we have that $M_a=0$","['analytic-number-theory', 'number-theory', 'prime-numbers']"
3625254,Let $x^2=y^2=1$ and $xy\neq yx$. There are $\binom{2n}{n}$ expressions of length $2n$ in $x$ and $y$ that are equal to $1$.,"This question is motivated by this link .  The statement is as follows.  ( Edit: Even if there are already two great answers, I would love to have a couple more answers.  Especially, I would like to see another, hopefully more combinatorial, proof of the bonus question below.) Question. Let $x$ and $y$ be noncommuting variables such that $x^2=y^2=1$ .  Multiplication is associative.  Prove that, for each positive integer $n$ , there are exactly $\displaystyle\binom{2n}{n}$ expressions of length $2n$ in $x$ and $y$ that are equal to $1$ .  For example, when $n=1$ , there are $2$ such expressions: $xx$ and $yy$ .  When $n=2$ , there are $6$ such expressions: $xxxx$ , $xxyy$ , $xyyx$ , $yxxy$ , $yyxx$ , and $yyyy$ . For a technical clarification, consider the free product $G:=C_2*C_2$ , where $C_2$ is the cyclic group of order $2$ .  Then, $G$ has the following presentation: $G=\langle x,y\,|\,x^2=y^2=1\rangle$ .  We want to find the number of strings of length $2n$ formed by $x$ and $y$ that can be reduced to $1$ . I would like to see how to prove this statement using a combinatorial argument, such as constructing a bijection, finding a generating function, etc.  However, any proof that is different from the proofs below is welcome.  (If you can visit the referred link and give it a combinatorial proof, that would be most appreciated.) Bonus. Let $s$ be a reduced word in $x$ and $y$ (that is, it can no longer be reduced using the rules $x^2=y^2=1$ ).  If $s$ has length $k$ , then show that, for any integer $n\geq 0$ , there are exactly $\displaystyle\binom{n+2k}{n}$ words in $x$ and $y$ of length $k+2n$ that can be reduced to $s$ . Elementary Proof We work in $R:=\mathbb{Z}[x,y]$ .  Note that $$x+y=x+xxy=x(1+xy)$$ and $$(x+y)^2=\big(x(1+xy)\big)^2=x(1+xy)\,x(1+xy)\,.$$ Because $$(1+xy)x=x+xyx=x(1+yx)\,,$$ we have $$(x+y)^2=xx(1+yx)(1+xy)=yx(1+xy)^2=(xy)^{-1}(1+xy)^2\,.$$ Therefore, $$(x+y)^{2n}=\Big((xy)^{-1}(1+xy)^2\Big)^n=(xy)^{-n}(1+xy)^{2n}\,.$$ Thus, there are $\displaystyle\binom{2n}{n}$ expressions of length $2n$ that are equal to $1$ . Algebraic Proof Here is another approach borrowing the idea from Julian Rosen .  Let $R$ denote the unital $\mathbb{Z}$ -algebra generated by $x$ and $y$ (i.e. $R=\mathbb{Z}[G]$ ).  Then, the $\mathbb{Z}$ -algebra homomorphism $$\varphi:R\to\text{Mat}_{2\times2}\big(\mathbb{Z}[t,t^{-1}]\big)$$ defined by sending $$x\mapsto\begin{bmatrix}0&1\\1&0\end{bmatrix}\text{ and }y\mapsto\begin{bmatrix}0&t^{-1}\\t&0\end{bmatrix}$$ is injective.  We can easily see that $$\varphi\big((x+y)^2\big)=t^{-1}(1+t)^2\,I\,,$$ where $I$ is the $2$ -by- $2$ identity matrix.  Hence, $$\varphi\big((x+y)^{2n}\big)=t^{-n}(1+t)^{2n}\,I\,,$$ and the assertion follows immediately. Geometric Proof Using the notations from the geometric proof in my answer here , recall that $x=\sigma_\alpha$ and $y=\sigma_\beta$ .  Thus, $$(x+y)^2=2+\sigma_\alpha\sigma_\beta+\sigma_\beta\sigma_\alpha=2+\rho_{2\alpha-2\beta}+\rho_{2\beta-2\alpha}\,.$$ (It can also be proven that $$\sigma_{\theta_1}+\sigma_{\theta_2}=2\,\cos(\theta_1-\theta_2)\,\sigma_{\frac{\theta_1+\theta_2}{2}}$$ for all $\theta_1,\theta_2\in\mathbb{R}$ .)  Note that $$\rho_{+\theta}+\rho_{-\theta}=2\,\cos(\theta)$$ for all $\theta\in\mathbb{R}$ .  Hence, $$(x+y)^2=2\,\big(1+\cos(2\alpha-2\beta)\big)=2^2\,\big(\cos(\alpha-\beta)\big)^2\,.$$ Then, the number of expressions of length $2n$ that are equal to $1$ is given by $$\begin{align}\frac{1}{(2\pi)^2}\,\int_0^{2\pi}\,\int_0^{2\pi}\,(x+y)^{2n}\,\text{d}\beta\,\text{d}\alpha&=\frac{1}{(2\pi)^2}\,\int_0^{2\pi}\,\int_0^{2\pi}\,2^{2n}\,\big(\cos(\alpha-\beta)\big)^{2n}\,\text{d}\beta\,\text{d}\alpha
\\&=\frac{1}{(2\pi)^2}\,2^{2n}\,\frac{\pi}{2^{2n-1}}\,\binom{2n}{n}\,(2\pi)=\binom{2n}{n}\,.
\end{align}$$","['combinatorial-proofs', 'group-rings', 'combinatorics', 'free-product', 'noncommutative-algebra']"
3625273,Does this prove that the limit does not exist?,"$$\lim_{(x,y)\to(0,0)}\frac{x}{x^{2}-y^{2}}$$ I tried with $y=mx$ and lateral limits. I got that: $$\lim_{x\to 0^{+}}\frac{1}{x(1-m^{2})}=+\infty$$ and $$\lim_{x\to 0^{-}}\frac{1}{x(1-m^{2})}=-\infty$$ Assumming $1-m^{2}>0$ . So the limit does not exist. It's correct?","['limits', 'multivariable-calculus']"
3625285,Evaluation of $\int^{1}_{0}\frac{x^4}{1+x^8}dx$,How can I Integrate $\displaystyle \int^{1}_{0}\frac{x^4}{1+x^8}dx$ ? I have searched in that forum and get the result for $\displaystyle \int^{\infty}_{-\infty}\frac{x^4}{1+x^8}dx$ or for $\displaystyle \int^{\infty}_{0}\frac{x^4}{1+x^8}dx$ Although i know the formula $\displaystyle \int^{\infty}_{0}\frac{x^{m-1}}{1+x^n}dx=\frac{\pi}{n}\cdot \csc\frac{\pi m}{n}$ But did not understand How can i use in my original Integration Help me please Thanks,"['integration', 'definite-integrals', 'analysis', 'real-analysis', 'calculus']"
3625303,Calculating the determinant of the Hessian of a function,"Suppose one is given a function \begin{equation}
f(x_1,\dots,x_n) = g\bigg(x_1,\bigg(\sum_{i=2}^n x_i^2\bigg)^{1/2}\bigg),
\end{equation} and denote \begin{equation}
t:=x_1 \quad \text{and}\quad r:= \bigg(\sum_{i=2}^n x_i^2\bigg)^{1/2}.
\end{equation} I am told that the determinant of the Hessian of $f$ is given by \begin{equation}
\det D^2f = (g_{tt}g_{rr}-g_{tr}^2)\bigg(\frac{g_r}{r}\bigg)^{n-2},
\end{equation} and it seems there must be an easy way to see this, but I cannot work it out. I have tried to derive this by computing the Hessian: the first partial derivatives are given by \begin{align}
\frac{\partial f}{\partial x_i} & = \frac{\partial g}{\partial t}\frac{\partial t}{\partial x_i} + \frac{\partial g}{\partial r}\frac{\partial r}{\partial x_i} = \begin{cases}g_t & \text{if } i=1 \\
g_r\frac{x_i}{r} & \text{if } i\not=1
\end{cases}
\end{align} and then the second partial derivatives are given by \begin{equation}
\frac{\partial^2 f}{\partial x_i\partial x_j} = \begin{cases}g_{tt} & \text{if } i=j=1 \\
g_{tr}\frac{x_j}{r} & \text{if }i=1, j\not=1 \\
g_{tr}\frac{x_i}{r} & \text{if }i\not=1, j=1 \\
g_{rr}\frac{x_i x_j}{r^2} + g_r\frac{\delta_{ij}}{r} - g_r\frac{x_ix_j}{r^2} & \text{if }i\not=1, j\not=1. 
\end{cases}
\end{equation} I was hoping the Hessian would be of a nice form (block diagonal or something) so I could compute the determinant easily, but this doesn't seem to be the case, unless I've calculated something wrong. Any help would be much appreciated! Thanks","['partial-derivative', 'multivariable-calculus', 'vector-analysis']"
3625334,Motivation for Kahler Geometry,"I have been studying Symplectic Geometry. Previously I studied Riemannian Geometry. In Symplectic Geometry I learned the existence of an almost complex structure and how some special almost complex manifolds are integrable thereby becoming a complex manifold. Now I am seeing that we want to focus on a manifold which has all 3 of these structures (symplectic,riemannian and complex) and in some sense are compatible. My question is : All we did was define special types of 2 forms on a manifold and then started looking at compatibility between them. So there should be no reason to give these forms extra privilege, I could  just as well define some weird structure like maybe a closed k-form which satisfies blah-blah properties and that would lead to a new geometry etc. How does a Mathematician know which structures are ""important"" and worth our attention?","['definition', 'symplectic-geometry', 'kahler-manifolds', 'differential-geometry']"
3625389,"""Minimal"" type rank of finitely generated abelian groups with torsion","I've been having trouble finding resources because searching up ""rank"" for abelian groups always brings up the free abelian rank, which is not what I'm looking for. However I'm interested in the more general concept of group rank. Define $${\rm rank}(G)=\min\{ \#S | S \text{ generates }G\}$$ I'd like to know if there's a reference/theorem regarding the rank (in the sense above) of finitely generated abelian groups with torsion elements. Is it true that the rank of such a group is the sum of the free rank and the rank of the torsion group? How subgroups of finitely generated abelian groups are characterized? In particular, what is the rank of $\mathbb{Z}^n \oplus  \mathbb{Z}/m\mathbb{Z}$ and what are its subgroups? For torsion free and finitely generated abelian groups, say, $A=\mathbb{Z}^n$ , there are many references that show that its subgroups are isomorphic to $\mathbb{Z}^r$ for $r\leq n$ . How much can be generalized in those claims to case with torsion? I'm sure this is standard material, and my only issue is not finding when looking this up, as other concepts appear instead. Any references appreciated.","['finitely-generated', 'group-theory', 'abelian-groups', 'reference-request']"
3625397,Limits in two variable with two parameters - Is there any strategy?,"I have this exercise: ""Find for which positive, real $a$ and $b$ , each of these limits exist"". $$
\lim_{(x,y) \to (0,0)}\frac{|x|^a\cdot|y|^b}{x^{88}+y^{12}}\qquad\lim_{x^2+y^2 \to +\infty}\frac{|x|^a\cdot|y|^b}{x^{88}+y^{12}}
$$ In ""1-D"" I used to eliminate one of the parameters and solve for the other one, or I was on search of indecision form to apply De l'Hospital… How can I move in ""2-D"" case? Is there any ""common"" strategy to follow?","['analysis', 'real-analysis', 'multivariable-calculus', 'calculus', 'limits']"
3625473,Weak convergence of a sequence of gaussian random vector,"Let $(X_n)_n$ be a gaussian random vector taking values in $\mathbb{R}^d,$ let $K_{X_n}$ denote the covariance matrix of $X_n.$ Show that if $(X_n)_n$ converges in distribution to $X,$ then $(K_{X_n})_n$ and $(E[X_n])_n$ converge. Find the distribution of $X.$ So in terms of characteristic function we have $$\forall x \in \mathbb{R}^d, \quad\lim_n\varphi_{X_n}(x)=e^{i \ ^tx\operatorname E[X_n]-\frac{1}{2}\,^txK_{X_n}x}=\varphi_X(x),$$ how can we use it in order to prove the convergence of $(K_{X_n})_n$ and $(\operatorname E[X_n])_n.$","['probability-limit-theorems', 'weak-convergence', 'probability-distributions', 'normal-distribution', 'probability-theory']"
3625523,Triple Summation of product of Binomial Coefficients $\sum_{k=0}^n\sum_{j=0}^n\sum_{i=0}^n\binom ni\binom{n-i}j\binom{n-j}k=5^n$?,Prove that: $$\sum_{k=0}^n\sum_{j=0}^n\sum_{i=0}^n\binom ni\binom{n-i}j\binom{n-j}k=5^n$$ I have tried expanding the binomial coefficients as fractions of factorials. $\sum\sum\binom ni\binom{n-i}j$ gives the multinomial expansion of $3^n$ but I cannot handle the remaining term of $\binom{n-j}k$ . I could not proceed further. Btw I have checked the validity of the question by programming it.,"['summation', 'binomial-coefficients', 'combinatorics']"
3625627,Finding the Sum of series $S = \sum_{n=1}^{243} \frac{1}{n^{4/5}} $.,"If $S = \sum_{n=1}^{243} \frac{1}{n^{4/5}} $ . Find the value of $\lfloor S \rfloor$ where $\lfloor \cdot \rfloor$ represents the greatest integer function. By approximation using definite integral, I know the answer lies in the set $[10,15]$ (approximately) but I don't know how to find the exact sum.","['summation-method', 'divergent-series', 'summation', 'sequences-and-series']"
3625636,Prove that $W$ is $T$-invariant if and only if $W^0$ is $T^t$-invariant.,"Let $T$ be a linear operator on a finite-dimensional vector space $V$ , and let $W$ be a subspace of $V$ . Let $W^0 \subset V^*$ be the annihilator of $W$ . Prove that $W$ is $T$ -invariant if and only if $W^0$ is $T^t$ -invariant. A hint is appreciated for the converse direction.","['linear-algebra', 'dual-spaces']"
3625725,What does measure zero mean in probability?,"If I have a continuous distribution of random variable $X$ then $P(X = k) = 0$ where $k$ is some specific value. I understand that this doesn't imply impossibility, as in the real world, no variable is completely continuous. We have limited precision for measuring physical phenomena, so while impossible from the math, it's physically possible. But setting aside the ""real-world"" argument, if we're talking about probability theory in the context of measure theory, what does $P(A) = 0$ mean? If $A$ isn't impossible, what does $0$ mean, if anything? This post suggests that the $0$ assigned by the probability measure is a result of a limit, that $A$ is so unlikely that when performing a large number of trials, the frequency approaches $0$ . So then why not distinguish between $P(A) = 0$ and $P(A)$ approaches $0$ (like an infinitesimal) where the former represents $A$ being impossible, that is, $A = \emptyset$ and the latter meaning an event $A$ is possible, just extremely unlikely? Why does the probability measure assign $0$ to both?","['measure-theory', 'soft-question', 'probability-theory']"
3625866,"Let $B$ have distribution $\text{Binomial}(n,p)$. Then what is $ \lim_{n \to \infty} \mathsf P\left[ \frac{B}{n} < p\right]$","Let $B$ have distribution $\text{Binomial}(n,p)$ . What is the tool for analysing $$ \lim_{n \to \infty} \mathsf P\left[ \frac{B}{n} < p\right]$$ ? I think the answer should be $ \lim_{n \to \infty} \mathsf P\left[ \frac{B}{n} < p\right]=\frac{1}{2}$ . My reasoning is that $np$ is roughly the median of $B$ hence the limit should be $1/2$ . 
However, I would like to see a proper proof.","['probability-theory', 'probability']"
3625872,Show that $\{x\}^A \approx A$,"I need to prove that $\{x\}^A \approx A$ , that is, that the set of all functions from a set with a single element (namely, x) to another set A is equipotent to the set A. Now the reasoning behind why this is true is clear: say A had only one element $y_0$ , then the only function from $\{x\}$ to A is the function $f(x) = y_0$ . Any other function would either not be a function (because $x$ would equal two different $f(x)$ ) or not belong in $\{x\}^A$ (because it wouldn't land in A).
 If A has two elements $y_0$ and $y_1$ , then the only distinct functions from $\{x\}$ to A are $f(x) = y_0$ and $f(x) = y_1$ . Clearly both sets always have the same cardinality, as the constant functions are always in $\{x\}^A$ . My problem is I don't know how to write this as a formal proof. Previously I proved two sets were equipotent by finding a bijective function between them. But since in this case one of the sets is a set of functions, I don't know how to define a function whose parameter is another arbitrary function. I also thought to do it by induction (suppose cardinality of A to be n and prove it works for n+1) but I don't feel its the right way. I also thought to do it by transitivity of equipotence, but since A can be any set, I'm not sure how to formulate this. Finally, I thought about contradiction, supposing them not to be equipotent, but I don't see what the contradiction will be. Any hint in the direction to start the proof would be appreciated. Thanks!","['elementary-set-theory', 'cardinals', 'proof-writing']"
3625907,Prove or disprove: $\lim_{n\to \infty}\left(a_{n+1} - \frac 12 a_n\right)=0 \Rightarrow \lim_{n\to \infty}a_n=0$,"I am stuck on a simple exercise. Let $(a_n)_{_n\in\mathbb N}$ be a sequence of real numbers. Prove or disprove the following statement: $$\lim_{n\to \infty}\left(a_{n+1} - \frac 12 a_n\right)=0 \Rightarrow \lim_{n\to \infty}a_n=0.$$ I tried to prove it directly but didn't make any progress. So I considered proving the contraposition: If you assume $(a_n)_{_{n\in\mathbb N}}$ is a convergent sequence, the contraposition of this statement is easy to prove. Let $\lim_{n\to \infty}a_n=:a\neq0$ , then $\lim_{n\to \infty}(a_{n+1} - \frac 12 a_n)=a-\frac a2= \frac a2 \neq 0$ , and therefore $\lim_{n\to \infty}(a_{n+1} - \frac 12 a_n)=a-\frac a2= \frac a2 \neq 0$ . But to correctly prove the contraposition, I also have to consider divergent series. Does $$(a_n)_{_{n\in\mathbb N}}\;divergent \Rightarrow \left( a_{n+1} - \frac 12 a_n \right)_{_{n\in\mathbb N}}\;divergent\; \lor\lim_{n\to \infty}\left(a_{n+1} - \frac 12 a_n\right)\neq0$$ hold? If yes, it would prove the mentioned statement above.","['sequences-and-series', 'real-analysis']"
3625911,Intersection of the zero sets of two multivariate polynomials,"Suppose now I have two multivariate polynomials, say $p_1: \mathbb{R}^n\to\mathbb{R}$ and $p_2: \mathbb{R}^n\to\mathbb{R}$ , $n\ge 2$ . Suppose they are both irreducible, and $A$ , $B$ are the zero sets of $p_1$ and $p_2$ respectively. Suppose "" $A$ and $B$ has a common part"". This description is not mathematically rigorous, but I cannot find a better way to describe it. So let me give some examples to explain it. For example, in 2D, $A$ and $B$ represent curves. Then "" $A$ and $B$ has a common part"" means that they share a common sub-arc. In 3D, then they share a common subsurface and so on. I am wondering under this assumption, can we say that $A$ must be equal to $B$ ? Or can we say that $p_1$ and $p_2$ are the same up to a scale? For $n=2$ . I think the answer is positive. If we have two curves which are given by the zero sets of the two polynomials, and the two curves share a sub-arc. Then the two irreducible polynomials must be the same up to a scale by Bezout's theorem. But I do not know what will happen if $n\ge 3$ . I guess we will still have the conclusion but I do not know how to show it.","['irreducible-polynomials', 'roots', 'algebraic-geometry', 'polynomials', 'differential-geometry']"
3625913,Showing that $\int_{-\infty}^{\infty}\frac{x^2}{(x^2+a^2)(x^2+b^2)}dx=\frac{\pi}{a+b}$ via Fourier Transform,"If $a,b>0$ , how can I prove this using Fourier Series $$\int_{-\infty}^{\infty}\frac{x^2}{(x^2+a^2)(x^2+b^2)}dx=\frac{\pi}{a+b}.$$ I tried to split the product and calculate the integral using Parceval's Theorem, but $\frac{x}{(x^2+a^2)}$ and $\frac{x^2}{(x^2+a^2)}$ aren't in $L^1(\mathbb{R})$ . Any hints hold be appreciated.","['integration', 'fourier-series', 'fourier-analysis', 'fourier-transform']"
3625936,On the qualitative behaviour of functions of the form $f’(x) = f(x)$ as x decreases,"Pretend that you don’t know about the function $ e^x$ (so pretend to erase it and everything you know about it from your memory, for argument’s sake). You want to draw graphs of functions that satisfy $f’(x) = f(x)$ . Not precisely, but you want to know it’s qualitative properties. $f(x) = 0 \forall x$ is a trivial solution, so let’s put this to one side. Now you also quickly realise that if $f(x) > 0$ for some $x,\ $ then the following qualitative behaviour of the function $f(x)$ must be true: both $f(x)$ and $f’(x)$ must be strictly positive and increasing as $x {\to} \infty$ . You realised this by drawing a small straight line of gradient 1 when $f(x) = 1$ and then $f’(x) = 1.1$ just to the right, when $f(x) = 1.1$ etc etc. Are there any arguments- graphical, using calculus and continuity and any other tools you might have, to determine whether or not $f(c)=0 $ for some $ c \in \mathbb{R}$ as $x$ decreases? If such a $c$ does exist, then it follows that $f(x) = 0 \forall x<c$ also, since: if $f(a) < 0$ for some $a<c$ then $\exists b $ with $a<b<c$ such that $f’(b) > 0$ , so $f’(b) \neq f(b)$ , a contradiction to the original condition. A similar argument can be made to show that $f(d) > 0$ for some $ d<c$ leads to a contradiction. But it’s not clear to me how you can argue that the graph wouldn’t attain f(x) = 0 for some x. I don’t see why our graph can’t look like for example, the piecewise curve $y=0$ for $x < 0$ and $y= x^4$ for $x \geq 0$ . Can an argument be made for why this isn’t possible this in a similar vein to one of my arguments/lines of reasoning above?","['geometry', 'analysis', 'real-analysis', 'calculus', 'algebra-precalculus']"
3625943,Can Hilbert spaces be defined over fields other than $\mathbb R$ and $\mathbb C$?,"Let $V$ be a vector space over a field $K$ . Suppose further that $K$ has the following structures: $K$ has a subfield $K_{\mathbb R}$ equipped with a field embedding $K_{\mathbb R}\hookrightarrow\mathbb R$ , so we can identity elements of $K_{\mathbb R}$ with elements of $\mathbb R$ . $K$ has an involution $*:K\to K,z\mapsto z^*$ , meaning $*$ is an automorphism and that $(z^*)^*=z$ for all $z\in K$ . Define a $(K,K_{\mathbb R},*,\hookrightarrow)$ -inner product space $\big(V,(K,K_{\mathbb R},*,\hookrightarrow),\langle\cdot,\cdot\rangle\big)$ as a $K$ -vector space (where $*,K_{\mathbb R},$ and $\hookrightarrow$ are the structures described above) together with a map $\langle\cdot,\cdot\rangle:V\times V\to K$ that satisfies the following properties: $\langle x,y\rangle=\langle y,x\rangle^*$ for all $x,y\in V$ . $\langle x,\cdot\rangle:V\to K$ is linear for all fixed $x\in V$ . $\forall x\in V\setminus\{0\}:\langle x,x\rangle>0$ . Condition (3) should be interpreted as saying $\langle x,x\rangle\in K_{\mathbb R}$ and the embedding (discussed above) identifies $\langle x,x\rangle$ with a positive real number. The idea of this definition is to allow for inner products to be defined in the most general setting possible and still agree with the usual definitions for $\mathbb R$ and $\mathbb C$ . Define a $(K,K_{\mathbb R},*,\hookrightarrow)$ -Hilbert space as a $(K,K_{\mathbb R},*,\hookrightarrow)$ -inner product space in which the induced norm $\lVert x\rVert =\sqrt{\langle x,x\rangle}$ makes $V$ into a complete metric space. Do there exist nontrivial $^\dagger$ $(K,K_{\mathbb R},*,\hookrightarrow)$ -Hilbert spaces for any fields $K\neq\mathbb R$ or $\mathbb C$ ? The "" $\neq$ "" should be read as ""not isomorphic to."" If yes, that would suggest interesting possible extensions of the usual definition of Hilbert space; if no, that would provide a justification for only ever defining or considering Hilbert spaces over $\mathbb R$ and $\mathbb C$ . $^\dagger$ By non-trivial, I mean $V\neq\{0\}$ , the single-element vector space.","['complete-spaces', 'inner-products', 'field-theory', 'hilbert-spaces', 'linear-algebra']"
3625986,Is there a coordinate-free definition of a differential operator?,"A differential operator is normally defined to be a sum of partial derivatives with respect to a given basis, i.e. something that looks like $\sum_{\vert {\alpha} \vert \leq n} a_{\alpha}(x) \cdot \partial^{\alpha}$ . Differential operators of interest tend to be naturally seen in a coordinate-free way -- for example, the laplacian is the divergence of the gradient. Is there a coordiante-free way to define a general differential operator?","['operator-theory', 'functional-analysis', 'analysis', 'partial-differential-equations']"
3626026,"Why is surjectivity ""harder"" than injectivity?","Injectivity and surjectivity are very intimately related, however, in any particular structure, one of them tends to be a much ""harder"" property - just looking at basic set theory, we have that a function is injective iff it has a left inverse, and surjective iff it has a right inverse. But immediately something crops up: the former is a harmless statement provable in ZF, but the latter is equivalent to the axiom of choice. Going on to basic algebra, we see that kernels  are totally related to the injectiveness of a function, and studying them tend to be much easier, while cokernels tend to bring in no new information (and at least, at the lower level, seem to be just a fancy but useless substitute for studying  surjectivity). We also see that theorems about surjectivity tend to be more important (off the top of my head, the isomorphism extension theorem in field theory). Surjectivity questions tend to have to be answered constructively, which in my experience, is generally hard. That is not to say that the opposite isn't true - from memory, projective resolutions were significantly easier than injective ones. My question is: is there a ""deep"" reason as to why one of the two tend to be much ""harder"" than the other?","['elementary-set-theory', 'abstract-algebra', 'soft-question', 'category-theory']"
3626034,Elementary (CFSG-avoiding) proof that the order of a group of composite order is bounded by the square of the order of its largest proper subgroup?,"It seems that the following is true.  For a finite group $G,$ define $u(G)$ to be the largest order of a proper subgroup of $G$ .  Then $\left|G\right|\leq u(G)^2$ provided that the order of $G$ is composite. However, my argument relies on the classification of finite simple groups in that I need to know that $\left|G\right| < u(G)^2$ , when $G$ is non-abelian simple, and I did this by checking the maximal subgroups of groups on the list . (The simple case is actually almost all of the work.)  This feels a bit like driving a thumb tack with a sledgehammer, especially since the remainder of the argument is completely elementary. Given how neatly the bound worked out (and assuming that this is indeed correct), it seems like the sort of thing that must have been known for a very long time, and probably has a fairly simple proof that I've been unable to discover for myself. Question. Can this be proved by more elementary means (that is, without resorting to the classification of finite simple groups)?","['group-theory', 'simple-groups']"
3626040,Introductory material for jets and jet bundles,"A student of mine would like to learn more about jets and jet bundles, and more in general about how to treat derivatives and differential equations in an invariant way.
She's also interested in the reformulation of some of the basic concepts of differential geometry from that point of view. Are there any books, papers, or notes that follow such an approach, and which are suitable for a first approach to the subject (there is motivation, etc)? She knows more than basic differential geometry, just not jet bundles yet. By the way, she studies physics, so if there are examples taken from physics, even better.","['smooth-manifolds', 'reference-request', 'jet-bundles', 'differential-topology', 'differential-geometry']"
3626046,Countable sum of orthogonal projections with orthogonal ranges; uniformly bounded monotone sequence of self-adjoint operators is strongly convergent,"Let $\mathcal{H}$ a Hilbert space. Let $P_j$ a sequence of orthogonal projections such that their ranges are mutually orthogonal. Then $\sum_1^\infty P_j \to P$ in the strong operator topology where $P$ is the orthogonal projection onto the closed span of the ranges of the $P_j$ . A version of this is used in B.C. Hall's Quantum Theory for Mathematicians and he claims that ""it follows from an elementary argument"". I can't figure that argument out, I can't even convince myself that $\sum_j P_j\psi$ converges in the norm topology for all $\psi \in \mathcal{H}$ . In your answer, you can suppose that I know that the analogous result holds for the finite sum case. Edit: JustDroppedIn provided enough info for me to understand the problem and develop an answer. The post they linked to gave the outline of a proof of a theorem necessary to proving the theorem I am trying to get in my question. Because this effectively answered my question, I accepted JustDroppedIn's answer. I couldn't find a uniform presentation of all the necessary results though (and I was unfamiliar with the linked theorem) so I wrote out proofs of every result, which I added in a separate answer, in case anyone might find this useful. Apologies for the bad formatting on my answer, I only know how to format things nicely on Latex.","['hilbert-spaces', 'functional-analysis']"
3626052,Likelihood Ratio Martingales,"I am reading about so-called ""likelihood ratio martingales"" in this handout. The example given is as follows. Let $(X_n : n \ge 1)$ be a sequence of iid random variables (say, on a probability space $(\Omega, \mathcal{F}, P)$ ) with common density $g$ . Suppose that $f$ is another density with the property that whenever $g(x) = 0$ , then $f(x) = 0$ . Set $L_0 = 1$ and $$ L_n = \prod_{i=1}^{n} \frac{f(X_i)}{g(X_i)}, \quad n \ge 1. $$ Then (the claim is that) $L = (L_n: n \ge 0)$ is a martingale w.r.t. the filtration $\{\mathcal{F}_n\}$ , where $\mathcal{F}_n = \sigma(X_1, ..., X_n)$ . As with many of these claims/proofs regarding martingales, it is always stated that the first two properties (i.e., (i) $L$ is $\{\mathcal{F}_n\}$ -adapted; and (ii) $E[|L_n|] < \infty$ , for all $n$ ) are either ""immediate,"" or ""trivial,"" or ""obvious,"" or what-have-you; and this example is no exception. However, I don't see why this is the case! Let's give it a shot... (i) $L$ is $\{\mathcal{F}_n\}$ -adapted means that each $L_n$ is $\mathcal{F}_n$ -measurable. So, since $\mathcal{F}_n = \sigma(X_1, ..., X_n)$ , we need to check that, for each Borel set $B$ , $L_n^{-1}(B) \in \sigma(X_1, ..., X_n)$ . Ok... so, let $B$ be a Borel set. Then $$L_n^{-1}(B) = \{\omega : L_n(\omega) \in B\} = \left\{\omega : \frac{f(X_1(\omega))f(X_2(\omega)) \cdots f(X_n(\omega))}{g(X_1(\omega))g(X_2(\omega)) \cdots g(X_n(\omega))} \in B\right\}. $$ Why is it obvious that this set belongs to $\mathcal{F}_n$ ?","['martingales', 'measure-theory', 'probability-theory']"
3626140,Show that $ (1-\epsilon)^q \lambda(E) \leq \lambda(E_\epsilon) $,"The question is the following: Suppose $f$ is a real-valued Lebesgue measurable function on a set $E\subset \mathbb{R}$ with finite measure. Given $1 > \varepsilon > 0$ . Let $E_\varepsilon = \{x:|f(x)|\geqslant \varepsilon\}$ . Suppose $$
\frac{1}{\lambda(E)}\int_E |f(x)|\ d\lambda \geqslant  1 \quad \text{and} \quad \frac{1}{\lambda(E)}\int_E |f(x)|^p\ d\lambda \leqslant  1
$$ for some $1<p<\infty$ . Show that $$
(1-\varepsilon)^q \lambda(E) \leqslant \lambda(E_\varepsilon)
$$ where $1/p+1/q = 1$ . I tried to compute $$
\int_{E \setminus E_\varepsilon} |f| \ d\lambda  = \int_{{x \in E:|f(x)| \leqslant \varepsilon}} |f| \leqslant \int_{{x \in E:|f(x)| \leq \varepsilon}} \varepsilon \ d\lambda = \epsilon \cdot \lambda(E \setminus E_\varepsilon)\leqslant\varepsilon \cdot(\lambda(E )-\lambda(E_\varepsilon))
$$ Therefore, we have \begin{align*}
    \int_{E_\varepsilon} |f| = \int_E |f| - \int_{E \setminus E_\varepsilon}|f| \geqslant \lambda(E) -  \varepsilon \cdot( \lambda(E )-\lambda( E_\varepsilon)) \geqslant (1-\varepsilon) \lambda(E) + \lambda(E_\varepsilon)
\end{align*} From Holder's inequality, $$
\int_{E_\varepsilon}|f| \leqslant \left(\int_{E_\varepsilon}|f|^p\right)^{1/p} \cdot \left(\int_{E_\varepsilon}|1|^q\right)^{1/q}  = \left(\int_{E_\varepsilon}|f|^p\right)^{1/p}\cdot(\lambda(E_\varepsilon))^{1/q}
$$ then we have $$
\left(\int_{E_\varepsilon}|f|^p\right)^{1/p}\cdot(\lambda(E_\varepsilon))^{1/q}\geqslant (1-\varepsilon) \lambda(E) + \lambda(E_\varepsilon)
$$ which does not imply anything. I know that I have to apply the Holder's Inequality some how from the relationship between $p$ and $q$ , but I can't proceed anywhere else. I have no idea how to use the second inequality as it always gives me something from the other direction. Any help and hint are appreciated!! Edit: I have already figured it out, and I was actually very close to the solution. Here attached the rest of my approach. \begin{align*}
    (1-\epsilon) \lambda(E) + \lambda(E_\epsilon)& \leq \left(\int_{E_\epsilon}|f|^p\right)^{1/p} \cdot(\lambda(E_\epsilon))^{1/q}\\
    &\leq \left(\int_{E}|f|^p\right)^{1/p} \cdot(\lambda(E_\epsilon))^{1/q}\\
    &\leq (\lambda(E))^{1/p} \cdot(\lambda(E_\epsilon))^{1/q}
\end{align*} and now it suffices to show that the above equation is equivalent to the conclusion $$
(1-\epsilon)^q\lambda(E)\leq \lambda(E_\epsilon)
$$ Divide both side by $(\lambda(E))^{1/p}$ and from the fact that $1-1/p = 1/q$ \begin{align*}
    (1-\epsilon) [\lambda(E)]^{1/q} + \lambda(E_\epsilon)(\lambda(E))^{-1/p}&\leq (\lambda(E_\epsilon))^{1/q} 
\end{align*} Raise both sides to the $q$ -th power, one has \begin{align*}
    \lambda(E_\epsilon) &\geq [(1-\epsilon) [\lambda(E)]^{1/q} + \lambda(E_\epsilon)(\lambda(E))^{-1/p}]^q \\
    &\geq [(1-\epsilon) [\lambda(E)]^{1/q}]^q \\
    &\geq (1-\epsilon)^q \lambda(E)
\end{align*}","['integration', 'measure-theory', 'lebesgue-integral', 'real-analysis', 'lp-spaces']"
3626174,The probability that a continuous random variable equals a certain value is 0: Does that apply to finding even/odd values??,"I understand that the probability for any X to equal a certain value is 0 (X is a continuous random variable with a given probability density function (pdf)). However, does that apply to finding the probability that X is even, or that X is odd? For example, the way I see it, it would be like P(X is odd) = P(X = 1 or 3 etc. from the interval a to b). Which means that the probability here is 0 by the rule above. Am I looking at this wrong? Should I be looking at it as the total number of odd x's by using 'and', not 'or', however that still would not be a range to find an integral? (x is an integer on the graph). I am working with a uniform distribution. To clarify, I want to find P(X is odd) on my range of values, and X is a continuous random variable with a pdf.","['probability-distributions', 'probability', 'random-variables']"
3626187,Intersections of Six Circles: Concurrence and Concyclicity,"After answering this question , I played around with the figure a little bit.  I found the following interesting results, but have been unable to prove them. Let $ABC$ be a triangle with circumscribed circle $\Gamma$ and incenter $I$ .  The straight lines $AI$ , $BI$ , and $CI$ meet $\Gamma$ again at $A_1$ , $B_1$ , and $C_1$ , respectively.  Let $x_1$ , $y_1$ , and $z_1$ denote the circumcircles of the triangles $AIB_1$ , $BIC_1$ , and $CIA_1$ , respectively, while $x_2$ , $y_2$ , and $z_2$ are the circumcircles of the triangles $AIC_1$ , $BIA_1$ , and $CIB_1$ , respectively.  Let $i\in\{1,2\}$ .  The circles $y_i$ and $z_i$ intersect again at $P_i$ ; the circles $z_i$ and $x_i$ intersect again at $Q_i$ ; and the circles $x_i$ and $y_i$ intersect again at $R_i$ .  Moreover, $y_1$ meets $z_2$ again at $P_3$ , $z_1$ meets $x_2$ again at $Q_3$ , and $x_1$ meets $y_2$ again at $R_3$ . (a) Show that the straight lines $Q_1R_2$ , $R_1P_2$ , and $P_1Q_2$ intersect at $I$ . (b) Prove that the circumcircle of the triangle $P_3Q_3R_3$ passes through $I$ . I think inversion about the incircle of the triangle $ABC$ may be a good approach, after all the images of $x_1$ , $x_2$ , $y_1$ , $y_2$ , $z_1$ , and $z_2$ under this inversion become straight lines.  Then, maybe, there are some theorems in projective geometry that can deal with the rest.  However, so far, I have not discovered the relations between these six lines to produce any proof yet. Below is some not-difficult-to-obtain information that may or may not help. The lines $B_1C_1$ , $C_1A_1$ , and $A_1B_1$ are the perpendicular bisectors of $AI$ , $BI$ , and $CI$ , respectively. Denote by $A_2$ and $A_3$ the second intersections of $AB$ with $y_1$ and $AC$ with $z_2$ , respectively; denote by $B_2$ and $B_3$ the second intersections of $BC$ with $z_1$ and $BA$ with $x_2$ , respectively; denote by $C_2$ and $C_3$ the second intersections of $CA$ with $x_1$ and $CB$ with $y_2$ , respectively.  Then, $A_2,A_3\in B_1C_1$ , $B_2B_3\in C_1A_1$ , and $C_2,C_3\in A_1B_1$ . The line $B_3C_2$ passes through $I$ and is parallel to $BC$ .  The line $C_3A_2$ passes through $I$ and is parallel to $CA$ .  The line $A_3B_2$ passes through $I$ and is parallel to $AB$ . The quadrilaterals $AA_2IA_3$ , $BB_2IB_3$ , and $CC_2IC_3$ are rhombi.","['euclidean-geometry', 'circles', 'geometry', 'triangles', 'plane-geometry']"
3626262,"Derive Likelihood Ratio test for testing $H_0: \mu=\sigma^2$, $H_1: \mu \not= \sigma^2$.","Let $X_1, . . . , X_n$ be independent $N(µ, σ^2)$ random variables. Derive the LRT for testing $H_0: \mu=\sigma^2$ , $H_1: \mu \not= \sigma^2$ . As per usual I found MLE of $\theta \in \Theta$ and $\theta \in \Theta_0$ to calculate: $$\Lambda=\frac{\sup_{\theta \in \Theta_0} L(\theta)}{\sup_{\theta \in \Theta}L(\theta)}=\frac{L(a,a)}{L(\bar{X},S^2)}$$ Where $a=\sqrt{\bar{X^2}+1/4}-1/2$ and $S^2=(1/n)\sum (X_i-\bar{X})^2$ . $L(a,a)=(2\pi a)^{-n/2}exp(\frac{-1}{2a}\sum (X_i-a)^2)$ $L(\bar{X},S^2)=(2\pi S^2)^{-n/2}exp(\frac{-1}{2S^2}\sum (X_i-\bar{X})^2)=(2\pi S^2)^{-n/2}exp(\frac{-1}{2S^2}nS^2)=(2\pi S^2)^{-n/2}exp(\frac{-n}{2})$ So I'm getting $$\Lambda=(\frac{eS^2}{a})^{n/2}\exp\{{\frac{-1}{2a}\sum(X_i-a)^2}\}$$ And here I'm stuck. Do I (can I even) simplify this more or can I just shove this into the LRT and move on? In the examples I'm given; they typically find the distribution of the statistic or find an expression in terms of some known distribution. (1) Did I make a mistake? (2) Can I simplify this further? In other words; how do I finish the problem that isn't just ''The LRT is $\chi[\Lambda \leq  c]$ ''","['statistics', 'solution-verification', 'probability', 'hypothesis-testing']"
3626310,Intuitive explanation of GP-LVM,"I am looking for an intuitive explanation of Gaussian Process Latent Variable Models (GP-LVM). Here is what I understand: GP-LVM is a nonlinear dimensionality-reduction method. I think ""nonlinear"" means that there is no linear relationship between the reduced dimensions that we end up with. PCA is a linear dimensionality-reduction method: We end up with principal components, which are the projection of the original data onto the principal axes. There is a linear relationship between principal components of different principal axes as we can always transform one into the other through scaling and addition. Does this sound right? GP-LVM assumes there are ""latent"" variables in a high-dimensional dataset. These are the ""important"" variables of the dataset as we can obtain all the other data through a nonlinear combination of the latent variables. Neil Lawrence gives an introduction in this presentation. He is trying to make a point with these rotated digits on slide 18. Can anyone explain to me what this is about? I understand that when being given a handwritten digit, i.e. 64 x 57 pixel values that are either 0 or 1, when we take random samples, we will never end up seeing the original digit. ""Sampling"" the image translates into taking random samples and arranging them on a 64 x 57 figure, doesn't it? Now he is rotating the digits a bit and plots principal components. Does this mean he has rotated the digit X times and for each rotated image he plots PC2 and PC3 ? Why doesn't he plot PC1 ? Very confused here. Finally, and this is where an intuitive explanation may reach its limits: How are Gaussian processes involved in predicting the original data from the latent variables? Also: How do we find the latent variables ?","['data-analysis', 'statistics']"
3626313,Proof check about bounded operator,"Let $X$ and $Y$ be Banach spaces, and fix a bounded linear operator $A \in \mathcal{B}(X, Y)$ . Choose $\mu \in Y^{*}$ ,  and define a functional $A^{*} \mu: X \rightarrow \mathbf{F}$ by $\left(A^{*} \mu\right)(x)=\mu(A x)$ ,  for $x \in X$ . I want to show that the mapping $A^*\colon \mu \mapsto A^{*} \mu$ is a bounded linear mapping of $Y^{*}$ into $X^*$ . Linear part is easy and my thought about bounded part is \begin{align}\|A^*\|&=\sup_{\|\mu\|=1}\|A^*\mu\|=\sup_{\|\mu\|=1}(\sup_{\|x\|=1}\|(A^*\mu)(x)\|)\\&=\sup_{\|\mu\|=1}(\sup_{\|x\|=1}\|\mu(Ax)\|)=\sup_{\|\mu\|=1}\|\mu A\|=\|A\|
\end{align} Is that correct?  Also, is there a way to show $\|A\|=\|A^*\|$ ?Any help is appreciated.",['functional-analysis']
3626456,Dimension of fibers of scheme over $\mathbb{Z}$,"Let $X$ be a reduced scheme of finite type over $\mathbb{Z}$ : given a prime $p \in \mathbb{Z}$ ,I'm going to denote as $X_p$ its base change over $\mathbb{F}_p$ . I would be interested in knowing the following:is it true that the dimension of $X_p$ is eventually costant?(i.e there exists an $n \in \mathbb{N}$ such that for every $p \geq n$ , we have $\dim X_p=n$ ). I tried to  reduce to the affine and irreducible  case, so that $X=\operatorname{Spec}(A)$ with $A$ a finitely generated domain. By a stronger version of Noether normalization lemma, there exists an $f \in \mathbb{Z}$ such that there exists a finite injective morphism $\mathbb{Z}_f[x_1,x_2, \dots, x_r] \subseteq A_f$ .Now, we have $\dim X_f=r+1$ and I would like that to imply $\dim X_p = r$ for every $p $ such that $p$ does not divide $f$ . The only thing that might be useful that comes to my mind here is   the standard result about fiber dimension: $$\dim \mathcal{O}_{X_y,x} \geq \dim \mathcal{O}_{X,x}-\dim \mathcal{O}_{Y,y} $$ However, we do not have a real control over $\dim \mathcal{O}_{X,x}$ so I do not know how to conclude. Is there any known statement in the literature? Are there any mild conditions under which this fact is true? I would also be interested in the following fact: suppose that eventually $\dim X_p=0$ so that every fiber is a finite collection of points. I would like to have $X_p(\overline{\mathbb{F}_p})$ to be also eventually constant. I imagine that one could actually prove something like that the number of irreducible components of maximal dimension of $X_p$ should be eventually constant, but I would know how to prove this.","['affine-schemes', 'algebraic-geometry', 'schemes', 'commutative-algebra']"
3626467,Example of an algebra which is not isomorphic to its opposite,"I was trying to solve an exercise (marked with a  star) that asks to come up with an example of an algebra $A$ which is not isomorphic to $A^{\mathrm{op}}$ . I thought at first that I just need to find an algebra containing elements $a, b$ , such that $ab \neq 0, \; ba = 0$ but then I realized that it doesn't give anything so now I don't know what to do. I googled some examples but they aren't natural for me, for instance, one of them was built out of  quiver and I never studied  quivers. Background: most of the Dummit & Foote book.","['ring-theory', 'abstract-algebra', 'noncommutative-algebra']"
3626499,How many isosceles triangles do you need to make any polygon?,"A while back, I had a question regarding constructing shapes with only isosceles triangles. I decided to give it a go again and it has once again stumped me. The question is: How many isosceles triangles would you need to be able to construct any $n$ sided polygon? I thought induction could work, but dealing with non convex polygons may make this difficult. I tried by finding polygons that require what I thought was the maximum amount, but this is unreliable and may not be enough to find a general expression in terms of $n$ . How would one go about solving a problem like this? Is this a well known result? Any help or guidance would be greatly appreciated!",['geometry']
3626518,Hadamard's lemma for arbitrary open subsets,"I am struggling with the following. It regards the variants of Hadamard's lemma for smooth functions. I have found basically two statements. The usual one, which can be found e.g. on Wikipedia ( https://en.wikipedia.org/wiki/Hadamard%27s_lemma ) is the following: Hadamard's Lemma I: Let $U \subseteq \mathbb{R}^{n}$ be a star-convex neighborhood of a point $a$ . Let $f: U \rightarrow \mathbb{R}^{p}$ be a smooth map. Then for any $x \in U$ , one has $f(x) = f(a) + (h^{(a)}(x))(x-a)$ , for some smooth map $h^{(a)}: U \rightarrow Lin(\mathbb{R}^{n},\mathbb{R}^{p})$ . Next, I have found a version for arbitrary open set $U$ , but with a weaker statement (see Lemma 2.2.7 in Duistermaat - Kolk https://books.google.cz/books?id=5KXkkGTnaYIC ) which says the following: Hadamard's Lemma II: Let $U \subseteq \mathbb{R}^{n}$ be any neighborhood of point $a$ . Let $f: U \rightarrow \mathbb{R}^{p}$ be differentiable at $a$ . Then for any $x \in U$ , one has $f(x) = f(a) + (h^{(a)}(x))(x-a)$ , for some map $h^{(a)}: U \rightarrow Lin(\mathbb{R}^{n},\mathbb{R}^{p})$ continuous at $a$ . The actual question: Can the star-convexity of $U$ in assumptions of Lemma I be dropped? I understand how it is used in the proof (integration along line segments connecting $a$ to $x$ ). Are there some counter-examples? It follows from the proof of Lemma II that $h^{(a)}$ is continous on entire $U$ , but I am unable to prove that it is smooth. Thank you, Jan","['multivariable-calculus', 'calculus', 'smooth-functions', 'smooth-manifolds']"
3626592,Combinatorial problem about round table,"Find the number of ways of selecting three people out of $10$ people sitting at a round table, such that no two people selected are consecutive. I recently learned about the stars and bars technique and thought I'd do a slightly changed version of it. I went to convert the question into its binary equivalent (using $0$ s and $1$ s to describe the same situation to make it mathematically more tangible) and thought I'd give stickers with $0$ and $1$ printed on them to those people sitting at the table assuming those who get $0$ are selected such that no two consecutive people get a $0$ . One of the combinations could be this $0, 1, 1, 0, 1, 0, 1, 1, 1, 1$ Note that they are sitting in circle, so first and last terms together cannot be $0$ because this would make them consecutive. Another combination could be $1, 1, 1, 0, 1, 0, 1, 0, 1, 1$ Since the order of selection obviously doesn't matter, I can choose to fix the $1$ s and then count the number of ways I can fill the spots in-between them which would make sure a distance between two selections is taken care of. Hence, let $_$ represent the possible positions $0$ can take: $_ 1 _ 1 _ 1 _ 1 _ 1 _ 1 _ 1$ Note that first and last spots are actually the same if you join the string of numbers and dashes to form a loop. I have $7$ spots to place $3$ $0$ s, order irrelevant, which makes it $\binom73$ choices, $35$ ways However, I saw the solution being done with Inclusion-Exclusion Principle, to which my answer does not match. What combinations did I skip in this, because I'm getting a smaller answer.",['combinatorics']
3626601,Poincare lemma: from local to global solutions,"I am reading a book by Moskowitz and Paliogiannis where I miss a point when trying to complete a proof of the Poincare lemma (which is left to the reader). The following facts are proved clearly in the book. Fact 1: Let $F:\Omega\to\mathbb{R}^{n}$ be a $C^{1}$ -mapping (vector field), $\Omega$ an open ball in $\mathbb{R}^{n}$ , and $$
 \forall i,j\in\{1,\dots,n\}\; \forall x\in\Omega: \quad \frac{\partial F_{i}}{\partial x_{j}}(x)=\frac{\partial F_{j}}{\partial x_{i}}(x).
$$ Then $F$ is conservative, i.e., there is a $C^{1}$ -funcion $f:\Omega\to\mathbb{R}$ such that $\mbox{grad} f =F$ . Fact 2: Let $F:\Omega\to\mathbb{R}^{n}$ be a $C^{1}$ -mapping, $\Omega$ a region (=open and connected) in $\mathbb{R}^{n}$ , and for any closed $C^{1}$ -curve $\gamma$ in $\Omega$ , it holds that $$
\int_{\gamma}F\cdot ds=0.
$$ Then $F$ is conservative. The goal is to prove the following: Theorem (Poincare lemma): Fact 1 remains true if $\Omega$ is any simply connected subset of $\mathbb{R}^{n}$ . I miss a point when trying to pass from balls to simply connected domains. I suppose that one should elaborate on Fact 2 and use the simple connectedness of $\Omega$ . Question: How should I construct the potential $f$ on a simply connected domain $\Omega$ provided that I am aware of Facts 1 and 2?","['vector-analysis', 'analysis', 'real-analysis']"
3626640,How were the five large solutions to the Fermat-Catalan conjecture found?,"The Fermat-Catalan conjecture is the statement that the equation $a^m + b^n = c^k$ has only finitely many solutions when $a, b, c$ are positive coprime integers, and $m,n,k$ are positive integers satisfying $\frac{1}{m} + \frac{1}{n} +\frac{1}{k} <1$ . So far, ten solutions are known; five are ""small"" while the other five are surprisingly large. The five large ones, found by Beukers and Zagier,  are: $33^8+1549034^2=15613^3$ $1414^3+2213459^2=65^7$ $9262^3+15312283^2=113^7$ $17^7+76271^3=21063928^2$ $43^8+96222^3=30042907^2$ The earliest mention (at least the one I could find) of these solutions can be found in this paper by Henri Darmon and Andrew Granville from the mid 1990s (page 3). Richard K. Guy, in the third edition of his book Unsolved Problems in Number Theory , published in 2004, gives this quick remark; The five big solutions were found by clever computations by Beukers and Zagier. ( page 115 ) Still no mention about how they were found. The ""computations"" would imply a computer brute-force search but seeing as how it was in the 90s, such a task would take an infeasible amount of time to finish. Looking around even further, I found a 2016 paper by Frits Beukers in which he quips: To illustrate the phenomena we encounter when solving the generalized Fermat equation, we give a partial solution of $x^2 + y^8 = z^3$ . This equation lends itself very well to a stepwise descent method. (page 3, Chapter 2) Could that be an explanation of how the large solutions were found? I am skeptical because the paper was written more than 20 years after the large solutions were mentioned in the aforementioned Henri Darmon's and Andrew Granville's paper. Besides, the method doesn't seem to always work because later in the paper he states the following: In many cases, like $x^3 + y^5 = z^7$ , this descent is not so obvious any more... (page 4, Chapter 2) So, how were the five large solutions found? Update: I got hold of the second edition of Richard K. Guy's book published in 1994 and there isn't any mention of the large solutions. Thus, it is very likely that in the third edition of his book, he gets his information from the paper by Henri Darmon and Andrew Granville.","['number-theory', 'conjectures']"
3626650,Stochastic Processes/Trajectory and a Standard Example for a Modification,"I try to understand the nature of the sample space $\Omega$ for a stochastic process $(X_t)_{t \in I}$ . Similar to the question here , I wonder what a fixed $\omega \in \Omega$ means, when we talk about a trajectory or path given by the function on $I$ , $t \rightarrow X_t(\omega)$ . I can follow the explanation here that $\omega$ encodes a whole sequence and in the book from Oksendal ""Stochastic Differential Equations"" it is written that $\Omega$ should be regognized as a subset of the space $\tilde{\Omega}=(\mathbb{R}^n)^I$ of all function from $I$ into $\mathbb{R}^n$ , if the stochastic process maps to $\mathbb{R}^n$ . However, checking the consistency of these statements (and my poor imagination) with examples of stochastic processes, I stumbled over the following example for a modification of a stochastic process see also here (which is quite standard as far as I get it): $\textbf{Example:}$ Let $\Omega = [0,\infty), \mathcal{A} = \mathcal{B}([0,\infty))$ and $P$ be a probability measure on $\mathcal{A}$ which has a density. Define two stochastic processes $(X(t): t \ge 0)$ and $(Y(t): t \ge 0)$ by \begin{align}
X(t)(\omega) = 
\begin{cases} 
1, \text{ if $t = \omega$},\\
0, \text{ otherwise}
\end{cases}
\quad Y(t)(\omega) = 0 \quad \text{for all $t \ge 0$ and all $\omega \in \Omega$.}
\end{align} Then $X$ and $Y$ are modifications of each other. I wonder how the idea that $\omega$ should encode the whole sequence relates to the specific sample space $\Omega = [0,\infty)$ given by the example. In this case $\omega \in \Omega$ is just a real number, isn't it? If this statement is true, then dimensionality of $\omega$ does not match my expectations. On the one hand, $I$ seems to be the non-negative real number line $\mathbb{R}_{\geq 0}$ . On the other hand, we obtain a single non-negative real number $\omega \in \Omega  \, \forall t \in I$ ? Where is the randomness (in time) in this case?","['stochastic-processes', 'measure-theory', 'probability-theory']"
3626718,Triangle construction given semiperimeter and radii of inscribed and circumscribed circles.,"Given $$ \rho,r,R $$ of a scalene triangle how to construct it geometrically , say when side $c$ is parallel to $x-$ axis... with no Ruler/Compass restriction?","['trigonometry', 'geometry', 'geometric-construction']"
3626835,Solution verification - Exercise 1.4 in Brezis's Functional Analysis,"Exercise 1.4 in Brezis's Functional Analysis, Sobolev Spaces and partial Differential Equations reads as follows: Let $E = c_0$ be the space of real sequences tending to $0$ with norm $||u|| = \sup_k |u_k|$ . In $E$ define for $u = (u_1, u_2, \ldots)$ $$
f(u) = \sum_1^\infty \frac{1}{2^n} u_n.
$$ 1. Check that $f$ is a continuous linear functional on $E$ and compute $||f||_{E^*}$ .
   2. Can one find some $u \in E$ such that $||u|| = 1$ and $f(u) = ||f||_{E^*}$ ? I was able to find a solution for this problem, but I lack confidence. Is the following correct? Linearity is trivial.  Now, note that $$
|f(u)| = \left|\sum_1^\infty \frac{1}{2^n} u_n\right| \leq \sum_1^\infty \frac{1}{2^n} |u_n|.
$$ The norm on $c_0$ , according to Section 11.3, is $$
||u|| = \sup_n |x_n|.
$$ We know that $x_n \to 0$ , so by the definition of convergence we have that $|u_n| \to 0$ . Then $(|u_n|)_n$ is bounded by $\sup_n |u_n|$ . Hence $$
|f(u)| \leq \left(\sup_n |u_n|\right) \left(\sum_1^\infty \frac{1}{2^n}\right) =  ||u||.
$$ We conclude that $f$ is bounded and hence continuous. Now, we have that $$
||f|| = \sup_{||u|| \leq 1} |f(u)| \leq 1, 
$$ by the above result. Fix $\varepsilon > 0$ . Then there is some $k \in \Bbb{N}$ such that $\sum_1^k 1/(2^n) > 1 - \varepsilon$ . But this is just $f(u_k)$ , where $$
u_k = (1, 1, \ldots, 1, 0, 0 \ldots),
$$ where the $1$ s go until the $k$ -th position. That means that for arbitrarily small $\varepsilon > 0$ there are elements in $B_{c_0}(0)$ such that $|f(u)| > 1 - \varepsilon$ . Hence $$
1 \leq \sup_{||x|| \leq 1} |f(u)| \leq 1
$$ and therefore $$
||f|| = 1.
$$ If there was such an $u$ , we would have $$
f(u) - 1 = 0 \implies \sum_1^\infty \frac{1}{2^n}(u_n - 1) = 0,
$$ which in turn implies that $u_n = 1$ for every $n$ . But this is absurd, since $u_n \to 0$ . Thanks in advance and kind regards.","['solution-verification', 'functional-analysis', 'sequences-and-series']"
3626907,"Given three sets $A,B$ and $C$. Does $A\cap C = B\cap C$ imply $A=B$? (Prove if this is true or give a counter example)","Given three sets $A,B$ and $C$ . Does $A\cap C = B\cap C$ imply $A=B$ ? (Prove if this is true or give a counter example) Im not entirely sure if I am right, however, I came up with this. 
Please correct me if I am wrong and help me. Thanks in advance. Given $A\cap C = B\cap C$ , $A$ is not equal to $B$ .
Counter example:- $A=\{1\}$ and $B=\{2\}$ and $C = \emptyset$ $A\cap C = \emptyset$ . Similarly, $B\cap C = \emptyset$ .
Thus $A\cap C=B\cap C$ . However, as we can see, they aren't equal. Is this correct? Please help
Thank you.",['elementary-set-theory']
3626939,"The number of pairs $(m,n)$ of coprime positive integers that divide $k$ is $d(k^2)$, where $d$ is the divisor counting function.","I recently found somewhere that, if $k$ is a fixed integer.Then the number of ordered pairs of positive integers $(m,n)$ such that they are coprime and both of them divide $k$ is $d(k^2)$ , where $d$ denotes ""the number of divisor function.""I tried but was unable to prove it.  I thought there might be some bijection between them but failed to prove it.","['combinatorial-proofs', 'number-theory', 'elementary-number-theory', 'combinatorics', 'divisor-counting-function']"
3627011,On line bundle on curves lying on a surface,"Let $C$ be a smooth irreducible curve on a complex algebraic surface $X$ and $L$ be a base point free line bundle on $C$ such that we have $H^0(L) \otimes H^0(\mathcal O_X(m)) \hookrightarrow H^0(L(m))$ for some positive $m$ .  Also assume that $H^0(L), H^0(\mathcal O_X(m)) \neq 0$ and $\text{deg}(L(m)) >0$ . Then my question is :  Under what condition(s)(cohomological or otherwise) on the line bundle $L$ the above inclusion becomes an isomorphism? An obvious answer is to assume $H^0(L(m)) =0$ , But I am looking for condition(s) other than this. Any help from anyone is welcome.",['algebraic-geometry']
3627020,Is all group theory permutation group theory?,"By Cayley's theorem every abstract group is isomorphic to some permutation group. Since the permutation group viewpoint has the advantage of considering the actions of the group on different sets, and therefore, of finding structure not just in the underlying set of the group, but in the behaviour of its elements, why do we not always consider the permutation representations of a group? In other words, is there an advantage to looking at ""groups"" proper rather than groups of permutations?
Is the answer different for finite and infinite groups? (Edit) In the comments, it was pointed out that the details introduced by considering a particular permutation/linear representation may be a hinderance when, for example, the focus is on combinatorial properties of groups given as their group presentation.
What are other examples of situations in group theory when the abstract view of the group is preferred?","['permutations', 'computational-algebra', 'representation-theory', 'abstract-algebra', 'group-theory']"
3627045,Polar Coordinate Transformation - Motivation,"I am trying to work out the reason why the integral $$ \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}{e^{-(x^2+y^2)}}\,dx\,dy $$ is, in polar coordinates, $$ \int_{-\infty}^{\infty}{e^{-r^2}} \,r\,dr\,d\theta$$ As I understand it, a polar coordinate transformation involves the following substitution: $$ (x,y) \rightarrow (r\cos{\theta},r\sin{\theta})$$ This would imply that $$-(x^2+y^2) = -((r\cos{\theta})^2+(r\sin{\theta})^2) = -r^2((\cos{\theta})^2+(\sin{\theta})^2) = -r^2 $$ which gets us this far $$ \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}{e^{-r^2}}\,dx\,dy $$ To motivate why $$ dx\,dy = r\,dr\,d\theta$$ I thought of the following argument: $$ dx = d(x(\theta,r)) = \frac{\partial(x(\theta,r))}{\partial\theta}d\theta + \frac{\partial(x(\theta,r))}{\partial r}dr \,+ \frac{\partial^2(x(\theta,r))}{(\partial r)^2}(dr)^2 + \, ... \\ =r(-\sin{\theta})\,d\theta+ dr\cos{\theta}\, +\,...$$ $$dy = r(\cos{\theta})\,d\theta+ dr\sin{\theta} \,+\,...$$ $$ \therefore\: dx\,dy = [r(-\sin{\theta})\,d\theta+ dr\cos{\theta}\, +\,...]\,[ r(\cos{\theta})\,d\theta+ dr\sin{\theta} \,+\,...] \\ = r\,dr\,d\theta \, ((\cos{\theta})^2 -(\sin{\theta})^2)\,+\,... \\
= r\,dr\,d\theta \, ((\cos{\theta})^2 -(\sin{\theta})^2),\, \text{ ignoring }\textit{o}((dr)^2) \text{ and } \textit{o}((d\theta)^2)\text{ terms.}  $$ However, I am off by a minus sign, which would enable me to argue $$dx\,dy = r\,dr\,d\theta \, ((\cos{\theta})^2 + (\sin{\theta})^2) = r\,dr\,d\theta $$ If it were correct, I would find this line of argument would be much more analytically convincing than the typical argument involving $$ dx\,dy = dA = r\,dr\,d\theta$$ which I find to be less mechanistically obvious than the above substitution-based argument that I considered but that I am not being able to fully justify. Could you please tell me whether my substitution-based argument can work, potentially by correcting some mistake or another that I might have made? If not, do you have any similarly analytical or mechanistic justification as to why $ dx\,dy = r\,dr\,d\theta$ ?","['integration', 'polar-coordinates', 'gaussian-integral', 'substitution']"
3627050,Term-wise Differentiation of an Infinite Series,"One way to prove if an infinite sum is to use the following theorem Theorem Suppose $\{f_n\}$ is a sequence of functions, differentiable on $[a, b]$ and such that $f_n(x_0)$ converges for some point $x_0$ on $[a,b]$ . If $f_n^\prime$ converges uniformly on $[a,b]$ , then $f_n$ converges uniformly on $[a, b]$ , to a function $f$ , and $$
f^\prime(x) = \lim_{n\to\infty} f_n^\prime(x) \qquad (a \leq x \leq b)
$$ Does this theorem have a converse? An example of what I mean: In Elementary Classical Analysis (Marsden) there is the following exercise (p. 316, #52): Can the series $$x = \sum_{k=1}^\infty \frac{x^k}{k} - \frac{x^{k+1}}{k+1} \quad 0 \leq x \leq 1 $$ be diﬀerentiated term by term? Letting $S_n =  \sum_{k=1}^n \frac{x^k}{k} - \frac{x^{k+1}}{k+1} = x-\frac{x^{n+1}}{n+1} $ , then $S_n' = 1  - x^{n+1}$ . This converges uniformly to $1$ on $[0,r], r < 1$ . So, on $[0,r], r < 1$ , we can differentiate term by term by the theorem. (term-by-term is an odd way of expressing the point of this exercise I feel. I think it is phrased this way because $S_n$ are differentiated term by term.) In this case, we know that the sum is equal to $x$ , so the derivative is equal to $1$ . The term-wise differentiation yields $0$ at $x=1$ , so term-wise differentiation is not possible at $x=1$ . But is there a general method to disproving this? (i.e., if the series was not equal to something that is easy to differentiate, can we conclude anything about it's differentiability from this theorem?","['real-analysis', 'calculus', 'uniform-convergence', 'sequences-and-series', 'derivatives']"
3627186,How many relations R are there on a set such that at least one ordered pair in R has a(a particular element) as its first element?,"I don't know what to do with this question...spacially the part that says ""at least""...
I know that the number of possible relations on a set with n elements is:
2^(n^2)
Can anyone tell me how can I answer this problem?
Thank you so much!","['relations', 'discrete-mathematics']"
3627187,"Completeness, UMVUE, MLE in uniform $(-\theta,2\theta)$ distribution","Let $\theta >0$ be a parameter and let $X_1,X_2,\ldots,X_n$ be a random sample with pdf $f(x\mid\theta)=\frac{1}{3\theta}$ if $-\theta \leq x\leq 2\theta$ and $0$ otherwise. a) Find the MLE of $\theta$ b) Is the MLE a sufficient statistic for $\theta$ ? c) Is the MLE a complete statistic for $\theta$ ? d) Is $\frac{n+1}{n}\cdot MLE$ the UMVUE of $\theta$ ? I've been able to solve a). The MLE of $\theta$ is $\max(-X_{(1)},\frac{X_{(n)}}{2}).$ Also, you can show that it is sufficient using the Factorization Theorem. However, I cannot solve the next questions I think because of the $\max$ in the MLE. Is there another way to express $\max(-X_{(1)},\frac{X_{(n)}}{2})$ ? Can I express the MLE as $\frac{|X|_{(n)}}{2}?$","['statistical-inference', 'statistics', 'probability-distributions', 'parameter-estimation', 'maximum-likelihood']"
3627260,Conditional expectation $\mathbb{E}(X|X+Y)$ for 2 different Binomial distributions,"Let X,Y be independent random variables, where X have Binomial distribution $B(n,p)$ and Y have Binomial distribution $B(m,p)$ . Find $\mathbb{E}(X|\sigma(X+Y))(\omega)$ . What I know is that X+Y have distribution $B(n+m,p)$ , so $\mathbb{E}(X|\sigma(X+Y))(\omega)= \sum_{k=0}^{n+m}\mathbb{E}(X|X+Y=k)\mathbf{1}_{X+Y=k}(\omega)$ and generally I have to find $\mathbb{E}(X|X+Y=k)$ , but I totally stuck because X and Y aren't i.i.d. If they will be, then by symmetry $\mathbb{E}(X|X+Y=k)=\mathbb{E}(Y|X+Y=k)$ , so $2\mathbb{E}(X|X+Y=k)$ will be equal to $\mathbb{E}(X|X+Y=k) + \mathbb{E}(Y|X+Y=k)= \mathbb{E}(X+Y|X+Y=k)=X+Y$ , so in this case $\mathbb{E}(X|X+Y=k)=\frac{X+Y}{2}$ , 
but I don't know if I can make use of this in my situation. I know also that $\mathbb{E}(X|X+Y=k)= \frac{1}{\mathbb{P}(X+Y=k)}\int_{X+Y=k}Xd\mathbb{P}$ and I know, that this integral will become a sum because we deal with discrete distribution, but I have know idea how to calculate this (and did't find any useful hint on internet so far).
How can I calculate such thing (the problem for me is that I'm integrating with respect to X and Y)?","['conditional-expectation', 'probability-theory', 'probability']"
3627449,Can I multiply 2 determinants of square matrices when they are of different in sizes?,"In the image attached, you can see the problem. (I am supposed to compute iii ) I know that any equally sized square matrices' determinants can be multiplied directly. $$det(AB) = det(A)det(B)$$ Here however one of the matrices is 4x4 and the other is 5x5 so I am not sure if that rule holds or if this value can be computed.","['matrices', 'determinant']"
3627464,Why is it false that for all $y\in\mathbb{R^n}$ the solution of the initial value problem $x(0) = y$ exists for all time $t$.,"Is the following statement false because our solutions for the initial value problem may not exist when $t=0$ ; depending on our function? Also, uniqueness does not exist if the system is nonlinear, such as quadratic. Hence, the statement below is false. Statement: Consider $x'=f(x)$ and suppose that for every initial condition $x(0)=x_0$ solutions exists and are unique for some time interval $[-a(x_0), a(x_0)]$ whose length depends on the initial value $x_0$ . Then for all $y\in\mathbb{R^n}$ the solution of the initial value problem $x(0) = y$ exists for all time $t$ .","['initial-value-problems', 'dynamical-systems', 'ordinary-differential-equations', 'real-analysis']"
3627473,"Closed form of $\sum_{k=0}^{\infty} \sum_{m=0}^{\infty} r^m \cdot t^k \binom{m+k}{k} \binom{m+k+1}{k}$ for fixed $r, t$","I would like to find the closed form for the double sum $$\sum_{k=0}^{\infty} \sum_{m=0}^{\infty} r^m \cdot t^k \binom{m+k}{k} \binom{m+k+1}{k} \tag 1$$ where $r, t$ are known values. When I plugged this into Mathematica, I got two equivalent sums: $$\sum_{m=0}^{\infty} r^m \space _2F_1(1+m, 2+m, 1, t) = \tag 2$$ $$\sum_{k=0}^{\infty} (1+k)t^k \space _2F_1(1+k, 2+k, 2, r) \tag 3$$ Here, $_2F_1$ is a hypergeometric function . What I noticed is that $_2F_1(1+m, 2+m, 1, t) = \frac{P_m}{(1-t)^{2m+2}}$ , where $P_m$ is a polynomial in $t$ of degree $m$ and that $_2F_1(1+k, 2+k, 2, r) = \frac{Q_k}{(1-r)^{2k+1}}$ , where $Q_k$ is a polynomial in $r$ of degree $k-1$ . I tried taking another approach, changing the indices of $(1)$ so that $s = k+m$ : $$\sum_{s=0}^{\infty} \sum_{k=0}^{s} r^{s-k} \cdot t^k \binom{s}{k} \binom{s+1}{k} \tag 4$$ This however, also led to a sum of hypergeometric functions: $$\sum_{s=0}^{\infty} r^s \space _2F_1 \left( -1-s, -s, 1, \frac{t}{r} \right) \tag 5$$ All these attempts seem pointless to me, since using the hypergeometric function just yields a more compact way of expressing the double sum (not an actual simplification), which brings me to the main question: How can I get a closed form of the original double sum? Edit: $r, t < 0$ if it makes any difference.","['binomial-coefficients', 'hypergeometric-function', 'sequences-and-series']"
3627512,"Is there a set $A \subset [0,1]$ such that $\int_{A \times A^\text{c}} \frac{\mathrm{d} x \, \mathrm{d} y}{\lvert x - y\vert}=\infty$?","The above question came up when I was trying to find a counterexample related to this problem. Clearly, the integral of $(x,y) \mapsto \lvert x-y \rvert^{-1}$ over $[0,1]^2$ is divergent. When integrating over a subset of the form $A \times A^\text{c}$ with Lebesgue measurable $A \subset [0,1]$ (and $A^\text{c} = [0,1] \setminus A$ ), however, the result is generally finite. I would like to know whether this is true for every $A$ . My thoughts so far: For the simple interval $A = [0,1/2]$ the integral has the finite value $\log(2)$ . The integrand is only singular near the point $(1/2,1/2)$ , which is not enough to make the two-dimensional integral diverge. The same is true if $A$ is a union of finitely many intervals. Therefore, in order to make the integral large we need 'a lot' of points $(x,y) \in A \times A^\text{c}$ for which $\lvert x - y \rvert$ is small. This can be achieved by choosing $A = [0,1] \cap \mathbb{Q}$ , but of course the integral is simply zero  in this case because $A \times A^\text{c}$ is a Lebesgue null set. Thus we also need to ensure that both $A$ and $A^\text{c}$ have positive measure. We can let $A$ be the fat Cantor set to fulfil both requirements: $A$ and $A^\text{c}$ have Lebesgue measure $\frac{1}{2}$ each and there is an infinite number of points on the diagonal of $[0,1]^2$ near which the integrand diverges. I have tried to show that the integral is finite/infinite using the sequence of simpler sets defined in the iterative construction of $A$ , but the corresponding integrals become complicated rather fast and it seems like I am stuck at this point. Question: Can we prove that $\int \limits_{A \times A^\text{c}} \frac{\mathrm{d} x \, \mathrm{d} y}{\lvert x - y\vert} < \infty$ holds for every Lebesgue measurable $A \subset [0,1]$ or find a counterexample?","['integration', 'lebesgue-measure', 'lebesgue-integral']"
3627747,Computing the first 2 terms of the Taylor series exp. for the center manifold & find the reduced equation on the center manifold.,I am having difficulty solving the problem below. It is from Meiss Dynamics book. Can I please receive help solving the following system? Thank you Consider the system $$x' = y$$ $$y'=-y+ax^2 + bxy.$$ Compute the first two terms of the Taylor series expansion for the center manifold and find the reduced equation on the center manifold. For what values of $a$ and $b$ is the origin stable? Unstable? Semi-stable? Note the linearization at the origin is not in Jordan Canonical Form.,"['linearization', 'ordinary-differential-equations', 'stability-in-odes', 'manifolds', 'dynamical-systems']"
3627750,"Why isn't $\oint_{C} f(z) = 2\pi i\, \mathrm{d}z$?","I was going over some practice problems T/F: If $C$ is the circle in $\mathbb{C}$ of radius $10$ centered at $z=2i$ with positive orientation and $$f(z) = \frac{\cos(4z)}{z}$$ then $$ \oint_{C} f(z) \mathrm{d}z = 2\pi i\, \mathrm{}.$$ I used the Cauchy Integral Theorem, with $z_0= 0$ and $f(z)= \cos(4z)$ . That gives the value as $2\pi i$ , but the answer is given as false? What mistake am I making?","['complex-analysis', 'complex-integration', 'cauchy-integral-formula']"
3627774,Prove that relation on $\mathbb Z$ ($3a-7b$ is even) is symmetric,"If you had a relation $R$ on $\mathbb{Z}$ defined by $aRb$ if $3a-7b$ is even how would you prove that it is symmetric? Here is my attempt: Let $y,z\in\mathbb{Z}$ assume $yRz$ then $3y-7z$ is even. This means $3y-7z=2k$ where $k\in\mathbb{Z}$ . Then, $y=\frac{2k+7z}{3}$ so, $3y-7z=3z-7(\frac{2k+7z}{3})=-\frac{2}{3}(7k+20z)$ . And that is where I stopped. How could you prove that this was even? if you factor out the two, the remaining equation is not even an integer.","['equivalence-relations', 'relations', 'discrete-mathematics']"
3627781,Question about when a projection from a fiber product is open,"This is lemma 9.5.6 in Vakil's Foundations of Algebraic Geometry. Let $X$ and $Y$ be schemes over a field $k$ . Then, we want to prove that the projection $X \times_k Y \rightarrow Y$ is open. First reduce to affine schemes, so we have $k$ -algebras $A$ and $B$ , and we need to prove that $B \rightarrow A \otimes_k B$ induces an open map $\operatorname{Spec} (A \otimes_k B) \rightarrow \operatorname{Spec} (B)$ is an open map. Let $f =\sum_{i=1}^n a_i \otimes b_i \in A \otimes_k B$ , and we want to prove that the image of $D(f)$ in $\operatorname{Spec}(B)$ is open. To do so, let $A'$ be the subring of $A$ generated by the $a_i$ . Then, $A'$ is finitely generated over $k$ . The ring homomorphism factors as $B \rightarrow A' \otimes_k B \rightarrow A \otimes_k B$ . This is the part where I don't understand. Vakil says we can now replace $A$ with $A'$ . What is the relationship between the image of $D(f) \subseteq A \otimes_k B$ and $D(f) \subseteq A' \otimes_k B$ ? $A' \rightarrow A$ is injective, and $B$ is flat as a $k$ -module, but injective morphisms don't correspond to open maps between schemes.","['algebraic-geometry', 'schemes']"
3627784,Does the fraction of distinct substrings in prefixes of the Thue–Morse sequence of length $2^n$ tend to $73/96$?,"Recall that the Thue–Morse sequence $^{[1]}$ $\!^{[2]}$ $\!^{[3]}$ is an infinite binary sequence that begins with $\,t_0 = 0,$ and whose each prefix $p_n$ of length $2^n$ is immediately followed by its bitwise complement (i.e. obtained by flipping $0\to1$ and $1\to0$ ): $$
\begin{array}{c|cc}&t_0&t_1&t_2&t_3&t_4&t_5&t_6&t_7&\!\!\!\dots\\\hline
p_0&0\\
p_1&0&\color{red}1\\
p_2&0&1&\color{red}1&\color{red}0\\
p_3&0&1&1&0&\color{red}1&\color{red}0&\color{red}0&\color{red}1\\
\cdots&\cdots\!\!
\end{array}
$$ We are interested in contiguous substrings of these prefixes. For a string $\mathcal{S}$ of length $\ell$ , the total number of its substrings, including the empty substring $\langle\unicode{x202f}\rangle$ and the string $\mathcal{S}$ itself, is $(\ell^2+\ell+2)/2.$ Hence, the total number of substrings in $p_n$ is $(4^n+2^n+2)/2.$ Clearly, not all of those substrings are distinct for $n>1$ . For example, $p_2 = \langle0\,1\,1\,0\rangle$ has $11$ substrings in total, but only $9$ distinct substrings: $$
\begin{array}{l|cc}&\langle\!\!\!&0&\color{#808080}1&\color{#b8b8b8}1&\color{#c8c8c8}0&\!\!\!\rangle\\\hline
1&\langle\!\!\!&&&&&\!\!\!\rangle\\\hdashline
2&\langle\!\!\!&0&&&&\!\!\!\rangle\\
&\langle\!\!\!&&&&\color{#c8c8c8}0&\!\!\!\rangle\\\hdashline
3&\langle\!\!\!&&\color{#808080}1&&&\!\!\!\rangle\\
&\langle\!\!\!&&&\color{#b8b8b8}1&&\!\!\!\rangle\\\hdashline
4&\langle\!\!\!&0&\color{#808080}1&&&\!\!\!\rangle\\
5&\langle\!\!\!&&\color{#808080}1&\color{#b8b8b8}1&&\!\!\!\rangle\\
6&\langle\!\!\!&&&\color{#b8b8b8}1&\color{#c8c8c8}0&\!\!\!\rangle\\
7&\langle\!\!\!&0&\color{#808080}1&\color{#b8b8b8}1&&\!\!\!\rangle\\
8&\langle\!\!\!&&\color{#808080}1&\color{#b8b8b8}1&\color{#c8c8c8}0&\!\!\!\rangle\\
9&\langle\!\!\!&0&\color{#808080}1&\color{#b8b8b8}1&\color{#c8c8c8}0&\!\!\!\rangle
\end{array}
$$ Among these, $\langle0\rangle$ and $\langle1\rangle$ appear in $p_2$ twice, so the fraction of distinct substrings in $p_2$ is $\,\stackrel9{}\!\!\unicode{x2215}_{\!\unicode{x202f}11}\!.$ Can we find a simple general formula for $\mathscr D_n$ , the number of distinct substrings in $p_n$ ? Let's try to compute a few terms: $$2,\,4,\,9,\,28,\,101,\,393,\,1561,\,6233,\,24921,\,99673,\,398681,\,1594713,\,6378841,\,\dots$$ These few terms can be computed by a brute-force approach, but using Coolwater 's program from here we can compute hundreds of thousands more. It is not too difficult to discover that for $n>2$ all known terms match a simple formula: $$\mathscr D_n\stackrel{\color{#d0d0d0}?}=\frac{73\cdot4^n+704}{192}\color{#d0d0d0}{,\,\,\text{for}\,\,n>2}\tag{$\diamond$}$$ Somewhat oddly, the three initial terms $\mathscr D_0=2,\,\mathscr D_1=4,$ and $\mathscr D_2=9$ do not match the general formula $(\diamond)$ , which results in non-integer rational values for these indexes. I conjecture that the general formula $(\diamond)$ is valid for all $n>2$ . $$\bbox[LemonChiffon]{\begin{array}{c}
\\
\hspace{1in}\text{Could you suggest a way to prove this conjecture?}\hspace{1in}\\
\vphantom.
\end{array}}$$ If the conjecture turns out to be true, then we have a remarkable corollary that for $n\to\infty$ the fraction of distinct substrings in the prefixes $p_n$ tends to a quite surprising limit: $$\mathscr L=\lim_{n\to\infty}\frac{73\cdot4^n+704}{192}{\large/}\frac{4^n+2^n+2}2=\frac{73}{96}.\tag{$\small\spadesuit$}$$","['conjectures', 'combinatorics-on-words', 'experimental-mathematics', 'discrete-mathematics', 'sequences-and-series']"
3627790,why dominated convergence theorem is related to the superiority of Lebesgue integration (over Riemann integration)?,"I read the following paragraph from wikipedia: In measure theory, Lebesgue's dominated convergence theorem provides sufficient conditions under which almost everywhere convergence of a sequence of functions implies convergence in the $L^1$ norm. Its power and utility are two of the primary theoretical advantages of Lebesgue integration over Riemann integration. Can any one explain why the power of this theorem is the primary theoretical advantage of Lebesgue integration over Riemann? It would be great if you could illustrate with examples. Thanks!","['riemann-integration', 'measure-theory', 'lebesgue-integral']"
3627804,"Show that there is a $c\in(0,1)$ such that $f(c)=\int_0^cf(x)dx$.","Question: Let $f:[0,1]\to\mathbb{R}$ be a continuous function such that $$\int_0^1f(x)dx=\int_0^1xf(x)dx.$$ Show that there is a $c\in(0,1)$ such that $$f(c)=\int_0^cf(x)dx.$$ My solution: Define the function $g:[0,1]\to\mathbb{R}$ , such that $$g(x)=x\int_0^x f(t)dt-\int_0^x tf(t)dt, \forall x\in[0,1].$$ Now since $f$ is continuous on $[0,1]$ , thus we can conclude by Fundamental Theorem of Calculus that $g$ is differentiable $\forall x\in[0,1]$ and $$g'(x)=\int_0^x f(t)dt+xf(x)-xf(x)=\int_0^xf(t)dt, \forall x\in[0,1].$$ Observe that $g(0)=g(1)=0$ . Hence by Rolle's Theorem we can conclude that $\exists b\in(0,1)$ , such that $g'(b)=0$ , i.e $$\int_0^b f(t)dt=0.$$ Now define $h:[0,1]\to\mathbb{R}$ , such that $$h(x)=e^{-x}g'(x), 
\forall x\in[0,1].$$ Now $h'(x)=-e^{-x}g'(x)+g''(x)e^{-x}=e^{-x}(g''(x)-g'(x)), \forall x\in[0,1].$ Observe that $h(0)=h(b)=0$ . Hence by Rolle's Theorem we can conclude that $\exists c\in(0,b)\subseteq (0,1)$ , such that $h'(c)=0$ . This implies that $$e^{-c}(g''(c)-g'(c))=0\\\implies g''(c)-g'(c)=0\hspace{0.3 cm}(\because e^{-c}\neq 0)\\\implies f(c)=\int_0^cf(x)dx.$$ Is this solution correct? And is there a better solution that this?","['solution-verification', 'real-analysis']"
