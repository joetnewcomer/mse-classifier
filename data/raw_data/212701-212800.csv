question_id,title,body,tags
4292993,Who's right regarding this proof of a property of the boundary of a union?,"The question is to prove $$\partial(A\cup B)\subseteq\partial A\cup \partial B.$$ My proof is, let $z$ be in $\partial (A \cup B)$ . Then for any $\epsilon > 0$ , the definition of boundary says $D_\epsilon (z)\cap (A \cup B) \neq \emptyset$ and $D_\epsilon (z)\cap (A \cup B)^c \neq \emptyset$ . This implies two statements, $\;\;D_\epsilon (z)\cap A \neq \emptyset$ or $D_\epsilon (z)\cap B \neq \emptyset$ $\;\;D_\epsilon (z)\cap A^c \neq \emptyset$ and $D_\epsilon (z)\cap B^c \neq \emptyset$ which implies either $z \in \partial A$ or $z\in \partial B$ , again from the definition of boundary. My professor marked my proof incorrect by saying, $$\forall \epsilon >0 (x\:or\:y)$$ does not imply $$(\forall \epsilon >0\:x)\: or \: (\forall \epsilon >0\:y)$$ I agree with this statement, but I don't understand how that applies to my proof.","['complex-analysis', 'general-topology', 'elementary-set-theory']"
4293032,Variance of $x$ co-ordinate of uniform random point on a circle.,"Let $P$ be a point uniformly randomly chosen on the perimiter unit circle. Let $X$ denote its $x$ co-ordinate.  What is the Var $[X]$ ? My method By symetery $\mathbb{E}[X] = 0 $ and so Var $[X]$ = $\mathbb{E}[X^2]$ Using a bit of trig I found the CDF of $X^2$ as $ F_{X^2}(t) = \mathbb{P}[X^2 \leq t] = 1-\frac{2cos^{-1}(\sqrt{t})}{\pi}$ . Using Desmos I then differentiated for the PDF and integrated for the expected value which gives $\mathbb{E}[X^2] = \frac{1}{2}$ . I confirmed this answer using a bit of python. Going forward Such a simple answer makes me think there must be a neat little trick to find $\mathbb{E}[X^2] = \frac{1}{2 } $ ? Can someone find an easier way of doing this? Possibly using the law of total variance conditioning on the $y$ co-ordinate? What about parametrizing the points in polar? Then we only deal with  Uniform $[0,2\pi] $ variable? Edit I initially intended on finding the variance of the $x$ co-ordinate of a point selected uniformly on the surface of the unit sphere. Could someone give some pointers for this too ?","['expected-value', 'statistics', 'probability']"
4293138,Prove $\lim_{x\to \infty}\frac{\sin x}{x^2} = 0$.,Prove using definition of a limit that $\lim_{x\to \infty}\frac{\sin x}{x^2} = 0$ . Proof: Let $\epsilon > 0$ . Note that $\left|\frac{\sin x}{x^2}\right| \leq \frac 1 {x^2}$ for $x\ne 0$ . Then choose $M= \frac 1{\sqrt{\epsilon}}$ . Then if $x> M$ implies that $\left|\frac{\sin x}{x^2}\right| \leq \frac 1 {x^2} < \epsilon$ . Am I allowed to give an $M$ the way I did?,"['epsilon-delta', 'solution-verification', 'real-analysis']"
4293141,"The $E_k$ in the definition of the simple function on pg. 61 in Royden real analysis ""4th edition"".","I want to prove that the sum of 2 simple functions is a simple function and to do so I want to use the following facts: $$\chi_{A_i}=\sum_{j}\chi_{A_i\cap B_j} \text{ and } \chi_{B_j}=\sum_{i}\chi_{A_i\cap B_j}.$$ But say if I want to prove the first fact, upon fixing $i,$ I want $A_i$ to be subset of $E$ and $E = \cup_{j}B_j$ Here is the definition of the simple function stated in Royden: If $\varphi$ is simple, has domain $E$ and takes the distinct values $c_1, \dots, c_n,$ then $$\varphi = \sum_{i=1}^{n_1} c_k \chi_{E_k} \text{ where } E_k = \{x \in E| \varphi(x) = c_k \}.$$ My question is: Are we considering the $E_k's$ in the definition of the simple function to be a partition of $E$ ? Could someone clarify this to me please?","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis', 'measurable-functions']"
4293176,Expansion of double integral,"I've come across this deceptively difficult integral: $$I(t)=\frac{e^{4t}}{t^2}\int_1^\infty dx\int_1^\infty dy\frac{e^{-2t(x+y)}}{(4xy-1)^2},$$ and I want to study its behavior around $t\rightarrow0^+$ . I found a simple series expression in terms of exponential integrals, but it doesn't seem to help much: $$I(t)=\frac{e^{4t}}{t^2}\sum_{n=0}^\infty\frac{n+1}{4^{n+2}}E_{n+2}^2(2t).$$ What are the typical tricks and techniques to handle this sort of problem? I expect that there will be terms like $1/t^2$ and $1/t$ with some factors of $\log(2t)$ and constant bits, but precisely extracting them has proven to be a challenge. Thanks in advance!","['integration', 'asymptotics', 'sequences-and-series']"
4293179,Number of solutions of $\cos^5x+\cos^5\left( x+\frac{2\pi}{3}\right) + \cos^5\left( x+\frac{4\pi}{3}\right) =0$,"Solve in the interval $[0,2\pi]$ : $$\cos^5x+\cos^5\left( x+\frac{2\pi}{3}\right) + \cos^5\left( x+\frac{4\pi}{3}\right)=0 $$ I tried expanding the L.H.S by applying the formula of $\cos(A+B)$ but it results in a quintic polynomial in terms of $\cos x $ . Wolfram alpha has simplified the left hand side all the way to $\frac{15}{16}\cos3x$ but I'm unable to think of any method to reach there.","['trigonometry', 'polynomials']"
4293188,normal vector after transformation,"Let $E$ subset of $\mathbb{R^n}$ such that $E$ has smooth boundary. If $F:\mathbb{R^n }\rightarrow \mathbb{R}^n$ is a diffeomorphism and $G = F^{-1}$ its inverse. Let $\nu_{F(E)}(z)$ be the normal vector of $F(E)$ at $y \in \partial F(E)$ . I'm trying to understand why $$\nu_{F(E)}(z) = \frac{\nabla G(z)^*\nu_E(G(z))}{|\nabla G(z)^*\nu_E(G(z))|}.$$ Here I believe $\nabla G(z)^*$ is the adjoint of the linear operator $\nabla G(z)$ . My attempt:
If $x \in \partial E$ and $\phi:U\subset \mathbb{R}^n \rightarrow \mathbb{R}$ such that $\{ y \in U \,| \phi(y)=0\} = \partial E \cap U$ then $$\nu_E(x) =\frac{ \nabla \phi(x)}{|\nabla \phi(x)|} .$$ Now $\phi\circ G: F(U) \rightarrow \mathbb{R}$ I believe gives the boundary of $F(E)$ near $F(x)=z$ then $$\frac{\nabla(\phi\circ G)(z)}{|\nabla(\phi\circ G)(z)|}= \nu_{F(E)}(z).$$ By the chain rule $$\nabla(\phi \circ G)(z) = \nabla \phi(G(z)) DG(z)$$ This where I got stuck. Would appreciate the help! *This is from Maggi's book on finite perimeter page 196.","['multivariable-calculus', 'derivatives', 'differential-geometry']"
4293189,Integration operator appears inside an integration operator as I try $~\mathcal{L}\left[x \cdot \sin^{}\left(x\right) \right]\left(s\right)~$,"$$  A:=\mathcal{L}\left[x \cdot \sin^{}\left(x\right) \right]\left(s\right) \tag{1}  $$ $$=\lim_{\beta\to\infty}\int_{0}^{\beta}\left(x\cdot\sin^{}\left(x\right)\right)\cdot\exp\left(-sx\right)\,dx$$ $$ = \lim_{\beta\to\infty}\int_{0}^{\beta}\left(x\right)\cdot \underbrace{\left( \sin^{}\left(x\right) \exp\left(-sx\right) \right)}_{\text{This part is to be integrated} }   \,dx  $$ $$ = \lim_{ \beta \to \infty} \left\{ \left[ x \cdot \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx   \right)  \right]_{0}^{\beta} - \int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx  \right)  \,dx   \right\}  $$ $$  \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx = - \frac{ s \cdot  \exp\left(-sx\right)   }{ \left( s^2+1 \right)    } \left( \sin^{}\left(x\right) + \frac{1}{s}\cos^{}\left(x\right)  \right) +\text{const} \tag{2}   $$ Should I have written the derivation of the above equation? $$ A= \lim_{ \beta \to \infty} \left\{ \left[ -\frac{  x  \cdot s \cdot \exp\left(-sx\right)   }{ \left( s^2+1 \right)    } \left( \sin^{}\left(x\right) + \frac{1}{s}\cos^{}\left(x\right)  \right)  \right]_{0}^{\beta} -  \int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx  \right)  \,dx    \right\}  $$ $$ = \lim_{ \beta \to \infty} \left\{ \underbrace{\left[ -\frac{  x  \cdot s   }{ \left( s^2+1 \right) e^{sx}   } \left( \sin^{}\left(x\right) + \frac{1}{s}\cos^{}\left(x\right)  \right)  \right]_{0}^{\beta}}_{\text{About}~\beta~ \text{,it converges to }~0   }  -  \int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx  \right)  \,dx    \right\}  $$ $$ = -\lim_{ \beta \to \infty} \underbrace{\int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx   \right)  \,dx}_\text{What can be done at here?}    $$","['integration', 'systems-of-equations', 'laplace-transform']"
4293200,atleast two balls identical in 5 bags,"There are five bags each containing identical sets of ten distinct chocolates. One chocolate is picked from each bag. The probability that at least two chocolates are identical is? I know the solution can be arrived by using 1-(none ball is picked is same) like this P(No two chocolates are identical) = $10 \cdot 9 \cdot 8 \cdot 7 \cdot 6 = 30240$ so P(no two chocolates are identical) $ = \dfrac{30240}{100000} = 0.3024$ P(At least two chocolates are identical) = $ 1$ – P(No two chocolates are identical) $ = 1 – 0.3024 = 0.6976$ But I used this approach
probability at least $2$ bags have identical chocolates is equal to = probability of two exactly two bag having same chocolates + probability of exactly three bag having same chocolates +probability of exactly four bags having same chocolates + probability of five bags have identical chocolates. Now I have calculated two bag having same chocolates is calculate as No. of ways of choosing two bags $C(5,2)$ and the these two bags have same chocolates rest have different chocolates so each $C(5,2)$ will have $10 \cdot 9 \cdot 8 \cdot 7$ (basically reducing number of bags having distinct chocolates to 4 by grouping pair ) So number of bags having exactly two identical chocolates is $ \displaystyle {5 \choose 2} \cdot 10 \cdot 9 \cdot 8 \cdot 7 = 50400$ Similarly number of bags having exactly three identical chocolates $ \displaystyle {5 \choose 3} \cdot 10 \cdot 9 \cdot 8 = 7200$ Similarly number of bags having exactly four identical chocolates $ \displaystyle {5 \choose 3} \cdot 10 \cdot 9 = 450$ Similarly number of bags having exactly five identical chocolates = $ \displaystyle {5 \choose 5} \cdot 10 = 10$ Total $58060$ out of total configuration $10 \cdot 10 \cdot 10 \cdot 10 \cdot 10 =100000$ So answer becomes $ \dfrac {58060}{100000} =0.5806$ I know I am missing something but what can you please help","['permutations', 'combinations', 'discrete-mathematics', 'probability']"
4293202,Prove $\phi:(\mathbb{Z}/n\mathbb{Z})^{\times}\to\text{Aut}(\mathbb{Z}_n)$ such that $[x]\mapsto \phi_{[x]}([t])=x[t]$ is an isomorphism,"I want to prove $(\mathbb{Z}/n\mathbb{Z})^{\times}\cong \text{Aut}(\mathbb{Z}_n)$ so (1) I proved $f_s:G\to G $ such that $g\mapsto sg=g+\overset{s}{\dots}+g\,$ is an automorphism on $G $ .  ( $\gcd(s,|G|)=1)$ (2) I proved $f:\mathbb{Z}_n\to\mathbb{Z}_n$ is an automorphis on $\mathbb{Z}_n \Leftrightarrow f([1])\in (\mathbb{Z}/n\mathbb{Z})^{\times}$ Finally I  made $\phi:(\mathbb{Z}/n\mathbb{Z})^{\times}\to\text{Aut}(\mathbb{Z}_n)$ such that $[x]\mapsto \phi_{[x]}([t])=x[t]$ so I must prove $\phi $ is an isomorphism. The $\gcd(|G|,x)=1$ because $x\in(\mathbb{Z}/n\mathbb{Z})^{\times}$ and by (1) $\text{Im}\,\phi\in \text{Aut}(\mathbb{Z}_n)$ Then I proved $\phi $ is an homomorphism, let $g,h\in(\mathbb{Z}/n\mathbb{Z})^{\times}:$ $$\phi_{[g][h]}([t])=(\phi_{[g]}\circ\phi_{[h]})([t])$$ Now I should prove $\phi $ is injective, for $g,h\in (\mathbb{Z}/n\mathbb{Z})^{\times}$ such that $\phi_{[g]}([t])=\phi_{[h]}([t])$ $$g[t]=h[t]$$ $$\vdots$$ $$g=t?$$ I am not sure how prove it ..I think maybe if I multiply both sides by $[t^{-1}]$ I could get the result? $\phi$ is surjective because if $\varphi\in \text{Aut}(\mathbb{Z}_n)$ then by (2) exists $g=\varphi([1])\in (\mathbb{Z}/n\mathbb{Z})^{\times}$ such that $\phi_g=\varphi $ ? Also I should prove $\phi$ is well-defined? Any hint please? Thank you for your help","['group-homomorphism', 'automorphism-group', 'group-isomorphism', 'finite-groups', 'group-theory']"
4293250,How to write a polar equation for a five-pointed star,"I can't get the formula right. There is a formula for an equilateral polygon I'm trying to translate it into code to draw a five-pointed star Here's how I translated the equation: _mytKrg.integerVector.append(_mytKrg.X + (_mytKrg.R * cos(2*sin(1)+M_PI*3)/(2*5))/((cos(2*sin(cos(1*5*grad_kol_toch)))+M_PI*3)/(2*5))); //point Х
_mytKrg.integerVector.append(_mytKrg.Y + (_mytKrg.R * sin(2*cos(1)+M_PI*3)/(2*5))/((sin(2*cos(sin(1*5*grad_kol_toch)))+M_PI*3)/(2*5))); //point У here is the result : Here's the whole code: zvezda.pro #ifndef KRUG_QT_H
#define KRUG_QT_H

#include <QMainWindow>
#include <QDebug>
#include <QPainter>
#include <QPen>
#include <QFont>
#include <QtMath>
#include <QPointF>

// структура для отрисовки круга
struct strKryg
{
    strKryg()
    {
        R = 0.0; kol_toch = 0;
        tocka_1 = 0; tocka_2 = 0;
        X = 0.0; Y = 0.0;
        integerVector.clear();
        point_for_line.clear();
    }
    float R;                            // радиус
    int kol_toch;                       // колич точек которые будут формировать круг
    int tocka_1;                        // точка выреза начало
    int tocka_2;                        // точка выреза конец
    float X; float Y;                   // расположение икса и игрика

    QVector<float> integerVector;       // вектор куда кладём точки (Х и У)
    QVector<QPointF> point_for_line;    // вектор куда кладём QPointf для line
};

class Krug_qt : public QMainWindow
{
    Q_OBJECT

public:
    Krug_qt(QWidget *parent = 0);
    ~Krug_qt();

    void krygVertexArray();
    void vvodZnach(float R, int kol_toch);
    void vvodZnach(float R, int kol_toch, float X, float Y);
    void vvodZnach(float R, int kol_toch, int tocka_1, int tocka_2, float X, float Y);



protected:
    void paintEvent(QPaintEvent *event);
    strKryg     _mytKrg;    // структура круга

};

#endif // KRUG_QT_H zvezda.cpp #include ""zvezda.h""

Krug_qt::Krug_qt(QWidget *parent)
    : QMainWindow(parent)
{

}

Krug_qt::~Krug_qt()
{

}

void Krug_qt::krygVertexArray()
{
    float grad = 360.0/_mytKrg.kol_toch;
    float grad_kol_toch = 0.0;
    // градусы*M_PI/180 = радианы
    int j;
    for (j = 0; j < _mytKrg.kol_toch; ++j){

        _mytKrg.integerVector.append(_mytKrg.X + (_mytKrg.R * cos(2*sin(1)+M_PI*3)/(2*5))/((cos(2*sin(cos(1*5*grad_kol_toch)))+M_PI*3)/(2*5)));
        _mytKrg.integerVector.append(_mytKrg.Y + (_mytKrg.R * sin(2*cos(1)+M_PI*3)/(2*5))/((sin(2*cos(sin(1*5*grad_kol_toch)))+M_PI*3)/(2*5)));

         grad_kol_toch = grad_kol_toch + grad;
    }
}
void Krug_qt::vvodZnach(float R, int kol_toch)
{
    _mytKrg.R = R;
    _mytKrg.kol_toch = kol_toch;
    _mytKrg.tocka_1 = 0;
    _mytKrg.tocka_2 = 0;
    _mytKrg.X = 0.0;
    _mytKrg.Y = 0.0;
}

void Krug_qt::vvodZnach(float R, int kol_toch, float X, float Y)
{
    _mytKrg.R = R;
    _mytKrg.kol_toch = kol_toch;
    _mytKrg.tocka_1 = 0;
    _mytKrg.tocka_2 = 0;
    _mytKrg.X = X;
    _mytKrg.Y = Y;
}

void Krug_qt::vvodZnach(float R, int kol_toch, int tocka_1, int tocka_2, float X, float Y)
{
    _mytKrg.R = R;
    _mytKrg.kol_toch = kol_toch;
    _mytKrg.tocka_1 = tocka_1;
    _mytKrg.tocka_2 = tocka_2;
    _mytKrg.X = 0.0;
    _mytKrg.Y = 0.0;
}

void Krug_qt::paintEvent(QPaintEvent *event)
{
    QPainter painter(this);                                   // Создаём объект отрисовщика
    QPen pen_abris(Qt::black, 2, Qt::SolidLine, Qt::FlatCap); // кисть обрисовки (компаса)
    painter.setRenderHint(QPainter::Antialiasing);            // убираем резкие кубики
    painter.setPen(pen_abris);                                // Устанавливаем кисть обрисовки
     vvodZnach(200, 180, 600, 400);
    krygVertexArray();                                        // Набираем массив
 
 
    int i; bool d_t;  int tocka_1; int tocka_2;            // используемые переменные
    d_t = true; i = 0; tocka_1 =_mytKrg.tocka_1; tocka_2 = _mytKrg.tocka_2;  // ТОЧКА ОТРИСОВКИ
    for( ; i<_mytKrg.kol_toch; ++i)
    {
        if(i > tocka_1)          // угол ""вырезания""  начало
        {
            d_t =  false;
            if( i < tocka_2)      // угол ""вырезания"" конец
            {
                tocka_1 = i*2;
                d_t = true;
            }
        }
        if(d_t == true)
        {
            // это отдельно все работает полноценный круг
            tocka_2 = tocka_1+1;
            painter.drawPoint(QPointF(_mytKrg.integerVector[tocka_1],_mytKrg.integerVector[tocka_2]));
            tocka_1 = tocka_1 +2;

            qDebug() << ""i :""<<i;
            qDebug() << ""_mytKrg.tocka_1 :""<<_mytKrg.tocka_1;
         }
    }
} alas, I can't write on stackoverflow, for some reason they called me maybe because I'm a Russian-speaking man I'm wondering if it's possible to draw with formulas ?? are there any books or training sites  ??? According to the advice of @Homieomorphism. I made changes to the code, but so far I still do not understand how to convert polar coordinates to Cartesian coordinates. float grad = 360.0/_mytKrg.kol_toch;
float grad_kol_toch = 0.0;
// градусы*M_PI/180 = радианы
int j; float chisl; float znam; float k = 1; float n =5; float m =3;
for (j = 0; j < _mytKrg.kol_toch; ++j)
{
    chisl = cos((2*asin(k)+M_PI/180*m)/(2*n));
    znam  = cos((2*asin(k*cos(n*grad_kol_toch)+M_PI/180))/(2*n));

    _mytKrg.integerVector.append(_mytKrg.X + (_mytKrg.R * (chisl/znam) * cos(grad_kol_toch*M_PI/180.0) ));
    _mytKrg.integerVector.append(_mytKrg.Y + (_mytKrg.R * (chisl/znam) * sin(grad_kol_toch*M_PI/180.0) ));

    grad_kol_toch = grad_kol_toch + grad;
}","['trigonometry', 'geometry', 'polar-coordinates']"
4293284,Represent statements with the help of quantifiers,"I need to represent the following statements using quantifiers in this task: (a) For any real number $x$ , if $x$ is rational, then also $\sqrt{x}$ . (b) For some natural numbers $n \geq 3$ and integers $x, y$ and $z$ , $x^{n} \cdot y^{n} = z^{n}$ holds. Are these correct: (a) $\quad \forall x \in \mathbb{R} : x\in\mathbb{Q}\Rightarrow\sqrt{x} \in \mathbb{Q}$ (b) $\quad \exists n \in \mathbb{N} \quad \land \quad \exists x,y,z \in \mathbb{Z} \quad , n \geq 3 :\quad  x^{n} \cdot y^{n} = z^{n}$ And in the second step we have to negate the above statements. Are these correct: (a) $\quad \lnot(\forall x \in \mathbb{R}: x\in\mathbb{Q}\Rightarrow\sqrt{x} \in \mathbb{Q}) \Leftrightarrow \exists x \in \mathbb{R} : x\in\mathbb{Q} \quad \land \quad \sqrt{x} \notin \mathbb{Q}$ (b) $ \quad \lnot(\exists n \in \mathbb{N} \quad \land \quad \exists x,y,z \in \mathbb{Z} \quad , n \geq 3 :\quad  x^{n} \cdot y^{n} = z^{n}) \Leftrightarrow \forall n \in \mathbb{N} \quad \land \quad \forall x,y,z \in \mathbb{Z}\quad , n \geq 3 : \quad  x^{n} \cdot y^{n} \neq z^{n}$","['quantifiers', 'predicate-logic', 'logic', 'discrete-mathematics']"
4293301,What is the logical reasoning behind the substitution of trigonometric functions for simplifying an inverse trigonometric expression?,"Several google searches in regards to this only led me to results about trigonometric substitution in regards to integrals, not to my specific question. My education board's math book and several solution sites for my book, and my school and tutors all seem to use this rather strange method for many questions which I am struggling to intuit For example, a question like the below: From what I understand, you take an expression that just so happens to be similar in form to a previously taught trigonometric expression, and substitute a trigonometric function in it that just so happens to work for simplification. That is where I am confused. I have asked my tutor, and a few people I know and still am struggling to understand this It seems arbitrary. How can we say we 'simplified' the equation and got a simpler general form if what we did involved us assuming x was a very specific trigonometric function, such that substituting any other function would NOT work? To me it is like saying $(x+3x^2) + 2$ can be simplified to $0$ , because $x + 3x^2$ always evaluates to some very special function $h(x)$ , which can be represented in the special form $x + 3x^2$ to evaluate to $-2$ in all cases. This very method seems like it would work for only a specific 'branch' right? How can one produce a general solution this way? How does it logically work?","['trigonometry', 'inverse-function']"
4293303,Why this function is analytic at $x=0$?,"In example 1 of Differential Equations 9th edition by Nagle page 436, we are asked to find the singular points for the second-order linear differential equation $$xy''+\frac{x}{(x+1)}y'+(\sin x)y=0$$ Dividing the equation by $x$ , we have the coefficient of the middle term is $p(x) = \frac{x}{x(x+1)}$ .  Clearly $p(x)$ is not defined at x=0, however, he states that $p(x)$ is indeed analytic at $x=0$ since we can cancel the $x$ in the numerator and denominator and this new function $ \frac{1}{x+1}$ does have a power series expansion around $x=0$ .  He references the following footnote: †Such points are called removable singularities. In this chapter we assume in such cases that the function has been defined (or redefined) so that it is analytic at the point. I'd like to have more explanation on why we can make this assumption. Does it have something to do with... changing one point will not change the solution to a differential equation? Why not? I'm also not sure why he uses the phrase ""In this chapter..."" and ""...in such cases.."" in the footnote. Does the rule not apply in other cases? Maybe not for nonlinear equations?  Why not?  When is the assumption not valid? It's possible the answer to these questions is found further along in the textbook, but I'm looking for some explanation now if possible as I'm having trouble wrapping my mind around what is meant by a function being analytic at a point. Thank you in advance for any help. *If I can learn the answers to the questions above, I should be able to understand why the coefficient of the last term $q(x) = \frac{\sin(x)}{x}$ is analytic at $x=0$ as well.  It's the same idea I believe.","['singularity', 'ordinary-differential-equations']"
4293305,"Basic properties of the Glasser function $G(x)=\int_0^x \sin(t\sin t)\,{\rm d}t$","This question brought to my attention the Glasser function $G(x)=\int_0^x \sin(t\sin t)\,\mathrm{d}t$ . Surely there are many variations and generalizations we could look into, but this one seems to have a name already so it's a nice place to start. The MathWorld article says this is problem 785 listed in a 1990 volume of Nieuw Archief voor Wiskunde (New Archive for Mathematics), which is probably only available physically overseas. It's also listed as problem 12767 in Genautica's compilation of 20,000 math problems, with a defunct hyperlink to umr.edu. Anyway, look at the graph. I'd classify the crests/valleys (i.e. local maxima and minima resp.) into two types: major and minor. A major valley always immediately precedes a major crest. There are zero minor crests or valleys between the first and second major crests, one between the second and third major crest, two between the third and fourth major crests, and so on. According to the MathWorld article: This valley-crest pattern holds indefinitely. We have the asymptotic $G(x)\sim 2\sqrt{x/\pi}$ Question . How do we justify these two properties? The extrema occur where $G'(x)=\sin(x\sin x)=0$ . I made a table of all solutions in the interval $[0,20]$ , labelled the extrema (MC = major crest, MV = major valley, mc = minor crest, mv = minor valley), and tagged each $x$ with the integer value of $k$ in $x\sin x=k\pi$ . The first thing that jumped out to me is how the values where $k=0$ are not the major extrema (which was my first guess before I checked out the second's $x$ coordinate), but rather just before the middle of the minor extrema! Indeed, from left to right, the value of $k$ (as a function of $x$ ) bounces down and up and down and up and so on, exactly one more each time. Dunno how to prove any of this, though. I'm not actually sure how to analytically even define what's a major extrema vs. a minor one. Indeed, an asymptotic series would be even more welcome than just the asymptotic. The only situation I've encountered for such asymptotic series involved judicious choices of integration-by-parts, but I have no idea what kind of parts to use for such a strange integrand. Also interesting is the constant $\sqrt{\pi}/2$ is the Gaussian integral $\int_0^\infty e^{-t^2}\,\mathrm{d}t$ . Relevant? From the graph (especially extending out to to $x=60$ or beyond), it seems like we could write $G(x)=g(x)+\phi(x)+\psi(x)$ , where: $g(x)$ is an increasing step function (like the prime counting function) whose jumps occur between adjacent major valley/crests and whose level heights are about the midpoint of the sequence of minor valleys/crests between major ones; $\phi(x)$ is oscillatory, with shrinking amplitude but otherwise about constant frequency, representing the major valleys/crests; and $\psi(x)$ is oscillatory with shrinking amplitude and linearly increasing frequency (so to speak), representing the minor valleys/crests.","['definite-integrals', 'special-functions', 'fourier-analysis', 'asymptotics', 'real-analysis']"
4293319,"Application of unexpected theorem ""all closed manifolds are a quotient of $\Bbb B^n$""!","Recently just accidentally I read a corollary in Lee's Introduction to Riemannian manifolds which surprised me: Corollary 10.35 ( LEE- IRM ). Every compact, connected, smooth $n$ -manifold is homeomorphic
to a quotient space of $\overline{\Bbb B}^n$ by an equivalence relation that identifies only points on
the boundary. To me this result is so much well-set and less in assumptions and restrictions. But has this result any application? I mean how this can help in differential geometry theorems as well as its face shows that it is so powerful? My second questions is this: Can one produce all closed smooth $n$ -manifolds by equivalence relations on $\partial \overline{\Bbb B}^n\simeq \Bbb S^{n-1}$ that come from some group actions? I mean can one represent or identify that equivalence relations by some (topological) group actions?","['differential-topology', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4293327,Fractional Part integral $I=\int_{0}^{1}\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}x\mathrm{d}y$,"Let $$I=\int_{0}^{1}\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}x\mathrm{d}y.$$ When I tried computing the integral I seem to be getting a different answer to Wolramaplha, and can't find a similar integral anywhere on MSE or the internet. Here's how I did it $$\int_{0}^{1}\left(\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}y\right)\mathrm{d}x$$ , Lets evaluate, (Note: $0<x<1$ ) $$I_1=\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}y =\int_{1}^{\infty}\frac{xy-\lfloor xy \rfloor  }{y^2}\mathrm{d}y=x(1-\gamma)$$ Therefore $I=\frac{(1-\gamma)}{2}$ . However Wolfram gives $I= 0.458868$ . Can someone help, or if my answer is wrong then provide a solution.",['analysis']
4293359,"Find the function(s) $f:\mathbb{R}\setminus\{0\}\to\mathbb{R}\setminus\{1\}$, $f(xy)=f(x)f(-y)-f(x)+f(y)$, $f(f(x))=\frac1{f\left(\frac1x\right)}$","Find all the functions $f:\mathbb{R}\setminus\{0\} \to \mathbb{R}\setminus\{1\}$ which satisfy the two conditions: $f(xy)=f(x)f(-y)-f(x)+f(y)$ $\text{for } \forall x, y \in \mathbb R\setminus\{0\}$ $f(f(x))=\dfrac 1 {f \left( \frac 1 x \right)} \text{ for } \forall x \in \mathbb R\setminus\{0, 1\} $ My expectation of the function $f$ is $f(x)=1-\dfrac 1 x$ My attempt: \begin{align} 
&\text{if } f \equiv 0: \text{Solution}. \\ 
&\text{if } f \not\equiv 0: \\ 
\ \\ 
&P(x, x): f(x^2)=f(x)f(-x). \\ 
&x=1; \\ & f(1)=f(1)f(-1). \\ 
&\text{if } f(1) \neq 0: \\ 
&f(-1)=1 \not\in \mathbb{R}\setminus\{1\}. \\ 
&\therefore f(1)=0. \\ \ \\ 
&\text{let } t \text{ s.t. } f(t) \neq 0. \\ \ \\ 
&P(t, 1): f(t)=f(t)f(-1)-f(t)+f(1) \\ 
&\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =f(t)f(-1)-f(t). \\ 
&\Rightarrow f(-1)=2. \\
\ \\
&f(f(-1))=\dfrac {1} {f(-1)}. \\
&\therefore f(2)= \dfrac 1 2. \\
\ \\
& f(f(2))= \frac 1 {f \Big(\dfrac {1} {2} \Big)}. \Rightarrow f \Big( \dfrac{1}{2} \Big) = \pm 1. \Rightarrow f \Big( \dfrac 1 2 \Big) = -1.
\ \\
& P(-1, -\frac 1 2): f \Big( \frac 1 2 \Big) = f(-1)f\Big( \frac 1 2 \Big) - f(-1)+f\Big(-\frac 1 2 \Big). \\
&\Rightarrow -1=2 \cdot (-1)-2+f\Big(-\frac 1 2 \Big). \\
&\therefore f\Big( -\frac 1 2 \Big)=3. \\
\ \\
&P(x, 1): f(-x)=f(x)f(1)-f(x)+f(-1) = -f(x)+2. \\
&\therefore f(x)+f(-x)=2. \\
\ \\
&\text{Substituting for the first original F.E.: } f(xy)-1=(f(x)-1)(f(y)-1). \\
\ \\
&g(x) = f(x)-1. \\
&\Rightarrow -g(xy)=g(x)g(y). \\
&g(g(x)+1)+1= \dfrac 1 {g(\frac 1 x)+1}. \\
&\therefore g(g(x)+1)=-\dfrac {g(\frac 1 x)} {g(\frac 1 x)+1}.
\end{align}","['functional-equations', 'algebra-precalculus', 'functions']"
4293390,Finding eigenvalues of a matrix to show that the forward Euler discretization for the method of lines is stable,"So here's the background on the problem at large, it won't be useful I think for what I'm asking help on since the formulation has nothing to do with finding eigenvalues. But it might be useful in giving clarity to the situation. So consider applying the method of lines to the heat equation $u_t=au_{xx}$ with initial conditions $u(0,x)=g(x),\, 0\leq x\leq 1$ and boundary conditions $u(t,0)=u(t,1)=0,\, t\geq 0$ . For simplicity, assume $\Delta x=\frac{1}{m+1}$ and let $y_i(t)$ approximate $u(x_i,t)$ where $x_i=i\Delta x,\, i=0,1,...,m+1$ . Then using second-order difference scheme to approximate the heat equation we arrive at $\frac{dy_i}{dt}=\frac{a(y_{i+1}-2y_i+y_{i-1})}{(\Delta x)^2},\, i=1,...,m$ with $y_0(t)=0$ and $y_{m+1}(t)=0$ . So now we get a system of ODE's with initial data $\vec{y}'=A\vec{y}$ where $A=\frac{a}{(\Delta x)^2}\begin{bmatrix} -2&1&0&.&.&0 \\ 1&-2&1&.&.&.\\ 0&1&-2&1&.&.\\.&.&.&.&.&. \\.&.&.&1&-2&1\\0&.&.&.&1&-2  \end{bmatrix}$ (Sorry if my matrix looks half-assed, but this is supposed to represent a symmetric block diagonal matrix) FYI: The component form for $\vec{y}'$ and $\vec{y}$ are trivial from numerical scheme. Now, for this next part we do a bit hand wavy since the focus isn't so much solving ODE's or PDE's the natural way like using separation of variables; we are focusing more on the numerical methods for them, so we suppose we are given the eigenfunctions beforehand, and they are, $\vec{v}^k=(v_1^k,v_2^k,...,v_m^k)^T=(\sin(k\pi\Delta x),\sin(2k\pi\Delta x),...,\sin(mk\pi\Delta x))^T,\, 1\leq k\leq m$ , or in simplified form $v_i^k=\sin(ik\pi\Delta x),\, i=1,...,m$ Now to my first question, how do I derive the eigenvalues $\lambda_k$ without making a huge mess making busy calculation? cause if it does involve that then I'd get on it and won't need your help on this part, but providing a more simple way of doing this that involves ideas discussed in numerical schemes for these equations is what I should be looking to do. But lets say we carried out this brute force calculation of the eigenvalues the natural way, wouldn't this lead to different eigenvalues for each row since they all have different trig functions? I probably could've asked this question without much of the PDE and numerical method jargon used to derive the matrix, but I wanted to show that I actually know what I'm trying to do. If you can provide me with the eigenvalue that would be all that is needed, I can figure out how to get there from what I have given you so far, unless it is super trivial and is a one-line calculation. Now, to my second question, how would I use the eigenvalues of A to show that the forward Euler discretization of the method of lines is stable if $h<\frac{1}{2a}(\Delta x)^2$ Why would we need a stability restriction for the time step for the forward Euler but not the backward Euler(Crank-Nicolson method)? Someone asked this similar problem but solutions were insufficient and didn't provide enough clarity of the concepts being discussed Method of Lines Diffusion Problem","['ordinary-differential-equations', 'numerical-optimization', 'partial-differential-equations', 'numerical-linear-algebra', 'numerical-methods']"
4293416,Evaluate: $I(\alpha)=\int_{0}^{\infty} \frac{\arctan\left ( \alpha (x-\operatorname{arsinh} x) \right ) }{x\sqrt{1+x^2} }\text{d}x$,"I am interested this type integrals. Let $$
I(\alpha):=\int_{0}^{\infty} \frac{\arctan\left ( \alpha 
(x-\operatorname{arsinh} x) \right ) }{x\sqrt{1+x^2} }\text{d}x.
$$ For example, some simple calculations showed that $$
I\left ( \frac{1}{\pi}  \right )=-\pi\ln\left ( \frac{\sqrt{\pi} }{2}  \right ).
$$ My Question is : Are there any extended results about $I(\alpha)$ ? Any suggestion would be appreciated.","['integration', 'calculus', 'contour-integration', 'definite-integrals']"
4293452,$\varepsilon$-chain property for the inverse function,"I'm trying to solve a problem of Lan Wen's differentiable Dynamical Systems book. Before writing the problem I introduce some notions in here: Denote by $X$ a compact metric space and by $f:X \to X$ a homeomorphism. By an $\varepsilon$ -chain we mean a sequence $x_0 , \cdots , x_n$ which satisfy $d(f(x_i),x_{i+1}) < \varepsilon$ for $ 0 \leq i \leq n-1$ Here is the problem: Prove that for any $\delta >0 $ there is $ \eta>0$ such that if $ x_0 ,\cdots , x_k$ is an $\eta$ -chain of $f$ , then $x_k , \cdots, x_0$ is a $\delta$ -chain of $f^{-1}$ . My try : I believe that here we could use the fact that $ f$ is uniformly continuous on $X$ and we could consider $0<\eta<\delta$ and for any $x,y \in X$ if $d(x,y) < \eta$ then $d(f(x) , f(y))<\varepsilon$ . considering this if we suppose $x_0 , \cdots , x_k$ is an $\eta$ -chain then we need to prove $x_k, \cdots , x_0$ is a $\delta$ -chain under $f^{-1}$ . Is this idea a true way to solve this problem or I need to think differently?","['general-topology', 'analysis', 'dynamical-systems']"
4293522,Showing that a stopping time is finite for a biased random walk.,"If we consider $X_i$ iid with $\mathbb{P}(X_i=1) = p$ and $\mathbb{P}(X_i=-1)=1-p$ . Where $p \in (1/2,1)$ . The random walk is then given by, $$S_n=\sum_{i=1}^n X_i. $$ We also define the stopping time $\tau = \inf\{k:S_k \in \{-\alpha,\beta\} \}$ with $\alpha,\beta>0$ in the natural numbers.
How to prove that $P(\tau<\infty)=1$ ? Intuitively it's clear that for sure at some point $S_n$ will hit $\beta$ (because the random walk has a tendincy to move upwards.). I was thinking about showing that $P(S_n=\infty \text{ i.o.})=1$ . But I'm not really sure how to make a rigorous argument.
Could someone help me with this?","['martingales', 'stopping-times', 'random-walk', 'probability']"
4293542,How many more men than women were there in the beginning?,"Let men= m , women= w , children= c From the statement, m=1.25w 0.8w=c Children left resulted in equal number of men and women, women=122 at the end. Can I assumed that the total number of people is 122+122=244 ? m+w+c=244 1.25w+w+0.8w=244 3.05w=244 w=80 m=100 The number of men more than women m-w=100-80=20 Is it the correct way to solve this problem?",['algebra-precalculus']
4293551,A space curve consisting of a spiral wound around a helix. (Slinky curve.),"I'm trying to use the parametric equations for the sinky curve to construct a meshable model for 3D magneto-static FEA of a transformer coil. I have the modellling and meshing covered but, I need to intertwine two conductors (twisted pair) and whilst I have  gotton close by trial and error, I've yet to get the two conductors to intertwine closely without intersecting.
(The red areas in the image below show the slight intersection between them.) So my question is, can a mathematician or two, help me avoid that intersection? The equations used are: x = ([r]+[a] Cos([w] [t]))*Cos([t]) y = ([r]+[a] Cos([w] [t]))*Sin([t]) z = [h]*[t]+[a] Sin([w] [t]) Where: a = diameter of the spiral = 0.25 h = the slope angle of the helix = 0.08 (1 translates to 2pi in the z per revolution) r = the radius of the helix - 3.95 w = the number of turns of the spiral per revolution of the helix = 4 t runs from 0 to 14 pi. These parameters give the curve below, and second copy rotated 45° gives the intertwine: The software that generates the model and detects the interferance is using math to do so; but it is closed source. Can a mathematician show me how to adjust the parameters I am using to avoid the interferance? I realise that this is couched in 'engineering terms', but I am an engineer. I've provided as much of the math as I understand; can you nudge me the rest of the way? Buk Update: Thanks to Jyrki's assistance below I've now produced my model of the transformer coil:",['geometry']
4293553,"Local flatness criterion: A morphism $X \to Y$ of schemes over $S$ is flat if and only if it is flat on all fibers $X_s \to Y_s$, $s \in S$?","In Nitsure's Part 2. Construction of Hilbert and Quot schemes in Fundamental Algebraic Geometry , there is the following Lemma Lemma 5.21. (3) Let $S$ be a noetherian scheme, and let $f: X \to S$ and $g: Y \to S$ be finite type flat morphisms. Let $\pi: Y \to X$ be any morphism such that $g = f \circ \pi$ . Let $y \in Y$ , let $x = \pi(y)$ , and let $s = g(y) = f(x)$ . If the restricted morphism $\pi_s: Y_s \to X_s$ between the fibers over $s$ is flat at $y \in Y_s$ , then $\pi$ is flat at $y \in Y$ . He notes that this is a consequence of the local criterion for flatness. So one version of the local criterion for flatness is the following (from Matsumura, Commutative ring theory , Thm 22.3) If $A$ is a noetherian ring and $M$ is an $I$ -adically ideal-separated module, then the following conditions are equivalent. $M$ is flat over $A$ ; $M/IM$ is flat over $A/I$ and $\operatorname{Tor}_1^A(A/I, M) = 0$ ; $I$ -adically ideal-separated means that for any ideal $\mathfrak a \subset A$ , the module $\mathfrak a \otimes_A M$ is $I$ -adically separated. For example if $M$ is a finite $A$ -module this is always satisfied, right? Since we want to know if $\mathcal O_{X, x} \to \mathcal O_{Y, y}$ is flat, I think we want to apply the local criterion for flatness to $A = \mathcal O_{X, x}$ and $M = \mathcal O_{Y, y}$ . Since we already know that $Y_s \to X_s$ is flat, we know that $\mathcal O_{X,x} / \mathfrak m_s \mathcal O_{X, x} \to \mathcal O_{Y, y} / \mathfrak m_s \mathcal O_{Y, y}$ is flat, so I guess $I = \mathfrak m_s \mathcal O_{X, x}$ is the correct choice. Then $\mathcal O_{Y, y}$ is $I$ -adically ideal-separated, because any ideal $\mathfrak a \subset \mathcal O_{X, x}$ is finitely generated, so $\mathfrak a \otimes_A \mathcal O_{Y, y}$ is a quotient of finitely many copies of $\mathcal O_{Y, y}$ and as $\mathcal O_{Y, y}$ is noetherian, the quotient is also $I$ -adically separated. But I don't know how to show $$\operatorname{Tor}^A_1(A/I, M) = 0.$$ I'm aware that I didn't yet use the flatness of $X \to S$ and $Y \to S$ , maybe that is important here? Any help would be appreciated :)","['algebraic-geometry', 'flatness', 'commutative-algebra']"
4293587,"Find $\angle CAD$ if $\triangle ABC$ is right angled at $B$, $\angle BAD = 30^\circ, \angle ADB = \angle ADC = 15^\circ$","Find angle $\theta$ in the below diagram. This is a question that was brought to me by a high school student. While I came up with a trigonometric solution and a synthetic solution, I am posting here to see more solutions that others come up with (esp. other synthetic solutions) My immediate solution involved combination of a simple construction and trigonometry, basically knowing that $\tan 30^\circ = \dfrac{1}{\sqrt3}$ and $\tan 15^\circ = 2 - \sqrt3$ . We draw perp from $D$ to $AB$ extend and $BC$ extend. We also note that $\angle DBC = \angle DBE = 45^\circ$ . If $AB = x, BE = y$ , we find $x$ in terms of $y$ . We next find $CF$ in terms of $y$ and subtracting from $y$ gives us $BC$ and we show $BC = x$ . Then in search of a synthetic solution, I drew a few more lines and as $FE$ is perpendicular bisector of $BD$ , $BG = GD, BH = HD$ . So we see that $AB = BG$ and by A-A-S, $\triangle BHC \cong \triangle BHG$ which leads to $AB = BC$ and we have $\theta = 15^\circ$ . Look forward to more interesting solutions.","['euclidean-geometry', 'trigonometry', 'geometric-transformation', 'geometry']"
4293604,An $\ell_p$ inequality for discrete martingales,"If $Y_n=\sum_{i=1}^n X_i$ is a martingale, where $X_i$ is a martingale difference sequence, $\mathbb{E}[X_n\mid \mathcal{F}_{n-1}]=0$ for all $n$ , we know that $$ \mathbb{E}\big[Y_n^2-Y_{n-1}^2\big]=\mathbb{E}X_n^2,$$ by using the simple fact that $Y_n^2=Y_{n-1}^2+2X_nY_{n-1}+X_n^2$ , where the cross term vanishes since $Y_{n-1}$ is $\mathcal F_{n-1}$ measurable and $X_i$ centred, that is, $$ \mathbb{E}[X_nY_{n-1}]=\mathbb{E}\big[Y_{n-1}\mathbb{E}[X_n\mid \mathcal{F}_{n-1}]\big] =0.$$ A similar property, but now as an inequality, holds if we replace the square with the absolute value, $$ \mathbb{E}\big[|Y_n|-|Y_{n-1}|\big]\le\mathbb{E}|X_n|.$$ Does something analogous hold for other powers? Namely, something along the lines of $$ \mathbb{E}\big[|Y_n|^r-|Y_{n-1}|^r\big]\le C\mathbb{E}|X_n|^r,$$ for $1<r<2$ and some $C>0$ ?","['stochastic-processes', 'probability-theory', 'martingales']"
4293610,Aut$(V)$ open subset of End$(V)$,"I have read in a book that Aut $(V)$ (set of all invertible linear maps) is an open subset of the End $(V)$ (set of all linear maps), where $V$ is a vector space. Can someone explain under what topology(ies) it is the case?","['general-topology', 'vector-spaces']"
4293703,Does a random variable with differentiable distribution function have density?,"Question: Suppose that $X: \Omega \to \mathbb{R}$ is a random variable and it's distribution function $F(x) = \mathbf{P}(\xi \le x)$ is differentiable for all $x$ . Is it true that $F'(x)$ is density? What do I know: Remark 1 . If $F'$ is continuous then $F'$ is density - see, e.g., A random variable $X$ with differentiable distribution function has a density Hence if there's counterexample $F$ then $F'$ is not continuous everywhere. Remark 2 . Absolutely continuous measures on $\mathbb{R}$ are precisely those that have densities. And if $g$ is differentiable everywhere then it doesn't follow that $g$ is absolutely continuous. Does the everywhere differentiability of $f$ imply it is absolutely continuous on a compact interval? Unfortunately, the example from the link doesn't help. Remark 3 (close to remark 2) . In order to make a counterexample it's sufficient to find $F$ such that $F'$ is not intergrable. It's easy to find differentiable function with nonintergable derivative. Consider $g(x) = x^2 \sin(\frac{1}{x^2})$ is differentiable for all $x$ and $\int_{0}^1 g(x) dx$ doesn't exist. Unfortunatelly, $g(x)$ is not a counterexample in our problem, because it's not monotone and hence it's not a distribution function. A function $(g(x) + 1000 x) \cdot const$ also doesn't ""work"". Addition: I found the similar question here https://math.stackexchange.com/questions/3387905/derivation-of-distribution-function-is-density-function/3387913 but there're no proofs there, so in fact there's still no answer. Addition2: If $F'$ exists for all $x$ then it doesn't follow that $F'$ is continious. Counterexample: we may consider $\tilde{F}(x) = \frac{g(x) - g(-5)}{g(5)}I_{|x|\le 5} + I_{x > 5}$ where $g(x) = x^2 \sin (\frac{1}x) + 5x + 10$ and make $F(x)$ which is smooth, nondecreasing and which coinsides with $F$ for all $x \in \mathbb{R} \backslash (U_{\frac1{100}}(-5) \cup U_{\frac1{100}}(5))$ .","['measure-theory', 'real-analysis', 'probability-theory', 'probability', 'density-function']"
4293749,Question about showing the equation defines an implicit function and give the Taylor expansion,"I have a question while doing the exercise, I want to be sure if my step is correct, the exercise is: Show that the equation $$x^3 - y^3 -3xy + 1 = 0$$ defines an implicit function $\phi:x \to y$ in a neighbourhood of $0$ such that $\phi (0) =1$ . Give the third order Taylor expansion of $\phi$ in this neighbourhood of $0$ . Here is my attempt: I have first calculated the derivative of $\frac{dF}{dx}$ and $\frac{dF}{dy}$ , and then find $dF=(3x^2-3y  \ \ \ -3y^2-3x)$ , then we have $det(\frac{dF}{dy})(0)=(-3y^2-3x)(0)=0$ but $\phi(0)=1 \ne 0$ now we have $$\frac{d\phi}{dy} = -[\frac{dF}{dy}(x,\phi(x)]^{-1}[\frac{dF}{dx}(x,\phi(x)]=\frac{3x^2-3y}{3y^2-3y}$$ Then Taylor expansion for $\phi(x)$ at $0$ is: $$\phi(0) + \phi'(0)(x) +\frac{\phi''(0)(x^2)}{2}+\frac{\phi'''(0)(x^3)}{6}+ ...$$ which gives me: $$1+0+0+0+...=1$$ I doubt my step is not correct, can somebody helps me to check, thank you very much in advance.","['multivariable-calculus', 'taylor-expansion', 'implicit-function-theorem']"
4293808,"How to understand the operation ""choose a random subset"" in combinatorics?","Hello I'm reading Tao and Vu's book additive combinatorics ，and I can not fully convince myself to believe the proof of Theorem 1.13 . In the proof, they constructed a set by the following way(It seems that I'm not allowed to upload picture now, so I type it down) Define a set $B\subset\mathbb{Z}^+$ randomly by requiring the events $n\in B$ (for $n\in \mathbb{Z}^+$ ) to be jointly independent with probability $\textbf{P}(n\in B)=\min\bigg(C\sqrt{\frac{\log n}{n}},1\bigg)$ , where $C$ is a large constant to be chosen later. Since I've not seen such a method before, I have several questions: If one talks about randomness , then there should be a probability space. In the proof they choose a set $B$ randomly , what is the probability space $(\Omega,\mathcal{F},\mathbb{P})$ here? Relating to the first question, how could I require that events $\{n\in B\}_{n\in\mathbb{Z}^+}$ to be jointly independent? Why could I require that events $\{n\in B\}_{n\in\mathbb{Z}^+}$ have the assigned probability? Any comments should be helpful!
[1]: https://i.sstatic.net/AWZN5.png","['measure-theory', 'additive-combinatorics', 'combinatorics', 'discrete-mathematics', 'probability']"
4293869,Mean Value Theorem on a Closed Set,"For mean value theorem on the real line, we consider a closed interval $[a,b]$ : Let $f:[a,b] \to \mathbb{R}$ be a continuous function on the closed interval $[a,b]$ , and differentiable on the open interval $(a,b)$ . Then there exists some $c$ in $(a,b)$ such that $$f'(c)=\frac {f(b)-f(a)}{b-a}.$$ However, for the generalization to higher dimensional case (Mean value theorem in several variables) there are multiple sources that define $f$ on an open set. For example, this is adopted from Wikipedia: Let $G$ be an open convex subset of $\mathbb {R}^{n}$ , and let $f:G\to \mathbb {R}$ be a differentiable function. Fix points $x,y\in G$ , and define $g(t)=f((1-t)x+ty)$ . Since $g$ is a differentiable function in one variable, then we can apply mean value theorem. 1- Why is $G$ taken to be an open set? 2- And what would be difference if we change it to any arbitrary convex set? (It maight have boundaries but we can define continuity and differentiability at the boundaries).","['multivariable-calculus', 'calculus', 'real-analysis']"
4293874,Extraneous solution when solving $x^2+x+1=0$ by getting $x^2=1/x$,"Let's assume that we have $$x^2+x+1=0.\tag1$$ Substituting $x=0$ , we get $1=0$ , so $0$ is not a root for the quadratic equation and thus, $x\neq0$ . Therefore, there exists $\frac{1}{x}$ , which we'll multiply by both sides of $(1)$ , giving us $$x+1+\frac{1}{x}=0.$$ We will, then, move $\frac{1}{x}$ to the other side and get $$x+1=-\frac{1}{x}.$$ If we add $x^2$ to both sides and note that $x^2+x+1=0$ , we will have $x^2-\frac{1}{x}=0$ . The real root of this equation is $x=1$ , which is not a root of $(1)$ . I was wondering at which step did I do something that was incorrect and resulted in this supposed root.","['algebra-precalculus', 'problem-solving']"
4293888,A conjecture about non-nilpotent groups,"A finite group, $G$ , is nilpotent if its upper central series terminates (at $i \in \mathbb{N})$ with $Z^i(G)=G$ , where $Z^i(G)$ is it's $i$ -th center which can be described as $\{x \in G\mid \forall y \in G : [x,y]\in Z_{i-1}(G)\}$ where $[x,y]$ is the commutator of $x$ and $y$ in $G$ (in other words, it is the set of elements that commute with one another up to an element of the $(i-1)$ -th center). A finite group, $G$ , is non-nilpotent if it is not nilpotent. In this case, we still have, by finiteness of $G$ that the upper central series will terminate after finitely many steps, but now we have an $i \in \mathbb{N}$ at where $Z^i(G)=Z^{i+1}(G)$ , and yet $Z^i(G)\neq G$ , but rather a proper subgroup of $G$ . In this case, since the upper centers are all characteristic and thus normal, we can consider the quotient group, $Q=G/Z^i(G)$ . This group must be centerless, else we could take the pullback of $Z(Q)$ under the natural projection to obtain $Z^{i+1}(G)\supset  Z^i(G)$ with equality not holding, contradicting our assumption that the upper central series stabilized at $i$ . This gives a rather elegant characterization of non-nilpotent finite groups: A non-nilpotent finite group $G$ can be viewed as an extension of a nilpotent normal subgroup, $U=Z^i(G)$ by a centerless quotient group, $Q$ . In the most extreme case, the central series terminates immediately in the trivial group, and you obtain a centerless $G=Q$ . This shows that the centerless groups are the primitive elements of the class of non-nilpotent finite groups with the rest being constructed from them by extensions of nilpotent groups by centerless groups. A (finite) group, $G$ , is complete if it is centerless and isomorphic to its automorphism group $G\simeq\text{Aut}(G)$ . Conjecture: Suppose we have a non-nilpotent finite group $G$ , whose upper central series terminates at $i>0$ , meaning that $G$ is not centerless, and $Z^i(G)=Z^{i+1}(G)$ . Let $U=Z^i(G)$ , and suppose further that the centerless quotient $Q=G/U$ is complete, meaning $Q\simeq\text{Aut}(Q)$ . Lastly, suppose that the upper central series for $\text{Aut}(G)$ terminates at some $i_1\geq i$ , that is, the upper central series for $\text{Aut}(G)$ is at least as long as the upper central series for $G$ . Note in particularly that $\text{Aut}(G)$ is non-nilpotent, and so has a centerless quotient $Q_1$ . Then the claim is that the centerless quotients are isomorphic, $Q_1\simeq Q$ . I came up with the conjecture based on computer searches in GAP looking at the automorphism series for various small finite groups as far as I could construct them in GAP (many eventually reach very large groups and run into issues with memory or the size of the computation). Looking at the upper central series for each term of the automorphism series and comparing how they changed as you step through the automorphism series lead me to propose that the above conjecture holds in general for finite groups. Naturally a counterexample will be enough to disprove the claim, but I didn't find any looking at over 50 different automorphism series, so it may not be easy to find if it exists. I'm not really sure how to approach trying to prove this either. I don't really know enough about the relationship between a group and its automorphism group to leverage the givens in this conjecture productively. Edit 1: This is a response to David A. Craven's answer:
I took the liberty of constructing the proposed group in GAP for $n=7$ . To do this, I needed to put all three of the $G_i$ 's into permutation groups so that the direct product operation in GAP would also return a permutation group, so that fast algorithms would be available for subsequent calculations. Specifically I used: G1:=SymmetricGroup(7);
G2:=Group((1,2,3,4),(1,2)(3,4)); #This is D8
G3:=Group((1,2,3),(4,5,6),(7,8,9)); #This (Z_3)^3
#That G2 and G3 are as claimed can be checked by IdSmallGroup, is should return (8,3) for G2 and (27,5) for G3.
#Now we can call DirectProduct and the result will be a permutation group
G:=DirectProduct(G1,G2,G3);
#Because this is a permutation group, computation of the upper central series is 'easy' for GAP, trying this without doing the above preparations lead to a computation that was taking minutes and still hadn't finished
U:=UpperCentralSeriesOfGroup(G); From the above I obtained that $G$ has upper central series of length $2$ . With $Z(G)\simeq (54,15)$ , and $Z^2(G)\simeq (216,151)$ (Where (x,y) is the GAP library id for order x, id y.). GAP StructureDescription for U[1] confirms that $Z^2(G)\simeq\mathbb{Z}_3^3\times D_8$ . Next I constructed the quotient and confirmed it was the symmetric group (necessarily $S_7$ by an easy order argument). After a computation that took a few seconds I got the automorphism group, $A1$ , but it was not in the form of a permutation group, so I used NiceMonomorphism and then NiceObject to obtain a permutation group isomorphic to the automorphism group. This allows for fast(er) algorithms to used for computing its upper central series, which I found to be of length $2$ with $Z(A1)\simeq (8,5)$ and $Z^2(A1)\simeq (32,46)$ (again using GAP ids for the groups). The size of $A1$ is 905748480 which is twice that predicted in the answer (Which would be 452874240 if it had the structure given in the answer). I was able to get GAP to give me a structure description for $A1$ , which was $\mathbb{Z}_2^2\times D_8\times S_7\times \text{PSL}(3,3)$ . I suspect the comment that the proposed automorphism group is, in fact, a subgroup of index 2 in the full automorphism group is correct. The quotient $Q1$ is of order 28304640 (and so obviously not isomorphic to $S_7$ ), GAP gives a structure description of $S_7\times \text{PSL}(3,3)$ for this quotient. (So it is still a counter example for $i_1=i$ case).","['automorphism-group', 'gap', 'conjectures', 'finite-groups', 'group-theory']"
4293910,"Optimal value in $\max_{x} \max_{y} f(x,y) = \max_{x,\ y} f(x,y)$","I am trying to understand a few things about sequential and simultaneous optimization in [1] . In this post, it is shown that $$\max_{x} \max_{y} f(x,y) = \max_{x,\ y} f(x,y).\tag{1}$$ Thanks to @Shiv Tavker comment, I understand that in order to get the optimal value $z^*=(x^*, y^*)$ in the RHS of $(1)$ , we have to solve the system $\nabla_z f(z) = 0$ w.r.t. $z$ , where $z = (x,y)$ . In addition, from the LHS of $(1)$ we have, $${x^*}' = \arg \max_x f(x, y)\tag{2}$$ and $${y^*}' = \arg \max_y f({x^*}', y)\tag{3}.$$ Let ${z^*}' = ({x^*}', {y^*}')$ . As far as I understand, I think that given $(1)$ , we can state that ${z^*}' \equiv {z^*}$ . However, I am thinking if there are any cases that ${z^*}' \equiv {z^*}$ does not hold? Could you please someone give some comments or an answer of things are not so simple? Any help is highly appreciated. EDIT1: Let $$\mathcal{f}(x, y) = -\frac{1}{2}\:\mathbf{z}^T \left(\mathbf{A} + \frac{xy}{2} \:\mathbf{I}\right)^{-1} \mathbf{z} - \frac{y}{6} \lambda - \frac{y}{12}x^3,$$ where $x\geq 0$ , $y,\lambda > 0$ , $\mathbf{A}$ a real symmetric positive semmi-definite, and $\mathbf{z}$ are fixed. EDIT2: Let $\mathbf{t}(x,y) = - (\mathbf{A} + 0.5 x y\: \mathbf{I})^{-1} \mathbf{z}$ . If we first solve $\partial_x f(x,y) = 0$ we get $$x = \| \mathbf{t}(x,y)\|\tag{4},$$ for $y >0$ . Then, if we solve $\partial_y f(x,y) = 0$ and use $(4)$ we get $$\sqrt[3]{\lambda} = \| \mathbf{t}(x,y)\|.\tag{5}$$ Next, suppose that we first solve $x = \| \mathbf{t}(x,y)\|$ w.r.t. $x$ to get an optimal ${x^*}'$ and then solve $\sqrt[3]{\lambda} = \| \mathbf{t}({x^*}', y)\|$ w.r.t. $y$ to get ${y^*}'$ . Can we say that ${z^*}' \equiv {z^*}$ ?","['convex-optimization', 'nonlinear-optimization', 'multivariable-calculus', 'non-convex-optimization', 'optimization']"
4293949,Integrating mass element of a spherical disc,"Suppose we have a solid disc of uniform surface density $\sigma$ , and radius $a$ .  I want to find its mass by integration. Normally we do this by the following method : We set the origin of a polar coordinate system at the center of the disc. Then we take a small area element, at a distance $r$ from this origin $O$ . Let $dm$ be the mass of this element. In polar coordinates, we know $dm=\sigma dxdy=\sigma rdrd\psi$ . Hence $$M=\int dm=\int_{0}^{2\pi}\int_{0}^{a}\sigma rdrd\psi = \sigma\pi a^2$$ However, what I want to do, is to make my origin at a point on the circumference of the disc, as follows : Now I want to carry out the same integration. What would be the new mass element, and the limits of integration, that would get me the exact same answer ? My guess is that the mass element would still be $dm=\sigma rdrd\psi$ however I'm not sure. The limits of integration would be very different though. Can anyone show me what these would be and how to find it, so that my final answer is exactly the same. Another guess is that the limits of $d\psi$ are symmetric, between some $[-c,c]$ where $c$ is some constant or maybe a function of $r$ such that $c=c(r)$ . However I think that it is symmetric even though I not know the form or the values. (note : I need $d\psi$ and $dr$ to be the variables of integration, not $dx$ , $dy$ or anything else. ) Any help would be highly appreciated.","['integration', 'definite-integrals', 'multivariable-calculus', 'polar-coordinates', 'multiple-integral']"
4294018,Matrix Equality - Bellman,"Question : Use the relation $|AB|=|A||B|$ to show that $$(a_1^2+a_2^2)(b_1^2+b_2^2)=(a_1b_1-a_2b_2)^2+(a_2b_1+a_1b_2)^2.$$ Attempt : It's easy to expand out the two sides to verify they are equivalent, but I don't understand how the hint comes into play here? Bellman denoted matrices by capital letters in the exposition, but I would assume here that we'd want $A$ and $B$ to be vectors in $\mathbb{R}^2$ ? Any help appreciated - thanks. Source : Bellman - ""Introduction to Matrix Analysis""","['matrices', 'linear-algebra']"
4294026,Construction of pre-Brownian motion from Markov transition kernels,"I am reading very nice notes on probability theory by Bruce Driver, but since I didn't have a lecture on the subject I sometimes get confused. In particular, I am confused about the existence of a pre-Brownian motion (process with independent increments where the increments are normally distributed, i.e. a Brownian motion but not necessarily continuous). Corollary 22.27 states the existence of pre-B motion and is based on exercise 17.8 (which says that if the pre-B motion would exist it would have some properties) and Theorem 17.11 which states that given Markov transition kernels we can construct a Markov process with those kernels. However, I am having a hard time piecing this together.","['markov-process', 'brownian-motion', 'probability-theory']"
4294044,Showing that $\lim_{x\to 0}-\frac{1}{x}+\sum_{j=0}^\infty(2j+1)e^{-j(j+1)x}=\frac{1}{3}$,"In finding the rotational partition function for a $C_{\infty v}$ molecule , one comes across the function \begin{equation}
Z(x)=\sum_{j=0}^\infty(2j+1)e^{-j(j+1)x}
\end{equation} Surprisingly, this admits the Laurent series \begin{equation}
Z(x)=\frac{1}{x}+\frac{1}{3}+\frac{1}{15}x+\frac{4}{315}x^2+\frac{1}{315}x^3+\ldots
\end{equation} The first term is generally found as the approximation \begin{equation}
Z(x)\approx \int_0^\infty(2j+1)e^{-j(j+1)x}dj=\frac{1}{x}\quad\quad\quad\text{for }x\ll1,
\end{equation} and the rest of the terms can be found by the Euler-Maclaurin formula . However, the Euler Maclaurin formula is a bit complicated, and I haven't figured out how to use it correctly to find the expansion of $Z(x)$ . I am interested primarily in proving the value of the constant term \begin{equation}
\lim_{x\to 0}\left(Z(x)-\frac{1}{x}\right)=\frac{1}{3}.
\end{equation} I've tried expanding the exponentials as a Taylor series, but the constant terms end up adding to infinity, and I can't figure out how to annihilate them with an expansion of $\frac{1}{x}.$ Considering the $\frac{1}{x}$ term in the Laurent series for $Z(x)$ , I figure that this problem might be able to be solved with Pade approximants, or Cauchy's integral formula.","['complex-analysis', 'limits', 'sequences-and-series', 'real-analysis']"
4294051,"Specify the classes of the following Markov chains, and determine whether they are transient or recurrent","Specify the classes of the following Markov chains, and determine whether they are transient or recurrent: $$\mathbb{P}_1=\begin{Vmatrix}0 & 1/2 & 1/2\\ 1/2 & 0 & 1/2\\ 1/2 & 1/2 & 0\end{Vmatrix},\quad\quad \mathbb{P}_2=\begin{Vmatrix}0 & 0 & 0 & 1\\ 0 & 0 & 0 & 1\\ 1/2 & 1/2 & 0 & 0\\ 0 & 0 & 1 & 0\end{Vmatrix}$$ $$\mathbb{P}_3=\begin{Vmatrix}1/2&0&1/2&0&0\\1/4&1/2&1/4&0&0\\1/2&0&1/2&0&0\\0&0&0&1/2&1/2\\0&0&0&1/2&1/2\end{Vmatrix},\quad \mathbb{P}_4=\begin{Vmatrix}1/4&3/4&0&0&0\\1/2&1/2&0&0&0\\0&0&1&0&0\\0&0&1/3&2/3&0\\1&0&0&0&0\end{Vmatrix}$$ My Attempt $\mathbb{P}_1$ : Closed class: $\{0,1,2\}$ . All states are recurrent. $\mathbb{P}_2$ : Closed classes: $\{0,1,2,3\}$ . States 0 and 1 communicate with state 3, which communicates with state 2, which communicates with states 0 and 1 again. Thus, all states are recurrent. $\mathbb{P}_3$ : Closed classes: $\{0,2\},\{3,4\}$ because states 0 and 2 communicate with each other and states 3 and 4 communicate with each other. Open class: $\{1\}$ because state 1 communicates with 0 but 0 does not communicate with 1. State 1 communicates with 2 but 2 does not communicate with 1. States 0, 2, 3, and 4 are recurrent. State 1 is transient. $\mathbb{P}_4$ :
Closed classes: $\{0,1\},\{2\}$ because states 0 and 1 communicate with each other, and state 2 communicates with itself.
States 0 and 1 are recurrent, states 2, 3, and 4 are transient. Can I get a formal and an intuitive definition of open and closed classes? I was able to figure this problem out from the patterns I have seen, but I would love to have a definition to go by. I am not entirely sure if I did this correctly, so any help would be appreciated. Thanks.","['stochastic-processes', 'statistics', 'transition-matrix', 'markov-chains']"
4294063,Discrete calculus: is my proof about the difference of two consecutive powers correct?,"Theorem statement: $\displaystyle \Delta^dF_n(x) = \sum_{k=0}^{n-1} \binom{n-1}{k} \Delta^{d-1}F_{n-1-k}(x)(-1)^k$ Proof: $$\begin{align} F_n(x) &= x^n =\Delta^0F(x) \\[2ex] \Delta^1F_n(x) &= x^n -(x-1)^n \\[2ex] &= x^n - \sum_{k=0}^n\binom nk x^{n-k}(-1)^k \\[2ex] &= \sum_{k=0}^{n-1}\binom{n-1}{k}x^{n-k}(-1)^{k} \\[2ex] &=x^{n-1} -(n-1)x^{n-2} + \ldots \pm (n-1)x^2 \mp 1 \\[2ex] &= \sum_{k=0}^{n-1}\binom{n-1}{k}\Delta^0F_{n-1-k}(x)(-1)^k\end{align}$$ $$\begin{align}\Delta^2F_n(x) &=\Delta^1F_n(x) -\Delta^1F_n(x-1) \\[2ex] &= \sum_{k=0}^{n-1}\binom{n-1}{k}x^{n-k}(-1)^k - \sum_{k=0}^{n-1}\binom{n-1}{k}(x-1)^{n-k}(-1)^k \\[2ex] & = x^{n-1}-(x-1)^{n-1}- (n-1)x^{n-2} - (n-1)(x-1)^{n-2} +\ldots \pm(n-1)x^2 -(n-1)(x-1)^2 \\[2ex] & = \sum_{k=0}^{n-1} \binom{n-1}{k}\Delta^1F_{n-1-k}(x)(-1)^k\end{align}$$ $$\begin{align} \Delta^3F_n(x) &= \Delta^2F_n(x) - \Delta^2F_n(x-1) \\[2ex] &= \sum_{k=0}^{n-1} \binom{n-1}{k}\Delta^1F_{n-1-k}(x)(-1)^k -\sum_{k=0}^{n-1} \binom{n-1}{k}\Delta^1F_{n-1-k}(x-1)(-1)^k \\[2ex] &= \sum_{k=0}^{n-1} \binom{n-1}{k} (\Delta^1F_{n-1-k}(x) - \Delta^1F_{n-1-k}(x-1))(-1)^k \\[2ex] &= \sum_{k=0}^{n-1} \binom{n-1}{k} \Delta^2F_{n-1-k}(x)(-1)^k \end{align}$$ $$\therefore \Delta^dF_n(x) = \sum_{k=0}^{n-1} \binom{n-1}{k} \Delta^{d-1}F_{n-1-k}(x)(-1)^k$$ Is this correct? If not, where did I go wrong? If it is correct, what can I do with this? Where can I apply the derived identity? I originally set out to prove that $\Delta^n F_n(x) = c \ \forall x$ , but I'm not sure if I'm any closer to this now.","['solution-verification', 'combinatorics', 'sequences-and-series', 'discrete-calculus', 'exponential-function']"
4294081,Find total charge of triangular region,"Charge is distributed over a triangular region in the 𝑥𝑦-plane bounded by the 𝑦-axis and the lines 𝑦=5−𝑥 and 𝑦=1+𝑥. The charge density at a point (𝑥,𝑦) is given by 𝜎(𝑥,𝑦)=𝑥+𝑦, measured in coulombs per square meter (C/m2). Find the total charge. I've graphed a triangle which then made me form the integral $$\int_0^3 \int_{y-1}^{5-y} (x+y) \,dx \, dy$$ , making me get $27$ . I believe there is something wrong with my calculation. Is it that I am using the wrong variable or something else?",['multivariable-calculus']
4294089,Calculating a spherical angle,"Given a sphere with a specified radius, and two perpendicular arcs produced by angles, phi and theta. To be clear, phi and theta are the angles which give rise to the arcs which meet at a right angle. I placed the labels, phi and theta, on the actual arcs because it gets crowded near the center of the sphere. I am looking for the interior angle which gives rise to the oblique arc on the sphere's surface. I read these links: https://en.wikipedia.org/wiki/Haversine_formula https://en.wikipedia.org/wiki/Great-circle_distance How do I measure distance on a globe? relationship between a great circle arc and a latitude circle arc at a given latitude but the derivation of an answer eludes me. I realize that spending time to answer a question so fundamental and which has probably been answered clearly in some text is a waste of bandwidth. So maybe someone knows of a clear, simple exposition of this problem to which I can be referred?",['geometry']
4294133,What's so hyperbolic about hyperbolic sets?,"In dynamics, we have the notion of a ""hyperbolic set"" for a diffeomorphism $f:M\to M$ of a Riemannian manifold. I am trying to connect this to my existing ideas surrounding the term ""hyperbolic"". To my understanding, $M$ itself need not be a hyperbolic manifold $^*$ , but maybe there is some more distant connection $^{**}$ at work? $^*$ Consider the action of $\begin{pmatrix}2&1\\1&1\end{pmatrix}$ on the torus. $^{**}$ Heh [EDIT] Someone wanted to know what a hyperbolic set was so I'm adding the definition here. Let $M$ be a Riemannian manifold and let $f:M\to M$ be a diffeomorphism. Then $\Lambda \subset M$ is hyperbolic if there are constants $C> 0$ and $\lambda \in (0,1)$ such that for every $x\in \Lambda$ , we can write $T_xM = E^u(x) \oplus E^s(x)$ . We require that $\|df^n_x v\| \le C\lambda^n \|v\|$ for $v\in E^s(x)$ and $n\ge 0$ , $\|df^{-n}_x v\| \le C\lambda^n \|v\|$ for $v\in E^u(x)$ and $n\ge 0$ , $df_x(E^u(x)) = E^u(f(x))$ , and $df_x(E^s(x)) = E^s(f(x))$ . In English, we have expanding and contracting directions. Standard example is, give me some $A\in GL_n(\mathbb{Z})$ with no eigenvalues on the unit circle. It'll induce an automorphism of the torus $\mathbb{R}^n/\mathbb{Z}^n$ . Then $\mathbb{T}^n$ is a hyperbolic set with respect to this automorphism, with the expanding directions being the sum of eigenspaces with eigenvalue $> 1$ , and the contracting directions being the sum of eigenspaces with eigenvalue $< 1$ .","['definition', 'dynamical-systems', 'differential-geometry']"
4294145,Compatibility of Frobenius norm and $1$-norm.,"I am trying to prove or disprove the compatibility (over $\mathbf{R}^n$ ) of the Frobenius norm of square matrices and the $1$ -norm for vectors. That is the norms $$\|A\| := \sqrt{\sum_{j=1}^n\sum_{i=1}^n a_{j,i}^2}$$ and $$\|\mathbf{x}\|_1 := \sum_{i=1}^n |x_i|$$ respectively. When I restrict my view to the case of $n=2$ , if $$A = \begin{bmatrix} a & b \\c& d \end{bmatrix}$$ and $\mathbf{x} = \begin{bmatrix} e \\ f \end{bmatrix}$ then I would essentially need to show that $$|ea+fb|+|ec+fd|\leq \sqrt{a^2 +b^2 +c^2+d^2} (|e|+|f|).$$ But this seems true and increasing the dimension would just make it more true. Any guidance on proving or a counterexample for $$\|A \mathbf{x}\|_1 \leq \|A\| \|\mathbf{x}\|_1$$ would be much appreciated.","['matrices', 'inequality', 'normed-spaces', 'linear-algebra']"
4294175,Intersection of relations [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question The below relations are given AUFTRAG( $A_1, A_2, A_3$ ) and KUNDE( $B_1, B_2, B_3$ ) with $A_1$ = AUFTRNR, $A_2$ = DATUM, $A_3$ = KUNDNR, $B_1$ = KUNDNR, $B_2$ = NAME and $B_3$ = ORT. I want to calculate AUFTRAG $\cap$ KUNDE. It holds that $R\cap S=R\setminus (R\setminus S)$ . First we consider $R\setminus S$ , so at the table AUFTRAG we delete the elements that are also in KUNDE. The column KUNDNR is common so we delete the rows that are in common. We delete this row from AUFTRAG and get : Is that correct?","['elementary-set-theory', 'logic', 'relations']"
4294218,Confused about the relationship between PCA and robust PCA,"I recently learned about PCA and robust PCA. I understand that PCA is identifying the principal components by finding the eigenvectors of the covariance matrix (which of course contains information about the ""directionality"" of the data). These principal components are then used to transform the data so that it is reoriented along the principal components (lets call this reoriented data matrix $D$ ). I also understand that robust-PCA can be written as a convex relaxation problem whereby solving it - using Lagrangian methods - yields a low rank matrix $L$ and a sparse matrix $S$ . Is it correct to say this low rank matrix L would be the analogue to the $D$ matrix I described earlier? To find what the principal components are in robust-PCA, would I simply take the eigenvectors of the low rank matrix $L$ ? Lastly, are robust PCA and PCA titled as such because they both will allow you to produce low rank representations of data, not because there is any algorithmic similarity in how they are performed?","['robust-statistics', 'statistics', 'principal-component-analysis']"
4294255,"Conditional expectation $E[f(X,Y)|\mathcal F]$ with $Y$ $\mathcal F$-measurable","Let $X,Y$ be real random variables on a probability space $(\Omega,\mathcal A,P)$ . Let $\mathcal F$ be a sub- $\sigma$ -algebra of $\mathcal A$ and $f:\mathbb R^2\to \mathbb R$ be Borel measurable. If $f(X,Y)$ is $P$ -integrable and $Y$ is $\mathcal F$ -measurable , is it true that $$E[f(X,Y)|\mathcal F](\omega)=\int f(x,Y(\omega))\kappa_{X,\mathcal F}(\omega,dx) \quad \quad (1)$$ for $P$ -almost all $\omega\in\Omega$ ? Here $\kappa_{X,\mathcal F}$ is a regular conditional distribution of $X$ given $\mathcal F$ .","['conditional-probability', 'measure-theory', 'probability-theory', 'conditional-expectation']"
4294329,Function transformations - what went wrong here...,"Looking at (b) only, $$ A(2,4), \quad B(4,0), \quad  C(8,4) $$ If I follow that answer in words，translation by the vector (3,0)
I know the graph will shift to the RIGHT by 3 units, so using the coordinates ,
we get $$ A(5,4), \quad B(7,0), \quad  C(11,4) $$ then if we reflect to the $y$ -axis, we get $$ A(-5,4), \quad B(-7,0), \quad  C(-11,4) $$ . However, if ""shift to the RIGHT by 3 units"", using function notation,
we should use $$
f(x) \mapsto f(x-3)
$$ Using this we should get $$ A(-1,4), \quad B(1,0), \quad  C(5,4) $$ ,
then we reflect again, $$ A(1,4), \quad B(-1,0), \quad  C(-5,4) $$ ,
which is the correct answer. (obviously, the $x$ coordinate for $c$ is wrong) WHY is the first method wrong????
Just moving the points to the right by adding $x$ coordinate 3,
then reflect gets the wrong answers, this is just confusing......","['functions', 'transformation', 'graphing-functions']"
4294345,Almost sure convergence of rescaled nondecreasing sequences of random variables,"Let us consider a sequence $(S_n)_n$ of $L^2$ random variables. Assume: $S_n \le S_{n+1}$ almost surely $S_n \to_{n \to \infty} +\infty$ almost surely $\frac{S_n}{\mathbb{E}[S_n]} \to_{n \to \infty} 1$ in the $L^2$ sense. Is it always true that $\frac{S_n}{\mathbb{E}[S_n]} \to_{n \to \infty} 1$ almost surely? If not, can you find a counterexample?","['examples-counterexamples', 'almost-everywhere', 'borel-cantelli-lemmas', 'convergence-divergence', 'probability-theory']"
4294353,"Proving $\arctan x = 2 \arctan \frac{x}{1 + \sqrt{1 + x^2}}$, starting from the integral representation of $\arctan$","Starting from the integral representation of the arctan function, $$\arctan x = \int_0^x \frac{dt}{1+t^2},$$ how does one prove the (half-angle) identity? $$\arctan x = 2 \arctan \frac{x}{1 + \sqrt{1 + x^2}}$$ I am interested in a clever change of variables that should do the trick. However, I am unable to find it.","['trigonometry', 'definite-integrals']"
4294439,Need clear and simple clarification on the difference between accuracy and precision,"Could someone suggest a better explanation of the difference between ""accuracy and precision concept"" other than this very common approach which is not clear to me. EDIT For example:
Why is picture in top rights corner is ""precise""? Is it because the dots are within the outer circle? Or is it because the dots are clustered together? What about the distance between the clustered dots center and the radius of the inner circle, this could be huge, would it still be precise? Thank you.","['statistics', 'terminology']"
4294497,"Prove that $\nabla_{-v}f(a) = -\nabla_{v}f(a)$, provided $\nabla_{v}f(a)$ exists.","Definition: $\nabla_{v}f(a)$ is said to be the directional derivative of $f$ at $a$ along the vector $v$ if $\forall \epsilon >0 \exists \delta>0$ s.t. $\forall h \in \mathbb{R}$ , $$0<|h|<\delta \implies \left|\left| \frac{f(a+hv)-f(a)}{h} - \nabla_{v}f(a) \right|\right|<\epsilon$$ or equivalently, $$\lim_{h \to 0} \frac{f(a+hv) - f(a)}{h} = \nabla_{v}f(a).$$ Note that we cannot use the eqaution relating the directional derivative to a the gradient since $f$ may not be differentiable at $a$ . Here is my attempt using the $\epsilon - \delta$ definition. Suppose $\nabla_{v}f(a)$ exists. We want to show that $\nabla_{-v}f(a) = -\nabla_{v}f(a)$ . Let $\epsilon>0$ . Then there exists $\delta_{0} >0$ s.t. for any $h \in \mathbb{R}, 0<|h|<\delta_{0}$ , $$\left|\left| \frac{f(a+hv)-f(a)}{h} - \nabla_{v}f(a) \right|\right|<\epsilon.$$ Now pick $\delta = $ (tbd). then for $0<|h|<\delta$ , $$\left|\left| \frac{f(a-hv)-f(a)}{h} + \nabla_{v}f(a) \right|\right|$$ Unfortunately, I got stuck here as i have no idea how to obtain an expression similar to $\left|\left| \frac{f(a+hv)-f(a)}{h} - \nabla_{v}f(a) \right|\right|<\epsilon$ from $\left|\left| \frac{f(a-hv)-f(a)}{h} + \nabla_{v}f(a) \right|\right|$ . Any hints/ideas/suggestions?","['multivariable-calculus', 'real-analysis']"
4294501,Is there any formula for $\operatorname{arctan}{(a+b)}$,Is there any formula for $$\operatorname{arctan}{(a+b)}$$ I know couple of formulas for trigonometric functions. But I don't know if such formulas exists for inverse trigonometric functions. I don't have a clue where to start with. Any hint will be appreciated.,"['algebra-precalculus', 'trigonometry']"
4294554,Derivative of a function using definition at a point,"I'm trying to find the derivative of the function $f(x)=x^2\sin x$ at $x=\pi/2$ . Using the definition of the derivative, I got: $$\lim_{x\to(\pi/2)} \frac{x^2\sin x-(\frac{\pi}{2})^2}{x-\frac{\pi}{2}}$$ I can't seem to be able to come up with any of the common factorising methods that can be used to remove the indeterminable denominator. Is there any way to manipulate this?","['calculus', 'derivatives']"
4294619,Calculating $B^{10}$,"Calculate $B^{10}$ when $$B = \begin{pmatrix} 1 & -1\\ 1 & 1 \end{pmatrix}$$ The way I did it was $$ B = I + A $$ where $$A = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$$ and $A^2=-I$ . Since $A$ and $I$ are commutative, $$\begin{aligned} B^2 &= (I+A)^2 = 2A \\ B^3 &= (I+A)2A = 2A-2I\\ B^4 &= (I+A)(2A-2I) = -4I\\ \vdots \\ B^{10} &= 32A \end{aligned}$$ Is there a simpler method or a smarter approach if you want to do this for, e.g., $B^{100}$ ?","['matrices', 'exponentiation', 'linear-algebra']"
4294650,How does the Frobenius really act on Weil sheaves in $\ell$-adic cohomology?,"Let $X_0$ be a connected scheme defined over $\mathbb F_{p}$ and let $X$ be the product $X_0 \times_{\mathbb {F}_p} \overline{\mathbb F_p}$ , as usual, with the natural map $\pi:X \to X_0$ . Given an etale sheaf $\mathcal G_0$ on $X_0$ , let $\mathcal G$ be the pullback $\pi^* \mathcal G_0$ . The Frobenius $F: X \to X$ induces, by adjunction induces a natural map $\mathcal G \to F_* F^* \mathcal G$ which in turn induces $H^i(X, \mathcal G) \to H^i(X, F^* \mathcal G)$ . The key ingredient to getting the Frobenius action on the cohomology is to get a map $F^*\mathcal G \to \mathcal G$ , which is an isomorphism. It is this map I have trouble understanding. In Bhargav Bhatt's seminar here, at the end of page 8 , we see the desired map being defined roughly as follows: Firstly there are the two projection maps $\text{pr}_1$ and $\text{pr}_2$ , $X\times_{X_0}X \to X$ , and they satisfy $\pi \circ \text{pr}_1=\pi \circ \text{pr}_2$ . From this, we get $\text{pr}_1^* \circ \pi^*\mathcal G_0=\text{pr}_2^* \circ \pi^*\mathcal G_0$ , which means that $\text{pr}_1^* \mathcal G$ and $\text{pr}_2^*\mathcal G$ are equal on the nose. However, there it is claimed only that $\text{pr}_1^* \mathcal G \cong \text{pr}_2^* \mathcal G$ . Next, $X\times_{X_0}X$ is claimed to be $\text{Gal}(\overline{\mathbb{F}_p}/\mathbb{F}_p)\times X$ . I don't see why this should be. A typical element of $X\times_{X_0}X$ is a pair $(x,y)$ such that $\pi(x)=\pi(y)$ . But since $X_0$ is the set of Galois orbits of $X$ , this means that there is an automorphism $\sigma \in \text{Gal}(\overline{\mathbb{F}_p}/\mathbb{F}_p)$ such that $y= \sigma(x)$ . But this $\sigma$ is defined only up to $\text{Gal}(\overline{\mathbb{F}_p}/\mathbb{F}_{p^{\text{deg}(x)}})$ , so I don't see why the product structure should hold. Even if I assume the above two assertions, how does the isomorphism $\text{pr}_1^* \mathcal G \cong \text{pr}_2^* \mathcal G$ decompose into $\text{Gal}(\overline{\mathbb{F}_p}/\mathbb{F}_p)$ -many distinct isomorphisms?","['etale-cohomology', 'algebraic-geometry', 'sheaf-cohomology', 'arithmetic-geometry']"
4294744,Ordering of last two boxes!,"Say that you have 6 boxes. All these boxes are uniform in all ways, except one. Which is colour. From this, we can see that 2 are red boxes, 2 are pink boxes, and 2 are black boxes? How would you find the probability that the last two boxes in all combinations possible are pink? I did something like this: Firstly, we know that there are 2 boxes of each type initially. Thus we have \begin{align}
\frac{6!}{2! \times 2! \times 2!}
\end{align} (we define this as A) Secondly, we know that the last 2 balls are pink. Now, we have that \begin{align}
\frac{4!}{2! \times 2! }
\end{align} (we define this as B) Thus, the probability is P = A/B. But I feel as if this is wrong as this looks like im approaching it as a ""given"" probability. Can someone guide me with this question?","['statistics', 'combinatorics', 'probability', 'combinations']"
4294747,In an attempt to find $I = \int_0^\infty \frac{t}{e^t-1}dt$,"I was trying to solve $I = \int_0^\infty \frac{t}{e^t-1}dt$ My approach I took the more general form of integral $f(s) = \int_0^{\infty}\frac{e^{-st}}{e^t-1}dt$ the same way as How to evaluate the integral $I = \int_o^{\infty} \frac{x}{\sqrt{e^{2\pi\sqrt{x}}-1}}dx$? So, my answer $$I = -\left[\frac{\partial f(s)}{\partial s}\right]_{s =0}$$ but that simply doesn't work. However, $\int_0^\infty \frac{t}{e^t-1}dt$ without using series doesn't answer my question as I'm interested in finding why finding derivative at $s = 0$ doesn't work Also, I'm interested in finding a number of different ways I can solve this problem","['polygamma', 'laplace-transform', 'real-analysis', 'calculus', 'riemann-zeta']"
4294820,Countable class of real-valued differentiable functions with different derivatives at a point must differ in a neighborhood.,"Consider the following statement. Consider a class of functions $\mathcal{F}=\{f_1,\ldots,f_N\}$ such
that $f_{n}: \mathbb{R} \rightarrow \mathbb{R}$ are differentiable for
all $n=1,\ldots, N$ . If there exists a point $x^{*} \in \mathbb{R}$ such that $f_{1}^{\prime}\left(x^{*}\right) \neq f_{n}^{\prime}\left(x^{*}\right)$ and $f_{1}\left(x^{*}\right) = f_{n}\left(x^{*}\right)$ for $n>1$ , then there exists $\varepsilon > 0$ such that $f_{1}(x) \neq f_{n}(x)$ for all $x \in (x^*,x^*+\varepsilon)$ and $n>1$ . The above statement is true when $N$ is a finite number, I wonder if we can extend this statement to the case where $\mathcal{F}$ is an infinite set of functions. To do so, we need extra assumptions because $f_1(x)=x$ and $f_n(x)=n x^2$ is a counterexample. What extra assumptions would need to be added to turn this into a true statement?","['derivatives', 'real-analysis']"
4294827,Calculus differentiation — What did I do wrong?,"I am learning Calculus with the book Calculus and Analytics Geometry, by George B. and Thomas, Jr., and I was doing the exercises on the Chapter 2.6 and using Wolfram Alpha to check my answer. I am told to differentiate $x(x^2+1)^{1/2}$ , so I tried using The Product Rule and The Power Rule for Rational Numbers: $\frac{d}{dt}x(x^2+1)^{1/2} = x\frac{d}{dx}\sqrt{x^2+1} + \sqrt{x^2+1}\frac{d}{dx}x = x\frac{1}{2}\frac{1}{\sqrt{x^2+1}} + \sqrt{x^2+1} = \frac{x}{2\sqrt{x^2+1}} + \frac{x^2+1}{\sqrt{x^2+1}} = \frac{x}{2\sqrt{x^2+1}} + \frac{2x^2+2}{2\sqrt{x^2+1}} = \frac{2x^2+x+2}{2\sqrt{x^2+1}}$ But Wolfram Alpha gives $\frac{2x^2 + 1}{\sqrt{x^2 + 1}}$ I am pretty sure I made a mistake, but I don't know where, sice it looks, to me, that I used the Product Rule and The Power Rule correctly. Hence my question: what did I do wrong? As a note, I don't know if this question is allowed, and my LaTeX isn't very good.","['calculus', 'derivatives']"
4294831,"On existence of a nonzero integer vector $x$ such that $\|Bx\|_1\leq \sqrt[n]{n!\,|\det(B)|}$","Let $B=(b_{ij})$ be a real invertible $n\times n$ matrix for some $n\ge2$ . Prove that there exists an integer vector $(x_1,\cdots, x_n)^T\ne0$ such that $\|Bx\|_1\leq\sqrt[n]{n!\,|\det(B)|}$ . There are several inequalities that might be useful for this problem: Holder's inequality, the Cauchy-Schwarz inequality for inner products, the AM-GM inequality, the power-mean inequality, and Jensen's inequality. I also know that $\det(B) = \sum_{\sigma \in \Pi_n}(-1)^\sigma b_{1, \sigma(1)}b_{2,\sigma(2)}\cdots b_{n,\sigma(n)},$ where $\Pi(n)$ is the set of permutations of $\{1,\cdots, n\}$ and that it can be obtained from cofactor expansion along any row or column, which might be useful for proving this inequality.","['real-analysis', 'matrices', 'calculus', 'linear-algebra', 'inequality']"
4294850,"Prove that $\int_1^2 g(x^3 - 3x)\,\mathrm dx=\int_0^1 g(x^3 - 3x)\,\mathrm dx$","Let $g:[-2, 2]\to\mathbb{R}$ be an even continuous function. Prove that $$\int_1^2g(x^3-3x)\,\mathrm dx=\int_0^1g(x^3-3x)\,\mathrm dx.$$ I found the following solution online, but I'm not sure why $$\int_0^1f(t)\,\mathrm d[x(t)]=\int_0^1 g(x^3-3x)\,\mathrm dx.$$ Also, why does $$\int_0^1g(t)\,\mathrm d[x(t)]=\int_{t=-1}^0 g(t)\,\mathrm d[y(t)]+\int_{t=-1}^0 g(t)\,\mathrm d[z(t)]\,?$$ Is it because $x(t) = -z(t)-y(t)$ and does $$\int_a^b\,\mathrm d(x(t))=-\int_a^b\,\mathrm d[y(t)]-\int_a^b\,\mathrm d[z(t)]\,?$$ For each $t\in [-1,0]$ , the equation $x^3-3x=t$ has three roots $x(t)\in[0,1]$ , $y(t)\in[1,\sqrt3]$ , $z(t)\in [-2,-\sqrt3]$ . By Vieta's Theorem, $x(t)+y(t)+z(t)=0$ $\forall$ $t$ . Also, $x^3-3x$ is differentiable on $\mathbb{R},$ increasing in $(-2,-\sqrt{3}), $ decreasing in $[0,1],$ and increasing in $[1,\sqrt{3}],$ so by the inverse function theorem, $x,y,z$ are all differentiable for $t\in (-1,0).$ We then have \begin{align*}
\int_0^1 g(x^3-3x)\,\mathrm dx&=\int_0^{-1}g(t)\,\mathrm d[x(t)]\\
&= \int_{-1}^0 g(t)\,\mathrm d[y(t)]+\int_{-1}^0g(t)\,\mathrm d[z(t)]\\
&=\int_1^{\sqrt3} g(y^3 -3y)\,\mathrm dy+\int_{-2}^{-\sqrt3}g(z^3- 3z)\,\mathrm dz\\
&=\int_1^{\sqrt3} g(y^3-3y)\,\mathrm dy + \int_{\sqrt3}^2g(z^3 - 3z)\,\mathrm dz&\text{(as g is even)}\\
&=\int_1^2 g(x^3-3x)\,\mathrm dx
\end{align*}","['integration', 'real-analysis', 'continuity', 'calculus', 'derivatives']"
4294912,Flow of an ODE and continuity with respect to initial conditions and parameter - what is the relation between them?,"Let $$\frac{dx}{dt}=f(t, x, \lambda)$$ be a parametric differential equation, where $f:D\subset \mathbb{R}\times \mathbb{R}^n\times \mathbb{R}^k \to \mathbb{R}^n$ ( $\lambda$ is the parameter). Suppose that for every $\lambda$ the ODE admits uniqueness of solutions. My lecturer defined the parameterized flow of this equation as follows ( $(t_0, x_0, \lambda_0)\in D$ is fixed) : it is the function $\alpha:I_1 \times I_0 \times G_0 \times \Lambda_0\in \mathcal{N}(t_0, t_0, x_0, \lambda_0)\to \mathbb{R}^n$ defined as follows: for every $(\tau, \xi, \lambda)\in I_0\times G_0\times \Lambda_0$ the function $\alpha(., \tau, \xi, \lambda):I_1\to \mathbb{R}^n$ is the unique solution of the Cauchy problem with the initial condition $f(\tau)=\xi$ . He then went on to show that if $f$ is continuous and Lipschitz in the second variable then this $\alpha$ is a continuous function and he told us that this shows the continuous dependence of the ODE with respect to the initial conditions and the parameter. But here I am a bit puzzled. I knew that if there is no parameter then the continuous dependence with respect to the initial conditions for an equation of the form $\frac{dx}{dt}=f(t, x)$ is something like this: for every $\epsilon>0$ , there is some $\delta>0$ (which depends on $\epsilon$ and the point where I am given the initial condition) such that if $\phi$ and $\psi$ are two solutions of the ODE and $||\phi(t_0)-\psi(t_0)||\le \delta$ , then $||\phi(t)-\psi(t)||\le \epsilon$ for every $t$ for which these solutions are defined $(*)$ . Basically, if the initial values are sufficiently close, then the solutions will be arbitrarily close. However, even though intuitively I can see that these notions are the same, I can't see exactly how the fact that my flow $\alpha$ is continuous implies something like $(*)$ . I think that I somethow need to write the definition of continuity, but I get confused with all those arguments of $\alpha$ . Could you please explain this to me?","['dynamical-systems', 'ordinary-differential-equations', 'real-analysis']"
4294941,Jacobian Matrix of inverse map [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I have a question which asks me to
show that the map f: $R^2 \rightarrow R^2$ defined by $$f(x,y)=(e^x+e^y, e^x+e^{-y})$$ is locally invertible about any point $(a,b) \in R^2$ , and compute the Jacobian matrix of the inverse map. I know if is locally invertible because the determinant of the Jacobian matrix is not zero. However, how do I find the Jacobian matrix of the inverse map? Do I find the inverse matrix directly or should I find the inverse of this map and then find the Jacobian? There are several similar problems so I would really appreciate it if someone could walk me through the process.",['multivariable-calculus']
4294973,$H_0^1(\mathbb{R}^d \setminus \{0\}) = H^1(\mathbb{R}^d \setminus \{0\})$ for $d = 2$,"In remark 17 of the book [1], it is stated that $$H_0^1(\mathbb{R}^d \setminus \{0\}) = H^1(\mathbb{R}^d \setminus \{0\})
\tag{1}
$$ for all spatial dimensions $d \geq 2$ . Here $H_0^1(\mathbb{R}^d \setminus \{0\})$ is defined to be the closure of $C_c^{\infty}(\mathbb{R}^d \setminus \{0\})$ in $H^1(\mathbb{R}^d \setminus \{0\})$ . I think I am able to prove this for $d \geq 3$ .
In my proof I make use of the fact that, for $d \geq 3$ , $$\frac{n^2}{n^d} \rightarrow 0 \qquad \textrm{as } n \rightarrow \infty$$ Clearly this argument doesn't work with $d=2$ .
Does anybody know how $H_0^1(\mathbb{R}^d \setminus \{0\}) = H^1(\mathbb{R}^d \setminus \{0\})$ can be proven in the $d=2$ case? Here is my attempted proof. Introduce a cut-off function $\eta \in C_c^{\infty}(\mathbb{R}^d)$ such that $0 \leq \eta \leq 1$ and $$
\eta(x) = \begin{cases}
1 & \textrm{ if } |x| \leq 1, \\
0 & \textrm{ if } |x| \geq 2.
\end{cases}
$$ Let $\eta_n(x) = \eta(nx)$ . Then, to prove $(1)$ , it suffices to show that for any $u \in H^1(\mathbb{R}^d \setminus \{0\})$ we have $$ \| \eta_n u \|_{H^1(\mathbb{R}^d \setminus \{0\})} \rightarrow 0.$$ However, note that $H^1(\mathbb{R}^d \setminus \{0\}) 
\cap L^{\infty}(\mathbb{R}^d \setminus \{0\})$ is dense in $H^1(\mathbb{R}^d \setminus \{0\})$ (see e.g. Item 3 of Remark 1.27 in https://math.aalto.fi/~jkkinnun/files/sobolev_spaces.pdf ).
Therefore, we can assume without loss of generality that $u \in H^1(\mathbb{R}^d \setminus \{0\}) 
\cap L^{\infty}(\mathbb{R}^d \setminus \{0\})$ .
Now, using dominated convergence, it is easily checked that $$
\| \eta_n u \|_{L^2(\mathbb{R}^d \setminus \{0\})} \rightarrow 0, \\
\| \eta_n (\nabla u) \|_{L^2(\mathbb{R}^d \setminus \{0\})} \rightarrow 0.
$$ Therefore, since $\nabla (\eta_n u) = (\nabla \eta_n) u + \eta_n (\nabla u)$ , it remains to show that $$
\| (\nabla \eta_n) u \|_{L^2(\mathbb{R}^d \setminus \{0\})} \rightarrow 0.
$$ However, because $u \in L^{\infty}(\mathbb{R}^d \setminus \{0\})$ we have $$
\| (\nabla \eta_n) u \|_{L^2(\mathbb{R}^d \setminus \{0\})} 
\leq 
\| \nabla \eta_n \|_{L^2(\mathbb{R}^d \setminus \{0\})} 
\| u \|_{L^{\infty}(\mathbb{R}^d \setminus \{0\})} 
$$ and therefore it suffices to show that $$
\| \nabla \eta_n \|_{L^2(\mathbb{R}^d \setminus \{0\})} 
\rightarrow 0.
$$ Let $B_n = \{ x \in \mathbb{R}^d : \| x\|\ \leq 2/n \}$ . We see that $$
\| \nabla \eta_n \|_{L^2(\mathbb{R}^d \setminus \{0\})}^2
= \int_{B_n} \| \nabla \eta_n \|^2
\leq n^2  \| \nabla \eta \|_{L^{\infty}(\mathbb{R}^d \setminus \{0\})}
\int_{B_n} 1
\leq C \frac{n^2}{n^d}.
$$ Thus if $d \geq 3$ we get $\| \nabla \eta_n \|_{L^2(\mathbb{R}^d \setminus \{0\})} 
\rightarrow 0$ as desired, completing the proof. [1]: Brezis, Haim , Functional analysis, Sobolev spaces and partial differential equations , Universitext. New York, NY: Springer (ISBN 978-0-387-70913-0/pbk; 978-0-387-70914-7/ebook). xiii, 599 p. (2011). ZBL1220.46002 .","['sobolev-spaces', 'functional-analysis']"
4294983,"How is ""$f(x)$ is the set of all children of $x$"" a function?","How is this a function? $f(x)$ is the set of all children of $x$ On my slides it says that: Though this f is a function, it is NOT a $H \to H$ function, because each person is associated with a set of people rather than one person. (This $f$ is a $H \to P(H)$ function.) I have some questions I thought a function associates each element from the domain to exactly one element in the codomain but I have never seen this kind of function where each element in the domain is mapped to a set of elements. How is this a function? How would that work? If someone could show me visually through a diagram that would be fantastic Some people don't have children, so that means elements in the domain wouldn't be mapped, so how would this be a function? How is the power set of $H$ going to be the set of children? Why are we using the domain elements to create the codomain (since we're taking the power set of $H$ ...our domain)? Thanks in advance",['functions']
4295057,resolve differential equation without using hypergeometric function?,"I can't resolve this differential equation $$dx=\left(\frac{a +be^{\frac{y}{c}}}{a+b}\right)^{cd} dy,$$ where $a, b, c, d \in \mathbb{R_{\geq 0}}.$ I tried with separable variables, i.e $$x=\int_{0}^{y}\left(\frac{a +be^{\frac{y}{c}}}{a+b}\right)^{cd} dy \\= \frac{1}{(a+b)^{cd}}\int_{0}^{y}\left(a +be^{\frac{y}{c}}\right)^{cd} dy,$$ but i can't find a solution without calling on Hypergeometric function. Appreciate your help!!!","['integration', 'ordinary-differential-equations']"
4295068,"Show that the function $u \in L^n(B^0(0,1))$ for $n > 1$.","I am working on Exercise 5.10.14 in Evans's Partial Differential Equations. I am trying to show that the function $$
u(x) := \log\Bigg(\log\Bigg(1 + \frac{1}{|x|}\Bigg)\Bigg)
$$ belongs to $L^n(B^0(0,1))$ with $n >1$ ,  where $B^0(0,1)$ is the open unit ball. By applying polar coordinates, we have. \begin{align*}
\int_{B^0(0,1))}\log\Bigg(\log\Bigg(1 + \frac{1}{|x|}\Bigg)\Bigg)^n\text{d}x &= \int_0^1\int_{\partial B^0(0,r))}\log\Bigg(\log\Bigg(1 + \frac{1}{|x|}\Bigg)\Bigg)^n\text{d}S(x)dr\\
&= \int_0^1 \log\Bigg(\log\Bigg(1 + \frac{1}{r}\Bigg)\Bigg)^n\Big(n\alpha(n)r^{n-1}\Big)\text{d}r\\
&=n\alpha(n) \int_0^1 \log\Bigg(\log\Bigg(1 + \frac{1}{r}\Bigg)\Bigg)^nr^{n-1}\text{d}r\\
\end{align*} From there, I thought about applying the change of variables $w = \log\Bigg(1 + \frac{1}{r}\Bigg)$ which after a bit of work, I think yields $$
-n\alpha(n)\int_{w(0)}^{w(1)} (r\log(w))^n\text{d}w
$$ I think this is infinite though. Any thoughts, hints, ideas?","['integration', 'functional-analysis', 'analysis', 'partial-differential-equations']"
4295089,"Simplifying $\cosh x + \sinh x$, $\cosh^2 x + \sinh^2 x$, $\cosh^2 x - \sinh^2 x$ using only the Taylor Series of $\cosh,\sinh$","I was trying to solve the following question: \begin{align*}
\sinh x &= x + \frac{x^3}{3!} + \frac{x^5}{5!} + \cdots \\
\cosh x &= 1 + \frac{x^2}{2!} + \frac{x^4}{4!} + \cdots
\end{align*} Using only this information, calculate $\cosh x + \sinh x$ , $\cosh^2 x + \sinh^2 x$ , and $\cosh^2 x - \sinh^2 x$ . Calculating $\cosh x + \sinh x$ was easy because it's just the Taylor Series of $e^x$ , but dealing with squaring is where it gets difficult because the Taylor Series are infinite. How can I circumvent the infinite portion to get $\cosh^2 x$ and $\sinh^2 x$ ?","['power-series', 'calculus', 'taylor-expansion']"
4295093,"Complex Analysis: Analytical Function $ f(z) ={z^{2}}/({\mathrm{e}^{x} \cos y+i \mathrm{e}^{x} \sin y})\,$?","Problem In which region of the complex plane is the following function analytic? $$f(z) = \dfrac{z^{2}}{\mathrm{e}^{x} \cos y+i \mathrm{e}^{x} \sin y}$$ If the function has a derivative over its domain, determine $f'(z)$ . Progress First I expressed $f (z)$ as $u (x, y) + v (x, y).$ $$f(z) = \left[ \dfrac{x.\cos y + y.\sin y}{\mathrm{e}^x} + \left( \dfrac{y\cos y + x\sin y}{\mathrm{e}^x} \right) i \right],$$ $$f(z) = u(x,y) + v(x,y).$$ Then I used the Cauchy-Riemann equations $\dfrac{\partial u}{\partial x} = -\left(\cos\left(y\right)x+y\sin\left(y\right)-\cos\left(y\right)\right)\mathrm{e}^{-x}$ $\dfrac{\partial u}{\partial y} = \mathrm{e}^{-x}\left(-x\sin\left(y\right)+\sin\left(y\right)+y\cos\left(y\right)\right)$ $\dfrac{\partial v}{\partial y} = \mathrm{e}^{-x}\left(-y\sin\left(y\right)+x\cos\left(y\right)+\cos\left(y\right)\right)$ $\dfrac{\partial v}{\partial x} = -\left(\sin\left(y\right)x-\sin\left(y\right)+y\cos\left(y\right)\right)\mathrm{e}^{-x}$ then $\begin{aligned}
&\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y} \\
&\frac{\partial v}{\partial x}=-\frac{\partial u}{\partial y}
\end{aligned}$ But I don't really understand the rationale for using the Cauchy-Rieman equations in this problem, and I can't determine what the region is. I am trying to see it from the most essential aspect (using epsilon and delta, and compact sets) but I don't understand.","['complex-analysis', 'cauchy-riemann-equations', 'functions']"
4295108,Seeking References for Facts from Diamond & Shurman,"I am looking for references for the two facts found in the bullet points at the bottom of p. 386 of Diamond & Shurman's ""A First Course in Modular Forms"" For a field $k$ and a curve $X/k$ of genus $g$ , if $M$ is coprime to the characteristic of $k$ , $$\operatorname{Pic}^0(X)[M] \cong \left(\mathbb Z/M\mathbb Z \right)^{2g}$$ If a curve $X/\mathbb Q$ has good reduction at prime $p \nmid M$ , then the reduction map is injective on $\operatorname{Pic}^0(X)[M]$ .","['algebraic-curves', 'modular-forms', 'algebraic-geometry']"
4295114,"Prove $\int_0^1\frac{\text{Li}_2(-x^2)}{\sqrt{1-x^2}}\,dx=\pi\int_0^1\frac{\ln\left(\frac{2}{1+\sqrt{1+x}}\right)}{x}\,dx$","I managed here to prove $$\int_0^1\frac{\text{Li}_2(-x^2)}{\sqrt{1-x^2}}\,dx=\pi\int_0^1\frac{\ln\left(\frac{2}{1+\sqrt{1+x}}\right)}{x}\,dx$$ but what I did was converting the LHS integral to a series then converting the series to the RHS integral. Is it possible to relate the two integrals without going through the series?","['integration', 'definite-integrals', 'special-functions', 'alternative-proof', 'polylogarithm']"
4295118,Probability that $6$ of $9$ coin flips yield heads given that the first flip yields tails [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question ANSWER ONLY part (c) Question : When coin 1 is flipped, it lands on heads with probability $\dfrac{4}{5}$ ; when coin $2$ is flipped it lands on heads with probability $\dfrac{7}{10}$ . (a)    If coin $1$ is flipped $11$ times, find the probability that it lands on heads at least $9$ times. (b)   If one of the coins is randomly selected and flipped $9$ times, what is the probability that it lands on heads exactly $6$ times? (c) In part (b), given that the first of these $9$ flips lands on tails, find the conditional probability that exactly $6$ of the $9$ flips land on heads. I got the answer to part (c) as $0.19688$ but it is wrong. My solution: $$\left(\left.\frac45\middle/\Big(\frac45+\frac7{10}\Big)\right.\right)\times\left({^8C_5}(\frac45)^5(\frac15)^3\right)+\left(\left.\frac7{10}\middle/\Big(\frac7{10}+\frac45\Big)\right.\right)\times\left({^8C_5}(\frac7{10})^5(\frac3{10})^3\right)
=019688$$","['conditional-probability', 'combinatorics', 'probability']"
4295122,Bijection $\mathbb{N} \to \mathbb{N}$,"I'm trying to write a ""non-trivial"" (i.e., non-identity) bijection $\mathbb{N} \to \mathbb{N}$ , where I define $\mathbb{N}$ to exclude $0$ . Pictorally, the map I have in mind works, sending odds to evens and evens to odds: $$f(n) = 
\begin{cases}
n + 1 & & \text{if $n$ is odd} \\ 
n - 1 & & \text{ if $n$ is even}
\end{cases}. 
$$ The map is certainly surjective. Let $m \in \mathbb{N}$ . If $m$ is even, $m-1$ is odd and a natural number, and we have $f(m) = (m-1) + 1 = m$ . If $m$ is odd, then $m+1$ is even, and we have $f(m+1) = (m+1)-1 = m$ . Injectivity is tougher to prove because of the case work. Suppose $f(a) = f(b)$ for $a,b \in \mathbb{N}$ . If $a,b$ are both odd, then $a + 1 = b + 1$ , so $a = b$ . If $a,b$ are both even, then $a-1 = b - 1$ , then $a = b$ . I really need to rule out the case where one of $a$ or $b$ is odd and the other is even. Suppose without loss of generality that $a$ is odd and $b$ is even. Then $f(a) = a + 1$ and $f(b) = b - 1$ . So $a + 1 = b - 1$ , so $a = b - 2$ . I don't know how to get a contradiction out of this.","['proof-explanation', 'functions']"
4295139,In need of a combinatorial proof of $1*2 + 2*3 + 3*4 + ... + n(n+1) = 2*{n+2\choose 3}$ [duplicate],"This question already has answers here : Proof of the hockey stick/Zhu Shijie identity $\sum\limits_{t=0}^n \binom tk = \binom{n+1}{k+1}$ (20 answers) Closed 2 years ago . $$1*2 + 2*3 + 3*4 + ... + n(n+1) = 2*{n+2\choose 3}$$ for natural numbers $n$ . For my discrete maths class, we are learning combinatorial proofs, and my prof handed this problem as practice. I'm aware of the process of a combinatorial proof (counting the same thing in two different ways), but I have no idea how to start this problem or how to relate the LHS to a counting problem. All I know is the RHS is twice the number of ways to make $3$ element subsets of a set of $n+2$ elements. A possible hint I found from a related problem in my book is to consider a set $$S = \{0, 1, 2, 3, ... , n, n+1\}$$ which has $n+2$ elements.","['combinatorial-proofs', 'discrete-mathematics']"
4295146,"Is it really in ""polar coordinates"" if there are no $\hat{r}$ and $\hat{\phi}$?","Context. Let's say that I am asked to write a $d\vec{r}$ vector along a circular path. I will sometimes hear my classmates say, ""We should use polar coordinates,"" by which they usually mean $$\tag1 d\vec{r} = \langle -r \sin \phi \ d\phi , r \cos \phi \ d\phi \rangle,$$ as opposed to rectangular coordinates, meaning $$\tag2 d\vec{r} =\left\langle dx, \frac{-x \ dx}{\sqrt{r^2 - x^2}} \right\rangle.$$ But I guess I would be tempted to say that (1) is still in rectangular coordinates since $d\vec{r} = \langle -r \sin \phi \ d\phi , r \cos \phi \ d\phi \rangle$ implies $d\vec{r} = (-r \sin \phi \ d\phi) \hat{x} + (r \cos \phi \ d\phi) \hat{y}$ , which is still written in the $\{\hat{x}, \hat{y}\}$ basis. And that to truly write this $d\vec{r}$ in polar coordinates would be to write $$\tag3 d\vec{r} = 0 \hat{r} + r d\phi \ \hat{\phi},$$ i.e. written in the $\{\hat{r}, \hat{\phi}\}$ basis. (...or maybe is it now a coordinate frame? I'm not sure.) Question. I guess my question is: of (1), (2), and (3), what would each representation be called? Which is in ""polar coordinates""? Thanks so much!","['multivariable-calculus', 'vector-analysis', 'polar-coordinates']"
4295186,Which pairs of groups are quotients of some group by isomorphic subgroups?,"We know that there exists a group $G$ with normal subgroups $N_1,N_2$ where $N_1\cong N_2$ but ${G\over N_1}\not\cong {G\over N_2}$ . Since this is the case it is natural to wonder about which pairs of groups can each be represented by quotients of some group by isomorphic subgroups. In particular, for which pairs of groups $H_1,H_2$ do there exist a group $G$ and normal subgroups $N_1,N_2$ of $G$ such that $N_1\cong N_2$ , $H_1\cong {G\over N_1}$ , and $H_2\cong {G\over N_2}$ ? Are there any nice conditions on such pairs guaranteeing the existence of these quotients? Or is it perhaps even possible to find such a $G,N_1,N_2$ whenever $H_1$ and $H_2$ are of the same order?","['group-theory', 'abstract-algebra', 'finite-groups']"
4295216,Rationalizing $\frac{\sqrt{1+\cos x}+\sqrt{1-\cos x}}{\sqrt{1+\cos x}-\sqrt{1-\cos x}}$ in two ways gives different answers,"I have a doubt see when we rationalize denominator of expression $$\frac{\sqrt{1+\cos x}+\sqrt{1-\cos x}}{\sqrt{1+\cos x}- \sqrt{1-\cos x}}$$ we get answer $$\frac{1+\sin x}{\cos x}$$ but when we rationalize numerator we get $$\frac{\cos x}{1+\sin x}$$ How is this possible, because rationalizing means just multiplying by $1$ ?","['rationalising-denominator', 'algebra-precalculus', 'trigonometry']"
4295237,Why are we using combination in probability question when the objects are identical?,"There are $3$ blue , $4$ red ,and $5$ green identical balls in an urn. We draw $3$ balls without replacement. What is the probability that all of these $3$ balls have different colors? $\mathbf{\text{Please read until the end }}$ This is a basic probabilty question. I solved it such that $$3! \times \frac{3}{12} \times \frac{4}{11} \times \frac{5}{10}=\frac{3}{11}$$ However, there is also another approach such that $$\frac{\binom{3}{1}\binom{4}{1}\binom{5}{1}}{\binom{12}{3}}=\frac{3}{11}$$ $\mathbf{\text{My Question}:}$ We know that when we use combination , it is valid for selecting $r$ objects among $n$ $\color{red}{\text{distinct}}$ objects. However , as you see , the question says that the balls are identical among each types. Then , why did we use combination ? For example , i think that using generating functions for finding denominator is more sensible than using combination ,i.e , $\binom{12}{3}$ . When i ask it to someone , they said that ""oh okey , but it works"". Hence , i want a good explanation from you guys. Do not say me "" we use combination because it gives us all possible arrangemets,i.e $3!$ "" ,  because i think that it cannot be the reason to use combination. I think also that we betray the definition of combination by using it to select identical objects. Then what is the reason behind seeing these identical objects like ""distinct"" ? What is the reason behind using combination to select identical objects ? Thanks in advance !!","['combinations', 'combinatorics', 'soft-question', 'probability-theory', 'probability']"
4295279,Does the following computer science/optimization theorem have a proof?,"I have been reading about a theorem in math called the ""Schema Theorem"" - this theorem is one of the first theorems from the field of evolutionary computing and genetic algorithms, largely responsible for justifying the use of ""genetic algorithms"" to solve optimization problems when the derivative of the objective function is either unknown or not clearly defined. ""Genetic algorithms"" can be used in many different types of optimization problems, such as finding the roots of a polynomial. For example, we could use the ""genetic algorithm"" to find the roots (the zeros) of the following polynomial (this polynomial will be referred to as the ""objective function"", i.e. the objective of the optimization/root finding): f(x, y) = x sin(4x) + 1.1y sin(2y) Essentially, the ""genetic algorithm"" would start by taking random values of ""x"" and ""y"" and record the corresponding value of ""f(x,y)"" for each random combination of ""x"" and ""y"" : the value of ""f(x,y)"" for a given combination of ""x"" and ""y"" is called the ""fitness"". The ""genetic algorithm"" works by taking many such random combinations of ""x"" and ""y"" and recording which combinations produce lower fitness values (i.e. which coordinates of ""x"" and ""y"" correspond to low elevation regions on the f(x,y) surface). The ""genetic algorithm"" then ""combines"" (i.e. ""mutates"") combinations of ""x"" and ""y"" that produced low fitness values, and re-evaluates ""f(x,y)"" at these new ""mutated combinations"" of ""x"" and ""y"". The ""genetic algorithm"" repeats this mutation process many times until it successive differences in ""f(x,y)"" are negligible, or after a predefined number of iterations - this is very useful in problems where evaluating the derivative of the objective function can be extremely time consuming or costly, or the derivative of the objective function is poorly defined (i.e. standard optimization algorithms like gradient descent and newton-raphson can not be performed). The optimization process for the ""genetic algorithm"" can be seen below: An interesting observation about the ""genetic algorithm"" is that it does not ""promise or guarantee"" convergence to the true minimum point of the objective function during the optimization process. But what the ""genetic algorithm"" does guarantee, is that ""the overall fitness of the objective function is guaranteed to improve as the number of iterations increase"". In other words, the ""genetic algorithm"" will reach closer to the true minimum of the objective function after ""m iterations"" compared to ""n iterations"", where m >> n. This idea is expressed in one of the original and fundamental theorems of genetic algorithms, called the ""Schema Theorem"" (see below): Apparently, this theorem states that ""schemas"" (e.g. combinations of ""x"" and ""y"") that produce better fitness values are more likely to ""survive after mutation"" - i.e. the expected number of ""schemas"" that produce better fitness values are likely to increase as the number of iterations increase. Question: I have spent a long time trying to either find a derivation of proof of the Schema Theorem - something which attempts to mathematically explain why the ""genetic algorithm"" is guaranteed to move towards the true optimum of the objection function as the number of iterations increase (rhetorical question: after all, why does the ""genetic algorithm"" not result in worse fitness results?). After all: Why does this inequality hold? I am very curious to see if there is a mathematical proof that justifies one of the most important and earliest theorems in ""genetic algorithms"" and evolutionary computing - does anyone know if the Schema Theorem has a proof? Thanks! Reference: https://en.wikipedia.org/wiki/Holland%27s_schema_theorem https://www.whitman.edu/Documents/Academics/Mathematics/2014/carrjk.pdf https://www.sciencedirect.com/topics/computer-science/schema-theorem (alternate statement of theorem) https://www.researchgate.net/publication/51892214_The_Exact_Schema_Theorem/link/02bfe50f6181790d85000000/download http://dynamics.org/Altenberg/FILES/LeeSTPT.pdf https://en.wikipedia.org/wiki/Genetic_algorithm https://cran.r-project.org/web/packages/GA/vignettes/GA.html Note: An Example of Optimization using the Genetic Algorithm in the R Programming Language library(GA)


  #define function to be optimized
Rastrigin <- function(x1, x2)
{
  20 + x1^2 + x2^2 - 10*(cos(2*pi*x1) + cos(2*pi*x2))
}

x1 <- x2 <- seq(-5.12, 5.12, by = 0.1)

#run optimization through the genetic algorithm (with constraints)
GA <- ga(type = ""real-valued"", 
         fitness =  function(x) -Rastrigin(x[1], x[2]),
         lower = c(-5.12, -5.12), upper = c(5.12, 5.12), 
         popSize = 50, maxiter = 1000, run = 100)


GA | iter = 1 | Mean = -37.396067 | Best =  -6.441093
GA | iter = 2 | Mean = -29.00421 | Best =  -2.51091
GA | iter = 3 | Mean = -22.89516 | Best =  -2.51091

....

GA | iter = 90 | Mean = -3.025916e+00 | Best = -9.301074e-06
GA | iter = 91 | Mean = -5.725962e+00 | Best = -9.301074e-06
GA | iter = 92 | Mean = -5.858303e+00 | Best = -9.301074e-06
....

GA | iter = 250 | Mean = -3.581667e+00 | Best = -1.103990e-06
GA | iter = 251 | Mean = -5.160585e+00 | Best = -1.103990e-06
GA | iter = 252 | Mean = -7.226593e+00 | Best = -1.103990e-06


summary(GA)

## Solution = 
##                x1           x2
## [1,] 5.722083e-05 9.223472e-05","['algorithms', 'optimization', 'inequality', 'derivatives', 'computer-science']"
4295281,Using geometry to solve quartic equations,"So I've recently come across this pretty neat geometrical way for solving depressed cubic equations where we use $x^3$ term as a cube $bx$ term as another cube and solve for x using some neat geometry, same goes for cubic and quadratic for quadratic we use $x-\frac b{2a}$ substitution to solve for $x$ , in cubic we use $x-\frac b{3a}$ substitution then turn into depressed cubic and solve the equation, now similarly with a quartic can be solved by using $x-\frac b{4a}$ substitution and then some algebra can give the value of $x$ . But I was wondering if there is a geometrical way to solve the quartic equation?","['abstract-algebra', 'geometry']"
4295302,"What are the ways to number a dodecahedral dice ""canonically"" up to rotational symmetry?","First, a ""canonical"" dice numbering, for this purpose, is one where opposite sides sum to 13. We can interpret all possible numberings as permutations of pairs of opposing faces, as well as the permutations within those pairs. This gives $2^6\times6!=46080$ possible combinations (regardless of rotational symmetry), described by the group $C_2\wr S_6$ . There are 60 rotational symmetries of a dodecahedron, described by the group $I$ (or $A_5$ ). There is only one element of $I$ (the identity) that will preserve any of the permutations; all other elements will modify the permutations in some way. By Burnside's Lemma, that means there are $\frac{46080}{60}=768$ canonical dice numberings up to rotational symmetry. This answer leaves me somewhat unsatisfied, however. It's good to have a number, but it isn't very intuitive to me. I'd really like some way to describe what these distinct combinations actually are, and that brings me to my question: Is there a group, or a group-like object, that describes the canonical dice numberings of a dodecahedral dice up to rotational symmetry? Or, alternatively, is there a combinatorical method of constructing these dice numberings? Sorry if this post is difficult to understand. I don't have a formal background at this level so my word choice might be a little off.","['finite-groups', 'symmetry', 'combinatorics', 'geometry']"
4295312,Solve the angle $\angle{DCB}$ in triangle $\triangle{ABC}$ with $\angle{A}=84^{\circ}$,"Where $\angle{A}=84^{\circ}, \angle{ACD}=42^{\circ}, BD=AC$ , find $\angle{BCD}$ . Wonder if there is solution without using trigonometric functions. I tried with getting circumcenter of triangle ABC, but seems hard to prove it forms an equilateral triangle with side AC. Also if trying from equilateral and form an Isosceles triangle with two angles of $24^{\circ}$ , and then form another isosceles triangle with top angle to be $\angle{B}=24^{\circ}$ , it is not easy to prove that $\angle{ADC}=54^{\circ}$ .","['euclidean-geometry', 'triangles', 'angle', 'geometry']"
4295331,How to compute this double integral with $dt$ and $dx$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question How to calculate the following integral? Any help is appreciated because I am not sure if there are some theorems I haven't learned to solve the question. Thanks. $\int_0^1 \frac{f(x)}{\sqrt{x}} dx $ where $f(x) = \int_1^x \frac{\ln(1+t)}{t} dt$ I just found the solution of the first two steps and it's as the following
And it is more confusing to me now. $\int_0^1 \frac{1}{\sqrt{x}} \int_1^x \frac{\ln(1+t)}{t} dt dx  $ $= -\int_0^1  \frac{\ln(1+t)}{t} dt \int_t^0  \frac{1}{\sqrt{x}} dx $ $= -2\int_0^1  \frac{\ln(1+t)}{\sqrt{t}} dt $ ... (From this step on, I guess I could solve it by letting $t=u^2$ )","['integration', 'multivariable-calculus', 'calculus']"
4295415,Coprime sequences from a linear recurrence.,"Given the linear recurrence $a_n=5a_{n-1}-2a_{n-2}$ and the initial conditions $x_0=0,x_1=1$ and $y_0=1, y_1=3$ we get the two sequences $$(x_n)=0,1,5,23,105,\ldots\\
  (y_n)=1,3,13,59,269,\ldots$$ which seem to be coprime is the sense that $gcd(x_n,y_n)=1$ for every $n$ (I checked it numerically up to $n=100$ ). To prove the conjecture I tried (without success) the induction principle and Bezout's lemma to generate numbers $r$ and $s$ with $rx_{n+1}+sy_{n+1}=1$ from the existence of numbers $p$ and $q$ with $px_n+qy_n=1$ .
Second, I solved the recurrence with the result $$x_n=\frac{\alpha^n-\beta^n}{\alpha-\beta},\quad y_n=\frac{1}{2}(x_n+\alpha^n+\beta^n)$$ where $\alpha$ and $\beta$ are the roots of the characteristic equation $x^2=5x-2$ . Again, I don't see how to derive a proof. I'm grateful for any help.","['elementary-number-theory', 'sequences-and-series']"
4295432,What can we conclude from the property $P(X<Y)=P(X>Y)=0.5$ of two random variables $X$ and $Y$?,"This is a similar question to Does the property $P(X<Y)=P(X>Y)=0.5$ imply equal expectations? I have a series of short questions related to: the medians of $X$ and $Y$ , the means, and the fact that $P(X>Y)=P(X<Y)=0.5$ . Let us introduce the following properties: \begin{align*}
P_{\mbox{prob}}: \quad P(X>Y)&=P(X<Y)=0.5,\\
P_{\mbox{mean}}: \hspace{0.9cm}\quad E[X]&=E[Y],\\
P_{\mbox{median}}: \quad med[X]&=med[Y].
\end{align*} By the post in Does the property $P(X<Y)=P(X>Y)=0.5$ imply equal expectations? we know that $P_{\mbox{prob}} \nRightarrow P_{\mbox{mean}}$ , and obviously we now that $P_{\mbox{mean}}$ and $P_{\mbox{median}}$ do not imply each other. My questions are: $P_{\mbox{prob}} \Rightarrow P_{\mbox{median}}$ ? $P_{\mbox{mean}} \Rightarrow P_{\mbox{prob}}$ ? $P_{\mbox{median}} \Rightarrow P_{\mbox{prob}}$ ? I am not sure if the continuous and discrete cases differ here. The continuous case is maybe more interesting. I tried $X$ uniformly supported on $[a,b]$ and $Y$ on $[c,d]$ such that $a+b=c+d$ so that means are equal. But then I get $P(X<Y)=P(X>Y)$ which is then not a counterexample for question 2, or 1.","['statistics', 'median', 'expected-value', 'probability', 'random-variables']"
4295433,Calculating function field of complex manifolds,"This question is related to an exercise on Huybrechts, Complex geometry . I want to calculate the algebraic dimension of the following complex manifolds: $\mathbb{P}^1$ , $\mathbb{P}^n$ , and $\mathbb{C}/\mathbb{Z}+i\mathbb{Z}$ . The algebraic dimension is defined as the trascendence degree of the function field of each complex manifold $X$ : $a(X)=\text{trdeg}_\mathbb{C} K(X)$ . For completion, the function field is the set of global meromorphic functions on the complex manifold. I have very little understanding of the tools I can use to determine these function fields, since I lack some of the algebraic background generally assumed for studying algebraic and complex geometry. Any hint or a general direction would suffice.","['complex-geometry', 'algebraic-geometry']"
4295472,Is the following operator (related to the Dirac's delta) bounded?,"Consider $\eta \colon [-1,0] \to \mathbb{R}$ of bounded variation and define the (delay) operator $\Phi \colon \mathcal{C}[-1,0] \to \mathbb{R}$ by $$\Phi f = \int_{-1}^{0} f d \eta$$ for $f \in \mathcal{C}[-1,0]$ where the integral is the Riemann-Stieltjes integral. $\Phi$ is a bounded linear operator acting on $\mathcal{C}[-1,0]$ and so by continuous embeddings it is a bounded linear operator acting on the Sobolev space $W^{1,2}[-1,0]$ . Now [Bátkai, András, and Susanna Piazzera. Semigroups for delay equations. CRC Press, 2005.] p.69 example 3.28 claims that $$\Phi f= f(-1), f \in W^{1,2}[-1,0]$$ is representable in the integral form before and it is a bounded linear operator. But $\eta$ in this case should be the Dirac's delta in $-1$ , i.e. $\eta = \delta_{-1}$ . It feels weird that it is a bounded linear operator on $W^{1,2}$ when usually distributions are defined as linear continuous functionals on $C_{0}^{\infty}$ which is a much smaller space? I mean ok this should be another way to define functionals from distributions right? From the point of view of calcultions it seems like that: $$|\Phi f|= |f(-1)| \leq |f|_{\mathcal{C}^0} \leq C |f|_{W^{1,2}}$$ where $|\cdot|_{\mathcal{C}^0}$ is the sup norm. Anyway since $\eta= \delta_{-1}$ is not a function can you define the Riemann-Stieltjes integral correctly?","['measure-theory', 'operator-theory', 'distribution-theory', 'functional-analysis', 'semigroup-of-operators']"
4295502,"If $\mathbb{E}\left[X\mid\mathcal{G}\right]$ is a random variable, then what is $\mathbb{E}\left[X\mid\mathcal{G}\right](\omega)$?","To be honest, I'm really struggling with the intuition of conditional expectation where we condition on a sub $\sigma$ -algebra. The definition in my lecture notes is as follows: Let $(\Omega,\mathcal{F},\mathbb{P})$ a probability space and $\mathcal{G}\subseteq\mathcal{F}$ a sub $\sigma$ -algebra and $X\in\mathcal{L}^1(\Omega,\mathcal{F},\mathbb{P})$ . The conditional expectation $\mathbb{E}\left[X\mid\mathcal{G}\right]$ is a subset of random variables in $Y\in\mathcal{L}^1(\Omega,\mathcal{F},\mathbb{P})$ for which: $$\int_GYd\mathbb{P}=\int_GXd\mathbb{P}\text{ for all }G\in\mathcal{G}$$ Although I technically understand the requirement in the definition, I have no intuition at all for what a conditional expectation in this sense means. If $\mathbb{E}\left[X\mid\mathcal{G}\right]$ is a random variable, then what is $\mathbb{E}\left[X\mid\mathcal{G}\right](\omega)$ ? What does it mean for conditional expectations to be independent?","['conditional-expectation', 'probability-theory', 'intuition']"
4295539,equivalence of two conditions on two matrices,"I would like to ask for your view on two different conditions for two matrices. I have the two matrices mentioned above, say $\Lambda_1$ and $\Lambda_2$ . Their dimensions are, for both matrices, $N\times P$ , and $N>P$ .
In some papers I am reading, I found that the following condition is needed $\Lambda_2 \neq \Lambda_1 G$ for all matrices $G$ , with $G$ of dimensions $P\times P$ . Nothing else is said on $G$ . Working on a related problem, I need the condition $\Lambda_2=\Lambda_1C + \tilde{\Lambda}_1$ where $C$ is some matrix of dimensions $P\times P$ . I do not have any specific conditions on $C$ (e.g. it could have full rank or not, it could be nonzero or all zeros, etc...). On the other hand, $\tilde{\Lambda}_1$ is an $N\times P$ matrix which belongs in the orthogonal space of $\Lambda_1$ - I mean that $\Lambda_1^{\prime}\tilde{\Lambda}_1=0$ My question is: how different are the two conditions mentioned above? Are they entirely equivalent, do they imply each other, does one imply the other at all...? Any comment would be welcome.","['matrices', 'orthogonality']"
4295540,System of three linear congruences with three variables,"I have the following system $$
\left\{ 
\begin{array}{c}
5x+20y+11z \equiv 13 \pmod{34}\\ 
16x+9y + 13z \equiv 24\pmod{34} \\ 
14x+15y+15z \equiv 10\pmod{34}
\end{array}
\right. 
$$ I'm still fairly new to this so I would like anyone so verify my solution. First, I multiplied equations $2$ and $3$ by $5$ , it's a regular transformation because $5$ and $34$ are coprime. After that, the coefficient next to $x$ in the second equation is $80$ , and the coefficient of $x$ in the third equation is $70$ , both of which are a multiply of 5 so we can eliminate them using the first equation. Hence we get $$
\left\{ 
\begin{array}{c}
5x+20y+11z \equiv 13 \pmod{34}\\ 
-275y -111z \equiv -88\pmod{34} \\ 
-245y-161z \equiv -198\pmod{34}
\end{array}
\right. 
$$ Next, I multiplied the third equation by $55$ . It is a regular transformation because $55$ and $34$ are coprime. The coefficient of $y$ in the third equation is $-13475$ , which is a multiple of $-275$ , so we can eliminate it by multiplying the second equation by $-49$ and adding it to the third equation. Now we are left with $-3416 \equiv -6578\pmod{34}$ . If we multiply this equation by $-1$ and reduce the coefficients (because they are larger than the modulus) we get $$16z \equiv 16\pmod{34}$$ We have two typical solutions, $z=1$ , $z=18$ . Now, I proceeded to plug in both values of $z$ gradually and I got two solutions: $x \equiv 22\pmod{34}$ , $y \equiv 15\pmod{34}$ , $z \equiv 1\pmod{34}$ and $x \equiv 5\pmod{34}$ , $y \equiv 32\pmod{34}$ , $z \equiv 18\pmod{34}$ NOTE : I omitted the process of plugging both values of $z$ one by one into the equations because I'm fairly sure I know how to proceed from there. I'm interested in knowing if my method of reducing the system to one equation with one variable is correct or not. Thanks! EDIT : After plugging in both sets of solutions, I get that both sets satisfy equations $(1)$ and $(2)$ , but in both cases of solutions I get that the third equation ends up being $4 \equiv 10\pmod{34}$ which is false. Where did I go wrong?","['congruence-relations', 'modular-arithmetic', 'discrete-mathematics']"
4295543,A problem about characteristic polynomial,"I would like you to help me with a problem. If we consider a matrix $$A = \begin{bmatrix}
       4 & 2 &-1 & 6\\
       3 & 0 & 2 & 4\\
      -1 & 0 & 3 & 0\\
       0 & 0 & 5 & 0
      \end{bmatrix}$$ the problem says that the characteristic polynomial is $P_A(\lambda) = (2-\lambda)(4-\lambda)(-1-\lambda)(5-\lambda)$ . However, when we calculate directly $$P_A(\lambda) = \det(A - \lambda I),$$ where $I$ is the identity metrix, we arrive at $$P_A(\lambda) = \lambda^4-7\lambda^3+5\lambda^2+52\lambda+40.$$ Hence, I have the following question. Is it possible to find another equivalent $A$ matrix using some property that preserves the characteristic polynomial and arrives at the proposed result? What am I doing wrong?","['matrices', 'linear-algebra', 'characteristic-polynomial']"
4295559,"Prove that for every $k$ we can find a sequence of consecutive numbers, such all the products of their number of divisors are not perfect squares","Prove that for every $k$ , there exists a sequence $a_1, a_2, \dots, a_k$ , consecutive natural numbers, such that, for every $i \neq j$ , $\tau(a_i) \cdot \tau(a_j)$ is not a perfect square. We are denoting $\tau(\ell)$ the number of divisors of a natural number $\ell$ . Approach, ideas: Consider $\mathcal{A}$ the set of numbers for which is the property is true (they are the begining of a sequence that contains consecutive numbers with that property). And consider $\mathcal{B} = \complement \left(\mathcal{A}\right)$ . We should prove that $\mathcal{B}$ is infinite,and that it has arbitrarily large distances between consecutive elements of $\mathcal{B}$ , to imply that the property is true for each $k$ . I think $\rho_\mathcal{B}$ should equal $0$ in this case, as it should contain arbitrarily large gaps, so arbitrarily small values of the fraction: $$\frac{|\mathcal{B}\cap [1, N]|}{N}$$ Therefore, the density: $$\rho = \lim_{n\to\infty} \frac{|\mathcal{B}\cap [1, N]|}{N}$$ However, I am not able to compute this limit to complete the proof.","['analytic-number-theory', 'number-theory', 'algebraic-number-theory', 'density-function']"
4295610,How to find du and dv?,Find $du$ and $dv$ if $u+v=x+y$ and $\frac{\sin(u)}{\sin(v)}=\frac{x}{y}$ . How to solve this? Found almost an answer: But how do we get $du=...$ from the second?,"['implicit-differentiation', 'analysis']"
4295663,"Find the smallest natural number $n$ such that every $n$-element subset of $S=\{1,2,\dots,280\}$ contains $5$ pairwise relatively prime numbers","A friend gave me the following question to solve- Let $S=\{1,2,\dots ,280\}$ . Find the smallest natural number $n$ such that every $n$ -element subset of $S$ contains $5$ pairwise relatively prime numbers. I am pretty sure, the solution has to do something with using Pigeonhole Principle, but I can't see how to use it. I can see that we need to somehow make a ""smart guess"" and then use PHP to prove it. But, I can't figure out that guess. Can somebody please help me out...","['pigeonhole-principle', 'combinations', 'combinatorics', 'coprime']"
4295669,"Showing that $C^1([0,1])$ space with the norm $||f||^2=\int_{0}^{1}|f|^2+\int_0^{1}|f'|^2$ is not a Banach Space","I am given the $C^1([0,1])$ space containing $f$ -s such that $f,f'\in C[0,1]$ , with the norm $||f||^2=\int_{0}^{1}|f|^2+\int_0^{1}|f'|^2$ and asked to show that it is not a Banach Space. What I am not so sure about is how; do I need to find $f_n$ such that $\int_0^{1}|f_n'|^2$ is infinite, or should I rather find $f'_n$ such that $\lim_{n\to \infty}f'n\notin C[0,1]$ , or is any of the two will be sufficient? I am having trouble to find ANY Cauchy sequence of functions, especially the conditioned ones. Should I start looking for derivatives sequence and integrate them?","['banach-spaces', 'complete-spaces', 'functional-analysis']"
4295728,What numbers are possible for the fractional chromatic numbers?,"Question: What numbers are possible as fractional chromatic numbers ? Clearly, all nonnegative integers are possible, just use the complete graph $K_n$ on $n$ vertices. Also, if we limit our scope to only finite simple graphs, then irrational numbers cannot be fractional chromatic numbers. For nonintegers, it is clear that fractional chromatic numbers have to be $\geq 2$ , because if the graph has an edge, then at least $2n$ colours are needed for the two connected vertices if we assign $n$ colours to a vertex. There are hence two main questions here: Can every rational number $\geq 2$ be the fractional chromatic number of a finite simple graph? Can every number $\geq 2$ be the fractional chromatic number of an infinite simple graph?","['graph-theory', 'combinatorics', 'coloring']"
4295731,Infinite binomial series over a column of Pascal's triangle: $F_k(x)= \sum\limits_{n=0}^\infty \binom{2n+k}n x^{2n+k}$,"Is there any closed form formula (or an equivalent) for this binomial infinite series : $$F_k(x)=  \sum_{n=0}^{\infty} \binom{2n+k}{ n } x^{2n+k} $$ in which $|x|<1$ and $k$ is a given integer? This is a sum over a (odd or even) infinitely long column of Pascal's triangle : For example, above : $$F_3(x)= x^3+ 5 x^5 + 21 x^7 + 84 x^9 +... $$ $F_k(x)$ appears in the Fourier Series development of $\frac{1}{1+a\,sin(x)}$ : the k-th harmonic would be of amplitude $F_k(a)$","['fourier-series', 'binomial-coefficients', 'sequences-and-series']"
4295791,What is the series $ \dfrac{1}{\left( x-{y}^{2} \right) \left(1- yx \right) \left( {x}^{2}-y \right)} $ expansion?,"Consider the series expansion $$
\frac{1}{\left( x-{y}^{2} \right)  \left(1- yx \right)  \left( {x}^{2}-y \right)}=P_0(y)+P_1(y)x+P_2(y)x^2+\cdots+P_n(y) x^n+\cdots.
$$ For small $n$ we have \begin{gather*}
P_0(y)=\frac{1}{y^3},\\
P_1(y)={\frac {{y}^{3}+1}{{y}^{5}}},\\
P_2(y)={\frac {{y}^{6}+2\,{y}^{3}+1}{{y}^{7}}},\\
P_3(y)={\frac {{y}^{9}+2\,{y}^{6}+2\,{y}^{3}+1}{{y}^{9}}},\\
P_4(y)=\frac{{y}^{12}+2\,{y}^{9}+3\,{y}^{6}+2\,{y}^{3}+1}{y^{11}},\\
P_5(y)={\frac {{y}^{15}+2\,{y}^{12}+3\,{y}^{9}+3\,{y}^{6}+2\,{y}^{3}+1}{{y}^{
13}}}.
\end{gather*} Question: What is a close expression for $P_n(y)$ ? My attempt.  The $P_n(y)$ has  a form $$
 P_n(y)=\frac{1}{y^{2n+3}}(c_{n,1}+c_{n,2}y^3+c_{n,3}y^{6}+\cdots+с_{n,k} y^{3(k-1)}+\cdots+c_{n,n+1}y^{3n})
$$ for some  unknown sequence $c_{n,k}.$ By empirical way I have found that $$
c(n,k)= \left \{
\begin{array}{l}
\min(n{-}k+2,k) , 1 \leq    k \leq n+1, \\
\\
0,  \text{ otherwise, }
\end{array}
\right.
$$ but I can't find a rigorous proof. Any help?","['power-series', 'combinatorics', 'sequences-and-series']"
