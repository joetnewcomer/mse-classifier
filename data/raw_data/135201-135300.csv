question_id,title,body,tags
2135839,Prove or disprove the following inequality?,"Let $Y \geq X \geq 0$ be two random variables with cdf $F_X$ and $F_Y$ such that $var(Y) \leq var(X) < \infty$. Is $$
\int_{z + \mathbb{E}[X]}^{\infty}\mathbb{E}[(X - t)^+]F_X(t)dt \geq \int_{z + \mathbb{E}[Y]}^{\infty}\mathbb{E}[(Y - t)^+]F_Y(t)dt
$$ 
true for all $z \in \mathbb{R}$? Edit: Moreover, assume that $esssup(X) = esssup(Y)$","['probability-theory', 'probability']"
2135840,Probabilistic classical algorithm on Deutsch-Jozsa problem,"Here is the description of Deutsch-Jozsa problem: Let $f: \{0,1\}^n \mapsto \{0,1\}$ be a function promised to be either constant or balanced ('balanced' means that $f$ outputs as many 0's as 1's). I need to show that a probabilistic classical algorithm making two evaluations of $f$ can with probability at least 2/3 correctly determine whether $f$ is constant or balanced. There is also a hint: Your guess does not need to be a deterministic function of the results of the two queries. Your result should not assume any particular a priori probabilities of having a constant or balanced function. I'm a bit lost here.  My thinking is that if the two evaluations are the different, then $f$ is definitely balanced.  Otherwise, $f$ could be either constant or balanced.  But the chance of success would be depending on the probability of $f$ being constant, which is against the given hint. How should I approach this problem?","['quantum-computation', 'algorithms', 'probability']"
2135888,Intuition behind the change of basis matrix,"My linear algebra class isn't particularly rigorous and my professor doesn't really provide much intuition for most of the theorems we learn, either. Because of this, I've made an effort to make sense of the theorems beyond ""the math works out to this result."" I'm finding difficulty finding intuition for the change of basis matrix. In class, we learned that for some subspace $V$, if we take a non-orthonormal basis, $\mathfrak{B} = \left( \vec{v_1}, \ldots, \vec{v_n} \right)$, and find an orthonormal basis using the Gram-Schmidt process, $\mathfrak{U} = \left( \vec{u_1}, \ldots, \vec{u_n} \right)$, we can write $$
\begin{pmatrix}
     |    &     |     &        &   | \\
\vec{v_1} & \vec{v_2} & \cdots & \vec{v_n} \\
     |    &     |     &        &   |
\end{pmatrix}
=
\begin{pmatrix}
     |    &     |     &        &   | \\
\vec{u_1} & \vec{u_2} & \cdots & \vec{u_n} \\
     |    &     |     &        &   |
\end{pmatrix} R,
$$ where $R$ is the change of basis matrix (I think) whose entries are related to the decomposition of each vector in $\mathfrak{B}$. For simplicity, I'll call the first matrix $B$ and the second, $U$. From there, it follows that for a vector $\vec{x}$, $$
\begin{align}
\vec{x} = B\left[ \vec{x} \right]_\mathfrak{B} &= U\left[ \vec{x} \right]_\mathfrak{U} \\
UR\left[ \vec{x} \right]_\mathfrak{B} &= U\left[ \vec{x} \right]_\mathfrak{U} \\
\Rightarrow R\left[ \vec{x} \right]_\mathfrak{B} &= \left[ \vec{x} \right]_\mathfrak{U}
\end{align}
$$ for that same $R$. So I think I've found intuition for this matrix $R$ in the context of the first equation: $R$ is sort of like an ""un-decomposition"" matrix because the columns tell you how to retrieve the vectors from the original basis using the orthonormal basis you created. However, from a linear transformation standpoint, it doesn't make much sense to me. In my head, I would take my matrix $U$ and transform that into $B$ using $R$, so I would expect the first equation to look like $$B = R(U)$$ but interestingly enough, it appears that $R$ is being transformed by $U$. So my first question is how do I make sense of this equation from a transformation perspective? Another thing I'm having trouble understanding is the relationship between the different coordinates of $\vec{x}$, which is the following equation: $$R\left[ \vec{x} \right]_\mathfrak{B} = \left[ \vec{x} \right]_\mathfrak{U}$$ $R$ is this ""un-decomposition"" matrix, but it also somehow magically transforms the coordinates of $\vec{x}$ with respect to $\mathfrak{B}$ to coordinates with respect to $\mathfrak{U}$. So, my next question is how come the same matrix $R$ serves both purposes? My main question overall is how can I interpret this matrix $R$ from a geometric/linear transformation perspective? Thank you in advance. EDIT: After exploring elementary matrices (which my class skipped), I found something very interesting. If we have an elementary matrix $L$ and a matrix of interest $A$, then the rows of $LA$ are given by linear combinations of the rows of $A$, with the coefficients given by the corresponding row of $L$. For example, if we had the product $$
\begin{pmatrix}
1 & -2 & 0 \\
0 &  1 & 0 \\
0 &  0 & 0
\end{pmatrix}
\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{pmatrix}
$$ the first row of the product would be given by $$
  1 \cdot \begin{pmatrix}  1 &  2 &  3 \end{pmatrix}
- 2 \cdot \begin{pmatrix}  4 &  5 &  6 \end{pmatrix}
+ 0 \cdot \begin{pmatrix}  7 &  8 &  9 \end{pmatrix}
=         \begin{pmatrix} -7 & -8 & -9 \end{pmatrix}
$$ and so on. This is quite similar to the situation with the first equation, $B = UR$. However, in that equation, what's happening is that we have linear combinations of the columns, as opposed to the rows as with left multiplication of elementary matrices. So, my idea was that if we put the elementary matrix to the right of our matrix of interest, then we'll have a linear combination of the columns instead. It turns out that my intuition was true. Using the same example as before, we expect the first column to be $$
  1 \cdot \begin{pmatrix}  1 \\  0 \\  0 \end{pmatrix}
+ 4 \cdot \begin{pmatrix} -2 \\  1 \\  0 \end{pmatrix}
+ 7 \cdot \begin{pmatrix}  0 \\  0 \\  1 \end{pmatrix}
=         \begin{pmatrix} -7 \\  4 \\  7 \end{pmatrix}
$$ and it is indeed correct! This idea provides intuition for the fact that $(AB)^T = B^TA^T$: $A$ tells us how to find the rows of $AB$ using linear combinations of the rows of $B$, so if we turn each row into columns, we expect to change the order of multiplication since left multiplication tells us how to find rows and right multiplication tells us how to find columns. Interpreting $R$ as an elementary matrix explains the order of multiplication in $B = UR$: $R$ is multiplied on the right and tells us how to find the columns vectors of $B$ as a linear combination of the column vectors of $U$. Lastly, it gives an idea as to why $R$ is multiplied on the left of our coordinate matrices: each row of the coordinate matrix corresponds to a vector in the associated basis. In this particular scenario, each row in $\left[ \vec{x} \right]_\mathfrak{B}$ corresponds to a column vector in $B$, which corresponds to a linear combination of column vectors in $U$. So, putting the rows in the correct linear combination in $\left[ \vec{x} \right]_\mathfrak{B}$ is analogous to putting columns in some linear combination together, which is why a single $R$ serves these two purposes.",['linear-algebra']
2135918,"Does L'Hopitals Rule hold for second derivative, third derivative, etc...?","Does L'Hopitals Rule hold for second derivative, third derivative, etc...? Assuming the function is differential up to the $k$th derivative, is the following true? $$\lim_{x\to c} \frac{f(x)}{g(x)} = \lim_{x\to c} \frac{f'(x)}{g'(x)} = \lim_{x\to c} \frac{f''(x)}{g''(x)} = \cdots = \lim_{x\to c} \frac{f^{(k)}(x)}{g^{(k)}(x)} = \lim_{x\to c} \frac{f^{(k+1)}(x)}{g^{(k+1)}(x)}$$","['calculus', 'limits']"
2135938,How do you express $\dfrac 1{1+x}$ as an infinite polynomial?,I have the function $\dfrac 1{1+x}$ which I want to express as an infinite polynomial. I believe the correct term is Taylor Series. How do I solve this problem?,"['derivatives', 'taylor-expansion', 'polynomials', 'calculus', 'sequences-and-series']"
2135945,Units of the integral group ring,"Let $G$ be a group of order $\le 4$. Find the unit group of the group ring $\mathbb Z[G]$. Trivially, if $|G|=1$, then the unit group is $\{\pm e\}$, where $e$ is the identity element fo $G$. For $|G| \ge 2$, what should I do? If $|G|=2$, let $a$ be a nonidentity element of $G$, and I let $(ma+ne)(pa+qe)=e$, and tried a long, messy calculation and found the answer. However, I totally stuck in the case $|G| \ge 3$. How should I find all the units in this case? I already know that a group of order 3 is cyclic, and a group of order 4 is either cyclic or isomorphic to Klein-4 group. Also, for the case $|G|=2$, is there more elegant solution without the brutal computation(just what I did)?","['abstract-algebra', 'ring-theory', 'group-theory']"
2135981,Calculus of Variations in Probability Theory,"Are there any places where the Calculus of Variations shows up (i.e. is used) in probability? It seems like it should be natural for functional optimization to appear (as it does in statistics, where it seems to appear in somewhat more applied areas, like regression and machine learning; or its use in stochastic optimal control theory), but I do not know of any interesting probabilistic problems that can be cast in a variational light, but are more on theoretical side, or maybe proofs of theorems in probability that use variational techniques. I suppose the Malliavin Calculus is related, but it seems more like an answer to the ""reverse"" question :)","['malliavin-calculus', 'probability-theory', 'functional-analysis', 'probability', 'calculus-of-variations']"
2135982,"Math Behind Creating a ""Perfect"" Star","I am busy looking to create star paths in my app, and I was wondering how to determine the ratio between the inner radius and the outer radius of the points of a star so that the star has ""straight"" lines across. I have a function that takes 3 parameters: pointCount = 5
outerRadius = 100
innerRadius = 50 Basically it goes around the circle and alternates between a point and an inside so the star looks like this: As you can see, the star is ""bulging"". What I am really trying to get is this star: There should be some mathematical formula that can take the ""outer"" radius and the number of points to calculate the ""inner"" radius? innerRadius = some_operation (pointCount, outerRadius)","['ratio', 'geometry']"
2136005,Find the minimum of $\overline{AP}+\overline{AQ}$.,"My niece asked me a math question: There is a point $A(-3,2)$ on the $x$-$y$ plane. A line $L$ is perpendicular to the line $x=y$. Line $L$ and $y=f(x)=2^x$ has an intersection point $P$. Line $L$ and $y=g(x)=\log_2 x$ has an intersection point $Q$. Find the minimum of $\overline{AP}+\overline{AQ}$. Note: My niece is a freshman in a senior high school. She have not learned calculus. Therefore, I can't use calculus to solve this question. The following is my try. First, let the coordinates of $P$ be $(a,2^a)$. Because $f(x)$ and $g(x)$ are inverse functions of each other, the coordinates of $Q$ is $(2^a,a)$. Then
$$
\overline{AP}+\overline{AQ}
= \sqrt{(-3-a)^2+(2-2^a)^2} + \sqrt{(-3-2^a)^2+(2-a)^2}. \tag{1}
$$
Because the two terms in $(1)$ are both positive numbers, we can use the inequality of arithmetic and geometric means (AM-GM inequality) to obtain
$$
\begin{align}
\overline{AP}+\overline{AQ} & \geq 2\sqrt{ \sqrt{(-3-a)^2+(2-2^a)^2} \cdot \sqrt{(-3-2^a)^2+(2-a)^2}} \\
& = 2 \{ [ (-3-a)^2+(2-2^a)^2][(-3-2^a)^2+(2-a)^2]\}^{1/4} \\
& = 2 [(a^2+6a+9+4-4\cdot 2^a+2^{2a})(2^{2a}+6\cdot 2^a+9+a^2-4a+4)]^{1/4} \\
& = 2 [(2^{2a}-2^{a+2}+a^2+6a+13)(a^{2a}+3\cdot 2^{a+1}+a^2-4a+13)]^{1/4}. \tag{2}
\end{align}
$$
I don't know how to continue.","['optimization', 'logarithms', 'exponential-function', 'algebra-precalculus', 'geometry']"
2136021,Solving an ODE from a PDE,"Solve this differential equation:
  $$z^2\frac{dR}{R}+\frac{z\,dz}{1+z^2}=\frac{dS}{S}\tag1$$ I tried separating the variables but could not. Also tried to show that LHS is of the form of the differential of a product of functions but could not. Originally this is coming from a PDE 
$$z\frac{\partial n}{\partial x}dx+\frac{n\,dz}{1+z^2}=\frac{1}{z}\cdot \frac{\partial n}{\partial y}dy\tag2$$
where $z=\frac{dy}{dx}$ and $n=n(x,y)$ and $y=y(x)$. The first equation can be obtained from $(2)$ by substituting $n=R(x)S(y)$. Please tell me how to solve the differential equations. Any suggestions for a general approach for solution of the PDE is also welcome. ADDENDUM: The equation $(2)$ has been obtained from solving the Euler Lagrange equation  for a ray of light travelling through a medium with refractive index $n(x,y)$. Now here, $L=L(y,y',x)=n(x,y(x))\sqrt{1+y'^2}$ where $y'=\frac{dy}{dx}$ So we solve $\frac{d}{dx}\left(\frac{\partial L}{\partial y'}\right)=\frac{\partial L}{\partial y}$ and accordingly get equation $(2)$.","['integration', 'ordinary-differential-equations', 'calculus', 'partial-differential-equations']"
2136054,"In a normal extension of a field, is there an automorphism that maps irreducible factors of a certain irreducible polynomial? [duplicate]","This question already has an answer here : If $F/k$ normal, $g,h \in F[X]$ monic irreducible factors of $f \in k[X]$, then $\exists\ \sigma \in Aut(F/k)$ s.t. $g^\sigma = h$. (1 answer) Closed 5 years ago . Let $F$ be a field, $f(x)$ be an irreducible polynomial in $F[x]$ and $E/F$ be a normal extension. Show that if $g(x)$, $h(x)$ are irreducible factors of $f(x)$ in $E[x]$ then there exists an automorphism $\sigma$ of $E$ over $F$ such that $\sigma(g)=h$. Does this result hold if we do not assume normal extension? What I've tried so far: Let $\overline{F}$ be the algebraic closure of $F$. Then, by definition, $f(x)\in F[x]$ splits completely over $\overline{F}$. So $$f(x)=(x-\alpha_1)\cdots(x-\alpha_n)(x-\beta_1)\cdots(x-\beta_m) (x-\gamma_1)\cdots(x-\gamma_k)$$ Since $g(x)$ and $h(x)$ are irreducible factors of $f(x)\in E[x]$ then we can write, without loss of generality, $n\leq m$ and $$g(x)=(x-\alpha_1)\cdots(x-\alpha_n)\qquad h(x)=(x-\beta_1)\cdots(x-\beta_m)$$ I want to define a map $\sigma:E\rightarrow E$ which maps $\alpha_i$ to $\beta_i$ (with this I can conclude $\sigma(g)=h$, right?). But the problems are: 1) I don't know $n=m$. 2) I don't know $\alpha_i,\beta_i\in E$. 3) Even if $\alpha_i,\beta_i\in E$, I'd only have a map on a subset of $E$. I don't know if I can extend this map to the whole $E$. EDIT: The question above can be found on Serge Lang's Algebra, Revised Third Edition, Volme 1, Chapter V, exercise 26, and is stated as: Let $k$ be a field, $f(X)$ an irreducible polynomial in $k[X]$, and
  let $K$ be a finite normal extension of $k$. If $g$, $h$ are monic
  irreducible factors of $f(X)$ in $K[X]$, show that there exists an
  automorphism $\sigma$ of $K$ over $k$ such that $\sigma = h^\sigma$.
  Give an example when this conclusion is not valid if $K$ is not normal
  over $k$.","['finite-fields', 'abstract-algebra', 'galois-theory', 'normal-extension']"
2136068,Are the points moving around a sphere in this manner always equidistant?,"I recently encountered this gif : Pretend that there are visible circles constructed along the paths of the smaller black and white ""discs"", tracing how their individual centers move as they revolve around the center of the whole design. These circles together form an imaginary sphere inside the design. Assuming that the exact centers of each smaller ""disc"" are points moving along the sphere, and that they move in perfect circles at the same rate, how does the distance between the points change over a single revolution? Are they equally distant from each other at all times, or is there a period in which they grow closer, which appears to happen when both ""discs"" enter the holes of the opposite color?","['spheres', 'spherical-geometry', 'geometry']"
2136077,Number of $4$-member committees from $3$ women and $5$ men that have at least $2$ women,The United States Supreme Court consists of 3 women and 5 men. In how many ways can a 4-member committee be formed if each committee must have at least two women?. I know that we have $^8C_4=70$ combinations. I'm stuck on how many committees can be formed with at least two women. Do I get the combination of committees that include all men and subtract with the $70$?,"['combinations', 'combinatorics']"
2136089,Contour integral of $\int_{0}^{\infty}\frac{x}{\sinh x}\operatorname{dx}$,"I'm not quite sure how to do this one. I've found a simple pole at $i\pi$ and thus the residue $\frac{p(i\pi)}{q'(i\pi)} = \frac{i\pi}{\sinh'{i\pi}} = \frac{i\pi}{\cosh{i\pi}} = \frac{i\pi}{-1} = -i\pi$ as well. When I go to integrate the function, my work so far is $$I' = \int_{-\infty}^{\infty}\frac{x}{\sinh x}dx+\int_{\infty}^{\pi i +\infty}\frac{z}{\sinh z}dz - \int_{-\infty}^{\infty}\frac{x+i\pi}{\sinh{(x+i\pi)}}dx + \int_{-\infty}^{\pi i -\infty}\frac{z}{\sinh z}dz$$ but I am not sure how to evaluate the contour from here. Any help is appreciated!","['complex-analysis', 'contour-integration']"
2136128,Euler Identity Derivation-Is this allowed?,"I was reading about how Euler derived his famous identity, $e^{i{\pi}}$. It said that it was discovered when Euler took the Taylor Expansion for $e^x$, and he multiplied the $x$ by $i$, and it gave him the formula: $$e^{ix}=1+ix-\frac {x^2}{2!}-\frac{ix^3}{3!}+\frac{x^4}{4!}...$$ He then separated the series by imaginary and real parts, and found $$e^{ix}=(1-\frac{x^2}{2!}+\frac{x^4}{4!}-...)+i(x-\frac{x^3}{3!}+\frac{x^5}{5!}-...)$$
He then recognized the two series as taylor series for sine and cosine,  $$e^{ix}=\cos x+{i}\sin x$$ Now, my question is, why was he able to split up the infinite series like that? From my understanding, if you change the order of terms in an infinite series you'll change the value of the sum, so why was he able to?","['sequences-and-series', 'complex-numbers', 'euler-mascheroni-constant']"
2136153,What's the $\lim_{m\to\infty}\prod_{k=1}^m (1-e^{-kn})$?,I try to show $\lim_{n\to\infty}\lim_{m\to\infty}\prod_{k=1}^m (1-e^{-kn})=1$. It seems we need to give a lower bound of $\lim_{m\to\infty}\prod_{k=1}^m (1-e^{-kn})$ depending on $n$ and as $n$ tends to infinity this lower bound tends to 1. I am trying to calculate $\log(\prod_{i=1}^m (1-e^{-in}))$ and see if it is closed to 0 with the fact that $\log(1-x)\approx -x$ as $x\to 0$. But I am not sure how to control the error.,"['real-analysis', 'limits', 'calculus', 'analysis']"
2136182,notation to avoid repetition in matrix expressions (e.g xx' and x'Ax),"Is there a notation to express expressions such as $x^{T}Ax$, $xx^{T}$, $X^TAX$ with only one instance of $x$, or $X$ appearing in each case? This is to reduce repetition, which becomes more severe when in place of $x$ or $X$ is a longer expression. For $x^{T}Ax$, I have seen $||x||_{A}^2$, which I like, despite possibly being misleading in the cases where it is not a norm (if $A$ is not positive definite).","['notation', 'linear-algebra']"
2136209,Is it possible that the die is fair?,"You visit your grandparents and notice an old die and notebook. The die is handmade and the edges are worn. Grandpa explains that he rolled this die 10,000 times, independently and under the same conditions, when he was a prisoner of war and recorded the results of the die rolls in his notebook: Side of the die         1    2        3       4         5    6 Number of outcomes   2607    1633    1148    1839    2552    221 He adds that he used to run an illegal gambling ring and that he fashioned this die out of bone himself. Which of the following follow from these observations?
I. The data suggests the die has a much lower probability of producing a ‘6’ compared to other outcomes.
II. It is possible, though very unlikely, that the die is fair. How do I determine if the die is fair? The P(6) here is 0.0221
and if I'm not wrong, both options I and II are correct?","['statistics', 'probability', 'dice']"
2136232,"How to evaluate $PV \int_0^\infty\frac{\cos(\ln x)}{x^2+1}\,dx$?","I'm trying to show $$PV \int_0^\infty\frac{\cos(\ln x)}{x^2+1}\,dx=\frac{\pi}{2\cosh(\pi/2)}.$$
My textbook says to do this by ""integrating $e^{i\ln z}/(z^2-1)$ around a contour like Figure 7.3 but rotated 90◦ clock- wise so the straight side is along the y axis."" I took the original integral and reformulated it like this:
$$I=\int_0^\infty\frac{\cos(\ln x)}{x^2+1}\,dx=\frac{1}{2}\text{ Re}\left[\int_{-\infty}^\infty\frac{e^{i\ln x}}{x^2+1}\,dx\right]$$ Next I set up the contour integral $$\oint_C\frac{e^{i\ln z}}{(z^2-1)}\,dz=2\pi i\sum Res$$ With $\sum Res = Res[z=1]$. I then managed to show that the semicircular paths around the contour go to zero as $R$ and $\epsilon$ go to infinity and zero respectively. This implies the contour integral is equal to the principal value of $f(z)$ evaluated from $-\infty$ to $\infty$. But my problem is that $\frac{1}{2}$ of the real part of the sum of the residues does not equal $\frac{\pi}{2\cosh(\pi/2)}$. Any pointers would be very much appreciated, I've been working on this for quite a while now!","['complex-analysis', 'contour-integration']"
2136237,How do I show that this integral $\int _0^1 x\sqrt {1+\frac {1} {x^4}} dx$ does not converge?,Shouldn’t I find a function whose integral over the interval does not converges either and is always less than $\;\sqrt{1+\frac{1}{x^4}}\;$ when close to zero so that the first integral is also divergent? I’m afraid I’m only familiarized with this approach.,"['algebra-precalculus', 'calculus', 'functions']"
2136241,How do I determine boundednesss and compactness of these operators on $l^{\infty}$ and $l^1$?,"I'm trying some practice questions and came upon the following operator $T$ defined on $l^{\infty}$. For a sequence $x\in l^{\infty}$, we define $T(x)_n=\frac{1}{n}\displaystyle\sum_{i=1}^{n}x_i$, i.e., the sequence $T(x)$ is the sequence of Cesaro sums of the sequence $x$. The exercise asks to investigate the boundedness and compactness of this operator for the space $l^{\infty}$ and for its restriction to $l^1$. I managed to prove that it is in fact bounded on $l^{\infty}$ and has norm 1. However, I can't quite prove (or disprove) boundedness for when it's restricted to $l^1$. Also, I have no idea how to prove compactness. I tried finding some sequence with no convergent subsequence in the sequences $T(x)$, but it seems that it involves some trick to find an example. I'd very much appreciate some help on this. thanks in advance.","['functional-analysis', 'sequences-and-series', 'operator-theory']"
2136246,Help find closed form: $\sum_{n=1}^{\infty}\sum_{k=0}^{m}(-1)^k{m\choose k}{n-ka\over (n+m-k)^m}$,"We observe the double sum: $$\sum_{n=1}^{\infty}\sum_{k=0}^{m}(-1)^k{m\choose k}{n-ka\over (n+m-k)^m}=f(m,a),m\ge2\tag1$$
  $a$ is not restricted We are trying to determine the closed form for $(1)$ Let expanded $(1)$ for $m=2,3$ and $4$ $$\sum_{n=1}^{\infty}{n\over (n+2)^2}-2\cdot{n-a\over (n+2)^2}+{n-2a\over n^2}=f(2,a)$$ $$\sum_{n=1}^{\infty}{n\over (n+3)^3}-3\cdot{n-a\over (n+2)^3}+3\cdot{n-2a\over (n+2)^3}-{n-3a\over n^3}=f(3,a)$$ $$\sum_{n=1}^{\infty}{n\over (n+4)^4}-4\cdot{n-a\over (n+3)^4}+6\cdot{n-2a\over (n+2)^4}-4\cdot{n-3a\over (n+1)^4}+{n-4a\over n^4}=f(4,a)$$ We was able to determine the closed form for $$f(2,a)=1-2a$$
$$f(3,a)={7\over8}(3a-1)$$
$$f(4,a)={575\over 648}(1-4a)$$ By the look of it, we can assume the general closed form $(1)$ might take the form of $$f(m,a)=(-1)^m(1-ma)g(m)$$ How can we find the general closed form for $(1)$?",['sequences-and-series']
2136298,A generalization for Serret's integral $\int_0^a \frac{\ln(1+ax)}{1+x^2}dx$,"The integral specified in the title appears in Gradshteyn & Ryzhik 4.291.18 , also followed by its sister integral: $$\int_0^a \frac{\ln(1+ax)}{1+x^2}dx=\int_0^a \frac{a\arctan x}{1+ax}dx=\frac12 \arctan a\ln(1+a^2) \tag 1$$ Some application of the above gives us some nice results, such as: $$\int_0^1\frac{\arctan x}x\ln\left(\frac{(1+x^2)^3}{(1+x)^2}\right)dx=0$$ Above follows by converting $\int_0^1 \frac{\arctan x\ln(1+x^2)}{x}dx$ to a double integral using $(1)$ and then swapping the order of the integrals. Or another one: $$\int_0^1 \frac{\arctan x\ln(1+x^2)}{x(1+x)}dx=\frac{\pi^3}{96}-\frac{\pi}{8}\ln^2 2$$ But the question here would be how to prove the result given in $(1)$ ? I tried to differentiate under the integral sign with respect to $a$ , but something nasty appeared. Are there any other ideas?","['integration', 'definite-integrals']"
2136314,Regular pentagon,"If we extend the sides of a convex pentagon we'll obtain 5 triangles. If all the triangles are equal, is a regular pentagon? I think is true but I'm not sure how to prove it. Thanks!","['convex-analysis', 'geometry']"
2136402,challenging sum,"Let $\phi$ the Euler totient, and $f : [0,1] \to \mathbb{C}$ piecewise continuous. Prove that 
  $$
\lim_{n\to\infty} \frac{1}{\phi(n)} \sum_{\substack{1\leq k \leq n\\ \gcd(k,n)=1}} f\Bigl(\frac{k}{n}\Bigr) = \int_{0}^{1} f(x)\,dx
$$","['number-theory', 'integration']"
2136463,When does a pure birth process explode?,"I'm trying to understand the criterion for a pure birth process to explode in finite time. Unfortunately, the script I'm studying only gives the result and not the proof. And I wasn't able to quickly solve it myself or find a suitable reference either. Let $(X_t, t\geq 0)$ be a pure birth process on $\mathbb N_0$, starting in $0$, with rates $\lambda_i$. Then it should be true that $(X_t)$ explodes in finite time if and only if $\sum_i 1/\lambda_i < \infty$. Now, this statement is as intuitive as it gets, but still I'd like to be able to see formally why this is true. We can characterize the time of explosion as follows: $T_k = \inf\{t>0 : X_t = k \}, \qquad \forall k \in N$, $T_\infty = \sup_k T_k$. Then I want to show: $$\sum_{i\in \mathbb N} \frac{1}{\lambda_i} < \infty \implies \mathbb P_0(T_\infty < \infty) = 1.$$ $$\sum_{i\in \mathbb N} \frac{1}{\lambda_i} = \infty \implies \mathbb P_0(T_\infty < \infty) = 0.$$ How do I go about that?","['markov-chains', 'probability-theory', 'birth-death-process']"
2136509,Probability that consonants and vowels are at same position and no letter is at its previous place,"The letters of the word ""MATHEMATICS"" are arranged in all possible ways.
Then, which is the probability that, in a randomly selected arrangement, vowels and consonants are at same position and no letter is at its previous place? Vowels are AEAI and consonants are MTHMTCS. The total number of arrangements is $$\displaystyle \frac{11!}{2!\cdot 2!\cdot 2!}$$ I am not able to go further, could someone help me? Thanks.",['combinatorics']
2136535,Expected Value for conditional chosen with repetition problem,"I am thinking of this for more than two weeks now and did not find any help in the literature: I have this experiment: There is an urn with 9 balls inside, numbered 1-9. You draw a ball and record the number. After that you put the ball back inside the urn. You do this until you have drawn one number 8 times (it does not matter, which one). How many draws do you expect to do until you get one number 8 times? The minimum number of draws is obviously 8 and the maximum is 64, but i did not find any distribution for this kind of problem. The only thing I know from simulation is, that the expected value is about 39.309.
 Any Ideas?","['combinations', 'statistics', 'probability']"
2136570,How to prove this Lambert W identity: $-\pi W(-1) = \int_{-\infty}^{\frac{-1}{e}} \Im ( W ' (x) ) \ln(1 + \frac{1}{x} ) dx $? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 16 days ago . Improve this question I was wondering how to prove this Lambert W identity: $-\pi W(-1) = \int_{-\infty}^{\frac{-1}{e}} \Im ( W ' (x) ) \ln(1 + \frac{1}{x})dx $ . Maybe with contour integration?","['harmonic-functions', 'lambert-w', 'calculus', 'integration', 'special-functions']"
2136575,"What is the geometrical interpretation of $f(x,y)=\begin{pmatrix}9&12\\12&16\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}$","Consider the matrix $A=\begin{pmatrix}9&12\\12&16\end{pmatrix}$. I show that $\begin{pmatrix}-4\\ 3\end{pmatrix}$ is a eigenvector or the eigen value $0$ and that $\begin{pmatrix}3\\ 4\end{pmatrix}$ is an eigenvector for the eigenvalue $25$. Therefore, in the basis $\left\{\begin{pmatrix}-4\\ 3\end{pmatrix},\begin{pmatrix}3\\ 4\end{pmatrix}\right\}$, is given by $$\begin{pmatrix}0&0\\0&25\end{pmatrix}.$$ Now, they ask me which type of application is $f$, the application given by the matrix $A$, but I don't know. I would say something like a projection on $Span\{(3,4)\}$ and an homothetic transformation in the direction of center $(0,0)$ and of parameter $\lambda=25$, is it correct ? Can we do better ?","['linear-algebra', 'geometry']"
2136579,Reparameterization of hyperprior distribution,"I'm reading about Bayesian data analysis by Gelman et al. and I'm having big trouble interpreting the following part in the book (note, the rat tumor rate $\theta$ in the following text has: $\theta \sim Beta(\alpha, \beta)$ ): Choosing a standard parameterization and setting up a ‘noninformative’
hyperprior dis- tribution. Because we have no immediately available
information about the distribution of tumor rates in populations of
rats, we seek a relatively diffuse hyperprior distribution for $(\alpha, \beta)$ .
Before assigning a hyperprior distribution, we reparameterize in terms
of $\text{logit}(\frac{\alpha}{\alpha+\beta}) = \log(\frac{\alpha}{\beta})$ and $\log(\alpha+\beta)$ , which are the logit of the
mean and the logarithm of the ‘sample size’ in the beta population
distribution for $θ$ . It would seem reasonable to assign independent
hyperprior distributions to the prior mean and ‘sample size,’ and we
use the logistic and logarithmic transformations to put each on a $(-\infty, \infty)$ scale. Unfortunately, a uniform prior density on these newly
transformed parameters yields an improper posterior density, with an
infinite integral in the limit $(\alpha+\beta) \rightarrow \infty$ , and so this particular prior
density cannot be used here. In a problem such as this with a
reasonably large amount of data, it is possible to set up a
‘noninformative’ hyperprior density that is dominated by the
likelihood and yields a proper posterior distribution. One reasonable
choice of diffuse hyperprior density is uniform on $(\frac{\alpha}{\alpha+\beta}, (\alpha+\beta)^{-1/2})$ , which when multiplied by the appropriate Jacobian yields the following densities on the original scale , $$p(\alpha, \beta) \propto (\alpha+\beta)^{−5/2},$$ and on the natural transformed scale : $$p\left(\log\left(\frac{\alpha}{\beta}\right), \log(\alpha+\beta)\right)\propto \alpha\beta(\alpha+\beta)^{−5/2}.$$ My problem is especially the bolded parts in the text. Question (1): What does the author explicitly mean by: "" is uniform on $(\frac{\alpha}{\alpha+\beta}, (\alpha+\beta)^{-1/2})$ "" Question (2): What is the appropriate Jacobian ? Question (3): How does the author arrive into the original and transformed scale priors? To me the book hides many details under the hood and makes understanding difficult for a beginner on the subject due to seemingly ambiguous text. P.S. if you need more information, or me to clarify my questions please let me know.","['bayesian', 'change-of-variable', 'probability']"
2136608,Is there an analogue of Galois Theory for multivariate polynomials?,"A while ago when I was learning Galois Theory for the first time, I asked my professor if there was an analogue of field extensions/Galois Theory for polynomials of $n \geq 2$ variables. The best I got was a ""yes, but it's tough"". I know that the typical approach doesn't really work, even for $n=2$. As if $f(x,y) \in k[x,y]$ is an irreducible polynomial, then the quotient corresponds to the coordinate ring of the variety defined by the zero set of $f$, and is not a field. A quick google on this returned a mathoverflow thread: https://mathoverflow.net/questions/81209/galois-theory-for-polynomials-in-several-variables but the discussion uses far more algebraic geometry than I know, and from what I could gather the discussion there didn't answer the question fully. Is there an analogue of field extensions and Galois Theory for multivariate polynomials? If anyone has a first-year grad student level explanation, I'd very much appreciate it.","['algebraic-number-theory', 'algebraic-geometry']"
2136617,Solving $\sin 2x + \cos 2x =-1$,"$$\sin 2x + \cos 2x =-1$$ How would I go about solving this equation? Some hint on how to start, so I can try to figure it out on my own.
Thank you",['trigonometry']
2136634,The set $\{\frac{\varphi(n)}n:n\in \Bbb N\}$ [duplicate],"This question already has an answer here : Prove that $\{\frac{\phi (n)}{n}\}_{n \in \Bbb N}$ is dense in $[0,1]$ (1 answer) Closed 7 years ago . Let $f(n)=\varphi(n)/n$, where $\varphi$ is the totient function. Since $0<\varphi(n)\le n$ and
$$\lim_{n\to \infty}f(p_n)=1$$
(where $\{p_n\}$ is the increasing sequence of primes) and
$$\lim_{n\to\infty}f(n\#)=0$$
we know that $\limsup f(n)=1$ and $\liminf f(n)=0$. But is the set $\{f(n):n\in\Bbb N\}$ dense in $[0,1]$? Warning : this is not a problem from a book, so it might be very hard (honestly, I have no idea).","['number-theory', 'analytic-number-theory']"
2136683,Convergence of the $e^x$ Taylor series,"I have figured out a way to show that $$e=\sum_{i=0}^\infty {1\over i!}$$ I am wanting to formally show that $$e^x = \sum_{i=0}^\infty {x^i\over i!}$$ I have been looking at power series/Taylor series for a long period of time (absolute convergence) and have seen multiple proofs that I look past because something seems illegitimate with radius of convergence. If someone explains the general proof behind Taylor series/power series absolutely converging so there is no gray area, that would work as well. I will probably ask some questions in the comment area if this is the case. There must be something I am missing.","['power-series', 'sequences-and-series']"
2136719,Least Squares Fit to Find Transform Between Points,"We are using the OpenCV library estimateRigidTransform function to find a mapping between two 2D point sets.  The mapping supports rotation, uniform scaling, and translation. $$T=\left[
    \begin{array}{cc|c}
      s\cos(\theta)&-s\sin(\theta)&T_x\\
      s\sin(\theta)&s\cos(\theta)&T_y
    \end{array}
\right] $$ The documentation claims the function solves the following problem. $$[A^*|b^*] = arg \min _{[A|b]} \sum _i \| \texttt{dst}[i] - A { \texttt{src}[i]}^T - b \| ^2$$
Here the left hand side is our transform $T$ and $dst$ and $src$ are the two point sets.  It appears to find the transform that minimizes the error (distance) between the $dst$ point set and the $src$ point set after transformation. The over defined problem of $N$ equations is converted to a system of 4 equations and 4 unknowns ($s\cos(\theta)$, $s\sin(\theta)$, $T_x$, $T_y$) as shown below.  $(a_x,a_y)$ is a point in the first point set $A$, and $(b_x,b_y)$ is a point in the second point set $B$.  Both point sets contain $N$ points.  The points have already been matched between the two sets, so the first point in set $A$ should map to the first point in set $B$ and so on. All the summations occur over the set of $N$ point pairs. $$\left[
    \begin{array}{cccc}
      \sum_{} (a_x^2+a_y^2)&0&\sum_{} a_x&\sum_{} a_y\\
      0&\sum_{} (a_x^2+a_y^2)&-\sum_{} a_y&\sum_{} a_x\\
      \sum_{} a_x&-\sum_{} a_y&N&0\\
      \sum_{} a_y&\sum_{} a_x&0&N\\
    \end{array}
\right]\left[
    \begin{array}{c}
      s\cos(\theta)\\
      s\sin(\theta)\\
      T_x\\
      T_y\\
    \end{array}
\right]=\left[
    \begin{array}{c}
      \sum_{} (a_xb_x+a_yb_y)\\
      \sum_{} (a_xb_y-a_yb_x)\\
      \sum_{} b_x\\
      \sum_{} b_y\\
    \end{array}
\right]
 $$ I got these equations from reverse engineering the source code.  The documentation then says that eigenvalue decomposition is used to solve the system.  I am trying to determine where this set of equations has come from.  The first and second rows are a mystery to me.  I recognize the third and fourth rows as summing both sides of the mapping equation for $x$ and $y$. $$s\cos(\theta)\sum_{} a_x-s\sin(\theta)\sum_{} a_y+NT_x=\sum_{} b_x$$
$$\sum_{} (a_xs\cos(\theta)-a_ys\sin(\theta)+T_x)=\sum_{} b_x$$
and
$$s\cos(\theta)\sum_{} a_y+s\sin(\theta)\sum_{} a_x+NT_y=\sum_{} b_y$$
$$\sum_{} (a_xs\sin(\theta)+a_ys\cos(\theta)+T_y)=\sum_{} b_y$$
Does anyone know where the first and second rows come from?  I assume OpenCV is implementing some form of least squares fitting to find the transform that best maps the points. EDIT: After playing around further, I now recognize the first row as:
$$a_x*row3+a_y*row4$$
And the second row as:
$$-a_y*row3+a_x*row4$$
The first two rows are created by combining the last 2 in different ways.
Doesn't this create of set of equations that are not independent any more? Or does the summations in the equations and the residual error somehow maintain their independence?","['systems-of-equations', 'linear-algebra', 'linear-transformations', 'geometry']"
2136724,Is $y(t)=\sin(2\pi t+\varphi)$ SSS?,"Let's say I define random variable $\varphi$ is uniformly distributed between $[0,2\pi).$ I then define Random process $y(t)=\sin(2\pi t+\varphi)$ Is this process SSS? For now, let's assume that the definition of SSS is that the PDF is not conditional on $t$ What about $$z(t)=\sin^2\left(2\pi t+\varphi\right) \text{ ?}$$","['stationary-processes', 'statistics', 'probability']"
2136732,Can the commutator of two fundamental groups be a fundamental group?,"Say I have two fundamental groups of nice spaces. There is a natrual map from the free product to the direct product of these groups. $$\varphi:\pi_1(X)*\pi_1(Y)\to \pi_1(X)\times\pi_1(Y)$$ This can geometrically be thought of as $\pi_1(X\lor Y)\to\pi_1(X\times Y)$.
The kernel of this homomorphism is $\ker\varphi=[\pi_1(X),\pi_1(Y)]$. Is there any geometric way of thinking about this commutator? I am hoping it is some $\pi_1$ of some construction of the spaces $X$ and $Y$.","['algebraic-topology', 'group-theory', 'geometric-group-theory', 'fundamental-groups']"
2136738,Find the value of $\binom{2000}{2} + \binom{2000}{5} + \binom{2000}{8} + \cdots \binom{2000}{2000}$,Find the value of $\binom{2000}{2} + \binom{2000}{5} + \binom{2000}{8} + \cdots \binom{2000}{2000}$ I've seen many complex proofs. I am looking for an elementary proof. I know the fact that $\binom{2000}{0} + \binom{2000}{1} + \binom{2000}{2} + \cdots \binom{2000}{2000} = 2^{2000}$. This may help here.,"['combinatorics', 'binomial-theorem', 'summation', 'binomial-coefficients']"
2136764,Find the maximum of positive integer $k$ so that for all positive real numbers $x$ we have: $x^6+x^5+x^4+x^2+x+4>kx^3$,Find the maximum of positive integer $k$ so that for all positive real numbers $x$ we have: $x^6+x^5+x^4+x^2+x+4>kx^3$ Since the power of the polynomial on LHS is greater than 3 I have no idea for it!,"['algebra-precalculus', 'inequality']"
2136779,Calculus: why do we define rate of change as $dy/dx$?,"I'm just starting to learn Calculus using Morris Klines' awesome book, ""Calculus, an intuitive and physical approach."" I really like it so far. I'm just at the beginning, and after learning how to differentiate I was wondering why rate of change is defined exactly the same for every function. Allow me to explain. If we deal, for example, with functions the describe distance traveled over time, and we search for the exact speed at a specific time along this distance, then I totally understand why we define the rate of change as $\frac{dy}{dx}$ - it follows perfectly the physical way speed is defined and being calculated: speed=distance/time. Here, $dy$=difference in distance='a distance' and $dx$=difference in time='an amount of time'. So it makes sense to me. All that is left to do is make $dx$ (time) approach 0 and calculate the result. My confusion comes when we deal with other kinds of physical quantities. Physical quantities whose very physical definition\calculation has nothing to do with division. As an example, let us view the area of a rectangle: $A=a*b$. Allow me to differentiate it, please, so you'll see what I mean. (Do forgive me as I do not know how to write subscripts on this forum.) Let us assume a is a constant and b is the independant variable. It follows then that for $b=b_1$ we get: $a_1 = a\cdot b_1.$ $a_2 = a\cdot (b_1+db)=a\cdot b_1+a\cdot d_b.$ $da = a_2-a_1=a\cdot b_1+a\cdot db-a\cdot b_1 = a\cdot db.$ So far, so good. But then, in the book, for some odd and strange reason, we simpy divide both sides of the equation by $db$. As mentioned above, the area of a rectangle is defined by MULTIPLYING two adjacent sides. It has nothing to do with division. So finding the rate of change of the rectangle area should also have nothing to do with divison. (In my opinion, of course, and I'll sooon explain why.) You may tell me that 'the rate of change of the rectangles' area' is just half the sentence - it needs to be in relation to something - and that is where the divison comes from. When you look at the relation between two things mathematically - you divide them. Hence, relation is a quotient by definition. But I disagree, and here is why. IMO, right where we stopped when we found the derivative of the rectangles' area IS the definition of 'rate of change in the area of the rectangle with relation to one of its sides' - it is right there in the last equation - it is the difference in areas ($dA$) between a rectangle whose side is $b_1$ and another whose side is slightly longer, $b_1+db$. I see here three variables: $dA$, $b_1$ and $db$. To me, that equation is also a mathematical relation between them that explains how they change with relation to one another. Following this logic - all we need to do now is let db get smaller and smaller until it reaches $0$ to find the exact change in area at $b_1$. But when we do so, the entire right side equals $0$. (Which stands to reason, by the way, because what it actually means is that we subtract the areas of two identical rectangles - so it should indeed zero and cancel out.) To me, this seems like the right way to calculate the rate of change IN THIS PARTICULAR SITUATION - an area of a rectangle with one side fixed as the other varies, as compared to dividing $dA$ by $db$. It seems to me that at times, using $\frac{dy}{dx}$ really is the right and logical choice, and in others, we use it to kind of ""cheat"" because it is an algebraic trick that yields us a solution other than 0. So, why is it that we define the rate of change EXACTLY the same for every function?",['calculus']
2136783,Period of continued fraction of $\sqrt{p}$,"Few years ago, one of my friends find that $\sqrt{p}$ has periodic continued fraction with odd (resp. even) period iff $p\equiv 1(mod 4)$ (resp. $p\equiv 3(mod 4)$) for a prime $p$. (You can observe this in here : http://oeis.org/search?q=1%2C2%2C1%2C2%2C4%2C2%2C1%2C2%2C2%2C5%2C4%2C2%2C1%2C2%2C6%2C2&language=english&go=Search ). However, I don't know any clues to prove this. He told that it may be related to Pell's equation. Do you have any ideas?","['number-theory', 'continued-fractions', 'elementary-number-theory']"
2136791,Differentiation wrt matrix involvoing Khatri-rao product,"I got a following minimization problem $$\min_{\mathbf{X}^{(1)}, \, \mathbf{X}^{(2)}} \;\left\| \mathbf{B} - \mathbf{A} (\mathbf{X}^{(1)} \odot \mathbf{X}^{(2)}) \right\|^{2}_{F},$$ where the matrices $\mathbf{B}\in \mathbb{R}^{100 \times 3}$, $\mathbf{A}\in \mathbb{R}^{100\times 36}$, $\mathbf{X}^{(1)}\in \mathbb{R}^{9 \times 3}$ and $\mathbf{X}^{(2)}\in \mathbb{R}^{4 \times 3}$. The operation $\odot$ refers to the Khatri-rao product . Given matrices $\mathbf{A}$ and $\mathbf{B}$, my problem is to find out matrices $\mathbf{X}^{(1)}$ and $\mathbf{X}^{(2)}$  such that $$\mathbb{f} = \left\| \mathbf{B} - \mathbf{A} (\mathbf{X}^{(1)} \odot \mathbf{X}^{(2)}) \right\|^{2}_{F}$$ is minimized. My idea is to compute the gradient of $\mathbb{f}$ with respect to $\mathbf{X}^{(1)}$ and $\mathbf{X}^{(2)}$ respectively. My question is, how to do differentiation with respect to a matrix? I have consulted a reference but the situation seems different because it involves a Khatri-rao product in $\mathbb{f}$. Thanks in advance. $\dfrac{\partial \mathbf{f}}{\partial \mathbf{X}^{(1)}}$ and $\dfrac{\partial \mathbf{f}}{\partial \mathbf{X}^{(2)}} $?","['optimization', 'matrices', 'calculus', 'multivariable-calculus', 'linear-algebra']"
2136792,Solve the inequality $\sin(x)\sin(3x) > \frac{1}{4}$,"Find the range of possible values of $x$ which satisfy the inequation $$\sin(x)\sin(3x) > \frac{1}{4}$$ SOURCE : Inequalities (PDF)( Page Number 6; Question Number 306) One simple observation is that both $x$ and $3x$ have to positive or negative simultaneously. I tried expanding $\sin(3x)$ by the regular indentity as : $$\sin(x) \times \big(3\sin(x)-4\sin^3(x)\big) > \frac {1}{4}$$ $$\implies \sin^2(x)\times\big(3-4\sin^2(x)\big) >\frac{1}{4}$$ I do not find any way of proceeding. Wolfram Alpha gives 4 sets of answers. Do I have to observe this problem ""case-by-case""? Can this question be solved without calculus ? Can anyone provide a hint to what should be done ? Thanks in Advance ! :)","['algebra-precalculus', 'contest-math', 'inequality', 'trigonometry']"
2136836,"A more advanced Example for ""The Pasting Lemma""?","Here is ""The Pasting Lemma"" from Munkres Topology 2E (p108 Theo 18.3): Let $X = A \cup B$, where A and B are closed in X. Let $f: A \to Y \text{ and } g: B \to Y$ be continuous. If $f(x) = g(x)$ for every $x \in A \cap B$, then f and g combine to give a continuous function $h: X \to Y$, defined by setting $h(x) = f(x) \text{ if } x \in A\text{, and } h(x) = g(x)\text{ if } x \in B.$ Munkres has there an easy example with function $h: R \to R$. But i did not come up with a more topological example. Also using google has returned no fruitful results.","['continuity', 'general-topology', 'functions']"
2136837,Divergence of $\vec{f} = \frac{1}{r^2} \hat{r}$ [duplicate],"This question already has answers here : Divergence of $\vec{F} = \frac{\hat{\mathrm{r}}}{r^{2}}$ (5 answers) Closed 3 years ago . Why am I getting zero divergence of function $\vec{f} = \frac{1}{r^2} \hat{r}$, where $r$ is the distance from the origin and $\hat{r}$ is the unit vector in the radial direction. The divergence of this function over a sphere of radius $R$, which includes the origin. $$\nabla \cdot f = \frac{1}{r^2} \frac{\partial}{\partial r}(r^2 f_r) = \frac{1}{r^2} \frac{\partial}{\partial r}(r^2 \frac{1}{r^2}) = 0$$","['calculus', 'vector-analysis']"
2136859,Rational solutions for $\tan(\pi x)=y$,"Is there a rational solution for the following equation? $$\tan (\pi x)=y\\y\neq-1,0,1$$ I guess there is none, but I have no idea how to solve/prove it. EDIT: I think also that if $y$ is rational, then $x$ is not even algebraic, but this must be much harder to prove.","['number-theory', 'trigonometry']"
2136882,Show that it is impossible to list the rational numbers in increasing order,"This is problem #6 from Section 1.2 of Ash's Real Variables With Basic Metric Space Topology . I am asked to show that it is impossible to list the rational numbers in increasing order. While I know it is possible to list a finite subset of the rational numbers in increasing order, I was wondering if the reason for the impossibility in this case is because there is no least element of $\mathbb{Q}$ ? I mean, it's possible to impose an ordering on $\mathbb{Q}$ (correct me if I'm wrong, but it seems possible to compare any two rationals). But the set $\mathbb{Q}$ is of course, a subset of itself, and the Well-Ordering Principle says that a set $S$ is Well-Ordered only if any subset of $S$ contains a minimal element. Since $\mathbb{Q}$ does not contain a minimal element, it does not appear that it is a Well-Ordered set. Is this the correct idea as to why it's impossible? If not, could somebody explain to me what the correct idea is and a strategy for proving it? Thank you ahead of time.","['order-theory', 'cardinals', 'elementary-set-theory', 'rational-numbers']"
2136902,Number of sets that are downward-closed,"Let $\mathbb{N}^{<\infty}$ be the set of all finite sequences of natural numbers (including the empty sequence) endowed with the extension ordering, so $s<t$ if $s$ is an initial segment of $t$. Call a subset $C\subset\mathbb{N}^{<\infty}$ downwards-closed if whenever $t\in C$ we also have $s\in C$ for all $s<t$. How many downwards-closed subsets are there?","['order-theory', 'elementary-set-theory']"
2136944,Differentiation under the integral sign and uniform integrability,"Let $(X,\mu)$ be a measure space (if it's convenient we can assume $\mu$ is finite).  Let $[a,b]$ be an interval, and suppose we have a function $f : [a,b] \times X \to \mathbb{R}$ such that: For each $x \in X$, we have $f(\cdot, x) \in C^1([a,b])$; For each $t \in [a,b]$, we have $f(t, \cdot), \partial_t f(t, \cdot) \in L^1(\mu)$. (If it helps I'm happy to also assume that $f$ and $\partial_t f$ are  jointly measurable.) Set $F(t) = \int_X f(t,x)\, \mu(dx)$.  The classical ""differentiation under the integral sign"" theorem says that if we assume the hypothesis
$$\text{There exists $g \in L^1(\mu)$ such that $\sup_{t \in [a,b]} |\partial_t f(t,x)| \le g(x)$} \tag{DOM}$$
then $F$ is differentiable on $(a,b)$ and $F'(t) = \int_X \partial_t f(t,x) \,\mu(dx)$.  Indeed, it would follow that $F \in C^1([a,b])$. Now, in many situations, one can weaken a ""domination"" hypothesis to uniform integrability.  (For instance, the dominated convergence theorem is extended by the Vitali convergence theorem .) So suppose we replace the hypothesis (DOM) with the following:
$$\text{$\{\partial_t f(t,\cdot) : t \in [a,b]\}$ is uniformly integrable with respect to $\mu$} \tag{UI}$$
Does the same conclusion hold? I'd think this would be standard if it's true, but I've never seen it written down.  But I also can't think of a counterexample. I would like to follow the proof of the classical result by proceeding as follows: Fix $t_0 \in (a,b)$ and an arbitrary sequence $t_n \to t_0$ with all $t_n \in (a,b)$.  We have $$F'(t_0) = \lim_{n \to \infty} \int_X \frac{f(t_n, x)-f(t_0, x)}{t_n - t_0}\,\mu(dx)$$
if the limit exists, and we would like to pass the limit under the integral sign.  This would be possible if the sequence of difference quotients $D_n(x) = \frac{f(t_n, x)-f(t_0, x)}{t_n - t_0}$ were uniformly integrable.  By the mean value theorem, we know that for each $x$ there is $t_n^*(x) \in (a,b)$ such that $D_n(x) = \partial_t f(t_n^*(x), x)$.  If we could choose $t_n$ independently of $x$, then $\{D_n\}$ would be dominated by a UI sequence and we would be done.  But of course that will not work in general. Note: there are a couple of inequivalent definitions of uniformly integrable .  I would be happy to adopt the stronger one, in which we assume that $\{\partial_t f(t,\cdot) : t \in [a,b]\}$ is bounded in $L^1$ norm. (In the specific case that I care about, $\mu$ is a probability measure and I can show $\sup_{t \in [a,b]} \|\partial_t f(t, \cdot)\|_{L^p(\mu)} < \infty$ for some $p>1$, which implies (UI) by the so-called ""crystal ball condition"".)","['derivatives', 'real-analysis', 'uniform-integrability', 'lebesgue-integral', 'measure-theory']"
2136974,Computing $\sum_{n=1}^∞\frac{1}{(n+1)(n+2)(n+3)....(n+p)}$,I have a series question which I can not solve for two days. $$\sum_{n=1}^\infty\frac{1}{(n+1)(n+2)(n+3)\dots(n+p)} = \frac{1}{p\cdot p!}$$ How can I prove and solve this equation?,"['telescopic-series', 'convergence-divergence', 'sequences-and-series', 'calculus']"
2136978,Name for functions whose derivative is expressed in terms of the function value.,"Is there a name for the class of functions whose derivative is expressed only in terms of the function value? One example is the exponential, another example is \begin{align}
s_1(t) = \frac{1}{1 + e^{-t}}
\end{align} with derivative \begin{align}
s'_1(t) = s_1(t)[1-s_1(t)].
\end{align} Clarification My question is related to writing about Neural Networks (NN). In neural networks you calculate the derivative of the output relative to the input by means of an algorithm called backpropagation , or backprop (which is really nothing but the chain rule expressed in a computationally-efficient manner). An important computational advantage while doing backprop is to store the function value when propagating forward, and using that function value to compute the derivative when propagating backward. This is only possible if the derivative only depends on the function value (and not on, say, the variable value). For example. Suppose that you have a working vector w : # w is currently storing the value of t, the independent variable
w = [1, 2, 3] in the first step you calculate the function value (you won't need the value of the independent variable $t$ anymore, so you overwrite the contents in memory) # w is currently storing the value of s(t) = 1 / [1 + exp(-t)]
w = [0.7310585786300049, 0.8807970779778823, 0.9525741268224334] in the next step you calculate the derivative value (you won't need the value of $s(t)$ anymore, so you overwrite the contents in memory) # w is currently storing the value of s'(t) = s(t)[1-s(t)]
w = [0.19661193324148185, 0.10499358540350662, 0.045176659730912] Notice that if $s'$ had a dependency on the value of $t$ (as opposed to only the value of $s(t)$) I would not be able to reuse the memory in w . The specific paragraph I'm trying to improve reads as follows: Pick an activation function whose derivative depends only on the
  function value, and not on the value of the independent variable. Such
  activation functions enable reusing the working memory during the
  backprop stage. And I'd like it to know if this could be expressed more precisely: pick a ??? activation function .",['derivatives']
2136984,Expectation of a mixed random variable given only the CDF,"I'm interested in the following question: Given only the cumulative distribution function $F(x)$ of a mixed random variable $X$, how does one proceed to calculate the expectation $E(X)$? By mixed I mean a variable which is neither continuous nor discrete. For example, the cdf could be:$$F(x)=\begin{cases}0&,x\in(-\infty,-1)\\
\frac13+\frac x3&,x\in [-1,0)\\
\frac12+\frac x3&,x\in [0,1)\\
1&,x\in [1,+\infty) \end{cases},$$
though it could be more complicated. Note that it isn't piecewise constant, nor continuous (there's a jump at $x=0$ for example). If $X$ was absolutely continuous, I guess the simplest approach would be to take the derivative of $F$ to get the density and then integrate for the expectation. If it was discrete, one could easily find the distribution law from the cdf itself, by seeing the size and location of jumps and then take the weighted sum for expectation. However, I don't have an idea how to go about calculating the expectation of a mixed variable. I should note that I'm not looking for the solution for the above example specifically, but a general method for solving the question at the top of the post.","['probability-theory', 'probability', 'expected-value']"
2137013,How can I quantify how 'evenly distributed' a given set of points are in a 2D plane?,"I am working on an estimation problem involving cameras, where the quality of a location of the camera is quantified by multiple factors: such as how many feature points (salient features in a given image) are visible in its field of view, as well as how evenly distributed they are in that space. The points are defined by their 2D coordinates in the image plane and then the challenge is to come up with a parameter that determines how evenly they're distributed throughout the image plane. The dimensions of the image are known. If 100 points are visible in the image but if they're all in a straight line, or grouped together in a corner, it's not a very good set, but if they're distributed evenly like a checkered pattern, it's perfect. What is a good way to define a parameter for this? The first thing I thought about was a Voronoi diagram, which would intuitively (visually) inform me of the distribution but I am having trouble actually quantifying it as a number that I can pass to my algorithm. At the end, I am looking for a number that quantifies this 'quality of distribution' of points, which will then be passed to an optimization pipeline that attempts to pick better viewpoints by minimizing (or maximizing) this number.","['coordinate-systems', 'image-processing', 'geometry']"
2137036,Discrete mathematics - venn diagram logic,"An anonymous survey of college students that determined their behaviors regarding alcohol, cigs, and illegal drugs. Results: 894 drank alcohol regularly 192 used illegal drugs 114 drank alcohol regularly and used illegal drugs 97 engaged in all three behaviors 665 smoked cigs 424 drank alcohol regularly and smoked cigs 119 smoked cigs and used illegal drugs 309 engaged in none of the behaviors a) find the number of students in the survey b) find the number of students who engaged in exactly two of these these behaviors I drew a venn diagram for this question. part a) I used all the students used in this diagram, it is incorrect. part b) I subtracted what I did for a by the number of students who did two or more. both of these parts are incorrect I believe.","['permutations', 'combinatorics', 'arithmetic', 'discrete-mathematics']"
2137067,Derivative of trace and determinant of matrix with respect to a vector,"I encounter a difficulty in one of my research dealing with derivative of a matrix. I will appreciate much for your expertise on this. Let $\mathbf{x }=vec\left( X \right) $ where $X $ is a $T\times r
$ matrix of unknown parameters ($T$ and $r$ are constant, $T>>r$) and $vec$
is the vectorization, let
\begin{equation}
\mathbf{H}\left( \mathbf{X }\right) =\left( X ,\mathbf{I}_{T}\right) ^{\prime },  \label{def_H_theta}
\end{equation}
where $\mathbf{I}_T$ is $T \times T $
identity matrix. Let
\begin{equation}
\Omega \left( \mathbf{X }\right) _{T\times T}=\left( \mathbf{I}%
_{T}+XX ^{\prime }\right) .  \label{var-cov_TS_multi}
\end{equation}
What is the derivative of
\begin{equation}
\frac{\partial \log \left\vert \Omega \left( \mathbf{X }\right)
\right\vert }{\partial \mathbf{x }}=??
\end{equation}
$|A|$ is the determinant of $A$,
and
\begin{equation}
\frac{\partial tr\left[ \mathbf{H}\left( \mathbf{X }\right) ^{\prime
}\Omega \left( \mathbf{X }\right) ^{-1}\mathbf{H}\left( \mathbf{%
X }\right) \right] }{\partial \mathbf{x }}=??
\end{equation}
where $tr$ is the trace operator.
Many thanks!","['derivatives', 'matrix-calculus']"
2137090,"Is there a ""$\text{sinc}$"" function for $\cos x$? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Is there are function to represent $x\mapsto\frac{\cos x}{x}$ as there is for $\sin x$, namely $\frac{\sin x}{x}=\text{sinc }x$?","['notation', 'trigonometry', 'soft-question']"
2137093,$\int_{0}^{1}{2n-x-x^3-x^5-\cdots-x^{4n-1}\over 1+x^2}\cdot{\mathrm dx\over \ln{x}}$,"Consider $$\int_{0}^{1}{2n-x-x^3-x^5-\cdots-x^{4n-1}\over 1+x^2}\cdot{\mathrm dx\over \ln{x}}=I\tag1$$
  $n\ge1$ How does one show that $I=2n\ln{\Gamma(3/4)\over \Gamma(5/4)}-\ln{[8^n(2n-1)!!]}?$ An attempt: $J=x+x^3+x^5+x^7+\cdots+$ $J=x(1+x^2+x^4+x^6+\cdots+)$ Geometric series $1+x+x^2+x^3+\cdots x^{n-1}={x(1-x^n)\over 1-x}$ $1+x^2+x^4+x^6+\cdots+x^{2n-2}={x^2(1-x^{2n})\over 1-x^2}$ $I$ becomes $$2n\int_{0}^{1}{1\over 1+x^2}\cdot{\mathrm dx\over \ln{x}}-\int_{0}^{1}{1-x^{2n}\over 1+x^2}\cdot{x^3\over \ln{x}}\mathrm dx=I\tag2$$ $x=\tan{y}$ then $dx=\sec^2{y}dy$ $$\int_{0}^{\pi/4}{1\over \ln{\tan{y}}}\mathrm dy-\int_{0}^{\pi/4}{1-\tan^{2n}{y}\over \ln{\tan{y}}}\cdot\tan^3{y}\mathrm dy\tag3$$ $$\int_{0}^{\pi/4}{1-\tan^3{y}\over \ln{\tan{y}}}\mathrm dy-\int_{0}^{\pi/4}{\tan^{2n}{y}\over \ln{\tan{y}}}\cdot\tan^3{y}\mathrm dy\tag4$$ $1-x^3=(1-x)(1+x+x^2)$ I am not sure what to do next","['integration', 'definite-integrals', 'power-series', 'calculus']"
2137096,Find characteristic function of given Random Variable,"Let $X_1, ..., X_n$ be iid ~Poi( $\frac {\lambda}{\sqrt n}$ ) Let $Y_n=\frac{(\sum_{k=1}^n X_k)-\lambda \sqrt n}{\sqrt \lambda n^{\frac 1 4}}$ Find the characteristic function of $Y_n$ It was a question in an older exam so I think there should be an easy/fast way to solve this. I see that $Y_n$ is standardised but not how that will help me here.
I started calculating $\Phi_{Y_n}(t)=\mathbb E[exp(itY_n)]=\mathbb E[exp(it\frac{(\sum_{k=1}^n X_k)-\lambda \sqrt n}{\sqrt \lambda n^{\frac 1 4}})]=exp(-it \sqrt\lambda n^{\frac 1 4 })  \mathbb E[exp(\frac {it\sum_{k=1}^n X_k)}{\sqrt \lambda n^{\frac 1 4}})]$ But from there it got messier and I don't think this is the wanted solution.
Any help is appreciated.","['characteristic-functions', 'probability-theory', 'probability-distributions']"
2137105,How to prove a space is not separable?,I have a general question on how to prove a space is not separable. I read some posts on this site and it seems like it suffices to find an uncountable family of pairwise disjoint open sets to prove a space is not separable. (here: The space of bounded continuous functions are not separable ) Why an uncountable family of pairwise disjoint open sets is enough?,['functional-analysis']
2137163,Events A and B are independent such that $P(A)=6P(B)$,"Events A and B are independent such that $P(A)=6P(B)$ and $P(A \cup B) =0.915.$ Find P(B). I know that $P(A \cup B)= P(A) + P(B) -P(A)P(B)$ Then $0.915=6P(B)+P(B)-6P(B)P(B)  \\  \rightarrow 0.915=7P(B)-6P(B)^2$. $-6P(B)^2+7P(B)-0.915=0$ Letting $a=-6, b=7, \& c=-0.915$ I can use the quadratic formula: $\frac{-b \pm \sqrt{b^2-4ac}}{2a}.$ Which gives me the roots of $1.0166..$ and $0.15$, since the $P(B) < 1$ then $P(B)$ is $0.15?$ Am I taking this the correct direction?",['probability']
2137168,A club has $9$ women and $8$ men. Count the number of different committees of size $4$ with $3$ or $4$ women.,1) A club has 9 women and $8$ men. Count the number of different committees of size 4 with $3$ or $4$ women. How can I take in account for $3$ women and $1$ man? I have ${9 \choose 4}$ thus far.,"['permutations', 'combinatorics', 'discrete-mathematics']"
2137197,Gateaux and Frechet derivatives on vector valued functions,"If $f:\mathbb{R}\to\mathbb{X}$ is a function from the real numbers to any normed vector space (finite or infinite dimension), and $f$ is Gateaux differentiable, is $f$ necessarily Frechet differentiable?","['real-analysis', 'gateaux-derivative', 'functional-analysis', 'multivariable-calculus', 'frechet-derivative']"
2137203,Multivariable calculus chain rule and change of variables,"I am considering the following question Transform the expression $\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}$ into one in $\rho$ and $\phi$ . Now my problem has arisen when considering $\frac{\partial \rho}{\partial x}$ . From considering $x=\rho cos(\phi)$ , I obtain $\frac{\partial x}{\partial \rho} = cos(\phi)$ so then $\frac{\partial \rho}{\partial x} = \frac{1}{cos(\phi)}$ And yet in the book I am looking at, they instead used $\rho = (x^2+y^2)^{1/2}$ to obtain $\frac{\partial \rho}{\partial x} = \frac{x}{(x^2+y^2)^{1/2}}=\frac{\rho cos(\phi)}{\rho} = cos{\phi}$ !!! It seems that maybe where I am going wrong is by not accounting for $y$ in the partial derivative of $\rho$ . However I don't understand why I should take it into account. As I am taking the partial derivative, and I not just taking the derivative where there is an $x$ explicitly stated? (Although I do see a bit of a problem using my 'rule', because if I were to take the other expression for $\rho$ as $y=\rho sin(\theta)$ , then I would get $\frac{\partial \rho}{\partial x} = 0$ . Perhpas I misunderstood the partial derivative as only taking the derivative when a variable is explicitly stated....","['polar-coordinates', 'partial-derivative', 'chain-rule', 'multivariable-calculus', 'change-of-variable']"
2137226,Continuity of a linear operator in Schwartz Space,"Let $f: \mathbb{R}\rightarrow\mathbb{R}$ be a $\mathbb{C}^{\infty}$ function which is bounded. Define $A:\mathcal{S}(\mathbb{R})\rightarrow\mathcal{S}(\mathbb{R})$ as $A(\phi)=f\phi$. Is $A$ continuous? Intuitively, I think this should not be true. If we take a function $f$ which is bounded but has an unbounded derivative then for a sequence $\{\phi_n\}\rightarrow0$ in $\mathcal{S}(\mathbb{R})$, $\{f\phi_n\}\nrightarrow0$ in $\mathcal{S}(\mathbb{R})$. I thought of taking,  $f=\sin(x^2)$. However, I'm unable to find a suitable $\{\phi_n\}$. Note: $\mathcal{S}(\mathbb{R})$ is the Schwartz space.","['schwartz-space', 'fourier-analysis', 'distribution-theory', 'functional-analysis', 'sobolev-spaces']"
2137257,Can a polynomial be expressed as an infinite sum of non-polynomial functions?,"The Taylor series represents a non-polynomial function as an infinite series of polynomials, so is it possible to express a polynomial function as an infinite series of non-polynomial functions?","['taylor-expansion', 'functions']"
2137269,What is a generating function in combinatorics?,"I just sat in on a lecture on exponential generating functions in combinatorics (I have no formal education in combinatorics myself). It was quite interesting, but I'm afraid I don't actually understand what the generating function is/does. I've tried doing some minimal research online, but everything I've seen seems to be either too complex or too general to understand well. For example, I know how to find the generating function for permutations of a finite set, $\frac{1}{1-x}$. But what role does $x $ play here, and what does the generating function tell us? I don't see how it's at all related to the species of permutations itself.","['permutations', 'combinatorics', 'generating-functions']"
2137292,How can you prove $\frac{n(n+1)(2n+1)}{6}+(n+1)^2= \frac{(n+1)(n+2)(2n+3)}{6}$ without much effort?,"I will keep it short and take only an extract (most important part) of
  the old task. $$\frac{n(n+1)(2n+1)}{6}+(n+1)^2= \frac{(n+1)(n+2)(2n+3)}{6}$$ What I have done is a lot work and time consuming, I have ""simply"" solved it. But I think with a lot less work, there would be an easier and faster way. It's just I cannot see it : / If anyone wants see, here is my long solution which I'm not happy with: $$\frac{n(n+1)(2n+1)+6(n+1)^2}{6}=\frac{(n^2+2n+n+2)(2n+3)}{6} \Leftrightarrow$$ $$\Leftrightarrow \frac{(2n^3+n^2+2n^2+n)+6n^2+12n+6}{6} = \frac{(n^2+3n+2)(2n+3)}{6} \Leftrightarrow$$ $$\Leftrightarrow \frac{2n^3+3n^2+n+6n^2+12n+6}{6}=\frac{2n^3+3n^2+6n^2+9n+4n+6}{6} \Leftrightarrow$$ $$\Leftrightarrow \frac{2n^3+9n^2+13n+6}{6}=\frac{2n^3+9n^2+13n+6}{6}$$","['algebra-precalculus', 'induction']"
2137354,Show that there exists $g:D\rightarrow \mathbb{C}$ is analytic and one-to-one such that $(g(z))^2=f(z^2).$,"Let $D=\{z: |z|<1\}$ and suppose that $f:D\rightarrow \mathbb{C}$ is analytic, one-to-one and $f(0)=0, f'(0)=1.$ Show that there exists $g:D\rightarrow \mathbb{C}$ is analytic and one-to-one such that $(g(z))^2=f(z^2).$ I proved this statement is as follows: Since $f$ is analytic and $f(0)=0, f'(0)=1$, we have $f(z)=z+a_1z^2+a_2z^3+...$ Then, $f(z^2)=z^2(1+a_1z^2+a_2z^3+...)=z^2h(z).$ If there is $z_0\in D$ such that $h(z_0)=0$, we have $f({z_0}^2)=0$, which implies $z_0^2=0$ since $f$ is 1-1. Thus, $z_0=0.$ However, it is clear that $h(0)=1\neq 0.$ Hence, we proved that $h$ is nonzero on $D$. Now, since $h$ and $\frac{1}{h}$ are both analytic on $D$, there is an analytic function $g_1:D\rightarrow \mathbb{C}$ such that $g_1^2=h.$ (this result follows from the theorem 13.11 in Real and Complex analysis by W. Rudin) Therefore, we define $g(z)=zg_1(z)$, it is clear that $g$ is analytic and satisfies $g(z)^2=f(z^2).$ However, I have no good idea how to explain that $g$ is 1-1. Can anyone give me some ideas how to solve this part? Thanks.",['complex-analysis']
2137379,"Find all singularities of $f(z)={z\over \sin({1\over z^2})}$, define them and compute the residues of $f$ at each","Find all singularities of $f(z)={z\over \sin({1\over z^2})}$, define them and compute the residues of $f$ in each. This was pretty meticulous for me. I found $z=0$ to be an essential singularity and ,$z=\pm{1\over \sqrt{\pi z}},\pm{i\over \sqrt{\pi z}},k\in \Bbb{Z}_+\setminus \{0\}$ to be poles. Although I tried computing the residues at the poles and it was a mass, what is more disturbing me is not being able to compute the residue at $z=0$. I tried integrating $f(z)$ along a circle as small as desired around $z=0$, but it also seems too tedious, and also, there are infinitely many poles in every circle I would choose, which makes it pointless. I am completely lost. Can you help me? If I got the approach wrong and I'd better look at it all differently, it would help me even more than guiding me with the essential singularity.",['complex-analysis']
2137384,Solving a polynomial in an easier manner,"I need to solve the following polynomial: $$(12x-1)(6x-1)(4x-1)(3x-1)=15$$ I tried do the multiplication and ended up with an even worse expression. There are a few other questions like this one on the list of exercises I'm trying to solve, and I just wanted an easier method, without having to do the ""brutal"" work.","['algebra-precalculus', 'roots', 'polynomials']"
2137416,Prove that $\int_{0}^{1}f(x)^2dx\geq 4$,"Let $f:[0,1]\to \mathbb{R} $ be an integrable function with $\int_{0}^{1}f(x)dx=\int_{0}^{1}xf(x)dx=1$ . Prove that $\int_{0}^{1}f(x)^2dx\geq 4$ . I got that $\int_{0}^{1}F(x)dx=F(0)$ , but I don't think it's useful at all.","['inequality', 'real-analysis', 'calculus', 'integration', 'definite-integrals']"
2137488,Existence of a subsequence in $L^{2}$ [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Let $\Omega \subset \mathbb{R}^{n}$ an open set and $(f_{n})_{n=1}^{\infty}$ a sequence in $L^{2}(\Omega)$ such that $||f_{n}||_{2} \to ||f||_{2}$, then there exists a subsequence $(f_{n_{k}})_{k}$ of $(f_{n})_{n=1}^{\infty}$ such that
$\int_{\Omega} |f_{n_{k}}  - f|^{2} \to 0$, when $k\to \infty$. The only thing I thought: How the sequence $(||f_{n}||_2)_{n=1}^{\infty}$ is limited because is convergent and $L^{2}(\Omega)$ is reflexive, then there exists a subsequence $f_{n_{k}} \rightharpoonup g$, but i don't know how to continue. Thanks","['functional-analysis', 'lp-spaces', 'convergence-divergence', 'hilbert-spaces']"
2137498,Blowing up families,"Suppose that $g: X \to Y$ is a map of varieties (perhaps flat), and we have a subvariety $V \subset X$. Important: I want $V$ to be flat over $X$, otherwise there are trivial counter examples. Consider now the $f : Bl_V X \to Y$ as a composition $Bl_V X \to X \to Y$, where $Bl_V X$ is the blow up. Let $y$ be a point in $Y$. Is $f^{-1}(y) \cong Bl_{V \cap g^{-1}(y)} g^{-1}(y)$. In other words, can I blow up in families? Example: Let's say I want to understand what happens when I blow up the plane at $n$ points. Let $H$ be the hilbert scheme of length n subscheme of $P^2$. Consider $H$ with its universal family $V \subset H \times P^2$. Then blow up $H \times P^2$ along $V$, to get $Bl_V (H \times P^2) \to H$. Are the fibers of this map the blow ups of $P^2$ at the corresponding subschemes?",['algebraic-geometry']
2137499,Degree of map obtained by intersecting tangents to complex projective curve with fixed line,"Let $X \subset \mathbb{C}P^2$ be a smooth projective curve, and for each $p \in X$, let $T_p$ denote the line tangent to $X$ at $p$. For any line $L \subset \mathbb{C}P^2$ in the plane, there is a map $X \to L$ that sends $p \in X$ to the unique point of intersection $T_p \cap L \in L$. How does the topological degree of this map vary with $L$?","['algebraic-topology', 'algebraic-geometry']"
2137530,Expected value of the number of distinct results of die rolls in $N$ trials,"Given $N$ trials of a die roll, where we have defined $D$ as the number of distinct outcomes, what would be the mean and standard deviation of $D$? If we have defined $I(k)$ as an indicator random variable which equals 1 if outcome $k$ (such as 6) appears at least once, and 0 otherwise, for $k\in\{ 1,\dots,6\}$, then by definition
$$D = \sum\limits_{k=1}^6 I(k)$$
How do the dependencies between the $I(k)$ play into the solution? (Which is the part that is tripping me up the most.)","['statistics', 'probability', 'dice', 'variance']"
2137570,How to prove that a quadrilateral with a circle inscribed inside it is cyclic?,"The question is given as follows: In the diagram below, $BF \perp HD$. Prove that $ACEG$ is a cyclic quadrilateral. In class, we were told to introduce the origin $O$ and draw radii $OB, OH, OF,$ and $OD$. We then noticed that $\angle FOD = 180°- \angle FED$ and $\angle HOB = 180° -\angle HAB $, were told after that to use Thales' Theorem twice. At this point, I don't know where to use Thales' Theorem, or where to go with the problem.","['euclidean-geometry', 'geometry']"
2137609,Prove that: $\sec^2 20^\circ +\sec^2 40^\circ +\sec^2 80^\circ = \textrm 36$,"Prove that: $\sec^2 20^\circ +\sec^2 40^\circ +\sec^2 80^\circ = \textrm 36$ My Attempt: $$L.H.S=\sec^2 20^\circ + \sec^2 40^\circ +\sec^2 80^\circ$$
$$=\dfrac {1}{\cos^2 20°} +\dfrac {1}{\cos^2 40°} +\dfrac {1}{\cos^2 80°}$$
$$=\dfrac {\cos^2 40°.\cos^2 80°+\cos^2 20°.\cos^2 80°+\cos^2 20°.\cos^2 40°}{\cos^2 20°.\cos^2 40°.\cos^2 80°}$$. I got paused here. Please help to prove this..","['algebra-precalculus', 'polynomials', 'cubics', 'trigonometry']"
2137650,Conflicting natural isomorphisms: How can separate identifications interfere with each other?,"I have been self-studying Walter Noll's Finite-Dimensional Spaces .  A tool used often that seems extremely powerful is the identification of various objects through natural isomorphisms.  I have been struggling with the full concept of naturality and trying to avoid getting too involved in category theory, if possible. One particular instance where I could not quite see an issue that was pointed out was in this Noll excerpt: 25 Tensor Products For any linear space $\mathcal V$ there is a natural isomorphism from $\operatorname{Lin}(\Bbb R,\mathcal V)$ onto $\mathcal V$, given by $\mathbf h\mapsto\mathbf h(1)$. The inverse isomorphism associates with $\mathbf v\in\mathcal V$ the mapping $\xi\mapsto\xi\mathbf v$ in $\operatorname{Lin}(\Bbb R,\mathcal V)$. We denote this mapping by $\bf v\otimes$ (read ""vee tensor"") so that
  $$\mathbf v\otimes\xi:=\xi\mathbf v\quad\text{for all}\quad\xi\in\Bbb R.\tag{25.1}$$
  In particular, there is a natural isomorphism from $\Bbb R$ onto $\Bbb R^*:=\operatorname{Lin}(\Bbb R, \Bbb R)$. It associates with every number $\eta\in\Bbb R$ the operation of multiplication with that number, so that $(25.1)$ reduces to $\eta\otimes\xi=\eta\xi$. We use this isomorphism to identify $\Bbb R^*$ with $\Bbb R$, i.e. we write $\eta=\eta\otimes$. However, when $\mathcal V\ne\Bbb R$, we do not identify $\operatorname{Lin}(\Bbb R,\mathcal V)$ with $\mathcal V$ because such an identification would conflict with the identification $\mathcal V\cong\mathcal V^{**}$ and lead to confusion. If $\boldsymbol\lambda\in\mathcal V^*=\operatorname{Lin}(\mathcal V,\Bbb R)$, we can consider $\boldsymbol\lambda^\top\in\operatorname{Lin}(\Bbb R^*,\mathcal V^*)\cong\operatorname{Lin}(\Bbb R,\mathcal V^*)$. Using the identification $\Bbb R^*\cong\Bbb R$, it follows from $(21.3)$ and $(25.1)$ that $\boldsymbol\lambda^\top\xi=\xi\boldsymbol\lambda=\boldsymbol\lambda\otimes\xi$ for all $\xi\in\Bbb R$, i.e. that $\boldsymbol\lambda^\top=\boldsymbol\lambda\otimes$. However, I feel like understanding this problem precisely could be enlightening. So my question is: can there be a natural isomorphism between two objects, another natural isomorphism between two other objects, and somehow a contradiction or confusion that arises because of an interrelationship? Sticking to the Noll example would be most helpful.  For example, we can make the identification $\mathcal V\cong\operatorname{Lin}(\Bbb R,\mathcal V)$ as defined by Noll's excerpt. We can also make the usual double dual identification $\mathcal V\cong\mathcal V^{**}$. Can someone point out a case where choosing to make both identifications leads to a contradiction or confusion? Ideally, an answer that I am looking for is limited to examples involving real finite-dimensional linear spaces and relies minimally on category theoretic language. On a side note, a suggestion of a reference to a linear algebra book that heavily motivates and makes use of natural isomorphisms would be much appreciated.","['duality-theorems', 'vector-space-isomorphism', 'category-theory', 'linear-algebra', 'vector-spaces']"
2137663,Show $\sum_{k=m}^n {k \choose r} = {n+1 \choose r+1} - {m \choose r+1}$,$$\sum_{k=m}^n {k \choose r} = {n+1 \choose r+1} - {m \choose r+1}$$ I'm stumped. Tried algebraically decomposing each side but I can't make ends meet.,"['combinatorics', 'summation', 'binomial-coefficients']"
2137701,Find a number M such that $\mid x^3 - x^2 +8x \mid\leq M$ for all $-2\leq x \leq 10$,Find a number M such that $\mid x^3 - x^2 +8x\mid \leq M$ for all $-2\leq x \leq 10$ I am not sure my answer is right or not.. I used triangle inequality. $\mid x^3 - x^2 +8x\mid \leq \mid x^3 \mid + \mid x^2  -8x \mid  \leq \mid x^3 \mid + \mid x^2 \mid + \mid 8x \mid $ $M = 10^3 + 10^2 + 8(10)$,"['real-analysis', 'analysis']"
2137719,How do I prove this function is differentiable?,"$$f(x) = \begin{cases}
\frac{ \langle Mx, x \rangle}{\| x \|}, & \text{for } x \neq 0 \\\\
0, & \text{for } x = 0
\end{cases}$$ $M$ is a symmetric matrix and $x \in \mathbb{R^n}$ Is there a particular property from linear algebra I need to keep in mind?","['multivariable-calculus', 'analysis', 'derivatives']"
2137754,Probability with loaded and fair dice,"I own five different six-sided dice. Four of the dice are fair dice, meaning they have values 1, 2, 3, 4, 5, 6. However, one of the dice is loaded; thus, it never shows 1, 2 or 3, but is equally likely to show the values 4, 5, or 6. For my experiment, I will pick up one random dice and roll it twice. The first thing I would like to calculate is the probability of getting two sixes. To calculate this, I first calculated the probability of getting one six and multiplied it by two. Suppose $S$ = event that two sixes are rolled. $$P(S) = 2(\frac45(\frac16) + \frac15(\frac13)) = .4 $$
However, I am not sure if this is correct. I need to calculate this because I would also like to calculate $P(L|S)$ where L = event that a loaded die was picked. Additionally, I feel this is incorrect, because if I change the '2' to a '10' to calculate it for 10 rolls instead of 2, I get a value over 1 which makes no sense. To summarize, how can I calculate $P(S)$ properly so I can calculate $P(L|S)$?","['statistics', 'dice', 'probability', 'elementary-set-theory']"
2137772,How to solve $2\tan^{-1}\sqrt{x-x^2}=\tan^{-1}x+\tan^{-1}(1-x)$,"$$2\tan^{-1}\sqrt{x-x^2}=\tan^{-1}x+\tan^{-1}(1-x)$$
I wasn't sure on how to go from here because I tried to draw triangles for each tangent function but that didnt work and I know that I can't distribute via doing tangent on both sides. (though I do enjoy complex methods in solving this, I do appreciate a high school level process of doing this problem.)","['algebra-precalculus', 'trigonometry']"
2137813,Proof that no normed space is compact,"The notes for my functional analysis class casually state that every nontrivial normed space is not compact, but do not give a proof. I can't tell if this is because it is a trivial proof or because it is too complicated to be given inline. Is there a good proof for this? I have searched around the internet for a while and haven't been able to find one.","['functional-analysis', 'normed-spaces']"
2137879,Differential forms and line integral in rotation group SO(3),"My major is mechanical engineering. Recently, I am working on some subject involving three-dimensional finite rotations. More specially, the necessary and sufficient conditions for an applied torque/moment be conservative in the finite rotation range. I have tried to read some math books, but I got more confused. The following is the description of the background. In mechanics, an externally torque generally exhibits unusual property of configuration-dependent, which means the torque varies from its initial value $\mathbf M_0$ to its current value $\mathbf M$ when moving along a curve lying on SO(3) staring form the identity $\mathbf I$ to the current position $\mathbf R$. In other words, the current counterpart $\mathbf M$ can be viewed as a $\mathbf explicit function$ of the rotation $\mathbf R \in SO(3) $. Let $\mathbf \delta \omega$ be the spatial spin (an element which belongs to the tangent space of SO(3) at the base point $\mathbf R$, i.e., $\mathbf \delta \omega \in T_{R}SO(3)$). Then the virtual work done by the torque over the spin is given by
$$\delta W = \mathbf M \cdot \delta \omega$$ where $\delta W $ is a real number, and ""$\cdot$"" means dot product.
In mathematics, $\mathbf M$ is an element of cotangent space of SO(3) at the base point $\mathbf R$, i.e., $\mathbf M \in T^{*}_{R}SO(3)$. On the other hand, if the rotation vector (axis-angle representation) $\mathbf \psi = \psi_{i} \mathbf e_{i}$ was used to parameterize the rotation manifold, $\mathbf R = exp(\hat \psi)$, then we can express the torque as $\mathbf M=\mathbf Q \mathbf M_0$ explicitly, where $\mathbf Q=\mathbf Q(\psi)$ is the transformation matrix relating the initial and current values of the torque. We also can represent the virtual rotation by $\mathbf \delta \psi$, the variation of rotation vector $\mathbf \psi$, $\mathbf \delta \psi \in T_{I}SO(3)$. The relation between $\mathbf \delta \omega$ and $\mathbf \delta \psi$ is given by $ \delta \omega = \mathbf L \delta \psi$, where $\mathbf L= \mathbf L(\psi)$ is the tangential operator, $\mathbf L:T_ISO(3)\to T_RSO(3)$. Thus, the virtual work can be rewritten as
$$ \mathbf \delta W = \mathbf L^T \mathbf M \cdot \delta \psi$$ My questions are: Which expression of the virtual work is a differental 1-form in SO(3) and why? How to calculate the line integral of the virtual work over a curve lying on SO(3) in terms of a differential 1-form? Thank you very much! EDIT 1: In the above description, the spin $\mathbf \delta \omega$ is not a differential, since there does not exist a variable from which the spin can be derived. It comes from the variation of the orthogonality condition of rotation matrix, $\mathbf \delta(\mathbf R \mathbf R^T=\mathbf I)=0$,  $\mathbf \delta \mathbf R=\widehat (\delta\omega) \mathbf R$. However, the variation $\mathbf \delta \psi$ of rotation vector is a differential.","['differential-forms', 'smooth-manifolds', 'differential-geometry', 'lie-groups']"
2138037,How to discretize a certain integral,"I would like to discretize the following integral operator: $$\frac{1}{s^2}\sum_{j=1}^N\mu_j\int d\mathbf{x}d\mathbf{x}'f(\mathbf{x})f(\mathbf{x'}) \left(x_j + x'_j - 2\mu_j\right)\hat{a}^\dagger(\mathbf{x})\vert0\rangle\langle0\vert\hat{a}(\mathbf{x'}),$$ where $d\mathbf{x} = dx_1 \cdots dx_N$ and $f(\mathbf{x}) = f(x_1) \cdots f(x_N)$ is the square root of a Gaussian such that $$f(x_i) = \frac{1}{(2\pi \sigma^2)^{1/4}} \exp\left[-\frac{(x_i - \mu_i)^2}{4 \sigma^2}\right]$$ I would like to discretize it so that it can be written in matrix form and its eigenbasis understood. I saw a similar post here , using Simpson's rule for the discretization, subsequently yielding the matrix. Context: In quantum statistical estimation theory, the Symmetric Logarithmic derivative is an important operator. It helps to define the quantum Fisher information but more importantly for pure states, its basis defines the optimal estimation strategy which saturates the corresponding quantum Cramer-Rao Bound.","['discrete-mathematics', 'operator-theory', 'numerical-methods', 'discrete-calculus', 'parameter-estimation']"
2138044,Value of $\left \lfloor{x}\right \rfloor+\left \lfloor{-x}\right \rfloor$?,"While reading about greatest integer function from a book, I found a question as $\left \lfloor{x}\right \rfloor+\left \lfloor{-x}\right \rfloor$  ? I attempted it as follows: We know: $x-1<\left \lfloor{x}\right \rfloor< x\tag1$ Also then: $-x-1 < \left \lfloor{-x}\right
\rfloor < -x\tag2$ Adding $(1)$ & $(2)$, we get $-2< \left \lfloor{x}\right \rfloor+\left \lfloor{-x}\right \rfloor<0$. This is the answer which I got, but the actual answer was  $\left \lfloor{x}\right \rfloor+\left \lfloor{-x}\right \rfloor= -1$. I am not getting this. Where my method has gone wrong? Please help me.","['graphing-functions', 'functions', 'limits']"
2138051,How many squares are crossed by the diagonal of a rectangle splitted into $N \times M$ squares,"I need help with this question Here is the problem: I have a rectangle with fixed size NxM, such that N, M are positive natural numbers. And this rectangle of size NxM is splitted into squares with size 1x1 ; actually if the rectangle sizes are 3x3, it is splited into 9 squares. This picture is showing 4 variants. And actually the picture is also showing the thing i want to find, the number of small squares crossed by the diagonal of the rectangle. In all 4 variants it is passing through 4 squares. Thanks in advance.",['geometry']
2138062,Analogue of Currying,"Background Given any function $$ f: A \times B \to C, \ \text{or} \ f \in \mathrm{Hom}(A \times B, C) $$ we can ""curry"" the function by a sort of reverse evaluation: $$ \mathrm{curry}: \mathrm{Hom}(A \times B, C) \to \mathrm{Hom}(A, \mathrm{Hom}(B,C)), $$ or $$ \ \mathrm{curry}(f): A \to (B \to C) $$ with $$ (\mathrm{curry}(f))(a) = f(a,\cdot). $$ My Question Can we define a mapping $$ *: \mathrm{Hom}(A \times B, C) \to \mathrm{Hom}(\mathrm{Hom}(A,B),C) $$ with $$ *(f): (A \to B) \to C $$ and is there a general name for this ? Also, can we define some mapping $$ \mathrm{Hom}(A, \mathrm{Hom}(B,C)) \to \mathrm{Hom}(\mathrm{Hom}(A,B),C)) $$ which somehow preserves structure nicely? Apologies for how simple-minded and unmotivated this question seems. It's just a result of messing about with things. This also may not be related to category theory so please feel free to remove the tag if so. Any recommended reading material is also greatly appreciated.","['category-theory', 'functions']"
2138170,$\operatorname{Aut}(G\times H)$ is isomorphic to the direct product $\operatorname{Aut}(G)\times\operatorname{Aut}(H)$.,"Suppose $G$ and $H$ are finite groups of relatively prime orders. Prove that $\operatorname{Aut}(G\times H)$ is isomorphic to the direct product $\operatorname{Aut}(G)\times\operatorname{Aut}(H)$ . My attempt: I can imagine that given some automorphism of $G\times H$ , $\phi(g,h)\rightarrow(g_2,h_2)$ where $g,g_2\in G$ and $h,h_2\in H$ , we can create two automorphisms for $G$ and $H$ where $\tilde{\phi}(g)=g_2$ and $\hat{\phi}(h)=h_2$ . Basically we can ""build"" an automorphism of $G\times H$ out of any two automorphisms of $G$ and $H$ , and hence $\operatorname{Aut}(G\times H)$ will be all possible combinations of automorphisms of $G$ and $H$ . I know this is a very imprecise method, but I'm looking for a step in the right direction. Any help appreciated!","['abstract-algebra', 'group-theory']"
2138241,Is my proof of $\lim_{x\to \infty}\frac 1x = 0$ correct?,"I tried to prove $$\lim_{x\to \infty}\frac 1x = 0$$
I started as thus
$$\lim_{x\to \infty}\frac 1x=\lim_{x\to \infty}\frac x{x^2}$$
Applying L'Hospital's Rule $$\lim_{x\to \infty}\frac 1x=\lim_{x\to \infty}\frac x{x^2}=\lim_{x\to \infty}\frac 1{2x}=\frac12\lim_{x\to \infty}\frac 1x$$
Thus,
$$\frac12\lim_{x\to \infty}\frac 1x=\lim_{x\to \infty}\frac 1x$$
which therefore implies 
$$\lim_{x\to \infty}\frac 1x = 0$$
QED.","['proof-verification', 'limits']"
2138250,Differential of composition of functions,"If $\mathbf f : \mathbb R^n \to \mathbb R^m$ is differentiable at $x_0 \in \mathbb R^n$, define the differential operator $$d\mathbf f_{x_0} : \mathbb R^n \to \mathbb R^m $$
as the linear operator that takes a small increment $h \in \mathbb R^n$ and outputs $D\mathbf f(x_0) \cdot h$, where $D\mathbf f(x_0) \in \operatorname {Mat}(m\times n,\mathbb R)$ is the Jacobian matrix of $\mathbf f$ at the point $x_0$. Let $\mathbf f$ be defined as before, and let $\mathbf g : \mathbb R^p \to \mathbb R^q$ be differentiable at $y_0 \in \mathbb R^p$. Call $a_0 \doteq \mathbf f(x_0)$ and $b_0 \doteq \mathbf g(y_0)$. Lastly, let $\mathbf h : \mathbb R^m \times \mathbb R^q \to \mathbb R^s$ be differentiable at $(a_0,b_0)$. How would I write down the differential $d\mathbf s_{(x_0,y_0)}$ of function $\mathbf s : \mathbb R^n \times \mathbb R^p \to \mathbb R^s$ defined as follows,
$$\mathbf s : (x,y) \mapsto \mathbf h(\mathbf f(x),\mathbf g(y)) $$
as a composition of the differentials $d\mathbf h_{(a_0,b_0)}$, $d\mathbf f_{x_0}$, and $d\mathbf g_{y_0}$? I know that the Jacobian matrix associated to $d\mathbf s_{(x_0,y_0)}$ should be in $\operatorname{Mat}(s \times pq, \mathbb R)$, but I'm not exactly sure what matrices need be multiplied together to construct it. I guess my brain is having a hard time understanding how to ""merge"" two functions such as $\mathbf f$ and $\mathbf g$ in an ordered couple, so I can't compile my composition diagram correctly.","['derivatives', 'real-analysis', 'function-and-relation-composition', 'multivariable-calculus', 'jacobian']"
2138254,Prove that: $\tan^6 20° - 33\tan^4 20° + 27\tan^2 20°=3$,"Prove that: $\tan^6 20° - 33\tan^4 20° + 27\tan^2 20°=3$ My Attempt: $$L.H.S=\tan^6 20 - 33\tan^4 20 + 27\tan^2 20°$$
$$=\tan^2 20°(\tan^4 20° - 33\tan^2 20°+27)$$
$$=(\sec^2 20° -1)(\tan^4 20° - 33\tan^2 20° + 27)$$ Please help me to continue from here..",['trigonometry']
2138274,Integration training? [duplicate],"This question already has answers here : Some users are mind bogglingly skilled at integration. How did they get there? (4 answers) Closed 7 years ago . I've been pretty frustrated lately with my poor integration skills. A lot of times I find that in my math or physics classes I understand the concepts behind a question, reduce it to an integral, and then find myself unable to solve it. Yet some people both on this board and at my university are deft hands at solving complicated integrals with a variety of tricks. I'd really like to get better at integrals and start approaching their level. I was hoping that there might be suggestions of workbooks or textbooks that are designed purely on increasing the reader's ability to solve difficult integrals, and also suggestions on a good general philosophy to take to get better at this as well. Also, I'm unsure if this is the wrong place to ask this question. If so, I apologize and I'll remove the question.","['integration', 'soft-question']"
2138303,is equivalence classes the same thing as cosets?,"In abstract algebra (modern algebra), is the equivalence classes the same thing as cosets? 
In the lecture notes that I have, it seems as though they are but is it a universal rule for the equivalence classes to mean the same thing cosets? or is the equivalence classes a subset of some set that equivalence relation holds ie (reflexivity, symmetric and transitivity)?",['abstract-algebra']
2138324,The Cauchy Principal value of a rational function with only real poles,"$\newcommand{\PV}{\operatorname{P.V.}}$I have a doubt about the Cauchy Principal Value of real rational functions. $f: \mathbb{R}\rightarrow \mathbb{R}$ is a rational function with $\deg(\text{denominator})>\deg(\text{numerator})$. $\{x_1,x_2,\ldots,x_n \} \subset \mathbb{R} $ is the  set of all $f$ poles $\PV$ exists and it is $$\PV \int_{-\infty}^{+\infty} f(x) \ dx=\pi i \left( \sum_{k=1}^n \operatorname{Res}(f,x_k) \right) $$ $$\pi i \left( \sum_{k=1}^n \operatorname{Res}(f,x_k) \right) \in \mathbb{I}$$ So: $$\PV \int_{-\infty}^{+\infty} f(x) \ dx=0$$
because:
$$\operatorname{Re} \left( \pi i \left( \sum_{k=1}^n \operatorname{Res}(f,x_k) \right) \right)=0$$ Is it true? $\PV \int_{-\infty}^{+\infty} f(x) \ dx$ doesn't exist if $\deg(\text{numerator}) \ge \deg(\text{denominator})$, does it? In general, which are the conditions of existence of P V? Is it correct? Thanks","['improper-integrals', 'real-analysis', 'integration', 'analysis']"
2138328,90 degree counter-clockwise rotation around a point,"How do you do a 90 degree counter-clockwise rotation around a point? I know around the origin it's $(-y,x)$, but what would it be around a point? $$(-y - a,x - b)$$
Where $(a,b)$ is the rotation point.",['linear-algebra']
2138330,What is dual to Cartesian Product,"I know that ""Cartesian Product"" of two sets $A$ and $B$ is the set of all possible tuples of $A$ and $B$. What is the dual of Cartesian Product? $\left(\frac AB\right)$? And how does it work?",['elementary-set-theory']
2138331,Jacobian of $A (A^\top X A)^{-1} A^\top$,"Let $A\in\mathbb{R}^{n\times m}$, $n\geq m$, be a full column rank matrix, and consider the function
\begin{align}
f&\colon \mathbb{R}^{n\times n} \to \mathbb{R}^{n\times n}\\
& X\mapsto A (A^\top X A)^{-1} A^\top,
\end{align}
where $\bullet^\top$ denotes transposition. Assuming that $(A^\top X A)^{-1}$ exists, I'm interested in the computation of the Jacobian matrix of $f$, i.e.
$$\tag{1}\label{a}
\mathbf{J}[f] = \left[\frac{\partial f(X)}{\partial X_{ij}}\right]\in\mathbb{R}^{n^2\times n^2}.
$$ I know that there exists a closed form expressions for the Jacobian of the inverse, namely $\mathbf{J}[X^{-1}]=-(X^{-\top} \otimes X^{-1})$ (see e.g. here , page 5). Hence, I wonder whether a similar closed-form expression can be derived for \eqref{a}. Thanks in advance.","['derivatives', 'matrices', 'matrix-calculus', 'multivariable-calculus', 'jacobian']"
