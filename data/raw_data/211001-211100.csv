question_id,title,body,tags
4244066,$3\times3$ invertible matrix in $\mathbb{F}_2$ such that $M^7 = I$ [duplicate],This question already has answers here : Show that there exists a $3 × 3$ invertible matrix $M$ with entries in $\mathbb{Z}/2\mathbb{Z}$ such that $M^7 = I_3$. (2 answers) Closed 2 years ago . Show that there exists a $3\times 3$ invertible matrix $M\neq I_3$ with entries in the field $\mathbb{F}_2$ such that $M^7 =I_3$ . Attempt: $M^7 =I$ can be factorized as $(M- I) ( M^6 + M^5 +...+M+I)=0$ but I don't think it will prove the existence of required M. Trying to find an M by hit and trial method seems a bad idea. Can you please suggest some elegant way of proving it?,"['matrices', 'linear-algebra']"
4244106,"Let $f(0,5)\to\mathbb R-\mathbb Q$ be a continuous function such that $f(2)=\pi$, then $f(\pi)=\pi$. True/False?","Let $f(0,5)\to\mathbb R-\mathbb Q$ be a continuous function such that $f(2)=\pi$ , then $f(\pi)=\pi$ . True/False? The answer given is True. I think we can prove this by taking a constant function as well. Is that correct? Does the given domain and range have anything to do here?","['calculus', 'functions']"
4244116,"Differential equation, tricky","How would I solve the following: $(x^2-y^2)dy+(y^2+x^2y^2)dx=0$ ? Here is what I did: $\frac{dy}{dx}(x^2-y^2)=-(y^2+x^2y^2)$ Dividing through, to leave differential on one side: $\frac{dy}{dx}=\frac{-(y^2+x^2y^2)}{x^2-y^2}$ I then proved this is a homogeneous differential equation.
This is how I Proved it:
Let $A(x,y)=-y^2-x^2y^2$ . $A(tx,ty)=-t^2y^2-t^2x^2t^2y^2$ When you factorize, you get: $t^2A(x,y)$ Do the same for the denominator, and make the conclusion that these are homogeneous.
Is this right?
I let $y=vx$ so that $\frac{dy}{dx}=x\frac{dv}{dx}+v$ However I get stuck from here, as the equation seems to not make sense for.
Anyone to please guide me","['calculus', 'ordinary-differential-equations']"
4244146,Doob-Dynkin Lemma for Stopped Sigma Field,"Suppose that $X_{0},X_{1},\ldots$ is a sequence of real-valued random variables.  Let $\mathcal{F}_{n}=\sigma(X_{0},\ldots,X_{n})$ .  The Doob-Dynkin lemma says that any $Y$ that is $\mathcal{F}_{n}$ -measurable can be written as $Y=g(X_{0},\ldots, X_{n})$ for $g:\mathbb{R}^{n+1}\rightarrow \mathbb{R}$ measurable (with respect to the Borel sigma fields).  This lemma is sometimes used to interpret $\mathbb{E}(Y| \mathcal{F}_{n})$ as either a random variable or a measurable function $\mathbb{R}^{n+1}\rightarrow \mathbb{R}$ (as in the second paragraph of https://en.wikipedia.org/wiki/Conditional_expectation ). If $N$ is a stopping time, we sometimes condition on $\mathcal{F}_{N}$ , as in the strong markov property.  Is there a version of the Doob-Dynkin lemma for $\mathcal{F}_{N}$ ?  My intuition is that an $\mathcal{F}_{N}$ -measurable random variable should be some kind of measurable function of the information $(N=n,X_{0}=x_{0},\ldots,X_{n}=x_{n})$ .","['measure-theory', 'probability-theory', 'markov-chains']"
4244217,Sufficient statistic for normal distribution (not iid),"I am a bit confused with this exercise, since I never worked with samples of this type. I would appreciate if you can help me. The exercise is as follows: Let $\{Xi\} \sim N(iθ, 1)$ for $i = 1, .... , n$ be an independent, but not identically distributed sample. Check that $T = \sum_iX_i$ it is a sufficient statistic for $θ$ . What I need is to verify that the $T$ statistic is sufficient for the $\theta$ parameter. I know how to do it with the Fisher and Neymann factorization theorem, but always with a identically distributed sample of random variables. In this case, the sample is not identically distributed. Therefore, I don't know how to verify it.","['statistics', 'sufficient-statistics']"
4244219,Prove two inequalities.,"How can we prove these two inequalities, I proved the first question which I guess will be used to prove the following inequalities, but I don't know how to start.  Let $a,b,c$ be there real numbers. Prove that if : $\sin a+ \sin b + \sin c\ge 2
\implies \cos a+ \cos b + \cos c\le \sqrt 5$ and $\sin a+ \sin b + \sin c\ge \frac 32\implies \sin (a-\pi/6)+ \sin(b-\pi/6) + \sin (c-\pi/6)\ge 0.$ The first question was to prove that if $x,y,z$ are real numbers, then $(x+y+z)^2\le 3(x^2+y^2+z^2)$ . I proved this part, it is a direct result using $x^2+y^2\ge 2xy$ and by expanding $(x+y+z)^2$ . Thanks for your help.","['algebra-precalculus', 'trigonometry']"
4244235,Convergence rate to the average value for a sequence that is uniformly distributed modulo $1$,"Let $x_n$ be a sequence of real numbers that asymptotically follow a uniform distribution modulo $1$ (for example, $x_n=n\sqrt{2}$ with $n$ positive integer $\leq N$ and $N\rightarrow \infty$ ). It is well known that the average value of $\{x_n\}$ , calculated for all $n \leq N$ , tends to $1/2$ as $N\rightarrow \infty$ . Similarly, the average value of $\{x_n\}^2$ tends to $1/3$ . The convergence to this value is often described by a probabilistic approach (e.g. citing the central limit theorem, the progressive tendency to the standard normal CDF, and so on). I am interested in describing this convergence rate  using a Big- $O$ notation, to point out its asymptotic behaviour in more direct way. In particular, writing $$\frac1N \sum_{1\leq n\leq N} \{x_n\}= \frac{1}{2}+R(N)$$ and $$\frac1N \sum_{1\leq n\leq N} \{x_n\}^2= \frac{1}{3}+S(N)$$ I would like to know the magnitude of the error terms $R(N)$ and $S(N)$ as $N \rightarrow \infty$ . By taking into account the CLT, the properties of the Irwin-Hall distribution and the Berry–Esseen theorem , it seems that both errors have magnitude $O(N^{-1})$ . However, I would be happy to obtain a formal proof of this.","['summation', 'uniform-distribution', 'number-theory', 'asymptotics', 'real-analysis']"
4244257,Intuition behind getting two straight lines as result,"Question : Find the equation of the straight line that passes through $(6,7)$ and makes an angle $45^{\circ}$ with the straight line $3x+4y=11$ . My solution (if you want, you can skip to the bottom) : Manipulating the given equation to get it to the slope-intercept form, $$3x+4y=11...(i)$$ $$\implies 4y=-3x+11$$ $$\implies y=\frac{-3}{4}x+\frac{11}{4}$$ Let, the slope of (i) is $m_1=\frac{-3}{4}$ , and the slope of our desired equation is $m_2$ . Now, according to the question, $$\tan(45^{\circ})=\pm\frac{-\frac{3}{4}-m_2}{1-\frac{3}{4}m_2}...(1)$$ $$\implies 1=\pm\frac{-\frac{3}{4}-m_2}{1-\frac{3}{4}m_2}$$ $$\implies \pm \frac{3}{4}+m_2=1-\frac{3}{4}m_2...(ii)$$ Picking positive value from (ii), $$\frac{3}{4}+m_2=1-\frac{3}{4}m_2$$ $$\implies m_2(1+\frac{3}{4})=1-\frac{3}{4}$$ $$\implies m_2=\frac{1-\frac{3}{4}}{1+\frac{3}{4}}$$ $$\implies m_2=\frac{1}{7}$$ Picking negative value from (ii), $$-\frac{3}{4}-m_2=1-\frac{3}{4}m_2$$ $$\implies -\frac{3}{4}-m_2=1-\frac{3}{4}m_2$$ $$\implies -m_2(1-\frac{3}{4})=1+\frac{3}{4}$$ $$\implies m_2=-\frac{1+\frac{3}{4}}{1-\frac{3}{4}}$$ $$\implies m_2=-7$$ Picking $m_2=\frac{1}{7}$ , the equation of the straight line that passes through $(6,7)$ , $$\frac{y-7}{x-6}=\frac{1}{7}$$ $$\implies 7y-49=x-6$$ $$\implies -x+7y-43=0$$ $$\implies x-7y+43=0...(iii)$$ Picking $m_2=-7$ , the equation of the straight line that passes through $(6,7)$ , $$\frac{y-7}{x-6}=-7$$ $$\implies -7x+42=y-7$$ $$\implies 7x+y-49=0...(iv)$$ The general form of equation (1) is, $$\tan\theta=\pm \frac{m_1-m_2}{1+m_1m_2}$$ Here, $\pm$ has been included to include both the acute and the obtuse angles that are formed when two lines with slopes $m_1$ and $m_2$ intersect each other. Now, I used this equation to find the straight line that makes $45^{\circ}$ with (i). Why am I getting 2 values of $m_2$ when there is only one value of $m_2$ in the general form of the equation? How can I reconcile between my getting of 2 values of $m_2$ with the $\pm$ sign arising due to the acute and obtuse angles?","['coordinate-systems', 'trigonometry', 'geometry']"
4244266,Ben and Jordan have three coins between them. Two of them are fair but one of them has a 4/7 chance of showing heads.,"Now, Ben and Jordan both flip each coin once and write down the outcomes. What is the probability they both get the same number of heads? My approach: 4 possible outcomes: 3H, 3T, 2H&1T, 2T&1H. $P(3H) = (\frac{1}{2})^2\frac{4}{7} *\binom{3}{1} = \frac {36}{84} $ $P(3T) = (\frac{1}{2})^2\frac{3}{7} *\binom{3}{1} = \frac{27}{84}$ $P(2H,1T) = \frac{2}{3}(\frac{1}{2})^2\frac{4}{7} + \frac{1}{3}(\frac{1}{2})^2\frac{3}{7} = \frac{11}{84}$ $P(2T,1H) = \frac{2}{3}(\frac{1}{2})^2\frac{3}{7} + \frac{1}{3}(\frac{1}{2})^2\frac{4}{7} = \frac{10}{84}$ P(same number of heads) = $(\frac {36}{84})^2 + (\frac{27}{84})^2 + (\frac{11}{84})^2 + (\frac{10}{84})^2 = \frac{1123}{3528} \approx 32\% $ What do you think?","['statistics', 'probability', 'combinations']"
4244279,Is there a complex equivalent of the convolution product?,"For two continuous real-valued functions $(f,g),$ the convolution product is defined as $$h(t)=(f*g) = \int_{-\infty}^{+\infty} f(\lambda )g(t-\lambda ) \, d\lambda $$ So for two complex valued functions $f(z)$ and $g(z)$ the convolution product would look something like: $$h(z)=(f*g)=\int_{z\in \Gamma} f(\omega)g(z-\omega) \, d\omega$$ where $\Gamma$ is a closed contour containing singularities. Now if $f$ and $g$ are entire functions then $h(z) = 0$ so this would only be useful for meromorphic functions with isolated singularities.","['integration', 'complex-analysis', 'convolution', 'analysis']"
4244287,Prove that there exists an integer $a$ with $1 \leq a \leq p-2$ such that neither $a^{p-1}-1$ nor $(a+1)^{p-1}-1$ is divisible by $p^2$.,"Let $p \geq 5$ be a prime number. Prove that there exists an integer $a$ with $1 \leq a \leq p-2$ such that neither $a^{p-1}-1$ nor $(a+1)^{p-1}-1$ is divisible by $p^2$ . All my progress: We have by FLT $\{1^p,2^p,\dots, {p-2}^p\}\equiv \{1,2,\dots , p-2\}
   \mod p.$ We also have by FLT, $p|a^{p-1}-1$ We want to show that $v_p(a^{p-1}-1)=1=v_p({a+1}^{p-1}-1).$ I also got $(p-a)^{p-1}\equiv a^{p-1}+a^{p-2}\cdot (p-1)\cdot
   a^{p-1}+p\cdot a^{p-2}$ Also, if we can show $ \{a^{p-1}-p \mod p^2:  1 \leq a \leq p-2\}=$ $\{0,p,\cdots, p(p-1)\} $ in some form, and then we will be done. Would be helpful if one can send hints in place of a solution.","['number-theory', 'elementary-number-theory']"
4244314,How to show that $\left\|F_n^* - F\right\|={{O}_{p}}\left({{n}^{-\frac{1}{2}}}\right)$,"Let $X_1,\ldots,X_n$ be iid from a cdf $F$ on $R^d$ , $X_1^*,\ldots,X_n^*$ iid from empirical cdf $F_n$ . Let $F_n^*$ be empirical cdf based on $X_i^*$ 's. Using DKW inequality, Let $\rho\left(F_1,F_2\right)=\left\|F_1-F_2\right\|$ How to show that (a) ${{\rho }_{\infty }}\left(F_{n}^{*},F\right)\xrightarrow{a.s}0$ (b) ${{\rho }_{\infty }}\left(F_{n}^{*},F\right)={{O}_{p}}\left({{n}^{-\frac{1}{2}}}\right)$ (c) ${{\rho }_{L_p}}(F_{n}^{*},F)={{O}_{p}}({{n}^{-\frac{1}{2}}})$ My thought is  that (a) \begin{equation}
\begin{aligned}
P({{\rho }_{\infty }}(F_{n}^{*},F)>z)& < P({{\rho }_{\infty }}(F_{n}^{*},{{F}_{n}})+{{\rho }_{\infty}}\left({{F}_{n}},F\right)>z) \\ 
 & < \int_{0}^{z}{P({{\rho }_{\infty }}({{F}_{n}},F)>{{z}_{1}})}P({{\rho }_{\infty }}(F_{n}^{*},{{F}_{n}})>z-{{z}_{1}})\mathrm d{{z}_{1}} \quad \text{(Correct?)}\\ 
 & \le \int_{0}^{z}{{{C}_{\varepsilon ,d}}{{e}^{-(2-\varepsilon )nz_{1}^{2}}}{{{{C}'}}_{\varepsilon ,d}}{{e}^{-(2-\varepsilon )n{{(z-{{z}_{1}})}^{2}}}}d{{z}_{1}}} (DKW)\\ 
 & ={{C}_{\varepsilon ,d}}{{{{C}'}}_{\varepsilon ,d}}\int_{0}^{z}{{{e}^{-(2-\varepsilon )nz_{1}^{2}}}{{e}^{-(2-\varepsilon )n{{(z-{{z}_{1}})}^{2}}}}d{{z}_{1}}} \\ 
 & ={{C}_{\varepsilon ,d}}{{{{C}'}}_{\varepsilon ,d}}\int_{0}^{z}{{{e}^{-(2-\varepsilon )n({{z}^{2}}-2{{z}_{1}}z+2z_{1}^{2})}}d{{z}_{1}}} \\ 
 & ={{C}_{\varepsilon ,d}}{{{{C}'}}_{\varepsilon ,d}}{{e}^{\frac{-(2-\varepsilon )n{{z}^{2}}}{2}}}\int_{0}^{z}{{{e}^{-(2-\varepsilon )n2{{({{z}_{1}}-\frac{z}{2})}^{2}}}}d{{z}_{1}}\propto {{{{C}''}}_{\varepsilon ,d}}{{e}^{-{{C}_{\varepsilon ,d}}^{\prime \prime \prime }n{{z}^{2}}}}}  
\end{aligned}
\end{equation} \begin{equation}
    \sum\limits_{n=1}^{\infty }{P({{\rho }_{\infty }}(F_{n}^{*},F)>z)\le \sum\limits_{n=1}^{\infty }{{{{{C}''}}_{\varepsilon ,d}}{{e}^{-{{C}_{\varepsilon ,d}}^{\prime \prime \prime }n{{z}^{2}}}}}}<\infty 
\end{equation} \begin{equation}
    {{\rho }_{\infty }}(F_{n}^{*},F)\xrightarrow{a.s}0
\end{equation} (b) From part a $P({{\rho }_{\infty }}(F_{n}^{*},F)>z)\le C{{e}^{an{{z}^{2}}}}$ , where $a,C$ are constant. Let $ C{{e}^{an{{z}^{2}}}}=\epsilon$ , then we can derive $z=\sqrt{\frac{\ln \varepsilon -\ln C}{an}}$ . So $\exists$ constant ${{D}_{\varepsilon }}=\frac{\sqrt{\ln \varepsilon -\ln C}}{\sqrt{a}}$ s.t. $\forall\epsilon>0$ , $P({{\rho }_{\infty }}(F_{n}^{*},F)>\frac{{{D}_{\varepsilon }}}{\sqrt{n}})\le \varepsilon $ . In other words, ${{\rho }_{\infty }}(F_{n}^{*},F)={{O}_{p}}({{n}^{-\frac{1}{2}}})$ . (c) $$\rho_{L_p}=\|F_n^*-F\|_{L_p}\le\|F_n^*-F_n\|_{L_p}+\|F_n-F\|_{L_p}\le[\|F_n^*-F_n\|_{\infty}\|F_n^*-F_n\|_{L_1}]^{\frac{1}{p}}+[\|F_n-F\|_{\infty}\|F_n-F\|_{L_1}]^{\frac{1}{p}}$$ Then I think we can also use DKW inequality, but I don't know how to deal with this $\|\|_{L_1}$ , do you have some ideas? I also post this question in mathoverflow https://mathoverflow.net/q/403434/362220","['metric-spaces', 'real-analysis', 'functional-analysis', 'probability-theory', 'probability']"
4244316,How do I calculate an integral that contains the floor function?,"I am trying to get my head around Abel's summation formula ( here ), but I am confused about how to deal with an integral involving the floor function. Let $a_1, a_2, a_3, ... , a_n, ...$ be a sequence of real or complex numbers, and define a partial sum function $A$ by $$A(t) = \sum_{1 \le n \le t} a_n$$ Further, fix real numbers $x < y$ and let $\phi$ be continuously differentiable function on $[x, y]$ . Then $$\sum_{1 \le n \le x} a_n \phi(n) = A(x) \phi(x) - \int_1^x A(u) \phi '(u) \, du$$ EXTRA INFO : My aim is find a formula of this form for a sum $\sum_{1 \le n \le x} \phi(n)$ . So ( back to original post ): Let the sequence $a_n$ be $a_0 = 0, a_1 = 1, a_2 = 1, ... $ . Then $\sum_{1 \le n \le x} a_n \phi(n) = \sum_{1 \le n \le x} \phi(n)$ and $A(x) = \lfloor x \rfloor$ . So, $$\sum_{1 \le n \le x} \phi(n) = \lfloor x \rfloor \phi(x) - \int_1^x \lfloor u \rfloor \phi '(u) \, du$$ But how do I calculate the integral $\int_1^x \lfloor u \rfloor \phi '(u) \, du$ ? Obviously $\lfloor u \rfloor$ is a constant for any given value of $u$ , but I don't know how to tackle the indefinite integral - not even for something as simple as $\phi(x) := x^2$ . (Note that this is just an example of $\phi$ ; I am after a more general solution.)","['integration', 'summation', 'ceiling-and-floor-functions']"
4244333,How to compute $\frac{1}{2\pi}\int_{-\pi}^\pi (\cos y + x)^{2k}dy$ and similar integrals,"For an integer $ k \ge 0$ and a scalar $x \in \mathbb R$ , define $$
\begin{split}
A_k(x) &:= \frac{1}{2\pi}\int_{-\pi}^\pi (\cos y + x)^{2k}dy,\\
B_k(x) &:= \frac{1}{2\pi}\int_{-\pi}^\pi (\cos y + x)^k(\sin y + x)^kdy,\\
C_k(x) &:= \frac{1}{2\pi}\int_{-\pi}^\pi\cos y (\cos y + x)^k(\sin y + x)^{k-1}dy
\end{split}
$$ Question. How to obtain analytic formulae for $A_k(x)$ , $B_k(x)$ , and $C_k(x)$ ? Attempt Let's attempt to compute $A_k(x)$ (since it looks the least intimidating). By Newton's binomial theorem, one can write $A_k(x) = \sum_{j=0}^k {2k\choose 2j} I_j(x) x^{2k-2j}$ , where $$
I_j(x) := \frac{1}{2\pi}\int_{-\pi}^\pi (\cos y)^{2j} dy=\frac{1}{2\pi}\int_0^{2\pi}(\cos y)^{2j}dy = \frac{1}{2^{2j}}{2j \choose j},
$$ Putting things together gives $$
A_k(x) = \sum_{j=0}^k{2k\choose 2j}{2j\choose j}2^{-2j}x^{2k-2j} = (2k)!\sum_{j=0}^k \frac{1}{(j!2^j)^2}\frac{x^{2k-2j}}{(2k-2j)!} = \ldots
$$ and I don't know how to go from here (though it looks like the coefficient of a certain order term in a series expansion).","['integration', 'special-functions', 'spherical-coordinates', 'generating-functions', 'power-series']"
4244335,On a sequence of continuous functions,"I am working on the problem: Let $\{ f_n \}$ be a sequence of real-valued functions on $[0,1]$ defined by $f_0 = f \in C[0,1]$ and $f_n$ is an
anti-derivative of $f_{n-1}.$ Suppose that for each $x \in [0, 1]$ there is $n \in \mathbb{N}$ for which $f_n(x) = 0.$ Prove that $f = 0.$ Here is my attempt: We first note that if $f_n$ is an anti-derivative of $f_{n-1}$ , this implies that $f_{n-1}$ is a derivative   of $f_n$ . i.e. \begin{equation}
    f_{n-1} = \frac{d}{dx}(f_n).
\end{equation} Case 1: If $n = 1$ such that $f_n = 0$ for all $x \in [0,1]$ . Here if $n = 1$ then $f_1(x) = 0$ for all $x \in [0,1]$ . Now, put $n = 1$ in equation (1). We get that $$f_{1-1} = \frac{d}{dx}(f_1(x)) \implies f_0 = 0 = f   \forall x \in [0,1].$$ Case 2: If $n>1$ such that $f_n(x) = 0$ for all $x \in [0,1]$ . Here if $n > 1$ , from equation (1) we   get that $$f_{n-1} = \frac{d}{dx}(f_n) \implies f_{n-1} = 0.$$ Now take the derivative of $f_{n-1}$ . $$\frac{d}{dx}(f_{n-1}) = f_{n-2} \implies \frac{d^2}{dx^2}(f_n) = \frac{d}{dx}(f_{n-1}) = f_{n-2}.$$ This then implies $$\frac{d^n}{dx^n}(f_n) = f_{n-n}   \forall x \in [0,1].$$ Hence $\frac{d^n}{dx^n}(0) = f_0 = f$ for all $x \in [0,1].$ Thus $f = 0$ for all $x \in [0,1].$ $\square$ Please let me know if I am on the right track or assist me in proving this correctly","['sequence-of-function', 'real-analysis', 'solution-verification', 'derivatives', 'baire-category']"
4244340,Evaluate $a$ and $b$ s.t. the piecewise function is differentiable at two points,"Evaluate $a$ and $b$ s.t. the piecewise function is differentiable at $x=1$ and $x=2$ at the same time. \begin{equation*}
f(x) = \left\{
        \begin{array}{ll}
            ax^2+bx+1 & \quad x ≥ 1 \\
            -1 & \quad x < 1
        \end{array}
    \right.
\end{equation*} So first, we gotta prove it's continuous at these points. $$f(1)=a+b+1$$ $$\lim_{x \to 1^-} -1=-1$$ $$\lim_{x \to 1^+} ax^2+bx+1=a+b+1$$ $-1=a+b+1 ⟹ a+b=-2$ How do I continue this for $x=2$ ? And then for differentbility? Im kinda new to this and need a little help.","['continuity', 'calculus', 'derivatives']"
4244389,Number of operations and cost of matrix multiplication,"Let's suppose we have a $2 \times 3$ matrix $A$ and a $3 \times 4$ matrix $B$ . If we multiply these two matrices, $A B$ , we'll have $ 3$ multiplication and $2$ addition for each entry in the resultant $2\times 4$ matrix, which will make the total of $24$ multiplications and $16 $ additions , which will make it $40$ operations needed for matrix multiplication. Thus, the cost of matrix multiplication should be $40$ as there are $40$ operations done. However, I notice that addition is not included wherever I read about it. Please explain to me where I am wrong.","['matrices', 'numerical-linear-algebra']"
4244396,"Is every ""almost everywhere derivative"" Henstock–Kurzweil integrable?","It is well known that the Henstock–Kurzweil integral fixes a lot of issues with trying to integrate derivatives. The second fundamental theorem of calculus for this integral states: Given that $f : [a,b] \rightarrow \mathbb{R}$ is a continuous function. If $f$ is differentiable co-countably everywhere (in other words: differentiable everywhere except for possibly a countable set of points), then: $f'$ is Henstock-Kurzweil integrable $\int_a^bf'(x) dx = f(b) - f(a)$ My question is what happens if you replace ""co-countably everywhere"" with ""almost everywhere""? Clearly the second statement no longer holds (the Cantor function provides a counter-example), but what about the first statement? If $f$ is continuous everywhere and differentiable almost everywhere, is $f'$ necessarily Henstock-Kurzweil integrable?","['integration', 'analysis', 'real-analysis', 'gauge-integral', 'derivatives']"
4244408,What area does the antiderivative represent?,"Consider the antiderivative of the function $e^{-x},$ which is $-e^{-x}.$ Evaluating the antiderivative at the value $0$ produces $-1.$ I was taught to conceptualize an antiderivative as an area under a curve, or a sum of progressively smaller approximate sections. But clearly, $-1$ cannot represent the area under under the $e^{-x}$ curve from $0$ to $0$ , $0$ to $\infty$ , or $-\infty$ to $0$ when you consider that the function is positive for all values of $x$ . Then what sum or area does the value of the antiderivative of $e^{-x}$ actually represent?","['integration', 'calculus', 'algebra-precalculus']"
4244411,Integration of $\sin^2x$ without using double angle identity of $\cos 2x$,"I want to integrate $\sin^2x$ without using the double angle identity of $\cos2x$ . Here's what I tried: $$
\int \sin^2x dx
= \int \sin x \tan x \cos x dx
$$ Let $u = \sin x$ => $du = \cos x dx$ And if $u = \sin x$ , $\tan x = \frac{u}{\sqrt{1-u^2}}$ , therefore $$
=\int u × \frac{u}{\sqrt{1-u^2}}du = \int \frac{u^2}{\sqrt{1-u^2}}du
$$ Now if $t = \sqrt{1 - u^2}$ , $2dt = \frac{du}{\sqrt{1-u^2}}$ and $u^2 = 1 - t^2$ $$
= 2\int (1-t^2)dt = 2t - \frac{2t^3}{3}$$ Substituting back $u$ and $\sin x$ $$
= 2\sqrt{1-u^2} - \frac{2}{3}(\sqrt{1 - u^2})^3
$$ $$
= 2\cos x - \frac{2}{3}\cos^3x
$$ But when you differentiate it you get $-2sin^3x$ All the steps seem right to me, why is the answer wrong or what I did is wrong, and is using the double angle formula the only way to integrate it?","['integration', 'calculus', 'trigonometry']"
4244469,Similarity reductions in Black Scholes PDE,"Suppose that $V(S,I,t)$ satisfies the equality $$
\frac{\partial V}{\partial t}+\frac{1}{2}\sigma^2S^2 \frac{\partial^2V}{\partial S^2}+S\frac{\partial V}{\partial I}+rS\frac{\partial V}{\partial S}-rV=0
$$ Here $I=\int _0^t S dt$ . Now, let $R=S/I$ and $V(S,R,t)=I\,W(R,t)$ .
Is it true that $W$ satisfies the following equality? $$\frac{\partial W}{\partial t}+\frac{1}{2}\sigma^2R^2\frac{\partial^2 W}{\partial R^2}+R(r-R)\frac{\partial W}{\partial R}-(r-R)W=0 
$$","['partial-derivative', 'calculus', 'analysis', 'partial-differential-equations']"
4244497,How to prove a non-monotone function is non-negative in some given interval?,"Consider the function $f(x, n) = -\left(2n+3\right) x^{n}+\left(2n+1\right)x^{\left(n+1\right)}+4\left(n+1\right)x^{\left(2n\right)}-\left(4n+1\right)x^{\left(2n+1\right)}+1, x\in (0, 1), n=1, 2,\cdots$ . We'd like to prove $f(x, n)$ is non-negative $\forall x \in (0, 1), n=1, 2, \cdots$ . Something I tried. We can plot the function in the interval and find it's positive and non-monotone. But since it is difficult to determine the root where the derivative is 0, I have no idea how to prove it is always positive in the interval $(0, 1)$ for and $n=1, 2, \cdots$ . I tried to compute the partial derivative about $n$ , but the partial derivative is not always positive or negative. Here is the graph for the partial derivative. Is there any way to prove the nonnegativity?","['calculus', 'functions', 'real-analysis']"
4244523,Is unimodular stable under local isomorphisms?,"Let $G$ and $H$ be locally compact groups. Suppose that $G$ and $H$ are locally isomorphic. If $G$ is unimodular, is it true that $H$ is unimodular ? Two topological groups $G$ and $H$ are said locally isomorphic if there exists open neighborhoods $V_G$ , $V_H$ of $e_G$ and $e_H$ and a homeomorphism $f:V_G \to V_H$ such that for all $x,y \in V_G$ such $xy \in V_G$ we have $f(xy)=f(x)f(y)$ and similarly for $f^{-1}$ .","['measure-theory', 'topological-groups', 'locally-compact-groups', 'lie-groups']"
4244525,Substituting $x$ with $\sin\theta$ during integration,"$\int x^2 \sqrt{1-x^2}dx =?$ I know how to solve this math but am facing a conceptual confusion. If we substitute $x$ with $\sin\theta$ we can simply solve this problem but my confusion lies in the part $\sqrt{1-x^2}$ . Putting $x=\sin\theta$ we can show this equals $\cos\theta$ . But why is it $\cos\theta$ instead of $\pm\cos\theta$ ?. Why are we only considering the principal values of $\theta$ ? Why are we just simply pulling of any substituent of $x$ and restricting their values? How is it not violating any identity? Pardon me for asking such type of question in this forum. Edit: Using second approach as shown by sir Jose Carlos Santos I solved the integral in the following way. $x=\sin\theta => dx=\cos\theta d\theta$ Now, $\int{x^2\sqrt{1-x^2} dx} $ $=\int{\sin^2\theta (-\cos\theta) \cos\theta d\theta} $ $= -\frac1 8 \int{(1-\cos4\theta) d\theta}$ $=-\frac1 8 [1-4\sin\theta\cos\theta(1-2\sin^2\theta)]+c $ $= -\frac 1 8 [\arcsin(x)+x\sqrt{1-x^2}(1-2x^2)]+c $ Where did I do wrong? There shouldn't have been a negative sign before $\arcsin(x)$","['integration', 'indefinite-integrals', 'trigonometry']"
4244545,For how many $n$ is $2021^n$ + $2022^n$ + $2023^n$ + ... + $2029^n$ prime?,"For how many $n$ is $2021^n$ + $2022^n$ + $2023^n$ + ... + $2029^n$ prime? My first thought is set x = 2021 so we can create: $x^n$ + $(x+1)^n$ + $(x+2)^n$ + ... + $(x+8)^n$ And then we expand each term even though we don't know $n$ ? So for example: $(x+1)^n$ = $x^n$ + $\dbinom{n}{1}$$x^{n-1}$ + $\dbinom{n}{2}$ $x^{n-2}$ + ... + $\dbinom{n}{n-1}$ $x^{n-(n-1)}$ + $\dbinom{n}{n}$ $x^{n-n}$ I understand this, but how do I go about showing that the original sum is prime? I don't think some obvious pattern will arise when I've expanded every term that will show whether or not it is prime.","['number-theory', 'binomial-coefficients', 'combinatorics', 'binomial-theorem']"
4244568,Projection matrices with constant diagonal,"Given $k<n$ , does there always exist an $n\times n$ projection matrix of rank $k$ with a constant diagonal, i.e., all diagonal entries equal to $k/n$ ? Here, when I say projection matrix, I mean specifically orthogonal projection, so $P=P^2=P^*$ It is easy to construct examples when $k=1$ , or when $k=2$ and $n$ is even, and given a Hadamard matrix of order $n$ , one can use it to construct examples of projection matrices of rank k and size n for all $k<n$ .  However, trying to find an example with $n=3, k=2$ seems difficult enough (the way I approached it required using a computer to solve a system of 6 quadratic equations in 6 unknowns). Is there a construction that works for general $n,k$ ? Is there a non-constructive proof that such a projection exists for every $n$ and $k$ ?","['matrices', 'projection-matrices', 'linear-algebra']"
4244588,How does this log rule work?,"In my textbook, I saw the following steps: Step (1): $\ln|v(t)-49| = -\frac{t}{5}+C$ Step (2): $v(t) = 49 + ce^{-t/5}$ How did they go from step 1 to 2.
When solving this, I would think to put $v(t) = e^{-t/5+C} +49$ .
Can someone explain how the c was brought to the front as a product?","['algebra-precalculus', 'ordinary-differential-equations', 'logarithms']"
4244598,Determine and classify stationary points of the function $f (x) = x(1− \ln x)$,I'm stuck on what to do after finding the derivative. So far I have done: $$f(x)=x(1-\ln x)$$ $$f'(x)=1(1-\ln x)\frac1x\tag{Applying the chain rule}$$ $$f'(x)=\frac1x(1-\ln x)$$ (as the original question differentiated) From there where do i proceed and determine and classify all the stationary points?,"['calculus', 'derivatives']"
4244606,"meaning of sentence that a ""presheaf/K-theory satisfies descent on a Grothendieck site""","I'm reading a post about Nisnevich topology and I would like
to clarify what the author means in Definition 1.5: We define $\mathrm{Spc}_S = L_{\mathrm{Nis}}\mathcal{P}(\mathrm{Sm}_S)$ to be the full
subcategory of $\mathcal{P}(\mathrm{Sm}_S)$ consisting of presheaves
that satisfy descent with respect to Nisnevich covers.
Such presheaves are also said to be Nisnevich local. I have a general question what does this precisely mean
if one says that *something satisfies Nisnevich descent *
or satisfy descent with respect to Nisnevich covers . More generally we can replace Nisnevich by any other
Grothendieck site. The something may be a presheaf. So may I assume that
the the meaning of the statement that a presheaf defined over a
Grothendieck site satisfies descent means just that
that this presheaf satisfies the sheaf axiom for every
cover with respect this Grothendieck topology; that is it's
just a sheaf with respect this Grothendieck topology? Is that's what is meant when is said that that
a presheaf satisfies descent over a certain cite? But the something may also be something else, e.g.
algebraic K-theory ( https://ncatlab.org/nlab/show/Nisnevich+site#idea ).
What does it mean here that Algebraic K-theory satisfies descent
over the Nisnevich site ?","['descent', 'algebraic-geometry', 'grothendieck-topologies', 'sheaf-theory']"
4244608,Is there a reason a J-1 ring is defined as not necessarily being J-0 ring,"In the comments of a recent solution I noticed that the wikipedia definition of J-1 ring does not necessarily make it a J-0 ring. Wikipedia says a commutative ring $R$ is: a J-0 ring if the set of regular elements of $Spec(R)$ contains a nonempty open set a J-1 ring if the set of regular elements is an open set of $Spec(R)$ The topology is, of course, the usual Zariski topology. This of course leaves the possibility that the set of regular elements is empty, and that would mean such a ring is J-1 but not J-0. As in the linked post, KReiser has pointed out that Wikipedia, the Stacks project, and Matsumura's books all seem to use the same definition, and it can indeed happen, for example, with $k[x]/(x)^2$ . Now, it seems to me rather abhorrent to choose a nomenclature like J-0, J-1, J-2 which does not wind up being a hierarchy, but I'm not a commutative algebraist, what do I know. My question here is: are there good reasons commutative algebraists prefer to admit rings with no regular elements in their spectrum to be J-1? Alternatively, why not admit rings with no regular elements in their spectrum to be J-0? The usual tradeoff would be, of course, that it makes certain statements in the theory easier, but I do not know the particular theory that uses this or have many references to it, so I'd like to ask those of you who are familiar whether or not the tradeoff is justifiable with examples. (Note: KReiser also provided a useful sufficient condition that J-1 will imply J-0: when $R$ is a reduced ring.)","['ring-theory', 'algebraic-geometry', 'commutative-algebra']"
4244624,"Using logarithmic differentiation or otherwise, differentiate $y = (x −1) (x − 2) (x − 3$)? And show that $y' = 3x^2-12x+11$","So far this is my method: \begin{align*}
y = (x-1)(x-2)(x-3) & \Longleftrightarrow \ln(y) = \ln(x-1) + \ln(x-2) + \ln(x-3)\\\\
& \Longleftrightarrow \frac{y'}{y} = \frac{1}{x-1} + \frac{1}{x-2} + \frac{1}{x-3}\\\\
& \Longleftrightarrow 
y' = y\left(\frac{1}{x-1} + \frac{1}{x-2} + \frac{1}{x-3}\right)
\end{align*}","['calculus', 'derivatives', 'algebra-precalculus']"
4244631,"$\int_0^{\pi/2} e^{-\tan^2(x)}\,dx=\frac{\pi e}{2}\big(1-\operatorname{erf}(1)\big)$ [duplicate]","This question already has answers here : Methods to solve $\int_{0}^{\infty} \frac{e^{-x^2}}{x^2 + 1}\:dx$ (3 answers) Closed 2 years ago . Prove that $$\int_0^{\pi/2} e^{-\tan^2(x)}\,dx=\frac{\pi e}{2}\big(1-\operatorname{erf}(1)\big)$$ My Attempt Let $u=\tan(x)\implies du=\sec^2(x)\,dx=1+u^2\,dx$ $$I=\int_0^{\pi/2} e^{-\tan^2(x)}\,dx=\int_0^\infty \frac{e^{-u^2}}{1+u^2}\,du$$ How do you proceed from here? I was thinking of integrating by parts but it doesn't seem to work. How do you solve this integral? Thank you for your time","['integration', 'calculus', 'error-function', 'real-analysis']"
4244669,How can I prove this property of an ellipse?,"I am reading  Maxwell's Matter and Motion and he has this construction as a step in deriving Newton's law of Gravitation from Kepler's First Law. In this construction $SU$ is equal to the ellipse's major axis $AB$ , and $PZ$ is the perpendicular bisector to $HU$ .
Maxwell states that $HZ \times SY=b^2$ , with $b$ being the length of the ellipse's semiminor axis. I can see how this is valid when $HZ=SY$ and I have an idea of how to derive it analytically, but I would like to know how to derive it from properties of the circle and the ellipse, etc. using classical geometry.","['conic-sections', 'geometry']"
4244684,How can I prove certain properties of these vector fields on $S^3$?,"On the unit 3-sphere in 4-dimensional Cartesian coordinate space, one can find a set of three orthonormal vector fields that parallelize the 3-sphere. (Utilize the correspondence between the unit 3-sphere and the coordinates of a geometric algebra rotor, and then infinitesimal displacements on the 3-sphere map to/from infinitesimal rotations in 3 orthogonal directions. See below for explicit expressions of the vector fields.) It turns out it is easiest to find the vector fields in terms of the extrinsic 4d coordinates on the 3-sphere - they are just linear functions (and very simple ones at that) of the 4d coordinates. But there are some properties of these vector fields that I want to prove - namely, that they satisfy certain commutation relations (analogous to the commutation relations of the angular momentum operators in quantum mechanics) and that the vector fields are divergence-free - that I only know how to prove by introducing an intrinsic 3d coordinate system. But doing so is laborious. My question is, is there a simple/elegant way to prove the commutation and divergence properties of these vector fields without introducing an intrinsic coordinate system, e.g. by using their symmetry properties or the fact that they generate infinitesimal rotations in orthogonal directions? For further info: on the 3-sphere characterized by the set of points $(w,x,y,z)$ such that $w^2+x^2+y^2+z^2=1$ , the three vector fields are: $$e_1=(-x,w,z,-y)$$ $$e_2=(-y,-z,w,x)$$ $$e_3=(-z,y,-x,w)$$ And the Lie brackets that I want to prove are: $$[e_i,e_j]=2\epsilon_{ijk}e_k$$","['vector-fields', 'differential-geometry']"
4244692,"I have a square, and place three dots along the 4 edges at random. What is the probability that the dots lie on distinct edges?","The correct answer is listed as (3/4) * (1/2) = 3/8. The reasoning is because it doesn't matter where you put the first dot, but after you put the first dot, it is 3/4 chance of a different edge, and then 1/2 chance of another different edge. However, if I think about it from a purely combinatorics way, there are ""4"" ways that three dots can be arranged on the square in which all dots are on distinct edges. There are then 12 ways that two dots can be on one edge and the third dot is on another edge (4 edges to put two dots, multiplied by 3 possible edges to put the third dot). Finally, there are 4 ways to put all three dots onto a single edge. This gives us (4/20) probability where the three dots are on distinct edges. This should work since each dot is independent of the previous dot and we can assume a uniform probability distribution. Why are these two values different?","['statistics', 'combinatorics', 'probability']"
4244699,What is the probability of picking a full set from multiset after $m$ draws?,"Suppose a bag contains $n$ balls labeled from $1$ to $n$ , and suppose I have $k$ of these bags. If I open all of these $k$ bags into an urn, then the urn is effectively a multiset with $kn$ elements: $k$ balls labeled $1$ , $k$ balls labeled $2$ , and so on up to $n$ . My question is If I were to randomly pick out balls from the urn one by one without replacement, what's the probability of having picked out a complete set of balls labeled $1$ to $n$ after $m$ balls have been pulled out of the urn? I'm not very familiar with probability, so I'm not sure what's the correct setup for the problem. I suspect the answer has to do with the binomial coefficients, as we're choosing $m$ elements from a set with $kn$ things, but I don't know how to account for the repetition of elements. Any and all help is greatly appreciated. Thank you!","['binomial-coefficients', 'combinatorics', 'recreational-mathematics', 'probability-theory', 'probability']"
4244705,Evaluate $\int_0^\infty \frac{e^{-x}}{x}\ln\big(\frac{1}{x}\big) \sin(x)dx$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm having trouble with this integral Evaluate $$\int_0^\infty \frac{e^{-x}}{x}\ln\left(\frac{1}{x}\right) \sin(x)dx$$ $$I=\int_0^\infty \frac{e^{-x}}{x}\ln\left(\frac{1}{x}\right) \sin(x)dx=\Im\left[\int_0^\infty \frac{e^{-x+ix}}{x}\ln\left(\frac{1}{x}\right) dx\right]$$ How would one proceed from here ? $u$ substitute $u=\ln(x)$ ? How do you solve this integral? Thank you for your time.","['integration', 'complex-analysis', 'calculus', 'definite-integrals']"
4244708,Showing a solution to 3rd order differential equations forms a subspace,"Let $S$ denote the set of all solutions of the following differential equation defined on $C^3[0,\infty)$ ; $$
\begin{align}
  \frac{d^3x}{dt^3} + b \frac{d^2x}{dt^2} + c \frac{dx}{dt} + dx = 0
\end{align}
$$ Show that $S$ is a linear subspace of $C^3[0, \infty)$ I know that subspaces are closed under addition and scalar multiplication and also contan the zero vector, but I'm not sure how to show that all solutions form a linear subspace. (I'm not sure how to solve this equation either) Final Answer Note that: $$
\begin{align}
   0 &= \frac{d^3x}{dt^3} + b \frac{d^2x}{dt^2} + c \frac{dx}{dt} + dx \\
     &= \alpha \left(\frac{d^3x}{dt^3} + b \frac{d^2x}{dt^2} + c \frac{dx}{dt} + dx\right)
\end{align}
$$ Where $\alpha$ is a constant. Consider arbitrary functions $f$ and $g$ and scalar $\alpha$ . By linearity of operators we have: $$
\begin{align}
\frac{d^3}{dt^3}(\alpha f + g ) &= \alpha \frac{d^3f}{dt^3} + \frac{d^3g}{dt^3} \\
\frac{d^2}{dt^2}(\alpha f + g ) &= \alpha \frac{d^2f}{dt^2} + \frac{d^2g}{dt^2} \\
\frac{d}{dt}(\alpha f + g ) &= \alpha \frac{df}{dt} + \frac{dg}{dt} \\
d(\alpha f + g) &= \alpha df + dg
\end{align}
$$ Now assign $f$ and $g$ to arbitrary individual solutions to our differential equation, we have: $$
\begin{align}
  0 &= \frac{d^3}{dt^3}(\alpha f + g) + b \frac{d^2}{dt^2}(\alpha f + g) + c \frac{d}{dt}(\alpha f + g) + d(\alpha f + g) \\
  &= \alpha(\frac{d^3}{dt^3}f + b \frac{d^2}{dt^2}f + c \frac{d}{dt}f + df ) + (\frac{d^3}{dt^3}g + b \frac{d^2}{dt^2}g + c \frac{d}{dt}g + dg) \\
  &= \alpha \pmb f + \pmb g 
\end{align}
$$ This shows that $\alpha \pmb f + \pmb g$ solves the ODE which proves that $\alpha \pmb f + \pmb g \in \mathcal S$ The addition holds because the sum that describes $\pmb f$ sums to $0$ . Likewise for $\pmb g$ . Therefore, all arbitrary combinations of $\pmb f$ and $\pmb g$ are in $\mathcal S$ (This can be seen by also allowing $\pmb f$ be $\pmb g$ and $\pmb g$ be $\pmb f$ ). Finally, $\pmb 0$ is also in the set of solutions. Thus, the set of all solutions forms a subspace.","['vector-spaces', 'ordinary-differential-equations']"
4244722,When to apply l'Hôpital rule?,"I'm confused about when to apply l'Hôpital Rule, here is an example: \begin{align} L=\lim_{x \to\infty} \frac{1}{x^2}\int_0^x \frac{t}{2+\sin(t)}dt\\ \end{align} Approach 1 : Let $F(x)$ is antiderivative of $\frac{x}{2+\sin(x)}$ , then $\int_ 0^x \frac{t}{2+\sin(t)}dt= F(x)-F(0)= F(x)+C$ . So we got our limit: \begin{align} L=\lim_{x\to\infty} \frac{F(x)+C}{x^2}\\ \end{align} It's easy to see that this is $\frac{\infty}{\infty}$ case so we apply l'Hôpital Rule: \begin{align} L=\lim_{x\to\infty} \frac{\frac{d}{dx}(F(x)+C)}{\frac{d}{dx}(x^2)}=\lim_{x\to\infty} \frac{\frac{x}{2+\sin(x)}}{2x}= \lim_{x\to\infty} \frac{1}{4+2\sin(x)}\\ \end{align} However, the limit results in nothing. Approach 2 : Use $\lim _{x \to\infty} \frac{1}{x^2}\int_0^x tf(\sin(t))dt=\frac{1}{4\pi}\int_{-\pi}^{\pi} f(\sin(t))dt $ . We have: \begin{align} L= \frac{1}{4\pi}\int_{-\pi}^{\pi} \frac{1}{2+\sin(t)}dt=\frac{1}{2\sqrt{3}}\\ \end{align} Can you help me point out any mistakes in approach 1 and why the l'Hôpital Rule doesn't work?","['limits', 'calculus']"
4244726,Computing derivatives of the sum $\sum_{n = -\infty}^\infty \frac{\lvert x \rvert}{(1 + n^2 x^2)^{3/2}}$,"Given a sum such as $\sum_{n = -\infty}^\infty \frac{\lvert x \rvert}{(1 + n^2 x^2)^\frac{3}{2}}$ , the first derivative (with respect to $x$ ) of the summand $\frac{\lvert x \rvert}{(1 + n^2 x^2)^\frac{3}{2}}$ does not exist. Nonetheless, numerical estimates such as: seem to indicate that the sum itself is a smooth function of $x$ (and it seems like all derivatives vanish at $x=0$ ). What tools are there to show that this sum defines a smooth function of $x$ , and to compute the derivatives (especially at $x = 0$ )?","['summation', 'smooth-functions', 'sequences-and-series', 'limits', 'derivatives']"
4244736,How to show that a quaternionic matrix is not a product of two involutions?,"Problem. Let $$A=\begin{pmatrix}i&0\\0&1\end{pmatrix}$$ be a matrix over the real quaternion ring $\mathbb{H}.$ Prove that it is similar to its inverse, but is not a product of two involutions. Can it be a product of commutators of involutions? A square matrix $A$ over a ring is called similar to $B$ if there exists an invertible matrix $P$ such that $A=PBP^{-1}.$ A square matrix $T$ over a ring is called an involution if $T^2=I$ is the identity matrix. A square matrix $A$ over a ring is called a commutator of involution if there exist involutions $T, S$ such that $A=TST^{-1}S^{-1}.$ I have shown $$A^{-1}=\begin{pmatrix}-i&0\\0&1\end{pmatrix},$$ and $$A=\begin{pmatrix}ij&0\\0&1\end{pmatrix}A^{-1}\begin{pmatrix}ij&0\\0&1\end{pmatrix}^{-1}.$$ But I don't answer this remaining. I don't know why it is not a product of two involutions. If this is true, then it can be a product of commutators of involutions, can't it? Thanks for all your support.","['matrices', 'abstract-algebra', 'linear-algebra', 'matrix-decomposition']"
4244744,Understanding $\tan\theta=\left|\frac{m_1-m_2}{1+m_1m_2}\right|$,"This is the formula for finding the angles between two straight lines: $$\tan\theta=\left|\frac{m_1-m_2}{1+m_1m_2}\right|$$ $$\implies \tan\theta=\pm \frac{m_1-m_2}{1+m_1m_2}$$ $$\implies \theta=\pm\arctan\left(\frac{m_1-m_2}{1+m_1m_2}\right)$$ In LHS, only positive values of $\theta$ can be inputted. Now, according to my book and this derivation , the $\pm$ has been included to include both the acute and obtuse angles between the straight lines. However, according to @AmanKushwaha , the $\pm$ sign has been included to include the positive (anticlockwise) and negative (clockwise) acute angles between the two straight lines. Who is correct? Moreover,  can't the acute angle measured in the clockwise direction also represent the obtuse angle formed between any two straight lines?","['coordinate-systems', 'trigonometry', 'geometry']"
4244746,Integral of $~\int\frac{\sqrt{x^{2}+l^{2}}}{x}dx~$ I tried the substitution of the variable but it didn't work,"I need to derive the below RHS from the below LHS. $$\int\frac{\sqrt{x^{2}+l^{2}}}{x}dx=\sqrt{x^{2}+l^{2}}-l\cdot\ln\left(\frac{l+\sqrt{x^2+l^2}}{x}\right)\tag{1}$$ $$x,l\in\mathbb R_{>0}\tag{2}$$ My tries are as below. $$x=l\cdot\tan^{}\left(\theta_{}\right)\tag{3}$$ $$\frac{dx}{d\theta}=l\cdot\sec^{2}\left(\theta_{}\right)\tag{4}$$ $$dx=l\cdot\sec^{2}\left(\theta_{}\right)\cdot d\theta\tag{5}$$ $$\sqrt{x^{2}+l^2}=\sqrt{\left(l\cdot\tan^{}\left(\theta_{}\right)\right)^{2}+l^{2}}\tag{6}$$ $$=\sqrt{l^{2}\cdot\tan^{2}\left(\theta_{}\right)+l^{2}}\tag{7}$$ $$=\sqrt{l^{2}\left(1+\tan^{2}\left(\theta_{}\right)\right)}\tag{8}$$ $$=l\sqrt{1+\tan^{2}\left(\theta_{}\right)}\tag{9}$$ $$=l\sqrt{\sec^{2}\left(\theta_{}\right)}\tag{10}$$ $$=l\sec^{}\left(\theta_{}\right)\tag{11}$$ $$\therefore~~\int\frac{\sqrt{x^2+l^2}}{x}dx=\int\frac{l\sec^{}\left(\theta_{}\right)}{l\cdot\tan^{}\left(\theta_{}\right)}\left(l\cdot\sec^{2}\left(\theta_{}\right)\cdot d\theta\right)\tag{12}$$ $$=\int\frac{l\cdot\sec^{3}\left(\theta_{}\right)}{\tan^{}\left(\theta_{}\right)}d\theta\tag{13}$$ $$=l\int\frac{\sec^{3}\left(\theta_{}\right)}{\sin^{}\left(\theta_{}\right)\cdot\sec^{}\left(\theta_{}\right)}d\theta\tag{14}$$ $$=l\int\frac{\sec^{2}\left(\theta_{}\right)}{\sin^{}\left(\theta_{}\right)}d\theta\tag{15}$$ $$A:=\int\frac{\sec^{2}\left(\theta_{}\right)}{\sin^{}\left(\theta_{}\right)}d\theta\tag{16}$$ $$t=\sec^{}\left(\theta_{}\right)\tag{17}$$ $$\frac{dt}{d\theta}=\left(\cos^{-1}\left(\theta_{}\right)\right)'~~\leftarrow~~\text{Not a}~\arccos\left(\theta_{}\right). \tag{18}$$ $$=\left(-1\right)\left(\cos^{-2}\left(\theta_{}\right)\right)\left(-\sin^{}\left(\theta_{}\right)\right)\tag{19}$$ $$=\frac{\sin^{}\left(\theta_{}\right)}{\cos^{2}\left(\theta_{}\right)}\tag{20}$$ $$\frac{d\theta}{dt}=\frac{\cos^{2}\left(\theta_{}\right)}{\sin^{}\left(\theta_{}\right)}\tag{21}$$ $$d\theta=\left(\frac{\cos^{2}\left(\theta_{}\right)}{\sin^{}\left(\theta_{}\right)}\right)dt\tag{22}$$ $$\cos^{}\left(\theta_{}\right)=\sec^{-1}\left(\theta_{}\right)\tag{23}$$ $$\therefore~~\cos^{2}\left(\theta_{}\right)=\sec^{-2}\left(\theta_{}\right)\tag{24}$$ $$d\theta=\left(\frac{\sec^{-2}\left(\theta_{}\right)}{\sin^{}\left(\theta_{}\right)}\right)dt\tag{25}$$ $$A=\int\frac{\sec^{2}\left(\theta_{}\right)}{\sin^{}\left(\theta_{}\right)}d\theta=\int\frac{\sec^{2}\left(\theta_{}\right)}{\sin^{}\left(\theta_{}\right)}\left(\left(\frac{\sec^{-2}\left(\theta_{}\right)}{\sin^{}\left(\theta_{}\right)}\right)dt\right)\tag{26}$$ $$=\int\frac{1}{\sin^{2}\left(\theta_{}\right)}dt~~\leftarrow~~\text{Not good}\tag{27}$$ How should I've done of the integration by substitution?","['integration', 'trigonometry', 'systems-of-equations', 'logarithms']"
4244796,Probability taking a loan in a bank,"A bank has $1500$ guests. From them the $1000$ has taken home loan and $700$ consumer loan. a) Calculate the probability that a guest has taken home loan given that he has taken consumer loan. b) The bank want to make an offer  for consumer loan for those who took a home loan but not a consumer loan. To how many people will they send the offer? c) Each day at the bank come average $4$ guests that want to take a loan. What is the probability that one day between 8 in the morning till 4 in the afternoon, that $1$ or $2$ guests get served? d) for the guests that took consumer loan we know that $40\%$ are women and $60\%$ are men. From the men the $50\%$ are of age $45-55$ and from the women $40\%$ . A guest of age $45-55$ is in the bank to take a loan. Which is the probability that it is a woman of age $45-55$ ? e) For the guests that took consumer loan we know that the probability  one of them took a loan less than $5000$ euros is $0.14$ . If we take randomly $7$ guests, then which is probability at most $2$ of them have taken a loan less than $5000$ euros ? f) If we check these $7$ guests whici is the probability only the fourth one has taken loan less than $7000$ euros? $$$$ a) Does it hold that $P(H\cap C)=\frac{200}{1500}$ since $1000+700-1500=200$ ? Then the probability is equal to $$P(H\mid C)=\frac{P(H\cap C)}{P(C)}=\frac{\frac{200}{1500}}{\frac{700}{1500}}=\frac{200}{700}\approx 28,57\%$$ Is that correct? b) We have to calculate the number for $H\cap C^c$ . Is this equal to $1000-700 =300$ ? c) Do we have Poisson distribution since we have an average? We have that $\lambda=4$ . Then $$P(X=1)+P(X=2)=e^{-4}\cdot \frac{4^1}{1!}+e^{-4}\cdot \frac{4^2}{2!}=\frac{12}{e^4}\approx 0.21979$$ Is that correct? d) Is the probability equal to \begin{align*}P(W\mid A)&=\frac{P(A\mid W)\cdot P(W)}{P(A)}\\ & =\frac{P(A\mid W)\cdot P(W)}{P(A\mid W)\cdot P(W)+P(A\mid M)\cdot P(M)}\\ & =\frac{0.40\cdot 0.40}{0.40\cdot 0.40+0.50\cdot 0.60}\\ & =34.78\%\end{align*} e) Is the probability equal to \begin{align*}P(X\leq 2)&=P(X=0)+P(X=1)+P(X=2)\\ & =\binom{7}{0}\cdot 0.14^0\cdot (1-0.14)^{7-0}+\binom{7}{1}\cdot 0.14^1\cdot (1-0.14)^{7-1}+\binom{7}{2}\cdot 0.14^2\cdot (1-0.14)^{7-2}\\ & \approx 0.9380\end{align*} f) Is there a typo and it must be $5000$ instead of $7000$ ? Or is it possible to calculate the probability with $7000$ without knowing the probability for ""sucess"" for that?","['statistics', 'conditional-probability', 'probability-theory', 'bayes-theorem', 'probability']"
4244811,Is this set compact or not?,"Let X be a topological space and $ f: X \to [0,1] $ be a closed continuous surjective map such that $f^{-1}(a)$ is compact for every $0\leq a\leq 1$ . Prove or disprove: X is compact. I thought of first trying to prove this. If I take $X\subseteq\bigcup_{i=1}^{\infty} O_i$ . But the problem is that all the information given in the question is regarding the set on the RHS ie [0,1] like surjectivity , continuity. So, I am unable to prove this. Also, I am not good with the  constructing examples of topological spaces so I am not able to construct a closed map which is also continuous and surjective. Can you please help?","['general-topology', 'compactness']"
4244874,Can we prove AM-GM Inequality using these integrals?,"I came across these two results recently: $$ \int_a^b \sqrt{\left(1-\dfrac{a}{x}\right)\left(\dfrac{b}{x}-1\right)} \: dx = \pi\left(\dfrac{a+b}{2} - \sqrt{ab}\right)$$ $$ \int_a^c \sqrt[3]{\left| \left(1-\dfrac{a}{x}\right)\left(1-\dfrac{b}{x}\right)\left(1-\dfrac{c}{x}\right)\right|} \: dx = \dfrac{2\pi}{\sqrt{3}}\left(\dfrac{a+b+c}{3} - \sqrt[3]{abc}\right)$$ for $0<a\leq b\leq c$ . I haven't tried to solve the first one yet, but I have an idea of how to approach it, namely using the substitution $x=a\cos^2\theta+b\sin^2\theta$ . I have no idea how to approach the second one, however. I think that the most interesting thing about the results above is that it seems like there is a proof for the AM-GM inequality hidden within. Clearly both integrands are positive and so the AM-GM falls out for the 2 and 3 variable case. All that is required is to prove the results. My question is twofold: How would the second integral be computed? Is there an approach using elementary techniques? Can this be generalised to prove the AM-GM inequality for $n$ -variables?","['integration', 'calculus', 'a.m.-g.m.-inequality', 'inequality']"
4244910,Matrix with only positive entries whose inverse has only positive entries,I'm looking for a class of matrices such that if it contains a matrix with only positive entries then the inverse of said matrix also has only positive entries. I imagine an example of such a class would be the class of orthogonal matrices where the inverse is the transpose but i'm looking for a more general class if possible.,['matrices']
4244923,Evaluating the integral $ \int_0^1 \frac{e^{-y^2(1+v^2)}}{(1+v^2)^n}dv$,"I am trying to evaluate the integral $$
 \int_0^1 \frac{e^{-y^2(1+v^2)}}{(1+v^2)^n}dv = e^{-y^2}\int_0^1 \frac{e^{-y^2v^2}}{(1+v^2)^n}dv
$$ for $n\in \mathbb{N}$ .For n=1 one finds Owen's T function , i.e. \begin{align}
\int_0^1 \frac{e^{-y^2(1+v^2)}}{(1+v^2)}dv=2\pi \operatorname{T}\left(\sqrt{2} y,1\right) = \frac{\pi}{2} \operatorname{erfc}(y) \left(1 - \frac{1}{2} \operatorname{erfc}(y)\right)
\end{align} A nice source on the Owen's T function is [ 2 ]. In [ 3 ] they state that \begin{align}
\int \frac{e^{-v^2}}{v^2 + 1} dv ,
\end{align} has no anti-derivative. Hence, I do not suspect one can find one for our integral. This integral occurs in a series I am integrating over for a approximation I am performing. Hence, it would already be nice if I could find the second (n=2) and third (n=3) term. Has someone an idea how to evaluate the integral. Many thank in advance!","['integration', 'definite-integrals', 'calculus', 'gaussian-integral', 'gaussian']"
4244966,Why using $p=y'$ doesn't lead to answer in $y^2(1+y'^2)=4$?,"I have the ODE $y^2(1+y'^2)=4$ to solve this I used the substitution $y'=p$ $$y^2(1+p^2)=4$$ $$2y(1+p^2)dy+2py^2dp=0$$ $$(p^2+1)dy+py\;dp=0$$ $$\frac{dy}y+\frac{p}{p^2+1}dp=0$$ $$\ln|y|+\frac12\ln|p^2+1|=\ln|c|$$ $$y\sqrt{p^2+1}=c$$ Using $p^2+1=\frac4{y^2}$ , I get $2=c$ ! I can't find my mistake.",['ordinary-differential-equations']
4244968,What does Aluffi mean by 'pointed set' in the book Algebra: Chapter 0?,"'Pointed sets' are not explicitly defined in the book and I have posted some instances of where they are mentioned according to increasing page number. Chapter 1 Page 19 (As far as I know, this is the first mentioned of pointed set in the book) Page 24 Chapter 2 Page 43 Page 64 I understand the construction in Chapter 1 page 24: Example 3.8, as well as the answer given here why does unique identity make groups pointed sets? , but I feel that the accepted answer does not address the reason as to why the uniqueness of the identity makes 'groups pointed sets' in the sense of Example 3.8 on page 24 (Assuming uniqueness of the identity is relevant at all). Questions According to https://en.wikipedia.org/wiki/Pointed_set , a pointed set is just a pair $(X,x)$ where $X$ is a set and $x\in X$ . But I'm not sure if this is what Aluffi means for otherwise, why does he mention in Chapter 2 page 43 right after proving that the identity is unique in any arbitrary group that this 'makes groups pointed sets' in the sense of Example 3.8 on page 24? So to me it seems that the author is suggesting that the uniqueness of the identity is a contributing factor to groups being pointed sets? But on the other hand considering Example 3.8 does not suggest anything about the requirement of uniqueness in the context of groups. So is uniqueness of the identity in groups important in establishing that groups are pointed sets? https://ncatlab.org/nlab/show/pointed+object defines pointed object $X$ to be an object equipped with a global element $1\to X$ where a global element is just a morphism from a terminal object $1$ to $X$ . A pointed set is the defined to be a pointed object is $\mathbf{Set}$ . Now if I take this definition of a pointed set then every non-empty set in $\mathbf{Set}$ is a pointed object, which isn't quite what the author had in mind when compared to Example 3.8? Or maybe I didn't understand the definition given on nLab. Is it possible to show via example that this definition in nLab is indeed equivalent? To complicate things further, if I use the definition in nLab then in Chapter 2 page 64 $\text{Hom}_{\mathbf{Grp}}(G,H)$ being a pointed set does not make sense to me since I don't even know what category this is in as an object. So what does Aluffi mean by 'pointed set'? What made $\text{Hom}_{\mathbf{Grp}}(G,H)$ a pointed set? Is it merely that it's not empty, or is there something 'special' about the trivial morphism that makes $\text{Hom}_{\mathbf{Grp}}(G,H)$ into a pointed set? At this point, I don't even know what pointed set means anymore and I feel perplexed at this point.","['group-theory', 'abstract-algebra', 'definition', 'category-theory']"
4244997,Estimating the value of $e$ using a random function,"I encountered the question which asks you to estimate the value of $\pi$ by using a function which generates a random real number in $[0,1]$ (and it's uniformly distributed). The way is to use the function twice to get a real pair which we can view it as a point on the Euclidean plane, then use the idea of the Monte Carlo method. I wonder if we can also use this function to estimate $e$ , however, I cannot find any geometrical interpretation of the number, or can this problem be solved in other ways?","['statistics', 'geometry', 'analysis', 'calculus', 'probability']"
4245011,A question about the Cantor Lebesgue function,"I am reading some lecture notes on the Cantor Function. Here is the construction that notes used. The notes used the Cantor set to construct a function. Here the Cantor set is defined to be $\{O_n \}$ which consists of deleted middle thirds. i.e. $$ O_1 = (1/3, 2/3)$$ $$O_2 = (1/9, 2/9) ∪ (1/3, 2/3) ∪ (7/9, 8/9)$$ $$O_3 = (1/27, 2/27) ∪ (1/9, 2/9) ∪ (7/27, 8/27) ∪ · · · ∪ (25/27, 26/27)$$ and so on... Next, they defined a sequence of functions $\{ \varphi_n\}$ as follows. Each function $\varphi_n$ will satisfy $\varphi_n(0) = 0$ and $\varphi_n(1) = 1.$ For $n = 1,$ they define $\varphi_1(x) = 1/2$ on $O_1$ , and extend it linearly to [0, 1]. For $n = 2,$ they defined $\varphi_2$ on the components of $O_2$ by $$\varphi_2(x) =
    \begin{cases}
      1/4, & \text{if}\ x \in (1/9,2/9) \\
      2/4, & \text{if}\ x  \in (1/3, 2/3) \\
3/4, & \text{if}\ x  \in (7/9, 8/9) \\
    \end{cases}$$ and extend it linearly to $[0, 1].$ Then, nn the components of $O_n$ , $\varphi_n$ takes values $1/2^n, 2/2^n, . . . ,(2^n − 1)/2^n,$ and extend it linearly to $[0, 1].$ Thus, a sequence of continuous functions $\{ \varphi_n \}$ is formed. My question here is that, the notes stated ""It is obvious that $| \varphi_{n+1}(x) − \varphi_n(x)| < 2^{−n}$ for every $x \in [0, 1].$ "" Can anyone explain to me why this is so obvious? I do not see it at all.","['cantor-set', 'sequences-and-series', 'real-analysis']"
4245059,What does it mean for a group to be free in a variety of groups,"I'm reading the paper ""Finitely generated cyclic extensions
of free groups are residually finite"" by Baumslag. One of the hypotheses of the main proposition in the paper is that for a group $G$ and $N < G$ a subgroup, $N$ is free in a nilpotent variety $\mathcal{V}$ of prime exponent $p$ . I understand that a variety of groups, roughly speaking, is a collection $\mathcal{V}$ of groups defined by some equation. It is nilpotent if every group in $\mathcal{V}$ is nilpotent, and has exponent $p$ if every group in $\mathcal{V}$ has exponent $p$ . What I don't understand is what Baumslag means by the fact that $N$ is free in $\mathcal{V}$ ? The source of my confusion lies in what comes next, mainly that the group $N$ is assumed to be a free group. Surely if $N$ is free, then it can never be contained in a variety of exponent $p$ ?","['group-theory', 'abstract-algebra', 'terminology']"
4245066,A 'non-numerical\analytic' proof that $\binom{n}{k}$ > $\binom{n}{k-1}$ for large $n \in \mathbb{N}$,"The number of $k$ -subsets of $[n]$ is given by the formula $\binom{n}{k}$ or $^nC_k$ . They famously occur in the expansion of $(1+x)^n$ and they are given by the formula $$\binom{n}{k}=\frac{n!}{(n-k)!k!}$$ Using this formula, it is easy to prove the inequality that $\binom{n}{k}>\binom{n}{k-1}$ for large enough $n \in \mathbb{N}$ . What this inequality says is that the number of ways of choosing $k-1$ -subsets is eventually smaller than the number of ways of choosing $k$ -subsets of $[n]$ . One more way of showing this is by observing that $\binom{n}{k}$ is a polynomial in $n$ of degree $k$ and then, we can see that it will outgrow $\binom{n}{k-1}$ which is a lower degree polynomial in $n$ . Is there a more natural way of seeing this inequality without the use of calculations with the use of something more combinatorial-like? Possibly by exhibiting an injection between the $k-1$ -subsets and $k$ -subsets? Or by another interpretation of the numbers where it is easier to get such an injection? Or something else altogether? $\mathbf{Remark}$ : My supervisor mentioned almost immediately that there was a way to see this using Symmetric Chain Decomposition. But I do not have the luxury of spending that much time on this. I apologise. I would be thankful if you could provide a proof based on the same. $\mathbf{Tl;dr}: $ What I am looking for is something more along the lines of bijections or a different interpretation of the binomial coefficients that makes the inequality easier to see. I hope the approach doesn't rely heavily on calculations and at the same time, explains why the the inequality reverses for small $n$ . Thank you, in advance.","['alternative-proof', 'elementary-functions', 'combinatorics', 'discrete-mathematics']"
4245073,Prove that $\sum{a^{2}bc}=\binom{n+3}{6}+\binom{n+2}{6}$,"Question We define $S$ as the set of positive integer triplets $(a,b,c)$ such that $a+b+c=n$ . I want to prove that the following expression is correct: $$
\sum_{S}{a^{2}bc}=\binom{n+3}{6}+\binom{n+2}{6}
$$ My Solution Left Hand Side Say that there is a line of $n$ balls. We color the first $a$ balls red, the next $b$ balls green, and the rest blue. We then choose two red balls randomly with replacement, choose one green ball randomly, and choose one blue ball randomly. The number of possibilities is given by $\sum_{S}{a^{2}bc}$ Right Hand Side If the same red ball is chosen twice; $$
\begin{align}
x_{1}&\text{ red balls lie on the left of the chosen red ball}\\
x_{2}&\text{ red balls lie on the right of the chosen red ball}\\
x_{3}&\text{ green balls lie on the left of the chosen green ball}\\
x_{4}&\text{ green balls lie on the right of the chosen green ball}\\
x_{5}&\text{ blue balls lie on the left of the chosen blue ball}\\
x_{6}&\text{ blue balls lie on the right of the chosen blue ball}
\end{align}
$$ Since $\sum_{i=1}^{6}{x_{i}=n-3}$ , using stars and bars we get $\binom{n+2}{5}$ possibilities. If different red balls are chosen, the approach is the same but we also need to count the number of red balls between the two chosen red ball and multiply by two because either red ball can be chosen first. This give us $2\binom{n+2}{6}$ possibilities. Since I counted the same possibilities, then the expressions must be equal: $$
\begin{align}
\sum_{S}{a^{2}bc}&=2\binom{n+2}{6}+\binom{n+2}{5}\\
\\
&=\binom{n+3}{6}+\binom{n+2}{6}
\end{align}
$$ I’d like to know if my solution is correct and if there is alternative solution or discussion.","['solution-verification', 'combinatorics', 'binomial-coefficients', 'combinatorial-proofs']"
4245077,Question 2.18 from Brezis: Would someone help me to understand a solution given to this question?,"I underlined parts of the solution where I'm struggling with. Would someone help me with this? In (Part 1) I did not understand the equality in (1), because from
the theory, for me $$N(A^*)=\{v\in D(A^*):  A^*v=0 \in E^*)\}\subset
   F^*$$ So, how can I find the equality in part 1, from this equality
above? In (Part 2), the doubts in (2) are: First of all, why $A$ being closed implies that $G(A)$ is a Banach subspace of $E\bigoplus F$ ?(I tried to explain using the closed graph theorem, but it did not work). Secondly, I did not understand why there exists a continuous linear functional such that $f|_{G(A)}\equiv 0$ ? Finally, I did not understand why $f(v,0)=1$ ? In topic (3) underlined, the doubts are: why $f\longrightarrow (0,f)$ is a bounded linear map? (Who is the domain and the codomain of $f$ ?) Also, why $f(0, Av_n)\longrightarrow 0?$ At topic (4) underlined, I did not understand why $\displaystyle\lim_{n\rightarrow \infty} f(v_n, Av_n)-f(0, Av_n)$ is going to be zero?","['banach-spaces', 'adjoint-operators', 'functional-analysis', 'closed-graph']"
4245114,Why is enveloping algebra called enveloping algebra?,"What does the enveloping algebra of $\mathfrak{g}$ have to do with envelopes? If $\mathfrak{g}$ is a Lie algebra, we take tensor algebra on $\mathfrak{g}$ and make quotient through ideal of T, so we put elements of $\mathfrak{g}$ into some equivalence classes determined by the ideal. How is this ""enveloping"" the $\mathfrak{g}$ ? What is the intuition? (Maybe there is no interesting interpretation of this and I am just being too curious, but had to try it). Remark 1 : I am refering to the enveloping algebra of $\mathfrak{g}$ , which is sometimes called universal enveloping algebra . It is the same. Remark 2 : I am using the definition from Dixmier:","['lie-algebras', 'envelope', 'tensors', 'abstract-algebra', 'lie-groups']"
4245143,Solution to first order ODE representing simple physics system,"Can you lead me to the solution to the following problem? $$\left\{
\begin{array}{>{\displaystyle\tallstrut}l@{}}
y'(t)+ay(t)=f(t)\\
f(t)=\delta (t-t_0)\\
y(0)=y_0
\end{array}
\right.$$ Following the standard rules for first order ordinary differential equations $$y(t)=y_0e^{-at}+e^{-at}\int^t_0f(s)e^{as}ds=y_0e^{-at}+F(t)\\
F(t)=\int^t_0f(s)e^{-a(t-s)}ds\\
$$ The book suggests the solution $$F(t)=
\left\{
\begin{array}{>{\displaystyle\tallstrut}l@{}}
0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ if\ \ t<t_0\\
e^{-a(t-t_0)}\ \ \ \ \ if\ \ t<t_0\\
\end{array}
\right. $$ but I can't understand why, as integrating myself I obtain $$F(t)=\frac{\delta}{a^2}[e^{-a t_0} (1 + a t_0)-e^{-a t} (1 + a t)]$$ .","['integration', 'solution-verification', 'ordinary-differential-equations']"
4245173,Checking the Folner condition only on a set of generators,"Let $G$ be a countable, discrete group with a set of generators $S$ . Let $F_n\subset G$ be a sequence of finite sets that satisfies $$\frac{|sF_n\triangle F_n|}{|F_n|}\to0$$ for all $s\in S$ . Is it true then that $F_n$ is a Folner sequence? In other words, is it true that $$\frac{|gF_n\triangle F_n|}{|F_n|}\to0$$ for all $g\in G$ ? In particular, does it suffice to check the Folner condition on the generators instead of the entire group? Let's set $\alpha_n(g)=\frac{|gF_n\triangle F_n|}{|F_n|}$ .
I believe that this question boils down to showing that (1) if $\alpha_n(g)\to0$ then $\alpha_n(g^{-1})\to0$ and that (2) if $\alpha_n(g)\to0$ and $\alpha_n(h)\to0$ then $\alpha_n(gh)\to0$ . For (2) I observed that $$\alpha_n(gh)=\frac{|ghF_n\triangle F_n|}{|F_n|}=\frac{|(ghF_n\triangle hF_n)\triangle(hF_n\triangle F_n)|}{|F_n|}\leq\frac{|ghF_n\triangle hF_n|}{|F_n|}+\alpha_n(h)=$$ $$=\frac{|g(hF_n)\triangle hF_n|}{|hF_n|}+\alpha_n(h)$$ and I would be done if I could get some estimate of the form $|ghF_n\triangle hF_n|\leq C|gF_n\triangle F_n|$ , but this seems impossible to me. Such an estimate would also be enough to show (1). Any hint or reference is appreciated.","['amenability', 'group-theory', 'functional-analysis']"
4245190,"If $\int_\Omega f d\mu < \infty$ and $f$ is non-negative, then $\mu(\{\omega\in\Omega:f(\omega)=\infty\})=0$","So I started reading about measure theory, and encountered the following question: Suppose $\int_\Omega fd\mu<\infty, f$ measurable and non-negative. Prove: $\mu\left(\{\omega\in\Omega:f(\omega)=\infty\}\right)=0$ . I assumed for a contradiction that $\mu(\{\omega\in\Omega:f(\omega)=\infty\})=c>0$ . I defined a sequence of functions $f_n$ s.t: $f_n(\omega)=
\begin{cases} 
     f(\omega) & f(\omega)\neq\infty\\
      n & f(\omega)=\infty 
   \end{cases}
\
$ Obviously, $f_1\leq f_2\leq...$ and $f_n\rightarrow f$ pointwise. Now I define $h_n=n\chi_{\{\omega|f(\omega)=\infty\}}$ . So $h_n$ is a simple function s.t $h_n\leq f_n$ and therefore $\int_\Omega f_nd\mu\geq\int_\Omega h_nd\mu=nc$ . So by monotone convergence we have: $\int_\Omega fd\mu=\lim\int_\Omega f_nd\mu\geq\lim nc=\infty$ - a contradiction. Only thing I wasn't able to show is that each $f_n$ is measurable.
Is my proof correct or am I missing something?","['measure-theory', 'solution-verification', 'measurable-functions']"
4245232,How does graph Fourier transform retain structural information?,"Context: I am reading about graph Laplacian matrices $L = D - A$ and how their eigenvectors correspond to Fourier modes. From spectral decomposition, $ L = U \Lambda U^{T}$ , where $\Lambda$ is a diagonal matrix of the eigenvalues and $U$ is a matrix of the eigenvectors. Then we can compute the Fourier transform of a signal $\mathbf{f}$ by doing the following calculation: $\mathbf{s} = \mathbf{U}^{T} \mathbf{f}$ . Reference: pg. 83 of the following notes here Question: How does the graph Fourier transform retain structural information about the graph? Do the eigenvectors/eigenvector matrix change for different Laplacian matrices? It sounds silly, but the Fourier modes seem like they would be the same across different Laplacian matrices. My understanding was that they were similar to terms within the DFT matrix. If, in fact, the eigenvectors of the Laplacian did change between different graphs, then I suppose those eigenvectors would retain structural information about the graph. Any help would be greatly appreciated.","['matrices', 'graph-theory', 'fourier-transform', 'spectral-graph-theory']"
4245312,Algebraically independent Plücker relations,"I'm trying to find algebraically independent Plücker relations for $Gr(2,\mathbb{C}^5)$ which generate the ideal. How do I find them and how do we prove if a Plücker relation is algebraically independent?","['grassmannian', 'homogeneous-spaces', 'algebraic-geometry', 'representation-theory']"
4245356,Show for all n ∈ IN that the inequality $(n!)^2<2^{n^{2}}$ holds (by induction),"Show by induction that $$(n!)^2<2^{n^2} \quad \forall n \in \mathbb{N} $$ $n=1 \leftrightarrow  1<2$ $(k!)^2<2^{k^2}$ $((k+1)(k!))^2 < 2^{(k+1)^2}$ and $(k^2+2k+1)(k!)^2<2k^2 \cdot 2^{2k+1}$ , but our assumption told us that, $(k!)^2<2^{k^2}$ , therefore I rewrote the inequality to $(k+1)^2<2^{2k+1}$ . However, this got me nowhere, I kept doing this for four different inequalities down the line without getting to a satisfying answer. May I get some help? Please.","['inequality', 'induction']"
4245393,Inverse Laplace transform of $\ln(s^2 +1)$ using countour integration,"I am trying to find the inverse Laplace transform of $\ln\left(s^{2}+1\right)$ using contour integration, Which could be found by: $$ \frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty}\ln\left(s^{2}+1\right)e^{st}ds $$ As the function has two branch points, at $s = \pm i$ , I considered the contour: Whereat the end I get that the integral of the arc vanishes, and the integrals over the negative real part of the contour cancel each other out. I'm just left with the integrals over the branch points, which I'm going to call $ \alpha_{1} $ and $ \alpha_{2} $ . For $ \alpha_{1} $ I get $ \int_{0}^{i}\left[\ln\left(s^{2}+1\right)+\frac{5i\pi}{2}\right]e^{st}ds + \int_{i}^{0}\left[\ln\left(s^{2}+1\right)+\frac{i\pi}{2}\right]e^{st}ds $ which is equal to $ 2i\pi\int_{0}^{i}e^{st}ds $ For $ \alpha_{2} $ I get $ \int_{0}^{-i}\left[\ln\left(s^{2}+1\right)-\frac{i\pi}{2}\right]e^{st}ds + \int_{-i}^{0}\left[\ln\left(s^{2}+1\right)+\frac{3i\pi}{2}\right]e^{st}ds $ which is equal to $ -2i\pi\int_{0}^{-i}e^{st}ds $ In my calculations, the semi-circle integrals over +i and -i are 0, so at the end my final answer would be $ \int_{\alpha_{1}}^{ }+\int_{\alpha_{2}}^{ } $ , and I get $ -\frac{2i\pi}{t}\left(e^{it}-e^{-it}\right) $ . As the final answer for the inverse Laplace I get $ \frac{-2i}{t}\sin\left(t\right) $ , which is not the correct answer. My question is what am I missing in the countour? Maybe I set up the arguments of $ \ln\left(s^{2}+1\right) $ wrong, or maybe the integrals over the real part does not cancel out, any help or hint would be welcome, thank you! The correct answer is $ \frac{2\cos\left(t\right)}{t} $","['complex-analysis', 'contour-integration', 'laplace-transform']"
4245402,Limit of functions measurable with respect to sub-sigma algebra [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $\{f_{\alpha}\}_{\alpha}$ be a net of functions in a measure space $(X,\mathcal F,\mu)$ . Suppose that each $f_{\alpha}$ is measurable with respect to a sub $\sigma$ -algebra $\mathcal G$ , and that the net has a limit, $f$ , in the weak-* topology of $L^{\infty}(X,\mathcal F,\mu)$ : that is, there is a $\mathcal F$ -measuable function $f$ so that $$\int f_{\alpha} \phi d\mu \to  \int f \phi d\mu$$ for every $\phi\in L^1(X,\mathcal F,\mu)$ ). Is $f$ $\mathcal G$ -measurable?","['measure-theory', 'measurable-functions', 'weak-convergence']"
4245405,How to solve the integral through a contour with branch points?,"Consider the integral $$ \int^{\infty}_{0} \frac{x^{\frac{1}{3}}}{1+x^2}dx $$ on the complex plane $$ \oint_{C} \frac{z^{\frac{1}{3}}}{1+z^2}dz $$ To find the poles $ 1+z^2=0 \Rightarrow z^2=-1 \Rightarrow z= \pm \sqrt{-1} \Rightarrow z= \pm i $ , by the residue theorems $$ \oint_{C} \frac{z^{\frac{1}{3}}}{1+z^2}dz = 2 \pi i \,\ Res f(z) $$ I know that $$  \oint_{C} \frac{z^{\frac{1}{3}}}{1+z^2}dz = \oint_{C_R} \frac{z^{\frac{1}{3}}}{1+z^2}dz  +\oint_{C_2} \frac{z^{\frac{1}{3}}}{1+z^2}dz + \oint_{C_r} \frac{z^{\frac{1}{3}}}{1+z^2}dz + \oint_{C_1} \frac{z^{\frac{1}{3}}}{1+z^2}dz  $$ the contour used is below: The integral in $C_R \rightarrow 0$ because $R \rightarrow \infty$ and the integral in $C_r \rightarrow 0$ because $r \rightarrow 0$ . But how can I calculate the integrals over $C_1$ and $C_2$ ? How can I calculate residuals? The teacher said the result is $$ I(1-e^{\frac{2 \pi i}{3}}) = 2 \pi i \Big[ \frac{e^{\frac{\pi i}{6}}}{2i}\Big] (1-e^{\frac{\pi i}{3}}) \quad \quad \Rightarrow  \quad \quad  I= \frac{\pi}{2 \sin(\frac{\pi}{3})} = \frac{\pi}{\sqrt{3}}$$","['complex-analysis', 'residue-calculus', 'complex-numbers']"
4245413,Proving a map is injective iff it has a left inverse,"Let $f:A\to B$ Prove the map $f$ is injective iff it has a left inverse. Starting with $f$ is injective Let $A$ not be $ \varnothing$ $f(A) = \{b \in B \ | \ b = f(a) \text{ for some } a \in A\}$ If there is a map $g: B \to A$ defined by $g(B) = \{a \in A \ | \ a = g(b)\text{ for some } b \in B\}$ where $b = f(a)$ The composite function $g \circ f(a): a \to f(a) \to a$ returns the identity on $A$ and is thus a left inverse. $g$ does not exist in this definition if $g(b_i) \neq a_i$ for some $i$ denoting an element of $A,B$ , which makes it the nullset Or $g(b_i) = a_i = a_{i+x}$ two or more elements of $A$ share an element of $B$ in the mapping of $f: A \to B$ It cannot be true because $a_i \neq a_{i+x}$ implies $f(a_i) \neq f(a_{i+x})$","['elementary-set-theory', 'functions', 'solution-verification']"
4245455,Existence of a limit or misspelled.,"If someone proposes the problem: Calculate the limit: $\lim_{x\rightarrow 2}\frac{x-2}{2-\sqrt{4}}$ For me the limit does not exist because in fact the function $\frac{x-2}{2-\sqrt{4}}$ does not exist. However, it is also true that the problem is misspelled. But my question is that if they already pose the problem to you like this, what is the correct thing to say, you cannot write such a limit (misspelled) or it does not exist.",['limits']
4245474,Measure given by $\nu(E)=\int_E gd\mu$. Want to prove $\int_\Omega fd\nu=\int_\Omega fgd\mu$,"Define a measure $\nu$ given by $\nu(E)=\int_E gd\mu$ for all $E\subseteq\Omega$ measurable and for some measurable non-negative $g$ . Prove $\int_\Omega fd\nu=\int_\Omega fgd\mu$ . First step I proved this for simple functions, which was pretty straight-forward. Also, proving $\int_\Omega fgd\mu\geq\int_\Omega fd\nu$ for any measurable non-negative $f$ was easy too. It's the other inequality I'm having trouble with. Following definition, $\int_\Omega fgd\mu=\operatorname{Sup}\{\int_\Omega hd\nu:0\leq h\leq fg, \text{h simple}\}$ . Now, if I could say that $fg\geq h$ implies $f\geq \frac{h}{g}$ then solving it would be easy, but I can because it can be undefined. So I'm stuck. Any hint would be helpful.","['measure-theory', 'lebesgue-measure', 'borel-measures', 'lebesgue-integral', 'real-analysis']"
4245502,"When is a g-dimensional subspace of $H^1(S; \mathbb{C})$ the $H^{1,0}$ of a complex structure on $S$?","Given a closed surface $S$ of genus $g \geq 1$ there are lots of choices of complex structure, and each one singles out a subspace of the surface's first cohomology group with complex coefficients corresponding to the classes of 1-forms which are holomorphic on that Riemann surface.  Given an ""appropriate"" subspace, when does it correspond to the $H^{1,0}$ for some choice of complex structure? To be more precise, let $\mathcal{T}(S)$ be the Teichmüller space of $S$ , whose elements are pairs equivalence classes of pairs $(X, f)$ where $X$ is a Riemann surface and $f : S \to X$ is a diffeomorphism, with $(X, f) \sim (Y, g)$ if there is a holomorphic $\Phi : X \to Y$ such that $g^{-1}\Phi f$ is isotopic to $\mathrm{id}_S$ .  Using the marking $f$ , we can pull $H^{1,0}(X)$ back to a complex $g$ -dimensional subspace of $H^1(S; \mathbb{C})$ .  It is not the case that every $g$ -dimensional $V \subseteq H^1(S; \mathbb{C})$ corresponds to some $H^{1,0}(X)$ for some choice of complex structure on $S$ .  For example, the theorems of Haupt and Kapovich (independently, see https://www.math.ucdavis.edu/~kapovich/EPR/fla2019.pdf ) state that a character $\chi : H_1(S; \mathbb{Z}) \to \mathbb{C}$ will correspond to a holomorphic 1-form on some choice of complex structure if two conditions are satisfied. If I had a $g$ -dimensional $V \subseteq H^1(S; \mathbb{C})$ where each element satisfied the Haupt-Kapovich conditions, is it true this subspace corresponds to some $H^{1,0}(X)$ ?  The issue that comes to mind is if $\chi, \kappa \in V$ they correspond to holomorphic 1-forms on Riemann surfaces, but is there a reason they should be holomorphic 1-forms on the same Riemann surface?","['complex-analysis', 'complex-geometry', 'teichmueller-theory', 'riemann-surfaces']"
4245503,Differential of transition map.,"I'm a beginner in the calculation of differentials in smooth manifolds. I was thoughtful about the case of the transition map differential. For example, in the complex projective line $\mathbb C P_1$ , the transition map $\phi_1 \circ \phi_2^{-1}$ is given by $\frac{1}{z}$ . To calculate the differential of this map at an arbitrary point $x$ just perform the conventional derivative calculation on $\mathbb C$ ? That is, $d_x (\phi_1 \circ \phi_2^{-1}) = \frac{-1}{x^2}$ , it seems to me that this makes sense, since $\phi_1 \circ \phi_2^ {-1}$ is a map of $\mathbb C$ to $\mathbb C$ , and tangent spaces can be identified with $\mathbb C$ . Is my reasoning correct?","['derivatives', 'smooth-manifolds', 'differential-geometry']"
4245515,Why does $\frac{\sin x}{x(x^2-4)}$ not have a vertical asymptote at $ x = 0 $,I suspect it has something to do with $$\lim_{x \to 0} \frac{\sin x}{x} = 1 $$ but I'm not too sure as I'm still using the precalc-learned rule where you find vertical asymptotes by finding $x$ values that makes the denominator of a function equal to $0$ .,['algebra-precalculus']
4245523,continuous extension of holomorphic function up to the boundary,"Denote $S=\{z \in \mathbb{C}|0 \leq \text{Re}(z) \leq 1\}$ , let $X$ be a Banach space and let $f:S^\circ \rightarrow X$ be a holomorphic function. Under what assumptions does $f$ have a unique continuous extension $\tilde{f}:S \rightarrow X$ ? (Here $S^\circ$ denotes the set $\{z \in \mathbb{C}|0 < \text{Re}(z) < 1\}$ ). The reason I ask this question : Using the Weierstrass M-test I saw that a series of the form $\sum_{n=1}^{+\infty}f_n(z)$ converges absolutely and uniformly on $\boldsymbol{S^\circ}$ (where $f_n:S \rightarrow X$ are holomorphic in $S^\circ$ and continuous in $S$ for every $n \in \mathbb{N}$ ) so that I was able to define a function $f:S^{\circ} \rightarrow X$ by the formula $f(z)=\sum_{n=1}^{+\infty}f_n(z)$ (where the convergence happens with respect to the norm of $X$ ). I would like to extend $f$ continuously in $S$ (in a unique way). Is it possible?","['complex-analysis', 'continuity', 'functional-analysis']"
4245538,How to construct the $C_{4v}$ character table given its Cayley table,"I have the following 'homework style' question: Complete the construction of the $C_{4v}$ character table started in the class. Just to be clear, this question is about all the symmetry operations regarding a square-based pyramid (or if you prefer molecules then please see this instead): I have essentially 2 questions regarding the solution given by the author, which in part, states that: The group $C_{4v}$ has 8 elements arranged in 5 conjugate classes: $E\,, \color{red}{2C_4}\,, C_2\,, \color{red}{2\sigma_v}\,, \color{red}{2\sigma_d}$ .
The number of classes equals the number of irreps, so the group has 5 inequivalent irreps. First we determine the dimensions of these irreps. We know that $$\sum_i n_i^2=m\tag{1}$$ where $n_i$ is the dimension of the ith irrep. For $C_{4v}$ , we have $m=8$ , so we are looking for five positive integers whose squares sum to 8, and after some thought we realise that $$8=1 + 1 + 1 + 1 + 4 = 1^2+1^2+1^2+1^2+2^2\tag{2}$$ so the group has 4 one-dimensional irreps and 1 two-dimensional irrep. $\color{red}{\text{This allows us to complete the ﬁrst column of the character table}}$ , corresponding to the characters for the identity element since these are equal to the dimensions of the irrep.
For any group, there is also the trivial irrep, where $\color{red}{\chi(C) = 1}$ for all classes. This is usually the ﬁrst row of the character table. So far we have: $$
\begin{array}{c|c|c|c}
 C_{4v} & E
& 2C_4 & C_2 & 2\sigma_v & 2\sigma_d \\\hline
 \chi^{(1)} & 1 & 1 & 1 & 1 & 1 \\\hline
 \chi^{(2)} & 1  \\\hline
 \chi^{(3)} & 1   \\\hline
 \chi^{(4)} & 1   \\\hline
 \chi^{(5)} & \color{red}{2}
\end{array}
$$ I have marked in red the parts of the authors' solution for which I do not understand. I first need to understand why the classes are $E\,, \color{red}{2C_4}\,, C_2\,, \color{red}{2\sigma_v}\,, \color{red}{2\sigma_d}$ , so starting from the Cayley table for $C_{4v}$ : The first problem is that the matrices as I have seen them on page 6 of this are, $$C_4=\begin{bmatrix}
\cos 2\pi/4 & -\sin 2\pi/4 & 0 \\
\sin 2\pi/4 & \cos 2\pi/4 & 0 \\
0 & 0 & 1
\end{bmatrix}=\begin{bmatrix}
0 & -1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix}$$ $$C_4^3=\begin{bmatrix}
\cos 3\pi/2 & -\sin 3\pi/2 & 0 \\
\sin 3\pi/2 & \cos 3\pi/2 & 0 \\
0 & 0 & 1
\end{bmatrix}=\begin{bmatrix}
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix}$$ $$C_2=\begin{bmatrix}
\cos \pi & -\sin \pi & 0 \\
\sin \pi & \cos \pi & 0 \\
0 & 0 & 1
\end{bmatrix}=\begin{bmatrix}
-1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & 1
\end{bmatrix}$$ $$\sigma_x=\begin{bmatrix}
-1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}$$ $$\sigma_y=\begin{bmatrix}
1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & 1
\end{bmatrix}$$ $$\sigma_d=\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix}$$ $$\sigma_{d^\prime}=\begin{bmatrix}
0 & -1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix}$$ and $$E=\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}\tag{!}$$ But, what I understood is that only the last IRREP, is two-dimensional, but in order for the group symmetry elements to be compatible, they must have the same dimension. Is there a problem here? Using what I learnt in my previous question , I'm going to compute the red conjugacy classes for $C_{4v}$ and see if they match up with $E\,, \color{red}{2C_4}\,, C_2\,, \color{red}{2\sigma_v}\,, \color{red}{2\sigma_d}$ . But to make things easier I first note that: $$
\begin{array}{c|c|c|c}
 C_{4v} & E
& C_4 & C_2 & C_4^3 & \sigma_x & \sigma_y & \sigma_d & \sigma_{d^\prime} \\\hline
 \mathrm{Order} & 1 & 4 & 2 & 4 & 2 & 2 & 2 & 2 \\\hline
 \mathrm{Inverse} & E & C_4^3 & C_2 & C_4 & \sigma_x & \sigma_y & \sigma_d & \sigma_{d^\prime}  \\\hline
\end{array}
$$ Now the class of $C_4$ is: $$C_2C_4{C_2}^{-1}=C_2C_4C_2=C_2C_4^3=C_4$$ $$\sigma_d C_4 \sigma_d^{-1}=\sigma_d C_4 \sigma_d=\sigma_d\sigma_x=C_4^3$$ I won't include the other 6 elements as it is tedious to write them all (which I have done on paper), and they are all either $C_4$ or $C_4^3$ . So the class of $C_4$ is $\{C_4, C_4^3\}$ . The class of $C_4^3$ is: $$C_2C_4^3{C_2}^{-1}=C_2C_4^3C_2=C_2C_4=C_4^3$$ $$\sigma_yC_4^3{\sigma_y}^{-1}=\sigma_yC_4^3\sigma_y=\sigma_y\sigma_{d^\prime}=C_4$$ again excluding the other 6 elements the class of $C_4^3$ is $\{C_4^3, C_4\}$ . So the conclusion is that the elements $C_4$ and $C_4^3$ belong to the same class, namely, $\{C_4, C_4^3\}$ . I could carry out the same calculation for the other two classes I marked red ( $\color{red}{2\sigma_v}\,, \color{red}{2\sigma_d}$ ), I'd like to avoid typing this out but what I find is that the class of $\sigma_x$ (or ${\sigma_y}$ ) is $\{\sigma_x, \sigma_y\}$ and the class of $\sigma_d$ (or $\sigma_{d^\prime}$ ) is $\{\sigma_d, \sigma_{d^\prime}\}$ . Now here is the problem: Why is the character table not being written like this: $$
\begin{array}{c|c|c|c}
 C_{4v} & E
& \{C_4, C_4^3\} & C_2 & \{\sigma_x, \sigma_y\} & \{\sigma_d, \sigma_{d^\prime}\} \\\hline
 \chi^{(1)} & 1 & 1 & 1 & 1 & 1 \\\hline
 \chi^{(2)} & 1  \\\hline
 \chi^{(3)} & 1   \\\hline
 \chi^{(4)} & 1   \\\hline
 \chi^{(5)} & \color{red}{2}
\end{array}
\tag{?}$$ Put simply, I don't understand how this $\{C_4, C_4^3\}$ class is the same as writing the class as "" $2C_4$ ""?  The same question also applies to "" $2\sigma_v$ "" and "" $2\sigma_d$ ""; is this some kind of special notation? I can only presume that the factor of 2 in front of these classes comes into it each time as there are 2 elements in these classes? Moving onto the second question I have, which, namely, is how the first row and column of the character table were obtained. I know a character is the sum of its diagonal elements of one of the matrices of a representation $D$ , $$\chi(g)=\sum_i D_{ii}(g)=\mathrm{Tr}\Big(\underline{\underline{D}}(g)\Big)\tag{3}$$ for some group element $g$ . I also know that the following orthogonality condition holds for characters: $$\sum_g{\chi^{(j)}}^*(g)\chi^{(i)}(g)=m\delta_{ij}\tag{4}$$ where $m$ is the order of the group.
But, what is the link between equations $(1)$ and $(2)$ and characters, $\chi(g)$ ? Put another way, the author so readily writes "" $\color{red}{\text{This allows us to complete the ﬁrst column of the character table}}$ "", but how does this relate to characters $\chi(g)$ , as given in the two formulae I stated, $\big((3)$ and $(4)\big)$ ? Update: This is an update in response to this comment: Look at the last row and column of those matrices. They can all be written as a sum of a $2×2$ matrix and a $1×1$ matrix. The $3$ -dimensional representation is the sum of the trivial and the $2$ -dimensional This has lead me to a new point of confusion: In one of the earlier quotes from my notes, I wrote that For any group, there is also the trivial irrep, where $\color{red}{\chi(C) = 1}\,$ $\color{red}{\text{for ALL classes}}$ . This is usually the ﬁrst row of the character table. The trace of all of the matrices (which are listed below the $C_{4v}$ Cayley table) for the group elements of $C_{4v}$ all give $1$ (Except $E$ where the trace is $3$ ). So by my logic the (partial) character table for $C_{4v}$ should look like $$
\begin{array}{c|c|c|c}
 C_{4v} & E
& \{C_4, C_4^3\} & C_2 & \{\sigma_x, \sigma_y\} & \{\sigma_d, \sigma_{d^\prime}\} \\\hline
 \chi^{(1)} & 1 & 1 & 1 & 1 & 1 \\\hline
 \chi^{(2)} & 1  \\\hline
 \chi^{(3)} & 1   \\\hline
 \chi^{(4)} & 1   \\\hline
 \chi^{(5)} & \color{blue}{3}
\end{array}
\tag{$\color{blue}{?}$}$$ But according to the comment quoted the above (partial) character table cannot be correct.
So looking at say, the $\{C_4, C_4^3\}$ class from the character table above, this class in its $3\times 3$ form does indeed have a trace of 1. However, since these matrices are in block-diagonal form they are also reducible. For example, $$C_4=\begin{bmatrix}
0 & -1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix}$$ can be reduced to IRREPs $\begin{bmatrix}
0 & -1  \\
1 & 0  \\
\end{bmatrix}$ and the trivial IRREP $1$ . I did this by considering the ' $z$ -component' of the original $3\times 3$ matrix then the remaining $x,y$ component block. My question is: Are the $3\times 3$ matrices I gave below the Cayley table for point group $C_{4v}$ of the correct dimensions for the group elements $\big(E,\,C_4,\, C_4^3,\, C_2, \,\sigma_x,\, \sigma_y,\, \sigma_d,\, \sigma_d^{\prime}\big)$ ? If these are the correct matrices then why must we reduce these matrices into irreducible forms and then calculate the trace, $\chi^{(i)}$ ? But more importantly, when we reduce the identity group element, $E$ , to it's sum of IRREPS: $E=\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}$ to $\begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix}$ and the trivial IRREP, $1$ . Why do we then discard the trivial IRREP $1$ and put a trace of $2$ in the character table? The reason I'm asking these questions is that it just confused me a lot when I saw that the trace of the $3\times 3$ group element matrices gave the same trace as the IRREPS (with the exception of $E$ ). Edit: Before the chat was cleared I was given some useful information from @David A. Craven who mentioned that: The blue $3$ shouldn't be in your partial character table. Your 3-dimensional rep is not irreducible, so has no place there. The $3$ -dimensional is the sum of the trivial and the 2-dimensional, and by looking at those matrices you can fill in the fifth row. It should be $2,\, 0,\, -2,\, 0, \,0$ . Notice that the sum of this and the first row gives you the traces of your $3\times 3$ matrices. & A representation is irreducible if there is no subspace of the vector space that is stabilized by your group, or equivalently, there is no basis with respect to which the matrices of the group action can be written as block-diagonal matrices (this equivalence is only valid over fields of characteristic $0$ ). Your matrices are already in block-diagonal form, therefore the representation is not irreducible. From the first quote, I notice that the sum of the first and fifth rows are $3,\,1,\,-1,\,1,\,1$ and these indeed are the traces of the $3 \times 3$ matrices belonging to classes $\{E\}$ , $\{C_4,\, C_4^3\}$ , $\{C_2\}$ , $\{\sigma_x,\,\sigma_y\}$ , and $\{\sigma_d,\,\sigma_{d^\prime}\}$ respectively. The last row of the table as mentioned in the quote above is $2,\, 0,\, -2,\, 0, \,0$ , and I notice that these traces were obtained by summing together the first 2 diagonal elements of the classes $\{E\}$ , $\{C_4,\, C_4^3\}$ , $\{C_2\}$ , $\{\sigma_x,\,\sigma_y\}$ , and $\{\sigma_d,\,\sigma_{d^\prime}\}$ respectively. This is now starting to make sense to me as I see that the last row of the character table corresponds to the only 2-dimensional IRREP. So from this, I now understand why the 3 (which I marked blue) must not appear in the character table. I also understand the meaning of the beginning of the second sentence in the first quote above by David: ""Your 3-dimensional rep is not irreducible, so has no place there."" - I understand this because the matrix for $E$ is in block-diagonal form, and as I understand it, a block-diagonal matrix is reducible. Now here is the only part I cannot understand, in the last sentence of the second quote by David where he mentions that ""Your matrices are already in block-diagonal form, therefore the representation is not irreducible."" But, it was my understanding that block-diagonal matrices only have entries on the leading diagonal (such as the $3\times 3$ matrix for identity, $E$ ) and zeroes elsewhere. But some of those matrices given below the $C_{4v}$ Cayley table have off-diagonal (non-zero) components (such as in the matrices for $C_4$ and $\sigma_d$ , etc.). So, in short, are block-diagonal matrices reducible or irreducible?","['group-theory', 'solution-verification', 'representation-theory', 'characters']"
4245596,Quaternionic Hard Lefschetz Theorem,"For a compact Kahler manifolds $M$ of real dimension $2n$ , the Hard Lefschetz theorem states that there is an isomorphism for each $0\leq i\leq n$ , $$ d^{n-i}\cup-\colon H^i(M;\mathbb{Q})\rightarrow H^{2n-i}(M;\mathbb{Q}),$$ where $H^i(-;\mathbb{Q})$ denotes the cohomology group with coefficients in rationals and $d\in H^2(M;\mathbb{Q})$ is the Kahler class. My question is that is there an analog of the Hard Lefschetz theorem for quaternionic Kahler manifolds? If the answer is Yes, please send me some references, thanks ahead. I guess the answer is yes and a possible statement may be as follows. Let $M$ be a quaternionic Kahler manifold of real dimension $4n$ , then there is a quaternionic Kahler class $d\in H^4(M;\mathbb{Q})$ and an isomorphism for each $0\leq i\leq n$ : $$d^{n-i}\cup-\colon H^i(M;\mathbb{Q})\rightarrow H^{4n-3i}(M;\mathbb{Q}).$$","['complex-geometry', 'kahler-manifolds', 'differential-geometry']"
4245599,$P(X_1 > X_2 + X_3 + 2)$ for a multivariate normal distribution,"I'm doing the next exercise by following the hints of the book but I don't get the final answer: Problem: Let $X=(X_1, X_2, X_3)$ have a multivariate normal distribution with mean vector 0 and variance-covariance matrix: ∑ = $ \begin{pmatrix} 1 & 0 & 0\\ 0 & 2 & 1\\  0 & 1 & 2 \end{pmatrix} $ Find: $P(X_1 > X_2 + X_3 + 2)$ My attempt: Let $Z = X_1 - X_2 - X_3$ . Note that $Z=p*X$ where $p= \begin{pmatrix} 1 & -1 & -1 \end{pmatrix}$ and $x =  \begin{pmatrix} X_1\\ X_2\\X_3 \end{pmatrix} $ Now I do the calculations: $E(Z)=p*E(X)=0$ , given p vector of constants and mean 0 by hypothesis. $Var(Z)=p*Var(X)*p=7$ , with one of the p's like row vector and the other one like column vector. The initial probability can be rewritten like: $P(X_1-X_2-X_3>2)=P(Z>2)$ . Someone told me a property that is like: $P\left(\frac{Z-mean}{\sqrt{var(Z)}}\right)>\frac{2- mean}{\sqrt{var(Z)}}$ but I'm not sure. The book gives a Hint that is: Find a vector a so that $aX= X_1 - X_2 - X_3$ and use the theorem: theorem 1 , but I'm not sure how to use it. Thanks in advance for the help. PD: sorry for not use very well HTML but I'm learning.","['statistics', 'probability-distributions', 'normal-distribution', 'probability-theory', 'probability']"
4245621,Evaluating $\lim_{x \to 0} \frac{1-\cos(x^2+2x)}{1-\cos(x^2+x)}$ without L'Hopital,"Could you help me with this limit please: $$
\lim_{x \to 0}  \frac{1-\cos(x^2+2x)}{1-\cos(x^2+x)}
$$ I know that with L'Hopital is easy but I want to do it without that theorem. I have already tried converting it  to: $$
 \frac{2\sin^2(x^2/2+x)}{2\sin^2(x^2/2+x/2)}
$$ or expanding the sin of the sum of two angles: $$
 \frac{\sin(x^2/2)\cos(x)+\sin(x)\cos(x^2/2)}{\sin(x^2/2)\cos(x/2)+\sin(x/2)\cos(x^2/2)}
$$ but I can not advance further than that and eliminate the indetermination 0/0. I would really appreciate if you could help me please.","['limits', 'calculus', 'limits-without-lhopital', 'trigonometry']"
4245638,Higher Level Mathematics & Piecewise [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question Apologies if this isn't the right place to enquire about this; I'm really looking for potential connections to what I've been doing as opposed to having a maths question answered. With that being said, I would heavily appreciate tips and advice as to potential techniques as well as connections. Background: I'm currently a first year university student (about three-quarters of the way through first year) so I'm not well-versed in higher topics in mathematics. Currently doing multivariable calculus and differential equations, and have had a rudimentary introduction to linear algebra (and plans to do real analysis and linear algebra next year). With that being said, the whole point of me asking here is to get answers as to where I can go in relation to some of the stuff I've been doing. I have a bit of an unnatural obsession with piecewise objects in general, and I've been told that some of the stuff I do has the potential to dive into various areas like topology, etc. So I run a site plainly called Piecewise where I really sort of explore and post some of the stuff to do with piecewise. More or less - I'm looking for places to go with some of the stuff I have been (and have not been) doing with piecewise objects. Even more specifically, I base a lot of the stuff I do on the foundation that the cases within piecewise objects aren't independent of one another; i.e. operations you perform on one case you perform on others in order to relate each case to one another. Current work : The above idea has yielded some interesting derivations, including the ability to interpolate a set of points as polynomials (with quite-flexible ideas), which was entirely accidental and had been a goal, that is, turning a set of points into a polynomial without solving a system of equations. That was one of my goals a couple of years ago during VCE which ended up going nowhere till more recently, when I was told that what I had derived appeared to be a lagrange-form polynomial; a polynomial derived as a consequence of lagrange interpolation. Later, I also rederived interpolation as newton-form polynomials using a similar method. Other things I've been able to do have largely been geometric and graphical in nature; deriving a continuous batman equation/relation from the original, equations of hollow hypercubes in n-dimensions, a formula for the sticking together of several functions along a given axis (I keep hearing things about the gluing lemma in this regard?) which can be extended into higher dimensions on surfaces, but takes on a far more ugly and restrictive formula. Obviously a lot of these things are fairly rudimentary in their approach and nature. Problem/Question : Naturally, I'm absolutely curious about where I can go with these ideas and potential connections to higher-level mathematics, not necessarily geometric in nature (although it seems to be a recurring theme). It furthermore seems to be an issue that with my peers, the line of thinking I'm on takes time to understand, so I have found it difficult to communicate my ideas a lot of the time - but have had occasional assistance. Specifically , what areas of maths might be the most relevant to what I'm doing, and, what potential subtopics could have some potential relevance to these piecewise objects, if any?
Additionally, from what I can see, no one else has really approached piecewise in this way before that I can find, which has led my friends and I to a few hypotheses: Exploring piecewise in this way is largely redundant and ultimately leads to very little that generalises to higher mathematics. It has been explored, and all exploration that has been done has ultimately been integrated into other things (wherein perhaps piecewise was used as a scaffolding that was later removed). It genuinely hasn't been explored before (and imho the least likely option). Naturally, I am also curious as to (1) whether I'm expending my time that on something will yield little or is 'useless', and (2) did I miss something? Has this been done before? Thanks in advance - and apologies if this isn't specific enough (also had no idea what to tag this post with).","['advice', 'geometry', 'functions', 'general-topology', 'soft-question']"
4245660,How to prove that these two sets are equal? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I think I can prove that $$\bigcup_{i=1}^{n}A_i\setminus\bigcap_{i=1}^{n} A_i
=(A_1\setminus A_2)\cup(A_2\setminus A_3)\cup\cdots\cup(A_{n-1}\setminus A_n)\cup(A_n\setminus A_1).$$ so,I think maybe there has(treat infinity as a circle): $$\bigcup_{n=1}^{\infty}A_n\setminus\bigcap_{n=1}^{\infty} A_n=(A_1\setminus A_2)\cup(A_2\setminus A_3)\cup\cdots$$ but I dot't know how to prove it.",['elementary-set-theory']
4245680,Probability of more than ${3\over 4}N$ heads in $N$ flips of a coin?,What is the probability of getting more than $ \frac { 3 N } 4 $ heads in $ N $ flips of coins? I know we need to use binomial distribution formula for this and sum it from $ N = \frac { 3 N } 4 $ to $ N $ . I can solve this when numbers are given but I'm struggling to solve because a general case is given. Any help appreciated.,"['distribution-tails', 'binomial-distribution', 'probability']"
4245685,Full House in Poker -- Why do we treat all possible hands to be equally likely?,"The problem asks us to find the probability of a full house in a well-shuffled deck of $52$ cards. The solution in the textbook states in the first line ""All of the $52\choose 5$ possible hands are equally likely by symmetry, so the naive definition [of Probability] is applicable."" Everything after this involves counting, which I understand. However, I do not see why all outcomes are equally likely. Here is why. Say we chose a card with rank $7$ . The number of $7$ 's left are now less than that of other ranks. This must mean the probability of choosing a different rank must be more than that of choosing another $7$ .
This means that the probability of an outcome (a hand) with ranks $(2, 3, 4, 5, 6)$ must be more than that of a hand with ranks $(2, 2, 2, 3, 3)$ . Kindly explain why the naive definition works here and why we treat every hand (all $52\choose 5$ hands) to be an equally likely outcome.","['poker', 'probability']"
4245716,Rings such that every module is a direct sum of generator modules,"Is there a classification of those rings $R$ for which the category of left $R$ -modules $\mathbf{Mod}(R)$ is generated by a small set of left $R$ -modules under direct sums? For example, every semisimple ring has this property, since every left $R$ -module is a direct sum of simple modules, which in turn are isomorphic to $R/I$ for some maximal left ideal $I \subseteq R$ . More generally, every Artinian principal ideal ring has the property that every left $R$ -module is a direct sum of cyclic modules. But in my question I don't require the generator modules to be cyclic.","['direct-sum', 'modules', 'ring-theory', 'abstract-algebra', 'semi-simple-rings']"
4245741,Is there a bijection hidden in this relationship?,"Let $n>r\geq0$ , \begin{equation}
\begin{aligned}
&X=\{(x_1,x_2,\cdots,x_n):x_1+x_2+\cdots+x_n=r,x_1,x_2,\cdots,x_n\in\mathbb{N}\},\\
&Y=\{(y_1,y_2,\cdots,y_{r+1}):y_1+y_2+\cdots+y_{r+1}=n-1,y_1,y_2,\cdots,y_{r+1}\in\mathbb{N}\}.\\
\end{aligned}
\end{equation} I want to establish a practical background to find the bijection between $X$ and $Y$ , I understand that solve $$x_1+x_2+\cdots+x_n=r$$ is equal to put $r$ identical balls into $n$ different boxes, but solve $$y_1+y_2+\cdots+y_{r+1}=n-1$$ not seems to be related to this background. Based on observation: maybe I can divide $$x_1+x_2+\cdots+x_n=r$$ into $r+1$ cases like that: \begin{equation}
\begin{aligned}
\quad & x_1+x_2+\cdots+x_n=r\\
&\Leftrightarrow x_1+x_2+\cdots+x_{n-1}\leq r,\\
&\Leftrightarrow
\left\{
\begin{array}{c}
x_1+x_2+\cdots+x_{n-1}=0,\\
x_1+x_2+\cdots+x_{n-1}=1,\\
x_1+x_2+\cdots+x_{n-1}=2,\\
\cdots\cdots\cdots\cdots\\
x_1+x_2+\cdots+x_{n-1}=r,
\end{array}
\right.
\end{aligned}
\end{equation} I think this $r+1$ cases has some relationship with these $r+1$ unknowns: $y_1,y_2,\cdots,y_{r+1}$ ,
corresponding to the equation: $y_1+y_2+\cdots+y_{r+1}=n-1$ , there also have $n-1$ unknowns: $x_1,x_2,\cdots,x_{n-1}$ in every cases above.
But I can’t see the essence of this relationship.
So I wonder: maybe there is a bijection hidden in this relationship ?",['combinatorics']
4245748,intersection of a hypersurface and a quasi affine variety,"Exercise I1.7 in Hartshorne states if $Z$ is an affine variety of dimension $r$ and $H$ a hypersurface then either $Z \subset H$ or every irreducible component $Z \cap H$ is dimension $r-1$ . My question is does this hold more generally where we replace irreducible $Z$ with $Z \setminus V$ with some Zariski closed subset $V$ ? In other words, suppose $Z$ is an affine variety and $V$ is some Zariski closed subset of $Z$ , and $\dim Z \setminus V = r$ . Then either $(Z \setminus V) \subset H$ or every irreducible component of $(Z \setminus V) \cap H$ is dimension $r-1$ ? I was wondering if removing a closed subset could have some effect on this nice behavior edit. I changed pure dimension of $r-1$ to every irreducible component is dimension $r-1$ .","['affine-varieties', 'krull-dimension', 'algebraic-geometry']"
4245774,What is the formal way find $a$ such that $\lim_{x\to4} \frac{ax-\sqrt{x}+6}{x-4} = \frac{3}{4}$?,"So I got this problem Determine the value of $a$ that satisfies the following limit $$\lim_{x\to4} \frac{ax-\sqrt{x}+6}{x-4} = \frac{3}{4}$$ If we substitute $x = 4$ , the value of the limit will be $\frac{4a + 4}{0}$ which is undefined. So, the numerator must have the same root as the denominator. With $x - 4 = (\sqrt{x} - 2)(\sqrt{x} + 2)$ , and the one which make the limit undefined is $(\sqrt{x} - 2)$ . From here, I conclude that the numerator must have ( $\sqrt{x} - 2$ ) as a root. And from here, I can do polynomial division and got that $a = -1$ . The other way is by argue that ""if the limit have a value, then make the result indeterminate form: $\frac{0}{0}$ . Then, we'll get $4a + 4 = 0$ and $a = -1$ . The problem is, I find my method is not good enough for essay problems.","['limits', 'calculus', 'limits-without-lhopital']"
4245790,Constructive proof that a linear relation between columns implies a linear relation between rows,"Let $M = \left(M_{ij}\right)$ be a square matrix and let $v = (v_{i}) \neq 0$ be such that $Mv = 0$ . This can be rephrased into requiring that $\sum_{i} v_{i} M_{i} = 0$ , where $\{M_{i}\}$ are the column vectors in $M$ , i.e. $(v_{j})$ gives a linear relation on the columns of $M$ . I can think of multiple proofs that the existence of $v$ implies the existence of a $w=(w_{j})$ such that $M^{t}w = 0$ , but none of them are constructive. How can we write done the coordinates of such a $w$ in terms of the coordinates of $v$ and $M$ ? In other words, how can we translate a linear relation of columns into a linear relation of rows? To be more explicit, I would like a non-trivial expression $w_j = f(v,M)$ such that when I compute $\sum_j w_j M_j$ (where $M_j$ are the row vectors in $M$ ) I obtain $0$ by using the fact that $\sum_{i} v_{i} M_{i} = 0$ . Does such an expression exist?","['matrices', 'linear-algebra']"
4245797,Center of wheel travels the length of circumference in one revolution,"I was wondering if there is a more mathematical/rigorous way of seeing that the wheel/circle/its center travels the length of wheel's circumference in one revolution. Intuitively, one could cover the wheel/circle with a string the length of which is exactly equal to its circumference. Then in one revolution the string would be spread so that we can see the center traveled the length which is equal to the circle's circumference.","['circles', 'geometry']"
4245810,What can we do to matrices that we can do to the regular good ol' numbers?,"I am a high school student and my formal Linear Algebra education consisted merely of the definition of matrices as a list of numbers and then some random properties. While reading the BetterExplained article on Linear Algebra and some of 3Blue1Brown's videos on the same did help, I still often face difficulties while solving problems and when I look up the solutions, all I'm thinking is ""Wait. You're allowed to do that to matrices too?"" So my question is - what can we do to these list of numbers that we can do to individual real and complex numbers? To clarify, I do know that we can add matrices by adding their individual elements together and multiply matrices in a row-column order and stuff like that. My doubts are along the lines of the ones listed below: Does $AB = CD$ imply that $B^{-1}A^{-1}=D^{-1}C^{-1}$ ? (where A, B, C and D are 4 non-singular matrices of appropriate orders) Is $A \times A^n = A^n \times A$ valid? (where A is a matrix and n is a natural number) Is multiplication of matrices commutative only when a matrix is being multiplied by a null matrix or unit matrix of appropriate order? Is the inverse of the matrix $A^n$ the same as the inverse of $A$ multiplied $n$ times to itself? I am not asking for proofs of the problems listed above - instead, I would greatly appreciate it if someone who has noticed some ""patterns"" in the doubts listed above would point me to some resource that I can study to clear all such doubts in my conceptual understanding. Alternatively, how should I approach matrix multiplication and their inverses in general that would solve these and other similar problems that I may be having? Thank you. Edit: These issues have been solved adequately and then some by Dave L. Renfro's comments on this question.","['matrices', 'matrix-equations', 'linear-algebra']"
4245816,Can a differential equation result in a tetrational growth?,"For an ordinary differential equation $$f(y^{(n)}(x), y^{(n-1)}(x),...,y''(x),y'(x),y(x),x) = 0$$ where $f$ can be written in a closed form using only elementary functions, is it possible for its solution $y(t)$ to grow as fast as tetration or faster? By ""written in a closed form using only elementary functions"", I mean that there only finite composition of addition, multiplication, exponentiation, exponential and trigonometric functions and their inverses are used, i.e. no tricks like infinite sums. By ""growing as fast as tetration or faster"", I mean that there is $k\in\mathbb N$ and $a >1$ , such that $y(x) \ge a \uparrow\uparrow \lfloor x\rfloor$ for all $x \ge k$ .","['tetration', 'ordinary-differential-equations']"
4245818,Continuity/differentiability of parametrized pullback of forms,"Let $M^n$ be a differentiable manifold and $\omega$ be a $k$ -form on $M$ . Suppose we have a continuous (differentiable) function $F: M \times M \rightarrow M$ , which we represent as a family of functions $(f_m)_{m \in M}$ . I am trying to see that the function $G: M \times M \rightarrow \Lambda^k(T^* M)$ , $G(m, p) = (f_m^* \omega)_p$ is continuous (differentiable if $F$ also is). While this seems intuitively obvious, I can't see a way to rigorously show this. Using trivializations of $\Lambda^k(T^* M)$ as $U \times \mathbb{R}^{\begin{pmatrix} n \\ k \end{pmatrix}}$ with $U$ open in $M$ seems absolutely hopeless, I cannot see a way to say something about $G^{-1}(U \times V)$ with $U \times V$ a basis set. Although I framed this in a general context, my motivation for this comes from the case when $M$ is a Lie group and $F$ is the multiplication function. As a side curiosity, what is the relationship between the continuity of the $G$ defined above and the map $m \in M \mapsto f_m^* \omega \in \Omega^k(M)$ with say the compact-open topology on $\Omega^k(M)$ ?","['continuity', 'general-topology', 'lie-groups', 'differential-forms', 'differential-geometry']"
4245843,Automorphism of principal bundles and sections of the adjoint bundle,"I am studying the paper of Biswas and Ramanan An infinitesimal study of the moduli space of hitching pairs and at some point they make a statement that I don't know if it is true because they don't give any kind of reference. The authors consider a smooth projective curve $C$ over $\mathbb{C}$ , and algebraic group $G$ . Let $P$ be a principal $G$ bundle over $C$ and let $P(\epsilon):=P\times\operatorname{Spec}(\mathbb{C}[\epsilon])$ where $\epsilon^2=0$ the $G$ -bundle over $C\times\operatorname{Spec}(\mathbb{C}[\epsilon])$ obtained by the pullback through the projection $C\times\operatorname{Spec}(\mathbb{C}[\epsilon])\rightarrow C$ on the first factor. They claim that is equivalent to give an automorphism of $P(\epsilon)$ which induces the identity over the closed point that to give a section of the adjoint bundle $ad P$ (the fibers are the lie algebra of $G$ ). For a section $s$ of $ad P$ , the corresponding automorphism is denoted by $1+\epsilon s$ .  Can anybody explain to me why this is true or at least give some reference for this (a priori, well-known) fact?","['algebraic-geometry', 'lie-algebras', 'schemes', 'principal-bundles']"
4245881,Expected number of tosses for a biased coin with Markov chain,"You have a biased coin, where the probability of flipping a head is 0.70. You flip once, and the coin comes up tails. What is the expected number of flips from that point (so counting that as flip #0) until the number of heads flipped in total equals the number of tails? My approach is wrong but I don't understand why!!! $x=0.7(1)+0.3(1+x)≈1.43$ . with 0.5 we get a head and we are done in 1 flip.
with 0.5 we get an additional tail and so we will have made 1 flip but then we will need to repeat X. Could you please explain clearly Markov chains. I am not very good at it.","['markov-chains', 'recurrence-relations', 'probability']"
4245882,What is the motivation for sequence invariant?,"I am working on q16 of chapter 1 in Arthur Engel's book: Problem Solving Strategies. Question: Each term in a sequence $1, 0, 1, 0, 1, 0, ...$ starting with the seventh is the sum of the last $6$ terms mod $10$ . Prove that the sequence $..., 0, 1, 0, 1, 0, 1, ...$ never occurs. Official solution: The invariant is $f(a_1, a_2, a_3, a_4, a_5, a_6) = 2a_1 + 4a_2 + 6a_3 + 8a_4 + 10a_5 + 12a_6 \quad(\text{mod } 10)$ . We have $f(1, 0, 1, 0, 1, 0) = 4$ but $f(0, 1, 0, 1, 0, 1) = 8$ , hence $0, 1, 0, 1, 0, 1$ can never occur. My translation/understanding of the solution: We want to find some property that holds in $1, 0, 1, 0, 1, 0$ and is preserved throughout the sequence but not in $0, 1, 0, 1, 0, 1$ , thus implying the latter can never occur. First we compute a few of the terms in the sequence using the recurrence $a_{i+1} = 2a_i - a_{i-6}$ , $$1, 0, 1, 0, 1, 0, 3, 5, 0, 9, 8, 5, 0, 7, 9, 8, 7, 6, 7, 4, 1, 3, 8, 9$$ but an invariant is not immediately obvious. We then note that $1, 0, 1, 0, 1, 0$ shifted one right is $0, 1, 0, 1, 0, 1$ , hence the former has more terms to the left and the latter has more terms to the right. If we take the weighted sum of the first $6$ elements and weight them more if they are on the left we would be assigning a larger value to $1, 0, 1, 0, 1, 0$ indicating that it has more non-zero terms on the left. More concretely: $$f(a_1, a_2, a_3, a_4, a_5, a_6) = w_1a_1 + w_2a_2 + w_3a_3 + w_4a_4 + w_5a_5 + w_6a_6$$ where $w_1 > w_2 > w_3 > w_4 > w_5 > w_6$ . However, how do we choose the weights (trial and error?), and why should $f \text{ mod } 10$ be invariant? (I know how to prove it is invariant, but don't understand why one would intuitively think of it as an invariant without being told beforehand.) Essentially, I am trying to answer: how could I have come up with the invariant?","['contest-math', 'solution-verification', 'sequences-and-series']"
4245888,"Entire function where at each $a \in \Bbb C$, at least one coefficient of the Taylor series at $a$ is real","I was reading about this problem yesterday: Suppose that for each $a\in \mathbb{C}$ at least one coefficient of the Taylor's series $f$ about $a$ is zero. Show that $f$ is a polynomial. I was wondering, if instead of Let $f:\mathbb{C}\rightarrow\mathbb{C}$ be a holomorphic function. Suppose that for each $a\in \mathbb{C}$ at least one coefficient of the Taylor's series $f$ about $a$ is zero . Show that $f$ is  a polynomial. we ask Let $f:\mathbb{C}\rightarrow\mathbb{C}$ be a holomorphic function. Suppose that for each $a\in \mathbb{C}$ at least one coefficient of the Taylor's series $f$ about $a$ is real . Then, is $f$ still a polynomial? It seems that I have fall into a logic mistake in approaching it: For $f(z)=\sum_{n=0}^\infty c_n(z-a)^n$ . Still, the coefficient for the series at $n$ is $c_n=\frac{f^{(n)}(a)}{n!}=r$ , for $n$ is the index and $r$ is the real coefficient. Does this means the coefficient $c_{n+1}$ should be $c_{n+1}$ because $f^{(n)}(a)=n! r$ seems like a constant? And then everything back to the ""zero"" statement. Am I right or wrong? If wrong, is there any other approaches to the new question?","['complex-analysis', 'power-series', 'polynomials', 'complex-numbers']"
4245891,Baby Rudin Theorem 8.2,"This is Baby Rudin Theorem 8.2 proof: In the last part of the proof, I don't understand why the following should be true for proof to work: $x>1-\delta$ .
Also, what does it have to do with $-1<x<1$ ? Any help is appreciated!","['power-series', 'limits']"
4245943,Prove that the polynomial has exactly 2 real roots by IVT or Rolle's Theorem,"I'm trying to prove this by IVT or Rolle's Theorem. Usually if it would say ""Prove it has only 1 real root"", I would assume it had 2 roots, take the derivative and if it was $≠0$ , then I would prove that it has only 1 root, so Rolle's Theorem. Im kind of stuck on this one, because I'm not able to suppose anything and then give a counterexample: $6x^4-7x+1=0$ Any tip or help would be much appreciated.","['rolles-theorem', 'derivatives']"
4245959,"Solving the system of ODE's $f'(t)=kg(t)e^{i\delta t}, g(t)=-kf(t)e^{-i\delta t}$","So I want to solve a system of differential equations with the two equations $$f'(t)=kg(t)e^{i\delta t}$$ $$g'(t)=-kf(t)e^{-i\delta t}$$ With $k,\delta$ being real valued constants. I'm unfortunately not very experienced with coupled differential equations like this. My initial thought was to perhaps use the laplace transform, but since I have products of two functions of $t$ on the RHS, I'm not quite sure how I'd go about transforming these. Any help here would be deeply appreciated.",['ordinary-differential-equations']
4245965,"Definition of piecewise differentiable curve in do Carmo: Riemannian Geometry; onesided derivative $\lim_{t\to a^+} c'(t)$ of a curve $c:[a,b]\to M$","Do Carmo defines a piecewise differential curve in the following manner: A piecewise differentiable curve is a continuous mapping $c:[a,b]\to M$ of a closed interval $[a,b]\subset \mathbb{R}$ into $M$ satisfying the following condition: there exists a partition $$a=t_0<t_1<\dots <t_{k-1}=b$$ of $[a,b]$ such that the restrictions $c\rvert_{[t_i, t_{i+1}]},i=0,\dots,k-1,$ are differentiable. We say that $c$ joins the points $c(a)$ and $c(b)$ . $c(t_i)$ is called a vertex of $c$ , and the angle formed by $\lim_{t\to t_i^+} c'(t)$ with $\lim_{t\to t_i^-} c'(t)$ is called the vertex angle at $c(t_i)$ ; here $\lim_{t\to t_i^+}$ ( $\lim_{t\to t_i^-})$ signifies that $t$ approaches $t_i$ trough values above (below) that of $t_i$ . As far as I know, do Carmo hasn't defined what is meant by differentiability of a curve $c: [t_i, t_{i+1}]\to M$ on a closed set. If I understand the answer given by John B in On the definition of piecewise differentiable curves correctly, it means that $c$ , as a curve on $(t_i, t_{i+1})$ , is differentiable and that the one-sided derivatives exist at the endpoints $t_i$ and $t_{i+1}$ . However, how would one go about calculating these derivatives or defining their existence? We have $$c'(t) f = (f \circ c)'(t)=\frac{d}{dt} (f \circ c)(t) = \lim_{h\to 0} \frac{f(c(t+h))-f(c(t))}{h}\quad \forall f\in C^{\infty}(M), t\in (t_i,t_{i+1}).$$ Does that mean that $\lim_{t\to t_i^+} c'(t)$ exists if $$\lim_{h\to 0^+} \frac{f(c(t_i^++h))-f(c(t_i^+))}{h}$$ exists for all $f\in C^{\infty}(M)$ ? This doesn't seem correct to me since a definition without the use of $f$ would be preferable. John B also mentioned that the differentiability defined by the existence of the one-sided derivatives is equivalent to the fact that there exists a differentiable extension of $c$ which is defined on a larger open interval. Could someone provide me with a reference for this fact?","['riemannian-geometry', 'smooth-manifolds', 'manifolds', 'derivatives', 'differential-geometry']"
4245987,Show that $(x_n)_n$ is convergent.,"Let $(x_n)_n$ be a bounded sequence. Let $a,b >0$ s.t. $\frac{a}{b}$ is an irrationnel. The sequences $(e^{iax_n})_n$ and $(e^{ibx_n})_n$ converge. Show that $(x_n)_n$ converges. Starting with the fact that $(e^{iax_n})_n$ and $(e^{ibx_n})_n$ converge, we get by definition that: Suppose that $(e^{iax_n})_n$ converges to $l_1$ , then $\forall \epsilon_1 >0 \ \exists N_1>0$ s.t. $\forall n > N_1$ : $|e^{iax_n}-l_1|< \epsilon_1$ . Suppose that $(e^{ibx_n})_n$ converges to $l_2$ , then $\forall \epsilon_2 >0 \ \exists N_2>0$ s.t. $\forall n > N_2$ : $|e^{ibx_n}-l_2|< \epsilon_2$ . From here I don't see how to show that $(x_n)_n$ is convergente.","['sequences-and-series', 'real-analysis']"
4246019,Continuity of minimum of a family of continuous functions on a compact space,"I have come up with the following problem, which I am sure has a simple solution, but I have not been able to find any until now, nor did I found any reference in literature: Let $f:\mathbb{R}\times K\rightarrow \mathbb{R}$ be a continuous function, with $K$ a compact space. For every $t\in\mathbb{R}$ , define $f_t:K\rightarrow\mathbb{R}$ as $f_t(x)=f(t,x)$ . Is it true that $\mathrm{min}_{x\in K}f_t(x)$ is continuous as a function $\mathbb{R}\rightarrow\mathbb{R}$ of the $t$ variable? Is the hypothesis on $K$ sufficient, or are more restrictive ones necessary, e.g. requiring that $K$ be a compact metric space? I was thinking about using the uniform continuity of the $f_t$ , or using some finite cover of $K$ by sufficiently small open balls (in which case it is necessary to assume that K is a metric space), or maybe even a proof by contradiction, but there simply does not seem to be any reasonable way to control the ""closeness"" of the minimum points of $f_t$ for different values of $t$ (especially because they might vary in cardinality). Any help would be much appreciated, thank you!","['continuity', 'analysis', 'compactness']"
