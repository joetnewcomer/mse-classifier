question_id,title,body,tags
2711504,"In how many ways can R red, B blue, and G green balls be arranged so that no two balls of the same colour are consecutive?","Given R red balls, B blue balls and G green balls, how many ways are there to arrange them on a straight line such that no two balls of the same color are next to each other? In essence, my question is a generalization of this question. In the above post, the accepted answer enumerates all cases in which there is a violation of the restriction. As such, it may not be so easy to compute if the quantity of each ball is large (e.g. 100 of each ball) as we have to compute each case manually. I wish to ask if there is a generalized formula/expression that can answer the same question for R red balls, B blue balls and G green balls (It doesn't matter even if the expression is not in closed-form because I want to compute this on a computer). Could someone please advise me?",['combinatorics']
2711513,If $X$ and $Y$ are independent r.v. then $E[Y \mid \sigma(X)] = E[Y]$ using $\sigma$-algebra's,"I have seen the following definition of a conditional expectency: Let $X$ be a r.v. in a probability space $(\Omega, \mathcal{F}, \mathcal{P})$ and let $\mathcal{G}\subseteq\mathcal{F}$ be a sub- $\sigma$ -algebra, then $E[X\mid \mathcal{G}]$ is an integrable random variable such that: $E[X\mid\mathcal{G}]$ is $\mathcal{G}$ -measurable $\int_G X \mathrm{d}\mathcal{P} = \int_G E[X\mid \mathcal{G}] \mathrm{d}\mathcal{P}$ for each $G\in \mathcal{G}$ Now I would like to prove the following: If $X$ and $Y$ are independent and Y is integrable, then $E[Y\mid \sigma(X)] = E[Y]$ This implies that I need to prove two things, Is $E[Y]$ $\sigma(X)$ -measurable? (where $\sigma(X) = \{X^{-1}(A):A\in \mathcal{B}(\mathbb{R})\}$ ) This looks ok, since $E[Y]$ can be seen as a function which maps $\Omega$ on $E[Y]\in \mathbb{R}$ . This implies $E[Y]^{-1}=\Omega \in \sigma(X)$ . Is $\int_G E[Y] \mathrm{d}\mathcal{P} \stackrel{?}{=} \int_G Y \mathrm{d}\mathcal{P}\qquad $ ( $\forall G\in \sigma(X))$ I'm don't see the implications of the independence here. The left hand side is equal to $E[Y]\mathcal{P}(G)$ , but how about that right hand side?
I know that $X$ independent from $Y$ implies $\sigma(X)$ and $\sigma(Y)$ independent, which implies $\forall A\in \sigma(X), \forall B\in \sigma(Y)$ , $\mathcal{P}(A\cap B)= \mathcal{P}(A)\cdot \mathcal{P}(B)$ but how can I use this here? And is $\int_G Y \mathrm{d}\mathcal{P}$ where $G\in \sigma(X)$ even properly defined?","['independence', 'probability-theory', 'conditional-expectation', 'measure-theory']"
2711535,Understanding the concept of $(X^TX)^{-1}X^Ty$,"$w = (X^TX)^{-1}X^Ty$ Equation above produces weight $w$ which solves quadratic minimization problem in linear functions. From my understanding, the goal is to find $w$ such that: $Xw ≈ y$ Thus, I need to minimize this function: $||Xw-y||^2$ According to wikipedia , Euclidean norm is used to minimize this function: $2X^T(Xw - y) = 0$ Then this equation is differentiated with respect of $w$, from my understanding we use multivariable chain rule: $X^TXw-X^Ty=0$ $X^TXw=X^Ty$ $w=(X^TX)^{-1}(X^Ty)$ Somehow, by utilizing Euclidean norm, I minimized the function, but I'm unable to understand how does it exactly work. Why is Euclidean norm used to solve quadratic minimization problems in linear functions?","['multivariable-calculus', 'least-squares', 'linear-algebra']"
2711555,Is my proof that $f[f^{-1}[B]] = B$ iff $B \subseteq \operatorname{Rng}(f)$ correct?,"Let $f:S\to T$ be a function. Show that for each subset $B \subseteq T$, we have $f[f^{-1}[B]] = B$ iff $B \subseteq \operatorname{Rng}(f)$. here is what I have : To show that $f^{-1}[f[B]] =B\Leftrightarrow B\subseteq \operatorname{Rng}(f)$ :
\begin{align}f^{-1}[f[B]] = B & \Leftrightarrow f(b_1)=f(b_2)\qquad \text{for some } b_1,b_2\in B \\ 
& \Leftrightarrow b_1=b_2 \\ 
& \Leftrightarrow f(b)\in B \\ 
& \Leftrightarrow\operatorname{Rng}(f)=B \\ 
& \Leftrightarrow B\subseteq \operatorname{Rng}(f)
\end{align}","['elementary-set-theory', 'proof-verification']"
2711566,Multivariate Faà di Bruno's formula,"I'm attempting to implement a computer algebra function using the combinatoric version of Faà di Bruno's formula presented by Michael Hardy in Combinatorics of Partial Derivatives that ""collapses"" partitions to account for multiple variables. The paper is mostly very well-written and intelligible (its examples are used in the Wikipedia article) but there's one thing I'm unclear about. To give an example: I do the following: Compute the integer partition of the order represented as nested sequences Take the unmixed partial of f at the order corresponding the number of blocks in each partition Compose it with g Multiply the composition with the mixed partials of f corresponding to the blocks in the partition Sum the functions corresponding to each partition I'm currently distributing the multiplication at each order/partition rather than collapsing partitions and multiplying by a scalar, so I'm duplicating some work but am just trying to get it correct right now). I think the problem is that I'm misunderstanding the composition at each order, i.e. that f'''(y) in Hardy's example is not , if fact, the unmixed second-order partial of f composed with g. I just can't think of anything else that could be meant by notation like f''(y)(dy/x1 * dy^2/dx2xdx3). Any clarification would be greatly appreciated.","['calculus', 'multivariable-calculus', 'algorithms', 'combinatorics', 'analytic-combinatorics']"
2711606,Compactness is preserved by continuous surjections.,"Let $f: X \to Y$ be a continuous surjection between two topological
  spaces. Show that $Y$ compact is, given that $X$ compact is. My attempt : Let $\mathcal{G}$ be an open cover of $Y$. Then, $\bigcup \mathcal{G} = Y$ and $$X = f^{-1}(Y) = f^{-1}(\bigcup\mathcal{G}) = \bigcup f^{-1}(\mathcal{G})$$ and because $f$ is continuous, it follows that $f^{-1}(\mathcal{G})$ is an open cover of $X$. By compactness, there exist $G_1 \dots,G_n \in \mathcal{G}$ such that $X = \bigcup_{i=1}^n f^{-1}(G_i)$ and hence $Y = \bigcup_{i=1}^nf(f^{-1}(G_i)) = \bigcup G_i$ (here we used that $f$ is surjective) and we conclude that $\{G_i\mid i=1, \dots n\}$ is a finite subcover of $\mathcal{G}$, and we deduce that $Y$ is compact. Is this correct?","['continuity', 'general-topology', 'compactness', 'proof-verification']"
2711615,"If $\,a_n\searrow 0\,$ and $\,\sum_{n=1}^\infty a_n<\infty,\,$ does this imply that $\,n\log n\, a_n\to 0$?","A quite elegant and classic exercise of Calculus (in infinite series) is the following: If the non-negative sequence $\{a_n\}$ is decreasing and $\sum_{n=1}^\infty a_n<\infty$, then $na_n\to 0$. To show this observe that, if $\,\sum_{n\ge n_0}a_n<\varepsilon/2$, then for for $n\ge 2n_0+1$, 
 $$\frac{\varepsilon}{2}>a_{\lfloor n/2\rfloor}+\cdots+a_n\ge \frac{1}{2}na_n\ge 0.$$ This, in a sense, is related to the fact that $\sum\frac{1}{n}=\infty$. To go one step further, since $\sum\frac{1}{n\log n}=\infty$, can we obtain, with the same assumptions on $\{a_n\}$, that $\,n\log n\, a_n\to 0$? This conjecture holds with the additional assumption that $b_n=na_n$ is also decreasing. To see this, let $n_0\in\mathbb N$, such that $\sum_{n\ge n_0}a_n<\varepsilon$. Then for $n\ge n_0^2$, we have
$$
\varepsilon>\sum_{\sqrt{n}\le k\le n}a_n\ge \sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor}\sum_{k=\lfloor2^{\ell-1}\log_2\sqrt{n}\rfloor+1}^{ \lfloor2^\ell\log_2\sqrt{n}\rfloor}a_k\ge \sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor} (2^{\ell-1}\log_2\sqrt{n}-1)a_{\lfloor2^\ell\log_2\sqrt{n}\rfloor} \\ \ge
\sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor} \Big(\frac{1}{2}{\lfloor2^\ell\log_2\sqrt{n}\rfloor}-1\Big)
a_{{\lfloor2^\ell\log_2\sqrt{n}\rfloor}}\ge \sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor}\Big(\frac{1}{2}n-1\Big)a_n\ge (\log_2 n-1)\Big(\frac{1}{2}n-1\Big)a_n.
$$ 
Hence, $(\log_2 n-1)\big(\frac{1}{2}n-1\big)a_n\to 0$, which implies that $\,\,n\log n\,a_n\to 0$. One step further: If $a_n>0$, $\sum_{n=1^\infty}a_n<0$ $a_n$, and the sequences $na_n$ and $n\log n a_n$ are decreasing, then
$$
n\log n \log\log n\,a_n\to 0.
$$","['logarithms', 'sequences-and-series', 'calculus', 'summation', 'convergence-divergence']"
2711621,Alternative for calculating the nth of quadratic sequence,"Given the quadratic sequence
$$f(n)=1, 7, 19, 37, \cdots$$ To calculate the $f(n)$ for $n\ge1$. $$f(n)=an^2+bn+c$$ We start with the general quadratic function, then sub in for $n:=1,2$ and $3$ $$f(1)=a+b+c$$
$$f(2)=4a+2b+c$$
$$f(3)=9a+3b+c$$ Now solve the simultaneous equations $$a+b+c=1\tag1$$
$$4a+2b+c=7\tag2$$
$$9a+3b+c=19\tag3$$ $(2)-(1)$ and $(3)-(2)$ $$3a+b=6\tag4$$
$$5a+b=12\tag5$$ $(5)-(4)$
$$a=3$$
$$b=-3$$
$$c=1$$ $$f(n)=3n^2-3n+1$$ This method is very long. Is there another easy of calculating the $f(n)$?",['algebra-precalculus']
2711625,"problem with non measurable, bochner-integrable function","Let $(S,\Sigma,\mu)$ be a measure space and $(X,\Vert .\Vert_X)$ a Banach space. I got a question concerning the properties of the Bochner Integral (as it is defined here: https://en.wikipedia.org/wiki/Bochner_integral ) in the case of an non-complete measure space (for definition and example look here: https://en.wikipedia.org/wiki/Complete_measure ). In the wikipedia article (or in 'Analysis in Banach Spaces - Volume 1' by Hytönen, van Neerven, Vera, Weis as a recent alternative reference) it is mentioned, that for a Bochner-measurable function $f:S\mapsto X$ bochner-integrability is equivalent to the lebesgue-integrability of $\Vert f\Vert_X$. On page 14. in the mentioned book, the authors claim: 'If f is Bochner integrable and f=g almost everywhere, then g is Bochner integrable and the Bochner integrals of f and g agree.' ($f, g:S\mapsto X$) Now if $(S,\Sigma,\mu)$ is non-complete, then there exists a set $M\in\Sigma$ with $\mu(M)=0$ and a subset $N\subset M$ such that $N\notin\Sigma$. For this set N, the characteristic function $\chi_N$ equals almost everywhere (on the complement of the nullset M) the Bochner integrable zero function, and hence, by the mentioned equivalence, $\chi_N$ is Lebesgue integrable without even being measurable. I hope someone can help me in my confusion.
Thanks so far
PS: One can find alternative examples without using the claimed fact, that Bochner integrability is preserved by an almost everywhere equality.","['integration', 'measure-theory']"
2711633,"Finding the point on $6x^2+y^2=262090$ that is nearest to the point $(1045,0)$","I know that to find the point on $6x^2+y^2=262090$ that is nearest to the point $(1045,0)$, we can try to minimize the squared distance 
$S=(x-1045)^2+262090-6x^2$. However, calculus tells us that this function does not have a minimum point (instead only a maximum point exists). But if we try to minimize the distance function ( without squaring) then we can find the minimum. So, When exactly can we actually square the distance function to find the max/min point for distance problems?","['derivatives', 'calculus']"
2711639,"Finding the partial derivatives of an integral given by $f(x,y) = \int_{y}^{x} \cos (t^2) \ dt$","I am asked to find the partial derivatives of an integral given by $f(x,y) = \int_{y}^{x} \cos (t^2) \ dt$. What I did was \begin{align}
f(x,y) &= \int_{y}^{x} \cos (t^2) \ dt\\
\\
\frac{\partial f}{\partial x} &= \frac{\partial}{\partial x} \int_{y}^{x} \cos (t^2) \ dt = \cos (x^2)\\
\frac{\partial f}{\partial y} &= - \frac{\partial}{\partial y} \int_{x}^{y} \cos (t^2) \ dt = - \cos (y^2)\\
\end{align} It seems very similar to what we do on single variable calculus (Fundamental Theorem of Calculus part 1), that is, substituting the variable and aplying the chain rule (which in this case will lead to a multiplication by $1$ in both cases). Can someone confirm my answer?","['multivariable-calculus', 'integration']"
2711647,Let $X$ be a left-invariant vector field. If $\exp(X)\in Z(G)$ then $X$ is right-invariant.,"Let $G$ be a Lie Group and $X$ a left-invariant vector field. Show that if $\exp(X)$ is in the center of $G$, i.e $\exp(X)\in Z(G)$, then $X$ is right-invariant. I'm trying to use the fact that $X$ is a left-invariant and right-invariant vector field iff $\mathrm{Ad}(g)(X)=X$ for all $g\in G$. Using this, and the relation $C_g(\exp(X))=\exp(\mathrm{Ad}(g)(X))$, where $C_g(h)=ghg^{-1}$, we have:
$$
\exp(X)=\exp(\mathrm{Ad}(g)(X)), \quad \forall g \in G.
$$ But I don't know how to conclude that $\mathrm{Ad}(g)(X)=X$ for all $g\in G$.","['vector-fields', 'smooth-manifolds', 'exponential-function', 'ordinary-differential-equations', 'lie-groups']"
2711676,Volume of Ellipsoid using Triple Integrals,"Given the general equation of the ellipsoid $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} =1$, I am supposed to use a 3D Jacobian to prove that the volume of the ellipsoid is $\frac{4}{3}\pi abc$ I decided to consider the first octant where $0\le x\le a, 0\le y \le b, 0 \le z \le c$ I then obtained $8\iiint _E dV$ where $E = \{(x, y, z): 0\le x \le a, 0\le y \le b\sqrt{1-\frac{x^2}{a^2}}, 0\le z \le c\sqrt{1-\frac{x^2}{a^2} - \frac{y^2}{b^2}} \}$ I understood that a 3D Jacobian requires 3 variables, $x$, $y$ and $z$, but in this case I noticed that I can simple reduce the triple integral into a double integral: $$8 \int_0^a \int_0^{b\sqrt{1-\frac{x^2}{a^2}}} c\sqrt{1-\frac{x^2}{a^2} - \frac{y^2}{b^2}} dydx$$ which I am not sure what substitution I should do in order to solve this, any advise on this matter is much appreciated!","['multivariable-calculus', 'jacobian']"
2711706,Fundamental theorem of calculus with 2-variables integrand,"I have an equation in the form $$\int_x^\infty f(x,y) \, dy = \int_x^\infty g(y) \, dy$$ from which I would like to derive a relation between the functions $f$ and $g$. If I take the partial derivative w.r. to $x$ of both sides, I get, using the fundamental theorem of calculus, $$\partial_x \left( \int_x^\infty f(x,y) \, dy\right) = -g(x)$$ However, I don't know what to do with the first term. Is there anything I can conclude about it without having to find an analytical expression for the integral?","['derivatives', 'partial-derivative', 'calculus', 'improper-integrals', 'integration']"
2711754,Can the product of infinitely many elements from $\mathbb Q$ be irrational?,"I know there are infinite sums of rational values, which are irrational (for example the Basel Problem). But I was wondering, whether the product of infinitely many rational numbers can be irrational. 
Thank you for your answers.","['sequences-and-series', 'irrational-numbers']"
2711764,How do we count solutions of $a^2 + b^2 + c^2\equiv n\pmod p$?,"Let's count number of solutions of $a^2 + b^2 + c^2 \equiv n\pmod p$ for each integer $n$ and each prime $p$.  I'd start by just writing a function: $$V_n(\mathbb{C}) = \left\{  (a,b,c) \in \mathbb{C}^3: a^2 + b^2 + c^2 = n \right\}$$ Maybe we could arrange this into some kind of Dirichlet series: $$ f(s) = \prod_p \frac{\# V_n(\mathbb{F}_p)}{p^s}
 = \prod_p \frac{\# \{ (a,b,c): a^2 + b^2 + c^2 = n  \}}{p^s} $$ This is now a Hasse-Weil zeta function or an Arithmetic zeta function, although I didn't mean to be that advanced. Here's the sequence of numbers for $n = 3$.  The left column is a prime number and the right column is the number of solutions.  The result looks somewhat random: [(2, 4),
 (3, 9),
 (5, 20),
 (7, 56),
 (11, 110),
 (13, 182),
 (17, 272),
 (19, 380),
 (23, 506),
 (29, 812)] Here is $n=7$ [(2, 4),
 (3, 6),
 (5, 20),
 (7, 49),
 (11, 132),
 (13, 156),
 (17, 272),
 (19, 342),
 (23, 552),
 (29, 870)]","['combinatorics', 'zeta-functions', 'algebraic-geometry']"
2711771,Derivation of a vector-valued function,"i have trouble to calculate the derivation of the the norm. I have given a curve $\gamma :[0,L] \rightarrow \mathbb{R^3}$ with $\gamma(0)=\gamma(L)$, $\Vert \gamma^´(s) \Vert=1$ and a second function $\beta(s)=\Vert \gamma(s)-\gamma(0)\Vert$. I´d like to calculate $\beta'(s)$ My approach:
I set $f(\cdot):= \Vert \cdot\Vert$ and $g(s):=\gamma(s)-\gamma(0)$, then i get by chain rule $\frac{d}{ds}f(g(s)) =f'(g(s))g'(s)=\frac{\gamma(s)-\gamma(0)}{\Vert \gamma(s)-\gamma(0)\Vert}\gamma'(s) $ where i use that $f'(\vec{v})=\frac{\vec{v}}{\Vert \vec{v} \Vert}$ Is it correct or did i miss something ?","['derivatives', 'proof-verification']"
2711779,Curl of gradient is not zero,"I have heard that for some functions $T$ , if we calculate $\nabla \times (\nabla  T )$ in $2$ -dimensional polar coordinates, then we get the delta function. Why do we get that result?","['curl', 'calculus', 'multivariable-calculus', 'vectors', 'vector-analysis']"
2711886,Spectral Theorem when the Hilbert space is separable,"Let $H$ be a complex Hilbert space. We recall the following theorem: Spectral Theorem: Let $A_1$ and $A_2$ be two commuting normal operators, then there exists a measure space $(X,\mathcal{E},\mu)$,
two functions $\varphi_1,\varphi_2\in L^\infty(\mu)$ and an unitary operator $U:H\longrightarrow L^2(\mu)$, such that each $A_k$ is unitarily equivalent to multiplication by $\varphi_k$, $k=1,2$. i.e.
$$UA_kU^*f=\varphi_kf,\;\forall f\in H,\,k=1,2.$$ If $H$ is separable, why $\mu$ can be taken  $\sigma$-finite?","['functional-analysis', 'reference-request', 'measure-theory']"
2711889,"Markov's inequality, unclear Wikipedia intuition","Search for the word ""intuitive"" here . I do not understand how $$E[X]=0\cdot \bar{a}+\frac{E[X]}{a}\cdot a$$ shows anything at all. I would like to see an intuitive proof of an upper bound of $P(x\geq a)$ which should be $$P(x\geq a)\leq \frac{E[X]}{a}.$$","['means', 'statistics', 'inequality', 'probability']"
2711891,"Showing inequality: $pe^{x(1-p)}+(1-p)e^{-xp} \leq e^{x^2(3/4)p}$ for $0 \leq p \leq 1/2, 0 \leq x \leq 1$?","How can I show that $$pe^{x(1-p)}+(1-p)e^{-xp} \leq e^{x^2(3/4)p}$$ for $0 \leq p \leq 1/2, 0 \leq x \leq 1$? I've been stuck on this for a long time; I tried expanding out the taylor series on either side, and I tried using convexity, but neither method seems to help. I was able to get the inequality down to $\leq  e^{x^2(1-p)p}$ on the right side using the method here but I couldn't see a way to further tighten the upper bound.","['statistics', 'inequality', 'exponential-function', 'moment-generating-functions']"
2711901,Finding a list of Cunningham chains online,"A succession of primes such that $p$, $2p+1$, $2(2p+1)+1$, $\ldots$ is called a Cunningham chain . There are several sites on the internet that show Cunningham chain records , i.e., the smallest prime starting a chain, the longest chain of the first or second kind, and so forth. However, I am interested in simple lists of Cunningham chains that allow to search for a chain of length $l$, containing a certain prime, $\ldots$ I am certain there is some sort of Cunningham chain database somewhere. Can anyone give such a link?","['number-theory', 'reference-request', 'prime-numbers']"
2711946,Residue of $\sin(\tan(z))$ at $z=\frac{\pi}{2}$,"In finding this residue, I took a Taylor expansion of $\sin(x)$ and substituted in $\tan(z)$ and then attempted to compute the residues of the odd powers of $\tan(z)$ at $z=\frac{\pi}{2}$. Using Wolfram Alpha, it seemed to be that: 
\begin{equation}
\operatorname{Res}(\tan^{2n-1}(z), \frac{\pi}{2}) = (-1)^n
\end{equation} 
Giving the residue of $\sin(\tan(z))$ as $-\sinh(1)$. Is this correct? Also, I was unable to show $\operatorname{Res}(\tan^{2n-1}(z), \frac{\pi}{2}) = (-1)^n$. Is this true and if so any hints on how to show it?","['complex-analysis', 'residue-calculus']"
2711951,Characteristic function of a random vector,"We consider the random vector  $ X\colon \Omega \to \mathbb {R}^n$ defined on the probability space $(\Omega, \mathfrak F, P)$. Let denote by $\Phi_{X}(x) = \mathbb E(e^{i\left<x, X\right>})$ its characteristic function. I would like to show the following equivalence: $X$ is a Gaussian vector if and only if $\Phi_{X}(x)$ is given by $$\Phi_{X}(x)= e^{i\left<m, x\right> -\frac{1}{2}\left<A x, x\right>} \qquad (*),$$
where $m=(\mathbb E(X_1), \dots, \mathbb E(X_n))$ and $A=Cov(X)$. I showed the direct sense (i.e. if $X$ is a Gaussian vector then $\Phi_{X}(x)$ is given as $(*)$). In fact, I have used the following $\Phi_{X}(x)=\Phi_{Z_x}(1) = \exp\{im_{x} - \frac{1}{2}\sigma_{x}^2\} = ...$, where $Z_x=\sum_{j=1}^{n}x_j X_j$ is a random variable in $N(m, \sigma^2)$ .... Now, I need help for the opposite direction. Thank you in advance","['stochastic-processes', 'characteristic-functions', 'probability-theory', 'probability', 'random-variables']"
2712023,How do we know phasors solve differential equations?,"I know that phasors are used to simplify the calculations, I also get why in AC, current in a capacitor leads the voltage by 90°, and lags for an inductor (we can see that by differentiating/integrating), but how do we know it will solve differential equations too? Thinking of sine and cosine terms as vectors really helps in adding/subtracting them. (But I don't see how it helps in multiplying/dividing) Suppose I have an RL circuit connected in series to an AC emf like this: Then, by phasors, we would say $\overline{Z} = \sqrt{R^2 + L^2 \omega^2}\angle\tan^{-1}(\omega L/R)$
or $\overline{i} = \displaystyle\frac{e_0}{\sqrt{R^2 + L^2 \omega^2}}\angle\tan^{-1}(-\omega L/R)$ as $\overline{i}=\displaystyle\frac{\overline{e}}{\overline{Z}}$ (Taking $\overline{e} = e_0\angle0$) but without using phasors, we would have written $+e_0\sin(\omega t)-iR - L\frac{\text{d}i}{\text{d}t} = 0$ and try to solve that differential equation, right? How do we know we're solving that differential equation by doing phasor algebra? By the expression I got for $\overline{i}$, $i(t) = \displaystyle\frac{e_0}{\sqrt{R^2 + L^2 \omega^2}} \sin(\omega t - \tan^{-1}(\omega L/R))$ Then I thought, is it really the solution to the differential equation $+e_0\sin(\omega t)-iR - L\frac{\text{d}i}{\text{d}t} = 0$? I looked up the solution to that equation and it's slightly different $$i(t) = \displaystyle\frac{e_0}{\sqrt{R^2 + L^2 \omega^2}} \sin(\omega t - \tan^{-1}(\omega L/R)) - c e^{-Rt/L}$$ According to this , the phasor answer is correct if c=0, which does not mean $i(0) = 0$, I checked the graph, but it isn't too different from the phasor answer for positive values of t. Why are we getting this extra term different from the phasor answer?","['physics', 'ordinary-differential-equations', 'calculus']"
2712031,How to solve $y'=(2x+y)^2$?,"I need to solve the ODE $$y'=(2x+y)^2$$ This ODE is Riccati's equation , but I can't bring it to a simpler form.  Thanks",['ordinary-differential-equations']
2712054,A good reference to study the Riemann Roch theorem for Varieties (specially surfaces),"I'm doing my graduation thesis in algebraic geometry and I intend it prove Riemann Roch for curves at the end. I thought that I would like to expand it to include the general theorem for surfaces or even for general varieties. My approach in the thesis does not include schemes. I only work over varieties (affine, projective, quasi-affine and quasi-projective). I know some good resources where I can find the proof in case of surfaces (e.g. Fulton). Now, I look for some resources the provide the general theorem (either for surfaces or for general varieties) and prove them so that I can understand the proof. My only requirement is that the treatment does not use scheme-theoretic techniques (but using sheaf etc ... is welcome) since that would require me to change everything I've done so far, which would not be reasonable in the remaining time. As I've been searching through the web, I noted that some references deal with the case of surfaces only over complex numbers (i.e. Riemann Surfaces) and restrict their treatment to that case and use things from complex geometry. Other references deal with the relative version due to Grothendick using schemes. I look for something in between. That is, I want a reference that deals with the theorem in case of a surface over any algebraically closed field, or even better, the theorem in case of a variety over any algebraically closed field. Can anyone point some resources (notes, books ... etc) that satisfy those requirements? I've skimmed Zaraski book on algebraic surfaces from the thirties but it seems to be using techniques of linear systems, which seems outdated and are due to the Italian school. I'm aware of the text book ""Topological methods in algebraic geometry"" by Hirzebruch. But I'm not sure if its approach coincides with my goals. Would I need to read all of it? Can I skip something If I'm only interested in Riemann-Roch? if yes, which?","['riemann-surfaces', 'surfaces', 'algebraic-geometry', 'geometry']"
2712082,Working with limits of functions of the type $f: \mathbb{R}^2 \to \mathbb{R}$,"I am trying to jump doing $\epsilon-\delta$ proofs of the limit of functions of one variable (i.e. $f:\mathbb{R} \to \mathbb{R}$), to functions of two variables (i.e. $g: \mathbb{R}^2 \to \mathbb{R}$) and I am struggling since some of the methods I learned for one variable do not immediately carry for two variables. Unfortunately, I find that my books are too eager to jump to nice theorems to calculate limits (limit of a sum is equal to the sum of the limits, etc.) but I insist on being able to find limits to the most elementary functions like $x+y,xy, x^2 + y^2, \frac{x}{y}, \frac{1}{x+y}$ using the $\epsilon-\delta$ definition (without more advanced notions like the topology of $\mathbb{R^n}$) of the limit until I gain some intuition to move on and prove more general theorems using this technique. In this post, I will show you one case I think I was able to work out correctly, and another where I am struggling. 1) Perharps the simplest case I could think was to show that $$\lim_{(x,y)\to(a,b)}(x+y) = a+b$$. In this case, the definition says that $f(x,y)=(x+y) \to (a+b)$ when $(x,y) \to (a,b)$ if $\forall \epsilon > 0$, $\exists \delta > 0$ such that for all $(x,y)$ in the domain of $f$, whenever $0<\sqrt{(x-a)^2+(y-b)^2}<\delta$, we have $|f(x,y) - (a+b)|<\epsilon$. For this, all I had to do was to realize that $\sqrt{(x-a)^2+(y-b)^2}<\delta$ implies that $|x-a|<\delta$ and $|y-b|<\delta$, therefore $|x + y - a -b| \le |x-a| + |y-b| < 2\delta$, therefore I should choose $\delta = \epsilon/2$. Is this line or reasoning correct? 2) Now, consider $$
\lim_{(x,y)\to(a,b)}xy = ab
$$ working backwards, I want $$
\begin{align}
|xy - ab| &< \epsilon\\
|(x-a+a)(y-b+b) -ab| &<\epsilon \\
|(x-a)(y-b) +b(x-a) +a(y-b)| &< \epsilon \qquad (*) \\
&\vdots\\
\sqrt{(x-a)^2+(y-b)^2}&<\delta
\end{align}
$$ (*) I got this idea from here . But that's it I am afraid. I don't know how to take it from there. I would appreciate any hint or guidance. Sorry if this post is too long. I wanted to provide context and background and would be happy to edit or split questions. Thank you in advance.","['multivariable-calculus', 'limits']"
2712102,Why is the dotproduct of direction and gradient the directional derivative? ($\nabla_\hat{v} f = \nabla f \bullet \hat{v}$),"I have seen the claim that the directional derivative of $f$ in the direction $\hat{v}$ , where $||\hat{v}|| = 1$ , (denoted $\nabla_\hat{v} f$ ) is equal to the gradient of $\nabla f$ dotted with $\hat{v}$ . I have tried to prove this to myself, but I got stuck: $\newcommand{\R}{\mathbb{R}}$ $\newcommand{\limit}[2]{\lim_{#1 \to #2}}$ $\newcommand{\pderiv}[2]{\dfrac{\partial#1}{\partial#2}}$ Let $f : \R^n \to \R$ I accept that $$ \nabla_{\hat{v}} f = \limit{h}{0} \frac{f(x + h\hat{v}) - f(x)}{h}$$ for making intuitive sense. Furthermore I know that $$ \pderiv{f}{x_i}  = \limit{h}{0} \frac{f(x + h\hat{i}) - f(x)}{h} $$ where $\hat{i}$ is the unit vector of the $i$ -th dimension. I also know that $$\nabla f =  \left( \pderiv{f}{x_1}, \dots, \pderiv{f}{x_n} \right)$$ Now I want to show that $\nabla_\hat{v} f = \nabla f \bullet \hat{v}$ : \begin{align*}
\nabla f \bullet \hat{v} & = \left( \pderiv{f}{x_1}, \dots, \pderiv{f}{x_n} \right) \bullet v\hat{v}\\
                    &= \sum_{i = 1}^{n} \pderiv{f}{x_i} \cdot \hat{v}_i\\
                    &= \sum_{i = 1}^{n} \limit{h}{0} \frac{f(x + h\hat{i}) - f(x)}{h} \cdot \hat{v}_i\\
                    &= \sum_{i = 1}^{n} \limit{h}{0} \frac{\hat{v}_if(x + h\hat{i}) - \hat{v}_if(x)}{h}\\
\end{align*} And now I am stuck. I don't see a way to transform the last line into $\limit{h}{0} \frac{f(x + h\hat{v}) - f(x)}{h}$ to reach $\nabla_{\hat{v}} f$ Can you help me out here? Edit I now conceptually understand why the dotproduct of direction and gradient is the directional derivative: Let's say we have a differentiable function $f$ from $\mathbb{R}^2$ to $\mathbb{R}^3$ mapping the $xy$ -plane into the $xyz$ -space and  we want to know the directional derivative of $f$ at a point $p = (x', y')$ for some vector $u$ . First what we need to know is, how much $f$ changes in $x$ direction and how much it changes in $y$ direction. Then we need to realize that for small distances whatever direction we go along the surface of $f$ , the total change in height is the sum of the change in height in the $x$ component of our direction and the change in height in $y$ component in our direction. But now we can weight the partial derivatives for the $x$ and $y$ direction with the components of $u$ to get their individual contributions for the direction in which $u$ is pointing! So if $u$ has an $x$ -component of $u_x$ we weight the partial derivative of $f$ for the $x$ direction accordingly. When we do the same for $y$ we get: $f'(x') \cdot u_x + f'(y') \cdot u_y$ which is in fact $\nabla f \bullet u$ I understood this after listeing to this lecture: https://www.youtube.com/watch?v=tDPp5uWSIiU","['multivariable-calculus', 'vectors', 'vector-analysis']"
2712128,"Let $g$ be a function such that $\underline{\int_{y\in B}}f(x,y)\leq g(x)\leq \overline{\int_{y\in B}}f(x,y)$ for all $x\in A$. Show that if $f$","(a) Exercise 1 of that section is : Let $f,g: Q\to \mathbb{R}$ be bounded function such that $f(x)\leq g(x)$ for $x\in Q$. Show that $\underline{\int_{Q}}f\leq \underline{\int_{Q}}g$ and $\overline{\int_{Q}}f\leq \overline{\int_{Q}}f$ , I think of using the exercise to conclude that $\underline{\int_{x\in A}}\underline{\int_{y\in B}}f(x,y)\leq \underline{\int_{x\in A}}g(x)$ and $\overline{\int_{x\in A}}g(x)\leq \overline{\int_{x\in A}}\overline{\int_{y\in B}}f(x,y)$ for all $x\in A$, But fubini's theorem tells me that $\int_{Q}f=\int_{x\in A}\underline{\int_{y\in B}}f(x,y)=\int_{x\in A}\overline{\int_{y\in B}}f(x,y)=\underline{\int_{x\in A}}\underline{\int_{y\in B}}f(x,y)\overline{\int_{x\in A}}\overline{\int_{y\in B}}f(x,y)$, with which $\overline{\int_{x\in A}}g(x)=\int_{Q}f=\underline{\int_{x\in A}}g(x)$, with this I could not conclude that $g$ is integrable over $A$? I need help for (b) and (c), could someone help me please? Thank you.","['real-analysis', 'calculus', 'multivariable-calculus', 'integration', 'vector-analysis']"
2712159,Helmholtz-like decomposition for the metric tensor.,"Question: Does there exist a Helmholtz-like decomposition of the metric tensor into holonomic and anholonomic counterparts? Motivation: I know that for a given tensor function $F$, one can find a vector-valued function $u$ and a tensor-valued function $A$ such that $F=Du+\text{Curl}A$. In this case, the requirement $\text{Curl}F=0$ is an integrability condition for $F$. What I wonder is if an analogous conclusion holds for a metric tensor $G$. In this case, the integrability condition is $\text{Riem}(G)=0$. When this is satisfied, there exists a vector-valued function $u$ such that $G=(Du)^T(Du)$. But what if $\text{Riem}(G)\neq 0$? For given $G$, would there exist a vector valued function $u$ and a (symmetric) tensor function $A$ such that 
$$
G=(Du)^T(Du) + \text{Riem}(A)\qquad (\text{or similar}?) 
$$","['tensors', 'differential-geometry']"
2712170,Direct image of coherent analytic sheaves,"Given holomorphic varieties $X$ and $Y$, and let $\phi: X \rightarrow Y$ be a morphism between these varieties. Consider a coherent analytic sheaf $\mathcal{S}$ on $X.$ Why, in general the direct image $\phi_{*} \mathcal{S}$ is not a coherent analytic sheaf on $Y$? Could someone give to me a counterexample?","['coherent-sheaves', 'sheaf-theory', 'algebraic-geometry']"
2712196,"If two measures coincide on bounded continuous functions, do they coincide on Borel subsets?","Let $X$ be a Hausdorff space, endowed with the Borel $\sigma$-algebra, and let $\mu, \nu$ be two regular Borel probabilities on $X$. Assume that $\int _X f \ \mathrm d \mu = \int _X f \ \mathrm d \nu$ for all $f \in C_b (X)$ (the bounded continuous complex functions). Does it follow that $\mu (B) = \nu (B)$ for all $B \in \mathcal B (X)$ (the Borel subsets)? It clearly happens for locally-compact spaces: taking $f \in C_0 (X)$ (the functions that vanish at infinity), we have that $\mu = \nu$ in $C_0 (X) ^*$ (with the Riesz-Markov theorem). I believe that this is true on completely regular spaces too, with a similar argument and by endowing $C_b (X)$ with the strict topology.","['borel-sets', 'probability-theory', 'functional-analysis', 'measure-theory', 'general-topology']"
2712225,Is there any subset in $\mathbb{R^2}$ such that...,"The intersection with every line L in $\mathbb{R^2}$ is open in $L$ with the topology $\mathcal{T}_L = \{G\cap L : G \text{ open in } \mathbb{R^2} \text{ with the Euclidean Topology } \} $, but the set is not open in $\mathbb{R^2}$ with the Euclidean Topology ? I couldnt find and such set but am pretty sure that there exist such a set , if you have any ideas let me know ! thanks in advance !",['general-topology']
2712245,Chance of three same consecutive answers in A/B/C/D test,"I am facing the following problem: What is the probability that there will be at least three same consecutive right answers in a $n$-question A/B/C/D test? I have started with $10$ questions while trying to solve this, so there are $4^{10}$ possible sequences. Now our three same results can begin at the first through the eighth place ($n-2$) because at the ninth place it would be $9-10-11$. For each of these possibilities there can be $4^{10-3} = 4^7$ combinations of $7$ remaining positions. So that's $8 \cdot 4^7$, and now we have to multiply by $4$ because there can be three or more As, Bs, Cs or Ds. That gives us $$\frac{8 \cdot 4^7 \cdot 4}{4^{10}}$$ which simplifies to $8/4^2 = 1/2 = 50\%$. I have checked the result practically with Python code with $100,000$ attempts, and I got about $38514/100000 = 0.38514 = 38.514\%$, which is not even remotely close to my result. Do you have any idea where I went wrong?","['combinatorics', 'probability']"
2712256,"Given finite field projections of a rational, can I find the original rational?","If you don't know a particular integer $a$, but you know several residues ($r_i$) of $a$ modulo several mutually prime integers ($m_i$), you can use the Chinese Remainder Theorem to find $r$ such that $a = r + k\prod{m_i}$, varying over $k$, with $0 \le r < \prod{m_i}$.  If you know $0 \le a < \prod{m_i}$, then you have $a = r$. Every rational number $a/b$ can be projected onto  a prime finite field $\mathbb{Z}/\mathbb{Z}_p$ (given that $b$ is invertible, i.e. $\gcd(p,b) = 1)$, for many $p$. Given a collection of these projections, $i$ pairs $(r_i, p_i)$ where $a/b \equiv r_i \mod p_i$, is there some process similar to the Chinese Remainder Theorem by which you can take that collection of pairs and work backwards to find $a/b$? For example, given the list of pairs $[(2,3), (3,5), (6,11), (500000004, 1000000007)]$, can you work backwards to find $1/2$? I realize that there will always be an infinity of integer and rational solutions for every collection of residues, but there will always be a smallest solution corresponding to the $0 \le r < \prod{m_i}$ above, perhaps a $a/b$ minimizing $|ab|$ or $a^2b^2$, and I would like to find that. If you need to, add in the ability to generate new residues for arbitrary given primes instead of working from a fixed input collection of pairs.  These rational numbers I'm trying to solve for
are solutions to some large linear equations, and these equations are convenient to solve over ""small"" finite fields ($p \le 2^{32}$), but inconvenient to solve over infinite-precision rationals.","['diophantine-equations', 'finite-fields', 'chinese-remainder-theorem', 'linear-algebra']"
2712272,Irreducible closed subsets of a scheme corresponds to points (Jim's solution),"I have a question concerning Jim's answer to the 1-1 correspondence ofirreducible closed subsets of a scheme and points. Specifically, I don't know how to verify the following statement: Now $U \cap Z$ has a unique generic point when $U$ is affine.  Show that if $V$ is also affine and $V \cap Z$ is nonempty then the generic point of $U \cap Z$ lies in $V$ and hence equals the unique generic point of $V \cap Z$. especially why "" the generic point of $U \cap Z$ lies in $V$ "" implies that the generic point of $U\cap Z$ "" hence equals the unique generic point of $V \cap Z$ ""","['general-topology', 'algebraic-geometry']"
2712285,"How can I prove reflexive closure preserves symmetry, anti-symmetry, and transitivity?","Let R be a relation in A. Let R' be the reflexive closure of R. IE in the graph of R, each possible self loop is completed, yielding R'. How can I prove that this operation on the graph will preserve all three properties above in all cases? Is there a way of doing this in a graph? Would a prose explanation suffice, as in by definitions? Furthermore how would I do the same for a symmetric closure on a reflexive relation?","['relations', 'discrete-mathematics']"
2712293,"Why is this an incorrect solution to ""Find how many $4$-digit numbers are divisible by $5$.""?","Problem: Find how many 4-digit numbers are divisible by 5. I started from two facts: The number divisible by 5 ends with either 0 or 5. The 4-digit number can't start with 0. Then I calculated how many permutations end with 5 or 0: since there are 4 placeholders, if I put in the last one 0 or 5, there are three left to be filled with any digit, therefore $2 \cdot 10^3=2000$ (I multiplied by two because if the number ends in either $0$ or $5$, there are $3$ placeholders in case of both $0$ and $5$). So, there are in total $2000$ permutations that end in $0$ or $5$. Then I calculated how many of them start with 0: since, if we put 0 in the first place, there are 3 placeholders left, and each of them can hold up to 10 digits, and we arrive at the conclusion that there are $10^3=1000$ permutations that start with $0$. Then I subtracted from the number of permutations that end in $0$ or $5$ the number of permutations that start with $0$: $2000-1000=1000$. And then I concluded that there $1000$ $4$-digit numbers that are divisible by $5$. But the solution is actually $1800$, which is arrived at like this: $9 \cdot 10 \cdot 10 \cdot 2=1800$. $9 \cdot 10 \cdot 10$ comes from that that when the last placeholder is filled, there are three placeholders left, each of which can be filled with any of the $10$ digits, except the first one that can't contain $0$. It is multiplied by two for both cases: $5$ in the last placeholder or $0$ in the last placeholder. Why is the way that I tried incorrect?",['combinatorics']
2712336,Simplifying an equation involving trigonometric matrices,"I have two $n$-dimensional ODEs: $$\begin{cases} \ddot x + K_1 x = 0 & \text{for $t$ in $[0,t_1)$}\\
\ddot x + K_2 x = v & \text{for $t$ in $[t_1,t_1+t_2)$} \end{cases}$$ where $K_1, K_2$ are symmetric positive definite matrices with $K_2=K_1+e_ne_n^\top$ ($e_n=[0 \ \dots \ 0 \ 1]^\top$). I am looking for $(x_0, \dot x_0)$ and $t_1,t_2>0$ that lead to periodic solutions. I managed to find some numerical solutions, but I am wondering if the following system of equations can be further simplified. Since $K_1$, $K_2$ are spd, there have spd square roots $L_1$, $L_2$. On each of the two intervals, the solutions can be expressed as functions of $L_1$, $L_2$: $$\begin{bmatrix} x(t) \\ \dot x(t)\end{bmatrix} = S_1(t) \begin{bmatrix} x_0 \\ \dot x_0\end{bmatrix} \quad \text{for $t$ in $[0,t_1)$}$$ $$\begin{bmatrix} x(t) \\ \dot x(t)\end{bmatrix} = S_2(t) \begin{bmatrix} x_0 \\ \dot x_0\end{bmatrix} +\begin{bmatrix} -L_2^{-2}v \\ 0 \end{bmatrix}\quad \text{for $t$ in $[t_1,t_1+t_2)$}$$ with $$S_i(t)=\begin{bmatrix} \cos(tL_i) & t\operatorname{sinc}(tL_i) \\ -L_i\sin(tL_i) & \cos(tL_i)\end{bmatrix}\in\mathbb{R}^{2n\times 2n}.$$ The problem of finding periodic solutions is reduced to finding $X_0=(x_0,\dot x_0)\in\mathbb{R}^{2n}$ and $(t_1,t_2)$ such that: $$\boxed{(S_2(t_2)S_1(t_1)-I)X_0 = (S_2(t_2) -I)V}$$
with $V^\top = [-(L_2^{-2}v)^\top,\ 0]$. I add the following two equations: $X_0 e_n = 1$ and $S_1(t_1)X_0e_n=1$, so there are $2n$ unknowns and $2n$ equations. The question is, can this problem be simplified by the choice of a more appropriate basis? Remarks: The solutions I found numerically all satisfy $\det(S_2(t_2)S_1(t_1)-I)=0$, the reason is not clear to me. I don't use the fact that $K_2=K_1+e_ne_n^\top$; could it be taken advantage of?","['trigonometry', 'ordinary-differential-equations', 'linear-algebra', 'change-of-basis']"
2712407,Intuitive explanation of Poisson distribution,"I've seen the formula most commonly derived as a continuum generalization of a binomial random variable with large $n$, small $p$ and finite $\lambda = np$ yielding $$ \lim_{n \to \infty} \binom{n}{x} p^x(1-p)^{n-x} = e^{-\lambda}\frac{\lambda ^ x}{x!}$$ It follows, from this derivation, that $$ \lim_{n \to \infty } = (1-p)^{n-x} = e^{-\lambda}$$ yields the probability of failing infinitely many times when the success rate is $\lambda$. However, from this approach, I could not grok the remaining term $$\frac { \lambda ^ x } {x!} $$ Question What insightful derivations (perhaps, from generalizations) of the Poisson random variable exist which leaves an intuition for each of the terms? My Answer: My answer, https://math.stackexchange.com/a/2727388/338817 comes from geometric approach to Gamma function intuition ( https://math.stackexchange.com/a/1651961/338817 ) which I quote: Note that $\frac{t^n}{n!}$ is the volume of the set
  $S_t=\{(t_1,t_2,\dots,t_n)\in\mathbb R^{n}\mid t_i\geq 0\text{ and } t_1+t_2+\cdots+t_n\leq t\}$","['probability-distributions', 'poisson-distribution', 'gamma-function', 'geometry']"
2712415,How to cut an irregular shape into 2 congruent parts,"Is it possible to cut this shape: into 2 congruent parts (equal area and shape). The guy who gave us this teaser said that it's possible. But i can't for the life of me figure out how. In the figure, $ABCD$ is a square, angle $CDE$ is a right angle, and the curve $AE$ is a circular arc with center $C$.","['congruences-geometry', 'geometry']"
2712427,Are there matrices such that $(AB-BA)^{71}=I_{69}$?,"Are there matrices $A,B \in \mathcal{M}_{69}(\mathbb{C})$ such that $$(AB-BA)^{71}=I_{69}?$$
  Here $I_{69}$ denotes the $69 \times 69$ matrix with $1$ on its main diagonal and $0$ everywhere else. My strong guess is that there are not. Let $C=AB-BA$. Then $\text{tr }C=0$ and $\text{tr }C^{71}=69$. If $\lambda$ is an eigenvalue of $C$, then $\lambda^{71}=1$. So \begin{align*}
\lambda_1+\lambda_2+\dots+\lambda_{69}=0 \\
\lambda_1^{71}+\lambda_2^{71}+\dots+\lambda_{69}^{71}=69
\end{align*}
I think some contradiction may come from here, but I don't see it yet. I also tried working with polynomials. If $p=X^{71}-1$, then $p(C)=0$. If we worked in $\mathcal{M}_{69}(\mathbb{Q})$ it would have been easier because the minimal polynomial of $C$ divides $p$ and since $(X-1)$ is the only irreductible factor of $p$ with its degree less or equal than $69$, we would have that the minimal polynomial of $C$ is $(X-1)$ and so $C=I_{69}$, which leads to a contradiction after applying trace.","['eigenvalues-eigenvectors', 'matrices', 'minimal-polynomials', 'trace', 'linear-algebra']"
2712442,"Proof of $\int_Ef\,dm=\int_Eg\,dm\implies f,\,g$ equal almost everywhere.","Consider the following problem, in particular, the second implication : ""Prove that $f$ and $g$ are equal almost everywhere if and only if $\int_Ef\,dm=\int_Eg\,dm$ for every measurable set $E$."" My problem: I've looked at the solution I've been given to the second implication, but I don't see how it actually shows what I'm trying to prove, and I would like help being convinced of it. Here is an outline of the procedure employed in the solutions. Start by assuming that $f\neq g$ on a set of positive measure, say $F\subset \mathbb R$. Then, $F=\{x:f(x)\neq g(x)\}$. As far as I can see, this is tantamount to assuming that $f$ is not equal to $g$ almost everywhere. Then you consider $F$ as $F=\{x:f(x)\gt g(x)\}\cup\{x:f(x)\lt g(x)\}$, and label each as $F_1$ and $F_2$ respectively. It then follows, since we have assumed $F$ to have positive measure, that either $F_1$ or $F_2$ has positive measure. Without loss of generality, we take $F_1$ to have positive measure. This means that $\forall x\in F_1,\,f(x)-g(x)\gt0$. We move then to compute $\int_{F_1}(f-g)dm$. To determine this integral, we use the fact that for a measurable subset $E\subset\mathbb R$ of positive measure and $f\in\mathcal L(\mathbb R^n,m),$ where $f$ is strictly positive on $E$,then $\int_Ef\,dm>0$. We apply this here to see that $\int_{F_1}(f-g)dm>0$, and then, using the properties of the Lebesgue integral, are able to draw that $\int_{F_1}f\,dm\neq\int_{F_1}g\,dm$. Now I understand all the little facets used throughout the proof, but I just don't see how it shows the second implication of the problem, namely, that if $\int_Ef\,dm=\int_Eg\,dm$ for every measurable set $E$ then $f$ and $g$ are equal almost everywhere, is true? How is this reasoning sufficient to deduce this? Are there any final details that I have missed that might better tie this all together?","['real-analysis', 'lebesgue-measure', 'functional-analysis', 'measure-theory', 'analysis']"
2712449,What is the name of the shape formed by a semicircle attached to a rectangle?,"I will be asking several questions like this. I have to figure out the names of the shapes of numerous parts that we make.  I have looked, but the internet has failed me for several shapes. Here's the first one: It's basically a rectangle with a semicircle on top of it. Don't know if it would have a different name if it was a square instead of a rectangle, but it really doesn't matter. Thank you very much for all your help and all your help to come for my other shape questions.","['terminology', 'geometry']"
2712539,Integral with respect to random measure is measurable,"Let $(\Omega, \mathcal{F})$ be a measurable space and $P$ be a random, $\mathcal{G}$-measurable finite measure on $(\Omega, \mathcal{F})$, with $\mathcal{G} \subseteq \mathcal{F}$. Is the following proposition true? Is my proof correct? Proposition . The function $f(\omega) = \int g dP(\omega)$ is $\mathcal{G}$-measurable for all bounded, $\mathcal{F}$-measurable functions $g$. Proof . The proposition holds if $g$ is the indicator function of $A \in \mathcal{F}$ simply because $P(A)(\omega)$ is $\mathcal{G}$-measurable in $\omega$ by definition. Since the bounded $\mathcal{G}$-measurable functions are closed under linear combinations and uniform limits, the entire proposition follows. Is that sufficient? I'm somehow not fully confident about the last sentence of the proof. Should I add that the bounded convergence theorem is used for the limit step?","['real-analysis', 'proof-verification', 'integration', 'measure-theory', 'random-functions']"
2712577,"Are continuous maps ""weaker"" than other morphisms?","The property of continuity (and hence smoothness) seems weaker than the properties of other morphisms, in the sense that a homeomorphism is a ""continuous bijection whose inverse is continuous"". In every other morphism type, the marking quality of the morphism is guaranteed for the inverse. An isomorphism of vector spaces is ""a bijective linear map"", I don't need to verify that the inverse is linear. An isomorphism of groups is ""a bijective map that preserves group structure"", I don't need to verify that the inverse preserves group structure. An isomorphism of rings is a ""bijective map that preserves ring structure"", I don't need to verify that the inverse preserves ring structure. There seems to be a trend that the bijective morphisms of ""algebraic"" categories seem to be guaranteed an inverse which is also a morphism, while in ""topological"" categories, that's not the case. Is there an interesting explanation for this? Thank you","['morphism', 'group-isomorphism', 'continuity', 'category-theory', 'general-topology']"
2712585,Find the mass of the parallelepipid,"""A parallelepiped is described by the vectors $(2,2,3),(2,4,3)$ and $(0,1,5)$ given that the density $= 2x+2y$, find the mass of the parallelepiped."" I can find the volume just fine, but setting up the integral to find the mass is giving me a lot of trouble. this is my best guess so far... $$20 \int_0^2  \int_0^4 \int_0^5  (2x+2y) dzdxdy$$ The vectors do stem from the origin",['multivariable-calculus']
2712632,Is a module that is isomorphic to its dual necessarily free?,"Suppose $M$ is a finitely generated module over an integral domain $R$. If there is a bilinear form $\langle-,-\rangle:M\times M\rightarrow R$ which induces an isomorphism $M\rightarrow \text{Hom}_R(M,R)$ via $m\mapsto \langle m,-\rangle$, is it possible to conclude that $M$ is a free $R$-module?","['abstract-algebra', 'linear-algebra']"
2712686,Boundary Value Problem: $u'' + u-u^3 = 0$,"Consider the differential equation $$\frac{d^2u}{dx^2} + u − u^3 = 0$$ where $u'(0) = u(L) = 0$ . If there is a solution which isn’t identically $0$ which satisfies $u(0) = u_0$, then find a relationship between $L$ and $u_0$. I derived, $$L =\int_0^1 \frac{dz}{\sqrt{(1-z^2)-(2/4)u_{0}(1-z^4)}}$$ where $z=u/u_{0}$ and $dz = du/u_{0}$ I was wondering if I was able to/how to approximate the minimal value of $L$ for which a non-zero solution exists.","['stability-in-odes', 'ordinary-differential-equations', 'mathematical-modeling']"
2712767,Generalization of the Friendship Theorem in graphs?,"If in a graph, any two vertices have an odd number of common neighbors, is it true that, there exists a vertex which is a 'universal friend', i.e adjacent to every other vertex? (It is known that in that case the number of vertices has to be odd: Prove that any graph G with an even number of vertices has two vertices with an even number of common neighbour ) If each of the odd numbers in question were 1, then this is the statement of the Friendship theorem .","['combinatorics', 'graph-theory', 'discrete-mathematics']"
2712815,Not-too-slow computation of Euler products / singular series,"I'd like to compute, to at least a few digits of accuracy, the constants that arise in Hardy-Littlewood conjecture F / Bateman-Horn conjecture , in particular for just a single quadratic polynomial. Specifically, consider (for instance) the following number, which arises when studying primes in the polynomial $n^2 + n + 1$: $$
\begin{align}
C_{1}
&= \frac{2}{1} \frac{2}{2} \frac{5}{4} \frac{5}{6} \frac{11}{10} \frac{11}{12} \frac{17}{16} \frac{17}{18} \frac{23}{22} \frac{29}{28} \frac{29}{30} \frac{35}{36} \frac{41}{40} \cdots \\
&= 
\prod_{p \equiv 1 \pmod3}\left(\frac{p - 2}{p - 1}\right)
\prod_{p \equiv 2 \pmod3}\left(\frac{p}{p - 1}\right) \\
&= 
\prod_{p \equiv 1 \pmod3}\left(\frac{1 - 2/p}{1 - 1/p}\right)
\prod_{p \equiv 2 \pmod3}\left(\frac{1}{1 - 1/p}\right) \\
\end{align}$$
where the products run over all primes that leave remainder $1$ and $2$ when divided by $3$, respectively. (Note: we cannot really write it as two separate infinite products; the order matters, so excuse the notation.) Now, if I try to compute this the naive way: #include <cstdio>
#include ""FJ64_16k.h"" // For a fast `is_prime` function: http://ceur-ws.org/Vol-1326/020-Forisek.pdf

int main() {
  double ans = 0.5;
  for (uint64_t p = 0; ; ++p) {
    if (p % 1000000 == 0) {
      printf(""%lld %.9f\n"", p, ans);
    }
    if (!is_prime(p)) continue;
    if (p % 3 == 1) ans *= (p - 2.0) / (p - 1.0);
    if (p % 3 == 2) ans *= p / (p - 1.0);
  }
} Then even after computing with primes up to 100 million, we seem to have only have about four decimal digits of accuracy: 99000000 1.120724721
100000000 1.120725012
101000000 1.120727310 For another example, if we start with the polynomial $n^2 + 5n + 1$ (discriminant $21$), then we need to compute (something like) the constant
$$C_2 = 
\prod_{p \in P_1 \cap Q_1 \cup P_2 \cap Q_2} \frac{(p-2)}{(p-1)} 
\prod_{p \in P_1 \cap Q_2 \cup P_2 \cap Q_1} \frac{p}{(p-1)} $$
where $P_1 = \{p \equiv 1 \pmod 3\}$, $P_2 = \{p \equiv 2 \pmod 3\}$, $Q_1 = \{p \equiv \text{$1$, $2$, or $4$}\pmod 7\}$, $Q_2 = \{p \equiv \text{$3$, $5$ or $6$}\pmod 7\}$
which is messier: we do one thing for $p \equiv 1, 4, 5, 16, 17, 20\pmod 21$ and another for $p \equiv 2, 8, 10, 11, 13, 19$. Question : How can we compute these constants reasonably quickly? I have seen the related questions on MathOverflow, and tried to read their answers and some of the mentioned references: Calculating the infinite product from the Hardy-Littlewood Conjecture F (2010) Calculating the constant in the Bateman-Horn-Stemmler conjecture (2011) High precision computation of Hardy-Littlewood constants ( DVI ) by Henri Cohen Some Constants from Number Theory (2001?) ( PS ) by Pascal Sebah and Xavier Gourdon Approximation of singular series and automata (2001), by Pieter Moree New quadratic polynomials with high densities of prime values (2003), by Jacobson and Williams Quadratic Polynomials which have a High Density of Prime Values (1990) by Fung and Williams On the Conjecture of Hardy & Littlewood concerning the Number of Primes of the $n^2 + a$ (1960), by Shanks Supplementary data and remarks concerning a Hardy-Littlewood conjecture (1963), by Shanks The calculation of certain Dirichlet series (1963), by Shanks and Wrench Note on the number of primes of the form $n^2 + 1$ (1922) by A. E. Western Some problems of ‘Partitio numerorum’; III: On the expression of a number as a sum of primes (1923), Hardy and Littlewood (the source of Conjecture F) But the papers take quite a lot of effort for me to follow (I've never encountered Dirichlet L-series before, for example), and I keep suspecting there should be something simpler, if we don't want so much. (The older papers are simpler, but Western's paper simply says “by the use of a transformation suggested by Mr. Littlewood” without a reference or explaining anything.) In particular, I don't care about getting thousands of digits, I'll be happy if I can get, say six (ten would be fantastic). (Or even 4 digits, if they can be computed a lot faster.) And I don't care about computing such series that arise out of really large numbers; like the $3$ and $7$ in the second example above I might have at most two-digit factors. With these looser constraints, is there something simpler that suffices, to compute these constants? What I'm looking for is either: ideally, a method of computation with sufficient detail, and which can be translated into a few lines of code that compute it from scratch (as in the program above) (after some reasonable amount of work on paper if necessary), or, the same, but it's ok to rely on certain constants (like $\zeta(2)$ say) that I can just look up and hard-code into the program, or on some existing functions (such as the logarithmic integral say), if they are well-known functions commonly available (in software libraries that are easy to install or have an online interface). I'm looking for an elementary exposition at undergraduate level, say. For instance, the answer by KConrad makes sense (though it's not clear how good the convergence is), but I don't know how to compute $L(1,\chi_D)$.","['analytic-number-theory', 'dirichlet-series', 'number-theory', 'computational-mathematics', 'euler-product']"
2712861,Open and connected subset in $\mathbb{R}^2$,"If $U$ is an open and connected subset in $\mathbb{R}^2$, then it is path connected. In further, we assume that $U$ is in a unit ball. If its complement $U^c$ is connected, then show that $U$ is simply connected. [Add] Definition : $X$ is simply connected if there is a continuous map $h: D^2\rightarrow X$ with $h|\partial D^2=c$ when $c: S^1\rightarrow X$ is continuous map.","['complex-analysis', 'general-topology']"
2712874,Prove that $f'(x) = \lim_{h\to 0^+ \\k\to 0^+} \frac{f(x+h) - f(x-k)}{h+k}$,"In the book of Calculus by Spivak, at page 152, question 22, it is asked that Suppose that $f$ is differentiable at $x$. Prove that $$f'(x) = \lim_{h\to 0^+ \\k\to 0^+} \frac{f(x+h) - f(x-k)}{h+k}$$ I have tried some algebraic trics & take a look at this , question, but couldn't find the exact derivation, so I would appreciate any help or hint. I mean it is clear that this is a more general case of 
$$f'(x) = \lim_{h\to 0} \frac{f(x+h) - f(x-h)}{2h},$$
and we are going to use similar ideas.","['epsilon-delta', 'real-analysis', 'calculus', 'limits']"
2712906,Does $\mathcal{B}(\mathcal{H})=\mathcal{H}\otimes\mathcal{H}^*$ in infinite dimensions?,"In quantum information contexts (where in most cases finite dimensions are considered) I have often seen the statement that the space of bounded linear operators $\mathcal{B}(\mathcal{H})$ on a Hilbert space is (isomorphic to) the tensor product of the Hilbert space $\mathcal{H}$ with its dual $H^*$ (space of linear functionals). Is this only true in finite dimensions (where all operators are bounded anyway) or does it also apply to infinite dimensions? May I ask for a not too-technical answer without a lot of math jargon (I am a physicist, my apologies!)","['functional-analysis', 'tensor-products', 'quantum-information', 'hilbert-spaces']"
2712922,Poincaré Duality and Intersection Pairing,"Suppose $M$ is an oriented compact $n$-dimensional manifold, then Poincaré duality is an isomorphism
\begin{equation}
\alpha  \in H^i(M,\mathbb{Z}) \mapsto [M] \cap \alpha  \in H_{n-i}(M,\mathbb{Z}),~ \alpha
\end{equation} 
where $[M]$ is the homology class associated to the manifold $M$. For details, see e.g. Algebraic Topology by Allen Hatcher. In Griffiths and Harris' book Principles of Algebraic Geometry, the intersection of two homology classes $A$ and $B$ is defined, and if we let $A^\vee$ be the Poincaré dual of $A$, etc, then there is 
\begin{equation}
A^\vee \cup B^\vee =(A\cap B)^{\vee}
\end{equation} I am wondering whether there are interesting explanations of this equality or interesting examples which could show the essence of it?","['algebraic-topology', 'differential-geometry']"
2712923,Relationship between Borel Sigma algebras,"In general, is it true that $\mathcal{B}(\mathbb{R^n})=(\mathcal{B}(\mathbb{R}))^n:= \mathcal{B}(\mathbb{R}) \times \mathcal{B}(\mathbb{R})\times \dots \times\mathcal{B}(\mathbb{R})$? I'd have to say yes, since we have $U_1,U_2\dots U_n$ open sets in $\mathbb{R}$ then their product is open in $\mathbb{R^n}$, so I'd suppose that an argument involving the sigma algebra generated by product of open sets and and the sigma algebra of all open sets in $\mathbb{R^n} $ should be the same. I'd appreciate some help in formalizing the argument though Thanks","['borel-sets', 'lebesgue-measure', 'measure-theory']"
2712932,"If $A,B$ Hermitian and $A-B$ has only nonnegative eigenvalues, show that $\lambda_i(A)\geq\lambda_i(B)$","Denote the sorted eigenvalues of an $n\times n$ matrix $C$ as $\lambda_1(C)\leq\dots\leq\lambda_n(C)$. I want to show that the following statement holds for all $i=1,\dots,n$ If $A,B$ Hermitian and $A-B$ has only nonnegative eigenvalues, show that $\lambda_i(A)\geq\lambda_i(B)$ Let me state Weyl's Theorem (Thm. 4.3.1 in Horn and Johnson's Matrix Analysis book) Let $A,B\in\mathbb{C}^{n\times n}$ be Hermitian and let the respective eigenvalues of $A$, $B$, and $A+B$ be $\{\lambda_i(A)\}_{i=1}^n$, $\{\lambda_i(B)\}_{i=1}^n$, and $\{\lambda_i(A+B)\}_{i=1}^n$, each algebraicly ordered as explained above (i.e. $\lambda_1(\dots)\leq\dots\leq\lambda_n(\dots)$). Then
  $$\lambda_i(A+B)\leq\lambda_{i+j}(A)+\lambda_{n-j}(B),\qquad j=0,\dots,n-i$$
  for each $i=1,\dots,n$.
  Also,
  $$\lambda_{i-j+1}(A)+\lambda_{j}(B)\leq\lambda_i(A+B),\qquad j=1,\dots,i$$
  for each $i=1,\dots,n$. Let $\lambda_1'(B')\leq\dots\leq\lambda_n'(B')$ denote the sorted eigenvalues of $B'=-B$. First, since $B$ Hermitian, then $B'$ is also Hermitian and if $(\lambda, \mathbf{x})$ is an eigenpair of $B$, then $(-\lambda, \mathbf{x})$ is an eigenpair of $B'$. Hence, if $\lambda_1(B)\leq\dots\leq\lambda_n(B)$ are the sorted eigenvalues of $B$, then $\lambda_1'(B')=-\lambda_n(B)\leq\dots\leq\lambda_n'(B')=-\lambda_1(B)$ are the sorted eigenvalues of $B'$. Let's apply the first part of the theorem for matrices $A,B'$ for $j=0$
\begin{eqnarray*}
\lambda_i(A+B')\leq\lambda_{i+j}(A)+\lambda_{n-j}'(B')&\Rightarrow&\lambda_{i}(A)+\lambda_{n}'(B')\geq\lambda_i(A+B')\\
&\Rightarrow&\lambda_{i}(A)+\lambda_{n}'(B')\geq0\qquad (A-B\text{ has nonnegative eigenvalues})\\
&\Rightarrow&\lambda_{i}(A)+\lambda_{n}'(-B)\geq0\\
&\Rightarrow&\lambda_{i}(A)-\lambda_{1}(B)\geq0\\
&\Rightarrow&\lambda_{i}(A)\geq\lambda_{1}(B)
\end{eqnarray*} But, this only shows that $\lambda_{1}(A)\geq\lambda_{1}(B)$ and I need $\lambda_{i}(A)\geq\lambda_{i}(B)$ for all values of $i$. What else can I do? I think I need to show that $\lambda_{1}(A)\geq\lambda_{n}(B)$ which would be enough.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2712956,Equation $x^{\frac{n+1}{n}}=x+1$,"Let $n$ be a positive integer. What is the positive value of $x$ such that $x^{\frac{n+1}{n}}=x+1$? This equation has a unique solution because the function $x^{\frac{n+1}{n}}-x$ is increasing. However, I'm not sure if we can get a closed form for $x$. If not, how fast does $x_n$, the solution for $n$, grow asymptotically in terms of $n$?",['algebra-precalculus']
2712985,Simple L^2 bound for bivariate Sobolev function on a square,"I have a rather basic question about Sobolev functions. I would need a reference or proof for the following inequality which seems to be well-known in approximation theory. Question: Let $\Omega=[x,x+h]\times[y,y+h]$ be a square of side-length $h$ and let $f\in H^s(\Omega)$ be a Sobolev funtion of regularity $s\in(1,2)$ such that $f=0$ on the vertices of $\Omega$. Does it hold that
$$\|f\|^2_{L^2(\Omega)}\leq C h^{2s}\|f\|^2_{H^s(\Omega)}?$$ I would like to use such a bound to get the rate of the approximation error of a function on $[0,1]^2$ by its piecewise linearly interpolated counterpart on a grid of size $h$, which explains the assumptions of roots on the vertices of the grid. Here is my argument for the univariate case: Let $I=[x,x+h]$ and $g:I\to R$ such that $g(x)=g(x+h)=0$. Assume first that $g\in H^1(I)$. Then weak differentiability, $g(x)=0$ and Cauchy-Schwarz imply
\begin{align}\|g\|^2_{L^2(I)}&=\int^{x+h}_xg(t)^2dt=\int^{x+h}_x\Big(g(x)+\int^t_xg'(s)ds\Big)^2dt\\
&\leq\int^{x+h}_x(t-x)\int^t_xg'(s)^2dsdt\leq\int^{x+h}_x(t-x)dt\int^{x+h}_xg'(s)^2ds\\
&=\frac{1}{2}h^2\|g'\|^2_{L^2(I)}\leq\frac{1}{2}h^2\|g\|^2_{H^1(I)}.
\end{align} Assume now that $g\in H^2(I)$. Then additionally using that there is some $x_0\in I$ with $g'(x_0)=0$ (since $g$ has to have an extremum on $I$, by $g(x)=g(x+h)=0$) and applying Cauchy-Schwarz twice yields
\begin{align}\|g\|^2_{L^2(I)}&=\int^{x+h}_xg(t)^2dt=\int^{x+h}_x\Big(g(x)+\int^t_xg'(s)ds\Big)^2dt\\
&=\int^{x+h}_x\Big(g(x)+\int^t_x\Big(g'(x_0)+\int^s_{x_0}g''(u)du\Big)ds\Big)^2dt\\
&=\int^{x+h}_x\Big(\int^t_x\Big(\int^s_{x_0}g''(u)du\Big)ds\Big)^2dt\\
&\leq\int^{x+h}_x(t-x)\int^t_x(s-x_0)\int^s_{x_0}g''(u)^2dudsdt\\
&\leq\int^{x+h}_x(t-x)\int^t_x(s-x)dsdt\int^{x+h}_xg''(u)^2du\\
&=\frac{1}{8}h^4\|g''\|^2_{L^2(I)}\leq\frac{1}{8}h^4\|g\|^2_{H^2(I)}.
\end{align}
Now an interpolation argument gives for $g\in H^s(I),s\in(1,2)$, the inequality
$$\|f\|^2_{L^2(I)}\leq Ch^{2s}\|f\|^2_{H^s(I)}.$$
For the bivariate case I have a few problems. For instance if $f\in H^1(\Omega)$ the point evaluations are not necessarily well-defined since the approximated function might not be continuous. But even if I would assume that I do not have to worry about that the same approach would give me (using $f(x)=0$)
$$\|f\|^2_{L^2(\Omega)}=\int_{\Omega}\Big(\int^1_0\langle\nabla f(t+u(t-x)),t-x\rangle\Big)^2dudt$$
and I am stuck at this point. For $f\in H^2(\Omega)$ I found a bound in this paper (by the proof of Lemma 1). Has anyone an idea or knows some helpful literature? Thank you!","['derivatives', 'integral-inequality', 'multivariable-calculus', 'approximation-theory', 'sobolev-spaces']"
2712986,Find the number of ways in which 6 persons out of 5 men and 5 women can be seated at a round table such that 2 men are never together.,"Find the number of ways in which 6 persons out of 5 men and 5 women can be seated at a round table such that 2 men are never together. My attempt: 6 people may be 3 men and 3 women, 2 men and 4 women or 1 man and 5 women.
Then by the gap method, 3 men and 3 women can be seated in $\binom{5}{3}2!3!=120$ ways. 2 men and 4 women can be seated in $\binom{5}{4}3!\frac{4!}{2!}=360$ ways 1 men and 5 women can be seated in $\binom{5}{5}4!\frac{5!}{4!}=120$ ways So the total number of ways is 600, but the answer given in my book is 5400. Where did I go wrong?",['combinatorics']
2713012,Can the boundaries of two pentagons intersect at $20$ points?,"This question is a follow-up to Maximum number of intersections between a quadrilateral and a pentagon , where it is shown that the boundaries $\partial Q,\partial P$ of a quadrilateral and a pentagon in the plane cannot intersect at more than $16$ points, since each side of $\partial Q$  meets $\partial P$ at an even number of points. Q: Given the boundaries $\partial P_1, \partial P_2$ of two pentagons in
   the plane, is it possible that $$ \left|\partial P_1 \cap \partial P_2
 \right| = 20?$$ Each side of $\partial P_1$ meets $\partial P_2$ at an even number of points, so equality is attained iff there is some configuration such that each side of $\partial P_1$ meets each side of $\partial P_2$ except one. $\left|\partial P_1 \cap \partial P_2\right| = 18$ is possible, as shown below, and I believe that $\left|\partial P_1 \cap \partial P_2\right| = 20$ is im possible, but I am failing to prove it.","['combinatorics', 'intersection-theory', 'curves']"
2713020,Why is the Monotone Convergence Theorem more famous than it's stronger cousin?,"I am reading Stein & Shakarchi. On page 62 we have the Monotone Convergence Theorem: Suppose $\{f_n\}$ is a sequence of non-negative measurable functions with $f_n\nearrow f.$ Then $\displaystyle \lim_{n \to \infty} \int f_n = \int f$. However, just before this theorem we have this much more powerful corrolary of Fatou's lemma: Suppose $f$ is a non-negative measurable function, and $\{f_n\}$ a sequence of non-negative measurable functions with $f_n(x) \le f(x)$ and $f_n(x) \to f(x)$ for a.e. $x$. Then $\displaystyle \lim_{n \to \infty} \int f_n = \int f$. To me, this second corollary seems strictly better than the Monotone Convergence Theorem, yet it is the M.C.T. that has a name and is used often. Am I misunderstanding the theorems, or is there a reason why the M.C.T. is more popular? Does this corollary have a name?","['real-analysis', 'lebesgue-integral', 'measure-theory', 'convergence-divergence', 'analysis']"
2713037,what's $Df(A)(X)$ if $f(A) = \det(A)$?,"let's begin with the simple case where $A$ is just a $2\times2$ matrix let $\begin{align} f :& \mathbb{R^{2\times2}} \to \mathbb{R} \\
& A \mapsto \det(A)
\end{align}$ I want to find the differential of this mapping if $A$ is invertible. as a hint I was suggested to compute the following limit : $\lim_{t \to 0} \frac1t [\det(I+tX) -1]$ where $X \in \mathbb{R^{2\times2}}$ the limit turns out to be just the trace of $X$ and since $\det(I) = 1 $ we have that $Df(I)(X) = Tr(X)$, right ? so I guess now that if I want to find $Df(A)(X)$ for $A$ invertible I have to compute this limit : $\lim_{t \to 0} \frac1t [\det(A+tX) -\det(A)] = \det(A)\lim_{t \to 0} \frac1t [\det(I+tA^{-1}X) -1] $ so $Df(A)(X) = \det(A)Tr(A^{-1}X)$, right ? now in higher dimensions the last step wouldn't change and I guess that in the first limit the expression $[\det(I+tX) -1]$ would be something of the form $tTr(X) +t^2(\cdots) + t^3(\cdots)+\cdots$ so it's all cool but what if $A$ is not invertible ? the $\det$ being some sort of a polynomial would still be differentiable, right ? but how do you construct the differential in this case ? Edit : my bad if $A$ is not invertible then $\det(A) = 0$ so I guess $Df(A)(X) = 0$ ? can someone confirm this ? Edit 2 : at the end it all comes down to evaluating this : $$\lim_{t \to 0 } \frac1t \det(A+tX)$$ for $A$ non-invertible and $X \in \mathbb{R^{n\times n}}$","['multivariable-calculus', 'differential-geometry']"
2713061,Jacobian of a quaternion rotation wrt the quaternion,"I am trying to implement an extended Kalman filter which takes a vector as a sensor measurement. To model this I need to rotate the vector to the satellite reference frame using quaternion rotation. For the filter I need to find the Jacobian of my measurement function. I would like to calculate the Jacobian of some function h which performs a passive quaternion rotation, where q is my quaternion and p is some vector: $h(q) = q p q^{-1}$ I'd like to find: $H = \frac{\partial h(q)}{\partial q}$ I'm using unit quaternions in the form $q = [w, \vec{v}]$ Many thanks.","['derivatives', 'quaternions']"
2713080,Events in Tail $\sigma$-algebra,"Let $X_1,X_2,...$ be real valued random variables. Put $\mathfrak S_n=\sigma (X_n)$ and $S_n=X_1+X_2+...+X_n$ . Let $\mathfrak T_n=\sigma (X_{n+1},X_{n+2},...)$ and define the tail $\sigma$ -algebra $\mathfrak T=\cap _n \mathfrak T_n$ . The book I am reading now says : $\{\limsup S_n>b\}\notin \mathfrak T$ But I don't get why this holds. Intuitively, this event seems to be unaffected by first finite happenings... Could you give me an example or explanation? Thank you in advance. Edit: For example, let $X_n =^{dist} U_{(-1,1)}$ for all $n$ , assuming that they are not independent and let $b=0$ . Is is still true that $\{\limsup S_n>b\}\notin \mathfrak T$ ?","['stochastic-processes', 'real-analysis', 'probability-theory', 'measure-theory', 'sequences-and-series']"
2713083,Why $\int\limits_{\sin(-5π/12)}^{\sin(5π/12)}\frac{dx}{1-x^2}=\ln\frac{1+\sin\frac{5π}{12}}{1-\sin\frac{5π}{12}}$?,"Why does an integral $$\int \frac{dx}{1-x^2}$$ with the limitless (undefined) interval equal to $$\frac 12\ln\frac{1+x}{1-x},$$ yet an integral $$\int\limits_{\sin(-5π/12)}^{\sin(5π/12)}\frac{dx}{1-x^2}$$ with an interval from $\sin\frac{-5π}{12}$ to $\sin\frac{5π}{12}$ has $$\ln\frac{1+\sin\frac{5π}{12}}{1-\sin\frac{5π}{12}}$$ without one half attached to ln?","['algebra-precalculus', 'definite-integrals']"
2713141,Limit of Integral over Any Measurable Subset Exists,"Let $\{f_n\}$ be a sequence of nonnegative measurable functions on $\Bbb{R}$ that converges pointwise on $\Bbb{R}$ to $f$ and $f$ be integrable over $\Bbb{R}$. Show that $$\mbox{If } \int_\Bbb{R} f = \lim_{n \to \infty} \int_\Bbb{R} f_n, \mbox{ then } \int_E f = \lim_{n \to \infty} \int_E f_n$$
  for any measurable set $E \subseteq \Bbb{R}$. I found a solution to this problem here (see the third and fourth pages), but I don't understand the following line from the proof: $$\int_{\Bbb{R} - E} f \le \liminf_{n} \int_{\Bbb{R} - E} f_n = \int_\Bbb{R}f - \limsup_n \int_E f_n$$ Is the equality valid? The only way I could see to justify this step was by noting that $\int_\Bbb{R} f_n = \int_E f_n + \int_{\Bbb{R} - E} f_n$ or $\int_{\Bbb{R} - E} f_n = \int_{\Bbb{R}} f_n - \int_E f_n$, and then $$\liminf_{n} \int_{\Bbb{R} - E} f_n = \liminf_n (\int_{\Bbb{R}} f_n - \int_E f_n) = \int_\Bbb{R} f - \limsup_n \int_E f_n$$ But would this not be fallacious, as we do not know whether the $f_n$ are integrable and therefore cannot subtract those integrals, which are potentially infinite, as we see fit? Is this solution incorrect, or am I misunderstanding something?","['real-analysis', 'measure-theory', 'proof-explanation']"
2713201,What is $\frac{d\left( (\cos(x))^{\cos(x)}\right)}{dx}$?,"How would you work something like this out? Are there similar problems to
$$\frac{d\left( (\cos(x))^{\cos(x)}\right)}{dx}$$
which could be worked out the same way?","['derivatives', 'calculus']"
2713207,Sequence of linear functionals on $\ell^{\infty}$,"I am having trouble with this question. Let $X$ be a normed space and let $T : X\to \ell^{\infty}$ be a bounded linear operator. $a)$ Show that there is a sequence $(f_n)_n$ in $X^*$ so that $Tx=(f_n(x))$ for all $x \in X.$ $b)$ Suppose that $X$ is a subspace of a normed space $Y$ . Show that there is a bounded linear operator $S: Y\to \ell^{\infty}$ so that $||S||=||T||$ and that $Sx = Tx$ for all $x \in X$ I am not sure how to start with question $a)$ . For $b)$ , I tried to find a $P: \ell^{\infty}\to \Bbb{R}$ so that the composite function $PT: X\to \Bbb{R}$ is a bounded linear functional. Then I applied the Hahn-Banach extension theorem to say that there exists a $PS: Y\to \Bbb{R}$ which is a bounded linear functional such that $PS(x)=PT(x)$ for all $x \in X$ . And then I got stuck. Please advise me on what to do. Thank you.",['functional-analysis']
2713210,Injectivity of $f_r : \mathbb{R} \to \mathbb{R}$ Defined as $f_r(x) = x^3 + rx + 1$: Inflection Points and Injectivity,"I have a function $f_r : \mathbb{R} \to \mathbb{R}$ defined as $f_r(x) = x^3 + rx + 1$. Provided $r > 0$, the derivative will always be strictly positive, meaning that the function will be injective (one-to-one). I am told the following: At $r = 0$, we have $f_r = x^3 + 1$, and this only has a turning point
  at $x = 0$, but the second derivative is also $0$ at this point. I am struggling to understand the point this statement is trying to make. It says that ""at $r = 0$, we have $f_r = x^3 + 1$, and this only has a turning point at $x = 0$"", but then it says "" but the second derivative is also $0$ at this point""; the way it is phrased seems to imply that it is trying to make some contrasting point between these two statements, but I am not understanding what this is? Since the second derivative is $0$ at the point, it means that it is an inflection point. But this doesn't mean that the function is not injective at this point, right? For instance, $f(x) = x^3$ has a turning point at $x = 0$, but its second derivative is also $0$ at this point, and $f(x) = x^3$ is injective (one-to-one). The function would still be injective for $r = 0$, right? Just because the derivative is positive at all points except $x = 0$ does not necessarily mean that the function is not injective at this point? Finally, I think my problem is that I'm struggling to understand what the presence of inflection points mean for injectivity. What DO the presence of inflection points mean for injectivity? I would greatly appreciate it if people could please take the time to clarify this.","['derivatives', 'real-analysis', 'maxima-minima', 'functions']"
2713214,normal domains on the plane,"Let $R\subset\mathbb{R}^2$ be a normal domain, normal with respect to the x-axis ( the definition I'm using ) Let $\gamma:\mathbb{R}^2\to\mathbb{R}^2$ be a rotation. Then: A. $\gamma(R)$ is normal with respect to the x-axis B. $\gamma(R)$ can be normal with respect to the y-axis C. $\gamma(R)$ is normal with respect to the x-axis or to the y-axis D. $\gamma(R)$ can be normal with respect to the x-axis and to the y-axis even if $R$ was normal with respect to the x-axis only The half-annulus is a simple example showing that A is false and B is true. I would guess C-true and D-false, but I have no proof for my suspicions.","['multivariable-calculus', 'geometry']"
2713284,Can numerical methods be used to prove a root does NOT exist?,"Let's say I have a continuous function $f : I \to \mathbb R$ for $I = [a,b]$ and I want to decide if it has a root or not in $I$. Pretend that I can evaluate it anywhere but cannot use analytical methods to learn anything more about it. I could obtain a grid of points $a = p_1 < \dots < p_n = b$, evaluate $f$ on each point, and make a scatterplot with some kind of interpolation. From this I may find a root, but if I don't see a root can I ever be confident that one actually does not exist? Can I know that there isn't wild behavior between some pair of points that I missed? Eg maybe for some $i$ the function dips down below $0$ really rapidly right after $p_i$ and returns right before $p_{i+1}$, so it looks flat but just because I've missed something. My question: what are the circumstances under which we can use a finite number of finite precision function evaluations to prove a root does not exist? My current guess is that if $f$ is Lipschitz then we could use its Lipschitz constant $K$ to make our grid fine enough that there's no way that $f$ could have a root between pairs of grid points if it doesn't visibly have one in the interpolated scatterplot. But if $K$ is large or $f$ is really close to $0$ then we may have a situation where the grid is required to be finer than finite precision can do. I also wonder if convexity would do the trick (which since it's stronger than Lipschitz on $I$ seems like a natural thing to try next).","['numerical-methods', 'real-analysis', 'roots']"
2713295,Derivative of log $x_i$ inside a $\log \sum$ of $x$,"I'm trying to differentiate the following: $\frac{\delta}{\delta \log x_m} \log \sum_{m=1}^M x_m^k \, f(y_m)$ So we only care about a particular index of $x$, and $k$ is a constant. I'm stumped at how to take the derivative of $\log x_m$. Since I can't push the $\log$ inside the sum, how can I even obtain $\log x_m$? I thought maybe the chain rule could apply here, but it's not obvious to me how that would work in this instance either.","['derivatives', 'logarithms', 'chain-rule', 'calculus']"
2713311,"$ \lim_{x \to \infty} [\frac{x^2+1}{x+1}-ax-b]=0 \ $ then show that $ \ a=1, \ b=-1 \ $","$ \lim_{x \to \infty} [\frac{x^2+1}{x+1}-ax-b]=0 \ $ then show that $ \ a=1, \ b=-1 \ $ Answer: $ \lim_{x \to \infty} [\frac{x^2+1}{x+1}-ax-b]=0 \\ \Rightarrow  \lim_{x \to \infty} [\frac{x^2+1-ax^2-ax-bx-b}{x+1}]=0 \\ \Rightarrow \lim_{x \to \infty} \frac{2x-2ax-a-b}{1}=0 \\ \Rightarrow 2x-2ax-a-b=0  \ \ (?)  $ Comparing both sides , we get $ 2-2a=0 \\ a+b=0 \ $ Solving , we get $ a=1 , \ b=-1 \ $ But I am not sure about the above line where question mark is there. Can you help me?",['limits']
2713384,Differentiability of the maximum,"For $i=1,\ldots,n$, let $f_i\colon(0,\infty)\to(0,1)$ and $g_i\colon(0,\infty)\to(0,1)$ be $C^{\infty}$ functions. Define $h\colon(0,\infty)\to(0,\infty)$ as follows:
$$h(t)=\max_{i=1,\ldots,n} \frac{f_i(t)}{g_i(t)}\qquad \forall t\in(0,\infty).$$ I am wondering if the following is true: The function $h$ is differentiable almost everywhere, and there exists $f,g\colon(0,\infty)\to(0,1)$ differentiable almost everywhere such that $h(t)=\frac{f(t)}{g(t)}$ for every $t\in (0,\infty)$. I am pretty sure it is true, but I would like to have confirmation (and, if possible, a reference). This question is quite related however it is not exactly the same. Still the answer of @Alex R. seems to confirm my belief that the affirmation is indeed true. Note: I don't know if it helps, but the case I am interested in have the additional properties that $h$ is strictly decreasing, $\lim_{t\to 0}h(t)=\infty$ and $\lim_{t\to \infty}h(t)=1$.","['derivatives', 'real-analysis', 'calculus']"
2713386,Stokes’ Theorem for arbitrary surface and boundary curve in $xz$-plane?,"I am tasked to find $\iint (\nabla \times {\bf V}) \cdot d{\bf S}$ for any surface whose bounding curve is in the $xz$-plane, where ${\bf V} = (xy + e^x) {\bf i}+ (x^2 -3y){\bf j} + (y^2 + z^2) {\bf k}$. I have attempted this via two methods and am stuck on both: 1) I’ve tried to employ Stokes’ Theorem directly. In the $xz$-plane, $y=0$, so {\bf V} becomes $e^x {\bf i}+ x^2 {\bf j}+ z^2 {\bf k}$, and $dy=0$ which makes the dot product $e^x dx + z^2 dz$. The problem is parametrising afterward. I’m unsure of how to approach this for an arbitrary curve. My intuition tells me that because this is a closed curve, the integral will sum up to zero, but I don’t know how to mathematically express this. 2) I also attempted to directly integrate the curl. The only interesting point to note is that the $y$-component is 0. Apart from that, I am unable to figure out how to obtain the normal vector to the surface to perform the integral. Any insight is appreciated, thank you!","['multivariable-calculus', 'surface-integrals', 'vector-fields', 'stokes-theorem']"
2713421,translation and dilation invariance of borel sets,"I am studying the book ""Real Analysis"" by Folland, and I have a question about the following. Folland writes on pg 37 that: Since the collection of open intervals is invariant under translations and dilations, the same is true of Borel sets in $R$ I understand the ""since $\dots$ dilations"" part, but why does this mean that the same is true of Borel sets? It seems true, but what is the proof of this claim?","['borel-sets', 'real-analysis', 'measure-theory']"
2713435,A rational matrix such that $A^{2017}=I_n$,"Let $A \in \mathcal{M}_{n}(\mathbb{Q})$ such that $A^{2017}=I_n$. Is it always true that $$A=I_n?$$ When $n \leq 2015$, I proved that this result must hold using the minimal polynomial and the fact that $2017$ is prime. I suspect that when $n \geq 2016$, this equality is not always true, but I can't find any matrix other than $I_n$ with that property, which also has only rational entries. I also thought of an approach with eigenvalues, but since they are all complex roots of $2017$, I am not sure how to make use of them here.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra', 'minimal-polynomials']"
2713460,Show existence of holomorphic function $g$ such that $f(g(z)) = g(z^n)$,"Let $f:\mathbb{C} \rightarrow \mathbb{C}$ be a holomorphic function, with $f(0)=0$, and $n\ge 0$ the multiplicity of $0$ as a zero of $f$. Show that there exist a holomorphic function $g:\mathbb{C}\rightarrow \mathbb{C}$ with $g(0)=0$ such that $$f(g(z)) = g(z^n).$$ I know the existence of $g$ such that $f(g(z)) = z^n$, but the above seems out of reach. Any ideas or possible sources for further research?",['complex-analysis']
2713465,Give an example of a function $f:\mathbb{R} \to \mathbb{R}$ that satisfies all three conditions,Give an example of a function $f:\mathbb{R} \to \mathbb{R}$ that satisfies all three conditions $f$  is bijective $f'(0)=0$ the inverse function $f^{-1}$ is not continuous at $0$ Does $f(x)=x^3$ satisfy the all three conditions? If not can any provide me an example?,"['continuity', 'real-analysis', 'examples-counterexamples', 'functions']"
2713500,Why do we need so many trigonometric definitions?,"Examples $$\sec(x) = \dfrac{1}{\cos(x)}$$
$$\cot(x) = \dfrac{1}{\tan(x)}$$ There are many more out there, but why do we need definitions that can be written with just $\sin , \cos ,\tan $ etc. in maths? Why can't they just be written as their expanded form? Most trigonometric functions can be written with just $\sin \cos$ and $\tan$. Why do we need so many? Additionally, I mean all definitions.","['math-history', 'trigonometry', 'soft-question', 'definition']"
2713531,Parachutist's descent with air resistance,"I have partially solved the following problem, taken from this book : Consider the free fall with air resistance modeled by $$\ddot x = \eta \dot x^2 - g$$ Solve this equation. (Hint: Introduce the velocity $v = \dot x$ as the new dependent variable.) Is there a limit to the speed the object can attain. If yes, find it. Consider the case of a parachutist. Suppose the chute is opened at a certain time $t_0 > 0$ . Model this situation by assuming $\eta = \eta_1$ for $0 < t < t_0$ and $\eta = \eta_2 > \eta_1$ for $t > t_0$ and match the solutions at $t = t_0$ . What does the solution look like? My partial solution is that, for a fixed value of $\eta$ , we have: $$A \dot x = \tanh(C - Bt) \\ \eta x = \ln \frac {e^{C - Bt}} {1 + e^{2C - 2Bt}} + D$$ where $A = \sqrt {\eta / g}$ , $B = \sqrt {\eta g}$ , and $C$ and $D$ are constants of integration. Suppose that I have computed constants $A_1, B_1, C_1, D_1$ for a given $\eta_1 > 0$ and initial conditions $x_1(0) = x_0$ and $\dot x_1(0) = v_0$ . I shall compute constants $A_2, B_2, C_2, D_2$ for a given $\eta_2 > \eta_1$ and initial conditions $x_2(t_0) = x_1(t_0)$ and $\dot x_2(t_0) = \dot x_1(t_0)$ . Expanding the last condition, we get $$A_2 \tanh (C_1 - B_1 t_0) = A_1 \tanh (C_2 - B_2 t_0)$$ Since $\eta_2 > \eta_1$ , it follows that $A_2 > A_1$ . If $t_0$ is large enough, the absolute value of $\tanh (C_1 - B_1 t_0)$ will be close enough to $1$ to make absolute value of $\tanh (C_2 - B_1 t_0)$ greater than $1$ , which is a contradiction. What should I conclude from this?","['physics', 'ordinary-differential-equations']"
2713537,A general theory of representation of algebraic structures,"Often algebraic objects like groups or monoids could be represented in terms of other objects, in the case of groups as automorphisms for example of graphs, or as permutations (automorphisms of a finite sets), or of some vector space. Is there a general theory about these representations? When could we switch between them (for example representing permutations by matrices, and elements by a basis vector representing characteristic vectors). Is there a general notion of ""isomorphism"" between different representations, how to capture abstractly the notion of an representation? Guess it might have to do with category theory? But then, does this theory just works in case of associative algebraic structures? And could this theory in some sense capture the ""richness"" of a representation, for example a matrix representation I would consider more ""rich"" as a permutation representation, as when computing with vectors and matrices more could be done, or more laws hold.","['representation-theory', 'abstract-algebra', 'group-theory']"
2713541,Set-theoretical inequality,"Let $G$ be a group and suppose $E \subseteq G$ finite and symmetric (i.e. $g^{-1} \in E$, for all $g \in E$) . Define $E^n=\lbrace g_1g_2...g_n \mid g_i \in E \rbrace$, for all $n \in \mathbb{N}$. For all $g \in G$, denote $gE^n=\lbrace gt \mid t \in E^n \rbrace$. Is there an elementary way to show that $$|gE^n \Delta E^n| \leq |E^{n+1}| - |E^n|,$$
where '$\Delta$' denotes the symmetric difference? $\textbf{EDIT}$: I misread the the question. It should be $g \in E$.","['inequality', 'group-theory', 'elementary-set-theory']"
2713635,$C^1$ metric in smooth manifold.,"Given a smooth manifold $M$ (Hausdorff,second countable), and let $g$ be a pseudo-Riemannian metric but is only $C^1$ (or $C^k$ or continuous). Can we induce a Levi-Civita connection? Can we talk about geodesics and exponential map, what about the regularities? Thank you in advance.","['riemannian-geometry', 'differential-geometry']"
2713657,Sum of the first integer powers of $n$ up to k,"Pascal's triangle has a lot of interesting patterns in it; one of which is the triangular numbers and their extensions. Mathematically: $$\sum_{n=1}^k1=\frac{k}{1}$$
$$\sum_{n=1}^kn=\frac{k}{1}\cdot\frac{k+1}{2}$$
$$\sum_{n=1}^kn^2=\frac{k}{1}\cdot\frac{k+1}{2}\cdot\frac{2k+1}{3}$$ At first, we could guess that the next summation is: $$\sum_{n=1}^kn^3 ?=\frac{k}{1}\cdot\frac{k+1}{2}\cdot\frac{2k+1}{3}\cdot\frac{3k+1}{4}$$ Yet this is off. However, it is off geometrically . Notice: $$\left(\sum_{n=1}^kn^3\right)-\frac{k}{1}\cdot\frac{k+1}{2}\cdot\frac{2k+1}{3}\cdot\frac{3k+1}{4}=error$$
$$k=1, r=0$$
$$k=2, r=0.25$$
$$k=3, r=1$$
$$k=4, r=2.5$$
$$k=5, r=5$$
$$k=6, r=8.75$$
... Consider the ratios of the errors: $$er(k)=\frac{r(k+1)}{r(k)}$$
$$k=1, r=udf$$
$$k=2, r=4$$
$$k=3, r=2.5$$
$$k=4, r=2$$
$$k=5, r=1.75$$
$$k=6, r=1.6$$ Then, rewriting the error as a function of n starting at k = 5: $$1.75=2.5-\frac{1.5}{2}$$
$$1.6=2.5-\frac{1.5}{2}-\frac{1.5}{10}$$
$$1.5=2.5-\frac{1.5}{2}-\frac{1.5}{10}-\frac{1.5}{15}$$
$$1.42857=2.5-\frac{1.5}{2}-\frac{1.5}{10}-\frac{1.5}{15}-\frac{1.5}{21}$$ The denominators in the series are from pascals triangle: (3rd columns, or dependent again on the triangular numbers) Then the total formula for equating the two is: $$\left(\frac{k}{1}\cdot\frac{\left(k+1\right)}{2}\cdot\frac{\left(2k+1\right)}{3}\cdot\frac{\left(3k+1\right)}{4}\right)-\left(\sum_{n=1}^kn^3\right)+\frac{1}{24}\left(k-1\right)k\left(k+1\right)=0$$ Super interesting! At least, I thought it was interesting how this the error is related back to the previous power's formula. Am I missing something obvious? Any input is greatly appreciated. (I'm not smart, so in the likely event I missed something obvious try not to be too harsh) Update: For the next power (4), I found the formula with trial and error: $$\left(\frac{k}{1}\cdot\frac{\left(k+1\right)}{2}\cdot\frac{\left(2k+1\right)}{3}\cdot\frac{\left(3k+1\right)}{4}\cdot\frac{\left(4k+1\right)}{5}\right)+\frac{1}{24}\left(k-1\right)k\left(k+1\right)+\frac{1}{12}\left(k-1\right)k\left(k+1\right)k$$ Any ideas on power (5) and so on? I'll continue to try and generalize it.","['algebra-precalculus', 'summation', 'binomial-coefficients', 'calculus']"
2713690,Showing $\mathbb{Z}[i]/(1+2i) \oplus\mathbb{Z}[i]/(6-i)\cong\mathbb{Z}[i]/(8+11i)$,"I am attempting to solve Ch 14 Problem 7.7 from Artin's algebra book. Let $R=\mathbb{Z}[i]$ and let $V$ be the R-module generated by elements $v_1$ and $v_2$ with relations $(1+i)v_1+(2-i)v_2=0$ and $3v_1+5iv_2=0$. Write this module as a direct sum of cyclic modules. Attempt I have obtained $V\cong R^2/ \begin{bmatrix} 1+i & 3 \\ 2-i & 5i \end{bmatrix} R^2 \cong R/[8+11i]R=\mathbb{Z}[i]/(8+11i)$. Now, I see that $(1+2i)(6-i)=8+11i.$ Now, I would like to show that $\mathbb{Z}[i]/(1+2i) \oplus\mathbb{Z}[i]/(6-i)\cong\mathbb{Z}[i]/(8+11i)$, so I can have $V$ as a direct sum of cyclic modules as needed, but how can I show this? I have already shown that $(1+2i,6-i)=(1)=\mathbb{Z}[i]$ and thus $(1+2i)+(6-i)=\mathbb{Z}[i]$. Intuition would suggest that $(1+2i)\oplus(6-i)=\mathbb{Z}[i]$, although I think this is false since $(i-6)(1+2i)+(1+2i)(6-i)=0$. I must confess that I am very new to module theory so please be patient with me. I don't even how it would be possible to have $\mathbb{Z}[i]/(1+2i) \oplus\mathbb{Z}[i]/(6-i)\cong\mathbb{Z}[i]/(8+11i)$ since $\mathbb{Z}[i]/(1+2i)$and $\mathbb{Z}[i]/(6-i)$ aren't even submodules of the same set.","['modules', 'complex-numbers', 'abstract-algebra', 'ring-theory', 'ideals']"
2713697,Minimum Least Squares Solution Using Pseudo Inverse (Derived from SVD) Is The Minimum Norm Solution - Extension from Vectors to Matrices,"Given $A \in \mathbb{R}^{m \times n}$ , $B \in \mathbb{R}^{k \times \ell}$ , and $C\in \mathbb{R}^{m \times \ell}$ . Show that for $X \in \mathbb{R}^{n \times k}$ $$ {A}^{\dagger} C {B}^{\dagger} = \arg \min_{X} {\left\| C - A X B \right\|}_{F} $$ is the unique solution. Note: This is an extension of the minimum $2$ -norm least squares problem. Hint: Use the SVDs of $A$ and $B$ . This is a homework problem, but I got stuck. After I do SVD for both $A$ and $B$ , I discard the $0$ terms to get a slim SVD. Then I can factor out some things, but still don't see an explicit expression formulating. Could somebody guide me in the right direction? Edit: 1.""F norm"": Frobenius norm 2. $A^+$ or $A^\dagger$ : conjugate transpose, Moore–Penrose pseudoinverse","['least-squares', 'optimization', 'matrix-calculus', 'svd', 'linear-algebra']"
2713786,"Knowing that $A^2+B^2=\sqrt{2+\sqrt{2}}\cdot AB$, prove that $n$ is a multiple of $16$","Let $A,B \in \mathcal{M}_{n}(\mathbb{R})$ such that $$A^2+B^2=\sqrt{2+\sqrt{2}}\cdot AB$$
  Knowing that $\det(AB-BA)>0$, prove that $n$ is multiple of $16$. I know that for this type of problems, one usually uses some identities such as $$(A+iB)(A-iB)=c(AB-BA)$$ and its conjugate, where $c$ is a complex number and since $A,B$ are real matrices, their determinants will be positive real numbers. Since $\det(AB-BA)>0$, that will lead to $c^n=0$ and with the help of some trigonometry it would follow that $n$ is the multiple of something. Here, however, I couldn't obatain the identity I described. That square root points, though, exactly to this method and some trigonometry.","['matrices', 'linear-algebra', 'determinant']"
2713810,"How can I construct polynomials with ""small"" coefficients generating a prime ""late""?","Let $f(x)$ be a polynomial with degree $5$, integer coefficients and positive leading coefficient. Let $M$ be the maximum of the absolute values of the coefficients. Assume the smallest non-negative integer $n$ such that $f(n)$ is prime is greater than $10\ 000$, but that such an $n$ exist. Can I construct polynomials with the desired property and ""small"" $M$ ? Can the smallest possible $M$ be determined efficiently ? Idea : Construct an irreducible polynomial such that the $\gcd$ of its values is $1$. This should generate at least one prime due to the Bunyakovsky-conjecture. The problem is to avoid an ""earlier"" prime. Motivation : Constructing ""hard cases"" for the bunyakovsky conjecture.","['irreducible-polynomials', 'polynomials', 'number-theory', 'prime-numbers', 'elementary-number-theory']"
2713837,"If $X$ is compact and $Y$ is closed. Show that there exists $x_0 \in X$, and $y_0 \in Y$ such that $|x_0-y_0| \leq |x-y|$ for all $x \in X$, $y \in Y$","Let $X$ , $Y$ be subsets of $\Bbb{R}$ , with $X$ compact and $Y$ closed. Show that there exists $x_0 \in X$ , and $y_0 \in Y$ such that $|x_0-y_0| \leq |x-y|$ for all $x \in X$ , $y \in Y$ . If $Z = \{|x-y| : x\in X, y\in Y\}$ , then $0$ is a lower bound of $Z$ so there exists $\alpha = \inf Z$ . So there is a sequence of elements in $Z$ such that it converges to $\alpha$ , this means that there are sequences $(x_n)$ in $X$ and $(y_n)$ in $Y$ such that $\lim |x_n-y_n| = \alpha$ . Now, because $X$ is compact, then there is a convergent subsequence of $(x_n)$ , $(x_{n_k})$ and it could converge to an $x_0$ in $X$ (because $X$ is closed). But I am stuck here, I think what I need is $(x_n)$ to be convergent but that does not necessarily happen here.","['general-topology', 'real-analysis']"
2713875,Is a nonsingular matrix not the same as an invertible matrix?,"Over an arbitrary ring $R$, a matrix $A$ is said to be invertible if it has an inverse with entries in the same ring. This happens iff $\det A$ is a unit of $R$. I've always thought that the terms ""invertible"" and ""nonsingular"" are synonymous. But I think the following problem (from Artin) suggests that they are not (at least over an arbitrary ring): Let $\varphi: \mathbb Z^k\to \mathbb Z^k$ be a homomorphism given by multiplication by an integer matrix $A$. Show that the image of $\varphi$ is of finite index if and only if $A$ is nonsingular and that if so, then the index is equal to $|\det A|.$ If I understand correctly, in this context ""nonsingular"" means $\det A\ne 0$. And this is not the same as invertible since if $A$ is invertible, then $\varphi$ is bijective, and the image is the whole $\mathbb Z^k$. So, am I correct in saying that a matrix $A$ over a ring $R$ is by definition nonsingular if $\det A\ne 0$? And being nonsingular does not imply being invertible (unless the underlying ring is a field)?","['matrices', 'abstract-algebra', 'linear-algebra', 'determinant']"
2713886,Degree of a Projective Curve,"Let $f \in k[X,Y,Z]$  be a homogeneous polynomial of degree $d$ and $V_+(f) := C \subset \mathbb{P}^2_k$ interpreted as closed subscheme (therefore $1$-dimensional, proper $k$-scheme) of $\mathbb{P}^2$. Let $i:C \to \mathbb{P}^2$ be the inclusion morphism. The degree of $C$ is by definition defined via $\deg(C) := \deg(i^*\mathcal{O}_{\mathbb{P}^2}(1))$, where $\mathcal{O}_{\mathbb{P}^2}(1)$ is the tautological line bundle. Here the $deg$-map from $Pic(C) \to \mathbb{Z}$ for an invertible sheaf $\mathcal{L}$ on $C$ is defined via $deg(\mathcal{L}) := \chi(\mathcal{L})- \chi(\mathcal{O}_C)$, where $\chi$ is the euler characteristics. How to deduce that $d=\deg(C)$ holds?","['algebraic-curves', 'algebraic-geometry']"
2713908,Question about the proof of the distributive property for sets,"I have been learning set theory from scratch again and I have a question about the distributive property for sets. It seems the only proof I found of it actually USES the very same distributive property while proving it. This seems circular to me, could someone explain why it is allowed? This is what I am trying to prove. $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$ I got this proof from a quora answer. Let $x \in A \cup (B \cap C)$. If $x \in A \cup (B \cap C)$ then $x$ is either in $A$ or in ($B \cap C$). $x \in A$ or $x \in (B \cap C)$ $x \in A$ or ($x \in B$ and $x \in C$) HERE!!! How does one get from the statement above to the one below without actually using the distributive property itself? ($x \in A$ or $x \in B$) and ($x \in A$ or $x \in C$) $x \in (A \cup B)$ and $x \in (A \cup C)$ $x \in (A ∪ B) \cap (A ∪ C)$ $x \in A \cup (B \cap C) \implies x \in (A \cup B) \cap (A \cup C)$. Thanks a lot!!","['logic', 'elementary-set-theory', 'proof-verification']"
2713964,Probability of remaining a whole pancake rather than two halves.,"A very interesting question. It is trivial for small number of pancakes but for 100 I was not able to find an analytical or manual way to figure out the probability. Thanks a lot in advance if you can share your ideas! Suppose you have 100 pancakes to eat. Everyday you eat a half of a pancake. In this way after one day there will remain 99 whole pancakes and a half pancake. Suppose your choice of the pancake is random. That is, you are equally likely to pick any remaining pancake, no matter it is a whole pancake or a half pancake, to eat everyday. Formally, if there are $X$ whole pancakes and $Y$ half pancakes at the beginning of some day, then the probability of each piece of pancake to be picked is $\frac{1}{X+Y}$. Then what's the probability of the event that after $99\times 2 = 198$ days of eating, there will remain a whole pancake rather than 2 halves of pancakes? Notice that the question might be interpreted in another way, as @Acccumulation noted in his answer. So please be careful about the interpretation. For example, denote the index of the day by $k$, and the number of whole pancakes at (the end of) day $k$ as $X_k$ (i.e., after you eat the pancake), then $P(X_0 = 100) = 1$, $P(X_1 = 99) = 1$, $P(X_2 = 99) =1/100, P(X_2 = 98) = 99/100 $. If we denote the number of half pancakes at (the end of) day $k$ as $Y_k$, then it's easy to see that $2X_k + Y_k + k = 200$ and 
$$P(X_{k+1} = X_k - 1) = \frac{X_k}{X_k+Y_k}, P(X_{k+1} = X_k) = \frac{Y_k}{X_k+Y_k}.$$
Or equivalently,
$$P(X_{k+1} = X_k - 1) = \frac{X_k}{200-k-X_k}, P(X_{k+1} = X_k) = \frac{200-k-2X_k}{200-k-X_k}.$$
However, this relationship depends on both the values of $X_k$ and $k$, which is hard to use for recursion by hand. Does anyone have some ideas to do recursion or go in some other directions? Thanks a lot!","['combinatorics', 'probability']"
2714003,Prove that $\int_0^\frac{\pi}{2}\cos^mx \sin^mxdx=2^{-m}\int_0^\frac{\pi}{2}\cos^mxdx$,"As stated in the title, I want to prove that: 
$$\int_0^\frac{\pi}{2}\cos^mx \sin^mxdx=2^{-m}\int_0^\frac{\pi}{2}\cos^mxdx$$ So far, I thought that $$\int_0^\frac{\pi}{2}\cos^mx \sin^mxdx=2^{-m}\int_0^\frac{\pi}{2}\sin^m2xdx$$
But that doesn't seem to take me anywhere. I would appreciate some help! 
Thanks!","['definite-integrals', 'trigonometry']"
2714018,strong convergence of product of operators,"I'm trying to prove the next: Let $H$ be a Hilbert space. Consider $\{T_{n}\}$ and $\{S_{n}\}$ sequences of $\mathcal{B}(H)$ such that $S_{n}\xrightarrow{s} S$ and $T_{n}\xrightarrow{s}T;$ here $S_{n}\xrightarrow{s} S$ means that $S_{n}$ converges strongly to $S,$ i.e. for each $x\in H,$ $S_{n}x\rightarrow Sx.$ Then $S_{n}T_{n}\xrightarrow{s}ST.$ So, for $x\in H,$ We have $||(S_{n}T_{n}-ST)x||\leq||(S_{n}-S)T_{n}x||+||S||||(T_{n}-T)x||.$ The second term of right side of the inequality above converges to zero. My doubt comes from the other term. If $y_{n}=T_{n}x$ it would seem such term converges to zero too because of the strong convergence of $S_{n},$ but such $y_{n}$ depends of $n;$ I have doubts about it. Any kind of help is thanked in advanced.","['functional-analysis', 'operator-theory']"
2714026,Heavy tailed distributions and their sum,"Let $X_{1}, X_{2}, \ldots, X_{n}$ be the sequence of i.i.d random variables with heavvy tailed distributions, i.e. 
$$p(x_{i}) \sim \frac{A}{x_{i}^{\alpha}}$$ 
as $x_{i} \rightarrow \infty$, where $p(x_{i})$ stands for the density of $X_{i}$. The question is: how to estimate the asymtotic of the density $p(y)$, where $$Y = X_{1} + X_{2} + \ldots + X_{n}$$ ? A pretty straightforward approach is the following: calculate the characteristic function of $X_{i}$, since the random variables are i.i.d, then $$\varphi_{Y}(t) = \varphi_{X_{1} + X_{2} + \ldots + X_{n}}(t) = (\varphi_{X_{1}}(t))^{n}$$ then apply the inverse Fourier transform to figure out the distribution of sum. Are there any, say, 'elegant' ways to approach the problem above?","['distribution-tails', 'characteristic-functions', 'probability-theory', 'probability-distributions', 'probability']"
2714039,Finding the number of permutations of $[12]$ of given orders,"Find the number of permutations of $[12]$ whose order is  $a)3 \\b)4 \\c)12$ As @астонвіллаолофмэллбэрг stated in the comments, it's not an easy process and I missed counting some possible permutations already.. My solution: a)I started choosing $3$ elements out of $12$ that are forming the order of the permutation And WLOG, I chose $1,2,3$ So the permutations I get are: $\begin{pmatrix}
    1 & 2 & 3 & 4 & \cdots & 11 & 12 \\
    \color{red}{2} & \color{red}{3} & \color{red}{1} & 4 &\cdots &  11  & 12
  \end{pmatrix}$ and  $\begin{pmatrix}
    1 & 2 & 3 & 4 &\cdots & 11 & 12 \\
   \color{red}{3} & \color{red}{1} & \color{red}{2} & 4 &\cdots &  11  & 12
  \end{pmatrix}$ So, the number of permutations of $[12]$ of order $3$ is $\binom{12}3\cdot 2 $ b)Similarly, I chose $4$ elements - namely $1,2,3,4$ - out of $12$ that are forming the order of the permutation So the permutations I get are: $\begin{pmatrix}
    1 & 2 & 3 & 4 & 5 & \cdots & 11 & 12 \\
    \color{red}{2} & \color{red}{3} & \color{red}{4} &\color{red}{1} & 5 & \cdots & 11 & 12
  \end{pmatrix}$, $\begin{pmatrix}
    1 & 2 & 3 & 4 & 5 & \cdots & 11 & 12 \\
    \color{red}{2} & \color{red}{4} & \color{red}{1} &\color{red}{3} & 5 & \cdots & 11 & 12
  \end{pmatrix}$, 
, $\begin{pmatrix}
    1 & 2 & 3 & 4 & 5 & \cdots & 11 & 12 \\
    \color{red}{3} & \color{red}{1} & \color{red}{4} &\color{red}{2} & 5 & \cdots & 11 & 12
  \end{pmatrix}$,
$\begin{pmatrix}
    1 & 2 & 3 & 4 & 5 & \cdots & 11 & 12 \\
    \color{red}{3} & \color{red}{4} & \color{red}{2} &\color{red}{1} & 5 & \cdots & 11 & 12
  \end{pmatrix}$, $\begin{pmatrix}
    1 & 2 & 3 & 4 & 5 & \cdots & 11 & 12 \\
    \color{red}{4} & \color{red}{1} & \color{red}{2} &\color{red}{3} & 5 & \cdots & 11 & 12
  \end{pmatrix}$,$\begin{pmatrix}
    1 & 2 & 3 & 4 & 5 & \cdots & 11 & 12 \\
    \color{red}{4} & \color{red}{3} & \color{red}{1} &\color{red}{2} & 5 & \cdots & 11 & 12
  \end{pmatrix}$, So, the number of permutations of $[12]$ of order $4$ is $\binom{12}4\cdot 6 $ c) For this part I think the problem as this: Since we want the permutation to be of order $12$, starting from $1$ : $1$ can be matched to one of the $11$ numbers out of $12$,excluding itself, say it matched to the $2$, then there remains $10$ numbers to be matched with $2$ , exluding $1$ and itself, and there remains $9$ numbers to be matched with $3$ , exluding $1$, $2$ and itself, so it goes in this fashion.. So, the number of permutations of $[12]$ of order $12$ is $11\cdot 10 \cdots 2\cdot 1=11!$ Are my solutions valid for $a,b$ and $c$?","['permutations', 'combinatorics']"
