question_id,title,body,tags
1653709,There exists a real number so that $X_n$ is a martingale,"I am working on the following problem: Let $Y_n$ be a sequence for which there exists constants $\alpha$ and $\beta$ with 
  $$
E(Y_{n+1}\mid \mathcal{F}_n)=\alpha Y_n +\beta Y_{n-1}
$$
  for each $n$. Show that there exists a real number $a$ such that $X_n=aY_n+Y_{n-1}$ is a martingale. Here the problem doesn't state it, but I am assuming that $(\mathcal{F}_n)_{n=1}^\infty$ is a filtration and $Y_n\in\mathcal{F}_n$. This is my work so far. If I want $X_n$ to be a martingale then I need $E(X_{n+1}\mid\mathcal{F}_n)=X_n$. So we need $E(aY_{n+1}+Y_n\mid\mathcal{F}_n)=aY_n+Y_{n-1}$, but 
$$
E(aY_{n+1}+Y_n\mid\mathcal{F}_n)=aE(Y_{n+1}\mid\mathcal{F}_n)+Y_n=(a\alpha+1) Y_n + a\beta Y_{n-1}
$$ So in order for $X_n$ to be a martingale we would need the constant $a$ to satisfy, $a=1/\beta,$ and $a=1/(1-\alpha).$ This is where I see an issue, unless $\alpha+\beta=1$ I don't see how such an $a$ can exist. Is there something I am missing here? For reference this problem is problem 9.12 from Probability, by Karr.","['probability-theory', 'conditional-expectation', 'martingales']"
1653713,Consecutive strings of heads problem,"So the question asks: We toss a fair coin $n$ times and record the outcome as a sequence of H and T. We say that there is a run of heads if there is a consecutive string H...H which starts either at the first toss or after the coin lands tails and which ends either at the last toss or before the coin lands tails. For example, the sequence HTHHTHHHHTTHH has four runs of heads. Find the expected number of runs of heads. So far I have: Suppose the probability that all $K$ tosses have the same type of toss equal to the probability of all heads in $K$ tosses + probability of all tails in $K$ tosses. Sets of subsequent $K$ tosses $=(n-k+1)$ Expected number of runs of heads equals the number of subsequent $K$ tosses times the probability that one set of $K$ tosses is a streak $= (n-k+1)\times 2\times (1/2)^k$ But I am really not sure about my solution, is this the right way to do this kind of probability problem?",['probability']
1653758,Mentally calculating trigonometric function values such as $\sin(47^\circ)$,"This may sound dumb, but does such a way exist to mentally (and quickly) determine the values of trigonometric functions such as $\sin(47^\circ)$ and so forth--quickly being a mere matter of seconds? My physics teacher suggested to our class that it is in fact possible, though I see no other ways apart from memorization and the standard methods involving triangles or trig identities (which are decidedly not quick). I'm on my high school's 'mathletes' team, and though it's unnecessary minutiae I thought that it would be a fun thing to share with my teammates. That, and calculating logarithms mentally, though it appears that the means for doing so has already been answered on this site. (Feel free to comment on the logarithm issue as well, however, if you do have a particularly clever method that you'd be willing to share.) Parameters: to 2-3 accurate decimal places is ideal. I'm not quite sure how that corresponds %-wise.","['trigonometry', 'mental-arithmetic']"
1653765,Proving a Trick to More Quickly Calculate N-Step Transition Probabilities,"So, I have been working on a homework problem all day that asks me to prove that: $P^n= \Pi +Q^n$
where P is the transition matrix of a finite-state regular Markov Chain, $\Pi$ is a matrix whose rows are the stationary distribution of P, and $Q = P - \Pi$. The fundamental relation that I have been attempting to show is that $(P-\Pi)^n=P^n-\Pi$. I'm not sure if there is a relevant matrix product property here that would allow me to establish this relation but I can't figure out how to proceed. Also, for those who have math backgrounds but not necessarily statistics backgrounds, the stationary distribution is just a left eigenvector with eigenvalue 1, and P is a stochastic matrix (rows summing to 1). Originally, I was attempting to rewrite the matrix product relation in sigma notation and try to achieve some sort of simplification. I didn't find anything by doing that, so here is my most current attempt: $P^n= \Pi +Q^n$ Substituting in for Q to get to the relation I referred to earlier: $P^n=\Pi+(P-\Pi)^n$ $P^n-\Pi=(P-\Pi)^n$ Then, I use the property that the eigenvalue of $\Pi$ is 1 to rewrite P as $\Pi*P$. $(\Pi*P-\Pi)^n=P^n-\Pi$ $\Pi^n*(P-1)^n=P^n-\Pi$ From here, I am unsure how to proceed but believe that this is a natural way to use the eigenvalue to alter the relation.","['stochastic-processes', 'matrices', 'markov-chains', 'statistics', 'probability']"
1653783,Show that $fg$ is differentiable at $\hat{x}$ and that $(fg)'(\hat{x})= g(\hat{x})f'(\hat{x}) + f(\hat{x})g'(\hat{x})$,"Let $U$ an open set in $\mathbb{R^n}$, $\hat{x} \in U$ and let $f : U
 \to \mathbb{R}$ and $g : U \to \mathbb{R}$ two different
  differentiable functions at $\hat{x}$. Show that $fg$ is
  differentiable at $\hat{x}$ and that $(fg)'(\hat{x})=
 g(\hat{x})f'(\hat{x}) + f(\hat{x})g'(\hat{x})$. I know basically the proof for two functions at one variable. However, the problem seems to be different when we have two functions with strictly more than one variable. I don't know why but I tried different things and I don't have the flash to finish this problem. So that's what I've done so far: We have $$\lim_{x \to \hat{x}} \frac{f(x)-f(\hat{x})-f'(\hat{x})(x-\hat{x})}{||x-\hat{x}||}$$ and $$\lim_{x \to \hat{x}} \frac{g(x)-g(\hat{x})-g'(\hat{x})(x-\hat{x})}{||x-\hat{x}||}$$ Let $h(x)=(fg)(x)$ and taken $h'(\hat{x}) = g(\hat{x})f'(\hat{x}) + f(\hat{x})g'(\hat{x})$. Then $$\lim_{x \to \hat{x}} \frac{h(x)-h(\hat{x})-h'(\hat{x})(x-\hat{x})}{||x-\hat{x}||} = \lim_{x \to \hat{x}} \frac{(fg)(x)-(fg)(\hat{x})-(g(\hat{x})f'(\hat{x}) + f(\hat{x})g'(\hat{x}))(x-\hat{x})}{||x-\hat{x}||}$$ From this time, I tried differents factorizations, but it block; here one of them : $$(fg)(x)-(fg)(\hat{x}) = f(x)(g(x) - g(\hat{x})) + g(\hat{x})(f(x)-f(\hat{x}))$$ and $$g(\hat{x})f'(\hat{x}) + f(\hat{x})g'(\hat{x}) = f(\hat{x})(g(\hat{x})- g'(\hat{x})) + g'(\hat{x})(f(\hat{x}) + f'(\hat{x}))$$ Does someone could tell me where I could make a correction? Otherwise, is there an easier way to approach this type of question?","['derivatives', 'real-analysis']"
1653805,Divergence in Riemannian Geometry (General Relativity),"I'm taking a course in General Relativity and I'm having some problems with the notation. I know that Einstein's tensor verifies $\nabla_aG^{ab}=0$. In physics textbooks this consequence of Bianchi identity is phrased as ""the tensor has 0 divergence"". I don't understand this because for me that identity means: If you take Einstein's tensor $G$ and take the covariant derivative $\nabla_a G$ then $(\nabla_a G)^{ab}=G^{ab}_{\quad;a}=0$. I cannot see the divergence in there. I studied that the divergence and all of those classical differential operators could be understood through the external derivative which makes sense.. So my question: Is $\nabla_a u^{a}$ called divergence because if you take the summation conventions it looks like a divergence? I come from a mathematical background and I think index notation is powerful but sometimes I feel like I'm missing the point.","['riemannian-geometry', 'differential-geometry', 'general-relativity']"
1653816,2-Norm of Non-Square Matrices,"So, the 2-norm of an $m \times n$ matrix for $m\geq n$ is defined by the max singular value/square of the max eigenvalue. But, if it's not square, and you're only given a matrix A (no x-vector), what do you do if 1. m>n. Do I have to go through the whole SVD process, since I can't find an eigenvalue? or 2. if n>m, since I can't do SVD then. Please note that I'm talking about if I'm only given a matrix A, so I can't use that $||A||{_2} =max _{ \space \vec x \neq 0}  \frac{||A\vec x||{_2}}{||\vec x||_{2}}  $ .","['matrices', 'normed-spaces', 'numerical-methods', 'linear-algebra', 'analysis']"
1653822,"In general, when does a ring have a division algorithm?","I'm working through Herstein's ""Abstract Algebra"" text, and am currently working through section 5. Theorem 4.5.5 introduces the division algorithm for polynomial rings over fields, which states: Given the polynomial $f(x), g(x) \in F[x]$, where $g(x) \neq 0$, then $$f(x) = q(x)g(x) + r(x),$$ where $q(x), r(x) \in F[x]$ and $r(x) = 0$ or $\deg r(x) < \deg g(x).$ What requirements must be put on a ring to ensure a division algorithm exists? It seems that the existence of some kind of norm is necessary (in this case, the $\deg$ function). In other words, does the division algorithm hold, say, in any integral domain? Or do you need a unique factorization domain, or perhaps a principle ideal domain instead? My thought is that it almost certainly holds in any Euclidean domain, but I'm wondering if this is too strong of a requirement.","['abstract-algebra', 'ring-theory', 'polynomials']"
1653830,Does the Cauchy Schwarz inequality hold on the L1 and L infinity norm? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question So i am wondering if the Cauchy Schwarz inequality holds for all p-norms, not just when p=2, which is the euclidean space. Thank you.","['real-analysis', 'analysis']"
1653867,Is a self-adjoint operator continuous on its domain?,"Let $H$ be a Hilbert space, and $A : D(A) \subset H \rightarrow H$ be an unbounded linear operator, with a domain $D(A)$ being dense in H.
We assume that $A$ is self-adjoint, that is $A^*=A$. Since $A$ is unbounded, we can find a sequence $x_n$ in the domain such that $x_n \rightarrow x$ with $x \notin D(A)$, meaning that $Ax_n$ does not converge. My question is the following: Is it true that $A$ is continuous relatively to its domain? More precisely: given a converging sequence $x_n \rightarrow x$ for which both the sequence $x_n$ and the limit point $x$ lie in $D(A)$, can we conclude that $Ax_n \rightarrow Ax$? Or is there any counter-example? I know that self-adjoint operators are closed, in the sense that their graph $D(A)\times A(D(A))$ is closed in $H \times H$, but continuity on the domain is something else.","['functional-analysis', 'operator-theory']"
1653877,Almost sure bounded imply finite expectation?,"Suppose that the random variable $X$ is $\mid X \mid<M$ almost surely, for some constant $M<\infty.$ Then can we say that $E(X)<C$ for some constant $C<\infty$? If the expectation is not bounded, is there any additional (other than bounded a.s.) conditions on $X$ that would give us a finite expectation? My confusion is there could be $0\cdot \infty$ terms where the random variable is infinite with probability $0$. And these terms are not defined well. I believe this post disproves the converse. This is not homework just self studying. Thank you.","['probability', 'measure-theory']"
1653889,"Is there a way to prove this exponential inequality: if $a>b$ then $a^a>b^b$ for $a,b>1$?","I came across this proposition while trying to prove that a function was injective: if $a>b$ then $a^a>b^b$, where $a$ and $b$ are real numbers bigger than $1$. Intuitively it (somehow) makes sense but I wonder if a rigorous proof can be made.
But, the initial problem I was trying to solve was to show that $f(x)=x^x$, where $x$ is just a positive real number, is injective . As the ""contrapositive method"" from the definition of an injective function didn't work out, I figured I could just show that my function was strictly increasing or decreasing, therefore the function would be injective. I looked at the graph of this function and I noticed I have a turning point at $x=1/e$ (as the user MXYMXY pointed out). Thus I had two cases for my function.","['real-analysis', 'inequality', 'exponential-function', 'functions', 'algebra-precalculus']"
1653892,Finite dimensional division algebra over $\Bbb{C}$,"Another abstract algebra question from my university days that has me stumped at where to start! I know what a division ring is and I think I understand what a division algebra over $\mathbb C$ is. (A division ring $D$ where there is an additional operation of scalar multiplication of elements of $D$ with elements of $\mathbb C$. I have come across this question and don't really know where to start. I'm not asking for a full proof, but a good starting point would be greatly appreciated! Let $D$ be a finite-dimensional division algebra over $\mathbb C$. Show that $D=\mathbb C$. Am I right in thinking that a finite-dimensial division algebra is one where $D$ is spanned by a certain (finite) number of (linearly independent?) elements of $D$? Thanks in advance, Andy.","['abstract-algebra', 'ring-theory', 'division-algebras']"
1653895,Conditional expectation with respect to two sigma algebras,"So the problem is to define two sigma algebras, a stochastic variable,
  specify a probabilty measure on the sample space $\Omega$ and show
  that the following relation doesn't equal: $$E[E[X|\mathcal{G}]|\mathcal{F}]\neq E[E[X|\mathcal{F}]|\mathcal{G}]$$ So I started by defining the sigma algebras: $$\Omega=\{a,b,c\}$$ $$\mathcal{F}=\{\emptyset,\Omega,a,(b,c)\}$$
  $$\mathcal{G}=\{\emptyset,\Omega,b,(a,c)\}$$ With the stochastic
  variable $X$: $$X(a)=0,X(b)=1,X(c)=2$$ Probability measures. P on $(\Omega,\mathcal{F})$: $$P(\emptyset)=0,P(\Omega)=1,P(a)=p,P(b\cup c)=1-p$$ P on $(\Omega,\mathcal{G})$: $$P(\emptyset)=0,P(\Omega)=1,P(b)=p,P(a\cup c)=1-p$$ So far I assume that I haven't messed up. The expectation part though...I'm stuck. I tried the following: $$Y=E[X|\mathcal{G}]$$
$$Z=E[X|\mathcal{F}]$$
$$Y=E[X|\mathcal{G}]=E[X|\mathcal{F}]$$
$$Y=E[X|\mathcal{G}]=E[X|b]1_b+E[X|a\cup c]1_{a\cup c}$$
$$Z=E[X|\mathcal{F}]=E[X|a]1_b+E[X|b\cup c]1_{b\cup c}$$ But after that I can't see how I should progress.","['probability-theory', 'conditional-expectation']"
1653917,Rotating a sphere,"I'm trying to rotate a sphere, and I'm having a bit of a problem calculating the angle to rotate it by.  I wonder if anyone can help me? On my sphere I've marked three points.  If the centre of the sphere is (0,0,0), then the points are where the x, y and z axis exit the sphere. For example: What I would like to do, is rotate the sphere so that an axis (lets say the z axis) exits the sphere such that these three points are all exactly the same distance from the z axis. For example, the z axis would exit the sphere approximately here: This is what I've got so far. First I rotate the sphere by 45 degrees around the x axis: So far, no problem. I then rotate the sphere by -45 degrees around the y axis. At first glance, it appears to have worked: But if I enlarge the circles marked on the sphere, it's obvious that the z axis is not exiting the sphere at the right point: Now I've done a bit of experimenting, and if I rotate the sphere by -35.1, not -45 degrees around the y axis, then it is roughly in the right position. I've spent the afternoon with pen and paper trying to figure out what I should be rotating by, but I just can't figure the exact angle to rotate by. Note: the application for this, is I'm trying to design a small stand to be 3d printed.  I would like the stand to be exactly level. If anyone can help, it'd be much appreciated! Thanks in advance! David.","['rotations', 'trigonometry', 'spheres', 'geometry', 'spherical-trigonometry']"
1653925,How is the second derivitive derived? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question As everyone knows that the derivitive of a function is notated as $\frac{dy}{dx}$ The question is:
How is the second derivitive $\left(\frac{d^2y}{dx^2}\right)$ notation derived?","['derivatives', 'calculus']"
1653930,Find the joint probability density given the support set,"Suppose that the support set of $(X,Y)$ is $$S_{X,Y}=\{(x,y)\in\mathbb{R}^2: x \geq 0 \text{ and } 0 \leq y \leq e^{-x/3}\}$$ $(X,Y)$ is uniformly distributed on $S_{X,Y}$ . a) Find the joint probability density function for $(X,Y)$ . b) Find the marginal PDFs for X and Y. c) Are X and Y independent? Explain. What I have tried a) Is the joint PDF $\int \int e^{-x/3}\text{d}x\text{d}y$ ?. If so, what are the bounds? b) Fix X, integrate over all Y and vice versa. c) Check if the joint PDF is the product of the marginals.","['continuity', 'statistics', 'probability', 'random-variables']"
1653953,Is Lebesgue measure translation invariant?,"I am trying to prove that the Lebesgue measure is translation-invariant. Namely, given a set $X\subseteq\mathbb{R}$ , I'd like to show $X + y$ is measurable and $\mathit{m}(X + y) = \mathit{m}(X)$ . Namely, that the measures -- not the outter measures alone -- agree. I am mostly stuck on demonstrating that the translation $X + y$ is measurable to begin with. Any ideas?","['real-analysis', 'measure-theory', 'analysis']"
1653964,Tridiagonal matrix w/trigonometric eigenvalues,"Let $n$ be a natural number and $B$ be the $n\times n$ square matrix of $0$'s and $1$'s
$$
B=\begin{pmatrix}
 0     & 1 & 0 & \ldots & 0 \\
 1     & 0 & 1 & \ldots & 0 \\
 0     & 1 & \ddots & \ddots & \vdots \\
 \vdots    & \vdots & \ddots & 0 & 1 \\
 0     & 0  & \ldots & 1 & 1 \\
\end{pmatrix}.
$$
 Is there a way to find the $n\times n$ diagonal matrix D, such that the eigenvalues of the product matrix $DB$ are
$$
 2\sin\left(\frac{k\pi}{2n+1}\right), \,\, k=1,2,\dots,n.
$$
 The pertaining old question: Eigenvalues of a tridiagonal trigonometric matrix","['eigenvalues-eigenvectors', 'quantum-mechanics', 'fourier-analysis', 'trigonometry', 'numerical-methods']"
1653979,Is there any integral for the Golden Ratio?,"I was wondering about important/famous mathematical constants, like $e$ , $\pi$ , $\gamma$ , and obviously the golden ratio $\phi$ .
The first three ones are really well known, and there are lots of integrals and series whose results are simply those constants. For example: $$ \pi = 2 e \int\limits_0^{+\infty} \frac{\cos(x)}{x^2+1}\ \text{d}x$$ $$ e = \sum_{k = 0}^{+\infty} \frac{1}{k!}$$ $$ \gamma = -\int\limits_{-\infty}^{+\infty} x\ e^{x - e^{x}}\ \text{d}x$$ Is there an interesting integral * (or some series) whose result is simply $\phi$ ? * Interesting integral means that things like $$\int\limits_0^{+\infty} e^{-\frac{x}{\phi}}\ \text{d}x$$ are not a good answer to my question.","['big-list', 'integration', 'golden-ratio', 'calculus']"
1653985,Possible method to prove infinite twin prime conjecture,"I have an idea looking more and more promising that may lead to proving the infinite twin prime conjecture. My idea would set up a correspondence between primes and twin prime pairs. Since primes have been proven infinite, twin primes would be shown infinite as well. Here it is: For every prime $p>7$ there exists at least one unique twin prime pair $(p_t,p_t+2)$ created using only primes less than $p$ as follows: $$(p_t,p_t+2)=(3\times5\times P_p\times p-4,\ \ 3\times 5\times p\times P_p-2)$$ or $$(p_t,p_t+2)=(3\times5\times P_p\times p+2,\ \ 3\times5\times p\times P_p+4)$$ where $P_p$ is some product of individual primes ($p_n$) and their powers ( although recent developments indicate powers may be unnecessary! ) such that each fits the following condition: $$5<p_n<p$$ Here's a few examples: $(3\times5\times43-4,\ \ 3\times5\times43-2)=(641,643)$ $(3\times5\times7^2\times11\times47+2,\ \ 3\times5\times7^2\times11\times47+4)=(379997,379999)$ My request is for one of the following: Someone to refine our program for a brute force testing method trying to find a counter-example to disprove my conjecture. Here's the code: Original Wolfram Notebook NEW AND IMPROVED Wolfram Notebook Someone to develop a proof of my conjecture; perhaps something related to the fact that the multitude of combinations/permutations etc. of primes ($5<p_p<p$) and their powers requires that there be at least one twin prime pair created. Perhaps a proof by contradiction? I.e. $p$ exists such that no twin prime is created is proven absurd, thus each $p$ maps to a unique twin prime, and as primes are infinite, so are twin primes? Need some help here! Maybe someone with rep to spare set a bounty? EDIT (2/15/16)
Thanks to @dbanet, I now have code needing some refinements. Still, what's astonishing is that we've checked the first $10,000$ primes and each has its own unique twin prime pair... and it didn't even require powers of primes; everything is to the 1st power! This fact alone should lend high credence to the conjecture that each prime may be mapped to (at least one) unique twin prime pair. I'm considering perhaps removing powers of primes from the original question. Here's the list up to 109 for verification. You can check each by adding $4$ or subtracting $2$ from the first in the pair and looking at the prime factors. All will include $3$, $5$ , $p$, and primes between $5$ and $p$ all to the $1$st power (prime#, prime, twin prime): 4, 7, {101},{103}
5, 11, {1151},{1153}
6, 13, {191},{193}
7, 17, {4337},{4339}
8, 19, {281},{283}
9, 23, {347},{349}
10, 29, {431},{433}
11, 31, {461},{463}
12, 37, {17207},{17209}
13, 41, {617},{619}
14, 43, {641},{643}
15, 47, {1225997},{1225999}
16, 53, {37361},{37363}
17, 59, {881},{883}
18, 61, {55817},{55819}
19, 67, {3616997},{3616999}
20, 71, {1061},{1063}
21, 73, {1091},{1093}
22, 79, {6141857},{6141859}
23, 83, {5922461},{5922463}
24, 89, {546625097},{546625099}
25, 97, {1451},{1453}
26, 101, {134837},{134839}
27, 103, {13888001},{13888003}
28, 107, {1607},{1609}
29, 109, {16969661},{16969663} EDIT (2/15/16)PM
Got a new list of twin primes because of https://mathematica.stackexchange.com/questions/107417/memory-limit-hit-optimize-code-for-finding-twin-primes . Here's the list of primes $2000-10000$ with the corresponding twin prime pairs! https://dl.dropboxusercontent.com/u/76769933/8000%20twin%20primes.txt And just for kicks here's the $100,000,000$th prime with the first found (probably not only) twin prime unique to it: $2038074743$ -- $(126984732620985857058143952617,$ $126984732620985857058143952619)$","['prime-numbers', 'linear-algebra', 'proof-verification']"
1653996,"Suppose a function is expressed by: $f(x)=f(x+1) - f(x-1)$ and $f(16)=20 , f(20)=16$ What is $f(20162016)$?","Math quiz bee question Suppose a function is expressed by: $$f(x)=f(x+1) + f(x-1)$$
  and
  $$f(16)=20 , f(20)=16.$$
  What is $f(20162016)$? Attempt at solution: $f(17)+f(15)=20$ $f(21)+f(19)=16$ And if I continue it, I can get the values of $f(21)$ and the other integers near it, but how can I get to the value of $f(20162016)$? This question was from a recent national quiz bee in my place, it is only for $30$ seconds. The correct answer that they said was $-36$. Edit 1: As some pointed out, I think I remembered it wrong and the question was:
$$f(x)=f(x+1)+f(x-1)$$","['recreational-mathematics', 'functions']"
1653999,Show that among all quadrilaterals of a given perimeter the square has the largest area,"Show that among all quadrilaterals of a given perimeter the square has the largest area. By Ptolemy's theorem we have that if $a,b,c,d$ are the side lengths of the quadrilateral then $ac+bd \geq d_1d_2$, which implies that $\text{Area}_{\text{quadrilateral}} \leq \dfrac{1}{2}d_1d_2$ where $d_1,d_2$ are the lengths of the diagonals. I then want to show for a given perimeter the maximal area is obtained for equality of the last inequality. We can't just say that the maximal area is for that of a square based on the last inequality since the maximal may not be achieved for a given perimeter. How do I continue?","['inequality', 'optimization', 'geometric-inequalities', 'geometry', 'area']"
1654001,Implicit 2nd order Runge-Kutta,"I'm familiar with explicit numerical methods for solving ODE including Euler's method, and even Runge-Kutta methods (2nd and 4th order). But I'm really confused when it comes to implicit methods. I understood the ''simplest'' implicit method, being Euler's backward method. I even did a code with it on Matlab and it worked just fine. But, what about implicit Runge-Kutta 2nd order? How can I get the K's that would make it implicit? I've been searching for examples and can't find anything. It would really help if I saw a solution using an implicit RK-2 for a ODE. Is there any book/paper or anything? Let's say we have this simple ODE: Y' = -2Y How would an implicit RK-2 work here?","['numerical-methods', 'ordinary-differential-equations', 'runge-kutta-methods', 'newton-raphson']"
1654046,Show that continuous functions on $\mathbb R$ are Borel-measurable,"Here are my thoughts. We want to show that $f^{-1}$ maps Borel sets to Borel sets. Let $f:\mathbb R\to\mathbb R$ be a continuous function and $\mathcal B(\mathbb R)$ a Borel $\sigma$-algebra. Let $B\in\mathcal B(\mathbb R)$ and $B$ open. Then $f^{-1}(B)$ is open as well, that is, there exists neighborhoods around every point of $f^{-1}(B)$ contained within. Since these neighborhoods are members of $\mathcal B(\mathbb R)$, their union are also members of $\mathcal B(\mathbb R)$. Hence $f^{-1}(B)$ is a member of $\mathcal B(\mathbb R)$. Here's where I'm stuck. I don't know how to show countably many such members can have their unions taken to give me $f^{-1}(B)$. I also don't know what to do with the closed sets in $\mathcal B(\mathbb R)$, especially singletons and those only generatable via complements. Hints?","['continuity', 'real-analysis', 'measure-theory', 'analysis']"
1654059,How to compress a series of numbers into a single number?,"For example, if you have $28,21,11,36$; how can you turn these into a single number such that some one else who knows the way you compressed them can restore the series unambiguously and in the original order with just the output? Attempted general formula: $f(\{a, b, c, d, ...\})=X$ $f^{-1}(X) = \{a, b, c, d, ...\}$",['discrete-mathematics']
1654094,Show that a matrix with (I) a row of zeros and (II) a column of zeros cannot be invertible (respectively),"Show that a matrix with a row of zeros cannot be invertible. Show that
a matrix with a column of zeros cannot be invertible. What I tried: I tried to show that a matrix $A \in M_n (\mathbb{R})$ such that $(A)_{ij} = 0 \forall j \in \mathbb{N}, j\leq n$ and then show that $A A^{-1} \neq I$ but I got stuck.","['matrices', 'linear-algebra']"
1654102,Conditional expectation $E(XY\mid Z)$,"I'm trying to solve the following problem: let $X$ and $Y$ be 2 independent standard normal random variables and let be $Z=X+Y$. Calculate $E(XY\mid Z)$. I tried many approaches, but without getting the result. Any suggestion? We know previously that $Z=X+Y$ and $W=X-Y$ are independent and that $E(X\mid Z)=\frac{1}{2}Z$.","['conditional-expectation', 'probability']"
1654123,Understanding the definition of the direct sum of subspaces of a vector space,I have a question regarding the definition of direct sum of a vector space in relation to subspaces. Definition: A vector space $V$ is called the direct sum of $W_1$ and $W_2$ if $W_1$ and $W_2$ are subspaces of $V$ such that $W_1\cap W_2 = \{0\}$ and $W_1 + W_2 = V$. We denote that $V$ is the direct sum of $W_1$ and $W_2$ by writing $V = W_1\oplus W_2$. Is this definition saying that any vector in $V$ can be written as a linear combination of the vectors in the set $W_1 + W_2$? Thanks!,"['direct-sum', 'linear-algebra', 'definition']"
1654126,Finding coefficient of polynomial?,"The coefficient of $x^{12}$ in $(x^3 + x^4 + x^5 + x^6 + …)^3$ is_______? My Try: Somewhere it explain as: The expression can be re-written as: $(x^3 (1+ x + x^2 + x^3 + …))^3=x^9(1+(x+x^2+x^3))^3$ Expanding $(1+(x+x^2+x^3))^3$ using binomial expansion: $(1+(x+x^2+x^3))^3 $ $= 1+3(x+x^2+x^3)+3*2/2((x+x^2+x^3)^2+3*2*1/6(x+x^2+x^3)^3…..$ The coefficient of $x^3$ will be $10$ , it is multiplied by $x^9$ outside, so coefficient of $x^{12}$ is $10$ . Can you please explain?","['combinatorics', 'binomial-coefficients', 'polynomials', 'functions']"
1654132,The maximum possible size of $R$ is_____?,"A function $f : N^+ → N^+$, defined on the set of positive integers $N^+$, satisfies the following properties: $f(n) = f(n/2)$ if $n$ is even $f(n) = f(n+5)$ if $n$ is odd Let $R = \{i|∃ j : f(j) = i\}$ be the set of distinct values that $f$ takes. The maximum possible size of $R$ is_____? My attempt: Let us assume$: f(1) = x.$ Then, $f(2) = f(2/2) = f(1) = x$ $f(3) = f(3+5) = f(8) = f(8/2) = f(4/2) = f(2/1) = f(1) = x$ Similarly, $f(4) = x$ $f(5) = f(5+5) = f(10/2) = f(5) = y$ So, it will have two values. All multiples of $5$ will have value $y$ and others will have value $x$. It will have $2$ different values. Can you explain in formal way, please?","['algebra-precalculus', 'relations', 'functions']"
1654173,Computing shortest path including specific edge,"Consider the weighted undirected graph with $4$ vertices, where the weight of edge $\{i, j\}$ is given by the entry $W_{i, j}$ in the matrix $W$. $$W =
\begin{bmatrix}
0&2&8&5\\
2&0&5&8\\
8&5&0&x\\
5&8&x&0\\
\end{bmatrix}
$$
The largest possible integer value of $x$, for which at least one shortest path between some pair of vertices will contain the edge with weight $x$ is ______? My attempt: Somewhere, answer is give $12$, and somewhere is $10$. According to me answer is $11$. Since, if we try to reach node_4 to node_3. There are three possible ways: Node_4 → Node_2 → Node_3 $=$ cost $= 8+5=13$ Node_4 → Node_1 → Node_2 → Node_3 $=$ cost $= 5+2+5=12$ Node_4 → Node_3 $=$ cost $= x =$ maximum value should be less than $12 = 11$ Can you explain in formal way, please?","['graph-theory', 'trees', 'algebra-precalculus', 'arithmetic', 'discrete-mathematics']"
1654177,Statistics: Conditional Probability,"$P(A│B)=\frac25$  ,$P(B)=\frac14$, $P(A)=\frac13$. Find $P(A\land B)$ $P(B|A)$ Here is what I did: Part 1. $$P(A\land B) = P(A) \cdot P(B)\\
= \frac13\cdot\frac14=\frac{1}{12}$$ Part 2. $$P(B|A) = \frac{P(B\land A)}{P(A)} = \frac{\frac1{12}}{\frac13}=\frac14$$ I'm not too sure if my answers are right and I will like them checked over. Thanks","['statistics', 'probability']"
1654204,Find $\lim_{n\to\infty} \frac{(n!)^{1/n}}{n}$. [duplicate],This question already has answers here : Finding the limit of $\frac {n}{\sqrt[n]{n!}}$ (11 answers) Closed 4 years ago . Find $$\lim_{n\to\infty} \frac{(n!)^{1/n}}{n}.$$ I don't know how to start. Hints are also appreciated.,"['calculus', 'limits']"
1654208,Elementary 3D geometry,"This is surely trivial, but my old brain can't remember how to do it. Assume a plane. A second plane intersects, forming line $AB$. The angle of intersection is $\theta$. A third plane intersects, crossing $AB$. At the point of intersection, we can project a triangle in Plane 3 whose sides are established by the intersections of Plane 3 with Plane 1 and Plane 2 respectively. If Plane 3 intersects $AB$ at 90 degrees, the vertex of the triangle (at $AB$) has angle $\theta$ -- this should be obvious, and if not, I've described the problem wrong. So suppose the intersection of Plane 3 is not 90 degrees but something else, $\phi$. What is the angle of the vertex of the triangle? (Suggestions for stating this more clearly are very welcome.)","['trigonometry', 'geometry']"
1654217,pdf is defined as $f_X(x)=P(X=x)$ but isn't $P(X=x)=0$,"When we define a probability distribution function, we say:
$f_X(x)=P(X=x)$ and thats equal to some function such as a gaussian But isn't $P(X=x)=0$ for a continuous random variable $X$. 
Is it correct that the height of the pdf function at a specific x represents the likelihood of this $x$.","['stochastic-processes', 'probability-theory', 'probability']"
1654256,Dual map is zero if and only if map is zero,"A problem from Linear Algebra Done Right (Third Ed): Suppose $W$ is finite dimensional and $T \in \mathcal{L}(V, W)$. Prove that $T=0$ if and only if its dual $T' \in \mathcal{L}(W', V')=0$. I am confused by the information that $W$ is finite dimensional. Why is that required, while nothing is said about finite dimensionality of $V$ ? There is a solution here: Does that fact that the dual map is zero imply that the map is zero? but I am more interested in the reason for the assumptions. I was able to solve it (or so I thought) without using a contrapositive argument or the assumption on $W$ which is what worries me. EDIT: Here is my argument. Since T' = 0,
 we get $\phi_w(Tv) = 0 \forall  \phi_w$, which yields Tv = 0 for all v, hence T = 0. I can ""reverse"" the argument to get the converse. What is my error?","['functional-analysis', 'linear-algebra', 'duality-theorems']"
1654273,Maximise difference between mean and median,"I have a set of numbers and I want to maximise the difference between the mean and the median by removing a given number of elements. For instance I have the following set: 0 5 10 45
Mean = 15
Median = 7.5
Mean - Median = 7.5 If we choose to remove 1 element I want to obtain the following 0 5 45
Mean = 16.66
Median = 5
Mean - Median = 11.66 => MAXIMISED We can assume that the set is sorted. Is there an algorithm for doing this?","['means', 'median', 'average', 'algorithms', 'elementary-set-theory']"
1654276,Is there a formula for fibonacci sequence?,"Is there a formula for fibonacci sequence? If yes, how to derive it. I was told in class yesterday about this series, and I want to know if we can generalize it to any n . If you don't know what the series is, It is a function such that $f(n)=f(n-1)+f(n-2)$ and $f(1)=1$ , $f(2)=1$",['sequences-and-series']
1654296,"Integration involving greatest integer function : $\int_0^{\pi} [\cot(x)] \, dx$",What the integral of $$\int_0^{\pi} [\cot(x)]dx$$  where $[\cdot]$ represents greatest integer function. I know integral of $\cot$ is $|\log(\sin(x))|$ but $\log$ is not defined for $0$ or is there something else I'm forgetting?,"['algebra-precalculus', 'integration', 'definite-integrals', 'calculus']"
1654321,If $\tan x$ is not a differentiable function then why does its differentiation $\sec^2(x)$ exists?,"$\tan x$ is not differentiable at $(2n + 1)90$ points, which means function itself is not differentiable. So, why does its differentiation $\sec^2(x)$ exists?","['derivatives', 'trigonometry', 'calculus', 'limits']"
1654369,Power series for complex exponential,"Let $z=x+iy$ where $x,y\in\mathbb{R}$. The exponential function is $$e^z=e^x(\cos{y}+i\sin{y}).$$ Using the power series of $e^x$, $\cos{y}$ and $\sin{y}$, find a power series representation for $e^{z}$. Recall that $$e^{x} = \sum_{k=0}^\infty \frac{x^k}{k!},\quad \cos{y} = \sum_{k=0}^\infty \frac{(iy)^{2k}}{(2k)!},\quad\sin{y} = \sum_{k=0}^\infty \frac{i^{2k}y^{2k+1}}{(2k+1)!}$$ Then \begin{align}
e^{z} &= \left[\sum_{k=0}^\infty \frac{x^k}{k!}\right]\left[\sum_{k=0}^\infty\left(\frac{(iy)^{2k}}{(2k)!}+i\frac{i^{2k}y^{2k+1}}{(2k+1)!}\right)\right]\\
&= \left[\sum_{k=0}^\infty \frac{x^k}{k!}\right]\left[\sum_{k=0}^\infty\left(\frac{(iy)^{2k}(2k+1)}{(2k+1)!}+i\frac{(iy)^{2k}y}{(2k+1)!}\right)\right]\\
&= \left[\sum_{k=0}^\infty \frac{x^k}{k!}\right]\left[\sum_{k=0}^\infty\frac{(iy)^{2k}}{(2k+1)!}\left(2k+1+iy\right)\right]
\end{align} Now I can't really see where to go.","['complex-analysis', 'power-series']"
1654385,Evaluation of $\int_{0}^{1}\frac{\arctan x}{1+x}dx$,"Evaluation of $$\int_{0}^{1}\frac{\tan^{-1}(x)}{1+x}dx = \int_{0}^{1}\frac{\arctan x}{1+x}dx$$ $\bf{My\; Try::}$ Let $$I = \int_{0}^{1}\frac{\tan^{-1}(ax)}{1+x}dx$$ Then $$\frac{dI}{da} = \frac{d}{da}\left[\int_{0}^{1}\frac{\tan^{-1}(ax)}{1+x}\right]dx = \int_{0}^{1}\frac{x}{(1+a^2x^2)(1+x)}dx$$ So we get $$I = \frac{1}{a^2}\int_{0}^{1}\frac{x}{(x+1)(x^2+k^2)}dx\;,$$ Where $\displaystyle k=\frac{1}{a^2}$ Now Using Partial fraction for $$\frac{x}{(x^2+k^2)(x+1)} = \frac{Ax+B}{x^2+k^2}+\frac{C}{x+1} = \frac{(A+C)x^2+(A+B)x+(B+Ck^2)}{(x^2+k^2)(x+1)}$$ So we get $A+C=0$ and $A+B=1$ and $B+Ck^2=0$ So we get $\displaystyle A=\frac{1}{1+k^2}$ and $\displaystyle B = \frac{k^2}{1+k^2}$ and $\displaystyle C=-\frac{1}{1+k^2}$ So $$\frac{dI}{da} = \frac{1}{a^2(1+k^2)}\int_{0}^{1}\left[\frac{x+k^2}{x^2+k^2}-\frac{1}{x+1}\right]dx$$ So $$\frac{dI}{da} = \frac{1}{a^2(1+k^2)}\left[\frac{1}{2}\ln(x^2+k^2)-k\tan^{-1}\left(\frac{x}{k}\right)-\ln(x+1)\right]_{0}^{1}$$ So we get $$\frac{dI}{da} = \frac{1}{a^2(1+k^2)}\left[\frac{1}{2}\ln(1+k^2)-\ln(k)-k\tan^{-1}\left(\frac{1}{k}\right)-\ln 2\right]$$ So So we get $$\frac{dI}{da}=\frac{1}{1+a^2}\left[\frac{1}{2}\ln(1+a^2)-\frac{1}{a}\tan^{-1}(a)-\ln2\right]$$ Now integration of above expression is very lengthy, Is there is any other method If yes Then plz explain here, Thanks","['integration', 'calculus']"
1654405,Checking a solution for $x'' + 4x' + 5 = 0 $,"Given the differential equation $x'' + 4x' + 5 = 0 $, I applied standard methods: $ \lambda^2 + 4\lambda + 5 = 0 \, \, \Rightarrow \lambda_{1,2} = -2 \pm i $ So a complex solution is $\hat x(t) = c_1 e^{(-2+i)t} + c_2 e^{(-2-i)t}$ And a real solution is $x(t) = Re(\hat x(t)) + Im (\hat x(t)) = e^{-2t}(d_1\cos(t) + d_2\sin(t))$ with $d_1 := c_1 +c_2 $ and $d_2 := c_1 - c_2 $. However, if I check my solution, I get a contradiction: $x'(t) = e^{-2t}(-d_1\sin(t) -2d_1\cos(t) + d_2\cos(t) -2d_2\sin(t))$ $x''(t) = e^{-2t}(3d_1 \cos(t) + 4d_1 \sin(t) + 3d_2 \sin(t) - 4d_2 \cos(t))$ If I insert those expressions in my original equation, I obtain $d_1 \cos(t) + d_2 \sin(t) = 1 $. Where's my mistake?",['ordinary-differential-equations']
1654418,Is the scheme-theoretic image stable under taking products?,"Let $f:X\to Y$ be a morphism of schemes and let $Z\subset Y$ be its scheme-theoretic image. If $T$ is any other scheme, consider the induced morphism $g=f\times 1_T:X\times T\to Y\times T$. Question . Is the scheme theoretic image of $g$ equal to $Z\times T$? Let $j:Z\times T\to Y\times T$ be the natural closed immersion. What I need to check is that the kernel of $$p:\mathcal O_{Y\times T}\to g_\ast\mathcal O_{X\times T},$$ which is the ideal of the scheme-theoretic image of $g$, agrees with the kernel of $$q:\mathcal O_{Y\times T}\to j_\ast\mathcal O_{Z\times T},$$ which is the ideal defining $Z\times T\subset Y\times T$. To simplify matters, let us assume $X$ is reduced or noetherian, so that the scheme-theoretic image can be computed affine-locally. So, given an affine open subset $R=\textrm{Spec }B\subset Y\times T$, I am left with checking that the kernel of $$B\to \Gamma\bigl(R,g_\ast\mathcal O_{X\times T}\bigr)=\mathcal O_{X\times T}\bigl(g^{-1}(R)\bigr)$$ agrees with the kernel of $$B\to \Gamma\bigl(R\cap (Z\times T),j_\ast \mathcal O_{Z\times T}\bigr)=\mathcal O_{Z\times T}\bigl(R\cap (Z\times T)\bigr).$$ Now it is probably the time to use that $Z$ is the scheme-theoretic image of $f$, but I do not see exactly how this would help. Does anyone have any suggestion? Thanks in advance!","['schemes', 'sheaf-theory', 'algebraic-geometry']"
1654419,A Homeomorphism is a Bijection between the Underlying Sets and between the Corresponding Topologies?,"I haven't seen it stated as such, which is why I'm raising it here for confirmation. In most references I'm seeing a homeomorphism $\phi$ between topological spaces $(W, \mathscr S)$ and $(X, \mathscr T)$ defined as: a bijection, $\phi: W \to T$ which is continuous, i.e. $\phi^{-1} (T \in \mathscr T) \in \mathscr S$, where $\phi^{-1} (T \in \mathscr T)$ is the pre-image of $T$ whose inverse $\phi^{-1}: T \to W$ is continuous. It's fairly easy to show that a bijection $\phi: W \to T$ gives a bijection $\phi: \mathscr P (W) \to \mathscr P (T)$ between their powersets (with the inverse defined by the pre-image), so condition 3 becomes $\phi (S \in \mathscr S) \in \mathscr T$, i.e. $\phi $ is an ""open map"" Recognizing that a topological space consists of a set and its collection of (open) subsets can't we then just say that a homeomorphism is mapping $\phi: W \to X, \phi: \mathscr S \to \mathscr T$ which is invertible in both cases. Equivalently, since $\mathscr S , \mathscr T$ are also sets, a homeomorphism  is  a bijection between the  underlying sets  and between  the corresponding topologies ?.","['general-topology', 'proof-verification']"
1654420,Let $P$ be a 4-th degree real polynomial with 5 conditions given. How to compute $P(4)$?,"Yesterday I was math tutoring a 18-years old girl. And she asked me for the following problem: given $P\in\Bbb R[X]_4$, i.e. $P$ a real polynomial of degree exactly $4$, such that: $P(1)=0$ It has a relative extrema in the points $x=2,3$, which value is $3$. compute $P(4)$. Now the second condition tells us that $P'(2)=P'(3)=0$ and $P(2)=P(3)=3$. Thus in total I have $5$ linear conditions on the $5$ real coefficients which define $P$, once we write it as
$$
P(x)=ax^4+bx^3+cx^2+dx+e.
$$
I.e. I have a linear system of $5$ equations in $5$ variables, which has (provided the conditions are all indipendent one each other) one solution: thus I'd have identified uniquely my polynomial, hence I could compute easily $P(4)$ and conclude my exercise. My problem is: this girl doesn't know matrices, Gauss elimination and all the linear algebra tool which help to solve quickly this kind of problems, thus in order to solve such a system she should do it by subsitutions and so on, which is really tedious and not instructive (to me, at least), and it seems weird that her teacher gave her such an exercise to solve. Moreover, what is asked is to compute $P(4)$, NOT to determine the polynomial $P$. So I am asking myself: is there another way to do it? A way which avoids all that calculation? I tried to write $P$ as
$$
P(x)=a(x-x_0)(x-x_1)(x-x_2)(x-x_3)
$$
but nothing good came out. Any idea?","['algebra-precalculus', 'polynomials']"
1654431,Why did the author warn 'Don't do it!' on evaluating the limit of $\lim_{x\to 0} \frac{1-\cos(1-\cos x)}{\sin ^4 x}$ this way?,"This is taken from Differential Calculus by Amit M Agarwal: Evaluate $$\lim_{x\to 0} \frac{1-\cos(1-\cos x)}{\sin ^4 x}$$ The question is quite easy using trigonometric identity viz. $1-\cos x = 2\sin^2\frac{x}{2}$ and then using $\lim_{x\to 0} \frac{\sin x}{x}= 1\,.$ The answer is $\frac{1}{8}\,.$ However, after evaluating the limit, the author cautioned as Don't do it! \begin{align}\lim_{x\to 0} \lim_{x\to 0} \frac{1-\cos(1-\cos x)}{\sin ^4 x} & =\lim_{x\to 0} \frac{1-\cos\left(\frac{1-\cos x}{x^2}\cdot x^2\right)}{x^4}\\ &= \lim_{x\to 0}\frac{1-\cos\left(\frac{x^2}{2}\right)}{x^4}\qquad \left(\textrm{As}\, \lim_{x\to 0}\frac{1-\cos x}{x^2}= \frac{1}{2} \right)\\&= \lim_{x\to 0}\frac{2\sin^2 \frac{x^2}{4}}{\frac{x^4}{16}\times 16}\\&= \frac{1}{8}\qquad \textrm{is wrong although the answer may be correct}\,.\end{align} Where is the 'wrong' in the evaluation? Edit: [...] the limit as $x\to 0$ is taken for a subexpression. That's generally invalid. We can't evaluate a limit inside a limit like that. While evaluating limit of a complicated expression one should not replace a sub-expression by its limit and continue with further calculations. Now, consider these limits: $$\bullet \lim_{x \to 4} \log(2x^{3/2}- 3x^{1/2}-1)$$ my book solves this as: $$\log\; [\lim_{x\to 4} 2 x^{3/2}- \lim_{x\to 4} 3x^{1/2} - \lim_{x\to 4} 1]= 2\log 3$$ Another one: $$\bullet \lim_{x\to 1} \sin(2x^2- x- 1)$$ This is solved as; $$\sin\;[\lim_{x\to 1} 2x^2 \lim_{x\to 1} x- \lim_{x\to 1} 1]= \sin 0= 0$$ The following limits are evaluated by first evaluating the limits of sub-expressions . Do these contradict the statement _ you can't take limit of a sub-expression while evaluating the limit of the whole function_?",['limits']
1654434,Geometric interpretation of different types of field extensions?,"In a first course on rings and fields we met the concept of field extensions, especially algebraic ones. The presentation of the material was very algebraic and felt a little lifeless. I was wondering whether there is some geometric way to think of (different types) of field extensions. I am familiar with the basic formalism of schemes and varieties, but I don't know algebraic geometry. In particular, I am curious how to think of splitting fields in geometric terms.","['extension-field', 'splitting-field', 'field-theory', 'algebraic-geometry']"
1654451,"How can I get the value for $\theta$ when $\cos(\theta) = \frac{\sqrt{3}-1}{2\sqrt2}$, $\sin(\theta) = \frac{\sqrt3+1}{2\sqrt2}$","Given 
$$\cos \theta = \frac{\sqrt{3}-1}{2\sqrt2}, \quad \sin \theta = \frac{\sqrt3+1}{2\sqrt2}$$ The book says that the final value is $\theta = \frac{\pi}{4} + \frac{\pi}{6}$. I can of course verify that's correct, but I don't understand how you could arrive at that conclusion from the above given data. Edit: I did think of the idea that the author may have made use of the identity $\cos(a+b) = \cos(a)\cos(b) - \sin(a)\sin(b)$ and guessed the values of $a$ and $b$, but I was hoping there was a better way of doing this.","['algebra-precalculus', 'trigonometry']"
1654460,Inequality $\displaystyle\sum_{k=1}^{n}a_k\cos(kx) < -\frac{1}{\epsilon}\left|\sum_{k=1}^{n}a_k\sin(kx)\right|$ - Miklos Schweitzer,"Prove that for every $\epsilon >0$ there is a positive integer $n$ and positive numbers $a_{1},...,a_{n}$ such that for every $\epsilon < x < 2\pi - \epsilon$ we have: $\displaystyle\sum_{k=1}^{n}a_k\cos(kx) < -\frac{1}{\epsilon}\left|\sum_{k=1}^{n}a_k\sin(kx)\right|$ This problem is from Miklos Schweitzer Competition 2000. I tried to consider the case when $a_{1}=...=a_{n}=1$ . I also tried to find polynomials such that $\displaystyle p(x) \leq -\frac{|p(\sqrt{1-x^2})|}{\epsilon}$ for all $x \in [-1,1]-[-\epsilon,\epsilon]$ but I couldn't make any progress.","['real-analysis', 'trigonometry', 'complex-numbers']"
1654512,Evaluate the integral $\int^{\infty}_{0} e^{-x}x^{100}dx$,"$$\int^{\infty}_{0} e^{-x}x^{100}dx$$ I am sure is something here I can not see, else it is integration by parts 100 times.","['improper-integrals', 'integration', 'definite-integrals']"
1654515,why dual of $l_1$ norm is $l_\infty$ and vice versa,"This might be a very dumb question but I am having a hard time to understand why dual of $l_1$ norm is $l_\infty$ and vice versa. The dual of a norm is denoted $\lVert\cdot\rVert_*$, defined as
$$
\lVert z\rVert_*=\sup\left\{z^Tx\mid\: \lVert x\rVert\leq1\right\}
$$
and $l_1$ norm is,
$$
\lVert x\rVert_1 = \sum(|x_i|)
$$
and $l_\infty$ norm is,
$$
\lVert x\rVert_\infty = \max(|x_1|,...,|x_n|)
$$
I am very confused on dual norm. Any explanation would be very helpful. Thanks","['functional-analysis', 'linear-algebra', 'convex-analysis']"
1654556,Finding $\lim_{x \to 0} \frac{180\sin x}{x}$,"I am in ninth grade so I am an amateur in mathematics and with no training in limits. I self derived this limit to find the value of $\pi$. I imagined a circle to be composed of infinitely small right triangles. The central angle is $x$ and hence the side closest to the circum will have side $r \sin x $ (where $r$ is the radius) There will be $360/x$ such triangles. Comparing this to the already existing circumference formula we will get $$\pi = \lim_{x \to 0}  \frac{180\sin x}{x}$$ But I have seen in Math Stack Exchange 
itself that: $$\lim_{x \to 0}\frac{\sin x}{x}=1 $$ As seen in the first line of the accepted answer to this question here: How to find $\lim_{x\to 0} \frac{1-\cos x \sqrt{\cos 2x}}{x^2}$ But shouldn't it be equal to $\pi/180$ as per my derivation? Where is my mistake?","['trigonometry', 'limits']"
1654572,Analyticity of $\tan(z)$ and radius of convergence,"Define $\tan(z)=\dfrac{\sin(z)}{\cos(z)}$ Where is this function defined and analytic? My answer: Our function is analytic wherever it has a convergent power series. Since (I am assuming) $\sin(z)$ and $\cos(z)$ are analytic the quotient is analytic wherever $\cos(z) \not\to 0$??? Is there more detail to this that I am missing? Without Cauchy is there a way to determine analyticity etc... Further more, once we have found the first few terms as I have $$z+\frac{z}{3}+\frac{2}{15}z^5$$ How do we make an estimate of the radius of convergence? Do I make an observation that the terms are getting close to $0$ and say perhaps the $nth$ root is heading there as well, $\therefore$ $R=\infty$. I'm fairly lost on this part. Thanks for your help!","['complex-analysis', 'analysis']"
1654609,"Processes $(X_t)_{t≥0}$ and $(Y_t)_{t≥0}$ are independent iff $(X_t)_{t∈I}$ and $(Y_t)_{t∈J}$ are independent for all finite $I,J⊆[0,∞)$","Let $\mathcal I:=\left\{I\subseteq[0,\infty):I\text{ is finite}\right\}$ and $(X_t)_{t\ge 0}$ and $(Y_t)_{t\ge 0}$ be stochastic processes on a common probability space taking values in some measurable spaces. Clearly, if $X$ and $Y$ are independent, i.e. $\sigma(X)=\sigma(X_t,t\ge 0)$ and $\sigma(Y)=\sigma(Y_t,t\ge 0)$ are independent, then $\mathcal A_I:=\sigma(X_t,t\in I)\subseteq\sigma(X)$ and $\mathcal B_J:=\sigma(Y_t,t\in J)\subseteq\sigma(Y)$ are independent for all $I,J\in\mathcal I$. How can we show, that 2. implies 1. too?","['stochastic-processes', 'probability-theory', 'independence']"
1654630,How many sewings are there on a soccer ball?,"A soccer ball is obtained by sewing $20$ hexagonal pieces of leather and $12$ pieces of leather of pentagonal shape. A sewing joins together the sides of two adjacent pieces. How many sewings are there ? My effort I was able to solve this problem by realizing that if I count the number of sewings adjacent to the hexagons and the ones adjacent to the pentagons I will be counting each sewing twice. So, the number of sewings is $$\cfrac{120 + 60}{2}=90.$$ Second Approach (this is the one I am asking about) If we count the sewings adjacent to the pentagons we have $12 \cdot 5 =60 $ sewings ,now to count the rest of the sewings I just observe that any other sewing starts at the edge of some pentagon,so I have $60$ other sewings,for a total of $120$ sewings . However this doesn't quite work, but if I look at the picture I have posted above it seems to be correct as I don't have any pentagon sharing a sewing with another pentagon. What am I missing?","['algebra-precalculus', 'combinatorics', 'contest-math', 'polyhedra']"
1654636,Binomial Distribution and Proof Relating to Factorials,I am studying probability and statistics at my university but haven't had a solid math course in awhile(mostly forget algebra dealing with factorials)thus I am stuck with the following proof. According to my book there is a recurrence relation algorithm detailing the following: $P(X=k+1) = c_k * P(X=k)$ with $c_k =[ \frac{(n-k)}{(k+1)} ] * \frac{p}{1-p}$ with X being a binomial random variable. I have the following proof down thus far: P(X=k+1) $=(\binom{N}{K+1}) * p^{k+1} * (1-p)^{n-k-1}=$ As per Binomial definition $= [ \frac{n!}{(k+1)!(n-k-1)!} ] * \frac{p}{1-p} * p^k * (1-p)^{n-k}$ $= [ \frac{n!}{(k+1)!(n-k-1)!} ] * \frac{p}{1-p} * P(X=k)$ And I am left to prove that $\frac{n!}{(k+1)!(n-k-1)!} = \frac{(n-k) }{ (k+1)}$ Any help with steps would be gratefully appreciated or if there is an issue with my proof thus far. Thank You,"['factorial', 'statistics', 'binomial-distribution']"
1654655,Punctured plane is not simply connected,"Adapt the following definition of ""simply connected space"" (taken from Wikipedia ): A space $X$ is simply connected if it's path connected and for any continuous map $f:S^1\rightarrow X$ can be extended to a continuous map $F:D^2\rightarrow X$ such that $F\mid_{S^1}=f$. Then it's well known (and obvious?) that the punctured plane $\Bbb R^2\setminus\{(0,0)\}$ with standard topology is not simply connected, because unit circle is not contractible (i.e. $f$ mapping $S^1$ to unit circle in plane cannot be extended to $F$ as in definition above). However, I have never seen an easy proof of this fact. My question is the following: Is there a proof that punctured plane is not simply connected which doesn't use any results beyond Jordan curve theorem or Jordan–Schoenflies theorem? If not, do you know of some proof of this fact which could nevertheless be considered elementary (in a way the proof of JCT using Brouwer fixed-point theorem is considered elementary)? Thanks in advance.","['general-topology', 'plane-curves']"
1654672,Lipschitz map between metric and normed spaces,"Let be $F:(X,d)\to V$ a map between $(X,d)$ metric space and $V$ normed space, such that for each $f\in V'$ (linear and continuous), $f\circ F$ is lipschitz map. Show that $F$ is a Lipschitz map. I try something like that: Proof: For every $f\in V'$, there are $C_f, L_f >0$ such that, for every $x,y\in X$
$$|f(x)|\le C_f |x|$$
and
$$|f(F(x)-F(y))|=|f(F(x))-f(F(y))|\le L_f d(x,y).$$ If $F(x)-F(y)\neq 0$, by Hahn-Banach theorem, there is $f_{xy}\in V'$ such that $$f_{xy}(F(x)-F(y))=|F(x)-F(y)|.$$ So, $$|f_{xy}(F(x))-f_{xy}(F(y))|=|F(x)-F(y)|\le L_{f_{xy}}d(x,y).$$ I stopped here. How can I to standardize the constant $L_{f_{xy}}$? Is it the good way to solve the problem? Any help? I appreciate.","['functional-analysis', 'normed-spaces', 'metric-spaces', 'lipschitz-functions']"
1654680,"Proof Directional Derivative Exists at (0,0)","Before I post this I would just like to state that I know that there is a very similar question with a very similar function but I've gone through the answer and it doesn't really help me. Consider a function: $f(x,y) = \frac{xy}{x^4+y^4} $ if $x \not = 0$ and $0$ otherwise. Show that the directional derivative exists in any direction at the point (0,0) We've also been given the definition of directional derivative in direction $e = (e_1, e_2)$ as: $ \frac{d f(a,b)}{d e}= \lim_{h \to 0} \frac {f(a + he_1, b + he_2) - f(a,b)}{h}$ Now using this definition I get to the solution as follows: $ \frac{df(0,0)}{de} = \lim_{h \to 0} \frac{\frac{he_1he_2}{(he_1)^4 + (he_2)^4}}{h}$ $ = \lim_{h \to 0} \frac{\frac{h^2e_1e_2}{h^4(e_1^4 + e_2^4)}}{h}$ $ = \lim_{h \to 0} \frac{h^2e_1e_2}{h^5(e_1^4 + e_2^4)}$ $ = \lim_{h \to 0} \frac{e_1e_2}{h^3(e_1^4 + e_2^4)} $ but clearly this limit diverges??? So the limit does not exist? Did I make a stupid algebraic mistake somewhere? Or am I missing something entirely? Thanks.","['derivatives', 'real-analysis', 'calculus', 'multivariable-calculus', 'vectors']"
1654687,Limit of $\left|\sin(n)\right|^{1/n}$,"I'm having trouble showing rigorously what is the limit of $x_n=|\sin(n)|^{1/n}$ in a rigorous manner. What I have shown is that, $x_n$ cannot converge to $0$ and is bounded by $1$, and that should suffice to show that $x_n$ effectively converges to $1$. However, I can't figure out how to formalize this proof, and show it in a rigorous manner. My guess would be to try and show that the limit of $|a_n|^{1/n}$ can be $1$ if $|a_n|$ is bounded by $1$ and does not converge to $0$. I don't know if this more general statement holds, and if it would simplify or complexify the problem.",['limits']
1654692,(Non)Existence of limits,"When we say that a limit of a function does not exist in $\mathbb{R}$ (or some metric space) does it make sense to say that it might exist somewhere else?
[I am trying to think along lines of existence of imaginary roots] If yes. Then give examples especially regarding $\mathbb{R}$.","['real-analysis', 'limits', 'functions', 'metric-spaces', 'analysis']"
1654729,"Parseval relation on inner product space for $\langle x,y \rangle$","Exercise 3.6-4 in Kreyszig asks to show that $\langle x,y \rangle = \sum_k \langle x,e_k \rangle \overline{\langle y,e_k \rangle}$ using the ""Parseval relation"": $\sum_k |\langle x, e_k \rangle |^2 = ||x||^2$, for all $x \in X$, where the $(e_k)$ form an orthonormal set. I'm a bit stumped here. I see how the relation on $||x||^2$ would follow from the first, but not the other way around.",['functional-analysis']
1654742,The set of finite unions of intervals with rational endpoints is countable.,"I don't know how to prove the following: Let $K:=\lbrace G : G$ is a union of finitely many intervals with rational endpoints$\rbrace$. Prove that $K$ is countably infinite. Here is my approach: The set of intervals with rational endpoints is countably infinite as there is a bijection between this set and $\mathbb{Q}\times \mathbb{Q}$. However, I don't know how to continue. I really appreciate any help you can provide.","['cardinals', 'elementary-set-theory']"
1654772,Random walks in $\mathbb{Z}^2$,"Consider a random walk on the integer lattice in the plane. If a “particle” making a random
walk arrives at a lattice point $p = (k_1,k_2)$ at the time $t$, then one of the four neighbors
$(k_1±1, k_2 )$, $(k_1 , k_2 ± 1)$ of p is selected with equal probability $\frac{1}{4}$ . The particle moves to that neighbor at time $t + 1$.
Let $D$ be a region in the plane (a square or a half plane for example), and let $B$ denote its boundary. Let $p$ be a point of $D$, and let $b$ be a boundary point. We’ll denote by $P_p(b)$ the probability that a random walk starting at $p$ exits at $b$, i.e., that $b$ is the first boundary point that is reached. I was wondering if someone could help me answer some questions if the region in the plane that we are considering is a rectangle. 1) What is the probability that a particle starting at $p$ never reaches the boundary?
2) What is the “exit time”, the expected time for a particle starting at $p$ to reach the boundary?
3) How does the exit time depend on $p$?","['probability-theory', 'statistics']"
1654777,Show that sample variance is unbiased and a consistent estimator,"I am having some trouble to prove that the sample variance is a consistent estimator. I have already proved that sample variance is unbiased. I understand that for point estimates T=Tn to be consistent if Tn converges in probably to theta. However, I am not sure how to approach this besides starting with the equation of the sample variance. Any help would be greatly appreciated. Thank you in advance.","['statistics', 'statistical-inference']"
1654782,Can someone provide some examples to illustrate the difference between Pointwise equicontinuity and Uniform equicontinuity?,"I don't know what is with the subject of pointwise and uniform equicontinuity, pretty much all the material you can find online are either: Proofs i.e. pointwise equicontinuity is uniform equicontinuity
provided the domain is compact No distinction made whatsoever between pointwise and uniform equicontinuity Only used to prove Arzela Ascoli and that's the end of the conversation on equicontinuity Can someone please provide a concrete example of a sequence of function that is pointwise equicontinuous but not uniform equicontinuous? Or some examples of what pointwise equicontinuous sequences and some examples of uniform equicontinuous sequences? I hope I am not asking too much. The only example I can think of is the trivial example: $f_n(x) = n$, but sequence is both pointwise and uniform equicontinuous so It doesn't really shine a light on the difference between the two concepts","['real-analysis', 'examples-counterexamples', 'equicontinuity', 'functional-analysis', 'sequences-and-series']"
1654807,Euler exponential continued fraction to compute the trigonometric functions and the golden ratio,"Using the Euler continued fraction for the exponent, which is convergent everywhere on the complex plane: $$e^{-z}=1-\cfrac{z}{1+z-\cfrac{z}{2+z-\cfrac{2z}{3+z-\cfrac{3z}{4+z-\cdots}}}}$$ We can theoretically compute any trigonometric function in the same way (usually, continued fractions only used to compute $\tan(x)$ since it can be expressed as a ratio). For example, using the trigonometric representation of the golden ratio, we can express it as: $$\phi=2 \cos \left( \frac{\pi}{5} \right)=\Re (e^{-i \frac{\pi}{5}})$$ The convergents of $e^{-i \frac{\pi}{5}}$ can be calculated with the following procedure: $$ \begin{bmatrix}p_n \\ q_n \end{bmatrix} = \begin{bmatrix}1 & 1 \\1 & 0 \end{bmatrix} \begin{bmatrix}1-i \frac{\pi}{5} & 1 \\i \frac{\pi}{5} & 0 \end{bmatrix} \dots \begin{bmatrix}n-1-i \frac{\pi}{5} & 1 \\(n-2)i \frac{\pi}{5} & 0 \end{bmatrix} \begin{bmatrix}n-i \frac{\pi}{5} \\(n-1)i \frac{\pi}{5} \end{bmatrix}  $$ $$\phi \approx \Re (\frac{p_n}{q_n})$$ Curiously, this continued fraction converges much faster, than the simple continued fraction for $\phi$: $$\phi=1+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cdots}}}$$ For $20$ iterations we obtain the following error and computation time (in Mathematica). For Euler's continued fraction: $$t=0.00191546~s$$
$$\delta= 2.1295 \cdot 10^{-24}$$ For a simple continued fraction: $$t=0.000310476~s$$
$$\delta= -3.7325 \cdot 10^{-9}$$ As you can see despite the longer computation, Euler's continued fraction gives almost triple precision for the same number of terms compared to the simple continued fraction. Is this way of computing trigonometric functions used anywhere? Is there some continued fraction for trigonometric functions with faster convergence, or is this the best one? Maybe there is a way to further increase the convergence by some transformations? Edit I figured it out - I don't have to use Euler's fraction for the exponential function, I can just use Euler's fraction for $\cos(x)$, which can be made directly from its Taylor series. $$\cos(x)=\cfrac{1}{1+\cfrac{x^2}{2-x^2+\cfrac{2x^2}{12-x^2+\cfrac{12x^2}{30-x^2+\cdots}}}}$$ It obviously converges much faster, because it contains $x^2$, not $x$. The $n^{th}$ coefficient is defined by the rule: $$\frac{(2n)!}{(2n-2)!}$$ As a result for $\phi$ we get: $$\phi=\cfrac{2}{1+\cfrac{\pi^2/25}{2-\pi^2/25+\cfrac{2\pi^2/25}{12-\pi^2/25+\cfrac{12\pi^2/25}{30-\pi^2/25+\cdots}}}}$$ I'm not able to use Mathematica right now, but I will check how speed and error compares to the other two continued fractions. But at least up to $n=4$ we have $\phi=1.618034$ which is correct for $5$ digits. As for the comment below due to Thomas Andrews ""...it is not surprising that continued fractions that allow coefficients outside the positive integers ""converge faster."" I put that in quotes because you are still going to be dealing with expressions in terms of $\pi$..."" - can someone clarify what he means?","['golden-ratio', 'trigonometry', 'continued-fractions']"
1654809,How many numbers between $1$ and $9999$ have sum of their digits equal to $8$? $16$?,"How many numbers between $1$ and $9999$ have sum of their digits equal to $8$? $16$?
Can someone tell me if I got the right answers? I solved both cases and I've got $148$ for $8$ and $633$ for $16$. I solved this problem using $x_1+x_2+x_3+x_4=8$ then $16$. After that I decided to take the case when $1$ number is bigger than $10$, but I'm not sure if that's the way I should do it for $8$. And then I substract the $4$ cases when $x_1=0$ and $7$ cases when $x_1=9$. Thank you.",['combinatorics']
1655828,A commutation between curl and integral,"I have been struggling to understand the only derivation of Ampère's law from the Biot-Savart law for a tridimensional distribution of current (which, needless to say, is not the case of a linear distribution of current discussed here ) that I have been able to find, i.e. Wikipedia's outline of proof , for more than a month with no result. I was not excluding that Wikipedia's outline of proof is one of those cases, whose set I have been told to be non-empy, where physics, at least at some level, renounces the rigour of mathematics, until I was told that the proof is indeed rigourous . Although I have recently asked for an alternative proof of the same entailment, the fact that it is not to be excluded that Wikipedia's outline of proof is rigourous has made me decide, as it has been suggested to me in these comments , to ask for an explanation of the single steps that I do not understand of that outline of proof . My first doubt starts at the first commutation between the integral and curl signs. I know that $$\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}=\nabla_r\times\left(\frac{\mathbf{J}(\mathbf{l})}{|\mathbf{r}-\mathbf{l}|}\right)$$provided that we read the components of $\nabla_r$ as ordinary derivatives (see below) in the sense of elementary multivariable calculus. The outline of proof says that $$\mathbf{B}(\mathbf{r}):=\iiint_V\,d^3l\,\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}=\nabla_r\times\iiint_V\,d^3l\,\frac{\mathbf{J}(\mathbf{l})}{|\mathbf{r}-\mathbf{l}|}.$$What mathematical result justifies that commutation between integral and curl? I heartily thank you for any answer. In order to take the problem on, it is obviously needed to understand what the integral and the derivatives in $\nabla$ are , but I am not sure about what they are. Since theorems such as Stokes' are usually applied when integrating $\nabla\times\mathbf{B}$, I would tempted to believe that the components of $\nabla_r$ are ordinary derivatives of elementary multivariable /vector calculus , but Dirac's $\delta$, which is a tool of the theory of distributions, ""pops out"" at a certain point in the outline of proof , and in the theory of distributions there exist derivatives of distributions which are a very different thing, although they are taken, as far as I know, with respect to the variables written as ""variables of integration"" in the distribution integral notation, while, here, we start with $\nabla_r\times \mathbf{B}$ with $r$, while the integral is $\iiint_V d^3l$ with $l$. As to the integral sign, I would tend to interpretate it as the Lebesgue integral $$\int_V\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}\,d\mu_{\mathbf{l}}$$ or as the limit of a Riemann integral (where $B(\mathbf{r},\varepsilon)$ is the ball centred in $\mathbf{r}$ with radius $\varepsilon$) $$\lim_{\varepsilon\to 0}\iiint_{V\setminus B(\mathbf{r},\varepsilon)}\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}\,dl_1dl_2dl_3$$but, then, if the commutations between integral and differential operators used Wikipedia's outline of proof were licit for this interpretation, we would then conclude that $\iiint_V\,d^3l\, \mathbf{J}(\mathbf{l})\nabla_l^2\left(\frac{1}{|\mathbf{r}-\mathbf{l}|}\right)$ $=\int_V \mathbf{J}(\mathbf{l})\nabla_l^2\left(\frac{1}{|\mathbf{r}-\mathbf{l}|}\right)\,d\mu_{\mathbf{l}}$ $=\int_{V\setminus{\{\mathbf{r}}\}} \mathbf{J}(\mathbf{l})\nabla_l^2\left(\frac{1}{|\mathbf{r}-\mathbf{l}|}\right)\,d\mu_{\mathbf{l}}=\mathbf{0}$ even where $\mathbf{J}(\mathbf{r})\ne\mathbf{0}$, which must not be the case.","['real-analysis', 'distribution-theory', 'physics', 'multivariable-calculus', 'vector-analysis']"
1655854,Divisor of a global section of a line bundle associated to a Weil divisor,"This is a simple question. Let $D$ be some Weil divisor on a non-singular projective variety $V$, $\mathcal{O}(D)$ the associated line bundle. Suppose $s\in H^0(V,\mathcal{O}(D))$ is a global section. How $div(s)$ and $D$ are related? For example on an elliptic curve an elliptic curve $E/K$ over an number field $K$ with have the line bundle $\mathcal{O}(O_E)$ associated to the neutral point of the elliptic curve. Then $H^0(E,\mathcal{O}(O_E))$ is of dimension $1$ generated by a global section of divisor $O_E$. Certainly in full generality we cannot have $div(s)=D$ especially if the divisor is not effective because a global section is regular as it seems to me.",['algebraic-geometry']
1655884,How many integer-sided right triangles are there whose sides are combinations?,"How many integer-sided right triangles exist whose sides are combinations of the form $\displaystyle \binom{x}{2},\displaystyle \binom{y}{2},\displaystyle \binom{z}{2}$? Attempt: This seems like a hard question, since I can't even think of one example to this. Mathematically we have, $$\left(\dfrac{x(x-1)}{2} \right)^2+\left (\dfrac{y(y-1)}{2} \right)^2 = \left(\dfrac{z(z-1)}{2} \right)^2\tag1$$ where we have to find all positive integer solutions $(x,y,z)$. I find this hard to do. But here was my idea. Since we have $x^2(x-1)^2+y^2(y-1)^2 = z^2(z-1)^2$, we can try doing $x = y+1$. If we can prove there are infinitely many solutions to, $$(y+1)^2y^2+y^2(y-1)^2 = z^2(z-1)^2\tag2$$ then we are done.","['diophantine-equations', 'binomial-coefficients', 'number-theory', 'combinatorics', 'pythagorean-triples']"
1655897,How to find which variable impacts the answer the most in this equation?,"In this equation, if two of the variables are held constant, which variable will bring out the maximum positive change in the answer? I tried doing this in excel, but I'm having trouble figuring out if I'm on the right track. I got 27 different scenarios in the excel sheet and sorted by the highest value. Instinct tells me this is not the correct way to determine this. $$S = \frac{AB}{1+C-B}$$",['multivariable-calculus']
1655901,The character group of $G$ for an abelian group $G$.,"Problem Statement : Prove that the one-dimensional characters of a group $G$ form a group under multiplication of functions, i.e. where the group operation is: 
  $$(\chi\cdot\chi')(g)=\chi(g)\chi'(g)$$
  This group is called the character group of $G$, and is often denoted by $\hat{G}$. Prove that if $G$ is abelian, then $|\hat{G}|=|G|$. So, I completely understand how to prove that $\hat{G}$ is a group, but I am unsure about the second part: proving that if $G$ is abelian, then $|\hat{G}|=|G|$. Clearly, $\hat{G}$ is abelian regardless of $G$ being abelian, since $\hat{G}\subset\mathbb{C}^\times$ and $\mathbb{C}^\times$ is commuative. So $G$ being abelian eliminates the possibility that $(\chi\cdot\chi')(gh)=(\chi\cdot\chi')(hg)$ with $gh\neq hg$. So basically, to prove that $|\hat{G}|=|G|$, we must show that for any distinct $g,h\in G$, then $(\chi\cdot\chi')(g)\neq(\chi\cdot\chi')(h)$, but I am unsure how I should approach this type of proof. I was thinking somehow prove that there is an isomorphism between $G$ and $\hat{G}$, but my professor said that constructing a homomorphism between the two groups would be too difficult without using material that we haven't learned yet. Any suggestions for showing that $|\hat{G}|=|G|$ are appreciated!","['abstract-algebra', 'characters', 'representation-theory', 'proof-explanation', 'group-theory']"
1655935,Convergence of the series $\sum\ln(1+\frac{(-1)^n}{n+1})$,"I want to show that the series whose nth term is 
$a_n=\ln(1+\frac{(-1)^n}{n+1})$ is convergent. I wanted to use the limit comparison test to compare it to the $p$ series but $a_n$ is not positive. I thought of writing the power series representation of $a_n$ using the power series representation of $\ln(1+x)$ with $x=b_n=\frac{(-1)^n}{n+1}$ we find that 
$$a_n=b_n-\frac{1}{2}b_n^2+\frac{1}{3}b_n^3-\frac{1}{4}b_n^4+\cdots$$
Now the seris $\sum b_n$ is convergent by the alternating series test and the other terms are all terms of absolutely convergent series but it is an infinte sum, can I say so ? I mean is the infinite sum of convergent series a convergent series ? Is this correct and is there any other way to do it ?",['sequences-and-series']
1655947,Are all operators to or from $\ell_1$ completely continuous?,"Let $E$ and $F$ be two Banach spaces, and let $T \in \mathcal{L}(E, F)$. Consider the following property (P). For every weakly convergent sequence $(u_n)$ in $E$, $u_n \rightharpoonup u$, then $Tu_n \to Tu$ strongly in $F$. Assume that either $E = \ell^1$ or $F = \ell^1$. Does every operator $T \in \mathcal{L}(E, F)$ satisfy (P)? Edit. Here, we denote by $\mathcal{L}(E, F)$ the space of continuous, i.e. bounded, linear operators from $E$ into $F$ equipped with the norm$$\|T\|_{\mathcal{L}(E, F)} = \sup_{x \in E,\,\|x\| \le 1} \|Tx\|.$$","['functional-analysis', 'banach-spaces', 'operator-theory']"
1655958,"Composition of a continuous function and a discontinuous function, can be continous.","Okay, I think I found an example of a continuous function $f$ composed with a discontinuous function $g$, that make a continuous function $h$. Okay let: $f:[0,1]\to [0,1)$ where $f(x)=\begin{cases}x \quad \textrm{if} \quad x\in[0,1)\\ 0 \quad \textrm{if} \quad x=1\end{cases}$ $g:[0,1)\to \mathbb{R^2}$ where $g(x)= (\cos(2\pi x),\sin(2\pi x))$ I am thinking the $h(x)=g(f(x))$ is continuous because, the only discontinuity that could occur is at $x=1$ which doesn't because $\lim\limits_{x\to 1}h(x)=(1,0)=h(1)$. But, I am sort of confused as to if my justification is correct or not.",['real-analysis']
1655976,Sum of series of perfect powers,"Consider the series, $1,2,3,2,5,6,7,2,3,10,11,12,13,14,15,2,17,18,19,20,21,22,23,24,5,26,3,\ldots$ The $i_{th}$ number of the sequence is the least integer $k$ such that $i = k^{\alpha}$ for some $\alpha.$ How do we find the sum of first $N$ terms of the above series. My Approach: Sum of first $N$ terms would be less than $n*(n+1)/2$. 
For each perfect power $k^{\alpha}$ less than equal to $N$, we need to subtract $k^{\alpha}-k$. But I am unable to converge onto the final formula or result.","['summation', 'perfect-powers', 'sequences-and-series']"
1655989,Show given relation $R$ is equivalence relation on $S$,"I will display the exact problem, then my questions.  I have searched to the extremes to figure this out and can't: Show that the given relation $R$ is an equivalence relation on set $S$.
$S$ is the set of ordered pairs of positive integers.
Define $R$ so that $(x_1,x_2)R(y_1, y_2)$ means that $x_1 + y_2 = y_1 + x_2$. Edit: Reflexive: If $(x_1, x_2)R(y_1, y_2)$ means $x_1 + y_2 = y_1 + x_2$ then $(x_1, x_2)R(x_1, x_2)$.  Such that $x_1 + x_2 = x_1 + x_2$.  This is true $\forall$ $\in$ $S$, therefore it is reflexive. Symmetric: If $(x_1, x_2)R(y_1, y_2)$ means $x_1 + y_2 = y_1 + x_2$ then $(y_1, y_2)R(x_1, x_2)$ means $y_1 +x_2 = x_1 + y_2$.  $y_1 +x_2 = x_1 + y_2$ is the equivalent as $x_1 + y_2 = y_1 + x_2$ when $\forall$ $\in$ $S$.  Therefore it is symmetric. Transitive: *If $(x_1, x_2)R(y_1, y_2)$ means $x_1 + y_2 = y_1 + x_2$, suppose   $α=(x_1,x_2)$ and $β=(y_1,y_2)$.  Since it is reflexive and symmetric, $α R β$ $β R α$ and $α R α$ are all true.  Since transitive is $x R y$, $y R z$, and $z R x$ we can say that it is transitive because $α R β$ $β R α$ and $α R α$ are all true. $\infty$ number of equivalence classes of $R$ Are all my reasons here valid and correct? Thank you!","['equivalence-relations', 'relations', 'discrete-mathematics']"
1655992,Give an example of a equicontinuous that does not converge uniformly,"Give an example of a equicontinuous sequence of functions ($f_n$) over a non-compact set $S\subset\Bbb R^n$ converging pointwise to a function $f$ at each $x\in S$, but $f_n$ does not converge uniformly to $f$ over $S$. I'm really stuck on this problem, and I thought about the cases of $f_n(x) = x^n$ with the domain ($0,1$) or $f_n(x) = sin(nx)$ over non-compact set, but I failed to derive an example. Could someone help me to find an example please? Thanks","['continuity', 'real-analysis', 'compactness', 'sequences-and-series']"
1655997,Combinatorics/Probability Insurance accident,An insurance company classifies people as normal or accident prone. Suppose that the probability that a normal person has an accident in a specified year is 0.2 and that for an accident prone person this probability is 0.6. Further suppose that 18% of the policyholders are accident prone. A policyholder had no accidents in a specified year. What is the probability that he or she is accident prone? What I did: $P(\text{Normal&NoAccident}) = 0.82 \times 0.80 = 0.6560$ $P(\text{Accident Prone & No Accident} ) = 0.18 \times 0.94 = 0.1692$ $P(\text{No Accident}) = 0.6560 + 0.1692 = 0.8252$ $P(\text{Accident Prone} | \text{No Accident}) = P(\text{Accident Prone & No Accient}) / P(\text{No Accident}) = 0.2050$ I feel like this is too simple for a probability class. Is there something I am missing?,"['permutations', 'combinatorics', 'statistics', 'probability']"
1656016,How does one show that a given polynomial is bijective?,"Let $P:\mathbb{R} \rightarrow \mathbb{R}$ be a given polynomial function. We wish to determine the bijectivity of $P$. The first thing that comes to mind is showing that the derivative does not change sign. However, in my view, this is not trivial for high-degree polynomials. This problem can be solved easily for specific types of polynomials. Let $P(x)=mx+b$ with $m≠0$, a polynomial of degree one. It is injective since $m(x+c) + b = mx + b \Longrightarrow c=0$. It is clearly surjective since the inverse $P^{-1}(x)=m^{-1}(x-b)$ is defined for all real $x$. Other observations we can make are that if we have a polynomial of the form $x^n + c$ for $n$ odd, this is bijective, that a polynomial of even degree is never bijective, and a degree three polynomial $ax^3 + bx^2 + cx + d$ with $a≠0$ is bijective iff $3ac \geq b^2$. The proofs of these assertions are not difficult. It gets difficult, however, when we have nontrivial odd degree polynomials of $n \geq 5$. This is where I am unsure how to proceed.","['algebra-precalculus', 'polynomials']"
1656036,Why is $\lim_{n\rightarrow\infty}\int f_n \mathrm{d} \mu=\int f \mathrm{d} \mu$ not true,"We know that if $f_n\in \mathcal{L}^1(\mu)$, and $f_n \rightarrow f$ pointwise, it is not necessary true that $\lim_{n\rightarrow\infty} \int f_n \mathrm{d} \mu=\int f \mathrm{d} \mu$.
Some theorems restrict the property of the function sequence to make it work (dominated convergence theorem), my question is what is the essential reason why it is not true?","['real-analysis', 'measure-theory']"
1656066,$\mathbb{G}_m$ action on $\operatorname{Spec}A$ is equivalent to grading... what does coassociativity do?,"I can interpret the other axioms for a grading via the definition of a group scheme action appropriately (I think): Let $H = k[x,x^{-1}]$ . We are given a coaction $ \rho : A \to A \otimes H$ , which is a morphism of $k$ algebras. The counit axiom implies that the coaction map $\rho: A \to A \otimes H$ is injective (it explicitly gives an inverse), hence the direct sum structure on $A \otimes H$ given by $H = \bigoplus_{ n \in \mathbb{Z}} k_{x^n} \otimes_k A$ pulls back via $\rho^{-1}$ to give a direct sum decomposition of $A$ : $A = \bigoplus_{n \in \mathbb{Z}} \rho^{-1} (k_{x^n} \otimes A)$ . That the coaction map is an algebra homomorphism implies that $A_n  A_m \subseteq A_{n + m}$ . These appear to be all of the conditions of a $\mathbb{Z}$ grading on the ring $A$ - namely, that $A$ can be written as a direct sum of submodules graded by $\mathbb{Z}$ which multiply in the correct way. But I haven't used the coassociativity. So I suspect that I am being stupid in some way I would appreciate being pointed out. Or perhaps the point of the coassiativity is to force the coaction to look like $f \to x^n \otimes f$ , for $f$ a degree $n$ homogeneous element in the grading induced by $1$ (i.e. living in $\rho^{-1} ( k_{x^n} \otimes_k A)$ ? However, this seems to come from the conunit axiom anyway, since the counit evaluates $x^n$ to be $1$ . ( $f$ must be sent to some element in $k_{x^n} \otimes_k A$ , i.e. some $ x^n \otimes_k g$ , but conuit implies that it must be the case that $g = f$ .) As for the converse - starting with a graded ring, then defining a coaction by sending a degree $n$ homogeneous element to $x^n \otimes f$ ... it seems that the coassociativity more or less comes for free. So even if one had just a not necessarily associative action of $G_m$ on $\operatorname{Spec}A$ (i.e., a $k$ -algebra coaction by $H$ satisfying just the couit axiom), it appears that it becomes automatically associative (it induces a grading, which then induces an associative coaction, which agrees at least on homogeneous elements). But this feels weird... I'm really confused!","['hopf-algebras', 'algebraic-geometry', 'commutative-algebra']"
1656076,Powerful applications of linear algebra?,"I'd like to see some neat, elegant applications of linear algebra. I'm a undergraduate student but I don't want to prevent people from posting things just because I won't understand them, but if it's undergraduate level even better. Examples: Cayley–Bacharach theorem Every element in a finite extension of a field is algebric Radon's theorem","['applications', 'linear-algebra', 'soft-question']"
1656104,Expected value of the sample covariance,"Let $X = (X_1,\dots,X_p)$ is a random (column) vector with values in $\mathbb R^p$. The covariance matrix $\mathrm{Cov}(X,X)$ is defined by $$\mathrm{Cov}(X) := E[(X-E[X])(X-E[X])^T]$$ By definition the $(i,j)$-component of the covariance matrix is the covariance $\mathrm{Cov}(X_i,X_j)$ of two random variables. If we have $n$ samples $x_1,\dots,x_n \in \mathbb R^p$, then it is known that the covariance matrix is estimated by the following matrix. $$Q = \frac{1}{n-1}\sum_k (x_k-\bar x)(x_k-\bar x)^T$$ Here $\bar x = n^{-1}\sum_k x_k$ is the vector of sample means of random variables $X_1,\dots,X_p$. I would like to know how to see that $Q$ estimates the covariance matrix. The $(i,j)$-component of $Q$ is given by $$\frac{1}{n-1}\sum_k (x_{ki}-\frac{1}{n}\sum_l x_{li})(x_{kj}-\frac{1}{n}\sum_l x_{lj})$$ Here $x_k = (x_{k1},\dots,x_{kp})^T$. To make notation easier, we put $Y=X_i$ and $Z=X_j$. Then I want to prove the following formula $$\mathrm{Cov}(Y,Z) = E\left[\frac{1}{n-1}\sum_{k=1}^n(Y_k-\bar Y)(Z_k - \bar Z)\right]$$ Here $Y_1,\dots,Y_n,Y$ are iid and $Z_1,\dots,Z_n,Z$ are iid. Actually I am very confused by my formulation and I am missing where to start. I understand the case where $Y=Z$, i.e. the sample variance. The biggest problem would be that the relation between $\mathrm{Cov}(Y,Z)$ and $\sigma_{ij} := \mathrm{Cov}(Y_i,Z_j)$ is unclear.","['statistics', 'probability', 'covariance']"
1656106,Determining certain units in a local ring. [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I've been stuck on this problem for a while: Let R be a commutative ring with $1 \neq 0$. If R has a unique maximal ideal (i.e. R is local), then either $x$ or $1-x$ (or both) are units in R.","['abstract-algebra', 'ring-theory']"
1656120,Formula to find the first intersection of two arithmetic progressions,"Formula to find the first intersection of two arithmetic progressions I am not good in math, but I need to determine if two generic arithmetic progressions have an intersection point and, in that case, find the first intersection. I've searched the web and found some solutions, but I couldn't understand them. Is it possible to have a simple formula or algorithm that finds the first intersection point of two arithmetic progressions? Example 1: $$
A_n = A_1 + (n - 1)d \\
AP1: A_1 = 1, d = 14 \Rightarrow \{1, 15, 29, 43, \dotsc \} \\
AP2: A_1 = 8, d = 21 \Rightarrow \{8, 29, 50, 71, \dotsc \} \\
$$
Result: First intersection point on $A_n = 29$ Example 2:
$$
A_n = A_1 + (n - 1)d \\
AP1: A_1 = 1, d = 14 \Rightarrow \{1, 15, 29, 43, \dotsc \} \\
AP2: A_1 = 8, d = 28 \Rightarrow \{8, 36, 64, 92, \dotsc \}
$$
Result: Does not have an intersection point The reason I need this is because I am developing a calendar (like Google Calendar) but where it is not allowed to create two event series that intersect each other. I've posted a similar question here .","['arithmetic-progressions', 'sequences-and-series', 'arithmetic']"
1656136,Example of a ring with an infinite inclusion chain of ideals [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I'm trying to track down an example of a ring in which there exists an infinite chain of ideals under inclusion. (i.e. $I_1 \subsetneq I_2 \subsetneq I_3 \subsetneq...$)","['abstract-algebra', 'ring-theory', 'examples-counterexamples', 'ideals']"
1656146,Why does free imply torsion-free?,"I want to verify that if $R$ is an integral domain and $M$ is an $R$ -module, that if $M$ is free, $M$ must also be torsion-free. Where can I start with this? I feel like it is obvious but I can't see it. I am just getting started with my course.","['abstract-algebra', 'modules']"
1656149,How many subsets of F contain no more than one element from G,"Given: $F = \{Q, K, B, L, I, M\}$ and $G = \{Q, K,B,M\}$, I want to solve: How many subsets of $F$ contain no more than one element from $G$? The reasoning I used was that Choose from: Q, K, B, M - 4 ways. Discard rest of choices - 1 way. L - 2 ways I - 2 ways This gives $ 4 \cdot 1 \cdot 2^2  = 16$ subsets. Also side question is it correct that the number of unique subsets in $F$ would be $ 2^6 $ subsets? Is $\{L, K\}$ the same as $\{K, L\}$?  I was told that sets are unordered thus a unique subset is the same a as subset.","['combinatorics', 'elementary-set-theory', 'discrete-mathematics']"
1656216,What is the winning strategy for this Game on the Power Set,"Given a finite set, players alternately choose proper subsets. Once a subset has been chosen, none of its subsets may be
chosen later. The last player to move wins. I figured out that, with optimal play, Player 1 wins. This is because he can either choose the null set and give away his move, becoming the second player, or not choose the null set and do a ""real"" move, staying the first player. Because he can choose, and one player wins with optimal play, Player 1 must win. However, I can't figure out a strategy for him, other than choosing/not choosing the null set.","['combinatorics', 'combinatorial-game-theory']"
1656218,"Mersenne, Fibonacci... are there other cases in which the existence of a given prime implies the existence of another prime?","Thinking about why it seems impossible to find a case in which the existence of a given prime implies the existence of another bigger prime, I tried to make a list of cases in which knowing that a number is prime then automatically we know that other one is prime. So far I just can recall two well known non-trivial situations: Mersenne numbers, if $p=2^n-1$ is prime then $n$ is prime as well. The opposite is not true. Fibonacci numbers, if $p=F_n$ is prime then $n$ is prime as well. The opposite is not true. In both cases $n$ is smaller than $p$ . I would like to ask the following questions: Did I forget more cases? It seems that the existence of the bigger prime $p$ implies the existence of the smaller prime $n$ but I can not recall any cases in which a single (only one required) smaller prime $n$ implies that a bigger prime $p$ automatically exists. Are there any papers regarding that impossibility? Are there such cases?
Thank you! Update 16/02/2016: as kindly explained in the comments by @DanielFischer and answer by @TitoPiezasIII, there are generalizations of both the Mersenne and Fibonacci numbers that are also part of the list.","['reference-request', 'prime-numbers', 'sequences-and-series', 'elementary-number-theory']"
1656221,Existence of second partial derivative,"let $f: \mathbf{R}^2 \to \mathbf{R}$ be a continuous function. Then can the derivative $\dfrac{\partial^2f}{\partial x \, \partial y}$ can exist without $\dfrac{\partial f}{\partial x}$ existing? I have no idea how to look for this?","['multivariable-calculus', 'real-analysis', 'partial-derivative']"
1656226,How to denote an unordered product of two sets?,"I would like to denote the following set: $$
\bigl\{\{a,b\}:a\in A\text{ and }b\in B\bigr\}
$$ I have found this ( Notation for unordered product of sets ) but I do not know how this is answer my question. The answer says to use the notation $\binom{V}{2}$ but I have two sets $A$ and $B$ not only one $V$. Thanks.","['reference-request', 'notation', 'elementary-set-theory']"
1656252,"For matrices, how to deduce $AB$ using $A^2, B^2, (A+B)^2$ and so on (any matrix squares)?","Assume matrix square can be calculated in $O(n^c)$ time, show that any square matrix multiplication can be done in the same $O(n^c)$ time. The problem I got is that $AB$ and $BA$ always occur together with same coefficients. So that I cannot get $AB$ only. Thanks in advance and any suggestion is welcomed.","['matrices', 'linear-algebra']"
1656270,Can I take Linear Algebra without having learned Vector Calculus?,"I need to take Linear Algebra to progress in my major and the course has Calculus III listed as a prerequisite. I've already taken Calculus III and passed with a C, although I didn't learn the smallest bit of it the entire semester (not a good semester for me). Should I reteach myself Calculus III and hold off on taking Linear Algebra until I do, or would that be a waste of time and should I just go ahead and take the Linear Algebra course? I really don't want to waste time teaching myself something I already took...but if I really need the knowledge, I might.","['soft-question', 'linear-algebra', 'calculus']"
1656281,"Arrange these functions in descending order: $\tan x^{\tan x}$, $\tan x^{\cot x}$, $\cot x^{\tan x}$, $\cot x^{\cot x}$, for $x\in(0,\pi/4)$","Given $$t_1=\tan x^{\tan x}$$ $$t_2=\tan x^{\cot x}$$ $$t_3=\cot x^{\tan x}$$ $$t_4=\cot x^{\cot x}$$ for $x\in(0, \frac{\pi}{4})$ . Arrange them in descending order. I tried in this way: in $(0, \frac{\pi}{4})$ from the graphs of $\tan x$ and $\cot x$ $$\cot x \gt \tan x$$ and in $(0, \frac{\pi}{4})$ both are positive  we have $$\cot x^{\cot x} \gt \tan x^{\cot x}$$ that is $$t_4 \gt t_2$$ and similarly $$\cot x^{\tan x} \gt \tan x^{\tan x}$$ that is $$t_3 \gt t_1$$ Now only comparison left is between $t_4$ and $t_1$ . if $f(x)=x^x$ it is decreasing in $(0, \frac{1}{e})$ and increasing from $(\frac{1}{e}, \frac{\pi}{4})$ so in $(0, \frac{1}{e})$ since $\cot x \gt \tan x$ we get $$\cot x^{\cot x} \lt \tan x^{\tan x}$$ that is $$t_4 \lt t_1$$ hence in $(0, \frac{1}{e})$ $$t_3 \gt t_1 \gt t_4 \gt t_2$$ and in $(\frac{1}{e}, \frac{\pi}{4})$ since $x^x$ is increasing we get $$\cot x^{\cot x} \gt \tan x^{\tan x}$$ that is $$t_4 \gt t_1$$ but i cannot arrange them in decreasing order in $(\frac{1}{e}, \frac{\pi}{4})$","['real-analysis', 'trigonometry']"
1656337,About Second-order Linear Homogenous ODE,"One way to solve second-order linear homogeneous ode with constant coefficients is to do the following things:
$$a\left(\frac{\mathrm d^2}{\mathrm dx^2}\right)f+b\left(\frac{\mathrm d}{\mathrm dx}\right)f+cf=0$$
$$aD^2f+bDf+cf=0$$
$$(D-\lambda_1I)(D-\lambda_2I)f=0$$
$$\Longrightarrow(D-\lambda_1I)f=0\textrm{ or }(D-\lambda_2I)f=0$$
What's the theoretical basis of the last step? This is equivalent to prove that $$\ker(T+\lambda I)\oplus\ker(T-\lambda I)=\ker(T^2-\lambda^2I).$$ However that's not generally true, at least when $\lambda=0$, since it becomes $\ker T=\ker T^2$. Is it due to the particularity of $D$ or does this formula hold unless $\lambda=0$?","['differential-operators', 'ordinary-differential-equations', 'linear-algebra', 'eigenfunctions']"
