question_id,title,body,tags
4698894,"show that uniform probability measure on $\{1,2,...n\}$ does not converge weakly to any probability measure","Let $P_n$ be uniform probability measure on $\{1,2,...n\}$ show that $P_n$ does not converge weakly to any probability measure. To prove this by contradiction, assume $P_n\Rightarrow P$ where, $P$ is some probability measure. Then, by definition of weak convergence, for any continuous bounded function, $f$ $$\int fdP_n\overset{n\rightarrow\infty}{\longrightarrow}\int fdP.$$ Let us take $f:[0,1]\rightarrow[0,1]\backepsilon f(x)=x$ , which is continuous and bounded. Then, $$\int f(x)P_n(dx)=\frac{1}{n}\sum_{i=1}^ni=\frac{n+1}{2}\overset{n\rightarrow\infty}{\longrightarrow}\infty$$ but, $$\int f(x)P(dx)=\int xP(dx)\overset{\text{why?}}{<}\infty.$$ Is the attempt correct. If it is, I am not very clear about why the second integral is always finite. Please help. Note. There is a similar question but it doesn't clear my doubt.","['weak-convergence', 'measure-theory', 'probability']"
4698899,How to calculate $\lim\limits_{x \to 0} \cos\left(2x\right)^{\frac{1}{x^{4}}}$?,"I was hoping for some guidance on how to approach the following limit, $\lim\limits_{x \to 0} \cos\left(2x\right)^{\frac{1}{x^{4}}}$ Now for context, I already know that the answer is $0$ , but I'm trying to  be more formal in how I prove it. The way I have been approaching this is to first convert it into a combination of exponentials and logarithms, which obtains $\lim\limits_{x \to 0} \exp\left(\frac{1}{x^{4}}\ln\left(\cos2x\right)\right)$ However, the problem with this is that we eventually see that the term inside the logarithm basically goes to 0, and thus the term inside the exponential goes to $-\infty$ , because of that, I don't believe this step is completely legitimate, especially as I move the limit in and out of the function later on and apply L'Hopital's rule. So how can one rigorously handle this process? If I play it fast and loose, I can just go through with that step, bring the limit inside of the exponential, apply L'Hopital, take out the $\frac{\sin2x}{2x}$ that pops up (since it equals $1$ ), plug $0$ into the cosine term, and be left with $e^{\lim\limits_{x \to 0}{\left(-\frac{1}{x^{2}}\right)}}$ . If I naively take the limit back out, I can use a very straightforward $\epsilon-\delta$ proof to show that the whole limit equals $0$ . But again, this is the result of having essentially taken the logarithm of $0$ in that earlier step, so I don't think it's legitimate. So, in essence, how can I either navigate around this divergent limit that pops up, or how can I more rigorously approach it if it does. It's been a while since I took real analysis, so I'm kind of shaky on this. Many thanks!","['limits', 'real-analysis']"
4698979,A point of the closed unit ball is the midpoint of two points of the unit sphere,"Let $(E, \lVert \cdot \rVert)$ be a real normed space of dimension $n \ge 2$ . Any point of the closed unit ball $B$ is the midpoint of two points of the unit sphere $S$ . Note: $E$ is not supposed to be Euclidean, and therefore $\lVert \cdot \rVert$ is whatever norm. My attempts I investigated a proof by contradiction and take $x \in B$ . I suppose that $x \neq 0$ , as the result is trivial in that case: $0$ is the midpoint of any diameter of $S$ . I consider the map $$\begin{array}{l|rcl}
f : & E \times E & \longrightarrow & \mathbb R \\
    & (y_1,y_2) & \longmapsto & \left\lVert\frac{y_1 + y_2}{2} - x\right\rVert \end{array}$$ $f$ is continuous for whatever norm equips $E \times E$ and all those norms are equivalent as $E$ is supposed to have a finite dimension. $S \times S$ is a compact subset of $E \times E$ , again because the dimension of $E$ is finite. Therefore $f$ attains its minimum in $S \times S$ at let say $(x_1,x_2)$ . If the minimum value is equal to zero, we're done. So I suppose that $f(x_1,x_2) \gt 0$ , and I have to seek a contradiction. For this, I try to find $(x_1 + \epsilon_1, x_2+\epsilon_2) \in S \times S$ ""close to $(x_1,x_2)$ "" such that $f(x_1 + \epsilon_1, x_2+\epsilon_2) \lt f(x_1,x_2)$ ... without much result so far. Any good idea? Subsidiary question: does the result also hold for infinite dimensional spaces $E$ ?","['general-topology', 'normed-spaces']"
4698999,Obtain gravity direction by observing a pendulum's movement,"Lets say a damped pendulum takes 10 seconds to slow down and stop.  In that time it might swing past vertical perhaps 25 times or so. We have an accurate clock that measures absolute time reliably.  We are given very accurate  speed and direction data (+/- radians per second for instance) for the pendulum in real-time.  But we don't know what absolute angle the pendulum started at. Is it possible to accurately determine which direction is ""down"" (e.g. gravity's local direction) by observing the motion of a swinging damped pendulum before it has stopped swinging? Background: Please bear with me since I'm not well versed in mathematics, I realise that this question isn't rigorously defined.  This problem is based in a real situation I'm facing with some electromechanical apparatus.  A pendulum suffers from friction in its motion which generally prevents it from finding vertical once it finally comes to rest (usually it's a degree or two to the left or right, somewhat randomly).  If it is possible to find ""down"" mathematically from the dynamic properties of a pendulum before it stops, then this would be a tremendous help.","['physics', 'calculus', 'circular-motion']"
4699000,"$f:[0,1]\to \mathbb R$ is differentiable, prove that $f$ must be either linear or $|f(1)-f(0)|<|f'(t)|$ for some $t\in (0,1)$","A function $f:[0,1]\to \mathbb R$ is such that $f$ is differentiable in it's domain.
Prove that $f$ , is either linear in $x$ or $$|f(1)-f(0)|<|f'(t)|$$ for some $t\in (0,1)$ My attempt: By Rolle theorem , we know that there exist atleast one $t\in (0,1)$ where the equality holds $$|f(1)-f(0)|=|f'(t)|$$ Now , if such equality would have hold for all $t$ in the domain, then clearly $f$ is linear as the slope remains constant. Now, I try to show that opposite inequality holds false i.e. : $$f(1)-f(0)>f'(t)$$ By integrating both sides, we get that : $$\int_0^1f(1)-f(0) \mathrm dt>\int_0^1 f'(t) \mathrm dt$$ $$f(1)-f(0)>f(1)-f(0)$$ which is a contradiction ! Problems: I had integrated without taking absolute value as asked in question and hence proof is incomplete. This was the last question in my test and hence was toughest. I believe that my (incomplete) proof is not right in itself because it's too simple. Can you help me to solve it in correct way ?","['maxima-minima', 'calculus', 'definite-integrals', 'derivatives']"
4699027,Hartshorne's definition of very ample sheaves,"I wonder if the definition of very ample sheaves of Hartshorne coincides with the definition of Stack projects. Let $f: X \to S$ be a morphism of schemes. Let $\mathcal{L}$ be an invertible $\mathcal{O}_ X$ -module. Assume that $X$ is noetherian so that Hartshorne's definition of ""immersion"" coincides with projects'. Definition of Stack projects: We say $\mathcal{L}$ is relatively very ample if there exists a quasi-coherent $\mathcal{O}_ S$ -module $\mathcal{E}$ and an immersion $i : X \to \mathbf{P}(\mathcal{E})$ over $S$ such that $\mathcal{L} \cong i^*\mathcal{O}_{\mathbf{P}(\mathcal{E})}(1)$ . Definition of Hartshorne: We say $\mathcal{L}$ is very ample relative to $S$ if there is an immersion $i: X \to \mathbf{P}_S^r$ for some $r$ , such that $\mathcal{L} \cong i^*(\mathcal{O}(1))$ .","['algebraic-geometry', 'sheaf-theory']"
4699065,Prove whether $\frac{a_0}{a - a_1} < - \frac{b}{( 1 - \frac{c}{\theta} ) a}$ always holds,"I am having a hard time proving which term is bigger: \begin{align}
\frac{a_0}{a - a_1} \quad \text{vs} \quad - \frac{b}{\left( 1 - \frac{c}{\theta} \right) a} 
\end{align} where: \begin{align}
c = \lambda (\epsilon - 1)(1+\varphi) > 0 \tag{1}
\end{align} \begin{align}
a &= \frac{- \delta \theta + \sqrt{\delta^2 \theta^2 + 4 c ( \epsilon - 1) (1 + \varphi) }}{2 c} \nonumber \tag{2}
\end{align} \begin{align}
a_1 &= \frac{- \delta \theta + \sqrt{\delta^2 \theta^2 + 4 \theta (\epsilon - 1) (1 + \varphi)}}{2 \theta}  \tag{3}
\end{align} \begin{align}
a_0 &= - \frac{b a_1}{\delta + a_1} \nonumber \tag{4}
\end{align} The following restrictions must be respected: \begin{align}
0 &< c < \theta \nonumber
\end{align} \begin{align}
b > 0 
\end{align} \begin{align}
a_0 < 0
\end{align} \begin{align}
0 < a_1 < a
\end{align} and $\epsilon > 1$ , $\varphi > 0$ , $\delta > 0$ , $\theta > 0$ , $\lambda > 0$ . Does anyone have a good idea to approach this? I am interested in understanding whether one term is always bigger than the other, rather than whether one term can be bigger than the other under some specific parameters. My attempt: I have reduced the problem to the following: \begin{align}
\frac{a_0}{a - a_1} &< - \frac{b}{\left( 1 - \frac{c}{\theta} \right) a} \\
- \frac{b a_1}{(\delta + a_1)(a - a_1)} &< - \frac{b}{\left( 1 - \frac{c}{\theta} \right) a} \nonumber \\
\frac{a_1 a}{(\delta + a_1)(a - a_1)} &> \frac{\theta}{\theta - c } \nonumber \\
\end{align} I have been trying to solve this inequality but with little success.","['algebra-precalculus', 'proof-writing', 'linear-algebra', 'inequality']"
4699085,Fair price: receive sum of the last decreasing sequence,"This is a Jane Street Interview question. What is the fair price of a game in which you shuffle nine cards labeled 1 through 9 then keep choosing whether to open the next card or stop, given that you will receive the sum of the last decreasing sequence? So far, I have solved for the expected value of having $2$ cards which is $\frac12 \times 3$ and the expected value of having $3$ cards, which is $2 \times \frac23$ . My question is, how do we even determine the expected value of the decreasing sequence in the first place. I cannot find anything online about this. After solving that, I believe we can simply compare expected values to determine optimal stopping.","['expected-value', 'sequences-and-series', 'game-theory', 'problem-solving', 'probability']"
4699113,"Is it true that $p^2 + 2 \sqrt{3}r \cdot p \geq 20Rr + 5r^2$ for an acute triangle with semiperimeter $p$, inradius $r$, and circumradius $R$?","I was wondering if the following inequality is true (and if so, why): $$p^2 + 2 \sqrt{3}r \cdot  p \geq 20Rr + 5r^2$$ for an acute triangle with semiperimeter $p$ , radius of incircle $r$ , and circumradius $R$ . My approach: I tried to prove this with Gerretsen $p^2 \geq 16Rr - 5r^2$ iff $ \sqrt{3} p\geq 2R + 5r$ which is clearly false. Then, maybe apply Euler inequality $R \geq 2r$ , by proving that $p^2 + 2\sqrt{3} rp \geq 20Rr + 2Rr + r^2$ iff $p \geq (R+r) \sqrt{3}$ which doesn't look true at all (a weird Mitrinovic-like inequality). And I'm stuck since all approaches that tackle directly the RHS will at some point diverge to $R \leq2r$ which is clearly false. Is that because I am taking the ""wrong road"" or simply because the inequality is false?
Any help is appreciated. P.S. I want to use the fact that I am considering an acute triangle, but I do not know how.","['inequality', 'geometry']"
4699128,Prove that $\int_{X>\alpha}XdP\le\alpha(-\log F(\alpha))+\int_\alpha^\infty(-\log F(t))\lambda(dt)$,"Prove that $$\int_{X>\alpha}XdP\le\alpha(-\log F(\alpha))+\int_\alpha^\infty(-\log F(t))\lambda(dt)\le\frac{1}{F(\alpha)}\int_{X>\alpha}XdP$$ While attempting to do this, using basic substitution and Fubini's, I ended up with $$\int_{X\alpha}XdP=\alpha(1-F(\alpha))+\int_\alpha^\infty(1-F(t))\lambda(dt)$$ Now, however using the fact that $F(\alpha)\in[0,1]$ , is it possible to show that $F(\alpha)-\log F(\alpha)\ge1$ which would complete the first inequality. $x-\log x$ is a decreasing function and at $x=1$ , it is exactly equal to $1$ . Is this correct? What about the second inequality? Any help?","['measure-theory', 'lebesgue-measure', 'probability-distributions', 'lebesgue-integral', 'expected-value']"
4699130,Derivative of a periodic function with respect to a parameter,"Assume we have a real periodic function $f(x)$ with some fundamental period $P$ . Let us introduce a coordinate transformation $x=ay$ , where $a \in \mathbb{R}$ and $a \neq 0$ . Then, on the one hand, $$
\frac{\partial f}{\partial a} = \frac{\partial f(ay)}{\partial (ay)} \frac{\partial(ay)}{\partial a} = \frac{\partial f(x)}{\partial x} y = a^{-1} \frac{\partial f(x)}{\partial x} x
$$ which is clearly not periodic (due to the $x$ term). On the other hand, I expect $\partial_a f$ to also be periodic in $x$ (see this question for example). Perhaps the latter statement is invalid in case of multivariable functions? What kind of requirement should one impose on coordinate transformations to make $\partial_a f$ periodic? Is there a general statement that is valid for any kind of coordinate transformation that depends on a parameter (i.e. $y=g_a (x)$ )?","['periodic-functions', 'calculus', 'derivatives']"
4699142,span of maximal orthonormal sets in a Hilbert space,"I am working through Walter Rudin's Real and Complex Analysis and I'm having trouble understanding something about Theorem 4.18: I understand the proof of the theorem, but I'm confused about the relationship between conditions (i) and (ii).  If $ \{u_\alpha\}$ is a maximal orthonormal set in H, that means that there are no extra vectors in H that we can add to $ \{u_\alpha\}$ which is orthogonoal to every vector in $ \{u_\alpha\}$ . For example, $R^3$ equipped with the dot product is a Hilbert space, and $\{u_\alpha\}=\{(1,0,0),(0,1,0),(0,0,1)\}$ would be a maximal orthonormal set in $R^3$ .  But in this case, the set of all finite linear combinations of members of $\{u_\alpha\}$ would not just be dense in H, but is equal to H.  My question is, wouldn't this be the case for every Hilbert space, and if not, could someone provide a good example to help build intuition as to how that works?","['hilbert-spaces', 'orthonormal', 'functional-analysis']"
4699189,Every Cauchy sequence can be written as the sum of an increasing Cauchy sequence and a decreasing Cauchy sequence,"I'm trying to solve the following exercise. Let $K$ be an ordered field and $(x_n)_{n\in \mathbb{N}}$ be an Cauchy sequence in $K$ . Show that there exists an increasing Cauchy sequence $(y_n)_{n\in \mathbb{N}}$ and a decreasing Cauchy sequence $(z_n)_{n\in \mathbb{N}}$ with $$x_n=y_n+z_n.$$ If the field were complete, it would be easier. But I'm not sure how to construct $(y_n)_{n\in \mathbb{N}}$ and $(z_n)_{n\in \mathbb{N}}$ without an existing limit.","['cauchy-sequences', 'analysis', 'real-analysis']"
4699217,"How to prove that if a $3 \times 3$ matrix has two equal rows, it has no inverse?","In my maths classes in school we have said that if a matrix has two equal rows then it has no inverse. I can see this by calculating that the determinant of any $3 \times 3$ matrix with two equal rows is always $0$ . I would like to know how if you have some $3 \times 3$ matrix with two equal rows, you can prove that the columns are linearly dependent, and that the matrix therefore has no inverse. I am going off the definition of linear independence being that you cannot have a linear combination that equals the zero vector. PS. I am a total beginner to linear algebra so I may have completely messed up some simple stuff in the question.","['matrices', 'linear-algebra', 'vectors', 'inverse']"
4699219,"Finding $\alpha$ and $\beta$ such that the graph of $g(x)=2\sqrt{x-\alpha}+\beta$ passes through $(3, -2)$, $(4,0)$, $(7,2)$","I've a homework problem that I'm struggling to solve. I need to find the value of $\alpha$ and $\beta$ that will make the function $$g(x) = 2\sqrt{x-\alpha} + \beta$$ pass through $(3, -2), (4,0), (7,2)$ . The best I got was try to convert the function to a straight line form by have $\alpha=x-x^2$ so that we left with $g(x) = 2x+\beta$ but I can't figure out what $\beta$ should be? Maybe my assumption that the graph should be a straight line is wrong? Thank you!",['algebra-precalculus']
4699235,Does the following non-measurable function exist?,"Does there exist a function $f:[0,1]\to[0,1]$ such the graph of $f$ is dense in $[0,1]\times[0,1]$ , and there exists a $M>0$ such for all $0<\epsilon_1,\epsilon_2 \le M$ and for all $0 < y_1 < 1$ , if $0\le y_1-\epsilon_1< y_1< y_1+\epsilon_2 \le 1$ then the pre-image of $[y_1-\epsilon_1,y_1+\epsilon_2]$ under $f$ is non-measurable in the sense of Caratheodory? If such a function exists, could we explictly define it? I'm not sure how to proceed. Any suggestions would be great. Attempt: Someone gave my question a second chance. I have a function in mind but it's complicated to describe. I'm not sure  the function gives what I want at the beggening of this post. My only evidence is a graph of countable points of the example at the end of the post. I need a simpler function. Suppose the base- $3$ expansion of real numbers, in interval $[x_1,x_2]$ , have infinite decimals that approach $x$ from the right side so when $x_1=x_2$ we get $f(x_1)=f(x_2)$ . Furthermore, for $\mathbb{N}\cup\left\{0\right\}=\mathbb{N}_{0}$ , if $r\in\mathbb{N}_{0}$ and $\text{digit}_{3}:\mathbb{R}\times \mathbb{Z}\to\left\{0,1,2\right\}$ is a function where $\text{digit}(x,r)$ takes the digit in the $3^{r}$ -th decimal fraction of the base- $3$ expansion of $x$ (e.g. $\text{digit}_{3}(1.789,2)=\text{digit}_{3}({1.210022{\cdot\cdot\cdot}}_{3},2)=1$ ), then $\left\{{g_r}^{\prime}\right\}_{r\in\mathbb{N}_{0}}$ is a sequence of functions (and $\left[\cdot\right]$ is the nearest integer function) such that ${g_r}^{\prime}:\mathbb{N}_0\to\mathbb{N}_0$ is defined to be: \begin{equation}
g_r^{\prime}(x)=\left[\frac{10}{3}\sin(rx)+\frac{10}{3}\right]
\end{equation} then for some $k:\mathbb{N}\to\mathbb{R}$ where $k(0)$ is a positive number and $k$ is strictly increasing such if $x_1,x_2\in\mathbb{R}$ , the intermediate function (before $f$ ) or $f_{1}:[0,1]\to[0,10]$ satisfies the problem at the beggenning of the post if the range is $[0,10]$ instead of $[0,1]$ . \begin{alignat}{2}
& f_{1}(x)  = &&\left|\left(\sum\limits_{r=0}^{\infty} g_{r+1}^{\prime}\!\left(\sum\limits_{p=r}^{r+k(r)}\text{digit}_{3}(x,p)\right)\!\!\bigg/3^{r}\right)-10\right|= \label{eq:025} \\
& && \left|\left(\left(\sum\limits_{r=0}^{\infty}\left[\frac{10}{3}\sin\left(\left(r+1\right)\left(\sum\limits_{p=r}^{r+k(r)}\text{digit}_{3}(x,p)\right)\right)+\frac{10}{3}\right]\right)\!\!\bigg /3^{r}\right)-10\right| \nonumber
\end{alignat} (One example of $k(r)$ that may satisfy the problem i.e. if the range is $[0,10]$ , is $k(r)=10r+20$ ) What we did was convert every digit of the base- $3$ expansion of $x$ to a pseudo-random number that is non-equally likely to be an integer, including and in-between, $0$ and $20/3$ . Furthermore, we attempt to make the function dense in $[0,1]\times[0,10]$ , by adding the $3^{r}$ -th decimal fraction with the next $k$ decimal fractions; however, we want to control the end-points of $[0,10]$ such that $f_1$ is dense in $\left[0,1\right]\times\left[0,1\right]$ (instead of $\left[0,1\right]\times[0,10]$ ) by manipulating $f_1$ to get: \begin{alignat}{2}
& f(x)  = && 1-\frac{1}{10}f_1(x)\label{eq:109}\\
& &&  1-\left(\frac{1}{10}\right)\left|\left(\left(\sum\limits_{r=0}^{\infty}\left[\frac{10}{3}\sin\left(\left(r+1\right)\left(\sum\limits_{p=r}^{r+k(r)}\text{digit}_{3}(x,p)\right)\right)+\frac{10}{3}\right]\right)\!\!\bigg/3^{r}\right)-10\right| \nonumber
\end{alignat} (e.g. $k(r)=10r+20$ ) you can use programming to visualize $f$ though I don't know if you can graph the entire function. (The programming I used is mathematica.) Clear[""Global`*""]
k[r_] := k[r] = 
  20 (* You can adjust k[r]; however, mathematica is unable to graph \
f when k[r] is steepy increasing e.g. for this function, k[r] must be \
less than 25 for the code to show a graph. *)

g1[xr_, r_] := 
 g1[xr, r] = 
  Round[(10/3) Sin[r xr] + (10/
      3)] (*Converts the (3^r)th decimal fraction,in the base 3 \
expansion of the x-values in[x1,x2] (defined as xr or x_r not x*r) \
into a psuedo-random number that's non-equally likely to spit a \
number between,and including, 0 and 20/3 *)

f[x_] := f[x] = 
  N[1 - ((1)/(10)) RealAbs[
      Sum[g1[Sum[
           RealDigits[x, 3, k[r], -r][[1]][[z]], {z, r + 1, k[r]}], 
          r + 1]/(3^r), {r, 0, 8}] - 
       10]] (*Defines function f,I assume the larger k[r]'s values, the more \
the function appears dense in [0,1]x[0,1]*)

p = .00005 (*Incremement between the x-values in the points of the \
graph below*)

ListPlot[Table[{x, f[x]}, {x, p, 1, 
   p}]] (*Graphs countable points of the functions but is not a \
complete accurate graph. There are uncountably many points that need \
to be included.*) Unfortunately, I only studied up to intro to advanced mathematics. This could be non-sense. (Without a deep undestanding of math I'm unable to prove if the function gives what I'm looking for.) Is there a simpler example?","['measure-theory', 'logic', 'real-analysis', 'definition', 'functions']"
4699238,Anti-derivative of $ |x^2-1|$,"I ran into a problem of finding the anti-derivative of a function which involves the absolute value function. Here is the problem: Question: If $v(t) = \langle|t^2-1|,4t-3\rangle$ is the velocity vector, find $s(t)$ the position vector. Given that $s(0) = \langle 1,1\rangle$ . My answer: $s(t) = \displaystyle \int_{0}^t v(x)dx$ . Do we split this into $2$ cases ? $0 \le t \le 1$ and $t \ge 1$ ? To be quite honest here my elementary differential geometry is quite rusty. I would appreciate to see a full analysis on this. Thanks. WY.",['multivariable-calculus']
4699266,A proof for $\left ( \lim_{x\rightarrow 0} {sin(x) \over x} \right )$ using Euler reflection formula.,"I found a proof for $\left( \lim\limits_{x\rightarrow 0} {\sin(x) \over x} \right)$ using Euler reflection formula I would like to ask if this proof can be strong mathematical proof? so let me start by
Euler reflection formula. $${\pi \over {\sin(\pi a)} } = \Gamma (a)\Gamma (1-a)$$ where $a\notin \mathbb{Z}$ let $x = \pi a \Rightarrow a = {x \over \pi}$ So the formula can be written as: $$\sin(x) = {\pi \over {\Gamma\left({x \over \pi}\right) \Gamma\left(1 - {x \over \pi}\right)}}$$ where ${x \over \pi}\notin \mathbb{Z}$ Thus, we have. $$ \lim_{x \rightarrow 0}{\sin(x) \over x} = \lim_{x \rightarrow 0}{1 \over {{x \over \pi}\Gamma\left({x \over \pi}\right) \Gamma\left(1 - {x \over \pi}\right)}}$$ $$= \lim_{x \rightarrow 0}{1 \over {\Gamma\left({{x \over \pi} + 1}\right) \Gamma\left(1 - {x \over \pi}\right)}}$$ $$ = {1 \over {\Gamma({0 + 1}) \Gamma(1 - 0)}}  = {1 \over {\Gamma({1})^2}} = 1$$",['limits']
4699269,Examples and Counterexamples of Relations which Satisfy Certain Properties,"Definition: Given a set $X$ , a relation $R$ on $X$ is any subset of $X\times X$ .  A relation $R$ on $X$ is said to be reflexive if $(x,x) \in R$ for all $x \in X$ , irreflexive if $(x,x) \not\in R$ for all $x \in X$ , transitive if $(x,y) \in R$ and $(y,z) \in R$ implies that $(x,z)\in R$ , intransitive (or antitransitive) if $(x,y) \in R$ and $(y,z) \in R$ implies that $(x,z)\not\in R$ , symmetric if $(x,y) \in R$ implies that $(y,x) \in R$ , antisymmetric if $(x,y) \in R$ and $(y,x) \in R$ implies that $x=y$ . Given any combination of the properties listed above, is there a nontrivial (i.e. nonempty) relation which satisfies that combination of properties?","['elementary-set-theory', 'relations', 'examples-counterexamples', 'faq']"
4699271,Exact sequence and short exact sequence,"In group theory, a sequence $$
G_0\stackrel{f_1}{\longrightarrow} G_1 \stackrel{f_2}{\longrightarrow} G_2 \stackrel{f_3}{\longrightarrow}\; . . .\, \stackrel{f_n}{\longrightarrow} G_n
$$ of groups and homomorphisms is called exact if the image of each homomorphism is the kernel of the subsequent one: $$
\operatorname{Im}f_i=\,\operatorname{Ker}f_i\quad,\qquad\mbox{for}\quad\mbox 1\leq i < n\;.
$$ Also see this picture borrowed from Wikipedia . (Here the sequence is set infinite in both directions, though.) If some morphism $f_{i+1}$ in this figure is a monomorphism, then its Ker is $e_i$ . After this figure, Wikipedia provides three examples: Example (a). In $\,e_0 \longrightarrow X \longrightarrow Y\,$ , $\;$ the image of $\,e_{\textstyle{_0}}$ is $\,e_{\textstyle{_X}}\,$ , $\;$ wherefore $\, X \longrightarrow Y\, $ is a monomorphism. QUESTION: What is the meaning of the left arrow, $\,e_{\textstyle{_0}} \longrightarrow X\,$ ? I thought, there should be some initial group $G_0$ on the left (so we would get $\,G_0 \longrightarrow X\,$ ). Or is it implied that this $G_0$ is trivial, i.e., consists of the neutral element $e_{\textstyle{_0}}$ solely? Example (b). In $\,X \longrightarrow Y \longrightarrow e\,$ , $\;$ the kernel of the rightmost map is $Y$ . $\;$ Hence the image of the map $X\longrightarrow Y$ is all of $Y$ . So this is an epimorphism. Example (c) is a combination of the former and latter ones. The sequence $0 \longrightarrow X \longrightarrow Y \longrightarrow 0\,$ is exact iff the map $X \longrightarrow Y$ is both a monomorphism and epimorphism (i.e., is an isomorphism). Wikipedia also defined a short exact sequence as $$
0\longrightarrow A \stackrel{f}{\longrightarrow} B \stackrel{g}{\longrightarrow} C \longrightarrow 0
$$ where $f$ is a monomorphism and $g$ is an epimorphism. I guess, "" $0$ "" on the left is the neutral element $\,e_{\textstyle{_0}}$ of the initial group, while "" $0$ "" on the right is the neutral element $e$ of the final group -- is that right? Based on Example (b) above, $g$ is epimorphic (surjective) in this short exact sequence. Based on Example (a), $f$ is monomorphic (injective), assuming that $\,e_{\textstyle{_0}}$ (denoted with "" $0$ "" on the left) is mapped to $\,e_{\textstyle{_A}}$ . But then I again fail to grasp the meaning of the leftmost arrow: why is the preimage of $A$ just a neutral element of the initial group? And why is it mapped exactly to the neutral element of $A$ ? I am missing something big here.","['group-theory', 'exact-sequence', 'finite-groups', 'category-theory']"
4699280,Formula for $\pi$: what did Doron Zeilberger mean instead of $\pi = \lim_{n} \frac{4^n}{{2n \choose n}n}$?,"I just watched the Vimeo video ""What is Pi and what it is not"" where Doron Zeilberger talks about $\pi$ . At around 16:00, he writes this on the blackboard: $$\pi = \lim_{n} \frac{4^n}{{2n \choose n}n}$$ But that doesn't work. As $n$ increases, this expression seems to tend towards $0$ . Does anyone have a clue as to what he might have meant to write? I'm guessing the expression needs only a little adjustment.","['limits', 'pi']"
4699285,Minimum of n iid chi-squared with k degrees of freedom,"I am currently trying to compute the expectation (a nice closed form distribution would be even better) of the minimum of n iid chi-squared with k degrees of freedom. After struggling with the pdf easily derived from the definition, I tried some numerical experiments and by making log-log plots, I have a strong belief (R2 of the regression > 0.998) that the solution is: $$\mathbb{E}\left[\min_{i \in [n]}X_i\right] = kn^{-\frac{1}{\sqrt{k}}}$$ Where $X_i \sim \chi^2(k)$ are iid. I don't find any way to prove it yet and would be grateful of any help or reference (I didn't find any either).","['statistics', 'probability-distributions', 'order-statistics', 'probability-theory', 'probability']"
4699298,Differentiable formula that computes rotations in interval arithmetic (bounding box of a family of rotated rectangles),"I want to know how to perform a rotation in 2D interval arithmetic . That amounts to computing the tightest interval containing $$ x\ \mathrm{cos}(\varphi) + y\ \mathrm{sin}(\varphi), \tag{1}$$ where $x$ , $y$ and $\varphi$ are all (real number) intervals. The first approach one might try is to directly use interval arithmetic on (1). However, this leads to poor bounds, e.g. taking $x = [-1,1]$ , $y = [-1,1]$ , $\varphi = [0,2 \pi]$ one gets $$\begin{align} x\ \mathrm{cos}(\varphi) + y\ \mathrm{sin}(\varphi) & = [-1,1] \ \mathrm{cos}([0,2 \pi]) +  [-1,1] \ \mathrm{sin}([0,2 \pi]) \\ &= [-1,1] \cdot [-1,1] + [-1,1] \cdot [-1,1] \\ &= [-2, 2]\end{align}$$ However, this isn't the tighest bound, because $x\ \mathrm{cos}(\varphi) + y\ \mathrm{sin}(\varphi)$ is clearly bounded by $\sqrt{x^2 + y^2}$ ; this observation leads to the tight bound $[-\sqrt{2}, \sqrt{2}]$ in this case. To eliminate the dependency problem with the variable $\varphi$ , we can rewrite the formula so that $\varphi$ only occurs once: $$ \sqrt{x^2 + y^2}\ \mathrm{cos}\left (\varphi - \mathrm{atan2}(x,y) \right ), \tag{2}$$ where $\mathrm{atan2}$ is the 2-argument arctangent function that gives the polar angle of the given point. (2) computes the optimal bound in the example above. Unfortunately, it can perform worse than (1) when $\varphi$ is a small interval. For example, taking $x = y = [0.9,1.1]$ and $\varphi = [0,0]$ , evaluating (2) with interval arithmetic gives an answer of about $[0.805, 1.204]$ , which is worse than the obvious answer $[0.9,1.1]$ that (1) computes. We've gotten rid of the dependency in $\varphi$ , but now we introduced a dependency problem in the variables $x$ and $y$ . Furthermore, taking the intersection of (1) and (2) doesn't always give tight bounds. For example, take $x = y = [1,2]$ , $\varphi = [0, \frac{\pi}{2}]$ . Evaluating (1) gives $[0,4]$ , (2) gives approximately $[0.63, 2\sqrt{2}]$ , while the actual answer is $[1,2\sqrt{2}]$ . Question: Is there a differentiable expression that can be evaluated using interval arithmetic and computes $ x\ \mathrm{cos}(\varphi) + y\ \mathrm{sin}(\varphi) $ , with a tight bound in all cases? I'm happy to assume that $x > 0$ and $y > 0$ , but $\varphi$ may span several quadrants. Note that I do need an actual formula which I can differentiate, as the problem I'm working on relies on automatic differentiation. That is, in practice, $\varphi$ is not just an interval but is an interval-valued differential, and I don't know how I would evaluate the expression with differentials if we are taking minimums & maximums over a finite set of values. I also don't want to simply formulate a nonlinear optimisation problem and solve it numerically, as again that doesn't give me a procedure that I can differentiate.","['interval-arithmetic', 'trigonometry', 'nonlinear-optimization', 'rotations']"
4699311,"For all natural numbers of $n$, prove by induction that $15 \mid (4^{2n+1} + 5^{2n+1} + 6^{2n+1})$.","What I have so far: We will prove by induction. Our induction predicate $P(n)$ is $15 \mid (4^{2n+1} + 5^{2n+1} + 6^{2n+1})$ . Base Case: $n = 0$ : $P(0)$ is $15 \mid (4^{2(0)+1}+5^{2(0)+1}+6^{2(0)+1})$ . $4+5+6 = 15$ , so $P(0)$ is true. Inductive Step: $n \geq 0$ : Let the natural number of $n$ be arbitrary and assume $P(n)$ is true. Now we show $P(n+1)$ is true, where $P(n+1)$ is $15 \mid (4^{2(n+1)+1} + 5^{2(n+1)+1} + 6^{2(n+1)+1})$ . $4^{2(n+1)+1} + 5^{2(n+1)+1} + 6^{2(n+1)+1} = 4^{2n+3} + 5^{2n+3} + 6^{2n+3}$ . That's all I have. I am unable to finish","['induction', 'proof-writing', 'discrete-mathematics']"
4699314,"Using de Moivre's Theorem, show that: $\cos(5\theta)=\cos^5\theta-10 \cos^3\theta \sin^2\theta+5 \cos\theta \sin^4\theta$","I started stating that: $$(\cos\theta+i \sin\theta)^5=\sum_{k=0}^5 \binom{5}{k} (\cos\theta)^{5-k}\cdot (i \sin\theta)^k$$ Then, I apply the binomial theorem and develop, so I ended up with this, but arranged: $$=\cos^5(\theta)-10 \cos^3(\theta) \sin^2(\theta)+5 \cos(\theta) \sin^4(\theta)+5\cos^4(\theta)-i \sin(\theta)\\-10 \cos^2(\theta) i \sin^3(\theta)+i \sin^5(\theta)$$ So you can see I get what I wanted to obtain, but I can't or dont know how to get rid of this part: $$5 \cos^4(\theta)-i \sin(\theta)-10 \cos^2(\theta) i \sin^3(\theta)+i \sin^5(\theta)$$ Until now, I got: $$i \sin(\theta)[\sin^2(\theta)\cos^2(\theta)]$$ I really don't know how to proceed, any help is appreciated.","['algebra-precalculus', 'binomial-coefficients', 'trigonometry', 'complex-numbers']"
4699334,Can we treat two equal sets as being distinct mathematical objects?,"When we have a set, is it correct to refer to another set as 'itself' when this other set is merely equal to it? More formally, I am asking whether or not two sets with the same elements can be considered to be two separate mathematical objects. I am a programmer so I am used to the ""same object"" being a distinct notion to ""the object to which this object is equal"". Therefore, this has led me to consider whether or not this concept is true within mathematics.","['elementary-set-theory', 'definition', 'notation', 'terminology']"
4699335,Prove inequalities (Calculus 1) $\ x - \frac{x^{3}}{6}< \sin x < x-\frac{x^{3}}{6} +\frac{x^{5}}{120}$ [duplicate],"This question already has answers here : Proving $x-\frac{x^3}{6} < \sin(x) < x - \frac{x^3}{6} + \frac{x^5}{120} ~~ \forall x \in \Bbb R^+$ using Taylor's expansion (5 answers) Closed last year . Prove the following inequality: $$x - \frac{x^{3}}{6}< \sin x < x-\frac{x^{3}}{6} +\frac{x^{5}}{120} ,\; \forall x > 0$$ I'm not sure my proof is correct. I separated the problem into proving the inequality on the left and on the right side. The left side wasn't really difficult,the right one,however,gave me a lot of work.I named $g(x)=x-\frac{x^{3}}{6} +\frac{x^{5}}{120}$ and $f(x) = \sin(x)$ and derived both equations multiple times until i reached the $5$ th and analysed the rate of growth of each one until I came into the conlusion that $f(x) < g(x)$ .I used the same method on the left side and it worked.Could anyone tell me if I'm on the right path here?Any help would be kindly appreciated.
Thank you in advance.","['calculus', 'derivatives', 'inequality']"
4699352,Filtration modelling a random number of observation,"Suppose we have i.i.d. r.v.'s $\{X_i\}$ and a random number of observations $N$ we are allowed, following some distribution, independent of the $\{X_i\}$ . So essentially we observe $X_1,\ldots,X_N$ . I am interested in constructing a filtration for the process of observing these values, and stopping according to some stopping rule as usual. However, what could happen is that my stopping rule could find out, AFTER an observation and not having stopped at that observation, that it was the last one allowed. We assign the value of infinity to the stopping time, in this case. Now my question is: what filtration models what we know at a step $n\leq N$ in this game? My guess would be $\sigma(X_1, \ldots, X_n, 1_{\{N=0\}},\ldots, 1_{\{N=n-1\}})$ . This makes sense when inspected step by step: if the game starts (this is expressed by $1_{\{N=0\}}=0$ ), at time 1, when receiving performing the first observation, I will only know that it is not the case that there were no observations at all, but I do not know yet if the observation I am taking will be the last: thus $\sigma(X_1,I_{\{N=0\}})$ is what I know. After making the decision whether to stop or continue, I will know $\sigma(X_1,I_{\{N=0\}},I_{\{N=1\}})$ , that is I will know whether $X_1$ was the last observation ( $I_{\{N=1\}}=1$ ) or not ( $I_{\{N=1\}}=0$ ). Next $\sigma(X_1,I_{\{N=0\}},I_{\{N=1\}}, X_2)$ and so on. What confuses me is that I am used to filtration expressing all the information available at time $n$ , in the sense of all that could have happened by that time, but my construction seems a little different and I suspect that there is a mistake. In fact, say we are at time 3 in the game, the information is supposed to be $\sigma(X_1,I_{\{N=0\}},I_{\{N=1\}}, X_2)$ . But clearly, if I am at time 3 in the game, just before I am inspecting $X_3$ , I know that $I_{\{N=0\}}=I_{\{N=1\}}=0$ , otherwise I would have not inspected $X_2$ at the second step. What I mean is that, focusing on the indicators, only sequences of zeroes and sequences of zeroes with one at the end are allowed, in terms of information, not every possible string of zero and ones. As soon as a one appears, the game stops and we set our stopping time to infinity. (If instead we stop by our decision, we have the stopped sigma algebra to model that.) Does this mean that the construction is redundant or even wrong? Thanks for any help.","['measure-theory', 'independence', 'probability-distributions', 'probability-theory', 'probability']"
4699353,Does this measurable function exist and satisfy my motivation?,"Main Question: Does there exist a function $f:[0,1]\to[0,1]$ such that: the function $f$ is measurable in the sense of Caratheodory the graph of $f$ is dense in $[0,1]\times[0,1]$ the collection of all subsets of the range of $f$ with pre-images under $f$ that are measurable in the sense of Caratheodory is a non-perfect dense set in the collection of all subsets of the range of $f$ , such that we define a topology where: Let ${\mathbb P}[0,1]$ be the collection of all subsets of $[0,1]$ modulo the equivalence relation $\sim$ defined by $E \sim F \Leftrightarrow {\lambda^{*}(E \Delta F)} = 0,$ where $\lambda^{*}$ is Lebesgue outer measure and $\Delta$ is the symmetric difference operation on sets. The set ${\mathbb P}[0,1]$ can be made into a complete metric space by defining the distance function $d,$ where $d(E,F) = {\lambda^{*} (E \Delta F)}.$ the graph of $f$ is non-uniform (i.e. without complete spacial randomness ) in $[0,1]\times[0,1]$ using the uniform probability measure , the expected value of $f$ is undefined? Motivation: We want to define a function $f:[0,1]\to[0,1]$ where the graph of $f$ is somewhat but not too evenly distributed (i.e. with complete spacial randomness ) in $[0,1]\times[0,1]$ , such that using the uniform probability measure , the expected value of $f$ is undefined so we can find an unique extension of the expected value which gives a finite value instead . Question on motivation: If the function from the main question exists, does it give the same function as the one from the motivation? Simplified Version of the Main Question: In the main question , a non-perfect dense set in the collection of all subsets of the range of $f$ is “topologically large” in the collection of all subsets of the range of $f$ , where the main question translates to the following: Does there exist measurable function $f:[0,1]\to[0,1]$ such the graph of $f$ is
dense in $[0,1]\times[0,1]$ , where the pre-image of a
“ random subset ” of the range of $f$ under $f$ is “likely” to be measurable in the sense of
Caratheodory, where the graph of $f$ is non-uniform (i.e. without complete spacial randomness ) in $[0,1]\times[0,1]$ , and w.r.t the uniform probability measure the expected value of $f$ is undefined? Attempt to Solve Both Questions: I can't prove an explicit example exists but here is my attempt from this question ( note in the link, the previous question didn't give a function that satisified the motivation in this post ): Suppose the base- $3$ expansion of real numbers, in interval $[0,1]$ , have infinite decimals that approach $x\in[0,1]$ from the right
side so when $0\le x_1,x_2\le 1$ (and $x_1=x_2$ ) we get $f(x_1)=f(x_2)$ . Furthermore, for $\mathbb{N}\cup\left\{0\right\}=\mathbb{N}_{0}$ , if $r\in\mathbb{N}_{0}$ and $\text{digit}_{3}:\mathbb{R}\times
 \mathbb{Z}\to\left\{0,1,2\right\}$ is a function where $\text{digit}_{3}(x,r)$ takes the digit in the $3^{r}$ -th decimal fraction
of the base- $3$ expansion of $x$ (e.g. $\text{digit}_{3}(1.789,2)=\text{digit}_{3}({1.210022{\cdot\cdot\cdot}}_{3},2)=1$ ), then $\left\{{g_r}^{\prime}\right\}_{r\in\mathbb{N}_{0}}$ is a
sequence of functions (and $\left[\cdot\right]$ is the nearest integer
function) such that ${g_r}^{\prime}:\mathbb{N}_0\to\mathbb{N}_0$ is
defined to be: \begin{equation}
  g_r^{\prime}(x)=\left[\frac{10}{3}\sin(rx)+\frac{10}{3}\right]
 \end{equation} then for some function $k:\mathbb{N}_{0}\to\mathbb{N}_{0}$ , where $k$ is strictly increasing, such that $k(0)$ is a positive
number, and
the intermediate function (before $f$ ) or $f_{1}:[0,1]\to[0,10]$ satisfies the main question if the range is $[0,10]$ instead of $[0,1]$ . \begin{alignat}{2} & f_{1}(x)  =
 &&\left|\left(\sum\limits_{r=0}^{\infty}
 g_{r+1}^{\prime}\!\left(\sum\limits_{p=r}^{r+k(r)}\text{digit}_{3}(x,p)\right)\!\!\bigg/3^{r}\right)-10\right|=
 \label{eq:025} \\ & &&
 \left|\left(\left(\sum\limits_{r=0}^{\infty}\left[\frac{10}{3}\sin\left(\left(r+1\right)\left(\sum\limits_{p=r}^{r+k(r)}\text{digit}_{3}(x,p)\right)\right)+\frac{10}{3}\right]\right)\!\!\bigg
 /3^{r}\right)-10\right| \nonumber \end{alignat} (One example of $k(r)$ that may satisfy the problem, i.e. if the
range is $[0,10]$ , is $k(r)=10r+20$ ) What we are doing with $f_1$ is we converted every digit of the base- $3$ expansion of $x$ into a pseudo-random number that is non-equally likely to be an integer,
including and in-between, $0$ and $20/3$ . Further, we attempt to
make the function dense in $[0,1]\times[0,10]$ by adding the $3^{r}$ -th decimal fraction with the next $k$ decimal fractions;
however, we want to control the end-points of $[0,10]$ such that $f_1$ is dense in $\left[0,1\right]\times\left[0,1\right]$ (instead of $\left[0,1\right]\times[0,10]$ ) by manipulating $f_1$ to get: \begin{alignat}{2} & f(x)  = && 1-\frac{1}{10}f_1(x)\label{eq:109}\\
 & && 
 1-\left(\frac{1}{10}\right)\left|\left(\left(\sum\limits_{r=0}^{\infty}\left[\frac{10}{3}\sin\left(\left(r+1\right)\left(\sum\limits_{p=r}^{r+k(r)}\text{digit}_{3}(x,p)\right)\right)+\frac{10}{3}\right]\right)\!\!\bigg/3^{r}\right)-10\right|
 \nonumber \end{alignat} (e.g. $k(r)=10r+20$ ) you can use programming to visualize $f$ though I
don't know if you can graph the entire function. (The programming I
used is mathematica.) Clear[""Global`*""]
k[r_] := k[r] = 
  20 (* You can adjust k[r]; however, mathematica is unable to graph \
f when k[r] is steepy increasing e.g. for this function, k[r] must be \
less than 25 for the code to show a graph. Instead, it should be k[r]=10r+20 *)

g1[xr_, r_] := 
 g1[xr, r] = 
  Round[(10/3) Sin[r xr] + (10/
      3)] (*Converts the (3^r)th decimal fraction,in the base 3 \
expansion of the x-values in[x1,x2] (defined as xr or x_r not x*r) \
into a psuedo-random number that's non-equally likely to spit a \
number between,and including, 0 and 20/3 *)

f[x_] := f[x] = 
  N[1 - ((1)/(10)) RealAbs[
      Sum[g1[Sum[
           RealDigits[x, 3, k[r], -r][[1]][[z]], {z, r + 1, k[r]}], 
          r + 1]/(3^r), {r, 0, 8}] - 
       10]] (*Defines function f,I assume the larger k[r]'s values, the more \
the function appears dense in [0,1]x[0,1]*)

p = .00005 (*Incremement between the x-values in the points of the \
graph below*)

ListPlot[Table[{x, f[x]}, {x, p, 1, 
   p}]] (*Graphs countable points of the functions but is not a \
complete accurate graph. There are uncountably many points that need \
to be included.*) Unfortunately, I only studied up to intro to advanced mathematics. This could be non-sense. (Without a deep undestanding of math I'm unable to prove if the function gives what I'm looking for.) Is there a simpler example?","['measure-theory', 'functions', 'definition', 'real-analysis']"
4699368,Is there any simple method to evaluate $\int \frac{\tan (3 x)}{\tan(x)+\sec(x)} d x$?,"Once I met the indefinite integral $$I=\int \frac{\tan (3 x)}{\tan x+\sec x} d x,$$ I thought of triple-angle formula of tangent $$\tan 3 x =\frac{3 \tan x-\tan ^3 x}{1-3 \tan ^2 x} \tag*{} $$ For simplicity, I first rationalised the denominator of $I$ as $$\displaystyle I=\int \tan (3 x)(\sec x-\tan x) d x\tag*{} $$ and get $\displaystyle \begin{aligned}I & =\int \frac{\left(3-\tan ^2 x\right) \tan x}{1-3 \tan ^2 x}(\sec x-\tan x) d x \\& =\int \frac{3-\tan ^2 x}{1-3 \tan ^2 x}\left(\sec x \tan x-\sec ^2 x+1\right) d x \\& = \underbrace{\int \frac{4-s^2}{4-3 s^2} d s}_{J} - \underbrace{\int \frac{3-t^2}{1-3 t^2} d t}_{K} + \underbrace{\int \frac{3-\tan ^2 x}{1-3 \tan ^2 x} d x}_{L} \end{aligned}\tag*{} $ where $s=\sec x$ and $t=\tan x$ . To deal with these $3$ integrals, we used the result: $$
\int \frac{1}{a^2-b^2 x^2} d x=\frac{1}{a b} \tanh ^{-1}\left(\frac{b x}{a}\right)+c
$$ For $J$ , $$\displaystyle \begin{aligned}\int \frac{4-s^2}{4-3 s^2} d s & =\frac{1}{3} \int \frac{8+\left(4-3 s^2\right)}{4-3 s^2} d s\\&=\frac{8}{3} \int \frac{d s}{2^2-(\sqrt{3} s)^2}+\frac{s}{3} \\& =\frac{4}{3 \sqrt{3}} \tanh ^{-1}\left(\frac{\sqrt{3} s}{2}\right)+\frac{s}{3}+c_1\end{aligned}\tag*{} $$ For $K$ , $\displaystyle \begin{aligned}K & =\int \frac{3-t^2}{1-3 t^2} d t \\& =\frac{1}{3} \int \frac{8+\left(1-3 t^2\right)}{1-3 t^2} d t \\& =\frac{8}{3 \sqrt{3}} \tanh ^{-1}(\sqrt{3} t)+\frac{t}{3}+c_2\end{aligned}\tag*{} $ For $L$ , $\displaystyle \begin{aligned}L & =\int \frac{3-\tan ^2 x}{1-3 \tan ^2 x} d x \\& =\int \frac{2 \sec ^2 x+\left(1-3 \tan ^2 x\right)}{1-3 \tan ^2 x} d x \\& =2 \int \frac{d t}{1-3 t^2}+x\\&=\frac{2}{\sqrt{3}} \tanh ^{-1}(\sqrt{3} t)+x+c_3\end{aligned}\tag*{} $ Now we can conclude that $ \boxed{\displaystyle I=\frac{4}{3\sqrt{3}} \tanh ^{-1}\left(\frac{\sqrt{3} \sec x}{2}\right)+\frac{\sec x}{3}-\frac{2}{3 \sqrt{3}} \tanh ^{-1}(\sqrt{3} \tan x)-\frac{\tan x}{3}+x+C }\tag*{} $ My solution is rather tedious and long. Is there any simpler method? Your comments and alternative methods are highly appreciated.","['integration', 'indefinite-integrals', 'trigonometric-integrals', 'hyperbolic-functions']"
4699372,Closed form of recursion an ceiling function,"I am trying to find a closed form for the recursion: $a_n=6a_{n-1}-4a_{n-2}$ with the conditions $a_0=1, a_1=3$ My attempt:
Let's assume there exists $c$ such that $a_n=c^n$ . (1)
Next step is to rewrite $a_n=6a_{n-1}-4a_{n-2}$ regarding our assumption (1), thus I get: $c^n=6c^{n-1}-4c^{n-2}$ Dividing by $n-2$ yields, $c^2=6c-4$ Next step is to solve the equation, $c_1= 3-\sqrt5, c_2=c_1= 3+\sqrt5$ . Now, $\lambda_1 c_1^n+\lambda_2 c_2^n$ should also be able to represent the recursion. The last step is to calculate the coefficients $\lambda_1,\lambda_2$ . For that, we use the initial condition given ant the beginning.
Thus, we get the euations: $\lambda_1+\lambda_2=1$ $\lambda_1 (3-\sqrt5)+\lambda_2 (3+\sqrt5)=3$ By solving I get $\lambda_1 = \lambda_2=\frac{1}{2}$ Thus, $a_n=\frac{1}{2} (3-\sqrt5)^n+\frac{1}{2} (3+\sqrt5)^n$ That's how far I got. Now someone told me that $a_n$ can also be written as $a_n=\lceil \frac{(3+\sqrt5)^n}{2} \rceil$ , where $\lceil . \rceil$ denotes the ceiling function. My question: Why can $a_n$ be written in that way using the ceiling function? Is this some special case, or is it possible to write other the closed form of other recursions to in such a way?","['combinatorics', 'discrete-mathematics']"
4699374,Why must it be either true or false that the empty set is a subset of all sets?,"I encountered a proof that the empty set is a subset of every set via this comment( Is ""The empty set is a subset of any set"" a convention? ) which shows that it cannot be false that the empty set is a subset of every set. Without necessarily going into a proof of how the empty set is a subset of every set,  I was wondering why the fact that it cannot be false that the empty set is a subset of every set shows that this is true-  could it not be the case that the concept of subsets is meaningless with regards to the empty set, and it is not enough to show that it could not be false; that this statement could neither true or false as it has no meaning in this context?
Also, I would appreciate some explanation as to how this condition holds ""vacuously"" as far as terminology, as I have learned that for an implication to be vacuously true, it is true when it's hypothesis is false. Thanks","['elementary-set-theory', 'logic']"
4699385,Finding $\displaystyle \lim_{n \rightarrow \infty} \sqrt{n} \int_0^1 \left(\frac{\sin t}{t}\right)^n dt$,"To find the limit $$\lim_{n \rightarrow \infty} \sqrt{n} \int_0^1 \left(\frac{\sin t}{t}\right)^n dt,$$ I attempted to use Laplace's method . We can express the given integral as $$\int_0^1 \exp \left(n \ln \left(\frac{\sin x}{x}\right)\right) dx$$ However, I encountered an issue with this approach. In Laplace's method, we require the function $f(x)=\ln \left(\frac{\sin x}{x}\right)$ to be twice differentiable. Additionally, the global maximum of $f(x)$ within the integration range must be unique and not located at the boundary points. Unfortunately, in this case, the maximum occurs at the boundary point $0$ , which prevents me from applying Laplace's method as intended. I have been thinking about this problem for quite some time, but I haven't been able to come up with any promising ideas. I would really appreciate any guidance or suggestions. Thank you in advance.","['integration', 'asymptotics', 'real-analysis']"
4699392,Asymptotic behavior of the inverse Fourier transform of $\frac{1}{|k|^2 + 1}$,"This math.SE answer calculated the following (inverse) Fourier transform in $n$ -dimensions $$
f(x) = \int_{\mathbb{R}^n} d^nk 
\frac{e^{ik\cdot x}}{|k|^2 + 1}
\propto |x|^{-\frac{n}{2}+1} K_{\frac{n}{2}-1}(|x|)
\tag{1}
$$ where $K_\alpha(x)$ is the modified Bessel function of the second kind . But in some physics applications, one is only interested in the large- $|x|$ asymptotic behavior of $f(x)$ . Using the asymptotic expansion $$
K_\alpha(x) \propto x^{-1/2} e^{-x} [1 + O(x^{-1})]
\quad \text{when } \mathrm{arg}(x) < 3\pi/2
$$ we obtain (which reproduces Eq. (14.23) in Assa Auerbach's Interacting Electrons and Quantum Magnetism ) $$
f(x) \sim |x|^{-(n-1)/2} e^{-x}
\tag{2}
$$ My question is: is there a simpler way than the cited math.SE answer that directly aims to find the asymptotic behavior of $f(x)$ for large $|x|$ ?","['multivariable-calculus', 'fourier-transform', 'asymptotics']"
4699407,Function proportional to the log likelihood for the Gaussian distribution,"The following question has been crossposted to CrossValidated upon recommendation from the community and a lack of responses here. Consider the following problem from a course on statistical inference: If we generate a sample $x_i$ for $i \in$ { $1 ... n$ } from $ p(x_i) = \sum_{k=1}^2 w_kp(x_i| \mu _k, \sigma^2_k)$ where $p(x_i| \mu _k, \sigma ^2_k)$ are Gaussian densities and $w_2 = 1 - w_1$ (where we assume that all the parameters are unknown) Find the log-likelihood function for the sample $x_{1:n}$ . The log-likelihood is defined as: $$ l( \theta | x_{1:n}) = \log(p(x_{1:n})) = \log( \Pi _{i=1}^n p(x_i)) = \sum _{i=1}^n \log(p(x_i))$$ Now substituting in for $p(x_i)$ we get: $$ l( \theta | x_{1:n}) = \sum_{i=1}^n \log \Big{(} \sum_{k=1}^2 w_k p(x_i | \mu _k , \sigma^2_k) \Big{)} $$ We can now substitute in the Gaussian densities which gives us: $$ l( \theta | x_{1:n}) = \sum_{i=1}^n \log \Big{[} w_1(2 \pi \sigma ^2 _1)^{-1/2} \exp \Big{(}\frac{(x - \mu _1)^2}{2 \sigma ^2 _1} \Big{)} + w_2 (2 \pi \sigma ^2 _2)^{-1/2} \exp \Big{(}\frac{(x - \mu _2)^2}{2 \sigma ^2 _2} \Big{)} \Big{]} $$ The following solution aligns with my working, however, there is a proportionality relation on the last line that is unclear to me. Why does the final line hold? I can't see a clear and obvious way to get from the point at which I have substituted the Gaussian densities into the sum to it being proportional to the given double summation. I understand that we can exclude any multiplicative constants, however, it seems as though the summation has been taken out of the logarithm at some stage in order to derive a proportionality relation. Although I'm not sure if this holds and if it does, why does it hold?","['statistical-inference', 'statistics', 'log-likelihood', 'normal-distribution', 'probability']"
4699427,"How do we get $\frac{\partial}{\partial t}|_{t=0}(h_t)_{ij}=\langle\nabla_{e_i}X,e_j\rangle+\langle e_i,\nabla_{e_j}X\rangle$?","Sorry, I will have to begin my question by directly pasting a few snapshots. The following material comes from Geometric Relativity by Dan A. Lee. I'd like to derive equation (2.4). What I have tried so far is that if $h_t=\Phi_t^*h$ , we would obtain $h_t=\Phi_t^*g$ . Since $d\Phi_t$ is an isomorphism, we should agree that $(e_i(t))$ is a local frame on $\Sigma_t$ . It follows that $$\begin{align}
\frac{\partial}{\partial t}(h_t)_{ij}&=\frac{\partial}{\partial t}\langle e_i(t),e_j(t)\rangle_{h_t}\\
&=\frac{\partial}{\partial t}(\Phi_t^*g)(e_i,e_j)\\
&=\frac{\partial}{\partial t}g_{\Phi_t}(e_i(t),e_j(t)).
\end{align}$$ Now I wonder why equation (C) is possible. Are there any tricks? Thank you. Edit. I got some ideas though still lost in equation (C). To arrive at equation (2.4), you can substitute $t=0$ into equation (E). To get (E), you can use equality of mixed partials to conclude the Lie bracket of $X_t$ and $e_i(t)$ is the zero vector field. Finally, equation (D) can be derived using the fact that Levi-Civita connections are metric connections.","['vector-fields', 'riemannian-geometry', 'differential-geometry']"
4699502,"Find $b$ If $f:\mathbb R\to\mathbb R, \;f(x)=\dfrac{x^2+bx+1}{x^2+2x+b}, (b>1)$ and $f(x), \dfrac{1}{f(x)}$ have same bounded Range","Let $f:\mathbb R\to\mathbb R, \;f(x)=\dfrac{x^2+bx+1}{x^2+2x+b}, (b>1)$ and $f(x), \dfrac{1}{f(x)}$ have the same bounded set as their range, then value of $b$ is My Approach: Let $f(x)=y=\dfrac{x^2+bx+1}{x^2+2x+b}\implies(y-1)x^2+(2y-b)x+(by-1)=0$ since $x\in \mathbb R$ we must have $D=b^2-4ac\geq 0\implies (4-4b)y^2+4y+(b^2-4)\geq0.......(1)$ Also Let $\dfrac{1}{f(x)}=t=\dfrac{x^2+2x+b}{x^2+bx+1}\implies(t-1)x^2+(tb-2)x+(t-b)=0$ Again since $x\in \mathbb R$ we must have $D=b^2-4ac\geq 0\implies (b^2-4)y^2+4y+(4-4b)\geq0.......(2)$ Since $(1)$ and $(2)$ both are positive for all $ y\in \text{Range}$ we must have $D\leq0\implies(4^2)-4(b^2-4)(4-4b)\leq0 \implies b^3-b^2-4b+5\leq0$ . After plotting graph of $b^3-b^2-4b+5=0$ in Desmos I got $b^3-b^2-4b+5\leq0$ for $b\leq {-2.08}$ which is wrong because it is given that $b>1$ . How can i get value of $b$ more than $1$ ?","['calculus', 'functions', 'quadratics', 'rational-functions']"
4699509,"What is the motivation to study $\Gamma \backslash \mathbb H $, where $\Gamma \subset Sl_2(\mathbb Z)$ is a congruence subgroup?","Let $\mathbb H \subset \mathbb C$ be the upper half plane. On $\mathbb H$ the group $Sl_2(\mathbb Z)$ acts by Möbius transformations $$\begin{pmatrix} a & b \\ c & d \end{pmatrix} . z = \frac{az + b}{cz + d}.$$ I know that the quotient $Sl_2(\mathbb Z) \backslash \mathbb H$ parametrises elliptic curves, because two elements $\tau, \tau' \in \mathbb H$ define the same lattice $\mathbb Z + \tau \mathbb Z$ if and only if they are in the same $Sl_2(\mathbb Z)$ -orbit. After studying the quotient $Sl_2(\mathbb Z) \backslash \mathbb H$ , a common theme seems to study quotients $\Gamma \backslash \mathbb H$ , where $\Gamma \subset Sl_2(\mathbb Z)$ is a group of finite index. In particular congruence subgroups $$\Gamma(k) = \{\, A \in Sl_2(\mathbb Z) : A \equiv I_2 \mod k \,\}$$ seem to be of interest.
I'm missing some motivation to do that. Why are the $\Gamma(k)$ interesting groups, and why is the quotient $\Gamma(k) \backslash \mathbb H$ interesting?","['complex-geometry', 'modular-forms', 'algebraic-geometry', 'elliptic-curves']"
4699513,"Finding pdf p(x,1) from p(x,0) given sde Xt","Assume $X_t$ satisfies the SDE: $$dX_t = X_tdt + dW_t$$ , where $W_t$ is standard normal. If we know that $X_0 = 1$ and there exists some pdf of $X_t$ , $p(x,0)$ , how can I find the the pdf of $X_t$ at time 1, $p(x,1)?$ I tried to use Fokker–Planck equation $$
\frac{\partial}{\partial t}p(x,t)= -\frac{\partial}{\partial x}[\mu(x,t)p(x,t)]+\frac12\frac{\partial^2}{\partial x^2}[\sigma^2(x,t)p(x,t)]
$$ , but I am not sure how I can utilize this equation without explicitly knowing $p(x,0)$ . Could you guys give me an advice on this problem? :D","['stochastic-processes', 'stochastic-calculus', 'probability', 'partial-differential-equations']"
4699550,Does the exponential operator $e^{tA}$ defined via functional calculus and via semi group coincide?,"Given $X$ is a Hilbert space and consider the operator $A$ on $X$ . Functional calculus:
given an unbounded operator $A$ (densely defined, closed and self-adjoint), we can define $e^{tA}$ by the spectral measure. Semi-group:
given an unbounded operator $A$ (densely defined, closed) which generates a continuous semi-group, we can define $e^{tA}$ by the Laplace transform of the resolvent operator. Question:
Do these two approaches give the same definition in the case where $A$ is densely defined, closed, self-adjoint and generates a continuous semi-group? Trivial case: if $A$ is bounded or $A$ has compact resolvent, I think this question is rather trivial in view of the behavior of the point spectrum.","['functional-calculus', 'operator-theory', 'functional-analysis', 'semigroup-of-operators', 'spectral-theory']"
4699555,How can an axiom be “false” under different interpretations of the primitive notions?,"I can’t wrap my head around what he means by the any meaning assigned to the primitives making the axioms true make o true… I don’t understand how an axiom could be true or false, it seems like axioms are always true and a false axiom is just not a axiom.","['elementary-set-theory', 'set-theory']"
4699589,Is there a connection between shoelace formula and Stokes theorem?,"The shoelace-formula is a method to calculate the area of a polygon. It is given as $$
A = 1/2 \sum_i{(x_i-x_{i+1})*(y_i+y_{i+1})}
$$ for cyclical $i$ . Expanding the product yields the terms $x_i y_i - x_{i+1}y_{i+1}$ which cancel with the next and previous terms of the sum, resulting in: $$
A = 1/2 \sum_i x_i y_{i+1} - x_{i+1} y_i = 1/2 \sum_i |\vec{X_i} \times \vec{X_{i+1}}|
$$ Which prompted my intuition to look for connections with Stokes' theorem but so far I got nowhere. Is this just a coincidence and Stokes' has nothing to do with this? I can see how this is basically a discrete version of Green's Theorem , but that just doesn't feel like any kind of deep understanding. And yes, I'm asking to satisfy my curiosity. The best concrete question I could think of is: Is there a field $F$ for which Stokes' theorem and the shoelace formula coincide for infinitesimally close polygon-corners? What I've got so far: The absolute value of a cross product is cumbersome and can be replaced by either a dot-product with the surface-normal or limiting the calculation to the z-axis instead. So either $|\vec{X_i} \times \vec{X_{i+1}}| = (\vec{X_i} \times \vec{X_{i+1}}) * \left({\begin{smallmatrix}0\\0\\ 1\end{smallmatrix}}\right) $ or use $A \simeq\left({\begin{smallmatrix}0\\0\\ A\end{smallmatrix}}\right)$ . To get from a sum to an integral, an infinitesimal delta is required. The delta is hidden in the cross-product: $$
\vec{X_i}\times\vec{X_{i+1}} = \vec{X_i}\times(\vec{X_i} + \vec{\Delta_i}) = \vec{X_i}\times\vec{\Delta_i}
$$ taking the limit $\vec{\Delta} \rightarrow 0$ : $$
A = \vec{n} * 1/2 \int_{\partial A} \vec { X } \times \vec{\partial A}
$$ The area $A$ can easily be interpreted as a surface integral over 1: $$
\int \int_A 1 = A
$$ Or, more in line with the concept of a vector-field: $$
\int \int_A \left(\begin{smallmatrix}0\\0\\ 1\end{smallmatrix}\right) = \left(\begin{smallmatrix}0\\0\\ A\end{smallmatrix}\right)
$$ All the elements are there, but my main problem is the cross product is on the wrong side. I could construct a field with unit curl everywhere to satisfy the left hand side, but then I don't know what to do on the right.","['integration', 'geometry', 'greens-theorem', 'stokes-theorem', 'discrete-mathematics']"
4699590,Is this proof of transitive property of implication correct?,"I would like to prove the transitive property of the implication symbol, namely: Prove that assuming $(A \implies B)$ and $(B \implies C)$ , it follows that $(A \implies C)$ . I have attempted a proof below and would like to know if it is correct. Assume $A$ holds. I know that $A$ and $A \implies B$ hold, thus $B$ holds. I know that $B$ and $B \implies C$ hold, thus $C$ holds. Assuming $A$ , I showed $C$ . Thus $A \implies C$ holds. Seems almost too simple. Is this a correct way to proceed?","['elementary-set-theory', 'propositional-calculus', 'solution-verification', 'logic']"
4699640,What does it mean to integrate some complex valued function with respect to a projection valued measure of an operator in functional calculus?,"I am reading about the spectral theory of bounded linear operators in a book called Quantum Theory for Mathematicians by Hall. The goal of the spectral theory presented by the author is to construct a reasonable notion for what it means to ""pass a bounded/unbounded"" operator to the argument of some function. Namely, if $A\in B(H)$ is a bounded linear operator and $f$ is, say, a continuous complex-valued function, then $$f(A) = \int_{\sigma(A)}f(\lambda)d\mu^A(\lambda)$$ for a unique projection valued measure $\mu^A$ which satisfies $A = \int_{\sigma(A)}\lambda d\mu^A(\lambda)$ with $\sigma(A)$ being the spectrum of $A$ . Theory and related proofs for the case of bounded linear operators is given in chapters 7 and 8. For the rest of this question I am assuming what you know how projection valued measures and the related tools work (since otherwise I would probably have to copy most of the book here). What is puzzling me is how the integral $\int_{\sigma(A)}f(\lambda)d\mu^A(\lambda)$ of a non-simple function, complex-valued, function is defined. I mean the following: If $f = \sum_{n=0}^N\alpha_n\chi_{E_n}$ for some measurable subsets of $\sigma(A)$ $E_1,\dots,E_N$ and $\alpha_1,\dots,\alpha_N\in\mathbb{C}$ , then $$\int_{\sigma(A)}f(\lambda)d\mu^A(\lambda) = \sum_{n=0}^\infty \alpha_n\mu^A(E_n)$$ which is a bounded linear operator as the finite sum of such operators. The definition which I have seen for the Lebesgue integral of a non-negative measurable function $f:X\to[0,\infty]$ in a measure space $(X,\mathcal{F},\mu)$ is $$\int_Xf(x)d\mu(x) = \sup\left\{\int_Xs(x)d\mu(x)\mid \text{$s:X\to[0,\infty]$ is a simple function and $\forall x\in X:0\leq s(x)\leq f(x)$}\right\}$$ And leads to this post: Since Hall does not define from which parts the integral of non-simple function is constructed from, I am lead to believe that we are sameish supremum ""under the hood"". But w.r.t. what should the supremum be taken against?","['measure-theory', 'operator-theory', 'hilbert-spaces', 'functional-analysis', 'spectral-theory']"
4699670,Solution to a nonlinear ODE,"$$ a_1 y'''''+ a_2 y'''+\left(a_3 + y^2 \right) y' = 0 $$ where $a_1, a_2, a_3$ are constants with $a_1>0$ and $a_2,a_3 \in \mathbb{R}$ . Is there a general solution $y(x)$ to the above differential equation? I am aware that there is an easy solution to the linearized version of the above equation $(a_1 y'''''+ a_2 y'''+ a_3 y' = 0)$ and want to know if there is a solution to the nonlinear equation too. The equation arises in a mechanics problem. We have an unevenly pre-stretched ribbon (narrow plate). The constants $a_1,a_2$ depend on prestretch, and $a_3$ is a regularizing parameter. We are interested in the mechanics of twisting this pre-stretched ribbon. The mechanical system can be seen in figure-1 of this arxiv paper","['nonlinear-system', 'asymptotics', 'ordinary-differential-equations', 'perturbation-theory']"
4699729,"If $\int_0^{u_n}\frac{e^t}{1+e^{-nt}}dt=1$, show: $\lim\limits_{n\to \infty}n(u_n-\ln2)=\frac{\ln2}{2}$","Let $f_n:\Bbb R^+\to\Bbb R$ be the function defined as follows: $$f_n(x)=\int_0^x\frac{e^t}{1+e^{-nt}}\,dt,\qquad x\in\Bbb R^+$$ and let $u_n$ a sequence defined by $f_n(u_n)=1$ for all $n\in\Bbb N,$ where $\Bbb N$ is the set of all natural numbers (positive integers). I have already proved that $$\int_{0}^{u_n}\frac{e^{(1-n)t}}{1+e^{-nt}}\,dt =e^{u_n}-2$$ and $$e^{u_n}-2\leqslant\frac1ne^{u_n}$$ and $$\lim_{n\to \infty}n(e^{u_n}-2)=\ln2$$ please help me how to prove $$\lim_{n\to \infty}n(u_n-\ln2)=\frac{\ln2}{2}$$","['integration', 'real-analysis']"
4699736,Find the value of segment $x$ in the triangle below,"In the figure, calculate x, knowing that $a^2+b^2=36$ (S: $3$ ) I try $HF=HG=HB=r$ $AC^2 = AB^2+BC^2 \implies (2R)^2=(BF+a)^2+(BG+b)^2=BF^2+2aBF+a^2+BG^2+b^2+2bBG$ $4R^2=36+BF(2a+BF)+BF^2+BG^2+BG(2b+BG)=36+BF(2a+BF+BF)+BG(BG+2b+BG)$ $4R^2 = 36+BF(2a+2BF)+BG(2b+2BG)$ $4R^2=36+2BF(a+BF)+2BG(b+BG)=36+2(AB.BF+BC.BG)$","['euclidean-geometry', 'geometry', 'plane-geometry']"
4699750,Calculate the integral $\int\limits_{0}^{1}\frac{x^{1-p}(1-x)^p}{(1+x)^2}dx$,"Calculate the integral $$\int\limits_{0}^{1}\frac{x^{1-p}(1-x)^p}{(1+x)^2}dx, \; -1<p<2$$ My attempt: I tried to use the Laplace transform $$\mathcal{L}\left \{ \frac{x^{1-p}(1-x)^p}{(1+x)^2} \right \}(s)=\int_0^1 \frac{x^{1-p}(1-x)^p}{(1+x)^2}e^{-sx}dx=\int_0^\infty e^{-sx}\frac{x^{1-p}(1-x)^p}{(1+x)^2}dx=\int_0^\infty e^{-sx}\int_0^\infty e^{-tu}\frac{(tu)^{1-p}[(1-t)+(1-u)]^p}{(1+tu)^2}dudt$$ I don't know what to do next. Is it possible to reduce it to a beta function? This integral is very similar to this function. But I haven't been able to find any substitute","['integration', 'calculus', 'real-analysis']"
4699769,"$\int_0^\infty \frac{u^5 \, J_0\left( u\right)}{\left( u^2+x^2 \right)^{1/2}}\,e^{- u-\left( u^2+x^2 \right)^{1/2} }\,\mathrm{d}u $","Consider the following infinite integral that emerged while solving a fluid physical problem involving viscous flow in porous media: $$
f(x) = \int_0^\infty
\frac{u^5 \, J_0 \left( u\right) }{\left( u^2+x^2 \right)^{1/2}} \, e^{- u-\left( u^2\,+\,x^2 \right)^{1/2}}
\, \mathrm{d} u \, ,
$$ wherein $x \in \mathbb{R}$ is the real variable.
Here, $J_0$ denotes the zeroth-order Bessel function of the first kind. While a closed analytical expression might most likely be delicate and far from being trivial, I was wondering whether the above integral can potentially be transformed into a single infinite sum . What I tried is to use the following infinite series expansion of the Bessel function: $$
J_0 (u) = \sum_{m \ge 0} \frac{(-1)^m}{m!^2} \left( \frac{u}{2} \right)^{2m} \, ,
$$ together with $$
e^{-\left( u^2\,+\,x^2 \right)^{1/2}} = \sum_{n \ge 0} \frac{(-1)^n}{n!} \left( u^2+x^2 \right)^{n/2} \, ,
$$ leading to a double sum, which does not seem to be converging to the original function for some reason. Alternatively, I tried to use Poisson's integral to write the Bessel function in the form $$
J_0(u) = \frac{2}{\pi} \int_0^1 \frac{\cos uv}{\left(1-v^2\right)^{1/2}} \, \mathrm{d} v
$$ and interchange the order of integration.
Unfortunately this trick also does not seem to be of great help. Any hints would be highly appreciated. Thank you! In particular, for $x=0$ , it can readily be shown that $$
f(0) = \frac{21 \sqrt{5}}{625} \, .
$$","['integration', 'improper-integrals', 'calculus', 'sequences-and-series', 'bessel-functions']"
4699793,Any partition of an at most countable set has a set of representatives.,"I'm trying to understand the demonstration of the following proposition Any partition of an at most countable set has a set of
representatives Proof: Let $\mathcal{P}$ be a partition of $A$ . Then there exists an equivalence relation $\sim$ on $A$ induced by $\mathcal{P}$ . Since $A$ is at most countable, the set of equivalence classes, $A / \sim=\left\{[a]_{\sim}: a \in A\right\}$ , is at most countable. Hence, $$
A / \sim=\left\langle\left[a_1\right]_{\sim},\left[a_2\right]_{\sim}, \ldots\right\rangle,
$$ and so there is a set of representatives: $\left\{a_1, a_2, \ldots\right\}$ . I do not understand why this result has come about: $$
A / \sim=\left\langle\left[a_1\right]_{\sim},\left[a_2\right]_{\sim}, \ldots\right\rangle,
$$ How to get to it?","['elementary-set-theory', 'cardinals', 'set-theory']"
4699802,$k$-th derivative of a rational function,"Let $f$ be the function given by $$
f(x) = \frac{1}{x+1}.
$$ As we easily check, the $k$ -th derivative of $f$ is given by $$
f^{(k)}(x) = (-1)^k \frac{k!}{(x+1)^{k+1}}.
$$ In particular $|f^{(k)}(x)| = \frac{k!}{(x+1)^{k+1}}$ . Suppose that $R_n$ , $n \ge 1$ is a rational function such that $\displaystyle R_n(x) = \frac{P_n(x)}{Q_n(x)}$ , where $P_n$ and $Q_n$ are polynomials, $R_n$ is proper, that is the degree of $Q_n$ is greater than the degree of $P_n$ . all coefficients of $P_n$ and $Q_n$ are positive, ( EDIT: ) and $Q_n(0) \neq 0$ , and $R_n$ converges to $f$ , in the sense that $$
\lim_{n \to \infty} R_n(x) = f(x).
$$ Does it follow that the $k$ -th derivative of $R_n$ is uniformly (in $k$ and $x$ ) bounded on the positive axis by the $k$ -th derivative of $f$ ? In other words, is it true that there exists $C > 0$ such that $$
|R_n^{(k)}(x)| \le C \frac{k!}{(x+1)^{k+1}}, \qquad x > 0,\ k \ge 1?
$$ Note that I would like to obtain the above estimate just for $x > 0$ : in this interval $x \mapsto x + 1$ and $Q_n$ are positive (hence, nonzero).","['rational-functions', 'derivatives', 'polynomials', 'real-analysis']"
4699808,"Deduce a probability inequality via ""standard symmetrization argument""","Let $\boldsymbol{A}\in\mathbb{R}^{n_1\times n_2}$ be some fixed matrix, and $\{\boldsymbol{X}_i\}_{i=1}^n$ be independent random matrices for which $\mathbb{E}(\boldsymbol{X}_i)=\boldsymbol{A}$ . I would like to deduce the following probability inequality $$
\mathbb{P}\left\{\left\|\frac{1}{n}\sum_{i=1}^n\boldsymbol{X}_i-\boldsymbol{A}\right\|\ge 3t\right\}\le\max_{\|\boldsymbol{u}\|=\|\boldsymbol{v}\|=1}\mathbb{P}\left\{\left\langle\frac{1}{n}\sum_{i=1}^n\boldsymbol{X}_i-\boldsymbol{A},\boldsymbol{u}\boldsymbol{v}^\top\right\rangle\ge t\right\}+4\mathbb{P}\left\{\left\|\frac{1}{n}\sum_{i=1}^n\varepsilon_i\boldsymbol{X}_i\right\|\ge t\right\},
$$ where the $\|\cdot\|$ is the matrix spectral norm, $\{\varepsilon_i\}_{i=1}^n$ are i.i.d. Rademacher (i.e., symmetric Bernoulli) random variables. The probability inequality above is a simplified version of the first step in Proof of Theorem 2 in (the arxiv version of) Yuan, M., & Zhang, C. H. (2017). Incoherent tensor norms and their applications in higher order tensor completion. IEEE Transactions on Information Theory, 63(10), 6753-6766. see https://arxiv.org/pdf/1606.03504.pdf , which was concerning tensors and their more specialized norms, so as to adapt the problem to a wider audience. The paper said ""the standard symmetrization argument gives"" the above inequality, but I totally have no idea about how to do this standard step. I know how to use the symmetrization technique to derive inequalities regarding expectations, such as $\mathbb{E}\left\|\sum_{i=1}^n \boldsymbol{X}_i\right\| \leq 2 \mathbb{E}\left\|\sum_{i=1}^n \varepsilon_i \boldsymbol{X}_i\right\|$ , but I don't know how to apply this to bound probabilities . The paper also gave the following reference about the used ""standard symmetrization argument"" Giné, E., & Zinn, J. (1984). Some limit theorems for empirical processes. The Annals of Probability, 929-989. see https://www.jstor.org/stable/2243347 , but unfortunately the contents of the paper are too advanced to be understandable to me. I am wondering if someone can give me a hint on how to deduce this standard inequality, or point out for me which part of the latter reference implies the result. Thanks in advance.","['inequality', 'probability-distributions', 'probability-theory', 'probability']"
4699811,Can set operatoins be expressed in terms of symmetric difference and set complement?,"I'm trying to prove that the main set operations $A \cap B$ , $A \cup B$ and $A \backslash B$ can be expressed in terms of the symmetric difference and the set complement $A \triangle B$ and $A^{c}$ . I'm having no success, since the symmetric difference is associative and commutative and also since $(A \triangle B)^{c} = A^{c} \triangle B$ , I figured if there's any way to express $A \cup B$ in terms of it then it must be something of the form $A_{1} \triangle A_{2} \triangle ... \triangle A_{n}$ where each $A_{i}$ is either $A$ , $B$ , or $A^{c}$ or $B^{c}$ . Say we group them so that the $A_{i}$ s that have $A$ are together, then we would have something like $A \triangle A \triangle A^{c} \triangle....$ an expression of this form can only ever be equal to $\emptyset, A, A^{c}, U$ where $U$ is the universe. Same goes for the string of $B$ s and $B^{c}$ s. Try out any combination of those four possible $A$ s and the four possible $B$ s and you get none of the set operations mentioned above. For this reason I'm starting to think it might just be impossible, even tho it is listed as an exercise on this book I'm reading.",['elementary-set-theory']
4699857,Reference request: unusual expansion of product of binomial coefficients,"There is a well-known formula for the product of the binomial coefficients: $$\binom{n}{a}\binom{n}{b}=\sum_{i=0}^{min(a,b)}\binom{a+b-i}{i,a-i,b-i}\binom{n}{a+b-i}$$ I'm interested in a different formula for the expansion of the product, where the RHS only contains binomial coefficients of the form $\binom{n+i}{a+b}$ : $$\binom{n}{a}\binom{n}{b}=\sum_{i=0}^{a+b}\binom{a}{i}\binom{b}{i}\binom{n+i}{a+b}$$ More generally, $$\binom{n+c}{a}\binom{n+d}{b}=\sum_{i=0}^{a+b}\binom{a-c+d}{i-c}\binom{b-d+c}{i-d}\binom{n+i}{a+b}$$ This holds whenever $0\le c \le a$ and $0 \le d \le b$ , and it comes from this MO answer regarding a quantized version of the formula . I'm almost certain that I'm not the first to find this formula, but identities involving binomial coefficients are hard to sort through. I was wondering if someone remembers having seen this before and could point me to a reference.","['quantum-groups', 'q-analogs', 'combinatorics', 'reference-request']"
4699859,Exterior Derivative and Lie Derivative on infinite dimensional manifolds,"Lately I have been trying to understand the chapter in Abraham and Marsden's Foundations of Mechanics on infinite-dimensional Hamiltonian systems. Now that I've finally got a feeling for the canonical 1-form on an infinite-dimensional cotangent bundle, I can't get how to arrive at the expression of its exterior derivative.
I have looked through Chernoff and Marsden's Properties of infinite-dimensional Hamiltonian systems and finally also at Abraham, Marsden and Ratiu's Manifolds, Tensor Analysis, and Applications book to get more exposure to some details, backgrounds and conventions. My precise question is how exactly to arrive from the expression of $\theta (x,\alpha) \cdot (e,\beta)= \alpha(e)$ -where $(x,\alpha)\in T^{\star}M$ is the footpoint and $(e,\beta)\in T_{(x,\alpha)}T^{\star}M$ describes the inserted tangent vector- to the given expression for the canonical symplectic form $\omega= -d\theta$ which is $\omega (e,\alpha) \cdot ((e_{1},\alpha_{1}),(e_{2},\alpha_{2}))= \alpha_{2}(e_{1})-\alpha_{1}(e_{2}) $ . In Abraham and Marsden's chapter they are talking about an induced/pulled back version of this on the tangent bundle, but my question is more about the original idea on the cotangent bundle as briefly presented in Chernoff and Marsden.
They supposedly use the local formula for exterior derivative in terms of Lie Derivatives, but I just don't understand how to apply or rather utilize that here, to arrive at this result for $\omega$ , so any comment or help with this step would be much appreciated! Thank you for taking the time and interest :)","['exterior-derivative', 'symplectic-geometry', 'classical-mechanics', 'differential-forms', 'differential-geometry']"
4699881,"Function $g$ has all directional derivatives at $(0,0)$","Let $g:\mathbb{R}^2\longrightarrow\mathbb{R}$ be the function defined by $$g(x,y)=\begin{cases}
\frac{x|y|}{\sqrt{x^2+y^2}} & \text{for } (x,y)\neq(0,0)\\
0 & \text{for } (x,y)=(0,0)
\end{cases}$$ Prove that all directional derivatives of $g$ exist at $(0,0)$ , but $g$ is not differentiable at $(0,0)$ . Problem/Approach: I have already proven that $g$ is not differentiable at $(0,0)$ . I am struggling with the directional derivatives at $(0,0)$ . If $v=(0,0)$ , then it immediately follows that $\nabla_v f(x_0)=0$ . For any vector $v=\begin{pmatrix}v_1\\v_2\end{pmatrix}\in\mathbb{R}^2$ and $\|v\|=1$ with $v\neq(0,0)$ , we have: \begin{align*}
\nabla_a f(0,0)&=\lim_{h\rightarrow 0}\frac{f(0+hv)-f(0,0)}{hv_1|v_2|}\\
&=\lim_{h\rightarrow 0}\frac{\sqrt{(hv_1)^2+(hv_2)^2}}{h}\\
&=\lim_{h\rightarrow 0}\frac{hv_1|v_2|}{\sqrt{(hv_1)^2+(hv_2)^2}h}\\
&=\lim_{h\rightarrow 0}\frac{v_1|v_2|}{\sqrt{v_1^2+v_2^2}h}\\(*)
&=\lim_{h\rightarrow 0}\frac{v_1|v_2|}{h}\\ 
&=\lim_{h\rightarrow 0}v_1|v_2|\\
&=v_1|v_2|=0\quad\Longleftrightarrow v_1=0\vee v_2=0
\end{align*} In (*), I referred to $h>0$ . It all seems very artificial to me, and I have to consider many cases, which makes me doubt whether $g$ has directional derivatives everywhere at $(0,0)$ at all. What am I doing wrong? Thanks for any help!","['multivariable-calculus', 'calculus', 'vector-analysis', 'partial-derivative', 'derivatives']"
4699907,Integral independent of the parameter? $\int_0^\infty\frac{1}{(1+x^a)(1+x)^2}dx$,"My claim is that: $$\int_0^\infty\frac{1}{(1+x^a)(1+x)^2}dx=\frac12$$ regardless of the value of $a$ . A couple examples are here and here . I came across this result while searching cool integrals to evaluate, but I could not tame this one. I know a similar result, that you can find in this answer . It states that: $$\int_0^\infty\frac{1}{(1+x^a)(1+x^2)}dx=\frac{\pi}{4}$$ and it is proved using the substitution $x=\tan u$ , however using the same substitution in my integral seems to be pointless. How should one proceed?","['integration', 'calculus', 'definite-integrals']"
4699952,Compute integral $\int^\infty_{-\infty}\frac{1}{\big(\frac89x^2+\frac23\big)\sqrt{\frac{x^2}{3}+1}}~dx$,"Problem: I am trying to show that $$\frac1{3\pi}\int^\infty_{-\infty}\frac{dx}{\left(\frac89x^2+\frac23\right)\sqrt{\frac{x^2}{3}+1}}=\frac13$$ This identity is link to the integral of the Airy function on $(0,\infty)$ .  I have made many attempts (substitution, contour integration ) to show that but no success so far (the integrand has branch singularities that makes integration ab little harder). If someone has a good idea or a trick to find this integral I will appreciate it. Thank you! Background: The Airy function can be expressed as a improper Riemann integral $$\operatorname{Ai}(s)=\frac{1}{2\pi}\int^\infty_{-\infty}e^{i\big(x^3/3 + sx\big)}\,dx,\qquad s\in\mathbb{R}$$ It is well known that $\operatorname{Ai}\in L_1(0,\infty)$ and that $$\int_{(0,\infty)}\operatorname{Ai}(s)\,ds =\frac13$$ I am aware some methods to obtain this integral that rely on Laplace's transform and distribution theory (here for example ), and also by the relation between the Airy function $\operatorname{Ai}$ and the modified Bessel function of the second kind. The former has the drawback that the validity of change of order of integration is harder to justify; latter is is more to my liking since one can use Fubini-Tonelli's theorem to justify change of order of integration,  however it requires knowledge of integral representation of Bessel functions (not a trivial feat). I am trying to obtain the value of the desired integral by simple methods of Lebesgue integration and Complex Analysis using contour deformation that gives an integrand suitable for Fubini's theorem: The change of variables $x=s^{1/2}u$ gives $$\operatorname{Ai}(s)=\frac{s^{1/2}}{2\pi}\int^\infty_{-\infty}e^{-s^{3/2}i\big(\frac{u^3}{3}+u\big)}\,du$$ Let $F(z)=\frac{z^3}{3}+z$ , $z\in\mathbb{C}$ . A simple calculation gives \begin{align}
\mathfrak{R}(F)(z)&=x^2y-\frac{y^3}{3}+y\\
\mathfrak{I}(F)(z)&=-\frac{x^3}{3}+xy^2-x
\end{align} By deforming the Contour of integration as in method of steepest descent and some estimates, we obtain that \begin{align}
\operatorname{Ai}(s)=\frac{s^{1/2}}{2\pi}\int_\gamma e^{-s^{3/2}\mathfrak{R}(F(z))}\,dz=\frac{s^{1/2}}{2\pi}\int^\infty_{-\infty} e^{-s^{3/2}\big(\frac89x^2+\frac23\big)\big(\frac{x^2}{3}+1\big)^{1/2}}\,dx\tag{1}
\end{align} where $\gamma$ is the upper branch of the hyperbola $y^2-\frac{x^2}{3}=1$ and where $\mathfrak{I}(F(z))=0$ .
The integrand in (1) is suitable for Fubini's theorem which gives $$\int^\infty_0\operatorname{Ai}(s)\,ds=\frac{1}{2\pi}\int^\infty_0 s^{1/2}\int_\gamma e^{-s^{3/2}\mathfrak{R}(F(z))}\,dz\,ds=\frac{1}{3\pi}\int^\infty_{-\infty}\frac{dx}{\big(\frac89x^2+\frac23\big)\big(\frac{x^2}{3}+1\big)^{1/2}}$$ Clearly the function $\phi(x)=\frac{1}{3\pi\big(\frac89x^2+\frac23\big)\big(\frac{x^2}{3}+1\big)^{1/2}}$ is Lebesgue integrable and in hindsight $\int_{\mathbb{R}}\phi=\frac13$ . I checked my calculation and verified, numerically, that (1) $\phi$ is the correct expression obtained from the method described above, and (2) that indeed it integrates to $1/3$ .","['complex-analysis', 'lebesgue-integral', 'special-functions', 'real-analysis']"
4699953,Every boundary point of a simply connected open set $U$ can be joined with $\infty$ with a curve $\gamma$ which does not intersect $U$,"I stumbled upon the following problem in one of my problem sets on the Riemann Mapping Theorem, which states: Let $U \subseteq \mathbb{C}$ be simply connected and open. Then, for every point $\alpha$ in the boundary of $U$ , there exists a curve $\gamma :[0,1] \to \mathbb{C} \cup \{ \infty \} $ (by curve we will understand continuous function) that joins $\alpha$ and $\infty$ , that is to say, $\gamma(0) = \alpha$ and $\gamma(1) = \infty$ and such that $U \subset \mathbb{C}   \setminus \gamma ([0,1])$ . This proposition was used in proving the following lemma used in the proof of the Riemann Mapping theorem: If $U \subset \mathbb{C}$ , $U \neq \mathbb{C}$ is simply connected and open, then there exists a  bijective analytic mapping $f: U \to V$ such that $V$ is an open subset of $\{ z \in \mathbb{C} : |z| = 1\}$ . However, I have tried to prove the theorem by contradiction and obtaining a closed curve in $U$ whose interior is not totally contained in $U$ , and some other attempts, but, I have not obtained much from them. I need some help in seeing how to prove this result. The characterization: $G \subseteq \mathbb{C}$ is simply connected iff for every closed jordan curve $\gamma : [0,1] \to G$ its interior lies entirely in $G$ can be used. The course which I am attending to and from where I got this problem is mainly based in Lang's and Silvermann's texbooks on the subject, but, I did not found any information concerning the result in them. Addendum: I have been able to prove that if $G$ is open and simply connected then its complement in the Riemann Sphere or in the extended plane, must be connected, however, I have not been able to prove that its complement is simply connected",['complex-analysis']
4700011,Looking for a weird function,"I am looking for a function $f: \mathbb{N} \rightarrow \mathbb{R^+}$ that satisfies these 3 conditions: There exist $c>0,k>0$ such that for all $n$ , $f(n) \leq c \cdot n^k$ Let $b\in \mathbb{N^{\geq 2}}$ . For any $d>0$ , there exists $n_0$ such that for all $n\geq n_0$ , we have $f(bn)\geq df(n)$ Edit : The original 2nd condition: Let $b\in \mathbb{N^{\geq 2}}$ . For any $d>0$ , there exists $n_0$ such that we have $f(bn_0)\geq df(n_0)$ , ie $\frac{f(bn)}{f(n)}$ is unbounded. $\exists n_0$ such that $\forall n \geq n_0$ we have $f(n+1) \geq f(n)$ Any hints on the general form of such a function? Edit: Any polynomial function wouldn't satisfy the 2nd condition. Any exponential/factorial function wouldn't satisfy the 1st condition. I have tried using online calculators to evaluate the limit of $\frac{\log((bn)!)}{\log(n)!}$ , it isn't unbounded. I can't really think of any other types of functions.","['functions', 'real-analysis']"
4700036,What is the difference between the Auxiliary Equation and the Characteristic Equation? (ODEs),"When learning to solve homogeneous ODEs with constant coefficients a, b, and c, the resulting polynomial ( $ar^2 + br + c$ ) is referred to in my text as the auxiliary equation . However, in the next section on solving homogeneous ODEs with variable coefficients (specifically dealing with an ODE of the form $at^2y'' + bty' + cy = 0$ ), the resulting polynomial
[ $ar^2 + (b-a)r + c$ ] is referred to as the characteristic equation . What--if any--is the primary difference/distinction being made in the names of the two polynomials in these applications?",['ordinary-differential-equations']
4700051,Simplifying arctan expression using Puiseux series,"Mathematica gives me the following expression which works well for $s$ near $0$ , any idea how to derive this manually? $$\left(\frac{s}{2}+\frac{\sqrt{2} \sqrt{s}}{\pi -2 \tan ^{-1}\left(\frac{\sqrt{s}}{\sqrt{2}}\right)}\right)^{-1} \approx \frac{\pi }{\sqrt{2} \sqrt{s}}$$ expr = 1/(s/2 + (Sqrt[2] Sqrt[s])/(\[Pi] - 2 ArcTan[Sqrt[s]/Sqrt[2]]));
asymp = Asymptotic[expr, s -> 0] This appears to be the first term in Series expansion below. Using $x=2s^2$ replacement, the series looks like below $$
\begin{array}{ccc}
  & \text{order} & \text{expr} \\
  & 0 & \frac{\pi }{2 x}-\frac{\pi ^2}{4}-1 \\
  & 1 & \frac{1}{8} \left(8 \pi +\pi ^3\right) x+\frac{\pi }{2 x}-\frac{\pi ^2}{4}-1 \\
  & 2 & \frac{1}{48} \left(-32-36 \pi ^2-3 \pi ^4\right) x^2+\frac{1}{8} \left(8 \pi +\pi ^3\right) x+\frac{\pi }{2 x}-\frac{\pi ^2}{4}-1 \\
\end{array}$$ Notebook","['calculus', 'laplace-transform', 'ordinary-differential-equations']"
4700070,Optimization problem involving the inverse matrix,"I have a question related to optimization. Given natural numbers $n$ and $\ell$ , matrices ${\bf K}_1, \dots, {\bf K}_\ell \in \Bbb R^{n \times n}$ and a vector ${\bf y} \in \Bbb R^n$ , define $${\bf K} := s_1 {\bf K}_1 + \dots + s_\ell {\bf K}_\ell + \lambda {\bf I}_n$$ where $s_1, \dots, s_\ell, \lambda \in \Bbb R$ , $\lambda > 0$ and ${\bf I}_n$ is the identity matrix. It is also given that the ${\bf K}_1, \dots, {\bf K}_\ell$ are all symmetric and semidefinite so ${\bf K}$ is invertible. The problem is to minimize ${\bf y}^\top {\bf K}^{-1} {\bf y}$ under the constraints $s_1, \dots s_\ell \ge 0$ and $s_1 + \dots + s_\ell = 1$ . I have searched that this problem is related to semidefinite programming (SDP) but since I have no background on optimization theory, I cannot proceed any further. How can I solve this problem?","['matrices', 'optimization', 'convex-optimization', 'semidefinite-programming']"
4700086,"$|x|=|y|=|z|=1$ and $x^3+y^3+z^3=-xyz$, are there infinite many values to $|x+y+z|$","I was playing around with complex numbers and I tried this. Let $x$ , $y$ , $z$ be complex numbers with the properties $$|x|=|y|=|z|=1,$$ $$x^3+y^3+z^3=-xyz.$$ The question is how many values the following expression can take? $$|x+y+z|.$$ It is easy to see that $$|x+y+z|≤3,$$ but I found only $2$ solutions
if $x=z=-y$ then $|x+y+z| =1$ ,
and $$x=\frac{1}{2}+\frac{\sqrt{3}}{2}i ,\; y=-\frac{1}{2}+\frac{\sqrt{3}}{2}i,\;  z=1 $$ then $|x+y+z|=2$ . I believe there are infinitely many values but I couldn't find any other than these two solution. I want a prove that there are infinite many values for $|x+y+z|$ . If there are finite values for $|x+y+z|$ then how many values? and also what is the greatest value of $|x+y+z|$ .","['complex-analysis', 'inequality', 'complex-numbers']"
4700087,Does every equalizing sequence converge?,"Let $(a_1,a_2, \dots, a_n) = (0, 0, \dots, 0)$ . There are sets $B_1, \dots, B_k\subseteq\{1,2,\dots,n\}$ , each of size $2$ . At time step $t$ , consider the index $i\in\{1,\dots,k\}$ such that $i\equiv t\pmod k$ , and let the two numbers in $B_i$ be $x$ and $y$ , where $a_x \ge a_y$ . If $a_x \ge a_y + 1$ , we increase $a_y$ by $1$ . Else, we increase $a_y$ until it is equal to $a_x$ , then increase both $a_y$ and $a_x$ until the total amount we increased is $1$ . Let $(b_1, \dots, b_n) = (a_1/t, \dots, a_n/t)$ . Is it true that, as $t\rightarrow\infty$ , the sequence $(b_1,\dots,b_n)$ converges? Example 1: $n = 3$ , $k=2$ , $B_1 = \{1,2\}$ , $B_2 = \{2,3\}$ . At $t = 1$ , we update $(a_1, a_2, a_3)$ to $(0.5,0.5,0)$ . At $t = 2$ , it becomes $(0.5, 0.75, 0.75)$ , so $(b_1,b_2,b_3) = (0.25,0.375,0.375)$ . At $t = 3$ , $(a_1,a_2,a_3) = (1.125,1.125,0.75)$ and $(b_1,b_2,b_3) = (0.375,0.375,0.25)$ . Eventually, $(b_1,b_2,b_3)$ converges to $(1/3,1/3,1/3)$ . Example 2: $n = 3$ , $k=4$ , $B_1 =B_2 = B_3 = \{1,2\}$ , $B_4 = \{2,3\}$ . At $t = 1$ , $(a_1, a_2, a_3)=(\frac12,\frac12,0)$ . At $t = 2$ , $(a_1, a_2, a_3)=(1, 1, 0)$ and $(b_1,b_2,b_3) = (\frac12,\frac12,0)$ . At $t = 3$ , $(a_1,a_2,a_3) = (\frac32,\frac32,0)$ and $(b_1,b_2,b_3) = (\frac12,\frac12,0)$ . At $t = 4$ , $(a_1,a_2,a_3) = (\frac32,\frac32,1)$ and $(b_1,b_2,b_3) = (\frac38,\frac38,\frac14)$ . Eventually, $(b_1,b_2,b_3)$ converges to $(\frac38,\frac38,\frac14)$ .","['convergence-divergence', 'sequences-and-series', 'dynamical-systems', 'real-analysis']"
4700098,What is the best bound for $f'$ knowing bounds on $f$ and $f'''$?,"Let $f:\mathbb R\to \mathbb R$ be a three-times differentiable function. Suppose $|f(x)|\le 1$ and $|f'''(x)|\le 3$ for any $x\in \mathbb R$ . Show that $|f'(x)|\le 1$ for any $x\in \mathbb R$ . The hint to this question is to apply Taylor's theorem on $[x-h,x]$ and $[x,x+h]$ for any $x\in \mathbb R$ and $h>0$ . I have $$
\begin{align}
f(x-h)&=f(x)-f'(x)h+{f''(x)\over 2}h^2-{f'''(w_1)\over 6}h^3, \\
f(x+h)&=f(x)+f'(x)h+{f''(x)\over 2}h^2+{f'''(w_2)\over 6}h^3
\end{align}
$$ Hence $$f(x+h)-f(x-h)=2hf'(x)+{f'''(w_1)+f'''(w_2)\over 6}h^3.$$ However, I don't know how to proceed from here.
I only know that $|f(x+h)-f(x-h)|\le 2$ . Can anyone help?","['interpolation-theory', 'derivatives', 'taylor-expansion', 'analysis']"
4700122,"Show there exists $c\in(0,1)$ such that $\frac{f'(1-c)}{f(1-c)}=\frac{2f'(c)}{f(c)}$","I got a question like this: Prove there exists $c\in(0,1)$ in which $\displaystyle \frac{f'(1-c)}{f(1-c)}=\frac{2f'(c)}{f(c)}$ , where $f:[0,1]\rightarrow\mathbb{R}$ is a differentiable function on $[0,1]$ such that $f(0)=0$ and $f(x)>0$ for any $x\in(0,1]$ . The first thing that comes to my mind is the Cauchy MVT, I tried with $g(x)=f(x)^2$ and created the term $g'(x)=2f(x)f'(x)$ so I can have the coefficient 2, but I could not proceed. Then I tried to rearrange the terms and notice $f'(1-c)f(c)-f'(c)f(1-c)=f'(c)f(1-c)$ is what we need to prove, the terms in the LHS is actually the derivative of $h(x)=f(x)f(1-x)$ , where $h(0)=h(1)=0$ , but I still failed to proceed. May I look for some advice?","['mean-value-theorem', 'derivatives', 'analysis']"
4700136,Hypotheses of Nakayama's lemma are necessary,"Nakayama's lemma. Let $M$ be a finitely generated $A$ -module, $I\subseteq A$ an ideal such that $I\subseteq \mathfrak{R}(A)$ Jacobson's radical of $A$ . If $IM=M$ , then $M=0$ . I'd like to show that both the finitely generated hypothesis on $M$ and the inclusion hypothesis in the Jacobson's radical $\mathfrak{R}(A)$ are necessary. Explicitly, I'm looking for: A non finitely generated $A$ -module $M\neq 0$ with an ideal $I\subseteq \mathfrak{R}(A)$ such that $IM=M$ . A finitely generated $A$ -module $M\neq 0$ with an ideal $I\nsubseteq \mathfrak{R}(A)$ such that $IM=M$ . I found that $\mathbb{Q}$ as $\mathbb{Z}_{(2)}$ -module with the maximal ideal $I=(2)\cdot\mathbb{Z}_{(2)}$ works for the first request. I couldn't find any example for the second request, apart from the trivial $\mathbb{Z}$ as $\mathbb{Z}$ -module with $I=\mathbb{Z}$ . Is there any example with an ideal $I\nsubseteq \mathfrak{R}(A)$ but still a proper ideal of $A$ ?","['abstract-algebra', 'commutative-algebra']"
4700151,Does every triangle satisfy $(a+b-c) \ge 8s\sum_{k=1}^{\infty}C_k \left(\frac{A}{s^2}\right)^{2k}$?,"For a fixed semi-perimeter $s$ and a fixed area $A$ , we can find infinitely many triangles of sides $a,b$ and $c$ whose semi-perimeter is $s$ and area is $A$ . I wanted to improve the triangle inequality by finding a expression for the minimum value of $a+b-c$ in terms of $s$ and $A$ . Partial progress was made in this question and also this question . I obtained the following inequality experimentally using Monte Carlo simulation. $$
a+b-c \ge \frac{8A^2}{s^3} + \frac{64A^4}{s^7} + \frac{896A^6}{s^{11}}
$$ The first term of $(1)$ is proved in first of the above links. $(1)$ suggests the folowing conjecture: Conjecture : There exists constants $C_k$ such that $$ \min(a+b-c) =  8s\sum_{k=1}^{\infty}C_k \left(\frac{A}{s^2}\right)^{2k}\tag 1 $$ Can this conjecture be proved and can we express $C_k$ in terms of $k$ ? We have $C_1 = 1, C_2 = 8$ and $C_3 = 112$ . Interestingly, there are several sequences in OEIS which begin with $1,8,112$ . SageMath Code for estimating $C_4$ import random as rn

R  = 1
x1 = 0
y1 = R
min1 = 10**999
stop = False
i = 1

while stop == False:
    x2 = R*rn.random()
    y2 = (R^2 - x2^2)^0.5
    x3 = R*rn.random()
    y3 = (R^2 - x3^2)^0.5
    a = (((x2)**2 + (y1 - y2)**2)**0.5)
    b = (((x2 - x3)**2 + (y2 - y3)**2)**0.5)
    c = (((x3 - x1)**2 + (y3 - y1)**2)**0.5)
    s    = ((a+b+c)/2)
    area = ((s*(s-a)*(s-b)*(s-c))**0.5)
        
    try:
        test1 = ((a+b-c) - (8*area^2/s^3 + 64*area^4/s^7 + 896*area^6/s^11)) / (area^8/s^15)
    except:
        continue

    if test1 < min1:
        min1 = test1
        print(a,b,c,min1/8)
    
        if min1 <= 0:
            print('exception', min1.n(), p)
            stop = True
            break
            
    if i%10^7 == 0:
        print(i)
    i = i + 1","['geometry', 'asymptotics', 'real-analysis', 'triangles', 'inequality']"
4700201,"In L'Hospital rule should we specify $g(x) \ne 0$ on $(a,b)$ in the theorem to avoid division by $0$? Does $g'(x)\ne 0$ imply $g(x)\ne 0$ on $(a,b)$?","L'Hospital rule (according to Rudin or this website ): Suppose that $f$ and $g$ are differentiable on the open interval $(a,b)$ , that $\lim_{x\to a^+} f(x) = 0$ , $\lim_{x\to a^+} g(x) = 0$ , that $g'(x) \ne 0$ on $(a,b)$ and that $\lim_{x\to a^+} \frac{f'}{g'}$ exists.
Then, $$\lim_{x\to a^+} \frac{f(x)}{g(x)} = \lim_{x\to a^+} \frac{f'(x)}{g'(x)}$$ Here is a proof (the proof given in the website ): Let $f(a)=g(a)=0$ Then $f,g$ are continuous on $[a,b)$ . Let $x\in (a,b)$ . $f,g$ are continuous on $[a,x]$ and differentiable on $(a,x)$ . By the generalized MVT, we have: $$f(x) g'(c)= g(x) f'(c)$$ for a certain $c\in (a,x)$ . Hence, $$\frac{f'(c)}{g'(c)} = \frac{f(x)}{g(x)} \tag{1}$$ We conclude by taking the limit as $x$ tends to $a^+$ . My question is: why does the statement of the theorem (be it from Rudin, on from the website ) doesn't require $g(x)\ne 0$ on $(a,b)$ ? In $(1)$ we get to divide by $g(x)$ . Do we have the guarantee that the RHS denominator doesn't blow up when the LHS denominator is different from $0$ as in $(1)$ ? Thank you. Edit: From $(1)$ we have: $$g(x) = \frac{g'(c)}{f'(c)} f(x)$$ However we don't have the guarantee that $f(x)/f'(c) \ne 0$","['mean-value-theorem', 'derivatives']"
4700232,Book recommendation on complex analysis with historical motivation,"I am looking for a ""thick"" book on complex analysis, which also has historical details such as the motivation behind it. Which problem lead to the creation/discovery of a certain topic. E.g. what is the reason behind fundamental groups how is it in connection with topology. Thanks in advance.","['complex-analysis', 'book-recommendation']"
4700308,Game: $\frac{1}{3}$ winning probability vs. $\frac{2}{3}$ winning probability,"$A$ and $B$ play a round-based game. Each round $A$ wins with probability $\frac{1}{3}$ and $B$ with probability $\frac{2}{3}$ . The loser of a round pays $1$ USD to the winner. The winner of the whole game is the one who wins all the USD from the other player. Now assume that $A$ starts with $n\geq 1$ USD and $B$ with $1$ USD. What is the winning probability of $B$ ? (Hint: Use recursion and the law of total probability) Let be $P(B)$ the probability that $B$ wins the game and $P(B|(k))$ the probability that $B$ wins the game if $B$ has $k\geq 1$ USD. So either one could understand the question in the sense that we must compute $P(B)$ or we must compute $P(B|(1))$ . However, in both cases we must apply law of total probability which requires us to find a disjoint decomposition of $B$ or $(B\cap (1))$ , respectively. As we don't know how the corresponding sets are defined we can't do this. Maybe someone can help me to disentangle this problem or show me how to set up a proper recursion without knowing how the sets look like.","['conditional-probability', 'infinite-games', 'probability', 'recursion']"
4700314,equations for cuspidal curves,"The space of morphisms $\mathbb P^1\to \mathbb P^2$ of degree $3$ is a Zariski open subset $U$ of $$
\mathbb P(H^0(\mathbb P^1, \mathcal O(3))^{\oplus 3}) \cong \mathbb P^{11}
$$ the generic rational map is a nodal cubic, so the subset $V\subset U$ of cuspidal curves is of codimension at least $1$ . How can one find equations for $\overline{V}\subset \mathbb P^{11}$ ? I expect it to be a hypersurface but I cannot figure out how to write down the equation.","['algebraic-curves', 'algebraic-geometry', 'geometry']"
4700320,How to prove: $ab+bc+ca+\sqrt[3]{abc}+20\ge 2\sum_{cyc}\sqrt{a(4a+ab+bc+ca)}$ if $a+b+c=5$,"Problem 1 . Let $a,b,c\ge 0: a+b+c=5$ . Prove that: $$ab+bc+ca+\sqrt[3]{abc}+20 \ge 2\left(\sqrt{a(4a+ab+bc+ca)}+\sqrt{b(4b+ab+bc+ca)}+\sqrt{c(4c+ab+bc+ca)}\right)$$ Equality holds iff $(a,b,c)=\left(\dfrac{5}{3},\dfrac{5}{3},\dfrac{5}{3}\right);\left(5,0,0\right)$ and per cyclic. Original link Here is my attempt: Case 1: $abc=0$ : WLOG, assume $a=0:b+c=5$ , we just need to prove: $$bc+20\ge 2\left(b\sqrt{(4+c)}+c\sqrt{(4+b)}\right)$$ Put $b=5-c$ , it becomes: $$-c^2+5c+20\ge 2\left((5-c)\sqrt{(4+c)}+c\sqrt{(9-c)}\right)$$ which is true $\forall 0\le c\le 5$ . See also here Case 2: $abc>0$ : Notice that $a=b=c=\dfrac{5}{3} \implies 4a+ab+bc+ca=9a$ .Using AM-GM inequality: $$2\sum_{cyc}\sqrt{a(4a+ab+bc+ca)}\le \frac{13(a+b+c)+3(ab+bc+ca)}{3}$$ Hence, we will prove: $$ab+bc+ca+\sqrt[3]{abc}+20\ge\frac{13(a+b+c)+3(ab+bc+ca)}{3}$$ Or: $\sqrt[3]{abc}\ge 1$ which is not true. I also try to prove stronger: $$\color{red}{\sqrt[3]{abc}+\dfrac{5(ab+bc+ca)}{a+b+c}+4(a+b+c)\ge 2\sum_{cyc}\sqrt{a\left(4a+\frac{5(a+b+c)}{3}\right)}}$$ But it is not true for $a=b=x\rightarrow \frac{5}{3}$ I am very appreciate if someone can give me an advice or a hint to continue solve the tough problem. P/S: Thank you @arqady for inviting me join this website. Hope to see your help! Edit: arqady's idea inspired me. Problem 2 . Given $a,b,c\ge 0$ . Prove that: $$\sqrt[3]{abc}+2+2(a+b+c)\ge$$ $$ \sqrt{a\left(4a+\sqrt[3]{abc}+4\right)}+\sqrt{b\left(4b+\sqrt[3]{abc}+4\right)}+\sqrt{c\left(4c+\sqrt[3]{abc}+4\right)}$$ Equality holds iff $a=b=c=1; a=b\Rightarrow +\infty;c=0$ and pers.","['uvw', 'inequality', 'a.m.-g.m.-inequality', 'cauchy-schwarz-inequality', 'algebra-precalculus']"
4700386,Modulus of characteristic function equal to 1 implies almost surely constant,"Let $X$ be a random variable, and let $\hat{\mu_X}$ be its characteristic function. Suppose that $|\hat{\mu_X}(u)| = |\hat{\mu_X}(v)| = 1$ for some $u,v \in \mathbb{R}^*$ , with $uv^{-1} \not \in \mathbb{Q}$ . We want to show that $X$ is a.s. constant. My solution : if $|\hat{\mu_X}(u)| = 1$ , it is not difficult to show that the image of $uX$ (on a set of full measure) lies in $\theta + 2 \pi \mathbb{Z}$ for some $\theta \in \mathbb{R}$ . Hence, the image of $X$ (almost surely) lies in: $$ \bigg( \frac{\theta_1}{u} + \frac{2 \pi}{u} \mathbb{Z} \bigg) \cap \bigg( \frac{\theta_2}{v} + \frac{2 \pi}{v} \mathbb{Z} \bigg)$$ for some $\theta_1, \theta_2 \in \mathbb{R}$ . This has at most one solution, by the conditions on $u,v$ , so $X$ is almost surely constant. Question : Is this proof correct? I'm slightly unsure because the question gives the hint to consider ""an independent copy of $X$ "", so maybe an argument like this is expected, but I don't see how it works here, and if the above is correct, then it is surely simpler. I was also wondering if it is relevant that $\langle u, v \rangle \le (\mathbb{R}, +)$ is dense, but I couldn't think of how that might be applied either. I would be interested to see any other arguments which can be used to solve this question.","['measure-theory', 'solution-verification', 'probability', 'random-variables']"
4700418,Integral with closed form? $\int_0^\infty\frac{x^n\ln x}{(1+x)^2(1+x^m)}dx$,"I am trying to evaluate $$I(n,m)=\int_0^\infty\frac{x^n\ln x}{(1+x)^2(1+x^m)}dx$$ and I'm strongly convinced that it has a nice closed form. This is because if you plug small values for $(n,m)$ , you get neat results in terms of $\pi^2$ . Here are some examples: $$I(0,2)=-\frac{\pi^2}{16}$$ $$I(0,3)=-\frac16-\frac{4\pi^2}{81}$$ $$I(0,4)=\frac{2\sqrt2-5}{32}\pi^2$$ $$I(1,3)=\frac16-\frac{2\pi^2}{81}$$ $$I(3,4)=\frac{4\sqrt2-5}{64}\pi^2$$ I managed to calculate some of them via partial fractions. The others are from WolframAlpha. Two interesting facts : 1) Notice that $$I(n,m)=-I(m-n,m)$$ This is easily obtained by letting $x=\frac1t$ in the integral. It follows that, for example: $$I(2,3)=\frac{2\pi^2}{81}-\frac16$$ $$I(1,4)=\frac{5-4\sqrt2}{64}\pi^2$$ 2) From the same result follows also that $I(n,2n)=0$ , and so we get beautiful results like $$\int_0^\infty\frac{\sqrt{x}\log x}{(1+x)^3}dx=0$$ I am clueless on how to solve the general case. I think it's useful to reduce it to $$I(n,m)=\int_0^1\frac{x^n\log x}{(1+x)^2(1+x^m)}dx-\int_0^1\frac{x^{m-n}\log x}{(1+x)^2(1+x^m)}dx$$ Expanding $\frac{1}{1+t^m}$ in series reduces the problem to evaluating integrals of the form $$\int_0^1\frac{x^a\log x}{(1+x)^2}dx$$ and then summing the result over $\mathbb N$ , but I'm not sure how to proceed from here.","['integration', 'calculus', 'definite-integrals']"
4700475,Three married couples on a rectangular table with 6 chairs,"There is a rectangular table as shown in the figure, which has three chairs on two sides each. There married couples sit on these chairs. Find the number of ways in which couples sit either in front of each other or adjacently. What I tried: Case 1: All couples are sitting in front of each other: $$3!\times2^3$$ $2^3$ is the number of ways in which the member of a couple can interchange the position and $3!$ is the permutation of the columns. Case 2: One couple is sitting in front of each other and two adjacently. $$2\times 3! \times 2^3$$ $2^3$ is the number of ways in which the member of a couple can interchange the position and $3!$ is the permutations of the couples and $2$ is because there are two ways of arrangement as shown in the figure. So total number of ways: $48+96=144$ But the given answer is $112$ What am I doing wrong?","['permutations', 'combinatorics']"
4700520,When does a distribution admit a closed top-dimensional differential form?,"Let $M^n$ be a smooth manifold. Let $\mathcal{D}$ be a $k$ -dimensional integrable distribution on $M$ . Denote by $\mathcal{D}^\bot\to M$ the vector bundle which is spanned by the differentials of the set of first integrals of $\mathcal{D}$ . That is, at a point $p$ we have $\mathcal{D}^\bot|_p = \left<\mathrm{d}f\ \Big|\ \forall\xi\in\mathcal{D}|_p\colon\xi(f) = 0\right>$ . Then for any point $p\in M$ there are functions $f^1_p, \ldots, f^{n - k}_p$ such that in a neighbourhood of $p$ the differential $(n - k)$ -form $\omega_{(p)} := \mathrm{d}f^1_p\wedge\ldots\wedge\mathrm{d}f^{n - k}_p$ is nowhere vanishing and the vector bundle $\mathcal{D}^\bot$ equals $\left<\mathrm{d}f^1_p, \ldots, \mathrm{d}f^{n - k}_p\right>$ , so $\omega_{(p)}$ is ""top-dimensional"". Clearly, we have $\mathrm{d}\omega_{(p)} = 0$ . Also, each differential $(n - k)$ -form $\tau\in\bigwedge^{n - k}\mathcal{D}^\bot|_p$ is proportional to $\omega_{(p)}$ . As one can see, the existence of such $\omega_{(p)}$ for each $p\in M$ is a local condition. What I wonder is when a similar condition is met globally. To be precise, the question is the following. When does there exist a nowhere vanishing $(n - k)$ -dimensional differential form $\omega$ such that $\mathrm{d}\omega = 0$ and at each point $p\in M$ the form $\omega$ is proportional to $\omega_{(p)}$ (or, equivalently, $\mathrm{d}\omega = 0$ and $\forall p\in M\colon\omega|_p\in\bigwedge^{n - k}\mathcal{D}^\bot|_p$ )? This obviously always holds for $k = 0, n$ (a constant function and a volume form will suffice). However, I have not been able to think of (and prove) a criterion even for $k = 1$ . In particular, I have tried using a partition of unity but have not succeeded.","['vector-bundles', 'differential-forms', 'differential-geometry']"
4700525,"Find all real numbers $a$ for equation $x^3 + ax^2 + 51x + 2023=0$, has two equal roots.","Problem: Find all real numbers $a$ for which the equation, $x^3 + ax^2 + 51x + 2023=0$ , has two equal roots. This problem is from an algebra round of a local high school math competition that has already ended My Work: To find when $x^3 + ax^2 + 51x + 2023$ has two equal roots, we can use the properties of the discriminant. For a cubic polynomial, the discriminant is given by the following formula: $Δ = 18abcd - 4b^3d + b^2c^2 - 4ac^3 - 27a^2d^2$ In our case, the cubic polynomial is $x^3 + ax^2 + 51x + 2023$ , so $a = 1$ , $b = a$ , $c = 51$ , and $d = 2023$ . If the polynomial has two equal roots, its discriminant must be equal to zero. So, we can set Δ equal to zero and solve for a: $0 = 18(1)(a)(51)(2023) - 4a^3(2023) + a^2(51)^2 - 4(1)(51)^3 - 27(1)^2(2023)^2$ However, after this step, I am really stuck. I would appreciate some help! Thank you!","['cubics', 'roots', 'roots-of-cubics', 'polynomials', 'algebra-precalculus']"
4700527,"With the epsilon-delta definition of continuity, are the rationals continuous?","My current knowledge is that a function is continuous at a point $x=a$ if and only if, for any $\epsilon>0$ , there exists some $\delta>0$ such that $$
 |x-a|<\delta \implies |f(x)-f(a)|<\varepsilon.
$$ If I have a function $f: \mathbb{Q} \to \mathbb{R}$ , say $f(x)=\log\left(\sqrt{x}\right)$ , where the domain is all positive rationals, surely I can always find a $\delta$ for any $\varepsilon$ ? However, I feel like I’ve heard that the rationals aren’t continuous. Is that false (that is, they are actually continuous), or is there an additional restriction to the definition of continuity?",['analysis']
4700530,"Two different models, both called Nil-geometry: what is the precise relation between them?","Trying to understand a bit about the so called Nil-Geometry, I have found two models (are there more?), namely: The Heisenberg group : we identify the points $(x,y,z)\in \mathbb{R}^3$ with the matrices of the form $\begin{pmatrix}1&x&z\\ 0&1&y\\ 0&0&1\end{pmatrix}$ and consider them as a subgroup of $Gl_3(\mathbb{R})$ , endowing $\mathbb{R}^3$ with a new (and nil - sorry for the pun) Lie group structure. The group operation looks like: $$(x_1,y_1,z_1)\ast (x_2,y_2,z_2) = (x_1+x_2,y_1+y_2,z_1+z_2+x_1y_2).$$ Then considering the riemannian metric $$ds_\ast^2:= dx^2+dy^2+(dz-xdy)^2$$ (which is left-invariant), we get a first model for the three-dimensional Nil-geometry. Consider now another group operation on $\mathbb{R}^3$ : $$(x_1,y_1,z_1)\star (x_2,y_2,z_2) = (x_1+x_2,y_1+y_2,z_1+z_2+\frac{1}{2}(x_1y_2-x_2y_1)),$$ and another (also left-invariant) riemannian metric, $$ds_\star^2 = dx^2+dy^2+(dz+\frac{1}{2}(ydx-xdy))^2.$$ This is also said to be a model for three-dimensional Nil-geometry. One can find these two models in different sources out there and everyone calls them by the same name. Indeed, they somehow look similar and I was able to prove some basic facts, for example, both groups are step-2 nilpotent (therefore considering left-invariant riemannian metrics and calling both Nil-geometry models seems plausible enough). However, if you optimistically look at the map $((x_1,y_1,z_1),\ast,ds_\ast)\mapsto ((x_1,y_1,z_1),\star,ds_\star)$ , hoping it to be an isometry (or at least something like a dilation), you find out it is not even a group homomorphism... So, what is the precise relation between these two models? Is there a not so obvious isometry (or dilation) between them? What does give us the right to call them both Nil-geometry models?","['riemannian-geometry', 'heisenberg-group', 'geometry', 'nil-geometry', 'lie-groups']"
4700554,Every proper subgroup of a simple group containing an order 45 element has an index of at least 14,"Every proper subgroup of a simple group containing an order $45$ element has an index of at least $14$ . So far I've supposed that $G$ is a simple group with an element of order $45$ . Then $45$ divides the order of $G$ . Also, suppose there's a proper subgroup $N$ with index $n$ . Then then there's a mapping $\phi:G\rightarrow S_n$ . Note that $G$ is simple so the kernel is trivial and this map is injective. Thus $|G|=|\text{Im}(\phi)|$ . Since the image is a subgroup of $S_n$ , $|\text{Im}(\phi)|$ divides $|S_n|=n!$ . Ultimately, $45$ divides $n!$ , but I think this shows that $n\geq 6$ , not $14$ . How can I make the jump to $14$ ? Any suggestions?","['symmetric-groups', 'simple-groups', 'group-theory', 'abstract-algebra']"
4700560,On the number of permutations on $n$ letters with the greatest cycle length of $k$,"I am trying to understand the theory ""On the number of permutations on $n$ objects with the greatest cycle length of $k$ "" by Solomon W Golomb and Peter Gaal. Let $L_{k,n}$ denotes the number of permutation on $n$ letters having a greatest cycle of length $k$ . I want to understand the formula $$L_{k,n}=\sum_{j=1}^{\lfloor n/k \rfloor}\frac{1}{j!k^j}\frac{n!}{(n-kj)!}\sum_{t=1}^{min(k-1,n-kj)}L_{t,n-kj} \quad \quad (1 <k\le n)$$ I am considering the case $\frac n4<k \le \frac n3$ . Then $$L_{k,n}=\frac {n!}{k} \left \{ 1-\sum_{j=1}^{n-2k}\frac{1}{k+j}+\sum_{j=1}^{n-3k-1}\sum_{l=1}^{n-3k-j} \frac{1}{2(k+j)(k+l)}        -\frac {1}{2k} \bigg[ 1-\sum_{j=1}^{n-3k}\frac{1}{k+j} -\frac{1}{3k}\bigg] \right \}$$ By the given restriction on $n$ there can be at most three cycles of lentgth $k$ I think the authors considered the following types of permutations along with different categories of greatest cycle length . $(a)$ Permutations having one cycle of lenth $k$ . $(b)$ Permutations having one cycle of lenth $k$ and two cycles of length greater than $k$ $(c)$ Permutations having one cycle of length $k$ and one cycle of length greater than $k$ $(d)$ Permutations having two cycle of length $k$ and one cycle of length greater than $k$ $(e)$ Permutations having two cycle of length $k$ $(f)$ Permutations having three cycles of lenth $k$ The number of permutations in category $(a)$ are ${n \choose k }(k-1)!(n-k)!=\frac{n!}{k}$ In category $(b)$ , let the two cycles of length greater than $k$ be of length $k+j$ and $k+l$ We want to find the bounds on $j$ and $l$ . Clearly $j,l\ge 1$ . Now, $k+(k+1)+(k+j)\le n$ $\implies j\le n-3k-1$ And , $k+(k+j)+(k+l)\le n$ $\implies l\le n-3k-j$ Then the number of permutations having a cycle of length $k$ and cycles of length $(k+j)$ and $(k+l)$ is given by $$\frac 12 {n \choose k}{n-k  \choose k+j}{n-2k-j \choose k+l}(k-1)!(k+j-1)!(k+l-1)!(n-3k-j-l)!$$ $$=\frac 12\frac{n!}{k}\frac{1}{(k+j)(k+l)}$$ Not sure why is the  factor $\frac 12$ present. I think its because exactly after $j$ covers half the way between $1$ and $(n-3k-1)$ , the pairs of values of $k+j$ and $k+l$ interchange with respect to the first half. (Generally we are dividing by $k!$ when we are selecting $k$ groups of equals size from a given number of objects (say) $n$ ) In category $(c)$ , let the cycles  of length greater than $k$ be $(k+j)$ . Then $j\ge1$ and $k+(k+j)\le n$ $\implies j\le n-2k$ So , number of permutations having a cycle of length $k$ and a cycle of length greater than $(k+j)$ is given by $${n \choose k}{n-k \choose k+j}(k-1)!(k+j-1)!(n-2k-j)!$$ $$=\frac {n!}{k}\frac {1}{k+j}$$ Similarly the other terms in the expression, I understand that the overall aim  here is to eliminate from the set of all permutations having a cycle of length $k$ , other unnecessary permutations having two cycles of length $k$ (because of overcounting) or cycles of length greater than $k$ . But I don't understand the actual interplay between the plus and minuses signs between the  different terms in the expression and how does that help in leading to the general formula. I am self studying and apologise for my seemingly confused way of writing.Thanks in advance . $UPDATE :$ Here's one idea I have got after revisiting the theory today .. Let $P(\underbrace{k,k,\ldots,k}_{t\text{-times}},k+j,k+l,...k+p) $ denote the set of permutations having some $t$ cycles of length $k$ and other cycles of length greater than $k$ Then if $\frac n4<k \le \frac n3$ , we have $L_{k,n}=|P(k)|-|P(k,k+j)|+|P(k,k+j,k+l)|-\left \{ |P(k,k)|-|P(k,k,k+j)|-\big [   |P(k,k,k)| \big ]   \right \}     $ So the brackets are somehow going nested starting with $P(k),P(k,k)...$ in the expression along with the alternate signs as one goes within a chain of fixed number of cycles of length $k$ and increasing the number of cycles of length greater than $k$ along the chain . OK I  try writing out the $L_{k,n}$ for the case $\frac n5<k \le \frac n4$ $$L_{k,n}=|P(k)|-|P(k,k+j)|+|P(k,k+j,k+l)|-|P(k,k+j,k+l,k+m)|-\left \{ |P(k,k)|-|P(k,k,k+j)|+|P(k,k,k+j,k+l)|-\big [ |P(k,k,k)|-  |P(k,k,k,k+j)| -|P(k,k,k,k)|\big ]   \right \}     $$ Which turns out to be matching with the formula provided in the linked text. So I guess my understanding is correct. Now coming to the general formula , I think the upper bound for $j=\lfloor n/k   \rfloor$ says how many maximum $k$ length cycles can be formed from the $n$ objects . The expression $$\frac{1}{j!k^j}\frac{n!}{(n-kj)!}=\frac{\left \{(k-1) !\right \}^j}{j!}{n \choose k}{{n-k} \choose k}\dots {{n-kj+k} \choose k}$$ tells how many permutations on $n$ objects have $j$ cycles of length $k$ Now , for each of the above permutations (for a fixed $j$ ), the inner sum tells us how many permutations (on the remaining $(n-kj)$ objects )are there of greatest cycle length from $1$ upto $(k-1)$ or $(n-kj)$ whichever is smaller. So the general formula gives us  all permutations having a certain no of cycles each of length $k$ along with cycles of length shorter than $k$ thus giving all permutations having greatest cycle of length $k$ Suggestions ??","['permutation-cycles', 'abstract-algebra', 'combinatorics', 'discrete-mathematics', 'probability']"
4700577,"Find all triples $(p, q, r)$ of primes such that $p - 2q + r = 1$ and $p^3 - 2q^3 + (4r)^3 = 2023$.","Problem Find all triples $(p, q, r)$ of primes such that $p - 2q + r = 1$ and $p^3 - 2q^3 + (4r)^3 = 2023$ . This problem is from the algebra round of a local high school competition that has already ended. Almost Solved! (With help from dxiv and insipidintegrator) We can rearrange the first equation as $p + r = 2q + 1$ . Since $2q + 1$ is always odd, either $p$ or $r$ must be even. The only even prime number is $2$ . Case 1: $p = 2$ If $p = 2$ , then the equation $p + r = 2q + 1$ becomes $r = 2q - 1$ . Substituting this value of r into the second equation, we have: $8 - 2q^3 + 64(2q - 1)^3 = 2023$ . This equation does not work because the sum of two even numbers can not equal an odd number. Therefore, case 1 does not work. Case 2: $r = 2$ If $r = 2$ , then the equation $p + r = 2q + 1$ becomes $p = 2q - 1$ . Substituting this value of $p$ into the second equation, we have: $(2q - 1)^3 - 2q^3 + (4(2))^3 = 2023$ . Expanding and simplifying: $6q^3 - 12q^2 + 6q - 1 + 512 = 2023$ , $6q^3 - 12q^2 + 6q = 1512$ . Dividing by 6, we get: $q^3 - 2q^2 + q - 252 = 0$ . It would take so long if I try to use rational root theorem to solve this. Is there any better way to solve this cubic? Also, has all my steps been correct so far? Thank you!","['elementary-number-theory', 'algebra-precalculus']"
4700580,Show that $\sum_{k=1}^\infty 1/(k(k+1)(k+1)!)=3-e$,"This is my attempt. I start with calling the sequence $x_n$ : $$x_n = \sum_{k=1}^{n} \frac{1}{k(k+1)(k+1)!}$$ Using partial fraction decomposition: $$\frac{1}{k(k+1)(k+1)!} = \frac{A}{k} + \frac{B}{k+1} + \frac{C}{(k+1)!}$$ Multiply both sides by $$(k(k+1)(k+1)!$$ $$1 = A(k+1)(k+1)! + B(k)(k+1)! + C(k)(k+1)$$ Using $$k=0 \rightarrow A=1$$ , $$k=-1 \rightarrow B=-1$$ , and $$k=1 \rightarrow C=-\frac{1}{2}$$ Rewrite the sequence: $$\sum_{k=1}^{n} \left[\frac{1}{k} - \frac{1}{k+1} - \frac{1}{2(k+1)!}\right]$$ The first two terms form a telescoping series: $$1 - \left(-\frac{1}{2} + \frac{1}{2}\right) + \left(\frac{1}{3} - \frac{1}{3}\right) + \ldots + \left(\frac{1}{n} - \frac{1}{n+1}\right) = 1 - \frac{1}{n+1}$$ For the last term in the sequence, we can add and subtract $$\left(\frac{1}{0!} + \frac{1}{1!}\right)$$ . Now, the last step is to sum the two results: $$1 - \frac{1}{n+1} - \frac{1}{2} \sum_{k=-1}^{n} \frac{1}{(k+1)!} + \frac{1}{2}\left(\frac{1}{0!} + \frac{1}{1!}\right)$$ Taking the limit as (n) approaches infinity, we get: $$1 - 0 - \frac{e}{2} + 1 = 2 - \frac{e}{2}$$ ""The right answer is $3-e$ ."" Where is the mistake and I tried so much to use partial fraction decomposition method but it didn't work😔 Last thing I will drag the book's solution for this question: I want someone to help me know how did the author evaluate the right-hand side equation??","['limits', 'summation', 'sequences-and-series', 'real-analysis']"
4700644,Enumerating “Cyclic Double Permutations”,"This is a generalization of a question first asked by loopy walt on Puzzling Stack Exchange: https://puzzling.stackexchange.com/q/120243 . I asked the following version of the question in the comments, and I couldn't resolve it myself. I will re-phrase this question to make its combinatorics more apparent. Definition. The given is an alphabet with $n$ letters $\mathfrak{A}=\{\mathrm{A,B,C,\ldots,N}\}$ and $m$ numbers $\mathfrak{N}=\{1,2,3,\ldots,m\}$ . I call a finite sequence $\sigma$ of alternating letters and numbers a “cyclic double permutation” if The consecutive letter-number pairs in $\sigma$ is a permutation of all letter-number pairs in $\mathfrak{A}\times\mathfrak{N}$ ; If the initial letter of $\sigma$ is moved to the end of $\sigma$ to form a new string $\tau$ of alternating numbers and letters, then the consecutive number-letter pairs in $\tau$ is a permutation of all number-letter pairs in $\mathfrak{N}\times\mathfrak{A}$ . Examples. Take $(n,m)=(3,4)$ . The string $$\sigma=\textrm{C3A3C4B1B2B3B4C2C1A2A4A1}$$ is a cyclic double permutation because $\sigma$ is a permutation of $\mathfrak{A}\times\mathfrak{N}=\{\textrm{A1},\textrm{A2},\ldots,\textrm{C4}\}$ and its companion (as in item 2 above) $\tau=\textrm{3A3C}\ldots\textrm{4A1}\underline{\textrm{C}}$ is a permutation of $\mathfrak{N}\times\mathfrak{A}=\{\textrm{1A},\textrm{2A},\ldots,\textrm{4C}\}$ . In particular, every cyclic double permutation $\sigma$ should start with a letter, and it should contain precisely $mn$ letters and $mn$ numbers. When $n=1$ , there are precisely $m!$ many cyclic double permutations. Namely, those are all the permutations of the set $\{\textrm{A1},\textrm{A2},\ldots,\textrm{A}m\}$ . The linked PSE question contains many answers with proofs that when $n=2$ , there are precisely $2^{m-1}(m!)^2$ many cyclic double permutations that start with the letter $\textrm{A}$ . (See for example xnor's answer .) By a symmetry argument, there are precisely $2^m(m!)^2$ many cyclic double permutations in total. Question Is the number of cyclic double permutations always equal to $(n!)^m(m!)^n$ ? I have checked using a computer that this holds whenever $n+m\le 7$ , and to my best knowledge this sequence has not shown up on OEIS yet.","['permutations', 'combinatorics', 'discrete-mathematics', 'eulerian-path']"
4700661,Steps in the proof of the corollary of fundamental theorem of calculus in Wikipedia,"I am unsure on a specific step in the proof of the corollary of FTC from Wikipedia . Suppose $F$ is an antiderivative of $f$ on the interval $[a,b]$ . Wiki used the fact that $G$ define as $$G(x) = \int_a^x f(x) \;dx$$ is an antiderivative of $f$ to show that $$F(a)+c = G(a) \tag{1}$$ for some constant $c$ . However, in wiki's proof of Fundamental Theorem of Calculus, we only concluded that $G$ as defined above is an antiderivative of $f$ on the interval $(a,b)$ . Since $G$ might not be an antiderivative of $f$ on the point $a$ , how can we use the property of antiderivative differ from each other by a constant to conclude equation (1)? Is there any other proof of FTC that shows $G$ is an antiderivative of $f$ on point $a$ and $b$ ? Thank you so much in advance for answering this! It has bother me for a while now and I can't figure it out.","['integration', 'calculus', 'derivatives', 'real-analysis']"
4700767,"Bunny & Probability, How to solve this probability?","Two boxes, $X$ and $Y$ , each box contains two bunnies, let $x_w$ be the white bunny in box $X$ ,and let $x_b$ be the black bunny in box $X$ . Similarly, $y_w$ be the white bunny in box $Y$ and $y_b$ be the black bunny in box $Y$ . We define the operation as: Simultaneously taking one bunny from $X$ , and taking one bunny from $Y$ , then interchange them to the other box. For example, initially, we have $X=(x_w, x_b), Y=(y_w, y_b)$ , if we choose $x_w$ from $X$ , and $y_b$ from $Y$ , after this operation, we get $X=(y_b, x_b), Y=(y_w, x_w)$ . After we do this operation by $n$ times, what is the probability that each box still contains one white bunny and one black bunny? (Not necessarily to be the original bunnies in boxes.) My attempt: If we define $P_n$ as the probability after $n$ operations. For $n=1$ , we have totally four cases, here we only need to look at one box, say box $X$ , since $Y$ is automatically settled once $X$ is settled. After the first operation, box $X$ could be $(y_w, x_b), (y_b, x_b), (x_w, y_w), (x_w, y_b)$ , so we know $P_1=2/4$ After two operations, each of above four cases will generate another four subcases, totally $16$ subcases, and some of them are repeated, by counting, I get $P_2=12/16$ But how to derive a general formula for $P_n$ ? Update Thank you for your help, and I can understand this problem now. But I have another question. (Should I ask here or put in a new question?) @A.J. uses the three term homogeneous equation, but @Mathfail uses two term inhomogeneous equation, and both give the same result. Is there a more deep connection between the two methods? Can I say, algebraically, the three term homogeneous equation is equivalent to two term inhomogeneous equation?","['permutations', 'combinatorics', 'probability']"
4700771,Is Mordell conjecture true for local field?,"Suppose $K$ is a local field (finite extension of $\mathbb Q _p$ ), and $X/K$ is a genus $>1$ projective smooth curve. Is it true that $X(K)$ is finite? (I think it's false, but could you give an counterexample?)","['number-theory', 'algebraic-geometry', 'arithmetic-geometry']"
4700786,Solve the equation: $ 3(\sin^3 x + \cos^3 x) + 4(\sin^7 x + \cos^7 x) = 7(\sin^5 x + \cos^5 x) $,"Problem Solve the equation in $[0, \pi]$ $$
3(\sin^3 x + \cos^3 x) + 4(\sin^7 x + \cos^7 x) 
= 7(\sin^5 x + \cos^5 x)
$$ This problem is from the algebra round of a local high school competition that has ended. I have not been able to get very far for this question, I can only see that we might try to use sum of cubes to simplify the first term into $3(\sin x + \cos x)(1 - \cos x \cdot \sin x)$ . Could I receive some help for this question? Thank you.","['algebra-precalculus', 'trigonometry']"
4700798,Does sub-module of free module over PID is free require commutativity?,"I read the proof in Lang's Algebra (page 880) that submodules of free $R$ -modules for $R$ a PID are themselves free. I can't figure out where the proof requires commutativity of $R$ . Is it possible to remove this assumption? In division rings, there are no zero divisors, and  the only ideals are $0$ and $R$ , so in particular every ideal is principal. Over division rings every module is free, so the result that submodules of free modules are free holds in this case. Is it true in general that rings $R$ without zero divisors where every left ideal is principal have the property that submodules of free left $R$ -modules are free? This part of the question is less important, but if commutativity is required, then where in Lang's proof does he assume it?","['principal-ideal-domains', 'free-modules', 'abstract-algebra']"
4700805,Solve the equation: $\frac{x^2 - 2x + 3}{\lbrace x\rbrace} + \frac{2023}{2}{\lbrace x\rbrace} = 2\lfloor x \rfloor + 88$,"Problem Solve the equation $\frac{x^2 - 2x + 3}{\lbrace x\rbrace} + \frac{2023}{2}{\lbrace x\rbrace} = 2\lfloor x \rfloor + 88$ , where $\lfloor x \rfloor$ and $\{x\}$ are the greatest integer less than or equal to $x$ and the fractional part of $x$ , respectively. This problem is from the Team Round of a local high school math competition that has ended recently My work so far $x$ can not be an integer because $\lbrace x\rbrace = 0$ will be dividing by zero.
Therefore, $x$ is not an integer. We can express $x$ as $\lfloor x \rfloor + \lbrace x\rbrace$ . Lets say $\lfloor x \rfloor = a$ , and $\lbrace x\rbrace = b$ . So $x = a + b.$ We have: $((a+b)^2 - 2(a+b) + 3) + \frac{2023}{2} b^2 = 2ab + 88b$ which turns into: $2a^2 + 2025b^2 - 4a -180b + 6 = 0$ (Hopefully I did my math correctly) I am not too sure what to do from here on. Am I going in the right direction? Am I supposed to solve this equation? Thank you for your help.","['fractional-part', 'algebra-precalculus', 'quadratics', 'ceiling-and-floor-functions']"
4700808,"A regular $n$-gon contains a regular $(n+1)$-gon, with no sides coinciding. What is the maximum number of points of contact between them?","A regular $n$ -gon contains a regular $(n+1)$ -gon. That is, they are in the same plane, and no part of the regular $(n+1)$ -gon is outside of the regular $n$ -gon. None of their sides coincide. There are no other restrictions. Among the integers $n\ge3$ , what is the absolute maximum number of points of contact between the inside polygon and the outside polygon? Context I made up this question. I think it's a natural question (that I haven't found asked anywhere) with a non-obvious answer. My attempt My guess is that the answer is four, and that this maximum can be attained for every $n\ge4$ , and that when this maximum is attained the polygons share a line of symmetry. I think any explanation must take into account the fact that $n$ and $n+1$ are coprime, but I don't have a clear idea about this. I made a desmos graph of a regular $6$ -gon containing a regular $7$ -gon. You can use the sliders to rotate and expand the hexagon, or translate the heptagon up and down. EDIT For example, here is a regular $4$ -gon containing a regular $5$ -gon, with four contact points. The equations of the lines can be found in this desmos graph .","['optimization', 'coprime', 'geometry', 'polygons']"
4700812,Seeking help to find $\int \frac{\ln ^2(1+\sin x)}{1+\sin x} d x.$,"Good News Thanks to @Quanto who used dilogarithm function and gave an elegant answer to the integral $$\boxed{\int \ln(1+\sin x)dx= -x\ln2-2 \Im \text{Li}_2(i e^{i x}) }$$ Hence \begin{align}& \int \frac{\ln ^2(1+\sin x)}{1+\sin x} d x\\
= &2x\ln2+4 \Im \text{Li}_2(i e^{i x})
)-4x+(\tan x-\sec x)\left[(\ln(1+\sin x)+2)^2 +4\right]+C
\end{align} When I met the integral $$\int \frac{\ln ^2(1+\sin x)}{1+\sin x} d x,$$ I first doubted whether it has elementary primitive. Then I tried the simpler one $$\int \frac{\ln (1+\sin x)}{1+\sin x} d x.$$ I first integrate the denominator for integration by parts $$\displaystyle \begin{aligned}\int \frac{1}{1+\sin x} d x = & \int \frac{1-\sin x}{\cos ^2 x} d x \\= & \int\left(\sec ^2 x-\tan x \sec x\right) d x \\= & \tan x-\sec x+c\end{aligned}\tag*{} $$ Now we can proceed the integration by parts. $$\displaystyle \begin{aligned}I & =\int \ln (1+\sin x) d(\tan x-\sec x) \\& =(\tan x-\sec x) \ln (1+\sin x)-\int(\tan x-\sec x) \frac{\cos x}{1+\sin x} \\& =\frac{\sin x-1}{\cos x} \ln (1+\sin x)-\int \frac{\sin x-1}{\cos x} \cdot \frac{\cos x}{1+\sin x} d x \\& =\frac{\sin x-1}{\cos x} \ln (1+\sin x)-\int\left(1-\frac{2}{1+\sin x}\right) d x \\& =\frac{\sin x-1}{\cos x} \ln (1+\sin x)-x+2(\tan x-\sec x)+C \\& = (\tan x-\sec x)\left[\ln (1+\sin x)+2\right]-x+C \end{aligned}\tag*{} $$ Then I tried the harder one similarly. Latest update $$
\begin{aligned}&\int \frac{\ln ^2(1+\sin x)}{1+\sin x} d x 
\\= & \int \ln ^2(1+\sin x) d(\tan x-\sec x) \\
= & (\tan x-\sec x) \ln ^2(1+\sin x)+2\int \frac{ \ln (1+\sin x)}{1+\sin x}(1-\sin x) d x\\=& 
(\tan x-\sec x) \ln ^2(1+\sin x)+4 \int \frac{\ln (1+\sin x)}{1+\sin x} d x  -2 \int \ln (1+\sin x) d x \\=& 
(\tan x-\sec x)\left[\left(\ln(1+\sin x)+2)^2+4\right]\right.-4x -2 \int \ln (1+\sin x) d x
\end{aligned}
$$ Then I was stuck by the last integral, the hardest one. Having no idea, I tried to use the expansion of $\ln(1+x)$ for $|x|<1$ , $$
\int \ln (1+\sin x) d x=\sum_{k=1}^{\infty} \frac{(-1)^{k-1}}{k} \int \sin ^k x d x
$$ Using $\sin x= \frac{e^{x i}-e^{-x i}}{2 i}$ , we get $$\int \sin ^{k} x d x=\frac{1}{2 ^k i ^k} \sum_{r=0}^k\left(\begin{array}{l}
k \\
r
\end{array}\right)(-1)^r\frac{e^{(k-2 r) x i}}{(k-2 r) i}$$ So $$\int \ln (1+\sin x) d x = -\sum_{k=1}^{\infty} \frac{i^{k-1}}{k2^k} \left[ \sum_{r=0}^k\left(\begin{array}{l}
k \\
r
\end{array}\right)(-1)^r\frac{e^{(k-2 r) x i}}{k-2 r}\right] +C$$ Now we can conclude that $$\boxed{\int \frac{\ln ^2(1+\sin x)}{1+\sin x} d x \\ =  
(\tan x-\sec x)\left[\left(\ln(1+\sin x)+2)^2+4\right]\right.-4x
\\+2\left(\sum_{k=1}^{\infty} \frac{i^{k-1}}{k2^k} \left[ \sum_{r=0}^k\left(\begin{array}{l}
k \\
r
\end{array}\right)(-1)^r\frac{e^{(k-2 r) x i}}{k-2 r}\right]\right)}$$ which is ugly and not a closed form. Can you help me to find the hardest one $\int \ln (1+\sin x) d x $ ? Your suggestion and help are highly appreciated.","['integration', 'indefinite-integrals', 'calculus', 'trigonometry']"
4700821,Fundamental questions about differential forms:,"I have read through multiple books and watched Dr. Theodore Shifrin's video series . However, the questions that I had when going through much  of the material have remained largely unanswered. They are: (1) Notational meaning: From my understanding, a differential form is a covector that takes as input a vector and returns some number. For instance take the 1-form $\omega = dx + 4dy$ . Is the ""mapping"" to some vector $v$ implicit in the definition? For instance, would we more rigorously be able to write out $\omega$ as $ \omega(v) = dx(v) + 4\cdot dy(v)$ ? Let $v$ = ( $\mathbf{i} + 2 \cdot \mathbf{j}$ ). Can we intuitively put the mapping as: $$  \omega(\mathbf{i} + 2 \cdot \mathbf{j}) = dx(\mathbf{i} + 2 \cdot \mathbf{j}) + 4\cdot dy(\mathbf{i} + 2 \cdot \mathbf{j}) = 1 + 8 \cdot 1 = 9$$ (2) Differential forms in fractions: In calculus we often run into certain terms of  form $\left({dy \over dx}\right)$ . Given the same vector $v$ , can this be written rigorously as: $$ {dy(v) \over dx(v)} = {dy(v) \over dx(v)} = {dy(\mathbf{i} + 2 \cdot \mathbf{j}) \over dx(\mathbf{i} + 2 \cdot \mathbf{j})} = {2 \over 1} = 2$$ (3) Ordinary and partial derivatives: For one dimension, we commonly write $d(A) = {\left( \partial A \over \partial x \right)} dx$ , where $ {\left( \partial A \over \partial x \right)}$ denotes the partial derivative of $A$ with respect to variable $x$ . Let $A$ = $x^2$ . In beginner calculus, we see that ${d A \over dx} = 2x$ . If we take my assumptions from (1) and (2) to be gospel and treat $dx(v)$ as a scalar then we can put: $$ {d A \over dx} = {(dA)(v) \over dx(v)} = {{\left( \partial A \over \partial x \right)} \cdot dx(v)\over dx(v)} = {\left( \partial A \over \partial x \right)} = {\left( \partial (x^2) \over \partial x \right)} = 2x$$ I am confused however. Some answers on pages like this one and this one seem to suggest that $dx$ and $\partial x$ are cosmetic and interchangeable. However, the $dx$ here seems to behave very differently from $\partial x$ , and it almost seems like $\partial x$ acts as some kind of fundamental underpinning operation. Are they completely divorced? Is there some underpinning structure I could know of? It is hard for me to understand this, especially since I struggle to unwrap my head from the way these things are manipulated in beginner calculus. I greatly appreciate any help with this.","['ordinary-differential-equations', 'vector-analysis', 'partial-differential-equations', 'differential-forms', 'differential-geometry']"
4700831,Irreducible components of certain (potentially non-Noetherian) spectrum,"Let $\Bbbk$ be an algebraically closed field (not necessarily characteristic zero), and let $R$ be a finitely generated $\Bbbk$ -algebra.  Let $S\subseteq R$ be a $\Bbbk$ -subalgebra, and let $X=\operatorname{Spec}S$ . Of course $S$ need not be finitely generated---but my question is, does $X$ still have irreducible components?  Can we write $X=X_1\cup \dots \cup X_r$ such that each $X_i$ is irreducible? Even simpler question: is it known whether $X$ must be a Noetherian topological space?  I'm guessing the answer is no, since I can't find such a statement.","['algebraic-geometry', 'commutative-algebra']"
4700843,How are $F^n$ and $F^\infty$ special cases of $F^S$? [duplicate],"This question already has an answer here : Intuition behind $F^n$ and $F^\infty$ being examples of function spaces, $F^S$. (1 answer) Closed last year . I'm teaching myself Linear Algebra using Axler's Linear Algebra Done Right book and I'm confused about some definitions mentioned in the book. The book defines $F$ as $\mathbb{R}$ or $\mathbb{C}$ where $F$ stands for a field. $F^n$ is the set of all lists of length $n$ of elements of $F$ : $$F^n = \{(x_1, x_2, ..., x_n): x_j \in F \text{ for } j = 1, 2, ..., n\}.$$ $F^\infty$ as the set of all sequences of elements of $F$ : $$F^\infty= \{(x_1, x_2, ...): x_j \in F \text{ for } j \in 1, 2, ...\}.$$ The book then introduces $F^S$ as below: If $S$ is a set, then $F^S$ denotes the set of functions from $S$ to $F$ . For $f, g \in F^S$ , the sum $f + g 
\in F^S$ is the function defined by $(f + g)(x) = f(x) + g(x)$ for all $x \in S$ . For $\lambda \in F$ and $f \in F^S$ , the product $\lambda f \in F^S$ is the function
defined by $(\lambda f)(x) = \lambda f(x)$ for all $x \in S$ . The book says that $F^n$ and $F^\infty$ are special cases of the vector space $F^S$ but I don't really get why that's the case. How can we express lists as functions from $S \to F$ ? Won't they then be functions from $S$ to $F^n$ (where $n$ is the length of the list)?","['vectors', 'functions', 'linear-algebra', 'vector-spaces']"
4700862,Confusion in the definition of left-hand and right-hand limit,"In Understanding Analysis, the right-hand limit is defined as Definition 4.6.2. Given a limit point $c$ of a set $A\subseteq \mathbb R$ and a function $F :A \to \mathbb R$ , we write $$\lim_{x\to c^+} f(x) = L$$ if for all $\epsilon > 0$ there exists a $\delta > 0$ such that $|f(x) - L| < \epsilon$ whenever $0 < x-c < \delta$ . I think the requirement that $c$ is a limit point of $A$ is not strong enough for the right-hand limit to be well defined, because if $(c,\infty) \cap A$ is empty and $c$ is a limit point of $A$ (which is possible consider $A = [0,1]$ and $c = 1$ ), then any real number can be the right-hand limit for $f$ at $c$ . Can anyone tell me whether I am correct at this? Maybe we can add definitions for ""left limit points"" and ""right limit point"" to solve this issue.","['limits', 'analysis', 'real-analysis']"
4700873,Show $\sum_p\frac{1}{p^{1+i}}$ converges,"I am struggling with exercise I.15 in Tenenbaum's Analytic Number Theory book. The problem says to show that (here $\color{blue}{i}$ is the imaginary unit) $$\sum_{p}\frac{1}{p^{1+i}}$$ converges using the strong form of the prime number theorem: $$\pi(x)=\frac{x}{\log(x)}+O\left(\frac{x}{(\log(x))^2}\right).$$ I had no problem showing $\sum_p\frac{1}{p^{1+\epsilon}}$ converges for $\epsilon>0$ using that $p_n\sim n\log(n)$ from the prime number theorem, but this is less obvious.","['analytic-number-theory', 'number-theory', 'asymptotics', 'prime-numbers']"
4700877,How is the pythagorean scale using fifths constructed?,"I was trying to construct the pythagorean scale using fifths. You know by multiplying the tonic with 3:2 ratios and then that with 4:3 ratios and so on. Now in this way the ratios keep on getting bigger. But isn't the pythagorean scale all about simple rational numbers? At least for the fourths and fifths. Well if I proceed this way the fourth isn't turning out to be 4:3 as it should be. But rather something quite unfathomably huge and obnoxious. What's happening here. Can somebody please help me out with a step by step construction of the scale? Edit: alright so basically suppose I start from a C and I go up a fifth to a G. So the G is in a 3:2 ratio from the C. Now if I go up another fifth from the G I'll go upto a D. Now the ratio of this D to G is 3/2 * 3/2 = 9/4. Since this 9:4 > 2:1, the D exceeds the higher C and is beyond the current octave at hand. So if we step it down an octave by multiplying it by 1/2, it comes to a 9/8 which is our major second. Now multiplying this 9/8 with 3/2 gives me 27/16 which is the major sixth. Likewise if I continue by the time I reach a fourth it's quite a big ratio. But pythagoras also said, a perfect fourth is supposed to be a 4:3 with the tonic. So what's going wrong here?","['music-theory', 'discrete-mathematics']"
