question_id,title,body,tags
4806926,Is an entire function with this property necessarily a polynomial?,"Given an entire function $f(z)$ , define $\displaystyle m(r):=\inf_{\vert z\vert=r}\vert f(z)\vert$ . If $f$ satisfies $$\limsup_{r\to+\infty} m(r)=+\infty,$$ can we assert that $f$ is a polynomial? This question comes to my mind after going through the discussion about the entire functions on Ahlfors. I tried to prove this by showing $\infty$ is a pole of $f$ but got stuck since $f$ may have infinitely many zeros.
Now I tend to believe that the statement is not true, but neither can I find a counterexample.","['complex-analysis', 'entire-functions']"
4806939,Is a Noetherian sheaf of rings stalkwise Noetherian?,"Let $X$ be a scheme such that $O_X$ is a coherent $O_X$ -module. Assume  that for every open subset $U\subset X$ and every family of coherent ideal sheaves $\{I_i\}_i$ of $O_U$ , $\sum_iI_i$ is a coherent $O_U$ -module. Do we have that $O_{X,x}$ is a Noetherian ring for every $x\in X$ ? The question is inspired by Def. A.7 in $D$ -modules and Microlocal Calculus by Kashiwara.","['coherent-sheaves', 'algebraic-geometry', 'schemes', 'coherent-rings']"
4806952,Decomposition of the symmetric group $\mathfrak{S}_{p+q}$,"Let $p,q \ge 1$ be two integers and $\mathfrak{S}_{p+q}$ the symmetric group of $\{1, \dots, p, p+1, \dots, p+q\}$ . Denote: by $\Gamma_{p,q}$ the subgroup of permutations $\alpha \in \mathfrak{S}_{p+q}$ such that $\alpha(i)=i$ for all $i \in \{1, \dots, p\}$ by $\Delta_{p,q}$ the subgroup of permutations $\beta \in \mathfrak{S}_{p+q}$ such that $\beta(j)=j$ for all $j \in \{p+1, \dots, p+q\}$ and by $\mathcal{S}_{p,q}$ the set of permutations $\sigma \in \mathfrak{S}_{p+q}$ such that $\sigma(1) \lt \sigma(2)  \lt \dots \lt \sigma(p)$ and $\sigma(p+1) \lt \sigma(p+2)  \lt \dots \lt \sigma(p+q)$ I'm trying to prove that any $s \in \mathfrak{S}_{p+q}$ can be written in a unique way as $$s = \sigma \circ \alpha \circ \beta$$ where $\sigma \in \mathcal{S}_{p,q}$ , $\alpha \in \Gamma_{p,q}$ and $\beta \in \Delta_{p,q}$ . I first tried to use a mockup with $p=2$ , $q = 3$ and the permutation: $$s=\begin{pmatrix}
 1 & 2 & 3 & 4 & 5\\
 4 & 2 & 5 & 1 & 3\end{pmatrix}$$ but I'm not able to get substantial things. Thanks for your ideas! Note: the topic is coming from the definition of the wedge product in exterior algebra.","['permutations', 'finite-groups', 'combinatorics', 'symmetric-groups', 'exterior-algebra']"
4806955,"Show that there can't be a graph with degrees $4,4,4,2,1$ and $1$","Just introduced to graph theory, I am asked whether or not there can be constructed a simple graph with six vertices, of degrees $4,4,4,2,1$ and $1$ . According to my textbook and my drawing skills, there can not be such a graph, but I have trouble explaining why that is. The sum of the degrees gives $16$ , so the number of the edges must be $8$ , and that alone doesn't prevent the graph from being possible (consider a parallelogram with its diagonals drawn - the two extra vertices are at the midpoints of two parallels). Also, so far we only have concluded that the number of vertices with an odd degree must be even , and in this case I can't make use of it. I also thought that maybe I should manipulate the problem to use the pigeonhole principle, but I can't find a way to manipulate the problem to do so. I did it in two certain combinations, but obviously I can't list all possible (attempted) graphs to show that the principle applies. Any help would be greatly appreciated! Thanks in advance!","['graph-theory', 'discrete-mathematics']"
4806980,"$\Bbb R/\Bbb Z$ is an angry electron, right?","I keep seeing $\Bbb R/\Bbb Z\cong S^1$ . Shouldn't this instead be the quotient of $\Bbb Z$ -many copies of $S^1$ by some fixed point? Isn't the correct quotient $S^1\cong\Bbb R/\{\{x,x+1\}:x\in\Bbb R\}\cong [0,1]/\{0,1\}$ ? Is it an abuse of notation? Am I going mad? My thinking is this: Fix $0\in S^1$ (we'll think of this as the ""origin,"" but it doesn't actually matter what point we choose). Then, $$\Bbb R/\Bbb Z\cong\left(\coprod_{n\in\Bbb Z}S^1\right)/\{(n,0):n\in\Bbb Z\}.$$ ""proof"": Let $n\in\Bbb Z$ . Then $[n,n+1]$ , considered as a subspace of $\Bbb R/\Bbb Z$ is a circle - it is locally homeomorphic to $\Bbb R$ , and $[n]=[n+1]$ . Choose $x\in\Bbb R,x\notin\Bbb Z$ . Clearly $[x]\ne[x+1]$ , so the circle $[m,m+1]$ is distinguishable from $[n,n+1]$ at all points except $[0]$ . Tag each interval $[n,n+1]$ with its left (or right) endpoint, and we have $$\Bbb R/\Bbb Z\cong\{(n,x):n\in\Bbb Z,x\in[0,1]/\{0,1\}\}\cong\left(\coprod_{n\in\Bbb Z}S^1\right)/\{(n,0):n\in\Bbb Z\}$$ It's an angry atomic orbital! A bunch of balloons! A big ol' flower! It isn't a circle. Clarification on notation: I didn't realize that it isn't a universal, but I've always thought of $$\coprod_{i\in I}X_i=\{(i,x):i\in I,x\in X_i\}$$ and $$X/Y=X/(Y\times Y\cup\operatorname{id}_X),$$ where $Y\times Y$ is taken to be the equivalence relation equating all points of $Y$ , as standard. Also, I just realized $\operatorname{id_X}=(=\cap (X\times X))$ , and that's wonderful.","['general-topology', 'quotient-spaces']"
4807040,Find a sum that consists of roots of polynomial and it's derivative polynomial,"I'm trying to solve this task: $f(x)$ is a polynomial of degree $n$ , it has different roots $\alpha_1, \alpha_2, ..., \alpha_n$ . Let $\mu_1, \mu_2, ..., \mu_{n-1}$ be the roots of the derivative polynomial $f'(x)$ . Find $\sum_{i=1}^{n} \sum_{j=1}^{n-1} \frac{1}{\alpha_i - \mu_j} (*) $ . I was trying to use this equation: $\frac{f'(x)}{f(x)} = \sum_{i=1}^{n} \frac{1}{x - \alpha_i}$ (where $\alpha_i$ are roots of $f(x)$ ) It looks like I can represented the sum $(*)$ in this way: $\sum_{i=1}^{n} \frac{f''(\alpha_i)}{f'(\alpha_i)}$ However, I am not sure if it correct to use second derivative here or if some $\alpha_i$ wouldn't be equal to some $\mu_j$ . Could somebody please give a hint? Thanks in advance.","['algebra-precalculus', 'polynomials']"
4807083,"Find variance of random variable $W\cos\theta+L\sin\theta$ where $L,W,\theta$ are normal random variables. Uncertainty propagation problem.","I have three variables - these are length ( $L$ ), width ( $W$ ) and angle ( $\theta$ ).
Each has a known mean and variance. $\theta$ is independent, but $L$ and $W$ are correlated, and I know their covariance.
I want to estimate the uncertainty in apparent size (see image) given by the equation $f=W \cos(\theta) + L \sin(\theta)$ . If I consider $W\cos(\theta)$ in isolation, then maybe the variance of this part becomes: $$\sigma^2_{\text{term 1}} = W^2 \cos^2(\theta) \left[ \left( \frac {\sigma_L} {L} \right)^2 + \left( \frac {\sin(\theta) \sigma_{\theta}} {\cos(\theta)} \right)^2 \right],$$ because $\sigma_{L\theta}$ is zero, since they are uncorrelated. If this is right, then the variance of the $L\sin(\theta)$ term in isolation would be: $$\sigma^2_{\text{term 2}} = L^2 \sin^2(\theta) \left[ \left( \frac{\sigma_W}{W} \right)^2 + \left( \frac {\cos(\theta)\sigma_{\theta}} {\sin(\theta)} \right)^2 \right],$$ and maybe I can combine them like this: $$\sigma^2 = \sigma^2_{\text{term 1}} + \sigma^2_{\text{term 2}} + 2\sigma_{\text{term 1}}\sigma_{\text{term 2}},$$ but I only know the covariance $\sigma_{LW}$ so it feels like I am approaching this in the wrong way. Any help would be greatly appreciated.  Unfortunately I am an engineer and not a mathematician! EDIT: $L,W,\theta$ are normally distributed random variables.","['statistics', 'probability-theory', 'probability-distributions', 'trigonometry', 'error-propagation']"
4807084,"Is this set, defined in terms of conjugation, closed under inverses?","Let $ G $ be a group. Let $ H $ be a finite subgroup. Define $ N_G^r(H) $ inductively by $$
N_G^{r+1}(H)=\{ g \in G: g H g^{-1} \subset N_G^r(H) \} 
$$ starting from $ N_G^1(H):= H $ . Note that $ N^2_G(H) = N_G(H) $ is just the normalizer in $ G $ of $ H $ . Although $ N_G^1(H)= H $ and $ N_G^2(H)= N_G(H) $ are both groups, the set $ N_G^r(H) $ is generally not a group for $ r \geq 3 $ . My question: Is $ N_G^r(H) $ closed under inverses? Some thoughts: If $ H $ is normal in $ G $ then $ N_G^r(H)=G $ for all $ r \geq 2 $ and the whole construction is trivial. So the question is only interesting if $ H $ is not a normal subgroup. It is certainly true for $ r=1,2 $ since groups are always closed under inverses. Context: This construction is used for a naturally occurring object called the Clifford hierarchy which appears in quantum computing. For example, the (single qubit) Clifford hierarchy is the case where $ G=\mathrm{U}(2) $ and $ H= \left\langle \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \right\rangle $ is the single qubit Pauli group. Update: As pointed out by Arturo Magidin in The set of all $x$ such that $xHx^{-1}\subseteq H$ is a subgroup, when $H\leq G$ if we allow $ H $ to be infinite then even $ N^2_G(H) $ is not necessarily closed under inverses. See the example given for $ G=GL(2,\mathbb{Q}) $ and $ H = \left\{\left.\left(\begin{array}{cc}
1 & m\\
0 & 1\end{array}\right)\in G\ \right|\ m\in\mathbb{Z}\right\} $ where $ x = \left(\begin{array}{cc}
2 & 0\\
0 & 1\end{array}\right) $ is in $ N^2_G(H) $ but $ x^{-1} $ is not.","['representation-theory', 'group-theory', 'finite-groups']"
4807087,"$\int_{0}^\infty\frac{\log^3(x)}{x^2+1}\,dx$","I want to solve $$
\int_{0}^\infty\frac{\log^3(x)}{x^2+1}\,dx=0.
$$ The problem is that when I use the curve I have that \begin{align*}
0
&=\int_{\Gamma}\frac{\log^3 z}{z^2+1}\\
&=\int_{\gamma_r}\frac{\log^3 z}{z^2+1}
+\int_{\gamma_R}\frac{\log^3 z}{z^2+1}
+\int_{r}^R\frac{\log^3x}{x^2+1}
+\int_{-R}^{-r}\frac{(\log|x|+i\pi)^3}{x^2+1}
\end{align*} where $\log z$ is the log without negative pure complex numbers. Also ( $\rho=R$ or $\rho=r$ ) \begin{align*}
\int_{\gamma_\rho}\frac{\log^3z}{z^2+1}
&=\int_0^\pi\frac{(\log\rho+i\theta)^3}{\rho^2e^{2i\theta}+1}i\rho e^{i\theta}\,d\theta
=\int_0^\pi\frac{\log^3\rho-i\theta^3+3i\theta\log^2\rho-3i\theta^2\log\rho}{\rho^2e^{2i\theta}+1}i\rho e^{i\theta}\,d\theta\\
&=\int_0^\pi\frac{\log^3\rho}{\rho^2e^{2i\theta}+1}i\rho e^{i\theta}\,d\theta
-i\int_0^\pi\frac{\theta^3}{\rho^2e^{2i\theta}+1}i\rho e^{i\theta}\,d\theta
+3i\int_0^\pi\frac{\theta\log^2\rho}{\rho^2e^{2i\theta}+1}i\rho e^{i\theta}\,d\theta
-3i\int_0^\pi\frac{\theta^2\log\rho}{\rho^2e^{2i\theta}+1}i\rho e^{i\theta}\,d\theta
\end{align*} And \begin{align*}
\int_{-R}^{-r}\frac{(\log|x|+i\pi)^3}{x^2+1}
=\int_{r}^{R}\frac{(\log|x|+i\pi)^3}{x^2+1}
=\int_{r}^{R}\frac{\log^3x-i\pi^3+3i\pi\log^2x-3i\pi^2\log x}{x^2+1}
\end{align*} But I'm uncertain if this is the way, cause is huge and I don't know how to lead with integration of $\frac{\log^2x}{x^2+1}$ .",['complex-analysis']
4807109,Subspaces of tensor products of Hilbert spaces,"Let $H$ be a Hilbert space, and let $A,B,C,D\subset H$ be closed subspaces that intersect trivially. Consider the subspaces $A\otimes B$ , $C\otimes D$ , viewed as closed subspaces of the Hilbert space $H\otimes H$ . (Note that these are all Hilbert space tensor products, i.e. the closure of the algebraic tensor product with respect to the usual inner product on $H\otimes H$ ). Is it true that $A\otimes B$ and $C\otimes D$ once again intersect trivially? This seems to be true in the case where all the spaces are finite dimensional, since then the Hilbert tensor product agrees with the algebraic one. However I have had trouble showing that the trivial intersection is maintained upon taking the closure of the algebraic tensor products in infinite dimensions.","['functional-analysis', 'real-analysis']"
4807113,How to distribute $n$ points in a sphere to maximize the angle between them?,"Assume there are $n$ points $\{x_i\}_{i=1}^n$ on the sphere of a $d$ -dimensional unit ball. Let $\theta_{ij}$ denote the angle between $x_i$ and $x_j$ . How to choose those $n$ points to maximize $\min_{i,j} \theta_{ij}$ ? When $d = 2$ , I think we can simply consider the
regular polygon with $n$ vertices and $\max \min_{i,j}\theta_{ij} = \frac{2\pi}{n}$ . But for $d \geq 3$ , it is hard to imagine.","['euclidean-geometry', 'geometry']"
4807185,Existence of center manifold,"I've been working on the following exercise: Prove that the system $$
\begin{cases}
\dot{x} = -x^3,\\
\dot{y} =  -y + x^2
\end{cases}
$$ has no analytic center manifold (supposed in the following way $y = h(x) = a_2x^2 + a_3x^3 + \cdots$ , then $a_{2n+1} = 0, n \geq 1, a_2 = 1, a_{n+2} = na_n, n \geq 2$ ). Is the manifold $C^{\infty}$ ? It's based on the example 2.5, page 315 of the book 'Methods in Bifurcation Theory' by Chow and Hale. How should I find the center manifold or even prove there is none? The book says it is not difficult to see but I am struggling. Thank you for your help!","['bifurcation', 'ordinary-differential-equations', 'dynamical-systems']"
4807216,How to give a closed form to $e^{a(x) \frac{d}{dx} + b(x)I}[f]$ in physicists style abuse of notation?,"In Quantum Mechanics we have the famous time evolution result (here $a$ is a constant) $$ e^{a \frac{d}{dx}}[f] = f(x+a) $$ Which is an abuse of notation but makes sense due to Taylor's Theorem. In this answer I show we can give a closed form to $e^{a(x) \frac{d}{dx}}$ whereas if we can find a constant $r$ and function $q$ so that: $$a(x) = \frac{q(r)}{q'(x)} $$ Then $$ e^{a(x) \frac{d}{dx}} [f] = f(q^{-1}(q(r)+q(x)))$$ . As an example if $q = \ln(x)$ and $r=2$ then: $e^{\ln(2) x \frac{d}{dx}} = f(2x)$ (letting $q(x)=x, r=a$ we also can prove the quantum mechanics result above as special case of this) Now we also know that $e^{b(x)I}[f] = e^{b(x)}f$ where $I$ is the identity operator. So with this in place I am curious if we can generally speaking give some kind of closed form for a generic exponential of a first order linear differential operator: $$ e^{a(x) \frac{d}{dx} + b(x)I}[f] $$ My question is ""what should this evaluate to?"" and to put some boundaries on it, can we expect a general formula which involves finitely many functions $w_1(x) ... w_k(x), c_1(x), ... c_k(x)$ and finitely many non-negative real numbers $d_1 ... d_k$ such that $$ e^{a(x) \frac{d}{dx} + b(x)I}[f] = w_1(x)f^{(d_1)}(c_1(x)) + w_2(x)f^{(d_2)}(c_2(x)) + ... w_k(x)f^{(d_k)}(c_k(x)) $$ Where $f^{(d_k)}$ indicates the $d_k$ fractional derivative of $f$ ? Some Ideas: My first intuition when working with this abuse of notation is to factor it via integration factors (and at this point this is more symbol shuffling than math, I can hardly give a definition of what any of this means): $$ e^{a \frac{d}{dx} + bI} = e^{a(x) e^{-\int \frac{a}{b}}  \left( e^{\int \frac{a}{b}} I \right)' }  $$ But this doesn't necessarily help since I don't have any tools at the moment for evaluating $ e^{\frac{d}{dx} \left(g(x) I \right)} $ . Even something as simple as $e^{\frac{d}{dx}(2I)}$ we know to be $f(x+2)$ . But how does one arrive at $f(x+2)$ from $2f(x)$ which is the interpretation of $2I$ or $e^2f$ which is $e^{2I}$ . It's just absolutely not clear to me how to proceed here. Another part of the trouble is that $a \frac{d}{dx}$ and $bI$ don't necessarily commute and because they do not commute I don't feel comfortable making the either of the jumps $e^{a \frac{d}{dx} + bI} = e^{a \frac{d}{dx}} \circ e^{bI}$ or $e^{bI} \circ e^{a \frac{d}{dx}}$ . (Actually we know for fact both those jumps are wrong from our earlier example) One simplification. If $c$ is a number and we did hypothetically know what $e^{\frac{d}{dx}(a(x)I)}$ was then $e^{c \frac{d}{dx}(a(x)I}$ would obviously just be the $c$ iterate of this linear operator. This comes in handy as $e^{\frac{d}{dx}(2I)} = e^{2 \frac{d}{dx}}$ then must be applying $e^{\frac{d}{dx}}$ twice. Of course $f \rightarrow f(x+1)$ twice is $f(x+2)$ .","['operator-algebras', 'operator-theory', 'functional-analysis', 'quantum-field-theory', 'partial-differential-equations']"
4807248,Why does the existence of a dispersion point imply total path disconnectedness?,"In a comment of Fractal of the topologist's sine curve is connected and totally path-disconnected? M W asserts that the existence of a dispersion point , a point for which the removal of results in a totally disconnected space, implies that the space is totally path disconnected (even before the removal). Why must this be the case?","['general-topology', 'path-connected', 'connectedness']"
4807267,Area Calculation of Region A (Using Double Integral),"I'm calculating the area of the region $A$ defined by the following constraints: $$ A = \{(x, y) \mid x^2 + y^2 \geq 2, \; x^2 + y^2 \leq 2x, \; y \geq 0\} $$ To calculate the area of the region $A$ , we can set up the double integral in Cartesian coordinates as: $$ A = \iint_A \ dx \ dy $$ The constraints $x^2 + y^2 \geq 2$ and $x^2 + y^2 \leq 2x$ define region $A$ in the $xy$ -plane. Additionally, the constraint $y \geq 0$ ensures that the region is above the $x$ -axis. Transforming to cylindrical coordinates, where $x = r \cos\theta$ and $y = r \sin\theta$ , the constraints are expressed as $r^2 \geq 2$ and $r^2 \leq 2r \cos\theta$ . The angle $\theta$ varies between $0$ and $\pi$ to address the upper half of the $xy$ -plane. The integral to calculate the area is: $$ A = \int_{0}^{\pi} \int_{\sqrt{2}}^{2\cos\theta} \ r \ dr \ d\theta $$ I appreciate any additional guidance or methods that can facilitate solving this integral and calculating the area of region $A$ . Thank you in advance for your assistance!","['integration', 'area', 'multivariable-calculus', 'calculus', 'polar-coordinates']"
4807322,How to find the limit of a composite function?,"I think there's a law something like this $\lim_{x\to a}f(g(x))= f(\lim_{x\to a} g(x))$ but it has two conditions: Limit of g(x) at $a$ should exist. $f(x)$ should be continuous at $\lim_{x\to a} g(x)$ . In the picture, I don't think $f(x)$ is continuous so how would I find this limit? There's an answer given in the book but I don't understand it.
Any help would be appreciated. Edit : The answers already given are understood by me, but I am not sure if they are correct, so if you could give a specific credible answer, I'd appreciate that.
Thanks!","['limits', 'limits-without-lhopital']"
4807328,Find distribution solution of differential equation,I want to find solution of the equation $T' +aT =0$ where $T \in D'(\mathbb{R})$ and $a \in \mathbb{R}$ . What I have done so far: $$\frac{d}{dx}(e^{ax}T) = a e^{ax}T + e^{ax} \frac{dT}{dx} = e^{ax} \left(aT +  \frac{dT}{dx} \right) = 0$$ This means that the distribution $e^{ax}T$ is induced by some constant $c$ .,"['functional-analysis', 'ordinary-differential-equations', 'distribution-theory']"
4807330,What does $\mathbb{E}[X|Y]$ (conditional expectation) mean?,"I am wondering what the conditional expectation $\mathbb{E}[X|Y]$ means- when I looked it up on Wikipedia, the only definition given is for $\mathbb{E}[X|Y=y]$ for some value $y$ which $Y$ assumes. I am mainly wondering what it means to condition on an entire random variable rather than an event.","['conditional-expectation', 'definition', 'probability-theory', 'probability']"
4807362,Second-order Taylor expansion for Operators,Let $u(t)$ and $v(t)$ be functions in $C^{\infty}$ . Then let $A(u)$ be an operator. A valid reference mentioned that the second-order Taylor expansion of the operator $A$ is: $$A(u+v) = A(u) + dA(u)[v] + \int_{0}^{1}(1-\alpha)d^{(2)}A(u + \alpha v)[v]^{2} d\alpha$$ What does the notation $dA(u)[v]$ and $d^{(2)}A(u + \alpha v)[v]^{2}$ mean?,"['nonlinear-analysis', 'approximation', 'ordinary-differential-equations', 'operator-theory', 'derivatives']"
4807374,Mean speed in a network,"I am new to this community so I hope this is the rigth place for this question. I am working on traffic simulations on a certain area, and I need to know which is the average speed in the area during the simulated period (4 hours). I know, for each vehicle in the simulation, the distance covered and the time needed, thus I can compute the mean speed of each vehicle by dividing those two. Now that I have a list of mean speeds, what is the more correct thing to do: should I compute the average of those mean speeds, or their harmonic average? And why one is more suitable than the other? By searching online I found out that harmonic average is commonly used to compute the mean speed of a vehicle, but does this apply also to multiple vehicles on multiple different streets? Second question: I will do multiple simulations with different parameters, and at the end I need to know the mean speed over the N simulations. Can I simply compute the mean (or harmonic mean?) of the mean values obtained in each simulation? I hope I have been clear enough. Thank you in advance.","['statistics', 'simulation', 'means']"
4807378,Is it possible to solve $2^{x} = x^{x} + x$ algebraically? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 7 months ago . Improve this question I came across the equation $2^{x} = x^{x} + x$ that was claimed to be only solvable via graphing or approximation. Is that the case, and if so why? I don't understand why the answer can't be described with operations.",['algebra-precalculus']
4807386,Can any arbitrary group act on any arbitrary set?,Usually the examples for group actions involve a permutation group $(S_n)$ of a set (with $n$ elements) acting on that set. But what about $S_n$ acting on a set that has more or less elements than $n$ ? How does that make sense ?,"['group-theory', 'group-actions']"
4807387,Can an exact vector field have loops as solutions?,"I was asking myself this question. We define for an exact vector field $\mathbf{v}(\mathbf x) \in \mathbb{R}^n$ , s.t. $\mathbf{v}(\mathbf x)=\nabla u(\mathbf x)$ for a potential $u$ , a solution as a function $f(t):[0,T] \rightarrow \mathbb{R}^n$ such that $f'(t)=\mathbf{v}(f(t))$ for every $t$ . The question is: if the vector field is exact, can there be solutions with $f(0)=f(T)$ , i.e. closed loops ? Observation: If we have the hypothesis that the vector field is always non zero maybe the situation is easier. In fact if such a solution would exist and we call $\gamma$ the associated loop, $\int_{\gamma} \mathbf{v}=\int_{\gamma} \nabla{u}=\mathbf{0}$ , so the integral of the vector field along the loop should be zero. This means that the component tangent to the loop should change sign, in particular there must be a point $p$ along the loop where the component of the vector field tangent to the loop vanishes. This should be true for every loop. For our loop the vector field is also always tangent to $\gamma$ , so this means that $\mathbf{v}(p)=\mathbf{0}$ . But this cannot be if the vector field is always non zero. Is the observation correct or formalizable with enough regularity conditions? And what about exact vector fields that can vanish? Are there general results on the loops that an exact vector field can have?","['integration', 'self-learning', 'vector-fields', 'analysis', 'closed-form']"
4807392,Base change of flat morphism preserve relative dimension,I have a flat morphism of relative dimension $d$ . Is it true that a base change of this morphism is a flat morphism of the same relative dimension? We can assume that all schemes are of finite type over some field and equidimensional.,['algebraic-geometry']
4807393,homology connectedness and open sets,"Let's make the following recursive definition.
We will say a topological space $X$ is $(-1)$ -connected if it is not the empty set $n$ -connected if it is $n-1$ -connected and not the union of two $n-1$ -connected open sets $U,V$ for which $U \cap V$ is not $n-1$ -connected. Is being $n$ -connected the same as having $\tilde H_0, \dots, \tilde H_n$ be trivial? If not, what's a counterexample?","['general-topology', 'algebraic-topology']"
4807403,"Show that the set $C := \left\{\left.\sum_{j=1}^\infty\lambda_j e_j\right|\, \lambda_j \in \mathbb{K},\,|\lambda_j| \leq a_j\right\}$ is compact in H.","I am trying to work out question 8.17 from the book 'Functional Analysis: an elementary introduction' by Haase.
The question is formulated as follows.
Let $(e_j)_{j\geq 1}$ be an orthonormal system in a Hilbert space $H$ , and let $a_j \geq 0$ be scalars with $\sum_{j=1}^\infty a_j^2 < \infty$ . Show that the set \begin{align}
    C := \left\{\left.\sum_{j=1}^\infty\lambda_j e_j\right|\, \lambda_j \in \mathbb{K},\,|\lambda_j| \leq a_j\right\}
\end{align} is compact in $H$ . This is my attempted solution:
I would go with the definition of sequential compactness here. My plan was either to construct a pairwise orthogonal sequence. Then, showing that $
\sum_{j=1}^\infty \|f_n\|^2 < \infty$ would imply that $\sum_j f_n$ converges in H. I thought about using Bessel's inequality, which turns out here to be an equality , since for some $f\in C$ we have \begin{align}
    \langle f, e_k\rangle = \langle\sum_{j=1}^\infty \lambda_j e_j, e_k\rangle = \sum_{j=1}^\infty \lambda_j \langle e_j, e_k\rangle = \lambda_k.
\end{align} Thus, we can write $f$ as follows \begin{align}
    f = \sum_{j=1}^\infty\lambda_j e_j = \sum_{j=1}^\infty\langle f, e_j\rangle e_j = Pf,
\end{align} the standard abstract Fourier series.
From this, we can deduce that \begin{align}
    \|f\|^2 = \sum_{j=1}^\infty |\langle f,e_j\rangle |^2 = \sum_{j=1}^\infty |\lambda_j|^2 \left(\leq \sum_{j=1}^\infty a_j^2 < \infty\right),
\end{align} using Pythagoras. Now,  To show that $C$ is compact we have to show that every sequence in $C$ has a converging subsequence. Let $(f_n)_{n\geq 1}$ be a sequence in $C$ . Let $(f_{n_k})_{k\geq 1}$ be a subsequence, we need to show that $(f_{n_k})_{k\geq 1}$ converges to some $f$ in $C$ . Each $f_n$ is of the form \begin{align}
    f_n = \sum_{j=1}^\infty \lambda_{n,j} e_{n, j},
\end{align} the coefficients, $\lambda_{n, \cdot}$ , depend on $n$ , and so does the 'positions', $e_{n, \cdot}$ , where we place these coefficients. The sequence $(f_n)_{n\geq 1}$ is not pairwise orthogonal. \begin{align}
    \langle f_n, f_m\rangle = \left\langle \sum_{j=1}^\infty \lambda_{n, j}e_{n,j}, \sum_{k=1}^\infty \lambda_{m, k}e_{m,k}\right\rangle = \sum_{j=1}^\infty\sum_{k=1}^\infty \lambda_{n,j}\lambda_{m, k} \langle e_{n,j}, e_{m,k}\rangle
\end{align} we can say nothing about the term $\langle e_{n,j}, e_{m,k}\rangle$ .
We could also try to show that this set $C$ is closed in some other compact set $D$ . But then we would have to construct some set $D$ , and I do not really get far with this approach either. Any help is much appreciated!","['hilbert-spaces', 'functional-analysis', 'compactness']"
4807435,How to integrate $\int \ln(x+\ln(x+\ln(x+...))) dx$?,"How to integrate: $$\int \ln(x+\ln(x+\ln(x+...))) dx$$ I am trying to integrate this expression by taking the above expression as $y$ . Therefore, $y=\ln(x+\ln(x+\ln(x+...)))$ . Now from this we can conclude that $y=\ln(x+y)$ . Therefore, $e^{y}=(x+y)$ . Now if I was asked to differentiate this expression then my method would be $e^{y}y'=1+y'$ . Therefore, $y'(e^{y}-1)=1$ . Therefore, $y'=\frac{1}{e^{y}-1}$ , where $y=\ln(x+\ln(x+\ln(x+...)))$ . But I can't integrate this expression. Please help me out with this question.","['integration', 'indefinite-integrals', 'calculus', 'logarithms']"
4807472,Probability that the row in the table contain only one bit one?,"I am an electrical engineer that is currently working with an $m \times n$ matrix that only contains bit 0 and bit 1 in its cell. Assuming that bit 0 and bit 1 can appear in a cell with probability $p$ and $q$ respectively, where $p+q=1$ . Also, assume that the probability of 1 and 0 in each cell is independent of any other cell. 1/ How can I evaluate the probability that a row contain only one bit 1 ? For example, row 1, row 2, and final row  contain only one bit 1 2/ On average, how many row will contain only one bit 1 ? In my application, there is a row dropping action that can be describe as follows: Starting at the row that contain only bit one (row 5) and begin to look up and look down (blue arrow), if we see bit 1 above (same column) or bit 1 right below it (same column) then we will initiate a drop row action for the corresponding row. Note that I have marked the bit 1 above and bit 1 below in a blue rectangular cell. So, in total 4 rows will be dropped. That is row 1, 2,7 and 8. 3/ On average, how many row will be dropped ? I have a guts feeling that this may have something to do with the binomial distribution or geometric distribution but cannot get me head around it. Please help me with this ? Edit for clarity : 1/ For question 3, it is possible that there are many rows that contains one bit 1. 2/ For question 3, row that contain one bit 1 will never be dropped ! Thank you for your enthusiasm !","['statistics', 'probability']"
4807494,Prove some points are concyclic,"question We consider the triangle $ABC$ with $AB$ , $AC$ and in which $(AX$ , $(AY$ , two half lines inside the $\angle BAC$ , isogonal (ie: $\angle BAX= \angle CAY$ ). We consider the points $E$ and $F$ outside the triangle from which the sides $(AB)$ respectively $( AC) $ are seen under the same angle, in different semi-planes determined by the line $AB$ . The circumscribed circle of the triangle $ABE$ cuts the semi-steps $(AX$ respectively $(A$ Y after the points $M$ and $P$ , respectively the circumscribed circle of the triangle $AFC$ cuts the semi-steps $(AX$ $(AY$ , after the points $N$ and $Q$ . Prove that: a) Points $M, N, P$ and $Q$ are concyclic: b) The centre of the circumcircle of the quadrilateral $MNPQ$ lies on the perpendicular bisector of the segment $BC$ . my drawing my idea $MBAP$ and $NQCA$ are inscribed quadrangles $=> \angle QAC= \angle NQA$ and $\angle BAM= \angle AMP$ We also know from the start that $\angle BAM= \angle QAC$ It means that $\angle NQA= \angle AMP=>$ what we had to prove I dont know what to do forward. Hope one of you can help me!
Thank you!","['angle', 'circles', 'geometry']"
4807506,Random points in 2-sphere,"Say we have the PDF of uniformly picking a random point in the unit $2$ -sphere, with distance from origin of the point being $r$ , given as $$
f_R(r) = \begin{cases}
    3r^2 & 0 \leq r \leq 1 \\
    0 & \text{otherwise}
\end{cases}
$$ We then want to extend this to n points. How would I find the distributions of $X$ , where $X$ is the second furthest point from the origin of these $n$ points? I imagine it is something along the lines of considering spheres within the sphere, but this isn't getting me anywhere.","['probability-distributions', 'geometric-probability', 'probability']"
4807517,$P\left\{\left|\sum_{i=1}^n\left(\xi_{i j k}^2-1\right)\right| \geq n \delta\right\} \leq 2 \exp \left(-\frac{n \delta^2}{32+4 \delta}\right) $,"This concentraion inequality was found when I read a paper about gaussian graphical model. The author gave this result directly. $$
P\left\{\left|\sum_{i=1}^n\left(\xi_{i j k}^2-1\right)\right| \geq n \delta\right\} \leq 2 \exp \left(-\frac{n \delta^2}{32+4 \delta}\right) 
$$ where $\xi_{i j k}$ 's are independent $N\left(0,1\right)$ .
I have checked some materials but I cannot get the value $32+4 \delta$ in the denominator.How did the author obtain this result?","['statistics', 'probability', 'inequality']"
4807522,Proving Exhaustion of Primitive Pythagorean Triples,"I claim that the following algorithm will always give a primitive Pythagorean triple. Let $d$ be some even number and let $D$ be the set containing all its factors. Then for any two coprime factors $d_1,d_2$ who multiply to $d$ where $d_1\equiv1\pmod 2$ and $d_2\equiv0\pmod2$ , the following is always a primitive Pythagorean triple: $$\left(d+d_1^2,d+\frac{d_2^2}{2},d+d_1^2+\frac{d_2^2}{2}\right)$$ Ex. $d=12$ has coprime factors $d_1=3$ and $d_2=4$ . Plugging into our formula: $$\left(12+3^2,12+\frac{4^2}{2},12+3^2+\frac{4^2}{2}\right)$$ $$(21,20,29)$$ I noticed this from doing some geometric constructions of inscribed circles of natural radius within known Pythagorean triple triangles and spotting patterns. I would be very surprised if this were not well known. It is fairly straightforward to show that this formula always generates a primitive Pythagorean triple by using prime factorizations. Choosing relatively composite factors simply generates nonprimitive triples. However, I also wrote a program to check if this approach exhausts primitive Pythagorean triples, and for the first ~2.7 million triples it does. But I have no idea how one might prove something like this. Some advice would be greatly appreciated.","['elementary-number-theory', 'pythagorean-triples', 'geometry']"
4807526,calculating relative focal length based on a cylinder,"I am trying to camera match a photo of a cylinder. The photo is taken relatively straight, and I can get a rotation matrix for the camera based on the elipsis created by projecting the top circle. The problem shows up when I try to figure out what focal length the camera has. My thinking is that given the principal point in the center of the image, and that the x and y axis(green and red) are equal in length in 3d space, I should be able to increase the focal length until they align in length with what is on the screen. Does this make sense, and is there a mathematical solution to this that I am currently not seeing? working on the answer by Hosam:
I created an image using threejs editor .
Placed a cylinder at origo with height 1 and radius 0.5. Placed the camera arbitrarily at $[0,0.64,1.96]$ Rotated it down(in x) by $-15.4deg$ And lastly set the field of view to $75deg$ (bottom to top) I then pulled the image into my tool and mapped out all points. from this I got: $$
a = 0.555 \\
b = 0.039 \\
c_0 = 0.506 \\
$$ I threw this into wolfram alpha as it is a bit quicker than writing the code for it 2 results came out that are quite similar to each other at roughly $0.12$ . so from that focal length I can get a field of view. $$
2 * atan(1 / 0.12)
$$ .
Which problematically does not equal $1.308$ (75deg is radians)","['trigonometry', 'computer-vision', 'geometry', '3d']"
4807559,"Is it true that C(ℝ,τK) = C(ℝ)?","Let $\tau_K$ be the $K$ -topology on reals and $\tau$ the Euclidean topology on reals. Let $f\colon (\mathbb{R}, \tau_K) \to (\mathbb{R}, \tau)$ be a continous function. Is it the case that $f$ remains continuous when we interpret it as a function $f\colon (\mathbb{R}, \tau) \to (\mathbb{R}, \tau)$ ? Topology $\tau_K$ consist of sets $U\setminus A$ where $U$ is open in Euclidean topology and $A \subset \{\,1/n : n \in \mathbb{Z}_+ \,\} =: K$ . Hence $\tau \subsetneq \tau_K$ . Check out Wikipedia for some interesting properties of $K$ -topology. I think the answer is yes. That is $C(\mathbb{R}, \tau_K) = C(\mathbb{R}, \tau)$ . Below is the sketch of the proof. I will be glad for feedback on its correctness, alternative proof (mine seems like an overkill), or disproof/counterexample. Context: $C(ℝ,τ_K) = C(ℝ)$ implies the weak topology on $ℝ$ generated by $C(ℝ,τ_K)$ is just the Euclidean topology. Proof sketch . Assume for the sake of a contradiciton that $f\colon (\mathbb{R}, \tau_K) \to (\mathbb{R}, \tau)$ is continous, but $\hat f \colon (\mathbb{R}, \tau) \to (\mathbb{R}, \tau)$ , which sends $x$ to $f(x)$ , is not contionus.
Both $\tau_K$ and $\tau$ are separable, hence sequential spaces.
Therefore the continuity of $f$ is equivalent to saying that for any $\tau_K$ -convergence sequence $x_n \to x$ we have $f(x_n) \to f(x)$ .
And $\hat f$ is not continous iff there exist $\tau$ -convergent sequence $x_n \to x$ for which $\hat f(x_n) \not \to \hat f(x)$ .
Hence by assumption there is a sequence $(x_n)$ which is $\tau$ -convergent to some $x$ (not $\tau_K$ -convergent) and $\hat f(x_n) \not \to f(x)$ .
But only $\tau$ -convergent sequences which are not $\tau_K$ -convergent are the ones $\tau$ -converging to $0$ with infinitely many, decreasing terms of the form $1/m$ . Let $(y_n)$ be such subsequence of $(x_n)$ . Take net $(z_\alpha)_{\alpha \in A}$ where $z_\alpha = \alpha \in A := (0,1) - K$ and $\preccurlyeq$ is the ordering of $A$ which is just reversed $\leq$ . That is $a \preccurlyeq b \iff b \leq a$ .
Net $(z_\alpha)$ conveges to $0$ in both topologies. Hence $f(x_\alpha) \to 0$ and $\hat f(\alpha) \to 0$ .
Take $\epsilon> 0$ so small that infinitely many $\hat f(y_n)$ are in $U := (-\infty, -2\epsilon) \cup (2 \epsilon, +\infty)$ . Such $\epsilon$ exists because $\hat f(y_n) \not \to \hat f(0)$ . Let $B = B(\epsilon, f(0))$ . On the other hand, because $f(x_\alpha) \to f(0)$ , there is $\beta \in A$ such that $\beta \preccurlyeq \alpha$ implies $f(x_\alpha) \in B$ . Hence for any $x \in (0, \beta) - K$ we have $f(x) \in B$ .
The set $$
V := f^{-1}(U) \cap f^{-1}(B) \cap (0, \beta)
$$ is $\tau_K$ -open, becasue $f$ is $\tau_K$ -contionus and $(0,\beta)$ is open. Note $(0,\beta) -K \subset f^{-1}(B)$ . Set $V$ is not empty because infinitely many $y_n$ 's are in $f^{-1}(U) \cap (0,\beta)$ and none of $y_n$ is in $(0, \beta)-K$ , because $y_n \in K$ .
But $V$ is a subset of $K$ , which is not $\tau_K$ open — a contradiction.","['continuity', 'general-topology', 'solution-verification']"
4807581,One problem about a barrier,"Suppose you have two points $A(0,0)$ and $B(L,0)$ on a plane $\mathbb{R}^2$ and $AB$ is a perpendicilar bisector of a straight barrier $\Omega=\{\ (x,y)\ |\ x=L/2,y\in [-\frac{a}{2},\frac{a}{2}] \}$ , also, $A$ and $B$ are standing on equal distances from $\Omega$ . So, let us call the middle point of $\Omega$ $O(L/2,0)$ , then $AO=BO=L/2$ . You want to move from $A$ to $B$ by the shortest line with constant velocity , but your moving point only has information about the barrier in an $\varepsilon$ -disc around itself. Thus, the trajectory of a moving point can be determined only in some neighbourhood of a moving point. For example, if $\varepsilon=\infty$ (it means that you can see the straight segment fully), you can move (without loss of generality due to the symmetry within the problem) from $A$ to the 'highest' point of $\Omega$ by a straight segment and then to $B$ in the same manner. In the case when $\varepsilon=L/2$ (it means that in the beginning of the motion we can see only the middle point of $\Omega$ ) the problem gets much more interesting! Our point will not be able to move by a straight segment to the peak of $\Omega$ as it will not have the full information about where the peak is. So there appears some curve from $A$ heading to the peak. The algorithm of finding the desirable path is as following: Move forfard, towards $\Omega$ , a bit, by a step $ds$ ; Find two points (one of which we can omit by the symmetry) of the intersection of the $\varepsilon$ -circle (to remind, $\varepsilon=L/2$ ); From one of the found points build a segment to the point $(ds,0)$ ; Move a bit, by $ds$ , in the direction of the segment built in (3); Repeat (1) to (4). This way seems to be extra-intuitive and informal. My question is: how to describe the path in terms of differential equations? I am attaching my considerations below, so feel free to check and fix them. Let us be searching the path in a parametric form $(x(t),y(t))$ . Firstly, $ds=\sqrt{\dot{x}^2+\dot{y}^2}dt$ . Secondly, by stepping by $ds$ towards $\Omega$ for the first time (at the zero iteration) our $\varepsilon$ -circle intersects the barrier in the points $$y=\sqrt{L^2-(L-ds)^2}.$$ Having these facts in the arsenal, we can conclude that at the first iteration we will be moving by the segment $(L-(L-(ds+dx_1))),y-dy_1)$ , where $dx_1$ and $dy_1$ stand for the translations in $x$ and $y$ after the zero iteration. After some algebraic transformations and saying that $dx_1=\dot{x},dy_1=\dot{y}$ I get the following equation: $$\dot{y}-\ddot{y}=\sqrt{2\ddot{x}\sqrt{\dot{x}^2+\dot{y}^2}(L-1)-2\dot{x}\ddot{x}}.$$ But one needs some other equation to make up a system of two equations determine the trajectory. By the way, at the moment $x_0=L-\sqrt{L^2-a^2/4}$ we will be moving to the peak by a straight segment as at this moment we will be able to see it.","['calculus', 'geometry', 'ordinary-differential-equations']"
4807635,Confusion between the equivalence of change of variables formulas,"In my course we have proven the following result Let $g$ is a non-negative measurable function on $G$ with measure $\nu$ . Suppose $f: E \rightarrow G$ is a measurable function and that $\mu$ is a measure on $(E, \mathcal{E})$ such that $$
\nu=\mu \circ f^{-1} .
$$ Then $$
\int_G g \mathrm{~d} \nu=\int_E g \circ f \mathrm{~d} \mu .
$$ Separetly then, we proved (via a different argument) Let $\phi:[a, b] \rightarrow \mathbb{R}$ be continuously differentiable and increasing. Then for any bounded Borel function $g$ , we have $$
\int_{\phi(a)}^{\phi(b)} g(y) \mathrm{d} y=\int_a^b g(\phi(x)) \phi^{\prime}(x) \mathrm{d} x .
$$ For $g$ non negative measurable I should be able to derive the second result from the first since it is more general. However, I am confused with what is going on. Here is how I am thinking about this formulation: First let us convert the second equation into a measure theoretic formulation: \begin{align*}
\int_{\mathbb{R} }^{} 1_{(\phi (a), \phi(b))} g \, \mathrm{d} \lambda = \int_{\mathbb{R} }^{} g \circ  \phi * 1_{(a,b)} * \phi' \, \mathrm{d} \lambda 
\end{align*} where $\lambda $ is the Lebesgue measure on $\mathbb{R} $ . My issue now is that I do not know how to apply the previous part. I need to seek $\mu $ such that \begin{align*}
\lambda  (A) = \mu (\phi^{-1} (A)).
\end{align*} But how do I find it? Question : Is it that we can't just derive the second from the first because it is hard to find such a $\mu $ moreover even if we find it it might not be the lebesgue measure as we desired? Additional : If so, when do we use the first? I am aware that one can derive formulas for lebesgue to lebesgue measure, however, the above attempted proof has managed to confuse me. Could someone help me understand what is going on?","['measure-theory', 'change-of-variable', 'probability']"
4807666,"Showing $\sum_{i=1}^n\tan\alpha_i\geq (n-1)\cdot \sum_{i=1}^n\cot\alpha_i$, for real $\alpha_i\in(0,\pi/2)$ with $\sum_{i=1}^n\cos^2\alpha_i=1$","Real numbers $\alpha_1,\ldots,\alpha_n \in \left(0,\ \frac{\pi} 2\right)$ satisfy the condition $\sum_{i=1}^n\cos^2\alpha_i=1$ .
Prove that $$\sum_{i=1}^n\tan\alpha_i\geq (n-1)\cdot \sum_{i=1}^n\cot\alpha_i$$ This is from the final part of the book, where no technique is suggested. I tried using the observation that $n-1 = \sum\sin^2\alpha_i$ , but substituting this into the inequality we obtain the inequality that is false in general (without the assumption $\sum\cos^2\alpha_i=1$ ). My most promising idea is the following. Thanspose the condition into: $$n\cdot \sum_{i=1}^n\cot\alpha_i \leq \sum_{i=1}^n\left(\tan\alpha_i + \cot\alpha_i\right).$$ $$n\cdot \sum_{i=1}^n\cot\alpha_i \leq \sum_{i=1}^n\frac 1{\sin\alpha_i\cdot \cos\alpha_i}.$$ $$n\cdot \sum_{i=1}^n\frac {\cos^2\alpha_i}{\sin\alpha_i\cdot \cos\alpha_i} \leq \sum_{i=1}^n\frac 1{\sin\alpha_i\cdot \cos\alpha_i}.$$ Then we could use Chebyshev's inequality for anti-monotone sequences (I don't know the name in English). Assuming $\alpha_i\leq \alpha_{i+1}$ , the sequence $a_i = \cos^2\alpha_i$ is decreasing and $b_i = 1/(\sin\alpha_i\cos\alpha_i)$ is increasing provided $\alpha_i\geq \pi/4$ . Therefore we have the proof for such angles: $$n\cdot \sum_{i=1}^n\frac {\cos^2\alpha_i}{\sin\alpha_i\cdot \cos\alpha_i} \stackrel{(\star)}\leq \left(\sum_{i=1}^n \cos^2\alpha_i\right)\cdot \sum_{i=1}^n\frac 1{\sin\alpha_i\cdot \cos\alpha_i} = \sum_{i=1}^n\frac 1{\sin\alpha_i\cdot \cos\alpha_i}.$$ Unfortunately, one angle $\alpha_1$ can be smaller than $\pi/4$ and the proof doesn't go. What's more, the inequality (*) also doesn't hold in general (if one angle is smaller than $\pi/4$ ). I started thinking if the conclusion is false, but I couldn't find the counterexample (I used spreadsheet).","['rearrangement-inequality', 'inequality', 'trigonometry', 'substitution']"
4807684,Matrix and Taylor Expansion of a Finite Continued Function,"I noticed a pattern between matrix and Taylor series of a finite continued fraction function. However, I don't know how to prove it or why they are related. Let $$
f_{1}(z)=\frac{1}{-z-1}
$$ $$
f_{2}(z)=\frac{1}{-z-\frac{1}{-z-1}}
$$ Similarly, we define $f_{3}(z),f_{4}(z)$ , etc. That is, $f_{l}(z)$ has $l$ layers. Let's consider the Taylor expansion of $f_{l}(z)$ at $z=0$ , and write $$
f_{l}(z)=a_{l}(0)+a_{l}(1)z+a_{l}(2)z^{2}+\cdots
$$ Now, let's consider the following $l \times l$ matrix $$
A_{l}=\begin{pmatrix}
    1 & 1 & \cdots & 1 & 1 \\
    1 & 1 & \cdots & 1 & 0\\
    &\vdots \\
    1 & 0 & \cdots & 0 & 0
  \end{pmatrix}
$$ Here is the amazing thing: $$
(A_{l}^{n})_{1,1}= \mid a_{l}(n) \mid
$$ where $(A_{l}^{n})_{1,1}$ stands for the $(1,1)$ -entry of the matrix $A_{l}^{n}$ . I don't have a proof, but I checked with many $l$ and $n$ and they all hold. My questions are the following: How to prove this equality? (I tried to prove it directly, but taking $n$ -th derivative of a continued fraction is a nightmare) What is the intuition behind it? Is there any deeper connection between this matrix and this sequence of finite continued fractions? Is it a special case of some deeper result? Thank you very much!","['continued-fractions', 'linear-algebra', 'taylor-expansion', 'real-analysis']"
4807704,Does $f(A) \cap f(f^{-1}(B))$ equal $f(A) \cap B$?,"More precisely, for a function $f: X \rightarrow Y$ and sets $A \subseteq X$ , $B \subseteq Y$ , does $f(A) \cap f(f^{-1}(B)) = f(A) \cap B$ even though $f(f^{-1}(B)) \subseteq B$ ? Editing to add my thoughts and some context (thanks for the feedback): after going through this list of basic properties of images and preimages as well as this question , I believe the statement to be true since $f(A) \cap f(f^{-1}(B)) = f(A \cap f^{-1}(B))$ , so the intersection in question is an image which may solve the issues that come with invalidly claiming $f(A) \cap f(f^{-1}(B)) = f(A) \cap B$ since $f(f^{-1}(B)) = B$ . However, I don't see how to use this fact to rigorously prove the claim.",['elementary-set-theory']
4807713,Is there a 3D version of the convolution and cross-correlation theorems for the Laplace transform?,"The Laplace-transform version of the convolution and cross-correlation theorems is essentially the same as the ""usual"" (Fourier-transform) version: if $\mathcal{L}[f(t)]$ is the Laplace transform of a function $f(t)$ , given by $$\mathcal{L}[f(t)]=\int_0^\infty f(t)\,e^{-st}\,\text{d}t,$$ and if $f\ast g$ is the convolution of $f$ and $g$ and $f★ g$ is the cross-correlation of $f$ and $g$ , given by $$(f\ast g)(t)=\int_0^t f(\tau)\,g(t-\tau)\,\text{d}\tau$$ and $$(f\star g)(t)=\int_0^\infty f^\ast(\tau)\,g(t+\tau)\,\text{d}\tau,$$ then $$\mathcal{L}[(f\ast g)(t)]=\mathcal{L}[f(t)]\,\mathcal{L}[g(t)]$$ and $$\mathcal{L}[(f\star g)(t)]=\mathcal{L}[f^\ast(-t^\ast)]\,\mathcal{L}[g(t)],$$ where $^\ast$ denotes complex conjugation. I'd like to know whether those also hold for functions of multiple variables (e.g. $f(t_1,t_2,t_3)$ and $g(t_1,t_2,t_3)$ ) and, if they do, whether any of the usual shenanigans involving higher-dimensional calculus occur here (e.g. whether the argument of $g$ in the convolution integral is $g(t_1-\tau_1,t_2-\tau_2,t_3-\tau_3)$ or $g(|\vec{t}-\vec{\tau}|)$ with $\vec{t}=(t_1,t_2,t_3),\vec{\tau}=(\tau_1,\tau_2,\tau_3)$ or something else, and similarly for the argument of $g$ in the correlation integral). The Wikipedia article on the convolution theorem states the theorem for the Fourier transform (as is usual as far as I'm aware), says that ""[t]he theorem also generally applies to multi-dimensional functions"" and then (in a different section) states that ""[t]his theorem also holds for the Laplace transform"", but it doesn't specifically state that the generalisation to functions of multiple variables also holds for the Laplace transform. Intuition says it should, but when I tried my hand at a simple proof I ran into a wall. Wikipedia has no article on the cross-correlation theorem, but it mentions the theorem on its article on the discrete Fourier transform has a section on the theorem (in which it doesn't mention the Laplace transform at all). The Wikipedia article on the Laplace transform has a section in which it states both theorems for single-variable functions. I haven't been able to find the required information, and especially the bit about the argument of $g$ in the integrals, anywhere.","['convolution', 'multivariable-calculus', 'laplace-transform']"
4807720,Is there a standard notation for the measure $E \mapsto \mu(E \cap S)$?,"If $\mu$ is a measure and $S$ is $\mu$ -measurable, it is easy to see that the measure $E \mapsto \mu(E \cap S)$ defines a measure on the $\sigma$ -algebra where $\mu$ is defined. I have been using the notation $\mu_S$ to denote this measure. However, I would like to use a standard notation, if there is one available.","['notation', 'measure-theory', 'real-analysis']"
4807729,"Stanley says $E_{n+1,k+1}=E_{n+1,k}+E_{n,n-k}$ about counting alternating permutations","I'm having some trouble understanding the solution that R. Stanley gives to the following: Let $n\geq k \geq 0$ and let $E_{n,k}$ be the number of alternating permutations of $[n+1]$ that starts with $k+1$ . Show that: $$
E_{n+1,k+1} = E_{n+1,k} + E_{n,n-k}.
$$ Stanley gives the following hint:
Show that $E_{n+1,k}$ is the number of alternating permutations of $[n+2]$ with first term $k+1$ and second term unequal to $k$ ,
and that $E_{n,n-k}$ is the number of alternating permutations of $[n+2]$ with first term $k+1$ and second term $k$ . Can someone help me understand it?","['permutations', 'graph-theory', 'recurrence-relations', 'combinatorics', 'discrete-mathematics']"
4807747,Integral of $\displaystyle\int_{|z| = 1} \frac{\sin(e^z)}{z^2} dz$,"I'm required to solve the integral $\displaystyle\int_{|z| = 1} \frac{\sin(e^z)}{z^2} dz$ . For this, I'm using the Cauchy Integral Theorem which states that $$f^{(n)}(a) = \frac{n!}{2\pi i} \int_{\gamma} \frac{f(z)}{(z-a)^{n+1}} dz$$ under certain conditions that I'm omitting. Thus, $$f^{(1)}(0) = \frac{1}{2\pi i} \int_{\gamma} \frac{\sin(e^z)}{z^2} dz \Longrightarrow  2\pi i f'(0) = \int_{\gamma} \frac{\sin(e^z)}{z^2} dz$$ where $f'(z) = \frac{d}{dz} \sin(e^z) = e^z \cos(e^z)$ , where it follows that $f'(0) = \cos(1)$ . Therefore, $$\int_{|z| = 1} \frac{\sin(e^z)}{z^2} dz = 2\pi i\cos(1)$$ It looks correct to me, so I'm just looking for a solution-verification second opinion. Thanks!","['complex-analysis', 'solution-verification']"
4807779,Is knowing unit basis vector enough to specify a coordinate system?,"In orthonormal curvilinear coordinate system, we define unit basis vector as $$\mathbf{\hat{e}}_u = \frac{1}{h_u} \frac{d\mathbf{r}}{du},$$ where $h_u = |\frac{d\mathbf{r}}{du}|$ . Suppose we only know $\mathbf{e}_u$ in relation to Cartesian coordinates, are we able to construct coordinate system from there? Using polar coordinates $(s,\phi)$ as an example, given only the information $$\mathbf{\hat{s}}=\frac{x\mathbf{\hat{x}} + y \mathbf{\hat{y}}}{\sqrt{x^2+y^2}}, \qquad \mathbf{\hat{\phi}}=\frac{-y\mathbf{\hat{x}}+x\mathbf{\hat{y}}}{\sqrt{x^2+y^2}}, $$ arę we able to construct a coordinate system with $\mathbf{\hat{s}}$ and $\mathbf{\hat{\phi}}$ as basis unit vector? More generally, suppose I want to construct a new coordinate system $(u,v)$ but is only given $$ \mathbf{\hat{u}}=f(x,y)\mathbf{\hat{x}} + g(x,y)\mathbf{\hat{y}}, \qquad \mathbf{\hat{v}}=g(x,y)\mathbf{\hat{x}} - f(x,y)\mathbf{\hat{y}} $$ as the unit basis vector. Is there a way to find $$\mathbf{\hat{u}}=f'(u,v)\mathbf{\hat{x}} + g'(u,v)\mathbf{\hat{y}}, \qquad \mathbf{\hat{v}}=f'(u,v)\mathbf{\hat{x}} - g'(u,v)\mathbf{\hat{y}} ,$$ which gives a relationship between $x=f(u,v), y=g(u,v)$ . This will enable the computation of norm $h_u$ of tangent vector $\frac{d\mathbf{r}}{du}$ . Then, we are able to specify the differential in $(u,v)$ as $$ d\mathbf{r}= \mathbf{\hat{u}} h_u du + \mathbf{\hat{v}} h_v dv .$$","['curvilinear-coordinates', 'multivariable-calculus', 'change-of-basis', 'polar-coordinates', 'differential-geometry']"
4807782,Construction of the free operad,"I am reading Fresse's ""Koszul duality of operads and homology of partition posets"" , and trying to understand the construction of the free operad $F(M)$ on a symmetric sequence $M$ as explained on page 70. To my understanding the arity $n$ -entry $F(M)$ of the free operad can be viewed as equivalence classes of trees (with $n$ leaves/entries) whose inner vertices are labelled by elements of the symmetric sequence $M$ (with the appropriate arity). Additionally, the leaves are labelled by elements of the set $\{1,…,n\}$ . I am unsure about the $\mathbb{S}_n$ -action on $F(M)(n)$ , though. Is it true that $\mathbb{S}_n$ acts on such a tree by permuting the labelling of the leaves? Does it also act on the labelled inner vertices somehow? I am confused since I do not see where in the construction we are using the $\mathbb{S}$ -module structure on $M$ ? Maybe in the definition of the tensor product of the treewise tensor module on page 69? I do not see how, though.","['free-modules', 'category-theory', 'operads', 'combinatorics', 'algebraic-topology']"
4807783,how to solve ode: $x''(t)-k^2\cdot x(-t)=0$,"In this ordinary differential equation $$x''(t)-k^2\cdot x(-t)=0$$ both $x(t)$ and $x(-t)$ are included. I wonder whether this still is an ode and how to solve it. I tried to use this property: $$x(t)=x_{odd}(t)+x_{even}(t)=\frac{x(t)-x(-t)}{2}+\frac{x(t)+x(-t)}{2}$$ to substitute $x(-t)$ but failed to get rid of "" $-t$ "". Now I don't know how to continue the process. Any suggestions are appreciated. Thanks.",['ordinary-differential-equations']
4807802,Number of Rolls of Die Until Sum Exceeds 6,"What is the expected number of rolls of a 6-sided die until the sum exceeds 6? As the expected value of one roll is 3.5, why is the answer not just $\frac{6}{3.5}$ ?","['expected-value', 'dice', 'probability']"
4807809,"Is there any significance or intuition to the fact that the same topology can be produced by different bases with different ""shapes""?","In $X = \mathbb R^n$ (finite $n$ to make it simple), we can use either the open balls with the Euclidean metric, $B_d(x, \epsilon)$ , or the Box topology (i.e., $n$ -dimensional rectangular prisms with finite or infinite sides), as the basis for a topology $\mathcal T$ , and these generate the same topology. We can see this because for any open set $U \in \mathcal T$ , for any $x \in X$ , we can find a basis element $B$ (from either of these) such that $x \in B$ and $B \subset U$ . This picture from Munkres illustrates it well: This makes enough sense to me. But I can't shake the feeling that they're still different, because they have different ""shapes"" (i.e., circles vs rectangles). For example, another way to generate a topology from a basis is by taking the collection of all unions of the basis elements. This paints a different picture of generating the topology, where for any open set of the topology, we must be able to form it by taking a (possibly infinite) union of basis elements. However, certain open sets of the topology will be formed trivially, or with the union of very few basis elements from one basis, while it may take the union of infinite basis elements from another basis. For example, to produce a rectangle with the ""circles"" basis, we would need infinite circles of decreasing diameter to get the corner or sides: But obviously only one from the ""rectangles"" basis. Is there any significance to this? or, is the significance that in topology, this really doesn't matter, and that's what's important?",['general-topology']
4807812,Evaluate the volume between a parabola and a sphere in high dimension,"Consider the unit ball $\mathbb{B}^{n+1}:=\{\boldsymbol{x}\in\mathbb{R}^{n+1}:\|\boldsymbol{x}\|_2\le 1\}$ and a parabola $p:\mathbb{R}^{n}\rightarrow\mathbb{R}$ for which $p(\boldsymbol{x}):=\rho\|\boldsymbol{x}\|_2^2/2$ , where $\rho>0$ is some parameter. I would like to know how the volume of $\mathbb{B}^{n+1}\cap\operatorname{epi}(p)=\mathbb{B}^{n+1}\cap\{(\boldsymbol{x},t)\in\mathbb{R}^{n}\times\mathbb{R}:p(\boldsymbol{x})\le t\}$ scales w.r.t. $\rho$ . (Notice that when $\rho=0$ , the interested volume is exactly the half of that of $\mathbb{B}^{n+1}$ ). Thanks in advance.","['integration', 'spheres', 'volume', 'geometry', 'multivariable-calculus']"
4807836,"What is the right term: ""Histogram"" of function values","I wonder what the correct technical term for this concept is: It's like a ""histogram"" or probability density of function values, like here . For example, if a particle moves along the trajectory $x(t) = \sin (t)$ , the probability density to find the particle at a random time $t_0$ is proportional to $$P(x) = \frac{\text{d}}{\text{d}x} \sin^{-1}(x) = \frac{1}{\sqrt{1 - x^2}}.$$ As you see, I'm not talking about a histogram with discrete bins, but some kind of continuous probability density. Another practical is the electrical field strength around a charged point particle. At a distance $\vec{r}$ the electrical field strength is $| \vec{E} | (\vec{r}) = \frac{E_0}{| \vec{r} |^2}$ . If I place a probe particle at some random $\vec{r}_0$ , the field it experiences $E$ is drawn from a probability distribution proportional to $$P(E) = -\frac{\text{d}}{\text{d}E} V(| \vec{E} | (\vec{r}) > E) = 2\pi \sqrt{\frac{{E_0}^3}{E^5}},$$ where $V(| \vec{E} | (\vec{r}) > E) = \frac{4}{3} \pi R^3(| \vec{E} | (\vec{r}) = E)$ is the volume of the sphere where the field strength $| \vec{E} | (\vec{r})$ is larger than $E$ , and $R(| \vec{E} | (\vec{r}) = E) = \sqrt{\frac{E_0}{E}}$ is that sphere's radius. So, what is the name of this histogram/probablity density of function values? Is it the same name if the function argument is a vector? How about when the function returns a vector? Then the histogram itself is multidimensional. Alternatively, if there is no rigorous technical term for the histogram/probability density, is there maybe a term for the process of going from the function $f(x)$ to its histogram $P_f(y)$ ?","['functions', 'terminology', 'probability-distributions']"
4807837,What is this step?,"Hello, I’m a first year engineering student and today our prof showed us this proof of the derivative of polynomials. However, I don’t understand how the second term is equal to the third term (the prof said we should know this from school so he did not explain). Could someone please explain to me about what happened there? I really can’t figure it out and don’t know how to search for it in the internet. Thanks in advance.","['proof-explanation', 'derivatives', 'analysis']"
4807849,"Suppose $g$ is a periodic function with period $k$. Let the stochastic process be defined as $X_t=g(t+T)$, where $T\sim U(0,k)$.","Suppose $g$ is a periodic function with period $k$ . Let the stochastic process be defined as $X_t=g(t+T)$ , where $T\sim U(0,k)$ . I'm trying to prove that $\{X_t:t \geq0\}$ is weak-sense stationary. I already proved that $\mu_X(t)=constant$ , for all $t$ . Now I need to prove that $R_X(t_1,t_2)$ only depends of $t_1-t_2$ or $t_2-t_1$ . By definition: $R_X(t_1,t_2)=\mathbb{E}(X_{t_{1}}X_{t_{2}})=\mathbb{E}\left[g(t_1+T)\,g(t_2+T)\right]=\frac{1}{k}\int_{0}^kg(t_1+T)\,g(t_2+T)\,dT$ . I don't know how to continue from this integral, I have tried to make variable changes and use the periodicity of $g$ , but I can't get anything in particular. Any help would be appreciated.","['stochastic-processes', 'statistics', 'probability']"
4807896,Finding the Coefficient for a Generating Function [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question I was doing a counting problem and came across the following generating function: $$\left(\sum_{i=0}^{7}x^i\right)^{100}=699.$$ I want to find the coefficient for $x^{699}.$ I tried to transform the equation into its equivalent form, the expression $$\left(\frac{x^8-1}{x-1}\right)^{100}.$$ However, I have no idea how to proceed. Can someone help me with this? Thanks a lot!","['combinatorics', 'generating-functions']"
4807909,Find the UMVUE of $P(X \leq c)$ in exponential distribution?,"Given iid observations $X_1,...,X_n$ on X with the pdf $f_\theta (x)=e^{-(x-\theta)} I(x > \theta)$ , find the UMVUE of $P(X \leq c)$ , for fixed $c>0$ . My attempt: I find $P(X \leq c)= 1-e^{-(c-\theta)}$ First, I find $T(X)=X_{(1)}$ is a complete and sufficient statistic for $\theta$ . The any UMVUE should be based on $T(X)=X_{(1)}$ . I compute $E(T(X))=\frac{\theta n+1}{n}$ . Thus, $T(X)=X_{(1)}$ is the UMVUE of $\frac{\theta n+1}{n}$ . Now there's still one step away. How to get the UMVUE of $1-e^{-(c-\theta)}$ . My idea is to compute $E[  1(X_1 \leq c)  |T]$ . But I don't know how to compute that. Or there exists other ideas, such as some transformation of the density $X_{(1)}$ and we can get the expectation directly to the $e^{\theta}$ .","['statistical-inference', 'statistics', 'parameter-estimation', 'exponential-distribution', 'probability']"
4807923,Inconsistency in heat equation over two semi-infinite rods,"Consider the 1D heat equation $$\frac{\partial u}{\partial t}(x,t) = \frac{\partial}{\partial x}\left(k(x)\frac{\partial u}{\partial x}(x,t)\right) \qquad x \in \mathbb{R}$$ where the thermal diffusivity is a piecewise constant function modelling two semi-infinite rods with different diffusivities coming in contact at $x=0$ , i.e. $k(x) = k_1$ for $x<0$ and $k(x) = k_2$ for $x\geq 0$ . The boundary conditions are well known boundary conditions for heat transfer across an interface: 1) temperature continuity $u(0^-,t) = u (0^+ , t)$ , and 2) heat flux continuity $k_1 u_x(0^-,t) = k_2 u_x(0^+,t)$ . There's one limiting case of this problem that seems inconsistent to me. Say $k_2 = 0$ . From the heat flux condition, we get that $u_x(0^-,t) = 0$ . So in this case, for the $x<0$ region, we just have the problem of a semi-infinite rod with insulating boundary condition. This should have a unique solution, which I'll call $u^*(x,t)$ . Now for the $x\geq 0$ region, we have $$\frac{\partial u}{\partial t}(x,t) =0 \qquad x\geq 0,$$ i.e. $u$ is constant in time. But how can $u$ be constant for $x\geq 0$ while $u^*$ is potentially time-dependent? This contradicts the continuity of temperature across the boundary $x = 0$ . It would be great if someone could clear up my confusion.","['heat-equation', 'ordinary-differential-equations', 'partial-differential-equations']"
4807936,Compact and connected in quotient space,"[In the question below, I use ""compact"" to mean ""compact Hausdorff"" and I use ""quasi-compact"" for what is commonly called ""compact"" in wikipedia and other sources.] The problem is, let $X$ be a compact space and define $x\sim y$ if the points are in the same connected component. Show that $X/\sim$ is totally disconnected and compact with quotient topology. My problem is not with the totally disconnected, it is with the compact, I know that the projection gives a quasi-compact quotient, but not compact because I have not proven that it is a Hausdorff space. One idea is that compact spaces are normal and use it on two connected components of $X$ (which are closed sets), so you could separate points on $X/\sim$ with open sets, but it's not clear to me , because I think that open sets could intersect with another same component. I know that totally disconnected spaces might not be hausdorff so I must think that the space is a quotient with this relation, the same goes for saying that a quotient of a hausdorff space might not be hausdorff. I don't know if using the intersection of clopen sets containing a point $x \in X$ to coincide with the connected component of $x$ in a compact space works for this. Does any of this work?","['connectedness', 'general-topology', 'compactness']"
4808068,Find Expectation of Product of Three Random Variables,"I have two normal random variables $X$ ~ $\mathcal{N}(\mu_X, \sigma^2_X)$ and $Y$ ~ $\mathcal{N}(\mu_Y, \sigma^2_Y)$ and one discrete uniform random variable $\theta$ ~Unif $(a,b)$ whose mean $\mu_\theta$ and variance $\sigma^2_\theta$ are known. To be clear, $\theta$ is uniformly distributed over a finite number of $b-a+1$ integers $a,a+1,a+2,...,b-1,b$ . I also have the following covariance matrix $$
\begin{bmatrix}
    \sigma^2_X       & \sigma_{X,Y} & 0 \\
    \sigma_{X,Y}       & \sigma^2_Y & 0 \\
    0       & 0 & \sigma^2_\theta 
\end{bmatrix}
$$ I am trying to determine $\mathbb{E} \Big[ X Y \sin2\theta \Big]$ . In other words, I am trying to determine the expectation of the product of three random variables: $X$ , $Y$ , and $\sin2\theta$ , the latter of which is a function of $\theta$ . I tried leveraging the formula for covariance as follows $$ \mathbb{E}[V_1V_2] = \sigma_{V_1,V_2}  + \mathbb{E}[V_1]\mathbb{E}[V_2] $$ but this only returns the expectation of the product of two random variables. I could possibly define $A = XY$ and try to find $\mathbb{E}[A,\sin2\theta]$ , but then I would need the covariance between $XY$ and $\sin2\theta$ , which I don't have and unfortunately I don't believe I can assume independence between $\sin2\theta$ and the product $XY$ . Any thoughts on how to proceed? Or do I simply lack the info I need? NOTE: I'm asking this question to assist in answering another question here on the math SE. Feel free to tackle it directly if you wish.","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4808139,A function with negative discontinuous derivative and many zeros,"I want to construct an example of a function $f:\mathbb{R} \to \mathbb{R}$ with the following properties. By the way, I am not sure if it can exist. $f(0) = 0$ $f'(0) < 0$ $f$ is continuous at $x=0$ $f'$ is not continuous at $x=0$ There is a sequence $x_k$ converging to $0$ such that $f(x_k)=0$ The following famous example $$
f(x) = 
\begin{cases} 
x^2 \sin(1/x) & x \neq 0 \\
0 & x=0. 
\end{cases}
$$ satisfies all the properties except $2$ . I thought that modifying it like $$
f(x) = 
\begin{cases} 
x^2 \sin(1/x) - x & x \neq 0 \\
0 & x=0. 
\end{cases}
$$ will make the derivative negative at $x=0$ but it destroys the property $5$ . How should I construct such an example? Or maybe it does not even exist?","['roots', 'analysis', 'continuity', 'calculus', 'derivatives']"
4808170,Extending a holomorphic function on $\mathbb{C}\backslash K$ to an entire function,"Consider a compact set $K\subset \mathbb{R}$ with positive measure (i.e. $\mu(K)>0$ ), and for $z\in\mathbb{C}\backslash K$ , define the holomorphic function $f$ on $\mathbb{C}\backslash K$ by \begin{equation}
f(z):=\int_K \frac{dt}{t-z}.
\end{equation} Now the question is can $f$ be extended to an entire function ? Clearly $f(z)\to 0$ as $z\to \infty$ in every direction, and I'm not familiar with entire functions of this kind, this seems to me a realy pointless question :(","['complex-analysis', 'entire-functions', 'analytic-continuation']"
4808230,Functions $f(x)$ for which the set of functions $af(x+b)$ is closed under addition,"Let $f(x)$ be a continuous function from $\mathbb{R}$ to $\mathbb{R}$ . Consider the set of functions $af(x+b)$ for $a,b \in \mathbb{R}$ . If that set is closed under addition, what can $f$ be? I suspect the possible $f$ 's are exponentials, sinusoids, constants and their products. (For example, the sum of any two sinusoids with the same period, each arbitrarily shifted and scaled, is another sinusoid with the same period - and I couldn't find any other periodic function with that property - that was my original motivation for the problem.) But I don't know how to prove there are no others. Another funny note is that $f(x)=x$ yields a set which is almost closed under addition, except its additive closure also includes nonzero constant functions, which don't have the form $a(x+b)$ .","['functional-equations', 'functions', 'real-analysis']"
4808260,Concentration inequalities for r.v. with $\beta$-exponential tails,"Let $(X_i)_{i \geq 1}$ be i.i.d. random variables such that $P(X_1 \geq t) \leq c_1e^{-c_2 t^{\beta}}$ for some $\beta > 0$ . Then clearly $\mathbb{E}[X_i] < \infty$ and the strong law of large numbers tells us that $S_n = \frac{1}{n} \sum_{i=1}^{n} X_i$ converges to $\mathbb{E}[X_1]$ almost surely. I want to obtain large deviations for $S_n$ . In particular, I want an estimate of the form $P(S_n > n\mathbb{E}[X_1] + t) \leq k_1 e^{-k_2 t^{\alpha}}$ . I know that if $\beta \geq 2$ , then $X_i$ are sub-gaussian and the tail is true with $\alpha=2$ . However, I have not been able to find anything about the case when $\beta \in (0,2)$ . I would appreciate any help.","['inequality', 'concentration-of-measure', 'probability-theory']"
4808276,Non Linear Differential Equation: Switching Exponentiation with Differentiation,"For which functions $y(x)$ can one swap the order of their derivative with their exponent? $$\left(\frac{d^my}{dx^m}\right)^n\overset{!}{=}\frac{d^m}{dx^m}\left(y^n\right), \quad\quad m,n\in\mathbb{N}^*$$ Of course there are some trivial solutions with constant functions or with $n=1$ . I also found the general solution for $m=1$ : $$\bigg(\frac{dy}{dx}\bigg)^n\overset{!}{=}\frac{d}{dx}\bigg(y^n\bigg) = n \frac{dy}{dx}y^{n-1}$$ $$\bigg(\frac{dy}{dx}\bigg)^{n-1}=ny^{n-1}$$ $$\frac{dy}{dx}= n^{\frac{1}{n-1}}y \quad\quad\Longleftrightarrow\quad\quad y = Ce^{x n^\frac{1}{n-1}} $$ But I'm already stuck with $m=2$ : $$\bigg(\frac{d^2y}{dx^2}\bigg)^n\overset{!}{=}\frac{d^2}{dx^2}\bigg(y^n\bigg) = n \frac{d}{dx}\bigg(y^{n-1}\frac{dy}{dx}\bigg) = n\bigg((n-1)y^{n-2}\bigg(\frac{dy}{dx}\bigg)^2+y^{n-1}\frac{d^2y}{dx^2}\bigg)$$ Is there a way to find a general solution for the triples $y$ , $m$ and $n$ . Or even just a few other non trivial solutions?","['derivatives', 'ordinary-differential-equations']"
4808330,Is $x^3$ really an increasing function for all intervals?,"I had an argument with my maths teacher today...
He says, along with another classmate of mine that $x^3$ is increasing for all intervals. I argue that it isn't.
If we look at conditions for increasing, decreasing and stable states of a function,
for a function $f(x)$ : function is increasing for $f'(x) > 0$ function is decreasing for $f'(x) < 0$ (as per my knowledge) function is neither increasing nor decreasing (i.e. it's stable) for $f'(x) = 0$ so for $x = 0$ , function $f(x) = x^3$ should be stable, NOT INCREASING. Hence the interval for which the function must be increasing, according to me, would be $\mathbb{R}$ - { $0$ }. This means, the function $f(x)=x^3$ should be increasing for all real values EXCEPT $x=0$ . My teacher denied, saying that we consider values of $f'(x) >= 0$ .
My response was to look at the function $f(x) = x^2$ which is increasing for interval $(0, \infty]$ and decreasing for interval $(\infty, 0]$ , without a doubt. again, I reason that this time too, $x=0$ is neither decreasing nor increasing hence it is not included in either of the intervals. My teacher, had nothing to say on this, simply telling me that these two examples are completely incomparable. [another reasoning that my classmate and my teacher gave was that $x=0$ is a single point. Thus the function has to be constant there. Since the function is increasing before and after the point $x=0$ , the function must be increasing for value $x=0$ as well. In my opinion, that is completely against the very idea of calculus. When we define the rate of change at a point, it is solely attributed to that point alone. The limit of the rate of change, as points around $x=0$ close in on 0 is 0, hence in my opinion, the function should be constant at $x=0$ , while increasing elsewhere in the set of real numbers] Am I correct?","['real-numbers', 'applications', 'calculus', 'functions', 'derivatives']"
4808337,Find a recurrence relation for the number of bit strings of length n that contain consecutive symbols that are the same,"Here is my attempt: First there is $2 \cdot 2^{n-1}$ ways if string ends with $00$ or $11$ . Second there are ways when string end with $10$ or $01$ . So it will give us $a(n-1)$ ways to solve remaining part. So the final answer would be $a(n) = 2^{n-1} + a(n-1)$ . Am I right? If it possible could you explain this in your way, because I could not completely understand what I wrote. Thanks)","['recursive-algorithms', 'discrete-mathematics', 'recursion']"
4808422,Least squares solution to underdetermined Lyapunov equation,"I need to solve an underdetermined Lyapunov equation for unknown $n\times n$ matrix $X$ . $$AX + XA = B$$ The naive method is to vectorize $x=\operatorname{vec}(X)$ and use a least squares solver on the following equation to find the least-squares solution. $$(I\otimes A + A\otimes I)x = \operatorname{vec}B$$ Experimentally I found that when $A$ and $B$ are positive semidefinite, I get the same solution by expressing $X$ in terms of $U,s$ , the eigenvectors and eigenvalues of $A$ as below ( proof ) and modifying pointwise (Hadamard) division to skip division by zero, like how pseudo-inverse implementations do it. $$X=U \left( \frac{U' BU}{s + s'} \right) U'$$ Does this appear in the literature, or does someone see a way to prove that this recovers least-squares solution? Example $$\text{A=}\left(
\begin{array}{ccc}
 8 & -8 & -8 \\
 -8 & 9 & 8 \\
 -8 & 8 & 8 \\
\end{array}
\right)$$ $$\text{B=}\left(
\begin{array}{ccc}
 5 & 5 & -5 \\
 5 & 9 & -3 \\
 -5 & -3 & 6 \\
\end{array}
\right)$$ This equation is underdetermined because $A$ and $B$ are singular, hence standard Lyapunov solver fails. However, both least squares and truncated spectral decomposition methods succeed with the same answer: $$X=\frac{1}{640}\left(
\begin{array}{ccc}
 1789 & 2928 & -1329 \\
 2928 & 4672 & -1968 \\
 -1329 & -1968 & 869 \\
\end{array}
\right)$$ Notebook","['matrix-equations', 'linear-algebra']"
4808423,Is there a function with the following properties?,"I am looking for a function $f(x,k)$ with the following properties: $$ f(0,k)=1$$ $$ f(1,k)=0$$ $$\lim_{k \to \infty}f(x,k)= \Bigg \{\begin{matrix}
1 & \text {if} & x=0\\
0 & \text {if} & x>0
\end{matrix}$$ For my project, I am currently using the function: $$g(x,k) = 1-\tanh(k \cdot x)$$ which lacks the property $g(1,k) = 0$ (unless $k$ tends to infinity). Is there such a function?","['limits', 'functions']"
4808438,Equivalence of Logarithm of Expectation of Power and Expectation of Logarithm for a Random Variable,"Does the following equality hold? Does it need any further assumptions? $$
\lim _{s \rightarrow 0} \log \left(\mathbb{E}\left[X^s\right]^{\frac{1}{s}}\right)=\mathbb{E}[\log (X)]
$$","['limits', 'probability-theory', 'probability']"
4808447,Existence of an isomorphism between $X$ and $Y$ when $g(X)=f(Y)$,"Assume we have regular real random variables $X$ and $Y$ and real-valued functions $g,f \in \mathcal{G}$ . Assume that $g(X)=f(Y)$ almost surely. I would like to know under which conditions on $\mathcal{G}$ there exists an isomorphism $h$ such that $X=h(Y)$ . There is the obvious case where $g$ and $f$ are invertible but I am interested in the case where $\mathcal{G}=\{g:\mathbb{R}^q \rightarrow \mathbb{R}^p\}$ where $p>q$ and thus $g$ and $h$ are not invertible.
I was wondering if there exists some lemma such as 5.25 in Einsiedler and Ward's Ergodic Theory with a View Towards Number Theory which guarantees that if $X=g(Z)$ and $Y=f(Z)$ there is an isomorphism between $X$ and $Y$ . I have the inverse scenario in my case.","['measure-theory', 'ergodic-theory', 'probability-theory', 'diffeomorphism']"
4808526,When exactly does the sequence of the universal coefficient theorem split?,"$\newcommand{\tor}{\operatorname{Tor}}$ It is left as an exercise in Weibel $3.6$ to conclude that: If $C$ is a complex of right $R$ -modules such that each $C_n$ and each boundary submodule $B_n$ is flat and $D$ is any complex of left $R$ -modules, then there is a short exact sequence: $$0\to\bigoplus_{p+q=n}H_p(C)\otimes_R H_q(D)\to H_n(C\otimes_R D)\to\bigoplus_{p+q=n-1}\tor^R_1(H_p(C),H_q(D))\to0$$ If $R$ is nice (he takes $R=\Bbb Z$ ) and $C_n$ is free for all $n$ then this sequence splits . I can prove there is this exact sequence. I am thoroughly unconvinced that it should split in general; at least, I'm unconvinced that the same arguments he uses (somewhat carelessly?) in the special case of $D$ being concentrated in degree zero will work in general. About "" $R$ is nice"", for the case where $D$ is just one module it suffices to consider $R$ a hereditary ring; then the sequence will split. What's my gripe? Well, the standard arguments conclude that $Z_\bullet$ and $B_\bullet$ (cycles and boundaries of $C$ ) are projective and flat and we say $0\to Z_n\to C_n\to B_{n-1}\to0$ splits, with some splitting $r:C_n\twoheadrightarrow Z_n$ . We run some argument and deduce the sequence exists, where the first map has an easy description. However, to create a projection map $H_n(C\otimes_R D)\to\bigoplus_{p+q=n}H_p(C)\otimes_R H_q(D)$ is hard, since the cycle subgroup of $C\otimes_R D$ is mysterious. Moreover , it should not be true that $Z_p\otimes D_q$ is a subgroup of the $n$ -cycles of $C\otimes_R D$ ; we can only expect this to be true if, say, the differentials of $D$ are trivial (as they are in the $D$ -is-a-single-module special case). But the only thing I can know how to do is consider the map $r\otimes1:C_p\otimes_R D_q\twoheadrightarrow Z_p\otimes_R D_q$ and there is absolutely no reason for this map to induce a map on homology. There is also no reason to expect we can magically extend this by some $Z_p\otimes_R D_q\overset{??}{\twoheadrightarrow}H_p(C)\otimes_R H_q(D)$ . I think that there is perhaps an error and we should assume something of $D$ too, if we expect this to split - such as assuming $D_q$ is free for every $q$ , because then there is a splitting $D_q\to Z(D)_q$ and we can make everything work using a generalised version of the case $D$ is just one module, in particular using the fact $Z(C)_p\otimes_R Z(D)_q$ is a subgroup of $Z(C\otimes_R D)_n$ . Note that in topology this hypothesis holds because $C,D$ will be the free singular complexes. Am I right, or am I just overthinking this? If I'm wrong, I would really appreciate some help constructing the splitting $H_n(C\otimes_R D)\twoheadrightarrow\bigoplus_{p+q=n}H_p(C)\otimes_R H_q(D)$ .","['homological-algebra', 'abstract-algebra', 'homology-cohomology']"
4808548,Is there a named theorem for this property?,"Point $P$ is outside circle $\odot{O}$ , and $A,B$ are tagent points from $P$ to $\odot{O}$ . $C,D$ are two points on $\odot{O}$ that are colinear with $P$ . Then we have $\dfrac{AC}{AD}=\dfrac{BC}{BD}$ . I wonder if there is a named theorem for this. It can be easily proved as following: $$
\begin{multline}\nonumber
\shoveleft \triangle{PAC}\sim\triangle{PDA}\implies \dfrac{AC}{AD}=\dfrac{PA}{PD}\\
\shoveleft \triangle{PBC}\sim\triangle{PDB}\implies \dfrac{BC}{BD}=\dfrac{PB}{PD}=\dfrac{PA}{PD}\\
\shoveleft \implies \dfrac{AC}{AD}=\dfrac{BC}{BD}
\end{multline}
$$",['geometry']
4808589,L'Hopital's Rule $\infty - \infty$ in numerator of rational function,"I am currently taking a course on Calculus 1. We just recently went over L'Hopital's Rule, which states the following: When evaluating $\lim\limits_{x \to a} \frac{f(x)}{g(x)}$ , if direct substitution yields 0/0 or $\pm\frac{\infty}{\infty}$ , then the limit of our original expression is equal to $\lim\limits_{x \to a} \frac{f'(x)}{g'(x)}$ . However, I recently came across the following problem: $\lim\limits_{x \to \infty} \frac{5x - 2x^2}{e^x+3}$ While the solution key indicates that we may simply use L'Hopital's Rule, I do not understand why. Moreover, if we utilize direct substitution, we will be left with $\frac{\infty - \infty}{\infty}$ . Presently, this is not equivalent to the indeterminate form $\pm\frac{\infty}{\infty}$ . Is there some sort of manipulation we can do to show that $\frac{\infty - \infty}{\infty}$ is equivalent to $\pm\frac{\infty}{\infty}$ ?","['limits', 'calculus']"
4808627,"If $a_1=1$ and $a_{n+1}=3-\frac{1}{a_n}$, show that the sequence converges and find the limit.","If $a_1=1$ and $a_{n+1}=3-\frac{1}{a_n}$ , $\forall n\in N$ , then the sequence converges, so find its limit. So I list the elements of the sequence as follows: $$1, 2, \frac{5}{2}, \frac{13}{5}, \frac{34}{13}, \frac{89}{34},\dots$$ And we see that this a increasing sequence, because $1\lt 2\lt 2.5\lt 2.6\lt 2.61\lt 2.6176$ , and we see too that the sequence is bounded, by 1 as an inferior bound, but we want to know its limit, wich we cannot clearly see from the sequence above. Then, to give an expression: $$a_{n+1}=3-\frac{1}{3-\frac{1}{a_{n-1}}}$$ And if we repeat the process of substitution we have: $$a_{n+1}=3-\frac{1}{3-\frac{1}{3-\frac{1}{a_{n-2}}}}$$ So, finding the limit: $$\lim\limits_{n\to \infty} 3-\frac{1}{\sum_{k=1}^{n-1}(3-\frac{1}{a_k})}$$ $$3- \lim\limits_{n\to \infty} \frac{1}{\sum_{k=1}^{\infty}(3-\frac{1}{a_k})}$$ $$3- \lim\limits_{n\to \infty} \frac{1}{\sum_{k=1}^{\infty}(3)}$$ $$3- \lim\limits_{n\to \infty} \frac{1}{3}= 3-\frac{1}{3}=\frac{8}{3}\approx 2.\overline6 7$$ Is this the right way to handle this? Let me know if I did something wrong or if I was unclear, thank you.","['calculus', 'solution-verification', 'convergence-divergence', 'sequences-and-series']"
4808651,Does anyone know why this is happening with 1/7? [duplicate],"This question already has answers here : Recurring decimal expansion of $\frac17$ (4 answers) Is there a reason why $\frac{1}{7}$ as a decimal so perfectly seems to follow multiples of $7$? (1 answer) Doubling sequences of the cyclic decimal parts of the fraction numbers (1 answer) Closed 8 months ago . (I've never posted on here before, so apologies for any formatting problems) I had always noticed this property of the decimal form of $1/7$ ( $0.14285714...$ ) where the decimals went $14$ , then $28$ , then $57$ , which is almost exactly the formula $f(x)=7*2^x$ , starting with $f(1)$ - except for $57$ , which is $1$ more than $56$ . However, if you add the next number in the series, $f(4)=112$ , to the tail end of $56$ , then you get $57$ and are left with the second $1$ and the $2$ to add to the end of the decimal, which is exactly where the $71$ lands in the $.14285714...$ string of digits. If you keep doing this with the next number in the formula, $f(5)=224$ , and add to the end of $112$ , you get $2+2$ , which accounts the $4$ and the $2$ , and so on. I continued this pattern and found that this process of adding digits together works all the way up to the $45th$ digit , which is where I stopped. Is this simply a coincidence or is there a reason for this to occur? And if there really is something behind this (which I'm sure there is, does anyone know why this happens? I would appreciate any attempt at solving this. Work for said question (apologies if it's messy, I did this work in a cramped notebook lol) I know this seems very confusing, but if anyone needs me to clarify, I will do my best.","['elementary-number-theory', 'decimal-expansion', 'sequences-and-series']"
4808722,"Cohomology $H(X^{(p)}, \mathbb Q_{\ell})$ of the Frobenius twist of a variety over a finite field","Let $X$ be a quasi-projective variety over a finite field $\mathbb F_q$ where $q$ is a power of a prime $p$ . Let $\ell$ be a prime number different from $p$ . We have the relative Frobenius morphism $$\mathcal F: X \to X^{(p)},$$ as defined for instance in the stack project here . How do the cohomology groups $H^i(X^{(p)} \times \overline{\mathbb F_p}, \mathbb Q_{\ell})$ compare with $H^i(X\times \overline{\mathbb F_p}, \mathbb Q_{\ell})$ ?","['etale-cohomology', 'finite-fields', 'algebraic-geometry']"
4808809,How to solve $\displaystyle\lim_{n\to\infty}\int_0^3\underbrace{\sin(\frac{\pi}{3}\sin(\frac{\pi}{3}...\sin(\frac{\pi}{3} x)...))}_\text{n sines}dx$?,"I saw this problem: Find $\lim\limits_{n \to \infty }\int_0^3 \underbrace{\sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \sin \left(\frac{\pi}{3}  \dots \sin\left(\frac{\pi}{3} x\right) \dots \right)\right)\right)}_\text{n times of sines}dx$ I tried to solve it, but I have no idea if my solution is correct or wrong. My attempt: $$I_n:=\int_0^3 \underbrace{\sin\left(\frac\pi3 \sin\left(\frac\pi3\dots(\frac{\pi}{3}\sin\left(\frac\pi3x\right) \dots \right)\right)}_\text{n sines} dx        $$ Let $x=\frac{3}{\pi }t$ , then $$I_n =\frac{3}{\pi}\int_0^\pi \underbrace{\sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \dots \frac{\pi}{3}\sin\left(\frac{\pi}{3}\sin\left(t\right)\right) \dots \right)\right)\right)}_\text{n sines}dt$$ $$=\frac{6}{\pi}\int_0^{\frac{\pi}{2}}\underbrace{\sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \dots \frac{\pi}{3}\sin\left(\sin\frac{\pi}{3}\left(t\right)\right) \dots \right)\right)\right)}_\text{n sines}dt$$ For all $t \in (0, \frac{\pi}{2})$ define $f_0(t) =t$ and $f_n(t)= \sin (\frac{\pi}{3}f_{n-1}(t))$ .
If $\sin(t)= \frac{1}{2}$ , then $f_n(t) = \frac{1}{2} \ \forall n \in \mathbb{N}$ .  If $\sin(t) > \frac{1}{2}= \frac{1}{2} +\varepsilon $ for some $\varepsilon >0 $ , then $\sin(\frac{\pi}{3} \sin(t))= \sin (\frac{\pi}{6}+\frac{\pi}{3} \varepsilon) = \frac{1}{2} \cos(\frac{\pi}{3} \varepsilon) +\frac{\sqrt{3}}{2} \sin(\frac{\pi}{3}  \varepsilon) < \frac{1}{2}+\frac{\pi \varepsilon }{2\sqrt{3}} < \frac{1}{2}+ \varepsilon $ which means that the sequence is monotone decreasing bounded below by $\frac{1}{2}$ (This is the same way to prove that the sequence of all $t$ st $\sin(t) <\frac{1}{2}$ is increasing and bounded above by $\frac{1}{2}$ .), So by monotone convergence theorem the limit exist and $f(x)=\lim\limits_{n \to \infty} f_n(x)$ since $f_n(x) =\sin(\frac{\pi}{3} f_{n-1}(x)) $ $f(x) =\sin(\frac{\pi}{3} f(x)) $ then  it is easy to guess that $f(x)= \frac{1}{2}$ the limit is $\frac{1}{2}$ then $f_n(t) \to \frac{1}{2}\  \forall t $ the last part is to prove that the sequence $f_n$ is uniformly convergent and this can be shown by Dini’s Theorem:- Suppose that $f_n$ is a monotone sequence of continuous functions
on $I := [a, b]$ that converges on $I$ to a continuous function $f$ . Then the convergence of the sequence is uniform. The sequences $f_n(x)$ is monotone increasing when $x\in (0, \frac{\pi}{6})$ , and monotone increasing if $x\in (\frac{\pi}{6},\frac{\pi}{2})$ , then $$ \lim\limits_{n \to \infty} I_n =\frac{6}{\pi} \lim\limits_{n \to \infty} \int_0 ^{\frac{\pi}{2}} f_n(x)dx= \frac{6}{\pi}
 \lim\limits_{n \to \infty}\left(  \int_0 ^{\frac{\pi}{6}} f_n(x)dx+\int_{\frac{\pi}{6}} ^{\frac{\pi}{2}} f_n(x)dx \right) $$ $$= \frac{6}{\pi} 
 \left(  \int_0 ^{\frac{\pi}{6}} \lim\limits_{n \to \infty}f_n(x)dx+\int_{\frac{\pi}{6}} ^{\frac{\pi}{2}} \lim\limits_{n \to \infty}f_n(x)dx \right) =\frac{6}{\pi} 
 \left(\frac{1}{2}\left( \frac{\pi}{2} \right) \right)=\frac{3}{2}$$ Is my solution correct? If it is not, where did I make a mistake and how can I solve this integral? What are other ways to solve this?","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'limits']"
4808856,"Is there a polynomial of degree at most 99 whose values at 1, 2,... , 1000 alternate between 0 and 1?","This is a question from Problem set 2 of MIT’s 18.700 Linear algebra course . We have the following setup. $V$ is a vector space of real polynomials with degree at most $99$ . Also, $T:V\to \mathbb{R}^{1000}$ is a linear map such that $$ T(p) = (p(1), \dots, p(1000)) $$ Part (a) asks you what $\dim\ker T$ is. I thought the only possible polynomial $p$ with $\deg p \leq 99$ such that $p(1)=\dots=p(1000)=0$ is the zero polynomial (i.e. $p=0$ ). The reason is because the factor theorem tells you if $p$ is a polynomial for which $p(1)=\dots=p(1000)$ , it must have $(x-1), \dots, (x-1000)$ as its factors. So, $\deg p \geq1000$ , which is contradictory to the assumption that $\deg p \leq 99$ . Therefore, $\dim\ker T = 0$ . Part (b) asks you what $\dim \text{range }T$ is. I used the theorem $\dim V = \dim\ker T + \dim \text{range }T$ for linear maps, and concluded $\dim \text{range }T = 100$ ( $\because \dim V = 100)$ . Part (c) is where I got stuck. It asks whether the vector $(0,1,0,1,\dots,0,1)$ is in the range of $T$ . I have one solution in mind, but it seems like it’s completely irrelevant to linear algebra, which makes me think this is not the solution that the instructor intended. My argument went similarly as in part (a). If there is a polynomial $p$ with $\deg p \leq 99$ such that $T(p) = (0, 1, 0, 1, \dots, 0, 1)$ , it means that $p$ has at least $500$ roots ( $x\in\{1, 3, 5, \dots, 999\}$ ), which implies $\deg p \geq 500$ (by the factor theorem), which again is a contradiction. I think my question can be phrased as: Can you deduce $(0,1,0,1,\dots,0,1)\notin \text{range }T$ just from the fact that (without referring to the factor theorem) $T:V\to \mathbb{R}^{1000}$ is a linear map $\dim V = 100$ $\dim \ker T = 0$ $\dim \text{range } T = 100$ I think there should be some more linear algebra-style of proof. I’d appreciate any comment or help! Thank you.","['vector-spaces', 'solution-verification', 'linear-algebra', 'polynomials', 'linear-transformations']"
4808868,Show that $\sum_{n=1}^{+\infty}\frac{1}{(n\cdot\sinh(n\pi))^2} = \frac{2}{3}\sum_{n=1}^{+\infty}\frac{(-1)^{n-1}}{(2n-1)^2} - \frac{11\pi^2}{180}$,"What I do so far \begin{align*}
\text{Show that} \quad &\sum_{n=1}^{+\infty}\frac{1}{(n\cdot\sinh(n\pi))^2} = \frac{2}{3}\sum_{n=1}^{+\infty}\frac{(-1)^{n-1}}{(2n-1)^2} - \frac{11\pi^2}{180} \\
\text{Lemma 1 } &\sum_{n = - \infty }^\infty \frac{1}{{z + n}} = \frac{\pi }{{\tan (\pi z)}} \\
\text{Lemma 2 } &\frac{1}{{\sinh^2(\pi z)}} = \frac{1}{{\pi^2 z^2}} + \frac{4z^2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{(z^2 + k^2)^2}} - \frac{2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{z^2 + k^2}} \\
&\text{Because:} \nonumber \\
&\frac{\pi }{{\tan (\pi z)}} = \sum_{k = - \infty }^\infty \frac{1}{{z + k}} \Rightarrow \left( \frac{\pi }{{\tan (\pi z)}} \right)' = -\sum_{k = - \infty }^\infty \frac{1}{{(z + k)^2}} \nonumber \\
&\Rightarrow \boxed{\frac{\pi^2}{\sin^2(\pi z)}} = \sum_{k = - \infty }^\infty \frac{1}{{(z + k)^2}} \Rightarrow \nonumber \\
&\Rightarrow \frac{\pi^2}{\sin^2(\pi iz)} = \sum_{k = - \infty }^\infty \frac{1}{{(iz + k)^2}} \Rightarrow \frac{\pi^2}{\sinh^2(\pi z)} = \sum_{k = - \infty }^\infty \frac{1}{{(iz + k)^2}} \Rightarrow \boxed{\frac{\pi^2}{\sinh^2(\pi z)} = \sum_{k = - \infty }^\infty \frac{1}{{(z + ik)^2}} = \sum_{k = - \infty }^\infty \frac{1}{{(z - ik)^2}}} \nonumber \\
&\text{then} \nonumber \\
&\frac{1}{{\sinh^2(\pi z)}} = \frac{1}{{2\pi^2}}\sum_{k = - \infty }^\infty \left( \frac{1}{{(z + ik)^2}} + \frac{1}{{(z - ik)^2}} \right) \nonumber \\
&= \frac{1}{{\pi^2}}\sum_{k = - \infty }^\infty \frac{z^2 - k^2}{{(z^2 + k^2)^2}} = \frac{1}{{\pi^2}}\sum_{k = - \infty }^\infty \frac{2z^2 - (z^2 + k^2)}{{(z^2 + k^2)^2}} \nonumber \\
&= \frac{1}{{\pi^2}} \left( 2z^2\sum_{k = - \infty }^\infty \frac{1}{{(z^2 + k^2)^2}} - \frac{1}{{\pi^2}}\sum_{k = - \infty }^\infty \frac{1}{{z^2 + k^2}} \right) \nonumber \\
&\Rightarrow \boxed{\frac{1}{{\sinh^2(\pi z)}} = \frac{1}{{\pi^2 z^2}} + \frac{4z^2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{(z^2 + k^2)^2}} - \frac{2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{z^2 + k^2}}}
\end{align*} We replace $z$ with $n$ and sum, so it results: \begin{align*}
&\sum_{n=1}^\infty \frac{1}{n^2\sinh^2(\pi n)} \\
&= \frac{1}{\pi^2}\sum_{n=1}^\infty \frac{1}{n^4} + \frac{4}{\pi^2}\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} - \frac{2}{\pi^2}\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{n^2(n^2+k^2)}
\end{align*} But it is known that $$
\frac{1}{\pi^2}\sum_{n=1}^\infty \frac{1}{n^4} = \frac{1}{\pi^2}\cdot \zeta(4) = \frac{\pi^2}{90}
$$ for the sum $$
\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{n^2(n^2+k^2)}
$$ we have: \begin{align*}
S &= \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{n^2(n^2+k^2)} \\
&= \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{k^2}\left(\frac{1}{n^2} - \frac{1}{n^2+k^2}\right) \\
&= \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{k^2}\frac{1}{n^2} - \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{k^2(n^2+k^2)} \\
&\Rightarrow S = \left(\sum_{n=1}^\infty \frac{1}{n^2}\right)\left(\sum_{k=1}^\infty \frac{1}{k^2}\right) - S \\
&\Rightarrow 2S = \left(\frac{\pi^2}{6}\right)^2 \\
&\Rightarrow S = \frac{\pi^4}{72}
\end{align*} So finally $$
\boxed{\sum_{n=1}^\infty \frac{1}{n^2\sinh^2(\pi n)} = \frac{4}{\pi^2}\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} - \frac{\pi^2}{60}}
$$ The final double sum is related to functions $\zeta(2)$ & $\zeta(4)$ that are missing all add-ins of natural ones that are not written as sum of two squares. From Fermat's theorem (proved by Euler) we know that if some natural number has a prime factor of form $p = 4k + 3$ raised to odd force, then it is not written as the sum of two squares. I read that $$
\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} = \zeta(2) \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \zeta(4)
$$ somewhere, but I have no proof of this, nor have I been able to discover any. Given the above: \begin{align*}
&\sum_{n=1}^\infty \frac{1}{n^2\sinh^2(\pi n)} \\
&= \frac{4}{\pi^2}\left(\zeta(2) \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \zeta(4)\right) - \frac{\pi^2}{60} \\
&= \frac{4}{\pi^2}\left(\frac{\pi^2}{6} \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \frac{\pi^4}{90}\right) - \frac{\pi^2}{60} \\
&= \frac{2}{3}\sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \left(\frac{2\pi^2}{45} + \frac{\pi^2}{60}\right) \\
&= \frac{2}{3}\sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \frac{11\pi^2}{180}
\end{align*} I would be very interested to see a proof of the relationship $$
\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} = \zeta(2) \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \zeta(4)
$$ which is true (tested with handmade program in Visual Basic).","['calculus', 'summation', 'sequences-and-series']"
4808919,When does a monoid admit a ring structure?,"In this question, it is asked when an abelian group admits a ring structure; that is, if $(G,+)$ is an abelian group, then under what conditions is there a binary operation $\cdot$ on $G$ such that $(G,+,\cdot)$ is a ring. (For the purposes of this question, rings are unital.) I am interested in the ""dual"" question of when a monoid admits a ring structure; that is, if $(M,\cdot)$ is a monoid, then under what conditions is there a binary operation $+$ on $M$ such that $(M,+,\cdot)$ is a ring?","['monoid', 'ring-theory', 'group-theory', 'abstract-algebra']"
4808934,"Analytic extension of $\text{Li}_0^{(1,0)}(z):=-\displaystyle\sum_{n=1}^{\infty}\ln(n)z^{n}$ for $|z|>1$","I would like to extend the domain of the following function: $$\text{Li}_0^{(1,0)}(z):=-\sum_{n=1}^{\infty}\ln(n)z^{n}\qquad\text{where }|z|<1$$ The part in red is the series, the part in green is a hypothetical extension I'll start by saying that I don't know any techniques for doing something like this, so any advice (links, names of theorems or similar) is useful. I could only find the value in $x=-1$ $$\text{Li}_0^{(1,0)}(-1)=\lim_{s\to 0}\left[ \sum_{n=1}^{\infty} \frac{(-1)^n\ln(n)}{n^s}\right] = -\frac{1}{2}\ln\left(\frac{\pi}{2}\right)$$ I've searched for possible representations on Wolfram , but none include the one with $\nu=0$ Unfortunately I found the integral representation of polylogarithms $\text{Li}_{\nu}(z)$ only for $\Re(\nu)>0$ From a computational point of view a series is heavier than an integral, so I was wondering if it was possible to represent $$\text{Li}_{0}^{(1,0)}(z):=\left.\frac{\partial}{\partial\nu}\text{Li}_{\nu}(z)\right|_{\nu=0}$$ without using infinite sums or products. the only formula I've found that include zero is this: $$\text{Li}_{\nu}(z) = -\frac{i}{2 π} \int_{-i ∞ + γ}^{i ∞ + γ} (-t)^{1 - \nu} (-z)^{-t} Γ(-t) Γ(t) \mathrm{d}t$$ $$\text{ for }(-1<γ<0\text{ and }z\neq 0\text{ and }(\text{arg}(-z)<π\text{ or }\Re(\nu)>1))$$ So $$\text{Li}^{(1,0)}_{\nu}(z) = \frac{i}{2 π} \int_{-i ∞ + γ}^{i ∞ + γ} (-t)^{1 - \nu}\ln(-t) (-z)^{-t} Γ(-t) Γ(t) \mathrm{d}t$$ $$\text{Li}^{(1,0)}_{0}(z) = -\frac{i}{2 π} \int_{-i ∞ + γ}^{i ∞ + γ} \ln(-t) (-z)^{-t} Γ(-t) Γ(t+1) \mathrm{d}t$$ $$\text{Li}^{(1,0)}_{0}(z) = \frac{i}{2} \int_{-i ∞ + γ}^{i ∞ + γ} \ln(-t) (-z)^{-t} \csc(\pi t) \mathrm{d}t$$ But with this function I would also like to work with graphs and I don't use programs that know how to manage this type of representations with complex numbers. Would anyone be able to suggest an alternative method to represent it? Update 1 The graph is something like this: Update 2 I obtained the following property: $$\Re\left[\text{Li}_0^{(1,0)}\left(e^{2\pi i x}\right)\right]=-\frac{1}{2}\left(\gamma+\ln\left(2\pi\right)+\frac{\psi\left(x\right)+\psi\left(1-x\right)}{2}\right)\qquad\text{for } 0<x<1$$ Hence also the following formulas for $n\geq 1$ : $$\begin{align}\Re\left[\text{Li}_{-2n}^{(1,0)}\left(e^{2\pi i x}\right)\right]&=&\frac{(-1)^{n-1}}{(2\pi)^{2n}}\frac{\psi^{(2n)}\left(x\right)+\psi^{(2n)}\left(1-x\right)}{4}\\\Im\left[\text{Li}_{-(2n-1)}^{(1,0)}\left(e^{2\pi i x}\right)\right]&=&\frac{(-1)^{n-1}}{(2\pi)^{2n-1}}\frac{\psi^{(2n-1)}\left(x\right)-\psi^{(2n-1)}\left(1-x\right)}{4}\end{align}$$","['special-functions', 'analytic-continuation', 'functions', 'sequences-and-series', 'analytic-functions']"
4808974,$\frac{x_1+x_2+\cdots+x_n}{n} \to a$ implies $\frac{x_1^p+x_2^p+\cdots+x_n^p}{n^p} \to 0$ for $p > 1$,"I'm encountering a mathematical analysis problem involving limits. Problem. Given a non-negative sequence $\{x_n\}$ satisfying $$\lim\limits_{n\to \infty}\frac{x_1+x_2+\cdots+x_n}{n}=a,$$ where $|a|<+\infty$ . Given a real number $p>1$ . Prove that $$\lim\limits_{n\to \infty}\frac{x_1^p+x_2^p+\cdots+x_n^p}{n^p}=0.$$ My Approach. I initially attempted to use Stolz's theorem to prove this problem. However, it became apparent that the converse of Stolz's theorem does not necessarily hold. Subsequently, I tried employing the squeeze theorem, but I couldn't find a suitable bounding inequality. I noticed that the form of this inequality is quite similar to the discrete Hardy's inequality. (Hardy's Inequality) Given a non-negative sequence $\{x_n\}$ and a real number $p>1$ , then $$\sum_{k=1}^{n}\left(\frac{x_1+x_2+\cdots+x_k}{k}\right)^p<\left(\frac{p}{p-1}\right)^p\sum_{k=1}^{n}a_k^p.$$ It is disappointing that the inequality here behaves in the opposite direction compared to the original problem. However, I have a faint intuition that the order of $\sum\limits_{k=1}^{n}a_k^p$ is around $O(n)$ . Can anyone help me? I would be very grateful.","['limits', 'analysis', 'sequences-and-series']"
4808981,Total curvature is area of image of Gauss map,"I am trying to solve the following exercise from the book Differential Geometry by Loring W. Tu: 5.4 Total curvature The total curvature of a smooth oriented surface $M$ in $\mathbb{R}^3$ is defined to be the integral $\int_M K$ if it exists, of the Gaussian curvature $K$ . Prove that the total curvature of $M$ is, up to sign, the area of the image of the Gauss map: $$\int_M K = \text{Area of} \ \nu(M).$$ I'm pretty sure that surfaces are assumed to be connected, otherwise this theorem is clearly false. (Consider two disjoint spheres in $\mathbb{R}^3$ with outward unit normal.) Even then I am not sure that this is true as stated. I know how to prove it in the case when $\nu$ is injective on the set of points where $K$ is non-zero. In this case the restriction of $\nu$ to this set is actually a diffeomorphism onto its image and if $dV_g$ and $dV$ are the volume forms of $M$ and $\mathbb{S}^2$ respectively, then the pullback of $dV$ with $\nu$ is just $KdV_g$ . My idea to solve the general case was to introduce a partition of unity with respect to an open cover such that on each element of the cover $\nu$ is a diffeomorphism onto its image, but I run into trouble with this approach because of the possible non-injectivity of $\nu$ . In fact when I brought this up with one of my professors, we also didn't know how to proceed and in turn tried to consider some examples. This discussion culminated in what we think is a counterexample which I will now describe. Consider the following subsets of $\mathbb{R}^3$ : $A=$ { $(x,y,z)\in \mathbb{R}^3 \vert x^2+y^2 \geq 2, z=0$ } and $B=$ { $(x,y,z)\in \mathbb{R}^3 \vert x^2+y^2+(z-1)^2 = 1, z \geq 1$ }. So $A$ is the plane $z=0$ with the open disk of radius $2$ removed and $B$ is the upper hemisphere of a unit sphere around the point $(0,0,1)$ . We can then imagine connecting these two in such a way that we get a smooth surface embedded in $\mathbb{R}^3$ . The image of the Gauss map of this surface clearly has positive volume no matter what unit normal vector field we pick, since it either contains the whole upper hemisphere or the whole lower hemisphere of $\mathbb{S}^2$ . Now consider the circle with radius $2$ in the plane $z=0$ parameterized anti-clockwise with unit speed such that it winds around only once and denote this parameterization with $\gamma$ . If we denote with $\Omega$ the surface it encloses and denote with $\kappa_N$ it's curvature with respect to the inward pointing normal relative to the plane $z=0$ then the Gauss-Bonnet formula (John M. Lee - Introduction to Riemannian Manifolds theorem 9.3) yields: $$\int_{\Omega}KdA + \int_{\gamma} \kappa_{N} ds = 2\pi.$$ And since the second integral is equal to $2\pi$ we get $\int_{\Omega}KdA=0$ , so this integral can't be equal (up to sign) to the area of the image of the Gauss map which is positive. This result is also independent on how we connected $A$ and $B$ to a smooth connected surface in $\mathbb{R}^3$ . I think the problem is that no matter how we connect $A$ and $B$ we won't be able to do it in a way that would make the Gauss map injective. The only answer on here that asks a similar question is this , but as I said, I understand this case. I want to know if the counterexample works and if it does are there even easier counterxamples? If the claim is actually true as stated, then I would like to see an argument why it is true. EDIT: It has been confirmed in the comments that this exercise is wrong as written and I have also located an errata for Tu's book on his website which adds that the Gauss map should be injective on points where $K\neq 0$ and that $K$ should be either non-negative or non-positive everywhere. This is so that the Gauss map is either orientation-preserving or orientation-reversing on the set where $K\neq 0$ . @TedShifrin also provided an easier counterexample: the torus in $\mathbb{R}^3$ .","['surfaces', 'riemannian-geometry', 'geometry', 'curvature', 'differential-geometry']"
4809085,"$X_i\sim \mathrm{UNIF}(0,\theta)$. Show that $S=X_{n:n}$ is sufficient for $\theta$ by the factorization criterion.","Consider a random sample from a uniform distribution $X_i\sim \mathrm{UNIF}(0,\theta)$ , where $\theta$ is unknown. Show that $S=X_{n:n}$ is sufficient for $\theta$ by the factorization criterion. Factorization Criterion : if $X_1,...,X_n$ have joint pdf $f(x_1,...,x_n;\theta)$ and if $S=(S_1,...,S_k)$ , then $S_1,...,S_k$ are jointly sufficient for $\theta$ if and only if $f(x_1,...,x_n;\theta)=g(s;\theta)h(x_1,...x_n)$ based on the questions, it is known that $X_i\sim \mathrm{UNIF}(0,\theta)$ so the pdf is $f(x) = 1/\theta$ , $0<=x<=\theta$ and the CDF is $F(x) = x/\theta$ so, the joint pdf is $f(x_1,...,x_n;\theta)=f(x_1;\theta)...f(x_n;\theta)$ $=(1/\theta)^n$ $=1/\theta^n$ $=g(x_{n:n};\theta)h(x_1,...,x_n)$ where $=g(x_{n:n};\theta) =  g(s;\theta) =1/\theta^n$ if $s<\theta$ and zero otherwise, and $h(x_1,...,x_n) = 1$ if $0<x_{1:n}$ and zero otherwise. So, $S=X_{n:n}$ is sufficient for $\theta$ But I don't understand why $g(x_{n:n};\theta) =  g(s;\theta) =1/\theta^n$ ?","['statistics', 'uniform-distribution', 'order-statistics', 'sufficient-statistics']"
4809127,Hexagon Area Question,"Triangle ABC has AB = 15, BC = 13, and AC = 14. Let O be the orthocenter of this triangle, and let the reflections from the orthocenter across sides AB, BC, and AC be points D, E, and F, respectively. Find the area of ADBECF. I was able to show that this hexagon was concyclic, but I don't know the best way to proceed from there.",['geometry']
4809160,Different Approaches for Proving Kantorovich Inequality,"Here is a statement of the famous Kantorovich inequality. Thoerem (Kantorovich). Let $A$ be a $n\times n$ symmetric and positive matrix. Furthermore, assume that its eigenvalues are $0 < \lambda_1 \leq \dots \leq \lambda_n$ . Then, the following inequality holds for all $\mathbf{x}\in\mathbb{R}^n$ \begin{equation}
\frac{(\mathbf{x}^{\top}A\mathbf{x})(\mathbf{x}^{\top}A^{-1}\mathbf{x})}{(\mathbf{x}^{\top}\mathbf{x})^2} 
\leq \frac{1}{4}\frac{(\lambda_1+\lambda_n)^2}{\lambda_1\lambda_n}
= \frac{1}{4}\Bigg(\sqrt{\frac{\lambda_1}{\lambda_n}}+\sqrt{\frac{\lambda_n}{\lambda_1}}\Bigg)^2.
\end{equation} There are a variety of proofs for this inequality. My aim for asking this question is three fold. First, to gather a list of all nice proofs about this inequality. Second, to see if a proof with constrained optimization techniques is possible. Third, to know how Kantorovich thought about the problem. Here are the main questions. Questions What are different approaches (excluding those mentioned below) for proving Kantorovich inequality? Can it be proved via constrained optimization techniques, continuing what I described below? How did Kantorovich prove it himself? Different Approaches This is an elegant and beautiful proof based on probability techniques. This is another proof by simple and clever algebra. A Constrained Optimization Way However, I am wondering if it can be proved via the most naive idea that comes to mind. Indeed, by maximizing the left hand side of the inequality! For this purpose, we can rewrite the left hand side by introducing $\mathbf{y} = \frac{\mathbf{x}}{\lVert\mathbf{x}\rVert}$ as below \begin{equation}
f(\mathbf{y}) = (\mathbf{y}^{\top}A\mathbf{y})(\mathbf{y}^{\top}A^{-1}\mathbf{y}).
\end{equation} Now, it seems natural to maximize $\phi(\mathbf{y})$ subject to the constraint $\mathbf{y}^{\top}\mathbf{y} = 1$ . To make the problem even simpler, one can use the spectral decomposition $A=Q^{\top}\Lambda Q$ to write $\phi(\mathbf{y})$ as \begin{equation}
g(\mathbf{z}) = \big(\sum_{i=1}^{n} \lambda_i z_i^2\big) \big(\sum_{i=1}^{n} \frac{1}{\lambda_i} z_i^2\big),
\end{equation} where $\mathbf{z} = Q \mathbf{y}$ . Finally, let $\xi_i = z_i^2$ to arrive at \begin{equation}
\phi(\boldsymbol{\xi}) = \big(\sum_{i=1}^{n} \lambda_i \xi_i\big) \big(\sum_{i=1}^{n} \frac{1}{\lambda_i} \xi_i\big)
= \sum_{i=1}^{n}\sum_{j=1}^{n} \frac{\lambda_i}{\lambda_j}\xi_i\xi_j
= \boldsymbol{\xi}B\boldsymbol{\xi},
\end{equation} with the constraints \begin{equation}
\sum_{i=1}^{n}\xi_i = 1, \qquad \xi_i \ge 0.
\end{equation} As we are usually fond of symmetric matrices we can replace $B$ by $\frac{1}{2}(B + B^{\top})$ because we know that $B = \frac{1}{2} (B + B^{\top}) + \frac{1}{2}(B - B^{\top})$ and $\frac{1}{2}\boldsymbol{\xi}(B - B^{\top})\boldsymbol{\xi} = 0.$ Consequently, $f$ can be rewritten as \begin{equation}
\phi(\boldsymbol{\xi})
= \frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n} \Bigg(\frac{\lambda_i}{\lambda_j} + \frac{\lambda_j}{\lambda_i}\Bigg)\xi_i\xi_j
= \frac{1}{2}\boldsymbol{\xi}H\boldsymbol{\xi}.
\end{equation} Can we find the maximizer of $\phi(\boldsymbol{\xi})$ subject to the aforementioned constraints via constrained optimization techniques?","['multivariable-calculus', 'linear-algebra', 'optimization', 'inequality', 'constraints']"
4809179,A textbook with a lot of intuitive explanations in advanced probability theory,"I am studying Durrett's Advanced Probability Theory, 5th ed. However, I find it very helpful to have more intuitive explanations like this . May I ask if there a textbook or online course or online video or any other resources like this?","['book-recommendation', 'reference-request', 'education', 'intuition', 'probability']"
4809195,What is $\lim_{x\to\infty}\frac{\int_{0}^{x}\cos\{t-\cos t\}dt}{x}$?,"I want to find a closed form for the average value of $\cos\{t-\cos t\}$ where $\{n\}$ denotes the fractional part of $n$ . I do not have experience finding an average value over an infinite domain but I assume it would be something like this: $$\lim_{x\to\infty}\frac{\int_{0}^{x}\cos\{t-\cos t\}dt}{x}$$ I am not sure if the limit converges. Here is what I have from Desmos: $$\begin{array}{|c|c|}
\hline
x & \frac{\int_{0}^{x}\cos\{t-\cos t\}dt}{x} \\\hline
10 & 0.829164368874\\\hline
10*10^3 & 0.834299716027\\\hline
10*10^6 & 0.859423358961\\\hline
10*10^9 & 0.840428861971\\\hline
10*10^{14} & 0.826897135363\\\hline
10*10^{15} & 0.958730802913\\\hline
10*10^{16} & 0.995401790748\\\hline
10*10^{26} & 0.999999999999\\\hline
10*10^{27} & 1\\\hline
\end{array}$$ As you can see, the limit appears to oscillate which makes sense after looking at the graph. When it gets to 10 $^{\text{15}}$ , it suddenly jumps up and begins to approach 1. My intuition tells me this is probably a bug because I am using such high numbers. I tried to solve it by hand but failed to figure out how to convert it to a summation. I have a few ideas to what it converges to (assuming it does and is not 1) but they are basically wild guesses. $$\cos(\cos(1))\approx 0.857553215846$$ $$\frac{\pi^2}{12}\approx 0.822467033424$$ All help is appreciated :)","['integration', 'improper-integrals', 'closed-form', 'limits', 'average']"
4809196,Show that the sequence $(3n-2)!!!/(3^n\cdot n!)$ converges to $0$.,"I came across this sequence while trying to prove that an alternating series converges by Leibniz's test. Here's the question and my answer . Define a sequence $\{a_n\}_{n\in\mathbb N}$ as follows. $$a_{n} =\frac{( 3n-2) !!!}{3^{n} \cdotp n!}=\frac{1}{3} \cdot \frac{4}{6} \cdot \frac{7}{9} \cdots \frac{3n-2}{3n}$$ I have shown that it is convergent. Numerically, I found that it converges to $0$ ... However, I struggled to find the limit. Here's what I did: Consider the sequence $$b_n=\left(\frac{1}{n}\left(\frac{1}{3} +\frac{4}{6} +\frac{7}{9} +\cdots +\frac{3n-2}{3n}\right)\right)^{n}$$ $a_1=b_1$ , however, for all $n\geq 2$ , $0<a_n<b_n\tag*{}$ (by A.M.-G.M. inequality). Show that $\lim b_n=0$ and conclude by sandwich theorem that $\lim a_n=0$ . $$\begin{aligned}
b_{n} & =\left(\frac{1}{n}\sum _{j=1}^{n}\left( 1-\frac{2}{3j}\right)\right)^{n}\\
 & =\left( 1-\frac{2}{3n} \cdot \sum _{j=1}^{n}\frac{1}{j}\right)^{n}
\end{aligned}$$ $$\color{blue}{\Rightarrow \lim b_n=\exp\left(-\frac{2}{3}\lim \sum _{j=1}^{n}\frac{1}{j}\right)}$$ This is a partial sum sequence of a well-known divergent series. $$\therefore \lim b_n=\lim_{x\to\infty}\exp\left(-\frac{2}{3}x\right)=0$$ I hope there is no mistake in my approach though I have doubts about the step which is coloured in blue: Is it always justified that $\lim (1+f(n)/n)^n$ will be same as $\lim\exp(f(n))$ ? I have done this in high school calculus without giving it much thought, however, I would want to introspect and analyze. I would also like to know if there's any alternate yet simpler way to find this limit... I have also thought of a generalization inspired by this post . We can rewrite in terms of binomial coefficients: $$a_n=\binom{-1/3}{n} (-1)^n$$ I am adding this, in case it's useful.","['limits', 'solution-verification', 'sequences-and-series']"
4809217,How would you compute $\int_0^\infty{\frac{dx}{x^{\frac{2}{5}} (x^2 + 1)^2 }}$.,"Given the following integral, how would you solve it? $$
\int_0^\infty{\frac{dx}{x^{\frac{2}{5}} (x^2 + 1)^2 }}
$$ because $f(x)$ is an even function: $$
\int_{-\infty}^\infty{\frac{dx}{2x^{\frac{2}{5}} (x^2 + 1)^2 }}
$$ By Residue integration, we have: $$
\int_\Gamma f(z)dz = 2 \pi i \sum \text{Res} \{f, z_i \}
$$ The countour $\Gamma$ is: $$
\Gamma = [\varepsilon, R] \cup\gamma_{R}(0, \pi) \cup [-R, -\varepsilon] \cup \gamma_\varepsilon (-\pi, 0)
$$ where: $$
\gamma_r(a, b): \; z =  re^{it} \; \; \; \; t\in (a, b)
$$ hence: $$
\text{Res} \{f, z = 0 \} = \lim_{z \to 0} \; \; (z - 0) \frac{1}{2z^{\frac{2}{5}} (z^2 + 1)^2 } = 0.
$$ $$
\text{Res} \{f, z = i \} = \lim_{z \to i} \; \; \frac{d}{dz} \left[(z - i)^2\frac{1}{2z^{\frac{2}{5}} (z^2 + 1)^2 } \right] = -\frac{7}{40}e^{3i \pi / 10}
$$ $$
\int_\Gamma f(z)dz = -\frac{7 i \pi}{20}e^{3i \pi / 10}
$$ Now, splitting the integrals: $$
\int_\Gamma f(z)dz = \int_{\gamma_R} f(z)dz + \int_{\gamma_\varepsilon} f(z)dz + \int_{-R}^{-\varepsilon} f(z) dz + \int^{R}_{\varepsilon} f(z) dz
$$ $$
\int_{\gamma_\varepsilon} f(z)dz = \pi i \text{Res} \{f, z = 0 \} = 0
$$ performing the limit where $R \to \infty$ and $\varepsilon \to 0$ : $$
\int_\Gamma f(z)dz  = \int_{-\infty}^{\infty} f(z)dz =  -\frac{7 i \pi}{20}e^{3i \pi / 10}
$$ But this is clearly wrong. I'm doing it right? How should I solve the integral with residues?","['definite-integrals', 'complex-analysis', 'stochastic-processes', 'calculus', 'probability']"
4809229,Find the maximum difference between the limits of integration,"Let $ C=\int_{a}^{b} (7x-x^2-10) dx $ where $a<b$ Determine the maximum value $(b-a)$ can assume if $C=0$ This question has troubled me for some time, and I would like some help to solve this problem.
The correct answer is $(b-a)≈5.2$ I plotted the graph of the quadratic polynomial and found that $a=0.5$ and $b=5$ leads to $C=0$ Here is the picture of the graph But this means that $(b-a)=4.5$ which is the wrong answer","['integration', 'definite-integrals', 'maxima-minima', 'calculus', 'functions']"
4809235,How do I calculate the prediction interval for a data set in python?,"I have a data set taken from real measurements that I have modeled with a simple univariate linear regression in python using scipy.stats.linregress , so the model is of the form $$y=\beta_0+\beta_1x$$ My goal is to be able to predict the response $\hat{y}$ with a new measured value $\hat{x}$ . Besides just plugging $\hat{x}$ into my model for a prediction, I would like to include the prediction interval to indicate the uncertainty in the prediction. I have found this equation but I’m not sure I’m implementing it correctly since I’m getting a value that seems larger than what I would expect from my understanding of the prediction interval. Particularly, the t-statistic is something that is confusing for me. If there is just a package that can do the calculation for me, please point me in the right direction because I didn’t find anything in the libraries I’m familiar with, though I am still a python novice. Code: import scipy.stats as sps
import numpy as np

x = np.array([0.506, 0.55, 0.479, 0.637, 0.558, 0.685, 0.508, 0.573, 0.612, 0.263, 0.366, 0.437,
 0.668, 0.506, 0.42, 0.341, 0.35, 0.544, 0.528, 0.513, 0.515, 0.399, 0.585, 0.499,
 0.488, 0.415, 0.512, 0.514, 0.468, 0.464, 0.35, 0.516, 0.459, 0.443, 0.497, 0.506,
 0.525, 0.408, 0.509, 0.285, 0.436, 0.509, 0.489])
y = np.array([134., 134.8, 128.8, 148.9, 140.7, 155.1, 132.5, 141.3, 146.5, 90.9, 117.7, 128.1,
 152.6, 134.5, 119.6, 101.1, 108.9, 137.7, 134., 130.6, 130.9, 115.4, 140.9, 132.6,
 129.1, 117.8, 137.4, 134.7, 130., 128.2, 116.3, 134.3, 127.1, 124.6, 129.2, 133.6,
 136.9, 115.9, 136.2,  98.6, 123.5, 129., 130.6])

model = sps.linregress(x,y)

new_x = 0.55

y_predictions = model[0] * x + model[1]
mse = np.mean((y - y_predictions)**2)
tss = ((x - np.mean(x))**2).sum()
x_mean = np.mean(x)
n = len(x)
t = (x_mean - new_x) / (np.std(x) / np.sqrt(n))

prediction_interval = t * np.sqrt(mse * (1 + 1.0 / n + (new_x - x_mean)**2 / tss))

print(prediction_interval) # = -14.1659827752476","['sums-of-squares', 'statistics', 'python', 'mean-square-error']"
4809236,Which matrix operation is occuring?,"I am currently going through a financial book that covers some matrix multiplication but I am not certain which operation is occurring to get this result. The book provides spreadsheets so I can see the result and therefore deduce what is happening but I don't know whether the math is correct or something else. The book is taking a [1xn] vector and multiplying this by an [nxn] matrix and then multiplying it by an [nx1] vector (whether first vector is [nx1] or [1xn] is not indicated by the book, same for the last vector). The literal formula provided by the book is $\Sigma = \sigma.\rho.\sigma^T$ where $\Sigma$ is the covariance matrix, $\sigma$ is the vector of standard deviations, $\rho$ is the correlation matrix. The following are the inputs and outputs, assuming $\sigma$ is a row vector: $\Sigma = \begin{bmatrix}\sigma_1 & \sigma_2\end{bmatrix} . \begin{bmatrix}\rho_{1,1} & \rho_{1,2}\\\rho_{2,1} & \rho_{2,2}\end{bmatrix} .\begin{bmatrix}\sigma_1\\\sigma_2\end{bmatrix} = \begin{bmatrix}\rho_{1,1}\sigma_1\sigma_1 & \rho_{1,2}\sigma_1\sigma_2\\\rho_{2,1}\sigma_2\sigma_1 & \rho_{2,2}\sigma_2\sigma_2\end{bmatrix}$ With a dot product (to my understanding), the dimensions of the matrix and vectors would result in a single scalar, not a matrix. If the dimensions of the vectors were flipped (column vector then row vector), the dot product would be impossible due to the different number of columns in the vector as rows in the matrix. Any help is greatly appreciate! I apologize if this is a simple problem but this has been puzzling me for awhile now.","['matrices', 'matrix-equations']"
4809269,Integral of $\int x^5 \cdot \sqrt{2 - x^3}dx$,"I have been working on this integral and cannot seem to notice an error I am making. Thanks for any advice in advance! For the following integral $$
\int x^5\sqrt{2 - x^3}\,dx 
$$ I considered the $u$ -substitution as such $$
u = x^3, 
\quad 
du = 3x^2\,dx
$$ Then I rewrote the integral as follows: $$
\frac{1}{3}\int u\sqrt{2 - u}\,du
$$ This I integrated by parts as follows \begin{align}
\frac{1}{3}\int u\sqrt{2 - u}\, du 
&= \frac{1}{3}\left( 
-\frac{2u\left(2-u\right)^{\frac{3}{2}}}{3} 
+ \frac{2}{3}\int\left(2-u\right)^{\frac{3}{2}}\, 
dx \right) \\ 
&= -\frac{2x^3\left( 
2-x^3\right)^{\frac{3}{2}}}{9} 
- \frac{4\left(2-x^3\right)^{\frac{5}{2}}}{45} + C 
\end{align} where $C$ is the integrating constant. The issue is that WolframAlpha obtains a different answer as such: $$
\frac{2}{-9} (2-x^3)^{2/3} \left(x^3+\frac{2}{3}\right) + D
$$ I was expecting it to only differ by the integrating constant but when I graphed them I realised they are not the same. Thanks for any insight into my mistake! Update: The mistake was found to be in the WolframAlpha interpreter as it was interpreted in the wrong manner, despite the fact that the only different thing I used was a different variable. In conclusion, be sure to put in precise and clear input!","['integration', 'analysis', 'real-analysis', 'calculus', 'indefinite-integrals']"
4809279,Uniqueness of the solution to systems of first-order linear PDEs,"Context: Let $\Omega \subset \mathbb{R}^p$ be an domain.
For functions $A_{jk}^i : \Omega \to \mathbb{R}$ and $B_k^i : \Omega \to \mathbb{R}$ with some regularity, I am interested in the following system of PDEs: $$
\sum_{j = 1}^n \sum_{k = 1}^m 
A_{jk}^i (x) \frac{\partial f_k}{\partial x_j} (x)
+
B_{k}^i (x) f_k (x)
=
0
\ \ \text{for} \ \ 
i = 1, \dots, r
$$ where $f = (f_1, \dots, f_m)$ are variables.
Assume that the number of equations is no less than the number of variables, i.e., $r \geq m.$ Also, assume the Dirichlet boundary condition: $f_k = 0$ on $\partial \Omega.$ Question: It is obvious that $f_k = 0$ is a solution.
I am wondering under what conditions on $A_{jk}^i$ and $B_k^i$ the uniqueness is guaranteed. What I have tried: I found some potentially useful books/papers/notes. (1) Wavefronts and Rays as Characteristics and Asymptotics by Andrej Bóna and Michael A Slawinski. In section 1.7 (page 24), they briefly explain how to solve the system. Their setup is almost the same as what I have in mind, but their method relies on the higher-order differentiability of $A_{jk}^i,$ which is not ideal for me. (2) METHODS OF MATHEMATICAL PHYSICS VOLUME II by R. COUHANT and D. HILBERT. In section 2.2 (pages 14-15), they address the system saying that by Cramer's rule, one can transform the system into independent PDEs. But I am not pretty sure how I should apply Cramer's rule to differential operators. (3) Lecture note by Evy Kersalé. In section 2.4, he discusses interesting methods to solve the system for $p = 2.$ I would like to know more general treatments. What I want: I conjecture that a sort of full rankness of $A_{jk}^i$ and $B_k^i$ is sufficient for the uniqueness, but I have not found such a result.
Also, I do not want to put too strong differentiability assumptions on $A_{jk}^i$ and $B_k^i.$ Ideally, they should be up to $C^1.$ Any thoughts and references are helpful. Thanks!","['linear-pde', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
4809289,"Conjecture: Given any five points, we can always draw a pair of non-intersecting circles whose diameter endpoints are four of those points.","Is the following conjecture true or false: Given any five coplanar points, we can always draw at least one pair of non-intersecting circles coplanar with the points, such that two of the given points are diameter endpoints of one circle, and another two of the given points are diameter endpoints of the other circle. Tangent circles are considered to be non-intersecting. Coincident circles are considered to be intersecting. Example: Another example: I cannot find a counter-example, nor can I prove the conjecture. I made a generator of five (pseudo)random points. (If, instead, we were given four points, the conjecture would not be true: for example, if the four points were the vertices of an equilateral triangle plus the centre, then we could not draw a pair of non-intersecting circles.) Context: I was thinking about this question about random points in a disk. Staring at various sets of five points, I came up with this conjecture. Edit: The generalized conjecture ( $2n+1$ points and $n$ circles) is not true . Edit 2: I have asked, and answered, a similar question . Maybe it might provide ideas for this question. Edit 3 Posted on MO .","['conjectures', 'circles', 'geometry', 'examples-counterexamples', 'discrete-mathematics']"
4809385,Determine conjugacy classes from group representation for character table,"Find character table of Klein four group The representation of Klein four group is, $V=\{a,b\mid a^2=b^2=(ab)^2=e\}$ Now, to determine the character table, first I need to find out all the conjugacy class. There are four elements in $V$ . But I don't see how to get the conjugacy classes from the representation. Hence, use permutation group elements, $V=\{e,(12),(34),(12)(34)\}$ , to explicitly get some idea. Now, isn't there should be three conjugacy classes? (as per identity, two cycle and 4 cycle) \begin{align*}
\begin{array}{c | c| c | c }
    & h_1=1 & h_2=2 & h_3=1\\
    \hline
   \chi^1 & 1 & 1 & 1\\
    \hline
   \chi^2 & a & c & e\\
    \hline
   \chi^3 & b & d & f\\
\end{array}
\end{align*} $$1^2+a^2+b^2=4\implies a^2+b^2=3$$ Then, either, $a=2,b=1$ or $a=1,b=2$ . Picking the first choice, and then apply first and second orthogonality formula I can fill the rest. but when I googled the solution, I get the table is incorrect. It is $4\times 4$ . It seems like the only issue I was facing to determine how to get conjugacy classes, if I have only given the representation. Is there any way to get that for smaller group? As @testaccount pointed out, I messed up with the conjugacy classes with $S_4$ . Fixing them I get, \begin{align*}
\begin{array}{c| c | c| c | c }
    & h_1=1 & h_2=1 & h_3=1 &h_4=1\\
    \hline
   \chi^1 & 1 & 1 & 1 & 1\\
    \hline
   \chi^2 & 1 & a & b & c\\
    \hline
   \chi^3 & 1 & d & e & f\\
    \hline
\chi^4 & 1 & g & h & i\\
\end{array}
\end{align*} \begin{align}
1+a+d+g&=0,\\
1+b+e+h&=0,\\
1+c+f+i&=0
\end{align} , \begin{align}
1+a+b+c&=0,\\
1+d+e+f&=0,\\
1+g+h+i&=0
\end{align} but now the system of equation seems very complicated to solve.","['representation-theory', 'group-theory', 'finite-groups', 'characters']"
4809393,Incorrect partial sum formula in textbook?,"I was helping my brother with his maths homework, where he has just started learning about arithmetic series and their formulas such as the sum of the first $n$ terms ( $S_n$ ) or finding the $n$ th term of the sequence ( $T_n$ ). While looking through some of the questions in his textbook, I came across this question: ""Find the original sequence $T_n$ if its partial sums $S_n$ are given by $S_n = n^2+5$ "" We can find $T_n$ if we take $S_n-S_{n-1}$ : $$S_n-S_{n-1}=n^2+5 - (n-1)^2-5=2n-1
$$ $$ \Rightarrow T_n
=2n-1$$ which is the expected solution to the question by the textbook. However upon trying to find an actual sequence of numbers that fit the solution I came up empty handed! I feel as if there is no way the sum can have a constant term but I'm not sure why. Is the textbook's solution actually correct? Is there such a series of numbers that works? If the textbook's solution is wrong, am I correct in assuming that for any arithmetic series of numbers, the sum of the series cannot be of the form $S_n=f(n)+k$ , where $k$ is a constant?","['arithmetic-progressions', 'sequences-and-series']"
4809416,When is this integral a polynomial?,"Consider the functions or integrals $f : \mathbb{R} \to \mathbb{R}$ of the form, $$ f(x) = \int_{a}^{b}K(|x-y|)u(y)dy $$ where $u$ is known and $K$ is a weakly singular kernel (or a continuous kernel for simplicity). How do we show that this function $f$ is a polynomial ? One approach would be to actually compute the integral, however, that is a difficult task. For example, consider the integral $$ \int_{-1}^{1} |x-y|^{-s}(1-y^2)^{\frac{1+s}{2}} dy = \frac{\pi (1+s)}{2\sin{\left( \pi\frac{1+s}{2} \right)}}(1-sx^2),$$ where $s 
\in (-1,1)$ , $x\in[-1,1]$ . Another approach would be to show higher order derivatives of this integral are zero, but that seems impossible because of the weakly singular kernel. Any hints? P.S : I have not proven the integral above yet. Edit : Maybe the question I have asked is too general. We can also start with the following problem: For what values of $s \in (0,1)$ is $$\int_{-1}^{1}(1-y^2)^s\ln{|x-y|}dy $$ a polynomial in $x$ ? Edit 2 : It appears that above integral is a polynomial for $x\in[-1,1]$ , $s=1/2$ .","['integration', 'singular-integrals', 'derivatives', 'polynomials']"
4809426,How important is getting nitty-gritty with ideals for algebraic number theory?,"Coming off an undergraduate course on number fields based on Marcus's textbook Number Fields , I am interested in taking the logical next step towards (local) class field theory, as well as Iwasawa theory. I had some difficulty with working with ideals and their prime decomposition in number rings for the first time: e.g., trouble with ideal operations, not really understanding how to compute the finite field $\mathcal{O}/\mathfrak{p}$ , the phenomenon of inertia as it relates to $[\mathcal{O}_L/\mathfrak{q}:\mathcal{O}_K/\mathfrak{p}]$ . I would like to eventually get better at working hands-on with ideals, e.g., computing the ideal class group of a number field using the Minkowski bound. However, I'm not sure how much intuition I should have for ideals in number rings (and Dedekind domains, more generally), as I go into algebraic number theory 'proper', i.e., CFT. Should I ground my intuitions in more classical settings before wading through the abstractions (idelic CFT, Galois cohomology, etc.)? Or would I be better served by picking it up along the way?","['algebraic-number-theory', 'number-theory', 'ideals', 'class-field-theory', 'soft-question']"
4809460,"What is the largest disk that will be completely covered by randomly placed disks of areas $1,\frac12,\frac13,\dots$ with probability $1$?","On a ""bottom"" disk of area $A$ , we place ""top"" disks of areas $1,\frac12,\frac13,\cdots$ such that the centre of each top disk is an independent uniformly random point on the bottom disk. Find the maximum value of $A$ such that the bottom disk will be completely covered by the top disks with probability $1$ , or show that there is no maximum. The harmonic series diverges, but the problem here is that the top disks overlap, so it is not clear to me whether a bottom disk of a given area will be completely covered by the top disks, with probability $1$ . I made a desmos graph to help visualise the disks. (This question was inspired by a question about rain droplets falling on a table.)","['circles', 'geometry', 'sequences-and-series', 'probability-theory', 'probability']"
4809497,Problems with the definition of $R^2$,"Wikipedia , defines $R^2$ or R-squared as follow: In statistics, the coefficient of determination, denoted $R^2$ or $r^2$ and pronounced ""R squared"", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s). It is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. I couldn't completely understand the above definition about $R^2$ . In the first paragraph, I'm not sure how variation in the dependent variable can be predicted by independent variables. To be specific, I cannot figure out what does predicting variation of a variable mean, I know about predicting a variable but not predicting variation of a variable. Also in the last part of the second paragraph, what does proportion of total variation of outcomes explained by model mean? For example, for $R^2= 0.9$ I think it means that % $90$ of variation of dependent variable is explained by the model. I couldn't figure out the meaning of variation being explained by a model.",['statistics']
