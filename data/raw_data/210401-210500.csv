question_id,title,body,tags
4228759,Nature of roots of two quadratic expressions,"Given $\,\left|px^2 +qx +r\right|\leqslant\left|Px^2 +Qx +R\right|\,$ for all real $x$ where $P,Q$ and $R$ are different from $p,q$ and $r$ , I wish to find the relation between the roots of these quadratic expressions assuming both of them have real roots.So, their discriminants are both positive. I'm unable to find where to start with. How do I mathematically prove this? I can visualise the scenario using a graph but, I don't get where to begin, mathematically. Can someone help me out to proceed?
Thanks in advance.",['algebra-precalculus']
4228784,Let $R$ be a commutative ring with unity. $s\in U(S) \iff \det(s) \in U(R)$,"Let $R$ be a commutative ring with $1_R$ and
let $$
S = \biggl\{ \begin{pmatrix} a & b \\ 0 & c \end{pmatrix} \;\Biggm| \; a,b,c \in R \;\biggr\}.
$$ If $s = \begin{pmatrix} a & b \\ 0 & c \end{pmatrix} \in S$ , is it true that $$
s \in U(S) \iff \det(s) \in U(R)?
$$ My instinct says it isn't but I cant find an example so I can make my mind right.","['matrices', 'ring-theory', 'abstract-algebra', 'linear-algebra', 'multilinear-algebra']"
4228826,Deducing an inequality from a Taylor series expansion,"Consider the inequality $$
1-\frac{x}{2}-\frac{x^2}{2} \le \sqrt{1-x} < 1-\frac{x}{2}
$$ for $0 < x < 1$ . The upper bound can be read off the Taylor expansion for $\sqrt{1-x}$ around $0$ , $$
\sqrt{1-x} = 1 - \frac{x}{2} - \frac{x^2}{8} - \frac{x^3}{16} - \dots
$$ by noting that all the non linear terms are negative. Can the left side inequality be read-off the expansion by a similar reasoning? Please do not try to prove the left side inequality by other means (such as minimizing $\sqrt{1-x} - 1 + \frac{x}{2} + \frac{x^2}{2}$ using derivatives).","['calculus', 'taylor-expansion', 'inequality']"
4228850,Artist needing to determine geometric angle for sculpture based on platonic solid,"Dear Mathematicians I need your help for a new sculpture! I will attach images  but first imagine 2 hexagons  - where one is rotated 30 deg. They are separated by 12 equilateral triangles. I need to confirm the angle between the 2 faces as show in the image. My computer render calculation makes it 145.222 deg but I need to confirm this is absolutely correct before proceeding to fabrication! Your help would be greatly appreciated, thank you! Pete This shows the angle I need to confirm: This is the computer design angle that I need to confirm: This is something like what the final sculpture will look like! This shows how I made it:","['art', 'geometry', 'platonic-solids']"
4228854,Combinatorial Geometric proof of $\binom{\binom{n}{2}}{3} > \binom{\binom{n}{3}}{2}$,"Consider the following diagram for n=5: $\displaystyle \binom{\binom{n}{2}}{3}$ represents the number of ways to choose 3 rotationally distinct faces from the three sides of the entire diagonal pyramid. $\displaystyle \binom{\binom{n}{3}}{2}$ represents the number of ways to choose 2 cubes from the lighter sub-pyramid. Each choice can be mapped to a cuboid by taking the cuboid hull (the smallest cuboid in the 3D grid that contains the selected objects) of the selections, but it is not immediately obvious that this proves the inequality. For example, there are $n\times m\times 1$ cuboids in the sub-pyramid that cannot be selected via the face selection process. An alternative geometric proof is equally welcome.","['euclidean-geometry', 'combinatorial-proofs', 'geometry', 'combinatorial-geometry', 'combinatorics']"
4228878,Does the limit $\lim_{x\to a^+}\frac{\cos{x}\ln(x-a)}{\ln(e^x-e^a)} $ exist?,"I am very confused about the limit of the expression below: $$\lim_{x\to a^+}\frac{\cos{x}\ln(x-a)}{\ln(e^x-e^a)} $$ By using L'Hospital's rule, I was managed to find the result which was $\cos a$ , same as other answers that I could possibly find on the Internet.
However, when I used GeoGebra to sketch the graph, the value of $f(x)$ tends to jump to infinity, both negative and positive. I wonder if my result was wrong, or the limit does not exist at all? (In the picture below, I chose $a=15$ for easy re-check). Thanks in advance!","['limits', 'calculus']"
4228911,Derivative of $\tan^{-1}\left(\sqrt{\frac{a-b}{a+b}}\tan \frac x2\right)$.,"Find the derivative of $\tan^{-1}\left(\sqrt{\frac{a-b}{a+b}}\tan \frac x2\right)$ . I'm learning differentiation and this is an exercise problem from my book. I used chain rule and got the following: $\begin{align}
\dfrac d{dx}\left[\tan^{-1}\left(\sqrt{\frac{a-b}{a+b}}\tan\frac x2\right)\right] &= \dfrac{1}{1+\frac{a-b}{a+b}\tan^2\frac x 2}\cdot\dfrac{d}{dx}\left(\sqrt{\frac{a-b}{a+b}}\tan \frac x2\right)\\ &= \dfrac{1}{1+\frac{a-b}{a+b}\tan^2\frac x 2}\cdot\frac 1 2\sqrt{\frac{a-b}{a+b}}\sec^2\frac x 2
\end{align}$ But this doesn't match the answer in the book. The given answer is $\frac{\sqrt{a^2-b^2}}{2(a+b\cos x)}$ . So, where did I go wrong and what is the correct solution?","['calculus', 'inverse-function', 'derivatives', 'chain-rule']"
4228936,Why is a Laplace transform with unfixed contour a good Ansatz to solve Bessel's differential equation? In which situations is this a good Ansatz?,"I've seen a remarkable Ansatz in solving an ODE with non constant coefficients and I am wondering: Why does this Ansatz work so well? Here is the description of the problem, the Ansatz and why I think it is remarkable:
Consider the homogeneous linear ODE $$xy''+y'+xy = 0.$$ This equation has a solution space spanned by Bessel functions $Y_0$ (diverging for $x\rightarrow 0$ ) and $J_0$ (finite for the same limit). One can obtain an integral representation of $J_0$ by using the following Ansatz: $$y(x) = \int_C e^{xz}P(z) \mathrm{d}z,$$ where $C$ is an appropriately chosen contour in the complex plane.
Of course, with such an Ansatz one wants to reduce the ODE to an algebraic equation, but normally, one achieves this via a Fourier or a Laplace transform. However, in both those approaches the contour is by definition fixed and I am wondering, where the insight comes from to set hopes in a Fourier style Ansatz with an unfixed contour. Are there other ODEs solveable by cooking up a solution using this Ansatz? What are the  typical features of such ODEs? Let me show how the Ansatz is employed to find $J_0$ , because this could help in answering my question. It is employed in two steps: Firstly, using the ODE, one obtains an equation that provides a simple ODE, secondly, this solution's boundary condition fixes the contour.
One starts by plugging in the Ansatz to obtain $$\int_C (xz^2 + z + x)e^{xz}P(z) \mathrm{d}z = 0$$ Then one assumes that there is an $S(z)$ such that the integrand equals the total derivative of $e^{xz}S(z)$ .
This requirement then yields the condition that $e^{xw}S(w)$ vanishes for $w$ in the boundary of the contour and relates $S$ and $P$ as follows: $$(xz^2 + z + x)P = x S + S'$$ This eqn holds for all x and thus we can compare the coefficients in front of $x^{0}$ and $x^{1}$ From those two conditions, we can divide out $P$ (assume $P$ to be non vanishing on the contour) to obtain a good ODE for $S$ reading $$ \frac{S'}{S} = \frac{z}{z^2 + 1},$$ whose solution provides us with $S = \sqrt{z^2 + 1}$ from which we can infer a contour running from $-i$ to $i$ , e.g. a straight line (due to the boundary condition on $e^{xw}S(w)$ ). Massaging $y$ a bit further, one ends up with the desired $J_0$ .","['fourier-transform', 'laplace-transform', 'ordinary-differential-equations', 'bessel-functions']"
4228974,I've been asked to learn function compositions. I am lost how to find (f ∘ g)(x) [duplicate],"This question already has answers here : How to compose two functions? (2 answers) Closed 2 years ago . I am trying to combine functions.  My dad explained that he cannot answer and maybe I should track math stack overflow.  I am not sure exactly how to type the questions because the keyboard doesn't have all the math symbols.  My online class doesn't allow copy and paste either.  I will try. I am trying to find $(f \circ g)(x)$ where $$f(x)=\frac{1}{x^2+3},$$ $$g(x)=\sqrt{x-2}.$$ My lesson said that  (f o g)(x) (the o looks like a degree sign, but placed down lower. My keyboard will not allow me to type the actual symbol) f(g(x)), but I don't understand how to use this formula to solve the problem, and my lesson isn't explaining very well either. I'm hoping somebody can help me out with this and I can actually interact with them to figure out how to do this. (I am in tenth grade, precal algebra. Attached is a screenshot of the problem that I am trying to solve.) $$
\begin{aligned}
\text{1. }&\text {Find }(f \circ g)(x) \text { where } f(x)=\frac{1}{x^{2}+3} \text { and } g(x)=\sqrt{x-2} \text {. }\\
&\text{a. }(f \circ g)(x)=\frac{1}{x-2}\\
&\text{b. }(f \circ g)(x)=\frac{1}{\sqrt{x-2}+3}\\
&\text{c. }(f \circ g)(x)=\frac{1}{x+1}\\
&\text{d. }(f \circ g)(x)=\sqrt{\frac{-2 x^{2}-5}{x^{2}+3}}
\end{aligned}
$$","['algebra-precalculus', 'function-and-relation-composition']"
4228976,Prove a chord of a parabola passes through a fixed point,"I am doing the end of chapter questions on parabolas in a pure maths book out of interest. I am struggling with this: A fixed point $P(ap^2,2ap)$ is taken on the parabola $y^2=4ax$ . Two points Q and R are chosen on the parabola such that the lines PQ and PR are perpendicular. Prove that the line QR passes through a fixed point, F, independent of Q and R, and that PF is normal to the parabola at P. I started by working out the slopes of PQ,QR and PR: $(\frac{2}{p+q},\frac{2}{r+q},\frac{-(p+q)}{2})$ and then I worked out the equation of QR, which I found to be: $y-2aq=\frac{2}{r+q}(x-aq^2) \rightarrow y=\frac{2x+2aqr}{r+q}$ The question wants an answer independent of Q and R so I reasoned I need to find an expression in p only.
I did find such an expression but even then I couldn't find where the fixed point should be. But even then, the question only gives the fixed point F after we are supposed to have found that such a point exists.","['conic-sections', 'geometry']"
4228989,Calculate the general solution and the specific solution of the following differential equation: $(3y - 1)^2) (y'^2) = 4y$,Calculate the general solution and the specific solution of the following differential equation: $(3y - 1)^2  (y')^2 = 4y.$ The general solution is $(x + C)^2 = y(y - 1)^2$ and the specific is $y = 0$ . I have tried rewriting it like $y' = \dfrac{2y}{3y + 1}$ but I don't know what to do next.,['ordinary-differential-equations']
4229004,plane to torus projection,"I am struggling to find a way to make a projection of a 2d plane $(x,y)$ onto a section of a torus. Is there a function $f$ that would map it depending on the given tor radii, the radii of its section and angles $\theta$ and $\phi$ ? Torus mapping Image:","['general-topology', 'analysis']"
4229041,Find the function which satisfies the following functional equation,This is the equation $$e^{ia}f\left[\int_{0}^a e^{ix}f(x)dx\right]=\int_{0}^a e^{ix}f(x)dx$$ I tried but wasn't able to figure out. Any suggestion towards how should I approach it are appreciated. My attempt : I tried to differentiate with respect to $a$ . I got this : $$f(a) = e^{ia}f’\left(\int_{0}^a e^{ix}f(x)dx\right)f(a) + if\left[\int_{0}^a e^{ix}f(x)dx\right]$$ then I tried to equate real and imaginary part but nothing happened.,"['calculus', 'ordinary-differential-equations']"
4229047,"Prove if $z=a+bi$ is a root of the polynomial P(z), $\overline{z} =a-bi$ will be a root too.","I have a polynomial $P( z) =a_{n} z^{n} +...+a_{1} z+a_{0}$ with real coefficients and I know that $z=a+bi$ is a root of it, but I don't know how to prove that $\overline{z}$ is a root too. Thanks!","['calculus', 'algebra-precalculus', 'complex-numbers']"
4229059,If $ab=ba$ what are the possible orders of $ab$?,"The question is: Let $a$ and $b$ be two elements of in a finite group $G$ , say $o(a) = m$ and $o(b) = n$ . If $ab = ba$ , determine all possible values of $o(ab)$ .(Assume $o(x)=$ the order of x) Proof. Assume $(m,n)=d$ , $m=dm'$ and $n=dn'$ , in the way that $(m',n')=1$ . $$\begin{align}
&d = p_1^{\alpha_1}\dots p_a^{\alpha_a}q_1^{\beta_1}\dots q_b^{\beta_b}t_1^{\gamma_1} \dots t_c^{\gamma_c}\\
&n' = p_1^{\alpha_1'}\dots p_a^{\alpha_a'}.n'', (n'',d)=1, \alpha_i' \ge 1\\
&m' = q_1^{\beta_1'}\dots q_b^{\beta_b'}.m'', (m'',d)=1, \beta_i' \ge 1
\end{align}$$ In fact, we break $d$ to three parts. First, primes that $n'$ posses. Second, primes that $m'$ posses. We use the fact that these two numbers do not have any primes in common too.( $(n',m')=1$ ). We want to find for what $r$ we have: $(ab)^r=1$ $$\begin{equation}
ab=ba \rightarrow (ab)^r=a^rb^r=1\implies a^r=(b^{-1})^r
\end{equation}\tag{1}\label{eq1}
$$ We know $b$ has order n then $b^{-1}$ has the same order too. If we have $a^r=(b^{-1})^r$ , so the we should have $o(a^r)=o((b^{-1})^r)$ . Moreover, We know order of elements in the cyclic group $\langle a \rangle$ and $\langle b^{-1} \rangle$ . $$\begin{equation}
\begin{cases}
o(a^r)=\frac{m}{(m,r)}\\[2ex]
o((b^{-1})^{r})=\frac{n}{(n,r)}
\end{cases}
\xrightarrow{\eqref{eq1}} \frac{m}{(m,r)}=\frac{n}{(n,r)}\implies m.(n,r)=n.(m,r)
\end{equation}\tag{2}\label{eq2}
$$ $$\begin{equation}
m.(n,r)=n.(m,r) \rightarrow dm'.(dn',r)=dn'(dm',r)\implies m'.(dn',r)=n'(dm',r)
\end{equation}\tag{3}\label{eq3}
$$ As a result, we should have $m' |(dm',r)$ , and this means that $m' |r$ . With the symmetry: $$\begin{equation}
\begin{cases}
m' |r\\[2ex]
n' |r
\end{cases}
\xrightarrow{(m',n')=1} n'm'|r \implies r=n'm'r'
\end{equation}\tag{4}\label{eq4}
$$ $$\begin{equation}
m'.(dn',r)=n'(dm',r) \xrightarrow{ r=n'm'r'} (d,r'm')=(d,r'n')
\end{equation}\tag{5}\label{eq5}
$$ Lemma 1: $ r|dn'm'$ . Proof. As we have $(ab)^{dn'm'}=a^{nm'}b^{mn'}=1_G$ . Thus $o(ab)|dn'm'$ . $\blacksquare$ As we proved that $r=n'm'r'$ , and by the use of Lemma 1: $$\begin{equation}
r=n'm'r'|dn'm' \rightarrow r'|d
\end{equation}\tag{6}\label{eq6}
$$ Consequently, we can write $r'$ in the following format: $$\begin{equation}
r'=p_1^{\zeta_1}\dots p_a^{\zeta_a}q_1^{\eta_1}\dots q_b^{\eta_b}t_1^{\mu_1} \dots t_c^{\mu_c}\\
 \zeta_i \le \alpha_i , \eta_i \le \beta_i,\mu_i \le \gamma_i \\
\end{equation}\tag{7}\label{eq7}
$$ Now we want to use the $\eqref{eq5}$ to prove that $\zeta_i=\alpha_i$ and $\eta_i=\beta_i$ . $$\begin{equation}
\begin{cases}
 (d,r'm')= p_1^{\min(\alpha_1,\zeta_1)} \dots p_1^{\min(\alpha_a,\zeta_a)} q_1^{\min(\beta_1,\eta_1+\beta_1^{'})} \dots q_b^{\min(\beta_b,\eta_b+\beta_b^{'})} t_1^{\mu_1} \dots t_c^{\mu_c}\\[2ex]
 (d,r'n')= p_1^{\min(\alpha_1,\zeta_1+\alpha_1^{'})} \dots p_1^{\min(\alpha_a,\zeta_a+\alpha_a^{'})} q_1^{\min(\beta_1,\eta_1)} \dots q_b^{\min(\beta_b,\eta_b)} t_1^{\mu_1} \dots t_c^{\mu_c}\\
\end{cases}
\xrightarrow{\eqref{eq5}}
\end{equation}\tag{8}\label{eq8}
$$ $$\begin{equation}
\begin{cases}
\min(\alpha_i,\zeta_i+\alpha_i')=\min(\alpha_i,\zeta_i)= \zeta_i; \forall  1 \le i \le a\\[2ex]
\min(\beta_j,\eta_j+\beta_j')=\min(\beta_j,\eta_j) = \eta_j\; \forall  1 \le j \le b 
\end{cases} 
\xrightarrow{\beta_j',\alpha_i' \ge 1}
\begin{cases}
\alpha_i=\zeta_i\\[2ex]
\beta_j =\eta_j
\end{cases} 
\end{equation}\tag{9}\label{eq9}
$$ In the above equation we use the facts that $r'|d$ and $(m'',d)=(n'',d)=1$ .
At the end we can conclude the following statement:
If $ab=ba$ , $o(a)=m$ and $o(b)=n$ . If $o(ab)=r$ then $r$ is the following format: $$\begin{align}
&d = p_1^{\alpha_1}\dots p_a^{\alpha_a}q_1^{\beta_1}\dots q_b^{\beta_b}t_1^{\gamma_1} \dots t_c^{\gamma_c}\\
&n' = p_1^{\alpha_1'}\dots p_a^{\alpha_a'}.n'', (n'',d)=1, \alpha_i' \ge 1\\
&m' = q_1^{\beta_1'}\dots q_b^{\beta_b'}.m'', (m'',d)=1, \beta_i' \ge 1\\
&\implies r=n'm'p_1^{\alpha_1}\dots p_a^{\alpha_a}q_1^{\beta_1}\dots q_b^{\beta_b}t_1^{\mu_1} \dots t_c^{\mu_c}, \mu_i \le \gamma_i
\end{align}$$ Questions: Is this correct or am I missing something? Are any other restrictions that can be added? Is there any way to check this orders exist or not?","['group-theory', 'abstract-algebra', 'solution-verification']"
4229148,Self-Study in Discrete Mathematics: Linear Recurrence Relations,"Through self-study of mathematics, I accidentally stumbled into an area called Linear Recurrence Relations. After poking around in a few different websites I think I'm starting to grasp the idea. I'm just looking to clarify something I read to see if I'm understanding something correctly.
On one of the of the websites I was looking at it, it gave the following example: $x_1=3, x_n=3x_{n-1}$ $x_2=3x_1=9$ $x_3=3x_2=27$ $x_4=3x_3=81$ Having ""solved"" the equation for multiple instances, it is noticed that the answers are powers of 3. Thereby making the formula $x_n=3^n$ . I have a few questions. Is it valid to re-write the starting formula as $x_{n+1}=3x_n$ or does this fundamentally change something with the base formula? Is it necessary to solve for several iterations of the base equation to derive the formula or is there a method to have inferred the $x_n=3^n$ from the original formula? Are the solvable iterations necessary for proof? I will probably have more questions. As this area of mathematics is new to me, and has been self-study. Also, I apologize for any improper use of LaTeX. I'm just now learning of it and how to write with it. So if something doesn't look right, it's probably due to my inexperience with it.","['recurrence-relations', 'discrete-mathematics']"
4229195,Categorize matrices such that each column has $c$ nonzero entries and each row has $b$ nonzero entries,"I would like to understand all matrices (including those which are nonsquare) which obey the following properties: Each column contains the same number $c$ of nonzero elements Each row contains the same number $b$ of nonzero elements For any such matrix I understand that the number of rows $s$ and the number of columns $a$ must obey the constraint $$ac=bs$$ When $c=1$ the solution matrix configuration takes the form of a modified identity matrix in which each column is duplicated $b$ times (an s×(nb) matrix), and when $b=1$ the same is true except there are instead $c$ duplicates of each row (an (sc)×n matrix). It is also clear that given a solution matrix with parameters $a,b,c,s$ one may easily find another solution $a,a-b,s-c,s$ is also a solution matrix. I understand that there may be more than one solution for a given pair $b,c$ . For example, $a,b,c,s=4,3,3,4$ and $a,b,c,s=7,3,3,7$ both have $b,c=3,3$ . One way to differentiate between these two solutions is to recognize that in the one with $a,s=4,4$ each row contains non-zero elements in 2 columns that each other row contains non-zero elements. That is, any pair of rows ""overlaps"" by exactly two columns in which they both contain non-zero elements. Call this the overlap number $d$ . While $a,b,c,d,s=4,3,3,2,4$ and $a,b,c,d,s=7,3,3,1,7$ are solutions, $a,b,c,d,s=4,3,3,1,4$ and $a,b,c,d,s=7,3,3,2,7$ are not. If $$n=\sum_{i=1}^{s-1}$$ then $c,d=2,1$ is a solution. Also, the tuplets $a,b,c,d,s=n,n-1,n-1,n-2,n$ correspond to square matrix solutions which may be represented with a first row of $n-1$ 1s and one 0, with each of the subsequent $n-1$ rows a cyclic rotation (by one element) of the previous. Is there a way to determine whether or not a given triplet $b,c,d$ (or $$a,s,d$$ $$a,c,d$$ $$b,s,d$$ or . . . ) is a valid solution without simply testing the triplet to see if it will provide a solution? Perhaps there is an algebraic relationship related to the prime factors of these values which one can use to generate all valid solutions. Any information would be greatly appreciated.","['matrices', 'linear-algebra', 'discrete-mathematics']"
4229231,Is it possible to place one queen and at least 29 knights in a 8x8 chess board such that no 2 pieces attack each other?,"Is it possible to place one queen and at least 29 knights in a 8 $\times$ 8 chess board such that no 2 pieces attack each other? I thought to try to use the bound $\lceil \frac{mn}{2} \rceil $ for the number of knights in a $m \times n$ chessboard such that they don't attack each other, but that requires $m,n >2$ , and for some queen positions, that doesn't apply. I also thought that if we had a queen, there are at most 7 rows to place the knights, so then there have to be 5 knights in one row. I'm not sure what use that could be though.","['chessboard', 'combinatorics']"
4229238,Difference between generating functions and formal power series,"So I was reading about generating functions and formal power series, and it seems that these two concepts are used interchangeably. Can someone please tell me the difference between them? Is generating functions a method that involves using formal power series? Thanks.","['formal-power-series', 'definition', 'combinatorics', 'generating-functions']"
4229275,Why doesn't MVT prove all derivatives are continuous?,"Obviously something is wrong with the following argument, but I'm not entirely what is. Let $f$ be differentiable in $(-1,1)$ and fix $0<x<1$ . Then, by MVT, there exists $0<y<x$ such that $$\frac{f(x)-f(0)}{x} = f'(y)$$ Taking the limit as $x \rightarrow 0$ on both sides gives $f'(0)$ on the LHS by definition, so we get $f'(0) = \lim_{x \rightarrow 0} f'(y)$ , which seems to prove $f'$ is continuous at zero since $y \rightarrow 0$ as $x \rightarrow 0$ . Again, I know this is wrong and can produce a counter-example (for instance, $f(x) = x^2\sin(1/x)$ , so $f'(0) = 1$ but $f'(x) < 0$ in neighborhoods of $0$ ). Can someone tell me exactly which step is invalid? Based on the conditions of L'Hopital's rule, the flaw seems to be that I'm assuming $\lim_{x \rightarrow 0} f'(y)$ exists, but I'm not sure if this is it since we just proved that it exists and equals $f'(x)$ .","['derivatives', 'real-analysis']"
4229288,Doubts regarding the change of order of integration,"$$\int_0^1\int_0^x \sqrt{x+y^2}\,dydx = \int_0^x\int_0^1 \sqrt{x+y^2}\,dxdy$$ Are these two integrals equivalent to each other? I assumed they weren't after imagining that one should also change the order of integration (in that case, analysing the region and changing the limits of integration). Am I confused about something, or does my reasoning make sense? If I'm right, I'd like to understand why you have to change the limits if you change the order of integration (I know that one can use Fubini's theorem on rectangular regions). Sorry if this isn't in the correct place, I have never posted here.","['multivariable-calculus', 'calculus']"
4229297,$g^{ij} \nabla_{\partial_j} \nabla_{\partial_i} V$?,"Question: How to simplify the following local expression: \begin{equation}\tag{*}
  g^{ij} \Big( \partial_j (\partial_i V^k + \Gamma_{im}^k V^m) - \Gamma_{ij}^l (\partial_l V^k + \Gamma_{lm}^k V^m) \Big),
\end{equation} where $V$ is a vector field on a Riemannian manifold $(M,g)$ , $\Gamma$ is the Christoffel symbols. Motivation: I am trying to get through the old paper of Dohrn and Guerra , which has the following quantity in its eqaution (12): \begin{equation}\tag{**}
  g^{ij} \nabla_{\partial_j} \nabla_{\partial_i} V,
\end{equation} where $\nabla$ is the Riemannian covariant derivative. According to my derivation from the preceding text of the paper, the quantity $(**)$ should coorespond to the local expression $(*)$ . However, a simple application of definitions gives the local expression of $(**)$ as follows, \begin{equation}
  g^{ij} \nabla_{\partial_j} \nabla_{\partial_i} V = g^{ij} \Big( \partial_j (\partial_i V^k + \Gamma_{im}^k V^m) + \Gamma_{jm}^k (\partial_i V^m + \Gamma_{il}^m V^l) \Big) \partial_k,
\end{equation} which does not coincide with (*). So I strongly suspect that the expression (**) in the paper is not correct. But I still want to know if it is possible to simplify the local expression (*) to a quantity with a global expression, which may be similar to (**) ? TIA...","['connections', 'riemannian-geometry', 'differential-geometry']"
4229316,What is the boundary of the upper half plane?,Let $\mathbb H : = \{z \in \mathbb C\ |\ \mathfrak {I} (z) \gt 0 \}.$ Then what will be $\partial \mathbb H\ $ ? I feel like it is $\mathbb R$ but in my book I found that it is $\mathbb R_{\infty}$ which might be thought of as the one point compactification of $\mathbb R.$ If we are in the extended complex plane then it's fine but if we aren't there how can we even talk of the point at infinity. This fact confuses me a lot. Could someone shed some light on it? Thanks for investing your valuable time in reading my question.,"['complex-analysis', 'riemann-sphere']"
4229360,Expected value of interesting random variable on sphere.,"Imagine we have a sphere. We will fix the orthogonal coordinate system $xyz$ . First, we paint the lower hemisphere in some color(for lower I mean we place the center of sphere in the origin, and hemisphere with negative $y$ -coordinate will be the lower). After that we rotate the sphere (randomly) and again paint the lower hemisphere. What is expected value for the number of such rotations for which sphere will be fully painted.
First, I consider simpler case. Just circle. So, there is no information for distribution of random choose of rotation, so it will be $U[0;\pi]$ (uniform). So let $\xi \in U[0;\pi]$ the rotation angle (also, we have a freedom of choosing the clockwise rotation or not, but I couldn’t think of how I can handle it). Now, let $N_{\pi} =\{ min n : \sum_{i=0}^n \xi_i > \pi\}$ . We can easily find the expected value of such random variable by recursion formula (I won’t provide it, since it is quite easy, and also we didn’t take into account choice of rotation direction). Even if I could solve the easier case. I don’t know how to apply it to original problem.","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4229362,"$\lim_{x\rightarrow0,y\rightarrow0}\frac{xy}{\ln(x^2+y^2)}=?$","$\lim\limits_{x\rightarrow0,y\rightarrow0}\frac{xy}{\ln(x^2+y^2)}=?$ Now intuitively(if I am right), when I substitute $x$ and $y$ with zeros I get $\frac{0}{-\infty}$ - which is $0$ . I also checked on Wolfram Alpha and solution is $0$ . If this is okay I struggle with formally writing the solution, or is it okay to leave it like this? --EDIT-- Now I also have to show the existence of partial derivatives at point $(0,0)$ $\lim\limits_{x\rightarrow 0}=\frac{f(x+0,0)-f(0,0)}{x}=...=0$ , is this okay? Same for $y$ .","['limits', 'multivariable-calculus']"
4229391,"evaluate $\int_0^{\pi/2} x^2\log(\sin x)\,dx$","I am a high school student , I know how to evaluate $\int_0^{\pi/2} x\log(\sin x)\,dx$ .
It would be great if someone can help me evaluating $\int_0^{\pi/2} x^2\log(\sin x)\,dx$ and tell me if this integral is elementary or non elementary .
I tried using the ""a-x"" property but it resulted in $0=0$","['integration', 'trigonometric-integrals', 'definite-integrals', 'real-analysis']"
4229437,"Using sign-reversing involution to prove that $\sum^{n}_{k=p} (-1)^{k} \binom{n}{k} \binom{k}{p} = \delta_{n,p} (-1)^p$ for any $p \leq n$.","My current progress is that I'm supposing $S$ to be $ \{ (A,B) | A \subset B \subset \{ 1, 2, ..., n \} ; n(A) = p \}$ ,
and $sgn(A,B) = (-1)^{n(B)}$ . This gives $$\sum_{(A,B) \in S} sgn(A,B) = \sum_{A \subset B \subset [n]} (-1)^{|B|}$$ $$ = \sum_{k} \sum_{B \in \binom{[n]}{k} } \sum_{A \in \binom{B}{k} } (-1)^k $$ $$ = \sum_{k} (-1)^k \binom{n}{k} \binom{k}{p}.$$ But I cannot define the involution $\iota$ , could somebody please guide or give me some more hint to this question? UPDATE : Now I can finally define $\iota ((A,B))$ as $(A, B \triangle \{ \alpha \} )$ for any abstract element $\alpha \in B$ , so it became $$sgn \iota ((A,B)) = sgn (A, B \triangle \{ \alpha \} )$$ $$= (-1)^{|B| - 1} $$ $$= - (-1)^{|B|} $$ $$ = - sgn ((A,B)) $$ Hence it is a sign-reversing involution. But now I have got another problem. What is $Fix ( \iota )$ ?! And I also lost how it became $(-1)^p \delta_{np}$ . RSVP if you have an idea or more hint. Thank you for your further information.","['involutions', 'summation', 'combinatorial-proofs', 'binomial-coefficients', 'combinatorics']"
4229438,What is the minimum number of tests to achieve statistical significance?,I'm dealing with a situation where in a large manufacturing facility we have approximately $2000$ plumbing fittings of the same make and model. $3$ of those fitting have failed within the last year. Each causing major property damage. We suspect that the plumbing components suffered degradation and we want to test a sample of the remaining plumbing components to see how wide spread the issue is (if at all). What is the best way to decide how big of a sample we should choose so that we do not under test or over test the installed components. Any idea where to begin?,['statistics']
4229452,How do I find the equation of an envelope?,"I read that you must solve the two equations $$g(x,y,c)=0\\\frac{\partial g}{\partial c}=0$$ for $x$ and $y$ as a function of $c$ , but how exactly do you go about doing this? The specific example I am trying to solve is where $$F(x,y,\alpha)=
-t y \sin\alpha \tan{\alpha\over2} - t \sin\alpha 
\left(x - r \cos\alpha - t \sin\alpha \tan{\alpha\over2}\right)$$ and find the parametric equation for the envelope ( $r$ and $t$ are constants). BTW, this is from this post if you are curious.","['envelope', 'geometry', 'partial-differential-equations', 'partial-derivative', 'derivatives']"
4229462,Hartshorne Exercise I.3.17 (e): Normalization of Affine Varieties,"I'm trying to solve the following exercise from Hartshorne, namely Exercise I.3.17 (e): (e) Let $Y$ be an affine variety. Show that there is a normal affine variety $\widetilde{Y}$ , and a morphism $\pi\colon\widetilde{Y}\to Y$ , with the propery that whenever $Z$ is a normal variety, and $\varphi\colon Z\to Y$ is a dominant morphism (i.e., $\varphi(Z)$ is dense in $Y$ ), then there is a unique morphism $\theta\colon Z\to\widetilde{Y}$ such that $\varphi=\pi\circ\theta$ . $\widetilde{Y}$ is called the normalization of $Y$ . You will need (3.9A) above. Here's my attempt: Let $A = A(Y)$ . By Finiteness of Integral Closure (Theorem 3.9A), we have that $\overline{A}$ is an affine domain. Thus there is an affine variety $\widetilde{Y}$ with $A(\widetilde{Y}) = \overline{A}$ . Furthermore, $\widetilde{Y}$ is normal. The canonical inclusion $A\hookrightarrow \overline{A}$ gives rise to morphism $\pi: \widetilde{Y}\to Y$ . Now, suppose $Z$ is another normal variety, which a dominant morphism $\varphi : Z\to Y$ . This induces an inclusion of function fields in the opposite direction $K(Y)\hookrightarrow K(Z)$ , which I'll call $\varphi^{\sharp}$ . Since $A(Z)$ is integrally closed, $\varphi^{\sharp}$ induces a unique map $A(\widetilde{Y}) = \overline{A}\to A(Z)$ such that the diagram $$ \begin{array}{cc}
A(\widetilde{Y})&\hookleftarrow A(Y)\\
\downarrow\varphi^{\sharp}&\swarrow_{\varphi^{\sharp}}\\
A(Z)
\end{array}$$ commutes. (The maps labelled $\varphi^{\sharp}$ above are just the relevant restrictions of $\varphi^{\sharp}$ .) Finally, we consider the above in the equivalent opposite category of affine varieties and we are done. I would be very grateful if someone could point out mistakes in my attempt, or confirm that it's correct. Thank you.","['algebraic-geometry', 'solution-verification', 'commutative-algebra']"
4229501,Twice differentiable function related problem [duplicate],"This question already has answers here : If $f(x)=f'(x)+f''(x)$ then show that $f(x)=0$ (4 answers) Closed 2 years ago . Let $f(x)$ be a real valued twice differentiable function on interval $[1,5]$ such that $f(1) = f(5) = 0$ and $f(x) = f'(x) + f''(x)$ , $\forall x \in \left[ {1,5} \right]$ , then $(f(2)+f(4)–f'(3))$ is equal to___________________. I am not able to approach this problem. Initially I thought of using Rolle's Theorem but function needs to be continuous in the closed interval $[1,5]$ and differentiable in the open interval $(1,5)$ . How do I perform the mathematical operation on these numbers","['calculus', 'functions', 'real-analysis']"
4229510,Difference between Sequence of Functions and Multivariable Functions,"I already know what is a Multivariable Function, which looks like the following: $$f(x,y)=\frac{x}{x+y}$$ And I just know that a Sequence of Function looks like $$f_n(x)=\frac{x}{x+n}$$ That's confusing to me. Why not, instead, just put n inside of the parentheses $$f(x,n)=\frac{x}{x+n}$$ where $$n \in N^*$$ I wonder what's the difference between those two and can that sub n be considered as a input variable of $f$ ? Thank you StackExchange. <3","['notation', 'multivariable-calculus', 'functions', 'sequence-of-function']"
4229521,"If $(Av,Au)=(v,u)$ then matrix $A$ is orthogonal","Let $A\in M_{n \times n}(\Bbb R)$ and suppose that for every $u, v \in \Bbb R^{n}$ $$(Av,Au) = (v,u)$$ where $(\cdot,\cdot)$ is the standard inner product on $\Bbb R^{n}$ . Prove $A$ is an orthogonal matrix. I wasn't able to solve it, I got to the point $\left(Av,Au\right)=\left(Av\right)^{T}Au=v^{T}A^{T}Au$ and $\left(v,u\right)=v^{T}u$ $\left(Av,Au\right)=\left(v,u\right)\ \ \ ➜\ \ \ v^{T}A^{T}Au\ =\ v^{T}u$ and now I'm stuck.. I don't know if I can conclude that $A^{T}A=I_{R^{n}}$ just by the last equation, and if not how to get to the point I can show it.. Now I have two questions, first the official solution is this Let $u = e_{i}$ and $v = e_{j}$ . Then $(a_{i} , a_{j} ) = (Ae_{i} , Ae_{j} ) = (e_{i} , e_{j} ) = δ_{i,j}$ . Thus, the columns of $A$ are
orthonormal, so $A$ is orthogonal. This is super unclear.. what is $a_{i},a_{j}$ ? what is $δ_{i,j}$ ?
I understand that $(e_{i},e_{j})=0$ if $i \ne j$ and 1 otherwise, but why can we choose $v,u$ if it's a for every claim? If someone can explain to be the logic behind the solution I'd be grateful. the second question is if there is another way to solve it?","['matrices', 'orthogonal-matrices', 'inner-products']"
4229537,continuous function defined on closed bounded interval is bounded? is the continuous necessary?,"I've seen the proof about this theorem, but I am wondering if this condition continuous is necessary? My gut feels as long as a function is defined on a closed set, which means every element in the domain are defined, will not play around like the open set which can go to infinity. e.g $$f:(0,1)\to \mathbb{R}\\x\to \frac{1}{x}$$ appreciate any comments.","['continuity', 'functions', 'analysis', 'real-analysis']"
4229579,Find the angles of given triangle ABC,"A triangle $ABC$ with angle bisectors $AA_1$ and $BB_1$ is given, such that $\angle AA_1B_1 = 24^\circ$ and $\angle BB_1A_1 = 18^\circ$ . Find the angles of the triangle. I've been stuck on this one for quite a long time. After denoting with $I$ the incenter of ABC and deriving that $\angle C = 96^\circ$ from $\angle AIB = 90^\circ + \frac12\angle C = 138^\circ$ , I really don't know how to continue. I tried using Geogebra to see everything clearer or at least guess the answer, and I concluded that $\angle A$ and $\angle B$ should be $12^\circ$ and $72^\circ$ respectively, but I'm not sure how to prove it. Any help would be appreciated. If I come up with something, I will post it right away. Thanks in advance!","['euclidean-geometry', 'angle', 'geometry', 'triangles', 'plane-geometry']"
4229586,A proof for Polya's urn model,"Question An urn initially contains $r$ red and $b$ blue balls. At each stage, a ball is randomly selected and returned along with $m$ other balls of the same colour. Let $X_k$ be the number of red balls drawn in the first $k$ draws. Conjecture the value of $\mathbb{E}(X_k)$ and verify your conjecture using a conditioning argument. Hints For $1 \leq i \leq k$ , define $$Y_i =
\begin{cases}
1 & \quad \mathrm{if\ draw\ } i\ \mathrm{from\ the\ urn\ is\ red}\\
0 & \quad \mathrm{if\ draw\ } i\ \mathrm{from\ the\ urn\ is\ blue}
\end{cases}$$ and evaluate $\mathbb{E}[Y_3 \mid X_2]$ . My working Conjecture: $\mathbb{E}(X_k) = \dfrac {kr} {r + b}$ . Let $R_k$ and $B_k$ be the events that a red or blue ball was drawn in the $k^{th}$ draw respectively. For $k = 2$ , we have $\begin{aligned}
\mathbb{P}(R_2) & = \mathbb{P}(R_2 \mid R_1)\mathbb{P}(R_1) + \mathbb{P}(R_2 \mid B_1)\mathbb{P}(B_1)\\[1 mm]
& = \frac {(r + b)\left(\frac r {r + b}\right) + m} {r + b + m}\left(\frac r {r + b}\right) + \frac {(r + b)\left(\frac r {r + b}\right)} {r + b + m}\left(1 - \frac r {r + b}\right)\\[1 mm]
& = \frac m {r + b + m}\left(\frac r {r + b}\right) + \frac {(r + b)\left(\frac r {r + b}\right)} {r + b + m}\\[1 mm]
& = \frac r {r + b},
\end{aligned}$ so the base case is true. Now, suppose the conjecture is true for $k = n$ and for $k = n + 1$ , we have $\begin{aligned}
\mathbb{P}(R_{n + 1}) & = \mathbb{P}(R_{n + 1} \mid R_n)\mathbb{P}(R_n) + \mathbb{P}(R_{n + 1} \mid B_n)\mathbb{P}(B_n)\\[1 mm]
& = \frac {[r + b + (n - 1)m]\left(\frac r {r + b}\right) + m} {r + b + nm} \left(\frac r {r + b}\right) + \frac {[r + b + (n - 1)m]\left(\frac r {r + b}\right)} {r + b + nm} \left(1 - \frac r {r + b}\right)\\[1 mm]
& = \frac m {r + b + nm} \left(\frac r {r + b}\right) + \frac {[r + b + (n - 1)m]\left(\frac r {r + b}\right)} {r + b + nm}\\[1 mm]
& = \frac r {r + b}
\end{aligned}$ Thus, by induction, we can see that the probability of drawing a red ball at every stage is constant at $\dfrac r {r + b}$ and independent of drawing any ball at any other stage, so $\mathbb{E}(X_k) = \dfrac {kr} {r + b}$ . I know that there are already quite a few proofs out there regarding Polya's urn model and although I believe my proof is valid, I am still posting this as I am not exactly sure how to make use of the hints given. Any intuitive suggestions would be greatly appreciated :)","['polya-urn-model', 'conditional-expectation', 'expected-value', 'solution-verification', 'probability']"
4229591,Example of a function $h$ which its Hardy-Littlewood maximal function $h^*$ is finite but unbounded,"I'm trying to solve the following problem from Axler's Measure, Integration & Real Analysis : Give an example of a Borel measurable function $h:\mathbb{R} \to [0,\infty)$ such that $h^*(b)<\infty$ for all $b\in \mathbb{R}$ but $\sup \{h^*(b):b\in\mathbb{R}\} = \infty.$ Here $h^*$ is the Hardy-Littlewood maximal function of $h$ which is defined as follows: $$h^*(b)=\sup_{t>0}\frac{1}{2t}\int_{b-t}^{b+t}\lvert h \rvert.$$ My first guess was $f(x)=\lvert x\rvert$ since it seems to have the property everywhere but it fails at $0$ . Now I think such a function could be the answer: $$\sum_{i=0}^{\infty}(i+1)\chi_{[i^2,i^2+1]}$$ where $\chi_{[a,b]}$ is the characteristic function of $[a,b]$ . But I'm not sure that this function is a Borel measurable one and if it is, I don't know how prove or disprove that this is the answer. Any help is appreciated.","['measure-theory', 'lebesgue-integral', 'real-analysis']"
4229639,How it is possible that left side oscillation of function be 0 but left side limit does not exist?,"Let A be domain of function. The oscillation of function defined as: $$\Omega_{A} f:=\sup _{A} f-\inf _{A}=\sup _{x, y \in A}|f(x)-f(y)|$$ And oscillation of function at point defined as: $$\omega_{f}(c):=\lim _{h \rightarrow 0+} \Omega_{] c-h, c+h[\cap A} f$$ Show that if $$\lim _{x \rightarrow c-} f(x)$$ exsists  and is finite
then $$
\lim _{x \rightarrow c-}\omega_{f}(x)=0 $$ Show that converse is false. Okey forward direction I understood. but I can't find any example that converse is false.","['limits', 'calculus', 'functions', 'real-analysis']"
4229692,Can we have a category of compactifications?,"Is there any work on compactifications of spaces in terms of category theory? I would like to know whether there is a defined category of compactifications; I will denote it Comp . Could you describe Comp like this? Objects = compactifications Morphisms = homeomorphisms between compactifications? Also, intuitively a Stone-Čech compactification is a ""biggest"" object and an Alexandroff compactification is a ""smallest"" object of Comp . Would they have any special properties then? I.e. would they be initial and terminal objects respectively? Thank you for your insights.","['slice-category', 'category-theory', 'compactification', 'general-topology', 'compactness']"
4229710,Showing the rank of a group $H$ is precisely the maximal rank $\alpha$ of a sequence of groups.,"I'm working with an inverse limit of groups and I'm trying to prove some property. Let $F_n$ be the free group of rank $n$ . We have a morphism from $f_n:F_n\to F_{n-1}$ that sends a word in $F_n$ to the word in $F_{n-1}$ with the instances of the $n$ -th generator removed, it is surjective and by composing those morphism we have $\cdots\to F_n\to F_{n-1}\to \cdots \to F_2\to F_1$ . We define $F$ as the inverse limit of this system, that is $F=\left\lbrace (w_n)_n\in\prod_i F_i \;\vert \;f_n(w_n)=w_{n-1}\right\rbrace$ . My goal is to show that if $H$ is a finitely generated subgroup of $F$ then $H$ is free. I think I have the proof, there's just one detail that I can't quite figure out. For each $n$ we have a projection $p_n:F\to F_n$ , so let $H$ be a finitely generated subgroup of $F$ we define for each $n$ a group $H_n=p_n(H)<F_n$ .
Since subgroups of free groups are free, each $H_n$ is free. I've shown that these free groups $H_n$ have a maximal rank $\alpha$ , and that there is an $i_0\in \mathbb{N}$ such that for all $i\geq i_0$ the rank of $H_i$ is precisely $\alpha$ . So for $i\geq i_0$ , the groups $H_i$ are the same (up to isomorphism since there's essentially one free group of each rank) and since finitely generated free group are Hopfian, the morphisms $f_n:H_n\to H_{n-1}$ (restricted to $H_n$ ) are isomorphisms. This allows us to show that $x\in H$ is solely determined by its image in $H_{i_0}$ . So taking $B\subset H$ that is mapped to a free basis for $H_{i_0}$ by $p_{i_0}$ , we can find a free basis for H and we're done. I've glossed over a bunch of details that are irrelevant to the question I'm about to ask but this is to give an idea of the context. I'm fairly confident that I should be able to prove that the maximal rank $\alpha$ is precisely the rank of $H$ , yet I can't seem to put my hand on why ? And since the rest of the proof sort of relies on it, it would help if I could prove it. Maybe something is wrong but I think it's supposed to work. At first I sort of hand waved it assuming filling in the details would be easy, but the more I think about it the less I know how to do it. I tried assuming $rank H > \alpha$ and then finding a contradiction to the fact that the morphism $f_i$ is an isomorphism (for $i\geq i_0$ ) but I didn't succeed. Not quite sure where to go from there. EDIT: for clarification, the groups $H_n$ 's rank is bounded by the rank of $H$ and its increasing with $n$ (or stationary) so it converges towards some $\alpha\in\mathbb{N}$ . What I can't prove is that the rank of $H$ is not just an upper bound but the least upper bound, ie $rank\, H =\alpha$ .","['group-theory', 'abstract-algebra', 'free-groups', 'limits-colimits']"
4229711,Does the Existence of Derivatives along All Smooth Paths Guarantee Multivariable Differentiability?,"This question is inspired by other relevant questions on MSE regarding continuity of partial derivatives and differentiability , existence of partial derivatives and differentiability , and limits along arbitrary smooth curves . I'm trying to get a handle on the extra bit of weirdness introduced by multivariable functions when it comes to differentiability and continuity. The applicable definition here is: Definition (Cain & Herod, 1997) : Let $f: D \rightarrow \mathbb{R}^p$ , where $D \subset \mathbb{R}^n$ and let $\mathbf{x}$ be an interior point of $D$ . Then $f$ is differentiable if there exists a linear function $L$ such that $\lim\limits_{\mathbf{h} \rightarrow \mathbf{0}} \dfrac{\|f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - L(\mathbf{h})\|}{\|\mathbf{h}\|} = \mathbf{0}$ So, given that the multivariable limit in the definition above requires that the limit exists (and is zero) independent of the path, can I interpret this as translating to the requirement that the derivative along any smooth path through $\mathbf{x}$ must exist? That is, if we define a so-called path derivative $D_\mathbf{p}f$ : $D_{\mathbf{p}}f : \mathbf{x} \mapsto  \lim\limits_{t \rightarrow t_0} \dfrac{f(\mathbf{x} + \mathbf{p}(t)) - f(\mathbf{x})}{\|\mathbf{p}(t)\|}$ where $\mathbf{p}$ defines any path with $t_0$ such that $\lim\limits_{t \rightarrow t_0} \mathbf{p}(t) = \mathbf{0}$ , If a path is defined as smooth if it is representable by an infinitely differentiable vector function, is differentiability equivalent to the existence of all path derivatives where $\mathbf{p}$ defines a smooth path? Reading this answer , I initially suspected that this path derivative condition was identical to the total derivative which defines differentiability, but I wonder what kind of wrinkle the requirement that $\mathbf{p}$ defines a smooth path introduces. Proving that differentiability implies the existence of all such path derivatives is straightforward enough. I can't see if there's a way to prove differentiability assuming the existence of all path derivatives, however... How does this condition compare to the condition of the existence of all partial derivatives and $n-1$ continuous partial derivatives in some $n$ -ball containing $\mathbf{x}$ (quoted here) ? Is it stronger/weaker? At the very least, I think that this condition is not useless. For example, if you can find a smooth path for which the path derivative does not exist, then $f$ is not differentiable. In reference to this question –where one partial derivative is not defined along some line with $\mathbf{x}$ deleted–the notion of path derivatives implies that such a function is automatically not differentiable.","['partial-derivative', 'multivariable-calculus', 'analysis', 'real-analysis']"
4229714,Is the cube the largest polyhedron whose subsets are all symmetric?,"Given a polyhedron $P$ with vertex set $V$ of size $n$ , say that it is subset-symmetric if, for every subset $S\subseteq V$ , there is a non-identity element of the full reflectional symmetry group of $P$ which fixes $S$ (not necessarily pointwise). Obviously, it suffices to check this property for $|S|\le\frac n2$ . One can verify by hand that the cube has this property, which is larger/more complicated than I would have intuitively guessed. Can it be proven that the cube has the most vertices of any subset-symmetric polyhedron? I would also be interested in a complete classification of such polyhedra. The ones I am aware of: Any pyramid or bipyramid whose base is an equilateral triangle, a rectangle, or a regular pentagon The cube (One could also ask this about symmetries of subsets of the edges of a polyhedron, in which case the cube fails, and I am not sure if any polyhedron besides the regular tetrahedron has this property.)","['symmetry', 'geometry', 'polyhedra']"
4229782,How can one solve this without expanding.,"\begin{array}{l}
\text { If } a+b+c=1, a b+b c+c a=2 \\
\text { and } a b c=3 \text {. What is the value } \\
\text { of } a^{4}+b^{4}+c^{4} \text { ? }
\end{array} This can be solved by expanding but is there any easy alternative method ? Here is how I solved : \begin{array}{l}
(a+b+c)^{2}=1\\
\Rightarrow a^{2}+b^{2}+c^{2}+2(a b+b c+c a)=1\\
\Rightarrow a^{2}+b^{2}+c^{2}=-3 \ldots . .(i)\\
\Rightarrow a b+b c+c a=2 \ldots . \text { (ii) }\\
\text { Squaring of equation (ii), }\\
\Rightarrow a^{2} b^{2}+b^{2} c^{2}+c^{2} a^{2}+2\left(a b^{2} c+b c^{2} a+\right.\\
\left.c a^{2} b\right)=4\\
\Rightarrow a^{2} b^{2}+b^{2} c^{2}+c^{2} a^{2}+2 a b c(a+b+c)=4\\
\Rightarrow a^{2} b^{2}+b^{2} c^{2}+c^{2} a^{2}+6=4
\end{array} \begin{array}{l}
\Rightarrow a^{2} b^{2}+b^{2} c^{2}+c^{2} a^{2}=-2 \ldots . \text { (iii) }\\
\text { Squaring of equation (i), }\\
\Rightarrow a^{4}+b^{4}+c^{4}+2\left(a^{2} b^{2}+b^{2} c^{2}+c^{2} a^{2}\right)=9\\
\Rightarrow a^{4}+b^{4}+c^{4}-4=9\\
\Rightarrow a^{4}+b^{4}+c^{4}=13
\end{array}","['algebra-precalculus', 'quadratics']"
4229819,$\lfloor a_n \rfloor$=? where $a_n=\sqrt{2005+\sqrt{2005+\sqrt{2005+...+\sqrt{2005}}}}$,"$\lfloor a_n \rfloor$ =?, where $$a_n=\sqrt{2005+\sqrt{2005+\sqrt{2005+...+\sqrt{2005}}}}$$ and the number of radicals is equal to n. $\lfloor a_1]=\lfloor \sqrt{2005} \rfloor=44$ , $\lfloor a_2 \rfloor=\lfloor \sqrt{2005+\sqrt{2005}} \rfloor=45$ , $\lfloor a_3 \rfloor=\lfloor \sqrt{2005+\sqrt{2005+\sqrt{2005}}} \rfloor=45$ and so on,
but at some point becomes $\lfloor a_k \rfloor=46$ and then $47$ and so on.
So we can say that $a_n$ depends on the value of $n$ , which in my opinion has no rule. In order, how it should be defined $\lfloor a_n \rfloor$ =? Thank you!",['algebra-precalculus']
4229827,Messed permutations,"Here's an exercise from an old Egyptian textbook published in 1828. I'll try to preserve the main idea while translating it into English. Let $k\geq2$ be a natural number. Consider a premutation $\sigma\in S_k$ defined on $k^*=\{1,...,k\}$ . Define the messing number of $\sigma$ as $m(\sigma)=\min\{|i-j|+|\sigma(i)-\sigma(j)|:i,j\in k^*,i\neq j\}$ . Prove that the maximal messing number $\max\{m(\theta):\theta\in S_k\}$ is at most $4$ . I'm pretty sure that there is a mistake but yet didn't find a counterexample. If you manage to find the counterexample (i.e. some permutation with messing number $\geq 5$ ) or prove that the assertion is true, then please let me know it! Also, if the statement is wrong, what is the lower bound of the maximal messing number?","['permutations', 'combinatorics']"
4229991,Exercise 5.25 in Baby Rudin,"Suppose $f$ is twice differentiable on $[a, b], f(a)<0, f(b)>0, f'(x)\ge\delta>0$ , and $0\leq f''(x)\leq M$ for all $x\in[a, b]$ . Let $\xi$ be the unique point in $(a, b)$ at which $f(\xi)=0$ . Part (b): Choose $x_1\in(\xi, b)$ , and define $\{x_n\}$ by $x_{n+1}=x_n-\frac{f(x_{n})}{f'(x_{n})}$ . Prove that $x_{n+1}<x_n$ and that $\lim_{n\to\infty}x_n=\xi$ . My attempt: We use Induction. It is easy to show that $x_2<x_1$ . Now suppose $x_{k+1}<x_k$ , then easily $f(x_{k})>0$ and $x_{k+2}=x_{k+1}-\frac{f(x_{k+1})}{f'(x_{k+1})}$ . But where do I go from here? How do I show that $f(x_{k+1})>0$ ? I tried to prove that $x_{k+1}\in(\xi, b)$ , but induction does not seem to work there as I failed to show that $x_n\in(\xi, b) \forall n\in\Bbb{N}$ . Please help.","['induction', 'derivatives', 'real-analysis']"
4229994,"How to justify interchange of summation and integral with $f_n(x)=x^2 \cos(2nx)/n$ on $[0, \pi/2]\times \mathbb{N}$?","I am trying to compute $$\int_0^{\pi/2}\sum_{n=1}^\infty  \frac{x^2\cos(2nx)}{n}\,dx.$$ To do so, I would like to interchange integral and sum, but I am having trouble justifying the validity of this exchange. By Fubini-Tonelli, we just need to check that $$\int_0^{\pi/2}\sum_{n=1}^\infty  \frac{x^2|\cos(2nx)|}{n}\,dx$$ is well-defined classically (the order is irrelevant to existence). However, I am not sure how to control or estimate this expression. I thought of re-writing the magnitude of $\cos(2nx)$ as $\frac{1}{\sqrt{2}}(1+\cos(4nx)^{1/2}$ but wasn't able to make much progress.","['integration', 'fourier-series', 'summation', 'fourier-analysis']"
4230019,is there a name in math given to the ratio of two probabilities?,"if an event has probability $\pi$ and the other event has probability $1-\pi$ . is there a mathematical name or label for the ratio of those probabilities, $\frac{\pi}{1-\pi}$ ?",['probability-theory']
4230054,"A dice puzzle and an ""obvious"" fact I cannot prove [duplicate]","This question already has answers here : Expected outcome for repeated dice rolls with dice fixing (5 answers) Closed 2 years ago . Background This question is inspired by this 538 ""Riddler Classic"" puzzle , and the following puzzle explanation below is copied from there: You have four standard dice, and your goal is simple: Maximize the sum of your rolls. So you roll all four dice at once, hoping to achieve a high score. But wait, there’s more! If you’re not happy with your roll, you can choose to reroll zero, one, two or three of the dice. In other words, you must “freeze” one or more dice and set them aside, never to be rerolled. You repeat this process with the remaining dice — you roll them all and then freeze at least one. You repeat this process until all the dice are frozen. If you play strategically, what score can you expect to achieve on average? Extra credit: Instead of four dice, what if you start with five dice? What if you start with six dice? What if you start with N dice? Question We are interested only in the general $n$ case, and we are interested in a question tangential to the puzzle's solution . Assuming perfect play, consider the expected value (EV) of your sum when starting with $n$ dice, which we'll denote $E_n$ . The question is: Is it possible that, for some $n$ , the following holds? $$E_{n+1} > E_n + 6$$ Alternatively, should you ever choose to re-roll a six when playing optimally? Intuitively, it seems like the answer to both of these equivalent questions should be ""no"".  But I cannot think of a rigorous argument to prove it.","['dice', 'combinatorics', 'probability']"
4230100,"Matrix spaces A,B with the same dimension implying B=QAP?","I have a conjecture: If two finite-dimensional matrix spaces $\mathcal{A},\mathcal{B}\subseteq \mathrm{M}(n,\mathbb{F})$ have the same dimension, then there would exist $P,Q\in\operatorname{GL}(n,\mathbb{F})$ such that $\mathcal{B}=Q^{-1}\mathcal{A}P$ . ( Reminder: Matrix spaces refer to linear spaces spanned by matrices . ) Is it true or false? It is well-known that two finite-dimensional linear spaces are isomorphic if and only if they have the same dimension. Also, this means there is a bijection from a basis of one space to a basis of the other. Does it complete the proof of my conjecture? It seems that if the bases are vectors instead of matrices, then the statement would be true. So I'm very confused if the two cases can be the same. Thank you!","['vector-spaces', 'vector-space-isomorphism', 'matrices', 'abstract-algebra', 'linear-algebra']"
4230110,How can I prove that all quadratic equations are not injective?,"I was trying to prove that any quadratic formula ( $ax^2 + bx + c$ ) will not be injective, but I have a little problem. I started by assuming $f(x) = f(y)$ .
We can put x and y into the general form of quadratic function, and we get $ax^2 + bx + c = ay^2 + by + c$ Subtract c from both sides and organise a little bit and you get $a(x^2-y^2) + b(x-y)=0$ $a(x+y)(x-y) + b(x-y)=0$ Here we can assume that $x\ne y$ , hence $x-y\ne 0$ , so divide both sides by x-y, and we get $a(x+y)+b=0$ However, from here I couldn't find any contradictions, which is a problem because there must be a contradiction as quadratic functions are not injective. Can anyone tell me what the contradiction is here?","['functions', 'quadratics']"
4230126,How to use the AC to extract a countable subset from an infinite set.,"I have done some searches on this site before I wrote this question. There were similar posts regarding this problem to some extent, but none of them was exactly what I want to ask, and none of them solved my problem (They only showed the requirement of AC). This proposition is well known: Every infinite set has a countable subset. I have this proof for it: Let $X$ be an infinite set. Since it's not empty, we have a element $x_0 \in X$ . Now we define recursively $$\begin{align*}X_0 &:= X \\
 \exists x_n &\in X_n \\ X_{n+1} &:= X_n\setminus\{x_n\}\end{align*}$$ And thus we have an infinite sequence $\{x_n\}$ , which gives a
countable subset of $X$ . I understand why the AC is required, as we are selecting infinitely from sets of indistinguishable elements. However, I don't know how to apply it. In my understanding, we can only use AC when the entire collection of sets is known. In this problem, that requires us to know the entire $\{X_n\}$ . But as we can see, each definition of $X_n$ is built on the previous one. We may use finite choice and induction to show that for every $n$ , $\forall m \le n, X_n \text{ is well-defined}$ . But without the axiom of choice, we cannot show that all $X_n$ are well-defined. Therefore, we cannot make the whole collection of $X_n$ known without the axiom of choice. However, in order to use the axiom of choice, we must know the entire collection. This seems to be a contradiction. So after some thoughts, I even begin to doubt if the AC can really be used in this problem. Thank you for reading my problem.","['elementary-set-theory', 'axiom-of-choice']"
4230133,Reunite the four Stars (A puzzle),"On an infinite plane, the Prime Star has disintegrated into four constituent stars, the North Star, the South Star, the East Star and the West Star, each traveling at a constant speed of 1 in their eponymous directions. The Star Guardian at the center wants to reunite the four Stars back into the Prime Star again, which can only be achieved if the four Stars meet at a single point in spacetime. Furthermore: The Star Guardian moves at a constant speed of $g$ , in any direction she wants. She is only able to carry one Star at a time. Once left alone, the Stars always travel in their eponymous directions at speed 1. If only two or three Stars meet, they will just pass through each other without any interaction. Let $G=\{g\vert$ the speed $g$ Guardian can reunite the Stars in finite time $\}$ . Define $g^*=\inf G$ . Question 1 : What is the value of $g^*$ ? Question 2 : if we can put the four Stars in any other initial positions, and give them preferred directions other than the four cardinal directions, what is the maximum $g^*$ we can get? Source: I originally created this as a puzzle for the site puzzling.stackexchange.com, but the dynamics turned out to be more complex than I intended for puzzle site. For progress on question 1, the smallest $g$ found so far is $10/3$ , but no one knows whether it could be even smaller. The puzzle was also previously posted to MathOverflow , but math.se may be a more appropriate site for it.","['nonlinear-system', 'geometry', 'dynamical-systems']"
4230181,Simple ODE problem in a 1964 paper of Peter Lax,"The paper ""*Development of singularities of solutions of the nonlinear hyperbolic partial differential equation"" by Peter D.Lax (1964). It starts with a simple theorem about ODEs. Theorem: Let $z(t)$ be the solution of the initial value problem $$dz/dt =a(t)z^2,\ \ \ z(0)=m$$ in the interval $(0,T)$ . Suppose that the function $a(t)$ satisfies the inequality $$0<A<a(t),\ \ \ \ \ \ 0\leq t\leq T,$$ and suppose that $m$ is positive; then $$T<(mA)^{-1}$$ the proof of Lax following Let $z_0(t)$ be the solution of the comparision equation $$dz_0/dt=Az_0^2,\ \ \ z_0(0)=m.$$ Since A is lower bound for $a(t)$ , it follows easily that $z_0(t)$ is lower bound for $z(t)$ for all positive t. Since $z_0=m/(1-mAt)\rightarrow \infty\ \ at\ \ t=(mA)^{-1}$ , it follows that $z(t)$ cannot exist beyond this time. $Q.E.D$ I try to verify ""Since A is lower bound for $a(t)$ , it follows easily that $z_0(t)$ is lower bound for $z(t)$ for all positive t."" in this way. To prove this, I think a following lemma. (My lemma) let $g:[0,T)\rightarrow \mathbb{R}$ be a function which are following; i) $g(0)\geq 0$ ;
ii) $g'(0)>0$ ;  i.e. $\forall \epsilon>0,\ \exists \delta>0$ s.t $0<t<\delta$ imply $|\frac{g(t)-g(0)}{t}-g'(0)|<\epsilon$ then $\exists T^*\in (0,T)$ s.t $g(t)\geq 0\ \ for\  t \in [0,T^*)$ proof) From my assumption, let $\epsilon= g'(0)/2$ then exist $\delta>0$ s.t $t\in(0,\delta)$ $$|\frac{g(t)-g(0)}{t}-g'(0)|<g'(0)/2$$ therefore $$g'(0)/2<[g(t)-g(0)]/t<3g'(0)/2\ \ \ for \ t \in (0,\delta)$$ $$g(t)>g'(0)t/2+g(0)>0\ \ for\ t\in(0,\delta)$$ because $g(0)\geq 0$ , $g(t)\geq0\ \ \ \ for\ \ \ t \in [0,\delta)$ . let $\delta= T*$ then proof is done. now prove ""Since A is lower bound for $a(t)$ , it follows easily that $z_0(t)$ is lower bound for $z(t)$ for all positive t."" claim) let $z(t),z_0(t)$ satisfy above suggestion then $z_0(t)\leq z(t)$ for $t \in [0,T)$ . (my proof) let $g(t)=z(t)-z_0(t)$ for $t \in [0,T) $ then $g(0)=m-m=0, g'(0)=(a(0)-A)m^2>0$ therefore $g(t)$ satisfy the suggestion of my lemma.
therefore $\exists T^*\in (0,T)$ s.t $t \in [0,T^*)$ imply $g(t)\geq 0$ . actually, $g(t)>0$ for $t\in(0,T^*)$ . therefore $z(t)\geq z_0(t)$ for $t \in [0,T^*)$ , $z(t)> z_0(t)$ for $t\in(0,T^*)$ . Now for contradiction, suppose there exist $T_p \in [T^*,T)$ s.t $z_0(T_p)>z(T_p)$ ; i.e $g(T_p)<0$ . Then there exist $T_1\in [T^*,T_p)$ s.t
i) $g(T_1)=0$ ;
ii) $g(t)>0 $ for $t\in (0,T_1)$ iii) $\exists \eta>0$ s.t $g(t)<0$ for $t\in(T_1,T_1+\eta)$ i.e) there exist $T_1$ which $g(t)$ first pass through from positive value to negative value. then we can find contradiction, at $t=T_1$ , $g'(T_1)=a(T_1)z(T_1)^2-A z_0(T_1)^2=(a(T_1)-A)z_0(T_1)^2>0 $ note that $z_0(T_1)= m/(1-mAt)>0$ by my lemma,there exist $T^{**}\in (T_1,T)$ s.t $t\in [T_1,T^{**})$ imply $g(t)\geq 0$ ; therefore $z(t)-z_0(t)=g(t)\geq 0 \ \ \forall t\in [0,T)$ . $z_0 is lower bound of z.$ this is my proof, but I think there are some errors in my proof. first, z is the solution for $dz/dt=a(t)z^2, \ \ \ z(0)=m$ in $(0,T)$ but I use $z'(0)-z_0'(0)>0$ for my proof, however $z'$ is not define for $t=0$ second, I use ""there exist $T_1$ which $g(t)$ first pass through from positive value to negative value."", but actually, I don't know how to clarify this sentence. If you have some idea to clarify my proof,or If you have better idea to prove ""Since A is lower bound for $a(t)$ , it follows easily that $z_0(t)$ is lower bound for $z(t)$ for all positive t."", please give me some help.","['partial-differential-equations', 'ordinary-differential-equations', 'real-analysis']"
4230187,"Faster Algorithm for counting non-negative tripple(a, b, c) satisfied (a + b + c <= S) and (a * b * c <= T) and subproblem divisor summatory function","Statement This problem Code Used this Paper Given $S, T (0 \leq S, T \leq 10^{18})$ I need to count the number of tuple $(a, b, c)$ satisfied $(\min(a, b, c) \geq 0)$ and $(a + b + c \leq S)$ and $(a \times b \times c \leq T)$ Combinatorics Approach I tried to fix $a$ to be the minimum $\Rightarrow (0 \leq a \leq \sqrt[3]{T})$ then count the number of pair $(b, c)$ satisfied $(\min(b, c) \geq a)$ and $(b + c \leq S - a)$ and $(b \times c \leq \lfloor \frac{T}{a} \rfloor)$ . Because some tuple $(a, b, c)$ may equal to $(a, c, b), (b, a, c), (b, c, a), (c, a, b)$ or $(c, b, a)$ (example $a = b$ ), while some may not (example $a < b < c$ ). Therefore I split into 4 cases Case [1] $\Rightarrow$ $0 \leq a < b < c$ and $(a \leq \sqrt[3]{T})$ $cnt_1 = \overset{0}{\underset{a = 0}{\LARGE\sum}} \overset{S}{\underset{b = 1}{\LARGE\sum}}\overset{S - b}{\underset{c = b + 1}{\LARGE\sum}} I(1) + \overset{\min(S, \lfloor \sqrt[3]{T} \rfloor)}{\underset{a = 1}{\LARGE\sum}} \overset{\min(S - a, \lfloor \frac{T}{a} \rfloor)}{\underset{b = a + 1}{\LARGE\sum}}\overset{\min(S - a - b, \lfloor \frac{T}{ab} \rfloor)}{\underset{c = b + 1}{\LARGE\sum}} I(1)$ $= \overset{S}{\underset{b = 1}{\LARGE\sum}} \max(0, S - 2b) + \overset{\min(S, \lfloor \sqrt[3]{T} \rfloor)}{\underset{a = 1}{\LARGE\sum}} \overset{\min(S - a, \frac{T}{a})}{\underset{b = a + 1}{\LARGE\sum}} \max(0, \min(S - a - b, \lfloor \frac{T}{ab} \rfloor) - b)$ $ = \lfloor \frac{S}{2} \rfloor \times S - 2 \times \overset{\lfloor \frac{S}{2} \rfloor}{\underset{x = 1}{\LARGE\sum}} I(x) + \overset{\min(S, \lfloor \sqrt[3]{T} \rfloor) }{\underset{a = 1}{\LARGE\sum}} \Huge($ $\overset{ f(a) }{\underset{x = 1}{\LARGE{\sum}}} \large{I(x)} + (S - a)(g(a) - a) - \overset{g(a)}{\underset{x = a + 1}{\LARGE\sum}} I(x) + \overset{f(a)}{\underset{x = g(a) + 1}{\LARGE\sum}} \lfloor \frac{T}{xa} \rfloor $$\Huge)$ for $I(x) = x$ for $f(a) = \Big\lfloor \min\Big(\frac{S - a}{2} \, \frac{T}{2a} + 1, \sqrt{\lfloor \frac{T}{a} \rfloor} \Big) \Big\rfloor$ and $g(a) = \max\Large(\normalsize k\ |\ S - a - k \leq \lfloor \frac{T}{a} \rfloor\Big)$ Case [2] $\Rightarrow$ $0 \leq a < b = c$ and $(a \leq \sqrt[3]{T})$ $cnt_2 = \overset{0}{\underset{a = 0}{\LARGE\sum}} \overset{\lfloor \frac{S}{2} \rfloor}{\underset{b = 1}{\LARGE\sum}} \overset{b}{\underset{c = b}{\LARGE\sum}} I(1) + \overset{\min(S, \lfloor \sqrt[3]{T} \rfloor)}{\underset{a = 1}{\LARGE\sum}} \overset{\min(\frac{S - a}{2}, \sqrt{\lfloor \frac{T}{a} \rfloor})}{\underset{b = 1}{\LARGE\sum}} \overset{b}{\underset{c = b}{\LARGE\sum}} I(1)$ $= \lfloor \frac{S}{2} \rfloor + \overset{min(S, \lfloor \sqrt[3]{T} \rfloor)}{\underset{a = 1}{\LARGE\sum}} max\Large( \normalsize 0, \Large \lfloor \normalsize min\Large(\normalsize \frac{S - a}{2}, \sqrt{\lfloor \frac{T}{a} \rfloor} \Large) \rfloor \normalsize - a \Large)$ Case [3] $\Rightarrow$ $0 \leq a = b < c$ and $(a \leq \sqrt[3]{T})$ $cnt_3 = \overset{0}{\underset{a = 0}{\LARGE\sum}} \overset{a}{\underset{b = a}{\LARGE\sum}} \overset{S}{\underset{c = 1}{\LARGE\sum}} I(1) + \overset{\min(\lfloor \frac{S}{2} \rfloor, \lfloor \sqrt[3]{T} \rfloor)}{\underset{a = 1}{\LARGE\sum}} \overset{a}{\underset{b = a}{\LARGE\sum}} \overset{\min(S - a - b, \lfloor \frac{T}{ab} \rfloor)}{\underset{c = b + 1}{\LARGE\sum}} I(1)$ $ = S + \overset{\min(\lfloor \frac{S}{2} \rfloor, \lfloor \sqrt[3]{T} \rfloor)}{\underset{a = 1}{\LARGE\sum}} \max\Large(\normalsize 0, min\Large(\normalsize S - 2a, \Large \lfloor \normalsize \frac{T}{a^2} \Large \rfloor \Large)\normalsize - a\Large)$ Case [4] $\Rightarrow$ $0 \leq a = b = c$ and $(a \leq \sqrt[3]{T})$ $cnt_4 = \overset{min(\frac{S}{3}, \sqrt[3]{T})}{\underset{a = 0}{\LARGE\sum}} \overset{a}{\underset{b = a}{\LARGE\sum}}\overset{b}{\underset{c = b}{\LARGE\sum}} I(1)$ $ = \lfloor \min(\frac{S}{3}, \sqrt[3]{T}) \rfloor + 1$ Now result = number of different tuple $(a, b, c)$ satisfied the condition In case [1] $(a, b, c) \neq (a, c, b) \neq (b, a, c) \neq (b, c, a) \neq (c, a, b) \neq (c, b, a)$ In case [2] $(a, b, c) \neq (b, c, a) \neq (c, a, b)$ In case [3] $(a, b, c) \neq (b, c, a) \neq (c, a, b)$ In case [4] $(a, b, c)$ is unique Therefore, $result = 6 \times cnt_1 + 3 \times cnt_2 + 3 \times cnt_3 + 1 \times cnt_4$ Algorithm Complexity It is known that $\overset{r}{\underset{x = l}{\Large\Sigma}} \normalsize (x) = \frac{r(r+1)}{2} - \frac{l(l-1)}{2}$ when $l \leq r$ and $\overset{r}{\underset{x = l}{\Large\Sigma}} \normalsize (x) = 0$ otherwise. Therefore this only take $O(1)$ complexity Let $f(p, q, x) = \overset{min(q, \lfloor \frac{T}{x} \rfloor)}{\underset{k = 1}{\Sigma}} \lfloor \frac{T}{xk} \rfloor$ There is $O(\sqrt{T})$ solution based on the fact that the number of different floor value in that series is $O(\sqrt{T})$ There is also a $O(\sqrt[3]{T} log(T))$ solution based on the fact that the number of different slopes in from the graph of that series is $O(\sqrt[3]{T})$ The overall complexity should be $O\Big(\normalsize \overset{min(S, \lfloor \sqrt[3]{T} \rfloor)}{\underset{a = 1}{\Huge \Sigma}} \LARGE \sqrt[3]{\Large \lfloor \normalsize \sqrt[2]{\Large \lfloor \normalsize \frac{T}{a}} \Large \rfloor} \Large \rfloor \Large)  \Big)$ I expected the to be $O(\sqrt{T})$ or even be $O(\sqrt[3]{T})$ as it would in theory But it seems to be about $O(T^{\frac{2}{3}})$ since I used many divisions and the correct solution (removed approximation part) for slope trick is not $O(T^{\frac{1}{3}})$ but much slower in theory (but still faster then all of my old correct-codes because it is division free as paper said) UPDATE My real complexity is only $O\Huge(\normalsize \overset{min(S, \lfloor \sqrt[3]{T} \rfloor)}{\underset{a = 1}{\Huge \Sigma}} \LARGE( \normalsize log_2(\Large \lfloor \normalsize \sqrt{\frac{T}{a}} \Large \rfloor) \LARGE + \Large \lfloor \normalsize \normalsize \frac{T}{a} \Large \rfloor \LARGE ^{^{\normalsize 1/3}}) \Huge) \normalsize = O\Large( \int_{_{\normalsize 1}}^{^{\normalsize T^{1/3}}}\frac{\normalsize  T^{1/3}}{\normalsize  a^{1/3}} \normalsize da \Large) \normalsize = O(T^{5/9})$ . Though I calculate upto $\Large \lfloor \normalsize min \Large( \normalsize (S - a) / 2, \sqrt{\frac{T}{a}} \Large) \rfloor$ only, but the number of unique value is still based on $\lfloor \frac{T}{a} \rfloor$ . Therefore make my mistake I also just remove the $\sqrt[3]{T}$ multiplying part as my mistake while writing latex. I also used derivatives to give closer bound Other known property It is easy to known that by not fixing $a$ as minimum, you can also achieve $O(S \times \sqrt[3]{T})$ . By not depending on $O(f(t))$ factor, the complexity is $O(S^2)$ but might be possible to improve There is special case when $S \leq \sqrt[3]{T}$ then we have $0 \leq a + b + c \leq S \leq \sqrt[3]{T} \leq T$ . Therefore we get rid of the product-limitation condition part and the answer will be $\frac{(S+1)(S+2)(S+3)}{6}$ When $S = T = n$ , the number of solution is approximately $1.5n^2$ Tested with 10.000.000 numbers $n \leq 10^9$ using this algorithm My Question: Is there a way to run with smaller $a$ but not to increase the counting (b, c) part's complexity Is there a good formula to calculate $f(p, q, x)$ from $f(p, q, x - 1)$ or even from $f(l, r, x - 1)$ for some $(l, r)$ Any of these questions below if it is solvable then the whole problem is solvable for $O(\sqrt[3]{T})$ ! Is there an alternative formula which can run in $O(\sqrt[2](T))$ or even $O(\sqrt[3](T))$ ? Is there a faster algorithm to calculate $f(p, q, x)$ ? Can I calculate $f(p, q, x)$ from $f(l, r, x - 1)$ for some $(l, r)$ faster than $O(log(T))$ ? Given a array $p[]$ of $T$ numbers where $p[i] = \lfloor \frac{T}{i} \rfloor$ . Given $O(\sqrt[3]{T})$ queries $Q(l, r, x) = \overset{r}{\underset{x = l}{\Sigma}} p[x]$ . Can this be solved in $O(\sqrt[3]{T})$ after somehow compressing p[] and optimizing the calculation ?","['combinatorics', 'harmonic-functions', 'algorithms']"
4230194,"Solving a second order ODE of the form $y'' + P(x)y' + Q(x)y = f(x)$, given one part of the solution","I have the following ODE $$x(x-1)y'' - (2x-1)y' + 2y = 2x^3-3x^2$$ where I'm given $$y_1 = x^2$$ Now, my workbook states that if we encounter such an ODE, where one part of the solution is given, the other part $y_2$ is $$y_2 = y_1 \int \frac{e^{-\int P(x)dx}}{y_1^2}dx$$ After I divide my equation by $x(x-1)$ and substituting into the formula for $y_2$ , I get $$y_2 = x^2\int\frac{(x-1)x}{x^4}$$ $$y_2 = \frac{1-2x}{2}$$ Now, $$y=c_1x^2 + c_2\frac{1-2x}{2}$$ Which is the solution of the homogenous equation. In order to find the particular solution, I used variation of constants, where I got the system: $$
\left\{ 
\begin{array}{c}
c_1'x^2 + c_2'\frac{1-2x}{2} = 0 \\
c_1' 2x - c_2' = \frac{2x^2 - 3x}{x-1}
\end{array}
\right. 
$$ The solution for which are $c_1' = \frac{(2x-3)(1-2x)}{x-1}$ and $c_2' = \frac{x^3(2x-3)}{x(x-1)^2}$ Which after integrating end up being $$c_1 = \ln|x-1| - 2(x-1)^2 + C_1$$ and $$c_2 = \frac{x^3}{x-1} + C_2$$ After plugging in into the solution, I get $$y = x^2 ( \ln|x-1| - 2(x-1)^2 + C_1) + \frac{1-2x}{2}(\frac{x^3}{x-1}+C_2)$$ However, this is not the same as the solution in my workbook which is $$y=C_1x^2 + C_2(-x+\frac{1}{2}) + x^3 - \frac{x^2}{2} + x-\frac{1}{2}$$ Where did I go wrong?",['ordinary-differential-equations']
4230198,Trace thought of as a non-degenerate symmetric bilinear form over $End_{\Bbb{R}}V$,"Let $V$ be a real finite-dimensional vector space of dimension $n$ . Let $End_{\Bbb{R}}V$ be the real vector space of linear mappings from $V$ to $V$ . $End_{\Bbb{R}}V$ has dimension $n^2$ . The trace is the unique (up to scale) $Tr\in(End_{\Bbb{R}}V)^{*}$ such that $Tr(A\circ B)=Tr(B\circ A)$ for every $A,B\in End_{\Bbb{R}}V$ . Normally we take the one that verifies $Tr(Id)=n$ , where $Id$ is the identity endomorphism of $V$ . This defines a non-degenerate symmetric bilinear form: $B_{Tr}:End_{\Bbb{R}}V\times End_{\Bbb{R}}V\to \Bbb{R}$ , $B_{Tr}(A,B):=Tr(A\circ B)$ . It is a well known result that non-degenerate symmetric bilinear forms over a real finite-dimensional vector space are classified by the index and the signature , so I wonder: Is there a formula to calculate the index and the signture of $B_{Tr}$ (maybe in terms of the dimension)? In other words, how do I find the corresponding $\pm1s$ ? There are $n^2$ of them. Who is an orthonormal basis for $(End_{\Bbb{R}}V,B_{Tr})$ ? In order to solve this, does it help the fact that the orthogonal complement of the symmetric endomorphisms are the alternating ones and viceversa? For the answerers: I have no knowledge about algebraic geometry. If you are able to connect your answer with ideas of the field of differential geometry it will be appreciated (I'm teaching myself it and that's how I came up with this question). Though coordinates help to understand things, finish with free-coordinate arguments will be appreciated too. Still any help will be welcome. Thanks in advance.","['trace', 'linear-algebra', 'bilinear-form', 'linear-transformations']"
4230209,Convergent subsequence of $x^n$ to the identity element in a compact group,"The following question was recently asked by the user @Héhéhé and was downvoted twice (as usual, without any comments), which led the OP to censor himself by deleting his question. As far as I know, this question is interesting and nontrivial. I will provide an answer within a few hours if nobody answers, but I am confident that the downvoters will propose their own answer in the meantime. Consider the following statement: Let $G$ be a compact group with identity element $e$ . For all $x \in G$ , the sequence $(x^n)_{n\in \mathbb N}$ has a subsequence that converges to $e$ . Is it true? If yes can you provide a proof (or a source to a proof)? If no can you provide a counter-example? If necessary, we can restrict ourselves to the case where $G$ is a metric space.","['convergence-divergence', 'topological-groups', 'compactness', 'sequences-and-series']"
4230215,Hitchin's definition of tangent space and tangent vectors,"In page 18 of N. Hitchin the tangent space $T_pM$ of a manifold $M$ in a point $p$ is defined as the dual of $T^\star_p M$ where $T^\star_pM $ is the quotient space: $$C^\infty(M)/Z_p(M) \quad\text{and}\, Z_p(M)=\big\{f\in C^\infty(M): d(f\circ \varphi^{-1
})_p=0 \text{ for all }(\mathcal U,\varphi) \textrm{
local chart in }p\big\}$$ A tangent vector at $p$ is not defined as an element of $T_pM$ . Instead it is defined as a linear map $X_p:C^\infty(M)\to \mathbb R$ that satisfies the Leibniz rule: $$X_p(fg)=X_p(f)g(p)+f(p)X_p(g)$$ Then Hitchin proves that $T_pM$ is isomorphic to the annihilator $Z_p(M)^\circ$ of $Z_p(M)$ in $C^\infty(M)$ and each tangent vector belongs to $Z_p(M)^\circ$ . 1.- Why a tangent vector cannot be defined simply as an element of $T_pM$ ? 2.- Are there elements of $T_pM$ that do not satisfy the Leibniz Rule?","['tangent-spaces', 'smooth-manifolds', 'quotient-spaces', 'manifolds', 'differential-geometry']"
4230227,Metric spaces: equivalence of neighborhood definition of continuity with usual one. Don't we need a constraint asserting existence of neighborhoods?,"From Kaplansky Set theory and metric spaces p. 80: Let $X,Y$ be metric spaces, let $f$ be a function $f: X \to Y$ , let $x_0$ be a point in $X$ , and let $y_0 := f(x_0)$ . Prove $f$ is continuous at $x_0$ if and only if, for any neighborhood $V$ of $y_0$ , there exists a neighborhood $U$ of $x_0$ with $f(U) \subset V$ . The author defines a neighborhood of $x$ as a subset of the metric space containing at least one open set that contains $x$ . Perhaps I don't understand the material very well yet, but don't we need to add an additional constraint to the theorem stating something along the lines ""there is at least one neighborhood of $y_0$ "", or something analogous? Maybe also for $x_0$ ? Because the proof requires as a first statement: ""Let V be an arbitrary neighborhood of $y_0$ "". But since the function we've defined is completely arbitrary, we know next to nothing about it, and so it might actually have no neighborhoods. Is there maybe another way to implicitly derive the existence of neighborhoods from what is already given that I'm not seeing? From continuity maybe? Thanks for any help!","['continuity', 'general-topology', 'real-analysis']"
4230260,Proving the $(n+1)$th moment of a continuous random variable,"Let $X$ be a continuous random variable with the probability density function $$
f(x)=\left\{\begin{array}{cc}
\frac{x \sin x}{\pi}, & 0<x<\pi \\
0, & \text { otherwise. }
\end{array}\right.
$$ Prove that $\mathbb{E}\left(X^{n+1}\right)+(n+1)(n+2) \mathbb{E}\left(X^{n-1}\right)=\pi^{n+1}$ . I've tried computing the first part $E(X^{n+1})$ and here's what I get: Proof Let $k = n+1$ . Notice that $$F_X(x) = \frac{1}{\pi}\int_0^{x} y \sin (y) \,dy = \frac{1}{\pi}(\sin(x) - x \cos(x)) $$ Then, using the identity $E(X^n) = n \int_0^{\infty}x^{n-1}(1-F_X(x)) \,dx$ , $$E(X^k) = k \int_0^{\pi} x^{k-1} (1-Fx(x)) \,dx$$ $$= k \int_0^{\pi} x^{k-1} (1-\frac{1}{\pi}(\sin(x) - x \cos(x)) \,dx $$ $$= k \int_0^{\pi} x^{k-1} - \frac{x^{k-1}}{\pi} \sin(x) - \frac{x^k}{\pi} \cos(x) \,dx $$ But I can't compute the definite integral of $x^k \cos(x) \,dx$ . Well, at least not in the course I'm doing, we are not learning gamma functions. The solutions say to ""use integration by parts twice"" and that's it. Is the way I'm going about this just completely wrong? How else could I prove this?","['expected-value', 'probability-distributions', 'probability', 'random-variables']"
4230273,What is the logical flaw in this reasoning? Abusing $T \equiv T \vee F$.,"IMPORTANT EDIT: I've noticed that people are focussing too much on my examples that they're distracted by my real question which is logic based. I use various examples to explain my ""logic based"" confusion, I'm not confused about the examples. I do not require explanations for my examples. This question stems from a question I asked long long long time ago and someone answered that it is incorrect to write $|x|=±x$ and I took their word for it because well... I wasn't experienced enough to ask the right questions. Since then my instinct to deal with $|x|$ has been to use: $$ |x|= \begin{cases} x, \ x≥0 \\\\ -x, \ x <0 \end{cases}$$ because well... that is the definition of $|x|$ . However I was going through my questions yesterday, when I realised, ""Wait hold on, why is the equation $|x|=±x$ incorrect? Because "" $±$ "" means ""plus OR minus"", you're not insisting that $|x|$ is $x$ AND $-x$ . You're only saying it is either $x$ OR $-x$ . But hold on, there's more. With that argument in mind, you can always write $\sqrt{9}=±3$ even though it's just $3$ . You can even go more bonkers with this logic by writing $$\sqrt{9} = 3 \text{ or } -3 \text{ or } -193e^2$$ as long as one of them is true. You get the point, right? You can keep adding on nonsense using the fact that $T \equiv T \vee F$ like so: $$\sin x = 0 \quad\equiv\quad \sin x =0 \;\text{ or }\; \cos x = 0$$ and then get $x \in \{\frac{nπ}{2} : n \in \mathbb{Z}\}$ as the solution, which is absurd. Where is the logical error here?","['propositional-calculus', 'absolute-value', 'logic', 'notation', 'algebra-precalculus']"
4230292,Find $\log_e3 - \frac{\log_e9}{2^2} + \frac{\log_e27}{3^2} - \frac{\log_e81}{4^2} + ...$,"Find $\log_e3 - \dfrac{\log_e9}{2^2} + \dfrac{\log_e27}{3^2} - \dfrac{\log_e81}{4^2} + ...$ What I Tried :- This is the same as :- $$\ln3 - \frac{\ln3}{2} + \frac{\ln3}{3} - \frac{\ln3}{4} + \dots$$ $$\rightarrow \ln3\bigg(1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \dots\bigg) $$ $$\rightarrow \ln3\Bigg(\sum_{n=1}^{\infty} (-1)^{n+1}\frac{1}{n}\Bigg)$$ Now, this summation's value came to be $\log 2$ , which I came to know after using Wolfram Alpha, but I am not sure how this came. Can someone help me? Also, the answer should be $(\ln 3)(\log 2)$ . $\rightarrow (\ln 3)\bigg(\dfrac{\ln 2}{\ln 10}\bigg).$ But I have the options, in my question, as :- $(a) (\log_e2)(\log_e3)$ $(b) (\log_e2)$ $(c) (\log_e3)$ $(d) \dfrac{\log_e5}{\log_3}$ So none of the options match. Can someone confirm? Also, can someone show me how the summation results to $\log 2$ ? Thank You.","['algebra-precalculus', 'logarithms', 'problem-solving', 'sequences-and-series']"
4230360,Proof matrix is invertible given matrix function,"Given that an $n$ by $n$ matrix $\mathbf{M}$ is defined as: $$\mathbf{M}_{j,i}=\frac{G(j)}{(j-\alpha)^i}$$ where $\forall k\in\mathbb{N}$ , $ G(k)\neq0$ and $\alpha\not\in\mathbb{N}$ . Show that the matrix $\mathbf{M}$ is invertible. I have tried using row reduction to find a formula for the determinant however it quickly becomes hard to do any operation that simplifies the matrix any further. So far I did: Divide every row by $G(j)$ , this gets rid of the $G(x)$ function entirely. Then I multiplied $R_n$ and $R_{n-1}$ by $(n-\alpha)$ and $(n-1-\alpha)$ respectively followed by making $R_n=R_n-R_{n-1}$ which got me a $0$ at the bottom left corner of the matrix however the rest of the terms became too complicated to deal with.","['matrices', 'algebra-precalculus', 'functions', 'linear-algebra']"
4230381,Extending Ky Fan's eigvenalues inequality to kernel operators,"Base result The following result in Terry Tao's book, 'p. 47, Ky Fan inequality' reads as: $$\sum_i\lambda_i(A+B) \leq \sum_i \lambda_i(A) + \lambda_i(B)$$ where $A, B$ are Hermitian matrices and $\lambda_i(X)$ with $X$ Hermitian denotes the $i$ 'th eigenvalue of $X$ , such that w.l.o.g. $\lambda_1(X) \geq \dots  \geq \lambda_n(X).$ Going from Hermitian matrices to kernel operators Consider the operator induced by a stationary positive semi-definite kernel $k(x, y) := k(\Vert x - y \Vert)$ $$Tf(x) = \int_0^1 k(x,y) f(y) d\mu$$ with $T \in L^2([0, 1], \mu)$ where $\mu$ is some arbitrary measure. I'm interested in Mercer kernels, i.e. kernels which have an eigenfunction expansion of the form $$k(x, y) := \sum_i \lambda_i \phi_i(x) \phi_i(y)$$ where $\big \langle \phi_i, \phi_j \big\rangle_{L^2(\mu)} = \delta_{ij}$ , and $\big\langle k(x, \cdot), \phi_i \big\rangle_{L^2(\mu)} = \lambda_i \phi_i(x)$ , and the  eigenvalues $\lambda_i$ decay very fast (i.e. polynomial and exponential decay). Actual Question I feel that Ky Fan's inequality should apply directly to kernel operators as well, i.e. considering $k_1(\cdot, \cdot), k_2(\cdot, \cdot)$ as before, under what conditions can we say $$\sum_i \lambda_i(k_1 + k_2) \leq \sum_i \lambda_i(k_1) + \lambda_i(k_2) $$ as in the Hermitian matrix case?","['eigenvalues-eigenvectors', 'functional-analysis', 'eigenfunctions', 'inequality', 'spectral-theory']"
4230394,Is a function that preserves the cross product necessarily linear in $\mathbb R^3$? $f(a) \times f(b) = a \times b$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Assume that $f: \mathbb{R^3} \rightarrow \mathbb{R^3}$ is a function such that $$
f(a) \times f(b)=a \times b
$$ for all $a,b \in \mathbb{R^3}$ , where '' $\times$ '' denotes the cross product in $\mathbb{R^3}$ .
Does $f$ have to be a linear mapping?","['functional-equations', 'cross-product', 'linear-algebra']"
4230403,When can remove absolute value symbol in ODE,"There are two examples $$
\frac{dy}{dx} = \frac{y}{x}
$$ and $$
2\frac{dy}{dx} = \frac{y}{x}
$$ The first one , we have $$
\ln |y| = \ln |x| + C_1 \Rightarrow |y| = e^{C_1}|x| \Rightarrow  y = Cx.
$$ Its absolute value sign can be removed and use a constants $C$ . The second one, we have $$
2\ln |y| = \ln |x| + C_1 \Rightarrow y^2 = e^{C_1}|x| \Rightarrow y^2 = C|x|.
$$ Its absolute value sign cant be removed. I use the above way to judge whether the absolute value can be removed. For the first one when $x > 0, y > 0$ , we have $$
	\ln y = \ln x + C_1 \Rightarrow y = C_2 x.
	$$ when $x < 0, y > 0$ , we have $$
	\ln y = \ln -x + C_1 \Rightarrow y = C_3 x.
	$$ when $x > 0 , y < 0$ , we have $$
	\ln -y = \ln x + C_1 \Rightarrow y= C_4 x.
	$$ when $x < 0, y < 0$ , we have $$
	\ln -y = \ln -x + C_1 \Rightarrow y = C_5 x.
	$$ when $y = 0$ , we have $$
	y = 0 \Rightarrow y = 0 \cdot x.
	$$ Thus, finally the solution can be $y = Cx$ . For the second, when $x > 0, y > 0$ , we have $$
y = C\sqrt{x}.
$$ when $x < 0, y > 0$ , we have $$
y = C\sqrt{-x}.
$$ Thus, its cant be removed absolute value. My doubt is there exists cheap way to judge the absolute value can be removed? Thanks in advance!","['ordinary-differential-equations', 'real-analysis']"
4230412,Linearity of derivative sanity check,"Currently I'm reading about the formalization of the derivative as a linear operator and more specifically reinterpreting the chain rule.  Given $f_1, f_2:\mathbb{R}^n\rightarrow\mathbb{R}^p$ and $g(x_1,x_2):\mathbb{R}^p\times\mathbb{R}^p\rightarrow\mathbb{R}^p$ where $(x_1,x_2)\mapsto \lambda x_1+x_2$ , lets take the derivative of $g(f_1, f_2)$ or in other words $\lambda f_1+ f_2$ . I want to apply the chain rule without using Jacobian matrices or partial derivatives too directly, so I used linear operator form: $$D(g\circ f)(a)=Dg(f(a))\circ D(f(a))$$ or $$D(g\circ f)=((Dg)\circ f)\circ Df$$ Here we know from the limit definition that $Dg=g=\lambda x_1 +x_2$ is equal to itself. If $f=(f_1,f_2)$ , I get $(Dg)\circ f=\lambda f_1+f_2$ as the first term and so the total derivative is $(\lambda f_1+f_2)\circ (Df_1, Df_2)$ which is totally incorrect. But we know $Df=(Df_1, Df_2)$ , so must $(Dg)\circ f$ be $g$ instead? This should be simple but I am probably misinterpreting the chain rule here.","['multivariable-calculus', 'derivatives']"
4230445,How does Kakutani's theorem imply consistency of the likelihood ratio estimator?,"I am currently trying to understand how consistency of the likelihood ratio estimator follows from Kakutani's theorem : Kakutani's theorem Let $(X_n)_{n\ge1}$ be a sequence of independent non-negative random variables of mean $1$ . Set $a_n:=E(\sqrt{X_n})\in(0,1]$ and define $M_0:=1,\,M_{n\ge 1}:=\prod_{i=1}^{n}X_i$ . Then $(M_n)_{n\ge 0}$ is a non-negative martingale and $M_n\to M_{\infty}$ a.s. for some random variable $M_\infty$ . Moreover, (a) if $\prod_n a_n>0,$ then $M_n\to M_{\infty}$ in $L^1$ and $E(M_{\infty})=1,$ (b) if $\prod_n a_n=0,$ then $M_{\infty}=0$ a.s. Corollary Let $P$ and $\tilde{P}$ be probability measures on a measurable space $(\Omega, \mathcal{F})$ . Let $(X_n)_{n\ge 1}$ be a sequence of random variables. Assume that, under $P$ (respectively $\tilde{P}$ ), the random variables $X_n$ are independent and $X_n$ has law $\mu_n$ (respectively $\tilde{\mu}_n$ ) for all $n$ . Suppose that $\tilde{\mu}_n=f_n\mu_n$ for all $n$ . Define the likelihood ratio $L_n:=\prod_{i=1}^{n}f_n(X_n)$ . Then, under $P$ , (a) if $\prod_n \int_{\mathbb{R}}\sqrt{f_n}d\mu_n>0$ , then $L_n$ converges a.s. and in $L^1$ , (b) if $\prod_n \int_{\mathbb{R}}\sqrt{f_n}d\mu_n=0$ , then $L_n\to 0$ a.s. In particular, if $\mu_n=\mu$ and $\tilde{\mu}_n=\tilde{\mu}$ for all $n$ with $\mu\neq \tilde{\mu}$ , then $P(L_n\to 0)=1,\ \tilde{P}(L_n\to\infty)=1$ . Since $f_n(X_n)$ is a random variable with mean $1$ under $P$ , I can understand that $P(L_n\to 0)=1$ in the corollary. However, where does $\tilde{P}(L_n\to\infty)=1$ come from? Furthermore, I have read that this implies consistency of the likelihood ratio statistic $L_n$ but for this we would have to fix real parameters for $\mu$ and $\tilde{\mu}$ . From the corollary, the obvious and only possible choices for $\mu$ and $\tilde{\mu}$ would be $0$ and $\infty$ , respectively. The problem here is that $\infty\not\in\mathbb{R}$ , or am I missing something e.g. that $\mathbb{R}\cup\{\infty\}$ is allowed as parameter?","['statistics', 'stochastic-processes', 'martingales', 'probability-theory', 'probability']"
4230463,Minimal Sufficient Statistics theorem 6.6.5 C&B proof,"The theorem goes as follows: Suppose the family of densities $\{f_0(x), .., f_k(0)\}$ all have common support. Then, (a) $$T(X) = \Big(\frac{f_1(X)}{f_0(X)}, \frac{f_2(X)}{f_0(X)}, .., \frac{f_k(X)}{f_0(X)}\Big)$$ is minimal sufficient for the family above. (b) If $\mathcal{F}$ is a family of densities with common support, and $\quad$ i) $f_i(x) \in \mathcal{F}, i= 0, 1, .., k$ $\quad$ ii) T(x) is sufficient for $\mathcal{F}$ then $T(x)$ is minimal sufficient for $\mathcal{F}$ . I already proved (a), but the textbook (Casella-Berger exercise 6.28) suggests to show that any sufficient statistic of $\mathcal{F}$ is a function of $T(x)$ defined in (a) somehow. I was thinking of picking a random set of densities from $\mathcal{F}$ , assuming that $T(x) = T(y)$ , and then showing that some arbitrary sufficient statistic $T'(x) = T'(y)$ for $\mathcal{F}$ is implied. But somehow got stuck.","['statistical-inference', 'statistics', 'probability']"
4230484,Turning sum into integral when the widths of the intervals are not uniform,"I have a sum of the form $$ S = \sum_\lambda f(\lambda), $$ where the ""index"" of the sum solves the equation $$ \tan(C \lambda) = \lambda.$$ This equation has a countably infinite number of solutions. One can see that as $C$ gets large, these solutions get closer and closer together, so it should be possible to express $S$ as an integral as $C \rightarrow \infty$ , but I am unclear how to achieve this. How can I introduce the vanishing width of the intervals into the sum to make a Riemann-type sum for $S$ ? Attempt: Set $$ S =  \lim_{N\rightarrow \infty}\sum_{n=-N}^N f(\lambda_n) $$ where $\tan(C\lambda_n) = \lambda_n$ and the $\lambda_n$ are sorted ( $\lambda_{n-1}< \lambda_n$ for all $n$ ).
Then $$ S = \lim_{N\rightarrow \infty} 2N\sum_{n=-N}^N f(2N\frac{\lambda_n}{2N})\frac{1}{2N} \sim \lim_{N\rightarrow \infty} 2N\int_{-N}^N f(2Nz)dz $$ Is this correct? Or am I missing something here? I suppose there are some strong constraints required on $f$ such that $S$ converges.","['integration', 'riemann-integration']"
4230488,"If $dx$ is a differential form in $\Gamma (V, \wedge^1 V^* \otimes V)$, what is $e^{idx}$?","I'm reading about the Thom form from in the paper: The Gauss-Bonnet-Chern Theorem on Riemannian Manifolds by Yin Li. In page 31, he defines the Berezin integral on a real vector space $V$ to be a nonzero linear map $B: \wedge^*V \rightarrow \mathbb{R}$ which vanishes on $\wedge^kV$ for $k < dim_\mathbb{R}V$ and he gives a canonical example of it given by projecting $\omega \in \wedge^*V^* $ onto $e_1 \wedge ... \wedge e_n$ where $e_1,...,e_n$ is a basis of $V$ . Then he says : Since the identity map $Id : V \rightarrow V$ can be identified with an element of $\Gamma (V, \wedge^0 V^* \otimes V)$ , we can take its exterior differential $dx \in \Gamma(V, \wedge^1 \otimes V)$ . Then the exponential $e^{idx}$ lies in $\Gamma (V, \wedge^*V^* \otimes \wedge^*V^*.$ We extend the Berezin integral to a map $B: \Gamma(V, \wedge^*V^* \otimes \wedge^*V^* \rightarrow \Gamma(V, \wedge^*V^*)$ by $$ B(\omega \otimes \xi)= \omega \otimes B(\xi) , \omega \in \Gamma(V,\wedge^*V^*), \xi \in \wedge^*V^*. $$ My problem is that I didn't understand the following steps of the proof of proposition 4.3.2 Proof: Let $\lbrace e_k \rbrace$ be the dual basis of $dx^k,$ we have $$B(e^{-idx}) = B(\prod_{k=1}^n (1- idx^k \otimes e_k)) = {(-i)}^n B ((dx^1 \otimes e_1) \wedge...\wedge (dx^n \otimes e_n)).$$ My understanding of $dx$ which is viewed as an element of $\Gamma (V, \wedge^1 V^* \otimes V)$ is to be $dx = dx^1 \otimes e_1 +...+ dx^n \otimes e_n$ but I don't know how he calculates $e^{idx}$ to get the formulas above ?","['differential-forms', 'differential-geometry']"
4230506,A Certain Quotient Space,"Learning about the quotient maps and quotient spaces, the whole ""gluing"" process interested me. While experimenting, I struggled to visualize a space on the quotient space $[0,1] \times [0,1]/ \sim $ with the equivalence relation $(s,0) \sim (0, 1-s)$ . where s $\in$ [0,1]. I tried to form 2 circles connecting the point $ (0,0) , (0,1)$ and $(1,0)$ together. Then, similar to the process of the space of Klein Bottle, I inverted one of the circles and connected it to the other one. Is my thought process correct? Is there an easier way of visualizing and does that space have a name?","['general-topology', 'quotient-spaces']"
4230572,Computing $\sum_{n=2}^{\infty}\frac{1}{(n^2-1)^2}=\frac{\pi^2}{12}-\frac{11}{16}$,"I want to compute the following infinite sum $$\sum_{n=2}^{\infty}\frac{1}{(n^2-1)^2}=\frac{\pi^2}{12}-\frac{11}{16}$$ To this goal, my strategy is to first compute $\sum_{n=2}^{\infty}\frac{1}{n^2-a^2}$ , then take the derivative of the result with respect to $a$ and let $a \rightarrow 1$ . $$
\begin{aligned}
\sum_{n=2}^{\infty}\frac{1}{n^2-a^2}&=\frac{1}{2a}\sum_{n=2}^{\infty}\left(\frac{1}{n-a}-\frac{1}{n+a}\right)\\
&=\frac{1}{2a}\sum_{n=2}^{\infty}\left(\int_0^1t^{n-a-1}dt-\int_0^1t^{n+a-1}dt\right)\\
&=\frac{1}{2a}\sum_{n=2}^{\infty}\left(\int_0^1t^{-a}t^{n-1}dt-\int_0^1t^{a}t^{n-1}dt\right)\\
&=\frac{1}{2a}\left(\int_0^1t^{-a}\left(\sum_{n=2}^{\infty}t^{n-1}\right)dt-\int_0^1t^{a}\left(\sum_{n=2}^{\infty}t^{n-1}\right)dt\right)\\
&=\frac{1}{2a}\left(\int_0^1t^{-a}\left(\sum_{n=1}^{\infty}t^{n}\right)dt-\int_0^1t^{a}\left(\sum_{n=1}^{\infty}t^{n}\right)dt\right)\\
&=\frac{1}{2a}\left(\int_0^1t^{-a}\left(\frac{t}{1-t}\right)dt-\int_0^1t^{a}\left(\frac{t}{1-t}\right)dt\right)\\
&=\frac{1}{2a}\left(\int_0^1\frac{t^{1-a}}{1-t}dt-\int_0^1\frac{t^{a+1}}{1-t}dt\right)\\
&=\frac{1}{2a}\int_0^1\frac{t^{1-a}-t^{a+1}}{1-t}dt\\
\end{aligned}
$$ Recall the result $$\int_0^1\frac{t^{z-1}-t^{w-1}}{1-t}dt=\psi(w)-\psi(z)$$ Hence $$\sum_{n=2}^{\infty}\frac{1}{n^2-a^2}=\frac{1}{2a}\left(\psi(a+2)-\psi(2-a) \right) \tag{1} $$ As a check, letting $a \rightarrow 1$ in $(1)$ I obtained $$
\begin{aligned}
\sum_{n=2}^{\infty}\frac{1}{n^2-1}&=\frac{1}{2}\left(\psi(3)-\psi(1) \right)\\
&=\frac{1}{2}\left(-\gamma+\frac{3}{2}+\gamma \right)\\
&=\frac{3}{4}\\
\end{aligned}
$$ Which agrees with Wolfram . Then, I differentiated $(1)$ w.r. to $a$ $$
\begin{aligned}
2a\sum_{n=2}^{\infty}\frac{1}{(n^2-a^2)^2}&=\frac{\partial }{\partial a}\left(\frac{\psi(a+2)}{2a}-\frac{\psi(2-a)}{2a} \right) \\
&=\left(\frac{2a\psi^{\prime}(a+2)-2\psi(a+2)}{4a^2}+\frac{2a\psi^{\prime}(2-a)+2\psi(2-a)}{4a^2} \right) \\
\end{aligned}
$$ $$\sum_{n=2}^{\infty}\frac{1}{(n^2-a^2)^2}=\left(\frac{a\psi^{\prime}(a+2)-\psi(a+2)}{4a^3}+\frac{a\psi^{\prime}(2-a)+\psi(2-a)}{4a^3} \right) \tag{2} $$ Letting $a=1$ in (2) $$
\begin{aligned}
\sum_{n=2}^{\infty}\frac{1}{(n^2-1)^2}&=\left(\frac{\psi^{\prime}(3)-\psi(3)}{4}+\frac{\psi^{\prime}(1)+\psi(1)}{4} \right)\\
&=\left(\frac{2\left(\frac{\pi^2}{6}-\color{red} {\frac{5}{4}} \right)-2\left(-\gamma+\frac{3}{2} \right)}{8}+\frac{\frac{\pi^2}{3}-2\gamma}{8} \right)\\
&=\frac{\pi^2}{12}-\frac{11}{16}
\end{aligned}
$$ Edit , as pointed out by @user, $\psi^{\prime}(3)=\frac{\pi^2}{6}-\frac{5}{4}$ instead of $\psi^{\prime}(3)=\frac{\pi^2}{6}-2$ , which gives now the right answer.","['integration', 'digamma-function', 'sequences-and-series']"
4230589,How to show $\int_{0}^{+\infty} d x \frac{\sqrt{x}}{\left(x^{2}+1\right)^{2}}=\frac{\pi}{4 \sqrt{2}} $ with residue theorem,"I'm trying to calculate the following integral using the residue theorem. $$
I=\int_{0}^{+\infty} d x \frac{\sqrt{x}}{\left(x^{2}+1\right)^{2}}=\frac{\pi}{4 \sqrt{2}}
$$ But somehow I'm not getting the correct results, and I was hoping you could help spot where things go wrong? Because I have looked it through several times and can't seem to find anything wrong. Here is my approach: Define: $$
f(z)=\frac{(z)^{1 / 2}}{\left(z^{2}+1\right)^{2}}=\frac{(z)^{1 / 2}}{(z+i)^{2}(z-i)^{2}}
$$ f has poles of 2nd order at $z=\pm i$ .
And it has a branch point at $z=0$ . Maintaining $f$ as a single-valued function is fairly easy, if I define $z=|z|e^{i\theta} $ and restrict $\theta\in[0,2\pi]$ . I want to integrate $f$ over the following contour: $$
\oint f(z) d z=\int_{\Gamma^-} f(z) d z+\int_{c_{r}} f(z) d z+\int_{\Gamma^{+}} f(z) d z+\int_{C_{R}} f(z) d z=2 \pi i \operatorname{Res}(f, z=i)
$$ The integral along the infinite halfcircle ( $C_R$ where $R\rightarrow \infty$ ) converges to zero. The same goes for the infinitesimal circle $c_r$ as $r\rightarrow 0$ . The parametrization of $\Gamma^+$ gives the integral we are trying to solve: $$
\int_{\Gamma^{+}} f(z) d z=\int_{0}^{\infty} \frac{\sqrt{x}}{\left(x^{2}+1\right)^{2}} d x=I
$$ If we want to parametrize $\Gamma^-$ we can set $z=xe^{i\pi}$ and $dz=-dx$ and integrate from $-\infty$ to $0$ : $$
\int_{\Gamma^-} f(z) d z=\int_{-\infty}^{0} \frac{x^{1 / 2} e^{i \pi / 2}}{\left(x^{2} e^{2 i \pi}+1\right)^{2}}(-d x)
$$ but substituting $x=-u$ and $dx=-du$ , changing sign of limits and reversing the limits ends up being equivalent with $$
=\int_{0}^{\infty} \frac{\left(u\right)^{1 / 2}}{\left(u^{2}+1\right)^{2}} d u=I
$$ Calculating the residue of the enclosed pole: $$
\operatorname{Res}(f, z=i)=\lim _{z \rightarrow i}\left[\frac{d}{d z}(z-i)^{2} f(z)\right]=\lim _{z \rightarrow i}\left[\frac{d}{d z} \frac{z^{1 / 2}}{(z+i)^{2}}\right]=\frac{e^{-i \pi / 4}}{8}=\frac{1}{8}\left(\frac{\sqrt{2}}{2}-\frac{\sqrt{2}}{2} i\right)
$$ Which ends up giving: $$
\begin{aligned}
&\Rightarrow \oint f(z) d z=2 I=2 \pi i \frac{1}{8}(1-i) \frac{\sqrt{2}}{2} \\
&\Rightarrow I=\int_{0}^{\infty} \frac{\sqrt{x}}{\left(x^{2}+1\right)} d x=\frac{\pi}{8} \frac{\sqrt{2}}{2} i(1-i)=\frac{\pi}{8} \frac{\sqrt{2}}{2}(1+i)
\end{aligned}
$$ Which of course isn't quite right.","['complex-analysis', 'complex-integration', 'residue-calculus']"
4230648,Abstract Manifolds - Is $\mathbb R^m$ only a $C^1$ manifold?,"I have a question that came to my mind. First, this is the definition of an abstract manifold we once used in a lecture: Definition . Let $M$ be metric space and $\left(V_{i}\right)_{i\in I}$ be an open cover of $M$ with open sets $U_i\subset\mathbb R^m$ and homeomorphisms $F_i: U_i\rightarrow V_i$ . Then we say $M$ is an (abstract) $m$ -dimensional $C^k$ manifold if for all two open sets $V_1$ , $V_2\subset M$ with maps $F_1$ and $F_2$ the transition map $$F_{2}^{-1}\circ F_1: \quad F_1^{-1}(V_1 \cap V_2) \rightarrow F_{2}^{-1}(V_1\cap V_2)$$ is a $C^k$ diffeomorphism. (I am aware that one can use an even more general definition by requiring only the spaces $M$ to be only a topological one instead of a metric space, as is done in [1], but let's stick to this for now.) Question : I was asking myself whether $M = \mathbb R^m$ itself is an abstract $m$ -dimensional $C^k$ manifold and to what degree $k$ it is. I myself found until now that $M = \mathbb R^m$ is only a $m$ -dimensional $C^1$ manifold, is this really correct? Explanation : According to [2], we can cover $\mathbb R^m$ by ""[t]he collection of all open discs with rational radii and rational center coordinates"". As the open sets $U_i$ , I would choose the same sets, thus $U_i = V_i$ . Now choose as the homeomorphisms $F_i$ simply the identity, and we have for the transition map $F_{2}^{-1}\circ F_1 = \text{Id} \circ \text{Id} = \text{Id}$ , which is obviously a $C^1$ diffeomorphism. However, this is not a $C^2$ , $\dots$ , $C^{\infty}$ diffeomorphism as we do not have an inverse function for the first, second, etc. derivate of the identity mapping. [1] http://www.math.lsa.umich.edu/~jchw/WOMPtalk-Manifolds.pdf [2] https://www.quora.com/How-does-the-plane-R-2-with-the-usual-topology-satisfy-the-second-axiom-of-countability?share=1","['manifolds', 'differential-geometry']"
4230666,Derivative of a Lyapunov function for a nonlinear system,"Let $$\begin{aligned}\begin{cases}\dot{x}_{1}=-\left( 2x_{1}-x_{2}\right)^3+\left( x_{1}-x_{2}\right)  \\
\dot{x}_{2}= -\left( 2x_{1}-x_{2}\right) ^{3}+2\left( x_{1}-x_{2}\right)\end{cases}\\
 \end{aligned}$$ Given $$V\left( \mathbb{x}\right) =\mathbb{x}^{T}P\mathbb{x} \qquad P=\begin{bmatrix}
5 & -3 \\
-3 & 2
\end{bmatrix}$$ Answer: $\dot V\left( x_{1},x_{2}\right) =-2\left[ \left( 2x_{1}-x_{2}\right) ^{4}+\left( x_{1}-x_{2}\right) ^{2}\right]$ My attempt: We can write $V(\mathbb{x})$ as $$\begin{aligned}V\left( x_1,x_2\right) &=5x_{1}^{2}-6x_{1}x_{2}+2x_{2}^{2}\\
&=\left( 2x_{1}-x_{2}\right) ^{2}+\left( x_{1}-x_{2}\right) ^{2}\end{aligned}$$ and defining the change of variables $z_{1}=2x_{1}-x_{2}$ and $z_{2}=x_{1}-x_{2}$ we obtain $$V(z_1,z_2)=z_1^2+z_2^2$$ and $$\begin{cases}\dot{z}_{1}=-z_{1}^{3}+z_{2}\\
\dot{z}_{2}=-z_{1}^{3}+2z_{2}\end{cases}$$ Now we can calculate $\dot V(z_1,z_2)$ as $$\begin{aligned}\dot V\left( z_{1},z_{2}\right) &=\dfrac{\partial V}{\partial z_{1}}\left( z_{1},z_{2}\right) \dot{z}_{1}+\dfrac{\partial V}{\partial z_{2}}\dot{z}_{2}\\
&=2z_{1}\left( -z_{1}^{3}+z_{2}\right) +2z_{2}\left( -z_{1}^{3}+2z_{2}\right) \end{aligned}$$","['stability-in-odes', 'multivariable-calculus', 'stability-theory', 'lyapunov-functions']"
4230676,No Simple Group of Order 1080,"I came up with what I think is a nice elementary proof, and I just want to double-check there are no mistakes: Proof that there is no simple group $G$ of order $1080=2^33^35$ . Suppose $G$ is simple. The candidates for the number of $5$ -Sylow subgroups are $6$ , $36$ , and $216$ . $216$ is impossible as the $5$ -Sylows would then be in the centers of their normalizers, implying the existence of a normal $5$ -complement, by the Burnside transfer theorem. $6$ is impossible as it would imply a non-trivial homomorphism from $G$ to $\frak{S}_6$ (the transitive action of $G$ on the $6$ $5$ -Sylows), which is impossible as $|G|>|\frak{S}_6|$ and $G$ is simple.
Thus, a $5$ -Sylow $P_5$ has a normalizer $N_5$ of order $\frac{1080}{36}=30$ .  Since $P_5$ is cyclic of order $5$ , ${\rm Aut}(P_5)$ is cyclic of order $4$ .  Thus, the $3$ -elements in $N_5$ commute with the $5$ -elements.  Note that in $N_5$ , the resulting cyclic subgroup of order $15$ has index $2$ and thus is normal.  The (Sylow) $3$ -subgroup of $N_5$ is thus normal (and unique) as it is characteristic in a normal subgroup. Now consider a $3$ -element $z$ that centralizes a $5$ -element.  (These exist in $N_5$ .)  This exists in some $3$ -Sylow, and in a group of order $27$ every centralizer has order at least $9$ .  ( $p$ -groups have non-trivial centers, so non-central elements are centralized by themselves and and non-trivial element of the center.)  Thus, $|Z(z)|$ is divisible by $45$ .  However, the $5$ -Sylow can't be normal in $Z(z)$ as its normalizer in $G$ only has order $30$ .  The only candidates for the number of $5$ -Sylows in $Z(z)$ is $6$ , since if it were $36$ , it would be a normal subgroup (the set of all $p$ -Sylows is closed under conjugation).  Thus, each $3$ -element $z$ that centralizes a $P_5$ centralizes $6$ of them. But we saw above that each $P_5$ is normalized by a unique $3$ -subgroup $\langle z\rangle$ of order $3$ .  We now see that each such $\langle z\rangle $ normalizes $6$ $5$ -Sylows.  Since there are $36$ $5$ -Sylows, there are $6$ such $\langle z\rangle $ (one-sixth as many as there are $5$ -Sylow subgroups), and $G$ acts non-trivially on them by conjugation, and thus has a non-trivial homomorphism to $\frak{S}_6$ .  Thus $G$ cannot be simple.","['finite-groups', 'simple-groups', 'solution-verification', 'sylow-theory', 'group-theory']"
4230685,Finding an English translation to Kazimierz Kuratowski's finiteness paper.,"Dear Maths Stackexchange. I am currently doing some reading into the different notions of how finiteness is modelled in set theory. One formulation that took my eye was Kazimierz Kuratowski's formulation which seems to me rather elegant. Whist I have no problem with understanding the mathematics of this formulation, I am nonetheless interested in viewing the original  source paper for this construction. The earliest text I can find is given in this short three page paper here in French. Unfortunately I cannot read French, so this is of little use to me in the given state. I have done some poking around on google and google scholar and come up blank when looking for translations of said paper into English. Thus I open the following question up with the hopes that someone knows the location of an appropriate reference: Is there an English source to this paper?","['elementary-set-theory', 'reference-request']"
4230700,"Where is the fallacy in this ""proof"" of $\lim_{x \to a} x^2 = a^2$? [duplicate]","This question already has answers here : Spivak's Calculus (Chapter 5, Problem 41): Proof that $\lim_{x \to a} x^2 = a^2$ (3 answers) Closed 2 years ago . I just started reading Michael Spivak's Calculus and I am at Chapter 5 - Limits now. I tried doing the problems, and at the very end is an interesting and hard problem (problem 41) which I really can't see the solution: (This problem is in Spivak's personal view.) 41. After sending the manuscript for the first edition of this book off to the printer, I thought of a much simpler way to prove that $\lim_{x \to a} x^2 = a^2$ and $\lim_{x \to a} x^3 = a^3$ without going through all the factoring tricks on page 95. Suppose, for example, that we want to prove that $\lim_{x \to a} x^2 = a^2$ , where $a > 0.$ Given $\epsilon > 0$ , we simply let $\delta$ be the minimum of $\sqrt{a^2 + \epsilon}  -a$ and $a - \sqrt{a^2 - \epsilon}$ (see Figure 19); then $|x - a| < \delta$ implies that $\sqrt{a^2 - \epsilon} < x < \sqrt{a^2 + \epsilon}$ , so $a^2 - \epsilon < x^2 < a^2 + \epsilon$ , or $|x^2 - a^2| < \epsilon$ . It is fortunate these pages had already been set, so I couldn't make these changes, because this ""proof"" is completely fallacious. Wherein lies the fallacy? I don't know where the fallacy is, but either way, the ""proof"" that he gave was very promising and for me, it's hard to find the error. So what is the fallacy in Spivak's ""proof""?","['limits', 'calculus', 'fake-proofs']"
4230736,Is it necessary to consider absolute values when solving the differential equation $\frac{dy}{dx}-\frac{1}{x}y=1$?,"Given the differential equation $$\frac{dy}{dx}-\frac{1}{x}y=1$$ I'd like to solve it and understand where I should use absolute value functions and why or why not. This is a problem from MIT OCW's 18.03SC Differential Equations, and in their solutions they don't make any considerations for absolute values when integrating $x^{-1}$ in solving this differential equation. Edit: I failed to specify that the MIT OCW problem also specifies an initial value y(1)=7. I now think that perhaps they disregard negative x because the solution with this initial value will be the one with non-negative x, ie the solution to the right of the singularity at x=0. First, I find an integrating factor $$u(x)=e^{-\int x^{-1} dx}=e^{-\ln |x|}=|x|^{-1}$$ Then I multiply the differential equation by the integrating factor $$|x|^{-1}\frac{dy}{dx}-|x|^{-1}x^{-1}y=|x|^{-1}$$ This is where the absolute value starts to become important. $$\frac{d}{dx}(|x|^{-1}y)=|x|^{-1}$$ We can take the latter step because if we have $f(x)=|x|^{-1}$ then $$f'(x)=\begin{cases}
-x^{-2} \ \ x\geq 0\\
x^{-2}\ \ x\leq 0\\
\end{cases}=-\frac{1}{x|x|}$$ $$|x|^{-1}y=\int |x|^{-1}dx+C$$ For the next step I will use that $$\int |x|^{-1}dx=\begin{cases}
\int x^{-1}dx = \ln(x)\ \ x \geq 0 \\
- \int x^{-1} = -\ln(|x|)=-\ln(-x)\ \ x \leq 0 \\
\end{cases}$$ So $$y(x)=\begin{cases}
x \ln(x) +cx\ \ x \geq 0 \\
x \ln(-x)-cx\ \ x \leq 0 \\
\end{cases}$$ In the case of the official solution to this problem the integrating factor is simply $x^{-1}$ and so the solution is just $y(x)=x \ln(x) +cx$ , the same solution I have but in my case that solution is only valid for non-negative x. Which answer is the inaccurate one?",['ordinary-differential-equations']
4230740,"If $a^{2}+b^{2} \leq 4$, prove that $a+b \leq 4$","\begin{equation}
\text { If } a^{2}+b^{2} \leq 4 \text { prove that } a+b \leq 4 \text { }
\end{equation} What I have tried: \begin{equation}
a^{2} \leq a^{2}+b^{2} \leq 4 \text { and } b^{2} \leq a^{2}+b^{2} \leq 4
\end{equation} \begin{equation}
|a| \leq 2 \text { and }|b| \leq 2
\end{equation} \begin{equation}\text { So } a+b 
\text { can get the maximum value, then } a \geq 0 \text { and } b \geq 0 \text { }
\end{equation} \begin{equation}
\text {  } 0 \leq a \leq 2 \text { and } 0 \leq b \leq 2 \text {}
\end{equation} \begin{equation}
(a-b)^{2}=a^{2}+b^{2}-2 a b \geq 0
\end{equation} \begin{equation}
a^{2}+b^{2} \geq 2 a b \text {  So at this step I am not certain what to do next. }
\end{equation}","['algebra-precalculus', 'inequality']"
4230767,Prove a inequality property of convex function,"Problem: Let $f: \mathbb{R}^n \to \overline{\mathbb{R}}$ be the convex function. Let $x_1,x_3
\in E$ (Euclidean space in $\mathbb{R}^n$ ) and $x_2 \in (x_1,x_3)$ .
Prove that $$\dfrac{f(x_3) -f(x_2)}{\Vert x_3-x_2 \Vert} \ge\dfrac{f(x_2)-f(x_1)}{\Vert x_2-x_1\Vert}.$$ My attempt: Since $x_2 \in (x_1,x_3)$ then there exists $t \in (0,1)$ such that $x_2 = tx_1 + (1-t)x_3$ . Thus we have $$f(x_3) - f(x_2)  \ge f(x_3)-tf(x_1)-(1-t)f(x_3) = tf(x_3)-tf(x_1).$$ Therefore $$\dfrac{f(x_3)-f(x_2)}{\Vert x_3-x_2\Vert} \ge \dfrac{t}{\vert t \vert}\dfrac{f(x_3)-f(x_1)}{\Vert x_3-x_1\Vert} \ge -\dfrac{f(x_3)-f(x_1)}{\Vert x_3-x_1\Vert} = \dfrac{f(x_1)-f(x_3)}{\Vert x_1-x_3\Vert}.$$ I have tried many different ways to have a result like the solution above but I failed. I wonder that the problem is right or not?","['multivariable-calculus', 'convex-analysis', 'inequality']"
4230782,Compute $\frac 17$ in $\Bbb{Z}_3$,"Compute $\frac 17$ in $\Bbb{Z}_3.$ We will have to solve $7x\equiv 1\pmod p,~~p=3.$ We get $x\equiv 1\pmod 3.$ Then $x\equiv 1+3a_1\pmod 9,$ so $7(1+3a_1)\equiv 1 \pmod 9$ basically lifting the exponent of $p=3,$ we get $1+3a_1\equiv 4\pmod 9\implies a_1\equiv 1\pmod 3.$ So let $$x\equiv 1+3\cdot 1+3^2\cdot a_2 \pmod 2\implies 7(4+3^2\cdot a_2)\equiv 1\pmod {27}\implies 4+3^2\cdot a_2\equiv 4\pmod {27}\implies a_2\equiv 0 \pmod 3.$$ So let $$ x\equiv 1+3\cdot 1+3^2\cdot 0+ 3^3\cdot a_3 \pmod {81}\implies 7(4+3^2\cdot 0+3^3\cdot a_3)\equiv 1\pmod {81}\implies 4+3^3\cdot a_3\equiv 58\pmod {81}\implies a_2\equiv 2 \pmod 3.$$ So let $$ x\equiv 1+3\cdot 1+3^2\cdot 0+ 3^3\cdot 2+3^4\cdot a_4 \pmod {243}\implies 7(4+3^2\cdot 0+3^3\cdot 2+3^4\cdot a_4)\equiv 1\pmod {243}\implies 1+3+54+3^4\cdot a_4\equiv 139\pmod {243}\implies a_4\equiv 1 \pmod 3.$$ So let $$ x\equiv 1+3\cdot 1+3^2\cdot 0+ 3^3\cdot 2+3^4\cdot 1+ 3^5\cdot a_5 \pmod {729}\implies 7(4+3^2\cdot 0+3^3\cdot 2+3^4\cdot 1+3^5\cdot a_5)\equiv 1\pmod {729}\implies 1+3+54+81\equiv 625\pmod {243}\implies a_5\equiv 2 \pmod 3.$$ I haven't worked out but I think $a_6$ is $0.$ So the sequence we are getting is $(a_0,a_1,a_2,a_3,a_4,\dots)=(1,1,0,2,1,2,\dots).$ But I am not sure if it's correct, since it's not being periodic. Any help?","['number-theory', 'p-adic-number-theory', 'elementary-number-theory']"
4230789,How do I find number of ways to choose 10 cards from a set of 7 blue and 8 red cards,"If given a set of $7$ blue cards and $8$ red cards and asked to pick $10$ at random how do I figure out how many possible ways there are to select those cards? I got a bit stumped after realizing $2^{10}$ doesn't really help here and I'm not sure how to calculate it in a way that would account for the fact that you can run out of one type of card. I also would like to know how to figure out the number of ways to select $5$ red and $5$ blue cards exactly. Thank you for any help Edit to clarify:
The cards are identical but the order they are drawn is important to the question. For example, B B B B B R R R R R and R R R R R B B B B B would count as $2$ ways to draw the cards.","['combinatorics', 'problem-solving']"
4230804,"Is there any connection between logarithmic algebraic geometry and the ""field with one element""?","Monoid schemes (a.k.a. $\frak M$ -schemes) have been introduced by Deitmar as a possible approach to geometry over the field with one element. These build upon monoids as the basic building blocks for algebraic geometry in the place of rings. There's also a variant of Connes--Consani which uses pointed monoids instead of monoids, giving so-called $\frak Mo$ -schemes. Connes--Consani's framework is in a sense more general than Deitmar's, as the latter only gives toric schemes. On the other side, logarithmic schemes (a.k.a. log schemes) are a notion developed with a view to smoothness problems, especially in the theory of moduli, roughly allowing ""logarithmic singularities"" to be considered as smooth. One defines a log structure on a scheme $X$ to be a sheaf of commutative unpointed monoids $\mathscr{M}$ on $X$ together with a homomorphism of sheaves of monoids $\alpha\colon\mathscr{M}\to\mathscr{O}_X$ such that $\alpha^{-1}(\mathscr{O}^\times_X)\cong\mathscr{O}^\times_X$ . Since monoids, and even monoid schemes (see Ogus's CUP book, Part II, Section 1.2 ) appear prominently in both theories, it is natural to ask how the two interact with each other. In particular, from the above we see that a log structure on a scheme $X$ may be thought of as a morphism of $\frak M$ -schemes $\alpha$ from $(X,\mathscr{M})$ to $(X,\mathscr{O}_X)$ such that $\alpha^{-1}(\mathscr{O}^\times_X)\cong\mathscr{O}^\times_X$ . What is the precise connection/motivation, besides this superficial similarity, between the two theories? Furthermore, in the case of geometry over the field with one element, it is beneficial to consider not monoids, but pointed monoids, passing from $\frak M$ -schemes to $\frak Mo$ -schemes. Is it useful to work with log structures carrying sheaves of pointed commutative monoids instead of sheaves of unpointed commutative monoids? Edit. Ogus himself mentions $\bf F_1$ in a non-trivial way twice in his book, first on page 192: and then on page 213:","['monoid', 'algebraic-geometry']"
4230821,Why is $\frac{n}{2(n+1)^2}\leq\frac{1}{4}$?,"I have the following exercise in my textbook and I'm not completely sure about one thing in the answer: Denote $\mathbb{R}_{+}=(0,\infty)$ . Consider the probability space $\mathbb{R}_+,\mathcal{B}(\mathbb{R}_+),P)$ where $P$ is the exponentiat distribution $$dP(x)=e^{-x}\mathrm{dx}$$ Consider the random variables $$f_n(x)\exp\left(\frac{n}{2(n+1)^2)}x^{1/n}\right)$$ Where $n\in\mathbb{N}$ . Using the dominated convergence theorem, prove that the limit $$\lim_{n\to\infty}E(f_n)$$ exists and find it. The answer to the question is the following: We have $$\lim+{n\to\infty}\frac{n}{2(n+1)^2}=0$$ and for each $x\in\mathbb{R}_+$ , $$\lim_{n\to\infty}x^{1/n}=x^0=1$$ Therefore, for each $x\in\mathbb{R}_+$ , $$\lim_{n\to\infty}f_n(x)=f(x),$$ Where $f(x)=x.$ For $x\in\mathbb{R}_+$ , we have $$x^{1/n}\leq \text{max}\{1,x\}\leq 1+x$$ and $$\frac{n}{2(n+1)^2}\leq\frac{1}{4}$$ The answer continues further and I understand all the logic after it. However i'm not sure why is the last inequality true. It seems to me that the maximum the LHS can attain is $\frac{1}{8}$ , for $n=1$ , since $$\frac{1}{2(4)}=\frac{1}{8}$$ Why would the author put the bound at $\frac{1}{4}$ ?","['inequality', 'analysis', 'probability', 'real-analysis']"
4230843,Rate of Change of Volume with respect to Surface Area [duplicate],"This question already has answers here : Why is the derivative of a circle's area its perimeter (and similarly for spheres)? (8 answers) Closed 2 years ago . This question explains why the derivative of the volume of a sphere is equal to its surface
area. This is based on the logic using the differential. However, to what extent (and when can this be generalized). For example A. The derivative of the volume of a cube with side s does not equal its surface area. B. The derivative of the volume of a cube with side 2r does equal its surface area. What is going on here under the hood? Can anyone explain without using Maths beyond Calc I/II. Also, I am not so interested in understanding why the derivative of the area is equal to the circumfence, as in understanding which shapes in general exhibit this property and why. Note: This paper explains this question, but is quite technical. Summarizing the main results in less technical language would be useful.","['volume', 'geometry', 'calculus', 'related-rates', 'derivatives']"
4230858,Is convex set still convex when endowed with a different Riemannian metric?,"Say $V \in \mathbb{R}^n$ is a closed convex set in the Euclidean space. And therefore I am free to define the projection of $x$ to $V$ as the point in $V$ that minimizes the distance to $x$ . Now suppose  I endow $\mathbb{R}^n$ with a different riemannian metric $g$ , and embed $(V, g)$ isometrically to $\mathbb{R}^N$ by nash embedding. Is the new $V$ still convex with respect to $\mathbb{R}^N$ ? And if so, let $d(\cdot, \cdot)$ be the distance induced by the trace metric, is the projection still $arg min \; d(x, V)$ ?","['manifolds', 'riemannian-geometry', 'differential-geometry']"
4230899,Why does $\sin(t) + \cos(t)$ itself look like a sine graph?,"So the other night I was randomly python scripting. I plotted $\sin(t) + \cos(t)$ vs $t$ for $t$ ranging between $0$ to $100$ with spacings of Δt = 0.1 . (It is a pretty basic code...) Anyhow, the below plot is the result. Why does it look like this? I cannot figure out why, surely it has something to do with some periodicity going on in $\sin(t) + \cos(t)$ ?",['trigonometry']
4230900,Differentiability on level sets of a locally Lipschitz function,"Proposition Let $f:\mathbb R^n \rightarrow \mathbb R$ be locally Lipschitz continuous, and $$Z := \{x \in \mathbb R^n \mid f(x) = 0 \}.$$ Then $Df(x) = 0$ for Leb $^n$ -a.e. $\,x \in Z.$ Where, $Df(x)$ is derivative of $f$ at $x$ . (ByLawrence C. Evans, Ronald F. Garzepy, ""Measure Theory and Fine Properties of Functions"", Chapter 3, Theorem 3.3(i)) At the beginning of the proof, it is stated that Choose $x \in Z$ so that $Df(x)$ exists, and $$\lim_{r \rightarrow 0} \frac{\textrm{Leb}^n(Z \cap B(x;r))}{\textrm{Leb}^n(B(x;r))} = 1;$$ Leb $^n$ a.e. point $x \in Z$ will do. for Leb $^n$ -a.e. $\,x \in Z.$ Why is this true? If $\textrm{Leb}^n(Z \setminus\textrm{Int}(Z)) =0$ , then this will be true in my opinion. But this is not always right. Could you give me some advice?","['lebesgue-measure', 'derivatives', 'lipschitz-functions']"
4230917,Explain to a 15 y.o. Byron Schmuland's answer that uses Summation and Product notations to solve the Crazy Lady Airplane Seat probability problem?,"Byron Schmuland's answer 1 is too abstruse for a 15 y.o. student who needs details! Please see my enumerated questions below. Let's find the chance that any customer ends up in the wrong seat. For $2\leq k\leq n$ , customer $k$ will get bumped when he finds his seat
occupied by someone with a smaller number, who was also bumped
by someone with a smaller number, and so on back to customer $1$ . This process can be summarized by the diagram $$1\longrightarrow j_1\longrightarrow j_2\longrightarrow\cdots\longrightarrow j_m\longrightarrow k.$$ Here $j_1<j_2<\cdots <j_m$ is any (possibly empty) increasing sequence of integers strictly
between $1$ and $k$ .
The probability of this sequence of events is $${1\over n}\times{1\over(n+1)-j_1}\times {1\over(n+1)-j_2}\times\cdots\times{1\over(n+1)-j_m}.$$ What exactly does $1/n$ , the first term, signify? Where do each of the ${1\over(n+1)-j_m}$ terms hail from and signify? How would you divine or forebode to formulate this term? Thus, the probability that customer $k$ gets bumped is $$p(k)={1\over n}{\color{red}{\sum}}\prod_{\ell=1}^m  {1\over(n+1)-j_\ell}$$ where the sum is over all sets of $j$ values $1<j_1<j_2<\cdots <j_m<k$ . That is, Where did the $\color{red}{\sum\limits_{1<j_1<j_2<\cdots <j_m<k}}$ stem from? It appears to come out of left field! \begin{eqnarray*}
p(k)&=&{1\over n}\color{limegreen}{\sum_{J\subseteq\{2,\dots,k-1\}}}\ \, \prod_{j\in J}{1\over (n+1)-j}\cr
    &=&{1\over n}\ \, \prod_{j=2}^{k-1} \left(1+{1\over (n+1)-j}\right) \cr 
    &=&\color{sienna}{{1\over n}\ \,\prod_{j=2}^{k-1} {(n+2)-j\over (n+1)-j}}\cr
    &=&\color{sienna}{1\over n+2-k}.
\end{eqnarray*} How does $\color{red}{\sum\limits_{1<j_1<j_2<\cdots <j_m<k}} \equiv \color{limegreen}{\sum\limits_{J\subseteq\{2,\dots,k-1\}}}$ ? How does $\color{sienna}{{1\over n}\ \,\prod\limits_{j=2}^{k-1} {(n+2)-j\over (n+1)-j}={1\over n+2-k}}$ ? In the case $k=n$ , we get $p(n)=1/2$ as in the other solutions. Maybe there is an intuitive explanation of the general formula; I couldn't think of one. Added reference: Finding your seat versus tossing a coin by Yared Nigussie, American Mathematical Monthly 121, June-July 2014,  545-546 . 1 I currently see no answer on this page by someone called ""Byron Schmuland"" , but this other question refers to the answerer as Byron Schmuland, and there's merely one deleted user. Then I deduced that user940 (the deleted user) was University of Alberta Prof. Byron Schmuland PhD Carleton University 1987.",['probability']
4230940,"$f(x)=\sum_{n=0}^{\infty} \frac{e^{-nx}}{1+n^2}$ is differentiable at $~(0, \infty)$","Let us consider a real valued function $~~f:[0 , \infty) \to \mathbb R~~$ defined by $$f(x)=\sum_{n=0}^{\infty} \frac{e^{-nx}}{1+n^2},~~x \in [0,\infty).$$ Show that $f$ is differentiable at $~(0, \infty)~~$ but $~~\displaystyle \lim_{x \to 0+} f'(x)~$ does not exists. My attempt: By using $~M-$ test I have proved that the series of functions $~~\displaystyle \sum_{n=0}^{\infty} f_n(x)~~$ uniformly convergent to $~~f(x)~~$ as given, where $$f_n(x)=\frac{e^{-nx}}{1+n^2},~~x \in [0,\infty).$$ Then we have $$|f_n(x)| =\left|\frac{e^{-nx}}{1+n^2}\right| \leq \frac{1}{1+n^2}.$$ So uniformly convergent. Hence $~~f'(x)=\displaystyle \sum_{n=0}^{\infty} f'_n(x)=\displaystyle \sum_{n=0}^{\infty} \frac{-ne^{-nx}}{1+n^2}.$ This follows that $~~f(x)~~$ is differentiable on $~(0,\infty).$ Now notice that $$\lim_{x \to 0+}f'(x)=\lim_{x \to 0+} \sum \frac{-ne^{-nx}}{1+n^2} = \sum \left(\lim_{x \to 0+} \frac{-ne^{-nx}}{1+n^2}\right)=-\sum \frac{n}{1+n^2}.$$ Since the above series is not convergent, it yields that the limit does not exists. Is my solution is all okay? Is anything I did wrong or can be solve in much simpler way please suggest me? Thanks for your time to look in my solution.","['sequence-of-function', 'real-analysis', 'uniform-convergence', 'sequences-and-series', 'derivatives']"
4231008,An interesting series related to primes satisfying $\sum_n x_{nk} = 0$ for all $k$,"Consider the series $\sum x_n$ where if $n$ is composite, say $n=p_1^{k_1}....p_m^{k_m}$ then $x_n=x_{p_1}^{k_1}....x_{p_m}^{k_m}$ and if $n$ is prime, $x_p=-x_1-...-x_{p-1}$ . I want to determine whether an $x_1\neq 0$ exists such that $\sum x_n=0$ . It feels like this will be true for all $|x_1|<1$ or none(more clear after getting a feel for the series). This would have the interesting consequence that $\sum x_{kn}=0$ for all integers $k$ . I have also written/copied from the internet a generator for the sequence in python, found here https://pastebin.com/jcqvSS0W The sequence feels more related to prime gaps rather than anything to do with primes being prime, which makes me suspect this is a difficult question.","['number-theory', 'sequences-and-series', 'prime-numbers', 'real-analysis']"
4231045,Calculate missing side of quadrilateral [duplicate],"This question already has an answer here : Better method to solve a geometric problem. (1 answer) Closed 2 years ago . This is a problem from a Geometry Facebook group. Segment $AB$ is split as shown in the picture, in sub-segments of length 2, 5 and 4. Also $BT=2$ and $AS=5$ . A, B, T and S are on the same circle. Also by extending TQ and SP, they intersect on point C, which supposedly is also on the same circle. We are looking for length of ST. I have tried it in Geogebra and there is no way to have C on the same circle. I suspect something is wrong. Since $ABTS$ is cyclic, I have tried to use several of its properties, for example, to draw its diagonals $d_1$ and $d_2$ (not shown in the image) and then use the property $d_1*d_2 = 11x+5$ but I am not getting anywhere. Any ideas?
Thank you!","['euclidean-geometry', 'geometry']"
4231048,Reference request: Allard's regularity Theorem for smooth submanifolds,"Allard's regularity Theorem (Theorem 5.2, https://web.stanford.edu/class/math285/ts-gmt.pdf ) asserts that a varifold can be locally written as a graph. Moreover, it gives a lower bound on the size of the domain in which this is possible. The size depends on the size of subsets of varifolds where balls approximate the size of Euclidean balls of the same dimension. I am not well versed in geometric measure theory and would like this Theorem for submanifolds of Euclidean space but I can't find it formulated anywhere. The abstract in this paper ( https://arxiv.org/pdf/1311.3963.pdf ) says the following: ""In the special case when V is a smooth manifold translates to the
following: If $\omega^{-1} \rho^{-n}Area(V \cap B_{\rho}(x))$ is sufficiently close to 1 and the unit
normal of V satisfies a $C^{0,\alpha}$ estimate, then $ V \cap B_{\rho}(x)$ is the graph of a $C^{1,\alpha}$ function with estimates."" I would like a theorem of the following form. Suppose $V$ is a closed smooth submanifold of $\mathbb{R}^n$ such that the second fundamental form of $V$ is bounded by $C$ and its derivatives by $C^2$ with positive injectivity radius $r_V$ . Then there is a radius $R(C,r_V)>0$ such that for any $p \in V$ there is a function $u_p$ $$
B_R(p) \cap V = graph(u_p)
$$ where $B_R(p)\subset \mathbb{R}^n$ Edit: For anyone interested, I found the answer to my question in Theorem 3.8 of https://people.math.sc.edu/howard/Notes/schur.pdf","['submanifold', 'geometric-measure-theory', 'differential-geometry']"
4231058,Coordinate free optimization of an integral,"I was trying to solve this question using vectors and ideas of gradient, all the context is same as original except the fact that point lines on the x-axis. I want here the point to lie anywhere on the plane. Find the value of  x0  at which the area of the pedal curve of the ellipse with respect to the point P( x0 , 0) is minimized My attempt: Let the tangent line be represented as $\tau$ and let the signed  distance from tangent line to point be $d_{\tau}(p)$ , finally let the unit normal vector of the tangent line be $n$ . Note that $p$ is given as an ordered pair of coordinates, in whatever coordinate system you use. Now, the foot of perpendicular is given as: $$f_{\tau} (p) = p-d_{\tau} (p) n$$ Now, we can parameterize the unit normal as a function of $t$ $$ F_{\tau} (p,t) = f_{\tau} (p) = p - d_{\tau} \left( p\right) n(t)$$ Now, we want the area of Pedal curve, we can use the gauss shoelace formula for this, treating $F_{\tau}(t)$ as a position vector: $$\begin{align} \int F_{\tau} (t,p)  \times \frac{d}{dt} F_{\tau} (t,p)  dt  = \int  \left[ p - d_{\tau}(p) n\right] \times \left[-d_{\tau}(p) \frac{dn}{dt}-\frac{d (d_{\tau}(p) )}{dt} n\right] dt=\\ \int \left[ d_{\tau}(p) \frac{dn}{dt}+n \frac{d (d_{\tau}(p) )}{dt} \right]\times \left[ p-d_{\tau}(p)n\right] dt= \int \frac{dn}{dt} \times \left[ p d_{\tau}(p)- (d_{\tau} (p))^2 n \right] + \frac{d (d_{\tau}(p) )}{dt}n \times \left[ p\right]dt \end{align}$$ The final problem is to find a point $p$ such that the integral: $$I(p) = \int_{0}^{2\pi} \frac{dn}{dt} \times \left[ p d_{\tau} (p) - d_{\tau}(p)^2 n \right] + \frac{d (d_{\tau}(p) )}{dt}n \times \left[ p\right] dt$$ Is minimized. Brute forcing by taking the parameterization $(a \cos \theta, b \sin \theta)$ of ellipse and the normals and $\frac{dn}{dt}$ calculations became literal hell. I thought perhaps we could use the idea of the gradient and see where that vanishes to find max i.e: find where point $p$ such that: $$ \nabla I(p) = 0$$ However, I am not sure how to compute the gradient of the integral in this case. Some help would be appreciated. p.s: Ideally speaking I'd like to finish the proof completely with 0 use of coordinates. 1/8/2022: We can make a simplification for the integral: $$G(p)= \oint \frac{dn}{dt} \times \left[ p d_{\tau} (p) - d_{\tau}(p)^2 n \right] dt \int_{0}^{2\pi} (\frac{dn}{dt} \times n) \left[ p_n d_{\tau} (p) - d_{\tau}(p)^2 \right] dt$$ Due to the fact that $ \frac{dn}{dt} \times p = \frac{dn}{dt} \times n d_{\tau}(p)$ where $d_{\tau}(p)$ is the component of $p$ along $n$ (i.e: vector of p to foot of perpendicular), hence $G(p)$ is zero everywhere. Hence, $$I(p)= \oint  \frac{d (d_{\tau}(p) )}{dt}n \times \left[ p\right] dt$$ I suppose, I would have to nudge the above expression in some direction $ dp $ and check the first order variation: $$I(p+ dp)= \oint n\frac{d}{dt} \left[ d_{\tau}(p) + \nabla d_{\tau}(p)  \cdot dp \right] \times \left[ p+   dp\right]dt$$ Not sure how to continue.. $$ dI = \oint \left[ n \frac{d(d_{\tau} (p))}{dt} \times dp + n \left[ \frac{d}{dt}  \nabla d_{\tau}(p)  \cdot dp \right] \times p\right] +O(|dp|^2)dt$$ To repeat: The goal is to find what $p$ is","['area', 'calculus', 'vector-analysis', 'optimization', 'differential-geometry']"
4231096,I don't understand this specific part of the textbook about countable union in the probability theory book [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question Why is it called countable union? Is it because when all sets are in union, it all becomes one big set?","['probability-theory', 'probability']"
4231097,A question about the derivatives of the Möbius inversion formula for $\zeta(s)$,"The following expression for the $\frac{1}{\zeta(s)}$ involving the Möbius function is well known: $$\frac{1}{\zeta(s)}=\sum _{n=1}^{\infty }\frac {\mu(n)}{n^s} \qquad s \in \mathbb{C},\Re(s) > 1$$ There also exists this (recursive) mobius inversion formula for $\zeta(s)$ : \begin{align}
\zeta(s) &= 1+\sum _{k=1}^{\infty } {\frac {{\mu} \left( k \right) 
}{k}\sum _{m=1}^{\infty }{\frac {\zeta \left( kms \right) -1}{m}}}\qquad s \in \mathbb{C},\Re(s) >0, s \ne \frac{1}{i}, i \in \mathbb{N}\\
\text{where the $d$-th derivative is:}\\
\zeta^{d}(s) &= \sum _{k=1}^{\infty } {{\mu} \left( k \right) 
\sum _{m=1}^{\infty }{\zeta^{d}\left( kms \right)}} (km)^{d-1} \qquad d \in \mathbb{N}, d > 0\\
\text{which is simplest for $d=1$:}\\
\zeta^{1}(s) &= \sum _{k=1}^{\infty } {{\mu} \left( k \right) 
\sum _{m=1}^{\infty }{\zeta^{1}\left( kms \right)}}\\
\end{align} Observe that in all equations, $k=m=1$ already yields the LHS, hence the remaining series must always sum to $0$ . Questions: Could any of these expressions be simplified further (I have tried swapping the sums, which worked fine, however didn't help simplifying things further)? Taking the anti-derivates for $d$ , always correctly recreates the $d-1$ derivative except for the step from $d=1$ to $d=0$ (i.e. when the constants $1$ and $-1$ have to re-appear). Is there a reason for why that final step to $d=0$ is more complicated?","['riemann-zeta', 'number-theory', 'derivatives', 'mobius-inversion']"
4231132,An easy method to solve this definite integral?,"I have this definite integral $$ \int_{-3}^3 \frac{1+x^2}{1+2^x} \, dx$$ But I'm not sure how to calculate this integral in an easy and direct way.
Using calculators like Wolfram Alpha I found the answer to be 12 but I needed some hints on how to solve this further using pen and paper.","['integration', 'calculus', 'definite-integrals']"
