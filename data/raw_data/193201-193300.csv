question_id,title,body,tags
3695969,"Find $\int_{0}^{\infty} \frac{\log(x) }{\sqrt{x} (x+1)^{2}}\,dx$","Need solve the next integral $$\int_{0}^{\infty} \frac{\log(x) }{\sqrt{x} (x+1)^{2}}\,dx$$ Tried something with Laurent’s series, but i can’t conclude anything. Thanks","['integration', 'complex-analysis', 'improper-integrals']"
3695994,Visualizing the total differential,"I'm trying to convince myself of the fact that $d z=\frac{\partial f}{\partial x} d x+\frac{\partial f}{\partial y} d y$ by looking at this picture (i.e. $dz$ should equal the sum of the two segments in red), but can't seem to do it. I guess I can go the difficult road, trying to compute $dz$ by using the Pythagorean formula on the diagonal of the square on top of the cube and the diagonal of the yellow parallelogram. But is there an easier way?","['multivariable-calculus', 'calculus', 'derivatives']"
3696005,Equation of the type polynomial${}= \bar{z}$,"Let $P(z)$ be a complex polynomial of degree 3. How many roots equation $P(z) = \bar{z}$ could have?
I had tried the following ideas but was unable to push through: Research the ring $\mathbb{C}[X]\otimes \mathbb{C}[\bar{X}]$ and ideals of it that annul specific amount of points. But polynomials like $X - \bar X$ had spoiled this idea, maybe it would be better if we will limit ourselves to polynomial with different inversion degree and straight degree but no idea how to approach it further Try to find out how polynomials twist the space and find amount of points that are gluing together and their image is a symmetry relative to real line, i had stopped at trivial case $aX^3$ and it was hard to add another degree. All polynomials of degree 3 are path-connected in $\mathbb{C}$ . Maybe I can find out that amount of solutions of $P(z) = \bar z$ is some nice characteristic of polynomials that have some properties to use. But this one was quite desperate measure already.","['complex-analysis', 'polynomials']"
3696013,When does a convex set have a unique outward normal direction?,"Let $C$ be a closed, nonempty, and convex set (in a real Hilbert space $\mathcal{X}$ ), and let $c\in C$ be a point on its boundary. When will the normal cone $N_Cc$ have a unique (nonzero) direction? My definition of the normal cone at $c$ is $N_Cc=\{x\in\mathcal{X} | (\forall d\in C) \langle x|d-c\rangle\leq 0\}$ . I already know this holds for many simple sets like balls and half-spaces, but I want a more general result. This excerpt from Rockafellar/Wets describes precisely the notion I'm looking for: When $x$ is any point on a curved boundary of the set $C$ , the [normal cone] reduces to a ray which corresponds to the outward normal direction indicated classically. However, the book provides no definition of a ""curved"" boundary. I'm looking for a rigorous characterization of this class of set.
Further references for geometry/convex analysis are greatly appreciated!","['convex-geometry', 'convex-analysis', 'geometry', 'differential-geometry']"
3696119,Comparing the sizes of null sets,"This question is about comparing the relative sizes of null sets by switching from open covers to open cover ing sequences (a la strong measure zero sets or microscopic sets ). The main question is whether the most obvious preorder is in fact linear, and the secondary question is whether this preorder coincides with a related ""game-theoretic"" preorder I know is linear. Below we either work in $\mathsf{ZF}+\mathsf{DC}+\mathsf{AD_\mathbb{R}}$ or restrict attention to appropriately tame sets of reals. Main question For $X\subseteq\mathbb{R}$ of measure zero, say that the efficiency of $X$ is the set $\mathsf{Eff}(X)$ of all sequences of positive reals $\alpha=(a_i)_{i\in\mathbb{N}}$ such that there is some sequence of nonempty rational open intervals $(U_i)_{i\in\mathbb{N}}$ with $m(U_i)\le a_i$ for each $i$ and $\bigcup_{i\in\mathbb{N}}U_i\supseteq X$ . We get a natural preorder from this notion, $\trianglelefteq$ , via $$X\trianglelefteq Y\iff \mathsf{Eff}(X)\supseteq\mathsf{Eff}(Y)$$ (that's not a typo - the idea is that $X\trianglelefteq Y$ means that $X$ is smaller than $Y$ , which is to say that it's more efficient). I would like to understand this preorder better, and in particular: Is $\trianglelefteq$ a linear preorder? Secondary question There is a different relation, of similar ""flavor"" to $\trianglelefteq$ , which I can prove is a linear preorder. Namely, given null $X,Y$ consider the game $E(X,Y)$ defined as follows: Players $1$ and $2$ alternately play individual nonempty rational open intervals building a sequence $U_0,V_0,U_1,V_1,...$ , with $m(U_i)\ge m(V_i)\ge m(U_{i+1})$ . Player $1$ wins a given play iff for each $n$ we have $\bigcup_{i>n}U_i\supseteq Y$ , but there is some $n$ such that $\bigcup_{i>n}V_i\not\supseteq X$ . (The win condition can be rephrased as a kind of redundancy property: "" $(U_i)_{i\in\mathbb{N}}$ covers $X$ with infinite repetition but $(V_i)_{i\in\mathbb{N}}$ does not cover $Y$ with infinite repetition."") Consider the relation $\sqsubseteq$ given by $$X\sqsubseteq Y\iff \mbox{player $2$ has a winning strategy in $E(X,Y)$}.$$ Like $\trianglelefteq$ this is clearly reflexive and transitive, and linearity follows from determinacy. If $X\not\sqsubseteq Y$ then player $1$ has a winning strategy $\Sigma$ in $E(X,Y)$ , and we can turn that into a winning strategy $\hat{\Sigma}$ for player $2$ in $E(Y,X)$ . This is where the redundancy part of the win condition comes in: $\hat{\Sigma}$ 's first move will be essentially useless, but this won't affect the ultimate outcome. So one natural way to get a positive answer to the main question would be to prove $\sqsubseteq$ is the same as $\trianglelefteq$ . However, I don't see how to do this: Do $\trianglelefteq$ and $\sqsubseteq$ coincide? I suspect that the answer is no, and in fact that $\trianglelefteq$ is not linear. (As an aside, the ""non-redundant-covering"" version of $\sqsubseteq$ is more similar to $\trianglelefteq$ so would make the secondary question more interesting, but I don't know that that relation is linear either so it's not obviously useful to the main question.)","['measure-theory', 'logic', 'real-analysis', 'descriptive-set-theory', 'infinite-games']"
3696142,"Are jointly measurable adapted processes relative to natural filtration from right-continuous processes, progressively measurable?","Recently i'm studying the book of Stroock and Varadhan - ""Multidimensional Diffusion Processes"" and i'm trying to solve this exercise : For the first part i argue in the following way : ""Because M is Polish there is a sequence of $\mathscr{F}$ -measurable simple functions $\{f_{k}\}_{k\in\mathbb{N}}$ , such that $f_{k}\to f$ pointwise. By the last result we can reduct the problem to f simple and after to the case $f=\chi_{A_{0}}$ with $A_{0}\in \mathscr{F}$ . Now, for each countable $I\subset [0,\infty]$ denote by $\mathscr{B}_{X^I}$ the product Borel $\sigma$ -algebra in $X^{I}$ , and define the $\mathscr{F}/\mathscr{B}_{X^I}$ -measurable map $\rho_{I}:E\to X^{I}$ given by $$\rho_{I}(q)=\left(\eta(t,q)\right)_{t\in I}.$$ Now, note that $$\Lambda = \left\{A\in\mathscr{F}; A=\rho_{I}^{-1}(B_{I})\, \mbox{for some countable}\, I\subset (0,\infty]\,\mbox{ and}\, B_{I}\in \mathscr{B}_{X^I}\right\}\subseteq \mathscr{F}$$ it's an $\sigma$ -algebra and contains the set the generators $\mathscr{F}$ , i.e. $\Lambda=\mathscr{F}$ , therefore exists a countable $I_{0}\subset [0,\infty)$ and $B_{I_{0}}\in \mathscr{B}_{X^{I}}$ such that $A_{0}=\rho_{I_{0}}^{-1}(B_{I_{0}})$ , and therefore $$\chi_{A_{0}}=\chi_{\rho_{I_{0}}^{-1}(B_{I_{0}})}(q)=\chi_{B_{I_{0}}}\circ \rho_{I_{0}}(q)=\chi_{B_{I_{0}}}\left((\eta(t,q))_{t\in I}\right).$$ and the first part follows."" For the second part i really can't easy adapt the last argue to gets the result except on the case that $\theta$ is $\sigma(\eta)/\mathscr{B}_{M}$ -measurable, cause in this case i use the fact that every adapted and right-continuous processes are progressively measurable and i apply this to $\eta$ . I apreciate every suggests to solve this second part. Another question about the result is : We can make the choose of $\{t_{i}\}_{i\in \mathbb{N}}$ independent of $t$ ?. (Because my choose of this sequence for the case $\theta$ being $\sigma(\eta)/\mathscr{B}_{M}$ -measurable depends of t) Thanks for any help. PS: The answer for this exercise it's a good answer for this question from MSE in the case Y jointly measurable.","['stochastic-processes', 'measure-theory']"
3696191,Simplifying expression using summation,Is there a way can simplify the following expression further? $$ \sum_{i=k}^n {i \choose k} a^i$$ where $a$ is some positive real number. I am aware that $$ \sum_{i=k}^n {i \choose k} = {n+1 \choose k+1}$$ but the $a^i$ is throwing me off,"['summation', 'combinatorics', 'discrete-mathematics']"
3696217,"Best re-roll strategy for a large straight in Yathzee, given $\{1,1,1,6,6\}$","Fun question. Today I was struck by this seemingly simple problem: given a roll with 5 dice, the outcome which is $\{1,1,1,6,6\}$ , and two rolls remaining, what is the optimal strategy to gain a ""large straight"", i.e. $\{1,2,3,4,5\}$ or $\{2,3,4,5,6\}$ , after the third roll? Candidate options are to fix a $1$ (or $6$ ) and roll the others, to fix both a $1$ and a $6$ , or re-roll with all dices. Before the last roll one can again fix dices, but also choose to roll with any dices that were fixed before. The problem is that the $\{1,1,1,6,6\}$ gives no hints for which of the two large-straight options one should go. If one would have only one roll remaining, the optimal strategy would be to re-roll with all dice. But now, with more rolls remaining, the optimal strategy seems to depend on the ""optimal strategy distribution"" of the next roll. Any insights?","['recreational-mathematics', 'combinatorics', 'dice']"
3696228,$(\int f_1d\mu)^2+\cdots+(\int f_nd\mu)^2\leq(\int \sqrt{f_1^2+\cdots+f_n^2}d\mu)^2$,"Let $(X, \mathfrak{B}, \mu)$ be a measurable space, possibly not $\sigma$ -finite, and $f_1, \cdots, f_n \colon X\to (-\infty, +\infty)$ be integrable functions on $X$ . Does $$(\int f_1d\mu)^2+\cdots+(\int f_nd\mu)^2\leq(\int \sqrt{f_1^2+\cdots+f_n^2}d\mu)^2$$ holds? (Since $\sqrt{f_1^2+\cdots+f_n^2}\leq |f_1|+\cdots+|f_n|$ , note that integrand in RHS is integrable.) My first attempt was to apply the Fubini's theorem and Cauchy-Schwarz to the LHS: $\begin{align}(LHS)&=(\int f_1(x)d\mu(x))(\int f_1(y)d\mu(y))+\cdots+(\int f_n(x)d\mu(x))(\int f_n(y)d\mu(y))\\&=\int f_1(x)f_1(y)+\cdots+f_n(x)f_n(y) d(\mu\otimes\mu)(x,y)\\ &\leq\int \sqrt{f_1^2(x)+\cdots+f_n^2(x)}\sqrt{f_1^2(y)+\cdots+f_n^2(y)}d(\mu\otimes\mu)(x,y)\\&=(RHS)\end{align} $ However this approach is valid only if $X$ is $\sigma$ -finite. Note that the inequation is equivalent to the following: If $f\colon X\to \mathbb{R}^n$ is integrable, $$|\int f d\mu|\leq \int|f| d\mu$$","['measure-theory', 'lebesgue-integral']"
3696322,Exploring extentions of Tetration,"Recently I've been kind of curious about tetration, specifically why it doesn't introduce any new inverse functions in the way lower operations do- addition needs subtraction, multiplication needs division and exponentiation needs roots and logs (two inverses because it isn't commutative.) So if the pattern continues, if the next diagonalization of the successor function doesn't perfectly inherit the properties of its predecessor then it should introduce a new inverse. But every description of tetration I see says that it inherits the log and root from exponentiation as the superlog and superroot, but exponentiation loses a fundamental property when it's iterated: associativity. $x^{x^x}$ is different from ${x^x}^x$ . My first theory was that we take for granted that tetration refers to ""top-down"" associativity because ""bottom-up"" is essentially just multiplicative exponentiation, ${(x^x)^x = x^{2x}}$ but it could just as easily be said that roots are just a change in associativity for formulas like ${{(x^y)}/z} \ne {x^{(y/z)}}$ That operation created gaps in the number line, but that led us to the complex numbers. So that made me think, could we have a constructive extension of the number line created by tetration, and could that reconcile the gaps in our ability to define non-integer power tower heights with a single analytic function? $\newcommand{\vc}[3]{\overset{#2}{\underset{#3}{#1}}}$ Unfortunately this theory didn't go anywhere; if I defined a value where ${f({x^{x^x}}) = {{x^x}^x}}$ , it would still have an infinite number of solutions in the complex plane. The reason imaginary numbers are useful because they can be uniquely defined in terms of existing numbers; even though 'i' doesn't have a numeric solution, it provides a unique and reversible solution to the equation ${x = (a+bi)^y}$ for any real numbers a,b and y. With ${z = {x \uparrow\uparrow y}}$ , we don't have a unique solution for y given only x or vice versa.  However, there are countably infinitely many roots which can be represented as a generalized tetrated form of the Lambert W function. ( https://math.eretrandre.org/tetrationforum/attachment.php?aid=1215 ) because ${x^{x^{x^...}} = e^{ln(x)*e^{ln(x)*e^{...}}} = e^{W(ln(a + 2\pi bi))^{W(ln(a + 2\pi bi))^...}}}$ where ${a + 2\pi bi}$ = x. Using this property, we can reduce to a single solution with two parameters: the branch of the W function and the complex component of x reduced by a factor of ${2\pi}$ . So theoretically, an analytic description of tetration wouldn't have 4 inverse relationships, but 5. Of course we generally take for granted that we use the -1 and 0 branches for the W lambert function, but with 5 variables we can define a unique relationship with tetration. To give each part of the relationship a name: $$x = Base$$ $$y = Height$$ $$z = Tetration$$ $$b = Branch$$ $$t = Twist$$ Given this complexity, let's change the notation: $$x = \vc{\overline{\sqrt[y]{z}}}{bWt}{}$$ $$y = \vc{slog_{x}{z}}{bWt}{}$$ $$z = x \vc{\nearrow}{}{bWt} \vc{y}{}{}$$ $$b = x \vc{\nearrow}{}{Wt} \vc{y}{z}{}$$ $$t = x \vc{\nearrow}{}{bW} \vc{y}{z}{}$$ There isn't an analytic function for non-integer values for y, b or t, but a cross-sectional one can be generated provided you have all but one of these variables defined. Let's say you have $$256 = 2 \vc{\nearrow}{}{0W(x)} \vc{3}{}{}$$ This means you have a power tower of three 2's equal to 256 whose solution is on the 0 branch of the W function, which is fine when t is an integer, but when t is .5, x must have the form ${a + (.5)* 2\pi i}$ for a real value a, meaning it must have an imaginary component that is defined independently of i, which 2 does not.  So to reconcile that, we would need a hypercomplex value. If you define specific values for these numbers, theoretically you could map out a 4d topological map of tetration, filling in the holes between the branches of the W function and heights for their respective power towers. I realize this isn't a super rigorous proof, it was just an idea I've been playing around with, so the main things I'm wondering is if: A: This is all actually a constructive solution or if I've been making some leaps on logic, particularly with the lambert W function. B: If it is valid, would 5 be the minimum number of variables or could it be reduced further? C: What's the minimum algebra that would be necessary to describe these values- Quaternions, a Bicomplex or something else? D: Could the same reasoning be extended to pentation, hexation, etc.?","['lambert-w', 'number-theory', 'tetration']"
3696342,If $\lim (f(x) + 1/f(x)) = 2 $ prove that $\lim_{x \to 0} f(x) =1 $,"Let $f:(-a,a) \setminus \{ 0 \} \to (0 , \infty) $ and assume $\lim_{x
 \to 0} \left( f(x) + \dfrac{1}{f(x) } \right) = 2$ . Prove using the
definition of limit that $\lim_{x \to 0} f(x) = 1$ Attempt: Let $L = \lim_{x \to 0} f(x) $ . Let $\epsilon > 0$ be given. If we can find some $\delta > 0$ with $|x| < \delta $ such that $|f(x) - 1 | < \epsilon $ then we will be done. We know that since $f(x) > 0$ , then $\lim 1/f(x) $ is defined. In fact, applying the limit to hypothesis, we end up with $$ L+ \dfrac{1}{L} = 2 $$ and certainly $L=1$ as desired. I am having difficulties making this proof formal in $\delta-\epsilon$ language. Can someone assist me?","['limits', 'calculus', 'real-analysis']"
3696388,Convergent Improper integral whose integrand tends to a non zero finite limit as x tends to infinity.,"Let $f:\mathbb{R} \to \mathbb{R}$ be a continuous function such that $\int\limits_0^\infty f(x)dx$ exists. If $f(x)\ge 0 \,\forall x\, \in \mathbb{R}$ , then prove or disprove that $\lim\limits_{x\to \infty}f(x)$ exists and is zero. 
If $f(x)$ is any function,then taking $f(x)=\sin(x^2),$ I am able to conclude that the result is false because $\lim\limits_{x\to \infty}\sin(x^2)$ does not exist even though $\int\limits_0^\infty \sin(x^2)dx$ is convergent.
But in the case of non negative functions,I am neither able to prove the result nor do I get a counter example.Thank you in advance for your help.","['integration', 'limits', 'real-analysis']"
3696392,Could finding the central tendency between mean and median be useful?,"I'm working with unpredictable data, it could have strong outliers in some cases, and not in others. Considering it's completely situational and random, would it make sense to just average the mean and median of the dataset to get the best of both worlds? Finding the central tendency of both common central tendencies. Or would that skewing be undesirable. Thanks.",['statistics']
3696394,A function that satisfies Cauchy-Riemann but is not holomorphic,"I'm attempting Chapter 1, Exercise 12 in Stein & Shakarchi's Complex Analysis , which is as follows: Consider the function defined by $$f(x+iy) = \sqrt{|x||y|}$$ whenever $x, y \in \mathbb{R}$ . Show that $f$ satisfies the Cauchy-Riemann equations at the origin, yet $f$ is not holomorphic at $0$ . I think that I have solved it, but since I don't have much experience with complex analysis, I'm not sure if my argument is valid/correct: If, for $x, y \in \mathbb{R}$ , we write $$f(x+iy) = \sqrt{|x||y|} = u(x,y)$$ and $v(x,y) = 0$ , since $f$ is a real-valued function, then, for $h \in \mathbb{R}$ , $$\frac{\partial u}{\partial x} = \lim_{h\rightarrow 0} \frac{u(x+h,y)-u(x,y)}{h} = \lim_{h\rightarrow 0}\frac{0}{h} = 0 = \frac{\partial v}{\partial y}$$ and similarly we find that $\partial u/\partial y = 0 = -\partial v/\partial x$ , so this function satisfies the Cauchy-Riemann equations. Now, for $h = h_1 + ih_2 \in \mathbb{C}$ , $$\lim_{h\rightarrow 0} \frac{f(z+h)-f(z)}{h} = \lim_{h\rightarrow 0} \frac{\sqrt{|h_1||h_2|}}{h_1+ih_2}$$ at the origin. Suppose that $h_1 = ab = h_2$ for real numbers $a,b > 0$ . Then $$\lim_{h\rightarrow 0} \frac{f(z+h)-f(z)}{h} = \lim_{ab\rightarrow 0} \frac{ab}{ab + iab} = \frac{1}{1+i}.$$ But if instead $h_1 = -ab = -h_2$ , then $$\lim_{h\rightarrow 0} \frac{f(z+h)-f(z)}{h} = \lim_{ab\rightarrow 0} \frac{ab}{-ab + iab} = \frac{1}{-1+i},$$ so the limit does not exist and hence $f$ is not holomorphic at the origin. I would greatly appreciate if somebody could check the above for correctness. (In particular, can I just assume that the $ab$ factorization exists? And is it sufficient to show that the limit is not the same by approaching from different directions?)",['complex-analysis']
3696404,A question about Terence Tao's definition of limiting values of functions: am I grasping it correctly?,"In the book ""Analysis I"", Terence Tao provides the following definition: Let $X$ be a subset of $\textbf{R}$ , let $f:X\to\textbf{R}$ be a function, let $E$ be a subset of $X$ , $x_{0}$ be an adherent point of $E$ , and let $L$ be a real number. We say that $f$ converges to $L$ at $x_{0}$ in $E$ , and write $\lim_{x\to x_{0};x\in E}f(x) = L$ , iff for every $\varepsilon > 0$ , there corresponds a $\delta > 0$ such that for every $x\in E$ one has that \begin{align*}
|x - x_{0}| < \delta \Rightarrow |f(x) - L| < \varepsilon
\end{align*} Similarly, in the book ''Analysis II'' the same author provides the following definition: Let $(X,d_{X})$ and $(Y,d_{Y})$ be metric spaces, let $E$ be a subset of $X$ , and let $f:X\to Y$ be a function. If $x_{0}\in X$ is an adherent point of $E$ , and $L\in Y$ , we say that $f(x)$ converges to $L$ in $Y$ as $x$ converges to $x_{0}$ in $E$ , or write $\lim_{x\to x_{0};x\in E}f(x) = L$ , if for every $\varepsilon > 0$ there exists $\delta > 0$ such that $d_{Y}(f(x),L) < \varepsilon$ for all $x\in E$ such that $d_{X}(x,x_{0}) < \delta$ . My question about these definitions is the following: what is the role of the set $E$ ? As far as I have understood, the set $E$ tells us how we are approaching $x_{0}$ . Let us consider an example. Let $X = \textbf{R}\backslash\{0\}\subseteq\textbf{R}$ and $f:X\to\textbf{R}$ be given by $f(x) = x/|x|$ . Thus if we consider $E = (0,\infty)$ , $x_{0} = 0\in\textbf{R}$ is an adherent point of $E$ . Hence we have that \begin{align*}
\lim_{x\to x_{0};x\in E}f(x) = \lim_{x\to 0;x\in(0,\infty)}\frac{x}{|x|} = \lim_{x\to 0;x\in(0,\infty)} 1 = 1
\end{align*} Similarly, if we choose $E = (-\infty,0)$ , $x_{0} = 0\in\textbf{R}$ is an adherent point of $E$ . Thus it results that \begin{align*}
\lim_{x\to x_{0};x\in E}f(x) = \lim_{x\to 0;x\in (-\infty,0)}\frac{x}{|x|} = \lim_{x\to 0;x\in(-\infty,0)} -1 = -1
\end{align*} At last, if we choose $E = X$ , the limit $\lim_{x\to 0;x\in E}f(x)$ is undefined. But I am little bit unsure about this. From the context, I assume that we are immersed in the metric space $(\textbf{R},|\cdot|)\supseteq X\supseteq E$ . Here it is another example which may be enlightening. Let $f:X\to\textbf{R}$ , where $X = \textbf{R}\backslash\{1\}\subseteq\textbf{R}$ , which is defined by \begin{align*}
f(x) = \frac{x^{2} - 1}{|x-1|}
\end{align*} If we choose $E = (1,+\infty)$ , then $1$ is an adherent point of $E$ . Thus we have that \begin{align*}
\lim_{x\to 1;x\in E}f(x) = \lim_{x\to 1;x\in (1,+\infty)}\frac{x^{2}-1}{|x-1|} = \lim_{x\to 1;x\in (1,+\infty)}\frac{x^{2}-1}{x-1} = \lim_{x\to 1;x\in (1,+\infty)} x+1 = 2
\end{align*} Similarly, if we choose $E = (-\infty,1)$ , $1$ stills a adherent point of $E$ . Thus we have \begin{align*}
\lim_{x\to 1;x\in E}f(x) = \lim_{x\to 1;x\in (-\infty,1)}\frac{x^{2}-1}{|x-1|} = \lim_{x\to 1;x\in (-\infty,1)}-\frac{x^{2}-1}{x-1} = \lim_{x\to 1;x\in (-\infty,1)} -x-1 = -2
\end{align*} Finally, if we choose $E = X = \textbf{R}\backslash\{1\}$ , the limit $\lim_{x\to 1;x\in E}f(x)$ is not defined. The same reasoning seems to apply to more general settings where we consider metric spaces other than the real line. Am I interpreting it correctly? If not, how should I grasp this concept? I am new to this. So any comment or contribution is appreciated. EDIT Here it is another example from the textbook which may help us understand it properly. Consider $f:\textbf{R}\to\textbf{R}$ to be the function defined by setting $f(x) = 1$ when $x = 0$ and $f(x) = 0$ when $x\neq 0$ . Thus if we choose $E = \textbf{R}\backslash\{0\}$ one has that $\lim_{x\to 0;x\in E}f(x) = 0$ . On the other hand, if $E = \textbf{R}$ , the limit $\lim_{x\to 0;x\in E}f(x)$ is not defined. After this example, he provides the following argument: Some authors only define the limit $\lim_{x\to x_{0};x\in E}f(x)$ when $E$ does not contain $x_{0}$ (so that $x_{0}$ is now a limit point of $E$ rather than an adherent point), or would use $\lim_{x\to x_{0};x\in E}f(x)$ to denote what we would call $\lim_{x\in x_{0};x\in E\backslash\{x_{0}\}}f(x)$ , but we have chosen a slightly more general notation, which allows the possibility that $E$ contains $x_{0}$ .","['limits', 'self-learning', 'metric-spaces', 'real-analysis']"
3696544,Converting cartesian to polar double integral,"Convert the following integral to polar coordinates. You do not need to evaluate. $$\int_{-3}^3 \int_{x}^{\sqrt{9-x^2}} x^2y dy dx$$ My work : I plotted the limits and I don't understand the bounded region due to $y=x$ , but still I got like this which is wrong I know I solved the integral it should be $\frac{-81}{5}$ but the integral in the polar coordinates I obtained is incorrect $$  \int\limits_{\pi/4}^{\pi}\int\limits_{0}^{3}r^4\cos^2 \theta \sin \theta dr d\theta+ \int\limits_{\pi}^{5\pi/4}\int\limits_{-3/\cos \theta}^{-3\sqrt2}r^4\cos^2 \theta \sin \theta dr d\theta$$ Can anyone help me recorrecting it the answer below is not complete, and it is definitely not the double of the answer ???","['integration', 'area', 'multivariable-calculus', 'calculus', 'polar-coordinates']"
3696611,"Is it true that $\mathbb{E}\left[\sum\limits_{k=1}^m \frac{\frac{1}{X_k}}{\sum_{j=1}^m \frac{1}{X_j}}\chi_{(r,+\infty)}(X_k)\right]\to0,m\to\infty?$","The following problem arose in the process of showing the convergence of a particular regression algorithms. Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space. Suppose that $X,X_1,X_2,...:\Omega\to(0,+\infty)$ are $\mathbb{P}-$ i.i.d. random variables such that $\forall r>0, \mathbb{P}(X<r)>0$ . Let $r>0$ . Is it true that \begin{equation}
\mathbb{E}\left[\sum_{k=1}^m \frac{\frac{1}{X_k}}{\sum_{j=1}^m \frac{1}{X_j}}\chi_{(r,+\infty)}(X_k)\right]\to0,m\to\infty?
\end{equation} If so, can we somehow specify the speed of convergence in terms of the quantity $m\mathbb{P}_X\big((0,r]\big)$ ?","['convergence-divergence', 'rate-of-convergence', 'probability-theory']"
3696710,Cardinality of finite sequences of infinite set,"I want to prove that if $A$ is a infinite set, then $|Fin(A)|=|FS(A)|=|A|$ , where $Fin(A)$ is the set of all finite subsets of $A$ and $|FS(A)|$ is the set of all finite sequences. Firstly, to prove $|Fin(A)|=|A|$ , $$|Fin(A)|=|\bigcup_{n < \omega}[A]^{n}|=|\bigcup_{n < \omega}A|=\sum_{n <\omega}|A|=|A|\aleph_0=|A|$$ where in the second equality i use that $|[A]^{n}|=|A^{n}|=|A|$ (i'm also using that $|A \times A|=|A|$ , thanks axiom of choice). For the second, I would to use a similar argument and write $FS(A)=\bigcup_{n < \omega}A^{n}$ , but i'm not truly sure about that equality.",['elementary-set-theory']
3696715,What number's factorial is $i$?,"I am trying to find the solution to the equation- $$\Gamma(z)=i$$ I have tried doing it the following way- LHS is- $$\displaystyle \int_{0}^{\infty}t^ze^{-t}\ dt$$ Taking $z=a+ib$ , we get- $$\displaystyle \int_{0}^{\infty}t^{a+ib}e^{-t}\ dt$$ or $$\displaystyle \int_{0}^{\infty}t^{a}t^{ib}e^{-t}\ dt$$ Using Euler's formula- $e^{i\theta}=\text{cos}\ \theta +\ i\ \text{sin}\ \theta$ , we have- $$\displaystyle \int_{0}^{\infty}t^{a}(\text{cos}\ (b\ \text{ln}t)  +\ i\ \text{sin}\ (b\ \text{ln}t)) e^{-t}\ dt$$ After this point, I am not being able to solve this integral. I have tried graphing it on desmos, but it doesn't seem like this question has any solution. How can I approach further in this?","['integration', 'complex-analysis', 'gamma-function']"
3696727,What kind of a function is $y=2^{\frac{1}{x}}$?,"I am self studying my calculus book and one of the exercises asks us to label the function as either constant, linear, power, polynomial, rational, algebraic, trigonometric, exponential, or logarithmic. The given function is $y=2^{1/x}$ .  It seems to me that this function satisfies none of the above labels.  The closest was perhaps exponential, but these are functions of the form $f(x)=a^x$ , and it is explicitly stated in the book that the domain of exponential functions is $(-\infty, \infty)$ , which rules out $y=2^{1/x}$ . There was also a function type called transcendental which seems to be a catch-all category for anything that is not algebraic, but it was not provided as one of the options for labeling in the exercise (perhaps an oversight by the person who wrote the question).","['calculus', 'functions']"
3696729,Order of Group with Quarternionic Type Representation,"If an irreducible complex representation of $G$ is of quarternionic type, then it must have even order, and so the group order must be even. However, is it further true that $G$ has order divisible by 4? So $|G|$ is never equal to $2$ mod $4$ ? I have checked and this is true for $|G| \leq 200$ .","['representation-theory', 'group-theory', 'finite-groups']"
3696805,Evaluate the integral $\iint_S(x)dy\wedge dz+(x+y)dz\wedge dx+(x^2+2z)dx\wedge dy$,"In a problem from my multivariable integration class, i've reached this problem. I will thank any comment with advice or answer. The problem asks me to calculate the integral $$\iint_S(x)dy\wedge dz+(x+y)dz\wedge dx+(x^2+2z)dx\wedge dy$$ Being $S$ the surface of the solid $V$ limited by: $$S_1=\{(x,y,z)\in\mathbb{R}:2x^2+y^2=4z\},$$ $$S_2=\{(x,y,z)\in\mathbb{R}:x^2+2z=2\}.$$ I'm told to solve twice it using two different methods: direct integration and Gauss' Theorem (divergence) . I've started trying with Gauss' Theorem, but i don't really get if i'm getting it correctly. The theorem tells that (under certaing region and surface conditions that this problem verifies) given $V$ a solid limited by a closed surface $S$ , $N$ the normal vector, and $F=(P,Q,R)$ a vectorial field of class $C^1$ , $$\iint_{\partial V}F=\iint_S(F\cdot N)d\sigma = \iiint_V\text{div}(F)dxdydz.$$ Being $\text{div}(F)=\frac{\partial P}{\partial x}+\frac{\partial Q}{\partial y}+\frac{\partial R}{\partial z}$ . I've started trying to find $P,Q,R$ in my example, but I'm not sure if that's possible, at least with my actual knowledge. What can I do to find my $\text{div}(F)$ ?. For the direct integration part, I have no idea about how to do it. I specially struggle finding the integration limits. Bounty edit: I need a step-by-step solution for both methods: direct integration and Gauss' Theroem (Divergence Theorem).","['integration', 'surface-integrals', 'lebesgue-integral', 'multivariable-calculus', 'differential-forms']"
3696807,Connections and second derivatives of curves,"If $M$ is a smooth manifold, let $AM$ be the subspace of the double tangent bundle $TTM$ consisting of vectors $v$ such that $\pi^{TM}(v) = \pi^X_*(v)$ , where $\pi^X: TX \to X$ is the projection of a tangent bundle. It is a bundle of affine spaces on $TM$ , modeled on the ""tautological"" bundle $TM \times_M TM$ . The significance of $AM$ is that if $\gamma$ is a smooth curve on $M$ , its second derivative $\ddot \gamma$ lies in $AM$ . If the manifold $M$ is equipped with a section of $AM$ defined on all of $TM$ , we can define a sort of covariant derivative of the tangent vector of a curve, by taking the second derivative and subtracting the fixed section. In particular, we get a notion of geodesic. So it seems that such a section plays some of the roles of a connection on $TM$ . Do these objects have a name? Where can I learn more about them? Can I think of them as connections in some more general sense?","['connections', 'derivatives', 'smooth-manifolds']"
3696858,"Decide whether this is correct, using the method of resolution. If not, provide a counter example.","I am new to logic and wanted to decide whether the following is correct using the method of resolution: |= p → ¬ (p → (p ∧ (p ∨ q))) My attempt to this I answered that the conclusion is incorrect, though the premises hold true. Thus, I begin with the negation of the conclusion and solve for validity. ¬ (p → ¬ (p→(p ∧(p ∨ q)))) ↔

                                   ¬ p ∧ ¬ (¬(p→(p ∧(p ∨ q)))) ↔

                                   ¬ p ∧ (p→( p→(p ∧(p ∨ q)))) ↔

                                   ¬ p ∧ (p→(p ∨ p)) ↔

                                   ¬ p ∧ (p→p) ↔ ¬ p ∨ p Therefore, I deduced two clauses from the negated conclusion,  ← p  and p  ← Resolution gives the clauses in one step.          ← p  and p  ←","['formal-proofs', 'propositional-calculus', 'logic', 'discrete-mathematics']"
3697071,"Examples of quadratic extensions K, L of $\mathbb{Q}$ such that KL has some properties.","Let $p$ be a prime integer, I want to find $p$ and K, L  extensions of $\mathbb{Q}$ such that K, L contain each a unique prime lying over $p$ but KL does not. Another, different, triplet such that The residue field extension of $\mathbb{Z}_p$ is trivial for K and L but not for KL. Is there a way to easily compute such examples? I can give examples of other cases (e.g p totally ramified in K and L but not in KL or inert in K and L but not in KL) but I'm finding the two above a bit more difficoult.","['galois-theory', 'number-theory', 'prime-factorization']"
3697102,Which proportions of areas can be formed by circles in the plane?,"Consider two circles in the plane, one red and one blue with a purple intersection. $\hskip2.5in$ The proportion of the area in the plane that is occupied by each color can be seen as a point on the 3-simplex if we say that each triple is (Red Area, Blue Area, Purple Area). The illustration above would correspond to some point close to (0.14, 0.84, 0.02). All points on the 3-simplex can be realized by circles in this way. If we think of this as a function, $f_2 : X^2 \rightarrow \triangle^3$ (where $X$ is some parameterization of circles), then we would say that it's surjective. If we now add a green circle to the plane we would be able to form 7 different colors. $\hskip2.5in$ And now we get a new function, $f_3 : X^3 \rightarrow \triangle^7$ . The difference is that this function isn't surjective. For example: consider a configuration where we only see yellow, purple and cyan, all having an area of $1/3$ each. If we let $F_3$ be the fraction of points on the simplex that are in the image of $f_3$ then we can ask ourselves: what is the value of $F_3$ ? This can of course be generalized to more circles with $f_n : X^n \rightarrow \triangle^{2^n-1}$ and $F_n$ defined similarly. In that case we can easily prove that $F_n \leq F_{n-1}$ by considering the restrictions on all circles but one and then adding the last one. We can also prove the stronger statement that $$
    F_n \leq \min(\{F_p^{\lfloor n/p \rfloor} \mid 1 < p < n\})
$$ This can be proved by splitting the circles into groups of size $p$ and using the fact that if one of the groups can't be formed then the whole configuration can't be formed. A corollary of this is that $$
(\exists n.  F_n \neq 1) \implies \left(\lim_{n \rightarrow \infty} F_n=0\right)
$$ But this argument doesn't construct the value of any $F_n$ .
So my questions are What is the value of $F_3$ ? What about for higher $n$ ? If we can't find exact values, are there at least better bounds?","['combinatorial-geometry', 'circles', 'geometry']"
3697109,A problem on acyclic graphs and its suspension,"Let $G$ be a simple graph. The clique complex, $\Delta(G)$ of the graph $G$ is the simplicial complex given by the collection of all complete subgraph of $G.$ Now we define the graph homology $H^{Gr}_\ast(G) : = H_\ast(\Delta(G))$ (the simplicial homology of $\Delta(G)$ ). Next, define a contractible graph as follows: A family $\mathcal{F}$ of graphs $G_1, G_2, \cdots, G_n ,\cdots$ is called contractible if (1) The trivial graph, $\ast \in \mathcal{F}.$ (2) Any graph of $\mathcal{F}$ can be obtained from the trivial graph by finite series of contractible transformations $\{T_1, T_2, T_3, T_4\}$ where $T_1$ : deleting of vertex $v$ . A vertex $v$ of a graph $G$ can be deleted, if $N_G(v):= \{ u \in V(G): \text{ the edge }[uv] \in E(G)\} \in \mathcal{F}.$ $T_2:$ Gluing of a vertex $v$ . If a subgraph $G_1$ of a graph $G$ is contractible, $G_1 \in \mathcal{F}$ the the vertex $v$ can be glued to the graph $G$ in such a manner that $N_G(v) =G_1,$ $T_3:$ deleting of an edge $[v_1v_2]$ . The edge $[v_1v_2]$ of $G$ can be deleted if $N_G(v_1)\cap N_G(v_2)\in \mathcal{F}.$ $T_4:$ Gluing of an edge $[v_1v_2]$ .  Let two vertices $v_1$ and $v_2$ of a graph $G$ be non-adjacent. The edge $[v_1v_2]$ of $G$ can be glued if $N_G(v_1)\cap N_G(v_2)\in \mathcal{F}.$ Any graph $G \in \mathcal{F}$ is called a contractible graph. $\mathbf{Question:}$ Let $G$ be a graph such that $H^{Gr}_\ast(G) =0 $ for $\ast>0$ and $\mathbb{Z}$ for $\ast=0.$ Then can we prove that the suspension graph $S(G)$ (suspension is considered as the topological  unreduced suspension on the vertices of $G$ ) is contractible in above sense. Thank you so much in advance. Any help will be appreciated.","['discrete-geometry', 'graph-theory', 'discrete-mathematics', 'homology-cohomology', 'algebraic-topology']"
3697125,Probability of returning to the origin,"I have been trying to solve the following problem: What is the probability of the particle returning back to the origin $o$ on a triangular lattice in $k$ steps? The approach used for the regular integer grid is not working for this problem, namely the particle should make the same number of steps in horizontal direction and the same number of steps in vertical direction. So I am stuck a bit. I also tried to find the lower and upper bounds, but my tries were not really successful. Any hints how to tackle this problem are appreciated.","['stochastic-processes', 'probability-theory']"
3697165,What is the volume of a hollow spherical shell?,"Despite some supporting references to physics, this question is purely mathematical on the calculation of volume in a curved metric space. Consider a Schwarzschild space (spacetime) defined by a hollow thin spherical shell of the mass $M$ and the radius $r>R$ , where $R=2M$ is the Schwarzschild radius in natural units given by setting physical constants to unity. The exterior metric is Schwarzschild, the interior metric is time dilated Minkowski. Both are defined in details here (scroll down to “EDIT”): Can separate manifold regions have the same coordinates? The volume inside the shell in tbe Schwarzschild coordinates is known for two extreme cases. (1) For $r\gg R$ , the volume is Euclidean. (2) For $r=R$ , the volume is zero according to The Volume Inside a Black Hole (chapter 3, pp. 5-6): There is zero volume inside the black hole in any Schwarzschild time slice of a Schwarzschild black hole spacetime. What is the volume inside the shell (as observed from outside) in other cases? I am interested in any scenarios, exact or approximate expressions or asymptotic cases. Thanks for your expert insight!","['manifolds', 'metric-spaces', 'mathematical-physics', 'differential-geometry']"
3697195,"For angles $A$ and $B$ in a triangle, is $\cos\frac B2-\cos \frac A2=\cos B-\cos A$ enough to conclude that $A=B$?","Brief enquiry: $$\cos\frac B2-\cos \frac A2=\cos B-\cos A$$ Optionally $$\sqrt\frac{1+\cos B}{2}-\cos B=\sqrt\frac{1+\cos A}{2}-\cos A$$ Is above equality sufficient to prove that it implies $A=B$ ? Detailed explanation and motivation for this question: Consider a triangle with bisectors of equal length: By definition: $\lvert AE\rvert = \lvert BD\rvert = D\\\frac A2+\frac A2 = A,\space\space\space\frac B2 + \frac B2 = B$ By cosine law: $$\lvert AE\rvert^2=y^2+a^2-2ya\cos B\\\lvert BD\rvert^2=x^2+a^2-2xa\cos A\\x^2=\lvert BD\rvert^2+a^2-2\lvert BD\rvert a\cos \frac B2\\y^2=\lvert AE\rvert^2+a^2-2\lvert AE\rvert a\cos \frac A2$$ By sine law: $$\frac{x}{D}=\frac{\sin \frac B2}{\sin A};\space\space\space \frac{y}{D}=\frac{\sin \frac A2}{\sin B}$$ $$\bigl[D=\lvert AE\rvert = \lvert BD\rvert\bigr]$$ Since bisectors are equal: $$x^2-2xa\cos A=y^2-2ya\cos B\implies \lvert BD\rvert \cos \frac B2 +x\cos A = \lvert AE\rvert \cos \frac A2 +y\cos B $$ $$D (\cos\frac B2-\cos \frac A2)=y\cos B-x\cos A $$ Dividing by D and substituting y and x we obtain: $$\cos\frac B2-\cos \frac A2=\frac{\sin \frac A2\cos B}{\sin B}-\frac{\sin \frac B2\cos A}{\sin A}$$ Consider triangles $\Delta$ ABE and $\Delta$ BAD Area of triangle $\Delta$ ABE : $$A = \frac{aD}{2}\sin\frac A2 = \frac{aD}{2}\sin B \implies \sin\frac A2 = \sin B$$ Similarly for triangle $\Delta$ BAD $$A=\frac{aD}{2}\sin\frac B2 = \frac{aD}{2}\sin A\implies \sin\frac B2 = \sin A$$ Therefore: $$\cos\frac B2-\cos \frac A2=\cos B-\cos A$$ Optionally $$\sqrt\frac{1+\cos B}{2}-\cos B=\sqrt\frac{1+\cos A}{2}-\cos A$$ Is above equality sufficient to prove that it implies A = B ?","['triangles', 'trigonometry', 'solution-verification', 'geometry']"
3697212,"Real analytic set on a compact domain, no zeros on the boundary - isolated points only?","I have a feeling that the following must be true, but I cannot figure out a proof. I have two real analytic functions, $f$ , $g$ , both $[0,1]^2\rightarrow\mathbb{R}$ .
I am interested in the set for which $f(x,y)=g(x,y)=0$ . Specifically, I would like to prove that this set is zero-dimensional. I know the following about these functions: For every $x_0 \in [0,1]$ , $f(x_0,y)=0$ has a solution. There exists $\epsilon \in(0,0.5)$ so that $g(x_0,y)=0$ has solutions only for $x_0\in [\epsilon,1-\epsilon]$ . Conversely, for every $y_0 \in [0,1]$ , $g(x,y_0)=0$ has a solution. There exists $\epsilon \in(0,0.5)$ so that $f(x,y_0)=0$ has solutions only for $y_0\in [\epsilon,1-\epsilon]$ . To put it into words, close to the boundary of the domain in $x$ -direction, $f$ has zeros but $g$ does not. Close to the boundary in $y$ -direction, $g$ has zeros but $f$ does not. This of course implies all solutions to the system are in the interior. My questions are: a) Is the information sufficient to conclude that solutions to $f=g=0$ are isolated points, i.e. there cannot be higher dimensional zero sets for the system? How would one show this? b) If it is false, what would be a counterexample? c) If it is true, is it also true for higher dimensions, say 3 functions $f,g,h:[0,1]^3\rightarrow\mathbb{R}$ with $f,g$ having zeros near and at $x=0,x=1$ , but not $h$ , $f,h$ having zeros near and at $y=0,y=1$ , but not $g$ , $g,h$ having zeros near and at $z=0,z=1$ , but not $f$ . Again the zero set for the system must lay in the interior. Can it contain paths? Any help is highly welcome. edit: I'll try to add a bit of thought we've had here.
If the solution set does contain a path, this path can not be parallel to either the $x$ - or $y$ -axis - for then one could apply the identity theorem, and the restriction of both functions to that parallel would have to be the zero function. This contradicts either function not having a solution close to the boundaries this parallel intersects. So then, if such a path exists, it must be possible locally to give a parametrization of one coordinate by the other, say $\bar x(y)$ , with $f(\bar x(y),y)=g(\bar x(y),y)=0$ for $y$ in some open interval. But I fail to derive a contradiction from that and the assumptions quite yet. edit2: H. H. Rugh's answer and other users in the comments have pointed out how to construct counterexamples by taking any $f,g$ with the described properties and then multiplying both by some $h$ that has a 1-dimensional zero set in the interior. That was very helpful, since I realize now that the information described above is not sufficient to conclude what I wish to conclude (i.e. isolated zeros only). However, I am still convinced that it holds for the system I am interested in, but I obviously will have to rethink how to approach it. Any hints regarding which properties might be helpful to establish that such a system allows only isolated solutions would be highly welcome.","['systems-of-equations', 'compactness', 'real-analysis']"
3697245,Where is the mistake with this conditional expectation calculation?,"Let $X$ and $Y$ be independent uniform random variables in $[0,1]$ and let $\alpha\geq 1$ . I am interested in computing $E(\alpha)=\mathbb{E}(X\mid X\geq \alpha Y)$ . Intuitively, I expect to have $E'(\alpha)>0$ as when $\alpha$ increases, conditional on $\{X\geq \alpha Y\}$ , I know $X$ can only take higher values. I am stuck with the computations, however. This is what I've done: We know that $$\mathbb{E}(X\mid X\geq \alpha Y)=\frac{\mathbb{E}(X\mathbb{I}_{X\geq \alpha Y})}{\mathbb{P}(X\geq \alpha Y)}.$$ As $X\in[0,1]$ , $$\mathbb{P}(X\geq \alpha Y)=\int_{0}^{1/\alpha}\mathbb{P}(X\geq \alpha y)\,dy+\underbrace{\int_{1/\alpha}^{1}\mathbb{P}(X\geq \alpha y)\,dy}_{=0}=\int_{0}^{1/\alpha}[1-F_X(\alpha y)]\,dy=\frac{1}{2\alpha}.$$ This expression seems about right, as I know that $\mathbb{P}(X>Y)=\frac{1}{2}$ , and it is decreasing in $\alpha$ as it should intuitively be. Likewise, I can compute $$\mathbb{E}(X\mathbb{I}_{X\geq \alpha Y})=\mathbb{E}\left(X\mathbb{I}_{X\geq \alpha Y}\mathbb{I}_{Y\leq \frac{1}{\alpha}}\right)+\underbrace{\mathbb{E}\left(X\mathbb{I}_{X\geq \alpha Y}\mathbb{I}_{Y> \frac{1}{\alpha}}\right)}_{=0}=\int_0^{1/\alpha}\int_{\alpha y}^1x\,dx\,dy=\int_{0}^{1/\alpha}\left(\frac{1}{2}-\frac{\alpha^2 y^2}{2}\right)dy= \frac{1}{3\alpha}$$ Then, $\mathbb{E}(X\mid X\geq \alpha Y)=\frac{2}{3}$ , which I know that it is true if $\alpha=1$ , but doesn't make sense for other $\alpha>1$ . Can someone point me where is the mistake in my computations? EDIT: As someone already pointed out, the calculations are correct. Can someone come up with a nice intuitive explanation for it?","['expected-value', 'conditional-expectation', 'probability']"
3697254,Why distance function differentiable for sufficiently smooth boundary?,"Suppose that $\Omega$ is bounded domain in $\mathbb{R}^n$ with $C^k$ boundary. Why is it that for points sufficiently close to the boundary the distance function $d = d( \cdot, \partial \Omega)$ is $C^k$ ? I can see it is enough to take a small ball around the boundary, and take a $C^k$ function $\psi$ defining the boundary and show that this give me a $C^k$ distance function, then use compactness. But, I don't see how to do this. Thoughts? Thanks","['elliptic-equations', 'real-analysis', 'partial-differential-equations', 'derivatives', 'differential-geometry']"
3697256,Prove that orthocenter of the triangle formed by the arc midpoints of triangle ABC is the incenter of ABC,"Let $ABC$ be an acute triangle inscribed in circle $W$ . Let $X$ be the midpoint of the arc $BC$ not containing $A$ and define $Y$ , $Z$ similarly. Show that the orthocenter of $XYZ$ is the incenter $I$ of $ABC$ .. This is Lemma 1.42 from Euclidean Geometry In Mathematical Olympiads, and I'm stuck. I tried defining a phantom point $D$ , where $D$ is the intersection of $AX$ and $ZY$ and then proving that angle $ADZ = 90$ degrees, but I wasn't successful.","['euclidean-geometry', 'triangles', 'circles', 'geometry']"
3697279,Complex Ordinary Differential Equations,"I would like to understand a bit on complex ordinary differential equations, since I just learned some theorems on complex integration. So I proposed my-self to solve the following one: $$f(z) = f'(z), \,\,\,\, f(1)=z_0$$ Let's suppose $f$ is analytic on $D$ such that the initial value is in. If it were a real ODE, that would could be solved through separation of variables. But that's not the case here, since we would have to integrate over a path. Any ideas or even the solution would be appreciated. However, I also would like to read any book on this subject. Thanks EDIT Well, as far as I know. There may be two ways: 1) Integrate $f$ over a rectifiable path and use the FTC for complex functions or 2) Separate $f$ into real and imaginary parts. The thing is, how do to the first way. Any ideas or comments?","['complex-analysis', 'complex-integration', 'ordinary-differential-equations']"
3697303,Finding real affine change of coordinates efficiently,"Consider the two equations $$13x^2 - 10xy + 13y^2 = 1$$ $$4u^2 + 9v^2 = 1$$ What is a better way to find the change of coordinates than setting $x = au + bv + e$ and $y = du+bv+f$ and doing some terrible computations? When I studied multivariable calculus, I did just that, but I suppose there should be a better way. This question is from Algebraic Geometry by Garrity.",['algebraic-geometry']
3697350,How long to catch up to a stream started 1 hour ago at 1.5x speed?,"I opened a stream that started an hour ago. Not wanting to miss anything, I started from the beginning and set it to 1.5x speed. How long will it take for me to catch up? I know that it will take 40 minutes to watch the hour that I missed ( $ \frac {60}{1.5}=40$ ) but during that time, the stream has generated another 40 minutes that I need to watch. This tells me I need to do some calculus, but it's been a decade since I took that course. Can someone help me come up with an equation?","['word-problem', 'algebra-precalculus', 'arithmetic']"
3697383,A sum involving fractional parts and prime numbers,"In this paper a formula involving fractional parts, denoted by $\{\cdot\}$ , is derived \begin{equation}
\sum_{\;\;\;\;\;d\leq x \\ d \equiv b \mod a}\Big\{ \frac{x}{d}\Big\} = \frac{x}{a}(1-\gamma) + O(\sqrt{x}).
\end{equation} A generalization of this is given also, which I understand. However, it also remarks that the following very interesting relation is also true \begin{equation}
\sum_{p \leq x} \Big\{\frac{x}{p}\Big\} = \frac{x}{\log x}(1-\gamma) + o\Big(\frac{x}{\log x}\Big)
\end{equation} where the sum is over primes $p\leq x$ . However the reference it gives here is in french, and I can't quite see how it is true from the derivation of the previous result (although I can see the intuitive link due to the prime number theorem). Does anyone know any references to the proof of the result, or can explain it?","['summation', 'number-theory', 'proof-explanation', 'fractional-part', 'prime-numbers']"
3697500,Linearly growing sets of numbers which allow for a unique decomposition,"I'm searching for sets $B=\{b_1,...b_n\}$ of $n$ distinct positive natural numbers with the following condition: The sum $\sum_{k=1}^n b_k=b^\star$ (where each element appears once), is unique, i.e. no other sum of $n$ elements from $B$ gives $b^\star$ . The largest element in $B$ grows only polynomial with $n$ . Clearly of powers of $2$ would suffice the first condition and there is also the Fibonacci Decomposition , but both examples don't match my second condition. The Frobenius coin problem seems to ask for something simliar in a certain way, but it also doesn't match quite. Any help appreciated... Update So far, brute force numerical experiments revealed the following sets with minimal maximal element: $n=4: B=\{1,2,5,7\}$ $n=5: B=\{1,2,6,12,14\}$ $n=6: B=\{1,3,11,22,23,27\}$",['number-theory']
3697546,Finding low-index normal subgroups of finitely presented groups in GAP,"I'm a new user of GAP looking to use it to find finite-index, normal subgroups of some finitely presented groups. To provide a concrete example, how would I find all the low-index (say index<200) normal subgroups of G, where F := FreeGroup(""a"",""b"");; G := F / [ F.1^4, F.2^5, (F.1*F.2)^2 ]; In particular, G, like the other groups I'm interested in, is infinite, so I really need a solution that only finds normal subgroups with index less than a cutoff. In principle, there is an algorithm here that does what I want. So the real question is just about doing so easily in GAP.","['gap', 'group-theory', 'normal-subgroups', 'finitely-generated']"
3697558,"Find all sequences that has $\sum_{i=1}^\infty a_i$ converges, where $a_i = \sum_{k=i+1}^\infty a_k^2$.","Find all sequences that has $\sum_{i=1}^\infty a_i$ converges, where $a_i = \sum_{k=i+1}^\infty a_k^2$ . My intuition is that the only sequence of this form is the zero sequence. Here's what I have so far: $a_n - a_{n+1} = a_{n+1}^2 \implies a_{n+1} = \sqrt{a_n + \frac{1}{4}}  - \frac{1}{2}$ , but it doesn't seem to lead me anywhere. Another line of thought is that if $a_i = 0$ for some $i$ , it means that $\sum_{k=i+1}^\infty a_k^2=0$ , which means that $a_k = 0$ for $k > i$ . This will also mean $a_{i-1} = 0, a_{i-2} = 0, \ldots$ , making the whole sequence the zero sequence. It means that $a_i >0 $ for all $i$ , yet $\lim a_i = 0$ . The last line I've tried is $a_1 = a_2^2 + a_3^2 + a_4^2 + \ldots, a_2 = a_3^2 + a_4^2 + \ldots$ , so $\sum_{i=1}^\infty a_i =  a_2^2 + a_3^2 + a_4^2 + \ldots + a_3^2 + a_4^2 + \ldots = a_2^2 + 2a_3^2 + 3a_4^2 = \sum_{i=2}^\infty (i-1)a_i^2$ , which implies a stronger condition of having $ia_i^2 \to 0$ . I'm hoping to get a contradiction but it doesn't seem to work. Python seems to suggest that $(a_n) \approx \frac{1}{n}$ for large $n$ . Any hints?","['sequences-and-series', 'real-analysis']"
3697564,"Can the definition of ""The Long Line"" be clarified?","In Steen and Seebach's ""Counterexamples in Topology"", we see the definition of the Long Line (counterexample 45). ""The long line $L$ is constructed from the ordinal space $[0, \Omega)$ (where $\Omega$ is the least uncountable ordinal) by placing between each ordinal $\alpha$ and its successor $\alpha + 1$ a copy of the unit interval $I = (0,1)$ . $L$ is then linearly ordered, and we give it the order topology."" Having given this a bit of thought, I need clarifying the following. Are the ordinals $0, 1, 2, \ldots, \alpha, \alpha + 1, \ldots$ part of the space, or is $L$ just $\Omega$ instances of $(0,1)$ concatenated? If the latter, then it appears there may be a homeomorphism between $L$ and $[0,\Omega) \times (0,1)$ under the lexicographic ordering. If the former, then it is very much less simple. So is $L$ like: $0, (0,1), 1, (0,1), 2, (0,1), \ldots, (0,1), \alpha, (0,1), \alpha + 1, (0,1), \ldots, (0,1), \Omega-1, (0,1)$ or is it like: $(0,1), (0,1), (0,1), \ldots, (0,1), (0,1), (0,1), \ldots, (0,1), (0,1)$ with $\Omega$ instances of $(0,1)$ ?",['general-topology']
3697659,Prove formula for $\int \frac{dx}{(1+x^2)^n}$,"I was reading a calculus book and I saw this reduction formula: $$\int \frac{dx}{(1+x^2)^n} = \frac{1}{2n-2}\frac{x}{(x^2+1)^{n-1}}+\frac{2n-3}{2n-2}\int\frac{1}{(x^2+1)^{n-1}}dx$$ Out of curiosity I attempted to prove it, but I got stuck near the end of it. My attempt: Let $x=\tan(t),  dx=\sec^2(t)dt$ Substituting in the original integral we get: $$\int\frac{\sec^2(t)}{(1+\tan^2(t))^n}{dt}$$ By trig identities the integral becomes something like this: $\int\frac{1}{[\sec^2(t)]^{n-1}}{dt}$ , which is equal to $\int{\cos^{2n-2}(t)}{dt}$ , then applying the reduction formula for cosine we get this thing: $$\int{\cos^{2n-2}(t)}{dt}= \frac{1}{2n-2}Â·\cos^{2n-3}(t)\sin(t) + \frac{2n-3}{2n-2}Â·\int{{\cos}^{2n-4}(t)}{dt}$$ Then after some algebraic and trigonometric manipulations the expression looks like this: $$\frac{1}{2n-2}Â·\frac{\tan(t)}{[1+\tan^{2}(t)]^{n-1}} + \frac{2n-3}{2n-2}\int{\cos}^{2n-4}(t){dt}$$ I only need to substitute $x=\tan(t)$ to get the first part of the formula, but I don't know how to manipulate $\int{{\cos}^{2n-4}(t){dt}}$ to get an expression that I can use to finish this problem. How do I proceed, did I make a mistake, will there ever be a proof for the Riemman Hypothesis? P.s
I tried breaking down $\int{{\cos}^{2n-4}(t){dt}}$ into $$\int{{\cos}^{2n-2}(t)\cos^{-2}(t){dt}}$$ But after playing around with that expression I get $\int\frac{1+x^{2}}{[1+x^{2}]^{n-1}}{dx}$ , which doesn't match the formula","['advice', 'reduction-formula', 'calculus', 'indefinite-integrals', 'trigonometry']"
3697719,Solve many linear equations of similar structure,"Given G : real and symmetric square matrix v : real column vector I need to solve n linear systems of the form \begin{align} A = \begin{pmatrix} G & v \\\ v^T & 0 \end{pmatrix}\end{align} \begin{align} Ax = b\end{align} Where n is large G : real and symmetric square matrix, constant for all n systems v : real column vector, changes for each system (Combination vector where at most 2 values are nonzero) b : is zero column vector with exception of the last element I want to know if there is a fast method to solve these many systems via exploiting this structure and suspect that there is a way to do this via eigenvalue decomposition of sums of hermitian matrices. However, I am unsure of how to combine the results. I currently solve n systems via a hermitian solver which doesn't scale well. For convenience, I provide the following equivalent python code import numpy as np
import scipy.linalg as sp_linalg

np.set_printoptions(threshold=np.inf, linewidth=100000, precision=3, suppress=True)

N = 10 # Size of A-1

G = np.random.random(size=(N, N))
G += G.T
G *= 2

v = np.zeros((N, 1))
v[np.random.choice(N, 2)] = 1.0

A = np.block([[G, v], [v.T, 0.0]])
A_G = np.block([[G, np.zeros((N, 1))], [np.zeros((1, N+1))]])
A_v = np.block([[np.zeros((N, N)), v], [v.T, 0.0]])

b = np.concatenate((np.zeros((N, 1)), np.random.random((1,1))))

###

x = sp_linalg.solve(A, b, assume_a='sym') # General solution to compare against

###

# for eigenvalue decomposition
# lambda_G, Q_G = np.linalg.eigh(A_G)
# lambda_v, Q_v = np.linalg.eigh(A_v) Thanks! Solution: I've taken the solution mentioned by eepperly16 and further generalized the problem. Now G : NxN random symetric matrix constant for all n systems v : NxM matrix of random variables The big idea is since v is now a matrix, an inverse of $-v^\top G^{-1} v$ rather than doing a simple divide. These changes include... $x_2 = -y_2 / (v^\top G^{-1}v)$ Becomes $x_2 = (v^\top G^{-1}v)^{-1} -y_2$ $x_1 = y_1 - x_2G^{-1}v$ Becomes $x_1 = y_1 - G^{-1}vx_2$ Since the result of this is always symmetric, that can be exploited with similar factorization. Note, however, that now the time complexity of the second stage expands proportionately to $O(M^2)$ . And finally the code with benchmark import numpy as np
import scipy.linalg as sp_linalg
import timeit

np.random.seed(40)
np.set_printoptions(threshold=8, linewidth=1000, precision=3, suppress=True)

N = 100 # Size of square matrix G
M = 10 # Number of columns in v

# Setup problem and randomize
def setup_and_randomize():

    # Create random symmetric matrix G on range (-1.0, 1.0)
    G = 2.0 * np.random.random(size=(N, N)) - 1.0
    G += G.T
    G *= 0.5

    # Create random rectangular matrix v on range (-1.0, 1.0)
    v = 2.0 * np.random.random(size=(N, M)) - 1.0

    A = np.block([[G, v], [v.T, np.zeros((M, M))]])

    b_1 = np.zeros((N, 1))
    b_2 = np.ones((M, 1))
    b = np.concatenate((b_1, b_2), axis=0)

    return A, G, v, b, b_1, b_2


# General solution to compare against
def naive_method(A, b):
    return sp_linalg.solve(A, b, assume_a='sym')


# Generalized solution created from eepperly16's solution Part 1
def answer_method_precompute(G, b_1, b_2):
    P, L, U = sp_linalg.lu(G, overwrite_a=True, check_finite=False)
    L_inv = sp_linalg.solve_triangular(L, np.eye(N), lower=True, trans='N', overwrite_b=True)
    U_inv = sp_linalg.solve_triangular(U, np.eye(N), lower=False, trans='N', overwrite_b=True)
    G_inv = U_inv @ L_inv @ P.T

    y_1 = G_inv @ b_1
    y_2 = b_2 - v.T @ y_1
    return y_1, y_2, G_inv

# Generalized solution crated from eepperly16's solution Part 2
def answer_method_main(v, y_1, y_2, G_inv):
    G_inv_dot_v = G_inv @ v

    # IF M >= 1 -----------------------------------------------------
    B = v.T @ G_inv_dot_v
    P, L, U = sp_linalg.lu(B, overwrite_a=True, check_finite=False)
    L_inv = sp_linalg.solve_triangular(L, np.eye(M), lower=True, trans='N', overwrite_b=True)
    U_inv = sp_linalg.solve_triangular(U, np.eye(M), lower=False, trans='N', overwrite_b=True)
    B_inv = U_inv @ L_inv @ P.T

    x_2 = B_inv @ -y_2
    x_1 = y_1 - G_inv_dot_v @ x_2

    # IF M == 1 -----------------------------------------------------
    # x_2 = -y_2 / (v.T @ G_inv_dot_v)
    # x_1 = y_1 - (x_2 * G_inv_dot_v)

    return np.concatenate((x_1, x_2), axis=0)

if __name__ == ""__main__"":

    # Verify Same Solution ------------------------------------------
    A, G, v, b, b_1, b_2 = setup_and_randomize()

    x_naive = naive_method(A, b)

    y_1, y_2, G_inv = answer_method_precompute(G, b_1, b_2)
    x_answer = answer_method_main(v, y_1, y_2, G_inv)

    print('Naive Solution:\t', x_naive.T)
    print('Final Solution:\t', x_answer.T)

    # Benchmark Performance ----------------------------------------------
    n_tests = 1000

    A, G, v, b, b_1, b_2 = setup_and_randomize()
    print('\nTimeit on naive_method', timeit.timeit('naive_method(A, b)', globals=globals(), number=n_tests))
    print('Timeit on answer_precompute', timeit.timeit('answer_method_precompute(G, b_1, b_2)', globals=globals(), number=n_tests))
    print('Timeit on answer_main', timeit.timeit('answer_method_main(v, y_1, y_2, G_inv)', globals=globals(), number=n_tests)) Which yields the following on my machine for 1000 iterations of N=100, M=10 Naive Solution:  [[ 0.33  -1.518  0.434 ... -0.394 -0.569  0.824]]
Final Solution:  [[ 0.33  -1.518  0.434 ... -0.394 -0.569  0.824]]

Timeit on naive_method 0.39002
Timeit on answer_precompute 0.46521499999999993
Timeit on answer_main 0.14545809999999992 Final Edit: I understand that with scipy, there are better ways to compute the inverse that better tie into one of many BLAS style libraries. Below are 2 ways to compute the inverse of G that work better than the initial solution. Also, enabling more flags on the naive solver also makes that timing calculation fairer. G_inv = sp_linalg.lu_solve(
            sp_linalg.lu_factor(G, overwrite_a=True, check_finite=False),
            np.eye(N), overwrite_b=True, check_finite=False)

L, D, perm = sp_linalg.ldl(G, overwrite_a=True, hermitian=True, check_finite=False)
    L_inv = sp_linalg.solve_triangular(L[perm, :], np.eye(N), lower=True, trans='N', overwrite_b=True, check_finite=False)[:, perm]
    G_inv = (L_inv.T / D.diagonal()) @ L_inv","['eigenvalues-eigenvectors', 'lu-decomposition', 'linear-algebra', 'numerical-linear-algebra', 'matrix-decomposition']"
3697747,Understanding a common proof for linearity of expectation,"For any two discrete random variables $X,Y:\Omega \to \mathbb{R}$ in a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ , linearity of expectation tells us that: $$\mathbb{E}(X+Y) := \sum_{t \in (X+Y)(\Omega)}t\cdot\mathbb{P}(X+Y = t) = \mathbb{E}X + \mathbb{E}Y$$ Here I use $(X+Y)(\Omega)$ to denote the image of $X + Y$ and $X + Y = t$ to denote the event $\{\omega \in \Omega\,\mid\, X(\omega) + Y(\omega) = t\}$ . In university lecture notes, textbooks, and online forums (like this one) you see a lot of proofs of this fact that begin something like this: $$\mathbb{E}(X + Y) := \sum_{x \in X(\Omega)} \sum_{y \in Y(\Omega)} (x + y)\cdot\mathbb{P}(X = x, Y = y)$$ How in any way does this coincide with the definition of expectation that I gave above? I realize that there is an equivalent definition (namely, $\mathbb{E}X = \sum_{\omega \in \Omega} X(\omega) \mathbb{P}(\{\omega\})$ ) that trivializes the proof. The way I prefer to prove this fact is to first prove equivalence between the two definitions and then use the second definition to prove linearity. However, I find that the above method of proof is very common in other areas of probability theory aside from just linearity of expectation, so I'd like to better understand it. To me this feels a lot like the Law of the Unconscious Statistician; the first step of the proof is actually a large jump in reasoning but, given little thought, appears intuitively true (though it is currently very unintuitive to me). How do I go above proving equivalence between the two sums? Is this true in other scenarios? For example: $$\mathbb{E}(XY) := \sum_{t \in (XY)(\Omega)} t \cdot \mathbb{P}(XY = t) \stackrel{??}{=} \sum_{x \in X(\Omega)} \sum_{y \in Y(\Omega)} xy\cdot\mathbb{P}(X = x, Y = y)$$ Does the above hold? Help is appreciated, thanks in advance.","['expected-value', 'probability-theory', 'random-variables']"
3697804,What angle is it asking for? How would i find it? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Worksheet Question: https://i.sstatic.net/YH9bF.png My Diagram: https://i.sstatic.net/wfJ0P.jpg What angle is it asking me to find? im really confused as to how im supposed to find it. Did i draw my diagram right? or did i label it wrong. Note: I didn't add in all the numbers, i just threw the prism together real quick in paint Thanks in advance everyone.",['trigonometry']
3697825,"Find $n$ and $m$, if $n,m$ are natural numbers, such that $m^6 + 279 = 2^n$.","Suppose $n, m \in \mathbb{N}$ . If $m^{6}+279=2^{n}$ , find $n, m$ . What are some ways to approach this question? Instinct told me to take logarithm but doesn't work to well.","['modular-arithmetic', 'number-theory', 'natural-numbers', 'discrete-mathematics', 'algebra-precalculus']"
3697845,"Show that there exists a real number $R≥0$ such that, for all $x$, $y\in [0$, $1]$ and all $n \in \mathbb{N}$, $|g_n(x)−g_n(y)|\le R|x−y|$.","Assume that $(f_n)_n$ is a sequence of functions continuous on $[0$ , $1]$ , differentiable on $(0,1)$ , that converge pointwise on that interval to a function $f$ , and such that each $f_n$ ′ is bounded on $(0$ , $1)$ and the sequence $(sup_{s\in(0,1)} |f_n′ (s)|)_n$ is bounded. Setting $g_n = f_n − f$ , we know that $g_n$ → 0 pointwise on $[0$ , $1]$ . Show that there exists a real number $R≥0$ such that, for all $x$ , $y\in[0,1]$ and all $n \in \mathbb{N}$ , $|g_n(x)−g_n(y)|\le R|x−y|$ . My attempt Here I used the Mean Value theroem as $g_n$ is continuous and differentiable because of $f_n$ . So for $x$ , $y \in [0,1]$ , $g_n'(c)$ = $|g_n(x) - g_n(y)| \over |x -y| $ . This can be rewritten  as $g_n'(c) |x - y|$ = $|f_n(x) − f(x)| - |f_n(y) − f(y)|$ . By pointwise convergence and as n -> infinity then $g_n'(c) |x - y|$ = $|f(x) − f(x)| - |f(y) − f(y)|$ which means $g_n'(c) |x - y|$ = 0 thus, $g_n'(c)$ is either $0$ or positive hence $g_n'(c) = R$ . Edit: I am rethinking my answer because I can't prove $g_n$ is continuous as I don't know if the limit function $f$ is continuous or not. Any push in the right direction is greatly appreciated. Edit 2: Thank you for the hint by @Saptak Bhattacharya and 
@Daniel Fischer.  I tried to use the triangle of inequality but I am not sure if what I did was right. This is how I did it: $|g_n(x) - g_n(y)| = |f_n(x) - f(x) - (f_n(y) - f(y))|$ $\leq$ $|f_n(x) - f_n(y)| + |f(y) - f(x)| $ $|f_n(x) - f_n(y)|$ is bounded by $M |x -y|$ (I've shown this by previous question). Hence $|f_n(x) - f(x) - (f_n(y) - f(y))|$ $\leq$ $M |x -y| + |f(y) - f(x)| $ . $|f_n(x) - f(x) - (f_n(y) - f(y))|$ $\leq$ $M |x -y| + |f(y)| + |f(x)| $ . $|f_n(x) - f_n(y)| + |f(x)| + |f(y)|$ $\leq$ $M |x -y| + |f(y)| + |f(x)| $ . $|f_n(x) - f_n(y)| \leq M |x -y|$","['sequence-of-function', 'functions', 'pointwise-convergence', 'real-analysis']"
3697878,USSR MO 1980 pigeonhole-principle [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $n \geq 3$ be an odd number. Show that there is a number in the set $\{2^1-1,2^2-1,...,2^{n-1} - 1\}$ which is divisible by $n$ .","['contest-math', 'pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
3697911,"Integrability of $\frac{1}{(x^2+y^2+z^2)^a}$ on $E=\{(x,y,z)\in \mathbb{R}^3: z>1, \ z^2(x^2+y^2)<1 \}$","Let $E=\{(x,y,z)\in \mathbb{R}^3: z>1, \ z^2(x^2+y^2)<1 \}$ and $$f_{a}(x)=\frac{1}{(x^2+y^2+z^2)^a}$$ I need to find all $a\in \mathbb{R}$ such that $f_a\in L^1(E).$ I already know a solution to this problem, and I want to understand why mine is wrong. My attempt 1) [Measurability check] First of all $E$ is open in $\mathbb{R}^3$ being a union over all $z>1$ of open disks of radius $\frac{1}{z}$ : $E=\bigcup_{z>1} E_z=\bigcup_{z>1}\{(x,y): x^2+y^2<\frac{1}{z^2}\}.$ Next, since $z>1,$ $f_a$ looks continous on every point of $E,$ thus is measurable. 2) [Positivity] The function $f_a$ is strictly positive for every $a\in \mathbb{R},$ thus to check integrability we can reduce ourselves to compute $$\int_E f_a \ d(x,y,z)$$ and by positivity of the function we can apply Tonelli's theorem, rewriting the integral as $$\int_1^{+\infty}\int_{x^2+y^2<\frac{1}{z^2}} f_a \ \ d(x,y) \ d(z)$$ 3)[Polar coordinates] We now compute the inner integral using polar coordinates to obtain $$\int_{x^2+y^2<\frac{1}{z^2}} f_a \ \ d(x,y)= \int_0^{2\pi} \int_0^{1/z} \frac{r}{(r^2+z^2)^a} \ dr d\theta= 2\pi \int_0^{1/z} \frac{r}{(r^2+z^2)^a} \ dr =...$$ changing variable $r\mapsto \ t= \phi(r)=r^2+z^2 $ $$...=\pi \int_{z^2}^{1/z^2+z^2} \frac{1}{t^a} \ dt= F(z,a) $$ 4) [Computation by cases] 4.i) For $a=1,$ we have $F(1,z)= \log(1/z+z^2)-\log(z^2)$ and $$\int_1^{+\infty}\log(1/z^2+z^2)-\log(z^2) \ dz= \int_1^{+\infty} \log(z^2(1/z^4+1))-\log(z^2) \ dz= \int_1^{+\infty}\log(1+1/z^4) \ dz < +\infty$$ and so $f_{a=1}\in L^1(E).$ 4.ii) Now we let $a \neq 0,$ and we have $$\int_{z^2}^{1/z^2+z^2}t^{-a} \ dt=\frac{(1/z^2+z^2)^{1-a}}{1-a}-\frac{(z^2)^{1-a}}{1-a}$$ and we have to evaluate $$\int_1^{+\infty}\frac{(1/z^2+z^2)^{1-a}}{1-a}-\frac{(z^2)^{1-a}}{1-a} \ dz $$ But $$\int_1^{+\infty}\frac{(1/z^2+z^2)^{1-a}}{1-a}-\frac{(z^2)^{1-a}}{1-a}
 \ dz \geq \int_1^{+\infty}\frac{(1/z^2)^{1-a}}{1-a}+
 \frac{(z^2)^{1-a}}{1-a} -\frac{(z^2)^{1-a}}{1-a} \ dz=
 \int_1^{+\infty}\frac{(1/z^2)^{1-a}}{1-a} \ dz > +\infty$$ for $0<2-2a\leq 1 \iff 0<1-a \leq \frac{1}{2} \iff a \geq \frac{1}{2}$ so that $f_a \notin L^1(E)$ for $a \geq \frac{1}{2};$ on the other hand, we have $$\int_1^{+\infty}\frac{(1/z^2+z^2)^{1-a}}{1-a}-\frac{(z^2)^{1-a}}{1-a}
 \ dz \leq \int_1^{+\infty}\frac{(1/z^2+z^2)^{1-a}}{1-a} \ dz$$ and since in a ngbh of $+\infty$ $$1/z^2+z^2 \sim z^2 $$ the latter is finite if and only if $$\int_1^{+\infty}\frac{(1/z^2)^{1-a}}{1-a}<+\infty$$ if and only if $$2-2a >1 \iff a<\frac{1}{2}$$ Can someone proof-read and tell me if there are any mistakes? The other solution I have, using spherical coordinates, has $f_a $ integrable $\iff$ $a=3/2$ or $a>-\frac{1}{2}, $ while my solution gives $f_a$ integrable iff $a=1$ or $a<\frac{1}{2}$ . I want to understand what went wrong with my solution.","['integration', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis', 'solution-verification']"
3697915,Geometric meaning of $\operatorname{Spec} \Bbb Z$ being final in the category of schemes,"Is there a geometric meaning to $\operatorname{Spec} \mathbb{Z}$ being a final object in the category of schemes, or more generally $\operatorname{Spec} S$ in the category of $S$ -schemes?",['algebraic-geometry']
3697967,Sum $ \sum_{n=0}^{\infty}\frac{n}{n+1}x^{n} $ [duplicate],This question already has answers here : Find a function for the infinite sum $\sum_{n=0}^\infty \frac{n}{n+1}x^n$ (2 answers) Closed 4 years ago . I want to find the sum function of the series $$ \sum_{n=0}^{\infty}\frac{n}{n+1}x^{n} $$ and I would like to get your assessment of my work cause I'm really not sure about its validity? I define the sum $$s(x) = \sum_{n=0}^{\infty}\frac{n}{n+1}x^{n}$$ and multipy with x $$ s(x)\cdot x = \sum_{n=0}^{\infty}\frac{n}{n+1}x^{n+1}  $$ then differentiate $$ (s(x)\cdot x)' = \sum_{n=0}^{\infty}nx^{n}  $$ divide by x $$ \frac{(s(x)\cdot x)'}{x} = \sum_{n=0}^{\infty}nx^{n-1}  $$ then integrate and find the sum of a regular geometric series $$ \int\frac{(s(x)\cdot x)'}{x} = \sum_{n=0}^{\infty}x^{n} = \frac{1}{1-x}  $$ differentiate again $$ (\int\frac{(s(x)\cdot x)'}{x})' = (\frac{1}{1-x})' \Rightarrow \frac{(s(x)\cdot x)'}{x} = \frac{1}{(1-x)^{2}} $$ multiply by x and take the integral $$ s(x)\cdot x = \int_{0}^{x}\frac{t}{(1-t)^{2}}dt = \ln(|x-1|) - \frac{1}{x-1} + C $$ To find the constant C let $x=0$ and find that $C=-1$ . Lastly divide by x to get the final result $$ s(x) = \frac{\ln(|x-1|)}{x} - \frac{1}{x(x-1)} - \frac{1}{x} $$ Can anyone verify that this is correct? Or is there another way to get the sum? Thanks!,"['power-series', 'solution-verification', 'geometric-series', 'sequences-and-series']"
3698012,How to show that two arcs are parallel with respect to poincare metric of the unit disc?,"Show that two circular arcs in the unit disc with common end points on that unit circle are noneuclidean parallels in the sense that the points on one arc are at constant distance from the other. For the sake of clarity of notation, I rearrange the promblem as follows: Let $C_1$ and $C_2$ be two arcs in the unit disc $\mathbb{D}$ with the same end points $a,b\in\partial\mathbb{D}$ . Show that $C_1$ and $C_2$ are parallel arcs with respect to the Poincare metric or equivalent the Hyperbolic distance. My Idea: It's known that the Hyperbolic distance in the unit disc with respect to the Poincare metric of $\mathbb{D}$ can be expressed as $d_{\rho_{\mathbb{D}}}(z_1,z_2)=\ln\frac{1+\left|\frac{z_1-z_2}{1-\overline{z_1}z_2}\right|}{1-\left|\frac{z_1-z_2}{1-\overline{z_1}z_2}\right|}$ . And as the definition of parallelism, it suffices to show that $\forall z\in C_1$ , $d_{\rho_{\mathbb{D}}}(z,C_2)$ is a constant. But as a matter of fact, computing the distance explicitly is virtually impossible. And as I encounter this problem in my complex analysis course, I do not known much about the hyberbolic geometry theory. So, at present, I do not not how to convert this problem to an equivalent but more solvable statement. This is a promblem from Ahlfors's conformal invariants:topics in geometric function theory. Any hint or solution is highly appreciated!","['complex-analysis', 'complex-geometry', 'hyperbolic-geometry']"
3698021,How to find if a 3d point is in/on/outside of tetrahedron,"How can I find if a 3d point is in/on/outside of tetrahedron defined by 3d coordinates (the point and the tetrahedron)?
This is what I found on ethernet: You now just check if a point $P$ is on the other side of the plane. The normal of each plane is pointing away from the center of the tetrahedron. So you just have to test against $4$ planes. Your plane equation looks like this: $ax+by+cz+d=0$ Just fill in the point values $(x,y,z)$ . If the sign of the result is $>0$ the point is of the same side as the normal, result $== 0$ , point lies in the plane, and in your case you want the third option: $<0$ means it is on the backside of the plane. If this is fulfilled for all $4$ planes, your point lies inside the tetrahedron. So if this is correct, I'm struggling to understand the equation $ax+by+cz+d=0$ . If I'm correct the $x,y,z$ stand for the point coordinates, what do $a,b,c,d$ stand for ?","['triangles', 'geometry', '3d']"
3698028,Find all continuous $f$ that $f(xy) = xf(y) + yf(x)$.,Determine all continuous $f : \mathbb R \rightarrow \mathbb R$ that satisfies $$f(xy) = xf(y) + yf(x)$$ I tried rewrite the equation as $f(xy) + f(x)f(y) + xy = (f(x) + x)(f(y) + y)$ and I know that $f(0) = f(1) = 0$ . Thanks in advance!,"['contest-math', 'continuity', 'functions', 'functional-equations']"
3698030,Where does the equation of asymptotes of a hyperbola come from?,"It's known that the asymptotes of a hyperbola $\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}=1$ is given by $y=\pm\frac{b}{a}x$ if $a>b$ . I  tried to find a proof of the fact that why the equations of these asymptotes are like that,however the only reference (Thomas calculus  book) that I found explained 
that the two asymptotes are derived by letting $\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}=0$ . It would be highly appreciated if someone prove why the equation of the asymptotes have such form.",['algebra-precalculus']
3698081,"Why are ""inner"" automorphisms named this way?","Similarly to "" center of a group"", also "" inner automorphism"" has a topological sounding. But, though for the former I could find some possible explanation in this site, for the latter I couldn't. So: Why are inner automorphisms named this way?","['group-theory', 'soft-question', 'terminology']"
3698099,Is this a new formula for Pell numbers?,"Working with Pythagorean triples, I found that Pell numbers (1,2,5,12,29,70...) provide needed $m,n$ input values for Euclid's formula $\quad(A=m^2-n^2\quad B=2mn\quad C=m^2+n^2)\quad$ when seeking $\quad (B-A=\pm1).\quad$ For example $F(2,1)=(3,4,5)\quad F(5,2)=(21,20,29)\quad F(12,5)=(119,120,169)...\quad. $ I found a formula to generate them in The Online Encyclopedia of Integer Sequences . I also found that I could generate these needed pairs sequentially with a formula found by solving $(B-A)$ for $x$ . I generalized the formula and now it generates both positive and negative Pell numbers in sequential order of increasing value. Any input value $n$ that does not yield an integer is not a Pell number. Any input Pell number yields the next (or prior) Pell number depending on the input sign. $$p=\pm\bigg(n+\sqrt{2n^2+(-1)^{n}}\bigg)\quad\text{where $\pm$ is the sign of the input Pell number. Note:}\frac{n}{|n|}\text{ does not work for }\frac{0}{|0|}$$ For example $$p_{-12}=-\bigg(-12+\sqrt{288^2+(-1)^{-12}}\bigg)=(-1)(-12+17)=-5$$ $$p_{-5}=-\bigg(-5+\sqrt{50+(-1)^{-5}}\bigg)=(-1)(-5+7)=-2$$ $$p_{-2}=-\bigg(-2+\sqrt{8+(-1)^{-2}}\bigg)=(-1)(-2+3)=-1$$ $$p_{-1}=-\bigg(-1+\sqrt{2+(-1)^{-1}}\bigg)=(-1)(-1+1)=0$$ $$p_{0}=\bigg(0+\sqrt{0+(-1)^{0}}\bigg)=(0+1)=1$$ $$p_{1}=\bigg(1+\sqrt{2+(-1)^{1}}\bigg)=(1+1)=2$$ $$p_{2}=\bigg(2+\sqrt{8+(-1)^{2}}\bigg)=(2+3)=5$$ $$p_{5}=\bigg(5+\sqrt{50+(-1)^{5}}\bigg)=(5+7)=12$$ Has anyone seen this formula before? Is it useful or anything else I can't think to ask?","['algebra-precalculus', 'quadratics', 'sequences-and-series']"
3698102,Solving Recurrent Relations using Backtracking,"The following formula has been provided: $a(n) = a(n-1) + a(n-2)\quad \mbox{with initial states}\quad 0 , 2  $ After some research the formula is found to be a Binet's Formula. It is required to convert the above recursive formula to an explicit formula using the Backtracking method. This is what i have done: \begin{align}
a(n) &= a(n-1) + a(n-2) = (a(n-2) + a(n-3)) + a(n-2)  
    \\ & = 2a(n-2) + a(n-3) = 2(a(n-3) + a(n-4)) + a(n-3)  
     \\ & = 3a(n-3) + 2a(n-4) = 3(a(n-4) + a(n-5)) + 2a(n-4)  
    \\ & = 5a(n-4) + 3a(n-5) = 5(a(n-5) + a(n-6)) + 3a(n-5)  
    \\ & = 8a(n-5) + 5a(n-6)
     \\ & \vdots
    \\ & \mbox{and so on}\ldots
\end{align} As it can be seen, there is a relationship with Pascal triangle.","['fibonacci-numbers', 'linear-algebra', 'recurrence-relations', 'discrete-mathematics']"
3698118,"If the angles of $\triangle ABC$ satisfy $\sin A+\sin B+\sin C\leq 1$, then show that one of them is more than $150^\circ$","This is an Olympiad question. In a $\triangle ABC$ , where $\sin A+\sin B+\sin C\leq 1$ , prove that one of the angles is more than $150^\circ$ . First of all I assumed that WLOG, $A\geq B \geq C$ . Then I tried solving the problem using triangle inequality and Sine rule. From there, and from the given statement in the question, I was able to establish that $\sin A < \frac{1}{2}$ . I know that I just need to establish that angle $A$ is greater than $90^\circ$ or something like that.","['trigonometry', 'triangles', 'inequality']"
3698149,Möbius transformation that maps the unit circle to itself,"I need to find necessary and sufficient conditions on the coefficients of a Möbius transform $T(z)=\frac{\tilde a z+\tilde b}{\tilde c z+ \tilde d}$ so that it maps the unit circle $\{z: |z|=1\}$ into itself. I initially thought that, since any Möbius transformation can be written as a finite composition of simple transformations (translations ( $z+a$ ), rotations ( $e^{i\theta}z$ ), dilations ( $az$ ) and inversions ( $\frac1z$ )) and since we do not want to dilate or move the unit circle, we then can write the required transformation as $T(z)=e^{i\alpha}z$ or $T(z)=\frac{e^{i\alpha}}{z}$ for some $\alpha\in (-\pi,\pi]$ . However, this does not look like the result I'm supposed to get. Where is my mistake? I then read the exercise hint, which says I should first write a transformation $R$ that maps the unit circle to $\mathbb{R}_\infty$ , and use transformations $S(z)$ that map $\mathbb{R}_\infty$ to $\mathbb{R}_\infty$ , which I believe are of the form $S(z)=\frac{az+b}{cz+d}$ where $a,b,c,d\in\mathbb{R}$ . I then chose $R(z)=\frac{z+1}{z-1}i$ and tried composing $T=R^{-1}\circ S\circ R$ to find the answer. However, I'm getting an ugly expression that does not seem to be correct either: $T(z)=\frac{(A+Bi)z-\overline{(A-Bi)}}{(A-Bi)z-\overline{(A+Bi)}}$ , where $A=b+ai$ and $B=d+ci$ . Could you help me see how to use the hint? I know there are other solutions for this problem on this site, but they solve it in different ways. Thank you!","['complex-analysis', 'mobius-transformation']"
3698161,Separation of variables in ODE with complex conjugate function,"I am trying to solve the following system: $$\dot z = \bar z·e^{it}$$ $$z:\mathbb{R}\rightarrow\mathbb{C},\qquad z(t_0) = z_0$$ I cannot figure out how to properly separate the right- and lefthandside. I tried: $$\int\frac{z}{||z||^2}dz + C= -ie^{it}$$ but wasn't successful. I will be glad about any tips.","['complex-analysis', 'ordinary-differential-equations']"
3698172,Proving $ 4\operatorname{arccot}(2)+\arctan\left(\frac{24}7\right)=\pi $. What am I doing wrong?,"$$
4\operatorname{arccot}(2)+\arctan\left(\frac{24}7\right)=\pi
$$ original image To prove the above result, I tried to equate the original expression to some constant $a$ such that $0<a<2.5\pi$ (from the range of the inverse tangent). When I try to solve for $a$ by taking the tangent or sine of both sides, I arrive at the equations: $$\begin{align}
\sin(a) &=0 \\
\tan(a) &=0
\end{align}$$ which gives me two solutions ( $\pi$ and $2\pi$ ) within the specified range. I have already seen other solutions using complex numbers, so I would really appreciate if someone could point out where I'm going wrong rather than a solution via another method.","['trigonometry', 'inverse-function']"
3698189,"Genus $3$ curves with a couple of distinct points $P,Q$ such that $4P \sim 4Q$","Let $C$ be a smooth curve of genus $3$ over $\mathbb{C}$ . Is it true that there exist $P\neq Q \in C$ such that $4P \sim 4Q$ ? ( $\sim$ denotes linear equivalence) Notice that if $C$ is hyperelliptic then this is true (just take two different points fixed by the hyperelliptic involution). My (very optimistic) guess is that the converse should hold. If we denote by $f \in K(C)$ the function whose divisor is $div(f)=4P-4Q$ one should try to verify that $f$ admits a square root in $K(C)$ the function field of $C$ . There is another euristic reason (that maybe can be made precise) that make me think that in general such a couple of points does not exist. Namely if I take a very general smooth quartic $C \subset \mathbb{P}^2$ so that I may suppose that it does not have flexes of order $4$ (equivalently $4P$ does not belong to the canonical system for any $P \in C$ ); then the $g^1_4$ induced by divisors of the form $4P$ are a $1-$ parameter family and I expect that the ramification is $3P+\sum_{i=1}^9P_i$ so that  generically the $P_i$ 's are distinct and in ""singular"" cases we have at worst points of multiplicity $2$ apart from $P$ (at least for a generic $C$ ).","['algebraic-curves', 'riemann-surfaces', 'divisors-algebraic-geometry', 'algebraic-geometry', 'function-fields']"
3698240,How to show this is a homotopy operator between $f_0$ and $f_1$?,"Given $f_0, f_1: M \rightarrow N$ maps between smooth manifolds. We defined a homotopy operator between them as a linear map $$ Q: \Omega^{k} (N) \rightarrow \Omega^{k-1} (M)$$ such that $$ f_{1}^{*} - f_{0}^{*} = d \circ Q + Q \circ d$$ holds. Problem: Let $f_0, f_1: M \rightarrow N$ be smooth maps, let $H: I \times M \rightarrow N$ be a homotopy between them, i.e. $H(0,x) = f_0 (x)$ and $H(1,x) = f_1 (x)$ . Also let $I_t : M \rightarrow I \times M: x \mapsto (t,x)$ . Prove that $$Q := \int_0^{1} I_t^{*} \circ \iota_{\partial_t} \circ H^{*} dt $$ is a homotopy operator between $f_1$ and $f_0$ . Here $\iota$ denotes interior multiplication. Attempt: I think I have to use the formula $$ \frac{d}{dt} \rho_t^{*} \alpha_t = \rho_t^{*} (L_{v_t} \alpha_t + \frac{d}{dt} \alpha_t)$$ where $\alpha_t$ is the isotopy of the time-dependent vector field $v_t$ and $L$ denotes the Lie derivative. Given $\alpha $ a $k$ -form on $N$ , I wanted to calculate $$ Q (d \alpha) + d (Q \alpha) = \int_{0}^1 (I_t^{*} \circ \iota_{\partial_t} \circ H^{*}) (d \alpha) dt + d  \int_0^{1} (I_t^{*} \circ \iota_{\partial_t} \circ H^{*}) \alpha dt $$ I was not sure how to work this out. Any suggestions/advice?","['symplectic-geometry', 'algebraic-topology', 'differential-geometry']"
3698332,Automorphism group of elliptic curves in characteristic 3,"I would show that the automorphism group of an elliptic curve E in characteristic $3$ and with J-invariant $0$ is isomorphic to the semidirect product of $Z/4Z$ and $Z/3Z$ .
The curve has equation of the form $Y^2Z=X^3+a_4XZ^2+a_6Z^3$ The substitutions preserving this form are: $$X=u^2X+rZ\\Y=u^3Y\\Z=Z$$ Then automorphisms of E have: $$u^4=1$$ and $$r^3+a_4r+a_6(1-u^2)=0$$ This is all I'm able to say about $u$ and $r$ . How can I proceed?","['algebraic-curves', 'automorphism-group', 'elliptic-curves', 'algebraic-geometry', 'group-theory']"
3698348,What's the relationship between Banach space and inner product space,I know that Banach space is a special Hilbert space. Inner product space is a special normed space. Hilbert space is a complete inner product space. Banach space is a complete normed space. I'm wondering what is the relationship between Banach space and inner product space.,"['inner-products', 'banach-spaces', 'functional-analysis', 'analysis']"
3698358,"How many ways to combine elements from ""different bags""?","I could not find this problem anywhere else, but maybe I am just using the wrong keywords (in that case, I'm sorry). Considering I have two bags, each one with $N$ elements inside, I need to pair each element of the first bag to an element in the second bag (therefore creating $N$ pairs). The question is how many ways there are to pair these elements. For example, let's consider two bags with $N=5$ : Bag_1  = [A, B, C, D, E] Bag_2 = [1, 2, 3, 4, 5] Creating pairs [A1, B2, C3, D4, E5] (one possibility) or [A2, B1, C3, D4, E5] (two possibilities) and so on... There are $5! = 120$ ways of pairing these elements. For the case of one bag containing $N$ different elements I could generalize the answer to be: $\frac{N!}{\prod{K_i!}}$ Where $K_i$ is the amount of each element present in the second bag (e.g. for [1,1,2,2,3] the denominator would be $2!2!1!$ ). But when both ""bags"" have repeated elements, I could not create a rule for describing how many different ways there were to pair the elements. This is a simplification of the problem, what I actually have to solve involves $N$ bags (creating not pairs, but groups of $N$ elements). But any light on the problem is appreciated. Thanks for your attention! Edit: I also need to mention that elements in both bags could be the same (e.g. both could be letters) and order does not matter in any way (e.g 'AB' is the same as 'BA').","['combinations', 'combinatorics']"
3698391,calculate $\sum_{n=0}^\infty \frac{3^n}{n!(n+3)}$ using power series,"let $f(x)=\frac{e^x-1-x-\frac{x^2}{2}}{x}$ , because $e^x = \sum_{n=0}^\infty \frac{x^n}{n!}$ , $f$ can be expressed as $$f(x) = \frac{\sum_{n=0}^\infty \frac{x^n}{n!}-1-x-\frac{x^2}{2}}{x}=\frac{\sum_{n=3}^\infty \frac{x^n}{n!}}{x}=\sum_{n=0}^\infty \frac{x^{n+2}}{(n+3)!}$$ the power series converge in $(-\infty, \infty)$ because $\lim_{n\to\infty} \sqrt[n]{\frac{1}{(n+3)!}}=0$ and let $f_n(x) = \frac{x^{n+2}}{(n+3)!} \Longrightarrow f'_n(x) = \frac{x^{n+1}}{(n+1)!(n+3)}$ , $\sum_{n=0}^\infty \frac{x^{n+1}}{(n+1)!(n+3)}= \sum_{n=0}^\infty f'_n(x)$ also converge in $(-\infty, \infty)$ (for the same reason), hence $$f'(x) = \sum_{n=0}^\infty \frac{x^{n+1}}{(n+1)!(n+3)}$$ by repeating this process once more I get $$f''(x) = \sum_{n=0}^\infty \frac{x^n}{n!(n+3)}$$ and if $x=3$ I get $$\sum_{n=0}^\infty \frac{3^n}{n!(n+3)} = f''(3)$$ which is what was looking for. my problem is that $f$ isn't defined for $x=0$ yet the series does converge for it as $\sum_{n=0}^\infty \frac{0^n}{n!(n+3)}=0$ , so was the function $f$ I used wrong? or could it be that I can't differentiate $f$ the way I did?","['power-series', 'sequences-and-series']"
3698439,Explanation of how to get $38x+19y=3xy$ into factorised form.,"The core problem I would like explained is how to get: $38x+19y=3xy$ into factorised form (not necessarily equal to zero though). This is the method proposed in my book: $3xy-19y-38x=0$ Multiplying by three: $9xy-57y-114x=0$ Then add $38(19)$ to both sides: $9xy-57y-114x+38(19)=38(19)$ Hence, we can write: $(3x-19)(3y-38)=2(19^2)$ I understand how each step leads to the next, but I do not understand how you would come up with this yourself. Specifically, what is the motivation for multiplying by three, or why do we want to add 38(19)?","['proof-explanation', 'algebra-precalculus', 'factoring']"
3698470,How to find real numbers a and b where the function is differentiable at 0,"$\mathbf{Question:}$ Find the real numbers $a$ and $b$ such that the following function is differentiable at $x=0$ $$
f(x)=
\begin{cases}
x^{2}+1 &x≥0\\
a\sin x+b\cos x & x<0\\
\end{cases}
$$ $\mathbf{My\ attempt:}$ $$
\begin{align}
\lim_{x\to 0-}f(x) & =\lim_{x\to 0-}a\sin x +b\cos x \\
& = a\sin (0) + b\cos (0) = b
\end{align}
$$ $$
\begin{align}
\lim_{x \to 0+}f(x) & = \lim_{x \to0+}x^{2}+1 =1
\end{align}
$$ So if $f(x)$ is continuous, $\lim_{x \to0-}f(x) = \lim_{x \to0+}f(x)=b$ Therefore, $b=1$ To find $a$ , we can set $f'(0)=f'(0)$ : $$
\begin{align}
\frac {d}{dx}[x^{2}+1]&=\frac {d}{dx}[a\sin x +b\cos x]\\
2x&=a\cos x-b\sin x\\
0&=a(1)+b(0)\\
a&=0\\
\end{align}
$$ Therefore, $a=0$ Thus, $
f(x)=
\begin{cases}
x^{2}+1 &x≥0\\
\cos x & x<0\\
\end{cases}
$ is differentiable at $x=0$","['limits-without-lhopital', 'calculus', 'functions', 'limits', 'derivatives']"
3698549,Proving that removing any vector of the linearly dependent set gives a linearly independent set,"Consider the matrix representing 6 linearly dependent vectors: $$\left(\begin{array}{llllll}
1 & 0 & 0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 0 & 1 \\
0 & 0 & 0 & 0 & 1 & 1
\end{array}\right)$$ I know how to prove that the vectors in this matrix are linearly dependent, but how can I show (concisely) that removing any one of the vectors we get a linearly independent set?","['matrices', 'vectors', 'linear-algebra', 'vector-spaces']"
3698554,Angular integrals used in QED,"I am reading a research paper and am stuck at a point where the author uses angular integrals.  I don't have any idea about it and would like help. 
 The angular integral is: $$I_k (y)=\int_0^\pi  {\sin^2(\theta)\cos^{2k}(\theta) \over \beta^2(y)-\cos^2 (\theta)} d\theta$$ The author directly give results to this integral without proof.
The results are: $$I_0 (y)=\pi \left(1-\sqrt{1-{1\over\beta^2 (y)} }  \right),$$ $$I_1 (y)={-\pi \over 2}+\beta^2 (y)I_0 (y),$$ I don't know how he got these results.  Can anyone please help me?","['integration', 'calculus', 'definite-integrals', 'mathematical-physics']"
3698601,"How can I justify that the open interval $(0,1)$ is the infinite union of closed intervals?","I have to show that: $$\bigcup_{i=2}^\infty [\frac{1}{n},\frac{n-1}{n}] = (0,1)$$ The first part: $$\bigcup_{i=2}^\infty [\frac{1}{n},\frac{n-1}{n}] \subset (0,1)$$ is easy to show, but for the second part: $$(0,1) \subset \bigcup_{i=2}^\infty [\frac{1}{n},\frac{n-1}{n}]$$ I don't have any idea how to prove it.  I can't use the Principle of Nested Intervals because it's clear that if $I_{k}=[\frac{1}{k},\frac{k-1}{k}]$ where $k$ is a natural number, $I_{k+1} \not\subset I_{k}$ and I have the union.","['elementary-set-theory', 'real-analysis']"
3698629,"sequence of function, what is wrong with my solution?","Prove that the following series converges uniformly in $[0,\infty)$ $$\sum\limits_{n=1}^\infty\frac{(-1)^n}{x+n}$$ So, I need to prove that for every $x\geq0$ and every $\epsilon > 0 $ the following is true: $$\lim \sup |S(x)-S_n(x)|=0$$ So, I found: $$\lim \sup |S(x)-S_n(x)| \leq \lim \sup \sum\limits_{k=n+1}^\infty\frac{1}{x+k}$$ but the latter doesn't converge to zero... how may I solve this?","['calculus', 'functions', 'convergence-divergence']"
3698666,For measurable $f: \mathbb{R} \rightarrow \mathbb{R}$ prove $f(x)$ and $\frac{1}{f(1/x)}$ cannot both be Lebesgue integrable.,"First question on MSE!  I'd appreciate hints, theorem suggestions, or method suggestions regarding the question in the title or below.  Please avoid full solutions.  I'm studying for an exam coming up and got stuck on this question: Problem Let $f: \mathbb{R}\rightarrow \mathbb{R}$ be a measurable function.  Prove that $f(x)$ and $\frac{1}{f(1/x)}$ cannot both be Lebesgue integrable. I've taken courses based on and read from Royden & Fitzpatrick if that helps with suggestions. My attempts so far have focused on trying to find contradictions assuming $f$ is integrable: i.e. $\int_{\mathbb{R}} |f| < \infty$ and defining $S_0 := \{x \in \mathbb{R} | f(x) = 0 \}$ .  I'm thinking that something is happening with zeros and infinities that destroys the measurability of the alternative function. Thanks in advance!","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'measurable-functions']"
3698748,Expression for Lipschitz constant for the $L^p$ norm function on $\mathbb R^n$,"Let $p \ge 1$ and $f:\mathbb R^n \to \mathbb R$ is given by $f(x):= \|x\|_p.$ Then is $f$ a Lipschitz function, and if yes, what's its Lipschitz constant? For $p=1,$ I see that it's $\sqrt{n}$ which follows from Cauchy-Schwartz inequality, for $p=2,$ it's just $1,$ but what is it for a general $p?$ Let's try to do some relevant computation below. $$ \sup_{x \ne y} \frac{|\|x\|_p - \|y\|_p|}{\|x-y\|_2} \le  \sup_{x \ne y} \frac{\|  x- y\|_p}{\|x-y\|_2} = \sup_{x \ne 0} \frac{\|x\|_p}{\|x\|_2}, $$ Note that the first inequality is obtained from triangle inequality, but it's actually an equality because we take the supremum and we plug in $y=0$ . So we indeed have: $$ \sup_{x \ne y} \frac{|\|x\|_p - \|y\|_p|}{\|x-y\|_2} =  \sup_{x \ne y} \frac{\|  x- y\|_p}{\|x-y\|_2} = sup_{x \ne 0} \frac{\|x\|_p}{\|x\|_2}, $$ Now note that, the last equantity is the operator norm of the identity operator from $(\mathbb R^n, \|\cdot\|_2) \to (\mathbb R^n, \|\cdot\|_p)$ , and this operator norm is of course finite, because the spaces are finite dimensional. So I guess my question translates to: what's $\sup_{x \ne 0} \frac{\| x\|_p}{\|x\|_2} = \sup_{\|x\|_2=1} \|x\|_p?$","['functional-analysis', 'lipschitz-functions', 'real-analysis']"
3698785,Show then that the inequality $(z-x)\int_{y}^zf(u)du≥(z-y)\int_{x}^zf(u)du$ holds for any $0 ≤ x < y < z.$,"QUESTION: Let $f : [0,∞) → \mathbb{R}$ be a non-decreasing continuous function. Show then that the inequality $$(z-x)\int_{y}^zf(u)du≥(z-y)\int_{x}^zf(u)du$$ holds for any $0 ≤ x < y < z.$ MY APPROACH: We observe that the integral on the L.H.S. represents the area of the curve $f(x)$ from $y$ to $z$ which is certainly smaller than 
( $\because$ the function is non-decreasing) that represented by the integral on the R.H.S which is from $x$ to $z$ $(\because x<y<z)$ . And obviously, $(z-x)>(z-y)$ , which is in accordance to the given inequality. Now since $x,y,z$ are arbitrary, how do we know that by how much one quantity is greater or smaller than the other. The inequality seems like- $$(greater)(smaller)≥(smaller)(greater)$$ How do I solve this?
Any help will be much appreciated. Thank you so much.","['integration', 'inequality', 'functions']"
3698832,Equivalence between two definitions of closure of $A\subseteq B$ under a function $f\colon B\to B$,"Synopsis In one of the exercises in Enderton's Elements of Set Theory , we are introduced to the concept of the closure of a set to another set under a function $f$ with two constructions. Let $f$ be a function from $B$ into $B$ and assume that $A \subseteq B$ . We have two possible methods for constructing the ""closure"" $C$ of $A$ under $f$ . First, we have the construction: $$C^* = \bigcap\{X \mid A \subseteq X \subseteq B \wedge f [\![X]\!] \subseteq X \}.$$ Alternatively, we could apply the recursion theorem to obtain the function $h$ for which $$h(0) = A$$ $$h(n^+) = h(n) \cup f[\![h(n)]\!].$$ Define $C_*$ to be $\bigcup \text{ran } h$ ; in other words: $$C_* = \bigcup_{i \in \omega} h(i).$$ In the exercise, we were asked to prove that $C^* = C_*$ , which I was able to do without too much effort. The next exercise, though, which asks us to find the closure $C$ if we take $B$ to be the set of real numbers, $f(x) = x^2$ , and $A$ the closed interval $[\frac{1}{2}, 1]$ , confused me a great deal. This is because the sets $C^*$ and $C_*$ described above make little sense to me intuitively. I have a general idea that the closure of $A$ under $f$ describe how the function $f$ always maps to values within $A$ , but I'm not sure how this closure is a set, or how the constructions above elucidate this concept. For example, when people say that the natural numbers are closed under multiplication, how is the closure under multiplication a set? Isn't that just a property? Anyways, all I've been able to make out from the above exercise with my incomplete understanding is that $1/2$ must be in $C$ and as such, $1/4$ , $1/8$ , $1/16$ ... all the way to $0$ . But I'm not sure if this means that the closure $C$ is the set $[0,1]$ . It seems like it might be right, but I can't understand how it fits into the definitions above! In conclusion, can someone please explain to me why the sets $C^*$ and $C_*$ are defined the way they are, and how I can understand the intuition behind them better? I feel like this is all just an issue with me not understanding the formal definitions. I'm self studying this course before I start my first year at university, so you guys are the closest thing I have to a professor! Thank you!","['elementary-set-theory', 'definition']"
3698864,Why does the number of possible probability distributions have the cardinality of the continuum?,"Wikipedia's article on parametric statistical models ( https://en.wikipedia.org/wiki/Parametric_model ) mentions that you could parameterize all probability distributions with a one-dimensional real parameter, since the set of all probability measures & $\mathbb{R}$ share the same cardinality. This fact is mentioned in the cited text (Bickel et al, Efficient and Adaptive Estimation for Semiparametric Models), but not proved or elaborated on. This is pretty neat to me. (If I'd been forced to guess, I would have guessed the set of possible probability distributions to be bigger, since pdfs are functions $\mathbb{R}\rightarrow\mathbb{R}$ , and we're counting probability distributions that don't have a density, too. It's got to be countable additivity constraining the number of possible distributions, but how?) Where could I go to find a proof of this, or is it straightforward enough to outline in an answer here? Does its proof depend on AC or the continuum hypothesis? We need some kind of condition on the cardinality of the sample space that neither Wikipedia or Bickel mention, right (if it's too big, then the number of degenerate probability distributions is too big)?","['statistical-inference', 'statistics', 'cardinals', 'measure-theory', 'probability-theory']"
3698934,Characteristic Function as a Fourier Transform,"The fourier transform of a function is defined to be: $$\hat{f}(\omega)=\int_{R}e^{-it\omega}f(t)dt$$ which I understand that essentially $e^{-it\omega}$ controls the frequency at  which our function $f(t)$ is wrapped around the unit circle in the complex plane, with $f(t)$ dictating the radius of the polar graph for a given $t$ . That is, if my frequency is $10$ then every rotation around the unit circle traverses $\frac{1}{10}$ seconds of my graph $f(t)$ in 10 rotations I have covered 10 seconds of my function $f(t)$ . The fourier transform outputs the central mass for a given frequency across all $t$ of our polar graph. For  wave functions this  intuitively makes  sense but what does the fourier transform for our probability density function tell us? How do I interpret frequency with respect to a non-wave function?","['characteristic-functions', 'fourier-analysis', 'probability-theory', 'probability']"
3698935,Lebesgue Measure: Is it Optimal?,"Consider a collection $\Sigma$ of subsets of $\mathbb{R}^d$ such that $A, B \in \Sigma \implies (A \cup B \in \Sigma
    \hspace{0.3mm} \text{ and } A \cap B \in \Sigma
    \hspace{0.3mm} \text{ and } A \setminus B \in \Sigma).$ $\Sigma$ is closed under countable unions and countable intersections. If $A$ is either open or closed in the usual topology of $\mathbb{R}^d$ , then $A \in \Sigma$ . If $A \in \Sigma$ and $c \in \mathbb{R}^d$ , then $c + A \in \Sigma$ . where $x \in c + A \iff x-c \in A$ .
Also, consider a function $\mu : \Sigma \to [0,+\infty]$ such that $\mu([0,1]^d) = 1$ . $\mu(\varnothing) = 0$ . If $A \in \Sigma$ and $c \in \mathbb{R}^d$ , then $\mu(c+A) = \mu(A)$ . $\mu$ is countably additive on $\Sigma$ . Then, if I'm not mistaken, it can be shown that If $A \in \Sigma$ and $A$ is Lebesgue measurable, then $\mu(A) = m(A)$ where $m$ is the Lebesgue measure. Loosely speaking, my question is what if $A$ is not Lebesgue measurable? Is it possible that a non- Lebesgue measurable set $A$ can be in $\Sigma$ , or would this automatically contradict disjoint additivity? Is it possible that $\mu(A)$ is not equal to the Lebesgue outer measure when $A \in \Sigma$ and $A$ is not Lebesgue measurable? An outline of a proof would be beautiful. If that's not feasible, then a reference would also be greatly appreciated. Thank you for any insight on the matter.","['measure-theory', 'lebesgue-measure']"
3698936,If $\{f_n\}$ is $L^1$-weakly convergent sequence then $\{f_n\}$ converges in measure?,"Let $(E,\mathcal{A},\mu)$ be a finite measure space. Let $\{f_n\}$ be a $L^1$ -weakly convergent sequence to $f\in L^1$ . Can we say that $\{f_n\}$ converges in measure to $f$ ?","['measure-theory', 'convergence-divergence', 'probability-theory']"
3699029,When do three closed balls have a nonempty intersection?,"Consider a real Hilbert space $\mathcal{X}$ . For $(c,\rho)\in\mathcal{X}\times
\mathbb{R}$ , I denote the closed ball $B(c;\rho) = \{x \in \mathcal{X}\, |\, \|x -c\|\leq\rho\}$ .
I am curious if y'all know of a generalization of the following equivalence: \begin{equation}
 \label{2sets} B(c_1;\rho_1) \cap B(c_2;\rho_2)
\neq \varnothing \quad \Leftrightarrow \quad \|c_1 - c_2\| \leq
\rho_1 + \rho_2.  \tag{*}
\end{equation} My question: Let $n\in\mathbb{N}$ , let $(c_i)_{1\leq i\leq n}\in\mathcal{X}^n$ , and let $(\rho_i)_{1\leq i\leq n}\in[0,+\infty[^n$ . Is there an equivalent statement for $$\bigcap_{1\leq i\leq n} B(c_i;\rho_i) \neq\varnothing,$$ in the same spirit of (*)? i.e. do you know of an equivalence which has (1) no quantifiers and (2) phrasing via finitely-many inequalities involving only $(c_i)_{1\leq i\leq n}$ and $(\rho_i)_{1\leq i \leq n}$ ?  I have not found a result, even for $n=3$ and $\mathcal{X}=\mathbb{R}^N$ . Here's a proof of (*), where $B_i$ denotes $B(c_i;\rho_i)$ : $(\implies)$ : Let $x \in B_1 \cap B_2$ . From the triangle inequality, $\|c_1 - c_2\| \leq \|c_1 - x\| + \|c_2 - x\| \leq \rho_1 + \rho_2$ . $(\impliedby)$ : Construct $x =\left(1 - \frac{\rho_1}{\rho_1+\rho_2}\right)c_1 + \frac{\rho_1}{\rho_1+\rho_2}c_2.$ Then $\|c_2 - x\| = (1 - \frac{\rho_1}{\rho_1+\rho_2})\|c_1-c_2\| \leq \rho_2$ , so $x \in B_2$ . Similarly, $\|c_1 - x\| = \frac{\rho_1}{\rho_1+\rho_2} \|c_2 - c_1\| \leq \rho_1$ , so $x \in B_1 \cap B_2 \neq \varnothing$ $\square$ Comments with partial results are appreciated! EDIT: This graph may be helpful. It also displays that commonly checked candidate intersection points (e.g. center of mass) are not always in the intersection.","['hilbert-spaces', 'convex-geometry', 'convex-analysis', 'geometry']"
3699031,Establishing a bijection,"Let $A,B$ be finite sets. Prove that $|A\cup B|=|A|+|B|-|A\cap B|$ by establishing a bijection from $A\cup B$ to $\{1,2,\ldots,|A|+|B|-|A\cap B|\}$ . These are some hints that I got but I'm still confused on how to come up with a bijection. Prove for disjoint finite sets $A,B$ ( i.e. $A\cap B=∅$ ) that $|A\cup B|=|A|+|B|$ by coming up with a bijection N→A∪B. $A\cup B=(A∖B)\cup(B∖A)\cup(A\cap B)$ is a disjoint (!) decomposition of $A\cup B$ . A=(A∖B)∪(A∩B) is a disjoint (!) decomposition of A B=(B∖A)∪(B∩A) is a disjoint (!) decomposition of B",['elementary-set-theory']
3699074,Proving $\operatorname{cos}(x+y)=\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y)$ using differentiation,While proving $\operatorname{cos}(x+y)=\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y)$ by this $$\operatorname{sin}(x+y)=\operatorname{sin}(x)\operatorname{cos}(y)+\operatorname{cos}(x)\operatorname{sin}(y) \\ \text{differentiating both sides w.r.t } x \\ \operatorname{cos}(x+y) \left(1+\frac{dy}{dx}\right)=(\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y))\left(1+\frac{dy}{dx}\right)\\ \text{for $\frac{dy}{dx} \neq -1$}\\\operatorname{cos}(x+y)=\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y) $$ Now I am confused what happens when $\frac{dy}{dx} = -1$,"['trigonometry', 'derivatives', 'almost-everywhere']"
3699121,Obtaining a series that converges to $\frac{\pi}{2\sqrt{2}}$ using Fourier series.,"I'm working in some exercises about Fourier series (I'm new in that topic) but the next exercise is so hard. Prove, finding the appropriate Fourier series, that $$\dfrac{\pi}{2\sqrt{2}}=1+\dfrac{1}{3}-\dfrac{1}{5}-\dfrac{1}{7}+\cdots$$ First of all, I noted that the series $\displaystyle\sum_{n=1}^{\infty}\dfrac{1}{2n-1}$ is divergent by the comparison test with the divergent series $\displaystyle\sum_{n=1}^{\infty}\dfrac{1}{2n}$ . Then, how can the series converges to $\dfrac{\pi}{2\sqrt{2}}$ ? Or am I missing something? Then, I tried to follow the steps in this answer but I can't because I don't have any function or an option to use. How can I do? Any hint? I really appreciate any help you can give me.","['fourier-series', 'sequences-and-series', 'real-analysis']"
3699123,Example of the non-commutative ring with the set of units are commutative,I was looking for an Example of the non-commutative ring with the set of units are commutative. it will be a great help. Thanks in advance.,"['ring-theory', 'abstract-algebra', 'noncommutative-algebra', 'commutative-algebra']"
3699135,do all matrices with $\det(A)=\pm 1$ form a group under multiplication?,"all matrices with determinant one form the special linear group. it is explained that because $\det(A) \det(B)=\det(AB)$ it is closed as $1*1=1$ and because the general linear group is a group, and special linear group is a part of the general one, and because all of the inverses must have determinant 1 and also be in the special linear group, the inversion axiom holds. doesn't this also hold for the group of matrices defined by $\det(A)= \pm 1$ ? $$(1)(1)=1,(-1)1=-1,1(-1)=-1,(-1)(-1)=1$$ so it is closed.can a similar argument to the special linear group prove this is a group? can anyone provide a counter example or prove this?","['group-theory', 'linear-algebra']"
3699161,"Show that $|b-a|\geq|\cos a-\cos b|$ for all real numbers $\,a\,$ and $\,b$","$\mathbf{Question:}$ Show that $|b-a|\geq|\cos a-\cos b|$ for all real numbers a and b. $\mathbf{My\ attempt:}$ The Mean Value Theorem states that if $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$ then there exists $c \in (a,b)$ such that $f'(c)=\frac {f(b)-f(a)}{b-a}.$ Using the MVT where $f(c)=\cos c:$ $$
f'(c)=\frac {f(b)-f(a)}{b-a}
$$ $$
f'(c)(b-a)=f(b)-f(a)
$$ $$
(-\sin c)(b-a)=\cos(b)-\cos(a)
$$ $$
(\sin c)(b-a)=\cos(a)-\cos(b)
$$ Taking the absolute value of both sides: $$
|\sin c||b-a|= |\cos(a)-\cos(b)|
$$ Because $|\sin c |\leq 1,\,$ we can bound $|\sin c|$ by $1$ $$
1 \cdot|b-a| \geq |\cos(a)-\cos(b)|
$$ Thus $|b-a|\geq|\cos a-\cos b|$ for all real numbers $a$ and $b$","['calculus', 'functions', 'absolute-value', 'real-analysis']"
3699167,"$f''(x) = g(x)$ and $g''(x) = f(x).$ Suppose also that $f(x)g(x)$ is linear in $x$ on $(a,b).$ Show that $f(x) = g(x) = 0$ for all $x ∈ (a,b).$","QUESTION: Let $f$ and $g$ be two non-decreasing twice differentiable functions defined on an interval $(a,b)$ such that for each $x ∈ (a,b), f''(x) = g(x)$ and $g''(x) = f(x).$ Suppose also that $f(x)g(x)$ is linear in $x$ on $(a,b).$ Show that we must have $f(x) = g(x) = 0$ for all $x ∈ (a,b).$ MY ANSWER: I have done the proof, but it's not a rigorous one.. this is what I did- Let $f(x)=x^k$ , $(k>0)$ which is increasing on $(a,b)$ . Now, $$f'(x)=kx^{k-1}$$ $$f''(x)=k(k-1)x^{k-2}$$ According to the question, $g(x)=k(k-1)x^{k-2}$ , therefore, $$g'(x)=k(k-1)(k-2)x^{k-3}$$ $$g''(x)=k(k-1)(k-2)(k-3)x^{k-4}$$ Now according to the question again, $f(x)=g''(x)$ , but $f(x)=x^k$ , therefore our statement implies that, $$x^k=k(k-1)(k-2)(k-3)x^{k-4}$$ Also, it is said that $f(x)g(x)$ must be linear in $x$ . Therefore, we observe that, $$k(k-1)(k-2)(k-3)x^kx^{k-4}$$ must be linear in $x$ . Which clearly states that, $$k+(k-4)=1$$ $$\therefore 2k-4=1$$ $$\implies k=\frac{5}2$$ Putting $k=\frac{5}2$ in the previous equation, we get, $$x^4=\frac{5}2(\frac{5}2-1)(\frac{5}2-2)(\frac{5}2-3)$$ $$\implies x^4=-\frac{15}{16}$$ which is clearly impossible for any $x$ in $\mathbb{R}$ . Therefore, we may conclude that $$k\neq\frac{5}2$$ and the only way to satisfy both the above statements is to make $x=0$ . Therefore, we can conclude that $f(x)=0$ and consequently $g(x)=0$ Note 1: we observe, if $k<4$ then the the value of the derivatives becomes zero somewhere in between and our proof works. Note 2: if we had assumed the function more generally as $f(x)=x^k+c$ then too, it would have worked, only $c$ would have become zero at last (it is forced to become).. Now, there are a hell lot of non-decreasing functions out there (even trigonometric functions if defined in suitable intervals) and obviously this proof is not rigorous. Without assuming any function, how do I proceed to do this? Any help will be much appreciated. Thank you.","['functions', 'derivatives']"
3699278,All prime divisors of $\frac{x^m+1}{x+1}$ are of the form $2km+1$.,"Let $m$ be an odd prime and $x$ be the product of all primes of the form $2km+1$ . Then all prime divisors of $\frac{x^m+1}{x+1}$ are of the form $2km+1$ . What I know is that $\frac{x^m+1}{x+1}$ is an integer. Here is the link to the answer which prompted this question. Can anyone help me how to prove this.
Any help would be appreciated. Thanks in advance.","['algebraic-number-theory', 'number-theory', 'elementary-number-theory', 'analytic-number-theory', 'prime-numbers']"
3699311,Matrix of the differentiation operation [duplicate],"This question already has an answer here : How do you write a differential operator as a matrix? (1 answer) Closed 4 years ago . Exercise: Find the matrix of the derivative operation $D$ related to the base $\{1, t, t^2,..., t^n\}$ $$D: \mathcal P_{n} \to \mathcal P_{n}$$ I found a possible solution to this exercise, given that $D(t^k)=kt^{k-1}$ $$ \begin{equation*}
D_{n+1,n+1} = 
\begin{pmatrix}
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 2 & \cdots & 0 \\
\vdots  & \vdots  & \vdots &\ddots & \vdots  \\
0 & 0 & 0 &\cdots & n \\
 0 & 0 & 0 &\cdots & 0 \\
\end{pmatrix}
\end{equation*}$$ Nevertheless, it doesn't convince me at all, because when multiplying the matrix with the vectors in $\mathcal P_{n}$ , the exponent remains the same. Is this solution correct?","['matrices', 'linear-algebra', 'linear-transformations']"
3699345,'Irregular' conformal mapping of square onto circle?,"Assume that I want to find the conformal mapping from a square onto the unit disk. In the regular case, where the four edge points are positioned along cardinal directions, this mapping seems to have a relatively simple solution . I have plotted this in the figure below (a). The scenario I am interested in is a bit more complicated. I still want to map from a square (or rectangle) onto a disk, but I want to place the edge points of the square irregularly on the target circle (figure b). Assume that I know the positions of these target points. Is it possible to find a mapping like this? If so, how would I go about solving this issue?","['complex-analysis', 'conformal-geometry']"
3699346,Solutions to $\sqrt{3 - \tan^2(\frac{3x}{2})} \cdot \sin x - \cos x = 2$.,"$$\sqrt{3 - \tan^2(\frac{3x}{2})} \cdot \sin x - \cos x = 2.$$ How can such an equation be solved? I don't know where to start? My attempt: $$(3 - \frac{\sin^2 3 y}{\cos^2 3 y}) \cdot 2 \cos y \sin y = (3 - 2 \sin^2 y)^2$$ , where $y = \frac{x}{2}$ . This does not simplify anything at all, I do not understand what to do next","['algebra-precalculus', 'trigonometry']"
3699357,Evaluation of Singular Integrals by Complex Contour Integration,"I am learning to compute singular integrals using the ' $i\epsilon$ -prescription' of complex contour integration from the book Mathematical Physics by V. Balakrishnan (Chapter 23, Article 23.3.4). Following the book, the integral I want to compute is, $$f(x_0)=\displaystyle{\int_a^b dx\frac{\phi (x)}{x-x_0}}\tag{1}$$ where $a < x_0 < b$ are real numbers, $\phi(x)$ is a sufficiently smooth function, and $\phi(x_0)\neq 0$ . The integrand obviously diverges at $x = x_0$ because of the factor $(x − x_0)$ in the denominator. As it stands, the Riemann integral in Eq.(1) does not exist, because of this nonintegrable singularity of the integrand. Suppose, however, we move the singularity away from the path of integration by giving it either a positive
  imaginary part $+i\epsilon$ (where $\epsilon > 0$ ), or a negative imaginary part $−i\epsilon$ . This step, called an $i\epsilon$ -prescription, makes the original integral well-defined. The integrals in the two cases are given, respectively, by $$f(x_0 ± i\epsilon) = \int_a^b dx \frac{\phi(x)}{x−(x_0±i\epsilon)} = \int_a^b dx\frac{\phi(x)}{x−x_0∓i\epsilon}\tag{2}$$ The question is: what happens as $\epsilon \to 0$ ? We can continue to keep the integral well-defined by distorting the path of integration away from the approaching singularity, to form a small semicircle of radius $\epsilon$ . This semicircle lies in the lower half-plane in the case of $\boldsymbol{f(x_0+i\epsilon)}$ , and in the upper half-plane in the case of $\boldsymbol{f(x_0 − i\epsilon)}$ . This is shown in figures (a) and (b) : On the small semicircles, the variable of integration is $z = x_0 + \epsilon e^{i\theta}$ , so that $dz =\epsilon e^{i\theta}id\theta$ . The argument $\theta$ runs from $\pi$ to $2\pi$ in the case of $f (x + i\epsilon)$ , and from $\pi$ to $0$ in the case of $f (x − i\epsilon)$ . Taking the limit $\epsilon \to 0$ then yields the Cauchy principal value integral from $a$ to $b$ , plus the contribution from the semicircle: $$\lim_{\epsilon \to 0}\int_a^b dx\frac{\phi(x)}{x−x_0∓i\epsilon}= P\int_a^b dx\frac{\phi(x)}{x-x_0}± i\pi\phi(x_0)\tag{3}$$ I have two questions : Firstly, I do not understand the line in bold. Why the semicircle lies in the lower half-plane in the case of $f(x_0+i\epsilon)$ , and in the upper half-plane in the case of $f(x_0 − i\epsilon)$ ? Secondly, when I am computing the contribution from the semi-circle, I do not get the value $i\pi\phi(x_0)$ . This is how I am doing it for $f(x_0+i\epsilon)$ : $$\begin{align}
\lim_{\epsilon \to 0}\int_{-\epsilon}^{+\epsilon}dz\frac{\phi(z)}{z-x_0-i\epsilon}&=\lim_{\epsilon \to 0}\int_{\pi}^{2\pi} d\theta \,\,\epsilon e^{i\theta}i\frac{\phi(x_0+\epsilon e^{i\theta})}{\epsilon (e^{i\theta}-i)}=i\lim_{\epsilon \to 0}\int_{\pi}^{2\pi} d\theta \,\, e^{i\theta}\frac{\phi(x_0+\epsilon e^{i\theta})}{(e^{i\theta}-i)}\\
&=i\lim_{\epsilon \to 0}\left(\left[\phi(x_0+\epsilon e^{i\theta})\int d\theta\frac{e^{i\theta}}{(e^{i\theta}-i)}\right]_{\pi}^{2\pi}-\epsilon\int_{\pi}^{2\pi} d\theta \,\left\{\frac{d\phi(x_0+\epsilon e^{i\theta})}{d\theta}\int d\theta \frac{e^{i\theta}}{(e^{i\theta}-i)}\right\}\right)
\end{align} $$ The second term vanishes by the application of the limit. Then, $$\begin{align}\lim_{\epsilon \to 0}\int_{-\epsilon}^{+\epsilon}dz\frac{\phi(z)}{z-x_0-i\epsilon}&=\lim_{\epsilon \to 0}\left(\left[\phi(x_0+\epsilon e^{i\theta})\ln{(e^{i\theta}-i)}\right]_{\pi}^{2\pi}\right)\\
&=\lim_{\epsilon \to 0}\left(\phi(x_0+\epsilon e^{i\theta})\ln{\left(\frac{1-i}{-1-i}\right)}\right)\\
&=\phi(x_0)\ln{\left(\frac{i-1}{i+1}\right)}\\
&=\phi(x_0)\ln{\left(\frac{{(i-1)}^2}{-2}\right)}\\
&=\phi(x_0)\ln{i}\\
&=\phi(x_0)\ln{e^{i\pi/2}}\\
&=i\frac{\pi}{2}\phi(x_0)
\end{align}$$ So, a factor of 2 is coming in the denominator which should not be there. Where am I getting wrong? Please help.","['complex-analysis', 'contour-integration', 'singular-integrals']"
3699409,Exchanging derivative and integral,"Define the function $g:[1,2]\rightarrow \mathbb{R}$ to be $$
g(t):= \int_0^{\frac{\pi}4} \arctan\frac{\sin 2x}{t-2\sin^2 x}\,dx.
$$ I would like to differentiate obtaining \begin{align*}
g'(t) &= \frac{\rm d}{{\rm d}t}\int_0^{\frac{\pi}4} \arctan\frac{\sin 2x}{t-2\sin^2 x}\,dx 
=\int_0^{\frac{\pi}4} \frac{\partial}{\partial t}\left(\arctan\frac{\sin 2x}{t-2\sin^2 x}\right)\,dx=\\
&= -\int_0^{\frac{\pi}4} \frac{\sin 2x}{2(t-1)\cos 2x + t(t-2)+2} \,dx.
\end{align*} and I would like to be sure to be allowed to do that. Call $f(x,t)$ the integrand function, defined over $\left[0,\frac{\pi}4\right]\times [1,2]$ . For all $t\in [1,2]$ we have $|f(x,t)|\leq \frac{\pi}2$ , which is integrable over $\left[0,\frac{\pi}4\right]$ , and for all $x\in \left[0,\frac{\pi}4\right]$ we have that $\frac{\partial f}{\partial t}(x,t)$ exists and satisfies $\left|\frac{\partial f}{\partial t}(x,t)\right|\leq \sin 2x$ , which again is summable over $\left[0,\frac{\pi}4\right]$ . Are this domination conditions enough to be allowed to derive under
  the integral sign?","['integration', 'derivatives', 'real-analysis']"
3699411,Triangles and circles,"How can I generalize this problem ?
That is, how to express $R$ as a function of $R_1,R_2,R_3$ in this triangle: The linked problem describes the special case that the outer triangle is equilateral, and there is an interesting solution in that case. This generalization seems of equal interest, and hopefully there is also a nice solution. I applied the Pythagorean Theorem several times but I couldn't isolate $R$ .
Can someone please help me?","['euclidean-geometry', 'circles', 'geometry']"
3699437,$\alpha$ is unique if $f(x) \leq \alpha \leq g(x)$ for all $x$ and $\lim_{x\to a} ( g(x)-f(x)) = 0$,"Let's say we have a function $f$ and another one as $g$ , both are functions of, say, $x$ . Let $\alpha$ be the number which lies between $f$ and $g$ for every $x$ , that is $$
∀x ( f(x) \leq \alpha \leq g(x) )$$ It is a given condition that $$\lim_{x\to a} ( g(x)-f(x)) = 0$$ Now, I want to know what does it mean to say "" $\alpha$ is the only number in between $f$ and $g$ in the limiting process as $x$ goes to $a$ "". My understanding speaks like this: The given condition that difference between $g$ and $f$ reduces to zero as $x$ goes to $a$ rigorously means that we can make $g(x)$ as close to $f(x)$ as we desire, and since $\alpha$ is always going to lie between them, a stage would come when $\alpha$ will be the only number in between them. But the flaw in my understanding is that it violates the elementary statement ""Between any two numbers there lies infinitely many numbers, no matter how they close they are"". So, no matter how close we can make $g$ to $f$ there always gonna lie infinitely many numbers not just $\alpha$ .","['real-numbers', 'limits', 'calculus', 'real-analysis']"
3699451,An application of Fubini’s theorem on Fourier transform,"Given $f,g\in L^1(\mathbb{R}^n)$ and we denote the Fourier transform of $f$ by $\widehat{f}$ . I want to prove that $$\int_{\mathbb{R}^n}\widehat{f}(x)g(x)~dx= \int_{\mathbb{R}^n}f(x)\widehat{g}(x)~dx.$$ Here’s my attempt: \begin{align}
\int_{\mathbb{R}^n}\widehat{f}(x)g(x)~dx=&\int _{\mathbb{R}^n}\left\{\int _{\mathbb{R}^n} f(t)e^{-2\pi it\cdot x}~dt\right\}g(x)dx\\
=&\int _{\mathbb{R}^n}\left\{\int _{\mathbb{R}^n} g(x)f(t)e^{-2\pi it\cdot x}~dt\right\}dx\\
{\color{red}{=}}&\int _{\mathbb{R}^n}\left\{\int_{\mathbb{R}^n} g(x)f(t)e^{-2\pi it\cdot x}dx\right\}~dt\\
=&\int_{\mathbb{R}^n}\left\{\int_{\mathbb{R}^n} g(x)e^{-2\pi it\cdot x}dx\right\}f(t)~dt\\
=&\int_{\mathbb{R}^n}f(t)\widehat{g}(t)~dt\\
=&\int_{\mathbb{R}^n}f(x)\widehat{g}(x)dx.
\end{align} In the third equation I used Fubini’s theorem. But here’s something that I’m not sure : if we want to apply Fubini’s theorem, then $F(x,t):= g(x)f(t)e^{-2\pi it\cdot x}$ must be $\mathbb{R}^n\times\mathbb{R}^n$ integrable. But I was stuck when trying to prove that $F(x,t)$ is integrable. Could you give me some help? Thanks!","['fourier-analysis', 'fourier-transform', 'metric-spaces', 'real-analysis', 'lp-spaces']"
