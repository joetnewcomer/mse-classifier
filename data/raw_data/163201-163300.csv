question_id,title,body,tags
2836970,Cavalieri's Principle,"In my measure theory class, my teacher wrote the following theorem: Theorem (Cavalieri's Principle): Let $f:\Omega\to [0,+\infty]$ be a measurable function. Then for every monotone, $\mathscr{C}^1$, function $\Phi:\mathbb{R}_+\to\mathbb{R}_+ $ which satisfies $\Phi(0)=0$, we have
  $$\int_\Omega \Phi(f(x))\:\mathrm{d}x=\int_{\mathbb{R}_+} \Phi'(\lambda)\rho_f(\lambda)\:\mathrm{d}\lambda.$$ Also, he has defined $\rho_f$ as the following function:
$$\lambda\mapsto \mu(\{x\in\Omega\:|\:f(x)>\lambda\}),$$
where $\mu$ is Lebesgue's measure. I (kind of) understand this theorem but I don't know why it is called Cavalieri's principle. It does not seem related to Cavalieri's principle to me. Am I missing some intuition here?","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2836973,Sigma algebra unequal to its universal completion,"Let $(\Omega, \mathcal{F}, P)$ be a probability space. The completion of $\mathcal{F}$ w.r.t. $P$ is the smallest sigma-algebra that contains $\mathcal{F}$ and all subsets of $P$-nullsets in $\mathcal{F}$. The universal completion is defined as the intersection of all universal completions of $\mathcal{F}$ w.r.t all probability measures on $(\Omega, \mathcal{F})$.
What is a (as minimal as possible) working example of a sigma-algebra that differs from its universal completion.","['probability-theory', 'measure-theory']"
2837014,"Qualitative study of $ \dot x = - y^2$, $\dot y= x^2 $","Make the qualitative study of the ODE on the plane
$$ \begin{cases}\dot x = - y^2\\ \dot y= x^2 \end{cases} $$
and determine how many solutions satisfy $y(0) = x(1) = 0$. My attempt. Let $f(x,y):=\begin{pmatrix}-y^2\\x^2 \end{pmatrix}$ be the vectorial field. It is well defined on all plane and $C^1$, then every Cauchy Problem has a unique local solution and every one of these solution can be extended in unique way to a maximal solution (not necessary globally defined). Now I note that the function $$E(x,y):=\frac{x^3}{3}+\frac{y^3}{3}$$ is a constant of motion, i.e. $\dot E(x(t),y(t))=c$, where $(x(t),y(t))$ is a solution. The orbits are the level sets of $E$, that are open curves like this: https://m.wolframalpha.com/input/?i=plot+x%5E3+%2B+y%5E3+%3D+10 , or https://m.wolframalpha.com/input/?i=plot+x%5E3+%2B+y%5E3+%3D+-10 . It is all correct? Do you think that it is enough ? The task ""Make the qualitative study"" is very generic. What would you add? I don't know how to determine how many solutions satisfy $y(0) = x(1) = 0$.","['ordinary-differential-equations', 'dynamical-systems', 'proof-verification']"
2837021,Algebraic manipulation in Vaughan's book on the circle method,"On page 125 of Vaughan's 1 book on the circle method we have in Chapter 5.2 on Waring's problem for $k=1$. I do not understand how to get from $(1)$ to $(2)$:
\begin{align}
f(z)^s &= \frac{1}{(1-z)^s}\tag{1}\\
&=\frac{1}{(s-1)!} \frac {d^{s-1}} {dz^{s-1}} \frac 1{1-z}\tag{2}
\end{align} I wonder what the step by step manipulations that go from $(1)$ to $(2)$? [1]  R. C. Vaughan, The Hardy-Littlewood method Cambridge University Press (1997)","['number-theory', 'complex-analysis', 'complex-integration']"
2837039,Integral representation for Euler constant,"I am trying to show that $$\gamma=\int_0^{\infty}\frac{\ln(1+x)+e^{-x}-1}{x^2}\;dx$$ I haven't done so much, but since this is related to $\psi (1)=\Gamma'(1)$ I have tried to work backwards. So $$\Gamma(x)=\int_0^{\infty} t^{x-1}e^{-t}\;dt\rightarrow \Gamma'(x)=\int_0^{\infty} t^{x-1}e^{-t}\ln t\;dt$$ $$\int_0^{1} t^{x-1}e^{-t}\ln t\;dt+\int_0^{1} \frac{e^{-\frac{1}{t}}\ln t}{t^2}\;dt$$ Where I used $t=\frac{1}{t}$ in the second integral, I also tried to recombine those two, but of no use. Also this is equal to: $$\int_0^{\infty} (\ln\left(1+\frac{1}{x}\right)+e^{-\frac{1}{x}}-1)dx$$ Now I am thinking to use series. Could you give me some help or hints?",['integration']
2837070,Derivative of matrix logarithm,"Let $A(x)$ be a differentiable matrix-valued function with $\det A(x)\ne 0\,\forall x$. I understand that
$$\frac{d}{dx}\log A(x)$$
does not have a simple expression in terms of $A$ and $dA/dx$ unless these two things commute, in which case the expression is
$$A(x)^{-1}\frac{dA}{dx}.$$
Let's say I don't want to assume that $A$ and $dA/dx$ commute but I only care about the trace. Is it true for any differentiable non-singular function $A(x)$ that
$$\text{tr}\left(A(x)^{-1}\frac{dA}{dx}\right)=\frac{d}{dx}\text{tr}\,\log A(x)=\frac{d}{dx}\log\det A(x)$$","['derivatives', 'matrices', 'logarithms', 'matrix-calculus', 'calculus']"
2837079,Infinite Knight's Tour,"Does there exists a one-to-one and onto map $f:\mathbb{N}\rightarrow \mathbb{Z}[i]$ such that $|f(n+1)-f(n)|=\sqrt{5}$ ? That is to get from $f(n)$ to $f(n+1)$ you have to move like a knight in chess: move two vertically and then one horizontally or two horizontally and then one vertically. I know there is a lot written about when a knight tour is possible on a finite board. Check out the wiki and wolfram for some background. I can see how you might be able to do this in a type of infinite outward spiral if you could travel down a corridor of an arbitrary length. It seems that there is already some interest in that . But you would also need to make sure that you ""end"" your journey on the far side of the corridor in  such a way that you could turn to travel down the next corridor. Motivation: The key part of my question is the infinite part. I have recently been thinking about how we can traverse infinite spaces which is particularly relevant to mathematicians (as opposed to say computer scientists). For example when computing something over the entire set of Guassian integers. Let's say that some function $f$ does exist. Then $$\sum_{z\in \mathbb{Z}[i]} \alpha_z =\sum_{n=1}^\infty \alpha_{f(n)}$$ I am not sure that such a thing could practically be done. But my point is that it's valuable to know different ways we can traverse infinite spaces.","['combinatorics', 'knight-tours', 'complex-numbers']"
2837100,Convergence of a infinite series for using it with Kroneckers Lemma,"I have the following problem. Let $X_1,X_2,...$ be independent random variables with the same continuous distribution. Let $E_n = \{X_n>X_m, \forall m<n\}$ be the event of a new record at time $n$ and $N_n$ the amount of records at time $n$. What I already did is to show that $E_1, E_2,...$ are independent with $P(E_n) = \dfrac{1}{n}$. So far so good. But the main goal is to proof $\dfrac{N_n}{log(n)} \to 1$ a.s. The idea is to use Kroneckers Lemma for which I have to show that $\sum_{k=1}^{\infty} \dfrac{I_{E_k} - 1/k}{log(k)}$ converges. I don't know how to approach the latter. Any help is appreciated.","['probability-theory', 'convergence-divergence']"
2837109,Cartesian product as a collection of indexed families,"Given an indexed family of sets $(X_i)_{i\in I}$, a canonical definition of Cartesian product is:
$$
\prod_{i \in I} X_i = \left\{f\ \Big|\ f: I \longrightarrow \bigcup_{i \in I} X_i\ \wedge (\forall i\in I)\big(f(i) \in X_i\big)\right\}
$$ I wonder if this definition is correct as well:
$$
\prod_{i \in I} X_i = \left\{t\ \Big|\ t=(x_i)_{i\in I}   \wedge (\forall i\in I)\big(x_i \in X_i\big)\right\}
$$ Setting $\mathcal{X}=\bigcup_{i \in I} X_i$, 
the domain of each family $t$ (in the second notation) is not  explicitly set, but values $t(i)$, that is $x_i$, should be in $X_i$ and so only those functions $t$ compatible with $\mathcal{X}$ are acceptable. Also, the cardinality of $\mathsf{Rng} f$ (in the first notation) is usually smaller than the cardinality of $\mathcal{X}$ and the same as $I$. That is not possible for $t$, since families are surjections .
So each $t$ should have a distinct domain, with the cardinality of $I$. Update I reply here to a comment regarding indexed families. I define an indexed family as an alias for surjective function. Given a surjective function:
\begin{align}
 x\colon I &\longrightarrow X \\
 i &\mapsto x_i = x(i),
\end{align}
it can be denoted also with: 
$$(x_i)_{i\in I}$$ 
The range of the function $x$ might be denoted with:
$$\{x_i\}_{i\in I}$$ 
 or
$\{ x_i \ \big|\ i \in I \} $. This is the standard definition I found in several books, e.g. Tourlakis, ""Lectures in Logic and Set Theory"" or Wikipedia .",['elementary-set-theory']
2837152,Assume that $k$ is a particular integer. Is $2k − 1$ odd?,Hello I'm having trouble wrapping my head around how $2k-1$ is odd. My solution shows this: Yes $2k-1$ is odd! $2k − 1 = 2(k − 1) +1$ and $k − 1$ is an integer because it is a difference of integers. This is my understanding. An odd number is represented as $2k+1$ so... $2k-1$ $2(k-1) +1$ Refer to $k-1$ as $n$ $2(n)+1$ Why can we substitute $k-1$ for $n$ ? Which operations on an integer equal to an integer?,['discrete-mathematics']
2837158,Is there any continuous surjective map from $\mathbb{R}^3$ to $\mathbb{R}^3$ s.t. for every $x$ in $\mathbb{R}^3$ $f(x) \cdot x= 0$?,"Is there any continuous surjective map from $\mathbb{R}^3$ to $\mathbb{R}^3$ such that for every $x$ in $\mathbb{R}^3$,  $f(x)  \cdot x=0$? What about i replace $\mathbb{R}^3$ to $\mathbb{R}^4$ ?","['algebraic-topology', 'functions']"
2837161,"Does least squares (approximate solution) minimize the orthogonal distance of $b$ to $Ax$, or does it minimize the error projected along the $b$ axis?","I have always been confused about whether the approximate solution to $Ax=b$ is equivalent to minimizing the average distance of all of the $b$ vectors to $Ax$, or whether it is minimizing the distance projected along the $b$ axis? (where $A$ is full rank and skinny and the system is overdetermined). Consider these two pictures: from http://www.statisticshowto.com/least-squares-regression-line/ and another figure on the same page: also from from http://www.statisticshowto.com/least-squares-regression-line/ . Notice these are two figures on the same page!  I do not understand how these are both being minimized at the same time.  can someone please explain?  thanks. this is a related question Least squares solutions and orthogonal projection?","['regression', 'regression-analysis', 'linear-regression', 'linear-transformations', 'linear-algebra']"
2837170,Sum of $\sum_{k=0}^{49} (-1)^{k} \binom{99}{2k}$,"I have a finite sum to evaluate. I do compute the sum using Euler's formula. I would appreciate a combinatorial argument, or an argument using a binomial identity. Problem Among all the sums obtained from adding the coordinates to the integral solutions of
\begin{equation*}
\sum_{k=0}^{49} (-1)^{k} \binom{99}{2k} = x^{y} ,
\end{equation*}
which is the biggest? Solution According to Euler's formula,
\begin{align*}
&2^{99/2} \left(\frac{-1}{\sqrt{2}} + i \frac{1}{\sqrt{2}}\right) \\
&\qquad = (2^{99/2}) e^{i \frac{3\pi}{4}} \\
&\qquad = 2^{99/2} e^{i \frac{99}{4} \pi} \\
&\qquad = (1 + i)^{99} \\
&\qquad = \sum_{k=0}^{49} (-1)^{k} \binom{99}{2k} + i \sum_{k=0}^{49} (-1)^{k} \binom{99}{2k+1} .
\end{align*}
So,
\begin{equation*}
-2^{49} = -2^{99/2} \left(\frac{1}{\sqrt{2}}\right) = \sum_{k=0}^{49} (-1)^{k} \binom{99}{2k} .
\end{equation*}
Since $-2$ is a prime number, the only integral solutions to the given equation are $(-2, \, 49)$, $(-2^{7}, \, 7)$, and $(-2^{49}, \, 1)$. The biggest sum obtained from adding the coordinates to the integral solutions is $-2 + 49 = 47$.",['combinatorics']
2837203,A subset of a finite set is finite,"I saw this problem in a book and the solution given and thought of a simpler solution. This makes me suspicious of its validity. If someone could point out why my solution is incorrect I would appreciate it. First, suppose that $|S|=1$ and $T\subset S$. Then $T=\emptyset$ or $S$. Hence it is finite.
Assume that if $|S|=n$ and if $T\subset S$ then $T$ is finite. Now let $|S|=n+1$ and $T\subset S$. If $T=S$ then indeed it is finite. If not, $\exists a\in S$ such that $a\notin T$. Then $T\subset S\setminus${$a$}. $|S\setminus${$a$}$|=n$ (which I have proved in a previous example). Then by the induction hypothesis $T$ is finite.","['elementary-set-theory', 'proof-verification']"
2837205,"Cardinality of $S=\{A,B\}$ where $A=B=\{1,2,3\}$.","Given a set $S = \{A,B\}$ such that $A = \{1,2,3\}$ and $B = \{1,2,3\}$, what is the cardinality of S? I know this may seem very trivial, and I am inclined to believe that the answer is $2$; my question comes from the fact that $A = B$. Since both $A$ and $B$ are the same, would that not be the same as having a repeated element, therefore making the answer $1$?","['elementary-set-theory', 'discrete-mathematics']"
2837213,$|f(x)-f(y)|\le C |g(x)-g(y)|$ and integrability of $g$ implies that of $f$,"Let $S\subset \mathbb R^n$ be a closed rectangle and $f,g: S\to \mathbb R$ functions such that $$|f(x)-f(y)|\le C |g(x)-g(y)|$$ for all $x,y\in S$. Show that if $g$ is Riemann integrable, then so if $f$, and if $f$ is, then so if $|f|$. There is an integrability criterion which I don't know how to connect with the given inequality. If $P$ is a partition, then $U(f,P)-L(f,P) < \epsilon $. Or should I use something different? Let $P$ be a partition of $S$ such that $(U(g,P) - L(g,P)<\epsilon/C$. On each subrectangle of partition $s$, $\sup_s f-\inf_s f=f(x_s)-f(y_s)$ (the sup and inf are attained because $f$ is continuous on the compact $s$). For each $s$, $|f(x_s)-f(y_s)|\le C |g(x_s)-g(y_s)| \le C (\sup_s g-\inf_s g)$. Thus $$U(f,P)-L(f,P)=\sum_s (\sup_s f-\inf_s f)\operatorname{vol}(s)\le C \sum_s(\sup_s g-\inf_s g)\operatorname{vol}(s)=C (U(g,P) - L(g,P)) < \epsilon$$","['multivariable-calculus', 'real-analysis', 'integration', 'calculus']"
2837223,If component functions are injective then is the function injective?,"If we have injective continuously differentiable function $f$, and if we define continuously differentiable function $g(x,y)=(f(x),y)$, then does it follow that $g$ is injective? Thanks","['real-analysis', 'calculus', 'multivariable-calculus', 'proof-writing', 'analysis']"
2837230,Pythagorean theorem in higher dimensions?,"When using vector notation in coordinate systems (Cartesian coordinates) we see that the magnitude of a vector in two dimensions is equal to the square root of its Y component squared added to its X component squared (Pythagorean theorem). But the same calculation is done for a three dimensional vector that has X, Y, and Z components. Is there a triangle that has four sides? (of course not, but how does this right triangle formula work for a calculation that involves more than two dimensions?).","['vectors', 'geometry']"
2837248,Is any square norm a $\mathcal{C}^\infty$ function?,"Is it true that if $\|\cdot\|:\mathbb{R}^n \to \mathbb{R}$ is an arbitrary norm
  then $\|\cdot\|^2$ is $\mathcal{C}^\infty$ function ? If the norm comes from an inner product $\langle\cdot,\cdot\rangle$, i.e. $$\|x\|^2 = \langle x , x \rangle; $$
Then it is easy to see that  $\| \cdot\|^2$ is a $\mathcal{C}^\infty$ function, because, if $\{v_1,...,v_n\}$ is an orthonomal basis with respect to the inner product $\langle\cdot, \cdot\rangle$, and $A$ $\in$ $M_n(\mathbb{R})$ is the change basis matrix of $\{e_1,...,e_n\}$ to $\{v_1, ..., v_n\}$ then $$\|x\|^2=\langle x, x\rangle = (A\cdot x)^{\text{T}}G (A\cdot x)$$ where $G$ is a matrix such that $[G]_{ij} = [\langle v_i, v_j\rangle]$. Using the above formula is easy to conclude that $\|\cdot \|^2$ is a $\mathcal{C}^\infty$ function. But when $\|\cdot \|$ is an arbitrary norm I just was able to conclude that $\| \cdot \|^2$ is a continuous function. Can anyone help me?","['multivariable-calculus', 'normed-spaces', 'analysis']"
2837299,Good book on large deviations theory,I am interested in reading about large deviations theory. Can anybody please suggest me any good book regarding this. Thanks in advance.,"['reference-request', 'probability-theory', 'book-recommendation', 'large-deviation-theory']"
2837391,degree of line bundle as an integral,"We consider the compact Riemann surface $\mathbb{P}^1$ (my notation for the Riemann sphere). I make an open cover $\mathcal{U} = \{U_1, U_2\}$ where $U_1 = \{z \in \mathbb{C} | |z| < 1+ \epsilon\}$ and $U_2 = \{z \in \mathbb{P}^1 | |z| > 1- \epsilon\}$. Let $L \to X$ be the line bundle on $\mathbb{P}^1$ that is determined by some transition function $g_{12} : U_1 \cap U_2 \to \mathbb{C}^*$. In other words, if $h_1: L_{U_1} \to U_1 \times \mathbb{C}$ and $h_2: L_{U_2} \to U_2 \times \mathbb{C}$ are the local trivialisations of this line bundle (and the line bundle is fully determined by the transtion function), then $h_1 \circ h_2^{-1} (x, t) = (x, g_{12}(x) t)$. Now I want to show that $\deg L = \frac{1}{2\pi i} \int_{|z| = 1} \frac{g_{12}'(z)}{g_{12}(z)}dz$. Of course, $\deg L = deg(D)$ where $D$ is the divisor of a meromorphic section $s$ of $L$ over $X$. I'm not sure where to begin...the $2\pi i$ reminds me of the residue theorem, and degree feels like Riemann-Roch but I don't know where to go....may I please have some help? This is question 29.2 of Forster's Lectures on Riemann Surfaces.","['riemann-surfaces', 'sheaf-theory', 'complex-geometry', 'algebraic-geometry', 'vector-bundles']"
2837401,Volume form on $S^2$,"I have some basic understanding problem on this. Any help is appreciated: The volume form of $S^2 \subset R^3$ is given by $$ \omega = x \ dy \land dz-y \ dx \land dz +z \ dx \land dy$$ In polar coordinates this becomes
$$ \omega = sin\Theta\ d\Theta \land d\phi$$ A volume form ought to be non-vanishing everywhere on the sphere. But how can this be since at the pole $\Theta=0$ the form seems to vanish identically? Second confusion: A volume form cannot be exact. If it was, by the use of Stokes law, the area of the sphere would be identically to zero which cannot be true.
However if one writes, 
$$ \omega = sin\Theta\ d\Theta \land d\phi=d(-cos\Theta \land d\phi)\equiv d\Lambda$$
where $\Lambda=-cos\Theta \ d\phi$ $,\omega$ seems closed!?. $\Lambda$ appears single-valued,smooth and everywhere well defined on the sphere. What is wrong with $\Lambda$? How can one see that $\omega$ is not exact without resorting to the argument based on Stokes law. Many thanks!",['differential-geometry']
2837404,Topology: Homeomorphism between finite complement topology in $\mathbb{R}$ and one of its subspaces,"My class notes say that because $U=\mathbb{R}\backslash\{x_1,x_2,..,x_n\}$ has the same cardinality than $\mathbb{R}$, there exists a homeomorphism between: $(U,T_{cof})$ and $(\mathbb{R},T_{cof})$, where $T_{cof}$ is the finite complement topology. I initially thought that having the same cardinality is a necessary condition but is not sufficient to have an homeomorphism.  Also, I can't manage to find a homeomorphism between these two.",['general-topology']
2837423,Use ampleness of morphisms to investigate objects,"I noticed a mathematical technique twice recently: When we want to know more about an object $X$, a strategy is to make use of Hom$(X,-)$. This methods arised in two cases. Kodaira embedding theorem : Let $X$ be a compact Kahler manifold with a positive holomorphic line bundle. Then $X$ is holomorphically embeddable into CP$^n$. The proof simply uses the ""richness"" of $V := H^0(X;\mathcal{O}(F))$ (a finite dimensional vector space by the Hodge's theorem) for some line bundle $F$, taking a basis $\{s_0,...,s_N\}$ of the vector space $V$, and construct the embedding $\phi_{|F|}: X \to P^{\,N}$ by taking $x$ to $[s_0(x):...:s_N(x)]$. Still, some arguments are needed, but for me the whole point is that $H^0(X;\mathcal{O}(F))$ is ""large enough"" . Peter-Weyl theorem : Let $G$ be a topologically (locally) compact group. While we cannot expect there is always a faithful finite dimensional representation of $G$, there are still ""enough"" many finite dimensional representation: For any $x\neq y \in G$, there exists a finite dimensional representation $\phi: G\to GL(V)$ such that $\phi(x)\neq\phi(y)$. It is well-known that this theorem has lots of powerful corollaries, and is perhaps the most important basis for pur investigation on (locally) compact groups. Here the same technique appears again, namely using that Rep$(G)$ is ""large enough"" to make progress. I believe such techniques must arise quite often in mathematics, and I would like to know if there are some other such examples. Thank you very much.","['big-list', 'complex-geometry', 'algebraic-geometry', 'representation-theory', 'category-theory']"
2837441,Condition to ensure $n$ equations in $\mathbb P^n$ cut out maximal points,"Let $\mathbb P^n$ be the projective space over $\mathbb C$, and let $F_i$ be $n$ homogenous polynomial equations with ${\rm deg} F_i=d_i$. My question is: What is the right condition to ensure the number of solutions is $\Pi d_i$? At first we need to put some conditions to ensume the intersection is of dimension zero (I guess it is $F_i$ being a regular sequence?). But this condition seems not enough to ensure the number is $\Pi d_i$.","['abstract-algebra', 'intersection-theory', 'algebraic-geometry', 'commutative-algebra']"
2837450,How to show $ \dim X \times Y = \dim X + \dim Y$?,"Let $X, Y$ be affine algebraic sets inside $\mathbb{A}^n_{\mathbb{C}}$. 
I was wondering how can one show that 
$\dim X \times Y = \dim X + \dim Y$? Any comments are appreciated. Thank you.",['algebraic-geometry']
2837460,Is a number ring a Dedekind domain?,"I'm studying the number field sieve factorization method, but I am having some trouble with the definition of number ring.
I have found two different definitions: A number ring is a subring of a number field. A number ring is the subring of algebraic integers of some field. They should not be equivalent, for example if we take the number field $\mathbb{Q}[\sqrt{5}]$, then for the first definition $\mathbb{Z}[\sqrt{5}]$ is a number ring, but for the second definition, the number ring is $\mathbb{Z}\left[\frac{1+\sqrt{5}}{2}\right]$. I'm pretty sure that with the second definition we have that a number ring is a Dedekind domain.
But what happens with the first definition? Thanks for your help.","['number-theory', 'dedekind-domain', 'algebraic-number-theory']"
2837511,"Definition of integration of forms over manifolds, Spivak.","Let $c$ be an orientation preserving k-cube in $M$( k dimensional manifold with boundary with orientation $\mu$) such that $c_{(k,0)} $ lies in $\partial M$ and is the only face that has any interior points in $\partial M$. $c_{(i,\alpha)}=c\circ (I^n_{(i,\alpha)})$ and $I^n_{(i,\alpha)}=(x^1,\cdots , x^{i-1},\alpha,x^i,\cdots,x^{n-1})$ $\omega$ is a k-1 form on $M$ which is $0$ outside of $c([0,1]^k)$. 1) In $\int_{c_{(k,0)}} \omega =(-1)^k\int _{\partial M}\omega$ How did we get $(-1)^k$ ? 2) In $ \int _{\partial c}\omega=\int_{(-1)^kc_{(k,0)}}\omega=(-1)^k\int_{c_{(k,0)}} \omega =\int _{\partial M}\omega $ I am concerned with only the first equality and included the others just to give sense of what he is trying to show. Shouldn't it be $\sum_{i,\alpha} (-1)^k\int_{c_{(i,\alpha)}}\omega$ ?
Why other faces not appear in the integration as a sum?","['real-analysis', 'calculus', 'multivariable-calculus', 'manifolds', 'general-topology']"
2837543,Calculate $\sum_{n=1}^\infty\frac{1}{n^2}$ and $\sum_{n=1}^\infty\frac{1}{n^4}$,"I need to do that using $$\sum_{n \in \mathbb{Z}}\frac{1}{(z-n)^2}=\left(\frac{\pi}{\sin \pi z}\right)^2$$ I've already prove that this is true. The thing is that this function in meromorphic and it's poles are $\mathbb{Z}$. So the natural evauation in $0$ is not possible. I tried to think of a way to do it with that using that the singular part of this function in $n\in\mathbb{Z}$ is $\frac{1}{(z-n)^2}$, but could find a way. What I was looking was sort of substract the term where $n=0$ and divide by $2$. For the second sum, I didn't find a clear way. The natural evaluation in $n^2-n$ is not possible since $n$ varies in the sum. Thanks","['complex-analysis', 'meromorphic-functions']"
2837547,trouble understanding lemma for normal basis theorem,"I'm having trouble understanding the following proof: Lemma: Let $K \subseteq L$ be a Galois extension and the Galois group $G(L/K) = \{ \sigma_1, ..., \sigma _n \}$. Elements $x_1, ..., x_n \in L$ form a $K$-basis of $L$ if and only if $\det [\sigma_i(x_j)] \neq 0.$ Proof: The given elements are linearly dependent if and only if there are elements $a_1,...,a_n\in K$ not all equal to $0$ such that $a_1 x_1 + \dots + a_n x_n =0$. Letting all $\sigma _i$ act on this equality, we get \begin{align*} a_1 \sigma_1 (x_1) + \dots + a_n \sigma_1(x_n) &= 0 \\ 
a_1 \sigma_2 (x_1) + \dots + a_n \sigma_2(x_n) &= 0 \\ \dots \\
a_1 \sigma_n (x_1) + \dots + a_n \sigma_n(x_n) &= 0. \end{align*}
  The above system of linear equations has a nonzero solution $a_1,...,a_n$ if and only if the determinant of the coefficient matrix (consisting of $\sigma_i(x_j) $) equals $0$. I understand that if there is a nonzero solution $a_1,...,a_n \in K$, then this determinant must be zero. Conversely, I do not understand why the determinant being zero implies there is a nonzero solution $a_1,...,a_n \in K$. Since the coefficients are in $L$, the only thing I should be able to deduce is that there is a nonzero solution $a_1,...,a_n \in L$, while in general $K \neq L$. What am I missing?","['galois-theory', 'linear-algebra']"
2837548,What tools one should use for inequalities?,"If $a,b,c>0$ prove that:
$$\frac{1}{a+4b+4c}+\frac{1}{4a+b+4c}+\frac{1}{4a+4b+c}\leq \frac{1}{3\sqrt[3]{abc}}.$$
My first try was the following:
$$\sum_{cyc}\frac{1}{a+4b+4c}\leq\sum_{cyc}\frac{1}{\sqrt[3]{16abc}}=\frac{1}{\sqrt[3]{16abc}}$$
But $\frac{1}{\sqrt[3]{16abc}}\geq \frac{1}{3\sqrt[3]{abc}}$ The I have tried your method from another post:
$$\sum_{cyc}\frac{1}{a+4b+4c}=\sum_{cyc}\frac{1}{a+2b+2(b+2c)}$$
$$\sum_{cyc}\frac{1}{a+2b+2(b+2c)}\leq\sum_{cyc}\frac{1}{9}\left (\frac{1^2}{a+2b}+\frac{2^2}{b+2c}  \right )$$
Where I got $$\sum_{cyc}\frac{1}{3}\left ( \frac{1}{a+2b} \right )\leq \frac{1}{3\sqrt[3]{abc}}$$ wich is false.
$$$$","['inequality', 'a.m.-g.m.-inequality', 'substitution', 'multivariable-calculus', 'uvw']"
2837553,Prove this integral related to the Ising model,"I came across this integral when learning the Ising model. Without external field Onsager's solution of a 2D square lattice with $J_2=0$ should equal the solution of a 1D Ising model, which leads to this $$
\int_0^{2\pi}\ln \left(\cosh 2K-\sinh 2K\cos\theta\right)\,\mathrm{d}\theta=4\pi\ln\cosh K
$$
where $K\in\mathbb{R}$. I tested this integral numerically using Mathematica and it holds. But I want to prove it. I tried splitting the LHS to $$
2\pi\ln\cosh2K+\int_0^{2\pi}\ln(1-\tanh 2K\cos\theta)\,\mathrm{d}\theta
$$
and expanding the log function using Taylor series, but then the integration result became a complicated series $$
-\sum_{n=1}^\infty \frac{1}{2n}\frac{2\pi}{4^n}\binom{2n}{n}\tanh^{2n}2K
$$
which does not easily relate to the RHS. Anyone has better idea?  Ising model is famous so this may have been done long ago.","['hyperbolic-functions', 'real-analysis', 'calculus', 'definite-integrals', 'trigonometric-integrals']"
2837557,Proof without mean value theorem that continuously partially differentiable implies differentiability,"For simplicity we consider the case of $2$ factors. Let $E_1,E_2,F$ be Banach spaces and $f:X\to F$, where $X\subseteq E:=E_1\times E_2$ is open. By first partial derivative we mean the derivaitve of $f(\cdot,e_2)$ when we fix $e_2\in E_2$. Second (or other) partial derivatives are defined analogously. I'm trying to prove the following theorem from H. Cartan's Differential Calculus : This is the proof provided in the book: where the mean value theorem used is this: My question: Can we avoid the use of the mean value theorem? I think proving $(3.7.2)$ doesn't need the mean value theorem. Here is my proof: That $f$ is differentiable at $x_0\in E$ with derivative $\partial f(x_0)$ is equivalent to saying that $$f(x)=f(x_0)+\partial f(x_0)(x-x_0)+r(x)\|x-x_0\|,$$ where $r:X\to F$ is continuous at $x_0$ with $r(x_0)=0$. Therefore, we have (again considering the case $n=2$ for simplicity) $$f(x_1,x_2)-f(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)(x_1-a_1)=\Big(\frac{\partial f}{\partial x_1}(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)\Big)(x_1-a_1)+r(x_1,x_2)\|x_1-a_1\|.$$ Since $\frac{\partial f}{\partial x_1}$ and $r$ are continuous at $(a_1,a_2)$, we can choose $\delta>0$ such that, for all $\|x_1-a_1\|+\|x_2-a_2\|<\delta$ (using the $\|\!\cdot\!\|_1$ product norm), we have $\big\|\frac{\partial f}{\partial x_1}(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)\big\|\leq\varepsilon/2$ (operator norm) and $\|r(x_1,x_2)\|\leq\varepsilon/2$, so $$\Big\|f(x_1,x_2)-f(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)(x_1-a_1)\Big\|\leq\varepsilon\|x_1-a_1\|.$$ Is my proof correct? If so, why is the author using the mean value theorem (which does not really simplify the proof in my eyes)?","['multivariable-calculus', 'proof-verification', 'calculus', 'analysis']"
2837582,New/useful method for summation of divergent series?,"Questions $$  S(n,x) = x+e^x + e^{e^x} + e^{e^{e^x}} + \dots \text{$n$ times}$$ Also obeys (see background for argument): $$ \frac{1}{2 \pi i} \oint e^{S(k,x)}  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n)} dt})}{\partial k} dk  =  \frac{\partial S(n,x)}{\partial x}$$ Can this be used in the Borel summation sense for divergent series? If so, when can it be used for analytical continuity (convergence issues)? Is it useful(intuitively I feel it should be more powerful than Borel summation)? In the following heuristic sense: $$ \kappa  = \sum_{n=1}^\infty a_n = \sum_n a_n \frac{\frac{\partial S(n,x)}{\partial x}}{1 + e^x + e^x e^{e^x} + e^x e^{e^x}  e^{e^{e^x}} + \dots \text{$n$ times} } $$ Using the L.H.S of the first equation: $$ \kappa  = \frac{1}{2 \pi i} \sum_n a_n \frac{\oint e^{S(k,x)}  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n)} dt})}{\partial k} dk }{1 + e^x + e^x e^{e^x} + e^x e^{e^x}  e^{e^{e^x}} + \dots \text{$n$ times} } $$ Swapping order of summation and contour integral: $$ \kappa  =^! \frac{1}{2 \pi i} \oint e^{S(k,x)} \sum_n \frac{ a_n  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n)} dt})}{\partial k}  }{1 + e^x + e^x e^{e^x} + e^x e^{e^x}  e^{e^{e^x}} + \dots \text{$n$ times} }dk $$ How can I make this rigorous? Background I've been recently studying the following series: $$ S(n,x) = x+e^x + e^{e^x} + e^{e^{e^x}} + \dots \text{$n$ times}$$ Where the $n$'th term is raising the $x$ exponentially $n$ number of times. $$ b_n(x) = \underbrace{e^{e^{e^{\dots}x}}}_{\text{$n$ times exponentially raised}} $$ $n$ number of times. Hence, we notice: $$ e^{S(r,x)} = \frac{\partial b_{r+1}(x)}{\partial x}$$ Summing both sides and defining $S(0,x) \equiv 0$: $$ \sum_{r=0}^n e^{S(r,x)}  = \sum_{r=1}^{n+1} \frac{\partial b_{r}(x)}{\partial x}  $$ Hence, we get: $$ \sum_{r=0}^n e^{S(r,x)}  =  \frac{\partial S(n+1,x)}{\partial x}$$ Rewriting the R.H.S using complex analysis as a contour integral over the whole complex plane: $$\frac{1}{2 \pi i} \oint \sum_{r=0}^n \frac{e^{S(k,x)}}{k-r}dk  =  \frac{\partial S(n+1,x)}{\partial x}$$ Taking $e^{S(k,x)}$ common: $$ \frac{1}{2 \pi i} \oint e^{S(k,x)} \sum_{r=0}^n \frac{1}{k-r}dk  =  \frac{\partial S(n+1,x)}{\partial x}$$ Further using $d \ln x = dx/x$ $$  \frac{1}{2 \pi i} \oint e^{S(k,x)} \sum_{r=0}^n d \ln({k-r})  =  \frac{\partial S(n+1,x)}{\partial x}$$ Rewriting as factorial: $$ \frac{1}{2 \pi i} \oint e^{S(k,x)} \frac{\partial \ln(\frac{(k)!}{(k-n-1)!})}{\partial k} dk  =  \frac{\partial S(n+1,x)}{\partial x}$$ Analytically continuing $k!$ using the gamma function: $$ \frac{1}{2 \pi i} \oint e^{S(k,x)}  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n-1)} dt})}{\partial k} dk  =  \frac{\partial S(n+1,x)}{\partial x}$$","['regularization', 'complex-analysis', 'tetration', 'divergent-series', 'sequences-and-series']"
2837639,Proving a function isn't injective by considering inverse,"I'm proving that a $C^r$ function $f:\mathbb{R^2}\rightarrow \mathbb{R}$ is not injective.  I've found a function $g:\mathbb{R^2}\rightarrow \mathbb{R^2}$ defined as $g(x,y)=\bigg(f(x,y),y\bigg)$, and proven that it's $C^r$.  To prove my main problem, I'm assuming $f$ is injective, and that allows me to apply the inverse function theorem to $g$, in such a way that gives me an open map from an open set in the domain of $g$ to the open image of $g$, such that $g$ has a differentiable inverse $g^{-1}:g(A)\rightarrow A$. Now I want to prove a contradiction in the injectivity of $f$, by showing that $g^{-1}$ is not defined at certain points, specifically a line.  Consider $f(x,y)=b$ then $g(x,y)=(b,y)$, Now I should be able to find that the line that $g^{-1}$ isn't defined on $\forall (b,z)$, $z\neq y$, since that would imply that   $f(x,z)=b$. I don't quite see how $g^{-1}$ being defined on $(b,z)$ implies that $f(x,z)=b$? Edit Here is the original problem.  It is problem $2$-$37$.  I've seen other solutions suggest to make $g$ satisfy the conditions of $f$ from $2$-$36$, which is why I left it in. Thanks!","['real-analysis', 'calculus', 'multivariable-calculus', 'proof-writing', 'analysis']"
2837657,Sphere eversions,"Regular homotopy classes of immersions $S^n\to \mathbb{R}^{n+1}$ are classified by the group $\pi_n(SO(n+1))$. In particular, it follows from the fact that $\pi_2(SO(3))=0$ that an immersed 2-sphere in $\mathbb{R}^3$ can be turned ""inside out,"" (called a sphere eversion ). For $n=4$, we have $\pi_4(SO(5))=\mathbb{Z}/2\mathbb{Z}$, so there are two regular homotopy classes of immersions $S^4\to \mathbb{R}^5$. What do these two classes correspond to? My guess is that they correspond to the two different orientations of $S^4$. So up to orientation, there is a unique regular homotopy class of immersions $S^4\to \mathbb{R}^4$. Is this correct?","['differential-geometry', 'differential-topology']"
2837660,"$f: \mathbb{D} \to \mathbb{D}$ holomorphic, $f(\frac{1}{2}) + f(-\frac{1}{2}) = 0$. Prove $|f(0)| \leq \frac{1}{4}$ [duplicate]","This question already has an answer here : If $f \in \operatorname{Hol}(D)$, $f(\frac{1}{2}) + f(-\frac{1}{2}) = 0$, prove that $|f(0)| \leq \frac{1}{4}$ (1 answer) Closed 5 years ago . Here's the problem I'm having trouble with: Let $f: \mathbb{D} \to \mathbb{D}$ be a holomorphic function with $f(\frac{1}{2}) + f(-\frac{1}{2}) = 0$. Prove $|f(0)|\leq \frac{1}{4}$. I suspect that I somehow need to use some variant of Schwarz's lemma. Here's my attempt: Define $h(z) = \frac{f(z) + f(-z)}{2}$. Then, $h(0) = f(0)$ and $h(-\frac{1}{2}) = h(\frac{1}{2}) = 0$. Also, $h:\mathbb{D} \to \mathbb{D}$ is holomorphic, and $h(0) = f(0)$. By Schwarz-Pick's lemma, we have $$|\frac{h(\frac{1}{2}) - h(0)}{1-\overline{h(0)}h(\frac{1}{2})}| \leq |\frac{\frac{1}{2}-0}{1-\overline{0}\cdot \frac{1}{2}}|.$$ This shows $|h(0)| = |f(0)| \leq \frac{1}{2}$, however, it doesn't show what I need. Is there a way to improve the inequality and get the desired result?","['complex-analysis', 'holomorphic-functions']"
2837712,Questions about proof of Raabe's test in Wikipedia,"After asking a question in another thread regarding a specific series, I was directed towards Raabe's test. However, I have some doubts regarding the proof at https://en.wikipedia.org/wiki/Ratio_test . Questions : 1) How does the author deduce $ca_Ne^{-R\log(n)}$ from $a_Ne^{-R(\frac{1}{N}+...+\frac{1}{n})}$? I mean specifically $\log(n)$. I tried to use Maclaurin expansion, but I could not see. 2) In the same expression $a_Ne^{-R(\frac{1}{N}+...+\frac{1}{n})}\geqslant ca_Ne^{-R\log(n)}$. What is $c$? Why use $c$?","['real-analysis', 'sequences-and-series', 'calculus']"
2837802,blocks of a normalized basis dominated by lp,"I would like to know whether the following conjecture is true, possibly with additional assumptions such as unconditionality. Conjecture 1. Suppose $(x_i)_{i=1}^\infty$ is a normalized basis for a Banach space $X$ which is dominated by the unit vectors in $\ell_p$.  Then $X$ contains a seminormalized basic sequence dominated by the canonical unit vectors $((e_k^{(n)})_{k=1}^n)_{n=1}^\infty$ in $(\oplus\ell_2^n)_p$. Discussion. The conjecture is trivial for $p\geq 2$, so if necessary we may assume $1\leq p<2$. By Dvoretsky's Theorem we can find $(u_k^{(n)})_{k=1}^n\subset X$ which is 2-equivalent to $(e_k^{(n)})_{k=1}^n$ for each $n\in\mathbb{N}$, and such that $\text{supp }u_k^{(m)}<\text{supp }u_j^{(n)}$ whenever  $m<n$. Can we prove that $((u_k^{(n)})_{k=1}^n)_{n=1}^\infty$ is dominated by $((e_k^{(n)})_{k=1}^n)_{n=1}^\infty$?","['functional-analysis', 'lp-spaces', 'banach-spaces']"
2837813,Sample variance recursive relation. $n S_{n+1}^2 = (n-1)S_n^2 + \big( \frac{n}{n+1}\big) (X_n - \overline{X}_{n-1})^2$,"Let $\overline{X}_k$ and $S_k^2$ denote the sample mean and sample variance based on the first $k$ observations. Then establish the following 
$$n S_{n+1}^2 = (n-1)S_n^2 + \big( \frac{n}{n+1}\big) (X_{n+1} - \overline{X}_n)^2.$$ I am trying to use the relation 
$$\overline{X}_{n+1} = \frac{X_{n+1} + n\overline{X}_n}{n+1}.$$ I tried to decompose the sum into the first $n$ and the last $n+1$th term, however I get
$$n S_{n+1}^2 = \sum_{i=1}^{n+1} (X_i - \overline{X}_{n+1})^2 = \sum_{i=1}^n (X_i - \frac{X_{n+1} + n \overline{X}_n}{n+1})^2 + (\frac{n}{n+1} X_{n+1} - \overline{X}_n)^2,$$
which has the square of $\frac{n}{n+1}$. How can I get this relation? I would greatly appreciate some help.","['algebra-precalculus', 'statistics']"
2837830,Isolating variables in three-variable equation system,"I am presented with the following equation system $x^2 = 8+(y-z)^2$ $y^2 = 12+(z-x)^2$ $z^2 = 24+(x-y)^2$ I don't even know where to start. I am assuming you need to isolate the variables to begin with but even then - I have been unable to reach any sort of progress.
Please help. Thank you.","['algebra-precalculus', 'factoring', 'systems-of-equations']"
2837847,"If $f:U\to V$ is holomorphic and $f'(z)\neq 0$ for all $z\in U$, then$f$ is locally bijective.","I am trying to solve the following problem: Let $f:U\to V$ be a holomorphic function such that $f'(z)\neq 0$ for all $z\in U$. Show that for all $z_0\in U$, there exists a disc $D_\varepsilon(z_0)\subseteq U$ such that $f:D_\varepsilon(z_0)\to f(D_\varepsilon(z_0))$ is bijective. Atempt: I am trying to use Rouche's Theorem, but I am stuck on a particular step. Let $z_0\in U$. Since $f'(z_0)\neq 0$, there is a disc $D_r(z_0)\subseteq U$ such that
$$f(z)=f(z_0)+(z-z_0)h(z),\quad\forall z\in D_r(z_0)$$
where $h$ is holomorphic on $D_r(z_0)$ and $h(z)\neq 0$ for all $z\in D_r(z_0)$. Let $0<\varepsilon<r$ to be defined later and fix $w\in D_\varepsilon(z_0)$. To show that $f$ is injective in $D_\varepsilon(z_0)$ is to show that the function $f(z)-f(w)$ has exactly one zero in $D_\varepsilon(z_0)$. But
$$F(z):=(z-z_0)h(z)$$
has exactly one zero in $D_\varepsilon(z_0)$, so we might want to apply Rouche's Theorem with $F$ and
$$G(z):=f(z)-f(w)-(z-z_0)h(z)=f(z_0)-f(w)=-(w-z_0)h(w)$$
to conclude that $F$ and $F+G$ have the same number of zeros and hence $f$ is injective. But that means that we need to find $0<\varepsilon<r$ such that $|G(z)|<|F(z)|$ on $\partial D_\varepsilon(z_0)$. That is,
$$|w-z_0||h(w)|<|z-z_0||h(z)|$$
for all $w\in D_\varepsilon(z_0)$ and $z\in\partial D_\varepsilon(z_0)$. Can we find such $\varepsilon>0$? It seems intuitive since if we expand $h$ in a power series at $z_0$, $h(z)=\sum_{n=0}^\infty a_n(z-z_0)^n$, then
$$|w-z_0||h(w)| = |w-z_0|\sum_{n=0}^\infty a_n|w-z_0|^n < |z-z_0|\sum_{n=0}^\infty a_n|z-z_0|^n.$$
for $w\in D_\varepsilon(z_0)$ and $z\in\partial D_\varepsilon(z_0)$. But this is of course not a proof.",['complex-analysis']
2837864,When does the equality hold in the Holder inequality?,"I am considering the series case. In the Holder inequality, we have 
$$\sum|x_iy_i|\leq\left(\sum|x_i|^p\right)^{\frac1p} \left(\sum|y_i|^q\right)^{\frac1q},$$
where $\frac1p+\frac1q=1,~p, q>1$. In Cauchy inequality (i.e., $p=q=2$), I know that the equality holds if and only if $x$ and $y$ are linearly dependent. I am wondering when the equality holds in the Holder inequality.","['functional-analysis', 'real-analysis', 'inequality', 'holder-inequality']"
2837869,Is the Graph of a Measurable Function Measurable?,"Let $(X, \mathcal F)$ and $(Y, \mathcal G)$ be measurable spaces and $f:X\to Y$ be a measurable function. Let $\Gamma_f:X\to X\times Y$ be the map defined as $\Gamma_f(x)=(x, f(x))$. Question. Is the image $\Gamma_f(X)$ a measurable subset of $X\times Y$, where the latter is equipped with the product $\sigma$-algebra? The above question has answer in the afirmative if $X=Y=\mathbf R$ and the sigma algebras $\mathcal F$ and $\mathcal G$ are the Borel $\sigma$-algebras.
This is because the map $X\times Y\to \mathbf R$ defined as $(x, y)\mapsto y-f(x)$ is a mesurable function whose zero-set is exactly $\Gamma_f(X)$. But what about arbitrary measurable spaces?",['measure-theory']
2837885,Prove $f:\mathbb{R^2}\rightarrow \mathbb{R}$ not injective by Inverse Function Theorem,"I'm proving that a $C^r$ function $f:\mathbb{R^2}\rightarrow \mathbb{R}$ is not injective. I've seen various solutions but I want to prove it using techniques related to the Inverse function theorem; because it's a problem from Spivaks ""Callculus on Manifolds"" text in the chapter on the Inverse function theorem. I've found a function $g:\mathbb{R^2}\rightarrow \mathbb{R^2}$ defined as $g(x,y)=\bigg(f(x,y),y\bigg)$, and proven that it's $C^r$.  To prove my main problem, I'm assuming $f$ is injective, and that allows me to apply the inverse function theorem to $g$, in such a way that gives me an open map from an open set in the domain of $g$ to the open image of $g$, such that $g$ has a differentiable inverse $g^{-1}:g(A)\rightarrow A$. Now I want to prove a contradiction in the injectivity of $f$, by showing that $g^{-1}$ is not defined at certain points, specifically a line, and this contradicts the differentiability of $g^{-1}$.  Consider $f(x,y)=b$ then $g(x,y)=(b,y)$, Now I should be able to find that the line that $g^{-1}$ isn't defined on $\forall (b,z)$, $z\neq y$, since that would imply that   $f(x,z)=b$. My Question I don't quite see how $g^{-1}$ being defined on $(b,z)$ implies that $f(x,z)=b$? $\phantom{}$ Here is the original problem.  I've seen other solutions suggest to make $g$ satisfy the conditions of $f$ from $2$-$36$, which is why I left it in. Thanks!","['real-analysis', 'calculus', 'multivariable-calculus', 'proof-writing', 'analysis']"
2837889,Differentiability of a determinant and its inverse,"Show that $\det:\mathbb{R}^{n\times m}\rightarrow\mathbb{R}, M \mapsto \det(M)$ is differentiable (without having to calculate its derivative. Show that its inverse is differentiable as well, i.e. for $\text{GL}(n)=\{M\in\mathbb{R}^{n\times m}:\det(M)\neq 0\}$ we have $\text{GL}(n)\rightarrow \text{GL}(n), M\mapsto M^{-1}$ is differentiable. Am I correct here that the correct procedure is to express the function as a polynomial making use of the Leibniz formula, and then argue that this function is infinitely differentiable as a composition of differentiable functions? Or am I missing a step here, or is possibly expressing it as a polynomial not allowed (or rather I would have to prove its polynomial representation first)? This one I'm lost on and wouldn't know how to do. It probably involves Cramer's rule in some fashion and the Jacobian matrix and determinant, but I would be incapable of writing this up I think. Does anybody know where I could look at a proof of this?","['real-analysis', 'calculus', 'proof-verification', 'multivariable-calculus', 'jacobian']"
2837891,Integrate using the method of undetermined coefficients,"I was trying to solve this integral using the method of Undetermined Coefficients . $$\int x^3\cos(3x)\,dx$$ My calculus book says: We try: $y=P(x)\cos(3x)+Q(x)\sin(3x)+C$, where $P(x)$ and $Q(x)$ are polynomials of degrees $m$ and $n$ respectively. $y'=P'(x)\cos(3x)-3P(x)\sin(3x)+Q'(x)\sin(3x)+3Q'(x)\cos(3x)=x^3\cos(3x)$ Equating coefficients of like trigonometric functions, we find: $P'(x)+3Q(x)=x^3$ and $Q'(x)-3P(x)=0$ The second of these equations requires that $m=n-1$. From the first we
  conclude that $n=3$, which implies that $m=2$. With this information I can calculate the correct integral, being: $$(-\frac{2}{27}+\frac{x^2}{3})\cos(3x)+(-\frac{2x}{9}+\frac{x^3}{3})\sin(3x)+C$$ QUESTION: My question is about the last sentence. Given these conditions couldn't it also be the case that $m=4$ and $n=5$? As a matter of fact I calculated that this would indeed give the correct answer as well, but I don't fully understand why there are two options and what is going on here. Any insight would be appreciated.","['integration', 'calculus']"
2837905,L2-orthogonality,"Consider the following functions: 
$$
\psi_1(x) = \sinh(x)-\sin(x) \quad \quad\psi_2(x)=\cosh(x)-\cos(x)$$ $$\phi_a(x)=\psi_1(x)\psi_2(a)-\psi_1(a)\psi_2(x)$$ and define $\pi_k(x)=\phi_{a_k}(a_kx)$ where $a_k$ is a sequence such that $\phi'_{a_k}(a_k)=0=\phi'_{-a_k}(-a_k)$ I need to show that those functions are $L_2$-orthogonal $\langle \pi_i,\pi_j \rangle =\delta_{i,j}=\int_0^1\pi_i(x)\pi_j(x)dx$ I am sitting at this problem for quite some time and tried to ""brute-force"" my way throug the calculation (applying partial integration). Is there any quick way to see that those functions are orthogonal ?","['orthogonality', 'normed-spaces', 'lp-spaces', 'ordinary-differential-equations']"
2837906,Is there one canonical principal polarization of a Jacobian per nonisomorphic curve?,"Why is there only one canonical principal polarization per Jacobian? I don't yet see why it is true, but I have seen ""the canonical polarization"" stated many times. This is perhaps a naive question, but I am nevertheless confused. Let us say we have two different curves $C$, $C'$ (over a perfect field $k$), such that $A:= Jac(C) \simeq Jac(C')$ as unpolarized abelian varieties. The canonical principal polarization (thought of as a symplectic form on $Jac(C)$) from $C$ associated to $Jac(C)$ comes from the intersection form $Q: H_1(C; \mathbb{Z}) \otimes H_1(C; \mathbb{Z}) \to\mathbb{Z}$. This is because $H^2(Jac(C); \mathbb{Z}) \simeq Hom(\bigwedge H_1(C; \mathbb{Z}); \mathbb{Z})$. Let us say that $C$ and $C'$ are not isomorphic, but they have isomorphic unpolarized Jacobians $A:= Jac(C) \simeq Jac(C')$ (such curves exist, e.g. this paper ). Then, $C'$ must produce a different canonical principal polarization of $A$ than $C$, otherwise $C$ and $C'$ would be isomorphic by Torelli's theorem. I have heard that the canonical principal polarization may be equivalently defined by the polarization associated to the Poincare correspondence on $Jac(C) \times Jac(C) \simeq Jac(C') \times Jac(C')$, which indicates there should only be one canonical principal polarization on $Jac(C) \simeq Jac(C')$. I do not understand (and cannot locate written) why the associated polarization of this correspondence is principal. Here are my questions: Let $A$ be an abelian variety in the Jacobian locus. Is there only one canonical principal polarization of $A$? If so, why is there only one canonical principal polarization of $A$, especially if $A$ is the Jacobian of multiple non-isomorphic curves?","['abelian-varieties', 'divisors-algebraic-geometry', 'algebraic-geometry']"
2837909,Why isn't the linear regression coefficient not just the average vector to data points?,"I am having trouble intuitively understanding the correctness of the formula to compute the coefficient for the regression line in a linear regression. I know the formula is $$\frac{\sum_{i=1}^N (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^N(x_i - \bar{x})^2}$$ I have at some point gone through the proof and mechanically understood it. But intuitively I still don't see why above formula computes the correct coefficient. In fact, intuitively I would have said, the coefficient for the regression line needs to be the average ratio of $y_i$ and $x_i$, $(x_i, y_i)$ being the data points. I wrote a small Jupyter-Notebook to illustrate this . I found that my naive approach is not completely wrong and in fact converges towards the correct value with more data, if the data scatters in a fixed intervall. So... what is the critical points that my naive approach gets wrong and what is the intuitive explanation for why the correct formula works better?","['regression', 'statistics', 'linear-regression']"
2837917,Directional derivative of $\frac{xy^2}{x^2+y^6}$,"Let $f:\mathbb{R}^2\rightarrow\mathbb{R}, (x,y)\mapsto\begin{cases}\frac{xy^2}{x^2+y^6},\,&(x,y)\neq(0,0)\\0,\,&\text{else}\end{cases}$. Show that for every direction $v\in\mathbb{R}^2\setminus\{0\}$ there exists a directional derivative $D_vf(0,0)$. Is $f$ continuous in $(0,0)$? Is $f$ differentiable in $(0,0)$? I'm stuck on this exercise. I tried applying the definition, i.e. showing that $\lim_{t\rightarrow 0}\frac{f(\xi+tv)-f(\xi)}{t}$ exists, but it's getting me nowhere. Can anybody tell me the right approach here or what I'm not seeing?","['multivariable-calculus', 'partial-derivative', 'calculus']"
2837918,Binomial Coefficient Summation Equivalence,"Following on a previous question, I'm supposed to prove the following:
  $$ \sum_{i=0}^{n-1} \binom{2n-2-i}{n-1} =  \sum_{i=0}^{n-1} \binom{n-1+i}{i}.$$
  Is there any simple conversion to come from the first term to the second one?","['statistics', 'summation', 'binomial-coefficients', 'binomial-distribution']"
2837919,How do I evaluate these derivatives? (Involving integrals),"I'm studying Green's functions, and I came acorss the following problem: To calculate $\frac{du}{dx}$, where:
$$u(x) = \int_{a}^{x} G_1(x,y)f(y)dy + \int_{x}^{b} G_2(x,y)f(y)dy$$
Where $a,b \in \mathbb{R}$, $G_1,G_2,f$ are functions. Let's first take a look at the first term:
$$u_1(x) = \int_{a}^{x} G_1(x,y)f(y)dy$$
Now, my textbook (Eugene Butkov's Mathematical Physics) says that:
$$u_1'(x) = \int_{a}^{x} \frac{\partial G_1}{\partial x} f(y)dy \quad \!\!\!\!\!+ G_1(x,x)f(x)$$ How come? I've tried to derive this formula but I couldn't go anywhere. I was able to verify it, however, in the case that $G_1(x,y)$ is a separable function $G_1(x,y)=a(x)b(y)$. Then
$$u(x) = a(x)\int_{a}^{x} b(y)f(y)dy$$
Hence, by the product rule
$$\frac{du(x)}{dx} = a'(x) \int_{a}^{x} b(y)f(y)dy \quad \!\!\!\!+ a(x)b(x)f(x)$$
Which is the verified result. However, I would like to see a proof of the formula for a more general case.","['derivatives', 'partial-derivative', 'calculus', 'multivariable-calculus', 'greens-function']"
2837923,What's an $\mathcal O_X$-algebra when $X= \operatorname{Spec} R$?,"Take $X= \operatorname{Spec}R$, $R$ a commutative ring with unit. What is an $\mathcal O_X$-algebra in that case? Is there more than just ordinary $R$-algebras? Thank you in advance.","['schemes', 'affine-schemes', 'algebraic-geometry']"
2837955,Random walk on the lamplighter group,"Consider a random walk on the lamplighter group with the following generating set: move left, move right, and toggle lamp. Start at the origin, with all lamps off. What is the probability that, after $t$ steps, the lamp at the origin is on? I started by letting $g(b,k,t)$ denote the number of words of length $t$  that set the lamp at the origin to $b$ and the lamplighter at position $k$. Thus we have the recurrence relation
\begin{align*}
  g(b,k,0)&=[b=0][k=0] \\
  g(b,k,t+1)&=g(b,k-1,t)+g(b,k+1,t)+g(b \oplus [k=0],k,t)
\end{align*} where $[\cdot]$ is the Iverson bracket and $\oplus$ is xor . Then I let $f(b,t)$ denote the number of words of length $t$ that set the lamp at the origin to $b$:
$$f(b,t) = \sum_{k \in \mathbb{Z}} g(b,k,t)$$ The answer to my question is $f(1,t) \cdot 3^{-t}$, since $3^t$ is the total number of words of length $t$. After some simplification, I obtained the following recurrence relation for $f$:
\begin{align*}
  f(b,0) &= [b=0] \\
  f(b,t+1) &= 3 f(b,t) - g(b, 0, t) + g(b \oplus 1, 0, t)
\end{align*} I'm trying to get rid of the remaining $g$ terms. $g(0,0,t)$ and $g(1,0,t)$ represent the number of $t$-length words that return to the origin while leaving its lamp off or on, respectively. I suspect I might be able to use Motzkin paths to solve for these. The number of $t$-length words that return to the origin is the $t$th central trinomial coefficient . That is, $$g(0,0,t)+g(1,0,t) = \sum_{i=0}^t \binom{t}{i} \binom{i}{t-i}$$ The first few coefficients are \begin{array}{c|c|c}
& b = 0 & b = 1 \\
t=0 & 1 & 0 \\
t=1 & 0 & 1 \\
t=2 & 3 & 0 \\
t=3 & 2 & 5 \\
t=4 & 15 & 4 \\
t=5 & 22 & 29 \\
t=6 & 93 & 48 \\
t=7 & 196 & 197 \\
t=8 & 659 & 448 \\
t=9 & 1650 & 1489
\end{array} EDIT: Let $L_k$ denote the set of words that shift the lamplighter by $k$. Then
$$[z^n]L_k(z) = \binom{n}{k}_2$$ where the RHS is the entry in row $n$ and column $k$ of the trinomial triangle . Let $L_k^-$ be the subset of $L_k$ such that the lamp at $k$ (or, equivalently in number, the lamp at the last position) is not toggled. Based on further investigation, it seems to be the case that
$$L_k^-(z) = L_k(z) \frac{1+z L_0(z)}{1+2z L_0(z)}$$ Is there a simple explanation for this relationship? I sense some kind of recursive definition of $L_k^-$ in terms of $L_k$, $L_0$, and $L_k^-$ itself.","['random-walk', 'probability', 'combinatorics', 'cayley-graphs', 'group-theory']"
2837985,Modulus and Exponents,"The question: Determine $N$ where $0$ $\leq$ $n$ $\leq$ $16$ such that $710^{447}$$\equiv$ $n$ $($ mod $17$ $)$ My attempt $710^{1}$ $\equiv$ $710$ (mod $17$) $\equiv$ $13$ $710^{2}$ $\equiv$ $13^{2}$ $\equiv$ $169$ (mod $17$) $\equiv$ $16$ $710^{3}$ $\equiv$ $16*13$ $\equiv$ $208$ (mod $17$) $\equiv$ $4$ $710^{4}$ $\equiv$ $16^{2}$ $\equiv$ $256$ (mod $17$) $\equiv$ $1$ $710^{5}$ $\equiv$ $16^{2}*13$ $\equiv$ $3328$ (mod $17$) $\equiv$ $13$ $710^{6}$ $\equiv$ $4^{2}$ $\equiv$ $16$ (mod $17$) $\equiv$ $16$ $710^{7}$ $\equiv$ $16*4*13$ $\equiv$ $832$ (mod $17$) $\equiv$ $16$ It follows, $$710^{3(149)}\equiv4^{148+1}\equiv4^{148}*4^{1}\equiv4^{2(74)}
*4^{1}$$
$$4^{2(74)}*4^{1}\equiv4^{8}*4^{5(7)}*4^{5(7)}*4^{5(7)}*4^{5(7)}*4^{1}$$ $$65536 (mod 17) \equiv 1$$ So, $$4^{8}*4^{5(7)}*4^{5(7)}*4^{5(7)}*4^{5(7)}\equiv4^{140}$$ And, $$4^{140}\equiv4^{4*5*7}$$ Since $$4^{4}=256$$ We obtain $$256 (mod 17) \equiv 1$$ and, $$1^{5*7}=1$$ Thus, $N$=$1$","['modular-arithmetic', 'discrete-mathematics']"
2838003,"Argument range for branch cut of $[-1,1]$?","I read somewhere on this site that for a branch cut between $(-\infty, 0]$, the range of values for $\arg(z)$ is $[-\pi,\pi]$, while for a branch cut between $[0,\infty)$, the range of values is $[0,2\pi)$. Why is this so? Is this just a definition, or is there some mathematical reasoning to it? Does this apply to branch cuts along the imaginary axis? Also, for a branch cut that crosses the origin, such as $[-1,1]$ for $\sqrt{z^2-1}$, what should the approach be? Edit: the question on stackexchange I was referring to is this one .","['complex-analysis', 'complex-numbers', 'branch-points', 'branch-cuts']"
2838005,"Show function $f(x,y)=(x^2-y^2,2xy)$ is $1$-$1$ by Inverse Function Theorem","I'm trying to prove the problem below - which comes from Munkres' ""Analysis on Manifolds"" book in the section on the Inverse function theorem. Since its in the chapter on the Inverse Function Theorem I figured I'd start by showing that $f$ satisfies the conditions of the theorem.  Writing the Jacobian shows that it's both $C^r$ and we get $\det f'(x,y)=4x^2+4y^2\neq0$ when $x>0$.  So we can apply the theorem but I'm unsure of how to proceed to show that $f$ is $1$-$1$.  And I didn't see how to use the hint they provided. Thanks! Let $f\colon \mathbf R^2\to \mathbf R^2$ be defined by the equation
  $$f(x,y)=(x^2-y^2,2xy).$$
  (a) Show that $f$ is one-to-one on the set of all $(x,y)$ with $x>0$. [Hint: If $f(x,y)=f(a,b)$, then $\|f(x,y)\|=\|f(a,b)\|$.]","['real-analysis', 'calculus', 'functions', 'multivariable-calculus', 'inverse-function-theorem']"
2838011,Prove $\int_{\Sigma_r} |\nabla\varphi|^2 d\sigma_r \ge \frac{2}{u(r)^2} \int_{\Sigma_r} (\varphi - \bar{\varphi})^2 d\sigma_r $,"Reference: this paper Given the deSitter-Schwarzschild metric with mass $m > 0$ and scalar
  curvature equal to $2$ is the metric $$\bigg(  1 -\frac{r^2}{3}-\frac{2m}{r} \bigg)^{-1} dr^2 + r^2
 dg_{\mathbb S^2} \tag{1}$$ defined on $(a_0 ,b_0) \times \mathbb S^2$, where $(a_0,b_0)=\{r>0:1
 -\frac{r^2}{3}-\frac{2m}{r}>0 \}$ and $g_{\mathbb S^2}$ is the standard metric on $\mathbb S^2$ with constant Gauss curvature equal
   to $1$. In order to deal with the metric in $(1)$, we use the warped product metric $g=dr^2 + u(r)^2 dg_{\mathbb S^2}$ on $\mathbb R \times \mathbb S^2$, where $u(r)$ is a positive real function. If we assume that $g$ has constant scalar curvature equal to $2$, then $u$ solves the following second-order differential equation $$u''(r)=\frac{1}{2}\bigg( \frac{1-u'(r)^2}{u(r)} - \frac{u(r)}{2}  \bigg) \tag{2}$$ Considering only positive solutions $u(r)$ to $(2)$ which are defined for all $r \in \mathbb R$, we get a one-parameter family of periodic rotationally symmetric metrics $g_a = dr^2 + u_a(r)^2 g_{\mathbb S^2}$ with constant scalar curvature equal to $2$, where $a \in (0,1)$ and $u_a(r)$ satisfies $u_a(0)=a= \text{min} ~u$ and $u'_a(0) =0$. These metrics are precisely the deSitter-Schwarzschild metrics on $\mathbb R \times \mathbb S^2$ defined in $(1)$. Next, let $(M,g)$ be a three-manifold and consider a two-sided compact surface $\Sigma \subset M$. The mass of $\Sigma \subset M$ is defined by $$m(\Sigma) =\ \bigg( \frac{|\Sigma|}{16\pi}  \bigg)^{1/2}  	\bigg( 1
 - \frac{1}{ 16\pi } \int_{\Sigma} H^2 d\sigma 
 	- \frac{ \Lambda}{24\pi } |\Sigma|  \bigg) \tag{3}$$ where $\Lambda = \text{inf}_M ~ R$, $R$ is the scalar curvature of
  $M$, $H$ is the mean curvature of $\Sigma$, $K_\Sigma$ is the Gauss
  curvature of $\Sigma$, and $|\Sigma| = \int_\Sigma d\sigma$. The first variation of $m$: $$\frac{d}{dt}m(\Sigma(t))\bigg|_{t=0} = -
 \frac{2|\Sigma|^{1/2}}{(16\pi)^{3/2}} \int_{\Sigma} \varphi
 \Delta_\Sigma H d\sigma \\
 + \frac{|\Sigma|^{1/2}}{(16\pi)^{3/2}} \int_{\Sigma} \bigg[ 2K_\Sigma - \frac{8\pi}{|\Sigma|  }+ \bigg( \frac{1}{2|\Sigma|}\int_\Sigma H^2 d\sigma - |A|^2 \bigg)   \bigg] H \phi d\sigma \\
 + \frac{|\Sigma|^{1/2}}{(16\pi)^{3/2}} \int_\Sigma (\Lambda - R) H\varphi d\sigma \tag{4}$$ Remark $1$. It follows from $(4)$  that if a two-sphere $\Sigma \subset M$ is umbilic where $|A|^2 = \frac{H^2}{2}$ and has constant Gauss curvature and $M$ has constant scalar curvature equal to $2$ along $\Sigma$, then $\Sigma$ is a critical point of the mass in $(3)$. Denote by $\Sigma_r$ the slice $\{r\} \times \mathbb S^2$. By Remark $1$, $\Sigma_r$ is a critical point for the mass in $(\mathbb R \times \mathbb S^2 , g_a)$, for all $r \in \mathbb R$ and $a \in (0,1)$. Moreover the mass of $\Sigma_r \subset (\mathbb R \times \mathbb S^2 , g_a)$ is constant for all $r \in \mathbb R$. It follows by a straightforward computation: $$\frac{d}{dr} m(\Sigma_r) = \frac{1}{2} u'(r)(1-u'(r)-u(r)^2-2u(r)u''(r)) \tag{5}$$ which is zero once $u(r)$ solves $(2)$, we obtain therefore that $m(\Sigma_r)$ is constant equal to $m(\Sigma_0)$. Now, since $g_{\Sigma_r} = u(r)^2 g_{\mathbb S^2}$, by the Poincare inequality we have
  $$ \int_{\Sigma_r} |\nabla\varphi|^2  d\sigma_r \ge \frac{2}{u(r)^2} \int_{\Sigma_r} (\varphi - \bar{\varphi})^2 d\sigma_r   = \frac{8\pi}{|\Sigma_r|}\int_{\Sigma_r} (\varphi - \bar{\varphi})^2 d\sigma_r \tag{6}$$ where $\varphi \in C^\infty(\Sigma_r)$. Question: Where does $(6)$ come from? Thank you.","['riemannian-geometry', 'differential-geometry', 'surfaces']"
2838027,Singletons and Pair Sets,"Are pair sets a consequence of singleton sets and a pairwise union? I'm using Tao's Analysis and though he states at the beginning of the chapter (chapter three, for reference) that some axioms in the book are redundant, he somewhat implies later on that the singleton/pair sets-axiom isn't.",['elementary-set-theory']
2838028,"For $Y\sim N(0,\sigma^2)$, find $\mathbb{E}(Y^n)$ for odd and even $n$ using the expectation of $G\sim \text{Gamma}(\alpha,\beta)$","For $\alpha,\beta>0$, the probability density function of a Gamma$(\alpha,\beta)$ random variable is given by
  $$f(x)=\frac{x^{\alpha-1}e^{\frac{-x}{\beta}}}{\Gamma(\alpha)\beta^\alpha} \ \ \ \ \ \ \ x>0$$
  For $\sigma>0$, the probability density function of a $N(0,\sigma^2)$ random variable is given by
  $$f(x)=\frac{e^{-\frac{x^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}} \ \ \ \ \ \ \ x\in\mathbb{R}$$ Now for $G\sim\text{Gamma}(\alpha,\beta)$, I have shown that for $k\in\mathbb{N}^+$
$$\mathbb{E}(G^k)=\frac{\beta^k\Gamma(\alpha+k)}{\Gamma(\alpha)}  \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ (1)$$
by manipulating the integrand $g^kf(g)$ Let $Y\sim N(0,\sigma^2)$, using the result $(1)$, or otherwise, show that for $n\in\mathbb{N}^+$
  $$\mathbb{E}(Y^n)=\ \begin{cases} 
      \frac{(2\sigma^2)^{\frac{n}{2}}\Gamma(\frac{1}{2}+\frac{n}{2})}{\Gamma(\frac{1}{2})} & \text{n is even} \\
     0 &\text{n is odd}  \\ 
   \end{cases}
$$ I decide to first tackle the odd case first, as this seemed most obvious. For odd $n$ $$
\mathbb{E}(Y^n)=\int_{-\infty}^{\infty} y^n\frac{e^{-\frac{y^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}} dy=0
$$
as the integrand is an odd function when $n$ is odd. For the even case, I have seen a solution that actually solved the expectation by definition. I wanted to use the result $(1)$, as I thought this could speed things up. But i'm unsure if my solution is valid. For even $n$ We first start with 
\begin{align}
&Y\sim N(0,\sigma^2) \\
&\Rightarrow \frac{Y}{\sigma}\sim N(0,1) \\
&\Rightarrow \frac{Y^2}{\sigma^2}\sim \chi^2_1 \\
&\Rightarrow \frac{Y^2}{\sigma^2}\sim \text{Gamma}\Big(\frac{1}{2},2\Big)\\
&\Rightarrow Y^2\sim\text{Gamma}\Big(\frac{1}{2},2\sigma^2\Big) \\
\end{align}
Now let $$Z=Y^2\Rightarrow Y=Z^\frac{1}{2}$$
Hence
$$\mathbb{E}(Y^n)=\mathbb{E}(Z^{\frac{n}{2}})=\frac{(2\sigma^2)^{\frac{n}{2}}\Gamma(\frac{1}{2}+\frac{n}{2})}{\Gamma(\frac{1}{2})}$$ I am not entirely sure that my logic for the even case, where I derived a gamma distribution from the normal distribution, is correct. Is this a sufficient answer?","['normal-distribution', 'proof-verification', 'gamma-distribution', 'statistics', 'probability']"
2838029,$T$-invariance of $U$ is equivalent to $T^{*}$-invariance of $U^{\perp}$,"Is the Following argument correct? Suppose $T\in\mathcal{L}(V)$ and $U$ is a subspace of $V$. Prove that $U$ is invariant under $T$ if and only if $U^{\perp}$ is invariant under $T^*$. Proof. Given that $U$ is invariant under $T$ then $\langle w,Tu\rangle = 0,\forall (u,w)\in U\times U^{\perp}$ equivalently $\langle w,(T^*)^*u\rangle = 0 ,\forall (u,w)\in U\times U^{\perp}$ this when taken together with the definition of the adjoint of $T$ is equivalent to $\langle T^*w,u\rangle =  0,\forall (u,w)\in U\times U^{\perp}$ thus $U^{\perp}$ is invariant under $T^{*}$. $\blacksquare$","['orthogonality', 'linear-algebra', 'proof-verification', 'adjoint-operators']"
2838034,Probability that a delaunay triangle contains the center of its circumcircle,"A Delaunay triangulation for a given set P of discrete points in a plane is a triangulation DT(P) such that no point in P is inside the circumcircle of any triangle in DT(P). https://en.wikipedia.org/wiki/Delaunay_triangulation In the Delaunay triangulation of a Poisson point process in the plane, the probability that a triangle contains the center of its circumcircle is ? https://en.wikipedia.org/wiki/Poisson_point_process I know for a fact that it is not $\frac{1}{4}$ which is the probability that the center of the circle is contained within the triangle formed by three random points on the circle. I find this troubling, I don't know how the probability changes from $\frac{1}{4}$ for the delaunay triangle probability.","['triangulation', 'probability', 'geometric-probability', 'geometry']"
2838041,Cohomology group of the affine plane without origin: need help with algebraic computation,"Let $k$ be a field and $X=\mathbb{A}^2_k\setminus \{(0,0)\}$ be the affine plane without origin. I would like to compute the first cohomology group $H^1(X,\mathcal{O})$ of this scheme. For this, I proceeded exactly the same way as in the answer of this related question . Everything is fine until the purely algebraic statement to which the problem reduces. It is the following: Consider the morphism $d:k[X,Y,X^{-1}]\oplus k[X,Y,Y^{-1}]\rightarrow k[X,Y,X^{-1},Y^{-1}]$ defined by sending the pair $(\frac{P}{X^i},\frac{Q}{Y^j})$ to $\frac{P}{X^i}-\frac{Q}{Y^j}$, where $P,Q\in k[X,Y]$. Then, we have $$k[X,Y,X^{-1},Y^{-1}]/\operatorname{Im}(d)=\operatorname{Span}_k\left(\frac{1}{X^iY^j}\right)_{i>0,j>0}$$ Surprisingly (for me), I fail to show this statement which shouldn't be that much of a problem normally. I understand why we have a surjective map $$\operatorname{Span}_k\left(\frac{1}{X^iY^j}\right)_{i>0,j>0}\rightarrow k[X,Y,X^{-1},Y^{-1}]/\operatorname{Im}(d)$$ however I can't see why injectivity holds. I couldn't show it by direct computation, and I don't see any kind of short elegant argument for it. Any help or hint with this problem would be gladly appreciated.","['sheaf-cohomology', 'algebraic-geometry', 'commutative-algebra']"
2838043,solve $y''+y'+y=\sin(x)$ with $y(0)=0$ and $y'(0)=1$ with Laplace Transformation,"I have this problem: 
$$y''+y'+y=\sin(x),$$ with $y(0)=0$ and $y'(0)=1$. I solve it using the Laplace Transform: $$\mathcal L(y''+y'+y)= \mathcal L(\sin(x))$$
$$s^2\mathcal L(y)-sy(0)-y'(0)+s\mathcal L(y)-y(0)+\mathcal L(y)=\frac{ 1}{s^2 +1}$$
$$\mathcal L(y)(s^2+s+1)=\frac{ 1}{s^2 +1}+1$$ $$\mathcal L(y)=\frac{ s^2 +2}{s^2 +1}\frac{ 1}{(s^2+s+1)}$$ What is the best way to get $y(x)$?","['inverse-laplace', 'ordinary-differential-equations', 'laplace-transform']"
2838046,The bound of Fourier series coefficients,"https://www.encyclopediaofmath.org/index.php/Fourier_transform_of_a_generalized_function For a periodic generalized function $f$, I see from the link that 
$$f(x) = \sum_{|k|=0}^\infty c_k(f)e ^{i\langle kw,x\rangle}, ~\left| c_k(f)\right|\le A \frac{1}{(1+k)^m}.$$ However, the definition or description of $m$ and $A$ are not given... Can anyone help me a little bit on this? Explain a little bit to me. Actually, if one can provide a reference, it will be very nice (not necessarily generalized functions, but only functions with one variable). I am thinking a question. This result could be of some help.","['functional-analysis', 'fourier-series', 'fourier-analysis', 'fourier-transform']"
2838067,Is it possible to prove that $\pi(n)>\pi(2n)-\pi(n)$ for all $n > 10$?,"Is it possible to prove that the number of prime numbers between $0$ and $n$ is greater than the number of prime numbers between $n$ and $2n$, for all $n > 10$? 
$$\pi(n)>\pi(2n)-\pi(n)$$ By the prime number theorem, we know that: $$\lim_{n\rightarrow\infty}\frac{\ln(n)\pi(n)}{n}=1$$ so: $$\lim_{n\rightarrow\infty}\pi(n)=\frac{n}{\ln(n)}$$
$$\lim_{n\rightarrow\infty}\pi(2n)=\frac{2n}{\ln(2n)}$$ $$\lim_{n\rightarrow\infty}\frac{\pi(n)}{\pi(2n)}=\frac{n\ln(2n)}{2n\ln(n)}=\frac{\ln(2n)}{2\ln(n)}$$ We know that: $$\lim_{n\rightarrow\infty}\frac{ln(2n)}{2ln(n)}=\frac{1}{2}$$ therefore: $$\lim_{n\rightarrow\infty}\frac{\pi(n)}{\pi(2n)}=\frac{1}{2}$$
$$\lim_{n\rightarrow\infty}2\pi(n)=\pi(2n)$$ $$\lim_{n\rightarrow\infty}\pi(n)=\pi(2n)-\pi(n)$$ This suggest that my inequality is false for $n=\infty$ because:
$$\pi(\infty)=\pi(2\infty)-\pi(\infty)$$ Also, someone told me that since the relation between $\pi(n)$ and $\frac{n}{\ln(n)}$ is asymptotic, there are infinitely many n for which:
$$\pi(n)<\pi(2n)-\pi(n)$$ but except for $n=2,4,10$, where the difference between $\pi(n)$ and $\pi(2n)-\pi(n)$ is $0$, the result of $\left(\pi(n)\right)-\left(\pi(2n)-\pi(n)\right)$ seems to grow slowly as n grows. Since the density of primes decrease as n grows, i don't understand how there could be more prime numbers between $n$ and $2n$ than between $0$ and $n$...","['number-theory', 'prime-numbers']"
2838068,"Prove that if $H$ is a subgroup of $G$, then the identity $e$ of $G$ is in $H$.","I am a little stuck in understanding the logic to this proof I found. Prove that if $H$ is a subgroup of $G$, then the identity $e$ of $G$ is in $H$. Since $H$ is a subgroup, it must have an identity $e_{H}$. Note that $e_{H}\circ e_{H}=e_{H}$ and $e\circ e_{H}=e_{H}\circ e=e_{H}$. Then $e_{H}\circ e_{H}=e\circ e_{H}$. Then $e_{H}\circ e_{H}\circ e=e\circ e_{H}\circ e$. Then $e_{H}=e$. The last part I do not understand. I understand the first side of the equation becomes $e_{H}$, but why does $e\circ e_{H}\circ e=e$?","['abstract-algebra', 'group-theory', 'proof-explanation']"
2838072,How to express $\frac{\frac{1}{2}}{\sqrt[4]{\frac{3}4}-\sqrt[4]{\frac{1}{4}}}$ into $\frac{1}{2}\sqrt{5+3\sqrt{3}+2\sqrt{12+7\sqrt{3}}}$,"I am just wondering how to express $\frac{\frac{1}{2}}{\sqrt[4]{\frac{3}4}-\sqrt[4]{\frac{1}{4}}}$ into $\frac{1}{2}\sqrt{5+3\sqrt{3}+2\sqrt{12+7\sqrt{3}}}$ (This one is taken from Wolfram BTW) Clearly, $\frac{\frac{1}{2}}{\sqrt[4]{\frac{3}4}-\sqrt[4]{\frac{1}{4}}}$ can be rewritten as $\frac{1}{2(\sqrt[4]{\frac{3}{4}}-\sqrt[4]{\frac{1}{4}})}$, but what should I do next? I am a little inexperienced in manipulating surds, so some helps are very much appreciated Thank you!","['algebra-precalculus', 'radicals', 'nested-radicals']"
2838077,Solving a quadratic equation in two variables: $x^2-4xy+3y^2=0$,"How can we solve this equation? $$x^2-4xy+3y^2=0$$ How many values of $x$ and $y$ are possible? (There are two variables so it may not be possible to solve it, but how can we guess the number of real solutions, i.e., real values of $x$ and $y$?)","['algebra-precalculus', 'factoring', 'quadratics']"
2838103,"What makes a linear system of equations ""unsolvable""?","I've been studying simple systems of equations, so I came up with this example off the top of my head:
\begin{cases}
x + y + z = 1 \\[4px]
x + y + 2z = 3 \\[4px]
x + y + 3z = -1
\end{cases} Combining the first two equations yields 
\begin{gather}
z = 2 \\ x + y = -1.
\end{gather} But substituting $z = 2$ in the third equation implies $$x + y = -7,$$ while substituting $x + y = -1$ in the third equation implies $$z = 0.$$ I notice that $x + y$ appears in all three equations, so if we define $w = x + y$ then this essentially becomes three equations in two variables, which explains why I could solve for a variable using only the first two equations, and why the third equation doesn't agree. So here is my question:
What is the distinguishing feature of systems of equations that determines whether or not they have a solution?
Perhaps put another way: in general, is there a ""check"" one can do on a system (aside from actually trying to solve it) to determine if there will be a solution? Edit: Thanks for all the input everyone. I'm satisfied knowing that in general, there is no way to simply inspect such a linear system to determine if it has a solution - rather, some work is required.
The geometric interpretations of linear systems of equations given in several answers were very helpful. Specifically: the interpretation of trying to find a vector $x$ in $Ax = b$ so that that the matrix $A$ represents a linear transformation mapping the vector $x$ to the vector $b$ (which may not be possible if $A$ maps $n$-dimensions to $n-1$ dimensions).","['linear-algebra', 'systems-of-equations']"
2838130,Canceling the Integral,"Suppose we begin to integrate $e^xcosh(x)\ dx$ by parts.  Regardless of our $u$ and $dv$ choices, we arrive at $$\int e^x\cosh x\ dx = e^x\cosh(x) - e^x\sinh(x) + \int e^x\cosh x\ dx\tag{Equation 1}$$  If we subtract the integral from both sides of $(\text{Equation }1)$, leaving behind only an integration constant $C$, we get $$C = e^x\cosh(x) - e^x\sinh(x)\tag{Equation 2}$$  Graphing $e^x\cosh(x) - e^x\sinh(x)$ would reveal that $C = 1$, but what's interesting is that even without graphing, or applying any identities, we can learn from the left side of $(\text{Equation }2)$ that the value of $e^x\cosh(x) - e^x\sinh(x)$ is a constant; it doesn't depend on $x$.  It's intriguing that such an unusual fact would emerge, while it had nothing to do with the original problem. Are there any other instances where canceling out the original integral during integration by parts is profitable, either for solving the integral, or discovering some external fact?","['hyperbolic-functions', 'periodic-functions', 'integration', 'calculus']"
2838138,Exercise 2.2.5 in Qing Liu's book,"This is paraphrased from exercise 2.2.5 in Qing Liu's book: Let $X$ be a topological space. Fix a sheaf $\mathcal{G}$ on $X$ and a
  closed point $x_{0}\in X$. We define a new sheaf $\mathcal{F}$ as follow:
  $\mathcal{F}(U)=\mathcal{G}(U)$ if $x_{0}\notin U$ and
  $\mathcal{F}(U)=\left\{s\in \mathcal{G}(U):s_{x_{0}}=0\right\}$
  otherwise. Prove that $\text{Supp}$ $\mathcal{F}= \text{Supp}$
  $\mathcal{G}\setminus \left\{x_{0}\right\}$. Here $\text{Supp}$
  $\mathcal{F}=\left\{x\in X:\mathcal{F}_{x}\neq 0 \right\}$. It is easy to see that $\text{Supp}$ $\mathcal{F}\subseteq \text{Supp}$ $\mathcal{G}\setminus \left\{x_{0}\right\}$, but I don't know how to prove the reverse that if $x\in \text{Supp}$ $\mathcal{G}\setminus \left\{x_{0}\right\}$ then $x\in \text{Supp}$ $\mathcal{F}$. Can anyone help? I'm sorry if my question is silly.",['algebraic-geometry']
2838160,Improper Integral - Partial Fractions or a Trick?,"$$\int_{-\infty}^{\infty}\frac{dx}{(x^2+ax+a^2)(x^2+bx+b^2)}$$ a and b are real constants How should I solve the above integral? Is there a nice, interesting method? or is partial fractions the only way to do it? I did solve it using partial fractions, but it got really lengthy and cumbersome - wondering if there's a nicer way to do it. Thanks a lot!","['improper-integrals', 'integration', 'definite-integrals']"
2838173,Approximation argument for Poisson integral formula,"In the PDE book, for a harmonic function $u \in C^2(B_R(0))\cap C^1(\overline B_R(0)),$ we have the following Poisson integral formula
  $$
u(y)=\frac{R^2-|y|^2}{n w_n R}\int_{\partial B_R(y)}\frac{u(x)}{|x-y|^n}ds_x
$$
  An approximation argument shows that the Poisson integral formula continues to hold for $u \in C^2(B_R(0))\cap C(\overline B_R(0)).$ What is the approximation argument? For a harmonic function $u \in C(\overline B_R(0))$, is there a sequence of harmonic functions $u_k \in  C^1(\overline B_R(0))$ such that $u_k \to u$ in $C(\overline B_R(0))$? I have no idea for harmonic functions. May I use convolution? Please let me know if you have any hint or comment for it. Thanks in advance!","['real-analysis', 'partial-differential-equations', 'linear-pde', 'multivariable-calculus', 'vector-analysis']"
2838180,Is there a converse of Abhyankar-Moh-Suzuki theorem?,"Let $f=f(t),g=g(t) \in k[t]$, $k$ is a field of characteristic zero. The well-known theorem of Abhyankar-Moh-Suzuki says the following:
If $k[f,g]=k[t]$, then $\deg(f) | \deg(g)$ or $\deg(g) | \deg (f)$. (Actually, there is a version in any characteristic, but characteristic zero is good enough for me. Moreover, in view of the second remark below, perhaps it is better to assume that $k$ is also algebraically closed). Is there some kind of converse, namely, something like: 
  If $\deg(f) | \deg(g)$ or $\deg(g) | \deg (f)$ and $f$ and $g$ satisfy property $P$, then $k[f,g]=k[t]$? Of course, property $P$ must include: $k(f,g)=k(t)$, but this alone is not enough, as $(f,g)=(t^6+t^2,t^3)$ shows; indeed, $k(t^6+t^2,t^3)=k(t^2,t^3)=k(t)$ but $k[t^6+t^2,t^3] \subsetneq k[t]$.
Notice that the condition that $\deg(g) | \deg(f)$ is quite 'artificial' since we just added $t^6=(t^3)^2$ to $t^2$, and $\{t^2,t^3\}$ has relatively prime degrees (so by AMS theorem there is no chance that $\{t^2,t^3\}$ will generate $k[t]$)
and $k[t^6+t^2,t^3]=k[t^2,t^3]$. I had several ideas for property $P$, but unfortunately they did not guarantee that $k[f,g]=k[t]$, for example: If both $f$ and $g$ are odd polynomials = having terms of odd degrees only. Notice that if $f$ and $g$ are just odd polynomials then they can be far from generating $k[t]$, for example, $(f,g)=(t^5,t^{15})$, for which $k(f,g)=k(t^5,t^{15})=k(t^5) \subsetneq k(t)$. This question only shows that if $f$ and $g$ are odd polynomials, then $k(f,g)=k(h)$, where $h$ is a polynomial of odd degree but not necessarily of degree $1$, like here that $h=t^5$ (notice that the existence of such $h$ follows from Luroth-Noether theorem). But even if we assume that two odd polynomials $f$ and $g$ satisfy $k(f,g)=k(t)$, there is no guarantee that $k[f,g]=k[t]$, as $(f,g)=(t^5,t^{15}+t^3)$ shows; indeed, $k(t^5,t^{15}+t^3)=k(t^5,t^3)=k(t^2,t^3)=k(t)$ but $k[t^5,t^{15}+t^3] \subsetneq k[t]$. Remarks: (1) There are very nice criteria for $k[f,g]=k[t]$ and $k(f,g)=k(t)$, see Theorem 2.1 of this paper . However, in practice, D-resultant seems not useful; perhaps it is useful only for very low degrees such as $(\deg(f),\deg(g)) \in \{(2,2),(3,3),(2,4)\}$. (2) If $k$ is also algebraically closed (this is necessary!), then: $k[f,g]=k[t]$ if and only if for all $t \in k$, $(f'(t),g'(t))\neq (0,0)$ and $H: t \mapsto (f(t),g(t))$ is injective, see this paper . However, showing that $H$ is injective is difficult (except low degrees cases).
Based on this result and on the answer to this question we can get: $k[f,g]=k[t]$ if and only if for all $\lambda,\mu \in k$, $\deg(\gcd(f(t)-\lambda,g(t)-\mu)) \leq 1$.
(I can add a proof for this if someone will require). Any comments are welcome!","['polynomials', 'field-theory', 'algebraic-geometry', 'commutative-algebra']"
2838189,Singularity of the product of two rectangular matrices?,"Let $A$ be an $m \times n$ matrix and $B$ be an $n \times m$ matrix where $m<n$. Then can we say that the product $AB_{m \times m}$ is always singular or always non-singular? Also, can we say that $BA_{n \times n}$ is always singular or non-singular?. Does this change any thing? I was thinking that since $m<n$ we have Rank$(A) \leq m$ and similarly Rank$(B) \leq n$ and also that Rank$(AB) \leq min($Rank$(A)$,Rank$(B)$), but will that help? How can I think about this problem?","['matrices', 'linear-algebra', 'determinant']"
2838190,Find $B=A^2+A$ knowing that $A^3=¥begin{bmatrix}4&3¥¥-3&-2¥end{bmatrix}$,Find $B=A^2+A$ knowing that $A^3=¥begin{bmatrix}4&3¥¥-3&-2¥end{bmatrix}$ Is there a way to solve this rather than just declaring a matrix $$A=¥begin{bmatrix}a&b¥¥c&d¥end{bmatrix}$$ and then trying to solve a system of cubic equations? My attempt: $$A^3 -I_2 = ¥begin{bmatrix} 3 & 3¥¥ -3 & -3¥end{bmatrix} = 9 ¥begin{bmatrix} 1 & 1¥¥ -1 & -1¥end{bmatrix}$$ and $$B=¥frac {A^3-I_2}{A-I_2}-I_2$$ $$(A-I_2)(A^2+A+I_2)=9¥begin{bmatrix}1&1¥¥-1&-1¥end{bmatrix}$$ But I get stuck here.,"['matrices', 'matrix-equations', 'linear-algebra']"
2838203,solving $y'-x^2y+y^2=2x$,"Find a general solution to the equation 
$$y'-x^2y+y^2=2x$$ I managed to guess a solution $y=x^2$
$$(x^2)'-x^2\cdot x^2+(x^2)^2=2x-x^4+x^4=2x$$
To get the general solution I tried plugging $y=z(x)+x^2$ into the equation, which gives
$$z'+x^2z+z^2=0$$
The equation is not exact
$$dz+(x^2z+z^2)dx=0$$
and I failed in the search for an integrating factor. Moreover, the solution in Wolfram is not so welcoming. Maybe there is some implicit solution...",['ordinary-differential-equations']
2838229,"""Distribution"" of numbers $0\leq a\leq b\leq c\leq d\leq 1$","A friend came up with the next problem:
Consider $0\leq a\leq b\leq c\leq d\leq 1$ numbers such that $a+b+c+d=1$. Are there numbers $a_{0}$, $b_{0}$, $c_{0}$, $d_{0}$ that minimize $$|a-a_{0}|+|b-b_{0}|+|c-c_{0}|+|d-d_{0}|$$ most of the time? I mean, if we repeat the process of choosing $a$, $b$, $c$ and $d$ randomly, is there a expected value for $a$, $b$, $c$ and $d$? And what it is? I tried to start with an easiest problem, with just $a$ and $b$, such that $0\leq a\leq b\leq 1$ and $a+b=1$ but I had no idea where to start, so I decided to try with some numerical sampling, and I did the next in Python: Choose a number $x\in [0,1]$ uniformely, and define $a=\min\{x,1-x\}$ and $b=\max\{x,1-x\}$. Clearly $0\leq a\leq b\leq 1$ and $a+b=1$. Doing this, and repeating a lot of times I got that the ""expected value"" for $a$ was $0.25$ and for $b$ was $0.75$, so the ratio is $1:3$ . Next, I tried the same in Python but with two values: Choose $x,y\in [0,1]$ uniformely, and let $x_{1}=\min\{x,y\}$ and $x_{2}=\max\{x,y\}$, now define $$A=\{x_{1},x_{2}-x_{1},1-x_{2}\}$$ and let $a_{1}$, $a_{2}$, $a_{3}$ be the elements of $A$ in increasing order. Clearly $0\leq a_{1}\leq a_{2}\leq a_{3}\leq 1$, and $a_{1}+a_{2}+a_{3}=1$, and repeting a lot of times I got that the ""expected values"" were of the ratio $2:5:11$ . Repeating the same method but now with one more variable I got that the ""expected values"" were on ratio $3:7:13:25$ . Lastly, with one more variable the ratios were $12:27:47:77:137$ . All this calculations were found just by trial and error, and are clearly non mathematically justified, but seems like, at least, a good conjecture. Is there any hidden pattern behind these ratios? Is there any reason for this numbers to came up? Any help would be appreciated.","['statistics', 'probability']"
2838247,Irrationality of $\sqrt{2}$ invoking some fact of definite integrals,"Few months ago I tried to prove that $\sqrt{2}$ is irrational using some object/fact from (elementary, say us as simplest than is possible) integral calculus. It is known that in some proofs of the irrationality of $\sqrt{2}$ is used the quantities $\sqrt{2}\pm a$, where $a$ is an integer (see 1 ). Again I would like to ask about it (as an open question and I hope that the question is anserwerable). Question. Is it possible to use any fact from integral calculus (you can to combine with the definition of Riemann integral, recurrence relations, calculation of areas, uses of the infinite...) to get a proof of the irrationaly of $\sqrt{2}$ (even if it is presented as more artificious than the ones that are in the literature)? Many thanks. My last attempt was to write expressions as $$\int_0^1\frac{1}{\sqrt{x}\sqrt{2-\sqrt{x}}}dx=4(\sqrt{2}-1),$$
and (playing with this kind of integrals) defining the integer $4$ as $$\mathcal{J}=\int_0^1\frac{1}{\sqrt{x}\sqrt{1-\sqrt{x}}}dx$$
then
$$\sum_{k=0}^{\mathcal{J}}\binom{\mathcal{J}}{k}\left(\int_0^1\frac{1}{\sqrt{x}\sqrt{\sqrt{\mathcal{J}}-\sqrt{x}}}dx\right)^k\cdot \mathcal{J}^{-k}=\mathcal{J}.$$ References: 1 Square root of 2 is irrational , Cut The Knot.","['radicals', 'irrational-numbers', 'calculus', 'integration', 'definite-integrals']"
2838262,Where Gaussian Random Distribution came from?,"I'm studying an introduction to probability, and before the explanation of the Central Limit Theorem the author presents the Gaussian Normal Distribution. It has a complex and a non-intuitive formula, even if it's very important in the Probability field.
Was the gaussian distribution found before the deduction of the CLT ? How was this distribution discovered ? And what are the properties that makes it so special ?","['statistics', 'probability']"
2838362,Complement Event,"Four people are chosen at random. What is the probability that: (a) One of the first three people chosen has their birthday in the same month as the fourth person? The solution given for this question is just the complement of $P(\text{None of the first three in same month as 4th})= 1- (\frac {11}{12})^3
$ I feel that the answer is wrong here because the complement of ""None of the first three in same month as 4th"" doesn't equal to ""exactly one of the first three people chosen has their birth day in the same month"" but rather the complement means that not all of the first three is in the same month as 4th? Therefore, shouldn't the answer be $\binom {3}{1} \frac {1}{12} \cdot\ \frac {11}{12} \cdot\ \frac {11}{12}$ (b) The first and second people chosen have their birthdays in the same month, given that there is some pair of people having their birthdays in the same month? For (b) the answer given was P(first and second in same month)/P(some pair in same month) In particular, the P(some pair in same month) I get that it's easier to use rule of complement to get the answer: $1-(\frac {12}{12} \frac {11}{12}\frac {10}{12}\frac {9}{12} ) $ However, how come my answer here is wrong: $\binom {4}{2} \frac {12}{12} \frac {1}{12} \frac {11}{12} \frac {10}{12}$ My rationale is, there are $4$ items in total, we are picking $2$ at a time to make them a pair thus the combination should be $4C2$. The next step is to figure out the probability. Assume that the 1st person can be any of the month $\frac {12}{12}$. Assuming the 2nd person share the same birthday as the 1st then it should be $\frac {1}{12}$. This completes the first pair therefore, the 3rd person has a probability of $\frac {11}{12}$ and subsequently the 4th person's probability is $\frac {10}{12}$ What is wrong with my reasoning here? Many thanks in advance!","['combinatorics', 'probability']"
2838369,Solve a recursive equation about chess,"If we know that the King stands in the square on the left side at the bottom and has each time 3 possible moves: he goes 1 square to right
he goes 1 square up
he moves diagonally to the right so if he start from the square f(1,1) then to know how many possibilities there are to get to square f(m,n) then we can use this equation :
$$
 f(m,n)=f(m-1,n-1)+f(m-1,n)+f(m,n-1)
$$
The question is how to prove that this equation down is a solution of equation for $f(n,n)$ :
 $$
 f(n, n) =\sum_{i=0}^{n-1} {n-1\choose i} {n-1+i \choose i}
$$","['chessboard', 'induction', 'recursion', 'combinatorics', 'discrete-mathematics']"
2838373,Series whose Cauchy product is absolutely convergent - A general example,"Is there series that is divergent or conditionally convergent with absolutely convergent Cauchy product? Seems like there is a group of these examples! Perhaps finding divergent series with absolutely convergent Cauchy product isn't that difficult (see here for an example), but perhaps finding conditionally convergent series with absolutely convergent Cauchy product is more complicated. I've been studying the paper by Florian Cajori after RRL posted a comment in my post: Conditionally converges $\sum_{k=0}^\infty a_n$, $\sum_{k=0}^\infty b_n$ and also their Cauchy product . Perhaps the paper is a bit 'ancient' (100 years ago) (perhaps mathematicians 100 years ago had a style a little bit differently than modern day mathematicians). Perhaps mathematicians 100 years ago had a style a little bit differently than modern day mathematicians. Or else, I think my mathematical maturity isn't enough, so I used lots of time to read his paper but still fails to understand all materials in the  paper. (For example, I only realize what is meant by 'dropping parentheses' after I read it for an hour...) The author of the paper gives two examples, the first example is about, there are conditionally convergent series such that their Cauchy product are absolutely convergent.  That example, is digested by me after some works. My works on this example, hope that there aren't typos or errors :). P.1 P.2 P.3 P.4 P.5 He gives 'the general method' later in the paper. That's what I'm stuggling. Not to say that I can't understand why the constraints are set that way, I even don't know how to prove the general method (and the author can't write down his delicate proof because the margin is too small). Following his words, I conjectured that: If  $\hspace{2ex}$$b,c\in\mathbb N, a=2c;\vert d\vert=\min\{\vert l\vert:b\equiv l\text{  }(\text{mod }a)\}, d\mid c$; $\hspace{4ex}$$\forall(k=1,...,c),\gamma_k=1$; $\forall (k=c+1,...,a),
> 
> \gamma_k=-1$; $\hspace{4ex}$$\forall(k=1,...,b),\delta_k\in\{-1,1\}$; $\hspace{4ex}$$\forall (n\ge m_1,k=1,...,a),s_{n,k}=\frac{\gamma_k}
{an+\alpha_k},\text{ while }m_1\in\mathbb Z$ $\hspace{8ex}$is chosen s.t. $(s_{n,k})$ are well-defined; $\hspace{4ex}$$\forall (n\ge m_2,k=1,...,b), t_{n,k}=\frac{\delta_k}
{bn+\beta_k},\text{ while }m_2\in\mathbb Z$ $\hspace{8ex}$is chosen s.t. $(t_{n,k})$ are well-defined; $\hspace{4ex}$$\forall(n\ge m_1,k=1,...,a), u_{an+k}=s_{n,k}$; $\hspace{4ex}$$\forall (n\ge m_2,k=1,...,b), v_{bn+k}= t_{n,k}$; $\hspace{4ex}$$m\ge \max\{am_1+1,bm_2+1\}$ is an arbitrary integer. Then $\sum_{n=m}^\infty u_n$ converges conditionally and Cauchy
  product of $\bigg(\sum_{n=m}^\infty u_n,\sum_{n=m}^\infty v_n\bigg)$, $\hspace{4ex}$i.e. $\sum_{n=m}^\infty c_n=\sum_{n=m}^\infty \bigg(
\sum_{k=m}^n u_k v_{n+m-k} \bigg)$ converges absolutely. Here's what I've tried: I hope to break (It is called 'dropping paraentheses', as mentioned in the paper) the series $\sum_n^\infty \Big(c_{nab+1}+c_{nab+2}+...+c_{nab+(ab-1)}\Big)$ into $\sum_n^\infty c_{nab}, \sum_n^\infty c_{nab+1},...\sum_n^\infty c_{nab+(ab-1)}$ so that it can be managed as the way for the particular example before. Should I require further that $(ab)$ divides $m$? But even after that,... Inspired by the proof of Merten's theorem , does $\sum_{i=0}^N\sum_{j=0}^i a_jb_{i-j}=\sum_{j=0}^Na_{N-j}B_j$ helps? Perhaps we can use the value of the series that converges conditionally to prove the Cauchy product above converges? But it is only convergent then, not necessarily absolutely. However, I thought, even managing the case when $a=b=4$ (the example before) is complicated and heavy, how can I allow $a,b$ to be arbitray (and maybe distinct) natural number and develop a general algorithm to prove the theorem? If there is something like mathematical induction to handle countable infinite amount of statement, which is applicable here, maybe I can handle it. Well... however, induction obviously doesn't seems to work here directly . (or maybe, there is, an exciting, trick?) I observed that in the first example stated in the paper, $\sum_{n=0}^\infty c_n=0$, which is because $\big(\sum_{n=0}^\infty a_n\big)\big(\sum_{n=0}^\infty b_n\big)=0$. Should we make more conditions (such as requiring $\sum_{n=0}^\infty u_n=0$?) according to this? Any hope will be appreciated. I may just want some constructive hints... Thank you!","['conditional-convergence', 'real-analysis', 'absolute-convergence', 'cauchy-product', 'sequences-and-series']"
2838397,"Linear Transformation Question $ \operatorname{Hom}(V,W)$ and $ \operatorname{Hom}(W,V)$ satisfying conditions","Can I please get help on this questions. I'm not sure how to start it or end it. Let $V$ and $W$ be vector spaces over a field $F$. Let $\alpha$ be an element of $ \operatorname{Hom}(V,W)$ and $\beta$ be an element of $ \operatorname{Hom}(W,V)$ satisfy the condition  $\alpha\cdot\beta\cdot\alpha=\alpha$. If $w \in  \operatorname{Im}(\alpha)$, show that $\alpha^{-1}(w) = \{ \beta(w)+v - \beta \alpha(v)  \mid v \in V \}$.","['linear-algebra', 'linear-transformations']"
2838415,Summation of series (general term),"Hi, 
I need help with this. I haven't got a clue how to solve this question. I'm familiar with AP, GP, HP, AGP, some special series... I tried, but couldn't figure out how to approach this question. I have attached two pictures, one is the question and the other one is the answer to the question. Thanks And, btw, there's something else I don't understand. It's pretty obvious that the general term of this series is t(n) = n(1-x)(1-2x)(1-3x).....[1-(n-1)x] 
But how is it possible? If we put n = 1, I don't think it produces the first term, which is 1. Likewise, putting n=2 doesn't produce the second term, which is 2(1-x). Am I missing something here?",['algebra-precalculus']
2838419,Show that a map $f\in C^1(\mathbb{C})$ is a polynomial of a degree less or equal to $n$.,"Show that a map $f\in C^1(\mathbb{C})$ with $|f(z)|\leq C |z|^{\alpha}$ for some $a<n+1$, a constant $C>0$ and all $z\in\mathbb{C}$ is a polynomial of a degree less or equal to $n$. This was given to me as a hard bonus exercise as part of my real analysis/multivariable calc course (first year student). Sadly, too hard for me and we only touched upon very introductionary complex analysis via Taylor series etc. I'm not even seeing the right approach. I'd be very thankful for somebody illustrating to me how to proceed here. I've never seen a proof for an exercise of this kind before.","['real-analysis', 'polynomials', 'calculus', 'multivariable-calculus', 'complex-analysis']"
2838451,how to get $\sum_{k=1}^\infty \arctan\biggr(\frac{10k}{(3k^2+2)(9k^2-1)}\biggr)=\log3-\frac{\pi}{4}$,"problem in the above asked equation of S.Ramanujan ! Hello everyone,this is a result of an entry described by ramanujan,i first request you to see the photo i have attached. MY ATTEMPTION From LHS in entry 3 in photo i shift $\frac{\pi}{4}$ to right,and for the another term i have done the following simple step $$\sum_{k=1}^{2n+1} \arctan(\frac{1}{n+k}) =\sum_{k=n+1}^{3n+1} \arctan(\frac{1}{k}) $$ MY PROBLEM How the RHS of the above EQn is converted in the picture using taylor's theorem ? HOW ? $$\sum_{k=n+1}^{3n+1} \arctan(\frac{1}{k})=  \sum_{k=n+1}^{3n+1} [\frac{1}{k}+O(\frac{1}{k^3})]$$ AND in the next step RHS just converted i.e. $$\sum_{k=n+1}^{3n+1} \frac{1}{k} + O(\frac{1}{n^2})$$ please help me, i dont know what is this O(x) and how it is generated from taylor's theorem ?
after that step i know how it is converted into riemann sum when limit tends to infinity i.e. $$\sum_{k=n+1}^{3n+1} \frac{1}{k} =\sum_{k=1}^{2n+1} \frac{1}{n+k} = \sum_{k=1}^{2n} \frac{1}{n+k}+\frac{1}{3n}$$ now for limit tends to infinity
$$\lim_{n\to\infty} \sum_{k=1}^{2n} \frac{1}{n+k}  + \lim_{n\to\infty}\frac{1}{3n}= \lim_{n\to \infty} \frac{1}{n} \sum_{k=1}^{2n} \frac{1}{1+\frac{k}{n}}=\int_{0}^{2} \frac{dx}{x+1} = \log{3}$$ SORRY for mistakes, this is my first question.","['harmonic-numbers', 'trigonometry', 'notation', 'calculus']"
2838452,"Using the residue theorem, calculate $\int_{0}^{2\pi}\frac{1}{1-2a\cos{\theta}+a^2}d\theta$","So I have some troubles for the following question:
Using the residue theorem, calculate : $\int_{0}^{2\pi}\frac{1}{1-2a\cos{\theta}+a^2}d\theta$ with i. $|a|$ < 1 , ii. |a| > 1 I guess that i. and ii. would give us different residues to calculate given that the surface isn't the same. First I solved for a in the denominator. I get a= $\cos{\theta}\pm\sin{\theta}$. Now I could use the fact that $\cos{\theta}=\frac{e^{i\theta}+e^{-i\theta}}{2}$ but I don't really see what to do next. I have seen other questions like Evaluating $\int_0^{2 \pi} \frac {\cos 2 \theta}{1 -2a \cos \theta +a^2}$ but the conditions and the function aren't exactly the same and I'm still stuck. Thanks for your help !","['complex-analysis', 'definite-integrals']"
2838491,"Trick, or Indefinite Integration?","Evaluate $$\int_{0}^{\pi/2}\dfrac{\sin^6 x}{\sin x + \cos x}\text{ d}x$$ Is there a nice, elegant way of solving the above integral? Here's what I did - 
Replaced $f(x)$ by $f(\pi/2-x)$, added and got rid of some common factors in numerator and denominator. This was followed by indefinite integration, and ultimately substituting the limits in the final expression. The above method is kind of long, though - and I'm wondering if there's a quicker, shorter and rather elegant way of getting around this integral. Could someone post a solution, and share ideas? Thanks a lot.","['integration', 'definite-integrals', 'trigonometric-integrals']"
2838497,Range of Lifted Differential Operators,"Let $D:C^\infty M \rightarrow C^\infty M$ a differential operator on a Riemannian manifold $M$. Define its maximal extension as closed operator $$D_\max:L^2M\supset \mathrm{dom}(D_\max):=\{u\in L^2M\vert Du\in L^2M\} \rightarrow L^2M, u \mapsto Du$$ Further suppose that $p\colon\hat M\rightarrow M$ is a finite-sheeted Riemannian covering and that $\hat D$ is the lift of $D$, i.e. $\hat Dp^*u=p^*Du$ for $u\in C^\infty M$. Similar to above define $\hat D_\max$ as closed operator in $L^2 \hat M$. Question : If $D_\max$ has closed range, does also $\hat D_\max$ have closed range? Below are some thoughts and proof attempts: Examples and alternative Characterization : If $M$ is compact, then closedness of range is often trivial. Let's look at some non-compact examples. We use the following characterization: ""A closed operator $A$ has closed range iff there is an ""Poincaré-inequality"" of the form $\Vert A u \Vert_{L^2} \ge C \Vert u - Qu\Vert_{L^2}$, where $Q\in \mathcal{B}(L^2M)$ is the orthogonal projection onto $\ker A$. "" E.g. if $M$ is a bounded open subset of euclidean space (with $\partial \bar M$  not too terrible) and $D= \nabla$, then the Poincaré-Wirtinger inequality tells us that $D_\max$ has closed range. However, if $M=\mathbb{R}^n$ then $\nabla$ does not have closed range. These two examples show that the closed range assumption is really a global condition, which makes it harder to deal with covering spaces. Also it suggests that $p$ better be finite-sheeted, otherwise we might for example run into trouble when lifting an operator from the torus to the euclidean plane. Lifting the property for invariant functions: Let $H_0\subset L^2\hat M$ be the subspace of functions which are invariant under the action of $\mathrm{Aut}(p)$, equivalently $H_0=\{p^*u \vert u\in L^2 M\}$. It's easy to see that the the Poincaré-inequality from $M$ lifts to functions in $H_0 \cap \mathrm{dom }(\hat D_\max)$. But does this imply that the Poincaré-inquality holds for all functions? In an attempt to understand this, let's consider a $2$-sheeted cover, for simplicity $p\colon S^2\rightarrow \mathbb{R}P^2$. Then $\mathrm{Aut}(p)$ is generated by the antipodal map $a$ and we have
$$
L^2\hat M = \ker (a^* -1 ) \oplus \ker (a^* + 1 )=: H_0 \oplus H_1.
$$
It'd be great if he had an isomorphism $T\colon H_0 \xrightarrow{\sim}H_1$ that commutes with $D$ (on say smooth functions). But this seems to be impossible. Finding $T$ which is an iso is not hard: Let $\chi$ the characteristic function of the upper half plane, then $Tu=\chi u - a^*(\chi u)$ defines an iso, but does not even preserve the smooth subspace. If we want smooth functions to be preserved, we immediately think of a multiplication operator $Tu=\varphi u$, for an antipodal map $\varphi: S^2\rightarrow S^1$ (it has to be nowhere-vanishing, so we might as well map into the circle). But this is forbidden by Borsuk-Ulam. Using partitions of $1$? Denote $\hat u=\vert\mathrm{Aut}(p)  \vert ^{-1}\cdot \sum_{a\in \mathrm{Aut}(p)}a^*u\in H_0$ and suppose $f_1 + \dots + f_N =1$ is a partition of unity such that $a^* \mathrm{supp} f_i \cap \mathrm{supp} f_i = \emptyset$ if $a\neq \mathrm{id}$. Then all information about $u \in L^2\hat M$ is contained in the collection $(f_1 u)\hat{~},\dots , (f_1 u)\hat{~}$. For each of those we have a Poincarè-inequality and I hoped to generalize it to $u$, but for that one needs control on $\Vert df_i \Vert_{L^\infty}$ (which in general isn't even finite).","['functional-analysis', 'covering-spaces', 'differential-geometry', 'partial-differential-equations']"
2838525,Evaluating trivial integral with linear algebra,$$\int_{-\infty}^{\infty} \frac{\text{d}x}{ax^2+bx+c} = \frac{\pi}{\sqrt{\det(A)}}$$ where $A = \begin{bmatrix}a&\frac{b}{2}\\\frac{b}{2}&c\end{bmatrix}$ The connection to matrix quadratic form is given via the equivalence: $$ax^2 + bx + c \equiv \begin{bmatrix}x&1\end{bmatrix}\begin{bmatrix}a&\frac{b}{2}\\\frac{b}{2}&c\end{bmatrix} \begin{bmatrix}x\\1\end{bmatrix} $$ What standard results from Linear Algebra make this integral a linear transformation of the standard integral? $$\int_{-\infty}^{\infty} \frac{\text{d}x}{x^2 + 1} = \pi$$,"['determinant', 'integration', 'definite-integrals', 'linear-transformations', 'linear-algebra']"
2838527,"Prove that $\displaystyle \prod_{1\leq a<b\leq \frac{p-1}{2}}\,\left(a^2+b^2\right)\equiv \pm1\pmod{p}$ for a prime $p\equiv 3\pmod{4}$.","Show that, if $p$ is a prime which is congruent to $3\pmod{4}$, then the product $$\prod_{1\le a < b \le \frac{p-1}{2}} (a^{2}+b^{2}) \equiv \pm{1} \pmod{p}\,.$$ 
I verified this for simpler cases but I am not sure how to proceed. For example, if $p=7$, then the product is $$5 \cdot 13 \cdot 10 \equiv -1\pmod{7}\,.$$","['number-theory', 'elementary-number-theory']"
2838539,Pontryagin charge of $SO(3)$ connection on a bundle over a $4$-manifold,"On page 4 of https://arxiv.org/pdf/1009.5365.pdf , the authors write: Given any $SO(3)$ connection $A$ on a bundle $E$ over a (not necessarily compact) $4$-manifold, $Z$, let $F(A)$ denote its curvature $2$-form. Define the Pontryagin charge of $A$ to be the real number
$$p_1(A)=-\frac{1}{8\pi^2}\int_Z\text{Tr}(F(A)\wedge F(A)),$$
provided this integral converges. When $Z$ is closed, $p_1(A)=\langle p_1(E),[Z]\rangle\in\mathbb{Z}$. Can you give a few examples of how to compute the Pontryagin charge?
Thank you!","['characteristic-classes', 'geometric-topology', 'differential-geometry']"
2838578,Is $\mathbb Q(\zeta_6)=\mathbb {Q}(\zeta_3)$?,"I got myself confused over the following: We have $$\mathbb Q(\zeta_3)=\mathbb Q(\exp(2\pi i/3))=\mathbb Q\left(\cos\frac{2\pi}{3}+i\sin\frac{2\pi}{3}\right)=\mathbb Q\left(-\frac{1}{2}+\frac{i\sqrt 3}{2}\right)=\mathbb Q(i\sqrt 3),$$ but also $$\mathbb Q(\zeta_6)=\mathbb Q(\exp(2\pi i/6))=\mathbb Q\left(\cos\frac{2\pi}{6}+i\sin\frac{2\pi}{6}\right)=\mathbb Q\left(\frac{1}{2}+\frac{i\sqrt 3}{2}\right)=\mathbb Q(i\sqrt 3).$$ So the fields are absolutely identical? $\Phi_6$ splits in $\mathbb Q (\zeta_3 )$ and vice versa?",['abstract-algebra']
2838604,Unsure if I have solved/proven this trigonometric inequality.,"This is my first post here. I apologize if it goes against any guidelines for posting. I study math as a hobby and am currently dealing with trigonometry on a high school level. I have so far learned the formulas for trigonometric addition and subtraction and double the angle, as well as what is in my language referred to as the ’trigonometric one’ - getting the radius of the unit circle by use of the pythagorean theorem. I have not yet gotten to deriving trigonometric functions. The following is a problem I could solve by plugging in a set of numbers, but in seeking a more elegant solution, perhaps, I found myself stuck and I don’t know what I am missing. I am appreciative of any help I get. The problem is as follows: Show that if $A$ is an angle and $0^\circ<A<90^\circ$ then $\hspace{0.3cm}\left( 1+\dfrac {1}{\sin A}\right) \left( 1+\dfrac {1}{\cos A}\right)>5$ I began with the following assumption: $$0^\circ<A<90^\circ\rightarrow0<{\sin A}<1\\0<{\cos A}<1\rightarrow\dfrac {1}{\sin A}\\\dfrac {1}{\cos A}>1$$ Given the above, it would follow that: $$\begin{aligned}
\lim _{A\rightarrow 90^\circ}\dfrac {1}{\cos A}&=\infty \\
\lim _{A\rightarrow 0^\circ}\dfrac {1}{\sin A}&=\infty
\end{aligned}$$ This alone doesn’t seem like enough to show what is asked. I can show that at $A=45^\circ$ the product is still greater than 5, but I am not sure how any offset in degrees from there affects two trigonometric terms such that the product is still greater than 5. I also tried solving the inequality but ended up with fractioned terms I couldn’t add up or a cubic function if you will, that I couldn’t solve.",['trigonometry']
2838616,Categorical data: Testing difference between two experiments,"I have the following experimental setup: Protein A is capable of cutting protein B in small fragments. The small fragments are identified and the nature of the last amino acid in each fragment is counted. Thus, in one experiment it is possible to detect all 20 amino acids but with a different total count. The total count depends on the nature of Protein A and the conditions of the experiment. At the end, for the two conditions tested I end up with a table like this: Amino-acid  Exp1   Exp2
A             0      3
R            20     12
G            10     15
H            14     22
E             5      0 with entries for all 20 amino acids and I also know the total number of fragments from Protein B that were identified in each condition. The question I need to answer is: Are the amino acids frequencies significantly different under the two experimental conditions? First I thought to use a chi-square test since with the chi-square test I can take into account the different number of fragments that were identified in the two conditions. But inevitably I will end up with expected values being 0 and thus I cannot use the chi-square test. Could you please point me in the direction of the test that can be used in this case? Thanks a lot in advance.","['statistics', 'chi-squared', 'hypothesis-testing']"
2838633,Determine Minimum Value.,"Find the minimum value of
  $$\frac{(x+1/x)^6-(x^6+1/x^6)-2}{(x+1/x)^3+(x^3+1/x^3)}$$
  for $x>0$. When $x=1$, $$\frac{(x+1/x)^6-(x^6+1/x^6)-2}{(x+1/x)^3+(x^3+1/x^3)}=6$$ I tried to plot some points on a graph and I observed that the minimum value is $6$. Any hints would be sufficient.
Thanks I think differentiation would be really complicated","['derivatives', 'maxima-minima', 'calculus']"
2838650,Multiplicative inverse questions,"This is my first time posting on Math exchange. I have been self-teaching myself Mathematics and recently started learning some Algebra. I was posed the following questions, in which I would like to answer before I proceed, but having some trouble finding proper solutions. So the format will be question, and my answer. What name do we give an element $a \in \mathbb{Z}_n$ that has a multiplicative inverse? A modular multiplicative inverse of an integer $a$ is an integer $x$ such that $ax \cong 1 (\mod m)$. This means that the remainder after dividing $ax$ by integer $m$ is equal to $1$. Give an example, for some $n \in \mathbb{N}$, of an element of $\mathbb{Z}_n$, that does not have a multiplicative inverse? This question is throwing me off because - does the $n$ we speak of matter for both $n
\in \mathbb{N}$ and $\mathbb{Z}_n$ ot solely $n$? My guess here would be $0$ because we cannot have an inverse of $0$. Prove that multiplicative inverse are unique; an element $a \in \mathbb{Z}_n$ cannot have two multiplicative inverses. Suppose that an element $a\in\mathbb{Z}_n$ has an inverse $a'$. Next, suppose that $a^*$ is also an inverse of $a$. Then, $a'$ is a solution to $a \cdot_n x=1$, and similarly $a^*$ has a solution to $a \cdot_n x=1$. Lemma: Suppose $a'$ is multiplicative inverse of $a$ in $\mathbb{Z}_n$, then for any $b \in \mathbb{Z}_n$ the equation $a \cdot_n x=b$ has a unique solution, such that $x = a' \cdot_n b$. By Lemma, equation $a \cdot_n x = 1$ has a unique solution, namely $x=a'=a^*$. Therefore, an element $a\in \mathbb{Z}_n$ cannot have two multiplicative inverses. Thank you for the help. I would love any constructive feedback for these. If you can help me elaborate on some ideas, that would be really helpful also. Thank you very much!","['abstract-algebra', 'modular-arithmetic', 'proof-verification']"
