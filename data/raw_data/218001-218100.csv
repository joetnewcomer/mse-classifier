question_id,title,body,tags
4449690,Can you prove that these two series are equal?,"Let for all $x>0$ $$ f(x)=\sum_{n=0}^{+\infty}\frac{1}{x(x+1)\dots(x+n)}$$ Can you prove that for all $x>0$ $$f(x)= e \sum_{n=0}^{+\infty}\frac{(-1)^n}{(x+n)n!} $$ this is a question in a test for undergraduate students. I checked that the series that defines $f$ converges. Moreover i proved that it uniformly converges in every interval of the form $[a,+\infty[$ with $a>0$ . One of my attempts to solve the exercise was to trying to differentiate both series and see if the expression of the derivatives was easier to handle. but I didn't get anywhere. Any suggestions?","['sequences-and-series', 'analysis', 'real-analysis']"
4449729,"Is there a mistake in an old question on this site: ""Where does the sum of sin(n) formula come from?""","https://math.stackexchange.com/a/1119137/29156 Shouldn't Abel's answer be: we will use the fact that $$2 \sin 1 \sin k = \cos(k-1) -\cos(k + 1)$$ let $S = \sin 1 + \sin 2 \cdots + \sin n,$ then $\begin{align}
2S \sin 1 &= 2\sin 1 \sin 1+ 2 \sin 1 \sin 2 + 2 \sin 1 \sin 3\cdots +2 \sin 1 \sin n \\
&=(1 - \cos 2) +(\color{red}{\cos 1} - \cos3) +(\color{red}{\cos 2} - \cos 4)+\cdots +(\cos (n- 1) - \cos ( n + 1) )\\
&=1 + \color{red}{\cos 1} - \color{red}{\cos (n)} - \color{red}{\cos(n+1)}
\end{align}$ ? Obviously this is still bounded","['trigonometry', 'solution-verification']"
4449789,A direct proof of the Vandermonde decomposition of a nonsingular Hankel matrix?,"I have been doggedly searching for a direct proof of the following theorem: Theorem 1: Let $H$ be a complex nonsingular $n\times n$ Hankel matrix. Then $H$ can be factorized $H = V^\top DV$ where $V$ is a complex $n\times n$ Vandermonde matrix and $D$ is a complex $n\times n$ diagonal matrix. There exist numerous references stating that this result is well known either without proof or with a citation to one of the references below which I have found insufficient for one reason or another. Here is an abbreviated summary of my research: References (e.g., here , here , and here ) that are unavailable to me or in Russian References (e.g., here , here , and here ) for a confluent Vandermonde decomposition of a rank-deficient finite and/or finite-rank infinite Hankel matrices: These do no prove the theorem I am after since the Vandermonde decomposition involves confluent Vandermonde matrices. References for a Vandermonde decomposition of positive semidefinite or positive definite Hankel matrix: These don't address the case of a general square Hankel matrix. References ( here , here , and here ) where the theorem is proven as a corollary of a rather length theory: these results do prove the theorem that I want. However, the result follows as a corollary of several pages of a lengthy theory drawing connections to other structured matrices, polynomials, and rational functions. It seems like this basic and essential fact should have a more direct proof. I am looking for as simple and direct a proof of Theorem 1 as possible, a reference to one in the literature, or some insight as to why a simple proof does not exist. Below the fold, I have outlined my attempt to provide such a proof. My Attempt Consider the entries of the Hankel matrix $$
H = \begin{bmatrix} h_0 & h_1 & \cdots & h_{n-1} \\
h_1 & h_2 & \cdots & h_{n+1} \\
\vdots & \vdots &\ddots & \vdots \\
h_{n-1} & h_n & \cdots & h_{2n-2} \end{bmatrix}.
$$ The Vandermonde decomposition in Theorem 1 is equivalent to finding a representation of the sequence $(h_k)_{0\le k\le 2n-2}$ of the following form: $$
h_k = \sum_{j=1}^n \alpha_j z_j^k, \quad k = 0,1,2,\ldots,2n-2. \tag{$\star$}
$$ Observe that the sequence $(h_k)_{0\le k\le 2n-2}$ has fewer parameters than the parametrization ( $\star$ ). To rectify this, add $h_{2n-1}$ , value to be set later, to the end of the sequence and require it to obey the parametrization ( $\star$ ) for $k=2n-1$ . We seek to identify the poles $z_1,\ldots,z_n$ by Prony's method . The key observation is a sequence of the form ( $\star$ ) is a solution to an order- $n$ difference equation: $$
h_{n+k} = c_{n-1} h_{k+(n-1)} + \cdots + c_0 h_k.
$$ We find the difference equation coefficients $c_0,\ldots,c_{n-1}$ by solving the following linear system of equations $$
\begin{bmatrix}
h_n \\ h_{n+1} \\ \vdots \\ h_{2n-1}
\end{bmatrix} = \begin{bmatrix} h_0 & h_1 & \cdots & h_{n-1} \\
h_1 & h_2 & \cdots & h_{n+1} \\
\vdots & \vdots &\ddots & \vdots \\
h_{n-1} & h_n & \cdots & h_{2n-2} \end{bmatrix} \begin{bmatrix} c_0 \\ c_1 \\ \vdots \\ c_{n-1}\end{bmatrix}.
$$ The matrix in this equation is precisely the matrix $H$ which we know to be nonzero. Thus, this equation has a unique solution $c_0,\ldots,c_{n-1}$ . Provided the characteristic equation $$
z^n - c_{n-1}z^{n-1} - \cdots - c_0z^0 = 0 \quad \text{has distinct roots} \quad z_1,\ldots,z_n, \tag{$\dagger$}
$$ there exist coefficients $\alpha_1,\ldots,\alpha_n$ such that ( $\star$ ) holds. The Vandermonde decomposition is proven under the assumption ( $\dagger$ ). The Vandermonde decomposition thus rests on choosing $h_{2n-1}$ such that ( $\dagger$ ) holds. Since $h_{2n-1}$ is arbitrary, the polynomial in ( $\dagger$ ) can be chosen to be any polynomial of the form $$
f(z) = g(z) + \alpha h(z)
$$ where $$
g(z) = z^n - d_{n-1} z^{n-1} - \cdots - d_0 z^0, \quad h(z) = -e_{n-1} z^{n-1} - \cdots - e_0 z^0 \tag{$\blacktriangle$}
$$ with $d_0,\ldots,d_{n-1}$ and $e_0,\ldots,e_{n-1}$ the solutions of $$
\begin{bmatrix}
h_n \\ h_{n+1} \\ \vdots \\ 0
\end{bmatrix} = H \begin{bmatrix} d_0 \\ d_1 \\ \vdots \\ d_{n-1}\end{bmatrix}, \quad \begin{bmatrix}
0 \\ 0 \\ \vdots \\ h_{2n-1}
\end{bmatrix} = H \begin{bmatrix} e_0 \\ e_1 \\ \vdots \\ e_{n-1}\end{bmatrix}.
$$ To this end, we have a helpful lemma (see, e.g., Lemma 7.7 ): Lemma 2. If any polynomials $g$ and $h$ are coprime , then $g - \alpha h$ has simple roots except for finitely many values $\alpha$ . Thus, Theorem 1 would be proven if I could show that $g$ and $h$ as defined in ( $\blacktriangle$ ) are coprime.","['hankel-matrices', 'linear-algebra', 'control-theory', 'reference-request']"
4449814,Why is this proof wrong? ( I proved the irrationality of $\pi$ using the most basic techniques so I suspect that it must have gone wrong somewhere),"So it starts off by way of contradiction, supposing $\pi\in \mathbb Q$ , then by De-Moivre's theorem for rational powers: $$\left((\cos(2k\pi)+i\sin(2k\pi)\right)^{\pi}= \cos(2k\pi^2)+i\sin(2k\pi^2)=1^{\pi}$$ Here $k\in \mathbb Z$ . Then you must have $$\cos(2m\pi^2)+i\sin(2m\pi^2)=
\cos(2p\pi^2)+i\sin(2p\pi^2)$$ for all $m,p$ that are integers. However equating real parts gives $$cos(2m\pi^2)=\cos(2p\pi^2)$$ , but this implies that $$2m\pi^2=2t\pi-2p\pi^2\quad  \text {or} \quad 2m\pi^2=2t\pi+2p\pi^2$$ (for some integer $t$ ) Which gives that $t= (m-p)\pi$ or $t= (m+p)\pi$ ; but choosing $m$ and $p$ appropriately such that for $\pi= \frac ab$ (where $a,b$ are coprime) you have $b$ doesn't divide $m\pm p$ , which gives $t$ is not an integer contrary to assumption.
The only remaining case is when $b=1$ which is trivially false as $3<\pi<4$ can be easily proven.","['complex-analysis', 'irrational-numbers', 'pi']"
4449888,Kneser theorem about the Klein bottle,"I know that in $1923$ H. Kneser showed that a continuous flow in a Klein bottle without singular points has a periodic trajectory. The original article is this , but does anyone know another old or new proof of this result? I would really like to read this result, I tried to do it from your original article but the language is too complicated for me. I searched on the internet but found almost nothing about the proof.","['ordinary-differential-equations', 'vector-fields', 'klein-bottle', 'smooth-manifolds', 'manifolds']"
4449893,Does $e^X=\lim_{n \to \infty}\left(\text{Id}+\frac{X}{n}\right)^n$ hold for matrices?,"Let $X$ be a $d \times d$ real matrix, $d>1$ . Is it true that $$
e^X=\lim_{n \to \infty}\left(\text{Id}+\frac{X}{n}\right)^n\,\,\,?
$$ Edit: It seems that this question is a duplicate . To make it more interesting then, I am asking the following: Can the density argument via diagonalizable matrices be made rigorous? Write $g(X)=e^X, f(X)=\lim_{n \to \infty}\left(\text{Id}+\frac{X}{n}\right)^n
$ . Then $f$ and $g$ agree on diagonalizable matrices (via the classical real-valued case). To deduce they coincide on all matrices, though, requires proving both functions are continuous . How do you prove that $f$ is continuous? (I guess one might try to prove uniform convergence?)","['multivariable-calculus', 'matrix-calculus', 'exponential-function', 'matrix-exponential']"
4449898,Some positive Maclaurin series related to $\cos$ and $\sin$ functions,"Consider the known inequality $$1 - \cos x \ge 0$$ for all $x\in \mathbb{R}$ . This is because the expression equals $2 \sin^2 \frac{x}{2}\ge 0$ . Using the power series expression for $\cos x$ , the inequality is equivalent to $$\frac{x^2}{2} - \frac{x^4}{4!} + \frac{x^6}{6!} -\cdots = \sum_{n\ge 1} (-1)^{n-1} \frac{x^{2n}}{(2n)!}\ge 0$$ for all $x \in \mathbb{R}$ with equality if and only if $x=2k \pi$ , $k\in \mathbb{Z}$ . Integrating from $0$ to a limit $x$ we get more inequalities $$\frac{x^3}{3!} - \frac{x^5}{5!} + \frac{x^7}{7!} - \cdots$$ $$\frac{x^4}{4!} - \frac{x^6}{6!} + \frac{x^8}{8!} - \cdots$$ $$\ldots $$ the ones with odd exponents valid for $x\ge 0$ , the other for all $x \in \mathbb{R}$ . Moreover, we have equalities only for $x = 0$ . We can rewrite the inequalities as follow: $$1 \ge \cos x$$ $$x \ge \sin x$$ $$1 - \frac{x^2}{2} \le \cos x$$ $$x - \frac{x^3}{3!} \le \sin x$$ $$\ldots $$ Now, the interesting thing is the following: take one of the inequalities above and divide the smaller by the larger term. We get a series of form $1 - $ a positive series. The only exception is for $\frac{\cos x}{1}$ and $\frac{\sin x}{x}$ , where the inverse is still positive ( well known). An example $$\frac{1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!}}{\cos x} = 1 - \frac{x^8}{40320} - \frac{11 x^{10}}{907200} - \frac{241 x^{12}}{47900160} - \frac{44567 x^{14}}{21794572800} - \frac{17345443 x^{16}}{20922789888000}-\cdots
$$ So this inequality for small enough $|x|$ implies $1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!}\le \cos x$ , that we already knew. I wonder if there would be a proof for the above statements.  Thank you for your interest!","['taylor-expansion', 'calculus', 'trigonometry']"
4449919,Why do greedy coloring algorithms mess up?,"It is a well-known fact that, for a graph, the greedy coloring algorithm does not always return the most optimal coloring. That is, it strongly depends on the ordering of the vertices as they are colored. I was trying to understand what exactly about a particular vertex ordering makes the GCA mess up. Please feel free to share your intuitions - that is more of what I am seeking, rather than rigorous details. For example , why is it (usually) better to place higher-degree vertices earlier in the ordering?","['coloring', 'graph-theory', 'combinatorics', 'algorithms', 'computer-science']"
4449934,"Prove that $\lim\limits_{(x,y) \to (0,0)} \frac{x^2}{\sqrt{x^2 +y^2}} = 0$ without substituting","I found this possible solution: Let $r^2=x^2 + y^2, x = r \cos(\theta)$ and $y=r\sin(\theta)$ . Then: $$
\begin{split}
\lim_{(x,y) \to (0,0)} \frac{x^2}{\sqrt{x^2 +y^2}}
 &= \lim_{r \to 0} \frac{(r \cos(\theta))^2}{\sqrt{r^2}} \\
 &= \lim_{r \to 0} \frac{r^2 \cos^2(\theta)}{r} \\
 &= \lim_{r \to 0} r \cos^2(\theta)
\end{split}$$ Since $\lvert \cos^2(\theta)\rvert \leq 1$ and $\lim\limits_{r \to 0} r = 0$ , we have: $$
\lim_{r \to 0} r \cos^2(\theta)
 = 0
 = \lim_{(x,y) \to (0,0)} \frac{x^2}{\sqrt{x^2 +y^2}}
$$ I learned this substitution in this class . First of all I would like to know if my solution is possible, but I would also would like to see a prove that does not use this kind of substitution.","['multivariable-calculus', 'limits', 'calculus', 'polar-coordinates']"
4449987,Brownian motion with a stopping time,"Let $x \geq 0,c<0,$ and a Brownian motion $(W_u)_{u}.$ Let $T:=\inf\{u \geq 0, B_u +cu\geq x\}.$ It follows that $Y:=\sup_{u \geq 0}(B_u+cu) \in ]0,\infty[.$ We want to verify that $\{Y \geq x\} \subset (T<\infty).$ Supposing that $Y(w) > x$ then the result follows. But what if $Y(w)=x,$ I can't see how to deduce it ? Do we need to use the continuity of the BM?","['real-analysis', 'stochastic-processes', 'stopping-times', 'brownian-motion', 'probability-theory']"
4450008,Multiclass Classification: Why do we exponentiate the softmax function?,"In the context of neural networks, we use the softmax output in multiclassification models. Firstly, let $P(y) = \sigma (z(2y-1))$ , which comes from the definition of sigmoid units. We define $\bf z=\bf W^\intercal \bf h+\bf b$ , a linear layer predicting unnormalized log probabilities with $z_i = -\log P(y=i|x)$ . The softmax of $z_i$ would result in $$\text{softmax}(\textbf{z})_i = \frac{\exp(z_i)}{\sum_j\exp(z_j)}=\frac{e^{\log P(y=i|x)}}{\sum_j e^{\log P(y=j|x)}}$$ I understand the general idea behind the softmax output unit. For example, in a feedforward neural network, our data $\mathbb{X}$ would be linearly transformed in the first layer computing $z=\bf w^\intercal h+b$ , to then pass to the output softmax layer. This output layer would have $k$ units, one for each class, and would compute the log probability $z_i$ of each of them to evaluate in the softmax function. I know $a)$ $\forall z_i \space |\space \exp^{z_i} \in [1, e]$ . In other words, the numerator of the softmax function will be within $1$ and $e$ , and the denominator will be a sum of terms all between $1$ and $e$ . Naturally, the probability of each $z_i$ will be normalized (they will add up to one). If $z_i > z_j, \text{softmax}(z_i) > \text{softmax}(z_j)$ . Therefore, if class $i$ has a greater dichotomized probability ( $i$ versus all the rest of the clases) than class $j$ , the softmax will preserve this relationship. What I don't understand is why do we need to exponentiate at all. Both of the previous conditions (normalization, preserved order) would still hold if the softmax function was only of the form $\frac{(z_i)}{\sum_j z_j}$ . Now, my intuition tells me it has something to do with the fact that the cost function of the softmax is, as usual, a negative log probability. Probably the exponential makes calculations in the cost function easier? Thanks in advance.","['exponentiation', 'neural-networks', 'machine-learning', 'exponential-function', 'probability']"
4450043,Derivative of diagonal matrix expression: $f(X)=\text{diag}(X)M^T\text{diag}^{-1}(MX)$,"Let X be a vector $\mathbb{R}^{n\times1}$ and M be a constant matrix $\mathbb{R}^{n\times n}$ , and given the function $f(X)$ how could i find the derivative of $f(X)$ with respect to $X$ ? In the expression diag( $X$ ) represents the diagonal matrix of $X$ . $$f(X)=\text{diag}(X)M^T\text{diag}^{-1}(MX)$$ Thank you in advance.","['matrices', 'calculus', 'matrix-calculus', 'matrix-equations', 'derivatives']"
4450058,Maximal unramified at p extension of $\mathbb{Q}$,"Let $R=\mathbb{Z}_{(p)}$ . What are finite etale algebras over $R$ ? Let $R \to A$ be a finite etale map where $Spec A$ is connected. Then $A$ is the normalisation of $R$ in $Frac(A)$ . Therefore, they all look like $\mathcal{O}_{K}$ for some number field $K$ . However, there are restrictions. Namely, the extension $K/\mathbb{Q}$ should be unramified at $p$ . Thus, I think that $\pi_{1}^{et} (R)$ is the Galois group of the maximal unramified at $p$ extension of $\mathbb{Q}$ . However, I haven't found any mentions of this object in google so maybe I'm wrong. Am I wrong? If not, is it related to the maximal unramified extension of $\mathbb{Q}_p$ in some sense? In particular, is $\pi_{1}^{et} (R) = \hat{\mathbb{Z}}?$","['number-theory', 'algebraic-geometry', 'commutative-algebra']"
4450066,Equivalence of the definition of differentials,"I have two definitions of a differential of a map $f: \mathbb{R}^n \rightarrow \mathbb{R}$ at $x$ denoted as $df_x$ . $df_x(h) = \sum_{i = 1}^n \frac{\partial f (x)}{\partial x_i}h_i$ $df_x(h) = \lim_{t \rightarrow 0} \frac{f(x + ht) - f(x)}{t}$ How do I go about showing this equivalence? Intuitively the equivalence makes sense. If we take $n = 2$ , the partials in each direction give a sense of how much $f$ changes in each respective dimension, so (1) makes sense. I can also see how (2) gives us the differential if we think of the change of a surface in the direction of $h$ . I started with (2) and began by saying $$df_x(h) + E(t) = \frac{f(x + ht) - f(x)}{t}$$ such that $\lim_{t \rightarrow 0} E(t) = 0$ . This gives us $f(x + h) = f(x) + df_x(h)t + o(t)$ . I then tried to show that $f(x + h) = f(x) + (\sum_{i = 1}^n \frac{\partial f(x)} + {\partial x_i}h_i) + o(t)$ but I'm not sure if this helps and I am stuck here.","['derivatives', 'differential-geometry']"
4450081,Limits involving exponents,"I don't understand this statement from Wolfram Alpha: Since $5^{2k+1}$ grows asymptotically slower than $3^{4k+1}$ as $k$ approaches $\infty$ , $$\lim_{k\to\infty} 3^{-4k-1}\cdot 5^{2k+1} = 0.$$ Doesn't $5^x$ increase faster than $3^x$ ? Does it have something to do with $$3^{-4k-1}\cdot 5^{2k+1}=\frac{5^{2k+1}}{3^{4k+1}}=\left(\frac{5}{3}\right)^{2k+1}\cdot\left(\frac{1}{3}\right)^{2k} \quad ?$$","['limits', 'calculus', 'limits-without-lhopital']"
4450121,Criteria for Hausdorff,"Let $f:X\to Y$ , $Y$ is Hausdorff, and $f$ is continuous. How to prove that $f$ is injective if $X$ is Hausdorff? It is easy enough to show that $f$ injective implies $X$ Hausdorff, and I have been able to find examples where, if $f$ is not injective, $X$ is not Hausdorff. However, is it possible to prove the above statement in general? I cannot seem to work it out from the definitions.","['continuity', 'general-topology', 'separation-axioms']"
4450122,what makes some line to be a tangent line on to other Graph.,"I learned tangent line as a concept associated with a circle. I thought to make a tangent line, it might have some specific point which is perpendicular to. as always did with circle did. But in the lesson of calculus, it doesn't seems any another point to associate. To draw tangent lines of some graph, how could we refer the slope of tangent line? Before we calculate? I mean what makes some line to be a tangent line on the graph.
How could be a tangent line with out any other point?","['geometry', 'calculus', 'plane-geometry', 'trigonometry', 'terminology']"
4450137,Changing Lebesgue Integral to Riemann Integral,"I recently ran across this problem regarding the change from a Lebesgue integral to a Riemann integral. Suppose $g$ is a continuous function from $[0, \infty)$ to $[0, \infty)$ . with $g(0) = 0$ and the derivative function $g'$ is continuous in $(0, \infty)$ . Suppose $f$ is non-negative and measurable on E. How do I show $ \int_E g \circ f \,d\mu  =  \int_{0}^{\infty} \mu(E_t)g'(t) \,dt ?$ where $E_t$ is the set $\{x \in E : f(x) > t\}$ . Any help will be really appreciated, thank you.","['integration', 'measure-theory']"
4450148,"What is a ""compactness argument""?","I'm working on this paper and I don't know what is meant by compactness argument in the proof of corrollary 4 page 226 which said that: the function $\lambda \to \|B-\lambda A\|$ (where A and B are in L(H) ) is continuous
with $$\lim_{|\lambda|\to\infty}\|B-\lambda A\|=\infty$$ so by  compactness argument , there exists $z_0\in \mathbb{C}$ s.t $$ \|B-z_0 A\|\leq \|(B-z_0 A)+\lambda A\|,\forall \lambda \in \mathbb{C}$$","['general-topology', 'operator-theory', 'functional-analysis', 'compactness']"
4450199,$\sum_{k=0}^\infty\frac{1}{k+1}\binom{3k+1}{k}\left(\frac{1}{2}\right)^{3k+2}$ converges to $\frac{3-\sqrt{5}}{2}$?,"I stumble upon the expression $$
\sum_{k=0}^\infty 
\frac{1}{k+1}
\binom{3k+1}{k}
\left( \frac{1}{2} \right)^{3k+2}
$$ and it seems to converge to $$
\frac{3-\sqrt{5}}{2}
$$ Do they equate ? How to prove that ? Not sure if it's helpful : $\frac{1}{k+1}\binom{3k+1}{k} , \; k\ge0$ is the OEIS sequence $A006013$ . I only have fundamental knowledge in combinatorics so I could only check it numerically that the convergence seems to be true . However I wouldn't mind learning new theories   .","['convergence-divergence', 'sequences-and-series']"
4450227,When is and isn’t a Surjection ‘self evident.’,"studying my first ever abstract maths course (how exciting), and one thing that often comes up is that when proving a surjection…sometimes it’s just given as ‘self evident,’ other times no matter how simple…we have to do some work to get marks. I want to understand where the line is, and get some pointers. ’Self evident’ example For example: The surjective property of a map between a subgroup $H$ and a left Coset $aH$ is seen as self-evident. Map $H \to aH, F(h) = ah$ “Take $x \in aH$ , then $x = ah$ , for some $h \in H$ by definition. So we have $F(h) = x$ thus our map is surjective.” Question : I appreciate this does seem obvious, but can anyone show me any subtle changes that might seem right, but would make this answer actually wrong. I.e. when the answer is so simple, I feel like even the smallest change would throw it off. Non self evident example $$F(x) = x^2 + 1$$ Here we actually have to find the inverse, so that we can show that always, $f(f^{-1}(x))$ gives us our desired $y$ …I.e the ‘ $x$ ’ that always exists such that $f(x) = y$ is $x = (y - 1)^{1/2}$ Then: $f(x) = f\left((y - 1)^{1/2}\right) = y$ [ Edit: Has been kindly noted that this isn't even a valid surjection over the real numbers (which I feel like kind of speaks to my question)...] $x^3 + 1$ should work... Question this error speaks to another problem I have. Is there any systematic way of knowing when that 'taking the inverse' method will fail...apart from just spotting an error where the domain and range are not equal? Question Can anybody suggest some maps where I might think oh this is ‘just self evident’ but actually I need to do some work? Question In a way I find the ‘self evident’ ones harder, because you kind of don’t need to do anything, but you do need to do something…can anyone give me an exact structure of what I need to say and why it works , and as suggested before, what things not to say (that might seem right) but wouldn’t get credit, or are simply wrong! Thanks for helping with all the questions, hope this isn’t too basic!","['functions', 'abstract-algebra']"
4450247,Let $f_n$ be a$f_{n+1}\left(x\right)=\frac{1}{x}\int _0^xf_n\left(t\right)dt\:$,"Let be $f_n $ be a sequence of functions and $f_0$ an continuous arbitrary function derivable in $0$ such that: $$f_{n+1}\left(x\right)=\frac{1}{x}\int _0^xf_n\left(t\right)dt$$ for every $n$ positive integer. The domain of $f_n$ is $[0,1]$ I was wondering if the following statement is true or not: $f'_n $ is uniformly convergent to the function: $g(x)=0$ for every $x \in [0,1]$ I was thinking that I need to prove that: $$sup_{x\in [0,1]}|f'_n(x)-f(x)|=0$$ that is to say for every $\epsilon>0$ there is a positive integer $N$ such that for every $n>N$ and ${x\in [0,1]}$ we have: $$|f'_n(x)-f(x)|<\epsilon$$ How should I proceed?",['functional-analysis']
4450266,Black-Scholes model with a derivative with payoff $S_{T}^{3}$,"Given a Black-Scholes Model and a derivative with payoff $S_{T}^{3}$ at time $T$ . Check that the value of that derivative at time t is $V_{t} = g(t, T)S_{t}^{3}$ , where $g(t, T)$ has to be determined. I know the value of a derivative given its payoff can be written as $V_{t} = e^{-r(T-t)}  E[X|F_{t}]$ , where X is the payoff $S_{T}^{3}$ , but I do not know how to follow from here. Edit: First, define $V_{t} = F(t, S_{t})$ , where $$F(t, S_{t}) = e^{-r(T-t)} E[(f(Se^{(T-t)(r-\sigma^{2}/2) + \sigma \sqrt{T - t} Z}$$ where $Z$ is a $N(0, 1)$ . Then $V_{t} = e^{-r(T-t)} E[(Se^{(T-t)(r-\sigma^{2}/2) + \sigma \sqrt{T - t} Z})^{3}]$ .","['stochastic-processes', 'stochastic-differential-equations', 'finance', 'probability-theory']"
4450319,"Husemoller: homotopy of linear clutching map (proposition $4.5$, pag. $187$)","Background : I'm currently studying vector bundle through the book of [husemoller,""fibre bundles""]
( https://www.maths.ed.ac.uk/~v1ranick/papers/husemoller ). The following question concerns a detail about a quite specific proof, which I'm going to simplify for the sake of comprehension. However, the pages I'm reffering to are $147-148$ , proposition $4.6$ . Let's define a linear clutching map as map of the form $p(x,z) = a(x)z+b(x)$ , invertible for $|z| \in S^1$ . This a continuos family of automorphism of the fibre over $x$ of a vector bundle $\zeta$ over $X$ (compact connected $CW$ , if needed). Let's also take in account $p_0(x)$ and $p_{\infty}(x)$ two linear maps from $\zeta_x \to \zeta_x$ such that $p_0^2 = p_0$ and $p_{\infty}^2 = p_{\infty}$ (which have an explicit form that I'd like to avoid for the moment if possible). Let's recall that with the ""involution"" property, it holds that the vector space, in this case we may assume $\mathbb{C}^n$ is the direct sum of kernel and image in both cases. Proposition: $p(x,z)p_0(x) = p_{\infty}(x)p(x,z)$ . The proposition give rise to two maps $p_{+} : im p_0 \to im p_{\infty}$ and $p_{-} : ker p_0 \to kerp_{\infty}$ , which are the restrictions of $p(x,z)$ . There is also the followinf proposition: Proposition $(4.5)$ : $p_{+} : im p_0 \to im p_{\infty}$ is an isomorphism for $|z| \geq 1$ while $p_{-} : ker p_0 \to kerp_{\infty}$ is an isomorphism for $|z| \leq 1$ . The background ends here. Problem : Now the problem is the following proposition: Proposition: Let $p_{+},p_{-}$ be defined as above. and let $p^t :=
> p_{+}^t+p_{-}^t$ where $p_{+}^t := a_{+}z+tb_{+}$ and $p_{-}^t:=
 ta_{-}z+b_{-}$ for $0 \leq t \leq 1$ . Then there is a homotopy of
linear clutching maps from $a_{+}z+b_{-}$ . Moreover, the bundles $[\zeta,p] \simeq [im p_0,z]\oplus [ker p_0,1]$ The proof given is the following: $\hspace{3.5cm}$ Questions : Why $p_{+}^t,p_{-}^t$ are isomorphism on their images for $0 \leq t\leq 1$ ? I can see an argument of rescaling by $t$ for $p_{-}$ since $|zt| \leq 1$ for $|z| \leq 1$ although I see a problem diving by $t$ in $p_{+}^t$ this rescaling would work for each $t \ne 0$ but in $t=0$ I think I have a problem, or there is a way to prove that $a_{+}$ is invertible apriori? Note that I'm taking $\{\infty\} \in \{|z| \geq 1\}$ so that $\{\infty\} \in \{|z| \geq 1\} \cup \{|z| \leq 1\}$ , perhaps it could help. Why should be true that $a_{+}$ and $b_{-}$ are isomorphism? Why the linear combination i.e the homotopy, which can be re-written as $(1-t)(a_{+}(x)z+b_{-}(x))+tp(x,z)$ is a linear clutching map (i.e invertible) for each $0 \leq t \leq 1?$ For $t = 0$ I don't see how $a_{+}(x)z+b_{-}(x)$ is even defined, since $a_{+}$ and $b_{-}$ are part of the restriction to $a_{+}$ and $b_{-}$ respectively, should take vector in the intersection of the domain, but $ker p_0 \cap im p_0 = \{0\} $ since they are in direct sum. Any help or reference in order to clarify this details would be appreciated, thanks in advance.","['proof-explanation', 'matrices', 'vector-bundles', 'linear-algebra', 'algebraic-topology']"
4450359,When the presheaf inverse image of a sheaf is already a sheaf,"In Milne's book Etale cohomology, p93, he defines $0$ -th cohomology with compact support of a separated variety $X$ (in this book a variety is a geometrically irreducible, geometrically reduce scheme of finite type over a field) to be \begin{equation}
\Gamma_c(X,F) = \bigcup \operatorname{ker}(\Gamma(X,F) \to \Gamma(X-Z,F))
\end{equation} where $Z$ run over the complete subvarieties of $X$ .
Take a compactification $j\colon X \to \bar X$ , then he claims (and we want) \begin{equation}
\Gamma_c(X,F) = \Gamma(\bar X,j_!F).
\end{equation} The proof goes as follows: From the exact sequence on $\bar X$ \begin{equation}
0 \to j_!F \to j_*F \to i_*i^*j_*F \to 0,
\end{equation} where $i\colon \bar X - X \hookrightarrow \bar X$ , we see that \begin{equation}
\Gamma(X,j_!F) = \operatorname{ker}(\Gamma(X,F) \to \Gamma(\bar X - X,i^*j_*F)).
\end{equation} The following formula in the book confuses me: \begin{equation}
\Gamma(\bar X - X,i^*j_*F)) = \operatorname{colim}\Gamma(V\times_{\bar X} X,F)
\end{equation} where $V\to \bar X$ is etale and contains $\bar X - X$ in its image. But chasing of definition shows \begin{equation}
\operatorname{colim}\Gamma(V\times_{\bar X} X,F) = i^pj_*F(\bar X - X)
\end{equation} where $i^p$ denotes the presheaf inverse image. Then it comes to my question in title: is it true that in the case $i\colon X_1 \to X$ being a closed immersion, for a sheaf $F$ on $X_{et}$ , $i^pF$ is already a sheaf? If not, then how to fix the proof?","['etale-cohomology', 'algebraic-geometry']"
4450438,Why do we care whether the support of a function is compact or not?,"This is a question for self-learning. I am too confused by the text to formulate a well-defined question now. I am reading analysis of functions, and confused by the motivations of some theorems and definitions. For example, we want to prove the following result: where $\phi$ is any function $\in C^0(\Omega)$ , and Why do we care whether the support of a function is compact or not? (My thought: since the support is a closure, so it must be closed; then as long as the support is bounded, i.e. the function vanishes as $x\to\infty$ , the support is compact.) Why do we define a function $\tau_x\phi$ with $y$ as the independent variable? (My thought: both $x, y \in \Omega$ , so here we define a function with arbitrarily chosen $x$ as a parameter, and maps a point in $\phi$ 's domain $\Omega$ , to the image of $\phi$ at the difference (distance) of the point from the parameter (reference point) $x$ . $\quad$ The corollary basically says that a translated $C^k_c(\Omega)$ function is still $C^k_c(\Omega)$ , at least if the translation $x$ is near $0$ . In other words, a small perturbation does not change the property of the function. $\quad$ Nevertheless, it seems to me that any finite translation $x < \infty$ should not change the compactness of support and $k$ -th differentiability, right? )","['self-learning', 'functional-analysis', 'real-analysis']"
4450443,"$c_n=a^n-b^n$, $S=\{c_1,c_2,...\}$ is finite then $0 \in S$","Statement: Let $  (R,+,\cdot)  $ be a ring that has at least two invertible
elements. Let $a,b$ be two invertible elements such that $$ord(a)=ord(b)=+\infty$$ Let consider the following sequence $$c_n=a^n-b^n$$ Let $S=\{c_1,c_2,...,c_m,...\}$ a set of elements
from $R$ . Prove that if $S=\{c_1,c_2,...\}$ is finite then $$0 \in S$$ Attempt: I obtained that: $$a^n-b^n=c_n$$ then $$a^{n+1}-ab^n=ac_n$$ and $$a^{n+1}-b^{n+1}=c_{n+1}$$ then $$(b-a)b^{n}=ac_n-c_{n+1}$$ Since $S$ is finite then $$M=\{ac_n-c_{n+1},n \in \mathbb Z_+\}$$ is finite. So $$\{(b-a)b^n,n \in \mathbb Z_+\}$$ is finite.
Then there exists $i$ and $j$ such that $$(b-a)b^i=(b-a)b^j$$ How should I continue with this. I would highly appreciate somebody's help.","['ring-theory', 'abstract-algebra']"
4450449,"(convex function) Let $f: [a,b] \to \mathbb R$ twice differentiable, and $f''(x) \ge 0$, $\forall x \in [a,b]$....","Let $f: [a,b] \to \mathbb R$ twice differentiable, and $f''(x) \ge 0$ , $\forall x \in [a,b]$ .Prove that $f(\frac{x_1 + x_2} {2}) \le \frac{1}{2}[f(x_1) + f(x_2)], \forall x_1, x_2 \in [a,b].$ My attempt: As a hypothesis we assume that $f''(x) \ge 0$ and we know that .If $f: (a,b) \to \mathbb R$ has second order derivatives on $(a, b)$ the function $f$ is convex if and only if $f''(x) \ge 0$ . If $f’’(x) \gt 0$ , the function $f$ is strictly convex. So $f$ is convex. And by the definition of convex, A function $f: (a,b) \to \mathbb R$ is calling convex on (a,b) if for any $x_1, x_2 \in (a,b)$ and for any pair of real numbers $\alpha_1 \ge 0, \alpha_2 \ge 0$ such that $\alpha_1 + \alpha_2 = 1$ the following inequality $f(\alpha_1x_1 + \alpha_2x_2) \le \alpha_1f(x_1) + \alpha_2f( x_2)$ . So we have: $f(\frac{x_1 + x_2}{2}) \le \frac{1}{2}[f(x_1) + f(x_2)] = f(\frac{1}{2}[x_1]+\frac{1}{2}[x_2])  \le \frac{1}{2}f(x_1) + \frac{1}{2}f(x_2) \to f(\alpha_1x_1 + \alpha_2x_2) \le \alpha_1f(x_1) + \alpha_2f(x_2)$ How is my answer? Thank's for any help","['solution-verification', 'derivatives', 'problem-solving', 'real-analysis']"
4450489,Which rings are rings of continuous functions?,"Now at MO . This is a question for which I've found a number of ""near-miss"" results online, which may actually be answers but whose direct relevance I haven't been able to see. Say that a ring $A$ is spatial iff there is some topological space $\mathcal{X}$ such that $A\cong C(\mathcal{X})$ , where $C(\mathcal{X})$ is the ring of continuous functions $\mathcal{X}\rightarrow\mathbb{R}$ . Is there a purely algebraic characterization of spatiality? I've been told that Gelfand representations are relevant here, but I don't immediately see how they answer the question; maybe I'm missing something, though. (Note however that I do mean to ask about rings, rather than more intricate structures like Banach algebras. Also note that I'm not assuming any tameness properties on the spaces which are candidates for witnessing spatiality.) EDIT: ""purely algebraic characterization"" is of course some serious weasel-wordery. Here's one way to make that precise (and so make possible a rigorous negative answer): Is there an $\mathcal{L}^2_{\infty,\infty}$ -sentence characterizing the spatial rings? Here $\mathcal{L}^2_{\infty,\infty}$ is the fully-infinitary version of second-order logic: we allow arbitrary-cardinality Boolean combinations and quantifications (over both first- and second-order objects). Of course, any specific $\mathcal{L}^2_{\infty,\infty}$ -sentence can only ""reach up"" to a particular cardinal, so this isn't actually as overkill as it may appear.","['logic', 'ring-theory', 'abstract-algebra', 'functional-analysis', 'general-topology']"
4450496,Method-of-moments estimator for a uniform distribution,"I have a sample of data points independently sampled from a uniform distribution with a density function $f(x)=\frac{1}{a}, 0\leq x \leq a$ . I need to use the method of moments to estimate $a_{mom}$ . My idea is to just calculate the variance, then multiply by 12 and take the square root so I get $b-a$ . Then I'm kind of stumped what to do further.","['statistics', 'estimation']"
4450509,Calculate: $\Delta=\left|\begin{array}{ccc} b c & c a & a b \\ a(b+c) & b(c+a) & c(a+b) \\ a^{2} & b^{2} & c^{2} \end{array}\right|$,"Calculate: $$\Delta=\left|\begin{array}{ccc}
b c & c a & a b \\
a(b+c) & b(c+a) & c(a+b) \\
a^{2} & b^{2} & c^{2}
\end{array}\right|$$ Does anyone know any easy way to calculate this determinant? I tried the classic way but I was wondering if anyone knows an easier method.","['matrices', 'determinant']"
4450519,Best betting strategy for an unfair random walk with a skewed payoff,"Say you start with bankroll $B$ and i.i.d. random variables $U_i$ with distribution $p=P(U_i=r)>.5$ and $q=1-p=P(U_i=-1)$ .  Your earnings from bet $i$ is $W_iU_i$ , where $W_i$ is your wager at step $i$ .  Notice that we think of $U_i$ as the payout for bet $i$ .  It follows then that our bankroll at step $n$ is $B_n=B+\sum_{i=1}^{n}W_iU_i$ .  We will also add the constraint that $W_{n+1} \leq B_{n}$ so that we cannot bet more than what we have made.  Suppose also that the expected value for $U_i$ is positive so that it makes sense to play the game in the first place.  Is there a known ""optimal"" choice for $W_i$ that balances both risk and reward?  I thought about trying to maximize the expected value of the $B_n$ but this didn't seem to help since all we get is $E(B_N)=B+E(U_1) \sum_{i=1}^N W_i$ , which suggests we need to bet our entire bankroll each turn to maximize earnings.  This is obviously not a winning strategy so we most likely also need to minimize the probability $Q_N$ that $B_n =0$ for time $n\leq N$ . This is where I'm stuck.  The bet size $W_i $ will almost surely depend on $B_{n-1}$ and the expected value of the $U_i$ . I thought of doing this in the context of ergodic theory as well.  After all we are basically dealing with an ergodic markov shift $(X,T)$ and here the $U_i$ are equal to $U_0(T^nx)$ . I would be happy with a solution that does not give an exact value for the $W_i$ but instead a method that is able to approximate it well.","['statistics', 'conditional-probability', 'probability-theory', 'probability', 'dynamical-systems']"
4450528,Measurability of integral with respect to a parameter-dependent measure,"Let $( \mathbb{P}_t )_{ t \geq 0 }$ be a family of probability measures on the measurable space $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ such that $$
\int_{\mathbb{R}} x^2 \mathbb{P}_t (dx) < \infty \quad \forall t \geq 0,
$$ and assume that the mapping $$
[0, \infty) \ni t \mapsto \mathbb{P}_t
$$ is continuous with respect to the $2$ nd Wasserstein metric denoted by $W_2(\cdot, \cdot)$ . Now let $f : \mathbb{R} \times [0, \infty) \rightarrow [0, \infty]$ be a measurable function. Is it true that the mapping $$
[0, \infty) \ni t \mapsto \int_{\mathbb{R}} f(x,t) \mathbb{P}_t(dx) \tag{1}
$$ is measurable, i.e., $\mathcal{B}([0, \infty))-\mathcal{B}([0, \infty])$ -measurable? Clearly, Tonelli's theorem would imply that for every $s \in [0, \infty)$ the mapping $$
[0, \infty) \ni t \mapsto \int_{\mathbb{R}} f(x,t) \mathbb{P}_{\color{red}s}(dx) \tag{1}
$$ satisfies the aforementioned measurability. But what can be said about the case above? Does one need additional assumptions on the function $f$ ? Is it maybe possible to show that the mapping in $(1)$ is continuous? Some further thoughts: Assume that the stochastic process $(X_t)_{t \geq 0}$ has (right-)continuous sample paths and the law of $X_t$ is given by $\mathbb{P}_t$ , $t \geq 0$ . The mapping $$
[0, \infty) \times \Omega \ni (t, \omega) \mapsto X_t (\omega)
$$ is jointly measurable. We can further observe that the mapping $$
[0, \infty) \times \Omega \ni (t, \omega) \mapsto ( X_t (\omega), t)
$$ is also jointly measurable (since every component is jointly measurable). It then follows that the composition $f(X_t(\omega), t)$ is jointly measurable. We can then apply Tonelli's theorem to conclude that $$
[0, \infty) \ni t \mapsto \mathbb{E}[f(X_t, t)] = \int_{\mathbb{R}} f(x,t) \mathbb{P}_t(dx)
$$ is measurable. So the question is whether such a stochastic process always exists. Could the continuity in the Wasserstein metric (which also implies weak convergence) be helpful?","['probability-theory', 'functional-analysis', 'measure-theory', 'real-analysis']"
4450573,Solution to a linear Backward SDE,"In my last question , Jose and I discussed about the solution to a linear backward SDE. I followed his steps and made it clear. Besides, I read a paper from Professor Peng talk about the linear Backward SDE. In that paper, Professor Peng gave the result of the linear backward SDE: for $g(t,Y_t,Z_t) = a_tY_t + b_tZ_t + c_t$ is a linear function, the solution of $Y_t$ can be write as the following form: $$Y_t = E[X_T\xi + \int _t^Tc_sX_sds|\mathscr{F}_t]$$ Where $X_t$ is the solution of the following SDE: $$dX_s=a_sX_sds + b_sX_sdB_s$$ Where $s\in[t,T]$ and $X_t = 1$ Here comes the problem: how can we get the form of the solution of $Y_t$ ? What I know is: $$X_tY_t = E[X_T\xi-\int_t^T X_sc_sds|\mathscr{F}_t]$$ But how to transform it into the solution of $Y_t$ ?","['stochastic-integrals', 'stochastic-analysis', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4450587,Is there a unified description of the geometric derivative?,"The exterior derivative ( $d$ or $\nabla\wedge$ ) is a very unifying concept, in that it subsumes the gradient of a scalar, the curl of a vector, and the ""divergence"" of a bivector, thus also lumping together the fundamental theorem of line integrals, Stokes' Theorem, and the divergence theorem. The interpretation of the exterior derivative might be ""the extent to which the field aligns with the boundary of an infinitesimal region"". The geometric derivative seems like it should offer further unification, because, at least for vectors, it combines the interior and exterior derivatives into one.  But the question is, how can I describe the meaning of the geometric derivative without resorting to describing each component separately? On the one hand, I'm under the impression this is kind of a common issue with geometric algebra: yes, it allows you to combine objects of different degrees, yet sometimes it appears impossible to conceive of the sum as an invariant whole. But not always.  For example, you can make a ""rotor"" by adding a scalar and a bivector.  Of course, the rotor is a transformation rather than a standalone object, but still it is a meaningful interpretation.  In 3D, this is simply an element of the even sub-algebra. Meanwhile, an even element of the 4D Clifford algebra can be construed as a transformation on bivectors, since it includes a duality rotation in addition to the spatial rotation. So, after taking the geometric derivative of a field of degree $k$ (or perhaps mixed degree), can we articulate what it is we have?  Is it some kind of transformation?  Or does it represent the way that the field varies in a generalized sense?  If so, what is that sense?","['exterior-derivative', 'vector-spaces', 'geometric-algebras', 'derivatives', 'clifford-algebras']"
4450636,Distribution of a random variable exercise,"I was reading section 1.1 of Terence Tao's book on random matrices, which is a review of probability theory, and I couldn't prove the following exercise, which I copied almost verbatim here: Independence. When studying the behaviour of a single
random variable X, the distribution of X captures all the probabilistic
information one wants to know about X. The following exercise is
one way of making this statement rigorous: Exercise 1.1.10. Let $X$ , $X'$ be random variables (on sample spaces $\Omega, \Omega'$ respectively) taking values in a measurable space R (e.g. the real numbers equipped with Borel sigma-algebra), such that they have the same distribution.
Show that after extending the spaces $\Omega, \Omega'$ , the two random variables are isomorphic, in the sense that there exists a probability
space isomorphism $\phi: \Omega \to \Omega'$ (i.e. an invertible extension map
whose inverse is also an extension map) such that $X = X' \circ \phi$ . By ""extending"" and ""extension map"" Tao means the following:
we say that one probability space $(\Omega', \mathcal{B'}, P')$ extends another probability space $(\Omega, \mathcal{B}, P)$ if there is a surjective map $\pi: \Omega' \to \Omega$ which is measurable and probability preserving (i.e. $P'(\pi^{-1}(E)) = P(E)$ for every $E \in \mathcal{B}$ ). Every event $E$ in the original probability space
is canonically identified with an event $\pi^{-1}(E)$ of the same probability in the extension. We also think of a random variable X on the original sample space as the ""same"" as the random variable $X \circ \pi$ on the extended sample space. This identification enables us to add in new random variables that are independent of the existing events and random variables. So far, I know that WLOG, I can assume that $\Omega$ equals $\Omega'$ by extending them to the product space $\Omega \times \Omega'$ . Then I am stuck.",['probability-theory']
4450650,Can a inscribed triangle divide a circle into 4 integer parts?,"Question Draw an inscribed triangle on a circle and divide the circle into four parts $A, B, C, D$ . Can the areas of these parts be integers? My attempt Find the area of the arc from the central angle $(\alpha, \beta, \theta)$ $$
\begin{aligned}
S &= π r^2\\
A &= \frac{r^2}{2}(\alpha-\sin\alpha)\\
B &= \frac{r^2}{2}(\beta-\sin\beta)\\
C &= \frac{r^2}{2}(\gamma-\sin\gamma)\\
\end{aligned}
$$ with constraints $$
\begin{aligned}
S &= A+B+C+D\\
2π &= \alpha + \beta + \gamma\\
\end{aligned}
$$ which can expand to $$
π r^2 = \frac{r^2}{2}(\alpha-\sin\alpha) + \frac{r^2}{2}(\beta-\sin\beta)+\frac{r^2}{2}(2π - \alpha - \beta-\sin(2π - \alpha - \beta))+D
$$ and then simplify as $$
D=\frac{r^2}{2}(4 \pi -2 (\alpha +\beta )+\sin (\alpha +\beta )+\sin (\alpha )+\sin (\beta ))
$$ so, for $\pi>\beta \geqslant \alpha > 0$ , $r\in\mathbb{R}_+$ have relation $$
\begin{aligned}
A &= \frac{r^2}{2}(\alpha-\sin\alpha)\\
B &= \frac{r^2}{2}(\beta-\sin\beta)\\
C &= \frac{r^2}{2}(2 \pi - (\alpha +\beta )+\sin(\alpha +\beta)\\
D &=\frac{r^2}{2}(4 \pi -2 (\alpha +\beta )+\sin (\alpha +\beta )+\sin (\alpha )+\sin (\beta ))\\
\end{aligned}
$$ Looks like both have the form $p\pi - q$ , if 4-parts are all rational numbers after removing this common factor, then you can adjust $r$ to make the four parts are integers. I don't know how to find a integer solution or explain no solution.",['geometry']
4450671,Solving $\tan ^{-1}(\frac{1-x}{1+x})=\frac{1}{2}\tan ^{-1}(x)$,"I was solving the following equation, $$\tan ^{-1}\left(\frac{1-x}{1+x}\right)=\frac{1}{2}\tan ^{-1}\left(x\right)$$ But I missed a solution (don't know where's the mistake in my work). Here's my work: $$\tan ^{-1}\left(\frac{1-x}{1+x}\right)=\frac{1}{2}\tan ^{-1}\left(x\right)$$ Putting $x = \tan(\theta)$ $$\begin{align} \tan ^{-1}\left(\frac{1-\tan\theta}{1+\tan\theta}\right)&=\frac{1}{2}\tan ^{-1}\left(\tan\theta\right)\\
\tan ^{-1}\left(\tan\left(\frac\pi 4 - \theta\right)\right)&=\frac{1}{2}\theta\\
\frac\pi 4 - \theta&=\frac{1}{2}\theta\\
\frac\pi 4&=\frac{1}{2}\theta + \theta\\
\frac\pi 4&=\frac{3}{2}\theta\\
\frac\pi 6&=\theta\\
\frac\pi 6&=\tan^{-1}(x)\\
\tan\frac\pi 6&=(x)\\
\frac1{\sqrt{3}}&=x\end{align}$$ I got only one solution, but the answer in my textbook is $\pm\frac{1}{\sqrt{3}}$ . Where is the mistake?","['trigonometry', 'solution-verification']"
4450714,Calculating the gradient of Log-Euclidean distance between SPD matrices on Riemannian manifold,"In the paper Log-Euclidean metrics for fast and simple calculus on diffusion tensors ,
the geodesic distance between SPD matrices $A,B$ is defined as $$d(A,B)=||\log A- \log B||_F,$$ where $F$ is the Frobenius norm. I did not find more information on the other aspects of the Riemannian structure. I am interested in the gradient (on Riemannian manifold) of the distance function $$
f(X) = d(X,A_0).
$$ I am not familiar with geometry other than some basics, $\langle grad f,X\rangle = Xf $ for all smooth vector field $X$ . How can I derive $grad f$ ?","['riemannian-geometry', 'linear-algebra', 'matrix-norms', 'derivatives', 'differential-geometry']"
4450788,Space of Differentiable Functions,"I am writing and I want to define a space of differentiable functions. I am aware of the space $C^1[a,b]$ however this also puts some constraints on the regularity of $f^\prime$ . In particular, $f^\prime\in C^1$ if $f$ is continuous, differentiable, and has continuous derivative. Is there some standard space that is used to denote the space of differentiable functions that doesn't necessarily require $f^\prime$ to be continuous?","['analysis', 'real-analysis', 'functions', 'functional-analysis', 'derivatives']"
4450859,A question about martingale and Brownian motion,"It is well known that the Brownian motion $B=(B_t)_{t\ge 0}$ is a martingale with respect to its natural filtration $\mathscr{F}_t$ and the fixed probability measure $\mathbb{P}$ , i.e. $$\mathbb{E}(B_t|\mathscr{F}_s)=B_s,\quad s\ge t$$ Now we limit the $t$ in the interval $[0,1]$ and enlarge the filtration with $\sigma(B_1)$ being added into each $\mathscr{F}_t$ , $t\in[0,1]$ , i.e. $$\tilde{\mathscr{F}_t}:=\sigma(\mathscr{F}_t \cup \sigma(B_1)) ,\quad t\in[0,1].$$ My question is how to calculte $\mathbb{E}(B_t|\tilde{\mathscr{F}}_s)$ $~$ for $~$ $0\le s\le t <1$ ?
Someone says the result is $\mathbb{E}(B_t|\tilde{\mathscr{F}}_s)=\frac{1-t}{1-s}B_s + \frac{t-s}{1-s} B_1$ but I don't know why and cannot  verify it. I had been stuck by this question for two days long and had no idea. If you know how to solve it , please do not hesitate to help me.
I am looking forward to your answers!","['martingales', 'brownian-motion', 'probability']"
4450879,"Spivak Calculus: $f$ satisfies $f''(x)+f'(x)g(x)-f(x)=0$, for some g. Prove that if 𝑓 is 0 at two points, then 𝑓 is 0 on the interval between them.","The following is a problem from chapter 11, ""Significance of the Derivative"" from Spivak's Calculus Suppose that $f$ satisfies $$f''(x)+f'(x)g(x)-f(x)=0\tag{1}$$ for some function $g$ . Prove that if $f$ is $0$ at two points, then $f$ is $0$ on the interval between them. Hint: Use Theorem 6. Here is the theorem referred to above. Theorem 6: Suppose $f''(a)$ exists. If $f$ has a local minimum at $a$ ,
then $f''(a) \geq 0$ ; if $f$ has a local maximum at $a$ , then $f''(a) \leq 0$ . The solution manual solution is as follows Suppose $f(a)=f(b)=0$ . If $x$ is a local maximum point of $f$ on $[a,b]$ , then $f'(x)=0$ and $f''(x) \leq 0$ ; from the equation $$f''(x)+f'(x)g(x)-f(x)=0$$ we can conclude that $f(x) \leq 0$ .
Similarly, $f$ cannot have a negative local minimum on $(a,b)$ . My question regards the above proof, which seems terse to the point that I don't understand how the desired result ensues from it. Let me go through it in more words and steps. If we assume that $(1)$ is true for all $x$ , then $f$ is (twice) differentiable at all $x$ . The interval $[a,b]$ is closed, and $f$ is continuous on this interval. Therefore any max or min on $[a,b]$ must either be a critical point, one of the endpoints, or points where $f$ isn't differentiable. Suppose there is a local max in $x_1 \in [a,b]$ . Theorem 6 tells us that $f''(x_1)<0$ and $(1)$ tell us that $$f''(x_1)=f(x_1)<0\tag{2}$$ The proof above stops here and seems to imply that whatever $(2)$ means or implies is clear. It is not, however, clear to me. Could we not have a situation such as Though I can't quite finish the reasoning, it seems the situation above also leads to a contradiction. Reasoning 1 Since $x_1$ is a local max, there is an interval around it where $f$ is smaller than $f(x_1)$ . In particular, $\exists x_2, x_2 < x_1 \land f(x_2)<f(x_1)$ . The Intermediate Value Theorem tells us that there is some $x_3 \in [a,x_2]$ such that $f(x_3)=f(x_1)$ . Hence, by Rolle's Theorem, there is some $x_4 \in [x_3, x_1]$ such that $f'(x_4)=0$ . But then $(1)$ tell us that $f''(x_4)=f(x_4) \leq 0$ . So $x_4$ is a local max. I can't quite finish this reasoning, but it seems to imply that there are infinite local maxima in $(a,b)$ , at every local max $f$ is negative, and every critical point at which $f$ is negative is a local max. The reasoning when we assume there is a local min in $(a,b)$ should be analogous. Is the reasoning above on the right track, and if so how do I finish it (ie explicitly conclude that $f=0$ in $[a,b]$ )? Reasoning 2 Since $f''(x_1)<0$ , and $f'(x)<0$ in an interval around $x_1$ , $f$ is decreasing as we move to the left of $x_1$ and decreasing as we move to the left of $x_1$ . But since $f(a)=f(b)=0$ , and $f$ is continuous, it will need to increase at some point between $x_1$ and $a$ , and between $x_1$ and $b$ . But then $f'$ needs to be positive, and hence $f''$ needs to be positive so that $f'$ increases from a negative value to a positive value. But then $f$ is positive at such points by $(1)$ , which contradicts the fact that actually $f$ is negative whenever the sign of $f''$ changes because $f$ is negative and decreasing when that happens. Again, this seems to mean contradiction, which means $f$ can't have a local max at a point where $f<0$ . I can't seem to satisfy myself with the details though. The reasoning doesn't seem rigorous enough.","['proof-explanation', 'calculus', 'derivatives']"
4450935,Centralizer/normalizer of degree $n$ extension embedded in $\mathrm{GL}_n(k)$,"Let $k$ be a finite field, and let $\ell/k$ be the unique degree $n$ extension of $k$ . Then, by choosing a basis for $\ell$ , we can identify $\ell^\times$ with a subgroup of $G=\mathrm{GL}_n(k)$ . Such an embedding is uniquely determined up to conjugacy, so we may ask: Question: What is the centralizer/normalizer of $\ell^\times\subset G$ ? A couple of observations: For each $g\in N_G(\ell^\times)$ , the conjugation action $g-g^{-1}\colon\ell\to\ell$ fixes the subfield $k$ (since $Z(G)=k^\times$ ), so there is a homomorphism $N_G(\ell^\times)/C_G(\ell^\times)\to\mathrm{Gal}(\ell/k)$ . Moreover, this is surjective, since if $\ell=k(\alpha)$ and $\sigma\in\mathrm{Gal}(\ell/k)$ is the generator, then $\alpha,\sigma(\alpha)\in G$ are conjugate (they are clearly conjugate over $\ell$ ). If $g\in C_G(\ell^\times)$ , then $\ell[g]\supset\ell$ is a commutative algebra. Maybe exploring properties of this algebra tells us something?? When $\ell/k$ is quadratic and $k$ has characteristic $>2$ , then $\ell=k(\sqrt a)$ for some $a\in k$ , so $\ell^\times$ can be identified with $$\{uI_2+v\begin{pmatrix}&a\\1&\end{pmatrix}:u,v\in k,(u,v)\ne(0,0)\}.$$ Now, by direct computation, we see that $C_G(\ell^\times)=\ell^\times$ , but $\begin{pmatrix}-1&\\&1\end{pmatrix}\in N_G(\ell^\times)$ corresponds to the generator of the Galois group of $\ell/k$ . In general, I believe $C_G(\ell^\times)=\ell^\times$ , and $N_G(\ell^\times)$ is some $C_n$ -extension, but I cannot prove it.","['matrices', 'group-theory', 'finite-fields', 'finite-groups']"
4450951,Singular solution of differential equation $p^2y+2px-y=0$,"I have the following differential equation before me for which I am required to find singular solution: $p^2y+2px-y=0$ where $p=\dfrac{dy}{dx}$ On solving it, I got the following general solution: $y^2=c^2+2cx$ We get $p$ -discriminant and $c$ -discriminant relation( they turn out to be same in this case) as: $x^2+y^2=0$ This is a point circle centered at origin. Can it be regarded as singular solution? Our general solution is a family of parabolas and $x^2+y^2=0$ does not touch any of them. Does it mean that for this differential equation, there is no singular solution. Please help.","['singular-solution', 'ordinary-differential-equations']"
4450985,Does a differential-geometry-isometry map a (sub)vector space to a (sub)vector space?,"Given an isometric embedding of the tangent bundle $TM$ of a Riemannian manifold with Sasaki metric into $ℝ^N$ with standard metric.
My intuition tells me that the vector spaces $T_pM$ ( $p ∈ M$ ) are mapped to affine sub spaces of $ℝ^N$ but cannot find a rigourous proof. Can you help? Additional thoughts : The existence of the isometric embedding is guaranteed by the Nash embedding theorem.
It is unclear to me though how this embedding looks like (see my other question ). Here isometry does not mean that the distance function is preserved (that would be isometry in the metric space sense) but the Riemannian metric is preserved: $⟨dι_v A, dι_v B⟩_{ℝ^N} = ⟨A, B⟩_{G_s}$ for any $v ∈ TM$ , $A, B ∈ T_vTM$ , $ι$ is the isometric embedding. The Sasaki metric restricted to $T_pM$ is what one would expect: the vertical part of $T_vTM$ ( $v ∈ T_pM$ ) can be identified with $T_pM$ but answer my question one probably has to be very careful how to exploit this identification.",['differential-geometry']
4450996,Smallest eigenvalue of a nearest neighbor matrix in $2$ dimensions.,"Consider a 2D square lattice with $n \times n$ lattice sites. A matrix $M_n$ of size $n^2 \times n^2$ is constructed by setting $M_{ij} = u$ (where $0 \leq u \leq 1$ ) if sites $i$ and $j$ are nearest neighbors, and all the diagonal elements $M_{ii} = 1$ . For example, with $2 \times 2$ lattice sites, we have $$M_2 = \begin{pmatrix} 1 & u & u & 0 \\ u & 1 & 0 & u \\ u & 0 & 1 & 0 \\ 0 & u & u & 1 \end{pmatrix}$$ (i.e. lattice 1 has nearest neighbors 2,3 and lattice 2 has nearest neighbors 1,4 etc...). The smallest eigenvalue of $M_2$ is $\lambda_\text{min}^{(2)} = 1-2u$ , for $M_3$ it is $\lambda_\text{min}^{(3)} = 1-2\sqrt{2}u$ , for $M_4$ it is $\lambda_\text{min}^{(4)} = 1-(1+\sqrt{5})u$ . Numerically I seem to get $\lambda_{\text{min}}^{(N)} \to 1-4u$ as $N \to \infty$ , but I am not sure how to prove it. Using the Gershgorin circle theorem, I am able to get the bound $\lambda_{\text{min}}^{(N)} \geq 1-4u$ so it seems like the matrix here saturates the lower bound. Is there a way to prove this?","['matrices', 'linear-algebra', 'symmetric-matrices', 'eigenvalues-eigenvectors']"
4451000,"How do I prove this these inequalities, preferably without calculus?","I'm trying to solve an inequality which stems from this problem: If x, y, z are positive real numbers and $x^5+y^5+z^5=3$ , prove $\frac{x^8}{y^3}+\frac{y^8}{z^3}+\frac{z^8}{x^3} \ge 3$ I thought that solving this directly would require an inequality I'm not familiar with, so I decided to set $y=ax$ and $z=bx$ (note $a>0$ , $b>0$ ), reducing the inequality to be proven to: $\frac{1}{a^3}+\frac{a^8}{b^3}+b^8≥1+a^5+b^5$ for all $a, b > 0$ . I tried proving it by proving the function $f$ with $f(a)=\frac{1}{a^3}+\frac{a^8}{b^3}+b^8≥1+a^5+b^5$ is always positive for $a>0$ and all $b>0$ . I graphed it out, and noticed it only attained a value of zero when $a=1$ and $b=1$ (for values of b between 0 and 10). Afterwards, it seemed to only increase. However, the 11th degree polynomials are simply too much for me to handle. So, my questions would be: How do I prove the original inequality? And how do I prove the inequality I have 'reduced' the original one to? An answer without calculus and without advanced inequality identities would be preferred, but I would prefer an answer with them than no answer at all.","['inequality', 'functions', 'derivatives']"
4451001,Prove that there is a circle containing exactly $2018$ points,"Problem Given a set $\mathtt{E}$ containing $2017^{2019}$ points on the plane. Prove that there is a circle containing exactly $2018$ points from the set $\mathtt{E}$ (these points are on the open disk). [Maths Olympiad (Morocco $2018$ )] My Approach It's enough to find a line dividing the plane into two parts, the first one contains $2018$ points of $\mathtt{E}$ , and the other contains the rest of the points of $\mathtt{E}$ . As we can take a circle tangent to this line and with a radius big enough. I wasn't able to prove this, but it seems true, and it seems that we can somehow use pigeonhole principle to prove it.","['contest-math', 'curves', 'pigeonhole-principle', 'circles', 'combinatorics']"
4451095,Is a map which sends a $3\times 3$ symmetric tensor to an element of $SO(3)$ which diagonalizes it necessarily discontinuous?,"For a $3\times 3$ symmetric matrix $Q$ , one can construct a map to $SO(3)$ which sends $Q$ to a matrix which diagonalizes it.
If $Q$ has distinct eigenvalues, there are three choices for rotation matrix: given one (guaranteed by the spectral theorem) you can always cyclically permute the columns to get two more.
Choosing one arbitrarily defines a locally smooth map.
That is, perturbing the entries of $Q$ slightly will only slightly perturb the eigenvalues and eigenvectors. However, if $Q$ has repeated eigenvalues then given $R \in SO(3)$ which diagonalizes $Q$ , we also have that $R'(\theta) R$ is a valid choice, where $R'(\theta)$ is a rotation by any angle $\theta$ in the plane spanned by the eigenvectors corresponding to repeated eigenvalues.
It seems reasonable to me that if we arbitrarily choose some $R$ , then slightly perturbing $Q$ could give another element of $SO(3)$ which is far away from $R$ but perhaps close to $R'(\theta') R$ for some $\theta'$ . The question is then, is it possible to construct a mapping so that no such discontinuities occur, or will such a mapping always be discontinuous at elements with repeated eigenvalues?","['differential-geometry', 'smooth-functions', 'linear-algebra', 'symmetric-matrices', 'rotations']"
4451122,Maximum number of linearly independent non-commuting matrices,"Let $S$ be a set of non-commuting, linearly independent $d \times d$ positive definite matrices (i.e., for any $A \neq B$ , $[A, B] = AB - BA \neq 0$ ). Is there any upper bound for the number of elements the set $S$ contains? (It is clear that it must be equal to or smaller than $d^2$ . Any reference to a book/article is appreciated). By positive definite matrix I mean matrices in the form $A = M^{\dagger}M$ where $M$ is a $d \times d$ invertible matrix over the field $\mathbb{C}$ . Linear independence, however, is (necessarily) considered over $\mathbb{R}$ , e.g., $A \neq a_1 B + a_2 C$ for any $a_1, a_2 \in \mathbb{R}$ and $A,B,C \in S$ .",['linear-algebra']
4451144,"Number of critical points of $f(x)=(x+\ln x)^x$ on $[1,\infty)$","How many critical points does the function $f(x)=(x+\ln x)^x$ has on $[1,\infty)$ ? Here Since $x$ and $\ln x$ and $a^x$ (for $a>0$ ) are strictly increasing, we can conclude that $f(x)$ is strictly increasing on $[1,\infty)$ and I thought it is enough to conclude $f'(x)=0$ has no solution but as a counterexample $g(x)=x^3$ is also strictly increasing but $g'(0)=0$ . But I'm wondering is it possible to prove it has no critical point without calculating derivative? By the way derivative is $f'(x)=(x+\ln x)^x\times (\ln(x+\ln x)+\frac{x+1}{x+\ln x})$ and it is not hard to recognize it has no critical points from it.","['calculus', 'functions', 'derivatives']"
4451161,"Advantage of ""integrating by differentiating"" ie.$\def\e{\varepsilon}\int_a^b f(x)dx=\lim_{\e\to0}f(d/d\e)\frac{e^{\e b}-e^{\e a}}\e$","I just stumbled upon a (german) article that features the following formula to compute integrals by differentiating: $$\def\e{\varepsilon}\int_a^b\!\! f(x)dx\ =\ \lim_{\e\to0}f\left(\frac d{d\e}\right)\frac{e^{\e b}-e^{\e a}}{\e} \tag{*}$$ As the article states, the method was initially developed to calculate / approximate path integrals in Quantum Field Theory.  The new (from 2014) approach may have it's merits in QFT, but the article also states that it can make computation of integrals over $\Bbb R$ more efficient is some cases. It took me some seconds to decode the meaning of the right hand side, though.  It means that $d/d \e$ is expanded by $f$ which has to be analytic, and then apply that operator to $g_{a,b}(\e) = (e^{\e b}-e^{\e a})/\e$ and finally let $\e\to 0$ . My question: If $f$ cannot be integrated easily or symbolically, why is the right hand side of $(*)$ easier to compute or to approximate than the left hand side?  As $f$ is analytic, it's expansion has to be known in order to apply $f$ to $d/d\e$ . Hence computing $n$ term of the RHS looks way more complicated than integrating $n$ terms of the power-series expansion of $f$ on the LHS.  Does the RHS converge faster?  That would basically mean that the RHS can ""guess"" expansion coefficients of $f$ . The artical also states that: The computer algebra system Maple imlpemented the approach in 2019 and could achieve considerable speed-ups of some calculations. So it appears useful in practice, even if the title ""Revolution in Calculus"" might be 90% click-bait. It there some rule-of-thumb when that approach might be advantageous?","['definite-integrals', 'complex-analysis', 'approximate-integration', 'numerical-methods', 'derivatives']"
4451232,Is this a group or a vector space?,"Suppose we have $E=\{\mathrm{Car}\}$ with the following operations: $\mathrm{Car}+\mathrm{Car}=\mathrm{Car}$ and $\lambda\cdot \mathrm{Car}=\mathrm{Car}$ for all $\lambda \in \mathbb{R}$ . I'm wondering if this could be a commutative group or a vector space. Attempt: seems to be a structure with close ""addition"", i.e., $\forall a,b\in E, \ a+b\in E$ , also it's associative, commutative and its unique element $\mathrm{Car}$ should be the neutral element.
So I think it should be an abelian group, but then the condition of $\lambda\cdot \mathrm{Car}=\mathrm{Car}$ makes me think there is more than one neutral element (in $\mathbb{R}$ ) which is not possible, same for Vector space, since the neutral element in $\mathbb{R}$ will not be unique. And end it up accepting this couldn't be vectorial space nor abelian group. It's ok?","['group-theory', 'vector-spaces']"
4451237,"Normal operator $T$, $\sigma(T) \subset \{0,1\} \Rightarrow$ $T$ is an orthogonal projection","Let $T$ be a bounded normal operator acting from a Hilbert space $H$ to itself. I must check that if the spectrum of $T$ lies in the set defined in the title, then $T$ is an othogonal projection. I know that $T$ is an orthogonal projection iff $T=T^*=T^2$ . I have also at my disposable multiple forms of the spectral theorem, which may come in handy. I must admit though I am still trying to grasp all the notions that are usually used in the statements of the theorem. For instance, I have seen that $T$ can be written as $$T=\int_{\sigma(T)}zE(dz)$$ where $E$ represents a spectral measure, i.e. a map from a sigma algebra to the space of projections on $H$ . Are these facts useful to our goal? Any hints are welcome, thank you.","['spectral-theory', 'functional-analysis', 'normal-operator']"
4451257,Asymptotic for $\sum_{k=1}^n k^n$,"Consider the OEIS sequence A031971 , which is defined as: $$a_n=\sum\limits_{k=1}^n k^n\quad\color{gray}{(1,\,5,\,36,\,354,\,4425,\,67171,\,1200304,\,.\!.\!.\!)}\tag{1}$$ I'm interested in the asymptotic behavior of $a_n$ for $n\to\infty$ . Empirically, it appears that $$a_n\stackrel{\color{gray}?}\sim\frac{e}{e-1}\,n^n\cdot\left(1-\frac{e+1}{2\,(e-1)^2}\,n^{-1}+c\,n^{-2}+\mathcal O\!\left(n^{-3}\right)\right),\tag{2}$$ where $c\approx0.6310116...$ (I haven't found a plausible closed form it). The leading term $\frac{e}{e-1}\,n^n$ is given in the OEIS. How can we prove this formula and find higher terms in it?","['oeis', 'experimental-mathematics', 'asymptotics', 'sequences-and-series']"
4451260,Showing that 49¢ is not makeable using the given conditions,"While going through 6.042J from MITOCW, in the text Mathematics for Computer Science , I came across the following problem at which I'm stuck. Now, I proceeded doing the proof in the following manner.
Suppose $C$ be the set of counterexamples to $P(n)$ where $$P(n)::=\text{It is possible to make a postage stamp of n}¢$$ So, $$C = \bigg\{n \in \mathbb{N}, n \geq50 \mid \text{P(n) is false} \bigg\}$$ where $\mathbb{N}$ denotes the set of non-negative integers (i.e., integers $\geq 0$ ) We assume that $C$ has a least element $m \in C \text{ such that P(m) doesn't hold}$ . Assuming that $\text{51¢, 52¢, 53¢, ..., 56¢}$ are all makeable (given in the question), we can say that $$m > 56$$ $$\implies m-6 > 50$$ Now, as $\text{m is the least element in C, P(m$-6$) has to be true}$ So, $(m-6)¢$ is makeable. Thus, adding $6¢$ to it, $(m-6)¢+6¢$ is also makeable, and thus $m¢$ is also makeable. Thus we arrive at a contradiction, i.e., $\text{P(m) is actually true}$ . So, $C$ must be empty. Thus it is possible to make any amount of postage over $50¢$ . Is this proof complete and correct? Why did they say to assume that all denominations from $51¢ \text{ to } 100¢ $ were makeable, when in fact, assuming upto $56¢$ would suffice? Also, for part $\text{(b)}$ , isn't $49¢$ actually makeable? Because $49¢ = 14¢+14¢+21¢$ So why does it say to prove that it is not? Is this part incorrect?","['well-orders', 'solution-verification', 'discrete-mathematics']"
4451265,Question on use of Ito's formula with integral in the function,"This question is being asked in the context of the Feynman-Kac formula. Suppose the real-valued process $Z$ satisfies the SDE $$dZ_t=b(Z_t)dt+\sigma(Z_t)dW_t.$$ Suppose we have functions $f:\mathbb{R}\to\mathbb{R}$ and $v:[0,T]\times\mathbb{R}\to\mathbb{R}$ (satisfying any necessary regularity conditions) and define $M_t=e^{-\int_0^tf(Z_s)ds}v(t,Z_t)$ . I am interested in the use of Itô's lemma to describe the evolution of $M$ . The notes I am reading (I am just deducing from skipped over calculations) seem to imply that the exponential part of the definition of $M$ is only relevant in the $t$ -derivative part of Ito's Lemma - but is it not a function of $Z_t$ , and therefore relevant in the $Z_t$ -derivative part? Ie the notes imply $$dM_t=e^{-\int_0^tf(Z_s)ds}\left(\left(-f(Z_t)v(t,Z_t)+\frac{\partial v}{\partial t}(t,Z_t)\right)dt+\frac{\partial v}{\partial z}(t,Z_t)dZ_t+\frac{\partial^2v}{\partial z^2}(t,Z_t)d[Z]_t\right)$$ so why is the $z$ -derivative of $e^{-\int_0^tf(Z_s)ds}$ not relevant?","['stochastic-integrals', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4451270,"If $A$ is a symmetric matrix, then $\det(A) \leq \prod\limits_{i = 1}^d a_{ii}.$","Prove or provide a counterexample. If $A = (a_{ij})$ is a symmetric matrix, then $\det(A) \leq \prod\limits_{i = 1}^d a_{ii}.$ The result is obviously true for diagonal matrices and here is a proof for positive semidefinite matrices. Proof when $A$ is also positive semidefinite. If the smallest eigenvalue of $A$ is zero, then $\det(A) = 0$ and we only need to show that $\prod\limits_{i = 1}^d a_{ii} \geq 0,$ which follows from the fact that $a_{ii} = c_i^\intercal A c_i,$ where $c_i$ is the $i$ th canonical vector. Suppose that the smallest eigenvalue of $A$ is positive, this implies that $A$ is positive definite. We can orthogonally diagonalise $$
A = P\Lambda P^\intercal = \lambda_1 x_{(1)} x_{(1)}^\intercal + \ldots + \lambda_d x_{(d)}x_{(d)}^\intercal,
$$ where $P = \left[x_{(1)}, \ldots, x_{(d)}\right]$ is an orthogonal matrix and $\lambda_1 \geq \ldots \geq \lambda_d > 0.$ Write $P^\intercal = [x_1, \ldots, x_d],$ i.e. the $x_i$ are the rows of $P.$ Then $$
a_{ii} = \sum_{j = 1}^d \lambda_j x_{ij}^2 = u(x_i),
$$ where $u$ is the function $u(\xi_1, \ldots, \xi_d) = \lambda_1 \xi_1^2 + \ldots + \lambda_d \xi_d^2.$ Therefore, we have $$
\prod_{i = 1}^d a_{ii} = \prod_{i = 1}^d u(x_i).
$$ We now show that $$
(\min u(x_1) \cdots u(x_d) \quad \text{subject to} \quad [x_1, \ldots, x_d] \quad \text{is orthogonal}) = \lambda_1 \cdots \lambda_d.
$$ If this is true, we are done. We first minimise over the last column $x_d$ subject only to the contraint that $\|x_d\|^2 = 1.$ It is obvious that $u(x_d) \geq \lambda_d$ with equality when $x_d = c_d$ (the $d$ th canonical vector). We can substitute this partial result, and our problem now becomes $$
(\min u(x_1) \cdots u(x_{d-1}) u(c_d) \quad \text{subject to} \quad [x_1, \ldots, x_{d-1}, c_d] \quad \text{is orthogonal}) = \lambda_1 \cdots \lambda_d.
$$ Since $u(c_d) = \lambda_d > 0,$ we may cancel it, and clearly the condition "" $[x_1, \ldots, x_{d-1}, c_d]$ is orthogonal"" is equivalent to "" $[x_1, \ldots, x_{d-1}]$ is orthogonal"" (in $\mathbf{R}^{d-1}$ where we assume all last entried of $x_1, \ldots, x_{d-1}$ are zero). Therefore, we have the same problem as before but one dimension less, and by an easy induction, we are done. QED","['determinant', 'matrices', 'linear-algebra', 'symmetric-matrices', 'optimization']"
4451298,Can eigenvalues be interpreted as coordinates in a vector space of projection operators?,"Consider the vector space $\mathbb R^n$ . Let $T : \mathbb R^n \to \mathbb R^n$ be a linear operator, and let $\mathbf T$ be its matrix representation with respect to the standard basis in $\mathbb R^n$ . Suppose that the eigendecomposition of $\mathbf T$ is \begin{align}
\mathbf T &= \mathbf Q \mathbf \Lambda \mathbf Q^{-1} \\
&= \lambda_1 \mathbf v_1 \mathbf u_1^T + \lambda_2 \mathbf v_2 \mathbf u_2^T + \cdots + \lambda_n \mathbf v_n \mathbf u_n^T
\end{align} where $\lambda_i$ is the $i$ th eigenvalue of $\mathbf{T}$ , $\mathbf{v}_i$ is the $i$ th column in $\mathbf Q$ , and $\mathbf{u}_i^T$ is the $i$ th row in $\mathbf Q^{-1}$ . The outer products $\mathbf v_i \mathbf u_i^T$ represent ""scaled"" projection operators, such that if each of them was replaced with $\mathbf v_i \mathbf u_i^T / \mathbf u_i^T \mathbf v_i$ , then they would be true projection operators. I am not sure if the set of all projection operators on $\mathbb R^n$ forms a vector space, since this set is not a subspace of the vector space of all linear operators on $\mathbb R^n$ (not closed under scalar multiplication). However, assuming that the set of all projection operators is indeed a vector space, wouldn't the outer products $\mathbf v_1 \mathbf u_1^T,\mathbf v_2 \mathbf u_2^T,\dots,\mathbf v_n \mathbf u_n^T$ form a basis for this vector space? Furthermore, since $\mathbf T$ is expressed as a linear combination of basis vectors, then wouldn't the eigenvalues be coordinates in this vector space? Essentially, what I am asking is: do linear operators on $\mathbb R^n$ live in a vector space of projections, and are they ""located"" by their eigenvalues in this vector space?",['linear-algebra']
4451304,Reference request for perfection of schemes over finite fields,"I am currently reading a paper from 2021 which uses ""perfection"" of schemes over finite fields. If $X$ is such a scheme over $\mathbb F_q$ , the associated perfection is denoted by $X^{\mathrm{perf}}$ . The whole process is used without reference, so that the construction seems to be well-known in that area of algebraic geometry. However it is the first time that I encounter this. Would somebody understand what kind of space $X^{\mathrm{perf}}$ is, and could recommend some material to learn more on the perfection process?","['finite-fields', 'algebraic-geometry', 'schemes', 'reference-request']"
4451310,Probability of pair of gloves selection,"In his wardrobe, Fred has a total of ten pairs of gloves. He had to pack his suitcase before a business meeting, and he chooses eight gloves without looking at them. We assume that any set of eight gloves has an equal chance of being chosen. I am told to calculate the likelihood that these 8 gloves do not contain any matching pairs, i.e. that no two (left and right) gloves are from the same pair. This is what I came up with, that is, the probability of success for each choice: $$\frac{20}{20}×\frac{18}{19}×\frac{16}{18}×...×\frac{6}{13}=\frac{384}{4199}≈0.09145$$ At first, I was a little confused by the wording but I believe this seems about right. Is there an alternative way to get the desired probability, e.g. with $1-...$ ? Thanks in advance for any feedback.","['combinatorics', 'probability']"
4451312,Counterexample for a convex problem,"The convex optimization problem is as follows: \begin{align}
    \underset{\mathbb{X},\mathbb{Y}\in\mathbb{S}_n^+}{\min}\quad &\operatorname{Tr}(X)+ \operatorname{Tr}\left(D Y \right)\nonumber\\
\text{s.t.}\;\; &AX+XA^T+BB^T\geq 0 \nonumber\\
 &\begin{bmatrix} YA+A^\top Y -\gamma I & YB \\ B^\top Y & -I\end{bmatrix} \preceq 0\nonumber\\
&\begin{bmatrix}
X&I\\I& Y
\end{bmatrix}\geq 0\nonumber
\end{align} I feel at optimality $XY$ might not be equal to I. Any counterexamples","['convex-optimization', 'control-theory', 'matrices', 'optimization', 'matrix-equations']"
4451324,Variance of product of Gaussian random variables,"Suppose I have $r = [r_1, r_2, ..., r_n]$ , which are iid and follow normal distribution of $N(\mu, \sigma^2)$ , then I have weight vector of $h = [h_1, h_2,  ...,h_n]$ ,
which iid followed $N(0, \sigma_h^2)$ , how can I calculate the $Var(\Sigma_i^nh_ir_i)$ ? suppose $h, r$ independent. How should I deal with the product of two random variables, what is the formula to expand it, I am a bit confused.","['statistics', 'variance', 'normal-distribution', 'random-variables']"
4451377,Misapplication of the divergence theorem when calculating a surface integral?,"Let $\mathbf{F} = (3y, -xz, yz^2)$ , and let $S=\{(x,y,z): z=\frac{1}{2}(x^2+y^2), z\leq 2\}$ .  Find $\iint_S (\nabla \times \mathbf{F}) \cdot d\mathbf{S}$ . Firstly, I know I can compute this quite easily using Stokes' theorem -- my question concerns only the divergence theorem. Using Stokes' theorem, I obtain an answer of $-20\pi$ . Now below is my working relating to the divergence theorem. If I let $\Sigma$ be the disc $x^2+y^2\leq 4$ on the plane $z=2$ , then $S \cup \Sigma$ is the boundary of $\Omega = \{(x,y,z) : \frac{1}{2}(x^2+y^2)\leq z\leq 2\}$ . So by the divergence theorem, $$ \iiint_\Omega \nabla \cdot \mathbf{F} dV = \iint_S \mathbf{F}\cdot d\mathbf{S} + \iint_\Sigma \mathbf{F}\cdot d\mathbf{S}. $$ For the region $\Sigma$ , the unit normal is $(0,0,1)$ , so we can calculate the surface integral as \begin{align*}
 \iint_\Sigma \mathbf{F}\cdot d\mathbf{S} &= \iint_{\Sigma} \mathbf{F}\cdot (0,0,1) \,dA \\ 
&= \iint_{x^2+y^2\leq 4 \\ z=2} yz^2 dA \\ 
&= 4 \iint_{x^2+y^2\leq 4} y dA \\
&= 4 \int_0 ^{2\pi} \int_0^2 (r\sin\theta) drd\theta \\ 
&= 4 \int_0 ^{2\pi} 2\sin \theta = 0.
\end{align*} Also, $\nabla \cdot \mathbf{F} = (0,0,2zy)$ , so \begin{align*}
\iiint_\Omega 2zy dV &= 2\int_0^2 \iint _{x^2+y^2\leq 2z} yz\,dV \\ 
&=2\int_0^2\int_0^{2\pi} \int_0^\sqrt{2z} (r\sin\theta z) dr d\theta dz
\end{align*} but this will still be $0$ because of the presence of $\sin \theta$ . This would imply that $\iint_S \mathbf{F}\cdot d\mathbf{S}=0$ . My question is: what went wrong with the second computation? I think it might be the way I handled the volume integral $\iiint_\Omega$ , but I'm not quite sure. Have I misunderstood the divergence theorem? Any guidance would be very much appreciated.","['multivariable-calculus', 'solution-verification', 'vector-analysis']"
4451380,How to create synthetic data for a decaying curve in order to extrapolate it beyond some point? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question In the following curve , I would like to extend the measurements beyond $x$ =1 in order to have a better estimate of the green curve compared to red line. Note: I do not have the analytical form of the function but only ( $x$ , $y$ ) data sets in the range (0, 1). Additional Question:
What is the functional form of the following curve?","['python', 'functions', 'exponential-function', 'extrapolation']"
4451384,Prove that $\{x\in A:P(x)\}$ is always a subset of $A$,"I'm trying to prove the following, Proposition. Let $A$ be a set and let $P(x)$ be a property pertaining to $x$ , then $\{x\in A:P(x)\}\subseteq A$ . Here is my proof: proof. To prove $\{x\in A:P(x)\}\subseteq A$ , we need to show that $\forall y$ , $y\in\{x\in A:P(x)\}\implies y\in A$ . Now suppose $y\in\{x\in A:P(x)\}$ . By definition this means $(y\in A$ and $P(y)$ is true). We divide into cases. For the case $y\in A$ , we have either $P(y)$ to be true or false. If $P(y)$ is true, then $y\in A\land P(y)$ is true and therefore the implication $y\in\{x\in A:P(x)\}\implies y\in A$ is true. If $P(y)$ is false, then the implication is vacuously true. Now suppose instead $y\notin A$ . We have either $P(y)$ to be true or false. But in either case $y\notin A$ , and so the implication is vacuously true. Hence $\{x\in A:P(x)\}\subseteq A$ , as claimed. is this correct?","['elementary-set-theory', 'solution-verification', 'real-analysis']"
4451395,Construct a fair game with a $N$ sided die,"You have a $N$ sided die. And $X$ players. You have to devise a game, such that only one player wins and every player is equally likely to win.
Also, the game should be finite (there shouldn't be a single infinite run in the sample space) Is it possible to construct such a game? (However complicated, doesn't matter) If yes, how? At the first glance, it seems like it isn't possible, since (For $N=6$ ) we can only have sample space of the sizes of powers of $2,3,6$ . But maybe there exists a complicated game where the sizes are different?","['game-theory', 'puzzle', 'probability-theory', 'probability']"
4451425,"Find the number of elements in $\{0,1\}^n$ with no more than three $1$'s or three $0$'s in a row","I'm trying to find a general formula for the number of elements $s_n$ in $\{0,1\}^n$ with no more than three $1$ 's or three $0$ 's in a row, where $n\geq1$ . I calculated $s_n$ for small values of $n$ but could not really come up with a formula to prove by induction. I also approached the problem combinatorially by considering two groups, one of three $0$ 's and one of three $1$ 's, and arranging them together with other arbitrary elements totalling $n$ elements in all, but since we want to find the number of elements with no more than three $0$ 's or $1$ 's in a row, and not exactly three, this approach does not work either","['combinatorics', 'discrete-mathematics']"
4451471,When are levy flights superdiffusive and why? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question A stable distribution that is centered and symmetrical has a value $\alpha \leq 1$ the expected value of the distribution is undefined. I have also been told that if a Levy Flight has it's step lengths drawn from the absolute of the stable distribution it will become superdiffusive when $\alpha = 1$ but not when $2 \geq \alpha > 1$ . Is it true that levy flights would only be superdiffusive when $\alpha \leq 1$ and why is that?","['levy-processes', 'statistics', 'probability-distributions', 'random-walk']"
4451567,Multiplicative energy and Cauchy-Schwartz,"Let $A$ be a finite set in a ring, and define $E(A) =\left|\left\{(a, b, c, d) \in {A}^{4}: c a=d b\right\}\right|.$ A number of papers (e.g. here ) quote the lower bound $$E({A}) \geq \frac{|{A}|^{4}}{|{A} {A}|}$$ where $AA = \{ab:a,b\in A\}$ , and say it follows from Cauchy-Schwartz. However, I cannot find a proof, so: how does the above inequality follow from an application of Cauchy-Schwartz?","['cauchy-schwarz-inequality', 'abstract-algebra', 'combinatorics', 'upper-lower-bounds']"
4451617,Can the real numbers be equally split into two sets of same measure?,"The rational numbers $\Bbb Q$ are dense in $\Bbb R$ , but they are still a set of measure zero, i.e. $$\begin{align}
\mu(\Bbb Q \cap [a,b]) &= 0 \\
\mu((\Bbb R\!\setminus\! \Bbb Q) \cap [a,b]) &= b-a \\ \tag 1
\end{align}$$ for any finite interval $[a,b]$ . Is it possible to have more equally distributed sets, so that neither of them is a set of measure 0, and this holds on any interval similar to (1)? More specifically, are there decompositions $A, B\subset \Bbb R$ and a measure $\mu$ such that all of the following conditions hold? $$\begin{align}
A\cap B = \emptyset\quad&\text{ and }\quad A\cup B = \Bbb R \\
\mu ([a,b]) &= b-a \\
\mu (A\cap [a,b]) &= (b-a) / 2 \\
\mu (B\cap [a,b]) &= (b-a) / 2 \\\tag 2
\end{align}$$ for any finite interval $[a,b]\subset\Bbb R$ ? The first line just states that $A,B$ is a decomposition of $\Bbb R$ , the second line is a common normalizing condition for $\mu$ . Or, at your option, that $$\begin{align}
\mu(A\cap[a,b]) &= (b-a)\kappa \qquad\text{ for some } 0<\kappa<1 \\
\mu(B\cap[a,b]) &= (b-a)(1-\kappa)
\end{align}$$ again for any finite interval $[a,b]$ .  And it might even be in order if $\kappa=\kappa(a,b)$ depends on $a$ and $b$ provided $0<\kappa(a,b)<1$ for finite intervals. My intuition says that there is no such decomposition, but maybe I am wrong.","['real-numbers', 'measure-theory', 'lebesgue-measure']"
4451639,Multiplicity of finite modules,"Let $k$ be an algebraically closed field. In the discussion of Bezout's theorem, the following definiton of multiplicity has been given: Definition. Let $R$ be a noetherian graded $k$ -algebra and $M$ a finitely generated $R$ -module. The multiplicity $\mu_{\mathfrak{p}}$ of $M$ at a prime ideal $\mathfrak{p} \subset R$ is defined as the length $l$ of $M_{\mathfrak{p}} = M \otimes_R R_\mathfrak{p}$ as $R_{\mathfrak{p}}$ -module. I have trouble understanding the definition: Why do we want our ring and module to be graded? I do not see why this is essential (or 'needed'). I tried to work out the following special case: If $(R, \mathfrak{m})$ is local and noetherian with residue field $K = R/\mathfrak{m}$ , then $R \cong R_\mathfrak{p}$ and $M \cong M_\mathfrak{p}$ . So, $\mu_{\mathfrak{p}}(M)$ is finite if and only if $\mathfrak{m}^d M = 0$ for some $d$ . Indeed, $$M \supset \mathfrak{m}M \supset \mathfrak{m}^2M \supset \cdots \supset \mathfrak{m}^s M$$ gives a composition series and, $M$ has finite length if and only if this composition series is finite, that is $\mathfrak{m}^s M = 0$ for some $s \in \mathbb{N}$ . What confuses me is the following: In this case, the lectures notes define $$\mu_\mathfrak{m}(M) := \dim_K(M \otimes_R K),$$ but $M \otimes_R K \cong M/\mathfrak{m}M$ , and this is a $K$ -vector space of dimension one. So, the multiplicity of $M$ over a local ring is always one? This seems rather unlikely to me. Where is the reasoning error? (See Edit below) How does this capture the geometric intuition of multiplicity? I am trying to understand how one comes up with this definition, and usually going from geometry backwards seems helpful. Here is my attempt: For simplicity, let $X = \mathbb{A}^2$ be the affine plane and $R = A(\mathbb{A}^2) \cong k[x,y]$ the coordinate ring. Suppose $p \in X$ , corresponding to the maximal ideal $\mathfrak{m}_p \subset k[x,y]$ . Then, $R_{\mathfrak{m}_p} \cong \mathcal{O}_{X,p}$ . Now, assume we have $f,g \in \mathcal{O}_{X,p}$ which cut out two curves $C_1 = Z(f)$ and $C_2 = Z(g)$ in the plane and $f(p) = g(p) = 0$ . That is, $f,g$ belong to the unique maximal ideal $\mathfrak{m}_{X,p}$ of $\mathcal{O}_{X,p}$ . At this point, I am not completely sure how to continue and what the ' $M$ ' should be. Does it make sense to set $M: = \mathcal{O}_{X,p}/I$ for $I = (f,g)$ ? This is a finitely generated $R$ -module, and in fact, it has vector space structure over $\mathcal{O}_{X,p}/\mathfrak{m}_{X,p}$ if we assumed that $I$ is $\mathfrak{m}_{X,p}$ -primary. But what do we gain by setting $f$ and $g$ to zero? If this is the approach to pursue, why does the dimension of this vector space correspond to the multiplicity in the sense of order of vanishing? This still remains in the dark, from my point of view. Note: Please assume no scheme theory knowledge in your answer. Edit: I realized that $\dim_K(M/\mathfrak{m}M)$ is not necessarily one-dimensional, but finite, since $M$ is a finite module. But then the asserted chain of submodules is not a composition series. So, one should possibly argue follows: $M/\mathfrak{m}M$ has finite length and from $$
l_R(M/\mathfrak{m}M) + l_R(\mathfrak{m}M) = l_R(M),
$$ we get that $M$ has finite length if and only if $\mathfrak{m}M$ has finite length.","['algebraic-geometry', 'commutative-algebra']"
4451721,If $E(X_n^2) = \infty$ then $\limsup \frac{|X_n|}{\sqrt{n}} \geq a$ almost surely.,"We have given $X_1,X_2,\ldots$ an i.i.d. sequence of random variables such that $$\Bbb{E}(X_1^2)=\infty$$ I claim that for all $a>0$ $$\Bbb{P}\left(\limsup_{n\rightarrow \infty} \frac{|X_n|}{\sqrt{n}}\geq a\right)=1$$ My idea was to use Borel-Cantelli, but somehow I'm a bit confused since I never used that $\Bbb{E}(X_1^2)=\infty$ . I wanted to do this as follows: Let $\Lambda_n=\left\{\frac{|X_n|}{\sqrt{n}}\geq a\right\}$ then $$\sum_{n=1}^\infty \Bbb{P}(\Lambda_n)=\sum_{n=1}^\infty \Bbb{P}\left(\frac{|X_n|}{\sqrt{n}}\geq a\right)=\sum_{n=1}^\infty 1-\Bbb{P}\left(\frac{|X_n|}{\sqrt{n}}\leq a\right)$$ Now if $$\sum_{n=1}^\infty 1-\Bbb{P}\left(\frac{|X_n|}{\sqrt{n}}\leq a\right)<\infty$$ then it would mean that for infinitely many $n\in \Bbb{N}$ $$\Bbb{P}\left(\frac{|X_n|}{\sqrt{n}}\leq a\right)=1$$ Here I think I need some argument to show that this is not possible right? If this works I then could apply Borel-Cantelli and would be done. I'm not sure if this is correct so. (I also thought about the central limit theorem but I don't think this is useful here)","['measure-theory', 'stochastic-calculus', 'borel-cantelli-lemmas', 'probability-theory', 'probability']"
4451733,"If $A$ and $B$ have the same measure, are they isomorphic?","During a discussion, the following result was mentioned. Theorem. Let $A$ and $B$ two Borel-measurable subsets of $\mathbb R$ with the same (positive and finite) Lebesgue measure. Then, there exists a measurable and bijective map from $A$ to $B$ . Apparently, this is well-known and ""I should be able to find it in any book"". However, I couldn't. Could anyone point at a reference, please? I wonder if it is (true and) easy to prove. Thanks!","['probability-theory', 'probability', 'reference-request']"
4451782,"$I(x) = -\int_0^1 \frac{1}{z}\ln\left(\frac{1-x z + \sqrt{1-2 x z+ z^2}}{2}\right)\,dz$","Is there a closed form integral for $$I(x) = -\int_0^1 \frac{1}{z}\ln\left(\frac{1-x z + \sqrt{1-2 x z+ z^2}}{2}\right)\,dz$$ for $-1 < x < 1$ ? This integral is related to Legendre polynomials as $$\frac{-1}{z}\ln\left(\frac{1-x z + \sqrt{1-2 x z+ z^2}}{2}\right)=\sum_{n=1}^{\infty} \frac{P_{n}(x)}{n} z^{n-1}$$ $$I(x) = \sum_{n=1}^{\infty} \frac{P_{n}(x)}{n^2}$$ where $P_{n}(x)$ satisfies $\sum_{n=0}^{\infty} P_n(x) z^2 = (1-2 x z + z^2)^{-
1/2}$ for all $-1< x< 1$ and $-1< z< 1$ . Integrating twice with respect to $z$ gives a relation \begin{align}\sum_{n=0}^{\infty} \frac{P_{n}(x)}{(n+1)(n+2)} &= \int_0^1 dz \int_0^z dz'\frac{1}{\sqrt{1 - 2 z' x +x^2}}\,. 
\\
&=
1 - \sqrt{2(1-x)} +(1-x) \ln\left(1+\sqrt{\frac{2}{1-x}}\right) \\&= 1-2s+2s^2 \ln\frac{1+s}{s} 
\end{align} where $s=\sin(\theta/2)$ and $x=\cos\theta$ . The elements of this sum are asymptotically equal to the sum defining $I$ , but I am not sure if this is useful to get a closed form for $I$ . Another related relation is $$I(x) = \int_0^1 \frac{dz}{z} \int_0^z \frac{dz'}{z'} \left[\frac{1}{\sqrt{1 - 2 z' x +x^2}}-1\right]\,. 
$$","['integration', 'legendre-polynomials', 'asymptotics', 'closed-form', 'generating-functions']"
4451800,Criteria for $3 \times 3$ matrix to positive definite,"Here it is said that a $2\times 2$ matrix $A$ is positive definite if and only if $tr(A) >0$ and $det(A)>0$ . This will not work if $A$ is $3\times 3$ . But is there any way to enforce the positive definiteness of the matrix $A$ via the trace and determinant of $A$ , if $A$ is of size $3\times 3$ ?","['matrices', 'numerical-linear-algebra', 'linear-algebra', 'positive-definite']"
4451818,When is the space of linear and bounded operators between Hilbert spaces a HIlbert space?,"Let $H_{1}$ and $H_{2}$ be two Hilbert spaces. My question is, when is the set of linear and bounded operators $\mathcal{L}(H_{1}, H_{2})$ with the usual norm a Hilbert space?
I think I've proved that whenever $H_{1}$ or $H_{2}$ are $1$ -dimensional the space of linear and bounded operators is a Hilbert space. But, what happens in arbitrary dimensions? Thanks in advance.","['hilbert-spaces', 'functional-analysis']"
4451846,"Spectrum of the operator on $L^2[0,1]$","Consider the operator T on $L^2[0,1]$ , given by $T(f(x)) = \int_{1-x}^1 f(y)dy$ . I want to find the spectrum of this operator. I know the only possible candidates are 0 and non-zero Eigen values of T, since it is a compact operator. Since it is infinite dimensional $0$ has to be in spectrum. Now I've to check only non-zero eigen values $\lambda$ of T, but $T(f(x))= \lambda f(x)$ gives the following ODE, $f'(x) = -\frac{1}{\lambda}f(1-x)$ . Please help me how to proceed from here.","['analysis', 'functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
4451880,Arrange N unit squares in form of a grid such that number of rectangle is maximum?,"You are given N square tiles of dimension 1×1. You have to arrange them in form of a grid such that total number of rectangle (of all possible dimensions) is maximum.
Hollows within the grid are not allowed. The grid should be a complete rectangle. Only extra tiles after a complete rectangle can be placed on the side to form additonal rectangles. Example : When N=8 27  Rectangles _ _ 
|_|_|_
|_|_|_|
|_|_|_| 30  Rectangles _ _ _ _
|_|_|_|_|
|_|_|_|_| 36  Rectangles _ _ _ _ _ _ _ _
|_|_|_|_|_|_|_|_| Hence, 36 rectangles is desirable.
Can anyone help me in formulating the formula for the same in terms of N? I received this problem from a friend who in turn might have recieved it from assignment or a maths contest.
My first thought of solving this problem is for each integer value of row calculate maximum possible columns and then put the remaining tiles along the smaller side(so that those tiles can form maximum combination with bigger side) . This way we might have to try out rows from 1 to $\sqrt N$ . Now I hear there can be better approach than this. Let me know if you can think of any.","['maxima-minima', 'rectangles', 'combinatorics', 'discrete-mathematics']"
4451936,"Is this an accurate equation for the ""tilt"" of an ellipse?","Say that an ellipse has its center on the origin and passes through a point $(r,\theta)$ (given in polar coordinates) and is not necessarily parallel to the $x$ and $y$ axes. The equation for such an ellipse may be given by the following: $$r = \frac{1}{\sqrt{\frac{\cos^2(\theta-k)}{a}+\frac{\sin^2{(\theta-k)}}{b}}}$$ which we can rewrite as $$\frac{1}{r^2} = \cos^2{\theta}\big(\frac{\cos^2{k}}{a}+\frac{\sin^2k}{b}\big)+\sin^2{\theta}\big(\frac{\sin^2{k}}{a}+\frac{\cos^2k}{b}\big)+\sin{2\theta}\big(\frac{\sin2k}{2b}-\frac{\sin2k}{2a}\big)$$ From here, we can first use the identity $\cos^2{k}=1-\sin^2{k}$ and then distribute and rearrange to get the following. $$\frac{1}{r^2}-\frac{\cos^2{\theta}}{a}-\frac{\sin^2{\theta}}{b}=\sin^2k\bigg(\cos^2{\theta}\big(\frac{1}{b}-\frac{1}{a}\big)+\sin^2{\theta}\big(\frac{1}{a}-\frac{1}{b}\big)\bigg)+\sin{2k}\big(\frac{\sin{2\theta}}{2b}-\frac{\sin{2\theta}}{2a}\big)$$ Using the squared angle theorem, we then can change this to $$\frac{1}{r^2}-\frac{\cos^2{\theta}}{a}-\frac{\sin^2{\theta}}{b}=\big(\frac{1-\cos{2k}}{2}\big)\bigg(\cos^2{\theta}\big(\frac{1}{b}-\frac{1}{a}\big)+\sin^2{\theta}\big(\frac{1}{a}-\frac{1}{b}\big)\bigg)+\sin{2k}\big(\frac{\sin{2\theta}}{2b}-\frac{\sin{2\theta}}{2a}\big)$$ Distributing and rearranging yields $$\frac{1}{r^2}-\frac{\cos^2{\theta}}{a}-\frac{\sin^2{\theta}}{b}-\frac{1}{2}\cos{2\theta}(\frac{1}{b}-\frac{1}{a}) = -\frac{1}{2}\big(\frac{1}{b}-\frac{1}{a}\big)\big(\cos(2k+2\theta)\big)$$ And finally, solving for $k$ gives us $$k = - \theta - \frac{1}{2}\arccos\bigg(\frac{2\big(\frac{1}{r^2}-\frac{cos^2\theta}{a}-\frac{\sin^2\theta}{b}-\frac{1}{2}\cos{2\theta}(\frac{1}{b}-\frac{1}{a})\big)}{\frac{1}{b}-\frac{1}{a}}\bigg)$$ I did skip past a few interstitial steps here, but I am confident that everything is correct. Is my work sound? I wouldn't find it surprising that the tilt of the ellipse is related to the length of its axes, but I wanted to make sure that I'm on the right path. To reiterate, here we are saying that $r$ and $\theta$ are known quantities, so don't worry about getting lost in a sea of variables. Thank you!","['trigonometry', 'conic-sections']"
4451956,"Rudin’s PMA, Theorem 11.20","This is the definition which we need for the theorem: (source) 11.19 $\; \;$ Definition $\; \;$ Let $s$ be a real-valued function defined on $X$ . If the range of $s$ is finite, we say that $s$ is a simple function . $\quad \quad$ Let $E \subset X$ , and put $$
K_{E}(x)= \begin{cases}1 & (x \in E), \\ 0 & (x \notin E).\end{cases}
$$ $K_{E}$ is called the characteristic function of $E$ . $\quad \quad$ Suppose the range of $s$ consists of the distinct numbers $c_{1}, \ldots, c_{n}$ . Let $$
E_{i}=\left\{x \mid s(x)=c_{i}\right\} \quad(i=1, \ldots, n) .
$$ $\quad \quad$ Then $$
s=\sum_{n=1}^{n} c_{l} K_{E_{1}},
$$ that is, every simple function is a finite linear combination of characteristic functions. It is clear that $s$ is measurable if and only if the sets $E_{1}, \ldots, E_{n}$ are measurable. $ \quad \quad$ It is of interest that every function can be approximated by simple functions: Here is the theorem: (source) 11.20 $\; \;$ Theorem $\; \;$ Let $f$ be a real function on $X$ . There exists a sequence $\left\{s_{n}\right\}$ of simple functions such that $s_{n}(x) \rightarrow f(x)$ as $n \rightarrow \infty$ , for every $x \in X$ . If $f$ is measurable, $\left\{s_{n}\right\}$ may be chosen to be a sequence of measurable functions. If $f \geq 0,\left\{s_{n}\right\}$ may be chosen to be a monotonically increasing sequence. $\quad$ Proof $\; \;$ If $f \geq 0$ , define $$
E_{n i}=\left\{x \mid \frac{i-1}{2^{n}} \leq f(x)<\frac{i}{2^{n}}\right\}, \quad F_{n}=\{x \mid f(x) \geq n\}
$$ for $n=1,2,3, \ldots, i=1,2, \ldots, n 2^{n}$ . Put $$
s_{n}=\sum_{i=1}^{n 2^{n}} \frac{i-1}{2^{n}} K_{E_{n l}}+n K_{F_{n}} .
 \quad \quad (50)$$ In the general case, let $f=f^{+}-f^{-}$ , and apply the preceding construction to $f^{+}$ and to $f^{-}$ . $\quad$ It may be noted that the sequence $\left\{s_{n}\right\}$ given by $(50)$ converges uniformly to $f$ if $f$ is bounded. I couldn't understand that why does the sequence { $s_n$ }, given by $(50)$ converge to $f$ . Any help would be appreciated.","['measure-theory', 'lebesgue-measure', 'measurable-functions', 'uniform-convergence', 'convergence-divergence']"
4451978,Using Jensen's inequality on $\mathbb{E}[1/x]$ when x can be both positive and negative,We know the function $f(x)=\frac{1}{x}$ is convex when $x$ is positive and concave when $x$ is negative. I want to show if $\mathbb{E}[\frac{1}{x}]$ is bigger than or smaller than $\frac{1}{\mathbb{E}[x]}$ using Jensen's inequality but what happens if $x$ is on the range for example $-1<x<1$ so that it is concave on some portion and convex on some portion? How do I know the direction of the inequality? Does the range of the function matter? For example when there are more positives than negatives $-1<x<2$ versus when there are more negative than positive $-2<x<1$ ? Thank you so much!,"['statistics', 'probability-distributions', 'jensen-inequality', 'probability']"
4452042,If an element in a Banach algebra is anihilated by an analytic function then it must be algebraic.,"Let $A$ be a Banach algebra, let $a\in A$ and suppose $f(a)=0$ , where $f$ is an analytic function defined on an open set $U$ containing $\sigma(a)$ .  Prove that $a$ is algebraic in the sense that $p(a)=0$ for some polynomial $p$ . PS: I have just answered an identical question a few minutes ago but then, almost immediately, the questioner deleted the question and with it my nice answer which I enjoyed very much writing. I am therefore asking it again and I'll soon post my answer here. Should anyone point me to some Stack Exchange guideline I'm disrespecting by doing so, I'll gladly delete everything again.","['banach-algebras', 'spectral-theory', 'functional-analysis', 'functional-calculus']"
4452078,Is there any function such that the limit of its derivative divided by its value to the nth power diverges?,"Recently, I have become intrigued with this functional: $$D_n=\lim \limits _{x\to \infty}\frac{f'(x)}{[f(x)]^n}.$$ In particular, provided that the function is both differentiable and increasing in magnitude for all $x$ , for which functions does $D_n$ diverge to infinity? First, I have considered $n=1$ . This has obvious solutions. For example, $e^{e^x}$ , which has derivative $e^x\cdot e^{e^x}$ , makes $D_1$ diverge to infinity, but not $D_2$ . Is there any function which satisfies the above conditions and makes $D_2$ diverge? What about a function which satisfies the above conditions and makes $D_n$ diverge for all values of $n$ ? As a reminder, here are the conditions for the functions: You must be able to choose an $x_0$ such that the function is differentiable for all $x>x_0$ . The absolute value of the function must be increasing for all $x$ on which it is defined. The function must be both defined on real numbers and real-valued.",['analysis']
4452085,how to approach the discrete counterpart of a continuous CDF of uniform random variable,"I am following an online example to get the CDF of a function $Y$ of uniform random variable $x \in [0, 1]$ $$ Y = \frac{30}{2-x}
$$ Based on the example, CDF is a distribution of probability for the case when random $Y$ is less than or equal to a given number $y$ , that is $$
  P(Y \leq y) = P\left(\frac{30}{2-x}\leq y\right) = P\left(x \leq 2 - \frac{30}{y}\right) = 2 - \frac{30}{y}
$$ Assuming that $Y$ is the temperature of a room, and the only fact we know is that the temperature is changed based on a function of uniform variable $x$ , such that the random temperature is bound in [15, 30] celsius degrees, once an hour. After the temperature is stabilized, it will be constant until the next hour. We would like to know the chance to see the room temperature below a certain celsius degress (within [15, 30]), which could be told by $$
  P(Y\leq y) = 2 - \frac{30}{y}
$$ This is not a uniform distribution. For example, the change to see the temperature between [15,20] degrees is $$P(15 \leq Y\leq 20) = 50\%$$ but the change to see the temperature between [20, 25] is $$P(20 \leq Y\leq 25) = 30\%$$ In all analyses, we consider the temperature distribution is continuous, any temperature between 15 and 30 is possible. But in the actual case, the temperature can only be changed in the increment of 0.1, which means we could only have $$15, 15.1, 15.2, 15.3, \cdots 24.8, 24.9, \cdots, 29.7, 29.8, 29.9$$ The random process should be modified as $$
  Y^* = 0.1\times\left\lfloor\frac{300}{2-x}\right\rfloor
$$ How do we approach $P(Y^*\leq y)$ when we only have some discrete values in the above case? I don't have a clue yet so I am trying the simulation below r = 0:0.00000001:1;
N = length(r);
y = floor(300./(2-r))*0.1;
P = ( length(find(y<=25)) - length(find(y<=20)) )/N % probability with tempature between [20, 25] I got the result $$P(20\leq Y^*\leq 25) = 29.7318\%$$ Similarly, the simulation get $$P(15 \leq Y^*\leq 20) = 49.4218\%$$ It makes sense that the probability is lower for only some discrete numbers are included, but is it any way to get the close-form to estimate $P(Y^*\leq y)$ ? Thanks. While I am reading one response, I am thinking is it correct to sum all (continuous) probabilities between [15, 15.1) for the probability of getting the discrete value 15, all continuous probabilities between [15.1, 15.2) for the probability of getting the discrete value 15.1 and so on. So we may have $$P(Y^*=15) = P(15\leq Y < 15.1) = 2 - \frac{30}{15.1} - \left[2 - \frac{30}{15}\right] = 0.0130708$$ and something like that.
Now if we add all $$P(Y^*=15) + P(Y^*=15.1) + \cdots + P(Y^*=20) = 0.501606$$ This number is actually more than 50%! Does it mean $P(15\leq Y \leq 20) < P(15 \leq Y^* \leq 20)$ ? I don't quite get it, if I do the simulation I get the result less than 50% for the discrete case, but from analytical, it is more than 50%.","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4452127,Fitting a statement in a long list of equivalent results,"Theorem. Suppose that $D\subset \mathbb C$ is a connected open set. The following are equivalent. Either $D=\mathbb C$ or $D$ is conformally equivalent to $\mathbb D$ . $D$ is homeomorphic to $\mathbb D$ . $D$ is simply connected. Ind( $\gamma,z)=0$ for every smooth closed curve $\gamma$ in $D$ and every $z\in\mathbb C\backslash D$ . $\mathbb C_{\infty}\backslash D$ is connected. If $f\in H(D)$ then there exists a sequence of polynomials $(P_{n})$ such that $P_{n}\to f$ uniformly on compact subsets of $D$ . $\int_{\gamma} f(z)\,dz=0$ for every $f\in H(D)$ and every smooth closed curve $\gamma$ in $D$ . If $f\in H(D)$ then there exists $F\in H(D)$ with $F'=f$ . If $u:D\to \mathbb R$ is harmonic then $u=\mathbb R$ e $(f)$ for some $f\in H(D)$ . If $f\in H(D)$ has no zero in $D$ then there exists $L\in H(D)$ with $f=e^{L}$ . If $f\in H(D)$ has no zero in $D$ then there exists $g\in H(D)$ with $g^{2}=f$ . The proof to this theorem can be found in David C. Ullrich's book Complex Made Simple, and it is surprisingly nice that it was proven as a chain (as in $1\implies 2\implies\cdots\implies 11\implies 1$ ). Let $D$ and $G$ be connected open subsets of $\mathbb C$ with $D\subset G$ . If $(f,D)$ is a function element which admits continuation along every path in $G$ which starts at a point of $D$ then we say that $(f,D)$ admits unrestricted continuation in $G$ . Now, consider the following: Proposition. Let $D$ be an open connected subset of the plane. $D$ is simply connected if and only if for every functional element $(f,A)$ , $A\subset D$ that admits unrestricted continuation in $D$ there exists a function $F\in H(D)$ such that $f=F\rvert_{A}$ . Between what numbers in the theorem would this proposition fit the best, in order to obtain a chain-like proof of the theorem?","['complex-analysis', 'analytic-continuation']"
4452170,How to rigorously fill in certain steps in the proof of L'Hôpital's Rule as it appears in Spivak's Calculus? [duplicate],"This question already has answers here : Understanding the Proof of L'Hopital's Rule (2 answers) Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved The following is L'Hôpital's Rule as it appears in Spivak's Calculus Suppose that $\lim\limits_{x \to a} f(x) = \lim\limits_{x \to a}
 g(x)=0$ and suppose also that $\lim\limits_{x \to a} \frac{f'(x)}{g'(x)}$ exists. Then $\lim\limits_{x \to a} \frac{f(x)}{g(x)}$ exists, and $$\lim\limits_{x \to a} \frac{f(x)}{g(x)}= \lim\limits_{x \to a}
 \frac{f'(x)}{g'(x)}$$ Spivak then provides a proof of the theorem, and at the very end writes (Once again, the reader is invited to supply the details of this part
of the argument) The following is my attempt at supplying details of certain steps in the proof. There are two steps which I understand at an intuitive level, but not at a formal mathematical level. The hypothesis that $\lim\limits_{x \to a} \frac{f'(x)}{g'(x)}$ exists
contains two implicit assumptions (1) there is an interval $(a-\delta, a+\delta)$ such that $f'(x)$ and $g'(x)$ exist for all $x$ in $(a-\delta, a+\delta)$ except, perhaps,
for $x=a$ (2) in this interval $g'(x) \neq 0$ with, once again, the possible
exception of $x=a$ To show this, recall that $\lim\limits_{x \to a} \frac{f'(x)}{g'(x)}=l$ means $$\forall \epsilon>0\ \exists \delta>0\ \forall x,\ 0<|x-a|<\delta \implies \left | \frac{f'(x)}{g'(x)} -l \right |<\epsilon$$ Therefore, $\frac{f'(x)}{g'(x)}$ is defined in $(a-\delta) \cup (a+\delta)$ , so $f'(x)$ and $g'(x)$ both exist and $g'(x) \neq 0$ in this interval. Note that since this is a limit for $x$ approaching $a$ nothing is said about $f'$ and $g'$ at $a$ . On the other hand, $f$ and $g$ are not even assumed to be defined at $a$ That is, all we assumed was $\lim\limits_{x \to a} f(x) = \lim\limits_{x \to a}
 g(x)=0$ . This says something about values of $f$ and $g$ near $a$ but not at $a$ . If we define $f(a)=g(a)=0$ (changing the previous values of $f(a)$ and $g(a)$ , if necessary), then $f$ and $g$ are continuous at $a$ . If $a<x<a+\delta$ , then the Mean Value Theorem and the Cauchy Mean Value
Theorem apply to $f$ and $g$ on the interval $[a,x]$ (and a similar
statement holds for $a-\delta<x<a$ . Why is the following step necessary First applying the Mean Value Theorem to $g$ , we see that $g(x) \neq
> 0$ , for if $g(x)=0$ there would be some $x_1$ in $(a,x)$ with $g'(x_1)=0$ , contradicting $(2)$ . Didn't we already know this fact, precisely from $(2)$ ? Now applying the Cauchy Mean Value Theorem to $f$ and $g$ , we see that there is a number $\alpha_x$ in $(a,x)$ such that $$[f(x)-0]g'(\alpha_x)=[g(x)-0]f'(\alpha_x)$$ or $$\frac{f(x)}{g(x)}=\frac{f'(\alpha_x)}{g'(\alpha_x)}$$ Now $\alpha_x$ approaches $a$ as $x$ approaches $a$ , because $\alpha_x$ is in $(a,x)$ . I understand this as: since $|\alpha_x-a|<|x-a|$ so $0<|x-a|<\delta \implies 0<|\alpha_x-a|<\delta$ Since we are assuming that $\lim\limits_{y \to a} \frac{f'(y)}{g'(y)}$ exists, it follows that $$\lim\limits_{x \to a} \frac{f(x)}{g(x)}=\lim\limits_{x \to a}
 \frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{y \to a}
 \frac{f'(y)}{g'(y)}$$ I don't quite understand how to prove this last step. $\lim\limits_{x \to a} \frac{f'(\alpha_x)}{g'(\alpha_x)}=l$ means $$\forall \epsilon\ \exists \delta>0\ \forall x, 0<|x-a|<\delta  \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right |<\epsilon$$ Where is the implication from $0<|\alpha_x-a|<\delta$ to $\left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right |$ ?","['limits', 'calculus', 'derivatives', 'proof-explanation']"
4452171,Example that M* is not reflexive,"Let $R$ be a noetherian ring. Set $(-)^\ast={\rm Hom}_R(-,R)$ . For each $R$ -module $N$ , let $\pi_N:N\rightarrow N^{\ast\ast}$ be the map which maps $n\in N$ to $(f\mapsto f(n))$ . $N$ is called reflexive if $\pi_N$ is isomorphism. Question: Does there exist a finitely generated $R$ -module $M$ such that $M^\ast$ is not reflexive? I guess the answer is yes. But I can’t find any example. For each $R$ -module $N$ , we can check directly that the composition $N^\ast\xrightarrow{\pi_{N^\ast}}N^{\ast\ast\ast}\xrightarrow{(\pi_N)^\ast}N^\ast$ is identity. In particular, $\pi_{N^\ast}$ is always invective.  I searched the internet. It is proved in Yoshino’s paper that if $R$ is Gorenstein in depth one, then $M^\ast$ is reflexive for each finitely generated $R$ -module $M$ ; see Lemma 4.4 of HOMOTOPY CATEGORIES OF UNBOUNDED COMPLEXES OF PROJECTIVE MODULES . Thank you in advance.","['homological-algebra', 'cohen-macaulay', 'abstract-algebra', 'linear-algebra', 'commutative-algebra']"
4452203,"Are these equations ""properly"" defined differential equations? (finite duration solutions to diff. eqs.)","Are these equations properly defined differential equations? Modifications were made to a deleted question to re-focus it. I am trying to find out if there exists any exact/accurate/non-approximated mathematical framework to work with physics under the assumption that phenomena have finite ending-times, instead of solutions that at best vanishes at infinity. Intro Definition 1 - Solutions of finite-duration: the solution $y(t)$ becomes exactly zero at a finite time $T<\infty$ by its own dynamics and stays there forever after $(t\geq T\Rightarrow y(t)=0)$ . So, they are different of just a piecewise section made by any arbitrarily function multiplied by rectangular function: it must solve the differential equation in the whole domain. (Here I just pick the shorter name its look more natural to me for several I found: ""finite-time"", ""finite-time-convergence"", ""finite-duration"", ""time-limited"", ""compact-supported time"", ""finite ending time"", ""singular solutions"", ""finite extinction time"", among others). Recently, in this answer I figure out that the following autonomous scalar ODE : $$\dot{y}=-\sqrt[n]{y} \tag{Eq. 1}\label{Eq. 1}$$ Could stand the finite-duration solutions, for any real-valued $n>1$ and positive ending time $T>0$ : $$ y(t)=\left[\frac{(n-1)}{n}\left(T-t\right)\right]^{\frac{n}{(n-1)}}\,\theta(T-t)\equiv y(0)\left[\frac{1}{2}\left(1-\frac{t}{T}+\left|1-\frac{t}{T}\right|\right)\right]^{\frac{n}{(n-1)}} \tag{Eq. 2}\label{Eq. 2}$$ with $\theta(t)$ the standard Heaviside step function . But later, working with the solution of \eqref{Eq. 2} I figure out that if I take its derivative I will end with: $$\begin{array}{r c l}
\dot{y} & = & -\left[\frac{(n-1)}{n}\left(T-t\right)\right]^{\frac{1}{(n-1)}}\,\theta(T-t)+\underbrace{\left[\frac{(n-1)}{n}\left(T-t\right)\right]^{\frac{n}{(n-1)}}\,\delta(T-t)}_{=\,0\,\text{due}\,(x-a)^n\delta(x-a)=0} \\
& = & -\left[\frac{(n-1)}{n}\left(T-t\right)\right]^{\frac{1}{(n-1)}}\,\theta(T-t) \\
& = & -\frac{n}{(n-1)}\frac{1}{(T-t)}y(t)
\end{array}$$ Which implies that the solutions $y(t)$ are also solving the non-autononous scalar ODE (is time-variant since the time $t$ is explicitely involved): $$\dot{y}+\frac{n}{(n-1)}\cdot \frac{y}{(T-t)} = 0 \tag{Eq. 3}\label{Eq. 3}$$ This last equation of \eqref{Eq. 3} resembles very much this another question were I found that the finite duration solution: $$z(t)=\exp\left(\frac{t}{(t-1)}\right)\cdot\theta(1-t) \tag{Eq. 4}\label{Eq. 4}$$ can solve the equation: $$\dot{z}+\frac{z}{(1-t)^2} = 0,\quad z(0)=1\tag{Eq. 5}\label{Eq. 5}$$ So I started to try another finite-duration solutions to see if their differential equations follows the same ""trend"" of having terms $(T-t)$ on it: which means that an standard integration constant is becoming part of the differential equation as its finite ending time $T$ . Main analysis On this question is found that similarly for equations \eqref{Eq. 3}: $$x(t)=\frac{1}{4}(2-t)^2\theta(2-t) \tag{Eq. 13}\label{Eq. 13}$$ is a finite duration solution to: $$\dot{x}=-\text{sgn}(x)\sqrt{|x|},\,x(0)=1\tag{Eq. 14}\label{Eq. 14}$$ so I will modify this solution to see what happens: $$q(t) = \frac{1}{4}(2-t)^2\cos(20\,t)\,\theta(2-t)\tag{Eq. 6}\label{Eq. 6}$$ which can be verified is solves the equation: $$\ddot{q}+\frac{4}{(2-t)}\cdot \dot{q}+\left(400+\frac{6}{(2-t)^2}\right)\cdot q = 0,\quad q(0)=1,\,\,\dot{q}(0)=-1\tag{Eq. 7}\label{Eq. 7}$$ Again, on \eqref{Eq. 7} there terms $(2-t) \cong (T-t)$ indicating that there exists solutions that are behaving like finite duration solutions (see plot of \eqref{Eq. 6}). But these ""construction"" of making finite duration solutions where these two conditions are met: The differential equation support the zero solution, and The differential equation also has at least one singular point in time $t=T$ where $x(T)=\dot{x}(T)=0$ both mentioned on this answer , becomes less obvious where more than one variable is considered. Let think and a slight modification to the function of this answer : $$U(t,x) = \exp\left(1-\frac{1}{(1-(x-t)^2)}\right)\theta(1-(x-t)^2) \tag{Eq. 8}\label{Eq. 8}$$ it will show to be compact supported in space as it moves in the time variable, fulfilling the standard wave equation with a velocity $c=1$ : $$\left(\frac{1}{c^2}\frac{\partial^2}{\partial t^2}-\nabla^2\right)U(t,x)=0 \overset{c=1}{\Rightarrow} U_{tt}-U_{xx}=0 \tag{Eq. 9}\label{Eq. 9}$$ Here the partial equation doesn't show the term $(T-t)$ , but it does make sense if the countour plot of \eqref{Eq. 6} is viewed: It is only compacted-supported in space and it is still never-ending in time. But if I test this modified version of \eqref{Eq. 8} mixed with the terms of \eqref{Eq. 2}: $$E(t,x) = U(t,x)\cdot \frac{1}{4}(2-t)^2\,\theta(2-t) = \exp\left(1-\frac{1}{(1-(x-t)^2)}\right)\theta(1-(x-t)^2)\cdot\frac{1}{4}(2-t)^2\,\theta(2-t) \tag{Eq. 10}\label{Eq. 10}$$ where this time it can be seen in its plot that is indeed behaving as also a finite duration solution in time: It could be verified that is solves the equation: $$E_{tt}-E_{xx}+\frac{4}{(2-t)}E_{t}+\frac{6}{(2-t)^2}E = 0 \tag{Eq. 11}\label{Eq. 11}$$ within the domain \eqref{Eq. 10} is different from zero (see here ). So again the terms $(T-t)$ are required for having finite-duration solutions. I beforehand know that some of these functions should be defined piecewise because has a undefined value, as example: $$ f(t) = \begin{cases} \exp\left(\frac{t}{(t-1)}\right),\quad t < 1 \\
                        0, \qquad t\geq 1 \end{cases} \tag{Eq. 12}\label{Eq. 12}$$ instead of the one-line version of \eqref{Eq. 4}, so I ask you now to forgive this shortcut for now: the functions still continuous on these points and for me are easy to work with them finding their differential equations on this way (I have a extended discussion of this here ), but if this annoying you, please just assume it is defined piecewise such as the function is zero in the undefined point. This comment is important, since I want to focus the discussion in the other singularities that appears on these equations (and sometimes the point could be coincident), and are the discontinuities on the differential equations due the terms $(T-t)$ . The questions 1) Are all these differential equations with terms $(T-t)$ ""well-defined"" as differential equations? 2) Which are the name of this kind of differential equations? (noting here there are ODEs and PDEs with these terms - hope there are any references) 3) Does this $(T-t)$ terms on the denominator make these differential equation Non-Lipschitz? Here looking for parallels with the papers by V. T. Haimo: Finite Time Differential Equations and Finite Time Controllers . 4) Does this $(T-t)$ terms on the denominator make possible the existence of solutions of finite duration? Remembering here that solutions of finite duration don't fulfill uniqueness (see the recently cited papers for better explanation), a singularity in my opinion makes sense since there are multiple values rising from a single point, which is similar with traditional mistakes made when dividing by zero - but formally speaking, I have no idea of what it is happening. Motivation I am trying to find a mathematical framework to work with dynamical systems under the assumption: the system achieves an end in finite time , which seems really logical to me through daily life experience, but I have found this is really messy when it is translated to math (I still didn't find a related framework). Current physics solutions at best ""vanishes at infinity"", which is not exact but fairly good for almost every possible application, so good, that I have had discussions with many people that affirm as true that movement last forever, which at least under the assumption that thermal noise is Gaussian, you can say if it still moving (neither if it has stopped), since already all the information is lost: Gaussian distribution is the Maximum entropy probability distribution for a phenomena with finite mean and finite power. Don't meaning the assumption of never ending movement is wrong, but affirming is true is indeed false (see the comments of this question to a more detailed discussion applied to the pendulum movement). This subtle distinction is seen for example, in the Galton board toy, where many people affirm that somehow order have risen from chaos, due the appearance of the Normal Distribution, when the right interpretation is that all the information of the path each ball have take when falling is already lost, and the Gaussian distribution is the maximum possible disordered way of been for this system (to have an intuition of why there is a lobe in the middle, take a look to Concentration inequality ). But nowadays there is a lot of theoretical physics squeezing their models to the very last drop and many counter-intuitive things are reaching mainstream: many Youtube videos talking about the ""nonexistence"" of time (is an illusion), and also many videos about the impossibility of current physics laws to determine ""the direction of the arrow of time"", and I would like to know if any of this things is due current models are not aware of the effects of keeping the assumption that phenomena have finite ending times. As example, in all the differential equations of this question there indeed a clue of which way the time is considered to be flowing (due the term $(T-t)$ ), and for the solutions, going backwards in time will lead to spontaneous rise from zero (being zero from a non-zero meassure interval in time). I know that current models are on the way they are because the models were designed to keep holding uniqueness of solutions, which if not could lead to a lot of struggles (see here some of this issues with the Norton's Dome example). But I also know now that for the existence of solutions of finite duration uniqueness of solutions must be abandoned, and I would like to know if this were known by physicists, if the current models would still been having their current formulations (to keep this on perspective, main models have about a hundred years old, and the first papers about finite duration solutions I have found are from 1985). And maybe because they are ""relativately new"", I only found these finite duration solutions on control theory papers (where they started assuming a high-knowledge background from the reader), and every book I find on differential equation keep their analysis tied to uniqueness, so they don't touch this topic. With this, any related book reference will be useful. Answering two firsts comments by @Desura Some user named @Desura mentioned the existence of the Frobenius method and the Fuchs' theorem for solving differential equations of the form $u''+\frac{p(t)}{t}u'+\frac{q(t)}{t^2}u=0$ , and also what is a Regular singular point . On all these pages you can see differential equations that really resemble the ones I have displayed above, so very much for share it. I would like to comment why I think these kind of methods aren't able to find solutions of finite duration by ""themselves"", and also how I think I am already using them through Wolfram-Alpha. On the mentioned papers by Vardia T. Haimo (1985) Finite Time Controllers and Finite Time Differential Equations , where only scalar autonomous ODEs of 1st and 2nd order are studied ( $x'=G(x)$ or $x''=G(x',x)$ ), then by assuming without lost of generality that $T=0$ , and also assuming that the right-hand-side of the ODE is at least class $C^1(\mathbb{R}\setminus\{0\})$ , the author mentioned that: ""One notices immediately that finite time differential equations cannot be Lipschitz at the origin. As all solutions reach zero in finite time, there is non-uniqueness of solutions through zero in backwards time. This, of course, violates the uniqueness condition for solutions of Lipschitz differential equations."" So, if the author is right, NO Lipschitz 1st or 2nd order scalar autonomous ODEs could stand solutions of finite duration , so classic linear ODEs are discarded. But reaching zero in finite time have also another consequence, and is that also are discarded classical solutions through Power Series which are analytical in the whole domain , since there exist a non-zero measure compact set of points identical to zero, the only analytical function that could support this matching is the zero function, due the Identity Theorem . Because of this last paragraph, is why I believe that a method that find the coefficients for a function of the form $\sum_k a_k(T-t)^k$ aren't able to describe a finite duration solution ""alone"", which is a traditional method for finding solution to ODEs (as the method shown by @Desura). But fortunately, being analytic in a piecewise part of the time domain and zero on the part ""looks like"" working if the two restrictions are met (now labeled after \eqref{Eq. 7}), as they are used for finding the solution of the now labeled \eqref{Eq. 14}, which is demonstrated on this answer . Here is where I am using the traditional ways of finding solutions to differential equations, to find the ""non-zero section"". But How this ""construction"" by ""stitching solutions"" is formally working, or if they are being or not formal solutions to the differential equations is something I don´t know (I am ""highly abusing"" of an answer without any formal prove - but because it ""make sense""), neither when we have to ""choose"" one solution or another (since there are multiple options now - at least with this ""construction"": the full solutions, the finite duration alternative, or the other piece that rise spontaneously from zero). This is why I am looking for a framework, but because as requirement uniqueness must be left aside , traditional books in differential equations don't touch this kind of solutions. Even so, on the mentioned papers of V. T. Haimo, this kind of solutions aren't studied at all, and more interestingly, there are given a 2nd order ODE example where is shown that depending on its parameters, the differential equation could allow having finite duration solutions, but outside ""this restriction"" the solutions are not of finite duration (unfortunately no exact solution is given). Last update I have just found the following papers under the term sublinear damping that shows example in physics of equations really similar to one showed on the papers by V. T. Haimo, containing terms of the form $\text{sgn}(\dot{x})|\dot{x}|^\alpha$ which are achieving finite extinction times without having the form of the Frobenius method with displaced terms $(T-t)$ : ""A note on the dynamics of an oscillator in the presence of strong friction"" - H.Amann & J.I.Diaz ""A conservation law with spatially localized sublinear damping"" - Christophe Besse & Sylvain Ervedoza ""Behavior of Solutions of Second-Order Differential Equations with Sublinear Damping"" - J. Karsai & J. R. Graef I don't know if maybe after knowing their exact solutions, they could be transformed on differential equations similar of the asked on the main question, as it was done for carrying \eqref{Eq. 2} to become into \eqref{Eq. 3} (I believe implying with this their decaying is exponential $\propto t^{-\beta},\,\beta>0$ for some $\beta$ ), maybe it could be proved with what is shown on the papers. Other similar examples were listed into this another question .","['ordinary-differential-equations', 'singular-solution', 'partial-differential-equations', 'finite-duration', 'dynamical-systems']"
4452247,Varieties with no vector bundles,"What are some examples of algebraic varieties over the complex numbers with no (algebraic) vector bundles other than the trivial ones? The only example I can think of is $ \mathbb{A}^n_{\mathbb{C}} $ and this is a very deep example - see the Quillen-Suslin theorem. The inspiration for this question is the following fact that I learnt quite late in life: The 3-sphere $ S^3 $ does not have any topological real vector bundles other than the trivial ones. In fact, the excellent answer to this question tells us much more.","['vector-bundles', 'algebraic-geometry', 'algebraic-topology']"
4452350,How to construct a locally injective entire holomorphic function such that the modulus is axially symmetric?,"Let $f$ be an entire holomorphic function, with $f'$ vanishing nowhere in the complex plane. Is it possible to construct such a function such that $|f(z)|$ is symmetric with respect both $x$ -axis and $y$ -axis? My attempt is trying to find a function of the form $f(z)=ze^{g(z^2)}$ , where $g$ is transcendental holomorphic and the coefficients of Taylor expansion of $g$ are real. Such function satisfies $|f(z)|=|f(-z)|=|f(\bar{z})|$ , but in order that $f'$ is nowhere vanishing, we need to guarantee that $1+2z^2g'(z^2)$ has no zeros. I've no idea how to fufill this step. Maybe we cannot construct in this way, because if $1+2z^2 g'(z^2)$ has no zeros, then there exists another holomorphic function $h$ such that $1+2z^2g'(z^2)=e^{h(z)}$ . This seems to be impossible. Any ideas and comments are fully apreciated.",['complex-analysis']
4452396,$\int _0^1f^2\left(x\right)dx-2\int _0^{\sqrt{3}-1}\:\left(x+1\right)f\left(x\right)dx\:+1=0$,"Let $   f  $ be an increasingly continuous function over $[0,1]$ such that: $$\int _0^1(f\left(x\right))^2dx-2\int _0^{\sqrt{3}-1}\:\left(x+1\right)f\left(x\right)dx\:+1=0$$ Find all functions $f$ with these properties. Attempt.: First, the function $f$ is integrable therefore the computations with integrals work there.
Then I did the following: $$\int _0^1(f\left(x\right))^2dx-2\int _0^{\sqrt{3}-1}\:\left(x+1\right)f\left(x\right)dx\:+1=$$ $$\int _0^{\sqrt{3}-1}(f\left(x\right))^2dx+\int _{\sqrt{3}-1}^1(f\left(x\right))^2dx-2\int _0^{\sqrt{3}-1}\:\left(x+1\right)f\left(x\right)dx\:+1=$$ $$\int _0^{\sqrt{3}-1}\left(f\left(x\right)-\left(x+1\right)\right)^2dx+\int _{\sqrt{3}-1}^1(f\left(x\right))^2dx-\int _0^{\sqrt{3}-1}\left(x+1\right)^2dx+1=0$$ Since $$-\int _0^{\sqrt{3}-1}\left(x+1\right)^2dx=-\frac{6\sqrt{3}-10}{3}-3+\sqrt{3}$$ Then $$\int _0^{\sqrt{3}-1}\left(f\left(x\right)dx-\left(x+1\right)\right)^2dx+\int _{\sqrt{3}-1}^1(f\left(x\right))^2dx-\frac{6\sqrt{3}-10}{3}-2+\sqrt{3}=0$$ $$\int _0^{\sqrt{3}-1}\left(f\left(x\right)dx-\left(x+1\right)\right)^2dx+\int _{\sqrt{3}-1}^1(f\left(x\right))^2dx+\frac{-3\sqrt{3}+4}{3}=0$$ How should I continue from there? I got stuck there. I am not sure this is going to lead me somewhere. Maybe I should have started differently. Has somebody an idea what the functions look like?","['continuity', 'functions', 'functional-analysis']"
4452409,Find all values of $m$ such that the equation $3^{x^2 + 2mx + 4m - 3} - 2 = \left|\dfrac{m - 2}{x + m}\right|$ has two distinct roots on $[-4; 0]$.,"Consider the equation $3^{x^2 + 2mx + 4m - 3} - 2 = \left|\dfrac{m - 2}{x + m}\right|$ . All values of $m$ such that the above equation has two distinct roots on $[-4; 0]$ are $$\begin{aligned} &&A. \, m \in [1; 3] &&B. \, m \in (1; 3)\\ &&C. \, m \in [1; 3] \setminus \{2\} &&D. \, m \in (-\infty; 1] \cup [3; +\infty) \end{aligned}$$ [For context, this question is taken from an exam whose format consists of 50 multiple-choice questions with a time limit of 90 minutes. Calculators are the only electronic device allowed in the testing room. (You know those scientific calculators sold at stationery stores and sometimes bookstores? They are the goods.) I need a solution that works within these constraints. Thanks for your cooperation, as always. (Do I need to sound this professional?) By the way, if the wording of the problem sounds rough, sorry for that. I'm not an expert at translating documents.] Let's do this question by process of elimination first. For $m = 2$ , the equation becomes $$3^{x^2 + 4x + 5} - 2 = 0 \implies x^2 + 4x + (5 - \log_32) = 0$$ And since the simplified discriminant (that's what it's called here) is $\Delta = 2^2 - (5 - \log_32) = \log_32 - 1 < 0$ , the above equation doesn't have any roots for $m = 2$ , which means we can eliminate choices $A$ and $B$ . Next up, for $m = 0$ , the equation becomes $$\begin{aligned} 3^{x^2 - 3} - 2 = \left|\dfrac{-2}{x}\right| &\iff \dfrac{3^{x^2}}{27} = 2 \times \left(1 + \dfrac{1}{|x|}\right) \iff \dfrac{3^{x^2}}{54} = \dfrac{|x| + 1}{|x|}\\ &\iff 3^{x^2}|x| = 54(|x| + 1) \iff (3^{x^2} - 54)|x| - 54 = 0 \end{aligned}$$ Consider function $f(x) = (3^{x^2} - 54)|x| - 54, x \in [-4; 0]$ . Using the TABLE function of my calculator, (isn't technology amazing?) we have this table below, (who could have guessed?) It seems that there's only one root on $[-4; 0]$ , which is $x = -2$ . Therefore, $m = 0$ is not one of the viable values. It can be concluded that the correct answer is $C. m \in [1; 3] \setminus \{2\}$ . ""You already found the correct answer. What're you asking then?"" You might think. Well, of course, I want to know how to actually solve this problem, still within the time limit required. Do I actually know what to do first? Uhhh, hmmm~ I wish... Anyhow, thanks for reading, (and even more so if you could help), have a great tomorrow, everyone~","['exponentiation', 'functions', 'exponential-function']"
4452445,Utility of the coordinate free definition of the derivative on manifolds.,"Preface: I am not an expert on the topic of smooth manifolds, nor do I have the perspective gained from knowing many theorems proven on smooth manifolds. Please try to look at the problem from the perspective of someone learning this topic for the first time. For future reference let $f:M \rightarrow N$ be a smooth function where $M$ and $N$ are smooth manifolds. Let the derivative of $f$ at $p\in M$ be denoted as the usual $df_p:T_pM \rightarrow T_{f(p)}N$ . Question: In what ways is it advantageous to define $df_p$ in a coordinate free way? I cannot see the motivation behind such a definition. To me there are three defining characteristics of the derivative at a point of a function between $\mathbb{R}^n$ and $\mathbb{R}^m$ . The derivative of a function offers an approximate relationship between perturbations of the input and corresponding perturbations of the output. This relationship is linear. This approximation is better than any other linear approximation. Let's check how many of these properties actually survive the abstraction process needed for a coordinate free generalization of the derivative on manifolds. Property 1: To interpret a tangent vector as a ""perturbation"" of the point p, we would need some sort of canonical mapping from $T_pM$ to $M$ . I do not know of any such mapping which doesn't rely on a local coordinates. Since we can't interpret the inputs and outputs of $df_p$ as perturbations, I assume that the notion of local approximation is abandoned as well. How could $df_p$ ""approximate"" $f$ when we don't even have a way to relate their respective domains and ranges. Thus I will assume that this property of the derivative has been stripped away in the abstraction process. Property 2: This is satisfied. Property 3: Since we have determined that without coordinates $df_p$ does not act as an approximation of $f$ , it cannot be the ""best"" approximation. Thus only one property survives. $df_p$ seems to be (to my unexperienced eyes) a glorified linear mapping with no substantial importance until local coordinates are introduced. Thus why don't we simply define the derivative in a coordinate dependent manner? This would also make the definition of the tangent space much simpler. The ""geometric tangent space"", $\mathbb{R}^n_p$ , as defined in Lee's ""Smooth Manifolds"" would suffice as a definition since we are assuming some coordinate representation. Is anything lost by doing this? In response to nicrot000’s answer: To summarize, nicrot000 states in his answer that it is the general trend of differential geometry to express concepts in coordinate free ways. This allows said concepts to be independent of arbitrary decisions (i.e. choice of local coordinates). /end of summary\ I guess my question is rooted in the fact that I’m not convinced arbitrary decisions are a bad thing in terms of defining the derivative. From my perspective, $df_p$ can only be interpreted in some meaningful way after local coordinates have been assumed. An analogy: Suppose we have a computer which we will refer to as the “idea machine”. The sole purpose of this computer is to run a program which, on command, generates a random idea. After you have clicked “generate idea”, the program will prompt you to choose from a set of languages (all of which can be “smoothly” translated between each other) in which the idea will be displayed on the screen (the choice of a language corresponds to choosing a local coordinate chart). There is however the option to choose no language (corresponding to coordinate free). If no language is chosen, nothing will be displayed on the screen and the idea will forever stay encoded by the ones and zeros in the hardware. We do however know that the idea exists independent of our arbitrary choice of language (hooray!). At the end of the day, if we want to know what the idea was then in some form or another we would have to use a “language” to interpret the ones and zeros in the hardware. Interpretation of the analogy: The idea produced by the idea machine represents the information encoded by the coordinate free function $df_p$ . The set of languages which one can choose from represents the smooth structure. Choosing a language to display the idea corresponds to choosing a coordinate chart to represent $df_p$ . We can choose to use a coordinate free representation however I do not see the purpose of this just as choosing no language is useless. From a developer standpoint, why would we go through the work of adding the no language option? i.e. why would we go through the work of developing a coordinate free definition of $df_p$ .","['differential', 'smooth-manifolds', 'definition', 'soft-question', 'differential-geometry']"
4452486,The Maclaurin series of $1-(1-\frac{x^2}{2} + \frac{x^4}{24})^{2/3}$ has all coefficients positive,"It was shown in a previous post that the Maclaurin series of $1 - \cos^{2/3} x$ has positive coefficients. There @Dr. Wolfgang Hintze: has noticed that the truncation $1- \frac{x^2}{2} + \frac{x^4}{24}$ can be substituted for $\cos x$ (  seems to be true for all the truncations). The proof is escaping me.
Thank you for your attention! $\bf{Added:}$ Thomas Laffey in this paper directs to a proof of the fact that if $a_1$ , $\ldots$ , $a_n\ge 0$ then $\alpha = \frac{1}{n}$ makes the following series positive: $$1- (\prod_{i=1}^n (1- a_i x))^{\alpha}$$ Numerical testing suggests that $\alpha = \frac{\sum a_i^2}{(\sum a_i)^2} \ge \frac{1}{n}$ works as well ( see the case $n=2$ tested here ). So in our case, instead of $\alpha = \frac{1}{2}$ we can take $\alpha = \frac{2}{3}$ . Clearly, this would then be the optimal value. This would be a test case for $n=2$ . The result for $\cos x$ used the special properties of the function ( solution of a certain differential equation of second order). Maybe $1- x/2 + x^2/24$ is as general as any quadratic with two positive (distinct) roots.","['calculus', 'taylor-expansion']"
4452579,Show that $2x-\sin{(2x)}-2(\sin{x})\sqrt{x^2-\sin^2{x}}\geq0$ for $x>0$.,"My question: Show that $2x-\sin{(2x)}-2(\sin{x})\sqrt{x^2-\sin^2{x}}\geq0$ for $x>0$ . (Feel free to ignore the rest of this message.) Context I was trying to solve the following problem, which I made up. The diagram shows a triangle and circular segment sharing a side. The lengths of the sides and arc are shown. Show that the area of the segment cannot be less than the area of the triangle. The problem is easily solved using the isoperimetric theorem . However, I wanted a solution that does not use the isopermetrmic theorem, because I am a high school teacher and I wanted to give this problem to my students, who do not know the isoperimetric theorem. Trying to solve the problem without the isoperimetric theorem, I let the central angle of the arc (not to be confused with the angle at the bottom of the triangle) be $2x$ . Setting the area of the segment to be greater than or equal to the area of the triangle, and simplifying, I got $$2x-\sin{(2x)}-2(\sin{x})\sqrt{x^2-\sin^2{x}}\geq0\text{ for }0<x<\pi.$$ Using desmos, I realized that something stronger can be shown: $$2x-\sin{(2x)}-2(\sin{x})\sqrt{x^2-\sin^2{x}}\geq0\text{ for }x>0.$$ So that is where my question comes from. I am not looking for answers to the question about the segment and triangle, which I mention only as context for the question in the title, which I think is interesting by itself. My attempts I have tried trigonometric identities, squaring both sides, even Maclaurin series, to no avail. It seems like there is no simple solution.","['trigonometry', 'inequality']"
4452655,Why does the monotone convergence fail here,"I am looking at the example $f_{n}(x) = n \chi_{(0, \frac{1}{n}]}$ . This converges to $0$ pointwise and graphing it out we can see that its a series of rectangles of area 1 but with growing height. I can see that the dominated convergence theorem would fail, since I can't find a bound for all $f_{n} as the heights are getting arbitrarily large. But I cannot see why the Monotone convergence theorem fails. It seems that the sequence is monotone and the value of each individual integral is finite?","['measure-theory', 'lebesgue-integral', 'monotone-functions']"
4452704,"What is the smallest unseen number in an iid sample? (From ""A number NOBODY has thought of - Numberphile"")","In this Numberphile video , the question: ""What is a number nobody has thought of?"" is addressed. The method is as follows: Estimate a number $N$ as the number of times humans have
thought of numbers Estimate a probability distribution $\mathbb{P}$ for what number you think of when you have a thought
and suppose that each though is an independent draw from this Calculate the distribution of the maximum of $N$ independent draws
from $\mathbb{P}$ More precisely, this is the answering the question: ""What is the largest number someone has thought of?"" (and if you add 1 you will surely get a number nobody has thought of). A more interesting question in my view is ""What is the smallest number nobody has thought of?"". If we agree with step 1 and step 2 from the Numberphile video, then this ammounts to: What is the distribution of the smallest integer $I$ so that $N$ iid draws from a distribution $\mathbb{P}$ on the integers does not contain $I$ . (In other words: the event that $\{I > x\}$ is the event that the numbers $\{1,\ldots,x\}$ all appear in our sample). Surely someone has thought of this before....is there an elegant way to calculate this?","['statistics', 'recreational-mathematics', 'probability-theory', 'probability', 'random-variables']"
4452780,"A circle, two tangents and a triangle - finding incircle center of triangle",Let be $K$ a circle. Two tangents  touch the the circle at $C$ and $D$ cross at a point $E$ . So there is a triangle $CDE$ . How do I show that the incircle center (where the bisectors of the triangle cross) is ON the circle $K$ ?? It's clear if it is drawn out..but I have  no idea how to show that.,['geometry']
4452791,Finding minimum value of $x^2+y^2+xy+x-4y+9$,"What is the minimum value of $f(x,y)=x^2+y^2+xy+x-4y+9$ ? I tried completing squares, $$x^2+y^2+xy+x-4y+9=\frac12(x^2+2xy+y^2+x^2+2x+1+y^2-8y+16+1)=\frac12[(x+y)^2+(x+1)^2+(y-4)^2+1]$$ But not sure how to continue.","['optimization', 'multivariable-calculus']"
