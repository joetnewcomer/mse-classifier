question_id,title,body,tags
1879817,Upper bound on Lipschitz continuous hessian,"Lipschitz continuous hessian for the twice differentiable function $f$ is defined as, for any $x, y$, $$\|\nabla^2 f(x) - \nabla^2 f(y)\| \leq M\|x-y\|$$ for some postive $M$, how to derive the upper bound of $|f(y) - f(x) - \nabla f(x)^T(y-x) - \frac{1}{2}(y - x) \nabla^2 f(x)^T (y-x)| \leq \frac{M}{6}\|y-x\|^3$","['inequality', 'convex-optimization', 'convex-analysis', 'functions']"
1879862,"Spectrum of $(Mf)(t):=tf(t)$ for $f$ is a complex-valued continuous function on $[0,1]$.","Let $X:=C([0,1],\mathbb{C})$ be equipped with $\Vert f\Vert_{\infty}:=sup_{t\in [0,1]}\vert f(t)\vert$ and define $M:X\to X$ by $(Mf)(t):=tf(t)$. Prove that the spectrum of $M$ is $\sigma(M)=[0,1]$ I solved this exercise but my solution is completely different than the one we got in the class. Is mine still correct? My solution: Assume there is some $\tilde{\lambda}\in [0,1]$ such that $\tilde{\lambda}\mathbb{1}-M$ is bijective. Then since $X$ is a Banach space by the inverse operator theorem $(\tilde{\lambda}\mathbb{1}-M)^{-1}$ is bounded. But this is not possible since $(\tilde{\lambda}\mathbb{1}-M)f(t)=(\tilde{\lambda}-t)f(t)$ and so its inverse diverges at $t=\tilde{\lambda}$. Solution from the class: Assume $\lambda\in \mathbb{C}\backslash [0,1]$ and define $R_{\lambda}f(t):=\frac{1}{\lambda-t}f(t)$. By assumption $inf_{t\in[0,1]}\vert \lambda - t\vert >0$. Hence $C:=\sup_{t\in[0,1]}\frac{1}{\lambda-t}<\infty$ is bounded. So $\Vert R_{\lambda}f\Vert_{\infty}=sup_t \vert \frac{1}{\lambda-t}f(t)\vert\leq C \Vert f\Vert_{\infty}$ $\Longrightarrow$ $\Vert R_{\lambda}\Vert=\sup_f \frac{\Vert R_{\lambda}f\Vert_{\infty}}{\Vert f\Vert_{\infty}}\leq C$ hence $R_{\lambda}$ is bounded. Obviously $(\lambda \mathbb{1}-M)R_{\lambda}=R_{\lambda}(\lambda \mathbb{1}-M)=\mathbb{1}$ so $\forall \lambda \in \mathbb{C}\backslash [0,1]$ $\lambda\mathbb{1}-M$ has an inverse.","['functional-analysis', 'spectral-theory']"
1879866,Double dual of torsion-free sheaves are locally free?,"I was reading Nakajima's ""Lectures on Hilbert schemes of points on
surfaces"". In Chapter 2 he says that if $E$ is a torsion-free sheaf then $E^{**}$ is locally free. I guess that by `torsion-free' sheaf he meant on every open sets $U \subset X = \mathbb{P}^2$, $E(U)$ is a torsion-free $\mathscr{O}_X(U)$-module. I also think that duality is defined by $E^*= \text{Hom}_{\mathscr{O}_X}(E,\mathscr{O}_X)$. But then how do I show that $E^{**}$ is locally free? Additional confusion on the matter: I'm not sure if I'm allowed to assume that $E$ is quasi-coherent or coherent here. He mentioned at the start of the chapter that $\text{rank} E = r$, which suggest that $E$ has to be coherent, but by talking about the rank of $E$ don't we already assume $E$ is locally free? How can we define a rank on a module that is not free?","['modules', 'sheaf-theory', 'algebraic-geometry']"
1879870,Help proving a function is continuously differentiable,"So i have a function $$f:\mathbb{R} \rightarrow \mathbb{R}$$  that's differentiable at x=0 and for $x,y \in \mathbb{R}$ let  this be true:
$$f(x+2y)=2f(x)f(y)$$ So how can i prove that this function is continuously differentiable? 
I thought i could use any of these theorems: Lagrange's and Rolle's, but i don't know which one and how, perhaps should i show at first it's differentable and then that derivative is continous?","['derivatives', 'calculus']"
1879891,Why does this optimization problem have a closed form solution that resembles least squares so much?,"Consider the following optimization problem $$\min_{\mathbf{Q}} \sum\limits_{i=1}^{n}{\|{{\mathbf{b}}_{i}}-{{\mathbf{Q}}^{T}}{{\mathbf{x}}_{i}}\|^2}+\lambda \|\mathbf{Q}\|^{2}$$ where $\mathbf{b_i}  $ is an $r$-dimensional vector, $\mathbf{Q}$ is an $n \times r$ matrix and $\mathbf{x_i} $ is an $n$-dimensional vector. The closed form solution is $$\mathbf{Q}={{(\mathbf{S}{{\mathbf{S}}^{T}}+\lambda \mathbf{I})}^{-1}}\mathbf{S}{{\mathbf{B}}^{T}}$$ Why does it resemble the least squares solution so much? How to conduct that?","['derivatives', 'least-squares', 'optimization', 'matrix-calculus', 'regularization']"
1879904,Monotonic property of solution of a linear (second-order) differential equation,"Consider the following differential equation
\begin{equation}
y''+q(t) y=0 ,
\end{equation}
where q(t) is a continuous function $\leq 0\;\; \forall t \in \mathbb{R}$. I'm trying to prove that each non-constant solution such that $y(0)=0$ is strictly monotone. I have tried integrating between 0 and t but in this way I can't show anything useful in fact by using the weighted average theorem I get
\begin{equation}
y'(t)=y'(0)-y(\xi_t)\int_{0}^{t}q(x)dx .
\end{equation}
I'm wondering if I should continue to prove that $y'>$ or $<0$ in a dense subset of $\mathbb{R}$ or if I should use a different approach.","['ordinary-differential-equations', 'monotone-class-theorem', 'monotone-functions']"
1879914,Determinant of a $4 \times 4$ matrix $A$ and $(\det(A))^5$,"Calculate $\det(A)$ and $\det(A)^5$:
$$A= \begin{bmatrix}a&a&a&a\\a&b&b&b\\a&b&c&c\\a&b&c&d\end{bmatrix}$$ I found $\det A$ with Laplace expansion: $$a(-abc+b^2c+a c^2-b c^2+a b d-b^2 d-a c d+b c d) .$$
But how can I determine $\det A^5$ easily/fast? I know that there is the possibility to use $\det(A^5)=\det(A)^5)$, but this is too long for given time to resolve the problem.","['matrices', 'linear-algebra', 'determinant']"
1879922,Some questions about highly composite numbers,"A highly composite number is a natural number $n\ge 1$, such that $t(m)<t(n)$ for all $m$ with $1\le m<n$ , where $t(n)$ is the number of divisors of $n$. The link shows that the prime factorization of such a number $n$ contains non-increasing exponents and contains all primes upto the largest prime factor of $n$. It is also claimed that for every $n>36$ the last exponent is always $1$, in other words, if $p$ is the largest prime factor of a highly composite number $n>36$, then $p^2$ does not divide $n$. How can I prove this ? The largest highly composite number not divisible by $9$, seems to be $$1680=2^4\cdot 3\cdot 5\cdot 7$$ Is this true, and how can I prove it ? Finally Given the sequence of the exponents, is there an efficient method to determine whether the sequence corresponds to a highly composite number ? It is clear that it is sufficient to test the numbers corresponding to a non-increasing sequence and that the length is bounded by the smallest primorial exceeding the given number $n$. But for large $n$, many such sequences have to be checked. Just for the sake of curiousity : What is the largest known highly composite number ?","['number-theory', 'prime-factorization', 'divisor-counting-function']"
1879933,Vector Multiplication with Multiple Kronecker Products,"My question concerns matrix-vector multiplications when your matrix has Kronecker structure, which can be done faster in that case. I know how to compute this for a matrix $A = A_1 \otimes A_2$, which has two components $A_i$:
$$Ax = (A_1 \otimes A_2)x = (A_1 \otimes A_2)vec(X) = vec(A_2XA_1^T)$$
where $vec(X) = x$ is the vectorization of $X$. However, I have no idea how to proceed for more components $A_i$. I can imagine doing something as follows:
$$Ax = (A_1 \otimes A_2 \otimes \cdots \otimes A_n)x = vec((A_2 \otimes \cdots A_n)XA_1^T) = (I_m \otimes A_2 \otimes \cdots \otimes A_n)vec(XA_1^T)$$
which provides me again with an actual matrix-vector multiplication. I was hoping to get a large identity matrix on the left-hand side this way, but no luck. ( EDIT : I realised it is impossible to do it this way, as the matrices $X$ and $A_1^T$ do not have the same dimensions.) I tried looking it up on-line, but I mainly get the case for two components. Can anyone of you help me out? Thanks!","['matrices', 'kronecker-product', 'linear-algebra']"
1879954,"Evaluate $\int_{-1}^{1} \frac{x^3\,e^{\sin \left( \cos \left( x \right) \right)}+1}{x^2+1}\mathrm{d}x$","$$\int_{-1}^{1} \frac{x^3\,e^{\sin \left( \cos \left( x \right) \right)}+1}{x^2+1}\mathrm{d}x$$ $\cos x$ is an even function, $\sin(even)$ is a compostion of of an odd function and an even function which is an even function. $e^{x}$ is neither even nor odd, so the function $e^{\sin( \cos ( x ))}$ is even, now $x^3\,e^{\sin \left( \cos \left( x \right) \right)}$ is a product of an even and odd function so it is odd. Overall we got $\int_{-1}^{1} \frac{odd+1}{even+1}$ What Can we say about $\int_{-1}^{1} \frac{odd+1}{even+1}=\int_{-1}^{1} \frac{odd}{even}$?","['integration', 'calculus']"
1879993,Proving a function is positive.,"I'm looking to prove that the following inequality holds: $\frac{(\ln x + 1)^{2} - \ln x}{(\ln x )^{2}} \geq \frac{x^{2}}{(1-x)^{2}} \quad \forall x \in (0,1) $ or equivalently that: $f(x) =\frac{(\ln x + 1)^{2} - \ln x}{(\ln x )^{2}} -  \frac{x^{2}}{(1-x)^{2}} \geq 0\quad \forall x \in (0,1) $ Attempts so far have involved finding the limits of f(x) at 0 and 1 and trying to show that f(x) is decreasing over this interval. Also tried to show that f(x) has no turning points over the interval, but I can't seem to get anywhere. The latest idea was to try to find a function such that $ f(x) \geq g(x) \geq 0 $ over (0,1) but this just means I have to deal with 2 functions rather than 1. Has anyone got any ideas how I might proceed? Thanks,
John.","['calculus', 'functions']"
1880049,Why $E[X|X=Y]\neq E[Y]$?,"The full question was: Let $X$ be the number of tosses until a coin with probability of $1\over 3$ to land on Heads does and $Y$ the number of tosses until a fair coin lands on Heads. What is $E[X|X=Y]$? The correct answer is apperently $3\over 2$, which is lower than both $E[X]$ and $E[Y]$. What I'm confused about is if I know that $X=Y$, and I know that $Y=2$, then also $X=2$, therefore $$E[X|X=Y]=\sum_k k\cdot P(X=k|X=Y) = \sum_k k \cdot P(Y=k) = E[Y]$$ which seems to be wrong, my guess would be because knowing that $X=Y$ also gives us information on $Y$, but I'm still not able to understand how or why it works.","['expectation', 'probability']"
1880077,Is the anticanonical bundle of a compact Kähler manifold positive?,"I am working on the proof of the Kodaira embedding theorem and on one part of this proof you have ""$−K_{X}+L^{\otimes k}$ is positive"". Since it is part of the requirements, it is clear that $L$ is positive. But generally, is the following statement true? Let $X$ be a compact Kähler manifold. Then the anticanonical bundle $−K_{X}$ (or $K_{X}^{*}$) is positive.","['kahler-manifolds', 'complex-geometry', 'algebraic-geometry']"
1880090,Decide if $\mathbb{R} \subset \mathbb{C}$ is open or closed.,"The solution states that the ball of radius $\epsilon >0$ around a real number $x$ always contains the non-real number $x+i\epsilon/2$. I don't understand the answer, for every number $x \in \mathbb{R}$ there is an open ball, right? For every $x \in \mathbb{R}$ there is an $r>0$ such that I can form an open ball $B_r(x)\subset \mathbb{R}$.","['multivariable-calculus', 'analysis']"
1880108,Inverse limit of finite sets with surjective maps has surjective projection?,"I'm doing some exercises about inverse limits and got stuck in the following: Exercise Let $(S_{i},\pi_{ji})$ be an inverse system of finite sets with all $\pi_{ji}$ surjective and let $S=\varprojlim S_{i}$. Use the fact that the inverse limit of an inverse system of nonempty compact Hausdorff spaces is itself nonempty compact Hausdorff to prove that all maps $\pi_{i}\colon S\to S_{i}$ determined by the $\pi_{ji}$ are surjective. My attempt First, I quickly note that the notation $\pi_{ji}$ here is for the map $\pi_{ji}\colon S_{j}\to S_{i}$, defined whenever $i\leq j$. Okay. We start using the given fact to justify that the inverse limit $S$ is nonempty, since equipping each given finite set (assuming nonempty) with the discrete topology gives the compactness and Hausdorff hypotheses. Now we need to prove that given $s_{i}\in S_{i}$, we can find $s\in S$ such that $\pi_{i}(s)=s_{i}$. We know that given such $s_{i}$, for every $j\geq i$, we know that there is $s_{j}\in S_{j}$ such that $\pi_{ji}(s_{j})=s_{i}$. Well it would then suffice to take an element that projects to $s_{j}$ in $S_{j}$ for one of these $j's$. However I'm not sure why this is supposed to happen. Also, when he says in the end ""... the maps determined by the $\pi_{ij}$ are surjective."", what does determined mean? I believe it is specifying these $\pi_{i}$ because they are (are they?) the only maps that satisfy that commutative diagram property with the $\pi_{ji}$'s. Any help, tips or comments are greatly appreciated. Thank you very much. EDIT Just found a similar question on the sidebar, but it is a little different since there the indexing set is the positive integers and here is totally arbitrary. The answer also invokes some set-theorical induction which I know nothing about, so I'm keeping this question open in case there is a possible answer that the other guy missed. Also, I see how ""induction"" would work, and my argument makes that clear. However, the 'initial' induction step is the trouble here, of course.","['abstract-algebra', 'profinite-groups', 'group-theory']"
1880136,A fact on C1 functions,"Consider $f:[0,1]\rightarrow \mathbb{R}$ and define 
\begin{equation}
\sigma_n(f):=\frac{1}{n}\sum_{k=1}^{n}f \left(\frac{k}{n}\right)
\end{equation}
for $n=1,2,\dots$ I'm trying to prove that if $f\in C^1([0,1])$, then there exists a constant (depending on the function $f$) $a(f)$ such that
\begin{equation}
\sigma_n(f)=a+\frac{\sigma_n(f')}{2n}+o\left(\frac{1}{n}\right).
\end{equation} I have tried to use Taylor' series about $\frac{k-1}{n}$ for $k=1,\dots,n$ but I can't figure it out...it seems to be not working...","['derivatives', 'taylor-expansion']"
1880158,When do we apply functions in our daily life? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question When do we apply functions in our daily life? When do we use discrete functions and when do we use continuous function? Any links, perhaps? Thank you.",['functions']
1880162,Does a 'simple random sample' have to be drawn from a population of independent and identically distributed random variables?,"I'm reading an Econometrics textbook and it talks about ""Simple Random Samples"" and ""IID draws"" (Independent, Identicaly Distributed variables). While it defines the two seperately, it also seems to imply that they are the same thing. From what I understand, a simple random sample is where n objects are selected from a population at random. While an IID draw is where n objects are randomly selected from a population of independent and identically distributed random variables. (by randomly selected I mean that each random variable is equally likely to be selected) So: Does is a 'simple random sample' the same thing as an 'iid draw' or are they not necessarily the same thing?","['statistics', 'sampling', 'probability', 'economics']"
1880171,Maximum likelihood estimator for translated uniform distribution,"Let $\theta\gt 0$ and $X_1, ..., X_n$ be independently and identically distributed with probability densitiy function $f_\theta(x) = \frac{1}{2\theta} \chi_{x\in[-\theta,\theta]}$, where $\chi$ is the indicator function.
  What is the maximum likelihood estimator for $\theta$? This came up while studying for an exam and I would like some verification on my work: I compute the product likelihood function first: $p_x(\theta)=\Pi_{i=1}^nf_\theta(x_i)=\frac{1}{(2\theta)^n}\Pi_{i=1}^n\chi_{x_i\in[-\theta,\theta]}=\frac{1}{(2\theta)^n}\chi_{\theta\ge \max|x_i|}$. Now in the case of $\max|x_i|=0$ the function has no maximizer. This can be ignored since the case has probability $0$. On the other hand if $\max|x_i|\gt0$ we see that $p$ is strictly decreasing in $\theta$ and therefore $\hat\theta=\max|x_i|$ is the unique maximizer and therefore the wanted estimator.","['maximum-likelihood', 'statistics', 'proof-verification']"
1880231,What is the value of $(-1)^{\frac {2}{2}}$?,"When working with real numbers, should we interpret it as $$\left( (-1)^{2} \right) ^{\frac 12}=1$$ or rather as $$\left( (-1)^{{\frac 12}} \right) ^2= \text {not real/undefined}$$ or rather as $$(-1)^{\ 1} = -1$$ What about when we work with complex numbers? EDIT: The reason why I called the second case ""undefined"" is because when we are only working with reals, $(-1)^{\frac 12}$ is not real. I don't think we can assume that further algebraic manipulations that work on real numbers also work on our $(-1)^{\frac 12}$.","['algebra-precalculus', 'real-analysis', 'complex-numbers', 'exponentiation']"
1880268,Computing Hasse-Witt matrix of a hyperelliptic curve,"Given a hyperelliptic curve $y^2=f(x)$, many papers simply state that the Hasse-Witt matrix can be found by looking at certain coefficients of $f(x)^{p-1}$. According to the definition of the Hasse-Witt matrix, however, the curve must be non-singular and complete. Why, then, do we not have to consider the full smooth model when computing the Hasse-Witt matrix for hyperelliptic curves?","['algebraic-curves', 'algebraic-geometry']"
1880291,"$\lim_{k \rightarrow \infty} \sup_{n \ge 1} a_{n, k} = \sup_{n \ge 1} \lim_{k \to \infty} a_{n,k}$?","This is important for me and somehow I can't think straight now so I need some help. Suppose $a_{n,k}$ is a real double sequence and it is bounded. Is it possible to prove the claim $$ \lim_{k \rightarrow \infty} \sup_{n \ge 1} a_{n, k} = \sup_{n \ge 1} \lim_{k \to \infty} a_{n,k}$$ Thanks!","['real-analysis', 'supremum-and-infimum', 'limits']"
1880319,"Is my proof, by strong induction, of for all $n\in\mathbb{N}$, $G_n=3^n-2^n$ correct?","Let the sequence  $G_0, G _1, G_2, ...$ be defined recursively as follows: $G_0=0, G_1=1,$ and $G_n=5G_{n-1}-6G_{n-2}$ for every $n\in\mathbb{N}, n\ge2$. Prove that for all $n\in\mathbb{N}$, $G_n=3^n-2^n$. Proof. By strong induction. Let the induction hypothesis, $P(n)$, be $G_n=3^n-2^n$ Base Case: For $(n=0)$, $P(0)$ is true because $3^0-2^0 =0$ For $(n=1)$, $P(1)$ is true because $3^1-2^1=1$ Inductive Step: Assume that $P(n-1)$ and $P(n-2)$, where $n\ge2$, are true for purposes of induction. So, we assume that $G_{n-1}=3^{n-1}-2^{n-1}$ and $G_{n-2}=3^{n-2}-2^{n-2}$, and we must show that $G_{ n }=3^{ n }-2^{ n }$. Since we assumed $P(n-1)$ and $P(n-2)$, we can rewrite $G_n=5G_{n-1}-6G_{n-2}$ as $G_n=5(3^{n-1}-2^{n-1})-{ 6 }(3^{n-2}-2^{n-2})$ So, we get: $\Rightarrow G_n=5\cdot 3^{ n-1 }-5\cdot 2^{ n-1 }-(\frac { 6 }{ 3 } \cdot 3^{ n-1 }-\frac { 6 }{ 2 } \cdot 2^{ n-1 })$ $\Rightarrow G_n=5\cdot 3^{ n-1 }-5\cdot 2^{ n-1 }-2\cdot 3^{ n-1 }+3\cdot 2^{ n-1 }$ $\Rightarrow G_n=5\cdot 3^{ n-1 }-2\cdot 3^{ n-1 }-5\cdot 2^{ n-1 }+3\cdot 2^{ n-1}$ $\Rightarrow G_n=3\cdot 3^{ n-1 }-2\cdot 2^{ n-1 }$ $\Rightarrow G_n=\frac { 1 }{ 3 } \cdot 3\cdot 3^n-\frac { 1 }{ 2 } \cdot 2\cdot 2^n$ $\Rightarrow G_n=3^n-2^n$ The only real issue I have at this point is that I don't know how to properly conclude this proof with a final statement. A hint/guidance in that regard would be much appreciated. In addition, please feel free to offer advice and/or constructive criticism about my proof.","['proof-verification', 'discrete-mathematics']"
1880322,Is my proof that $ B- \bigcup_{i=i}A_i \subseteq \bigcap_{i=I}(B - A_i)$ correct?,"Suppose $x$ is arbitrary . Let  $$ x \in B- \bigcup_{i=i}A_i$$ Then $$ x \in B \land x \notin \bigcup_{i=i}A_i$$ So  $$ x \in B \land \forall i \in I \rightarrow x \notin A_i $$ Thus, $$ x \in \bigcap_{i=I}(B - A_i)$$ Since $x$ is arbitrary, $$B- \bigcup_{i=i}A_i \subseteq  \bigcap_{i=I}(B - A_i)$$ I'm not really sure about my conclusion since $ x \in \bigcap_{i=I}(B - A_i)$ seems to be equivalent to $ \forall i \in I \rightarrow x \notin A_i \land x  \in B  $ and not the third line of my proof. Or are they equivalent?","['proof-writing', 'proof-explanation', 'elementary-set-theory', 'proof-verification']"
1880334,Definition of a range in Set Theory,"I'm reading Enderton's 'Elements of Set Theory' and he makes this claim: The set $ \bigcup \{\ y\mid(x,y) \in F\} $  is equal to F(x) whenever F is a function and x $\in$ dom F. Question: why isn't $\{\ y\mid(x,y) \in F\} $ enough for this equality? Why do we need arbitrary union? Why doesn't that set define the range of F without arbitrary union?",['elementary-set-theory']
1880347,An algebra (of sets) is a sigma algebra iff it is a monotone class,"Let $X$ be a set and $\mathcal{A}$ an algebra of sets of $X$. Show that $\mathcal{A}$ is a $\sigma$-algebra iff it is a monotone class. The fact that a $\sigma$-algebra is a monotone class is trivial; $\sigma$-algebras are closed under countable unions and intersections, so it is in particular closed under monotone limits. It is the converse that is troubling me. I would like to assume that any family of sets of $\mathcal{A}$ could be ordered under inclusion. Then this family can be written as an increasing chain of inclusions and the statement follows. But this seems like hand-waving, or simply false (since I do not particularly use the fact that $\mathcal{A}$ is an algebra). Any hints are appreciated.","['real-analysis', 'measure-theory']"
1880455,Is it true that if the derivative of a function is not continuous then the function is not differentiable?,"It has been along time since I did real analysis. Someone asked me this question and I could not respond. I am not sure if this is the right place to ask. He said: The function $f: [0, \infty)\mapsto [0, \infty)$ given by $f(x)=\sqrt{x}$ is not differentiable at $0$ because its derivative $f'(x)=1/(2\sqrt{x})$ is not continuous at $0$. I said (what I could remember): It is not differentiable at $0$ because $\lim_{x\to 0}(f(x)-f(0))/(x-0)=\infty$. He said that I know that but what I told is it true?","['derivatives', 'real-analysis', 'continuity']"
1880464,small populations representativeness,"A question relating to home purchase loan denial rates, which I am examining as part of a work project. It's also a question that has come up in other aspects of my work. It is essentially about how much weight to put into figures drawn from small populations (not samples.) I know that when you are dealing with samples of a population you can talk about measures of variance (standard deviation, etc.) and margins of error. I have also seen, in researching this question, the coefficient of variance used in ratio data. But those only apply, as I understand it, when you are using a sample to estimate a population parameter. What I want to talk about is something like this: Suppose that I were the second person to apply to purchase a home in my town, and that the previous applicant had been successful. I could look at his success and say, ""Well, so far there has been a 100 percent success rate, so my chances are good!"", but that would probably be a little naïve. Is there a specific quantitative measure of how much weight you should put into small population figures for ""probabilistic"" purposes like this?","['statistics', 'probability']"
1880491,"Integral $\int_0^1 \frac{\ln (2-x)}{2-x^2} \, dx$","Here is an integral that I am trying to solve for quite some time. Find a closed form for the integral: $$\mathcal{J}=\int_0^1 \frac{\log (2-x)}{2-x^2} \, {\rm d}x$$ Here is what I've done. \begin{align*}
\int_{0}^{1}\frac{\log (2-x)}{2-x^2} \, {\rm d}x &= \int_{0}^{1} \frac{\log (2-x)}{\left ( \sqrt{2}-x \right )\left ( \sqrt{2}+x \right )} \, {\rm d}x\\ 
 &= \int_{0}^{1}\left [ \frac{\log (2-x)}{2\sqrt{2}(x+\sqrt{2})} + \frac{\log (2-x)}{2\sqrt{2} \left ( \sqrt{2}-x \right )}  \right ] \, {\rm d}x \\ 
 &=\frac{1}{2\sqrt{2}} \left [ \int_{0}^{1} \frac{\log (2-x)}{\sqrt{2}+x}\, {\rm d}x + \int_{0}^{1} \frac{\log (2-x)}{\sqrt{2}-x} \, {\rm d}x \right ] 
\end{align*} Now W|A is able to evaluate the last integrals. Indeed they boil down to: $\displaystyle \int_{0}^{1}\frac{\log(2-x)}{\sqrt{2}+x} \, {\rm d}x = {\rm Li}_2 \left ( 1-\frac{1}{\sqrt{2}} \right ) -{\rm Li}_2 \left ( 2-\sqrt{2} \right ) - \log 2 \log \left ( \sqrt{2}-1 \right )$ $\displaystyle \int_{0}^{1}\frac{\log(2-x)}{\sqrt{2}-x} \, {\rm d}x = -{\rm Li}_2 \left ( 1+\frac{1}{\sqrt{2}} \right ) +{\rm Li}_2 \left ( 2+\sqrt{2} \right ) + i \pi \log 2 + \log 2 \log \left ( \sqrt{2}+1 \right )$ However I cannot simplify the dilogs here. Before this approach I had done the following: \begin{align*}
\int_{0}^{1} \frac{\log(2-x)}{2-x^2} &=\frac{1}{2}\int_{0}^{1} \frac{\log(2-x)}{1- \frac{x^2}{2}} \, {\rm d}x \\ 
 &=\frac{1}{2}\int_{0}^{1} \frac{\log(2-x)}{1- \left ( \frac{x}{\sqrt{2}} \right )^2} \, {\rm d}x \\ 
 &= \frac{1}{2} \int_{0}^{1} \log(2-x) \sum_{n=0}^{\infty} \left ( \frac{x}{\sqrt{2}} \right )^{2n} \, {\rm d}x\\ 
 &=\frac{1}{2} \int_{0}^{1} \log(2-x) \sum_{n=0}^{\infty} \frac{x^{2n}}{2^n} \, {\rm d}x \\ 
 &= \frac{1}{2} \sum_{n=0}^{\infty} \frac{1}{2^n} \int_{0}^{1} x^{2n} \log(2-x) \, {\rm d}x \\
 &=??
\end{align*} In this approach I am unable to evaluate the integral $\displaystyle \int_0^1 x^{2n} \log (2-x) \, {\rm d}x$. As a side note if we replace that $2-x$ with $1-x$ then we have that: \begin{align*}
\int_{0}^{1}\frac{\ln (1-x)}{2-x^2} \, {\rm d}x &=\frac{1}{2}\sum_{n=0}^{\infty} \frac{1}{2^n} \int_{0}^{1}x^{2n} \ln (1-x) \, {\rm d}x \\ 
 &\overset{(*)}{=} -\frac{1}{2} \sum_{n=0}^{\infty} \frac{1}{2^n} \frac{\mathcal{H}_{2n+1}}{2n+1}\\ 
 &=-\frac{1}{2}\sum_{n=0}^{\infty} \frac{\mathcal{H}_{2n+1}}{2^n (2n+1)}
\end{align*} $(*)$ since $\displaystyle \int_{0}^{1}x^n \ln (1-x) \, {\rm d}x = -\frac{\mathcal{H}_{n+1}}{n+1}$. The series is quite easy to calculate although it shall still contain polylogs. Questions: Can we simplify those polylogs in the first attempt I have made? Do you see another way of evaluating the original integral?","['special-functions', 'real-analysis', 'integration']"
1880510,Principal and Associated Bundles in Hermitian Yang-Mills?,"So I have a good appreciation of the non-abelian Yang-Mills story.  We take a Riemannian manifold $M$ and a symmetry group $G$ giving rise to an infinite-dimensional gauge group $\mathcal{G}$.  In addition, we take a principal $G$-bundle $P$ on $M$, with $\mathcal{A}$ the infinite-dimensional affine space of connections on $P$.  Remarkably, $\mathcal{A}/\mathcal{G}$ is a finite dimensional space (not in general, but for example in Yang-Mills theory on Riemann surfaces, it's finite dimensional), and the Yang-Mills functional $\rm{YM}(\cdot)$ is a map from this space to $\mathbb{R}_{\geq 0}$, whose critical points satisfy the Yang-Mills equations.  Then of course, we have the instantons which are self-dual and anti-self dual connections.  This part of the story I understand well (I hope!) but I could use some assistance when it comes to the Hermitian Yang-Mills story. Here, people start discussing a holomorphic vector bundle $E$ over $M$.  Well, certainly this isn't the $G$-bundle $P$, and I can only guess that it's meant to be the associated vector bundle to $P$?  But doesn't this require picking a particular representation of the group $G$?  Moreover, the Hermitian Yang-Mills equation is a second order PDE for the curvature $F$, which seems to be the curvature of a connection on $E$.  How does this relate to curvatures of connections in $\mathcal{A}$, if at all?  I've heard Kobayashi-Hitchin relates instantons to stable holomorphic vector bundles, so I think they must be related, but I'm not sure how.","['differential-geometry', 'algebraic-geometry']"
1880540,A finite intersection of open sets is open,"I have to prove that a finite intersection of open sets is open. My idea is to do a proof by contradiction. The definition of an open set is as follows: A subset $U \subset \mathbb{R^n}$ is open if for every point $x \in U$, there exists $ r > 0$ such that the open ball $B_r(x)$ is contained in U. Is my starting point valid? Let $A_1, A_2 ... A_k$ open sets and let $x\in \bigcap_{i=1}^{k} A_i$. Suppose the intersection is not open and then there must be an $x\in  \bigcap_{i=1}^{k} A_i$ such that given any $\epsilon>0,\ B_{\epsilon}(x)\subset \bigcup_{i=1}^{k} A_i^c$.  From here I could look for a particular $\epsilon$ to find a contradiction.","['multivariable-calculus', 'analysis', 'proof-verification']"
1880542,Prerequisites for Algebraic Geometry,Is it possible to get into algebraic geometry by just knowing calculus and linear algebra or is this too far of a stretch? If not could anyone give me a list of book/lecture notes recommendations in chronological order to dive into algebraic geometry with my state of knowledge? Thanks,['algebraic-geometry']
1880553,Analytic continuation of $\sum z^{2n}$,"If I'm not misusing the root test, the convergence radius of $\sum z^{2n}$ is $\lim \sup \sqrt[2n]{1}=1$ (is this correct?). Now, is there a closed-form expression of $f(z)=\sum z^{2n}$ so that it be analytically continuated beyond this radius of convergence? Any hint would be appreciated.","['complex-analysis', 'analytic-continuation', 'power-series']"
1880573,LU Decomposition vs. Cholesky Decomposition,What is the difference between LU Decomposition and Cholesky Decomposition about using these methods to solving linear equation systems? Could you explain the difference with a simple example? Also could you explain the differences between these decomposition methods in: inverse of a matrix forward and backward substitution pivoting,"['cholesky-decomposition', 'matrices', 'matrix-decomposition', 'lu-decomposition', 'linear-algebra']"
1880625,Why do we divide the kernel estimate function by h and also the entire equation by h?,"I believe the explanation is that the kernel estimate function would integrate to 1. But I do not quite understand the intuition behind it. How would dividing by h help to make the function integrate to 1. Also, what is the output of the kernel  k(.) if it were Uniform. EXPLANATION","['statistics', 'estimation', 'functions']"
1880631,"How did they sum this infinite series: $\sum_{n=1,3,5...} \frac1n e^{-\frac{n\pi x}a}\sin{\frac{n\pi y}a}$?","I am studying Electrodynamics by Griffith. In potential chapter, I encountered this. $$\sum_{n=1,3,5...} \frac{1}{n} e^{{\large -\frac{n\pi x}{a}}}\sin{\frac{n\pi y}{a}} = \frac{1}{2}  \arctan\left(\frac{\sin(\frac{\pi y}{a})}{\sinh(\frac{\pi x}{a})}\right)$$ Can anyone please show me how did he get this? Any help will be appreciated!","['fourier-series', 'sequences-and-series', 'calculus', 'closed-form']"
1880641,Folland's Real Analysis 7.11,"Suppose $\mu$ is a Radon measure on $X$ such that $\mu(\left\lbrace x \right\rbrace)=0$ for all $x \in X$, and $A \in \mathbb{B}_X$ satisfies $0 < \mu(A) < \infty$. Then for any $\alpha$ such that $0 < \alpha < \mu(A)$ there is a Borel set $B \subset A$ such that $\mu(B) = \alpha$ I'm not even sure how to start this problem. It seems like a nice result, but any hints on how to start?","['real-analysis', 'measure-theory']"
1880680,Variance of the Sample Variance of a normal distribution,"I'm trying to calculate the variance of the sample variance of a normal distribution. Let $Y_1,Y_2,...,Y_n$ be a sample of size $n$ from a normal distribution with mean $\mu$ and variance $\sigma^2$. Define the unbiased sample variance as $S^2=\frac1{n-1}\sum_{i=1}^{n}(\bar Y-Y_i)^2$ where $\bar Y$ is the sample mean. As we know, $\frac{(n-1)S^2}{\sigma^2}$ is $ \chi^2$ distributed with $n-1$ degrees of freedom, so $$Var(S^2)=\frac{\sigma^4}{(n-1)^2}Var(\frac{(n-1)S^2}{\sigma^2})=\frac{2\sigma^4}{n-1}$$However I am trying to calculate it in a different way. Notice that $\bar Y$ is normal with mean $\mu$ and variance $\frac {\sigma^2}n$. Each $Y_i$ is normal with mean $\mu$ and variance $\sigma^2$. It follows that $\bar Y-Y_i$ is normal with mean $0$ and variance $\frac {\sigma^2}n+\sigma^2=\frac{(n+1)\sigma^2}{n}$. Therefore $\frac{\bar Y-Y_i}{\sqrt{\frac{(n+1)\sigma^2}{n}}}$ is a standard normal variable. Using this and the fact that $Z^2=\chi^2$ I get that:
$$Var(S^2)=\frac 1{(n-1)^2}Var(\sum_{i=1}^{n}(\bar Y-Y_i)^2)=\frac 1{(n-1)^2}Var(\sum_{i=1}^{n}(\frac{\bar Y-Y_i}{\sqrt{\frac{(n+1)\sigma^2}{n}}})^2\times\frac{(n+1)\sigma^2}{n})$$
$$=\frac {(n+1)^2\sigma^4}{n^2(n-1)^2}Var(\sum_{i=1}^{n}Z_i^2)=\frac {2n(n+1)^2\sigma^4}{n^2(n-1)^2}=\frac {2(n+1)^2\sigma^4}{n(n-1)^2}$$
since $\sum_{i=1}^{n}Z_i^2$ is $\chi^2$ distributed with $n$ degrees of freedom. My derivation of the variance seems correct to me but the answer is clearly not, where did I go wrong?","['statistics', 'variance', 'normal-distribution', 'random-variables']"
1880686,Orbit space of torus homeomorphic to mobius strip,"Let $\mathbb{T}^2$ be the torus and have $\mathbb{Z}_2$ action on the torus by permuting the coordinates. I am trying to prove the orbit space is congruent to the mobius strip $\mathbb{T}^2/\mathbb{Z}^2 \cong M$. I am writing $M\cong I^2/\alpha$, where $I=[0,1]$, $\alpha(x)=1-x$ and we identify the points $(x,0)\sim (\alpha(x),1)$.
I know it is useful for this problem to identify the torus as the quotient $I^2/\sim_0$ with left glued to right and top glued to bottom (referring to sides) and then the action on the torus corresponds to switching coordinates on the square, which is reflecting over the diagonal $(x,x)$. What I don't understand is how to combine these relations of gluing and permuting to show this space is homeomorphic to the quotient above with $\alpha$. I think that with both relations we will identify the edges together since $(x,0)\sim_0(x,1)\sim(1,x) \sim_0(0,x)$. Then points not on the edges will either be identified with themselves if they are on the diagonal or just their permutation. But I can't figure out how to map this to the Mobius strip... How do you proceed formally with proving these spaces are homeomorphic? Obviously I'm not looking for all the details but I'm stuck both trying to visualize it (i get to folding the square over the diagonal $(x,x)$ but then I cant picture how to get the mobius strip from there on). Any help is appreciative. Also a different generalization than the natural one: instead of permuting the coordinates in $\mathbb{T}^k$ and taking the action by $S_n$ consider the action by reversing the coordinates, which can still be done by using $\mathbb{Z}_2$. What is this orbit space homeomorphic to? Would it be $M^k$ when $k$ is even and something else for odd $k$?",['general-topology']
1880701,sufficient condition for a real matrix to have all real eigenvalues,"We all know that eigenvalues of a Hermitian matrix are real, but I am looking for sufficient conditions for a general real matrix (not necessarily symmetric) to have only real eigenvalues. So far, I know that totally positive matrices have this property I want. Are there any other sufficient conditions besides total positivity?","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
1880711,How to integrate the Heat Kernel?,"Let $P(x, y, t) := (4 \pi t)^{-v/2}\exp\left(\frac{-|x-y|^2}{4t}\right)$, with $x, y \in \mathbb{R}^v$ and $t \in \mathbb{R}$, denote the Heat Kernel. QUESTION: How to show that: $$Q(x, y, t) := \int^t_0 P(x, y, s)\,\mathrm ds$$ behaves like: $|x - y|^\gamma \exp\left(\frac{-|x-y|^2}{4t}\right)$ in the region: $$A :=\{(x, y) \mid |x-y| \leq 4 \sqrt{t}\}$$ for some suitably chosen $\gamma$?","['real-analysis', 'partial-differential-equations', 'operator-theory', 'functional-analysis', 'integration']"
1880721,Equivalent definitions of absolutely continuous functions,"A function $f:[a,b]\rightarrow \mathbb{C}$ is said to be absolutely continuous if for each $\epsilon>0$, there exists $\delta>0$ such that for every mutually disjoint finite sequence of closed sub-intervals $\{[a_1,b_1],...,[a_n,b_n]\}$ of $[a,b]$ satisfying $\sum_{i=1}^n |b_i-a_i|<\delta$, $\sum_{i=1}^n |f(b_i)-f(a_i)|<\epsilon$ holds. Let $f:[a,b]\rightarrow \mathbb{C}$ be an absolutely continuous function and $\epsilon>0$. Then, how do I prove that there exists $\delta>0$ such that for every mutually disjoint finite sequence of open sub-intervals $\{(a_1,b_1),...,(a_n,b_n)\}$ of $[a,b]$ satisfying $\sum_{i=1}^n |b_i-a_i|<\delta$, $\sum_{i=1}^n |f(b_i)-f(a_i)|<\epsilon$ holds ?","['continuity', 'real-analysis', 'analysis']"
1880727,For which topological measure spaces do open sets always have positive measure?,"I want to generalize the result for $\mathbb{R}^n$, that any non-trivial open set has positive measure. Open sets and measure zero Question: For which types of topological measure spaces is it true that every non-trivial (i.e. non-empty) open set has positive measure? My thinking was that only inner regularity of the measure, combined with local compactness of the space, would be necessary. There are some problems with this: (1) We don't just need that every point has a compact neighborhood for this to work -- otherwise we would be able to prove that any set containing a compact set has positive measure. We need that every point has a compact neighborhood with positive measure. This does not seem to follow from the given candidate assumptions. And (2) The possible definitions of local compactness vary depending on whether or not the space is Hausdorff. Obviously the topological measure space needs to be Hausdorff in order for it to be inner regular, but there are weaker notions which might be sufficient and which don't require $T_2$. Apparently we also need left-invariance, i.e. that our topological measure space is in fact a measurable topological group. measure of open set with measure Haar But from the answer given to that question the following is still unclear to me: Do we really need Hausdorff? And do we really need group structure and left-invariance to conclude that every point has a compact neighborhood with positive measure?","['general-topology', 'topological-groups', 'measure-theory', 'haar-measure']"
1880736,How do I determine the point on a square inside a circle depending on an angle?,"I can't figure out how to find $(s_x, s_y)$ (see picture, the blue marked intersection).
I have $\alpha$, and the square is perfectly inside the square. Assume the radius is 1, since that isn't very important.
Hobby programmers aren't the best at math I'm guessing, heh.. Sorry if this is very trivial.",['trigonometry']
1880741,Why can't calculus be done on the rational numbers?,"I was once told that one must have a notion of the reals to take limits of functions. I don't see how this is true since it can be written for all functions from the rationals to the rationals, which I will denote $f$ , that $$\forall L,a,x:(L,a,x\in\mathbb{Q})\forall \epsilon:(\epsilon>0)\space \exists\delta:(\delta>0)$$ $$\lim_{x\rightarrow a}f(x)=L\leftrightarrow(\mid x-a\mid<\epsilon\leftrightarrow\mid f(x)-L\mid<\delta) $$ Since, as far as I know, functions like $\mid x\mid$ and relations like $<$ can be defined on the rationals. Is it true you couldn't do calculus on just the rational numbers? At the moment I can't think of any rational functions that differentiate to real functions. If it's true that it isn't formally constructible on the rationals, what about the algebraic numbers? Edit Thanks for all the help, but I haven't seen anyone explicitly address whether or not we could construct integrals with only algebraic numbers. Please explain why or why not this is possible.","['real-analysis', 'rational-numbers', 'calculus', 'limits']"
1880746,"If $G/Z(G)$ is Abelian and $\{e\}\ne H\triangleleft G$, then $H\cap Z(G)\ne\{e\}$","I am somewhat unfamiliar with commutator subgroups, so I am not sure about the last couple of lines of this proof. Let $G$ be a group such that $G/Z(G)$ is Abelian, and let $\{e\}\ne H\triangleleft G.$  Show that $H\cap Z(G)\ne\{e\}.$ Consider the commutator subgroup $G'<G.$ For any $N\triangleleft G$ with $G/N$ Abelian, we have that $G'\subset N.$ Thus since $Z(G)$ is a normal subgroup of $G$ and the quotient group is Abelian by assumption, $G'\subset Z(G).$ Now let $g\in G$ and $h\in H$. Then 
$$ghg^{-1}h^{-1} = (ghg^{-1})h^{-1}\in H\cap G'.$$ Now if $ghg^{-1}h^{-1}\ne e$ for some $g,h,$ then we're done. But if $ghg^{-1}h^{-1} = e$ for all $g\in G,\; h\in H$, then this implies $H\subset Z(G)$.",['group-theory']
1881774,Selecting nonadjacent people in a circle,"Suppose 19 people are sitting at a circular table.  In how many ways can we select 5 of the people so that no two of them were sitting next to each other? (My idea is to first consider the situation where they are sitting in a row, but I'm not sure how to adjust this for the circular case.) EDIT: I am not identifying two rotations of the same seating as being identical.","['combinatorics', 'discrete-mathematics']"
1881811,Why is $x^{-1} = \frac{1}{x}$?,"I've always taken for granted that $x^{-1} = \frac{1}{x}$ because it works and everything, but how can I see that this must be true? I am not looking for answers like ""If you have $x^n / x^m$ then this is $x^{n-m}$ and so $x^{0}/x^1 = 1/x = x^{0-1} = x^{-1}$ because then I am stuck wondering whether it probably makes sense that $x^n$ can be defined when $n$ is negative (I hesitate to just assume that because we can plug numbers into something that it will be valid).","['algebra-precalculus', 'real-analysis', 'number-theory', 'inverse']"
1881822,show that $19-5\sqrt[3]{2}-8\sqrt[3]{4}$ is a unit in $\mathbb{Z}[\sqrt[3]{2}]$,"I found by numerical experiment the norm of $19-5\sqrt[3]{2}-8\sqrt[3]{4}$ (result of multiplying conjugates) is: $$0.9999999999989706-4.4408920985006262 \times 10^{-16}i$$ but I am betting this is just $1$.  How can I show this is a unit in $\mathbb{Z}[\sqrt[3]{2}]$?  Here is another possible unit:
$$ 521-62\sqrt[3]{2}-279\sqrt[3]{4} $$","['algebra-precalculus', 'algebraic-number-theory']"
1881891,"Probability that $\operatorname{Erlang}(2,\mu _2)$ is greater than $\exp(\mu _1)$","I'm trying to work out that, given that $X\sim \exp(\mu_1)$ and $Y\sim \operatorname{Erlang}(2,\mu _2)$, what is $\mathbb{P}(X<Y)$?
So far I have:
$$\mathbb{P}(X<Y)=\int_0^{\infty}\mathbb{P}(X<t \mid Y\in dt)\mathbb{P}(Y \in dt)$$
$$=\int_0^{\infty}(1-e^{-\mu _1t}){\mu _2}^2te^{-\mu _2t}dt$$
I can do this integral, but it looks like it will be a great big mess. Am I on the right track?","['gamma-distribution', 'statistics', 'probability']"
1881895,Probability distribution question with drawing letters from a box,"Draw letters with replacement from the following box: [S  T  A  T I  S  T  I  C  S] Let $Y$ be the distribution of the number of times the letter 'A' is drawn until you get the letter 'S' 5 times. Find $P(Y = y)$ , $E[Y]$ and $Var[Y]$ . I'm having trouble finding $P(Y = y)$ . My initial thought was that I could use the Negative Binomial distribution, but I'm not looking for a number of trials until the rth success.","['statistics', 'probability', 'probability-distributions']"
1881918,"Compact subset of $C[0,1]$","Question : Let $\sigma: [0,1] \to \mathbb{R}^{\ge 0}$ be a nonnegative, continuous function such that $\sigma(0) =0$. For each real $\lambda \ge 0$ define the following set:
$$
F_ \lambda = \left\{ f \in C[0,1] : |f(0)| \le \lambda |f(1)| \text{ and } |f(x)-f(y)| \le \sigma\left(\left|x-y\right|\right) \text{ for all } x,y \in [0,1]\right\}
$$
Characterize the $\lambda$ for which $F_\lambda$ is a compact subset of $C[0,1]$. Solution : (Let $\epsilon>0)$. I'm trying to use Arzela Ascoli. First, equicontinuity. Since $\sigma$ is continuous and $\sigma(0)=0$, there is some $\delta$ so that $\sigma(x) < \epsilon$ whenever $0<x<\delta$. Then, for $|x-y|<\delta$
$$
|f(x)-f(y)| \le \sigma(|x-y|) <\epsilon
$$
so $F_\lambda$ is equicontinuous for all $\lambda$. Closedness is straightforward . . . Let $\|f_n - f\|_\infty \to 0$ where $(f_n) \subset F_\lambda$. The limit $f$ is continuous, and continuity of $f$ ensures that the other two criterion of $F_\lambda$ are satisfied. (Take limits.) The difficulty is with (equi)boundedness. Since $\sigma$ is continuous on a closed interval $[0,1]$, we have $\| \sigma\|_\infty \le M < \infty$ by the Extreme Value Theorem. If $\lambda=0$, then $|f(x)| \le |f(x)-f(0)| + |f(0)| \le \sigma(x) + 0 \le M$, so $F_0$ is uniformly bounded. If $1 \le \lambda<\infty$, then the constant functions $f_n(x) = n$ are in $F_\lambda$, but they have no uniform bound. My main troubles are with $0 < \lambda <1$ . . . I have been trying to construct specific sequences that are unbounded and satisfy $|f_n(0)| \le \lambda |f_n(1)|$, but I am never sure if $|f_n(x)-f_n(y)| \le \sigma(|x-y|)$ without knowing more about $\sigma$. (Maybe that's the point) Thank you.","['functional-analysis', 'real-analysis', 'compactness']"
1881926,Understand $\bowtie$ symbol : Non-commuting action of both group,"Note this is a part of physics paper arXiv-9909150 . And please assume that i am not familiar with advanced algebraic knowledge(Group theory, abstract algebra, etc..). Excerpt from above paper, The proposed U-duality group is then generated by \begin{align}
G(Z)=SL(2,Z) \bowtie SO(d,d;Z),
\label{orig}
\end{align} where $\bowtie$ refers to the non-commuting 
action of both groups. I want to understand $\bowtie$ symbol, explicitly.  They said $\bowtie$ refers to the non-commuting action of both groups, but i am not pretty sure about its meaning. First, is $G(Z)$ is kind of direct product of $SL(2,Z)$ and $SO(d,d;Z)$? 
I mean G(Z) satisfies the both properties of $SL(2,Z)$ and $SO(d,d;Z)$? Second, non-commuting action means, $SL(2,Z) \bowtie SO(d,d;Z)$ and $SO(d,d;Z) \bowtie SL(2,Z)$ are different?","['abstract-algebra', 'group-theory']"
1881933,Can someone explain why amoebas are closed?,"From Wikipedia, Claim: any amoeba is closed. I attempted to use the preimage of closed under continuous map is closed but I realized that my skill with complex functions was too feeble, so I found a proof instead http://www.sciencesmaths-paris.fr/upload/Contenu/Notes%20et%20resumes%20cours/Notes_Cours_Mikhalkin/lecture-notes1.pdf Can someone use simple language or a series of simple propositions to explain what the proof is trying to do? If I were to attempt to use the standard proof: the preimage of closed set under continuous map is closed, what are the necessary ingredients contained in this proof? Thanks!","['complex-analysis', 'general-topology', 'algebraic-geometry', 'proof-explanation']"
1881976,Help proving an inequality in calculus $0 \leq \frac{x\ln(x)}{x^2-1}\leq \frac{1}{2}$,"So as the title states, I need help with proving this inequality: $$0 \leq \frac{x\ln(x)}{x^2-1}\leq \frac{1}{2} $$
Also an important thisn is that $x>1$. So I thought I could look different kinds fo x, but it's a subject of calculus, so perhaps it's related to Lagrange's or Rolle's theorem, but i cannot get an idea, i tried changing the inequality but it's not really working, so any help with solution would be really appreciated.
Thank you in advance.","['inequality', 'calculus']"
1881977,Filling a unit cube with countable balls.,"Is it possible to fill an $n-$dimensional unit cube with countable number of non-overlapping $n-$dimensional balls? By $n-$dimensional unit cube, I am thinking of the set
$$
C:=\{\ x\in\Bbb R^n\ |\ 0\le x_i\le 1\ \text{for all}\ i=1,2,\dots,n\ \}
$$
where $x=(x_1,\dots,x_n)$. An $n-$dimensional ball centered at $x_0$ is defined to be
$$
B(x_0;r):=\{\ x\in\Bbb R^n\ |\ d(x_0,x)<r\ \}
$$ 
where $d$ is the Euclidean distance function. A sequence of balls $(B_1,B_2,\dots)$ is said to fill the unit cube if they are pairwise-disjoint , each $B_i\subset C$ and
  $$
\sum_{i=1}^{\infty}\mu(B_i)=1
$$ 
  where $\mu$ is the Lebesgue measure on $\Bbb R^n$. Intuitively, I feel that it should be possible to fill the unit cube with many balls. However, I can't think of a way to prove it. Since $C$ is separable, I thought about enumerating its rational points into a sequence $\{c_1,c_2,c_3,\dots \}$ and form balls around them such that $B_i:=B(c_i,r_i)$ where
$$
r_i:=\sup\{\ r\in\Bbb R^+\ |\ B(c_i;r)\ \text{is contained in $C$ and doesn't intersect $B_1,B_2,\dots,B_{i-1}$} \}.
$$ 
I don't know if this method works or not. I would really appreciate if anyone can suggest a way to finish the prove, suggest a new method, or give a negative answer to my question. An answer that only works in dimension $2$ or $3$ is also very welcomed.","['geometric-measure-theory', 'real-analysis', 'measure-theory', 'geometry']"
1882052,Why do some curves not admit etale coverings?,"Let $C$ be a non-singular projective curve of genus $g\geq2$ over $\mathbb{C}$. If $g\geq 3$ and $C$ is general, why does $C$ not admit etale coverings $\pi:C\rightarrow C'$ of degree $>1$? Is this due to the fact, that a general curve of genus $g\geq 3$ does not have automorphisms? If $g=2$, why does no curve $C$ of this genus admit etale coverings $C\rightarrow C'$? (I think I figured this one out: this follows from the Riemann-Hurwitz formula.)","['algebraic-curves', 'covering-spaces', 'algebraic-geometry']"
1882060,Why do we need the extra condition of being 'Fredholm of index zero' when showing that an operator has a bounded inverse?,"In finite dimensional linear algebra if we have $A x = 0$, and $\ker(A) = 0$, then $A$ is invertible. But it seems having a trivial kernel is not a sufficient condition for invertibility when dealing with infinite dimensional operators. Consider the following theorem: Theorem Let $S_D: L^2(\partial D) \to W_1^2(\partial D)$ be the single layer potential for the Laplacian in $\mathbb{R}^3$ where $D$ is a bounded Lipschitz domain in $\mathbb{R}^3$. Let $\phi \in L^2(\partial D)$ satisfy 
$$S_D[\phi] = 0 \quad \text{on} \quad \partial D.$$
It can be shown then that $\phi = 0$. Now we want to prove that $S_D$ has a bounded inverse. Proof As $W_1^2(\partial D) \hookrightarrow{} L^2(\partial D)$ is compact, and $S_D$ maps $L^2(\partial D)$ into $W_1^2(\partial D)$ boundedly, we have that $S_D$ is Fredholm with index zero. And as we have that $\ker(S_D) = \{0\}$, this means that $S_D$ has a bounded inverse. Question Unlike in a finite dimensional matrix problem, in this case having a trivial kernel $\ker(S_D) = \{0\}$ was not enough to show that $S_D$ is invertible. We also needed to show that $S_D$ is Fredholm with index zero . Why is this? I am not very experienced with Fredholm theory. I know that being Fredholm of index zero means that an operator has finite dimensional kernel finite dimensional cokernel closed range $\dim(\ker) - \dim(\text{coker}) = 0$ (for the index zero property) So why do we need these extra properties when dealing with the invertibility of an infinite dimensional operator as opposed to a finite dimensional matrix?","['functional-analysis', 'sobolev-spaces', 'operator-theory']"
1882069,Why does polynomial factorization generalize to matrices,"I'm reading about linear algebra and I came across with the following theorem where I have a problem convincing myself: Theorem 2.1 $\,$ Every linear operator on a finite-dimensional complex vector space has an eigenvalue. Proof: To show that $T$ (our linear operator on $V$) has an
  eigenvalue, fix any nonzero vector $v \in V$. The vectors $v, Tv,
 T^2v,..., T^nv$ cannot be linearly independent, because $V$ has
  dimension $n$ and we have $n + 1$ vectors. Thus there exist complex
  numbers $a_0,...,a_n$, not all $0$, such that $$a_0v + a_1Tv + ··· + a_nT^nv = 0.$$ Make the $a$’s the coefficients
  of a polynomial, which can be written in factored form as $$a_0 + a_1z
 + ··· + a_nz^n = c(z − r_1)\cdots(z − r_m),$$ where $c$ is a non-zero complex number, each $r_j$ is complex, and the equation holds for all
  complex $z$. We then have $${\color{red}{ 0=(a_0I + a_1T + ··· + a_nT^n)v= c(T − r_1I)\cdots(T −
 r_mI)v}},$$ which means that $T − r_j$ I is not injective for at least
  one $j$. In other words, $T$ has an eigenvalue.$\;\blacksquare$ I'm having trouble with the factorized form of the matrix polynomial (in red). I understood that the factorization holds for a polynomial by the fundamental theorem of algebra but why does this also hold for matrices? In other words, why is the the part I highlighted true? Does such an factorization always exist? Could I have some help to see this? Thank you =) P.S. here is my reference (page 3). UPDATE: Someone else also has asked the same question before it seems.",['linear-algebra']
1882077,Order of middle group in short exact sequence,"If we have a short exacts sequence of abelian group with $A$ and $C$ finite, $0\to A\to B\to C$, is it true that $|B|=|A|\cdot |C|$ even if it does not split?","['finite-groups', 'abstract-algebra', 'group-theory']"
1882130,Calculating Gramian matrix from Euclidean distance matrix,"For $v_1, \dots, v_n \in \mathbb{R}^n$ we have Euclidean distance matrix $D = (\|v_i - v_j\|^2)_{ij}$ and Gramian matrix $G = (v_i \cdot v_j)_{ij} = V^TV$, where $V = (v_1, \dots, v_n)$. If the $v_i$ are mean-centered ($\sum_i v_i = 0$) then the Gramian matrix can be calculated from the Euclidean distance matrix by $G = -\frac{1}{2}CDC,$ where $C = I - \frac{1}{n}\bf{1}^T \bf{1}$ is the n-by-n centering matrix. What is an elegant way to show this identity? Note: I can prove this, but only by entering ""indices hell"": Write the components of $D$ and $G$ as $d_{ij}$ and $g_{ij}$ respectively. Then: $$d_{ij}=g_{ii}+g_{jj}-2g_{ij}$$
$$\sum_{j} g_{ij} = v_i \cdot (\sum_j v_j) = 0$$
$$\sum_{j}d_{ij} = \sum_{j}d_{ji} = \sum_j (g_{ii} + g_{jj} - 2g_{ij})=ng_{ii}+Tr(G)$$ Let $CD = (h_{ij})_{ij}$ and $CDC = (h'_{ij})_{ij}$. Then $$h_{ij} = d_{ij} - \frac{1}{n}\sum_k d_{kj} = (g_{ii} + g_{jj}-2g_{ij}) - \frac{1}{n}(ng_{jj}+Tr(G))$$
$$= g_{ii}-2g_{ij}-\frac{1}{n}Tr(G)$$ $$\sum_k h_{ik} = n g_{ii}-Tr(G)$$ $$h'_{ij} = h_{ij}-\frac{1}{n}\sum_k h_{ik} = 2g_{ij}$$ and hence $G = \frac{1}{2}CDC$. I've thought about whether one might be able to make use of the Cholesky decomposition of $D$ (since then $G = V^TV$ and $D = L^TL$, so the identity would be $V^TV=\frac{1}{2}H^TL^TLH$ and then perhaps once could relate $V$ to $LH$ somehow), but I'm not sure when $D$ is positive-definite. Also, I am aware of this related question .","['matrices', 'linear-algebra']"
1882133,Sum of all numbers that can be made out of a set of digits with possibly many duplicates,"The problem goes as follows, let $S$ be a set of nonzero digits, with possibly repeating or missing digits. Let $K \geq |S|$, I want to find the sum of all numbers with $K$ or less digits, whose nonzero digits exhaust the set $S$. To elaborate, suppose $K = 3$ and $S = \{1,1\}$. Then the numbers satisfying the criteria are $11,101$ and $110$ so the sum would be $222$. Now I want to consider large sets of numbers (for programming purposes), denote these by $S = \{d_1,\ldots,d_1,d_2,\ldots,d_2,\ldots d_j,\ldots,d_j\}$. I choose this notation to reflect the fact that there may be multiple duplicates and that not all the digits $1,\ldots, 9$ need to be included in the set. The way I want to attack this problem is by decomposing the numbers created by the digits into single digits numbers. I tried the following approach but I am not sure it is right: Extend the set $S$ by $K - |S|$ zeroes. Then using the elements in $S$ we can create exactly 
$$ \frac{K!}{m_0!m_1!\cdots m_9!}$$
numbers, where $m_j$ denotes the multiplicity of $d_i$ in $S$. Now of these numbers, the fraction $ m_i / K$ will have $d_i$ as the last digit, and for the other digits the fraction is the same. Thus the total sum should be
$$ \frac{K!}{m_0!m_1!\cdots m_9!} \cdot \underbrace{11 \ldots 11}_{k \ \text{times}} \cdot \sum\limits_{i=0}^9 d_i \frac{m_i}{K}.$$ The $\underbrace{11 \ldots 11}_{k \ \text{times}}$ reflects the sum of the weights the digits have at all possible positions. In the example I showed we would have $S = \{1,1,0\}$ and $K = 3$, giving the total sum of
$$\frac{3!}{0!2!}\cdot 111 \cdot \left( 1 \cdot \frac{2}{3} \right) = 222,$$
which is correct. The formula can be rewritten as 
$$ \frac{(K-1)!}{(K-|S|)!m_1!\cdots m_9!} \cdot \underbrace{11 \ldots 11}_{k \ \text{times}} \cdot \sum S.$$
I wrote a small algorithm that implements this method of calculating the sum as well as a brute force implementation but the numbers do not seem to  match. The motivation I give above for the formula is not really rigorous, especially the part about the fraction of all numbers that have $d_i$ as a certain digit. So is my reasoning correct? Thanks!","['number-theory', 'combinatorics']"
1882141,Decide if the series converges and prove it using comparison test: $\sum_{k=1}^{\infty}\frac{3k^{2}+k+1}{k^{4}+k^{3}+4}$,"Decide if the series converges and prove it using comparison test:
  $\sum_{k=1}^{\infty}\frac{3k^{2}+k+1}{k^{4}+k^{3}+4}$ $$\sum_{k=1}^{\infty}\frac{3k^{2}+k+1}{k^{4}+k^{3}+4}< \frac{k^{2}+k}{k^{4}+k^{3}} < \frac{k^{2}}{k^{4}} \leq \frac{1}{k^{2}}$$ We know (from our readings) that $\sum_{k=1}^{\infty}\frac{1}{k^{2}}$ is a converging series. Thus the complete series will converge. Did I do everything correctly?","['convergence-divergence', 'sequences-and-series', 'calculus', 'analysis']"
1882147,Computing expected cost for exponential random variable,"I am reading Probability and Statistics for Engineering and the Sciences . Exercise 63, Chapter 4 says: A consumer is trying to decide between two long-distance calling plans. The first one charges a flat rate of 10¢ per minute, whereas the second charges a flat rate of 99¢ for calls up to 20 minutes in duration and then 10¢ for each additional minute exceeding 20 (assume that calls lasting a noninteger number of minutes are charged proportionately to a whole-minute’s charge). Suppose the consumer’s distribution of call duration is exponential with parameter $\lambda$ . Which plan is better if expected call duration is 10 minutes? 15 minutes? Assuming the first question, when the duration is 10 minutes, I computed the cost of the first plan as: $h_1(x) = 10 * E[x] = 10 * 10 = 100$ However, how do I compute the cost for the second plan ( $h_2(x)$ ) ? I tried with: $h_2(x) = 99 * F(x \leq 20) + 10 * (1 - F(x \leq 20)) \approx  87$ But the correct result is $112.53$ .","['exponential-distribution', 'statistics', 'probability']"
1882178,Independence of the data and the parameter in Machine Learning,"In one of the lectures, prof. Nando de Freitas explains the use of Bayesian rule to logistic regression. Here's the video and the slides . In particular, on slide 10 (around 34:50 on the video) NdF writes the posterior as following: $$p(\theta \mid X,y)=\frac{p(y \mid X,\theta)p(\theta)}{p(y \mid X)}$$ where $(X, y)$ are the observed data $D$ and $\theta$ is the parameter of the model. (1) Strict application of Bayesian rule gives a slightly different equation: $$p(\theta \mid X,y)=\frac{p(y \mid X,\theta)p(\theta\mid X)}{p(y \mid X)}$$ $X$ is just ignored from the condition to form a prior. For me it's not obvious that $X$ and $\theta$ are independent. Why is it true then? (2) NdF repeats a similar reasoning on the next slide: $$p(y_{n+1}\mid x_{n+1}, D) =$$
$$\int{p(y_{n+1}, \theta \mid x_{n+1}, D)}d\theta=$$
$$\int{p(y_{n+1}\mid \theta, x_{n+1}, D)} p(\theta \mid x_{n+1}, D)d\theta=$$
$$\int{p(y_{n+1}\mid \theta, x_{n+1})} p(\theta\mid D)d\theta$$ In the last equation two conditions disappear, $D$ and $x_{n+1}$.
The argument is as follows (around 40:20 on the video): $\theta$ already contains the information about $D$, hence $D$ is redundant. Plus $x_{n+1}$ doesn't give any information to the posterior, hence $x_{n+1}$ is redundant. I don't quite understand this reasoning and the nature of $\theta$ as a random variable. The dependence of $\theta$ and $x$ is not straightforward, but it looks like to compute $p(\theta \mid x)$ we need to marginalize over all $y$. Would appreciate if someone explains the intuition behind it.","['bayes-theorem', 'machine-learning', 'probability-theory', 'probability']"
1882179,Closed form for the integral $\int_0^\infty \frac{dx}{\sqrt{(x+a)(x+b)(x+c)(x+d)}}$,"Let's consider the function defined by the integral: $$R(a,b,c,d)=\int_0^\infty \frac{dx}{\sqrt{(x+a)(x+b)(x+c)(x+d)}}$$ I'm interested in the case $a,b,c,d \in \mathbb{R}^+$. Obviously, the function is symmetric in all four parameters. This function has some really nice properties. $$R(ka,kb,kc,kd)=\frac{1}{k} R(a,b,c,d)$$ Thus: $$R(a,a,a,a)=\frac{1}{a}$$ Moreover: $$R(a,a,b,b)=\frac{\ln a-\ln b}{a-b}$$ This is the reciprocal of the logarithmic mean of the numbers $a$ and $b$. For the case of $R(a,a,b,c)$ we can use the Euler substitution to obtain: $$R(a,a,b,c)=2 \int_{\sqrt{bc}-a}^{\frac{b+c}{2}-a} \frac{dt}{t^2-(a-b)(a-c)}=$$ $$=\frac{1}{\sqrt{(a-b)(a-c)}} \left( \ln \frac{\sqrt{(a-b)(a-c)}+\sqrt{bc}-a}{\sqrt{(a-b)(a-c)}-\sqrt{bc}+a}-\ln \frac{\sqrt{(a-b)(a-c)}+\frac{b+c}{2}-a}{\sqrt{(a-b)(a-c)}-\frac{b+c}{2}+a} \right)$$ As far as I see, this function is deeply related to various means (you can see arithmetic, geometric and logarithmic means represented in the above expressions). Is there a closed form for the general case of $R(a,b,c,d)$ for $a,b,c,d \in \mathbb{R}^+$? Perhaps, in terms of hypergeometric functions or elliptic integrals?","['means', 'integration', 'definite-integrals', 'elliptic-integrals', 'special-functions']"
1882204,Decide if the series converges and prove it using comparison test: $\sum_{k=1}^{\infty}\frac{1}{2k+3}$,"Decide if the series converges and prove it using comparison test:
  $\sum_{k=1}^{\infty}\frac{1}{2k+3}$ $$\frac{1}{2k+3} <\frac{1}{2k}<\frac{1}{k}$$ and since $\sum_{k=1}^{\infty}\frac{1}{k}$ diverges (we have defined that in our readings, so I can claim it here), the complete series will diverge, too. Did I do everything correctly? What makes me feel a bit unsure is that there is no ""$\leq$"" sign anywhere and when I look up the comparison test on the internet, I don't see any ""<"" or "">"" in the definition. Is it correct anyway?","['convergence-divergence', 'sequences-and-series', 'calculus', 'analysis']"
1882221,Why is that the extended real line $\mathbb{\overline R}$ do not enjoy widespread use as $\mathbb{R}$?,"Let  $\mathbb{\overline R}$ denote the extended real line. In a course on topology, I have heard people say that compact Hausdorff space is a space which topologists love and  $\mathbb{\overline R}$ seems to carry a lot of those good properties. So that's why I don't quite understand why we insist working with $\mathbb{R}$ instead of  $\mathbb{\overline R}$? In undergrad analysis and topology, you almost never heard about $\mathbb{\overline R}$. What you hear instead is you hear your professor telling you that $\infty$ is not a number. But when we start talking about measure theory, which I had a hell of a time with, it seems to be standard to say that $m(\mathbb{R}) = +\infty$. To me it would seem to ease a lot of the troubles with the reals by simply capping it off at the end. Maybe I exaggerated, the real has its problems (for example our computer cannot represent every real, the physical world is quantized but we use the reals to represent physical quantities anyways...), but at least this seems to me be the logical step towards having a more ""comfortable"" space to work with. So can someone give a strong reason why $\mathbb{\overline R}$ do not enjoy as widespread of use as $\mathbb{R}$?","['general-topology', 'real-analysis', 'real-numbers']"
1882243,Calculate $\sum_{k=2}^{\infty}\frac{3}{5^{k-1}}$,Calculate $\sum_{k=2}^{\infty}\frac{3}{5^{k-1}}$ \begin{align}\sum_{k=2}^{\infty}\frac{3}{5^{k-1}}&=3\sum_{k=2}^{\infty}\frac{1}{5^{k-1}}\\&=3\sum_{k=2}^{\infty}\frac{1}{5^{k}}\cdot\frac{1}{5^{-1}}\\&=3\sum_{k=2}^{\infty}\left( \frac{1}{5} \right )^{k}\cdot5\\&= 15\left( \sum_{k=0}^{\infty}\left( \frac{1}{5} \right )^{k}-2 \right )\\&=15\left( \frac{1}{1-\frac{1}{5}}-2 \right )\\&=-\frac{45}{4}\end{align} Did I do it correctly?,"['sequences-and-series', 'calculus', 'analysis']"
1882262,Why Is $\sqrt{\det(A^TA)}$ A Volume / Volume Factor?,"The determinant of an $n\times n$ matrix is a volume / volume factor. So far, I'm good in my understanding. You take a linear map, encode it as a matrix, compute the volume of the parallelepiped (or whatever the proper name is) spanned by the column vectors, and look at the factor by which this transformation scaled the unit $n$-dimensional volume from before the transformation to the new one. That scaling is the determinant. There are many ways to view the determinant, but this is the most interesting to me, because I can visualize it. Now, what if I have a transformation from $\mathbb{R}^n$ to $\mathbb{R^m}$, encode it by an $m\times n$ matrix, and want the $n$-dimensional volume of the parallelepiped spanned by the column vectors of my matrix? This is a well-grounded question (think of a 2-d parallellogram embedded arbitrarily in 3-space: what is it's area?), but pretty much never addressed in linear algebra courses / books. Apparently (check e.g. the Wikipedia entry for determinants) I'm supposed to compute $\sqrt{\det(A^TA)}$ now. This makes sense in the $m=n$ scenario (except that orientation changes might be lost due to the square root?), since $|\det(A)|=\sqrt{\det(A)^2}=\sqrt{\det(A^T)\det(A)}=\sqrt{\det(A^TA)}$, but I just can't visualize it in the case $n\neq m$. I see that the end result of the product $A^TA$ is an $n\times n$ matrix, so clearly the determinant is then an n-dimensional volume / volume factor, but I can't see why I get the correct volume. Any help?","['determinant', 'linear-transformations', 'geometry', 'geometric-algebras', 'linear-algebra']"
1882294,How can we show that $\sqrt{\pi}=\lim_{k\to \infty}{1\over \sqrt{k}}\left(1+2\sum_{n=1}^{\infty}e^{-n^2\over k}\right)?$,"Limit $$\sqrt{\pi}=\lim_{k\to \infty}{1\over \sqrt{k}}\left(1+2\sum_{n=1}^{\infty}e^{-n^2\over k}\right)?\tag1$$ $$\sqrt{\pi}=\lim_{k\to \infty}{1\over \sqrt{k}}+\lim_{k\to\infty}{2\over \sqrt{k}}\sum_{n=1}^{\infty}e^{-n^2\over k}?\tag2$$ $$\sqrt{\pi}=\lim_{k\to\infty}{2\over \sqrt{k}}\sum_{n=1}^{\infty}e^{-n^2\over k}?\tag3$$ I checked via a sum calculator it seem to converges, but don't know how to verify it.","['sequences-and-series', 'limits']"
1882305,Eigenvalues of an infinite dimensional linear operator,"I have seen across the site questions regarding eigenvalues of linear maps between infinite dimensional vector spaces, and it regularly comes up that it is possible for there to be none or for every scalar to be an eigenvalue. When eigenvalues do exist, it seems that some of the familiar properties from finite dimensions carry across, such as this . I would define an eigenvalue of a linear operator $L$ as any scalar $\lambda$ such that for some non-zero $u \in V$, the relevant vector space, $(L-\lambda I)(u)=0$. Let's suppose that $X$ is a topological space, and I am working with some vector space of continuous functions from $X$ into $\mathbb{R}$, such as $C(X)$ or $C_b(X)$; call this vector space $V$. Let $L:V \to V$ be an endomorphism. I want to know what conditions on $X$, $V$ and/or $L$ : Imply the certain existence of eigenvalues for $L$. Imply the existence of a basis for $V$ of eigenfunctions/vectors of $L$. Ensure that there is a countable number of of eigenvalues, rather than a continuum. I have experienced, for example, the case in Sturm-Liouville theory where we have $X \subset \mathbb{R}$ compact, $V=\{C^\infty(X):$ boundary conditions met$\}$ and $L$ a second-order self-adjoint differential operator, where all three of these conditions are satisfied simultaneously; alternatively I have seen the same but with '$X$ compact' replaced with 'for all $f \in V$, $f$ vanishes at $\infty$'. Infinite differentiability and self-adjointness are rather strong and specific conditions in a more general context, but perhaps there are some weaker or related constraints which can be applied to more general topological spaces? I expect local compactness or compactness may be an important conditions on $X$, for a start. There are obviously some simple maps (such as the identity and the zero map) which have eigenvalues for any $X$, $V$, but I am interested in as broad a range of maps as possible. Edit: I would be satisfied with simultaneous conditions on $X$, $L$, $V$ which ensure at least point 1. I have seen plenty of discouraging non-examples, most taking the form of some kind of shift operator, but I reason that if some linear operators have eigenvalues, then there must be a way to characterise the class of operators which do in a way which will probably involve the underlying spaces. Having done some more research myself, the spectral theorem applies to all normal operators , which form a broader class than self-adjoint operators, but is still not exclusive to these (see section 6 of that link in particular), so my question remains.","['functional-analysis', 'real-analysis', 'linear-algebra', 'linear-transformations']"
1882331,Apply chain rule and product rule on matrix differentiation,"I have a question on taking the time derivative (i.e. w.r.t. t variable in the following function) of a quadratic function $V=x(t)^TPx(t)$. $t\in R^+$ has the physical meaning of time. $x(t)\in R^n$, $P\in R^{n\times n}$ is a positive definite matrix. x(t) has its own linear dynamics:$\dot{x}(t)=Ax(t),A\in R^{n\times n}$. I ran in to this problem when I was reading materials of Lyapunov function of linear systems, and from the materials I knew the answer should be $\dot{V}=x^T(A^TP+PA)x$. The above correct answer can be obtained by applying the product rule:
$\dot{V}=\dot{x}^TPx+x^TP\dot{x}=(Ax)^TPx+x^TP(Ax)=x^T(A^TP+PA)x$. But when I was deriving it, I use the chain rule, and got different answer:
$\dot{V}=\frac{\partial V}{\partial x}\dot{x}=\frac{\partial x^TPx}{\partial x}\dot{x}=x^T(P+P^T)(Ax)=x^T(PA+P^TA)x=2x^TPAx$, the last step was becasue of the symmetric property of $P$ being positive definite. Was there anything wrong I did with applying the chain rule? or should product rule being applied before chain rule? Thank you for your answer in advance!","['matrices', 'derivatives']"
1882347,How is the Kullback-Leibler distance between probability measures well defined,"Let $\Delta$ be the set of all probability measures on the measurable space $(S,\mathcal{B})$. Assume that all the measures in $\Delta$ are dominated by a measure $\mu$ on $(S,\mathcal{B})$ and let $\psi_Q = \dfrac{dQ}{d\mu}$ denote the Radon-Nikodym derivative of $Q$ w.r.t $\mu$. Given $\mu$, we can define $$K_{Q:P} = \int \psi_P \log \left (\dfrac{\psi_P}{\psi_Q} \right) d\mu.$$ It is already known that $K_{Q:P} \geq 0$. We can use either Jensen's inequality or Gibbson inequality to prove this. In one of the papers that I was going through ( this paper ), the author says that ""the quanity $K_{Q:P}$ itself is well defined and finite if and only if the function $\psi_P \log \left (\dfrac{\psi_P}{\psi_Q} \right)$ is integrable w.r.t $\mu$. Integrability yields in turn the relation $\psi_Q(x) > 0$ for $P$-almost all $x \in S$, which means $P$ is absolutely continuous w.r.t $Q$."" I am not able to see how the integral being finite will imply and will be implied by the absolute continuity of the measures. Because it is possible that the integral is $\infty$.","['probability-theory', 'measure-theory']"
1882362,Volume of tetrahedron using triple integral,"Find the volume between $x=0\\y=0\\z=0$ and $2x+y+z=2$ using triple integral Attempt: $$\int_0^2\int_0^2 \int _0^{(2-y-z)/2} \, dx \, dy \, dz=\cdots$$ I'm not so sure about choosing the limits for the middle integral, can someone please explain me how should I ""see"" what should be the limits for the middle one?","['multivariable-calculus', 'integration', 'definite-integrals']"
1882398,Nested families of balls in Banach spaces,"Is it possible to find a Banach space $X$, $x_i\in X$ and $r_i > 0$ indexed by some $i\in I$, possibly infinite, such that for each $n\in \mathbb{N}$ $$\bigcap_{i\in I} B\big(x_i, (1+\tfrac{1}{n})r_i\big)\neq \varnothing$$ but $$\bigcap_{i\in I} B\big(x_i, r_i\big)= \varnothing ?$$ Here $B(x,r)$ stands for the closed ball centred at $x$ with radius $r$. There is no restriction on cardinality of $I$ here (for instance, $I$ may be uncountable). Also the radii $r_i$ may be unbounded too. Note that $X$ cannot be isometric to a dual space (which rules out $\ell_1$ and all reflexive spaces, for example). For those who don't believe, here is the proof. If $X$ is a dual space, then all closed balls are weakly* compact by the Banach-Alaoglu theorem. However, 
$$\bigcap_{i\in I} B\big(x_i, r_i\big) = \bigcap_n \bigcap_{i\in I} B\big(x_i, (1+\tfrac{1}{n})r_i\big) $$
must be non-empty being the intersection of a descending sequence of compact sets .","['functional-analysis', 'banach-spaces', 'metric-spaces']"
1882399,An inequality involving hyperbolic sine functions,"The following problem arised in my research work and has been challenging me for several days: Prove that for all $r\in (0,1)$ and for all $x>0$, we have $$\frac{\sinh(r(2-r)x)}{r(2-r)x} \bigg[ \frac{\sinh((1-r)x)}{(1-r)x} \bigg]^2 > \frac{\sinh(x)}{x} \frac{\sinh((1-r)^2x)}{(1-r)^2x}.$$ Below are my thoughts. The inequality is obviously true when $x$ is large enough, thanks to the equivalence $\sinh(y)\sim \frac{1}{2}\exp(y)$, $y\rightarrow +\infty$, and to the fact that $$r(2-r)+2(1-r) > 1 + (1-r)^2$$ for $r\in(0,1)$. The inequality also holds true when $x$ is small enough, since it can be readily shown that the Taylor expansion of the difference between the LHS and the RHS has $$\frac{1}{45}r^2(1-r)^2(2-r)^2 x^4$$ as the leading term. However, the difficulty lies in establishing the claim for all $x>0$, which is ""testified"" by various graphic display softwares (Maple, Gnuplot, WolframAlpha). A first natural transformation is to take the logarithm of both sides and to invoke the convexity of the function $x\mapsto\ln(\sinh(x)/x)$. But this leads nowhere: Jensen's inequality alone cannot help us deriving $$f(r(2-r)x)+2f((1-r)x) > f(x)+f((1-r)^2x)$$ for any convex function $f$ (consider a linear function and its opposite). As a second attack, we can multiply both sides by $4r(2-r)(1-r)^2x^3$ and use the linearization formulae $$2\sinh(a)\sinh(b) = \cosh(a+b)-\cosh(a-b)$$ $$2\sinh(a)\cosh(b) = \sinh(a+b) + \sinh(a-b)$$ in order to obtain the equivalent inequality $$\sinh((2-r^2)x) -\sinh((r^2-4r+2)x) - 2\sinh(r(2-r)x) > 2r(2-r)x \big( \cosh((r^2-2r+2)x)- \cosh(r(2-r)x)\big).$$ The direct study of the difference between the new LHS and the new RHS appears to be unwieldy. Nevertheless, it is now straightforward to compute the Taylor series of this new difference. This series (whose radius of convergence is infinite) is equal to $$\sum_{n=0}^{\infty} \frac{a_n(r)}{(2n+1)!} x^{2n+1},$$ with $$a_n(r)= (2-r^2)^{2n+1} - (r^2-4r+2)^{2n+1} + 4n (r(2-r))^{2n+1} - (4n+2) r(2-r)(r^2-2r+2)^{2n}.$$ The first three coefficients are easily shown to vanish, i.e., $a_0(r)=a_1(r) = a_2(r) = 0$. Pushing further the calculations yields $$a_3(r)=448 r^3(1-r)^4(2-r)^3$$ $$a_4(r)=768 r^3(1-r)^4(2-r)^3 [3(1-r)^4 + 2(1-r)^2 + 3]$$ $$a_5(r)= 1408 r^3(1-r)^4(2-r)^3 [5(1-r)^8 + 12(1-r)^6 + 6(1-r)^4 + 12(1-r)^2 + 5].$$ These suggest that $a_n(r)$ is the product of $r^3(1-r)^4(2-r)^3$ with some even and symmetric polynomials in $1-r$ whose coefficients are all non-negative. The conjecture $a_n(r)> 0$ for $r\in(0,1)$ implies the desired result, but again I cannot prove it for $n\geq 6$. Is there a general technique to prove that the power series coefficients of a given function are all non-negative? According to what I could find in the litterature, this seems to be a delicate issue. Is there any fresh approach to the initial question? Many thanks for any help.","['hyperbolic-functions', 'real-analysis', 'inequality']"
1882409,Help with the limit of an integral,I am trying to evaluate this limit $$\lim \limits_{x \to \infty} \frac {1}{\ln x} \int_{0}^{x^2} \frac{t^5-t^2+8}{2t^6+t^2+4}    dt=? $$ Any help will be appreciated.,"['indefinite-integrals', 'integration', 'calculus', 'limits']"
1882415,Closed form for this sum with hyperbolic cotangent $\sum _{n=1}^{\infty }\frac{\coth (xn)}{n^3}$,Is there a closed form for this sum? $$\sum _{n=1}^{\infty }\frac{\coth (xn)}{n^3}$$ I have tried using $$x\coth \left(xn\right)=\frac{1}{n}+2n\sum _{k=1}^{\infty }\frac{1}{n^2+\left(\frac{\pi }{x}\right)^2k^2}$$ To solve but got stuck at evaluating $$\sum _{n=1}^{ \infty}\sum _{k=1}^{\infty }\frac{1}{k^2\left(n^2+\left(\frac{\pi }{x}\right)^2k^2\right)}$$,"['trigonometry', 'sequences-and-series']"
1882462,"Given the sum of elements of all $2^n$ subsets of a multiset with $n$ elements, find the elements of the multiset.","By multiset, I mean a set which can have repeated elements. I'm looking for an algorithm for this problem. Edit: The elements of the given multiset are all >= 0. Edit2: Example: Given {0, 1, 3, 3, 4, 4, 6, 7}, the output should be {1, 3, 3}","['combinatorics', 'algorithms']"
1882473,Good free online PDFs for probability and statistics (with practice problems and answers)?,I'd like to rebuild from the ground up and really make sure I have a strong grasp on probability and statistics. I am able to compute answers in a pinch but I'd like a more formal understanding so I can really ensure I am being systematic about it. For example I have no idea what a probability mass/density function is. Are there any good resources that go over all the basics and fundamentals while also providing answers to check my work against?,"['statistics', 'probability', 'online-resources']"
1882506,Which ODE systems can be integrated by quadrature?,"I am interested if it can be proven the non integrability by quadrature of a particular system of ordinary differential equations. And I read that the differential Galois theory could be the answer, but I want to be sure before I start studying it and realize after several weeks that I have wasted my time. Suppose you have a nonlinear system of first order of ordinary differential equations:
\begin{eqnarray}
\dfrac{d v}{d r}(r)&=&\dfrac{R\sinh(r)}{r\sinh(R)}\left[1-\zeta_1\sqrt{\sigma^2(r)+2(\sigma(r)-\beta(r))^2}\right]-\epsilon\left[1+\zeta_2\sqrt{\sigma^2(r)+2(\sigma(r)-\beta(r))^2}\right]\\&-&2\dfrac{v(r)}{r},\\
\dfrac{d\sigma}{d r}(r)&=&-\dfrac{2\beta(r)}{r},\\
v(r)\dfrac{d\beta}{d r}(r)&=&2\left(\dfrac{1}{e^{\beta(r)}+2}\dfrac{d v}{d r}(r)+\left(\dfrac{2}{e^{\beta(r)}+2}-1\right)\dfrac{v(r)}{r}\right),
\end{eqnarray}
with $r\in(0,R)$. It is possible with the differential theory of Galois prove that this system can or can't be integrated by quadratures? If it isn't possible, there is another way to prove this? Thanks in advance.","['galois-theory', 'ordinary-differential-equations', 'nonlinear-system']"
1882509,"Unresolved,why does negative exponent turns into a fraction?","As we know that, according to multiplicative identity 2 to the power of 3 means,
1*2*2*2=8;but why 2 to the power -3 is equal to the 1/8,if i think it is the reverse of multiplicative identity then from where 1 is coming?
i don't want only mathematical proof,but also the intuitive story behind this?
Why it is turning into a fraction and also why it positive fraction?
Why minus is vanished?","['algebra-precalculus', 'intuition']"
1882536,Evaluation of $I=\int \frac{1}{\sin (x-a) \sin(x-b) \sin(x-c)}.dx$,"Evaluate the given integral: $$I=\int \frac{1}{\sin (x-a) \sin(x-b) \sin(x-c)}.dx$$ When it was $I=\int \frac{1}{\sin (x-a) \sin(x-b)}.dx$, I solved it by multiplying and dividing by $\sin (b-a)$. But I am not getting how to proceed here.. Could someone help me with this?","['indefinite-integrals', 'integration', 'calculus']"
1882555,"The integral $\int_0^1 x^n \log(2-x) \, dx$","In this question I had stuck on the integral: $$\int_0^1 x^n \log(2-x) \, {\rm d}x$$ Claude Leibovici says that $\displaystyle \int_0^1 x^n \log(2-x) \, {\rm d}x=\frac{\, _2F_1\left(1,n+2;n+3;\frac{1}{2}\right)}{2 (n+1) (n+2)}=a_n+b_n \log 2$ where $a_n, b_n$ are sequences of rational numbers. The statement seems to be true since if we run the integral for some values of $n$ we get that: $$\begin{array}{||c|c|c|c|c||}
\hline
\text{Integral}&n & a_n & b_n & \text{Value} \\
\hline
\int_0^1 x \log(2-x) \, {\rm d}x&1 & -\frac{5}{4} & 2 & -\frac{5}{4}+ 2 \log 2 \\\\
\int_0^1 x^2 \log(2-x) \, {\rm d}x&2 & -\frac{16}{9} & \frac{8}{3} & -\frac{16}{9} + \frac{8}{3} \log 2 \\\\
\int_0^1 x^3 \log(2-x) \, {\rm d}x&3 & -\frac{131}{48} & 4& -\frac{131}{48} + 4 \log 2 \\
\hline
\end{array}$$ It seems that $a_n$ is a sequence of negative rational numbers and $b_n$ a sequence of positive rational numbers. However I don't see a pattern of how to connect those numbers. Does anyone see? Maybe it is worth it to take a look at the more general case: $$\int_0^1 x^n \log(\alpha -x) \, {\rm d}x \; \quad \mathbb{N} \ni \alpha \geq 2$$ That would be more tedious. What I had come as a solution but I could not see how to proceed was something like this: \begin{align*}
\int_{0}^{1}x^n \log(\alpha -x) \, {\rm d}x &= \int_{0}^{1}x^n \log \left [ \alpha \left ( 1 - \frac{x}{\alpha} \right ) \right ] \, {\rm d}x \\ 
 &=\int_{0}^{1}x^n \left [ \log \alpha + \log \left ( 1- \frac{x}{\alpha} \right ) \right ] \, {\rm d}x \\ 
 &=\log \alpha \int_{0}^{1} x^n \, {\rm d}x + \int_{0}^{1} x^n \log \left ( 1- \frac{x}{\alpha} \right ) \, {\rm d}x \\ 
 &= \frac{\log \alpha}{n+1} - \int_{0}^{1} x^n \sum_{n=1}^{\infty} \frac{\left ( \frac{x}{\alpha} \right )^n}{n} \, {\rm d}x \\ 
 &= \frac{\log \alpha}{n+1} - \sum_{n=1}^{\infty} \frac{1}{n \alpha^n} \int_{0}^{1} x^{2n} \, {\rm d}x\\ 
 &= \frac{\log \alpha}{n+1} - \frac{1}{2}\sum_{n=1}^{\infty} \frac{1}{n \alpha^n (2n+1)} \\
 &= ??
\end{align*} It seems that: $$\sum_{n=1}^{\infty} \frac{1}{n \alpha^n (2n+1)} = -2\sqrt{\alpha} \; {\rm arctanh}  \left ( \frac{1}{\sqrt{\alpha}} \right )  - \log \left ( \frac{\alpha -1}{\alpha} \right ) +2$$ I guess that won't be too difficult to prove taking into account that: $${\rm arctanh}(x) = \sum_{n=0}^{\infty} \frac{x^{2n+1}}{2n+1} $$","['special-functions', 'real-analysis', 'integration']"
1882557,Dense constructible set is open,"Is it true that a dense constructible set of a topological space is open? (It is likely that some conditions are needed on the topological space, maybe noetherianity). How could you prove it? Edit: a constructible set is a finite union of locally closed sets, and a locally closed subset is the intersection of a closed subset and an open subset. As is explained here , I know that a constructible set contains a dense open subset of its closure (when the topological space is noetherian), but my question is different.",['general-topology']
1882574,"$\operatorname{Hom}_k(k,V)$ is a vector space?","Is it true that a vector space is just the set of maps from the underlying field to the space itself.  I.e. if $V$ is a vector space of the field $k$ then
$$
V\cong \operatorname{Hom}_k(k,V)
$$ if so then this would make an intuitive understanding of the dual space $V^*$ somewhat trivial since
$$
V^{**}\cong V\cong \operatorname{Hom}_k(k,V)\implies V^*=\operatorname{Hom}_k(V,k) 
$$
If true, an explanation of why $V\cong \operatorname{Hom}_k(k,V)$ with a simple example or two would provide a lot of clarity for me since I could easily grasp the dual vector space idea from that point. Edit:  I actually had to read two proposed answers a couple times for the idea to sink in but I could only pick one answer.","['modules', 'abstract-algebra', 'functional-analysis', 'linear-algebra', 'vector-spaces']"
1882590,Paired t-test: More distance results in lower p-value?,"In my test I alter one parameter in three stages (p1 After plotting it, it is quite obvious that the parameter positively effects the outcome, because all 3 lines are above each other (no intersection). However, when I do the 3 paired t-test (p1 vs p2, p1 vs. p3, p2 vs. p3), the p-value of the test between the lower and the middle line is smaller than between the lower and the upper line. Logically it should not be the case, since the gap is larger for the extreme cases, right? I was wondering whether my t-test is wrong or whether the fact that the curves are not parallel and the upper line is fluctuating a bit more (but still never intersects even the middle line!) overrules the larger gap and therefore explains the worse p-value. (all p-values are very low, the bigger gap between the extremes is 1,70*E-23, the smaller gap between low and middle is 2,93*E-25) Looking forward to hearing your expertise on the subject. :)",['statistics']
1882632,"Let $X$ be a plane curve over $\mathbb{C}$ defined by an irreducible polynomial $f\in\mathbb{C}[z,w]$. Then $X$ is connected in $\mathbb{C}^2$.","Under the Zariski topology, this is true by basic Algebraic Geometry. However, I was wondering about how to prove this for the metric topology on $\mathbb{C}$. In Algebraic Curves and Riemann Surfaces by Miranda, this is stated as a fact, with a reference to Shafarevich (without giving page number). However, I can't quite see any obvious way to prove this. More formally, we let $f(z,w)\in\mathbb{C}[z,w]$ be an irreducible polynomial, and we define $X=\{(z,w)\in\mathbb{C}^2\mid f(z,w)=0\}$, and give $X\subset\mathbb{C}^2$ the subspace topology. I wish to prove that given disjoint closed sets $X_1$ and $X_2$ s.t. $X=X_1\cup X_2$, either $X_1=\emptyset$ or $X_2=\emptyset$. I'm struggling to find the connection between the Zariski topology and the metric topology.","['riemann-surfaces', 'differential-geometry', 'algebraic-geometry']"
1882633,Cox proof of product rule - step explanation,"I'm going through ""Probability Theory - The logic of science"" written by E.T. Jaynes and I have a problem with one step on page 27/28 in the proof of the product rule.
The idea here is that we have a function $F$ for which:
$$(AB|C)=F[(B|C), (A|BC)]$$
We want to proof its associativity:
$$F[F(x, y), z] = F[x, F(y,z)]$$
We assume that F is differentiable. We denote:
$$u = F(x,y)$$
$$v = F(y,z)$$
$$F_1(x,y) = \frac{\partial F}{\partial x}$$
$$F_2(x,y) = \frac{\partial F}{\partial y}$$
With this assumptions, we want to prove:
$$F(x, v) = F(u, z)$$
We differentiate w.r.t. x and y:
$$F_1(x,v)=F_1(u,z)F_1(x,y)$$
$$F_2(x,v)F_1(y,z)=F_1(u,z)F_2(x,y)$$
We then use:
$$G(x,y)=\frac{F_2(x,y)}{F_1(x,y)}$$
And get equation E1:
$$G(x,v)F_1(y,z)=G(x,y)$$
Which can be rewritten as equation E2:
$$G(x,v)F_2(y,z)=G(x,y)G(y,z)$$
Now there's the part is hard for me to understand. We differentiate E1 w.r.t. z and E2 w.r.t. y. The proof states that left hand sides of both derivatives are the same. How is that? When I'm using product rule of calculus and chain rule, I get summation in E1 left side:
$$G_2(x,v)F_2(y,z)F_1(y,z)+G(x,v)F_{12}(y,z)$$
and for equation E2, left side equals:
$$G_2(x,v)F_1(y,z)F_2(y,z)+G(x,v)F_{21}(y,z)$$
So there's the quation - where's my mistake? I can see that first product of both equations derivative are the same but how this equations are the same? I can see two options: first, I shouldn't use product rule here (why?), second, for some reason 
$$F_{12}(y,z)=F_{21}(y,z)$$
but I can't see reason for any of these.
I've checked original Cox proof in his book ""The algebra of probable inference"" but it's quite the same as in Jaynes book.","['probability', 'proof-explanation']"
1882637,generating function: how many possibilities are there to throw 10 different dice so that their sum is 25,"My discrete math textbook has the following solution to the problem using generating functions (the solution has to use generating functions): $$f(x)=(x+x^2+x^3+x^4+x^5+x^6)^{10}$$ Once we start developing the formal power series we get the following: $$x^{10}(1-x^6)^{10}\frac{1}{(1-x)^{10}}$$
Because of $x^{10}$ we now need to find the coefficient of only $x^{15}$:
$$\binom{10}{0}\binom{10-1+15}{15}-\binom{10}{1}\binom{10-1+9}{9}+\binom{10}{2}\binom{10-1+3}{3}$$
I completely understand the first two expressions but why are we using the third one:
$$\binom{10}{2}\binom{10-1+3}{3}$$
Where does the $3$ come from and why this isn't enough:
$$\binom{10}{0}\binom{10-1+15}{15}-\binom{10}{1}\binom{10-1+9}{9}$$
?","['generating-functions', 'combinatorics', 'dice']"
1882652,Are all associative operations essentially function composition?,"It is well known that function composition is associative. Is the converse true? Is any associative operation essentially expressible as function composition? If not, what's an interesting example of an associative operation that is not expressible as function composition?","['associativity', 'functions']"
1882679,"For a sequence $\{f_j\}$ of holomorphic functions, what can we say given $\sum_{j=1}^\infty |f_j(0)|$ converges.","In particular, let $\{f_j\}$ a sequence of holomorphic functions from $D(0,1)$ to $D(0,1)$ \ $\{0\}$, where $D(0,1)$ denotes the unit disk. I want to show that if $\sum_{j=1}^\infty |f_j(0)|$ converges, then $\sum_{j=1}^\infty f_j(z)^2$ converges absolutely and uniformly on compact sets in $D(0,1/3)$. I need a hint to get started. I think there may be a theorem relating to this kind of thing of which I am not aware.",['complex-analysis']
1882695,Find $\int_0^1\frac{\ln^2(1-x)}{x}\ dx$,"In solving $\displaystyle\int_0^\frac{\pi}{4}\dfrac{\ln(\sin x)\ln(\cos x)}{\sin x\cos x}\ dx,$ I have found that this is equal to $\dfrac{1}{16}\displaystyle\int_0^1\dfrac{\ln^2(1-x)}{x}\ dx.$ WolframAlpha says that the desired value is $\dfrac{\zeta(3)}{8},$ so I suspect a conversion to a series is necessary. How do I prove $\displaystyle\int_0^1\dfrac{\ln^2(1-x)}{x}\ dx=\displaystyle\sum_{n=1}^\infty\dfrac{2}{n^3}$? Note that the above integral can also be given as $\displaystyle\int_0^1\dfrac{\ln^2x}{1-x}\ dx$, which I know is equal to $\displaystyle\sum_{n=0}^\infty x^n\ln^2x.$ Also for reference, here is a picture of my original work to get to this point.","['integration', 'definite-integrals', 'sequences-and-series', 'calculus']"
1882715,"$P(x)\in\mathbb Z[x]$. There exist $n\in\mathbb Z^+$, $y\in\mathbb Z$ such that $\underbrace{P(P(\ldots P(y)\ldots))}_{n}=y$. Prove $P(P(y))=y$.","$P(x)\in\mathbb Z[x]$ . There exist $n\in\mathbb Z^+$ , $y\in\mathbb Z$ such that $$\underbrace{P(P(\ldots P(y)\ldots))}_{n}=y$$ Prove $P(P(y))=y$ . I think the notation $P^n(y)=y$ is also standard somewhere. If $n\in\{1,2\}$ , then it's trivial. I don't know the exact source of the problem. It could be an olympiad problem, but I'm not sure.","['algebra-precalculus', 'polynomials']"
1882722,Show that $\alpha^n\notin k$ for $n\ge 4$ where $\alpha\in\bar{k}$ and $[K:k]=n!.$,"Let $k$ be a field, let $f(X) \in k[X]$ be a separable polynomial of degree $n$ whose Galois group is isomorphic to $S_n$ , and let $\alpha$ be a root of $f(X)$ in some algebraic closure $k$ . (a) Show that $f(X)$ is irreducible. (b) Show that $Aut_k(k(\alpha)) = \{\mathrm{id}\}$ if $n \ge 3.$ (c) Show that $\alpha^n \notin k$ if $ n\ge 4.$ I know what to do for (a) and (b): (a) Let $G$ be the Galois group of $f$ . For the sake of argument, suppose $f(x)$ is reducible. Then $f$ can be written as a product of distinct irreducible factors (since $k[x]$ is a UFD). Let $\alpha_i, \alpha_j$ be distinct roots of two such irreducible factors. But then any $\sigma\in G$ must send $\alpha_i$ to another root of its minimal polynomial. In particular, we can't have $\sigma(\alpha_i) = \alpha_j$ , hence $G$ is not isomorphic to a transitive subgroup of $S_n$ . But $S_n$ is a transitive subgroup of itself, a contradiction. (b) It suffices to show that $\alpha$ is the only root of $f(x)$ in $k(\alpha)$ . Suppose there is another root $\beta\in k(\alpha)$ . Then over $k(\alpha)$ , $$f(x) = (x-\alpha)(x-\beta)g(x),$$ where $\deg g(x) = n-2.$ Let $K$ be the splitting field of $f$ over $k$ . Then since $G\cong S_n,$ we have $[K:k]=n!.$ And since $\deg g = n-2,$ we have $[K:k(\alpha)]\le (n-2)!,$ and $[k(\alpha):k]=n$ by the irreducibility of $f$ over $k$ . But then $$[K:k] = [K:k(\alpha)][k(\alpha):k]\le (n-2)!\cdot n<n!,$$ contradicting that $G\cong S_n.$ Now how do we attack (c)?","['abstract-algebra', 'galois-theory']"
1882746,Where are the irrationals in this set?,"Let's define a sequence of families of sets inside $[0, 1]$ interval: $$
F_n= \left\{ \left(\frac{j-1}{2^n},\frac{j}{2^n}\right] \mid \forall j \in \{ 1, \cdots, 2^n\} \wedge n \in \mathbb{N} \right\}
$$ Now, we know that $[0, 1] = \{0\} \cup(0, 1]$. Now we are goint to define a new set $S_n$ as follow: $$
S_n = \bigcup_{X \in F_n} X
$$ For me it's clear that $S_n = (0, 1]$ as long as $n$ is a natural fixed number, and so, it contains all irrational numbers in $(0, 1]$. The question is: does $S_\infty$ contain the irrationals? It is obvious that this set is not empty and contains rationals numbers. Now the questions: Does $S_\infty$ loses the irrationals? Are all rationals contained in the set $S_\infty$? Is $S_\infty = (0,1]$?? Many thanks in advance!!! (RE)$^2$EDIT: Lets define $S_\infty$. Let's start defining $F_\infty$: $$
F_\infty = \{ x \in (0, 1] \; | \; x \in X_j \wedge X_j \in F_n \; \text{with} \; j \leq n \; \forall n, j \in \mathbb{N} \}
$$ This implies $F_\infty$ would become a infinite (but countable?) partition of $(0,1]$ in disjoint subsets containing just a point. So, $S_\infty$ would just be: $$
S_n = \bigcup_{X \in F_\infty} X
$$ Thanks @Zev Chonoles and @Asaf Karagila for the useful comments!","['real-numbers', 'measure-theory', 'general-topology', 'metric-spaces', 'cantor-set']"
1882794,How to prove that no prime factor of $x^2-x+1$ is of the form $6k-1$,"Consider sequence $x^2-x+1$ ($1,3,7,13,21,31,43,57,73,91,\dots$). Let's consider prime factorization of each term. 
$$3=3$$
$$7=7$$
$$13=13$$
$$21=3\times7$$ It seems that the only prime factors we ever get are 3 and those of the form $6k+1$. In fact, prime factorization of the first 10 000 terms of the sequence gives 7233 distinct primes and all of them (except 3) are $6k+1$. That no member of the sequence is ever divisible by a prime of the form $6k-1$ is a purely empirical conjecture. Is there a formal proof for it (or a counterexample)?","['number-theory', 'prime-factorization', 'prime-numbers', 'elementary-number-theory']"
1882866,A combinatorial interpretation of a counting problen,"This question is about a surprising formula that is the answer to a counting problem. To me it suggests that there might just be a nice combinatorial interpretation of the problem that I have not yet found. Even if there is not, I'm sure there are much more elegant solutions than mine. The problem Suppose we have an $n\times m$ grid of squares with $n$ rows and $m$ columns. In every square we write the number of rectangles that can be made inside the grid and in which it is contained. Consider for example the green square in the image below. We see that it is contained in $8$ rectangles that can be made inside this $2 \times 3$ grid. If we count this number for every square we get. Now let $f(n,m)$ denote the sum of these numbers, so $f(2,3)=4 \cdot 6 + 2 \cdot 8=40$. The task is to find a general formula for $f(n,m)$. My solution Consider the square with coordinates $(i,j)$, as in the image below. Any rectangle that it is contained in must have its top left corner in the area shown in red. Its bottom right corner must be in the area shown in blue. So the square $(i,j)$ is contained in exactly $\color{red}{ i \cdot j} \cdot \color{blue}{ \left( n - i + 1 \right) \cdot \left( m - j + 1 \right)}$ rectangles. Now we only need to sum over all squares to get 
$$
f(n,m)= \sum_{i=1}^n \sum_{j=1}^m \left [ i \cdot j \cdot 
\left( n - i + 1 \right) \cdot \left( m - j + 1 \right) \right ].$$
To make this calculation a little easier we can introduce $S_n = \displaystyle \sum_{i=1}^n i$ and $T_n = \displaystyle \sum_{i=1}^n i^2$ so that 
\begin{align*}
f(n,m) &= \sum_{i=1}^n \sum_{j=1}^m \left [ i \cdot j \cdot 
\left( n - i + 1 \right) \cdot \left( m - j + 1 \right) \right ] \\
&= \sum_{i=1}^n \sum_{j=1}^m \left [ i^2 j^2 - m\cdot i^2 j -i^2 j - n \cdot i j^2 -i j^2+ n m \cdot i j+ m \cdot i j + n \cdot i j +i j \right ] \\
&= T_n T_m - m\cdot T_n S_m - T_n S_m - n \cdot S_n T_m - S_n T_m + n m \cdot S_n S_m + m \cdot S_n S_m + n \cdot S_n S_m + S_n S_m  \\
&= \left(S_n + n\cdot S_n - T_n\right)\cdot \left(S_m + m\cdot S_m - T_m\right)
\end{align*} Now we can use the well known formulas for $S_n$ and $T_n$ to find that 
$$
S_n + n\cdot S_n - T_n = \frac{1}{6} n (n+1) (n+2) = \binom{n+2}{3}.
$$
Putting this together we get that 
$$
f(n,m) = \binom{n+2}{3} \cdot \binom{m+2}{3}.
$$ I was pretty surprised by the simple nature of this answer. Especially by the fact that it seems to suggest that somewhere along the line $3$ things get chosen out of $n+2$ things and also from $m+2$ things. It gives me a small spark of hope that there exists a nice counting argument that solves this problem. Can anyone give a combinatorial interpretation of the formula for $f(n,m)$? More elegant solutions to the problem are also welcome.","['combinatorics', 'rectangles']"
1882884,How can I find the presentation of the hyperplane in a linear system?,"Consider the family:
$$
\begin{matrix}
X & = & \textbf{Proj}\left(\frac{\mathbb{C}[s,t,u][x,y,z]}{s(x^4 -y^2z^2) + t(x^2y^2 - z^4) - u(x^4 + y^4 + z^4)}\right) & \\
\downarrow & & \downarrow\\
B & = & \textbf{Proj}(\mathbb{C}[s,t,u])
\end{matrix}
$$
How can I find the hyperplane $H_x \subset B$ of points whose fibers contain a point $x \in X$? For example, let $x = [0:1:1:1:(-1)^{(1/3)}:(-1)^{(1/3)}]$. For reference, I am looking at Nicolaescu's introduction to morse theory (page 252). For reference, here is what I have tried: if I have the point $[s:t:u:a:b:c] \in X$, then I think the hyperplane will be spanned by the set of points
$$
\{s,t,u \in \mathbb{C}: s(a^4 - b^2c^2) + t(a^2b^2 - c^4) + u(a^4 + b^4 + c^4)\}
$$
I am guessing this because of the statement afterwards
$$
H_x = \{ P \in U : P(x) = 0 \}
$$
where $U$ is the projective plane parametrizing this family, but I am not sure why this is true.","['divisors-algebraic-geometry', 'projective-geometry', 'algebraic-geometry', 'projective-schemes']"
