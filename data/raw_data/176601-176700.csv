question_id,title,body,tags
3174357,Evaluating $\int_0^{\infty} \frac{\sin t}{t^{\alpha}} \mathrm{d}t$,"Find all $\alpha \in \mathbb{R}$ such that : $$I_\alpha = \int_0^{\infty} \frac{\sin t}{t^{\alpha}} \mathrm{d}t$$ converges My book says the following : 
We know that $\int_1^\infty \frac{\sin t}{t^{\alpha}} \mathrm{d}t$ converges for all $\alpha > 0$ . Moreover near $0^+$ we have : $\frac{\sin t}{t^{\alpha}} \sim 1/t^{\alpha-1}$ thus : $\int_0^1 \frac{\sin t}{t^{\alpha}} \mathrm{d}t$ converges for all $\alpha < 2$ . So the answer is $ 0 < \alpha < 2$ . I don't understand this argument and I feel like something is missing. Since we are dealing with non-absolutely convergente integrals we can't seperate cases like this right ? To be more clear the convergence of $\int_a^b f(t) \mathrm{d}t$ with $a, b \in \mathbb{R} \cup \{\pm \infty \}$ doesn't mean that for all $c \in (a,b)$ we have the convergence of : $\int_a^c f(t) \mathrm{d}t$ and the convergence of : $\int_c^b f(t) \mathrm{d}t$ right ? I mean this is true only for absolutely integrable functions ? So with this solution it only proves that for all $0 < \alpha < 2$ $I_\alpha$ converges but it doesn't prove that these are the only $\alpha$ for which $I_\alpha$ converges. Thus If I am not mistaken there is a missing argument in the above right ? I hope my problem is clear, thank you !","['integration', 'real-analysis', 'calculus', 'sequences-and-series', 'trigonometry']"
3174362,When is the matrix $I-P$ nilpotent?,"Note: sorry for the long post. My question is in the second quote. A few years ago, I saw the following problem on Facebook. I would like to ask a generalised version of this problem. Here is the original problem: A number of people sits at a roundtable. Each of them puts a coin in front of them with either head or tail facing up. Each of them is going to flip the coin in front of them (or not flip the coin) according to the following rules: Each of them checks the coin in front of the person to the right. If the coin to the right is tail up, flip the coin in front to the other side; if the coin to the right is head up, do not flip the coin in front. This flipping is done simultaneously, that is, they simultaneously check the coin to the right, and simultaneously flip (or not flip) the coin in front. After flipping the coins, they can repeat the above checking and flipping again, and again, until possibly all coins end up with head up, which no more flipping will happen, or maybe they cannot end up with all coins head up at all. For what number of people $n$ would it happen that no matter initially the people put the coins tail up or head up, they will always end up with all coins head up after those flipping? The answer is when $n$ is a power of $2$ (except $2^0$ ). Here is how a found it: The simultaneous state of the coins (whether they are head up or tail up) is modelled by a column vector of $n$ entries, with each entry being $0$ or $1$ . Say we index each person as $1,2,\dots,n$ (a certain person is number $1$ , and the guy to the left is number $2$ , and the guy to the left of number $2$ is number $3$ , and so on, with the guy to the right of number $1$ is number $n$ ). The $i$ -th entry of the vector being $0$ means the coin in front of the $i$ -th guy is head up, while being $1$ means the coin is tail up. As a note, the entries are to be interpreted as elements in $\Bbb Z/2\Bbb Z$ . The checking and flipping process is modelled by the matrix $I+P$ , where $I$ is the $n\times n$ identity matrix, and $P$ is the $n\times n$ permutation matrix where the sub-diagonal entries are $1$ , the top-right entry is $1$ , and all else entries are $0$ . Again, $I+P$ is a matrix whose entries are elements in $\Bbb Z/2\Bbb Z$ . A round of checking and flipping is modelled by multiplying the matrix $I+P$ to the column vector modelling the states of coins. It takes some effort to realise that any initial vectors will end up becoming zero vector after multiplying $I+P$ enough of times, if and only if $I+P$ is nilpotent. So we are to find out for what $n$ is $I+P$ nilpotent. We try to square the matrix $I+P$ , and see that $(I+P)^2=I+2P+P^2=I+P^2$ because $2P$ is the zero matrix (the entries are from $\Bbb Z/2\Bbb Z$ ). We keep squaring to see $(I+P)^{2^m}=I+P^{2^m}$ . So we want $I+P^{2^m}=0$ for large enough $m$ , or equivalently $P^{2^m}=I$ ( $I=-I$ in $\Bbb Z/2\Bbb Z$ ). We know that this is possible if and only if $n$ is a power of $2$ , because the order of $P$ is $n$ , so that $n$ must divide $2^m$ if we want $P^{2^m}=I$ . So $I+P$ is nilpotent if and only if $n$ is a power of $2$ . Now I generalise the problem to the following: Consider the $n\times n$ matrix $I-P$ whose entries are elements in $\Bbb Z/k\Bbb Z$ for some integer $k\ge3$ , where $I$ is the identity matrix, and $P$ is the permutation matrix whose sub-diagonal entries are $1$ , top-right entry is $1$ , and all else are $0$ . Find necessary and sufficient conditions on $n$ such that $I-P$ is nilpotent. When $k$ is a prime, the above method easily generalises to show that necessary and sufficient condition is that $n$ is a power of $k$ . Raise $I-P$ to the power of the prime number $k$ consecutively to see that $$(I-P)^k=I+C_1^k(-P)+C_2^k(-P)^2+\cdots+C_{k-1}^k(-P)^{k-1}+(-P)^k=I-P^k,$$ $$(I-P)^{k^m}=I-P^{k^m},$$ because $C_r^k=0$ in $\Bbb Z/k\Bbb Z$ for each of $r=1,2,\dots,k-1$ due to $k$ being prime (the prime factor $k$ in the numerator $k!$ is not cancelled by any factor in $(k-r)!r!$ ). We want $I-P^{k^m}=0$ for large enough $m$ , and this is possible if and only if $n$ is a power of $k$ . When $k$ is composite, the above trick does not work. I only made a few conjectures on what $n$ and $k$ works. Before that, note the following observation: if for a standard basis vector $e_i$ , $(I-P)^me_i=0$ for large enough $m$ , then $I-P$ is nilpotent. This is due to that $I-P$ is self-similar by $P$ : $P^{-1}(I-P)P=I-P$ , so that $(I-P)^me_i=0$ means $$(I-P)^me_{i+1}=(I-P)^mPe_i=PP^{-1}(I-P)^mPe_i=P(I-P)^me_i=0$$ as well, and $(I-P)^me_{i+2}=0$ , and so on. With this, we can do some faster checking whether $I-P$ is nilpotent. I checked that when $k=4$ , $n=2$ and $n=4$ are solutions (that $I-P$ is nilpotent would be nilpotent), and when $k=9$ , $n=3$ is a solution. I conjecture that when $k$ is a square of a prime, $n$ being a power of the prime is a necessary and sufficient condition for $I-P$ to be nilpotent. But I don't have any idea about other composite numbers.","['modular-arithmetic', 'matrices', 'nilpotence', 'discrete-mathematics', 'recreational-mathematics']"
3174374,Existence of an idempotent element $\not = 1$ and $\not = 0$,"I have a problem to solve the following problem: If $1=e_1+e_2$ with non-units $e_1,e_2 \in R$ and if $e_1e_2$ is nilpotent, then there is an idempotent element $e\not =0$ , $e\not =1$ . Perhaps it is a straightforward problem, but I could not solve it so far.","['algebraic-geometry', 'ring-theory', 'abstract-algebra', 'idempotents']"
3174403,Showing $f(t) = t^4 + 2t^2 + 9$ is reducible over $\Bbb Q$,"So, as a part of a homework problem, I'm tasked with showing that $f(t) = t^4 + 2t^2 + 9$ , despite having no roots in the rationals, is reducible over them. The former task - showing a lack of rational roots - is pretty easy, but the latter is proving an absolute pain. And I suspect it's actually not reducible. It is obvious that since $f$ has no rational roots, we know that, if $f$ is reducible, it must have a factorization $f = gh$ where $g,h$ are quadratics. If we made the substitution $u = t^2$ then $f(u) = u^2 + 2u + 9$ . This doesn't seem immediately factorizable by elementary methods (i.e. the whole ""sum of factors of constant term sum to the linear coefficient"" thing we learn in middle school). Okay so what now? My immediate thought was to play around with the fundamental theorem of algebra just to simplify matters. So we find roots for $f$ in terms of $u$ , which are $u = -1 + 4i \sqrt 2, \overline u = -1 - 4i \sqrt 2$ . Edit: As noted in one of the answers, by egreg, I actually did not find the correct roots in terms of $u$ : I forgot to divide the coefficient of the root by two. And, of course, other answers have also noted I overlooked some factoring techniques. Oh well, such is life. In any event, note that this error rippled forward from here in my work and made everything more problematic. Then by our substitution, the roots to $f$ in terms of $t$ are $\sqrt u, -\sqrt u, \sqrt {\overline{u}}, - \sqrt{\overline{u}}$ . Thus by the fundamental theorem of algebra, $$f(t) = (t-\sqrt u)(t-\sqrt {\overline{u}})(t+\sqrt u)(t+\sqrt {\overline{u}})$$ Surely, some pairing of these four factors in two groups of two will multiply together to give a quadratic in rational coefficients, yielding that $f$ is reducible. However, no matter how I pair them, I can't seem to make this work. Consider our first factor. If you multiply it by the second, the linear coefficient is $-(\sqrt u + \sqrt{ \overline {u}})$ . Wolfram gives this as $\sqrt{2\sqrt{33} - 2}$ . Obviously not rational. If you choose the third, then you get a square root of a nonreal complex number as the constant term. Thus, it is not rational. If we choose the fourth, then we get the linear coefficient to be $\sqrt{ \overline {u}} - \sqrt u$ . Wolfram gives this to be a purely imaginary number, same coefficient as before otherwise. So no matter which way we pair the first factor it seems we cannot generate a quadratic with rational coefficients. Does this mean there's some sort of mistake on the assignment, that this polynomial is irreducible? Am I overlooking something, making an error somewhere? (I've been dealing with this for a few hours now and it's not unrealistic, given how late the hour is, that I might have overlooked a flaw somewhere.)","['irreducible-polynomials', 'proof-verification', 'abstract-algebra', 'factoring', 'polynomials']"
3174407,Opposite real number identities(why $\cos(-x)=\cos x$),"Ive been studying about opposite real number identities and Ive been stuck on this question on why $\cos(-x)=\cos x$ .
Okay so if we consider that the given circle is a unit circle and triangle $pom$ and triangle $qom$ are congruent then how $\cos(-x)=\cos x$ ? According to me when we will do base/hypotenuse for triangle qom then it will come $om/oq$ which should give $-\cos x$ as $oq$ is negative, right? Please help me through this.",['trigonometry']
3174459,Using abstract Hilbert spaces to solve differential equations,"There are techniques for solving PDE's, such as Fock-Schwinger method in physics, which involve translating the problem from the language of distributions to the language of the abstract Hilbert spaces. For example, the Green's function equation is: $$L(i\partial_x,x)G(x,x')= \delta(x-x')$$ A typical approach to deal with such equations is to note that we can choose vectors in Hilbert space as: $$\langle x|x'\rangle  = \delta(x-x')$$ and, having this in mind, move $x$ and $i\partial_x$ ""into"" Hilbert space according to the rule: $$i\partial_x \langle x|x'\rangle  = \langle x|P|x'\rangle  [1]$$ $$x \langle x|x'\rangle  = \langle x|Q|x'\rangle  [2]$$ where $P$ is chosen to commute with $Q$ as $[P,Q]=i * I$ . Therefore, inside bra-ket a linear operator $L$ is represented as: $$L(i\partial_x,x)\langle x|x'\rangle  = \langle x|L(P,Q)|x'\rangle $$ and analytic functions of $L$ , such as $exp(itL)$ , will move into bra-ket with some additional terms due to commutation. My question is the following. On what mathematical grounds can we use rules [1] and [2]? I see that they are somehow related to the Stone - von Neumann theorem, but I don't see the exact justification. Moreover, $\langle x|x'\rangle $ is being normalized in terms of distributions, not scalars, that is kind of a generalization to the usual notion of vector spaces with dot product, which also needs justification . Any textbook on Hilbert spaces or Quantum Mechanics that clarifies these rules? (I mean, in a more or less strict way, not ""on the physical grounds"")
Is there any other book which highly exploits this Hilbert space technique to solve different kind of problems? I proved similar relations in terms of distributions: $$\delta(x-x') = \int dX \space \delta(X-x) \space \delta(X-x')$$ $$x \space \delta(x-x') = \int dX \space \delta(X-x) \space x \space \delta(X-x')$$ and by definition of the derivative in the space of distributions $$\partial_x  \delta(X-x) \to -\partial_X  \delta(X-x) \to  +\delta(X-x)\partial_X$$ I finally get: $$i\partial_x \space \delta(x-x') = \int dX \space \delta(X-x) \space i\partial_X \space \delta(X-x')$$ which looks like a dot product in Hilbert space if with make a map: $$\delta(X-x) \to |x\rangle $$ $$\int dX \space \delta(X-x) \space \delta(X-x') \space \to \space \langle x|x'\rangle $$ And also, I have another question. Is there a proof that also justifies this kind of a map between distribution-like objects to Hilbert space vectors?","['hilbert-spaces', 'operator-theory', 'functional-analysis', 'mathematical-physics']"
3174488,How to show if $X$ is Killing field then it is tangent to the geodesic spheres centred at a point $p$?,"Let $M$ be a Riemannianiam manifold with Levi-Civita connection and $X $ be a smooth vector field on $M$ . Let $\phi : (-\epsilon, \epsilon) × V \to M$ be the local flow of $X$ in $M$ . Problem is- if $X$ is a killing field on $M $ , $p \in M$ and $U$ be a normal neighborhood of $p$ . If $p$ is the unique point satisfying $X(p)= 0$ then in $U$ , $X$ is tangent to the geodesic spheres centred at $p$ . My attempt: With some hints, I've got from here and there, I know that first, we need to have that $\phi(t,p) = p \forall t\in(-\epsilon, \epsilon)$ , which I am able to show. Next, I think to show the required, it suffices to show that for any $q \in exp_{p}(S(0,\epsilon) = S_({p}(\epsilon))$ , a geodesic sphere contained  in $U$ which is of the form $q= exp_{p}(v)$ for some $v$ such that $|v|=\epsilon$ , the radial vector joining $p$ to $q$ is orthogonal to $X(q)$ . But I don't know how to proceed to show this. 
Also, where do we use the uniqueness of point $p$ as the only zero of $X$ ? It would be lovely if anyone could provide me some hint to solve this. 
Thank you!!","['riemannian-geometry', 'differential-geometry']"
3174514,Fourier series of regular polygons,"The definition of a regular polygon by two real-valued functions $(x(t)$ , $y(t))$ – or alternatively by a complex-valued function $x(t) + iy(t)$ – suggests to calculate the Fourier series $a_k$ , $b_k$ by $$a_k \sim \int_0^{2\pi}(x(t)+y(t))\cos(kt)\mathrm{d}t$$ $$b_k \sim \int_0^{2\pi}(x(t)+y(t))\sin(kt)\mathrm{d}t$$ Not surprisingly the two functions $$a(t) \sim \sum_{k=0}^\infty a_k\cos(kt)$$ $$b(t) \sim \sum_{k=0}^\infty b_k\sin(kt)$$ are linearized approximations of the cosine and sine function (very much like the regular polygons are linearized approximations of the circle): Rotating the $n$ -gon by $\frac{\pi}{n}$ yields another figure with another Fourier series and with another linearized approximation of the cosine and sine function: For the rotated square, I already know an explicit formula for $a_k$ - thanks to user J.M.'s comment on another question : $$a^{(4)}_k(\pi/4) \sim \begin{cases}
+k^{-2} & \text{ for } k \equiv 1 \mod 8 \text{ or } k \equiv 7 \mod 8\\
-k^{-2} & \text{ for } k \equiv 3 \mod 8 \text{ or } k \equiv 5 \mod 8\\
 0 & \text{ otherwise } 
\end{cases}$$ and I guess it's a rather straight forward exercise to generalize this for arbitrary $n$ -gons. Nevertheless I didn't manage to find a concise closed formula for $a^{(n)}_k(\alpha)$ , $\alpha = 0,\pi/n$ . Question 1: Can someone give a closed formula for $a^{(n)}_k(\alpha)$ , $\alpha = 0,\pi/n$ ? (I assume this formula will only contain $k^{-2}$ terms. So it's more about the period length, positions of the zeros, and the alternation of the signs. Note that the smallest $k>1$ with $a^{(n)}_k, b^{(n)}_k \neq 0$ is just $k = n-1$ , see the gallery below.) Three other questions I have: Question 2: How does $a^{(n)}_k(\alpha)$ look like for arbitrary rotation angles – not just $\alpha = \pi/n$ ? Question 3: Does it make sense to ask for something like a ""convolution"" $c^{(n)}_k(\alpha)$ that takes the series $a^{(n)}_k(0)$ to the series $a^{(n)}_k(\alpha)$ by $$a^{(n)}_k(\alpha) = \sum_{m=0}^\infty a^{(n)}_m(0)c^{(n)}_{k-m}(\alpha)$$ If so: What would $c^{(n)}_k(\alpha)$ look like? Question 4: Is the exponent $2$ in $k^{-2}$ just a coincidence or is it by deeper reasons the dimension of the plane? Gallery","['fourier-series', 'trigonometry', 'fourier-analysis', 'polygons']"
3174538,"What is the intuition behind short exact sequences of groups; in particular, what is the intuition behind group extensions?","What is the intuition behind short exact sequences of groups; in particular, what is the intuition behind group extensions? I'm sorry that the definitions below are a bit haphazard but they're how I learnt about them, chronologically. In Johnson's ""Presentation $\color{red}{s}$ of Groups,"" page 100, there is the following . . . Definition 1: A diagram in a category $\mathfrak{C}$ , which consists of objects $\{A_n\mid n\in\Bbb Z\}$ and morphisms $$\partial_n: A_n\to A_{n+1}, n\in \Bbb Z,\tag{6}$$ is called a sequence in $\mathfrak{C}$ . Such a sequence is called exact if $$\operatorname{Im}\partial_n=\ker \partial_{n+1},\,\text{ for all }n\in \Bbb Z$$ [. . .] A short exact sequence in the category $\mathfrak{C}_{\Bbb R}$ of right $\Bbb R$ -modules is an exact sequence of the form $(6)$ with all but three consecutive terms equal to zero. [. . .] Also, ibid. , page 101, is this: It is fairly obvious that a sequence $$0\longrightarrow A\stackrel{\theta}{\longrightarrow}B\stackrel{\phi}{\longrightarrow}C\longrightarrow 0$$ is a short exact sequence if and only if the following conditions hold: $\theta$ is one-to-one, $\phi$ is onto, $\theta\phi=0$ , $\ker \phi\le\operatorname{Im}\theta$ . I'm reading Baumslag's ""Topics in Combinatorial Group Theory"" . Section III.2 on semidirect products starts with Let $$1\longrightarrow A\stackrel{\alpha}{\longrightarrow}E\stackrel{\beta}{\longrightarrow}Q\longrightarrow 1$$ be a short exact sequence of groups. We term $E$ an extension of $A$ by $Q$ . Thoughts: I'm aware that semidirect products can be seen as short exact sequences but this is not something I understand yet. My view of semidirect products is as if they are defined by a particular presentation and my go-to examples are the dihedral groups. Please help :)","['category-theory', 'exact-sequence', 'intuition', 'group-theory', 'soft-question']"
3174561,Infinite Abelian subgroup of infinite non Abelian group example,"My thought is that we may take GL(2,F) as the group and this is obviously infinite and non abelian since matrix multiplication does not commute. Then I thought that if we make $\langle g\rangle$ , for some $g$ in $\mathrm{GL}(2,F)$ , which will be cyclic and hence Abelian, for instance: $ g= 
\bigg[
\begin{matrix}
1&0\\0&2 
\end{matrix}
\bigg]
$ . Then $g^n$ will be in the form $ g^n= 
\bigg[
\begin{matrix}
1&0\\0&2^n 
\end{matrix}
\bigg]
$ . This is obviously infinite since $g^n=e \Leftrightarrow n = 0$ .
Would this example work? Much thanks in advance!","['group-theory', 'matrices', 'field-theory', 'abstract-algebra', 'abelian-groups']"
3174573,"Efficient method to find $H$ given by $H(x)=\int_0^x f(x-u) f(x-au) e^u \, du$","Question Let $f:[0,\infty) \rightarrow [0,\infty)$ be some continuously differentiable function and $a \in (0,1)$ then we define the function $H:[0,\infty) \rightarrow [0,\infty)$ by letting: $$H(x)=\int_0^x f(x-u) f(x-au) e^{-u} \, du.$$ Suppose we would like to compute $H$ numerically, then one way to do this would be to simply compute the integral in the right hand for each value of $x$ . I am however looking for a more efficient method as is possible for the special case $a=0$ . Special case $a=0$ In this case we define $K(x) = \int_0^x f(x-u) e^{-u} \, du$ and we see that $H(x)=f(x) K(x)$ . We now show that there is a simple method to compute K(x).
We find by a simple change of variables $v=x-u$ that: $$
K(x)=\int_0^x f(v) e^{v-x} \, dv =  \int_0^x f(v) e^{v}\, dv \cdot e^{-x}.
$$ Applying the product rule we obtain $K'(x)=f(x) e^x e^{-x} - \int_0^x f(v)\, e^{v} \, dv\, e^{-x}$ applying the definition of $H$ and some rewriting we obtain: $$
K'(x)=f(x)-K(x).
$$ This is a differential equation which can be solved much more quickly and $H(x)$ is easily obtained from $K(x)$ .","['integral-equations', 'ordinary-differential-equations', 'real-analysis', 'calculus', 'numerical-methods']"
3174583,Integral form of the conservation law $u_t+f(u)_x=0$,"Consider the  conservation law given by $$u_t+f(u)_x=0$$ We know that in general weak solutions are not smooth but are bounded in $L^{\infty}$ norm (they do not belong to Sobolev spaces). However while deriving the numerical schemes most of the books say, integrating the conservation law over $(a,b) \times (t_1,t_2)$ and applying fundamental theorem of calculus we get $$\int_a^b u(x,t_1)dx - \int_a^b u(x,t_2)dx= -\int_{t_1}^{t_2} f\big(u(b,t)\big)dt+ \int_{t_1}^{t_2} f\big(u(a,t)\big)dt$$ I have the following doubts: How can we perform integration by parts as the solution does not possess any regularity? If a function satisfies the above integral formulation, can we say that it is a weak solution? Conversely, if $u$ is a weak solution, will it satisfy the above integral formulation? If so how to prove it?
Thank you.","['hyperbolic-equations', 'numerical-methods', 'analysis', 'partial-differential-equations']"
3174599,What is the distribution of the angle between two random vectors?,"Let $x,y\stackrel{\text{i.i.d.}}{\sim}\mathcal{N}(0,I_d)$ be two random $d$ -dimensional standard normal distributed vectors and let $\theta$ denote the angle between them. ( $I_d$ denotes the $d$ -dimensional identity matrix). Do you know about a probability distribution that models either the distribution of $\theta\in[0,\pi]$ , or the distribution of $\cos(\theta)\in[-1,1]$ , or the distribution of $\frac{\cos(\theta)+1}{2}\in[0,1]$ ? A paper (on page 18, second paragraph) says that the third option is distributed according to $Beta(d/2,d/2)$ . Can you derive that? Or can you point me to a resource proving it? Or might it be that they're just using an approximation?","['probability-distributions', 'probability']"
3174680,Symmetric strategy for 100 prisoners and a light bulb problem,"Remark: The problem is the prisoners and lightbulb problem , but without the usual probabilistic frame (the ogre chooses as he pleases). Also, the strategy is symmetric and the dwarves don't have a sense of time. The link above does not give a solution for these hypothesis. In more detail, the problem goes as follows: $100$ immortal dwarves are captured by an immortal ogre in order for him to play a game. The dwarves are in separate cells and never communicate, and each day the ogre chooses arbitrarily one dwarf and brings him to a room with a lightbulb which is either on or off. The dwarf can leave it as it is or switch it.  On the first day, the dwarve decide on a strategy, and the lightbulb is off. The dwarves are able to go out if one day one of them can say that all $100$ dwarves have been taken to the lightbulb (and be right about it). Two important hypothesis: the choice of the ogre is arbitrary ( not random , arbitrary), but the game isn't unfair so he promises that he plans to take each dwarf an infinite number of times to the lightbulb room, the dwarves have no sense of time (so in particular they cannot know how many dwarves went to the room before them, or if the ogre took them to the room several times in a row). Equivalently, we could say the ogre takes a dwarf to the room whenever he wants. $ $ Question: before the game, all $100$ dwarves meet one (last?) time to decide on a strategy. A. Can they find  a winning strategy? B. [Contains hint for 1...] Can they find a winning symmetric strategy (i.e. each dwarf has the same action policy) ? I don't have an answer for B. I am putting this question here since I think some (very simple) combinatorics may be needed to find a proof for a symmetric strategy. Here is my answer for A: one of the dwarf is chosen as a 'leader': only him can switch on the light, and only him can speak to the ogre. So each time he switches it on (or not if it is already) and count the number of times he switched it on. All $99$ other dwarves can only switch the light off once if they find it on (and then do nothing more). Once the leader has switched the light on $100$ times (it is off at the beginning), he tells the ogre that all dwarves have seen the lightbulb. Using the fact that after any number of days, the ogre will still bring each dwarf again in the room, it is easy to show that the dwarves will win.","['game-theory', 'combinatorics']"
3174763,Is there a standard formulation of a 'null element'?,"I want to define the set $F$ of functions on a set $X$ that can be generated from a finite number of selected operations defined on $X$ (the set from which these operations are selected need not be finite, but each function needs to expressible using a finite number of operations). At first I had considered a subset of the infinite product... $$F(X)\subseteq X^X=\prod_{x\in X}X$$ ...for which $f\in F(X)$ is a function (whose value at $x$ is $f(x)=f_x$ ) but then I realized that said product does not include obvious cases where a function is undefined at a point in its domain, for example $f(0)$ where $f:\mathbb{C}\to\mathbb{C};\ f(z)=1/z$ . The easiest solution I could think of was to define a 'nonelement' $\bot$ so that if $f:X\to X$ , then $f(x)\ \text{undefined}\iff f(x)=\bot$ . This is basically saying that the set of solutions $y$ to the equation $y=f(x)$ is empty. For example, I might have $f:\mathbb{C}\to\mathbb{C};\ f(z)=1/z\implies f(0)=\bot$ . Then I can modify the product accordingly to include functions which are undefined at some point... $$F(X)\subseteq\prod_{x\in X}X\cup\{\bot\}$$ Obviously, there are some problems with having such a 'null element' a few of which were addressed in this question , but none of them are too daunting on their own. The real difficulty is getting various definitions to work together. I can see a lot of use for a null element, so imagine someone has already done this. Is there an accepted convention for such an element? are there any sources which look at it in detail? Abstract/universal algebra tag is for context - If $(X,S_{op})$ is the algebraic structure consisting of the set $X$ and the collection of operations $S_{op}$ , then $F(X)$ as described above is the equivalent of the class of elementary functions in $(X,S_{op})$ .","['elementary-set-theory', 'abstract-algebra', 'universal-algebra', 'reference-request']"
3174808,Solve $f(x)=\frac{1}{2}\int_{x-1}^{x+1}{f(t)}dt$,"Let $f(x)\in\mathbb{C}(\mathbb{R})$ , $\inf\lim\limits_{|x|\to\infty}{f(x)}=0$ (that is $\inf\lim\limits_{x\to+\infty}{f(x)}=0$ and $\inf\lim\limits_{x\to-\infty}{f(x)}=0$ ), such that $$f(x)=\frac{1}{2}\int_{x-1}^{x+1}{f(t)}dt$$ Prove that: $$f(x)\equiv 0,\  x\in\mathbb{R}$$ My attempt I try to solve this equation, but I have trouble in it. Obviously, $f(x)= ax + b$ satisfies the equation. However, I don't know how to deal with the general case. I want to get some help. Thanks.","['integration', 'calculus', 'real-analysis']"
3174816,Chern class of fiber bundle,"I have a fiber bunde $F\to E\stackrel \pi \to B$ and i want to calculate its first Chern class $c_1(E)=c_1(TE)$ . How can i do this? I read here , that $$TE \stackrel \sim = \pi^* TB \oplus T_\pi E, $$ where $T_\pi E$ consists of the tangent vectors tangent to the fibers. So $$c_1(TE)= \pi^* c_1(TB) + c_1(T_\pi E).$$ Its clear how to calculate $\pi^* c_1(TB)$ , but what about $c_1(T_\pi E)$ ? Is it true that $c_1(T_\pi E)= c_1(TF)$ ?. The bundle $T_\pi E$ seems to be in some sense $B\times TF$ , but i don't know how to make this precise.","['characteristic-classes', 'algebraic-topology', 'differential-geometry']"
3174861,Prove that there are infinitely many primes which are primitive roots modulo $N$,"Assuming $N$ has a primitive root, show that there are infinitely many primes which are primitive roots modulo $N$ . It is obviously true using Dirichlet's theorem on primes, but I want to prove without this. There is a given hint: Try to mimic the proof of that there are infinitely many primes of the form $3n-1$ , $4n+3$ or $5n\pm 2$ . This proof basically is as follows: If $N=q_1\cdots q_s$ is, say, congruent to 3 modulo 4, then one of $q_i$ should be congruent to 3 modulo 4. List all such primes $p_1,\cdots,p_r$ , and let $N = \alpha p_1\cdots p_r + C$ for some $\alpha$ and $C$ so that $N$ cannot be divided by any of $p_i$ but it must has a prime factor of the given form, leading to a contradiction. I tried to, but failed to show both steps: Can I derive that if $M = q_1\cdots q_s$ is a primitive root modulo $N$ then one of $q_i$ is also a primitive root modulo $N$ ? Counterexample by Robert: $2$ and $6$ are not primitive roots mod $7$ , but $2\cdot 6=12$ is. What if $q_i$ 's are primes? Counterexample by Annyeong: $52=2\cdot 2\cdot 13\equiv 3 \pmod 7$ is a primitive root but $2$ and $13\equiv 6$ are not modulo $7$ . Any other method to get the similar proof? I think $N$ should be sort of a polynomial of $p_1\cdots p_r$ , as in the proof for $2kp+1$ -primes How to choose $\alpha$ and $C$ above? We cannot prove that there are infinitely many primes congruent to a specific primitive root in this way, by Murty. (See the comment below by Vincent.) Any helps and hints are welcome! Update : Professor has retracted this problem from the homework.","['number-theory', 'elementary-number-theory', 'primitive-roots']"
3174866,Does the function $f(x) = \sum_{n=1}^\infty P(|X_n| > x)$ have any special properties?,"Consider independent random variables $X_1, X_2,\ldots$ . By the Borel-Cantelli lemma, if $X_n\to_{a.s.}0$ then for any $x > 0$ $$
f(x) = \sum_{n=1}^\infty P(|X_n| > x) < \infty.
$$ Question: Does the function $f:\mathbb R_{++} \to \mathbb R$ have a name and does it have any interesting properties?","['measure-theory', 'analysis', 'real-analysis', 'probability-theory', 'terminology']"
3174896,Why do the principle curvatures provide the maximum and minimum values of the normal curvature of a curve?,"I am having difficulty showing that the principle curvatures maximise and minimise the normal curvature of a given curve, say $\alpha(t) \in S$ where $S$ is a regular surface. Given $T$ is the unit tangent of $\alpha$ and $N$ being the Guass map on $S$ , the normal curvature is defined as $$k_n:=\langle \dot{T}, N \rangle$$ I want to understand why the eigenvalues of $-dN$ (i.e. the Wingarten map) maximise $k_n$ above. Furthermore I'm really curious to see if this is a consequence of some theorem /more general result describing how the eigenvalues/vectors of a matrix optimise various quantities associated with the matrix in question. If anyone has a spare moment to comment on either of these two questions I'd be very grateful! -----Partial Answer----- Ok after looking at the problem for a little while I realised you can express $k_n$ as $$k_n=-\langle T, \dot{N} \rangle=\langle T, -dN(\alpha') \rangle=\left\langle \frac{\alpha'}{|\alpha'|}, -dN(\alpha') \right\rangle$$ If we let $\alpha$ have unit speed to simplify the algebra, and express $-dN$ in some eigenbasis say $\{v_1,v_2\}$ so it is diagonal, we have: $\alpha'=(\lambda_1,\lambda_2)$ as written in the $\{v_1,v_2\}$ basis. The expression for normal curvature becomes $$k_n=\left \langle  \begin{pmatrix}\lambda_1 \\ \lambda_2\end{pmatrix}, \begin{pmatrix}k_1 & 0\\ 0 & k_2\end{pmatrix} \begin{pmatrix}\lambda_1 \\ \lambda_2\end{pmatrix}\right \rangle=\begin{pmatrix}\lambda_1 & \lambda_2\end{pmatrix} \begin{pmatrix}k_1 & 0\\ 0 & k_2\end{pmatrix} \begin{pmatrix}\lambda_1 \\ \lambda_2\end{pmatrix}=k_1\lambda_1^2+k_2\lambda_2^2$$ You can check that the above expression has maxima $k_1$ and minima $k_2$ when $\lambda_1^2+\lambda_2^2$ is bounded (as we have assumed because of $\alpha$ having unit speed). QED?","['curvature', 'geometry', 'eigenvalues-eigenvectors', 'differential-geometry']"
3174940,How to differentiate a matrix equation w.r.t a vector?,"I have a matrix equation that yields a scalar $$f(M) = MAM^T - 2 \sum_i^{N} \log(M_i)$$ Where $M$ is a $1 \times N$ row vector, and $A$ is an $N \times N$ matrix. As such, the result $f$ is a scalar. How does one take the derivative of $f$ w.r.t $M$ ? I have seen matrix cookbook define derivatives of matrices w.r.t specific index values, but I couldn't find a definition of differentiation w.r.t vectors. My intuition is something like $$\frac{\partial f}{\partial M} = 2(AM - M^{\circ -1})$$ with my reasoning being the two $M$ 's in the first term yield $2AM$ once differentiated, and the log term yields $M$ where each element is raised to the $-1$ power ( $\frac{d \log x}{dx} = x^{-1}$ ). Again, I'm not sure if I've done this correctly, and the fact that $f$ is a scalar makes it a bit more confusing.","['matrices', 'derivatives', 'calculus', 'linear-algebra']"
3174945,Convert a number to $1$ or $-1$,"Is there a way to convert any number to either $1$ or $-1$ depending on its sign? For example: 13    =  1
-13    = -1
-670.2 = -1
 8.22  =  1 Lets say X is the number, i could do X / X and i would always get $1$ but the minus would get lost.",['functions']
3174952,Find the value of $\lambda$ in $\frac{3-\tan^2 {\pi\over 7}}{1-\tan^2 {\pi\over 7}}=\lambda\cos{\pi\over 7}$,"Find the value of $\lambda$ in $$\dfrac{3-\tan^2 {\pi\over 7}}{1-\tan^2 {\pi\over 7}}=\lambda\cos{\pi\over 7}$$ The numerator looks similar to expansion of $\tan 3x$ , so I tried this $$\dfrac{3\tan {\pi\over 7}-\tan^3 {\pi\over 7}}{\tan {\pi\over 7}\left(1-\tan^2 {\pi\over 7}\right)}=\lambda\cos{\pi\over 7}$$ $$\dfrac{\left(3\tan {\pi\over 7}-\tan^3 {\pi\over 7}\right)\left(1-3\tan^2 {\pi\over 7}\right)}{\tan {\pi\over 7}\left(1-\tan^2 {\pi\over 7}\right)\left(1-3\tan^2 {\pi\over 7}\right)}=\lambda\cos{\pi\over 7}$$ $$\dfrac{\tan {3\pi\over 7}\left(1-3\tan^2 {\pi\over 7}\right)}{\sin {\pi\over 7}\left(1-\tan^2 {\pi\over 7}\right)}=\lambda$$ But I'm stuck here. Need help. Thanks in advance.",['trigonometry']
3174958,How to find Sturm-Liouville problem eigenvalue and function?,"So I have the following Sturm-Liouville problem: $$
 y'' + \lambda y = 0 
$$ Such that $ \lambda > 0 $ and the initial conditions are as follows: $$ y (0) + y'(0) = 0 $$ $$ y(1) + y'(1) = 0 $$ So my attempt at this goes something like this: I know that the $\lambda$ is positive so the solution must be: $$ y(t) = A\cos(\sqrt(\lambda)t) + B\sin(\sqrt(\lambda)t)$$ and: $$ y'(t) = -A\sqrt{\lambda}\sin(\sqrt{\lambda}t) + B\sqrt{\lambda}\cos(\sqrt{\lambda})t)$$ Such that $A$ and $B$ are constants. So I can evaluate the solution at the first initial condition: \begin{align}
&=A\cos(\sqrt{\lambda}0) + B\sin(\sqrt{\lambda}0) + -A\sqrt{\lambda})\sin(\sqrt{\lambda}0) + B\sqrt{\lambda}\cos(\sqrt{\lambda}0)\\
&=A  + B\sqrt{\lambda}
\end{align} Thus I know: $$ A = -B\sqrt{\lambda}$$ Evaluating at the second initial condition: \begin{align}
&=A\cos(\sqrt{\lambda}1) + B\sin(\sqrt{\lambda}1)  -A\sqrt{\lambda}\sin(\sqrt{\lambda}1) + B\sqrt{\lambda}\cos(\sqrt{\lambda}1)\\
&=A\cos(\sqrt{\lambda})  + B\sin(\sqrt{\lambda}) -A\sqrt{\lambda}\sin(\sqrt{\lambda}) + B\sqrt{\lambda}\cos(\sqrt{\lambda})
\end{align} I'm not sure where to go from here, I factored the second initial condition by cos and sin but that led me to a trivial solution for A and B like this: $$(A + B\sqrt{\lambda})\cos(\sqrt{\lambda}) + (B - A\sqrt{\lambda})\sin(\sqrt{\lambda}) = 0$$ Plugging in $$ A = -B\sqrt{\lambda}$$ . $$(-B\sqrt{\lambda} + B\sqrt{\lambda})\cos(\sqrt{\lambda}) + (B + B\sqrt{\lambda}\sqrt{\lambda})\sin(\sqrt{\lambda}) = 0$$ $$ (B + B\lambda)\sin(\sqrt{\lambda}) = 0$$ So assuming $B$ isn't $0$ , then I know $\lambda = (n\pi)^2$ but how do I find the value of B?
Any guidance would be greatly appreciated! Thank you.","['eigenfunctions', 'sturm-liouville', 'ordinary-differential-equations', 'partial-differential-equations']"
3174979,Calculate $\sum_{0 \le k } \binom{n+k}{2k} \binom{2k}{k} \frac{(-1)^k}{k+1}$,"Calculate $$\sum_{0 \le k } \binom{n+k}{2k} \binom{2k}{k}
 \frac{(-1)^k}{k+1}$$ My approach $$\sum_{0 \le k } \binom{n+k}{2k} \binom{2k}{k}
 \frac{(-1)^k}{k+1} = \\
\sum_{0 \le k } \binom{n+k}{k} \binom{n}{k}
 \frac{(-1)^k}{k+1} = \\
\frac{1}{n+1}\sum_{0 \le k } \binom{n+k}{k} \binom{n+1}{k+1}(-1)^k = \\
\frac{1}{n+1}\sum_{0 \le k } \binom{k - 1 - n - k}{k} \binom{n+1}{k+1}
$$ But unfortunately I have stucked, I don't know how I can finish that... The main obstacle which I see is $$\binom{- 1 - n}{k} $$ is looks so dangerous because $- 1 - n<0$","['summation', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
3175026,Explicit formulas for Stirling numbers of second kind,"I have seen some recursive formulas for Stirling I and II kind but I am especially interested in explicit formulas. I was thinking and get: $$ \left\{ {m \atop n} \right\} =  \sum_{k_i>0 \wedge k_1+k_2+...+k_n = m} \frac{m!}{n!}\cdot \frac{1}{k_1! \cdot k_2! \cdot ... \cdot k_n!}$$ I am not sure if it is ok but supposedly it is (I created it with combinatorics interpretation) Can somebody look at that and tell me if it is ok/wrong? By the way,if you have some nice formulas, I will be grateful for sharing that.","['stirling-numbers', 'discrete-mathematics']"
3175027,Symmetrizability of shallow water equations,"Consider the shallow water equation \begin{equation}h_t+(hu)_x=0\\
(hu)_t+\left(hu^2+\frac{g}{2}h^2 \right)_x=0
\end{equation} I want to know the entropy of this system? I understood that if their exists a change of variable which symmetrizes the system, then system admits strictly convex entropy.. But I am unable to proceed... Please help","['hyperbolic-equations', 'linear-algebra', 'analysis', 'partial-differential-equations']"
3175034,"For any $f:A\to A$ and any relation $R$ on $A$, define $S$ on $A$ by $aSb$ iff $(f(a),(f(b))\in R$. Does $S$ reflexive or symmetric imply it for $R$?","Let $A = \{1,2,3,4\}$ . For any function $f : A \rightarrow A$ and any relation $R$ on $A$ , we define the relation $S$ on $A$ by: for any $a,b \in A,   aSb$ iff $(f(a),(f(b)) \in R$ . Prove / Disprove: (a) $\forall$ functions $f : A \rightarrow A$ and all relations $R$ on $A$ , if $S$ is reflexive then $R$ is reflexive. (b) $\forall$ functions $f : A \rightarrow A$ and all relations $R$ on $A$ , if $S$ is symmetric then $R$ is symmetric. For both (a) and (b) I think they are false, but I am not sure on this as I do not know exactly how to start my proofs. For (a) could I just give an example where the function $f(a)$ equals a constant for every element in $A$ ? i.e. $f(a) = 1,  \forall a \in A$ . Then go from there to show $S$ is reflexive, but $R$ is not reflexive. For (b) I was thinking of doing the same thing, and assigning $f(a) = 1, \forall a \in A$ and showing an example where $S$ is symmetric but $R$ isn't.","['relations', 'proof-writing', 'discrete-mathematics']"
3175039,Is a rolling $z$-score a good proxy for a derivative?,"Given a time series $\{V_t\}_{t=1}^{n}$ , where $t \mapsto V_t \in \mathbb{R}$ , I want to have some smoothed notion of the ""derivative"" of this time series. It was recommended that I look at $$\tilde{V}_t = \frac{V_t - \text{$k$-step moving average of } V_t}{\text{$k$-step std dev of $V_t$}}.$$ The $k$ -step moving average of $V_t$ is $\frac{1}{k} \sum_{i=0}^{k-1}V_{t-i}$ . This quantity, empirically, seems to act something like derivative. (For a line it is constant, for a sine curve it is almost a cosine curve, etc.) Can someone explain why, mathematically, this would be a heuristic for a derivative? Thanks.","['time-series', 'statistics', 'discrete-mathematics', 'real-analysis']"
3175051,Domain of $y= \ln\lfloor x^2+x+1\rfloor$,"Find the domain of $ y=\ln\lfloor x^2+x+1\rfloor$ My attempt:- Since $x^2+x+1$ is always positive and also $\lfloor x^2+x+1\rfloor$ must be greater than 0 but $\lfloor x^2+x+1\rfloor$ is negative between $(-1,0)$ so the domain of the given question must be $x\in\mathbb R \setminus(-1,0)$ . Is there any another way to find the domain?",['functions']
3175093,Spivak's Notation on Linear Transformation,"At the bottom of page 3 of Spivak's Calculus on Manifolds Book the author mentions a linear transformation of a basis: $T:\bf{R^n}\rightarrow \bf{R}^m$ is a  matrix $A$ with $m$ rows and $n$ columns. But the transformation formulae of the individual basis, $e_i$ , is confusing: $T(e_i) = \sum_{j=1}^m a_{ji}e_j$ and the coefficients of $T(e_j)$ are the columns of A. To take a simple example if A is a 3*2 matrix $\begin{bmatrix}3&1\\2&3\\1&5\end{bmatrix}$ and we want to transform a $\bf{R}^2 \rightarrow \bf{R}^3 $ then $T(e_1)$ should be $\begin{equation} \begin{bmatrix}3&1\\2&3\\1&5\end{bmatrix} * \begin{bmatrix}1\\0\end{bmatrix} \end{equation}$ which is product of the rows of A and the first basis column vector giving the column vector $\begin{bmatrix}3\\2\\1\end{bmatrix}$ (similarly for $e_2$ ), but then the formulae for this should be $T(e_i) = \sum_{j=1}^{n} a_{ij}*e_j$ not what the author has written out. What am I missing?","['manifolds', 'multivariable-calculus']"
3175102,Relation between Levi-Civita connection and any another metric connection.,"On a Riemannian manifold $(M,g),$ we have $D^g,$ Levi-Civita connection, the only connection that is metric (i.e. $D^g g=0$ ) and without torsion (i.e. $T^{D^g}=0$ ). My question is that if I have say $\nabla$ another connection on $(M,g)$ that is metric (i.e. $\nabla g =0$ ) and it has torsion (otherwise $\nabla=D^g$ ) is there any relation between $\nabla $ and $D^g$ ?","['connections', 'riemannian-geometry', 'differential-geometry']"
3175106,Double Sequences solving method,"I am searching some information on what I think is called a real double sequence . What I call double sequence is a function from $\mathbb{N} \times \mathbb{N} \to \mathbb{R}$ . By opposition to this, I call simple sequence (just a regular sequence), a function $\mathbb{N}  \to \mathbb{R}$ . Recursion relation I learnt that for a simple sequence , there are methods for solving recursion definitions of a sequence. For example, the simple function $u(n)$ defined as $\forall n\in\mathbb{N}, u(n+1)= u(n)+ u(n-1)$ , we solve the characteristic polynomial associated with this recursion relation, and we find that the space of solution $S$ is the span of the Fibonacci numbers $F_n$ and Lucas numbers $L_n$ : $S\in Span(F_n,L_n)=Vect(F_n,L_n)$ . My problem However, I have some interests in double sequences $u(i,j)$ defined by a recursion relation so that $\forall (i,j)\in \mathbb{N}^2,u(i,j) = f_1(i,j) \cdot u(i,j-1) + f_2(i,j) \cdot u(i-1,j-1)$ , with the first term $u(0,0)$ being equal to a real constant, let's say $u(0,0) = \alpha$ ; and that $\forall (i,j)\in \mathbb{Z}^2, (i<0 \vee j<0),u(i,j) = 0$ so the terms with negative index in the recursion relation cancels out nicely. A change of variable? Someone suggested me to perform a change of variable so that we get a double sequence $v(i,j)$ with the form $\forall (i,j)\in \mathbb{N}^2,v(i,j) = g_1(i,j) \cdot v(i,j-1) + g_2(i,j) \cdot v(i-1,j)$ . So, it makes sense for me because it seems so much easier to compute. My questions My questions are: Is there a general method for solving $ \forall (i,j)\in \mathbb{N}^2.$ $$ v(i,j) = g_1(i,j) \cdot v(i,j-1) + g_2(i,j) \cdot v(i-1,j) ?$$ Is there a method if $g_1$ and $g_2$ are polynomials of second order of $i$ and $j$ : $g_1(i,j) = a + bi + ci^2 + dj + dj^2 + eij + fi^2j + gi^2j^2 + \dots $ ? Is there a matrix representation of this system that could be used to compute this? Edit due to Yuval Filmus' comment Edit: The comment turned out to be a little bit helpful with the user named Yuval Filmus telling me about generating functions and linking this book . Basically the idea of this clue is to replace the study of $u(i,j)$ by the study of $\sum \sum u(i,j) x^i y^j$ ; then, you replace the terms with partial derivatives as described on the book, and you turn your recursion problem in a PDE problem . However, I the problem is that the functions $g_1$ and $g_2$ are polynomials of order 2, and I don't know how to make it appear. So I would like to know if someone could explicitly turn my problem in a PDE problem then solve it? Then, once the function $\sum \sum u(i,j) x^i y^j$ has been found, you just do the do product with the family of functions $x^i$ and $y^j$ to get $u(i,j)$ . Side note: I didn't know about this method to solve recursion problem, but I knew the other way around: replace a PDE with $\sum \sum u(i,j) x^i y^j$ then identify the recursion relation, then solve it (assuming it was an easy one); then try to recognize it with the well-known Taylor expansions.","['partial-derivative', 'partial-differential-equations', 'sequences-and-series']"
3175139,"Prove: $[k+y-1-v]{v \choose k}\geq \sum_{j=0}^a(-1)^j \left( \sum_{i=0}^k{v-i \choose k-i}r_i(j)\right)+\epsilon(a,k,p)$","I'm studying the ramsey numbers, especially $R(3,6)=18$ for Graver and Jackel, and i have tried to understand the theorem $2$ for quite some time but I have not succeeded. Theorem 1: Let $G$ be a graph with girth $z$ and let $g_i$ be the number
of connected subgraphs of $G$ with $i$ edges. Then for all integers $w < z$ , $$(-1)^wI(G)\leq (-1)^w\sum_{i=0}^w(-1)^ig_i$$ Where the girth of a graph is the length of a shortest cycle contained in the graph. $I(G)$ : Maximum independient set of $G$ $G$ is an $(3, y)$ -graph if $3 > C(G)$ and $y > I(G)$ , that is to say, in a $(3,y)$ -graph there can not be triangles. Consider an independent set $H_1$ in a $(3, y)$ -graph $G$ and let $H_2$ be the subgraph spanned by the remaining points. A point $p$ of $H_2$ is said to be above a set of points $S$ from $H_1$ if all edges from $p$ to $H_1$ have their other end-point in $S$ . Any subgraph of $H_2$ will be said to be above $S$ if all of its points are above $S$ . Finally we define the graph with support $S$ (supp) as the subgraph in $H_2$ spanned by the points of $H_2$ which are above $S$ . Theorem 2: Let $G$ be a $(3, y)$ -graph with a independent set $H_1$ . In either case let $H_1$ contain $v$ points and let $r_i(j)$ be the number of connected subgraphs of $H_2$ with $j$ edges and having a total of $i$ edges from these points of $H_2$ to points of $H_1$ . Also let $G_j$ be
the set of connected subgraphs of $H_2$ with j edges. For $K$ a subgraph of $H_2$ , let $\omega(K)$ be the number of points of $H_1$ which are joined to $K$ by an edge and $\mu(K)$ equal the number of edges from $K$ to $H_1$ . Then $$[k+y-1-v]{v \choose k}\geq \sum_{j=0}^a(-1)^j \left( \sum_{i=0}^k{v-i \choose k-i}r_i(j)\right)+\epsilon(a,k,p)$$ where $a$ is odd and all subgraphs of $G$ with a $k$ -set as support have girth greater than $a$ , and where $$\epsilon(a,k,p)=\sum_{j=0}^a(-1)^j\left\{\sum_{G\in G_j}\left[{v-\omega(G) \choose k-\omega(G)}- {v-\mu(G) \choose k-\mu(G)}\right] \right\}$$ Proof (reformulated for me): Given that $G$ is $(3,y)-graph$ and $H_1$ is a independient set, then $v=|H_1|\leq y-1$ , that is to say $(y-1)-(v-k)\geq 0$ . Now, get $T$ a maximum independient set in $K$ $(|T|=I(K))$ , then $T\sqcup(H_1-S)$ is a independient set content in $G$ ; then $$y-1\geq |T\sqcup(H_1-S)|=I(K)+(v-k)$$ $$[k+y-1-v] \geq I(K)$$ for theorem $1$ $$I(K)\geq \sum_{i=0}^a(-1)^ig_i$$ where $g_i$ is the number of connected subgraphs of $K$ which have $i$ edges. Hence $$[k+y-1-v] \geq \sum_{i=0}^a(-1)^ig_i$$ part of the test that I do not understand: Now summing this inequality over all subsets of $H_1$ containing $k$ points, the left side is $$[k+y-1-v]{v \choose k}$$ To compute the right side consider a connected subgraph $K$ with $j$ edges
( $K \in G_j$ ). K will be above exactly $${v-\omega(K) \choose k-\omega(K)}$$ $k$ -sets of $H_1$ and hence will appear in the summation that many times
with $(-1)^j$ as a coefficient each time. Thus $$[k+y-1-v]{v \choose k}\geq \sum_{j=0}^a(-1)^j\left\{ \sum_{K\in G_j}{v-\omega(K) \choose k-\omega(K)} \right\}$$ TRY: First, all subsets of $H_1$ containing $k$ points are ${v \choose k}$ , then $$[k+y-1-v]{v \choose k}\geq \sum_{i=0}^a(-1)^ig_i{v \choose k}$$ Second, I know that if $\omega(K)=|supp(K)|\leq k$ , then the number of $k$ -subset $S$ of $H_1$ such that $K$ is above $S$ are exactly $${v-\omega(K) \choose k-\omega(K)}$$ but i dont understand how can he replace $$\sum_{K\in G_j}{v-\omega(K) \choose k-\omega(K)}$$ for $$g_j{v \choose k}$$ can you help me understand that point, or will I be misunderstanding and not replacing.!!","['combinatorial-proofs', 'graph-theory', 'proof-explanation', 'combinatorics', 'ramsey-theory']"
3175200,Proof of $ f:I_n \rightarrow I_m \Rightarrow n = m$,"I am trying to improve my proof-writing skills. Would the following proof be correct for the $ f:I_n \rightarrow I_m \Rightarrow n = m$ bit? Problem: Prove that the notion of number of elements of a nonempty finite set is a well defined concept. More precisely, prove that there exists a bijection $ f:I_n \rightarrow I_m $ if and only if $n = m$ . Attempt: First prove $ f:I_n \rightarrow I_m \Rightarrow n = m$ . Assume that $ f $ is a bijective function such that $ f:I_n \rightarrow I_m $ By definition, since $f$ is injective, $ \forall a, b \in I_n, f(a) = f(b) \Rightarrow a = b $ , where $ f(a), f(b) \in I_m $ . Therefore every element in $I_m$ corresponds to at most one element in $I_n$ . $\quad (1)$ Also by definition, since $f$ is surjective, $ \forall b \in I_m, \exists a \in I_n $ . That is every element in $I_m$ corresponds to at least one element in $I_n$ . $ \quad (2)$ Now if $ n > m$ , then by $(2)$ some element in $I_n$ corresponds to an element in $I_m$ which is already mapped to. This cannot be true. Likewise, if $ n < m$ then by $(1)$ some element in $I_n$ corresponds to more than one element in $I_m$ . Again, this cannot be true. Therefore $n = m$ . EDIT: $I_n = \{  j \in \mathbb{N} ; 1 \leq j \leq n \}$ EDIT 2: Going into more detail as per the guidance in the comments. I have only proven statement $(1)$ as the proof for statement $(2)$ is similar. By definition, since $f$ is injective, $ \forall a, b \in I_n, f(a) = f(b) \Rightarrow a = b $ , where $ f(a), f(b) \in I_m $ . Therefore every element in $I_m$ corresponds to at most one element in $I_n$ . $\quad (1)$ Proof of this statement: Select an arbitrary element $b \in I_m$ . Assume that $f(a_1) = b$ and $f(a_2) = b$ where $a_1 \neq a_2$ . But since $f$ is injective we know that $f(a_1) = f(a_2) \Rightarrow a_1 = a_2$ . Therefore every element in $I_m$ corresponds at most to a single element in $I_n$ .","['elementary-set-theory', 'proof-writing', 'proof-verification', 'combinatorics']"
3175212,Understand the free group universal property applied to $D_n$,"For $n ≥ 3$ and $D_n$ the dihedral group of order $2n$ has the presentation $$\langle r, s : r^n = s^2 = srsr = 1\rangle.$$ Prove that for all $(a, b) \in (\Bbb Z/n\Bbb Z)^2$ , there exists a morphism $f$ verifying $f(r) = r^a , f (s) = r^b s$ . In the ""hint"" solution I have, it mentions the following: The universal properties of free groups and the quotient show that $f$ is well defined if $r^a$ and $r^b$ s verify the relations of $r$ and $s$ . I don't understand, what is exactly meant by the free group? Is it $\Bbb Z$ or $D_{2n}$ ? I know what is a free group but I can't really connect the dots. Thanks for any help.","['group-presentation', 'dihedral-groups', 'abstract-algebra', 'free-groups', 'group-theory']"
3175222,Converting Jordan Normal Form into Real Jordan Form,"Given the matrix $$\begin{bmatrix}
    0 & 0 & 0 & -8\\
    1 & 0 & 0& 16 \\
    0 & 1 & 0 & -14 \\
    0 & 0 & 1 & 6 \\
    \end{bmatrix}$$ find its real canonical form. Thanks to Wiki, I got the part where I finished Jordan normal form like below : \begin{bmatrix}
1-i & 0 & 0 & 0\\
0 & 1+i & 0 & 0 \\
0 & 0 & 2 & 1 \\
0 & 0 & 0 & 2 \\
\end{bmatrix} Now, I am stuck and have no clue how to convert this into ""REAL"" jordan form. 
Thank you.","['matrices', 'jordan-normal-form', 'linear-algebra', 'matrix-decomposition']"
3175225,When is the Subgroup Lattice Graded?,"Let $G$ be a finite group.  We say that the lattice of subgroups of $G$ is graded if it is possible to assign a non-negative integer rank $r(H)$ to each subgroup $H$ in such a way that the following two properties hold. If $H \leq K$ , then $r(H) \leq r(K)$ . If $H \leq K$ and no group is strictly between $H$ and $K$ , then $r(K)=r(H)+1$ . Examples of groups with graded lattices include Abelian groups ( $r(H)$ is the number of prime factors in $|H|$ , counted with multiplicity). The symmetric group $S_3$ (the identity has rank $0$ , the whole group has rank $2$ , all other subgroups have rank $1$ ) The quaternion group (identity has rank $0$ , $\{1, -1\}$ has rank $1$ , the subgroups generated by $i$ , $j$ , and $k$ have rank $2$ , and the whole group has rank $3$ ). An example of a group without a graded lattice is $A_4$ , the alternating group on $4$ elements.  One way to see this is to note that you have the two chains $$\{e\} \leq \{e, (12)(34)\} \leq \{e, (12)(34), (13)(24), (14)(23)\} \leq A_4$$ and $$\{e\} \leq \{e, (123), (132)\} \leq A_4$$ The two chains are both maximal ( $A_4$ has no subgroups of order $6$ ), but looking at the two chains give conflicting values for the rank of the whole group vs. that of the identity. Is there a general description of which groups do or do not have graded subgroup lattices? I feel like this should have been studied somewhere before, but can't find anything on it.","['finite-groups', 'order-theory', 'combinatorics', 'lattice-orders', 'group-theory']"
3175287,Time complexity for finding the nth Fibonacci number using matrices,"I have a question about the time complexity of finding the nth Fibonacci number using matrices. I know that you can find $F_n$ from: $
\begin{pmatrix}
1 & 1\\
1 & 0\\
\end{pmatrix}^n = \begin{pmatrix}
F_{n+1} & F_n\\
F_n & F_{n-1}\\
\end{pmatrix}
$ From above, $F_n$ is the entry in row 1, column 2 of the matrix. I have read in an online source that raising the Fibonacci Q-matrix to the power of n takes O(n) time. I am not sure why this is the case. Is matrix multiplication an operation that can be done in constant time on computers? That is the only reason I can see the above operation taking O(n) time (since you would have to do n-1 multiplications to raise the Q-matrix to the power of n). Any insights are appreciated.","['fibonacci-numbers', 'asymptotics', 'matrices', 'linear-algebra', 'computer-science']"
3175405,Are polynomials dense is L2 of the unit disk?,"Let $D$ be the unit disk in the complex plane, and let $X$ be the subset of $L^2(D)$ consisting of polynomials in the complex variable $z=x+iy$ with complex coefficients.  My question is, is $X$ dense in $L^2(D)$ ? If not, does anyone know of a function in $L^2(D)$ which cannot be written as an $L^2$ limit of polynomials?","['orthonormal', 'complex-analysis', 'hilbert-spaces', 'lp-spaces', 'functional-analysis']"
3175432,Constructing posets from collections of posets,"Say you have a bunch of posets consisting of partial orders over the set of, let's just say, letters in the English language (a, b, c, etc). Say you collect a bunch of these posets as objects of a discrete category, let's name it $\textbf{Conf}$ for ""confused,"" because I am. I'm wondering if it is possible to talk about constructing a particular poset over the set of English letters from the objects in this category. Suppose for example that you have 26 posets in the category labeled A, B, C, etc., and luckily $a$ is the ""greatest"" object in $A$ , $b$ is the ""second greatest"" object in $B$ , etc. It seems like it should be perfectly logical to say that you can draw a functor from this category to an individual poset that consists of $ z \leq ... \leq b \leq a$ . It would map $A$ to $a$ and so on, and the empty set of morphisms from $B$ to $A$ to the singleton set of morphisms from $b$ to $a$ and so on. In fact, I don't think you even need to say anything about $a$ being greatest in $A$ , $b$ being second greatest in $B$ , etc., nor about having 26 posets. You can just map however you like, really. But say you have a different rule. Say that $a$ is the terminal object for all objects of $\textbf{Conf}$ . Is it possible to have a functor that says ""if an element is a terminal object for all the posets in $\textbf{Conf}$ , then make it the terminal element in the poset that is the target of the functor from $\textbf{Conf}$ ""? On the one hand, doing so seems extremely intuitive. On the other hand, I don't understand how to do so. The objects of $\textbf{Conf}$ ""are"" posets, but only in the same way that, say, the elements of the set of mountain ranges ""are"" mountain ranges. It is not like the set of mountain ranges would be incredibly heavy to lift. Elements of sets don't have terminal objects, that's something a category itself has, so how can the posets in $\textbf{Conf}$ have terminal objects, and therefore, how can such a rule be constructed? I feel like this should be incredibly obvious, but I can't figure it out. Basically, I feel like collecting the posets into $\textbf{Conf}$ ""deletes"" the morphisms within each individual poset, and so I don't understand conceptually how to refer back to them. I thought that I had thought of a way to do it, which is to stick all the morphisms of each of the posets in $\textbf{Conf}$ into a single category with the letters of the English alphabet as objects and a morphism from $a$ to $b$ for each instance that a poset in $\textbf{Conf}$ has a morphism from $a$ to $b$ , and a morphism from $b$ to $a$ for each instance that a poset in $\textbf{Conf}$ has a morphism from $b$ to $a$ . So this category is no longer a poset (I don't know what it's called anymore), but it doesn't ""delete"" the morphisms. Then if $a$ was terminal for every poset in $\textbf{Conf}$ , it would still be terminal in this new category. But I don't believe that you can in general construct a poset from this new category since if it's true in this new category that $a \to b$ and $b \to a$ , then you cannot establish a functor from this new category to a poset since you will either fail to preserve composition or violate antisymmetry of the target poset. So...I guess my question is really just: how do you get a poset out of a collection of posets, and how exactly should you collect them in order to do it? And relatedly, what am I doing wrong? Here's some extra context.","['elementary-set-theory', 'category-theory']"
3175461,Solutions of sin(x) = cos(x),I know that the solutions to the equation $\sin(x) = \cos(x)$ are : $ x= \frac{\pi}{4}$ (45°) ; $ x= \frac{5 \pi}{4}$ (225°) However when I try to solve it algebraically I get the following : $$  \sin x = \cos x$$ $$  \sin^2 x = \cos^2 x$$ $$  \sin^2 = 1 - \sin^2 x$$ $$  2\sin^2 x = 1$$ $$  sin^2 x = \frac{1}{2}$$ $$ \sqrt {sin^2 x} = \sqrt{\frac{1}{2}}$$ $$ \sqrt {sin^2 x} = \sqrt{\frac{1}{2}}$$ $$ \sin x= \lvert\frac{1}{\sqrt2}\rvert$$ $$ \sin x= \frac{\sqrt2}{2}  ;  \sin x= -\frac{\sqrt2}{2}$$ So if I look for all the values of $x$ that solve the above I should get not only $ x= \frac{\pi}{4}$ (45°) ; $ x= \frac{5 \pi}{4}$ (225°) but also $ x= \frac{3\pi}{4}$ (135°) ; $ x= \frac{7 \pi}{4}$ (315°). What am I doing wrong?,"['algebra-precalculus', 'trigonometry']"
3175473,The splitting of Galois representations,"Suppose $X$ is a smooth projective variety defined over a number field $K$ , then the etale cohomology $H^i_{et}(X,\mathbb{Q}_\ell)$ defines a continuous representation of the absolute Galois group $\text{Gal}(\overline{K}/K)$ . Suppose that for every good prime $\mathfrak{p}$ of $K$ , the characteristic polynomial of the Frobenius $F_{\mathfrak{p}}$ factors into \begin{equation}
P_{\mathfrak{p}}(T)=\text{Det}(1-F_{\mathfrak{p}}T)|_{H^i_{et}(X,\mathbb{Q}_\ell)}=f_{\mathfrak{p}}(T) \cdot g_{\mathfrak{p}}(T)
\end{equation} where the factorization happens in the ring $\mathbb{Z}[T]$ . To avoid trivial cases, let us assume $\text{Deg}\,f_{\mathfrak{p}}>0$ and $\text{Deg}\,g_{\mathfrak{p}}>0$ . Question: is $H^i_{et}(X,\mathbb{Q}_\ell)$ the direct sum of two Galois representations, i.e. $M_1 \oplus M_2$ , such that the characteristic polynomial of the Frobenius acting on $M_1$ (resp. $M_2$ ) is $f_{\mathfrak{p}}$ (resp. $g_{\mathfrak{p}}$ )? P.S. I gather if $P_{\mathfrak{p}}(T)$ can be factored further into product of polynomials of lower degree, we should combine correct factors to give the right $f_{\mathfrak{p}}$ (resp. $g_{\mathfrak{p}}$ ).","['etale-cohomology', 'number-theory', 'algebraic-geometry', 'galois-representations']"
3175501,Is the L2 limit of a polynomial holomorphic?,"This is a follow-up to my question here . Let $D$ be the unit disk, and for each $n$ let $f_n\in L^2(D)$ be a polynomial in $z=x+iy$ with complex coefficients.  And suppose that $f_n\rightarrow f$ with respect to the $L^2(D)$ norm for some $f\in L^2(D)$ .  My question is, is it necessarily true that $f$ is holomorphic? If not, does anyone know of a counterexample?  I ask because this is true for uniform convergence.","['complex-analysis', 'lp-spaces', 'hilbert-spaces', 'functional-analysis']"
3175560,How many degrees of freedom does a rank-1 matrix have?,"Perhaps a stupid question, but I never formally learned exactly what ""degrees of freedom"" are. Let $X = a b^T$ where $a \in \mathbf{R}^m$ and $b \in \mathbf{R}^n$ . Am I correct in thinking that $X$ has $m+n$ degrees of freedom since it's specified by $m+n$ numbers?","['statistics', 'linear-algebra']"
3175577,Is $\ 101\ $ the only solution of $\ 2^{n-1}\equiv 203\mod n\ $?,"I wonder whether the congruence $$2^{n-1}\equiv 203\mod n$$ with integer $n>1$ has only the solution $n=101$ . Up to $n=10^9$ , there is no solution. Since $n$ must be odd, every prime factor $p$ of $n$ must have $203$ as a quadratic residue modulo $p$ and $2^k\equiv 203 \mod p$ must be solvable. Deeper analysis reveals that the smallest possible prime factors are $17$ and $53$ . If $17$ is a prime factor , $n$ must be of the form $136k + 85$ . If $53$ is a prime factor , $n$ must be of the form $2756k +477$ .","['number-theory', 'modular-arithmetic', 'elementary-number-theory']"
3175693,Synchronization in Coupled Nonlinear Oscillators,"I hope with this first question I respect the standards of the forum :) I am currently working on an ANN model for Working Memory (Short Term memory in the Brain) and as a first step I am studying a reduced dimensional system. Unfortunately, I am kind of new to the theory of Nonlinear oscillators, therefore I have some doubts about how to approach the problem. I attach here the equations and what I found at the moment about the system and what I think it would be interesting to analyze. I have two 2-dimensional units that interacts with their outputs and synchronize on a limit cycle. $\dot d_1 = - d_1 -e_1 + \epsilon\sigma(d_2) \\
\tau_a \dot e_1 = g_a\sigma(d_1) - e_1 \\$ $\dot d_2 = - d_2 -e_2 + \epsilon\sigma(d_1) \\
\tau_a \dot e_2 = g_a\sigma(d_2) - e_2$ $\sigma(d_i) = 1 - \frac{2}{1+e^d}$ From some simple simulations one can see that the system above synchronize ( $||d_1-d_2||->0$ ) on a stable limit cycle. Limit_Cycle , Synch Now we can go with the questions: My intuition suggested me that to study the limit cycle's properties (with Poincaré-Bendixsson theorem for example) I can study the system on the synchronization manifold where $d_1=d_2$ . On this invariant manifold with with a fairly simple bifurcation analysis, one can study how the system's behaviour changes with changing the parameters that define the dynamics Q1) In order to study the synchronization of the two systems, what could be a good approach? I read several works but most of them are on systems that are diffusively coupled (the coupling depends on a function of the difference of the output/states of the system). Updates One idea could be to study the error dynamics. By defining the quantities $x = d_1 - d_2$ and $y = e_1 - e_2$ we obtain: $\dot x = - x - y + \epsilon(\sigma(d_2)-\sigma(d_1)) \\
\tau_a \dot y = g_a(\sigma(d_1)-\sigma(d_2)) - y \\$ The main issue is to 'get rid of' the quantity $(\sigma(d_2)-\sigma(d_1))$ and replace it with some function $f(x)$ of the only difference. Unfortunately, this is not possible since: $a(d_1, d_2) = \sigma(d_2)-\sigma(d_1) = \frac{e^{d_2}-e^{d_1}}{(1+e^{d_1})(1+e^{d_2})}$ Although the function $a(d_1, d_2)$ can not be expressed only as a function of the difference $x$ it could be interesting to see $a(d_1, d_2)$ as a time-varying function $f(x, t)$ that preserves some properties, for example $x f(x,t)<0, \forall x\neq0$ . At this point if one finds the right tools the GAS of the synchronization manifold could be proven. (Maybe some passivity analysis or similar for time-varying systems) (If somebody is interested in simulating the system by themselves, parameters that could be used are: $\tau_a = 2.7, g_a=97, \epsilon>2(1+1/\tau_a)$ )","['ordinary-differential-equations', 'lyapunov-functions', 'control-theory', 'nonlinear-system', 'synchronization']"
3175712,Arranging playdate groups,"At my kids' school, the kids are meeting in playdate groups of two girls and two boys every month. The groups are constructed to get as much variation in the groups over the months. Having seen too many cases of either of my kids getting paired up with the same one or two kids months in a row, I set my mind to making a small (F#) program that given a list of girls' names and a list of boys' names would output a list of lists of 4-tuples. A 4-tuple corresponds to a playdate group for four months (one playdate per kid per month). Each list of 4-tuples corresponds to a complete set of playdate groups for a four months' period. The list of lists is the smallest set of playdate group set ensuring ""complete variation"". This proved a LOT harder then expected! I have qualified my ""complete variation"" expectation in the following requirements: In every list of groups, each kid is present in exactly one group Each kid shall be in a playdate group with every other kid of same gender at least once and at most twice Each boy (girl, resp.) shall be in a group with every girl at least once, but as few times as possible For two consecutive lists of groups the number of same kids being in group together shall be as small as possible For a list of boys (a,b,c,d) and girls (A,B,C,D), a solution is:
[[(a,b,A,B), (c,d,C,D)],
 [(a,c,A,C), (b,d,B,D)],
 [(a,d,B,C), (b,c,A,D)],
 [(a,b,C,D), (c,d,A,B)]] This is done totally by hand and my problem is what a general algorithm would look like. Any ideas? I have made a function (based on the round-robin algorithm) that given a list of names, constructs the list of unique 2-tuples (e.g., for the example above: [a,b,c,d] --> [(a,b),(a,c),(a,d),(b,c),(b,d),(c,d)]), but I am not sure whether this is useful in solving my problem. The algorithm will eventually need to check that each of these combinations is matched by at least one group, but is there a need to generate the tuples up front. My intention here is fully altruistic! I would like to make a solution that all parents can utilise free of charge and the code will be published on GitHub! Any help is greatly appreciated!! Thanks
-- Thomas","['graph-theory', 'combinatorial-designs', 'combinatorics', 'algorithms']"
3175721,"Solving $\int_{c_1}^{c_2}dF = \int_{c_1}^{c_2}dG$ for $c_1$, $c_2$","Following an exercise in Uniformly Most Powerful tests and the correspoding Neyman–Pearson lemma,    it is possible to show that a certain test exists, where the critical values $c_1$ and $c_2$ can be solved under a certain $\alpha$ using the following system of equations: $$\left\{  
 \begin{array}{rl}
\mathbb{P}(c_1 < \chi^2_{2\lambda} < c_2) = 1 - \alpha \\
\mathbb{P}(c_1 < \chi^2_{2\lambda+2} < c_2) = 1 - \alpha 
 \end{array} \right.$$ Here $\lambda > 0$ is known. However, I'm not sure how to proceed: in one way, this corresponds to $$\left\{  
 \begin{array}{rl}
F(c_2) - F(c_1) = 1 - \alpha \\
G(c_2) - G(c_1)  = 1 - \alpha 
 \end{array} \right.$$ where $F$ and $G$ are the CDF's of $\chi^{2}_{2\lambda}$ and $\chi^{2}_{2\lambda+2}$ , but I can't see a way to extract the values using only inverses. On the other hand, this then seems like an integral equation $$\left\{  
 \begin{array}{rl}
\int_{c_1}^{c_2}dF = 1 - \alpha \\
\int_{c_1}^{c_2}dG  = 1 - \alpha 
 \end{array} \right.$$ but not having much experience with integration, I'm not sure how are such equations usually solved. Any tips or suggestions would be appreciated!","['calculus', 'systems-of-equations', 'statistics', 'real-analysis']"
3175743,2nd order non linear differential equation,"A physical problem that I start to solve led me to this Second order non linear differential equation: $$f(x)\biggl(\frac{\text d^2y}{\text dx^2}-f(x)y^\alpha\biggr)=a \frac{\text dy}{\text dx}+b $$ with this conditions: $y(0)=0$ and $y'(0)=0$ and where $y = y(x)$ and $a,b,\alpha$ are constants. The $f(x)$ is a simpler polynomial of the type: $$ f(x) = c+dx$$ with $c,d$ constants. I try different substitutions (for exemple $u=y^\alpha$ , $u=y^\alpha y'$ ) to find analytical solution but I could not to solve the problem. Can anyone help me or suggest some ideas to solve the equation. Thanks",['ordinary-differential-equations']
3175751,Prove $\mathrm{tr}(A^2) \leq\mathrm{tr}(A^TA)$,"Prove that $\mathrm{tr}(A^2) \leq \mathrm{tr}(A^TA)$ . I saw the below link before and think it is related to this question $A,B$ be Hermitian.Is this true that $tr[(AB)^2]\le tr(A^2B^2)$? but still can't solve it. I also tried to use $\mathrm{tr}(A^TA)\geq0$ .","['matrices', 'trace', 'linear-algebra']"
3175754,"Is the space of piecewise continuous functions on $[0,1]$ complete in $L^2[0,1]$?","If we consider $PC[0,1]$ as a subset of $L^2[0,1]$ , is it complete when equipped with the $L^2$ norm? I have been trying to prove this for some time but did not get very far. The search for a counterexample has also proved fruitless. I would therefore be grateful for some help. Thank you.","['lebesgue-integral', 'functional-analysis']"
3175786,"If $C_0, C_1, C_2, .., C_n$ are the binomial coefficients in the expansion of $(1+x)^n$","If $C_0, C_1, C_2,...,C_n$ are the binomial coefficients in the expansion of $(1+x)^n$ , prove that: $$C_{r}.C_{n} + C_{r+1}.C_{n-1} +......+ C_{n}.C_{r} = C(2n, n+r) =\dfrac {(2n)!}{(n-r)! (n+r)!}$$ Is there any way to approach this sort of questions using calculus (derivatives or integration)?","['integration', 'calculus', 'binomial-coefficients', 'binomial-theorem', 'derivatives']"
3175897,Figures and Numbers: Relating properties of geometric shapes and their Fourier series,"Consider two types of parametrized curves $\gamma:[0,2\pi]\rightarrow  \mathbb{R}^2$ open curves $\gamma_\sim(t) = (t,a(t) + b(t))$ closed curves $\gamma_\bigcirc(t) = (a(t),b(t)) = a(t) + ib(t)$ with $a(t)$ , $b(t)$ being $2\pi$ -periodic functions, i.e. $a(0) = a(2\pi)$ and $b(0) = b(2\pi)$ . These curves – considered as geometric shapes – have some properties, for example: symmetries (rotational and others) number of singularities (where the curve goes to infinity) number of vertices (where the curve is not smooth) number of straight edges being convex number of self-intersection points (only for closed curves) As a periodic function, $a(t) + b(t)$ has Fourier series $$a_k = \int_0^{2\pi}(a(t)+b(t))\cos(kt)\mathrm{d}t$$ $$b_k = \int_0^{2\pi}(a(t)+b(t))\sin(kt)\mathrm{d}t$$ For the sake of simplicity consider only such $a(t)$ , $b(t)$ with $$a(t) = \sum_{k=0}^\infty a_k\cos(kt)$$ $$b(t) = \sum_{k=0}^\infty b_k\sin(kt)$$ (constant factors omitted), i.e. $a(t)$ being an even function and $b(t)$ being an odd function. The Fourier series $a_k$ , $b_k$ have some properties, too, for example having an envelope of shape $k^{-c}$ or $c^{-k}$ for some $c\geq 1$ a periodic pattern by which the signs of $a_k$ , $b_k$ change between $+1$ , $0$ and $-1$ $a_k$ and $b_k$ having the same envelope $a_k$ and $b_k$ having equal or shifted periodic patterns Sticking to these sets of properties (of shapes and Fourier series) one may ask: How do these properties relate, and what can be told about a shape knowing the properties of its Fourier series and vice versa? Here is just one relation that I already have found : The Fourier series $$a_k = \begin{cases}
 +k^{-2} & \text{ for } k \equiv 1 \pmod n \\
 +k^{-2} & \text{ for } k \equiv (n-1) \pmod n\\ 0 & \text{ otherwise } \end{cases}$$ $$b_k = \begin{cases}
 +k^{-2} & \text{ for } k \equiv 1 \pmod n\\
 -k^{-2} & \text{ for } k \equiv (n-1) \pmod n\\ 0 & \text{ otherwise } \end{cases}$$ give rise to a regular $n$ -gon (with all its symmetries, vertices, and straight edges). My question (which is a big-list question of sort) asks for more examples, preferrably ""simple"" and otherwise ""astonishing"" ones. Going one step further: How are relations between the Fourier series of two different shapes reflected in relations between the two shapes (the one being a rotation, homeomorphic distortion or other transformation of the other) and vice versa? Again I have found a specific example: The Fourier series $$a_k = \begin{cases}
 +k^{-2} & \text{ for } k \equiv 1 \pmod {2n} \\
 -k^{-2} & \text{ for } k \equiv (n-1) \pmod {2n}\\  
 -k^{-2} & \text{ for } k \equiv (n+1) \pmod {2n}\\  
+k^{-2} & \text{ for } k \equiv (2n-1) \pmod {2n}\\
 0 & \text{ otherwise }  \end{cases}$$ $$b_k = \begin{cases}
 +k^{-2} & \text{ for } k \equiv 1 \pmod {2n} \\
 +k^{-2} & \text{ for } k \equiv (n-1) \pmod {2n}\\
 -k^{-2} & \text{ for } k \equiv (n+1) \pmod {2n}\\  
 -k^{-2} & \text{ for } k \equiv (2n-1) \pmod {2n} \\  
0 & \text{ otherwise }  \end{cases}$$ give rise to a regular $n$ -gon rotated by $\pi/n$ . And then this one : The Fourier series $$a_k = \begin{cases}  +\gamma^{-k} & \text{ for } k \equiv 1 \pmod 4
 \\  +\gamma^{-k} & \text{ for } k \equiv 3 \pmod 4\\ 0 & \text{
 otherwise } \end{cases}$$ $$b_k = \begin{cases}  +\gamma^{-k} & \text{ for } k \equiv 1 \pmod
 4\\  -\gamma^{-k} & \text{ for } k \equiv 3 \pmod 4\\ 0 & \text{
 otherwise } \end{cases}$$ with $\gamma = \sqrt{2+2\sqrt{3}+\sqrt{15+8\sqrt{3}}}=3.291795...$ gives rise to a square with rounded corners . Further examples are welcome. I wrote a little tool to play around with Fourier series and compare them with the shapes they give rise to. The following gallery shows by some simple examples what can be done and observed with it. If you are interested in using it by yourself, just drop  me an email. [There is a follow-up question to this one: The number $\pi$ in an unexpected context .] Edit : I would guess the following is true. The shape $\gamma_\bigcirc(t)=(a(t),b(t))$ defined by the series $a_k$ , $b_k$ is symmetric with respect to the y-axis iff $a_k, b_k = 0$ for even $k$ . In this case $\gamma_\sim(t) = (t,a(t)+b(t))$ is an odd function, i.e. invariant under rotation by $\pi$ around the point $(\frac{\pi}{2},0)$ .","['visualization', 'big-list', 'geometry', 'experimental-mathematics', 'fourier-series']"
3175909,Solve the second-order equation $y''=(2y+3)(y')^2$,"Solve the second-order equation $y''=(2y+3)(y')^2$ My Trial Let $z=z(y):\;y'=z.$ Then, \begin{align} y''=\dfrac{dz}{dx}=\dfrac{dz}{dy} \dfrac{dy}{dx}=z'\dfrac{dy}{dx}  .\end{align} So, \begin{align} \dfrac{dz}{dx}=(2y+3)(z)^2\iff \dfrac{1}{z^2}\dfrac{dz}{dx}=(2y+3)\iff \dfrac{1}{z^2}dz=(2y+3)dx\end{align} I'm stuck here as I am not sure of how to deal with the right-hand side, maybe I should treat it as partial integration or not. Can anyone provide a hint or help me continue?",['ordinary-differential-equations']
3176009,"On a sphere, what is the formula for a great circle in latitude and longitude","Let $\theta$ be latitude, $\phi$ be longitude. I need to find the formula for the great circle passing ( $\theta_0$ , 0) and (0, $\phi_0$ ).  This seems a easy and common problem, but I can not find any reference for it to check my answer. The closest problem may be the great-circle navigation problem. Can anyone help? I guess the answer to be $\frac{\phi \cos \theta}{\phi_0 \cos \theta_0} + \frac{\theta \cos \phi}{\theta_0 \cos \phi_0} = 1$ .  But I have difficulty to prove it","['spherical-trigonometry', 'geometry']"
3176018,Calculate $\sum_{k=0}^{n} \frac{(-1)^k}{2k+1}\binom{n}{k} $,"Calculate $\sum_{k=0}^{n} \frac{(-1)^k}{2k+1}\binom{n}{k} $ My attempt $$\sum_{k=0}^{n} \frac{(-1)^k}{2k+1}\binom{n}{k} = \\
\sum_{k=0}^{n} \frac{1}{2k+1}\binom{k-n-1}{k} = \\
\sum_{k=0}^{n} \frac{1}{2k+1}\binom{k-n-1}{-n-1} $$ now let $$ s:= -n - 1 = const $$ so $$\sum_{k=0}^{n} \frac{1}{2k+1}\binom{k-n-1}{-n-1}  = \\
\sum_{k=0}^{n} \frac{1}{2k+1}\binom{k+s}{s}
$$ And there I stucked.
I was thinking if I can use that formula: $$ \sum_k \binom{k}{m} = \binom{n+1}{m+1} $$ but $ \frac{1}{2k+1}$ is an obstacle for me. How I can ""remove"" that?","['summation', 'discrete-mathematics']"
3176023,"Should the isomorphism theorems be seen as an ""interface"" between algebra and category theory?","My first instinct when I thought about algebra in category theory, was to try to ""generalize the isomorphism theorems in category theory"". So I tried to prove the generalization of ""the image of a group homomorphism is isomorphic to the quotient group generated by its kernel"". But then I found out that in category subobjects are actually defined in terms of monomorphisms, which for the category Grp is essentially implicitly using that isomorphism theorem. So is it correct that I shouldn't be trying to prove the isomorphism theorems in category theory? Is it correct that instead, the isomorphism theorems should be seen as justifying talking about algebraic structures (among other structuers) in terms of structure preserving morphisms? in that sense they are like the ""interface"" between category theoretical algebra (e.g. talking about groups in terms of group homomorphisms) and ""set-theoretic"" algebra (talking about groups in terms of the elements of the group, and cosets and so forth).","['abstract-algebra', 'category-theory', 'group-isomorphism']"
3176052,Find the symmetrical matrix $A$ so that $Q(\vec x) = \vec x^TA \vec x$,"$Q(\vec x) = x_1^2+x_1x_2+x_2^2$ The matrix $A=\begin{bmatrix}1 & 0.5 \\ 0.5 & 1\end{bmatrix}$ seems to do the job. But what's the general procedure for finding a solution? I can just think of setting it up like this for more clarity: $\begin{bmatrix}x_1 & x_2\end{bmatrix}\begin{bmatrix}? & ?\\ ? & ?\end{bmatrix}\begin{bmatrix}x_1\\ x_2\end{bmatrix} =\begin{bmatrix}x_1^2+x_1x_2+x_2^2\end{bmatrix}$ But after that I'm lost, there surely must be some concepts I can apply.","['matrices', 'matrix-equations', 'linear-algebra', 'linear-transformations']"
3176072,Prove that $\lim_{a \to \infty} \sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = 1$.,"The above sum (without the $\lim$ notation) is convergent $\forall a \in \Bbb{N}^+$ , because: $$
\sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = \sum_{n=1}^{\infty} \frac{n! n! \dots n!}{n^n n^n \dots n^n} \stackrel{\quad \text{because} \\ \forall n \in \Bbb{N}^+ \ n! \le n^n}{\le} \sum_{n=1}^{\infty} \frac{n!}{n^n} \approx \\ \stackrel{\quad \text{Stirling-} \\ \text{approximation}}{\approx}  
 \sum_{n=1}^{\infty} \frac{\sqrt{2\pi n}\left(\frac{n}{e}\right)^n}{n^n} = \sum_{n=1}^{\infty} \frac{\sqrt{2\pi n}}{e^n} = \sqrt{2\pi} \sum_{n=1}^{\infty}\frac{\sqrt{n}}{e^n} = \\ = \sqrt{2\pi} \ Li_{-\frac{1}{2}}\left(\frac{1}{e}\right) \approx 1.7728
$$ It is also decreasing $\forall a \in \Bbb{N}^+$ : $$\sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = \sum_{n=1}^{\infty} \left(\frac{(n!)^{a-1}}{n^{(a-1)n}}\right)\frac{n!}{n^n} \le \sum_{n=1}^{\infty} \frac{(n!)^{a-1}}{n^{(a-1)n}}$$ Because the left side is multiplied by a term that is always $\le 1$ , namely $$\forall n \in \Bbb{N}^+ \quad \frac{n!}{n^n} \le 1$$ So the limit exists: $$\exists \lim_{a \to \infty} \sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = ? \in \Bbb{R}^+_0$$ I examined the sum in Python and wrote a code to calculate it up to $a=20$ , each time adding up the sum up to $n=1000$ : import math

def sum_term(n,a):
    return (pow(math.factorial(n),a))/pow(n,a*n)

for a in range(1,21):
    value = 0
    for n in range(1,1001):
        value += sum_term(n,a)
    print(""a = "" + str(a) + "" | lim = "" + str(value)) And its output was: a = 1 | lim = 1.879853862175259
a = 2 | lim = 1.3099287490030924
a = 3 | lim = 1.1368584537249211
a = 4 | lim = 1.065018132743388
a = 5 | lim = 1.0317992491522754
a = 6 | lim = 1.0157461094449747
a = 7 | lim = 1.0078393253936435
a = 8 | lim = 1.0039122029986458
a = 9 | lim = 1.0019544471210995
a = 10 | lim = 1.0009768562327848
a = 11 | lim = 1.000488346517213
a = 12 | lim = 1.0002441551281933
a = 13 | lim = 1.0001220735353726
a = 14 | lim = 1.0000610358724382
a = 15 | lim = 1.0000305177372775
a = 16 | lim = 1.0000152588244295
a = 17 | lim = 1.0000076294023905
a = 18 | lim = 1.0000038146990122
a = 19 | lim = 1.000001907349021
a = 20 | lim = 1.0000009536744026 It is pretty convincing that the limit tends to $1$ , however, I want to prove this mathematically. Is there a way to do this with perhaps the Squeeze theorem or other methods?","['summation', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
3176104,Proof $\sum_{k=1}^n (-1)^{k+1} \binom{n}{k}\frac{1}{k} = H_n$ by induction [duplicate],"This question already has answers here : Proving Binomial Identity without calculus (6 answers) Closed 4 years ago . I found interesting task:
Calculate $$\sum_{k=1}^n (-1)^{k+1} \binom{n}{k}\frac{1}{k}$$ I have calculated some first values and I see that it is $H_n$ . I found there tip that it can be solved by induction or by ""integral"" trick by considering $\sum_{k=1}^n(-1)^k{n\choose k}x^{k-1}$ I don't know what is that trick so I decided to solve it by induction. Let $S_n = \sum_{k=1}^n (-1)^{k+1} \binom{n}{k}\frac{1}{k} $ $$ S_1 = 1 = H_1 \text{ ok.} $$ $$S_{n+1} =  \sum_{k=1}^{n+1} (-1)^{k+1} \binom{n+1}{k}\frac{1}{k}  = \\
-\sum_{k=0}^{n} (-1)^{k+1} \binom{n+1}{k+1}\frac{1}{k+1}$$ but I have problem with use induction assumption. $$-\sum_{k=0}^{n} (-1)^{k+1} \binom{n}{k}\frac{n+1}{(k+1)^2} = \\
-(n+1)\sum_{k=0}^{n} (-1)^{k+1} \binom{n}{k}\frac{1}{(k+1)^2}$$ but know I have $\frac{1}{(k+1)^2} $ instead of something like $\frac{1}{k}$","['summation', 'discrete-mathematics']"
3176132,ODE for the differential of a flow on a manifold,"This question is a consequence of my horrible knowledge in differential geometry. It can be stated as follows. Consider the solution $y_p(t)$ to the ODE $$\partial_t y_p(t) = X(y_p(t)), \qquad y_p(0) = p$$ where we are given a manifold $M \ni p$ and a vector field $X$ (everything is smooth). This induces a flow $\varphi(t,p) = y_p(t).$ In Euclidian space I can write the equation for the derivative of this flow: $$
\partial_t D_x\varphi(t,x) = D_x X (\varphi(t,x)) \cdot D_x\varphi(t,x), \qquad D_x \varphi(0,x) = Id.
$$ My question is: how do I write this equation in and ""invariant"" way on a manifold? Crucially, what is the correct generalisation for the matrix product $ D_x X (\varphi(t,x)) \cdot D_x\varphi(t,x)$ ? In my not-knowledge of differential geometry I learned that differentiating vector fields it one of the slightly complicated things... Similar questions have been asked. Here the same question appeared, but the answer regards the determinant of the Jacobian and not the Jacobian, I believe ( https://mathoverflow.net/questions/284718/derivative-of-the-flow-for-odes-on-manifolds ). EDIT: In the answer I posted below it appears that given a connection it is possible to write down an ODE for the flow in the classical sense. The following question is still open: Is it possible to write an ODE for the differential in a generalized way (without assuming the existence of a connection), such that given a connection the equation reduces to the classical one?","['ordinary-differential-equations', 'differential-geometry']"
3176139,A proof for $\nabla |u|^p = p\ \text{sgn}(u)|u|^{p-1}\nabla u$.,"I believe the formula $$\nabla |u|^p = p\ \text{sgn}(u)|u|^{p-1}\nabla u
$$ to be true for $u\in W^{1,p}(\Omega)$ , where $\Omega$ is an open domain with nice boundary, but failed to find a good reference for this.  I am sorry if this has been asked already. The usual assumption that I usually see regarding the chain rule for Sobolev functions, i.e. $$
\nabla F(u) = F'(u)\nabla u,
$$ is that $F\in C^1$ with bounded derivative (or perhaps a Lipschitz function). I want $F(t)=|t|^p$ , which doesn't satisfy that assumption. My idea of the proof goes as follow: I'd like to mimic the proof of the case $p=1$ so first we show a similar result for $$
F_\varepsilon(t) = (\varepsilon^2 + t^2)^{p/2}
$$ by approximation with $F_\varepsilon(u_n)$ , where $u_n\to u$ in $W^{1,p}$ and $u_n\in C_c^\infty(\Omega)$ . Then we take $\varepsilon \to 0$ to approximate $|\cdot|^p$ . While the usual argument using the Dominated Convergence Theorem wouldn't work, I think Vitali Convergence Theorem might do the trick. Does anyone know a reference to this result? Also, if the formula doesn't hold in general spaces (i.e. $u\notin W^{1,p}$ ) then does anyone have a counterexample? I have seen the proofs of the cases $p=1,2$ . Finding a reference for a general $p\in [1,\infty)$ turns out to be a lot harder than I anticipated.","['reference-request', 'real-analysis', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
3176147,Help solving first-order differential equation [duplicate],"This question already has answers here : Differential equation $y=xy' + \frac{1}{2}(y')^2$ (4 answers) How to solve differential equation $y=xy'+\frac12y'^2$ [closed] (4 answers) Closed 5 years ago . I have first-order differential equation $$y=xy'+ \frac{1}{2}(y')^{2}$$ Maybe, with this someone will find way to solve it $$\frac{1}{2}y'(2x+y')=y$$ I thought I can use $x^2+y=t$ for subtitution and when I derivate, I have $t'=2x+y'\\(t'-2x)t'=2t-2x^2$ which is acctualy the same as previous. I don't have idea how to start..",['ordinary-differential-equations']
3176151,how to check a propriety using r studio,"I have to check that this propriety $Z \sim N(0,1)$ and $U\sim \chi ^{2}(10)$ then $ Z/\sqrt{U/10} \sim T(10)$ is true using r studio if anyone can help , much appreciate","['statistics', 'probability', 'hypothesis-testing']"
3176175,A proof for solution of first order differential equation,"Let $f\left( x \right)$ and $g\left( x \right)$ be continuous functions on $\left[ 0,\infty  \right)$ satisfying $f\left( x \right)>0$ for all $x\ge 0$ , $$
\int_{0}^{\infty }{f\left( x \right)dx=\infty \quad and\quad }M=\int_{0}^{\infty }{\frac{{{g}^{2}}\left( x \right)}{f\left( x \right)}dx<\infty }.
$$ My Question : I would like to show that a solution of the differential equation: $$
{y}'\left( x \right)=f\left( x \right)y\left( x \right)+g\left( x \right)
$$ Satisfying $$
\int_{0}^{\infty }{f\left( x \right){{y}^{2}}\left( x \right)dx<\infty }
$$ is uniquely determined and satisfies: $$
 \int_{0}^{\infty }{f\left( x \right){{y}^{2}}\left( x \right)dx<M}.
$$ Note :I think the proof of this statement is based on the formula which gives the solution of first order differential equation and the Cauchy-Schwarz inequality, however i don't know how to continue. Any ideas?","['integration', 'calculus', 'ordinary-differential-equations', 'real-analysis']"
3176215,"EDIT: Solve the second-order ODE $y''+\frac{1}{x}y'=a,\;a$ constant using variation of parameters","Note: I changed $y$ to $y'$ . Solve the second-order ODE $y''+\frac{1}{x}y'=a,\;\;a$ constant. For me to use Variation of parameters, I need to find two linearly independent solutions $y_1$ and $y_2$ , where the particular solution, $y_p$ is given by \begin{align} y_p(x)=-y_1(x)\int\dfrac{r(x)y_2(x)}{W(y_1,y_2)}dx+y_2(x)\int\dfrac{r(x)y_1(x)}{W(y_1,y_2)}dx  \end{align} and $W,\;r(x)$ are Wronskian and the right-hand side, respectively. Here, $r(x)=a.$ So, I need to look for the solution to $y''+\frac{1}{x}y'=0$ to get $y_1$ and $y_2.$ If I get them, I'm done. Is there any way of getting these two?",['ordinary-differential-equations']
3176228,Does the average primeness of natural numbers tend to zero?,"Note 1 : This questions requires some new definitions, namely ""continuous primeness"" which I have made. Everyone is welcome to improve the definition  without altering the spirit of the question. Click here for a somewhat related question . A number is either prime or composite, hence primality is a binary concept. Instead I wanted to put a value of primality to every number using some function $f$ such that $f(n) = 1$ iff $n$ is a prime otherwise, $0 < f(n) < 1$ and as the number divisors of $n$ increases, $f(n)$ decreases on average. Thus $f(n)$ is a measure of the degree of primeness of $n$ where 1 is a perfect prime and 0 is a hypothetical perfect composite. Hence $\frac{1}{N}\sum_{r \le N} f(r)$ can be interpreted as a measure of average primeness of the first $N$ integers. After trying several definitions and going through the ones in literature , I came up with: Define $f(n) = \dfrac{2s_n}{n-1}$ for $n \ge 2$ , where $s_n$ is the
  standard deviation of the divisors of $n$ . One advantage of using standard deviation is that even if two numbers have the same number of divisor their value of $f$ appears to be different hence their measure of primeness will be different. Question 1: Does the average primeness tend to zero? i.e. does the following hold? $$
\lim_{N \to \infty} \frac{1}{N}\sum_{r = 2}^N f(r) = 0
$$ Question 2 : Is $f(n)$ injective over composites? i.e., do there exist composites $3 < m < n$ such that $f(m) = f(n)$ ? My progress $f(4.35\times 10^8) \approx 0.5919$ and decreasing so the limit if it exists must be between 0 and 0.5919. For $2 \le i \le n$ , the minimum value of $f(i)$ occurs at the largest highly composite number $\le n$ . Note 2 : Here standard deviation of $x_1, x_2, \ldots , x_n$ is defined as $\sqrt \frac{\sum_{i=1}^{n} (x-x_i)^2}{n}$ . Also notice that even if we define standard deviation as $\sqrt \frac{\sum_{i=1}^{n} (x-x_i)^2}{n-1}$ our questions remain unaffected because in this case in the definition of $f$ , we will be multiplying with $\sqrt 2$ instead of $2$ to normalize $f$ in the interval $(0,1)$ . Note 3 : Posted this question in MO and got answer for question 1. Indeed the limit tends to zero. Question 2 is still open.","['statistics', 'number-theory', 'natural-numbers', 'limits', 'prime-numbers']"
3176335,Functional equation in single-variable calculus,"Suppose we know that $f(x) \in C^2$ and $f(x)$ defined for all real numbers. Furthermore, $f(x)$ has following property: $$ \forall x,y \in \Bbb R \quad f(x+y) - f(x) =  yf'(x + \frac{y}{2})$$ How to proof that $f(x) = ax^2+bx +c \quad (a,b,c - const)$ ? Note: according to the textbook from which I get this task (B.P. Demidovich's Problems in mathematical analysis ), it can be proved without using multivariable calculus and integration.","['functional-equations', 'calculus', 'derivatives']"
3176395,Searching for tagged balls under constraints,"(This is a modification of this problem .) You have $k$ white balls and $k$ black balls, and know that exactly one of each color is radioactive.  You can place any number of balls of your choosing in a radiation detector that reveals only whether $0$ , $1$ or $2$ measured balls are radioactive. Specify an algorithm that identifies the two radioactive balls in as few such measurements as possible in the worst case. Some thoughts: The total uncertainty in the initial system, in bits, is $\log_2 k + \log_2 k = 2 \log_2 k = \log_2 k^2$ . (Sparked by a comment by @leonbloy.) The selection is a Bernoulli process and the most information one can obtain is $3/2$ bits.  Thus in principle the total number of needed measurements is bounded by: $$\left\lceil \frac{2\log_2 k}{3/2}\right \rceil.$$ Here is a decision tree for the small case $k=2$ .  We label the balls $b_1, b_2, w_1, w_2$ and first measure $b_1$ with $w_1$ .  (I'm pretty sure that testing one black and one white provides the maximum information.)  The arcs denote the output of the measurement.  In the case of $0$ or $2$ detected radioactive balls, we are done and the red boxed leafs are our answers. If the result is $1$ , we then measure $b_1 w_2$ which can only give a $0$ or $2$ output, and our final conclusions are shown at the bottom leafs. The expected number of measurements is ${\cal E}[m] = \frac{1}{2} \cdot 1 + \frac{1}{2} \cdot 2 = 1.5$ . Of course there are other ways to guarantee an answer in two measurements, for instance one measurement on a single black, then one measurement on a single white.  But that algorithm guarantees you will need two measurements, i.e, ${\cal E}[m] = 2$ .  (Admittedly, both algorithms have the same worst case performance.) The decision tree above gives you an answer half of the time with just a single measurement--it exploits more from the three-way answer than would a two-way answer. (I have a feeling this problem has been solved in the literature, but I couldn't find it on SE nor with a cursory search online.)","['coding-theory', 'probability', 'information-theory']"
3176401,Ultralimit of metric spaces is complete,"Consider the following short proof that all ultralimits are complete (Thy typo $d_omega$ is of course to be read as $d_\omega$ .) There are two things that I dont understand about this proof. 1) Why is the sequence $(y_n)$ bounded? Or more precisely why is the sequence of distances $(d_n(y_n,p_n))$ to the basepoints $p_n\in X_n$ bounded? The argument here looks incomplete to me, because first of all the sequence $(y_n)$ is left undefined on the set $\bigcap N_j$ . But this set may be infinite, so the assertion seems to be wrong if we just define it arbitrarily on these points. So I think one should either say that we can pick the sequence $N_j$ such that $\bigcap N_j=\emptyset$ (because $\omega$ is non-princilpe(!)) or that we just assign $y_n=p_n$ for $n\in\bigcap N_j$ . However with these fixes it is still not clear to me why the sequence is bounded. 2) Why is $d_\omega(x^I,y)\le2\varepsilon$ ? [The proof is from  John Roe. Lectures on Coarse Geometry]","['proof-explanation', 'general-topology', 'complete-spaces', 'metric-spaces']"
3176402,Bound on gradient of Harmonic functions,"Let $G\subseteq\mathbb{C}$ be a domain and assume $u:G\to\mathbb{R}$ is a harmonic function such that $|u(z)|\leq M$ for all $z\in G$ . Show that $|\nabla u(z)|\leq\frac{2M}{r}$ for $0<r<dist(z,\partial G$ ) and for all $z\in G$ . ( $\nabla u$ is the gradient of $u$ ). Attempt: As we know, there is some harmonic function $v:G\to\mathbb{R}$ such that $f=u+iv$ is holomorphic. I thought it might help to use Cauchy integral formula. For a given $z_0\in G$ and $0<r<dist(z_0,\partial G$ ) we have: $f'(z_0)=\frac{1}{2\pi i}\int_{B_r(z_0)}\frac{f(\xi)}{(\xi-z_0)^2} d\xi=\frac{1}{2\pi}\int_0^{2\pi}\frac{f(z_0+re^{it})}{re^{it}}dt=\frac{1}{2\pi r}\int_0^{2\pi}f(z_0+re^{it})(cost-isint)dt$ Alright, now using the fact that $f'(z_0)=u_x(z_0)+iv_x(z_0)$ I tried to compare real parts. What I got is the following: $u_x(z_0)=\frac{1}{2\pi r}\int_ 0^{2\pi} u(z_0+re^{it})cost+v(z_0+re^{it})sint dt$ I hoped I can get a bound on both $u_x$ and $u_y$ and from there get the required bound on the gradient. However as you can see I got that $u_x$ is dependent on the values of $v$ and this is a function I cannot bound since I don't know anything about it. So I'm stuck. Any ideas?","['complex-analysis', 'harmonic-functions']"
3176449,"Complex: If $|f|<\varepsilon$, then $\frac{1}{\pi}\int_E \frac{|f|}{|z-w|}<\varepsilon$?","Is this true? Let $f:E\xrightarrow{}\mathbb{C}$ be holomorphic on the interior of a compact set $E$ , and let $\varepsilon>0.$ If $|f|<\varepsilon$ on $E$ , then $$\frac{1}{\pi}\int_E \frac{|f(w)|}{|z-w|} dm(w)<\varepsilon$$ for $z\in E$ , where $m$ is the two-dimensional Lebesgue measure. Important edit: $E$ is COMPACT.","['complex-analysis', 'complex-integration']"
3176482,Probability distribution of discrete variables made of continuous variables,"In my Econometrics class yesterday, our teacher discussed a sample dataset that measured the amount of money spent per patient on doctor's visits in a year. This excluded hospital visits and the cost of drugs. The context was a discussion of generalized linear models, and for the purposes of Stata, he found that a gamma distribution worked best. Good enough. But I started thinking about the dataset itself, and the pdf curve ""total dollars per patient"" might follow. I'm thinking of it as two variables: 1) The number of visits per person (k). Let's use a Poisson distribution with λ=1, so p(0)=0.368, p(1)=0.368, p(2)=0.184, etc. 2) The cost of each visit (x). Let's use a normal curve with a mean of 50 dollars and a SD of 5 dollars. A person's total expenditure would be the sum of normally distributed variables, or x = N(k50, k25) for k=0 to infinity visits. So a person with no visits would spend nothing, ~.95 of people with one visit would spend between 40 and 60, ~.95 of those with two visits would spend between ~85.86 and ~114.14, three visits would spend ~132.68 and ~167.32, etc. I think a pdf curve would be some combination of these two functions. My guess is that it would look like a vertical line a 0 and a series of decreasing, ever flattening humps at 50, 100, 150 dollars, etc, where the integral of x=(0, inf.) equals 0.632 (because zero visits is p=.0368) Can anyone help me put the pieces together? What would that function be? I've taken through Calc III (multivariate/vector calculus), FYI. Thanks.","['statistics', 'probability-distributions', 'probability']"
3176490,How to find $\lim_{x \to 0^+} x^{\sqrt{x}}$ using L'Hospital's Rule.,"I am trying to find $\lim_{x \to 0^+} x^{\sqrt{x}}$ using L'Hospital's Rule. I've differentiate the function, but it doesn't seem like that has helped at all. $$\lim_{x \to 0^+} x^{\sqrt{x}} = \lim_{x \to 0^+} x^{\sqrt{x}} (\frac{\ln(x)}{2\sqrt{x}} + \frac{1}{\sqrt{x}})$$ $$\frac{d}{dx} y = \frac{d}{dx} \ln(x^{\sqrt{x}}) \to \frac{1}{y} \cdot y'= \sqrt{x}\ln(x) \to y' = y(\frac{\ln(x)}{2\sqrt{x}} + \frac{1}{\sqrt{x}}) \to y' = x^{\sqrt{x}} (\frac{\ln(x)}{2\sqrt{x}} + \frac{1}{\sqrt{x}})$$ How can I figure out what this limit represents?","['limits', 'calculus']"
3176528,Why does this argument for conditional expectation fail?,"I have a question and I know it is wrong. However I do not understand where I am messing up. If somebody could explain where I am going wrong, that would be great. If we have a probability space $(X,\mathcal{F},\mu)$ , and a sub-sigma-algebra $\mathcal{A}\subset \mathcal{F}$ , the conditional expectation for $f\in L^1(X,\mathcal{F},\mu)$ is defined to be the (a.s) unique measurable $\mathbb{E}[f\mid \mathcal{A}]\in L^1(X,\mathcal{A},\mu)$ such that $\int_A f\,d\mu=\int_A \mathbb{E}[f\mid \mathcal{A}]\,d\mu$ for all $A\in \mathcal{A}$ . My question is does $\int_X f\,d\mu$ not satisfy this? Of course, it would be nonsensical to say that the conditional expectation wrt some arbitrary sub-sigma-algebra is equal to the expectation. However, if we plug $\int_Xf\,d\mu$ into the above for $\mathbb{E}[f\mid \mathcal{A}]$ , we have $\int_A(\int_Xf\,d\mu)\,d\mu=\int_X(\int_A fd\mu)d\mu=\int_Afd\mu$ where the first equality uses Fubini. I think the use of Fubini is suspicious. Any further explanations are welcome! (INFO: The reason I came across this was as follows. I was looking at a proof of Birkhoff's Ergodic theorem, and in it it claimed that the conditional expectation with respect to the sigma-algebra consisting of sets of measure $0$ or $1$ was equal to the expectation. I believe this uses Fubini?) EDIT: Sorry if I did not explain it correctly. I basically am saying that the constant function $\int_X fd\mu$ is in $L^1(X,\mathcal{A},\mu)$ and satisfies $\int_A fd\mu=\int_A (\int_X fd\mu)d\mu$ , and so does this not mean $\int_X fd\mu= \mathbb{E}[f\mid \mathcal{A}]$ ?","['measure-theory', 'ergodic-theory', 'conditional-expectation', 'expected-value', 'probability-theory']"
3176543,Intersection point of 2 lines defined by 2 points each,"I'm implementing this in code, but I'll rewrite it so that it is easier understood (like pseudocode): # a = pt 1 on line 1
# b = pt 2 on line 1
# c = pt 1 on line 2
# d = pt 2 on line 2
def intersect(a,b,c,d):

    # stuff for line 1
    a1 = b.y-a.y
    b1 = a.x-b.x
    c1 = a1*a.x + b1*a.y

    # stuff for line 2
    a2 = d.y-c.y
    b2 = c.x-d.x
    c2 = a2*c.x + b2*c.y

    determinant = a1*b2 - a2*b1

    if (determinant == 0):
        # Return (infinity, infinity) if they never intersect
        # By ""never intersect"", I mean that the lines are parallel to each other
        return math.inf, math,inf
    else:
        x = (b2*c1 - b1*c2)/determinant
        y = (a1*c2 - a2*c1)/determinant
        return x,y All the above works, ... but only does by assuming that the lines extend infinitely in each direction, like a linear equation. I'll show what I mean here . There are the 2 lines, red and green, and the gold dot is what is returned when I test this code ... but the lines don't actually intersect. What can be used to test whether the lines truly intersect? Heres the actual Python code if needed.","['matrices', 'python', 'linear-algebra']"
3176559,Find the general form of the solutions of the recurrence relations $a_n=8a_{n-2}-16a_{n-4}$,Was hoping to confirm my steps and answer. The characteristic equation of $a_n=8a_{n-2}-16a_{n-4}$ is $$r^4 = 8r^2 - 16$$ $$r^4 - 8r^2 + 16= r^4 - 4r^2 -4r^2 + 16= (r^2-4)(r^2-4) = (r-2)^2(r+2)^2$$ There are roots $2$ with multiplicity $2$ and $-2$ with multiplicity $2$ therefore solutions of this recurrence relation are of the form: $$a_n=\alpha_1 2^n+\alpha_2n2^n+\alpha_3(-2)^n+\alpha_4n(-2)^n$$,"['linear-algebra', 'recurrence-relations', 'combinatorics']"
3176568,Distributional derivative of Weierstrass function,"How can we compute the distributional derivative of the Weierstrass function $$W(x) =\sum_{k=1}^\infty \lambda^{(s-2)k}\sin(\lambda^k x)$$ where $s \in (0,2)$ and $\lambda$ are fixed parameters? We know that the Weierstrass function is nowhere differentiable. This implies that it does not have a weak derivative. However, since $W \in L^1_{loc}$ , we can consider the associated distribution $T_W$ and compute its distributional derivative. I'm having troubles doing that computation because of the series representation of $W$ .","['measure-theory', 'distribution-theory', 'calculus', 'sobolev-spaces', 'functional-analysis']"
3176593,Consider the non-homogeneous linear recurrence relations $a_n=2a_{n-1}+2^n$ find all solutions.,"I'm having a hard time understanding how to find all solutions of the form $a_n = a^{(h)}_n+a_n^{(p)}$ I show that $a_n=n2^n \to a_n=2(n-1)2^{n-1} +2^n=2^n(n-1+1)=n2^n$ . I can show that $a_n^{(h)}$ characteristic equation $r-2=0 \to a_n^{(h)}=\alpha2^n$ But I'm stuck on $a_n^{(p)}$ characteristic equation $C2^n=2C\cdot2^{n-1}+2^n$ Simplifies to $C \neq C+1$ , Looking online I saw that the solution is $a_n=c\cdot2^n+n2^n$ , but I'm not sure how to get there.","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
3176599,How does convergence of sequences define a topology? Question about the definition of the topology of the Gromov boundary.,"I am not sure how the second bullet defines a topology on $\partial X$ ?
What exactly is the topology? What are the open sets, and/or, what are the closed sets? How does convergence of sequences define a topology?","['metric-spaces', 'group-theory', 'abstract-algebra', 'geometric-group-theory', 'general-topology']"
3176617,Is $x^2 + y^2= 36$ a function or not?,"$x^2 + y^2= 36,\   y\in\mathbb [0,6] $ is a function or not ? My attempt: $x=\pm\sqrt{36-y^2}$ Since for every value of $y$ , $x$ has two possible values, so the given relation is not a function . But my book says that it is a function. Why?",['functions']
3176629,Nine people sat around a table..,"In the evening, pizza was ordered nine people sat around a round table, 50 slices of pizza were served to these nine people. Prove that there were two people sitting next to each other who ate at least 12 pizza slices. I used the pigeon hole principle to determine 50/9 = 5.5 => 6 Therefore, at least one person ate 6 slice of pizza. I just don't know how to prove that two people ate at least 12 slices.. Help would be greatly appreciated!","['pigeonhole-principle', 'discrete-mathematics']"
3176645,What is the Largest Dimension of the Set of Matrices with $\text{trace} AB = 0$,"Suppose $W$ is a subspace of $M_n(\mathbb{R})$ with property that $\text{trace}(AB) = 0$ for all $A,B \in W$ . I want to find the largest possible dimension of $W$ . It seems like the answer is $n(n - 1)/2$ , I can find such set of matrices namely the set of all triangular matrices, either upper or lower bot not both, with zero diagonal. They are of the form $$\begin{bmatrix}
0 & * & * & * & \dots & * \\ 
0 & 0 & * & * & \dots & *\\
0 & 0 & 0 & * & \dots & *&\\
\vdots & \vdots & \vdots & \vdots & \ddots & *\\
0 & 0 & 0 & 0 &\dots & 0\end{bmatrix}$$ (or the lower one). I have problem showing this is the largest dimension it can get. Clearly one of the basis of this subspace is the set of matrices with entry 1 in one of those $*$ and zero otherwise, there are $n(n - 1)/2$ such matrices. I can show that when adding one more matrix to this basis the property $\text{trace}(AB)$ can no longer hold. Is this the correct way to prove it? Because I think there's an error with this since I start with the uppertriangular then add one more(not general).","['matrices', 'trace', 'linear-algebra']"
3176651,"If $(\pi,V)$ is irreducible, then $\pi(G)$ spans $\operatorname{End}(V)$","Let $(\pi,V)$ be a finite dimensional complex representation of a finite group $G$ .  If $\pi$ is irreducible, then the linear span of $\pi(g) : g \in G$ is equal to $\operatorname{End}_{\mathbb C}(V)$ .  One way to see this is by considering the linear map of the group algebra $\mathbb C[G]$ into the direct sum of the $\operatorname{End}_{\mathbb C}(V)$ for $V$ irreducible.  It follows from the Peter-Weyl theorem that this map is an isomorphism.  But this takes a bit of work. Is there a simple proof that $\mathbb C[G] \rightarrow \operatorname{End}_{\mathbb C}(V)$ is surjective when $(\pi,V)$ is irreducible?","['abstract-algebra', 'representation-theory', 'lie-groups']"
3176672,Matrix expression for $\operatorname{vec}(X^{\top}X)$?,"Let $X$ be an $m$ by $n$ matrix. I would like to find the $n^2$ by $(mn)^2$ matrix $B$ such that $$
\operatorname{vec}\left(X^{\top}X\right) = B \operatorname{vec}\left(\operatorname{vec}\left(X\right)\left(\operatorname{vec}\left(X\right)\right)^{\top}\right).$$ Here $\operatorname{vec}(\cdot)$ is the vector function that unrolls a matrix into a vector columnwise.
I would guess $B$ involves the Kronecker products, and some $I_m$ and $I_n$ matrices and some 1 vectors, but I am having a hard time with all the indices.","['matrices', 'kronecker-product']"
3176675,$\frac{\partial }{\partial z}$ and $\frac{\partial }{\partial \bar z}$: Wirtinger derivative?,"I am studying the following notations: $$
\frac{\partial}{\partial z} = \frac{1}{2}\left(\frac{\partial}{\partial x} - i\frac{\partial}{\partial y}\right),\qquad
\frac{\partial}{\partial\bar{z}} = \frac{1}{2}\left(\frac{\partial}{\partial x} + i\frac{\partial}{\partial y}\right).
\tag{1}
$$ It might be obtained if we would use the chain rule (I don't know if it is right). Note that, for $z=x+iy,$ $x=\frac{z+ \bar z}{2}~\hbox{and}~y=\frac{z-\bar z}{2i}=-\frac{i}{2}(z-\bar z).$ Then $$
\frac{\partial x}{\partial z}=\frac{1}{2},~\frac{\partial x}{\partial \bar z}=\frac{1}{2},~\frac{\partial y}{\partial z}=-\frac{i}{2},~\frac{\partial y}{\partial \bar z}=\frac{i}{2}.
$$ Let $f(x,y)=u(x,y)+iv(x,y)$ .  Then $\hat f(z,\bar z)=u(x,y)+iv(x,y),$ and \begin{eqnarray*}
\frac{\partial \hat f}{\partial z}=\frac{\partial f}{\partial z}
&=&\frac{\partial u}{\partial x}\frac{\partial x}{\partial z}+\frac{\partial u}{\partial y}\frac{\partial y}{\partial z}+i\left[\frac{\partial v}{\partial x}\frac{\partial x}{\partial z}+\frac{\partial v}{\partial y}\frac{\partial y}{\partial z}\right] \\
&=& \frac{1}{2}[u_x -i u_y +i(v_x-iv_y)] \\ 
&=& \frac{1}{2}\left[ u_x+iv_x-i(u_y+iv_y)\right] =\frac{1}{2} (f_x-i f_y),
\end{eqnarray*} where $f_x=u_x+iv_x$ and $f_y=u_y+iv_y.$ Similarly, \begin{eqnarray*}
\frac{\partial f}{\partial \bar z}
&=&\frac{1}{2}(f_x+i f_y) =\frac{1}{2}\left[ u_x-v_y +i(u_y+v_x)\right].
\end{eqnarray*} First Question. Are the above assertions true or just give the intuition for $(1)$ ? Using $(1)$ , it is easy to check that $$
\frac{\partial z}{\partial z} = \frac{\partial \bar{z}}{\partial \bar{z}} = 1,\qquad
\frac{\partial \bar{z}}{\partial z} = \frac{\partial z}{\partial \bar{z}} = 0.
$$ Second question. I don't know if the following assertions are true: $$
\frac{\partial}{\partial z} f(x,y) = D_{1}\hat f(z, \bar{z}),\qquad
\frac{\partial}{\partial \bar{z}} f(x,y) = D_{2}\hat f(z, \bar{z}),
$$ If they are true, how to prove them? Is it easy or do I know something more complicated? It seems that this question is related to the first one. May I use the chain rule used above? If so, I can understand all of it. I would be grateful if you give any comments for my questions. Thanks in advance.",['complex-analysis']
3176683,Do injective outer measure-preserving functions preserve measurable sets?,"Suppose $f:\mathbb{R}^n\to\mathbb{R}^n$ is an injection, such that for any $E\subseteq\mathbb{R}^n$ we have $m^*(E)=m^*(f(E))$ . Is it true that for any $E\subseteq\mathbb{R}^n$ measurable, $f(E)$ is always measurable?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3176696,Finding a closed form for coefficients in $x^{3n}=x_0\left(a_nx+b_n+\frac {c_n}{x}\right)$,"Consider, $$
x^3=x+1
$$ Let $x_0$ be a solution to the above equation. Now consider $x^{3n}$ . For $n=2$ we have: $$
x^6=(x+1)^2
$$ $$
=x^2+2x+1
$$ $$
=x\left(x+2+\frac {1}{x}\right)
$$ $$
=x_0\left(x+2+\frac {1}{x}\right)
$$ For $n=3$ we have: $$
x^9=(x+1)^3
$$ $$
=x^3+3x^2+3x+1
$$ $$
=3x^2+4x+2
$$ $$
=x\left(3x+4+\frac {2}{x}\right)
$$ $$
=x_0\left(3x+4+\frac {2}{x}\right)
$$ Similarly for $n=4$ we have: $$
x^{12}=x_0\left(7x+9+\frac {5}{x}\right)
$$ In general we have: $$
x^{3n}=x_0\left(a_nx+b_n+\frac {c_n}{x}\right),
$$ Where, $$
a_{n+1}=a_n+b_n, a_2=1,
$$ $$
b_{n+1}=a_n+b_n+c_n, b_2=2,
$$ $$
c_{n+1}=a_n+c_n, c_2=1.
$$ My question is: is there a closed form for $a_n,b_n$ and $c_n$ ?
Any help would be appreciated.","['algebra-precalculus', 'closed-form', 'recursion']"
3176741,"Finding ""minimal"" union of sets containing all elements [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I have a list of elements, with each element belonging to a number of sets . For example: Jack -  Developer, Footballer, Video gamer John -  Carpenter, historian, footballer Jim  -  Professor, politician, Man Jill -  Professor, Footballer I want to find the ""minimal"" union of sets (Developer, Footballer, etc.) that will contain all elements (Jack, John, Jim, Jill). A trivial solution is union of all sets. That is correct, but is not minimal (in terms of number of sets in the union) in the general case. Please note that I am interested in the general case, not just in the example provided.",['elementary-set-theory']
3176754,"Example 4, Sec. 29, in Munkres' TOPOLOGY, 2nd ed: How is the one-point compactification of the real line homeomorphic with the circle?","Here is Theorem 29.1 in the book Topology by James R. Munkres, 2nd edition: Let $X$ be a [topological] space. Then $X$ is locally compact Hausdorff if and only if there exists a [topological] space $Y$ satisfying the following conditions: (1) $X$ is a subspace of $Y$ . (2) The set $Y - X$ consists of a single point. (3) $Y$ is a compact Hausdorff space. If $Y$ and $Y^\prime$ are two [topological] spaces satisfying these conditions, then there is a homeomorphism of $Y$ with $Y^\prime$ that equals the identity map on $X$ . Following the proof of this theorem, Munkres gives this definition: If $Y$ is a compact Hausdorff space and $X$ is a proper subspace of $Y$ whose closure equals $Y$ , then $Y$ is said to be a compactification of $X$ . If $Y - X$ equals a single point, then $Y$ is called the one-point compactification of $X$ . Thus from Theorem 29.1 we can conclude the following: If $X$ is a topological space that is locally compact and Hausdorff but not compact, then $X$ has a one-point-compactification, and conversely. Now here is Example 4, Sec. 29, in Munkres' Topology : The one-point compactification of the real line $\mathbb{R}$ is homeomorphic with the circle, . . . [How to prove this?] Similarly, the one-point compactification of $\mathbb{R}^2$ is homeomorphic to the sphere $S^2$ . [How to prove this?] The real line $\mathbb{R}$ is the set of real numbers with the standard (or usual) topology having as a basis all the open intervals of the form $(a, b)$ , where $a, b \in \mathbb{R}$ and $a < b$ . How to proceed from these facts and show explicitly and rigorously that the one-point compactification of $\mathbb{R}$ is homeomorphic with the (unit) circle $S^1$ and that the one-point compactification of the plane $\mathbb{R}^2$ is homeomorphic with the (unit) sphere $S^2$ ? PS: The map $f \colon \mathbb{R} \longrightarrow (-1, 1)$ , $$ r \mapsto \frac{ r }{ \sqrt{1 + r^2} } $$ is a homeomorphism. The derivative $f^\prime$ of $f$ is given by $$ f^\prime(r) = \frac{1}{ (1+ r^2) \sqrt{ 1+r^2 } } > 0$$ for all $r \in \mathbb{R}$ so that $f$ is strictly increasing and hence also injective. Of course, $f$ is continuous. Moreover, $$ \lim_{r \to +\infty} f(r) = +1, \ \mbox{ and } \ \lim_{r \to -\infty} f(r) = -1. $$ Thus we indeed have $$ 
f \big(\mathbb{R}\big) = (-1, 1). 
$$ The inverse $f^{-1} \colon (-1, 1) \longrightarrow \mathbb{R}$ is given by $$ f^{-1}(s) = \frac{s}{\sqrt{1-s^2} }, $$ which is also continuous. Thus $f$ is a homeomorphism of $\mathbb{R}$ with $(-1, 1)$ . And, let $g \colon (-1, 1) \longrightarrow  S^1 \setminus \{ (-1, 0 ) \}$ be the mapping $$ t \mapsto \left( \cos \pi t \ , \  \sin \pi t  \right). $$ Then $g$ is a homeomorphism of $(-1, 1)$ with $S^1 \setminus \{ (-1, 0) \}$ , [Am I right?] which in turn is dense in $S^1$ . Thus the map $g \circ f \colon \mathbb{R} \longrightarrow S^1\setminus \{ (-1, 0) \}$ is a homeomorphism of $\mathbb{R}$ with $S^1 \setminus \{ (-1, 0) \}$ , which is dense in the compact Hausdorff space $S^1$ . Therefore the one-point compactification of $\mathbb{R}$ is $S^1$ . Is my reasoning correct?","['general-topology', 'compactification', 'compactness']"
3176785,atomic formula and $\Pi_1^0$ is still $\Pi_1^0?$,"Currently I am reading Simpson's Subsystem of Second Order Arithmetic , Chapter II.3, Primitive Recursion. Notations: $(i,j) = (i+j)^2+i$ Theorem II. $3.2$ The following is provable in RCA $_0.$ If $f:X\to Y$ and $g:Y\to Z$ then there exists $h=gf:X\to Z$ defined by $h(i) = g(f(i)).$ In the proof, the author introduced the following formula $$\exists j((i,j)\in f \wedge (j,k)\in g)\leftrightarrow (i\in X\wedge \forall j((i,j)\in f\rightarrow (j,k)\in g)).$$ Then he quoted by $\Delta_1^0$ comprehension that $h$ exists such that $$(i,k)\in h \leftrightarrow \exists j((i,j)\in f \wedge (j,k)\in g).$$ I fail to understand why is the formula $\Delta_1^0.$ I have a vague feeling that left hand side is $\Sigma_1^0$ while the other side is $\Pi_1^0$ .
However, on the right side, the quantifier is inside the formula, not at outside. 
Would this affect the formula?","['reverse-math', 'functions', 'logic']"
3176791,Intuition behind Covering Axioms,"Many concepts in General Topology are the direct abstraction of very profound and natural concepts (think of structures as topology or uniformity themselves, separation axioms, quotient and identification topologies, connection, compactness...) I am having hard times trying to understand the fundamental idea behind Covering Axioms, namely concepts as paracompactness and the various types of refinements (point and nbd finite, starring, barycentric). I understand the definitions and I know their importance with respect to metrizability and other procedures such as partitions of unity, but can't figure them out, and they seem to me rather tachnical. I have tried to see them as a generalization of compactness but this does not seem very fruitful. So I ask: -Do Covering Axioms have a direct interpretation, or are they just a technical 
 means to other constructions? -What is the extent of their relevance? -Is there a direct ""categorical"" link with ordinals, and order concepts, which 
 appear very frequently in proofs?","['ordinals', 'paracompactness', 'intuition', 'general-topology', 'soft-question']"
3176802,Homotopy equivalence of covering spaces [duplicate],"This question already has an answer here : Homotopy equivalence of universal cover (1 answer) Closed 5 years ago . I was trying to solve the following exercise in Hatcher (1.3.8). Let $p:(\tilde{X},\tilde{x})\to(X,x)$ and $q:(\tilde{Y},\tilde{y})\to(Y,y)$ simply-connected covering spaces. Assume $X,Y$ path-connected and locally path-connected spaces such that $X\simeq Y$ . Then $\tilde{X}\simeq \tilde{Y}$ . My thoughts: Let $f:X\to Y$ be a homotopy equivalence, $x_0\in X$ . Define $y_0:=f(x_0)$ . We get the following diagram. There exists a unique lift of $f$ if $f_*(\pi_1(X))\leq p_*(\pi_1(Y))=\{*\}$ called $F:(X,x_0)\to (\tilde{Y},\tilde{y_0})$ with $f=q\circ F$ . Define $\tilde{f}:\tilde{X}\to\tilde{Y}$ by $\tilde{f}=F\circ p$ . I want to show that $\tilde{f}$ is a homotopoy equivalence. I don't see why $f_*(\pi_1(X))=\{*\}$ . Any thoughts?","['general-topology', 'algebraic-topology', 'covering-spaces']"
3176830,"Classifying the stationary points of $f(x, y) = 4xy-x^4-y^4 $","$f(x, y) = 4xy-x^4-y^4 $ The gradient of this function is $0$ in $(-1, -1), (0, 0),(1, 1)$ I tried to compute the determinant of the Hessian Matrix, but it's 0 for every point, I always get a null eigenvalue for each point, so no matter what I can't find out what kind of points these are. For $(0, 0)$ , since $f(0, 0) = 0$ , $f(x, 0) = -x^4$ and $f(0, y) = -y^4$ . Since both will always be negative, isn't that supposed to be a maximum or a minimum point? According to Wolfram it's a saddle point and I can't understand why.","['maxima-minima', 'multivariable-calculus', 'hessian-matrix']"
3176886,Set containing a set containing itself,"I have to prove that for sets $x_0,x_1,\dots,x_n$ , it is impossible that $x_0\in x_1\in\dots\in x_n\in x_0$ . I tried to prove this statement using induction. For $n=0$ , this is clear to me, it follows immediately from the axiom of regularity. However, I even failed to prove this for $n=1$ so far. I thought I should again somehow use the axiom of regularity, as it appears to me that this is the only axiom with which one can disprove the existence of a set. Nevertheless, I am stuck, and I would appreciate any help in clearing things up.",['elementary-set-theory']
3176917,The limit distribution of Wilcoxon signed rank statistic?,"An alternative representation of the Wilcoxon signed rank statistic $V$ is $V=\sum_{i\le j}\mathbb{I}_{\{X_i+X_j>0\}}=\sum_i\mathbb{I}_{\{X_i>0\}}+\sum_{i<j}\mathbb{I}_{\{X_i+X_j>0\}}$ where $X_1,\cdots, X_n$ are i.i.d. . How to find the limiting distribution of $V$ without requiring that $X_i$ has
a symmetric distribution? I know it can be solved by projection method. But I don't know exactly how it works. Who can help me write down the details of the proof? Thanks a lot.","['weak-convergence', 'statistics', 'asymptotics']"
3176968,Does the plane $x_3 = 1$ contain any periodic solutions?,"I am studying the following system of equations: \begin{align}
\dot{x_1} &= x_1 - x_1x_2 - x_2^3 + x_3(x_1^2 + x_2^2 - 1 - x_1 + x_1x_2 + x_2^3)\\
\dot{x_2} &= x_1 - x_3(x_1 - x_2 + 2x_1x_2)\\
\dot{x_3} &= (x_3 -1)(x_3 + 2x_3x_2^2 + x_3^3)
\end{align} The set/plane $x_3 = 1$ is an invariant set. I am curious whether this invariant set contains any periodic solutions, and how I can find out. I have the following definitions to work with: Definition (periodic solution): Suppose that $x = \phi(t)$ is a solution of the equation $\dot{x} = f(x)$ , $x\in D\subset\mathbb{R}^n$ and suppose there exists a positive number $T$ such that $\phi(t + T) = \phi(t)$ for all $t\in\mathbb{R}$ . Then $\phi(t)$ is called a periodic solution of the equation with period $T$ . Definition (invariant set) : Consider the equation $\dot{x} = f(x)$ , $x\in D\subset \mathbb{R}^n$ . The set $M\subset D$ is invariant if the solution $x(t)$ with $x(0)\in M$ is contained in $M$ for $-\infty <t < \infty$ . If this property is valid only for $t\geq 0 (t\leq 0)$ then $M$ is called a positive (negative) invariant set. Question: How should I find out whether the plane $x_3 =1$ contains any periodic solutions? I understand that $x_3 = 1$ is an invariant set since $\dot{x_3} = 0$ in $x_3 = 1$ , but I don't really know how I should use this fact to find out whether there are any periodic solutions in $x_3 = 1$ . Thanks!","['systems-of-equations', 'ordinary-differential-equations']"
3177097,Simplify the sum $ \sum_{i=0}^{k}(-1)^i i \binom{n}{i} \binom{n}{k-i}$,How to deal with combinatoric interpretation (or just solving it in algebraic way) when we have $(-1)^i$ factor in our sum? Example task: Simplify the sum: $$ \sum_{i=0}^{k}(-1)^i i \binom{n}{i} \binom{n}{k-i} \text{ for } 0\le k \le n $$ For task without $(-1)^i$ $$ \sum_{i=0}^{k} i \binom{n}{i} \binom{n}{k-i} = n \binom{2 n-1}{k-1} $$ I can write that interpretation: I have $n$ rabbits and $k$ slots Each rabbit can be in both slot of first type and second type slots of first type + second type = $k$ Lets double rabbits I choose one rabbit as an king and it will be also a rabbit to slot of first type so I need to choose $2n-1$ rabbit for $k-1$ slots But I don't know how to deal with $(-1)^i$,"['binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
3177140,How do you prove this using mathematical induction,"I am unsure if my proof method is correct for this problem here: Use mathematical induction to prove that for each integer $n ≥ 3, 4^n ≤ 5^n − 60.$ This is my working out so far: Base case $n = 3,$ $4^3 ≤ 5^3 - 60$ . P(3) is true Inductive hypothesis: Assume P(k) is true.
Show $P(k+1)$ is true. $4^{k+1} ≤ 5^{k+1} - 60$ . LHS = $4^{k+1}$ = $4(4^k)$ $<4(5^k)-60$ $<5(5^k)-60$ = $5^{k+1}-60$ Therefore, by the principle of mathematical induction?, for every integer $n ≥ 3, 4^n ≤ 5^n − 60.$ Not quite sure if this is the right way to prove it.","['induction', 'proof-verification', 'discrete-mathematics']"
