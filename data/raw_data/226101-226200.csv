question_id,title,body,tags
4666696,"Is ‚àû a limit point of $\mathbb R$ ? If not, how to understand Rudin's definition at the beginning of chapter $4$ (PMA)?","I am starting reading the $4^{th}$ chapter of PMA from Walter Rudin. The chapter is about continuity and it defines $$\lim_{x\to a} f(x)$$ (for a function mapping a metric space $E$ into a metric space $F$ ) saying $a$ is a limit point of $E$ . I don't understand: for a function whose domain is $\mathbb R$ , does it mean that for $\lim_{x\to \infty} f(x)$ we have ‚àû being a limit point of $\mathbb R$ ? According to this post , $\mathbb R '=\mathbb R$ , and ‚àû is not a limit point of $\mathbb R$ . PS: Here is the (beggining of the) definition:","['general-topology', 'functions']"
4666735,Using an integral to generate rational approximations of $\pi$,"Let $$f(r)=\int_0^1\frac{x^r(1-x)^r}{1+x^2}dx.$$ One surprising fact is that $f(4)=\frac{22}{7}-\pi.$ This got me thinking. Surely, it can't be a coincidence that a fairly accurate rational approximation for $\pi$ shows up in this way, right? Well, this is what I've found so far: $$\boxed{f(0)=\frac{\pi}{4}√¢‚Ä∞ÀÜ0.785}$$ $$\boxed{f(4)=\frac{22}{7}-\pi√¢‚Ä∞ÀÜ0.001}$$ $$\boxed{f(8)=4\pi-\frac{188684}{15015}√¢‚Ä∞ÀÜ3.64√É‚Äî10^{-6}}$$ $$\boxed{f(12)=\frac{431302721}{8580495}-16\pi√¢‚Ä∞ÀÜ1.18√É‚Äî10^{-8}}$$ $$\boxed{f(16)=64\pi-\frac{5930158704872}{29494189725}√¢‚Ä∞ÀÜ4.00√É‚Äî10^{-11}}$$ Let $k\in\{0,1,2,\ldots\}.$ It seems to me that $f(4k)=4^{k-1}((-1)^k\pi+(-1)^{k+1}R_k),$ where $R_k$ is some rational approximation of $\pi.$ We have: $$\boxed{R_0=0}$$ $$\boxed{R_1=\frac{22}{7}}$$ $$\boxed{R_2=\frac{188684}{15015√É‚Äî4}}$$ $$\boxed{R_3=\frac{431302721}{8580495√É‚Äî16}}$$ $$\boxed{R_4=\frac{5930158704872}{29494189725√É‚Äî64}}$$ I have three conjectures at this point: $$\textbf{C1:}\lim_{r\to\infty}f(r)=0.$$ $$\textbf{C2:}\forall k\in\{0,1,2,\ldots\},f(4k)=4^{k-1}((-1)^k\pi+(-1)^{k+1}R_k),R_k\in\mathbb{Q}.$$ $$\textbf{C3:}\lim_{k\to\infty}R_k=\pi.$$ Clearly, $(\textbf{C1}\wedge\textbf{C2})\implies\textbf{C3}.$ How do we go about proving these conjectures? It seems like $\textbf{C2}$ is the toughest of the three. $\textbf{C1}$ makes intuitive sense as the integrand is a decreasing function of $r.$","['definite-integrals', 'real-analysis', 'calculus', 'pi', 'rational-numbers']"
4666736,"Let $G$ be a circle of radius $R > 0$. Let $G_1, G_2,...,G_n$ be $n$ circles of equal radius $r > 0$, Then prove the following;","Let $G$ be a circle of radius $R > 0$ . Let $G_1, G_2, ..., G_n$ be $n$ circles of equal radius $r > 0$ . Suppose each of the $n$ circles $G_1, G_2, ..., G_n$ touches the circle $G$ externally. Also for $i = 1, 2, 3,...,n-1$ , the circle $G_i$ touches $G_{i+1}$ externally and $G_n$ touches $G_1$ externally, then prove that if $n=12$ , $\sqrt2(\sqrt 3 +1)r>R$ . I started this question with drawing out $G_1$ and $G_2$ and then tried the concept of the center of $G$ being bisected $n$ times, as in the angle subtended by the centers of $G_1$ and $G_2$ on center of $G$ as $\frac{2œÄ}{n}$ . It's here that I'm stuck, most probably as there is some trigonometry involved to create a relation with $r$ and $R$ . Is this approach the shortest and correct? EDIT- i managed to get $(r+R)\sin\frac{œÄ}{n}=r$ .","['algebra-precalculus', 'circles', 'geometry', 'sequences-and-series']"
4666776,Phase spaces for second order differential equations - Arnold's ODE,"this is my first post here, so please excuse me if I do anything wrong I'm a high school student, and my question is about phase planes, which I got interested in after learning about differential equations. In Arnold's ODE book, he introduces the notion of a phase plane here: This phase plane seems to be different from other vector fields I've seen, which has normally axes $x$ against $t$ , or $y$ against $x$ . It seems to be different because it is '2 dimensional' and that this case arises for second order differential equations. Gilbert Strang's course on YouTube also introduces this, but doesn't explicitly explain why the axes of the phase plane are $y'$ and $y$ . A comment in the video said that it is because 'any second order ODE can be transformed into a system of 2 first order ODEs'. My question is that I do not understand the process of defining the coordinates $x_1$ and $x_2$ of the phase space to be the position and velocity, instead of maybe the velocity and time, or using a three dimensional vector field, with position velocity and time. I know that time is specific to this case, which is time-dependent, but is time somehow built into the solution to the differential equation? I am not sure where time fits into this and how we can interpret the phase space without time. It would be helpful enough to point me in any further reading I would have to do to understand this, instead of a full answer if possible. I have tried to look for an explanation which relates creating phase plots to transforming second order equations to a system of first order ones, but to no avail. Thanks very much to anyone who can help! References: What is ""Phase Space"" in differential equations and classical mechanics? What is the defininition of phase Space in simple terms? Arnold's ODE computation of phase velocity","['vector-fields', 'mathematical-modeling', 'ordinary-differential-equations']"
4666780,Finding $\sin(\alpha+\beta)$ and $\cos(\alpha+\beta)$ if $\sin\alpha+\sin\beta=a$ and $\cos\alpha+\cos\beta=b$,"If: $\sin(\alpha) + \sin(\beta) = a$ and $\cos(\alpha) + \cos(\beta) = b$ Determine: $\sin(\alpha + \beta) = ?$ and $\cos(\alpha + \beta) = ?$ The right answers: $$\sin(\alpha + \beta) = \frac{2ab}{a^2 + b^2} \qquad
\cos(\alpha + \beta) = \frac{b^2 - a^2}{a^2+b^2}$$ I started by a*b, which results in $$\sin\alpha\cos\alpha + \sin\alpha\cos\beta + \sin\beta\cos\alpha + \sin\beta\cos\beta = ab \tag1$$ $$\sin\alpha\cos\alpha + \sin(\alpha + \beta) + \sin\beta\cos\beta = ab \tag2$$ $$\sin(\alpha + \beta) = ab - \frac{1}{2}(\sin2\alpha + \sin0) - \frac{1}{2}(\sin2\beta + \sin0) \tag3$$ $$\sin(\alpha + \beta) = ab - \frac{1}{2}(\sin2\alpha + \sin2\beta) \tag4$$ From this point, I kinda got stuck",['trigonometry']
4666781,Does Hessian of rank one mean curvature in one direction and flatness in the other,"I'm following a course on convex optimization by Stephen P. Boyd. https://learning.edx.org/course/course-v1:StanfordOnline+SOE-YCVX0001+1T2020/ My question is about Lecture 3 ( https://youtu.be/JrWSn5wW_Z0 ). Boyd gives a function: quadratic-over-linear : $f(x,y) = x^2/y,\: y>0$ The Hessian matrix of the function is: $$\nabla^2f(x,y)=\begin{bmatrix}2/y&&-2x/y^2\\-2x/y^2&&2x^2/y^3\end{bmatrix}=\frac{2}{y^3}\begin{bmatrix}y\\-x\end{bmatrix}\begin{bmatrix}y\\-x\end{bmatrix}^T$$ So  the Hessian matrix has rank one. Boyd says at https://youtu.be/JrWSn5wW_Z0?t=1527 : The fact that it's rank one tells you that at any point on that surface it's
sort of curving in one direction, but not the other.
Alright, so that's what it says. In one direction, it's flat.
In the other direction, it's got positive curvature. I don't see why it's true for any point. For example, $\nabla^2f(1,1)= \begin{bmatrix}2&&-2\\-2&&2\end{bmatrix}$ . So the function has curvature in both $x$ and $y$ direction. Maybe Boyd meant to say that this is true for any point $(x_0, y_0)$ where $\nabla f(x_0,y_0) = 0$ ?
In that case the line $x_0=0$ would represent the set of such points, and the Hessian matrix would be $\nabla^2f(0,y_0)= \begin{bmatrix}2/y_0&&0\\0&&0\end{bmatrix}$ . This would mean that the function has curvature only in $y$ direction. So, what am I missing here? Is the claim really true for any point? Is it true only for points where the gradient is zero? Is it true for any function in general?","['multivariable-calculus', 'convex-analysis', 'hessian-matrix']"
4666800,Upper bound for the norm of a matrix by using an upper bound on the entries,"Let $F:\mathbb{R}^3\setminus\{0\}\to\mathbb{R}^3$ be a function of class $C^1$ such that $$(x_1, x_2, x_3)\in \mathbb{R}^3\setminus\{0\}\mapsto F_i(x_1, x_2, x_3)\in\mathbb{R}, \quad \forall i\in\{1, 2, 3\}.$$ As an exercise, I have to find an upper bound (better if as tight as possible) for $$\left\Vert\left(\frac{\partial F_i}{\partial x_j}(y)\right)_{i, j=1, 2, 3}\right\Vert  $$ where the above notation refers to the norm of the matrix $\left(\frac{\partial F_i}{\partial x_j}(y)\right)_{i, j=1, 2, 3}$ as it is defined in https://en.wikipedia.org/wiki/Matrix_norm (the norm defined through the supremum). $\textbf{EDIT:}$ By following the idea given in the answer by @kieransquared, I checked again all the computations.
The only information I have is that $$ \frac{\partial F_i}{\partial x_j}(y) \le\begin{cases}
\displaystyle\frac{1}{\alpha^{\beta +1}} -\frac{y_i^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i=j\\[10pt]
\displaystyle -\frac{y_j^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i\neq j,
\end{cases}$$ where $a, \beta\ge 1$ , $\alpha,\gamma >0$ constants and $|x|$ denotes the euclidean norm of the vector $(x_1, x_2, x_3)$ . Hence, by using the triangle inequality, it follows that $$ \left\vert\frac{\partial F_i}{\partial x_j}(y)\right\vert \le\begin{cases}
\displaystyle\frac{1}{\alpha^{\beta +1}} +\frac{y_i^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i=j\\[10pt]
\displaystyle \frac{y_j^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i\neq j,
\end{cases}$$ Anyway, I can not deduce the desired upper bound from that information. I hope someone could help. Thank you.","['real-analysis', 'matrices', 'calculus', 'matrix-norms', 'upper-lower-bounds']"
4666899,Polynomial functional equation,"Find all polynomials $f : R \rightarrow R$ such that $f\left(\frac{1}{x+1}\right)=\frac{1}{f(x)-1}$ . Since the functions must be polynomials, I tried using an argument by degrees, but this did not lead me anywhere. Can someone help with some ideas?","['functional-equations', 'functions', 'polynomials']"
4666901,Suppose $P_1$ and $P_2$ are two $n$-dimensional convex polytopes. Does $\partial P_1 \subseteq\partial P_2$ imply that $P_1 = P_2$?,"Given two convex polytopes $P_1$ and $P_2$ with the same dimension, I want to know if the boundary of $P_1$ (denoted $\partial P_1$ ) being contained in the boundary of $P_2$ (denoted $\partial P_2$ ) is enough to conclude that $P_1$ and $P_2$ are the same polytope. From $\partial P_1 \subseteq\partial P_2$ we immediately have that $\mathcal V_1 \subseteq \mathcal V_2$ (where $\mathcal V_i$ is the vertex set of $P_i$ ), which also implies that $P_1 \subseteq P_2$ . My rough idea of how to proceed is as follows: Suppose that there exists $v^\dagger$ such that $v^\dagger \in \mathcal V_2 \setminus \mathcal V_1$ ; if there are many such $v^\dagger$ , pick it so that there exists a vertex, $v$ , adjacent to it (in $P_2$ ) such that $v \in \mathcal V_1$ . Now, there must be a vertex $v^*$ in $\mathcal V_1$ such that $v^*$ is adjacent to $v$ in $P_1$ but not in $P_2$ (not sure how to show this formally, but basically the idea is that in $P_2$ , $v^\dagger$ is ''in-between'' $v$ and $v^*$ ...). Then, the convex hull of $\{v,v^*\} \subseteq \partial P_1$ but $\{v,v^*\} \not\subseteq \partial P_2$ , a contradiction. Therefore, we have $\mathcal V_1 = \mathcal V_2$ , giving us that $P_1 = P_2$ . Appreciate any ideas about how to formalize the step of showing that such a $v^*$ must exist, or showing that the claim is false (or, if it is correct, an alternate proof!). I know that the $v^*$ existence step must necessarily leverage that the polytopes have the same dimension, but it's not clear to me how to do this exactly...","['analytic-geometry', 'polytopes', 'convex-hulls', 'convex-geometry', 'geometry']"
4666925,"If $f(x,t) = g(u)$, how do I relate $\iint{fdxdt}$ to $\int{gdu}$?","I'm learning about Fourier Transforms in the context of travelling waves on a dispersive medium, and my textbook sort of handwaves a simplification in which $$ \textbf{F}(k,\omega) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,t)e^{-i(kx-\omega t)}dxdt $$ becomes $$ \textbf{F}(k) = \int_{-\infty}^{\infty}f(x,0)e^{-ikx}dx $$ Which, in conjunction with a dispersion relation $\omega =\omega (k)$ , allows the Fourier series for the travelling wave to be expressed as a single integral with respect to k. In trying to wrap my head around the subtleties of why this works, I ran into a more general problem that I think is at the crux of my confusion: if $f(x,t)=g(u)$ where $u$ is a function of $x$ and $t$ , is there a way to relate $\iint{fdxdt}$ to $\int{gdu}$ ? Of course $du=\frac{\partial u}{\partial x}dx + \frac{\partial u}{\partial t}dt$ , but I have no clue how to use that to turn a single integral into two. I was also having trouble finding anything about this online since I don't know if there's a name for this simplification. Many thanks in advance for any and all help. Edit: I've managed to grasp a intuitive understanding for the Fourier simplification, but I'm still confused on how to mathematically justify it. Essentially, if you have a formula for the waveform at a specific time, then you can do a spatial FT to get the wavenumbers k that comprise it. Then, if you know the dispersion relation, essentially knowing the wavespeed associated with each wavenumber, then you can imagine pushing all the component waves of the spatial FT at their associated wavespeed, giving $$ f(x,t)=\frac{1}{2\pi} \int_{-\infty}^\infty \textbf{F}(k)e^{i(kx-\omega (k)t)}dk $$ With $\textbf{F}(k)$ being what I had written above. While I have a clear enough picture in my head as to why this works, I'm still having trouble justifying it algebraically. Hopefully the following picture makes my question clear:","['multivariable-calculus', 'fourier-transform']"
4666964,Rudin's RCA The Hahn-Banach Theorem.,"There is the theorem: If $M$ is a subspace of a normed linear space $X$ and if $f$ is a bounded linear functional on $M$ , then $f$ can be extended to a bounded linear functional $F$ on $X$ so that $||F||$ $=$ $||f||$ . There is the proof: We first assume that $X$ is a real normed linear space and, consequently, that $f$ is a real-linear bounded functional on $M$ . If $||f||$ $=$ $0$ , the desired extension is $F$ $=$ $0$ . Omitting this case, there is no loss of generality in assuming that $||f||$ $=$ $1$ . Choose $x_0$ $\in$ $X$ , $x_0$ $\notin$ $M$ , and let $M_1$ be the vector space spanned by $M$ and $x_0$ . Then $M_1$ consists of all vectors of the form $x$ $+$ $\lambda x_0$ , where $x$ $\in$ $M$ and $\lambda$ is a real scalar. If we define $f_1(x + \lambda x_0)$ $=$ $f(x)$ $+$ $\lambda \alpha$ , where $\alpha$ is any fixed real number, it is trivial to verify that an extension of $f$ to a linear functional on $M_1$ is obtained. The problem is to choose $\alpha$ so that the extended functional still has norm $1$ . This will be the case provided that $|f(x) + \lambda \alpha |$ $\leq$ $|| x + \lambda x_0||$ ( $x$ $\in$ $M$ , $\lambda$ real). Replace $x$ by $-\lambda x$ and divide both sides of $|f(x) + \lambda \alpha |$ $\leq$ $|| x + \lambda x_0||$ by $|\lambda|$ . The requirement is then that $|f(x) - \alpha|$ $\leq$ $||x-x_0||$ ( $x$ $\in$ $M$ ), i.e., that $A_x$ $\leq$ $\alpha$ $\leq$ $B_x$ for all $x$ $\in$ $M$ , where $A_x$ $=$ $f(x)$ $-$ $||x-x_0||$ and $B_x$ $=$ $f(x)$ $+$ $||x-x_0||$ . There exists such an $\alpha$ if and only if all the intervals $[A_x,B_x]$ have a common point, i.e., if and only if $A_x$ $\leq$ $B_y$ for all $x$ and $y$ $\in$ $M$ . I have two questions : $(1)$ -   How is discussing the case where $||f||$ $=$ $1$ enough for the proof of the  theorem? $(2)$ -  How do we conclude that There exists such an $\alpha$ if and only if all the intervals $[A_x,B_x]$ have a common point, i.e., if and only if $A_x$ $\leq$ $B_y$ ? Any help would be appreciated.","['banach-spaces', 'normed-spaces', 'analysis', 'functional-analysis', 'linear-transformations']"
4666992,How to find the particular solution to $y'' + \cos(t) y' = \cos(t)$?,"I want to find the  particular solution to $$y'' + \cos(t) y' = \cos(t)$$ I know that the homogeneous solution of this differential equation is given by $$y_h (t) = A + Be^{- t \cos (t)}$$ with $A,B \in \mathbb{R}$ . I think I can use the method of undetermined coefficients to find the particular solution, but I just can't figure out how.",['ordinary-differential-equations']
4667098,Prove that $A^{c}\subseteq B\cap C^{c}$ iff $(A\cup B)\cap(A\cup C^{c}) = U$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Let $A$ and $B$ be subsets of the universal set $U$ . Prove that $$A^{c}\subseteq B\cap C^{c} \Longleftrightarrow (A\cup B)\cap(A\cup C^{c}) = U$$ We haven't learned any laws surrounding subsets in proofs, I feel like law of subtraction may be a good place to start but I can't figure out where to go","['set-theory', 'logic', 'discrete-mathematics', 'computer-science']"
4667110,How to find $p(n+2)$ for a $n$th degree polynomial?,"Here's the exact question: If $p(x)$ denotes a polynomial of degree $n$ , such that $p(k) = 1/k$ for $k = 1, 2 , 3 , ..., n + 1$ , determine $p(2019)$ for $n = 2017$ Here was my initial approach: Take $f(x) = p(x) - 1/x$ Let the leading coefficient of $f(x)$ be $m$ Then, $$f(x) = m(x - 2)(x - 3)(x - 4)......(x - 2017)(x - 2018)$$ will be zero for $x = k$ I tried to plug in $0$ , but $1/0$ won't help at anything.Then I tried $-1$ , but that didn't help either. And then, I tried $1$ , and it only suggests that $m$ should be $0$ which is not probable for a $n$ th degree polynomial Can anyone please help me at this? PS: This question may not be similar to Suppose that $P(x)$ is a polynomial of... regarding the final solution So this question should not be a duplicate.....","['functions', 'polynomials']"
4667125,"Solve $\sin a =\sin x+\sin y, \cos a =\cos x+\cos y$ for $x$ and $y$ in terms of $a$","I saw this on quora. Solve $\begin{array}\\
\sin a
&=\sin x+\sin y\\
\cos a
&=\cos x+\cos y\\
\end{array}
$ for $x$ and $y$ in terms of $a$ . Here is my solution. My question:
is there a better way
of solving this? My answer is $y
=a+\frac{\pi}{3}
$ and $x
=a-\frac{\pi}{3}
$ . Squaring and adding, $1
= 2+2\sin(x)\sin(y)+2\cos(x)\cos(y)
$ or $\cos(x-y)
=-\frac12
$ so $x-y
=\pm\frac{2\pi}{3}
$ so either $x
=y+\frac{2\pi}{3}
$ or $x
=y-\frac{2\pi}{3}
$ . (I'm not worrying about the $2 k\pi$ for now.) Substituting $x
=y+\frac{2\pi}{3}
$ in the first, $\begin{array}\\
\sin a
&=\sin x+\sin y\\
&=\sin(y+\frac{2\pi}{3})+\sin y\\
&=\sin(y)\cos(\frac{2\pi}{3})+\cos(y)\sin(\frac{2\pi}{3})+\sin y\\
&=-\frac12\sin(y)+\frac{\sqrt{3}}{2}\cos(y)+\sin y\\
&=\frac12\sin(y)+\frac{\sqrt{3}}{2}\cos(y)\\
&=\sin(\frac{\pi}{6})\sin(y)+\cos(\frac{\pi}{6})\cos(y)\\
&=\cos(y-\frac{\pi}{6})\\
&=\sin(\frac{\pi}{2}-(y-\frac{\pi}{6}))
\qquad\text{(using } \cos(z)=\sin(\frac{\pi}{2}-z))\\
&=\sin(\frac{2\pi}{3}-y)\\
\end{array}
$ so $y
=-a+\frac{2\pi}{3}
$ and $x
=y+\frac{2\pi}{3}
=-a+\frac{4\pi}{3}
$ . Checking, $\begin{array}\\
\sin x+\sin y
=\sin(a)\\
\cos x+\cos y
=-\cos(a)\\
\end{array}
$ so this doesn't work!!! Note:
the checking was done by
Wolfram Alpha. Try the other solution. Substituting $x
=y-\frac{2\pi}{3}
$ in the first, $\begin{array}\\
\sin a
&=\sin x+\sin y\\
&=\sin(y-\frac{2\pi}{3})+\sin y\\
&=\sin(y)\cos(-\frac{2\pi}{3})+\cos(y)\sin(-\frac{2\pi}{3})+\sin y\\
&=-\frac12\sin(y)-\frac{\sqrt{3}}{2}\cos(y)+\sin y\\
&=\frac12\sin(y)-\frac{\sqrt{3}}{2}\cos(y)\\
&=\sin(\frac{\pi}{6})\sin(y)-\cos(\frac{\pi}{6})\cos(y)\\
&=-\cos(y+\frac{\pi}{6})\\
&=-\sin(\frac{\pi}{2}-(y+\frac{\pi}{6}))\\
&=-\sin(\frac{\pi}{3}-y)\\
&=\sin(y-\frac{\pi}{3})\\
\end{array}
$ so $y
=a+\frac{\pi}{3}
$ and $x
=y-\frac{2\pi}{3}
=a-\frac{\pi}{3}
$ . Upon checking,
this works.","['trigonometry', 'systems-of-equations']"
4667146,Interpreting Gaussian measurements in terms of information theory,"I have a quantity that I want to measure, and I have obtained three sets of measurements A, B, and C, each represented by their mean $\mu_A$ , $\mu_B$ , $\mu_C$ , and standard deviation $\sigma_A$ , $\sigma_B$ , $\sigma_C$ , respectively. Assuming that the measurements follow a Gaussian distribution, I want to understand which set of measurements contains more information about the true value of the quantity being measured from an information theory perspective. To give some context, let me provide numerical examples. Let's say that the true value of the quantity being measured is $x = 10$ . Set A has measurements with mean $\mu_A = 9.9$ and $\sigma_A = 0.1$ , set B has mean $\mu_B = 10$ and $\sigma_B = 1.0$ , and set C mean $\mu_C = 8$ and $\sigma_C = 10$ . It's intuitive to say that it's set A due to the low standard deviation (assuming there's no bias), but how can I quantify the amount of information that each set of measurements provides about the true value (perhaps as $-\log_2 p(X)$ )? Is there any way to compare the information content of the sets A, B, and C? Thank you very much for your help!","['descriptive-statistics', 'statistics', 'entropy', 'information-theory']"
4667151,"Prove: $f: \mathbb{R} \rightarrow \mathbb{R}$ st for every $x \in \mathbb{R}$ there exists $n$ st $f^{(n)}(x) = 0$, f is a polynomial.","If $f: \mathbb{R} \rightarrow \mathbb{R}$ is a smooth function such that for every $x \in \mathbb{R}$ there exists $n$ such that $f^{(n)}(x) = 0$, then f is a polynomial. I'm kind of lost on this one. I know that I have to use Baire's category theorem somewhere here, but I'm not sure exactly how.","['general-topology', 'baire-category']"
4667162,Prove transitivity of relation (defined by ùë•ùëÖùë¶ if the arithmetic mean of ùë• and ùë¶ equals the geometric mean of ùë• and ùë¶) on positive real numbers,"A relation $ùëÖ$ is defined on the set $\mathbb{R}^{+}$ of positive real numbers by $ùë•ùëÖùë¶$ if the arithmetic mean of $x$ and $y$ equals the
geometric mean of $ùë•$ and $ùë¶$ , that is, if $$\frac{1}{2} \cdot (ùë• + ùë¶) = \sqrt{xy}$$ In order to prove $R$ is an equivalence equation I proved that $R$ is reflexive and symmetric but I am having trouble proving it being transitive. I supposed $x,y,z \in \mathbb{R}^{+}$ such that $xRy$ and $yRz$ and got two equations $$
\begin{cases}
\frac{1}{2}(x+y)=\sqrt{xy} \\
\frac{1}{2}(y+z)=\sqrt{yz} \\
\end{cases}
$$ but I am struggling to reach $\frac{1}{2}(ùë• + z) = \sqrt{ùë•z}$ , I tried multiplying $\sqrt{z}$ and $\sqrt{x}$ to both side of each equations and substituting but I can't seem to eliminate $y$ .","['equivalence-relations', 'discrete-mathematics']"
4667172,$0\leq u\leq P_r\implies u=\lambda P_r$,"Let $u$ be a real-valued harmonic function on the open unit disk such that $0\leq u(r,\theta)\leq P_r(\theta)$ for all $0\leq r<1$ and $\theta$ , where $P_r(\theta)$ denotes the Poisson kernel. How can I deduce from this that $u(r,\theta)=\lambda P_r(\theta)$ for some constant $\lambda$ ? If the claim is true, then certainly $\lambda=u(0,0)$ , so we have a candidate value for $\lambda$ . But I'm not sure how to deduce $u(r,\theta)=\lambda P_r(\theta)$ on the whole unit disk.","['complex-analysis', 'harmonic-analysis', 'harmonic-functions', 'analysis']"
4667173,Is ${\rm Sym}(X)$ countable if $X$ is countable?,"Is ${\rm Sym}(X)$ countable if $X$ is countable? I tried to write all elements in order, like pick the first element as the permutation with the smallest $f(1)$ . If there are several permutations with the same $f(1)$ , pick the one with the smallest $f(2)$ , or $f(3)$ ..., then do the same thing to pick the second, the third element and so on. I wonder if this shows that ${\rm Sym}(X)$ is countable.","['symmetric-groups', 'group-theory', 'functions', 'infinity']"
4667182,Discovering $I_n=\int_0^{\frac{\pi}{2}} \frac{dx}{(1+\tan x)^{4n+2}}$ is rational.,"Latest Edit Great thanks to Quanto who gave a complete proof to the question by setting up a simple reduction formula on $I_n$ below: \begin{align} \boxed{I_{n}
=\ \frac1{2(4n+1)}+ \frac1{2(4n)}+ \frac1{4(4n-1)}-\frac14I_{n-1}}
\end{align} Let‚Äôs start with the easy one. $$
\begin{aligned}I_2& =\int_{-\frac{\pi}{4}}^{\frac{\pi}{4}} \frac{1}{\left[1+\tan \left(\frac{\pi}{4}-x\right)\right]^2} d x \\
& =\frac{1}{4} \int_{-\frac{\pi}{4}}^{\frac{\pi}{4}}(1+\tan x)^2 d x \\
& =\frac{1}{4} \int_{-\frac{\pi}{4}}^{\frac{\pi}{4}}\left(1+2 \tan x+\tan ^2 x\right) d x \\
& =\frac{1}{2} \int_0^{\frac{\pi}{4}}\left(1+\tan ^2 x\right) d x \quad \textrm{ (Properties of odd and even functions.)}
\\
& =\frac{1}{2} \int_0^{\frac{\pi}{4}} \sec ^2 x d x \\
& =\frac{1}{2}[\tan x]_0^{\frac{\pi}{4}} \\
& =\frac{1}{2}
\end{aligned}
$$ To proceed, I tried  further with the same method of evaluating $I_2$ by putting $x\mapsto \frac{\pi}{4} -x$ which transforms $$
\begin{aligned}
I_n & =\frac{1}{2^n} \int_{-\frac{\pi}{4}}^{\frac{\pi}{4}}(1+\tan x)^n d x \\
& =\frac{1}{2^n} \int_{-\frac{\pi}{4}}^{\frac{\pi}{4}} \sum_{k=0}^{n}\left(\begin{array}{l}
n \\
k
\end{array}\right) \tan ^k x d x \\
& =\frac{1}{2^{n-1}} \sum_{k=0}^{\left[\frac{n}{2}\right]}\left(\begin{array}{l}
n \\
2 k
\end{array}\right) \int_0^{\frac{\pi}{4}} \tan ^{2 k} x d x \quad \textrm{ (Properties of odd and even functions.)}
\end{aligned}
$$ Using the reduction formula for $\int_0^{\frac{\pi}{4}} \tan ^{2 k} x d x,$ we can continue our evaluation on $I_n$ as below: $$
\begin{aligned}
 I_3=-\frac{\pi}{8}+\frac{3}{4},\quad  I_4=\frac{2}{3} -\frac{\pi}{8}, \quad  I_5=\frac{5}{12}-\frac{\pi}{16},
\end{aligned}
$$ For $n=6$ , letting $x\mapsto \frac{\pi}{4}-x  $ transforms the integral into $$
\begin{aligned}
\int_0^{\frac{\pi}{2}} \frac{d x}{(1+\tan x)^6} & =\int_{-\frac{\pi}{4}}^{\frac{\pi}{4}} \frac{d x}{\left[1+\tan \left(\frac{\pi}{4}-x\right)\right]^6} \\
& =\frac{1}{64} \int_{-\frac{\pi}{4}}^{\frac{\pi}{4}}(1+\tan x)^6 d x\\\\
\end{aligned}
$$ Expanding the integrand into $3$ odd and $4$ even functions simplifies it into $$
\begin{aligned}I_6&=\frac{1}{32} \int_0^{\frac{\pi}{4}}\left(1 +\tan ^6 x +15 \tan ^2 x+15 \tan ^4 x\right) d x\\&= \frac{1}{32} \int_0^{\frac{\pi}{4}}\left(1-\tan ^2 x+\tan ^4 x+15 \tan ^2 x\right) d(\tan x)\\& =\frac{1}{32}\left[\tan x+\frac{14 \tan ^3 x}{3}+\frac{\tan ^5 x}{5}\right]_0^{\frac{\pi}{4}}\\&=\frac{11}{60}  \end{aligned}
$$ When evaluating $I_6$ , I found the symmetry of the binomial coefficients in expansion, I can group the corresponding terms together and found that $I_n$ has no $\pi$ when $n\equiv 2 \quad  \pmod 4$ , i.e. $$I_{4n+2}\in Q.$$ Proof: $$
\begin{aligned}
I_{n} & =\frac{1}{2^{4 n+2}} \int_{-\frac{\pi}{4}}^{\frac{\pi}{4}}(1+\tan x)^{4 n+2} d x \\
& = \frac{1}{2^{4 n+2}} \int_{-\frac{\pi}{4}}^{\frac{\pi}{4}} \sum_{k=0}^{2n+1} {4n+2\choose 2k}  \tan ^k x d x\\
\end{aligned}
$$ Using the properties of odd and even function simplifies the integral $$
I_{n}=\frac{1}{2^{4 n+1}} \sum_{k=0}^{2 n+1}\int_0^{\frac{\pi}{4}}\left(\begin{array}{c}
4 n+2 \\
2 k
\end{array}\right) \tan ^{2 k} x d x
$$ Grouping the symmetric terms gives $$
I_{n}=\frac{1}{2^{4 n+1}} \sum_{k=0}^{n+1}\left(\begin{array}{c}
4 n+2 \\
2 k
\end{array}\right) \int_0^{\frac{\pi}{4}}
\left(\tan ^{2 k} x+\tan ^{4 n+2-2 k} x\right) d x
$$ $(-1)^k+(-1)^{2 n+1-k} =(-1)^k\left[1+(-1)^{2(n-k)+1}\right] =0 \Rightarrow 1+y| y^k+y^{2 n+1-k}. $ Therefore $\tan ^{2 k} x+\tan ^{4 n+2-2 k} x$ is divisible by $1+\tan^2x$ and hence $$\tan ^{2 k} x+\tan ^{4 n+2-2 k} x=(1+\tan^2x)P_k(\tan x)$$ for some polynomial $P_k(y)$ with integer coefficients. $$
\begin{aligned}
I_{n}= & \frac{1}{2^{4 n+1}} \sum_{k=0}^{n+1}\left(\begin{array}{c}
4 n+2 \\
2 k
\end{array}\right) \int_0^{\frac{\pi}{4}} P_k(\tan x) d(\tan x) \\
= & \frac{1}{2^{4 n+1}} \sum_{k=0}^{n+1}\left(\begin{array}{c}
4 n+2 \\
2 k
\end{array}\right) Q_k(1) \in \mathbb{Q} \\
\end{aligned}
$$ where $ Q_k^{\prime}(y)=P_k(y).$ Is there any other simpler proof? Comments and alternative methods are highly appreciated.","['integration', 'calculus', 'trigonometric-integrals', 'polynomials', 'indefinite-integrals']"
4667183,Can a Jordan curve contain measure-theoretic interior points of the domain it bounds?,"Let $I$ be an interval, and $\gamma:I\to \mathbb{R}^2$ a Jordan curve. By the Jordan--Schoenflies Theorem $\gamma(I)$ splits up $\mathbb{R}^2$ in two connected pieces, that is, $\mathbb{R}^2\setminus \gamma(I)$ is made by exactly two connected components, a bounded one, $V$ , and an unbounded one $U$ , that have as boundary the image of the curve $\gamma(I)$ . The bounded one is homeomorphic to the unit disk. I know that are bad-behaved curves, √† la Knopp--Osgood, such that their image has Hausdorff dimension $2$ (if you prefer, a slightly stronger thing is to say it has positive $2$ -dimensional Lebesgue measure). I am wondering whether I can tell something on the measure-theoretic interior on $V$ , denoted by $V^{(1)}$ . We say that a point $p\in \mathbb{R}^2$ belongs to $V^{(1)}$ if $$\lim_{r\to 0^+} \frac{|B_r(p)\cap V|}{\pi r^2} = 1.$$ Can $\gamma(I)$ contain any point that belongs to the measure-theoretic interior of $V$ ? Yes: any curve with an ""inner cusp"" will do. Can the set of these points have positive ${H}^1$ -measure? If yes, is this still true if I exclude the curve from being a Knopp--Osgood one?","['plane-curves', 'measure-theory', 'geometric-measure-theory']"
4667195,"Prove that $\prod_{1\leq i,j\leq n}\frac{1+a_ia_j}{1-a_ia_j}\geq1$ for $n$ real numbers $a_i\in(-1,1)$","I'm trying to prove the following inequality: $$\prod_{1\leq i,j\leq n}\frac{1+a_ia_j}{1-a_ia_j}\geq1\tag{1}$$ for $a_i\in(-1,1)$ . I first tried induction but doesn't seem to work well. Special cases can be proved like $n=2,3$ using brute force but I am trying to find a simpler proof. Some observations: Define the function $f_n(x_1,x_2,\ldots,x_n)=\prod_{1\leq i,j\leq n}\frac{1+x_ix_j}{1-x_ix_j}$ for $(x_1,\ldots,x_n)\in(-1,1)^n$ . Note that if $a_i\geq0$ for all $i$ then all the terms in the product are at least $1$ and hence $(1)$ is trivially true. So, is it possible to prove the following? $$f_n(x_1,x_2,\ldots,x_n)\geq1\iff f_n(-x_1,x_2,\ldots,x_n)\geq1$$ , if yes then by symmetry we can repeat this process and make all $a_i\geq0$ . Also, using Induction we can do the following: Base case $n=1$ is trivial. Assuming for some $n\geq1$ , we see that $$f_n(x_1,x_2,\ldots,x_n)f_n(-x_1,x_2,\ldots,x_n)=f_{n-1}(x_2,\ldots,x_n)^2\geq1$$ and hence at least one of the following is true: $f_n(x_1,x_2,\ldots,x_n)\geq1$ or $f_n(-x_1,x_2,\ldots,x_n)\geq1$ . The case of equality As @RiverLi pointed out and @MartinR wrote an answer, it is evident that equality holds if and only if $$\sum_{k=1}^{\infty}\frac{1}{2k-1}\left( \sum_{i=1}^n a_i^{2k-1}\right)^2=0$$ The partial sums form an increasing sequence of nonnegative reals. So, the series can evaluate to zero if and only if $$\sum_{i=1}^na_i^{2k-1}=0,k\geq1$$ which is a separate and interesting problem and is discussed here.","['contest-math', 'algebra-precalculus', 'inequality', 'real-analysis']"
4667204,Find the probability of at least one of us is chosen,"I am struggling with a question as follows, There will be a draw between 10000 attendants lets say. In total, 1000 people will be choosen. Me and 4 of my friends have joined to the draw. So we have 5 people in. I wonder, what should be my approach to calculate what is the probability that at least one of us will be choosen? I mean, it looks like each have 1000/10000 = 0.1 probability to be choosen. But I cannot simply add 0.1 5 times because then if we were 10 people, result would be 100% probability that at least one of us will be choosen by this method, however, it is not true of course. Also, another thing bothers me is the fact that the second one to be choosen will not be 0.1, instead 999/9999, and 3rd one 998/9998, and so on. So I cannot say that each will have 0.1 probability. If the numbers were smaller, the difference in probabilities would be very noticable. As seen, I need a bit of enlightment about how to approach such a problem. By the way, my best solution to this is 1 - (1 - 0.1)^5 Which is 1 - the joint probability of all of us not being choosen, which I think should give the probability that at least one of us will be chosen. However, the thing about 0.1 not being valid for 2nd, 3rd, 4th, and 5th attendants bothers me with this solution. Thanks!","['combinations', 'probability-theory', 'probability']"
4667238,Closed form of $\int_0^\pi \frac{1}{(1+\sin x)^n} d x$,"Starting with the simplest one, we can rationalize the denominator first. $$
\begin{aligned}
I_1& =\int_0^\pi \frac{1}{1+\sin x} d x \\
& =\int_0^\pi \frac{1-\sin x}{\cos ^2 x} d x \\
& =\int_0^\pi\left(\sec ^2 x-\sec x \tan x\right) d x \\
& =[\tan x-\sec x]_0^\pi \\
& =2
\end{aligned}
$$ As the power increases, this method is rather long and I started to use half-angle formula of cosine. $$
\begin{aligned}
I_2 & =2 \int_0^{\frac{\pi}{2}} \frac{1}{(1+\sin x)^2} d x \\
& =2 \int_0^{\frac{\pi}{2}} \frac{1}{(1+\cos x)^2} d x \\
& =2 \int_0^{\frac{\pi}{2}} \frac{1}{\left(2 \cos ^2 \frac{x}{2}\right)^2} d x \\
& =\frac{1}{2} \int_0^{\frac{\pi}{2}} \sec ^4 \frac{x}{2} d x \\& =\int_0^{\frac{\pi}{4}} \sec ^4 x d x \\
& =\int_0^{\frac{\pi}{4}}\left(1+\tan ^2 x\right) d(\tan x) \\
& =1+\frac{1}{3}\\
& =\frac{4}{3}
\end{aligned}
$$ Then I try to generalize the result for $I_n$ similarly. $$
\begin{aligned}
\int_0^\pi \frac{d x}{(1+\sin x)^n} & =\frac{1}{2^{n-2}} \int_0^{\frac{\pi}{4}} \sec ^{2 n} x d x \\
& =\frac{1}{2^{n-2}} \int_0^{\frac{\pi}{4}}\left(1+\tan ^2 x\right)^{n-1} d(\tan x) \\
& =\frac{1}{2^{n-2}}\int_0^{\frac{\pi}{4}}  \sum_{k=0}^{n-1}\left(\begin{array}{c}
n-1 \\
k
\end{array}\right) \tan^{2k} x d(\tan x) \\
& =\frac{1}{2^{n-2}} \sum_{k=0}^{n-1}\left(\begin{array}{c}
n-1 \\
k
\end{array}\right) \frac{1}{2 k+1}
\end{aligned}
$$ Is there any other simpler answer or method? Comments and alternatives are highly appreciated.","['calculus', 'definite-integrals', 'trigonometry']"
4667272,How to compute the number of tuples such that no element appears exactly once?,"I'm trying to compute the number of $t$ -tuples such that no element appears exactly once in the tuple, and each element can take values from $[N]=\left\{0,\ldots,N-1\right\}$ . Let us denote $u_{N, t}$ this quantity. For instance, for $t=4$ and $N=128$ , $(1, 4, 1, 4)$ is a valid tuple (each element within the tuple has at least one other copy) but $(1, 4, 1, 1)$ is not ( $4$ is alone). It is fairly simple to see that: $u(N, 1)=0$ , since every tuple will contain a single element, it can't have any other copies of it $u(N, 2)=N$ , since every valid tuple can be written as $(x, x)$ , with $x\in[N]$ $u(N, 3)=N$ , since every valid tuple can be written as $(x, x, x)$ My reasoning for the general case is the following: $u_{N, t}$ is equal to $N^t$ minus the number of tuples such that at least one element is unique. For $k\geqslant1$ , we thus choose the positions of the unique elements, for which we have $\binom{t}{k}$ choices We have to choose the values for these elements, for which we have $\frac{N!}{(N-k)!}$ choices. We finally have to choose the values for the other $N-k$ elements. We know that they cannot contain a single unique element, thus we have $u_{N-k,t-k}$ choices. All in all, we have: $$u_{N, t}=N^t-\sum_{k=1}^t\binom{t}{k}\frac{N!}{(N-k)!}u_{N-k, t-k}$$ However: I'm not sure this formula is correct: for $t=1$ and $t=2$ there are some corner cases that make it most likely wrong (we don't get $0$ and $N$ respectively), but it might work for $t\geqslant3$ For large values of $N$ , this can be quite tedious to compute (dynamic programming?) I would especially be interested in a closed-form for $u_{N, t}$ (or at least some asymptotic behavior for $t=o(N)$ ) Is there any clever way to compute this quantity that I missed?","['inclusion-exclusion', 'combinatorics']"
4667277,"Difference in usage between function, mapping, functional, form, and operator?","The word function has many synonyms (or close to synonyms), including: map functional form operator transformation What is the difference, in meaning or usage, between them? I understand that exact definition may not be standard.  But there certainly seems to be a difference in typical usage.  For example, multilinear form : what is form in that context, and how does it typically differ from function ? What is the difference in typical usage of each quasi-synonym for function ?","['operator-theory', 'functions', 'functional-analysis', 'differential-forms', 'terminology']"
4667292,Is there a limit/convergence in that sequence of ratios of primeproducts?,"In another question I discuss the function on primes $$ r(p) = { p \over 2^{E_2} 3^{E_3} 5^{E_5} ... q^{E_q} } \tag 1
$$ where $p \in  \mathbb P$ and $q$ is the next smaller prime before $p$ . Here $$E_q = { q \over (q-1)^2 } \tag 2 $$ Looking at $p \lt 1e8$ it seems, this could converge to some value above $0.5221...$ but it might as well increase unboundedly with increasing $p$ . Q: Does a limit $ \lim_{p \to \infty} r(p)$ exist? And in case it exists, what value does it assume? (Graphic from linked question): Another graphic with x-scale doubly logarithmic: A graphic showing even more values (up to $p \lt 10^8$ ) but the upper and lower hullcurves giving some suggestion for convergence:","['number-theory', 'prime-numbers', 'sequences-and-series']"
4667332,"Prove $\mathbb{P}_k^1 \cong \operatorname{Proj} k[x,y,z] / (x^2 + y^2 -z^2)$","Let $k$ be a field with $\operatorname{char} k \neq 2$ .
I think it's true that: $$\def\Proj{\operatorname{Proj}}\Proj k[x,y,z] / (x^2 + y^2 -z^2) \cong \Proj k[\lambda, \mu] = \mathbb{P}_k^1$$ The left hand side is the projective version of a ""circle"". My guess that these schemes are isomorphic comes from the following ""isomorphism"", which is constructed using the well known ""line-intersection-trick"" with the point "" $(-1 : 0 : 1)$ "" on the circle: $$
\begin{align}
(x : y : z) &\mapsto \begin{cases} (y : x + z) & \text{for }(x : y : z) \neq (-1 : 0 : 1) \\ (x - z : y) & \text{for }(x : y : z) \neq (1 : 0 : 1)\end{cases} \\
(\mu^2 - \lambda^2 : 2 \lambda \mu : \mu^2 + \lambda^2) &\leftarrow (\lambda : \mu)
\end{align}
$$ This is how one would give a morphisms of classical varieties. Applying those two morphisms after each other (and cancelling!) yields the identity, respectively. It makes me pretty confident that this yields an isomorphism of schemes. However, I don't know how to show this. Or at least, I don't know how to do it effectively. There should be a pretty elegant+fast way to do it from this prototype, no?
But difficulties I came across are: We can not just show that these induce isomorphisms of graded rings, because they don't. This is for one thing because we need two charts (see second point), but also because we need cancellation. For example, applying our maps $\mathbb{P}_k^1 \to \Proj k[x,y,z]/I \to \mathbb{P}_k^1$ with the first chart yields $(\lambda : \mu) \mapsto (2\lambda \mu : 2 \mu^2)$ . This is the identity, but to see it we need to cancel by $2\mu$ , and in particular it doesn't define an isomorphism of rings (also not localized ones, no?). To define the morphism to $\mathbb{P}^1$ , we need two charts. (I think it's not possible to do it with only one chart, no?) So the proof would need to use at least some localizations. If anybody could show me how to elegantly+efficiently turn my sketch of the morphisms into a proof that the schemes are isomorphic, I would be very grateful!",['algebraic-geometry']
4667411,"How can I prove that $\vdash \exists \overline{z}(\overline{z}=\{x,y\})$","I'm studying the one-sorted version of NBG (Neumann-Bernays-G√∂del) Class Theory (that is, NBG with only one alphabet). I want to prove that $\vdash \exists z(\exists Z(z\in Z)\wedge z=\{x,y\})$ but I don't how. Below are some inference rules and axioms I'm using: Axiom: Let $\Gamma$ be a list of formulas, $\phi ,\psi$ be two formulas e $x,y$ be two variables. We assume that $\forall x\phi \vdash \phi $ . Moreover, if $y$ is not a bounded variable of $\phi$ , then $\forall x\phi \vdash \phi [y/x]$ Suppose that $x$ isn't a free variable of $\psi$ . If $\Gamma,\phi \vdash \psi$ , then $\Gamma,\exists x\phi \vdash \psi $ . I want to emphasize that I'm NOT using the existential instantiation . Let $\phi$ be a formula. We denote $\color{red}{\forall \overline{x}(\phi (\overline{x}))}:=\forall x\big(\exists X(x\in X)\Rightarrow \phi (x)\big)$ $\color{red}{\exists {\overline x}(\phi (\overline{x}))}:=\exists x\big(\exists X(x\in X)\wedge \phi (x)\big)$ . Axiom of Pairing: We assume that $$\vdash \forall \overline{x}\forall \overline{y}\exists
 \overline{z}\forall w\big(w\in \overline{z}\Leftrightarrow
 (w=\overline{x}\vee w=\overline{y})\big)$$ Below is what I did: Definition: Let $x,y,Z$ be three classes. We say that $x$ and $y$ are the only elements of $Z$ if $\vdash \forall w\big(w\in Z\Leftrightarrow (w=x\vee w=y)\big)$ Proposition: Let $x,y,Z_1,Z_2$ be four classes. If $x$ and $y$ are the only elements of $Z_1$ , then $(Z_2=Z_1)\equiv \forall w\big(w\in Z_2\Leftrightarrow (w=x\vee w=y)\big)$ . Because of the previous proposition, if $x$ and $y$ are the only elements of a class $Z$ , then we can denote $Z$ by $\color{red}{\{x,y\}}$ . Using the axiom of pairing and the previous notations, I want to prove that, given any two sets $x$ and $y$ , we have $\vdash \exists \overline{z}(\overline{z}=\{x,y\})$ . That is, I want to prove that if $\vdash \exists X(x\in X)$ and $\vdash \exists Y(y\in Y)$ then $\vdash \exists z\big(\exists Z(z\in Z)\wedge z=\{x,y\}\big)$ . I was able to prove that $$\vdash \exists z\big(\exists Z(z\in Z)\wedge \forall w(w\in z\Leftrightarrow (w=x\vee w=y))\big)\color{red}{(1)}.$$ So, to prove what I want, I just need to prove that $\vdash \forall w\big(w\in z\Leftrightarrow (w=x\vee w=y)\big)$ and use the previous proposition. However, since existential instantiation isn't allowed, I don't know how to obtain the previous deduction from $(1)$ and from the basic rules of inference . Thank you for your attention!","['predicate-logic', 'first-order-logic', 'logic', 'elementary-set-theory', 'set-theory']"
4667435,PI Controller - Integral term is causing instability,"I am simulating the height of the reservoir in a hydro power plant using Matlab, without Simulink. The radial gates of the power plant are used to control the height of the water in the reservoir. This is done by implementing a PI controller on the radial gates. First, I am trying to let things work with only one radial gate. The flow into the reservoir is chosen to be $$Q_{in} = 400 \,\,m^3/s. \qquad \qquad (1)$$ The output flow through a radial gate is given by $$Q_{out} = \frac{dV}{dt} = C_g(D) \cdot D \cdot L \cdot \sqrt{2\cdot g \cdot H_g} \qquad \qquad (2)$$ where $$H_g = H - \frac{D}{2} \qquad \qquad (3)$$ . Rewriting eq. (2) with respect to (3) gives $$Q_{out} = C_g(D) \cdot D \cdot L \cdot \sqrt{2g (H - \frac{D}{2})} \\
= C_g(D) \cdot D \cdot L \cdot \sqrt{g} \cdot \sqrt{2H-D}. \qquad \qquad (4)$$ Grouping all the constants as $C_1 = L \cdot \sqrt{g}$ gives $$Q_{out} = C_g(D) \cdot D \cdot C_1 \cdot \sqrt{2H-D} \qquad \qquad (5)$$ Eq. (5) includes following variables and constants: $C_g(D)$ is a function dependent on the gate opening $D$ . $D$ is the gate opening. It's interval is 0 - 8.7 meters. $C_1$ consists of $L$ which is the width of the gate and gravity $g$ . $H$ is the height of the reservoir, calculated from the edge of the lower part of the
gate where $D = 0$ . Furthermore, I am using a PI controller to control the height $H$ . I am using a for-loop in Matlab to simulate this problem. The discrete PI controller I use is calculated as follows. The error in Matlab is calculated by $$error(1,i) = H(1,i) - H_d(1,i)$$ where $H$ is the height and $H_d$ is the reference value or the set point. The height of the reservoir should always be at $116 m$ above sea level.
In the program, I subtract the lower edge of the radial gate, where $D = 0m$ , which is $H_{edge} = 110m$ , so the radial gate is placed very high above sea level. I also choose the initial height in the simulation to be $H_{initial} = 115.8 m$ , therefore, $$H(1,1) = H_{initial} - H_{edge} = 115.8 - 110 = 5.8m \\
H_d(1,1) = H_{d} - H_{edge} = 116 - 110 = 6m.$$ This gives an initial error of $error = -0.2m$ . Next thing I do is to calculate the $P$ and the $I$ terms: $$PI\_P(1,i) = k_p \cdot error(1,i) \\
PI\_I(1,i) = (\frac{k_i}{T_i}) \cdot (PI\_I(1,i-1) + 0.5 \cdot (error(1,i) + error(1,i-1))\cdot dt) - intWindup(1,i)$$ The integral term is actually calculated in the program as if i == 1
        PI_I(1,i) = ((ki/Ti)*error(1,i)*dt) - intWindup(1,i);
    else
        PI_I(1,i) = ((ki/Ti)*(PI_I(1,i-1) + 0.5*(error(1,i) + error(1,i-1))*dt)) - intWindup(1,i);
    end The controller output is then calculated as the sum of the two terms: $$
PI\_u(1,i) = PI\_P(1,i) + PI\_I(1,i)
$$ Since the radial gate can only vary between 0m up to 8.7m, I used a integral windup method called ""backwards calculation"" so that the integrator will not ""windup"". This is shown in the image below: Next thing I do is calculate the saturation $$u\_sat(1,i)  = CheckSaturation(PI\_u(1,i),Dmax,Dmin)$$ The function I wrote is given below function u_sat = CheckSaturation(u,gateMax,gateMin)
% Controller saturation check

    if u >= gateMax
        u_sat = gateMax;
    elseif u <= gateMin
        u_sat = gateMin;
    else
        u_sat = u;
    end
end After this, I calculate the integral windup as $$intWindup(1,i+1) = intWindupBackCalculation(PI\_u(1,i),u\_sat(1,i),k\_lim)$$ where $k\_lim = 1$ . The function I wrote is given by function val = intWindupBackCalculation(u,u_sat,k_lim)
    diff = u - u_sat;
    val = diff * k_lim;
end Now finally, I update the model of the reservoir with the following code Cg1(1,i)     = Cg_D(D1(1,i),Dmin,Dmax);


    % The physical system
    Qout_gate1(1,i)  = Cg1(1,i) * C1 * D1(1,i) * sqrt(2*H(1,i) - D1(1,i));
    Qout(1,i)        = Qout_gate1(1,i) + Qout_m;
    H_dot(1,i)      = (1/A(1,i)) * (Qin(1,i) - Qout(1,i));
    H(1,i+1)        = H(1,i) + 0.5 * (H_dot(1,i) + H_dot(1,i+1)) * dt; This is simply calculation of eq. (2) from above, $Qout\_m = 0$ is the flow through the machines which is excluded in the calculations. The differential equation used to calculate the change of height is $$
\dot H = \frac{1}{A} (Q_{in} - Q_{out})
$$ then in the last line of the code, I use discrete integral to calculate the height. My question starts now . Below is a plot where I choose $k_p = 40$ and $k_i = 0.1$ $H$ is the change of height per second $H_d$ is the desired height, or the reference level, 116m $D1$ is the radial gate opening in meters error is simply the error, the blue line, while the integral windup is the red line.
it is also possible to see the proportional and integral gains in the legend. Qin is the red line and Qout is the blue line $PI\_P$ is the proportional term of the controller $PI\_I$ is the integral term of the controller In these figures above, it can be seen that the controller tries to keep the water height at 116m but there is this settling error, I think it's related to slowness of the integral term. It can also be seen that the radial gates opens up and it tries to regulate the water heigt. The error can be seen as very small related to the integral windup, but the error is there and it's not always zero. The flow in vs. flow out starts to be equal slowly. On the last figure, it can be clearly seen that the proportional term and the integral term is working. Now I choose $k_p = 40$ again, but the integral gain $k_i = 1$ In the figures above, the settling error is much smaller now and the response is much faster. The integrator is doing much more now, but not enough. I want the water level to be at 116m above sea level with only +/- 5cm. In real life, this system is very slow. The reservoir is huge, its area is $A = 3.8 \cdot 1000^2 \, \, m^2$ or $3.8 \,\, km^2$ . Now I want to increase the integral gain, and go above 1. So I choose $k_p = 40$ and $k_i = 2$ . In the figures above, it can clearly be seen that everything is very slow now which is not acceptable. Now I increase the integral term to $k_i = 5$ and everything goes unstable and crashes as you can see in the figures below. My question is:
What can I do about this problem? Do you have any idea of what I'm doing wrong or what I could do better? The integrator is being a little bit of problem for me and I thing it's a bit strange that the system crashes. The whole code is below: t0      = 0.0;           % [s] Initial time
days    = 0;
hours   = 8;
minutes = 60;
seconds = 60;

F_freq = 50;    % Frequency (increase -> smaller dt, decrease greater dt)
Fs = 2*F_freq;   % Samples per second (Sample frequency Fs must be at least two times the frequency F_freq to avoid aliasing)
dt = 1/Fs;       % Seconds per sample

if days == 0 && hours ~= 0 && minutes ~= 0 && seconds ~= 0
    t_f     = seconds*minutes*hours; % [s] Final time
elseif days == 0 && hours == 0 && minutes ~= 0 && seconds ~= 0
    t_f     = seconds*minutes; % [s] Final time
elseif days == 0 && hours == 0 && minutes == 0 && seconds ~= 0
    t_f     = seconds; % [s] Final time
else
    t_f     = seconds*minutes*hours*days; % [s] Final time
end

t       = (t0:dt:t_f-dt)';    % [s] Sample instants
T_plot  = t';

Nsim    = length(t); % A           = 3.8 * 1000^2; % [m^2]     Area of Hagal√≥n, ""equivalent to 3.8 km^2""
% Qout_machine  = 352;      % [m^3/s]   Max flow through machines.
% Qin(1,1:Nsim)         = [200*ones(1,round(Nsim/8)) 300*ones(1,round(Nsim/8)) 400*ones(1,round(Nsim/8)) 500*ones(1,round(Nsim/8))...
%                          600*ones(1,round(Nsim/8)) 700*ones(1,round(Nsim/8)) 600*ones(1,round(Nsim/8)) 500*ones(1,round(Nsim/8))];          % [m^3/s]   Flow into Hagal√≥n

Qin(1,1:Nsim)         = 400;
  
H_edge      = 110;
H_initial   = 115.8 - H_edge;        % [m]       Initial height of the lagoon
% Hd          = [116*ones(1,round(Nsim/2)) 116.4*ones(1,round(Nsim/2))] - H_edge;                  
% [m]       H desired (reference value) 
Hd(1,1:Nsim)          = 116 - H_edge;          % [m]       H desired (reference value) 

% Parameters for radial gate
L       = 12;           % [m]
g_sqrt  = sqrt(9.82);   % [m/s^2]
C1      = L*g_sqrt;     % Group C1 as a constant [m^2/s^2]

Dmax    = 8.7;          % [m] Maximum opening D of a radial gate
% Dmax    = 15;         % [m] Maximum opening D of a radial gate
Dmin    = 0;            % [m] Minimum opening D of a radial gate

a_slope = 8.7/522; % Model
H_dot(1,1:Nsim)     = 0;
H(1,1:Nsim)         = H_initial;
D1(1,1:Nsim)        = 0;
Cg1(1,1:Nsim)       = Cg_D(D1(1,1),Dmin,Dmax);
A(1,1:Nsim)         = 3.8 * 1000^2;
Qout_m              = 0; % Flow out of two machines. Max flow is Qout_m = 352 m^3/s
Qout_gate1(1,1:Nsim)= 0;
Qout_gate1(1,1)= Cg1(1,1) * C1 * D1(1,1) * sqrt(2*H(1,1) - D1(1,1));
Qout(1,1:Nsim)      = 0;

% PI
error(1,1:Nsim)     = H(1,1) - Hd(1,1);
PI_P(1,1:Nsim)      = 0;
% PI_I(1,1:Nsim)      = 0.000518663;
PI_I(1,1:Nsim)      = 0;
PI_u(1,1:Nsim)      = 0;
u_sat(1,1:Nsim)     = 0;

% P controller for the radial gate opening
error_gate(1,1:Nsim)    = 0;
PI_P_gate(1,1:Nsim)     = 0;
PI_I_gate(1,1:Nsim)     = 0;
PI_u_gate(1,1:Nsim)     = 0;
u_sat_gate(1,1:Nsim)    = 0; % P and I gain coefficentes for the height of the reservoir
kp          = 40;               % Proportional Gain
% tau_i       = seconds*minutes;  % Time constant for I term
% ki          = 1/46104;
ki          = 5;
k_lim       = 1;
% ki          = 1/tau_i;          % Integral Gain
% ki=0;
% ki=kp;
% Ti          = 1/dt;             % Time constant for integral term
Ti          = 1;

% P gain coefficient for the gate opening
kp_gate     = 1;

% Anti-Reset Windup switch coefficient
kc(1,1:Nsim)        = 1;
intWindup(1,1:Nsim) = 0;
kc_gate(1,1:Nsim)   = 1; for i = 1 : Nsim-1


% PI controller for the height in the reservoir

error(1,i) = H(1,i) - Hd(1,i); % Correct one

PI_P(1,i)   = kp*error(1,i);
if i == 1
    PI_I(1,i) = ((ki/Ti)*error(1,i)*dt) - intWindup(1,i);
else
    PI_I(1,i) = ((ki/Ti)*(PI_I(1,i-1) + 0.5*(error(1,i) + error(1,i-1))*dt)) - intWindup(1,i);
end
PI_u(1,i)   = PI_P(1,i) + PI_I(1,i);

% Check saturation of the radial gate
u_sat(1,i)  = CheckSaturation(PI_u(1,i),Dmax,Dmin);

% Calculage anti-reset windup calculation using clamping
%     kc(1,i+1)     = intWindupClamping(error(1,i+1),PI_u(1,i),u_sat(1,i));
  intWindup(1,i+1) = intWindupBackCalculation(PI_u(1,i),u_sat(1,i),k_lim);

% Feedback for the opening of the radial gate (slowness)
if i == 1
    error_gate(1,i) = 0;
    D1(1,i) = kp_gate*a_slope*error_gate(1,i);
else
    error_gate(1,i) = u_sat(1,i) - D1(1,i-1);
    D1(1,i) = D1(1,i-1) + kp_gate*a_slope*error_gate(1,i);
end
%     D1(1,i) = u_sat(1,i);

Cg1(1,i)     = Cg_D(D1(1,i),Dmin,Dmax);
%     Cg1(1,i)     = 0.7;

% The physical system
Qout_gate1(1,i)  = Cg1(1,i) * C1 * D1(1,i) * sqrt(2*H(1,i) - D1(1,i));
Qout(1,i)        = Qout_gate1(1,i) + Qout_m;
H_dot(1,i)      = (1/A(1,i)) * (Qin(1,i) - Qout(1,i));
H(1,i+1)        = H(1,i) + 0.5 * (H_dot(1,i) + H_dot(1,i+1)) * dt;

%     A(1,i) = AreaReservoir(H(1,i+1));

end

PI_P(1,i+1)   = kp*error(1,i+1);
Qout_gate1(1,i+1)  = Cg1(1,i) * C1 * D1(1,i+1) * sqrt(2*H(1,i) - D1(1,i+1)); figure; % Change of height
%sgtitle('Change of height per sec - Tank')
sgtitle('Change of height per sec - Tank')
subplot(4,1,1)
plot(T_plot,H + H_edge,""LineWidth"",2); hold on;
plot(T_plot,Hd + H_edge,""--"",""LineWidth"",2);
legend(""H"",""Hd - reference"");
ylabel('H [m]'); xlabel('Time [s]')
xlim([t0 t_f]); grid;
% ylim([0 120]); grid;
subplot(4,1,2)
plot(T_plot,D1,""LineWidth"",2); hold on;
legend('D1')
ylabel('D1 [m]'); xlabel('Time [s]')
xlim([t0 t_f]); grid;
subplot(4,1,3)
plot(T_plot,error,""LineWidth"",2); hold on;
% plot(T_plot,kc,""LineWidth"",2);
plot(T_plot,intWindup,""LineWidth"",2);
ylabel('error [m]'); xlabel('Time [s]')
xlim([t0 t_f]); grid;
comment_1 = sprintf(""error - kp = %.6f"",kp);
comment_2 = sprintf(""Windup - ki = %.6f"",ki);
legend(comment_1,comment_2)
subplot(4,1,4)
plot(T_plot,Qout,""LineWidth"",2); hold on;
plot(T_plot,Qin,""LineWidth"",2);
ylabel('Q [m]'); xlabel('Time [s] (2 dagar)')
xlim([t0 t_f]); ylim([0 (max(Qin)+100)]); grid;
legend(""Qout [m^3/s] & D = 5m"", ""Qin [m^3/s] & D = 5m"")
% title(""Qin & Qout vs. time"") figure; % Change of height
%sgtitle('Change of height per sec - Tank')
sgtitle('PI controller - Tank')
subplot(2,1,1)
plot(T_plot,PI_P,""LineWidth"",2); 
ylabel('PI_P [m]'); xlabel('Time [s]')
xlim([t0 t_f]); grid;
subplot(2,1,2)
plot(T_plot,PI_I,""LineWidth"",2); 
ylabel('PI_I [m]'); xlabel('Time [s]')
xlim([t0 t_f]); grid; function Cg_vs_D = Cg_D(D,Dmin,Dmax) 
% Calculation of Cg variable of a ratial gate

n = 7; % Degree of the polynomial, got it from the interpolation process for Cg vs. D

if D < Dmin
    Cg_vs_D = 0.7743;
elseif D > Dmax
    Cg_vs_D = 0.6513;
else
    Cg_vs_D =   -5.1854385647666910937633559519621684330559219233692e-07*D^n ...
            + 2.3009388243133869431924706794312385227385675534606e-05*D^(n-1) ...
            - 0.00039326051774348001841691280233703764679376035928726*D^(n-2) ...
            + 0.0032245124407480371502010552120509601081721484661102*D^(n-3) ...
            - 0.012403237757656533982175695030036877142265439033508*D^(n-4) ...
            + 0.01661740981151977464280733443047211039811372756958*D^(n-5) ...
            - 0.012264797555097968484449921788836945779621601104736*D^(n-6) ...
            + 0.77434799435072565465532079542754217982292175292969;
end
end function u_sat = CheckSaturation(u,gateMax,gateMin)
% Controller saturation check

if u >= gateMax
    u_sat = gateMax;
elseif u <= gateMin
    u_sat = gateMin;
else
    u_sat = u;
end
end function val = intWindupBackCalculation(u,u_sat,k_lim)

diff = u - u_sat;
val = diff * k_lim;

end","['simulation', 'control-theory', 'matlab', 'discrete-mathematics']"
4667438,Finding the distance from a point a distance $z$ above the center of a square to any point on the edge,"I was working on an electrostatics problem that I thought I was doing correctly. However, upon reading the solution I see I was not. I will post my attempt and the solution below and then ask a few (math pertinent) questions about both my attempt and the solution provided. I would greatly appreciate any help in understanding this problem. Thanks! Problem Statement Find the electric field a distance $z$ above the center of a square loop (side length $a$ ) carrying uniform line charge $\lambda$ . My attempt Well, first I knew that the $x$ and $y$ components would cancel due to symmetry so that only leaves the $z$ component. For the $z$ component itself I noted that there are $4$ line segments of equal length (hence equal charge) and so we could calculate the electric field due to one segment then just multiply it by $4$ (since each contribution would be pointing in the positive $z$ direction). Doing so yielded $$E_z = \frac{1}{\pi\epsilon_0}\int_{-\frac{a}{2}}^{\frac{a}{2}} \frac{\lambda dx}{R^2}\sin\theta \hat{z} = \frac{1}{\pi\epsilon_0}\int_{-\frac{a}{2}}^{\frac{a}{2}} \frac{\lambda dx}{(z^2 + (\frac{a}{2}^2)}\frac{z}{\sqrt{(z^2 + (\frac{a}{2})^2}} \hat{z} = \frac{1}{\pi\epsilon_0}\int_{-\frac{a}{2}}^{\frac{a}{2}} \frac{\lambda z dx}{(z^2 + (\frac{a}{2})^2)^{\frac{3}{2}}} \hat{z} = \frac{\lambda z}{\pi\epsilon_0(z^2 + (\frac{a}{2})^2)^{\frac{3}{2}}}[x]\vert^{\frac{a}{2}}_{-\frac{a}{2}} = \frac{\lambda z a}{\pi\epsilon_0(z^2 + (\frac{a}{2})^2)^{\frac{3}{2}}}$$ Where the distance from the center of an edge to the point $P$ is given by $R = \sqrt{z^2+ (\frac{a}{2})^2}$ and I implicitly multiplied by $4$ from the start by writing $\frac{1}{\pi\epsilon_0}$ Provided Solution They note the distance from $P$ to the center of the edge is also $R = \sqrt{z^2+ (\frac{a}{2})^2}$ and that the electric field due to one edge is then $$E_1 = \frac{1}{4\pi\epsilon_0}\frac{\lambda a}{\sqrt{z^2 + \frac{a^2}{4}}\sqrt{z^2 + \frac{a^2}{4} + \frac{a^2}{4}}}$$ Hence the total vertical contribution to the electric field is found by multiplying by $4\sin\theta = 4\frac{z}{\sqrt{z^2 \frac{a^2}{4}}}$ which yields $$E = \frac{1}{4\pi\epsilon_0}\frac{4\lambda az}{(z^2 + \frac{a^2}{4})\sqrt{z^2 + \frac{a^2}{2}}}$$ My questions I would just like help seeing where they found the expression for the field contribution for one segment. In particular the expression for the distance between the point $P$ and the source charge. I feel like maybe it comes about from integrating with $x$ -dependence in the denominator. It feels a bit ""wrong"" to not have any $x$ -dependence in my expression for the distance since the distance will change as we integrate along the line charge. The only way I can think about incorporating this is: for every small increment moved on the $x$ axis, we would create a right triangle (in the xy plane) with opposite and adjacent sides defined by the distance from the center of the square in the plane (static), $\frac{a}{2}$ , and the distance we've moved along the $x$ -axis from $0$ (varying) such that the hypotenuse of that triangle in the xy-plane would then become the xy-component of the $3D$ triangle used to calculate the distance from the point $P$ to a point on the edge. But that seems quite cumbersome. Any help in understanding where their expression for the distance came from and/or how to account for varying distance as we move along the line charge will be greatly appreciated.","['physics', 'trigonometry', 'electromagnetism', 'vectors']"
4667442,"How many 6-digit integers greater than 321,000 can be formed such that each of the digits 1, 2, 3, 4, 5, and 6 is used once in each 6-digit integer?","How many 6-digit integers greater than 321,000 can be formed such that each of the digits 1, 2, 3, 4, 5, and 6 is used once in each 6-digit integer? Keep original(fix) the hundred thousands, ten thousands, and thousands place (all permutations of the hundreds,tens, and ones is $3!.$ Keep original(fix) hundred thousands, ten thousands, and the thousands place to two (all permutations of the hundreds,tens, and ones is $3!.$ Keep original(fix) the hundred thousands, ten thousands, but the thousands place to four (all permutations of the hundreds,tens, and ones is $3!.$ Keep original(fix) the hundred thousands, ten thousands, but the thousands place to five (all permutations of the hundreds,tens, and ones is $3!.$ Keep original(fix) the hundred thousands, ten thousands, but the thousands place to six (all permutations of the hundreds,tens, and ones is $3!.$ So far we have $ 5 \cdot 3!$ Keep original(fix) the hundred thousands, but ten thousands is $4$ , the thousands (all permutations of the hundreds,tens, and ones is $3!.$ Keep original(fix) the hundred thousands, but ten thousands is $5$ , the thousands (all permutations of the hundreds,tens, and ones is $3!.$ .. Keep original(fix) the hundred thousands, but ten thousands is $6$ , the thousands (all permutations of the hundreds,tens, and ones is $3!.$ .. Keep original(fix) the hundred thousands, but ten thousands is $4$ , the thousands (all permutations of the hundreds,tens, and ones is $3!.$ Keep original(fix) the hundred thousands, but ten thousands is $5$ , the thousands (all permutations of the hundreds,tens, and ones is $3!.$ .. Set the hundred thousands to four, fix the ten thousands, the thousands (all permutations of the hundreds,tens, and ones is $3!.$ .. Set the hundred thousands to five, fix the ten thousands, the thousands (all permutations of the hundreds,tens, and ones is $3!.$ .. Set the hundred thousands to six, fix the ten thousands, the thousands (all permutations of the hundreds, tens, and ones is $3!.$ .. This is a bad system. There are cases I'm not enumerating, and I don't have a system to enumerate them that allows me to multi-task and count multiple things at the same time. How to continue to capture all cases?",['combinatorics']
4667462,Neumann series of the derivative operator,"Consider the Nemuann series of the $D=\frac{d}{dx}$ operator: $T = 1 + D+ D^2+...$ Applying T on $x$ yields $x+1$ (i.e., $Tx=x+1$ ) for $x \in R$ . I would appreciate if someone helps me to resolve the following confusion. Since the sum above converges, we can rewrite $T$ as $\frac{1}{1-D}$ . Now, lets compute $Tx$ as follows: $Tx=(1 + D+ D^2+...)x=\frac{1}{1-D}x\equiv y \implies y - Dy = x \implies y-y^{'}=x,$ which is a first order differential equation with $y(x)=x+1-ce^{x}$ as the general solution, where $c$ is a constant. The question : why two different solutions? I understand that the first one is a specific case of the second general solution (with $c=0$ ). My confusion is that I was expecting to get one solution no matter which method I use. This may sound very basic but I cannot wrap my head around it. Thanks.","['operator-theory', 'taylor-expansion', 'ordinary-differential-equations']"
4667517,Is there a reason why $\frac{1}{7}$ as a decimal so perfectly seems to follow multiples of $7$?,"$\frac{1}{7} = 0.\overline{142857}$ So it's easy to learn. Double $7$ three times, then add $1$ to the end to set up for a repeat. $0.(2 \times 7 = 14)(2 \times 14 = 28)(2 \times 28 = 56 + 1 = 57)$ , $+1$ indicates it's time to repeat because we've reached $7$ and can start doubling again to get $14$ . Is this just a big coincidence?","['algebra-precalculus', 'arithmetic']"
4667534,question of partition in double integral,"in the definition of double integral,what the first to do is to divide region into small subrectangles . my question is :do these partitions have to be rectangles? for example ,when dealing with polar coordinates,region is divided into many small polar rectangles which are not rectangles.so can I use other shape to divide a region ,for exmple:parallelgram or triangle ,Are these also called  double integral? if these are also double integral why does the definition of double integral only use subrectangle for partition","['partitions-for-integration', 'multivariable-calculus', 'multiple-integral']"
4667542,Echelon basis for modular forms $M_{2}(\Gamma_{0}(23))$,"This is referring to Example 9.15 in William Stein's book 'Modular forms: a computational approach'. In this example, we are to calculate the newform of weight 2 level 23 in $S_{2}(\Gamma_{0}(23))$ . It starts by working out the Manin symbols $(0,0),(1,0),(0,1)$ and the matrix form of Hecke operator $T_{2}$ . We find that $T_{2}$ has an eigenspace (ker( $T_{2}-3$ )) spanned by the Eisenstein series of level 23 weight 2, and the other ( $V =$ ker( $T_{2}^{2}+T_{2}-1$ )) corresponds to $S_{2}(\Gamma_{0}(23))$ . Then the algorithm goes on to projecting onto $V$ , which get me confused when he says the following matrix $$
\begin{pmatrix}
0 & 0 & 1\\
1 & 0 & -2/11\\
0 & 1 & -3/11
\end{pmatrix}
$$ has the first two columns being the 'echelon basis for $V$ ' and last column being 'echelon basis for the Eisenstein subspace'. I have looked through the related sections in the book and I am still confused about where this comes from, and how we may calculate such a matrix for general levels (say $N = 11,37,etc$ ).","['number-theory', 'modular-function', 'modular-forms', 'algebraic-number-theory']"
4667549,What is the value of $\cos A$ given $\sin A$?,"If we have $\sin A = 0.5$ , and were asked to find $\cos A$ , then using the identity $\cos^2 A = 1 ‚Äì \sin^2 A$ we can do so. But this means that $\cos A = \pm \sqrt{1 ‚Äì \sin^2 A}$ . However this implies that there are exactly two cosine values we are looking for. Is this correct? Moreover, I found that it is usual that when I define a trigonometric equation, say $\sin A = 0.5$ , then according to my algebra, there are exactly 2 values for $\cos A$ , exactly 2 values for $\sin 2A$ , and exactly 4 values for $\sin \frac A 2$ . Is this correct? I ask because I have never been taught this. Thank you in advance.","['algebra-precalculus', 'trigonometry']"
4667576,Integral : from Summation to Integral notation,"There is a question I found it is a formula, a Riemann sum, where the sum is in expanded form.
And the function(s) are rational functions. If $n$ is a positive integer, then $$\lim_{n\to\infty} \frac1n \left(\frac1{1+\frac1n} + \frac1{1+\frac2n} + \cdots + \frac1{1+\frac nn}\right)$$ can be expressed as (A) $\displaystyle \int_0^1 \frac1x \, dx$ (B) $\displaystyle \int_1^2 \frac1{x+1}\,dx$ (C) $\displaystyle \int_1^2 x \, dx$ (D) $\displaystyle \int_1^2 \frac2{x+1}\,dx$ (E) $\displaystyle \int_1^2 \frac1x\,dx$ SO, in this I did what can be called 'clearing of fraction' on the inside square brackets.
and then multiplying in the $1/n$ , the resulting rational functions are of the form: $1/(n+i)$ where the $i$ is the running index $i =1,2,3,\ldots$ . At first when I evaluated this as $n\to\infty$ ,
Realized it summed in a way that was not a nice number, it looked like the logarithm of a number you can say based on the expression. I now think that this sum/integral may Diverge due to looking like a harmonic series, but that running index that I call $i$ and that $n$ is a positive integer, that eventually you take to infinity, is a key thing. But trying to take this and seeing it as a Riemann sum and hence grouping the limit and Sigma sign to get the integral representation is where I am having difficulty seeing this. Hope someone can help.","['integration', 'summation']"
4667589,Constrained variables in set-builder notation,"I have a set of variables $x_i \in \mathbb{R}$ that are subject to upper bounds $X_i$ such that $x_i \leq X_i $ for each $ i \in \{1, 2, ..., n\}$ . I want to write this in set-builder notation. I tried $\{x_i\ |\  x_i \leq X_i, i \in \{1, 2, ..., n\}\}$ but it does not seem right to include the $\{1, 2, ..., n\}$ set in the definition of the $x_i$ 's. What is the correct way to do it?","['elementary-set-theory', 'notation']"
4667705,Evaluate $\int_0^\pi \left(\frac{x}{1+x\sin x}\right)^2 \mathrm{d}x$,"I used series and substitution and Feynman trick but they didn't work. Do you have any ideas? $$\int_0^\pi \left(\frac{x}{1+x\sin x}\right)^2 \mathrm{d}x$$ I could simplify it like this: $$\overset{/x^2}{\rightarrow} \int_{0}^{\pi} \frac{1}{(\frac{1}{x}+\sin x)^2} \mathrm{d} x $$ and I used different substitutions but I failed. For example I supposed $x=\frac{1}{u}$ , $u=\tan(\frac{x}{2})$ .
Then I got stuck. And I tried these forms: $$I(a)=\int_{0}^{\pi} \frac{x^a}{(1+x\sin x)^2} \mathrm{d} x,\\
I(a)= \int_{0}^{\pi} \frac{x^2}{(1+x\sin ax)^2} \mathrm{d} x,$$ but they did not work.","['calculus', 'definite-integrals', 'trigonometry']"
4667761,Hartshorne Algebraic Geometry Exercise III.11.5 (On Picard groups of Formal Completions),"Let $\widehat{X}$ be the formal completion of $X=\mathbb{P}^N_k$ along a hypersurface for $N\geq 4$ . The exercise is to prove $\operatorname{Pic}(\widehat{X})\rightarrow \operatorname{Pic}(Y)$ is an isomorphism. The hint is to use Exercise II.9.6, Exercise III.4.6, and Exercise III.5.5. Let me explain the crux of each part that I understand so far. For II.9.6,  the main point of the application is that $\operatorname{Pic}(\widehat{X})\cong \varprojlim_n \operatorname{Pic}(\widehat{X})(X_n)$ where $X_n$ is the scheme $(\widehat{X},\mathcal{O}_{\widehat{X}}/\mathfrak{I}^n)$ where $\mathfrak{I}$ is the ideal of definition to this formal scheme. In other words, $X_n$ is precisely $(Y,\mathcal{O}_X/\mathscr{I}^n_Y)$ for $\mathscr{I}_Y$ the ideal sheaf of $Y$ .  The only thing one needs to check is that the inverse system of global sections groups is Mittag-Leffler and this should easy enough due to a Noetherian hypothesis. The next part is to study the maps $\operatorname{Pic}(X_{n+1})\rightarrow \operatorname{Pic}(X_n)$ . These induced maps clearly exist and these Picard groups can be identified with $H^1$ of their unit sheaves. This leads to the next hint -- I take a short exact sequence $$
0\rightarrow \mathscr{I}_Y^n/\mathscr{I}_Y^{n+1}\rightarrow \mathcal{O}_{X_{n+1}}^*\rightarrow \mathcal{O}_{X_{n}}^*\rightarrow 0
$$ which exists for each $n$ . More importantly, Exercise III.4.6 give rise to an exact sequence of abelian groups $$
\dots \rightarrow H^1(X,\mathscr{I}_Y^n/\mathscr{I}^{n+1}_Y)\rightarrow \operatorname{Pic}(X_{n+1})\rightarrow\operatorname{Pic}(X_{n})\rightarrow H^2(X,\mathscr{I}^{n}_Y/\mathscr{I}^{n+1}_Y)\rightarrow\dots 
$$ Now my thought would be that the last hint (Exercise III.5.5) should be applicable to this case to get some cohomology to vanish. However, the hypothesis that $N\geq 4$ is needed to show that $H^2(Y,\mathscr{I}^n,\mathscr{I}^{n+1})$ is always vanishing. In this case, we always have a surjective map $\operatorname{Pic}(X_{n+1})\rightarrow \operatorname{Pic}(X_n)$ for all $n\geq 1$ . And thereby I get a surjection for the desired map above. Crux of the Post / Question: How do we get injectivity? In general, I do not think we can get $H^1(\mathscr{I}^n/\mathscr{I}^{n+1})$ to vanish so I am stuck...","['algebraic-geometry', 'picard-group', 'commutative-algebra']"
4667801,Milne's definition of component group,"I am trying to understand the following definition from Milne's Algebraic Groups. Let $G$ be an algebraic group over $k$ and let $k^s$ be the separable closure of $k$ . Let $\pi_0(G)$ be the etale $k$ -scheme of connected components of $G$ and let $G\to \pi_0(G)$ be the canonical morphism of algebraic schemes. Taking the $k^s$ points of this morphism, one gets $G(k^s)\to \pi_0(G_{k^s})$ . Milne then claims that because the identity component $G^\circ$ is normal in $G$ , then there exists a unique group structure on $\pi_0(G_{k^s})$ such that the map $G(k^s)\to \pi_0(G_{k^s})$ is a group homomorphism respecting the Galois action, which would then make $\pi_0(G)$ an etale group scheme over $k$ . Does this mean that the map $G(k^s)\to \pi_0(G_{k^s})$ is a surjection?  And the fibres of this map are the connected components of $G(k^s)$ (topology induced from $G_{k^s}$ ) and $G^\circ (k^s)$ is the connected component of the identity in $G(k^s)$ ? If so, why are these true? I know that the map $G_{k^s}\to \pi_0(G_{k^s})$ is surjective, the fibres are the connected components of $G_{k^s}$ . I am not sure of the situation when $G_{k^s}$ is replaced by $G(k^s)$ . Could $G(k^s)$ be empty? For example, in the book of Qing Liu, existence of an element of $G(k^s)$ seems to require $G$ be geometrically reduced which is not generally satisfied by algebraic groups as defined by Milne.","['algebraic-geometry', 'algebraic-groups']"
4667820,Are the Dominating Families Of Functions (Domsets) Uncountable?,"Preface : For $f, g: \mathbb{Z}_{\geqslant 0} \rightarrow \mathbb{Z}_{\geqslant 0}$ , we write $f \leqslant^* g$ for $$
\exists m \in \mathbb{Z}_{\geqslant 0} \forall n \in \mathbb{Z}_{\geqslant m} f(n) \leqslant g(n) .
$$ Let us call a set $\mathcal{D}$ a domset if (i) all the elements of $\mathcal{D}$ are functions $\mathbb{Z}_{\geqslant 0} \rightarrow \mathbb{Z}_{\geqslant 0}$ ; and (ii) for every $f: \mathbb{Z}_{\geqslant 0} \rightarrow \mathbb{Z}_{\geqslant 0}$ , there exists $g \in \mathcal{D}$ such that $f \leqslant^* g$ . Question : Are all domsets uncountable? Now surely, the specific domset containing all functions $\mathbb{Z}_{\geqslant 0} \rightarrow \mathbb{Z}_{\geqslant 0}$ is uncountable since the symmetric group $\sum_{\mathbb{Z\geqslant 0}}$ is uncountable and is a subset of the set of all functions $\mathbb{Z}_{\geqslant 0} \rightarrow \mathbb{Z}_{\geqslant 0}$ . But I'm unsure if all domsets are uncountable. Any help is greatly appreciated!","['set-theory', 'functions', 'discrete-mathematics']"
4667901,Ash - Complex Variables - Sufficient conditions for the absolute and uniform convergence of the infinite product of functions,"I am self studying Ash & Novinger's Complex Variables. The authors prove the following theorem (see Page 4, Subsection 6.1.6) : Proposition. Let $g_1 , g_2, \ldots$ be a sequence of bounded complex valued functions, each defined on a  set $S$ . If the series $\sum_{n=1}^{\infty} \lvert g_n \rvert$ converges uniformly on $S$ ,
then the product $\prod_{n=1}^{\infty} (1+g_n)$ converges absolutely
and uniformly on $S$ . Furthermore, if $f(z)=\prod_{n=1}^{\infty}
 (1+g_n(z)), z\in S$ , then $f(z)=0$ iff $1+g_n(z)=0$ for some $n\in
 \mathbb N$ . Is the bounded condition redundant? The proof never makes use of it, in fact, uniform convergence of $\sum_{n=1}^{\infty} |g_n|$ implies the boundedness of $g_n$ 's after a certain stage. Here's how it can be show: Let $\sigma_n (x) = \sum_{k=1}^{n} |g_k(z)|$ . Since $\sigma_n$ converges uniformly, it is uniformly Cauchy ( see here ). Thus, there is some $K \in \mathbb N$ such that for every $m,n \ge K$ , we have that $|\sigma_{m} (x) - \sigma_{n} (x)| < 1$ . The result is now immediate by choosing $m=n+1$ for each $n \ge N$ . Since the first few terms cannot affect convergence, we may in fact get rid of them. So what is the point of boundedness here?","['complex-analysis', 'infinite-product', 'weierstrass-factorization']"
4667909,How do you build Intuition for trig identities?,"There are so many trigonometry identities and I understand how to apply them and even derive many of them. However, I never really have a clear understanding of what's going on. I wondered if most people take these identities for granted after they have proven them. Or how do you build intuition for dealing with these?",['trigonometry']
4667934,Tensorial Ito formula for the tangent space of a manifold,"Consider a compact Riemannian manifold $M$ with Riemannian structure given by a metric $g$ . Given a chart $(U, (x^i))$ and two vector fields $G_{\alpha}, G_{\beta} \in \Gamma(TM)$ it is possible to define the SDE $dX_t=G_\alpha(X_s) dt + G_\beta(X_s) dW_t$ where $W_t$ is a standard Brownian motion on the tangent space and $X_t \in TM$ .
All the references that i looked at use the Stratonovich stochastic integral in which the Ito formula doesn't contains the diffusion term. Is it true that for the Ito integral the ito formula is $f(X_t)=f(X_0)+ \int_0^t \left(G_\alpha+ \frac{1}{2} \nabla_{G_\beta}G_\beta \right) f(X_s) ds + \int_0^t G_\beta dW_t  $ If this is correct, given a frame $\left(\frac{\partial}{\partial x^i}\right)$ and in $U$ the formula above became $df(X_t)= \left(G_{\alpha}^i \frac{\partial}{\partial x^i} + G_{\beta}^iG_{\beta}^i\frac{\partial^2}{\partial x^i \partial x^i}\right)f(X_t)dt + G_{\beta}^i\frac{\partial}{\partial x^i}f(X_t) dW_t $ is there a way to make this formula tensorial while preserving the martigale property?","['stochastic-integrals', 'stochastic-processes', 'stochastic-differential-equations', 'stochastic-calculus', 'differential-geometry']"
4667974,"If we draw cards from a deck without replacement, how many cards would I have to draw on average until I obtain two kings","You have probably heard a similar problem from 50 challenging problems in probability: ""How many cards do I have to draw until I obtain an ace from a standard deck of playing cards?"" The answer for the above problem is 10.6, which can be obtain through manual calculation or using partitions. I wanted to extend this problem to drawing two cards of a set, and I was wondering how this could be done. My initial thoughts on the two approaches were: Brute force approach: $E[\text{two kings}] = P[\text{two kings in 2 draws}]\times2 + P[\text{two kings in 3 draws}]\times 3 + \dots + P[\text{two kings in 52 draws}]\times 52$ $E[X] = (\frac{4}{52} \frac{3}{51} 2) + {2 \choose 1} (\frac{4}{52}\frac{48}{51}\frac{3}{50}) + \dots + {52 \choose 1} (\frac{4}{52} \frac{48}{51} \dots \frac{1}{1})$ Partition Approach: $E[\text{two kings}] = E[\text{second king|first king}] + E[\text{first king}]$ And this is where I draw a blank, if I take the average value of the first king here, I get 10.6, and the partition approach no longer works. Any assistance on this would be great.","['expected-value', 'conditional-expectation', 'probability']"
4667980,"Directional derivative zero, minimum and convexity","Let $f:\mathbb R^n \to \mathbb R$ be convex, let $u\in \mathbb R^n$ , $v\in \mathbb R^n\setminus \{0\}$ and assume that the directional derivative of $f$ at $u$ in direction $v$ is $0$ . I'm wondering if this implies that $f$ has a global minimum at $x$ .
The function $g:\mathbb R\to \mathbb R, t\mapsto f(x+tv)$ is convex and its right-derivative at $0$ is zero, hence $g$ has a global minimum at $0$ , thus $f$ is minimized at $x$ when restricted along direction $v$ . I don't see a reason why $x$ should be a global minimizer. Can someone provide a counterexample ?","['convex-optimization', 'multivariable-calculus', 'convex-analysis', 'examples-counterexamples']"
4668011,"Calculate the side of rectangle using only length, no area or perimeter",My geometry is a little rusty due to not doing it for so long. I  am trying to find the side of a rectangle by only knowing one sides length. Is this possible? Length of one side is $0.21875$ . I have gone about this using trig by dividing the rectangle into half creating two right angle triangles with theta thus equaling 45 degrees. Is this the correct way of going about this? Answers I get are: $0.1350496052$ (Taking $0.21875$ as Opposite angle) and $0.354332582$ (Taking $0.21875$ as Adjacent angle) Calculations: $\tan(45) = \frac OA$ Thanks,"['trigonometry', 'geometry']"
4668019,Are Haar measures localizable?,"I'm trying to prove that Haar measures are localizable. we know that Haar measures are decomposable ( see Haar measures are decomposable ) in the sense that: A measure space $(X,\mathfrak{M},\mu)$ is decomposable if: (i) $X$ is a disjoint union of measurable subsets, $X=\bigcup_{i\in I}X_{i}$ , with $\mu(X_{i})<\infty$ for all $i$ . (ii) $\mu(E)=\sum_{i\in I}\mu(E\cap X_i)$ for every measurable set $E$ of finite measure. (iii) if $E\subseteq X$ and $E\cap X_i\in \frak{M}$ for all $i$ , then $E\in\frak{M}$ . if relation (ii) holds for every $E\in \frak{M}$ , then we called $(X,\frak{M},\mu)$ localizable. $\color{blue}{\textbf{can someone tell me that is my solution correct or not?}}$ $\color{green}{\textbf{my solution:}}$ $\color{red}{\textbf{the idea of proof is to prove that:}}$ $\color{red}{\bigg[}$ there exists a measure $\lambda$ on ${\frak{B}}_G$ that is radon and translation invariant and hence is a Haar measure on $G$ and by uniqueness of haar measure, we prove that the first Haar measure $\mu$ is equal with this new measure $\lambda$ and finally, the measure $\lambda$ have a property that localizability is proved. $\color{red}{\bigg]}$ if $G$ be a locally compact group  and $(G,{\frak{B}}_G,\mu)$ be Haar measure space where ${\frak{B}}_G$ is the borel $\sigma$ -algebra, then if we define a measure $\lambda$ on ${\frak{B}}_G$ such that: \begin{equation}
    \lambda(E)=
    \begin{cases}
      \underset{i\in I}{\sum}\mu(E\cap X_i) & \;\;\;\;\;\;\;\;\;\;\;\underset{i\in I}{\sum}\mu(E\cap X_i)<\infty \\
      \infty & \;\;\;\;\;\;\;\;\;\;\;\underset{i\in I}{\sum}\mu(E\cap X_i)=\infty
    \end{cases}
  \end{equation} $\color{red}{\textbf{proof that $\lambda$ is a translation invariant measure:}}$ if $\{E_j\}_{j\in\mathbb{N}}$ be a family of disjoint measurable sets, then if $\underset{i\in I}{\sum}\mu\bigg[\bigg(\underset{j\in\mathbb{N}}{\bigcup}E_j\bigg)\bigcap X_i\bigg]<\infty$ , then $\underset{i\in I}{\sum}\mu(E_j\cap X_i)<\infty$ for every $j\in\mathbb{N}$ and  we have $\lambda\bigg(\underset{j\in\mathbb{N}}{\bigcup}E_j\bigg)=\underset{i\in I}{\sum}\mu\bigg[\bigg(\underset{j\in\mathbb{N}}{\bigcup}E_j\bigg)\bigcap X_i\bigg]=\underset{i\in I}{\sum}\mu\bigg[\underset{j\in \mathbb{N}}{\bigcup}\bigg(E_j\bigcap X_i\bigg)\bigg]=\underset{i \in I}{\sum}\underset{j\in \mathbb{N}}{\sum}\mu(E_j\cap X_i)\color{red}=\underset{j\in \mathbb{N}}{\sum}\underset{i\in I}{\sum}\mu(E_j\cap X_i)=\underset{j\in\mathbb{N}}{\sum}\lambda(E_j)$ equality in red follows from following relation in measure theory. if we consider $\mu$ as counting measure and $X$ as arbitrary set. $$\int_X\underset{j\in\mathbb{N}}{\sum}f_j(x)d\mu=\underset{j\in \mathbb{N}}{\sum}\int_Xf_j(x)d\mu$$ now if $\underset{i\in I}{\sum}\mu\bigg[\bigg(\underset{j\in\mathbb{N}}{\bigcup}E_j\bigg)\bigcap X_i\bigg]=\infty$ , then we have $\underset{j\in\mathbb{N}}{\sum}\lambda(E_j)=\infty$ and $\lambda\bigg(\underset{j\in\mathbb{N}}{\bigcup}E_j\bigg)=\infty$ .
also it is trivial that $\mu(\phi)=0$ and is proved that $\lambda$ is really a measure on ${\frak{B}}_G$ . it is easy to see that $\lambda$ is translation invariant. we have to prove that the measure $\lambda$ is radon. $\color{red}{\textbf{proof that $\lambda$ is finite on compact sets:}}$ if $K\in {\frak{B}}_G$ is a compact set, then $\mu(K)<\infty$ and by property (ii) of decomposability, it follows that $\underset{i\in I}{\sum}\mu(K\cap X_i)=\mu(K)<\infty$ . hence $\lambda(K)=\underset{i\in I}{\sum}\mu(K\cap X_i)<\infty$ and $\lambda$ is finite on compact sets. $\color{red}{\textbf{proof that $\lambda$ is inner regular on sets of finite measure:}}$ if $E\in{\frak{B}}_G$ and $\lambda(E)<\infty$ and for arbitrary $\epsilon>0$ , then $\lambda(E)=\underset{i\in I}{\sum}\mu(E\cap X_i)$ and by definition, there exists a finite set $F\subseteq I$ suchthat $\underset{i\in I}{\sum}\mu(E\cap X_i)-\frac{\epsilon}{2}<\underset{i\in F}{\sum}\mu(E\cap X_i)$ now since $\mu\bigg[\underset{i\in F}{\bigcup}(E\cap X_i)\bigg]<\mu(E)<\infty$ , then there exists a compact set $K\subseteq \underset{i\in F}{\bigcup}(E\cap X_i)\subseteq E$ such that $$\mu\bigg[\underset{i\in F}{\bigcup}(E\cap X_i)\bigg]-\frac{\epsilon}{2}<\mu(K)$$ and it follows that: $\lambda(E)-\epsilon=\underset{i\in I}{\sum}\mu(E\cap X_i)-\epsilon<\underset{i\in F}{\sum}\mu(E\cap X_i)-\frac{\epsilon}{2}=\mu\bigg[\underset{i\in F}{\bigcup}(E\cap X_i)\bigg]-\frac{\epsilon}{2}<\mu(K)$ hence we have $$ \lambda(E)-\epsilon<\mu(K) $$ where $K\subseteq E$ . hence
inner regularity on sets with finite measure is proved. $\color{red}{\textbf{proof that $\lambda$ is outer regular}}$ if $E\in {\frak{B}}_G$ with $\lambda(E)=\infty$ . then since $\lambda$ is a measure and $E\subseteq G$ , then we have $\lambda(G)=\infty$ and since $G$ is open, then $\lambda$ is inner regular on sets with infinite measure. now if $E\in {\frak{B}}_G$ and $\lambda(E)<\infty$ and for arbitrary $\epsilon>0$ , since $\underset{i\in I}{\sum}\mu(E\cap X_i)=\lambda(E)<\infty$ , then $\mu(E\cap X_i)=0$ except for countable number  of $i\in \mathbb{N}\subseteq I$ . hence we have $$\lambda(E)=\underset{i\in I}{\sum}\mu(E\cap X_i)=\underset{i\in \mathbb{N}}{\sum}\mu(E\cap X_i)=\mu\bigg[E\bigcap\bigg(\underset{i\in \mathbb{N}}{\bigcup}X_i\bigg)\bigg]$$ now, there exists an open set $O_1\supseteq \bigg[E\bigcap\bigg(\underset{i\in \mathbb{N}}{\bigcup}X_i\bigg)\bigg]$ such that $\mu(O_1)<\mu\bigg[E\bigcap\bigg(\underset{i\in \mathbb{N}}{\bigcup}X_i\bigg)\bigg]+\epsilon$ and it follows that $$\mu(O_1)<\lambda(E)+\epsilon\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(1)$$ now, there exists a compct set $K\subseteq \bigg[E\bigcap\bigg(\underset{i\in I\setminus\mathbb{N}}{\bigcup}X_i\bigg)\bigg]$ such that $$\mu\bigg[E\bigcap\bigg(\underset{i\in I\setminus\mathbb{N}}{\bigcup}X_i\bigg)\bigg]<\frac{\epsilon}{2}+\mu(K)\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(2)$$ but in the case of Haar measure, the set $\{X_i\}_{i \in I}$ is equal with $\{gU'_n|g\in C, n\in\mathbb{N}\}$ and $H=\underset{n\in \mathbb{N}}{\bigcup}U'_n$ where $H$ is a open subgroup of $G$ and the set $C$ contains one poin from every coset $gH$ (for details, see Haar measures are decomposable ).
know if $K\subseteq G$ be a compact set, then since $K\subseteq \underset{g\in C}{\bigcup}gH$ and $\{gH|g\in C\}$ is an open cover of $K$ , then since $K$ is compact, there exists a finite set $g_1,...,g_n$ such that $K\subseteq \underset{i=1}{\overset{n}{\bigcup}}g_iH$ .
hence $K$ is subset of union of countable numbers of $X_i$ $\Bigg(K\subseteq \underset{i\in \mathbb{N'}}{\bigcup}X_i\Bigg)$ where $\mathbb{N'}$ is a countable subset of $I$ . now we have $$K\subseteq \bigg[E\bigcap\bigg(\underset{i\in I\setminus\mathbb{N}}{\bigcup}X_i\bigg)\bigg]\bigcap\bigg[\underset{i\in \mathbb{N'}}{\bigcup}X_i\bigg]=\bigg[E\bigcap\bigg(\underset{i\in\mathbb{N''}}{\bigcup}X_i\bigg)\bigg]$$ where $\mathbb{N''}\subseteq I\setminus\mathbb{N}$ is a countable subset of $I$ and we have $$\mu(K)\leq\mu\bigg[E\bigcap\bigg(\underset{i\in\mathbb{N''}}{\bigcup}X_i\bigg)\bigg]=\underset{i\in \mathbb{N''}}{\sum}\mu(E\cap X_i)=0$$ because we know that if $i\in I\setminus\mathbb{N}$ , then $\mu(E\cap X_i)=0$ . hence $\mu(K)=0$ and from formula $(2)$ , we have $\mu\bigg[E\bigcap\bigg(\underset{i\in I\setminus\mathbb{N}}{\bigcup}X_i\bigg)\bigg]<\frac{\epsilon}{2}+\mu(K)=\frac{\epsilon}{2}$ also there exists an open set $O_2\supseteq \bigg[E\bigcap\bigg(\underset{i\in I\setminus\mathbb{N}}{\bigcup}X_i\bigg)\bigg]$ such that $\mu(O_2)<\mu\bigg[E\bigcap\bigg(\underset{i\in I\setminus\mathbb{N}}{\bigcup}X_i\bigg)\bigg]+\frac{\epsilon}{2}$ hence we have $$\mu(O_2)<\mu\bigg[E\bigcap\bigg(\underset{i\in I\setminus\mathbb{N}}{\bigcup}X_i\bigg)\bigg]+\frac{\epsilon}{2}<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(3)$$ and by formula $(1)$ and $(3)$ , we have $$\mu(O_1\cup O_2)\leq\mu(O_1)+\mu(O_2)<[\lambda(E)+\epsilon]+\epsilon=\lambda(E)+2\epsilon$$ where $E\subseteq \bigg[E\bigcap\bigg(\underset{i\in \mathbb{N}}{\bigcup}X_i\bigg)\bigg]\bigcup \bigg[E\bigcap\bigg(\underset{i\in I\setminus\mathbb{N}}{\bigcup}X_i\bigg)\bigg]\subseteq O_1\cup O_2$ . but since $\lambda(E)<\infty$ , then $\mu(O_1\cup O_2)<\infty$ and by second decomposability property of Haar measure, we have $$\infty>\mu(O_1\cup O_2)=\underset{i\in I}{\sum}\mu\bigg[\bigg(O_1\cup O_2\bigg)\cap X_i\bigg]$$ hence by definition of $\lambda$ , it follows that $$\lambda(O_1\cup O_2)=\underset{i\in I}{\sum}\mu\bigg[\bigg(O_1\cup O_2\bigg)\cap X_i\bigg]=\mu(O_1\cup O_2)<\lambda(E)+2\epsilon$$ hence we have $$ \lambda(O_1\cup O_2)<\lambda(E)+2\epsilon $$ where $E\subseteq O_1\cup O_2$ . and is proved that $\lambda$ is outer regular. $\color{red}{\textbf{proof that $\mu$ is localizable}}$ now by uniqueness of Haar measure, we know that $\lambda=\mu$ and we have: \begin{equation}
    \mu(E)=\lambda(E)=
    \begin{cases}
      \underset{i\in I}{\sum}\mu(E\cap X_i) & \;\;\;\;\;\;\;\;\;\;\;\underset{i\in I}{\sum}\mu(E\cap X_i)<\infty \\
      \infty & \;\;\;\;\;\;\;\;\;\;\;\underset{i\in I}{\sum}\mu(E\cap X_i)=\infty
    \end{cases}
  \end{equation} and it follows that in any case we have $\mu(E)=\underset{i\in I}{\sum}\mu(E\cap X_i)$ and second decomposability property of Haar measure  holds for every $E\in {\frak{B}}_G$ and it follows that Haar measure $\mu$ is localizable.","['measure-theory', 'harmonic-analysis', 'topological-groups', 'haar-measure', 'locally-compact-groups']"
4668043,Intersection of tangent of parabola with directix and tangent at vertex,"I saw this question from Advanced Porblems in Coordinate Geometry by Vikas Gupta for JEE Advanced pertaining to Conic Section. If the line $x + y ‚àí1 = 0$ is a tangent to a parabola with focus $(1, 2)$ at $A$ and intersects the directrix at $B$ and tangent at vertex at $C$ respectively, then $AC, BC$ is equal to : We can solve it by forming an equation of parabola, finding slope etc. (takes about an A4 page) but that is not my concern here. When I checked upon the authors solution to see if my approach was correct, I was amazed!
This is how the he solved the problem. And then, the most beautiful I thing I have ever seen... $$BC*AC=(CS)^2$$ So my doubt is how was he so certain that the circle would pass exactly through the points $A, B$ and $S$ . Is it some sort of a property or an axiom or some weird result? I thought for a while, but nothing seems to strike. Any help would be appreciated. Many thanks!","['reference-works', 'coordinate-systems', 'conic-sections', 'geometry']"
4668045,What is the definition of a fiber of a vector bundle in algebraic geometry?,"I am learning the variations of Hodge structure, yet getting stuck at the very first beginning. Let $S$ be a projective nonsingular variety over $\mathbb{C}$ . I have seen that a variation of Hodge structure of weight $k$ on $S$ is the data A $\mathbb{Q}$ -local system $V$ , A decreasing separated exhaustive filtration $\{F^p\mathcal{V}\}_{p \in \mathbb{Z}}$ of holomorphic subbundles of $\mathcal{V} := V \otimes_{\underline{\mathbb{Q}}} \mathcal{O}_S$ such that for any $s \in S$ , the fiber $(V_s, F^pV_s)$ is a Hodge structure of weight $k$ and the canonical flat connection $\nabla$ satisfies the Griffith transversality. Question 1 : I am quite confused on the meaning of the "" fiber "" $(V_s, F^pV_s)$ . Could someone tell me the definition or provide a reference on what it means? As I understand it, a $\mathbb{Q}$ -local system $V$ is a locally constant sheaf of finite dimensional $\mathbb{Q}$ -vector space. (I guess the dimensions locally are constant since $S$ is irreducible? Am I right?) I have been searching, and guessing that it might be $V_s := \mathcal{V}_s \otimes_{\mathcal{O}_{S,s}} \kappa(s)$ , where $\kappa(s)$ is the residue field of $s \in S$ , and similar for $F^p \mathcal{V}$ . Yet trying to be a Hodge structure, it has to be a $\mathbb{Q}$ -vector space , but I cannot see why it is a $\mathbb{Q}$ -vector space. Moreover, Question 2 : what does the subbundle in the definition mean? And what does people mean by saying a ""holomorphic bundle""? As far as I can see, $\mathcal{V}$ is a locally free $\mathcal{O}_S$ -module of finite rank since locally it looks like $\mathbb{Q}^{\oplus N} \otimes_{\mathbb{Q}} \mathcal{O}_S \cong \mathcal{O}_S^{\oplus n}$ if we ignore the sheafification. (I am a little uncertain on this, since I have heard people saying that it is coherent, yet seldomly they say it is locally free.) So does the ""subbundle"" mean a sub- $\mathcal{O}_S$ -module that is still locally free? Thank you all for answering and commenting!  :)  Sorry for being so stupid.","['hodge-theory', 'algebraic-number-theory', 'algebraic-geometry', 'complex-geometry']"
4668051,Boundary conditions for weight function in weak form of an ODE or PDE,"I am really quite confused by how to pick a weight function's boundary conditions when evaluating the weak form of a differential equation. For example, given this ODE in the domain $0<x<1$ : $$u'''' + \frac{2}{x} u''' = 0$$ where $u(0) = u(1) = 0$ (Dirichlet BCs) and $u'(0) = u'(1) = 0$ (Neumann BCs), we pick a weight function $w(x)$ with the exact same Dirichlet and Neumann BCs , i.e. $w(0) = w(1) = w'(0) = w'(1) = 0$ , such that the boundary terms from subsequent IBP all vanish . However, when given this ODE in the domain $-1<x<1$ : $$xu''+2u'+2u = 0$$ with $u(1) = 0$ and $u(-1) = -2$ , we instead pick an arbitrary weight function such that $w(1) = w(-1) = 0$ ( not the same as the Dirichlet BCs of $u$ ) such that the boundary term from the subsequent IBP vanishes . Lastly, given Poisson's equation (a PDE): $$- \nabla^2u = f$$ with $$
\begin{aligned}
u=0 & \text { on } S_g, \\
\nabla u \cdot \boldsymbol{n}=h & \text { on } S_h,
\end{aligned}
$$ where $S_g$ and $S_h$ cover the entire boundary but do not overlap, the weight function we pick is 0 on $S_g$ (i.e. same Dirichlet BC but not the same Neumann BC ). This leaves us with the following weak form: $$
\int_V \nabla w \cdot \nabla u d V=\int_V w f d V+\int_{S_h} w h d S 
$$ where now the boundary term from IBP hasn't fully vanished (since we only picked $w=0$ on $S_g$ and not $S$ entirely). So I can't seem to figure out how to pick the boundary conditions of the weight function $w$ . E.g. in the case of the two ODEs I described, we pick the BCs of $w$ such that the boundary terms from IBP vanish (i.e. are equal to 0). Whereas with the PDE, the weight function we pick has boundary conditions such that the boundary term (surface integral) doesn't vanish. Is there a different approach to ODEs and PDEs? I would very much appreciate someone describing the general rules to follow with regards to the boundary conditions of $w$ .","['boundary-value-problem', 'ordinary-differential-equations', 'partial-differential-equations']"
4668076,Word and number ladder puzzles,"Introduction $
\begin{array}{}
\begin{array}{c|c|c}
\text{1} & \text{SIZE}\\ 
\hline
2 &  \\
3 &  \\
4 &  \\
5 &  \\
\hline
6 & \text{RANK}
\end{array}
&
\begin{array}{c|c|c}
1 & \text{SIZE}\\ 
\hline
2 & \text{SINE} \\
3 & \text{LINE} \\
4 & \text{LINK} \\
5 & \text{RINK} \\
\hline
6 & \text{RANK} 
\end{array}
\end{array}
$ On the left is an example of a Word Ladder puzzle, invented (allegedly) by Lewis Carroll. One possible solution is on the right. The aim is to place a valid word in each row that is the same as the one preceding it except for one letter change. There are four words to fill in, so there are five letter changes between the start and end: at least one  additional letter must be introduced (the L in line 3 of my example.) I got to wondering how many possible solutions a given puzzle may have. Puzzles vary in this because of the restriction that all words must be valid, but if we use numbers instead of words that restriction disappears. Furthermore, in the word problem all eight of the given letters are different, but in my digital version I want to allow duplicates. So I am attempting to calculate the number of ways that one $4$ -digit number may be changed into another, changing one digit at a time, such that all the $4$ -digit numbers are different, and five changes are made (so over six lines). Leading $0$ 's and duplicate digits are allowed. First case and a solution The easiest case to enumerate is where none of the digits in the final row are in the same position in the starting row (e.g. changing $1234$ into $5678$ or changing $1234$ into $4321$ ). Four of the five digit changes are ""fixed"" in that once a digit is placed in its correct position in the final row it can't be changed. The other change I call ""free"" in that it doesn't appear in that position in the final row, so any valid digit will do. Here is an example using numbers: \begin{array}{ll}
1234 & \text{ Start}\\
\hline
5234 & 5 \text{ is fixed}\\
5634 & 6 \text{ is fixed}\\
5639 & 9 \text{ is free}\\
5679 & 7 \text{ is fixed}\\
\hline
5678 & 8 \text{ is fixed}
\end{array} The four fixed digits can appear in any order, and so these lines contribute a factor of $4!$ to the total. Which leaves the line containing the free digit. The number of possibilities for this line depends on when it is introduced: as more of the fixed digits are put in position, there are fewer places for the free digit to go. Although all 10 digits are theoretically available, each line must be different to the one(s) preceding it (i.e. one digit must change each time), and the free digit must not be the same as the digit in the same position in the final line (or it wouldn't be free). So only eight digits are available. The possibilities of what can happen where are: \begin{array}{clcc}
\text{Line}^1 & \text{Positions available} & \text{Possible lines}\\
\hline
2 & 4 & 32\\
3 & 3 & 24\\
4 & 2 & 16\\
5 & 1 & \phantom{0}8
\end{array} $^1$ i.e. the line in which the free digit is introduced. So the line containing the free digit can appear in $32+24+16+8 = 80$ ways.
Putting all this together, there are $24 \cdot 80 = 1920$ possible ways to complete each game. Other cases The next case is where one of the target digits is already in position, e.g. changing $1234$ into $1678$ . There are two cases here: i) force the target digit already in position to change. Another line then has to be used up to change it back again, so there is room for only one free digit. ii) do not change the target digit already in position. The problem becomes one of changing the other three digits over five lines, so two free digits are required. There are $\binom42 = 6$ ways in which they can be introduced (the final change must be fixed, not free). This gives six sub-cases of varying complexity. As above, the number of possibilities depends on when the free digits come in. I shan't take up time and space describing them here. The remaining cases (changing $1234$ into $1278$ (up to three free digits) and changing $1234$ into $1238$ (up to four free digits)) have increasing complexity. My question Is there an easier way to enumerate all this? Maybe consider the whole problem in one go rather than considering all these cases? Or a different way to divide into cases that makes for easier computation?","['recreational-mathematics', 'puzzle', 'combinatorics']"
4668105,Spearman's rho for tied ranks,"I know how to prove that Spearman's $ \rho / r_s$ can be written as $r_s = 1-\frac{6\sum\limits_{i=1}^nd_i^2}{n(n^2-1)}$ with $d_i=R(x_i)-R(y_i)$ when there are no ties. Now I came across the formula for tied ranks: $r_s = 1-\frac{6\left(\sum\limits_{i=1}^nd_i^2+\sum\limits_{j=1}^n\frac{t_j^3-t_j}{12}\right)}{n(n^2-1)}$ where $t_j$ represents the $j^{th}$ tie length. (I found this formula at the following link: https://www.onlinemath4all.com/spearman-rank-correlation-coefficient.html ) Another version of this formula would be $r_s = \frac{\frac{n^3-n}{6}-\sum d_i^2 - \sum T_x -\sum T_y}{\sqrt{(\frac{n^3-n}{6}-2\sum T_x)(\frac{n^3-n}{6}-2\sum T_y)}}$ with $\sum T_x = \frac{\sum t_i^3-t_i}{12}$ and $\sum T_y = \frac{\sum t_i^3-t_i}{12}$ where $t_i$ is the number of groups the random variables $X$ and $Y$ tie respectively. (I found this formula at the following link: http://webspace.ship.edu/pgmarr/Geo441/Lectures/Lec%2011%20-%20Spearman%27s%20and%20Cramer%27s%20Correlation.pdf ) I'm unable to prove either of both formulas for tied ranks. I tried to rewrite $R(x_i)$ in a different way but always failed to obtain the formula.
My question therefore is if anyone has a hint on how to prove any of the two formulas or maybe if someone has a complete proof? Thank you very much in advance for any help.","['statistics', 'correlation']"
4668157,$F_X \cong F_Y \Rightarrow |X| = |Y|$ where is the mistake in this proof,"Statement.
Let $F_X, F_Y$ free groups over $X,Y$ respectively. Suppose there is an isomorphism $\phi: F_X \cong F_Y$ . Then $|X|= |Y|$ . My proof. Let $x \in F_X$ be a word of length one, this is, $x = t_1$ . Let $\phi(x) = r_1\cdots r_m \in F_Y$ . Then $t_1 = \phi^{-1}(r_1)\cdots \phi^{-1}(r_m)$ . If $m>1$ , there exists $i$ such that $\phi^{-1}(r_i)\phi^{-1}(r_{i+1})=e$ , but then $r_ir_{i+1}=e$ , in contradiction with $\phi(x)$ being reduced. So $m = 1$ . From here is easy to see that $|X \cup X^{-1}| = |Y \cup Y^{-1}|$ and then $|X| = |Y|$ . However, it seems like the proof for the statement is more complicated than this, as in free groups: $F_X\cong F_Y\Rightarrow|X|=|Y|$ . So I suppose that my proof is wrong, but I don't know where.","['group-theory', 'solution-verification', 'free-groups', 'group-isomorphism']"
4668158,"If $a_{n+1}=a_n-a_n^2$, determine the convergence of $\sum a_n$","Given $a_1 = 0.5 $ and $ a_{n+1} = a_n - a_n^2,$ determine the convergence of $ \sum_{n=1}^{\infty} a_n $ . I have found out that $\lim_{n \to \infty} a_n = 0$ , so the necessary condition holds, and d'Alembert cannot help me. If I try Raabe criterion, I need something about $a_n$ asymptotic, but I could not find it.","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4668161,"Bounding the large semicircular contour in integrals like $\oint\frac{e^{iz}}{z}\, dz$ [duplicate]","This question already has answers here : convergence of $\int_\gamma \frac{e^{iz}}{z}dz$ (1 answer) How to derive the following estimate from example 3.9 in Bruce Palka's textbook: $|\int_\beta\frac{e^{iz}}{z}\,dz| \leq \frac{\pi(1-e^{-r})}{r}$ (2 answers) Closed last year . This post was edited and submitted for review last year and failed to reopen the post: Original close reason(s) were not resolved Considering the large semicircular arc $\Gamma$ in the upper half plane of $\oint{\frac{e^{iz}}{z}\,dz}$ using the Estimation Lemma with $z=Re^{i\theta}, \theta\in[0,\pi]$ means : $$\sup\left|\frac{e^{iRe^{i\theta}}}{Re^{i\theta}}\right|= \sup\frac{|e^{iR(\cos\theta+i\sin\theta)}|}{R}=\sup\frac{|e^{-R\sin\theta}|}{R}$$ How can the numerator be simplified? In the range $\sin\theta$ can be positive, and as $R\to\infty$ the original semicircle wouldn‚Äôt vanish. If the integral inequality $\left|\int_{a}^{b}{f(x)\,dx}\right|\leq\int_{a}^{b}{|f(x)|\, dx}$ is used you can double the integral and use $\frac{\pi}{2}$ as the bound instead, and then use Jordan‚Äôs inequality. But the Estimation lemma follows from the integral inequality, so it must still be possible to bound the expression. EDIT : Yes, Jordan's Lemma can be used, but I was more interested in how this extends to the definition of the Estimation Lemma and the absolute value (given it does not contain an integral, but rather an upper bound)","['complex-analysis', 'contour-integration']"
4668208,Expanding Arcus Tangens as Taylor Series around 1: Symmetries,"Expanding $\arctan$ around 0 as a Taylor series yields $$
\arctan x = \sum_{n=0}^\infty (-1)^n\frac{x^{2n+1}}{2n+1}
$$ for $x$ 's that are in the region of convergence.  All even terms vanish, which is due to the symmetry $\arctan (-x)=-\arctan x$ , i.e. $\arctan$ is an odd function. Now expanding around 1 (with radius of convergence $\sqrt2$ ) yields something like: $$\begin{align}
\arctan(x+1) - \arctan(1) =&  \frac12x - \frac1{4}x^2 + \frac1{12}x^3 - \frac1{40}x^5 + \frac1{48}x^6 - \frac1{112}x^7 + \frac1{288}x^9 - \frac1{320}x^{10}\\
 & {} + \frac1{704}x^{11} - \frac1{1664}x^{13} + \frac1{1792}x^{14} - 
\frac1{3840}x^{15} + \frac1{8704}x^{17} - \frac1{9216}x^{18} + \frac1{19456}x^{19} \cdots
\end{align}$$ As you can see, every 4-th term vanishes, i.e. powers of $x$ with exponents of $0\bmod4$ do vanish. Questions : What does this tell us about symmetries of $\arctan$ or any other function that shows such behaviour? Is there a straight forward way to see that every 4th term does actually vanish without carriying out fancy computations? $\arctan$ satisfies $\arctan x+ \arctan(1/x)=\pi/2$ , but I do not see how (or whether) this  symmetry has something to do with the question.","['complex-analysis', 'functional-equations', 'power-series', 'symmetry']"
4668263,Problem with the differential equation $2u_{xx}-3u_{xy}+u_{yy}+u_x-u_y=1$,"Specify the largest domain in which the given Cauchy problem has a single solution, and find this solution $$2u_{xx}-3u_{xy}+u_{yy}+u_x-u_y=1, \; u\Bigg|_{x=0,y>0}=-2y, \; u_x\Bigg|_{x=0,y>0}=-1$$ Note that a linear hyperbolic equation of second order is given. To
find the domain in which the Cauchy problem has a single solution, we
first find the characteristic equations $$2y'^2-3y+1=0\Rightarrow y'=\left \{ \frac{1}{2},1 \right \}$$ If $y' = 1/2$ we get $y = x/2 + C_1$ , then at $x = 0$ , $y = C_1$ , and $u =-2y = -2C_1$ . Hence our characteristic passes through the point $(0, C_1)$ ,
and $u = -2C_1$ . Given $y' = 1$ we obtain $y = x + C_2$ , then at $x = 0$ , $y = C_2$ , and $u_x = -1$ . Hence, our characteristic passes through point $(0, C_2)$ , and $u_x = -1$ . Substitute the variables $Œæ = x - y$ and $Œ∑ = x/2 + y$ , then $$
\begin{aligned}
\frac{\partial}{\partial x} & =\frac{\partial \xi}{\partial x} \frac{\partial}{\partial \xi}+\frac{\partial \eta}{\partial x} \frac{\partial}{\partial \eta}=\frac{\partial}{\partial \xi}+\frac{1}{2} \frac{\partial}{\partial \eta} \\
\frac{\partial}{\partial y} & =\frac{\partial \xi}{\partial y} \frac{\partial}{\partial \xi}+\frac{\partial \eta}{\partial y} \frac{\partial}{\partial \eta}=-\frac{\partial}{\partial \xi}+\frac{\partial}{\partial \eta}
\end{aligned}
$$ $$
\begin{aligned}
2 u_{x x}-3 u_{x y}+u_{y y}=2 & \left(\frac{\partial^2}{\partial \xi^2}+\frac{\partial^2}{\partial \eta^2}\right) u-3\left(\frac{\partial^2}{\partial \xi^2}-\frac{\partial^2}{\partial \eta^2}\right) u+\left(\frac{\partial^2}{\partial \xi^2}+\frac{\partial^2}{\partial \eta^2}\right) u \Rightarrow \\
& \Rightarrow u_{\xi \eta}=\frac{1}{2} \Rightarrow u_{\xi}=\frac{1}{2} \eta+f(\xi) \Rightarrow u_{\xi \eta}=\frac{1}{4} \xi \eta+F(\xi)+G(\eta)
\end{aligned}
$$ \begin{gathered}
\frac{1}{4} y-2 y-G(y)=-1 \Rightarrow G(y)=-\frac{7}{4} y+1 \Rightarrow F(-y)=-2 y-\left(-\frac{7}{4} y+1\right)=\frac{1}{4} y-1 \Rightarrow \\
\Rightarrow u_{\xi \eta}=\frac{1}{4} \xi \eta+\frac{1}{4} \xi-1-\frac{7}{4} \eta+1=\frac{1}{4} \xi \eta+\frac{1}{4} \xi-\frac{7}{4} \eta \Rightarrow \\
\Rightarrow u(x, y)=\frac{1}{4}(x-y)\left(\frac{1}{2} x+y\right)+\frac{1}{4}(x-y)-\frac{7}{4}\left(\frac{1}{2} x+y\right)=\frac{x^2}{8}+\frac{x y}{8}-\frac{5 x}{8}-\frac{y^2}{4}-2 y
\end{gathered} The problem is that I substitute $x=0$ and don't get $-2y$ . I have solved the problem incorrectly. Can you tell me how to solve it correctly?","['cauchy-problem', 'ordinary-differential-equations', 'partial-differential-equations']"
4668284,"Do we know when $supp(\mu_y)= h^{-1}(\{y\})$, for a regular conditional conditional probability $\mu$ with regard to $h$?","The situation:
Let $X$ and $Y$ are Polish spaces equipped with Borel probability measures $\mu$ and $\nu$ such that $\nu=h_*\mu$ is the push forward measure with regard to a continuous (measurable) function $h:X\to Y$ . It is commonly known that in this case $\mu_y(\cdot)=\mu(\cdot\vert h=y)$ is a regular conditional probability with regard to $h$ . So $$
\int_A \mu_y(\cdot)d\nu=\mu(\cdot\cap h^{-1}(A))
$$ It is also known that for the fiber measures $\mu_y(h^{-1}(\{y\}))=1$ .
Assume that we also know the support of the fiber measures $supp(\mu_y)$ is finitely bounded( $\#supp(\mu_y)<n$ ). Obviously $\mu_y$ is the sum of weighted dirac measures in this case and $supp(\mu_y)\subseteq h^{-1}(\{y\})$ . Are there any conditions on anything such that we can force $supp(\mu_y)= h^{-1}(\{y\})$ $\nu$ -almost surely?
Do we have any additional properties of the fiber measures?","['conditional-probability', 'measure-theory']"
4668297,"What does it mean for a tiling (in particular, one involving the recently discovered ""Hat"" monotile) to be ""aperiodic""?","In these articles ""Mathematicians Excited About New 13-Sided Shape Called 'the Hat'"" (Gizmodo) , ""An 'einstein' tile? Mathematicians discover pattern that never repeats"" (Interesting Engineering) , from the paper An aperiodic monotile (arXiv) from Smith, et al., they make a claim Researchers identified a shape that was previously only theoretical: a 13-sided configuration called ‚Äúthe hat‚Äù that can tile a surface without repeating."" If we look at the image, we can see at least three places where the pattern repeats What do they mean exactly when they say that? Is it that they can repeat but not touch or something else? What exactly are they proving to show this given we can see repeating patterns?","['tessellations', 'geometry', 'tiling']"
4668390,Some questions on the backwards heat equation,"Let $\Delta f = f''$ on $\mathbb R$ ; we know the semigroup $(P_t)_{t \geq 0} = (e^{t\Delta})_{t \geq 0}$ acts on functions $f \in L^2(\mathbb R)$ via the heat kernel $$ u(x, t) = P_tf(x) = \int_{\mathbb R} \frac{1}{(4\pi t)^{1/2}}e^{-|x-y|^2/(4t)}f(y)dy. $$ This makes sense though for any function $f$ growing slower than any Gaussian, e.g. if $f(x) = e^x$ then $P_tf(x) = e^te^x$ , and if $f(x) = x^2$ then $P_tf(x) = x^2 + 2t$ . If I prescribe data at, say, $t = 1$ , then I can solve backwards in time: if $u(x, 1) = P_1f(x) = e^x$ or $u(x, 1) = x^2$ then respectively the solution is $u(x, 0) = f(x) = e^{-t}e^x$ or $u(x, 0) = x^2 - 2$ . Even if $u$ has very rapid growth at infinity, e.g. $u(x, 1) = e^{\alpha x^2}$ with $\alpha > 0$ , we can solve to find $f(x) = \sqrt{1+4\alpha}e^{\alpha x^2/(1 + 4\alpha)}$ . Formally, I know the solution should be $$e^{-\Delta}u(x, 1) = \sum_{k \geq 0} \frac{(-1)^k}{k!}\Delta^k u(x, 1)$$ which, e.g. for polynomials, is a finite sum and, e.g. for exponentials (and probably the Gaussian too), is a convergent infinite sum. My question is: is there a class of functions (to which these non $L^2(\mathbb R)$ examples belong) for which this inversion/backwards heat equation makes sense and has nice estimates? For instance, if $u \in L^2(\gamma)$ where $\gamma$ is the standard Gaussian measure, we can write $u$ as an infinite sum of $u_n = \sum_{i=0}^\infty a_iH_i$ of Hermite polynomials (being an orthonormal base for $L^2(\gamma)$ ), and with enough decay on the $(a_i)$ the formal sum $f_n = e^{-\Delta}u_n$ converges to some $f \in L^2(\gamma)$ and it has the expected behaviour (that is, $P_1f = u$ ). But in doing so I lose (or do not know how to recover) statements about smoothness (which I expect is important since the heat kernel is smoothing) or even continuity. I have an inkling this question is also related to the nature of the error term (based off a paper [""A well posed problem for the backward heat equation"" by Miranker) in the expansion $$e^{-\Delta}u = u - \Delta u + \frac{1}{2}\Delta^2 u - \cdots$$ and I suspect the answer is that the question is ""well-posed"" if $u(x, 1) \leq Ce^{\alpha x}$ has at most exponential growth based on some simple examples for which the error $|e^{-\Delta u} - u| \leq C(1 + |\Delta u|)$ is roughly controlled by the first term. But I do not know if this problem is no longer well-posed if I introduce some pathological behaviour, e.g. by adding a fast oscillation like $u(x, 1) = e^x + x^2\sin^2(x^2)$ and if I need some extra conditions to prohibit this.","['ordinary-differential-equations', 'heat-equation', 'real-analysis', 'functional-analysis', 'partial-differential-equations']"
4668428,"To prove Stokes's theorem, why does it suffice to prove it for $\mathbb{R}^n$ and for $\mathcal{H}^n$?","In Tu's An Introduction to Manifolds , he states Stokes's theorem as: For any smooth $(n-1)$ -form $\omega$ with compact support on the oriented $n$ -dimensional manifold $M$ , $$\int_M d\omega = \int_{\partial M}\omega.$$ In his proof he chooses an atlas $\{(U_\alpha, \phi_\alpha)\}$ for $M$ in which each $U_\alpha$ is diffeomorphic to either $\mathbb{R}^n$ or $\mathcal{H}^n$ via an orientation-preserving diffeomorphism. He then says Suppose Stokes's theorem holds for $\mathbb{R}^n$ and for $\mathcal{H}^n$ . Then it holds for all the charts $U_\alpha$ in our atlas, which are diffeomorphic to $\mathbb{R}^n$ or $\mathcal{H}^n$ . I am having trouble seeing how Stokes's theorem applying to $\mathbb{R}^n$ and $\mathcal{H}^n$ implies it also holds for all the charts $U_\alpha$ . Is he using the diffeomorphism and a pull back to make some sort of change of variables that preserves the integral?","['differential-forms', 'stokes-theorem', 'smooth-manifolds', 'differential-geometry']"
4668495,How to solve this least-squares-like problem?,"Given the finite set of matrices $\Bbb X := \{ {\bf X}_1, {\bf X}_2, \dots, {\bf X}_N \} \subset \mathbb{C}^{N_t \times L}$ and the matrix ${\bf Y} \in \mathbb{C}^{N_r \times L}$ , $$ \left( \hat{H}, \hat{X} \right) = \arg \min_{{\bf H} \in \mathbb{C}^{N_r \times N_t}, \\ {\bf X}\in\mathbb{X}} \| {\bf Y} - {\bf H} {\bf X} \|_{\text{F}}^2 $$ where $N_r$ and $N_t$ denote the number of antennas at the receiver and at the transmitter, respectively, and $L$ denotes the total number of time slots. I am trying to solve the optimization problem above in MATLAB.
I do not understand how we proceed. Any help will be highly appreciated.","['matrices', 'optimization', 'least-squares', 'maximum-likelihood']"
4668501,Prove that differential equations are right continuous with respect to initial values.,"Here the question. Consider the initial value problem: \begin{equation}
\begin{cases} \frac{dy}{dx}=f(x,y) \\y(x_0)=y_0,
\end{cases}
\end{equation} $f(x,y)$ is continuous. Suppose that $y=\phi(x;x_0,y_0)$ is the maximum solution of the initial value problem. Prove: $\phi(x;x_0,y_0)$ is right continuous for $y$ , that is \begin{equation}
\lim_{y_1\to y_0^+}\phi(x;x_0,y_1)=\phi(x;x_0,y_0)
\end{equation} establishs on $|x-x_0|\leq \alpha$ , $\alpha$ is constant. Below is my idea. Consider the equations \begin{equation}
(*)_n=\begin{cases}\frac{dy}{dx}=f(x,y)+\frac{1}{n}, \\y(x_0)=y_0,
\end{cases}
\end{equation} write the sequence of solutions as $\{\phi_n(x;x_0,y_0)\}$ .It has a uniformly convergent subsequence,let itself be uniformly convergent. Assume that $\phi(x;x_0,y_0)$ is not right continuous for $y_0$ , then $\exists \epsilon>0,\forall \delta>0,\exists y_{\delta}-y_0<\delta$ , such that $$|\phi(x;x_0,y_{\delta})-\phi(x;x_0,y_0)|\geq  \epsilon.$$ Let $\{\delta_n\}$ decreases to $0$ ,The corresponding $y_{\delta}$ is denoted by $y_n$ . According to the uniform convergence, $\exists N>0,\forall n>N$ , we have \begin{equation}
|\phi_n(x;x_0,y_0)-\phi(x;x_0,y_0)|<\epsilon.
\end{equation} Then we can get \begin{equation}
|\phi(x;x_0,y_{n})-\phi(x;x_0,y_0)|\leq|\phi(x;x_0,y_{n})-\phi_n(x;x_0,y_{0})|+|\phi_n(x;x_0,y_{0})-\phi(x;x_0,y_0)|.
\end{equation} The latter item can be controlled by $\epsilon$ ,but I don't know how to handle the previous item.Do you have a good idea? I really appreciate it!","['initial-value-problems', 'ordinary-differential-equations']"
4668507,"Prove indirectly: $\forall$ $ùë•, ùë¶ \in \mathbb{Z}$, if $ùë•^3 + ùë¶^3$ is even, then $ùë• + ùë¶$ is even.","I'm stuck after negating the statement to there exists $x,y \in\mathbb{Z}$ , $ùë•^3 + ùë¶^3$ is even and $ùë• + ùë¶$ is odd.","['proof-writing', 'discrete-mathematics']"
4668524,Show that the MLE of $\alpha$ is consistent by definition.,"Suppose that $(X_1,\dots, X_n)$ is an iid random sample from $X\sim f(x;\alpha, \beta)$ and $$
f(x;\alpha, \beta)=\frac{\alpha x^{\alpha-1}}{\beta^{\alpha}}, \, 0<x\le \beta, \alpha>0, \beta>0,
$$ Show that the MLE of $\alpha$ is consistent by definition. My work:
Note that the log-likelihood: $$
\ell(\alpha, \beta)=n\log\alpha-n\alpha\log\beta+\sum_{i=1}^n \log x_i^{\alpha-1} I[1<X_{(1)}<X_{(n)}\le \beta]
$$ where $X_{(1)}\le X_{(2)}\le \dots \le X_{(n)}$ . Then the MLE of $\beta$ is $$
\hat{\beta}=X_{(n)}.
$$ By the invariant of MLE and solving $\frac{\partial \ell(\alpha, X_{(n)})}{\partial \alpha}=0$ , the MLE of $\alpha$ is $$
\hat{\alpha}=\frac{1}{\log X_{(n)}-\frac{1}{n}\sum \log X_i}
$$ By Weak law of large number, we have $$
\frac{1}{n}\sum \log X_i \to E[\log X_1]=\log \beta-\frac{1}{\alpha}.
$$ Note that for every $\epsilon>0$ , $$
P(|X_{(n)}-\beta|>\epsilon)=P(\beta-X_{(n)}>\epsilon)=\frac{(\beta-\epsilon)^{n\alpha}}{\beta^{n\alpha}}\to 0
$$ as $n\to \infty$ . (Because $\frac{\beta-\epsilon}{\beta}<1$ ) Hence, $\hat{\beta}=X_{(n)}\to \beta$ in probability. Hence, by continuous mapping theorem $$
\hat{\alpha}=\frac{1}{\log X_{(n)}-\frac{1}{n}\sum \log X_i}\to \alpha
$$ Is my proof right?","['statistics', 'probability']"
4668532,How to find an explicit formula for this function?,"Let us take $$
\mathbb{N} := \{ 1, 2, 3, \ldots \},
$$ and let the function $f \colon \mathbb{N} \longrightarrow \mathbb{N} \times \mathbb{N}$ have the following values: $$
\begin{align} 
& f(1) := (1, 1), \\ 
& f(2) := (1, 2), f(3) := (2, 1), \\
& f(4) := (1, 3), f(5) := (2, 2), f(6) := (3, 1), \\
& f(7) := (1, 4), f(8) := (2, 3), f(9) := (3, 2), f(10) := (4, 1), \\
& f(11) := (1, 5), f(12) := (2, 4), f(13) := (3, 3), f(14) := (4, 2), f(15) := (5, 1), \\
& \ldots. 
\end{align}
$$ Apparently, this function $f$ is bijective. How to find an explicit formula for this function? How to rigorously show that this function is bijective? I know that the inverse function $f^{-1} \colon \mathbb{N} \times \mathbb{N} \longrightarrow \mathbb{N}$ is given by the formula $$
f^{-1} (m, n) := \frac{ (m+n -2 ) (m + n -1 ) }{2} + m. 
$$","['elementary-set-theory', 'functions', 'derivation-of-formulae']"
4668538,What is the distinction between variable and number in these two expressions?,"The author here discriminates between the two functions as a is a fixed number, and x is a variable. I am confused as to this distinction, as it seems the reason why we would need to use 'a' instead of a number is to account for many different possibilities of 'a', such that 'a' is essentially a variable. I would appreciate an explanation as to the difference between these two, and when something expressing the meaning of 'a' would be used as opposed to something with the meaning of 'x'.
thanks",['algebra-precalculus']
4668568,"A vector function with ""nearly"" identity derivative","I encountered the following exercise in calculus course: Let $U$ be a convex open subset of $\mathbb{R}^n$ and $f \colon U \to \mathbb{R}^n$ differentiable. Suppose that $\lVert Df(\mathbf{x}) - I_n \rVert < 1$ for all $\mathbf{x} \in U$ where $I_n \colon \mathbb{R}^n \to \mathbb{R}^n$ is an identity map. Show that $f$ is injective. Furthermore, show that $f(U)$ is open. $\lVert \cdot\rVert$ is an operator norm, i.e. $\lVert A \rVert = \max_{x\in S}\lVert Ax\rVert$ where $S$ is an unit sphere. I think there is a convexity condition to show that $f$ is injective. If $f(a) = f(b)$ for some $a,b \in \mathbb{R}^n$ , then consider a path $c(t) = ta + (1-t)b$ and do something with this. Since only condition given is that $\lVert Df(\mathbf{x}) - I_n \rVert < 1$ , I tried to find some point such that $Df(\mathbf{x})$ is ""far away"" from $I_n$ . I have a vague idea to use MVT, or Rolle's theorem, but I am seriously stuck. How can I use those conditions to prove injectivity of $f$ and that $f(U)$ is open? Thanks in advance!","['calculus', 'derivatives', 'vector-analysis']"
4668629,Solving $\int_0^4 \frac{\left(y^2-4 y+5\right) \sin (y-2)}{2 y^2-8 y+11} d y$,"$$\int_0^4 \frac{\left(y^2-4 y+5\right) \sin (y-2)}{2 y^2-8 y+11} d y$$ Here is my attempt $$
\begin{aligned}
& y-2=t \Rightarrow d y=d t \\
& y^2-4 y+4=t^2 \Rightarrow 2 t^2=2 y^2-8 y+8 \\
& {y^2-4 y+5=t^2+1} \\
& 2 y^2-8 y+11=2 t^2+3 \\
& 2 y^2-8 y+10+1=2 t^3+3 \\
& 2\left(y^2-4 y+5\right)+1=2 t^3+3 . \\
& {\int_0^4 \frac{2\left(t^2+1\right) \sin t}{2\left(t^2+1\right)+1} d t} \\
& \Rightarrow \int_0^4\frac{\sin t}{\left.1+\frac{1}{2\left(t^2+1\right)}\right)} d t
\end{aligned}
$$ No other substitution seems to work. Any help or quickies on this one would be appreciated. Thanks","['integration', 'calculus', 'definite-integrals']"
4668662,Change the order of this double integration,"Suppose I want to determine: $$
\int_{0}^{1}\int_{x}^{e^{x}}2y-x\;dydx
$$ In this order, i.e. integrating with respect to y then x, it is straight forward enough to evaluate to: $$
\int_{0}^{1}\int_{x}^{e^{x}}2y-x\;dydx = \frac{e^{2}-3}{2}\quad\text{(*)}
$$ However, looking at the domain of integration, I think it should be able to be evaluated by integrating with respect to x first then y last. I'm aware that this would mean changing the limits and I think it should become: $$
\int_{0}^{e}\int_{ln|y|}^{y}2y-x\:dxdy\quad\text{(**)}
$$ However, after evaluating $\text{(**)}$ , it does not equal to $\text{(*)}$ . Can someone please explain to me, if I was to change the order of integration to become $dxdy$ , what should be the limits of the integration be? Thank you. Edit: I would like to add a diagram of the domain region, which is the region bounded by the blue, green, red, and orange curves. Perhaps @Davis Yoshida is implying that because ln(y) isn't define to be all positive, we have to split the region and add them together? However, this isn't what I want. If it is possible, can the order of the integration be swap to become dxdy without splitting?","['integration', 'multivariable-calculus', 'multiple-integral', 'definite-integrals']"
4668675,Show that the function is strictly increasing.,"How should I show that the function $f$ defined below is strictly increasing for $x\in(0,1)$ ? I have considered its first derivative, but it seems too complicated to deduce $f'>0$ from there. $f(x)=\frac{3x^2+4x(1-x)+4(1-x)^2}{4x^2}\ln^2{(1-x)}$ $f'(x)=\dfrac{\ln\left(1-x\right)\left(3x^3+\left(2\ln\left(1-x\right)-4\right)x^2+\left(4-6\ln\left(1-x\right)\right)x+4\ln\left(1-x\right)\right)}{2\left(x-1\right)x^3}$","['limits', 'calculus', 'functions', 'algebra-precalculus']"
4668684,Struggling with infinite series convergence tests,"I was working on some series calculus questions and am struggling with this particular one: This is what I answered: CG ‚Äì $\int_2^\infty(\frac{1}{n\ln(n)})dn=\infty$ , so it diverges according to Integral Test. Also, it isn't a $p$ -series but it is close to one so can be compared with $p$ -series. BE ‚Äì This one is a $p$ -series, I know that it converges since its limit $=0$ , and the graph of $y=\frac{4}{x^{\pi}}$ confirms this. BD ‚Äì This one is an alternating series. Also, $\lim_{n\rightarrow\infty}(\frac{1}{n\pi})=0$ , the graph of $y=\frac{1}{x\pi}$ shows that it is positive and decreasing for $x\geq 1$ . So, it converges conditionally according to Alternating Series Test. AE ‚Äì This one is a $p$ -series where $p=1+\frac{1}{2}=\frac{3}{2}>1$ , so it converges. Since it isn't an alternating series, I stated that it absolutely converges. CG ‚Äì For this one, I assumed it is similar to number (1), since it is comparable to a $p$ -series. And similarly to (1), it diverges according to the Integral Test. The system shows that at least one answer is incorrect (but not which one). Can I please get some help on this?","['calculus', 'convergence-divergence', 'sequences-and-series']"
4668707,"An alternative lower bound for $\prod_{ i,j = 1}^n\frac{1+a_ia_j}{1-a_ia_j} $","In Prove that $\prod_{1\leq i,j\leq n}\frac{1+a_ia_j}{1-a_ia_j}\geq1$ for $n$ real numbers $a_i\in(-1,1)$ it was shown that for real numbers $a_1, \ldots, a_n \in (-1, 1)$ we always have $$
 P = \prod_{ i,j = 1}^n\frac{1+a_ia_j}{1-a_ia_j} \ge 1 \, .
$$ The proof (originally from AoPS ) uses the Taylor series of the logarithm to derive an explicit formula for the logarithm of that product as an infinite sum of squares with positive coefficients: $$ \tag{*}
 \ln P = 2 \sum_{k=1}^\infty \frac{1}{2k-1} \left( \sum_{i=1}^n a_i^{2k-1}\right)^2 \ge 0 \, .
$$ If we omit all terms for $k \ge 2$ on the right-hand side then we get the weaker inequality $$ \tag 1
 \ln P \ge 2 ( a_1 + a_2 + \cdots + a_n)^2 
$$ or equivalently $$ \tag 2
 P \ge e^{2( a_1 + a_2 + \cdots + a_n)^2 }
$$ My question: Is there a simpler/more direct way to obtain $(1)$ or $(2)$ without the use of infinite series? We cannot use $$
 \ln(1+a_ia_j) - \ln(1-a_ia_j) \ge 2 a_i a_j
$$ because that holds only if $a_ia_j \ge 0$ . Another idea is to consider the function $$
 f(x) = \prod_{ i,j = 1}^n\frac{1+x^2a_ia_j}{1-x^2a_ia_j} \, .
$$ for $0 \le x \le 1$ . From the representation $(*)$ we know that $\ln f(x)$ is increasing in $x$ , but it is not obvious (to me) how to prove that directly, since the terms in $$
 \frac{d}{dx} \ln f(x) = \sum_{i, j=1}^n \frac{4a_i a_j x}{1-a_i^2a_j^2 x^4}
$$ can be both positive and negative.","['alternative-proof', 'inequality', 'real-analysis']"
4668709,"Show that $A=\{(x,y)\in \mathbb{R}^{2}:y>x^2\}$ is open in $\mathbb{R}^{2}$ with the usual metric.","I want to prove that $A=\{(x,y)\in \mathbb{R}^{2}:y>x^2\}=\{(x,y)\in \mathbb{R}^{2}:y-x^2>0\}$ . We then want to see that there exists $r>0$ such that $B((x_{0}, y_{0}); r) \subseteq \{(x,y)\in \mathbb{R}^{2}:y>x^2\}$ . let $(a, b)\in B((x_{0}, y_{0}); r)$ then $\sqrt{ (a-x_{0})^2+(b-y_{0})^2} <r$ Someone told me it might work to take $r=\frac{1}{n}(y_{0}-x_{0}^2)$ Where clearly $r>0$ , should really prove that $b>a^{2}$ . But I don't see it really clear, any suggestions?","['general-topology', 'metric-spaces', 'real-analysis']"
4668763,Calculating a lower bound of Lebesgue measure of some Borel set,"I have the following set. I need to calculate a lower bound on its measure in order to prove something about some distribution. $I \subseteq [0,1)$ . Take the set $\mathcal{J}(I) = I \bigcap \underset{\underset{(p,q)=1}{Q/2\leq q\leq Q} }\bigcup B(\frac{p}{q},\frac{1}{4q^{2}})$ . I wish to prove that there is a constant $C>0$ s.t for every interval $I\subseteq [0,1)$ there is some $Q\geq Q_{0}$ where $C\lambda(I) Q^{2}\leq \lambda(\mathcal{J}(I))$ . The question stems from a course I take on Diophantine approximations, so maybe tools like Dirichlet approximation theorem can also be of some assistance.","['measure-theory', 'lebesgue-measure', 'diophantine-approximation', 'borel-measures']"
4668766,A special line bundle on a product $X\times Y$ is the pullback of a line bundle on the Albanese variety $\mathrm{Alb}(X\times Y)$,"Let $X$ , $Y$ be smooth complex projective varieties and $L$ a line bundle on $X\times Y$ . Assume that $L|_{X\times \{y\}}\in \mathrm{Pic}^{0}(X)$ for any closed point $y\in Y$ and $L|_{\{x\}\times Y}\in \mathrm{Pic}^{0}(Y)$ for any closed point $x\in X$ . How to show there is a line bundle $M$ on $\mathrm{Alb}(X\times Y)$ such that $L=\mathrm{alb}_{X\times Y}^{*} M?$ (For a smooth proper $\mathbb{C}$ -variety $Z$ , $\mathrm{alb}_{Z}$ denotes the Albanese map $Z\rightarrow \mathrm{Alb}(Z)=(\mathrm{Pic}_{Z}^0)^{\vee}$ .) By $L|_{X\times\{y\}}\in \mathrm{Pic}^0(X)$ , I know that $L$ is the pullback of the poincare line bundle on $X\times \mathrm{Pic}_{X}^0$ . Similarly, $L$ is the pullback of the poincare line bundle on $\mathrm{Pic}_{Y}^0\times Y$ .","['complex-geometry', 'algebraic-geometry', 'birational-geometry']"
4668778,Logical approach of a simple combinatorics problem [duplicate],"This question already has answers here : Consider all one-to-one and onto functions $f:\{1,2,3,4\} \rightarrow \{1,2,3,4\}$ which satisfy: if $f(k)$ is odd then $f(k+1)$ is even, k = 1, 2, 3 (2 answers) Closed last year . Consider all functions f:{1,2,3,4}‚Üí{1,2,3,4} which are one-one, onto
and satisfy the following property: if f(k)is odd then f(k+1)is even,
k =1,2,3. Then, number of such functions So this is how I solved the problem If o denoted odd and e denotes even then,
the possibilities of the range of f(1),f(2),f(3),f(4) respectively based on the specified conditions including k=4 will be o e o e } First o which is for f(1) can take either 1 or 3 and the first e which is for f(2) can take 2 or 4. So we totally have $2*^2C_1$ selections = 4 e o e o } Similarly 4 combinations here if f(1) starts with even value o e e o } Similarly 4 here too Therefore $4*3=12$ Possibilities.
My math textbook says 12 as well. But is my approach correct?","['functions', 'combinatorics']"
4668806,Is the exponential map of a complex unital Banach algebra surjective onto the set of (two-sidedly) invertible elements?,"Let $A$ be a complex unital Banach algebra, and let $A^\times$ denote the set of (two-sidedly) invertible elements of $A$ . Let $\exp : A \rightarrow A^\times$ denote the exponential map. My question is, in general is the exponential map surjective onto $A^\times$ ? What I understand so far is as follows: If we replace ""complex"" with ""real"", then the answer becomes no. E.g. we can take $A$ to be $\mathbb{R}$ itself, or more generally the matrix algebra $\mathrm{Mat}_n(\mathbb{R})$ for any integer $n>0$ . (reference) The answer becomes ""yes"" in the case $A = \mathrm{Mat}_n(\mathbb{C})$ , for integer $n>0$ . (reference) However I'm not sure how to generalize the 2nd bullet point, nor how to find a counter-example. Would anyone have any suggestions on how to think about this?","['banach-algebras', 'functional-analysis']"
4668808,What are the procedures to analyze the similarity of graphs?,This question is taken from a very challenging calculus problems book called the Advanced Problems in Mathematics by Vikas Gupta And I have absolutely no clue on how to approach questions of the kind. My initial thought since y is composite function was to check for a graph similar to the outermost function which is that of tan. Now option D looked compelling. But I was incorrect. Then I decided to substitute values like say $x=\pi/4$ but that would yield $\tan(1/{\sqrt{2}})$ which is hard to compute. So please do advise me on how to approach this question efficiently. Thanks,"['trigonometry', 'functions', 'graphing-functions', 'function-and-relation-composition']"
4668809,"Integrals of Jacobi $\vartheta$ functions on the interval $[1,+\infty)$","I start from the following obvious observation, which is declared to be( $q=e^{-\pi x}$ ): \begin{aligned}
\int_{1}^{\infty}x\vartheta_2(q)^4\vartheta_4(q)^4
\text{d}x&=\int_{0}^{1}x\vartheta_2(q)^4\vartheta_4(q)^4
\text{d}x\\
&=\frac12\int_{0}^{\infty}x\vartheta_2(q)^4\vartheta_4(q)^4
\text{d}x=\frac14,
\end{aligned} in which the functional equation $\vartheta_2(e^{-\pi/x})=\sqrt{x}\vartheta_4(e^{-\pi x})$ is used. The last equality is derived from $$
\int_{0}^{\infty}x^{s-1}\vartheta_2(q)^4\vartheta_4(q)^4\text{d}x
=16\frac{\Gamma(s)}{\pi^s}\lambda(s)\eta(s-3).
$$ Then, I am confronted with the new one $$
\int_{1}^{\infty}\theta_2(q)^4\theta_4(q)^4
\text{d}x=\frac{2G}{\pi^2}.
$$ These two integrals have cousins widely as well. For example, some experiments suggest \begin{aligned}
&\int_{1}^{\infty}(x^2-1)\vartheta_2
(q)^4\vartheta_3(q)^4
=\frac{8G}{\pi^2}-\frac{14}{\pi^3}\zeta(3),\\
&\int_{1}^{\infty}(1+x)^2
\vartheta_2(q)^2\vartheta_3(q)^4\vartheta_4(q)^4\text{d}x
=\frac{\Gamma\left ( \frac14 \right)^8}{8\pi^6},
\end{aligned} the integral $$
\int_{1}^{\infty}
\left [ (x-1)\vartheta_2(q)^4\vartheta_3(q)^2
+2\sqrt{2}x\vartheta_2(q)^3\vartheta_4(q)^3  \right ]\text{d}x
=2-\frac{8G}{\pi^2}
$$ included. By substituting $x=K^\prime(k)/K(k)$ , we obtain \begin{aligned}
&\int_{0}^{\frac{1}{\sqrt{2} } }
K(k)\left ( K(k)+K^\prime(k) \right )^2\text{d}k
=\frac{\Gamma\left ( \frac14 \right )^8 }{128\pi^2},\\
&\int_{\frac{1}{\sqrt{2} } }^{1} 
\frac{K(k)^2-K^\prime(k)^2}{k} \text{d}k
=\pi G-\frac{7}{4}\zeta(3),\\
&\int_{\frac{1}{\sqrt{2} } }^{1} 
\left(\frac{K(k)-K^\prime(k)}{k}+\frac{2\sqrt{2}\sqrt{k}K(k)  }{(1-k^2)^{1/4}}  \right)\text{d}k
=\frac{\pi^2}{2}-2G.
\end{aligned} Question. Is there any ways being able to prove the integrals listed above? The 2nd Question. I wonder if it exists similar ones with the interval $[\sqrt{n},+\infty)$ , where $n\in\mathbb{Q}_{>0}$ .","['integration', 'dirichlet-series', 'calculus', 'theta-functions', 'elliptic-integrals']"
4668840,"Given increasing sequence of numbers, what is guaranteed min length the longest subseq. s.t. differences of terms are either decreasing or increasing?","It would be better if I could fit ""differences of consecutive terms"" in the title, but I ran out of space. Anyway, here is a more precise version of my question: Given $n,$ for any given distinct real numbers $\ x_1,\ x_2,\ \ldots,\ x_n\ $ such that $\ x_1 < x_2 < \ldots < x_n,\ $ what is the guaranteed minimum length, $\ k,\ $ of the longest (ordered, i.e. increasing) subsequence $\ \left( x_{n_1},\ x_{n_2},\ \ldots,\ x_{n_k} \right)\ $ of $\ (x_1,\ x_2,\ldots,\ x_n),\ $ such that either $\ x_{n_i} - x_{n_{i-1}} \leq x_{n_{i+1}} - x_{n_i}\quad \forall\ 2\leq i\leq k-1,\quad $ or $\quad x_{n_i} - x_{n_{i-1}} \geq x_{n_{i+1}} - x_{n_i}\quad \forall\ 2\leq i\leq k-1?$ For example, if we have $ 2,5,6,9,10,12$ , then the longest subsequence with the desired property has length $4$ . But does every length- $6$ sequence have at least $k=4?$ If so then $k(n=6) = 4.$ The question is, what is the function $k(n)$ like in general?","['pigeonhole-principle', 'real-analysis', 'ramsey-theory', 'sequences-and-series', 'inequality']"
4668861,Single objective optimization with $n$ independent variables,"I have a single objective optimization problem.
The problem can consist of $n$ independent variables $(x_1,x_2,...,x_n)$ four functions $f_i(\bf{x})$ and a function $Q(\bf{x})$ . I want to monitor how a composite function $F$ , composed of four individual functions $f_1(x_1,x_2\ldots x_n),f_2(x_1,x_2,\ldots,x_n),f_3(x_1,x_2\ldots,x_n),f_4(x_1,x_2,\ldots,x_n)$ , tracks a the rate of change (gradient) of the function $Q(x_1,x_2,\ldots,x_n)$ for $x_n\in[\alpha,\beta]$ with $\alpha,\beta\in\mathbb{R}$ . Œô will probably use the weighted sum method to compose $F$ so $F$ will be in the form $F=a_1f_1+a_2f_2+a_3f_3+a_4f_4$ with $\displaystyle \sum_ia_i=1$ to sweep through different combinations of weights. My question is how would I compare the rates of change between $F$ and $Q$ ?
If I had a single independent variable, I would probably create $J = Q'-F'$ and I would try to minimize it. I thought something along the lines of $J = \nabla Q -\nabla F$ but would this hold? Is there any general approach in these kinds of problems?","['nonlinear-optimization', 'multivariable-calculus', 'calculus', 'vector-analysis', 'optimization']"
4668891,If $ùí´(A) ‚à™ ùí´(B) = ùí´(A ‚à™ B)$ then $A ‚äÜ B$ or $B ‚äÜ A$. Help needed.,"I was trying to prove the following: For any sets $A$ and $B$ , if $ùí´(A) ‚à™ ùí´(B) = ùí´(A ‚à™ B)$ then $A ‚äÜ B$ or $B ‚äÜ A$ . My proof: Let $A$ and $B$ be arbitrary sets. Suppose $ùí´(A) ‚à™ ùí´(B) = ùí´(A ‚à™ B)$ . Assume, by way of contradiction, that $(¬¨(A ‚äÜ B) ‚àß ¬¨(B ‚äÜ A))$ . Then there are elements $y$ and $x$ , such that $x ‚àà A ‚àß x ‚àâ B$ and $y ‚àà B ‚àß y ‚àâ A$ . Notice $x ‚àà A ‚à™ B$ , and $y ‚àà A ‚à™ B$ . Let $W = \lbrace x,y \rbrace$ . Then $W ‚äÜ A ‚à™ B$ . So $W ‚àà ùí´(A ‚à™ B)$ . Hence $W ‚àà ùí´(A) ‚à™ ùí´(B)$ . Assume $W ‚àà ùí´(A)$ . Then $W ‚äÜ A$ , so $x,y ‚àà A$ . But $y ‚àâ A$ , so $W ‚àâ ùí´(A)$ . Then $W ‚àà ùí´(B)$ , but then $x,y ‚àà B$ , in particular $x ‚àà B$ . But that's a contradiction, so $W ‚àâ ùí´(b)$ . Thus, we have $W ‚àà ùí´(A) ‚à™ ùí´(B)$ and $W ‚àâ ùí´(A)$ and $W ‚àâ ùí´(B)$ . Therefore, our assumption that $(¬¨(A ‚äÜ B) ‚àß ¬¨(B ‚äÜ A))$ must be wrong. Ergo, $A ‚äÜ B$ or $B ‚äÜ A$ . I am suspicious of the step where I create a set $W = \lbrace x,y\rbrace$ . It seems a bit odd, to be able to create a set out of the blue, especially since $W$ is not arbitrary, it includes $x$ and $y$ . While I can see that $x$ and $y$ must exist, I don't see why they must be in the same set. Is this proof correct?","['proof-explanation', 'proof-writing', 'solution-verification', 'discrete-mathematics', 'elementary-set-theory']"
4668907,"When is $\int_0^\infty dk \int_0^\infty dq \int_0^R dt \,f(k,q,t,r)\stackrel{?}{=}g(r)$ where both $f$ and $g$ are known functions","I would like to know under which circumstances the following triple integral can be evaluated analytically as $$
\int_{k=0}^{k=\infty} \int_{q=0}^{q=\infty} \int_{t=0}^{t=R} f(k,q,t,r) \,\mathrm{d}t \,\mathrm{d}q \,\mathrm{d}k \stackrel{?}{=} g(r)  
\qquad\qquad (0 < t,r < R) \, , 
$$ where the two function $f$ and $g$ are given explicitely by $$
f(k,q,t,r) = \frac{4qk}{\pi \alpha^4} \, \left( Q-q\right) \left( K+k \right)  \left( e^{-k} - e^{-K} \right)  \sin(qt) \sin(kt) J_1(qr)  \, , 
$$ and $$
g(r) = \frac{r}{s^3} \left( \frac{6}{\alpha^2 s^2}  -2e^{-\alpha s} \left( 1 + \frac{3}{\alpha s} + \frac{3}{\alpha^2 s^2} \right) \right) \, .
$$ Here, we have defined for the sake of convenience the abbreviations $K = \sqrt{k^2+\alpha^2}$ , $Q = \sqrt{q^2+\alpha^2}$ , and $s=\sqrt{1+r^2}$ .
Here, $\alpha$ is a positive real number. Please note that $t \in [0,R]$ and that $k,q \in [0,\infty)$ . A numerical evaluation shows that the result is surprisingly accurate up to a very small additive constant that I do not succeed to evaluate exactly yet. As $R\to\infty$ , it can be checked that the identity is exact. How I proceeded is to use inverse Hankel transform wrt the variable $q$ and inverse sine Fourier transform wrt the variable $k$ .
Even though I know that $$
\int_0^\infty r g(r) J_1(qr) \, \mathrm{d}r =
\frac{2q}{\alpha^2} \left(  e^{-q} - e^{-Q}\right) \, ,
$$ I was unable to rigorously prove that. In particular, in the limit $\alpha \to 0$ , this can easily be shown upon using the identity $$
\int_0^\infty \sin(qt) J_1(qr) \, \mathrm{d}q
= \frac{t H(r-t)}{r \left( r^2-t^2\right)^\frac{1}{2}} \, .
$$ Any insight is highly appreciated. Thank you.","['integration', 'fourier-analysis', 'fourier-transform', 'calculus', 'integral-transforms']"
4668934,Solving a tricky equation $4x^3-5x^2-5 = 0$,How does one solve $4x^3-5x^2-5 = 0$ ? I've tried the substitution $y = x - \frac{5}{12}$ but then I ended up with this monster of an equation: $864y^3 - 450y -1205 = 0$ . Now I'm stuck. Any help would be greatly appreciated.,"['algebra-precalculus', 'roots-of-cubics']"
4668957,Functions that are coercive along every line,"A function $f:\mathbb{R}^n\to\mathbb{R}$ is called coercive if $$
\lim_{\|x\|\to\infty}f(x)=\infty.
$$ To show that $f$ is coercive, we need to prove that for every sequence $\{x_n\}$ with $\|x_n\|\to\infty$ , it holds that $f(x_n)\to\infty$ . My question: Is it sufficient to check that $f$ is coercive along every line? That is, does the condition $$
\lim_{t\to\infty}f(x+td)=\infty,\quad\forall\,x\in\mathbb{R}^n,\quad\forall\,d\in\mathbb{R}^n\backslash\{0\}
$$ imply that $f$ is coercive? If not, what additional assumptions (e.g., convexity and/or $C^1$ ) can we impose on $f$ to make this true?","['coercive', 'multivariable-calculus', 'convex-analysis', 'real-analysis']"
4668965,"Hints on showing that the sequence $\frac{1}{\#S_n}\sum_{l\in S_n}\mu\circ f^{-l},S_n\subset\mathbb{N}$ converges to $f$-invariant probability measure","Let $X$ be a metric space and $\mu$ be any probability measure on $X$ . Suppose that $f:X\to X$ is a continuous $\mu$ -measurable function and that $(S_n)_n$ is a sequence of subsets of the natural numbers $\mathbb{N}$ such that $\# S_n\to\infty,n\to\infty$ for the counting measure $\#$ . Define the map $\alpha_n(B) := \frac{1}{\# S_n}\sum_{l\in S_n}\mu(f^{-l}(B))$ for $B\subset X$ . It is then not hard to see that $\alpha_n$ is a probability measure on $X$ . Suppose then that $(n_k)_k$ is a subsequence of indices such that the subsequence $(\alpha_{n_k})_k$ converges in weak* topology to a probability measure $\alpha$ over $X$ , i.e. for any bounded continuous function $g:X\to\mathbb{R}$ we have $$\lim_{k\to\infty}\left|\int_Xg(y)d\alpha_{n_k}(y) - \int_X g(y)d\alpha(y)\right| = 0$$ where $$\int_Xg(y)d\alpha_{n_k}(y) = \frac{1}{\# S_{n_k}}\sum_{l\in S_{n_k}}\int_X(g\circ f^l)(y)d\mu(y)$$ as one can verify easily. ( Extra assumption: ) Take it as granted that if $f:X\to X$ is a continuous map then its pushforward map $f_*\mu(B) := \mu(f^{-1}(B)), B\subset X$ is continuous in weak* topology. I.e. if $\mu_n\to\mu$ in weak* as $n\to\infty$ then $\lim_{n\to\infty}f_*(\mu_n) = f_*\mu$ . ( Problem: ) I want to show that the prior limit measure $\alpha$ is $f$ -invariant, that for any $\alpha$ measurable subset $B\subset X$ we have $\alpha(B) = \alpha(f^{-1}(B))$ . ( Solution attempt/thoughts: ) Let $g:X\to \mathbb{R}$ be a fixed bounded and continuous function. By continuity of $f_*$ , $$f_*\alpha = f_*\left(\lim_{k\to\infty}\alpha_{n_k}\right) = \lim_{k\to\infty}\frac{1}{\# S_{n_k}}\sum_{l\in S_{n_k}}\mu\circ f^{-l - 1}$$ Therefore I would want to conclude that the difference $$I := \left|\frac{1}{\# S_{n_k}}\sum_{l\in S_{n_k}}\int_X(g\circ f^{l})(y) - (g\circ f^{l+1})(y)d\mu(y)\right|$$ can be made arbitrarily small after some sufficiently large $k$ . But since nothing guarantees that $l\in S_n\implies l+1\in S_n$ I have no idea how to relate these two quantities of the integrand. One could use triangle inequality and the fact that $\alpha_{n_k}$ s are probability measures to show that $$\left|\int_X(g\circ f^{l}) - (g\circ f^{l+1})(y)(y)d\mu(y)\right|\leq 2\sup_{y\in X}\left|g(y)\right|$$ for any $l\in S_n$ . But without additional information you would get this same bound for every summand occurring in $I$ and so the averaging is lost. What to do? I am not necessarily looking for a full-blown answer, but I'll appreciate any help/hints that you might be able to give. Thanks!","['measure-theory', 'weak-convergence', 'probability-distributions', 'weak-topology', 'probability-theory']"
4669015,Why is span defined as the linear combination with *finite* terms?,"I understand that, for a vector space $V$ with scalar field $K$ , the linear span of a family of vectors $S\subseteq V$ is usually defined as $$ \mathrm{sp}(S):=\left\{ \sum_{i=1}^k a_iv_i \mid k\in \mathbb{N},  a_i \in K, v_i\in S\right\} $$ In words: the span is the set of finite linear combinations of the vectors in $S$ . My question is about the motivation for defining the span to be a finite linear combination. Is there some uncomfortable conclusions  that would result from using a definition like $$ \mathrm{sp}(S):= \left\{ \sum_{v\in S} a_v v \mid a_v \in K \right\}$$ I suspect that this has something to do with preserving important structures of $S$ in the span of $S$ (perhaps topological structures?), but I have no concrete examples.","['motivation', 'topological-vector-spaces', 'definition', 'linear-algebra']"
4669060,Closed form for $\sum_{n=1}^\infty x^{-\frac{2\pi}{n}} e^{-2\pi n}$,"I need a closed form for $$ \sum_{n=1}^\infty x^{-\frac{2\pi}{n}} e^{-2\pi n}$$ where $x\in[1,\infty)$ For $x=1$ we have the sum as $$ \sum_{n=1}^\infty e^{-2\pi n}=\frac{1}{e^{2\pi}-1}$$ For $1<x<\infty$ we can write the sum as $$ \sum_{n=1}^\infty (x^{\frac{1}{n}} e^n)^{-2 \pi} $$ Any help would be appreciated. Thanks.","['summation', 'sequences-and-series', 'exponential-sum', 'real-analysis']"
4669100,Lambda calculus novice seeking help with defining isempty for list representation,"I'm exploring the concept of untyped lambda calculus and I'm facing a challenge with defining the isempty function. I have a few definitions that I'm working with, which are: true = Œª xy. x
false = Œª xy. y
nil = Œª x. x
pair = Œª xyb. b x y Based on these definitions, a list is constructed by combining nil (e.g. (pair x nil) ) and pair (e.g. (pair x (pair y nil)) ). I initially thought that I could define isempty as (Œª x. x True ???) since nil evaluates to true . However, the resulting list from a non-empty list would be in the form (True x y) , which makes it unclear what to do when neither true nor false can be assigned to x and y . I'm hoping to get some guidance on how to approach defining the isempty function within the context of untyped lambda calculus.","['computer-science', 'functions', 'discrete-mathematics', 'lambda-calculus', 'computability']"
4669121,Transformation of a vector field on $\mathbb{R}^2$ into one on the complex plane,"To gain a better understanding of vector fields and their flow computation, I wanted to experiment with a coordinate transformation: real coordinate in $\mathbb{R}^2$ <-> polar coordinates <-> complex plane. I have no problem with polar coordinates, but I got really confused, about how to deal with the transformation into the complex plane. I have chosen a simple vector field that describes a counterclockwise circular movement: $$ V = -y \partial_x + x \partial_y. $$ Given a starting point $(x_0,y_0) \in \mathbb{R}^2$ , we know that there exist an integral curve $\phi \colon (-\epsilon, \epsilon) \to \mathbb{R}^2$ for some $\epsilon >0$ . It also holds that $$ \phi'(t) = \phi_1'(t) \partial_x + \phi_2'(t) \partial_y = V_{\phi(t)}. $$ So, to find $\phi$ , we need to solve the following system of ODEs: $$\phi_1'(t) = - \phi_2(t),$$ $$\phi_2'(t) = \phi_1(t).$$ A solution for a given initial problem is $$ \phi(t) = (x_0 \cos t - y_0 \sin t, x_0 \sin t + y_0 \cos t) \in \mathbb{R}^2.$$ If we transform this solution into complex numbers, we get that $\phi(t) = (x_0 + i y_0) e^{it} =: z_0  e^{it}$ . I find this form much easier to deal with, so I asked myself, how could I skip all this sine-cosine-computations, in such a way that I just need to solve something like $$\frac{\partial}{\partial t} \phi(t) = i \phi(t).$$ As in the case of the polar coordinate transformation, I started with the transformation of my vector field. Somehow I ended up on the Wiki page reading about the Wirtinger derivative (never heard about it before) and managed to transform my vector field to $$ V = i (z \frac{\partial}{\partial z} - \bar{z} \frac{\partial}{\partial \bar{z}}).$$ After thinking a lot about this new form of the vector field and the result that I wanted to achieve with the complex exponential function, I understood that I have no idea how these two thoughts connect. I have a feeling that I try to combine apples with chairs. I do not know how to get to the point, where I can compare the components of the vector field and the velocity of the curve $\phi$ as I did it in the example above. Or in general, I don't really understand, how to find a flow for this complex vector field. I also don't understand, why I have two ""components"" ( $\partial_z$ and $\partial_\bar{z}$ ). I have thought that I will get a vector field in the form of $V = i z \partial_z$ , in my head, this would somehow connect to the solution that I have. I would gladly appreciate any help in getting a (better) understanding of this problem.","['ordinary-differential-equations', 'differential-geometry']"
4669148,Are these numbers always integers?,"Consider the following two alternating sequences: $$ A=\bigg\lbrace f^{(1)1}(x),\cdot\cdot\cdot \bigg \rbrace \bigg|_{x=1/e}=\bigg \lbrace-1,2,-6,32,-320,4452,-70798, \cdot\cdot\cdot \bigg \rbrace$$ $$B=\bigg\lbrace f^{(1)1}(x),\cdot\cdot\cdot \bigg \rbrace \bigg|_{x=e}=\bigg \lbrace-1,10,-150,3088,-81220,2603748, \cdot\cdot\cdot \bigg \rbrace$$ for $f(x)=\exp\bigg(\frac{1}{\log x} \bigg)$ and $f^{(n)n}(x)$ means ""nth derivative of the nth power of $f$ "" and where each term in $A$ is understood to be evaluated at $x=1/e$ , and each term in $B$ is understood to be evaluated at $x=e.$ Are these numbers always even except the first term? Are these numbers always integers? Note that $f^{(4)2}(x)$ evaluated at $x=1/e$ is not integral. So not every combination gives an integral value. I'm not sure why this pattern occurs with the derivatives.","['elementary-number-theory', 'calculus', 'functions', 'sequences-and-series', 'derivatives']"
4669164,"Counting the number of ways of organizing an odd number of objects into 3 odd groups, elegant method?","I came across this problem on a blog I've been reading (link here but not necessary for understanding the problem). You have to split an odd number N of distinct objects into three different groups such that the number of objects in each group is an odd number. In how many ways can this be done? The blog goes into a combinatorial solution, giving an answer of $\frac{3^N-3}{4}$ . My question is: Is there an elegant reason why it is almost exactly a quarter of the possible distributions that work? More interestingly, is there a simpler argument for why it is exactly the number it is? As an example of what I'm looking for, for the easier question of how many ways there are to split an even number of distinct items into two groups with an even number of items, you can have a given item (item A, say), and for every selection where A is in the first group, there is one for which it is in the second group (just by moving it from group 1 to group 2). Since exactly one of those possibilities has an even number in each group, exactly half of the possible selections will work. Therefore the total number of ways is $2^{N-1}$ . Intuitively, something similar should work for three groups. Is there an argument along similar lines for the three group case?","['combinatorics', 'combinatorial-proofs']"
4669178,Tuple notation for non-negative integers,"I have three variables $x_1, x_2, x_3$ that can only be non-negative integers, i.e., $x_1 \in \mathbb{N}_0$ , $x_2 \in \mathbb{N}_0$ , and $x_3 \in \mathbb{N}_0$ . Is it correct to write this in a tuple like $(x_1,x_2,x_3) \in \mathbb{N}_0^3$ ? If not, what is the correct way?","['elementary-set-theory', 'notation']"
4669209,"Solve the system : $ x^2 + x - 1 = y $, $ y^2 + y - 1 = z $, $ z^2 + z - 1 = x $ .","Solve the system of equations in x,y,z : $ x^2 + x - 1 = y $ , $ y^2 + y - 1 = z $ , $ z^2 + z - 1 = x $ . I'd tried analysing various manipulations but can't figure out . What I've tried: $$ x^2 + x - 1 = y \ ...(1)$$ $$ y^2 + y - 1 = z \ ...(2)$$ $$ z^2 + z - 1 = x \ ...(3)$$ From (1), (2) and (3) , $ x^2 + y^2 + z^2 = 3 $ From (1), $ x(x+1) = y+1 $ & $ (x+1)(x-1) = y-x $ From (2), $ y(y+1) = z+1 $ & $ (y+1)(y-1) = z-y $ From (3), $ z(z+1) = x+1 $ & $ (z+1)(z-1) = x-z $ Any way it's quite obvious to see that $ x = y = z = 1 $ is one of the possible solutions.
But no idea how to use these informations to get all the possible values of x,y and z .","['algebra-precalculus', 'systems-of-equations']"
4669233,Local systems defined by higher homotopy groups,"I should mention I have very little background in algebraic topology and don't really know much about homotopy groups besides the definition. I am aware that for a topological space $X$ and a point $x \in X$ the fundamental group $\pi_1(X,x)$ acts on all homotopy groups $\pi_n(X,x)$ . In particular, this means the $\mathbf{Q}$ -vector space $\mathbf{Q} \otimes_\mathbf{Z}\pi_n(X,x)$ is endowed with a linear action by $\pi_1(X,x)$ and hence defines a local system of $\mathbf{Q}$ -vector spaces on $X$ . If $X$ is a (complex) manifold or something and one can appeal to tools akin to the Riemann-Hilbert correspondence and other interpretations of these objects, can one describe these explicitly? Are they 'tautological' local systems in some sense... ? Can anything be said about the family of local systems $(\mathbf{Q}\otimes_\mathbf{Z}\pi_n(X,x))_{n \geq 2}$ and can these be recognised as something else? I'm not sure my question makes much sense and I was just trying to satiate a curiosity :) Any words of wisdom are appreciated and thank you very much! :D Edit: moved to MO","['complex-geometry', 'algebraic-geometry', 'algebraic-topology', 'differential-geometry']"
