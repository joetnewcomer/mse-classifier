question_id,title,body,tags
1474486,Find expectation of Z (normal),"Assume the following PDF . $$f(z) = \frac{1}{\sqrt{2π}}e^{-(z^2 / 2)}$$ Find E(z). For now, I got E(z) = $\int_{-\infty}^{\infty} f_z(z) dz $ And for any odd function F, (i.e. F(−x) = −F(x)∀x ∈ R), $\int_{-\infty}^{\infty} f_z(z) dz= 0$ So, the function zf(z) = $$ z*e^{-(z^2 / 2)} \frac{1}{\sqrt{2π}}$$ is an odd function. Hence E(Z) = 0. Is it correct? Or is there another way to solve this problem?","['normal-distribution', 'probability', 'expectation']"
1474494,Seeking more information regarding the function $\varphi(n) = \sum_{i=1}^n \left[\binom{n}{i} \prod_{j=1}^i(i-j+1)^{2^j}\right].$,"Define a function $\varphi : \mathbb{N} \rightarrow \mathbb{N}$ as follows. $$\varphi(n) = \sum_{i=1}^n \left[\binom{n}{i} \prod_{j=1}^i(i-j+1)^{2^j}\right]$$ The motivation is that according to this article, we have: for all natural $n$, $\varphi(n)$ is the cardinality of the free band on $n$ generators. Questions. I'd like to learn more about $\varphi$. Q0. Does it have a standard name? Q1. Are there other, apparently unrelated counting problems to which $\varphi$ is the solution? Q2. Does $\varphi$ satisfy any interesting identities? Q3. If so, is there a good resource for learning these identities? Remark. This function helps explain where the number $14$ comes from in the Kuratowski closure-complement theorem . Basically, this happens because $$2(\varphi(2)+1) = 2(6+1) = 14.$$ To see the relevance of $\varphi(2),$ note that if $X$ is a topological space, then letting $k_X$ and $i_X$ denote the closure and interior operators on $X$ respectively, then the semigroup $S_X$ generated by $\{k_X,i_X\}$ is a band; furthermore, we can choose $X$ appropriately (e.g. $X = \mathbb{R}$) such that $S_X$ is the free band on these two elements.","['monoid', 'abstract-algebra', 'combinatorics', 'semigroups', 'reference-request']"
1474502,The distribution of fourier coefficients of a Rademacher sequence,"Assume that $x$ is a Rademacher vector or sequence with $x_i\in \{-1,+1\},i=1,2,...,N$ and $\Pr(x_i=+1)=\Pr(x_i=-1)=0.5$. What's the distribution of $\text{fft}(x)$? Is that a gaussian distribution? What's the mean and variance of it?","['fourier-analysis', 'statistics']"
1474528,How to find $\lim_{x\to +\infty}{\left(\sqrt{\pi}x-\sum_{n=0}^{\infty}\frac{(-1)^nx^{2n+2}}{2n!(2n+1)(2n+2)}\right)}$?,"$$\lim_{x\to+\infty}{\left(\sqrt{\pi}x-\sum_{n=0}^{\infty}\frac{(-1)^nx^{2n+2}}{2n!(2n+1)(2n+2)}\right)}$$ This limit comes from the following integral, which I expand it into Taylor series.
$$S=2\int_0^{+\infty}\left(\frac{\sqrt{\pi}}{2}-\int_0^xe^{-t^2}dt\right)dx$$ My attempt is to solve the integral $S$, but I can't find a proper way, so I expand it into Taylor series and the question I ask is where I stuck.","['calculus', 'limits', 'definite-integrals', 'sequences-and-series', 'integration']"
1474536,Number of ways in which $5$ boys and $5$ girls are ordered in such a way that exactly $4$ girls stand consecutively in the queue,"Let $n$ be the number of ways in which $5$ boys and $5$ girls can stand in a queue in such a way that all the girls stand consecutively in the queue. Let $m$ be the number of ways in which $5$ boys and $5$ girls in such a way that exactly $4$ girls stand consecutively in the queue. Then find $\frac{m}{n}$. I could find $n$ correctly as $6!\times 5!$, but I am finding $m$ as $7!\times 4!\times \binom{5}{1}$, but the book says $m$ is $6!\times 4!\times 5\times \binom{5}{4}$. Where have I gone wrong? What is the proper logic to find $m$?",['combinatorics']
1474554,$L^2$-norm of a solution of the heat equation,"Let $u\in\mathcal{C}^{2,1}(\mathbb{R}^n\times (0,\infty))$ be a solution of the heat equation $$\left[\begin{array}{ll}u_t-\Delta u=0& \mathrm{in}\ \mathbb{R}^n\times(0,\infty)\\ u(x,0)=u_0(x),&x\in\mathbb{R}^n\end{array}\right.$$
where $u_0\in\mathcal{C}^0_c(\mathbb{R}^n)$. The boundary conditions above are meant in the following sense: $\lim_{t\to 0, x\to x_0}u(x,t)=u_0(x_0)$ for all $x_0\in\mathbb{R}^n$. If we assume in advance that $|u|\to 0$ as $|x|\to\infty$ then the following relation holds for all $t>0$:
$$\|u(\cdot,t)\|_{L^2(\mathbb{R}^n)}\leq\|u_0\|_{L^2(\mathbb{R}^n)}.$$
How to prove this result? I am required to use an energy method whence I started with $E(t):=\int_{\mathbb{R}^n}u(x,t)^2dx$ and tried to prove that this function is differentiable for $t>0$ with non-positive derivative. Therefore I introduced $\Omega_n:=B_n(0)$ and considered $E_n(t):=\int_{\Omega_n}u(x,t)^2dx$ (although it is not clear to me why the derivative of $E_n$ converges to the one of $E$). Differentiating $E_n$ and using the PDE as well as integratio by parts I arrive at:
$$E_n'(t)=\int_{\Omega_n}2uu_tdx=2\int_{\Omega_n}2u\Delta udx=-2\int_{\Omega_n}|\nabla u|^2dx+2\oint_{\partial\Omega_n}u\frac{\partial u}{\partial\nu}dS.$$
How to proceed? Although I know that the modulus of $u$ converges to 0, I do not know how to conclude something similar for the normal derivative. Is this ansatz reasonable after all? Thank you very in advance!","['heat-equation', 'real-analysis', 'partial-differential-equations']"
1474598,Find a recurrence relation for the sequence $u_n=$ number of nonnegative integral solutions of $2a+5b=n$.,"Find a recurrence for the sequence $u_n=$ number of nonnegative
  integral solutions of $$2a+5b=n.$$ I think I can use a generating function, but I'm a bit confused at this point. Is anyone is able to give me a hint? I would like to solve the problem myself, so of emblem, I would ask you not to solve the problem.",['discrete-mathematics']
1474629,"Picking random integers $a_i$ until the $\gcd(a_1,a_2,\dots,a_n)$ is 1","When we say ""pick a random integer"", the integers are in the range $[1, N]$ . Problem: Consider the following instructions: Pick a random integer. Find the greatest common divisor of all the integers which have been picked so far. If the greatest common divisor is not 1, go back to the first step to pick a random integer. When this process ends, let $X$ be the total number of integers you have picked. I would like to find $E(X)$ in terms of $N$ . Example of this process: (where $N=4$ ) I picked the random integer, 2. $\gcd(2)=2$ I picked another random integer, 4. $\gcd(2, 4) = 2$ I picked another random integer, 2. $\gcd(2, 4, 2) = 2$ I picked another random integer, 3. $\gcd(2, 4, 2, 3) = 1$ In total, I picked $X=4$ integers, $2, 4, 2, 3$ . My ideas: A easy upper bound can be shown to be $N$ , which is the expected number of numbers to be chosen before getting a 1. Let $M(g, N)$ be the expected number of integers I will need after getting $g$ as the greatest common divisor of the integers so far. We have $E(X)=1+\frac{\sum_{i=1}^NM(g, N)}{N}$ . Small cases: $N=1\rightarrow E(X)=1$ $N=2\rightarrow E(X)=2$","['probability', 'elementary-number-theory']"
1474649,Translating set definition to English,"The definition of the union set between two sets is: $$A \cup B = \{x | x\in A \lor x\in B\}$$
How do you say this in English? Do you say '$A \cup B$ is the set of all elements $x$, such that $x$ is an element of $A$ or $x$ is an element of $B$'? Isn't this the same as saying $\forall x ( x\in A \lor x \in B)$. What is the difference between the two?",['discrete-mathematics']
1474656,"Extension theorem for Sobolev spaces $W^{1,\infty}(\Omega)$: is there an elementary proof?","Basically, I have the same question as in Extension of $W^{1,\infty}(\Omega)$ : Given a bounded, open set $\Omega\subset\mathbb{R}^n$ with $\mathcal{C}^1$-boundary and another open bounded set $V\supset\supset\Omega$. I want to find an extension operator $E:W^{1,\infty}(\Omega)\rightarrow W^{1,\infty}(\mathbb{R}^n)$ which is linear, bounded and satisfies:
$Eu=u$ $dx$-a.e on $\Omega$ as well as $\mathrm{supp}(Eu)\subset V$ for each $u\in W^{1,\infty}(\Omega)$. I do not know the result about the coincidence of $W^{1,\infty}(\Omega)$ with Lipschitz continuous functions as stated in the link above. Moreover, all approximation results I know for Sobolev functions are for $p<\infty$. I was wondering whether it is possible to construct a Cauchy sequence in $W^{1,\infty}(\mathbb{R}^n)$ by mollifying the zero-extension of $u\in W^{1,\infty}(\Omega)$. Maybe the limit has the desired properties. Unfortunately, I was not able to show the Cauchy-property. Does someone know an elementary proof of the desired result?","['sobolev-spaces', 'functional-analysis']"
1474658,measure induced by a monotone continuous function,"let $F$ be a continuous monotone increasing function on real line.if $A$ is an interval with endpoint $a,b$ we define:
$\mu _F (A)=F(b)-F(a)$ and if $A$ is a disjoint union of intervals, $A=\cup_{1}^n A_i$ $\mu _F (A)=\Sigma _{i=1}^n \mu _F (A_i)$ prove that $\mu_F$ is countably additive. this is what i've done: $F$ maps the intervals to intervals , so $\mu _F (A)=\mu_LF(A)$ when $\mu_L$ is lebesgue measure on real line. and we have: $\Sigma _{i=1}^n \mu _F (A_i)=\mu_F(\cup_{1}^n A_i)=\mu_L(F(\cup_{1}^n A_i))$ and if a tend $N$ to infinity, i get: $\Sigma _{i=1}^\infty \mu _F (A_i)=\mu_L(F(\cup_{1}^\infty A_i))$ but how to say these equal $\mu_F(\cup_{1}^\infty A_i)$? thanks.","['real-analysis', 'measure-theory']"
1474669,Convergence in measure imply integrals converge?,"Suppose $(X, \mathcal{M}, \mu)$ is a measure space such that $\mu(X)<\infty$, and let $(f_{n})_{n\geq 1}$ be a sequence of measurable functions converging in measure to a measurable function $f$. Does this imply that $$\int_{X} f d\mu =  \lim_{n\to \infty} \int_{X} f_{n}  d\mu ?$$ All I know is that if $f_{n} \to f$ in measure, then there exists a subsequence $(f_{n_{j}})_{j \geq 1}$ such that $f_{n_{j}} \to f$ $\mu$- almost everywhere which implies the above, but I do not know if this is true for the sequence in general.","['real-analysis', 'measure-theory']"
1474679,Counterexample to $L^1$-boundedness of the maximal operator $f \mapsto f^\#$ with $f^{\sharp}(x):=\sup_{Q\ni x}\frac{1}{|Q|}\int_{Q}|f-(f)_{Q}|dy$,"My question concerns the sharp maximal operator, mapping a locally integrable function $f\colon\mathbb{R}^{n}\to\mathbb{R}$ to $f^{\sharp}\colon \mathbb{R}^{n}\to\mathbb{R}$, where $$f^{\sharp}(x):=\sup_{Q\ni x}\frac{1}{|Q|}\int_{Q}|f-(f)_{Q}|dy,$$ the $\sup$ running over all cubes with sides parallel to the coordinate axes and containing $x$. Similar to the Hardy-Littlewood maximal operator which maps $L^{1}$ to weak-$L^{1}$, but not $L^{1}$ to $L^{1}$, I guess that the sharp maximal operator does not map $L^{1}$ to $L^{1}$. The standard example for this fact with respect to the Hardy Littlewood maximal operator, namely, the indicator function of the unit cube in $\mathbb{R}^{n}$ does not seem to serve as a counterexample in this situation. My question: Is there a similar ""cheap"" counterexample which shows that the sharp maximal operator does not map $L^{1}$ to $L^{1}$ - I would appreciate this very much! I've tried to find a solution to this issue in the web, but all references do not deal with this, so that I guess that it's either entirely trivial and I don't see it - or one does have a bound $\|f^{\sharp}\|_{L^{1}}\leq C \|f\|_{L^{1}}$. However, referring to Variable Lebesgue Spaces of Cruz-Uribe & Fiorenza (Birkhaeuser 2013), p. 230, one has $\|f\|_{L^{p}}\leq c\|f^{\sharp}\|_{L^{p}}$ for all $f\in L^{p}$, $1\leq p<\infty$. Since $f^{\sharp}\leq C Mf$ with $Mf$ the Hardy-Littlewood maximal function pointwisely, one would obtain $\|f\|_{L^{p}}\leq c \|f^{\sharp}\|_{L^{p}}\leq C \|Mf\|_{L^{p}}\leq C'\|f\|_{L^{p}}$ provided $1<p<\infty$ which implies that the norms $\|\cdot\|_{L^{p}}$ and $\|(\cdot)^{\sharp}\|_{L^{p}}$ are equivalent, but somehow I cannot image that this remains valid for $p=1$ (which would be the case if one had a bound of $f^{\sharp}$ in $L^{1}$-norm against the $L^{1}$-norm of $f$).","['lp-spaces', 'harmonic-analysis', 'functional-analysis']"
1474685,A question about gradient.,"The gradient of a function of two variable $f(x,y)$ is given by 
$$\left( \frac{\partial f}{\partial x} ,\frac{\partial f}{\partial y}\right). $$
It is also evident that gradient points in the direction of the greatest increase or decrease of a function at a point. My question is that whether the gradient vector is tangent to the function $f(x,y)$ at a point or not. The tangent here means that if you cut the graph of the function f(x,y) along the direction of the gradient vector then you will have a 2D curve formed where the function is cut. So is the gradient vector tangent to that curve at the particular point.",['multivariable-calculus']
1474696,Video lectures on Functional Analysis,"I am looking for excellent VIDEO lectures on functional analysis.
They should be (1) in English (2) the video quality and voice is good (3) the lecture should not be presented in boring style I am very thankful for your suggestions",['functional-analysis']
1474702,Is it acceptable to use reduced row echelon to show basis?,"I am asked to show that { a , b , c } forms a basis for $\Bbb R^3$. I'm just wondering if it is acceptable to use reduced row echelon to show it since it is not shown that way in the marking scheme? $$a= \begin{pmatrix}
        2\\
        -1\\
        1 \\
        \end{pmatrix}b= \begin{pmatrix}
        1\\
        1\\
        1 \\
        \end{pmatrix}
 c= \begin{pmatrix}
        0\\
        1\\
        -1 \\
        \end{pmatrix}$$ Here's how I did it: For the vectors to form a basis, they must be linearly independent such that
$$a_1\begin{pmatrix}
        2\\
        -1\\
        1 \\
        \end{pmatrix}+a_2\begin{pmatrix}
        1\\
        1\\
        1 \\
        \end{pmatrix}+a_3\begin{pmatrix}
        0\\
        1\\
        -1 \\
        \end{pmatrix}=\begin{pmatrix}
        0\\
        0\\
        0\\
        \end{pmatrix}$$ So, I did reduced row echelon and I got..
$$\begin{bmatrix} 2 & 1 & 0 \\-1 & 1 & 1\\  1 & 1 & -1 \\
     \end{bmatrix} \rightarrow \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0\\
        0 & 0 & 1 \\
        \end{bmatrix}$$
After that, I solve for $a_1, a_2$ and $a_3$
$$\begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0\\
        0 & 0 & 1 \\
        \end{bmatrix}\cdot \begin{Bmatrix}
        a_1\\
        a_2\\
        a_3 \\
        \end{Bmatrix}=0$$ and I got $a_1=a_2=a_3=0$. Then, I conclude that { a , b , c } forms a basis.","['self-learning', 'linear-algebra']"
1474715,Why the case of independence of random variables is more important than any other specific type of dependence?,"Maybe a stupid question but why is the case of independence of, say, two random variables $X$ and $Y$ is in some ways considered to be more ``central'' or more important than any other type of fixed dependence between $X$ and $Y$?","['probability-theory', 'probability', 'random-variables']"
1474738,"Prove $\nabla u \times \nabla v =0$ is a necessary and sufficient condition that $u$ and $v$ are functionally related by the equation $F(u,v) = 0$","Let $u$ and $v$ be differentiable functions of $x$,  $y$ and $z$. Show
  that a necessary and sufficient condition that $u$ and $v$ are
  functionally related by the equation $F(u,v) = 0$ is that $$\nabla u
 \times \nabla v =0$$","['vector-analysis', 'multivariable-calculus']"
1474767,Maximizing $3 x^2+2 \sqrt{2} x y$ with $x^4+y^4=1$,"I want to have maxizing value of $3 x^2+2 \sqrt{2} x y$  when $x^4+y^4=1$, $x>0,y>0$.
How can I solve it.","['optimization', 'algebra-precalculus']"
1474799,"Limit towards infinity, definition and proof?","I have a question regarding an assignment in school. We are learning about limit, epsilon-delta proofs. This is an assignment that I have done but still need to correct. a) Give a proper definition of what it means for a general sequence $$\lim_{x\to\infty} a_n = \infty$$ b) Recall $ n! = 1 \cdot 2 \cdot 3 \cdot 4 \cdot \cdot \cdot n $ Use the definition from a) to show that $n!$ grows faster than $2^n$. That is, show that; $$ \lim_{n\to\infty} \frac{n!}{2^n} = \infty $$ Ok so there is what I did the first time handing it in. I compared the ratio to show that $a_{n+1} > a_n $ Which we could not use according to my teacher since we have not proved ratio or done them in class yet. I know for an expression to go towards infinity it must not be bounded from above and it must be increasing as the n gets larger. I hafto use my definition in a) and also I have stated another definition in b). That is the definition of an increasing sequence wich is, if increasing:$$ a_{n+1} > a_n $$ I have been trying to use induction and I can show with induction that $ n! > 2^n \forall n \geq 4$ But this according to my teacher is not proof enough that the sequence is going towards infinity when n is going to infinity. I know I am supposed to use induction, but how? I also know that I hafto use my definition from a) and that I should start with: Let $ \varepsilon > 0 $ be given (fixed but unknown). My teacher showed this information: He said to use $1) a_n \geq \frac{n}{4}$ ok! and $n\geq 1$. Or we could use $2) a_n \geq \frac{n}{2}$ ok, and $n \geq 2$. Or third, we could also use $3) a_n \geq n$ ok and $n \geq 6$ We will hafto prove it and I would assume induction would be a good proof? Start with basecase, n = 4 (could choose any basecase but it has to be 1 or more in this case, if i'd choose 1)), then $$\frac{4 \cdot 3 \cdot 2 \cdot 1}{2^4} \geq \frac{n}{4}$$, which is ok.  Then after this i would hafto do induction step, where I assume that it holds for n = k and then it would also hold for n = k + 1. But here Im insecure about exactley what I do. I know I need to prove the induction hypothesis, but exactley what would be the induction hypothesis. Do I prove this???; $$\frac{k(k+1)!}{2^{k+1}} \geq \frac{(k+1)}{4}$$ And how exactly would the following computations look? I find the n! term hard to work with. After the induction proof I guess I also need to find an n, from the definition in a), so that $a_n > \varepsilon$. This I also find hard. I know that the formula should be $$ n \geq 4\varepsilon+1$$ This implies that $\frac{n}{4} > \varepsilon$, since $a_n \geq \frac{4}{n}$ we have, $a_n > \varepsilon$. A good choice when \varepsilon is uknown would then be $$N = 4 \varepsilon +1$$. I understand this reasoning and I understand that N can be made a function of epsilon. I understand epsilon is the challange and to prove the limit exists all challanges, epsilon, can be answered with this N, if the limit is infinity (although infinity is not a number, so the sequence is not convergeing, but divergeing when it goes towards infinity) the expression can with this N always be made larger than any epsilon. My problem is the algebra. How do I find this N? The induction proof, how do I prove that it holds for n=k+1?? I think i have problems overall just getting the assignment to sync and really be sure that I have used the defn. in a accurate, that I hve proven the sequence is growing and that it is going towards infinity. I'ts like I'm almost there and understand it, but there is still some part missing. I think mainly it's the n! term that is giving me problems. If I had $\frac{n+1}{2} > \varepsilon \leftrightarrow n+1 > 2\varepsilon \leftrightarrow n > (2\varepsilon) - 1$. Here the n would have been very easy to compute. Oh well, now it is'nt. So help is much wanted and needed ! :)","['analysis', 'calculus', 'limits']"
1474806,Double sum involving $\cos$,"I ran across a double sum and was wondering if someone may be adept at evaluating it. I must admit that my double summation skills could be better, and I am always ready to learn more. Show that: $$\sum_{n=1}^{\infty}\sum_{k=1}^{\infty}\frac{\cos(\frac{\pi}{3}(n-k))}{nk(n+k)}=\frac{-1}{9}\zeta(3)+\frac{\pi\sqrt{3}}{27}\psi_{1}(1/3)-\frac{2\pi^{3}\sqrt{3}}{81}$$ I found this while playing around with: $$\int_{0}^{1}\frac{\log(1-xe^{\frac{\pi i}{3}})\log(1-xe^{\frac{-\pi i}{3}})}{x}dx$$ I would enjoy seeing a clever evaluation of the sum. I tried breaking it up as: $$\sum_{n=1}^{\infty}\sum_{k=1}^{\infty}\frac{\cos(\frac{\pi k}{3})\cos(\frac{\pi n}{3})}{kn^{2}}$$ $$-\sum_{n=1}^{\infty}\sum_{k=1}^{\infty}\frac{\cos(\frac{\pi k}{3})\cos(\frac{\pi n}{3})}{n^{2}(k+n)}$$ $$+\sum_{n=1}^{\infty}\sum_{k=1}^{\infty}\frac{\sin(\frac{\pi k}{3})\sin(\frac{\pi n}{3})}{kn^{2}}$$ $$-\sum_{n=1}^{\infty}\sum_{k=1}^{\infty}\frac{\sin(\frac{\pi k}{3})\sin(\frac{\pi n}{3})}{n^{2}(k+n)}$$ The third from the top evaluates to $$\frac{\pi}{3}Cl_{2}(\frac{\pi}{3})=\frac{\pi}{3}\left(\frac{\sqrt{3}}{6}\psi_{1}(1/3)-\frac{\pi^{2}\sqrt{3}}{9}\right)$$ I think the top one evaluates to 0. The other 2 are a little more challenging. Would anyone enjoy lending a hand and showing a method to evaluate said double sum...or even the integral for that matter?.","['summation', 'sequences-and-series']"
1474808,What does it mean to 'preserve the first fundamental form'?,"I'm a bit confused about the phrase 'preserving the first fundamental form', or 'The Gaussian curvature is determined by the first fundamental form'. For example, let's say I have two surfaces $M$ and $M'$. How can they possibly have 'the same fundamental form', when: The first fundamental form is a concept defined relative to some parametrisation The domains of the various fundamental forms will not even coincide: $T_pM\times T_pM$ vs $T_{p'}M'\times T_{p'}M'$. I just don't see how to interpret the phrase 'having the same fundamental form' or something being 'determined by the fundamental form' given that it is such a 'relatively-defined' notion. Any help in clearing this up would be much appreciated. Thanks Oke here is a concrete example of something which confuses me: This source defines the first fundamental form as follows: $$I_P(U,V)=U\cdot V,\text{for } U,V\in T_PM\ \ (\subset \mathbb{R}^3)$$ From this definition it is clear that this is independent of parametrisation, since it is completely in terms of the inner product on $\mathbb{R}^3$. However, the confusion starts when they then give the following definition (These are notes by Theodore Shifrin): Suppose $M$ and $M^*$ are surfaces. We say they are locally isometric if for each $P\in M$ there are a reg-param $x:U\to M$ with $x(u_0,v_0)=P$ and $x^*:U\to M^*$, with the property that $I_P=I_{P^*}^*$, whenever $P=x(u,v)$ and $P^*=x^*(u,v)$. That is, the function $f=x^*\circ x^{-1}$ is a one-to-one correspondence that preserves the first fundamental form . Oke so I just don't see how this definition of a local isomertic surfaces is even well defined when working with the above definition of the first fundamental form. What does is possibly mean that $I_P=I_{P^*}^*$? Usually for a function equality they need to at least have the same domains, but that's not the case here. The only thing I can imagine is that $f$ induces through it's derivative a map $T_PM\to T_{f(P)}M^*$, and so that we say that $$I_P=I_{P^*}^* \text{ iff } I_P(x,y)=I_{P^*}^*(f'(x),f'(y))$$ And so that in general for a map $f$ to preserve the first fundamental form, this above definition is what it means. At least this is independent of parametrisation, so that's nice.","['differential-geometry', 'surfaces', 'definition']"
1474812,How to prove the cyclic property of the trace?,"I need to prove that in general case (for every possible combination of square matrices) trace of the product of said matrices stays the same after some permutation iff that permutation is cyclic. I can prove it one way: $\text{tr}(A_1A_2 \ldots A_{n-1}A_n) = \text{tr}(A_nA_1A_2 \ldots A_{n-1})$, but I'm having problems with proving the other part: ""if the trace didn't change, then the permutation was cyclic"". If someone could at least point me in the right direction, I would be very grateful.","['trace', 'matrices']"
1474845,$U(n)$ is a subgroup of $SO(2n)$,"How can I show that $U(n)$ is a subgroup of $SO(2n)$? I can see how we can identify $\mathbb{C}^n$ with $\mathbb{R}^{2n}$ by mapping $a+ib\mapsto (a,b)$, but after that I'm a bit confused. In particular, the determinant of a matrix in $U(n)$ has to be nonzero, whereas in $SO(2n)$ it has to be 1. How do we make this transition?",['group-theory']
1474849,Show that this sum is an integer.,"I have to show that $$g\left(\frac{1}{2015}\right) + g\left(\frac{2}{2015}\right) +\cdots + g\left(\frac{2014}{2015}\right) $$
is an integer. Here $g(t)=\dfrac{3^t}{3^t+3^{1/2}}$. I tried to solve it using power series, but I can not finish with any convincing argument to said that the sum is an integer. (Also the use of a computer isn't allowed.)","['summation', 'calculus', 'elementary-number-theory']"
1474867,How to bound $\int_{0}^{a}{\frac{1-\cos x}{x^2}}$?,"I was trying to prove $$\left|\int_{0}^{a}{\frac{1-\cos{x}}{x^2}}dx-\frac{\pi}{2}\right|\leq \frac{3}{a}$$ or $\leq \frac{2}{a}$. My work: I would like to use Fubini's theorem to prove it. I notice that $\frac{1}{x^2}=\int^{\infty}_{0}{ue^{-xu}}du$. Then, I got $\int_{0}^{a}{\frac{1-\cos{x}}{x^2}}dx=\int_{0}^{\infty}u\int_{0}^{a}{(1-\cos{x})e^{-xu}}dxdu$. Then, I got $\int_{0}^{a}{(1-\cos{x})e^{-xu}}dx=-e^{-au}u+\frac{1}{u+u^3}+e^{-au}\frac{u^2\cos{a}-u\sin{a}}{u+u^3}$. Then, $\int_{0}^{a}{\frac{1-\cos{x}}{x^2}}dx=\int_0^{\infty}u(\frac{e^{au}-1}{u}+\frac{u-e^{au}(u\cos{a}+\sin{a})}{1+u^2})du\\=\int_0^{\infty}({e^{au}+\frac{-ue^{au}(u\cos{a}+\sin{a}-2)}{1+u^2}})du+\frac{\pi}{2}.$ I was trying to show $|\int_0^{\infty}({e^{au}+\frac{-ue^{au}(u\cos{a}+\sin{a}-2)}{1+u^2}})du|\leq\frac{3}{a}$ or $\frac{2}{a}$. But I do not have a clue. Can some give me hints?","['calculus', 'inequality']"
1474931,"On evaluating the Riemann zeta function, including that $\zeta(2)\gt \varphi$ where $\varphi$ is the golden ratio","A week ago, I got the following : For a positive integer $k$, using Cauchy–Schwarz inequality, 
  $$\left(\sum_{n=1}^{\infty}\frac{1}{n^k(n+1)^k}\right)^2\lt \left(\sum_{n=1}^{\infty}\frac{1}{n^{2k}}\right)\left(\sum_{n=1}^{\infty}\frac{1}{(n+1)^{2k}}\right)=\zeta(2k)(\zeta(2k)-1),$$
  i.e.
  $$\zeta(2k)^2-\zeta(2k)-\left(\sum_{n=1}^{\infty}\frac{1}{n^k(n+1)^k}\right)^2\gt 0.$$
  So, 
  $$\zeta(2k)\gt \dfrac{1+\sqrt{1+4\left(\sum_{n=1}^{\infty}\dfrac{1}{n^k(n+1)^k}\right)^2}}{2}.$$ From this, we can have the followings : $$\zeta(2)\gt \varphi,\qquad\zeta(4)\gt \dfrac{1+\sqrt{1+4\left(\dfrac{\pi^2}{3}-3\right)^2}}{2},\qquad \zeta(6)\gt \dfrac{1+\sqrt{1+4\left(10-\pi^2\right)^2}}{2}$$where $\varphi=\frac{1+\sqrt 5}{2}$ is the golden ratio . Now let us define a sequence $\{a_k\}$ as
$$a_k=\zeta(2k)-\dfrac{1+\sqrt{1+4\left(\sum_{n=1}^{\infty}\dfrac{1}{n^k(n+1)^k}\right)^2}}{2}$$ Then, it seems that $\{a_k\}$ is decreasing :
$$a_1\approx 0.0269,\quad a_2\approx 0.0044,\quad a_3\approx 0.0006.$$
But I've been facing difficulty in proving that. Question : Is $\{a_k\}$ decreasing? If so, how can we prove that?","['number-theory', 'riemann-zeta', 'sequences-and-series', 'golden-ratio', 'inequality']"
1474966,"Help with understanding point from Nassim Taleb's book ""Dynamic Hedging""","This is an excerpt from Nassim Taleb's book ""Dynamic Hedging"" (a book on option trading strategies) page vii Most examples in this book are presented as generic situations. The
  volatility will be defined as 15.7% (to make one standard deviation
  equal to 1% daily move). I'm trying to understand where the numbers 15.7% and 1% came from. How were they derived or estimated? Edit: Adding more context, here's a screen cap of the actual text:","['probability', 'probability-distributions', 'standard-deviation']"
1474968,The coefficient of $x^9$ in the expansion of $(1+x)(1+x^2)(1+x^3)\cdots(1+x^{100})?$,What is the coefficient of $x^9$ in the expansion of $(1+x)(1+x^2)(1+x^3)\cdots (1+x^{100})?$ I manually expanded $(1+x)(1+x^2)(1+x^3)...(1+x^{10})$ and calculated the coefficient of $x^9$ as $8$ but i dont know how to solve it without expanding.Please help me.,"['sequences-and-series', 'combinatorics']"
1474995,"find an approximate solution, up to the order of epsilon","The question is to find an approximate solution, up to the order of epsilon of following problem. $$y'' + y+\epsilon y^3 = 0$$
$$y(0) = a$$
$$y'(0) = 0$$ I tried to solve the given problem using perturbation theory. $$y(t) = y_0(t) + \epsilon y_1(t) + \epsilon^2 y_2(t) + \cdots$$
$$t = w(\epsilon)t = (1 + \epsilon w_1 + \epsilon^2 w_2 + \cdots )t$$ However, i failed to find an appropriate approximate solution... help me!!","['approximation', 'ordinary-differential-equations', 'perturbation-theory']"
1475012,Gauss Divergence theorem gives a wrong result for a surface integral,"Evaluate $\iint _{{S}}(y^{2}z^{2}i+z^{2}x^{2}j+x^{2}y^{2}k)\cdot \,ds$ where $S$ is the part of the sphere $x^{2}+y^{2}+z^{2}=1$\, above the xy-plane. Answer to this question is $\pi/24$ as given in this link but when I try to evaluate it using Gauss's Divergence theorem it fails. Clearly all partial derivatives are zero. Hence I get the answer is zero. What is the reason that divergence theorem does not provide the answer or fails here?","['surface-integrals', 'vector-analysis', 'multivariable-calculus']"
1475013,Is differentiating on both sides of an equation allowed? [duplicate],"This question already has answers here : When is differentiating an equation valid? (2 answers) Closed 8 years ago . Let's say we have $x^2=25$
So we have two real roots ie $+5$ and $-5$. But if we were to differentiate on both sides with respect to $x$ we'll have the equation $2x=0$ which gives us the only root as $x=0$. So does differentiating on both sides of an equation alter it? If it does, then how do we conveniently do it in Integration by substitutions? If not then what exactly is going on here ?",['calculus']
1475020,"Solving the second-order, linear, inhomogeneous ODE $y'' - 2y'\tan(x)-y=\sin(x)$","I have the following ODE: $$y'' - 2y'\tan(x)-y=\sin(x)$$ I am at a loss where to start. All the methods described in my textbook assume knowledge of the complementary function to solve $2$nd order ODEs with variable coefficients. However in a case like this, it seems that the roots of the related homogeneous ODE,
$$y'' - 2y'\tan(x)-y=0 ,$$ cannot be found. Could anyone give me a hint on how to start tackling this problem? Or what methods to use?","['calculus', 'ordinary-differential-equations']"
1475053,Directed acyclic graph and adjacency matrix,How can I prove that a directed graph is acyclic if and only if the vertices can be sorted in such a way that the adjacency matrix has upper triangular form with only zeros in the diagonal? I know that a graph is acyclic as long $A^n$ has zeroes along the diagonal for every $n \geq 1$. How can I prove the above statement?,"['graph-theory', 'discrete-mathematics']"
1475056,Discrete Math - Show Logically $X$ beat $Y$ exactly 2 Times,"I was given a statement that I need to show logically. The question is #6 on this page . I have figured out the other 3 on my own but am stuck on this part of question: Define the predicate $B(x, y)$ to mean that $x$ beat $y$ in the race. Give the logical expression equivalent to 'Sam beat exactly two people'. I know that the use of a universal or existential quantifier is most likely required here but I am not sure how to write the expression ""equal to two"" in conjunction with them. Any help would be greatly appreciated.","['logic', 'discrete-mathematics']"
1475148,Blackwell's example in Markov process theory and Kolmogorov's extension theorem,"I'm reading Continuous Time Markov Processes: An Introduction by Thomas M. Liggett . Chapter 2.4 is devoted to Blackwell's example . Let $E=\left\{0,1\right\}$, $\mathcal E:=2^E$ and $X$ be the ( Question 1 : ""Is $X$ really uniquely determined?"") Markov process with values in $(E,\mathcal E)$ and $Q$-matrix (also called transition rate matrix ) $$\left(\begin{matrix}-\beta&\beta\\\delta&-\delta\end{matrix}\right)$$ for some $\beta,\delta>0$. Now, let $\left\{\beta_n\right\},\left\{\delta_n\right\}\subseteq(0,\infty)$ and $\left\{X_n\right\}$ be an independent ( Question 2 : ""Why is it possible to guarantee independence?"") family of two-state Markov proceesses according to $X$ above. Now, let $\pi_n:E^{\mathbb N}\to E$ be the $n$-th coordinate map and define $Y$ by $$\pi_n\circ Y=X_n\;,$$ i.e. $Y$ is the process with values in $\left(E^{\mathbb N_0},\mathcal E^{\otimes\mathbb N}\right)$ defined by $$Y(t):=\left(X_1(t),X_2(t),\ldots\right)\;\;\;\text{for }t\ge 0\;.$$ Liggett states, that $Y$ ""is well-defined by the Kolmogorv extension theorem"", which can be stated as follows: Let $(\Omega_i,\mathcal A_i)$ be a Borelian measurable space and $\left(\operatorname P_J:J\subseteq I\text{ finite}\right)$ be a projective family of probability measures on $$\left(\Omega_J,\mathcal A_J\right):=\left(\prod_{j\in J}\Omega_j,\bigotimes_{j\in J}\mathcal A_j\right)\;.$$ Then, there is a unique probability measure $\operatorname P$ on $(\Omega_I,\mathcal A_I)$ with $$\operatorname P_J=\operatorname P\circ\pi_J^{-1}\;\;\;\text{for all finite }J\subseteq I\;,$$ where $\pi_J:\Omega_I\to\Omega_J* are the canonical projections. Question 3 : I don't understand why we need the extension theorem and why it guarantees well-definedness. Why should $Y$ not be well-defined, at all?","['probability-theory', 'measure-theory', 'markov-process', 'markov-chains', 'stochastic-processes']"
1475201,Show that matrix $M$ is not orthogonal if it contains column of all ones.,"Original problem: We have invertible matrix $M$ dimension $n$ over the $\mathbb F_2$. If $M$ contains column of all ones, then there exists two different rows $V,W$ of $M$ such that: $$<V,W>=\sum_{i}v_iw_i=1. $$ After sometime and with the help of my friend we find out that the problem is equal to the $$M*M^T\neq I,$$ where $I$ is the identical matrix. And this problem can be solved by solving this one: We have invertible matrix $M$ dimension $n$ over the $\mathbb F_2$. If $M$ contains column of all ones, then it is not orthogonal matrix. With $\mathbb F_2$ we denote the field with two elements: zero and one. Every suggestions and ideas are welcome. Edit: It's all ok when $n$ is even, but the odd $n$'s are problem.","['linear-algebra', 'matrices']"
1475211,If $\cos(-\theta) = \cos \theta$ then why is the value of $\cos(-\theta) $ negative when $\theta \lt -90^\circ$?,"I'm learning Trigonometry right now with myself and currently learning about trigonometric functions. I'm a little bit confused right now. If  $\cos(-\theta) = \cos \theta$, then why does $\cos \theta$ have  negative values when $\theta$ is less than $-90^\circ$?",['trigonometry']
1475233,"Equality $H^i(K,\mathcal{F}_{|K})=\varinjlim_{U\supset K}H^i(U,\mathcal{F}_{|U})$ for a constructible sheaf","The setting is the following. I have a complex algebraic variety $X$, and $\mathcal{F}$ is a constructible sheaf on it (i.e. there is a stratification of Zariski-locally closed subsets $X=\sqcup_{i \in I} X_i$, and $\mathcal{F}$ is locally constant on each $X_i$ with respect to the euclidean topology). My question is the following. How to show  $H^i(K,\mathcal{F}_{|K})=\varinjlim_{U\supset K}H^i(U,\mathcal{F}_{|U})$? This is used in the proof of Artin-Grothendieck theorem in the book Positivity in Algebraic Geometry, by Lazarsfeld. Both complete answers and references are welcome!","['complex-geometry', 'sheaf-cohomology', 'homology-cohomology', 'algebraic-geometry', 'algebraic-topology']"
1475235,Why doesn't $e^x$ have an inverse in the complex plane?,Why doesn't $e^x$ have an inverse in the complex plane? Can someone please clarify it?,"['complex-analysis', 'complex-numbers']"
1475253,Evaluate Integrals by Changing to Polar Coordinates,"I'm working on this question for my Calculus III Homework: Evaluate the given integral by changing to polar coordinates. $$\iint_{R} (5x-y)\,dA$$ where R is the region in the first quadrant enclosed by the circle
$x^2 + y^2 = 16$ and the lines $x = 0$ and $y = x$. I mapped out the coordinates and got $\displaystyle\iint_R (5r\cos\theta-r\sin\theta)\,r \,dr\, d\theta$, where $0 \le r \le 4$ and $0 \le \theta \le \pi/4 $. Working it out it came out to $64 \sqrt{2} \, 64/3$, which was incorrect. If anyone could point out where I went wrong (most likely with defining coordinates), I would appreciate it very much.","['polar-coordinates', 'multivariable-calculus', 'integration']"
1475264,"Why is the intersection of countably many homogeneously Suslin subsets of $\,^{\omega} \omega$ homogeneously Suslin?","I found this assertion in these notes: The derived model theorem (Steel) right in the beginning on page 3, together with the remark that this is 'not too hard to show'. Unfortunately, I'm stuck at the moment. Could somebody point me to where I can find a proof? In the books I checked I can find similar results only on the weakly homogeneously Suslin sets. (edit: I did find the result in Woodin's ""The axiom of determinacy"", but again without proof.) edit: The only thing I can come up with right now is: If $T_n$ is a tree on $\omega\times Z_n$ with $A_n = p[T_n]$ for $n<\omega$, then $T$ on $\omega\times \Pi_{n<\omega} Z_n$ could be defined by $(s,(t_n\, |\, n<\omega))\in T$ iff $(s,t_n) \in T_n$ for all $n$, indeed yielding $p[T]= \bigcap_{n<\omega} A_n$; then for each $s\in\,^{<\omega} \omega$, I need a measure on $\Pi_{n<\omega}T_{n,s}$ with measures on each individual $T_{n,s} = \{ t\in\,^{<\omega} Z\, |\, (s,t)\in T_n\}$ given. But I know of no general way to define a measure on the (entired powerset of the) product.","['descriptive-set-theory', 'set-theory', 'trees', 'measure-theory']"
1475279,proving a given curve is a geodesic,"I am trying to solve the following problem from Lee's Riemannian Manifolds : where the curve $\gamma : I \to \mathbb{R}^2$ is given by $\gamma(t) = (a(t),b(t))$ so that $M$ is parametrized as $\varphi(\theta,t) = (a(t) \cos \theta, a(t) \sin \theta, b(t))$. I completed part (a), and got the following: $$\begin{array}{cccc}
\Gamma_{\theta \theta}^{\theta} = 0, & \Gamma_{\theta \theta}^t = - a(t) \dot{a}(t), & \Gamma_{\theta t}^{\theta} = \dfrac{\dot{a}(t)}{a(t)}, & \Gamma_{\theta t}^t = 0, \\ 
\Gamma_{t \theta}^{\theta} = \dfrac{\dot{a}(t)}{a(t)}, & \Gamma_{t \theta}^t = 0, & \Gamma_{tt}^{\theta} = 0, & \text{and } \Gamma_{tt}^t = 0.
\end{array}$$ For part (b), I think I should be showing that the curve $\alpha : I \to M$ defined by $\alpha(t) = \varphi(\theta_0,t) = (a(t) \cos \theta_0, a(t) \sin \theta_0,b(t))$ satisfies the geodesic equation $$\ddot{\alpha}^k(t) + \dot{\alpha}^i(t) \dot{\alpha}^j(t) \Gamma_{ij}^k (\alpha(t)) = 0.$$  However, I'm confused as to how to perform the calculation when my curve is in standard coordinates and my Christoffel symbols are in $(\theta,t)$ coordinates.  Can anyone tell me where I'm going wrong in trying to do the calculation and what I should be doing instead? Also, I'm assuming that for part (c), I should again be using the geodesic equation, but that I should be able to derive a condition on the ""latitude circle"" after performing the calculation.","['geometry', 'riemannian-geometry', 'differential-geometry']"
1475290,Equilateral Triangle from three complex points,"I need some help proving this, I've seen it proven in the other direction (prove the formula if it is an equilateral) but cant figure out how to prove it this way around. Given three complex numbers $z_1, z_2, z_3$ prove that the points $z_1, z_2, z_3$ are vertices of an equilateral triangle in $\Bbb C$, if $$z_1^2 + z_2^2 + z_3^2 = z_1z_2 + z_1z_3 + z_2z_3$$",['complex-analysis']
1475291,In what sense are the linear characters among the irreducible characters,"Let $G$ be a finite group and denote by $\mathbb C^{\times}$ the multiplicative group of the complex numbers. A linear character is a homomorphism $\chi : G \to \mathbb C^{\times}$. A presentation of $G$ is a homomorphism $\pi : G \to GL(V)$ for some finite-dimensional vector space. If $(\pi, V)$ is a representation of $G$, then the character of $\pi$ is the function $\chi_{\pi}(g) = \mbox{tr} \; \pi(g)$, i.e. the trace of the transformation $\pi(g)$. Now I am reading these lecture notes by Daniel Bump (the linear characters are introduced here ), where he says at the end of chapter 2.4 on characters: f $(\pi, V)$ is a representation, its character $\chi_{\pi}$ is the function $G \longrightarrow \mathbb{C}$ defined by $\chi_{\pi} (g) = \operatorname{tr} \; \pi (g)$. The characters are class functions, meaning that they are constant on conjugacy classes. If $h$ is the number of conjugacy classes of the group, the number of isomorphism classes of irreducible representations of $G$ is also $h$, and if representatives of these are $(\pi_i, V_i)$, let $\chi_i = \chi_{\pi_i}$. These are the irreducible characters of $G$. Among them are the linear characters, which are the homomorphisms $G \longrightarrow \mathbb{C}^{\times}$. I do not understand the last sentence "" Among them are the linear characters, which are the homomorphisms $G \longrightarrow \mathbb{C}^{\times}$ ."" In general for the trace we have $\operatorname{tr}(AB) \ne \operatorname{tr}(A)\operatorname{tr}(B)$, so I guess in general a function $\chi(g) = \operatorname{tr} \; \pi(g)$ could not be a homomorphism $G \to \mathbb C^{\times}$, so how is this statement meant? Or is the trace multiplicative in this special setting?","['representation-theory', 'characters', 'abstract-algebra', 'group-theory', 'finite-groups']"
1475347,Sinkhorn theorem for doubly stochastic matrices,"I was reading something about doubly stochastic matrices and got stuck while reading the original proof of the uniqueness part of the Sinkhorn theorem . I'm not able to understand the logic. Could someone help me in figuring it out? I state below the theorem and the proof as written in the original paper. Theorem: To a given strictly positive $N \times N$ matrix $A$ corresponds exactly one doubly stochastic matrix $T=D_1AD_2$ where $D_1$ and $D_2$ are diagonal matrices with positive diagonals. The matrices $D_1$ and $D_2$ are themselves unique up to a scalar factor. Uniqueness proof: if there exist two different pairs of diagonal matrices $C_1, C_2$ and $D_1, D_2$ such that both $C_1 A C_2$ and $D_1 A D_2$ are doubly stochastic, then this means that there exist a positive doubly stochastic matrix $P$ and matrices $$B_1 = \mbox{diag}[b_{11}, b_{12},\dots, b_{1N}]$$ and $$B_2 = \mbox{diag}[b_{21}, b_{22}, \dots, b_{2N}]$$ which are not multiple of identity matrix, for which $B_1 P B_2$ is also doubly stochastic. But this is impossible since by convexity one obtains: $$\min_jb_{2j} \le \frac{1}{b_{1i}} \le \max_j b_{2j}$$ and $$\min_ib_{1j}\le \frac{1}{b_{2j}} \le \max_jb_{1i}$$ which leads to a contradiction if $b_{1i} b_{2j} \ne 1$ for some $i$ and $j$. It follows that $C_1 = p D_1$, $C_2 = p^{-1} D_2$ for some $p > 0$. What I do not understand: why from the existence of $C_1,C_2$ and $D_1,D_2$ it will exist $P$, $B_1$ and $B_2$? What is the convexity argument? And why there is a contradiction? And how does it imply the thesis? Thanks.","['stochastic-matrices', 'matrices']"
1475354,Can a relation be both symmetric and antisymmetric; or neither? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Can some relation be at the same time symmetric and antisymmetric? And, can a relation be neither one nor the other?
Please give me an example for your answer.","['elementary-set-theory', 'examples-counterexamples', 'relations']"
1475373,"Consider $\mathsf{A,B,C},$ $\dots$ in a sans serif font. Each of these gives a graph in the plane. Sort these into homeomorphism classes.","Problem: Show that homeomorphism is an equivalence relation on topological spaces. Now consider the capital letters of the alphabet $\mathsf{A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z},$ in a sans serif font. Each of these gives a graph in the plane. Sort these into homeomorphism classes. (The partition may depend on the font! In particular, $\mathsf{K}$ can be tricky.) What I got so far: $\mathsf{C\cong G\cong I \cong J \cong L \cong U \cong V \cong Z}$ They are not closed curves and can be curved and bent to one another. $\mathsf{D\cong O}$ Closed curves that can easily be bent to one another. $\mathsf{C \ncong O}$ since removing a point in $\mathsf{C}$ makes it disconnected while $\mathsf{O}$ stays connected. $\mathsf{E\ncong O}$ for the same reason. $\mathsf{E\cong F}$ The standing line on $\mathsf{E}$ can be shrunk and the bottom line rotating and shrinking to make the straight standing line for $\mathsf{F}$. And the same reverse process for $\mathsf{F}$, which are homeomorphisms. $\mathsf{K\cong X}$, this is easy to see. and $\mathsf{K \ncong H}$ since deleting a point in $K$ gives 4 components but not in $H$. I don't see any congruent letters for $\mathsf{A}$ and $\mathsf{H,R}$, so I think each is alone. $\mathsf{P\cong Q}$ is clear. $\mathsf{E\cong F\cong T \cong Y}$ as well. $\mathsf{M\cong N\cong W}$ is also easy to see. Is this correct, I would appreciate any corrections.","['algebraic-topology', 'general-topology']"
1475378,What is the probability that nobody is born in the same month?,"You have 12 people in a room, what is the probability that nobody is born in the same month? So far i have: $\frac{12!}{12^{12}}$ but i am not sure if this is right. If anyone could confirm this is the way to go or tell me where i am wrong it would help me alot. Thank you.",['probability']
1475435,Principal branch of logarithm,"I need to compute $\log(1+i)$ and $\text{Log}(1+i)$, $\text{Log}$ meaning the principal branch. What I do is to express $(1+i)$ in polar coordinates, then equate it with $$\sqrt {2}(\cos \pi/4+ i\sin \pi/4)= \sqrt{2}\mathrm{e}^{\frac{\pi i}{4}}$$ Is this true? Can I now take $\log \sqrt{2}\mathrm{e}^{\frac{\pi i}{4}}=\frac{\sqrt{2}\pi i}{4}$? And now $\text{Log}(1+i)=\log|\sqrt{2}|+\frac {\pi i}{4}+ 2\pi k$? Can someone please tell what is wrong with my computation? What I am really confused about is how different it is to compute the principal brach and the other Help would be appreciated!","['complex-analysis', 'complex-numbers', 'logarithms']"
1475451,Providing counterexamples to the claim: If $|A \cap B| < |A|$ then $|A|>|B|$,"I'm having problems answering the following question: Give a counter example to show that the following
  statement is false: For any sets $A$ and $B$, if $|A \cap B| < |A|$ then $|A|>|B|$ Presumably we can't be sure whether $|A|>|B|$ as we don't know how many elements there are in $B$ which aren't shared with $A.$","['elementary-set-theory', 'examples-counterexamples', 'logic']"
1475469,Solve the recurrence relation $a_n = 4a_{n-1} - 3a_{n-2} + 2^n $,"Solve the recurrence relation $$a_n = 4a_{n-1} - 3a_{n-2} + 2^n $$ With initial conditions: $a_1 = 1$ $a_2 = 11$ I have done similar recurrence relation problems to this, but none that were a non-homogeneous recurrence relation such as this one. So far I have: $$r^n = 4^{n-1} - 3^{n-2} $$ Divide both sides by $$\frac{1}{r^{n-2}}$$ Giving me this as my Auxiliary Equation: $$ r^n -  4r + 3 = 0  $$ I then solved for the $r$ values and got $r = -4$ and $r = 1$ I am stumped from here as to where the non-homogeneous piece comes into play, any help is appreciated.","['recurrence-relations', 'discrete-mathematics']"
1475495,"Real valued function, $M$, $s_n \rightarrow s; |f(s_n)| \le M$ for $s$ in open $U$. (Problem from Gamelin and Greene)","Gamelin and Green, Introduction To Topology Ch. 1 section 3 problem 10. Let $f$ be a real-valued function  on $\mathbb{R}$, the real numbers.  Show that there exist $M \gt 0$ and a nonempty open subset $U$ of $\mathbb{R} $ such that for any $s \in U$ , there is a sequence $\{s_n\}$ satisfying $s_n \rightarrow s$ and $|f(s_n)| \le M, n \ge 1$. I intuitively get this.  Every point in an open set has a sequence converging to it, so this is just saying if any $s_n$ are ""too big"" I can find another $s'_n$ close by that isn't ""too big"". ($s$, itself, can be as big as it wants.) This is obviously true if $f$ is continuous or even just bounded on any open interval. (Just let U be the image $f(A)$ of any open interval, $A$, and let $M = \sup f(A)$). For this not to be true, $f$ needs to be unbounded on every possible open set. Not merely unbounded which is easily possible but.... If $V_M =\{x : f(x) \ge M\}$ will always always be open no matter how large the $M$.  Which seems intuitively impossible.  But, I don't have it. At least I don't with concepts of the first 15 pages of the book. (Metric spaces, closed and open sets, convergent sequences, reals having least upper bound property, but not compactness; it's a pretty dense book.)",['general-topology']
1475501,Prove the probability measure of $X=Y$ is $0$,"Let $(X,Y)$ be a random variable that takes values in $\mathbb{R}^2$. We say $X$ and $Y$ are independent if $E(f(X)g(Y))=Ef(X)Eg(Y)$. Prove that if $P(X=a)=0$ for all $a\in\mathbb{R}$, then $P(X=Y)=0$. My work: We can define $\mu(A)=P(X\in A)$ and $\nu(B)=P(Y\in B)$ as probability measure. Then, one has $P(X=a)=\mu(a)=0$. But I have trouble to prove $P(X=Y)=P(X=a,Y=a)=\mu(a)\nu(a)=0$, so we do not know whether two variables are independent or not. Can someone give me hints?","['probability-theory', 'independence', 'measure-theory']"
1475551,"Prove that if $|S| \ge 2^{n−1} + 1$, then $S$ contains two elements which are disjoint from each other.","I'm trying to use the pigeonhole principle to prove that if $S$ is a subset of the power set of the first $n$ positive integers, and if $S$ has at least $2^{n-1}+1$ elements, then $S$ must contain a pair of pairwise disjoint sets. I've been playing around with a small set $\{1,2,3\}$ and I can see the theorem seems to be true in this case. But I can't see, for instance, how to construct the pigeonholes so that $\{2\}$ and $\{3\}$, for instance, end up in the same pigeonhole. I've been looking at which subsets of the power set the elements in $S$ do NOT contain, to no avail. Can someone give me a hint about which pigeonholes to look at? I'm adding this in response to the comments below: Demonstrate that it is possible for |S| (i.e., the number of subsets of {1, . . . , n} contained as elements of S ) to equal 2n−1 without any two elements of S being disjoint from each other.   I can do this. This provides context.","['discrete-mathematics', 'elementary-set-theory', 'pigeonhole-principle', 'combinatorics']"
1475565,Intuition for Inclusion-Exclusion Principle,"Many of us are familiar with the inclusion-exclusion principle . I think the principle makes total sense when applied to the two or three sets and we have the following: $|A\cup B|=|A|+|B|-|A\cap B|$ $|A\cup B\cup C|=|A|+|B|+|C|-|A\cap B|-|A\cap C|-|B\cap C|+|A \cap B \cap C|\text{.}$ However, it is not as easy to understand how this works in the general case. In lieu of a rigorous proof, it is easy to see that the IEP rests on the following principle: suppose that $x$ is a member of $n$ sets. Then $x$ gets counted $n$ times on the first count, subtracted $n$ choose $2$ times on the second count, added back in $n$ choose $3$ times on the third count, etc . In other words: $${n \choose 1}-{n\choose 2}+{n\choose 3}-{n\choose 4}+\cdots+(-1)^{n+1}{n \choose n}=1$$ Or, perhaps more appropriately written as $${n\choose 0}-{n \choose 1}+{n\choose 2}+\cdots+(-1)^{n}{n \choose n}=0$$ I should clarify that the proof of this is easy to do algebraically , but I am looking for a useful intuitive explanation of the above property, and I'm curious how people view the IEP from a combinatorial perspective.","['inclusion-exclusion', 'intuition', 'combinatorics']"
1475577,There exists a divisor linearly equivalent to $P$ not containing $P$,"Let $X$ be a non-singular projective curve over $\mathbb C$ and let $P\in  X$ be a closed point. Which is, in your opinion,  the easiest way to prove that there exists a divisor $D$ of $X$ satisfying the following conditions: $D$ is linearly equivalent to $P$. $D$ doesn't contain $P$. Thank you in advance.","['algebraic-geometry', 'schemes', 'divisors-algebraic-geometry', 'algebraic-curves']"
1475580,Help showing $\lim\limits _{x\to -\infty \:}\left(\sqrt{4\cdot \:x^2-5\cdot \:x}+2\cdot \:x\right) = \frac{5}{4}$,"I am stuck solving the following limit. I know the answer is 5/4, I just can't get it. This is the steps I have done so far. $\lim _ \limits{x\to -\infty \:}\left(\sqrt{4\cdot \:x^2-5\cdot \:x}+2\cdot \:x\right)$ Multiply by Conjugate $\lim _ \limits{x\to -\infty \:}\left(\sqrt{4\cdot \:x^2-5\cdot \:x}+2\cdot \:x\right)\cdot \frac{\left(\sqrt{4\cdot \:\:x^2-5\cdot \:\:x}-2\cdot \:\:x\right)}{\left(\sqrt{4\cdot \:\:x^2-5\cdot \:\:x}-2\cdot \:\:x\right)}$ Multiply Out $\lim\limits_{x\to -\infty \:}\cdot \frac{\left(4\cdot \:\:\:x^2-5\cdot \:\:\:x-4\cdot \:\:x\right)}{\left(\sqrt{4\cdot \:\:x^2-5\cdot \:\:x}-2\cdot \:\:x\right)}$ Combine Like Terms $\lim\limits_{x\to -\infty \:}\cdot \frac{\left(4\cdot \:\:\:x^2-9\cdot \:\:x\right)}{\left(\sqrt{4\cdot \:\:x^2-5\cdot \:\:x}-2\cdot \:\:x\right)}$ Factor out x $\lim\limits_{x\to -\infty \:}\cdot \frac{x\left(4\cdot \:x-9\right)}{\left(\sqrt{x^2\left(4-\frac{5}{x}\right)}-2\cdot \:\:x\right)}$ Pull out x of sqrt and factor again $\lim \limits_{x\to -\infty \:}\cdot \frac{x\left(4\cdot \:x-9\right)}{x\left(\sqrt{\left(4-\frac{5}{x}\right)}-2\right)}$ Now cancel x terms $\lim \limits_{x\to -\infty \:}\frac{4\cdot \:\:x-9}{\sqrt{\left(4-\frac{5}{x}\right)}-2}$ Now I don't know what to do next. 
If I plug in I get $\frac{4\cdot \:\:\:-\infty \:-9}{\sqrt{\left(4-0\right)}-2}=\:\frac{-\infty \:}{0}$ Which doesn't equal $\frac{5}{4}$?","['calculus', 'limits']"
1475595,"Suggestion for reference book for differential forms, differentiable manifolds and other topics","I am currently taking a course on multivariable calculus and our professor is following the book by Do Carmo: Differential forms and applications. I feel the text is too rigorous, which I really appreciate and it appeals to me as well, but it is making things quite difficult for me to understand.Definitions and theorems are becoming increasingly complex. Could anyone suggest a book I could use together with Do Carmo's book so that I am able to understand the text better. My goal is to master the material presented in Do Carmo's book.As of now we have completed the first three chapters from the text which are- Differential forms, Line integrals and Differentiable manifolds. 
Thanks.","['differential-forms', 'reference-request', 'manifolds', 'book-recommendation', 'multivariable-calculus']"
1475604,Continuity of $f^{-1}$,So I read this theorem: Suppose $f$ is a continuous $1-1$ mapping of a compact metric space $X$ onto a metric space $Y$. Then the inverse mapping $f^{-1}$ defined on $Y$ by $$ f^{-1}(f(x))=x \quad x\in X $$ is a continuous mapping of $Y$ onto $X$. Can someone please explain why compactness of $X$ is necessary here? I mean why is it not possible for $f^{-1}$ to be continuous without $X$ being compact?,"['metric-spaces', 'general-topology']"
1475625,How do different inner products give different angles?,"I know that for each inner product $\langle , \rangle_{A}$ on $\mathbb{R}^n$, there is an associated positive definite symmetric matrix $A$ so that $\langle x,y \rangle = x^{T}Ay$.  I was wondering if it is possible to find a function relating the angles between $x,y$ in $\mathbb{R}^n$ with the dot product, to the angle in another inner product space. For example a function $f(\theta,A)$, so that if $\theta$ is the dot product between $x$ and $y$ in $\mathbb{R}^n$, then $f(\theta,A) = \langle x,y \rangle_A$","['geometry', 'linear-algebra', 'inner-products']"
1475633,Why is it wrong to derive the chain rule this way?,"My book says that the chain rule can stated as $$\dfrac{dy}{dx} = \dfrac{dy}{dt} \dfrac{dt}{dx}$$ However, it the book says that it is incorrect to reason that the chain rule is true because the $dt$'s cancel out. Why is it incorrect? I've heard people saying that differentials are not really numbers, but I don't know what to think because my book had just finished a section where it defined $dy$ and $dx$ as $(y-\Delta y)$ and $(x-\Delta x)$ respectively, and the book even used values such as .01 to substitute in place of these differentials.","['calculus', 'chain-rule', 'multivariable-calculus', 'ordinary-differential-equations', 'derivatives']"
1475634,$3\times3\times3$ hypermatrix multiplication,"Here's an image showing what I am trying to do: The two hypermatrices are multiplied together by taking appropriate slices from each hypermatrix, and realised into a vector by an associated vector, given by ( a , b , c ) in this case. But I can't figure out how to do the simple matrix multiplication. For example red times green times vector equals blue, but there doesn't seem to be an obvious orientation for the matrices.",['matrices']
1475654,"In a game of drawing straws, why are all turns equally good?","For example, there are 3 straws in a pile - 2 long and 1 short. The person who draws the shortest straw loses. When a straw is drawn, it is removed from the pile. Drawing first, second, or last all apparently result in the same probability for drawing the shortest straw. For me, this is counter-intuitive. I originally thought that drawing the straw later would be more beneficial because the previous drawers would have to not draw the short straw before you have a chance to. Is there a reason why drawing on all turns results in the same probability for drawing the short straw?","['probability', 'game-theory']"
1475664,Isometries preserve inner product in tangent spaces,"I'm trying to prove that given two surfaces $S_1$ and $S_2$ and an isometric map $f:S_1\to S_2$, the maps $df_p:T_pS_1\to T_pS_2$ preserve inner products for any $p\in S_1$, i.e., 
$$\langle df_p(x),df_p(y)\rangle_{T_pS_2}=\langle x,y\rangle_{T_pS_1}$$
for any two $x,y\in T_pS_1$. I don't know very well where to start from, and there is not too much information of this results in many of the classical books (they seem to assume this result as trivial, but it is not trivial at all for me). By the way, the definition I have of isometric maps is: A diffeomorphism $f:S_1\to S_2$ is said to be isometric if the length of any curve $\gamma$ in $S_1$ is the same length of the curve $f\circ\gamma$","['differential-geometry', 'isometry']"
1475665,Calculate a cubic curve from two slopes and two points,"I have a question that is supposed to be solved using derivatives. The question asks to find the cubic polynomial function $f(x)$ where $y=13(x-1)+4$ is tangent to $f(x)$ at $(1,4)$ and $y=(x+1)+6$ is tangent to $f(x)$ at $(-1,6)$ How do I go about finding the function $f(x)$ using this information and the fact that this is a question specifically about derivatives?","['polynomials', 'derivatives']"
1475681,Graph coloring - Cutting and stiching,"I'm doing a university assignment and I thought I was good at graph coloring until I attempted these questions. Let Cn be the cycle graph with n vertices and n edges and let Pn(x) be its chromatic polynomial. (a) Explain why P3(x) = x (x − 1) (x − 2). So this is quite simple, as all edges are attached so by the time you get to the last vertex you can use x-2 colors, so I'm alright with this. (b) If n > 3 , describe the two graphs that are obtained when one ‘cuts Cn open along an edge’ and then ‘stitches it back up.’ This is what has me stumped, n > 3 allows for an infinite amount of answers? If someone could explain to me how to answer this, that would be awesome. If I could understand this question I could answer C, D. (c) Hence find an expression for Pn(x) in terms of Pn−1(x) and another polynomial. (d) Hence prove by Induction that Pn(x) = (x − 1)n + (−1)n(x − 1) for all n ≥ 3 (e) In how many ways can the cycle graph with 11 vertices be colored using 10 colours? Any help  you can provide would be greatly appreciated.","['coloring', 'graph-theory', 'discrete-mathematics']"
1475682,Construct a Second Order ODE given the fundamental solutions,"I need to construct a second order linear differential equation for which $\{ \sin (x), x \sin (x) \}$ is the set of fundamental solutions.
I am completely lost on this problem and have been trying different approaches for days now. First, I tried simple variations of $ y'' + y = 0$ but I couldn't come up with anything successful. Then I tried using the identity $$ \dfrac{dW}{dx} + PW = 0 $$ and solving for $~P~$ , knowing that the Wronskian $~W~$ of the solution set is $ \sin^2 (x) $ . I got something like $ P = -\cot(x) $ so I tried variations of $$ y'' -\cot(x)y' + y = 0 $$ but again nothing worked.
So I started looking through my book over and over to see what types of ODE's could produce such a solution set, and from what I can tell, there are no constant-coefficient ODE's that can possibly produce this solution set. The only way I learned to solve variable coefficient second order equations was by Cauchy-Euler's method, but I don't see how that can output such a solution set either. I've never come across a second order ODE that has the solution $ \sin(x) $ without $ \cos(x) $ . 
Any ideas?","['wronskian', 'ordinary-differential-equations']"
1475712,Finding abs max with trig function,"I have $g(x)=\sqrt{3}-2\cos(x)$ on $[0,\pi]$. I did $g'(x)=-(-2\sin(x))=2\sin(x)$ and then $\pi$ and $0$ as the critical numbers. I evaluated the original function at each of the critical numbers and got 
\begin{eqnarray}
g(0)&=&-0.2679\\
g(\pi)&=&3.732\\
g(2\pi)&=&-0.2679
\end{eqnarray} That would put $3.732$ as the abs max but that's not right. What am I doing wrong here?","['optimization', 'trigonometry', 'calculus', 'derivatives']"
1475726,Locating three sets of collinear points,"Given any three distinct points $A,B,C$ and a circle $C(O)$, construct points $D,E,F$ on the circle such that $A,D,E$ are collinear, $B,E,F$ are collinear and, $C,F,D$ are collinear. One such solution is indicated on the diagram below. I have enough analytic and numerical evidence to indicate that these points exists. In fact, there are two such sets of points, as shown by @coproc below. However I would like a geometric construction. One idea: My idea was to invert the points $A,B,C$ in the circle to find points $A',B',C'$ respectively. The problems then becomes equivalent to the following problem. Given three points $A',B',C'$ and a circle $C(O)$. Construct three circles such that One circle passes through points $A'$ and $O$, another passes through points $B'$ and $O$, the third passes through points $C'$ and $O$ and, the three circles intersect pairwise on $C(O)$. The required points $D,E,F$ are just the points of intersection of these three circles.","['euclidean-geometry', 'geometry', 'circles', 'geometric-transformation']"
1475732,Verify the following trigonometric identity.,"Verify the following trig identity: $$\sin(3\theta)-\sin\theta = 2\cos(2\theta)\sin\theta$$ Here is my work so far. $\sin(3\theta)-\sin\theta = 2\cos(2\theta)\sin\theta$ LHS:$$\sin(\theta+2\theta)-\sin\theta$$
$$\sin\theta \cos(2\theta)+\sin(2\theta)\cos\theta-\sin\theta$$
$$\sin\theta \cos(2\theta)+(2\sin\theta \cos\theta)\cos\theta-\sin\theta$$ Where do I go from here? I think I should leave the first term in the line above as is, and try and manipulate the second two terms to equal $\sin\theta \cos(2\theta)$, then the LHS will add together to equal $2\cos(2\theta)\sin\theta$, and the identity will be verified. How do you suggest I get there? Any hints or advice would be appreciated. EDIT: I ended up verifying this identity using the identity frank000 mentioned in comments. Thanks to everyone for the input, it was all very helpful.",['trigonometry']
1475756,Question about Gysin map (pushforward in cohomology),"The setting is the following: $X \subset \mathbb{CP}^r$ is an algebraic subvariety of dimension $n$, codimension $e=r-n$, and degree $d$. Call $j$ the inclusion. Then, Poincaré duality induces a map 
\begin{equation}
j_*:H^i(X,\mathbb{C}) \rightarrow H^{i+2e}(\mathbb{CP}^r,\mathbb{C}).
\end{equation}
Denote by $\xi \in H^2(\mathbb{CP}^r,\mathbb{C})$ the hyperplane class and by $\psi$ its pullback to $X$. Then I am given two identities: (1) $j_*j^*(x)=x \cup (d \xi^e)$; (2) $j^*j_*(y)=y\cup(d \psi^e)$. While I see that the first one comes from projection formula and the class of $X$ being $d \xi ^e \in H^*(\mathbb{CP}^r,\mathbb{C})$, I can not figure out the second one. I have been pushing symbols for a while, but without success. The only thing I got is that both sides of (2), if capped with $[X]$ and pushed forward, give the same thing. How can I show the second relation?","['complex-geometry', 'intersection-theory', 'homology-cohomology', 'algebraic-geometry', 'algebraic-topology']"
1475777,Bayes Theorem/Law of total probability question.,"I'm having a hard time building intuition behind some Bayes Theorem/Law of total probability problems and understanding why my attempts are incorrect in the first place, for this question in particular: Question: It is believed that one percent of children have autism. A test for autism has been developed whereby 90% of autistic children are correctly identified as having autism but 3% of non-autistic children are incorrectly identified as having autism. A child is tested at two independent clinics.
What is the probability that the two clinics have the same diagnosis? Attempt at solving the problem: Let $A$ be the event that a child has autism, and $B$ the event where a child is tested positive for autism (in a clinic). We are given: $P(A)=0.01$ (one percent of children have autism). $P(B|A)=0.9$ and $P(B|\overline{A})=0.03$ (corresponds to 90% of autistic children having a positive test result and 3% of non-autistic children having a positive test result) Let $1$ and $2$ denote the clinics for which a child is tested. Then the probability that both clinics yield the same diagnosis is: $P((B_{1}\cap B_{2})\cup(\overline{B_{1}}\cap \overline{B_{2}})) = P(B_{1}\cap B_{2}) + P(\overline{B_{1}}\cap \overline{B_{2}})$(*) where $B_i=B$ for $i=1,2$ (This should follow from the fact that both clinics receive the same probabilities for diagnosing a child) At this point I had two different ways to approach the problem, I could either express (*) as: $P(B_{1}\cap B_{2}) + P(\overline{B_{1}}\cap \overline{B_{2}}) = P(A)P(B_{1}\cap B_{2}|A)+P(\overline{A})P(B_{1}\cap B_{2}|\overline{A})+P(A)P(\overline{B_{1}}\cap \overline{B_{2}}|A)+P(\overline{A})P(\overline{B_{1}}\cap \overline{B_{2}}|\overline{A})$ (follows from the law of total probability) (Knowing $B_1$ and $B_2$ are independent, it follows I can express $P({B_{1}}\cap {B_{2}}|A)$ as $P({B_{1}}|A)P({B_{2}}|A)$, where this approach leads me to the 'correct' answer) or I could compute $P(B)=P(A)P(B|A)+P(\overline{A})P(B|\overline{A})$ and express (*) as: $P(B_{1}\cap B_{2}) + P(\overline{B_{1}}\cap \overline{B_{2}}) = P(B_{1})P(B_{2})+P(\overline{B_{1}})P(\overline{B_{2}})$ (this follows from the clinics being independent and computing B from the law of total probability) Why does computing the probability in this second approach lead to me to the wrong answer?","['probability', 'discrete-mathematics', 'bayes-theorem']"
1475782,Is it true that $\sum_{n = 0}^\infty \frac{a_n}{n}$ is also convergent? [duplicate],"This question already has answers here : If $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$ (6 answers) $\sum \limits_{n=1}^{\infty}{a_n^2}$ converges $\implies \sum \limits_{n=1}^{\infty}{\frac{a_n}{n}}$ [duplicate] (1 answer) Closed 8 years ago . Let $\{a_n\}_{n = 0}^\infty$ be a sequence of real numbers such that the series $\sum_\limits{n = 0}^\infty |a_n|^2$ is convergent. Is it true that $\sum_\limits{n = 0}^\infty \dfrac{a_n}{n}$ is also convergent? My try : Since $\sum_\limits{n = 0}^\infty |a_n|^2$ is convergent, $$\begin{align}\lim_\limits{n\to \infty} |a_n|^2=0 &\implies \forall \epsilon >0 \quad \exists N\in \mathbb N \quad \forall n>N \quad|a_n|^2<\epsilon \\ &\implies \forall n>N \quad |a_n|<\sqrt \epsilon \\ &\implies -\sqrt\epsilon<a_n<\sqrt \epsilon  \end{align}$$ Hence $\lim _\limits{n\to \infty} \dfrac{a_n}{n}=0$. Now $S_n=\sum_\limits{i=1}^n \dfrac{a_i}{n}\implies |S_n|\leq \sum_\limits{i=1}^n \dfrac{|a_i|}{n}\leq \sum _{i=1}^n |a_i|$. Am I heading in the right way or there is any counter-example? How to complete the proof from here? Please help.","['sequences-and-series', 'real-analysis']"
1475797,Find a function smooth at one isolated point,The definition of $C^\infty$ at one point $x$ of a function $f$ is that the derivative $D^{(k)}$ of arbitrary order $k$ exists in a neighbourhood of $x$. But we may have a smaller and smaller neighbourhood as $k$ increases. So whether there is a function that is $C^{\infty}$ at one point $x$ but not in any small deleted neighbourhood $U$? I want to construct it by convergence but don't know where to start.,"['derivatives', 'functions']"
1475803,"How show that $|a_{n}-1|\le c\lambda ^n,\lambda\in (0,1)$","I'm looking for proof that Let $a_{0},a_{1}>0$,and such
$$a_{n+2}=\dfrac{2}{a_{n+1}+a_{n}}$$ show that: there exist $\lambda,c>0$ such
$$|a_{n}-1|\le c\lambda ^n,\lambda\in (0,1)$$ I tried using induction, but i'm not sure how it would work ,Thanks in advance.",['sequences-and-series']
1475827,Is there a non-constant function $f$ such that $f'(x) = f(x - 1)$?,"In discrete calculus, where the difference operator $\Delta f = f(x + 1) - f(x)$ takes the place of $\frac{d}{dx}$, Fibonacci sequences are given by the functions satisfying: $$
\Delta f(x) = f(x - 1)
$$ Is there a non-constant function such that $\frac{d}{dx}f(x) = f(x - 1)$? If it exists, it would be the ""continuous analogue of the Fibonacci sequence"" in this sense, which seems cool.","['discrete-calculus', 'calculus', 'fibonacci-numbers']"
1475911,"To integrate a function $f:\Bbb C \to \Bbb C$, do I need the domain of integration to be a curve?","To integrate a function $f:\Bbb C \to \Bbb C$,  do I need the domain of integration to be a curve? Isn't it possible to do something like $$\int_\Bbb C f(z)dz?$$ Consider the function $e^{-z^{10}}$, can we calculate $\int_\Bbb C e^{-z^{10}}dz$? To be extremely clear, I'm not interested in how to calculate the integral for that especific function, but on how it would be done. I would also like to get a bit of clarification on what things like $dz\wedge d\overline z$ mean.","['complex-analysis', 'integration']"
1475913,What's the value of $\frac{(x-1)x^2}{x-1}$ when $x=1$?,"Consider the function 
$$f(x)= \frac{(x-1)x^2}{x-1},\quad x \in \Bbb R\ .$$
This function gives simply gives $f(x)=x^2$ by cancelling the term $x-1$, if I am not wrong. The variable here is $x$ which can take arbitrary value, we just cancel that term as for any $x$, suppose $x=0,2,3,4, \ldots,-1,-2,-3,\ldots$ then the term is going to get cancelled, but what about the value $1$? If we have $x=1$ there then the term would not be defined, so why do we always cancel the similar terms without thinking what values variable takes? 
I know this question is really very strange but I want to be clear about my approach.
This question is somewhat which I never asked before.","['indeterminate-forms', 'functions']"
1475921,Is the Intersection of Countably Many Countable Sets Countable?,"Is the intersection of countably many countable sets countable?  My thinking is yes, because the intersection of two sets results in a set of size less than or equal to the largest set.  My other question, is the intersection of countably many countable sets recursively enumerable?  I can imagine writing a program that lists the members of the union of countably many infinite sets by diagonal enumeration, but I can't say the same for intersection.  My assumption is that this is not possible because we could never check to see whether the first element of the first set belongs to all the sets.",['elementary-set-theory']
1475930,How should I calculate $\lim_{n\rightarrow \infty} \frac{1^n+2^n+3^n+...+n^n}{n^n}$ [duplicate],"This question already has answers here : How to evaluate $ \lim \limits_{n\to \infty} \sum \limits_ {k=1}^n \frac{k^n}{n^n}$? (6 answers) Closed 5 years ago . How should I calculate the below limit $$\lim_{n\rightarrow \infty} \frac{1^n+2^n+3^n+...+n^n}{n^n}$$
I have no idea where to start from.","['sequences-and-series', 'limits', 'real-analysis']"
1475931,"If $\lim_{x\to 1}\frac{F(x)}{G(x)}=\frac{1}{14},$then find the value of $f(\frac{1}{2})$","Let $f:R\to R$ be a continuous odd function,which vanishes at exactly one point and $f(1)=\frac{1}{2}$.Suppose that $F(x)=\int_{-1}^{x}f(t)dt,\hspace{1cm}\forall x\in[-1,2]$,and $G(x)=\int_{-1}^{x}t|f(f(t))|dt\hspace{1cm} \forall x\in[-1,2]$.If  $\lim_{x\to 1}\frac{F(x)}{G(x)}=\frac{1}{14},$then find the value of $f(\frac{1}{2})$ This question was asked in JEE Advanced.I reached near completion but has one doubt. $\lim_{x\to 1}\frac{F(x)}{G(x)}$ is in $\frac{0}{0}$ form.As $F(1)=\int_{-1}^{1}f(t)dt=0,f(x)$ being a odd function and $G(1)=\int_{-1}^{1}t|f(f(t))|dt=0$.As the integrand being an odd function. Then i applied L Hospital rule,limit becomes $\lim_{x\to 1}\frac{F'(x)}{G'(x)}$,where F'(x) and G'(x) are derivatives of F(x) and G(x) respectively. $\lim_{x\to 1}\frac{F'(x)}{G'(x)}=\lim_{x\to 1}\frac{f(x)}{x|f(f(x))|}=\frac{f(1)}{|f(f(1))|}=\frac{1}{2|f(\frac{1}{2})|}=\frac{1}{14}$ $|f(\frac{1}{2})|=7\Rightarrow f(\frac{1}{2})=\pm 7 $ But in the answer only $7$ is given.I am not clear why have they ignored $-7$.Please help me.","['limits', 'functions']"
1475933,"A simple question about the solution to $mx''=-kx$, mass on a spring","We did this in class, and we found the solution to $m\ddot x=-kx$. I am fine with the whole solution, i.e.: -let $\omega ^2= k/m$ to turn the problem into $\ddot x+\omega ^2x=0$ ...(1); -Guess that the solution is of the form $x(t)=Ae^{\lambda t}$ where $A$ and $\lambda$ are constants -calculate $x$ and $\ddot x$ and plug that into (1) and find the zeros of the auxiliary polynomial to get $\lambda = ^{+}_{-}i\omega t$ So now we have two linearly independent solutions: $x_{1}(t)=A_{1}e^{i\omega t}$ and $x_{2}(t)=A_{2}e^{-i\omega t}$ so the general solution is $x(t)=A_{1}e^{i\omega t}+A_{2}e^{-i\omega t}$ From here, the professor explained that since $e^{i\omega t}=\cos(\omega t)+i\sin(\omega t)$ and $e^{-i\omega t}=\cos(\omega t)-i\sin(\omega t)$, the general solution then becomes $x(t)=B_1\cos(\omega t)+B_2\sin(\omega t)$ Here is my question: when I tried doing this at home, I couldn't understand how the $i$ disappeared from the $\sin(\omega t)$ term. Any explanations? What I did that led me to this problem was: By the given equations for $e$, I got: $x(t)=A_1\cos(\omega t)+A_1i\sin(\omega t)+A_2\cos(\omega t)-A_2i\sin(\omega t)=(A_1+A_2)\cos(\omega t)+(A_1-A_2)i\sin(\omega t)=B_1\cos(\omega t)+B_2i\sin(\omega t)$ where $B_!=A_1+A_2$ and $B_2=A_1-A_2$","['calculus', 'ordinary-differential-equations', 'integration']"
1475958,Minimum possible order of a group with elements of order 1 to 5,"If G is a group containing elements of order 1 to 5, then what is the minimum possible order of this group? My answer: We know that the order of the elements of a group divides the order of group, so the minimum possible order of G would be the l.c.m of 1, 2, 3, 4 & 5 which is 120. I just wanted to confirm if I am right.",['group-theory']
1476010,"Computing the dimension of a vector space of matrices that commute with a given matrix B,","This is part $2$ of the question that I am working on. For part $1$, I showed that the space of $5\times 5$ matrices which commute with a given matrix $B$, with the ground field = $\mathbb R$ , is a vector space. But how can I compute its dimension? Thanks,","['vector-spaces', 'linear-algebra']"
1476223,"Find $\lim\limits_{(x,y) \to(0,0)} \frac{e^{xy} -1}{ x^2 + y^2} $","I need to prove that the limit does not exits Find $\lim\limits_{(x,y) \to(0,0)} \frac{e^{xy}-1}{ x^2 + y^2} $ I tried finding the limit as f(x) approaches from x and y axes but I still get an indeterminate answer.","['multivariable-calculus', 'limits']"
1476234,Are relative uncertainties additive?,"I'm given a quantity which is defined as $$a=\frac{bcd}{ef}$$
and I know the relative uncertainties in each of $b,c,d,e,f$. I'm supposed to find relative uncertainty in $a$. As far as I have learnt, we have $$\%\frac{\Delta a}{a}=\%\frac{\Delta b}{b}+\%\frac{\Delta c}{c}+\%\frac{\Delta d}{d}+\%\frac{\Delta e}{e}+\%\frac{\Delta f}{f}$$
The $\%$ sign indicates that the relative uncertainties are in percentage. However, is this actually correct, that is, are relative uncertainties really additive? I couldn't find a good reference which explicitly states so.","['percentages', 'statistics', 'algebra-precalculus']"
1476235,Does every torus $T\subset S^3$ bounds a solid tours $S^1\times D^2\subset S^3$?,"I want to show this by using Alexander's Theorem's proof method.
So here's what I thought. As I surger $T$, I have 2 $S^2$. So one bounds $S^1$ and the other bounds $D^2$. 
By reversing the surgery, I have that $T$ bounds a solid torus $S^1\times D^2$ in $S^3$. Is it correct?","['manifolds', 'general-topology', 'geometric-topology']"
1476310,Function continuity $(x^2 - 1)/( x - 1)$,"What can you say about the function continuity of $$\frac{x^2 - 1}{x - 1}$$ at $x = 1$? There is an asymptote at $x = 1$, however the limit as $x$ goes to $1$ becomes $2$ because $f(x) = x + 1$ after factoring, also the limit becomes $2$ because we again factor, so in total by definition the function is continuous, however if we draw the graph then there is an asymptote at $x = 1$, so it's discontinuous. So in then final result what can we say about the continuity of this function?","['continuity', 'limits', 'functions']"
1476313,Simplify the fraction with radicals,I want to simplify this fraction $$ \frac{\sqrt{6} + \sqrt{10} + \sqrt{15} + 2}{\sqrt{6} - \sqrt{10} + \sqrt{15} - 2} $$ I've tried to group up the denominator members like $ (\sqrt{6} + \sqrt{15}) - (\sqrt{10} + 2) $ and then amplify with $ (\sqrt{6} + \sqrt{15}) + (\sqrt{10} + 2) $,"['fractions', 'radicals', 'algebra-precalculus']"
1476323,the $\sigma$-algebra on $\mathbb{R}$ generated by the collection of all one-point subsets,"Is my proof correct? Let $\mathscr{A}$ be the collection of all subsets $A$ of $\mathbb{R}$ such that either $A$ or $A^{c}$ is countable, and let $\mathscr{B}$ be the collection of all singleton subsets of $\mathbb{R}$. It is clear that $\mathscr{B}$ is a subcollection of $\mathscr{A}$, so that the $\sigma$-algebra $\sigma(\mathscr{B})$ generated by $\mathscr{B}$ is contained in $\mathscr{A}$, since $\mathscr{A}$ is a $\sigma$-algebra. Now any $A\in \mathscr{A}$ is either countable in which case $A$ will be a member of $\sigma(\mathscr{B})$, or its complement $A^{c}$ is countable in which case $A^{c}$ will be a member of $\sigma(\mathscr{B})$,and since $\sigma$-algebras are closed under complementation it follows that $A$ will be a member of $\sigma(\mathscr{B})$. Hence $\mathscr{A}$ is a subcollection of $\sigma(\mathscr{B})$, and we conclude that $\sigma(\mathscr{B})=\mathscr{A}$.","['proof-verification', 'real-analysis', 'measure-theory']"
1476370,"a proof that the borel $\sigma$-algebra on $\mathbb{R}$ is generated by the collection of intervals $(-\infty,b]$ ,$b$ being a rational number","Is my argument correct? Let $\mathscr{B}(\mathbb{R})$ denote the Borel subsets of $\mathbb{R}$,and let $\mathscr{A}$ be the $\sigma$-algebra generated by the collection of intervals of the form $(-\infty,b]$ for which $b$ is a rational number. It is known that $\mathscr{B}(\mathbb{R})$ is generated by the collection of half-open intervals of the form $(-\infty,r]$ where $r$ is a real number. It is clear that $\mathscr{B}(\mathbb{R})$ contains $\mathscr{A}$. Now each interval $(-\infty,m]$ where $m$ is an irrational number is equal to the intersection of a collection of intervals of the form $(-\infty,q_n]$ where ${q_n}$ is a decreasing sequence of rational numbers approaching $m$ (such a sequence exists because $\mathbb{Q}$ is dense in $\mathbb{R}$). Since $\mathscr{A}$ is a $\sigma$-algebra it follows that $(-\infty,m]$ lies in $\mathscr{A}$ and hence $\mathscr{B}(\mathbb{R})$ is a subcollection of $\mathscr{A}$, i.e. $\mathscr{B}(\mathbb{R})=\mathscr{A}$.","['proof-verification', 'measure-theory']"
1476371,$\epsilon$ - $\delta$ definition of a limit - smaller $\epsilon$ implies smaller $\delta$?,"The definition in my book is as follows: Let $f$ be a function defined on an open interval containing $c$ (except possibly at $c$) and let $L$ be a real number. The statement $$\lim_{x \to c} f(x) = L$$ means that for each $\epsilon>0$ there exists a $\delta>0$ such that if $0<|x-c|<\delta$, then $|f(x)-L|<\epsilon$. With the definition the way it is, I don't see how choosing a smaller and smaller $\epsilon$ implies a smaller and smaller $\delta$. To me, in order to produce that implication, we would need to restrict $\epsilon$ to be small enough to force $f(x)$ to be strictly increasing/decreasing on $(L-\epsilon, L+\epsilon)$, and define increasing/decreasing without the use of derivatives. However, that is not part of the definition. P.S. Please refrain from using too much notation for logic, I am not familiar with most of the symbols such as the upside down A and such.","['calculus', 'limits', 'definition', 'real-analysis', 'epsilon-delta']"
1476409,a proof that $\mathscr{B}(\mathbb{R})$ is generated by compact subsets of $\mathscr{R}$,"Is my argument correct? Let $\mathscr{B}(\mathbb{R})$ be the Borel subsets of $\mathbb{R}$,and let $\mathscr{A}$ be the $\sigma-algebra$ generated by the collection of all compact subsets of $\mathbb{R}$. It is know that $\mathscr{B}(\mathbb{R})$ is generated by the collection of all closed subsets of $\mathbb{R}$, so that $\mathscr{B}(\mathbb{R}) \supset \mathscr{A}$. If $C$ is an unbounded closed subset of $\mathbb{R}$,then $C$ is equal to the union of the countable collection of all sets of the form $\bar{B_n}\cap C$ where $B_n$ is a ball of radius $n$. Each such set is closed and bounded so that it is compact. It follows that $\mathscr{B}(\mathbb{R})\subset \mathscr{A}$. Hence $\mathscr{B}(\mathbb{R})=\mathscr{A}$.",['measure-theory']
1476443,An example that the union of $\sigma$-algebras need not be a $\sigma$-algebra,"Does the following example show that the union of $\sigma$-algebras may fail to be a $\sigma$-algebra? Let $X=\{a,b,c,d\}$ and let $\mathscr{A}=\{\emptyset,X,\{a,b\},\{c,d\}\}$ and $\mathscr{B}=\{\emptyset,X,\{a,c\},\{b,d\}\}$. Then $\mathscr{A}$ and $\mathscr{B}$ are $\sigma$-algebras while $\mathscr{A}\cup \mathscr{B}=\{\emptyset,X,\{a,b\},\{c,d\},\{a,c\},\{b,d\}\}$ is not a $\sigma$-algebra since it is not closed under intersection.",['measure-theory']
1476489,Derivative with respect to function,"I am looking to calculate the derivative of a functional $\phi(\rho)$ with respect to $\rho$, that looks like $$\phi[\rho](x)=\rho(x)\int_0^1\log|x-y|\rho(y)dy.$$ I have read that the Gateaux derivative or Frechet derivative and calculus of variations are the right key words to look for. However I am very new to functional analysis and I am not sure who to proceed. Could someone help? PS: I need this for a computation so numerically I wanted to do $(\phi(\rho+tu)-\phi(\rho))/t$ for a small $t$ to approximate the derivative. Is this correct? I wouldn't mind to have an analytic result though. Thank you!","['calculus-of-variations', 'functional-analysis']"
1476500,How many ways are there to put 'n' figures on a chess board?,"How many ways are there to put n figures on an n x n chess board, if in at least one vertical row there are no figures?
note: the figures are indistinguishable from each other","['discrete-mathematics', 'combinatorics']"
1476518,"Suppose f is uniformly continuous on I and J. Prove that if I intersecting J are non-empty, then f is uniformly continuous on I union J.","Problem: Let I and J be two non-degenerate intervals and suppose f is uniformly continuous on I and J. Prove that if I intersecting J are non-empty, then f is uniformly continuous on I union J. I understand what non-degenerate means. But my thought was since f is uniformly continuous on I and J, then it is clearly uniformly continuous on I union J. This is incorrect though...Help!","['analysis', 'real-analysis']"
1476528,With the given notation explain how any function $f: A \to B$ determines a section of $\pi_A$,"$\pi_A ((a, b)) := a$ and $\pi_B((a, b)) := b$. I think a section is a right-inverse of a function. So, $\pi_A \circ f(a) = \pi_A(f(a)) = \pi_A(b) = \pi_A(\pi_B((a, b))) = a.$ I am not sure if any of this makes sense seeing as how $\pi_A$ takes a pair, not just a single element from either $A$ or $B$. Am I at least goig in the right direction?",['elementary-set-theory']
1476568,Strange definitions about basic probability - need clarification,"In the book Condition. The Geometry of Numerical Algorithms - Peter Burgisser, Felipe Cucker , there is an explanation that doesn't make sense to me. It's just a problem about definitions, but I really need a light on this. This section of the book is supposed to be a review of probability, so it's not necessary to know the book to answer my question, just some good knowledge of probability theory. Below is the page from the book which I'm having trouble with. The first problem comes from the integral $\int_M f = 1$, which makes part of the definition of $f$. This integral has to be defined in respect to some measure over $M$ (more precisely, there is a measure $\mu$ over $M$ so that $\int_M f = \int_M f\ d\mu$) . Are they assuming this definition makes sense for any measure over $M$? Or maybe already there is some probability measure over $M$? My second problem is: after the definition of density, they say we can use $f$ to define a probability measure $\mu$ on $M$ with another integral, but this another integral doesn't seen to depend on $f$. Where $f$ is hidden in this definition? And again, we are integrating in respect to what measure? Finally, this is just an observation, but I need some clarification on this too. Usually we work with sample space $\Omega$ instead data space $M$. I'm assuming this is just a mater of conventions, so this is ok. But as far as I know, usually we work with some random variable $X:\Omega\to S$ ($S$ is a Polish space) and the density is a function $f:S\to[0,\infty]$. The way they are defining the density (before mention random variables) makes it be $f:\Omega\to[0,\infty]$. Honestly, I never saw a density defined directly on the sample space. Is this a normal routine? Thank you and sorry for the long post.",['probability-theory']
