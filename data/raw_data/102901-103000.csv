question_id,title,body,tags
1436636,Another conditionl leading to irrationality of $\sum _{k=1}^ \infty \dfrac 1{n_k}$?,"If $\{n_k\}$ is a strictly increasing sequence of positive integers such that $\lim \inf _{k \to \infty} n_k ^{1/2^k} >1$ and $\lim _{k \to \infty} n_k^{1/2^k}$ does not exist , then is it true that $\sum _{k=1}^ \infty \dfrac 1{n_k}$ is irrational ?","['number-theory', 'real-analysis', 'sequences-and-series', 'irrational-numbers', 'analysis']"
1436678,The wreath product $K ~\mbox{wr}_{\Gamma} ~ H$ acts faithful on $\Delta \times \Gamma$ iff $K$ acts faithful on $\Delta$,"Let $K$ and $H$ be groups, and let $H$ act on $\Gamma$. Also let $\operatorname{Fun}(\Gamma, K) = \{ f : f : \Gamma \to K \}$ be the set of all functions from $\Gamma$ to $K$. This set is a group under pointwise multiplication
$$
 (fg)(\gamma) := f(\gamma)g(\gamma)
$$
and $H$ acts on $\operatorname{Fun}(\Gamma, K)$ by setting for $f : \Gamma \to K$ and $x \in H$ the function $f^x$ to be
$$
 f^x(\gamma) := f(\gamma^{x^{-1}})
$$
for all $\gamma\in \Gamma$. Now the semidirect product
$$
 \operatorname{Fun}(\Gamma, K) \rtimes H
$$
is called the wreath product of $K$ and $H$ and denoted by $K ~\mbox{wr}_{\Gamma} ~ H$. Its elements are tupels $(f, x)$ of functions and elements from $H$. If $K$ also acts on some set $\Delta$, then the wreath product acts in a natural way on $\Delta \times \Gamma$, by setting for $(\delta, \gamma)$
$$
 (\delta, \gamma)^{(f,x)} := (\delta^{f(\gamma)}, \gamma^x).
$$
(This action is called the imprimitive action of the wreath product on wikipedia ). Now I want to prove, in the case we have an action of $K$ on $\Delta$ and $H$ on $\Gamma$, and therefore an action of the wreath product on $\Delta \times \Gamma$, that: The wreath product group $G = K ~\mbox{wr}_{\Gamma}~ H$ acts faithfully on $\Delta \times \Gamma$ if and only if $K$ acts faithfully on $\Delta$. (This is exercise 2.6.6. from the book Permutation Groups by Dixon & Mortimer). My Attempt: Let $G$ act faithfully on $\Delta \times \Gamma$ and suppose for each $\delta \in \Delta$ we have $\delta^k = \delta$. Denote by $\overline k : \Gamma \to K$ the constant function with $\overline k(\gamma) = k$. The we have for each $(\delta, \gamma)$ that
$$
 (\delta, \gamma)^{(\overline k, 1)} = (\delta^{\overline k(\gamma)}, \gamma^1)
  = (\delta^k, \gamma) = (\delta, \gamma).
$$
Hence as $G$ acts faithful $(\overline k, 1) = (\overline 1, 1)$ as
$(\overline 1, 1)$ is the identity in $G$, but this shows that $\overline k = \overline 1$ or $k = 1$. Now conversely let $K$ act faithfully on $\Delta$ and suppose we have
$$
 (\delta, \gamma)^{(f,x)} = (\delta, \gamma)
$$
for each $(\delta, \gamma) \in \Delta \times \Gamma$. By this for each $\delta \in \Delta$ and each $\gamma \in \Gamma$ we have
$$
 \delta^{f(\gamma)} = \delta \quad \mbox{ and } \quad
 \gamma^{x} = \gamma.
$$
As $K$ acts faithful we have $f(\gamma) = 1$ for each $\gamma$, i.e. $f$ is the constant function which equals $1$ (denoted $\overline 1$ above). Now we just need that $x = 1$ and we are done, but I do not see that this is implied by the above (it would follow if also $H$ acts faithful, but this is not part of the exercise, nor is it included in the general definition of the wreath product)? So is there any way to show that $(f, x) = (\overline 1, 1)$? Maybe the exercise is wrong and it should read: The wreath product acts faithful on $\Delta \times \Gamma$ if and only if $K$ and $H$ act both faithful on $\Delta$ and $\Gamma$. but the errata does not mentions this, so I am not sure?","['abstract-algebra', 'group-theory', 'group-actions', 'permutations']"
1436696,Strong convergence of total variation of signed measure,"Let $\nu_n$ be a sequence of finite signed radon measure such that $\nu_n\to \nu$ strongly for a finite signed radon measure $\nu$. Let $|\nu_n|$ denote the total variation measure of $\nu_n$. We know that $|\nu_n|$ is a positive Radon measure. My question: do I have $|\nu_n|\to |\nu|$ strongly? and do I have $||\nu|-|\mu||\leq |\nu-\mu|$ for two arbitrary finite signed measure $\nu$ and $\mu$? I know the above statement is absolutely false if $\nu_n\to \nu$ only in weak star sense. But I somehow remembered for strong convergence it is true but I can not find the source. So, if it is true, please confirm it for me and directly me to a reference, if not... maybe a counter example? Thank you!","['real-analysis', 'measure-theory']"
1436753,Can the sheaf associated to an indecomposable graded module be decomposable?,Let $R$ be a graded commutative ring such that $X=\operatorname{Proj}(R)$ is a smooth projective variety. Let $M$ be a finite graded module over $R$ and let $\mathcal{F}=\widetilde{M}$ be the associated coherent sheaf on $X$. If $M$ is indecomposable (i.e. can not be presented as a direct sum of two nonzero graded modules) is it true that $\mathcal{F}$ is also indecomposable?,"['algebraic-geometry', 'graded-rings', 'graded-modules']"
1436778,Can $\sum_{k\in M}\frac{1}{k}$ be a large integer?,"I'm interested in the following question: Given an integer $n_0$. Is there always an integer $n>n_0$ and a finite subset $M\subset \mathbb N$ with $\sum_{k\in M}\frac{1}{k} = n$. This is not a homework problem, I don't know if there is an easy solution.
I appreciate any hint.",['sequences-and-series']
1436828,"What does ""arg inf"" mean?",I noticed this term on this post . But the term arg inf is not clearly defined.,"['analysis', 'notation']"
1436853,"$f$ an entire function, $|f(1/n)|\leq n^{-n}$ for $n\in \mathbb{N}$. Show that $f$ is constant.","I'm working on old qualifying exam problems and this one came up. Suppose $f$ is an entire function on $\mathbb{C}$, and that $|f(\frac{1}{n})| \leq n^{-n}$ for all $n\in \mathbb{Z}_{>0}$. Prove that $f$ is constant. My approaches: One obvious thing to do is to try and show that $f$ is bounded and appeal to Liouville's theorem. However, our bounds are for small values of $z$, so this did not yield much success. One can see that $f(0)=0$ from this condition. Also,
$$
 f'(0) = \lim_{n\to \infty} \frac{f(\frac{1}{n}) - f(0)}{\frac{1}{n}} = \lim_{n\to \infty} n f\left(\frac{1}{n}\right).
$$
Note that
$$
 \left|nf\left(\frac{1}{n}\right)\right| \leq n^{-n+1} \to 0 \text{ as } n\to \infty.
$$
Thus, $f'(0)=0$. I would like to keep trying to show that $f^{(n)}(0)=0$ and use the fact that $f$ is entire to get that $f=0$ identically on $\mathbb{C}$. But I get stuck because I don't have an estimate for $f'(1/n)$. I think what makes this problem harder is that we only have an estimate on $f$ at a countable set, instead of the usual problem where we know some kind of bound on $f$ everywhere. Any hints are appreciated.",['complex-analysis']
1436856,Showing that $f^{(n)}(0)=0$ for $f(x)=e^{-1/x^2}$ if $x\neq 0$ and $f(0)=0$.,"Define $f$ as follows: $f(x)=e^{-1/x^2}$ if $x\neq 0$ and $f(0)=0$. Show that $f^{(n)}(0)$ is continuous for all $x$ and $f^{(n)}(0)=0$. $n=1,2,\dots$. To show this, I have shown that for $x\neq 0$, we have $f^{(n)}(x)=e^{-1/x^2}P_{3n}(1/x),$ where $P_{3n}(t)$ is a real polynomial of degree $3n$. Hence, to show that $f^{(n)}(0)=0,$ I will use induction. $n=1$ case is trivial. So suppose that $n=k$ holds. Then as $n=k+1$, we have $\frac{f^{(k)}(x)-f^{(k)}(0)}{x-0}=\frac{f^{(k)}(x)}{x}=\frac{e^{-1/x^2}P_{3k}(1/x)}{x}$. Now I need to show that the limit of the above fraction as $x\to 0$ tends to $0$. For the case $x\to 0^{+}$, I can argue as follows. Replace $t=1/x$ in the above fraction, then we get $\frac{e^{-1/x^2}P_{3k}(1/x)}{x}=\frac{tP_{3k}(t)}{e^{t^2}}=(\frac{tP_{3k}(t)}{e^t})(\frac{e^t}{e^{t^2}})\to 0$ as $t\to \infty$, since we have $\lim_{x\to \infty}\frac{P(x)}{e^x}=0$. However, I have trouble showing the case for $x\to 0^{-}$, since $\lim_{\to -\infty}\frac{P(x)}{e^x}$ does not exist for all polynomials. How can I solve this problem? I would greatly appreciate any help. (added) My attempt: $\lim_{x\to 0^{-}}\frac{e^{-1/x^2}P_{3k}(1/x)}{x}=\lim_{x\to 0^{-}}e^{-1/x^2}P_{3k+1}(1/x)=\lim_{x\to 0^{+}}e^{-1/x^2}P_{3k+1}(-1/x)=\lim_{t\to \infty}e^{-t^2}P_{3k+1}(-t)=\lim_{t\to\infty}\frac{P_{3k+1}(-t)}{e^t}\cdot \frac{e^t}{e^{t^2}}=0\cdot 0=0$ $\lim_{t\to\infty}\frac{P_{3k+1}(-t)}{e^t}=0$, since $|\frac{P_{3k+1}(-t)}{e^t}|\le\frac{P_{3k+1}(|-t|)}{e^t} \to 0$ as $t\to \infty$.","['analysis', 'calculus', 'real-analysis']"
1436867,"show that a function $A$ such that $ \rho (Ax,Ay)< \rho (x,y) $ $ \forall x\neq y $ not necessary has a fix point $ (Ax\neq x \space \forall x )$","I donÂ´t know an  example wich   $ \rho (Ax,Ay)< \rho (x,y) $    $  \forall x\neq y $   is not sufficient for the existence of a fixed point .
can anybody help me? please","['analysis', 'functional-analysis', 'functions']"
1436868,Finding irreducible components of $\mathrm{Spec}(R/ \mathbb I)$,"I'm trying to solve the following exercise: Let $R=\mathbb Z[\sqrt{-5}]$ and $\mathbb I=(1+\sqrt{-5})$. Find the irreducible components of $\mathrm{Spec}(R/\mathbb I)$. For solving this problem I'm trying to use following exercise from Atiyah and Macdonald book: Let $A$ be a commutative ring with unit, $X = \mathrm{Spec}(A) $ with the Zariski topology. Then the irreducible components  of $X$ are $\lbrace V(p) : p\subset A \ \text{minimal prime ideal} \rbrace$ where $V(p) =\lbrace q \ \text{prime ideal } \mid p\subset q\rbrace$. My Solution: As $R \cong \mathbb Z[x]/(x^2+5)$, hence $R/ \mathbb I \cong \mathbb Z/ 6 \mathbb Z$. Hence by above exercise for finding irreducible components of $\mathrm{Spec}(R/\mathbb I)$ we just need to find minimal prime ideals of $\mathbb Z/ 6 \mathbb Z$. Hence irreducible components of $\mathrm{Spec}(R/\mathbb I)$ are $V(2)$ and $V(3)$ where $(2)$ and $(3)$ denotes the prime ideals generated by $2$ and $3$ respectively. Is this solution correct?","['algebraic-geometry', 'proof-verification', 'commutative-algebra']"
1436903,Two trigonometrical answers for same exercise. Which is the right one?,"I'm trying to resolve a trigonometrical exercise. I have two ways to resolve it and I receive two different answers. If you could help explain me why one way is a wrong way to resolve it (without doing reference to the trigonometric functions please). Using double-angle formulas one way is:
\begin{align}
\cos(2x) = \cos^2x \\
\ 2\cos^2x -1 = \cos^2x \\
\ \cos^2x - 1 = 0 \\
\ \cos^2x = 1 \\
\ x = 360^\circ k
\end{align}
The second way is:
\begin{align}
\cos(2x) = \cos^2x \\
\ \cos^2x - \sin^2x = \cos^2x \\
\ -\sin^2x = 0 \\
\ \sin^2x = 0\\
\ x = 180^\circ k
\end{align}
As you see there are two answers, I know that the last one is the right one but don't know why (please don't do reference to the graphs).",['trigonometry']
1436940,"Give a bijection $f: (c,d) \to \Bbb R$ (f no trigonometric) to prove every open interval has the same cardinality of R [duplicate]","This question already has answers here : Prove: Any open interval has the same cardinality of $\Bbb R$ (without using trigonometric functions) (6 answers) Closed 8 years ago . I want to prove that every open interval has the same cardinality of R. I've proved that $|(a,b)|=|(c,d)|$ so I may find a bijection $f: (a,b) \to (c,d)$. I need a bijection $f: (c,d) \to \Bbb R$ (a bijection defined by an interval to $\Bbb R$) The problem is that the only function like this that I know is tan(x) that is bijective in ($- \pi/2, \pi/2)$ but I am not allowed to use trigonometric functions. Does anybody know another function like this?","['elementary-set-theory', 'calculus', 'functions', 'cardinals', 'graphing-functions']"
1436973,Does $G_{\delta}+q$ sets cover $\Bbb{R}$ a.e,"Let $G_{\delta}$ be countable intersections of given open sets with positive Lebesgue measure on $[a,b]$. My question is that if  $G_{\delta}+q$ covers $\Bbb{R}$ a.e, i.e. is 
$$
\bigcup_{q \in \mathbb{Q}}(q+G_{\delta})=\Bbb{R}-N
$$
true? ($N$ is of Lebesgue measure zero). $G_{\delta}$ must be uncountable for it has positive Lebesgue measure. But it may has empty interior. I need help on this question.","['lebesgue-measure', 'real-analysis', 'measure-theory']"
1436992,"Prove that if $B$ is the set of rationals in $[0,1]$ with a finite subcover, then: $1 \leq \sum_{k=1}^n m^*(I_k)$","I have the following homework problem: Let $B$ be the set of rational numbers in the interval $[0,1]$, and let $\{I_k\}_{k=1}^n$ be a finite collection of open intervals that covers $B$. Prove that $\sum_{k=1}^n m^*(I_k) \geq 1$. This is my approach, First we have that $B = \mathbb{Q}\cap [0,1]$. Now, $B \subset \bigcup_{k=1}^nI_k$, where $I_k$ are open intervals.In particular $0 \in B \wedge 1 \in B$.Now, since $B \subset \bigcup_{k=1}^nI_k$ then there exist positive numbers, $\epsilon_0, \epsilon_1$ such that:
$$(0-\epsilon_0,0+\epsilon_0)\subset \bigcup_{k=1}^nI_k$$
$$(1-\epsilon_0,1+\epsilon_0)\subset \bigcup_{k=1}^nI_k.$$ Clearly, $-e_0 \notin [0,1]$ and $e_1 \notin [0,1]$.I am not sure, if so far I am in the right track.","['lebesgue-measure', 'real-analysis', 'measure-theory']"
1437023,"Why is $[0, 1] \cap \mathbb{Q}$ not compact in $\mathbb{Q}$? [duplicate]","This question already has answers here : Is $[0,1] \cap \Bbb Q$ a compact subset of $\Bbb Q$? (6 answers) Closed 5 years ago . Statement: $[a, b] \cap \mathbb{Q}$ in $\mathbb{Q}$ is not compact. Thus the interior of all compact subsets of $\mathbb{Q}$ is $\emptyset$. I am trying to understand the first sentence. I read that a closed subspace of a compact space is compact, so for example, consider the unit interval $[0, 1]$ which is a compact space. Take a closed subspace $[0, 1] \cap \mathbb{Q}$ of $[0, 1]$. This set is closed since it just consists of all the rational numbers in between $0$ and $1$, including $0$ and $1$. So it is a closed subspace of a compact space. But why isn't this compact?","['rational-numbers', 'general-topology', 'compactness']"
1437024,Existence of essential supremum for an uncountable sequence of measurable functions,"Let $(\Omega, \mathscr{F}, P)$ be a probability space and $\{X_i, i \in I\}\subset \mathbb{L}^0(\mathscr{F})$ , where $I$ can be uncountable. I want to prove that there exists unique (in P-a.s. sense) essential supremum $X := \text{ess sup}_{i\in I}^PX_i$ in the following sense: $X \in \mathbb{L}^0(\mathscr{F})$ $X\geq X_i, P-a.s. , \forall i \in I$ If another $\tilde{X}$ satisfies the above two properties, then $\tilde{X}\geq X, P-a.s.$ I know the definition for essential supremum of a function , which is in fact a number. But here we have a sequence of functions, where the essential supremum is itself a function. How are these definitions connected to each other? Any suggestions how I can solve this problem? $\;$","['probability-theory', 'measure-theory']"
1437036,Why induction can't work for infinite number? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Loosely speaking, there is no such number n+1= infinity. Is there any way to prove that induction can not work for infinite numbers in formal way?","['calculus', 'induction']"
1437070,"Integral $(1, 1)$ forms and holomorphic line bundles","Let $X$ be a complex manifold. We say that a cohomology class in $H^2(X,\mathbb{C})$ is integral if it lies in the image of the natural morphism $j : H^2(X,\mathbb{Z}) \longrightarrow H^2(X,\mathbb{C})$. Define the first Chern class of a holomorphic line bundle $L \in \text{Pic}(X)$ on $X$ as the image of $L$ under the boundary map $c_1: \text{Pic}(X) \cong H^1(X,\mathcal{O}^*_X) \longrightarrow H^2(X,\mathbb{Z})$. Question : Given a $d$-closed differential form $\omega$ of type $(1,1)$ on $X$ with integral class $[\omega] \in H^2(X,\mathbb{C})$, does it follow automatically that there exists a holomorphic line bundle $L \in \text{Pic}(X)$ such that the image of $c_1(L)$ under $j$ is equal to $[\omega]$, up to some $2\pi i$ factor or so? I would like an answer without assuming that $X$ is compact KÃ¤hler, so that Lefschetz's theorem on $(1,1)$ classes does not apply. Also, in case the answer is positive, can anyone point out a reference?","['complex-geometry', 'algebraic-geometry', 'differential-geometry']"
1437086,Power series expansion of $e^{-1/x^2}$ at a point different from 0,The function $f(x)=e^{-1/x^2}$ ($f(0)=0$) does not have a power series expansion at $z_0=0$. Now my question: Is there a power series for $f$ centered at $z_0\neq0$ with convergence radius greater than $|z_0|$?,['calculus']
1437098,"How do I prove that $\,\left\lvert\,\int f \,dg\, \right\rvert \leq \int\left\lvert f \right\rvert \,d\left\lvert g\right\rvert$?","Let $\;g:[a,b]\rightarrow \mathbb{C}\,$ be a function of bounded variation. Let $\;f:[a,b]\rightarrow \mathbb{C}\,$ be a function Riemann-integrable along $g$. Define $\,\alpha\left(x\right) = V_a^x \left(g\right)$, for all $\,x\in\left[a,b\right]$. How do I prove that $\displaystyle\,\left\lvert\,\int f \,dg\, \right\rvert \leq  \int \big\lvert\, f\, \big\rvert \,d\alpha$? I have proven it for the case $\,g\,$ is monotonic, but in general how do I prove it? Set $\,g=h+ik\,$ where $\,h,k\,$ are reals. Then, set $\,h_1\left(x\right)=1/2\big(V_a^x\left(h\right) +h\left(x\right)\big)\, $ and $\,h_2\left(x\right)=1/2\big(V_a^x\left(h\right) - h\left(x\right)\big)\,$ and $\,k_1\left(x\right) = 1/2\big(V_a^x \left(k\right) +k\left(x\right)\big)\,$ and $\,k_2\left(x\right) = 1/2\big(V_a^x \left(k\right) - k\left(x\right)\big)$. Then, 
\begin{align}
\left\lvert\int_a^b f \,dg \right\rvert 
& = 
\left\lvert\int_a^b f \,dh_1 - \int_a^b f\, dh_2 + i\left(\int_a^b f\, dk_1 -\int_a^b f \,dk_2\right) \right\rvert
\\ & \leq 
\left\lvert\int_a^b f \,dh_1 \right\rvert + \left\lvert\int_a^b f \,dh_2 \right\rvert + \left\lvert\int_a^b f\, dk_1 \right\rvert + \left\lvert\int_a^b f \,dk_2 \right\rvert 
\\  & \leq 
\int_a^b \big\lvert\, f\, \big\rvert \,dh_1 + \int_a^b \big\lvert\, f\, \big\rvert\,dh_2 + \int_a^b  \big\lvert\, f\, \big\rvert\,dk_1 + \int_a^b \big\lvert\, f\, \big\rvert \,dk_2 
\\  & = 
\int_a^b \big\lvert \,f\left(x\right) \big\rvert \,dV_a^x(h) + \int_a^b \big\lvert \,f\left(x\right)\big\rvert\,dV_a^x\left(k\right) 
\\  & = 
\int_a^b \big\lvert \,f\left(x\right)\big\rvert \, d\big( V_a^x\left(h\right) + V_a^x \left(k\right) \big).
\end{align} However, $\,V_a^x\left(h\right)+ V_a^x\left(k\right)\geq V_a^x\left(g\right)\,$ for all $\,x$, we cannot relate the above inequalities to prove the inequality in question. How do I prove it? Thank you in advance.","['analysis', 'real-analysis', 'riemann-integration', 'integration']"
1437103,How do I solve this complex integration problem?.,"I want to find the value of  $$I=\int_{|z|=r}\frac{|dz|}{|z-z_0|^4},$$ where $|z_0|\neq r>0$.","['contour-integration', 'complex-analysis']"
1437110,Show that the area under the curve of a measurabe function is Lebesgue measurable,"Problem: Let $E \subset R^n$ be a Lebesgue measurable set and $f : E \to [0,\infty)$ be a Lebesgue measurable function. Suppose
$$A = \{(x,y) \in R^{n+1} : 0 \le y \le f(x), x\in E\}.$$
Let $\lambda_1$ denote Lebsgue measure in $R^1$, $\lambda_n$ denote the Lebesgue measure in $R^n$, and $\lambda_{n+1}$ denote the Lebesgue measure in $R^{n+1}$. (a) Show that the set $A$ is Lebesgue measurable on $R^{n+1}$. (b) Show that
$$\lambda_{n+1}(A) = \int_E f(x) d\lambda_n (x) = \int_0^\infty \lambda_n (\{x \in E : f(x) \ge y\}) d\lambda_1(y).$$ My attempt at a solution: I know that this is just a basic application of the Fubini-Tonelli theorem, but I can't seem to wrap my head around it for some reason. For (a), I know that the set $A$ is the ""area under the curve."" But I'm not sure how to show that this is measurable from Fubini Tonelli. For part (b), the first equality seems obvious, and I don't know how much proof is necessary. For the second equality, I have
$$\int_E f(x)d\lambda_n(x) = \int_0^\infty \int_{R^n} [f(x) \cdot \chi_E(x)] d\lambda_n(x) d\lambda_1(y),$$
by the Tonelli theorem, since $f$ is non-negative. But I don't quite see how to pull off that inner integration to get the integrand we want. I would really appreciate some hints/intuitive explanations to point me in the right direction. Thanks!","['real-analysis', 'lebesgue-integral', 'measure-theory']"
1437160,Induction proof: n lines in a plane,"Assume that there are $n$ infinitely long straight lines lying on a plane
in such a way that no two lines are parallel, and no three lines intersect
at a single point. Prove that these lines divide the plane into $\frac{n^2+n+2}{2}$ regions. (Hint: Any two non-parallel straight lines on a plane must
intersect at exactly one point.) Can someone tell me where to start here? I know to prove the base where $n=0$, but don't know how to proceed. 
Any help is greatly appreciated!","['geometry', 'induction']"
1437170,nth derivative of: $F(x)=1-\sqrt{1-x^2}$,"I've gotten this function from probability generating functions, and I want to calculate it's nth derivative (With respect to $x$). This is:
$$F(x)=1-\sqrt{1-x^2}$$ Is there a practical way to do it? Or for another approach, I just need the derivatives calculated in $x=0$, to calculate it's MacLaurin series. Any practical way to do it?","['sequences-and-series', 'calculus', 'stochastic-processes']"
1437198,Property of elements of a positive definite matrix,"I am stuck in proving a property of a positive definite matrix. Let $A$ be a positive definite matrix. Then:
  $$|A_{ij}| \le \sqrt{A_{ii}A_{jj}} \le \frac{A_{ii}+A_{jj}}{2}.$$ My work: As for every non zero x, $x^TAx \gt 0$,
consider $x^T = \begin{bmatrix}0& 0 &0 &\cdots& 0 &1& 0&\cdots&0 &-1& 0&\cdots&0 \end{bmatrix}$ with $1$  at position $i$ and $-1$ at position $j$.
 $$x^TAx = A_{ii}+A_{jj}-2*A_{ij} \gt 0.$$ Need help in completing the proof.","['linear-algebra', 'inequality', 'matrices']"
1437216,Explicit construction of a sigma algebra that makes a simple function measurable,"This is a question from an old exam. I'd like to see if my answer is correct. I'd appreciate any suggestion. Thanks :) Let $f: (\mathbb{R},\mathscr{S}) \to (\mathbb{R}, \mathscr{B}(\mathbb{R}))$ defined as follows $$f(x):=3\cdot \mathbb{1}_{[1,3]}(x)+2\cdot \mathbb{1}_{[2,4]}(x)$$ where $\mathscr{B}(\mathbb{R})$ is the Borel $\sigma$-algebra of the real numbers, and $\mathbb{1}_A(x)$ is the indicator function, i.e., if $x$ belongs to $A$, $\mathbb{1}_A(x)=1$ and it is zero otherwise. What is the minimum $\sigma$-algebra $\mathscr{S}$ that makes $f$ a measurable function (write explicitly the sigma algebra)? Answer : Let $B_1=[1,3]$ and $B_2=[2,4]$. Now let $\mathscr{D}$ be the collection of all the intersection $C_1\cap C_2$, where either $C_i=B_i$ or $C_i=B_i^c$ for $i\in \{1,2\}$. Then $$\mathscr{D}=\{\,[1,2),\,[2,3],\,(3,4],\,(-\infty,1)\cup (4,+\infty)\,\}$$ Now, we enumerate the elements in $\mathscr{D}$, i.e., $\mathscr{D}=\{D_j: \text{ for } j= 1,\ldots, 4 \}$, and define $\mathscr{A}$ as the collection of all the possible unions of the elements in $\mathscr{D}$, that is $$A\in \mathscr{A} \iff A=\bigcup\bigg\{D_j : j\in J \text{  where  } J \subset\{1,2,3,4\}\bigg\}$$ Thus, we have the follwing
\begin{align*}\mathscr{A}= \bigg \{ &\varnothing,\,[1,2),\,[2,3],\,(3,4],\,(-\infty,1)\cup (4,+\infty),\,[1,3],\, [1,2)\cup (3,4],\,(-\infty,2)\cup (4,+\infty),\\
&[2,4],\,(-\infty,1)\cup [2,3]\cup (4,+\infty),\,(-\infty,1)\cup (3,+\infty), [1,4],\,(-\infty,3]\cup (4,+\infty),\\
& (-\infty,2)\cup (3,+\infty),\,(-\infty,1)\cup [2,+\infty),\, \mathbb{R} \bigg\} \end{align*} Now clearly $\mathscr{A}$ is a $\sigma$-algebra (it is closed under countable unions, complementation and also $\mathbb{R}$ belongs to it). On the other hand, the function $f$ can be written by elements in $\mathscr{D}$ (which are a partition of the reals) in the following way $$f(x)=3 \cdot \mathbb{1}_{[1,2)}(x)+5\cdot \mathbb{1}_{[2,3]}(x)+2 \cdot \mathbb{1}_{(3,4]} (x)$$ Therefore, in the above expression of $f$ it is clear that $\mathscr{A}$ makes $f$ measurable.The minimum sigma algebra $\mathscr{S}$ that makes $f$ measurable, must contain $[1,2),\,[2,3],\,(3,4]$. Then $\mathscr{D}$ belongs to $\mathscr{S}$, since $(-\infty,1)\cup (4,+\infty)=[1,4]^c$, and so $\mathscr{A}\subset \mathscr{S}$. The other inclusion is easy by minimality of $\mathscr{S}$. Then $\mathscr{S} =\mathscr{A}$ as desired.","['self-learning', 'measure-theory']"
1437274,"What is happening to a curve in space at a point where it has small curvature, but high torsion","Can anyone give me an intuitive explanation for the kind of twisting or bending that a curve in $\mathbb{R}^{3}$ undergoes when it's at a point of small curvature, but high torsion? I understand that torsion measures the failure for a curve to be planar i.e. be contained in a plane. So a curve with a high torsion in some region must be twisting in various directions through various planes in 3-space. Curvature on the other hand measures how rapidly the tangent vectors are changing with respect to the arc length of the curve. So it seems like my intuition wants to think that high torsion implies high curvature, but that's certainly not the case. I was wondering if someone can explain this","['geometry', 'curvature', 'differential-geometry']"
1437287,Differentiating geometric series,On Wikipedia it is stated that by differentiating the following formula holds: $$ \sum_n n q^n = {1\over (1-q)^2}$$ Does this not require a proof? It seems to me because the series is infinite it is not clear that differentiation commutes with taking the limit. How to prove this?,"['power-series', 'limits', 'real-analysis', 'derivatives']"
1437339,From Cauchy in Measure to Almost Sure Convergence,"My setup is the following. Consider a sequence of random variables $(X_n)_n$ such that for every $\delta>0$ 
\begin{align*}
\lim_{n,m\rightarrow\infty}\mathbb{P}[\sup_{m<k\leq n}|X_m-X_k|\geq \delta]=0
\end{align*} And I have to prove that there is a random variable $X$ such that the sequence converges to $X$ almost surely. What I have:
The assumption implies the sequence is Cauchy in measure so there is a random variable $X$ such that the sequence converges to $X$ but in measure. Naturally, that is my aspirant to the a.s. limit. Furthermore, there exists a subsequence that converges a.s. to $X$ but I can't manage to prove the whole sequence converges. Any suggestions? Thanks a lot!","['probability-theory', 'convergence-divergence', 'measure-theory']"
1437363,The measure of a (not necessarily disjoint) union,"Let $(X,\Sigma,\mu)$ be a measurable space. Prove that, for $A,B \in \Sigma$, $\mu(A) + \mu(B) = \mu(A \cup B) + \mu(A \cap B)$. Sorry that I don't have many thoughts to add here, but I really don't know where to get started with this proof. Obviously this reduces to the countable additivity axiom when $A$ and $B$ are disjoint, so perhaps I need to be clever with symmetric differences and make use of that? Please, only give hints, not full solutions.","['elementary-set-theory', 'measure-theory']"
1437381,"Bounded sequence which is not convergent, but differences of consecutive terms converge to zero","I have a question that says ""Show that there is a bounded sequence $x_n$ which is not convergent but has the property that $x_n - x_{n+1} \to 0$ as $n \to 0$. What does this mean? Do I need to come up with an example or does the problem actually want me to prove such proposition? By the way, I see that this sequence looks like Cauchy because of $x_n - x_{n+1} \to 0$ as $n \to 0$, but it is obviously not.","['sequences-and-series', 'examples-counterexamples', 'real-analysis']"
1437392,Question about $\alpha-$plane in twistor theory,"In twistor theory, given the complexified Minkowski space $CM$ and the projective twistor space $PT$, an $\alpha-$plane is defined as the correspondence in $CM$ wit a point $Z \in PT$. But I found two versions of it. (a) One in ""Twistor geometry and field theory"" by Ward and Wells, where $\alpha-$plane is a projective complex plane in $CM$ corresponding to a point $Z \in PT$ without mentioning if it's self-dual or not. (b) Another is by Penrose that $\alpha-$plane is a two-complex-dimensional locus of points in $CM$ which is incident to $Z$ and also it's self-dual. I am confused if the two versions are different. (1) Are the two definitions mean the same thing? i.e., every point of the projective complex plane in (a) is incident to $Z$ as defined in (b)? (2) Is the definition by Penrose in (b) a subset of the former in (a), so that only a subset of the points of the $\alpha-$plane in (a) is self-dual? Since according to (a), the $\alpha-$plane is a PROJECTIVE complex plane for any point $Z \in PT$. It seems that any point in $CM$ must fall on at least one $\alpha-$plane corresponding to some point $Z' \in PT$. Then the self-duality of Penrose should not hold for the definition (a) since not every point in $CM$ leads to self-duality property. (3) If (b) is really a subset of (a), then not every point in $CM$ falls on an $\alpha-$plane and the union of all $\alpha-$planes is just a subset of $CM$, this sounds reasonable . To explain my question, I put the definitions (a)(b) in figures. 
For definition (a), $P$ is in fact just $PT$ and the $\alpha-$plane is given by $\tilde{p}$ as given in Figure 1. Figure 2 and 3 show the schemes of (a) and (b). I am new on this topic. Can anybody help to clarify the concept? Figure 1. Definition (a) of Ward & Wells [Twistor geometry and field theory] Figure 2. Definition (a) of Ward & Wells, figure from Dorje C. Brody and Lane P. Hughston [2005]. Figure 3. Definition (b) of Penrose [The road to reality]","['general-relativity', 'algebraic-geometry', 'differential-geometry']"
1437421,Indefinite integral that Wolfram Alpha can't solve,"Does anybody know the indefinite integral problems that Wolfram Alpha can not solve but human can solve by using elementary functions? (The meaning of ""solve"" here is to representãprimitive function by using elementary functions.)","['calculus', 'wolfram-alpha', 'indefinite-integrals', 'integration', 'symbolic-computation']"
1437463,Solving Inner Product Equations,"I'm trying to solve an exercise from Cheney's Analysis for Applied Mathematics . Let $X$ be a normed linear space with $a,b,c\in X$ taken as fixed vectors, and consider the equation $x+\langle x, a\rangle c = b$ The goal being to find a general solution $x$. There are obvious case by case trivial solutions (i.e. when $b\perp a$, $x=b$). Based on the fact that this exercise is in the first section on Hilbert spaces, I'm inclined to think I should be able to be solved by applying axioms for an inner product space. I've tried taking the inner product of both sides with some combination of $a,b,$ or $c$, but thus far, to no success.","['analysis', 'inner-products', 'functional-analysis']"
1437489,"Finding y prime, derivative trouble","I have $$y=\frac{4x}{x^{3/2}} - 8x - 2 \cos(\frac{\pi}{4}) $$ The best I could get it to was $$\frac{4}{2}x^{-3/4} - 6x - 2\cos(\frac{\pi}{4})  $$ but the answer doesn't even have a cos or even sin in it. I understood the example problem before this just fine, but it was a lot simpler and I'm completely lost on this...","['calculus', 'derivatives']"
1437507,Writing a Closed form expression Discrete maths,"$$â_{i=1}^n a_i=n^2-n$$
Write a closed form expression for $$â_{i=1}^{n-1} a_i$$ in terms of n and then simplify. Hello, Just asking this question to see if this answer is right.
I'm still not 100% sure about these types of questions. This is what I've gotten. $$â_{i=1}^{n-1} a_i = a_1 + a_2 + a_3 +...+n+n-1...(1)$$ 
$$â_{i=1}^n a_i = a_1+ a_2 + a_3+...+n ...(2)$$
Sub (1) into (2)
$$â_{i=1}^{n-1} a_i = â_{i=1}^n a_i + n-1$$
we know that (1) = $n^2-n$
therefore $$â_{i=1}^{n-1} a_i = n^2 -n + n-1$$
and simplified that is $n^2 -1$ I'm not sure about the n+ n-1 part and that seems to be a big factor in whether this question is right. any insight would be appreciated.","['sequences-and-series', 'solution-verification', 'discrete-mathematics']"
1437513,"Use of max and min ""functions"" [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question ""Let a,x,y be elements of R. If a < min(x,y) then a < x and a < y"" The above is a theorem that I encountered in my Real Analysis course. I do not want to ask about what it means, but rather concerning the ""min(x,y)"" being used. The nature of it I am not clear about , even after a conversation with my lecturer about it. This is within the context of proving limits for the reals . My current understanding: this allows us to ensure that we have a positive real number, that min(x,y) is assuredly so. I have also seem a similar thing in the context of epsilon-N proofs (sequences) , but min is replaced with max.","['limits', 'real-analysis', 'proof-writing']"
1437551,Homeomorphism $(\mathbb{CP}^1)^m/S_m \overset{\sim}{\to} \mathbb{CP}^m$?,"For each $m \ge 1$, how do I construct a homeomorphism $(\mathbb{CP}^1)^m/S_m \overset{\sim}{\to} \mathbb{CP}^m$? My thoughts so far: I probably want to identify $\mathbb{C}^{m+1}$ with the vector space $\mathbb{C}^m[z, w]$ of degree $m$ homogeneous polynomials in two variables and use that a polynomial in one variable is determined by its roots up to a nonzero constant factor somehow... but from here, I'm kinda of stuck. Could someone help? EDIT: Progress so far. After reading Eric Wofsey's response, I have the following. Since a projective variety is determined by its homogeneous coordinate ring, it suffices to determine the homogeneous coordinate ring of $(\mathbb{CP}^1)^m/S_m$. For $(\mathbb{CP}^1)^m$, the coordinate ring is the graded ring $\mathcal{S}$ whose $d$th degree part is the $m$-fold tensor product (over $\mathbb{C}$) of $\mathbb{C}[s, t]_{d}$. That is, $S_d$ consists of polynomials in $s^1, t^1, s^2, t^2, \dots, s^m, t^m$ of multidegree ($d, d, \dots, d$). And $S_m$ acts in the obvious way, i.e. if $\sigma(i) = j$, then $\sigma$ sends $s^i$ to $s^j$ and $t^i$ to $t^j$. The homogeneous coordinate ring of $(\mathbb{CP}^1)^m/S_m$ is just the ring of invariants of the $S_m$ action on $\mathcal{S}$. But I still have two questions. Which polynomials are invariant under this action? I still don't see how to conclude that Eric Wofsey's map is a homeomorphism. How do I see that Eric Wofsey's map is a homeomorphism?","['projective-space', 'algebraic-geometry', 'general-topology']"
1437583,"In a group, does $(xy)^n=x^ny^n$ for $n\geq 3$ imply $xy=yx$?","Let $G$ be a group and let $x,y\in G$ such that $(xy)^n=x^n y^n$
for every $n\geq 3$. Does it necessarily follow that $x$ and $y$
commute ? My thoughts : from $(xy)^{n+1}=x^{n+1} y^{n+1}$ we deduce
$x^{n+1} y^{n+1}=xy(xy)^n=xyx^ny^n$, whence $x^ny=yx^n$ for every
$n\geq 3$, so we are done if $x$ (or $y$) has finite order. But I
do not see how to continue when both $x$ and $y$ have infinite
order. I seem to remember that an identical (or very close) question was asked
here on MSE some time ago, but I couldn't find it.",['group-theory']
1437615,computation of the probability of a random variable,"$(Y_n)_n$ is a sequence of random variable i.i.d such that $Y_1=1$ with probability $p$ and $Y_1=-k$ with probability $1-p$. $(S_n)_n$ is a sequence of random variables defined as $S_0=0$ and $S_n=\sum_{i=0}^n Y_i$. I have to show that if $\rho=P(\exists n \in \mathbb{N} : S_n\geq 1)$ then 
$\rho=p+(1-p)\rho^{k+1}$. I don't really know how to start so any help is appreciate.","['probability-theory', 'stochastic-calculus', 'probability', 'stochastic-processes']"
1437624,Number Theoretic Transform (NTT) example not working out,"I'm reading up on the NTT, which is a generalisation of the DFT. I'm working in $\mathbb{F}_5$ with primitive root $w=2 \mod 5$. Suppose I want to compute the NTT of $x=(1,4)$. So far I have obtained: $$\hat{x}=\mathcal{N}(x)=\left(\sum_{j=0}^1 2^{jk}x_j\mod 5\right)_{k=0}^1=(1+4,1+2\cdot 4)\equiv(0,4)\mod 5.$$ Applying the inverse NTT should recover $x$. But... $$\mathcal{N}^{-1}(\hat{x})=\left(\frac{1}{2}\sum_{k=0}^1 2^{-jk}\hat{x}_k\mod 5\right)_{j=0}^1=\left(\frac{1}{2}(0+4),\frac{1}{2}(0+2^{-1}\cdot 4)\right)\equiv(2,1).$$ But this is not the same as $x$. What am I doing/thinking wrong? I'm thinking it could be to do with how I'm working with ""inverses"", e.g. 1/2, in modular arithmetic?","['fourier-analysis', 'number-theory', 'modular-arithmetic', 'finite-fields', 'convolution']"
1437635,direct relationship between diffusion and wave equation,"We find direct relationship between the heat and wave equation. Let $u(x,t)$ solve the wave equation on the whole line, and suppose the second derivatives of $u$ are bounded. Let: 
$$v(x,t) = \frac{c}{\sqrt{4\pi kt}}\int_{-\infty}^{\infty}e^{-s^2c^2/4kt}u(x,s)ds$$ Show that $v(x,t)$ solves the diffusion equation $u_t -ku_{xx} = 0$ I know I have to write this $v(x,t)$ in the form of $v(x,t) = \int_{-\infty}^{\infty} H(s,t)u(x,s)ds$. But, I don't know how to differentiate this to satisfy the diffusion equation. Can someone please show me how? Thank you a lot.","['ordinary-differential-equations', 'partial-differential-equations']"
1437644,Tightness and Inner Regularity,"Let $P$ be a probability measure on a Borel $\sigma$-algebra (on some metric space, $\Omega$). It is called tight if for every $\epsilon >0$, there exists a compact $K$ such that $P(X \in K) \geq 1 - \epsilon$. It is called inner regular if $P(A) = \sup \{P(K) | K \subset A, K \text{ compact}\}$ for every $A$ in the Borel $\sigma$-algebra. It is clear that inner regularity implies tightness if we choose $A=\Omega$. For an arbitrary set $A$, using tightness we can obtain a
compact $K(\epsilon)$ such that $P(A\cap K(\epsilon)) \geq P(A) - \epsilon$. If $A$ is closed then $K \cap A$ is compact and hence we get
inner regularity for closed sets $A$. How can I prove this for general $A$? Thanks.",['measure-theory']
1437664,Does a series with bounded partial sums converge if the summands go to $0$?,"Let $a_0$, $a_1$, $a_2$, $\ldots$ be a sequence of real numbers. Suppose that the sequence $(S_n)$ is bounded, where $S_n = \sum_{k=1}^n a_k$ and that $a_n \to 0$ for $n \to \infty$. Does this imply that $(S_n)$ converges?","['analysis', 'sequences-and-series']"
1437675,Prove that the norm of a linear transformation satisfies the inequality $\|Tx\| \leq M\|x\|$ for all $x$. [duplicate],"This question already has an answer here : Proof that $\|fx\| \leq \|f\|\cdot\|x\|$ (1 answer) Closed 6 years ago . Proof. Let $T$ be a bounded linear operator. Then $$\|T\|=\sup_{x\neq 0}\frac{\|Tx\|}{\|x\|}$$ So $\|T\| \geq \frac{\|Tx\|}{\|x\|}$, that is, $\|T\|\cdot\|x\|\geq \|Tx\|$. I'm not understand if I let $M=\|T\|$. Is this proof correct?","['normed-spaces', 'functional-analysis', 'linear-transformations']"
1437701,Prove that if $\left|G\right|=105$ than $G$ has a normal Sylow $5-$subgroup and a normal Sylow a $7-$subgroup,"I cann't  prove the following statement (Dummit, Foote, Abstract algebra 4.5.17) Prove that if $\left|G\right|=105$ then $G$ has a normal Sylow $5-$subgroup and a normal Sylow a $7-$subgroup. It simply to prove that $G$ has a normal Sylow $5-$subgroup or a normal Sylow a $7-$subgroup: The number of Sylow  $5-$subgroup is of the form $n_5=(1+5k)$ Assume that $G$ hasn't a normal then $k\neq 0.$ Note that $n_5 |  21.$ Hence $k=4$ and $n_5=21.$ Then the number of elements g with $\left|g\right|=5, (g \in G),$ is $21\cdot4=84.$
The number of Sylow  $7-$supgroup is of the form $n_7=(1+7k).$ Assume that $G$ hasn't a normal then $k\neq 0.$ Note that $n_7 |  15.$ Hence $k=2$ and $n_7=15.$ Then the number of elements $g$ such that  $\left|g\right|=7, (g \in G),$ is $15\cdot6=90.$ Since $90+84>105,$  then  $G$ has a normal Sylow $5-$subgroup or a normal Sylow a $7-$subgroup. But why  does $G$ have a normal Sylow $5-$subgroup and a normal Sylow a $7-$subgroup?","['abstract-algebra', 'sylow-theory', 'group-theory', 'finite-groups']"
1437713,Functions where the total derivative is zero,"I came across the following problem: Let $f: \mathbb{R}^n \to \mathbb{R}$ be partial differentiable to every variable and let $\nabla f(x)=0, \forall x \in \mathbb{R}^n $ Proof that  $f$ is constant. Intuitively I understand that this is true and I sort of understand how to proof it. For a function from $\mathbb{R}$ to $\mathbb{R}$ I know this is true, but now I need to go to a N-dimensional function. My idea is as follows. Let $x,y \in \mathbb{R}^n$, now we need to show that $f(x)=f(y)$. For the one dimensional case this is already proven. So I would start by proving that $f(x_1,...,x_n) =f(y_1,x_2,...,x_n)$ and then the next step would be $f(y_1,x_2,...,x_n)=f(y_1,y_2,x_3...,x_n)$ etc. How would I do this formally and is this the correct approach? My second question is what would happen if we have a function $g : \mathbb{R}^n \to \mathbb{R}^p$ and the total derivative $Dg(x)=0, \forall x\in \mathbb{R}^n$. This function $g(x)$ should also be constant. How can I use this question to proof that? My thoughts on this case are that I should use a family of functions defined by: $\phi _i: \mathbb{R}^n \to \mathbb{R}, x \mapsto g_i(x)$ Then by using the first question all $g_i(x)$ are constant, but can I than say that the composition of all $g_i(x)$ is also constant?","['derivatives', 'real-analysis', 'functions']"
1437733,Finding roots and factors of multivariate polynomials,"I know that in order to factor a one dimensional polynomial one can find the roots with some method, for instance a numerical newton method. Then one can systematically divide with $(variable-root)$ for each root found and then be done. Is there any analogous way to do this for multivariate polynomials? Does there exist any ""unique"" or ""natural"" factorization for those? It is obvious we can do this in the case our polynomial is separable i.e. $$P(x_1,x_2,\cdots,x_n) = P_1(x_1)P_2(x_2) \cdots P_n(x_n)$$ because then we could just factor each $P_k(x_k)$ separately. But what about the general case?","['polynomials', 'factoring', 'multivariable-calculus']"
1437746,Reflexive but not separable space,"I'm trying to find an example of normed vector space that is reflexive but not separable. (Separable but not reflexive is easy, for example $L^1$).","['separable-spaces', 'examples-counterexamples', 'functional-analysis', 'reflexive-space']"
1437764,How to find the general solution of $yy^{''}-(y')^2=y^2lny$?,first I tried : $y'=p$ $y''=p\frac{dp}{dy}$ $yp\frac{dp}{dy}-p^2=y^2lny$ $w=p^2$ $\frac{dw}{dy}-\frac{2w}y=2y^2lny$ $wy^{-2}=2\int lnydy$ $p^2=2y+cy^2$ secondly $\frac{yy''-(y')^2}{y^2}=lny$ $d(\frac{y'}{y})=d(xlny+c)$ $\frac{y'}{y}=xlny+c$ and lastly $\frac{y''}{y'}-\frac{y'}{y}=\frac{ylny}{y'}$ $lny'-lny=\int\frac{ylny}{y'}$,"['ordinary-differential-equations', 'integration']"
1437774,Which one is greater $\left(5/2\right)^{2/5}$ or $\left(7/2\right)^{2/7}$?,"I have a question here: Which of $\left(5/2\right)^{2/5}$ and $\left(7/2\right)^{2/7}$ is greater? I tried comparison by the function $y=x^{1/x}$ and found the derivative as follows 
$$\frac{\partial y}{\partial x}=\frac{1}{x}(x^{\frac{1}{x}-1})+x^{1/x}\ln x=x^{1/x}(1/x^2+\ln x)$$ I got stuck here, what to do next? My teacher told me the answer is $\left(5/2\right)^{2/5}$. I want to know how it is.","['calculus', 'exponentiation', 'algebra-precalculus', 'number-comparison', 'derivatives']"
1437778,Map from a normed space to its double dual.,"Let $X$ be a normed space. Let the function $J:X \rightarrow X'' $ be defined by 
$$J(x)(x')=x'(x)\ \forall\ x'\in X' $$ where $X''=\{f:X'\rightarrow\mathbb{C} \mid \hbox{$f$ is bounded linear}  \}$ and $X'$ is the dual of $X$. Is $J$ injective? I was trying this, Suppose we have $x,y\in X $such that 
$$J(x)=J(y)$$
$$\Rightarrow  J(x)(x')=J(y)(x') $$
$$\Rightarrow x'(x)=x'(y)$$
$$\Rightarrow x'(x-y)=0$$
But to get $x=y $ we must have $x'$ to be injective.
I am stuck here. Is this the correct way to go about it?",['functional-analysis']
1437842,Three-phased probability,"In a TV show, you must pass 3 phases to make it to the live show.
From the first phase, $80$% of the people go home, the rest can continue to the second phase.
From the second phase, $70$% of the people go home, those who made it this far, have to get throught the third pase, there, only $25$% of the people succeed. There are $3$ several tasks, I wrote my ideas in each of them: 1, How many percent of the people can make it live? My idea: Let us have $X$ people in the beginning. After the first phase, only $\frac2{10}X$ people are still in. After the second phase, only $\frac3{10}*\frac2{10}X$ people are still in, and after the last phase, only $\frac14 * \frac3{10} * \frac2{10} X$ people are in, which is $\frac{3}{200}X$($1,5$% of the original) 2, There is one person, and we only know, that he made throught the first phase. What is the probability, that we will see him live? My idea: Since he made through the first phase, now he only need to make through the rest, which is $\frac{3}{10} * \frac{1}{4} = \frac{3}{40}$. 3, Examine the people, who didn't make it live. How many of them went home after the first, second and third phase? If we have $X$ people, $\frac{8}{10}X$ went home after the first phase, $\frac{2}{10}*\frac{7}{10}X$ went home after the second phase, and $\frac{2}{10}*\frac{3}{10}*\frac{3}{4}X$ went home after the last phase. Sorry for the long task, but I am quite new to this subject. Are my approaches correct? Thanks for any help!","['probability', 'statistics']"
1437869,intersections and unions of families of open sets,"Suppose that $F_j$ is a collection of open balls in $\mathbb{R}^n$ for each $j\in J$, where $J$ is a countable index-set. Let $A_1 := \text{int}\bigcap_{j\in J}\left( \bigcup_{B\in F_j} B \right)$ and
$A_2 := \bigcup_{B\in \cap_{j\in J} F_j}B$. I am trying to investigate if $A_1 = A_2$. I know that $A_2\subseteq A_1$. $A_1$ and $A_2$ are open so it suffices to check for an arbitrary open ball $B'\subseteq A_2$. Then there exists $\{B_k\}_{k=1}^\infty \subseteq \cap_{j\in J}F_j$ such that $B' = \cup_{k=1}^\infty B_k$. Thus $B'\subseteq \cup_{B\in F_j} B$ for all $j\in J$, and therefore also in the intersection. I have not been able to prove $A_1\subseteq A_2$, but I have not been able to construct a counter example either. Any help would be much appreciated. EDIT: Ok I just found a very simple counter example. Let $J = \{1,2\}$ with $F_1 = \{B_1\}$ and $F_2 = \{B_2\}$ for $B_1\neq B_2$ but $B_1\cap B_2\neq \emptyset$. Then $A_1 = B_1\cap B_2 \neq \emptyset = A_2$. So let me rephrase the question. Are there some mild conditions on the $F_j$ families such that $A_1\subseteq A_2$? One easy case is if $F_{j} = F_{j'}$ for all $j,j'\in J$. Could something similar hold if we let $J = \mathbb{N}$ and assume $\cap_{j=1}^\infty F_j \neq \emptyset$ and $F_{j+1}\subseteq F_{j}$?","['elementary-set-theory', 'general-topology']"
1437874,Confusing probability question,"I have got a task, which seems a quite confusing for me. It is simple: In a market, they sell eggs in egg holders, they store $10$ of them in each. There is $60$% chance, that all of the eggs are ok, $30$% chance, that exactly $1$ of them is broken, and $10$% chance, that exactly $2$ of them are broken(it is random, which one is broken). We buy an egg holder, and after we grab our first egg, we are sad, because it is broken. What is the probability, that there is one more broken egg in our holder? The ""logical"" way would be: $30$% of them have $1$ broken egg, $10$% of them have $2$, so, to have $2$ broken, the chance must be $\frac14$. But I am not really sure if that is the correct approach, since the broken egg can be anywhere, getting a broken one for first may be not that easy, or is that independent?(Maybe, I could use Bayes Theorem somehow)? Any help appreciated.","['probability', 'statistics', 'bayes-theorem']"
1437926,Asymptotics for incomplete $\Gamma$ with equal arguments,"On the digital library of mathematical functions, there is a uniform asymptotic expansion of the incomplete gamma function of equal arguments $\Gamma(z,z)$ for large $z$ ( http://dlmf.nist.gov/8.11#v -- formula 8.11.12). The lowest order terms are $$\Gamma(z,z) \sim z^{z-1}e^{-z} \left(\sqrt{\frac{\pi}{2}} z^{\frac{1}{2}} - \frac{1}{3} + \frac{\sqrt{2 \pi} }{24 z^{\frac{1}{2}}} + \ldots \right)$$ The digital library unfortunately cites no sources. Anybody has any idea how this was derived? I have tried to derive it with Laplace's approximation. In the integral that defines the incomplete $\Gamma$ , the maximum of the integrand is at $t=z-1$ while the lower limit of integration is $z$ . So the maximum lies outside the integration range. You must therefore approximate the integrand for small positive values of $t -z$ , and this gives something completely different from the digital-library formula. Moreover: the function I am really interested in is $\Gamma(z,z-a)$ , calculated for large $z$ . Here $a >1$ is a fixed real number, so the maximum is outside the integration range. But the Laplace method may not be sufficient because I need corrections from the $t=z-a$ too. Does anybody have any hint on how to calculate this?","['asymptotics', 'analysis']"
1437938,"Problem with limit 2 variables $\lim_{(x,y) \to (0,0)} \frac{x^2 y^2}{x^2 + y^2}$ [duplicate]","This question already has answers here : evaluating the limit $ \lim_{(x,y) \rightarrow (0,0) } \frac{x^2y^2}{x^2 + y^2} $ [duplicate] (5 answers) Closed 8 years ago . I tried to solve the limit: $\lim \frac{x^2 y^2}{x^2 + y^2}$
with $(x,y) \to (0,0)$ I tried it with the paths $x=0, y=0, y=x, y=x^2$ and everything went to $0$. Now I'm suspicious that this limit really goes to $0$, but how I prove it? Thank you and sorry my english.","['calculus', 'limits', 'multivariable-calculus']"
1437943,General formula for integration on $m$-dimensional hypersurface in $\mathbb{R}^n$ ($m<n$)?,"Let $S$ be a two-dimensional surface embedded in $\mathbb{R}^3$. We suppose that $S$ is parameterized as $\vec x (t,s)$, where $t,s$ vary in some region $T$ of the plane. Then the surface integral of a scalar function $f(\vec x)$ over $S$ is given by: $$\int\int_S f dS = \int\int_T f(\vec x(t,s)) \left| \frac{\partial\vec x}{\partial s} \times \frac{\partial\vec x}{\partial t} \right| dt ds $$ What is the generalization of this formula to hypersurfaces of dimension $m$ embeeded in $\mathbb{R}^n$, where $m<n$? That is, let $S$ be parameterized as $\vec x(t_1,\dots,t_m)$, where $\vec x \in \mathbb{R}^n$ and $t_1,\dots,t_m$ vary over some region of $\mathbb{R}^m$. Then how can I express the following hypersurface-integral: $$\int\dots\int_S f dS$$ as an integration over the variables $t_1,\dots,t_m$? Note that I am assuming the usual Euclidean measures in $\mathbb{R}^n$.","['multivariable-calculus', 'integration']"
1437946,A.s. convergence of densities implies convergence in distribution?,"Problem. Let $X,X_1,X_2,...$ be random variables with distribution functions $F, F_1, F_2,...$ and $\lambda_1$ densities $f, f_1,f_2,..$ respectively. Is it true that $$\lim_{n\to \infty} f_n = f \text{ a.s.} \implies X_n\to_dX?$$
  Is the converse true? I think that ""$\implies$"" is true and the converse is false. Suppose $f_n\to f$ a.s. By Polya's theorem, $X_n\to_dX$ is equivalent to $\sup_{x\in \Bbb R} |F_n(x) - F(x)| \to 0$, so let's try to show that. $$\begin{eqnarray} \sup_{x\in \Bbb R} |F_n(x) - F(x)|&=&\sup_{x\in \Bbb R} \left| \int_{(-\infty,x)}f_n(t)dt - \int_{(-\infty,x)}f(t)dt\right|
\\&\leq& \sup_{x\in \Bbb R} \int_{(-\infty, x)}|f_n(t)-f(t)| dt
\\&=& \int_{\Bbb R}|f_n(t)-f(t)| dt
\end{eqnarray}$$
which converges to $0$ as $n\to \infty$ by ScheffÃ©'s Lemma. Is that an acceptable proof for ""$\implies$""?
I was not able to prove the converse, nor find a counter example to it, but I suspect it is false. Could you please share a hint? Thank you.",['probability-theory']
1437962,"In an interval notation answer, are you supposed to put a space between the two terms or not?","For example, if the answer is [-1, â) should you write it like that or like this [-1,â). Does it make a difference?","['notation', 'trigonometry']"
1437965,"How to solve this limit 2 variables $\lim_{(x,y) \to (4,1)} \frac{y \sqrt x - 2y - \sqrt x + 2}{4 - x + x \sqrt y - 4 \sqrt y}$","Please anybody can help me solve this? $$\lim \frac{y \sqrt x - 2y - \sqrt x + 2}{4 - x + x \sqrt y - 4 \sqrt y}$$
with $(x,y) \to (4,1)$ Thank you!","['calculus', 'limits', 'multivariable-calculus']"
1437993,Differentiability of a 2-variable function,"In this morning's Mathematical Analysis 2 exam, students were asked to study the continuity and differentiability of: $$f(x,y)=\left\{\begin{array}{cc} \dfrac{xye^{-\frac{1}{(x+y)^2}}}{x^2+2e^{-\frac{2}{(x+y)^2}}} & x\neq-y \\ {} \\ 0 & x=-y \end{array}\right.$$ This question answered the continuity problem. However, I feel if I try the same things I tried there for the differentiability, I will get stuck on the limit involved. Now, it is easy to verify that the partial derivatives in the origin are both zero, since the function is always 0 on the axes. So to prove differentiability, I would have to prove: $$\lim_{(h,k)\to(0,0)}\frac{f(h,k)}{\sqrt{h^2+k^2}}=0,$$ which is the limit proved zero for continuity except for the denominator, which makes it impossible to use the trick the answer to the linked question used. Polar coordinates give all the same problem as continuity. I can try substituting $k=m|h|^\alpha$, so for $\alpha<\frac12$ I can use the asymptotic in the comments to the linked question, and I'm left with $\frac{m|h|^{\alpha-2}\operatorname{sgn}h}{\sqrt{1+m^2|h|^{2\alpha-2}}}e^{-\frac{1}{m^2|h|^{2\alpha}}}$, and I'm not all too sure that tends to 0. And anyway I would still find problems with $\alpha>\frac12$, and L'Hospital, in this case, is just terrible. Any suggestions? PS The present question and this one are NOT duplicates, because that one focuses on continuity and this one focuses on differentiability. I thought I had made that clear in this question, but evidently it is not that clear since this question has been marked as a possible duplicate of the continuity one.","['multivariable-calculus', 'limits', 'differential']"
1437995,"Does the inner product $\langle \cdot, \cdot \rangle$ induce any other norms other than the 2 norm?","In the lecture my professor wrote that the standard inner product on $R^n$ is given by $\langle x, y \rangle = x^Ty = \sum\limits_{i=1}^n x_i y_i$ which induces a norm $\sqrt{\langle x,x \rangle} = \|x\|_2$ My question is do inner products induce other types of norms...or rather are norms such as the 1-norm or the $\infty$-norm induces by some inner product?","['vector-spaces', 'linear-algebra', 'inner-products', 'normed-spaces']"
1438065,Conditions for Linear Independence for functions defined by integration,"Given that the set of strictly positive and continuous functions $$f_i(x,y) >0, \quad i=1,\dots,n$$  are defined on $[0,1]^2$  and
 $\mathbb{R}$-linearly  independent for $(x,y)  \in [0,1]^2$. That is if $c_1, \ldots, c_n \in \mathbb{R}$ and  $\sum_{i=1}^n c_i f_i(x,y)=0$ for all $(x,y) \in [0,1]^2$ then each $c_i$ is zero.
 Let $g_i$ be defined by
$$
g_i(x) = \int_{y\in [0,1] } f_i(x,y) d y, \quad  i=1,\dots,n
$$
what are other conditions are needed on $f_i(x,y)$ so that the set of functions $g_i(x)$ are  $\mathbb{R}$-linearly  independent for $(x,y)  \in [0,1]$. I'm having difficulties on this problem and any suggestions or references to read would be greatly appreciated. Some thoughts or attempts that I have done are If $f_i(x,y)= h_i(x) k_i(y)$ where the set of $h_i(x)$ is  $\mathbb{R}$-linearly  independent for $x  \in [0,1]$ and if the set of $k_i(y)$ is  $\mathbb{R}$-linearly  independent for $y  \in [0,1]$ then both $f_i(x,y)$ and $g_i$ are $\mathbb{R}$-linearly  independent? If $f_i(x,y)$ is also $\mathbb{R}$-linearly independent when $x \in [0,1]$ for every fixed $y  \in [0,1]$. I'm just not sure how the proof would work in this case.",['functional-analysis']
1438076,"How do I change âx, ây, P(x, y) into ây, âx, P(x, y)?","I'm very confused as to how to even begin, any explanation or help would be really appreciated. I understand Universal and Existential Quantifiers but the actual process of proving it is what confuses me.",['discrete-mathematics']
1438143,Density given by variable-coefficient PDE,"I am looking for a time-dependent probability density $f(x,y,t)$ solving the equation $$-\frac{\partial f}{\partial t} = \alpha\cdot \big(y - F(x)\big)\frac{\partial f}{\partial x}+\beta\cdot \big(G(y)-x\big)\frac{\partial f}{\partial y},$$
where the coefficient functions $F$ and $G$ given by\begin{align}F(x) &= a_0 + a_1 x + o(x - x^{0}),\\ G(y) &=b_0 + b_1y + o(y-y^{0}).\end{align} My question is: What general class of equations does this PDE belong to, and what would be your angle of attack on such problem (if it makes sense at all)? Remark I: Under the assumption of existence of a solution, my initial angle of attack was to rewrite the equation as
$$i \frac{\partial f}{\partial t} = L\,f$$
where $L$ is an operator acting on $f$ given by $L = -i\left[\alpha\big(y - F(x)\big)\frac{\partial}{\partial x}+\beta\big(G(y)-x\big)\frac{\partial}{\partial y}\right]$. Then - I suppose - we can formally express the density $f$ as $$f(x,y,t)=\sum_k c_ke^{-i\lambda_k t}\varphi_k(x,y),$$
where $\lambda_k$ and $\varphi_k$ are the eigenvalues and eigenfunctions corresponding to the operator $L$. I am thus led to considering the equations 
$$L(\varphi_k) = \lambda_k\,\varphi_k$$
and swap the solutions $\varphi_k$ and $\lambda_k$ back the into the formal solution. But I have not been able to find the eigenfunctions, perhaps my patience was too short. Remark II: A necessary condition, I suppose, for the existence of a density solving the PDE is the existence of curves $x$ and $y$ solving the ""predator-prey"" equations
$$\frac{dx}{dt} = \alpha\cdot (y - F(x)),\qquad \frac{dy}{dt} = \beta\cdot (G(y)-x).$$
If we neglect the little-o terms, by putting them equal to zero  (is that completely illegal?), the curves $x$ and $y$ are given by \begin{align}x(t) &=-\frac{a^0}{a^1} + C_1 e^{-a^1\mu t} + \frac{1}{1-a^1}\left[\frac{b^0}{b^1}+\frac{a^0}{a^1b^1}-C_2 e^{b^1\lambda t}-\frac{C_4}{b^1}e^{-a^1\mu t}\right]\\ y(t) &= \frac{a^1}{1-a^1}\left[\frac{b^0}{b^1}+\frac{a^0}{a^1b^1}-C_5 e^{b^1\lambda t}-\frac{C_6}{b^1}e^{-a^1\mu t}\right] \end{align}
where $C_1$, $C_2$, $C_3$, $C_4$, $C_5$, and $C_6$ are arbitrary constants.","['probability-theory', 'functional-analysis', 'probability', 'ordinary-differential-equations', 'partial-differential-equations']"
1438160,Intersection of clopen sets that contain x is the connected component of x (if X is compact),"Let $X$ be a topological space, $x\in X $, $C$ is a connected component of $x$. Define $A$ to be the intersection of all the open-and-closed sets that contain $x$ (also called the pseudo-component sometimes). I wish to show that $A=C$, if $X$ is also compact (without it I think there is a counter-example). Obviously $C$ is contained in $A$ (always) since any clopen set that contains $x$ contains $C$.
Also, $A$ is closed in $X$ so if $X$ is compact, $A$ is also compact. Not sure on how to proceed (tried supposing that $A$ is not contained in $C$ and getting a cover of $A$, doesn't seem to work). Any help will be appreciated!","['examples-counterexamples', 'connectedness', 'general-topology', 'compactness']"
1438216,Permutation of multiset,"How many 8-permutation are there of the letters of the word
  'ADDRESSES'? My textbook suggests that we should divide the situation into cases where a different letter is removed. In other words, for the multiset $\{1 \cdot A, 2 \cdot D, 1 \cdot R, 2 \cdot E, 3 \cdot S\}$, we count the number of permutation of the follow set: $\{0 \cdot A, 2 \cdot D, 1 \cdot R, 2 \cdot E, 3 \cdot S\}$ $\{1 \cdot A, 1 \cdot D, 1 \cdot R, 2 \cdot E, 3 \cdot S\}$ $\{1 \cdot A, 2 \cdot D, 0 \cdot R, 2 \cdot E, 3 \cdot S\}$ $\{1 \cdot A, 2 \cdot D, 1 \cdot R, 1 \cdot E, 3 \cdot S\}$ $\{1 \cdot A, 2 \cdot D, 1 \cdot R, 2 \cdot E, 2 \cdot S\}$ It is easy to show that the total number of 8-permutation is $15120$. I am not happy with this case-dividing computation and I want to have a direct computation of the result. Accidentally, I find that $$C_8^9\frac{8!}{2!2!3!}=15120.$$ I further test this formula, e.g. 3-permutation of a multiset with 4 elements, and it actually works. So I think there should be a nice explanation why the formula works. Can anyone explain that for me?","['combinatorial-proofs', 'combinatorics']"
1438238,"Can $ \lbrace \cap_{i\in J} A_i \rbrace_{J\subset \mathbb{N} , |J| = \infty} $be uncountably infinite?","Given countable collection $\lbrace A_i \rbrace$ of subsets $[0,1]$,can the collection $$ \lbrace \cap_{i\in J} A_i \rbrace_{J\subset \mathbb{N} , |J| = \infty} $$
be uncountably infinite?",['elementary-set-theory']
1438242,Finding ray direction with smallest angle from a ray to a rectangle in 3D,"In 3D I have been given a ray and a rectangle: $$Ray=\mathbf{r_o}+t*\mathbf{r_d}$$
 $$Rect=\mathbf{p_o}+p*\mathbf{p_x}+q*\mathbf{p_y}$$ where $\mathbf{r_o}$ is the ray origin $\mathbf{r_d}$ is the ray unit direction vector $t>0$ $\mathbf{p_o}$ is the rectangle center $-rectWidth/2 < p < rectWidth/2$ $-rectHeight/2 < q < rectHeight/2$ $\mathbf{p_x}$ is the rectangle unit x-vector $\mathbf{p_y}$ is the rectangle unit y-vector $\mathbf{p_x}$ and $\mathbf{p_y}$ are orthogonal, i.e. $\mathbf{p_x} \cdot \mathbf{p_y} = 0$ I'm trying to find $$Ray'=\mathbf{r_o} + t*\mathbf{r_d'}$$ that intersects the rectangle and minimizes the angle between $\mathbf{r_d}$ and $\mathbf{r_d'}$ (in other words maximizes $\mathbf{r_d} \cdot \mathbf{r_d'}$). If $\mathbf{r_d}$ intersects the rectangle then naturally $\mathbf{r_d'}=\mathbf{r_d}$ and otherwise $\mathbf{r_d'}$ points towards the rectangle edge. I have tried couple of solutions which unfortunately do not give correct results: 1) Calculate $p$ and $q$ at the intersection between the ray and the plane of the rectangle, and clamp $p$ and $q$ to the rectangle domain. Then calculate $\mathbf{v}=\mathbf{p_o}+p*\mathbf{p_x} + q*\mathbf{p_y} - \mathbf{r_o}$ and $\mathbf{r'_d}=\mathbf{v}/||\mathbf{v}||$. This works ""mostly right"", but breaks down when the ray points away from the rectangle (i.e. there's no intersection). Even if the ray points towards the rectangle, clamping $p$ and $q$ between $-rectWidth/2 < p < rectWidth/2$, and $-rectHeight/2 < q < rectHeight/2$ doesn't give correct result in all cases. 2) Project the ray to the rectangle plane and calculate intersection of the infinite line going along the projected ray and the rectangle. If the line intersects the rectangle, test which of the two intersection point gives the smallest angle. If the line doesn't intersect the rectangle, test which of the 4 corners of the rectangle gives the smallest angle. This seems to be quite wrong result and worse than option 1) I don't need the exact solution for my application, so if there are some approximations that doesn't have too large errors I'm happy to try those out. If it helps, the rectangle parameters can also be simplified with affine transformation of the ray so that $\mathbf{p_o}=[0, 0, 0]$, $\mathbf{p_x}=[1, 0, 0]$, $\mathbf{p_y}=[0, 1, 0]$, $rectWidth=1$ and $rectHeight=1$.","['geometry', '3d']"
1438262,Serre's surjective theorem importance.,"I'm studying Serre's paper in wich he shows the following theorem: Let K be a number field, $E$ an elliptic curves over K without CM. Then the representation $$\rho_{\ell}:\mathrm{Gal}(\bar K/K)\longrightarrow\mathrm{Aut}(E[\ell])$$ is surjective for all but finitely prime numbers $\ell$. I see the beauty of this theorem, however what consequence it has? What is its importance?","['number-theory', 'galois-theory', 'representation-theory']"
1438271,What is the precise definition of the ambient space,"For instance, what is the ambient space of a singleton $\{x\}$, where $x \in \mathbb{R}$? Can it be the singleton itself? $\mathbb{R}$? $\mathbb{R}^n$? or some arbitary set that happens to contain $\{x\}$?","['geometry', 'definition', 'terminology']"
1438319,Prove that $\lim_\limits{x\to 0}{\frac{e^x-1}{x}}=1$ without derivatives,"Prove that $\lim_\limits{x\to 0}{\frac{e^x-1}{x}}=1$ . I currently know only one approach (using L'Hopital 's Rule and derivatives) as follows: $$\lim_\limits{x\to 0}{\frac{e^x-1}{x}}=\lim_\limits{x\to 0}{\frac{\left(e^x-1\right)'}{x'}}=\lim_\limits{x\to 0}{\left(e^x\right)}=e^0=1$$ Here I ask for other proofs than those, preferably neither using derivatives in any way nor using Taylor, etc. For the purposes of this post, I define the exponential by any of the following limits: $$e^x
=\lim_\limits{n\to +\infty}{\left( 1+\frac{x}{n}\right)^n}=\lim_\limits{n\to +\infty}{\left[ \left( 1+\frac{1}{n}\right)^{n\cdot x}\right] }=\left[ \lim_\limits{n\to +\infty}{\left( 1+\frac{1}{n}\right)^n}\right] ^x.$$ Note: An approach for $\lim_\limits{x\to 0^+}{(x\ln x)}$ without using derivatives can be found here .","['limits-without-lhopital', 'limits', 'exponential-function']"
1438364,Limit representation for $\log(x)$,I know you can express $\log(x)$ as $$ \lim_ {n\rightarrow\infty} n (x^{1/n} - 1) $$ But I'm have a hard time getting started. Any hints?,"['calculus', 'limits', 'logarithms']"
1438381,"Closed-form of $\int_0^1 \frac{\ln^2(x)}{\sqrt{x(a-bx)}}\,dx$","I'm interesed in the following integral, for $a,b>0$:
$$
\mathcal{I}(a,b) := \int_0^1 \frac{\ln^2(x)}{\sqrt{x(a-bx)}}\,dx
$$ Mathematica could evaluate it in term of hypergeometric functions, but I'm looking for a simpler closed-form. If it is too difficult, then it would be nice to see a proof for the following two special cases:
$$\begin{align}
\mathcal{I}(1,1) &= \int_0^1 \frac{\ln^2(x)}{\sqrt{x(1-x)}}\,dx = \frac{\pi^3}{3}+4\pi\ln^2(2)\\
\mathcal{I}(4,1) &= \int_0^1 \frac{\ln^2(x)}{\sqrt{x(4-x)}}\,dx = \frac{7\pi^3}{27}
\end{align}$$
Any other simple special case are welcome.","['closed-form', 'calculus', 'definite-integrals', 'integration']"
1438386,$C^k$ one-parameter family of metrics,"Consider a smooth Riemannian manifold $M$ and a $C^k$ one-parameter family of Riemannian metrics $g_t$ on $M$. Here $k$ could be any integer, $k$ could be infinity, when the one-parameter family $g_t$ is smooth in time, or $k = \omega$, when the one-parameter family $g_t$ is real analytic in time. Now, as the metric varies, the Laplacian associated to the metric varies, and hence its spectrum also varies in time. My question is, if $g_t$ is $C^k$ in time, are the eigenvalues of the Laplacian also $C^k$ in time for $k$ integer, $k = \infty$ or $k = \omega$? In case such results are well-known (which I am assuming they are), what is a good reference to learn about such results? Thanks for any guidance.","['riemannian-geometry', 'functional-analysis', 'reference-request', 'differential-geometry', 'spectral-theory']"
1438411,are surjective etale morphisms between connected schemes finite?,"Let $f : X\rightarrow Y$ be a surjective etale morphism with $X,Y$ both connected (maybe I need to assume noetherian?) Must $f$ be finite? If not, are there conditions can I impose on $X$ and $Y$ to ensure that $f$ is finite?",['algebraic-geometry']
1438452,Matrices with Parallel Column Differences,"I have two $p\times q$ matrices $A$ and $B$ that belong to the set:
$$
\{X\in R^{p\times q}: \sum_{i,j} X_{i,j}^2=1 ; \sum_{i,j} X_{i,j}=0 \}
$$ Also, showing the $i$'th column of $A$ by $a_i$, I know that
$$
(a_i - a_j) = c_{ij} (b_i-b_j) ; \qquad c_{ij}>0
$$ 
which means that $(a_i - a_j)$ and $(b_i - b_j)$ are parallel and in the same direction for all $i$ and $j$. Can I conclude $A=B$ ?","['matrix-calculus', 'linear-algebra', 'algebra-precalculus', 'matrices']"
1438462,Expressing an inequality constraint as a linear matrix inequality (LMI),"I am trying to formulate an optimization problem as a semidefinite program (SDP). My optimization variable is  $\mathbf x = [x_1, x_2, \dots, x_N]'$, where $\mathbf x$ is an $N \times 1$ vector, and one of my constraints is $$\prod_{i=1}^{N} x_i \ge a$$ Can I express this inequality constraint as a linear matrix inequality (LMI)?","['matrices', 'semidefinite-programming', 'optimization', 'linear-matrix-inequality', 'linear-algebra']"
1438495,Mistake in reasoning about Sobolev spaces,"I am new to Sobolev spaces and, while trying to construct a proof, I make some subtle mistake that I cannot detect. The setting: let $C \subset \Bbb R^n$ be a closed, measure-$0$ set. Let $U = \Bbb R ^n \setminus C$. Let $f : \Bbb R ^n \to \Bbb C$ be continuous, $f$ smooth of infinite order on $U$ but not derivable on $C$, and $f \in L^p (\Bbb R^n)$. 1) Clearly, $f \in W ^{\infty, p} (U) \ \forall p \ge 1$. 2) By defining $f_\alpha (x) = \left\{ \begin{array} {cc} (\partial _\alpha f) (x), & x \in U \\ 0, & x \in C \end{array} \right.$ for every multi-index $\alpha$, $f_\alpha$ is a weak derivative of $f$ on $\Bbb R^n$, so $f \in W^{\infty, p} (\Bbb R^n)$. Note that this seems to have nothing to do with Sobolev's extension theorem. 3) Localize: choose a small enough ball $B_x$ around every $x$. The restriction $f \big| _{B_x}$ will belong to $W ^{\infty, p} (B_x)$. 4) By Sobolev's embedding theorem, $f \big| _{B_x}$ will be smooth of infinite order on $B_x$. 5) Glueing these restrictions together, $f$ will be smooth of infinite order on $\Bbb R^n$, in particular on $C$, where is was supposed not to be so. Where am I wrong? (There may be several mistakes above, not just one.) (The motivation behind my question: replace $\Bbb R^n$ by a Riemannian manifold $M$, fix some $p \in M$ and let $f = d(p, \cdot)^2$ ($d$ the distance) and let $C$ be the cut locus of $p$.)","['sobolev-spaces', 'distribution-theory', 'functional-analysis', 'lp-spaces', 'weak-derivatives']"
1438516,Can I derive the formula for expected value of continuous random variables from the discrete case?,"I've previously asked the question on Stats SE, but I guess it fits the Math SE better. Is it possible to rigorously derive the formula for expected value of continuous random variable starting with expected variable in discrete case, i.e. $$E[X] = \sum_{i=1}^{n}p_i x_i$$ to obtain $$E[X]=\int_{-\infty}^{\infty}xf(x)dx$$ When formulating the definition for continuous case, the intention was, I believe, to make it 'equivalent' to the discrete case. So for example, I'd like $E[X]$ of a continuous random variable $X$ to be equal to the sum of every possible value of $X$ times probability of that particular value. The problem is that the probability of any particular value of $X$ is $0$, and the expected value calculated that way would always be $0$. But some people tried to convince me that it's possible overcome these issues with help of Lebesgue integral. Could anyone explain intuitively how is that possible? I'm convinced that no matter what integration we use, we cannot somehow magically assign non-zero probabilities to single values the random variable $X$ might take. They will always be $0$! Or maybe there's no magic involved and best we can do is work with infinitely thin intervals of $X$? From what I've managed to find out (I don't have time to study all measure theory and Lebesgue integration at the moment to figure it out on my own) it's about approximating the original continuous random variable $X$ with step function, and the more steps there are, the better is the approximation. But still, all we have is calculating probability of intervals (they are infinitely thin though, but they are intervals anyway). The fact that something gets closer and closer to original function in the limit doesn't mean it behaves the same as the original function (check the very popular example here ). In the 'very popular example' above, even though the curve approaches the circle, its length will never be the same as the perimeter of that circle. 
Similarly, here , in 'The Riemann-Stieltjes integral: intuition' part. they say the discrete r.v. $X_n$ converges to continuous r.v. $X$, as $n \to \infty$, in the limit it becomes the same variable. So it's important to ask whether it's reasonable to expect the approximation of the cont. r.v. $X$ to behave the same as the original random variable $X$, even if in the limit the are 'indistinguishable' in the limit, whatever that word means in mathematics. The curve and circle are indistinguishable too, but still have different properties. So apparently it's that the continuous case is not derived from discrete case, but is a generalization of the discrete case. I guess in every mathematical theory, there is no such thing as 'the only correct' generalization, so the contunous formula could look different. If you claim this is the only 'valid one', then shouldn't we call it a derivation of the formula?","['probability', 'calculus', 'lebesgue-integral']"
1438549,The affine special linear group acts doubly transitive,"Let $F$ be a field and $d \ge 2$. Denote by $ASL_d(F)$ the affine special linear group , i.e. the group of all transformation on $F^d$ with $t_{A,v}(u) = Au + v$ and $\det A = 1$. I want to show that this group is $2$-transitive (also called doubly transitive) on the points from $F^d$. For this I have to show that for $x,y, x', y'$ with $x \ne y$ and $x' \ne y'$ I can find an element $t_{A,v} \in ASL_d(F)$ such that
$$
 x' = t_{A,v}(x) = Ax + v \qquad\mbox{and}\qquad
 y' = t_{A,v}(y) = Ay + v.
$$
Any ideas or hints how to solve this? I have solved it in the case $d = 2$ by a cumbersome solution of the equations involved, with the restriction on the determinant for $d = 2$ with the above I have five equations and six unknows (the elements of the matrix and of the translation vector $v$), so this could be solved. But my solution is quite messy and involves a lot of rearrangemnt and equation handling, so any short solution would be preferred?","['linear-transformations', 'abstract-algebra', 'group-theory', 'group-actions', 'linear-algebra']"
1438566,Intuition and slick proof that distributions are special cases of hyperfunctions?,"I am looking for some slick or simple proof that distributions are a special case of hyperfunctions. Furthermore, what is the intuition behind this fact? Why should one think of hyperfunctions as 'distributions of infinite order' as wikipedia says? I have not studied hyperfunctions, so I only think of them naively as the difference of two holomorphic functions (in one variable) on the real line. [ By the way, I think making a hyperfunctions tag may be worth considering .]","['distribution-theory', 'functional-analysis']"
1438567,Questions concerning the Integration of Integer Tetration,"I've been interested in finding the antiderivative of integer tetration, a function defined as iterative exponentiation. Integer tetration is written as $^n$$x$ where $^1$$x =x$, $^2$$x =x^x$, $^3$$x =$ $x^{\scriptscriptstyle x^{x}}$ and so forth where $n=1,2,3\ldots$ (Further info can be found on Wikipedia . One solution to the problem is found on the MathWorld page (see equation #10), but the given solution is difficult to evaluate; accordingly, I am searching for an simpler representation than the result given. (Note that my questions are located at the bottom of this post, and number theorists are encouraged to skip to that point... everything leading up to the questions is background and context.) Using Wolfram Alpha I found the Puiseux series of $x^x$ to be the following:$$1+ x\log(x)+\frac{1}{2}x^2\log^2(x) + \frac{1}{6}x^3\log^3(x)\ldots$$ $$ = \sum_{n=0}^{\infty} \frac{x^n \log^n(x)}{n!} \qquad 1.$$ 
As such, the antiderivative can be found simply by integrating the series:
$$\int x^xdx = \int \bigg(\sum_{n=0}^{\infty} \frac{x^n \log^n(x)}{n!}\bigg)dx = \sum_{n=0}^{\infty}\bigg(\frac{1}{n!}\int x^n \log^n(x)dx\bigg)$$
We then note the following: (again, pulled from Wolfram Alpha)
$$\int x^n \log^n(x)dx = \frac{\Gamma(n+1,-(n+1)\log(x))(-n-1)^{-n} }{(n+1)} \qquad 2. $$
All that is left is to substitute in Equation 2 and convert the denominator the gamma function.
$$\sum_{n=0}^{\infty}\bigg(\frac{1}{n!}\int x^n \log^n(x)dx\bigg) = \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1,-(n+1)\log(x))(-n-1)^{-n} }{\Gamma(n+2)}\bigg)$$
This solution can be checked by substituting in x=1, yielding the Sophomore's Dream :
$$ \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1,-(n+1)\log(1))(-n-1)^{-n} }{\Gamma(n+2)}\bigg) = \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1,0)(-n-1)^{-n} }{\Gamma(n+2)}\bigg)$$
$$= \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1)(-n-1)^{-n} }{\Gamma(n+2)}\bigg) = \sum_{n=0}^{\infty}\bigg((-1)^n(n+1)^{n+1}\bigg)$$
$$= 1-\frac{1}{4}+\frac{1}{27}-\frac{1}{64}\,\ldots \, = -\sum_{n=1}^{\infty}(-n)^{-n} = \int_{0}^{1}x^xdx$$
Further, any other definite integral of $x^x$ can be calculated the same way. (Note that $F'(x)$ is $x^x$)
$$\int_{0}^{r}x^xdx = F(r) - \lim_{a\to 0}F(a)$$
$$= F(r) - \sum_{n=0}^{\infty}\lim_{a\to 0}\bigg(\frac{\Gamma(n+1,-(n+1)\log(a))(-n-1)^{-n} }{\Gamma(n+2)}\bigg) = F(r)$$ No standard Puiseux series can be found for $^n$$x$ when $n>2$, although Wolfram Alpha tells me that ""Generalized Puiseux series"" exists for these functions. For $^3$$x$ the series is as such:
$$x + x^2\log^2(x) + \frac{1}{2}x^3\log^3(x)[1+\log(x)] + \frac{1}{6}x^4\log^4(x)[1+3\log(x)+\log^2(x)]\ldots$$
Notice that this series closely resembles Equation 1, except for the additional powers of $\log(x)$. If we temporarily exclude the coefficients on these additional terms we find that the $n^{th}$ term of the series is multiplied by $\sum_{n=0}^{n-2}\log^n(x)$ (except for the first term). As a result, we can represent the series as follow, where the additional coefficients are represented as $C_{kn}$
$$\sum_{n=2}^{\infty}\bigg[\frac{x^n\log^n(x)}{\Gamma(n)}\sum_{k=0}^{n-2}\bigg(C_{kn}\log^k(x)\bigg)\bigg] = \sum_{n=2}^{\infty}\sum_{k=0}^{n-2}\bigg(\frac{C_{kn}\log^{k+n}(x)x^n}{\Gamma(n)}\bigg)  \qquad 3. $$
Listing the coefficients generated, we get $[1],[1,1],[1,3,1],[1,7,6,1]\ldots$ A quick search on the OEIS revealed that these are in fact the Stirling numbers of the second kind, written as $S_n^k$ or $S(n,k)$. Substituting this into the series representation above, we get the following series for $^3$$x$:
$$x + \sum_{n=2}^{\infty}\sum_{k=0}^{n-2}\bigg(\frac{S_{n-1}^{k+1}\log^{k+n}(x)x^n}{\Gamma(n)}\bigg) = x + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{S_{n+1}^{k+1}\log^{k+n+2}(x)x^{n+2}}{\Gamma(n+2)}\bigg)$$ Going through a similar process to the one outlined for $x^x$, $\int$ $^3$$xdx$ can be shown to be the following equation:
$$\frac{x^2}{2} + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{\Gamma[n+3+k,-(n+3)\log(x)]S_{n+1}^{k+1}}{\Gamma(n+2)(-(n+3))^{n+3+k}}\bigg)$$ Likewise, we find that
$$\int_{0}^{r} {^3x}dx = \frac{r^2}{2} + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{\Gamma[n+3+k,-(n+3)\log(r)]S_{n+1}^{k+1}}{\Gamma(n+2)(-(n+3))^{n+3+k}}\bigg)$$
The hard part is when we get to the generalized Puiseux series for $^4x$, which is:
$$1+x\log(x)+\frac{1}{2}x^2\log^2(x)[1+2\log(x)]+\frac{1}{6}x^3 \log^3(x) [1+9 \log(x)+3\log^2(x)]+\ldots$$
$$***$$
Here is where the real questions start. This series has the first few powers of $x\log(x)$ as $x^x$, namely $1+x\log(x)$. I wonder if this is related to the fact that the function $y = (x^x)^y$ is the same as $y = (^{2n}x)^y$? Equivalently, $y=x^y$ is the same as $y = (^{2n-1}x)^y$. We can see this same symmetry in that for all $^{2n-1}x$ I checked (up to $^7x$) the first two terms are $x+x^2\log^2(x)$, while for all $^{2n}x$ I checked the first two terms are $1+x\log(x)$. Thus, my first question is somewhat broad... Is there any simple explanation for this symmetry ? My next question concerns the series for $^4x$. We can represent this in a very similar form to Equation 1 , except for the additional powers of $\log(x)$, reminiscent of the additions to Equation 1 . Thus, we get the form
$$\sum_{n=0}^{\infty} \bigg[\frac{x^n \log^n(x)}{n!}\sum_{k=0}^{n-1}\bigg(C_{kn}\log^k(x)\bigg)\bigg] = 1 + \sum_{n=1}^{\infty} \bigg[\frac{x^n \log^n(x)}{n!}\sum_{k=0}^{n-1}\bigg(C_{kn}\log^k(x)\bigg)\bigg]$$
$$= 1 + \sum_{n=1}^{\infty}\sum_{k=0}^{n-1}\bigg(\frac{x^n \log^{n+k}(x)C_{kn}}{n!}\bigg) = 1 + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{x^{n+1} \log^{n+1+k}(x)C_{kn}}{\Gamma(n+2)}\bigg)$$
Having already worked with $^2x$ and $^3x$ integrating this series would not be difficult, but I can't figure out what the constants are. Thus, my main question is: What is the pattern for the numbers $[1,2],[1,9,3],[1,28,36,4],[1,75,245,110,5],[1,186,1290,1410,300,6]...$ where each number is some $C_{kn}$? I have noticed that the first member of each set is 1, and the second term in each set is $nS_n^2$ (where the first set is n=2, the second is n=3, and so forth and $S_n^k$ are the Stirling numbers of the second kind). The nth set also contains n members (using the definition of n above), and the final member of each set is n. Other than these observations, I can't find a way to represent these numbers. My final question is ""How do we calculate a generalized Puiseux series, specifically for $^nx$ where $n>2$? I feel that if I had more knowledge on how these series are calculated I could gain insight into my other questions, but I cannot find much literature on finding generalized Puiseux series, let alone literature elementary enough for me to understand and apply to tetration. (Note: If I should split this post into three seperate posts for each question I can, but I included them all here to avoid posting excessively. If I should edit my tags to reach a wider audience I will do so, although I am not sure what specific fields of mathematics my questions fall under)","['tetration', 'sequences-and-series', 'number-theory', 'calculus']"
1438580,$ \int^{\frac{\pi }{4} }_{0} \cos ^{\frac{3}{2} }\left(2 \theta \right) \cos \left( \theta \right) d\theta $,$$ \int^{\frac{\pi }{4} }_{0} \cos ^{\frac{3}{2} }\left(2 \theta \right) \cos \left( \theta \right) d\theta $$ This integration above I tried to solve it by and get $$ \int^{\frac{\pi }{4} }_{0} \left( 1-2\sin ^{2}\left( \theta \right) \right) ^{\frac{3}{2} }d\sin \left( \theta \right)$$ and I tried to evaluate the power but I find this is useless. My question is: how I can get this integration in the closed form? Thanks.,"['calculus', 'definite-integrals', 'integration']"
1438582,"First order differential equation $y'y^2=y+xy'$, switching dependent and independent variable","I want to solve the differential equation $y'y^2=y+xy'$. I notice that the differential is not in the general form of first order linear equations which is $y'+P(x)y=Q(x)$. In fact the above equation is not linear. I read that I have to switch the dependent and independent variable of the problem and then the problem will turn into linear first order differential equation. I don't quite understand the concept of switching dependent and independent variable, can someone explain?",['ordinary-differential-equations']
1438629,Showing a function $\varphi_t$ is a homeomorphism,"Question: Let $f: \Omega = \mathbb R \times \mathbb R^n \to \mathbb R^n$ continuous with $f(t,x) = f(x)$, locally Lipschitz such that $|f| \leq M$ in $\Omega$. Show that for every $t \in \mathbb R$ $$\begin{align}\varphi_t : \mathbb R^n &\to \mathbb R^n\\x_0 &\mapsto \varphi(t, x_0)\end{align}$$
  is a homeomorphism, where $\varphi (t,x_0)$ is a unique solution defined in $\mathbb R$ for the IPV $$x' = f(x) ,\,\, x(0) = x_0 \tag{*}$$ Attempt: $\varphi_t$ is injective As $\varphi (t,x_0)$ is unique for each $x_0$, if $x_0 \neq y_0$ then $\varphi_t (x_0) = \varphi (t,x_0) \neq \varphi (t,y_0) = \varphi_t (y_0) $. $\varphi_t$ is surjective $\varphi_t$ is injective and defined from $\mathbb R^n$ to $\mathbb R^n$ $\varphi_t$ is continuous Suppose it isn't. Then there exists a sequence of points $x_n$ in $\mathbb R^n$ such that $x_n \to x_0$ and $\varphi (t,x_n)\not\to \varphi(t,x_0)$. Consider $\varphi_n(\tau) = \varphi(\tau,x_n)$, where, $\tau \in [0,t]$. Then $$|\varphi'_n(\tau)| = |\varphi'(\tau, x_n)| = |f(\tau, \varphi(\tau, x_n))| \leq M$$ Then the set $E =\{\varphi_n : [0,1] \to \mathbb R^n\}$ is equicontinous. Plus, by the Mean Value Theorem $$|\varphi_n(\tau) - \varphi_n (\sigma)| \leq \sup_{\theta \in [0,1]} |\varphi'(\theta)| |\tau - \sigma| \leq M |\tau - \sigma|$$ Thus $(\varphi_n)$ is uniformly bounded. By the ArzelÃ¡-Ascoli Theorem there exists a subsequence $(\varphi_{n_k})$ converging to $\hat\varphi_0 \neq \varphi_0$ given by $$\varphi_{n_k} = x_{n_k} + \int_0^{\tau} f(s, \varphi_{n_k}(s)) ds$$ Letting $k\to \infty$ we have $$\hat\varphi_{0} = x_{0} + \int_0^{\tau} f(s, \hat\varphi_{0}(s)) ds$$ which is solution to $(*)$ and $\hat\varphi_0 \neq \varphi_0$, yielding a contradiction. $\varphi_t^{-1}$ is continuous $\phi_{t + s} = \phi_t \circ \phi_s$, for any given $t,s \in \mathbb R$. Note: We still don't have the Continous Dependence on solutions Theorem I'm having trouble with the last two parts.","['analysis', 'real-analysis', 'ordinary-differential-equations']"
1438643,How to calculate generalized Puiseux series?,"I recently posted this , post containing a series of questions concerning the integration of ${x^{x^{x^x}}}$. In order to do so, I wrote ${x^{x^{x^x}}}$ as the following infinite summation:
$$1 + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{x^{n+1} \log^{n+1+k}(x)C_{kn}}{\Gamma(n+2)}\bigg)$$
I got to this summation using a ""generalized Puiseux series"". I mention in the post that I am unsure of how to calculate such a series, but it seemed like more of a side-note and I decided that the question deserved its own post. I am trying to determine the mysteries coefficents in the series (written as $C_{kn}$), but I am unsure how to derive them by calculating the generalized Puiseux series by hand. I have looked up literature on standard Puiseux series, and I can somewhat follow it... however, I can find nearly nothing on generalized Puiseux series, especially literature I am capable of following. While $x^x$ can be written as a standard Puiseux series, $x^{x^x}$ also requires the generalized form, so understanding the derivation of that series might work as well if it is simpler.$\\$ In short, how do we calculate a generalized Puiseux series, specifically for $x^{x^x}$ or for ${x^{x^{x^x}}}$?","['tetration', 'sequences-and-series']"
1438645,"Tangent to a line is the line itself. But, def. of tangent touches at 1 point?","Does a line have a tangent line?  It would be the line itself.  The slope of the tangent line is the slope of the line itself.  This is verified by a derivative example.   But, isn't the definition of a tangent line ""a line that touches at one point""?  Doesn't the tangent line to a line touch at infinite points? $f(x) = 2x+1$ $f'(x) = 2$ This implies a line does have a tangent, right?","['geometry', 'calculus', 'algebra-precalculus']"
1438651,First order differential equation involving inverses,"My question is to find the solutions to the following $\frac{df(x)}{dx} = f^{-1} (x)$ where $f^{-1} (x)$ refers to the inverse of the function f. The domain really isn't important, though I am interested in either (-inf, inf) or (0, inf), so if any solutions are known for more restricted domains then they are welcome. I cannot find any material relating to this type of question in any of my calculus and differential equations textbooks and references; it seems quite unorthodox. Any material which covers this type of diff equation would be wlecome","['inverse', 'ordinary-differential-equations']"
1438658,Proof that any linear system cannot have exactly 2 solutions.,"How would you go about proving that for any system of linear equations (whether all are homogenous or not) can only have either (if this is true): One solution Infinitely many solutions No solutions I found this a bit difficult to prove (even though its a very fundamental thing about any linear equation). The intuitive geometric explanation is that a line can only intersect at one point, and if it intersects at a later point, it can't be a linear equation, but I don't think this is a convincing proof. I thought of if you assume that there are two (or more, but I picked two) solutions for some linear system, then for the points in between Solution Set 1 : X 1 , X 2 ....., X n Solution Set 2 : X 1 , X 2 ....., X n Then (I think), the points between S 1 and S 2 , must be infinitely many points (and thus infinitely many solutions) such that these points can also satisfy the linear system, which would mean the system has 2 infinite solutions. However, I don't think this is rigorous enough and nor do I understand completely why its true. Can anyone help in explaining (correcting) and elaborating on the intuition and proof of this?","['systems-of-equations', 'linear-algebra']"
1438708,"Why is this rotation ""incorrect""?","I've been trying to use the following formula for the rotation of a point around the origin: $$
\begin{bmatrix}
x' \\ y'
\end{bmatrix} =
\begin{bmatrix}
\cos{\theta} & -\sin{\theta} \\
\sin{\theta} & \cos{\theta}
\end{bmatrix}
\begin{bmatrix}
x \\ y
\end{bmatrix}
$$ Now, I'm trying to apply this formula to the coordinate $(5,3)$ and rotating it $90$ degrees clockwise, and I ended up with the following result: $$
\begin{bmatrix}
x' \\ y'
\end{bmatrix} =
\begin{bmatrix}
\cos{90} & -\sin{90} \\
\sin{90} & \cos{90}
\end{bmatrix}
\begin{bmatrix}
5 \\ 3
\end{bmatrix} 
\\
=
\begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
5 \\ 3
\end{bmatrix} 
\\
=
\begin{bmatrix}
0(5) -1(3) \\
1(5) + 0(3)
\end{bmatrix} \\
=
\begin{bmatrix}
-3 \\ 5
\end{bmatrix}
$$ I ended up with the rotated coordinates $(-3,5)$. Unfortunately, this was wrong. Can anyone tell me what I'm doing wrong, and how I can do it correctly? I tried this method on other coordinate points, and all of them were wrong as well.","['geometry', 'rotations', 'trigonometry']"
1438714,"Existence of closed, non self-intersecting geodesics on compact manifolds","Let $(M,g)$ be a compact Riemannian manifold. It is well known that there always exists a nontrivial closed geodesic in $M$ , which is the so-called Lusternik-Fet theorem. But such a geodesic could well self-intersect (in a transversal way). What is known about the existence of geodesics which do not self-intersect?
(I do not know whether 'simple' is the right term for describing this) Do they always exist? Or are there counterexamples in which all closed geodesics are self-intersecting?","['differential-geometry', 'riemannian-geometry', 'reference-request']"
1438716,Sokal's short proof of the uniform boundedness principle,"The proof I'm referring to can be found here . 1 I have worked through this proof and am literally at the last line of the UBP proof but am stuck. I have two questions: Firstly, how does he obtain the inequality $\| x - x_n \| \leq \frac{1}{2}3^{-n}$ ? I can get $\| x - x_n \| \leq (6)3^{-n}$ via geometric series (unless I've done the arithmetic wrong!). I suspect this will do, based on what follows, but I'd still like to know how to get this estimate. Secondly, immediately after this, he has an inequality $\| T_n x \| \geq \frac{1}{6} 3^{-n} \| T_n \|$ . Where does this inequality come from? Seems like I'm missing something simple here again. In any case, thanks in advance! 1 Sokal, Alan D. , A really simple elementary proof of the uniform boundedness theorem , Am. Math. Mon. 118, No. 5, 450-452 (2011). ZBL1223.46022 , MR2805031 .","['solution-verification', 'banach-spaces', 'functional-analysis']"
1438726,Compute the set of vertices of a polytope,"Consider the following polytope
$$ \Xi = \{ (\vec{a}_1\cdot \vec{x}+b_1, \cdots, \vec{a}_m \cdot \vec{x}+b_m)\mid C\vec{x}\leq d\}$$
how do I compute the set of vertices of $\Xi$?","['geometry', 'linear-algebra']"
1438737,Finding an error estimation for the De MoivreâLaplace theorem,"Context for my question: For one part of my thesis I try to find an upper bound for the error in the normal approximation of the binomial distribution following the standard proof of the De MoivreâLaplace theorem with Stirling's formula . To make it concrete: Let $B_n$ be binomially distributed and let $N$ have the standardized normal distribution. I want to find an upper bound for $$\epsilon_n = \sup_{a<b} \left|\mathcal P\left(a\le \frac{B_n-np}{\sqrt{np(1-p)}} \le b\right)-\mathcal P(a \le N \le b)\right|$$ I want to compare this error with the best known error estimation of the Berry-Essee theorem for the binomial distribution . So far I have only found a proof which shows that $\epsilon_n \in O\left(\frac 1{\sqrt n}\right)$. See this proof by MÃ¡rton BalÃ¡zs and BÃ¡lint TÃ³th (which also just considered $\left|\mathcal P\left(a\le \frac{B_n-np}{\sqrt{np(1-p)}} \le b\right)-\mathcal P(a \le N \le b)\right|$ without the supremum). Other proofs do not investigate the error at all (see for example this proof on Wikipedia ). My Question: Do you know any proof in a textbook / paper / article where the theorem by De Moivre and Laplace is proved with Stirling's formula and the total error is estimated? The value of any occurring constants in the error estimate shall also be calculated. Can you point me to this proof? Update: I reasked the question on MO, see https://mathoverflow.net/questions/219253/finding-an-error-estimation-for-the-de-moivre-laplace-theorem-with-stirlings-fo","['probability-theory', 'estimation', 'normal-distribution', 'statistics', 'reference-request']"
1438762,"PDE $(\partial_{tt}+a\partial_t-b\nabla^2)f(r,t)=0$","I am interested in solving the linear PDE for $f(r,t)$
$$
(\partial_{tt}+a\partial_t-b\nabla^2)f(r,t)=0
$$
$$
\nabla^2\equiv \frac{1}{r}\partial_r(r\partial_r)-\frac{1}{r^2}=\partial_{rr}+\frac{1}{r}\partial_r-\frac{1}{r^2}
$$
with  conditions
$$
\frac{\partial f(0,t)}{\partial t}=0,\quad \frac{\partial f(R,t)}{\partial t}=d\cos (\omega t)
$$
where $a,b,d,R>0$. You can see the laplacian like term is written in a cylindrical basis, so I assume Bessel function will arise. I would like a complete solution which includes the steps to find $f(r,t)$ and details in evaluating the expansion coefficients of the fourier-bessel series that may arise in $f(r,t)$. Thank you!  enjoy the bounty!","['calculus', 'special-functions', 'fourier-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
1438794,Gluing Together Borel Measures,"Does anyone know a standard reference for the following, which I assume is true: X a topological space, $\{U_i\}$ an open cover, $\mu_i$ a collection of regular Borel measures agreeing on overlaps. Then there's a regular Borel measure on X agreeing with the $\mu_i$.  If you want, assume X is a manifold.","['analysis', 'general-topology', 'measure-theory']"
1438838,Seeking for a hint to a limit question $\lim_{n \to \infty}(a_{n+1}^{\alpha}-a_n^{\alpha})=0$,"Assume ${a_n}$ is a positive, strictly increasing sequence, while $a_{n+1}-a_n$ is bounded. Prove: for every real number $\alpha \in (0,1)$, $$\lim_{n \to \infty}(a_{n+1}^{\alpha}-a_n^{\alpha})=0.$$ I'm wondering how can I associate $a_{n+1}-a_n$ with $a_{n+1}^{\alpha}-a_n^{\alpha}$.","['sequences-and-series', 'limits', 'real-analysis']"
1438840,How to show holomorphic de Rham complex is exact?,"Supppse $X$ is a smooth algebraic manifold, how does one show the holomorphic de Rham complex is exact in characteristic $0$? I knew a method is to resolve it by double complex $A^{p,q}$, then the column exactness shows it is quasi isomorphic to differential de Rham complex, thus exact. But this method is transcendental, is there a algebraic way of showing this?",['algebraic-geometry']
