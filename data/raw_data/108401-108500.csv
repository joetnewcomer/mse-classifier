question_id,title,body,tags
1555968,Laplace Transform of equation,I'm having trouble with the laplace transform: $\mathcal{L} \lbrace \sqrt{\frac{t}{\pi}}\cos(2 t) \rbrace$ The problem gives me the transform identity $\mathcal{L} \lbrace \frac{\cos(2 t)}{\sqrt{\pi t}} \rbrace = \frac{e^{-2/s}}{\sqrt{s}}$ but i'm not sure/confused as to why that would help me,"['ordinary-differential-equations', 'laplace-transform']"
1555971,Singular Perturbation Approx. for $\epsilon y'' + \frac{2 \epsilon}{t} y'-y=0$,"Use singular perturbation techniques to find the leading order uniform approximation to the solution to the boundary value problem $$\epsilon y'' + \frac{2 \epsilon}{t} y'-y=0$$ $0<t<1$ and $y(0)=0  ,  y'(1)=1$ This has a boundary layer near $t=1$ I am having trouble figuring out how to compute the inner approximation. I am used to doing these problems with $t=0$ as the boundary layer. Using these methods, I get the outer approximation to be $y_0=0$, which I know is correct, but I'm at a loss on how to proceed with the inner approximation. I would appreciate if someone could work out the inner layer so that I could see how it is done.","['boundary-value-problem', 'ordinary-differential-equations', 'perturbation-theory']"
1555986,Finding the coefficient of x^n generating functions,Find the cofficient of $x^n$ in the expansion of $b^m x^m \over (1âˆ’bx)^{m+1}$ Here b is a real number. Note your answer may depend on conditions involving m and n. I started off by isolating $1\over(1-bx)^{m+1}$ to make it fit the generating function $$\sum_{n=0}^\infty b^n {n+m \choose n} x^n $$ But then after adding back in the $b^m$ and the $x^m$ I get stuck here $$\sum_{n=0}^\infty b^n {n+m \choose n} x^n b^m x^m $$ I know from here it should be a matter of simple algebra but I seem to be stuck.,"['generating-functions', 'discrete-mathematics']"
1555995,Derivatives question on partial derivatives,"If $z=f(x,y)$ and $x=e^u \cos v$, $y=e^u \sin v$ then show that 
$y \frac{dz}{du} + x \frac{dz}{dv} = e^{2u} \frac{dz}{dy}$","['derivatives', 'exponential-function', 'partial-derivative', 'trigonometry']"
1555999,An exponential family problem,"I don't know how to express the problem to the form
$f(x|\theta)=h(x)c(\theta)(\sum_{i=1}^{k}\omega_i(\theta)t_i(x))$ Let $X$ have pdf $f(x)=\frac{1}{\beta}e^{-(x-\alpha)/\beta}$, $x>\alpha$ Determine whether $f(x)$ is an exponential a. if both $\alpha$ and $\beta$ are unknown. b. if only $\beta$ is unknown I thought those questions are same but it is not.","['exponential-distribution', 'statistics']"
1556008,How can I differentiate the following using Matrix Calculus?,"The mathematical expression is like this: $f(\mu_{q(\beta)}, \Sigma_{q(\beta)}) = \mathbf{1}_{n}^{T} \exp \left \{ X \mu_{q(\beta)} + \frac{1}{2} \operatorname{diagonal}(X \Sigma_{q(\beta)} X^{T}) \right\}$ where $\exp$ indicates element-wise exponentiation and $\operatorname{diagonal}$ is a vector with the diagonal entries of the matrix as its components. Additionally, $\mathbf{1}_{n}^{T}$ is a vector in $\mathbb{R}^{n}$ with only ones as its entries and for notational ease, I used lower-case for vectors whereas matrices are in upper-case. $\Sigma_{q(\beta)}$ is symmetric whereas $X$ is not. And I want to differentiate this by $\mu_{q(\beta)}$ and $\Sigma_{q(\beta)}$ each. So $\frac{\partial f}{\partial \mu_{q(\beta)}}$ and $\frac{\partial f}{\partial \Sigma_{q(\beta)}}$ are what I need.","['derivatives', 'matrix-calculus', 'linear-algebra', 'calculus']"
1556018,Simple proof that a Lebesgue-measurable and additive function is linear,"Let $(\Bbb R, \mathcal A_{\Bbb R}^*,\overline{\lambda})$ be the complete lebesgue-measure space. Let $f:\Bbb R\to \Bbb R$ be an additive function and also Lebesgue-measurable: $$\text{I want to prove that:}\;\;f(x)=f(1)\ x\;\;\forall x\in\Bbb R$$ Proof: So first I prove some clearly results. Lemma 1 : $\;f(x-y)=f(x)-f(y)\;\forall x,y\in\Bbb R$ Let $x,y\in\Bbb R$ so, since $f$ is additive, we get that:
$$f(0)=f(0+0)=f(0)+f(0)\Rightarrow\ f(0)=0\\
\Rightarrow\ 0=f(0)=f(x+(-x))=f(x)+f(-x)\Leftrightarrow\ -f(x)=f(-x)$$ thereby $\;f(x-y)=f(x)+f(-y)=f(x)-f(y).$ Lemma 2 : $\;r\ f(x)=f(rx)\;\;\forall r\in\Bbb Q\;\forall x\in\Bbb R$ Let $x\in\Bbb R$ we state that $\;n\ f(x)=f(nx)\;\forall n\in\Bbb N$. Because (by induction over n) for $n=2$ is clear that $f(2x)=f(x+x)=f(x)+f(x)=2f(x)$, if we suppose that $f(nx)=nf(x)$ follows for $n$ we get that $f((n+1)x)=f(nx+x)=f(nx)+f(x)=nf(x)+f(x)=(n+1)f(x)$. Then let $\;m=-n\;\;\forall n\in\Bbb N$ thus $\;f(mx)=f(-nx)=-nf(x)=mf(x)$ and hence $f(zn)=zf(x)\;\;\forall z\in\Bbb Z\Rightarrow\ r=\frac{n}{m}$ with $m,n\in\Bbb Z$, $(n,m)=1$ and $m\ne 0$ By last, let $r\in\Bbb Q\setminus\Bbb Z$. So, since $f(mx)=mf(x),$ by taking $x=\frac{y}{m}$ and $q=\frac{1}{m}$ we get that $f(y)=\frac{1}{q}f\big(\frac{y}{m}\big)\Leftrightarrow\ qf(y)=f\big(\frac{y}{m}\big)\Leftrightarrow\ \frac{1}{m}f(y)=f\big(\frac{y}{m}\big)$ and thus $f(rx)=f\big(\frac{n}{m} x\big)=nf\big(\frac{1}{m}x\big)=\frac{n}{m}f(x)=rf(x)$. Now I want to prove that $f$ is continuous: So, it's clear that: 
$$\bigcup_{m>0}\{x\in\Bbb R:|f(x)|\le m\}=\Bbb R\ne\emptyset$$
Let $A_m=\{x\in\Bbb R:|f(x)|\le m\}$, then for each fixed $t\in\Bbb R\;\exists\ m_t>0\;$ s.t. $\;t\in A_{m_t}\;$ so $\;\overline{\lambda}(A_{m_t})>0$.(where $A_m\in\mathcal A_{\Bbb R}^*$ follows since $f^{-1}([-m,m])=A_m$ and $f$ is measurable) So, by Steinhaus Theorem , there $\exists\ \delta>0\;$ s.t. if $\;|x|<\delta$ we get that: $$\overline{\lambda}\big(A_{m_t}\cap (A_{m_t}+x)\big)>0\\
\text{and that}\;\; (-\delta,\delta)\subset \{a-a':a,a'\in A_{m_t}\}=:\ A_{m_t}-A_{m_t}$$ Thereby $\forall x\in (-\delta,\delta)\;\;\exists\ a,a'\in A_{m_t}\;$ s.t. $\; x=a-a'\Rightarrow\ f(x)=f(a-a')=f(a)-f(a')\Rightarrow\ |f(x)|=|f(a)-f(a')|\le m_t+m_t=2m_t$. Namely $\forall x\in (-\delta,\delta)\Rightarrow\ |f(x)|\le 2m_t$ so taking $\delta^*=\frac{\delta}{2^{n+1}}$ for $\; n\in\Bbb N\;\;$ we get that: $$\text{if}\;\; |x-y|<\delta^*\Rightarrow\  2^{n+1}|x-y|<\delta\Rightarrow\ |2^{n+1}(x-y)|<\delta\\
\Rightarrow\Big|f\big(2^{n+1}(x-y)\big)\Big|\le 2m_t$$ thus (by lemma 1 and 2) $$|f(x)-f(y)|=|f(x-y)|=\frac{1}{2^{n+1}}\Big|f\big(2^{n+1}(x-y)\big)\Big|\le \frac{2m_t}{2^{n+1}}=\frac{m_t}{2^n}\\
\Rightarrow\ |f(x)-f(y)|\le \frac{m_t}{2^n}\to 0,\;\;\ n\to\infty $$ therefore $f$ is continuous. So at last, let $\epsilon>0$: If $f(1)=0$, $f(x)=xf(1)$ follows straightforward. If $f(1)\ne 0$, we take $\delta_{\epsilon}=\frac{\epsilon}{2|f(1)|}$ so if $|x|<\delta_{\epsilon}$ then
$$|f(x)-xf(1)|\le |f(x)|+|x||f(1)|$$ where $|f(x)|<\frac{\epsilon}{2}$ by the particular continuity of $f$ at $0$, so $$|f(x)-xf(1)|\le |f(x)|+|x||f(1)|<\frac{\epsilon}{2}+\delta_{\epsilon}|f(1)|=\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon$$ hence $f(x)=xf(1)\;\;\forall x\in\Bbb R.$","['lebesgue-measure', 'measure-theory']"
1556102,Determining poles and order of $1/\sin(z)$,How can I find the poles and the order of $\dfrac{1}{\sin(z)}$?,['complex-analysis']
1556187,Is the Cartesian product of sets associative? [duplicate],"This question already has an answer here : Associativity of Cartesian Product (1 answer) Closed 8 years ago . Is it associative? In general sense, if we compare $A \times (B \times C)$ and $(A \times B) \times C$, then $(a,(b,c))$ does not equal $((a,b),c)$, but both of those pairs may be interpreted as one triplet $(a,b,c)$. Is it correct to do that and can we use associativity of Cartesian product in set theory problems?",['elementary-set-theory']
1556202,Use MVT to compare the value of two numbers,"Use MVT to Prove that $.99^5>= .95$ I realize I should find a function before using MVT. However, the only function I can think about is $f(x)=x^5$, which doesn't work in this case. Any idea how about how to find a proper function?",['derivatives']
1556217,How to find $\int_0^{2\pi}\log(\alpha+\beta\cos(x))\mathrm{d}x$,"Is there a closed-form formula for the following integral $$
\int_0^{2\pi}\log(\alpha+\beta\cos(x))\mathrm{d}x
$$ where $\alpha$ and $\beta$ are constants which assure that $\alpha+\beta\cos(x)>0$ for any $x\in[0,2\pi]$.","['integration', 'definite-integrals']"
1556226,"How to show $\frac{\mathbb{Z}_m\times \mathbb{Z}_n}{\langle (a,b)\rangle}\simeq \mathbb{Z}_{\frac mc}\times \mathbb{Z}_{\frac nd}$?","Suppose we have the group $\mathbb{Z}_m\times\mathbb{Z}_n$ , and $(a,b)\in\mathbb{Z}_m\times\mathbb{Z}_n$ . We need to justify that (i) There exist $c, d$ such that $\langle (a,b)\rangle$ is isomorphic to the group $\mathbb{Z}_c\times \mathbb{Z}_d$ with $c\mid m, d\mid n$ . (ii) $\dfrac{\mathbb{Z}_m\times \mathbb{Z}_n}{\langle (a,b)\rangle}\simeq \mathbb{Z}_{\frac mc}\times \mathbb{Z}_{\frac nd}$ . How to show these ? The first one I tried as: Since $\langle (a,b)\rangle$ is cyclic there is $\alpha$ such that $\langle (a,b)\rangle\simeq \mathbb{Z}_\alpha$ . And then $\alpha\mid mn$ which means we can find two relatively prime $c,d$ such that $cd=\alpha, c\mid m, d\mid n$ and $\mathbb{Z}_\alpha\simeq \mathbb{Z}_c\times \mathbb{Z}_d$ . Then ?","['finite-groups', 'abstract-algebra', 'cyclic-groups', 'direct-product', 'group-theory']"
1556232,How can I justify that the partial sums of $\frac{(2n)^k}{k!}$ is less than the number $\frac{(2n)^n}{n!}$?,"I am currently using Rouche's Theorem from complex analysis but am working on an upper bound and want to show $$\sum_{k=0}^{n-1}\frac{(2n)^k}{k!}< \frac{(2n)^n}{n!}$$ Any suggestions are welcome. Thanks,","['inequality', 'sequences-and-series', 'calculus', 'complex-analysis', 'power-series']"
1556239,What is the mathematical notation for Convex Hull?,"I've been scanning through scientific papers, this site and just googling for it, but I can't find a commonly accepted notation for the convex hull. So my question is; if there is, what is the standard notation for convex hull? Seems to me people just use their favorite out of a large collection of notations (or invent their own notation) to denote the convex hull of, say $S$, including
\begin{align}
\mathrm{conv}(S),\ \mathrm{CH}(S),\ \mathrm{conv.hull}(S),\ \mathrm{Co}(S),\ \mathrm{C}(S),
\end{align} which I find frustrating. :)","['notation', 'linear-algebra', 'convex-analysis']"
1556307,What are the applications of functional analysis?,"I recently had a course on functional analysis. I was thinking of studying the mathematical applications of functional analysis. I came to know it had some applications on calculus of variations. I am not specifically interested in applications of functional analysis on pure branches of mathematics but rather interested in applied mathematics. Can anyone give a brief on what are the mathematical applications of functional analysis? Also, please suggest some good books for it.","['functional-analysis', 'reference-request', 'book-recommendation', 'applications']"
1556352,Folding a rectangular paper such that one corner moves along an opposite side,"A rectangular paper is folded such that one corner moves along the opposite side. Prove that all the creases formed are tangent to a parabola. Attempt: Let the paper be oriented such that its in the first quadrant and has one corner as the origin and sides along the axis. 
After folding like so: Let one side of the paper be $a$. The equation of the crease is $$y=(x-h)\tan\theta$$
Also, in $\Delta LOH$, 
$$\cos(\pi-2\theta)=-\cos(2\theta)=\frac{h}{a-h}$$
$$-\frac{1-\tan^2\theta}{1+\tan^2\theta}=\frac{h}{a-h}$$
Substituting the value of $\tan\theta$ from equation of crease, 
$$ah^2+2(y^2-ax)h+a(x^2-y^2)=0$$ I am not getting anywhere close to proving the statement given.",['geometry']
1556364,"Minimising Function, derivative with exponentials","First of all, I apologise for not giving a more descriptive title. I really do not know how to word it. I'll go straight into the meat of the question. If a function $$h(x)=\frac{e^x-1}{x^5}$$ is to be minimised, then you go about finding the first derivative and solving that for zero to find the critical points. I've done that and get the following
$$h'(x)=\frac{xe^x-5e^x+5}{x^6}$$
and to find the critical points we then get $xe^x-5e^x+5=0$. I am either being extremely stupid or there is no way which you can use to isolate $x$ and so can't solve via a ""straightforward"" method. It should follow into a transcendental equation in the form $x=g(x)$. My research suggests there is a solution in terms of Lambert W functions, however this has not yet be taught in my university course, and checking with the lecturer we do not need them, i.e. ""straightforward"" method... What exactly am I missing here? Thanks in advance.","['derivatives', 'exponential-function', 'optimization']"
1556374,Proving that $f'$ is measurable on $\mathbb R$ if$f$ is differentiable on $\mathbb R$,"Since $f$ is differentiable on $\mathbb R$ it is then continuous on $\mathbb R$,making $f$ measurable. (this we proved in class-that continuous functions are measurable).
I tried to use this to prove the measurability of $f'(x)$. $$f'(x_0)=\lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}$$ What is inside the limit is measurable because it is an algebraic expression on measurable functions. What is left to prove is proving that the limit of this measurable function is measurable. Is it possible to go about this in this manner?","['real-analysis', 'calculus', 'functional-analysis', 'continuity', 'measure-theory']"
1556406,Geometric interpretation of a quintic's roots as a pentagon?,"According to this subsection, "" Geometric interpretation of a cubic's roots "", given, $$F(x)=x^3+ax^2+bx+c=0$$ with three real roots, ""...then the roots are the projection on the $x$-axis of the vertices $A, B, C$ of an equilateral triangle . The center of the triangle has the same abscissa as the inflection point."" $\hskip2.7in$ $$\text{Fig.1}$$ Questions: In general, given $F(x) = 0$ of degree $n>2$ with $n$ real roots, are the roots the projection on the $x$-axis of the vertices of a regular n-gon ? If so (and at least for deg $n=4,5$), what features of the $n$-gon (like $\theta$, center's abscissa, etc) can be given a closed-form in terms of the roots?","['roots', 'real-numbers', 'trigonometry', 'geometry']"
1556434,"Are there cube-free numbers $n$, for which the number of groups of order $n$ is unknown?","For squarefree $n$, there is a formula allowing to compute the number of groups of order $n$. I do not think that such a formula exists for cubefree numbers. If a cubefree number $n$ has the property, that there is no prime power $p^k|n$, such that $p^k\equiv 1\ (\ mod\ q\ )$ for some prime $q|n$, then every group of order $n$ is abelian and $n$ is called an abelian number. In OEIS, I found a slightly different definition of abelian numbers. Is the criterion I mentioned correct ? The number of abelian groups of order $n$ can be easily calculated (assuming the prime factorization of $n$ is known). But what is the situation for general cubefree numbers $n$ ? Is the cubefree case easy enough that the number of groups can be efficiently calculated, or are there cubefree numbers $n$ (of course with known factorization), for which the number of groups of order $n$ is unknown ?","['finite-groups', 'group-theory', 'groups-enumeration']"
1556438,Lie groups for beginners: Lie group of hyperbolic geometry,"I am trying to understand Lie groups and their relation to (2 dimensional) hyperbolic geometry. as far as I understand it  (which is not very far, I am pushing my understanding here) the  Lie-group is the set of all isometric transformations  in a geometry. So in hyperbolic geometry this is the set of all reflections , translations, rotations, horolation and maybe other (hyperbolic) length preserving transformations. But then What does it mean that ""the transformation group of hyperbolic geometry is the Orthochronous Lorentz group $O ( 1 , n ) / O ( 1 ) $ ?"" (found for example at https://en.wikipedia.org/wiki/Klein_geometry#Examples ) As far as I can follow it  (and I am pushing my understanding here) it should depend on which mode of hyperbolic geometry you use. In the Poincare disk model the transformation group is the set of 1) all circle inversions in circles orthogonal to the boundary circle and 2) their combinations. (the first one being reflections in hyperbolic lines, the second one multiple reflections. In the Poincare half plane model they are another transformation group. the set of 1) all circle inversions in circles centered on the boundary circle 2) reflections in lines orthogonal to the boundary line and 3) their combinations. (the first two being reflections in hyperbolic lines, the third one multiple reflections.) But then I got stumped what does this has to do with the lorentz group? or any other named group $ SO(2)$ ,  $ SO(2)$or $SO^+(2)$ or $ O ( 1 , n ) / ( O ( 1 ) Ã— O ( n ) ) $ (from https://en.wikipedia.org/wiki/Hyperbolic_geometry#Homogeneous_structure , I guess n= 2 here but I don't even understand the formula)? I could do with a basic ""Introduction to Lie groups for hyperbolic critters"" book, recommendations welcome.","['book-recommendation', 'hyperbolic-geometry', 'group-theory', 'symmetric-groups', 'lie-groups']"
1556443,how to solve fokker-planck equation using space-time laplace transform?,"I was wondering about how to solve a simple linear Fokker-Planck equation using space-time Laplace transform on space interval $[0,+ \ \infty)$ , $$\frac{\partial f(x,t)}{\partial t}= k_1 \frac{\partial f(x,t)}{\partial x} + k_2 \frac{\partial^2 f(x,t)}{\partial x^2}.$$ The usual method is to do Laplace transform in time and then solve the spatial differential equation by transforming it into a Sturm-Liouville problem. But I feel that for a special case of semi-infinite, i.e., $[0, \ \infty)$ , one can solve it easily if used Laplace transform for space-time instead of only time. EDIT: The OP didn't specify boundary conditions, making the problem ill-defined. Among the many possible boundary conditions confining the process to the interval $[0,+\infty)$ , one of the most used ones are the reflecting boundary conditions, $$\left[k_1 f(x,t)+k_2\frac{\partial}{\partial x}f(x,t)\right]_{x=0}=0,\quad \forall t.$$","['stochastic-processes', 'ordinary-differential-equations', 'partial-differential-equations']"
1556453,Prove that if $\lim_{n\to \infty}{\frac{a_{n+1}}{a_n}}=x$ then $\lim_{n\to \infty}{\sqrt[n]{a_n}}=x$,"Prove that if $\lim_{n\to \infty}{\frac{a_{n+1}}{a_n}}=x$ then $\lim_{n\to \infty}{\sqrt[n]{a_n}}=x$ My proposed solution uses the following prepositions: Proposition 4.7. Let $a_n$ be a sequence of real numbers such that ${\sqrt[n]{a_n}}$ converges to L. If L < 1
the sequence converges to zero, if L > 1 the sequence is divergent, if L = 1 the test is inconclusive.
Proposition 4.8. Let $a_n$ be a sequence of real numbers such that $\frac{a_{n+1}}{a_n}$ converges to L. If L < 1 the sequence converges to zero, if L > 1 the sequence is divergent, if L = 1 the test is inconclusive. These tests are perfectly equivalent and so their limits must be the same. That is my solution but we were given the hint that we could use the result $\lim_{n\to \infty}{a_n^s}=x^s$ where s is rational and I have not used this hint which makes me think my solution is wrong. Also my solution seems too simple.",['limits']
1556556,"Sequence of polynomials $p_n$ converging to a non-polynomial. Show $\text{deg}\, p_n\to\infty$","If $f\in C[a,b]$ is not a polynomial, then show that for any sequence of polynomials $p_n$ that converges to $f$ uniformly, one must have that $\text{degree of } p_n \to\infty$.","['continuity', 'real-analysis', 'sequences-and-series']"
1556560,What does the case of $\operatorname{Spec}C^{\infty}(M)$ tell us about the relavance of scheme theory to general rings?,"I guess a lot of people with previous exposure to differential geometry have had this naive question pop out in their mind when studying schemes for the first time. The category of compact smooth real manifolds is contra-equivalent to the category of smooth rings on them $C^{\infty} (-)$. The inverse functor to the global section functor takes maximal ideals and makes sheafs out of the smooth rings. By a partion of unity argument the ring of global sections determines the sheaf so wer'e good. (At least, I hope we are. It's not something I found written explicitly in any book). On the other hand, the famous result for schemes gives the equivalence $\mathsf {Aff} \cong (\mathsf{Ring})^{op}$. This suggests that the correct and uniform way to think about rings geometrically is studying the functor $Spec$. There are many reasons for why studying $\operatorname{Spec}C^{\infty}(M)$ Is not so fruitfull. Elaboration on this in answers would be welcome as well, though my question is a more philosophical one. Question: Given what we know about the unsuitability of scheme theory for the study of smooth rings why should we believe that it's the right generalization for algebraic geometry over arbitrary rings? Personally, (and I hope It's okay to express my opinion despite my ignonrance) I think this is a good argument for studying general locally ringed spaces (Of which schemes are a part of but not at the center necessarily)... What do you think?","['algebraic-geometry', 'schemes', 'ring-theory', 'soft-question', 'differential-geometry']"
1556575,Idea behind the definition of different ideal,"Let $L/K$ be an extension of number fields. Let $I$ be a fractional ideal in $L$ and $$I^*:=\{x\in L \mid \text{Tr}_{L/K}(xI)\subset \mathcal{O}_K\}.$$
The different of $I$ is the following fractional ideal
$$\mathcal{D}_{L/K}(I):=(I^*)^{-1}.$$
I understand the importance of the different ideal in the study of ramification. For example, we know that if $P$ is a prime in $\mathcal{O}_K$ and $P\mathcal{O}_L=Q^eI$, with $(Q, I)=1$, then $Q^{e-1}\mid \mathcal{D}_{L/K}(\mathcal{O}_L)$. 
But I don't understand what is the idea behind its definition. What (historically) led to that definition?","['abstract-algebra', 'algebraic-number-theory', 'ramification', 'integer-rings', 'ideals']"
1556588,How do I find maximum degree of a vertex in $G$?,"Common Data for Questions $1, 2, 3:$ The $2^n$ vertices of a graph $G$ corresponds to all subsets of a set of size $n$ , for $nâ‰¥6$ .  Two vertices of $G$ are adjacent if and only if the corresponding sets intersect in exactly two elements. Q.1. The number of vertices of degree zero in $G$ is: $1$ $n$ $n + 1$ $2^n$ Q.2.The maximum degree of a vertex in $G$ is: $^{n/2}C_2.2^{n/2}$ $2^{nâˆ’2}$ $2^{nâˆ’3}Ã—3$ $2^{nâˆ’1}$ Q.3. The number of connected components in $G$ is: $n$ $n+2$ $2^{n/2}$ $\frac{2^n}{n}$ My Try: Given there set size is $n$ where $n\geq6$ . Number of vertices is $2^n$ of graph $G$ . Note that power of $n$ elements, i.e. $2^n$ elements in powerset of $n$ elements where $1$ -element of size is $0$ i.e $\phi$ . $n$ -element of size is $1$ i.e $\{1\} ,\{2\} ,\{3\},....,\{n\}$ . Similarly size of $k$ , total number of elements are $^nC_k$ . Now, Two vertices of $G$ are adjacent if and only if the corresponding sets intersect in exactly two elements. Note that $1$ -element of size is $0$ i.e $\phi$ and $n$ -element of size is $1$ i.e $\{1\} ,\{2\} ,\{3\},....,\{n\}$ have less than two elements that can not be connected to any other vertices due to less number of elements in that sets, Total such element are $ = 1 + n$ and remaining (i.e. $2^n - (n+1)$ ) are connected, since these have more than one elements in that sets. So, total number of connected components in $G$ is $ = (n+1)$ disconnected $+1$ ((i.e. $2^n - (n+1)$ )remaining  are connected) $ = n + 2$ . I'm not getting Q.2. Can you explain little bit please, how do I find maximum degree of a vertex in $G$ ? Somewhere, it explained and answer is given $(^kC_2.2^{(n-k)}) =  ^3C_2 . 2^{(n-3)} = 3.2^{(n-3)}$ .","['combinatorics', 'graph-theory', 'discrete-mathematics']"
1556607,What are the tightest known bounds for the number of groups of order $2048$?,"The number of groups of order $2048$ is unknown. What are the tightest known bounds (lower and upper bounds : I am interested in both) for the number of groups of order $2048$ ? I know the asymptotic formula for $p^k$, but I do not think that it gives a useful bound for $p^k=2048$. Somewhere, I read that a subset of the groups (but I do not remember what kind of subset) was calculated to obtain a lower bound. Can it be estimated how long it will take to determine the number ?","['finite-groups', 'reference-request', 'group-theory']"
1556609,Computing a line integral along a circle in $3$-D,"Let $C$ be the curve of intersection of the two surfaces $x+y=2 , x^2+y^2+z^2=2(x+y)$ . The curve is to be traversed in clockwise direction as viewed from the origin . The what is the value of $\int_Cydx+zdy+xdz$ ? I am not even able to parametrize the curve of intersection . Please help . Thanks in advance","['multivariable-calculus', 'line-integrals']"
1556611,Universe set and nullary intersection,"Let 
$$\mathbf{B}=\{B_i : i \in I_1\} \subseteq 2^\Omega$$
and suppose that 
$$\cup(\mathbf{B}) \neq \Omega$$
If $I_2 \subseteq I_1$ then
$$\cap\{B_i : i \in I_1\} \subseteq \cap\{B_i : i \in I_2\}$$
therefore the operator
$$\cap : 2^{2^\Omega} \mapsto 2^\Omega$$
is, loosely  speaking, decreasing. Perhaps, this could be a naif reasoning in favor of
$$\cap \varnothing = \Omega$$
but it raises a paradox: given that each $B_i$ ""doesn't remember"" what set it has been cutted out from, we can also conceivably conjecture
$$\cap \varnothing = \cup(\mathbf{B})$$
and, by hypothesis 
$$\cap \varnothing = \cup(\mathbf{B}) \neq \Omega = \cap \varnothing$$
Obiviously, something is dead wrong.
I bumped into this pitfall because I'm studying general topology, and I suspect 
that it can lead to major misunderstandings.","['general-topology', 'elementary-set-theory']"
1556613,Evaluate $\int e^{2\theta} \sin (3\theta)\ d\theta$ [duplicate],This question already has answers here : Evaluate the Integral: $\int e^{2\theta}\ \sin 3\theta\ d\theta$ (3 answers) Closed 8 years ago . Evaluate $$\int e^{2\theta} \sin (3\theta)\ d\theta .$$ I am little stuck as to what I can do after this point. Please tell me if my method overall is flawed:,"['indefinite-integrals', 'integration', 'calculus']"
1556614,An inequality of integrals,"Let $f \in L^{2}(\mathbb{R})$ be continuously differentiable on $\mathbb{R}$. I am trying to show the following:
$( \int |f|^{2} dx)^{2} \leq 4 ( \int |xf(x)|^{2} dx) ( \int |f'|^{2} dx))$. My first thought is to think about this inequality as $ ||f||^{4}_{2} \leq 4 ||xf(x)||^{2}_{2} ||f'||^{2}_{2} $ and apply Holder's inequality to get $ 4 (\int |xf(x)f'(x)| dx)^{2} \leq 4 ||xf(x)||^{2}_{2} ||f'||^{2}_{2}$ but beyond this I have no intuition, especially what to do with the continuously differentiable assumption. Could someone lend me a hint?","['inequality', 'integral-inequality', 'measure-theory']"
1556670,measure of irrational number,"I've once read a proof about this and I'm trying to remember how it went. We want to show that if we randomly select a number $x$ from the set $[0,1],$ then $P[  \text {x is irrational} ] = 1$","['real-numbers', 'probability', 'measure-theory']"
1556749,Classical and intuitionistic propositional logic in the propositions-as-sets interpretation,"I'm looking for a way to describe classical and intuitionistic propositional logic such that the transition between the two seems natural and intuitive.  I came up with the following but I'm unsure if it's actual true. Just like the propositions-as-types interpretation for intuitionistic logic, one can give a propositions-as-sets interpretation for classical logic.  If the logical language consists of $\land,\lor,\Rightarrow,\bot$, we can interpret these symbols as the set operations $\times,\oplus,\rightarrow,\emptyset$ and a propositional formula $\phi[X_1,\dots,X_n]$ is valid iff we can prove (in set theory enriched by $\times,\oplus,\rightarrow,\emptyset$) that $\exists Y.Y\in\tilde\phi[X_1,\dots,X_n]$ for the corresponding set theoretical term $\tilde\phi$.  (Since the $X_i$ appear free in $\tilde\phi$, we can also prove $\forall X_1,\dots,X_n.\exists Y.Y\in\tilde\phi[X_1,\dots,X_n]$ then.) I'm pretty sure that this part is correct, since basically not much is done, one only asks about emptyness of sets and we know that $X\times Y$ is empty iff $X$ or $Y$ is, $X\oplus Y$ is empty iff $X$ and $Y$ is, $X\rightarrow Y$ is empty iff $X$ but not $Y$ is such as $\emptyset$ is empty. Now I feel like one could say that the propositions which are intuitionistically true are exactly those where we find canonical inhabitants of the respective set.  For example, we can show $\exists Y.Y\in X\oplus(X\rightarrow\emptyset)$ but there is no canonical choice for such $Y$.  I thought maybe one could make precise what is meant by 'canonical inhabitant of $\phi$' by saying there exists a (set theoretical) formula $\chi[X_1,\dots,X_n,Y]$ such that we can prove: $$(\exists! Y.\chi[X_1,\dots,X_n,Y])\land(\forall Y.\chi[X_1,\dots,X_n,Y]\Rightarrow Y\in\tilde\phi[X_1,\dots,X_n])$$ (Since the $X_i$ appear free in the formula, as before the $Y$ can depend on them.) Is there some theory along these lines, if it makes any sense at all? EDIT: This conjecture is not true, take $\phi[X]:=\bot\Rightarrow X$.  Then clearly $\tilde\phi[X]=\emptyset\rightarrow X$ equals $\{\emptyset\}$ (no matter what $X$ is because $\emptyset$ is an initial object in the category of sets) so it contains a very canonical inhabitant (i.e. this inhabitant doesn't even depend on $X$). But maybe it remains true if we do not translate $\bot$ to $\emptyset$ but just some unspecified fresh variable $X_\bot$ and place the condition $X_\bot\subseteq X_i$ over everything?","['intuitionistic-logic', 'reference-request', 'propositional-calculus', 'logic', 'elementary-set-theory']"
1556766,The number of integers $n$ such that the quadratic equation $nx^2+(n+1)x+(n+2)=0$ has rational roots is,"The number of integers $n$ such that the quadratic equation $nx^2+(n+1)x+(n+2)=0$ has rational roots is $(A)0\hspace{1cm}(B)1\hspace{1cm}(C)2\hspace{1cm}(D)3$ The condition for the rational roots is that the discriminant should be greater than or equal to zero and discriminant should be a perfect square. Discriminant$=(n+1)^2-4n(n+2)=n^2+2n+1-4n^2-8n=-3n^2-6n+1$ $-3n^2-6n+1\geq 0.......................(1)$ I dont know how to apply the perfect square condition here.With the equation $(1)$,i get only one integer $n$.But the correct answer given in my book is $2$.Two integer $n$ are possible.$n=-1,n=-2$. Please help me.Thanks","['algebra-precalculus', 'polynomials', 'quadratics']"
1556796,Is $(3^p-1)/2$ always squarefree?,I have little conjecture. Maybe it's stupid i don't know. Let $p>5$ be a prime number. Then $(3^p-1)/2$ is always squarefree? It's true for $p<192$.(I used Mathematica.),"['number-theory', 'prime-factorization', 'prime-numbers']"
1556805,Branch cut for $\sqrt{1-z^{2}}$ and Taylor's expansion!,"I'm working in a problem that involves the equation 
$$
w(z)=\sqrt{1-z^{2}} \,\, .
$$ I already know that there're two branch points in this equation, namely $\pm 1$, so there's a Riemann surface covering the domain of the function where the branch cut is from the $-1$ to $1$, as shown in the figure below. My purpose is make an expansion through Maclaurin serie around the $0$ point, but I don't know if it's a suitable point to expand that function, I mean, If there's some kind of problems or inconsistency in expanding around that point. if that's not a suitable point, how could I deform the branch cut line to make this a suitable point for expansion? Greetings!","['taylor-expansion', 'complex-analysis', 'power-series', 'sequences-and-series', 'branch-cuts']"
1556811,Does there exist an $n$ such that all groups of order $n$ are Abelian?,I know that all groups of order $\leq$ 5 are Abelian and all groups of prime order are Abelian. Are there any other examples? If so is there something special about the orders of these groups?,"['finite-groups', 'abelian-groups', 'group-theory']"
1556861,"Find all smooth functions such that $f(x)f(y)=\int_{x-y}^{x+y}f(t)dt$ for all $x,y \in \mathbb{R}$","From Art of Problem Solving: Find all continuous, differentiable functions f with domain $\mathbb{R}$ such that
  $$f(x)f(y)=\int_{x-y}^{x+y}f(t)dt$$
  for all $x,y \in \mathbb{R}$. Find all $Polynomial Functions$ $Trigonometric Functions$ $Functions$ Then prove all functions found are the only functions that satisfy the equation. As some hints provided by the book, the following functions work: $2x$ $c$ $\frac{2}{c}\sin(cx)$ It is also noted that there is another family of functions that satisfies the equation. The problem should be able to be solved without any multivariable methods. As of now I have only been able to conclude types of functions that satisfy the equation when $x=0$ or $y=0$.","['ordinary-differential-equations', 'calculus']"
1556864,Find Least Squares Regression Line,"I have a problem where I need to find the least squares regression line. I have found $\beta_0$ and $\beta_1$ in the following equation $$y = \beta_0 + \beta_1 \cdot x + \epsilon$$ So I have both the vectors $y$ and $x$. I know that $\hat{y}$ the vector predictor of $y$ is $x \cdot \beta$ and that the residual vector is $\epsilon = y - \hat{y}$. I know also that the least squares regression line looks something like this $$\hat{y} = a + b \cdot x$$
and that what I need to find is $a$ and $b$, but I don't know exactly how to do it. Currently I am using Matlab, and I need to do it in Matlab. Any idea how should I proceed, based on the fact that I am using Matlab? Correct me if I did/said something wrong anyway.","['least-squares', 'linear-regression', 'statistics', 'convex-optimization', 'matlab']"
1556867,Is $f(x)=e^x \cdot \cos(e^x)$ a tempered distribution?,"Let $f(x)=e^x \cdot \cos(e^x)$. Define $$T_f(\varphi)=\int_{-\infty}^{+\infty} f(x) \cdot \varphi(x) \ .$$ 
I would like to know if $T_f$ defined with the formula above defines a tempered distribution (in the sense of the definition given here ).","['functional-analysis', 'distribution-theory']"
1556868,Evaluate $\lim_{n\to \infty}{\sqrt[n]\frac{(2n)!}{n^n\times{n!}}}$,$$\lim_{n\to \infty}{\sqrt[n]\frac{(2n)!}{n^n\times{n!}}}$$ It is a sequence and n is natural It looks like I should use $\lim_{n\to \infty}{\sqrt[n]{a_n}}=x$ but I don't know how. Does it mean that $a_n=\frac{(2n)!}{n^n\times{n!}}$ and then do it from there or is $a_n=\sqrt[n]\frac{(2n)!}{n^n\times{n!}}$ I have never used this before and I am not sure what to do,['limits']
1556878,Matrix Differential Equation $P'(t)= A(t)P(t)$,"Given $n\times n$ matrix $P(t)$ and $A(t)$ , if $P(t)$ satisfies the matrix differential equation $P'(t)= A(t)P(t)$ and the initial condition $P(0)=P_0$. Then Prove 
  $$\det P(t) = \det P_0 \times \exp\left(\int_0^t tr(A)(s)\,ds\right)$$ If the matrix $A$ is constant matrix then it is easy.
But I don't know how to prove it when $A = A(t)$ which has dependent on variable $t$.
Please help...","['matrix-calculus', 'ordinary-differential-equations']"
1556881,using symmetry or geometry for a double integral $\iint_D (4-\sqrt{9-x^2-y^2}+\sin(xy))dA$,"Using either geometry or symmetry, evaluate: $$\iint_D \left(4-\sqrt{9-x^2-y^2}+\sin(xy)\right)dA$$ where $D$ is the disk with $r=3$ centered at $(0,0)$. [ What I did ] I separated them to 3 integrals. For the first integral I got 
$4$ times the area of $D$. The second one I got volume of the hemisphere with radius $3$ and the last is I believe a value of $0$ by symmetry. So I get a total of $18\pi$. I am not sure about the first integral since I realized that if I am getting the area of D then I can't just combine it with the volume I found from the second integral. This is my observation, however I am not really sure. Please help.","['multivariable-calculus', 'integration', 'calculus']"
1556890,Prove that zeros of f are poles of 1/f,"Let $f$ be analytic at $z=z_0$ and have a zero of $n$th order at $z=z_0$. Then $1/f(z)$ has a pole of $n$th order at $z=z_0$. I want to prove this, and for this I expand $f(z)$ as a power series,
\begin{align*}
f(z) = \sum_{k=0}^\infty c_k (z-z_0)^k
\end{align*}
Since we know that $(z-z_0)$ is zero at $z=z_0$ all the way up to order $n$, i.e. $(z-z_0)^k = 0$ all the way up to $k=n$ (since it can be rewritten as $(z-z_0$ and by the definition of a zero $a_0=0$ such that $f(z_0) = 0$, it follows that
\begin{align*}
\frac{1}{f(z)} = \frac{1}{\sum_{k=0}^\infty c_k (z-z_0)^k},
\end{align*}
such that $1/f \rightarrow \infty$ about the same point. Is this proof complete enough or am I missing something?",['complex-analysis']
1556892,Calculating quotient ring over polynomial ring without guessing,"Suppose we have a quotient ring over a polynomial ring, i.e. we have an ideal $I$ and a ring, $K[X_1,...,X_n]$, then when we can we identify $K[X_1,...,X_n]/I$? What do I mean by this? Well, for example, we have $\mathbb{R}[x]/(x^2+1) \cong \mathbb{C}$, and $\mathbb{C}[x,y]/(x-y) \cong \mathbb{C}[x]$. So given an ideal, and a ring, is there any way of seeing what $K[X_1,...,X_n)/I$ is ""naturally"" isomorphic to, in a sense? I've come across this when trying to identify prime ideals. I.e., given an ideal, how can we quickly check that it's prime?","['maximal-and-prime-ideals', 'quotient-spaces', 'algebraic-geometry']"
1556980,"Semantics: 'determinant bundle', top exterior power of vector bundle","From what I can dig up, given a vector bundle $E\rightarrow X$, the determinant bundle associated to this is $\Lambda^{n}E\rightarrow X$, where $n$ is the rank of $E\rightarrow X$.  Is this the same thing as ""the determinant of the vector bundle $E\rightarrow X$""? If this is true, then does the statement ""$E\rightarrow X$ has trivial determinant"" mean that $\Lambda^{n}E\rightarrow X$ is trivializable as a vector bundle""?","['differential-geometry', 'fiber-bundles', 'vector-bundles', 'linear-algebra']"
1556981,Trigonometric identity proof $\cos(A) + \cos( B)$,"What is a nice proof of $$\cos(A) + \cos( B)= 2\cos\Big(\frac{A+B}{2}\Big)\cos\Big(\frac{A+B}{2}\Big)$$? 
I can prove it starting with the RHS but i want to be able to quickly prove it starting on the LHS as I won't have access to a formula book when I need it.","['algebra-precalculus', 'trigonometry']"
1556988,Is the Kernel locally free?,"Suppose, on a smooth projective complex variety $X$ that we are given an effective divisor $D$ and $A\in\mathrm{Pic}(D)$ a globally generated line bundle on $D$ with $r$ independent sections. Then we have a surjection $\mathcal{O}_X^{\oplus r}\to A$ (here we view $A$ as a torsion sheaf on $X$ with support on $D$) and thus an exact sequence
$$0\to F\to\mathcal{O}_X^{\oplus r}\to A \to 0$$
where $F$ is the kernel sheaf. Is $F$ locally free? What about the same situation but with a general vector bundle $V$ in place of $\mathcal{O}_X^{\oplus r}$ ?","['vector-bundles', 'algebraic-geometry', 'commutative-algebra']"
1557015,The domain of $x^x$?,"This one looks simple, but apparently there is something more to it. 
$$f{(x)=x^x}$$
I read somewhere that the domain is $\Bbb R_+$, a friend said that $x\lt-1, x\gt0$... I'm really confused, because i don't understand why the domain isn't just all the real numbers.
According to any grapher online the domain is $\Bbb R_+$.
Any Thoughts on the matter? Can someone explain what am I missing?",['real-analysis']
1557024,Prove the identity Binomial Series,"Use $(1-x)^{2n} = (1-x)^n(1-x)^n$ to prove the identity $${2n \choose n} = \sum_k {n \choose k}^2$$
I converted $(1-x)^{2n}$ into a binomial series yielding $$\sum_{k=0}^{2n} {2n \choose k} (-x)^k$$
And I converted $(1-x)^{n}$ into a binomial series yielding $$\sum_{k=0}^{n} {n \choose k} (-x)^k$$
I combined these series to get $${2n \choose k} (-x)^k = \sum_{k=0}^{n} {n \choose k} (-x)^k {n \choose n-k} (-x)^{n-k}$$
I divided through by $(-x)^k$ to eliminate the x terms and now I am stuck at $${2n \choose k} =\sum_{k=0}^{n} {n \choose k} {n \choose n-k}$$
Basically I am struggling to connect the final dots and create the punchline of the proof. Any help would be greatly appreciated!","['binomial-coefficients', 'discrete-mathematics']"
1557058,Exact Sum of Series,"I am a tutor at university, and one of my students brought me this question, which I was unable to work out. It is from a past final exam in calculus II, so any response should be very basic in what machinery it uses, although it may be complicated. The series is: $$\sum \limits_{n=1}^{\infty} \frac{(-1)^n}{(2n+3)(3^n)}.$$ Normally I'm pretty good with infinite series. It is clear enough to me that this sum converges. None of the kind of obvious rearrangements yielded anything, and I couldn't come up with any smart tricks in the time we had. I put it into Wolfram and got a very striking answer indeed. Wolfram reports the value to be $\frac{1}{6}(16-3\sqrt{3} \pi)$. It does this using something it calls the ""Lerch Transcendent"" ( link here about Lerch) . After looking around, I think maybe I can understand how the summing is done, if you knew about this guy and special values it takes. But how could I do it as a calculus II student, never having seen anything like this monstrosity before?","['sequences-and-series', 'calculus']"
1557070,Transpose of $(X'X)^{-1}$,"I am taking a Phd class in econometrics, and the following is used constantly, for $X$ a $n\times k$ matrix, $n \neq k$:
$$(X'X)^{-1} = ((X'X)^{-1})'$$
with ""$'$"" standing for transpose. Having a rather weak background in linear algebra, I cannot understand why this is true. For example:
${\underset{k\times1}{\underbrace{\left(\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}\underset{k\times1}{\underbrace{X'u}}\right)}}\underset{1\times k}{\underbrace{\left(\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}\underset{k\times1}{\underbrace{X'u}}\right)'}}}={\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}\underset{k\times n}{\underbrace{X'}}\underset{n\times n}{\underbrace{uu'}}\underset{n\times k}{\underbrace{X}}\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}}$ To my understanding the far right expression on the RHS should be different.","['matrices', 'transpose', 'linear-algebra', 'regression']"
1557074,Do the polynomials $(1+z/n)^n$ converge compactly to $e^z$ on $\mathbb{C}$?,"The question is Do the polynomials $p_n(x)=(1+z/n)^n$ converge compactly (or uniformly on compact subsets) to $e^z$ on $\mathbb{C}$? I thought about expanding
$$p_n(z)=\sum_{k=0}^n a_k^{(n)}z^k$$
where
$$a_k^{(n)}=\binom{n}{k}\frac{1}{n^k}=\frac{1}{k!}\prod_{j=0}^{k-1}\left(1-\frac{j}{n}\right)$$
and trying to show that $\frac{1}{k!}-a_k^{(n)}$ decreases sufficiently fast on any closed ball. That is, I tried to show
$$\lim_{n\rightarrow\infty}\max_{z\in\overline{B_0(A)}}\left|\sum_{k=0}^n\frac{z^k}{k!}-p_n(z)\right|=0$$
for any fixed $A>0$, but I had difficulty with this approach. Any help is appreciated.","['complex-analysis', 'exponential-function', 'complex-numbers', 'uniform-convergence']"
1557094,Proving the continuity of a function with respect to a measure,"Let $\mathcal{M}([0,1])$ be the space of all real finite measures on $[0,1]$, with norm $\|\mu\|=|\mu|([0,1])$ and consider the function $$u(y)=\int_{[0,1]}\min\{x,y\}\mu(dx)$$ for $y\in[0,1]$. I want to prove that it's continuous. So far, I have $$u(y)=x\int_{[0,x]}\mu(dy)+\int_{(x,1]}y\mu(dy)$$ My immediate question would be: how do I evaluate these integrals? But of course, I'd like to know how to answer this question in general.","['stochastic-processes', 'probability-theory', 'measure-theory', 'stochastic-analysis', 'analysis']"
1557103,Meaning of adding rows matrix,"English is not my mother tongue and I'm studying Algebra using a book in English. This sentence came up to me in an exercise ""every row of matrix $A$ adds to zero"". What does that mean, in concrete? EDIT: Full exercise: If every row of $A$ adds to zero, prove that $\det A = 0$. If every row adds to $1$, prove that $\det (A-I) = 0$. Show by example that this does not imply $\det A = 1$.","['matrices', 'linear-algebra', 'terminology']"
1557117,Is every locally compact Hausdorff space paracompact?,"It seems likely that for any open cover, we can construct a locally finite refinement using the local compactness of the space. I can't figure out how to work the construction though, and I'm not yet convinced that there is no counterexample.",['general-topology']
1557140,"Prob 6, Sec 7 in Munkres' TOPOLOGY, 2nd ed: The existence of an injection of a superset into the set means the sets have the same cardinality?","Let $A$ and $B$ be two sets such that $B \subset A$ and there is an injection $f \colon A \to B$. Then how to show that $A$ and $B$ have the same cardinality? Munkres' Hint: We define $A_1 \colon= A$, $B_1 \colon= B$, and, for $n > 1$, we define $A_n \colon= f(A_{n-1})$ and $B_n \colon= f(B_{n-1})$. Thus, we have 
$$B_1 = B \subset A_1 = A$$
and 
$$A_2 = f(A_1) = f(A) \subset B = B_1,$$
that is, 
$$A_2 \subset B_1 \subset A_1.$$
Now if, for any +ive integer $n \geq 1$, we have 
$$B_{n+1} \subset B_n \subset A_n, $$
then we also have
$$B_{n+2} = f(B_{n+1}) \subset f(B_n) = B_{n+1}$$
and 
$$B_{n+1} = f(B_n) \subset f(A_n) = A_{n+1},  $$
that is, 
$$B_{n+2} \subset B_{n+1} \subset A_{n+1}.$$
Hence using induction we can conclude that
$$A_{n+1} \subset B_n \subset A_n \ \mbox{ for each } \ n \in \mathbb{N}.$$
That is, 
$$A_1 \supset B_1 \supset A_2 \supset B_2 \supset A_3 \supset B_3 \supset \ldots.$$ Now let the map $h \colon A \to B$ be defined as follows:
$$ h(x) \colon= 
\begin{cases} 
f(x) \ & \mbox{ if } \ x \in A_n - B_n \ \mbox{ for some } \ n; \\
x \ & \mbox{ otherwise}. 
\end{cases}
$$
Munkres claims that this map $h$ is a bijection. How do we show this? Injectivity of $h$: Suppose $a, x \in A$ such that $h(a) = h(x)$. We need to show that $a=x$. If there exist $m, n \in \mathbb{N}$ such that $a \in A_m - B_m$ and $x \in A_n - B_n$, then we must have 
$$f(a) = h(a) = h(b) = f(b),$$
which implies $a = b$. On the other hand, if $a \not\in A_n - B_n$ for any $n \in \mathbb{N}$ and $x \not\in A_n - B_n$ for any $n \in \mathbb{N}$, then we have 
$$a = h(a) = h(b) = b.$$ How to show that $a = b$ in the following situation? There exists some $m \in \mathbb{N}$ such that $a \in A_m - B_m$ but 
$x \not\in A_n - B_n$ for any $n \in \mathbb{N}$. Surjectivity of $h$: Let $b \in B$. Then $b \in A$. If $b \not\in A_n - B_n$ for any $n \in \mathbb{N}$, then we have 
$b = h(b)$. On the other hand, if, for some $k \in \mathbb{N}$, we have $b \in A_k - B_k$, then let $k$ be the smallest such positive integer. If $k = 1$, then we have $b \in A - B$, which is a contradiction as $b \in B$, by our hypothesis. So we must have $k > 1$; rather, $k \geq 2$. As $b \in A_k - B_k$ and as $A_k \subset B_{k-1}$, so we must have $b \in B_{k-1}$. What next?","['general-topology', 'cardinals', 'elementary-set-theory']"
1557258,How to solve $(a^2 - x^2) \frac {\mathrm d y} {\mathrm d x} + 2xy + (a^2 - y^2) \frac {\mathrm d x} {\mathrm d y}=0$,"The differential equation I am trying to solve is: $\displaystyle (a^2 - x^2) \frac {\mathrm d y} {\mathrm d x} + 2xy + (a^2 - y^2) \frac {\mathrm d x} {\mathrm d y}=0$ How do I go about this? I have tried integrating it but I'm not sure how to manage the first term, do I just integrate it twice with respect to y?",['ordinary-differential-equations']
1557282,Does a limit to this Hypergeometric Function Exist Analytically?,"I am interested in evaluating limit $$\lim_{x\rightarrow\pi/2}\left[(\cos x)^n\, _2F_1\left(-\frac{n}{2},-n-m+1;\frac{1}{2}-n;-\frac{16m c}{\cos^2x}\right)\right],
$$
where $n$ is a positive even integer, $m$ and $c$ are reals. I checked with Mathematica, and this expression goes to a limiting value at $x\rightarrow \pi/2$, but I want to know if there is any analytical expression interms of $m$ and $c$.","['special-functions', 'hypergeometric-function', 'calculus', 'limits']"
1557293,Exchanging limits with norms and linear functionals,"In a normed vector space $X$, when can we say: $\lim\|x_n\|=\|\lim x_n\|$ and further, if $f\in X^{*}$, when can we say: $\lim fx_n=f(\lim x_n)$?","['functional-analysis', 'normed-spaces', 'real-analysis', 'limits']"
1557331,Finding the cardinalty of a subset of $\mathcal{P}(\mathbb{N}) $,"I'm trying to find the cardinality of a certain set and I'm stuck. The problem is, we haven't learned about cardinality nor about any of its rules and equalities. We are asked to find a set whose ""cardinality"" is equal to the given set, and prove the sets are equivalent by finding a bijective and surjective function between them. The question is: ""Let $\mathcal{P}(\mathbb{N}) $ be the power set of $\mathbb{N}$ . We define the following subset, $B \subseteq \mathcal{P}(\mathbb{N}) $ ,
 in the following way: $B=\lbrace A \in \mathcal{P}(\mathbb{N}) |$ if $p\in A$ is a prime number then $\forall n\in \mathbb{N} \Rightarrow$ $n\cdot p\in A \rbrace$ What is the cardinality of $B$ ? Prove your claim by finding an appropriate bijective and surjective function."" As I'd stated before, we cannot use any equalities associated with cardinalities. All we can do is guess and try to prove. I would appreciate any hints as to how to define an appropriate function.","['cardinals', 'elementary-set-theory']"
1557361,range of $T$ equals range of $T^2$ if and only if the intersection of the range and kernel of $T$ is trivial,"Let $V$ be a finite-dimensional vector space over a field $F$ and let $T$ be an operator on $V$. Prove $\text{range}(T^2) = \text{range}(T)$ if and only if $\text{range}(T) \cap \ker(T) = \{0 \}$. Proof. Assume $v \in \text{range}(T) \cap \ker(T) = \text{range}(T^2) \cap \ker(T)$. Then there are vectors $v_1,v_2 \in V$ such that $(\star) \hspace{1mm} v = T^2(v_1)=T(v_2)$ and $T(v)=0$ Thus $T(v) = T^3(v_1) = T^2(v_2) = 0$, implying that $$T(v) = T^3(v_1)+T^2(v_2)= T(T^2(v_1)+T(v_2)) \Longleftrightarrow v = T^2(v_1)+T(v_2)= v + v = 2v$$ $$\hspace{5.6cm} \Longleftrightarrow v = 2v \Rightarrow v = 0.$$ Conversely, assume that the intersection of $\text{range}(T)$ and $\ker(T)$ is trivial and let $v \in \text{range}(T^2)$. Then there exists $u \in V$ such that $v = T^2(u)=T(T(u))$. Since $T$ is an operator, $T(u) \in V$, implying that there exists a vector $w \in V$ such that $w = T(u)$. Hence $T^2(u)=T(w) = v$, implying that $v \in \text{range}(T)$, and so $\text{range}(T^2) \subset \text{range} (T)$. This is where I get stuck. I'm trying to show that $\dim \text{range} (T^2) = \dim \text{range}(T)$, so the above inclusion implies equality, but I am having trouble. I know we probably need to apply the rank-nullity theorem combined with our assumption that the intersection of the range and kernel is trivial, but I am not sure how.",['linear-algebra']
1557366,"Can we, in a certain way, quantify the measure of non-differentiability of functions that are continuous everywhere but differentiable nowhere?","I am not sure how to ask this question because it seems to me that my thoughts on this topic are not clear enough, but I will give it a try. What, really, do I want to know? Well, I would like to know is there any ""measure"" on how ""far"" is such a function from being differentiable at some point? In other words, and hopefully, more precise ones: Suppose that $f$ is some function that is continuous at every point of its domain and differentiable nowhere. Is there a way to assign to every point of such a function a number which measures ""how far is function from being differentiable"" at that point, or, ""how non-differentiable"" the function at that point is? I like to believe that some of you really understand the spirit of the question and what it is all about, although I at the moment do not know how to state it more correctly and more rigorously.","['soft-question', 'analysis']"
1557370,Does the Divergence Theorem hold for arbitrary tensor fields?,"So, a heads up, this is my first post and I'm a fairly new user.  Additionally, my math knowledge tops out at vector calculus and ODEs, but don't shy away from answering beyond my understanding should it be necessary. Anyway, my question is simply whether the divergence theorem holds for fields other than general 3-vector fields, and if so, what changes.  I imagine the dot product would be become an inner product and the gradient operator would have to evolve somehow, but I have no idea how. For simplicity, I think it can be assumed that I'm asking whether the divergence theorem holds for rank-2 tensors. Thanks in advance for any insight.","['multivariable-calculus', 'tensors']"
1557389,Prove the inverse of a strictly increasing function is differentiable.,"So, I was given the following problem as part of a homework assignment. Suppose $f'(x) > 0$ in $(a,b)$. Prove that $f$ is strictly increasing in $(a,b)$, and let $g$ be its inverse function. Prove that $g$ is differentiable, and that
  $$g'(f(x)) = \frac{1}{f'(x)}$$ I have proven that $f$ is strictly increasing in $(a,b)$, and I could prove that $g'(f(x)) = 1/f'(x)$ if I could prove that $g$ is differentiable. The problem is that I am having trouble with a proof of that. Any advice? Also, as a reference, this is exercise 5.2 from Baby Rudin.","['derivatives', 'real-analysis', 'monotone-functions', 'analysis']"
1557482,Geometrical interpretation of $\lim_{h \to 0} \frac{f(x_0+h) - f(x_0+h) }{2h}$,"Let $f:D \subset \Bbb R\to \Bbb R$ is $f$ differentiable at $x_0$ Is it true that: $$f'(x_0)=\lim_{h \to 0} \frac{f(x_0+h) - f(x_0+h) }{2h}$$ $$f'(x_0)=\lim_{h \to 0} \frac{f(x_0+2h) - f(x_0)}{h} \text{ ?}$$ Write the geometrical interpretation of each case. For the first one I have already proved that are equals and the reciprocal is not true, but I do not know what is the geometrical interpretation. Any ideas?","['derivatives', 'real-analysis', 'calculus']"
1557483,Prove that there exists only one prime number of the form $p^2âˆ’1$ where $pâ‰¥2$ is an integer,"By factoring  $p^2 âˆ’ 1$, we have $(p + 1)(p - 1)$. I know that $p = 2$ which gives $3$ is the only solution. However, how do I prove that $p = 2$ is the only integer which gives a prime?","['discrete-mathematics', 'proof-writing', 'proof-verification', 'elementary-number-theory']"
1557494,The trigonometric expression $\frac{1}{2}\sin x$ is equivalent to,The trigonometric expression $\frac{1}{2}\sin x$ is equivalent to?,['trigonometry']
1557507,Show that $\sup_n \mathbb{E}(|S_n|)<\infty$ implies $\mathbb{E} \left( \sup_n |S_n| \right)<\infty$ for $S_n=\sum_{k=1}^n X_k$ with $X_k$ iid,"Let $S_n=\sum^n_{k=1}X_k$, and $X_k$'s are mutually independent. Suppose $X_k$ are integrable, and $\sup_nE|S_n|<\infty$. Show that $E(\sup_n|S_n|)<\infty$. I have shown Ottaviani's inequality: $P(\max_{k\leq n}|S_k|\geq t+s)\leq P(|S_n|\geq t)+P(\max_{k\leq n}|S_k|\geq t+s)\max_{k\leq n}P(|S_n-S_k|>s)$. I think this should be helpful, however, I don't know how.","['martingales', 'probability-theory', 'inequality', 'expectation']"
1557519,Mapping from reals to naturals if naturals can be used infinitely many times,"This may have been asked before, since it seems like an obvious question, but I couldn't find it. I just read some interesting stuff about set theory and how the size of the natural numbers is not the same as the size of the real numbers. How I understand it is that there is no function $f$ whose domain is all natural numbers and whose range is all real numbers such that $f^{-1}$ also exists. Obviously, if we allow each natural number to pair with two real numbers instead of just one, there still aren't enough natural numbers. This is pretty intuitive. But what if we allowed each natural number to pair with countably infinitely many real numbers. Then does a mapping exist? Mathematically, does there exist a function $f$ (domain: all real numbers, range all natural numbers) such that for every natural number $n$, the set of real numbers $r$ satisfying $f(r) = n$ is countably infinite?","['real-numbers', 'elementary-set-theory', 'elementary-number-theory']"
1557579,"Train wait problem, probability","If one commuter train comes every $15$ minutes and another comes every $40$ minutes, what is the average amount of time one would have to wait before getting on a train? Suppose that they are not synchronized. The answer by the way is $6.5$ minutes. I don't see how the trains are not synchronized. Because they will arrive at the same time in $15*40$ minutes. Not sure if I understood the problem. I think it relates to the uniform distribution and the expected value of the uniform distribution. Please show steps in how you solve it.","['probability', 'probability-distributions']"
1557584,Identity of $8\sin^2(t)\cos^2(t)$,"I know this probably has a simple answer, but I am having trouble understanding the steps to find the identity for this problem. This is the answer I was provided: $$8\sin^2(x)\cos^2(x) = 2\sin^2(2x)$$ The closest Identity I can find is: $$\sin(x)\cos(y) = 1/2[\sin(x+y) + \sin(x-y)]$$ Which would give $$\frac 1 2 [\sin(x+x) + \sin(x-x)] = \frac 1 2 [\sin(2x)+\sin(0)] = \frac 1 2 [\sin(2x)+1]$$ Which plugged in would give:
$4(\sin(2x) + 1)$ Which clearly isn't what I'm looking for... tell me what I'm doing wrong please!",['trigonometry']
1557625,Proving the chain rule of a given function,"Suppose that $f'(2)=3$, $f'(5)=4$, and let $h(x)$ be the composite function $h(x) = f(x^2+1)$. Find $h'(2)$ I get how to prove the $f'g(x)*g'(x)$ part, which leads to $4*g'(2)$ but how do I prove $g'(2)$ with the information given? Or was I given $f'(2)=3$ to throw me off?","['derivatives', 'real-analysis', 'function-and-relation-composition']"
1557642,Metric is continuous function [duplicate],"This question already has answers here : How to prove the continuity of the metric function? (2 answers) Closed 6 years ago . Let $X$ be a metric space , $d$ is the metric , show that $d$ is a continuous function from $X\times X$ to $R$. I think the definition is all we need , but I just don't know where to start , can anyone help me.","['general-topology', 'metric-spaces']"
1557646,Are compactly supported smooth functions dense in continuous functions?,"Let $\Omega\subset \Bbb R^N$ be open(or a domain if needed). Is it true that $C^{\infty}_c(\Omega)$ is dense in $C^0(\Omega)$? Actually, I'm always confused about some sets(especially $C^{\infty}_c$) are dense in other sets. Can there be a easy explanation or insight?","['real-analysis', 'analysis']"
1557665,Is the scalar multiple of an eigenvector also an eigenvector for a particular eigenvalue?,"I'm working on a problem from my textbook and found that $\left(\frac{1}{2}, \frac{1}{2}, 1\right)$ is an eigenvector for a particular eigenvalue of $4$. The textbook solution says that the answer is $(1, 1, 2)$ which is just $2 \times \left(\frac{1}{2}, \frac{1}{2}, 1\right)$","['eigenvalues-eigenvectors', 'linear-algebra']"
1557676,Proving the intersection of distinct eigenspaces is trivial,"Suppose $\lambda_1$ and $\lambda_2$ are different eigenvalues of $T$. Prove $E_{\lambda_1} \cap E_{\lambda_2}= \{\vec0\}$. 
I have a basic idea of what to do. Since both eigenvalues are distinct, doesn't that mean the basis for each space are linearly independent of each other so that no vector in one is in the span of the basis of the other? I'm just looking on how to formalize these ideas.",['linear-algebra']
1557761,Number of $n^2\times n^2$ permutation matrices with a 1 in each $n\times n$ subgrid,"I found the following question in a paper I was trying to solve: The following figure shows a $3^2 \times 3^2$ grid divided into $3^2$ subgrids of size $3 \times 3$. This grid has $81$ cells, $9$ in each subgrid. Now consider an $n^2 \times n^2$ grid divided into $n^2$ subgrids of size $n \times n$. Find the number of ways in which you can select $n^2$ cells from this grid such that there is exactly one cell coming from each subgrid, one from each row and one from each column. My try: Since we have $n^2$ rows, $n^2$ columns and $n^2$ subgrids in total, we have to choose one and only one cell from each of them. Let's choose them one at a time. We can choose the first cell in $n^4$ many ways. Then, we'll have to avoid that subgrid, that column and that row that we've chosen the first one from when choosing the second cell. So, we have $n^4-n^2-2n(n-1)$ choices. We can continue this to get the total number of possible ways. But, I think there's a hole. Say, we've chosen the first cell from the subgrid of the up-left corner and the second from the subgrid just right to it so that it doesn't violate any rules. Then, when finding the number of ways we can choose the third cell, we would have substracted some of the cells twice. I think you get it. Please, if anyone can help me solving this problem, it'd be greatly appreciated.","['combinatorics', 'discrete-mathematics']"
1557838,A map from zeros of $\zeta(s)$ to zeros of $C(s)?$,"Let $P(s),C(s),\zeta(s)$ be the prime zeta function, the analogous composite zeta function, and the classical zeta function. I do not know whether it is known that there are infinitely many zeros of C, (or P). As an exercise I tried to show this. My question is whether A-C below is a plausible argument (in the sense that it could be made into a proof) for the proposition: Proposition: There are infinitely many zeros of $C(s)$ (and $ P(s)$). A. Loosely speaking, a zero of an analytic function $f$ is a continuous function of $f.$ Since $\zeta,P,C$ are analytic, it seems that Paul Rosenbloom's 1969 paper in J. of Approximation Thy, ""Perturbation of the Zeros of Analytic Functions I"", applies, in particular Hurwitz's thm. If $f_n\to f$ the limit pts. of the zeros of $f_n$ are the zeros of f. More to the point, he appears to show that (with conditions) if f is close to g, near any point where g is close to 0 there is a point where g is exactly zero. B. We can map every nontrivial zero of $\zeta(s)$ to a zero or set of zeros of $C(s)$ and conversely. Starting at a nontrivial zero of $\zeta(s)$ we subtract (if necessary, small multiples of) terms of the analytic continuation of $P(s)$ from $\zeta(s),$ which shifts the zero of $\zeta$ stepwise towards that of $C(s).$ In a small finite number of steps the zero of the perturbed function approximates that of $C(s).$ C. Two zeros may map to the same zero of C (or P) but this is not I think an issue. If we can show that for given zero $z$ of $\zeta$ and the (finite) set of zeros $s$ to which $z$ can be perturbed, the largest distance $|z-s_o|$ is finite, then we only have to choose the next $z$--as $\Im(z)$ increases--sufficiently large that none of the zeros to which it perturbs could possibly be $s_o.$ Continuing in this way we get an infinite sequence of zeros of C. One possible problem is that one may be drawing a zero of $P$ (or $C$) towards a point which is a zero of $\zeta,$ hence a badly-behaved point of (the analytic continuation of) P. For example, it seems hard to find a path from $C(0.38+12.4~i)= 0$ to $\zeta(.5+14.1~i)=0,$ but by adding a convenient number $z$ we can get a path $C\to C+z \to \zeta+z \to \zeta$ which avoids discontinuities. Since  we have a path from $C(.32+15.4~i)=0$ to $\zeta(0.5+14.1~i)=0$ the mapping is evidently many-to-one, but still would show infinitely many zeros of P and C if the idea is otherwise sound. Example. There is an analytic continuation of $\zeta$ for $\sigma \leq 1.$ There is an analytic continuation of $P$ valid for $ 0 < \sigma. $ The continuation for $P$ obtained by Mobius inversion is $$P(s)= \sum_{k=1}^\infty  \frac{\mu(k)}{k}\log \zeta(k s).$$ Let $P_N(s) = \sum_{k=1}^N \frac{\mu(k)}{k}\log \zeta (k s).$ $\zeta(1/2+56.446... i) = 0.$ $\zeta(.5+55.8~ i) - P_1(.5+55.8 ~i)\approx 0. $ $\zeta(.65+55.8 ~i)-P_2(.65+55.8 ~i)\approx 0.$ ... $\zeta(.64475 +55.8908~ i) -  P_{20}(.64475+55.8908~ i) \approx C(.64475+55.8908 ~i)\approx 0.$ Detail of the above calculation with different zero: perturbing the first nontrivial zero of $\zeta$ to a zero of $\zeta(s)-(\mu(1)/1)\log \zeta(1(s))$ in increments of 1/20. For the table the perturbation begins at the first nontrivial zero of $\zeta\approx (0.5+ 14.13~i).$ $\zeta(a+b~ i)- (m/20)(\mu(1)/1)\log \zeta(1(a+b~i))\approx 0$ $$\begin{array}{c | c | c | }  m & a & b   \\ \hline
0 & 0.5 & 14.13   \\ \hline
1 & 0.42 &14.3  \\ \hline   
2 & 0.36& 14.4\\ \hline
3 & 0.33& 14.5 \\ \hline
4 & 0.32 & 14.6   \\ \hline    
... & ... & ... \\  \hline
17& 0.24 & 15.4 \\   \hline
18 & 0.22 & 15.5\\    \hline
19 & 0.22 & 15.58\\ \hline
20 & 0.22 & 15.6 \\ \hline
 \end{array}$$ The analogous problem for $P(s)$ is intended as part of the question.","['complex-analysis', 'analytic-number-theory', 'zeta-functions']"
1557857,Expected value for blackjack,"In the game of blackjack, the odds of winning each hand are slightly less than 50 percent. As you play an infinite amount of hands, you would always lose money because you would win less than 50 percent of the time. By this rationale, wouldn't your highest odds of winning be if you only played one game?","['probability-theory', 'probability', 'gambling']"
1557878,Do infinite dimensional Hermitian operators admit a complete basis of eigenvectors?,"I'm currently taking a quantum mechanics course. We have proven that hermitian operators always have real eigenvalues, that we can choose the eigenvectors to be orthonormal, and that finite dimensional hermitian operators are diagonalizable (i.e., admit a complete basis of eigenvectors). Can this last result be generalized to the infinite dimensional case? The standard proof seems to use induction on the dimension of the operator, so this proof certainly doesn't carry over.","['eigenvalues-eigenvectors', 'quantum-mechanics', 'linear-algebra']"
1557936,A product of smooth manifolds together with one smooth manifold with boundary is a smooth manifold with boundary,"Suppose $M_1, \dots M_k$ are smooth manifolds and $N$ is a smooth manifold with boundary. Then how do I see that  $M_1 \times \dots \times M_k \times N$ is a smooth manifold with boundary, and$$\partial(M_1 \times \dots \times M_k \times N) = M_1 \times \dots \times M_k \times \partial N?$$","['differential-topology', 'smooth-manifolds', 'manifolds', 'general-topology', 'differential-geometry']"
1557954,How to show $\sum_{k=n}^\infty{\frac{1}{k!}} \leq \frac{2}{n!}$,"$$\sum_{k=n}^\infty{\frac{1}{k!}} \leq \frac{2}{n!}$$ Can someone show why this estimate holds true? I tried quite a bit but couldn't really find a way to approach this. WolframAlpha says it is true but I don't know what the gamma function is. $$ \sum_{k=n}^\infty{\frac{1}{k!}} = \frac{1}{n!} + \sum_{k = n+1}^\infty \frac{1}{k!}$$
So then I need to show that$$ \sum_{k=n+1}^\infty{\frac{1}{k!}} \leq \frac{1}{(n+1)!} ~~\Big[\leq \frac{1}{n!}\Big]$$ Is it possible to do this by induction? I don't really know how to approach this now.","['real-analysis', 'inequality', 'sequences-and-series', 'factorial', 'power-series']"
1557961,Independence of time series data,"I have a time series data with $52$ observations and I would like to check for the independence between observations. The ACF for correlation and covariance of my data look I am aware that $covariance = 0$ does not imply independence, except for Gaussian process. I wonder if I can use ACF to show the independence of my data or there are ways to justify it? Many thanks.","['stochastic-processes', 'statistics', 'probability', 'time-series']"
1557979,"$\text{Hom}_k(M,N)\cong M^*\otimes_k N$ as Hopf-algebra modules.","I'm reading Representations and Cohomology by D.J. Benson. At the beginning of the third chapter the following is explained: Let $\Lambda$ be a bialgebra over $R$ and $M,N$ left $\Lambda$-modules. We make $M\otimes_R N$ into a $\Lambda$-module as follows: If
$$\Delta(\lambda)=\sum_i \mu_i\otimes \nu_i$$ then 
$$\lambda(m\otimes n)=\sum_i \mu_i(m)\otimes \nu_i(n).$$
We can also make $R$ into a $\Lambda$-module via $\lambda(r)=\varepsilon(\lambda)r$. If $\Lambda$ is a Hopf-algebra over $R$ and $M,N$ are left $\Lambda$-modules, then we make $\text{Hom}_R(M,N)$ into a $\Lambda$-module as follows: If $$\Delta(\lambda)=\sum_i \mu_i\otimes \nu_i$$ and $\phi\in \text{Hom}_R(M,N)$, then 
$$\lambda(\phi)(m)=\sum_i\mu_i(\phi(\eta(\nu_i)(m))),$$
where $\eta$ means the antipode of $\Lambda$.
We write $M^*=\text{Hom}_R(M,R)$. Note that we are viewing $M^*$ as a left $\Lambda$-module. Because of the antipode $\eta$, we can regard right $\Lambda$-modules as left $\Lambda$-modules via $\lambda m = m\eta(\lambda)$ and vice-versa. Now, suppose that $R=k$ is a field and $\Lambda$ is a cocommutative Hopf-algebra. Suppose that $M,N$ are two left $\Lambda$-modules that are finite dimensional as $k$-vector spaces, then the natural vector space isomorphism
$$\text{Hom}_k(M,N)\cong M^*\otimes_kN$$ is a $\Lambda$-module isomorphism. I don't understand why this last statement is true. I know that the natural bijection is given by 
$$f:M^*\otimes_k N\rightarrow \text{Hom}_k(M,N):\phi\otimes w\mapsto \phi(\cdot)w.$$ I only have to show that this map is a $\Lambda$-module morphism. So let $\lambda\in \Lambda$ such that $\Delta(\lambda)=\sum_i\mu_i\otimes \nu_i=\sum_i\nu_i\otimes \mu_i$ (cocommutativity), and we calculate
\begin{eqnarray*}
 (\lambda\cdot f(\phi\otimes w))(v) &=& \sum_i \mu_i(f(\phi\otimes w)(\eta(\nu_i)(v)))\\
&=& \sum_i \mu_i(\phi(\eta(\nu_i)(v))w),\\
f(\lambda\cdot (\phi\otimes w))(v) &=& f(\sum_i (\mu_i\phi)\otimes (\nu_i w))(v)\\
&=& \sum_i (\mu_i\phi)(v)\nu_i w.
\end{eqnarray*} By cocommutativity $\mu_i$ and $\nu_i$ are interchangeable in the calculation, however if I want to proceed, we need to consider $\Delta(\mu_i)$ to calculate $\mu_i\phi$ since $\mu_i\in \Lambda$ and $\phi\in \text{Hom}_k(M,k)$ (which we gave a $\Lambda$-module structure.) Now this leads to nowhere, so how do I see that this really is a $\Lambda$-module morphism? Thank you in advance!","['modules', 'proof-verification', 'representation-theory', 'hopf-algebras', 'linear-algebra']"
1558047,How calculators do trigonometry [duplicate],"This question already has answers here : How does a calculator calculate the sine, cosine, tangent using just a number? (3 answers) Closed 8 years ago . I need to use a calculator to find trigonometric ratios like $\sin(41),\cos(32)$. How do calculators work for trigonometry? Also, as calculators are invented by the human mind can we do it mentally? Is there any binary trick or something like that?","['calculator', 'trigonometry']"
1558066,Is $xRy \iff x+y = 0$ an equivalence relation?,"$R$ is a relation on real numbers. $xRy \iff x+y = 0 $.
Is it an equivalence relation? My answer is no proof: -(Reflexive) let $x = a$ , $aRa \iff 2a=0$.
 Since $2a = 0$ doesn't hold for every real number $a$, $R$ is not reflexive. Since $R$ isn't reflexive, $R$ is not an equivalence relation. Is my reasoning correct ?","['equivalence-relations', 'proof-verification', 'discrete-mathematics']"
1558079,Bounding the distance between roots of a polynomial?,"Let $p(x) = c_n x^{n} + \cdots + c_0$ be a seperable polynomial with each $c_i \in \mathbb C$, call its roots $r_i$. The smallest gap between any two of the roots is $d = \text{min}_{i \not = j}|r_i - r_j|$. Is there any approximate lower bound for the smallest gap? Some simple expression $F$: $$F(c_n, \cdots, c_0) < d$$ The idea is that it should be much easier to compute than numerically approximating each root and finding the minimimum distance directly. More generally I would like to learn about the geometry of a polynomials roots in terms of its coefficients if anyone knew some notes on books on this.","['roots', 'polynomials', 'geometry']"
1558091,If J is a 101Ã—101 matrix with all entries equal to 1 and let I denote the identity matrix of order 101. Then what is the determinant of J-I?,If $J$ is a $101\times 101$ matrix with all entries equal to $1$ and let $I$ denote the identity matrix of order $101$. Then what is the determinant of $J-I$ ?,"['matrices', 'linear-algebra', 'determinant']"
1558101,Probability exercise about SLLN,"Let $\{X_n\}$ be i.i.d random variables. $E[X_1]=0$. Then $\sum_{i=1}^{n}X_i\over n$ converges almost surely to zero. I know that when the sequence $\{X_n\}$ satisfies $\sum_{i=1}^{\infty}{Var(X_i)\over i^2} \lt \infty$, the conclusion holds. So I tried to cut $X_n$ to make it satisfy the condition. Let $Y_n=X_n1_{\{a_n\le X_n\lt b_n\}}$, choose proper $a_n$ and $b_n$ can let $Y_n$ satisfy the condition. I also hope to let $E[Y_n]=0$, but that is hard. Any advice on this problem will be appreciated.","['law-of-large-numbers', 'probability-theory', 'probability']"
1558132,Markov process and filtration,"I would like to restate the question. I'm reading Revuz/Yor's definition of Markov process (P81), they started from transition function, and define the $P_t f(x)$ as usual (let's only consider the time homogenous case here). A process $X_t$ is called Markov w.r.t $\mathcal F_t$ if $E(f(X_{t+s})|\mathcal F_t) = P_sf(X_t)\tag{1}$ My previous impression had been that, once a transition function is defined, the induced process $X_t$ should be naturally Markov (w.r.t the natural filtration $\mathcal F^X_t$, of course). It is not clear to me, what is the intuition  of using the more general filtration $\mathcal F_t$ in the definition. More specifically, consider $\mathcal F_t = \mathcal F_t^X \vee \mathcal F_t^Y $ where $Y_t$ is another process. If $X_t$ is Markov under $\mathcal F^X_t$, what kind $Y_t$ will make equation(1) hold? Since equation (1) implies that $E(f(X_{t+s})|\mathcal F_t) = E(f(X_{t+s})|\mathcal F_t^X)$ which means, knowing the additional information about $Y$ (up to time $t$) will not change how $X$ evolves after $t$. This seems to suggest that $X$ is somehow independent of $Y$, or $Y$ does not provide additional information on how $X$ will evolve. Is there more accurate (and hopefully non-trivial) characterization of such $Y$? [Below is the old post, will delete later] Suppose $X_t$ is a Markov process with respect to its natural filtration. Let $Y_t$ be another process. Let $\mathcal F_t=\mathcal F_t^{X,Y}$ be the filtration generated by $X_t,Y_t$ jointly. Question: under what condition, is $X_t$ Markovian with respect to $\mathcal F_t$? Intuitively, as long as $\mathcal F_t^Y$ does not have any ""future"" information about $X_t$, it should be OK. So I guess something like $E(f(X_{t+s})|\mathcal{F}_t^Y)\in \mathcal F_t^X$ for any positive or bounded measurable $f$. Thanks [EDIT] Some interesting special cases: 
(1) $X$ and $Y$ are independent. 
(2) $X$ and $Y$ are jointly Markovian. I'm not quite sure about (2). To be more specific, if $X$ is Markovian with respect to the natural filtration, and $(X,Y)$ is also Markovian with respect to the natural filtration, prove: $X_t$ is Markovian with respect to $\mathcal F_t^{X,Y}$ [EDIT] Sorry, keep changing my mind. I guess the above is not true, but need a counter example.","['stochastic-processes', 'probability-theory', 'markov-process', 'stochastic-calculus']"
1558133,Different function with the same derivative,"Today at school I entered in a problem when the professor asked us to differentiate the following function: $$f(x)=\arctan\left(\frac {x-1}{x+1}\right)$$ With the basic rules of differentiation I came to a confusing result: $$f'(x)=\frac 1{1+x^2}$$ And the teacher agreed, and so does Wolfram (I checked at home) but what surprised me is that it's the same derivative as $$f(x)=\arctan x$$
$$f'(x)=\frac 1{1+x^2}$$ So I'm wondering: is that wrong in some sense ? Are the two function equals indeed ? If I integrate $\frac 1{1+x^2}$ what should I choose from the two ? Are there any other examples of different functions with the same derivative?","['derivatives', 'calculus']"
1558160,Definition of the Tangent Space,"I'm watching a series of lectures on differential geometry, and I've run into a bit of a problem with the definition of the tangent space. We first defined a tangent space as $\{(p,v) | v \in \mathbb{R}^n\}$ , which makes sense to me: it's the set of all vectors attached at point $p$ . We then defined the directional derivative as $$
(Df)(p,v) = \lim_{t \rightarrow 0} \frac{f(p + tv) - f(p)}{t}
$$ We expanded that to this: $$
(Df)(p,v) = \left( \sum_{i = 0}^{n}v_i \left.\frac{\partial}{\partial x_i}\right|_{p}
\right) f$$ This makes sense to me; we have defined the directional derivative as an operator that is applied to the function. Here's the part where I lose the plot. I'm then told that, if I think about it, the portion inside the parentheses is really interchangeable with $(p,v)$ . I'm afraid that I've thought about it, and I can't see the equivalence. $\sum_{i = 0}^{n}v_i \left.\frac{\partial}{\partial x_i}\right|_{p}$ is an operator (isn't it?) whereas $(p,v)$ is a ordered pair of elements of $\mathbb{R}^n$ . Does that mean that the expression $(p,v)(f)$ makes sense? What would that mean? I must be thinking about this the wrong way; can someone clarify?",['differential-geometry']
1558179,Show $X_1$ and $X_2$ have a common Gaussian distribution,"Anyone has any idea about the following question? Let $\Bbb E(X_1^2)$ and $\Bbb E(X_2^2)$ be finite. Show that if $X_1$ and $X_2$ are independent and likewise $X_1+X_2$ and $X_1-X_2$, then both $X_1$ and $X_2$ have a common Gaussian distribution. There is a hint that ""$2x_1 = 2x_1-x_2+x_2$ and $2x_2=2x_2-x_1+x_1$ and then apply central limit theorem four times"", but I still have no idea how to do this. Thank you.","['independence', 'central-limit-theorem', 'probability', 'normal-distribution']"
1558231,Prove that $\sum_{n=1}^\infty \sin(n^p)$ diverges for all $p>0$,"Prove that the series $\displaystyle\sum_{n=1}^\infty \sin(n^p)$ diverges for all $p>0$ .
This should be simple but I have been failing. My latest attempt is Cauchy's criterion.","['divergent-series', 'sequences-and-series']"
1558234,What is the derivative of matrix vector product $(A^Tx)$ with respect to A?,"What is the derivative of a vector with respect to a matrix? 
Specifically, $\frac{d(A^Tx)}{dA} = ? $, where $ A \in R^{n \times m}$ and $x \in R^n$.","['matrices', 'matrix-calculus', 'derivatives']"
1558245,On $e^{5x}+e^{4x}+e^{3x}+e^{2x}+e^{x}+1$,"Define the following, $$F_2(x) := \frac{1}{2}+\frac{(2x)}{1!} B_2\Big(\tfrac{1}{2}\Big)+\frac{(2x)^2}{2!}B_3\Big(\tfrac{1}{2}\Big)+\frac{(2x)^3}{3!}B_4\Big(\tfrac{1}{2}\Big)+\dots $$ $$\color{brown}{F_3(x)} := \frac{1}{3}+\frac{(3x)}{1!} B_2\Big(\tfrac{1}{3}\Big)+\frac{(3x)^2}{2!}B_3\Big(\tfrac{1}{3}\Big)+\frac{(3x)^3}{3!}B_4\Big(\tfrac{1}{3}\Big)+\dots $$ $$F_4(x) := \frac{1}{4}+\frac{(4x)}{1!} B_2\Big(\tfrac{1}{4}\Big)+\frac{(4x)^2}{2!}B_3\Big(\tfrac{1}{4}\Big)+\frac{(4x)^3}{3!}B_4\Big(\tfrac{1}{4}\Big)+\dots $$ $$\color{brown}{F_6(x)}  := \frac{1}{6}+\frac{(6x)}{1!} B_2\Big(\tfrac{1}{6}\Big)+\frac{(6x)^2}{2!}B_3\Big(\tfrac{1}{6}\Big)+\frac{(6x)^3}{3!}B_4\Big(\tfrac{1}{6}\Big)+\dots $$ From Summation of Series (2nd ed) by L.B Jolley, page 26, it seems that, $$\color{brown}{F_3(x)} \overset{?}= \frac{1}{1+e^x+e^{2x}}\\
\color{brown}{F_6(x)} \overset{?}= \frac{1}{1+e^{x}+e^{2x}+e^{3x}+e^{4x}+e^{5x}}\tag1$$ I assume that $B_n(x)$ are the Bernoulli polynomials . However, when I try to numerically evaluate those two $F_k(x)$ using Mathematica , the LHS does not agree with the RHS. Questions: How to interpret $B(n)$ such that $(1)$ holds true? And do $F_2(x)$ and $F_4(x)$ evaluate similar to $(1)$?","['power-series', 'bernoulli-numbers', 'exponential-function', 'sequences-and-series']"
1558255,"is there a function f:Râ†’R , differentiable on (a,b) but not on [a,b] ?(f is continuous on [a,b])","is there a function like f:Râ†’R such that :
1) f is continuous on [a,b]
2) f is differentiable on (a,b)
3) f(a)=f(b) but f is not differentiable on [a,b]??? (if the answer is no , then prove that f is also differentiable on [a,b]) this question came up in calculus 1 class and I think that f should be differentiable on [a,b] but can't prove it...
I don't know what is the condition 3 for!!!","['derivatives', 'calculus']"
1558353,When is the quotient ring finite?,"My question is in the title: Given a commutative ring $R$ with identity $1\neq 0$ and an ideal $M$ of $R$. For what condition of $M$, the quotient ring $R/M$ is finite? My question comes from the theorem: if $M$ is a maximal ideal, then $M$ is a prime ideal. In general, the converse is not true. However, we know that if $M$ is a prime ideal then $R/M$ is an integral domain, and if $R/M$ is also finite, then it is a field, and the converse holds. Thank you so much for any help.","['abstract-algebra', 'ring-theory', 'commutative-algebra']"
