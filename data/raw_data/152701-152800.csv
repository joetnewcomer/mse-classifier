question_id,title,body,tags
2562456,"$\int^\infty_0\frac{x^2e^{-x/y}}y\,dx$","I've come across a rather difficult expression to integrate.
$$\int^\infty_0\frac{x^2e^{-x/y}}y\,dx$$
Is there an easy way to solve this? I've solved it manually which involved multiple u-subs and integration by parts, and took up a decent amount of my time. This comes from a probability textbook so I doubt they want me to spend so much time solving an integral. The final answer should be $2y^2$.","['integration', 'definite-integrals']"
2562483,$\mathbb{R}$ is not the field of fractions of a UFD,I need to prove the following. If $D$ is an UFD and if $$\mathbb{R}\cong \operatorname{Frac}(D)$$ Then $\mathbb{R}\cong D$. I have no idea how to prove it. I tried using the fact that the fraction field is a localisation and that localisation is flat.,"['abstract-algebra', 'ring-theory', 'unique-factorization-domains']"
2562501,Equivalence of two definitions of local functors,"All my functors are from commutative rings to sets.  I've seen two different definitions of a local functor.  In one definition we say $X$ is local if whenever $Y$ is a functor with open cover $Y_i$ the standard sequence
$$\mathrm{Mor}(Y, X) \to \prod_i\mathrm{Mor}(Y_i, X) \rightrightarrows \prod_{i, j}\mathrm{Mor}(Y_i \cap Y_j, X)$$
is exact.  I've also seen a functor defined as local if for all algebras $A$ and $x_1, \ldots, x_n \in A$ that generate the unit ideal the sequence
$$X(A) \to \prod_iX(A_{x_i}) \rightrightarrows \prod_{i, j}X(A_{x_ix_j})$$
is exact. In general, my question is why are these two definitions equivalent?  The second definition is what you get when you plug $Y = \hom(A, -)$ and $Y_i = D(x_i) = \hom(A_{x_i}, -)$ into the first definition and use Yoneda, so specifically it's the other direction that's confusing me.  Why does being able to glue maps from affine functors and this special type of open subfunctor imply you can glue maps from any functor?","['category-theory', 'functors', 'representable-functor', 'algebraic-geometry']"
2562542,Convergence of $f_n(x) = \frac{nx}{1+n^2x^2}$,"(Disclaimer: This is a homework question.) $$\text{For }n\text{ in }\mathbb{N}\text{ let }f_{n}:\mathbb{R} \rightarrow \mathbb{R} \text{ be defined by } f_{n}(x)=\frac{nx}{1+n^2x^2}
$$ Show that the sequence $(f_{n})_{n\in\mathbb{N}}$ is convergent. Is this convergence uniform? My attempt: as $n \rightarrow \infty\quad f_{n} \rightarrow \frac{1}{nx}  \implies f_{n} \rightarrow f = \left\{\begin{matrix}
 &0  &x\neq0 \\ 
 &??  &x = 0 
\end{matrix}\right.
$ It seems to me $f$ is discontinous at $x=0$, unless I misunderstood. For uniform convergence it's required that:$$\forall \varepsilon>0\quad \exists K_{\varepsilon}\in\mathbb{N} \quad s.t.\quad \forall n\geq K_{\varepsilon}\\||f_{n}(x)-f(x)||<\varepsilon$$
if we pick $n=k$ and $x_{k} = \frac{1}{k}$ then $$f_{n} = 1\quad \forall n\in\mathbb{N}\\\implies||f_{n}(x)-f(x)|| = ||1-0||=1$$
Does this imply $f_{n}$ is not uniformly convergent?","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'limits']"
2562589,Ratio of parts of an intersected segment in a rectangle,"In rectangle $ABCD$, points $E$ and $F$ lie on sides $BC$ and $CD$ respectively. Point $F$ is the midpoint of $CD$ and $BE=\frac13BC$. Segments $AC$ and $FE$ intersect at point $P$. What is the ratio of $AP$ to $PC$? Express your answer as a common fraction. I'm not sure where to start, do I need to use the fact that $FCE$ is a right triangle, or just the ratios, any help would be appreciated.","['ratio', 'geometry']"
2562597,Solution of $\int\frac{\sqrt {x^3-4}}{x}dx$ : Need Hints,"$$\int\frac{\sqrt {x^3-4}} x \, dx$$ My attempt: $ \displaystyle \int\frac{3x^2\sqrt {x^3-4}}{3x^3}\,dx$ Then, substituting $u=x^3$; $\displaystyle \int\frac{\sqrt {u-4}}{3u} \, du$ $$\int\frac{u-4}{3u\sqrt{u-4}} \, du$$ $$\int\frac{1}{3\sqrt{u-4}}\,du-4\int\frac{1}{3u\sqrt{u-4}}\,du $$ I am having trouble with the 2nd part. And Wolfram Alpha says Can you give me some hints on how to get arctanh function here?","['indefinite-integrals', 'integration', 'calculus']"
2562629,Combinatorics and inequalities,"I've ran across a few combinatoric problems with inequalities in them and I'm curious to know just how to do them or if they aren't different from combinatorics with the equality symbol. For example: How many solutions are there for non-negative integers $a, b, c$, and $d$ such that $a + b + c + d \leq 35$?",['combinatorics']
2562640,Evaluate $\lim_{x\to0}\frac{\ln((1+x)^{1+x})}{x^2}-\frac1x$,"$$L=\lim_{x\to0}\frac{\ln((1+x)^{1+x})}{x^2}-\frac1x$$
I am not sure of my answer. Please help me.
$$L=\lim_{x\to0}\frac{1+x}x\frac{\ln(1+x)}x-\frac1x=\lim_{x\to0}\frac{1+x-1}x=1$$","['calculus', 'limits']"
2562712,Affine open subsets of elliptic curves,Let $A$ be a finite set of closed points of an elliptic curve $E$ (over an algebraically closed field). My question is that is the open subset $U=E\backslash A$ affine? Hints and references are all welcome. Thanks a lot.,['algebraic-geometry']
2562782,Is it possible to compute the integral $\int_{0}^{\infty} \frac{\cos x}{1+x^2} \mathrm{d}x$ using ODE?,"My question is: Is it possible to compute the integral $$\int_{0}^{\infty} \frac{\cos x}{1+x^2} \mathrm{d}x$$ using ODE? My trial: Let 
$$
I(a,b) = \int_{0}^{\infty} e^{-bx}\frac{\cos ax}{1+x^2} \mathrm{d}x
$$
then by Dominant Convergence theorem, $I(a,b)$ is continuous on $[0,2] \times [0,1]$. So we only need to compute $I(1,0)$. Fix any $b\in (0,1]$, we can get the following ODE:
$$
I(a,b)-I^{''}_{aa}(a,b) = \int_{0}^{\infty} e^{-bx}\cos ax \mathrm{d}x=\frac{b}{a^2 + b^2}
$$
I have difficulty to proceed. It seems hard to solve this second order ODE. Or any other method using ODE to compute this? Thank you!","['real-analysis', 'integration', 'ordinary-differential-equations', 'calculus']"
2562813,Probability of 6 die rolls,"If two fair $6$-sided dice are tossed six times, find the probability that
  the sixth sum obtained is not a repetition. The solution given to me is not very helpful in explaining the steps, and my attempt at it is far away from the final answer. This question appears to be different from the standard die questions I am used to. Intuition and explanation would be appreciated. The Solution: $2*\frac{1}{36}(\frac{35}{36})^5+2*\frac{1}{18}(\frac{7}{18})^5+2*\frac{1}{12}(\frac{11}{12})^5+2*\frac{1}{9}(\frac{8}{9})^5+2*\frac{5}{36}(\frac{31}{36})^5+\frac{1}{6}(\frac{5}{6})^5\approx0.5614$","['probability', 'dice']"
2562866,"Does $(Ax)(t) = \int_{0}^{t} x(s) ds$ has eigenvectors for $A : C[0,1] \to C[0,1]$?","The question is as follows: Consider the operator $A : C[0,1] \to C[0,1]$ given by $(Ax)(t) = \int_{0}^{t} x(s) ds.$ Does $A$ has eigenvectors? Also find its spectrum $\sigma(A)?$ $\textbf{Some efforts:}$ If $\lambda \neq 0$ were an eigenvalue of $A$ with an eigenvector $x$, then we have $x(t) = \frac{1}{\lambda} \int_{0}^{t} x(s) ds $. This means that $x$ is absolutly continuous and $x'(t) = \frac{x(t)}{\lambda}$ with initial value $x(0)=0$. This imply that $x(t) = 0$ for $t \in [0,1]$ is the only eigenvector. And since $\sigma(A)$ can not be empty, we have $\sigma(A) = \{ 0 \}$. And zero is not an eigenvalue of $A$. Can you please let me know if I have any misunderstanding and if my calculation is wrong? Thanks!","['functional-analysis', 'real-analysis', 'operator-theory']"
2562937,Hessian of non-linear quadratic form,"I have a quadratic form like 
$$ ( \boldsymbol x - f( \boldsymbol x ) )^T \boldsymbol A ( \boldsymbol x - f( \boldsymbol x ) )$$
 with $f(\cdot): \mathbb{R}^n \rightarrow \mathbb{R}^m$. And I would like to compute the Hessian of it based on Jacobeans and higher order derivatives of $f(\cdot)$. I already found some hints (Section Basic properties) for the product rule, but couldn't figure out how to apply this further to the Hessian. Edit: My current approach:
$z(x) = g(x)h(x)$, where $g(x) = (x-f)^T$ and $h(x)=A(x-f)$
The first derivative is then given by
$$
D[z] = h^TD[g] + gD[h]
$$
When using $u(x):\mathbb{R}^n\rightarrow\mathbb{R}^{m\times p}$, $v(x):\mathbb{R}^n\rightarrow\mathbb{R}^{m\times q}$ from here, page 4, second last equation $$
D[uv] = (v^T\otimes I_m) D[f] + (I_q\otimes u)D[v]
$$
Where in my case we have $m=q=1, p=n$. Applying this to the first derivative above I get
$$
D^2[z] = (D[g])^T D[h^T] + \{ I_n \otimes h^T \} D^2[g] + (D[h])^T D[g] + \{ I_n\otimes g \} D^2[h]\\\\
= \{ I_n- D[f^T] \}^T\{ I_n - D[f] \}^T A^T + \{ I_n \otimes (x-f)^TA^T \} D^2[g] + \{ I_n -D[f] \}^TA^T\{ I_n - D[f^T] \} + \{ I_n \otimes (x-f)^T \} D^2[h]
$$
My questions are now basically? Is that correct?
And how are $D[g]=-D^2[f^T]$ and $D^2[h]=-A\cdot{}D[D[f]^T]$ actually defined?
Is the former like 
$$-\begin{bmatrix} D^2[f_1] \\ \vdots  \\ D^2[f_n] \end{bmatrix}$$ i.e. stacked Hessians of the elements of $f(x)$ ? But what about the latter?","['matrices', 'hessian-matrix', 'derivatives']"
2562992,Real and imaginary parts of a complex function [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Is it always possible to separate the real and the imaginary parts of a complex function ? And why ?
I always did it by calculations, but is there a theorem that says that the division in real and imaginary parts is always possible ?",['complex-analysis']
2563002,"Suppose that $X\sim\operatorname{Exp}(\theta)$, show that no unbiased estimator for $\theta$ exists.","Exercise: Suppose that $X\sim\operatorname{Exp}(\theta)$, show that no unbiased estimator for $\theta$ exists. Hint: use the fact that $X$ is a complete and sufficient statistic for $\theta$. I have the following definitions to work with: Definition 1: An estimator $d(X)$ is unbiased for $\theta$ if $\operatorname{E}_\theta(d(X)) = \theta.$ Definition 2: Statistic $T$ is sufficient for $\theta\in\Omega$ if the conditional distribution of $X$ given $T$ is known (does not depend on $\theta$). Definition 3: Statistic $T$ is complete for $\theta\in\Omega$ if for any Borel function $f$ we have that $\operatorname{E}_\theta f(T) = 0$ for all $P_\theta\in\Omega$ implies that $f(T) = 0$. What I've tried: I'm not sure how to show that no unbiased estimator exists by using completeness or sufficiency, but I think it's possible in fact use a proof by contradiction, as is done here. If $d(X)$ is an unbiased estimator we have that $$\operatorname{E}_\theta d(X) = \int d(x)\theta e^{-\theta x}dx = \theta.$$ Unfortunately I don't know how to proceed from here. I've thought of using sufficiency and completeness to show that the above equation is not possible, but I haven't succeeded. Question: How do I show that there exists no unbiased estimator for $\theta$? Thanks in advance!","['parameter-estimation', 'statistics', 'statistical-inference', 'probability-distributions']"
2563087,Example of multivalued function that attains maximum when values form evenly spaced vector,"I have four variables $x,y,z,w$, such that $x\ge1,w\le7, x\le y\le z\le w$. I need to find a function that attains maximum when $x,y,z,w$ are evenly spaced on $[1,7]$, i.e. $(x,y,z,w)=\left(1,3,5,7\right)$ and minimum when all variables are equal. $(4-(y-x))^2+(4-(z-x))^2+(4-(z-y))^2+(4-(w-x))^2+(4-(w-y))^2+(4-(w-z))^2$ is an example of such function, but depending on values $4-(y-x)$ can be either positive or negative. I need it to be either positive or negative. Perhaps, somebody knows other examples of such functions or relevant literature. Thank you in advance.","['nonlinear-optimization', 'optimization', 'partial-derivative', 'calculus', 'multivariable-calculus']"
2563089,Square of an increasing function over an interval.,"Let $f:I\rightarrow R$ be an increasing function where I is an interval in R .Then (a) $f^2$ is always increasing (b) $f^2$ is always decreasing (c)$f^2$ is constant$\Rightarrow  f$ is constant (d)) $f^2$ may be neither increasing nor decreasing. My try If we take $I:[-1,1]$ and $f(x)=x$ then option (a) and (b) will be wrong. For option (c). Let$ f^2(x)=k$ then $f(x)= \sqrt k  \  \ or -  \sqrt k$. So (c) may be write. But the correct answer is option (a) Can anyone explain.
This question was asked in entrance exam.Please see this image","['calculus', 'functions']"
2563187,If $\mathbb{E}(\sup_n |X_n|)< \infty$ then $(X_n)_n$ is uniformly integrable,"Let $\{X_n: n\ge 1\}$ be a sequence of random variables satisfying $E [ \sup | X_n|] < \infty $. Show that $\{X_n\}$ is uniformly integrable. I know this is a basic question from Financial Mathematics. I am not good in this mathematics so I uploaded this question here. I would like to get some help from anyone to solve this question. 
Thank you","['stochastic-processes', 'probability-theory', 'uniform-integrability']"
2563200,Probability of $\mathcal{O}_{\mathbb{Q}(\alpha)}=\mathbb{Z}[\alpha]$,"Let $f(x)\in \mathbb{Z}[x]$ be a irreducible polynomial of degree $n$ and let $\alpha$ be a root of $f(x)$. One can show that $\mathcal{O}_{\mathbb{Q}(\alpha)}=\mathbb{Z}[\alpha]$ if discriminant of $f(x)$ is square-free, and converse does not hold in general. I want to know the probability of this event, i.e. 
$$
p_{n}=\lim_{N\to\infty} \frac{\#\{f(x):\mathcal{O}_{\mathbb{Q}(\alpha)}=\mathbb{Z}[\alpha],\alpha\text{ is a zero of $f(x)$}\}}{\#\{f(x)\in \mathbb{Z}[x]:f(x)\text{ is monic irreducible polynomial of degree $n$}, h(f)\leq N\}}
$$
where $h(f)=\max\{|a_{0}|, \dots, |a_{n-1}|\}$ for $f(x)=x^{n}+a_{n-1}x^{n-1}+\cdots+a_{0}$. I don't know whether the limit exists or not, but as I explained above, lower bound of the limit is 
$$
q_{n}=\lim_{N\to\infty} \frac{\#\{f(x):disc(f)\text{ is square-free}\}}{\#\{f(x)\in \mathbb{Z}[x]:f(x)\text{ is monic irreducible polynomial of degree $n$}, h(f)\leq N\}}.
$$
For example, in case of $n=2$, we have
$$
q_{2} = \lim_{N\to\infty}\frac{\{a, b\in \mathbb{Z}:|a|, |b|\leq N, a^{2}-4b\text{ is square-free}\}}{\{a, b\in \mathbb{Z}:|a|, |b|\leq N, a^{2}-4b\text{ is not square}\}}
$$ Edit : With computer, I computed $q_{2}$ and N=10 -> 0.6
N=100 -> 0.454497
N=500 -> 0.425595
N=1000 -> 0.419174
N=5000 -> 0.41124 and my computer doesn't work for $N=10000$..","['number-theory', 'algebraic-number-theory']"
2563222,On the tribonacci constant $T$ and $4\arctan\left(\frac1{\sqrt{T^3}}\right)+\arctan\Big(\frac1{\sqrt{(2T+1)^4-1}}\Big)=\frac{\pi}2$,"We have, $$4\arctan\left(\frac1{\sqrt{\phi^3}}\right)\color{red}-\arctan\left(\frac1{\sqrt{\phi^6-1}}\right)=\frac{\pi}2$$ $$4\arctan\left(\frac1{\sqrt{T^3}}\right)\color{blue}+\arctan\left(\frac1{\sqrt{(2T+1)^4-1}}\right)=\frac{\pi}2$$ with golden ratio $\phi$ and tribonacci constant $T$. Note that, $$\begin{aligned}\left(\sqrt{\phi^3}+i\right)^4\left(\sqrt{\phi^6-1}\color{red}-i\right)&=(\phi^3+1)^2\sqrt{-\phi^6}\\ \left(\sqrt{T^3}+i\right)^4\left(\sqrt{(2T+1)^4-1}\color{blue}+i\right)&=(T^3+1)^2\sqrt{-(2T+1)^4}\end{aligned}$$ The first one is from this post , while the second is from a session with Mathematica. However, I'm having a little trouble with the tetranacci constant . Q: What would be analogous formulas using the tetranacci constant (and higher)?","['golden-ratio', 'trigonometry', 'constants', 'pi']"
2563231,How to Simplify $ \frac{\sin(3x)+\sin^3(x)}{\cos(3x)-\cos^3(x)} $,"Simplify $$ \frac{\sin(3x)+\sin^3(x)}{\cos(3x)-\cos^3(x)}.$$
  The solution is : $-\cot(x)$ I tried to: $$\frac{\sin(2x)\cos(x)+\cos(2x)\sin(x)+\sin^3(x)}{\cos(2x)\cos(x)+\sin(2x)\sin(x)-\cos^3(x)}.$$","['algebra-precalculus', 'real-analysis', 'trigonometry', 'calculus']"
2563245,How to calculate the number of graphs without vertices of degree 0 using inclusion-exclusion principle?,"So there is this homework question where we have to determine the number of graphs with no vertices of degree 0 using the inclusion-exclusion principle. With $V = {1, 2, ... n}$ and the answer should be a function of n. I know this is the inclusion-exclusion principle rule and that the total number of graphs possible is $2^{\binom{n}{2}}$. So that would be $|A_1| + |A_2| + ... + |A_n|$. $$\left| \bigcup_{i = 1}^{n} A_i \right| = \sum_{k = 1}^{n} (-1)^{k - 1} \sum_{I \in \binom{\{1, 2, \ldots, n\}~}{k}} \left| \bigcap_{i \in I} A_i\right|$$ But I don't really know what $|A_1 \cap A_2|$ is supposed to mean/be in this example and neither the others. Can someone help me with this?","['inclusion-exclusion', 'graph-theory', 'discrete-mathematics']"
2563282,Higher powers of matrix,"$$
    A = \begin{bmatrix}
    1 & 1 & 13 \\
    5 & 2 & 6 \\
    -2 & -1 & -3 \\
    \end{bmatrix}
$$
Find $A^{14}+3A-2I$. One way is to find $A^2$, then $A^4$, then $A^8$, then $A^{14}$. Another way is using eigen values and diagonal matrix concept. In our university exam, this question is given only for 5 marks. So I am wondering if there is any simple way to do it. Appreciate any hint.",['matrices']
2563284,Sanity check: self-adjoint operator on Sobolev space,"I just wanted to check if the conclusion below is true, and whether the following reasoning works: Let $H^i$ be the Sobolev spaces on a compact manifold $M$ and $D$ a self-adjoint (in the $H^0$-inner product) first-order differential operator on $M$. $D$ is initially defined only on $C_c^\infty(M)$ but can be extended to a bounded operator 
$$H^{i+1}\rightarrow H^i$$ for all $i\geq 0$. When viewed this way, $D$ is self-adjoint, in the sense of unbounded operators, in every $H^i$-inner product. That is, $$\langle Dx,y\rangle_i = \langle x,Dy\rangle_i,$$ for all $x,y\in H^{i+1}\subseteq H^i$. This follows from self-adjointness of $D:H^1\rightarrow H^0$. By the criterion for self-adjointness, we know that $D^2+1:H^2\rightarrow H^0$ is a bounded invertible operator. Conclusion: The bounded operator $$D^2+1:H^{i+2}\rightarrow H^{i}$$ is invertible for every $i\geq 0$. Thanks!","['proof-verification', 'functional-analysis', 'differential-geometry', 'sobolev-spaces', 'analysis']"
2563306,Combinatorial proof of the identity ${{n}\brack {m}}_{q}{{m}\brack {k}}_{q} = {{n}\brack {k}}_{q}{{n-k}\brack {m-k}}_{q}$,"Problem Let $$ [n]_q = \sum_{0 \leq k < n} q^k = \frac{1-q^n}{1-q} $$ and $$[n]_q! = \prod_{0 \leq m < n} \sum_{0 \leq k < m} q^k. $$ Define $$ {{n}\brack{k}}_q = \frac{[n]_q!}{[k]_q![n-k]_q!}. $$ Prove the trinomial revision identity $$ {{n}\brack {m}}_{q}{{m}\brack {k}}_{q} = {{n}\brack {k}}_{q}{{n-k}\brack {m-k}}_{q}, \, n \geq m \geq k \geq 0. $$ My question One of the combinatorial meaning for ${{n}\brack{k}}_q$ is given as follows. Consider the lattice paths $p$ from $(0,0)$ to $(k,n-k)$ that takes only steps East and North. Then ${{n}\brack{k}}_q$ counts all such paths according to the area underneath such paths. So, is there any combinatorial proof of trinomial revision identity using lattice paths?","['combinatorial-proofs', 'elementary-number-theory', 'combinatorics', 'q-analogs', 'discrete-mathematics']"
2563344,Solving the limit $\lim_{x\rightarrow+\infty}(\sqrt[5]{x^5-x^4}-x)$,"Recently, I am struggling to solve the limit: $$\lim_{x\rightarrow+\infty}(\sqrt[5]{x^5-x^4}-x)$$ 
If I try to make some fraction with nominator $-x^4$ and some irrational denominator by multiplying, it becomes more complex. Can anyone help about this with more easier way?","['radicals', 'indeterminate-forms', 'calculus', 'limits']"
2563423,"What do $∞^1$, $∞^2$, or $∞^3$ mean?","In studying the history of Lie theory, I have seen the symbols $∞^1$, $∞^2$, and $∞^3$ (cf. Page's excellent Ordinary Differential Equations (1897), passim ). What exactly do they mean?","['math-history', 'transformation', 'terminology', 'group-theory', 'lie-groups']"
2563446,"On convergence of Bertrand series $\sum\limits_{n=2}^{\infty} \frac{1}{n^{\alpha}\ln^{\beta}(n)}$ where $\alpha, \beta \in \mathbb{R}$","Study the convergence of $$\sum_{n=2}^{\infty}
 \frac{1}{n^{\alpha}\ln^{\beta}(n)}$$ where $\alpha, \beta \in
 \mathbb{R}$ I have proved that: This series diverges when $\alpha \leq 0$ . This series converges when $\alpha > 1, \beta > 0$ This series diverges when $0 < \alpha < 1, \beta > 0$ This series converges when $\alpha = 1, \beta > 1$ Question: What happens when $\alpha > 0$ and $ \beta < 0$ ? There are other questions on MSE which ask about this series, but this question is distinct because I would like an argument which does not rely on the integral test for series convergence, and this question considers all real $\alpha$ and $\beta$ , while other questions ask only about $\alpha, \beta > 0$ , where we can apply Cauchy condensation criterion","['real-analysis', 'limits', 'sequences-and-series', 'calculus', 'convergence-divergence']"
2563453,Prove that for every $w \in \mathbb{C}$ it is true that $z + \sin z = w$ has exactly one root $z \in \mathbb{C}$.,Prove that for every $w \in \mathbb{C}$ it is true that $z + \sin z = w$ has exactly one root $z \in \mathbb{C}$. The idea I have is to use Rouche's theorem.,"['complex-analysis', 'functions']"
2563455,Determinant of triangular matrix,"I understand that you can find the determinant of a matrix along its diagonal if it is in triangular form. For a matrix such as the following $$\begin{pmatrix}
1 & 5 & 0\\ 
2 & 4 & -1\\
0 &-2 & 0 
\end{pmatrix}$$ When put into triangular form, I get $$\begin{pmatrix}
1 & 5 & 0\\
0 & 1 & 1/6\\
0 & 0 & 1/3
\end{pmatrix}$$ Since I multiplied row two by $-\frac16$ during the row reduction, I would expect the determinant to be $$ 1 \cdot 1 \cdot \frac13 \cdot \left(-\frac16\right),$$ but the answer for the determinant of the original matrix is $-2$ . Where exactly am I going wrong?","['matrices', 'linear-algebra', 'determinant']"
2563532,How to prove that $\Gamma$ is finitely generated?,"Let $\pi \colon \tilde{M} \to M$ be the abelian cover, where $M$ is a compact, oriented smooth and finite-dimensional manifold. Let $\alpha \in \Omega^1(M)$ be a closed 1-form such that the kernel of its period homomorphism $$\phi_{[\alpha]} \colon \pi_1(M) \to \mathbb{R}$$ is equal to the group $\pi_\#(\pi_1(\tilde{M}))$. Now we can define the quotient $\Gamma=\pi_1(M)/\ker\phi_{[\alpha]}$ which is isomorphic to the group of deck transformations $\text{Deck}(\tilde{M}).$ Apparently $\Gamma$ turns out to be isomorphic to $\mathbb{Z}^m$ which I tried to prove as follows: The period homomorphism descends to an injective homomorphism $ \tilde{\phi}_{[\alpha]} \colon \Gamma \to \mathbb{R}$. With this it is easy to verify that $\Gamma$ is abelian and torsion-free. By showing that $\Gamma$ is finitely generated one could conclude that $\Gamma \cong \mathbb{Z}^m$ using the Classification of Finitely Generated Abelian Groups, but I was not able to do so: One idea was to somehow use the fact $M$ is a compact manifold and then apply Corollary A.8./A.9. in Hatcher, but my algebra is rusty so I'm not sure if I can recover the needed information on the quotient $\pi_1(M)/\ker\phi_{[\alpha]}$. Or maybe this follows from some ""covering space"" theory. In any case, any help is greatly appreciated! $\textbf{Edit:}$
Thanks to anomaly for the answer. Maybe someone can give an alternate proof that does not invoke seriously involved results like the ones I quoted from Hatcher.","['abstract-algebra', 'covering-spaces', 'group-actions', 'group-theory', 'differential-geometry']"
2563541,"$\operatorname{Ext}^1(M,R/m)=0$ iff $M$ is projective","In this answer it is suggested that over a commutative local ring, a module $M$ is projective iff $\operatorname{Ext}^1_R(M,R/m)=0$. A similar result holds for flatness and Tor. In the case of Tor, I can find a proof in Robert Ash's Commutative Algebra , which involves specific isomorphisms with tensor. But I cannot prove the case of Ext. Can anyone show me the proof? Thank you. FYI, the ""proof"" in the next answer in that link is quite vague to me, so it would be ok if you guys can clear things out here.","['derived-functors', 'projective-module', 'homological-algebra', 'functions']"
2563573,Expected value of max of a Stochastic process,"Ciao all, I'm working on some stochastic processes and I'm stuck on this problem. Let $S_t$ be the stochastic process defined by:
$$
dS_t = \sigma S_t dW_t
$$
with initial data $S_0 \in \mathbb{R}^+$. I'm trying to compute the expected value:
$$
\mathbb{E}\left[ \max_{t \in [0, T]}S_t \right]
$$ but it's not clear how to fight the problem.
For example I cannot use Ito's Lemma since the function is not $C^1$ w.r.t. $S_t$. I've started from a simpler case: $$
\begin{align}
\mathbb{E} [\max(S_t, S_T)] & = \mathbb{E}[S_t]\mathbb{P}(S_t > S_T) + \mathbb{E}[S_T]\mathbb{P}(S_T > S_t)\\
& = S_0 \left(\mathbb{P}(S_T > S_t) + \mathbb{P}(S_t > S_T) \right)
\end{align}
$$
At this point the two terms on the rhs can be computed directly. My idea was to extend this computation for all $t$ but I'm sure I have to take in account in somehow that the expected value at a future time is conditionated by all the past times. Can you help be, even with some ideas?
Thank you,
ciao! AM","['stochastic-processes', 'probability', 'stochastic-calculus', 'stochastic-analysis']"
2563594,How do we show that minimal expected length for confidence interval for normal with unknown mean is the following?,"How do we show that the $(1-\alpha)100\%$ confidence interval $(\bar{Y}-z_{\alpha-\alpha_1} \frac{\sigma}{\sqrt{n}}, \bar{Y}+z_{\alpha_1} \frac{\sigma}{\sqrt{n}})$ for the unknown normal mean $\mu$ has a minimal expected length when it is symmetric, that is, $\alpha_1 =\frac{\alpha}{2}$ Intuitively the middle has the largest percentages. But how do we formally prove it?","['statistics', 'confidence-interval', 'normal-distribution', 'probability-distributions']"
2563600,An exercise with prime numbers,"I am trying to give a solution to the following exercise: Let $f$ be a function from the set of positive integers to itself such that, for every $n$, the number of positive integer divisors of $n$ is equal to $f(f(n))$. For example, $f(f(6)) = 4$ and $f(f(25)) = 3$. Prove that if $p$ is prime, then $f(p)$ is also prime. My try: If $p$ is prime, then $f(f(p)) = 2$ by definition of $f$, which means $f(f(f(p))) = f(2)$. If $f(p)$ is not prime, then either $f(p) = 1$ - i.e. $f(f(f(p))) = 1$ - or $f(2) = f(f(f(p))) > 2$ as at least $1$ and $f(p)$ divide $f(p)$. In the first case we get $f(f(p)) = 2 = f(1)$, which yields $f(2) = 1$. I do not know how to go on from here. Can anybody help me? Thanks.","['prime-numbers', 'functions', 'proof-verification']"
2563609,"For which $u$ does the derivative $f'(u,0)$ of $f(x,y)=|x|+|y|$ exist?","Let $f: \mathbb{R}^2\to \mathbb{R}$ be defined by setting $f(x,y)=|x|+|y|$ (a) For which vectors $u\ne 0$ does $f'(0; u)$ exist? Evaluate it when it
exists. (b) Do $D_1f$ and $D_2f$ exist at $0$? (c) Is $f$ differentiable at $0$? (d) Is $f$ continuous at $0$? I have thought a lot about this problem: For (a), be $u\neq 0, u:=(h,k)$, then $\lim_{t\to 0}\frac{f(0+tu)-f(0)}{t}=\lim_{t\to 0}\frac{f(th,tk)}{t}=\lim_{t\to 0}\frac{|th|+|tk|}{t}=(|h|+|k|)\lim_{t\to 0}\frac{|t|}{t}$ and just as $\lim_{t\to 0}\frac{|t|}{t}$ does not exist, necessarily $h=k=0$, with which no directional derivative exists, this immediately tells us that in (b) neither $D_1f$ and $D_2f$ exist and that $f$ is not differentiable in $(0,0)$. Also clearly $f$ is continuous in $(0,0)$. Is this reasoning correct? Could someone help me by giving me suggestions? Thank you.","['real-analysis', 'calculus', 'multivariable-calculus', 'analysis', 'vector-analysis']"
2563623,Relation between Pascal's triangle and fibonacci series. [duplicate],"This question already has answers here : How to show that this binomial sum satisfies the Fibonacci relation? (6 answers) Fibonacci Numbers Proof: $ f_n = \binom n0 + \binom{n-1}1 +\dots+ \binom{n-k}k$ (3 answers) Closed 6 years ago . Accidentally, I had seen this relation. I tried to find the formula Here is my try:- $$f_0= {{0}\choose{0}}  $$ $$f_1={{1}\choose{0}} $$ $$f_2={{2}\choose{0}}+ {{1}\choose{1}} $$ $$f_3= {{3}\choose{0}}+ {{2}\choose{1}} $$ $$f_4= {{4}\choose{0}}+ {{3}\choose{1}}+{{2}\choose{2}} $$
$$...$$ Generalizing, $$f_n=\sum_{s+r=n,s\ge r}{{s}\choose{r}}, 0\le s,r \le n $$ How to write the correct formula of n-th term. Why this pattern is coming. I could verify using calculation for upto n=10. How to give a rigorous proof?.","['number-theory', 'fibonacci-numbers', 'binomial-coefficients', 'sequences-and-series']"
2563633,Green's functions/fundamental solution to a non-constant coefficients pde,"We already know the relationship between Green's function and solution to elliptic partial differential equation, i.e $$u(y)=\int_{\partial \Omega}u\frac{\partial G}{\partial n} ds+\int_\Omega G\Delta u dx $$ where $n$ is the unit outward normal , $G$ is Green's function on $\Omega$, and $p=\frac{\partial G}{\partial n}$ is called the Poisson kernel. So now I'm wondering the case if we have a partial differential operator $$Lu(t,x)=\partial_tu(t,x)+\sum_{i,j}^da_{i,j}(t,x)\frac{\partial^2u(t,x)}{\partial x_i\partial x_j}$$ 
I know that in this case we call $G$ the fundamental solution if $$LG(s,y;t,x)=\delta(s-t)\delta(x-y)$$ 
Then we can find that $$u(t,x)=\int G(s,y;t,x)Lu(s,y)dsdy$$ without boundary conditions, if my understanding is correct. Are there the similar relationship between fundamental solution and Poisson kernel (if this notion is correct) on the boundary?  In one paper, $$L=\frac{\partial}{\partial t}+\operatorname{div}\big(A(x,t)\nabla_x\big)$$ where $A$ is a $d\times d$  real symmetric matrix, uniformly elliptic on $\Omega=D\times(0,\infty)$. They have the corresponding Poisson kernel $$p(x,t;y,s)=\frac{\partial G}{\partial N(y,s)}(x,t;y,s)$$ where $N(y,s)=A(y,s)n(y)$ with $n(y)$ is the unit inner normal to $\partial D$ at $y$. It confused me for several days. I really want to figure out how it was calculated.","['partial-differential-equations', 'potential-theory', 'elliptic-equations', 'functional-analysis', 'elliptic-operators']"
2563697,the set of all complex numbers constitutes a one-dimensional complex vector space.,"Show that the set of all real numbers, with the usual addition and
multiplication, constitutes a one-dimensional real vector space, and the
set of all complex numbers constitutes a one-dimensional complex
vector space. I know that all properties to be vector space are fulfilled in real and complex but I have difficulty is in the dimension and the base of each vector space respectively. Scalars in the vector space of real numbers are real numbers and likewise with complexes? The basis for both spaces is $\{1\}$ or for the real ones it is $\{1\}$ and for the complexes it is $\{i\}$? How does one prove that these spaces have dimension $1$?
 Thank you very much.","['real-analysis', 'complex-numbers', 'calculus', 'functional-analysis', 'linear-algebra']"
2563699,"$G$ acts as a group of automorphisms on $A$, $\textrm{Spec}(A^G)=G \backslash \textrm{Spec}(A)$ [duplicate]","This question already has an answer here : Action of finite group of automorphisms on Spec A (1 answer) Closed 6 years ago . $\DeclareMathOperator{\Spec}{Spec}$
I'm starting to read about quotients of group schemes and am working through some basic exercises.  This is from Algebraic Geometry and Arithmetic Curves by Qing Liu.  I'm trying to figure out part (a). I want to show that if $\mathfrak P_1, \mathfrak P_2$ are primes of $A$ with $\mathfrak P_1 \cap A^G = \mathfrak P_2 \cap A^G$, then $\mathfrak P_1 = \sigma \mathfrak P_2$ for some $\sigma \in G$. Since $A$ is integral over $R:=A^G$ by part (b), the problem becomes to show that for any prime $\mathfrak p$ of $R$, the group $G$ acts transitively on the primes in $A$ lying over $\mathfrak p$. If we just stick to maximal ideals, I can solve the problem by modifying an argument from basic algebraic number theory: Solution when $\mathfrak p$ is a maximal ideal : Since $R \subseteq A$ is integral, every prime lying over $\mathfrak p$ is also maximal.  Let $\mathfrak P, \mathfrak Q$ be distinct primes of $A$ lying over $\mathfrak p$.  Suppose that $\mathfrak P \neq \sigma \mathfrak P$ for any $\sigma \in G$.  Then $\mathfrak P$ and $\sigma \mathfrak Q$ are comaximal ideals for every $\sigma \in G$, and so by the Chinese remainder theorem there exists a solution $x \in A$ to the system $$x \equiv 0 \pmod{\mathfrak P}$$ $$x \equiv 1 \pmod{\sigma^{-1}\mathfrak Q} : \sigma \in G $$ Then $\sigma(x) - 1 \in \mathfrak Q$ for all $\sigma$, so $\sigma(x)$ is never in $\mathfrak Q$.  Hence neither is $y := \prod\limits_{\sigma \in G} \sigma(x)$.  But $y \in \mathfrak P \cap A^G = \mathfrak p \subseteq \mathfrak Q$, contradiction.  $\blacksquare$ I had an idea of how to reduce to the case where $\mathfrak p$ is maximal.  Let $S = R - \mathfrak p$.  Then the inclusion $R \subseteq A$ induces an injective ring homomorphism $R_{\mathfrak p} = R\otimes_R R_{\mathfrak p} \rightarrow A \otimes_R R_{\mathfrak p} = S^{-1}A$.  By tensoring with $1_{R_{\mathfrak p}}$, we still get an action of $G$ as a group of automorphisms of the ring $S^{-1}A$. If I can show that $R_{\mathfrak p} = (S^{-1}A)^G$, then I will be in the same situation as before, with $\mathfrak p R_{\mathfrak p}$ a maximal ideal. From the diagram $$\begin{array} \textrm{Spec } A & \leftarrow & \Spec S^{-1}A \\ \downarrow & & \downarrow \\ \Spec A^G & \leftarrow  &\Spec (A^G)_{\mathfrak p} \end{array}$$ with horizontal injections and vertical surjections, with the action of $G$ on the top right object being the restriction of the action on the top left, I'll get the result.","['algebraic-geometry', 'commutative-algebra']"
2563704,Taylor series for $\sqrt{1+x^2}$,"I want to expand 
$$f(x)=\sqrt{1+x^2}$$ in powers of $x-2$ I started by getting the maclaurin series
$$\sqrt{1+x}=1+\frac{1}{2}x+\frac{1}{2} \left( \frac{-1}{2} \right) \frac{x^2}{2!} + \frac{1}{2} \left( \frac{-1}{2}\right) \left(\frac{-3}{2}\right)\frac{x^3}{3!}$$
$$\sqrt{1+x^2}=1+\frac{1}{2}x^2+\frac{1}{2}\left(\frac{-1}{2}\right)\frac{x^4}{2!}+\frac{1}{2} \left(\frac{-1}{2}\right)\left(\frac{-3}{2}\right)\frac{x^6}{3!}$$ Then
$$\sqrt{1+x^2}=\sqrt{1+(x-2)^2-4+4x}=\sqrt{(x-2)^2-3+4x}=\sqrt{(x-2)^2-3+4(x-2)+8}$$
I could not complete , what can we do then ? (I know that we can differentiate the function and substitute in the Taylor formula , but I want a shorter way) for example : 
if we want to expand $$g(x)=\frac{1}{1-x}$$ around $x=2$ we can start by \begin{align}
g(x) & =\frac{1}{1-x}=\frac{1}{1-(x-2)-2}=\frac{-1}{1+(x+2)} \\[10pt]
& =-[1-(x-2)+(x-2)^2-(x-2)^3+\cdots]
\end{align} So  I want to get convert $f(x)$ to a form that we can write its expansion without getting derivatives, like I did with $g(x)$ above","['taylor-expansion', 'calculus']"
2563718,"How do I prove $d\omega = \frac{1}{p!}(\partial \omega_{i_1 \ldots i_p}/\partial x^j) \,dx^j \wedge dx^{i_1} \wedge \ldots \wedge dx^{i_p}$?","How do I prove $d\omega = \frac{1}{p!}(\partial \omega_{i_1 \ldots i_p}/\partial x^j) \, dx^j \wedge dx^{i_1} \wedge \ldots \wedge dx^{i_p}$ for $$\omega = \frac{1}{p!} \omega_{i_1 \ldots i_p} dx^{i_1} \wedge \ldots \wedge dx^{i_p} \text{?}$$ I can use the following: a) $d(\alpha + \beta) = d\alpha + d\beta$ b) $d^2 = 0$, c) $df = \frac{\partial f}{\partial x^j} \, dx^j$, d) $d(f\omega) = df \wedge \omega + f \, d\omega$, e) $d(dx^{i_1} \wedge \ldots \wedge dx^{i_p}) = 0$;","['differential-forms', 'differential-geometry', 'exterior-algebra']"
2563758,Are all elements of order $2$ in $\mathrm{Gal}(\overline{\Bbb Q} / \Bbb Q)$ conjugate?,"Let $G$ be the absolute Galois group of $\Bbb Q$. Is it true that any two elements of order $2$ in $G$ are conjugate (in $G$) ? I've seen this question , but the answer only shows that any element of finite order in $G$ has order $≤2$. Apparently, Artin answered positively to my question, but I found no reference for this result. Any help would be appreciated.","['abstract-algebra', 'galois-theory', 'field-theory']"
2563785,$A \rightarrow B$ injective implies $f^{\#}$ an injective morphism of sheaves?,"$\DeclareMathOperator{\Spec}{Spec}$
Had something I was thinking about and wanted to check if I was mistaken anywhere.  Let $Y = \Spec A, X = \Spec B$ be affine schemes, with $\phi: A \rightarrow B$ a ring homomorphism with corresponding morphism $(f,f^{\#}): X \rightarrow Y$. Suppose that $\phi$ is injective.  Then for every prime $\mathfrak p$ of $A$, we get an injective ring homomorphism $A_{\mathfrak p} = A_{\mathfrak p} \otimes_A A \rightarrow A_{\mathfrak p} \otimes_A B = S^{-1}B$, where $S = A - \mathfrak p$. Then $f^{\#}: \mathcal O_Y \rightarrow f_{\ast}\mathcal O_X$ is an injective morphism of sheaves, since if we check at each stalk, we have $$(f_{\ast} \mathcal O_X)_\mathfrak p = \varinjlim_{V \ni \mathfrak p} \mathcal O_X(f^{-1}\mathfrak p) = \varinjlim_{x \in A - \mathfrak p} \mathcal O_X(D_B(x)) = \varinjlim_{x \in S} B_x = S^{-1}B$$ and $f^{\#}_{\mathfrak p}$ is just this ring homomorphism $A_{\mathfrak p} \rightarrow S^{-1}B$, which is injective. So the injectivity of $\phi$ is equivalent to the injectivity of the morphism of sheaves $f^{\#}$?",['algebraic-geometry']
2563838,"Prob. 11, Sec. 23, in Munkres' TOPOLOGY, 2nd ed: If $p\colon X\to Y$ is a quotient map, $p^{-1}(\{y\})$ is connected, and $Y$ is connected, then","Here is Prob. 11, Sec. 23, in the book Topology by James R. Munkres, 2nd edition: Let $p \colon X \to Y$ be a quotient map. Show that if each set $p^{-1}(\{ y \} )$ is connected and if $Y$ is connected, then $X$ is connected. My Attempt: By definition, $p$ is a surjective map of the topological space $X$ onto the topological space $Y$, such that, for each set $V \subset Y$, the set $p^{-1}(V)$ is open in $X$ if and only if $V$ is open in $Y$. Now suppose that $X$ is not connected. Then, by definition, we can write $X = C \cup D$, where the sets $C$ and $D$ are non-empty, disjoint, and open in $X$;   that is, let the sets $C, D$ constitute a separation of $X$. Let $y \in Y$. As the set $p^{-1}(\{ y \})$ is connected, so by Lemma 23.2 in Munkres, we must have either $p^{-1}(\{ y \}) \subset C$ or $p^{-1}(\{ y \}) \subset D$, but not both since $C \cap D = \emptyset$. In particular, for every point $x \in X$, as $p(x) \in Y$, so we must have either $p^{-1}( \{  p(x) \} ) \subset C$ or $p^{-1}( \{  p(x)  \} ) \subset D$, but not both. Am I right? Let $U$ and $V$ be the following subsets of $Y$. 
  $$ U \colon= \{ \ y \in Y \ \colon \ p^{-1}(\{ y \}) \subset C \ \}, \qquad 
V \colon= \{ \ y \in Y \ \colon \ p^{-1}(\{ y \}) \subset D \ \}. $$ 
  Then $U$ and $V$ are disjoint sets, because $C$ and $D$ are disjoint. Am I right? Is the very last assertion in need of further elaboration? Moreover, since $p$ is surjective and since $C \cup D = X$, we can conclude that $Y = U \cup V$. Am I right? If so, then do I need to be any more explicit as to why this is so? We now show that $p^{-1}(U) = C$ and $p^{-1}(V) = D$. Let $x \in p^{-1}(U)$. Then $p(x) \in U$, and so $p^{-1}( \{ p(x) \} ) \subset C$, by the definition of set $U$. But as $x \in p^{-1}( \{ p(x) \} )$, so $x \in C$ also. Thus we have $p^{-1}(U) \subset C$. Is my reasoning correct? Conversely, suppose that $x \in C$. As in the fourth paragraph of this proof, we can conclude that either  $p^{-1}( \{ p(x) \} ) \subset C$ or $p^{-1}( \{ p(x) \} ) \subset D$, but not both. But $x \in C \cap  p^{-1}( \{ p(x) \} )$. So we must have $p^{-1}( \{ p(x) \} ) \subset C$, which implies that $p(x) \in U$ and hence that $x \in p^{-1}(U)$. Thus $C \subset p^{-1}(U)$. Is this part of my reasoning correct and clear enough too? From the preceding two paragraphs, we can conclude that $C = p^{-1}(U)$. Similarly, we can also show that $D = p^{-1}(V)$. Now as $C = p^{-1}(U)$ and $D = p^{-1}(V)$ are open in $X$ and as $p$ is a quotient map, so the sets $U$ and $V$ are open in $Y$. As $p$ is surjective, so we have $$ p(C) = p\left( p^{-1}(U) \right) = U, $$
  and similarly also $p(D) = V$. Now as $C$ and $D$ are assumed to be non-empty and as $p(C) = U$ and $p(D) = V$, so both $U$ and $V$ are non-empty as well. Thus we have shown that $U$ and $V$ are non-empty disjoint open sets in the topological space $Y$ such that $Y = U \cup V$. Thus the sets $U, V$ constitute a separation of $Y$, which contradicts our hypothesis that $Y$ is connected. Thus our supposition that $X$ is not connected is wrong. Hence $X$ is connected. Is this proof correct? If not, then at which point(s) is (are) there problems therein?","['connectedness', 'proof-verification', 'continuity', 'general-topology', 'quotient-spaces']"
2563906,Order Factorization in finite groups,"Let $g$ be an element of a finite group $G$ such that $o(g)$; the order of $g$ ; is of the form $o(g)=abc$ where 
$\gcd(a,b)=\gcd(a,c)=\gcd(b,c)=1$. Is it possible to find three elements $x,y,z$ in $G$ such that $g=xyz$ with $o(x)=a$,   $o(y)=b$ and  $o(z)=c$ ? Clearly this is possible when one of $a,b$ or $c$ equal to 1. But I have no idea in the case when $a, b$ and $c$ are all different to 1.","['finite-groups', 'group-theory']"
2563909,Find Points on a Plane,"I have the equation for a plane $Ax + By + Cz + D = 0$ and I have a known point on the plane $(a, b, c)$. How can I find two more points on the plane? What I need is a generic algorithm that produces two arbitrary points when the plane is not known in advance. Background: I have some CAD software that can create planes based on three points. I have the equation only, so I need to go from equation to points and pass to the CAD software. I don't know in advance what the equation will be so it needs to work for all possible equations. Thanks!","['vectors', 'geometry']"
2563916,"How many ways are there to traverse a 3 by 3 grid such that you start at (0,0) and end at (2,2)?","How many ways are there to traverse an $n$ by $n$ grid such that you start at $(0,0)$ and end at $(n-1,n-1)$ given these conditions: 1)You can traverse each branch at most one time. 2)You can pass through each node at most one time. 3)You can move north, south, east, and west. I'm familiar with a similar question where you can only move north and east, but the question becomes significantly more challenging when you add these other directions. I initially tried doing this using combinatorics and then by induction, both to no avail. I then focused on the 3 by 3 grid ($n=1,2$ are simple) and started to break the 3 by 3 grid down into smaller and smaller grids and counted the ways to get from the new point on the smaller grids to $(2,2)$ and added. I got 2 paths for a 1 by 1 grid, 12 ways for a 2 by 2 grid, and 152 paths for a 3 by 3 grid (not entirely sure if this is correct, but I think the process is a valid one). I was unable to generalize (or validate my solution for $n=1,2,3$) this problem for an $n$ by $n$ grid. How would you do this?","['graph-theory', 'combinatorial-geometry', 'problem-solving', 'combinatorics', 'word-problem']"
2563932,Independence of supremum of random variables,"Consider a probability space  $ (\Omega, \mathcal{F}, \mathbb{P}) $ and a countable set of random variables $ \{X_n \mid n \in \mathbb{N}\} $ such that each random variable is independent of a fixed sub sigma-algebra $ \mathcal{G} \subseteq \mathcal{F}. $ Can one conclude that $ \sup_{n \in \mathbb{N}}X_n $ is independent of $ \mathcal{G} $?","['independence', 'probability-theory']"
2563956,Find all natural numbers $n$ such that $2^n$ divides $3^n -1$ [duplicate],"This question already has an answer here : When does $2^{n}$ divide $3^{n}-1$ (1 answer) Closed 6 years ago . Find all natural numbers $n$ such that $2^n$ divides $3^n -1$ I think that the only solutions are $n = 0,1,2,4$, but I have no idea on how to prove it. I tried to write $3^n-1$ as $1+3+3^2+...+3^{n-1}$ and manipulate the sum but found my self at the equally hard problem of finding the power of two dividing $3^k+1$",['number-theory']
2563987,How to show that $\frac{dy}{dx}=\frac{dy}{d(x-c)}$?,"It seems intuitive to me that $\frac{dy}{dx}=\frac{dy}{d(x-c)}$ (the derivative of $y$ with respect to $(x-c)$, where $c$ is a constant), since subtracting a constant from $x$ doesn't change the slope of $y$, but how can I show it? Thanks in advance.","['derivatives', 'calculus']"
2563995,Is there a reverse triangle inequality equivalent for $L^p$ spaces with $0<p<1$?,"I was trying to follow the proof for the reverse triangle inequality in the reals, but I got an inconsistency. Obviously, something is wrong, but I can't see what. Here is what I did: In $L^p$ spaces with $0<p<1$, we have the reverse Minkowski's inequality: $$||f+g||_p \geq ||f||_p + ||g||_p$$ So the following derivation is true: $||f+g||_p - ||f||_p \geq ||g||_p$ Given that this is true for any $f,g \in L^p$, we can substitute $g$ with $g-f$ to get: $||g||_p - ||f||_p \geq ||g-f||_p \tag{1}\label{1}$ On the other hand, from reverse Minkowski's inequality, we get: $||f+g||_p - ||g||_p \geq ||f||_p$ Substituting $f$ with $f-g$ we get: $||f||_p - ||g||_p \geq ||f-g||_p$ Multiplying by $-1$, we therefore obtain: $||g||_p - ||f||_p \leq -||g-f||_p\tag{2}\label{2}$ Equations \ref{1} and \ref{2} are supposed to be true at the same time, but they are obviously inconsistent. However, I cannot find any mistake in my reasoning. What have I done wrong?","['functional-analysis', 'real-analysis', 'inequality', 'lp-spaces']"
2564008,"$a_n$ is convergent, $b_n$ bounded, prove $\sum a_n b_n$ converges","Suppose that $\sum_{n=0}^\infty a_n$ is a convergent series with $a_n$ > $0$ and suppose that $(b_n){_{n\in\mathbb{N}}}$ is a bounded sequence of positive numbers. Show that $\sum_{n=0}^\infty a_n b_n$ is convergent. Since $b_n$ is bounded and $b_n$ > $0$, can we conclude that there exists M > $0$ such that $0$ < $b_n$ < M or only that $b_n$ > $0$?
If we cannot conclude that $b_n$ < M then how can we answer this question?","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2564018,"Does every sub-lattice of $(2^E, \subseteq)$ have to contain $\emptyset$?","Let $E$ be a finite set, let $2^E$ denote its power set, then it is well-known that $(2^E, \subseteq)$ is not only a poset, but even a poset with the meets equal to intersections and joins equal to unions. From this it follows that every family of open sets defined on $E$ is a sub-lattice, as well as every family of closed sets, since ""arbitrary intersections"" corresponds to ""finite intersections"" since $2^E$ is finite, likewise ""arbitrary intersections"" corresponds to ""finite intersectins"" for the same reason. The question in the title corresponds then to the converse of this fact, namely: Question: For any finite set $E$, does any sub-lattice of $(2^E, \subseteq)$ define a topology? (With the topology defined equivalently via open sets or closed sets -- all finite topological spaces are Alexandrov , so the properties of open sets and closed sets are the same .) Clearly being a sub-lattice implies closure under finite non-empty unions and finite non-empty intersections. But whether a sub-lattice has to be closed under empty unions and empty intersections, hence contain both $\emptyset$ and $E$ and therefore be a topology is what makes my head hurt. Issue 1: One will often read that the third axiom of open sets (or closed sets), that both $\emptyset$ and $E$ have to be open (respectively closed) sets is supposed to be redundant , since closure under finite intersections (respectively finite unions) implies closure under the empty intersection so that $E$ is open (respectively closure under the empty union so that $\emptyset$ is closed). However, this logic seems somewhat flawed, depending specifically on how the closure under finite unions/intersections axiom is stated. For definiteness consider the case of closed sets. One will often write that closed sets are closed under finite unions in the following manner: If $X_1, X_2$ are both closed, then $X_1 \cup X_2$ is also closed. Clearly, by induction , it follows from this that all non-empty finite unions are again closed sets (the union of one set, case $n=1$, following from taking $X_1 = X_2$). But $n=0< 1$ and $n=0<2$; so how can closure under an empty union follow by induction from finite unions for $n=1,2$ as base cases? However, if one simply wrote ""Closed sets are closed under finite unions"", then it would be clear to me that the empty union should be included, because that is clearly a finite union. Issue 2: The flats of a matroid with ground set $E$ always form a sub-lattice of $(2^E, \subseteq)$. So if this converse were true, it would mean that every family of flats forms a topology on the ground set. However, in general matroid axiomatizations and topological space axiomatizations are supposed to be inequivalent. This answer gives an example of how the matroid analog of topological closed sets, flats, need not be closed under unions. The Kuratowski closure axioms are also evidently different from the matroid closure axioms. Likewise the cyclic set axioms for matroids guarantee closure under finite unions, but not under finite intersections. The hyperplane axioms are similar but not the same as the closed base axioms , and the circuit axioms are similar but not the same as the open base axioms . Finally, Oxley, in Matroid Theory , only states that the least element of the flat lattice is $cl(\emptyset)$, which seems to imply that the (matroid) closure of the empty set is not always the empty set, hence $\emptyset$ is not always a flat thus not always the least element of the flat sub-lattice of $(2^E, \subseteq)$. In contrast, $cl(\emptyset)=\emptyset$ always for the topological (Kuratowski) closure, since the empty set is always closed, either per fiat or due to closure under finite unions. The lattice of flats is not a sub-lattice of $(2^E, \subseteq)$, since the join operation is different. The meet operation is the same though. Also the claim in issue one corresponds to one of the flat axioms being ""$E$ is a flat"" even though the second flat axiom is ""$F_1, F_2$ flats implies $F_1 \cap F_2$ are flats"". I.e. the second axiom doesn't seem to imply closure under empty intersection, hence not closure under all finite intersections. Issue 3: 2 : The footnote 6 at the bottom of page 10 here says that [emphasis mine]: A lattice is a partial order in which any finite non-empty set has an infimum and supremum. Equivalently , any two elements
  $x$ and $y$ have a supremum $ x \lor y$, and an infimum $ x \land y$. In particular, based on this statement it does not seem like any empty set has to have an infimum (it can, but it doesn't have to), which for $(2^E, \subseteq)$ would seem to correspond to sub-lattices not needing to be closed under empty intersections. Likewise, it does not seem like any empty set has to have a supremum, in particular it does not seem like sub-lattices of $(2^E, \subseteq)$ need to contain $\emptyset$. Moreover, the statement that ""the fact that the supremum and infimum only have to exist for non-empty finite sets in the lattice is equivalent to any two elements being closed under meet or join"" seems to correspond to my claim that the statement [$X_1, X_2$ closed $\implies$ $X_1 \cup X_2$ closed] can't use induction to prove closure under empty unions. Issue 4 3: The answers to these questions (1) (2) seem to contradict the statements made in the footnote of the paper referenced above.","['axioms', 'lattice-orders', 'elementary-set-theory', 'matroids', 'discrete-mathematics']"
2564026,Asymptotic length of reduced word on free group with replacements,"This seems to be an elementary question, but it's proving hard for me to just Google. Suppose you have a sequence which picks elements out of $\{a, a^{-1}, b, b^{-1}, c, c^{-1}\}$ with equal probability. After, say, seven steps you'll get words like $a b c c^{-1} a c b^{-1} $ which in this case reduces to $a b a c  b^{-1} $ after canceling inverses. My question is this: if I write $c$ as some string of $a, a^{-1}, b, b^{-1}$, (and write $c^{-1}$ as the inverse of that) are there any theorems about the expected length of the new sequence after reduction? For example, I could take $c = a^{-1} b b$ so my previous word becomes just $ab b$. I care mostly about the case of a very long word.","['reference-request', 'free-groups', 'probability', 'combinatorics-on-words', 'group-theory']"
2564042,"Prove that if $A$ has a maximum, then the maximum must be the supremum","Prove that given $A\subseteq\mathbb{R}$, if has a maximum $a_0$, then $a_0$ must be the supremum. In order to prove this, I have written the following: Let $A\subseteq \mathbb{R}$, nonempty set because $a_{0}$ $\in A$ for hypotesis. Then, since $a_{0}$ $\in A$ is the largest element of the set $A$   (i.e. for each element $A$ of A we have that $ a\leq a_{0}$) $A$ is bounded above. Then, A has a supremum. Let $s$ be the least upper bound of $A,$ so we have that for each element $a$ of $A$, $a\leq s$, for being $s$ an upper bound. In particular, $a_0\leq s$, which implies that $s=a_0$. Then, we have that $\sup(A)=\max(A)=a_0$, which is exactly what we wanted to prove. Observation: this proof is true if $s \in A$ I'm not sure if my proof is correct. I will be thankful if someone tells me.","['real-analysis', 'supremum-and-infimum', 'analysis', 'proof-verification']"
2564093,Left-multiplication of a finite field element - Matrix representation,"Can someone explain me why and how a left multiplication of an element of a finite field GF(2^k) can be seen as a linear transformation on GF(2^k) over GF(2)? 
I read this https://www.maa.org/sites/default/files/Wardlaw47052.pdf but it is not clear to me. Thank you!","['finite-fields', 'abstract-algebra', 'linear-algebra', 'linear-transformations']"
2564139,Is the Space $C^\alpha$ separable?,"Let $C^\alpha$be the space of continuous functions f(x) on [0,1]. Such that $ sup \frac{|f(x_1)-f(x_2|}{|x_1-x_2|^\alpha}$  ( $ 0\le x_1 \le x_2 \le 1$. Introduce in this space the norm $||f||_C^\alpha = |f(0)|$ + $ sup \frac{|f(x_1)-f(x_2|}{|x_1-x_2|^\alpha}$  ( $ 0\le x_1 \le x_2 \le 1$. I need to know is it separable or not and why ?
I think it's not separable but don't know why ?","['functional-analysis', 'holder-spaces']"
2564150,Create odd function from arbitrary function,"I have a product of two arbitrary functions $f$ and $g$,
$$y(x) = f(x)g(x)$$
and I want to make the product $y$ odd. I know $f$, e.g. it's a typical Lorentzian function
$$f(x) = \dfrac{b}{(x-a)+b^2},$$
but I want to deduce $g$. What strategy can I use to find $g$? Context: I want to start by assuming that these functions are well behaved, defined in real space, and differentiable everywhere. How would I find $g$ so that the product $fg$ becomes odd? Then my goal is to make $f$ more complex (e.g. multiplying the Lorentzian above with the discontinuous Bose-Einstein distribution , for example) then see if it's still possible to make $fg$ an odd function. This reference summarises odd/even functions but I didn't find it too helpful for my problem.","['algebra-precalculus', 'calculus', 'functions']"
2564158,Use Viete's relations to prove the roots of the equation $x^3+ax+b=0$ satisfy $(x_1-x_2)^2(x_1-x_3)^2(x_2-x_3)^2=-4a^3-27b^2$,"Use Viete's relations to prove that the roots $x_1$, $x_2$, and $x_3$ of the equation $x^3+ax+b=0$ satisfy the identity $(x_1-x_2)^2(x_1-x_3)^2(x_2-x_3)^2=-4a^3-27b^2$. I know that viete's relations state that the roots $x_1$, $x_2$, and $x_3$ of the equation $x^3-px^2+qx-r=0$ have the property $p=x_1+x_2+x_3$, $q=x_1x_2+x_1x_3+x_2x_3$ and $r=x_1x_2x_3$. My question is whether or not there is a way to do this without multiplying out $(x_1-x_2)^2(x_1-x_3)^2(x_2-x_3)^2$ and showing that it factors into $4(x_1+x_2+x_3)^3+27(x_1x_2x_3)$ because that algebra involved in that looks like it will be nasty.","['algebra-precalculus', 'number-theory', 'elliptic-curves']"
2564236,How can I formulate the 3-SAT problem as a 0-1 Linear integer program?,"I understand the 3-Sat problem but I do not understand 0-1 Linear Integer Program. I know in a linear integer program I would have an indicator variable $X_i$ that indicates whether a clause is true or not, and I want to maximize this? Can someone show me how to represent this problem as a 0-1 Linear Integer program?","['linear-programming', 'discrete-mathematics', 'algorithms', 'linear-approximation', 'approximation']"
2564251,"Integrate $\int \frac{\sin x \cos x}{\sin^4x + \cos^4x} \,dx$","Integrate $$\int \frac{\sin x \cos x}{\sin^4x + \cos^4x}dx$$ I solved the question by using the identity $\cos^4(x)+\sin^4(x) = \frac{1}{4}(\cos4x+3)$ and the substitution $u=\cos4x +3$, which turned it into a relatively familiar integral (see my answer below). However, I'm pretty sure there are easier ways I am missing, so please feel free to post alternative answers. There is a similar question here . Problem Source: James Stewart Calculus, 6E","['indefinite-integrals', 'integration', 'trigonometry', 'calculus']"
2564336,How to calculate the series $\sum_{n=0}^{\infty}\frac{1}{1+\left(\frac{b+2bn}{a}\right)^{2}}$?,"First of all consider \begin{align}
\displaystyle \int_{0}^{\infty}\frac{\sin(ax)}{\sinh(bx)}dx & = \int_{0}^{\infty}2\frac{\sin(ax)}{e^{bx}-e^{-bx}} \\
& = 2\int_{0}^{\infty}\frac{\sin(ax)e^{-bx}}{1-e^{-2bx}} \\
& = 2\int_{0}^{\infty}\sin(ax)\sum_{n=0}^{\infty}e^{-(bx+2bnx)} \\
& = 2\sum_{n = 0}^{\infty} \int_{0}^{\infty}\sin(ax)e^{-x(b+2bn)} dx \\
& = 2\sum_{n=0}^{\infty}\frac{a}{a^{2}+(b+2bn)^{2}} \\
& = \frac{2}{a}\sum_{n=0}^{\infty}\frac{1}{1+\left(\frac{b+2bn}{a}\right)^{2}}.
\end{align} Now, how can we calculate this sum? My teacher said me about Poisson's method? But I haven't seen it yet. Any ideas?","['real-analysis', 'integration', 'sequences-and-series']"
2564339,Meaning of this exclamation mark?,"In section 3 of the paper https://www.sciencedirect.com/science/article/pii/S0723086907000151 The author constructs a fiber bundle $(\rho_n)\zeta$ by taking the pullback of the diagram $S^8\xrightarrow{\rho_n} S^8$ with $\zeta$ above the rightmost $S^8$ (sorry dont know who to draw it online).    We're treating the sphere's as the one point compactifications of the octonions.  Here the map $\rho_n$ is just the map sending an octonion to it's $n$th power. In the next paragraph the author starts talking about a vector bundle $(\rho_n)^!\zeta$.  It doesn't seem to be explained what this exclamation notation means, and I haven't seen it before.  Any ideas? EDIT/UPDATE:  I'm tempted to believe that the exclamation is meant to indicate the isomorphism class of $(\rho_n)\zeta$ in $KO(S^8)$, however if that were the case, wouldn't the author also be using the notation $n^!\zeta$ to talk about the class of $n\zeta$ in $KO(S^8)$?  Instead they just use $n\zeta$... EDIT #2:  I'm nearly convinced that there is a typo and the initial definition of $(\rho_n)\zeta$ should have included an exclamation.  I have attempted to contact the author and will update when and if he responds. The professor who passed this paper on to me is currently out of the country, but when he returns I will also ask him if I don't have answer yet, and share his take.","['k-theory', 'octonions', 'fiber-bundles', 'differential-geometry']"
2564407,Lower bound for binomial tail probability conditional on composite parameter space,"Suppose that $X\sim \text{Bin}(n,\theta)$ is a binomial random variable and we are interested in the quantity
$$
P(X>c\mid\theta\leq\theta_0)
$$
where $\theta_0\in(0,1)$ is a fixed, known quantity. Is it possible to find a lower bound for this probability? So far, I can express the following:
\begin{eqnarray*}
P(X>c\mid\theta\leq\theta_0)&=&\frac{P(X>c,\theta\leq\theta_0)}{P(\theta\leq\theta_0)}\\
&=&\frac{1}{\Theta(\theta_0)}\int_0^{\theta_0}P(X>c\mid t)\, d\Theta(t)
\end{eqnarray*}
where $\Theta(t)=P(\theta\leq t)$ is some unspecified weight function (distribution) on $\theta$. From this post I can bound the integrand further from below, assuming that $c/n>t$:
$$
P(X>c\mid t)\geq \frac{1}{\sqrt{2n}} \exp\left(-c\log\frac{c}{nt}\right)
$$ This is where I get stuck. I can't seem to bound this further, preferably with something that depends on $\theta_0$. Are there any references to bound this type of quantity? EDIT One possible idea is to note that $\theta_0$ is the supremum of $\theta$. So, for some $d_n\downarrow 0$, we can consider the set $\mathcal{B}=\{\theta\in(0,1):\theta>\theta_0-d_n\}$. Then
\begin{eqnarray*}
P(X>c\mid\theta\leq\theta_0)&=&\frac{1}{\Theta(\theta_0)}\int_0^{\theta_0}P(X>c\mid t)\, d\Theta(t)\\
&\geq&\frac{1}{\Theta(\theta_0)}\int_{t\in(0,\theta_0)\cap\mathcal{B}}P(X>c\mid t)\, d\Theta(t)\\
&>&\frac{\Theta(\{\theta\in (0,\theta_0)\cap\mathcal{B} \})}{\Theta(\theta_0)}P(X>c\mid \theta_0-d_n)\\
&\geq& \frac{1}{\sqrt{2n}} \exp\left(-c\log\frac{c}{n(\theta_0-d_n)}\right)\frac{\Theta(\{\theta\in (0,\theta_0)\cap\mathcal{B} \})}{\Theta(\theta_0)}
\end{eqnarray*}
where the second inequality follows from the fact that $P(X>c\mid \theta)$ increases with $\theta$, so that $P(X>c\mid \theta)>P(X>c\mid\theta_0-d_n)$ on $\mathcal{B}$. Since $\Theta(\theta_0)\leq 1$, I can also remove the quantity from the denominator above and get a smaller bound.","['statistics', 'binomial-distribution', 'probability']"
2564438,Terms for 3 aspects of a function,"If, for some function $f$ and some value $p\ne 0$, $$\forall x, f(x)=f(x-p),$$ then $f$ is periodic and $p$, if $>0$ and minimal, is $f$'s period . If, instead, for some values $u\ne 0$ and $v$,
$$\forall x, f(x)=f(x-u)+v,$$
then what is the correct adjective to describe $f$, and what are the correct terms for $u$ and $v$ in relation to this $f$? The domain is a subset of $\mathbb{R}$. The codomain might, but need not, be a subset of $\mathbb{R}$; just any group. I have some terminology in mind, but have come to doubt my term for $v$ because (w.r.t. functions) that term is used to denote something else. So I seek other opinions. In the fullness of time I will edit this OP to state the terms I use at the moment, but I want to avoid the situation where people answer ""yes, that's OK"" (which doesn't me any further) and others are put off answering because this question looks to them as if it has a satisfactory answer (which doesn't get future readers any further than I am). EDIT: A staircase function is piecewise constant on intervals of length $u$, so I feel that dxiv's suggestion of ""staircase-like function"" suggests that the function might be something like that -- that is not my intention. Anyway, the terminology I already knew is that from combinatorial game theory: ""arithmetico-periodic"", ""period"" and ""saltus"". ""Period"" is of course problematic because of the more specific sense, which is well established. And ""saltus"" is problematic because of its sense ""jump discontinuity"".","['terminology', 'functions']"
2564463,A vector bundle which has an orientation-reversing isomorphism has a subbundle of rank $1$?,"Let $E$ be a smooth real vector bundle of even rank, over a smooth manifold $M$. Suppose there exist an orientation-reversing vector bundle isomorphism $\Phi:E \to E$. Is it true that $E$ has a subbundle of rank $1$? (We don't need an orientation on $E$, since for maps from a vector space to itself, the notion of orientation-preserving or reversing is always well-defined, without the need to actually choose an orientation on the space). Here is one approach (which was suggested to me by Amitai Yuval , who also raised this question): We can put a metric on $E$, and take the orthogonal polar factor $Q$ of $\Phi$, which will now be an isometric orientation-reversing isomorphism. So, $Q$ will have at least one non-zero fixed point at each fiber $E_x$ (see below**). My hope is that somehow we can extract a continuously changing family of fixed points along the different fibers (one at each fiber) that will form a subbundle of rank $1$. Of course, there can be problems of multiplicity, so perhaps some perturbation argument is needed. Can this approach work? **Here we use the fact $\text{rank}(E)$ is even: at each fiber $Q_x$ is essentially an orthogonal matrix with negative determinant. Since its complex eigenvalues comes in conjugate pairs, and the determinant is real negative, there must be some real negative eigenvalues. Since $\det Q_x<0$ the number of the negative eigenvalues must be odd. Since $\dim E_x$ is even, we conclude there must be a positive eigenvalue, which must be $1$, since $Q_x$ is orthogonal. Motivation: I am trying to find out which real vector bundles admit orientation-reversing isomorphisms. Of course, every bundle of odd rank admits one: the map $x \to -x$. Now suppose the rank is even . If $E$ admits a subbundle $F$ of rank $1$, we can define an orientation-reversing isomorphism as follows: $$\Phi|_F=\text{Id}_F,\Phi|_{{F}^{\perp}}=-\text{Id}|_{{F}^{\perp}}$$ where ${F}^{\perp}$ is some complement of $F$. My question is if this condition (""there exist a subbundle of rank $1$"") is necessary.","['orientation', 'vector-bundles', 'differential-geometry', 'differential-topology']"
2564489,Cohomology of constant sheaf Z on circle S1,"Consider sheaf cohomology defined by derived functor, which means use injective resolutions of sheaves to define cohomology group. Now, if Z is the constant sheaf of group of integers on unit circle S1 with its usual topology. How to computer the first cohomology group of Z on S1?",['algebraic-geometry']
2564493,Is there a way to solve for the missing angle?,"I was working on this problem. I tried to draw ${AC}$, ${BD}$ as isosceles triangle and divide into cases to find the missing angle $??$, but I got stuck. Can someone help me please or give me a clue?","['polygons', 'triangles', 'geometry']"
2564573,Is this a good way to understand the uncountability of uncomputable numbers (and the countability of the computables)?,"My feeling is the computable numbers are countable because they are associated with a finite string of symbols that give an algorithm for computing the number to arbitrary precision. The set of all finite strings in a given language is obviously countable as it is the countable union of finite sets. Because $\mathbb R\setminus\textrm{Computable}=\textrm{Uncomputable}$, the uncomputables are uncountable.","['computability', 'elementary-set-theory', 'proof-verification']"
2564598,How to prove Fermat varieties in characteristic $p$ are unirational?,"In  Shioda's famous paper ""An Example of Unirational Surfaces in Characteristic $p$"", the author proved that Fermat surface over $k$ with characteristic $p$
$$
x_1^n +x_2^n + x_3^n+x_4^n =0
$$
is unirational if $ q \equiv -1 (\text{mod } n)$ for some $p$-power $q$ At end of this paper, he mentioned that he proved it in high dimension case with similar method, i.e
$$ x_1^n +x_2^n + \cdots +x_{m+1}^n =0$$
is unirational if $m \geq 3$ is odd and $q \equiv -1 (\text{mod } n)$ for some $p$-power $q$.  But I can't prove it with his method in surface case. Is there any one know how to prove it?",['algebraic-geometry']
2564612,Venn diagram with rectangles for $n > 3$,"Is it possible to draw a Venn diagram with rectangles for $n > 3$? If yes, is it possible for all $n\in\mathbb{N}$? If no, up to which $n$ is it possible? Furthermore, the rectangles should have the following properties same width and height axis-aligned (not rotated)","['combinatorics', 'combinatorial-geometry', 'rectangles', 'discrete-mathematics']"
2564626,Convergence of sum of expectations,"Let $X_1,X_2,...$ be independent random variables with finite expectation.
If $\sum_{i=0}^\infty Var(X_i) < \infty$, Show that $\sum_{i=0}^\infty(X_i-E[X_i])$ converges almost surely. $E[X_i]$ means a expectation of $X_i$ How to solve it? Use Weak Law of large numbers?","['probability-limit-theorems', 'law-of-large-numbers', 'probability-theory', 'convergence-divergence', 'sequences-and-series']"
2564628,boolean algebra simplify using kmap,"question: this is standar sop,try to simplify it using kmap ----------  --------- 
        AB\CD    00    01    11   10 
         00       0     0    1     1
         01       0     0    0     0  
         11       1     1    1     1  
         10       0     0    1     1 this is the right kmap. the  answer is $ab+b'c$ this is the wrong one. suppose I did it like this. can I simplify this to get the right kmap? =$ab+ac+a'b'c$ =$c.!(a+a'b')+ab$ -> applied demorgan, am I right? =$c(a'.(a+b))+ab$ =$c(a'a+a'b)+ab$ =$a'bc+ab$ =$b(a'c+a)$ ->should I apply demorgan again? , but after I apply it, I didn't get
=$ab+b'c$ is my assumption true? Boolean algebra : $AB+A'B+AB'$ =A(B+B')+B(A+A') -> why $A'B$ changed to B(A+A') , what law is this?","['boolean-algebra', 'computer-science', 'logic', 'discrete-mathematics']"
2564629,Justification of truth table of conditional statement if $p$ then $q$,"I have gone through various sites but i can't understand the justification for the truth table of ""If $p$ then $q$"". Is it accepted by the mathematicians without any proof or justification?
\begin{array}{c|c|c}
p & q & p\rightarrow q \\ \hline
T & T & T \\
T & F & F \\
F & T & T \\
F & F & T 
\end{array}
Last two values of truth table seems a bit confusing how is true?","['boolean-algebra', 'discrete-mathematics']"
2564657,Mathematical induction proof that $f(n)=\frac{1}{2}+\frac{3}{2}(-1)^n$,"The function $f(n)$ for $n=0,1...$ has the recursive definition $$f(n)= \begin{cases} 2 & \text {for n=0} \\ -f(n-1)+1 & \text{for n=1,2...} \end{cases}$$
  Prove by induction that the following equation holds: $$f(n)=\frac{1}{2}+\frac{3}{2}(-1)^n$$ So, I begin by checking that the basic step holds $f(0)=\frac{1}{2}+\frac{3}{2}(-1)^0=2$ OK Assume that the equation holds for a given $n$ Show that n+1 holds: $f(n+1)=\frac{1}{2}+\frac{3}{2}(-1)^{n+1} \Rightarrow f(n+1)=\frac{1}{2}+\frac{3}{2}(-1)^{n} \cdot (-1) = -f(n)-\frac{1}{2}$ I get kind of stuck here. Any advice on how I should approach this?","['induction', 'discrete-mathematics']"
2564710,"How to prove that: $\lim_{n\to\infty}\frac1{n!}\int_0^ne^{-t}t^n\,dt.=\frac{1}{2}$","I want to Show that $$\lim_{n\to\infty}\frac1{n!}\int_0^ne^{-t}t^n\,dt=\frac12.$$ I tried to squeeze the sequence as follows $$\frac{n^{n}(1-e^{-n})}{n!}= \frac1{n!}\int_0^ne^{-t}n^n\,dt\ge \frac1{n!}\int_0^ne^{-t}t^n\,dt \ge \frac1{n!}\int_0^ne^{-n}t^n\,dt =\frac{n^{n+1}e^{-n}}{(n+1)!} .$$ Then what next? any idea?","['real-analysis', 'limits', 'calculus', 'integration', 'sequences-and-series']"
2564720,Prove that a triangle is right triangle,"I'd like to know if there's any theorem to prove that the triangle ACB' is a right triangle and that the angle ACB' is 90°. We know that ACB and A'B'C'  are right triangles, so in my opinion ACB' is also a right triangle, but I don't know how to prove it If there's any theorem or explanation please let me know.","['angle', 'triangles', 'geometry']"
2564721,Does anyone have a nice formula for repeatedly differentiating a composite?,"If $$u\colon \mathbb{R}^n \to \mathbb{R},$$ $$f\colon \mathbb{R} \to \mathbb{R}$$ $$x=(x_1,\ldots,x_n)\in \mathbb{R}^n, \nabla=(\partial_{x_1},\ldots,\partial_{x_n})$$ is there a nice formula for
$$\nabla^k(f(u(x)))?$$
This seems to be something I have to work with a lot...","['multivariable-calculus', 'calculus']"
2564756,Existence a subset of $\Bbb R$ such that $A\cap A^\prime=\emptyset$ with $\overline{A}$ uncountable,"Is there a subset $A$ of $\Bbb R$ such that $A\cap A^\prime=\emptyset$ and  $\overline{A}$ uncountable? My Attempt: $$A\cap A^\prime=\emptyset\quad \Rightarrow \quad \overline{A}=A\cup A'=(A\cap A^\prime)\cup A=A$$
so $A$ is closed and  $A'\subseteq A$ thus $A'=A\cap A^\prime=\emptyset$.","['general-topology', 'real-analysis']"
2564773,Problem on limit of sequences,"I started solving the question by taking  $b_{n}=\frac{(1^{1^{^{p}}}2^{2^{p}}...(n+1){^{(n+1)^{p}}})^{\frac{1}{(n+1)^{p+1}}}}{  (1^{1^{^{p}}}2^{2^{p}}...(n){^{(n)^{p}}})^{\frac{1}{(n)^{p+1}}}}$
 where  $\lim _{n \to \infty }b_{n}=e^{\frac{-2}{(p+1)^{2}}}$ and $lim_{n \to \infty } x_{n}=(1^{1^{^{p}}}2^{2^{p}}...(n){^{(n)^{p}}})^{\frac{1}{(n)^{p+1}}}(b_{n}-1)$ . But I don't know how to solve it further can someone please  help  me with a hint or suggest some other method for solving this question","['real-analysis', 'sequences-and-series', 'limits']"
2564798,Existence of continuous function $f$ on $\Bbb R$ which vanishes exactly on $A\subset \Bbb R$,"Question For any closed subset $A \subset \Bbb R$, does there exist a continuous function $f$ on $\Bbb R$ which vanishes exactly on $A$? If we take $A=[a,\infty)$ or $(-\infty,a]$ or $\{a_1,a_2,\cdots,a_n\}$ or $\bigcup_{i=1}^{k} [a_i,b_i]$, then indeed we can have such a continuous function. But when I thought about cantor set or the set $\{\frac 1n : n \in \Bbb N\} \cup \{0\}$, which are closed in $\Bbb R$, I couldn't think of a continuous map vanishing exactly at these two sets. How should I go about this problem?","['real-analysis', 'continuity', 'general-topology', 'metric-spaces', 'analysis']"
2564868,Question on linear independence of particular vectors of $\mathbb{R}^8$,"Given any four points $z_1,z_2,z_3,z_4\in \mathbb{C}$, all different from zero, consider the following seven vectors of $\mathbb{C}^4$:
$$v_1:=(0,z_2-0,0,0)$$
$$v_2:=(0,0,z_3-0,0)$$
$$v_3:=(z_1-z_2,z_2-z_1,0,0)$$
$$v_4:=(0,z_2-z_4,0,z_4-z_2)$$
$$v_5:=(0,0,z_3-z_4,z_4-z_3)$$
$$v_6:=(z_1-z_3,0,z_3-z_1,0)$$
$$v_7:=(z_1-z_4,0,0,z_4-z_1)$$
Under what conditions on $z_1,z_2,z_3,z_4$ are these seven vectors $v_1,\dots,v_7$ linearly independent as vectors of $\mathbb{R}^8$? Of course I know how to consider the vectors $v_i$ as vectors of $\mathbb{R}^8$. The difficult part (which is the aim of the question) is to deduce all sufficient conditions for their linear independence. Edit: User MvG posted an answer which involves the use of a computer algebra system, but I am actually interested in a method which could be solved by hand (without a computer).","['linear-algebra', 'algebraic-geometry', 'geometry']"
2564880,Making conclusions from data with mixed up labels?,"We have a question which asks something strange, something which seems like it would prevent any conclusions being made from the data. The question is as follows: The subjects of a study came from a genetically pure strain of rats. From each litter, one rat was selected at random for the treatment and one for the control group. Each animal in the treatment group lived with 11 others in a large cage, furnished with playthings which were changed daily. Animals in the control group lived in isolation, with no toys. After a month, the experimental animals were killed and their cortex weights (in milligrams) were recorded. We are then given a table of data - one column with the treatment group and one with the control group and several rows of recorded weights. The first two questions involve finding if there is an association between the treatment and cortex weights, they were no problem. However, the third question is as follows: After doing this analysis you are told that the lab assistant who recorded the weights had mixed up the labels that identified the pairs of siblings. Test whether treatment affects the cortex weight of rats using appropriate graphical methods and a parametric statistical test. Check that the assumptions of the test are satisfied. I cannot think of how we can use the data if the labels of the rats have been mixed up. It is worded as if the labels aren't simply swapped but rather the results are mixed. Surely if they were mixed up then the data is jumbled together and we cannot make any meaningful conclusions as we cannot separate the control and treatment? I would really appreciate if anyone could shed some light on what this question is expecting. All the best!",['statistics']
2564928,Prove $||T^n||=||T||^n$ if $T\in B(H)$ is self-adjoint,"Let $H$ be a complex Hilbert space and let $T\in B(H)$ be self-adjoint. I have already proven that $||T^{2^k}||=||T||^{2^k}$ for $k=0,1,2,\dots$. Now I have to prove the more general statement that if $n\geq 1$, $||T^n||=||T||^n$. I can write $T^{2^k}=T^n T^{2^k-n}$ for $1\leq n<2^k$. But how do I argue from there?","['functional-analysis', 'adjoint-operators']"
2564969,What is needed to specify a group?,"I have come across several groups, some of which have the same number of generating elements and of the same orders. Take, for instance, $D_{2n}$ and $S_n$. I have never seen it read explicitly, but it seems to me that many groups have some number of generating elements of a given order, and then also have some additional structure on top of this, such as the requirement that $sr=r^{-1}s$ for the Dihedral group, which allows the ""object"" to still satisfy the group axioms but it has a restricted set of allowed elements compared with the group generated by the elements with no restriction alone. Another example coming to mind is any Abelian group, which has this additional structure on it. Are these properties (i.e. the number of generating elements, their orders, and any additional properties/constraints) necessary and sufficient to specify a group? I am trying to think in terms of isomorphisms; I would try to show two groups are 'the same' by showing that an isomorphism exists between them. onsidering this, it certainly seems sufficient that two groups are matched with these properties. The isomorphism can then simply map the corresponding elements on to each other. But is it necessary? Or, in other words, if two groups are not exactly matched in these properties, is there no isomorphism between them? Perhaps it also makes a difference if the groups are of infinite order. Then reerring to 'the number of generating elements' and 'their orders' seems a bit suspect...","['abstract-algebra', 'group-presentation', 'group-theory', 'group-isomorphism']"
2564978,Expectation of the minimum of two first passage times of a standard Brownian motion,"Let $W_t$ be a standard Brownian motion. For any real constant $a$, $T_a=\min(t≥0:W_t=a)$. I know how to derive the distribution of $T_a$. Now given $a>0$ and $b<0$, I want to compute $E[\min(T_a,T_b)]$. Any help? Are $T_a$ and $T_b$ independent?","['stochastic-processes', 'expectation', 'probability-theory', 'stopping-times', 'brownian-motion']"
2564985,Which one is a tail event?,"I have to check if the following 4 events are tail events or not.
$X_j \in [0,1]$ are independent random variables, we define $\tau = \bigcap_n \tau_n$ where $\tau_n = \sigma(X_{n+1},X_{n+2},\dots)$, so $\tau$ is the tail-sigma-algebra. I'll write $X_j$ instead of $X_j(\omega)$
\begin{align*}
a) & \sum_1^\infty X_j< \infty\\
b) & \sum_1^\infty X_j^2 > 2\\
c) & \prod_1^\infty X_j > 2\\
d) & \sum_{n=1}^\infty \prod_{k=1}^n X_k < \infty
\end{align*}
a) $\left\{\sum_{j=1}^\infty X_j < \infty \right\} = \left\{\sum_{j=k}^\infty X_j + \sum_{i=1}^k X_i< \infty \right\}$. Since $\sum_{i=1}^k X_i <C$ for some $C>0$ the rhs is true/false if and only if the first sum is finite/infinite, right? So we fix $k\in \mathbb{N}$ and get $\left\{\sum_{j=1}^\infty X_j < \infty \right\} = \left\{\sum_{j=k}^\infty X_j < \infty \right\} \in \sigma(X_{i},i\ge k)$ which completes the proof because it's independent of the first k random variables and the event is a tail event. Am I right? b) I would say the event isn't a tail event. There might be an $\omega$ s.t. $\sum_{j=1}^k X_j^2 > 2$ for a $k \in \mathbb{N}$ and $\sum_{j=k}^\infty X_j^2 = 0 $. Is this correct? Can I assume, that such an $\omega$ exists? c) This event can obviously never be true since $X_j \in [0,1]$. So it depends on nothing and is therefore a tail event? I really don't know. d) I would argue as in a) and maybe define $\tilde{X}_n = \prod_{k=1}^n X_k$, then $\tilde{X}_n \in [0,1]$ and we have the same as in a) I would appreciate if someone can check if my answers or ideas are correct.","['real-analysis', 'measure-theory', 'probability-theory']"
2564999,Maximize expectation of concave function with respect to unitary matrix,"Let $\mathbf{x} \sim \mathcal{N}(\mathbf{m},\mathbf{C})$ and let $\mathbf{D}$ be a diagonal matrix with positive entries and of the same dimension as $\mathbf{C}$. Let $f(z)$ be a strictly increasing and concave function in $z$. Considering all the possible unitary matrices $\mathbf{U}$ (i.e., $\mathbf{U} \mathbf{U}^{\mathrm{T}}=\mathbf{I}$), I suspect the following holds: \begin{align}
\mathrm{argmax}_{\mathbf{U}} \mathbb{E}_{\mathbf{x}} \big[ f \big( \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big) \big] = \mathrm{argmax}_{\mathbf{U}} f \big( \mathbb{E}_{\mathbf{x}} \big[ \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big] \big).
\end{align} However, I don't know how to prove it formally. My intuition is that, by optimizing over unitary matrices $\mathbf{U}$, we are just playing with the ""directions"" (note that $\mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}}$ can be seen as the eigendecomposition of a symmetric and positive definite matrix), and I suspect that the directions that maximize $\mathbb{E}_{\mathbf{x}} \big[ f \big( \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big) \big]$ should be the ones maximizing $\mathbb{E}_{\mathbf{x}} \big[ \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big]$. It would be great if someone could confirm this. Note: to make things easier, we can consider $f(z) = \log(z)$ (which is strictly increasing and concave).","['matrices', 'expectation', 'convex-optimization', 'normal-distribution']"
2565030,Existence of a continuous surjective map [duplicate],"This question already has an answer here : Does there exist a continuous surjection from $\Bbb R^3-S^2$ to $\Bbb R^2-\{(0,0)\}$? (1 answer) Closed 5 years ago . I got this question in an exam recently. I haven't been able to solve it. The question goes like this : Does there exist a continuous surjective map from $\mathbb{R}^3\setminus \mathbb{S}^2$ to $\mathbb{R }^2\setminus \{0\}$? I was proceeding in kind of a naïve fashion by trying to construct a  suitable map from $\mathbb{R}^3$ to $\mathbb{R }^2$ which vanishes exactly on the unit sphere. But I doubt of that will help. Even though I am not quite sure I think the claim is false and will require some nontrivial use of tricky techniques on topology. Can someone help me with some hints?","['algebraic-topology', 'analysis']"
2565032,Every function can $f \colon \mathbb R \to \mathbb R$ can be represented as the sum of two one-to-one functions,"How can one use the Principle of Transfinite Induction, i.e, ""Let $P(z)$ be a mathematical statement that depends on the ordinal $z$. Suppose whenever $P(\eta)$ is true $\forall \eta<z,P(z)$ is also true. Then $P(z)$ is true for every ordinal $z$."" To prove that every function $$f:\mathbb{R} \to \mathbb{R}$$ can be represented by as the sum of two one-to-one functions?","['transfinite-induction', 'elementary-set-theory']"
2565052,"Show that $ \int_{S^2} (xy)^2\, dS = \frac{4\pi}{15}$","I have been constructing some examples related to spherical harmonics.  I wanted to know the $L^2$ norm of the function $f(x,y,z) \in S^2 = \{ x^2 + y^2 + z^2 = 1\}$. Wikipedia suggests this normalization: $$ \int_{S^2} (xy)^2\, dS  = \frac{4\pi}{15}$$ Are there any derivations that do not involve spherical coordinates ? I am hoping for a more algebraic derivation.  Maybe it will use spherical coordinates implicitly. \begin{eqnarray*}
x &=& \cos \theta \cos \phi \\
y &=& \sin \theta \cos \phi \\
z &=& \sin \phi
\end{eqnarray*} For smaller degree terms there is a trick.  I would do the integral of $1$: $$ \lvert\lvert\,1 \,\rvert\rvert^2 = \frac{1}{4\pi}\int_{S^2} 1 \, dS   = 1$$ The averages of homogenous linear terms (or cubic terms) is zero.  Quadratics have a trick: $$ \lvert\lvert\,x \,\rvert\rvert^2 = \lvert\lvert\,y \,\rvert\rvert^2 = \lvert\lvert\,z \,\rvert\rvert^2 = 
\frac{1}{4\pi}\int_{S^2} x^2 \, dS   =
\frac{1}{3} \frac{1}{4\pi}\int_{S^2} (x^2 + y^2 + z^2) \, dS   = \frac{1}{3}$$ I don't think there's such a trick for quadratic polynomials of 4th degree","['multivariable-calculus', 'spherical-harmonics', 'differential-geometry']"
2565067,"If $|x_{n+1} - x_n| < \epsilon,$ is $(x_n) $ a Cauchy sequence?","Suppose that $\, \forall \epsilon >0, \, \exists N, \,$ such that $$|x_{n+1} - x_n| < \epsilon\, \quad \forall n \geq N.$$ Is the sequence $(x_n)$ a Cauchy sequence? If so, prove it; if not, give a counterexample. My professor gave this counterexample: Let $$x_n := \sqrt{n} $$ then $\, \forall\epsilon > 0\, $ take $ \, N = \lfloor \frac{1}{\epsilon ^2}\rfloor +1 \, $ then we have $$ |x_n - x_{n+1} | = \sqrt{n+1} - \sqrt{n} = \frac{1}{\sqrt{n} + \sqrt{n+1}} < \frac{1}{\sqrt{n}} \leq \frac{1}{\sqrt{N}} < \epsilon $$ But $(x_n) $ is divergent, so $(x_n)$ is not Cauchy. I disagreed that this proof was valid, because the sequence $x_n := \sqrt{n} \, $ doesn't satisfy the hypothesis, particularly that if $\forall \epsilon > 0$ $$ | x_{n+1} - x_n| < \epsilon\, , \quad \forall n \geq N$$
is satisfied, then the hypothesis requires that $$ | x_{n+2} - x_{n+1}| < \epsilon\, , \quad $$
is also satisfied, and so on. Every single time I've disagreed with my analysis professor I've been wrong and I think this is not an exception. Really I would like someone to point out why the following proof (my attempt) that the sequence is Cauchy is wrong, please: For $m \geq n$ \begin{align}
|x_m - x_n| &= |x_m - x_{m-1} + x_{m-1} - \cdots + x_{n+2} - x_{n+1} - x_{n+1} + x_n| \\ 
&\leq |x_m - x_{m-1}| + |x_{m-1} - x_{m-2}| + \cdots + |x_{n+2} - x_{n+1}| + |x_{n+1} + x_n| \\
&< (m-n)\epsilon
\end{align} Therefore the sequence is Cauchy. Where I think I'm wrong: the $(m-n)$ factor. I'm claiming that the error of each term can be made as small as we like so adding up however many terms makes no difference. Actually now, that I've typed this all out it seems obvious that I'm wrong, for example: I could have every term in a sum bounded by a one trillionth, but then add up a number of  sequential terms well past one trillion, and make the total error as large as I would like, regardless of $\epsilon$. Can someone please confirm if that thinking is correct.","['telescopic-series', 'real-analysis', 'cauchy-sequences', 'convergence-divergence']"
2565084,"Let $A,B,C$ in $M_n(\mathbb C).$ Suppose $CA = BC$ and $rank(C) = r$. Show that $A$ and $B$ have at least $r$ eigenvalues in common.","Let $A,B,C$ in $M_n(\mathbb C).$ Suppose $CA = BC$ and $rank(C) = r$.
  Show that $A$ and $B$ have at least $r$ eigenvalues (counted with
  multiplicity) in common. I do not see how to use characteristic polynomial, and minimal polynomial does not give the number of eigenvalues counted with multiplicity. If someone has an hint... There exist $P$ and $Q$ invertible matrices such that $C = PJ_rQ$ with $J_r$ having first $r$ diagonal coefficients equal to $1$, and $0$ elsewhere. N.B. : It is supposed to be able to be solved without Jordan reduction since it is not in my curriculum (anyway, it is possible it simplifies the problem).","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2565113,Why is the fact that a quotient group is a group relevant?,"I'm studying the basics of quotient groups. I understand that if you build a quotient set from cosets of a group and the subgroup you are using to build them is normal then you end up with a group. I fail to see why the fact that we can define operations in the quotient set and make it into a group is meaningful. What is the motivation to do it?
Either historical perspective, practical perspective or any other to understand why we care about this is welcome.","['abstract-algebra', 'soft-question', 'quotient-group', 'group-theory', 'motivation']"
2565147,Show a function defined by an integral is Borel measurable.,"Let $f: \left [ 0, 1 \right ] \times \left [ 0, 1 \right ] \rightarrow \mathbb{R}$ satisfy: $f_x\left ( y \right ):= f\left ( x, y \right ) : \left [ 0, 1 \right ]\rightarrow \mathbb{R} $ is Riemann integrable; $f_y\left ( x \right ):= f\left ( x, y \right ) : \left [ 0, 1 \right ]\rightarrow \mathbb{R} $ is Borel measurable. Then prove that $g(x):=\int_{\left [ 0, 1 \right ]} f_x\left ( y \right ) dy$ is Borel measurable. I tried at the first consider Fubini theorem. However, it doesn't work due to the condition failure. Later I tried to prove directly from the definition of Borel measurability, and it does not go anywhere further. Also, $f$ itself may not be measurable in the product measure; and Riemann integrability does not imply Borel measurability either. Would you please give me some hints or ideas? Thank you.","['real-analysis', 'borel-measures', 'product-measure', 'riemann-integration', 'measure-theory']"
2565164,Evaluate $\int_{1}^{4} \int_{-1}^{2z} \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy \; dx\; dz$,"$\int_{1}^{4} \int_{-1}^{2z} \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy \; dx\; dz$ Consider $ \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy =  $ $\int_{0}^{\sqrt{3}x} \frac{x}{x^2 +y^2} - \frac{y}{x^2+y^2} \;dy$ $\int_{0}^{\sqrt{3}x} \frac{x}{x^2 +y^2} \; dy  = \arctan(\sqrt{3}) = \pi/3$ $x^2 + y^2 = t \Rightarrow y \; dy = dt/2$ $ \int_{0}^{\sqrt{3}x} \frac{y}{x^2 +y^2} \;dy = \int_{x^2}^{3+x^2} \frac{dt}{t} = \frac{\ln(3+x^2)}{2} - \ln(x)$ $ \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy  = \pi/3 + \ln(x) - \frac{\ln(3+x^2)}{2}$ Now it will be very lengthy and  difficult to integrate this with respect to x and z.
 So I am stuck here. Is there any easy method to solve such questions ?",['multivariable-calculus']
2565177,Existence and Uniqueness of Equilibrium Points in Non-Linear Dynamical Systems,"Let $\dot{X} = F(X)$ be a non-linear dynamical system. I'm interested in knowing if there are any existence theorems for an equilibrium point, that is, an $X^*$ that satisfies: $$ F(X^*) = 0 $$ I know that in the linear case $(F(X) = AX)$, it's trivial to establish that $X^* = 0$ is the equilibrium point; but I haven't found any theorem that guarantees the existence of equilibrium points in the non-linear case. In case there is a theorem that guarantees existence, I'm also interested in a theorem that could tell us something about the uniqueness (or lack thereof) of equilibrium points. So far, I've only come up with the idea that one should check whether $F(X) + X = G(X)$ satisfies Brouwer's fixed point theorem to check if the system has an equilibrium point, but I don't know if this is the right approach. Thanks!","['fixed-point-theorems', 'ordinary-differential-equations', 'dynamical-systems', 'nonlinear-dynamics']"
2565265,"If a non-linear map preserves inner product, does it necessarily preserve distance?","Let $(S, \langle\, , \rangle)$ be a prehilbertian space. I know that if a map $f: S \rightarrow S$ preserves norm but is not linear then it need not preserve metric. For instance, consider $f : (r\cos (\theta ), r\sin (\theta )) \mapsto$ $(r\cos (r\theta ), r\sin (r\theta )) $. We have $d(f(0,1),f(0,2))=||f(0,1)-f(0,2)||=||(0,1)-f(0,2)|| > 1=d((0,1),(0,2))$ Now I'm trying to find a counterexample for a map that preserves inner-product but, as above, doesn't preserve metric, but I've got a feeling this doesn't exist, am I right ?","['real-analysis', 'isometry', 'normed-spaces', 'functional-analysis', 'inner-products']"
2565269,Time and work. Arithmetic.,"$A$, $B$ and $C$ working together completed a job in $10$ days. However, $C$ only worked for the first three days when $\dfrac{37}{100}$ of the job was done. Also, the work done by $A$ in $5$ days equals the work done by $B$ in $4$ days. How many days will the fastest worker take to complete the given work alone? Is my approach right? My attempt: Let the rate of work done by $A,B,C$ be $a \,\text{units/day}, b \,\text{units/day}, c \,\text{units/day}$ respectively. Since, $C$ works only on the first $3$ days (along with $A$ and $B$), and $B$ and $A$ works together for the rest of the time, $$\text{Total work}=(a+b+c)\cdot3+(a+b)\cdot7=10(a+b)+3c$$ Also, $$3c=\frac{37}{100}[10(a+b)+3c]$$ so $$189c=370(a+b) \tag{1}$$ and $$5a=4b \tag{2}$$ Where is the third equation that would solve the problem?","['algebra-precalculus', 'word-problem', 'ratio']"
