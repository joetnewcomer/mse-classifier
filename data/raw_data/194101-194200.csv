question_id,title,body,tags
3727408,"How to find many bijective functions from rationals on $(0,1)$ to rationals on $(0,1)$","Let $S=\{x\in\Bbb Q:\ 0<x<1\}$ . I am trying to find a sequence of bijective functions from $S$ to itself, where each function is strictly increasing. But currently I can only think of $f(x)=x$ which is a trivial example. Intuitively I think there are infinitely many such functions but I am struggling to construct them. Thanks in advance for any help or hint.","['functions', 'monotone-functions', 'real-analysis']"
3727464,Convergence of difference quotient in $L^{p}(\mathbb{R}^{n})$,"Let $f \in W^{1,p}(\mathbb{R}^{n})$ , where $p \in (1,\infty)$ . Let us define $f^{i}_{h}$ as $$
f^{i}_{h}(x) := \frac{f(x+he_{i}) - f(x)}{|h|}.
$$ Prove that $$
||f^{i}_{h} - D_{i}f||_{L^{p}(\mathbb{R}^{n})} \to 0.
$$ I don't have any idea how to prove that. I've just heard about 7.11 from Trudinger's book about elliptic equations, but I don't have any concludes from that lemmas. Maybe it's a simple fact, but I will be really grateful for help, proof or source, where can I find that.","['lp-spaces', 'sobolev-spaces', 'functional-analysis', 'weak-derivatives']"
3727472,TV Distance between Bernoulli and Poisson,"With P=Ber(p) and Q=Poiss(p) where $p\in (0,1)$ , I try to find the TV Distance T(P,Q). To find the Total Variance Distance we can use the discrete formula $$T(P,Q) = 1/2 * \sum_{x\in E} |p_\theta(x) - p_{\theta'(x)}|$$ which I translate to $$ T(P,Q) = 1/2 * \sum_0^\infty |p^x (1-p)^{1-x} - \frac{p^x*e^{-p}}{x!}|$$ I then take the seperate case $x=0$ out of the summation and get $$=1/2 * |(1-p)-e^{-p}| + 1/2 * \sum_1^\infty |p^x (1-p)^{1-x} - \frac{p^x*e^{-p}}{x!}|$$ where due to probability $p\in (0,1)$ the second term is 1-P(x=0), therefore $$=1/2 *( |(1-p)-e^{-p}| + |1- (1-p)+e^{-p}|)$$ $$=1/2 *( |(1-p)-e^{-p}| + |p+e^{-p}|)$$ But it seems I am making a mistake somewhere, as it doesn't seem correct.","['statistics', 'total-variation', 'probability']"
3727512,Does $\operatorname E[e^{-X}Y]\le c$ imply $\operatorname E[Y]\le c\operatorname E[e^X]$?,"Quick question: Let $X,Y$ be real-valued random variables, $Y$ being nonnegative, such that $\operatorname E[e^{-X}Y]\le c$ for some $c\ge0$ . Can we somehow show that $\operatorname E[Y]\le c\operatorname E[e^X]$ ? It's clearly true when $X$ is nonrandom. In the situation I've got in mind, $X$ is $\sigma(Y)$ measurable, which might be useful.","['expected-value', 'measure-theory', 'probability-theory']"
3727530,Numerically unstable matrix decompositions (to simple perturbations e.g. $A+ \varepsilon A$),"So basically the title says it all. I would like to find a decomposition, or something similar, e.g. any transformation that would ""notice"" a slight perturbation in a given matrix; as in $f(A) - f(A+ \varepsilon * A)$ is relatively big. It would be desirable if it were reversible. Let's say the matrix has nonnegative values and they are bounded from above. An example could be LSB type algorithms. All of the normally used decompositions are stable. E.g. singular value decomposition: $U, S,V$ won't change much if I make slight changes to the original matrix.","['matrices', 'perturbation-theory']"
3727552,Application of distance between probability measures,"Let $P\sim Q$ be two equivalent probability measures. There seem to exsist different notions of how to define a difference between the two probability measures/distributions. For example, Total variation: $$\delta(P,Q)=\sup_{A} |P(A)-Q(A)|$$ Kullback–Leibler divergence: $$D_{KL}(P,Q)=\int_\mathbb{R}p(x)\ln\left(\frac{p(x)}{q(x)}\right)\mathrm{d}x$$ Hellinger distance : $$H^2(P,Q)=\int_\mathbb{R}\left(\sqrt{p(x)}-\sqrt{q(x)}\right)^2\mathrm{d}x$$ Bhattacharyya distance: $$B(P,Q)=-\ln\left(\int_\mathbb{R}\sqrt{p(x)q(x)}\mathrm{d}x\right)$$ Jensen–Shannon divergence: $$JSD(P,Q)=\frac{1}{2}D_{KL}\left(P,\frac{P+Q}{2}\right)+\frac{1}{2}D_{KL}\left(Q,\frac{P+Q}{2}\right)$$ I've got two questions. What is the intuitive meaning? Is it as simple as: if the distance between $P$ and $Q$ is big, then an unlikely event under $P$ may be very likely under $Q$ and vice versa? Does any of these differences tell me anything about how $E^P[X]$ differs from $E^Q[X]$ for a measurable random variable $X$ ? What about higher moments of $X$ ?","['measure-theory', 'probability-distributions', 'expected-value', 'probability-theory', 'probability']"
3727557,"Notion of ""almost all"" using probability theory by Terence Tao","I try to understand arXiv:1909.03562  by Terence Tao. He uses the following definition of ""almost all"": For a finite not empty subset $R$ of $\mathbb{N}$ , $\text{Log}(R)$ is defined to be a random variable taking values in $R$ with logarithmically uniform distribution, so we have \begin{align} \mathbb{P}(\text{Log}(R) \in A) =\frac{\sum_{N \in A \cap R}{\frac{1}{N}}}{\sum_{N \in R}{\frac{1}{N}}}\end{align} With this, the logarithmically density of $A \subset \mathbb{N}$ is defined as $\lim_{x \to \infty} \mathbb{P}(\text{Log}(\mathbb{N} \cap[1, x]) \in A)$ . Then a property $P$ holds for almost all $N$ if $\lim_{x \to \infty}\mathbb{P}(P(\text{Log}(\mathbb{N} \cap[1, x])) )=1$ , so if it holds on a set of logarithmically density $1$ . As I have only little experience with probability theory, I do not have an intuition for this definition. How does it relate to the ""classical"" definition of ""almost all"": Holding for all, but finitely many elements? I think, that if a statement holds for almost all in the classical definition it also holds in this sense, but what about the other direction? How large can the size of the set of exceptions get and how does this all relate to logarithms? Thank you in advance for your help.","['number-theory', 'probability-theory']"
3727602,Prove that $\ln(1+\frac{1}{x}) < \frac{1}{({x^2 + x})^{1/2}}$,"Prove that $$\ln\left(1+\frac{1}{x}\right) < \frac{1}{({x^2 + x})^{1/2}}.$$ I assumed $$g(x) =  \frac{1}{({x^2 + x})^{1/2}} - \ln\left(1+\frac{1}{x}\right)$$ and got it's derivative which is negative, which means, it is a decreasing function. But, it is a function where $\ln(1+\frac{1}{x})$ never exceeds $\frac{1}{({x^2 + x})^{1/2}}$ . Graph for the function Also, $g(x)$ don't have any extremum points in $x > 0$ . Both tends to $0$ as $x$ tends to $\infty$ . How do I prove the inequality?","['inequality', 'derivatives']"
3727619,"Show that $\sqrt n(G_n − \frac1e) →^d N(0,\sigma^2)$","Let $X_1,X_2\ldots $ be a sequence of independent, identically
distributed with $X_i\sim U(0,1)$ For the sequence of geometric means $G_n = \big(\prod_{i=1}^n X_i
\big)^{1/n}$ show that $\sqrt n(G_n − \frac1e) →^d N(0,\sigma^2)$ , for
some $\sigma.  $ Find $\sigma$ My attempt: $\ln(G_n)= \frac1n \ln(\prod_{i=1}^n X_i) = \frac1n \sum_1^n  \ln(X_i)$ Let $Y_i = \ln(X_i) \Rightarrow \ln(G_n) = \overline{Y} = \frac1n \sum_1^n Y_i$ I found out that, $ \mu = E(Y_1) = \int_0^1 \ln x \;dx  = -1$ and $\sigma^2 = Var(Y_1) = \int_0^1 (\ln(x) +1)^2 \; dx = 1$ I know by Central Limit Theorem, that $$\frac{\sqrt n (\overline{Y} - (-1))}{1} \sim N(0,1) $$ $$ \Rightarrow \sqrt n (\overline{Y} - (-1)) \sim N(0,1) $$ Using delta method with $f(x) = e^x$ , we get $$ \Rightarrow \sqrt n (e^{\overline{Y}} - \frac1e) \sim \frac1e N(0,1) $$ $$ \Rightarrow \sqrt n (G_n - \frac1e) \sim \frac1e N(0,1)  \sim N(0,\frac1{e^2})$$ So, $ \sigma = \frac1e$ Is this correct?","['delta-method', 'central-limit-theorem', 'probability-theory', 'weak-convergence']"
3727632,Find the minimum of $x^3+\frac{1}{x^2}$ for $x>0$,"Finding this minimum must be only done using ineaqualities. $x^3+\frac{1}{x^2}=\frac{1}{2}x^3+\frac{1}{2}x^3+\frac{1}{3x^2}+\frac{1}{3x^2}+\frac{1}{3x^2}$ Using inequalities of arithemtic and geometric means: $\frac{\frac{1}{2}x^3+\frac{1}{2}x^3+\frac{1}{3x^2}+\frac{1}{3x^2}+\frac{1}{3x^2}+1}{6}\geqslant
 \sqrt[6]{\frac{1}{2}x^3\frac{1}{2}x^3\frac{1}{3x^2}\frac{1}{3x^2}\frac{1}{3x^2}}=\sqrt[6]{\frac{1}{108}}\Rightarrow x^3+\frac{1}{x^2}\geqslant 6\sqrt[6]{\frac{1}{108}}-1 $ Sadly $\ 6\sqrt[6]{\frac{1}{108}}-1$ is not correct answer, it is not the minimum.","['maxima-minima', 'algebra-precalculus', 'a.m.-g.m.-inequality', 'inequality']"
3727709,Sufficient condition to be a countable set,Let $A$ a non empty set. Which of the following conditions is sufficient because A is a countable set: $A \subset \mathbb N$ $\mathbb N \subset A$ exists a surjective function $f: \mathbb N \rightarrow A$ and $A$ is infinite exists a succession $a_n$ so that $\{a_n:n \in \mathbb N\}=A$ My attemp: is false because if the set $A$ is finite I can't find a bijective function fraom $\mathbb N$ to $A$ is false because if I take $A=\mathbb R$ is not countable but for the other cases?,['elementary-set-theory']
3727724,How to integrate $\int_0^\infty \left( \frac{\sin az}{z^2+1}\right)^2 dz$,"I have to evaluate the following integral for $a>0$ : $$\int_0^\infty \left( \frac{\sin az}{z^2+1}\right)^2 dz$$ I don't exactly know how to do this kind of integral. But I think I need to use the residue theorem. Maybe I could use the trick $\cos(2a) = 1 - \sin^2(a)$ , but that makes the integral much more difficult I think.","['integration', 'complex-analysis', 'residue-calculus', 'cauchy-integral-formula']"
3727727,Why does L'Hospital's rule require the limit to exist? About the proof.,"Proof of L'Hospital's rule (only special case this time): Let's assume, that $$ f(a)=g(a)=0 .$$ Using the MVT, we get $$ \frac{f(x)}{g(x)}=\frac{f(x)-f(a)}{g(x)-g(a)}=\frac{f'(\zeta)}{g'(\zeta)}, \ \mathrm{where} \ \zeta \in (a, x) \ \mathrm{or} \ \zeta \in (x, a) $$ and thus $$
\lim_{x \rightarrow a} \frac{f(x)}{g(x)}=\lim_{x \rightarrow a} \frac{f'(\zeta)}{g'(\zeta)}=\lim_{\zeta \rightarrow a} \frac{f'(\zeta)}{g'(\zeta)}=\lim_{x \rightarrow a} \frac{f'(x)}{g'(x)}.
$$ In literature, it's always assumed that the limit $$ \lim_{x \rightarrow a} \frac{f'(x)}{g'(x)} $$ exists. Why is that and how is it shown in the proof? According to the proof above, shouldn't the limit of f/g  be equal to the limit of f'/g' always? If lim f'/g' is not defined, then lim f/g shouldn't be defined either. Obviously this isn't true, because there are examples of functions f and g s.t.  lim f/g exists even though lim f'/g' does not exist.","['limits', 'solution-verification', 'analysis']"
3727821,To Prove $ \bigcap_{i \in I} A_i \in \bigcap_{i \in I} P(A_i) $,"$$ \bigcap_{i \in I} A_i \in \bigcap_{i \in I} P(A_i)  $$ , $ I \neq \phi $ MY ATTEMPT I use proof by Contradiction.  Assume $ \bigcap_{i \in I} A_i \notin \bigcap_{i \in I} P(A_i)  $$ Let $ x \in  \bigcap_{i \in I} A_i $ i.e $ \{ x \}  \notin ( P(A_1 ) \land P(A_2 \land... \land P(A_n)) )$ So $ \{x\} \not \subseteq A_i  \forall i \in I$ So $ x \notin A_i \forall i \in I$ So $x \notin \bigcap_{i \in I} A_i  $ Hence we arrive at contradiction","['elementary-set-theory', 'discrete-mathematics']"
3727891,What parameter is the Wilcoxon 2-sample statistic estimating?,"I'm reading about the Wilcoxon 2-sample statistic. They say that this is an estimator for the following parameter: $$\theta(F,G) = \int F \, dG = P(X \leq Y)$$ where $F$ and $G$ are continuous distribution functions for the random variables $X$ and $Y$ . How does $\theta$ give us $P(X \leq Y)$ ? Asked another way: why does $\int F \, dG $ imply $P(X \leq Y)$ ? [For context this is the first example of 5.1.3 in Serfling (1980)]","['statistics', 'probability-theory', 'probability']"
3727939,Knight on a $3\times 4$ board: Hamiltonian graphs,"A chess knight sits on a $3\times 4$ board. Is it possible for the knight to jump into the $12$ squares without jumping twice in any of them and ending and starting in the same box? What if it starts and ends in the different boxes? I have drawn the graph that represents this problem and by looking at it I know that the answer to the first question is that it is impossible, but the second one is possible. However I can't find a mathematical reasoning to prove this. I know that my problem is equivalent to finding a hamiltonian cycle in the first case and a hamiltonian path in the second, but I don't know how to do this in any other way that trying to draw different paths. Could someone please help me with the mathematical reasoning?","['graph-theory', 'hamiltonicity', 'discrete-mathematics']"
3727984,Jensen inequality in measure theory : why doesn't the convex function need to be nonnegative?,"This section of the Wikipedia article on Jensen's inequality states that if $g$ is an integrable function on a measure space with mass $1$ and $\varphi$ is a convex function, then $$\varphi \left( \int g \right) \leq \int \varphi \circ g $$ What troubles me is that I can see no mention of a hypothesis ensuring that $\varphi \circ g$ actually has an integral (like that it is integrable or else $\varphi$ is nonnegative).
I assumed it was simply missing, but then I noticed that not only does the same article in French also omit these hypotheses but furthermore, it even bothers to mention that the integral on the right may be infinite, suggesting in my opinion that $\varphi$ should be assumed to be nonnegative.","['integration', 'measure-theory', 'probability-theory', 'jensen-inequality', 'holder-inequality']"
3727987,Find Recursive Definition from given formula,"I've read some ways about how to derive a formula from a recursive definition, but what about this one? I started solving this formula $$ a_n = 2^n + 5^n n , n \in \mathbb{N} $$ gives you the recursive definition of what and how you go about figuring that out? Any tips for me? My calculations so far $$ (r-2)(r-5) = r^2 - 7r + 10 $$ Am i doing right?","['discrete-mathematics', 'recursion']"
3727996,Maximal Solution of ODE Bounded by a Positive Function,"Let $F:\mathbb{R}^{1+d}\rightarrow\mathbb{R}^{d}$ continuous such that $\|F(t,x)\|\leq g(\|x\|)$ for some function $g:[0,\infty)\rightarrow\mathbb{R}$ satisfying \begin{equation}
\int_{0}^{\infty}\frac{ds}{g(s)}=\infty. \;\;\;\;\;\;\;\;\;\;\;(1)
\end{equation} Prove that all maximum solution of $x'=F(t,x)$ with $x(t_0)=0$ are definite for all $t\geq t_0$ . I tried it this way: By Picard Theorem, we can write the ODE solution as $$\gamma(t)=\int_{t_0}^{t}F(s,\gamma(s))ds.$$ So, by hypothesis we have $$\|\gamma(t)\|\leq\int_{t_0}^{t}g(\|\gamma(s)\|)ds.$$ From here, I didn't know how to continue. I've no idea how to use $(1)$ . Somebody can help me? Any hint? Thanks! :D",['ordinary-differential-equations']
3728053,"If $X$ is a nonnegative $\sigma$-subGaussian random variable with $P(X=0)\ge p$, what is a good upper bound for $P(X \ge h)$?","Let $X$ be a nonnegative random variable and let $\sigma \in [0,\infty)$ and $p \in (0,1)$ such that (1) $P(X=0) \ge p$ (2) $Var(X) \le \sigma^2$ For $h \ge 0$ , define $c_X(h):=P(X \ge h)$ . The following result was established in a paper of S. Bobkov . For every $h \ge \dfrac{\sigma}{\sqrt{p(1-p)}}$ , it holds that $P(X \ge h) \le \dfrac{p\sigma^2}{ph^2-\sigma^2}$ . In the referenced paper, the above inequality is labeled as (2.6). Now, suppose we replace condition (2) with the following condition (2') $X$ is $\sigma^2$ -subGaussian, meaning that $P(|X-EX| > t) \le 2\exp(-t^2/(2\sigma^2))$ for all $t \ge 0$ . Question. What is a good upper bound for $c_X(h)$ as a function of $p$ , $\sigma$ , and $h$ , in this case ? One would expect to get stronger to obtain a stronger tail-bound than previously. N.B.: Of course, if the worst comes to the worst, I'll be fine with a bound which works only works sufficiently large $h$ .","['statistics', 'concentration-of-measure', 'geometric-probability', 'distribution-tails', 'probability']"
3728071,Elliptic curves with supersingular reduction have irreducible mod $p$ representations?,"Let $E$ be an elliptic curve over $\mathbb Q_p$ and suppose that $E$ has good reduction at a prime $p$ . I read here that if $E$ has ordinary (resp. supersingular) reduction at $p$ then the mod $p$ representation of $E$ is reducible (resp. irreducible). Why is this true?  If the reduction at $p$ is ordinary, I think reducibility follows essentially because the reduction map $E[p]\rightarrow \tilde E[p]$ is $G_{\mathbb Q_p}$ -equivariant, so its kernel is a $G_{\mathbb Q_p}$ -stable copy of $\mathbb{Z}/p\mathbb{Z}$ in $E[p]$ . But in the supersingular case, the reduction map is just zero, so it doesn't seem to be of any help... Can anyone point me in the right direction?","['number-theory', 'galois-representations', 'elliptic-curves']"
3728087,"Deciding convergence/divergence of $\sum_{m \geq 1} \frac{1}{m^3} \sum\limits_{\substack{k=1\\(m,k) = 1}}^m k \sin\left(\frac{2\pi k n}{m}\right)$","Let $n$ be a positive integer. I am attempting to determine whether the series $$
\sum_{m \geq 1} \frac{1}{m^3} \sum_{\substack{k=1\\(m,k) = 1}}^m k \sin\left(\frac{2\pi k n}{m}\right)
$$ converges or diverges. I've tried the (admittedly naive) bound $|k\sin\left(\frac{2\pi kn}{m}\right)| < k$ , but this doesn't help: $$
\sum_{m \geq 1} \frac{1}{m^3} \sum_{\substack{k=1\\(m,k) = 1}}^m k \sin\left(\frac{2\pi k n}{m}\right) < \sum_{m \geq 1} \frac{1}{m^3} \sum_{\substack{k=1\\(m,k) = 1}}^m k \left|\sin\left(\frac{2\pi k n}{m}\right)\right| < \sum_{m\geq 1}\frac{1}{m^3} m\sum_{\substack{k=1\\(m,k) = 1}}^m 1 = \sum_{m\geq 1}\frac{\varphi(m)}{m^2} = \infty,
$$ where $\varphi$ is the totient function. I've also try leveraging the fact that $$
\sum_{\substack{k=1\\(m,k) = 1}}^m \sin\left(\frac{2\pi kn}{m}\right) = 0,
$$ but that too led to a dead end. Is there an obvious trick to determine whether this series converges or diverges?","['analytic-number-theory', 'number-theory', 'exponential-sum', 'sequences-and-series']"
3728109,Why the exceptional divisor of blowup of $\{x^2+yt=0\}$ has multiplicity one (but not two)?,"Let $X$ be the affine surface $\{x^2+yt=0\}\subseteq \mathbb C^3$ , then $X$ has an $A_1$ singularity at $0$ . Consider $X$ as a family of curves via the projection to the last coordiate $$\pi:X\to \mathbb C, (x,y,t)\mapsto t.$$ Let $\sigma:\tilde{X}\to X$ be the blowup of $X$ at $0$ with exceptional divisor $E$ , then it is claimed in Harris-Morrison's Moduli of Curves , page 133, that the exceptional divisor $E$ has multiplicity one . Does the multiplicity means the order of vanishing of $(\sigma\circ\pi)^*(t)$ on the component $E$ ? How to obtain $E$ has multiplicity one? Here is another example in Harris-Morrison , where the family is $y^2-x^3-t=0$ and it is shown that exceptional divisor of blowup acquires with multiplicity two . (Note the major difference between two examples is the smoothness of the original total space.) I was imitating its computation to work in my case: Blowup $0\in \mathbb C^3$ and get $W\subseteq \mathbb C^3\times \mathbb P^2_{[\alpha,\beta,\gamma]}$ with equations $$x\beta=y\alpha,x\gamma=t\alpha,y\gamma=t\beta.$$ Then $\tilde{X}\subset W$ is the strict transform of this blowup and $E$ is the conic $\{\alpha^2+\beta\gamma=0\}\subseteq \mathbb P^2_{[\alpha,\beta,\gamma]}$ . By setting $\gamma=1$ , we have affine equation of $\tilde{X}$ $$x=t\alpha, \ y=t\beta, \ t^2(\alpha^2+\beta)=0.$$ To me, the appearance of the $t^2$ means the vanishing order of $t$ on exceptional $E$ has order two . This contradicts the conclusion in the book. Am I missing something? Edited : The picture attached below is a piece from page 133 of Moduli of curves , and the last sentence is where I am trying to understand:","['algebraic-geometry', 'blowup', 'birational-geometry']"
3728141,Getting the gist of von Neumann algebras,"I am having an issue with my studying: I am focusing on C*-algebra theory, but I am encountering many propositions that have to do with von Neumann algebras. I cannot understand the ultraweak topology, especially the fact that it is independent of the representation $M\subset B(H)$ . I cannot understand what that means, since by the definition I have encountered von Neumann algebras are defined to live in some $B(H)$ . I also cannot understand what we mean when we say normal representation/ functional. I would like to understand these terms but I do not know where to study from. Other notions I am encountering are the enveloping von Neumann algebra and the double dual of a $C^*$ -algebra and the isometric isomorphism between the two. I am looking for a reference to study these concepts but as I said, this is not my focus so I do not want to deal with all the details. A short presentation would be fine for me. Right now I have seen the definition of a von Neumann algebra as a $*$ -subalgebra of some $B(H)$ that is closed in SOT, the equivalence of it being closed in WOT, the double commutant theorem, the fact that vN algebra's are always unital, the fact that they are closed under polar decomposition and that they contain many projections (i.e. they are the norm-closed linear span of their projections). I have also seen Kaplansky's density theorem. Oh, I have also seen the construction of a predual through the trace-class operators and all, but it never came in handy. Any suggestions? I tried Kadison and Ringrose but their presentation seems way too detailed for me.","['von-neumann-algebras', 'functional-analysis', 'operator-algebras']"
3728169,Equality of Moment Generating Functions,"Let $X,Y$ be be random variables whose moment generating functions $s\mapsto \mathbb{E}(e^{sX})$ exist and agree on either the interval $(-\delta,0]$ or on the interval $[0,\delta)$ for some $\delta > 0$ .  Do $X$ and $Y$ have the same distribution? In particular, is the following argument outline valid: The Laplace transforms (with $s$ now in $\mathbb{C}$ ), $s\mapsto \mathbb{E}(e^{sX})$ exist on some strip $\text{Re}(s)\in (-\delta,0)$ or $\text{Re}(s)\in (0,\delta)$ and are analytic there.  Therefore, they agree on that strip, and so they agree on the boundary $\text{Re}(s)=0$ , so the characteristic functions are the same.  That implies the distributions are the same.","['fourier-transform', 'laplace-transform', 'moment-generating-functions', 'probability-theory', 'probability']"
3728215,Sufficient condtions for almost sure convergence - proof verification,"I want to prove the following lemma, relating to the proof of Kolmogorov's Two Series Theorem: Obviously, it suffices to show that $\{Y_k\}_{k \in \mathbb{N}}$ is Cauchy for almost every $\omega$ .  I'm not sure if my approach to this is correct, particularly on the part concerning the set inclusion (I've put this in all caps below).  Please help me verify if you can, thanks! My ideas: Lemma #1: For every $\delta > 0$ , $\lim_{m \rightarrow \infty} P(\cup_{k = m}^\infty|Y_k - Y_m| > \delta) = 0$ . Proof: Let $n \in \mathbb{N}$ , $\epsilon > 0$ and choose $N$ such that $m \ge N$ implies $P(\cup_{k = m}^{m+n} |Y_k - Y_m| > \delta) < \epsilon/2$ (possible from assumption).  Define $A_n \equiv \cup_{k = m}^{m+n} |Y_k - Y_m| > \delta$ .  For fixed $m$ we have $A_n \subseteq A_{n+1}$ so that $\forall m \ge N$ , $$P\left(\bigcup_{k=m}^\infty |Y_k-Y_m| > \delta\right) = P(\cup_{n=1}^\infty A_n) = \lim_{n \rightarrow \infty}P(A_n) \leq \epsilon/2 < \epsilon \quad \quad \square$$ For the main problem (Lemma 10.3), we try to show that for each $\delta$ , $$A_\delta \equiv \bigcap_{N=1}^\infty \bigcup_{m, n \ge N} \{ |Y_m - Y_n| > \delta \}$$ is a nullset, because this will imply Cauchy-ness by taking the union over $\delta \in \mathbb{Q}$ for example. Fix $N^*$ such that $m \ge N^*$ implies $P(\cup_{k = m}^\infty |Y_k - Y_m| > \delta/2) < \epsilon$ .  Then (THE SECOND INCLUSION HERE IS THE PART I'M REALLY NOT SURE ABOUT): $$A_\delta \subseteq \bigcup_{n, m \ge N^*} \{|Y_n - Y_m| > \delta\} \subseteq \bigcup_{m=N^*}^\infty \{|Y_{N^*} - Y_m| > \delta/2\}$$ The second inclusion follows from the fact that, if for all $m \ge N^*$ , $|Y_m - Y_{N^*}| \leq \delta/2$ , then for all $n, m \ge N^*$ , $|Y_n - Y_m| \leq |Y_n - Y_{N^*}| + |Y_m - Y_{N^*}| \leq \delta$ (i.e. if we are not in the rightmost set, we cannot be in the previous set in the sequence of inclusions).  Thus $P(A_\delta) < \epsilon$ .  Since $\epsilon$ was arbitrary, we are done. $\quad \square$","['convergence-divergence', 'probability-theory', 'almost-everywhere']"
3728255,Why do we need prime ideals in the spectrum of a ring?,"I'm reading Atiyah Macdonald, where they introduce in the exercises of chapter one a topological space $\operatorname{Spec}(A)$ associated to a ring $A$ , which is defined as $\operatorname{Spec}(A) \equiv \{ I : \text{I is a prime ideal in A} \}$ .
I have some questions about this topological space: Why should the ideals be prime? As far as I can tell, it seems to be a technical condition to allow union of open sets to work. Is there a deeper reason? Why do we generate the closed sets as collections of prime ideals? As far as I can tell, there is nothing that breaks with infinite union and intersection, so we can just as well take the sets to be open ? Why is the function that takes subsets of the ring to a closed set called $V$ in the text? It cannot be 'variety': it is taking elements/points (geometry) into ideals (algebra). If anything, it is an ""anti-variety"". Perhaps I have missed something in proving that $\operatorname{Spec}(A)$ is a topological space, so I will recapitulate proof sketches below. First, the topology on $\operatorname{Spec}(A)$ is given by stating that the closed sets of the topology are given by: $$
V: 2^A \rightarrow 2^{\operatorname{Spec}}; 
V(S) \equiv \{ I \in \operatorname{Spec}(A): S \subseteq I\} \\ 
 \tau_\text{closed} \equiv \{ V(S): S \subseteq A \}
$$ That is, for every subset $S$ of $A$ , the set of prime ideals that contain $S$ [which is denoted as $V(S)$ ] is a closed set. Now we check that: $\emptyset \in \tau$ since $V(R) = \{ I \in \operatorname{Spec}(A) : R \subseteq I \} = \emptyset$ [no proper ideal contains the whole ring] $\operatorname{Spec}(A) \in \tau$ since $V(\{0\})= \{ I \in \operatorname{Spec}(A) : \{ 0 \} \subseteq I \} = \operatorname{Spec}(A)$ [every ideal contains zero] Intersection: $$
V(S) \cap V(S') = \{I \in \operatorname{Spec}(A): S \subseteq I\} \cap \{I \in \operatorname{Spec}(A): S' \subseteq I\} \\
= \{ I \in \operatorname{Spec}(A): S \cup S' \subseteq I \} = V(S \cup S')
$$ 4. Union [The part where prime matters]: $$
\begin{align*}
&V(S) \cup V(S') = \{I \in \operatorname{Spec}(A): S \subseteq I\} \cup \{I \in \operatorname{Spec}(A): S' \subseteq I \} \\
&= \{I \in \operatorname{Spec}(A): S \subseteq I \lor S' \subseteq I\} \\
&= \{I \in \operatorname{Spec}(A): S S' \subseteq I\} \ \ \text{(Since $I$ is prime, $ss' \in I \implies s \in I \lor s' \in I)$} \\
&= V(SS')
\end{align*}
$$ This union of $V(\cdot)$ s should also work with infinite unions, since we will get $\cup_i V(S_i) = \prod_i  S_i$ . I suppose the problem is that we do not have a topology on $A$ to define infinite products of elements? If so, does this construction work in a ring that possesses a topology to talk about infinite products?","['ring-theory', 'algebraic-geometry', 'solution-verification', 'commutative-algebra']"
3728277,"Etymology of the term ""positive definite""","According to Wikipedia In linear algebra, a symmetric $n\times n$ real matrix $M$ is said to be positive-definite if the scalar ${\displaystyle z^{\textsf {T}}Mz}$ is strictly positive for every non-zero column vector $z$ of $n$ real numbers. From this definition, I understand how the label ""positive"" fits. However, I don't see how this makes a matrix ""definite"", in an intuitive sense. In everyday life, I think of something ""definite"" as being ""not vague"". Is there any connection between this understanding and the mathematical definition? And where did this terminology get established?","['matrices', 'positive-definite', 'terminology']"
3728279,even or odd card game,"There are cards numbered $1,2,\ldots,2n$ where $n$ is odd.  The deck is shuffled.
The top two cards are drawn. If the sum is even the even player gets a point and if the sum is odd the odd player gets a point.  The next two cards are drawn and if the sum is even the even player gets a point and if the sum is odd the odd player gets a point.  The game continues until all the cards are drawn.  Which player will get the most points?
Now for one draw odd is favored because once a even card is drawn there are more odd cards in the deck so it would appear that odd is favored.  Here are my results.  (Possibly wrong because the math is complicated) $$
\begin{array}{|l|l l|}
6\text{ cards} & \text{even wins }\dfrac{3}{5} & \text{odd wins }\dfrac{2}{5}\\\hline
   10\text{ cards} & \text{even wins }\dfrac{67}{315} & \text{odd wins }\dfrac{248}{315}\\\hline
   14\text{ cards} & \text{even wins }\dfrac{597}{1001} & \text{odd wins }\dfrac{404}{1001}\\ 
\end{array}
$$ Each of these results was a surprise to me.  Is there a simple explanation.  What happens as $n$ gets large?","['combinatorics', 'card-games']"
3728306,Solving $\frac{x^2 + 12x + 4}{x+2} = 6\sqrt x$,"I need some tips with solving this algebraic equation $$\frac{x^2 + 12x + 4}{x+2} = 6\sqrt x$$ I've tried subtracting: $$\frac{8x}{x+2}$$ and also setting $$\sqrt x = t$$ Where ""t"" is a substitution to make things simpler. This is how it ends up: $$x + 2 = 6\sqrt x - \frac{8x}{x+2}$$ or $$t^2 + 2 = 6t - \frac{8t^2}{t^2+2}$$ Unfortunately, I couldn't find a way from here without getting polynomials of the fourth degree or equations with $$x\sqrt x$$ I'd just like to clarify that I'm not looking for the solution here. I'd just like it if I could have some pointers or tips on where to go from here, or even if I did something wrong. Thanks in advance!!!","['algebra-precalculus', 'quadratics']"
3728340,Conceptual reason why height of unit tetrahedron is the same as the distance between opposite faces of an octahedron?,"One of my favorite mathematics visualizations shows why attaching a tetrahedron to a triangular face of a square pyramid results in a polyhedron with five faces instead of the seven faces one might expect. One thing that I've noticed is that if you ""subtract"" a tetrahedron from an octahedron along a face, something interesting happens: the fourth vertex of the tetrahedron lands on the octahedron's opposite face. This means that the distance between opposite faces of an octagon is precisely the same as the distance from a vertex of a tetrahedron to its opposite face. Is there a clear way to see this is the case without simply computing it? (It looks like this may follow from heropup's answer , but I'd prefer an explanation that would convince a high school student.)","['polyhedra', 'soft-question', 'geometry', 'intuition']"
3728344,I need help calculating tent pole length for my lost poles.,"I lost the two fiberglass shock corded tent poles to my small dome tent. The tent base is 7'X 7' and the two poles insert in external sleeves that extend across the top to opposite corners, crossing each other at the top. I estimate the inside clearance height inside the middle of the tent to be about 54 inches (4.5 feet). So the poles would have to create an arc a little higher than that because the tent hangs below the supporting poles.
I can order replacement poles, but don't know what their length should be. I'm guessing around 14-14.5 feet, but I could be way off.
Thanks for any help.
Frank.",['geometry']
3728369,"How can this statement be false? ""If $\forall x \in D$, $P(x)$ then $\exists x \in D$ such that $P(x)$.""","I'm a college student taking a discrete mathematic course for summer. I took midterm last monday and got back grades and solutions for the exam, but I'm still confused with this specific question. The question is: Let $D$ represent a set and $P(x)$ represent a predicate where $x\in D$ .
Is this a true or false statement? Explain briefly. ""If $\forall x \in D$ , $P(x)$ then $\exists x \in D$ such that $P(x)$ ."" And the answer is: This is a false statement. How can this statement be false? ( It's late night so I don't wanna bother  my professor)",['discrete-mathematics']
3728371,Curve family whose velocities are normalized exponentials,"For a fixed $\gamma > 0$ , consider the family of curves parametrized by $\Theta=(\vec a,\vec b)$ : $$\vec v_\Theta(t) = \frac{\vec a + e^{\gamma t}\vec b}{||\vec a + e^{\gamma t}\vec b||_2}$$ I'm interested in their integrals $$\vec x_\Theta(t) = \int_0^t \vec v(t')\,dt'$$ $$\vec w_\Theta(t) = \int_0^t e^{\gamma t'}\vec v(t')\,dt'$$ I don't need $\vec x_\Theta(t)$ and $\vec w_\Theta(t)$ as functions of $\Theta=(\vec a,\vec b)$ . Rather, I'd like to know if there's a nice form for a general element of the set $$\{\vec x_\Theta(\cdot): \Theta\in \mathbb R^n \times \mathbb R^n\}$$ and similarly for $\vec w_\Theta(\cdot)$ . For example, an answer might take the form of another parametric family, parametrized by $\Theta$ or by something more convenient. I suppose there is no loss of generality in assuming we're in two dimensions (spanned by $\vec a$ and $\vec b$ ), in which case we can say that $\vec v$ is a unit vector whose direction satisfies $$\tan\theta(t)
= \frac{a_y + e^{\gamma t}b_y}{a_x + e^{\gamma t}b_x}$$ Are these well-known types of curves? At the very minimum, is there a faster and higher-precision way to compute the curves than numerical integration?","['integration', 'calculus', 'exponential-function', 'geometry']"
3728390,X Hausdorff: every point has precompact neighborhood iff X has a basis of precompact open sets,"Claim Suppose $X$ is Hausdorff, then: Every point $p\in X$ has a precompact neighborhood in $X$ $\Longleftrightarrow$ $X$ has a basis of precompact open subsets Question: It's not clear to me why it's necessary for X to be Hausdorff; so, what is missing from the following proof that X being Hausdorff provides? Proof: "" $\Longrightarrow$ "" Suppose that every point $p \in X$ has a precompact neighborhood in $X$ . We will construct a basis using precompact open subsets of $X$ . Denote the set of precompact neighborhoods for each point as $\mathbb{U} = \left\{ U_p \right\}_{p\in X}$ . Denote the topology of $X$ as $\tau$ . We claim that $\forall u \in \tau$ and $\forall U_p \in \mathbb{U}$ , the intersection, $u_p = u \cap U_p$ is precompact. This follows from the fact that every closed subset of a compact subspace is compact. Thus, $\overline{u}_p \subset \overline{U}_p$ implies that $\overline{u}_p$ is compact. Therefore, the following is a precompact basis: $\mathcal{B} = \left\{u_p: \exists u \in \tau, \exists U_p \in \mathbb{U}\,\, \mathrm{ s.t. }\,\, u_p = u \cap U_p \right\}$ "" $\Longleftarrow$ "" Suppose $X$ has a basis of precompact open subsets, which we'll denote $\mathcal{B}$ . $X$ is open, so $X =\underset{u\in \mathcal{B}}\cup u$ . Thus, $p \in X$ implies that $\exists u \in \mathcal{B}$ such that $p \in u$ . So, every $p$ has a precompact neighborhood.","['general-topology', 'compactness']"
3728485,General method for finding summation of series with $n^{th}$ term difference in AP,"What is the method for finding the summation of a series whose $n^{th}$ difference between consecutive terms is in an AP? For example, $$2,12,36,80,150,252...$$ Taking the first term difference we get another series as such - $10,24,44,70,102...$ . Taking it's term difference we get the series $14,20,26,32$ which is in an Arithmetic Progression. Or, $$1,13,53,143,311,591,1023...$$ In this, the series formed by the $3^{rd}$ order difference is in AP.","['algebra-precalculus', 'arithmetic-progressions', 'sequences-and-series']"
3728502,"Show that $f(x)=\sqrt 2$ has no solutions $f(x)=\sin x\cos x(2+\sin x)$ and $x\in [0,\frac{\pi}{2}]$.","Show that $f(x)=\sqrt 2$ has no solutions when $$f(x)=\sin x\cos x(2+\sin x)$$ and $x\in [0,\frac{\pi}{2}]$ . My attempt: Since $f(x)$ is continuous in its domain, it is enough to show that maximum value attained by $f$ in $[0,\frac{\pi}{2}]$ is less than $\sqrt 2$ . Also, since $f(0)=f(\pi/2)=0$ , it must hold true that $f'(c)=0$ for some $c\in[0,\frac{\pi}{2}]$ (Rolle's Theorem). And since $f(\pi/6)>0$ , there has to exist a maxima. But differentiation doesn't help me here, because when I set $f'(x)$ to $0$ , I get (on rearrangement and using basic trigonometry) a cubic in $\sin x$ : $$3t^3+4t^2-2t-2=0$$ where $t=\sin x$ . The above cubic doesn't have any rational roots and I'm stuck. Any help will be great. Thanks! Edit: I created this question by myself to solve in a pen-paper test. So methods that involve usage of calculators are useless. No offence. P.S. Please keep in mind that I'm barely seventeen, so no highly advance math please!","['calculus', 'derivatives', 'trigonometry']"
3728539,Prove that Odd graph has girth of 6,"I'm stuck in a problem and need help figuring it out. 1.1.28. $(+)$ The Odd Graph $O_{k}$ . The vertices of the graph $O_{k}$ are the $k$ -element subsets of $\{1,2, \ldots, 2 k+1\} .$ Two vertices are adjacent if and only if they are disjoint sets. Thus $O_{2}$ is the Petersen graph. Prove that the girth of $O_{k}$ is 6 if $k \geq 3$ source : West, Introduction to Graph Theory My solution : Let [2k+1] denote the set {1, 2, 3, ... 2k+1} Clearly girth cannot be 1 or 2. Also girth cannot be 3, unless k = 1,
as, for girth = 3, we need 3 disjoint k-element subset of [2k+1].
Say x, y, z. Then $|(x \bigcup y \bigcup z)| \le 2k+1, i.e. 3k \le 2k+1, i.e. k\le1$ . Girth cannot be equal to 4,
as, consider any two non-adjacent vertices x, y. We prove that they have
exactly one common neighbour. Say, z.
Clearly , $k-1\ge|x \bigcap y|\ge1,$ as x and y should have some common element(s), also, $|x\bigcap z| = |y\bigcap z| = |x\bigcap y\bigcap z| = 0.$ Now, $|x\bigcup y\bigcup z|\le2k+1.$ i.e, expanding using set principles we get, $k-1\le|x \bigcap y|.$ Thus, $k-1=|x \bigcap y|$ .i.e. x and y differ only in single element. And $|(x\bigcup y)|=k+1$ , using set principles. Clearly, z should be k-element subset from the set $[2k+1]-(x\bigcup y)$ , the size of which is k. So, z is uniquely determined, and no other set full-filling these criteria exists, Therefore, girth of 4 is impossible. I'm unable to proceed further to show the non-existence of girth of 5, for $k\ge3$ .
Please help to proceed further.","['graph-theory', 'algebraic-graph-theory', 'combinatorics', 'discrete-mathematics']"
3728546,Existence of infinitely many sequences of this kind that are prime-free,"Prove that there exists infinitely many positive integers, $x$ , such that the sequence $(a_n)$ : $$a_0=1, a_1=x+1, a_{n+2}=xa_{n+1}-a_n$$ is prime-free (i.e. all terms are composite). Note: observe that $a_{4n}\equiv a_{4n+1}\equiv 1  ~(mod ~p)$ and $a_{4n+2}\equiv a_{4n+3}\equiv x-1  ~(mod ~p)$ , I guess terms in this sequence that share a common prime factor of $x\pm1$ have their positions repeat in a periodic pattern.","['number-theory', 'prime-numbers', 'elementary-number-theory', 'sequences-and-series']"
3728564,"Lee smooth manifolds, proposition 11.38 (line integral formula)","Proposition 11.38 of Lee, Introduction to smooth manifolds states if $\gamma:[a,b] \mapsto M$ is a piecewise smooth curve segment, then the line integral of $\omega$ over $\gamma$ can be written as $\int_{\gamma} \omega = \int_{a}^{b} \omega_{\gamma(t)}(\gamma'(t)) dt$ . He uses coordinates chart to prove it (chop it into segements such that each image is contained in an coordinate chart, and then compute it in the coordiante to prove), but why can't we do it directly like this ? $$\int_{\gamma} \omega = \int_{[a,b]} \gamma^* (\omega) = \int_{a}^{b} \gamma_t^*(\omega) = \int_{a}^{b} \omega_{\gamma(t)} \gamma'(t) dt$$ The third equality follows from the definition of pull-backs: $F_p^*(\omega)(v) = \omega_{F(p)} dF_p(v)$ , so I don't really see the point of using coordinate charts ?","['manifolds', 'differential-geometry']"
3728586,Integrate a weighted Bessel function over the unit disk,"I would like to evaluate a complex-valued integral of the form $$
I_e = \int_0^1 x e^{iax} J_0(b \sqrt{1-x^2}) dx
$$ where $a$ and $b$ are real numbers (not necessarily positive) and $J_0(z)$ is the Bessel function of the first kind. An alternative statement of the problem can be considered by making a change of variables $z=b\sqrt{1-x^2}, c = a/b$ , so that $$
I_e = \frac{1}{b^2} \int_0^b z e^{ic\sqrt{b^2-z^2}} J_0(z) dz.
$$ I am particularly interested in the special case of $0\leq b \leq 100$ with $c = 1$ or $-4 \leq c \leq -1$ . The task boils down to evaluating two real-valued integrals $$
I_s = \frac{1}{b^2} \int_0^b z \sin(c\sqrt{b^2-z^2}) J_0(z) dz
$$ $$
I_c = \frac{1}{b^2} \int_0^b z \cos(c\sqrt{b^2-z^2}) J_0(z) dz
$$ The integral with the sine has a simple form given by Gradshteyn and Ryzhik (6.738.1) which, after simplification, becomes $$
I_s = c \frac{j_1(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}} = a \frac{j_1(\sqrt{a^2 + b^2})}{\sqrt{a^2 + b^2}}
$$ where $j_1(z)$ is the spherical Bessel function of the first kind. I am not exactly sure how this expression was derived. Perhaps it holds a clue. I tried substituting the integral form of the Bessel function and integrating analytically but did not get very far. By symmetry, I naively expected the integral involving the cosine to be proportional to the spherical Bessel function of the second kind $y_1(z)$ (and thus, the complex-valued integral to be proportional to the spherical Hankel function of the second kind), but that does not appear to be the case. $$
I_c \neq -c \frac{y_1(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}}
$$ Particularly, because of the cosine term inside the Bessel function, $$ \lim_{b\to0} I_c \neq \lim_{b\to0} -c \frac{y_1(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}}$$ A better approximation can be achieved by dropping the cosine part of the spherical Bessel $$
I_c \approx c \frac{\sin(b\sqrt{c^2 + 1})}{b(c^2 + 1)}
$$ Indeed, if we plot $I_c \times b$ , we can see that it is a regular sine wave for $b \geq 4$ . My current goal is to find a correction term (via a series expansion, perhaps) that would improve the approximation for $b < 4$ . I've only found a single identity connected to the approximation given above. Tables of Integral Transforms , Vol. 2, p. 337, eq. 29 gives $$
\int_0^b \frac{z}{\sqrt{b^2-z^2}} \cos(c\sqrt{b^2-z^2}) J_0(z) dz =  \frac{\sin(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}}
$$ I am not sure what's the best way to connect this identity to $
I_c$ . The two integrals only differ by the first term (and the constant $
1/b^2$ ). One way is to perform a Taylor expansion around the origin: $$\frac{z}{\sqrt{b^2-z^2}} = \frac{z}{b} + \frac{1}{2} \frac{z^3}{b^3} + O(z^5) $$ The left part is from the identity, and the first term on the right is from $I_c$ . The cubic term does not appear to help, so perhaps this is not the right expansion to use in this case (and perhaps I should expand at infinity rather than at the origin). I would appreciate any tips or guidance. Thank you!","['integration', 'approximate-integration', 'calculus', 'trigonometry', 'bessel-functions']"
3728681,Proof of Hadamard's theorem in book of Do Carmo,"I'm currently reading the book of Do Carmo but I don't understand something in the proof of Hadamard's theorem. He first proves two lemmas which I understand completely but in the actual proof of the theorem, he states that $\exp_p : T_pM \to M$ is a local isometry because it is a local diffeomorphism. However, he then says that the induced metric on $T_pM$ is complete because the geodesics that pass through the origin are straight lines. I don't really see how this follows and I was wondering if someone could help me understand.","['geodesic', 'riemannian-geometry', 'differential-geometry']"
3728700,Does the Fundamental Theorem of Calculus tell us that integration is the 'opposite' of differentiation?,"I have often read that the Fundamental Theorem of Calculus (FTC) tells us that integration is the opposite of differentiation. I have always found this summary confusing, so I will lay out what I think people mean when they make such a statement. The First FTC implies the existence of antiderivatives for every function, $f$ , that is continuous on a particular interval, say $[a,b]$ . Generally, we denote this antiderivative as $F$ . Differentiating $F$ gets back to our original function, $f$ . So when people say that 'integration is the opposite of differentiation', what they mean is that an antiderivative of a function can be computed using a definite integral. The Second FTC is more powerful than the First FTC, as it tells us that definite integrals can be computed using the antiderivative of a function (which is generally more useful than knowing that one possible antiderivative of $f$ can be computed using a definite integral, $F$ ). For the Second FTC, I don't understand how this is related to 'integration being the opposite of differentiation' at all. The Second FTC shows us the link between antiderivatives (indefinite integrals) and definite integrals. It is extremely useful for trying to find the area under a curve, but I'm not sure how this relates to integration and differentiation being 'opposites'. Is there something about the First FTC or the Second FTC that has a bigger implication about integration being the opposite of differentiation, or is my understanding correct?","['integration', 'definite-integrals', 'calculus', 'indefinite-integrals', 'derivatives']"
3728702,"What are Minkowski space and Lorentzian manifolds, formally speaking?","I am in general confused about what Minkowski space is . I'll write down what I know and what I believe Minkowski space is. I'd appreciate any corrections. A Riemannian manifold is a manifold (so it locally looks like $\mathbb R^n$ ) equipped with a non-negative positive symmetric bilinear form (the metric). Hyperbolic space is a type of Riemannian manifold, where it locally looks like $\mathbb R^n$ , but globally the space has negative curvature. This gives it all of the weird properties we know and love [geodesics getting exponentially farther away, thin triangles, etc.] 'Minkowski space' naively speaking is some space $\mathbb  M \equiv \mathbb (\mathbb R^4, d)$ equipped with the metric $d(p, q) = p_0 q_0 - p_1 q_1 - p_2 q_2 - p_3 q_3$ . This looks exactly like the hyperboloid model of hyperbolic space . So it far to say that Minkowski space is literally the hyperboloid model of hyperbolic space? Next, a 'Lorentzian manifold' is a pseudo-riemannian manifold which locally looks like Minkowski space $\mathbb M$ [contrast with the Riemannian manifold which locally looks like $\mathbb R^n$ ]. Globally, it is is given by a manifold which is equipped with a non-degenerate symmetric bilinear form: note that here, the metric can be negative definite . When we talk about a 'flat Lorentzian manifold', we are talking about how the different 'local Minkowski spaces' fit together. A flat Lorentzian manifold is still hyperbolic , because minkowski space is hyperbolic. Rather, the flat here refers to the fact that there is no curvature across the local Minkowski spaces fitting together. So we are to imagine many copies of Minkowski space, each of which fit together 'perfectly', and hence there is no curvature. But locally, the manifold is Minkowski, and thus has constant negative curvature 'at each local point'. Wikipedia talks about the phrase [locally flat Is this correct? am I completely off? I find this very confusing, because Wikipedia keeps talking about float Lorentzian manifolds. To quote: Just as Euclidean space $\mathbb {R} ^{n}$ can be thought of as the model Riemannian manifold, Minkowski space $\mathbb {R} ^{n-1,1}$ with the flat Minkowski metric is the model Lorentzian manifold. My understanding of the situation is that because in a Pseudo-Riemannian manifold we can have the metric be negative , we can simply set the metric to $diag(1, -1, -1, -1)$ and get hyperbolic space. This is flat because the second derivatives vanish (indeed, the first derivatives vanish), and hence the space cannot have curvature. On the other hand, in the Riemannian case, we need to setup the hyperbolicity through curvature by assembling copies of $\mathbb R^n$ . Is what I have written sane, or am I completely off the mark? I'm looking for clarifications and spotting mistakes in my mental model of the physics I am studying with the math that I know.","['riemannian-geometry', 'semi-riemannian-geometry', 'hyperbolic-geometry', 'general-relativity', 'differential-geometry']"
3728760,Why is this assumption needed in Cauchy's theorem?,"I am studying complex analysis and Cauchy's theorem states: Suppose that a function $f$ is analytic in a simply connected domain $D$ and that $f'$ is continuous in $D$ . Then for every simple closed contour $C$ in $D$ , $\oint_C f(z)dz = 0$ Next after this theorem the book presents Cauchy-Goursat theorem which states that we don't actually need $f'$ to be continuous as assumption. My question: If it is given that function $f$ is analytic in a domain $D$ doesn't it mean that function $f$ is infinitely differentiable in that domain? Then we know that $f'$ is differentiable and so we know that $f'$ must be continuous. What I don't understand is why it is a big deal removing the assumption of continuous derivative if it is already implied by analyticity of the function. What am I missing?","['complex-analysis', 'complex-integration']"
3728786,Pairwise disjoint sets of the form $\{\lfloor n\alpha \rfloor : n \in \mathbb{N}\}.$,"Let $S_{\alpha} = \{\lfloor n\alpha \rfloor : n \in \mathbb{N}\}.$ I am working on a problem which asks to show that $\mathbb{N}$ cannot be partitioned as the pairwise disjoint union of $S_{\alpha}, S_{\beta}, S_{\gamma}$ for some $\alpha, \beta, \gamma.$ I am familiar with Beatty sequences, but this problem is slightly different. To start attacking this problem, I first wish to place conditions under which $S_{\alpha}, S_{\beta}$ are disjoint. Let $S = \frac{1}{\alpha}+\frac{1}{\beta}.$ If disjointness is equivalent to $S = 1$ and $\alpha, \beta$ being irrational, we would be done by Beatty's Theorem. I have shown $S_{\alpha}, S_{\beta}$ intersect if $S>1.$ I have also shown they intersect if one of $\alpha, \beta$ is rational. But what if $S<1$ ? It seems counterintuitive that making the sequences less dense would still allow them to intersect, no matter how small $S$ is. But whenever I test $2$ sequences on Desmos with various small values of $S,$ they always overlap. This is my only justification for why the result must be true. Here is my attempt at finding an intersection: $\alpha, \beta$ being irrational and $\lfloor \alpha n \rfloor = \lfloor \beta m \rfloor = k$ is equivalent to $\frac{k}{\alpha} < n < \frac{k+1}{\alpha}, \frac{k}{\beta} < m < \frac{k+1}{\beta}.$ If we add the $2$ equations, we get $kS < m+n < (k+1)S.$ Since $S < 1,$ we can certainly find integers $m,n, k$ to satisfy this. But we cannot proceed backwards because only $2$ inequalities have been satisfied while we need $4.$ How should I proceed? Any hints? Update: I have solved the original problem with a different method, but I'm still curious about the answer to this question.","['number-theory', 'ceiling-and-floor-functions']"
3728868,Converges uniformly on an arbitrary closed disc implies on every compact subsets,"Suppose that we have a given sequence of functions $(f_n)_{n\geq 0}$ . The goal is to show that it converges uniformly on every compact subsets of $\mathbb{C}$ . Let $R>0$ be arbitrary, and define, say, $C_R=\{z\in \mathbb{C}\mid |z-1|\leq R\}$ . If one has shown that $(f_n)_{n\geq 0}$ converges uniformly on $C_R$ , how do you conclude mathematically that it then converges uniformly on every compact subsets of $\mathbb{C}$ ? I can't find a correct way to conclude: obviously, I would just say that $R$ could be taken arbitrarily large, but in this case, what's been shown, is that it actually converges on every compact subsets containing $1$ . See the definition of $C_R$ , which is a closed disc around $1$ with centre $R$ .","['proof-writing', 'complex-analysis', 'alternative-proof', 'solution-verification', 'uniform-convergence']"
3728903,Existence of $A$ such that $ \lim_{x\to\infty}\operatorname{poly}(x) e^{-x} \sum_{n\in A} \frac{x^n}{n!}=1 $,"I want to know if there exists a set $A \subseteq \mathbb{N}$ such that $$
\lim_{x\to\infty} x^2 e^{-x} \sum_{n\in A} \dfrac{x^n}{n!}=1
$$ More generally, the question will be the existence of a set $A$ that $$
\lim_{x\to\infty}\operatorname{poly}(x) e^{-x} \sum_{n\in A} \dfrac{x^n}{n!}=1
$$ When $A$ is finite, it is obvious that the limit must be $0$ . But when $A$ is infinite, the structure of $A$ can be very complex, and I don't know how to proceed further.","['limits', 'analysis', 'sequences-and-series']"
3728967,Quaternionic and octonionic analogues of the Basel problem,"It is a well-known fact that $$\sum_{0\neq n\in\mathbb{Z}} \frac{1}{n^k} = r_k (2\pi)^k$$ for any integer $k>1$ , where $r_k$ are rational numbers which can be given explicitly in terms of Bernoulli numbers. For example, for $k=2$ the sum equals $\pi^2/3$ (this is essentially the Basel problem ), and for $k=4$ it equals $\pi^4/45$ . Note that for odd $k$ the sum vanishes. The theory of elliptic curves with complex multiplication allows us to extend this result to systems of complex integers such as the Gaussian integers , or more generally the ring of integers in an imaginary quadratic number field of class number 1. Namely, for $k>2$ we have $$\sum_{0\neq \lambda\in\mathbb{Z[\omega]}} \frac{1}{\lambda^k} = r_k \varpi^k,$$ where again $r_k$ are rational constants and $\varpi \in \mathbb{R}$ (the ""complex $2\pi$ "") depends only on the ring $\mathcal{O}=\mathbb{Z[\omega]}$ and is an algebraic multiple of a so-called Chowla–Selberg period , given by a product of powers of certain gamma factors (note that the sum is always a real number since it is invariant under conjugation). For example, for the Eisenstein ( $\omega = (1+\sqrt{3} i)/2$ ), Gaussian ( $\omega = i$ ) and Kleinian ( $\omega = (1+\sqrt{7} i)/2$ ) integers, we have respectively $$\varpi_3 = 3^{-1/4} \sqrt{2\pi} \left(\frac{\Gamma(1/3)}{\Gamma(2/3)}\right)^{3/2}, \quad \varpi_4 = 4^{-1/4} \sqrt{2\pi} \left(\frac{\Gamma(1/4)}{\Gamma(3/4)}\right), \quad \varpi_7 = 7^{-1/4} \sqrt{2\pi} \left(\frac{\Gamma(1/7)\Gamma(2/7)\Gamma(4/7)}{\Gamma(3/7)\Gamma(5/7)\Gamma(6/7)}\right)^{1/2}.$$ For higher class numbers there is a similar formula, though in that case $r_k$ will in general not be rational but algebraic. A nice exposition of this result can be found in Section 6.3 of these notes . My question is whether this is still true for hypercomplex number systems, such as the Hurwitz integers or the octonionic integers .  Define $$S_k[\mathcal{O}] = \sum_{0\neq \lambda\in\mathcal{O}} \frac{1}{\lambda^k}$$ for $k>\operatorname{dim} \mathcal{O}$ , where $\mathcal{O}$ is now an order in a totally definite rational quaternion/octonion algebra of class number 1. The restriction on $k$ is so that the sum converges absolutely. Subquestion 1: Do we have $S_k[\mathcal{O}] = r_k \varpi^k$ for some rational sequence $r_k$ and some real number $\varpi$ depending only on $\mathcal{O}$ (a ""quaternionic/octonionic $2\pi$ "")? Obviously $\varpi$ will only be defined up to a nonzero rational factor. An equivalent question is whether $(S_m[\mathcal{O}])^n/(S_n[\mathcal{O}])^m$ is rational for any $m, n$ such that $S_n[\mathcal{O}]\neq 0$ . Subquestion 2: If so, can (some fixed choice of) $\varpi$ be expressed in terms of
known constants such as $\zeta'(-1)$ or $\zeta'(-3)$ ? The reason I'm mentioning these particular constants is that in the previous cases (real and complex) the period $\varpi$ turns out to be equal to $e^{-\zeta'(\mathcal{O},0)/\zeta(\mathcal{O},0)}$ up to an algebraic factor, where the zeta function attached to the ring of integers $\mathcal{O}=\mathbb{Z}$ or $\mathbb{Z[\omega]}$ is defined as $$\zeta(\mathcal{O},s) = \sum_{0\neq \lambda\in\mathcal{O}} |\lambda|^{-s}.$$ (This is in general not the same as the previous sums, note the absolute value). In the case that $\mathcal{O}$ is instead a quaternionic or octonionic order, the logarithmic derivative of this zeta function at $s=0$ can be expressed in terms of $\zeta'(-1)$ or $\zeta'(-3)$ respectively, where $\zeta(s)$ is the ordinary Riemann zeta function. Update: I have calculated a few sums numerically for the ring of Hurwitz quaternions. The result is $$S_6[\mathcal{O}] \approx 10.76,\quad S_8[\mathcal{O}] \approx 1.196,\quad S_{12}[\mathcal{O}] \approx 23.9905.$$ Unfortunately the calculations take a lot of time, and right now the precision is not senough to determine whether e.g. $S_{12}[\mathcal{O}]/(S_6[\mathcal{O}])^2$ is rational to any degree of confidence.","['complex-multiplication', 'octonions', 'analytic-number-theory', 'sequences-and-series', 'quaternions']"
3728997,Why does this recursive function looks sinusoidal?,"For $k,n\in \mathbb{N_+}$ and $k\leq n$ , define $s(n,k)$ recursively by: $$
\begin{align}
& s(1,1)=1\\
& \\
& s(n,1)=s(n-1,n-1)\\
& \\
& s(n,2)=\frac{n-2}{n-1}s(n-1,1)\\
& \\
& s(n,k)=\frac{n-k}{n-1}s(n-1,k-1)+\frac{k-2}{n-1}s(n-1,k-2)
\end{align}
$$ When I plot $s(n,k)$ against $k$ for fixed $n$ 's, I always get a curve that looks sinusoidal: Is there an explanation for why it should be so? I understand that a 2nd order linear recurrence relation with constant coefficients can have periodic solutions , but this function is apparently not such an example, since you have $s(n-1,k-1)$ and $s(n-1,k-2)$ on the right hand side instead of $s(n,\cdot)$ , not to mention the changing coefficients.","['trigonometry', 'recurrence-relations', 'partial-differential-equations']"
3729006,Show that if $E(X|G)\le X$ a.s. then $X=E(X|G)$ a.s.,"Let $(\Omega, \mathcal{F},P)$ be a probability space and $\mathcal{G}$ a sub- $\sigma$ -algebra of $\mathcal{F}$ . Let $X$ be an integrable random variable such that $E(X|\mathcal{G})\le X$ a.s. Show that $X=E(X|\mathcal{G})$ a.s. I think that I have overthought myself into a circle on this one. I know that: \begin{equation*}
\text{If $X$ and $Y$ are integrable r.v.'s then:}\,\, X=Y \,\,\text{a.s.} \iff \int_{A}XdP=\int_AYdP \,\,\text{for all} \,\,A\in\mathcal{F}
\end{equation*} and I know that: \begin{align*}
\int_GXdP=\int_GE(X|\mathcal{G})dP \,\,\text{for all}\,\,G\in\mathcal{G}
\end{align*} I know we need to be careful using these facts as $X\in\mathcal{F}$ whereas $E(X|\mathcal{G})\in\mathcal{G}$ but I don't see any way to connect these facts with the assumption that $E(X|\mathcal{G})\le X$ a.s., so any help here would be greatly appreciated.","['conditional-probability', 'self-learning', 'probability-theory', 'probability']"
3729099,Showing that Lebesgue Dominated convergence theorem is false in case of Riemann integration.,"I was reading Tom Apostol book called ""Mathematical Analysis"" and I read this statement: the Lebesgue Dominated convergence theorem is false in case of Riemann integration. Here is the statement of LDCT: My question is: Could someone give me an example that shows that LDCT is false in the case of Riemann integration, please?","['measure-theory', 'lebesgue-integral', 'examples-counterexamples', 'analysis', 'real-analysis']"
3729110,Integrate $\int_0^1 \ln{\left(\ln{\sqrt{1-x}}\right)} \mathop{dx}$,Problem says to integrate $$\int_0^1 \ln{\left(\ln{\sqrt{1-x}}\right)} \mathop{dx}$$ I try $u=1-x$ and got $$\int_0^1 -\ln{2}+\ln{(\ln{u})} \mathop{du}$$ Then $t=\ln{u}$ $$-\ln{2}+\int_{-\infty}^0 e^t \ln{t}$$ Now what?,"['integration', 'definite-integrals', 'improper-integrals', 'complex-analysis', 'complex-integration']"
3729167,Double-sided exponential distribution - Lindeberg's condition,"If double-sided exponential distribution has density $$
g(x) = \frac{\lambda}{2}e^{-\lambda|x|}
$$ and we have a sequence of independent r.v.s $X_n$ where each of them has double-sided exponential distribution with parameter $\lambda_n = \frac{1}{\sqrt{n}}$ , prove that there exist sequences $a_n, b_n$ such that $$
\frac{\left(\sum_\limits{1\leq k \leq n}X_k\right)-a_n}{b_n} \text { converges weakly to } N(0,1)
$$ I know this is a Lindeberg's CLT type of task but I was unable to proceed any further than finding out that $\mathbb{E}X^2 = \frac{2}{\lambda^2}$ and $\mathbb{E}X = \frac{2}{\lambda}$ . If we took $X_n$ s with $\lambda=\frac{1}{\sqrt{n}}$ that would make $\sum \mathbb{E} X_n \sim cn$ for some $c \in \mathbb{R}$ and $\sum \mathbb{E} X_n^2 \sim d n^3$ for some $d \in \mathbb{R}$ . Could anyone solve this example? I tried looking for similar example but there aren't many examples of such CLT on math.stackexchange.","['central-limit-theorem', 'probability-theory', 'probability']"
3729180,Smooth projective surface whose hyperplane sections are elliptic curves is ruled,"This is a problem from chapter 6 in Beauville's book Complex Algebraic Surfaces . I have a smooth projective surface $S$ whose smooth hyperplane sections $H$ are elliptic curves. I want to show that $S$ is either a del Pezzo surface or is an elliptic ruled surface. I want to proceed by the hint in the book. I have managed to show that $q=h^1(S,\mathcal O_S)\le 1$ by looking at two holomorphic 1-forms, restricting them to the hyperplane sections, doing some stuff and getting that they have to be proportional everywhere. Now I want to take care of the cases $q=0$ and $q=1$ separately. When $q=0$ , Beauville suggests that I show that $K\equiv -H$ , i.e. that the canonical class is the negative of $H$ in $\mathrm{Pic}(S)$ . This would solve this part of the problem because I have a result from an exercise a while ago saying if $K\equiv -H$ then $S$ is a del Pezzo surface. The problem is that I'm not sure how to go about showing this. The best I can think of is to show that $H=-K$ in cohomology, which would suffice because the Chern class map $\mathrm{Pic}(S)\to H^2(S,\mathbb Z)$ is an injection since $q=0$ . However I'm not sure how this helps. To be frank, I don't know how to go abut showing two divisor classes are the same if I don't have some explicit description for them. For the case $q=1$ , I'm hoping that the previous part will show me that $H.K<0$ in general or something, as this would imply that $S$ is ruled. I haven't thought too much about this part to be honest. Any help is appreciated. I'm more looking for a hint or nudge that will get me thinking in the right direction, but even just ideas which are not fleshed out at all could be helpful.","['divisors-algebraic-geometry', 'algebraic-geometry']"
3729195,Does a uniform random variable contain enough randomness to generate any random vector?,"Let $U$ be distributed uniformly on $[0,1]$ and let $F$ be the cumulative distribution function of some real-valued random variable $X$ . Let $G(y) = \inf\{x:\, F(x) \ge y\}$ be the associated quantile function. Then the transformed random variable $G(U)$ has cdf $F$ , i.e. $$\Pr(G(U) \le x) = F(x)$$ holds for all $x \in \mathbb R$ (see here ). So $U$ ""contains enough randomness"" to represent any real-valued random variable $X$ . Question: Let $U \sim \mathrm{Unif}([0,1])$ and let $(X, Y)$ be a $\mathbb R^2$ -valued random vector with cdf $F(x, y) = \Pr(X\le x, Y\le y).$ Can we always find a (measurable) function $\varphi: \mathbb R\to \mathbb R^2$ such that for $Z = (Z_1,Z_2) = \varphi(U)$ we have that $$\Pr(Z_1 \le x, Z_2 \le y ) = F(x, y)$$ holds for all $x, y \in \mathbb R$ ?","['measure-theory', 'probability-distributions', 'probability-theory', 'real-analysis']"
3729197,Looking for Distinct solutions to $x_1+x_2+x_3=100$ such that at least one of them should be greater than 40,"For this problem suppose that the $x_i$ 's must be non-negative integers, i.e., $x_i∈{0,1,2,⋯}$ for $i=1,2,3$ . How many distinct solutions does the following equation have such that at least one of the $x_i$ 's is larger than 40? $$x_1+x_2+x_3=100$$ I tried using different methods but I think I got closest with advanced PIE:
I thought to do it with complement, such that $20\le x_i \le 40$ , because if any of them are less 20 one must be greater than 40.
have in the back of your mind that without restrictions there are ${102\choose 2}$ solutions.
firstly i  took those $20$ s and gave to all $x_i$ s and got $$x_1+x_2+x_3=40$$ such that $0\le x_i \le20$ and after that did PIE method which gave me $231$ solutions. Lastly, I did what it was asking for $${102\choose 2}-231$$ I think thins answer is too big to be true, so please help me","['inclusion-exclusion', 'combinatorics']"
3729219,Confused about arrangements on a word,"I'm trying to do this problem from ""The Probability Tutoring Book"" by Carol Ash. If the letters in ILLINOIS are arranged at random, find the probability that the permutation begins or ends with L. My attempt: We'll first find the total number of permutations.
Total Permutations: $\frac {8!}{(2!)(3!)}$ My reasoning for that total is we can arrange the eight letters in ILLINOIS $8!$ times. Since there are two L's and three I's, any arrangement where we just swap the I's with each other or the L's with each other, we have the same string(i.e. I $L_1L_2$ INOIS is the same as I $L_2L_1$ INOIS.) Now I'll go about using the inclusion and exclusion principle to find the probability that our string begins with L or ends with L. Total strings that begin with L: $\frac {7!}{(2!)(3!)}$ Where I use the same reasoning as above, but this time I fix the first letter to be ""L"" and can permute the other letters $7!$ ways I use the same reasoning for the total strings that end with L : $\frac {7!}{(2!)(3!)}$ Now I take care of double counting, the total number of strings with both L as the first and last letter is $\frac {6!}{(2!)(3!)}$ . Again, I applied the same reasoning as above. Putting everything together, $$P(\text
{begins or ends with L}) = \left(\frac {1}{\frac {8!}{(2!)(3!)}}\right) \times\left(\frac {7!}{(2!)(3!)} + \frac {7!}{(2!)(3!)} - \frac {6!}{(2!)(3!)}\right) = \frac {13}{56}$$ However, according to the answers, the correct answer is $\frac {26}{56}$ , so I'm off by a factor or 2. A possible reasoning could be that I $L_1L_2$ INOIS is NOT the same as I $L_2L_1$ INOIS. Why would this be? Attempting this again on my differently, $P(\text{first letter is L}) = P(\text{last letter is L}) = \frac {2}{8}$ since there are 2 L's of 8 letters $P(\text{first letter is L} \cap \text {last  letter is L}) = \frac{2}{8}\frac{1}{7}$ since there are 2 L's for the first letter and then they'll be 1 L for the last letter. Putting everything together: $P(\text{first letter is L} \cup \text{last  letter is L}) = \frac{2}{8} + \frac {2}{8} -  \frac{2}{8}\frac{1}{7} = \frac {26}{56}$ which concurs with the answer in the book. But I'm still struggling to figure out why my original answer was wrong. Did I undercount?","['discrete-mathematics', 'combinatorics', 'probability']"
3729225,Question about functionals on $L^p$ spaces,"Suppose $(X,\mathcal A,\mu)$ be a compact topological space with a regular Borel measure $\mu$ . In particular $\mu$ is a finite measure. Assume $f\in L^1(X)$ .Suppose $1\leq p<\infty$ and $D\subset L^p(X)$ is a dense subset. Assume that the map $$T_f:D\rightarrow \mathbb C$$ $$h\mapsto \int_Xhfd\mu $$ is a bounded linear functional. Then is $f$ necessarily in $L^{p'}$ where $\frac{1}{p}+\frac{1}{p'}=1$ ? I know that $T_f$ extends uniquely to a bounded linear functional $$T_f:L^p(X)\rightarrow \mathbb C$$ So $\exists! \ g\in L^{p'}(X)$ such that $T_f(h)=\displaystyle\int_Xhgd\mu=T_g(h) $ Since $C(X)\hookrightarrow L^p(X)$ is continuous, we get $T_f,T_g\in C(X)^*=\mathcal M(X)$ , the space of regular complex Borel measures. The corresponding complex measures are $$\mu_f(K)=\displaystyle\int_K fd\mu $$ $$\mu_g(K)=\displaystyle\int_K gd\mu $$ But $T_f, T_g$ agree on $C(X)$ and hence $\mu_g=\mu_f$ i.e. for all $K\subset X$ , compact $$\int_K fd\mu=\int_K g d\mu $$ $$\implies f=g \ \text{a.e.}$$ In particular $f\in L^{p'}(X)$ . I hope this is an okay solution. I would be glad if someone can give an alterbative solution without using complex measures.","['measure-theory', 'analysis', 'alternative-proof', 'solution-verification', 'functional-analysis']"
3729345,The concept of differential and derivative,"The concept of derivative and differential has always caused me confusion. So, I was reviewing analysis of several variables, and in a book I found the following definition Let $X, Y$ be Banach space, $U\subset X$ an open set and $f: U\to Y$ an application, differentiable at a point $a\in U$ . The (unique) linear application $A: X\to Y$ , which satisfies \eqref{eq1} is called the derivative of $f$ at point $a$ , denoted by $df(a):= A: X\to Y$ . If $f$ is
differentiable at any point in $U$ , then the application $$df:U\to\mathcal{L}(X,Y) \qquad a\mapsto df(a)$$ is called the differential of $f$ . $$\forall\;\varepsilon>0\;\exists\:\delta>0\;\forall\: h\in X, \text{ such that } 0<\left\lVert h  \right\lVert_{X}<\delta \Rightarrow a+h\in U \text{ and } \frac{\left\lVert f(a+h)-f(a)-Ah \right\lVert_{Y}}{\left\lVert h\right\lVert_{X}}<\varepsilon\tag{1}\label{eq1}$$ Through the site there are several posts about the difference between derivative and differential, to to quote a few (as a reference): What is the practical difference between a differential and a derivative? Differential vs Derivative Are the differential and derivative of a single-variable function exactly the same thing? But despite the good answers, I was still having trouble understanding the difference between these two concepts, until I came across the definition above. So, I would like to see if I really understand these concepts (my interest is restricted to functions $f:\mathbb{R}^m\to\mathbb{R}^n$ ). I thought of the following example: Let $f:\mathbb{R}^2\to\mathbb{R}^2$ , be defined by $f(x,y)=e^x(\cos y,\sin y)$ . So, the differential is given by $$df:\mathbb{R}^2\to\mathcal{L}(\mathbb{R}^2 ,\mathbb{R}^2)\quad\text{where}\quad df=
    \begin{pmatrix}
    e^x\cos y & -e^x\sin y \\
    e^x\sin y & e^x\cos y \\
    \end{pmatrix}
$$ And the derivative "" only makes sense "" to speak, when talking about a derivative at a point, so, considering the point $(0,2\pi)$ , the derivative at this point is given by $$df(0,2\pi)=
    \begin{pmatrix}
    1 & 0 \\
    0 & 1 \\
    \end{pmatrix}
$$ which is a linear transformation from $\mathbb{R}^2$ to $\mathbb{R}^2$ . I would like to know if my example is correct, that is, if I managed to understand the difference between these concepts.","['analysis', 'multivariable-calculus', 'calculus', 'derivatives', 'terminology']"
3729429,Proof that the jump measure of a Lévy process is a Poisson random measure,"Let $(X_t)_{t \geq 0}$ be an $\mathbb{R}^d$ -valued Lévy process and consider its associated jump measure $N_t: \Omega \times \mathbb{B}(\mathbb{R}^d \setminus \{0\}) \to \bar{\mathbb{N}}_0$ given by \begin{equation*}
N_t(\omega,B):=\#\left\{0 \leq s \leq t \mid \Delta X_s(\omega) \in B\right\}
\end{equation*} I am looking for a rigorous proof that, for each $t \geq 0$ , $N_t$ is in fact a Poisson random measure on the measure space $(\mathbb{R}^d, \mathbb{B}(\mathbb{R}^d \setminus \{0\}), \mu)$ , where $\mu$ is the intensity measure $\mu(B):=t\operatorname{\mathbb{E}}(N_1(B))$ . That is, I would like to prove that $N_t$ satisfies the following definition: $\mathbf{Definition}$ : Let $(\Omega, \mathbb{F}, P)$ be a probability space and $(\mathcal{X}, \mathbb{E}, \mu)$ a $\sigma$ -finite measure space. A Poisson random measure with intensity measure $\mu$ is a mapping $N: \Omega \times \mathbb{E} \rightarrow \mathbb{N}_0$ satisfying (i) For every $\omega \in \Omega,$ the $\operatorname{map} B \mapsto N(\omega, B)$ is a measure on $(\mathcal{X}, \mathbb{E})$ (ii) For every $B \in \mathbb{E}$ , the map $\omega \mapsto N(\omega, B)$ is a random variable (i.e. measurable) and $N(\cdot, B) \sim Pois $ ( $\mu(B)$ ) (iii) If $B_1, \ldots, B_n$ are disjoint, then $N(\cdot, B_1), \ldots, N(\cdot, B_n)$ are mutually independent. I am aware that Sato, in his book $\textit{Lévy Processes and Infinitely Divisible Distributions}$ , provides as proof. However, the approach seems quite involved, and I was wondering if a more direct approach is available. In particular, I would like to know if a simple proof of the measurability in condition (ii) is available. Thank you!","['levy-processes', 'stochastic-processes', 'measure-theory', 'probability-theory']"
3729432,Pencil and holomorphic map $M\setminus B\to\mathbb{P}^1$,"On Griffiths and Harris, Principles of Algebraic Geometry, p.138, right after the proof of the Bertini's theorem. It says: The essential point here is that a pencil $\{D_\lambda\}_{\lambda\in\mathbb{P}^1}$ with base locus $B$ gives a holomorphic mapping $M\setminus B\to\mathbb{P}^1$ since by linearity every $p\in M]\setminus B$ is on a unique $D_\lambda$ . My questions are: How to see the divisors in $\{D_\lambda\}_{\lambda\in\mathbb{P}^1}$ cover the whole $M$ ? In other words, to any $p\in M\setminus B$ , why is there always a divisor from the pencil s.t. $p$ is on it? (I can see the uniqueness though) Why is the map $M\setminus B\to\mathbb{P}^1$ sending each point to the unique divisor containing it a holomorphic map? I don't see how is that related to Bertini's theorem as well as its proof.","['complex-geometry', 'divisors-algebraic-geometry', 'algebraic-geometry']"
3729436,Proof that the union of connected sets where the intersection of the closure of one with the other is non-empty.,"The problem says: Prove that if $(X,d)$ is a metric space and $A, B$ are connected subsets of $ X$ , then if $cl(A)\cap B\neq\emptyset$ , $A\cup B$ is connected. To show this, I supposed the contrary, that $A\cup B$ is disconnected and thus, $A\cup B=C\cup D$ , with $cl(C)\cap D=\emptyset$ and $cl(D)\cap C=\emptyset$ .
Then I can define the function: $$f:C\cup D\to \{0,1\}$$ $$f(x)=\begin{cases}0 ,& x\in C, \\ 1,& x\in D.\end{cases}$$ Which is continuous and not constant. If $x\in C$ , then $x\in A$ or $x\in B$ . WLOG, suppose it's in $A$ , then because $f$ is continuous, the image of connected sets is connected and thus, $f(a)=f(x), \forall a\in A$ . Because, $cl(A)\cap B\neq\emptyset$ , if $x\in cl(A)\cap B$ there is a sequence $\{x_n\}$ of points in $A$ such that: $$lim_{n\to\infty}x_n=x$$ And, because $f$ is continuous, $$lim_{n\to\infty}f(x_n)=f(x)$$ And because each $x_n\in A$ , we can conclude that $f(x)=1$ . But because $x\in B$ , we can alsio conclude that $f(b)=1\forall b\in B$ . But then, $f$ is constant, which is a contradiction. Is this correct, or am I missing something? I feel that when I use the function, I made a leap of logic by regarding $C\cup D$ as a metric space.","['general-topology', 'metric-spaces', 'analysis', 'real-analysis']"
3729438,Confidence interval for generalized variance (determinant of covariance matrix),"Let $X\sim N(\mu,\Sigma)$ be a random variable in $\mathbb{R}^d$ with multivariate normal distribution. Let $\hat\Sigma$ be the maximum likelihood estimator for $\Sigma$ , \begin{align}
\hat\Sigma=\frac1n\sum_{i=1}^n(\vec{x}_i-\bar x)(\vec{x}_i-\bar x)^T,
\end{align} where $\vec{x}_i\in\mathbb{R}^d$ is the $i$ th sample vector and $\bar x\in\mathbb{R}^d$ is the sample mean of $X$ . What is a 95% confidence interval for $\det(\Sigma)$ ? That is, what is a confidence interval around $\det(\hat\Sigma)$ that will contain $\det(\Sigma)$ 95% of the time a sample is taken? I was led to this question because I am trying to find confidence intervals for estimates of entropy and mutual information for multivariate normally distributed variables. Edit : According to the article ""The distribution of the determinant of a complex Wishart distributed matrix"" by N.R. Goodman (see note at the end about the distibution of the determinant of a (real) Wishart distributed matrix, which is attributed to Wilks (1934)), the ratio $n^d(\det\hat\Sigma/\det\Sigma)$ is distributed as the product of $d$ independent $\chi^2$ random variables with $n, n-1, \cdots, n-d+1$ degress of freedom: \begin{align}
  \frac{\det\hat\Sigma}{\det\Sigma}\sim \prod_{k=0}^{d-1} \frac{Y_k}{n},\text{ where } Y_k\sim\chi^2_{(n-k)}.
\end{align} I checked this empirically with n=100, d=20, and it worked. The image at the bottom shows that the distribution of the variables on the left and right overlap almost identically. I simulated the variable on the left by choosing a fixed positive definite 20x20 covariance matrix $\Sigma$ , and creating n=100 samples to find $\hat\Sigma$ , and repeated $10^5$ times. I simulated the distribution on the right by sampling the appropriate $\chi^2$ distributions, dividing by n and forming the product ( $10^6$ times because it was faster). In this case (n=100, d=20) the confidence interval for $\det\hat\Sigma/\det\Sigma$ is approximately $[0.0344,0.3120]$ (which is interesting since it doesn't include 1). This means that if $l(n,d)$ is the .025-quantile, and $u(n,d)$ is the .975-quantile for $\prod_{k=0}^{d-1}\frac{Y_k}{n}$ , then \begin{align}
 l(n,d) &\le \frac{\det\hat\Sigma}{\det\Sigma} \le u(n,d) \\
 \frac{\det\hat\Sigma}{u(n,d)} &\le \det\Sigma \le \frac{\det\hat\Sigma}{l(n,d)}
\end{align} 95% of the time. So what I am hoping for is expressions for $l(n,d)$ and $u(n,d)$ . Approximations in terms of n and d would be fine. (Let's say something like second order in $1/n$ and $1/d$ or any good approximation method that converges a lot faster than sampling the $\chi^2$ distributions). Thank you! Any help would be greatly appreciated.","['statistical-inference', 'statistics', 'confidence-interval', 'entropy', 'matrix-calculus']"
3729502,Computing $\lim_{x\rightarrow 0}{\frac{xe^x- e^x + 1}{x(e^x-1)}}$ without L'Hôpital's rule or Taylor series,"This limit really stamped me because i'm not allowed to use L'Hôpital's rule or Taylor's series, please help! I think the limit is $\frac{1}{2}$ , but i don't know how to prove it without the L'Hôpital's rule or Taylor's series $$\lim_{x\rightarrow 0}{\frac{xe^x- e^x + 1}{x(e^x-1)}}$$","['limits', 'calculus', 'limits-without-lhopital']"
3729532,Find the coefficient of $x^{31}$ in $(1+x+x^2+x^3+\ldots)^k$,"Find the coefficient of $x^{31}$ in $(1+x+x^2+x^3+\ldots)^k$ , where $k$ is a natural number Background : I have to use generating functions to do this I have thought about using the product rule, defining $f(x)=1+x+x^2+x^3+\ldots$ and $g(x)=(f(x))^k$ but this doesn't seem too useful since I dont know the value of $k$ , and even if I knew it, if it were a large number the process would be very tedious. Could someone help me please?","['polynomials', 'discrete-mathematics', 'generating-functions']"
3729545,Sheaf cohomology of $\mathbb{A}^3$ minus the origin,"For each $i\geq 0$ , describe $H^i(X,\mathcal{O}_X)$ where $X:=\text{Spec}(k[x,y, z])\setminus\{(x,y,z)\}$ . The first definition of cohomology I've learned involves injective resolutions, which I have no idea how to apply here. I've read some authors who claimed that Cech cohomology is often useful to compute sheaf cohomology in real life, so I decided to take that road. If $A:=k[x,y,z]$ and $Y:=\text{Spec}(A)$ , I've thought about using the affine open cover $U_x:=Y\setminus V(x)$ , $U_y:=Y\setminus V(y)$ and $U_z:=Y\setminus V(z)$ . That way, $\mathcal{O}_X(U_x)=A_x$ , $\mathcal{O}_X(U_y)=A_y$ and $\mathcal{O}_X(U_z)=A_z$ and consequently: $$C^0:=C^0(\mathcal{O}_X)=A_x\times A_y\times A_z$$ $$C^1=A_{xy}\times A_{xz}\times A_{yz}$$ $$C^2=A_{xyz}$$ I've checked that $\left(\frac{a}{x^n},\frac{b}{y^m},\frac{c}{z^\ell}\right)\in\ker(d^0)\Leftrightarrow\frac{a}{x^n}=\frac{b}{x^m}=\frac{c}{z^\ell}\in k[x,y,z]$ , so $\ker(d^0)\simeq A$ and $H^0(X,\mathcal{O}_X)=A$ . Now, determining $H^1(X,\mathcal{O}_X)=\ker(d^1)/\text{im}(d^0)$ and $H^2(X,\mathcal{O}_X)=C^1/\text{im}(d^1)$ is considerably more complicated and made me wonder whether or not this is the best option. I've also tried the simpler case $\text{Spec}(k[x,y])\setminus\{(x,y)\}$ , and even then I found hard to describe $H^1(X,\mathcal{O}_X)$ explicitly. This looks like a standard problem, so I can't help but wonder if there isn't a simpler approach which could work even for the general case $\text{Spec}(k[x_1,...,x_n])\setminus\{(x_1,...,x_n)\}$ .","['affine-schemes', 'algebraic-geometry', 'sheaf-cohomology']"
3729568,How to do eigendecomposition on giant dense PSD matrices?,"I need to perform eigendecomposition on giant matrices (at least with dimensions of 600K by 600K). I need both eigenvalues and eigenvectors, however, only top k of them, e.g. k=100. In addition, the matrices are known to be positive definite (if that helps in any way). However, unfortunately the matrices are dense and thus sparsity-based approaches do not apply. I think the only hope is to perform some form of random sampling of the giant matrix (e.g. rows or columns, or small sub-matrices), then do some computation on it, and repeat this in a loop in a way that each iteration can progressively produce a better estimate of the top k eigenvalues and eigenvectors. Is this possible? If so, could you please advise me about the method? Thank you! Golabi","['numerical-linear-algebra', 'numerical-methods', 'linear-algebra']"
3729620,The function that generates a measurable graph is measurable,"I have seen a lot of questions trying to show that graphs are measurable. However, I'm asking for the other direction. This is the question: Suppose $(X, \mathcal{S})$ is a measurable space and $f : X \to [ 0, \infty ] $ is a function. Let $\mathcal{B}$ denote the σ-algebra of Borel subsets of $( 0, \infty )$ . Prove that $U_f \in S \otimes \mathcal{B}$ if and only if $f$ is an $\mathcal{S}$ -measurable function. Definition of $U_f$ : Suppose $X$ is a set and $f : X \to [ 0, ∞ ]$ is a function. Then the region under the graph of $f$ , denoted $U_f$ , is defined by $U_f = \{( x, t ) \in X \times ( 0, \infty ) : 0 < t < f ( x )\} $ . I'm only asking how to show the forward direction: if $U_f \in S \otimes \mathcal{B}$ then $f$ is an $\mathcal{S}$ -measurable function. Here $\mathcal{S} \otimes \mathcal{B}$ is defined as the smallest $\sigma$ -algebra that contains $\{A \times B: A \in \mathcal{S}, B \in \mathcal{B} \}$ . I'm trying to show that $f^{-1}((a, \infty)) \in \mathcal{S}$ , but I'm completely stuck. Can I get some help?","['measurable-sets', 'measure-theory', 'measurable-functions']"
3729633,Minimizing the area of triangle based on three circumcenters,"I'd like to ""resurrect"" this not long time ago deleted question .
It looks interesting and not immediately obvious
(unless I'm missing something trivial). Given an acute triangle $ABC$ and its circumscribed circle centered
at $O$ . A variable point $X$ is placed  on the minor arc $AB$ of the
circle; segments $CX$ and $AB$ meet at $D$ .  The circumcenters of $\triangle ADX$ and $\triangle BDX$ are $Y$ and $Z$ , respectively.
How can we find the  location of the point $X$ for which the area of $\triangle OYZ$ is minimized? Numerical tests suggest that $\triangle OYZ$ is always similar to the reference $\triangle ABC$ , $\angle ZOY=\angle BCA$ , and \begin{align}	
\min_{X\in AB}S_{OYZ}(X)
&=\tfrac14\,S_{ABC}
\end{align} when $CX\perp AB$ . Complex numbers/coordinate geometry approach
with unit circle centered at the origin
using known function for
line/line intersection
and the location of the circumcenter
based on the coordinates of the three vertices
lead to too unreasonably overcomplicated expressions.","['optimization', 'triangles', 'circles', 'geometry']"
3729741,"Proving:$\operatorname{Proj}_{U^\perp}(x)=-\frac1{\det(A^TA)} X(u_1,\ldots, u_{n-2}, X(u_1,\ldots, u_{n-2}, x))$","The problem I'm trying to solve is as follows, which was posed to me by my professor as an exercise: Let $x, u_i \in \Bbb R^n$ , $ A = (u_1, u_2, \ldots, u_{n-2})$ and $\{u_1, u_2, \ldots, u_{n-2}\}$ is linearly independent. Let $U = \text{Col}(A)$ . Then, show $\operatorname{Proj}_{U^\perp}(x) =- \frac1{\det(A^{T}A)} X(u_1,\ldots, u_{n-2}, X(u_1, \ldots, u_{n-2}, x))$ . Here is my proof so far: We want to show that $$x - \operatorname{Proj}_U(x) = -\frac1{\det(A^TA)} X(u_1,\ldots, u_{n-2}, X(u_1,\ldots, u_{n-2}, x)).$$ Equivalently, $\begin{aligned}x - A(A^TA)^{-1}A^Tx &= -\frac1{\det(A^{T}A)} X(u_1, \ldots, u_{n-2}, X(u_1,\ldots, u_{n-2}, x)) \\\iff A\text{ adj}(A^TA)A^Tx - x\det(A^TA) &= X(u_1,\ldots, u_{n-2}, X(u_1, \ldots, u_{n-2}, x)).\end{aligned}$ Now, I've used a fact proven in class that $$\begin{aligned}&\ X(u_1, \ldots,u_{n-2}, X(u_1,\ldots, u_{n-2}, x))\\&=\left(\sum\limits_{i=1}^{n-2} (-1)^{n+i} \det((B^TA)^{(i)})u_i\right) - \det((B^TA)^{(n-1)})x\end{aligned}$$ where $B = (u_1, u_2, \ldots, u_{n-2}, x)$ and $(B^TA)^{(i)}$ is obtained by removing the $i-\text{th}$ row of $B^TA$ . Observing that $(B^TA)^{(n-1)} = A^TA$ , we can rewrite the goal, so now we need to show that $$A\text{ adj}(A^TA)A^Tx = \left(\sum\limits_{i=1}^{n-2} (-1)^{n+i}\det((B^TA)^{(i)})u_i\right).$$ This is about where I am out of ideas on how to proceed. I think I am onto something, but I am not sure how to prove this last goal. Any observations, hints, or solutions would be very much appreciated!","['transpose', 'determinant', 'linear-algebra', 'laplace-expansion']"
3729753,How can I construct a nilpotent matrix of order 100 and index 98?,"I know to construct a nilpotent matrix of order $n$ with index of nilpotency $n$ , but how to construct a nilpotent matrix of order $n$ but index of nilpotency $(n-2)$ ? Is there any general rule for the same?","['matrices', 'nilpotence', 'examples-counterexamples']"
3729762,"Solutions of $e^x-1-k\cdot \arctan{x}=0$,","consider $h(x)=e^x-1-k\cdot\arctan{x}$ ,then find on which condition on $k$ there will be two solutions for $h(x)=0$ ( $k$ is real). I let $f(x)=e^x-1$ and $g(x)=k.\arctan{x}$ and let $f(x)-g(x)=0$ has roots $0$ and $y$ . I found that if $y$ tends to $0$ then $k$ tends to $1$ . How to do further?","['calculus', 'functions', 'derivatives', 'trigonometry']"
3729784,Find all continuous function $ \frac{1}{2} \int_{0}^{x}(f(t))^{2} d t=\frac{1}{x}\left(\int_{0}^{x} f(t) d t\right)^{2} $,"Find all continuous function $f:(0, \infty) \rightarrow(0, \infty) \ni f(1)=1$ and $$
\frac{1}{2} \int_{0}^{x}(f(t))^{2} d t=\frac{1}{x}\left(\int_{0}^{x} f(t) d t\right)^{2}
$$ My approach:- Let $ F(x)=\int_{0}^{x} f(t) dt$ and $G(x)=\int_{0}^{x}(f(t))^{2} dt$ . Since $f:(0, \infty) \rightarrow(0, \infty)$ we have $F(x)>0 (\forall x>0)$ Also, $\frac{1}{2} G(x)=\frac{1}{x}\{F(x)\}^{2},$ from the given condition on differentiation, we have $$
\frac{1}{2} G^{\prime}(x)=\frac{1}{x} \cdot 2 F(x) \cdot F^{\prime}(x)-\frac{1}{x^{2}}(F(x))^{2}
$$ Next I am confused what to do??
Any suggestion or solution would be appreciated.","['integration', 'calculus']"
3729834,Definition of subsequence used in defining accumulation points,"According to the definition given on this page , a number $a$ is an accumulation point of a sequence $(a_n)$ if there is a subsequence $(a_{n_k})$ that converges to $a$ in the $\lim_{k\to \infty}$ . What does the word subsequence mean in this definition? In the page linked above, Theorem 1 says that if a sequence converges, then it has only one accumulation point, namely, the value that the original sequence converges to. For example, example 2 on the page linked above claims that the only accumulation point of $$(a_n) : a_n = \frac{n+1}{n}, \quad n \in \mathbb{N}$$ is $1$ because $\lim_{n \to \infty} a_n = 1. $ Now, Wikipedia says that a subsequence is formed by deleting terms from the parent sequence. So, what if we picked the subsequence consisting only of the first term with $n=1$ ? Then $a_1 =2$ and $2$ appears to be an accumulation point. Must the subsequence be infinite in size, and if so, how can we modify the Wikipedia definition of a subsequence to require that an infinite number of terms remain after deletion?","['definition', 'sequences-and-series', 'real-analysis']"
3729878,"If $f(x) = x^4 - x^2 + 1$, find the values of $x$ such that $f(f(f(x))) \le x^8$","If $$f(x) = x^4 - x^2 + 1$$ find the values of $x$ such that $f(f(f(x))) \le x^8$ I noticed that $f(\pm 1) = 1 \implies \underbrace{f(f(f(... f(\pm 1)..)))}_{\text{n times}} = 1$ . thus, $f(f(f(x))) = x^8$ at $x = \pm 1$ . Fortunately, these were the only two solutions to this problem. However, this is hacky at best and incomplete at worst. Can somebody provide a rigorous proof for this problem?","['algebra-precalculus', 'functions', 'polynomials', 'real-analysis']"
3729898,h Numbering $8$ vertices of cube from $1$ to $8$ such that there is no 2 consecutive number adjacent,"How many ways are there to number $8$ vertices of a cube using numbers
from $1-8$ such that there are no two consecutive numbers adjacent on
the cube ( $1$ and $8$ are considered to be consecutive). I know that the answer is $480$ (with symmetries/etc), however, I am looking for a solution. I see that there are $\frac{8!}{8\times 3}=1680$ ways (without symmetry/etc) to number the cube. I can't use graph theory for the problem. It was originally a probability problem.","['permutations', 'combinatorics']"
3729927,Evaluate the integral using Euler integrals,"I have the following integral: $$\int_{0}^\infty \frac{\sqrt{x}}{7+x^7} \ dx$$ I want to evaluate this using the Euler integral. What I have tried: I tried to make a substitution, because I want to evaluate it via gamma integrals. But I can not find the substitution. Can somebody help me with the substitution? My attempt: I made the substitution $$t = \frac{1}{7}x^7, \ \ \ x = (7x)^{1/7}, \ \ \ dx = (7t)^{-6/7} dt, \ \ \ \Rightarrow x^{1/2} = (7t)^{1/14}$$ I fill in and receive: $$\int_{0}^\infty \frac{\sqrt{x}}{7+x^7} \ dx = \frac{1}{7} \int_{0}^\infty \frac{\sqrt{x}}{1+\frac{1}{7}x^7} \ dx = \frac{7^{(-11/14)}}{7}\int_{0}^\infty \frac{t^{(1/14) - (6/7)}}{1+t} \ dt$$ After that, I continued: $$\frac{7^{(-11/14)}}{7}\int_{0}^\infty \frac{t^{(-11/14)}}{1+t} \ dt = \frac{7^{(-11/14)}}{7} B(\frac{3}{14}, 1-\frac{3}{14}) = \frac{7^{(-11/14)}}{7} \frac{\Gamma(\frac{3}{14})\cdot \Gamma(1-\frac{3}{14})}{\Gamma(1)} = \frac{7^{(-11/14)}}{7}\frac{\pi}{\sin(\frac{3\pi}{14})}$$ But the answer has to be $\frac{1}{7^{25/14}}\frac{\pi}{\sin(\frac{3\pi}{14})}$ Where did I make the mistake?","['integration', 'complex-analysis']"
3729963,Identifying a group that originated from a Wirtinger presentation.,"Given the group presentation $$
\langle x_1,x_2,x_3,x_4,x_5,x_6\mid x_2x_1x_2^{-1}x_4^{-1}, x_3x_2x_3^{-1}x_5^{-1}, x_4x_5x_4^{-1}x_2^{-1}, x_1x_3x_1^{-1}x_6^{-1}, x_5x_6x_5^{-1}x_3^{-1}, x_6x_4x_6^{-1}x_1^{-1} \rangle.
$$ I would like to simplify this presentation, or even identify the group. However I am stuck on this problem for a couple of days. The group originated from a Wirtinger presentation of the link $6_1^3$ ( http://katlas.math.toronto.edu/wiki/L6a5 ). Help is really appreciated.","['combinatorial-group-theory', 'group-presentation', 'group-theory', 'algebraic-topology']"
3729998,A summation identity,"I have encountered this identity in Page 616 of Mathematical Methods for Students of Physics and Related Fields (Second Edition) by Sadri Hassani: $$
\sum_{m = 0}^{n}\left(-1\right)^{m}\,
{\left(\,{2n + 2m}\,\right)!
\over \left(\,{n + m}\,\right)!\,\left(\,{n - m}\,\right)!\,
\left(\,{2m}\,\right)!} = \left(\,{-4}\,\right)^n
$$ .
I don't know how one can obtain it directly, however, I tried to prove it by induction. Thus, for $n = 1$ , the identity is valid. If we assume its validity for $n$ , we have to show that $\sum_{m = 0}^{n + 1} (-1)^m \frac{(2n + 2m + 2)!}{(n + m + 1)! (n - m + 1)! (2 m)!} = (-4)^{n + 1}$ . The thing that comes to one's mind is that to separate the ( $n + 1$ )th term in the left-hand side of the above, and write it as $(-1)^{n + 1} \frac{(4n + 4)!}{(2n + 2)! (2n + 2)!} + \sum_{m = 0}^{n} (-1)^m \frac{(2n + 2m + 2)!}{(n + m + 1)! (n - m + 1)! (2 m)!}$ , which with a little simplification, it becomes $(-1)^{n + 1} \frac{(4n + 4)!}{(2n + 2)! (2n + 2)!} + 2 \sum_{m = 0}^{n} (-1)^m \frac{(2n + 2m + 1) (2n + 2m)!}{(n - m + 1) (n + m)! (n - m)! (2 m)!}$ . It seems to me that one cannot simplify it more in order to be able to use the assumption; one could divide $\frac{2n + 2m + 1}{n - m + 1}$ but it doesn't seem to lead anywhere. Any help to proceed from here is appreciated!","['summation-method', 'summation', 'induction', 'sequences-and-series']"
3730006,"$X_1,\ldots, X_{100} \stackrel{iid}{\sim} N(\mu,1)$ iid. Only $X_i <0$ is recorded. 40 observations are less than 0. What is the MLE of $\mu$?","$X_1,\ldots, X_{100} \stackrel{iid}{\sim} N(\mu,1)$ but only $X_i <0$ is recorded. 40 observations are less than 0. What is the MLE of $\mu$ ? Attempt Let $X_i = \mu + Z_i$ , $Z_i\sim N(0,1)$ let $N$ be the number of $X_i<0$ $P(X<0) = P(\mu+ Z < 0) = P(Z<-\mu) = \Phi(-\mu)$ $P(N=40) = \binom{100}{40}(1-\Phi(-\mu))^{60}(\Phi(-\mu))^{40}$ $\ldots \implies$ the MLE of $\mu$ is $\hat \mu= -\Phi^{-1}(2/5) \approx 0.2533$ is this approach correct?","['statistics', 'probability-distributions', 'probability']"
3730031,Show that $f^{-1}(B)=A$,"I started yesterday my study of functions. I’m following the book “Proofs and Fundamentals” , by Ethan D. Bloch, and I’m having some trouble in starting myself in formal proofs that involve functions. This is one of my problems. Problem: Let $A$ and $B$ be sets and $f \colon A \to B$ be a function. Show that $f^{-1}(B) = A$ . So far, I understand that I will have to show that $f^{-1}(B) \subseteq A$ and $A \subseteq f^{-1}(B)$ . The only definition that I have used is that of a function given by Bloch as a subset of $A \times B$ . So, my proof was something like this. By definition, $f^{-1}(B) = \{a \in A \mid \exists b \in B: f(a) = b\}$ . Let $x \in f^{-1}(B)$ . Then $x \in A$ and it exists some $b \in B$ such that $f(x) = b$ . In particular, $x \in A$ . Hence, $f^{-1}(B) \subseteq A$ . Now, let $a \in A$ . Since $f$ is a function from $A$ to $B$ , there exists a unique ordered pair of the form $(x,y)$ for all $x \in A$ and some $y \in B$ with $y = f(x)$ . So, there must be a unique ordered pair $(a,b)$ with $b \in B$ and $b = f(a)$ . For that, $f(a) \in B$ ; and by definition, $a \in f^{-1}(B)$ . Hence, $A \subseteq f^{-1}(B)$ . Therefore, we have that $f^{-1}(B)=A$ . $\square$ Please give all the feedback to turn this proof as formal as possible. Thank you for your time.","['elementary-set-theory', 'proof-writing', 'functions', 'solution-verification']"
3730044,"Karlin-Rubin theorem, normal distribution","Let $X_1,...,X_n \sim N(\mu, \sigma^2)$ , where $\mu$ is known and $\sigma^2$ is unknown. We have a pair of hypotheses: $ 
\begin{cases}
H_0: \sigma^2\leq a   \\
H_1: \sigma^2 > a
\end{cases}$ So I use Karlin-Rubin theorem: $\phi(x)=
\begin{cases}
1& \text{if $T(x) \geq k$ }    \\
0& \text{if $T(x) < k$ }
\end{cases}$ My PDF is: $f(x)= \exp[\frac{-1}{2\sigma^2}(x-\mu)^2 -\frac{1}{2}\ln2\pi\sigma^2]$ Of course it is an exponential family, with $C(\sigma^2)=-\frac{1}{2\sigma^2}$ which is increasing, so my $T(X)=\sum(X_i-\mu)^2$ . Here comes the struggle: I've see someone do it in this fashion: $\phi(x)=
\begin{cases}
1& \text{if $\sum(X_i-\mu)^2 \leq k$ }    \\
0& \text{if $\sum(X_i-\mu)^2 > k$ }
\end{cases}$ so the inequalities are reversed. Here comes the question: why? EDIT Another question is about $k$ . We use a condition that $E_a[ϕ(X)]=α$ to obtain $k$ and thus (at some point) we set our $σ^2$ to be equal to $a$ . We do it because if the condtion is fulfilled for $σ^2=a$ then is fulfilled also for $σ^2>a$ ? So, to put it in the simple way, we reject the null hypothesis the ""more"" the greater $σ^2$ is than $a$ ? An example: in this case $\Bbb P(\sum(\frac{X_i -\mu}{\sigma})^2\leq\frac{k}{\sigma^2})=1-\alpha$ as $\sum(\frac{X_i -\mu}{\sigma})^2 \sim \mathcal X_n^2$ so: $\mathcal X_{n,1-\alpha}^2=\frac{k}{\sigma^2}$ $k=\mathcal X_{n,1-\alpha}^2a^2$ Why do we put $a$ up there? Is it because: $ 
\begin{cases}
H_0: \sigma^2\leq a   \\
H_1: \sigma^2 > a
\end{cases} \equiv  
\begin{cases}
H_0: \sigma^2= a   \\
H_1: \sigma^2 > a
\end{cases}$ ?","['statistics', 'normal-distribution', 'hypothesis-testing']"
3730064,How to tell WHETHER two line segments intersect from the coordinates of both line segments only?,"I have to write a function that calculates which of the lines of the polygon (shown in the picture), if any, will be intersected by a user-drawn line. Given the polygon below... I. polygon with line These are the coordinates for the Polygon: const polygonPoints = [ {x: 100, y: 100}, {x: 200, y: 50}, {x: 300, y: 50}, {x: 400, y: 200}, {x: 350, y: 250}, {x: 200, y: 300}, {x: 150, y: 300}, ] And for example sake, here are the coordinates for the user-drawn line: [{""x"":13,""y"":276},{""x"":480,""y"":84}] II. polygon with user drawn line coordinates How would one determine which of the lines of the Polygon would be intersected based on only the coordinates alone? i.e without drawing the polygon and the user drawn line out on a graph Additional context: Background and motivation - the background is that computers need to be able to check whether lines intersect without the freedom to draw it out oon a graph, by hand, making it relevant to motion tracking and 2d/3d space detection. Your current progress - my current progress is trying to take the x and y coordinates of both lines and seeing whether there is a pattern between the ones that do appear to intersect and the ones that don't. However I am sure there is a mathematical solution to this that I do not know which is much easier.",['geometry']
3730088,Exercise 3.4.7 from Tao Analysis I (Set of all partial functions),"This is an exercise you can find here , but I recall the context: Let $X, Y$ be sets. Define a partial function from $X$ to $Y$ to be
any function $f: X' \rightarrow Y'$ with $X' \subseteq X$ and $Y'\subseteq Y$ .
Show that the collection of all partial functions from $X$ to $Y$ is itself a set. Tao's hint is to use the following four results from set theory exposed in his textbook: Lemma 3.4.9. Let $X$ be a set. Then there exists a set $\{Y \, : \, Y \text{ is a subset of } X\}$ . It is denoted $2^X$ . Axiom 3.10. Power set axiom: let $X$ and $Y$ be sets. Then there exists a set, denoted $Y^X$ ,  which consists of all the functions from $X$ to $Y$ . Axiom 3.6. Replacement axiom. Axiom 3.11. Union axiom: let $A$ be a set, whose all elements are themselves sets. Then there exists a set $\bigcup A$ whose elements are those objects which are elements of elements of $A$ , i.e., $x \in \bigcup A$ iff $x \in S$ for some $S \in A$ .
A consequence: if one has some set $I$ , and for each element $\alpha \in I$ we have one set $A_\alpha$ , then we can form the union set $\bigcup_{\alpha \in I} A_\alpha$ by defining: $\bigcup_{\alpha \in I} A_\alpha := \bigcup \{ A_\alpha \, | \, \alpha \in I\}$ . There are some very complete solutions out there, e.g. here . My sketch of proof is much shorter, thus I think that there are many errors in it. Here it is: Let be $X' \subseteq X$ and $Y' \subseteq Y$ . If both $X'$ and $Y'$ are fixed, then per the power set axiom (3.10), there exists a set $Y'^{X'}$ which consists of all the functions from $X'$ to $Y'$ . By lemma 3.4.9, there exist a set $2^X$ which consists of all the subsets of $X$ , and a set $2^Y$ which consists of all the subsets of $Y$ . Now we fix an element $X'$ of $2^X$ . Let be $Y'$ an element of the set $2^Y$ , $f$ a function, and $P$ the property `` $P(Y', f)$ : $f$ is a function from $X'$ to $Y'$ ''. Per the replacement axiom, there exists a set $\{f \, | \, P(Y', f) \text{ is true for some } Y' \in 2^Y\} = \{f \, | \, f: X' \rightarrow Y' \text{ for some } Y' \in 2^Y\}$ . This set is related to a fixed subset $X' \subseteq X$ , so let's denote this set $S_{X'} = \{f \, | \, f: X' \rightarrow Y' \text{ for some } Y' \in 2^Y\}$ . Now we apply the union set (3.11), especially in its second formulation. If we denote $I = 2^X$ , then for each element $X' \in I$ we do have one set $S_{X'}$ , which is defined above. Thus, there exists a set $\bigcup_{X' \in 2^X} S_{X'} := \bigcup \{S_{X'} \, | \, X' \in 2^X\}$ . And, for every function $f$ , we have $f \in \bigcup \{S_{X'} \, | \, X' \in 2^X\}$ iff there exists $X' \in 2^X$ such that $f \in S_{X'}$ , i.e. if there exists $X' \subset X$ and $Y' \subset Y$ such that $f: X' \rightarrow Y'$ . Consequently, we have proved that there exists a set which consists of the collection of all partial functions from $X$ to $Y$ . What does make this proof incomplete and/or incorrect? Thanks!","['functions', 'solution-verification', 'set-theory']"
3730091,Hall's theorem usage,"I got this question from my professor and I have no idea how to solve this, any help would be appreciated ""12 politicians give speech every day. Everyone of them uses the same set of 12 arguments. They use different one every day and make sure they won't use the same argument as somebody else already used that day. Using Hall's theorem prove that everyone of them will be able to take part in the 9th day of election without using the same argument twice or using an argument that their opposition used that day""","['graph-theory', 'matching-theory', 'discrete-mathematics']"
3730153,Are the null hypothesis and alternative hypothesis mutually exclusive?,"I am having a little bit of trouble understanding the concept of p-values. On the one hand, we are saying that the null hypothesis is a hypothesis we set, and the alternative hypothesis is something we would like to prove. In that case, how does it make sense to say that the p-value is the probability of the alternative hypothesis being true given that the null hypothesis is true if from the above definition it seems that both are mutually exclusive?","['p-value', 'statistical-inference', 'statistics', 'hypothesis-testing']"
3730178,Solve: $f''(x) = f^{-1}(x)$,I found this differential equation $$ f''(x) = f^{-1}(x) $$ which is interesting to me. I wonder if there is a calculus oriented way to solve this differential equation. Is there some method to to solve differential equations containing inverse of a function ?,"['golden-ratio', 'ordinary-differential-equations', 'inverse-function', 'calculus', 'polynomials']"
3730180,A question on Triangle Inequality in $\mathbb{R}^n$,"I'm reading a textbook on Topology. We know that $(\rho,\mathbb{R}^n)$ is a metric space, where $$\rho(x,y)=\sqrt{\sum_{i=1}^n(x_i-y_i)^2}$$ for any $x=(x_1,x_2,\ldots,x_n),y=(y_1,y_2,\ldots,y_n)\in\mathbb{R}^n$ . When proving that $\rho(x,z)\le \rho(x,y)+\rho(y,z)$ , the author uses Schwarz Inequality. I can understand the method, but I wonder if we can do it directly. We know that three non-collinear points can determine a plane. If those three points $x,y,z$ are on a single line, then of course we can apply the Triangle Inequality on $\mathbb{R}$ ; if they are not, then they are on a same plane, still we can apply the Triangle Inequality. Isn't it just a question on $\mathbb{R}^2$ essentially? Maybe I'm missing something, but I can't find it myself. Is my reasoning correct? Thank you!","['euclidean-geometry', 'metric-spaces', 'geometry', 'real-analysis', 'general-topology']"
3730184,Counterexample for 2-22 of Spivak's Calculus on Manifolds?,"If $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ and $D_2f = 0$ , show that $f$ is independent of the second variable. I was thinking of ways to show this, when I came across what I think might be a counterexample. Possible counterexample: Consider the function $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ defined by $$f(x,y) =
\begin{cases} 
x & \text{ if $y \geq 0$} \\
x^2 & \text{ if $y < 0$.} \\
\end{cases}$$ Then $D_2f = 0$ , but $f(x,1) = x, f(x,-1) = x^2 \Rightarrow f(x,1) \neq f(x,-1)$ , showing that $f$ is not independent of the second variable. Am I missing something here? It seems like the above theorem should work, i.e. that $f$ is independent of the second variable, but the counterexample seems convincing enough that I'm afraid I might have overlooked something. An idea that just came to be is that $\lim_{y\rightarrow 0^-} \frac{f(x_0,y)-f(x_0,0)}{y-0}=\infty$ , which does not equal to $\lim_{y\rightarrow 0^+} \frac{f(x_0,y)-f(x_0,0)}{y-0} = 0$ . Does it sound right?","['multivariable-calculus', 'calculus', 'derivatives']"
3730208,How to solve arithmetic problems involving infinite sets of integers such as this one?,"Let $A\subset \mathbb{N}$ be an infinite set. Let $N = 10^{2020}$ . Prove that : $$ \exists(n,m)\in A^2,\quad \exists p \;\text{prime} \geq N,\quad p|n+m$$ I couldn't manage to solve this problem. How to do it? I thought of supposing that $\forall(n,m)\in A^2,\quad \forall p \;\text{prime}\quad p|n+m \quad \Rightarrow \quad p <N$ and see where it ends but i'm not sure how to continue. Could you help me? Thanks","['elementary-number-theory', 'additive-combinatorics', 'discrete-mathematics']"
3730237,Find the values of $a$ and $b$ if $\lim_{x\to -\infty}$ $\sqrt{x^2-x+1} + ax - b = 0$?,"I took out x from the square root and reached the following expression, $$\lim_{x\to -\infty} x\sqrt{1-\frac{1}{x}+\frac{1}{x^2}} + ax - b = 0$$ then I separated the part of the expression which contains x from b and tried evaluating its limit, giving me the following expression, $$\lim_{x\to -\infty} \frac{\sqrt{1-\frac{1}{x}+\frac{1}{x^2}}+a}{\frac{1}{x}} $$ Now, the problem is that I am unable to proceed from here. What I have read and learnt till now tells me that since the denominator of this expression tends to 0 the numerator must also tend to 0 for the limit to exist as a finite number, but I don't understand why only 0? Can't it be something else? Can someone please explain me the approach to solving such questions and the reason behind why it works? And it would be great if you could tell me the values of a and b in this case. Note : Assuming that the numerator must also tend to 0 for the limit to be finite, and then applying L'Hopital's rule I got the values of a and b as -1 and -1/2 respectively. The only thing is that I don't have the answer of this question with me, so it would be great if you could tell me what I am getting is right or wrong.","['indeterminate-forms', 'limits', 'calculus']"
3730254,"Function defined ""over"" a set or ""on"" a set?","Is the sentence Given an Euclidean space $X$ , over which a function $f: X → \mathbb R$ is defined ... correct, or do I write Given an Euclidean space $X$ , on which a function $f: X → \mathbb R$ is defined ... in my publication? With Since a function is defined on its entire domain, ... https://en.wikipedia.org/wiki/Domain_of_a_function seems to favour the second, while I saw the first in quite a few of places at math.stackexchange.com, e.g. What is the measure of functions defined over $\overline{\mathbb{Q}}$ . EDIT: https://english.stackexchange.com/questions/117234/function-defined-on-over-the-set-a suggests that it would be ""on"" if $f$ is defined on arbitrary set and ""over"" if the function is defined over sort of (algebraic) structure (or strictly speaking over the underlying set), where the structure is somewhat ""respected"" by the function. So it might be be Given an Euclidean space $X$ , over which a function $f: X → \mathbb R$ is defined  ... Given a set $X$ , on which a function $f: X → \mathbb R$ is defined ... What is to make of this (as mathematicians, instead of linguists ;-))?","['functions', 'terminology']"
3730263,do Carmo's Riemannian Geometry exercise 1.4(a) - Metric of Lobatchevski Geometry,"Consider the following problem: A function $g:\mathbb R\to\mathbb R$ given by $g(t)=yt+x$ , $t$ , $x$ , $y\in\mathbb R$ , $y>0$ , is called a proper affine function. The subset of all such functions with respect to the usual composition law is a Lie group $G$ . As a differentiable manifold $G$ is simply the upper half-plane, that is, $\{(x,y)\in\mathbb R^2|y>0\}$ with the usual differentiable structure. Prove that: (a) The left-invariant Riemannian metric of $G$ which at the neutral element $e=(0,1)$ coincides with the Euclidean metric ( $g_{11}=g_{22}=1,g_{12}=0$ ) is given by $g_{11}=g_{22}=\frac{1}{y^2},g_{12}=0$ . This question already has an answer here , but I did not understand the argument. Also, I took another path and I would like to know if what I did is reasonable and how to finnish the question following this line of thought. Here is my attempt: Note that the tangent space of $G$ at the point $g \in G$ is nothing but $\mathbb R^2$ . The parametrization $x$ is just the identity of $\mathbb R^2$ . Then $$
d x_g(1, 0) = (1, 0), \quad d x_g(0, 1) = (0, 1)
$$ for all $g \in G$ and therefore $$
g_{ij}(0, 1) = \delta_{ij},
$$ since $\langle \cdot, \cdot \rangle_e$ coincides with the euclidean inner product. Now, for $g \in G$ we have \begin{align*}
g_{11}( x^{-1}(g)) = & \langle d x_g(1, 0), d x_g(1, 0) \rangle_g \\
 = & \langle d (L_{g^{-1}})_g(1, 0), d (L_{g^{-1}})_g(1, 0) \rangle_e
\end{align*} I am stuck on how to compute the derivative $d(L_{g^{-1}})_g$ . Any hints will be the most appreciated. Thanks in advance and kind regards.","['manifolds', 'riemannian-geometry', 'lie-groups', 'differential-geometry']"
3730290,Unfolding of recurrence in generalization of Josephus problem,"I have been going through the bit-wise generalization of Josephus problem in Concrete Mathematics. And so the author came up with these relations (I do understand this part) $$f(1) = α ;$$ $$f(2n + j) = 2f(n) + β_j ,$$ $$\text{ for } j = 0, 1 \text{ and } n \geq 1$$ this part I understand , now the unfolding of the recurrence However, I can't understand how the unfolding happens and why do we have two and four in the beginning of the second and third equation. \begin{align*}f(b_m b_{m−1}...b_1 b_0)_2 &= 2f(b_m b_{m−1} . . . b_1)_2+ β_{b_0}\\&=4f(b_mb_{m−1}...b_2)_2 + 2β_{b_1} + β_{b_0}\\&=
2^mf((b_m)_2) +2^{m−1}β_{b_{m−1}} + · · · +2β_{b_1}+β_{b_0}
\\&=2^mα + 2^{m−1}β_{b_{m−1}} + · · · + 2β_{b_1} + β_{b_0}\end{align*} Can someone shed more light how to evaluate that?","['binary', 'recurrence-relations', 'discrete-mathematics', 'algorithms']"
3730299,Derivative when function approaches infinity.,"Let $f$ be a continous and differentiable function in it's natural domain. If $$\lim_{x\to a-}f(x)=\infty$$ , is it always true, that $$ \lim_{x\to a-} f'(x) \ge0 $$ if the limit exists (or is infinite)? Intuitively this seems to be true, but can this be proven exactly?","['limits', 'derivatives', 'infinity']"
3730356,Prove that there exists and angle $\alpha$ and $r \in \Bbb R$ such that $a\cos x + b\sin x = r\cos\alpha$,"Let's say that we have an expression $a\cos x + b\sin x$ where $a \in \Bbb R$ and $b \in \Bbb R$ . I was learning about finding the minimum and maximum values of an expression of this form for some given value of $a$ and $b$ by expressing it in terms of a single trigonometric function. My textbook did it by assuming that $a = m\sin\phi$ and $b = m\cos\phi$ , where $m \in \Bbb R$ and $\phi$ is some angle. But I couldn't wrap my head  around the fact that any two real numbers can be expressed as the product of another real number and a trigonometric function for some angle. So, I decided to take another approach which is highly similar to this one. It is solely based on the assumption that the expression can be expressed in the form of $r\cos\theta$ , where $r \in \Bbb R$ and $\theta$ is some angle. Once this assumption is proved, here's how I will continue it : $$a\cos x + b\sin x = r\cos\theta$$ Let's say that $\theta = \alpha + x$ . So : $$a\cos x + b\sin x = r\cos(\alpha + x) = (r\cos\alpha)\cos x + (-r\sin\alpha)\sin x$$ This gives us the values of $a$ and $b$ as $r\cos\alpha$ and $-r\sin\alpha$ respectively. So, it would work perfectly if I can prove the assumption mentioned above. Unfortunately, I haven't been able to prove it yet. I was successful in proving it's converse, though i.e. for a given expression, say $p\cos\gamma$ , where $p \in \Bbb R$ and $\gamma$ is some angle, it can be expressed in the form of $c\cos\delta + d\sin\delta$ where $c \in \Bbb R$ , $d \in \Bbb R$ and $\delta$ is some angle. This is highly similar to what I've stated above (what I'd do once the assumption is proved). First, we assume that $\gamma = \beta + \delta$ , where $\beta$ and $\delta$ are two angles that fit in the equation. $$\therefore p\cos\gamma = p\cos(\beta + \delta) = p(\cos\beta\cos\delta - \sin\beta\sin\delta) = (p\cos\beta)\cos\delta + (-p\sin\beta)\sin\delta$$ Substituting $p\cos\beta$ by $c$ and $-p\sin\beta$ by $d$ , we can arrive at $c\cos\delta + d\sin\delta$ . I don't know if this will be helpful in proving the initial assumption that an expression $a\cos x + b\sin x$ can be expressed as $r\cos\theta$ for some angle $\theta$ and for some real value of $r$ . I'd really appreciate help in proving this. Thanks! PS : I'm not familiar with Euler's formula","['trigonometry', 'proof-writing']"
3730370,The matrix norm of the identity matrix disturbed by a small matrix,"So I am consider the norm of matrix $\|I-C\|_2$ , where $C$ is a positive definite matrix with a very small norm, what can we say about $\|I-C\|_2$ ? Like, is it smaller than $1$ ? Or can I express it w.r.t $C$ ? Thank you in advance!","['spectral-norm', 'matrices', 'matrix-norms', 'linear-algebra', 'symmetric-matrices']"
3730386,Find generating function for $F_{2n}$,"Given that $F(x)=\sum_{n=0}^\infty F_nx^n= \frac{x}{1-x-x^2}$ , where $F_n(x)$ is the $n^{th}$ term of the Fibonacci series, and $F(x)$ is the generating function associated to it, find the generating function associated to $F_{2n}$ I know that $F_{2n}=F^2_n+2F_nF_{n-1}$ but this doesn't seem to help much. How can I do this?","['fibonacci-numbers', 'discrete-mathematics', 'generating-functions']"
3730423,The existence of group isomorphism between Euclidean space.,"Is there any group isomorphism for addition $\mathbb{R}^n$ to $\mathbb{R}^m$ where $n\neq m$ ? I could prove that there exists any vector space isomorphism or smooth map, but I still could not know that if we consider only abelian group structure for the addition of them, then the group isomorphism between them exits or not.","['free-abelian-group', 'group-theory', 'abstract-algebra', 'abelian-groups']"
