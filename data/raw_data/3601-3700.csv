question_id,title,body,tags
25876,Probability of 3 people in a room of 30 having the same birthday,I have been looking at the birthday problem (http://en.wikipedia.org/wiki/Birthday_problem) and I am trying to figure out what the probability of 3 people sharing a birthday in a room of 30 people is. (Instead of 2). I thought I understood the problem but I guess not since I have no idea how to do it with 3.,"['birthday', 'probability']"
25883,"Equivalence of Rolle's theorem, the mean value theorem, and the least upper bound property?","How to show that Rolle's theorem, the Mean Value Theorem are equivalent to the least upper bound property? I'm thinking of starting like this: Let F be an ordered field that does not satisfy the least upper bound property, and then deduce that F does not satisfy either Rolle's or MVT. But I'm not sure how to continue, any help please? thanks!",['real-analysis']
25885,How to prove that square-summable sequences form a Hilbert space?,"Let $\ell^2$ be the set of sequences $x = (x_n)_{n\in\mathbb{N}}$ ( $x_n \in \mathbb{C}$ ) such that $\sum_{k\in\mathbb{N}} \left|x_k\right|^2 < \infty$ , how can I prove that $\ell^2$ is a Hilbert space (with dot-product $\left(x,y\right) = \sum_{k\in\mathbb{N}} x_k\overline{y_k}$ ). This is a standard textbook exercise: apparently this is easy and, even to me, it seems self-evident. However, I don't know what to do with the infinite sum.","['hilbert-spaces', 'sequences-and-series']"
25886,Using proof by cases -- stuck,"Let $n$ be an integer.
  If $3$ does not divide $n$, then $3$ divides $n^2 - 1$. I'm trying to prove this using a ""proof by cases"". However, I'm lost as to how to start. I thought proof by cases started by making a logical expression, but I can't seem to find the right one. Thanks","['logic', 'discrete-mathematics']"
25903,Derive Fourier transform of sinc function,"We know that the Fourier transform of the sinc function is the rectangular function (or top hat). However, I'm at a loss as to how to prove it. Most textbooks and online sources start with the rectangular function, show that $$\int_{-\infty}^\infty \text{rect}(x)e^{i\omega x}dx=\int_{-1/2}^{1/2}e^{i\omega x}dx=\left.\frac{e^{i\omega x}}{i\omega}\right\vert_{-1/2}^{1/2}=\text{sinc}(\omega/2)$$ and then just invoke duality and claim that the Fourier transform of the sinc function is the rectangular function. Is there any way of deriving this directly? i.e., starting with the sinc function? I've tried, but I'm not sure as to how to proceed. I know that the sinc is not Lebesgue integrable and only improper Riemann integrable. Some vague recollection of these being important in the Fourier transform hinders my thought process. Can someone clear things up for me?","['fourier-analysis', 'integration']"
25914,"Proof by Strong Induction: $n = 2^a b,\, b\,$ odd, every natural a product of an odd and a power of 2","Can someone guide me in the right direction on this question? Prove that every $n$ in $\mathbb{N}$ can be written as a product of an odd integer and a non-negative integer power of $2$ . For instance: $36 = 2^2(9)$ , $80 = 2^4(5)$ , $17 = 2^0(17)$ , $64 = 2^6(1)$ , etc... Any hints in the right direction are appreciated (please explain for a beginner).
Thanks.","['induction', 'elementary-number-theory', 'divisibility', 'discrete-mathematics']"
25925,Nested radicals,Let $S$ be the set of functions $f:\mathbb{R}\to \mathbb{R}$ such that $\sqrt{f(1)+\sqrt{f(2)+\sqrt{f(3)+\dots}}}$ converges. A function $q(x)$ dominates $p(x)$ if there exist an m such that $q(x)\gt p(x)$ for all $x\gt m$. Take all functions $f(x)$ from $S$ and put $O(f(x))$ in $S2$. Which function $g(x)$ in $S2$ dominates all others? Are there asymptotic lower and upper bounds on $g(x)$?,"['nested-radicals', 'convergence-divergence', 'sequences-and-series', 'functions']"
25927,How to show that the following eigenvectors have to be orthogonal? [duplicate],"This question already has answers here : Eigenvectors of real symmetric matrices are orthogonal (7 answers) Closed 3 years ago . I have the following problem: Suppose that $A$ is a symmetric matrix, with $A$ = $A^{T}$ . Suppose $\vec{v}$ and $\vec{w}$ are eigenvectors of $A$ associated
with distinct eigenvalues. Show that $\vec{v}$ and $\vec{w}$ must be orthogonal.
(Hint: Show that a$\vec{v}$ $\cdot$ $\vec{w}$ = $\vec{v}$ $\cdot$ b$\vec{w}$.) I am unsure how to approach this, even with the hint taken into account. I tried to use the fact that orthogonal complement of Im(A) is in Ker of A transpose, and since they are equal it is also in ker A, but that didn't get me anywhere (I am probably thinking in the wrong direction). Thanks in advance for your hints!",['linear-algebra']
25935,Example of Convergent Series,"Can anyone think of sequences $\{a_n\}$, $\{b_n\}$ such that $\sum a_n$ diverges, ${b_n}\to\infty$, but $\sum a_nb_n $ converges? Thank you. Note that $\{a_n\}$ must have infinitely many positive terms and infinitely many negative terms. Edit: I get the feeling that only Qiaochu Yuan could answer this one... ;)",['sequences-and-series']
25948,Order of a particular given permutation = LCM(order of all disjoint cycles) ?,"If I write (1 3 4 6)(2 3 4)(4 6 1) as a product of disjoint cycles, I get (164)(23), is it true that order of (1 3 4 6)(2 3 4)(4 6 1) is lcm(2,3)=6 (just the order of its disjoint cycles)?",['group-theory']
25963,Help with the proof of a Gaussian type inequality and some numerical results,"I am trying to figure out if I made a mistake in the following proof - in particular I have been trying to verify the inequality (*) below.  I have attached my proof but I have been getting some error messages in Mathematica that make me think something is wrong when I try to compare the left hand side of the inequality to the right hand side.  The background of this problem is related to the study of moments of self similar processes and I have no idea if the estimates are valid besides from some crude numerical calculations for $N= 2,3,4$ and all $H_i = 1/2$.  I have attached the proof for the case N =2. 1) Did I make a mistake in one of the estimates or applying fubini somewhere?  In particular I am starting to worry about line (**) is breaking the integral up like this and then using $x_1 < x_2$ and $x_2 < x_1$ in the respective regions of integration to arrive at the estimate on the next line valid? Let $H_1 \in (0,1), c$ a positive constant and let $\Phi(t) = \int_{t}^{\infty} e^{\frac{-x^2}{2}}dx$.  Then we have the following inequality (*) $\int_{1}^{\infty} \int_{1}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2 \leq 2 \Phi(1) \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} }dx_2$ Proof: $\int_{1}^{\infty} \int_{1}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2$ (**) $ = \int_{1}^{\infty} \int_{1}^{x_2}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2 
+ \int_{1}^{\infty} \int_{x_2}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2 $ $ \leq \int_{1}^{\infty} \int_{1}^{x_2}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} } dx_1 dx_2 + \int_{1}^{\infty} \int_{x_2}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c}  |x_1|^{1+H_1} } dx_1 dx_2 $ $ =  \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} } (\int_{1}^{x_2} e^{-\frac{ x_{1}^{2}}{2}}dx_1)dx_2 + \int_{1}^{\infty}  e^{ -\frac{ x_{1}^{2}}{2} + \frac{1}{c} |x_1| ^{1+H_1} } (\int_{x_2}^{\infty} e^{-\frac{ x_{2}^{2}}{2}}dx_2)dx_1 $ $ \leq 2  \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} } (\int_{1}^{\infty} e^{-\frac{ x_{1}^{2}}{2}}dx_1)dx_2 
 = 2 \Phi(1) \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} }dx_2$ 2)I have tried comparing the LHS to the right hand side for the case $N = 2, N= 4$ and all H_i = 1/2 but Mathematica gives me the following errors attached below.  I am thinking I will just need to test the above inequality's for  $N > 3$ in matlab but I am not sure what the easiest method would be.  Would monte carlo integration apply to these type of integrals even though they are over an unbounded region?  Any numerical suggestions/references are greatly appreciated NIntegrate[ 
 2*(1/Sqrt[2*Pi])^2*Exp [1/2 (-x ^2 - y^2)] + 1/10*x*y^(1/2), {x, 0, 
  Infinity}, {y, 0, Infinity}] NIntegrate::slwcon: Numerical integration converging too slowly; suspect one of the following: singularity, value of the integration is 0, highly oscillatory integrand, or WorkingPrecision too small. >> NIntegrate::ncvb: NIntegrate failed to converge to prescribed accuracy after 18 recursive bisections in x near {x,y} = {4.557658617658054*10^1348705,0.184661}. NIntegrate obtained 1.541751650430305 15.954589770191005*^88741243 and 3.653228868630915 15.954589770191005*^88741243 for the integral and error estimates. >> 0.*10^88741243 NIntegrate[
 2*(1/Sqrt[2*Pi])^4 Exp [
   1/2 (-x^2 - y^2 - z^2 - w^2) + 
    1/10 x Sqrt[y] z^(1/4)* w^(1/8)], {x, 0, [Infinity]}, {y, 
  0, [Infinity]}, {z, 0, [Infinity]}, {w, 0, [Infinity]}] NIntegrate::slwcon: Numerical integration converging too slowly; suspect one of the following: singularity, value of the integration is 0, highly oscillatory integrand, or WorkingPrecision too small. >> NIntegrate::eincr: The global error of the strategy GlobalAdaptive has increased more than 2000 times. The global error is expected to decrease monotonically after a number of integrand evaluations. Suspect one of the following: the working precision is insufficient for the specified precision goal; the integrand is highly oscillatory or it is not a (piecewise) smooth function; or the true value of the integral is 0. Increasing the value of the GlobalAdaptive option MaxErrorIncreases might lead to a convergent numerical integration. NIntegrate obtained 0.1321276729737014 and 4.5493840913708495 *^-7 for the integral and error estimates. >> 0.132128","['numerical-methods', 'calculus', 'real-analysis', 'analysis']"
25966,If $f(\frac {x} {x+1})=x^2$ then $f(x)=?$,"$$f \left (\frac{x}{x+1} \right) = x^2 \implies f(x)=\;?$$ I encountered this exercise, and and don't know how to solve it. In what category of math does this belong? In what book/website I can study/exercise myself? thank you very much.","['calculus', 'functions']"
25977,Prove a property holds on a metric space if it holds on a dense subset,"For the past two weeks, I've tried to prove two different results that hold the same structure: Suppose a property that holds for a dense subset of a metric space.
Prove that it holds for the entire metric space. In this case, both questions were related to uniform convergence: i) Show that if $(f_{n})$ is a sequence of continuous functions on M, and if $\sum_{n=1}^{\infty}f_{n}$ is converges uniformly on a dense subset A of M, then the series converges uniformly on all of M. ii) Let A be a dense subset of M. If $(f_{n})$ is a sequence of continuous functions on M, and if the sequence converges uniformly on A, prove that $(f_{n})$ converges uniformly on M. My questions are the following: What is the framework I should have in my mind when trying to prove these kinds of properties? What is the basic frame of mind you have when proving such results? Because I feel that I'm missing something, that is, my intuition about how a dense subset works is not strong enough. By the way, I'm sorry for the wall of text :) Thanks in advance!","['real-analysis', 'analysis']"
25978,Existence of a measure that preserves a given mapping,"Let $(X, d)$ be a compact metric space and let $T:X \to X$ be a continuous mapping. Now, does there exist a probability measure $\nu$ such that $T_* \nu = \nu$ (the first thing is the image measure)? Now I want to do this using a fixed-point theorem. So I start like this: Define the operator $\psi_T$ from $C(X)$ into itself by $\psi_T f = f \circ T$. Now we also know that $C(X)^* = P(X)$ where $P(X)$ are the Borel probability measures on $X$. So we have a mapping $\psi_T^* : P(X) \to P(X)$. Further we have that: $$\langle f, \underbrace{\psi_T^* \mu}_{= \nu} \rangle = \langle \psi_T f, \mu \rangle = \int f \circ T \, d\mu.$$
And $\nu$ is completely determined by
$$\int f \, d\nu = \int f \circ T \, d\mu$$ Further, since $X$ is compact I know that $P(X)$ is compact (hence complete) with respect to the Bounded Lipschitz metric. This one is given by
$$d_\text{BL}(\mu, \nu) := \sup \left \{ \left | \int f \, d\mu - \int f \, d\nu \right | : f \in \text{BL}(X,d), \|f\|_\text{BL} \leq 1 \right \}.$$ So everything is fine for the Banach fixed point theorem except one thing: is $d_\text{BL}$ a contraction? I don't think so. Does someone have an idea if I'm on the right track or does someone have a suggestion how to continue? By the way, this is homework.",['measure-theory']
25982,A question on continued fraction,"Let $a$ be a positive irrational number. Let $p_k/q_k, p_{k+1}/q_{k+1}$ be two consecutive
convergents of its simple continued fraction, where $k\ge 1$. Is it possible that both $$|a-(p_k/q_k)|<1/(2q_k^2)$$ and $$|a-(p_{k+1}/q_{k+1})|<1/(2q_{k+1}^2)$$
are true? I can only prove that at least one of these inequalities is true.","['continued-fractions', 'number-theory']"
25983,When is the vector space of continuous functions on a compact Hausdorff space finite dimensional?,I know that the vector space of all real valued continuous functions on a compact Hausdorff space can be infinite dimensional.  When will it be finite dimensional?  And how will I identify that vector space with $\mathbb{R}^n$ for some $n$?,['general-topology']
26007,Chebyshev's versus Markov's inequality,More important than knowing inequalities by hearts is knowing when and how to apply them and what they express. Regarding Chebyshev's and Markov's inequality. What is the relation (if any) between them? Which one is more strict (and in which situation)? Is there an easy way to understand what they express (kind of like drawing a triangle for the triangle inequality)? What is a typical application in probability theory? What are applications outside of probability theory?,"['probability-theory', 'inequality', 'probability']"
26012,Measure Theory and Integrals of Characteristic Functions,"Given two sets of finite measure in $\mathbb{R}$ say, $E$ and $F$, and their characteristic functions $\chi_E$ and $\chi_F$, can somebody show that $\chi_E\ast\chi_F(x)$ (the convolution) is a continuous function of $x$?  This is a qual problem from an old qual that I'm studying, and I cannot figure it out.  If we were dealing with continuous functions or mollifiers or something it would be straightforward, but what if the sets $E$ and $F$ are somehow pathological, like the Cantor set, or something like that? Thanks!","['measure-theory', 'functional-analysis']"
26014,Proof that retract of contractible space is contractible,"I'm reading Hatcher and I'm doing exercise 9 on page 19. Can you tell me if my answer is correct? Exercise: Show that a retract of a contractible space is contractible. Proof: Let $X$ be a contractible space, i.e. $\exists f :X \rightarrow \{ * \}$ , $g: \{ * \} \rightarrow X$ continuous such that $fg \simeq id_{\{ *\}}$ and $gf \simeq id_X$ and let $r:X \rightarrow X$ be a retraction of $X$ i.e. $r$ continuous and $r(X) = A$ and $r\mid_A = id_A$ . ( Edited using Anton's answer) Define $f^\prime := f\mid_A$ and $g^\prime := r \circ g$ . Now we need to show $f^\prime \circ g^\prime \simeq id_{ \{ * \} }$ and $g^\prime \circ f^\prime \simeq id_A$ . $f^\prime \circ g^\prime \simeq id_{ \{ * \} }$ follows from $f^\prime \circ g^\prime = id_{ \{ * \} }$ (because there is only one function $\{ * \} \rightarrow \{ * \}$ ). From $gf \simeq id_X$ we have a homotopy $H: I \times X \rightarrow X$ such that $H(0,x) = g \circ f (x)$ and $H(1,x) = id_X$ . From this we build a homotopy $H^\prime : I \times A \rightarrow A$ by defining $H^\prime := r \circ H \mid_{I \times A}$ . Then $H^\prime (0,x) = g^\prime \circ f\mid_A (x)$ and $H^\prime (1,x) = id_A$ . I'm particularly dis satisfied with the amount of detail in my reasoning but I can't seem to produce what I'm looking for. Many thanks for your help!","['general-topology', 'homotopy-theory', 'algebraic-topology']"
26021,How to convert a powerseries into a Dirichlet series?,"[Update 3] : I realized, that the series $s(h)$ below is simply a ""(general) Dirichlet-series"" (1), so after I know the principle how to find the Taylor-series for some function (which may be given by its Dirichlet-series) I can now make my question more precise as: how to find the Dirichlet-series-representation of a function, given its Taylor-series? (1) - the attribute ""general"" means a series $ \sum_{k=1}^{\infty} a_k*e^{- \lambda _k s}$ where the $\lambda _k$ form a strictly monotone sequence of real numbers increasing to $+\infty$ (K. Knopp, ""Infinite series..."", german ed.) (I did not change the text below) [end update 3] In the study of functional iteration and the Schröder function I'm concerned with series of the type:
$$ s(h)= a_0* (u^0)^h + a_1* (u^1)^h + a_2 * (u^2)^h + ... $$ 
where h is the (iteration-) ""height""-parameter. Usually, if this converges, in the numerical evaluation I change order of computation:
$$ s(h)= a_0*(u^h)^0 + a_1*(u^h)^1 + a_2*(u^h)^2 + ... $$ 
which means in many cases I handle this as a power series in $u^h$ . But when I considered further analysis, for instance the derivative with respect to h , I differentiate using $h$ as highest exponent. (BTW, what is the name of that type of series?) Now by accident I asked Pari/GP for the coefficients of $s(x)$ and what I got back was a power series in x . I never thought deeper about such a conversion, except that I remember the case of the conversion of the zeta-series into a power series using the Stieltjes constants. Example: s(h) = 2*1^h -1.09662*1/4^h + 0.100215*1/16^h  -0.00366327*1/64^h 
     + 0.0000717362*1/256^h  -0.000000874084*1/1024^h + ... - ... 
ps(x) = 1.00000 + 1.25723*x - 0.699161*x^2 + 0.172874*x^3 + 0.0350749*x^4    
     - 0.0550780*x^5 + 0.0288564*x^6 - 0.00942667*x^7 + O(x^8) Well, I think, what Pari/GP internally does is to compute the derivatives of $s(h)$ and construct the Taylor series. But now I'm curious (and that's my question): How could I convert a power series ps(x) into a series of the s(h) -type? [update] I think the comment to Mitch would be a good additional background and explanation for the question, so I copy it to here and extend it a bit I'm originally interested in the method : how one would do such a conversion of $ps(x)$ to $s(h)$ ? Note that some values for $s(h)$ are $s(1) \approx 1.73205$ , $s(2) \approx 1.93185$. My example comes from iteration of the function $$f(x) = \sqrt{2+x}$$ $$f(f(x))=\sqrt{2+\sqrt{2+x}} $$  $$f(f(f(x))) =\sqrt{2+\sqrt{2+\sqrt{2+x}}} $$ and so on. Now $f(x)$ has a power series in $x$, as well as $f(f(x))$ and for each iterate there is another power series. The method of Schröder functions applied to a recentered version of $f(x)$, which has no constant term (also called ""regular iteration""), allows to find a power series $F(x,h)$ in two variables where $h$ means the iteration-""height"". In such a function $F(x,h)$  the coefficients at powers of $x$ are polynomials in $\lambda^{ h}$ (where $\lambda$ is an eigenvalue of the function which I usually denote as $u$ for ASCII-readability). If I set $x=1$ and reorder summation collecting like powers of $\lambda^h $ I get a series in $\lambda^h$ only which I called $S(h)$ This function gives exactly the value of the h 'th iterate of $f(x)$ beginning at x=1 simply meaning $  s(h) = F(1,h) $ But $s(h)$ has not the usual form of a power series in $h$ as shown above. When I asked Pari/GP for an evaluation at some h I accidentally typed $s(x)$ and Pari/GP gave back a power series in x , I called it above $ps(x)$. $s()$ and $ps()$ have completely different forms but give the same result: $s(h) = ps(h)$ where both functions converge. The $s(h)$ as well as the $ps(x)$-form allow also continuous (i.e fractional) iteration giving the same values. Interestingly the behave of the convergence of the two forms is completely different. The limit for $h\to\infty$ is immediately visible in $s(h)$ but for $ps(h)$ that would likely be a divergent series, while for $h=0$ we see immediately that $ps(h)=1$ but would not recognize it by $s(h)$","['power-series', 'sequences-and-series', 'functional-analysis']"
26026,Bounding the integral of a $C^1$ function using its gradient,"Let $f \in C^1_c(\Omega)$ where $\Omega \subset \mathbb{R}^d$ is a bounded domain. Let $\phi \in C^1_c(\mathbb{R}^d)$ be an approximation of the identity (i.e. $\int_{\mathbb{R}^d} \phi=1$, $\phi \geq 0$, $\phi_\epsilon := \frac{1}{\epsilon^d} \phi(\frac{x}{\epsilon})$. How would you prove that $$\int_\Omega |f(x) - f \ast \phi_\epsilon(x)| dx \leq \epsilon \int_\Omega |\nabla f| dx?$$ I'm trying to show that the family of $C^1_c$ functions convolved with a mollifier is uniformly close to the function in $L^1$ (which would be true after having this result if we assume something like the family of functions being bounded in $W^{1,1}(\Omega)$).","['functional-analysis', 'real-analysis']"
26042,Understanding conditional independence of two random variables given a third one,"I am reading the Wikipedia article on conditional independence . There seems to be Two definitions for conditional independence of Two random variables $X$ and $Y$ given another one $Z$: Two random variables $X$ and $Y$ are
conditionally independent given a
third random variable $Z$ if and
only if they are independent in
their conditional probability
distribution given $Z$. That is, $X$
and $Y$ are conditionally
independent given $Z$ if and only
if, given any value of $Z$, the
probability distribution of $X$ is
the same for all values of $Y$ and
the probability distribution of $Y$
is the same for all values of $X$. Two random variables $X$ and $Y$ are
conditionally independent given a
random variable $Z$ if they are
independent given $\sigma(Z)$: the
$\sigma$-algebra generated by $Z$. Two events $R$ and $B$ are
conditionally independent given a
$\sigma$-algebra $\Sigma$ if
    $$\Pr(R \cap B \mid \Sigma) = \Pr(R \mid \Sigma)\Pr(B \mid
    \Sigma)\ a.s.$$ where $\Pr(A \mid
    \Sigma)$ denotes the conditional
expectation of the indicator
function of the event $A$, given the
sigma algebra $\Sigma$. That is, $$ \Pr(A \mid \Sigma) :=
    \operatorname{E}[\chi_A\mid\Sigma].$$ Two random variables X and Y are
conditionally independent given a
$\sigma$-algebra $\Sigma$ if the
above equation holds for all $R$ in
$\sigma(X)$ and $B$ in $\sigma(Y)$. I can understand the second definition, but my questions are: What does the first definitions mean
actually? I have tried several times
on reading it, but fail to get what
it means? Can someone rephrase it
using rigorous and clean language,
for example, by writing the definition in terms of some formulae? Do the two definitions agree with
each other? Why? ADDED: I was wondering if the following is the correct way to understand the first definition. Notice that $P(\chi_A \mid Z)$ is defined as $E(\chi_A \mid Z)$ and therefore is a random variable. When the conditional probability $P(\cdot \mid Z)$ is  ""regular"", i.e. when $P(\cdot \mid Z)(\omega)$ is a probability measure for each point $\omega$ in the underlying sample space $(\Omega, \mathcal{F}, P)$, does conditional independence between $X$ and $Y$ given $Z$ mean that $X$ and $Y$ are independent w.r.t. every probability measure defined by $P(\cdot \mid Z)(\omega), \forall \omega \in \Omega$? If yes, is the conditional probability $P(\cdot \mid Z)$ always  guaranteed to be ""regular""? So that there is no need to explicitly write this ""regular"" assumption? Thanks and regards!","['probability-theory', 'measure-theory']"
26052,Equidecomposability of a Cube into 6 Trirectangular Tetrahedra,"In grade school we learn that a square can be equidecomposed into two congruent right isosceles triangles. Does the following three dimensional generalization hold? Consider a trirectangular tetrahedron with vertices at $(0,0,0)$, $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$ and volume $\frac{1}{6}$. (It has three orthogonal right triangular faces and one equilateral triangular face opposite the (tri-right-angled) common vertex.) The inscribing cube of the tetrahedron has unit side length and unit volume. Is there a dissection of this cube into 6 tetrahedra of the above form? Provided that such a dissection exists and this is the way it is to be done, it's easy to see how 4 tetrahedra can be glued together to give the boundary of the cube, but I don't quite see how the other two should fit inside.",['geometry']
26053,"How to calculate a confidence interval for $p$, the parameter of the binomial distribution?","Suppose I have a random variable X that is known to follow the binomial distribution B, but whose parameter $p$ is unknown. I have observed 100 samples of X, and they all came out true. How can I calculate a 95% confidence interval for the value of $p$ (which stands for P(true)) of the underlying distribution B?","['statistics', 'probability']"
26061,Questions about the concept of strong Markov property,"I am trying to understand the concept of strong Markov property quoted from Wikipedia : Suppose that $X=(X_t:t\geq 0)$ is a
  stochastic process on a probability
  space
  $(\Omega,\mathcal{F},\mathbb{P})$ with
  natural filtration
  $\{\mathcal{F}\}_{t\geq 0}$. Then $X$
  is said to have the strong Markov
  property if, for each stopping time
  $\tau$, conditioned on the event
  $\{\tau < \infty\}$, the process
  $X_{\tau + \cdot}$ (which maybe needs
  to be defined) is independent from
  $\mathcal{F}_{\tau}:=\{A \in  \mathcal{F}: \tau \cap A \in  \mathcal{F}_t ,\, \ t \geq 0\}$ and
  $X_{\tau + t} − X_{\tau}$ has the same
  distribution as $X_t$ for each $t \geq  0$. Here are some questions that make me stuck: In $\mathcal{F}_{\tau}:=\{A \in
    \mathcal{F}: \tau \cap A \in
    \mathcal{F}_t ,\, \ t \geq 0\} $,
what does $\tau \cap A $ mean?
$\tau$ is a stopping time and
therefore a random variable and $A$
is a $\mathcal{F}$-measurable
subset, but what does $\tau \cap A$
mean? How is the process $X_{\tau + \cdot}$ defined from the process $X_{\cdot}$ ? Is it the translated
version of the latter by $\tau$? How is the conditional independence
between a process, such as $X_{\tau
    + \cdot}$,  and the sigma algebra, such as $\mathcal{F}_{\tau}$, given
an event, such as $\{\tau <
    \infty\}$, defined? Related question, is independence
between a random variable and a
sigma algebra defined as
independence between the sigma
algebra of the random variable and
the sigma algebra? Is ""$X_{\tau+ t}  − X_{\tau}$ has the
same distribution as $X_t$ for each
$t \geq 0$"" also conditional on the
event $\{\tau < \infty\}$? Thanks and regards!","['probability-theory', 'stochastic-processes']"
26071,When is the image of a linear operator closed?,"Let $X$ , $Y$ be Banach spaces. Let $T \colon X \to Y$ be a bounded linear operator.
Under what circumstances is the image of $T$ closed in $Y$ (except finite-dimensional image). In particular, I wonder under which assumptions $T \colon X \to T(X)$ is a bounded linear bijection between Banach spaces, so it is at least an isomorphism onto its image by bounded inverse theorem.","['functional-analysis', 'banach-spaces']"
26080,Why eliminate radicals in the denominator? [rationalizing the denominator] [duplicate],"This question already has answers here : Why rationalize the denominator? (15 answers) Closed 9 years ago . Why do all school algebra texts define simplest form for expressions with radicals to not allow a radical in the denominator. For the classic example, $1/\sqrt{3}$ needs to be ""simplified"" to $\sqrt{3}/3$. Is there a mathematical or other reason? And does the same apply to exponential notation -- are students expected to ""simplify"" $3^{-1/2}$ to $3^{1/2}/3$ ?","['education', 'algebra-precalculus', 'convention', 'field-theory']"
26089,"Finding the distance between two triangles, one inside the other",I have two right triangles One is a $6$-$8$-$10$ and inside is a $3$-$4$-$5$ and the space between the two triangles is a uniform amount. I made a really awkward and weird pic of the diagram and I need to solve for $X$ How would I go about solving this? I couldn't figure out any good approaches.,['geometry']
26105,Further questions about conditional independence of two random variables given a third one,"This post is a continuation of my previous one , but can be read as being stand-alone. I am trying to understand the concept of conditional independence of two random variables given a third one. Following is the definition that I think is the right one: Two random variables $X$ and $Y$ are
  conditionally independent given a
  random variable $Z$ if they are
  independent given $\sigma(Z)$: the
  $\sigma$-algebra generated by $Z$. Two events $R$ and $B$ are
  conditionally independent given a
  $\sigma$-algebra $\Sigma$ if
      $$\Pr(R \cap B \mid \Sigma) = \Pr(R \mid \Sigma)\Pr(B \mid \Sigma)\ a.s.$$ where $\Pr(A \mid \Sigma)$
  denotes the conditional expectation of
  the indicator function of the event
  $A$, given the sigma algebra $\Sigma$. Two random variables $X$ and $Y$ are
  conditionally independent given a
  $\sigma$-algebra $\Sigma$ if the above
  equation holds for all $R$ in
  $\sigma(X)$ and $B$ in $\sigma(Y)$. My questions are Is the definition equivalent to that
$X$ and $Y$ are conditional
independent given $Z$ iff $E(X
\times Y \mid Z) = E(X\mid Z) \times
E(Y \mid Z)$? If yes, can this be
proved from the definition, by that
any measurable function (here random
variable) can be pointwise
approximated by a sequence of simple
functions (a simple function is the finite linear combination of indicator functions), just as used in
definition of Lebesgue integral, and
that conditional expectation and taking pointwise limit can be exchangeable? Since both $E(X\mid Z)$ and $E(Y
\mid Z)$ are random variables, is
conditional independence between $X$
and $Y$ given $Z$ equivalent to that
$E(X\mid Z)$ and $E(Y \mid Z)$ are
independent random variables w.r.t.
the underlying probability measure
$P$ of the underlying sample space
$(\Omega, \mathcal{F}, P)$? Or are the two related somehow? Thanks and regards!","['probability-theory', 'measure-theory']"
26106,integration by partial fraction,"I can not figure out how to integrate 
$$\int\frac{3x^2+x+4}{x^3+x} \, dx.$$
I got as far as factoring the denominator which gives us $x(x^2+1)$. So from there I got
$$\frac{3x^2+x+4}{x(x^2+1)} = \frac{A}{x} + \frac{Bx+C}{x^2+1}$$
which would give us: $(3x^2 + x + 4) = A(x^2 + 1) + (Bx+C)x$ This is where Im stuck because I can not figure out the values for $B$ and $C$. Please help.","['calculus', 'integration']"
26121,Proof by induction: $\sum\limits_{i=0}^n i \cdot i! = (n+1)!-1$,"Let n be a positive natural number , $n\ge 0$, then. $\displaystyle\sum_{i=0}^n i \cdot i!= (n+1)!-1$ Here is my attempt.I'm not going to write the base case because I understand that part. Assuming  $\displaystyle\sum_{i=0}^n i \cdot i!= (n+1)!-1$ is true. We wish to show. $\displaystyle \sum_{i=0}^{n+1} i \cdot i!= (n+2)!-1$. Thus. $\displaystyle\sum_{i=0}^{n+1} i \cdot i!= (\sum_{i=0}^n i \cdot i!) $ This is where I get stuck.","['induction', 'algebra-precalculus']"
26131,Visual representation of groups,"I vaguely remember seeing something like a ""picture"" of various groups a while back. It was as if the elements of the group were each associated with a point and many points had segments connecting them, but not all were connected. Does anyone know what I am talking about? If so, would you care to take the time to explain the basics (or point me to a resource, if you think google won't help)? Thanks.","['soft-question', 'abstract-algebra']"
26132,Probability from a collection of independent predictions,"Reframed question: Team A and Team B will be playing each other in a sporting event where there is one winner and one loser; a tie is impossible.  A man, wanting to make money from placing a bet on the winner, consults with three omniscient seers.  These seers know the outcome of the game already, being omniscient. The first seer informs the man (truthfully) that he randomly lies 20% of the time when telling the outcome of a sporting event, telling the truth the rest of the time. The second seer says (truthfully) that he is just like the first, except he lies 40% of the time. The third seer says (truthfully) that he is just like the first and second seers, except he lies 70% of the time. The first seer tells the man that team A will win.
The second seer tells the man that team A will lose.
The third seer tells the man that team A will win. What is the probability that team A will win? Each seers' determination of whether to lie is independent of the others.  You can imagine that each seer rolls a ten sided die to make their decision of whether to lie. After the above question is answered, I am curious if someone can come up with a general formula for any number of seers with any probability of lying, with any set of predictions.","['statistics', 'probability']"
26142,Integer-valued random variables must converge in distribution to a integer-valued random variable?,"Let $X_1,X_2,...$ be a sequence of integer-valued random variables that converge in distribution to some random variable $X$. Am I right in thinking that we can always pick $X$ to be integer valued? I thought like this: Let $F_n$ be the distribution function of $X_n$ and $F$ be the distribution function of $X$. Then $X$ cannot be integer valued only if there are two points $x_1$ and $x_2$ such that
$$F(x_1) \neq F(x_2)$$ 
and
$$|x_1-x_2|<1$$ Now since a distribution function cannot have more than a countable number of points of discontinuity, we can pick $x_1$ and $x_2$ such that they are points of continuity of $F$. But then for large enough $n$
$$F_n(x_1) \neq F_n(x_2)$$
But this contradicts the fact that $X_n$ is integer-valued. Am I right? [This is motivated by exercise 5.12 of Wasserman's All of Statistics where he assumes that $X$ is integer-valued]",['probability-theory']
26143,Formula for the largest distance to a set of points,"I have $n$ points $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$ all located in the unit square $[0,1] \times [0,1]$. I am trying to compute the largest distance from a point in the unit square to the closest $(x_i,y_i)$. Is there some sort of formula for this quantity in terms of the $x_i,y_i$? Thanks.","['optimization', 'geometry', 'calculus']"
26149,Generalizing Quadratic Reciprocity Law with Dilates,"Eisenstein's proof of the Quadratic Reciprocity (QR) (and its Jacobi symbol generalization) both rely on counting lattice points in two congruent triangles. If we take $t$-dilates of these triangles, does an extension of Gauss' Lemma still hold with some dependence on $t$? That is, can we assign a Legendre symbol $(p|q)$, or some simple extension thereof depending on $t$, to $(-1)^{M(t)}$, where $M(t)$ is the number of lattice points in the $t$-dilate of the same triangle used in the standard proof and derive from it a ""$t$-dilated"" version of QR?","['combinatorics', 'number-theory']"
26167,Expectation of the maximum of i.i.d. geometric random variables,"Given $n$ independent geometric random variables $X_n$, each with probability parameter $p$ (and thus expectation $E\left(X_n\right) = \frac{1}{p}$), what is
$$E_n = E\left(\max_{i \in 1 .. n}X_n\right)$$ If we instead look at a continuous-time analogue, e.g. exponential random variables $Y_n$ with rate parameter $\lambda$, this is simple:
$$E\left(\max_{i \in 1 .. n}Y_n\right) = \sum_{i=1}^n\frac{1}{i\lambda}$$ (I think this is right... that's the time for the first plus the time for the second plus ... plus the time for the last.) However, I can't find something similarly nice for the discrete-time case. What I have done is to construct a Markov chain modelling the number of the $X_n$ that haven't yet ""hit"". (i.e. at each time interval, perform a binomial trial on the number of $X_n$ remaining to see which ""hit"", and then move to the number that didn't ""hit"".) This gives
$$E_n = 1 + \sum_{i=0}^n \left(\begin{matrix}n\\i\end{matrix}\right)p^{n-i}(1-p)^iE_i$$
which gives the correct answer, but is a nightmare of recursion to calculate. I'm hoping for something in a shorter form.","['probability-distributions', 'probability']"
26170,Matrix Inverses and Eigenvalues,"I was working on this problem here below, but seem to not know a precise or clean way to show the proof to this question below. I had about a few ways of doing it, but the statements/operations were pretty loosely used. The problem is as follows: Show that ${\bf A}^{-1}$ exists if and only if the eigenvalues $ \lambda _i$ ,  $1 \leq i \leq n$ of $\bf{A}$ are all non-zero, and then ${\bf A}^{-1}$ has the eigenvalues given by $ \frac{1}{\lambda _i}$, $1 \leq i \leq n$. Thanks.","['matrices', 'linear-algebra', 'inverse', 'eigenvalues-eigenvectors']"
26192,Good Online Resources to learn Multivariate Statistics?,"Are there any good online resources to learn multivariate statistics? ( With topics such as Multiple Linear Regression, Principal Component Analysis, Factor Analysis, Cluster Analysis and Discriminant Analysis)","['statistics', 'online-resources']"
26203,Induction problem: log of product equals sum of logs,"Please help me prove by induction that $\displaystyle\forall n\in {{\mathbb{N}}^{*}}$, $\displaystyle\forall {{a}_{1}},\ldots ,{{a}_{n}}\in {\mathbb{R}}^{*}_{+}$, $\displaystyle \ln \left( \prod\limits_{j=1}^{n}{{{a}_{j}}} \right)=\sum\limits_{j=1}^{n}{\ln \left( {{a}_{j}} \right)}$. Deduce that $\displaystyle \forall n\in \mathbb{Z},\forall a\in {\mathbb{R}}^{*}_{+}$, $\displaystyle \ln \left( {{a}^{n}} \right)=n\ln a$.","['induction', 'logarithms', 'algebra-precalculus']"
26204,Is the local inverse of an analytical function locally analytical as well?,"Take for example $f(z)=e^z$, so the inverse is $z(f) = \ln(f) + n\pi i$ for an arbitrarily chosen (but fixed) branch $n\in\mathbb N$. Now if $f$ is restricted to e.g. $0<|1-f|<1$ such that the essential singularity at $f=0$ is not part of the definition region, is $z(f)$ then analytical in that region?","['exponentiation', 'complex-analysis']"
26205,What is the 'implicit function theorem'?,"Please give me an intuitive explanation of 'implicit function theorem'. I read some bits and pieces of information from some textbook, but they look too confusing, especially I do not understand why they use Jacobian matrix to illustrate this theorem.","['multivariable-calculus', 'intuition', 'real-analysis']"
26215,Fubini's theorem problem,"Let $f$ be a non-negative measurable function in $\mathbb{R}$. Suppose that $$\iint_{\mathbb{R}^2}{f(4x)f(x-3y)\,dxdy}=2\,.$$ Calculate $\int_{-\infty}^{\infty}{f(x)\,dx}$. My first thought was to change the order of integration so that I integrate in $y$ first, since there's only a single $y$ in the integrand... but I'm not sure how/if that even helps me. Then the more I thought about it, the less clear it was to me that Fubini's theorem even applies as it's written. Somehow I need a function of two variables. So should I set $g(x,y) = f(4x)f(x-3y)$ and do something with that? At least Fubini's theorem applies for $g(x,y)$, since we know it's integrable on $\mathbb{R}^2$.    .... Maybe? I'm just pretty lost on this, so any help you could offer would be great. Thanks!",['real-analysis']
26231,Regarding a proof involving geometric series,"Someone asked this question about how many ways there are to prove $0.999\dots = 1$ and I posted this: $$  0.99999 \dots = \sum_{k = 1}^\infty \frac{9}{10^k} = 9  \sum_{k = 1}^\infty \frac{1}{10^k} = 9 \Big ( \frac{1}{1 - \frac{1}{10}} - 1\Big ) = \frac{9}{9} = 1$$ The question was a duplicate so in the end it was closed but before that someone  wrote in a comment to the question: ""Guys, please stop posting pseudo-proofs on an exact duplicate!"" and I got down votes, so I assume this proof is wrong. Now I would like to know, of course, why this proof is wrong. I have thought about it but somehow I can't seem to find the mistake. Many thanks for your help. The original can be found here .","['sequences-and-series', 'geometric-progressions', 'real-analysis', 'alternative-proof', 'decimal-expansion']"
26243,Center-commutator duality,"I'm reading this article by Keith Conrad, on subgroup series. I'm having trouble with a statement he does at page 6: Any subgroup of $G$ which contains $[G,G]$ is normal in $G$ . He says this as evidence that commutator and center play dual roles, since any subgroup of $G$ contained in $Z(G)$ is normal in $G$ . Now, this I'm sure I understand, but I don't see how the quoted line holds. What I have read is that $[G,G]$ is the least normal subgroup of $G$ such that the quotient is abelian, which seems related. Also, while we're at it: are the center and the commutator ""really"" (as in, categorically) dual constructions? I'm quite a novice in category theory, so please excuse me if this question is trivial.","['group-theory', 'abstract-algebra']"
26258,The Laplace transform of the first hitting time of Brownian motion,"Let $B_t$ be the standard Brownian motion process, $a > 0$, and let $H_a = \inf \{ t : B_t > a \}$ be a stopping time. I want to show that the Laplace transform of $H_a$ is
$$\mathbb{E}[\exp(-\lambda H_a)] = \exp (-\sqrt{2\lambda} H_a)$$
by considering the martingale $$M_t = \exp \left(\theta B_t -\frac{1}{2}\theta^2 t\right)$$ There's an obvious argument to follow here: assuming the optional stopping theorem applies, we have
$$1 = \mathbb{E}[M_{H_a}] = \mathbb{E} \left[ \exp \left(\theta a - \frac{1}{2}\theta^2 H_a\right) \right] = \exp(\sqrt{2\lambda} a) \mathbb{E} \left[ \exp(-\lambda H_a) \right]$$
where $\theta = \sqrt{2\lambda}$. This is exactly what we wished to show. However, as far as I can tell, the hypotheses of the optional stopping theorem are not satisfied here. Here is the statement I have: If $(X_n)$ is a martingale and $T$ is an a.s. bounded stopping time, then $\mathbb{E}[X_T] = \mathbb{E}[X_0]$. I think not all is lost yet. $M_t > 0$ for all $t$, so the martingale convergence theorem applies, and $M_t \to M_\infty$ a.s. for some integrable random variable $M_\infty$. For each $t$, $H_a \wedge t = \min \{ H_a, t \}$ is a bounded stopping time, so certainly $\mathbb{E}[M_{H_a \wedge t}] = \mathbb{E}[M_0]$. But,
$$\mathbb{E}[M_{H_a \wedge t}] = \mathbb{E}[M_{H_a} \mathbf{1}_{\{H_a \le t\}}] + \mathbb{E}[M_t \mathbf{1}_{\{H_a > t\}}]$$
and clearly what one wants to do is to take $t \to \infty$ on both sides. But here's where I get stuck: I'm sure I need a convergence theorem here in order to conclude that the equation remains valid in the limit. Now, $0 < M_{H_a} = \exp \left(\theta a - \frac{1}{2} \theta^2 H_a \right) \le \exp(\theta a)$, so the dominated convergence theorem applies, and so
$$\lim_{t \to \infty} \mathbb{E}[M_{H_a} \mathbf{1}_{\{H_a \le t\}}] = \mathbb{E}[M_{H_a} \mathbf{1}_{\{H_a < \infty\}}]$$
and I believe Fatou's lemma gives me that
$$\liminf_{t \to \infty} \mathbb{E}[M_t \mathbf{1}_{\{H_a > t\}}] \ge \mathbb{E}[M_{\infty} \mathbf{1}_{\{H_a = \infty\}}]$$
but I think what I need is the equality $$\lim_{t \to \infty} \mathbb{E}[M_t \mathbf{1}_{\{H_a > t\}}] = \mathbb{E}[M_\infty \mathbf{1}_{\{H_a = \infty\}}]$$
and as far as I can tell neither the monotone convergence theorem nor the dominated convergence theorem applies here. Is there anything I can do to rescue this line of thought?","['stochastic-processes', 'martingales', 'measure-theory', 'probability-theory', 'brownian-motion']"
26259,Formal power series - a question,"I've been reading generationgfunctionology by Herbert S. Wilf (you can find a copy of the second edition on the author's page here ). On page 33 he does something I find weird. He wants to shuffle the index forward and does so like this:
\begin{align*}
(f_{n+1})_{n\in N_0} &= \frac{(f(X)-f(0))}{X}\\
(f_{n})_{n\in N_0} &= \sum_{n\in N_0} f_nx^n
\end{align*} Why is this allowed? Namely $X$ has a noninvertible (0) constant term, so how is this division (multiplication with the reciprocal) defined within the ring of formal power series?","['power-series', 'generating-functions', 'abstract-algebra']"
26273,Converse of the Weierstrass $M$-Test?,"I was assigned a few problems in my Honors Calculus II class, and one of them was kind of interesting to do: Suppose that $f_{n}$ are nonnegative bounded functions on $A$ and let $M_{n} =  \sup f_{n}$. If $\displaystyle\sum\limits_{n=1}^\infty f_{n}$ converges uniformly on $A$, does it follow that $\displaystyle\sum\limits_{n=1}^\infty M_{n}$ converges (a converse to the Weirstrass $M$-test)? I know that this question has been asked before , but I'm trying not to just copy an answer off the internet and instead to come up with an example of my own to see if I can actually understand the theorems that I'm learning. To provide a counterexample, I tried to create a function which has a diverging $\sup$, but I'm not too confident that my proof is valid. Here it goes: $$
\text Let \ f_{n}(x) = \begin{cases}
  \begin{cases}
    \frac{1 + x}{2}, & \text{if }x \in (-1,0]\\
    \frac{1 - x}{2}, & \text{if }x \in (0,1)\\
    0, & \text{elsewhere}
  \end{cases}, & \text{if }n \text{ even}\\
  \begin{cases}
    x, & \text{if }x \in (0,1]\\
    2 - x, & \text{if }x \in (1,2)\\
    0, & \text{elsewhere}
  \end{cases}, & \text{if }n \text{ odd}
\end{cases}
$$ Now, $\text Let f(x) = 0$. From this definition, I can conclude that
$$
\sup\{f_{n}\} = \begin{cases}
  \frac{1}{2}, & \text{if }n \text{ even}\\
  1, & \text{if }n \text{ odd}
\end{cases}
$$ Now, to show that ${f_{n}(x)}$ is uniformly convergent, the definition of uniform convergence is used: $$
\forall_{\epsilon > 0}\ \exists_{N}\ \text s.t.\ \forall_{x}\ \text if\ n > N, |f(x) - f_{n}(x)| < \epsilon
$$ Since $f_{n}(x)$ is strictly nonnegative and $f(x) = 0$, $|f(x) - f_{n}(x)| = f_{n}(x)$. By definition, $\epsilon > 0$, and since $f_{n}(x) = 0$ for $x \geq 2, f(x) - f_{n}(x) = 0 < \epsilon$ for $x \geq 2$. Therefore there exists a $N$ (namely, $N = 1$) which proves that the sequence is uniformly convergent. Since $\lim_{n\to\infty} f_{n} \neq 0$, by the Limit Test, the infinite sum  $\displaystyle\sum\limits_{n=1}^\infty \sup{f_{n}}$ diverges, which disproves the converse of the Weierstrass $M$-Test. $\blacksquare$ This is the first time I've actually used LaTeX, so I'm sorry for the way it looks. Is there anything that I can do to make this proof better (or even valid, if it's wrong), or is it fine the way it is? This might be a bit of a long question...","['calculus', 'real-analysis']"
26279,trigonometric inequality,"please help me establish(etablir): $\forall n\in \mathbb{N}-\left\{ 0,\left. 1 \right\} \right.$ , $x\in \mathbb{R}-\left\{ \pi \mathbb{Z} \right\}$ , $\left| \sin \left( nx \right) \right|<n\left| \sin x \right|$ thx in advance...","['inequality', 'trigonometry', 'algebra-precalculus']"
26280,On the properties of the Sobolev Spaces $H^s$,"Let $H^s(\mathbb{R}^d):= \{ u \in \mathcal{S}' : (1+|\xi|^2)^{s/2}\hat{u}(\xi) \in L^2(\mathbb{R}^d)\}$. It can be shown that this space is a Hilbert space and that $H^s \subset H^t$ if $t \leq s$. Now suppose we have $t > s>0$ such that $H^s \subsetneq H^t$. Then we know that $(H^s)^* \supsetneq (H^t)^*$, where $^*$ denotes the dual. At the same time, since $H^s$ is a Hilbert space, the Riesz Representation theorem tells us that $H^s \cong (H^s)^*$ and similarly, $H^t \cong (H^t)^*$. But then doesn't $(H^s)^* \supsetneq (H^t)^*$ imply that $H^s \supsetneq H^t$, which contradicts our earlier statement that $H^s \subsetneq H^t$? What part of the argument here falls apart?",['functional-analysis']
26287,Counting solutions of a cubic congruence using Gauss sums,"In the introduction to André Weil's Number of solutions of equations in finite fields he mentions article 358 of Gauss' Disquisitiones . Can someone please show me the connection here: How does the Gaussian sum of order $3$, for primes of the form $p = 3n+1$ determine the number of solutions for all congruence $ax^3 - by^3 = 1 \pmod p$?","['combinatorial-number-theory', 'number-theory']"
26295,What is the most general mathematical object that defines a solution to an ordinary differential equation?,"What is the most general object that defines a solution to an ordinary differential equation?  (I don't know enough to know if this question is specific enough.  I am hoping the answer will be something like ""a function"", ""a continuous function"", ""a piecewise continuous function"" ... or something like this.)",['ordinary-differential-equations']
26306,directional derivative in a manifold,"Let us assume that directional derivative of a function $f$ exists at a point $p$ (i.e.,$ D_v(f)$) for all vectors $v \in \mathbb{R}^{n}$. Does it imply that the function is differentiable?","['calculus', 'differential-geometry', 'analysis']"
26311,Proof That $\mathbb{R} \setminus \mathbb{Q}$ Is Not an $F_{\sigma}$ Set,"I am trying to prove that the set of irrational numbers $\mathbb{R} \setminus \mathbb{Q}$ is not an $F_{\sigma}$ set. Here's my attempt: Assume that indeed $\mathbb{R} \setminus \mathbb{Q}$ is an $F_{\sigma}$ set. Then we may write it as a countable union of closed subsets $C_i$:
$$
\mathbb{R} \setminus \mathbb{Q} = \bigcup_{i=1}^{\infty}  \ C_i
$$
But $\text{int} ( \mathbb{R} \setminus \mathbb{Q}) = \emptyset$, so in fact each $C_i$ has empty interior as well. But then each $C_i$ is nowhere dense, hence $
\mathbb{R} \setminus \mathbb{Q} = \bigcup_{i=1}^{\infty}  \ C_i$ is thin. But we know $\mathbb{R} \setminus \mathbb{Q}$ is thick, a contradiction. This seems a bit too simple. I looked this up online, and although I haven't found the solution anywhere, many times there is a hint: Use Baire's Theorem. Have I skipped an important step I should explain further or is Baire's Theorem used implicitly in my proof? Or is my proof wrong? Thanks. EDIT: Thin and thick might not be the most standard terms so: Thin = meager = 1st Baire category","['baire-category', 'real-analysis']"
26323,A confusion about Ker($A$) and Ker($A^{T}$),"Today we were discussing how for an nxn orthogonal projection matrix from $\mathbb{R^{n}}$ onto a subspace W, Ker($A$)=$(Im$A$)^{\perp}$=$W^{\perp}$ and that Ker($A^{T}$) is also $W^{\perp}$. This prompted the question of what conditions are necessary for a matrix so that the kernels of it and its transpose are equal. It looks like it always works when it's an orthogonal projection, but we struggled to find an example of a square matrix for which the aforementioned identities would not hold, and consequently for which Ker($A$)$\neq$Ker($A^{T}$). It looks like we need to find a transformation whose Kernel would not consist only of things perpendicular to its image, but we were wondering if that was possible or if our reasoning was correct at all. Could you please clarify this issue (I know it's convoluted) and try to point out where we were right and wrong, and when do those identities hold?","['matrices', 'linear-algebra', 'transpose']"
26325,"Does integration by parts with ""deja vu"" have a name?","In some integration by parts problems, such as evaluating the integral of $e^x \cos x$ or $\sec^ 3 x$, one performs integration by parts (possibly more than once, and possibly together with algebraic manipulations) and eventually the original integral appears again. To beginning students, this may superficially appear to be ""circular reasoning"" that doesn't solve the problem.  But it does, because if we have $\int f(x) dx = g(x) + K \int f(x) dx$ where $K \ne 1$, then rearranging gives $\int f(x) dx = \frac{1}{1-K} g(x)$. My question: Does this technique have a commonly used name? I once saw it called ""integration by parts with deja vu"" in some supplemental study materials for a calculus course.  I don't know who thought of that name but I've taken to using it with my students.","['terminology', 'ring-theory', 'calculus', 'integration']"
26327,How many strings of length $n$ with $m$ ones have no more than $k$ consecutive ones? [duplicate],"This question already has answers here : A formula for bit strings with no $k$ consecutive $1$'s using generating functions (2 answers) Closed 4 years ago . Here is a problem from generatingfunctionology that I'm stuck on: I'm trying to get started on part (a). I broke the string up like this. If the last digit is $0$, the number of possible strings is then $f(n-1,m,k)$. If the last digit is $1$, there are two subcases. If the $n-1^{th}$ digit is $0$, then we can cut them both off and the number of strings is $f(n-2,m-1,k)$. However, if the $n-1^{th}$ digit is $1$, then I don't know what to say, since even if I cut both last $1$s off, I can't have the last $k-2$ numbers of my $n-2$ long string be $1$s, but it's entirely possible that I could have $k-2$ $1$s earlier in the string. So I have something like:
$$
f(n,m,k)=f(n-1,m,k)+f(n-2,m-1,k)+???
$$
and I don't know what third term to put there. What is the third term? Thanks. If it's not trouble, I may ask questions on parts (b) and (c) when I get there.","['generating-functions', 'combinatorics']"
26345,Inclusion of subgroups implies the group is cyclic,"Let $G$ be a finite group such that for any two subgroups $H_{1}$ and $H_{2}$ of $G$ we have $H_{1} \subseteq H_{2}$ or $H_{2} \subseteq H_{1}$. Why this implies $G$ is a cyclic group? Ah. I think this works: Suppose $g \in G$ then $G = \langle g \rangle$. Suppose otherwise, then we can find $z$ such that $z \not \in \langle g  \rangle$. Now let $H_{1}=\langle z \rangle$ and $H_{2}=\langle g \rangle$. By assumption we have $H_{1} \subseteq H_{2}$ or $H_{2} \subseteq H_{1}$. In both cases we get the contradiction $z \in \langle g \rangle$.",['group-theory']
26358,Solve $\sin x = 1 - x$,"How would you be solve sin x = 1 - x, without drawing the graph and manually measuring the point of intersection?",['trigonometry']
26368,Why do we call it trace?,"For any module $P$, we define $\mathrm{tr}(P)=\sum \mathrm{im}(f)$, where $f$ ranges over all elements of $\mathrm{Hom}(P,R)$， and call it trace. Why does it have such a name? Does it have any relation to the trace of matrix？","['modules', 'terminology', 'abstract-algebra']"
26371,Is a linear transformation onto or one-to-one?,"The definition of one-to-one was pretty straight forward. If the vectors are lin.indep the transformation would be one-to-one. But could it also be onto? The definition of onto was a little more abstract. Would a zero-row in reduced echelon form have any effect on this? I just assumed that because it has a couple of free variables it would be onto, but that zero-row set me off a bit. Say if the matrix is 4 by 5. With two free variables. And the zero-row in echelon form. (Sorry for not posting the given matrix, but that is to specific. And I don't want to get a ban from uni for asking online. The task is determine the onto/one-to-one of to matrices) I'll check back after class and update the question if more information is desirable. Update The definitions in the book is this; 
Onto: 
$T: \mathbb R^n \to \mathbb R^m $ is said to be onto $\mathbb R^m  $ if each b in 
$\mathbb R^m  $ is the image of at least one x in $\mathbb R^n  $ One-to-one:
$T: \mathbb R^n \to \mathbb R^m $ is said to be one-to-one $\mathbb R^m  $ if each b in 
$\mathbb R^m  $ is the image of at most one x in $\mathbb R^n  $ And then, there is another theorem that states that a linear transformation is one-to-one iff the equation T(x) = 0 has only the trivial solution. That doesn't say anything about onto. Then there is this bit that confused be about onto: 
Let $T: \mathbb R^n \to \mathbb R^m $ be a linear transformation and let A be the standard matrix for T. Then: 
T maps $T: \mathbb R^n$ onto $\mathbb R^m $ iff the columns of A span $\mathbb R^m $.",['linear-algebra']
26379,Partial fraction of $\frac{x^n}{(1-x)(1-2x)(1-3x)\ldots(1-nx)}$,How should you break $\displaystyle \frac{x^n}{(1-x)(1-2x)(1-3x)\ldots(1-nx)}$ into partial fractions?,"['algebra-precalculus', 'partial-fractions', 'polynomials']"
26386,Questions about geometric distribution,"I have some trouble understanding the record value for a sequence of i.i.d. random variables of geometric distribution. Following quotation is from Univariate discrete distributions By Norman Lloyd Johnson, Adrienne W. Kemp, Samuel Kotz . The lack-of-memory property of the
  geometric distribution gives it a role
  comparable to that of the exponential
  distribution. There are a number of
  characterizations of the geometric
  distribution based on record values. For the  record time $T_n$, If $X_j$ is observed at time $j$ ,
  then the record time sequence $\{T_n ,n \geq 0\}$ is defined as
  $T_0 = 1$
  with probability $1$ and $T_n = \min\{j : X_j > X_{t_{n−1}} \}$ for
  $n\geq 1$. Is there a typo in $T_n = \min \{j : X_j >
    X_{t_{n−1}} \}$? Should it be instead $T_n
    = \min\{j : X_j > X_{T_{n−1}} \}$? For the  record value $R_n$, The record value sequence
  $\{R_n \}$ is defined as $R_n =X_{T_n} , n = 0, 1, 2, ...$. Suppose
  that the $X_j$ ’s are iid geometric
  variables with pmf $$p_x = p(1 − p)^{x−1}, x = 1, 2, ...$$ Then $R_n
    =X_{T_n} = \sum_{j=0}^{n} X_j$ is
  distributed as the sum of $n + 1$ iid
  geometric variables. why does the second equality in
""$R_n =X_{T_n} = \sum_{j=0}^{n} X_j$
"" hold? For the process of the record values
$\{ R_n, n \in \mathbb{N}\}$, Each of the following properties
  characterizes the geometric
  distribution: (i) Independence: The rv’s $R_0 , R_1  - R_0 , ... , R_{n+1} - R_n ,...$ are independent. (ii) Same Distribution: $R_{n+1} - R_n$ has the same distribution as
  $R_0$ . (iii) Constant Regression: $E[R_{n+1}- R_n |R_n ]$ is constant. How to show that the three
properties hold? Are they derived
from the memoryless property of
geometric distribution? What else can we say about the
process based on the memoryless
property of geometric distribution? Thanks for your advice!","['statistics', 'stochastic-processes', 'probability-distributions', 'probability']"
26387,Absolute value of all values in a matrix,"How do I express the matlab function abs(M) , on a matrix $M$ , in mathematical terms? I thought about norms or just $|M|$ , but these return scalars, not another matrix of the same size as $M$ . Sorry for the rough explanation, english isn't my primary language.","['matrices', 'absolute-value']"
26389,Why is the real line not used in Descriptive Set Theory?,"In most Descriptive Set Theory books, the rationale for working with the Baire space ($\mathbb{N}^{\mathbb{N}}$) as opposed to the real line ($\mathbb{R}$) is that the connectedness of the latter causes 'technical difficulties'. My question is, what are these technical difficulties, and why does Descriptive Set Theory (normally?) stick to zero-dimensional Polish spaces? Thanks in advance.","['general-topology', 'set-theory', 'descriptive-set-theory']"
26399,A question about $F$-distribution,"Let $f(m,n,w)$ be the probability density function of $F$ variable with $m$ numerator $df$ and $n$ denominator $df$ , i.e. $$f(m,n,w)=\frac{\Gamma\left(\frac{m+n}{2}\right)(m/n)^{m/2}}{\Gamma(m/2)\Gamma(n/2)}w^{(m/2)-1}\left(1+\frac{mw}{n}\right)^{-(m+n)/2}$$ I am interested in the infimum of $\int_1^\infty f(m,n,w) dw$ over all $m,n$ . From my
exploration, this seems to be $$1-\mbox{erf}(1/\sqrt{2})\approx 0.3173$$ where $\mbox{erf}(x)=\frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2} dt$ . Can anyone prove this or point me to a reference?",['statistics']
26424,A samurai cuts a piece of bamboo,"Suppose a samurai wants to try out his new sword and cuts a piece of bamboo twice, randomly, so now there are $3$ lenghts of bamboo. What is the probability of these 3 pieces being able to form a triangle? I have never came across a continuous probability problem before, but I tried doing it anyway and got a result of 0.25 probability. My solution: Let $L$ be the original lenght of the bamboo, $x$ be the place of the first cut and $y$ be the place of the second cut. Writing out all the 3 triangle inequalities, we come to the conclusion that no piece of bamboo can have more than $L/2$ lenght, then the probability we're looking for is:
$$
\frac{\int_{x=0}^{L/2}(\int_{y=L/2}^{x+L/2}(1)dy)dx}{\int_{x=0}^{L}(\int_{y=x}^L(1)dy)dx}=0.25
$$",['probability']
26425,Behaviour of the series $\exp_p(x)=\sum_{k=0}^{\infty}\frac{x^k}{(k!)^p}$ depending on $p\approx 2$?,"Note:This is more a math-recreational question Consider the series $\exp_p(x)=\sum_{k=0}^{\infty}\frac{x^k}{(k!)^p}$ which is some systematic modification of the exponential function. It's $\exp_1(x)=\exp(x)  $ if p=1 . I'm interested in the behaviour for $x=0\to -\infty$ depending on change of the parameter $p$. [edit] For the plots, and also for the discussion of the oscillation I removed the constant 1 from the function. Then I edited that question for correction. But after I got some answer I better roll back the edits of the title and the formula and state that removal for the plots and the discussion of the oscillation only here explicitely instead. Please excuse that inconvenience [edit 2] First oberservation is, that if $p>1$ the function begins to oscillate. If $p=2$ the oscillation diminuishes but for $p>2$ in general the oscillation seems to increase and even for a couple of values $p=2 + \epsilon $ which I checked manually where the (possibly) eventual increase occurs after an initial decrease. So my questions are: is it true, that at p=2 and x from $ 0\to -\infty$ the amplitude of the oscillation diminueshes to zero? is it true, that at $p=2+\epsilon, \epsilon>0$ and x from $ 0\to -\infty$ the amplitude of the oscillation eventually increases without bound? How could I approach that question, for instance by considering the form of the power series, the analysis of the powers of the factorials. Are there any ""keystones"" which might be helpful? This is a plot for $p=2$ so $f(x)=\exp_2(-(x^2)) -1 $. I used the squaring of the x to see more of the oscillation. The -1 is to locate the oscillation around the x-axis. This is a plot for $p=2.02$ so $f(x)=\exp_{2.02}(-x^2) -1 $. Again I used the squaring of the x to see more of the oscillation. Bigger epsilons let the oscillation increase at an x nearer to the vertical $x=0$-line.","['special-functions', 'sequences-and-series', 'recreational-mathematics']"
26427,proving derivative in real analysis,"I have proved the following problem, can you help me check if there is any loopholes in my proof? Let I be an open interval in R, let $c \in I$, and let $f, g\colon I\to \mathbb{R}$ be functions. Suppose that $f(c) = g(c)$, and that $f(x) \leq g(x)$ for all $x \in I$. Prove that if f and g are differentiable at c, then f'(c) = g'(c). So the following is my proof: Let $x \in I$, if $x \lt c$, then $\lim\limits_{x\rightarrow c-}\frac{f(x) 
        - f(c)}{x-c}$ = $\lim\limits_{x\rightarrow c-}\frac{f(c) 
        - f(x)}{c-x} \geq \lim\limits_{x\rightarrow c-}\frac{g(c) 
        - g(x)}{c-x} = \lim\limits_{x\rightarrow c-}\frac{g(x) 
        - g(c)}{x-c}$ if $x \gt c$, then $\lim\limits_{x\rightarrow c+}\frac{f(x) 
        - f(c)}{x-c} \leq \lim\limits_{x\rightarrow c+}\frac{g(x) 
        - g(c)}{x-c}$ Since $f$ and $g$ are differentiable at $c$, we know that $\lim\limits_{x\rightarrow c-}\frac{f(x) 
        - f(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{f(x) 
        - f(c)}{x-c}=f'(c)$ and $\lim\limits_{x\rightarrow c-}\frac{g(x) 
        - g(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{g(x) 
        - g(c)}{x-c} = g'(c)$ Thus $\lim\limits_{x\rightarrow c-}\frac{f(x) 
        - f(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{f(x) 
        - f(c)}{x-c} = \lim\limits_{x\rightarrow c-}\frac{g(x) 
        - g(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{g(x) 
        - g(c)}{x-c}$ Therefore $f'(c) = g'(c)$. I can't find any problem in my proof, but for some reason I'm not feeling comfortable with it. However, if it's totally correct, just tell me :) Thanks!!",['real-analysis']
26444,"Ideals of $\mathscr{I}(V) = \text{Hom}(V,V)$","I have been trying this notoriously difficult problem for quite some time but i haven't made any progress. Let $\mathscr{I}(V)$ denote the set of all homomorphisms $f : V \to V$. That is $\mathscr{I}(V) = \text{Hom}(V,V)$. Suppose $V$ is a vector space over a field $K$ and $\text{dim}_{K}(V)>1$, then prove that $\mathscr{I}(V)$ has no proper two sided ideals . This means we have to show that $\mathscr{I}(V)$ has no two sided ideals other than $(0)$ and $\mathscr{I}(V)$. Next, does the conclusion of the above problem remain true if $V$ is infinite dimensional . Moreover, since Field's are the ubiquitous algebraic structures to have trivial ideals i tried thinking of proving $\mathscr{I}(V)$ to be a field. But i haven't yet taken a single step forward in terms of this problem.","['ideals', 'linear-algebra', 'abstract-algebra']"
26445,Division by zero,"I came up some definitions I have sort of difficulty to distinguish. In parentheses are my questions. $\dfrac {x}{0}$ is Impossible (  If it's impossible it can't have neither infinite solutions or even one. Nevertheless, both $1.$ and $2.$ are divided by zero, but only $2. $ has infinite solutions so as $1.$ has none solution, how and why ?) $\dfrac {0}{0}$ is Undefined and has infinite solutions. (How come one be Undefined and yet has infinite solutions ?) $\dfrac {0}{x}$ and $x \ne 0$, it's okay for me, no problem, but if someone else wants to add something about it, feel free to do it.","['arithmetic', 'algebra-precalculus']"
26459,"If $\lim\limits_{x\to a}f(x)$ exists, and $\lim\limits_{x\to a}[f(x) + g(x)]$ exists, does it follow that $\lim\limits_{x\to a}g(x)$ exists?","This is a question from Spivak's Calculus. Question statement: If $\lim\limits_{x\to a}f(x)$ exists, and $\lim\limits_{x\to a}[f(x) + g(x)]$ exists, does it follow that $\lim\limits_{x\to a} g(x)$ exists? I have not been able to find a counterexample, so it seems that it exists. Here is my attempt at proof, and I'm mostly interested in validity of my proof. Following is given: $\forall \epsilon, \exists \delta_1: 0 < |x-a| < \delta_1 \Rightarrow |f(x) - L_1| < \epsilon$ $\forall \epsilon, \exists \delta_2: 0 < |x-a| < \delta_2 \Rightarrow |f(x) + g(x) - L_2| < \epsilon$ Let $\delta_2 \le \delta_1$. If greater $\delta_2$ works, then smaller one will of course work, and if smaller one works than that inequality follows from that.
Let $L_2 - L_1 = L_3$, that is $L_2 = L_3 + L_1$. Then we substitute: $|f(x) + g(x) - L_1 - L_3| < \epsilon$ $|(g(x) - L_3) - (L_1 - f(x))| < \epsilon$ And by the inequality $|a-b| \ge |a| - |b|$: $|g(x) - L_3| - |L_1 - f(x)| \le  |(g(x) - L_3) - (L_1 - f(x))| < \epsilon$ $|g(x) - L_3| - |f(x) - L_1| < \epsilon$ $|g(x) - L_3| < \epsilon + |f(x) - L_1|$
Since $\delta_2 \le \delta_1$, we can always choose epsilon greater than $|f(x) - L_1|$ so we can make substitution: $|g(x) - L_3| < 2\epsilon$ And we can make $2\epsilon$ as small as we wish. This completes the proof. Thank you for any help!","['calculus', 'limits']"
26487,How to find a nontrivial Lie group transformation?,"Dear all, I am studying symmetry method for ODEs.
The book written by George W.bluman (can be found in google books) discusses the procedure of determining ODE admitting a given group in details. However, as to the converse way, it only gives some theories, but no examples. I am trying to solve the exercise 3.2 - 9, P120: given ODE $(y-\frac{3}{2}x-3)y'+y=0$, find a nontrivial Lie group of transformations admitted by the ODE;
and find the general solution of the ODE. Please help me out here.
Thank you. Cheers,
Jan","['ordinary-differential-equations', 'lie-groups']"
26499,"For all integers b, c, and d, if x is rational such that x^2+bx+c=d, then x is an integer","Prove or disprove the following statment: For all integers b, c,and d, if x is a rational number such that $x^2+bx+c=d$, then x is an integer. This is a homework question from the book Discrete Mathematics for Computer Scientists by Stein, Drysdale and Bogart. I since x is rational I thought I could start off with: ${(\frac{m}{n})}^2+b\frac{m}{n}=d-c$ But I don't know where to go from here. Or I could try using the quadratic formula $x=\frac{1}{2}\left(\pm\sqrt{b^{2}-4c+4d}-b\right)$ but I am very weak with elementary number theory that I don't know where to go.  I am thinking that regardless of if $\sqrt{b^{2}-4c+4d}$ is an integer or not, the fact that I have $x=\frac{1}{2}*\pm$ SomeNumber means that x is not an integer. I am new to writing proofs, and unfortunately, I don't really know how to prove this.  Any hints would be appreciated. Thank you. Edit:  By plugging in simple numbers, for example x=1, b=1, c=1 and d=3 I can see that x is probably an integer, for all integers b,c,and d - so that means my thinking about the quadratic formula is not correct.  I will still work on this. 2nd Edit:  Now I plug in more numbers and don't get integers. For example $x^2+2x+3=4$.  I am also new to this site, so I am not sure if I should continue to edit the post or write in the comments sections anytime I think of something new.  Please advise. 3rd Edit: I think I know what to do.  The last section of the book covered universal quantifiers.  I believe the authors are are trying to get me to realize that they are saying $\forall b, c, d \in Z$ and I only need to give one one example for which the assertion is untrue.  And in my previous edit, b=2, c=3, and d=4 did not result in x being an integer.","['elementary-number-theory', 'discrete-mathematics']"
26503,A differential-functional equation: $f'(f^{-1}(x)) = 1/g(x)$,"Problem : Given $g(x)$, solve the equation $f'(f^{-1}(x)) = \frac{1}{g(x)}$ for an invertible and differentiable function $f(x)$. So far I have tried setting $y = f^{-1}(x) \Leftrightarrow x = f(y)$, obtaining the differential equation
$$
f'(y) = \frac{dx}{dy} = \frac{1}{g(x)}
$$ which we then solve to obtain $y$ in terms of $x$, i.e. to obtain the inverse function $f^{-1}(x)$. If this is easily inverted then we can find $f(x)$. What I am interested in is if anyone knows a better way to solve this, desirably one which allows us to determine $f(x)$ directly. I'm new to this kind of equation so please correct me if there's a better term for it than ""differential-functional"" :) Edit: I probably should say that in the particular context I am considering, $g(x)$ is the norm of a non-zero vector $\vec{r}(x)$ and is hence always positive.","['ordinary-differential-equations', 'functional-equations']"
26514,Continuous image of compact sets are compact,"How to prove: Continuous function $f : M\to N$ maps compact set to compact set? The simplest case in real analysis: if $f: [a,b] \rightarrow \mathbb{R}$ is continuous, then I need to show that $f([a,b])$ is closed and bounded, by the Heine-Borel theorem. I have proved the boundedness, and I need some insight on how to prove that $f([a,b])$ is closed, i.e. $f([a,b])=[c,d]$ . From the Extreme Value Theorem, we know that $c$ and $d$ can be achieved, but how to prove that if $c < x < d$ , then $x \in f([a,b])$ ? What if $M, N$ are metric spaces? What if $M$ and $N$ are general topological spaces?","['general-topology', 'continuity', 'real-analysis', 'metric-spaces', 'compactness']"
26527,Turning a closed-form generating function back to ordinary power series,"If I know the formal power series, I know how to find the closed form: $$\displaystyle F = \sum_{n=0}^{\infty} {X^n} = 1 + X^1 + X^2 + X^3 + ...$$ $$\displaystyle F \cdot X = X \cdot \sum_{n=0}^{\infty} {X^n} = X^1 + X^2 + X^3 + X^4 + ...$$ $$\displaystyle F - F \cdot X = 1 $$
$$\displaystyle F = \frac 1 {1 - X} $$ But if I only know the closed form $\frac 1 {1 - X}$, how do I turn it back into the series $1 + X^1 + ...$? In other words, how do I do extract the coefficients if I only know the closed form and I do not know that $\frac 1 {1 - X}$ corresponds to $1 + X^1 + ...$. My textbook and everywhere I looked at seems to avoid talking about this, and somehow magically transform things back and forth with a set of known formulas. Is there a better way to do this, or is formula matching the best we can do? Edit: This is the type of questions I need to solve: Find the coefficient of $X^8$ in the formal power series $(1 - 3X^4)^{-6}$","['power-series', 'generating-functions', 'discrete-mathematics']"
26528,$m$ balls $n$ boxes probability problem,"I encountered this problem in my probability class, and we weren't able to solve it, so I would like to know the answer. If you have $m$ balls and $n$ boxes, with $n < m$, and you insert the balls into the boxes randomly, what is the probability that all the boxes have at least one ball in it? The problem doesn't specify if the balls are distinguishable or not, so you may assume either, so another question would be, if you assume they are distinguishable will you get the same answer as assuming they are not distinguishable? (This would be great because I think the non-distinguishable case is easier). I appreciate any insight on the problem.",['probability']
26530,Determining Angle,"If we are given two sides say $a$, $b$ and an angle $X$. How can we determine whether this angle $X$ is opposite to $a$ or $b$ (i.e. $A$, $B$) or the third included angle $C$ ?","['geometry', 'trigonometry']"
26545,The general argument to prove a set is closed/open,"I am taking a topology course and we are now learning open and closed set. I am a bit confused to how to prove that a set is closed or opened, how should I approach these kind of problems. For example: Question 1: Let $(\mathcal{X},d)$ be an arbitrary metric space. Prove that any set which contains a finite number of points $\{x_1,x_2,\ldots,x_n\}$ is closed. Solution: If we take the point $x_i$ where $1\leq i\leq n$ and no matter how small we make $r$, in the ball some points are outside of our set. Hence the set is closed. Question 2: Let $(\mathcal{X},d)$ be an arbitrary metric space. Prove that $B(\mathcal{X},r)=\{y\in\mathcal{X}:d(x,y)<r\}$ is open. Solution: Take a point $y_0$ in the ball, and make a ball with radius $\frac{1}{2}(r-d(x,y_0))$, the all the points in this ball are in the actual ball in the question. Hence the ball in the question is open. Are these proves right? I myself do not feel that I am proving anything. Could you please teach me the correct proof if these are not correct? Thanks",['general-topology']
26548,How to get Fermat's descent working on the conic $x^4+y^4=2z^2$?,"Fermat solved the Diophantine equation $(x^2)^2 + (y^2)^2 = z^2$ using descent, the key step was using the Pythagorean triples: $x^2 = u^2 - v^2$ $y^2 = 2 u v$ $z = u^2 + v^2$ but then it is seen that $x,v,u$ is another Pythagorean triple, $x = m^2 - n^2$ $v = 2 m n$ $u = m^2 + n^2$ then the observation that $y^2 = 2^2 u m n$ (along with some coprimality conditions) implies that $u$,$m$ and $n$ are squares - which gives us the descent step needed. I wanted to use this technique to solve $(x^2)^2 + (y^2)^2 = 2 z^2$ but it does not seem to work out. Parameterizing the curve through $(1,1,1)$ gives: $x^2 = -m^2 - 2 m n + n^2$ $y^2 = m^2 - 2 m n - n^2$ $z = m^2 + n^2$ Nothing here seems to match up so well with the original problem (as it did in Fermat's case) and parameterizing through different points doesn't help either. Is there any way to make descent work here? Was Fermat just lucky or is there a reason why descent can't work out here? update: I found the parametrization $x^2 = 2m^2 - n^2$ $y^2 = 2m^2 - 4mn + n^2$ $z = 2m^2 - 2mn + n^2$ which looks like a better candidate for descent but I haven't been able to actually perform it. Since I cannot see how to show that $n$ and $x$ are both squares.","['diophantine-equations', 'infinite-descent', 'number-theory']"
26564,Solving inequality with two absolute values,"Hey, !
In my pre-calculus class the teacher showed the solution of the following example:
\begin{align}
\vert x-3 \vert   \lt \vert  x - 4 \vert  + x
\end{align}
He started by stated the domains needed to be checked: \begin{aligned}
\lbrack 4, +\infty ) \newline
\lbrack 3, 4 ) \newline
( -\infty, 3)
\end{aligned} Which I don't have a based lead on how he come to these domains and then deducted the following inequalities ( for each domain respectively):
\begin{align}
x-3 \lt x - 4 + x \newline
x-3 \lt -(x-4) + x \newline
-(x-3) \lt -(x-4) + x
\end{align} The final solution was \begin{aligned} (-1, +\infty ) \end{aligned} Now I cannot understand how he deducted the domains and the right inequalities(there is one missing possibility):
\begin{align} -(x-3) \lt x-4 + x \end{align} It may be apparent but I still cannot wrap my mind around it. Thanks!","['inequality', 'absolute-value', 'algebra-precalculus']"
26569,Why is the generalized quaternion group $Q_n$ not a semi-direct product?,Why is the generalized quaternion group $Q_n$ not a semidirect product?,"['group-theory', 'abstract-algebra']"
26571,What is happening in a linear algebra computation?,"About a year ago I took a Linear Algebra class that was required for my degree.  Unfortunately that class had an unidentified pre-requisite and started at a much higher level then I really needed.  Going in I had no prior experience with linear algebra.  I can definitely see how understanding linear algebra would be a very good thing to have in my field so I've been trying to piece it together ever since and have felt like I'm close but I just don't quite get it.  I understand that linear algebra is a way to solve a lot of equations rapidly... In my mind this seems like that means finding values for the variables... but it didn't seem like we ever did... Instead we were doing things like multiplication of matrices and that made no sense.  Or we would apply advance algorithms to get the matrix into certain forms which the reason for never made sense to me. So what does it mean to solve a system of equations?  What are some real world examples that might make understanding linear algebra easier?  Why are orthogonal and other types of matrices so special?  Any insights, examples, suggestions are greatly appreciated!",['linear-algebra']
26573,"What is the least number of symmetric polynomials needed to determine a unique solution z_1,...,z_n?","Is there a system $\{s_1, \cdots, s_m\}$ of symmetric polynomials of $z_1, \cdots, z_n \in \mathbb{C}$ such that $$s_1(z_1, \cdots, z_n) = c_1$$
$$s_2(z_1, \cdots, z_n) = c_2$$
$$\cdots$$
$$s_m(z_1, \cdots, z_n) = c_m$$ has at most one solution $(z_1, \cdots, z_n)$ up to permutation, for all choices $c_1, \cdots, c_m \in \mathbb{C}$? If so, what is the minimum value of $m$?  And what are all such systems of polynomials $\{s_1, \cdots, s_m\}$ with this property?",['algebraic-geometry']
26605,Solving $\displaystyle \cos(x-\alpha)\cos(x-\beta) = \cos{\alpha}\cos{\beta}+\sin^2{x}$,"Solve $\displaystyle \cos(x-\alpha)\cos(x-\beta) = \cos{\alpha}\cos{\beta}+\sin^2{x}$. My attempt: $\displaystyle \cos(x-\alpha)\cos(x-\beta) = \cos{\alpha}\cos{\beta}+\sin^2{x} \Rightarrow \cos(x-\alpha)\cos(x-\beta)-\cos{\alpha}\cos{\beta} = \sin^2{x},$ and $$\begin{aligned}LHS & = \frac{1}{2}\cos\left(2x-\alpha-\beta\right)+\frac{1}{2}\cos\left(\alpha-\beta\right)-\frac{1}{2}\cos\left(\alpha-\beta\right)-\frac{1}{2}\cos\left(\alpha+\beta\right)\\& = \frac{1}{2}\cos\left(2x-\alpha-\beta\right)-\frac{1}{2}\cos\left(\alpha+\beta\right) \\& = -\sin\left(\frac{2x-\alpha-\beta+\alpha+\beta}{2}\right)\sin\left(\frac{2x-\alpha-\beta-\alpha-\beta}{2}\right) \\& = -\sin{x}\sin\left(x-\alpha-\beta\right)\end{aligned}$$ Thus, we have: $$\begin{aligned} & -\sin{x}\sin\left(x-\alpha -\beta\right) = \sin^2{x} \\& \Rightarrow \sin^2{x}+\sin{x}\sin\left(x-\alpha-\beta\right) = 0 \\& \Rightarrow \sin{x}\left(\sin{x}+\sin\left(x-\alpha-\beta\right)\right) = 0\end{aligned} $$ So either $\sin{x} = 0$ or $\sin{x} = \sin\left(\alpha+\beta-x\right)$. If $\sin{x} = 0$, then we have $\sin{x} = 0 \Rightarrow \sin{x}$ $= \sin\left(0\right)$ $\Rightarrow x = n\pi$ or $(2n+1)\pi$ for $n\in\mathbb{Z}$ -- we can write this as $k\pi$, where $k\in\mathbb{Z}$. If, on the other hand, $\sin{x} = \sin\left(\alpha+\beta-x\right)$ then $x = 2n\pi+\alpha+\beta-x \Rightarrow x = n\pi+\frac{1}{2}\left(\alpha+\beta\right)$, or $ x = 2n\pi+\alpha+\beta-x $ (EDIT: this was meant to be $x = (2n+1)\pi-\alpha-\beta+x$), which contains no solutions (is that the right way to put it?). Thus the solutions for the equation are $n\pi+\frac{1}{2}\left(\alpha+\beta\right)$ and $k\pi$, for any integers $k$ and $n$. Question: The answer in the book has the condition $\alpha+\beta \ne (2m+1)\pi$ -- why is that? Request: If you have the time, please critique the way my solution is presented (reasoning, use of notation, flow etc  - I've an admission test in which this plays big part soon). Is my use of $\Rightarrow$ ok?",['trigonometry']
26607,Braid groups and representations,"I was wondering if $\mathbb{Z} \wr S_n$, where $\mathbb{Z}$ is the usual group of integers, $S_n$ the symmetric group on n elements and $\wr$ the wreath product of two groups, contains the braid group $B_n$. I was also wondering if $n+1$-dimensional matrices of the form : $$\begin{bmatrix} 1&0&0 \\\\ 1&0&1 \\\\ 1&1&0 \end{bmatrix}$$ for B2 $$\begin{bmatrix} 1&0&0&0 \\\\ 1&0&1&0 \\\\ 1&1&0&0 \\\\ 0&0&0&1 \end{bmatrix}$$
and
$$\begin{bmatrix} 1&0&0&0 \\\\ 0&1&0&0 \\\\ 1&0&0&1 \\\\ 1&0&1&0 \end{bmatrix}$$
 for B3 and so on... form a representation of the braid group $B_n$. Thank you for your help A. Popoff","['wreath-product', 'linear-algebra', 'group-theory', 'abstract-algebra']"
26609,"Given particle undergoing Geometric Brownian Motion, want to find formula for probability that max-min > z after n days","Consider a particle undergoing geometric brownian motion with drift $\mu$ and volatility $\sigma$ e.g. as in here . Let $W_t$ denote this geometric brownian motion with drift at time $t$. I am looking for a formula to calculate: $$
\mathbb{P}\big(\max_{0 \leq t \leq n} W_t - \min_{0\leq t \leq n} W_t > z\big)  
$$
The inputs to the formula will be $\mu$, $\sigma$, $z$, and $n$.","['stochastic-processes', 'probability']"
26623,Smooth Manifold with Trivial Tangent Bundle,"So, I'm a little confused about one statement made in class today : If M is a smooth manifold without boundary such that the tangent
  bundle of M is trivial, then M is orientable. Is this always true ?","['manifolds', 'differential-geometry']"
26626,Sequence of powers of Gaussian integers capturing all positive integers?,"Fix a complex number $z=x+iy$ where $x,y\in \mathbf{Z}$ Consider the sequence generated by the powers $$z^0, z^1, z^2, z^3,z^4 \ldots$$ The question is whether it is possible to capture any positive integer as either the real part or the imaginary part of this sequence of the powers of a fixed complex number. From some calculation and reasoning, it is very easy to see that in order to generate all positive integers, $x$ and $y$ have to be relatively prime; it appears that the sequence of real parts and the sequence of imaginary parts blows up very early and hence it would not be unwise to conjecture that such a complex number does not exist. Is there any way to get hold of this problem? Thanks in advance. My curiosity partly grew out of observations on various polynomial bijection questions e.g. , I saw on MathOverflow . I hope that at least the simple statement of the problem appeals to one's thoughts.","['polynomials', 'complex-analysis', 'number-theory']"
26628,Infinitely differentiable function with compact support,"I already know that the function
$$
f(x) = 
\begin{cases}
\exp(- \frac{1}{x^2}), \quad  x > 0 \\
0 , \quad x \leq 0

\end{cases}

$$ is infinitely differentiable throughout $\mathbb R$. The only real problem, of course, lies in showing that $f^{(k)} (0) = 0$ for any positive integer $k$. What I have not been able to deduce is that $$
\phi(x) = 
\begin{cases}
\exp(- \frac{1}{1 - x^2}), \quad  |x| < 1 \\
0 , \quad |x| \geq 1

\end{cases}

$$ is also infinitely differentiable throughout $\mathbb R$, using the previous function. The problem now is finding out what happens at $x = 1,-1$. Does the substitution $\zeta ^2 = 1 - x^2$ work, or is there another way to prove this? Thank you all!",['calculus']
26629,How can I re-arrange this equation?,I haven't used my algebra skills much for years and they seem to have atrophied significantly! I'm having real trouble working out how to re-arrange a formula I've come across to get $x$ by itself on the left hand side. It looks like this: $\frac{x}{\sqrt{A^{2}-x^{2}}}=\frac{B+\sqrt{C+Dx}}{E+\sqrt{F+G\sqrt{A^{2}-x^{2}}}}$ I've tried every method I can remember but I can't get rid of those pesky square roots! Any ideas?,['algebra-precalculus']
26632,Monotonic behavior of a function,"I have the following problem related to a statistics question: Prove that the function defined for $x\ge 1, y\ge 1$,
$$f(x,y)=\frac{\Gamma\left(\frac{x+y}{2}\right)(x/y)^{x/2}}{\Gamma(x/2)\Gamma(y/2)}\int_1^\infty w^{(x/2)-1}\left(1+\frac{xw}{y}\right)^{-(x+y)/2} dw$$ is increasing in $x$ for each $y\ge 1$ and decreasing in $y$ for each $x\ge 1$. (Here $\Gamma$ is the gamma function.) Trying to prove by using derivatives seems difficult.","['statistics', 'special-functions', 'analysis']"
26640,Do you know of any Calculus text defining the exponential and logarithm functions in an alternative way?,"The story in nearly every introductory Calculus book is well known by everybody: you don't have the ""right"" to raise a number to an irrational power, so forget exponents for now and let's take a look at $y=x^{-1}$. How odd, the innocent formula for a power function's antiderivative breaks down but gee, it must have an antiderivative, it's smooth! Let's examine its properties... ...and in the end, Rosebud was his sled, no, wait, the mysterious antiderivative turns out to have an inverse that corresponds exactly to the elementary school concept of exponents, only it works for irrational exponents too! The hero wins! The End. But what if we start from the opposite end? Start with the innocent, only-defined-for-rationals (so far) exponential function $y=k^x$, $k>0$, and if $x_0$ is irrational, prove that, as $x$ (while staying rational) approaches $x_0$, $k^x$ approaches some specific real number. Define that such number is $k^{x_0}$. And from there, prove that our New! Improved! $k^x$ is continuous, has a derivative that's also an exponential, that there is some $k=e$ for which the exponential is its own derivative, that the inverse of $e^x$ has $x^{-1}$ as its derivative etc etc... Do you know of any Calculus text that takes that approach?","['logarithms', 'calculus', 'definition']"
26644,Showing a subset $A$ of $\omega$ is infinite,"This is a concept that seems very intuitive to me, but I feel my proof is messy. Could someone more experienced than I perhaps offer some criticism/tell me if I'm even correct? Let $A\subset\omega$. I want to show that $A$ is infinite if and only if
$$
\forall_m(m\in\omega\implies\exists_n(n\in A\land (m\lt n))).
$$ Here $a<b\iff a\in b$ for $a,b\in\omega$. Also, $A\sim B$ means two sets are in bijection. I think my proof is too roundabout. I prove $\implies$ by the contrapositive. Suppose the above property is not true. So there exists an $m$ such that there is no $n\in A$ such that $m<n$. This means for all $n\in A$, $n\leq m$, which implies $n\lt m^{+}$. So $A\subseteq m^+$. If $A=m^+$, then obviously $A\sim m^+$, so by definition $A$ is finite.
If $A\subsetneq m^+$, then we know $A\sim p$ for some $p<m^+$, so $A$ is finite by definition. For $\impliedby$, I feel things get even worse. I try to prove the contrapositive by contradiction. I suppose $A$ is finite, but the property holds. Now if $A$ is empty, the property is false, a contradiction, so the contrapositive holds as needed. So assume $A$ is nonempty. Since $A$ is finite, $A$ must have a greatest element. (I try to justify this by applying the well-ordering principle on the reverse ordering. This is probably my biggest concern, is it acceptable?) Denote this element $k$. Consider $k^+$. Then $k^+\in\omega$, and by the property, we have that there is a $p\in A$ such that $k^+<p$, but $k<p$, a contradiction. Apologies for this messy proof. How can I clean it up/correct it? Many thanks.",['elementary-set-theory']
26656,Motivation for product structure,"I guess I understand some reasons why we should care for complex structure on manifolds, but what is the reason why product structure is studied? Does it arise naturally somewhere? The product structure on a smooth manifold $M$ is given by a (1,1)-tensor $E$ such that $E \neq \pm1$ and $E^2 = 1$ and the following integrability condition holds: $$E[X,Y] = [EX,Y] + [X,EY] - E[EX,EY]$$
Compare it to a complex structure $J$ such that $J^2 = -1$ and $$J[X,Y] = [JX,Y] + [X,JY] + J[JX,JY]$$ I'm still studying basic literature, one thing I do know is that on a Lie group it $E$ induces a double algebra structure on a correspoinding Lie algebra, and thus two foliations on the group, if I remember correctly; I guess it is generalized for an arbitrary manifold, so this 'splitting' of the tangent bundle and how it's compatible with other structures (complex, Hermitian, Kähler etc., e.g. a complex product structure is defined by the two respective structures such that $JE = -EJ$) is probably one of the reasons to study it, but I'm not sure this reason is good enough on its own.","['differential-geometry', 'soft-question']"
26662,Eigenvalues and eigenvectors in physics?,How can we physically interpret an eigenvalue or an eigenvector in linear algebra?,"['linear-algebra', 'eigenvalues-eigenvectors']"
26672,"Finding limit of function with more than one variable: $\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}}$","$$\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}}$$ Approaching (0,0) along x or along y both result in the limit approaching 0, so you want to make sure that the limit exists by doing more tests. My solutions manual uses $x = y^2$ (or $y = x^2$). Why either of those? Why not $y=x$ or $x=y$? Why a parabola?","['multivariable-calculus', 'calculus', 'limits']"
26676,What is the difference between outer measure and Lebesgue measure?,"What is the difference between outer measure and Lebesgue measure? We know that there are sets which are not Lebesgue measurable, whereas we know that outer measure is defined for any subset of $\mathbb{R}$.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
26678,"How to calculate the expected value $E(XY)$ with known $E(X)$, $E(Y)$ and $\sigma_{i}$?","I am trying to understand the value of $\bar{x_{1} x_{2}}+E(x_{1} x_{2})$. For all $i$, $E(x_{i})$ and $\sigma_{i}$ are given. Wikipedia gives the joint probability density function: $E(XY) = \int \int x y j(x,y) dx dy$ then I can find out from wikipedia that: $E(XY) = Cov(X,Y) + E(X)E(Y)$ and by Cauchy-Swartz: $| Cov(X,Y) | \leq \sigma(X) \sigma(Y)$ but I cannot find a precise formula to find the value of $E(XY)$, only an upper bound with finite variances. A crux point to find the matrix $\rho$ so that I can calculate the $\sigma$ matrix. So how can I calculate the $E(XY)$ when only $E(x_{i})$ and $\sigma_{i}$ for all $i$ are given?","['statistics', 'standard-deviation']"
