question_id,title,body,tags
4632279,Basis for infinite dimensional vector space definition,I'm reading Luenberger's Optimization by Vector Space Methods which has the following definition: A finite set $S$ of linearly independent vectors is said to be a basis for the space $X$ if $S$ generates $X$ . A vector space having a finite basis is said to be finite dimensional . All other vector spaces are said to be infinite dimensional . Am I going crazy for thinking that this definition does not really allow for infinite dimensional spaces since $S$ is defined as a finite set? Maybe there's some subtlety I'm missing here?,"['linear-algebra', 'vector-spaces']"
4632290,Is there a scenario for when changing the order of different quantifiers in a nested quantifier retain the original meaning?,"I was exploring the difference in meaning of a proposition when changing the order of two different quantifiers in a nested quantifier. I've been sitting here playing around with various scenarios and doing a bit of research, but I'm unable to come to a decisive answer. Does changing the order of different quantifiers always change the meaning of a logical proposition? There was two scenarios I was exploring: Not changing the order of Quantifiers but keeping the order of the variables (e.g. For all x there exists a y ,changed to, For all y there exists an x) Changing both the order of the quantifiers and the variables (e.g. For all x there exists a y ,changed to, There exists an x for all y) If anyone could give me a scenario or a reasoning as to why it's not possible, that would be amazing. Thanks in advance.","['boolean-algebra', 'logic', 'discrete-mathematics']"
4632291,"If we have an open set $S$ and multiply every element of $S$ by a fixed complex number $z_{0}$, is the resultant set open?","Suppose S in a complex open set with elements $s \in \mathbb{C}$ , if we multiply all elements by a fixed complex number $z_0$ , will the resultant set be open as well? My intuition says yes, as multiplying a given open set geometrically has the effect of rotating and stretching the area defined by the set, so this new area should also be open. I am just not sure how I would go about proving it. Any help would be appreciated. EDIT: As some have pointed out the constraint on $z_0$ is that it is non-zero.",['complex-analysis']
4632299,Holomorphic maps into ruled surfaces,"Let $\Sigma$ be a compact Riemann surfaces. Let $L \to \Sigma$ be a holomorphic line bundle. This gives rise to a ruled surface $\mathbb{P}(\underline{\mathbb{C}} \oplus L) \to \Sigma$ , where $\mathbb{P}$ denotes the fiber-wise projectivization and $\underline{\mathbb{C}}:= \Sigma \times \mathbb{C} \to \Sigma$ is the trivial line bundle. Now, we can take any meromorphic section of $s$ of $L$ , which then gives rise to a holomorphic section $u$ of the ruled suface via the following construction:
As long as $z$ is no pole of $s$ , we define the section $u$ to be $$
u(z)=[1:s(z)]
$$ Note that this section is holomrophic if we take the chart of $\mathbb{CP}^1$ not containing the point $[0:1]$ . If $p$ is a pole, we have locally that $s$ contains no other pole or zero and we can write locally $$
s(z)=z^{-m}g(z)
$$ for some $m \in \mathbb{N}$ and $g(z)$ zero- and polefree. Then we define $$
u(z)=z^{-m}[1:g(z)],
$$ which is holomorhpic, if we take the chart of $\mathbb{CP}^1$ not containing $[0:1]$ . So meromorphic sections of $L$ give rise to holomorphic sections of $\mathbb{P}(\underline{\mathbb{C}} \oplus L)$ . My question now is: Does every meromorphic section of this ruled surface arise in that way? I have the feeling, wether it does work or does not work, relies somehow on the compactness of $\Sigma$ . There is the know result on the complex plane (non-compact), for example, that every meromorphic function on $\mathbb{C}$ must not be rational (take $e^z$ ), however, on its compactification, it must be rational. Furthermore, my knowledge in algebraic geometry is a a bit lacking, so the question might even yield a trivial answer.","['complex-analysis', 'complex-geometry', 'algebraic-geometry', 'differential-geometry']"
4632315,Differentiation of Composite Functions (Chain Rule Related),"Suppose I have the arbitrary functions $f, g$ , and $h: \mathbb{R}^2 \longrightarrow \mathbb{R}$ . The question I have is, what is the derivative with respect to $x$ of: \begin{equation}
f(g(x,y), h(x, y))
\end{equation} It is important that the functional relationship between g(x,y) and h(x,y) remains arbitrary. I do understand the chain rule if it were to be applied to a simple case such as $f(g(x,y))$ , in which case this would be equal to $f_x(g(x,y))g_x(x,y)$ , where $f_x$ and $g_x$ are the partial derivatives with respect to $x$ . Thank you.","['calculus', 'derivatives', 'chain-rule']"
4632367,On the differentiability of increasing functions,"I'm trying to follow the proof of this website http://mathonline.wikidot.com/lebesgue-s-theorem-for-the-differentiability-of-monotone-fun about the differentiability a.e. of increasing functions. Although, there's something I don't understand. It says: ""Now note that for each $k\in\{1,2,...,n\}$ we have that $E\cap[c_k,d_k]\subseteq\{x\in[c_k,d_k]:\overline{D}f(x)\geq|\alpha|\}$ "". I don't know how to obtain that absolute value of $\alpha$ . The only thing I know about that set is the following: $$E\cap[c_k,d_k]=\{x\in[c_k,d_k]:\underline{D}f(x)<\beta<\alpha<\overline{D}f(x)<+\infty\}.$$ Thanks in advance.","['lebesgue-measure', 'monotone-functions', 'bounded-variation', 'calculus', 'derivatives']"
4632399,"Easy ways to prove that a binary $[9,3,5]$ linear code can't exist","I've tried to solve an exercise that goes like this: Prove that a $[9,3,d]$ linear binary code that corrects 2 mistakes doesn't exist. To prove this, first I've determined that $d=5$ is the most restrictive case. Then, I've tried to use Hamming's, Singleton's and Plotkin's bounds in order to easily see if said code can't exist but I haven't been able to prove anything with the bounds. Another thing I've tried is to use the fact that $d-1$ is the largest number of linearly independent columns in the control matrix. This way, I've proved that said code cannot exist by analizing the different combinations of vectors to put in each column. However, this has taken such a long time and I was wondering wether there was a time efficient way to solve this kind of exercises. Thanks in advance","['linear-algebra', 'combinatorics', 'discrete-mathematics', 'upper-lower-bounds', 'coding-theory']"
4632492,Finding the derivative of $y = x^{(x+1)(x+2)(x+3)(x+4)\ldots(x+n)}$. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I'm trying to find the derivative of this function with respect to $x$ : $$y = x^{(x+1)(x+2)(x+3)(x+4)\ldots(x+n)}$$ I was thinking about using $\ln$ to solve this, but I'm not sure if that's the right way to go about it. Any ideas or suggestions would be greatly appreciated. Thanks for your help!","['calculus', 'derivatives', 'products']"
4632595,Conjecture about difference of Fibonacci numbers and primes,"I'm curious to see if this conjecture is true: if $ m > 4 $ is a positive integer not divisible by $ 2 $ or $ 3 $ , it's possible to find a positive integer $ n $ such that the difference of the $ 2 $ Fibonacci numbers $ F_{m+n}-F_n $ is a prime number. Various example of $ [m, n] $ , with the smallest n, are $[5,3],[7,3], [11,4], [13,5], [17,3],..., [619,1353]...$ I don't know if this could be of any help: I converted the primes I found that way in the Zeckendorf representation and they are an alternating sequence of $ 1 $ and $ 0 $ ending with two adjacent $ 0 $ and $ 1 $ (and eventually just various $ 0 $ ) Edit: I posted this problem on Mathoverflow too","['number-theory', 'conjectures', 'fibonacci-numbers', 'prime-numbers']"
4632689,Equivalence of definition of group action,"It is known that $\varphi:G\times X\to X$ is a group action of group $G$ on non-empty set $X$ if $\varphi(e, x)=x$ , $\varphi(hg, x)=\varphi(h, \varphi(g, x))$ for all $g, h\in G$ and for all $x\in X$ . Assume that $G$ is finitely generated group and generating set is $S$ i.e. $G=\langle S\rangle$ . It is clear that if $\varphi:G\times X\to X$ is a group action, then $\varphi(s_1s_2, x)=\varphi(s_1, \varphi(s_2, x))$ for all $s_1, s_2\in S$ and for all $x\in X$ and $\varphi(e, x)=x$ . What can say about the converse of it?
Is it true that if $\varphi(e, x)=x$ , $\forall x\in X$ , $\varphi(s_1s_2, x)=\varphi(s_1, \varphi(s_2, x))$ $\forall s_1, s_2\in S$ and  all $x\in X$ then $\varphi:G\times X\to X$ is a group action? Please help me to know it.","['group-theory', 'group-actions']"
4632693,Seeking a practical layperson's explanation of the math behind spoke length selection for wheelbuilding,"I build bicycle wheels for a living. I am exceptionally frustrated with the quality and consistency of the calculation tools available for spoke lengths. For those of you who donâ€™t build wheels, let me mention that the spoke length needs to be accurate within a millimeter, (less than 1/3 of one percent). Otherwise you run the risk of spokes which are too long (pops your tire) or too short (unsightly and creates a weak wheel). Compounding the task, manufacturing tolerances of rims, spokes, and hubs vary and they stretch and deform during building. With such a moving target, a tiny error in length calculation can become a time consuming failure. Here is the basic mathematical formula for spoke length determination: $$ l = \sqrt {a^2+r{_1}^2+r{_2}^2-2r_1r_2 cos(\alpha)}$$ Where: a = distance from the central point of the hub to the hub flange, for example 30 mm. This is the base of the triangle that defines the length of the spoke in a radial pattern or zero cross pattern. r1 = spoke hole circle radius of the hub. This is the radius of the circle defined by the center of the spoke holes in the hub flange. r2 = nipple seat radius, equal to half the ERD of the rim. (ERD is effective rim diameter. It is the circle defined by the point where the spoken nipple contacts the rim. It is effectively the endpoint of the spoke on the rim end.) m = number of spokes to be used for one side of the wheel, for example 36/2=18 (Wheels are typically built with 24, 28, 32, or 40 total spokes, with half that number being used for each side of the wheel) k = number of crossings per spoke (This number is difficult to explain, but will be either zero, one, two or three. 95% of my wheels are 3X, with the remainder being 2X.  It refers to the number of times in a given spoke crosses over or under other spokes in the wheel. More crossings equal a stronger wheel because there is more crossbracing between spokes. More in-depth explanation here ) Î± = 360Â° k/m For most bicycle wheel builders, this is an intimidating formula. I would like to see a practical demonstration of this formula in use. For a given hub and rim combination, how do I calculate spoke length using this formula? An example data set would be: A Rohloff Speedhub 500/14 is a fairly simple hub. It is a symmetrical
pattern hub. Both hub flanges have a circle diameter of 100mm $(r^1)$ . The
manufacturer specifies a two cross lacing $(k)$ on this hub. The center to flange distance on both sides of the hub is also
symmetrical, with a distance of 29mm $(a)$ from the center to each flange. This particular hub and rim are drilled for 36 spokes $(m)$ . An ERD of 594mm $(r^2)$ for a Sun Ringle MTX33 rim completes the necessary
data. As I recently built this wheel, I know that it requires a spoke length of 260mm. However, given only this information, this formula, and a pencil and paper, I would not be able to calculate the correct spoke length. Please help me understand how to solve this equation given this data set. In addition, I would like to know if it is possible, or rather how it is possible, to re-create this formula in an Excel spreadsheet or similar program. Any help is greatly appreciated.","['trigonometry', 'recreational-mathematics']"
4632714,Why is the volume of a cone not half the volume of the cylinder of which it is a part?,"Now I'm aware of the proof for the volume of a cone being a third of the volume of the corresponding cylinder and the proof is satisfactory. Here's an alternate approach that seems to imply that the volume of the cone is actually half the volume of the cylinder. Now it clearly is a wrong result and hence my approach as a whole or some step  within it is wrong. I'm not able to spot the error here and would appreciate help. It is as follows, Imagine a cylinder with a radius r and a height h. Imagine the cylinder to be translucent. Now construct a cone inside the cylinder. Draw an altitude right through the middle of the cone, through the core of the entire structure (cone inside the cylinder). Imagine the structure right in front of your eyes. Consider a 2D vertical cross section of the shape right through the middle. The cone can be divided into two triangles on either side of the altitude with areas equal to (rh)/2. There are two inverted triangles on either side of the cone which would become the parts of the left-over cylinder should the cone be removed. Areas of these triangles are also (rh)/2. This parity of the two inner triangles(inside the cone) and two outer triangles (outside the cone) would be true throughout the cylinder. In other words, an infinite number of such pairs exists about the central altitude line. All such pairs, when added up, make up the solid cylinder. This implies that the volume of the cone and the volume of the left-over shape when the cone is removed are the same, that is, the cone is half the volume of the cylinder. What am I missing? I apologise for framing such a long and maybe, to some degree, an unintelligible question. I would have provided an animation had I the know-how.","['fake-proofs', 'geometry', 'volume']"
4632734,"Closed form of a function $f(z)$ whose $\it{only}$ zeros are $n\pi$,$n\pi\omega$ and $n\pi\omega^2$","I need a closed form of  a function $f(z)$ whose $\it{only}$ zeros are $n\pi$ , $n\pi \omega$ and $n\pi\omega^2 $ where $\omega$ is a cube root of unity and $n\in\mathbb{N}$ . So we need a function of type $$f(z)=\prod_{n=1}^{\infty}\left(1-\frac{z}{n\pi}\right) \left(1-\frac{z}{n\pi\omega}\right)  \left(1-\frac{z}{n\pi\omega^2}\right) $$ To find a closed form for above function we take logarithmic derivatives assuming $z$ which are not the points of zeros to get $$\frac{f'(z)}{f(z)}=\sum_{n=1}^{\infty}\left(\frac{1}{z-n\pi}\right)+\sum_{n=1}^{\infty}\left(\frac{1}{z-n\pi\omega}\right)+ \sum_{n=1}^{\infty}\left(\frac{1}{z-n\pi\omega^2}\right) $$ Any help would be highly appreciated.","['analysis', 'real-analysis', 'complex-analysis', 'functions', 'infinite-product']"
4632752,"What is the order of the braid group with n strands, on the 2-sphere?","Consider the the braid group of $n$ strands on the 2-sphere. Visually, this is a braid between two concentric circles. How many different braids are there for a given $n$ ? I have tried drawing the Cayley diagram for $n=3$ , which turns out to have order 12. Here is the diagram, with generators $a,b$ having inverses $A,B$ , with identity $e$ . This shows 2 different layouts of the same graph. I have also tried drawing the Cayley diagram for $n=4$ , but it seems to be much larger. Reminder of the definitions: The braid group on the 2-sphere with $n$ strands is generated by $s_1,\dots,s_{n-1}$ each representing a single-crossing braid between adjacent strands; this obeys the following relations: $s_is_{i+1}s_i = s_{i+1}s_is_{i+1}$ (the ""braid identity"") $s_is_j = s_js_i$ unless $i =j-1$ or $i=j+1$ (non-adjacent crossings commute) $s_1s_2\dots s_{n-1}~s_{n-1}\dots s_2s_1 = e$ (a special identity for the 2-sphere)","['group-theory', 'braid-groups']"
4632818,Guess the particular solution to an exponential function`?,"Solve this differential equation $y''+2y'+y = e^{-t}$ . I got the homogenous solution to be $y_h= (Bt + C)e^{-t}$ But I don't know what my guess to the particular function should be? $Ae^{-t}$ solves the homogenous function so it can't be it. Then my
next try was $Ate^{-t}$ , but this is what I got: $y = Ate^{-t}$ $y'=A(e^{-t} - t^2 e^{-t})$ $y'' = A(-te^{-t} - 2te^{-t} +t^3e^{-t})$ Then I when I put it in the equation I get. $Ae^{-t}(t^3-2t^2-2t+2)=e^{-t}$ But I have two variables and therefore I can't solve it. I need help.","['ordinary-differential-equations', 'multivariable-calculus', 'calculus', 'derivatives', 'exponential-function']"
4632826,sum of 100 terms of logarithmic expression,Calculate value of $\displaystyle \sum^{100}_{k=1}\ln\bigg(\frac{(2k+1)^4+\frac{1}{4}}{16k^4+\frac{1}{4}}\bigg)$ My try :: $\displaystyle x^4+4y^4$ $=(x^2+2xy+2y^2)(x^2-2xy+2y^2)$ So sum $\displaystyle \sum^{100}_{k=1}\ln\bigg(\frac{1+4(2k+1)^4}{1+4(2k)^4}\bigg)$ $\displaystyle =\sum^{100}_{k=1}\ln\bigg[\frac{(1+2(2k+1)+2(2k+1)^2)(1-2(2k+1)+2(2k+1)^2)}{(1+2(2k)+2(2k)^2)(1-2(2k)+2(2k)^2)}\bigg]$ How can I decompose that complex expression into partial fractions?,['sequences-and-series']
4632869,Use of Poincare inequality,"Suppose $\Omega \subset \mathbb{R}$ is a bounded domain. Then using
Poincare-Wirtinger, one can prove that for functions $f \in W^{1,2}(\Omega)$ there exists $C>0$ such that $$\|u\|_{L^{2}}^{2} \le C\|u'\|_{L^{2}}^{2} + C\left( \int u(x)~dx
 \right)^{2}.  $$ Question: Suppose we have a function $u$ which satisfies $$  \|u'\|_{L^{2}}^{2} + \left( \int u(x)~dx \right)^{2} \le C .          
 $$ Can we conclude that $u \in W^{1,2}(\Omega)$ ? The reason I am not sure is because in order for us to use the first inequality we need that $u \in W^{1,2}(\Omega)$ which a-priori we don't have. I think the statement could be true and we can prove it by some density argument but I am not sure how to proceed.","['partial-differential-equations', 'sobolev-spaces', 'functional-analysis', 'real-analysis']"
4632946,How to find the derivative w.r.t. lower limits?,"How to find $\frac{d}{dy} \int_{y}^{\infty} \int_{2y}^{\infty} y f(x_2)dx_2 f(x_1)dx_1$ , where $x_1$ and $x_2$ are two independent continuous random variables and $f(x_1)$ and $f(x_2)$ are their PDFs. Can I still use Leibniz's rule?","['integration', 'derivatives', 'probability', 'leibniz-integral-rule']"
4633052,Find the sum of $\sum_{k=1}^n k(k-1) {{n} \choose {k-1}}$,"Find the sum of $$\sum_{k=1}^n k(k-1) {{n} \choose {k-1}}$$ The solution in the book is a lot different than what I tried
what they did in the book is say $k-1=t$ then they expanded from this point $\sum_{t=0}^{n-1} (t+1)(n) {{n-1} \choose {t-1}}$ and the final answer is $n[(n-1)(2^{n-2}-1)+2^n-2]$ but I did not understand their way as it complicated to me I thought about using derivatives but I got stuck and I am not sure if it really works I just remember using derivatives for some cases in class and I tried it $(x+b)^n=$ ${n \choose 0} x^0 b^{n-0}+ {n \choose 1} x^1 b^{n-1} +...+ {n \choose n} x^n b^{n-n} $ derivative with respect to $x$ is $n(x+b)^n-1=$ $0+ {n \choose 1} b^{n-1} +...+ {n \choose n} nx^{n-1} b^{n-n} $ then derivative again $n(n-1)(x+b)^n-2=$ $0+0+ {n \choose 2} 2b^{n-2} +...+ {n \choose n} n(n-1)x^{n-2} b^{n-n} $ but I could not continue from here is it ok to use derivatives ? is there another way other than the book(just expanding)? thanks for any tips and help!","['summation', 'binomial-coefficients', 'combinatorics']"
4633060,Motivation of complex numbers : Relation between exponential and trigonometric functions,"I have a query regarding the motivation of complex numbers. If we expand the functions $e^{ix},\cos x$ & $\sin x$ in terms of Taylor series, then comparing real and imaginary parts we get $e^{ix}=\cos x+i\sin x.$ With the help of this we can express $\cos x$ and $\sin x$ in terms of $e^{ix}$ and $e^{-ix}.$ But what is the geometrical interpretation of this expression? i.e. Why the exponential function can be expressed in terms of trigonometric functions? In particular, any equation of an oscillation contains either sine function or cosine function or both. Mathematically due to Hooks law the restoring force is proportional to negative displacement(since displacement tends to the equilibrium position). The solution to the corresponding differential equation involves the imaginary number `i' and the trigonometric functions. But what is the philosophical explanation behind this? Without this mathematical computation how can we explain the involvement of imaginary axis in oscillatory motion geometrically? Will the oscillatory curve be considered as two-dimensional only? Since trigonometric functions are involved, what is the significance of the corresponding angle? Please help me to understand the concept properly.
Thanks in advance.","['complex-analysis', 'trigonometry', 'exponential-function']"
4633066,Continuity of maximum eigenvalue function,"Let $f:S^n_{+}(\mathbb{R})\rightarrow \mathbb{R}_+$ be defined as $$f(A)=\lambda_{\max}(A),$$ where $S^n_{+}(\mathbb{R})$ is set of all positive definite $n\times n$ matrices.
What can we say about the continuity and differentiability of $f$ ? Intuitively, I feel that it is continuous because if we perturb a matrix, its eigenvalues do not change abruptly. But I don't know how to approach it.","['matrix-calculus', 'functions', 'functional-analysis', 'real-analysis']"
4633184,$\int_{0}^{1}\ln^2\Gamma(x)\ln\Gamma(1-x)\ dx$,"I came across the following integral $$\int_{0}^{1}\ln^2\Gamma(x)\ln\Gamma(1-x)\ dx$$ and I have no idea where to even start. I know you can interchange the $x$ and $1-x$ by king's property, and that $\int_{0}^{1}\ln^2\Gamma(x)dx$ has a nice closed form but otherwise I don't know what to do.","['integration', 'gamma-function', 'definite-integrals', 'logarithms']"
4633199,Is it possible to define a topology on the real line such that 0 and non-zero integers are dense but no finite subset of non-zero integers is dense?,"Is it possible to define a topology $\mathcal{T}$ on $\mathbb{R}$ such that $\{0\}$ and $\mathbb{Z}^*=\mathbb{Z}\setminus\{0\}$ are dense but no finite $F\subseteq \mathbb{Z}^*$ is dense? This is what I know. I know that $\{0\}$ nor $\mathbb{Z}^*$ can be open. Therefore, $\{0\}$ can't be close (since that would imply that $\{0\}$ is nowhere dense) which means that $(\mathbb{R},\mathcal{T})$ can't be $T_1$ . I also know that, if the said topology were to exist, it would contain a $G_\delta$ set $E$ with empty interior such that $E\cap\mathbb{Z}=\{0\}$ . Could it be that $E=\{0\}$ , i.e. that $\{0\}$ is $G_\delta$ ? Proof of the things I know. First, $\{0\}$ nor $\mathbb{Z}^*$ can't be open since they are disjoint dense sets. Now, since $F$ isn't dense $\forall F\subseteq_{fin}\mathbb{Z}^*$ , we know that $\forall n\in\mathbb{Z}^*\exists \emptyset\neq O_n\in\mathcal{T}:n\not\in O_n$ and, since $\{0\}$ is dense, $\forall n\in\mathbb{Z}^*:0\in O_n$ . Then, we define $E:=\bigcap\limits_{n\in\mathbb{Z}} O_n$ which is a $G_\delta$ set such that $E\cap\mathbb{Z}=\{0\}$ and with empty interior since $$\text{Int}(E)\neq\emptyset\Rightarrow \text{Int}(E)\cap\mathbb{Z}^*\neq\emptyset\Rightarrow E\cap\mathbb{Z}^*\neq\emptyset$$","['dense-subspaces', 'general-topology', 'separable-spaces']"
4633254,Prove the 3 lines are concurrent,"In the figure, $A$ is the midpoint of a side of a regular 18-gon. The black polygon is a regular Nonagon. $O$ is the centre. I found that $BC, EF,OA$ are always concurrent but I couldnâ€™t prove it. Any hint or solution will be appreciated. I tried adding lines and construct some triangles but seem non of them work. I am good at plane euclidean geometry, trigonometry (Up to Year 12)but I suspect there is a elegant euclidean geometrical proof existing.",['geometry']
4633271,Are $S^1$ and $O_2(\mathbb{R})$ isomorphic?,"Let $S^1 = \{z \in \mathbb{C} | |z| = 1\}$ and $O_2(\mathbb{R}) = \{A \in GL_2(\mathbb{R})|A^TA = I_2 \}$ . Is the group $(S^1, \cdot)$ isomorphic to $(O_2(\mathbb{R}), \cdot)$ ? Prove it. This is a question from ""Prova Extramuros (2013)"", a exam from Brazil. This question asks which of the options are isomorphisms. I easily found the correct option, but in this one I had a little trouble in proving that there was no isomorphism. I interpreted $(S^1, \cdot)$ as a rotation group and $(O_2(\mathbb{R}),\cdot)$ as a rotation and reflection group, so they could not be isomorphic. Is this reasoning correct? Is there a way to prove it more rigorously?","['group-theory', 'linear-groups', 'linear-algebra']"
4633276,Simplifying the surd $\sqrt{3 - \sqrt{8}}$,"Recently I was solving a math problem and I came across the following ( Only part of the problem ): $$\sqrt{3 - \sqrt{8}}$$ Here's is what I did to simplify the above: $$(a-b)^2=a^2+b^2-2ab$$ $$\sqrt{1+2-2\sqrt{2}}$$ $$\sqrt{(1-\sqrt{2})^2}$$ $$=1-\sqrt{2}$$ When you expand $(1-\sqrt{2})^2$ you do get $3 - \sqrt{8}$ . However, I found that upon the expansion of $(\sqrt{2}-1)^2$ I also arrive at the same answer as before $3 - \sqrt{8}$ . Yet the in the solution of the problem I was doing it required me to use the latter $(\sqrt{2}-1)^2$ to find the correct answer. $(1-\sqrt{2})^2$ did not help to solve the problem. Why is this the case? Why does the latter only hold true? I suspect this might have something to do with the square root function only returning a positive value. Might this be the case? If so, could someone explain why a square root function can only return a positive value. My working above seems to give a negative answer ( As $\sqrt{2}>1 )$ in the square root function, but I can't seem to find what I'm doing wrong.","['algebra-precalculus', 'functions', 'radicals']"
4633280,Can you get a $\mathbb{C}$-basis of $\mathbb{C}^{n}$ from an $\mathbb{R}$-basis by picking one vector out of each of $n$ pairs?,"Let $T = \{v_1, \dots,  v_{2n}\} \subseteq \mathbb{C}^n$ be a $\mathbb{R}$ -linearly independent set of vectors. Now consider the $2^n$ subsets $S \subseteq T$ of size $n$ which contain exactly one of $v_1$ and $v_{n+1}$ , exactly one of $v_2$ and $v_{n+2}$ etc. Question: Is one of these subsets necessarily a $\mathbb{C}$ -basis? This is equivalent to asking whether there exist $t_1, \dots, t_n \in \mathbb{Z}$ such that the vectors $v_1 + t_1 v_{n+1}, \:v_2 + t_2 v_{n+2}, \dots$ are $\mathbb{C}$ -linearly independent. Proof. Suppose such $t_i$ exist. Then $(v_1 + t_1 v_{n+1}) \wedge \dots \wedge (v_n + t_n v_{n+1})$ is a nonvanishing polynomial in $\underline{t} \in \mathbb{R}^n$ , so one of its coeffcients must be nonzero. But each coefficient is the wedge of one of the sets $S$ as above. Conversely, if you have a subset $S$ corresponding to replacing $v_j$ by $v_{n + j}$ for some $j \in A \subseteq \{1, \dots, n\}$ then you can set $t_j = 0$ for $j \not\in A$ and pick $t_j$ very large for $j \in A$ . Since being linearly independent is an open condition, this will give linearly independent vectors $v_j + t_j v_{n+j}$ . $\square$","['linear-algebra', 'abelian-varieties']"
4633379,"Check an $\epsilon-\delta$ proof of $\lim_{ (x,y) \to (3,1)} \frac{x}{y} = 3$","I am going to prove $$\lim_{ (x,y) \to (3,1)} \frac{x}{y} = 3$$ by using $\epsilon-\delta$ argument. My Attempts Preliminary Analysis For every $\epsilon >0$ , we need to find a suitable $\delta$ such that: If $\Vert (x,y)-(3,1)\Vert _2 < \delta$ then $\left| \frac{x}{y} - 3\right| < \epsilon.$ First note that $$\left| \frac{x}{y} - 3\right| = \left| \frac{x}{y} - 3y + 3y -3\right| \le \left|\frac{1}{y}\right||x-3| + 3|y-1|.$$ It is obvious that $$|x-3| \le \Vert (x,y)-(3,1)\Vert_2\; \text{and} \; |y-1| \le \Vert (x,y)-(3,1)\Vert_2.$$ Suppose $\Vert (x,y)-(3,1)\Vert_2 < \frac{1}{2}$ then $|y-1| <\frac{1}{2}.$ As a result we have $$\frac{1}{2} < y < \frac{3}{2} \Longrightarrow \frac{1}{|y|} < 2.$$ Hence, $$\left| \frac{x}{y} - 3\right| \le \left|\frac{1}{y}\right||x-3| + 3|y-1|< 2\delta+3\delta = 5\delta.$$ Formal Proof For every $\epsilon > 0$ , we must
choose $$\delta= \min\left\{\frac{1}{2}, \frac{\epsilon}{5}\right\}.$$ If $\Vert (x,y)-(3,1)\Vert _2 < \delta$ then $\left| \frac{x}{y} - 3\right| < \epsilon.$ Is my proof correct?","['multivariable-calculus', 'solution-verification', 'epsilon-delta']"
4633388,Evaluating $\int_{-\infty}^{\infty}\frac{\ln\left(1+x^{8}\right)}{x^{2}\left(1+x^{2}\right)^{2}}dx$,"(Motivation) Here is an integral I made up for fun: $$\int_{-\infty}^{\infty}\frac{\ln\left(1+x^{8}\right)}{x^{2}\left(1+x^{2}\right)^{2}}dx.$$ WolframAlpha doesn't seem to come up with a closed form , but I believe it is $$2\pi\left(1+\sqrt{4+2\sqrt{2}}\right)-3\pi\left(\ln\left(\sqrt{2}\right)+\operatorname{arctanh}\left(\frac{2}{7}\sqrt{10+\sqrt{2}}\right)\right).$$ (Question) Aside from my attempt below, how else could one solve this? Could there possibly a more elegant method than constructing a 4-keyhole contour? I will try to shorten my attempt as much as possible because it is quite lengthy. If someone wants to read all of it and find any little mistakes, then I would greatly appreciate it, as someone who is seeking to expand his complex analysis skills. (Attempt) Let $f(z) = \dfrac{\log(1+z^8)}{z^2(1+z^2)^2}$ . Its pole is $z=i$ with an order of $2$ . Notice $z=0$ is a removable singularity because we can redefine $f(z) = z^6 -2z^8 +3z^{10}-4z^{12}+O\left(z^{13}\right)$ to make it holomorphic at $z=0$ . The branch points of $f(z)$ are obtained by $$1+z^8=0 \implies z = \exp\left(\frac{i\pi}{8}(2n+1)\right)$$ where $n \in \left\{0, 1, \ldots, 7\right\}.$ Also, let $B_n = \exp\left(\dfrac{i\pi}{8}(2n+1)\right)$ . Additionally, $$\log\left(1+z^{8}\right)=\log\left(\prod_{n=0}^{7}\left(z-B_{n}\right)\right)=\sum_{n=0}^{7}\log\left(z-B_{n}\right)=\sum_{n=0}^{7}\left(\log\left|z-B_{n}\right|+i\operatorname{arg}\left(z-B_{n}\right)\right).$$ Let $A_n(z) = \operatorname{arg}\left(z-B_{n}\right)$ . For $n \in \left\{0,1,2,3\right\}$ , let $A_n(z) \in \left(\dfrac{\pi}{8}\left(2n+1\right),\dfrac{\pi}{8}\left(2n+1\right)+2\pi\right)$ . For $n \in \left\{4,5,6,7\right\}$ , let $A_n(z) \in \left(\dfrac{\pi}{8}\left(2n+1\right)-2\pi,\dfrac{\pi}{8}\left(2n+1\right)\right)$ . For convenience, I created this graphic of what the contour looks like. It can also be viewed here . By Cauchy's Residue Theorem, we have $$2\pi i\operatorname{Res}(f(z),z=i) = \left(\int_{-R}^{R}+\sum_{k=1}^{8}\int_{\lambda_k}+\sum_{m=0}^{3}\int_{\gamma_m}+\int_{\Gamma}\right)f\left(z\right)dz.$$ Perhaps I will write an addendum explaining how to show the integrals over $\Gamma$ and $\gamma_m$ , for each $m$ listed in the sum, go to $0$ . This is how to calculate the residue: $$2\pi i\operatorname{Res}\left(\dfrac{\log(1+z^8)}{z^2(1+z^2)^2},z=i\right) = 2\pi i\cdot\frac{1}{\left(2-1\right)!}\lim_{z \to i}\frac{d^{2-1}}{dz^{2-1}}\frac{\log\left(1+z^{8}\right)\left(z-i\right)^{2}}{z^{2}\left(z+i\right)^{2}\left(z-i\right)^{2}} = 2\pi-\frac{3\pi}{2}\ln\left(2\right)$$ after some basic calculus grunt work. This is how to calculate the $\lambda_1$ and $\lambda_2$ integrals: $$
\begin{align}
& \lim_{\epsilon \to 0}\lim_{\lambda_1,\lambda_2 \to \Lambda_0}\lim_{R\to\infty}\left(\int_{\lambda_1}f(z-i\epsilon)d(z-i\epsilon) + \int_{\lambda_2}f(z+i\epsilon)d(z+i\epsilon)\right) \\
=&-\lim_{R\to\infty}\lim_{\epsilon\to 0}\int_{e^{i\pi/8}}^{Re^{i\pi/8}}\frac{d(z-i\epsilon)}{(z-i\epsilon)^2(1+(z-i\epsilon)^2)^2}\cdot \left(\log|1+(z-i\epsilon)^8|+i\sum_{n=0}^{7}A_n(z-i\epsilon)\right) \\
&+\lim_{R\to\infty}\lim_{\epsilon\to 0}\int_{e^{i\pi/8}}^{Re^{i\pi/8}}\frac{d(z+i\epsilon)}{(z+i\epsilon)^2(1+(z+i\epsilon)^2)^2}\cdot \left(\log|1+(z+i\epsilon)^8|+i\sum_{n=0}^{7}A_n(z+i\epsilon)\right) \\
=& -2\pi i\int_{e^{i\pi/8}}^{i\infty}\frac{dz}{z^{2}\left(1+z^{2}\right)^{2}} \tag{1}\\
\end{align}
$$ where in $(1)$ we used $\displaystyle \lim_{\epsilon \to 0}A_0(z-i\epsilon) = \dfrac{\pi}{8}+2\pi$ and $\displaystyle \lim_{\epsilon \to 0}A_0(z+i\epsilon) = \dfrac{\pi}{8}.$ Since $$\int \frac{dz}{z^2(1+z^2)^2} = -\frac{3}{2}\arctan\left(z\right)-\frac{3z^{2}+2}{2z^{3}+2z}+C,$$ we can use the Fundamental Theorem of Line Integrals to get the desired integral over $\Lambda_0$ to be $$-2\pi i\left(e^{-\frac{i\pi}{8}}+\frac{1}{4}\sec\left(\frac{\pi}{8}\right)-\frac{3}{2}\arctan\left(e^{-\frac{i\pi}{8}}\right)\right).$$ We can apply this same sort of process for the other $\lambda$ integrals. After a lot of work, combining those contributions ultimately yields $$3\pi\operatorname{arctanh}\left(\frac{2}{7}\sqrt{10+\sqrt{2}}\right)-2\pi\sqrt{4+2\sqrt{2}}.$$ Finally, we put everything together: $$2\pi-\frac{3\pi}{2}\ln\left(2\right)=\int_{-\infty}^{\infty}\frac{\log\left(1+z^{8}\right)}{z^{2}\left(1+z^{2}\right)^{2}}dz+3\pi\operatorname{arctanh}\left(\frac{2}{7}\sqrt{10+\sqrt{2}}\right)-2\pi\sqrt{4+2\sqrt{2}}+0+0+0+0+0.$$ In conclusion, the integral in question equals $$2\pi\left(1+\sqrt{4+2\sqrt{2}}\right)-3\pi\left(\ln\left(\sqrt{2}\right)+\operatorname{arctanh}\left(\frac{2}{7}\sqrt{10+\sqrt{2}}\right)\right).$$ Any answer/comment shedding some light is appreciated.","['integration', 'improper-integrals', 'complex-analysis', 'calculus', 'contour-integration']"
4633390,Proving that ${(1-\frac{2}{x^2})}^x < \frac{x-1}{x+1}$ for any $x > 2$.,Proof that ${\left(1-\dfrac{2}{x^2}\right)}^x\!< \dfrac{x-1}{x+1}$ for any $x > 2$ . any ideas?,"['limits', 'inequality', 'binomial-coefficients']"
4633391,Without Calculator find $\left\lfloor 2 \cos \left(50^{\circ}\right)+\sqrt{3}\right\rfloor$,"Without Calculator find $$\left\lfloor 2 \cos \left(50^{\circ}\right)+\sqrt{3}\right\rfloor$$ Where $\left \lfloor x \right \rfloor $ represents floor function. My Try: Let $x=2\cos(50^{\circ})+\sqrt{3}$ . We have $$\begin{aligned}
& \cos \left(50^{\circ}\right)<\cos \left(45^{\circ}\right) \\
\Rightarrow \quad & 2 \cos \left(50^{\circ}\right)<\sqrt{2} \\
\Rightarrow \quad & x<\sqrt{3}+\sqrt{2}<3.14
\end{aligned}$$ Now I am struggling hard to prove that $x>3$","['trigonometry', 'ceiling-and-floor-functions', 'monotone-functions', 'inequality']"
4633394,Range of the trigonometric function $f(x)= \frac{3}{4 - 5\sin(x)}$,"I was trying to find the range of the function $$f(x) = \frac{3}{4-5\sin(x)} $$ The first thought that came to mind was: Let's put the minimum and maximum value of $\sin(x)$ as $\{1,-1\}$ accordingly to find maximum and minimum of the function. However, then I got range $[-3,\frac{1}{3}]$ , which was totally wrong when I used a graphing calculator. The true answer is $(-\infty,-3] âˆª [\frac{1}{3}, \infty)  $ Now, I want explaination what I did wrong here. Link for graph","['trigonometry', 'functions']"
4633486,Find the angles without using trigonometry,"In the figure, given that $JA=JB$ , $\angle BAK=20^{\circ} $ , $\angle KAJ=50^{\circ} $ , $\angle KBA=40^{\circ} $ , Find $\angle BJK$ . I succeed in solving it using trigonometry (quite tedious), but the answer turn out to be very neat and i believe there must be a very simple way to solve it using plane euclidean geometry. (pretty sure it exist but i tried many construction and still donâ€™t work) I want to see the solution without trigonometry. Any hints or solution is greatly appreciated. BTW, the angle i found it $10^{\circ} $ (neat right?)","['euclidean-geometry', 'recreational-mathematics', 'geometry']"
4633508,Solve $\frac{x-8}{x-10}+\frac{x-4}{x-6}=\frac{x-5}{x-7}+\frac{x-7}{x-9}$,"Solve $\frac{x-8}{x-10}+\frac{x-4}{x-6}=\frac{x-5}{x-7}+\frac{x-7}{x-9}$ $\Rightarrow \frac{(x-10)+2}{x-10}+\frac{(x-6)+2}{x-6}=\frac{(x-7)+2}{x-7}+\frac{(x-9)+2}{x-9} \ \ \ ...(1)$ $\Rightarrow 1+\frac{2}{x-10}+1+\frac{2}{x-6}=1+\frac{2}{x-7}+1+\frac{2}{x-9}\ \ \ ...(2)$ $\Rightarrow \frac{2}{x-10}+\frac{2}{x-6}=\frac{2}{x-7}+\frac{2}{x-9}\ \ \ ...(3)$ $\Rightarrow \frac{1}{2}(\frac{2}{x-10}+\frac{2}{x-6})=\frac{1}{2}(\frac{2}{x-7}+\frac{2}{x-9})\ \ \ ...(4)$ $\Rightarrow \frac{1}{x-10}+\frac{1}{x-6}=\frac{1}{x-7}+\frac{1}{x-9}\ \ \ ...(5)$ $\Rightarrow \frac{x-6+x-10}{(x-10)(x-6)}=\frac{x-9+x-7}{(x-7)(x-9)}\ \ \ ...(6)$ $\Rightarrow \frac{2x-16}{(x-10)(x-6)}=\frac{2x-16}{(x-7)(x-9)}\ \ \ ...(7)$ $\Rightarrow (x-10)(x-6)=(x-7)(x-9)\ \ \ ...(8)$ $\Rightarrow x^2-16x+60=x^2-16x+63\ \ \ ...(9)$ $\Rightarrow 60=63$ I feel my calculations are all correct, but 60 cannot equal 63, so something went wrong, but I can't see it. Thanks for the help.",['algebra-precalculus']
4633527,Prove that $\lim_{n\to\infty} a_n=0$.,"Let $a_n>0$ such that $$\lim_{n\to\infty} \left(\frac{a_{n+1}}{a_n}\right)^n<1.$$ Prove that $\lim\limits_{n\to\infty}a_n=0$ . Initially, I tried: $$\begin{split}
\lim_{n\to\infty} \left(\frac{a_{n+1}}{a_n}\right)^n<1 &\implies \lim_{n\to\infty} e^{\ln\left(\dfrac{a_{n+1}}{a_n}\right)^n}<e^0\\ &\implies \lim_{n\to\infty} e^{\frac{1}{n}\ln\left(\frac{a_{n+1}}{a_n}\right)}<e^0\\ &\implies \displaystyle\lim_{n\to\infty} \frac{1}{n}\ln\left(\frac{a_{n+1}}{a_n}\right)<0
\end{split}$$ I got slighty desparate for an answer so I assumed that the limit $\displaystyle\lim_{n\to\infty} \left(\frac{a_{n+1}}{a_n}\right)$ exists. Since the limit exists and $a_n>0$ then: $$
\displaystyle\lim_{n\to\infty} \left(\frac{a_{n+1}}{a_n}\right) = \displaystyle\lim_{n\to\infty} (a_n)^\frac{1}{n}
$$ Combining this with the inequality results in: $$
\displaystyle\lim_{n\to\infty}\ln(a_n)<0
$$ If anyone has any suggestions, I'll gladly listen. Thanks for the help!","['limits', 'calculus', 'sequences-and-series']"
4633569,Upper Bounding the Discrete Entropy by the Expectation,"Let $\mathcal P$ be the set of probability mass functions (pmfs) on $\mathbb Z_{>0}$ , i.e. for $p=(p(x))_{x\in\mathbb Z_{>0}}\in\mathcal P$ we have $p\ge 0$ and $\sum_{x=1}^\infty p(x)=1$ .
Let $H(p)=-\sum_{x=1}^\infty p(x)\ln(p(x))$ be the entropy and $E(p)=\sum_{x=1}^\infty p(x)x$ the expectation. Further, let $s(p)=|\{x\in\mathbb Z_{>0}:p(x)>0\}|$ be the size of the support of $p$ . For $s\in\mathbb Z_{>0}$ let $\mathcal P_s=\{p\in\mathcal P:s(p)=s\}$ , and further let $\mathcal P_\infty=\{p\in\mathcal P:s(p)=\infty,E(p)<\infty\}$ . Question : What are the best bounds for $r(s)=\sup_{p\in\mathcal P_s}H(p)/E(p)$ ? Motivation : I want good upper bounds for the entropy in terms of the support size and the expectation. Background :
The question only makes sense if we consider strictly positive random variables, otherwise $r(s)$ would be infinite, which can be seen by taking limits towards the one-point mass on $0$ . For this question we can assume that $p$ is supported on $\{1,\dots,s\}$ , respectively $\mathbb Z_{>0}$ for $s=\infty$ , and non-increasing, since ordering the weights minimizes the expectation while preserving the entropy. For $s<\infty$ we have $r(s)\le\ln(s)$ because $H(p)\le\ln(s)$ is maximal for the uniform distribution and $E(p)\ge 1$ . Of course, with the uniform distribution we also get a lower bound, namely $r(s)\ge 2\ln(s)/(s+1)$ , which is not tight because the entropy is stationary at the uniform distribution while the expectation is not. Also, we know that the supremum is attained due to continuity. Of course, identifying the maximizers would be highly desirable. For $s=\infty$ it is known that $H(p)=E(p)=\infty$ is possible, as discussed here .
As can be seen here , there are also quite a few follow-up questions. Unfortunately, I am not convinced by the given answer, and am still not aware of an answer to the question if $H(p)<\infty$ for all $p\in\mathcal P_\infty$ .
Should this be true, we may of course still have $r(\infty)=\infty$ , and in any case explicit maximizing sequences would be highly desirable. Finally, a similar question regarding a lower bound can be found here . Update :
A limiting argument directly yields that $r(s+1)\ge r(s)$ .
As discussed here , we have $H(p)\le\ln(E(p)+0.5)+1$ given by Theorem 8 in this preprint.
The map $f(x)=(\ln(x+0.5)+1)/x$ is decreasing on $[1,\infty)$ , with $f(x)=1$ for $x\approx 1.858$ .
Since we can assume that $p$ is non-increasing, we have $E(p)\le\frac{1+s}{2}$ . We clearly have $r(1)=0$ and a discussion of $f(p_1)=\frac{H(p_1)}{p_1+2(1-p_1)}$ gives the maximizer $p_1=\frac{1}{2}(\sqrt 5-1)\approx 0.618$ , the expectation $E(p)\approx 1.382$ and $r(2)\approx 0.481$ .
For $s=3$ we fix $\mu\in(1,2]$ and consider $p_1\ge p_2\ge p_3\ge 0$ with $p_1+p_2+p_3=1$ and $E(p)=\mu$ . Set $p_3=x$ and observe that $p_1=2-\mu+x$ , $p_2=\mu-1-2x$ , and that $\max(0,\frac{2}{3}\mu-1)\le x\le\frac{1}{3}(\mu-1)$ . The derivative $\ln(\frac{p_2^2}{p_1p_3})$ of the entropy on this restriction is decreasing with exactly one root $x=\frac{1}{2}\mu-\frac{1}{3}-\frac{1}{6}\sqrt{4-3(2-\mu)^2}$ . Numerical evaluation gives \begin{align*}
p_1&\approx 0.544\\
p_2&\approx 0.296\\
p_3&\approx 0.161\\
E(p)&\approx 1.617\\
r(3)&\approx 0.609.
\end{align*}","['entropy', 'probability-distributions', 'examples-counterexamples', 'information-theory', 'probability']"
4633571,Solving for x in logarithmic equation $\log_4(2x) = \frac{1}{2}x^2 - 1$,"I am trying to solve for $x$ in the equation $\log_4(2x) = \frac{1}{2}x^2 - 1$ . I have tried converting the logarithmic expression to exponential form, but I am not able to isolate $x$ in the resulting equation. This is what I have tried as of now: $$\log_4(2x) + \log_4(4) = \frac{1}{2}x^2$$ $$\log_4(8x) = \frac{1}{2}x^2$$ $$2\log_4(8x) = x^2$$ $$\log_4(64x^2) = x^2$$ $$64x^2 = 4^{x^2}$$ after which I am not too sure on how to find x","['exponentiation', 'algebra-precalculus', 'logarithms']"
4633607,"Number of strings that are made of $3$ 'a', $1$ 'b', $2$ 'c' and $2$ 'd' such that $aaa$ does not appear","Number of strings that are made of $3$ 'a', $1$ 'b', $2$ 'c' and $2$ 'd' such that $aaa$ does not appear I tried to first place the other letters such that $bccdd$ then we will have gaps ${\_b\_ c\_ c\_d \_d\_}$ for that we have $1!\cdot 2!\cdot 2!$ I thought to multiply by ${8 \choose 1}\cdot 1! \cdot {7 \choose 2} \cdot 2! \cdot {5 \choose 2 } \cdot 2!$ because of the total number of letters we have
but that is already wrong as the correct answer should be $1500$ and this is beyond $1500$ I got stuck here how do I continue after placing the letters, how to place the ""a"" i thoguht about needing two cases , one for a single ""a"" and one for ""aa""?
thanks for any tips and help!","['combinatorics-on-words', 'combinatorics', 'discrete-mathematics']"
4633622,Rules for Choosing Bounds and Initial Conditions when Using 2nd Order Runge Kutta Methods,"I have a question regarding 2nd order Runge-Kutta methods, specifically where it regards the bounds of the solution. Let's say I have to solve a 1st order ODE $\frac{dy}{dx}=f(x,y)$ numerically using Heun's method: $$y_{i+1}=y_i+\frac{1}{2}h(k_1+k_2)$$ $$k_1=f(x_i,y_i), k_2=f(x_i+h,y_i+k_1h)$$ The ODE can be solved within a range let's say $a<x<b$ . My question is whether the initial condition needs to coincide with $x=a$ or whether it only needs between the bounds. Here is an example I was messing around with when I noticed the potential issue. Solve the following ODE using Heun's method within the bounds $a<x<b$ with a step size of $h=0.2$ . $$xy^\prime+y=x\sin(x), y(\pi)=2$$ If the range is $1<x<3$ where $x_0=\pi$ is outside of the range the numerical error is very high giving the following plot after calculating in MATLAB. I noticed the shape of the approximation was similar to the exact solution. If I change the range to $3<x<5$ so that $x_0=\pi$ falls within the range, the error is significantly smaller. Is this because $a<x_0<b$ or is it because $\pi\approx3$ and I should have started at $x_0=3$ ?","['recursive-algorithms', 'numerical-methods', 'runge-kutta-methods', 'ordinary-differential-equations']"
4633626,Let $(M_t)$ be a continuous square-integrable martingale with independent increments. Is $t \mapsto \mathbb E[M_t^2]$ continuous?,"I'm reading a remark at page 4 of these lecture notes. Let $\left(\mathcal{F}_t, t \in \mathbb{R}_{+}\right)$ be a filtration and $M=\left(M_t, t \in \mathbb{R}_{+}\right)$ be a continuous square-integrable martingale with respect to $\left(\mathcal{F}_t, t \in \mathbb{R}_{+}\right)$ . Reminder. The quadratic variation of $M$ is the unique process $\left(\langle M\rangle_t, t \in \mathbb{R}_{+}\right)$ which is increasing, continuous and adapted to $\left(\mathcal{F}_t, t \in \mathbb{R}_{+}\right)$ , such that $\langle M\rangle_0=0$ a.s. and $\left(M_t^2-\langle M\rangle_t, t \in \mathbb{R}_{+}\right)$ is a martingale with respect to $\left(\mathcal{F}_t, t \in \mathbb{R}_{+}\right)$ . Lemma 1.1. For all $t>s \geq 0$ , $$
\mathbb{E}\left(\left(M_t-M_s\right)^2 | \mathcal{F}_s\right)=\mathbb{E}\left(\langle M\rangle_t-\langle M\rangle_s | \mathcal{F}_s\right).
$$ Remark. In general, $\langle M\rangle_t$ is not deterministic, but when $M$ has independent increments, then $\langle M\rangle_t=\mathbb{E}\left(M_t^2\right)-\mathbb{E}\left(M_0^2\right)$ (and is therefore deterministic). My understanding In other threads ( 1 , 2 , 3 , 4 , 5 ), the remark holds if in addition $M$ is a Gaussian process, i.e., if $M$ is Gaussian then $t \mapsto \mathbb{E}\left(M_t^2\right)$ is continuous. Could you confirm that the remark is not necessarily correct?","['stochastic-processes', 'probability-theory', 'martingales']"
4633659,Estimation of the eigenvalues of a matrix,"Given the following matrix \begin{pmatrix}0 & 1 & 0 & 0\\
-k & -\lambda_1 & -k & 0\\
0 & 0 & 0 & 1\\
-k & 0 & -k & -\lambda_2\end{pmatrix} where $k,\lambda_1$ and $\lambda_2$ are strictly positive, is there a way to estimate the eigenvalues, or at least say if they are positive or negative, complex or real? One eigenvalue is  zero, but to find the others you need to solve a parametric cubic equation, which is not easy. Note that this matrix is the representation of a mechanical system, so a physical context may come in handy.","['matrices', 'estimation', 'eigenvalues-eigenvectors']"
4633732,$\frac{\partial \text{score}(x; \lambda)}{\partial \lambda}$.,Apparently I forgot how to do take a derivative.. This is the score of some Poisson distribution random variables: $\text{score}(x; \lambda) = \frac{x}{\lambda} - 1$ . Now I want to take the derivative according to the parameter $\lambda$ . $\frac{\partial score(x; \lambda)}{\partial \lambda}$ .,"['fisher-information', 'derivatives']"
4633756,"Let $ f(x) = x^{ \alpha-1}(1+x)^{- \alpha-\beta}\frac{Î“(Î±+Î²)}{Î“(Î±)Î“(Î²)},$ where $Î“(x) = (x âˆ’ 1)Î“(x-1)$ and $Î±,Î² > 1$.","Let $ f(x) =  x^{ \alpha-1}(1+x)^{- \alpha-\beta}\frac{Î“(Î±+Î²)}{Î“(Î±)Î“(Î²)},$ where $Î“(x) = (x âˆ’ 1)Î“(x-1)$ and $Î±,Î² > 1$ . Show that $E(X) = \frac{Î±}{Î²-1}.$ I've tried in a couple of ways, but couldn't see the direction. I've used moment generating function to obtain $E(X)$ , but couldn't see a way to simplify. Also, I've tried converting the part $f(x) = x^{ \alpha-1}(1+x)^{- \alpha-\beta} $ into some form similar to $ B(Î±,Î²)=\int_0^1 x^{Î±âˆ’1}(1âˆ’x)^{Î²âˆ’1}dx$ so that I could create some gamma functions, which eventually could lead to do a cross out with the given gamma functions $\frac{Î“(Î±+Î²)}{Î“(Î±)Î“(Î²)}.$ In class, we did not really cover the basics of the gamma function, but mostly on expected values. Can someone suggest what topic or section I should read in the textbook for some ideas? E.g. proof of $E(X)$ in terms of a gamma functions, etc. Thank you.","['statistical-inference', 'expected-value', 'gamma-function', 'functions', 'beta-function']"
4633862,How to integrate $\frac{\tan x}{x+\sin x}$?,"How to calculate the integral $$\int\frac{\tan x}{x+\sin x}\mathrm dx\;\;?$$ I use a website called integral of the day, all the previous ones have answers, but today I cannot find an answer for it and no online calculators can do it either. It comes from this website: https://www.sammserver.com/iotd/ , but as it is the next day it has changed now.","['integration', 'calculus']"
4633883,Median Voter Models in Two Dimensions (computing area of a bounded sector),"I've been trying to work out a way of computing area for a two-dimensional median voter model I've been working on. A, B, and C are political parties that can choose where they want to be on the political spectrum (left/right, authoritarian/libertarian). Each party wants to maximise the area of the graph that they gain within the bounds of the political spectrum in order to maximise the number of voters they get (assuming political preference is uniformly distributed across the spectrum): Points can be moved around and cannot completely overlap. I want to know if there is an expression I can use to find the area of the zone closest to A / closest to B / closest to C thereby finding who wins purely based off the coordinates of each party and the size of the political spectrum box. So far all I have been able to do is graph it out using perpendicular bisectors, but nothing further. Any help would be appreciated :D","['area', 'voting-theory', 'geometry', 'game-theory', 'recreational-mathematics']"
4633899,Balls in Bins with Bounded Discrepancy,"How many ways are there to distribute N balls into M ordered bins, with the constraint that the number of balls in two adjacent bins differs by at most one? Letting $n_i$ be the number of balls in the $i$ th bin, this discrepancy constraint requires that $|n_i - n_j| \leq 1$ for all $i,j \in [M]$ such that $|i - j| \leq 1$ . Without the discrepancy constraint, I know the answer can be computed easily using stars and bars. However, I'm stuck on how to solve it with the discrepancy constraint.","['combinatorics', 'balls-in-bins', 'discrete-mathematics']"
4633916,"Constructing a counter-example of an increasing function such that $|\alpha(x)/x|, |\alpha(x)/\alpha(x/2)| \to \infty$ as $x\to 0^+$","Consider a continuous function $\alpha:[0,\infty) \to [0,\infty)$ satisfying the following properties: $\alpha(0)=0;$ $\alpha$ is a strictly increasing function; $\alpha$ is smooth in $(0,\infty);$ and $|\alpha(x)/x| \to \infty$ as $x\to 0^+$ . Under these conditions, is it true that there exists $\delta>0$ and $Îœ >0$ such that $$ \left|\frac{\alpha(x)}{\alpha(x/2)}\right| \leq M,\ \forall\ x\in(0,\delta)\ ?$$ I do not know if the above question is either true or false. Could someone please help me to solve this question? Searching for a counter-example: Since I could not prove the above question, I started looking for a counter-example. However, the examples that I was able to find that satisfy 1-4, such as $\alpha(x) = x^\beta$ , for $0<\beta<1$ ; $\alpha(x) = -x \log (x)$ in a neighborhood of $0;$ and $\alpha(x) = (1-x^x)/x$ in a neighborhood of $0.$ However, all these functions fulfil the desired property.","['calculus', 'derivatives', 'examples-counterexamples', 'real-analysis']"
4633943,General definition of POVM,"Wikipedia gives the following definition of positive operator-valued measure (POVM): A POVM on a measurable space $(X,M)$ is a function $F$ defined on $M$ whose values are bounded non-negative self-adjoint operators on a Hilbert space $\mathcal{H}$ such that $F(X) = 1_\mathcal{H}$ and for every $\psi \in \mathcal{H}$ the map $E \mapsto \langle F(E) \psi , \psi \rangle$ is a non-negative countably additive measure on $M$ . When the set of outcomes $X$ is finite and/or the Hilbert space has finite dimension, a simpler definition can be easily found in standard textbooks, but in the general case none of the books I've checked (on quantum mechanics, functional analysis or $C^*$ -algebras) contains a definition. Do you have a reference, other than Wikipedia, for an equivalent definition of POVM?","['quantum-mechanics', 'measure-theory', 'functional-analysis', 'reference-request']"
4633944,number of distinct commutators and the cardinality of the center of a group,"Suppose $G$ is a group, $Z(G)=\{g \mid\ \forall a \in G,\ gag^{-1}a^{-1}=e\}$ is the center of $G$ , and $K(G)=\{gag^{-1}a^{-1}\mid a,g\in G\}$ is the subset of all commutators. I am wondering whether there is any way to find $|K(G)|$ based on $|G|$ and $|Z(G)|$ . I have tried to use group actions, but, since $K(G)$ ranges over two variables ( $g$ and $a$ ), I could not succeed. It seems it needs a generalization of group actions so that two elements of a group act on an element of a set simultaneously. If we consider $K(a)= \{gag^{-1}a^{-1}\mid g\in G\}$ then $|K(a)|=|\bar{a}|=\frac{|G|}{|Z_G(a)|}$ , where $Z_G(a)=\{g \mid gag^{-1}a^{-1}=e\}$ is the centralizer of $a$ and $\bar{a}=\{gag^{-1}\mid g\in G\}$ is the conjugacy class of $a$ . How can we find a formula among $|K(G)|$ , $|G|$ , and $|Z(G)|$ like $|K(a)|=\frac{|G|}{|Z_G(a)|}$ ? Edited I changed the notations from $C(G)$ to $Z(G)$ , $C_G(a)$ to $Z_G(a)$ , and from $Z$ to $K(G)$ so that they become more standard.","['group-actions', 'group-theory', 'abstract-algebra']"
4633975,Solve $\frac{x+25}{x-5}=\frac{2x+75}{2x-15}$,"Solve $\dfrac{x+25}{x-5}=\dfrac{2x+75}{2x-15}$ $\Rightarrow \dfrac{(x-5)+30}{x-5}=\dfrac{(2x-15)+90}{2x-15} 
 \ \ \ ...(1)$ $\Rightarrow 1+\dfrac{30}{x-5}=1+\dfrac{90}{2x-15}\ \ \ ...(2)$ $\Rightarrow \dfrac{30}{x-5}=\dfrac{90}{2x-15}\ \ \ ...(3)$ $\Rightarrow 3(2x-15)=9(x-5)\ \ \ ...(4)$ $\Rightarrow 6x-45=9x-45\ \ \ ...(5)$ $\Rightarrow 6x=9x\ \ \ ...(6)$ $\Rightarrow 2x=3x\ \ \ ...(7)$ So the answer becomes $3x-2x=0 \Rightarrow x=0$ which works in the original equation. My question is if I divide $2x=3x$ by $x$ , I get $2=3$ which is not valid. Why can't I divide by $x$ ? Is it because for $x=0$ , $\frac{x}{x}$ is not defined and hence it gives $2=3$ ? And if that is the case, then why is it in $(1)$ , $\frac{x-5}{x-5}=1$ is valid? What if $x=5$ , then wouldn't this fraction become undefined as well? I'm confused why in this case the variable $x$ can be divided in the beginning but at $(7)$ it doesn't work. Thanks for helping.",['algebra-precalculus']
4633981,Qualifying exam problem (complex analysis),"I found this problem in a Ph.D. Qualifing Exam: Let $f$ be an entire function. Suppose that $f(z)=f(z+1)$ and $|f(z)|\leq e^{|z|}$ for all $z\in\mathbb{C}$ . Prove that $f$ is a constant function. I guess that we need to use the Louville's Theorem in order to prove that statement, so it is missing to show that $f$ is a bounded function. However, I am not sure how to apply the hypothesis to do that. Can you give me some advice to complete that proof?",['complex-analysis']
4633992,Show that $a^2+b^2+c^2=x^2+y^2+z^2$,"Let $a,b,c,x,y,z$ be real numbers such that $$ a^2+x^2=b^2+y^2=c^2+z^2=(a+b)^2+(x+y)^2=(b+c)^2+(y+z)^2=(c+a)^2+(z+x)^2 $$ Show that $a^2+b^2+c^2=x^2+y^2+z^2$ . Progress: Note that $$0=\sum_{\text{cyc}} (a + b)^2 + (x + y)^2 - \sum_{\text{cyc}} a^2 + x^2 = (a + b + c)^2 + (x + y + z)^2\implies a+b+c=x+y+z=0.$$ So we need to show $$ab+bc+ca=xy+yz+zx.$$ Also, taking $c=-(a+b),z=-(x+y)$ , we get $$a^2+x^2=b^2+y^2=(a+b)^2+(x+y)^2.$$ So we get $$y^2+b^2+2ab+2yx=0\implies \sum_{cyc}y^2+b^2+2ab+2yx=0\implies (a+b)^2+(b+c)^2+(c+a)^2+(x+y)^2+(y+x)^2+(x+z)^2=0\implies a=b=c=x=y=z=0. $$ I am not sure of the last part. So can someone check? Can someone also give an alternate solution to this problem?","['contest-math', 'algebra-precalculus', 'solution-verification', 'alternative-proof']"
4634031,Satisfying the trigonometric equation with the roots found on solving.,"This is the question, When I am solving this I am getting the equation $$(2x)(3x+2)(x-3)=0$$ When I am putting $x=-\frac{2}{3}$ and $x=3$ the equation is satisfied and when I am putting $x=0$ the equation goes like $$\frac{\pi}{4} + \frac{\pi}{4} = \frac{\pi}{2}$$ Clearly, it satisfies the equation but can I do it this way, as the range of tan inverse x is ( $\frac{\pi}{2}, \frac{\pi}{2}$ ). Moreover, can you check if the $x=-2/3$ satisfies the equation? This is how I am solving it,","['trigonometry', 'inverse-function']"
4634040,What is the result or equivalent order of this infinite sum?,"As described by the title, can we obtain an analytical result for the following infinite sum or provide an equivalent order as $p \rightarrow 1$ ? $$
\sum_{j=1}^{\infty} \left(\frac{p^j}{1-p^j}\right)^{i+1},i\ \text{is non-negative integer}.
$$ Now, I only know that $$\sum_{j=1}^{\infty} \frac{p^j}{1-p^j}\approx \frac{1}{1-p}\ln\left(\frac{1}{1-p}\right).$$ I often have to compute the sum of infinite series in my research. I really appreciate if one can provide a textbook or useful handbook to reference.","['reference-request', 'sequences-and-series']"
4634041,"(Dis)Proving $\lim_{x\to\infty}\frac1{x^s}\int_0^xt^{s-1}f(\sin t)dt=\frac1{2\pi s}\int_0^{2\pi}f(\sin t)dt$, for $s>0$ and $f$ defined on $[-1,1]$","Prove (or disprove) the following conjecture: Given $s>0$ , function $f(x)$ is defined for all $x\in[-1;1]$ , then: $$\lim_{x\to\infty}\frac{1}{x^s}\int_0^x t^{s-1}f(\sin(t))\,dt=\frac{1}{2{\pi}s}\int_0^{2\pi} f(\sin(t))\,dt$$ I can only prove the conjecture for $s=2$ (using $\int_0^{(2n+1)\pi}tf(\sin(t))\,dt=\frac{(2n+1)\pi}{2}\int_0^{(2n+1)\pi}f(\sin(t))\,dt$ ( $*$ )) and $s=1$ (obvious). However, the substitution trick $t=(2n+1)\pi-u$ in ( $*$ ) doesn't work for $s$ is even or $s$ is non-integer. It seems to me that the conjecture is true but all my tools for evaluating limit problems don't work here. ( Update ) The answer to the problem above was done in What is $\lim_{t\rightarrow \infty }\int_{a}^{b}f(x,\sin(tx))dx$ï¼Ÿ (I moved my generalization to another post)","['integration', 'limits', 'calculus']"
4634046,Closed-form solutions to $x''+\frac{k}{m}\ x+\mu\ g\ \text{sgn}(x')=0$,"Closed-form solutions to $x''+\frac{k}{m}\ x+\mu\ g\ \text{sgn}(x')=0$ Introduction______________________ I am looking for simple mechanics models that could have closed-form solutions that achieves finite extinction times where it becomes zero for their own system dynamics and stays there forever after. Looking for simple systems I found in Wikipedia that a mass sliding in a horizontal plane under Coulomb friction is modeled by the differential equation of the Newton's 2nd law as: $$  m\ x'' = -F - \mu\ m\ g\ \text{sgn}(x')$$ where the mass $m$ , the earth's gravity acceleration $g$ , and the kinetic coefficient of friction $\mu$ are positive constants. I found here: brick sliding in an horizontal plane after an initial push that the mass sliding after an initial push indeed slides until it stops moving, being their behavior described through a piecewise polynomial closed-form solution, representing the scenario where the force $F = 0$ . As example is pretty obvious, which is good since can be easily found the procedure is right since results matches the classic answers found through energy analysis of the system. Now, to move into the next step of difficulty, I want to model the exact example of the Wikipedia page, the case where the mass is attached to a spring, where the only additional force present besides the Coulomb damping. So modeling the spring as the classical force $F = k x$ with $k$ the string constant ( $k>0$ ), the previous equation becomes: $$  m\ x'' = -k\ x - \mu\ m\ g\ \text{sgn}(x')$$ The question I have tried unsuccessful to solve the equation as I did for the case $F=0$ , and so far I don't find any papers with closed-form solutions to the equation: $$  x'' = -\frac{k}{m}\ x - \mu\ g\ \text{sgn}(x')$$ If I made every constant equal to one, then Wolfram-Alpha shows the following: As expected for a non-linear equation there are multiple solutions. I am specially interested in the solutions were the mass stop moving (which is impossible to represent accurately through a linear ODE or non-piecewise power series, as explained here - otherwise it will violate the Identity Theorem ), but differently from the mentioned example where I used a self-named endiness constraint : there exists a time $T>0$ such as $x(t) = 0,\ \forall t>T$ , but for what I have found on the papers, in this mass-spring system also exists the possibility that the mass stops moving in a different position than the original rest position, so it could have a final position $x_f$ constant, such as $x(t) = x_f,\ \forall t>T$ . I hope you could find the closed-form solution that stops moving , showing the equations that determines the finite extinction time $T$ and the final position $x_f$ , showing how you found them. Added later____________ Thinking in how the system could stop moving in a position different from equilibrium, I am expecting to find a solution of the form: $$f(t) = x(t)\theta(T-t)+x(T)\theta(t-T)$$ for some function $x(t)$ such as $f'(t)$ is a function finite duration (it achieve by self-dynamics the value zero ""continuously"" and stay there forever after), and with $\theta(t)$ is the Heaviside step function . Here I got messed with the integration constant by going backwards from $f'(t)$ to $f(t)$ since I don't know how you make appear the term $x(T)\theta(t-T)$ (this is the problem with these special functions that are hidden distributions - if you could explain this also, it would be great). This was finally answered here , but I still don't figure out how the solution will become a constant: for example think in the following numerical aproximation $x''+x+\text{sgn}(x')=0$ with arbitrary initial conditions $x(0)=-\frac{3\pi}{2}$ and $x'(0)=\frac{\pi}{2}$ shown numerically here : I don't have any clue of how the solution will satisfy the differential equation after it stops moving, neither how it will match the ""discrete-fix-value but variable term"" made by $\text{sgn}(x')$ . If you could explain it theoretically to have insight I will appreciate it, since I have intuition that the solution will be within the theory of distributions, at least in part (thinking in thing like $xf(x)=xg(x) \Rightarrow f(x)=g(x)+c\delta(x)$ ). Also, the solution should have some decaying term as is shown numerically for $x''+x+\text{sgn}(x')=0,\ \ x(0)=3\pi/2,\ \ x'(0)=10$ in here : This last point is important: as example, if I use the ansatz $x(t)=c_1\sin(t)+c_2\cos(t)-\text{sgn}\left(c_1\cos(t)-c_2\sin(t)\right)$ for the equation $x''+x+\text{sgn}(x')=0$ and I ignore every distribution-alike term like $\delta(f(t))\equiv 0$ I can see in Wolfram-Alpha that for every value of $t$ the differential equation is fulfilled, but as you could see from the solution the decaying term is missing, so it is not the solution for the problem - here maybe the distribution theory have something to say. What I did: $$\begin{array}{r l}
x''+x+\text{sgn}(x') = 0 & \Biggr| \frac{\partial}{\partial t} \\
\Rightarrow x'''+x'+2 x''\delta(x') =0 & \Biggr| \cdot x'\\
\Rightarrow x'x'''+(x')^2+\require{cancel}\cancel{2x''\cdot\underbrace{x'\delta(x')}_{\text{since }x\delta(x)\ =\ 0}} = 0 & \Biggr| y=x'\\
\Rightarrow y(y''+y)=0 & \Biggr| y\neq 0\ \forall t\\
\Rightarrow y(t) = c_3\sin(t)+c_4\cos(t) & \Biggr| \int \, dt \\
\Rightarrow x(t) = c_1\sin(t)+c_2\cos(t) + c &
\end{array}$$ but for inspection one could notice that the integration constant $c$ cannot really be constant since when using the solution in the equation $x''+x+\text{sgn}(x')=0$ I need something to ""kill"" the term $\text{sgn}(\cdot)$ , so the ""constant"" must not be constant, which it is also weird. And also the decaying term is still missing. My guess is that the decaying term should be a piece-wise polynomial of the form $(T-t)^q\theta(T-t)$ such as every derivative which rise delta function got canceled by $x^n\delta(x) = 0$ , similar to what happened in brick sliding in an horizontal plane after an initial push , but here since the velocity profile becomes zero, but the solution becomes a constant that can be different from zero, it is not easy to figure out how the answer will become a constant value - I hope that maybe this could help someone else to make a/the closed-form solution. I have checked in online Octave if the decaying behavior were just a mistake of Wolfram-Alpha (since the answer founded looks it solves at least the procedure I use to find it), but it looks is right, both could happen: (i) a decaying behavior, and (ii) and ending position different from equilibrium - both behaviors unattainable by a simple harmonic oscillator . f = @(t,y) [y(2);-sign(y(2))-y(1)]; 
t0 = 0; y0 = [10;-4];
opt=odeset('RelTol',1e-3,'AbsTol',1e-4);
[ts,ys] = ode45(f,[t0,20],y0,opt);
plot(ts,ys(:,1),'b'); Plot of the numerical solution to $x''+x+\text{sgn}(x')=0,\,x(0)=10,\,x'(0)=-4$ : 2nd Added later____________ Thinking in the following form: $$\begin{array}{r c l}
\text{let }y & = & x-\text{sgn}(x') \\
\Rightarrow y' & = & x'-\delta(x')x'' \\
\Rightarrow y'' & = & x'' -\delta'(x')(x'')^2-\delta(x')x'''
\end{array}$$ Now, since $$\delta(f(x)) = \sum\limits_{n}|f'(x_n)|^{-1}\delta(x-x_n)\text{ with }f(x_n)=0,\,f'(x_n)\neq 0 $$ I will work from now on assuming that every term $\delta(\cdot)\equiv 0$ since they will affect only a zero-measure points, this hoping to find the solution outside this problematic points and later figure out how to solve in this places: So with this, the previous analysis becomes: $$\begin{array}{r c l}
\text{let }y & = & x-\text{sgn}(x') \\
\Rightarrow y' & = & x' \\
\Rightarrow y'' & = & x''
\end{array}$$ So now I have $y''+y+\text{sgn}(y') = 0 \iff x''+ x -\text{sgn}(x')+\text{sgn}(x')=0 \iff x''+x = 0$ But now I don't know how to solve $x''+x=0$ without a only-trigonometric solution. I tried unsuccessfully something of the form: $$ x = (T-t)\theta(T-t)(c_1\sin(t)+c_2\cos(t)) $$ which at least cancel every polynomial term letting only trigonometric functions to be cancelled. If I plot the following solution in online Octave : f = @(t,y) [y(2);-sign(y(2))-y(1)]; 
t0 = 0; y0 = [30;-4];
opt=odeset('RelTol',1e-3,'AbsTol',1e-4);
[ts,ys] = ode45(f,[t0,50],y0,opt);
plot(ts,ys(:,1),'b'); Is not hard to see that the decay is indeed linear, so I don't believe I am too lost about the solution. Hope you could share some ideas. Finally I have tested the linear decay and it isn't purely lineal (more clear at the end), as can be seen contrasted in the following graph: 3rd attempt________________ Following the change of variables that @eyeballfrog have used in their answer, lets think in the differential equation as: $$z(\tau)''+\frac{k}{m}\ z(\tau)+\mu\ g\ \text{sgn}(z'(\tau))=0$$ Now let $\tau = w t$ with $w = \sqrt{\frac{k}{m}}$ , so $\frac{k}{m} = w^2$ and $z(\tau) = \frac{\mu g}{w^2}x(w\ \tau)$ .
Now I would like to know which happens in $ z(\tau)''+ w^2 z(\tau)+\mu\ g\ \text{sgn}(z'(\tau))=0$ . Then, I have that: $$\begin{array}{r c l}
z'(\tau) = \frac{\mu g}{w^2} \frac{\partial}{\partial \tau} x(w\tau) = \frac{\mu g}{w^2} x'(w\tau) w & = & \frac{\mu g}{w}x'(w\tau) \\
\Rightarrow z''(\tau) = \frac{\partial}{\partial \tau}\left( \frac{\mu g}{w}x'(w\tau)\right) = \frac{\mu g}{w} x''(w \tau) w & = & \mu g\ x''(w\tau)\\
\end{array}$$ Replacing I will have now that: $$\begin{array}{c}
\mu g\ x''(w\tau)+ w^2 \frac{\mu g}{w^2}x(w\ \tau)+\mu\ g\ \text{sgn}(\frac{\mu g}{w}x'(w\tau))=0 \\
\iff \mu g \left(x''(w\tau) +x(w\tau)+\text{sgn}(\frac{\mu g}{w}x'(w\tau))\right) = 0 \\
\end{array}$$ Now using that $\mu g \neq 0$ and also that $\frac{\mu g}{w}>0$ such its true that $$\text{sgn}(\frac{\mu g}{w}x'(w\tau))  = \frac{\frac{\mu g}{w}x'(w\tau)}{|\frac{\mu g}{w}x'(w\tau)|} = \frac{\frac{\mu g}{w}x'(w\tau)}{|\frac{\mu g}{w} | \cdot|x'(w\tau)|} = \text{sgn}(x'(w\tau))$$ then I will have I only need to find the solutions to: $$x''(t) +x(t)+\text{sgn}(x'(t)) = 0$$ Which is why I am working in this equation now: the solution will work only if $\mu>0$ and $g>0$ and $k>0$ and $m>0$ , keep this in mind. Now, I have found to make appear a decaying behavior in the solution but I don't think is a $100\%$ rigorous: $$\begin{array}{r l}
x''+x+\text{sgn}(x') = 0 & \Biggr| \frac{\partial}{\partial t} \\
\Rightarrow x'''+x'+2 x''\delta(x') =0 & \Biggr| \cdot \delta(x')\\
\Rightarrow \delta (x')\left(x'''+ x' \right) + 2x''(\delta(x'))^2= 0 & \Biggr| \text{assuming arbitrarily that }(\delta(x))^2\equiv\delta(x)\\
\Rightarrow ?\quad \delta (x')\left(x'''+ x' + 2x''\right) = 0 & \Biggr| y = x' \\
\Rightarrow y''+y+2y'=0 & \\
\Rightarrow y(t) = c_3\exp(-t)+c_4\ t \exp(-t) & \Biggr| \int \, dt \\
\Rightarrow x(t) = c_1\exp(-t)+c_2\ t \exp(-t) + c &
\end{array}$$ Which is interesting since indeed got a polynomial term, also jointly with an exponential, but in a way that the derivative keep the same structure except for a constant: this thinking in that the speed profile needs to end at zero at time $T$ , but the position needs to become a constant, kind of fit with this result. Since the decay looks quite linear, I tried for the exponential term something of form $\frac{e^{T}}{e^{T}-1}\left(1-e^{T-t}\right)$ in order it affects little at the beginning, but that ends at zero so it keeps the same decay as the polynomial term, again of the form $(T-t)$ so it have a zero at time $T$ , and since the trigonometric function is already tested as solving the solution, I incorporated for now arbitrarily as a linear addition that will be predominant at the end of the movement (should be the same function as the one attached to the decay, at least in principle, in order to keep the number of integration constants that could be determined by the initial conditions): Which looks quite promising for being just a not-fitted mix of the solutions I have being found (not rigorously), so I think maybe the answer have the form: $$x(t) = M\theta(T-t)(t-T)(1-e^{-(T-t)})\left(a\sin(t)+b\cos(t)\right)+N\left(a\sin(t)+b\cos(t)\right)+C$$ or like $$x(t) = M\theta(T-t)|(t-T)(1-e^{-(T-t)})|\left(a\sin(t)+b\cos(t)\right)+N\left(a\sin(t)+b\cos(t)\right)+C$$ with some constants $M,\ N,\ T,\ a,\ b,\ C$ to be determined by initial conditions, but I have no clue if the $\text{sgn}(x')$ should be added, while looking good in theory, it will introduce unobserved ""jumps"" on the solution. Best numerical attempt so far_________ I uploaded because is quite good , even when I just search by hand some values for the constants, without making any ""goodness-of-fit"" approach for estimating them: f = @(t,y) [y(2);-sign(y(2))-y(1)]; 
t0 = 0; y0 = [30;-4];
opt=odeset('RelTol',1e-3,'AbsTol',1e-4);
[ts,ys] = ode45(f,[t0,50],y0,opt); 
x = (30+3/2)*((50-ts)/50)*(exp(50)/(exp(50)-1)).*(1-exp(ts-50)).*cos(ts)-3/2*cos(ts)+1/2*sin(ts);
plot(ts,ys(:,1),'b',ts,x,'r'); Added later: The solution is not a pure/piecewise trigonometric function As some of the answers I have got solve the equation by cases, solutions displayed, looking ""reasonable"" in the math shown, they are made by pure trigonometric functions, or by piecewise constant amplitude decays by half-cycle, which from the numerical solutions could be seen they aren't right , since the plot shows that every lobe is asymmetric proving that the solution is under a gradual decay, as make sense from the physical point of view of the problem, since friction is always present and not piecewise during the movement of the object through time. As a fast example in online Octave , here the following plot counting approximately the grid squares between zeros and the point of min/max: f = @(t,y) [y(2);-sign(y(2))-y(1)]; t0 = 0; y0 = [10;-4]; opt=odeset('RelTol',1e-3,'AbsTol',1e-4); [ts,ys] = ode45(f,[t0,20],y0,opt); plot(ts,ys(:,1),'b'), set(gca,'xtick',[0:0.5:20]), set(gca,'ytick',[-10:0.5:10]), grid on;","['ordinary-differential-equations', 'singular-solution', 'physics', 'finite-duration', 'dynamical-systems']"
4634097,Prove that a semigroup satisfying $a^pb^q=ba$ is commutative,"Let $(S, \cdot)$ be a semigroup. There are natural numbers $p,q \geq 2$ such that $a^pb^q=ba$ for all $a,b \in S$ . Prove that $S$ is commutative. I wrote $$\begin{align}
a^{p+1}b^{q+1} &=b^{(q+1)p}a^{(p+1)q} \\
 &=b^{p}\cdot(b^q)^p \cdot (a^p)^q\cdot a^q \\
 &=b^p\cdot a^p \cdot b^q \cdot a^q \\
 &= b^p\cdot b \cdot a \cdot a^q \\
 &=b^{p+1}a^{q+1}.
\end{align}$$ From the given identity I also got $a^{p+1}b^{q+1}=abab$ . Using $a^{p+1}b^{q+1}=b^{p+1}a^{q+1}$ I then got $abab=baba$ . Making $a=b$ in the statement gives $a^{p+q}=a^2$ . I don't know what to do from there.","['binary-operations', 'group-theory', 'abstract-algebra', 'semigroups']"
4634120,Real number known not to be a period,"I am working a bit with problems in non-archimedean settings inspired by the famous periods conjecture by Kontsevich-Zagier. I was preparing a talk and wanted to give of background about the initial motivation, namely periods. A period in my definition is a complex number whose real and imaginary parts are both values of an integral over a subset of $\mathbb{R}^n$ defined by inequalities of polynomials over $\mathbb{Q}$ ,  where the integrand can be an arbitrary rational function with rational coefficients (or equivalently actually just 1). I know that there are a lot of open problems around periods, besides the famous conjecture itself it is not known whether e.g. $e$ is a period or not, whether $\frac{1}{\pi}$ is a period or not etc. What I could not find is an example of a real number that is known not to be a period and might it just be by some obscure construction as in the case of say normal numbers. I usually rather work with $p$ -adic integrals, so I also don't know how such an construction would look like, so my question is summarized as: Do we know a specific computable real number, that is provably not a period and if so, how do we construct it?","['number-theory', 'transcendence-theory', 'real-algebraic-geometry', 'transcendental-numbers']"
4634130,Proving the equivalence of two inequalities,"Let $f:\mathbb{R} \to \mathbb{R}$ be a function that admits primitives on $\mathbb{R}$ and $F:\mathbb{R} \to \mathbb{R}$ one primitive (i.e. $F'(x)=f(x), \forall x \in \mathbb{R}$ ). Prove that the following two inequalities are equivalent: \begin{align}
a)&& (f(x)-f(y))(x-y) &\ge (x-y)^2, && \forall x, y \in \mathbb{R}
\\b)&& F(x)-F(y) &\ge \frac{1}{2}(x-y)^2+f(y)(x-y), &&\forall x, y \in \mathbb{R}
\end{align} My approach was to prove $b) \implies a)$ . It is obvious, because we if $b)$ is true for all pairs $(x, y)$ then it is true for $(y, x)$ by adding the two relations we obtain "" $a)$ "". But I have not found a way to prove the converse statement. My first thought was to use the mean value theorem to get $F$ into the equation, but the mean value theorem implies the existence of a point, which would not satisfy the $\forall$ quantifier.","['analysis', 'real-analysis', 'calculus', 'indefinite-integrals', 'derivatives']"
4634167,How $\int_1^{\infty} P[Y>t] dt \leq \frac 1 {\alpha -1}$ in this proof of martingale maximal inequality?,"I have just encountered this question Suppose $({X_n}, n \in \mathbb N)$ is a martingale. Let $n \ge 1$ and $\alpha > 1$ such that $E\left[|X_n|^\alpha\right]<\infty$ . Then $$
E\left[\max_{0\leq k \leq n}|X_k| \right]\leq \frac{\alpha}{\alpha-1} E[|X_n|^{\alpha}]^{\frac{1}{\alpha}}.
$$ Hint: $$
E\left[\max_{0\leq k \leq n}|X_k| \right]=\int _{0}^{\infty} P \left [\max_{0\leq k \leq n}|X_k|>t \right ] dt.
$$ Now use the maximal inequality on the submartingale $|X_n|^{\alpha}$ (Let ${X_n}$ be a submartingale for which $X_n\geq 0$ for all n. Then for any positive $\lambda>0$ we have $\lambda P[\max_{0\leq k\leq n} X_k>\lambda] \leq E[X_n]$ ). and its answer Hints: apply submartingale inequality to $\{Y_j:1\leq j \leq n\}$ where $Y_j=\frac {|X_j|^{\alpha}} {E(|X_n|^{\alpha})^{\frac 1 {\alpha}}}$ . [By Jensen's inequality this is indeed a submartingale]. Note that $$
\int_0^{\infty} P [Y>t] dt \leq 1 + \int_1^{\infty} P[Y>t] dt \leq 1+ \frac 1 {\alpha -1}=\frac {\alpha} {\alpha -1}.
$$ My understanding I assume $Y =\max_{0\leq k \leq n}|Y_k|$ . By maximal inequality for the sub-martingale $(Y_n, n \in \mathbb N)$ , $$
\int_1^{\infty} P[Y>t] dt \le \int_1^{\infty} \frac{E[Y_n]}{t} dt = E(|X_n|^{\alpha})^{1-\frac 1 {\alpha}}\int_1^{\infty} \frac{1}{t} dt \color{red}{= +\infty}.
$$ Could you please elaborate on how $\int_1^{\infty} P[Y>t] dt \leq \frac 1 {\alpha -1}$ in the answer?","['proof-explanation', 'stochastic-processes', 'martingales', 'inequality', 'probability-theory']"
4634181,Find n given that $\lim\limits_{x\rightarrow0} \frac{1-\sqrt{\cos2x}.\sqrt[3]{\cos3x}.\sqrt[4]{\cos4x}...\sqrt[n]{\cos{nx}}}{x^2} = 10$,"I'm trying to solve this rather interesting problem. We have been given that $\lim\limits_{x\rightarrow0}  \frac{1-\sqrt{\cos2x}.\sqrt[3]{\cos3x}.\sqrt[4]{\cos4x}...\sqrt[n]{\cos{nx}}}{x^2}$ = 10   and we are required to find n . This is the $\frac{0}{0}$ form, so we can use L'HÃ´pital's rule . After taking the derivatives of the numerator and the denominator separately, the problem becomes- $\lim\limits_{x\rightarrow0}  \frac{-\frac{d}{dx}\sqrt{\cos2x}.\sqrt[3]{\cos3x}.\sqrt[4]{\cos4x}...\sqrt[n]{\cos{nx}}}{2x}$ = 10 $\lim\limits_{x\rightarrow0}  \frac{-\frac{d}{dx}\prod_{i = 2}^{n} (\cos ix)^\frac{1}{i}}{2x}$ = 10 Now, the numerator looks like a pretty difficult expression to differentiate. Here, I decided to simplify the expression in the numerator first. Let y= $\prod_{i = 2}^{n} (\cos ix)^\frac{1}{i}$ Now, we can simplify the expression by taking the natural logarithm of both sides. $\log(y)$ = $\log(\prod_{i = 2}^{n} (\cos ix)^\frac{1}{i})$ Now we can use the property of logarithms to simplify the expression, $\log(xy)=\log(x)+\log(y)$ $\log(y)$ = $\sum_{i = 2}^{n} \frac{1}{i}.\log(\cos ix)$ This is where I ran out of ideas to simplify this expression any further. Any ideas would be appreciated.",['limits']
4634188,Clifford's Theorem for algebraic groups,"This is Exercise 4.2 in Milne's 'Algebraic Groups': Let $G$ be an algebraic group over $k$ and $H$ a normal algebraic subgroup of $G$ . From a representation $(V,r)$ of $H$ and a $g\in G(k)$ , we get a conjugate representation $h \mapsto r(ghg^{-1})$ of $H$ . Assume that $G(k)$ is schematically dense in $G$ , and let $(V,r)$ be a simple representation of $G$ . Show that $(V, r|_H)$ is semisimple and that all of its simple constituents are conjugate and have the same multiplicity. This is basically a generalization of Clifford's theorem to algebraic groups. The original version says that a simple representation of an abstract group splits as a direct sum of conjugate simple representations after restriction to a normal subgroup. In particular, for any $k-$ algebra $R$ it shows the representation $(V\otimes R,r)$ of $G(R)$ splits as a direct sum of conjugate representations of $H(R)$ , i.e. representations of the form $W_R^{g_i}$ conjugate to some representation $W_R$ , where $W_R^{g_i}$ have the same underlying space as $W_R$ and $h\in H(R)$ acts on $W_R^{g_i}$ as $g_ihg_i^{-1}$ acts on $W_R$ with $g_i\in G(R)$ . In particular, this is true for $R=k$ . The only thing that remains is to show that these $g_i$ can be chosen in a consistent way for all $R$ , i.e. as base changes of some $g_i\in G(k).$ This is where the schematic density should come into play, but I can't seem to understand how exactly. I apologise for the messy notation, I still am not quite sure whether spelling everything out explicitly makes the writing more clear or more overburdened with notation when working with algebraic groups as group schemes.","['algebraic-geometry', 'representation-theory', 'algebraic-groups']"
4634191,Solutions of the ODE $y'' + a(x) y' - xa(x) y = 0$,"I am reading an article and at some point we reach a differential equation of the form $$y'' + \left(\frac{1}{x} + \frac{x}{2}\right) y' - \left(\frac{1}{x^2} + \frac{1}{2}\right) y = \frac{1}{x}.$$ Then the author claims that the first solution of the homogenous equation (equation with right hand side set to zero) is given by $y_1(x) = x$ (this one is quite obvious) and the other one is given by $$y_2(x) = y_1(x) \int y_1(x)^{-2} e^{-\int a(x)}dx,$$ for $$a = \frac{1}{x} + \frac{x}{2}.$$ Basically, our ODE can be written as $$y'' + a(x) y' - xa(x) y = \frac{1}{x}.$$ Moreover, he also claims that the solution to the non-homogeneous equation should be $$y(x) = -y_1(x) \int y_1(x)^{-2}e^{-\int a(x)} \int \frac{1}{z} e^{-\int a(x)} y_1(z) dzdx.$$ Does one of you have any idea how he gets these solutions for the homogenous and non-homogenous equations ? The particular solution for the non-homogenous solution looks a bit like the one we should get by using the variation of parameters method but I wasn't able to find the right result, I'm not really good at solving ODEs.","['ordinary-differential-equations', 'real-analysis']"
4634210,Why are two disjoint events defined to be independent if one has zero probability?,"Let $A$ and $B$ be two disjoint events in a probability space and suppose that one of the two events has zero probability. According to the standard definition of independence, this means that $A$ and $B$ are independent. Unfortunately this definition seems very counter-intuitive to me: If both events are non-empty, then I would instead define them to be dependent, as the occurrence of one event excludes the occurrence of the other. Why has the math community accepted a different definition? Just out of convenience? What would be the consequences if we changed the definition?","['conditional-probability', 'independence', 'probability', 'terminology']"
4634238,Find all the values that define an implicit function,"Let $h\colon\mathbb{R}^2\to\mathbb{R}$ defined by $h(x,y)=x^2+y^3+xy+x^3+ay$ (where $a\in\mathbb{R}$ ). a) Find the values of $a$ for which $h(x,y)=0$ defines $y$ as a $\mathscr{C}^1$ implicit function of $x$ in some open neighborhood $U\times V$ of $(0,0).$ b) Find the values of $a$ for which $h(x,y)=0$ defines $x$ as a $\mathscr{C}^1$ implicit function of $y$ in some open neighborhood of $(0,0)$ . c) Let $f$ be the implicit function $U\to\mathbb{R}$ found in question
a), and let $F\colon U\times\mathbb{R}\to\mathbb{R}$ defined as $$F(x,y)=\big(e^{x+y}+x^2-1\mathbin,f(x)+y\cos(x)\big).$$ Show that $F$ has an inverse $F^{-1}$ , of class $\mathscr{C}^1$ , defined in some
neighborhood of $(0,0)$ . Show that $G:=F\circ F+F^{-1}$ is
differentiable at $(0,0)$ , and calculate ${\rm D}G(0,0)$ . Hi everyone, I'm having problems with this exercise, in the first part, what I did was using the implicit function theorem to show that if $a \neq 0$ , then y=f(x) and bla, bla. BUT, if $a \neq 0$ ,
I don't know how to procede, 'cause the differential isn't invertible, and that isn't enough to prove that x doesn't define y as an implicit function. Same goes for the second one, cause the differential is equal to zero, and I can't use the implicit function theorem.
So, any ideas of how I should proceed?","['inverse-function-theorem', 'analysis', 'real-analysis', 'multivariable-calculus', 'implicit-function-theorem']"
4634252,Proof of n-times matrix multiplication,"I want to proof the following: $$
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n=  \left[ {\begin{array}{cc}
    1 & 0 \\
    2^n-1 & 2^n \\
  \end{array} } \right]
$$ I betitle the matrix on the left with $A$ and the matrix on the right with $B$ . I am not finding the right idea. I started with stating that the resulting matrix of $
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n
$ has to be a $2\times2$ matrix because of the definition of matrix multiplication. Then I wrote the following $$  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n=  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]\times  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]\times\dots\times  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]=  \left[ {\begin{array}{cc}
    1\times 1+0\times 1 & 1\times 0 + 0\times 2 \\
    1\times 1 + 2\times 1 & 1\times 0 + 2\times 2 \\
  \end{array} } \right]\times\dots\times  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]=  \left[ {\begin{array}{cc}
    1 & 0 \\
    3 & 4 \\
  \end{array} } \right]\times\dots\times  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]$$ My idea behind this was that this should show that the first line will never change, because we always just multiply with the same matrix again. I think this is quite easy to see. For the second line it's in my opinion quite easy to see that the second number ( $b_{22}$ ) is always the old second number ( $a_{22}$ ) times 2 and because the first number with wich we started ( $a_{22}$ ) is the number 2 it's just always powers of 2. And the first number ( $b_{21}$ ) is always the old first number ( $a_{21}$ ) plus the old second number ( $a_{22}$ ). And because the first number with wich we started ( $a_{21}$ ) is on less than the second number with wich we started ( $a_{22}$ ) it's always one less than the power of 2. I have problems with explaining my proof and I am not quite sure if it's correct, so can maybe someone recommend a more exact way of proofing this, because I don't know if my sentences are exact enough. Induction Try Thanks to the comment from @Jaap Sherphuis I now tried to proof the statement with induction and started with the basecase $n=1$ . $$  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^1=\left[ {\begin{array}{cc}
    1 & 0 \\
    2^1-1 & 2^1 \\
  \end{array} } \right]$$ This is obviously true. Now I take $
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n=  \left[ {\begin{array}{cc}
    1 & 0 \\
    2^n-1 & 2^n \\
  \end{array} } \right]
$ as given and try to show with it the statement for $n=n+1$ . $$
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^{n+1}=  \left[ {\begin{array}{cc}
    1 & 0 \\
    2^{n+1}-1 & 2^{n+1} \\
  \end{array} } \right]\Leftrightarrow
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n \times   \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]=  \left[ {\begin{array}{cc}
    1 & 0 \\
    2^n-1 & 2^n \\
  \end{array} } \right] \times   \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]
$$ Because $
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n=  \left[ {\begin{array}{cc}
    1 & 0 \\
    2^n-1 & 2^n \\
  \end{array} } \right]
$ is true the whole statement is true for $n=n+1$ and thus the whole statement is true for every $n\in\mathbb{N}$ . Is this the correct approach?","['matrices', 'solution-verification', 'linear-algebra']"
4634268,Can we neglect matrices with smaller eigenvalues in comparison to ones with larger eigenvalues?,"Can two matrices be compared as being ""small"" and ""large""? For example, consider  matrix $X=X(t)$ as a function of parameter $t$ (say for time), such that \begin{equation}
\frac{d X}{dt} = YX^2 + Z
\end{equation} for some constant matrices $Y$ and $Z$ . Under what conditions can one neglect matrix $Z$ in this equation?","['matrices', 'matrix-equations']"
4634273,Is there a rigorous way to show it is not a sufficient statistic?,"Source of the question: the exercise 6.22(a) of Statistical Inference Book by Casella and Berger. Let $X_1,...,X_n$ be a random sample from a population with pdf $$f(x|\theta)=\theta x^{\theta-1},0<x<1, \theta>0.$$ Is $\Sigma X_i$ sufficient for $\theta$ ? I know how to find a sufficient statistics. I can use either factorization theorem or exponential family way. One sufficient statistics is $\prod_i X_i$ . I also know how to find other sufficient statistics by using any function of $\prod_i X_i$ . But I am considering whether there exists a more rigorous way to conclude $\Sigma X_i$ is not sufficient.","['statistical-inference', 'statistics']"
4634282,Gaussian Elimination - where did I go wrong?,"I have just learned about Gaussian Elimination and I decided to try an example question. I was trying to solve a question and I realised later that I had copied the question wrong but I still decided to proceed and solve the system of linear equations. However, when I checked my answer online using an online calculator it gave different values meaning my solution was wrong. I've checked my working out and I can't seem to figure out where I've gone wrong. I'd appreciate if someone could help me figure out my mistake. I've used an online tool to convert my writing into LATEX but it's kind of messed up. I have no idea how to write in LATEX.  As a result, I have attached images as well: Working Out Page 1 Working Out Page 2 Also, I'd appreciate any suggestions on how to improve my work. (Next time, I'm just going to pivot on the 3 to avoid fractions.) Here it is: Q) $$
\begin{aligned}
& 3 x-2 y-4 z=3 \\
& 2 x+3 y+3 z=15 \\
& 5 x-3 y+z=14 \\
& {\left[\begin{array}{rrr|r}
3^* & -2 & -4 & 3 \\
2 & 3 & 3 & 15 \\
5 & -3 & 1 & 14
\end{array}\right] \quad R_1 \times \frac{1}{3}=R_1}
\end{aligned}
$$ $$
\begin{aligned}
& {\left[\begin{array}{ccc|c}
1 & -\frac{2}{3} & -\frac{4}{3} & 1 \\
2 & 3 & 3 & 15 \\
5 & -3 & 1 & 14
\end{array}\right] R_2-2 R_1 \rightarrow R_2} \\
& {\left[\begin{array}{ccc|c}
1 & -\frac{2}{3} & -4 / 3 & 1 \\
0 & 3 / 3 & 17 / 3 & 13 \\
5 & -3 & 1 & 14
\end{array}\right] R_3-5 R_1 \rightarrow R_3}
\end{aligned}
$$ $$
\left[\begin{array}{ccc|c}
1 & -2 / 3 & -4 / 3 & 1 \\
0 & 13 / 3 & 17 / 3 & 13 \\
0 & 1 / 3 & 23 / 3 & 9
\end{array}\right] \text { }
$$ \begin{aligned}
& {\left[\begin{array}{ccc|c}
1 & -2 / 3 & -4 / 3 & 1 \\
0 & 1 & 17 / 13 & 3 \\
0 & 1 / 3 & 23 / 3 & 9
\end{array}\right] \quad \begin{array}{l}
R_2 \times 3 / 13 \\
R_3-1 / 3 R_2 \rightarrow R_3
\end{array}} \\
& {\left[\begin{array}{ccc|c}
1 & 0 & -6 / 13 & 1 \\
0 & 1 & 17 / 13 & 3 \\
0 & 0 & 94 / 13 & 11
\end{array}\right] \quad R_1+2 / 3 R_2 \rightarrow R_1} \\
& {\left[\begin{array}{ccc|c|}
1 & 0 & -6 / 1 & 1 \\
0 & 1 & 17 / 13 & 3 \\
0 & 0 & 1 & 143 / 94
\end{array}\right] \quad \begin{array}{l}
R_1+\frac{6 R_3}{13} \rightarrow R_1 \\
R_2-\frac{17 R_3}{3} \rightarrow R_2
\end{array}} \\
& {\left[\begin{array} { l l l | l } 
{ 1 } & { 0 } & { 0 } & { 8 0 / 4 7 } \\
{ 0 } & { 1 } & { 0 } & { 9 5 / 9 4 } \\
{ 0 } & { 0 } & { 1 } & { 1 4 3 / 9 4 }
\end{array} \quad \left[x=\frac{165}{47} \quad y=\frac{73}{47} z=\frac{52}{47}\right.\right.} \\
&
\end{aligned}","['matrices', 'gaussian-elimination', 'linear-algebra']"
4634290,Morphisms from an open set of an affine variety to a projective space,"Let $X$ be an affine variety, with coordinate ring $A(X)$ , and take an open set $U\subset X$ . Is it true that any morphism of varieties $U\to \mathbb P^{n-1}$ is of the form $x\mapsto (g_1(x):\dots:g_n(x))$ , for $g_1,\dots,g_n\in A(X)$ such that $X-Z(g_1,\dots,g_n)\supset U$ ? The converse implication seems easily true; instead the direction of the part in italics is surely true if $X=\mathbb A^1$ , or $X=\mathbb A^1-\{0\}$ , and $n=2$ ; but if $X=\mathbb A^2$ or $X=\mathbb A^2-\{0\}$ , I already don't understand how to extend the proof. Am I trying to prove something true at least? Thank you","['affine-varieties', 'algebraic-geometry']"
4634456,Trying to solve a functional equation,"Let $a_n$ be a sequence of strictly positive real numbers such that $\lim_{n \to \infty}a_n=0$ . Find all functions $f: \mathbb{R} \to \mathbb{R}$ that admit primitives(i.e. there exists a function $F:\mathbb{R} \to \mathbb{R}$ such that $\frac{dF(x)}{dx}=f(x), \forall x \in \mathbb{R}$ ) and satisfies the following equality $$2f(x)=f(x+a_n)+f(x-a_n), \forall x \in \mathbb{R}, \forall n \in \mathbb{N}$$ My approach to this problem was to first use the fact that there exists a function $F:\mathbb{R} \to \mathbb{R}$ , such that $F'(x)=f(x)$ , so the equation becomes $2F(x)=F(x+a_n)+F(x-a_n)+C_n$ , for some arbitrary constant $C_n$ . Rearranging the terms, we get that $$0=\frac{F(x+a_n)-F(x)}{a_n}-\frac{F(x-a_n)-F(x)}{-a_n} + \frac{C_n}{a_n}$$ Because $a_n$ approaches $0$ we can take the limit as $n$ approaches $\infty$ , and because $F$ is differentiable, we obtain $0=\lim_{n \to \infty} \frac{C_n}{a_n}$ . Therefore $\lim_{n \to \infty}C_n=0$ , but that is not useful. $f(x)=0$ , $f(x)=k, F(x)=kx$ satisfy the equation, but I  cannot seem to deduce them. Did I do a mistake or is the exercise wrong?","['analysis', 'real-analysis', 'calculus', 'limits', 'derivatives']"
4634473,"Order of $xy$ in various quotients of the free product $G_1 * G_2$, where $x \in G_1, y \in G_2$ are nontrivial","Say we have two arbitrary nontrivial groups $G_1$ and $G_2$ , and some arbitrary nontrivial elements $x \in G_1, y \in G_2$ . Then it is known that the order of $xy$ in the free product $G_1 * G_2$ is infinite. However, quotienting $G_1 * G_2$ by the normal subgroup generated by $(xy)^n$ should create a group where the image of $xy$ has order dividing $n$ . The question I have is about the word ""dividing"" there: does this process always set the order of $xy$ to exactly $n$ (for any $n \in \mathbb{N}$ )? Or is there some counterexample where, say, quotienting the free product by the normal subgroup generated by $(xy)^4$ makes $xy$ only have order $2$ , or something along those lines? (If this isn't true for $xy$ but it is true for some other word like $xyx^{-1}y^{-1}$ then I would be interested in knowing a word that it is true for.)","['quotient-group', 'abstract-algebra', 'free-groups', 'free-product', 'group-theory']"
4634495,Y.A. Rozanov 'Probability Theory A Concise Course' Problem 2.17,"Problem Statement: Given any $n$ events $A_1,A_2, ...,A_n$ , prove that the probability of exactly $m$ ( $m \le n$ ) of them happening is $$P_m - \binom{m+1}{m}P_{m+1} + \binom{m+2}{m}P_{m+2} - \cdots \pm \binom{n}{m}P_n$$ where $P_k = \sum_{1\le i_1<i_2 \cdots <i_k \le n}\Pr(\bigcap_{r=1}^k A_{i_r})$ My thoughts: This looks quite similar to the inclusion-exclusion principle where we had: $$\sum_{i=1}^n(-1)^{i+1}P_i$$ but here we have $$\sum_{i=m}^n(-1)^{i+m}\binom{i}{m}P_i$$ I understand that $P_k$ implies you should sum the probabilities of all the combinations of $A$ of length $k$ . Since this already includes the combinations, I don't understand why we need a binomial coefficient before each term. To find the required probability, one can add all possible probabilities of a combination of $m$ events happening, then subtract those of $m+1$ events happening (since the first one includes this). The same question has been asked before here and here . But in the first one, the answerer uses the Indicator function (I'm aware of its basic properties, but the answerer does something which is not clear to me), and in the second, the OP themselves provide an answer (but it has a flaw (?) - they take $\Pr(M \cap N^C) = \Pr(M) - \Pr(N)$ ). So, I'm looking for an answer that at least clearly states how can I proceed with the proof, the reason for the binomial coefficients and uses the Indicator function as less as possible (since till now the author has not discussed it in the book).","['proof-writing', 'probability-theory', 'probability']"
4634517,Why $\bigg\lfloor \frac{n^m}{\binom{n}{m}}\bigg\rfloor=m!$ for $n>(m+1)^{m+2}$?,"Let $n,m\in \mathbb{N}_0$ . I need to prove that if $n>(m+1)^{m+2}$ then $$ \Bigg\lfloor \frac{n^m}{\binom{n}{m}}\Bigg\rfloor=m!.$$ Since $$ \frac{n^m}{\binom{n}{m}}=\frac{n^m(n-m)!}{n!}m!=m!+\bigg(\frac{n^m(n-m)!}{n!}-1\bigg)m!,$$ I have tried to see when $$\frac{n^m(n-m)!}{n!}-1<\frac{1}{m!}.$$ Can anyone help me or give me a hint? Thanks in advance!","['number-theory', 'ceiling-and-floor-functions', 'binomial-coefficients', 'integers']"
4634533,Covering a $kn+1\times kn+1$ region on a $(k+1)n-1\times (k+1)n-1$ square grid,"We are given a $\left((k+1)n - 1\right)\times \left((k+1)n-1\right)$ square grid and tiles of size $1\times n$ . We can place the tiles anywhere on the board, provided that they never cover the same area more than once. The tile orientation (horizontal or vertical) does not matter. The question asked is can we put some number of these tiles on the board so that there exists a $(kn+1)\times(kn+1)$ region which is fully covered by the tiles? After a few attempts I suppose the answer is negative. I have tried various tricks with coloring the grid, but with no success. I think it's crucial that we can never fit $k + 1$ tiles oriented horizontally in the same row (an analogous observation is true for columns), so maybe there is a clever way of coloring based on that. The question originally asked for the case $k=3, n=5$ under the assumption the $(kn+1)\times(kn+1)$ region must not share any border with the whole grid. However, as was suggested in the comments, this assumption is not necessary.","['contest-math', 'puzzle', 'combinatorics', 'game-theory', 'tiling']"
4634563,The commutator of Holomorph of generalized quaternion is abelian?,"Let $Q_{2^{n}} = \langle x, y | x^{2^{n-1}}=y^4 = 1, x^{2^{n-2}}=y^2, y^{-1}xy = x^{-1} \rangle$ - generalized quaternion group of order $2^{n}$ . $\operatorname{Hol}(Q_{2^{n}})$ - Holomorph of this group Let $G = [ \operatorname{Hol}(Q_{2^{n+1}}), \operatorname{Hol}(Q_{2^{n}})]$ - commutator of $\operatorname{Hol}(Q_{2^{n+1}})$ I'm wondering whether $G$ is abelian. Using the SAGE, I checked that this is true for $n = 3 ... 9$ . Some auxiliary facts: $${\rm Aut}(Q_{2^n}) \cong  \left\{ \begin{pmatrix} a & b \\ 0 & 1 \end{pmatrix} : a \in \mathbb{Z}^*_{2^{n-1}}, b\in \mathbb{Z}_{2^{n-1}} \right\} \cong AGL(\mathbb{Z}_{2^{n-1}}, 1)$$ such that for $\varphi \in {\rm Aut}(Q_{2^n})$ $$\varphi (x) = x^a, \varphi (y) = x^by$$ $[Q_{2^n}, Q_{2^n}] = \langle x^2 \rangle$ $[{\rm Aut}(Q_{2^n}), {\rm Aut}(Q_{2^n})] = \left\{ \varphi : \varphi(x) = x, \varphi (y) = x^by,\space b\in 2\mathbb{Z}_{2^{n-1}}\right\}$","['derived-subgroup', 'holomorph', 'group-theory', 'abelian-groups']"
4634597,When can a double integral be interpreted as a surface area?,"Let $D$ be a closed, bounded domain in $\mathbb R^2$ and let $\vec r(u,v) = \langle x(u,v), y(u,v), z(u,v) \rangle$ for $(u, v) \in D$ be a parametrization of a smooth surface $S \subseteq \mathbb R^3$ .  Then the area of $S$ is $$\iint_D \| \vec r_u \times \vec r_v \| \,  dA$$ I am interested in what might be considered a kind of converse of this: Is it always possible to interpret an integral $\iint_D f \, dA$ as the area of some surface? More precisely, given a (non-negative) function $f:\mathbb R^2 \to \mathbb R$ , defined on some domain $D$ , under what conditions can we find a surface $S \subseteq \mathbb R^3$ and a parametrization $\vec r(u,v): D \to S$ such that $$f(u,v) = \| \vec r_u \times \vec r_v \|$$ so that consequently $$\iint_D f \, dA = \iint_D \| \vec r_u \times \vec r_v \| \,  dA \, ?$$",['multivariable-calculus']
4634653,Probability of poker pair - what's wrong with this?,"Thanks to others here, I know the correct way to calculate the probability of one pair in 5-card poker is P(one pair) = $C(13, 1) * C(4, 2) * C(12, 3) * [C(4, 1)]^3 / C(52, 5)$ = 0.4225690276... Why doesn't the following work? The answer comes out to exactly 1/10 of the correct answer (to over 20 decimal places) - I find that highly coincidental but I can't figure out where the factor of 10 comes from. Once you've chosen the first card, you have a 3/51 probability of choosing the matching denomination for a pair. Then you have a 48/50 probability of choosing a different denomination out of the remaining 50 cards for card #3. Then you have a 44/49 probability of choosing another different denomination for card #4. Then you have a 40/48 probability of choosing a fourth different denomination for card #5. P(one pair) = 3/51 * 48/50 * 44/49 * 40/48 = 0.04225690276..., exactly 1/10 the answer above. Thanks!","['poker', 'probability']"
4634663,"Number of distinct arrangement of $(a,b,c,d,e)$","If $a<b<c<d<e $ be positive  integer such that $a+b+c+d+e=20$ . Then number of distinct arrangement of $(a,b,c,d,e)$ is Here the largest value of $e$ is $10$ like $a\ b\ c\ d\ e$ as $ \ \  1\ 2\ 3\ 4\ 10$ And least value is $6$ like $ a\ b\ c\ d\ e$ as $\ \ 2\ 3\ 4\ 5\ 6$ Now after that solution given in book as Total number of ways $ \displaystyle =\binom{4}{0}+\frac{\binom{4}{1}}{4}+\frac{\binom{4}{2}}{3}+\frac{\binom{4}{3}}{2}+\frac{\binom{4}{4}}{1}$ $\displaystyle = 1+1+2+2+1=7$ I did not understand last $2$ line i  e  solution given in book Please have a look on that part",['combinatorics']
4634734,Is there a first-order sentence that can distinguish cyclic and non-cyclic finite Abelian groups?,"Let $\mathfrak{A}$ be the class of all Abelian groups. It is easy to show that there is no first-order sentence $\varphi$ that can distinguish cyclic and non-cyclic groups in this setting. Consider the group $(\mathbb{Z}, +)$ , which is cyclic. By upward LÃ¶wenheim-Skolem, there exists a group $G$ with cardinality $\beth_1$ that is elementarily equivalent to $(\mathbb{Z}, +)$ . However, $G$ cannot be cyclic since any group with exactly one generator is countable. Suppose, then, that we consider $\mathfrak{B}$ , the class of all finite Abelian groups. Is there a sentence in the language of groups for $\mathfrak{B}$ that distinguishes cyclic and non-cyclic groups?","['first-order-logic', 'logic', 'group-theory', 'abstract-algebra', 'abelian-groups']"
4634735,"Proof that axiom of replacement won't generate a set going arbitrarily high in the cumulative hierarchy, without relying on replacement?","Is it possible to prove, in first-order ZFC, that the axiom of replacement can't generate a set that includes members going arbitrarily high in the cumulative hierarchy, but without using the axiom of replacement itself for the proof?  Or in other words, to prove without using replacement that the kind of mapping used by replacement won't include elements in its output image going arbitrarily high in the hierarchy? If it's helpful, part of the motivation for this question is the issue raised here [1] arguing that the replacement axiom doesn't follow directly from the iterative conception of sets: ""When Replacement has been justified according to the iterative conception, the reasoning has in fact been circular as it was in Zermelo [1930a], with some feature of the cumulative hierarchy picture newly adduced solely for this purpose.""  In particular, one concern you might have with the axiom of replacement, from the standpoint of the iterative conception of sets, is that the mapping in replacement might map to sets arbitrarily high in the hierarchy, so there might be no valid set formed by the image of the mapping (just a class).  Now, if you have full ZFC including replacement, then you can show that the mappings used by replacement can never map arbitrarily high in the hierarchy, but what I'm unclear on is whether this can be shown without relying on the axiom of replacement for any part of the proof (since in the context of the linked paper the goal would be to intuitively justify replacement from the iterative conception, and avoid circularity in this justification). Here is a related question, but for 2nd order ZFC [2]. Note, the answer to this previous question assumes that there is an inaccessible cardinal in the metatheory, but the current question should rely only on the ZFC axioms. Edit: remove proof sketch for now, since not sure if it was on the right track. [1] https://www.jstor.org/stable/41472440 [2] https://math.stackexchange.com/questions/1263848/are-categorical-second-order-axiomatizations-of-set-theory-inconsistent-due-to-t]] [3] Proof that we can't get $\omega + \omega$ without Replacement","['elementary-set-theory', 'philosophy', 'order-theory', 'set-theory']"
4634796,"Prove $ \int_0^{\infty} \frac{\ln ^3 x}{(1+x)^{n}} d x =A+B \pi^2$ for some rational numbers $A$ and $B$, where $n\neq 1 $","After knowing that $$
I_2=\int_0^{+\infty} \frac{\ln ^3 x}{(1+x)^{2}} d x \stackrel{x\mapsto\frac{1}{x}}{=} -\int_0^{+\infty} \frac{\ln ^3 x}{(1+x)^{2}} d x \Rightarrow \int_0^{+\infty} \frac{\ln ^3 x}{(1+x)^{2}} d x=0
$$ I start to investigate the integrals with higher powers $$I_n=\int_0^{+\infty} \frac{\ln ^3 x}{(1+x)^{n}} d x  $$ where $n$ is a natural number greater than $2$ . We first split the interval into two as $$
\begin{aligned}
\int_0^{+\infty} \frac{\ln ^3 x}{(1+x)^3} d x & =\int_0^1 \frac{\ln ^3 x}{(1+x)^3} d x+\int_1^{+\infty} \frac{\ln ^3 x}{(1+x)^3} d x \\
& =\int_0^1 \frac{\ln ^3 x}{(1+x)^3} d x+\int_1^0 \frac{\ln ^3\left(\frac{1}{x}\right)}{\left(1+\frac{1}{x}\right)^3} \frac{d x}{-x^2} \\
& =\int_0^1 \frac{\ln ^3 x}{(1+x)^3} d x-\int_0^1 \frac{x \ln ^3 x}{(1+x)^3} d x
\end{aligned}
$$ For any $|x|<1$ , we have $$
\frac{1}{1+x}=\sum_{k \rightarrow 0}^{\infty}(-1)^k x^k
$$ Differentiating both sides w.r.t. $x$ twice yields $$
\frac{1}{(1+x)^3}=\frac{1}{2} \sum_{k=0}^{\infty}(-1)^k(k+2)(k+1) x^k
$$ Plugging into the integrand, we have $$
\int_0^{\infty} \frac{\ln ^3 x}{(1+x)^3}dx=\frac{1}{2}\left[\sum_{k=0}^{\infty}(-1)^k(k+2)(k+1)\left( \int_0^1 x^k \ln ^3 x d x-\int_0^1 x^{k+1} \ln ^3 x d x \right)\right] 
$$ Noting that $$
\begin{aligned}
\int_0^1 x^n \ln ^3 x d x & =\left.\frac{\partial^3}{\partial a^3} \int_0^1 x^a d x\right|_{x=w} \\
& =\left.\frac{\partial^3}{\partial a^3}\left(\frac{1}{a+1}\right)\right|_{x=n} \\
& =-\frac{6}{(n+1)^4}
\end{aligned}
$$ $$
\begin{aligned}I_3&=\frac{1}{2} \sum_{k=0}^{\infty}(-1)^k(k+2)(k+1)\left[-\frac{6}{(k+1)^4}+\frac{6}{(k+2)^4}\right]\\&= 3 \left[-\sum_{k=0}^{\infty} \frac{(-1)^k(k+2)}{(k+1)^3}+\sum_{k=0}^{\infty} \frac{(-1)^k(k+1)}{(k+2)^3}\right]\\&= -3\left[\sum_{k=0}^{\infty} \frac{(-1)^k(k+2)}{(k+1)^3}+\sum_{k=1}^{\infty} \frac{(-1)^kk}{(k+1)^3}\right]\\&=-3 \sum_{k=0}^{\infty} \frac{(-1)^k(2 k+2)}{(k+1)^3}\\&=-6 \sum_{k=0}^{\infty} \frac{(-1)^k}{(k+1)^2}\\&=-\frac{\pi^2}{2}\end{aligned}
$$ Letâ€™s continue with $I_4$ by considering their difference $$
\begin{aligned}
D & =\int_0^{+\infty} \frac{\ln ^3 x}{(1+x)^3} d x -\int_0^{+\infty} \frac{\ln ^3 x}{(1+x)^4} d x \\
& =\int_0^{+\infty} \frac{x \ln ^3 x}{(1+x)^4} d x \\
& =\int_0^{+\infty} \frac{\frac{1}{x} \ln ^3 \frac{1}{x}}{\left(1+\frac{1}{x}\right)^4} \frac{d x}{x^2} \\
& =-\int_0^{+\infty} \frac{x \ln ^3 x}{(1+x)^4} d x \\
& =-D\\\Rightarrow D&=0 
\end{aligned}
$$ Hence $$\boxed{I_4=I_3=-\frac{\pi^2}{2} }$$ By Wolfram-alpha, we get $$
\begin{aligned}
& I_5=-\frac{1}{24}\left(6+11 \pi^2\right) \\
& I_6=-\frac{1}{12}\left(6+5 \pi^2\right) \\
& I_7=-\frac{1}{360}(255+137 \pi^2)
\end{aligned}
$$ I guess that in general, $$
I_n=A+B \pi^2 \textrm{  for some rational numbers } A \textrm{ and }B.
$$ Can we prove it further?  Your help or suggestions are highly appreciated.","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'pi']"
4634819,Historical proof of BÃ©zout's theorem,"BÃ©zout's theorem says that given $f, g$ homogeneous polynomials over $\mathbb{CP}^2$ with degrees $n, m$ respectively, the number of intersections of their zero sets is exactly $nm$ , counted with multiplicities. I know of two proofs that can be found in modern references. One uses the more classical machinery of resultants (in which intersection multiplicity is defined as the exponent of a particular term in a particular resultant), and the other uses the more modern machinery of local rings (in which intersection multiplicity is defined as the dimension of a particular vector space). Wikipedia alludes to a third, less rigorous proof that goes somewhat along these lines: The intersection multiplicity of $f$ and $g$ at $p$ can be computed by applying small perturbations to the coefficients of $f$ and $g$ , then counting the maximum possible number of intersections in a small neighborhood of $p$ . In doing so, we reduce the problem to counting intersections of generic polynomials, which is easy (but I don't see why it's easy). Apparently, this is BÃ©zout's original proof, so I tried reading a translation of his original book General Theory of Algebraic Equations . But I have a difficult time parsing the older language. Is anyone able to help me write down the correct definitions and sketch the basic steps of this argument? Thanks!","['algebraic-geometry', 'intersection-theory', 'math-history']"
4634827,"Closed form for $\int_0^1 \left\{ \frac{m}{x}\right\}^n\,dx$","I am trying to find the closed form for the following integral: $$\int_0^1 \left\{ \frac{m}{x}\right\}^n\,dx, \quad \forall m,n \in \mathbb{N}$$ where $\{x\}$ denotes the fractional part of $x$ . This integral is a generalization of some of the fractional part integrals found in Ovidiu Furdui's book Limits, Series, and Fractional Part Integrals: Problems in Mathematical Analysis . My attempt: $$I(m,n):= \int_0^1 \left\{ \frac{m}{x}\right\}^n\,dx, \quad \forall m,n \in \mathbb{N}.$$ \begin{align*}
I(m,n)
&\stackrel{x \mapsto \frac{m}{x}}{=} m\int_m^\infty \frac{\{x\}^n}{x^2}\,dx\\
&= m\sum_{k = m}^\infty \int_k^{k+1} \frac{(x-k)^n}{x^2}\,dx\\
&= m\sum_{k = m}^\infty \sum_{\ell=0}^n (-1)^\ell \binom{n}{\ell} k^n\int_k^{k+1} \frac{x^{n - \ell}}{x^2}\,dx\\
&= m\sum_{k = m}^\infty\left[ \sum_{\ell=0}^{n-2}\left[ (-1)^\ell \binom{n}{\ell} k^n\int_k^{k+1} \frac{x^{n - \ell}}{x^2}\,dx\right] + (-1)^{n-1}n k^{n-1} \int_0^1 \frac{1}{x}\,dx + (-1)^n k^n\int_k^{k+1} \frac{1}{x^2}\,dx\right]\\
&= m\sum_{k = m}^\infty\left[ \sum_{\ell=0}^{n-2}\left[ (-1)^\ell \binom{n}{\ell} k^n\frac{(k+1)^{n-\ell-1}-k^{n-\ell-1}}{n-\ell-1}\right] + (-1)^{n-1}n k^{n-1} \log\left(\frac{k+1}{k} \right) + (-1)^n \frac{k^{n-1}}{k+1}\right]\\
&= \lim_{N \to \infty} m\sum_{k = m}^N \left[ \sum_{\ell=0}^{n-2}\left[ (-1)^\ell \binom{n}{\ell} k^n\frac{(k+1)^{n-\ell-1}-k^{n-\ell-1}}{n-\ell-1} \right] + (-1)^{n-1}n k^{n-1} \log\left(\frac{k+1}{k} \right) + (-1)^n \frac{k^{n-1}}{k+1}\right]\\
&= \lim_{N \to \infty} m\left[ \sum_{\ell=0}^{n-2}\sum_{k = m}^N\left[ (-1)^\ell \binom{n}{\ell} k^n\frac{(k+1)^{n-\ell-1}-k^{n-\ell-1}}{n-\ell-1} \right] + (-1)^{n-1}n \sum_{k = m}^N \log\left[\left(\frac{k+1}{k} \right)^{k^{n-1}}\right] + (-1)^n \sum_{k = m}^N \frac{k^{n-1}}{k+1}\right].
\end{align*} From here I began to struggle. I believe I I was able to make some good progress with the sum with the logarithm. To do so I defined something I will call the generalized hyperfactorials (I do not know if such a function exists in literature): $$\mathrm{H}(k,n) := \prod_{j = 1}^n j^{j^k}.$$ We see that if $k = 0$ , we get the regular factorial. If we let $k = 1$ , we get the hyperfactorial. With this function, I believe I the partial sum with the logarithms has a closed form: \begin{align*}
\sum_{k = m}^N \log\left[\left(\frac{k+1}{k} \right)^{k^{n-1}}\right]
&= \log\left[\prod_{k = m}^N\left(\frac{k+1}{k} \right)^{k^{n-1}}\right]\\
&= \log\left[\frac{(m+1)^{m^{n-1}} \times (m+2)^{(m+1)^{n-1}} \times \ldots \times N^{(N-1)^{n-1}} \times (N+1)^{N^{n-1}}}{m^{m^{n-1}} \times (m+1)^{(m+1)^{n-1}} \times (m+2)^{(m+2)^{n-1}} \times \ldots \times N^{N^{n-1}}}\right]\\
&= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \left((m+1)^{m^{n-1} - (m+1)^{n-1}} \times (m+2)^{(m+1)^{n-1} - (m+2)^{n-1}} \times N^{(N - 1)^{n-1} - N^{n-1}}\right)\right]\\
&= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \left((m+1)^{((m+1) - 1)^{n-1} - (m+1)^{n-1}} \times (m+2)^{((m+2) - 1)^{n-1} - (m+2)^{n-1}} \times N^{(N - 1)^{n-1} - N^{n-1}}\right)\right]\\
&= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \left((m+1)^{\sum_{p=1}^{n-1} (-1)^p \binom{n-1}{p}(m+1)^{n - p -1}} \times (m+2)^{\sum_{p=1}^{n-1} (-1)^p \binom{n-1}{p}(m+2)^{n - p -1}} \times \ldots \times N^{\sum_{p=1}^{n-1} (-1)^p \binom{n-1}{p} N^{n - p -1}}\right)\right]\\
&= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \prod_{p=1}^{n-1}\left((m+1)^{(m+1)^{n - p -1}} \times (m+2)^{(m+2)^{n - p -1}} \times \ldots \times N^{ N^{n - p -1}}\right)^{(-1)^p \binom{n-1}{p}}\right]\\
&= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \prod_{p=1}^{n-1}\left(\frac{\mathrm{H}(n -p-1,N)}{\mathrm{H}(n -p-1,m)}\right)^{(-1)^p \binom{n-1}{p}}\right].
\end{align*} Hopefully I did not make any typos. I believe the integrals has a strong connection to the Bendersky-Adamchik constants. Here are some papers on the constants Some new quicker approximations of Glaisherâ€“Kinkelin's and Benderskyâ€“Adamchik's constants Closed-form calculation of infinite products of Glaisher-type related to Dirichlet series From these papers, we can use the limit definition of the Bendersky-Adamchik constants to derive Stirling-like asymptotic formulas for the generalized hyperfactorials, which I conjecture will be handy when evaluating the limit. I also believe that the other sums left to be simplified could be simplified via the Euler-Maclaurin summation formula or even Faulhaber's formula somewhere down the line. Possibly the Bernoulli numbers from the definition of the limit definition of the Bendersky-Adamchik constants interact with Bernoulli numbers from the Euler-Maclaurin summation formula. Is it possible that I am approaching this integral the wrong way? If so, what would be the best way to tackle this integral?","['integration', 'summation', 'improper-integrals', 'calculus', 'sequences-and-series']"
4634896,Are there infinite prime of this form ? $\lfloor n\sin^{2}\left(n\right)+1\rfloor$ where $n$ is a prime .,"If $n$ is a prime let's define (we are in radians): $$\lfloor n\sin^{2}\left(n\right)+1\rfloor$$ Do we have infinitly  primes of the form $$\lfloor n\sin^{2}\left(n\right)+1\rfloor$$ For small primes we have : $(11,11),(13,3),(61,57),...$ As a random trial, we have an example: $n=100043$ . It gives us $72467$ , which is a prime. In this case we have: $$100043=2+3 \cdot 33347$$ $33347$ is a prime, so we have a new pair: $(33347,17837)$ . Perhaps we can use a famous result due to Euler: $$x\sin^{2}\left(x\right)=x\left(x\prod_{n=1}^{\infty}\left(1-\frac{x^{2}}{n^{2}\pi^{2}}\right)\right)^{2}$$ Following the advice @Peter if $1<y<2$ : $$y=x\sin^{2}\left(x\right)+1=\exp\left(-\sum_{n=1}^{\infty}\frac{\left(-1\right)^{n}\left(x\sin^{2}\left(x\right)\right)^{n}}{n}\right)$$ If We have : $$f_{mi}\left(x\right)=\exp\left(-\sum_{n=1}^{i}\frac{\left(-1\right)^{n}\left(x\left(x\prod_{k=1}^{m}\left(1-\frac{x^{2}}{k^{2}\pi^{2}}\right)\right)^{2}\right)^{n}}{n}\right)$$ Then $C$ a positive integer  : $$f_{mi}(C\pi)=f_m(C\pi)f_i(C\pi)=1$$ Perhaps it's a multiplicative function https://en.wikipedia.org/wiki/Multiplicative_function .But I'm not sure... Nota bene : For $p$ an integer sufficiently large , $i,m\in[p,\infty]$ $\exists \varepsilon \in[0,1],x\in[C\pi-\varepsilon ,C\pi+\varepsilon]$ such that : $$x\sin^2(x)+1\leq \exp\left(-\sum_{n=1}^{i}\frac{\left(-1\right)^{n}\left(x\left(x\prod_{k=1}^{m}\left(1-\frac{x^{2}}{k^{2}\pi^{2}}\right)\right)^{2}\right)^{n}}{n}\right)$$ Last attempt : We need to find see Willans formula https://en.wikipedia.org/wiki/Formula_for_primes : $$p_n-1=\sum_{i=1}^{2^{n}}\operatorname{floor}\left(\left(\frac{n}{\sum_{j=1}^{i}\operatorname{floor}\left(\left(\cos\left(\frac{\left(\left(j-1\right)!+1\right)}{j}\pi\right)\right)^{2}\right)}\right)^{\frac{1}{n}}\right)=\lfloor k\sin^2(k)\rfloor$$ Do we have infinite  primes of this kind ? If yes, how do we show it? As in my  trial example, what is the maximum number of pairs we can make if we subtract by two and divide by three? PS: Can you propose a name for this kind of pair of primes?","['elementary-number-theory', 'trigonometry', 'gcd-and-lcm', 'prime-numbers']"
4634945,Zariski topology and morphisms that coincide on a dense set,"In Miles Reid Undergraduate Algebraic Geometry book it is stated informally, about the Zariski topology: (1) two morphisms which coincide on a dense open set coincide everywhere I am suprised that he requires a dense open set since morphisms are continuous maps for the Zariski topology and I thought that for a general topological space we had the following statement (2) two continuous maps which coincide on a dense set coincide everywhere My question is: is (2) a valid statement of general topology? If not, what specificities of the Zariski topology make it invalid? (compared to, for instance, a metric space â€” which I'm more familiar with) EDIT: Also, a link to a proof of the correct statement, with the precise conditions, would be much appreciated","['zariski-topology', 'general-topology', 'algebraic-geometry']"
4635033,"For an elliptic curve $E$, how can we describe canonical divisor or addition $\operatorname{Sym}^2(E) \to E$ as projectivization of a vector bundle?","Let $E$ be an elliptic curve. Then I know that $S = \operatorname{Sym}^2(E) = E \times E / S_2$ is a smooth surface, and if we choose an origin $P_0 \in E$ , we get a $\mathbb P^1$ -bundle $$\pi: S \to E, \{P,Q\} \mapsto P+Q.$$ So $S$ is a ruled surface over $E$ . I would like to know determine a canonical divisor on $S$ , in particular I wonder if it is a multiple of the diagonal $\Delta \subset S$ . One strategy to understand $S$ and $\pi$ better would be to determine a vector bundle $\mathcal E$ on $E$ such that $S = \mathbb P(\mathcal E)$ , but I don't know how to do that. Any help would be appreciated :)","['algebraic-curves', 'vector-bundles', 'algebraic-geometry', 'surfaces']"
4635100,A magic basis of $\mathbb{C}^5$,"This is a a small $n$ restriction of another question . Find a $5\times 5$ matrix of unit vectors $\xi_{ij}\in \mathbb{C}^5$ such that: Entries along rows and columns are orthogonal, that is for $1\leq i,j,k,l\leq 5$ : $$\delta_{i,k}+\delta_{j,l}=1\implies \langle \xi_{ij},\xi_{kl}\rangle=0.$$ Entries not on a common row or column are neither parallel, anti-parallel, nor orthogonal, again for $1\leq i,j,k,l\leq 5$ :: $$\delta_{i,k}+\delta_{j,l}=0\implies 0<|\langle \xi_{ij},\xi_{kl}\rangle|<1$$ This is proving a real wicked problem. Motivation and an $n=4$ example in the original questions. Attempts thus far: I have started with the first two rows : $$\xi=\frac12\begin{bmatrix} \begin{pmatrix} 2 \\ 0 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix} & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix}
\\  \begin{pmatrix} 0 \\ 2 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} \star \\ 0 \\ \star \\ \star\\ \star\end{pmatrix} & \begin{pmatrix} \star \\ 0 \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} \star \\ 0 \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} \star  \\ 0 \\ \star \\ \star\\ \star\end{pmatrix} \end{bmatrix}$$ To get orthogonality along the first row, we are multiplying four numbers, and along columns, just three. I started with 24th roots of unity but then this reduced to sixth roots, powers of $w=\exp(2\pi i/6)$ . You can get some of what you want with this by considering what combinations of four and three sixth roots give zero. It seems possible to get all but one of: orthogonal along rows one, two, three appropriate orthogonality between rows one and two, and one and three appropriate orthogonality between rows two and three But it seems to fail before it ever gets to rows four or five, or indeed the second condition of being non-orthogonal nor parallel, nor anti-parallel. This wasn't the best I did, but something like: $$\xi=\frac12\begin{bmatrix} \begin{pmatrix} 2 \\ 0 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} 0 \\ 1 \\ 1 \\ 1\\ 1\end{pmatrix} & \begin{pmatrix} 0 \\ 1 \\ 1 \\ -1\\ -1\end{pmatrix}  & \begin{pmatrix} 0 \\ w \\ w^4 \\ 1\\ -1\end{pmatrix}  & \begin{pmatrix} 0 \\ 1\\ -1 \\ w^2\\ w^5\end{pmatrix}
\\  \begin{pmatrix} 0 \\ 2 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} w \\ 0 \\ 1 \\ w^2\\ w^4\end{pmatrix} & \begin{pmatrix} w \\ 0 \\ 1 \\ w^5\\ 1\end{pmatrix}  & \begin{pmatrix} w^2 \\ 0 \\ w^4 \\ w^2\\ w\end{pmatrix}  & \begin{pmatrix} w \\ 0 \\ -1 \\ w^4\\ -1\end{pmatrix}
\\  \begin{pmatrix} 0 \\ 0 \\ 2 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} 1\\ w \\ 0 \\ w^5\\ -1\end{pmatrix} & \begin{pmatrix} w^2 \\ w \\ 0 \\ w^2\\ 1\end{pmatrix}  & \begin{pmatrix} -1\\ w^2 \\ 0 \\ w^5\\ 1\end{pmatrix}  & \begin{pmatrix} w^2 \\ w \\ 0 \\ w\\ w^2\end{pmatrix}  \end{bmatrix}$$ This attempt falls down on orthogonality in row three, and orthogonality in row 2 vs row 3.","['inner-products', 'orthogonality', 'matrices', 'roots-of-unity', 'complex-numbers']"
4635118,Automorphisms countable set,"I am stucked with the following exercise from an elementary set theory course. Any help, even just a small advice on how to start, would be appreciated. Let $(S,<)$ be a total order and suppose $S$ to be a countable set. If
the cardinality of the  automorphisms of $(S,<)$ is more than
countable, prove that then it is exactly $2^{\aleph_0}$ . This is of
course without assuming CH. I edited the orginal post by writing the correct hypothesis, namely $\vert \operatorname{Aut}(S) \vert>\aleph_0$ .","['elementary-set-theory', 'cardinals']"
4635125,Probability of even number of dice result [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Suppose we roll a dice (with 6 sides) 21 times. What are the odds of getting either 1 or 2 an even amount of times? I tried to calculate it by representing the number of times we get 1 or two as a binomial variable $B(n=21, p=\frac{1}{3})$ , And summing the probability of getting $0, 2,\ldots, 20$ and I get the probability is $0.5$ , which from what I saw in a different way of calculation is wrong. Where did my method go wrong?",['probability']
4635155,"Is there such a thing as a ""noncommutative field""?","A field $F$ is a set that is essentially a commutative group in two ways: $(F,+)$ is a commutative group with an identity element we call $0$ . $(F\setminus\{0\},\times)$ is a commutative group with an identity element we call $1$ . These two operations work together under some kind of distributive property: $a\times (b + c) = (a\times b) + (a\times c)$ This got me thinking, what if a set $G$ had these same three properties but we dropped the conditions of commutativity for one (or both) of the binary operations. Is this something that is interesting to algebraists? Does such a set not even exist? Do these kinds of object already some name that I am unaware of? Any insight on this matter would be welcomed.","['field-theory', 'group-theory', 'abstract-algebra']"
4635194,Extensions of Height-Preserving Isomorphisms in Ulm's Theorem,"I am working through the proof of Ulm's Theorem in Kaplansky's book ``Infinite Abelian Groups."" Ulm's theorem states that two countable reduced abelian $p$ -groups $G,H$ with the same Ulm invariants are isomorphic. Kaplansky's particular proof of Ulm's theorem is important because it is often used, word-for-word, to prove similar classification theorems for other kinds of structures. The key step in this proof is to extend a given height-preserving isomorphism to another height-preserving isomorphism. The part I'm confused about has nothing to do with the Ulm invariants or the fact that these groups are reduced, so please excuse me if I don't define these notions. However, central to the proof is the notion of height. We can define, inductively, $G_0 = G$ , $G_{\alpha + 1} = pG_{\alpha}$ , and $G_{\alpha}$ for limit $\alpha$ is defined in the obvious way by taking intersections. For $x \in G$ , we define the height of $x$ , denoted $h(x)$ , to be $\infty$ if $x=0$ and $\alpha$ if $x \in G_{\alpha}$ but not $G_{\alpha + 1}$ . It can be easily shown that if $h(x) \neq h(y)$ then $h(x+y) =$ min $(h(x),h(y))$ and  if $h(x) = h(y)$ then $h(x+y) \geq h(x)$ . The proof proceeds in stages. At stage $n$ , we start with a height-preserving isomorphism $V:S \rightarrow T$ between finite subgroups $S,T$ of $G,H$ respectively. We then take an arbitrary element $x$ of $G$ and extend $V$ to include $x$ in its domain. We are allowed to assume that i) $x \not \in S$ but $px \in S$ , ii) $x$ is proper with respect to $S$ , meaning that $h(x) \geq h(x+s)$ for all $s \in S$ , and iii) $h(px) \geq h(p(x+s))$ for all $s \in S$ . We then find an element $w \in H$ such that $w \not \in T$ , $pw = V(px)$ , $h(w) = h(x)$ , and $w$ is proper with respect to $T$ . Extending $V$ in the obvious way we get a map $V': \langle S,x \rangle \rightarrow \langle T,w \rangle$ , which should still be an isomorphism and height-preserving. My question is: Why is $V'$ height-preserving? Note that all heights are supposed to be calculated with respect to the larger groups $G,H$ . Now, an arbitrary element of $\langle S,x \rangle$ is of the form $s \pm nx$ . Since for any $x$ , $h(x) = h(-x)$ , it suffices to show that $V'$ preserves the height of an element of the form $s+nx$ . Moreover since $px \in S$ we may assume $n < p$ . That is, we want to show that $$ h(nx+s) = h(nw + V(s)).$$ So far, I have only been able to show that $h(x+s) = h(w+V(s))$ , which follows since $h(x+s) =$ min $(h(x), h(s))$ by the fact that $x$ is proper with respect to $S$ . There is also a proof of Ulm's theorem in Fuchs' ``Infinite Abelian Groups"", which seems to claim that this fact is enough to conclude that $V'$ is height-preserving. I don't understand this at all; there doesn't seem to be any clear connection between $h(nx+s)$ and $h(x+s)$ .  Even ignoring this advice, I am pretty stuck since height does not seem to be well-behaved with respect to addition.","['group-theory', 'abstract-algebra', 'abelian-groups']"
4635204,Testing convergence of $ \sum_{n=1}^{\infty} (-1)^n \frac{\sqrt[n]{3}-1}{\cot(\frac{1}{n})+(-1)^n} $,"I'm having trouble testing the convergence of the following series: $$
\sum_{n=1}^{\infty} (-1)^n \frac{\sqrt[n]{3}-1}{\cot(\frac{1}{n})+(-1)^n}
$$ Any advice?","['calculus', 'convergence-divergence', 'sequences-and-series']"
4635208,How to solve the given differential equation?,"Given $T$ is a function in $t$ $(x>0)$ . And $T(@t=0)=40$ . And which satisfies, $$ \frac{dt}{4}= \frac{3\cdot dT}{2} + \frac{T\cdot dt}{400+t}$$ Then find $T$ I tried putting $400+t=u$ and then, $$ \frac{du}{4}= \frac{3\cdot dT}{2} + \frac{T\cdot du}{u}$$ And multiplying by $4u$ both sides gives, $$u\cdot du= 6\cdot u\cdot dT+ 4\cdot T \cdot du$$ And by integrating by parts bay taking common $4$ gives, $$ \frac{u^2}{2}=4\cdot u \cdot T+ \int 2\cdot u \cdot dT+c$$ But now I am stuck here. Can someone help me solving this?","['integration', 'functions', 'derivatives', 'ordinary-differential-equations']"
4635225,Show that $(a_{n})_{n\geq1}$ is convergent,"We have $(a_{n})_{n\geq1}$ a sequence of real numbers such that $a_{n}^{2}>a_{n+1}^{2}$ and $a_{n+1}^{3}+a_{n}^{3}>1$ , for every $n\geq1$ . Show that $(a_{n})_{n\geq1}$ is convergent. I tried taking case by case for signs of terms and monotony but I feel like it is another method for that problem.","['inequality', 'convergence-divergence', 'sequences-and-series']"
4635302,Probability of computer chips being defective?,"Out of 10 computer chips, four are defective.
Find the following.
If three chips are randomly chosen for testing (without replacement), compute the probability that at most 2 of them are defective. Three scenarios possible, zero chips are defective. One chip is defective and lastly two chips are defective.
Using Combination Formula. $$
C(n,k)=\frac{n!}{k!(n-k)!}.
$$ defective $=4 $ non defective $= 6$ Total Ways = $\binom{10}{3}$ X= $2 $ defective chips from $3 $ selected chips. $$P(X\le2)=P(0)+P(1)+P(2)\\\frac{\binom{4}{0}\binom{6}{3}+\binom{4}{1}\binom{6}{2}+\binom{4}{2}\binom{6}{1}}{\binom{10}{3}}=.96667$$ Based on the above, would this be an appropriate way to solve the problem?","['conditional-probability', 'statistics', 'combinatorics', 'probability']"
4635315,Stochastic model of a RL circuit,"I would like to solve stochastic electric circuits numerically. I already know that I should use Milstein method to solve those systems. However, I'm not sure if the stochastic differential equations are correct, if the modeling is correct. Thus, could you give me a feedback whether or not my reasoning is correct? Consider a series RL circuit . Mathematically, it can be modeled as: \begin{equation}
\begin{cases}
\dot{I}(t) = \frac{V_L(t)}{L} \\
\dot{V_L}(t) = \dot{V}(t) - R \dot{I}(t)
\end{cases}
\end{equation} As previously stated, I want it to be a stochastic electric circuit. Thus, all parameters should be random variables. For example, I want them to satisfy: \begin{equation}
\begin{cases}
L = L_0 + e^{B_t} \\
R = R_0 + e^{B_t} \\
V(t) = V_0 \sin(120 \pi t) + \sigma B_t
\end{cases}
\end{equation} where $B_t$ stands for brownian motion and the other constants are all real positive ones. With that in mind, Rewrite both equations in differential form: \begin{equation}
\begin{cases}
dI = \frac{V_L(t)}{L}dt \\
dV_L = dV - R dI
\end{cases}
\end{equation} Substitute the parameters: \begin{equation}
\begin{cases}
dI = \frac{V_L(t)}{L_0 + e^{B_t}}dt \\
dV_L = 120 \pi V_0 \cos( 120 \pi t) dt + \sigma dB_t - \underbrace{(R_0 + e^{B_t}) dI}_{?}
\end{cases}
\end{equation} The part ""?"" feels weird to me. Because I'm used to solve such kind of systems numerically: \begin{equation}
d \vec{X} = \vec{F}(t, \vec{X}) dt + \vec{G}(t, \vec{X}) d\vec{W}
\end{equation} Surely, I could use substitution. Thus I would arrive at the last kind of equation for one dimension. But I would like to solve systems of SDEs numerically. Is this reasoning correct? Personally, I do think it is. But this field is quite new to me and sometimes tricky. Thanks P.S.: I have just read this book on stochastic calculus . I'm an electrical engineering student. EDIT The original problem is: I would like to solve stochastic electrical circuits numerically. That means I have the following things to do: model the electrical circuit using stochastic differential equations. Which I'm yet not sure if the reasoning is correct; find the equations that can entirely describe the dynamics of the circuit; set up the following equation: \begin{equation}
d \vec{X} = \vec{F}(t, \vec{X}) dt + \vec{G}(t, \vec{X}) d\vec{W}
\end{equation} use Milstein's method to solve it numerically. An attempt to answer question 1 has been done right above. There I try to model a series RL circuit. The answers to question 2 and 3 depends whether or not my attempt to answer question 1 is correct or not. That's why this question focuses on the validity of the stochastic model and its reasoning. Finally, question 4 is already solved. I know how to implement Milstein's method numerically.","['mathematical-modeling', 'ordinary-differential-equations', 'stochastic-processes', 'stochastic-differential-equations', 'numerical-methods']"
4635363,Mock PRMO question,"I find a interesting question of my PRMO mock and 2019 AMC 10A The rectangular floor of a bathroom is covered with square tiles (all of the same size). A
spider starts at one corner of the bathroom, and walks to the corner diagonally opposite. For
example, the figure below shows a 6 Ã— 8 bathroom, in which the spider touches 12 tiles on
its path. (A spider doesnâ€™t touch a tile if it just walks over the grout at the corner of a tile.)
For an $m\times n$ bathroom, how many tiles does the spider touch on its walk? My assumption is that number of tiles bugs walk through is $2* (\text{no. of columns})$ but I am unable to prove it. Can anyone please help me with solution?","['contest-math', 'recreational-mathematics', 'discrete-mathematics']"
4635416,"Median of symmetric distribution, where median is defined using inverse of CDF","Let $X$ be a symmetric random variable, that is $X$ and $-X$ have the same distribution function $F$ . Suppose that $F$ is continuous and strictly increasing in a neighborhood of $0$ . Then prove that the median $m$ of $F$ is equal to $0$ , where we define $m:=\inf\{x\in \mathbb{R}|F(x)\ge \frac{1}{2}\}$ . This definition of the median kind of annoys me. I could easily show that $\mathbb{P}(X\le 0)\ge \frac{1}{2}$ and $\mathbb{P}(X\ge 0)\ge \frac{1}{2}$ and by the usual definition of the median I would be done, but I don't know how to deal with that $\inf$ . I could only observe that my first equality implies that $m<0$ . I think that the point of the qustion is to use that $F$ is invertible on that neighborhood, but I can't make any progress.","['statistics', 'supremum-and-infimum', 'probability']"
4635461,"Is $PGL(2,\mathbb R)$ isomorphic to $SO(1,2)$?","Consider the following representation $\rho:GL(2,\mathbb R)\to GL(3,\mathbb R):G\mapsto \hat{G}$ where $G=\begin{pmatrix}
\alpha&\beta\\
\gamma&\delta
\end{pmatrix}\in GL(2,\mathbb R)$ and $\hat{G}:=
\frac{1}{\Delta}
\begin{pmatrix}
\frac{1}{2}\big(\alpha^2+\beta^2+\gamma^2+\delta^2\big)&\frac{\sqrt{2}}{2}(\alpha\gamma+\beta\delta)&-\frac{1}{2}\big(\alpha^2+\beta^2-\gamma^2-\delta^2\big)\\
\sqrt{2}\, (\alpha\beta+\gamma\delta)&\alpha\delta+\beta\gamma&-\sqrt{2}\, (\alpha\beta-\gamma\delta)\\
-\frac{1}{2}\big(\alpha^2-\beta^2+\gamma^2-\delta^2\big)&-\frac{\sqrt{2}}{2}(\alpha\gamma-\beta\delta)&\frac{1}{2}\big(\alpha^2-\beta^2-\gamma^2+\delta^2\big)
\end{pmatrix}\in GL(3,\mathbb R)$ with $\Delta=\det G=\alpha\delta-\beta\gamma$ . The kernel of $\rho$ is the normal subgroup of nonzero scalar transformations of $\mathbb R^2$ , namely $Z=\{\lambda I_2|\lambda\in\mathbb R^\times\}$ . Hence, $\rho$ induces a faithful representation $PGL(2,\mathbb R)\to GL(3,\mathbb R)$ where $PGL(2,\mathbb R)$ is the projective general linear group $PGL(2,\mathbb R)=GL(2,\mathbb R)/Z$ . Note furthermore that $\det \hat{G}=1$ and $\hat{G}^T\eta\hat{G}=\eta$ where $\eta=\begin{pmatrix}
-2&0&0\\
0&1&0\\
0&0&2
\end{pmatrix}$ , hence $\hat{G}\in SO(1,2)$ [since $\eta$ has signature $(1,2)$ ] where $SO(1,2)$ stands for the proper Lorentz group in $2+1$ dimensions. Whenever $G\in SL(2,\mathbb R)$ , $\hat{G}{}^0{}_{0}=\frac{1}{2}\big(\alpha^2+\beta^2+\gamma^2+\delta^2\big)>0$ , so that $\hat{G}\in SO^+(1,2)$ with $SO^+(1,2)$ the proper orthochronous Lorentz group in $2+1$ dimensions, so that $\rho$ reduces to the well-known isomorphism $PSL(2,\mathbb R)\to SO^+(1,2)$ , where $PSL(2,\mathbb R)=SL(2,\mathbb R)/\{-1,1\}$ is the projective special linear group in 2 dimensions. Question: Does $\rho$ induce an isomorphism $PGL(2,\mathbb R)\to SO(1,2)$ ? If yes, are there some references discussing this isomorphism?","['group-isomorphism', 'finite-groups', 'matrices', 'group-theory', 'lie-groups']"
