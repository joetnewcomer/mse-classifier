question_id,title,body,tags
2077769,How many different ways can a number N be expressed as a sum of K different positive integers?,How many different ways can a number $n \in N$ be expressed as a sum of $k$ different positive numbers?,"['number-theory', 'combinatorics', 'discrete-mathematics']"
2077772,Weaker Assumption for Convergence of the Neumann series,"I learned about a theorem which states: Let $A \in \mathbb C^{n \times n}$ . The Neumann series converges if
and only if it holds for the spectral radius that $r(A) < 1$ . In this case, $I − A$ is invertible and we have $$ \sum_{n = 0}^\infty A^n = (I - A)^{-1}.$$ I could show one implication of this theorem in a Banach space setting: Let $X$ a Banach space and $T: X \to X$ a compact operator. If the Neumann series convergences it holds that $r(T) < 1$ . But I have no idea how to show the other implication in this setting. I wondered if one can make such a theorem work for operators on Banach spaces or Hilbert spaces? Is there a analogous theorem for a suitable infinite dimensional setting (maybe compact operators)? If there is one, I would really enjoy some literature advice, because I couldn't find anything related to that using google. Thanks in advance :)","['functional-analysis', 'reference-request', 'spectral-theory', 'operator-theory']"
2077798,Limit of finite-rank operators is compact - between NON-complete spaces,"Let $E$ and $F$ be two normed vector spaces. Let $T$ be a norm-limit of finite-rank continuous operators between $E$ and $F$. Prove that $T$ is compact. The problem is that without being able to use completeness, I don't really see how to get started. Let $x_n$ be a sequence in $\overline B_E(0, 1)$, then by compactness $T_k(x_n)$ has a convergent subsequence for each $k$... then maybe I extract a subsequence $y_n$ such that $T_k(y_n)$ converges for all $k$... and then perhaps I can show that $T(y_n)$ converges. But without completeness, how do I show that a sequence converges? Normally you would construct a candidate for the limit and then check convergence, but I when I try to do that here I end up wanting to prove that other sequences converge, so I'm back where I started.","['functional-analysis', 'compact-operators']"
2077805,Confusion over $a_k = (1+\frac{1}{k})^k = e$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I'm currently just watching an MIT lecture about differentiating exps and logs and it mentions that $a_k = (1+\frac{1}{k})^k = e.$ I've seen the proof and ""understand"" that $\lim_{k \to \infty} \ln(a_k) = 1$ s0 the limit of $a_k = e$. The problem is when I read $e = \lim_{k \to \infty}  (1+\frac{1}{k})^k$,  it looks like it says $e = 1$ because I read it as $e = (1+(0))^\infty = 1$, and I'm having a hard time working out where the confusion is. Thanks.",['algebra-precalculus']
2077821,Obtaining a level-$\alpha$ likelihood ratio test for $H_0: \theta = \theta_0$ vs. $H_1: \theta \neq \theta_0$ for $f_\theta (x) = \theta x^{\theta-1}$,"Suppose I have a sequence of iid random variables $X_1, \ldots, X_n$ following the pdf: $$
f_\theta (x) = \theta x^{\theta-1}
$$ for $\theta >0$ and $0 <x<1$. I would like to obtain a level-$\alpha$ likelihood ratio test for the null hypothesis $H_0: \theta = \theta_0$ versus the two-sided alternative $H_1: \theta \neq \theta_0$ where $\theta_0$ is a known constant. MY ATTEMPT: I first construct the ratio: \begin{align}
\lambda(x) &= \frac{\sup_{\theta=\theta_0}L(\theta\mid X)}{\sup_{\theta\neq\theta_0}L(\theta\mid X)} \\[10pt]
&= \frac{\theta_0^n \left(e^{\sum \log x_i}\right)^{\theta_0-1}}{\left(\frac n {-\sum \log x_i}\right)^n \left(e^{\sum \log x_i}\right)^{\left(\frac n {-\sum \log x_i} - 1\right)}} \\[10pt] &= \left(\frac{-\theta_0 \sum \log x_i} n \right)^n e^{n+\theta_0 \sum \log x_i}
\end{align} The denominator is calculated using the MLE of $\theta$ which is $\theta^\text{MLE} = \frac n {-\sum \log x_i}$. Now, I'd like to find the likelihood ratio test, in that I would like to choose a constant $c$ such that: $$
\alpha = \sup_{\theta = \theta_0} P_\theta (\lambda(x) \leq c)
$$ Now, $-\sum \log x_i \sim \operatorname{Gamma}(n, \theta)$ but I CANNOT isolate the above equation due to the $\log$ form. What is the right answer here? Thanks!","['maximum-likelihood', 'statistics', 'hypothesis-testing', 'statistical-inference']"
2077827,How to show the following process is a local martingale but not a martingale?,"I am confronted with the same problem in this thread , which hasn't had a complete answer yet. In the sequel, we denote by $Y^T$ the stopped process $Y^T_t = Y_{T \wedge t}$ . Consider the following process: $$
 X_t = \begin{cases}
  W_{t/(1-t)}^T &\text{for } 0 \le t < 1,\\
  -1 &\text{for } 1 \le t < \infty.
 \end{cases}
$$ where $W$ is a Brownian motion and $T = \inf\{t: W_t = -1\}$ . It's easy to tell that $X$ is not a martingale. Now on wikipedia , it claims that the sequence of stopping times $\{\tau_k\}$ localizes $X$ , where $\tau_k = \inf\{t: X_t = k\} \wedge k$ . I am confused about how to prove such claim. The ""details"" on the webpage seem a bit obscure to me. On the other hand I found in the book ""Stochastic calculus and applications"" (page 133, Example 5.6.9) a similar example. The authors consider the process $X_t+1$ (where $X_t$ is defined as in our problem) and for uniformity in symbols I slightly change their proof. Unlike the example on wikipedia, they explicitly specify the filtration $\{\tilde{\mathscr{F}}_t = \mathscr{F}_{t/(1-t)}\}$ to which, as they assert, $X_t$ is local martingale. They construct the following stopping times: $$
S_n = \frac{n}{n+1}I(T \geq n) + \Big(\frac{T}{T+1}+n\Big)I(T<n)
$$ Then, they say that the following equation can be established: $$
X^{S_n}_t = W^{T \wedge n}_{t/(1-t)},
$$ which entails that $X^{S_n}$ is a $\{\tilde{\mathscr{F}_t}\}$ -martingale. To me the second approach isn't clear either. Indeed, I can't see how in their proof $\tilde{\mathscr{F}_t}$ can be defined for $t \geq 1$ . Also, $X^{S_n}_t = W^{T \wedge n}_{t/(1-t)}$ seems to hold only for $t<1$ . After doing some algebra I get $$
X^{S_n}_t = W^{T \wedge n}_{t/(1-t)} I(t<1) + W_{T \wedge n} I(t \geq 1),
$$ instead. The rightmost term in the above equation, namely $W_{T \wedge n} I(t \geq 1)$ , frustrates me in attempt to show martingale property of $X^{S_n}$ . I also considered an alternative approach: to show that for any bounded stopping time $S$ , $E[X^{S_n}_S] = 0$ . Still I failed to complete the proof. Can anyone help me with this problem?","['martingales', 'probability-theory', 'brownian-motion']"
2077830,Prove that $ \inf_{-1 < x< 1}|f^{k}| \leq \frac{2^{k(k+1)/2}k^k}{\lambda^k} $ for $|f_{-1 < x < 1}(x)| \leq 1$ and $\lambda$ is length of $x_1 - x_0$,"Let $$  |f_{-1 < x < 1}(x)| \leq 1 $$ $\lambda $ is length of conventional interval $I$ in $(-1, 1)$. Prove that on this interval: $$ \inf_{x \in I}|f^{k}| \leq \frac{2^{k(k+1)/2}k^k}{\lambda^k} $$ Where $k$ is k-th derivative, and  $f^{k-1}(x)$ for $1 ...k-1$ is continuous function. Since if function is not monotonic on this interval, it's derivative  $\inf_{x \in i}|f'(x)| = 0$, so let's consider the critical case when function and all it's derivatives until $k$ are monotonically increasing (decreasing) functions. Using Lagrange's formula for Taylor polynomial's reminder $$ f(x) = f(x_0) + f'(x_0)(x-x_0) + ... \frac{f^k(x_0)(x-x_0)^k}{k!} + \frac{f^{k+1}(\xi)(x - x_o)^{k+1}}{(k+1)!}$$ Take $x_0$ at $i$ left point, and choosing x at $i$ right point, for $k=0$ $$ f(x_o) + f'(\xi)\lambda \leq 1 $$ Since we consider increasing function, $f(x_0)$ minimum value is $-1$ $$ inf|f(x)| \leq 2/\lambda $$ This leads to formula: $$ \inf_{-1 < x< 1}|f^{k}| \leq \frac{2^{k+1}k!}{\lambda^k} $$ EDIT However, we can't just interchange derivative value of some order at point with obtained bound from above for $|\inf f^k(x)|$ Other thoughts This may be somehow related to $e$. At $n\to \infty$  $\frac{(n+1)^{(n+1)}}{(n+1)!} * \frac{n!}{n^{n}} \to e $ Take a look at different variation of Taylor polynomial reminder's formula: $$ r_k = \frac{1}{(k+1)!}f^{k+1}(\xi)(x - \xi)^k(x - x_0) $$ So if we could bound $(x-\xi)^k \leq(1 - \frac{1}{k})^k \to e^{-1}$, then $k^k$ starts to make sense, as we increase some previous term by $e * (k+1)$ So if we devide distance $ x - x_0$ by 2 at each term, starting with $\lambda$, it looks very close.",['real-analysis']
2077831,Algorithm for Random irregular polygon in between two shapes,"This is not a homework problem. It is meant as a challenge for people who really enjoy math and have time to spare. Background Info Suppose you have a 2D Cartesian coordinate system. There are three shapes: R , C , and P . R is a large rectangle. Its left side is along the vertical axis, and its bottom side is along the horizontal axis, such that its bottom-left corner is at the origin (0, 0). C is a small circle that is located somewhere inside of R . The center of C is not necessarily at R 's geometric center. C 's border cannot intersect with any part of R 's border. P is an irregular  polygon of N sides. It is a simple, convex polygon (not self-intersecting, all angles under 180 degrees). R surrounds P , and P surrounds C .  In other words, P 's corners and sides exist in the region between C 's border and R 's border. The corners of P do not necessarily touch the sides of R . Any of P 's sides may be tangent to C , but none of P 's sides may overlap inside of C . Objective Design an algorithm that generates a random variation of P 's corners. The corners of P are placed at random distances and random angles relative to C 's center. The algorithm's output is an ordered set of Cartesian coordinates, arranged by counter-clockwise position around C . You are given the following constant values: the width and height of the bounding rectangle R the radius and center of the circle C the number N of corners for polygon P the maximum distance between the center of C and any of P 's corners If this is solvable, how would you implement this algorithm? Or if this is not solvable, can you explain why not? What would need to change so that it becomes solvable?","['puzzle', 'algorithms', 'random', 'geometry']"
2077835,$f(P)=f(Q)$ implies that $P=Q$,"Let $(X,\mathbb{H})$ and $(Y,\mathbb{F})$ be two measurable spaces. Assume that $P$ and $Q$ be probability measures on $(X,\mathbb{H})$ and that $f:X\to Y$ is a $\mathbb{H}/\mathbb{F}$-measurable mapping. What are the weakest (alternatively some weak) conditions on $f$ for which
$$
f(P)=f(Q) \implies P=Q,
$$
holds? Here $f(P)$ and $f(Q)$ are push-forward measures. If we work under the assumption that $X$ and $Y$ are metric spaces and $P,Q$ are Borel measures then it is sufficient to say that $f$ is a homeomorphism, but what does this translate to when we have no topology on $X$ and $Y$ only $\sigma$-algebras.","['general-topology', 'probability', 'measure-theory', 'probability-theory']"
2077861,Minimum Covariance Between Bernoulli Variables,"Suppose $X_1,...,X_n \sim Ber(\frac{1}{2})$, and that $COV(X_i,X_j) = COV(X_k,X_l)$ for $k\neq l,j\neq i$. How small can the covariance be? My attempt: We know that $COV(X_i,X_j) = E(X_1X_2)-E(X_1)E(X_2) = E(X_1X_2)-\frac{1}{4}\geq -\frac{1}{4}$ since $Img(X_iX_j) = \{0,1\}$. And I have an example where that number is attained, simply letting $X_2 =1$ iff $X_1 = 0$. So the minimum is $-\frac{1}{4}$. How off is this?","['probability-theory', 'probability', 'covariance']"
2077867,"How to justify differentiation under the integral in calculation of $ \int_0^1\frac{\log(1+x)}{1+x^2}\,\mathrm dx $?","In this post the author uses differentiation under the integral sign to evaluate the integral $$
\int_0^1\frac{\log(1+x)}{1+x^2}\,\mathrm dx
$$ by setting up the function $$
F(a) = \int_0^1\frac{\log(1+ax)}{1+x^2}\,\mathrm dx,
$$ and computing $F'(a) = \int_0^1\frac{\partial}{\partial a}\big[\frac{\log(1+ax)}{1+x^2}\big]\,\mathrm dx$. My question is how do we formally justify the use of differentiation under the integral sign here? Some thoughts. Let's put $f(x,a) = \frac{\log(1+ax)}{1+x^2}$ for $x\in[0,1]$ and $a$ in some domain $A\subseteq\mathbf R$. I know that sufficient conditions for this procedure are that $f(x,a)$ and $\frac{\partial f}{\partial a}(x,a)$ are continuous on $[0,1]\times A$, that $F(a)$ exists for each $x\in[0,1]$, and that for each $a\in A$, $\big|\frac{\partial f}{\partial a}(x,a)\big|\le g(x)$ where $\int_0^1g(x)\,\mathrm dx<\infty$. I'm not sure how to use these ideas to put together a proof, and maybe we don't need each of these ideas to do it. So how do we justify this?","['real-analysis', 'integration']"
2077881,Choosing Open Sets in Applying the Seifert-Van Kampen Theorem to Topological space,"I am studying some algebraic topology and have come across a question that asks to find the fundamental group of a space given below.  Naturally, my inclination is to use the Seifert-Van Kampen Theorem, but I am not quite sure what to choose as my sets.  Can anyone point me in the right direction or explain what would be reasonable choices of sets? A topological space X consists of four triangles with a common side $PQ$. Their sides $AP$, $PB$, $CP$, and $PD$ are glued together (note the directions!) and their sides $CP$, $PQ$ and $QC$ are also glued together. Compute the fundamental group of $X$ in terms of generators.","['algebraic-topology', 'general-topology', 'fundamental-groups']"
2077883,Incomplete proof.,"$f$ and $g$ are functions of real variables, strictly increasing and strictly decreasing respectively on $\Bbb R$ (both surjections), I'm asked to prove that there exists at most one solution to the equation $f(x)=g(x)$. My attempt: Suppose $f(x)=g(x)=m,$ for some $x\in \Bbb R \implies \exists a,b \in \Bbb R | a\lt x\lt b$. Then : $$f(a)\lt f(x)\lt f(b)$$ and $$-g(a)\lt -g(x)\lt -g(b).$$ Hence $$f(a)-g(a)\lt f(x)-g(x)\lt f(b)-g(b).$$ So if we can prove that $f(a)-g(a)\lt 0$ and $f(b)-g(b)\gt 0$. Hence for $f(a)-g(a)=p\lt 0$ and $f(b)-g(b)=q\gt 0$, we will have $p\lt 0\lt q \equiv \top$, but I couldn't prove it. Any help would be appreciated.","['real-analysis', 'proof-writing', 'functions']"
2077884,Forming second order differential equation from given solutions using the Wronskian. (and some general help),"For 
$$y''+p(x)y'+q(x)y=0 \space\space\space\space\space\space\space\space\space\space\space 
(\star)$$
 we have
 $$W'(x)=-p(x)W(x) \space\space\space\space\space\space\space\space\space\space\space (\dagger)$$
 where $W(x)$ is the Wronskian defined as 
$W(x)=y_1y_2'-y_1'y_2$ 
for linearly indepedent solutions $y_1$ and $y_2$ of $(\star)$. The question asks us to use this to find $p(x)$ and $q(x)$ such that
 $y_1(x)=1+\cos x$ and $y_2=\sin x$. Solving $(\dagger)$ for $W(x)$ we
 have $W(x)=W_0\int^x_{x_0}-p(u) \space du$. Here's where I'm having issues; my notes have that $W_0=W(x_0)$ for
  some $x_0$, now it says we can pick $x_0$ as whatever we like and I'm
  fine with that, but what was bothering me (and the question goes onto
  ask) is that if we can pick $x_0$ as whatever we like, what happens at
  $x_0=\pi$? We would get $W=0$ which is confusing me as Abel's theorem
  has that $W(x)=0$ or $W(x)\ne0$ for all $x$- Have I misunderstood? Further, if we can have $W_0$ as whatever we like by picking 
  convenient $x_0$, we can surely just pick $W_0=1$ then putting in the 
  given $y_1$ and $y_2$ we have $(1+\cos x)\sin x+\sin x\cos x=\sin 
 2x+\sin x=\exp({\int^x_{x_0}-p(u)\space du})$ $$\implies p(x)=-\frac 
 d{dx}(\log(\sin 2x+\sin x)=-\frac{2\cos 2x+\cos x}{\sin 2x+\sin x}$$
  where does $q(x)$ come into things? Setting $q(x)=0$ with this $p(x)$
  does not work. I think I just haven't quite got my head around some of this stuff yet so an answer or general help is much appreciated. Thank you.",['ordinary-differential-equations']
2077903,Derivatives in differential geometry,"I am really attracted by the field of differential geometry which generalize analysis on euclidean spaces that I've been working with my whole life.
However by learning the field I encountered different notion of derivatives, namely : The tangent vector ( directional derivative of a function ) The ehresman connection ( derivative of a section ) The covariant derivative The exterior derivative ( derivative on the graded antisymmetric multilinear algebra of differential forms ) The exterior covariant derivative (Used to define curvature from a one form ? The Lie derivative ( derivative along curves of vector and tensors fields ) First of all I know I lack knowledge and intuition on all of them. I know the definition of them, I know their difference, to what object they act on, what info we need to use them etc. I am not looking here for definitions. I would like to know why do we need so many definition and approach to derivatives. I would hope to have a general version of derivatives that will apply to all objects but that doesn't seem to be the case here. What are the need for all of them? In what case do we need one and not the others ? Also I would really appreciate a references that would list all of thoses derivatives, minor intuition on them and also explain why and when do we have to use one or the other. PS: I have already saw most of the other responses on this forum thanks.","['derivatives', 'reference-request', 'differential-geometry']"
2077915,what are the different applications of group theory in CS? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question What are some applications of abstract algebra in computer science an undergraduate could begin exploring after a first course? Gallian's text goes into Hamming distance, coding theory, etc., I vaguely recall seeing discussions of abstract algebra in theory of computation / automata theory but what else? I'm not familiar with any applications past this. Bonus: What are some textbooks / resources that one could learn about said applications?","['applications', 'abstract-algebra', 'book-recommendation', 'computer-science']"
2077931,Is $y={(-1)^{x\overπ}+(-1)^{-x\overπ}\over 2}$ the same as $y=\cos x$?,"I have always been intrigued by the equation $y=(-1)^x$, perhaps because it is so simple yet so difficult to find information about. It's the closest thing to a trigonometric function attainable using only ""basic"" math. If you evaluate it for only whole numbers, it exhibits trigonometric behavior, going up and down the x-axis to a peak of $±1$. When we start plugging in decimal values, it gets tricky, yielding ""real"" values for fractions with odd numbers and complex values for fractions with even numbers. What do you do with this infinitesimal oscillation? A normal graphing software will not graph the equation. Wolfram Alpha, however, offers a graph, which looks like this. I, unfortunately, couldn't understand how the website arrives at these curves, when really $(-1)^{1\over3}$ should equal $-1$. Yet, I discovered how the cosine function can be expressed in terms of $y=(-1)^x$ by manipulating Euler's identity. $$e^{iπ}=-1$$
$$e^{iπx}={(-1)}^x$$
$$e^{ix}={(-1)}^{x\overπ}$$
$$e^{-ix}={(-1)}^{{-x\overπ}}$$ Since $\cos x={e^{ix}+e^{-ix}\over 2}$, adding the two above equations and dividing by two allows us to conclude that $${(-1)^{x\overπ}+(-1)^{-x\overπ}\over 2}=\cos x$$ Wolfram Alpha confirms: I have not seen this transformation anywhere . Am I the first to find it? It would be pretty cool if I were. Another stunning relationship I found is that the equation $y=(-1)^{-ix\overπ}$ yields the ordinary function $y=e^x$. This can be proven almost the exact same way. I think this is fascinating, and I am confused why so little is written about this. The only problem is, the relationships work when you derive them, but when you plug in numbers, why the graphs would look like that is baffling.","['exponential-function', 'trigonometry', 'complex-numbers']"
2077980,Nonlinear PDE related to zero Hessian,"Consider
$$f_{xx}f_{yy}-f_{xy}^2 =0$$ where $f=f(x,y)$ is a well behaved function. This PDE is the determinant of the Hessian. Solutions for $f$ are the trivial solution, $f=$constant and $f=ax+by$ for $a,b$ constant. However, $f=f(x-cy)$ for $c$ a constant also satisfies this equation. This implies there are lines of constant value along $\xi = x-cy$. That is, the curvature alongs these lines is zero which agrees with my intuition as the determinant is proportional to the Gaussian curvature. Are there more general solutions?","['differential-geometry', 'partial-differential-equations']"
2077981,Alternating series; first term is 0. Do I have a problem?,"I have an alternate series which I want to test for convergence or divergence. The series is as follows: $$\sum_{n=1}^\infty (-1)^n \frac{n^2-1}{n^3+1}$$ I know how to test this for convergence, but the first term is $0$ and so ""$n+1$"" terms are not allways smaller than $n$ terms. I have seen the answer and the series is convergent (although not absolutely, but I knew that from testing $\sum_{n=1}^\infty \frac{n^2-1}{n^3+1}$ in a previous exercise), can I just ""throw out"" the $0$ and say it doesn't matter in the grand scheme of things? The terms of the series tend to $0$, so the conditions for convergence in alternate series are satisfied except for that nasty $0$.","['real-analysis', 'sequences-and-series']"
2077990,$\sum\limits_{k=1}^{n-1}\frac{1}{\sqrt{k(n-k)}}\geq 1$ proof,"$\sum_\limits{k=1}^{n-1}\frac{1}{\sqrt{k(n-k)}}\geq 1$ for all $n\geq 2$ Basecase n=2 $\sum_\limits{k=1}^{2-1}\frac{1}{\sqrt{k(2-k)}}=1\geq 1$ Assumption $\sum_\limits{k=1}^{n-1}\frac{1}{\sqrt{k(n-k)}}\geq 1$ holds for some $n$ Claim $\sum_\limits{k=1}^{n}\frac{1}{\sqrt{k(n+1-k)}}\geq 1$ holds too Step
Assume $n$ is odd $\sum_\limits{k=1}^{n}\frac{1}{\sqrt{k(n+1-k)}}=\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{2(n-1)}}+...+\frac{1}{\sqrt{n}}$ Then, to determine which of those terms is the smallest, we want to find the maximum of $k(n+1-k)$. The deriviative would be $n+1-2k$. So $k=\frac{n+1}{2}$, that's why we assume $n$ is odd in this step. So our smallest term looks like: $\frac{1}{\sqrt{(\frac{n+1}{2})^2}}=\frac{2}{n+1}$ And since we add this term $n$ times, the sum is bounded below by $\frac{2}{n+1}n=\frac{2n}{n+1}$. Via induction it is very easy to see, that $\frac{2n}{n+1}>1$ for all n>1, which is all we care about. Now, how do I proceed for even $n$?","['radicals', 'inequality', 'summation', 'sequences-and-series']"
2077997,Unexpected Proofs Using Generating Functions,"I recently came across this beautiful proof by Erdős that uses generating functions in a unique way: Let $S = \{a_1, \cdots, a_n \}$ be a finite set of positive integers such that no two subsets of $S$ have the same sum. Prove that $$\sum_{i=1}^n \frac{1}{a_i} < 2.$$ Question: Are there any more examples of surprising or unexpected proofs using generating functions that this community is aware of? (Please refrain from posting answers that are widely known such as change making, closed form for Fibonacci, etc.) The proof of the above theorem: Proof:
Suppose $0< x < 1$. We have 
$$\prod_{i=1}^n (1 + x^{a_i}) < \sum_{i = 0}^{\infty} x^i = \frac{1}{1-x}.$$ Thus, 
$$\begin{align*}
\sum_{i=1}^n \log(1+x^{a_i}) &< - \log(1-x) \\
\sum_{i=1}^n \int_0^1 \frac{\log(1+x^{a_i})}{x} \ dx &< - \int_0^1 \frac{\log(1-x)}x \ dx .
\end{align*}$$
Putting $x^{a_i} = y$, we obtain 
$$\begin{align*}
\sum_{i=1}^{n} \frac{1}{a_i} \int_0^1 \frac{\log(1+y)}{y} \ dy < - \int_0^1 \frac{\log(1-x)}{x} \ dx 
\end{align*}$$ i.e.,
$$\sum_{i=1}^n \frac{1}{a_i} \left( \frac{\pi^2}{12} \right) < \frac{\pi^2}6.$$
Thus, $\sum_{i=1}^n \frac{1}{a_i} < 2$ and the theorem is proved.","['generating-functions', 'combinatorics', 'big-list', 'analysis']"
2078004,Prove $\sqrt{x^2-x+1}\ge\log{\left(\frac{e^x+e}{2}\right)}$,"The question: Let $x\in \Bbb{R}$,  show that
$$\sqrt{x^2-x+1}\ge\log{\left(\dfrac{e^x+e}{2}\right)}$$ My attempt: I have proven that 
$$\sqrt{x^2-x+1}\ge\dfrac{1}{2}(x+1)$$
since the right-hand side is the tangent line to $f(x)=\sqrt{x^2-x+1}$ at $x=1$. So it suffices to prove
$$\dfrac{1}{2}(x+1)\ge \log{\left(\dfrac{e^x+e}{2}\right)},$$
but I haven't managed to do this.","['inequality', 'calculus']"
2078041,"Solve $y'' + y = \sec^2 x, 0 < x < \frac{\pi}{2}$","The equation: $y'' + y = \sec^2 x, 0 < x < \frac{\pi}{2}$ I can't seem to find $u'/v'$ values. My system of equations is: $u' \sin x + v'\cos x = 0$ $u' \cos x - v'\sin x  = \sec^2 x$ I believe that $u = ∫\sec xdx$ and $v = ∫\sec x \tan xdx$, but I cannot for the life of me get these equations to solve for those integrations. Any ideas? Thank you so much in advance.","['integration', 'ordinary-differential-equations']"
2078090,Upper bound for the strict partition on K summands,"In number theory and combinatorics, a partition of a positive integer $n$, also called an integer partition, is a way of writing $n$ as a sum of positive integers. Partitions into distinct parts are sometimes called ""strict partitions"". Is there a polynomial upper bound for the strict partition on $K$ summands?","['number-theory', 'combinatorics', 'asymptotics', 'discrete-mathematics']"
2078100,A property similar to paracompacness,"Definitaion1: A family $\{A_t\}_{t\in S}$ of subsets of a
topological space $X$ is locally finite if for every point $x\in
X$ there exists a neighborhood $U$ of $x$ such that the set
$\{s\in S : U \cap A\neq \emptyset\}$ is finite. Dfinition2: A topological space $X$ is called a *-space space if $X$ is a Hausdorff space and every open cover of $X$ has
a locally finite subcover. Are *-spaces famous? Or are there any equivalence condition for
them? (Note that they are not paracompact)",['general-topology']
2078101,"Formula for adjugate of matrix: $\operatorname{adj}(s\mathbf{I}-\mathbf{A}) = \mathrm{\Delta} p(s,\mathbf{A})$","The following (roughly) is written in the Adjugate Matrix Wikipedia page : If $$ p(t)~{\stackrel {\text{def}}{=}}~\det(t\mathbf {I} -\mathbf {A} )=\sum _{i=0}^{n}p_{i}t^{i}$$ is the characteristic polynomial of the real matrix $n$-by-$n$ matrix $\mathbf A$, then $$ \operatorname{adj}(s\mathbf{I}-\mathbf{A}) = \mathrm{\Delta} p(s,\mathbf{A})$$ where $$ \mathrm {\Delta } p(s,t)~=\sum _{j=0}^{n-1}\sum _{i=0}^{n-j-1}p_{i+j+1}s^{i}t^{j} $$ is the first divided difference of $p$. Can anyone prove this, or provide a reference, please. I'm most interested in the case where $s=1$, since this then gives me a nice polynomial expansion of $\operatorname{adj}(\mathbf{I}-\mathbf{A})$ in terms of powers of $\mathbf{A}$, which is well-known already, I would guess. A proof or a reference for this special case would be welcome, too.","['matrices', 'matrix-equations', 'linear-algebra']"
2078126,"$x + \frac1y = y + \frac1z = z + \frac1x$, then value of $xyz$ is? [duplicate]","This question already has answers here : If $a+\frac1b=b+\frac1c=c+\frac1a$ for distinct $a$, $b$, $c$, how to find the value of $abc$? (7 answers) Closed 5 years ago . If $x,y,z$ are distinct positive numbers, such that $$x + \frac1y = y + \frac1z = z + \frac1x $$ then value of $xyz$ is? $$A)\ 4\quad  B)\ 3\quad C)\ 2\quad  D)\ 1$$ My attempt: 1.I equaled the equation to '$k$'. Using the AM-GM inequality, I found that $k>2$ (the equality does not hold because all are distinct). However, this, I couldn't put to much use. 2. For the next attempt, I substituted the values of $x$ and $y$ in terms of $z$ and $k$ in $xyz$. What I am getting is $xyz=zk^2 - k - z$. That's the farthest I could do..Please, help.","['algebra-precalculus', 'systems-of-equations']"
2078131,Show that $\int_0^\infty \frac{x\log(1+x^2)}{e^{2\pi x}+1}dx=\frac{19}{24} - \frac{23}{24}\log 2 - \frac12\log A$,"Any idea on how to prove the following integral $$\int_{0}^{\infty} {x\log(1+x^2)\over e^{2\pi x}+1}dx =\require{cancel} \cancel{\frac{17}{24} - \frac{23}{24}\log 2 + \frac{1}{2}\log A}={\frac{19}{24} - \frac{23}{24}\log 2 - \frac{1}{2}\log A }$$ Where $A$ is the Glaisher–Kinkelin constant. We define $$A= \lim_{n \to \infty}\frac{H(n)}{n^{n^2/2+n/2+1/12}e^{-n^2/4}}$$
Where $$H(n) = \prod^{n}_{k=1} k^k $$ I would start by $$F(z) = \int^\infty_0 \frac{2xz}{(x^2+z^2)(e^{2\pi x}+1)} \, dx$$ I know that $$\frac{2t}{e^{2\pi t}-1} =\frac{1}{\pi}-t+\frac{2t^2}{\pi}\sum_{k=1}^\infty\frac{1}{k^2+t^2} $$ But I can't find a similar one for $$\frac{1}{e^{2\pi t}+1}$$","['integration', 'definite-integrals', 'calculus']"
2078142,A question about local compactness and $\sigma$-compactness,"Let $X$ be locally compact metric space which is $\sigma$-compact also. I want to show that $X=\bigcup\limits_{n=1}^{\infty}K_n$, where $K_n$'s are compact subsets of $X$ satisfying $K_n\subset K_{n+1}^{0}$ for all $n\in \mathbb N$. I know that since $X$ is $\sigma-$compact, therefore there exists a sequence of compact subsets $(C_n)$ of $X$ such that $X=\bigcup\limits_{n=1}^{\infty}C_n$. I am not getting any idea how to construct $K_n$'s. Please help!","['general-topology', 'metric-spaces', 'compactness']"
2078187,Why is $a^{x+y}=a^xa^y$,"After reading chapter 1 of Rudin's Principles of mathematical analysis, I started working on the exercises. Exercise 6 was a big surprise since exercises 1-5 had all been very simple, whereas 6 had me stumped for a ver long time until I read solutions online. The problem is: For integers $m,n,p,q$ let $r=m/n=p/q$, prove that for positive $b$
$$(b^m)^{1/n}=(b^p)^{1/q}$$
Then show that for rationals $r,s$
$$b^{r+s}=b^rb^s$$
If $$b^x=\sup \{ b^t : t\le x,t\in \mathbb{Q}\}$$
Show that for reals $x,y$
$$b^{x+y}=b^xb^y$$
After reading the solutions I think I could have solved the first and second parts of the question, but for the last part, the solution made use of several propositions, corollaries and had to prove a lemma before starting the demonstration. This made me a little suspicious, since exercise 7 asked for the proof of several of these facts. So I was wondering if there are alternate proofs, and if there are, what is the simplest you know? This is the link to the solutions I am talking about","['real-analysis', 'proof-writing']"
2078197,"Angles in triangles are in AP, find:$\dfrac{a}{c}\sin 2A+\dfrac{c}{a}\sin 2C$","If angles of a $\triangle{ABC}$ are in A.P. then find the value of : $$\dfrac{a}{c}\sin 2A+\dfrac{c}{a}\sin 2C$$ where $a,b,c$ are sides and $A,B,C$ are angles. My attempts: Let $\angle A =x-d , \angle B=x, \angle C=x+d\ \implies \angle B=60^{o}$. So one possibility is following: $\angle A=30^{o}$ $\angle C=90^{o}$ By sine law: $\dfrac{a}{\sin A}=\dfrac{b}{\sin B}=\dfrac{c}{\sin C}=2R$ $\implies \dfrac{a}{c}\sin 2A+\dfrac{c}{a}\sin 2C=\dfrac{\sin^{2}A \sin 2A+ \sin^{2} C \sin 2C}{\sin A \sin C} $ By value of $\angle A\ ,\angle C$. Answer comes to be $\dfrac{\sqrt{3}}{4}$. Is it correct, if not then where I go wrong? And I don't want to prove this by an assumption that $\angle A=30, \angle C=90$. How to do this without assuming angles. Edit as suggested by @ rohan : original question is follows: If angles of a $\triangle{ABC}$ are in A.P. then find the value of : $$\dfrac{a}{c}\sin 2C+\dfrac{c}{a}\sin 2A$$ where $a,b,c$ are sides and $A,B,C$ are angles. I earnestly apologise for wasting your precious time, it was not deliberate.","['triangles', 'trigonometry', 'sequences-and-series', 'geometry']"
2078213,Understanding Theorem 17.2 in Munkres,"Essentially the only problem I'm having with this whole proof is the assertion that $A = Y \cap C \implies Y - A = Y \cap (X - C)$, I just can't seem to see how the implication holds true. I tried to derive it from elementary set theory operations but failed. Any hint on how to prove this assertion, that $A = Y \cap C \implies Y - A = Y \cap (X - C)$?","['general-topology', 'elementary-set-theory', 'proof-explanation']"
2078218,"Prob. 11 (d), Chap. 3 in Baby Rudin: Given $a_n > 0$, is this condition also sufficient for divergence of $\sum \frac{a_n}{1+na_n}$?","Here's Prob. 11 (d), Chap. 3 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $a_n > 0$ and that the series $\sum a_n$ is divergent. Then what can be said about the convergence of $$\sum \frac{a_n}{1+na_n}?$$ I know that if $\left\{ n a_n \right\}$ is bounded above or has a positive lower bound, then this series diverges. Does the converse hold as well? Suppose that $\left\{ n a_n \right\}$ is neither bounded above nor has a positive lower bound. Then there is a subsequence $\left\{ n_k a_{n_k} \right\}$ such that $$n_k a_{n_k} \geq k$$ for all $k$. And, there is a subsequence $\left\{ m_r a_{m_r} \right\}$ such that $$m_r a_{m_r} < \frac{1}{r}$$ for all $r$. What next?","['real-analysis', 'sequences-and-series', 'absolute-convergence', 'convergence-divergence', 'analysis']"
2078232,Does there exist an exponential function not vanishing at $-\infty?$,I am searching for a non-constant function $f:\mathbb{R}\rightarrow \mathbb{R}$ with the following properties: 1) $f(a+b)=f(a)f(b)$ 2) $\lim\limits_{x\rightarrow -\infty} f(x) = 1$. Is it possible to find such a function or is there a reason why such a function can not exist?,"['real-analysis', 'exponential-function', 'analysis', 'functions']"
2078251,A property of an open cover,"Let $\{A_i\}_{i\in S}$ be an open cover for a regular topological space $X$.
 The family $\{A_i\}_{i\in S}$ is called locally finite if for every point $x\in X$ there exists a neighborhood $U$ of $x$ such that the set
$\{s\in S : U \cap A_s\neq \emptyset\}$ is finite. Let the family $\{A_i\}_{i\in S}$ have the property that every point of $X$ is contained in only finitely many of $A_i'$s. Can we deduce that the family $\{A_i\}_{i\in S}$ is locally finite?",['general-topology']
2078264,Is there such a thing as an infinity vector?,"I've been see the following question on group theory: Let $p$ be a prime, and let $G = SL2(p)$ be the group of $2 \times 2$ matrices of determinant $1$ with entries in the ﬁeld $F(p)$ of integers $\mod p$. (i) Deﬁne the action of $G$ on $X = F(p) \cup \{ \infty \}$ by Mobius transformations. [You need not show that it is a group action.] State the orbit-stabiliser theorem. Determine the orbit of $\infty$ and the stabiliser of $\infty$. Hence compute the order of $SL2(p)$. I know matrices are isomorphic to Mobius maps, but not how the action of a mobius map can be used to define the action of a matrix (I don't really know what this part means to be honest).
I tried the next part, but wasnn't sure whether the consider the vector $(\infty,\infty)$, $(\infty,a)$ or $(b,\infty)$ $(a,b \in F(p))$.
Any help would be greatly appreciated!! (Sorry the question title isn't very related to the question, I just didn't know what to put specifically!)","['matrices', 'infinity', 'group-theory', 'mobius-transformation']"
2078269,Sequence of continuous linear functionals over a sequence of Hilbert spaces,"Let $H_n$ be a sequence of complex Hilbert spaces such that $H_{n+1}\subsetneq H_n$ and $\bigcap_{n=1}^\infty H_n=\{v_0\}$ Let $T_1:H_1\to \mathbb C$ be a continuous linear functional such that $T_1(v_0)=0$ Let, for each $n \in \mathbb N$, $T_n:H_n\to\mathbb C$ the continuous linear functionals obtained by restriction of $T_1$ to $H_n$ I would like to know if $\lim_{n\to\infty}\lVert T_n \rVert = 0$ Thanks for any suggestion.","['functional-analysis', 'sequences-and-series']"
2078273,Does $\sum \frac{a_n}{1+n a_n}$ converge if $a_n = \frac{1}{\sqrt{n}}$ if $n$ is a perfect square and $a_n = \frac{1}{n^2}$ otherwise?,"Given the sequence $\left\{ a_n \right\}$, where 
$$a_n = \begin{cases} \frac{1}{\sqrt{n}} \ \mbox{ if } n \mbox{ is a perfect square} \\ \frac{1}{n^2} \ \mbox{ otherwise}, \end{cases}$$ 
does the series $$\sum \frac{a_n}{1 + n a_n}$$ converge or diverge? My effort: If $n = m^2$, then we have 
$$\frac{a_n}{1+na_n} = \frac{ \frac{1}{m} }{ 1 + m^2 \frac{1}{m} } = \frac{1}{m(m+1)} = \frac{1}{\sqrt{n} (1 + \sqrt{n})}, $$ 
and otherwise 
$$ \frac{a_n}{1+na_n} = \frac{ \frac{1}{n^2}}{1+ n \frac{1}{n^2}} = \frac{1}{n^2 + n}. $$ 
Thus we have 
$$ \frac{a_n}{1+na_n} = \begin{cases} \frac{1}{\sqrt{n} (1 + \sqrt{n})} \ \mbox{ if } n \mbox{ is a perfect square} \\ \frac{1}{n( 1 + n)} \ \mbox{ otherwise}. \end{cases} $$ What next? How to proceed from here? An afterthought: If $n$ is a perfect square, then we note that 
$$\frac{a_n}{1+na_n} = \frac{1}{\sqrt{n}(1+ \sqrt{n})} \geq \frac{1}{2n},$$","['real-analysis', 'sequences-and-series', 'calculus', 'convergence-divergence', 'analysis']"
2078289,Permutations vs. Combinations,What are the word modifiers in order to distinguish a given if it is a permutation or a combination? Or simply describe what is permutation and combination.,"['descriptive-statistics', 'statistics']"
2078291,Simple limit. Used L'Hôpital's rule. Didn't work.,"$\lim_{x\rightarrow 0}f(x)$ $f(x)=\frac{\exp (\arcsin \left (x  \right ))-\exp (\sin \left (x  \right ))}{\exp (\arctan \left (x  \right ))-\exp (\tan \left (x  \right ))}$ I tried the L'Hôpital's rule as mentioned in the title and replaced ($\arcsin x$,$\arctan x$,$\sin x$,$\tan x$) with $\sim_{0}$ $x$ but still have an  indeterminate form. Any hints please ?","['exponential-function', 'calculus', 'limits']"
2078305,A question about $L^p$ spaces,"If $1 \leqslant p_1,p_2 \leqslant \infty$ and $L^{p_1}(\mathbb{R}) \subseteq L^{p_2}(\mathbb{R})$ ,prove that $p_1=p_2$ Can some one give me a hint to solve this? Thank you in advance!","['lp-spaces', 'lebesgue-integral', 'measure-theory']"
2078312,Roll a dice infinite times BUT pay $1 to play - strategy?,"I am stuck with a question related to the optimal strategy of rolling a dice. Its an extension of the problem ""Roll a dice and take either the amount on the dice or re-roll (a maximum of twice)"". Heres the link to the basic version and solution: The expected payoff of a dice game So the idea is you roll a 6 sided dice, and you can EITHER take the money shown on the dice, OR pay \$$1$ and play again . You can now re-roll as many times as you like. What is the optimal strategy and whats the fair value of the game? I tried to use the same approach by working backwards as the basic case above, but after a few iterations, it seems to be never ending. This doesn't make sense intuitively, because after 6 rolls, you have spent \$$6$ to play and can only win a maximum of \$$6$ on your next roll, so it would never be optimal to roll more than 6 times!",['probability']
2078331,Is this solvable (geometrical image included)?,"This image has been floating in the net for a while now: with some approaches to ""clarify"" it: but is there really an analytical way of solving it? The proportions and are of course out of scale (per Gimp, magic wand and histogram). Being new here, please add appropriate tags if needed.",['geometry']
2078365,Non-probabilistic analogue of the Second Borel-Cantelli lemma,"The first Borel-Cantelli Lemma says that if we have events $E_i$ and $\sum_iP(E_i)<\infty$ then the probability infinitely many events occur is 0. The second is a partial converse saying if the events are independent and $\sum_i P(E_i) = \infty$ than the probability infinitely many events occur is 1. It's clear that the first has an analogue on non-probability measure spaces, namely that for a sequence of measurable sets $A_i$, if $\sum_i \mu(A_i) < \infty$ then $\mu(\bigcap_{n=1}^\infty \bigcup_{k=n}^\infty A_k) = 0.$ However, because of the independence assumption, the analogue of the second seems much less natural since I've never seen independence formulated outside probability theory. On the other hand, I've never really studied analysis too deeply. My question is Is the analogue of the first Borel-Cantelli lemma used frequently in non-probabilistic contexts? Is there another nice interpretation of it that has nothing to do with probability (other than just a mechanical interpretation of the proof)? Does the purely measure-theoretical analogue of independence ever come up naturally outside probability theory? If so, does the second Borel-Cantelli lemma have utility outside probability theory?","['real-analysis', 'probability-theory', 'borel-cantelli-lemmas', 'measure-theory', 'soft-question']"
2078401,Determine the spectrum of a shift-operator,"Let $\{a_n\}_{n=1}^{\infty} \subset \mathbb C$ be a sequence such that $|a_n|=r$ for all $n \geq 1$ and $r \geq 0$ Define $T: \ell^2 \to \ell^2$ by $T(x_1,x_2,x_3,...) = (0,a_1x_1,a_2x_2,....) $ with $(x_1,x_2,....) \in l^2$ Determine the spectrum of T. The spectrum of an operator is defined as $\{\lambda \in \mathbb{C}: T-\lambda I  \ \text{is not invertible}\}$ I know how to calculate the eigenvalues of a matrix, but here I have really no idea to start. Can anyone help?","['functional-analysis', 'operator-theory']"
2078408,Deriving $\sin{ix}=i\sinh{x}$,"Derive $\sin{ix}=i\sinh{x}$ from $(5)$. What is $\sin{i}$?
  $$\cos{x}=\frac{1}{2}\left(e^{ix}+e^{-ix}\right)\quad\text{and}\quad\sin{x}=\frac{1}{2i}\left(e^{ix}-e^{-ix}\right)\tag 5$$ We have
$$\begin{align}
\sin{ix}&=\frac{1}{2i}\left(e^{i^2x}-e^{-i^2x}\right)\\
&=\frac{1}{2i}\left(e^{-x}-e^x\right)\\
&=-\frac{\sinh{x}}{i}
\end{align}$$
It does not look like what we are deriving. What went wrong?","['hyperbolic-functions', 'trigonometry']"
2078418,minimum value of $\displaystyle f(x) = \frac{(1+x)^{0.8}}{1+x^{0.8}}$ without derivative,"minimum value of $\displaystyle f(x) = \frac{(1+x)^{0.8}}{1+x^{0.8}},x\geq 0$ without derivative Binomial expansion of $\displaystyle (1+x)^{0.8} = 1+0.8 x-\frac{(0.8\cdot (0.8-1)x^2)}{2}+\cdots $ but from above does not get anything could some help me with this","['inequality', 'optimization', 'functions', 'algebra-precalculus', 'fractions']"
2078432,Zariski cotangent space of cusp at origin and at generic point,"I want to explicitly compute the Zariski cotangent spaces of the cusp $X=Z(x^3-y^2)\subset \mathbb{C}^2$. I can work with the definitions, but I have no idea how to actually compute this (this is a problem I tend to have...). So by definition we consider the ring 
$$A=\mathbb{C}[x,y]/\langle x^3-y^2\rangle$$
And then some maximal ideal $\mathfrak{n}=\langle \overline{x-1},\overline{y-1}\rangle\subset A$. Then we compute
$$A_{\mathfrak{n}}=\left( \mathbb{C}[x,y]/\langle x^3-y^2\rangle \right)_{\langle \overline{x-1},\overline{y-1}\rangle }$$
and consider its maximal ideal $\mathfrak{m}=\mathfrak{n}A_{\mathfrak{n}}$, and finally we compute $\mathfrak{m}/\mathfrak{m}^2$. Frankly I have no idea how to go about this. I know that
$$A\cong \mathbb{C}[t^2,t^3]=A'$$
and it seems that under this isomorphism $\mathfrak{n}$ would be send to $\mathfrak{n}'=\langle t^2-1,t^3-1\rangle$. Now I still don't see how to compute $A'_{\mathfrak{n}'}$, but this might just be because I am bad at this... Any help would be appreciated.","['algebraic-geometry', 'commutative-algebra']"
2078481,Dense subspaces of Hilbert space with trivial intersection,"I am looking for an example of a Hilbert space $\mathcal H$ and dense subspaces $U_1,U_2\subset \mathcal H$ such that $U_1\cap U_2 = \{0\}$. The best I have achieved is one-dimensional intersection, take $L^2([0,1])$ as the Hilbert space and for $U_1$ the simple functions and for $U_2$ the polynomials.","['functional-analysis', 'hilbert-spaces']"
2078486,Find $5x^3+11y^3+13z^3=0$ integer solutions,"Problem statement:
Find all integer solutions of equation $5x^3+11y^3+13z^3=0$. My attempt:
I tried to reason that if there's a solution, then, since we have odd powers of variables we have 4 different scenarios to explore: 1) $5x^3 = 11y^3 + 13z^3$ 2) $11y^3 = 5x^3 + 13z^3$ 3) $13z^3 = 5x^3 + 11y^3$ 4) $13z^3 = 5x^3 = 11y^3$. Since there are constant numbers $13$, $5$, and $11$ before variables, the only integer that would make all three equal would be $\boxed{0}$ While my road of thought is leading me to something, I think that I am doing something in a little bit too complicated way. A few hints: This question is from a chapter about invariance. The problem is asked just after $x^3 - 3y^3 - 9z^3 = 0$, which is solved when you show that, if $(x, y, z)$ is a solution, then $(\frac{x}{3^n}, \frac{y}{3^n}, \frac{z}{3^n})$, where $n$ is any natural number, is also a solution. So, it seems that this equation should be solved in a similar manner. It should not be very complicated - it is just beginning of a high school level book.","['number-theory', 'invariance', 'elementary-number-theory']"
2078494,"Show that $\mathbb{Q}(a)/\mathbb{Q}$ is normal, where $a$ is a root of the irreducible polynomial $x^3-3x-1$","We have that $E=\mathbb{Q}(a)$, where $a\in \mathbb{C}$ is a root of the irreducible polynomial $x^3-3x-1\in \mathbb{Q}[x]$. I want to show that $E/\mathbb{Q}$ is normal. I have done the following: Let $b\in E$. 
A basis of the extension is $1, a, a^2$. So, $b$ can be written as $$b=q_0+q_1a+q_2a^2$$ We have to find the minimal irreducible polynomial of $b$ over $\mathbb{Q}$ and compute the other roots to check if they are in $E$, or not? Since $[E:\mathbb{Q}]=3$, we have that $\deg m(b,\mathbb{Q})\leq 3$. So, the general form of that polynomial is $Ax^3+Bx^2+Cx+D$. So, do we have to replace $x$ with $b=q_0+q_1a+q_2a^2$, compute that polynomial, knowing that $a^3=3a+1$, and find the other roots? Or is there an other way to show that?","['irreducible-polynomials', 'abstract-algebra', 'extension-field']"
2078531,First Chern class of toric manifolds,"I have been reading a Mirror Symmetry monograph , and its physical arguments seem to imply that all toric manifolds have semipositive-definite first Chern class . Is this true, and if yes, how does one show this rigorously? Solution from physics: In Chapter 7 (page 102) of the Mirror Symmetry monograph, it is said that ""Toric varieties can be described as the set of ground states of an appropriately gauged linear sigma model (GLSM)"". However, from Chapters 14 and 15 of the same book, it can be deduced that the GLSM can only provide a description of toric manifolds $X$ with $c_1(X)\geq0$ (the reason is roughly that nonlinear sigma models for $X$ with $c_1(X)<0$ are not well-defined). Thank you.","['complex-geometry', 'algebraic-geometry', 'mathematical-physics', 'toric-geometry', 'mirror-symmetry']"
2078534,Proof that $\mathbb{E}(S_n ^2) = \sigma ^2$,"If $X_i$ iid with variance $\sigma$ then I want to prove that $S_n^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i -\bar X_n )^2$ is an unbiased estimate of the variance $\sigma$. So here I go: \begin{equation}
\begin{aligned}
\mathbb{E}(S_n ^2)&= \frac{1}{n-1}\sum_{i=1} ^{n} \mathbb{E}(X_i -\bar X_n )^2\\
&= \frac{1}{n-1}\sum_{i=1} ^{n} \mathbb{E}(X_i^2  -2X_i\bar X_n + \bar X_n^2) \\
&= \frac{1}{n-1}\sum_{i=1} ^{n}  \mathbb{E}(X_i^2  -\frac{2}{n}X_i ^2  -\frac{2}{n}\sum_{j\neq i} X_i X_j + \bar X_n^2)\\
&= \frac{1}{n-1}\left\{ (n-2)\mathbb{E}(X_1 ^2) -\frac{2}{n}\sum_{i=1} ^{n}\sum_{j\neq i}\mathbb{E}(X_i)\mathbb{E}(X_j) + \sigma ^2 + \mathbb{E}(X_1)^2 \right\}
\end{aligned}
\end{equation}
where I used the fact that for $X_i$, $X_j$ independent we have $\mathbb{E}(X_i X_j) = \mathbb{E}(X_i)\mathbb{E}X_j)$ and that $\mathbb{E}(\bar X_n ^2) = \frac{\sigma^2 + \mathbb{E}(X_1)^2}{n}$. Finally, after rearranging the first and last terms: \begin{equation}
\begin{aligned}
\mathbb{E}(S_n ^2) &=  \frac{1}{n-1}\left\{ (n-1)\mathbb{E}(X_1 ^2) -\frac{2}{n}n(n-1)\mathbb{E}(X_1)^2  \right\}\\
&= \mathbb{E}(X_1 ^2) -2\mathbb{E}(X_1)^2\\
&\neq \mathbb{E}(X_1 ^2) -\mathbb{E}(X_1)^2
\end{aligned}
\end{equation} I'm off by a factor $2$. Can someone help me point out my mistake?","['statistics', 'estimation', 'variance']"
2078552,"Prove $0<a_k\in \mathbb R$ and $\prod\limits_{k=1}^n a_k =1$, then $\prod\limits_{k=1}^n (1+a_k) \ge 2^n$ [duplicate]","This question already has answers here : What is the minimum value of $(1 + a_1)(1 + a_2). . .(1 + a_n)$? (4 answers) Closed 7 years ago . Prove:$$0<a_k\in \mathbb R\quad and\quad\prod_{k=1}^n a_k =1,\quad then\quad \prod_{k=1}^n (1+a_k) \ge 2^n$$ (*) I guess that the minimum of $\prod\limits_{k=1}^n (1+a_k)$ happens when all $a_k$'s are $1$, but can't prove it. Thanks.","['algebra-precalculus', 'products', 'inequality']"
2078588,Integer programming formulation of Takuzu,"A Takuzu is a logic-based number puzzle whose objective is to fill a (usually $10 \times 10$) grid with ones and zeros such that the following conditions are satisfied: Each row and each column have an equal number of ones and zeros. Each row and each column have a maximum of $2$ equal adjacent numbers. All rows are different and all columns are different (but a row can be the same as a column). I am interested in an integer programming (IP) formulation to solve such a puzzle. Denote $A$ as the $2n \times 2n$ matrix representing the grid of such a puzzle (let's assume a square grid) and let $\mathcal{N} = \{1,\ldots,2n\}$ . The IP formulation is then given by \begin{align}
\min 1 \\
\text{s.t.} \quad \sum_{j \in \mathcal{N}}a_{ij} & = n \quad \forall \ i \in \mathcal{N} \tag{1.1} \\
\sum_{i \in \mathcal{N}}a_{ij} & = n \quad \forall \ j \in \mathcal{N} \tag{1.2}  \\
\sum_{j \in \mathcal{N}} |a_{ij}-a_{kj}| & \geq 1 \quad \forall \ i \in \mathcal{N}, \forall \ k \in \mathcal{N}, i \neq k \tag{3.1} \\
\sum_{i \in \mathcal{N}} |a_{ij}-a_{ik}| & \geq 1 \quad \forall \ j \in \mathcal{N}, \forall \ k \in \mathcal{N}, j \neq k \tag{3.2} \\
a_{ij} & \in \mathbb{B} \quad \forall \ i \in \mathcal{N}, \forall \ j \in \mathcal{N}
\end{align} I have two main questions: 1 : Is the formulation valid? Any tips regarding notation or solvability? 2 : How do I enforce the 2nd condition in my program?","['integer-programming', 'linear-programming', 'combinatorics', 'recreational-mathematics', 'discrete-mathematics']"
2078594,How many ways are there to color vertexes of a $n\times n$ square that in every $1\times 1$ squares we should have $2$ blue and $2$ red vertexes?,How many ways are there to color vertexes of a $n\times n$ square that in every $1\times 1$ squares we should have $2$ blue and $2$ red vertexes? My attempt :I had found an answer but it is not in the book because for the first corner we can choose two vertexes to color blue which gives a multiply of $3$ but the answer in the book is $2(2^{n+1}-1)$.Any hints?,['combinatorics']
2078599,Leibniz formula for determinants,"I don't fully understand the formula for Leibniz formula of determinant. It is in my book as:\
$\ \Sigma sgn(\pi )A_{1\pi (1)}...A_{n\pi (n)}$ I kind of understand that it has to do with eradicating repeated rows and the amount of rows that have been interchanged but that is about the extent of it. An example of a 3x3 matrix would be great, thanks.","['matrices', 'linear-algebra', 'determinant']"
2078607,Prove that $\left(\left\{\sqrt{x^4+18x^2+162}\right\}\right)$ is a strictly decreasing sequence,"Prove that $\left(\left\{\sqrt{x^4+18x^2+162}\right\}\right)$ is a strictly decreasing sequence where $x \in \mathbb{Z}^+$ after some point. Note: $\{r\}$ denotes the fractional part of a real number $r$. In order to show this, we must show that for all $x$ we have $$\left\{\sqrt{x^4+18x^2+162}\right\} > \left\{\sqrt{(x+1)^4+18(x+1)^2+162}\right\}.$$ How do we show this?",['number-theory']
2078609,What's the fairest turn sequence for n players?,"For two players, in some sense, the fairest possible turn order is the Thue-Morse-sequence: 01101001100101101001011001101001.... It arises by taking turns taking turns taking..., so: 01|01|01|01|01|01|01|01 ->  
01 10|01 10|01 10|01 10 ->  
01 10 10 01|01 10 10 01 ->  
01 10 10 01 10 01 01 10 ... And, among many other nice properties, it happens to have the advantage the first player gets at turn 1 to converge away to 0 over time.
This idea, the fairness of a given turn-order, is my sole concern here. Now there already is a question asking what the n-player generalization of the Thue-Morse-sequence is: https://math.stackexchange.com/a/1549434/49989 However, all it mentions is how to generate these generalized sequences. Not whether, or in what sense they share the same properites. For instance, from that answer, here is the 3-player variant: 0, 012, 012120201, 012120201120201012201012120 arrived at by starting with 0 and using the following replacement rules: (0→012),(1→120),(2→201) Since our set now contains 3 elements which can be arranged in 6 ways, there are, however, more sequences that might ultimately be equivalent. For instance, what about the following? Is it at a disadvantage over the sequence given by the answer above? (0→021),(1→102),(2→210)
0, 021, 021210102, 021210102210102021102021210 As far as I can tell, no: All it effectively did was to switch around the players with the label 1 and 2 respectively, leaving alone 0. However, if you can pick either sequence, you could once again go with a take turns taking turns approach: (0→ 012|021 ),(1→ 120|102),(2→ 201|210) Though at this point I'm not sure which variant to take here: Either you'd just take the thue-morse sequence per-step or per-digit to determine which one of the two to pic, or perhaps each digit should have its own thue-morse sequence attached to it to pick which one to go first. Here are the first couple steps in either case (these are hand-computed so hopefully I made no mistake): thue-morse: 01101001100101101001011001101001...
3 player thue-morse per-step
0, 012, 021102210, 021210102102021210210102021,...
3 player thue-morse per-bit
0, 012, 012102210, 012102210120021210210120012,...
3 player thue-morse per-player
0, 012, 012120201, 012120201102210021210021102,... So it's probably hard to tell from those sequences what exactly I did for each of them, so let's be a bit clearer: for per-step, I simply generated each partial sequence with the substitution rules where I alternated what rule to pick per step. 
0 -> 012 because the first bit of the thue-morse sequence is 0 012 -> 021102210 because the second digit is 1 021102210 -> 021210102102021210210102021 because the third bit is 1, the next step would go with the first rule set again since its bit is 0. for per-bit, I simply picked what rule to apply based on what digit we are looking at. 0 is the first digit. The first digit in the Thue-Morse-sequence is 0. Therefore, use the 1st substitution rule: |0| -> |012|. 1 is the second digit. The second digit in the Thue-Morse-sequence is 1.  Therefore, use the 2nd substitution rule: 0|1|2 -> 012|102|. 2 is the third digit. The third digit in the Thue-Morse sequence is 1. Therefore, use the 2nd substitution rule: 01|2|102 -> 012102|201|... for per-player I kept track of the occurrence of each digit individually, the 1st 0, 1, and 2 respectively would use the 1st substitution rule, the 2nd and 3rd (each corresponding to 1) the 2nd respectively, the 4th occurance the 1st again... Though except for the 1st case,  I didn't use the rule for substitution but rather as recursion, so I just tacked the next three digits to the end of the sequence based on the current digit and the current rule. And perhaps there are other fairly natural rules (the most natural-seeming one to me is the per-digit rule) Now, back to my question. As said, I'd like to know the fairest possible turn order for n players. Does the (comparatively simple) method in the answer I linked fulfill that criterion, or would that price go to a different sequence, perhaps one of my suggested alternatives? If the latter then I assume a similar generalization would have to be taken for higher player counts: For four players 0123 there could be six plausible substitution rules which could either be fixed or chosen in an alternating pattern that combines the best 3-player sequence and the thue-morse sequence. And in general, for $n$ players, the $\left(n-1\right)!$ choices of presumably fair sequences could all be shuffled up further for each of the n-1 fair sequences that come before them.","['fair-division', 'sequences-and-series', 'combinatorial-game-theory']"
2078610,Borel Cantelli Lemma Implies Almost Sure Convergence,"I am struggling to see the exact nitty gritty details of how to prove the following theorem: Borel Cantelli Implies Almost Sure Convergence I am specifically getting caught up in the step of trying to show that: \begin{equation*}
\mathbb{P}(\limsup A_n(\epsilon)) =0 \qquad\Longrightarrow \qquad \mathbb{P}(\lim X_n =X) = 1
\end{equation*} I can intuitively see the argument using the ""infinitely often"" definition of the limsup of the sequence of events ...but I would like to show it using $\epsilon$'s with the formal definition of convergence in limits + the definition of:
\begin{equation*}
\limsup A_n(\epsilon) = \bigcap_{n=1}^\infty \bigcup_{i=1}^n A_i(\epsilon)
\end{equation*} Thanks for any help in advanced!","['borel-cantelli-lemmas', 'probability-theory', 'measure-theory']"
2078615,Find $\lim \limits_{x\to 0}\frac{\log\left(\cos x\right)}{x^2}$ without L'Hopital,"$$\lim_{x\to 0}\frac{\log\left(\cos x\right)}{x^2}$$ I've been triyng to: show $\displaystyle -\frac{\pi}{2}<x<\frac{\pi}{2}\Rightarrow\frac{\log\left(\cos x\right)}{x^2}<-\frac{1}{2}$ find a function so that $\displaystyle f(x)<\frac{\log\left(\cos x\right)}{x^2}$ and $\displaystyle \lim\limits_{x\to0}f(x) = -\frac{1}{2}$ And then apply the squeeze principle, but haven't managed any of these.","['limits', 'logarithms', 'trigonometry', 'calculus', 'limits-without-lhopital']"
2078621,Application of the Dominated convergence theorem for series,"The following exercise is taken from a Calculus I course exam: Let $k\in \mathbb N$. Prove the existence of $$x = \lim_{k\to \infty}\sum_{n=1}^{\infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right)$$
  and calculate $x$. This is an obvious application of the Dominated convergence theorem for infinite series. I think I found a solution, however I am not sure if it is entirely correct: Step 1: We have to show that for all $n\in \mathbb N$ the limit $\lim_{k\to \infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right)$ exists. This holds since, for all $n\in \mathbb N$: $$\lim_{k\to \infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \lim_{k\to \infty}\exp(-n) \cdot \lim_{k \to \infty}\exp\left(\frac{k}{n}e^{-\frac{k}{n}}\right) =  \exp(-n),$$
having used that the exponential function grows faster than every polynomial. Step2: We have to show the existence of a majorant sequence. Since $\frac{k}{n}e^{-\frac{k}{n}}$ converges towards $0$ for all $n\in \mathbb N$ we can choose $k\in \mathbb N$ so big that $\frac{k}{n}e^{-\frac{k}{n}} \leq \frac{1}{2}$ (or any other finite number). It thereby follows that $$\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \exp(-n) \cdot \exp\left(\frac{k}{n}e^{-\frac{k}{n}}\right) \leq \exp(-n) \cdot \exp\left(\frac{1}{2}\right) =: b_n$$ 
and $\sum b_n$ obviously converges (geometric series). Now we can apply the Dominated convergence theorem yielding the existence of $x$. Furthermore, we get $$x = \lim_{k\to \infty}\sum_{n=1}^{\infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \sum_{n=1}^{\infty}\lim_{k\to \infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \sum_{n=1}^{\infty}\exp(-n) \quad\quad\quad= \frac{1}{e}\sum_{n=0}^{\infty}\exp(-n) = \frac{1}{e-1}$$ Is my approach correct or does it have flaws? Step 2 is the one I am not happy with but don't know how to do it better. Thanks in advance for your feedback!","['real-analysis', 'exponential-function', 'calculus', 'convergence-divergence', 'analysis']"
2078628,Writing an explicit function (hard),"I need to explicitly write a function as follows: For any set C with at least 2 (different) objects, find a function $F ∈ (C → (C → C)) → (C → C)$ such that $∀ f ∈ C → (C → C).F (f) \notin Imf$  . Any ideas how to approach this? Thanks!","['elementary-set-theory', 'functions']"
2078632,Least square fitting multiple values,"This question shows how to least square fit a function to some data points: Creating a function incrementally That question/answer show how to fit a function of the form
$y=f(x)$ using $(x,y)$ pairs of data points. That can be generalized to surfaces and hyper-volumes by instead of having powers of $x$, having the permutations of all the powers of each of the arguments.  For example, the function $z=f(x,y)=Axy+Bx+Cy+D$ would have an $A^TA$ matrix and $A^Tz$ vector that consisted of the combinations of the powers of $x$ and $y$: $x^1y^1,x^1y^0,x^0y^1,x^0y^0$ otherwise written as $xy,x,y,1$. The pattern continues for higher dimension functions such as $w=f(x,y,z)$ and higher. That all makes well enough sense to me but now I'm wondering, what if I have a vector function such as $(z,w)=f(x,y)$? Is there a way, given $(x,y)$ points to do a least square fit function to those points? If there isn't, are there any other decent / common methods to fit a function of that form to points?","['least-squares', 'data-analysis', 'curves', 'functions', 'surfaces']"
2078658,Does there exist any continuous function whose partials doesn't exist?,"Does there exist a continuous function of $f : \mathbb R^2 \longrightarrow \mathbb R$ such that it is continuous whose both the partial derivatives don't exist. I think the function $f : \mathbb R^2 \longrightarrow \mathbb R$ defined by $f(x,y) = |x|(1 + y)$, where $(x,y) \in \mathbb R^2$ has the above property at $(0,0)$. But I can't prove that $f$ is continuous at $(0,0)$ by $\epsilon-\delta$ method. Please help me. Thank you in advance.","['multivariable-calculus', 'real-analysis', 'continuity']"
2078698,What is the remainder when $7^{7^{7^{7..........Infinity }}}$ is divided by $5$? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question What is the remainder when $7^{7^{7^{7.........Infinity }}}$ is divided by $5$ ? My try : $7^7$ when divided by $5$ gives the remainder $3$,and similarly, $7^{7^7}$ when divided by $5$ again gives the remainder $3$,and so, i know that upto infinity it will give remainder $3$. But, Above approach does not shows any real Maths. How to approach for such questions ? Edit : Can I stop the power tower to something like $7^{4k + R}$ ?","['discrete-mathematics', 'elementary-number-theory']"
2078703,"If $f:\mathbb{R}\to\mathbb{R}$ is continuous, differentiable at $0$, with $f(0)=1$, $f'(0)=1$, and $f(s+t)=f(s)f(t)$, how to show that $f(x)=e^x$?","Hi guys I wondered whether you could help me to prove the following, This is part of a longer exam question which I'm revising now. Also could you recommend any good books with proofs relating to calculus please. Suppose $f:\mathbb R \to \mathbb R$ is a continuous function such that: $f$ is differentiable at $0$ with $f(0) = 1$ and $f'(0)=1$ $f(s+t) = f(s)f(t) $ for all $s, t\in \mathbb R$ Prove that $f(x)>0$ for all $x\in \mathbb R$. Prove that $f$ is differentiable on $\mathbb R$ with $f'(x)=f(x)$ for $x\in \mathbb R$. Deduce that $f(x)e^{-x}$ is constant, and hence $f(x)=e^x$ Thank-you :)","['derivatives', 'real-analysis', 'calculus']"
2078731,"Multivariable integral, probably related to the gamma function","Let $x=\left[ x_1,x_2,..,x_n \right]^{T}$ represent the vector of all variables and $D$ be a diagonal matrix, the question is to integrate or give an approximate answer:
$\idotsint_{[0,\infty]^{n}} (x^{T}Dx)^{\alpha-1}exp(-x^{T}Dx) \,dx_1 \dots dx_n$","['multivariable-calculus', 'improper-integrals', 'integration', 'gamma-function']"
2078755,Finding a space $X$ such that $\dim C(X)=n$.,"Let $n\in \mathbb{N}$ . Is there some topological space $X$ such that $C(X)$ is a finite dimensional ring with $\dim C(X) = n$? Here, $C(X):=\{ f:X \to \mathbb{R} \mid f$  is continuous$\}$ and $\dim C(X)$ means Krull dimension.","['krull-dimension', 'general-topology', 'algebraic-geometry', 'commutative-algebra']"
2078780,Help find closed form:$\sum_{n=1}^{\infty}{m!\over n\cdot n(n+1)(n+2)\cdots(n+m)}$,show that and determine the closed form $$\sum_{n=1}^{\infty}{m!\over n\cdot n(n+1)(n+2)\cdots(n+m)}=\sum_{n=1}^{\infty}{{1\over n} }\sum_{k=0}^{m}(-1)^k{m\choose k}{1\over (n+k)}$$ $m\ge0$ My try: I was observing the favourite Euler's sum $$\zeta(2)=\sum_{n=1}^{\infty}{1\over n^2}$$ I found the following patterns. I can't find the closed form. $$\sum_{n=1}^{\infty}{0!\over n\cdot n}=\sum_{n=1}^{\infty}{{1\over n}\left({1\over n}\right)}$$ $$\sum_{n=1}^{\infty}{1!\over n\cdot n(n+1)}=\sum_{n=1}^{\infty}{{1\over n}\left({1\over n}-{1\over n+1}\right)}$$ $$\sum_{n=1}^{\infty}{2!\over n\cdot n(n+1)(n+2)}=\sum_{n=1}^{\infty}{{1\over n}\left({1\over n}-{2\over n+1}+{1\over n+2}\right)}$$ $$\sum_{n=1}^{\infty}{3!\over n\cdot n(n+1)(n+2)(n+3)}=\sum_{n=1}^{\infty}{{1\over n}\left({1\over n}-{3\over n+1}+{3\over n+2}-{1\over n+3}\right)}$$,['sequences-and-series']
2078791,Does convergence in the mean imply convergence in measure?,"There are a lot of types of convergences that imply other convergences, like almost uniform convergence implies convergence in measure, etc..., but this is the one that I cannot seem to find an answer for. Let me throw the definitions out there: A sequence $\{f_n\}$ of a.e. Real-valued, measurable functions is said to be $\underline{\text{convergent in measure}}$ if there is a measurable function $f$ such that for all $\epsilon >0$ 
  \begin{equation} \lim_{n\rightarrow\infty} \mu[\{x; |f_n(x)-f(x)|\geq \epsilon\}=0\end{equation} A sequence $\{f_n\}$ of integrable functions $\underline{\text{converges in the mean}}$ to $f$ if for an integrable function $f$ it holds that $\int |f_n-f| d\mu \rightarrow 0$ as $n\rightarrow \infty$. My intuition says that convergence in mean does NOT imply convergence in measure, just because the first seems too weak to imply the second, but honestly I have no way of knowing if thats true or how to prove it. Thanks for any help!","['uniform-convergence', 'real-analysis', 'measure-theory', 'convergence-divergence']"
2078792,How did people get the inspiration for the sums of cubes formula?,"I stumbled upon this neat formula for sums of cubes with arbitrary $x,y\in\mathbb{Z}$$$(x^2+9xy-y^2)^3+(12x^2-4xy+2y^2)^3=(9x^2-7xy-y^2)^3+(10x^2+2y^2)^3\tag1$$
With $1729=1^3+12^3=9^3+10^3$ as its first instance. And I believe that this formula was used by Ramanujan to find a formula for$$a^3+b^3=c^3\pm1$$
So my question? Questions: What would be someone's thinking process when finding other formulas such as $(1)$? Are there any other formulas similar to $(1)$? I'm thinking along the lines of starting with $$(x^2+axy+by^2)^3+(cx^2+dxy+ey^2)^3=(fx^2+gxy+hy^2)^3+(ix^2+jxy+ky^2)^3$$
But even Mathematica can't solve the ensuing system that follows. So for the moment, I'm stuck.",['algebraic-geometry']
2078796,Finding the nth term of a numeric sequence- Newton's little formula explanation,"In a contest problem book, I found a reference to Newton's little formula that may be used to find the nth term of a numeric sequence. Specifically, it is a formula that is based on the differences between consecutive terms that is computed at each level until the differences match. An example application of this formula for computing the nth term of the series (15, 55, 123, 225, 367, 555, 795, ....) involves computing the differences as shown below: 1) 1st Level difference is (40, 68, 102, 142, 188, 240)
2) 2nd Level difference is (28, 34, 40, 46, 52)
3) 3rd Level difference is (6, 6, 6, 6, 6) Now the nth term is  $$15{n-1\choose 0} + 40{n-1\choose 1} + 28{n-1\choose 2} + 6{n-1\choose 3}$$ where the constant multipliers are the first term of the differences at each level in addition to the first term of the sequence itself. I was not able to find any reference to this formula or a proof of it after searching on the web. Any explanation of this method is appreciated.",['sequences-and-series']
2078844,Taking the derivative inside the integral (Liebniz Rule for differentiation under the integral sign),"I have a function I would like to differentiate but am wondering if I my method is allowable: If $\displaystyle f(x)= \int_{a}^{b} h(t) \:\mathrm{d}t$, what is the derivative of $f$ with respect to $x$, if $x$ occurs in the expression of $h(t)$. Can I solve this by simply taking the derivative of $h(t)$ as I would normally any function while treating instances of $t$ as constants? Or must I account for the integration before taking the derivative. I understand that if I could compute the integral I would end up with an expression for $f(x)$ in terms of $x$ and that the integral is kind of just a place holder for that expression, but I am unsure of whether or not taking the derivative before computing the integral would change the result. Any advice would be appreciated as well as any suggested readings on this type of problem!","['derivatives', 'integration', 'calculus']"
2078853,Why does this process generate the factorial of the exponent? [duplicate],"This question already has an answer here : Property of $\{0^n, 1^n, \ldots\}$ [duplicate] (1 answer) Closed 7 years ago . Consider the process of taking a series of numbers and constructing a new series consisting of the difference between consecutive terms, and repeating this until a constant is reached: $$2,8,18,32,50\\6,10,14,18\\4,4,4$$ When this process is applied to sequences of the form $f(n) = n^a$, the constant reached seems to always be $a!$: $$1,2,3\\1,1$$ $$1,4,9,16\\3,5,7\\2,2\\$$ $$1,8,27,64,125\\7,19,37,61\\12,18,24\\6,6$$ $$1,16,81,256,625,1296\\15,65,175,369,671\\50,110,194,302\\60,84,108\\24,24$$ Can it be proven?","['factorial', 'sequences-and-series']"
2078869,Another way of proving :$\int_{0}^{1}{x-x^3+x^5-x^7\over (1+x^4)\ln{x}}dx=-\ln{2}$,Prove that $$\int_{0}^{1}{x-x^3+x^5-x^7\over (1+x^4)\ln{x}}dx=-\ln{2}$$ My try $x-x^2+x^3+x^5-x^7=x(1-x^2)+x^5(1-x^2)=x(1+x^4)(1-x^2)$ $$\int_{0}^{1}{x(1+x^4)(1-x^2)\over (1+x^4)\ln{x}}dx$$ Applying Frullani theorem $$\int_{0}^{1}{x-x^3\over \ln{x}}dx$$ $$\int_{0}^{1}{x-x^3\over \ln{x}}dx=-\ln{2}$$,"['integration', 'definite-integrals']"
2078873,Using Wallis' formula to show the limit of $a_n:=n!\left(\frac{e}{n}\right)^n n^{-1/2}.$,"Let $$a_n:=n!\bigg(\frac{e}{n}\bigg)^n n^{-1/2}.$$ With the help of Wallis' formula 
$$\frac{\pi}{2} = \prod_{n=1}^\infty \frac{4n^2}{4n^2-1}=\lim_{m\to \infty}\frac{2^{4m}(m!)^4}{((2m)!)^2 (2m+1)}$$ show that the limit of $a_n$ is $\sqrt{2\pi}$. I don't have any idea on how to derive the limit from this formula. I would greatly appreciate any hints or solutions.","['real-analysis', 'calculus', 'limits']"
2078937,What nice properties does exponentiation have?,"Exponentiation of course satisfies a number of nontrivial identities: $x^{y+z}=x^yx^z$ $(x^y)^z=x^{yz}$ $x^0=1$, $x^1=x$ However, these identities all involve functions other than exponentiation (I'm thinking of $0$ and $1$ as nullary functions, here). My question is what identities hold of exponentiation alone . That is: What is the equational theory of $(\mathbb{N}, exp)$? To be clear, I mean ""identity"" in the strict, universal-algebraic sense: one term equals another term, where each term is built from variables and exponentiation alone. Also, an identity has to hold on all of $\mathbb{N}$: identities which hold only on, say, numbers divisible by $17$ don't count. A related question: Is that theory axiomatized by finitely many equations? Note: A previous version of this question asked whether there were any nontrivial identities at all. This was extremely silly of me, as pointed out almost immediately by Stefan Perko below: $(x^y)^z=(x^z)^y$.","['universal-algebra', 'abstract-algebra', 'logic', 'exponentiation']"
2078942,Exercise 8.18 in Algebraic Graph Theory by Godsil and Royle,"This is at page 190 of Algebraic Graph Theory by  Chris Godsil and Gordon Royle. Let $\mathbf B$ be the submatrix of the symmetric matrix $\mathbf A$ obtained by deleting the $i$th row and column of $\mathbf A.$ Problem 1: Show that if $\mathbf x$ is an eigenvector for $\mathbf A$ such that $x_i=0$, then the vector $\mathbf y$ we get by deleting the $i$th coordinate from $\mathbf x$ is an eigenvector for $\mathbf B$. We call $\mathbf y$ the restriction of $\mathbf x$, and $\mathbf x$ the extension of $\mathbf y$. Now, suppose that $\theta$ is a common eigenvalue of $\mathbf A$ and $\mathbf B$, and that its multiplicity as an eigenvalue of $\mathbf A$ is $m$. Problem 2: If the multiplicity of $\theta$ as an eigenvalue of $\mathbf B$ is $m-1$, show that each $\theta$-eigenvectors of $\mathbf B$ extends to an eigenvector for $\mathbf A$. (It also has the third question, but I didn't copy it.) The first problem is a direct consequence of the definition of the eigenvector that $\mathbf A \mathbf x = \theta \mathbf x$. But I have no idea on how to prove the second problem. How can I prove it?","['eigenvalues-eigenvectors', 'graph-theory', 'matrices', 'adjacency-matrix', 'spectral-graph-theory']"
2078948,"Consider a function $f(x, y) = Ax^5 + Bx^4y + Cx^3y^2 + Dx^2y^3 + xy^4 − y^5$","Consider a function $f(x, y) = Ax^5 + Bx^4y + Cx^3y^2 + Dx^2y^3 + xy^4 − y^5$ where $A$ , $B$ , $C$ , $D$ are unspecified real numbers. Determine the values of $A$ , $B$ , $C$ , $D$ such that $f(x,y)$ satisfies $f_{xx}(x,y) + f_{yy}(x,y) = 0$ What I did so far, I took double derivative of x and y: $f_{xx}(x,y) = 20Ax^3+12Bx^2y+6Cxy^2+2Dy^3$ $f_{yy}(x,y) = 2Cx^3+6Dx^2y+12xy^2-20y^3$ How can I satisfies $f_{xx}(x,y) + f_{yy}(x,y) = 0$ ? There are no terms that cancels, subtracts or add.","['multivariable-calculus', 'calculus']"
2078966,Category of quasicoherent sheaves not abelian,"Wikipedia mentions that the category of quasicoherent sheaves need not form an abelian category on general ringed spaces.  Is there a `naturally occurring' example of this failing, even for locally ringed spaces?","['quasicoherent-sheaves', 'sheaf-theory', 'algebraic-geometry', 'coherent-sheaves', 'abelian-categories']"
2078969,Hints on integrating (rather complicated) exponential function,"I am interested in computing the following integral
$$\int_0^\infty \frac{1}{\sqrt{t}}\exp\left(-\frac{(x-t)^2}{t} - t\right)\, dt. $$
A hint is good enough for me!","['integration', 'partial-differential-equations']"
2078977,is there a large list of highly composite number?,"I'm searching for some very big {more less $2^{1024}$} highly composite number (HCN), for a number theory experiment, so I asked myself ""if there are prime list this size, why not HCN lists this size"" so instead of searching them, I could just copy them, but I have not found any list enough big, I have just found this list: https://oeis.org/A002182/list but is verry small. I could work with a not so big list, but I still need lots of very big, very composite numbers",['number-theory']
2078987,Probability that the difference of the max and min of three random numbers between 0 and 2 is less than 1/4?,Three numbers are chosen at random between 0 and 2. What is the probability that the difference between the largest number and the least number is less than 1/4? This is a problem from JHMT Calculus 2011. I'm trying to see if there are any better solutions than the one provided. Here is the solution provided:,['probability']
2078995,Is there a set of axioms governing the properties of derivatives in calculus that include this particular axiom and what would it be?,"Moved from Math Overflow due to not being regarded as a high degree of research Note: I am looking in particular at real valued/real input functions at all values regardless of differentiability. In this question a series of axioms or postulates governing calculus are proposed. Granted, that is abstract calculus rather than real number calculus. Is there any known way to write a similar set of postulates governing real number calculus involving derivatives, integrals, and (ideally) allowing the construction of differential equations but with the following statement selected as one of the axioms without redundancy or contradiction? ""if and only if a function is constant does it have a derivative of 0 for all real numbers"" My ultimate purpose is to negate the aforementioned axiom and so having a complete set of axioms would make it convenient for me to convey the actual meaning behind negating the statement since one can ultimately fall back upon the statements similar to how we developed non-Euclidean geometry. Some potential axioms that might be relevant that I thought of were: ""All elements of a derivation set are the inverse of the antiderivative where defined"" (might be better proposed as a conjecture) The derivation set of any function may not equal the empty set. Update: After discussing this with a few others more deeply, and noticing some non-uniqueness properties and things I've realized that the derivative need not be unique given the sort of things I would want to exist. Therefore, the following definitions deal with that issue: A derivation set is a set of a functions that can potentially result from differentiation being applied to some function. A derivative is an operator whose results from being applied to some function is some element of the derivation set for that function. In this sense altered forms of derivatives would be solutions sets of functions that satisfy some equation rather than necessarily a unique operator. However, the equation itself is probably not something trivially apparent by my guess or something one could derive in a quick manner.","['derivatives', 'axioms', 'calculus', 'research']"
2078998,Galois groupoid of covering map: are endomorphisms of universal covering spaces automorphisms?,"Suppose we're dealing with nice spaces which are the coproducts of their connected components. Then there's a functor $\Pi_0:\mathsf{Top}\rightarrow \mathsf{Set}$ taking a space to its set of connected components. Let $p:E\to B$ be a covering map. The Galois ""groupoid"" of $p$  $$\Pi_0(E\times _B E\times _B E)\rightarrow \Pi_0(E\times _BE) \rightrightarrows \Pi_0(E)$$ is defined as the image along $\Pi _0$ of
$$E\times _B E\times _B E\rightarrow E\times _BE \rightrightarrows E$$ where the left arrow forgets the middle element of a triple $(x,y,z)$, and the arrows on the right are the pullback projections (the unit arrow is given by the diagonal). Theorem 6.7.4 of Borceux and Janelidze's Galois Theories says that if $p$ is a universal covering map with connected $E$ (and therefore $B$), then the Galois groupoid is a group isomorphic to $\mathsf{Aut}(p)$ - the group of automorphisms of $p$ (over $B$). I tried to calculate the Galois ""groupoid"" of a covering map with connected $E$ (and therefore $B$) without any further assumptions and found it isomorphic to $\mathsf{End}(p)$: The connectedness of $E$ is equivalent to $\Pi_0(E)=\bf 1$ so there's only one object and the ""groupoid"" is indeed a group has a single object. To calculate $\Pi_0(E\times _BE)$ we use the distributivity of the category of spaces, the fact covering maps have discrete fibers, and the fact $\Pi_0$ is left adjoint to discrete spaces. This shows $$\Pi_0(E\times _BE)\cong \Pi_0(\coprod_b(p^{-1}\left\{ b\right\}\times p^{-1}\left\{ b\right\} ))\cong \coprod_b \Pi_0(p^{-1}\left\{ b\right\}\times p^{-1}\left\{ b\right\}).$$ Now suppose the fibers are of size $n$. Then $|\Pi_0(p^{-1}\left\{ b\right\}\times p^{-1}\left\{ b\right\})|=n^2$, and it seems like $\Pi_0(E\times _BE)$ is comprised of endomorphisms of $p$. Is this reasoning correct? Does it imply that endomorphisms of universal covering maps are automorphisms? What's the intuition for this? Added. My original formulation was misleading: the Galois ""groupoid"" is not generally a groupoid unless $p$ is a principal bundle. I think it's just an internal category in general. (This sits well with the fact $\Pi_0(E\times _BE)$ is not a group for a general covering map.)","['galois-theory', 'covering-spaces', 'category-theory', 'algebraic-topology', 'general-topology']"
2079003,Rates and Ratio work problem,"I've encountered this problem It takes $60$ minutes for $7$ people to paint $5$ walls. How many minutes does it take $10$ people to paint $10$ walls. The answer to this one is $84$ minutes. However, How did it come up to this answer? can someone explain to me why? Can anyone give a step by step method on how did we lead to this/solve this kind of problems?","['algebra-precalculus', 'ratio']"
2079007,Why is this way used to solve for base 10 to base $16$,"In one of my courses there is a problem to convert $142$ base $10$ to base $16$, I know the answer is $8E$ base $16$ just through dividing $142$ by $16$ but the solution is show to use base $2$ like this $2^7+2^3+2^2+2^1=142$ and then in binary $10001110$ base $2 = 8E$ base $16$. So why do this way and isn't it more time consuming or this the proper way to do things?",['discrete-mathematics']
2079011,Matrix product calculation of coefficients of squared polynomial,"Answers to this question describe how, if we square an order- $m$ polynomial with coefficients $a_k$ , $$ p(x) = \left( \sum_{k=0}^{m} a_k x^k \right)^2 = \sum_{k=0}^{2m} c_k x^k $$ the coefficients $c_k$ of the resulting order- $2m$ polynomial can be calculated as $$ c_k = \sum_{j=0}^{k} a_j a_{k-j} $$ provided we take $a_j = 0$ in cases where $j \notin \{ 0, \dots, m\}$ . Question: If we let $\mathbf{c}$ be a length $2m+1$ vector containing all the $c_k$ , and $\mathbf{a}$ be a length $m+1$ vector containing all the $a_k$ , is it possible to express $\mathbf{c}$ as a matrix product of $\mathbf{a}$ with other matrices? I've been thinking about constructing some matrices to pad the $\mathbf{a}$ with zeros up to the length of $\mathbf{c}$ , then making a reversed copy, but haven't been able to get any further. Thanks!","['matrices', 'polynomials', 'convolution']"
2079062,Fancy slash thing? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 7 years ago . Improve this question I'm an early high school student so I haven't yet encountered the symbol I am looking for in school, but what is the symbol that looks kind of like a big "" / "" but with curled ends called?  Sometimes it has numbers/variables above or below it. I just need to know what it's called so I can research it on my own. I think it's used in calculus... maybe?","['notation', 'calculus']"
2079082,Expanding powers of $\tan^n \theta$,"I know there are standard techniques to expand $\sin^n \theta$ and $\cos^n \theta$ $(n \in \mathbb{N})$ in terms of sines and cosines of multiples of $\theta$. We take a complex number $z = \cos \theta + i \sin \theta$. Then $$
\cos k\theta = {1 \over 2} \left ( z^k + {1 \over z^k} \right )
$$ where $k \in \mathbb{N}$ and $$
\cos^n \theta = {1 \over 2^n} \left ( z + {1 \over z} \right )^n .
$$ You just expand the second equation and repeatedly apply the first, to get your desired result. We do a similar thing for $\sin^n \theta$. My question is: Is there any standard technique to expand $\tan^n \theta$ as a sum of tangents or sines or cosines of multiples of $\theta$? EDIT: I noticed that an expansion like what I want doesn't exist even for $\tan^2 \theta$. The best we can do is: $\tan^2 \theta = 1 - 2 {\tan \theta \over \tan 2\theta}$. Now that it seems like an expansion might not exist, can anyone please justify why it doesn't exist? But if something does exist, then what is it? I'd prefer the expansion to be something which is easily integrable, just like with the expansions for $\sin^n \theta$ and $\cos^n \theta$.","['trigonometry', 'complex-numbers']"
2079098,Checking if Tedious Limit is $0$.,"I am trying to rigorously (without a calculator) show
$$\lim_{n\to \infty }\sqrt{n}\color{blue}{{{n}\choose {\Big[ np + \sqrt{np(1-p)}\,\Big]}}p^{\Big[np + \sqrt{np(1-p)} \,\Big]}{(1-p)^{\bigg(n-\Big[np + \sqrt{np(1-p)}\,\Big]\bigg)}}}=0$$ where $0<p<1$ and $[\cdot ]$ denotes the nearest integer. 
This limit was formed while playing with the $\color{blue}{\text{binomial mass function}}$ and while fiddling with the proof of the DeMoivre/Laplace Central Limit Theorem: https://en.wikipedia.org/wiki/De_Moivre%E2%80%93Laplace_theorem I am not completely convinced that the limit is 0, but after seeing some computations on Mathematica https://mathematica.stackexchange.com/questions/134523/why-wont-limit-evaluate-and-what-can-be-done-about-it?noredirect=1#comment362965_134523 , I see that it tends to $0$ for several rational values of $p$. I want to see whether the limit is $0$ for all $p$ in $(0,1)$. What I've tried: (1) Using ${{a}\choose{b}}\le (\frac{ae}{b})^b$ and dropping $[ \cdot]$ for simplicity (and hopefully at no cost) to get $$\sqrt{n}\Bigg(\frac{ne}{np + \sqrt{np(1-p)}}\Bigg)^{np + \sqrt{np(1-p)}}p^{np + \sqrt{np(1-p)}}(1-p)^{n-np + \sqrt{np(1-p)}} \\\le \sqrt{n}\Bigg(\frac{nep}{np(1-p)+n\sqrt{\frac{p(1-p)}{n}}(1-p)
}\Bigg)^{np+ \sqrt{np(1-p)}}(1-p)^n\\\le \sqrt{n}\bigg(\frac{e}{(1-p)}\bigg)^{2np}(1-p)^n$$ but according to Mathematica the latest expression doesn't tend to 0, so I may need a better upper bound.","['computational-mathematics', 'calculus', 'limits']"
2079099,Convergence of harmonic series multiplied by partial sums of harmonic series,"How can I show if the following series is convergent or divergent
$$\sum_{n=1}^\infty \frac1n \left ( 1 + \frac12 + \frac13 + \cdots + \frac1n \right) = \sum_{n = 1}^\infty\frac{1}{n}\sum_{k = 1}^n\frac{1}{k}$$
If it were a sequence I could have easily used Cauchy first theorem.","['harmonic-numbers', 'sequences-and-series', 'convergence-divergence']"
2079112,Proof of $\epsilon_{ijk}\epsilon_{klm}=\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}$ [duplicate],"This question already has answers here : Proof relation between Levi-Civita symbol and Kronecker deltas in Group Theory (2 answers) Closed 3 years ago . I'm a student of physics. There is an identity in tensor calculus involving Kronecker deltas ans Levi-Civita pseudo tensors is given by $$\epsilon_{ijk}\epsilon_{klm}=\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}$$ which is extensively used in physics in deriving various identities. I have neither found a proof of this in physics textbooks nor in Wikipedia. In particular, how does the above formula follow from the definition of $\epsilon_{ijk}$ tensor$$\epsilon_{ijk} =
  \begin{cases}
         +1 & \text{ for even permutations }, \\
         -1 & \text{ for odd permutations } ,\\
    \;\;\,0 & \text{ for repetition of indices },
\end{cases}$$ This is the only definition of I'm familiar with.","['matrices', 'tensor-products', 'tensors', 'kronecker-symbol']"
2079122,"Is this space Dense in $ C[0,1]$","Is the space $\{f\in C[0,1]\mid \int_0^1f\neq 0\}$ dense in $C[0,1]$ with sup-norm topology. I think yes, because it is the inverse image of the set $\mathbb{R} \setminus \{0\}$",['functional-analysis']
2079123,Bounded sequence of functions has subsequence convergent a.e.?,"Does bounded sequence of functions have subsequence convergent a.e.? There is a equivalent question : Does weak convergent sequence of functions have subsequence convergent a.e.? These two are equivalent because of Banach-Steinhaus theorem .
Before I ask here, I thought Bolzano-Weierstrass theorem which is every bounded sequence has convergent subsequence. And my question comes up from 'this theorem can be applied to sequence of functions?'.
If the space of functions should be special (like reflexive or compact or housdorff), please mention about it. I didn't study functional analysis but attended courses : Partial differential equation and real analysis for graduate student.","['functional-analysis', 'real-analysis', 'weak-convergence']"
2079137,Generating multivariate normal samples - why Cholesky?,"Hello everyone and happy new year! May all your hopes and aspirations come true and the forces of evil be confused and disoriented on the way to your house. With that out of the way... I am trying to write a computer code that gets a vector $\mu \in R^n $ and matrix $\Sigma \in \mathbb R^{n \times n}$ and generates random samples from the multivariate normal distribution with mean $\mu$ and covariance $\Sigma$. The problem : I am only allowed to use the program to sample from the single variable normal distribution with mean $0$ and variance $1$: $N(0, 1)$. The proposed solution : Define a vector of zeros (initially) $v \in \mathbb R^n$, now for all $i$ from $1$ to $n$, draw from a single variable normal dist: $v_i \overset{}{\sim} N(0, 1)$. Now do a Cholesky decomposition on $\Sigma$: $\Sigma = LL^T$. Now finally the random vector we want that is distributed from the multivariate gaussian is $Lv + \mu$. My question is why? I don't understand the intuition, if it was a single dimensional distribution $N(\mu, \sigma^2)$ then I understand why $\sigma ^2 v + \mu$ is a good idea, so why cholesky? Wouldn't we want $\Sigma v + \mu$?","['numerical-methods', 'probability', 'linear-algebra', 'cholesky-decomposition']"
2079143,Volume of this peculiar set,"What is the volume of the set $$S=\{x=(x_1,x_2,x_3,\ldots,x_n) \in \mathbb{R}^n \mid 0\leq x_1 \leq x_2\leq x_3\leq\cdots\leq x_n\leq1\}\text{?}$$ I think this is related to the volume of the unit ball in $\mathbb{R}^n$. Any ideas. Thanks beforehand.","['multivariable-calculus', 'calculus']"
2079148,"What does the symbol $f^*\langle v,w\rangle _p$ mean in DoCarmo's Differential Geometry?","I'm preparing the differential geometric exam, but I don't have the textbook on hand. My teacher's lecture note left a symbol $f^*\langle v,w\rangle _p$ without further explanation, so I don't know what it means. I think it is hard for me to google something especially in the topic of differential geometry--many people have their own definitions and terminologies. So I decide to ask here. Let me first state a definition in our teacher's lecture notes: (the appointed textbook is doCarmo's) If $S_1,S_2$ are surfaces, a smooth map $f:S_1\rightarrow S_2$ is called local isometry if it takes any curve in $S_1$ to a curve of the same length in $S_2$. Next, the note said that, If $f:S_1\rightarrow S_2$ is differentiable, $p\in S_1$ and $v,w\in T_p(S_1)$, then $f^*\langle v,w\rangle _p=\langle df_p(v),df_p(w)\rangle_{f(p)}$, where $f^*\langle\cdot,\cdot\rangle_p$ is a symmetric bilinear form on $T_pS$. So my question is, what is the symbol $f^*\langle v,w\rangle_p$ here? What does it mean? Is $f^*$ related to the original function $f$?",['differential-geometry']
2079165,In how many ways can you colorize the vertices of a grid with 4 colors so that every unit-square vertices are all of a different color?,"In how many ways can you colorize the vertices of an $(n\times n)$-square forming an $(n+1)$-grid with $4$ colors so that every unit-square vertices are all of a different color ? Precisions: $(n\times n)$-square : square whose edge length is $n$. $(n+1)$-grid : square grid containing $(n+1)$ vertices per side, and composed of $n^2$ unit-squares of dimension $(1\times 1)$. Here are some examples for $n=1,2,3$ My attempt : I have posted a similar question here I tried to do the same way (Because the answer is like that too ($24\times (2^{n}-1)$)) but it didn't answer. Any hints?","['combinatorics', 'graph-theory']"
