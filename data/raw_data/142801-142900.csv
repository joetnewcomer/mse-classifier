question_id,title,body,tags
2318882,"$W^{1,p}_0 \cap C^\infty \subset C^\infty_0$?","Is this true: $W^{1,p}_0 \cap C^\infty \subset C^\infty_0$? If this is true, how to prove it? If not, what is a counter-example? Notation: Denote $C^\infty_0$ the set of all real-valued smooth function $f$ on
 $\mathbb R$ such that $\lim_{x\to \pm\infty} f(x)=0$, and denote $C^\infty_c$ the set of all real-valued smooth function $f$ on
$\mathbb R$such that the support of $f$ is compact. Finally, define $W^{1,p}_0$ is the completion of $C^\infty_c$ with respect to the Sobolev norm $||\cdot||_{W^{1,p}}$","['real-analysis', 'partial-differential-equations', 'functional-analysis', 'sobolev-spaces', 'analysis']"
2318888,Limit at endpoints,"The definition of limit states (Spivak): The function $f$ approaches the limit $l$ near $a$ means : for every
  $\varepsilon >0$ there is some $\delta >0$ such that, for all $x$, if
  $0< |x-a|<\delta$, then $|f(x)-l|<\varepsilon$. and if its not true that $f$ approaches $l$ near $a$: There is some $\varepsilon>0$ such that for every $\delta>0$ there
  is some $x$  which satisfies $0<|x-a|<\delta$ but not
  $|f(x)-l|<\varepsilon$. There are some textbooks that provide the definition of limit with the additional condition ""for all $x \in D_{f}\dots$"", but it seems to me that Spivak doesn't find it necessary. So, supposing this condition is neither logically necessary nor implicit, suppose $\lim_{x\to a^{+}}g(x)=L$ for some function $g:[a,b]\to \mathbb{R}$.  In order to verify that actually $\lim_{x\to a}g(x)=L$, we could prove that the negated form of the limit definition above is false for $g$ near $a$. However, when $x<a$, $x$ is outside de domain of $g$, and $f(x)$ is not defined. So, perhaps, it's not the case that we have a vacuously true statement, because testing the veracity of an expression containing $|f(x)-L|$ does not even make logical sense at all. What happens in a logical statement when a free variable takes a particular instance that turns an expression of it into nonsense? The statement should be ignored as a whole? In the example above, when $x<a$, for any $\delta>0$ there are plenty of $x$ which satisfy $0<|x-a|<\delta$. But, is it true that  $|f(x)-L|<\varepsilon$? By which logical means does the definition of limit given still applies when $g$ approaches $a$?","['propositional-calculus', 'calculus', 'limits']"
2318901,Prove the maximum of the function $\det x$ restrained to the sphere $\sum x_{ij}^2=n$ is $1$,"Let $M(n\times n)=\mathbb{R}^{n^2}$. Prove the maximum of $f:\mathbb{R}^{n^2}\to\mathbb{R}$, given by $f(x)=\det x$, restrained by the sphere $\sum_{i,j} x_{ij}^2=n$, is reached in an orthogonal matrix, so its value is $1$. Attempt: define $\varphi=\sum_{i,j} x_{ij}^2=n$, so $\nabla\varphi(x)=2[x_{ij}]$ $\nabla f(x)=(-1)^{i+j}X_{ij}$, with $X_{ij}$ the $ij$-minor of $x$. Using the Lagrange Multipliers, with $M=\varphi^{-1}(n)$, the maximum of $f|_M$ is given when $(-1)^{i+j}X_{ij}=2\lambda x_{ij}\Rightarrow(-1)^{i+j}X_{ij}x_{ij}=2\lambda x_{ij}^2$. I sum in $i$ and $j$ to get: $n\det x=2\lambda\sum_{i,j} x_{ij}^2=2\lambda n\Rightarrow\det x = 2\lambda$. I couldn't go further, specially proving that the matrix is orthogonal. Any help?","['real-analysis', 'lagrange-multiplier']"
2318906,Universal algebraic geometry,"I don't know the usual name for this (sub-)field of maths, but basically I'm talking about the study of algebraic geometry over general algebraic structures, in the sense of universal algebra. I think that field is specifically restricted to studying ""equational Noetherian structures"". My questions are the following: what is this field precisely about, what are its paradigmatic questions? What are some examples of interesting results? 
Can anyone also provide references about it?","['universal-algebra', 'reference-request', 'algebraic-geometry']"
2318960,Is this proof correct? I'm trying to prove a tautology without truth tables.,"Conclude that $(p\land q)\rightarrow p$ is a tautology without using truth tables. Here's what I did: $$(p\land q)\rightarrow p
\equiv \lnot(p\land q)\lor p$$ By De Morgan's Law: $$\lnot(p\land q)\lor p\equiv \lnot p\lor \lnot q\lor p$$ Then By the Associative Property: $$\lnot p\lor \lnot q\lor p\equiv \lnot q \lor(\lnot p\lor p)$$ After this, I specified that $\lnot p\lor p$ is always true, so by the definition of a disjunction $\lnot q \lor(\lnot p\lor p)$ must also be true. Therefore, since $(p\land q)\rightarrow p\equiv \lnot q \lor(\lnot p\lor p)$ and $\lnot q \lor(\lnot p\lor p)$ is always true, it stands to reason that $(p\land q)\rightarrow p$ is also always true. However, my textbook simply says ""If the hypothesis $p\land q$ is true,
then by the definition of conjunction, the conclusion $p$ must
also be true."" I don't understand how this concludes the statement is a tautology. Is my proof not correct? Did I go overboard?","['propositional-calculus', 'logic', 'discrete-mathematics']"
2318972,One step in the proof that the series of $1/{n^2}$ equals ${\pi^2}/6$ that I don't understand,"I know questions about the specific series $$\sum_{n=1}^{\infty} {1 \over {n^2}} = {{\pi^2} \over 6}$$ have been posted before, but there is one specific step in the proof that I don't understand I have yet to find a sufficient answer. I had already started working out the exercise and was getting the same results as in the following link: http://empslocal.ex.ac.uk/people/staff/rjchapma/etc/zeta2.pdf We want to first show $$\int_{(0,1)^2} {1 \over {1-xy}} d{\lambda^2}(x,y) = \sum_{n=1}^{\infty} {1 \over {n^2}}$$ by using a geometric series. I defined $${f_m}(x,y) :\doteq \lim_{m\to\infty} \sum_{n=0}^{m} {1 \over {(xy)^2}} = \lim_{m\to\infty} {{1 - {(xy)^{m+1}}} \over {1 - xy}}$$ This is a monotone increasing sequence of nonnegative functions that converges to $1 \over {1 - xy}$.
Thus, we can use the monotone convergence theorem to exchange the sum and integral; further, by using an appropriate index shift, Tonelli's theorem, and the linearity of integrals, we get: $$\int_{(0,1)^2} {1 \over {1-xy}} d{\lambda^2}(x,y) = \lim_{m\to\infty} \sum_{n=1}^{m+1} \int_{0}^{1} \int_{0}^{1} {1 \over {(xy)^{n-1}}} dx dy$$ I can easily show that if $n = 1$ then the above double integral is equal to $1$, and if $n \geq 3$, the double integral is equal to $1 \over {n^2}$. My problem arises by $n=2$. In this case we have $$\int_{0}^{1} \int_{0}^{1} {1 \over xy} dx dy$$ which clearly does not exist. Am I just seeing incorrectly? Is there something obvious that I'm missing?  How can I justify this? I appreciate any help!","['lebesgue-integral', 'sequences-and-series']"
2318984,Inverse of the matrix,"$$U(a,b)=\left(\begin{matrix}a&b&b&b\\b&a&b&b\\b&b&a&b\\b&b&b&a\end{matrix}\right)$$
Is there an easy way to find the inverse of $U(1,2)$ , a trick to solve this problem easy?(its part of an exam with answers)","['matrices', 'linear-algebra', 'inverse']"
2318987,Product of uniformly convex spaces is uniformly convex,"I've been looking for a not too complicated solution to the following problem: Let $X,Y$ be uniformly convex Banach spaces and for $1<p<\infty$ define the $p$-direct sum $X \oplus_p Y = X \times Y$ equipped with the norm,
  $$ \lVert (x,y) \rVert_p = \left( \lVert x \rVert^p + \lVert y \rVert^p\right)^{\frac1p}$$
  for $x \in X,$ $y \in Y.$ Show that $X \oplus_p Y$ is uniformly convex. Here we say a Banach space $X$ is uniformly convex if for all $\varepsilon >0,$ there is $\delta > 0$ such that for all $x,y \in X$ with $\lVert x \rVert , \lVert y \rVert \leq 1,$ $\lVert x -y \rVert > \varepsilon$ implies $\lVert \frac{x+y}2 \rVert < 1-\delta.$ If proving a particular case, say $p=2,$ is significantly easier/simpler I would also be interested in that. I don't have much to say for an attempt, as everything I've tried doesn't get me far. The main issue is that the condition $\lVert (x,y) \rVert_p \leq 1$ doesn't tell us much about the sizes of $\lVert x \rVert$ and $\lVert y \rVert$ which makes it difficult to apply the uniform convexity property of the two spaces. I did find a proof in Clarkson's original paper, where a generalisation of this result is shown. However I find the proof seems overly complicated and technical, defining a bunch of seemingly arbitrary quantities, passing to subsequences, making careful estimates, etc. I feel there should be a cleaner proof for this special case. Edit (11th Nov 2017) : As pointed out in the comments, my definition of uniform convexity was incorrect. It previously said $\lVert x -y \rVert < \varepsilon$ implies $\lVert \frac{x+y}2 \rVert > 1-\delta$ and has since been fixed.","['functional-analysis', 'banach-spaces']"
2318997,Is it possible to place 26 points inside a rectangle that is 20 cm by 15 cm so that the distance between every pair of points is greater than 5 cm?,"I need help to answer the following question: Is it possible to place 26 points inside a rectangle that is $20\, cm$ by
  $15\,cm$ so that the distance between every pair of points is greater
  than $5\, cm$? I haven't learned any mathematical ways to find a solution; whether it maybe yes or no, to a problem like this so it would be very helpful if you could help me with this question.","['combinatorics', 'rectangles']"
2319021,Direct sum of exact sequences?,"In Hatcher's Algebraic Topology textbook, he has been referring to ""direct sum of exact sequences"". As far as I know he's never defined this and I can't find what I'm looking for online. Without a precise definition I can only guess what he means but I'm not entirely sure. For example, he refers to taking the direct sum of free resolutions of abelian groups $H$ and $H'$ to obtain a free resolution for $H \oplus H'$. What exactly does he mean here?","['abstract-algebra', 'exact-sequence', 'direct-sum', 'homological-algebra']"
2319026,Proving every derivative of $\sqrt\cos x$ is unbounded?,"This question is related to one I posted earlier today. I am $99$% sure the claim is true, and I can of course prove it for the first two or three derivatives, but I don't know how to jump to the infinite case. Is induction of some kind in order? I'm not sure how to proceed.","['derivatives', 'real-analysis', 'calculus']"
2319030,Probability of ordered dice,"Suppose you throw five dice, order them from lowest to highest number, then select the third die. What is the probability that the third die was number 2? Or, in general, when throwing $n$ dice and selecting the $i$th die (sorted from low to high), what is the probability that the number of that die is $k_i$? So the above example was $n=5,i=3,k_i=2$. How do you determine a general formula for finding the probability?","['combinatorics', 'probability']"
2319058,Proof verification: Zariski topology is quasi-compact,"I am solving the following problem from AM commutative algebra. I am solving commutative algebra in my spare time to get good grip in it. Prove that the Zariski topology is Quasi-compact. Here is my proof, I just want to make sure that I have everything correct. We want to prove that X is quasi-compact. Suppose we have a covering of X by basic open sets $\{ X_{f_i} \}_{i \in \mathcal{I}}$, i.e we have that $$X = \bigcup_{i \in \mathcal{I}}X_{f_i} \iff \bigcap_{i \in \mathcal{I}} V\big( (f_i) \big) = \emptyset.$$ If $\{(f_i)\}_{i \in \mathcal{I}}$ doesn't generate 1, then there exists a prime J such that J contains the ideal generated by all the $f_i$. Thus, $J \supseteq (f_i)$ for all $i \in \mathcal{I}$, which is a contradiction. Thus, the ideal generated by all the $f_i$ agree with the whole ring, so in particular we must have: $$1 = \Sigma_{i \in \mathcal{J}} g_i f_i \text{       } (g_i \in A)$$ Where $\mathcal{J}$ is some finite subset of I. To finish the proof we should show that: $\{X_{f_i}\}_{i \in \mathcal{J}}$ cover X.  That is we want to prove that 
$$\bigcup_{i \in \mathcal{J}} X_{f_i} = X$$ Since $\mathcal{J}$ is a finite subset we can rewrite the above equality as, that is we want to show that: $$\bigcup_{i = 1}^{i = m} X_{f_i} = X \iff \emptyset =  \bigcap_{i = 1}^{i = m}V\big( (f_i) \big) = V(\bigcup_{i = 1}^{i = m} f_i) =  V\big( (f_1,\ldots, f_m)\big)$$ Where the last equality is from exercise 1.15 part a. Since we have that 
$$1 = \Sigma_{i \in \mathcal{J}} g_i f_i \text{       } (g_i \in A)$$ This means that 
$$(f_1,\ldots,f_m) = (1)$$ Thus the equality below is satisfied: 
$$V\big( (f_1,\ldots, f_m)\big) = \emptyset = \bigcap_{i = 1}^{i = m}V\big( (f_i) \big)$$ And from the equivalence earlier we get our result.","['solution-verification', 'algebraic-geometry', 'commutative-algebra']"
2319099,Zero Polynomials: Help Me Get out of a Circular Argument,"The following exercise appears at the beginning of Apostol's Calculus (Vol. 1), before any calculus has been developed, so it's not looking for a solution involving derivatives or Taylor's theorem: Let $f(x)$ be a polynomial of degree $n$. If $f(x) = 0$ for $n+1$ distinct real values of $x$, then every
  coefficient $c_k$ is zero and $f(x)=0$ for all real $x$. I can easily prove the second part - that is, $f(x)=0$ everywhere - by applying the factor theorem $n$ times, but this doesn't say anything about the coefficients. So I tried using an inductive argument, but I couldn't make it work, for two reasons: If $f(x)$ has degree $n+1$ and is zero everywhere, then $f(x) = xg(x)$ where $g(x)$ has degree $n$. We can say that $g(x)$ is zero for all $x\neq 0$, but not for $x=0$. Therefore we can't apply the induction step. Even if we got past problem 1., all we'd have shown is that $f(x)$ is equal to a polynomial of degree $n+1$ with all-zero coefficients. The argument is still not complete, because we don't know that every polynomial has a unique representation. The only way I know how to prove this uses the fact that if a polynomial is zero everywhere, it has all-zero coefficients - which is what we're trying to prove in the first place. I found a proof that an everywhere-zero polynomial has all-zero coefficients, but this can't possibly be what Apostol is asking for from a beginning calculus student. Is there another, simpler proof?","['polynomials', 'roots', 'proof-verification', 'algebra-precalculus', 'induction']"
2319170,Find the values of $a$ $\in$ $\mathbb{R}$ where the system $Ax=x$ allows a solution different to the null one,"I have to find the values of $a$ $\in$ $\mathbb{R}$ where the system $Ax=x$ allows a solution different to the null one, and then solve the system with those values I found of the following matrix: $A=\begin{bmatrix}2&0&2&\\2&a+1&a&\\-1&a&0&\end{bmatrix}$ So, considering the statement, what I did is the following: First, $Ax-x=0$ Then, $(A-I)x=0$ And after, I found the cases of $a$ $\in$ $\mathbb{R}$ where: $Det(A-I)=0$ And, I did that, considering that the system will admit a solution different from the trivial one, if the determinant of the matrix of this particular system is equal to zero. Is my reasoning correct?","['linear-algebra', 'determinant']"
2319184,What metric is used to measure consistency in scores?,"So suppose you are trying to compare 2 people's consistency in Bowling where the max score is 300. Standard deviation seems like it would not be reliable to measure consistency in performance because large variations are seen without context. If player A gets 104, 115, and 180 while player B gets 120, 123, and 127, player B is seen as the more consistently better one if you plainly use standard deviation. If you use the mean of both players' data, player A's average will be affected by the outlier. So I'm wondering which formula can be reliably used to determine who is more consistent as well as better performing overall.","['statistics', 'data-analysis']"
2319194,Well-Ordering Principle to Show All fractions can be written in lowest terms [duplicate],"This question already has an answer here : Prove that there's no fractions that can't be written in lowest term with Well Ordering Principle (1 answer) Closed 2 years ago . This is from Class Note from 6.042 ocw courses at MIT: ""Well Ordering Principle"" section: You can read the original here at page 1 and 2; Well Ordering Principle: http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/MIT6_042JF10_chap03.pdf In fact, looking back, we took the Well Ordering Principle for granted
  in proving that $\sqrt{2}$ is irrational. That
  proof assumed that for any positive integers $m$ and $n$, the fraction $\frac{m}{n}$
  can be written in lowest terms, that is, in the form $\frac{m'}{n'}$ where $m'$
  and $n'$ are positive integers with no common factors. How do we know
  this is always possible? Suppose to the contrary that there were $m$, $n$ in $\mathbb{Z}^+$ such that the fraction $\frac{m}{n}$ cannot be written in lowest
  terms. Now let $C$ be the set of positive integers that are numerators
  of such fractions. Then $m$ in $C$, so $C$ is nonempty. Therefore, by Well
  Ordering, there must be a smallest integer, $m_0$ in $C$. So by definition of
  $C$, there is an integer $n_0 > 0$ such that the fraction $\frac{m_0}{n_0}$ cannot be
  written in lowest terms. This means that $m_0$ and $n_0$ must have a common
  factor, $p > 1$. But $(\frac{m_0}{p}) / (\frac{n_0}{p}) = \frac{m_0}{n_0}$ so any way of expressing the left hand fraction in lowest terms would
  also work for $\frac{m_0}{n_0}$, which implies the fraction($\frac{m_0}{p}) / (\frac{n_0}{p})$ cannot be in written in lowest terms either. So by definition of $C$, the numerator, $\frac{m_0}{p}$, is in $C$. But $\frac{m_0}{p} < m_0$,
  which contradicts the fact that $m_0$ is the smallest element of $C$. Since
  the assumption that $C$ is nonempty leads to a contradiction, it follows
  that $C$ must be empty. That is, that there are no numerators of
  fractions that can’t be written in lowest terms, and hence there are
  no such fractions at all. I don't really understand the part where, the author says: This means that $m_0$ and $n_0$ must have a common factor, $p > 1$ . BECAUSE $\frac {m_0}  {n_0}$ is a irreducible fraction, both $m_0$ and $n_0$ have no common factors other than 1. If they had common factors other than one, then $\frac {m_0}  {n_0}$ would not be a irreducible fraction. I think that IT IS NOT THE CASE THAT $m_0$ and $n_0$ must have a common factor, $p > 1$. Let that fraction be $2/3$, $2/3$ is irreducible. 2 and 3 have no common factors other than $1$. I assume that if a fraction cannot be written in lowest terms, then the fraction is irreducible, i.e. something like one half. If you didn't want to access MIT. look no further","['discrete-mathematics', 'prime-factorization', 'elementary-number-theory']"
2319210,Repeatedly taking differences on a polynomial yields the factorial of its degree?,"Consider a function such that it takes in polynomial function and creates an array of its outputs and then using that array creates another new array by calculating the absolute difference between the first $2$ values and keeps doing this until it reaches an array full of zeros. This is much easier to show you by example. For example take $F(x)= x^2$, the first array would be $1,4,9,16,25,36,49,64,81$ and so on, the second would be $3,5,7,9,11,13,15,17,19$ ( the difference between the first value and the second one) but the third one is where it gets interesting as if we continue the pattern we would get an array filled with only $2$'s and after that it would only be zeros. Lets do another example, $F(x)=x^3$ $1,8,27,64,125,216,343$ $7,19,37,61,91,\dotsc$ but here is the interesting part if we continue this $12,18,24,30,\dotsc$ and once more then we get $6,6,6,6,6,\dotsc$ after that it would just be an array of zeros There are $2$ main observation that I made about this Firstly, the value that is begin repeated indefinitely is equals to the factorial of the functions power. Meaning that for $F(x)=x^2$ the value being repeated is $2!$. For $F(x)=x^3$ , it's $3! $ and this is true for all polynomials (I tried it up to $x^7$, after that it got too messy) Secondly, the value that is repeated always occurs on the $n$th iteration of the function. Meaning that for $F(x)=x^2$, we have to go through the processes $2$ times before we find the value. For $F(x)=x^3$, we have to go through it $3$ times before getting the value. Is there any way to prove this and does this mean anything at all?","['number-theory', 'factorial', 'sequences-and-series', 'analysis']"
2319266,Gauss map and Minkowski functionals,"Let $K$ be a convex body and let $\| \cdot \|_{K}$ be the correspoding Minkowski functional $$\| x \|_{K} = \inf\{\lambda > 0 : x \in \lambda K \}$$ Let us consider the following map $f: K \rightarrow \partial \mathring K$ 
 such that $f(x) = \nabla \| x \|_{K}$ Here $\partial \mathring K$ stands for the polar set, i.e.
$$\partial \mathring K = \{ y \in \partial K : \sup_{x \in \partial K}{\langle x, y \rangle} \leq 1 \}$$ It is pointed out that $f$ pretends to be a Gauss map $v_{K}: \partial K \rightarrow S^{n-1}$ that maps the outer unit normal to the boundary to the unit sphere. Are there any easy ways to recover it geometrically? It looks as if there is a direct relationship between the fact that the subgradients of convex functions are presicely the outer normal vector of supporting hyperplanes of sublevel sets and the statement above, but i can't see any fast ways to figure it out.","['functional-analysis', 'optimal-transport', 'convex-analysis']"
2319270,Integration – what should I do next?,"$$\iint_\Omega (x^2+3y^3)\mathrm dx \mathrm dy $$
$$\Omega: 0\le x^2+y^2\le 1$$
I integrated it twice and then got this $$Cy+\frac{x^2y^2}{4}+xy+c  $$ What should I do next?","['integration', 'calculus']"
2319305,Solve transport equation $\frac{\partial\phi}{\partial t}+\phi\frac{\partial\phi}{\partial x}=0$ using method of characteristics,"I'm trying to solve the following transport equation $$\frac{\partial\phi}{\partial t}+\phi\frac{\partial\phi}{\partial x}=0$$ subject to the initial condition $$\phi(x,0)=f(x)=\left\{
  \begin{array}{l l}
    1,\quad x<0 \\
    0, \quad x\geq0 
\end{array} \right. $$ This is my solution: $\frac{dx}{dt}=\phi \Rightarrow x=\phi t+C$. At $t=0$ let $x=s$. Therefore $x=\phi t+s$. So along a characteristic we have $s=x-\phi t$. For $s<0$, $\phi=1$ so $s=x-t$. For $s\geq0$, $\phi=0$ so $s=x$. $\frac{d\phi}{dt}=0 \Rightarrow \phi(x,t)=f(x)$ and given $\phi=f(s)$ at $x=s,t=0$ we find that $$u(x,t)=f(s)=\left\{\begin{array}{l l}
    1,\quad s<0 \\
    0, \quad s\geq0 
\end{array} \right.$$ $$\Rightarrow \phi(x,t)=\left\{\begin{array}{l l}
    1,\quad x-t<0 \\
    0, \quad x\geq0 
\end{array} \right.$$ Apparently the answer is $$ \phi(x,t)=\left\{\begin{array}{l l}
    1,\quad t>2x \\
    0, \quad t<2x 
\end{array} \right.$$ I'm not sure where I went wrong. Can someone please tell me where my working is wrong?","['ordinary-differential-equations', 'characteristics', 'partial-differential-equations']"
2319347,How to find such an isotropic space?,"Suppose $(V,f)$ is a nonsingular orthogonal geometry with dimension $n$, and $W$ is a totally isotropic space with dimension $r$. How to find such a totally isotropic space $N$ 
 with dimension $r$ such that 1)$V=W^\bot  \oplus N$ 2)$V=W \oplus H\oplus N$ where $H=W^\bot \cap N^\bot$ In addition, if the first condition holds, then the second one is true. Suppose there's a base $\{a_1,\dots,a_s,a_{-1},\dots,a_{-(n-s)}\}$ that $f$ under this base is $\begin{pmatrix} I_s& 0\\0&-I_{n-s}\end{pmatrix}$. And suppose $W=\langle a_1+a_{-1},...,a_r+a_{-r}\rangle$. Then how to construct such a $N$? It seems that $N=\langle a_1+a_{-(n-2r+1)}, \dots ,a_r+a_{-(n-r)}\rangle $ satisfies the conditions, but I not pretty sure. Am I wrong or right? If I'm right, then how to prove it? Is there an clearer way to see this? Thanks in advance. $\textbf{EDIT}$:After reading some suggestions on meta, I realized that I was not explicit with my post and efforts. As I've mentioned above, I $\textbf{seemed}$ to find an $N$ that satisfied the conditions. But when I examined this $N$ in the general case $n$, I found that the calculation was too difficult. So I worked with some simple cases when $n=5$, $r=s=2$ and $n=9$, $r=s=3$. In these cases, the first condition is always satisfied. But as for the second one, it's not clear because the basis of $H$ is really complicated. So I'm thinking that my answer might be wrong or I chose a ""complicated"" basis. Sorry that this might be a silly question. Thanks again.","['abstract-algebra', 'quadratic-forms', 'linear-algebra', 'vector-spaces']"
2319382,Determinant in inner product space,"Looking at the vector space $\Bbb C^n$ with the standart inner product, Let $v_1,...v_n \in \Bbb C^n$ and $A \in M_n(\Bbb C)$ a matrix with columns $v_1,...,v_n$. Prove that: $$ \lvert detA\rvert \leq \prod_{j=1}^n ||v_j||$$ Furthermore, prove that equality holds if and only if $v_1,...,v_n$ is an orthogonal sequence.","['orthogonality', 'inner-products', 'linear-algebra', 'determinant']"
2319393,"What does the partial derivative notation $\frac{\partial (f,g)}{\partial (x,y)}$ mean?","I am currently reading an old math book which contains the following unexplained notation: Let $f(x,y)$ and $g(x,y)$ be functions $\mathbb{R}^2\rightarrow \mathbb{R}$. The notation $$\frac{\partial(f,g)}{\partial(x,y)}$$ apparently refers to a function of the form $\mathbb{R}^2\rightarrow \mathbb{R}$. I am not sure, but I suspect it may be defined as $$\frac{\partial(f,g)}{\partial(x,y)} \equiv \frac{\partial f}{\partial x}\frac{\partial g}{\partial y}-\frac{\partial f}{\partial y}\frac{\partial g}{\partial x}.$$ Is this notation/definition common in any particular field? And especially if so, could there be an obvious interpretation of the following notation, which this book also uses without explanation? $$\frac{\partial[f,g]}{\partial(x,y)}$$ Thanks for your help. Edit: I believe @Fred is correct that the parentheses are used to denote the Jacobian.
Here is the notation, as used in a simplified excerpt of Calculating Curves by Ron Doerfler and others: $$
\left\{ \begin{array}{c} 0 = \frac{\partial u}{\partial y} f_1(x) +
\frac{\partial v}{\partial y}\\ 0 = \frac{\partial u}{\partial x}
 f_2(y) + \frac{\partial v}{\partial x}\\ \end{array}\right.$$ Let us assume that $\frac{\partial(u,v)}{\partial(x,y)}=0$. Then the
  above equations yield that $$\frac{\partial u}{\partial x}\frac{\partial u}{\partial y}[f_1(x) -
 f_2(y)] = 0.$$ Thus we can posit that $$\frac{\partial(u,v)}{\partial(x,y)} = \frac{\partial u}{\partial
x}\frac{\partial v}{\partial y}-\frac{\partial v}{\partial
 x}\frac{\partial u}{\partial y} = e^\theta.$$ I am still unsure about the meaning of the square bracket notation. The  square brackets are a little more difficult to place in context, but here is an attempt: $$g_3(z) = u f_3(z) + v$$ We clearly have $\frac{\partial[g_3(z), z]}{\partial(x,y)} = 0$. By substituting the above equation and observing that $\frac{\partial[f_3(z), z]}{\partial(x,y)} = 0$, we obtain $$f_3(z)\frac{\partial(u,z)}{\partial(x,y)} + \frac{\partial(v,z)}{\partial(x,y)} = 0.$$ Possibly the square brackets are simply an alias for round brackets which are used to avoid potentially visually-noisy nested round brackets(?).","['derivatives', 'partial-derivative']"
2319421,Possible number of bombs in minesweeper game,"This question is about the minesweeper game, and I really don't know how to think about this. Suppose that we are playing the game and we have already opened some number of cells. Then the closed cells which are adjacent to certain set of open cells form a group of cells, which I call clusters. Here are some examples of clusters (closed cells inside the black borders): For each cluster $C$ we can consider the set $M(C)$ of possible number of mines it can contain. For example, on the image above, for the cluster adjacent to cells with numbers 2 and 3, this set is $\{3, 4, 5\}$. Suppose that we have some cluster $C$ and $a,b$ are respectively the possible minimum and maximum number of mines it can contain, and moreover $b\geq a+2$. Is it true that for every integer $c\in(a,b)$ it can contain exactly $c$ mines?",['combinatorics']
2319426,"Prove that if $a\equiv b \pmod m $ , then $a \bmod m = b \bmod m$ [duplicate]","This question already has answers here : Congruent iff Same Remainder (CISR) Confusion (2 answers) Closed 3 years ago . Question Prove that if $a\equiv b \pmod  m$ , then $a \bmod  m = b \bmod m$ Approach Given, $a\equiv b \pmod m$ $\implies m\mid (a-b)$ $\implies (a-b)=m\cdot k$ $\implies a=b+m\cdot k$ Now, $a \bmod  m$ can be written as $\implies b+m\cdot k  \pmod m$ No idea how to move forward to get $$b \pmod m \impliedby b +m\cdot k  \pmod m$$ Please help me out!!! Thanks!","['number-theory', 'discrete-mathematics', 'elementary-number-theory']"
2319438,Evaluate the integral $ \int _0^{+\infty} \frac{x^m}{(a+bx^n)^p}$,"I'm trying to evaluate the following integral: $$ \int _0^{+\infty} \frac{x^m}{(a+bx^n)^p}$$ $a>0, b>0, n>0$ Could you please say if my reasoning is correct? I thought that it was a good idea to use Chebyshev theorem on the integration of binomial differentials to evaluate the integral. Then I change the integral to the form:
$$ \int _0^{+\infty} x^m(a+bx^n)^{-p}$$ However, the task gives no information regarding $p$ and whether $m, n \in \Bbb{Z} $. So I need to look at 4 cases: $p \in \Bbb{Z}$ Then use the substitution $x=t^r$ ,  $dx=rt^{r-1}$ where $r$ is common denominator of rational numbers $m$ and $n$. $\frac{m+1}{n} \in \Bbb{Z}$ Then use the substitution $t=\sqrt[r]{a+bx^n}$ where $r$ is denominator of $p$. $\frac{m+1}{n}+p \in \Bbb{Z}$ Then use the substitution $t=\sqrt[r]{\frac{a+bx^n}{x^n}}$ where $r$ is denominator of $p$. If all 3 previous cases are not applicable, then it's impossible to evaluate the integral. Is my reasoning correct? I don't really know how to proceed further with these substitutions to evaluate the integral since there are no specific numbers here. Any help is very much appreciated!","['integration', 'calculus']"
2319457,Quotient of normed space ; Isometry,"I am struggling with the following problem:
$T : \mathbb{X} \to \mathbb{Y}$ is a linear operator, and $\mathbb{X}$ , $\mathbb{Y}$ are normed spaces. Show that : $T$ is a ""quotient map"" iff the induced operator $T': \mathbb{X}/\ker(T) \to \mathbb{Y}$ is an isometry. (I'm working with a German textbook and in that terminology a quotient map between normed spaces $\mathbb{X}$ and $\mathbb{Y}$ is a linear operator that maps the open unit ball in $\mathbb{X}$ surjectively onto the open unit ball in $\mathbb{Y}$ ; don't know whether that is a standard). So far I could easily proof that if $T'$ is an isometry $T$ must be a ""quotient map"" , but I can't show the other implication, so I would appreciate a hint . Thanks so far
Daniel","['functional-analysis', 'normed-spaces', 'isometry']"
2319506,Is $df$ exact for a map between manifolds?,"Let $M,N$ be $d$-dimensional Riemannian manifolds, $f:M \to N$ a smooth map.
Then $df \in \Omega^1\big({M,f^*TN}\big)=\Gamma(T^*M \otimes f^*TN)$. Let $\nabla$ be the pullback connection on $f^*TN$ induced by the Levi-Civita connection on $TN$. Does there exist an element $\sigma \in \Gamma(f^*TN)$ such that $\nabla \sigma=df$? Denote by $d_{\nabla}:\Omega^k\big({M,f^*TN}\big) \to \Omega^{k+1}\big(M,f^*TN\big)$ the associated exterior derivative. Then, if such a $\sigma$ exist, then $-R^{f^*TN} \wedge \sigma =d_{\nabla} d_{\nabla} \sigma=d_{\nabla} \nabla \sigma=d_{\nabla} df=0$, where the last equality comes from the symmetry of the connection on $TN$. Thus, $R^{f^*TN} \wedge \sigma =0$ is a necessary condition for such a $\sigma$. ($R^{f^*TN}$ is the curvature tensor of $\nabla$).","['vector-bundles', 'differential-forms', 'riemannian-geometry', 'differential-geometry']"
2319508,Geometric interpretation of k-th power of first n natural numbers and summation using Pick's theorem,"I want to know is there any interesting properties of this approach or generalization to find  $S_k(n)=1^k+2^k+3^k+\cdots+n^k$ by using Pick's Theorem $S=i+\tfrac{b}{2}-1$, where $i$-number of interior points and $b$ - number of boundary points and the following geometric interpretation: $(\textbf{1})$ - Let consider $S_1(n)$ then for first steps: \begin{array}{|c|c|c|}
\hline
n& b_n & i_n \\ \hline
 1& 4& 0\\ \hline
 2&  8& 0\\ \hline
 3&  12& 1\\ \hline
 4&  16& 3\\ \hline
 5&  20& 6\\ \hline
 ...&  ...& ...\\ \hline
 n&  4n& \tfrac{(n-1)(n-2)}{2}\\ \hline
\end{array} $(\textbf{2})$ - Let consider $S_2(n)$ then for first steps: \begin{array}{|c|c|c|}
\hline
n& b_n & i_n \\ \hline
 1& 4& 0\\ \hline
 2&  10& 1\\ \hline
 3&  18& 6\\ \hline
 4&  28& 17\\ \hline
 5&  40& 36\\ \hline
 ...&  ...& ...\\ \hline
 n&  n(n+3)& \tfrac{(n-1)(n^2+n-3)}{3}\\ \hline
\end{array} $(\textbf{3})$ - Let consider $S_3(n)$ then for first steps: \begin{array}{|c|c|c|}
\hline
n& b_n & i_n \\ \hline
 1& 4& 0\\ \hline
 2&  14& 3\\ \hline
 3&  30& 22\\ \hline
 4&  52& 75\\ \hline
 ...&  ...& ...\\ \hline
 n&  n(3n+1)& \tfrac{(n-1)(n+1)(n^2+2n-4)}{4}\\ \hline
\end{array}
$...$ $(\textbf{k})$ - Let consider $S_k(n)$ :$\textbf{???}$ So, $\textbf {the questions}$ are: How to find general formulas for $b_n$ and $i_n$ for any step $k$ when we depict our sums as follows: $S_k(n)=1^2\cdot1^{k-2}+2^2\cdot2^{k-2}+3^2\cdot3^{k-2}+...+n^2\cdot n^{k-2}$ - so $n^{k-2}$ times we take $n^2$ and add belows the previous squares? And, the second question: are such constructions involved in any math research or have some interesting properties?","['discrete-geometry', 'combinatorial-geometry', 'combinatorics', 'integer-lattices', 'power-series']"
2319510,Solve the equation $x^4+(x-1)(x^2-2x+2)=0$,With $x \in \mathbb{R}$ . Solve the equation $$x^4+(x-1)(x^2-2x+2)=0$$ Idea $1$ : $(x^2-x+1)(x^2+2x-2)=0$ Idea $2$ : $(2x^2+x-1)^2=(3x-3)^2$ Idea $3$ : $<=>x^4(x-1)x^2-2(x-1)^2=0$ Let $y=x-1 \dots$ I need another way because I am collecting methods of solving with quartic equations,"['algebra-precalculus', 'quartics']"
2319524,Integral Function Paradox,"I have found a result that truly puzzles me in a calculation. 
Assume a function $f(x)$. Then there should be some function $I(x)$ such that $I(f(x)) = \int(f(x)) dx$
Taking the derivative with respect to x would yield:
$I'(f(x)) f'(x) = f(x)$
But this does not make sense does it? Because in this respect
$f'(x) = 0"" $ would imply $f(x) = 0""$, which it does not of course. So where is my mistake?","['calculus', 'functions']"
2319560,Geometric proof Brahmagupta-Fibonacci identity,"The Brahmagupta-Fibonacci identity states that for all $a,b,c,d$, $$(a^2+b^2)(c^2+d^2)=(ad-bc)^2+(ac+bd)^2.$$ 
This is easy to prove algebraically, and says essentially that the norm on complex numbers is multiplicative. Is there a geometric proof though? Bak and Newman's textbook ""Complex Analysis"" suggests there is (p.19), but I can't see how it would go.","['complex-numbers', 'geometry']"
2319579,How to obtain uniform bounds on a polynomial by looking at its coefficients.,"To be more precise, let $p:[0,1]\to \mathbb R$ be a polynomial of degree $n$. Picking a basis of polynomials (e.g. monomials based at $0$), we can represent $p$ $$p(x)=\sum_{j=0}^na_jx^j,$$ for some unique coefficients $a_j\in \mathbb R$. Now, as $p$ is a continuous function on a compact set, it is bounded. I would like to be able to obtain relatively sharp information about the bound on $p$ by looking at the above coefficients, $a_j$. For example, one has $$\|p\|_{\infty}\leq\sum_{j=1}^n |a_j|\leq n\max_j|a_j|.\quad(\star)$$ But in general, this will clearly be a bad bound as one can have polynomials with many large (in absolute value) coefficients which manages to be small due to cancellations of positive and negative coefficients. So my (unfortunately rather vague) question is how can one get bounds sharper than the brute-force $(\star)$? Ideally, something that doesn't rely upon the one-dimensional nature of the domain. Perhaps [see Olivier below] my life would be easier with a different choice of basis polynomials?","['real-analysis', 'polynomials', 'analysis']"
2319583,Group presentation of a direct product.,"Suppose $G_1=\langle X_1|R_1\rangle$ , $G_2=\langle X_2|R_2\rangle$ , $X_1\cap X_2=\emptyset$ . I want to show that $G_1\times G_2=\langle X_1\cup X_2|R_1\cup R_2\cup[X_1,X_2] \rangle$ . By using universal property of free groups, I have obtained homomorphisms $f_i:F(X_i)\rightarrow F(X_1 \cup X_2)$ . Also I can define homomorphisms $\pi_i:F(X_i) \rightarrow G_i$ such that $\ker(\pi_i)= R_i^{F(X_i)}=$ normal closure of $R_i$ . I want to define a group $G = \langle X_1\cup X_2|R\rangle$ with $R$ satisfying the conditions of direct product, and somehow $R$ contains $[X_1,X_2]$ . But I'm kind of stuck here.","['combinatorial-group-theory', 'direct-product', 'group-theory', 'group-presentation']"
2319613,Change integration order,"I am confused due to graphics $$\int_0^2\mathrm{d}x\int_{x}^{2x}f(x,y)\,\mathrm{d}y$$ well, for reverse order we have to find $x=y$ and $x=\frac{y}2{}$ as a functional limits for $dx$ but I do not know how determine number limits for $dy$ Plot does not make things clear: How should I handle?","['multivariable-calculus', 'multiple-integral']"
2319655,When is a Sudoku like table solvable,"Given a $n\times n$ table is it possible to fill each cell with one of the numbers $1,2,3,\cdots,n$ such that in each column,each row and each diagonal (i.e Denoting $(x,y)$ as number of column and row $(2,1)$ and $(1,2)$ form the first diagonal) every number appears exactly once? For which $n$ can we fill the table? Context : I've been given this problem on a contest few months ago but just for $n=4,5$ which I solved easily since $n=4$ is impossible and for $n=5$ we have
$$$$\begin{array}{|c|c|c|c|c|} \hline
    1&2&3&4&5\\  \hline
3&4&5&1&2\\  
   \hline
5&1&2&3&4\\  \hline
2&3&4&5&1\\ \hline
4&5&1&2&3\\ \hline\end{array}$$$$
But I was interested in a more general statement I think I've also proved that for $n=6$ it's impossible by trying to fill the table manually. My guess is that for even $n$ it's not solvable and for odd $n$ it's solvable but I have no idea how to approach it except to fill it manually. EDIT: For prime $n$ we can fill each cell $(i,j)$ with $i+2j\pmod{n}$ except when $i+2j\equiv0\pmod{n}$ then we write $n$ instead for example such filling with $n=7$ (the $n=5$ example is the same filling if you look at $(j,i)$ instead of $(i,j)$)
$$$$\begin{array}{|c|c|c|c|c|c|c|} \hline
    3&5&7&2&4&6&1\\  \hline
4&6&1&3&5&7&2\\  
   \hline
5&7&2&4&6&1&3\\  \hline
6&1&3&5&7&2&4\\ \hline
7&2&4&6&1&3&5\\\hline 1&3&5&7&2&4&6\\\hline2&4&6&1&3&5&7\\\hline\end{array}$$$$ PROOF OF THE EDIT: For the same row if cells $(i_1,j)$ and $(i_2,j)$ have the same value we have that $$i_1+2j\equiv i_2+2j\pmod{n}$$ implies $i_1\equiv i_2$ which is possible only if $i_1=i_2$. Same logic applies to the column for cells $(i,j_1),(i,j_2)$ we get $$i+2j_1\equiv i+2j_2\pmod{n}$$ when $n$ is prime it implies $j_1=j_2$ if $(i_1,j_1),(i_2,j_2)$ are on a diagonal we have $$|i_1-i_2|=|j_1-j_2|$$ now assuming they have the same value $$i_1+2j_1\equiv i_2+2j_2\pmod{n}$$ then $i_1-i_2\equiv 2(j_2-j_1)\pmod{n}$ which implies $1\equiv \pm 2\pmod{n}$ which is absurd.","['combinatorics', 'contest-math', 'latin-square', 'recreational-mathematics']"
2319657,"Show that if $\sum\limits_nnE|X_{n}|^2$ converges for uncorrelated, mean zero $X_n$s, then $\sum\limits_{i=1}^{n}X_i$ converges almost surely","Let ($X_n$) be a sequence of uncorrelated random variables of mean zero such that 
$$\sum_{n=1}^{\infty}nE|X_{n}|^2 < \infty $$
Show that $S_n = \sum_{i=1}^{n}X_i$ converges almost surely. This is the second problem of S.-T. Yau College Student Mathematics Contests 2014. I tried to solve the problem by using a similar inequality of Kolmogorov, but how to use the coefficient of $n$ before $E|X_n|^2$ makes me confused.","['probability-theory', 'sequences-and-series', 'convergence-divergence']"
2319661,Borel-Weil-Bott theorem vs the usual formula,"I am confused about the Borel-Weil-Bott theorem. For example, if I try to compute $H^i(\mathbb{P}^5,\mathcal{O}(-8)),$ Borel-Weil-Bott computation is the following: Find the sequence: $\sigma= (0,0,0,0,0,8)+(6,5,4,3,2,1)=(6,5,4,3,2,9)$ There are no repetitions, so not all cohomologies are zero. There are 5 'disarrangements', that is, in this sequence there are 5 pairs of numbers that are not decreasing. We should sort $\sigma$: $\tilde{\sigma}=(9,6,5,4,3,2)$ and subtract $(6,5,4,3,2,1)$, the result is $(3,1,1,1,1,1).$ Finally, the answer is $H^5(\mathbb{P}^5,\mathcal{O}(-8))=W_{(3,1,1,1,1,1)}.$ But we know that $H^5(\mathbb{P}^5,\mathcal{O}(-8))=Sym^2(V)=W_{(2,0,0,0,0,0)}.$ I don't understand why the results are different.","['sheaf-cohomology', 'projective-space', 'representation-theory', 'algebraic-geometry']"
2319667,Cofinite\discrete subspace of a T1 space?,"Let $(X,\tau)$ be a $T_1$ -space and $X$ is an infinite set.  Then $(X,\tau)$ has a subspace homeomorphic to $(\mathbb{N},\tau_2)$ , where $\tau_2$ is either the finite-closed topology or the discrete topology. Update attempt: As suggested from Daniel Fischer's comment, a solution is presented in the answer section.","['general-topology', 'separation-axioms']"
2319680,Double orthogonal complement of a finite dimensional subspace,"Let $\mathbb{W}$ be a finite dimensional subspace of an inner product space $\mathbb{V}$. Prove or disprove the following: the double orthogonal complement of $\mathbb{W}$ is equal to itself, or, in other words, $(\mathbb{W}^\perp)^\perp = \mathbb{W}.$ We might begin a potential proof as follows. If $\vec{v} \in \mathbb{W}$, then $\langle \vec{v}, \vec{w}\rangle = \vec{0}$ for any vector $\vec{w} \in \mathbb{W}^\perp.$ Therefore, by definition, $\vec{v} \in (\mathbb{W}^\perp)^\perp$, and so $\mathbb{W} \subseteq (\mathbb{W}^\perp)^\perp.$ Now, it's easy to prove equality if $\mathbb{V}$ is finite dimensional. For instance, we can use the fact that $\dim \mathbb{W} + \dim \mathbb{W}^\perp = \dim \mathbb{V}$ to show that $\dim \mathbb{W} = \dim (\mathbb{W}^\perp)^\perp$ which implies the desired result. However, in this case, $\mathbb{V}$ is not necessarily finite dimensional and so this step is not valid. In the same vein, if we were to relax the condition that $\mathbb{W}$ must be finite dimensional, then the statement is false. Let $\mathbb{V}$ be the inner product space of all polynomials with real coefficients under the inner product
$$
\langle a_0 + a_1x + \dots + a_n x^n, b_0 + b_1x + \dots + b_k x^k\rangle = a_0b_0 + a_1b_1 + \dots + a_jb_j
$$
where $j$ is the lesser of $n$ and $k$. Then, the subspace 
$$
\mathbb{W} = \{ f(x) \in \mathbb{V} \mid f(1) = 0\}
$$
of $\mathbb{V}$ has orthogonal complement $\{ \vec{0}\}$ which shows that $(\mathbb{W}^\perp)^\perp = \mathbb{V} \neq \mathbb{W}$. The question, then, is: does there exist a counter example where $\mathbb{V}$ is infinite dimensional, but the subspace $\mathbb{W}$ is not? My intuition tells me yes, but I'm stuck.","['inner-products', 'linear-algebra', 'vector-spaces']"
2319734,What kind of a property implies (sequentially compact $\iff$ compact)?,"It is well known that the topology of metric spaces and topological notions in metric spaces are pretty much entirely determined by sequences. For example one has: The closure of a set is the same as the limit points of sequences in the set. A map whose domain is our space is continuous iff it is sequentially continuous. A subset is compact iff it is sequentially compact. In first countable spaces or, more generally, sequential spaces the first two statements are still valid. The third is not necessarily true, for example the long ray is first countable and sequentially compact but certainly not compact. In what kind of spaces is property 3 valid? This question is a bit vague. For example property 1 is valid in sequential spaces, but the definition of sequential is exactly that property 1 holds. A prettier answer, if the question would have been about property 1, would be first countable spaces. The definition of first countable is not artificial in this context and covers a very large class of spaces. If you have any further examples, it would be nice to know: Are there other sequential notions that are equivalent to a topological notion in metric spaces but not necessarily equivalent in first countable spaces?","['general-topology', 'metric-spaces', 'sequences-and-series', 'compactness']"
2319741,"Theorem 6.10 in Baby Rudin: If $f$ is bounded on $[a, b]$ with only finitely many points of discontinuity at which $\alpha$ is continuous, then","Here is Theorem 6.10 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is bounded on $[a, b]$, $f$ has only finitely many points of discontinuity on $[a, b]$, and $\alpha$ is continuous at every point at which $f$ is discontinuous. Then $f \in \mathscr{R}(\alpha)$. Here $\alpha$ is a monotonically increasing function, and by $f \in \mathscr{R}(\alpha)$ we mean the integral $\int_a^b f(x) \mathrm{d} \alpha(x)$ exists. First of all, here are Definitions 6.1 and 6.2 in Baby Rudin, 3rd edition: Definition 6.1: Let $[a, b]$ be a given interval. By a partition $P$ of $[a, b]$ we mean a finite set of points $x_0, x_1, \ldots, x_n$, where 
  $$ a = x_0 \leq x_1 \leq \cdots \leq x_{n-1} \leq x_n = b.$$
  We write 
  $$ \Delta x_i = x_i - x_{i-1} \qquad (i = 1, \ldots, n). $$ 
  Now suppose $f$ is a bounded real function defined on $[a, b]$. Corresponding to each partition $P$ of $[a, b]$ we put
  $$
\begin{align}
 M_i &= \sup f(x) \qquad (x_{i-1} \leq x \leq x_i), \\
m_i &= \inf f(x) \qquad (x_{i-1} \leq x \leq x_i), \\
U(P, f) &= \sum_{i=1}^n M_i \Delta x_i, \\
L(P, f) &= \sum_{i=1}^n m_i \Delta x_i,
\end{align}
 $$
  and finally 
  $$ 
\begin{align}
\tag{1} \overline{\int_a^b} f dx &= \inf U(P, f), \\
\tag{2} \underline{\int_a^b} f dx &= \sup L(P, f),\\\,
\end{align}
$$
  where the $\inf$ and the $\sup$ are taken over all partitions $P$ of $[a, b]$. The left members of (1) and (2) are called the upper and lower Riemann integrals of $f$ over $[a, b]$, respectively. If the upper and lower integrals are equal, we say that $f$ is Riemann-integrable on $[a, b]$, we write $f \in \mathscr{R}$ (that is, $\mathscr{R}$ denotes the set of Riemann-integrable functions), and we denote the common value of (1) and (2) by 
  $$ \tag{3} \int_a^b f dx, $$
  or by 
  $$ \tag{4} \int_a^b f(x) dx. $$
  This is the Riemann integral of $f$ over $[a, b]$. Since $f$ is bounded, there exist two numbers, $m$ and $M$, such that 
  $$ m \leq f(x) \leq M \qquad (a \leq x \leq b). $$
  Hence, for every $P$, 
  $$ m(b-a) \leq L(P, f) \leq U(P, f) \leq M (b-a), $$
  so that the numbers $L(P, f)$ and $U(P, f)$ form a bounded set. This shows that the upper and lower integrals are defined for every bounded function $f$. . . . Definition 6.2: Let $\alpha$ be a monotonically increasing function on $[a, b]$ (since $\alpha(a)$ and $\alpha(b)$ are finite, it follows that $\alpha$ is bounded on $[a, b]$). Corresponding to each partition $P$ of $[a, b]$, we write 
  $$ \Delta \alpha_i = \alpha \left( x_i \right) -  \alpha \left( x_{i-1} \right). $$
  It is clear that $\Delta \alpha_i \geq 0$. For any real function $f$ which is bounded on $[a, b]$ we put 
  $$ 
\begin{align}
U(P, f, \alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i, \\
L(P, f, \alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i, 
\end{align}
$$
  where $M_i$, $m_i$ have the same meaning as in Definition 6.1, and we define 
  $$
\begin{align}
\tag{5} \overline{\int_a^b} f d \alpha = \inf U(P, f, \alpha), \\
\tag{6} \underline{\int_a^b} f d \alpha = \sup L(P, f, \alpha), \\\,
\end{align}
$$
  the $\inf$ and $\sup$ again being taken over all partitions. If the left members of (5) and (6) are equal, we denote their common value by 
  $$ \tag{7} \int_a^b f d \alpha $$
  or sometimes by 
  $$ \tag{8} \int_a^b f(x) d \alpha(x). $$
  This is the Riemann-Stieltjes integral (or simply the Stieltjes integral ) of $f$ with respect to $\alpha$, over $[a, b]$. If (7) exists, i.e., if (5) and (6) are equal, we say that $f$ is integrable with respect to $\alpha$, in the Riemann sense, and write $f \in \mathscr{R}(\alpha)$. And, here is Rudin's proof: Let $\varepsilon > 0$ be given. Put $M = \sup \left\vert f(x) \right\vert$, let $E$ be the set of points at which $f$ is discontinuous. Since $E$ is finite and $\alpha$ is continuous at every point of $E$, we can cover $E$ by finitely many disjoint intervals $\left[ u_j, v_j \right] \subset [a, b]$ such that the sum of the corresponding differences $\alpha\left(v_j\right) - \alpha \left( u_j \right)$ is less than $\varepsilon$. Furthermore, we can place these intervals in such a way that every point of $E \cap (a, b)$ lies in the interior of some $\left[ u_j, v_j \right]$. Remove the segments $\left( u_j, v_j \right)$ from $[a, b]$. The remaining set $K$ is compact. Hence $f$ is uniformly continuous on $K$, and there exists $\delta > 0$ such that $\left\vert f(s) - f(t) \right\vert < \varepsilon$ if $s \in K$, $t \in K$, $\left\vert s-t \right\vert < \delta$. Now form a partition $P = \left\{ x_0, x_1, \ldots, x_n \right\}$ of $[a, b]$, as follows: Each $u_j$ occurs in $P$. Each $v_j$ occurs in $P$. No point of any segment $\left( u_j, v_j \right)$ occurs in $P$. If $x_{i-1}$ is not one of the $u_j$, then $\Delta \alpha_i < \delta$. Note that $M_i - m_i \leq 2M$ for every $i$, and that $M_i - m_i \leq \varepsilon$ unless $x_{i-1}$ is one of the $u_j$. Hence, as in the proof of Theorem 6.8, 
  $$ U(P, f, \alpha) - L(P, f, \alpha) \leq \left[ \alpha(b) - \alpha(a) \right] \varepsilon + 2M \varepsilon.$$
  Since $\varepsilon$ is arbitrary, Theorem 6.6 shows that $f \in \mathscr{R}(\alpha)$. Here is Theorem 6.8 in Baby Rudin, 3rd edition: If $f$ is continuous on $[a, b]$, then $f \in \mathscr{R}(\alpha)$ on $[a, b]$. And, here is Rudin's proof: Let $\varepsilon > 0$ be given. Choose $\eta > 0$ so that 
  $$ \left[ \alpha(b) - \alpha(a) \right] \eta < \varepsilon.$$
  Since $f$ is uniformly continuous on $[a, b]$ (Theorem 4.19), there exists a $\delta > 0$ such that 
  $$ \vert f(x) - f(t) \vert < \eta \tag{16}$$
  if $x \in [a, b]$, $t \in [a, b]$, and $\vert x-t \vert < \delta$. If $P$ is any partition of $[a, b]$ such that $\Delta x_i < \delta$ for all $i$, then (16) implies that 
  $$ M_i - m_i \leq \eta \qquad (i = 1, \ldots, n) \tag{17} $$
  and therefore 
  $$ U(P, f, \alpha) - L(P, f, \alpha) = \sum_{i=1}^n \left( M_i - m_i \right) \Delta \alpha_i \leq \eta \sum_{i=1}^n \Delta \alpha_i = \eta \left[ \alpha(b) - \alpha(a) \right] < \varepsilon. $$
  By Theorem 6.6, $f \in \mathscr{R}(\alpha)$. Here is Theorem 6.6 in Baby Rudin, 3rd edition: $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon > 0$ there exists a partition $P$ such that 
  $$ U(P, f, \alpha) - L(P, f, \alpha) < \varepsilon.$$ Finally, here is Theorem 4.19 in Baby Rudin, 3rd edition: Let $f$ be a continuous mapping of a compact metric space $X$ into a metric space $Y$. Then $f$ is uniformly continuous on $X$. Now I have the following questions: Can we make Rudin's proof of Theorem 6.10 more explicit and rigorous (perhaps by modifying its presentation in some way)? And, is there any alternative proof of this very theorem (preferably using the same machinary that Rudin has developed so far in the book)?","['real-analysis', 'riemann-integration', 'integration', 'definite-integrals', 'analysis']"
2319746,What is the significance of the $\mathbb Z_2 \times \mathbb Z_2$ subgroup of $SO(3)$?,"If one thinks of $SO(3)$ as the rotations in three-dimensional space, then the $\mathbb Z_2\times \mathbb Z_2$ subgroup is given by the 180 degree rotations along three perpendicular axes (which indeed commute). On a mathematical level this just seems like --at least on first sight-- one of the many finite subgroups of $SO(3)$ . However, I am a physicist, and there this subgroup plays an important role. In particular, physicists are interested in the fact that $H^2_\textrm{group}(SO(3);U(1)) \cong H^2_\textrm{group}(\mathbb Z_2 \times \mathbb Z_2; U(1))$, i.e. if one is interested in the distinct classes of projective representations of $SO(3)$, it is sufficient to focus on $\mathbb Z_2\times \mathbb Z_2 \subset SO(3)$. (This ties into something physicists call topological phases of matter; in fact this particular piece of math is relevant for the recent Nobel prize.) So at least from this perspective , $\mathbb Z_2\times \mathbb Z_2$ can be seen as a sort of `skeleton' of $SO(3)$ (in purely figurative language). So I was wondering: is there something mathematically significant about the $\mathbb Z_2 \times \mathbb Z_2$ subgroup of $SO(3)$? I.e. is there some natural characterization which defines it as a subgroup, leading to a notion that is applicable to more general groups? (Or is the fact that it is has the same [second] group cohomology simply an uninteresting and limited curiosity?) Is its relation to the generators of $SO(3)$ of particular importance?","['group-cohomology', 'group-theory', 'lie-algebras', 'lie-groups']"
2319759,How do you find the most possible combinations in a word problem? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question In a word problem, they say a guy has 14 cats, dogs, and Guiana pigs. What are all the possible combinations the guy could have? I don't understand how to solve this problem. I tried my own ways but none of them work. Can you please help me?",['combinatorics']
2319777,Is this an ellipse?,"Is this parameterisation an ellipse:
  \begin{align}x(t) &= \frac{2 \cos(t)}{1 + a \sin(t)}\\
y(t) &= \frac{2 \sin(t)}{1 + a \sin(t)}\end{align}
   where $a$ is a real positive parameter. I tried to do it the naive way but couldn't find a definitive answer. Plotting our curve with the help of Geogebra gives the following very ellipse like graph: Any help would be appreciated.","['conic-sections', 'parametrization', 'differential-geometry', 'analytic-geometry']"
2319840,$\frac {\pi} {1+\frac {\pi} {2+\frac {\pi} {3+\frac {\pi} {4+\frac {\pi} {\ddots}}}}}$,"$$\cfrac \pi {1+\cfrac \pi {2+\cfrac \pi {3+\cfrac \pi {4+\cfrac {\pi} {\ddots}}}}}$$ I don't want a solution to this. I just want to know to begin ! I've been trying for almost an hour now. Thanks for the help EDIT 1 : Here is what I tried : $f(n) = n + \frac {\pi} {f(n+1)}$ $f(n+1)(f(n)-n) = \pi$ $f(1)f(0)=\pi$ since, $\pi$ is the factor of two numbers. So one must be 1 and other must be $\pi.$ $f(2)(f(1)-1)=\pi$ If $f(1)=1$, then, $f(2)(1-1)=\pi$ which is impossible. $So, f(0) = 1$ must be True! Hence the answer is 1 Can someone please verify ? Thanks :)","['continued-fractions', 'sequences-and-series']"
2319874,Geodesics in Riemannian Manifolds,"Suppose you wanted to know the geodesic between two points in a manifold, where the points do not exist in the same chart. Would the fact that they are not in the same chart make a difference? How would you work out the components of the metric when they are dependent on locally defined tangent vectors but your curve exists in multiple coordinate charts?","['metric-spaces', 'riemannian-geometry', 'differential-geometry']"
2319882,How to express a hypergeometric function as beta and/or gamma functions,"The CDF of the student t distribution can be represented by $$\frac{1}{2} \cdot \beta_{x^2/(x^2+v)}\left(\frac 1 2,\frac v 2\right); \qquad x\in[-\sqrt{v},0]$$ Where we have a t distribution with $v>0$ degrees of freedom and $\beta_z(a,b)$ is the incomplete beta function. I'm interested in it's derivative w.r.t. $v$, which happens to contain the term $$_3\tilde{F}_2\left(\frac{v}{2},\frac{v}{2},-\frac{1}{2};\frac{v}{2}+1,\frac{v}{2}+1;1-\frac{x^2}{x^2+v}\right)$$ I was able to get to this point using wolfram's derivatives of the incomplete beta function . However, for this to be useful to me, I would like to express it in terms of functions I'm more familiar with such as the gamma, polygamma, and beta functions. How might this be done? Also, how do the $\tilde{F}$ functions differ from the $F$ functions, and what are the $\tilde{F}$ functions called? I couldn't find any information on them at all.","['complex-analysis', 'statistics']"
2319885,Explicit Connection on Frame bundle $F\mathbb{R}$,"I am trying to write down all the connections on the simple frame bundle $F\mathbb{R} \rightarrow \mathbb{R}$ as a choice of horizontal subspaces, but I get stuck. Please feel free to correct any of my statements, also I provide only results as I feel confident with the following derivations. Let $F\mathbb{R} \rightarrow \mathbb{R}$ be the frame bundle where $F\mathbb{R}\xrightarrow{\sim} \mathbb{R}\times\mathbb{R}_*$ and the projection map $\pi : \mathbb{R}\times\mathbb{R}_* \rightarrow \mathbb{R}$ the obvious projection onto the first factor. We consider the group $GL(1,\mathbb{R})$ acting on the left. We say that a tangent vector is in the vertical subspace if it is in $ker(\pi_*)$ where $\pi_*$ is the push forward map. Or similarly if the tangent vector is in the form $X^A_pf = \frac{\partial}{\partial t}\big|_0 f(p.exp(tA))$ where A is in the Lie algebra of the group.
So : 1) I found that for $X_p = f(a,b)^i\frac{\partial}{\partial x^i}\big|_{(a,b)} \implies \pi_*(X_p) = f(p)^1\frac{\partial}{\partial t}\big|_a$ So we can conclude that $X_p$ is Vertical if and only if $f(p)^1 = 0$. To verify this result I tried to calculate $X^A_p$. For this I need to be able to compute $exp(tA)$ which is defined in my textbook as : $exp: T_eG \rightarrow T_pP$ which in my case is $exp: T_o\mathbb{R} \rightarrow T_{(a,b)}(\mathbb{R}\times \mathbb{R})$ and $exp(A) = \gamma^A(1)$ with $\gamma^A(1)$ the integral curve of the Left invariant vector field generated by A. So I started off by calculating the left invariant vector field : Let $A = a\frac{\partial}{\partial t}\big|_0$ be a vector in the Lie algebra. Define $L_g$ to be the left action : $L_g(a) = a + g$. I found out that $L_{g*}(A) = a\frac{\partial}{\partial t}\big|_g$. Now to calculate the integral curve we should have : $\gamma'(t) = a$ which leads me to having $\gamma(t) = at$ so that $\gamma(1) = exp(A) = a$ So we proceed with $X^A_{(b,c)}f = \frac{\partial}{\partial t}\big|_0 f((b,c).exp(tA)) = \frac{\partial}{\partial t}\big|_0 f((b,c).ta) = \frac{\partial}{\partial t}\big|_0 f((b,cta) = ca\partial_2\big|_{(b,c)}f $ which is indeed equivalent to $f(p)^1 = 0$ since only the second component is non-zero. So since the horizontal subspace is :
\begin{align}
T_{(a,b)}\mathbb{R}\times \mathbb{R}_* = Hor \oplus Ver\\
R_{g*}Hor_{(a,b)} = Hor_{(a,bg)} 
\end{align} I set $X_{(a,b)} = f(a,b)^i\frac{\partial}{\partial x^i}\big|_{(a,b)} \in Hor \implies R_{g*}(X_{(a,b)}) = f(a,b)^1\frac{\partial}{\partial x^1}\big|_{(a,bg)} + gf(a,b)^2\frac{\partial}{\partial x^2}\big|_{(a,bg)}$ So we have the following constraints : $f(a,bg)^1 = f(a,b)^1 \implies f(a,b)^1 = f(a,1)^1$ $f(a,bg)^2 = gf(a,b)^2 \implies f(a,g)^2 = gf(a,1)^2$ From here I do not know what does it mean to :
1) Make a specific choice for Hor , for me Hor is completely determined bu the previous constraints.
2) How to project a tangent vector to Ver and Hor ? and How does that depend on the specific choice of Hor ? ( I need this to compute an explicit equation for the connection form $\omega_p = i^{-1}\circ Ver$. Thanks. EDIT :
What is p in the third paragraph ? p is actually a point in the frame bundle $F\mathbb{R}$, so concretely $p\in \mathbb{R}\times \mathbb{R}_*$. p was a bad choice of notation. What is $exp(tA)$ This is a lie group element generated by the lie algebra element A. This is the Lie algebra of the Lie group $GL(1,\mathbb{R})$. We consider the left invariant vector field generated by A, then we find the integral curve $\gamma$ where the tangent vector of the curve at the identity is A, and then $exp(tA) = \gamma(t)$ What is $f(a,b)^i$ Yes this f is different from the f in the previous paragraph. It is the coordinates of the vector in the basis $\frac{\partial}{\partial x^i}\big|_{(a,b)}$. $(a,b)$ is point in the Frame bundle. And yes $p$ is a point in the frame bundle as well, I realize the notation is messy... But they are all point in the Frame bundle. $f(p)^1$ is the first coordinate.","['connections', 'differential-geometry']"
2319889,"For a discrete stopping time $\tau$, $\mathcal{F}_\tau^X = \sigma(X(t\wedge\tau):t\ge 0).$","Let $X$ be a stochastic process, and let $\tau$ be a discrete $\{\mathcal{F}_t^X\}-$stopping time. Show that $$\mathcal{F}_\tau^X = \sigma(X(t\wedge \tau):t\ge 0).$$ I am struggling to find a way to show this identity. I know that $\mathcal{F}_{\tau}^X=\{A\in \mathcal{F}_\infty: A \cap \{\tau\le t\} \in \mathcal{F}_t\}$. But I don't know how to use this fact to prove the above identity. I would greatly appreciate any help.","['stochastic-processes', 'probability-theory', 'stopping-times', 'filtrations']"
2319912,Interpreting the sum as a diffeomorphism,"Consider the manifold $M = \{(x,x^2) : x \in \mathbb{R}\}$ and for $\varepsilon > 0$ let $M_{\varepsilon}  = \bigcup_{p \in M} B_{\varepsilon}(p)$. How can I find $\varepsilon > 0$ such that the map
$$
F \colon NM \cap (\mathbb{R}^2 \times B_{\varepsilon}(0)) \rightarrow M_{\varepsilon} \colon (p,w) \mapsto p + w
$$
is a diffeomorphism? The definition of the normal bundle is
$$
N M = \{ (p,w) \in \mathbb{R}^2 \times \mathbb{R}^2 : p \in M \text{ and } w \in (T_{p} \, M)^{\bot} \}.
$$","['real-analysis', 'differential-topology', 'functions', 'manifolds', 'differential-geometry']"
2319916,Intuition for the need of generalizing from mappings to morphisms to functors in supermathematics?,"I am currently reading this paper about the categorical formulation of superalgebras and supergeometry, where in definition 2.3 it says that to change the parity of a right supermodule a morphism will not do the job as morphisms have to preserve parity and a functor has to be used instead (and the right supermodules are then also objects of the corresponding category). This makes me wanting to (intuitively at first) really know what changes when ramping up the level of abstraction when going from Vector spaces and mappings between them to Modules and morhismes to Objects in a category (?) and functors I would like to get a rather intuitive overview that explains what morphisms can do that ordinary mappings can not and what additional superpowers (pun intended) functors have apart from changing parity compared to morphisms? Reading the definition 2.3 in the paper I was also wondering if supersymmetry transformations should then strictly speaking be functors to ... Even thoug I like the answer I got elsewhere , I would like to learn what the somewhat larger community here has to say too.","['algebraic-geometry', 'supermanifolds', 'category-theory', 'supergeometry', 'differential-geometry']"
2319957,Clean Limit Proof,"While attempting to solve $\int_0^\infty \frac{\sin x}{x} dx$ using Differentation Under the Integral Sign, I have stumbled across the follow limit:
$$\lim_{a \to \infty}\int _0^{\infty}\frac{x\sin \left(ax\right)}{x^2+1}dx \tag{1}$$
Now, this should go to $0$, but I have been struggling to show this cleanly without resorting to Complex Analysis. I have managed to squeeze out a proof using Integration by Parts and letting $u=\frac{x}{x^2+1}$ to get
$$\lim_{a \to \infty}\int _0^{\infty}\frac{x\sin \left(ax\right)}{x^2+1}dx =\lim_{a \to \infty}\frac{1}{2a}\int_{-\infty}^{\infty}\frac{\left(x^2-1\right)\cos \left(ax\right)}{\left(x^2+1\right)^2}dx$$
All that is left is to note that $-1 < \cos(ax) < 1$ and to apply the squeeze theorem. However, I am seeking alternative proofs that are clean and straightforward. Another way I could potentially go about this is by noting
$$\int_0^{\infty}\frac{x\sin \left(ax\right)}{x^2+1}dx = \sum_{n=0}^\infty\left(\int_{2n\pi/a}^{(2n+1)\pi/a}\frac{x\sin \left(ax\right)}{x^2+1}dx\;- \int_{(2n+1)\pi/a}^{(2n+2)\pi/a}\frac{x\sin \left(ax\right)}{x^2+1}dx\right)$$
I could now bound the difference between the two integrals; however, this seems even more tedious than my first attempt. What other real analysis methods can be used to evaluate (1) cleanly and efficiently?","['limits', 'calculus', 'integration', 'definite-integrals', 'sequences-and-series']"
2319969,Is the proof of $\lim_{\theta\to 0} \frac{\sin \theta}{\theta}=1$ in some high school textbooks circular?,"I was taught the following proof in high school. By constructing triangles with $0<\theta<\pi/2$ and a circle with radius $r$ and by comparing the areas, we have
$$\frac{1}{2}r^2\sin\theta\cos\theta \le \frac{1}{2}r^2\theta\le\frac{1}{2}r^2\tan\theta$$
Hence
$$\cos\theta\le\frac{\theta}{\sin\theta}\le\frac{1}{\cos\theta}$$
Then by squeeze theorem, we have the result. My question is, the middle term in the above inequality comes from the fact that the area of the circle is $\pi r^2$, which in my textbooks, is later proved by integration. But the integration requires results in calculus which comes from the fact that
$$\lim_{\theta\to 0}\frac{\sin\theta}{\theta}=1$$",['calculus']
2319979,Proving the sequence $\sqrt{n+1}-\sqrt{n}$ is convergent,"I know the $\epsilon - \delta$ definition of a limit in this case is $\forall \epsilon >0 \exists N\in \mathbb{N}\forall n \in \mathbb{N} (n \geq N \implies \sqrt{n+1}-\sqrt{n}<\epsilon)$. So far, I have been able to show: $$\sqrt{n+1}-\sqrt{n} = \frac{(\sqrt{n+1}-\sqrt{n})(\sqrt{n+1}+\sqrt{n})}{(\sqrt{n+1}+\sqrt{n})} = \frac{1}{\sqrt{n+1}+\sqrt{n}}.$$ This is the part where I find myself stuck. What do I need to do next?","['sequences-and-series', 'limits']"
2319987,Explicit formular for special value of Dedekind zeta functions,Is there any formula for $\zeta_{F}(1-k)$ where $F$ is real quadratic field and $k$ is positive even integers? Here $\zeta_{F}(s)$ is a Dedekind zeta function over a number field $F$.,"['number-theory', 'analytic-number-theory']"
2320047,Why is $\ln(1-x) \approx -x$ when $x$ is small?,I saw this in a proof for the Central Limit Theorem: $\ln(1-x) \approx -x$ when $x$ is small It seems to be true when I plug in small values of $x$. But why does it work?,['statistics']
2320051,Definite integral integration by parts,"Can we write the integration by parts for definite integral the following way: $$\int^a_b f(x)g(x)dx=f(x)\int^a_b g(x)dx-\int^a_b  \left[  \dfrac{df(x)}{dx}\int^a_b g(x)dx   \right]dx   $$ My book gives the following formula for definite integral integration by parts: $$\int^a_b f(x)g(x)dx=\left[f(x)\int g(x)dx\right]^a_b                  
-\int^a_b \left[  \dfrac{df(x)}{dx}\int g(x)dx  \right]dx  $$ Are the two formulas equivalent or not? Why/Why not?","['integration', 'definite-integrals', 'integration-by-parts', 'calculus']"
2320090,Limit of a $p$-adic function and L'Hôpital's rule.,"Let $k$ be a fixed positive integer and $\operatorname{val}_p$ be the $p$-adic valuation on $\mathbb{Z}_p$. Let $n$ be a natural number. My question is finding the following limit: $$\lim_{n \rightarrow \infty}\frac{n^k}{p^{\operatorname{val}_p(n!)}}.$$ I guess that this limit should be zero; because I guess that $\operatorname{val}_p(n!)$ is like $\dfrac{n}{p-1}$ for large $n$ (I know for sure that $\operatorname{val}_p(n!)$ is certainly less than $\dfrac{n}{p-1}$) and I apply the classical L.Hopital's rule in calculus (although its not relevant to the $p$-adic case) to the limit $\lim_{x \rightarrow \infty} \dfrac{x^k}{p^{x/(p-1)}}$ and by L.Hopital's rule, taking repeated derivatives, I get that $\lim_{x \rightarrow \infty} \dfrac{x^k}{p^{x/(p-1)}}=0$ which gives me the feeling that $\lim_{n \rightarrow \infty} \dfrac{n^k}{p^{\operatorname{val}_p(n!)}}=0$. Thanks in advance for help and for explanations","['p-adic-number-theory', 'limits', 'abstract-algebra', 'number-theory', 'representation-theory']"
2320122,"Does there exist continuous functions $P$ and $Q$ on [0,1]","Does there exist continuous functions  $P$ and $Q$ on $[0,1]$ such that $y(t)=sin(t^2)$  is a solution to $y''+Py'+Qy=0$ on $[\frac1n,1]$ for all $n\geq1$? I find $y'$ and $y''$ and compare with the given differential equation and found that $P(t)= -1/t$ and which is not continuous on [0,1]. So such function $P$ can not exist. Is my concept is correct . Please help me to solve this also if any other method .
Thanks in advance.","['derivatives', 'ordinary-differential-equations']"
2320129,Continuity of a map between two metric spaces,"Consider the map $D:C^{1}([-1,1]) \rightarrow \mathbb{R}:f \mapsto f'(0)$, where $C^{1}([-1,1])$ is the vector space of functions from $[-1,1]$ to $\mathbb{R}$ that are at least once differentiable and have a continuous derivative. Also consider the $d_{\infty}$-metric on $C^{1}([-1,1])$ and the Euclidian metric on $\mathbb{R}$, where $d_{\infty}(f,g)= \sup \{ |f(x)-g(x) |$ where $x \in C^{1}([-1,1]) \}$. What can we say about the continuity of this function? It holds that $D$ is continuous if and only if for every $f \in C^{1}([-1,1])$ and $\epsilon >0$ there exists a $\delta >0$ so that for every $g \in C^{1}([-1,1])$, $|f'(0)-g'(0)| < \epsilon$ if $\sup \{ |f(x)-g(x) |$ where $x \in C^{1}([-1,1]) \} < \delta$. Now I don't really see how to deduce anything on continuity. Can someone help?","['derivatives', 'metric-spaces', 'continuity']"
2320176,"If $\tau$ is an $(\mathcal F_t)_{t\in I}$-stopping time and $t\in I$, then $\tau\wedge t$ is $\mathcal F_t$-measurable","Let $(\Omega,\mathcal A)$ be a measurable space $I\subseteq\mathbb R$ $(\mathcal F_t)_{t\in I}$ be a filtration of $\mathcal A$ $\tau$ be an $\mathcal F$-stopping time, i.e. $\tau:\Omega\to I\cup\sup I$ is $\mathcal A$-$\mathcal B(I\cup\sup I)$-measurable ($\mathcal B(E)$ denotes the Borel $\sigma$-algebra on $E\subseteq[-\infty,\infty])$ and $$\left\{\tau\le t\right\}\in\mathcal F_t\;\;\;\text{for all }t\in I\tag1$$ $t\in I$ How can we show that $\tau\wedge t$ is $\mathcal F_t$-measurable? Let $\iota$ denote the inclusion of $I\cup\sup I$ into $\overline{\mathbb R}$. Note that $$\mathcal B(A)=\left.\mathcal B(\overline{\mathbb R})\right|_A=\left\{A\cap B:B\in\mathcal B(\overline{\mathbb R})\right\}\;\;\;\text{for all }A\subseteq\overline{\mathbb R}\tag2$$ and hence $\tau$ is $\mathcal A$-$\mathcal B(I\cup\sup I)$-measurable iff $\iota\tau$ is $\mathcal A$-$\mathcal B(\overline{\mathbb R})$-measurable. Let $t\in I$. By the former argument, $\tau\wedge t$ is $\mathcal F_t$-measurable iff $$\left\{\tau\wedge t\le b\right\}\in\mathcal F_t\tag3\;\;\;\text{for all }b\in\mathbb R\;.$$ If $b\ge t$, then $$\left\{\tau\wedge t\le b\right\}=\left\{\tau\le b\right\}\cup\underbrace{\left\{t\le b\right\}}_{=\:\Omega}=\Omega\in\mathcal F_t\;.\tag4$$ If $b\in(-\infty,t)$, then $\left\{t\le b\right\}=\emptyset$. In that case, if $(-\infty,b]\cap I=\emptyset$, then $$\left\{\tau\wedge t\le b\right\}=\left\{\tau\le b\right\}=\emptyset\in\mathcal F_t\tag5\;.$$ Now, we arrived at the crucial part: If $(-\infty,b]\cap I$ is closed and nonempty, then $$s:=\max\left((-\infty,b]\cap I\right)<t$$ is well-defined and hence $$\left\{\tau\wedge t\le b\right\}=\left\{\tau\le b\right\}=\left\{\tau\le s\right\}\in\mathcal F_s\subseteq\mathcal F_t\tag6\;.$$ But what can we do otherwise?","['stochastic-processes', 'probability-theory', 'stopping-times']"
2320181,Find complex number Z in $\lvert Z\rvert= Z+3-2i$,"$$\lvert Z\rvert = Z+ 3-2i$$ what I did so far is
let $Z = a +bi$
so $$\sqrt{a^2 + b^2} = a+bi+3-2i$$ $$\sqrt{a^2 + b^2} = a+3 + i (b-2)$$ now what I'm thinking is squaring both sides but that doesn't work, any tips?","['linear-algebra', 'complex-numbers']"
2320250,"Why should kids learn how to use a compass and straightedge, and not rely on a drawing program? [closed]","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 years ago . Improve this question I am curious why it is necessary for people to learn how to use compasses and straightedges in geometry, and not just rely on a drawing program. I have a couple ideas, but I might be missing something or have a gap, so any opinions supported by facts or credible sources is great!","['euclidean-geometry', 'geometric-construction', 'geometry']"
2320275,The group $\mathbb Z_n$ is isomorphic to a subgroup of $GL_2(\mathbb R)$.,I need to prove the following: The group $\mathbb Z_n$ is isomorphic to a subgroup of $GL_2(\mathbb R)$. How can I prove this? $\mathbb Z_n$ is of order $n$ so it is isomorphic to a subgroup of $GL_n(\mathbb R)$. I know this is true but here it is given $GL_2(\mathbb R)$.,"['group-isomorphism', 'linear-groups', 'group-theory', 'cyclic-groups']"
2320284,Find Angle Between Two Curves at Point of Intersection [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Find angle between these two curves at point of intersection : 
$$K_1: x^2y^2 + y^4 = 1$$ and 
$$K_2 : x^2 + y^2 = 4 $$ Thanks!",['derivatives']
2320310,Possible Combinatorial Identity and Ordered Partitions?,"I want to consider the following expansion. $$\bigg(\frac{\sin(\frac{x}{2})}{\frac{x}{2}}\bigg)^{2g-2} = \sum_{h=0}^{\infty} \varphi^{(g)}_{h} x^{2h}$$ for $g \geq 2$, where the coefficients are given explixitly by, $$\varphi^{(g)}_{h} = \frac{(-1)^{h}}{2^{2g+2h-3}} \sum_{k=0}^{g-2} (-1)^{k}\binom{2g-2}{k}\frac{(2k-2g+2)^{2h+2g-2}}{(2h+2g-2)!}.$$ Now, I want to consider ordered partitions of an integer $n$.  Such an ordered partition $\pi \in \mathcal{P}_{n}$ can be written as an $n$-tuple $$\pi = \{ \pi_{1}, \pi_{2}, \ldots, \pi_{n} \}.$$ I think this is probably familiar, but for an example of how this works, the following ordered partition of $n=4$ $$\{ \cdot \,\,|\,\, \cdot \,\,\,\, \cdot \,\,\,\, \cdot \}$$ I would write as $\{1,3,0,0\}$ because the second and third barriers are not present.  Now, an object that appeared naturally in something I was studying, is the following sum over ordered partitions of products of the coefficients $\varphi$ above: $$\sum_{\pi \in \mathcal{P}_{n}} (-1)^{\epsilon_{\pi}}\varphi^{(g-1)}_{\pi_{1}} \varphi^{(g-2)}_{\pi_{2}} \cdots \varphi_{\pi_{n}}^{(g-n)}$$ for $g \geq2$ and $1 \leq n \leq g-2$, where $\epsilon_{\pi}$ is the number of non-zero entries in the tuple. So my question simply is, can anyone imagine an identity which simplifies this above sum, or let's me write it in terms of more well-known objects? In particular, are they related to Bernoulli numbers by chance?  This might just be some random, unattractive quantity, but my instincts tell me it might be nice. I wish I could provide the context in which this arises but it would require too much preamble.  Long story short, these sums (multiplied by some Bernoulli numbers) are the coefficients of a sum of Eisenstein Series which amazingly has integral coefficients.","['algebra-precalculus', 'combinatorics', 'permutations', 'functions']"
2320328,Calculation of $\zeta(2)$,"I was attempting to calculate the value of $\zeta(2)$ (I already knew what the value is, but I was trying to derive it). I started with the sum
$$\sum_{x=1}^\infty \frac{1}{x^2}$$
and I created the function
$$g(a)=\sum_{x=1}^\infty \frac{a^x}{x^2}$$
and differentiated both sides to get
$$g'(a)=\sum_{x=1}^\infty \frac{a^{x-1}}{x}$$
$$ag'(a)=\sum_{x=1}^\infty \frac{a^x}{x}$$
I then used the formula
$$\sum_{x=1}^\infty \frac{a^x}{x}=-\ln(|1-a|)$$
and so
$$ag'(a)=-\ln(|1-a|)$$
$$g'(a)=-\frac{\ln(|1-a|)}{a}$$
Since $\zeta(2)=g(1)$, does that mean that
$$\zeta(2)=-\int_0^1 \frac{\ln(|1-a|)}{a}$$
Is my reasoning correct here? If so, how do I evaluate this integral? I can't seem to find an indefinite integral for it.","['integration', 'riemann-zeta', 'sequences-and-series']"
2320341,"Is it true that cohomological dimension of a group $G$ over a ring $R$ is zero, then group ring $RG$ is semisimple?","Let $R$ be a ring and $G$ a group and let $RG$ be the group ring. 
 It is known that  $R$ into a left 
$RG$-module with trivial $G$-action, and define
$cd_R G$ to be projective dimension of  the left module $_{RG}R$.  $cd_R G$ is 
also called the cohomological dimension of $G$ over $R$. 
By this introduction, I want to know that the following statement 
is true or false. A group $G$ has cohomological dimension $0$ if and only if its group ring $RG$ is semisimple. I have seen the above statement in Wikipedia without any reference!
You can see below link: https://en.wikipedia.org/wiki/Cohomological_dimension I guess that the above statement is not true.  But I cannot find 
any counter example.","['representation-theory', 'abstract-algebra', 'ring-theory', 'group-theory']"
2320358,Combinatorics question based on ProjectEuler 606,"Motivation The following text is from Problem 606 from Project Euler : A gozinta chain for $n$ is a sequence $\{1,a,b,...,n\}$ where each element properly divides the next.
  For example, there are eight distinct gozinta chains for $12$:
  $$\{1,12\}, \{1,2,12\}, \{1,2,4,12\}, \{1,2,6,12\}, \{1,3,12\}, \{1,3,6,12\}, \{1,4,12\},\{1,6,12\}.$$
  Let $S(n)$ be the sum of all numbers, $k$, not exceeding $n$, which have $252$ distinct gozinta chains.
  You are given $S(10^6)=8462952$ and $S(10^{12})=623291998881978$.
  Find $S(10^{36})$, giving the last nine digits of your answer. Given a number $n\in \mathbb N$ we can  write its prime factorization $n=p_1^{k_1}\cdot\ldots\cdot p_m^{k_m}$. Every gozinta chain $(z_1=1,z_1,\ldots,z_{r-1},z_r=n)$ of length $r$ for $n$ can be built as
$$\begin{align} 
 z_r=&             p_1^{k_1}\cdot\ldots\cdot p_m^{k_m}\\
 z_{r-1}= &  p_1^{k_1-t^{(r-1)}_{1}}\cdot\ldots\cdot p_m^{k_m-t^{(r-1)}_{m}}\\
&\vdots\\
z_1     = &   p_1^{k_1-\sum_{j=1}^{r-1}t^{(j)}_1}\cdot\ldots\cdot p_m^{k_m-\sum_{j=1}^{r-1}t^{(j)}_m}\\
z_0 =&p_1^{k_1-\sum_{j=0}^{r-1}t^{(j)}_1}\cdot\ldots\cdot p_m^{k_m-\sum_{j=0}^{r-1}t^{(j)}_m}=1
\end{align}
 $$
through the backward recursion
$$z_{s-1}=\frac{z_s}{p_1^{t_1^{(s-1)}}\cdot\ldots\cdot p_m^{t^{(s-1)}_m}}$$
with the tuples $(t_1^{(0)},\ldots,t_m^{(0)}),\ldots,(t_1^{(r-1)},\ldots,t_m^{(r-1)})$ satisfying the two constraints $$
\begin{align}
\sum_{j=0}^{r-1}t_\nu^{(j)}&=k_{\nu} && \forall \nu=1,\ldots, m\tag{1}\\ 
(t_1^{(j)},\ldots,t_m^{(j)})&\neq(0,\ldots,0) && \forall j=1,\ldots r\tag{2}
\end{align}$$ The Problem I would like to understand more about the function $\alpha((k_1,\ldots,k_m))$ that associates the number of gozinta chains for $n$ to the multiplicities of $n$'s prime factors. Remark 1 : We could try to enumerate the tuples satisfying $(1),(2)$. For instance, if one supposes $m=1$, (i.e. $n=p^k$) the problem is equivalent to asking how many ordered partitions of $k$ exist. It is well known that such number is $2^{k-1}$. Then $\alpha(k)=2^{k-1}$ for $k>0$. The problem of enumerating tuples satisfying $(1),(2)$ could be seen as a generalization of integer partitioning to $m$-tuples. Remark 2 : The problem could be modelled combinatorially in the following way. Suppose we have $m$ bins with $k_1,\ldots, k_m$ balls. At each of our $r-1$ turns we must extract at least one ball. At the last turn ($r-1$) we must have removed all of the balls from the bins. In how many different ways can we do this without fixing the length of the game? Question Is there a known answer in the literature to the above mentioned combinatorics problem? I would expect either A reference following the idea of Remark 1, essentially correlating the result to ordered partitions of $m$-tuples. I am unaware of such results and have been able to find results only relative to ordered partitions of integers. A combinatorial approach following Remark 2. I probably have used nonstandard notation as I am not familiar with the subject. Note : I published this question even though it refers to a Project Euler question following guidance from this meta answer .","['project-euler', 'combinatorics', 'integer-partitions']"
2320429,Differentiation of functions defined by Borel measures.,"My question has to do with a detail in the proof of the following: Let $\mu$ be a finite Borel measure on $\mathbb R$, and let $F : \mathbb R\to \mathbb R$  be defined
by $F(x) =\mu ((−∞,x]).$ If $\mu$ is differentiable at $a$, then $F$ is differentiable at $a$, and $F'(a) = (D\mu)(a)$ where $$D\mu(a)=\limsup_{\epsilon \downarrow 0}\left \{ \frac{\mu (I))}{\lambda (I)}: a\in I; |I|<\epsilon\right \}.$$ The proof goes as follows: if $x<a$ then $\frac{F(x)-F(a)}{x-a}=\frac{\mu ([a,x])}{\lambda ([a,x])}$  whereas  $\frac{F(x)-F(a)}{x-a}=\frac{\mu ((a,x])}{\lambda ((a,x])}$ if $x>a.$ In the first case, we have $$D\mu(a)=\limsup_{\epsilon \downarrow 0^-}\left \{ \frac{\mu ([a,x])}{\lambda ([a,x])}: a\in I; |I|<\epsilon\right \}=\lim_{x\to a^{-}}\frac{F(x)-F(a)}{x-a}.$$ In the second case, we want to assert that $$D\mu(a)=\limsup_{\epsilon \downarrow 0^+}\left \{ \frac{\mu ([a,x])}{\lambda ([a,x])}: a\in I; |I|<\epsilon\right \}=\lim_{x\to a^{+}}\frac{F(x)-F(a)}{x-a}$$ and then the result is immediate. But this is not quite right because the intervals above (when $x>a$) are $\it{half-open}.$ Now it is pretty obvious that these intervals can be approximated to any desired degree of accuracy by considering intervals of the form $[a+1/n,x]$ so I am looking for a rigorous way to work this into the definition of $D\mu.$ Maybe along these lines: If $D\mu(a)=d$ then for all $r >0,$ there is an $\epsilon>0$ such that $\ \left |  \frac{\mu([a,x])}{\lambda([a,x])}-d \right |<r$ whenever $x-a<\epsilon.$ Then for each integer $n$  such that $a+1/n<x$ we have  $x-(a+1/n)=x-1/n-a<\epsilon$ and  then, $\left |\frac{\mu([a+1/n,x])}{\lambda([a+1/n,x])}  -d\right |<r.$ Since this is true for all integers $n$ such that $a+1/n<x$, the result follows. On the other hand, since $\mu$ is differentiable at $a$ we must have $\mu (\left \{ a \right \})=0, $ so perhaps it is as easy as noting that $[a,x]=(a,x]\cup \left \{ a \right \}.$","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2320456,Infinite zeros in infinite series,"The problem: Given that $$\frac{\pi}{4} = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \frac{1}{9} + \ldots $$ Prove $$\frac{\pi}{3} = 1 + \frac{1}{5} - \frac{1}{7} - \frac{1}{11} + \frac{1}{13} + \frac{1}{17} + \ldots$$ My solution: We know $$
\begin{align}
\frac{\pi}{4} & = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \frac{1}{9} -\frac{1}{11} + \frac{1}{13} - \frac{1}{15} + \ldots \\
\\
\frac{\pi}{12} & = \frac{1}{3} - \frac{1}{9} + \frac{1}{15} - \frac{1}{21} + \frac{1}{27} -\frac{1}{33} + \frac{1}{39} - \frac{1}{45} + \ldots\\
\\
& = 0 + \frac{1}{3} + 0 + 0 - \frac{1}{9} + 0 + 0 + \frac{1}{15} + 0 + 0 - \frac{1}{21}
\end{align}
$$ now add them together: $$
\begin{align}
\frac{\pi}{4} + \frac{\pi}{12} & = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \frac{1}{9} - \frac{1}{11} + \frac{1}{13} - \frac{1}{15} + \ldots \\
\\
& + 0 + \frac{1}{3} + 0 + 0 - \frac{1}{9} + 0 + 0 + \frac{1}{15} + \ldots \\
\end{align}
$$ and we will get: $$
\begin{align}
\frac{\pi}{3} & = 1 + 0 + \frac{1}{5} - \frac{1}{7} + 0 -\frac{1}{11} + \frac{1}{13} + 0 + \ldots \\
& =  1 + \frac{1}{5} - \frac{1}{7} -\frac{1}{11} + \frac{1}{13} + \ldots
\end{align}
$$ My questions: I inserted/removed infinite zeros into/from the series, is that OK? My solution relies on the fact that $\Sigma a_n + \Sigma b_n = \Sigma (a_n + b_n)$ and $k \Sigma a_n = \Sigma k a_n$. Is this always true for convergent infinite series? If so, why is it? (yeah I know this is a stupid question, but since I'm adding infinite terms up, I'd better pay some attention.) Bouns question: Can I arbitrarily (arbitrariness isn't infinity, you know) insert/remove zeros into/from a convergent infinite series, without changing its convergence value?","['fourier-series', 'infinity', 'convergence-divergence', 'summation', 'sequences-and-series']"
2320484,Does the number of cycles in a random permutation converge to Poisson as $n\to\infty$?,"Let $\sigma(n)$ be a permutation on ${1,2,\ldots,n}$ . Let $C_k$ denote the number of cycles of length $k$ . It is known that for fixed $k$ , $C_k$ converges in distribution as ${n \rightarrow \infty}$ to the Poisson distribution of intensity ${\frac{1}{k}}.$ What can we say about the distribution of the number of cycles $C = \sum_{k=1}^n C_k$ . For a fixed $n$ , I want to say that the distribution of $C$ is (approximately) Poisson with intensity $1 + \frac{1}{2}+ \frac{1}{3}+\cdots+\frac{1}{n} = H(n).$ The problem is this doesn't converge but can we say anything about say the $r$ th moment of C as $n\to\infty$ ? We know the mean $\mathbb{E}C = H(n)$ and you can show the variance is $<H(n)$ for all $n$ .","['permutations', 'combinatorics']"
2320494,"When is the union of infinitely many closed sets, closed?","It is known that, in general, the union of infinitely many closed sets need not be closed . However, in the following case, apparently, the union is closed: Suppose there is a large closed polygon $C$ , inside which there is a square $S$ (green). Consider the set of all closed convex objects that contain $S$ and are contained in $C$ . Then, apparently, the union of all these closed objects is closed. My questions: Is the above claim true, and if so, how to prove it? In general, what are conditions for infinite set of closed sets to be closed, especially in $\mathbb{R}^2$ ?","['geometric-topology', 'geometry']"
2320497,Elementary proof of completeness of the spherical harmonics for a fixed $L$,"This question asked how to prove the completeness of the spherical harmonics in the sense that $\{Y_{Lm}\}$ spans the set of square-integrable functions on the sphere. I'm looking for an elementary proof of what I think should be a much simpler theorem. Suppose that we're interested in the set of all functions on the sphere that have the same $L$, i.e., the set of all complex-valued functions that are eigenfunctions of the Laplacian with eigenvalue $-L(L+1)$. I'm looking for the most elementary possible proof that for this fixed $L$, the set $\{Y_{Lm}\}$ for varying $m$ is a basis. That is, I want an elementary proof that the multiplicity is $2L+1$. Ideally I would like this to be at a freshman physics level, intelligible even to people who haven't had linear algebra. (So the language would actually be less sophisticated that in my statement of the problem above.) I would be perfectly happy with a proof restricted to $L=1$. The closest I've been able to come is an approach that is kind of complicated, and I'm not sure it's even right. First we reduce the problem to real-valued functions by forming linear combinations of $Y_{Lm}$ and $Y_{L,-m}$. Next we argue that if the hypothesis were false, we would be able to witness its failure with a state $W$ of definite $m$ (i.e., one whose azimuthal variation is $e^{im}$) that is independent of $Y_{Lm}$. Now form linear combinations of $W$ with $Y_{Lm}$ such that the linear combination $X$ vanishes at some $\theta$. This can be done at almost all $\theta$. Finally, tune the choice of $\theta$ so that for this particular value of $\theta$, not just $X$ but $\partial X/\partial \theta$ vanishes as well. Now argue that by uniqueness of solutions to 2nd order differential equations, $X$ vanishes identically, which is a contradiction. I'm not sure that the last part even works, since the standard uniqueness theorems may not work for a space with this topology. Is there a simpler way to go about this? I also thought about physical arguments involving state counting. If the $L=1$ state has multiplicity 3, then coupling two spin-1 states gives 9 states, and this makes sense because you get spin-0, spin-1, and spin-2 couplings, for a total of 1+3+5=9 states. But all this really seems to do is prove that if the multiplicity of spin $L$ is $2L+1$, we get self-consistency --- it doesn't seem to show that $2L+1$ is necessary. Maybe there is some sort of nice argument saying that if we can couple two states of spin 1 to make spin 0, then this spin 0 state must be unique?","['spherical-harmonics', 'group-theory']"
2320515,"Finding all primes $p,q$ with $p^2+q^2=9pq-13$","The last month I was trying to solve a problem of a magazine, and I found the following equation $$p^2+q^2=9pq-13,$$
Where $p$ and $q$ are primes. We need to get solutions when $p$ and $q$ are odd, because if any of them is even, the only solution that works is $(2,17)$. Any ideas will be much appreciated. I analyzed the discriminant of the quadratic equation and we need to find solutions of $77q^2-52=k^2$, this is a variation of Pell's equations.","['number-theory', 'vieta-jumping', 'diophantine-equations', 'elementary-number-theory']"
2320523,Taylor series of $f(z)=\frac{\sin z}{z}$ at $z=1$,"My answer is: $$\sum_{n\geq 0} \frac{ (-1)^{n+1} z^{2n+1}(z-1)^{n}}{(2n+1)!}$$ I'm really confused, and I don't know if I'm correct, I mean, I have the term $z^{2n+1}$, is it okay?","['complex-analysis', 'taylor-expansion']"
2320551,Maximum Likelihood Estimate With Factorial,How do you find the maximum likelihood estimate of this function? $$P_x(k;\theta) = \frac{\theta^{2k} e^{-\theta^2}}{k!}$$ I'm really just having trouble with the factorial part... really not sure how to approach this problem. Thanks,"['maximum-likelihood', 'statistics']"
2320562,"Let $f:[a,b]] \to \mathbb{R}$ absolutely continuous and increasing function. Prove $\int_A f'd\lambda=\lambda(f(A))$","Let $f:[a,b] \to \mathbb{R}$ be absolutely continuous and increasing function. Prove $$\int_A f'd\lambda=\lambda(f(A))$$ for all $A \subset [a,b]$ measurable. My idea is, $f$ absolutely continuous and increasing function. Then $f$ is integrable. Define $$\mu:\mathbb{I} \rightarrow [0,\infty], \qquad \mu([a,b])=\int f'(x)dx.$$ Consider $$\mu([a,b])=f(b)-f(a)=\lambda([f(a),f(b)])=\lambda(f([a,b])).$$ Now, $\mu=\lambda \circ f\;$ in $\;\mathbb{I}\;$ and $\;\mu([a,b])<\infty.$ Then, by Caratheodory's Theorem, $\overline{\mu}$ is a unique extension. $$\implies \overline{\mu}=\mu=\lambda \circ f \text{ in } \mathbb{B}.$$ Thanks so much for any help.","['real-analysis', 'integration', 'lebesgue-integral', 'measure-theory']"
2320585,How to prove that some combinatorical function has minimum on borders?,"One maximum likelihood estimation is calculated according to equation: $$\widehat{\ell}=\operatorname*{argmin}_\ell\mathbb{E}f(\ell,k),$$ where $f(\ell,k)$ is a function of two variables: $$f(\ell,k)=\binom \ell k \binom {n-\ell} {m-k}.$$ One of arguments $k$ is random variable with hypergeometric distribution: $$P(k)=\frac{\binom {\ell-g_1}{k-g_2}\binom{h_1-\ell+g_1}{h_2-k+g_2}}{\binom {h_1}{h_2}}.$$ This probability has values more then zero on interval $k\in[g_2,g_2+h_2]$. I want to prove that if $\ell\in[g_1,g_1+h_1]$, then minimum of objective function locates on one of the borders: $$\min\limits_\ell\mathbb{E}f(\ell,k)=\min\left(\binom {g_1} {g_2}\binom {n-g_1} {m-g_2},\binom {g_1+h_1} {g_2+h_2}\binom {n-g_1-h_1} {m-g_2-h_2}\right),$$ where all variables and constants in binomial cooefs are nonnegative integers. To prove it I tried to calculate:
$$\mathbb{E}f(\ell,k)=\sum\limits_k f(\ell,k)P(k)$$ $$\mathbb{E}f(\ell,k)={\binom {h_1}{h_2}}^{-1} \sum\limits_{k=\max(g_2,g_2+h_2+\ell-g_1-h_1)}^{\min(\ell-g_1,h_2)} \binom \ell k \binom {n-\ell} {m-k}\binom {\ell-g_1}{k-g_2}\binom{h_1-\ell+g_1}{h_2-k+g_2}$$ But when I read ""Concrete mathematics"" I can not find some formula or trick to use in this sum. It looks like the some modification of Vandermonde's identity, but ... how to use this to get closed form of this? It is possible that closed form doesn't exist. Another question is how to find this descrete function extremum? Am I going right way trying to calculate last expression? May be, there exist another way to prove that minimum is on borders. Actually, I use log-likelihood instead likelihood (first equation) to estimate $\ell$ beacuse this helps to avoids big integers. I'm not sure about the equation:
$$\operatorname*{argmin}\limits_\ell\mathbb{E}\log{f(\ell,k)}=\operatorname*{argmin}\limits_\ell \mathbb{E}f(\ell,k)$$ To ilustrate this problem in terms of log-likelihood I posted this function realization and sample mean over 100 realizations. We can see that hypothesis i want to prove can be statistically accepted. Existing of proof is my question to mathematical community. Thank you for any help or comments!","['algebra-precalculus', 'combinatorics', 'statistics', 'combinatorial-proofs']"
2320596,"If $\Vert Tx-Ty \Vert = \Vert x-y \Vert$ for all $x,y \in X$ and $T(0)=0$ then T is a linear aplication. [duplicate]","This question already has answers here : Showing that an Isometry on the Euclidean Plane fixing the origin is Linear (3 answers) Closed 7 years ago . Problem: Lets $X$ and $Y$ normed vector spaces $T:X\rightarrow Y$ a aplication such that $\Vert Tx-Ty \Vert = \Vert x-y \Vert$ for all $x,y \in X$ and $T(0)=0$ then T is a linear aplication. My attempt: If I evaluate in $0$:
$$\Vert Tx \Vert = \Vert x\Vert \quad \mbox{for all $x \in X$}$$
Then,
$$\Vert T(x+y) \Vert = \Vert x+y\Vert \leq \Vert x\Vert + \Vert y\Vert = \Vert Tx\Vert+\Vert Ty\Vert$$
But, I do not know how to continue.","['functional-analysis', 'normed-spaces']"
2320624,If $B$ is invertible then there exists a scalar $c$ such that $A+cB$ is not invertible,"Let $A,B$ be $n\times n$ complex matrices. If $B$ is invertible then there exists a scalar $c \in \mathbb C$ such that $A+cB$ is not invertible. Since $\det(A+cB)$ is a polynomial in $\mathbb C$, it must have a root in $\mathbb C$, i.e. there must exist a $c$ such that $\det(A+cB)=0$. Then why is the condition ""$B$ is invertible"" necessary?","['matrices', 'linear-algebra']"
2320671,The covariance of the inversion number and the Major Index,"In The Joy of Brute Force , Doron Zeilberger wrote:
$
\def\inv{\mathop{\rm inv} \nolimits}
\def\maj{\mathop{\rm maj} \nolimits}
\def\des{\mathop{\rm des} \nolimits}
$ As usual, for a permutation $\pi$ of length $n$, let $\inv \pi$ be the
  number of $(i,j)$, such that $1 \leq i <j \leq n$ and $\pi[i]>\pi[j]$,
  and $\maj \pi$ be the sum of $i$, such that $1 \leq i <n$ and
  $\pi[i]>\pi[i+1]$. Svante Janson asked Don Knuth, who asked me, about
  the covariance of $\inv$ and $\maj$. The answer is ${{n} \choose
> {2}}/4$. To prove it, I asked Shalosh to compute the average of the
  quantity  $(\inv \pi - E(\inv))(\maj \pi - E(\maj)) $ over all
  permutations of a given length $n$, and it gave me, for $n=1,2,3,4,5$,
  the values $0,1/4,3/4,3/2,5/2$, respectively. Since we know a
  priori that this is a polynomial of degree $\leq 4$, this must be it! Why the covariance must be a polynomial of $n$ of degree $\leq 4$? It does not seem so obvious. The clue given by Zeilberger is: This is the old trick to compute moments of combinatorial
  statistics, described nicely in Graham, Knuth, and Patashnik's
  Concrete Math, section 8.2, by changing the order of summation. It
  applies equally well to covariance.  Rather than actually carrying out
  the gory details, we observe that this is always a polynomial whose
  degree is trivial to bound. I checked that section of Concrete Math, but I am not sure which part is related.","['permutations', 'probability-theory']"
2320726,Are real numbers vectors or scalars?,"I am trying to understand the difference between scalars and vectors. I know the basic definition that vectors are magnitude with direction. But we also call vectors those entities that belong to vector spaces and can be added. I had previously asked this question ,  a comment to this answer to the question states that real numbers can be both vectors and scalars, how is that even possible ? If so then what is a vector and what is a scalar ? How can they be different but be represented simultaneously by the same mathematical concept ?","['real-numbers', 'linear-algebra']"
2320756,When is uniform tightness plus weak convergence of the fdd enough to conclude convergence in law of $X_n(t)$?,"Uniform tightness plus weak convergence of the finite dimensional distributions is sufficient for a stochastic processes $X_n(t)$ to converge in law in spaces like the Donsker space $D[0,1]$ or the space of continuous functions $C[0,1]$, all equipped with a proper metric. Generally : When is tightness plus weak convergence of the finite dimensional distributions enough to conclude that a Stochastic process converges in distribution to a limiting process? Specifically : I was wondering about the case of convergence in the space of $\ell^{\infty}(\mathbb R)$, the space of bounded real valued functions equipped with the $\lVert\cdot\rVert_{\infty}$ - norm. It's interesting, some books, like Klenke ""Probability theory"", only introduce convergence of the fdd in a continuous path space, so not even in $D[0,1]$, some references only deal with the Donsker space. Is there a general result - something like: In any metric path space it is sufficient to show the weak convergence of the finite dimensional distributions and the uniform tightness of the distributions? Is there any reference, maybe specifically for the $\ell^{\infty}(\mathbb R)$ case?","['stochastic-processes', 'reference-request', 'probability-theory', 'measure-theory']"
2320761,Can $\theta$ be a rational multiple of $2\pi$?,"In this question the author conjectured that a certain angle $\theta$ could be a rational multiple of $2\pi$ only in the case it was an integer multiple of $\pi/2$. In my answer I found an explicit representation for $\theta$:
$$
\cos\theta=1+2\cos\left(2r\pi\right),
$$
where $r$ is any rational number such that the right hand side of the equation is comprised between $-1$ and $1$. This equation is certainly satisfied if $\theta=k\pi/2$, with $k$ integer. My question: is it possible to prove that no other solution $\theta$ can be expressed as a rational multiple of $2\pi$?","['trigonometry', 'algebraic-number-theory']"
2320778,Measure theory on an abstract Boolean $\sigma$-algebra. Where can I read something about?,"A Boolean $\sigma$-algebra is a Boolean algebra $(\mathcal{B},\vee,\wedge,1,0)$ such that every countable collection $(a_n)_{n\in\mathbb{N}}$ has a supremum (for the partial order $a\leq b$ whenever $b=a\vee b$). A measure on $\mathcal{B}$ could be some function $\mu:\mathcal{B}\rightarrow\mathbb{R}$ which is non-negative, $\mu(0)=0$ and $\sigma$-aditive. A pair $(\mathcal{B},\mu)$ is called a measure-algebra. Is there any book where measure theory is studied on abstract measure algebras? Thanks","['boolean-algebra', 'measure-theory']"
2320781,Signature of restriction of a bilinear form,"Assume I have a real bilinear form $b$ of signature $(p,q)$ on $E=\Bbb R^n$ , and a subspace $F$ of $E$ of dimension $d$ . What are the possible signatures $(\alpha, \beta)$ of the restriction of $b$ to $F \times F$ ? I saw the question possible signatures of bilinear form on subspaces , but it is unanswered. For instance, if $q=1, p=n-1$ , apparently the only possibilities for $b\vert_{F \times F}$ are $(\alpha, \beta) = (d,0), (d-1,1), (d-1,0)$ . I was able to show that if $\beta \geq 1$ , then we must have $(\alpha, \beta) = (d-1,1)$ , but how to do the case $\beta=0$ ? 
What about more general cases for $p$ and $q$ ? (For instance, do we have $\alpha ≤ p, \beta ≤ q$ ?). Thank you!","['bilinear-form', 'abstract-algebra', 'linear-algebra']"
2320814,Show that the supremum of a collection of lower semicontinuous function is lower semicontinuous,"I know there's already a question with a title very similar to this, unfortunately as I understand the OP skips over the part of the proof that is not clear to me. Let $I$ be a set and $f_\alpha$, $\alpha \in I$ be a collection of lower semicontinuous functions. Show that $g=\sup\limits_{\alpha\in I}\,f_\alpha$ is lower semicontinuous. My attempt: let $S_f(t)=\{x\in\mathbb{R}^n: f(x)>t\}$ for $t\in\mathbb{R}$. It is easy to see that $f$ is lower semicontinuous if and only if $S_f(t)$ is an open set, now define $$S(t)=\bigcup_\limits{\alpha\in I}S_{f_\alpha}(t).$$ Obviously $S(t)$ is open. I should now show that $S_{g}(t)=S(t)$, which ends the proof. Now, obviously $S(t)\subset S_g(t)$ because if $f_\alpha(x)>t$ for some $\alpha\in I$ then $g(x)>t$ because $g(x)>f_\alpha(x)$ for all $\alpha\in I$, but I fail to see why $S_g(t)\subset S(t)$, I think I should use the definition of supremum but I don't see how. I'm thinking I should prove it for a particular $t\in \mathbb{R}$ because that seems very untrue for any $t$.
Thank you.","['continuity', 'real-analysis', 'semicontinuous-functions']"
2320848,Exactly three distinct valued metric.,"The discrete metric $d_0$ can take two values $0$ and $1$. Can a metric
  function $d_X$ on a set $X$ attain exactly three distinct values? Going through the route of actually finding a metric instead of disproving one exists, I tried finding a metric which has values $0$, $a$ and $b$. But by trying to define it in a similar way as the discrete metric, I had a hard time finding one that could satisfy the three requirements to be a metric. Disproving it on the other hand sounds pretty easy tho, by just taking $x, y, z \in X$, I set the two values to random distances like $d_X(x, y) = a = d_X(y,x)$ , $d_X(x, z) = b = d_X(z, x)$ and finally $d_X(y, z) = c = d_X(z, y)$, with $c$ being either $a$ or $b$. Applying this to the triangle inequality gives us. $d_X(x, z) \leq d_X(x, y) + d_X(y,z)$ $d_X(x, y) \leq d_X(x, z) + d_X(z,y)$ $d_X(y, z) \leq d_X(y, x) + d_X(x,z)$ Which gives us the linear inequalities: $b \leq a + c$ $a \leq b + c$ $c \leq a + b$ This cannot hold if $c$ is either $a$ or $b$.
Is this proof correct? And more importantly, isn't there a better proof, because this sounds like a very inefficient proof.","['metric-spaces', 'discrete-mathematics']"
2320853,What happens if we try to define the Lebesgue integral by an infimum?,"Let $(X,\mathcal{E},\mu)$ denote a measure space. Then each measureable function $f : X \rightarrow [0,\infty]$ has a Lebesgue integral $\int f d\mu$ given by take a supremum of the integrals of simple functions that are bounded above by $f$. Question. Suppose we instead try taking infima of simple functions that are bounded below by $f$. I assume the resulting integral is ""poorly behaved"", since no one ever talks about it. My question is quite simply, what happens if we do this?","['lebesgue-integral', 'measure-theory']"
2320879,$f:S^1\rightarrow S^1$ injective but not surjective,"I thought this question was trivial, but I actually can't answer it, I hope I'm not missing something important. Let $S^1:=\{z\in \mathbb{C} \textit{ such that } |z|=1\}$. Can there be an injective continuous function $f:S^1\rightarrow S^1$ which is not surjective? Then this question generalizes to: Consider $M_n$ a n-dimensional differential compact manifold. Can there be $f:M_n\rightarrow M_n$ continuous and injective but not surjective?","['real-analysis', 'functions', 'geometry', 'differential-geometry', 'analysis']"
2320881,"If $p+q+r=0$, find the value of the determinant","If $p+q+r=0$ , prove that the value of the determinant $$ \Delta= \begin{vmatrix}
pa & qb &rc \\ 
qc & ra &pb\\ 
rb& pc & qa  \\ 
\end{vmatrix} =-pqr \begin{vmatrix}
a & b &c \\ 
b & c &a\\ 
c& a & b  \\ 
\end{vmatrix}$$ My Try:Since $p+q+r=0$ we have $$a(p+q+r)+b(p+q+r)+c(p+q+r)=0$$ $\implies$ $$(ap+qc+rb)+(qb+ra+pc)+(rc+pb+qa)=0 \tag{1}$$ Now applying $C_1 \to C_1+C_2+C_3$ and then applying $R_1 \to R_1+R_2+R_3$ for $\Delta$ we get $$\Delta=  \begin{vmatrix}
0 & qb &rc \\ 
qc+ra+pb & ra &pb\\ 
rb+pc+qa& pc & qa  \\ 
\end{vmatrix}$$ Any clue here?","['matrices', 'linear-algebra', 'determinant']"
2320905,Obtaining derivative of log of sigmoid function,"I saw the following result:
$$
\dfrac{\mathrm{d}}{\mathrm{d}x} \left( \log\left( \dfrac{1}{1+\mathrm{e}^{-x}} \right) \right) = \dfrac{1}{\mathrm{e}^x+1}
$$
What are the intermediary steps for obtaining this result?","['derivatives', 'logarithms', 'exponential-function']"
2320937,Fundamental theorem of calculus for multivariable function,"In a proof, they have written: $$\frac{f(x,y+t)-f(x,y)}{t}=\int_0^1 f_y(x,y+st)ds$$
But doesnt the fundamental theorem of calculus say $$\int_0^1 f_y(x,y+st)ds=f(x,y+t)-f(x,y)?$$
I'm sure the answer is obvious, but for the life of me I can't see where this extra $1/t$ coming from.
Thanks!","['real-analysis', 'integration', 'ordinary-differential-equations', 'calculus']"
2320996,High School Projectile Motion and Quadratics,"High school students are learning about the basics of solving quadratics and trigonometric ratios, including trigonometric inverses. The eventual goal of their project is to be able to show a reasonable firing solution, given in initial angle $\theta$ and initial velocity $v_0$. Projectile motion is given by $$y=\left(\tan{\theta}\right)x-\left(\frac{g}{2v_0^2\cos^2{\theta}}\right)x^2$$ where $x$ and $y$ are horizontal and vertical displacement. In this equation, one might change $\theta$ or $v_0$ and the equation behaves as expected - allowing students to directly give a reasonable solution. My goal is to find a way to help my students build up understanding from basic quadratics, which appear like a trajectory in shape, to a more accurate representation of trajectory given by the equation above. Since the trajectory equation is quadratic in nature and can be reduced to $ax+bx^2$, and because students will also learn trigonometric ratios, this is reasonable. Note that I am giving the context because the answer needs to be at a relatively low level, and I also hope that someone might have a contribution for the approach. Students initially create quadratics that look like projectile motion using quadratics of the form $y=-x(x-a)$. This gives a nice way to discuss solutions (the impact point) using the zero product property. We build up to $ax^2 +bx+c$ form, now discussing other ways to find solutions. Then I ask students to convert this equation to an initial angle and velocity (after discussion of what a firing solution might constitute). They realize quickly that such an equation doesn't make a lot of necessary information very obvious - eg, gravity, angle, velocity, etc. While we eventually discuss the trajectory motion equation (sadly, I haven't discovered how to build this in a way high school geometry students understand), we initially account for gravity using the equation $$h(t)=\frac{1}{2}gt^2+v_{0,y} t+h_0$$ where $t$ is time in seconds, $g$ is acceleration due to gravity, and $h(t)$ is height in meters, and $v_{0,y}$ is initial $y$ velocity. My question is: can $h(t)$ be modified with a particular initial angle $\theta$ and initial velocity $v_0$? My suspicion is that the answer to this question will look like the parameterized version of the trajectory equation.","['trigonometry', 'mathematical-physics', 'geometry', 'projectile-motion', 'quadratics']"
2320997,Cohomological dimension of direct product,"Let $\operatorname{cd}$ denote the cohomological dimension of a group, i.e. the minimal length of a projective resolution of $\mathbb{Z}$ over the group ring. Let $G_1$ and $G_2$ be groups. It is easy to see that $$\operatorname{cd}(G_1 \times G_2) \leq \operatorname{cd}(G_1)+\operatorname{cd}(G_2) \ , $$ (using the tensor product of resolutions), but is there a clear criterion that implies equality? In particular, are there groups $G$ with $\operatorname{cd}(G\times G) < 2 \operatorname{cd}(G)$ ?","['group-cohomology', 'homological-algebra', 'group-theory']"
2321000,A New Definition of Derivative,"Update 2018/4/18: I've found a book in which the definition 5) is discussed. See Topology, Calculus and Approximation by Vilmos Komornik, published by Springer-Verlag , page 98, Lemma 4.1. Original Question: I've come across ""Carathéodory Derivative"" in my textbook, which is, Definition 1) Let $f:\mathbb{R}\to \mathbb{R},\quad t\mapsto f(t)$ be a function, $a\in \mathbb{R}.$ Then if there exists a map $\varphi:\mathbb{R}\to \mathbb{R}, \quad t\mapsto \varphi(t)$ , which satisfies $$1) \quad f(x)-f(a)=\varphi(x)\cdot(x-a),\forall x\in \mathbb{R};$$ $$2) \quad  \text{$\varphi $ is continuous at the point a} ,$$ then we call $\varphi(a)$ the derivative of $f$ at point $a$ . And compared with the traditional definition of derivative: Definition 2) Let $f:\mathbb{R}\to \mathbb{R},\quad t\mapsto f(t)$ be a function, $a\in \mathbb{R}.$ Then if the limit $$\lim_{x\to a}{f(x)-f(a)\over{x-a}}$$ exists, then the value of this limit is called the derivative of $f$ at point $a$ . I can prove that (it's not difficult) these two definitions above are equivalent to each other. But when I look at the high-dimensional condition, things get complicated. Definition 3) Let $f:\mathbb{R}^n\to \mathbb{R}^m,\quad t\mapsto f(t)$ be a multivariate function, $a\in \mathbb{R}^n,$ Then if there exists a map $\varphi:\mathbb{R}\to M_{m\times n}(\mathbb{R}),\quad t\mapsto \varphi(t)$ , which satisfies $$1) \quad f(x)-f(a)=\varphi(x)\cdot(x-a),\forall x\in \mathbb{R}^n;$$ $$2) \quad  \text{$\varphi $ is continuous at the point a} ,$$ then we call $\varphi(a)$ the derivative of $f$ at point $a$ . And consider the traditional definition of derivative Definition 4) Let $f:\mathbb{R}^n\to \mathbb{R}^m,\quad t\mapsto f(t)$ be a multivariate function, $a\in \mathbb{R}^n.$ Then if there exists a matrix $A\in M_{m\times n}(\mathbb{R}),$ such that $$\lim_{x\to a}{||f(x)-f(a)-A\cdot (x-a)||\over{||x-a||}}=0,$$ then matrix $A$ is called the derivative of $f$ at point $a$ . Question: I expect that definition 3) is equivalent to definition 4), but I can only prove that $\mathrm{def}\ 3)\Rightarrow \mathrm{def}\ 4).$ I doubt whether $\mathrm{def}\ 4)\Rightarrow \mathrm{def}\ 3)$ is correct. Any help is appreciated. P.S. Now I am able to do some generalization to definition 3). Definition 5) Let $E,F$ be two Banach spaces, $a\in E.$ $\mathcal{L}(E;F)$ be the set of continuous linear map $E\to F,$ then consider function $f:E\to F, \quad t\mapsto f(t),$ then if there exists a map $\varphi:E\to \mathcal{L}(E;F), \ t\mapsto \varphi(t),$ such that $$1) \quad f(x)-f(a)=(\varphi(x))(x-a),\forall x\in E;$$ $$2) \quad  \text{$\varphi $ is continuous at the point a} ,$$ then we call $\varphi(a)$ the derivative of $f$ at point $a.$ Using Hahn-Banach theorem, we can see this definition is also equivalent to the classic definition of derivative on Banach space. P.P.S: A more general condition is, Definition 6) Let $E,F$ be two additive groups, and $\mathcal{T}$ be a topology over $E,$ $\mathcal{T'}$ be a topology over $\mathcal{L}(E;F)$ , $a\in E.$ Here $\mathcal{L}(E;F)$ is the set of continuous linear map $E\to F.$ Consider function $f:E\to F, \quad t\mapsto f(t),$ then if there exists a map $\varphi:(E,\mathcal{T})\to (\mathcal{L}(E;F),\mathcal{T'}), \ t\mapsto \varphi(t),$ such that $$1) \quad f(x)-f(a)=(\varphi(x))(x-a),\forall x\in E;$$ $$2) \quad  \text{$\varphi $ is continuous at the point a} ,$$ then we call $\varphi(a)$ a derivative of $f$ at point $a,$ with respect to topology $\mathcal{T}$ and topology $\mathcal{T'}.$ (Under this condition the derivative may not be unique.)","['derivatives', 'calculus']"
2321075,"$n>1$, $p_n$ is the prime number nth then $p_{n+1}^{n^2} > p_n^{n^2+1}$ (conjecture)","Let $p_n$ is prime number nth: If n>1 then: $$p_{n+1}^{n^2} > p_n^{n^2+1}$$ I checked the conjecture above true for first fifty million primes . Could You give your remark, reference, or your proof of conjecture above?","['number-theory', 'reference-request', 'maximal-and-prime-ideals', 'prime-numbers']"
