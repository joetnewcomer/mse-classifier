question_id,title,body,tags
3023983,How to inverse a block diagonal matrix?,"Given a matrix $$x = \begin{bmatrix} 
40 & 0 & 0 & 0\\
0 & 80 & 100 & 0 \\
0 & 40 & 120 & 0 \\
0 & 0 & 0 & 60\end{bmatrix}$$ How to find the inverse of that matrix?
What I know: $\det(x) = ac-bd$ , 
inverse of a 2x2 matrix: $$x^{-1} = \frac{1}{\det(x)}\cdot \begin{bmatrix} d &-b\\ -c &a\end{bmatrix}.$$ There is a lot of content online; however none of them has a specific numerical example.","['matrices', 'numerical-linear-algebra', 'matrix-decomposition']"
3024039,Show that $q$ must divide one of the prime integer factors of $N(q)$.,"The following is from Aluffi's Algebra Why this is true? : .. since q is prime in Z[i] ⊇ Z, q must divide one of
the prime integer factors of N(q).","['number-theory', 'abstract-algebra', 'algebraic-number-theory', 'gaussian-integral']"
3024046,Why $\mathbb P(X_t=Y_t\text{ for all }t)=1$ if $\mathbb P(X_t=Y_t)=1$ for all $t$?,"Let $X_t$ and $Y_t$ two continuous stochastic process on $(\Omega ,\mathcal F,\mathbb P)$ s.t. $X_t=Y_t$ a.s. for all $t$ . Why $\mathbb P\{X_t=Y_t\text{ for all }t\}=1$ ? The solution goes as : We have that $X_r=Y_r$ a.s. for all $r\in\mathbb Q$ . Therefore, $\mathbb P\{\sup_{s\in\mathbb R}|X_s-Y_s|>0\}=0$ . The claim follow. I really don't understand the argument. And for me $\mathbb P(X_t=Y_t)=1$ for all $t$ is really the same as $\mathbb P(X_t=Y_t\text{ for all }t)=1$ . I don't see the difference.","['measure-theory', 'probability']"
3024090,Inequality for $\sin(20°)$,"Prove that $$\frac{1}{3} < \sin{20°} < \frac{7}{20}$$ Attempt $$\sin60°=3\sin20°-4\sin^{3}(20°)$$ Taking $\sin20°$ =x I got the the equation as $$8x^3-6x+\sqrt{3} =0$$ But from here I am not able to do anything. Any suggestions? Thanks!
Edit-graph of p(x)","['trigonometry', 'functional-inequalities', 'inequality']"
3024120,Why is $\lim_{x\to -\infty}\sqrt{x^2+5x+3}+x = \lim_{x\to -\infty}(-x)+x=\lim_{x\to -\infty}0 = 0$ not correct?,"$\lim_{x\to -\infty}\sqrt{x^2+5x+3}+x = \lim_{x\to -\infty}(-x)+x=\lim_{x\to -\infty}0 = 0$ Apparently, the 2nd step is illegal here. Probably because for $x=-\infty$ I'd get $(+\infty-\infty)$ which is not possible. I see why this wouldn't be possible, I'm not sure if it really is the cause which makes that equation illegal though. But now, apparently, I could do this: $\lim_{x\to -\infty}\sqrt{x^2+5x+3}+x=\lim_{x\to -\infty}\frac{[\sqrt{x^2+5x+3}+x][\sqrt{x^2+5x+3}-x]}{\sqrt{x^2+5x+3}-x}=\lim_{x\to -\infty}\frac{x^2+5x+3-x^2}{\sqrt{x^2+5x+3}-x}=\lim_{x\to -\infty}\frac{5x+3}{\sqrt{x^2+5x+3}-x}=\lim_{x\to -\infty}\frac{5+3/x}{\sqrt{1+5/x+3/x^2}-1}=-5/2$ which gives me the correct result. But in the 3th step I used $x^2-x^2=0$ , how is that legal?
Also, in the 2nd step I implicitly used: $-x\sqrt{x^2+5x+3}+x\sqrt{x^2+5x+3}=0$ Which also seems to be fine, but why?","['limits', 'calculus']"
3024199,write every integer as a signed sum of integer power an integer.,"This has been an issue, I haven't been able to solve it yet despite many attempts. The problem is the following: $ \forall n \in \mathbb{N}^*, \forall M \in \mathbb{N}, \exists K \in \mathbb{N} | \exists (\epsilon_i)_{0<i\leq K} \in (0,1)^K$ such as: $$ M = \sum_{i=1}^K (-1)^{\epsilon_i}i^n $$ for the case $n=1$ : It comes naturally from the fact that the difference of two consecutive numbers is equal to $1$ . We just have to sum it. For the case $n=2$ : Again it's not that hard: $$(n+1)^2-n^2 = 2n+1$$ $$(n+3)^2-(n+2)^2 = 2n+5$$ Combining this two formulae we get that with $4$ numbers we can reach $4$ . 
With this we only need to show that we can reach $1,2$ or $3$ and we wil have done it. $1$ and $3$ are easy. For $2$ , 
we look at $\frac{\mathbb{Z}}{4\mathbb{Z}}$ and we see easily that $1+2^2+3^2 = 2$ so $2$ is reachable. Hence the demonstration for $n=2$ . However I can't find the way to go from those intuitive steps to the general step. If it's a duplicate please let me know. This is a just for fun project, no general background nor context. Thanks in advance.",['algebra-precalculus']
3024217,"Are there i.i.d andom variables $X$ and $Y$ such that either $X+Y$, $X-Y$, $X*Y$ or $X/Y$ is continuously uniformly distributed on some interval?","I think if they are iid it is impossible but I'm not entirely sure...
I tried to calculate pdfs of convoultions/product/ratio distributions and demand that the result is a constant...
But then I did not know how to continue Anyone has some idea?","['probability-theory', 'probability']"
3024231,"Is it possible for the sequence $\{\frac{x_{n+1}}{x_n}\}$ to be unbounded but have $\lim_{n\to\infty} x_n = x$, $x_n \ne 0$","Given: $$
\begin{cases}
\lim_{n\to\infty} x_n = x \\
x_n \ne 0 \\
n \in \mathbb N
\end{cases}
$$ Is it possible for $\{\frac{x_{n+1}}{x_n}\}$ to be and unbounded sequence? This problem comes in the context of two others which are: Does $\lim_{n\to\infty}\frac{x_{n+1}}{x_n}$ exist? Not necessarily if we for example consider: $$
x_n = \frac{sin{\pi n \over \sqrt2}}{n}
$$ If the limit exists and is equal to $q$ , prove $|q| \le 1$ This one is also easy to show using the definition of a monotone sequence. The third part as of the question section asks whether $\{\frac{x_{n+1}}{x_n}\}$ may be unbounded, and the answer suggests that this is indeed possible, but I don't see how. What would be such a sequence?","['limits', 'calculus', 'examples-counterexamples', 'sequences-and-series']"
3024383,Prove $\int_0^1\frac{\log(t^2-t+1)}{t^2-t}\mathrm dt=\frac{\pi^2}9$,"I am in the middle of proving that $$\sum_{k\geq1}\frac1{k^2{2k\choose k}}=\frac{\pi^2}{18}$$ And I have reduced the series to $$\sum_{k\geq1}\frac1{k^2{2k\choose k}}=\frac12\int_0^1\frac{\log(t^2-t+1)}{t^2-t}\mathrm dt$$ But this integral is giving me issues. I broke up the integral $$\int_0^1\frac{\log(t^2-t+1)}{t^2-t}\mathrm dt=\int_0^1\frac{\log(t^2-t+1)}{t-1}\mathrm dt-\int_0^1\frac{\log(t^2-t+1)}t\mathrm dt$$ I preformed the substitution $t-1=u$ on the first integral, then split it up: $$\int_0^1\frac{\log(t^2-t+1)}{t-1}\mathrm dt=\int_{-1}^0\frac{\log(2u+i\sqrt3+1)}u\mathrm du+\int_{-1}^0\frac{\log(2u-i\sqrt3+1)}u\mathrm du-2\log2\int_{-1}^0\frac{\mathrm du}u$$ But the last term diverges, but I don't know what I did wrong. In any case, I would be surprised if there wasn't an easier way to go about this. Any suggestions? Thanks.","['integration', 'alternative-proof', 'sequences-and-series']"
3024496,Computing an almost Vandermonde matrix,"I have this determinant which looks like a Vandermonde matrix $$D=\begin{vmatrix}1& a_1 & \cdots & a_1^{n-2}& a_1^n\\
1& a_2 & \cdots & a_2^{n-2}& a_2^n\\
\vdots &\vdots & \ddots & \vdots & \vdots\\
1& a_n & \cdots & a_n^{n-2}& a_n^n
\end{vmatrix}$$ Using the software maxima I found that probably $D$ has this form $$D= \prod_{i<j}(a_j-a_i)(a_1+a_2+\cdots+ a_n)$$ but I couldn't prove it. Is my conjecture true and how can I prove it?","['determinant', 'linear-algebra']"
3024503,Can I efficiently enumerate all numbers in a range that have a prime factor in another given range?,"Suppose $a<b$ are positive integers. The object is to determine all the numbers $x\in [a,b]$ having a prime factor in the range $[c,d]$ efficiently (that is without factoring all the numbers in the range or other brute-force approaches). Example : Which numbers in the range $[40!-10^9,40!+10^9]$ have a prime factor in the range $8\cdot 10^{15},10^{16}$ ?","['number-theory', 'prime-factorization', 'elementary-number-theory']"
3024543,What are the properties of countably infinite sets compared to sets of higher cardinality?,"When looking at mathematical definitions, there are quite a few cases where we limit certain properties to countably infinite sets (e.g. $\sigma$ -Algebras ). In some cases we set this limit as we'd lose our intuition of what would happen if we chose an even bigger infinity, in other cases there are hard facts at work. Yet, the only property of countably infinite sets that comes to my mind is ... well, that they are countable, and the higher cardinalities are not. I'm sure though that there are many more characteristic properties of countably infinite sets that get lost when moving up to higher cardinalities - so what are they?","['elementary-set-theory', 'cardinals']"
3024715,A visual subject in group theory,"My math teacher gave us the following instruction: « Pick any subject of your choice (in math of course) and in 2 months present it to the class ». I really like this idea, and I really like group theory hence I am looking for an interesting/intuitive subject in group theory which has visual results/theorems. I know the basics of group theory: Lagrange theorem, Sylow theorems, group actions, the structure of finite abelian groups, Cayley graphs... For example, I looked briefly at Lie groups and representation theory but this is far too complicated for me and I feel like there’s a huge gap between the above results and these two fields. I’ve also looked at the braid group which is very interesting but it seems to me like there aren’t so many things that can be said about this group (except studying the word problem on this group). Here is, briefly, what I know in maths: linear algebra (vector spaces, matrices, reduction, Jordan...), topology (only the topology of normed vector spaces), analysis (Riemann integration, ...). Thank you in advance!
(if you are not sure don’t hesitate to post suggestions in the comment section :) )","['finite-groups', 'abstract-algebra', 'linear-algebra', 'intuition', 'group-theory']"
3024718,Proof that 4 points lie on a circle and that center of this circle lies on the circumcircle of $\triangle ABC$,"Given is acute triangle $ABC$ . Let $D$ be foot of altitude from vertex $A$ . Let $D_1$ be a point so that line of symmetry between $D_1$ and $D$ is line $AB$ . Let $D_2$ be a point so that line of symmetry between $D_2$ and $D$ is line $AC$ . Let points $E_1, E_2$ be on line $BC$ so that $D_1E_1 \parallel AB$ and $D_2E_2 \parallel AC$ . Proof that points $D_1, E_1, D_2, E_2$ lie on same circle and that center of this circle lies on the circumscribed circle of triangle $ABC$ . My plan was to first prove that $D_1E_1E_2D_2$ is cyclic quadrilateral, in other words that $\angle E_1E_2D_2 + \angle E_1D_1D_2 = 180°$ or that $\angle E_1D_2D_1 = \angle E_1E_2D_1$ . This would mean that points $D_1, E_1, D_2, E_2$ lie on same circle. However, without success. Can someone help me with this please?","['euclidean-geometry', 'quadrilateral', 'circles', 'geometry', 'triangles']"
3024739,Prove the norm of operator derived from orthogonal projections is less or equal to 1,"Suppose $H$ is a Hilbert space. We have two orthogonal projections $P_1: H\rightarrow A_1$ and $P_2: H\rightarrow A_2$ . Here $A_1$ and $A_2$ are two closed subspace of $H$ . Prove that $$
\lVert P_1 - P_2\rVert \le 1
$$ Any hints may help. Thank you.","['linear-algebra', 'functional-analysis']"
3024775,Higher Order Multivariable Taylor Expansions,"The quadratic multivariable Taylor approximation of a function $f(x, y)$ around a point $(a, b)$ is given by $f(a, b) + f_x(a, b)(x - a) + f_y(a, b)(y - b) + \frac{1}{2}f_{xx}(a, b)(x - a)^2 + f_{xy}(a, b)(x - a)(y - b) + \frac{1}{2}f_{yy}(a, b)(y - b)^2$ There is a vectorized version of this expression which works for functions and points with an arbitrary number of variables, $f(\vec{a})\ + ∇f(\vec{a})\cdot(\vec{x} - \vec{a}) + \frac{1}{2}(\vec{x} - \vec{a})^TH(\vec{a})(\vec{x} - \vec{a})$ , where the elements of $\vec{a}$ are the coordinates of the point around which we're approximating, the elements of $\vec{x}$ are the inputs of $f$ , and $H$ is the Hessian matrix of $f$ . Following the pattern of single variable Taylor approximations, the cubic multivariable Taylor approximation should be $f(a, b) + f_x(a, b)(x - a) + f_y(a, b)(y - b) + \frac{1}{2}f_{xx}(a, b)(x - a)^2 + f_{xy}(a, b)(x - a)(y - b) + \frac{1}{2}f_{yy}(a, b)(y - b)^2 + \frac{1}{3!}f_{xxx}(a, b)(x - a)^3 + \frac{3}{3!}f_{xxy}(a, b)(x - a)^2(y - b) + \frac{3}{3!}f_{xyy}(a, b)(x - a)(y - b)^2 + \frac{1}{3!}f_{yyy}(a, b)(y - b)^3$ The amount of simplification in my coefficients is inconsistent, and I'm intentionally not generalizing the approximation order by introducing additional variables or sigma notation, as this expression already requires more than enough mental unpacking in my opinion, and it can only get worse in vector notation.  Instead I'll use an image and words to describe the pattern I have in mind. Every unique partial derivative up to the order of the approximation constitutes a term (third order approximation includes all zeroth, first, second, and third partial derivatives), and the numerator of the fraction coefficient for each term is the number of equivalent permutations of that partial derivative ( $f_{xyy} = f_{yxy} = f_{yyx}$ , so the numberator is $3$ ). Is this the correct extension of the quadratic multivariable Taylor approximation to cubic approximations, and a correct pattern for writing quartic and higher approximations, and how can we put this into vector notation?  An optimal answer, in order to avoid layering on too much abstraction, will likely include the expression for a cubic approximation in vector notation (possibly also quartic, if helpful) and an explanation in words of how to continue the pattern into higher order approximations (i.e., do we require an increasingly higher order ""tensor,"" a ""Hyperhessian"" of sorts, as we continue to add more terms?).","['permutations', 'tensors', 'multivariable-calculus', 'taylor-expansion', 'partial-derivative']"
3024798,$\det(A^2+A-I_2)+\det(A^2+I_2) = 5$,"Let $A \in M_{2\times 2}(\mathbb{C})$ and $\det(A)=1\DeclareMathOperator{\tr}{tr}$ Prove that $\det(A^2+A-I_2)+\det(A^2+I_2) = 5$ using Cayley-Hamilton Theorem $A^2-\tr(A)A+\det(A)I_2=0$ $\det\big(\tr(A)A-\det(A)I_2-I_2\big) + \det\big(A(A+A^{-1})\big)=5$ $\det\big(\tr(A)A-I_2(\det(A)+1)\big)+\det(A+A^{-1})=5$ using https://math.stackexchange.com/q/1937052 $\det(A+B)=\det A+\det B+\det A⋅\tr(A^{-1}B) $ $\tr(A)\det(A)-\big(\det(A)+1\big)\det(I_2)+\tr(A)\big(-\det(A)-1\big)^{-1}\tr(I^{-1}A)+\det(A)+\det(A^{-1})+\tr(A^2)=5
$ $\tr(A)-2-\tr(A)^20.5+1+1+\tr(A^2)=5$ $\tr(A)-\tr(A)^20.5+\tr\big(\tr(A)A-\det(A)I_2\big)=5$ $\tr(A)-\tr(A)^20.5+\tr(A)^2-\tr(I_2)=5$ $\tr(A)+0.5\tr(A)^2-2=5$ This is where I am stuck. And also I don't know how to prove the identity which I cited.","['determinant', 'trace', 'proof-verification', 'matrices', 'linear-algebra']"
3024800,Asymptotic of sum $\sum_{j=1}^n j^{f(n)}$,"What is known about the asymptotic of $\sum_{j=1}^n j^{f(n)}$ where the exponent is some function that grows with $n$ ?  For instance, if $f(n) = k$ is constant, then we know it's $\frac{1}{k+1}n^{k+1} + O(n^k)$ .  If $f(n) = n$ , it seems that the sum is dominated by the last few terms and behaves like a geometric series with $r=1/e$ , so that the sum grows as $n^n\frac{e}{e-1}$ (plus some error term).  What happens if e.g. $f(n) = n^\alpha$ for $0 < \alpha < 1$ or $f(n) = \log n$ ?","['summation', 'asymptotics', 'analysis', 'real-analysis', 'limits']"
3024831,Find a new point on a rectangle given an angle from the center,"I am a software engineer and I'm trying to edit some images.  Trying to find a good formula to find a new point on the rectangle, I know the center x and y coordinates, the height and width of the rectangle, and the theta in the direction of the new point. if I can find the length of the line from the center to the edge I can just take a sin and cos to get my new position.  The issue I'm having is that it seems like there are a lot of special cases given different angles for whether I should be shifting it with regards to width or height etc and I was hoping that someone would have a simpler formula I could use. Thanks a lot for taking the time to try and help me!",['trigonometry']
3024836,"Congruence proof. Show $a$ is odd, $b$ is even, and $ a \equiv 1$ mod $4 $ under certain conditions.","If $ p \equiv$ $1$ mod $4$ , $ p = a^2 + b^2$ , and $a + bi \equiv 1$ mod $2+2i$ , then $a$ is odd and $b$ is even. Moreover, if $4|b$ , then $ a \equiv 1$ mod $4$ , and if $ 4 \nmid b$ , then $ a \equiv -1$ mod $4$ . Proof: $ a + bi \equiv 1(2+2i) $ implies that $ a + bi \equiv 1$ mod $2 $ . and so $a$ is odd and $b$ is even. Since $ 4 = -2(i-1)(i+1) $ it follows that if $4 \mid b$ then $a+bi \equiv a \equiv 1$ mod $2+2i$ . Taking conjugates $ a \equiv 1$ mod $2-2i $ . Thus $(2+2i)(2-2i) = 8 \mid (a+1)^2 $ and $ a \equiv 1$ mod $4$ . If $ 4 \nmid b $ then $ b = 4k + 2 $ for some $ k $ . Thus $ a + bi \equiv a+2i \equiv 1$ mod $2+2i $ . Since $ 2i \equiv -2$ mod $2+2i $ we have $ a \equiv 3 \equiv -1$ mod $2+2i $ . As before $ 8 \mid (a+1)^2 $ and so $ a \equiv -1$ mod $4 $ . So I'm trying to understand this proof, but unfortunately I'm a beginner.So I don't understand this proof. Here are my questions: 1) Why implies $ a + bi \equiv 1(2+2i) $ that $ a + bi \equiv 1$ mod $2 $ ? And why is this enough to say that $a$ is odd and $b$ is even? 2) Why do we have $a+bi \equiv a \equiv 1$ mod $2+2i$ , if $4|b$ ? ( second line of the proof ) 3) Why do we have $(2+2i)(2-2i) = 8 \mid (a+1)^2 $ and $ a \equiv 1$ mod $4$ ? I can imagine that these questions are very simple for you, but I really want to understand this proof. So please try to be accurate. Thank you.","['number-theory', 'modular-arithmetic', 'complex-numbers']"
3024837,Constructing an Infinite Family of Graphs,"In a number of graph theory books that I am working through I come across questions that ask something like $\textit{'Construct an infinite}$ $\textit{family of graphs with _________'}$ and state some property in the blank space that the family of graphs have to satisfy. These questions totally stump me, so my question is what, if any, is a good method of attack for these types of questions? Also what is a 'solution' to these questions meant to look like? As an example, I am currently trying to work through the following problem from John Meier's book $\textit{Groups, Graphs and Trees; An Introduction to the Geometry of Infinite Groups}$ that asks: Construct infinitely many distinct (2,3)-biregular graphs In this case I understand what a biregular graph is and can construct a number of distinct graphs by hand, however, I don't understand how to generalise it to get infinitely many distinct graphs of this type.","['graph-theory', 'combinatorics']"
3024859,Dilatation of a measurable set is a measurable set.,"Let $E$ Lebesgue  measurable set. Let $\lambda\in\mathbb{R},\lambda\neq 0$ . 
Let $\lambda E=\left\{\lambda a:a\in E\right\}$ . Show that $\lambda E$ is measurable. I have this. Let $f:E\to \lambda E$ $f(a)=\lambda a$ continuos and bijective. Now $g:\lambda E\to E$ with $g=f^{-1}$ . i.e. $g(b)=\frac{b}{\lambda}$ Then $\lambda E=g^{-1}(E)$ and $g$ continuous, therefore $\lambda E$ is measurable set.
It is correct?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3024896,Seeking Methods to solve $ I = \int_{0}^{\frac{\pi}{2}} \frac{\arctan\left(\sin(x)\right)}{\sin(x)}\:dx$,"I was wondering what methods people knew of to solve the following definite integral? I have found a method using Feynman's Trick (see below) but am curious as to whether there are other Feynman's Tricks and/or Methods that can be used to solve it: $$ I = \int_{0}^{\frac{\pi}{2}} \frac{\arctan\left(\sin(x)\right)}{\sin(x)}\:dx$$ My method: Let $$ I(t) = \int_{0}^{\frac{\pi}{2}} \frac{\arctan\left(t\sin(x)\right)}{\sin(x)}\:dx$$ Thus, \begin{align}
 I'(t) &= \int_{0}^{\frac{\pi}{2}} \frac{\sin(x)}{\left(t^2\sin^2(x) + 1\right)\sin(x)}\:dx = \int_{0}^{\frac{\pi}{2}} \frac{1}{t^2\sin^2(x) + 1}\:dx \\
&= \left[\frac{1}{\sqrt{t^2 + 1}} \arctan\left(\sqrt{t^2 + 1}\tan(x) \right)\right]_{0}^{\frac{\pi}{2}} = \sqrt{t^2 + 1}\frac{\pi}{2}
\end{align} Thus $$I(t) = \frac{\pi}{2}\sinh^{-1}(t) + C$$ Now $$I(0) = C = \int_{0}^{\frac{\pi}{2}} \frac{\arctan\left(0\cdot\sin(x)\right)}{\sin(x)}\:dx = 0$$ Thus $$I(t) =  \frac{\pi}{2}\sinh^{-1}(t)$$ And finally, $$I = I(1) = \int_{0}^{\frac{\pi}{2}} \frac{\arctan\left(\sin(x)\right)}{\sin(x)}\:dx = \frac{\pi}{2}\sinh^{-1}(1) = \frac{\pi}{2}\ln\left|1 + \sqrt{2}\right|$$","['integration', 'definite-integrals']"
3024947,Minimizing the sum of KL divergences,"Given a list of probability distributions $q_i$ , what distribution $p$ minimizes the sum of KL divergences (if they exist) to and from each of them? That is, how do I determine $$\operatorname*{argmin}_p \sum_i D_\text{KL}(p \mathbin{\Vert} q_i)$$ and $$\operatorname*{argmin}_p \sum_i D_\text{KL}(q_i \mathbin{\Vert} p)$$ I recall reading somewhere that, in the Jensen-Shannon divergence, \begin{align*}
  D_\text{JS}(p, q) &= \frac{D_\text{KL}(p \mathbin{\Vert} r) + D_\text{KL}(q \mathbin{\Vert} r)}{2} \\
  r = &\frac{p + q}{2}
\end{align*} The midpoint distribution $r$ is precisely $$r = \operatorname*{argmin}_s \frac{D_\text{KL}(p \mathbin{\Vert} s) + D_\text{KL}(q \mathbin{\Vert} s)}{2}$$ I can't find a reference for this, however.","['probability-distributions', 'probability', 'reference-request']"
3024968,Solving a functional equation arising from a probability problem: $g(kx)^2=g(x)$,"I am trying to find solutions to the following functional equation: $$g(kx)^2=g(x)\text.$$ Here, $x$ is in $\mathbb{R}$ and $k$ is a constant. In particular, I'm looking for solutions for $k=2^{-1/4}$ . Furthermore, I need that the Fourier inverse of $g$ is a density (i.e., nonnegative and integral over $\mathbb{R}$ is 1). The function $g(x)=e^{-x^4}$ satisfies the equation, but does not have a nonnegative Fourier inverse. I have deduced that $g(0)$ must be one, but have little experience with functional equations and am at a loss at how to proceed. (It may very well be the case that there are no other solutions.) The context is the following. I am tasked with finding (or showing that there exist none) i.i.d. random variables $X$ and $Y$ such that $\frac{X+Y}{2^{1/4}}\sim X$ . If you assume that $X$ and $Y$ have density $f$ , then standard Fourier arguments show that $\hat{f}=g$ must satisfy the functional equation above. The requirement that the Fourier inverse of $g$ be nonnegative comes from the fact that $f$ is a density. Any hint, either with the functional equation or the original problem, would be greatly appreciated. In particular, with regards to the original problem, I have been able to deduce that $E[X]$ is either 0 or infinite, and in either case, $E[X^2]$ is infinite, but I am not sure how to proceed to show either existence or non-existence.","['functional-equations', 'fourier-analysis', 'analysis', 'real-analysis', 'probability']"
3024982,Which Nonorientable 3 manifolds have torsion in $H_{1}$?,"In thinking about closed, nonorientable 3 manifolds I've been interested in finding nonorientable loops which represent torsion classes in homology. More precisely, for a closed nonorientable 3 manifold $M$ I've wondered when one can find a class $\gamma \in H_{1}(M; \mathbb{Z})$ so that $\gamma$ is torsion, and $w_{1}(M)[\bar{\gamma}]$ is nontrivial (where $w_{1}(M)$ is the first Stiefel-Whitney class of the tangent bundle and $\bar{\gamma}$ is the reduction mod 2 of $\gamma$ ). This has led me to the following questions: Can one characterize which nonorientable 3 manifolds $M$ have torsion classes in $H_{1}(M ; \mathbb{Z})$ ? My list of examples of nonorientable 3 manifolds is short, but among them I've noticed that some do: 1) $S^{1} \times N_{h}$ ,  where $N_{h} \cong (\mathbb{R}P^{2})^{\#h}$ and some don't: 2) $S^{1} \tilde{\times} S^{2}$ Secondly, For those 3 manifolds $M$ with torsion in $H_{1}(M; \mathbb{Z})$ , can one characterize which have torsion classes $\gamma$ satisfying $w_{1}(M)[\bar \gamma]$ =1? I've noted here as well that some do: $S^{1} \times N_{1}$ and some don't: $S^{1} \times N_{2}$ .","['geometric-topology', 'general-topology', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
3025016,Why is the metric on a hyperboloid different than the metric on a sphere?,"I am talking about the unit sphere and unit hyperboloid in $\mathbb{R}^3$ . To get a metric you might use the riemann metric and the length of a curve. To calculate the length of a curve $\gamma$ from $a$ to $b$ we have $$l_{g}(\gamma) = \int_{a}^{b} \sqrt{g_{\gamma(t)}(\gamma^{'}(t),\gamma^{'}(t))} dt$$ with a dot product $g$ . Somehow in the spherical case this dot product is induced by the standard dot product from $\mathbb{R}^3 : \langle x,y\rangle = x_1y_1+x_2y_2+x_3y_3$ while in the hyperbolic case we use the Minkowski dot product from the Minkwoski space $\langle x,y\rangle_{\rm Minkowski} = x_1y_1+x_2y_2-x_3y_3$ Although the euclidean, spherical und hyperbolic surface do not share any curves, the same curve $\gamma$ would have the same length in euclidean and spherical geometry and a different length in hyperbolic geometry. 
This makes us especially bad in guessing hyperbolic distances while we are okay in the spherical case. But why is this the case? I cannot think of a difference of the hyperbolic surface and spherical surface that would justify the different dot products. They are both quadrics. One has positive curvature the other has negative. Does this make a difference?","['geometry', 'riemannian-geometry', 'differential-geometry']"
3025036,"decomposition group and inertia group, the minimal polynomial,surjectivity of the map $D_{M/P}\rightarrow Gal$","Can anyone explain the underlined sentence? For notation, A:Dedekind domain, K=Frac(A), L/K:Galois extension, B:The integral closure of A in L, M:A maximal ideal of B, P:The intersection of M and A (hence the maximal ideal of A), $D_{M/P}$ :the decomposition group. I reckon the way we take $\alpha$ is the key, but cannot make it to the conclusion, 'we find that the only non-zero roots of...'. I read some of the close questions already answered but none of them was using this type of logic. Thank you in advance.","['algebraic-number-theory', 'number-theory', 'galois-theory', 'abstract-algebra', 'arithmetic-geometry']"
3025149,Find all infinite sets of integer solutions,"The solution to a math problem that I was working turned out to be the formula $$r + \sqrt{w(2r-w)}$$ My new question became, what are all the possible solutions for $r$ and $w$ where $r$ and $w$ are integers $> 0$ such that the expression yields an integer? I tried to show that there are infinite solutions for $r$ when $w=2$ . $$2(2r-2) = (2n)^2$$ $$2r - 2 = 2n^2$$ $$r-1=n^2$$ $$r=n^2 + 1$$ for when $n > 1$ so that $r > 0$ . For example, when $w=2, n = 6$ $$r = 6^2 + 1 = 37$$ plugging $w$ and $r$ back into the expression yields an integer. $$37 + \sqrt{2(2 \cdot 37 - 2)} = 49$$ I began to realize that there seems to be an infinite amount of integers $r$ for any given $w$ . I wrote a program that finds $r$ for any given $w$ and compiled them into a table. I discovered that the set for $r$ can be generated by the function $an^2+bn+c$ where $n$ is an integer $>0$ $$\begin{array}{c|c|c|} 
\text{w} & \text{r} & \text{a} & \text{b} & \text{c}\\ \hline
\text{1} & \{5, 13, 25, 41, 61, 85, 113, 145, ...\} & 2 & 2 & 1 \\ \hline
\text{2} & \{5, 10, 17, 26, 37, 50, 65, 82, ...\} & 1 & 2 & 2 \\ \hline
\text{3} & \{15, 39, 75, 123, 183, 255, 339, 435, ...\} & 6 & 6 & 3 \\ \hline
\text{4} & \{10, 20, 34, 52, 74, 100, 130, 164, ...\} & 2 & 4 & 4 \\ \hline
\text{5} & \{25, 65, 125, 205, 305, 425, 565, 725, ...\} & 10 & 10 & 5 \\ \hline
\text{6} & \{15, 30, 51, 78, 111, 150, 195, 246, ...\} & 3 & 6 & 6 \\ \hline
\text{7} & \{35, 91, 175, 287, 427, 595, 791, 1015, ...\} & 14 & 14 & 7 \\ \hline
\text{8} & \{13, 20, 29, 40, 53, 68, 85, 104, ...\} & 1 & 4 & 8 \\ \hline
\text{9} & \{17, 29, 45, 65, 89, 117, 149, 185, ...\} & 2 & 6 & 9 \\ \hline
\text{10} & \{25, 50, 85, 130, 185, 250, 325, 410, ...\} & 5 & 10 & 10 \\ \hline
\end{array}$$ I noticed a pattern in the polynomial generating functions, and attempted to describe it in a piecewise function. Because of my programming background, I am having a difficult time expressing it in math terms. Consider $q(w)$ a function that returns a function in terms of n where $n > 0$ that will generate all possible integer $r$ for a given $w$ . The order of the piecewise must be followed from top to bottom, so if 2 is passed as $w$ , the piecewise will use the odd power of 2 case before the even case. $$
 q(w) =
\begin{cases}
2n^2 + 2n\sqrt{w} + w,  & \text{if $w$ is square} \\
n^2 + n \cdot 2^{(log_2(w) + 1)/2} + w, & \text{if $w$ is odd power of 2 (ex. $2^{3}$)} \\
w \cdot q(1), & \text{if $w$ is odd} \\
w / 2 \cdot q(2), & \text{if $w$ is even} \\
\end{cases}
$$ For example, $q(25)$ generates $$2n^2 + 2n \sqrt{25} + 25$$ which will yield all $r$ for $w=25$ given any integer $n$ where $n > 0$ For example, choosing $n=12$ (an arbitrary integer $> 0$ ) yields $$2(12)^2 + 2(12)\cdot 5 + 25 = 433$$ If we plug $w=25$ and $r=433$ into our original expression, it evaluates to an integer: $578$ . From what I can tell, this piecewise function covers all the possible solutions for $r$ and $w$ , as checked by a computer program. Of course, I have proved nothing here, and I would like to know how to prove something like this, and I would especially like to know if there is a better way to go about solving this and if there was a simple answer that I missed. Thank you for all replies.","['number-theory', 'discrete-mathematics']"
3025208,Harmonic functions in the half-plane,"Denote by $\mathbb{H}$ the upper half-plane $$
\mathbb{H} := \left\{ x \in \mathbb{R}^n : x_n > 0\right\}. 
$$ Suppose that $u \in C^2(\mathbb{H}) \cap C(\bar{\mathbb{H}})$ is a bounded harmonic function such that $u \leq 0$ on $\partial\mathbb{H} = \{ x_n = 0\}$ . Is it possible to conclude that $u \leq 0$ in all of $\mathbb{H}$ ? I know this is the case for $n = 2$ but am unable to establish the general case $n \geq 3$ .","['harmonic-functions', 'analysis', 'partial-differential-equations']"
3025226,What conditions on the moments make a measure a probability measure?,"For a positive Borel measure $\mu$ on the real line, let $\displaystyle{m_n = \int_{-\infty}^\infty x^n d\mu(x)}$ , i.e. the $n$ th moments of the measure.  Are there any conditions on $m_n$ for when $\mu$ can be made into a probability measure, i.e. $\mu(\mathbb{R}) < \infty$ ?  Recall that the moments of a uniform distribution on $[-1,1]$ are $1/(n+1)$ for even $n$ .  The measure given by density $p(x)= 1/(2|x|)$ for $x \in [-1,1]$ has moments $1/n$ for even $n$ .  So it seems that two moment sequences with very similar asymptotic behavior can give different answers to this question. Edit I should specify that $m_0$ is of course not given, since one can read off the answer from the $0$ th moment.","['moment-generating-functions', 'probability-distributions', 'probability-theory', 'moment-problem']"
3025231,What is the length of the hypotenuse?,"We have $n$ isosceles-right-angled triangles. The hypotenuse of the $n^{\textrm{th}}$ triangle is the base of the $(n+1)^{\textrm{th}}$ triangle. For the first triangle, $T_{1}$ , the length of the base is $1$ unit. What is the length of the hypotenuse of $T_{25}$ ? One way of doing this, is to find the hypotenuses of all triangles (one-by-one). So the hypotenuse of the first triangle, $H_{1}=\sqrt{1^2+1^2}=\sqrt{2}$ . The hypotenuse of the second triangle, $H_{2}=\sqrt{(\sqrt{2})^2+(\sqrt{2})^2}=2$ , and so on until we reach $H_{25}$ which is the hypotenuse of $T_{25}$ . The mentioned way is tedious, is there a better way?","['triangles', 'trigonometry', 'geometry', 'polygons']"
3025282,"Limit, Riemann Sum, Integration, Natural logarithm","For any natural number $m$ , $\lim_{n\rightarrow \infty }\left ( \frac{1}{n+1}+\frac{1}{n+2}+\frac{1}{n+3}+\cdots +\frac{1}{mn} \right )=\ln (m)$ . I tried to prove the statement in the following way. Proof: $$\lim_{n\rightarrow \infty }\left ( \frac{1}{n+1}+\frac{1}{n+2}+\frac{1}{n+3}+\cdots +\frac{1}{mn} \right )=\lim_{n\rightarrow \infty }\sum_{r=1}^{(m-1)n}\frac{1}{n+r}$$ Dividing the numerator and the denominator of $\frac{1}{n+r}$ by $n$ , we get $\frac{1/n}{1+r/n}$ . Therefore, $$\lim_{n\rightarrow \infty }\sum_{r=1}^{(m-1)n}\frac{1}{n+r}=\lim_{n\rightarrow \infty }\frac{1}{n}\sum_{r=1}^{(m-1)n}\frac{1}{1+r/n}$$ this is a Riemann sum, so replacing $\frac{1}{n}$ with $dx$ , $\frac{r}{n}$ with $x$ , and integrating between the limits $x=0$ and $x=m-1$ we get $ $$\int_{0}^{m-1}\frac{dx}{1+x}=\ln(1+x)|_{0}^{m-1}=\ln(m)-\ln(1)=\ln(m)\blacksquare$$ Is this a valid way?","['integration', 'logarithms', 'riemann-sum', 'limits', 'riemann-integration']"
3025329,an inverse of the Artin-Hasse exponential?,"In the p-adic world the Artin-Hasse exponential is the sollowing power series: $$
  E_p(x)= \exp \left( \sum_{n=0}^{\infty}\frac{x^{p^n}}{p^n} \right)
$$ where $E_p(x)\in 1+x\mathbb{Z}_{(p)}[[x]]$ with radius of convergence $r=1$ . My question is : as the classical exponential it is possibile define a sort of 'logarithm' which invert this series? Thanks for the suggestions!","['power-series', 'number-theory', 'p-adic-number-theory', 'exponential-function']"
3025375,Finding $\lim\limits_{n→∞}n^3(\sqrt{n^2+\sqrt{n^4+1}}-n\sqrt2)$,"What is $$\lim_{n→∞}n^3(\sqrt{n^2+\sqrt{n^4+1}}-n\sqrt2)?$$ So it is $$\lim_{n→∞}\frac{n^3(\sqrt{n^2+\sqrt{n^4+1}})^2-(n\sqrt{2})^2}{\sqrt{n^2+\sqrt{n^4+1}}+n\sqrt{2}}=\lim_{n→∞}\frac{n^3(n^2+\sqrt{n^4+1}-2n^2)}{\sqrt{n^2+\sqrt{n^4+1}}+n\sqrt{2}}.$$ I do not know what to do next, because my resuts is $∞$ but the answer from book is $\dfrac{1}{4\sqrt{2}}$ .","['limits', 'calculus']"
3025386,If $A\times B$ is an element of a product $\sigma$-algebra $\mathcal A\times\mathcal B$ then do we have $A\in\mathcal A$ and $B\in\mathcal B$?,"If $\mathcal A$ is a $\sigma$ -algebra on set $X$ and $\mathcal B$ is a $\sigma$ -algebra on set $Y$ then it is well known that $\mathcal A\times\mathcal B$ is the notation for a $\sigma$ -algebra on $X\times Y$ that is generated by sets $A\times B$ with $A\in\mathcal A$ and $B\in\mathcal B$ . So evidently we have: $$A\in\mathcal A\text{ and }B\in\mathcal B\implies A\times B\in\mathcal A\times\mathcal B\tag1$$ Now my question: Is the converse of $(1)$ also true? I have always believed it is without bothering, but when I tried to find a proof for this ""obvious"" fact I regretfully failed.","['measure-theory', 'probability-theory']"
3025415,$\overline{f}$ is isomorphism in abelian category,"Suppose $f: A \longrightarrow B$ is a morphism in an abelian category $\mathcal{C}$ . What I consider an abelian category: $\mathcal{C}$ is additive. Every morphism has a kernel and a cokernel. Every monomorphism is a kernel and every epimorphism is a cokernel. With that, we can define: $Im(f)= kernel(cokernel(f))$ $Coim(f)=cokernel(kernel(f))$ where $k: K \longrightarrow A$ is a kernel of $f$ if $ k \circ f = 0_{K,B}$ and whenever $h \circ f = 0$ , $h$ factors uniquely through $k$ . (i.e. $h= k \circ h'$ ). And $q:B \longrightarrow C$ is a cokernel of $f$ if $ f \circ q = 0_{A,C}$ and whenever $f \circ h = 0$ , $h$ factors uniquely through $q$ (i.e. $h = h' \circ q$ ). Notation : $0_{A,B}$ is the zero morphism obtained composing $A \longrightarrow 0$ and $0 \longrightarrow B$ . Once I have defined $Im(f)$ and $Coim(f)$ , I want to check that there exists a natural map between them, called $\overline{f}$ which is isomorphism. I have been working with epimorphisms and monomorphisms notions but I am a little bit lost. Any help/hint? Related but do not understand: Equivalent conditions for a preabelian category to be abelian","['abelian-categories', 'abstract-algebra', 'category-theory', 'commutative-algebra']"
3025435,"""Seeing yourself"" in a 3-manifold","In his book ""The Shape of Space"" (2nd ed), Jeffrey Weeks talks about a technique called ""Cosmic Cristallography"" to find the global topology of the space we live in.  Starting with the assumption that we're living in a closed 3-manifold and that the observable universe is larger than the real universe, he says that if we see multiple images of the Milky Way, the 3-manifold is multiconnected (like e.g. a 3-torus) while otherwise it would be simply connected. I don't get this.  Wouldn't we see multiple images even if we lived in a 3-sphere? Imagine you're living on a 2-sphere and your world is 2-dimensional and there's a southbound light ray starting at the North Pole, ""traveling"" along a geodesic (meridian) and reaching, say, Hamburg.  If there's another light ray in the opposite direction it'll travel around the South Pole but eventually also reach Hamburg, won't it? EDIT: In reply to a comment, here a some more details. In a previous chapter, Weeks says that the focus is on ""homogeneous, isotropic 3-manifolds"".  He then includes a list of possible candidates with their geometries which includes the 3-sphere (elliptic) and the 3-torus (Euclidean). The relevant sentences then are: ""In a multiconnected space we see multiple images of ourselves.  So testing whether the real universe is multiconnected or not is easy, right?  We just point our telescopes out into the night sky. If we see images of our Milky Way galaxy out there, then the universe is multiconnected. If we don't see images of the Milky Way, then either space is simply connected, or it's multi connected but on too large a scale for us to observe it.""","['manifolds', 'general-topology']"
3025506,Computing $2^{2^1}+2^{2^2}+2^{2^3}+\cdots+2^{2^n}$,"How can I compute the following sum? $$2^{2^1}+2^{2^2}+2^{2^3}+\cdots+2^{2^n}$$ My attempt was to apply the known formula for the sum of an geometric progression, but it seems that the ratio is variable. So there is a formula for this type of sum?","['calculus', 'algebra-precalculus']"
3025509,Cohomology ring of $H^*(A_5;\mathbb Z_2)$?,"What is the cohomology ring of $H^*(A_5;\mathbb Z_2)$ ? Here $A_5$ is the alternating group on $5$ letters. I am comfortable with the Lyndon-Hochschild-Serre spectral sequence, and understand how to use it to compute the cohomology of $A_4$ . However, because $A_5$ is simple, there is no extension $1 \to G \to A_5 \to H \to 1$ , so I'm not sure how to use this.","['group-theory', 'abstract-algebra', 'group-cohomology', 'algebraic-topology']"
3025518,Difference between arcsin and inverse sine.,"I first learned that arcsin and inverse sine are two ways of saying the same thing. But then I was thinking about the inverse sine function being a function, so it must be limited in it's range from -1 to +1. What would you call the sine function reflected through the line y=x that is not limited in range ? Sure, it would not be a function but what would you call this graph ? Could it be that arcsin is not a function and has infinite solutions whereas inverse sine is a function and has only one solution, e.g. $\arcsin (0.5) = \frac{\pi}{6}+2n\pi,n \in\mathbb Z, \sin^{-1}(0.5)=\frac{\pi}{6}$ ? If not and they both have only one solution then how would you express the graph that has infinite solutions ? I found this: https://www.dummies.com/education/math/trigonometry/how-to-distinguish-between-trigonometry-functions-and-relations/ It seems a convention of capital/small letters are used to distinguish functions and relations.","['trigonometry', 'inverse-function']"
3025526,Uniqueness of the maximizer of the Hardy inequality,"CONVENTION . All functions in this post are nonnegative and defined on $[0, \infty)$ . The Hardy operator is $$
Hf(x)=\frac1x\int_0^x f(y)\, dy, $$ and, as it is well known, it satisfies the inequality $$\tag{H}
\|Hf\|_{p}\le \frac{p}{p-1}\|f\|_{p},\quad \text{for }p>1, $$ where the constant $p/(p-1)$ is the best possible, in the sense that (H) fails if it is replaced by any strictly smaller one. In a vague sense, the function $f(x)=x^{-1/p}$ is a maximizer for (H), if one ""neglects a logarithmic divergence in both sides"" (as said in this answer of Terry Tao ). More precisely, as shown, for example, in the answers to this old question of mine , some sequences that approximate $f$ also saturate (H); it is the case of $$
f_n(x)=x^{-\frac1p -\frac1n}\mathbf 1_{(1, \infty)} $$ and of $$
f_n(x)=x^{-\frac1p}\mathbf 1_{(1, n)}.$$ My question is, roughly speaking: is $f(x)=x^{-1/p}$ the ""only"" maximizer to (H)? More precisely: Question . Suppose that $$ \frac{\|Hf_n\|_p}{\|f_n\|_p}\to \frac{p}{p-1}.$$ Does there exist a sequence $\lambda_n>0$ such that $$f_n(\lambda_nx)\to x^{-\frac1 p}\mathbf 1_{(1, \infty)},$$ almost everywhere on $(0, \infty)$ ? Remark . The presence of $\lambda_n$ is necessary to prevent trivial counterexamples; indeed, the ratio $\|Hf\|_p / \|f\|_p$ is invariant under the scaling transformation $$f\mapsto f_\lambda(x)=f(\lambda x).$$ FINAL NOTE . David C. Ullrich gave an exhaustive negative answer to the present question. His answer was edited heavily and so it may be a bit hard to read. Please see my Summary to David C. Ullrich's answer .","['harmonic-analysis', 'analysis', 'real-analysis', 'optimization', 'inequality']"
3025527,Is the graph $y=\frac{x^2}{x}$ continuous?,"I randomly came across the thought today that since dividing by zero is undefined, will the graph $y=\frac{x^2}{x}$ be discontinuous? Or would it be considered reducible and therefore continuous and identical to the graph $y=x$ . Thanks everyone.","['continuity', 'calculus', 'functions', 'rational-functions']"
3025528,How does group cohomology behaves where coefficient is direct sum?,"Let $G$ be a finite group and $A_1$ and $A_2$ be $\mathbb ZG$ modules. Is it always true that for any $n\in \mathbb N$ , the cohomology group of $G$ with coefficients in $A_1\oplus A_2$ is the same as the following: $$H^n(G; A_1\oplus A_2)=H^n(G; A_1)\oplus H^n(G; A_2) ?$$ If not, then how to calculate $H^n(G; A_1\oplus A_2)$ in terms of $H^n(G; A_1)$ and $H^n(G; A_2)$ ? Thanks in advance.","['group-theory', 'abstract-algebra', 'group-cohomology', 'algebraic-topology']"
3025549,Least Exactly Problem,"I read following problem and solution: In a recent test of $100$ students, $95$ answered question #1 correctly $75$ answered question #2 correctly $97$ answered question #3 correctly $95$ answered question #4 correctly $96$ answered question #5 correctly What is the smallest number of students who could have answered exactly $4$ of the $5$ questions correctly? The solution is: A maximum of $75$ could have answered all questions correctly. Of the remaining $25$ , the $5$ who got question #1 wrong, the $3$ who got the question #3 wrong, the $5$ who got question #4 wrong and the $4$ who got question #5 wrong could have been uniquely different giving $17$ who got exactly $3$ questions correct. This leaves $8$ who got exactly $4$ questions correct. My question: What maths topic/book is this problem involved? Is ""uniquely different"" a term? (it looks like $5 + 3 + 5 + 4 = 17$ , but what does it mean? Or why it means who got exactly $3$ questions correct?) Why the ""least exactly $4$ "" problem is $8$ ( $25 - 17 = 8$ )? As an extension, What is the smallest number of students who could have answered exactly $1/5, 2/5, 5/5$ questions correctly? Thank you in advance. M",['combinatorics']
3025640,Evaluate the limit of $\lim_{n\to\infty}\frac{1}{n^2}\left(\frac{2}{1}+\frac{9}{2}+\frac{64}{9}+\cdots+\frac{(n+1)^{n}}{n^{n-1}}\right)$,$$\lim_{n\to\infty}\frac{1}{n^2}\left(\frac{2}{1}+\frac{9}{2}+\frac{64}{9}+\cdots+\frac{(n+1)^{n}}{n^{n-1}}\right)$$ My try: The limit can be written as follows: $$\lim_{n\to\infty}\left(\frac{1}{n^2}\cdot\sum_{k=1}^{n}\frac{(k+1)^{k}}{k^{k-1}}\right)$$ Evaluate the following series: $\sum_{k=1}^{\infty}\frac{(k+1)^{k}}{k^{k-1}}$ $\frac{(k+1)^{k}}{k^{k-1}}=k\cdot\frac{(k+1)^{k}}{k^{k}}=k\cdot\left(1+\frac{k+1}{k}-1\right)^k=k\cdot\left(1+\frac{1}{k}\right)^k$ Then: $\lim_{k\to\infty}\frac{(k+1)^{k}}{k^{k-1}}=\lim_{k\to\infty}k\cdot\left(1+\frac{1}{k}\right)^k=e\cdot\infty\neq0 \Longrightarrow \sum_{k=1}^{\infty}\frac{(k+1)^{k}}{k^{k-1}}$ diverges. Therefore: $$\lim_{n\to\infty}\frac{1}{n^2}\left(\frac{2}{1}+\frac{9}{2}+\frac{64}{9}+\cdots+\frac{(n+1)^{n}}{n^{n-1}}\right)=0\cdot\infty$$ What to do next?,"['limits', 'calculus', 'sequences-and-series']"
3025711,Potential for Monotone Operator,"I have a question about understanding the proof of Theorem 4.11 in the paper A Potential Theory for Monotone Multivalued Operators (accessible here ).  The authors claim to construct a convex functional and I'm not sure I follow their argument. My specific question is at the end, but I provide some background from the paper below before. Background : The paper shows, how for a pair of dual locally convex topological vector spaces $(X,X')$ and a monotone set-valued operator $M:X \to X'$ , one can define a notion of path integral along polygonal paths (as the restriction of $M$ to any straight line in one's is monotone and hence Riemann-integrable). The authors call $M$ conservative if its path integral around any closed polygonal path in its domain (the set of points of $X$ where it is non-empty valued) is zero.  The authors define the integral of $M$ along any line segment $[x,y] \subseteq \textrm{dom}(M)$ via: $$
\int_{0}^1 \langle M(x + t(y-x)), y-x \rangle \, dt = \sup \bigg\{\sum_{i=0}^{n-1}\langle x_i^*, x_{i+1} - x_i\rangle\bigg\} = \inf\bigg\{\sum_{i=0}^{n-1}\langle x_{i+1}^*, x_{i+1}- x_i\rangle \bigg\}
$$ where $x_i^* \in M(x_i)$ , and the sup/inf are over all refinements of the line segment, and just follow from their respective arguments being the left/right Riemann sums of monotone increasing functions. The authors then state the following theorem (4.11, p. 623), which I reproduce below. Theorem 4.11 :  To any conservative monotone multivalued map $M:X \to X'$ with a polygonally path connected domain, there corresponds, to within an arbitrary additive constant, a convex potential $f: X \to \mathbb{R} \cup \{+\infty\}$ , which is the restriction on $\textrm{dom}(M)$ of a lower semicontinuous proper convex functional.  The potential $f$ is assumed to be $+\infty$ outside $\textrm{dom}(M)$ and is defined on $\textrm{dom}(M)$ by: $$
\begin{aligned}
f(x) - f(x_0) & = \int_\pi \langle M(z), dz\rangle = \\
& =\sup\bigg\{\sum_{i=0}^{n-1}\langle x_i^*, x_{i+1}- x_i\rangle + \langle x_n^*, x- x_n\rangle \bigg\}\\
& = \inf\bigg\{\sum_{i=0}^{n-1} \langle x_{i+1}^*, x_{i+1} -x_i\rangle + \langle x^*, x-x_n\rangle\bigg\}
\end{aligned}
$$ where the sup/inf are again over all refinements of the poylgonal path $\pi$ . Source of confusion : The proof argues that, by definition, on $\textrm{dom}(M)$ , $f$ is equal to the lower semicontinuous proper convex function defined as the pointwise supremum of a family of continuous affine functions, the Riemann sums.  I don't follow this step.  Normally, when I have seen that results about the supremum of a family of affine functionals is convex, the family of functionals does not vary point to point, whereas here it seems to, as long as $\textrm{dom}(M)$ is not convex (which the authors are explicitly allowing for).  For example, if I have two points $x,y \in \textrm{dom}(M)$ but for which the line segment connecting them is not, it is not clear to me that how the suprema of the set of affine functionals given by refining a path $\pi_x$ from $x_0$ to $x$ relates to the suprema over refinements for a given path $\pi_y$ . I'd be happy to provide my attempt to verify too and where I get stuck, but as this question is already fairly long I'll leave it for now, and I suspect the answer is probably something simple. Question : Why is $f$ lower semicontinuous/convex?","['real-analysis', 'functional-analysis', 'vector-analysis', 'potential-theory', 'convex-analysis']"
3025737,Where does one even need Bernoulli's Inequality?,"Which Theorems/Lemmas/Results actually use Bernoulli's inequality? I don't seem to remember using it very often - which probably makes sense, as it's not a very strong inequality and can be proven easily. However, where do you actually use Bernoulli?","['inequality', 'real-analysis']"
3025762,Area of triangle using double integrals,"I have one (rather simple) problem, but I'm stuck and can't figure out what I'm constantly doing wrong. I need to calculate area of triangle with points at $(0,0)$ , $(t,0)$ , $(t,\frac{t}{2})$ . In other words triangle under function $y=\frac{x}{2}$ , for $x\in [0,t]$ I thought it is calculated with $$ \int_0^t \int_0^\frac{t}{2} dudv$$ But it turns out that this equals to $\frac{t^2}{2}$ , when obviously this area is $\frac{t\times\frac{t}{2}}{2} = \frac{t^2}{4}$ .
What am I doing wrong here?
I need to calculate it this way, not with single integral, or geometrically.","['integration', 'multivariable-calculus', 'multiple-integral', 'area']"
3025774,Non convergence of a series of random variables,"Question: Let $(X_n), n\in\mathbb{N}$ be a sequence of independent r.v.s such that $P(X_n=n^4)=\frac{1}{n^4}$ and $P(X_n=-1)=1-\frac{1}{n^4}$ . Study the a.s. convergence of $S_n=\sum_{i=1}^n X_n$ as $n\rightarrow +\infty$ . My Attempt: I have been simulating the stochastic process $S_n$ in R to understand whether convergence was possible at all but in none of the simulations I performed I obtained a finite value (all of the paths go to -9999). Also, clearly, $\sum_{i=1}^n \operatorname{Var}(S_n)\rightarrow +\infty$ as $n\rightarrow +\infty$ . Can I thus conclude that $S_n$ does NOT converge a.s.? Many thanks in advance for the help!","['borel-cantelli-lemmas', 'convergence-divergence', 'probability-theory', 'probability']"
3025828,should I learn measure theory before learning probability?,I am currently looking to learn about probability and statistics since I am interested in actuarial science. I have some knowledge on real analysis(rudins book except the last 2 chapters) and linear algebra(axlers linear algebra done right). I have very little prior knowledge about prob/stat. When researching prob/stat books to order I encountered the distinction between books that use measure theory and those that don't. Anyway I am not really sure where to start and was wondering if someone could kindly recommend some books and which order to read them in.,"['measure-theory', 'book-recommendation', 'probability']"
3025846,"You throw a dice 3 times,probabilty whats the probability of the number k to be the highest?","You throw a fair dice 3 times,
let X be the highest number you will get. What is the PDF of $P(X=K), K=1,...6$ ? The answer is : $$\frac{k^3-(k-1)^3}{216}$$ However, I can't understand it, could help me to understand why is this the right answer? Thank you","['statistics', 'dice', 'probability']"
3025871,$f''(x)\leq 0$ implies $f$ concave,"I'm using the following definition: $f:[0,\infty)\rightarrow[0,\infty)$ is concave if $\forall x,y\in[0,\infty)$ and $s\in[0,1]$ , we have $$f(sx+(1-s)y)\geq sf(x)+(1-s)f(y) $$ I need to prove that every function $f:[0,\infty)\rightarrow[0,\infty)$ twice differentiable satisfying $f''(x)\leq 0$ for all $x\in[0,\infty)$ is concave. I found the reciprocal, but not this statement. Well, I know that $f''(x)\leq 0$ implies that for every $x<y$ , $f'(y)\leq f'(x)$ . Can someone give me just some hints?","['functions', 'derivatives']"
3025910,Show there can be no co-ordinate patch at this point,"I am attempting to prove that the subset of $\mathbf{R}^3$ satisfying $x^2 + z^2 = y^2$ is not a surface, where a surface is a subset of $\mathbf{R}^3$ for every point in which there is a co-ordinate patch whose image contains that point and is contained in the subset and a co-ordinate patch is a smooth, injective, regular function from an open region of $\mathbf{R}^2$ to $\mathbf{R}^3$ . Having seen this visualized, I know this space is composed of two infinite cones joined at their tips (that is, at the origin). Intuitively, I then know that this is the problematic point that stops the entire set being a surface. However, I am having trouble constructing a reason why there cannot be a co-ordinate patch containing the origin; that is, showing which of the conditions on a co-ordinate patch cannot be satisfied, and why. My attempt : If $\bar{x} : D \to \mathbf{R}^3$ were such a co-ordinate patch, I feel that because $D$ is open and $\bar{x}$ is continuous that there would necessarily be a ""jump"" from one of the cones composing the space to the other. As in, there would exist some point on the cone with positive $y$ and some point on the other cone with negative $y$ whose inputs in $D$ are close but which are obviously not close on the space. Any help is greatly appreciated. Thank you in advance.","['manifolds', 'multivariable-calculus', 'surfaces', 'differential-geometry']"
3025913,Number of solutions of the equation $\cos(\pi\sqrt{x-4})\cos(\pi\sqrt{x})=1$,"Find the number of solutions of the equation $\cos(\pi\sqrt{x-4})\cos(\pi\sqrt{x})=1$ \begin{align}
2\cos(\pi\sqrt{x-4})&.\cos(\pi\sqrt{x})=2\\\implies\cos\Big[\pi(\sqrt{x-4}+\sqrt{x})\Big]&+\cos\Big[\pi(\sqrt{x-4}-\sqrt{x})\Big]=2\\
\implies\cos\Big[\pi(\sqrt{x-4}+\sqrt{x})\Big]=1\quad&\&\quad\cos\Big[\pi(\sqrt{x-4}-\sqrt{x})\Big]=1\\
\pi(\sqrt{x-4}+\sqrt{x})=2n\pi\quad&\&\quad\pi(\sqrt{x-4}-\sqrt{x})=2m\pi\\
\sqrt{x-4}+\sqrt{x}=2n\quad&\&\quad\sqrt{x-4}-\sqrt{x}=2m\\
2\sqrt{x}=2(n-m)\quad&\&\quad2\sqrt{x-4}=2(n+m)\\
\sqrt{x}=n-m\quad&\&\quad\sqrt{x-4}=n+m\quad\&\quad x\geq4
\end{align} How do I properly find the solutions ? Or can I simply say $$
x=(n-m)^2=(n+m)^2-4nm=x-4-4nm\implies nm=-1\\
\implies x=\bigg[n+\frac{1}{n}\bigg]^2\in\mathbb{Z}\implies n,\frac{1}{n}\in\mathbb{Z}\\
\implies n\neq0\implies n=1,x=4\text{ is the only solution}
$$",['trigonometry']
3025920,What is a principal minor of a matrix?,I was going through the book on operation research by Hamdy A.Taha. It referred to principal minor of a hessian matrix. Can someone explain what is meant by a principal minor? Is it different from 'minor of a matrix'?,"['matrices', 'optimization', 'operations-research']"
3025955,Not sure what derivative to take,"I have a relation $$\frac{(x\cos(\tan^{-1}(x_1))-y\sin(\tan^{-1}(x_1)))^2}{a^2}+\frac{(y\cos(\tan^{-1}(x_1))+x\sin(\tan^{-1}(x_1)))^2}{b^2}=1$$ $a^2$ and $b^2$ are both functions solely in terms of $x_1$ . For various real number values of $x_1$ , the function on the Cartesian coordinate plane is that of a rotated ellipse. I want to find what the equation of the derivative of that ellipse trends towards as $x_1$ approaches toward infinity. I believe to get that I need the limit as $x_1$ approaches infinity of some sort of function in both $x_1$ and $x$ in which I can substitute $x_1$ and get the equation of the derivative of the  ellipse that is formed by that value of $x_1$ . 
I presume to get that I need to take some type of derivative, however, I do not know what type that might be, or whether it is even possible to get the derivatives of the ellipses simply by substituting a variable.","['calculus', 'derivatives']"
3025971,$\mu$ is $\sigma$-finite $\iff \exists$ function $f \in \mathcal{L}^{1}(\mu)$ with $f(x)>0$ for all $x \in X$,"Let $(X,\mathcal{A}, \mu)$ be a measure space. Prove: $\mu$ is $\sigma$ -finite $\iff \exists$ function $f \in \mathcal{L}^{1}(\mu)$ with $f(x)>0$ for all $x \in X$ My ideas $""\Leftarrow""$ Let $f \in \mathcal{L}^{1}(\mu)$ with $f(x)>0$ , $\forall x \in X$ . So, $f$ is measurable. This implies that for a $B_{n}$ defined as $B_{n}:=\{f>\frac{1}{n}\}$ which is measurable, so $\in \mathcal{A}$ . By definition, $\{f>0\}=\bigcup_{n\in \mathbb N}B_{n}\in \mathcal{A}$ ,
but since $f > 0, \forall x \in X$ then $X\subseteq\bigcup_{n\in \mathbb N}B_{n}$ and $\mu(B_{n})<\infty, \forall n \in \mathbb N$ , since $\int_{X}fd\mu < \infty$$\Rightarrow \mu$ is $\sigma-$ finite. $""\Rightarrow""$ I have no idea how to define this function, particularly as $f>0$ , $\forall x \in X$","['integration', 'measure-theory', 'real-analysis']"
3025981,Show that $e^{tA} = \sum\limits_{k=0}^{n-1} f_k(t)A^k$,"Let $A$ be a $n\times n$ matrix such that the characteristic polynomial of $A$ is $$P(\lambda)=\lambda^n+a_{n-1}\lambda^{n-1}+...+a_1\lambda+a_0$$ Now consider the nth order differential equation $$\frac{d^nx}{dt^n}+a_{n-1}\frac{d^{n-1}x}{dt^{n-1}}+...+a_1\frac{dx}{dt}+a_0x=0$$ and let $f_0(t),\: f_1(t),\: ...,\: f_{n-1}(t)$ be the unique solutions satisfying the inital conditions $$f_j^l(0)= \begin{cases} 1 & j=l \\ 0 & j \neq l\end{cases}$$ for $0 \leq j,\: l\leq n-1$ Show that $e^{tA} = f_0(t)I+f_1(t)A+f_2(t)A^2+...+f_{n-1}(t)A^{n-1}$ I know that $e^{tA}$ is the function $M(t)$ such that $\frac{dM}{dt}=AM$ and $M(0) = I$ . So all I need to do is show that the right hand side of the equation satisfies these two conditions. The second one is easy enough to prove; however, the first one presents some trouble. My plan is to show that $\frac{d^n}{dt^n}f_j(t) = A^nf_j(t)$ for every unique solution $f_j$ , and based on the definitions of $A$ and the solutions to the differential equations, I suspect that Cayley-Hamilton theorem could be used here. However, I haven't been able to engineer it in a way to prove the above statement.","['cayley-hamilton', 'linear-algebra', 'ordinary-differential-equations']"
3026015,Sum of terms with recurrence relation,"I have the following sequence, where $s$ is some positive multiple of 4: \begin{equation}
L_n = \begin{cases}
\frac{(s-2)!}{2^{s/4-1}(s/2)!(s/4-1)!},  & \text{for $n=1$} \\ \\
L_{n-1}\cdot\frac{2(s/4-n+1)}{s-2n+1}, & \text{for $n=2,3,...\frac{s}{4}$}
\end{cases}
\end{equation} I would like to find the value of $\sum_{k=1}^{s/4}L_k$ . So far I have tried factoring, but this can't be done repeatedly in any effective way. I'm mainly stumped by how messy these terms are; simplification just doesn't seem possible. I might be willing to settle for upper and lower bounds, provided that they're better than the trivial ones ( $s/4$ times the smallest and biggest terms). Asymptotic results might be ok as well. Or even just a suggestion of a strategy which might work. This is not a homework problem, so there's no reason to believe that a solution will be simple, unfortunately. Edit: I'm currently trying to obtain an upper and lower bound by making estimates on the coefficient on $L_{n-1}$ . I know that for $n=2$ it's very close to $1/2$ , and then decreases to 0 as $n$ gets larger.","['summation', 'recurrence-relations', 'combinatorics', 'discrete-mathematics', 'upper-lower-bounds']"
3026060,Is every loop in a 3-manifold homotopic to some loop on its boundary?,"Consider a solid region of Euclidean 3-space, or more precisely, a compact, connected 3-dimensional submanifold $U \subset E^3$ bounded by a smooth oriented surface $\Sigma = \partial U$ .  Very roughly speaking, can one find a representative of each homotopy class of loops in $U$ as a loop on the boundary?  More precisely: Question: For every loop $\gamma$ in the fundamental group $\pi_1(b,U)$ (where $b$ is any point of $U$ ), does there exist a loop $\tilde{\gamma}$ homotopic to $\gamma$ that is contained entirely in $\Sigma$ ? By homotopic here, we of course mean that $\gamma$ and $\tilde{\gamma}$ are related by a homotopy in $U$ , not just in $E^3$ .  I.e., there exists some continuous map $\Gamma: [0,1] \times S^1 \to U$ such that $\Gamma(0,s) = \gamma(s)$ and $\Gamma(1,s) = \tilde{\gamma}(s)$ .","['differential-topology', 'homotopy-theory', 'algebraic-topology', 'differential-geometry']"
3026062,Natural numbers as a subset of integer numbers: $\mathbb{N}\subset\mathbb{Z}$.,"Within set theory, having the natural numbers $\mathbb{N}$ built as the minimal inductive set with the corresponding additive and multiplicative operations defined, integers $\mathbb{Z}$ can be set as equivalence classes of parallel diagonals of $\mathbb{N}\times\mathbb{N}$ , which contain a copy of the natural numbers. See Set Theoretic Definition of Numbers . Is there any alternative definition of the set $\mathbb{Z}$ , starting from $\mathbb{N}$ already defined as usual, such that $$\mathbb{N}\subset\mathbb{Z}$$ as sets, preserving the sum and product operations?","['elementary-set-theory', 'number-theory', 'integers', 'natural-numbers']"
3026066,Expected values of squares,"Question A fair coin is tossed three times. Let Y be the random variable that denotes the square of the number of heads. For example, in the outcome HTH, there are two heads and Y = 4. What is E[Y]? My answer: possible outcomes to toss a coin three times : 0, 1, 2, 3
possible outcomes of Y : 0, 1, 4, 9
E[Y] = (1/6 * 0) + (1/6 * 1) + (1/6 * 4) + (1/6 * 9) Is it ok? Thanks!","['discrete-mathematics', 'probability']"
3026083,L'Hospital's Rule application with raised exponents.,I am having a little trouble going about: $$\lim_{x\to \infty} \left(\frac{14x}{14x+10}\right)^{10x}$$ Using $\ln$ properties we can bring down the $10x$ exponent and have: $$\ln y=10x\ln\left(\frac{14x}{14x+10}\right)$$ And from here I get stuck trying to apply L'Hospital's Rule to find the limit.,"['limits', 'calculus', 'derivatives']"
3026086,Infinite Integration using the ceiling function [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question $$\int_0^\infty \frac{\sin(x\pi)}{\lceil x \rceil^2 + \lceil x \rceil} dx$$ My teacher recently gave me this and it's stumped me.",['integration']
3026096,Elliptic function with essential singularity,"The Jacobi theta function, $\theta(u;\tau)$ (in some convention which will be implicit below), has the following elliptic transformation behavior: $$ \theta(u+ m + n \tau;\tau) = (-1)^{m+n} e^{2 \pi i (-n u - \frac{1}{2} n^2 \tau)} \theta(u,\tau) $$ In particular, this means that $\log \theta(u;\tau)$ picks up a linear shift under such a transformation.  Then if we take two derivatives with respect to $u$ , $(\log \theta(u;\tau))''$ , this kills this linear piece and we find an elliptic function with a double pole at $u=0$ , which is precisely the Weierstrass p-function.  On the other hand, if we take a single derivative, $(\log \theta(u;\tau))' = \theta'(u;\tau)/\theta(u;\tau)$ , we get a function  with a single simple pole at $u=0$ , but this can still pick up shifts by integer multiples of $2 \pi i$ under elliptic transformations.  Then it is natural to consider the exponential: $$ f(u) = \exp \bigg(\frac{\theta'(u;\tau)}{\theta(u;\tau)} \bigg) $$ Then $f(u)$ is an elliptic  function with an essential singularity at $u=0$ .  I was wondering if this function (or its log) has any special name or significance in the study of elliptic functions.",['complex-analysis']
3026098,Set of injective operators is a dense residual set in $\mathcal{B}(\mathfrak{X})$,"Let $\mathfrak{X}$ be a Banach space and $\mathcal{B}(\mathfrak{X})$ the set of bounded linear operators mapping $\mathfrak{X}$ to $\mathfrak{X}$ . In [1] below it is shown that the set of invertible operators is not necessarily dense in $\mathcal{B}(\mathfrak{X})$ , although it is well known that it is an open set. In [2], it is shown that the set of injections (I suppose we may call this $\mathcal{I}(\mathfrak{X})$ ) is not necessarily open in $\mathcal{B}(\mathfrak{X})$ . My question is on how ""common"" injectiveness is in $\mathcal{B}(\mathfrak{X})$ . In particular, the results above leave open the possibility that $\mathcal{I}(\mathfrak{X})$ is a dense $G_\delta$ set, and so topologically generic. In either case, it would be interesting to know whether density or the $G_\delta$ condition hold. I believe that the spectral theorem already implies that normal operators on Hilbert spaces may be approximated by injective operators (see for instance [3]). Any results that you guys know of would be appreciated! [1] Showing that $\mathcal{G}(\ell_2)$ is not dense in $\mathcal{B}(\ell_2)$ via the right shift [2] Do bounded linear operators on a Banach space which are injective or have dense range form an open subspace? [3] The set of invertible normal operator is dense in the set of normal operator","['banach-spaces', 'operator-theory', 'functional-analysis']"
3026169,A set that's provably nonempty but with nothing provably in it,"Working in a consistent theory (say, ZFC), is there a set/class that is provably nonempty, but nothing is provably in it? Formally, ( $T\vdash \exists x:x\in A)\land (\forall x,T\nvdash x\in A)$","['elementary-set-theory', 'logic']"
3026214,Find eigenvalues of the matrix,"Find eigenvalues of the $(n+1) \times (n+1)$ -matrix $$ \left( \begin {array}{cccccccc} 0&0&0&0&0&0&-n&0\\ 0
&0&0&0&0&-(n-1)&0&1\\ 0&0&0&0&-(n-2)&0&2&0
\\ 0&0&0&\ldots&0&3&0&0\\ 0&0&-3&0&\ldots&0
&0&0\\ 0&-2&0&n-2&0&0&0&0\\ -1&0&n-1&0
&0&0&0&0\\ 0&n&0&0&0&0&0&0\end {array} \right)
$$ I have tried for some small $n$ by hand and have got that for $n=3$ the eigenvalues are $1,-1,3,-3$ , for $n=4$ eigenvalues are $0,2,-2,4,-4$ . So I am conjectured than for arbitrary $n$ the eigenvalues are $n, n-2, n-4, \ldots, -n+2,-n.$ How to  prove it?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3026266,a.e. point wise convergence on finite measured set implies convergence in measure,"Let $f,f_k$ for $k \in \mathbb{N}$ be measurable finite a.e. on measurable set $E$ . Suppose $f_k \rightarrow f$ point wise a.e., then $f_k$ converges to $f$ in measure on $E$ . This is a basic result and the proof follows from basic definitions I believe, I want to know why we need finite valued and finite measure, is it for the obvious reasons that infinite measure does not converge and finite valued so that the limit as $k$ tends to $\infty$ makes sense? because converging pointwise a.e. means that for $x \in E$ with $E$ having positive finite measure, one has lim $_{k \rightarrow \infty}$ $f_k(x) = f(x)$ so lim $_{k \rightarrow \infty}$ $f_k(x) - f(x)$ = $0$ , and we wish to show for any $\epsilon > 0$ that $m(\{x : \vert f_k(x) - f(x) \vert > \epsilon \}) \rightarrow 0$ as $k \rightarrow \infty$ and since $x$ is taken out of some set of finite positive measure, we obtain (and I am not sure this works) $m(\{x : \vert 0 \vert > \epsilon \}) \rightarrow 0$ as $k \rightarrow \infty$ I get confused here, So sorry for being naive in measure theory, I apologize and thank you in advance! if there is any simple small subtle silly errors I am making, any intuitive assumptions I am incorrectly making please let me know, I am very eager to strengthen my measure theory skills.","['measure-theory', 'lebesgue-measure', 'convergence-divergence', 'real-analysis']"
3026285,Why does $ \operatorname{Var}(X) = E[X^2] - (E[X])^2 $,"$ \operatorname{Var}(X) = E[X^2] - (E[X])^2 $ I have seen and understand (mathematically) the proof for this. What I want to understand is: intuitively, why is this true? What does this formula tell us? From the formula, we see that if we subtract the square of expected value of x from the expected value of $ x^2 $ , we get a measure of dispersion in the data (or in the case of standard deviation, the root of this value gets us a measure of dispersion in the data). So it seems that there is some linkage between the expected value of $ x^2 $ and $ x $ . How do I make sense of this formula? For example, the formula $$ \sigma^2 = \frac 1n \sum_{i = 1}^n (x_i - \bar{x})^2 $$ makes perfect intuitive sense. It simply gives us the average of squares of deviations from the mean. What does the other formula tell us?","['statistics', 'variance', 'probability']"
3026308,"A probability inequality: probability that the normalized sum of i.i.d. random variables is bounded below, is bounded below","I have been told that the following fact is true. Let $X_1,X_2,X_3,\dots$ be i.i.d. random variables. Then there exists $\epsilon$ such that for all $n$ , $$\mathbb{P}\left(\frac{|X_1+\dots+X_n|}{\sqrt{n}}\geq \epsilon\right)\geq \delta.$$ Observe that $\epsilon$ does not depend on $n$ . Both $\epsilon$ and $\delta$ are $>0$ . However, I am struggling to prove this, or find any reference. The kicker is that the $X_i$ 's need not have finite mean or variance. In fact, I am interested in applying this ""fact"" to a situation where the $X_i$ 's must have infinite variance (but possibly zero mean), so elementary things like Markov or Chebyshev's inequality won't help. I am unsure on how to proceed. Any hint would be greatly appreciated! Update on progress: For the $X_i$ that I am interested in, I have deduced the condition $$X_1+X_2+\dots+X_{2^k} \sim2^{k/4}X_i.$$ I also have proved the inequality $$\mathbb{P}(|S_n|>t) \geq \frac{1}{2}\mathbb{P}(\max_j|X_j|>t)\geq\frac{1}{2}(1-e^{-n(1-F(t)+F(-t))}),$$ where $F$ is the c.d.f. of $X_i$ . By $S_n$ , I mean the $S_n=X_1+\dots+X_n$ . Thus it seems like the issue boils down to analyzing the distribution of $X_i$ .","['statistics', 'probability-limit-theorems', 'real-analysis', 'probability-theory', 'probability']"
3026362,Seeking Methods to solve $\int_{0}^{\frac{\pi}{2}} \ln\left|\sec^2(x) + \tan^4(x) \right|\:dx $,"After weeks of going back and forth I've been able to solve the following definite integral: $$I = \int_{0}^{\frac{\pi}{2}} \ln\left|\sec^2(x) + \tan^4(x) \right|\:dx $$ To solve this I employ Feynman's Trick with Glasser's Master Theorom but I'm excited to learn of other methods that can be employed. Are there any other 'tricks' that can be used? or alternatively series based solutions? or transformations? (or anything for that matter). For those who may be interested my process was: (1) First make the substitution: $u = \tan(x)$ $$I = \int_{0}^{\infty} \frac{\ln\left|u^2 + 1 + u^4 \right|}{u^2 + 1}\:du = \int_{0}^{\infty} \frac{\ln\left|1 + u^2\left(u^2 + 1\right) \right|}{u^2 + 1}\:du$$ (2) Now employ Feynman's Trick by introducing a new parameter: $$I(t) = \int_{0}^{\infty} \frac{\ln\left|1 + t^2u^2\left(u^2 + 1\right) \right|}{u^2 + 1}\:du$$ Note here that $I = I(1)$ and $I(0) = 0$ (3) Take the derivative w.r.t 't' $$I'(t) = \int_{0}^{\infty} \frac{2tu^2\left(u^2 + 1\right)}{1 + t^2u^2\left(u^2 + 1\right)}\frac{1}{u^2 + 1}\:du = \frac{1}{t} \int_{-\infty}^{\infty} \frac{1}{\left(u - \frac{1}{tu}\right)^2 + \frac{2}{
t} + 1}\:du$$ (4) Employ Glasser's Master Theorem: $$I'(t) =  \frac{1}{t} \int_{-\infty}^{\infty} \frac{1}{\left(u - \frac{1}{tu}\right)^2 + \frac{2}{t} + 1} \:du= \frac{1}{t}\int_{-\infty}^{\infty}\frac{1}{u^2 + \frac{2}{t} + 1} \:du$$ As: $\frac{2}{t} + 1 > 0 $ we arrive at $$I'(t) = \frac{1}{t}\left[\frac{1}{\sqrt{\frac{2}{t} + 1}}\arctan\left(\frac{u}{\frac{2}{t} + 1}\right)\right]_{-\infty}^{\infty}= \frac{\pi}{\sqrt{t\left(t + 2\right)}}$$ (5) We now integrate w.r.t 't' $$I(t) = \int \frac{\pi}{\sqrt{t\left(t + 2\right)}}\:dt = 2\pi\sinh^{-1}\left(\frac{t}{\sqrt{2}} \right) + C$$ Where $C$ is the constant of integration. As above $I(0) = 0 \rightarrow C = 0$ and so, our final solution is given by: $$I = I(1) = 2\pi\sinh^{-1}\left(\frac{1}{\sqrt{2}} \right)$$","['integration', 'definite-integrals', 'integral-transforms']"
3026446,"Learning $\arcsin, \arccos, \arctan$ - how to?","Sorry for asking such question. I have a very basic understanding of $\arcsin, \arccos, \arctan$ functions. I do know how their graph looks like and not much more beyond that. Calculate: Which specific keywords should I google to learn how to solve the following tasks? I think those aren't equations (googling 'cyclometric equations' was a dead end). Perhaps you would like to share with some link to a beginner-friendly learning source? Thank you.","['trigonometry', 'inverse-function', 'analysis']"
3026447,Synthetic solution to this geometry problem?,"Consider the following diagram. In the isosceles triangle $\triangle ABC$ with $AB=AC$ , it is given that $BC=2$ . Two points $M,N$ lie on $AB,AC$ respectively so that $AM=NC$ . Prove: $MN$ is at least $1$ . (Source: 1990 High School Olympiad held in Xi'an, China) I've already solved this problem by doing some coordinate geometry, setting $AM=NC=t$ , finding $MN$ as a function of $t$ , then minimising that function. But this is quite tedious, which led me to wonder what the synthetic geometry solution, which I couldn't find, is. (Btw, ""synthetic"" means without the use of coordinate geometry, and hopefully with as little algebra as possible as well.)","['contest-math', 'euclidean-geometry', 'geometry']"
3026478,Find the integral of the vector field,"Let the vector field $$\vec{F}\left(\vec{x}\right)=\begin{pmatrix}{x_1^2+2x_3}\\ x_1x_2\\
 x_3^2-2x_1\end{pmatrix}$$ Compute the integral $\int _C\vec{F}\left(\vec{x}\right)d\vec{x}$ from the origin to the point $P(1/2/3)$ if $C$ is a straight line from the origin to $P$ . So in the book they only give us answers, but not how to get the answers. I calculated the integral and got $$\int _0^112t^2-8t$$ which equals to zero, however in the book the answers is $9\frac{2}{3}$ . I think the book is wrong because I just don't see how we can get that answer. Or am I wrong?","['integration', 'definite-integrals', 'vectors', 'multivariable-calculus', 'calculus']"
3026509,What is the asymptotic order of $\sum_{k=0}^n {n\choose k}^2$?,"What is the asymptotic order of $\sum_{k=0}^n {n\choose k}^2$ ? That is, find $g(n)$ such that $$\lim_{n\to \infty}\frac{\sum_{k=0}^n {n\choose k}^2}{g(n)}=1$$ We can expand the binomial coefficient and use Stirling's approximation but I can not determine g(n).","['asymptotics', 'probability']"
3026519,Tetration of non-integers: is there something wrong with this approach?,"I'm trying to figure out a formula for tetration that will work for
non-integer heights. I know the usual recurrence relation for tetration
( $x \in \mathbb{R}, \text{ }n \in \mathbb{N})$ : $${^{n}x} = \begin{cases} 1 &\text{if }n=0 \\ \\ x^{\left(^{(n-1)}x\right)} &\text{if }n>0 \end{cases}$$ I also know that $x^y=e^{y \ln x}$ for positive $x$ . I combined these two and formed this recurrence: $$
{^y}x = 
f(x,y) = 
\begin{cases}
e^{y \ln x}                & \text{if }0 \lt y \le 1 \\
\\
e^{f(x,\text{ }y-1) \ln x} & \text{if }1 \lt y 
\end{cases}
$$ Playing around with this in Maxima, I got correct answers for integer $y$ , and reasonable-looking answers for non-integers.  Yet I have read
numerous sources stating that a general formula for tetration is very difficult. So, my question: have I a correct solution for a limited domain, or am I
off in the weeds and it just happens to work for integers? Thank you.","['exponentiation', 'functions', 'tetration']"
3026603,"Prove if the product of $k$ matrices $A_1$ ... $A_k$ is nonsingular, then each matrix $A_i$ is nonsingular.","I'm having trouble proving this without using determinants. I know how to prove it with the product of just two matrices, but I'm not sure how to generalize this to a product of k matrices. Is there a way to do this proof without determinants? To clarify the question, each matrix is an $n$ by $n$ matrix. For example, I know that if the determinant of the product is nonzero, then all of the determinants of the individual matrices must also be nonzero. I also know that if a product of two matrices $A$ and $B$ , $AB$ is nonsingular, then there exists a matrix $C$ so that $C(AB) = I$ and $(AB)C = I$ , and so $(CA)B = I$ and $A(BC) = I$ , so $A$ and $B$ are both invertible, and thus nonsingular. I'm looking for a way to generalize this. Or just any other way to prove this without determinants. Thanks so much!","['matrices', 'linear-algebra']"
3026641,Find the Floquet multipliersfor the Markus & Yamabe system,"I need some help with the following excersice.Find the minimum period and the Floquet multipliers $\bf\lambda_{1},\lambda_{2}$ of the following matrix. $A(t)=\begin{bmatrix}-1+\frac{3}{2}cos^2(t) & 1-\frac{3}{2}cos(t)sin(t)\\-1-\frac{3}{2}cos(t)sin(t) & -1+\frac{3}{2}sin^2(t)\end{bmatrix}$ It is obvious that the minimum period of A is $\pi$ ,just by using the double-angle formulas $\cos^2(t)=\frac{1+cos(2t)}{2},sin^2(t)=\frac{1-cos(2t)}{2}$ .Furthermore by using the relation $$\prod_{i=1}^2\lambda_{i}=e^{\intop_0^T trace(A(s))ds}$$ we get easily that $\prod_{i=1}^2\lambda_{i}=e^{-\frac{\pi}{2}}$ .But now i am stuck!","['ordinary-differential-equations', 'dynamical-systems']"
3026671,Can roots of a polynomial stay on one side of the complex plane as the coefficients vary?,"Suppose a fixed $n^{\text{th}}$ degree monic polynomial is given $$ p(x) = x^{n} + a_{n-1} x^{n-1} + \dots + a_0,$$ with coefficients vector $a = (a_{n-1}, \dots , a_0) \in \mathbb R^n$ . Now we consider a parameterized family \begin{align*}
p(x, r) = x^{n} + ra_{n-1} x^{n-1} + \dots + ra_0,
\end{align*} with $r \in \mathbb R$ . Suppose for some $r_0 \in \mathbb R$ , we have $p(i \alpha, r_0) = 0$ , i.e., $p(x, r_0)$ has a zero on the imaginary axis. Let us further assume for some $\delta > 0$ , we have $p(x, r)$ has all its zeros on the right half plane of $\mathbb C$ for each $r \in (r_0, r_0 + \delta)$ . My question is: is it possible that for all $r \in (r_0 - \delta, r_0)$ we also have $p(x, r)$ has all its zeros on the right half plane (we may include the imaginary axis). That is, is there a monic polynomial, that allows us to parametrize as above, such that the parametrized family has zeros touching the imaginary axis but all the zeros are confined in the right half plane when we vary $r$ continuously? As commented by @saulspatz, $x^2 + r$ will have all its zeros on the imaginary axis if $r < 0$ . I was in mind asking the case that at least there exists some $r \in (r_0 - \delta, r_0)$ such that some of the roots of $p(x, r)$ will move off the imaginary axis.","['algebraic-geometry', 'abstract-algebra', 'linear-algebra', 'polynomials']"
3026672,What is the difference between ergodicity and the law of large numbers?,"I want to begin by saying that I know absolutely no measure theory. To my knowledge, roughly speaking a stochastic process is ergodic if its time average converges to the expectation (space average) over a long period of time. For an iid sequence $X_{1} \ldots X_{n}$ with finite mean, I noticed that if consider the index as time then the average of this sequence $\frac{1}{n} \sum_{i=1}^{n} X_{i}$ is the very definition of time average. The law of large numbers states this will converge to $E[X_{i}]$ as $n \rightarrow \infty$ . So, I am not sure how the law of large numbers is different from ergodicity? Looks to me they are saying the same thing. Can a stochastic process be ergodic if it isn't iid? I am also not sure how the definition of ergodicity coincides with the definition given in the context of markov chains, where the chain is ergodic if it is aperiodic, irreducible, and finite mean recurrence time.","['ergodic-theory', 'markov-chains', 'stochastic-processes', 'law-of-large-numbers', 'probability']"
3026676,Prove that the function $f(z) = \sqrt{|Re(z) Im(z)|}$ satisfies the Cauchy-Riemann equations,"I'm asked the following question: Prove that the function $f: \mathbb{C} \to \mathbb{C}$ defined by $$f(z) = \sqrt{|Re(z) Im(z)|}$$ satisfies the Cauchy-Riemann equation,
  but not differentiable there. However, the function is basically $f(x+iy) = \sqrt{|xy|},$ i.e $f(z) = u + iv$ , $u = \sqrt{|xy|}$ and $v = 0$ . However, for example, the partial derivatives of $f$ wrt $x$ is $\frac{\pm \sqrt{y} }{ 2|x|} $ , and this function does not have a limit at the origin, so $\frac{\partial u}{\partial x } $ does not exist at the origin, hence how can it satisfy the Cauchy-Riemann equation ? However","['complex-analysis', 'real-analysis']"
3026700,Bounded Harmonic Functions on the Disk,"Denote by $\mathbb{D}$ the open unit disk in $\mathbb{R}^2$ . 
Is it possible to find a bounded harmonic function $u : \mathbb{D} \to \mathbb{R}$ that is not uniformly continuous? I tried using functions that oscillate near $\partial \mathbb{D}$ but was unable to get anything substantial.","['uniform-continuity', 'harmonic-functions', 'real-analysis', 'complex-analysis', 'partial-differential-equations']"
3026712,Divergence Theorem - Cone,"Here's the question: 
Evaluate the surface integral $\iint _S F\cdot n \space dA$ by the divergence theorem. $ \mathit F = [xy, yz, zx]$ , S the surface of the cone $x^2 + y^2 \le 4z^2, \space \space 0 \le z \le 2 $ This is my working for this question. $$\nabla F = y + z + x $$ Parametric equation of S: $$ (r,v,u) = (r\cos(v), r\sin(v), u) $$ The limits for each variable: $$ 0\le v \le 2\pi \quad 0 \le u  \le 2 \quad 0\le r \le 2u $$ Jacobian is $r$ The new divergence is $$\nabla F = r\cos (v) + r \sin (v) + u $$ Hence evaluating the triple integral yields $16\pi$ Now, my working out for the triple integral is correct but I am not sure about my parametric equation and limits. I do not know the correct answer to this question since my textbook only provides answers to odd-number problems. However, I came across this website http://www.slader.com/textbook/9780471488859-advanced-engineering-mathematics-9th-edition/463/problems/18/# and their answer is different to mine. Did I do something wrong?","['integration', 'divergence-operator', 'conic-sections', 'multivariable-calculus', 'parametric']"
3026718,Determining the domain of a polynomial with rational exponents,"I've completely confused myself as to determining the domain of polynomial expressions that have rational powers, e.g. $$y = x^{2/3} \qquad \text{or} \qquad y_2 = (x^2-1)^{2/ 3}.$$ A calculus textbook I've consulted asserts that the domain of $y_2$ is all real values of $x$ ; however, when I plot the function using Grapher or Wolframalpha, it excludes the values for $-1 \leq x \leq 1$ : Similarly, when I try to use Wolfram to evaluate $(-4)^{2/ 3}$ it returns a complex number, whereas I would've expected a real value (because I thought we could think of this as $(-4)^{2/ 3} = \big((-4)^2\big)^{1/ 3} = (16)^{1/ 3} = \sqrt[3]{16}$ , which I thought was the same as $\big( (-4)^{1/ 3} \big)^{2} = (-\sqrt[3]{4})^{2} = \sqrt[3]{4}^2$ I consulted this question , which clarified that the property $a^{bc} = (a^b)^c$ only applies to all $a$ , if $b$ $c$ are integers (otherwise, we must assume $a > 0$ ), but I'm still unclear as to how I can determine the domain of these functions, given the disagreement between my textbook and what I'm finding with graphing apps. Any clarification would be greatly appreciated! ps. I could not determine which of the ""domain"" tags was appropriate, I did not see anything like ""domain of a function,"" and most say not to use for this purpose. Edit I just consulted a 3rd textbook that includes the following statement: For all real numbers $a$ for which the indicated roots exist, and for any rational number $m/n$ , $$a^{m/n} = (a^{1/n})^m$$ . This prompted me to try graphing the function as $y_2 = \big((x^2-1)^{1 /3}\big)^2$ ; however, this yielded the same result as before (ie excluding -1 < x < 1). On Wolfram, I tried y = (cube-root(x^2-1))^2 and this yielded the same graph as the one provided by my textbook. I must admit, I'm a bit at a loss as to why $y_2 = \big((x^2-1)^{1 /3}\big)^2$ didn't resolve the issue. If that had worked, it would've made (some) sense to me.","['functions', 'graphing-functions']"
3026757,Show that each permutation in $A_4$ has a square root.,"Show that $ A_4 =  \{ \sigma \in S_4 \mid \sigma = \tau^2 \text{ for some }
        \tau \in S_4 \} $ . $S_4$ is the permutations of 1,2,3,4. $A_4$ is the alternating group of $S_4$ . Let $ B = \{ \sigma \in S_4 \mid \sigma = \tau^2 \text{ for some }
        \tau \in S_4 \}$ . It is easy to see that $B\subseteq A_4$ . I am having difficulty showing that the cycles of length 3 (e.g. (1,2,3))
have a square root $\tau$ . Below is my work for the other cycles. Now we note that $A_4 = \{ (), (1,2)(3,4), (1,3)(2,4), (1,4)(2,3), (1,2,3), (1,3,2), (1,3,4),
 (1,4,3), \\(1,2,4), (1,4,2), (2,3,4), (2,4,3) \}$ .
 Thus $ A_4 $ has three different types of permutations $ e, $ those of length 3,
 and products of disjoint transpositions.
 First we note that $ () = e \in B$ since it can be expressed as $ (1,2)(1,2) $ .\
 Next, for all the permutations consisting of a product of disjoint cycles $ (a_1,a_2)(a_3,a_4) $ we can express them as $ (a_1,a_3,a_2,a_4)^2 $ . It remains to be shown that the length-3 cycles have square roots in $S_4$ .","['permutations', 'group-theory', 'abstract-algebra']"
3026850,Proof that the operator $\Theta\vec{\omega}=-\mu\Delta\vec{\omega}-(\lambda+\mu)\nabla(\nabla\cdot\vec{\omega})$ is positive and self-adjoint,"I have the following partial differential equation (eigenvalue equation) \begin{equation}
\Theta \vec{\omega}=\alpha\vec{\omega}
\end{equation} where I have defined the linear partial differential operator $\Theta$ as $$\Theta\vec{\omega}=-\mu\Delta\vec{\omega}-(\lambda+\mu)\nabla(\nabla\cdot\vec{\omega}).$$ I would like to show that this operator is positive self-adjoint with respect to the $L^2$ scalar product $(u,v)=\int_B u^*\cdot v~ d^3 r$ . In a paper which I found ( link , doi ) they said it is obvious, however, I would like to prove this explicitly but I struggle a lot. Can anyone give me a hint or some intuition how to prove that? In my case $B$ denotes the body which is a sphere with radius $R$ and $\lambda,\mu$ are the Lamé parameters. This equation one encounters often in linear and isotropic elasticity. If I could show this, than an immediate consequence would be that the eigenvalues are real and positive as well as the eigenfunctions to different eigenvalues are orthogonal, that is clear an easy to prove if the operator is self-adjoint. Is it also possible to prove that the eigenfunction form a complete set of a basis, so that I can write every vector field defined on $B$ as a linear combination of the eigenfunctions?","['ordinary-differential-equations', 'mathematical-physics']"
3026877,Differentiation for a function in the integral form.,"I want to know generally how we differentiate a function $F(x)$ in the following form, $$F(x)=\int_a^x f(x,t)dt$$ For example, if we can work out the explicit form of $F(x)$ as the example below $$F(x)=\int_0^x e^{xt}dt=\frac{e^{x^2}-1}{x}$$ if we differentiate $F(x)$ , we get $$\frac {dF(x)}{dx}=\frac{2x^2e^{x^2}-e^{x^2}+1}{x^2}$$ This simple example shows a very different approach from the case when we different $F(x)=\int_a^xf(t)dt$ , which simply gives us $f(x)$ . Now suppose I have the formula for calculating the money I get after $T$ years, $P(T)$ with continuous depositing, of which the rate is given by $S(t)$ , and continuous compounding, of which the annual rate is given by $r$ . The formula is $$P(T)=\int_0^TS(t)e^{r(T-t)}dt$$ Since the $S(t)$ is not given explicitly, then how do I get an expression for $\frac{dP(T)}{dT}$ ? I have tried using the first principle by $$\frac{dP(T)}{dT}=\lim_{\Delta T\to0} \frac{P(T+\Delta T)-P(T)}{\Delta T}$$ and I could not figure it out. Is there a way to find an expression for the derivative $\frac{dP(T)}{dT}$ ?","['calculus', 'implicit-differentiation', 'derivatives']"
3026929,Limit $ \lim_{n\to\infty} \left(\frac{3n^2-n+1}{2n^2+n+1}\right)^{\large \frac{n^2}{1-n}}$,"I'm required to compute the limit of the following sequence: $$ \lim_{n\to\infty} \left(\frac{3n^2-n+1}{2n^2+n+1}\right)^{\large \frac{n^2}{1-n}}$$ I'm not sure as to how to approach it. I've tried tackling it in a few ways, but none of them led me to a form I know how to tackle. Let $a_n$ denote the base. $$ \lim_{n\to\infty}(a_n)^{\frac{n^2}{1-n}}= \left(\lim_{n\to\infty}(a_n)^{\large \frac{1}{1-n}}\right)^{n^2}=\lim_{n\to\infty} \left(\sqrt[1-n]{a_n}\right)^{n^2}=1^{\infty} $$ From there though I don't know how to compute: $$e^{\lim_{n\to\infty} \left(\sqrt[n-1]{a_n}n^2\right)}$$ I tried using the ln function though I couldn't get any further. I'd be glad for help :)","['limits', 'calculus', 'real-analysis']"
3026957,Approximation of arbitrary convex function,"I am currently studying Jürgen Moser's ""A New Proof of de Giorgi's Theorem Concerning the Regularity Problem for Elliptic Differential Equations"". During the proof of the first lemma, he claims that an arbitrary convex function $f$ can be approximated by a sequence of twice continously differentiable convex functions $f_m$ , such that $f_m''(u)=0$ for large $|u|$ , $f_m \rightarrow f$ and $f_m'(u) \rightarrow f'(u)$ , where $f'(u)$ exists. Having looked in several books about convex analysis (Rockafellar), I found out that a convex function should be twice differentiable almost everywhere. I further found in one of Rockafellar's books (convex analysis, thm. 25.7) a Theorem that shows the claim, except for the restriction on $f''$ . 
Now the question is how can we come up with that restriction on $f''$ and maybe someone has a reference for a book as well, since I would be interested in seeing the proof.","['convex-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
3027067,if such three condition find the $\sin{\angle OPA}$,"Convex quadrilateral $ABCD$ ,and circumcenter is $O$ ,if  Point $P$ lie on sides $AD$ ,and such $$\dfrac{AP}{PD}=\dfrac{8}{5},~~PA+PB=3AB,~~PB+PC=2BC,~~PC+PD=\dfrac{3}{2}CD$$ find $\sin{\angle OPA}$ I try let $AB=a,BC=b,CD=c,DA=d,,AP=x,PD=y,x+y=d$ ,and $OA=1$ ,then I get very ugly
use this wiki $$1=R=\dfrac{1}{4}\sqrt{\dfrac{(ab+cd)(ac+bd)(ad+bc)}{(s-a)(s-b)(s-c)(s-d)}}$$ where $s$ is semiperimeter. I want use this identity $$\cos{(\angle APB+\angle CPB+\angle CPD)}=0$$ or $\angle APB=\angle 1,\angle CPB=\angle 2,\angle CPD=\angle 3$ $$\cos{\angle 1}\cos{\angle 2}\cos{\angle 3}=\sin{\angle 1}\sin{\angle 2}\cos{\angle 3}+\sin{\angle 1}\sin{\angle 3}\cos{\angle 2}+\sin{\angle 2}\sin{\angle 3}\cos{\angle 1}$$","['contest-math', 'geometry']"
3027091,Time complexity of Catalan number,"The below solution is taken from Stack Overflow which has a very large number of
up votes and it was accepted also, but I have a very very small doubt in this. The solution is the exact copy I didn't change anything. Since the question is very old, no one seems to reply there. The following function produces the nth number in Catalan numbers. Actually I have doubt regarding the time complexity of this problem. int catalan(int n)
{
    if (n==0 || n==1)
        return 1;
    
    int sum = 0;
    for(int i=1;i<n;i++)
        sum += catalan(i)*catalan(n-i);
    return sum;
} Here is the solution: My doubt: Why is it $c(n+1)-c(n)=2+2c(n)$ ? It should be $c(n+1)-c(n)=2c(n)$ because all terms till $c(n-1)$ will be cancelled in line number 6. To evaluate the complexity, let us focus on the number of recursive calls performed, let $C(n)$ . A call for $n$ implies exactly $2(n-1)$ recursive calls, each of them adding their own costs, $2(C(1)+C(2)+...C(n-1))$ . A call for $n+1$ implies exactly $2n$ recursive calls, each of them adding their own costs, $2(C(1)+C(2)+...C(n-1)+C(n))$ . By difference, $C(n+1)-C(n) = 2+2C(n)$ , which can be written $C(n) = 2+3C(n-1)$ . C(1) = 0
C(2) = 2+2C(1) = 2+3C(0) = 2
C(3) = 4+2(C(1)+C(2)) = 2+3C(2) = 8
C(3) = 6+2(C(1)+C(2)+C(3)) = 2+3C(3) = 26
C(4) = 8+2(C(1)+C(2)+C(3)+C(4)) = 2+3C(4) = 80
...
C(n) = 2n-2+2(C(1)+C(2)+...C(n-1)) = 2+3C(n-1) To solve this recurrence easily, notice that C(n)+1 = 3(C(n-1)+1) = 9(C(n-2)+1) = ...3^(n-2)(C(2)+1) = 3^(n-1) Hence, for $n>1$ the exact formula is C(n) = 3^(n-1)-1 The number of calls to Catalan(1) (constant time), is also $C(n)$ , and the numbers of adds or multiplies are $\frac{C(n)}{2}$ each. It is easy to reduce the complexity from $O(3^n)$ to $O(2^n)$ by noting that all terms in the loop (except the middle one) are computed twice - but that still doesn't make it an acceptable implementation :)","['catalan-numbers', 'discrete-mathematics']"
3027093,Global existence of IVP on $\mathbb R$,"Suppose $f(t, x)\in C^1(\mathbb R^2)$ and satisfies $|f(t, x) |\leq 1+|x|$ for any $(t, x) \in\mathbb R^2$ . Then prove that the IVP $x'(t) =f(t, x)$ and $x(0)=0$ has a solution defined on whole $\mathbb R$ . I tried to find local solutions by restricting $f$ to rectangles. But it seems like it couldn't be guaranteed that the solution even exists on the whole interval. Another idea is to try to prove it by approximating $f$ uniformly by polynomials. I'm sure how to make it work. I'd like to get hints on how to solve this. Thanks in advance.","['metric-spaces', 'ordinary-differential-equations']"
3027111,"Prove that the spectral radius $\rho(A)$ is a continuous function, where $A$ is a square matrix.","Let $||. ||$ be some norm on $\mathbb{R^n}.$ Interpret the real-valued matrices as a Euclidean space, $\mathbb{R^{n^2}}=Mat_{n\times n},$ and prove that the following are continuous functions of the $n^2$ matrix entries $A_{ij}$ (a) The spectral radius of $A$ , $\rho(A)=\max\{|\lambda| ~~| \text{such that}~\lambda 
~\text{is an eigenvalue of}~A  \}.$ (b) The spectral radius of the inverse, $f(A)=\rho (A^{-1}),$ on the domain of invertible matrices. For part (b), I think the goal is $\displaystyle\lim_{\Delta A \to 0} \rho(A+\Delta A)=\rho(A).$ I tried to use Gelfand formula. Then I have $\rho(A)=\displaystyle\lim_{\Delta A \to 0} \rho(A+\Delta A)=\displaystyle\lim_{\Delta A \to 0} \lim_{k\to \infty}||(A+\Delta A)^k||^{1/k},$ which becomes more complicated. In addition, I tried to use the operator norm to bound the spectral radius, but I felt that I was still far away from the proof. Why does the part (b) need to prove if the part (a) is true? $f(A)=\rho(A) $ restrict on the domain of invertable matrices. I was confuled about part(b). I know how to prove this statement using a theorem in complex analysis: The roots of a complex-valued polynomial are continuous wrt the coefficients of the polynomial. By using this theorem, (b) is very easy to prove. I am looking for another proof of (b) without the theorem in complex analysis.","['matrices', 'linear-algebra']"
3027141,Find the coefficient of $x^{70}$ in $(x^1-1)(x^2-2)(x^3-3)\cdots(x^{12}-12)$.,Find the coefficient of $x^{70}$ in $(x^1-1)(x^2-2)(x^3-3)\cdots (x^{12}-12)$ . I tried to solve this problem using theory of equation the coefficient of $x^{70}$ will be the  sum of products taking two at a time. But this very very exhaustive I want to know some another method as it will be proficient in higher powers.,"['algebra-precalculus', 'polynomials']"
3027162,Taylor series with a base point different from $0$,What's the need for $f(x) = \sum_{k=0}^{\infty}\frac{f^{(k)}(a)}{k!}(x-a)^{k}$ if we already have the formula at $0$ ? Isn't the $(x-a)$ just making the $a$ as the new origin? When is this formula more useful than that at $0$ ?,"['functions', 'taylor-expansion']"
3027194,A counterexample to the epsilon-delta criterion for Absolute Continuity of Measures,"Let $p>0$ , and let $\mu$ be a Borel measure on $[0,\infty)$ defined by $\mu(E)=\int_Ex^pd\lambda$ where $\lambda$ denotes Lebesgue measure.  Show that $\mu$ is absolutely continuous with respect to $\lambda$ , but $\mu$ does not meet the epsilon-delta criterion for absolute continuity, namely for every $\epsilon>0$ there's a $\delta>0$ such that $\mu(E)<\epsilon$ whenever $\lambda(E)<\delta$ . I've managed to prove that $\mu$ is absolutely continuous with respect to $\lambda$ , but I'm not sure how to approach the second part.  I basically need to find a sequence of sets $E_n$ (in $B([1,\infty)$ ) such that $\lambda(E_n)\rightarrow 0$ but $\int_{E_n}x^pd\lambda$ does not go to zero.  But I can't even think of such a sequence in the case where $p=1$ , let alone the general case.","['measure-theory', 'borel-measures', 'lebesgue-integral', 'absolute-continuity', 'radon-nikodym']"
3027233,"If $f:\mathbb{R}\to\mathbb{R}$ is infinitely-differentiable, and $f(x+y)-f(y-x)=2xf^\prime(y)$, then it is a polynomial of degree less than $2$","$S$ is set of family of infinite differentiable function from $\mathbb R \to \mathbb R$ with $\forall x,y\in R$ $$f(x+y)-f(y-x)=2xf^\prime(y)$$ then I have to prove that $S$ only contain all polynomials of degree less than $2$ . My attempt: I can show that all polynomial of degree less than 2 satisfies that property From given equation, I think it uses Mean value theorem, but I am not able to show that is only function. Please only provide me hint. I wanted to solve this problem.","['functional-equations', 'smooth-functions', 'real-analysis', 'functions', 'derivatives']"
3027272,"consider the initial value problem $ y'=y^{1/3}, y(0)=0$","consider the initial value problem $y'=y^{\frac{1}{3}} , y(0)=0 $ This is already asked Here I have a doubt in those answer, as that question is 6 years old, i ask this seperately. Its is written in comment that the function is ill-defined in negative axis. What does it means? Please explain this intuitively Also in that accepted answer how answerer got that idea of that function. It's not trivial",['ordinary-differential-equations']
