question_id,title,body,tags
1400409,Small question concerning the proof of the unbiased estimate of the population variance,"The main purpose of this question is to check my understanding. As in I have an answer that I think is correct, but I'm not sure, since Stats is not my forte. So given a random variable $X$, we take some samples from it and denote by $X_i$ the random variable ""the $i$th observation from each sample"" and denote by $\overline{X}$ the random variable the sample mean. We're trying to show that:
$$E\left[\frac{1}{n-1}\sum(X_i-\overline{X})^2\right] = \sigma^2 \tag 1$$
where $\operatorname{Var}(X)=\sigma^2$. Let $Y_i = X_i-\overline{X}$ and since $E[X_i]=E[\overline{X}]$ we have that $E(Y_i)=0$. This means that the LHS of (1) becomes: $$
\begin{align}
E\left[\frac{1}{n-1}\sum(Y_i^2)\right] & = \frac{1}{n-1}\sum E[Y_i^2] \\[8pt]
& = \frac{1}{n-1}\sum (\operatorname{Var}(Y_i)-(E[Y_i])^2) \\[8pt]
& = \frac{1}{n-1}\sum \operatorname{Var}(Y_i) \\[8pt]
& =\frac{1}{n-1}\sum \operatorname{Var}(X_i-\overline{X})
\end{align}
$$ Here is where I made the mistake of saying that $$\operatorname{Var}(X_i-\overline{X}) = \operatorname{Var}(X_i)+(-1)^2\operatorname{Var}(\overline{X}) = \sigma^2+\frac{\sigma^2}{n} = \sigma^2\frac{n+1}{n}$$ However, the proof I was following gave a longer calculation ending with $Var(Y_i)=\sigma^2\frac{n-1}{n}$ which then lead immediately to the desired result.
So I wondered where I had gone wrong. My educated guess is that it is not true that $\operatorname{Var}(X_i-\overline{X})=\operatorname{Var}(X_i)+(-1)^2\operatorname{Var}(\overline{X})$ because $\operatorname{Var}(X+Y) = \operatorname{Var}(x)+\operatorname{Var}(Y)$ only if the random variables are independent and $X_i$ and $\overline{X}$ are not. Have I explained my error correctly or is there another mistake in my calculations? Thank you,
Diana",['statistics']
1400412,Prove that: $ (a<b<c) \implies (a<\frac{a+b+c}{3}<c) $,"Prove that: $$ (a<b<c) \implies (a<\frac{a+b+c}{3}<c) $$
I'm having problem proving these implications (I don't know how they're called in English). Can you tell me what I have to read to understand these?",['algebra-precalculus']
1400439,Find $\int_0^\frac{\pi}{2} \frac {\theta \cos \theta } { \sin \theta + \sin ^ 3 \theta }\:d\theta$,"My Calc 2 teacher wasn't able to solve this:
$$\int_0^\frac{\pi}{2} \frac {\theta \cos \theta  } { \sin \theta + \sin ^ 3 \theta }\:d\theta$$ Can someone help me solve this?","['trigonometry', 'calculus', 'integration']"
1400454,Group homomorphisms $\mathbb{Q}\to\mathbb{R}$,"Find all the group homomorphisms from $(\mathbb{Q}, +)$ into $(\mathbb{R}, +)$. My attempt: If $\mathbb{Q}$ were a cyclic group, I could tell that any homomorphism will be determined by the image of generator. But here $\mathbb{Q}$ is not a cyclic group, so there's no generator.
All one can say is that: if $f$ is a homomorphism then $f(0)=0$. But this doesn't help me to solve this problem. So how should it be tackled?","['abstract-algebra', 'group-theory']"
1400464,Vector bundles in Ravi Vakil's notes on quasicoherent sheaves,"In chapter 13 (Quasicoherent and coherent sheaves) of Ravi Vakil's wonderful notes , the author starts by discussing vector bundles, supposedly for motivation. Having understood that each locally free sheaf gives rise to a vector bundle and vice versa, I don't understand how vector bundles ""help"" tell the story at all. The definition of a locally free sheaf seems to me far more natural, while vector bundles feel (to me) opaque and forced. In what sense do vector bundles motivate locally free sheaves? Is this
  simply because most people encounter vector bundles in their
  mathematical life earlier than sheaves? As I mentioned in this question , quasi-coherent sheaves are an enlargement of locally-free sheaves into an abelian category. Hence there is a homological algebra of quasi-coherent sheaves and in particular of locally-free ones. So perhaps there are some categories of vector bundles which tempt one to do homological algebra, thereby motivating the sheaf POV.. What are some simple examples of categories of vector bundles that tempt you
  to do homological algebra?","['sheaf-cohomology', 'vector-bundles', 'algebraic-geometry', 'quasicoherent-sheaves', 'sheaf-theory']"
1400481,Why did Serre choose coherent sheaves?,"First thing - I don't know any algebraic geometry. I'm trying to understand a little bit about quasi-coherent sheaves but not for the sake of AG, so please rely on as little knowledge as possible. What follows is an excerpt from Dieudonné's History of Algebraic Geometry , VIII.2.21: ""Serre's principal goal is to extend, as much as possible, to his varieties the results on sheaf cohomology described above for the classical case ($k=\mathbb C$). He restricts to coherent $\mathcal O_X$-modules (in order to be able to use the exact cohomology sequence with the definition of the cohomology groups (""Čech cohomology"") of which he avails himself)..."" The exact cohomology sequence mentioned is the one induced by a short exact sequence of abelian sheaves
$$0\longrightarrow \mathcal N\longrightarrow \mathcal G\longrightarrow \mathcal G/\mathcal N\longrightarrow 0$$ From this I understand the original purpose of coherent sheaves is simply the fact they form an abelian category. However, I don't understand why this justifies restricting to coherent sheaves nor why they were specifically chosen from all the abelian subcategories of $\mathcal O_X$-modules: Couldn't Serre do his homological algebra equally well in $\mathcal
 O_X$-$\mathsf{Mod}$? What's the benefit of restricting to
  $\mathsf{Coh}(X)$?
  Why exactly did coherent sheaves come into play?","['sheaf-cohomology', 'algebraic-geometry', 'quasicoherent-sheaves', 'sheaf-theory', 'coherent-sheaves']"
1400485,Does the sum of reciprocals of primes congruent to $1 \mod{4}$ diverge?,"Let $P$ be the set of primes $p$ greater than $3$ such that $p\equiv1 \pmod{4}$. Does the following sum converge or diverge? $$
\sum_{p\in P}\frac{1}{p}
$$","['prime-numbers', 'sequences-and-series', 'number-theory', 'analytic-number-theory']"
1400489,Differentiable but not Holomorphic?,Can I get a few examples of complex functions being complex differentiable at a point but not holomorphic in their domain?,['complex-analysis']
1400491,Showing $\limsup \frac{|S_n|}{n}=\infty$,"$X_n$'s are i.i.d symmetric with $E|X_1|=\infty$. Then $\limsup \frac{|S_n|}{n}=\infty$. How do I show $\limsup \frac{S_n}{n}=\infty$ and $\liminf \frac{S_n}{n}=-\infty$? My attempt : Let $c=\limsup \frac{S_n}{n}=\limsup \frac{-S_n}{n}=-\liminf \frac{S_n}{n}$. (Since $X_n\overset{d}{=}-X_n$) As $\limsup \geq \liminf$, we've $c\geq -c \Rightarrow c\geq 0$ Now $\infty=\limsup \frac{|S_n|}{n} \geq \limsup \frac{S_n}{n}=c\hspace{5pt}$ i.e. $0\leq c \leq \infty$ which is trivially true. How do I show $c=\infty$? I appreciate any kind of hint/help. Thank you,","['probability-theory', 'probability', 'random-variables', 'law-of-large-numbers']"
1400501,Question about Hartshorne's proof of Halphen's Theorem (proposition IV.6.1),"My question comes from the proof of Theorem 6.1 in section IV.6 of Hartshorne, where I don't understand the very last step. The theorem is as follows: A curve $X$ of genus $g\geq 2$ has a nonspecial very ample divisor $D$ of degree $d$ if and only if $d\geq g+3$ . The necessity is shown, and then sufficiency. The idea is to show that the set $S$ of divisors $D \in X^d$ such that there exists $D' \sim D$ and points $P,Q \in X$ with $E = D'-P-Q$ an effective special divisor has dimension $\leq g+2$ . Because $d\geq g+3$ this means there is some $D\notin S$ that is nonspecial and very ample of degree $d$ . Hartshorne shows that the set of divisors of the form $E+P+Q$ in $X^d$ that are nonspecial with $E$ a special effective divisor has dimension $\leq g+1$ . The part that confuses me comes afterwards. Namely, as $E$ is special the Riemann-Roch tells us that $\dim |E| \geq d-1-g$ , and similarly that $\dim |E+Q+P| = d-g$ . Because the difference between these two dimensions is at most 1, this somehow implies that the set of divisors $S$ as above has dimension $\leq g+2$ . I don't understand this implication. We have divisors of the form $E+Q+P$ , each of which gives a linear system of dimension $d-g$ , and all of which form a set of divisors of dimension $\leq g+1$ . How does the difference in dimension of $E$ and $E+P+Q$ tell us the dimension of $S$ ? Any help in understanding this is appreciated.",['algebraic-geometry']
1400503,"Caratheodory criterion, sufficient but is it necessary?","When constructing the Lebesgue measure on $\mathbb{R}$, it is shown that the sets that satisfy the caratheodory criterion form a sigma-algebra (the Lebesgue sigma-algebra), and also that the countable additivity of the Lebesgue outer measure is satified on this sigma-algebra. But is the caratheodory necessary? That is, could there be a bigger sigma-algebra, or another sigma-algebra not comparable with the Lebesgue sigma-algebra, where the caratheodory criterion is not satisfied by all the sets, but countable additivity still holds?","['lebesgue-measure', 'measure-theory']"
1400518,Understanding $Gal(\bar k /k)$,"According to this book , An Introduction to the Langlands Program , One of the fundamental goals of modern number theory is to understand the Galois group $Gal(\bar k /k)$ where $k$ is a local or a global field. What is meant by understanding this group?","['field-theory', 'p-adic-number-theory', 'galois-theory', 'group-theory', 'algebraic-number-theory']"
1400519,Show that $\frac{x}{3!}-\frac{x^3}{5!}+\frac{x^5}{7!}-\cdots\leq \frac{1}{\pi}$.,"My problem is to show that
  $$\frac{x}{3!}-\frac{x^3}{5!}+\frac{x^5}{7!}-\cdots\leq \frac{1}{\pi}$$
  for all $x\in\Bbb R$. I was thinking of first finding the max and then show that its less than $1/\pi$. But it is hard to find it. I get that the series is equal to
$$f(x)=\frac{x-\sin x}{x^2}.$$
Then,
$$f'(x)=\frac{1-\cos x}{x^2}-\frac{2(x-\sin x)}{x^3}=0$$
if and only if
$$2\sin x=x(1+\cos x),$$
which I am unable to solve, appart from the obvious solutions $x=0$ and $x=\pi$. But if $x=\pi$ is the max, then we are done because $f(\pi)=1/\pi$.","['power-series', 'calculus', 'inequality']"
1400524,"Ramsey number $R(K_4,K_4,K_4)$.","I've done a bit of googling, but I can't seem to locate any bounds for $R(4,4,4)$.  Here, $R(n_1,n_2,n_3)$ is the generalized Ramsey number where $n_1,n_2,n_3$ are orders of complete graphs.  So, in this case, we're looking for the least positive integer, $n$, such that any red-blue-green coloring of the edges of $K_n$ necessarily admits a monochromatic $K_4$. Like I said, I've done some searching but turned up absolutely nothing.  Have there been any attempts at solving this?  i.e. do we have any upper or lower bounds for this? Tangentially, but related,  I also can't find the upper bound for the general multi-color case.  That is $R(G_1,G_2,G_3,...,G_n) \le ?$  In the two color case, we have that $R(F,H) \le \binom{s+t-2}{t-1}$ where $s$ and $t$ are the orders of $F$ and $H$, respectively.  Is there a combinatorial upper bound for the multi-color case?  We know it to exist by Ramsey's theorem, but what is the bound?  Or do we not have a closed form bound on this? EDIT:  I found the multi-color case in this question which gives $$R(k_1, \ldots, k_r) \leq 2 - r + \sum_{i=1}^r R(k_1, \ldots, k_{i-1}, k_i - 1, k_{i+1}, \ldots, k_r).$$ So, the question still remains.  Is there a better upper bound for $R(4,4,4)$ besides the one given above?  Explicitly:  $$R(4,4,4)\le 2-3+R(3,4,4)+R(4,3,4)+R(4,4,3)=-1+3\cdot R(3,4,4)\le -1 + 3(79) = 236$$.  The result used here is that $R(3,4,4) \le 79$  This is given in the same paper linked in the question I linked previously. This survey paper here The same paper also gives a lower bound: $R(4,4,4) \ge 128$.  I feel like this survey paper would have reflected any recent advancements in either lower or upper bounds for $R(4,4,4)$.  So, for now, it looks like the answer to my question is no.","['graph-theory', 'ramsey-theory', 'combinatorics']"
1400538,Show that Cauchy's function is infinitely differentiable,"Show that 
$$f(x)=
\begin{cases}
exp(-\frac{1}{x^2}), & \text{if $x\gt 0$} \\[2ex]
0, & \text{if $x\le 0$ }
\end{cases}$$ is infinitely differentiable. Clearly $f^{(n)}(x)=0$ for all $x\lt 0$ and $f_{-}^{(n)}(0)=0$. Also since the derivatives of $exp(-1/x^2)$ produce $exp(-1/x^2)$ and a polynomial in $1/x$, using the rules for differentiation we can evaluate $f^{(n)}$ if $x\gt 0$ with any $n$. It remains to show that $f_{+}^{(n)}(0)=0$. Now this final part is where I'm struggling. How can I show this part? I would greatly appreciate any help.","['analysis', 'calculus', 'real-analysis', 'derivatives']"
1400587,A circle's sine wave is an ellipse's...,"We all know what a sine wave is, and how it relates to a circle. 
What is the vertical and horizontal distance when I take a point and drag it along the perimeter of the ellipse? It definitely has to be a sinusoidal function, as an ellipse is a closed conic, much like the circle...","['conic-sections', 'trigonometry']"
1400596,Bear of an integral,"I have a pretty ferocious integral to solve, and would be over the moon if I were able to get some sort of analytic expression / insight for it. $$ I = \int_{r}^{\infty} r_0^{-5/2} W_{-i\alpha'/2, \nu}\left(\frac{-ir_0^2\omega'}{L}\right) W_{-i\alpha/2, \nu}\left(\frac{-ir_0^2\omega}{L}\right) (i^\beta J_{\beta}\left(-i\hat{k}r_0\right)) dr_0$$ All I've really done so far is look at the integrand in Mathematica (plotting Abs(integrand)): Where $W$ is the $W$ Whittaker function ( http://mathworld.wolfram.com/WhittakerFunction.html ) and $J$ is the Bessel function of the first kind ( http://mathworld.wolfram.com/BesselFunctionoftheFirstKind.html ) And also expand the integrand for small $r_0$ to gain some insight as to what happens at small $r$ , but this is far too imprecise for the application 
(particular solution of a  differential equation using Sturm-Liouville theory). For the application, the best thing I could do is write $I = I_0 + f(r)$ where $I_0$ is some analytically determined constant. r-dependence is the first datum of importance, and then $\hat{k},\omega',\alpha, \beta, \omega, \alpha'$ dependence. Note that these constants should be positive. If it helps, note that $\omega' = \omega + \hat{\omega}$ and $\alpha = k^2L/2\omega, \alpha' = (k+\hat{k})^2 L /(2 \omega')$","['asymptotics', 'calculus', 'special-functions']"
1400605,Convergent/divergent series,"Is the following series divergent/convergent? $$S=1-\frac{1}{2}-\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}-\frac{1}{7}-\frac{1}{8}-\frac{1}{9}-\frac{1}{10}+\frac{1}{11}+\frac{1}{12}+\frac{1}{13}+\frac{1}{14}+\frac{1}{15}-...$$ I think it is divergent since
$$
\begin{align}
S&>1-\frac{1}{2}-\frac{1}{2}+4\cdot\frac{1}{6}-\frac{4}{7}+\frac{5}{15}-\frac{6}{16}+...\\
&=1/3-4/7+1/3-6/16+...=\sum_{n=1}^\infty 1/3-\alpha_n
\end{align}
$$
where $\alpha_n=4/7, 6/16, 8/22$ which tends to 0, so the series on the right hand side is divergent. Is this the right answer? Thanks","['sequences-and-series', 'real-analysis']"
1400631,LU-factorization: why can't I get a unit lower triangular matrix?,"I want to find an $LU$-factorization of the following matrix: \begin{align*} A = \begin{pmatrix} 3 & -6 & 9 \\ -2 & 7 & -2 \\ 0 & 1 & 5 \end{pmatrix} \end{align*} This matrix is invertible (the determinant is $33$), so I should be getting a $LU$ decomposition where  $L$ is a lower triangular matrix with only $1s$ on the diagonal. But that's not what I'm getting. Here is what I did: \begin{align*} \begin{pmatrix} 3 & -6 & 9 \\ -2 & 7 & -2 \\ 0 & 1 & 5 \end{pmatrix} & \begin{matrix} \xrightarrow{R_1 \rightarrow 1/3 R_1} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ -2 & 7 & -2 \\ 0 & 1 & 5 \end{pmatrix} \begin{matrix} \xrightarrow{R_2 \rightarrow R_2 + 2R_1} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ 0 & 3 & 4 \\ 0 & 1 & 5 \end{pmatrix} \\ & \begin{matrix} \xrightarrow{R_2 \rightarrow 1/3 R_2} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ 0 & 1 & \frac{4}{3} \\ 0 & 1 & 5 \end{pmatrix} \begin{matrix} \xrightarrow{R_3 \rightarrow R_3 - R_2} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ 0 & 1 & \frac{4}{3} \\ 0 & 0 & \frac{11}{3} \end{pmatrix} = U. \end{align*} This matrix is now in echelon form. The elementary matrices corresponding to the operations are: \begin{align*} E_1 & = \begin{pmatrix} \frac{1}{3} & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \qquad (R_1 \rightarrow 1/3 R_1) \quad E_2 = \begin{pmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \qquad (R_2 \rightarrow R_2 + 2R_1) \\ E_3 &= \begin{pmatrix} 1 & 0 & 0 \\ 0 & \frac{1}{3} & 0 \\ 0 & 0 & 1 \end{pmatrix} \qquad (R_2 \rightarrow 1/3 R_2) \quad E_4 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -1 & 1 \end{pmatrix} \qquad (R_3 \rightarrow R_3 - R_2) \end{align*} Hence we have $E_4 E_3 E_2 E_1 A = U$. Now \begin{align*} E_4 E_3 E_2 E_1 = M = \begin{pmatrix} \frac{1}{3} & 0 & 0 \\ \frac{2}{9} & \frac{1}{3} & 0 \\ - \frac{2}{9} & - \frac{1}{3} & 1 \end{pmatrix} \end{align*} is a lower triangular matrix. Now we can write \begin{align*} A = (E_4 E_3 E_2 E_1)^{-1} U = M^{-1} U = LU, \end{align*} with \begin{align*} M^{-1} = L = \begin{pmatrix} 3 & 0 & 0 \\ -2 & 3 & 0 \\ 0 & 1 & 1 \end{pmatrix} \end{align*} But why does my matrix $L$ not have $1s$ on the diagonal? I thought an $LU$-factorization of an invertible matrix is unique, and that in that case $L$ is an unit lower triangular matrix? Did I overlook something or made a mistake? I haven't interchanged any rows. Please help!","['matrices', 'lu-decomposition', 'matrix-decomposition', 'gaussian-elimination', 'linear-algebra']"
1400657,Find $\lim\limits_{x\to0}\frac{\sqrt{1+\tan x}-\sqrt{1+x}}{\sin^2x}$,"Find: $$\lim\limits_{x\to0}\frac{\sqrt{1+\tan x}-\sqrt{1+x}}{\sin^2x}$$ I used L'Hospital's rule, but after second application it is still not possible to determine the limit. When applying Taylor series, I get wrong result ($\frac{-1}{6}$). What method to use? Result should be $\frac{1}{4}$","['calculus', 'limits']"
1400662,Find eigenvalues of unspecified matrix,"Find all possible eigenvalues of a $2\times 2$ matrix $A$ satisfying
  $$\det(A^2)I-2\det(A)A+A^2=0.$$ Well, if $Av=\lambda v$ then
$$\det(A^2)v-2\det(A)\lambda v+\lambda^2 v=(\det(A)^2-2\det(A)\lambda+\lambda^2) v=(\det(A)-\lambda)^2v=0$$
so $\lambda=\det(A)$. Can we go further and find $\det(A)$? Or perhaps a set of possible values? We have
$$0=\det(\det(A^2)I-2\det(A)A+A^2)=(\det(\det(A)I-A))^2$$
so
$$\det(A-\det(A)I)=0.$$
Can we solve for $\det(A)$? Edit: We even have
$$(A-\det(A)I)^2=0$$
which is a set of 4 equations in 4 unknowns, so perhaps this fully determines $A$. But how?","['linear-algebra', 'matrices']"
1400663,$\left \| \left \| f \right \|_{L^{p}} \right \|_{L^{q}} \leq \left \| \left \| f \right \|_{L^{q}} \right \|_{L^{p}} $ for $0<p\leq q$,"Let f be bounded on $X\times Y$ measure space with $\mathbb{P}\times\mathbb{Q}$ probability measure, show that for $0<p\leq q$: $\left \| \left \| f \right \|_{L^{p}(\mathbb{P})} \right \|_{L^{q}(\mathbb{Q})} \leq \left \| \left \| f \right \|_{L^{q}(\mathbb{Q})} \right \|_{L^{p}(\mathbb{P})} $. For example,
$\left \| \left \| f \right \|_{L^{p}(\mathbb{P})} \right \|_{L^{q}(\mathbb{Q})}=(\int (\int |f(x,y)|^{p}d\mathbb{P}(x))^{q/p}d\mathbb{Q}(y))^{1/q}$. Only hints please. Attempts a)Because $\frac{q}{p}\geq 1$ by Jensen's: $\left \| \left \| f \right \|_{L^{p}(\mathbb{P})} \right \|_{L^{q}(\mathbb{Q})}^{q} \leq (\int \int |f(x,y)|^{q}dQ dP)^{p/q} $. But because $p/q<1$, Jensen's does not work again. b)For simple function $f=\sum a_{i}1_{A_{i}\times B_{i}}$ and $p=1$ and $q=2$ we get $\left \| \left \| f \right \|_{L^{p}(\mathbb{P})} \right \|_{L^{q}(\mathbb{Q})} =[\sum a^{2}_{i}P(A_{i})^{2}Q(B_{i})+2\sum_{i<j}a_{i}a_{j}P(A_{i})Q(B_{i}\cap B_{j})P(A_{j})]^{1/2}$ and on the other hand $\left \| \left \| f \right \|_{L^{q}(\mathbb{Q})} \right \|_{L^{p}(\mathbb{P})}=\int[\sum a^{2}_{i}1_{A_{i}}Q^{2}(B_{i})+2\sum_{i<j}a_{i}a_{j}1_{A_{i}\cap A_{j}}Q(B_{i}\cap B_{j})]^{1/2}dP.$ 3)Say $f(x,y)=f(y,x)$ or more generally $\left \| f \right \|_{L^{p}(\mathbb{P})}=\left \| f \right \|_{L^{p}(\mathbb{Q})}$ and that $p>1$, then $\left \| f \right \|_{L^{p}(\mathbb{P})}\leq \left \| f \right \|_{L^{q}(\mathbb{Q})}$.","['probability-theory', 'real-analysis', 'lp-spaces']"
1400677,An mixed weak star convergence problem,"Let $\Omega\subset \mathbb R^N$ open bounded. Given a sequence of Radon measure $(\mu_n)$ such that $\mu_n\to \mu$ in weak star sense in $\mathcal M_b(\Omega)$ and $\|\mu_n\|\nearrow \|\mu\|$. Also given a function $v\geq 1$ such that $v\in L^1_{\text{loc}}(\Omega)$ and $l.s.c$. Suppose $v$ has properties that there exists a Lipschitz continuous sequence $v_n$ such that $1\leq v_n\leq v$ and $v_n\nearrow v$ for all $x\in\Omega$. Note that $v$ may not bounded above. Assume there exists a function $u\in C(\Omega)$ such that $u/v\in C_c(\Omega)$. My question: do we have 
$$
\lim_{n\to\infty}\int_\Omega \frac{u}{v_n}\,d\mu_n = \int_\Omega \frac{u}{v}\,d\mu
$$ My try: Writing 
$$
\int_\Omega \frac{u}{v_n}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu = \int_\Omega \frac{u}{v_n}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu_n+\int_\Omega \frac{u}{v}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu
$$
The last two goes to 0 by the definition of weak star convergence. But I don't know how to deal with first two. I was trying to use dominated convergence but it is not obvious... I realize that since $u/v\in C_c(\Omega)$ and $v$ is finite a.e., it makes $u$ has compact support and hence $u$, $u/v_n\in C_c(\Omega)$ since $u\in C(\Omega)$. Now, I could write few more steps...
\begin{align*}
\left|\int_\Omega \frac{u}{v_n}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu_n\right| &= \left|\int_\Omega u(1/v_n-1/v)\,d\mu_n\right|\\
&=\left|\int_\Omega u(1/v_n-1/v)\,d(\mu_n-\mu+\mu)\right|\\
&\leq \left|\int_\Omega u(1/v_n-1/v)\,d(\mu_n-\mu)\right|+\left|\int_\Omega u(1/v_n-1/v)\,d\mu\right|
\end{align*}
The last one can be done by dominated convergence. But the first one...Maybe I should use the fact that $\|\mu_n\|\to\|\mu\|$? I know generally I should not hope 
$$
\lim_{n\to\infty}\int_\Omega \frac{u}{v_n}\,d\mu_n = \int_\Omega \frac{u}{v}\,d\mu
$$
since it would require that $u/v_n\to u/v$ uniformly which I don't have. But since I in additional have $0\leq 1/v\leq 1/v_n\leq 1$ and $\|\mu_n\|\to \|\mu\|$, I may expect my result is true.","['real-analysis', 'functional-analysis', 'measure-theory']"
1400680,Find $\int \frac{\sqrt{1-x^2}}{1+x^2}\hspace{1mm}dx$,"Find $$\int \dfrac{\sqrt{1-x^2}}{1+x^2}\hspace{1mm}dx$$ Any hints! I will do the work, just give me a clue","['calculus', 'indefinite-integrals', 'integration']"
1400698,Question about Kolmogorov extension theorem,"I need some help understanding the relationship between the following two theorems Theorem 1: Let $\{\mu_n\}$ be a sequence of probability measures on $(\mathbb{R},\mathcal{B}(\mathbb{R}))$, where $\mathcal{B}(\mathbb{R})$ denotes the Borel sets. Then, there exists a probability space $(\Omega,\mathcal{F},\mathbb{P})$ and a sequence of independent random variables $\{X_n\}$ such that $\mathbb{P}(X_n \in B) = \mu_n(B), n\geq 1$, where $B$ is Borel set of $\mathbb{R}$ And a version of Kolmogorov Extension Theorem is given as Theorem 2: For every $n$, let $\mu^n$ be a probability measure on $(\mathbb{R}^n,\mathcal{B}(\mathbb{R}^n)$. For $1\leq m \leq n$, let $\Pi_{m,n}$ be the projetion map given as:
  $$ \Pi_{m,n}: B \in \mathcal{B}(\mathbb{R}^m) \to \Pi_{m,n}(B) \in \mathcal{B}(\mathbb{R}^n) $$
  $$\Pi_{m,n}(B) = \{(x_1,x_2,\dots,x_n) \in \mathbb{R}^n : (x_1,x_2,\dots,x_m) \in B \}$$ Suppose $\mu^n$ satisfies that $\forall n \geq 1, \forall 1 \leq m \leq n,$ $\mu^n \circ \Pi_{m,n} = \mu^m$. Then, there exists a probability space $(\Omega,\mathcal{F},\mathbb{P})$ such that $\mathbb{P}((X_1,X_2,\dots,X_n) \in B) = \mu^n(B)$, where $B$ is a Borel set in $R^n$. *Remark:  Theorem 1 is a special case of Theorem 2 when $\mu^n = \prod_{i=1}^n \mu_i$ I do not understand the remark in the Theorem 2. Why is Theorem 1 a special case of Theorem 2? I am confused since the Theorem 1 shows the existence of an independent sequence of random variables whereas the Theorem 2 does not say whether a sequence is independent. Is there something that I am missing?","['probability-theory', 'probability', 'measure-theory']"
1400699,Show that Brownian motion on the unit circle is exponentially ergodic and has the uniform measure as its invariant distribution.,"My search results keep bring up planar Brownian motion on the unit disk. However, I am specifically referring to $e^{jW_{t}} = [\cos(W_t),\sin(W_t)]^{T}$ where $W_t$ is Brownian motion.
I am at a loss here, so any advice to point me in the right direction would be greatly appreciated. Thanks in advance.","['ergodic-theory', 'markov-process', 'stochastic-processes', 'brownian-motion', 'probability']"
1400702,"Convert $(-1.0, 1.0)$ to degrees",I'm trying to convert an analog stick from a game controller into degrees. It gives me a range from $-1$ to $+1$ on the $x$ and $y$ axes. I can get values for $x$ and $y$. If dead right is $0$ degrees how can I find the angle of the red dot in degrees?,"['linear-algebra', 'trigonometry']"
1400730,What is the probability that this harmonic series with randomly chosen signs will converge?,"Suppose we fix $p$ between $0$ and $1$ (without loss of generalization, we can assume $p \leq 1/2$). Then suppose we form the series $\sum_n a_n / n$ where the $a_n$ are independent random variables and each $a_n$ equals $1$ with probability $p$ and equals $-1$ with probability $1-p$. (Hence why we can assume $p \leq 1/2$.) What is the probability that this series converges, as a function of $p$? Clearly if the series diverges with probability 1 for $p = 1/2$ then it always diverges with probability $1$ for any $p$, so perhaps the case $p=1/2$ is the most interesting. (However, if other values of $p$ give non-zero probability of convergence that would also obviously be interesting.)","['sequences-and-series', 'probability']"
1400755,Prove explicitly that if a function has a left inverse it is injective and if it has a right inverse it is surjective,"A function g : S → T is said to be a left inverse for the function f : T → S if g◦f equals the identity function on $T$. In this case, f is also a right inverse for g . Prove explicitly that That t any a function has a left inverse it is injective and if it has a right inverse it is surjective, give an example of a function that has 2 distinct left inverses. I'm not too sure if i understand the difference the difference of between a left and right inverse to begin with. I think it's left inverse if in f: S --> T and g: T --> S meaning in f is you plug in S to f(x) you get T and for g(x) plugging in T  you get S. So g(x) is the inverse of f(x) and vice versa. Left inverse f(g(x)) = S and  right inverse when rather g(f(x)) = T. Definitions have $1_S$ and $1_T$ which confuses me. To be injective i also know that for every value of the codomain T there must be a unique value of S the domain. So there cannot be more than one value of T that maps into a value of S. To surjective and everyone value of T the codomain must map to a value the domain S. So for example e^x  is not  surjective for all real x because its range is y > 0 rather than having a range of all real x.  However you could cut the parameters to something like x>0 and even say it's bijective where it is both surjective and injective. .",['elementary-set-theory']
1400762,When does $\det e^A=e^{\det A}?$,"Which $2\times 2$ matrices satisfy the equation
  $$\det e^A=e^{\det A}?$$ I know that $\det e^A=e^{\operatorname{trace}A}$ so assuming $A$ is real we get
$$\operatorname{trace}A=\det A.$$
Then,
$$\det(A-\lambda I)=\lambda^2-2\operatorname{trace}(A)\lambda+\det(A)=(\lambda-\det(A))^2$$
so the only eigenvalue is
$$\lambda=\det(A)=\operatorname{trace}A.$$
Hence,
$$\operatorname{trace}A=2\lambda=2\operatorname{trace}A\implies\operatorname{trace}A=\det A=0.$$
Write
$$A=\begin{pmatrix}a&b\\c&-a\end{pmatrix}$$
so that $\operatorname{trace}A=0$ is already taken into account. Then,
$$\det A=-a^2-bc=0$$
so $a^2=-bc$. Thus,
$$A=\begin{pmatrix}\sqrt{-bc}&b\\c&-\sqrt{-bc}\end{pmatrix}$$
Is that the end of the solution?",['linear-algebra']
1400769,How to solve $\lim_{n\to \infty}\sin(1)\times \sin(2)\times\sin(3)\times\ldots\times\sin(n)$,"The limits I'm trying to solve are: $$\lim_{n\to \infty}\sin(1)\times\sin(2)\times\sin(3)\times\ldots\times\sin(n)$$
$$\lim_{n\to \infty}n\times\sin(1)\times\sin(2)\times\sin(3)\times\ldots\times\sin(n)$$ For the former limit, my (probably incorrect) solution is that $\sin(1)\times\sin(2)\times\sin(3)\ldots$ are constants, so the limit can be written as $$\sin(1)\times\sin(2)\times\sin(3)\times\ldots\times\sin(n-1)\cdot \lim_{n\to \infty}\sin(n)$$ and $\lim_{n\to \infty}\sin(n)$ simply does not exist, because $\sin(n)$ does not settle on a single value when ${n\to \infty}$.","['calculus', 'limits']"
1400800,Why is this true for matrices? Linearly dependent columns $\implies$ not invertible,"If $A$ is a square matrix with linearly dependent columns, then $A$ is not invertible. Why is this true for matrices?","['linear-algebra', 'matrices']"
1400809,How do I prove this $\frac{dx^n}{dx}=nx^{n-1}$ is true for every $n\geq 1$ to convince my students?,"let  $p_n(x)=x^n$ be a polynomial of degree $n$. I need help to be able to explain to my students why the derivative of $p$ is defined as follows: $$
p_n'(x)=\frac{dx^n}{dx}=nx^{n-1}
$$ for every $n\geq1$. Note: I'd prefer geometric proofs if any exist. Edit :I edited the question as it is related to the recent question Thank you for any help","['polynomials', 'proof-verification', 'derivatives']"
1400818,Showing that a holomorphic function $f : \mathbb{C}\setminus\{0\} \to \mathbb{C}$ with $f(2z) = f(z)$ is constant,"Let $f : \mathbb{C}\setminus\{0\} \to \mathbb{C}$ be a holomorphic function satisfying $f(2z) = f(z)$ for all $z \in \mathbb{C}\setminus\{0\}$. Show that $f$ is constant. Here $f$ is defined as a map $\mathbb{C}\setminus\{0\}\rightarrow \mathbb{C}$. If $0$ is a removable singularity, then it is clear to me that $f(z) = f(0)$ for all $z$. Further, I can rule out the possibility that $0$ is a pole since this would lead to a contradiction on the absolute value of $f$ as I take successive values of $z,z/2,z/4, z/8$, etc. However, I cannot currently rule out the possibility that such an $f$ exists which is non-constant and has an essential singularity at $0$. Can anybody suggest a way please?",['complex-analysis']
1400821,Probability of drawing two green balls without replacement,"A box contains of some green balls and some white balls. Given that the probability of drawing two green balls without replacement is 0.5, what is the smallest total number of ball inside this box? Hence, state the total number of green and white balls. The smallest total number of balls inside the box should be 4, the total number of green balls is 3 and the total number of white balls is 1. I'm just using the trial and error to get these answer. Is there any other way to do it?","['probability', 'statistics']"
1400829,Determine all functions $f:\mathbb{Q}\to\mathbb{Q}$ satisfying the functional equation $f(2f(x) + f(y)) = 2x + y$,"Determine all functions $f$ defined on the set of rational numbers that take
  rational values for which
  $$f(2f(x) + f(y)) = 2x + y \tag{1}$$
  for each x and y. This question is from the 2008 Canada National Olympiad. The form of the defining equation strongly suggests a linear function, and that's all I found. Is there a trick somewhere that admits another class of solutions? Since $x,y\in\mathbb{Q}$ but otherwise take on all possible values, we are free to impose constraints at will. $$x=y=0: f(3f(0))=0 \tag{2}$$ $$y=x: f(3f(x))=3x \tag{3}$$ $$x=0: f(2f(0)+f(y))=y \tag{4}$$ $$y=0: f(2f(x)+f(0))=2x \tag{5}$$ By (5) and (2) See $\color{blue}{\text{grand_chat}}$'s reasoning: $$f(x)=0 \implies x=0 \tag{6}$$ So then by (2) again : $$f(0) = 0 \tag{7}$$ Put $y=2x$ in (4) and equate to (5): $$\begin{align}
& f(2f(0)+f(2x)) = f(2f(x)+f(0))  \\
\implies & 2f(0)+f(2x) = 2f(x)+f(0) &(\color{blue}{\text{f invertible}}) \\
\implies & 2f(x)-f(2x) = f(0) = 0 \\
\implies & f(x)=kx \tag{8}
\end{align}$$ Now by (3): $$f(3f(x))=f(3kx)=3k^2x=3x \implies x=0 \lor k=\pm1$$ Therefore $$\boxed{f(x)=kx,\quad k\in\{-1,1\}}$$","['contest-math', 'algebra-precalculus', 'functional-equations']"
1400862,Multiplication operators on $L^2$,"Let $X$ be a $\sigma$-finite measure space, and let $g$ a measurable complex-valued function $X$, which lies in $L^\infty(X)$. I would like to determine sufficient and necessary properties for the operator $T:L^2(X)\rightarrow L^2(X)$ by $f\mapsto gf$ to be self-adjoint, an isometry (i.e. inner-product preserving), to be surjective, and to be injective. I feel that it is self-adjoint iff $g$ is real-valued, an isometry iff it takes values in the unit circle, surjective or injective iff it is constant. Is this true?","['lebesgue-integral', 'functional-analysis', 'measure-theory']"
1400877,A condition equivalent to equicontinuity,"I am doing a problem which is an application of the Arzela-Ascoli theorem, which boils down to proving that a certain condition is equivalent to equicontinuity. Specifically, I am given a sequence of real valued absolutely continuous functions $\{f_n\}$ on the interval $[0, 1]$ and told that there is a constant $K$ such that $\int_0^1 (f'_n(x))^2 dx \leq K$ for all $n$, The absolute continuity of the functions lets me conclude that $f_n' \in L^1([0, 1])$ for all $n$, but I don't know how to prove equicontinuity from here.","['real-analysis', 'functional-analysis']"
1400901,Bounding $f'$ in terms of $f$ and $f''$,"Assume that $f: \mathbb{R} \to [0,\infty)$ is $C^2$ and $|f''(x)| \leq A$ for all $x$.  Show that the inequality $$(f'(x))^2 \le 2Af(x)$$ holds for all $x$. The hint given in the question was, ""Taylor's theorem."" I had thought in the beginning that the bound was pretty straightforward, but I've been stuck for a bit now. I have that, centering my Taylor expansion about some point $a$, gives $$f(x) = f(a) + f'(a)(x-a) + \frac{f^{''}(\psi)}{2!}(x-a)^2$$ for some $\psi$ in the interval $(x,a)$.  The last term is the Lagrange remainder. Then $$f(x) = f(a) + f'(a)(x-a) + \frac{f^{''}(\psi)}{2!}(x-a)^2$$ $$\le f(a) + f'(a)(x-a) + \frac{|f^{''}(\psi)|}{2!}(x-a)^2$$ $$\le f(a) + f'(a)(x-a) + \frac{A}{2!}(x-a)^2$$ $$\implies f(x)-f(a)\le  f'(a)(x-a) + \frac{A}{2!}(x-a)^2$$ $$\implies \frac{f(x)-f(a)}{x-a}\le  f'(a) + \frac{A}{2!}(x-a)$$ $$\implies f'(\eta)\le  f'(a) + \frac{A}{2!}(x-a)$$
(applying the Mean Value Theorem on the left hand side) and this is where I am stuck. Any hints are welcome. Thanks,","['taylor-expansion', 'sequences-and-series', 'calculus', 'real-analysis']"
1400902,"Limit of an Integral, Then taking Sum","I am given that $I_n=\int^1_0x^ne^x\,dx$ Now, how can I find the value of the following limit:
$$\lim_{n\to\infty}\left(\sum_{k=1}^{n}\frac{I_{k+1}}{k}\right)$$ I suppose solving for $I_n$ is that necessary first here?","['summation', 'calculus', 'limits', 'integration']"
1400932,"limit involving $e$, ending up without $e$.",Compute the limit $$ \lim_{n \rightarrow \infty} \sqrt n \cdot \left[\left(1+\dfrac 1 {n+1}\right)^{n+1}-\left(1+\dfrac 1 {n}\right)^{n}\right]$$ we have a bit complicated solution using Mean value theorem. Looking for others,"['analysis', 'limits']"
1400968,Can the interior of a manifold be orientable but not its boundary?,"Suppose $M^m$ is a manifold with boundary. If we are given an orientation for $M$, we can then derive an orientation for $\partial M$ by considering the orientation of $TM$ at $\partial M$ and then using an outward-pointing vector to get an orientation of $T(\partial M)$. This made me wonder: is it possible that $M^\circ = M \setminus \partial M$ is orientable but $M$ is not? Is it possible that $M^\circ$ is orientable but $\partial M$ is not?","['differential-geometry', 'smooth-manifolds', 'orientation']"
1400969,"inequality involving $x$, $x^3$,$\sin(x),\cos(x)$","Let $x \in \left[0,\dfrac {\pi} 2 \right]$. Prove the inequality $$6x \ge 6\sin x +x^3 \cdot \cos x$$ there is nice solution using Taylor expansion. Is there other one?",['analysis']
1400976,"Prove that $\int f(x)g(y) \,d(\mu \times \nu) = [\int f\, d\mu ][\int g \,d\nu]$","I got stuck on this problem to figure out how to calculate the integral on left-handed side, because we can't use Tonelli-Fubini theorem for this problem (lack of $\sigma$-finite condition). Hope someone can help me or give some hint to solve it. I'm thinking about building two simple function sequences $\{f_n\}$ and $\{g_n\}$ which converge to $f$ and $g$ correspondingly, but can't come to the conclusion. Thanks so much. Let $(X, \mathcal{M}, \mu)$ and $(Y, \mathcal{N}, v)$ be arbitrary measure spaces. If $f \in L^1(\mu)$ and $g \in L^1(v)$, prove that $h(x, y) = f(x)g(y) \in L^1(\mu \times v)$ and $\int h \, d(\mu \times v) = [\int f\,d\mu][\int g\,dv]$.","['real-analysis', 'measure-theory', 'integration']"
1400985,Prove that every such $f$ is $=0$ everywhere,"Let $f: \mathbb{R} \to \mathbb{R}$ be differentiable;
  let $0 \leq f'(x) \leq f(x)$ for all $x \in \mathbb{R}$;
  and let $f$ vanish at some point. Prove that $f = 0$ on $\mathbb{R}$. Since there is some $c \in \mathbb{R}$ such that $f(c) = 0$, by the mean-value theorem for derivatives for every $x < c$ there is some $x_{1} \in ]x,c[$ such that $f(c) - f(x) = -f(x) = f'(x_{1})(c-x) \geq 0$. Since $f \geq f'\geq 0$ on $\mathbb{R}$, we conclude that $f(x) = 0$ for all $x < c$. But what about $x > c$? The mean-value theorem in this case seems not to be helpful?","['real-analysis', 'derivatives']"
1400987,Given $x\in \left(0; \frac\pi2\right)$. Prove that $\sin x>\frac{2x}{\pi}$,"Given $x\in \left(0; \frac\pi2\right)$. Prove that $$\sin x>\frac{2x}{\pi}$$ This is my try: Let $y=\sin x-\frac{2x}\pi\implies y ' = \cos x - \frac2\pi\implies y ''=-\sin x <0; \forall x\in \left(0; \frac\pi2\right)$
$\implies \left\{\begin{matrix} y '(x)> y '(\frac\pi2) = -\frac2\pi\\ y' (x) < y'(0) =1-\frac2\pi\end{matrix}\right.$ $\bullet\quad y ' < 0 \implies y (x) > y (\frac\pi2)=0$ $\bullet\quad y' >0 \implies y(x) > y(0) =0$ But it went wrong.",['trigonometry']
1401032,Help with calculating a Riemann-Stieltjes integral.,"The question is to calculate $\int_{ - 1}^2 {xd\omega (x)}  = 0$ where $\omega (x) = \left\{ {\begin{array}{*{20}{c}} 0&{1 \le x \le 2}\\ 1&{0 \le x < 1}\\ 2&{ - 1 \le x < 0} \end{array}} \right.$ $f(x)=x$ is continuous, and $\omega(x)$ is of bounded variation (since it is bounded decreasing), then $\int_{ - 1}^2 {xd\omega (x)} $ exists. It has been pointed out that the following calculation is incorrect since $\phi$ has discontinuity. I also checked my textbook which states differentiation of $\phi$ holds only when $\phi$ is continuously differentiable. $\int_{ - 1}^2 {xd\omega (x)}  = \int_{ - 1}^0 {xd\omega (x)}  + \int_0^1 {xd\omega (x)}  + \int_1^2 {xd\omega (x)}  = 0 + 0 + 0 = 0$. Then what is the right way to calculate this integral if we cannot use derivative? Integration by parts? Or other method? Thank you! Previous problem . Then I have a problem with a theorem in my textbook. The theorem is attached below, where $\int_E f $ is a Lebesgue integral on set $E$, and $\omega(\alpha)=\mu\{x\in E:f(x)>\alpha\}$. Now suppose $\Omega =[0,2]$ is the whole set and $(\Omega,\cal M)$ is a measure space where $\cal M$ is the set of all Lebesgue measurable subsets of $\Omega$. Function ${\chi _{[0,1]}}(x) = \left\{ {\begin{array}{*{20}{c}}
1&{x \in [0,1]}\\
0&{x \notin [0,1]}
\end{array}} \right.$ is Lebesgue measurable. Since $ - 1 < {\chi _{[0,1]}}(x) \le 2$ on $[0,2]$, then by the above theorem it looks that we have $\int_\Omega  {{\chi _{[0,1]}}(x)}  =  - \int_{ - 1}^2 {\alpha d\omega (\alpha )} $ It is clear that $\omega (\alpha ) = \mu (\{ x \in \Omega :{\chi _{[0,1]}}(x) > \alpha \} ) = \left\{ {\begin{array}{*{20}{c}}
0&{1 \le \alpha  \le 2}\\
1&{0 \le \alpha  < 1}\\
2&{ - 1 \le \alpha  < 0}
\end{array}} \right.$, so I think the right-hand-side $ - \int_{ - 1}^2 {\alpha d\omega (\alpha )} = 0$. But the left-hand-side $\int_\Omega  {{\chi _{[0,1]}}(x)}  = 1$ by Lebesgue integration. The theorem in the textbook is definitely correct, so what's wrong with my reasoning ?? Thank you!","['real-analysis', 'measure-theory']"
1401034,"If $f(x,y)$ is continuous in $(x_0,y_0)$, then there is a neighborhood of $(x_0,y_0)$ such that $f(x,y)>\frac12f(x_0,y_0)$","The exercise asks me to prove 2 things: 1) $f(x,y) $ is continuous in $(x_0,y_0)$, $f(x_0,y_0)>0$ then there is a neighborhood such that $f(x,y)>\frac{1}{2}f(x_0,y_0)$ My idea: $f$ is continuous, then $|(x,y)-(x_0,y_0)|< \delta\implies |f(x,y)-f(x_0,y_0)|< \epsilon \implies f(x_0,y_0) -\epsilon <f(x,y) < f(x_0,y_0) + \epsilon$. It's true for all $\epsilon$, so if I choose $\epsilon = \frac{1}{2}f(x_0,y_0)$
we have: $$\frac{1}{2}f(x_0,y_0)<f(x,y) < f(x_0,y_0) + \frac{1}{2}f(x_0,y_0)$$ 2) Suppose $f$ is continuous in a domain $D$. Suppose that $f(x,y)$ is positive for at least $1$ point of $D$ and negative for at least one point of D. Then $f(x,y) = 0$ for at least one point of $D$. (suggestion: use $1$) How to use $1$ to prove $2$? As I know, this can be understood as the mean value theorem for multivariables, but I couldn't find a proof that used $1$.","['calculus', 'multivariable-calculus']"
1401170,Parallel transport along a 2-sphere.,"I'm currently learning about parallel transport and connections and we were considering the parallel transport of a tangent vector along a sphere as given in the picture below. From my understanding, by defining a connection on your manifold, you provide a way to identify vectors at one point of the manifold with vectors at another point on the manifold via parallel transporting the vector. So in the given example, when the initial vector is parallely transported along the closed curve it returns to the same spot as a different vector. Is this because there has not been defined a correct connection on the sphere (that takes into account the curvature of the 2-sphere)? In which case, when a connection is defined on the sphere, (i.e. by the covariant derivative) parallel transport of any vector along a closed curve back to its initial position will result in the same vector? So this is in fact an example of the need for a connection, and not just the standard derivative? Thanks in advance!","['differential-geometry', 'connections']"
1401174,Proving a Lipschitz constant does not exist.,"So I wish to find for each of these functions a Lipschitz constant or prove that none exists. So my definition for a function to be Lipschitz is: A function $f:[a,b] \rightarrow \mathbb{R}$ is Lipschitz if there exists a $L$ such that $|f(x) - f(y)| \leq L|x-y|$ for all $x,y \in [a,b]$. $f(x) = \frac{1}{x}$ for $x \in (0, 1]$ $f(x) = e^x$ for $x \in \mathbb{R}$ $f(x) = \sqrt{1-x^2}$ for $x \in [-1,1]$ My attempt for 1) to prove $f$ is not Lipschitz is via contradiction. Suppose that $x, y \in (0,1]$ and $f$ is Lipschitz. Then there exists a $L$ such that, $$|\frac{1}{x} -\frac{1}{y}| = \frac{|x-y|}{|xy|} \leq L |x-y|$$ for all $x,y \in (0, 1]$. This would imply that, $\frac{1}{|xy|} \leq L$ for all $x,y \in (0, 1]$. But such a $L$ cannot exist since we can make $x, y$ as small as we like, the fraction will grow. So in conclusion I attempted 1) but not sure if I am correct. I am stuck on 2 and 3. Anybody can give me some hints? I'm thinking of using the Mean Value Theorem on question 2. Other than that I have no idea how to start.",['calculus']
1401206,How to evaluate the integral $\int_{-1}^1 e^{ax^2+bx+c\sqrt{1-x^2}}dx$,"Can anyone show if the following integral can be evaluated in closed form? 
\begin{equation}
\int_{-1}^1 e^{ax^2+bx+c\sqrt{1-x^2}}dx
\end{equation}
The variable $x$ can be replaced by $\cos{\theta}$, with corresponding change of the interval of integration.",['integration']
1401213,$\cos \alpha+\cos(\alpha+\beta)+\cos(\alpha+2\beta)+.....+\cos(\alpha+(n-1)\beta)=0 $,"If each side of a regular polygon of $n$ sides subtend an angle $\alpha$ at the center of the polygon and each exterior angle of the polygon is $\beta$,then prove that $\cos \alpha+\cos(\alpha+\beta)+\cos(\alpha+2\beta)+.....+\cos(\alpha+(n-1)\beta)=0 $ Since this is a regular polygon.Therefore,each $\alpha=\frac{2\pi}{n}$ and since each external angle is $\beta$.So by geometry,$\alpha=\beta.$ $\cos \alpha+\cos(\alpha+\beta)+\cos(\alpha+2\beta)+.....+\cos(\alpha+(n-1)\beta)=\frac{\cos\frac{n\beta}{2}}{\cos\frac{\beta}{2}}\cos\frac{2\alpha+(n-1)\beta}{2}$ Now putting $\alpha=\beta=\frac{2\pi}{n}$ does not give me answer.What mistake did i make?","['geometry', 'polygons', 'trigonometry']"
1401229,$\lim_{n\to \infty}\frac{S_1+S_2+S_3+.....+S_n}{n}=\frac{1}{2}\cot\frac{\theta}{2}$,Let $S_n=\sin \theta+\sin 2\theta+\sin 3\theta+.......+\sin n\theta$.Prove that $\lim_{n\to \infty}\frac{S_1+S_2+S_3+.....+S_n}{n}=\frac{1}{2}\cot\frac{\theta}{2}$ $\lim_{n\to \infty}\frac{S_1+S_2+S_3+.....+S_n}{n}=\lim_{n\to \infty}\frac{\sin \theta+(\sin \theta+\sin 2\theta)+(\sin \theta+\sin 2\theta+\sin 3\theta)+.....+(\sin \theta+\sin 2\theta+\sin 3\theta+.......+\sin n\theta)}{n}$ $=\lim_{n\to \infty}\frac{n\sin \theta+(n-1)\sin 2\theta+(n-2)\sin  3\theta+.....+2\sin (n-1)\theta+\sin n\theta}{n}=\lim_{n\to \infty}\frac{\sum_{k=1}^{n}(n-k+1)\sin k\theta}{n}$ and then i stuck.Please help me get through.,"['limits', 'trigonometry']"
1401238,Product rule of the derivative of a matrix by a vector,"I am trying to express the derivative of the outer product of the $(n\times m)$-matrix 
$\mathbf{A}=\mathbf{A}(\mathbf{x})$ with respect to the $p$-vector $\mathbf{x}$. This is, I want to rewrite $\frac{\partial \mathbf{A}\mathbf{A}^T}{\partial \mathbf{x}}$ using a product rule. My intuition tells me that I must have something like
$$
\frac{\partial \mathbf{A}\mathbf{A}^T}{\partial \mathbf{x}}=\mathbf{A}\otimes\frac{\partial \mathbf{A}}{\partial \mathbf{x}}+\frac{\partial \mathbf{A}}{\partial \mathbf{x}}\otimes\mathbf{A}.
$$
Any help or confirmation on this? Thanks!","['calculus', 'matrix-calculus', 'matrices']"
1401244,Collinearity problem (Newton-Gauss line),"I had some troubles with this problem : Let $ABCD$ be a convex quadrilateral. $M$ and $N$ are the midpoints of
  the diagonals $AC$ and $BD$. The sides $AB$ and $CD$ are extended
  until they intersect. The intersection point is $E$. The sides $AD$
  and $BC$ are extended until they intersect. The intersection point is
  $F$. Let $P$ be the midpoint of the segment $[EF]$. Prove that $M$, $N$, $P$ are 
  collinear. First, I found that a quadrilateral in which the opposite sides interesect is also known as a complete quadrilateral .
Then, the line $M-N-P$ is known as Newton-Gauss line and the problem above as Newton's Problem . I've taught about solving it using areas. I've used the property that median divides the triangle in two echivalent triangles (with the same area) . Many properties can be derived from it. I haven't figure out, but I'm interested in a proof using areas. I would appreciate some suggestions. Thanks!","['euclidean-geometry', 'geometry', 'area', 'quadrilateral']"
1401261,Finding the distance from the origin to the surface $xy^2 z^4 = 32$ using the method of Lagrange Multipliers,"Problem: Find the distance from the origin to the surface $xy^2z^4 = 32$. Attempt: The Lagrange equation for this problem is $L(x,y,z, \lambda) = x^2 + y^2 + z^2 + \lambda (xy^2 z^4 - 32)$. Setting the first partials to zero we have \begin{align*} \frac{\partial L}{\partial x} &= 2x + \lambda y^2 z^4 = 0 \qquad (1)  \\ \frac{\partial L}{\partial y} &= 2y + 2 \lambda x y z^4 = 0 \qquad (2) \\ \frac{\partial L}{\partial z} &= 2z + 4 \lambda x y^2 z^3 = 0 \qquad (3) \\ \frac{\partial L}{\partial \lambda} &= xy^2 z^4 - 32 = 0 \qquad (4)
\end{align*}
Now I'm having a hard time solving this system for $x,y$ and $z$. Here is what I did so far. From $(1)$ and $(2)$ we get \begin{align*} \frac{2x}{y^2 z^4} = - \lambda \qquad \text{and} \qquad \frac{1}{xz^4} = - \lambda \end{align*} Thus $\frac{2x}{y^2 z^4}  = \frac{1}{xz^4} $ or $y^2 = 2x^2$ after simplification. Also, from $(2)$ and $(3)$ we can deduce that \begin{align*} \frac{1}{xz^4} = - \lambda = \frac{2z}{4xy^2 z^3} \end{align*} so that $2y^2 = z^2$ after simplification. Now I used all this and substituted it into $(4)$. This gave me \begin{align*} x(2x^2) (4y^4) - 32 = 0 \end{align*} or (since $y^4 = 4x^4)$ \begin{align*} 8x^3 (4x^4) - 32 = 0 \end{align*} 
This means that $32x^7 - 32 = 0$, so that $x = 1$. Then $y^2 = 2$, so that $y = \pm \sqrt{2}$. Then $z^2 = 4$, so that $z = \pm 2$. So I found the points $(x,y,z) = (1, \sqrt{2}, 2)$ and $(1, - \sqrt{2}, -2)$. They both give me the distance $\sqrt{x^2 + y^2 + z^2} = \sqrt{7}$, so I'm guessing they are equal? Is my reasoning correct?","['lagrange-multiplier', 'optimization', 'calculus', 'multivariable-calculus']"
1401305,"Finding $\int_0^{\pi/8} x\sin 2x\,dx$","I have a trigonometric equation, when integrated and evaluated should be a specific value. I cannot get that value. The question: $$\int_0^{\pi/8} x\sin 2x\,dx$$ The answer should be 
$$\frac{4-\pi}{16\cdot 2^{1/2}}$$ --this is written in the book.","['trigonometry', 'calculus', 'integration']"
1401323,Finding slope $\frac{dy}{dx}$ of tangent line to a curve defined in polar coordinates,"Problem: Let the curve $f$ be defined by $r = e^{\theta}$ . Compute the slope $\frac{dy}{dx}$ of the tangent line to $f$ . Then use your result to define a function $g(x,\theta)$ that is a tangent line to $f$ for every $\theta$ . Find the angle $\zeta$ between $0$ and $\frac{\pi}{2}$ where the tangent line to $f(\zeta)$ intersects the $x$ -axis in the point $x = 3$ . Attempt at solution: We have $$ \frac{dy}{dx} = \frac{r \cos(\theta) + \frac{dr}{d \theta} \sin(\theta)}{ -r \sin(\theta) + \frac{dr}{d \theta}\cos(\theta)} = \bigg(\frac{dy/d\theta}{dx /d \theta} \bigg) $$ hence \begin{align*} \frac{dy}{dx} = \frac{e^{\theta} \cos(\theta) + e^{\theta} \sin(\theta) } {-e^{\theta} \sin(\theta) + e^{\theta} \cos(\theta)} = m
\end{align*} Now, I defined my function $g$ as follows: $$ g(x, \theta) = m(x - \theta) + e^{\theta} $$ But I'm not sure if the last part is correct, i.e. the $e^{\theta}$ . If the tangent line has to go through the point $x=3$ aswell, then we must have \begin{align*}
g(3,\theta)
= m(3 - \theta) + e^{\theta}.
\end{align*} Then, should I try solving the equation \begin{align*}
m(3- \zeta) + e^{\zeta}
= 0
\end{align*} for $\zeta$ with Maple, searching for solutions around the interval $\left[0, \frac{\pi}{2}\right]$ ?
If I do this, I get the numerical value $1.25$ , which lies between $0$ and $\frac{\pi}{2}$ . So is my reasoning correct? Help would be appreciated!","['calculus', 'real-analysis', 'multivariable-calculus']"
1401345,"Is it meaningful to take ""exterior products"" of vector fields?","Let $M$ denote a smooth manifold. I've read that a differential $k$-form is a smooth section of the $k$th exterior power of the cotangent bundle of $M$. However I barely understand what this means, and I'm trying to understand it better by tinkering with the definition. It seems that there is a notion of ""$k$-vectorfield"" obtained by putting the tangent bundle in place of the cotangent bundle. As in: Potentially Silly Definition . A $k$-vectorfield is a smooth section of the $k$th exterior power of the tangent bundle of $M$. Following this line of thought, it seems that we can take wedge products of vector fields. As in: $$f \frac{\partial}{\partial x} \wedge \frac{\partial}{\partial y} + g\frac{\partial}{\partial y} \wedge \frac{\partial}{\partial z}$$ Question. Is this, like, a thing? If not, why is it only the cotangent bundle whose exterior powers make sense and/or matter?",['differential-geometry']
1401365,A problem about Frattini subgroup of a subgroup,"Let $H$ be a subgroup of $G$ . Does that imply, that $\Phi(H)\le \Phi(G)$ ? If not, then what properties $G$ must have for it to be true. $\Phi$ stands for Fattini subgroup .","['abstract-algebra', 'group-theory', 'frattini-subgroup']"
1401377,Help finding the fundamental group of $S^2 \cup \{xyz=0\}$,"let $X=S^2 \cup \{xyz=0\}\subset\mathbb{R}^3$ be the union of the unit sphere with the 3 coordinate planes. I'd like to find the fundamental group of $X$. These are my ideas: I think the first thing I should do is to retract all the points outside the sphere to the sphere (is that possible? how?) then using spherical coordinates I could make the following deformation: 
$(1,\phi,\theta)\to ((\sin\phi \sin\theta \cos \phi\cos \theta)^t,\phi,\theta)$. This collapses all the point in $S^2 \cap \{xyz=0\}$ to $0$ obtaining $8$ deformed spheres touching each other in $0$ (how can I prove rigorously that they are simply connected), using Van Kampen theorem we can say that $X$ is simply connected.","['fundamental-groups', 'algebraic-topology', 'general-topology']"
1401415,Evaluate $\sum_{r=0}^n \binom{n}{r}\sin rx \cos (n-r)x$,"Evaluate $$ \sum_{r=0}^n \left[\binom{n}{r}\cdot\sin rx \cdot \cos (n-r)x\right] $$ I tried to use binomial identities, but since there are trigonometric terms, I don't have the idea how to approach it. Can anyone please help?","['binomial-theorem', 'binomial-coefficients', 'algebra-precalculus', 'trigonometry']"
1401424,"Question about equivalent norms on $W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$.","Assume $\mathbb{‎‎H}=W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$ with the norm induced from inner product
$$\langle u,v\rangle_{‎\mathbb{‎‎H}}=\int_\Omega \Delta u \,\Delta v\, dx$$
for any $u \in W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$ it is well-known that we have inequality (Rellich inequality)
$$ ‎\Lambda_N ‎\int_{\Omega}‎\frac{u^2}{|x|^4}\mathrm{d}x ‎\leq ‎\int_\Omega ‎|\Delta u|^2 \, ‎\mathrm{d}x‎
$$
 where $‎\Lambda_N=(‎\frac{N^2(N-4)^2}{16})‎$ is optimal constant and also it is known that
$$\|u\|^2=‎\int_\Omega ‎‎\Big(|\Delta u|^2-\Lambda_N \frac{u^2}{|x|^4}\Big) \, \mathrm{d}x $$ ‎
Defines an other norm on $W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$. My question is this that is these two norm equivalent? To see an improved case of this inequality with reminder term see Corollary 1 of Paper . I know if I could show that $W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$ is a hilbert space with the new inner product then this two norms must be equivalent due to Open mapping theorem.","['regularity-theory-of-pdes', 'sobolev-spaces', 'partial-differential-equations', 'analysis', 'functional-inequalities']"
1401465,Rigorous meaning of the expression $dz = dx + idy$,"Many complex analysis books just define $dz$ by $dz = dx + idy$. In smooth manifold theory, the expressions like $dx$, $dy$, $df$ have precise meaning: covector field. My question is: What is the precise meaning of the expression '$dz = dx + idy$'? Do we understand this expression in terms of linear functionals on tangent space?",['complex-analysis']
1401480,Do finite additivity and countable subadditivity imply countable additivity?,"Given a Measure Space and f a positive set function on the sigma-algebra of the space (not identically infinite), how could I prove that f is a measure given the hypothesis above? I've tried both by contradiction and induction, but I struggle a bit with the intuition.",['measure-theory']
1401483,Does trivial fundamental group imply contractible?,"Let $X$ be a path-connected topological space with a trivial fundamental group:
  $$\pi_1(X,x_0)=\{e\}.$$
  Does $X$ have to be homotopic to a point? I know that the converse is true: a contractible space has trivial fundamental group. But what about the converse? Does the fundamental group tells us enough of the space to fix its homotopy type when it is trivial?","['fundamental-groups', 'algebraic-topology', 'general-topology']"
1401503,Genus and faces of a graph,"I am trying to determine the genus of a simple, undirected, connected graph using Euler's formula. However, I'm having trouble computing the number of faces of this graph: I seem to be confused about the nature of the casual relationship between face and genus. According to the discussion below, ""faces"" of a non-planar graph the definition of a face is dependent on planarity and spatial embeddings. How, then, can a delineation of the number of yet-defined ""faces"" be used to determine spatial embeddings? It feels like aporia to me. If anyone could explain, I would greatly appreciate it.","['logic', 'graph-theory', 'topological-graph-theory', 'discrete-mathematics']"
1401538,A group homomorphism from a simple group is injective,"Let $G_1$ be a simple group, that is the only normal subgroups of $G_1$ are itself and the trivial subgroup. If $\phi : G_1 \rightarrow G_2$ is a group homomorphism, does that mean $\phi$ is injective? Could someone explain?","['abstract-algebra', 'group-homomorphism', 'simple-groups']"
1401547,Solving linear differential equations,"Find the general solution for the following equation: $$\frac{dy}{dt}+2ty=\sin(t)e^{-t^2}$$
  Find a solution for which $y(0)=0$ First I found the integrating factor which is $e^{t^2}$ Multiplying both sides gives $$e^{t^2}\frac{dy}{dt}+e^{t^2}2ty=e^{t^2}\sin(t)e^{-t^2}$$ which simplifies to $$\frac{d}{dt}(e^{t^2}y)=\sin(t)$$ Integrating both sides gives $$e^{t^2}y=-\cos(t)$$ Now rearranging gives $$y(t)=\frac{-\cos(t)}{e^{t^2}}$$ However this doesnt give $y(0)=0$ could anyone help as to where I have gone wrong? thanks!","['calculus', 'elementary-functions', 'integration', 'ordinary-differential-equations', 'derivatives']"
1401552,How to show that $|\exp(z)-1|\le2|z|$ for $|z|\le 1$,"How to show that $|\exp(z)-1|\le2|z|$ for $|z|\le 1$ $\displaystyle|\exp(z)-1|=\big|\sum\limits_{k=1}^\infty\frac{z^k}{k!}\big|\le\sum\limits_{k=1}^\infty\frac{|z|^k}{k!}=|z|+\sum\limits_{k=2}^\infty\frac{|z|^k}{k!}$ now it remains to prove that the sum, the most right is $\le|z|$ Since $\sum\limits_{k=2}^\infty\frac{|z|^k}{k!}\le\sum\limits_{k=0}^\infty\left(\frac{|z|}{2}\right)^k-\frac{|z|}{2}-1+\frac{|z|^2}{4}$ (geometric series) the RHS is $\displaystyle \frac{1}{1-\frac{|z|}{2}}-\frac{|z|}{2}-1+\frac{|z|^2}{4}=\frac{\frac{|z|}{2}}{\frac{2}{|z|}-1}+\frac{|z|^2}{4}\le|z|$ am I right ?","['exponential-function', 'proof-verification', 'complex-analysis', 'inequality']"
1401563,$R^2$ is not isometric to $R^3$,Is there a direct proof for showing that $R^2$ is not isometric to $R^3$ (with the usual metrics)? I know that they are not homeomorohic but I think there should be some direct and easy proof for showing that they are not isometric.,"['metric-spaces', 'general-topology']"
1401566,Weighted projective space and $\mathrm{Proj}$,"I'm trying to solve a problem from Jenia Tevelev's notes on GIT. (Problem 5 at the end of this pdf .) Compute $$\operatorname{Proj}\frac{\mathbb{C}[x,y,z]}{(x^5+y^3+z^2)}$$ where $\operatorname{wt.}x,y,z=12,20,30$. Here is what I have so far. Let $R=\mathbb{C}[x,y,z]$ with the above weighting, so we have the natural graded ring structure given by the weighted degree: $R=\bigoplus_{n\geqslant0}R_n$. Then $R\cong R^{(2)}\cong R^{(4)}\cong R^{(20)}\cong R^{(60)}$, where $$R^{(k)}=\bigoplus_{n\geqslant0}R_{kn}.$$
Under these transformation, the ideal given by the polynomial in the question transforms as $$(x^5+y^3+z^2)\mapsto (x^5+y^3+z^2)\mapsto (x^5+y^3+z)\mapsto (x+y^3+z)\mapsto (x+y+z).$$ So, with $S=\frac{\mathbb{C}[x,y,z]}{(x^5+y^3+z^2)}$ we have that $$S^{(60)} = \left(\frac{\mathbb{C}[x,y,z]}{(x^5+y^3+z^2)}\right)^{(60)} = \frac{\mathbb{C}[x,y,z]^{(60)}}{(x+y+z)} = \frac{\mathbb{C}[x^5,y^3,z^2]}{(x+y+z)}.$$ Questions: Is all of the above right? In particular, is it true that $$\left(\frac{\mathbb{C}[x,y,z]}{(x^5+y^3+z^2)}\right)^{(60)} \neq \frac{\mathbb{C}[x,y,z]^{(60)}}{(x^5+y^3+z^2)}.$$
That is, we can't just 'carry through' the truncation, we have to see how the ideal changes as well. How can I proceed from here? Edit: It seems that the rather useful fact that I missed is that $$\operatorname{Proj}\frac{k[x_0,\ldots,x_n]}{I}\cong\mathbb{V}(I)\subset\mathbb{P}(a_0,\ldots,a_n)$$
  where $\operatorname{wt.}x_i=a_i$. Then the problem boils down to understanding $$\frac{\mathbb{V}_{\mathrm{aff}}(x^5+y^3+z^2)\setminus\{0\}}{\mathbb{G}_m}$$ where $\mathbb{G}_m$ acts on $\mathbb{A}^3$ via the standard way for weighted projective space. But I'm not sure if this is true when the generators are of different degrees, in which case we do need to look at the truncation first. (If this is not the case then I'd be super interested to know how both methods proceed - via the truncation and just jumping straight in.) Then it looks like it might make sense to guess that $$\operatorname{Proj}\frac{\mathbb{C}[x^5,y^3,z^2]}{(x+y+z)} = \mathbb{V}(x+y+z)\subset\mathbb{P}(60,60,60)$$ but the issue is that $(x+y+z)$ is not homogeneous, and our ideal is given in terms of elements that we can't make from our generators.","['algebraic-geometry', 'graded-rings']"
1401586,Find $\lim\limits_{n\to+\infty}\sqrt[n]{\prod\limits_{k=1}^{n}{n\choose k}}$,"I tried to compute the product of binomial coefficients. I found that 
$$\prod\limits_{k=0}^{n}{n\choose k}=\frac{H^2(n)}{(n!)^{n+1}}$$ I am not familiar with hyperfactorial function. How to find this binomial product?","['radicals', 'sequences-and-series', 'limits', 'binomial-coefficients']"
1401588,Proof that applying the difference operator to a $d$-degree polynomial $d$ times yields $d!a_d$,"Let $L$ be the lag operator and $\triangledown:=(1-L)$ be the difference operator , that is, given a polynomial $p(t)$, we have 
$$L(p(t))=p(t-1)\qquad \triangledown(p(t))=p(t) - p(t-1)$$ I am interested in proving that
  $$
\triangledown^d \sum_{i=0}^{d}a_it^i = a_dd!
$$ It can be shown that $c<d \implies \triangledown^dt^c=0$. Hence, the above polynomial should (?) be equivalent to
$$
\triangledown^da_dt^d 
$$
which, together with the linearity of the difference operator, suggests that the problem is equivalent to prove that
$$
a_d(\triangledown^dt^d) = (a_d)d! 
$$ 
that is
$$
\triangledown^dt^d = d! 
$$ 
or, equivalently
$$
\triangledown^dt^d = (1-L)^dt^d=\sum_{k=0}^{d}(-1)^{d-k}\binom{d}{k}L^k(t)^d = d! \iff
$$ 
$$
\triangledown^dt^d = (1-L)^dt^d=\sum_{k=0}^{d}(-1)^{d-k}\binom{d}{k}(t-k)^d = d!
$$ 
but from this point I am stuck.
I would like to tag the entry with a Lag-operator tag but do not have sufficient reputation yet. Any help would be much appreciated.","['polynomials', 'derivatives']"
1401589,$G$ be a finite group and $G'$ be its commutator subgroup and $\widehat G$ be the character group of $G$ ; then $G/G' \cong \widehat G$?,"Let $G$ be a finite group and $G'$ be its commutator subgroup and $\widehat G$ be the character  group of $G$ ; then is it true that $G/G' \cong \widehat G$ ? I am completely stuck , please help . thanks in advance","['characters', 'group-theory', 'finite-groups']"
1401596,Abelian group and their subgroups,Is it true that If an abelian group has subgroups of order m and n respectively then it has a subgroup whose order is the least common multiple of m and n? If it is then can anyone explain it with a valid proof?,"['abelian-groups', 'group-theory']"
1401610,"Let $x,y \in \mathbb C^n$ , where $n>1$ ; then does there exist symmetric $A \in M_n(\mathbb C) $ such that $Ax=y$?","Let $x,y \in \mathbb C^n$ , where $n>1$ ; then does there exist $A \in M_n(\mathbb C) $ such that $Ax=y$ ? Can we find such a symmetric matrix $A$ ?","['vector-spaces', 'linear-algebra', 'matrices']"
1401612,Classification of dense and complete linear orders,"Question. Is there a decent classification theorem for linear orders satisfying all three of: Dense. Given a pair of elements $y,x$ with $y>x$, there exists $k$ satisfying $y>k>x$. Complete. Given a non-empty subset $A$, if $A$ is bounded above, then $A$ has a least upper bound. Endless. There is neither a greatest element nor a least element. Comments: Assuming separability, $\mathbb{R}$ is the only example. . There are other examples. . I'm happy to assume some further conditions, like ""for any two points, there is an order-automorphism mapping the first point to the second."" However, I'd like models of unboundedly large cardinalities.","['order-theory', 'general-topology', 'infinitary-combinatorics']"
1401618,The Limit of an Integral Containing Exponentials,"I am unsure how to show this. Suppose $\delta(s)$ defined on $(-\infty , s_*)$ is increasing and satisfies $\lim _{s\rightarrow s_*} \delta = \lim _{s\rightarrow s_*} \frac{d \delta}{d s} = \infty$ then $e^{-\delta (s)}\int _{s_0}^se^{\delta (s')}ds'\rightarrow 0$ as $s\rightarrow s_*$ for any $s_0<s_*$. The hint is to rewrite the integration variables with respect to $\delta$. This means we need consider $\lim _{\delta _s\rightarrow\infty}e^{-\delta}\int _{\delta _0}^{\delta _s}e^{\delta}(\frac{d\delta}{ds})^{-1}d\delta$ but I am unable to proceed further. Any help is much appreciated.","['infinity', 'limits', 'substitution', 'exponential-function', 'integration']"
1401649,Translation of 'morphisme net'?,"In French, one refers to a certain 'morphisme net'. I am looking for the English translation of this. EDIT: The term appears here on p.22 Lemme 2.7.2. Unfortunately I have not been able to find the French definition either. Thanks!","['mathematical-french', 'translation-request', 'algebraic-geometry']"
1401661,"How many different dice exist? That is, how many ways can you make distinct dice that cannot be rotated to show they are the same?","Dice are cubes with pips (small dots) on their sides, representing
  numbers 1 through 6. Two dice are considered the same if they can be
  rotated and placed in such a way that they present matching numbers on
  the top, bottom, left, right, front, and back sides. Below is an example of two dice that can be rotated to show that they
  are the same if the 2-pip and 4-pip sides are opposite and the 3-pip
  and 5-pip sides are also opposite. https://www.dropbox.com/s/6q56njm11hu3f36/Screenshot%202015-08-18%2012.02.11.png?dl=0 How many different dice exist? That is, how many ways can you make
  distinct dice that cannot be rotated to show they are the same? Note:
  This problem does not involve rolling the dice or the probability of
  roll outcomes. I'm having trouble understanding exactly what is being asked in this question. I understand that I have to find how many different ways the dice can be placed to show that they are the same, but saying they cannot be rotated confuses me. Could somebody make an attempt at rewording this? Or walking me through how to solve this?","['dice', 'combinatorics']"
1401672,$L^2(\mathbb{R})$ sequence such that $\sum_{n=1}^{\infty}\int_{\mathbb{R}}f_n(x)g(x)d\mu(x)=0$,"I am currently studying for an analysis qualifying exam, and this problem has been bothering me: Suppose we have a sequence $\{f_n\}$ in $L^2(\mathbb{R})$ such that $\sum_{n=1}^{\infty}||f_n||_2^2<\infty$ and $\sum_{n=1}^{\infty} f_n(x)=0$ for almost every $x\in\mathbb{R}$. Then for every $g\in L^2(\mathbb{R})$, $\sum_{n=1}^{\infty}\int_{\mathbb{R}}f_n(x)g(x)d\mu(x)$ exists and is equal to zero. My first thought was to use Cauchy-Schwarz, but the problem is that the norm of $f_n$ is squared in the sum above. My other thought was to try and use something like the Dominated Convergence Theorem on a function $h_m(x)=\sum_{n=1}^{m}f_n(x)$, but I think I am forgetting something easy about the relationship between $|f_n(x)|$ and $||f_n||_2$. Any pointers would be appreciated. Thank you in advance!","['lp-spaces', 'real-analysis', 'lebesgue-integral']"
1401680,"$a^2+b^2=2Rc$,where $R$ is the circumradius of the triangle.Then prove that $ABC$ is a right triangle","If in a triangle $ABC$,$c$ is the longest side and $a^2+b^2=2Rc$,where $R$ is the circumradius of the triangle.Then prove that $ABC$ is a right triangle. $a^2+b^2=2Rc\Rightarrow a^2+b^2=\frac{c^2}{\sin C}$ $\sin C=\frac{c^2}{a^2+b^2}$,how to proceed ahead?I am stuck.",['trigonometry']
1401688,What is an example of Gâteaux differentiable but not Fréchet differentiable at a point in a finite-dimensional space?,"Let $V,W$ be nonzero normed spaces over $\mathbb{K}$ such that $V$ is finite-dimensional. Let $E$ open in $\mathbb{K}$ and $p\in E$. Let $f:E\rightarrow W$ be Gâteaux-differentiable at $p$. Is $f$ necessarily Fréchet-differentiable at $p$ in this case? I think this is not true in general, but cannot find a counterexample. What would be a counterexample?","['gateaux-derivative', 'examples-counterexamples', 'multivariable-calculus']"
1401695,"Is this solution mathematically ""legal""?","I have the sequence
$$
a_n = \frac{n \cos n}{n^2 + 1}
$$
and I'm trying to evaluate the limit of $a_n$ as $n\to\infty$
$$
\begin{align*}
\lim_{n\to\infty}a_n&= \lim_{n\to\infty}\frac{n \cos n}{n^2 + 1}\\ &= \lim_{n\to\infty}\cos n \cdot \lim_{n\to\infty} \frac{n}{n^2 + 1}\\
\end{align*}
$$
Using L’hôpital’s rule twice on $\frac{n}{n^2+1}$
$$
\begin{align*}
\lim_{n\to\infty}a_n&= \lim_{n\to\infty}\cos n \cdot \lim_{n\to\infty} \frac{0}{2} \\ &= 0
\end{align*}
$$ Is there any flaw in this method?","['sequences-and-series', 'limits']"
1401716,Tangent planes to $2+x^2+y^2$ and that contains the $x$ axis,"I need to find the tangent planes to $f(x,y) = 2+x^2+y^2$ and that contains the $x$ axis, so that's what I did: $$z = z_0 + \frac{\partial f(x_0,y_0)}{\partial x}(x-x_0)+\frac{\partial f(x_0,y_0)}{\partial y}(y-y_0) \implies \\ z = 2 + x_0^2 + y_0^2 + 2x_0(x-x_0) + 2y_0(y-y_0) \implies \\ 2xx_0 + 2yy_0-z-x_0^2-y_0^2+2=0$$ So since the plane must contain the $x$ axis, its normal vector must have the form $(0,y,z)$. The normal vector fot the plane I found is: $$(2x_0, 2y_0, -1)$$ so $x_0 = 0, y_0 = y_0$ our plane has the form: $$2y_0y -z -y_0^2+2 = 0$$ but when I plot this graph for some values of $y_0$ I only get 1 tangent plane at $y_0\approx 1.5$","['calculus', 'multivariable-calculus', 'linear-algebra']"
1401726,Proving a known zero of the Riemann Zeta has real part exactly 1/2,"Much effort has been expended on a famous unsolved problem about the Riemann Zeta function $\zeta(s)$ .  Not surprisingly, it's called the Riemann hypothesis, which asserts: $$ \zeta(s) = 0 \Rightarrow \operatorname{Re} s = \frac1 2 \text{ or } \operatorname{Im} s = 0 .$$ Now there are numerical methods for approximating $\zeta(s)$, but as I understand, no one knows any exact values except at even integers, which include the trivial zeroes for which $\operatorname{Im} s = 0 $.  So I've always wondered: For the rest, how does one prove $\operatorname{Re} s = 1/2$ exactly ? (All I know that seems vaguely useful is the argument principle, which I'm not sure helps here, but I'd be happy to learn about other techniques that aren't too far advanced.) Edit :  Looking over this again, I found out I missed the closed-form values at odd negative integers .  This doesn't affect the question but seemed worth correcting.  Thanks to the people who contributed.","['riemann-hypothesis', 'conjectures', 'special-functions', 'riemann-zeta', 'complex-analysis']"
1401728,Sufficient Condition for $f\in L^{1}(\mathbb{R}^{d})$ to belong to $L^{2}(\mathbb{R}^{d})$ in terms of its Fourier coefficients,"Question. Let $\left\{\varphi_{j}\right\}$ be a complete orthonormal system for $L^{2}(\mathbb{R}^{d})$ such that each $\varphi_{j}\in C_{b}(\mathbb{R}^{d})$ (the space of continuous, bounded functions). Let $f\geq 0\in L^{1}(\mathbb{R}^{d})$ and suppose that $$\sum_{j=1}^{\infty}\left|\langle{f,\varphi_{j}}\rangle\right|^{2}<\infty \tag{*}$$ Does it follow that $f\in L^{2}(\mathbb{R}^{d})$ and therefore $f=\sum_{j}\langle{f,\varphi_{j}}\rangle\varphi_{j}$? This question is motivated by the one asked here by someone else, but for which I have offered a bounty. I am trying to consider a simpler version of the problem in the linked question. I've tried playing around with approximation arguments (i.e. construct a sequence $f_{n}\in L^{2}(\mathbb{R}^{d})$ which converges to $f$ in some sense) but I run into problems in trying to use (*) to control $\left\|f_{n}\right\|_{L^{2}}$. For instance, if we could produce a sequence $f_{n}\rightarrow f$ in $L^{1}(\mathbb{R}^{d})$ such that $\sup_{n}\left\|f_{n}\right\|_{L^{2}}<\infty$, then there would exist $g\in L^{2}(\mathbb{R}^{d})$ and a subsequence $f_{n_{k}}\rightarrow g$ weakly in $L^{2}(\mathbb{R}^{d})$. It then follows that $f=g\in L^{2}(\mathbb{R}^{d})$. Any help would for this question, as well as the linked one, would be great. Update: The question linked to appears to have answered negatively by David C. Ullrich.","['orthonormal', 'hilbert-spaces', 'functional-analysis', 'measure-theory']"
1401760,Find $\lim_{n\to\infty}\frac{a^n}{n!}$ [duplicate],"This question already has answers here : Calculus - Prove $\lim_{n \to \infty} \frac{x^n}{n!}=0$ [duplicate] (2 answers) Closed 6 years ago . First I tried to use integration:
$$y=\lim_{n\to\infty}\frac{a^n}{n!}=\lim_{n\to\infty}\frac{a}{1}\cdot\frac{a}{2}\cdot\frac{a}{3}\cdots\frac{a}{n}$$
$$\log y=\lim_{n\to\infty}\sum_{r=1}^n\log\frac{a}{r}$$
But I could not express it as a riemann integral . Now I am thinking about sandwich theorem. $$\frac{a}{n!}=\frac{a}{1}\cdot\frac{a}{2}\cdot\frac{a}{3}\cdots\frac{a}{t} \cdot\frac{a}{t+1}\cdot\frac{a}{t+2}\cdots\frac{a}{n}=\frac{a}{t!}\cdot\frac{a}{t+1}\cdot\frac{a}{t+2}\cdots\frac{a}{n}$$
Since $\frac{a}{t+1}>\frac{a}{t+2}>\frac{a}{t+1}>\cdots>\frac{a}{n}$
$$\frac{a^n}{n!}<\frac{a^t}{t!}\cdot\big(\frac{a}{t+1}\big)^{n-t}$$
since $\frac{a}{t+1}<1$, $$\lim_{n\to\infty}\big(\frac{a}{t+1}\big)^{n-t}=0$$
Hence, $$\lim_{n\to\infty}\frac{a^t}{t!}\big(\frac{a}{t+1}\big)^{n-t}=0$$
And by using sandwich theorem, $y=0$. Is this correct?","['calculus', 'limits']"
1401765,Infinite Integration in Limits of Integration,"Given the following:
$$ u_0 = \int \limits_{ 0 } ^{ 1 } x \, dx , \:\:\: u_1 = \int \limits^{ \int \limits_{ 1/2 } ^{ 1 } x \, dx } _{ \int \limits_{ 0 } ^{ 1/2 } x \, dx } x \,dx , \:\:\: u_2 = \int \limits_{ { \int \limits_{{\int \limits _{0} ^{1/4} x \, dx}} ^{{\int \limits_{1/4} ^{2/4} x \, dx}} x \, dx}} ^{{\int \limits_{{\int \limits_{2/4} ^{3/4} x \, dx}} ^{{\int \limits_{3/4} ^{1} x \, dx}} x \, dx}} x \,dx$$ So as we can see, $ \{ u_{ i } \} _{ i = 0 } ^{ \infty } $ is in fact a sequence of real numbers, in which only three of the first few terms are shown. So now, if $P = \displaystyle \prod _{ n = 0 } ^{ 2015 } \frac{ 1 } { 4 u_{ n } }$, how can we find $  \log_{2}P$?","['sequences-and-series', 'calculus', 'integration']"
1401773,Calulate a limit involving $\zeta{(\zeta{(z)})}$,"I'm currently trying to evaluate the following limit:
$$
\lambda=\lim_{z\to\infty}{\left[2^z-\left(\frac{4}{3}\right)^z-\zeta{(\zeta{(z)})}\right]}
$$
A look at numerical approximations suggests, that $\lambda=1-\gamma$, but I have no idea how to tackle the iterated zeta function. I know that $\zeta(x)$ is asymptotical to $\frac{1}{1-x}$ when $x\to 1$, but this seems not to help here. Any help will be highly appreciated!","['asymptotics', 'zeta-functions', 'limits']"
1401777,gateaux derivative and frechet derivative,"In calculus, we have the following equation $DF(x,y)=\partial F_xdx+\partial F_ydy$ if $F$ is differentiable. I think such equation still holds for frechet derivative, but not for gateaux derivative. So do we have a similar version of this equation when we are using gateaux derivative to find the derivative of a function like $F(x,y)$? Thanks!","['analysis', 'calculus', 'multivariable-calculus', 'derivatives']"
1401785,Is convergence in probability sometimes equivalent to almost sure convergence?,"I was reading on sufficient and necessary conditions for the strong law of large numbers on this encyclopedia of math page , and I came across the following curious passage: The existence of such examples is not at all obvious at first sight. The reason is that even though, in general, convergence in probability is weaker than convergence with probability one, nevertheless the two types of convergence are equivalent, for example, in the case of series of independent random variables. (""Such examples"" in the above refers to an example of a sequence $(X_k:k\in\mathbb N)$ where the weak law of large numbers holds but the strong law fails, due to some dependence). This really surprised me, as I have never heard of this claim, and unfortunately, there is no reference for it on the website. I was not able to find a reference for it with a google search or in the textbooks that I have either. If this is indeed true, I would be very interested in a reference, or a counterexample if it is mistaken.","['probability-theory', 'reference-request', 'convergence-divergence', 'probability-limit-theorems']"
1401791,Solving a 2nd-order elliptic PDE with non-constant coefficients,"I wonder how I can solve the following 2nd-order PDE on the positive semiplane $\{x>0\}$: $$(\partial_x^2+\frac{1}{x}\partial_y^2)\phi=\delta(x-x_0)\delta(y).$$ I notice that the l.h.s. is the Laplace-Beltrami operator of a metric of negative curvature (according to my calculations) applied to the function $\phi$.  I would appreciate if there is some nice, reasonably general solution to this PDE.","['differential-geometry', 'surfaces', 'partial-differential-equations']"
1401795,Creating a minimal sufficient statistic with Likelihood function,"To find a minimal sufficient statistic you can take the likelihood ratio and find a function $T$ so that the ratio does not depend on the parameter $\theta$
, as page 18 here http://sites.stat.psu.edu/~mga/514/Ch6.pdf does it. Since I do not understand the proof, I wonder if I can understand this via some logic or intuition.","['statistics', 'statistical-inference']"
1401819,A question on ordinary differential inequality,"Could we find a solution $f=f(x)$ to the following initial problem for the OD inequality?
$$3xf'+f-\sqrt{6f}\leq 0,\quad f(0)=0,\quad f(8/3)=6.$$ . Added: The above question is in fact a special case of the following question: Could we find a solution $f=f(x)$ to the following initial problem for the OD inequality?
$$xf'+(u+1/6)f-2\sqrt{uf}\leq 0,\quad f(0)=0,\quad f(8/3)=6,$$
where $u$ is a to-be-determined function.","['analysis', 'ordinary-differential-equations']"
1401823,Swapping the integrand and variables being integrated,"I am reading a paper in which the following integral is solved, but I am not sure how to derive the answer myself.  The integral is: $$\int_{F_{x-1}}^{F_x} du {\int_{G_{y-1}}^{G_y}{dv}\  C^*(u,v)} \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  [1] $$ I know that the answer simplifies to:
$$ \frac{f_X g_Y}{4} * \{ C^*(F_{X}, G_{Y}) + C^*(F_{X}, G_{Y-1}) + C^*(F_{X-1}, G_{Y}) + C^*(F_{X-1}, G_{Y-1}) \} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [2]$$ The original integral is, 
$$\int_{F_{x-1}}^{F_x}{\int_{G_{y-1}}^{G_y}{C^*(u,v)}\ du\ dv } \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  [3] $$
which I think is supposed to be the same as Equation [1]. What I do know for a fact is the following:
$$\int_{F_{x-1}}^{F_x} du {\int_{G_{y-1}}^{G_y}{dv}} = (F(x)-F(x-1))\times(G(y)-G(y-1)) = f_X g_Y \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [4]$$ From Equation [4], I can kind of see where the answer is coming from, but obviously I cannot just pull out $C^*(u,v)$ from the integral itself?  I am beginning to think that the author rewrote Equation [3] in the form of Equation [1] deliberately, but I am unsure as to why?  It seems that the function to be integrated, $C^*(u,v)$ is swapped with the variables to be integrated but I haven't seen this form of an integral before, so I'm not 100% sure what I'm working with. Something else that I tried is the following:
$$ \int^2_1 x dx = 2 - 1/2 = 3/2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [5]$$
$$ \int^2_1 dx\  x =  \frac{x|^2_1 * (1 + 2)}{2} = 3/2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [6]$$
But really, I'm not sure where the divide by 2 (I just put it in there to make the answers equal) in Equation 6 comes from, or whether I just got lucky somehow and this is not a rule in general? Any help is greatly appreciated! Edit #1: All, so I investigated the simple example that I tried further and I think I have a solution.  Generalizing Equation's 5 and 6, what I want to see is when the following is true:
$$ \int_a^b{f(x)dx} = F(b)-F(a) \stackrel{?}{=} \int_a^b{dx\ f(x)} = \frac{(b-a)*(f(a)+f(b))}{2} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [7] $$
Lets have a look at this geometrically ... The blue line represents a general function $f(x)$.  What we see here is the black hatched area represents the value of $\int_a^b{f(x) dx}$ while the red hatched area represents the value of $\frac{(b-a)(f(a)+f(b))}{2}$.  So now from this, lets think about when these two values will be equal.  For this, refer to Figure 2 below. We can see that the red-hatched area and the black-hatched area will be equal if the areas of the orange triangle's are equivalent.  This means that the LHS and RHS of Equation 7 are equivalent if that condition holds true.  I think that means that we can say in general that Equation 7 is true if the function being integrated is linear in the region of integration.  If the function is not linear in the region of integration, then if the area's (not necessarily traingular any longer) are equivalent then Equation 7 still holds.  I think the derivation then for Equation [2] is a 2-D extension of this principle.  Anybody have thoughts on this?","['probability', 'definite-integrals', 'integration']"
1401826,selection of balls of three colors with restrictions,"I have asked a similar question here and answers were very helpful. I tried doing similar questions and could solve them comfortably. However, I myself came up with a question like this and wondering how to solve this. Suppose there are 20 black balls, 10  yellow balls and 5 brown balls. Balls of same color are identical. Order of selection does not matter. How many ways can a selection of 15 balls be made? How many ways can a selection of 15 balls be made if 2 brown must be selected always? My approach (1) number of solutions of $x_1+x_2+x_3 = 15$ with $0\le x_1\le 20; 0 \le x_2 \le 10; 0 \le x_3 \le5$ is the solution. Is this right? if so, is there any formula for this? (2) Select the 2 brown balls in (only 1 way) and then find number of solutions of  $x_1+x_2+x_3 = 13$ with $0\le x_1\le 20; 0 \le x_2 \le 10; 0 \le x_3 \le3$ Please give directions on how to approach similar problems. thanks.",['combinatorics']
1401868,Fibonacci proof question $\sum_{i=1}^nF_i = F_{n+2} - 1$ [duplicate],"This question already has answers here : For the Fibonacci sequence prove that $\sum_{i=1}^n F_i= F_{n+2} - 1$ (4 answers) Closed 7 years ago . The sequence of numbers $F_n$ for $n \in N$ defined below are called the Fibonnaci numbers.
 $F_1 = F_2 = 1$, and for $n \geq 2$, $F_{n+1} = F_n + F_{n-1}$.
 Prove the following facts about the Fibonnaci numbers For each of the following, $n \in N$ a) $\displaystyle \sum_{i=1}^nF_i = F_{n+2} - 1$ I started this proof by using strong induction To prove something by strong induction, you have to prove that if all natural numbers strictly less than $N$ have the property, then $N$ has the property. $n \geq 2$, $F_{n+1} = F_n + F_{n-1}$ Check basis step $n=2$: $F_1 + F_2 = 1 + 1 = 2 = 3-1 = F_4 - 1$, therefore TRUE I'm unsure how to go any further, when it comes to strong induction I'm lost on how to set up my IH, and proceed for the rest of the steps","['summation', 'induction', 'discrete-mathematics', 'fibonacci-numbers']"
1401870,"Existence of a solution to $f(x) = \int_0^1 k(x,y) f(y) dy$","Let $X = (0,1)\times (0,1)$ with the Lebesgue measure, and $k\colon X \to \mathbb{R}$ be a measurable non-negative function such that
$$ \int_0^1 k(x,y) dy = 1$$
for every $x \in (0,1)$. My question is: What (more) should we assume on $k$ to guarantee that there exists a unique (up to equivalence class) function $f \in L^1(0,1)$ such that
  $$f(x) = \int_0^1 k(y,x) f(y) dy$$
  for every $x \in (0,1)$ and $\| f \|_{L^1(0,1)} = 1$. Additionally, if we can prove that such unique $f$ exists, when it is strictly positive a.e.? I would be grateful for any reference. Edit: I realized that it makes no sense to ask for a unique solution, since if there is one solution, then all its scalar multiple are also solutions. Hence, I added the requirement that $f \in L^1(0,1)$ and $\| f \|_{L^1(0,1)} = 1$.","['functional-analysis', 'functional-equations']"
