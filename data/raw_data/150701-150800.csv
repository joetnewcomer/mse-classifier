question_id,title,body,tags
2510610,Area element of a spherical surface,"I am trying to find out the area element of a sphere given by the equation: $$r^2= x^2 +y^2+z^2$$ The sphere is centered around the origin of the Cartesian basis vectors $(e_x,e_y,e_z)$. The spherical-polar basis vectors are $(e_r,e_\theta,e_\phi)$ which is related to the cartesian basis vectors as follows: $$e_r=(\sin \theta \cos \phi )e_x+(\sin \theta \sin \phi )e_y+(\cos \theta )e_z \tag 1 $$
$$e_\theta =(\cos \theta \cos \phi )e_x+(\cos \theta \sin \phi )e_y−(\sin \theta )e_z \tag 2$$
$$  e_\phi =(−\sin \phi )e_x+(\cos \phi )e_y \tag 3$$ Reference image: this would give a rough idea of the setup but the basis vectors are set as in this image: . The vector surface element can be given as the cross product of $d\overrightarrow r_1$ and $d\overrightarrow r_2$ i.e. :
$$dS = d\overrightarrow r_1×d\overrightarrow r_2$$ and by taking its magnitude I would get its area. What I think is that the vectors $d\overrightarrow r_1$ and $d\overrightarrow r_2$ can be given by linear combination of the basis vectors $e_r$ $e_\phi$ and $e_\theta$ and therefore:
 $$d\overrightarrow r= a_r e_r + a_\phi e_\phi + a_\theta e_\theta$$ so,  $d\overrightarrow r_1$ and $d\overrightarrow r_2$ can be given as:
 $$d\overrightarrow r_1= 0 e_r + d\phi e_\phi + 0 e_\theta \tag 4$$ and
 $$d\overrightarrow r_2= 0 e_r + 0 e_\phi + d\theta e_\theta \tag 5$$ I can then substitute equation $(1),(2)$ & $(3)$ in $(4)$ & $(5)$ to get $d\overrightarrow r_1$ and $d\overrightarrow r_2$ in terms of the Cartesian basis vectors $(e_x,e_y,e_z)$. My question is: Am I using the correct values of the coefficients $a_r, a_\phi $ and $a_\theta$? Is this approach valid and how/where do I make use of the equation of sphere $r^2= x^2 +y^2+z^2$ in $d\overrightarrow r_1$ and $d\overrightarrow r_2$ so that I can further find their cross product? I would appreciate any help.","['multivariable-calculus', 'linear-algebra', 'calculus']"
2510619,Fundamental Group of a Quotient under Group Action,Let $X$ be a simply connected topological space (therefore with trivial fundamental group) and $G$ a group which acts on $X$ freely. Recently I read that for fundamental group of $X/G$ holds the equation $\pi_1(X/G) =G$. My questions are : how to see it (any sources where it explained detailed; I suppose that it was deduced by a category equivalence $(Cov/X) \leftrightarrow ?$ but I forgot what was the second category). Do there exist weaker conditions (explicitely if $G-$action not freely or if $X$ not simply connected) for which the equation $\pi_1(X/G) =G$ stil holds?,['general-topology']
2510624,Distribution of rounded normally distributed values,"Suppose I draw some values from $X_i \sim {\mathcal{N}( \mu, \, \sigma^2 )} $ and then round them to the nearest integer. 
Are those rounded values then Binomial distributed?","['statistics', 'binomial-distribution', 'normal-distribution']"
2510635,Connectedness of the boundary of a plane convex set,Let $S$ be a closed convex set of $\mathbb{R}^2.$ Let us assume that $S$ is NOT a strip (which has a boundary with two connected components.) How can I prove that $\partial S$ is connected ? It seems pretty evident on a picture but the complete formal proof is quite difficult for me. Thanks for any help. Edit I've just found this article where the proof is given in $\mathbb{R}^3$. I think it can be adapted in $\mathbb{R}^2$. However I am still open for straightforward arguments for plane convex sets.,"['convex-geometry', 'metric-spaces', 'convex-analysis', 'geometry']"
2510702,About the derivative definition for real valued functions,"I have some questions about the definitions of the derivatives of real valued functions which are mostly to make sure I got things correctly. Are the following statements correct? The analogous of partial derivatives of functions $f:\Bbb R^n \to \Bbb R$ for functions $g:\Bbb R \to \Bbb R$ is derivatives, correct? The analogous of the derivative of functions such as $f$ for functions such as $g$ can also be defined and it is something different than the usual derivative of $g$, correct? (If $g(x)=(x-2)^3+3$ then $g'(x)=3(x-2)^2$ which is not a linear map but $Dg=$??) If the above are correct then we call these linear mappings derivatives because they simply have very similar properties to the derivatives of functions defined in $\Bbb R$. I hope an answer alleviates this confusion. Thanks in advance","['derivatives', 'real-analysis', 'partial-derivative', 'calculus']"
2510720,Exercise chapter 2.12 real analysis by E.M. Stein and R.Shakarchi,"Exercise chapter 2.12 real analysis by E.M. Stein and R.Shakarchi : Show that there are $f \in L^1(\mathbb{R^d})$ and a sequence $\{ f_n \}$ with $f_n  \in L^1(\mathbb{R^d})$ such
that $\|f_n-f\|_1 \to 0$ but $f_n \to f$ for no $x$ (pointwise convergence). [Hint: In R, let $f_n:=\chi_ {I_n}$ , where $ I_n$ is an appropriately chosen sequence of intervals
with $m(I_n) \to 0$ .] I find : $$\chi_{[0,1]}, \chi_{[0,1/2]}, \chi_{[1/2,1]},\chi_{[0,1/3]},\chi_{[1/3,2/3]},\chi_{[2/3,1]}, \dots $$ This sequence $\to 0$ in $L^1,$ but is  pointwise nowhere ? Why? Is this true: for any given $x$ there are many infinitly $n$ shuch that $f_n (x)=1$ ? My example is for $d=1$ or $ L^1(\mathbb{R})$ . How we can show for all $d \in\mathbb{N} $ or $L^1(\mathbb{R^d})$ it is true ?","['real-analysis', 'normed-spaces', 'functional-analysis', 'lp-spaces', 'measure-theory']"
2510774,"Why are the group axioms called ""axioms""? [duplicate]","This question already has answers here : Meaning of the word ""axiom"" (4 answers) Closed 6 years ago . My book says: A group is an ordered pair $(G, \cdot)$ where $G$ is a set and $\cdot$ is a binary operation satisfying the following axioms [emphasis mine]: $\vdots$ Why doesn't it say A group is an ordered pair $(G, \cdot)$ where $G$ is a set and $\cdot$ is a binary operation satisfying the following propreties : $\vdots$ As far as I know, axioms are statements which we take for granted; I don't understand why the group axioms are something we have to take for granted; we already know that there exist objects (such as the set of permutations of $3$ objects) which satisfy those propreties. Even if we knew of no objects which satisfy those propreties, then perhaps the axiom should be ""There exists an object which satisfies those propreties"" rather the axioms being the propreties themselves.","['terminology', 'abstract-algebra', 'logic', 'group-theory']"
2510790,Explanation regarding limit of function of two independent variables as sequences.,"Please help!
If $f(x,y)$ is a real valued function of two variables which is continuous .Then
$\lim_{n\to\infty}f(x_{n},y_{n})=f(\lim_{n\to\infty} x_{n},\lim_{n\to\infty}y_{n})$, where $x_{n}$ and $y_{n}$ are any sequences of real numbers whose limits exist.
Is this true in general(Give short proof)?","['multivariable-calculus', 'real-analysis']"
2510795,Closed form for $\sum_{k=0}^\infty \frac{x^k}{(k+1)^4 k!}$,"A graduate student in physics asked me if there was a closed form for the following series, which he had found as the second virial coefficient for a gas in an exponential potential: $$\sum_{k=0}^\infty \frac{x^k}{(k+1)^4 k!}$$ where $x < 0$. This is the generalized hypergeometric function $_4 F_4(1, 1, 1, 1; 2, 2, 2, 2; x)$ by definition. We were skeptical that it could be simplified, as neither the series nor any of its first few derivatives and antiderivatives is the Taylor series of a function that we could recognize. Even if there's not a truly closed form, we're still curious whether it has a formula with special functions other than the generalized hypergeometric.",['sequences-and-series']
2510796,identity for the second fundamental form for cones,"I'm trying to understand the following: Let $C$ be a $n$ dimensional minimal cone in $\mathbb R^{n+1}$ with vertex at $0$ with second fundamental form $h_{ij} = h_{ji}$. (Minimal implies $\sum_i h_{ii} = 0$) Also, let $h_{ijk}$ be the components of the one-form definied by $$\sum_k h_{ijk} \omega_k = \mathrm d h_{ij} - \sum_k h_{ik} \, \omega_{kj} - \sum_k h_{jk} \, \omega_{ki},$$
where all sums are from $1$ to $n$, $\{\omega_1,\cdots,\omega_n\}$ are the dual frames to $\{e_1,\cdots,e_n\}$ and $\{\omega_{ij}\}_{1\leq i,j \leq n}$ are the connection one-forms. Choose a frame $\{e_1,\cdots,e_n\}$ such that $h_{ij}$ is diagonal and $e_n$ is in radial direction (ie. $e_n = x/|x|$). Then we have $h_{ij} = h_{nn} = 0$, $i \neq j$ and $h_{ijn} = - |x|^{-1} \, h_{ij}$, $i,j = 1,\cdots,n$ Now I understand that $h_{nn} = 0$ since in the radial direction, the cone just looks like a straight line. My Question: How do I see the second identity? $$h_{ijn} = - |x|^{-1} \, h_{ij}, \quad i,j = 1,\cdots,n$$ EDIT So my idea was to look at this 
\begin{align}
h_{ijn} & = (\mathrm d h_{ij})(e_n) - \sum_k h_{ik} \, \omega_{kj}(e_n) - \sum_k h_{jk} \, \omega_{ki}(e_n) \\
% & = \partial_n h_{ii} \, \delta_{ij} - \Gamma^i_{nj} \, h_{ii} - \Gamma^j_{ni} \, h_{jj}
\end{align} Any suggestions?","['riemannian-geometry', 'differential-geometry']"
2510854,Norm of K less than or equal to norm of $\phi$,"The following problem has been presented to me: Let $k : [a, b] × [a, b] → \mathbb{F}$ be continuous, and consider the integral operator $K : C_{\mathbb{F}}[a, b] → C_{\mathbb{F}}[a, b]$, defined by
$$ Kf(s) = \int_{a}^{b} k(s, t)f(t) dt \hspace{2cm} (s ∈ [a, b], f ∈ C_{\mathbb{F}}[a, b]) $$
Define $φ: [a, b] → \mathbb{R}$ by
$$ φ(s) = \int_{a}^{b} |k(s, t)| dt (s ∈ [a, b]).$$ Prove that $||K|| \leq ||\phi||_{\infty}=\max_{s \in [a,b]}\{|\phi(s)|\}$. I have trouble with two parts of this problem: What does $||K||$ mean? Is it the supremum norm over both $f$ and $s$, such that $|K(f(s))|$ is greater than any other combination of $f$ and $s$ for $K$? When assuming that that is the case, I found:
$$ |K(f(s))| \leq \int_{a}^{b} |k(s,t)||f(t)| dt $$
But how do I proceed? I need to get the $f(t)$ out of the integral somehow, I need to ""seperate"" the f(t) from the rest to obtain the supremum norm of $\phi$, but how do I do that? I know that $|f(x)| \leq \|f\|_{\infty}$, but when considering the supremum norm for $K$, the supremum norm of $f$ does not come into play, correct? Any and all help would be very much appreciated.","['functional-analysis', 'normed-spaces']"
2510861,Bijection between subsets of a Set is an Equivalence Relation in its Power Set,"I'm working through Halmos's book and find in Section 12 the claim that an equivalence or bijection between subsets of some set $X$ is an equivalence relation in the power set $P(X)$. Intuitively, this is reasonable, and Halmos claims that it's easy to verify. My approach is to note that a bijection, or what Halmos calls an equivalence $\sim$, between subsets $A,B,C$ behaves very much like reflexivity, symmetry and transitivity. I'm having a problem verifying the corresponding equivalence relation in $P(X)$. My hunch is to build a map from $X$ to its power set and show that it preserves the structure of the bijection in $X$ as a relation in $P(X)$. If this is a good approach, do I need to explicitly state the mapping or function? Perhaps building a function is overkill, but it seems that simply observing that $A \sim B$ and $B \sim A$, and the like, in $X$ is similar to $ARB, BRA$ in $P(X)$ does not quite verify this claim.",['elementary-set-theory']
2510862,How to prove this recursive convergence by induction,"I am having difficulties in proving that the following recursive sequence converges: $$a_{n+1}=\frac{2a_{n}+1}{a_{n}+2} :a_{1}=0$$
I have tried proving this by mathematical induction by first proving it is increasing and secondly that it has an upper bound. Proving it is increasing: Since $a_{2}\geq a_{1}$ then we assume that $a_{k+1}\geq a_{k}$ and try to show that it implies $a_{k+2}\geq a_{k+1}$. I try to show this below and get stuck quite quickly: $$a_{k+1}\geq a_{k} \Leftrightarrow 2a_{k+1}+1 \geq 2a_{k}+1$$ As you can see I cannot divide by $a_{n}+2$ to show that $a_{k+1}\geq a_{k} \implies a_{k+2}\geq a_{k+1}$. My question is therefore if I am doing something worng or if this cannot be proven by induction, and if not, which other methods exist to prove this.","['induction', 'calculus', 'analysis']"
2510901,What Motivated the Definition of the Orthogonality of Functions?,"I am curious to know why the orthogonality of two (real) functions $f(x)$, $g(x)$ is given by: $$\int_{-L}^{L} f(x) \,g(x) \; \text{d}x = 0$$ I see a kind of similarity between this definition and the orthogonality of vectors $\vec{v}$, $\vec{w}$ $\in$ $\mathbb{R}^n$, $\,$ viz. $\vec{v} \cdot \vec{w} = v_i \, w_i = 0$. It even makes sense to me that the domain of integration should play an important role in this result. However, I'm at a loss to imagine a) the context that would've prompted such an extension; b) the meaning of orthogonality (i.e. is there any way of thinking of this that is as intuitive as the geometric orthogonality of the vector version, where we can intuitively understand the meaning of orthogonality for vectors in $\mathbb{R}^2$ and $\mathbb{R}^3$ and extend the concept to higher dimensions?). Perhaps the most concise way of asking my question would be is there an alternative way of viewing the definition of orthogonality of functions that is analogous to the geometric definition of the vector dot product (i.e. $\vec{v} \cdot \vec{w} = |\vec{v}|\, |\vec{w}| \cos\theta$)? I looked at this question , but it doesn't really get at what I'm after.","['orthogonality', 'functions', 'inner-products']"
2510940,Can the surface of the closed unit ball be compact,"It is known that in the Banach space with infinite dimension, the closed unit ball is not compact. I am just wondering if it is possible for the surface of the ball, say $\|x\|=1$ can be compact.","['functional-analysis', 'normed-spaces', 'banach-spaces', 'compactness']"
2510959,What is the ratio of $\frac {BK}{BE}$? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Here is my diagram Here is what are given $$|BD| = 2$$ $$|DC| = 5$$ $$|AE| = 2$$ $$|EC| = 3$$ What is the ratio of $\frac {BK}{BE}$? I'm so confused right now.",['geometry']
2510961,Recommendations for Abstract Algebra references with explicit set theory exposition,"As someone that finds themselves a little deficient with set theory, I would like to ask for an abstract algebra resource that really breaks down set theory and its related proofs. Videos, textbooks, or any other form of informational media would be appreciated. If it helps, I am preparing to take my first course in Abstract Algebra in January, and this is an undergrad level class.","['abstract-algebra', 'reference-request', 'elementary-set-theory']"
2510965,Understanding the working of creation and annihilation operators on Fock space,"I have some problem in understanding how exactly the creation and annihilation operators work on a Fock space. We begin with a separable Hilbert space $\mathcal H$ with inner product $\langle,\rangle$ ( why is separable needed? ) and define the full Fock space on $\mathcal H$ by $\mathcal F(\mathcal H)=\huge\oplus\large_{n\geq0}\mathcal H^{\otimes n}$ where $\mathcal H^{\otimes0}=\mathbb C\Omega$, $\Omega$ being an element of $\mathcal H$ with unit norm. For $f\in\mathcal H$, define the left creation operator $l^\star(f)\in \mathcal B(\mathcal F(\mathcal H))$ by $l^*(f)(f_1\otimes...\otimes f_n)=f\otimes f_1\otimes...\otimes f_n$ and $l^*(f)(\Omega)=f$. Similarly define right creation operator. Define left annihilation operator $l(f)$ by $l(f)(f_1\otimes...\otimes f_n)=\langle f,f_1\rangle f_2\otimes...\otimes f_n$ with $l(f)(\Omega)=0$. Similarly define right annihilation operator. These are things I found in a standard text. I have some doubts with the notations mainly. Suppose I look at $x:=a\Omega\oplus b\oplus (c\otimes d)\in\mathcal F(\mathcal H)$. What is the result of $l^*(f)(x)$ and $l(f)(x)$? Here are my guesses: $l^*(f)(x)=0\oplus af\oplus (f\otimes x)\oplus (f\otimes c\otimes d)$, and $l(f)(x)=\langle f,b\rangle\Omega \oplus \langle f,c\rangle d$. Are these correct? Next we come to vacuum expectation: for any $g\in \mathcal B(\mathcal F(\mathcal H))$ define the function $\varphi(g)=\langle \Omega,g(\Omega)\rangle_{0,0}$. Now this $\Omega\in\mathcal H$, so how is the coordinatewise product of inner products defined? Shall we view $\Omega=\Omega\oplus 0\oplus 0\otimes 0\oplus...$ so that in effect, $\langle \Omega,g(\Omega)\rangle_{0,0}$ is the coefficient of $\Omega$ in $g(\Omega)$?","['functional-analysis', 'operator-theory']"
2510980,"How does $F(X_1,X_2)$ change with $\epsilon$ change in $X_1$ when $X_1$ and $X_2$ are entangled?","I have the following function $F$ $$F(X_1,X_2)=\frac{X_1}{X_1+X_2}$$ Where $X_1$ and $X_2$ are functions of a variable $k$. I'm trying to investigate how $F$ changes with a $\epsilon$ change in $X_1$. However, I cannot simply do $$F(X_1+\epsilon,X_2)=\frac{X_1+\epsilon}{X_1+\epsilon+X_2}$$ Since if $X_1$ changes (due to a change in $k$) $X_2$ will also change. Hence, it makes sense (to me) to do the following. Define, $A:=\frac{dX_1}{dk}$ and $B:=\frac{dX_2}{dk}$. Then we have $$F(X_1+\epsilon,X_2)==\frac{X_1+\epsilon}{X_1+\epsilon+X_2+\epsilon(B/A)}$$ For instance if $B=2A$, then a change of $\epsilon$ in $X_1$ will come with a $2\epsilon$ change in $X_2$. Does this make sense?","['multivariable-calculus', 'partial-derivative', 'calculus', 'derivatives']"
2510988,"What is the equivalence relation on the Set $A=\{1,2,3,4,5\}$","I have a set $A=\{1,2,3,4,5\}$, where its partition set is $E=\{\{1,5\}, \{2,3,4\}\}$ I am not sure what kind of equivalence relation gives a rise to above partition.","['relations', 'equivalence-relations', 'set-partition', 'elementary-set-theory']"
2511009,Problem 15 from Herstein's book,Given two sets $S$ and $T$ we declare $S<T$ if there is a mapping of $T$ onto $S$ but no mapping of $S$ onto $T$. Prove that if $S<T$ and $T<U$ then $S<U$. My Proof: Since $S<T$ then $\exists$ onto mapping $f_1:T\to S$ and since $T<U$ then $\exists$ onto mapping $f_2:U\to T$ then mapping $f_1\circ f_2:U\to S$ is also onto mapping. We have done with the first part of proposition. But how to prove that no mapping from $S$ to $U$ is onto. I have tried to do it by contradiction but no results. Can anyone help please with that.,"['elementary-set-theory', 'functions']"
2511044,Computing: $\int_0^{\infty}\frac{dx}{(x+a)((\ln x)^2+\pi^2)}.$,"Assume $a>0$ and evaluate $$\int_0^{\infty}\frac{dx}{(x+a)((\ln x)^2+\pi^2)}.$$ My biggest issue when doing these problems is deciding which closed curve I should try, so if someone could point me in the right direction with at least that I would appreciate it!","['logarithms', 'complex-analysis', 'complex-integration']"
2511095,Lagrange's Theorem and Quadratic Character of $-2$,"Let $p$ be an odd prime. We know that the polynomial $x^{p-1}-1$ splits into linear factors modulo $p$. If $p$ is of the form $4k+1$ then we can write
$$x^{p-1}-1=x^{4k}-1=(x^{2k}+1)(x^{2k}-1).$$
The theorem of Lagrange tells us that any polynomial congruence of degree $n$ mod $p$ has at most $n$ solutions. Hence we can deduce from this factorization that $-1$ is a quadratic residue modulo $p$. Similarly if $p$ is of the form $3k+1$ we can write $4(x^{p-1}-1)=4(x^{3k}-1)=(x^k-1)((2x^{k}+1)^2+3)$ and deduce that $-3$ is a quadratic residue mod $p$. Can we prove in this fashion that $-2$ is a quadratic residue mod $p$ if $p$ is of the form $8k+1$ or $8k+3$? Note that I am interested only in this specific method. I know how to prove this using different means.","['number-theory', 'quadratic-residues', 'elementary-number-theory']"
2511100,Need help understanding Group Actions,"I have a feeling it will be tempting to mark my question as a duplicate, since I know this question is common (help to understand group actions), but my question is actually very specific. I have read about many different definitions, explanations, examples, and watched videos about group actions (including on this website) and had people explain it to me, and I feel like I understand what they are telling me but somehow I have some lingering misunderstanding. But, I finally figured out how exactly I want to ask my question, and it involves me explaining a little about what I do understand. I am going to use the following definition of group action (but I know there are others which I have seen before); this one is from my textbook by Dummit and Foote: $\underline{Definition}$ A group action of a group $G$ on a set $A$ is map from $G$ x $A$ (written as $g \cdot a$, for all $g \in G$ and $a \in A$) satisfying the following properties. $1) \hspace{.2 cm} g_{1} \cdot (g_{2} \cdot a) = (g_{1}g_{2}) \cdot a$ for all $g_{1},g_{2} \in G, a \in A$, and $2) \hspace{.2 cm} 1 \cdot a=a,$ for all $a \in A$ Now, my question involves the different notations between $g_{1} \cdot (g_{2} \cdot a)$ and $g_{1}(g_{2}a)$. I effectively want to know how equating these two expressions is different than proving associativity (and, i know that my last sentence is slightly different than part 1 of the definition of group action). My understanding is that the ""dot"" symbol (""\cdot"" in latex notation) is supposed to mean ""acting"" and is sometimes written in different ways, but ultimately it could be a different operation than the operation defined as the group operation of $G$. ...I do understand that the set $A$ does not need to be a group or a subset of $G$, and so we can't define the $\cdot$ operation as being the same as the group operation of $G$. But, doesn't that make my above definition from Dummit and Foote incomplete or circular, since it doesn't define the operation $\cdot$?  It seems like it's telling me to use the idea of ""acting"" in order to decide whether there is an action...which is circular, right? ...My ultimate question is how exactly part $1)$ of the above definition is different from group associativity, when $\cdot$ is not explicitly defined in most of the problems I have seen?  It this above definition just so general, that it can't be used unless a specific example is given where $\cdot$ is defined? I thought I understood this all before (group actions), and I recall that it helped me to understand that a group operation is an action from $G$ on itself. But my problem is figuring out the operation when $G$ is not acting on itself, when many examples and problems that I see ALSO do not define the operation $\cdot$.  I would think you have to explicitly define $\cdot$ in order to even talk about whether something is a group action. Also...as I was typing all that, I think I learned one more thing...a group action is actually a mapping , correct? It's not an element, or even a binary operation at all (I was thinking of it as a binary operation). And, a map is defined by some combinations of operations of elements (say, conjugation of an element by another element, or multiplication of an element by another element, or whatever any map might be defined as), and so, is this why the operation $\cdot$ that I keep talking about is never explicitly defined in a problem?  Is it because $\cdot$ is referring to the relationships defined by the mapping, which would essentially be the thing I've been referring to as the ""operation"" $\cdot$?",['abstract-algebra']
2511111,Prove that the following map has at least $k-2$ fixed points,"This was an exercise in my algebraic topology class: Let $S^1$ denote the circle and $f: S^1 \to S^1$ is a continuous map with a fixed point $x_0$. The induced homomorphism by $f$ between the fundamental groups is multiplication by $k$: $f_\ast: \Pi_1(S^1, x_0)\to \Pi_1(S^1, x_0): [g] \mapsto k[g].$ Prove that $f$ has at least $k-2$ fixed points (excluding $x_0$). Give an example of such a map with exactly $k-1$ fixed points. For the first question, we got the hint to consider $f$ as a map $f':[0,1] \to S^1$ with $f'(0) = f'(1) = x_0$ and to look at its lifts to the universal cover $\mathbb{R}$. I tried using the lifting correspondence and to find the order of $p^{-1}(x_0)$ where $p$ is the covering map, but got stuck... For the second part, I took the map $f(z) = z^k$ (since we can pick $x_0 = 1$ by applying a rotation of the circle if necessary). This map has the $k-1$th roots of unity as fixed points and the induced homomorphism corresponds to multiplication by $k$. Can anyone give a (second) hint for the first question? Is my answer on the second question correct?","['algebraic-topology', 'general-topology', 'group-theory']"
2511115,$\sum_{n=1}^{\infty}\frac{1}{n^2} <\frac{33}{20}$ using elementary inequalities,There are many ingenious ways for proving $$\zeta(2)=\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{\pi^2}{6} \approx 1.6449$$ Using the inequality $$\frac{1}{n^2} < \frac{1}{n^{2}-\frac{1}{4}} =\frac{1}{n-\frac{1}{2}}-\frac{1}{n+\frac{1}{2}}$$ we can see that $$\sum_{n=2}^{\infty}\frac{1}{n^2} <\frac{2}{3} \Rightarrow \zeta(2)<\frac{5}{3}=1.6666$$ Can we improve upon these bounds using elementary inequalities? Like is it possible to show (of course without assuming $\zeta(2)\approx 1.64449$) that $\zeta(2)<\frac{33}{20}$? If there are much nicer bounds which follow using elementary inequalities I would be happy to see them.,"['real-analysis', 'upper-lower-bounds']"
2511178,coprime vs relatively prime,"Since I am not a native english speaker and really couldn't find it:
Which definition in a ring (of course not integers) belongs to which word: Let $R$ be a ring (commutative with 1) and $a,b \in R$, then $a,b$ are called relatively prime resp. coprime if For any $c \in R: c$ divides $a$ and $b \implies c \in R^\times$ or The ideal generated for $a$ and $b$: $\,(a,b)$ is $R$. Of course these two definitions coincide in PIDs but in other rings they obviously differ. The question is now, which is which? An easy example where it differs would be the polynomial ring over 2 variables $R=K[X,Y]$ for some field $K$ and $a=X, b=Y$. They fullfill 1. but not 2.","['abstract-algebra', 'ring-theory', 'definition']"
2511273,Uncountable measure space with discrete sigma algebra,"Take $X=[0,1]$, equipped with the discrete metric. Then the topology of $X$ is
$\mathcal{T}=2^{[0,1]}$ and the induced Borel $\sigma$-algebra is again $\mathcal{B}=2^{[0,1]}$. What nontrivial probability measures can one define on the measurable space $(X,\mathcal{B})$?","['real-analysis', 'measure-theory']"
2511279,Analysis of a function: Showing that Quantum Mechanics violates relativity,"Consider the Hilbert space $L^2(\mathbb{R}^d)$ and a self-adjoint, positive operator $H$ (Hamiltonian). Let $\psi_t$ be a solution to the Schrödinger equation (with $\psi_0$ the initial condition), then consider the function $$t \mapsto \int_{\mathbb{R}} \overline{f(\boldsymbol{x})} e^{-iHt} \psi_0(\boldsymbol{x}) \ d^dx.$$ Assume that $\psi_o$ is compactly supported in some ball of radius $r>0$ centred at the origin. I am supposed to be observing how quantum mechanics violates special relativity by showing that the support of $\psi_t$ grows faster than the speed of light, but I'm lost as to where to go with this. How to proceed? Thanks.","['quantum-mechanics', 'partial-differential-equations', 'quantum-field-theory', 'special-relativity', 'analysis']"
2511285,Is there a non-constant polynomial such that...,"Is there a non-constant polynomial $p$ with positive coefficients such that function $ x \mapsto p(x^2) - p(x)$ is decreasing on the interval $[1,+\infty)$? This was a question from the last year's math analysis exam. The answer was the following: For $1\le x \lt y$ and $n \in \Bbb N $ we have $y^{2n}-x^{2n} = (y^n+x^n)(y^n-x^n) > y^n - x^n$, so $ x \mapsto p(x^2)-p(x)$ is strictly increasing function on $[1,+\infty)$, for every $n \in \Bbb N$. Because polynomial p has positive coefficients and is not constant, such function does not exist. I have only recently started this class so I have hard times wrapping my head around this. Why is $y^{2n} - x^{2n}$ used to prove that the function is increasing? Why does this ($y^{2n}-x^{2n} > y^n - x^n$) mean that the function is increasing? 
Does it mean that $p(y^{2n})-p(x^{2n}) > p(y)-p(x)$ so that means  $p(y^{2n}) - p(y)- (p(x^{2n})- p(x))>0$ or something like this? If somebody could just explain the answer, it would be much appreciated!","['algebra-precalculus', 'polynomials', 'analysis', 'functions']"
2511343,Lebesgue Decomposition Notation,"The Lebesgue Decomposition Theorem I am using is: Let $\mu$ be a $\sigma$-finite signed measure and let $\nu$ be finite positive measure.  Then, there exist positive measures $\lambda$ and $\rho$ such that: $\nu = \lambda + \rho$ $\lambda \perp \mu$ $\rho \ll \mu$ This makes sense to me.  An exercise I'm given uses a slightly different notation, and I'm not really sure how to interpret it.  It says that the Lebesgue decomposition of $\nu$ can be written:
\begin{align}
d\nu = d\lambda + f d\mu  &  &(1)
\end{align}
This is confusing to me, as I have learned that, given $\eta$ $\sigma$-finite, $\nu$ finite, and $\nu \ll \mu$: \begin{align}
d\nu &= f d\mu & &\Leftrightarrow & \nu(A) = \int_A f d\mu \quad \forall A \in \mathcal{A} & &(2)
\end{align} How do I reconcile this notion of the Radon-Nikodym derivative in (2) with the expression in (1)?  The expression in (1) seems sort of nonsensical to me (what does $d \lambda$ even mean here?), so clearly I'm missing something.","['real-analysis', 'lebesgue-integral', 'measure-theory']"
2511344,Probability of events A and B occurring in order AABB,In a given problem there are two possibilities: The probability of A is 20% The probability of B is 80% How would you calculate the probability of the events occurring in order AABB ? (they are independent) $P(BB|AA) = \frac{P(AA \cap BB)}{P(AA)}$ I think: $P(AA) = (20)(20) $ $P(AA \cap BB) = (20)(20)(80)(80)$ Using the equation I get 64% which seems too high? What am I doing wrong?,"['statistics', 'probability']"
2511346,Linear mapping $\mathbb{N_0}\times\mathbb{N_0}\to \mathbb{N_0}$,"Suppose $L(m,n)=am+bn+c$ mapping $\mathbb{N_0}\times\mathbb{N_0}$ to $\mathbb{N_0}$, not a constant. We see that $L(0,0)=c$ is necessarily integer. The same true of $a=L(1,0)-c$ and $b=L(0,1)-c$. In fact, $a$ and $b$ must be nonnegative integers, not both zero. I have understood everything, except that $a,b\geqslant 0$. Can anyone explain how to derive that?",['elementary-set-theory']
2511357,"Are there any non-discrete subsets of $\mathbb{R}$ for which this ""binary search game"" would terminate?","Let $X \subseteq \mathbb{R}$ be some subset of the real numbers. We start with two players, Player 1 and Player 2, and each is given a point in $X$, Player $x_1$ and Player 2 $x_2$. The players take turns sequentially. On Player 1's turn, Player 1 chooses a point $y \in X$ (the condition that $y \in X$ is crucial) between $x_1^{(n)}$ and $x_2^{(n)}$, i.e. $x_1^{(n)} <y < x_2^{(n)}$, and then forces Player 2 to mover there, i.e. $x_2^{(n+1)} \leftarrow y$. Then on Player 2's turn, Player 2 chooses another point $z \in X$ such that $x_1^{(n)} < z < x_2^{(n+1)}$ and forces Player 1 to move there, i.e. $x_1^{(n+1)} \leftarrow z$. The game ends on Player 1's turn when there does not exist any $x \in X$ such that $x_1^{(n)} < x < x_2^{(n)}$, and Player 1 wins because Player 2 can't force them to move anywhere. Similarly, the game ends on Player 2' turn when there does not exist any $x \in X$ such that $x_1^{(n)} < x < x_2^{(n+1)}$, and Player 2 wins because Player 1 can't force them to move anywhere. Question: Are there any non-discrete subsets of $\mathbb{R}$ for which this ""binary search game"" would terminate for every player strategy used by Player 1 and/or Player 2? In particular, I claim that this would always terminate for $X$ discrete. Given $X$ discrete, the game starts by restricting to the space $[x_1, x_2] \cap X$, which would then be compact and discrete, and thus finite . Then termination would follow from a modification of the proof that regular binary search terminates. Also, it seems like in order for this to terminate, we need that $[x_1, x_2] \cap X$ is finite (please correct me if I am wrong, I have yet to attempt a rigorous proof). Thus if such a non-discrete $X$ exists, it would have to be such that $[x_1, x_2] \cap X$ is finite for every pair $(x_1, x_2) \in \mathbb{R}$ . Note: When I say discrete, I mean in the topological sense that all points are open sets (the discrete topology), not that the set is countable (e.g. the rational numbers are a countable subset of the reals but not a discrete subset).","['general-topology', 'examples-counterexamples', 'computer-science', 'discrete-mathematics']"
2511369,Integration by parts and non-absolutely continuous distributions,"Let $x\in [a,b]$ be a real random variable with distribution $H$ that is not absolutely continuous (w.r.t Lebesgue measure). 
I saw this in a paper:
$$
\int_a^b xH(dx) = b-\int_a^bH(x)dx.
$$
I get it is integration by parts but I am not sure why it should work. Is there some implicit assumption on $H$ at work here or does this always hold? 
Edit: I think there may be an implicit assumption that $H$ is a Borel measure.","['lebesgue-measure', 'continuity', 'integration', 'lebesgue-integral', 'measure-theory']"
2511378,Showing that $p(x):=\inf\left\{a>0|\frac{1}{a}x\in K\right\}$ is a norm under certain circumstances,"Let $(X,\|\cdot\|_X)$ be a normed space and $K\subset X$ an open, convex set with $0\in K$ . Let $p:X\rightarrow\mathbb{R}$ be the Minkowski-functional of $K$ defined by: $$p(x):=\inf\left\{a>0 | \frac{1}{a}x\in K\right\}$$ i) Show that: If $K$ is symmetric $(-K=K)$ and bounded, then $p(x)$ is a norm in $X$ , which is equivalent to $\|\cdot\|_X$ Do I have to show, that $p(x)$ is well defined?
I thought about the following:
If $x=0$ : $$p(0)=\inf\{a>0|0\in K\}$$ $$=\inf\{a>0\}$$ $$=0$$ The positivity for $x\neq0$ is obvious.
Then I found a theorem about the minkowski-functional: Let $K$ be convex, open with $0\in K$ and $p(x)$ defined like above, then: i) $p$ is sublinear, ii) There is a $M>0 \forall x\in X: 0\le p(x)\le M\|x\|_X$ iii) $K=\{x\in X|p(x)<1\}$ With the sublinearity of $p$ , the homogenity and the triangle inequality are clear. My problem here is: Where do I need the information, that $K$ is bounded? Now I have to show that $p(x)$ is equivalent to $\|\cdot\|_X$ . Which means: There are $c,C>0$ , so that $$c\|x\|_X\le p(x)\le C\|x\|_X$$ The theorem gives us a $C$ with $p(x)\le C\|x\|_X$ but I have no idea how to find the $c$ for the lower bound. Could someone help me with these problems?","['functional-analysis', 'normed-spaces']"
2511386,How to show $F_1/F_2$ is bounded?,"If $P(x,y)$ and $Q(x,y)$ are homogenous polynomials of degree $n$ and
  $m$ respectively, then the function $f$ is defined as
  $f(x,y)=\frac{P(x,y)}{Q(x,y)}$ [when $Q(x,y)\neq 0$] and 0 [when
  $Q(x,y)=0$]. This type of a function will be continuous at $(0,0)$ if
  $n>m$. I was thinking of proving it using $P(x,y)=x^nF_1(y/x)$ and $P(x,y)=x^mF_2(y/x)$ and writing $f(x,y)$ as $x^{n-m}(F_1/F_2)$. If $F_1$ by $F_2$ is bounded then we can say that the function $f(x,y)$ is continuous at $(0,0)$ since the double limit $(x,y)\to (0,0)$ exists there. Or perhaps even using polar coordinates might be useful. Any hints how to show $F_1/F_2$ is bounded?","['multivariable-calculus', 'continuity', 'limits']"
2511394,Problem computing $\int_{-\infty}^\infty \frac{\tan^{-1}(x)}{x}dx$ using contour integral.,"I am trying to solve $$\int_{-\infty}^\infty \frac{\tan^{-1}x}{x}\ dx$$ using a contour integral. My Work: Define a contour $C$ such that: Now we have $$\int_{C}\frac{\tan^{-1}x}{x}\ dx=\int_{-\infty}^\infty \frac{\tan^{-1}x}{x}\ dx+\int_{\text{Arc}}\frac{\tan^{-1}x}{x}\ dx$$
Now parametizing the integral over the arc: $$\int_{\text{Arc}}\frac{\tan^{-1}x}{x}\ dx=\lim_{R\to \infty}\int_{0}^\pi \frac{\tan^{-1}(Re^{i\theta})}{Re^{i\theta}}iRe^{i\theta}\ d\theta=\lim_{R\to \infty} i\int_{0}^\pi \tan^{-1}(Re^{i\theta})\ d\theta=\frac{i\pi^2}{2}$$
We also note that the entire contour integral does not contain any poles, so it is $0$. However this is where I run into a problem because that implies:$$\int_{-\infty}^\infty \frac{\tan^{-1}x}{x}\ dx=-\frac{i\pi^2}{2}$$
Which is obviously not true. If anyone can point out why my approach does not work or where I went wrong that would be great. Any help is appreciated.","['complex-analysis', 'integration', 'contour-integration', 'calculus']"
2511421,Correct probability calculation for Minesweeper,"When I play Minesweeper, every now and then I reach a point where I have to make a guess. I then try to calculate the probability for every option to be a mine in order to choose the safest move. But sometimes I find that different calculations result in different probabilities, which means I'm doing it wrong in some sense, and this is what I want to clarify. Below is an example of such a dilemma. Notice the green mark in the upper-left region. I have two choices next to the 4-mark, each with a probability of $\frac{1}{2}$. On the other hand, from the 5-mark point of view, we need to select 2 squares out of 3, suggesting a $\frac{2}{3}$ probability for each square. We can also follow a third calculation, by starting from the 2-mark on the rightmost square on this remaining ""island"": if the upper square is a mine, it can be easily seen that the upper square of the 4-mark must also be a mine; if not- it can be shown that both 4-mark squares have equal probability to be mines. This implies that the upper 4-mark square has probability of $\frac{3}{4}$ to be a mine - again a contradiction. A more ""desperate"" attempt would be to say that all three ""calculation trajectories"" are equally likely, thus we need to add the calculated probabilities and sum them up with a factor of $\frac{1}{3}$. But that is quite awkward, and I'm sure there is more solid reasoning here, but I wasn't able to prove it myself. So... what is the correct way to calculate the probability? As a last remark - I assume here that the remaining island is large enough such that no further meaningful information can be extracted from the remaining squares, such as the number of remaining mines, or a direct enumeration of all possible mine distributions. Thanks, and I hope you find this intriguing as I do. :)","['combinatorics', 'recreational-mathematics', 'probability']"
2511427,Finding ordinary and singular points,"I just have a quick question regarding the following ODE: $$P(x) y'' + Q(x) y' + R(x)=0.$$ To find the singular points of this ODE, is it correct to set $P(x)$ to $0$ and just solve for the $x$ terms? Or is there another method for finding the singular points. I understand you can take the limit of both $Q(x)$ and $R(x)$ over $P(x)$ respectively to check whether a given point is singular/ordinary. But my main question is, if I am asked to specifically FIND the point, is setting $P(x)$ equal to $0$ and solving for the x values the correct method? How about for finding ordinary points? Is there a method to find the point, or do I just look at $P(x)$ and think to myself, what values of $x$ can I plug into $P(x)$ such that $P(x)$ will not equal $0$? Thank you!",['ordinary-differential-equations']
2511452,How do I calculate the *signed* area of a triangle in 3D space?,"The signed area of a triangle is often given simply as \begin{equation}
  A\left( \Delta \right) = \frac{1}{2}\left|\begin{matrix}
    x_{0} & y_{0} & 1  \\
    x_{1} & y_{1} & 1  \\
    x_{2} & y_{2} & 1
\end{matrix}\right|
\end{equation} for $V_i = \left(x_i,y_i\right)$. This, of course, only works for vertices located on a 2D plane. What is the analog for the signed area of a triangle in 3D space, having $x,y,z$ coordinates?","['triangles', 'geometry']"
2511467,What is a gradient matrix?,"I know what the gradient of a function is, but I have this problem which seems to have something else in mind. The problem reads Write the oscillator equation $y''-\cos y=0$ as a system of first-order equations with $x_1=y$ and $x_2=y'$ and find the gradient matrix $\nabla f$, and compute its eigenvalue as a function of $x$.  Draw some typical trajectories in phase space and indicate where you expect divergence, based on eigenvalue analysis. Does this just mean to turn the system into a matrix like $$\begin{pmatrix}x_1\\x_2\end{pmatrix}'=\begin{pmatrix}0&1\\0&0\end{pmatrix}\begin{pmatrix}x_1\\x_2\end{pmatrix}+\begin{pmatrix}0\\-\cos x_1\end{pmatrix}$$ Would the coefficient matrix be the gradient matrix?  Or would it be the fundamental matrix solution?  I've tried googling the term and nothing seems to be what this problem is referring to.","['matrices', 'ordinary-differential-equations']"
2511494,How to expand $(x^{n-1}+\cdots+x+1)^2$ (nicely),"sorry if this is a basic question but I am trying to show the following expansion holds over $\mathbb{Z}$: $(x^{n-1}+\cdots+x+1)^2=x^{2n-2}+2x^{2n-3}+\cdots+(n-1)x^n+nx^{n-1}+(n-1)x^{n-2}+\cdots+2x+1$. Now I can show this in by sheer brute force, but it wasn't nice and certainly wasn't pretty. So I am just wondering if there are any snazzy ways to show this? If it helps, I am assuming $x^m=1$ for some $m>n-1$.","['algebra-precalculus', 'polynomials']"
2511542,Meaning of Observability in LTI systems - how to actually find states.,"I am working on LTI system study and I am somewhat confused about what a system being observable actually means from the perspective of the application. For LTI systems observability proofs usually involve taking the output vector $y(t)$ and $n$ derivatives and solving a system of linear equations using the fact the observability matrix is full rank. My question is, how are the derivatives of the output vector supposed to be found? It seems like this theorem is purely of theoretical significance since I don't see a way for someone to compute the output vectors derivatives from measurements.","['dynamical-systems', 'systems-of-equations', 'control-theory', 'mathematical-modeling', 'ordinary-differential-equations']"
2511545,Show that $\operatorname E\left(\frac{\widehat{E} \widehat{E}^T}{n-q}\right) = \Sigma$,"We have $Y = X\beta+E,$ $X\in\mathbb R^{n\times q}$ and has rank $q,$ $n\gg q,$ $\beta\in\mathbb R^{q\times1},$ $E$ is a random vector taking values in $\mathbb R^n,$ $\operatorname{E}(E)=0\in\mathbb R^n,$ $\operatorname{var}(E) = \Sigma,$ so $\Sigma$ is a symmetric $n\times n$ real matrix. The variance $\Sigma$ is strictly positive-definite. As usual the hat matrix is the $n\times n$ matrix $H = X(X^T X)^{-1} X^T,$ of rank $q,$ which is the matrix of the orthogonal projection onto the column space of $X.$ And the vector of observable residuals is $\widehat E = (I-H)Y.$ The question is how to show that
$$\operatorname E \left(\frac{\widehat{E} \widehat{E}^T}{n-q} \right) = \Sigma$$ My Process: I know the $\operatorname E(\frac{1}{n-q}) = \frac{1}{n-q}$ So I am left with $\operatorname E\left[\widehat{E} \widehat{E}^T \right]$ This simplifies to $\operatorname E[Y^T (I-H) Y]$ I don't believe I need to expand the hat-matrix $H$. What I want to do at this point is introduce $\Sigma$ somehow. The only thing I can think of is to introduce something like $\Sigma^{1/2} \Sigma^{-1/2}$ or $\Sigma \Sigma^{-1}$ but I'm not sure where or how to introduce this. EDIT: Sorry for lack of clarity. This is a MULTIVARIATE STATISTICS problem. I've been so focused on just this that I forgot that other math exists :'D $\Sigma$ is the Covariance Matrix. $n$ is the number of observations. $q$ independent variables. $\widehat{E}$ is the estimator for the error matrix.","['statistics', 'linear-algebra']"
2511554,Computing the determinant of a large matrix?,"How would I go about computing the determinant of large matrices, such as $6 \times 6$. I believe that I need to use multilinear maps, but I am not sure how I can go about computing the determinant in a nice and efficient way. Can anyone show me how I can determine the determinant of the matrix below in a simple and efficient way? \begin{pmatrix}
0 &  0&  1&  1& 1 & 1\\ 
1 & 0 & 0 &  0&  0& 1\\ 
1 & 0 &  1& 1 & 1 &1 \\ 
0 & 1 & 1 & 1 & 0 &1 \\ 
0 & 1 &  0& 1 &  0& 0\\ 
0 &  0&  1& 0 &  0& 0
\end{pmatrix}","['matrices', 'linear-algebra', 'determinant']"
2511555,Prove that an open subfunctor of a scheme is itself a scheme,"There is an exercise for functor-of-points approach of algebraic geometry.
Our definition of scheme is: A scheme is a functor such that (1) it is local (2) it has an open affine cover. I think I have solved (2), but I am worring that if I have got (1) correct: The following is a possible solution: I am not so sure whether it is correct. The main reason is that it seems that I have proved every subfunctor of a local functor is local, it is obviously not the case. And I have not use the openess of the subfunctor here. But I checked it several times and cannot find where is the problem. Thanks for telling me what is wrong here! If something is wrong here, how to correct it? Or if it is correct and there is just something that I have not fully understand, please point it out. Many thanks!","['schemes', 'algebraic-geometry', 'functors', 'proof-verification']"
2511571,"Why is $\int_0^\infty f(x) \, dx = \frac{1}{2}\int_0^\infty f(x) \,dx+ \frac{f(\frac{1}{x})}{x^2} \, dx $ true?",I have found a method of integration on this website: https://brilliant.org/wiki/integration-tricks/ But I'm not sure how the U-sub offered in the article gets to the conclusion it states. I'm not too sure where to start. My knowledge of U-sub didn't seem to work with achieving their answer. Any help is appreciated.,"['integration', 'calculus']"
2511623,Alternating dice roll game,"Imagine we played a game using a dice. I start, then you get a turn, then me again and so on. The first person to get a 6 wins. What is my probability of winning if I start? Now consider we did the same but with a coin. Without calculation, would this probability increase or decrease? Here's what I was thinking: On average, you get 6 every $6$ rolls. So on average, the $6$th person to roll will win. So you going first doesn't change the probability (three rolls each), which must therefore be $\frac 12$. Using this logic for a two-sided die, we'd get the same probability: $\frac 12$. These results are extremely unintuitive to me and and almost certainly incorrect. I'd like to know (1) the flaw in the reasoning, and (2) how to get the correct solution.","['problem-solving', 'probability', 'dice']"
2511648,Eigenvalue problem with symmetric matrix with diagonal diagonal blocks,"I would like to efficiently compute all the eigenvalues and eigenvectors of a real matrix A for which the structure is as follows: $A =\begin{bmatrix}
D_1 & C\\C^T & D_2
\end{bmatrix}$ , in which $D_1$ is a $n_1 \times n_1$ diagonal matrix with strictly positive elements and $D_2$ is a $n_2 \times n_2$ diagonal matrix with strictly positive elements, and where $C$ is a fully-populated $n_1 \times n_2$ matrix with full rank. I would like to know if there is a way to exploit the fact that $D_1$ and $D_2$ are diagonal, in order to speed-up the calculation. For now, I am directly using MATLAB routine eig applied to A: [V,D] = eig(A) and it isn't faster than for a fully-populated real symmetric positive-definite matrix of the same size, even for the case $n_1 \ll n_2$ for which $C$ is very thin. Are you aware of any way to exploit such a structure? Thanks very much, Olivier","['eigenvalues-eigenvectors', 'numerical-linear-algebra', 'matrices', 'symmetric-matrices', 'linear-algebra']"
2511655,"If |$G|=p^3$ and $|Z(G)|\ge p^2$, prove that G is abelian","If |$G|=p^3$ and $|Z(G)|\ge p^2$, prove that G is abelian As my idea If $G/Z(G)$ is cyclic then $G$ is abelian since $|Z(G)|\ge p^2$  then $|Z(G)|=p^2,p^3$ then $G/Z(G)=p,1$ so cyclic then G is abelian is  am correct","['abstract-algebra', 'group-theory']"
2511663,Do Mobius transformations preserve Hausdorff dimension?,"Do Mobius transformations preserve Hausdorff dimension? This may be related to: Is there a measure invariant with respect to the Möbius transformation? I believe the answer is yes, but I want some intuition behind it, and eventually, a proof. Since a Mobius transformation is a combination of translations, dilations, rotations, and inversions, it suffices to show that each of these mappings preserves the Hausdorff dimension. Motivation: This image and this image are the same (up to an inversion/Mobius transformation). In both images, the ""dust"" or ""residual set"" left between the tangent circles has the same Hausdorff dimension $ \delta \approx 1.30568$.","['hausdorff-measure', 'complex-analysis', 'real-analysis', 'mobius-transformation']"
2511674,Constant value in definition of Big-O notation,"Most definitions of Big-O notation I've seen look like the following one: $f(n) = O(g(n))$ means that there are positive constants $c$ and $k$,
  such that $f(n) \leq c \cdot g(n)$ for all $ n \geq k$. I can't understand why saying that some $c$ has to exist and why we can't simply say that $f(n) \leq g(n)$, instead of $f(n) \leq c \cdot g(n)$. Overall, if one function dominates another (in this case the dominating one is $g(n)$), there's always a point above which $g(n)$ surpasses $f(x)$. So my question is - is it really required to mention existence of constant $c$? If yes, why is that? If no, why do people do that?","['computational-complexity', 'asymptotics', 'functions']"
2511677,Every closed subspace of a compact space is compact using finite intersection property,"My book states that the fact that ""every closed subspace of a compact space is compact"" is a consequence of the following theorem: A Hausdorff space is compact if and only if every family of closed subsets of $X$ which has the finite intersection property has non-empty intersection. For completeness: A family $\mathcal{F} = {F_s}_{s\in S}$ has the finite intersection property if $\mathcal{F} \ne \emptyset$ and the intersection of any finite number of the $F_s$ is non-empty. I'm having trouble seeing how to use this theorem. Should we apply it to the whole space (which is indeed Hausdorff since being Hausdorff is a requirement for compactness in my book), or whether to apply it to the closed set in question.","['general-topology', 'compactness']"
2511699,"Is there a good upper bound for $(x-1)^n-x^n$ for $x\ge 1$ and $n=2,3,4,...$?","For an positive integer $n\ge 2$, is there a good upper bound for $(x-1)^n-x^n$ for $x> 1$? Revised question: Is it less than $-\log(x-1)$ for all $x>1$? By Binomial theorem, $(x-1)^n-x^n=\sum_{k=1}^n\binom{n}{k}x^{n-k}(-1)^k$. Or by Mean Value Theorem, it is equal to $-n\xi^{n-1}$ for some $\xi\in(x-1,x)$. I wonder if there are good estimate of this function?","['combinatorics', 'analysis']"
2511706,What's wrong with this argument for finding Aut($\mathbb{Z_p} \times \mathbb{Z_p}$)?,"I happen to know that Aut($\mathbb{Z_p} \times \mathbb{Z_p}$) $\simeq$ $GL_2(p)$, and so has order $p(p^2-1)(p-1)$. Which means something is wrong with the following argument: An automorphism $\phi$ is determined by its value on the two generating elements $(1,0)
$ and $(0,1)$ of $\mathbb{Z_p} \times \mathbb{Z_p}$. These must be sent to elements of the same order, $p$. In fact every nonzero element of $\mathbb{Z_p} \times \mathbb{Z_p}$ has order $p$, so there are $p^2-1$ choices to which each generator must go.
Therefore the number of automorphisms is $(p^2-1)^2$. So I've overcounted by $(p^2-1)(p-1)$; I imagine some of the homomorphisms in my argument were not distinct, or not isomorphisms. is there a way to easily rectify for the overcounting? Or do we throw the whole argument away?","['finite-groups', 'permutations', 'group-theory']"
2511728,"Given $f$ is absolutely integrable on $[1, \infty)$, prove that $\lim_{n \to \infty} \int_1^\infty f(x^n) dx = 0$",We know that $\int_1^\infty |f| dx < \infty $. I believe we also know that $\lim_{x \to \infty} f(x) = 0$. I'm unsure how to proceed further but am guessing this involves some application of Dirichlet's Test and integration by parts?,"['improper-integrals', 'real-analysis', 'integration']"
2511745,"Cardinality ""arithmetic"" on infinite sets","I've been working on a problem and when I looked around the site I couldn't seem to find much that looked like what I was trying to solve. My problem is as follows: Let $A$ be an infinite set and denote card($A$) by $\alpha$. If $B$ is an infinite set, denote card($B$) by $\beta$. Define $\alpha\beta$ to be card($A\times B$). Let $B'$ be a set of disjoint from $A$ such that card($B$)=card($B'$). Define $\alpha+\beta$ to be card ($A\cup B'$). Denote by $B^A$ the set of all maps of $A$ into $B$, and denote card($B^A$) by $\beta^{\alpha}$. Let $C$ be an infinite set and abbreviate card($C$) by $\gamma$. Prove (a). $\alpha(\beta+\gamma)=\alpha\beta+\alpha\gamma$. Attempt: Let $f:A\times(B\cup C')\rightarrow(A\times B)\cup(A\times C')$ be defined by $f(a,x)=(a,x)$ where card$(C')=$card$(C)$ and $C'$ is disjoint from $B$. Suppose that $f(a,x)=f(b,y)$. Then $(a,x)=(b,y)$ so $f$ is injective. Now, if $(a,x)\in(A\times B)\cup(A\times C)$ then $f(a,x)=(a,x)$ so $f$ is surjective. Therefore, $f$ is bijective and the claim is true. My thought process is that $A\times(B\cup C')=(A\times B)\cup(A\times C')$ so the identity map is a bijection which gives me that the cardinalities are the same. (b) $\alpha\beta=\beta\alpha$ Attempt: Let $f:A\times B\rightarrow B\times A$ be defined by $f(a,b)=(b,a)$. Now, suppose that $f(a,b)=f(c,d)$. Then $(b,a)=(d,c)$ so $(a,b)=(c,d)$ and we conclude that $f$ is injective. Now, if $(b,a)\in B\times A$ then $f(a,b)=(b,a)$ so $f$ is surjective. Therefore, $f$ is bijective so the claim is true. This one seems fairly clear to me although I will gladly accept any criticism. (c) $\alpha^{\beta+\gamma}=\alpha^{\beta}\alpha^{\gamma}$. Attempt: Let $\phi:A^{B\cup C'}\rightarrow A^B\times A^{C'}$ be defined by $\phi(\alpha)=(\beta,\delta)$ where $\alpha:B\cup C'\rightarrow A$, $\beta:B\rightarrow A$, and $\delta:C'\rightarrow A$. Furthermore, $(\beta,\delta)(x)=\beta(x)$ if $x\in B$ or $\delta(x)$ if $x\in C'$. Suppose that $\phi(\alpha)(x)=\phi(\sigma)(x)$ where $\phi(\sigma)=(\rho,\tau)$. Then $(\beta,\delta)(x)=(\rho,\tau)(x)$ so if $x\in B$ then $\beta(x)=\rho(x)$ otherwise if $x\in C'$ then $\delta(x)=\tau(x)$. Thus, $\beta=\rho$ and $\delta=\tau$ so $\alpha=\sigma$ and we conclude that $\phi$ is injective. Now, if $(\beta,\delta)\in A^B\times A^{C'}$ then $\phi(\alpha)(x)=(\beta,delta)(x)$ so $\phi$ is surjective. Therefore, the claim is true. I admit I am a bit lost on this one. I've tried to define a function using the given definitions but am confused on having a product of two sets of maps and how they relate. Any help would be appreciated.","['cardinals', 'elementary-set-theory']"
2511778,Lyapunov's inequality in Probability,"I have a question about the proof of the inequality. The well known result stats Let $Z$ be a RV and let $0<s<t$. Then $$E(|Z|^s)^{1/s} \leq E(|Z|^t)^{1/t}$$ The proof follows almost immediately from the Holder Inequality $$E(|XY|)\leq E(|X|^p)^{1/p} E(|Y|^q)^{1/q}$$
for $1<p,q$ such that $1/p+1/q =1$.
Taking $Z=|X|^s, Y=1, p= t/s $ we can derive the Lyapunov's inequality. even though it's not mentioned , I think one must assume that $E(Z) =E(|X|^s)<\infty$ to apply the inequality . However, I'm not assuming that in my hypothesis. My question is, what happens if $E(Z)=\infty$? Does the inequality still holds? In other words, $E(|X|^t)=\infty$ as well? (If $1<s<t $ it follows easyly but I don't know how to argue in other cases)","['probability-theory', 'integral-inequality', 'expectation', 'random-variables']"
2511793,show this inequality $\left(\sum_{i=1}^{n}x_{i}+n\right)^n\ge \left(\prod_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}\frac{1}{x_{i}}+n\right)^n$,"Let $x_{i}\ge 1$,show that
$$\left(\sum_{i=1}^{n}x_{i}+n\right)^n\ge \left(\prod_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}\dfrac{1}{x_{i}}+n\right)^n$$ or 
$$\left(\dfrac{\sum_{i=1}^{n}x_{i}+n}{\sum_{i=1}^{n}\dfrac{1}{x_{i}}+n}\right)^n\ge \prod_{i=1}^{n}x_{i}$$
and it seem use AM-GM inequality?
$$\sum_{i=1}^{n}x_{i}\ge n\sqrt[n]{x_{1}x_{2}\cdots x_{n}}$$
$$\sum_{i=1}^{n}\dfrac{1}{x_{i}}\ge \dfrac{n}{\sqrt[n]{x_{1}x_{2}\cdots x_{n}}}$$ let $\sqrt[n]{x_{1}x_{2}\cdots x_{n}}=t$,since 
$$\Longleftrightarrow \left(\dfrac{t+1}{\frac{1}{t}+1}\right)^n\ge t^n$$But I can't it","['multivariable-calculus', 'radicals', 'inequality', 'calculus']"
2511799,Example for not having closest polynomial of $f$ in some infinite dimensional vector space,"I have a question about some subspace of $C[0,1]$ in my text book introducing.( $C[0,1]$ is all the rea-valued continuous function space on [0.1]). here is contents in my textbooks Consider the subspace $S=\{h \in C[0,1] : h(0)=0\}$ and $T=\{h \in S : \int_{0}^{1}h(x) =0 \}$ Let $g(x)=x$ and consider the distance of $g$ to $T$. Note that $g(0)=0$ but $$\int_{0}^{1}g(x)dx=\frac{1}{2}$$ suppose that $h\in T$, and compute that $$\frac{1}{2}=\int_{0}^{1}(g(x)-h(x))dx \le \int_{0}^{1}||g-h||_\infty dx =||g-h||_\infty$$ if $||g-h||_\infty=\frac{1}{2}$ then this inequality must be an equality. this can occur only if 
  * $$g(x)-h(x)=||g-h||_\infty=\frac{1}{2}$$***** this implies that $h(x)=x-\frac{1}{2}.$ Note that $h$ does not lie in $T$ because $h(0)=0$ So the distance $\frac{1}{2}$ is not attained I am curious about how to happen ""$||g-h||_\infty = \frac{1}{2}$ $\Rightarrow$ $g(x)-h(x)=||g-h||_\infty=\frac{1}{2}$"" on the above text.the converse is trivial, but origin proposition need to be proved. please give me a hint about proposition","['functional-analysis', 'analysis', 'approximation']"
2511836,Compact set in complex plane,"If $(a, b) \subset \mathbb{R} \subset \mathbb{C}$, and it can be shown that $(a, b)$ is an open subset of $\mathbb{R} $ yet it is not an open subset of $\mathbb{C} $.If $[a, b] \subset \mathbb{R}$ is compact, how would you show $[a, b]$ is compact in $\mathbb{C}$? Whats the difference in the proof?",['general-topology']
2511866,Multivariable uniform convergence and differentiation,"There is a well known theorem on the relationship between uniform convergence of univariate functions and differentiation.  Quoting Theorem 7.17 from Rudin 1976, Principles of Mathematical Analysis : Theorem: Suppose $\{f_n\}$ is a sequence of functions, differentiable on $[a, b]$ and such that $\{f_n(x_0)\}$ converges for some point $x_0$ on $[a,b]$. If $\{f_n'\}$ converges uniformly on $[a,b]$, then $\{f_n\}$ converges uniformly on $[a,b]$ to a function $f$, and $f'(x) = \lim_{n\rightarrow\infty} f_n'(x)$. My question is, does a generalisation of this theorem exist for sequences of functions $f_n: \mathbb{R}^n \rightarrow \mathbb{R}?$ For example, can $f'_n$ be replaced by $\nabla f_n$ and $[a,b]$ be replaced by a compact set?","['multivariable-calculus', 'real-analysis']"
2511894,"If $X$ is uniform on $(-1,1)$, find $g(x)$, so that $Y = g(X)$ has pdf $f_Y (y) = 2e^{-2y}$","If $X$ is uniformly distributed in $(-1, 1)$, find $g(x)$, so that the random variable $Y = g(X)$ may
  have the density function $f_Y (y) = 2e^{-2y}, \  y > 0.$ Suppose $g:(-1,1) \rightarrow \mathbb{R}_{>0}$ be a monotonically increasing function such that $Y=g(X)$. Let $a \in (-1,1)$ and $b=g(a)$. Therefore we have $$P(Y \leq b)=P(X \leq a) $$ $P(Y \leq b) =\int_0^b 2e^{-2y} dy = 1-e^{-2b} $ and $P(X \leq a) = \frac{a+1}{2}$ (X is uniform). Therefore $1-e^{-2b}=\frac{a+1}{2}$. Hence $b = -\frac{1}{2} \log (\frac{1-a}{2})$. Therefore $$ g(x) = -\frac{1}{2} \log (\frac{1-x}{2}). $$ Is this correct? Help!","['statistics', 'random-variables', 'probability-distributions']"
2511943,"leaf-labelled unordered, rooted binary trees and perfect matchings","While playing with findstat.org, I noticed the following coincidence: The number of leaf labelled unordered rooted binary trees with $n+1$ leaves $\{1,\dots,n+1\}$ , with the leaf labelled $1$ at distance $k$ to the root ( http://findstat.org/St001041 ) equals the number of perfect matchings of $\{1,\dots,2n\}$ with $k$ terminal closers ( http://findstat.org/St000838 ). The distribution of these numbers is given at http://oeis.org/A102625 , and I expect that a computational proof would not be very hard to find. However, I am interested in a bijective proof. UPDATE: belated, I should mention that I eventually found a bijective proof but only have a rather brief writeup currently.","['combinatorics', 'recreational-mathematics', 'alternative-proof']"
2511975,How to (quickly) determine whether a function is totally differentiable,"I'm studying up for an exam, and an example question is: Show that the function $f(x,y):=(x^2y-\frac13y^3, \; \frac13 x^3-xy^2)$ is fully differentiable and determine its derivative. Now I'm able to solve this with about a page of tedious writing, but since it's an exam question I figured there must be a quick(er) way to do this. Hence this question. My solution would be to calculate partial derivatives and plugging them (pairwise) in to the limit of the derivative so that we may show both the first and second element of the $f(\textbf{v})$ vector to go to zero. (Using the definition saying that if
$$\lim_{h\rightarrow0}\frac{|f(x+h)-f(x)-A(h)|}{|h|}=0,$$
then $f'(x) = A$ with $A$ a linear transformation.","['multivariable-calculus', 'real-analysis']"
2512008,Meaning of the differential of a map between manifolds as an approximation,"The differential $df_x$ of a map $f$ between two Euclidean spaces is the linear approximation which satisfies $f(x + v) = df_x(v) + o(|v|)$. The meaning of the word ""approxiamtion"" is clear here. I'm struggling to get what is the meaning of the differential of a map between two smooth manifolds as an approximation. In general there is no concept of a distance between two points in a manifold or a length of a vector in a tangent space of a manifold. Then how could we say about approximation precisely? In what sense does the tangent space at a point of a manifold locally approximate the manifold? In what sense does the differential of a map between smooth manifolds approximate the map?","['manifolds', 'differential-geometry']"
2512014,How to intepret cotangent laplacian?,I have the above slide in my lecture notes. The question is why do people define such things? How is it useful?,"['computational-geometry', 'differential-geometry']"
2512021,Show that partial pivoting leads to an $LU$ decomposition of $PA$.,"Exercise: show that for every non-singular matrix $A$, partial pivoting leads to an $LU$ decomposition of $PA$ so: $PA = LU$. I have the following theorems I can use: Theorem 1: Assume that the coefficient matrix $A\in\mathbb{R}^{n\times n}$ of the linear system $Au = f$ is non-singular and that it can be brought to its upper triangular form $U$ using $n-1$ row operations without scaling and without interchanges in such a way that pivot elements $a_{k,k}^{(k-1)}$ for $k = 1,...,n-1$ are non-zero. Then the $n-1$ Gaussian transformation $M_k$ for $k=1, ..., n-1$ exist such that
  \begin{equation}
\begin{split}
M_{n-1}M_{n-2}...M_1A = U\Leftrightarrow  A &= (M_{n-1}...M_1)^{-1}U\\
\Leftrightarrow A &= LU
\end{split}
\end{equation} and Theorem 2: If Gaussian elimination with partial pivoting is used to compute the upper triangularization
  \begin{equation}
M_{n-1}P_{n-1}...M_1P_1A = U,
\end{equation} 
  then 
  \begin{equation}
PA = LU,
\end{equation}
  where $P = P_{n-1}...P_1$ and $L$ is a unit lower triangular matrix with $\left|l_{ij}\right| \leq1$. What I think I should do: From Theorem 2 I know that if the $LU$ decomposition exists, Gaussian elimination with partial pivoting will lead to an $LU$ decomposition of $PA$. So I need to show that every non-singular matrix $A$ has a $LU$ decomposition. Theorem 1 states that if a non-singular matrix $A$ can be brought to its upper triangular form $U$ then an $LU$ decomposition exists. So I think that I need to show that every non-singular matrix A can be brought to its upper triangular form $U$. Question: How do I solve this exercise? Am I on the right track? Thanks in advance!","['matrices', 'matrix-decomposition', 'linear-algebra']"
2512025,"What are the possible prime factors of $3^n+2$ , where $n$ is a positive integer?","What are the possible prime factors of $3^n+2$, where $n$ is a positive integer ? It is clear that a prime $p$ for which neither $-2$ nor $-6$ is a quadratic residue modulo $p$, cannot be a prime factor of $3^n+2$, so the primes congruent to $13$ or $23$ modulo $24$ can be ruled out. But is there a simple necessary and sufficient condition whether $p$ can be a prime factor of $3^n+2$ ?","['number-theory', 'prime-numbers', 'divisibility']"
2512088,Distance between line and point in homogeneous coordinates?,"I can't seem to find any definitive answer to this question. Assuming I have a 2D line in homogeneous coordinates defined by $$l = (a, b, c)^T$$ and a point in 2D space, $$x = (x, y)^T,$$
how do I find the perpendicular distance between the two. I know it has something to do with the dot product, but I can't remember exactly how it works. It would be nice if you could walk me through why your answer is correct.","['homogeneous-spaces', 'projective-space', 'linear-algebra', 'geometry']"
2512218,Borel Functional Calculus of Commuting Operators,"let $H$ be a Hilbert space, $T_1,T_2 \in L(H)$ two self-adjoint operators such that $T_1T_2=T_2T_1 \in L(H)$, i.e. they commute and their product is continuous. $E_1,E_2$ are their assigned spectral measures. Let $f: \sigma(T_1)\rightarrow \mathbb{K}, g: \sigma(T_2)\rightarrow \mathbb{K}$ be borel-measurable functions.
Show that the operators also commute: $\psi_1(f)\psi_2(g)=\psi_2(g)\psi_1(f), \ i=1,2$ and $ \psi_i$ being the borel functional calculus with respect to $T_i$. An Idea would be: Let $f=\chi_A,\ g=\chi_B$  be the indicatorfunctions for A,B each a borelset in the spectrum of $T_1,T_2$. 
$G:= \psi_1(f)\psi_2(g)$, with that I inspect $\left<Gx,x\right>=\left<x,G^*x\right>$. If I can show from there that $G=G^*$, one could approximate arbitrary measurable and bounded $f,g$ with a linear combination of simple functions. Would this be a good way?","['functional-analysis', 'functional-calculus', 'spectral-theory']"
2512223,Does set $S=(-1)^n$ have any limit points?,"This set can have only $2$ values $1$ for even $n$ and $-1$ for odd $n$.
Thus, I don't think it should have any limit points being a finite set. But notes from a reputed institute show that both these ($1$ and $-1$) are limit points.
What am I not understanding?
Same case for $S={\cos(n\pi /2)/nEn}$ - it shows $D(s)=\{-1, 0, 1\}$","['real-analysis', 'limits']"
2512236,Number of Lattice Points on a Circle,"I have made the following conjecture:the number of lattice points on a circle with equation $x^2 +y^2 = n$, where $n$ is an integer with a prime factorization containing only primes in the form of $4k+1$, is four times the number of divisors of $n$. So, for example, consider the circle $x^2 +y^2 = 65$. In this case, $65 = 1 \times 5 \times 13$ and the divisors of 65 are $1,5,13,65$. Thus, by my conjecture, the number of lattice points on this circle is $4 \times 4$ which is 16 lattice points. I do not know how to go about this proof, and any help would be appreciated.","['number-theory', 'integer-lattices', 'prime-numbers', 'circles']"
2512255,What is a good argument that for well-ordered subsets it is more reasonable to take initial segments rather than arbitrary subsets?,"Typically when studying some class of mathematical objects, we study them together with some kind of corresponding notions of substructure and with ""reasonably-behaved"" maps between them. (Such as groups, subgroups, homomorphisms; vector spaces, subspaces, linear maps; topological spaces, subspaces, continuous maps.) This is especially natural from the viewpoint of category theory. Usually when dealing with well-ordered sets , initial segments are more suitable than arbitrary subsets. (For example, comparison of ordinals is usually defined in terms of isomorphisms with initial segment. When we have a chain of well-ordered sets and we want to get again a well-ordered sets - which might happen in some arguments based on Zorn Lemma - this does not work for ordering by inclusion, but it does work if we use initial segments.) After gaining some experience with well-orders, it is possible to accept the fact that this is a reasonable thing to do, simply because it works. However, is there some insight into definition of well-ordering which would lead more directly to the fact that initial segments are ""the right version"" of substructure in this case? This seems to me to be related to the question what is a reasonable ""morphism"" between two well-ordered sets. I am not sure what would I choose as morphism if I wanted to work with category of well-ordered sets - maybe a reasonable choice could be something which behaves reasonably well w.r.t. successsors and suprema? (I.e., something similar to normal functions ; only if we want to include also the case that the map is not injective, we would probably have to modify this notion slightly.) TL:DR; Is there some perspective from which initial segments (rather than arbitrary subsets) of well-ordered sets can be immediately seen as the most natural version of substructure/subobject for well-ordered sets?","['order-theory', 'well-orders', 'elementary-set-theory']"
2512268,How to rigorously prove that $S^1 \wedge S^1 \cong S^2$?,"Defining $S^1\wedge S^1$ as $S^1\times S^1 \Big/ S^1\vee S^1$ and seeing the torus as a square with opposite sides identified, $S^1\vee S^1$ is just the boundary of the rectangle, so $S^1\wedge S^1$ would be the quotient of the disk $D^2$ with its boundary, that is
$$S^1\wedge S^1 = D^2/\partial D^2 \cong S^2$$
This explanation is okay to me, but I'd like to see a complete proof of this fact, and I don't know where to start. If it's possible I would like some hint on how to proceed rather than a whole answer. Thank you in advance.","['algebraic-topology', 'general-topology', 'quotient-spaces']"
2512277,How to calculate $\lim_{x\to\infty}{\left(1+\frac{1}{3x}\right)}^{4x}$?,The limit $$\lim_{x\to\infty}{\displaystyle \left(1+\frac{1}{3x}\right)}^{4x}$$ apparently has a value of $e^{4/3}$. I can't see why this would be the case.,"['exponential-function', 'limits']"
2512297,Expectation of Brownian motion increments under a permutation.,"Tl;dr, here is my question. Let
$$\gamma(x):=\frac{e^{-x^2/2}}{\sqrt{2\pi}}$$
denote the standard Gaussian density, fix an integer $n\in\mathbb N$, and let $(B_t)_{t\geq0}$ denote a standard Brownian motion. Given any permutation $\sigma\in S_n$ on $n$ symbols, does it hold that
\begin{align*}
E[\gamma(B_{\sigma(1)})\gamma(B_{\sigma(2)}-B_{\sigma(1)})\cdots\gamma(B_{\sigma(n)}-B_{\sigma(n-1)})]
&\leq E[\gamma(B_1)\gamma(B_2-B_1)\cdots\gamma(B_n-B_{n-1})]\\
&=E[\gamma(B_1)]^n?\tag{1}
\end{align*} My reason for asking is the comparison of the two following computations. One the one hand, consider the integral
$$I=\int_{\mathbb R^n}\left(\sum_{\sigma\in S_n}\gamma(x_{\sigma(1)})\gamma(x_{\sigma(2)}-x_{\sigma(1)})\cdots\gamma(x_{\sigma(n)}-x_{\sigma(n-1)})\right)^2~d x_1\cdots dx_n,$$
where $S_n$ denotes the symmetric group of permutations on $n$ symbols. By Jensen's inequality, we have
$$I\leq n!\sum_{\sigma\in S_n}\int_{\mathbb R^n}\gamma(x_{\sigma(1)})^2\gamma(x_{\sigma(2)}-x_{\sigma(1)})^2\cdots\gamma(x_{\sigma(n)}-x_{\sigma(n-1)})^2~d x_1\cdots dx_n,$$
which, up to relabeling the $x_i$ variables, gives
$$I\leq (n!)^2\int_{\mathbb R^n}\gamma(x_{1})^2\gamma(x_{2}-x_{1})^2\cdots\gamma(x_{n}-x_{n-1})^2~d x_1\cdots dx_n.$$
Then,
\begin{align*}
\int_{\mathbb R^n}\gamma(x_{1})^2\gamma(x_{2}-x_{1})^2\cdots\gamma(x_{n}-x_{n-1})^2~d x_1\cdots dx_n
&=E[\gamma(B_1)\gamma(B_2-B_1)\cdots\gamma(B_n-B_{n-1})]\\
&=E[\gamma(B_1)]^n,\end{align*}
so we conclude that
$$I\leq(n!)^2E[\gamma(B_1)].\tag{2}$$ On the other hand,  if we expand the square in $I$ directly, up to a relabeling of indices, we get
\begin{align*}
I&=n!\sum_{\sigma\in S_n}\int_{\mathbb R^n}\prod_{k=1}^n\gamma(x_k-x_{k-1})\gamma(x_{\sigma(k)}-x_{\sigma(k-1)})~d x_1\cdots dx_n\\
&=n!\sum_{\sigma\in S_n}E[\gamma(B_{\sigma(1)})\gamma(B_{\sigma(2)}-B_{\sigma(1)})\cdots\gamma(B_{\sigma(n)}-B_{\sigma(n-1)})].
\end{align*}
Comparing this with inequality $(2)$, this suggests that $(1)$ might be true, but I have no probabilistic intuition/explanation for why this should or should not be the case.","['probability-theory', 'brownian-motion']"
2512308,"$[0,1]$ cannot be partititioned into two sets with given properties","Show that the interval $[0,1]$ cannot be partitioned into two disjoint sets $A$ and $B$ such that $B=A+a$ for some real number $a$. My proof: Suppose by contradiction that $[0,1]=A\sqcup B$ where $B=A+a$ and WLOG let $a>0$. It's easy to verify that $0\in A$ and $1-a\in A$. Also it's easy to prove that $A\subset [0,1-a]$ and $B=A+a\subset [a,1]$. First case: If $1-a<a$ then $a>\frac{1}{2}$. It is easy to show that in this case $\frac{1}{2}\notin [0,1-a]$ and $\frac{1}{2}\notin [a,1]$ $\Rightarrow$ $1/2\notin A$ and $1/2\notin B$ but $1/2\in [0,1]$. This is a contradiction. Second case: If $1-a=a$ then $a=\frac{1}{2}$. In this case $A\subset [0,\frac{1}{2}]$ and $B\subset [\frac{1}{2},1]$. Since $0\in A$ then $\frac{1}{2}\in B$. Since $1\in B$ then $1-\frac{1}{2}\in A$. Thus $\frac{1}{2}\in A\cap B$. Contradiction. Third case: If $1-a>a$ then $a<\frac{1}{2}$. Let $I=[a,1-a]$ and it can be shown that $I\cap A\neq \varnothing$ and $I\cap B\neq \varnothing$. I have stuck here and not able to derive contradiction. Can anyone please give a hint how to overcome this obstacle?",['elementary-set-theory']
2512311,solution of the integral equation,"let $y(x)$ be the solution of the integral equation $$ y(x) = x-\int _0 ^x xt^2y(t)dt, x>0$$ Then value of $y(\sqrt 2)?$","['integral-equations', 'ordinary-differential-equations']"
2512316,Existance of the derivative of a function of several variables whose domain is closed.,"Define $f: [a,b]\to \Bbb R$, we can talk about the derivative of $f$ at the boundary points $a$ and $b$. I am interested in the case when $f$ is a function of several variables. Define $f:E\to \Bbb R^m$ with $E\subset\Bbb R^n$. One can talk about the derivative of $f$ only if $E$ is open. I have no idea why this is the case. The answer here: https://math.stackexchange.com/a/504567/135775 , says that if $E$ is not open the the Jacobian  of $f$ need not be unique. Can someone give an explanation of why one cannot define the derivative if $E$ is closed or maybe an example where the Jacobian is not unique.","['derivatives', 'real-analysis', 'calculus', 'multivariable-calculus', 'jacobian']"
2512326,"How to calculate $\int_0^\pi x\,\cos^4x\, dx$","Assume that $$\int_{0}^\pi x\,f(\sin(x))dx=\frac{\pi}2 \int_{0}^\pi f(\sin(x))dx$$ and use it to calculate $$\int_{0}^\pi x\,\cos^{4}(x)\, dx$$ Can anyone help me with that? I proved the identity but I am stuck with the rest.","['integration', 'definite-integrals', 'trigonometric-integrals', 'analysis']"
2512339,"Solve for $a,b,c,d \in \Bbb R$, given that $a^2+b^2+c^2+d^2-ab-bc-cd-d+\frac 25 =0$","Today, I came across an equation in practice mock-test of my coaching institute, aiming for engineering entrance examination (The course for the test wasn't topic-specific, it was a test of complete high school mathematics). It was having four variables and only one equation. While analyzing my test paper, this is the only problem I (and my friends too) couldn't figure out even after giving this problem several hours. So I came here for some help. Question : Solve for $a,b,c,d \in \Bbb R$, given that $$a^2+b^2+c^2+d^2-ab-bc-cd-d+\frac 25 =0$$ Since only one equation is given, there must be involvement of making of perfect squares, such that they all add up to $0$. Thus, resulting in few more equations. But how to? I tried a lot of things, such as making $(a-b)^2 $ by adding the missing terms and subtracting again, but got no success. Thanks!","['algebra-precalculus', 'square-numbers', 'polynomials', 'systems-of-equations']"
2512364,Intuitive way to understand Gauss-Bonnet Theorem,"Let $M$ be a closed(=compact, without boundary) 2-dimensional Riemannian manifold. Let $K:M\to \mathbb{R}$ , $dA$ , and $\chi(A)$ be the (Gaussian) curvature, volume element, and Euler characteristic of $M$ . Then The Gauss-Bonnet theorem says $$\int_M KdA = 2\pi \chi(M).$$ There are quite a lot of intuitive explanations for Levi-Civita connection, exterior derivatives and other concepts in Differential and Riemannian geometry. But I couldn't find any resource with an intuitive explanation of Gauss-Bonnet Theorem. The proof I am aware of (given in John Lee's Riemannian Manifolds) seems like a trick of notation and usage of Stoke's theorem. I tried to come up with a more visual way of thinking about this theorem using Gauss map but so far wasn't able to do so. What is the intuition behind this theorem? How did Gauss come up with it himself? (If it was him) So far for me, it seems that the way one would come up with such a theorem is to notice the pattern for several shapes, write down the formula and try to prove it using the available techniques. Yet, It seems unsatisfying. All references are welcome. Thank you.","['intuition', 'riemannian-geometry', 'differential-geometry', 'curvature']"
2512424,Groups with given automorphism groups,"It is an easy exercise to show that all finite groups with at least three elements have at least one non-trivial automorphism; in other words, there are - up to isomorphism - only finitely many finite groups $G$ such that $\text{Aut}(G)=1$ (to be exact, just two: $1$ and $C_2$ ). Is an analogous statement true for all finite groups? I.e., given a finite group $A$ , are there - again up to isomorphism - only finitely many groups $G$ with $\text{Aut}(G)\cong A$ ? If yes, is there an upper bound on the number of such groups $G$ depending on a property of $A$ (e.g. its order)? And if not, which groups arise as counterexamples? And finally, what does the situation look like for infinite groups $G$ with a given finite automorphism group? And what if infinite automorphism groups $A$ are considered?","['group-theory', 'automorphism-group']"
2512438,Domain with positive-existential subset that is not diophantine,"Let $R$ be a ring. Call a subset $A$ of $R$ diophantine if it is of the form
$$
\lbrace x \in R \mid \exists z_1, \ldots, z_n \in R : f(x, z_1, \ldots, z_n) = 0 \rbrace
$$
for some $n \in \mathbb{N}$ and some polynomial $f \in \mathbb{Z}[X, Z_1, \ldots, Z_n]$. Call $A$ positive-existential if it is a finite union of finite intersections of diophantine sets. If $R$ is a domain, then clearly a finite union of diophantine sets is again diophantine. If additionally the fraction field of $R$ does not contain the algebraic closure of its prime field (i.e. $\mathbb{Q}$ if the characteristic is $0$ or $\mathbb{F}_p$ if the characteristic is $p > 0$) then one can use the existence of an anisotropic form in two variables over this field to show that an intersection of diophantine sets is again diophantine. Hence in this case, all positive-existential sets are diophantine. For a counterexample for the implication positive-existential $\Rightarrow$ diophantine in general, it suffices to note that, for a ring $R$, the diophantine subsets of the product ring $R \times R$ are precisely those of the form $A \times A$ for some diophantine subset $A$ of $R$. This makes it easy to construct positive-existential subsets which are not diophantine by taking the union of two such sets. For example, let $R$ be a field, set
$$
A = \lbrace x \in R \mid \exists y \in R : x \cdot y = 1 \rbrace = R\setminus \lbrace 0 \rbrace ,
$$
then $A \times A \cup \lbrace (0,0) \rbrace$ is positive-existential but not diophantine. Question : can we find a domain $R$ in which not all positive-existential subsets are already diophantine, i.e. there exists two diophantine subsets of which the intersection is not diophantine? Could $R$ even be a field? I have been trying with $\mathbb{C}[T]$ and $\mathbb{C}(T)$ but have had no luck.","['affine-varieties', 'first-order-logic', 'logic', 'algebraic-geometry']"
2512463,Find (recursive formulas for) $\sum_{k=1}^n\lfloor k\varphi\rfloor^2$ and/or $\sum_{k=1}^nk\lfloor k\varphi\rfloor$,"I need to find $S_2(n)=\sum_{k=1}^n\lfloor k\varphi\rfloor^2$, where $\varphi$ is golden ratio. I use the approach used for $S(\alpha,n)=\sum_{k=1}^n\lfloor k\alpha\rfloor$, where $\alpha$ is arbitrary irrational, that can be found here (see Recursive formula in the 1st answer): Solve summation $\sum_{i=1}^n \lfloor e\cdot i \rfloor $ First, let $S_2(\alpha,n)=\sum_{k=1}^n\lfloor k\alpha\rfloor^2$, where $\alpha$ is an arbitrary irrational. We then need to find $S_2(\varphi,n)$. For the first step in the approach mentioned above we have $\alpha=\varphi\in(1,2)$, hence we use Case 2 in the recursive formulas:
$$
S_2(\varphi,n)=\frac{(N-1)N(2N-1)}{6}-S_2(\beta(\varphi),n_1),
$$
where $N=\lceil n\varphi\rceil$, $\beta(\varphi)=\frac{\varphi}{\varphi-1}=\varphi^2$, and $n_1=\lfloor\frac{N}{\beta(\varphi)}\rfloor=\lfloor\frac{N}{\varphi^2}\rfloor$. At the second step, we need to evaluate $S_2(\varphi^2,n_1)$. Only thing is that $\varphi^2>2$, so we have Case 1 in the recursive formulas, which looks like this:
$$
S_2(\varphi^2,n_1)=\sum_{k=1}^{n_1}\lfloor k\varphi^2\rfloor^2=\sum_{k=1}^{n_1}\lfloor k(\varphi+1)\rfloor^2=\sum_{k=1}^{n_1}(k + \lfloor k\varphi\rfloor)^2=
$$
$$
=\sum_{k=1}^{n_1}(k^2+2k\lfloor k\varphi\rfloor+\lfloor k\varphi\rfloor^2)=\sum_{k=1}^{n_1}k^2+2\sum_{k=1}^{n_1}k\lfloor k\varphi\rfloor+S_2(\varphi,n_1).
$$
I've no idea what to do with the middle sum. I tried to bypass Case 1-situation by exploiting the fact that $\lfloor n\varphi^2\rfloor=\lfloor\lfloor n\varphi\rfloor\varphi\rfloor+1=A_{n,1}+1$, where $A_{n,1}$ can be found in the $n$-th row and 1st column of the Wythoff array. The Case-1 sum would then look like this
$$
S_2(\varphi^2,n_1)=\sum_{k=1}^{n_1}\lfloor k\varphi^2\rfloor^2=\sum_{k=1}^{n_1}(A_{n,1}+1)^2,
$$
but I've no idea how to calculate numbers from the Wythoff array fast, let alone their squares. This may somehow be connected to the Zeckendorf representation, but I don't see how. Any ideas? Thanks.","['summation', 'golden-ratio', 'sequences-and-series', 'ceiling-and-floor-functions']"
2512481,Chi-square goodness of fit test proof,"I understand the classcial $\chi^2$ ""goodness of fit"" test used in Statistics, in which we compute $\sum_{i=1}^n \frac{(O_i - E_i)^2}{E_i}$ and, by comparing this quantity to a value found in a table of $\chi^2$ law (with a given risk $\alpha = 5\%$ for example), we decide if we should or not accept the hypothesis that the sample is likely to be an observation or not of a given distribution. But I haven't found a good precise proof online yet, that shows that it's not only a good ""recipe"", but also has a strict proof, using probability theory (I know it exists, but I haven't found one yet). Do you know a good detailed proof?","['statistics', 'chi-squared']"
2512485,Why is $e^{a\pi i}\neq (-1)^a$?,Why are the following statements incorrect? I have trouble understanding my mistake. $$e^{a\cdot \pi i} = e^{\pi i^a} = (-1)^a $$ $$e^{a\cdot 2\pi i} = e^{2\pi i^a} = (1)^a =1 $$ Any clues would be appreciated!,"['complex-analysis', 'exponential-function', 'complex-numbers']"
2512509,Integral of softmax (i.e multi-variate sigmoid) over hyper-cube,"Let $n$ and $k$ be a positive integers and $\mathbf{b}_1,\ldots,\mathbf{b}_k \in  \mathbb R^n$ (with $\mathbf{b}_l \ne 0$ for at least one $l$), $c_1,\ldots,c_k \in \mathbb R$. What does the following integral $$I(\mathbf{b}_1,\ldots,\mathbf{b}_k,c_1,\ldots,c_k): =\int_{[0,1]^n}\frac{1}{1 + \sum_{l=1}^k\exp(\mathbf{x}^T\mathbf{b}_l + c_l)}d \mathbf{x}$$ evaluate to ? Observations The 1-dimensional binary case (i.e $n=k=1$) is trivial. Indeed, from
$$\int \frac{1}{1 + \exp(-x)}dx = \ln (1 + \exp(x)) + \text{constant},
$$ one gets
$$
\begin{split}
I(b, c) &= \int_0^1 \frac{1}{1 + \exp(bx + c)}dx
= -\frac{1}{b} \int_{-c}^{-b-c} \frac{1}{1 + \exp(-z)}dz \\
&= \frac{1}{b}\ln\left(\frac{1 + \exp(-c)}{1 + \exp(-b-c)}\right),
\end{split}
$$
where we've used the change of variable: $-z = bx + c$. In particular, one has $$ I(-1,0) = \int_0^1 \frac{1}{1 + \exp(-x)}dx = \ln\left(\frac{1 + e}{2}\right)
\approx 0.62$$","['sampling', 'multivariable-calculus', 'integration', 'jacobian', 'change-of-variable']"
2512517,What's the difference between a fingerprint and a hash?,"My understanding is that both a fingerprint and a hash are functions that take as input some arbitrarily long bitstring, and output a bitstring of a fixed size.  The Wikipedia page for Hash Functions says: Hash functions are related to (and often confused with) ... fingerprints But try as I may, I can't find any sources that mention the difference between the two.  Do you know what the difference between a hash function and a fingerprinting function  is?","['hash-function', 'functions', 'computer-science']"
2512551,Compute/approximate percentile from percentiles of smaller time windows,"As the title says, I am interested in computing or approximating a percentile (e.g. 95%) for a large time interval (e.g., 1h) based on the percentiles of smaller time windows (30s). I am aware that the correct way would be to compute the percentile from scratch starting from the raw data. However, the data set is very large and would take way too much time (a 30s window can have 100k-100M data points). To simplify things, assume that the distribution for all the time intervals are the same but the constants might be somewhat different. For example, values in a window follow a Poisson distribution but the parameters of the distribution might vary somewhat between windows. To put it in more visual terms, the shape of all the distributions is the same but there might be a ""scaling"" factor that changes. I do not require a mathematical proof but some insight on what would be a reasonable way to proceed. Right now I am computing the median value of all the percentiles I am interested in. Could I improve on this?","['statistics', 'percentile']"
2512647,Is $\mathbb{N}\cup \big\{\sqrt{2}\big\}$ an uncountable set? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Is $\mathbb{N}\cup \big\{\sqrt{2}\big\}$ an uncountable set? I think it is.",['elementary-set-theory']
2512656,Is $det (A-A^T) = 0$ for a $n \times n$ matrix? ($n$ odd),"I have the following problem:
We have a $n \times n$ matrix where $n$ is odd. Is $det (A-A^T) = 0$? I don't know if this is true or not, I met this while working at the following problem: ""Let $A$ be a $n \times n$ matrix where $AA^T=I_n$. Prove that $tr A \leq n$ and for $n$ odd, $det (A^2-I_n)=0$."" (Romanian Olympiad, 2007).
The inequality was easy, using the known inequality $tr(AB) \cdot tr(A^TB^T) \leq tr(AA^T) \cdot tr(BB^T)$. At the second point I thought about the following relations:
$(A-I_n)(A^T+I_n)=A-A^T$ and
$(A+I_n)(A^T-I_n)=A^T-A$. So we have that $det (A^2-I_n) \cdot det((A^T)^2-I_n)=-(det(A-A^T))^2$ and if what I say is true, than the problem is a step closer to be solved. For $n=1$ and $n=3$ it is true.","['matrices', 'linear-algebra', 'determinant']"
2512709,Can we have a power density but not a natural density?,"For $M \subset \mathbb{N}$ (in this post I follow the convention $\min \mathbb{N} = 1$) and $\alpha \in [0,1]$ define $$S_{M,\alpha}(x) = \sum_{\substack{n\in M \\ n \leqslant x}} \frac{1}{n^{\alpha}}$$ on $[1,+\infty)$. For sets $A \subset B \subset \mathbb{N}$ with $S_{B,\alpha}(x) \to +\infty$ as $x\to +\infty$, we define the upper $\alpha$-density of $A$ in $B$ as $$\overline{D}_{\alpha}(A;B) = \limsup_{x\to +\infty} \frac{S_{A,\alpha}(x)}{S_{B,\alpha}(x)}.$$ The lower $\alpha$-density $\underline{D}_{\alpha}(A;B)$ is defined analogously (using $\liminf$ instead of $\limsup$). We say that $A$ has an $\alpha$-density in $B$ if $\overline{D}_{\alpha}(A;B) = \underline{D}_{\alpha}(A;B)$, and then denote that value with $D_{\alpha}(A;B)$. For $\alpha = 0$ we have the familiar natural density (also called asymptotic density) and for $\alpha = 1$ the logarithmic density. The cases $\alpha \in (0,1)$ shall be called ""power densities"". Via summation by parts, it is straightforward to show that when $B$ is substantial, i.e. $\lim\limits_{x\to +\infty} S_{B,1}(x) = +\infty$, $$\underline{D}_{\alpha}(A;B) \leqslant \underline{D}_{\beta}(A;B) \leqslant \overline{D}_{\beta}(A;B) \leqslant \overline{D}_{\alpha}(A;B)$$ for all $A \subset B$ and $0 \leqslant \alpha < \beta \leqslant 1$. So if $A$ has an $\alpha$-density in $B$ for some $\alpha < 1$, then $D_{\beta}(A;B)$ exists for all $\beta \in [\alpha,1]$ and all these densities coincide. And for example the set $A_1$ of positive integers whose first (decimal) digit is $1$ has logarithmic density $\frac{\log 2}{\log 10}$ in $\mathbb{N}$, but $\underline{D}_{\alpha}(A_1;\mathbb{N}) < \overline{D}_{\alpha}(A_1;\mathbb{N})$ for all $\alpha \in [0,1)$. So the question arises whether there is a substantial set $B$ and a subset $A$ such that $A$ has no natural density in $B$, but $D_{\alpha}(A;B)$ exists for some $\alpha \in (0,1)$. Again via summation by parts it is easy to see that for well-behaved $B$ - e.g. $\underline{D}_0(B;\mathbb{N}) > 0$, or such that $\frac{S_{B,0}(x)}{x}$ tends to $0$ in a nice fashion, like for the set of primes - the existence of $D_{\alpha}(A;B)$ for an $\alpha < 1$ implies the existence of $D_0(A;B)$. But I did not find a proof of that for arbitrary substantial $B$, nor did I manage to find an example where $D_{\alpha}(A;B)$ exists for some $\alpha \in (0,1)$ and $D_0(A;B)$ doesn't exist. So: Do there exist $A \subset B \subset \mathbb{N}$ such that $B$ is substantial, $A$ doesn't have a natural density in $B$ and $D_{\alpha}(A;B)$ exists for an $\alpha \in (0,1)$?","['real-analysis', 'analytic-number-theory', 'summation-by-parts']"
2512730,Solving Algebraic Riccati Equation by using Newton-Raphson's method?,Assume that we have a function called $$F(X) = 0$$ That function can be written as the Algebraic Riccati Equation: $$F(X) = A^T X + XA - XBR^{-1}B^TX + Q = 0$$ Where the solution to the equation is matrix $X$. To do a Newton-Raphson solveing method. I need to use this equation: $$X_{i+1} = X_{i} - F'(X_{i})^{-1}F(X_{i})$$ Where $F'(X)$ is the jacobian function of $F(X)$ and $X_{i+1}$ is going the be the solution to the Algebraic Riccati Equation. My question is: How can I find the derivative of $F(X)$ ? Can I just assume that the derivative is: $$F'(X) = A^T + A - XBR^{-1}B^T$$ ?,"['derivatives', 'optimal-control', 'matrices', 'newton-raphson', 'numerical-methods']"
2512768,Normal distribution is not single-parameter exponential family,"I want to show that the family of normal distributions is not a single-parameter exponential family, i.e. there aren't functions $h,g,\eta,T$ such that $$h(x)g(\mu,\sigma^2)\exp(\eta(\mu,\sigma^2) T(x)) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)$$ for all $x\in\mathbb R$ and $(\mu,\sigma^2)\in\mathbb R\times\mathbb R_{>0}$. I tried fixing one argument, in order to get some information about the other functions. For example $\mu=0$ looks much simpler but it didn't lead me anywhere. Can anybody give me a hint?","['probability-theory', 'statistics', 'probability-distributions']"
2512787,Fourier Transform of Exponential Integral combination $e^{-x}\operatorname{Ei}(x)-e^{x}\operatorname{Ei}(-x)$,"When trying to solve an ODE using Fourier methods, I met the following solution: $$\omega \left(e^{-\omega}\operatorname{Ei}(\omega) - e^{ax}\operatorname{Ei}(-\omega)\right)$$ Which I have to Fourier transform back. I attempted using my ' dirty physics ' approach: Use the definition of the Exponential Integral to write, $$ \operatorname{Ei}(\omega) = -\int_{-\omega}^\infty \text{d}t~ t^{-1} e^{-t} = e^\omega \int_0^\infty \, \text{d}u\frac{e^{-u}}{\omega-u}$$ And thus we can write the combination in the brackets as $$e^{-\omega}\operatorname{Ei}(\omega)-e^{\omega}\operatorname{Ei}(-\omega) = 2\omega\int_0^\infty \text{d}u \, \frac{e^{-u}}{\omega^2-u^2} $$ Do the Fourier transform for the above by changing the order of integration, \begin{align}
\int_{-\infty}^\infty \frac{\text{d}\omega}{2\pi} ~e^{i\omega x}\left[ e^{-\omega} \operatorname{Ei}(\omega)-e^\omega \operatorname{Ei}(-\omega)\right] &= \int_{-\infty}^\infty \frac{\text{d}\omega}{2\pi}~e^{i\omega x} 2\omega\int_0^\infty \text{d}u \, \frac{e^{-u}}{\omega^2-u^2}\\
&= \int_0^\infty \text{d}u~ e^{-u} \int_{-\infty}^\infty \frac{\text{d}\omega}{2\pi}~\frac{2\omega}{\omega^2-u^2}e^{i\omega x}\\
&=i\operatorname{sgn}(x)\int_{0}^{\infty} \text{d}u~ e^{-u} \cos(xu)\\
&=i~\frac{\operatorname{sgn}(x)}{1+x^2}
\end{align} Use the usual property of the Fourier Transform that $\mathcal{F}[i\omega f(\omega)] = \partial_x f(x)$ , \begin{align}
\int_{-\infty}^{\infty}\frac{\text{d}\omega}{2\pi} ~e^{i\omega x} \omega\left[ e^{-\omega}\operatorname{Ei}(\omega)-e^\omega \operatorname{Ei}(-\omega)\right] = \partial_x \left(\frac{\operatorname{sgn}(x)}{1+x^2}\right) = \frac{2\delta(x)}{1+x^2}-\frac{2|x|}{(1+x^2)^2}
\end{align} However if you evaluate the above in Mathematica, I get simply $2\delta(x)$ as the result, which is the first piece of the above. Does anyone know where I am messing it up and would be able to help with a rigorous treatment?","['special-functions', 'integration', 'fourier-transform']"
2512830,Arithmetic mean is to addition as Harmonic mean is to ...?,"Take $n$ real numbers $x_1,\ldots,x_n.$ The Arithmetic mean $A_n=\frac{1}{n}(x_1+\ldots+x_n)$ is the answer to the question: ""Which number, when added up $n$ times, is equal to the sum of the $x_1,\ldots,x_n?$"" If the numbers are non-negative, then the Geometric mean $G_n=(x_1\ldots x_n)^{1/n}$ is the answer to the question: ""Which number, when multiplied $n$ times by itself, is equal to the product of the $x_1,\ldots,x_n?$"" So the Arithmetic and Geometric and Geometric mean are associated with addition and multiplication, respectively. My question: Is there an operation, call it shmultiplication, such that the Harmonic mean $H_n=\frac{n}{\frac{1}{x_1}+\ldots+\frac{1}{x_n}}$ is the answer to the question: ""Which number, when shmultiplied $n$ times with itself, is equal to the shmultiplication of the  $x_1,\ldots,x_n?$"" (And if yes, does shmultiplication have a proper name?)","['means', 'descriptive-statistics', 'statistics', 'average']"
2512930,Proving a group is Abelian,"Let $G$ be a group with the property that in every subset of 4 distinct elements, there exists at least a pair of commuting elements.
Show that G is Abelian. I have thought so far that if G isn't abelian then if x,y dont commute and given subset of index 3 then the subset $\{x,y,xy\} \implies xy = yx$. Can I find something similar for the 4-element case?",['abstract-algebra']
