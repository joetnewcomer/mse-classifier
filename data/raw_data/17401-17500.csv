question_id,title,body,tags
142969,Problem about the definition of Euclidean domain,"In the definition of domain, we first define a degree function $\vartheta: R^\times \rightarrow \mathbb{N}$ with such two constraints: (1) $\vartheta(f)\leq \vartheta(fg)$ for all $f,g\in R^\times$. (2) for all $f,g\in R$ with $f\in R^\times$, there exist $q,r\in R$ with $g=qf+r$ and either $r=0$ or $\vartheta(r)<\vartheta(f)$. I wonder why we need the first constraints? I think with only the second constraint, it is enough to prove the theorem: every Euclidean ring is a PID. Can anyone give me a example where the first constraint is used?",['abstract-algebra']
142979,Definition of Separability Degree,"For an assignment, I am trying to determine the separability degree of some algebraic field extension $L/K$. The definition of the separability degree of polynomial is not difficult to find at all, namely it is the degree of the unique irreducible, separable polynomial we can associate with any polynomial. As of yet, I have been unable to find the definition of the separability degree of a field extension. Could someone give this definition or point me in the right direction to a definition? Based on the fact that if $L$ is the splitting field for $K$, then $|Aut(L/K)|\leq [L:K]$ with equality if and only if $L$ is separable over $K$, I am tempted to guess that the separability degree of $L/K$ has something to do with $Aut(L/K)$. Is this justified?","['definition', 'abstract-algebra', 'field-theory']"
142984,The approximation of set,"$A,B$ are closed sets of $\mathbb R^n$ such that $A \subset {\rm{Int}}(B)$. Is there a positive continuous function $\sigma (x)$ on $B$ such that if $f$ is a continuous function from $B$ to $\mathbb R^n$ such that $\left| {f(x) - x} \right| < \sigma (x)$, then $f(B)$ contains $A$?",['general-topology']
142989,Number of White squares,In how many ways three white square can be selected on a $8 \times 8$ chessboard such that no two squares are in same row or column. I am not able to reach on a conclusion for three squares. I have solved the problem for $2$ squares. Please help me out and if possible provide a detailed explanation.,['combinatorics']
142992,"Find all limit points of $M=\left \{ \frac{1}{n}+\frac{1}{m}+\frac{1}{k} : m,n,k \in \mathbb{N} \right \}$ in space $(M,\rho_{e})$","Find all limit points of $M=\left \{ \frac{1}{n}+\frac{1}{m}+\frac{1}{k}  : m,n,k \in \mathbb{N} \right \}$  in $(M,\rho_{e})$. I've founded, by building proper sequences, that points in the set$L=\left \{ \frac{1}{a} : a \in \mathbb{N} \right \}\cup \left \{ \frac{1}{a}+\frac{1}{b} : a,b \in \mathbb{N} \right \}$ are limit points, but what about other points from the set $M\setminus L$ ?","['general-topology', 'sequences-and-series', 'limits']"
143009,Mahalanobis  distance to probability between 0.0 and 1.0,"given the mahalanobis distance : $D_M^2(x) = (x-\mu)^T S^{-1}(x-\mu)$ how can I obtain the probability of $x = ( x_1, x_2, x_3, \dots, x_N )^T$ belonging to the data set given by covariance matrix $S$ and mean vector $\mu = ( \mu_1, \mu_2, \mu_3, \dots , \mu_N )^T$? If sample count is needed this is denoted $m$. I would like something I can use in a computer algorithm. Related to this I could ask how to obtain the hyper-ellipsoid that defines the confidence interval for e.g. 95%?","['statistics', 'probability']"
143017,Twisting a sheaf by a Divisor,"My question is how to define the twisting of a sheaf $\mathcal{L}$ by a divisor $D$. In specific I'm interested in the twisting of the canonical bundle $\omega$ of a non-compact Riemann surface $X$ by a divisor of points. (The points are the missing points of the compactification, in my case they are a finite number.) My guess on the definition is the following. I take $D=p_1+...+ p_n$. On $X$ Weil and Cartier divisors are the same, so $D$ is also a Cartier divisor and it is well defined the associated sheaf $\mathcal{L}(D)$. Then I ""define"" the twisted canonical bundle by: $\omega(D)=\omega \otimes \mathcal{L}(D)$. Is my ""definition"" correct? If it is I don't really see its geometric meaning. If it is not can you please point out a reference for it? Thank you!","['sheaf-theory', 'algebraic-geometry']"
143019,Julia Set of polynomials,"If $f$ is a polynomial and $z\in\mathbb{C}$, show that either $f^n(z)\rightarrow\infty$ or $\{f^n(z) : n\geq 1\}$ is a bounded set. Here, $f^2(z)=f(f(z))$ and $f^n(z)=f(f^{n-1}(z))$ for $n\geq 2$ I had a proof structured as followed: 1) Suppose $\{f^n(z) : n\geq 1\}$ is unbounded. Then there exists a subsequence $(n_k)$ such that $f^{n_k}(z)\rightarrow\infty$. 2) We are done if we can show that $|f^{k}(z)|$ is monotone everntually. But the trouble is 2) is really tedious to verify. I am just wondering whether there is a more pretty way to do this.","['fractals', 'complex-analysis']"
143034,Question on product measures,"The following problem is proving stubborn. I humbly request assistance. If $f$ and $g$ are integrable functions on $\mathbb R$ and $F(x,y) = f(x)g(y)$, then $F$ is measurable, integrable on $\mathbb R\times \mathbb R$ and 
$$\int_ {\mathbb R\times \mathbb R}F~d(\mu\times \mu)=\int_{\mathbb R}f~d\mu \int_{\mathbb R}g~d\mu.$$ Can I do this for the first two parts of the problem? If I let $A$ and $B$ be measurable subsets of $\mathbb R$. Set $f = 1_A, g=1_B$. Then $f = 1_{A\times B}$, $A\times B$ is measurable, so $f$ is measurable. $1_{A\times B}$ is integrable, so $f$ is integrable on $\mathbb R \times \mathbb R$. Furthermore $$\int_{\mathbb R} F~d(\mu \times \mu) = (\mu\times \mu)(A\times B) = \mu(A)\cdot \mu(B) = \int_{\mathbb R} f ~d\mu \int_{\mathbb R }g~d\mu .$$ This is all I'm able to do now. How about the second part?",['measure-theory']
143043,Maximum of 2 random variables,"This might be an easy question but currently I'm fighting over it with our TA who gave it in the final exam. Let $X, Y$ random variables and define $Z= \max \{X, Y\}$ what is the distribution of 
$Z \mid \{X > Y\}$? Should be $Z\mid \{X>Y\}\sim X$, right?! Thank you all.",['probability']
143097,Formula similar to $EX=\sum\limits_{i=1}^{\infty}P\left(X\geq i\right)$ to compute $E(X^n)$?,"Is there a formula like 
$$
EX=\sum_{i=1}^{\infty}P\left(X\geq i\right)
$$ (which can be found on Wikipedia and holds for positive $X$)
for $EX^{n}$ ? And I don't mean this one, $$
EX^{n}=\sum_{i=1}^{\infty}P\left(X\geq\sqrt[n]{i}\right),
$$ which is immediate, if we take $Y=X^{n}$ and use the above formula
for $Y$. I mean a ""more elegant"" one - if there is one.","['probability-theory', 'expectation']"
143098,Coproduct in the category of (noncommutative) associative algebras,"For the case of commutative algebras, I know that the coproduct is given by the tensor product, but how is the situation in the general case? (for associative, but not necessarily commutative algebras over a ring $A$). Does the coproduct even exist in general or if not, when does it exist? If it helps, we may assume that $A$ itself is commutative. I guess the construction would be something akin to the construction of the free products of groups in group theory, but it would be nice to see some more explicit details (but maybe that would be very messy?) I did not have much luck in finding information about it on the web anyway.","['noncommutative-algebra', 'ring-theory', 'category-theory', 'abstract-algebra']"
143116,Multiplying by a $1\times 1$ matrix?,"For matrix multiplication to work, you have to multiply an $m \times n$ matrix by an $n \times p$ matrix, so we have $$\bigg(m \times n\bigg)\bigg(   n\times p \bigg).$$ But what about a $1 \times 1$ matrix? Is this just a scalar? But every matrix can be mulitplied by a scalar; so do $1 \times 1$ matrices break the rule?",['matrices']
143135,Derivative of $\ln\sqrt{\frac{e^{x^2}}{e^x+2}}$?,"Being: $\ln f(x)=\log_ef(x)$ I started derivating $$\ln\sqrt{\dfrac{e^{x^2}}{e^x+2}}$$ but I get to a point that I don't know how to follow. I try to get it by derivating the logarithm directly and by using the logarithmic properties such us: $$\ln f(x)^n=n\ln f(x)$$ and $$\ln \frac{f(x)}{Q(x)}=\ln f(x)-\ln Q(x)$$ but I don't get it. By the time I have done this:
\begin{align}
f(x)&=\ln\sqrt{\dfrac{e^{x^2}}{e^x+2}}\\
&=\frac{\ln e^{x^2}-\ln (e^x+2)}{2}\\
&=\frac{x^2-\ln (e^x+2)}{2}\\
\text{And derivating:}\\
f'(x)&=\frac{1}{2}\left(\frac{d}{dx}x^2-\frac{d}{dx}\ln (e^x+2)\right)\\
&=x-\frac{1}{2}\left(\frac{e^x}{e^x+2}\right)
\end{align} So I finally get this: $f'(x)=x-\dfrac{e^x}{2e^x+4}$ But WolframAlpha says that $$f'(x)=\frac{x(x^2-5)}{x^2-4}$$ If I'm wrong, what do I do wrong? And if I'm right, how can I get from $x-\dfrac{e^x}{2e^x+4}$ to $\dfrac{x(x^2-5)}{x^2-4}$","['calculus', 'derivatives']"
143150,What is  the method to compute $\binom{n}{r}$ in a recursive manner?,"How do you solve this? Find out which recurrence relation involving $\dbinom{n}{r}$ is valid, and thus prove that we can compute $\dbinom{n}{r}$ in a recursive manner. I appreciate any help. Thank You","['recurrence-relations', 'combinatorics']"
143152,Limits of $f(x)=x-x$,"It's obvious that $f(x)=x-x=0$. But what exactly happens here? You have a function $f(x)=x-x$ and you have to calculate the limits when $x\to \infty$ This'll be like this:
$$\lim\limits_{x\to \infty}f(x)=\infty - \infty$$ That's an indetermination, and you have to multiply both sides with the conjugate of $f(x)$, which is equal to $x+x$. \begin{align}
f(x)&=x-x\\
&=\frac{(x-x)(x+x)}{x+x}\\
&=\frac{x^2-x^2}{x+x}
\end{align} If we do the limits now the answer is going to be: $$\lim\limits_{x\to \infty}f(x)=\frac{\infty - \infty}{\infty+\infty}$$ Which it's another type of indetermination(I think). What happens here?Can there be an error multiplying with it's conjugate in both sides? Is there another case like this? Or am I completely wrong?","['infinity', 'limits']"
143171,Refining Kolmogorov's Inequality,"I am trying to show that whenever we have a sequence of random variables that are symmetric distribued about the origin and have finite fourth moment, then Kolmogorov's inequality will be bounded above by the fourth moment/ t^4. Originally, we have that the bound is second moment (variance)/ t^2. My approach has been to use binomial expansion and set bounds, but I am unable to do so. Please give me some advise. Ok- I think I have made progress on this now and I have a solution. Please give me your comments in the ""answers"", since a lot of space in the comments has been wasted on unnecessary censorship ram-i-fications. Here it goes: First of all, we will do a binomial expansion of $(x_1 + x_2 + x_3+...+x_n)^4$ into $(x_1 + x_2 +...x_k) + (x_(k+1)+...+x_n)$. So I evaluate $E(x_1 + x_2 +...+x_n)^4$ in this way.  The general idea is then to see the odd moments cancel out, and I have to show that the cross terms are either zero or just positive. Then, I can have that 
$E(x_1 + x_2 +...+x_n)^4 \geq E(x_1 +...+x_k)^4$. Then, we use Jensen's with our convex function x^2 on positive reals to finish job since we already know Kolmogorov's Inequality is true. Hence, we have refined it.","['probability-theory', 'probability']"
143173,Showing the inequality $|\alpha + \beta|^p \leq 2^{p-1}(|\alpha|^p + |\beta|^p)$,"I have a small question that I think is very basic but I am unsure how to tackle since my background in computing inequalities is embarrassingly weak - I would like to show that, for a real number $p \geq 1$ and complex numbers $\alpha, \beta$ , I have \begin{equation}
|\alpha + \beta|^p \leq 2^{p-1}(|\alpha|^p + |\beta|^p)
\end{equation} I thought it would be best to rewrite this as \begin{equation}
\left|\frac{\alpha + \beta}{2}\right|^p \leq \frac{|\alpha|^p + |\beta|^p}{2}
\end{equation} but then I am unsure what to do next - is this a sensible start anyways ? Any help would be great ! (P.S. this is not a homework question - I am currently trying to brush up my knowledge of $L^p$ spaces, and this inequality came up as a statement. I thought it might be worthwhile to  make sure I can fill in the gaps to improve my skills in computing inequalities.)","['inequality', 'calculus', 'functional-analysis', 'real-analysis']"
143181,Showing $f\left(A\right)=e^{A^{2}}$ is differentiable.,"Let $f\left(A\right)=e^{A^{2}}$  where $A$  is an $n\times n$  matrix. Show that $f$  is differentiable and compute its derivative. I know this is kind of a basic question, but I am not sure how to solve it. By definition, we want to show that there exists a matrix $B$  such that $$\lim_{H\to0}\frac{\left|f\left(A+H\right)-f\left(A\right)-BH\right|}{\left|H\right|}=0.$$ Is this right? But since we're working with matrices I am not sure what to do. Any guidence would be appreciated.","['real-analysis', 'analysis']"
143186,Derivative of a random variable w.r.t. a deterministic variable,"I'm reading about time series and I thought of this procedure: can you differentiate a function containing a random variable. For example: $f(t) = a t + b + \epsilon$ where $\epsilon \sim N(0,1)$. Then: $df/dt = \lim\limits_{\delta t \to 0} {(f(t + \delta t) - f(t))/ \delta t} = (a \delta t + \epsilon_2 - \epsilon_1)/\delta t = a + (\epsilon_2 - \epsilon_1)/\delta t$ But: $\epsilon_2 - \epsilon_1 = \xi$ where $\xi \sim N(0,2)$. But this means that we have a random variable over an infinitesimally small value. so $\xi/\delta t$ will be infinite except for the cases when $\xi$ happens to be 0. Am I doing something wrong?","['statistics', 'calculus', 'probability', 'signal-processing']"
143213,"What is the simplest way to show that $\cos(r \pi)$ is irrational if $r$ is rational and $r \in (0,1/2)\setminus\{1/3\}$?","What is the simplest way to show that $\cos(r \pi)$ is irrational if $r$ is rational and $\displaystyle r \in \left(0,\frac{1}{2} \right)\setminus \left\{\frac{1}{3} \right\}$? I proved it using the following sequence $x_1 = \cos(r \pi)$; $x_{k} = 2 x_{k-1}^2-1$ and periodicity of the cosine function.
Is there any proof that is based on definition of rational numbers and trigonometric identities only?
Thanks!",['calculus']
143214,Using Rayleigh Quotient to approximate the first eigenvalue of the Laplace operator on the unit disk,"Let $D\subset\mathbb{R}^{2}$ unit disk, the first eigenvalue of the Laplace operator
holds: $\lambda_{1}=\inf\left\{ \frac{\int_{D}\left|\triangledown f\right|^{2}dv}{\int_{D}\left|f\right|^{2}dv}\,\,\mid\,\, f\in C_{c}^{\infty}\left(D\right)\right\} 
$ I know that $5<\lambda_{1}<6$ but I wish to have this result using the Rayleigh
quotient Thank you for your help.","['spectral-theory', 'functional-analysis', 'differential-geometry']"
143219,derivative of indicator function,"I have an indicator function $I(D\leq Q)$which is equal to $1$ if $D\leq Q$ and $0$ otherwise. What would be derivative of this function with respect to different variables such as $D$ or $Q$ or $P$ ($D$ is a function of $P$). Clarification to what I am trying to do: $D$ represents demand which is a function of price, assume $D=a-bp$ $Q$ represents quantity or supply, which is assumed to be fixed $$\text{profit} = p\min(D,Q)= PDI(D\lt Q)+PQI(Q\leq D)$$ I want to take derivative of profit with respect to price. Thanks in advance",['derivatives']
143222,What does $dx$ mean?,"$dx$ appears in differential equations, such us derivatives and integrals. For example, a function $f(x)$ its first derivative is $\dfrac{d}{dx}f(x)$ and its integral $\displaystyle\int f(x)dx$. But I don't really understand what $dx$ is.","['notation', 'calculus']"
143225,Show that $\sum\limits_{i=1}^N\sin^2\frac{m\pi i}{N+1}=\frac{N+1}{2}$,"I have recently found this excercise and was not able to solve it so far.
Show that $$\sum_{i=1}^N\sin^2\frac{m\pi i}{N+1}=\frac{N+1}{2}\;,$$ where $m \in \lbrace1,2,...,N\rbrace$. This was one of my attempts: $$\sum_{i=1}^N\sin^2\frac{m\pi i}{N+1}=\sum_{i=1}^N\frac{1}{2}\left[\cos(0)-\cos\frac{2m\pi i}{N+1}\right]=\frac{1}{2}-\frac{1}{2}\sum_{i=1}^N\cos\frac{2m\pi i}{N+1}$$ It seems the last sum should be equal to $-N$. Can anyone give me a clue?","['trigonometry', 'sequences-and-series']"
143228,A light beam between  two mirrors.,"$|AB|$ and $|BC|$ are mirror surfaces. The light beam starts from point A with $\beta$ angle to x axis as shown the picture below. 1) What is the condition of the system parameters to reach to point $B$ $(x_0,0)$ after reflections between mirrors? 2) What is the reach time that depends on $x_0,\beta,\alpha$ if the beam can reach point $B$? Assumtions: Mirrors are perfect plane and there is no loss during reflections and the speed of light is $c$.",['geometry']
143248,How can one solve the equation $\sqrt{x\sqrt{x} - x} = 1-x$?,$$\sqrt{x\sqrt{x} - x} = 1-x$$ I know the solution but have no idea how to solve it analytically.,['algebra-precalculus']
143257,$A$ be a $10\times 10$ matrix in which each row has exactly one entry equal to 1. find the possible value of the determinant,"Let $A$ be a $10\times 10$ matrix in which each row has exactly one entry equal to $1$. And remaining nine entries of the row being $0$. Which of the following is not a possible value of the determinant?
$0, 1 ,-1, 10$.
I am able to identify for $2\times 2$ cross two matrices for which possible value of determinant is $1$ or $-1$. How to identify for such a big size matrix? Can we identify such matrix?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors', 'determinant']"
143258,Adapted Towers of Hanoi from Concrete Mathematics - number of arrangements,"I have a doubt concerning an exercise from Chapter 1 of ""Concrete Mathematics"". Actually, my doubt is in one exercise ( exercise 3 ), but, since it depends on the previous exercise (2), I'm including it here too, with the respective solution that I found. Yes, I know that I could simply check the answer in Appendix A, since ""Concrete Mathematics"" gives answers to practically every exercise. But here's why I don't want to do this: I don't know how far my own attempt is from the correct answer. So, I would like someone to check if my attempt makes sense and give hints if there is something wrong or incomplete. If I checked the official solution right away, and it was very far from what I did, I would possibly end up missing the opportunity to find the correct answer (and I would also be unsure on whether my own attempt is also valid). 2. Find the shortest sequence of moves that transfers a tower of n disks from the left peg A to the right peg B, if direct moves between A and B are disallowed. (Each move must be or to or from the middle peg. As usual, a larger disk must never appear above a smaller one.) Note: I have no doubt in this particular exercise, but I included it because the exercise in which I have doubt depends on this one. I'm including the full detailed solution just for the sake of showing my work. First, the simplest case: if there are 0 disks, we need $T_0 = 0$ moves. For  $T_n$, we need to move the largest disk to peg B. For this, first we have to move the upper $n - 1$ disks to peg B, which requires $T_{n-1}$ moves. Then, we move the largest disk to the middle peg (1 move). Then, we can move the $n - 1$ disks back into peg A ($T_{n-1}$ more moves). Then, we move the largest disk to peg B (one more move). Finally, we can move the $n-1$ disks to peg B ($T_{n-1}$ more moves). So, we get the following recurrence relation: $T_0=0$ $\\ T_n = 3T_{n-1}+2 \ \ \ \ \text{for } n\geq1$ So, we have that $T_0 = 0$, $T_1 = 3T_0 + 2 = 2$, $T_2 = 3T_1 + 2 = 8$, $T_3 = 3T_2 + 2 = 26$, etc. Working with these numerical examples, we can eventually see that $T_n = 3^n - 1$. This can be proven by induction; for the base case: $T_0 = 3^0 - 1 = 0$. For the inductive step, assume that it is true that $T_{k} = 3^k - 1$ for some k; given this, we want to show that $T_{k+1} = 3^{k+1} - 1$. This is true, because: $T_{k+1} = 3T_{k} + 2 = 3(3^{k} - 1) + 2 = 3^{k+1} - 3 + 2 = 3^{k+1} - 1$. 3. (This is the exercise that I want someone to verify.) Show that, in the process of transferring a tower under the restrictions of the preceding exercise, we will actually encounter every properly stacked arrangement of n disks on three pegs. I manually tested some basic cases ($n = 1$, $n = 2$) by using the recurrence relation of the previous exercise, and concluded that the steps necessary to move the n disks from peg A to peg B really generate all the arrangements, and every possible configuration of the n disks in the 3 pegs arises, including the initial configuration (that is, when all disks are on peg A). Now, I will try to explain why this recurrence relation gives all the arrangements of n disks. My explanation may not be very clear, and it may be confusing. First of all, the initial configuration (that is, all disks are on peg A) is one possible arrangement. Let's use the following reasoning: there are only two things: the largest disk is one thing and the top $n-1$ disks are another thing. Let's consider the top $n-1$ disks as one indivisible block (as if it were only one disk). Then, applying the recurrence relation, we can see that: 1) When the largest disk is on peg A, the top $n-1$ disks are moved successively from peg A to pegs M (middle peg) and then B. This will give all possible arrangements between the largest disk and the top $n-1$ disks (visualized as a block) where the largest disk is on peg A. 2) When the largest disk is on peg M, the top $n-1$ disks are moved from peg B to peg M, then to peg A. This will give all possible arrangements where the largest disk is on peg M. 3) When the largest disk is on peg B, the top $n-1$ disks are moved from peg A to peg M, then to peg B. This will give all possible arrangements where the largest disk is on peg B. Since this is a recursive function, the same argument above will apply recursively to the movements of the top $n-1$ disks; that is, the largest of the $n-1$ disks will be one thing and the top $n-2$ disks will be another thing. And so on, recursively. From here, we can conclude that all the possible arrangements will be obtained. To finish this answer, I will calculate the number of arrangements, in the following way: for each of the $n$ disks, in decreasing order of size (from the largest to the smallest), choose one of the 3 pegs and put the disk in the peg. So, for each disk, there are 3 choices of peg. Therefore, the number of arrangements is $3^n$. This makes sense, because the number of moves is $3^n-1$. The number of arrangements is one plus the number of moves, because it includes the initial configuration (where all disks are on peg A).","['recurrence-relations', 'discrete-mathematics', 'algorithms']"
143259,"Cover times and hitting times of random walks, once again.","This is a followup to my question Cover times and hitting times of random walks . Consider a random walk on an undirected graph with $n$ vertices which, at each step, moves to a uniformly random neighbor. Define $T(u,v)$ to be the expected time until such a walk, starting from $u$, arrives at $v$, and let $T = \max_{u,v} T(u,v)$. Define $G(u)$ to be the expected time until such a walk, starting from $u$, visits every vertex and let $G = \max_u G(u)$. Is it true that  $$G \leq cT \log^k n$$ for some constants $c,k$ which do not depend on the graph or on $n$?","['probability-theory', 'graph-theory', 'stochastic-processes']"
143274,Finding derivative of $\sqrt{9-x}$,"I am trying to find the derivative of $\sqrt{9-x}$ using the definition of a derivative $$\lim_{h\to 0} \frac {f(a+h)-f(a)}{h} $$ $$\lim_{h\to 0} \frac {\sqrt{9-(a+h)}-\sqrt{9-a}}{h} $$ So to simplify I multiply by the conjugate $$\lim_{h\to0} \frac {\sqrt{9-(a+h)}-\sqrt{9-a}}{h}\cdot \frac{ \sqrt{9-(a+h)}+ \sqrt{9-a}}{\sqrt{9-(a+h)}+\sqrt{9-a}}$$ which gives me $$\frac {-2a-h}{h(\sqrt{9-(a+h)}+\sqrt{9-a})}$$ I have no idea what to do from here, obviously I can easily get the derivative using other methods but with this one I have no idea how to proceed.","['calculus', 'derivatives', 'limits']"
143285,Finding the equation of the normal line,"I have a question to find the equations of the tangent line and the normal line to the curve at the given point. I can find the equation for the tangent line easily but I am not sure what a normal line is and there is no example that I can find. $y=x^4 + 2e^x$ at (0,2) From that I do see that if I plug in 0 I get 2 as a result so my guess was that if I plug in another number I can use that to get the slope but it gave an incorrect answer.","['calculus', 'algebra-precalculus']"
143306,$f$ continuous iff $\operatorname{graph}(f)$ is compact,"The Problem: Let $(E,\tau_E)$ be a compact space and $(F,\tau_F)$ be a Hausdorff space. Show that a function $f:E\rightarrow F$ is continuous if and only if its graph is compact. My Work: First assume $(E,\tau_E)$ compact and $(F,\tau_F)$ a Hausdorff space . Assume $f:E\rightarrow F$ continuous. Then certainly $f(E)$ is compact. Then $$\operatorname{graph}(f)\subseteq E\times f(E)\subseteq E\times F.$$ Since the graph is closed ( we know this since $(F,\tau_F)$ Hausdorff and $f$ continouous) and $E\times f(E)$ is compact, as the product of two compact sets, than somehow this should give us that $\operatorname{graph}(f)$ compact. I was thinking the canonical projections would be helpful here but i'm unsure. As for the other way, I'm unsure. Any help is appreciated.","['general-topology', 'continuity', 'compactness']"
143313,How to show that $x/|x|$ is continuous?,"This is from p.156 of Topology, Munkres : The unit sphere $S^{n-1}$ in $\mathbb{R}^n$ is path-connected, since it is the continuous image of the surjective function $g: \mathbb{R}^n -0\to S^{n-1}$ by $g(x)=x/|x|$. (Note that the punctured euclidean space $\mathbb{R}^n -0$ is path-connected for $n>1$.) But how can I show that $g$ is continuous? I have an approach but this looks not cool: Since the domain and codomain are metric spaces, we can use epsilon-delta method. So 
$$\begin{align*}
\left|\frac x {|x|} - \frac y {|y|}\right| &= \frac1{|x||y|}\Big|\big(x|y|-|x|y\big)\Big|\\
&\le \frac1{|x||y|}\Big(\Big|\big(x|y|-|y|y\big)\Big|+\Big|\big(|y|y-|x|y\big)\Big|\Big)\\
&\le \frac1{|x||y|}\Big(|y||x-y|+||y|-|x|||y|\Big)\\
&\le \dfrac{|x-y|}{|x|}+\dfrac{|y-x|}{|x|}\\
&<2\dfrac{\delta}{|x|}\;.
\end{align*}$$ So at first by choosing delta to be smaller than $2\dfrac{\delta}{|x|}<\epsilon$, this function is continuous. Is it right? Are there any cool method?",['general-topology']
143324,"Simplify the expression: $\sum_{i=0}^{\infty}\sum_{j=0}^i\frac{X^{i-j}}{(i-j)!}\cdot\frac{Y^j}{j!}$,where X and Y are square matrices(not commutative)","I want to know how to simplify the following expression by using the fact that $\sum_{i=0}^\infty \frac{X^i}{i!}=e^X$. The expression to be simplified is as follows: $$\sum_{i=0}^{\infty} \sum_{j=0}^i \frac{X^{i-j}}{(i-j)!} \cdot \frac{Y^j}{j!}\;,$$ where  $X$ and $Y$ are square matrices (not commutative). (That is, $X\cdot Y \neq Y \cdot X$).","['matrices', 'sequences-and-series', 'linear-algebra', 'calculus']"
143329,Combinatorial progressions and cubes,"A k-term combinatorial progression of order d (abbreviated as k-CP(d)) is defined as an integer sequence $x_1<x_2<\cdots x_k$ such that the differences between successive numbers are at most d in number, i.e. the set $\{x_{i+1}-x_i:1\le i\le k-1\}$ has cardinality at most d. We say that a set of positive integers A has property CP if, for some fixed d, the set contains a k-CP(d) $\forall k\ge 1$. A m-cube is defined as a set of the form $<a,y_1,\cdots y_m>=\{a+\sum_{i=1}^m e_iy_i : e_i \in \{0,1\}\}$. We say that a set of positive integers A has the property C if A contains an m-cube $\forall m\ge 1$. I am trying to establish that $CP\implies C$. The author of the paper I am reading from says it is sufficient to prove the statement: For all $m,d\ge 1$ there exists $r=r(d,m)$ such that if $x_1,x_2\cdots x_r$ is an r-CP(d) then $\{x_1,x_2\cdots x_r\}$ contains an m-cube. My problem is why is this sufficient to establish this statement? The point I am stuck on is that we are guaranteed a combinatorial progression of a desired length but not guaranteed a combinatorial progression of the desired order. Hence in searching for an m-cube we may only use CP to get r-CP(d') where d' is bigger then d and hence the hypothesis (requisite r-CP(d) forces an m-cube) may not be invoked. In case anyone is interested this is from an Erdos paper where the author remarks that it is easy to see this. I can't find it easy at all and will be obliged if someone guides me. Thanks a lot.","['elementary-set-theory', 'combinatorics']"
143334,Minimum set of US coins to count each prime number less than 100,"Say I wanted to be able to carry enough coins in my pocket such that at any time, I could count out exact change totaling any of the prime numbers less than 100.  How would I determine the minimum set of coins I would need to carry?  I don't care about being able to count out multiple primes - meaning the set does not have to remain useful after one number has been counted out. So, given the standard US coins: penny, nickel, dime, and quarter - what would be the minimum number of each coin that I would need to be able to count out exactly $3, 5, 7, 11$ up to $97$ cents.","['prime-numbers', 'number-theory', 'combinatorics']"
143341,$2\times 2 $ matrices over $\mathbb{C}$ that satisfy $\mathrm A^3=\mathrm A$,"Let $\mathrm A \in \mathbb C^{2 \times 2}$. How many $2 \times 2$ matrices $\mathrm A$ satisfy $\mathrm A^{3} = \mathrm A$. Infinitely many? If it is $3 \times 3$ matrix then by applying Cayley-Hamilton theorem I could have said that given matrix is diagonalizable. Also zero is eigenvalue of $\mathrm A$. So it would be collection of all singular diagonalizable matrices. But how to count them? Here I have $2 \times 2$ matrices i feel I can't apply the Cayley-Hamilton Theorem here?
I am stuck with these thoughts?","['vector-spaces', 'matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
143346,How to define the Jacobian matrix for all points of a finite-type k-scheme?,"Ravi Vakil's notes 13.2.D asks: ""Show that if the Jacobian matrix for X = Spec $k[x_1,\ldots,x_n]/(f_1,\ldots f_r)$ has corank d at all closed points, then it has corank d at all points. (Hint: the locus where the Jacobian matrix has corank d can be described in terms of vanishing and nonvanishing of certain explicit matrices.)"" But I can't even see how to define the Jacobian matrix for non-closed points. He defines the Jacobian matrix as the $n$ by $r$ matrix such that $a_{ij} = $ the partial derivative of $f_j$ with respect to $x_i.$ If p is a closed point, then this makes sense; this partial derivative can be evaluated at p. But I don't see how he is defining the Jacobian matrix for all points, not just closed points.","['algebraic-geometry', 'schemes']"
143359,Three-dimensional art galleries,"The well-known art gallery problem starts with an ""art gallery"" (a simple polygon in the plane, not necessarily convex) and asks for the minimum number of ""guards"" (points on the polygon) required to ""observe the whole gallery"" (to have the property that for any point in the interior of the polygon, there is a line segment from that point to a ""guard"" that lies entirely within the polygon).  Chvatal showed that if the polygon has $n$ vertices, then $\lfloor n/3\rfloor$ guards are sufficient, and sometimes necessary, to observe the whole gallery. If you forget about trying to minimize the number of guards, and simply want to place guards so that they see the whole gallery, it is reasonably clear that if you place a guard at each vertex of a simple polygon, they will be able to observe the whole gallery. One way to see this is to note that the assertion is clear for triangles, and then to recall (or to convince oneself) that any simple polygon can be triangulated without adding vertices. If visualizing an entire triangulation of the polygon is too ""global"", one can think ""locally"" as follows.  Fix any point $p$ in the gallery interior.  Choose a point $q$ on the polygon such that the distance from $p$ to $q$ is minimized.  The line segment from $p$ to $q$ lies within the gallery.  If $q$ is a vertex, we are done.  Otherwise, $q$ is on the interior of an edge.  Pick a direction on that edge and move $q$ along the edge in that direction.  Eventually, one of two things will happen: (1) the point $q$ becomes a vertex, or (2) there is a first time at which the line segment from $p$ to $q$ intersects the polygon somewhere besides $q$.  In case (1) we are done, and in case (2), we can convince ourselves that at the time that this happens, the closest point to $p$ on the intersection of the polygon and the line segment must be a vertex, and we are again done. Now switch from two dimensions to three so an ""art gallery"" is now a polyhedron.  If you place a guard at each vertex, can they observe the whole gallery? The answer, in general, is no. It may not be clear why it is no, but it is relatively clear that the arguments just given do not generalize in any simple way. There are polyhedra that cannot be ""triangulated"" into tetrahedra without adding vertices.  A famous example of this is the Schoenhardt polyhedron .  (Yet: experimenting with this applet convinced me that the vertices of this polyhedron do see all of its interior.) The ""given $p$, pick a closest point $q$ on the polyhedron, and then move $q$ in some direction"" idea clearly cannot work (at least without judicious choice of direction), because in the case (2) there is no reason for the closest point on the intersection of the line segment from $p$ to $q$ with the polyhedron to be a vertex in the three-dimensional case.  It can pretty obviously be on the interior of some edge. So it's not counterintuitive, to me, that there are polyhedra whose vertices cannot observe their interiors.  But I'd like a better mental image of what such a polyhedron can actually ""look like.""  (A better image, for example, than what I get from the picture on the Wikipedia entry for the art gallery problem.) Can somebody describe a polyhedron, in such a way that it is in some sense ""obvious"" that its vertices cannot see all of its interior?  So that it is possible to form a clear mental picture of what it would look like to be inside such a polyhedron, at a point where you can't see the vertices?  (What do you see?)","['geometry', 'polyhedra', 'visualization']"
143362,How does composition affect eigendecomposition?,What relationship is there between the eigenvalues and vectors of linear operator $T$ and the composition $A T$ or $T A$? I'm also interested in analogous results for SVD.,"['linear-algebra', 'eigenvalues-eigenvectors']"
143370,Can we say that there exist an integer $n$ such $A+nB$ invertible?,"If $A$ and $B$ are $3\times 3$ matrices and $A$ is invertible, then can we say that there exists an integer $n$ such that $A+nB$ invertible? I was trying to show this by choosing $n$ such that eignevalues of $A+nB$ are non-zero. In the case where $B = I$ we can find the eigenvalues of $A+nB$ that would be $\lambda + nB$ (though I am not certain about its proof). This choosing of $n$ such that $\lambda$ is not equal to $-n$ times an eigenvalue of $B$ will serve the purpose. But I am not sure about general $B$. What if I take arbitrary matrices $A$ and $B$.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors', 'determinant']"
143374,matrix differential equation,"Given a matrix $X(t)=e^{tA}$, we know that $X(t)$ is the solution of the following matrix differential equation:
$$ \frac{dX(t)}{dt} =X(t) \cdot A .$$ Now could anyone help to construct a matrix differential equation in terms of $Y(t)$, such that $Y(t)=e^{tA} \cdot e^{tB}$ is its solution? (NOTE: the matrices $A$ and $B$ do not commute, meaning that $e^{A+B} \neq e^A \cdot e^B.$ )","['matrix-equations', 'matrices', 'linear-algebra', 'ordinary-differential-equations']"
143380,Inverse Josephus problem,"Josephus problem is rather well known - every $m$ -th person in circle of $n$ people is killed - question is where in the circle was the last person standing? Let us make an inverse of that problem and ask What is the least step $m$ for which $k$ -th person is the last one standing in a circle of $n$ people ( $n$ and $k$ are given)? There are two logical questions to ask: Does such $m$ always exist? (Conjecture: yes - computer tests show it is correct up to $n=300$ ). What is the upper bound for $m$ ? $\text{lcm}(1,2,\ldots,n)$ seems logical, but is there something lower (computer tests show a number of outliers that make it hard to estimate such a bound).",['number-theory']
143383,Two-Point boundary value problem,"To solve ${d^2y \over dx^2} =f(x)$, $0<x<1$ with $y(0)=\alpha, y(1) = \beta$. We can get a finite difference approximation by taking $$\frac{y_{j+1}-2y_j+y_{j-1}}{h^2} =f_j \\\Rightarrow \frac{1}{2}y_{j+1}-y_j+\frac{1}{2}y_{j-1} =\frac{h^2}{2}f_j$$ Then we get a system of linear equations which can be written as $\left(\begin{array}{ccccc}
-1 & 1/2 & 0 &0 &0 \\
1/2  & -1 & 1/2 &0 &0\\
0 & 1/2 & -1 &1/2&0\\
0 & 0 & 1/2&-1 &1/2\\
0 & 0 & 0 &1/2 &-1\end{array} \right) \cdot \left(\begin{array}{c}
y_1 \\
y_2  \\
y_3  \\
y_4  \\
y_5 \end{array} \right) = \left(\begin{array}{c}
\frac{h^2}{2}f_1 -\frac{\alpha}{2}\\
\frac{h^2}{2}f_2  \\
\frac{h^2}{2}f_3  \\
\frac{h^2}{2}f_4 \\
\frac{h^2}{2}f_5 -\frac{\beta}{2} \end{array} \right)$ which we can solve by gaussian elimination. I got the idea from http://www2.imperial.ac.uk/~pavl/finite_dff.pdf I understand the above, but how do I calculate/find/write out $f_2$, $f_3$, $f_4$ etc.? What if my DE is$$-{d^2y\over dx^2} + {dy\over dx} =x \Leftrightarrow {d^2y\over dx^2}={dy\over dx}-x $$ with $u(0)=\alpha, u(1) = \beta$? Edit:
I rearranged the equation and got $f_{j+1}(1-\frac{h}{2})+y_{j-1}(1+\frac{h}{2})+y_j(-2)=h^2f_j$. $j=1 \Rightarrow f_{2}(1-\frac{h}{2})+y_{0}(1+\frac{h}{2})+y_1(-2)=h^2f_1$. So does $h^2f_1 = \frac{1}{36}\cdot(-\frac{1}{6})?$ And does $h^2f_2 = \frac{1}{36}\cdot(-\frac{2}{6})$?","['ordinary-differential-equations', 'numerical-methods']"
143398,The coefficient of $x^{18}$ in $(1+x^5+x^7)^{20}$,"I was asked about a simple question that is: ""What is the coefficient of $x^{18}$ in $(1+x^5+x^7)^{20}$? Generally, we know that; $$(x+y+z)^n= \sum_{n_{1}+n_{2}+n_{3}=n}\left(\frac{n!}{n_{1}!n_{2}!n_{3}!}\right)x^{n_{1}} y^{n_{2}}z^{n_{3}} $$
So, this question could be answered easily based on above formula. May I ask, if we can find another way for solving such this problem or not. Thanks for sharing the thoughts.","['generating-functions', 'polynomials', 'combinatorics']"
143414,Is the size of the Galois group always $n$ factorial?,"I am studying field theory, and I just started the chapter on Galois theory. Since a Galois extension is the splitting field of some polynomial $p(x)$ and this polynomial have exactly $n$ roots in the extension field I think that every automorphism of the extension permutes the roots of $p(x)$ [and for every permutation there exist an automorphism]. So I deduced that that size of the Galois group, that is the number of automorphisms from the extension to itself that fix the field we started with, is $n!$ for some natural $n$. Am I right ?","['galois-theory', 'abstract-algebra', 'field-theory']"
143416,Continuous map from two intervals to the closed topologist's sine curve?,"The closed topologist's sine curve is the set $\{(x,y)\in\mathbb{R}^2 : x=0, -1\le y\le 1\}\cup\{(x,y)\in\mathbb{R}^2 : y=\sin\frac{1}{x}, 0<x\le \pi\}.$ Clearly it is not possible to continuously map the closed unit interval onto this set, as it fails to be path connected. But what about the disjoint union of two intervals? I would guess not, I would hope the topologist's sine curve is more badly behaved, but I can't see how to show it's not possible. Is the continuous image of a locally (path) connected space, necessarily locally (path) connected under some reasonable assumptions?",['general-topology']
143419,On the semantics of the gamma distribution,"Good Morning. I have recently been faced with modeling a quantity that is best modeled via a Gamma distribution. I have noticed that, in the characterization of the distribution via the parameters $k$ and $θ$, the mean of the distribution is $kθ$, whereas the ""top"" of the distribution, representing the point that is sampled with the highest probability, is found at point $θ( k - 1)$. This contradicts my intuition. If the mean represents the expected value of the distribution, then why is it not placed at the ""top"" of the distribution, much like in the normal distribution? Thanks, Jason","['statistics', 'probability', 'probability-theory']"
143424,Using Chebyshev's inequality to obtain lower bounds,"I need help with a question I found in Master Stats. I'm unaware of Chebyshev's inequality hence I can't do this question, can anyone help. Q) A company produces planks whose length is a random variable of mean 2.5m
and standard deviation 0.1m. Use Chebyshev's inequality to obtain a lower bound on the
probability that the length of planks does not differ more than 0.5m from the mean length. Thanks in advance",['probability']
143433,What is $\frac{\det(A+tI)}{\det(B+tI)}$ as $t\to0$?,"If $A$ and $B$ are two real $2\times 2$ matrices with $\det A = 0 $ and $\det B = 0 $ and $\mathrm{tr}(B)$ is non zero. then what will be limit of $$\lim_{t\to0}\frac{\det(A+tI)}{\det(B+tI)}$$ 
I used the formula $\lambda^2-\mathrm{tr} A+\det A = 0$. then i think answer is $\dfrac{\mathrm{tr}(A)}{\mathrm{tr}(B)}$. Am I correct? What would be expansion of $\det(A+tI)$ for a $2\times 2$ matrices?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors', 'determinant']"
143440,Need help understanding a lift of a vector field,"This is a question from my differential geometry assignment: Let $\pi:M\to N$ be a submersion between two smooth manifolds and $X\in \Gamma(TN)$ is a vector field. We need to show that there is a smooth vector field on $M$ that is $\pi$-related to $X$. Also determine a necessary and sufficient condition on $\pi$ for a lift of any vector field to be unique. So what I've got so far is that, since $\pi$ is a submersion, at each $p\in M$, $\pi_{*p}$ is surjective. So for every $X_{\pi(p)}$ I can choose a $Y_p\in T_pM$ such that $\pi_{*p}Y_p=X_{\pi(p)}$ and define a local smooth vector field in a neighbourhood $U_p$ of $p$. Do it for every $p\in M$ and $\{U_p\}$ gives an open cover of $M$. My problem here is that, firstly I don't know how I could get a local smooth vector field from $Y_p$. I thought of using an integral curve and extend $Y_p$ according to the curve, but it does not give me a vector field on an open set. Secondly, in order to get a global smooth vector field, in the overlapping, say a point $r\in U_p\cap U_q$, I am going to have $Y^1_r, Y^2_r$ that come from the two local vector fields defined by two points $p$ and $q$. Then I should probably sum up all the vectors in $T_rM$, then use the partition of unity subordinate to $\{U_p\}$. But I'm not sure how to apply the partition of unity. I'm stuck now, how can I go from here? Please do not post  complete solutions. Thanks.",['differential-geometry']
143449,Riemann's thinking on symmetrizing the zeta functional equation,"In the translated version of Riemann's classic On the Number of Prime Numbers less than a Given Quantity , he quickly derives the zeta functional equation through contour integration essentially as $$\zeta(s)=2(2\pi)^{s-1}\Gamma(1-s)\sin(\tfrac12\pi s)\zeta(1-s).$$ and says three lines later that it may be expressed as $$\xi(s) = \pi^{-s/2}\ \Gamma\left(\frac{s}{2}\right)\ \zeta(s)=\xi(1-s).$$ After reading MO-Q7656 , I started wondering whether, as his ideas evolved before he wrote the paper , he first constructed $\xi(s)$ by noticing that multiplying $\zeta(s)$ by $\Gamma(\frac{s}{2})$ introduces a simple pole at $s=0$ thereby reflecting the pole of $\zeta(s)$ at $s=1$ through the line $s=1/2$ and that the other simple poles of $\Gamma(\frac{s}{2})$ are removed by the zeros on the real line of the zeta function. The $\pi^{-s/2}$ can easily be determined as a normalization by an entire function $c^s$ where $c$ is a constant, using the complex conjugate symmetry of the gamma and zeta fct. about the real axis. Anyone familiar with how his ideas (thinking) evolved? (Update 5/13/2012) Riemann in his fourth equality in his paper, before he writes down the functional eqn., has $$2sin(\pi s)(s-1)!\zeta(s)=i\int_{+\infty}^{+\infty}\frac{(-x)^{s-1}}{e^x-1}dx$$ where the contour sandwiches the positive real axis and surrounds the origin in the positive sense. For $m=0,1,2, ...,$ this gives $$\zeta(-m)=\frac{(-1)^{m}}{2\pi i}\oint_{|z|=1}\frac{m!}{z^{m+1}}\frac{1}{e^z-1}dz=\frac{(-1)^{m}}{m+1}\frac{1}{2\pi i}\oint_{|z|=1}\frac{(m+1)!}{z^{m+2}}\frac{z}{e^z-1}dz$$ from whence you can see, if you are familiar with the exponential generating fct. for the Bernoulli numbers, that the integral vanishes for even $m$. Riemann certainly was familiar with these numbers and states that the integral relation he gives implies the vanishing of $\zeta(s)$ for $m$ even (but gives no explicit proof). Edwards in Riemann's Zeta Function (pg. 12, Dover ed.) even speculates that "".. it may well have been this problem of deriving (2) [Euler's formula for $\zeta(2n)$ for positive $n$] anew which led Riemann to the discovery of the functional equation ...."" Riemann gives two proofs of the fct. eqn.--the first based on contour integration and the singularities of $\frac{1}{e^z-1}$, the second, on the theta function. Edwards wonders: ""Since the second proof renders the first proof wholly unnecessary, one may ask why Riemann included the first proof at all. Perhaps the first proof shows the argument by which he originally discovered the functional equation or perhaps it exhibits some properties which were important in his understanding of it."" In fact, Riemann states on page 3 of his paper, ""This property of the function [$\xi(s)=\xi(1-s)$] induced me to introduce, in place of $(s-1)!$, the integral $(s/2-1)!$ into the general term of the series"" for zeta. And then he proceeds to introduce the Jacobi theta function. Edit Dec 18, 2014 In the early 1820s, both Abel and Plana separately published what is now called the Abel-Plana formula (see Wikipedia). In the title of Plana's, he mentions the Bernoulliens. I wonder how much Riemann was influenced by these papers.","['riemann-zeta', 'math-history', 'functional-equations', 'analysis']"
143454,Computing $A^{50}$ for a given matrix $A$,"$A =\left(
               \begin{array}{ccc}
                 1 & 0 & 0 \\
                 1 & 0 & 1 \\
                 0 & 1 & 0 \\
               \end{array}
             \right)$ then what would be $A^{50}$?
For real entries","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
143462,Laurent series for an even function,"Show that if the Laurent series $\sum_{n=-\infty}^{\infty}a_n(z-z_0)^n$ represents an even function, then $a_{2n+1}=0$ for $n=0,\pm 1,\pm 2,\ldots$, and if it represents an odd function, then $a_{2n}=0$ for $n=0,\pm 1,\pm 2,\ldots$. where $a_n=\frac{1}{2\pi i}\int_C \frac{f(z)}{(z-z_0)^{n+1}}dz$ I know the fact that if $f(-z)=f(z)$ then $f$ is even. But I have difficulty applying this to show what i need to have.",['complex-analysis']
143468,Entire function bounded by a polynomial is a polynomial,"Suppose that an entire function $f(z)$ satisfies $\left|f(z)\right|\leq k\left|z\right|^n$ for sufficiently large $\left|z\right|$, where $n\in\mathbb{Z^+}$ and $k>0$ is constant. Show that $f$ is a polynomial of degree at most $n$.",['complex-analysis']
143471,Approximating the logarithm of a Laplace transform,"Suppose $X$ is a random variable on $\mathbb R_+$ with finite mean, i.e. $\mathbb E X <+\infty$. Let $F_X(t)$ be its c.d.f. and $\mathcal{L}_X(\cdot)$ its Laplace transform, i.e.
$\mathcal{L}_X(s)=\int_0^\infty e^{-s t} d F(t)$ Can one conclude immediately that, as $s \to 0$, $\log \mathcal{L}_X(s) \approx -s \mathbb E X + o(s^2) $ ? If not, suppose now that $X$ has finite moments of all orders. Can one now conclude that, as $s \to 0$, $\log \mathcal{L}_X(s) \approx -s \mathbb E X + o(s^2) $ ?","['laplace-transform', 'approximation', 'analysis', 'probability', 'taylor-expansion']"
143473,Finding $\lim\limits_{x\to 0}\frac{\sin{3x}}{x}$,"I am trying to find the limit of $$\lim_{x\to 0}\frac{\sin{3x}}{x}$$ I have no idea what I am supposed to do. I know the identity that, $$\lim_{x\to0}\frac{\sin{x}}{x} = 1$$ but that will not be good enough on a test and I am not sure why that is true anyways. I do not know how I am supposed to proceed with this problem.","['calculus', 'limits']"
143479,All irreducible representations of Pauli group,"I'm supposed to find out all irreducible representations of Pauli group, that is, the group generated by Pauli matrices $\sigma_k(k=1,2,3)$. It has 16 elements: $\pm 1, \pm i, \pm \sigma_k, \pm i \sigma_k$, and 10 classes: $\{1\}, \{-1\}, \{i\}, \{-i\}, \{\pm \sigma_k\}, \{\pm i \sigma_k\}$. Since the sum of square of dimension of representations equals the order of the group, and the number of irreducible representations equals the number of classes, the only possible combination is 8 1-D representations, and 2 2-D representations. But the character of the 2-D representation must be $\chi=(2,-2,2i,-2i,0,\cdots,0)$ or the character will have a norm larger than 1, and that means reducible. Thus there can only be one 2-D irreducible representation of Pauli group. That contradicts above. Then what goes wrong in my deduction?","['representation-theory', 'group-theory']"
143482,Integrating a product of exponential and complementary error function with square-root of variable in the denominator,"I need to evaluate
\begin{equation}
\int_a^\infty \mathrm{erfc}\left( \frac{b}{\sqrt{c\cdot h}}  \right) e^{-d\cdot h} dh
\end{equation}
where $\mathrm{erfc}(s) = \frac{2}{\sqrt{\pi}} \int_{s}^{\infty} \exp(-t^2) dt$. A closed-form expression is appreciated since ultimately, I need to do \begin{equation}
\int_0^\infty \left(
\int_{k\cdot y}^\infty \mathrm{erfc}\left( \frac{b}{\sqrt{c\cdot h}}  \right) e^{-d\cdot h} dh
\right)
e^{-m \cdot y} dy
\end{equation} I've noticed that a similar function - the Q-function - such that
\begin{align}
  Q(s) &= \frac{1}{\sqrt{2\pi}} \int_s^\infty e^{-\frac{x^2}{2}}dx \\
       &=\frac{1}{2} \mathrm{erfc}(\frac{x}{\sqrt{2}})
\end{align}
and the Q-function has an alternative representation
\begin{align}
  Q(s) = \frac{1}{\pi} \int_0^{\frac{\pi}{2}} \exp{\left(\frac{-s^2}{2\sin^2{\phi}} \right)}d\phi
\end{align}
but I'm not sure if this helps.","['improper-integrals', 'calculus', 'integration', 'definite-integrals', 'error-function']"
143483,Convergence of Indicator function.,"I was going through Egorov's theorem on wikipedia. It gives a example why should be  $\mu(A)<\infty $. Sequence of real valued indicator function is taken. It claims that the sequence: 
 $ f_n(x)=1_{[n,n+1]}(x) $ converges pointwise for $n\in N$ and $x\in\Re$.
I am not able to understand how it converges pointwise to $0$ ?",['measure-theory']
143488,Find out dimension of the eigenspace of a given linear transformation $T$,"Let $T:\mathbb{R^4}\rightarrow \mathbb{R^{4}}$ be defined by $T(x,y,z,w)=(x+y+5w,x+2y+w,-z+2w,5x+y+2z)$ then what would be the dimension of the eigenspace of $T$? One approach may be to find out eigenvalues and then eigenvectors.
Is there any other approach that will consume less amount of time
and calculation?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
143502,"Maximizing $f(x,y)$","Could somebody please shed some light on this problem? Let $x,y \in \mathbb R$, we wish to maximize $f(x,y)=\frac{x^2-y^2}{(x^2+y^2)^2}$ by finding suitable values of $x,y$. Setting $\partial f\over \partial x$ and $\partial f \over \partial y$ as $0$ gives $x(x^2-3y^2)=0=y(y^2-3x^2)$ but these give $x=y=0$ which is not acceptable! Any ideas? Thank you.","['optimization', 'ordinary-differential-equations']"
143511,Probability and permutations,"I performed an experiment in which an individual had to order 5 items (i.e. his ""response"" was something like $(3,2,1,4,5)$ or some other permutation). The correct ordering was $(1,2,3,4,5)$ and I want to know the probability of getting 1, 2, 3 or 5 of the numbers right by chance alone. I know that the probability of getting all right by chance alone is $1/5!$, and I think that probability of getting 3 right is: $$
\frac{5 \choose 2}{5!}
$$ Because if you get 3 right you are just ""swapping"" two items from $(1,2,3,4,5)$. I am having difficulty to deduce the rest of the probabilities. I have written an R code that simulates the process, so I have an approximate answer. I just wanted to know how to do it by hand. Thank you in advance!","['probability', 'combinatorics']"
143516,Eigenvalues of a matrix and its square,"Ok so I messed up my last question, I'll rephrase it: Is there a matrix $A$ of real elements, for which this holds true: $A^2$ has more unique eigenvalues than $A$. If not, then what about if the elements of $A$ were complex numbers? I didn't manage to find such a matrix yet, so I tried proving that it's impossible.
I know that the eigenvalues can be calculated by constructing the characteristic polynomial for both $A$ and $A^2$ and finding the roots. $$|A - tI| = 0$$
$$|A^2 - tI| = 0$$
But in the general case with a $n*n$ matrix the resulting polynomials are way too complicated to say anything about, so there must be another way to do it. Thanks","['linear-algebra', 'polynomials']"
143530,design matrices,"Given a linear model $Y = X\beta + \epsilon$ with three treatments and six subjects where $X$ is the design matrix, suppose $X = \begin{matrix}1 & 1 & 0\\
1 & 1 & 0\\
1 & 0 & 1\\
1 & 0 & 1\\
1 & -1 & -1\\
1 & -1 & -1
\end{matrix}$ and X'=
  \begin{matrix}1 & 0 & 0\\
1 & 0 & 0\\
1 & 1 & 0\\
1 & 1 & 0\\
1 & 0 & 1\\
1 & 0 & 1
\end{matrix} with response vector $Y=[Y_{11} Y_{12} Y_{21} Y_{22} Y_{31} Y_{32}]^{T}$ Do these two matrices give you the same model? I've had some trouble trying to figure out how, given that $\beta = (\beta_1 \beta_2 \beta_3)^T$, these two designs can give the equivalent model. Don't they have entirely different values of $\beta$ for each $Y_{ij}$? EDIT: The constraints are $\sum_{i=1}^{3}\beta_i = 0$","['statistics', 'regression']"
143547,properties of a real analytic function,"If there are a radius $r>0$ and constants $M,C\in\mathbb R$ for all $y\in U$ with
$$|\partial^if(x)|\leq M\cdot i!\cdot C^{|i|}\space\space\space\space \forall x\in\mathbb B_r(y),i\in\mathbb N_0^n$$
then $f\in C^\infty(U)$ is real analaytic. But I don't have any idea how to prove this.
I just know a function is called analytic if there are power series (convergent) in each point of U. thanks for helping! :)","['calculus', 'asymptotics', 'harmonic-analysis', 'real-analysis', 'analysis']"
143563,Algorithms for symbolic definite integration?,"What are the algorithms for symbolic definite integration? Apart from computing the antiderivative first. What are the basic ideas behind such algorithms? As far as I got it, the main idea behind symbolic indefinite integration is that we actually know what kind of terms should be in the answer. And it is easy to believe in this, since it is enough to think what kind of terms are produced during differentiation. But for definite integrals it seems that the answer could be virtually any combination of known functions and constants. I'm aware of experimental methods involving high-precision numerical integration and then using things like inverse symbolic calculator . But that's not what I'm looking for.","['definite-integrals', 'symbolic-computation', 'computer-algebra-systems', 'abstract-algebra']"
143571,Question Regarding Existence of One Sided Limits,"In a Calculus book, I had read the following proposition: For a function $f:X\to \mathbb{R}$, $X\subseteq \mathbb{R}$ then $\lim\limits_{x\to x_0}f(x)$ exists if and only if $\lim\limits_{x\to x_0^+}f(x)$ and $\lim\limits_{x\to x_0^-}f(x)$ exist and $\lim\limits_{x\to x_0^+}f(x)=\lim\limits_{x\to x_0^-}f(x)$. It has come to my attention however that this is wrong. Let $f:\left[a,b\right] \to \mathbb{R}$, $f(x)=x$. Note that $a$ is an accumulation point (from the right) of the domain of $f$. Obviously, $\lim\limits_{x\to a^-}f(x)$ does not exist since $a$ is not an accumulation point from the left of the domain of $f$. Using the definition of a limit of a real function (or using the fact that $f$ is continuous on $a$) we can derive that $\lim\limits_{x\to a}f(x)=a$ which is a contradiction to the proposition above. My question is, in the proposition do we need to additionaly suppose that $x_0$ is an accumulation point from the right and the left of $X$? If $x_0$ is an accumulation point of $X$ only from the right and $\lim\limits_{x\to x_0^+}f(x)$ exists then is it true that $\lim\limits_{x\to x_0}f(x)$ exists and $\lim\limits_{x\to x_0}f(x)=\lim\limits_{x\to x_0^+}f(x)$?","['calculus', 'limits']"
143579,Meaning of Nc in Paul Erdős and Rényi's paper On Random Graphs,"In Paul Erdős and Rényi's 1959 paper On Random Graphs I , they describe the number of edges in a random graph by the function (1) Nc = [1/2 * nlogn + cn] where n is the number of nodes in the graph, c is ""an arbitrary fixed real number"" and [x] denotes the integer part of x. They go on to use a number of graphs of the form G(n, Nc), where G(n, Nc) denotes a random graph with n nodes and Nc edges. Nc appears to be an arbitrary (?) function to return a number of edges based on some c and n , but I don't understand how c is selected or why the function is relevant. When the authors discuss graphs of the type G(n, Nc), is there any special property besides the graph just having some arbitrary number of edges? I.e, could I replace Nc with some function for a random number of edges between 0 and the maximum possible edges for the graph with the same results?","['graph-theory', 'discrete-mathematics']"
143583,Is there a natural way to multiply measures?,"Given two measures $\mu$ and $\nu$ on some measurable space $X$, is there a way to multiply them to get $\mu \cdot \nu$, another measure on $X$ (and not $X \times X$, as for the usual notion of product measure)? Here's a case where I know how to give a definition: if both $\mu$ and $\nu$ are absolutely continuous with respect to some common measure $\lambda$, then we can take their Radon–Nikodym derivatives with respect to that measure to obtain two functions $f_\mu$ and $f_\nu$, so that $\mu = \int f_\mu d\lambda$, $\nu = \int f_\nu d\lambda$ which we can then multiply, to give us $\mu \cdot \nu = \int (f_\mu \cdot f_\nu) d\lambda$. This came up in the context of Monte–Carlo integration, and in particular Monte–Carlo path tracing. In this case, the measure space could be, say, the set of angles at which an incoming light ray bouncing off an object could be reflected, $\mu$ would be a probability measure describing the probability of outgoing angles, and $\nu$ would be a measure describing the light sources in the scene visible from that point of reflection. The idea of the multiplication $\mu \cdot \nu$ is to produce something which describes the sampling of light sources at that point, depending on the incoming ray (and $\mu$, on top of just $\nu$).","['probability-theory', 'monte-carlo', 'measure-theory', 'probability-distributions']"
143587,"There exists a unique function $u\in C^0[-a,a]$ which satisfies this property","The problem: Let $a>0$ and let $g\in C^0([-a,a])$. Prove that there exists a unique function $u\in C^0([-a,a])$ such that $$u(x)=\frac x2u\left(\frac x2\right)+g(x),$$ for all $x\in[-a,a]$. My attempt At first sight I thought to approach this problem as a fixed point problem from $C^0([-a,a])$ to $C^0([-2a,2a])$, which are both Banach spaces if equipped with the maximum norm. However i needed to define a contraction, because as it stands it is not clear wether my operator $$(Tu)(x)=\frac x2u\left(\frac x2\right)+g(x)$$ is a contraction or not. Therefore I tried to slightly modify the operator and I picked a $c>a>0$ and defined $$T_cu=\frac 1cTu.$$ $T_cu$ is in fact a contraction, hence by the contraction lemma i have for granted the existence and the uniqueness of a function $u_c\in C^0([-a,a])$, which is a fixed point for $T_cu.$ Clearly this is not what I wanted and it seems difficult to me to finish using this approach. Am I right, is all what I have done useless? And if this were the case, how to solve this problem? Thanks in advance.","['banach-spaces', 'real-analysis']"
143591,Calculate $\lim\limits_{x\to -1}\frac{x^2+3x+2}{x^2+2x+1}$,I've just have a mathematics exam and a question was this: Calculate the limits of $\dfrac{x^2+3x+2}{x^2+2x+1}$ when $x\text{ aproaches }-1$. I started by dividing it using the polynomial long division . But I always get $\frac{0}{0}$. How is this limit evaluated?,['limits']
143594,are non-degenerate critical points always isolated?,"I have a question regarding the isolation of critical points of a function: Suppose $f : \mathbb{R}^n \to \mathbb{R}$ is a $C^\infty$ function such that $f$ has a non - degenerate critical point at $0 \in \mathbb{R}^n$. That is, we have $\triangledown f (0) = 0$, and the Hessian $\left((\partial^2 f/ \partial x_i \partial x_j ) (0) \right)$ is invertible. Can I deduce from this that the critical point at $0$ is an isolated critical point ? My guess is to say yes, because the fact that the Hessian is non-degenerate forces $f$ to change its value in the vicinity, and in all directions. But I am unsure, in particular with regards to the last statements (""in all directions"") - though that should be encoded by the fact all eigenvalues of the Hessian are non - zero. Is this the right way to think about the question of whether the critical point is isolated ?
Thanks for your thoughts !","['differential-geometry', 'real-analysis']"
143607,An abelian group of order 100,"The first part of the problem asks you to prove that an abelian group $G$ with order $100$ must contain an element of order $10$. 
For this part, I use Sylow theorem to list possiblities for $H$ and $K$ where $|H|$=$2^{2}$ and $|K|$=$5^{2}$.  $K$ must be normal since there is not subgroup of order $25$ while $H$ might not be normal since $1$$\equiv$$25$ mod $2$. Also I proved that $H$$K$=$G$ and they have only the identity element in common. Then, if $H$ and $K$ are normal. G must be isomorphic to one of the following groups: $Z_{4}$ $\times$ $Z_{25}$ $Z_{2}$ $\times$ $Z_{2}$ $\times$ $Z_{25}$ $Z_{4}$ $\times$ $Z_{5}$ $\times$ $Z_{5}$ $Z_{2}$ $\times$ $Z_{2}$ $\times$ $Z_{5}$ $\times$ $Z_{5}$ From above, it is easily to pick up element of order $10$ for each. But my confusion is that since $G$ is an abelian group, how can I use the theorem that any finite abelian group is isomorphic to a direct product of cyclic groups? Also, since the second part asks you if no element of $G$ has order greater than $10$, what are its torsion coefficients? I think my way of listing the possibilities are too complicated. Are there any more explicit ways to solve the problem? Thanks a lot.","['finite-groups', 'group-theory', 'abstract-algebra', 'abelian-groups']"
143616,Approximation Lemma in Serre's Local Fields,"Let $A$ be a Dedekind domain, and let $K$ be its field of fractions.
In Serre's Local Fields , the following Lemma is stated. Approximation Lemma Let $k$ be a positive integer. For every $i$, $1\leq i \leq k$, let $\mathfrak p_i$ be prime ideals of $A$, $x_i$ elements of $K$, and $n_i$ integers. Then there exists an $x\in L$ such that $v_{\mathfrak p_i}(x - x_i)\geq n_i$ for all $i$, and $v_{\mathfrak q} \geq 0$ for $\mathfrak q \neq \mathfrak p_1,\ldots, \mathfrak p_k$. To get a better feel for this I would like to be able to actually find the $x$ stated in Lemma. I have tried to follow the proof to do this, and there is one crucial step that I don't understand at all. At the start, after assuming that the $x_i$ are in $A$, it is stated that ""by linearity, one may assume that $x_2 = \ldots = x_k = 0$"". I don't see why we can assume this, and I don't even know what is meant by linearity here. Any suggestion as to what it means in the proof, or alternative (preferably constructive) proofs would be much appreciated. As a further, related question, this lemma is often stated as showing that we can find an element that is in one collection of ideals, and not in some other ideal (for example, the proof of proposition 19). I don't see how the Lemma controls an element NOT being in an ideal - it has no control over how high the valuation at an ideal is. Any hints about this would be appreciated too.","['commutative-algebra', 'algebraic-number-theory', 'abstract-algebra']"
143619,Integrally closed with roots of identity,"Let $\lambda_1,...,\lambda_n$ be roots of unity with $n\geq 2$. Assume that $$\frac{1}{n}\sum_{1}^{n}\lambda_i$$ is integral over $\mathbb{Z}$. Show either $\sum_{1}^{n}\lambda_i=0$ or $\lambda_1=\cdots=\lambda_n$. I only know the basic definition of integral elements, so does there exists a basic proof to the problem?","['commutative-algebra', 'abstract-algebra']"
143624,(Game Theory) Incomplete Information extension of the 'Centipede' Game,"This question is an extension of the Centipede Game . My prof. posed this to me in class and I can't figure out how to approach this problem. Imagine in this game, there is an alternative possibility (with very low probability, 0.0001, 0.0000001, etc.) that the first player is irrational, and always goes right instead of down. If this were true, it would make sense for player 2 to always go right (except at the last node) to achieve the highest payoff. The tricky part for me is: now assume that P1 and P2 are both rational, and that they know that each other are rational, but P1 does not know that P2 knows that P1 is rational — how does that affect the game (with regards to the equilibrium outcome)? Backwards induction is pretty straightforward in the original form of the game, is there an alternative method that can be better used for this incomplete information version? (I was thinking of assigning probabilities — starting from the second-to-last node — that would make the payouts equivalent for P2's response to an irrational/rational version of P1, but that seems quite tedious for the ~100 nodes, and I feel there must be come generalized approach to this.)","['game-theory', 'probability']"
143645,Linear algebra exam questions,"Pick out the true statements: There exist $n\times n$ matrices $A$ and $B$ with real entries such that $(I-(AB-BA)^n) = 0$. If $A$ is symmetric and positive definite matrix then $tr(A)^n\geq n^n \det(A)$. 
:( I am stucked, unable to solve this problem.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
143647,Is a closed subset of isolated points in a compact set necessarily finite?,"If I have a compact set $A$ and a closed subset $\Sigma \subset A$ which only contains isolated points (that is, none of them is a limit point). Does the compactness of $A$ then force $\Sigma$ to have finite cardinality ? Here is my attempt at a proof that the above question can be answered in the positive: Suppose for contradiction that $\Sigma$ contains infinitely many distinct points. EDIT :
Then we can construct a sequence of points in $\Sigma$ which consists of distinct points. By compactness of A, this sequence must have a convergent subsequence, and by the fact that $\Sigma$ is closed, this limit lies in $\Sigma$. But then it cannot be a limit point, because all points in $\Sigma$ are isolated. So the subsequence must eventually constant and equal to the limit, contrary to the construction of the sequence. Is the reasoning above correct ? If no, what did go wrong ?",['general-topology']
143660,How do I show that an interleaved sequence converges?,"Suppose $A_n \to L$ and $B_n \to L$. I need to show that the sequence $A_1,B_1,A_2,B_2,A_3,B_3,\dots$ converges to $L$.
Now, I know that both $A_n$ and $B_n$ are less than or equal to $L$ for all $n$, however, how can I show that if you interleave the items in $A_n$ and $B_n$, it will also converge to $L$? It makes sense, I just don't know how to approach this problem.","['convergence-divergence', 'sequences-and-series']"
143669,"naive bayes, understanding the correctness of a model and computation","I implemented naive bayes algorithm to predict an emotion ( happy , sad ) for blogs using the formula provided by Manning's Information Retrieval book http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf essentially for a given document, it comes down to comparing p(word = a,b,c | label = bad) p(label = bad) vs. p(word = a,b,c | label = good) p (label = good) now I wonder since we have all the counts of the words for each emotion, Can we reframe the problem such as this: p(label = bad | word = a) * p(label = bad | word = b) * p(label = bad | word = c) vs. p(label = good | word = a) * p(label = good | word = b) * p(label = good | word = c) Does this computation make sense? This is a more general question... how do you evaluate the correctness of this computation? If so, what is the implication of this model vs the formula found in the textbook? for example, are there different claims about independence? With naive bayes, the assumption is that the words are ""conditionally"" independent. It seems to me that the textbook model is where you compare 2 factories (happy, sad) and you compare the likelihood of making that string of words from the 2 factories as opposed to say rolling several dies and using each die's signal of being good or bad fyi I took one basic probability course","['statistics', 'probability']"
143678,Find out trace of a given matrix $A$ with entries from $\mathbb{Z}_{227}$,"Let $A$ be a $227\times 227$ matrix with entries in $\mathbb{Z}_{227}$ such that all of its eigen values are distinct. What would be its trace?
I think it is zero by adding all 227 elements but i am not sure. Edited: Here I have assumed that eigenvalues are in a base field.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
143684,Let $B$ be a nilpotent $n\times n$ matrix with complex entries let $A = B-I$ then find $\det(A)$,Let $B$ be a given nilpotent $n\times n$ matrix with complex entries. Let $A = B-I$   find out $\det(A)$. What if B is orthogonal or skew symmetric matrix? Then can we say anything about its trace and determinant?,"['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
143686,Proof that ODE solutions with Wronskian identically zero are linearly dependent,"According to Wikipedia , if the Wronskian of two functions is always zero, then they are not necessarily linearly dependent. But it seems that if the two functions are solutions of the same homogeneous second-order linear differential equation, then the condition $W[y_1, y_2](t) = 0$ does indeed imply that they are linearly dependent. Online, I found that if two functions are real analytic and their Wronskian is identically zero, then they are necessarily linearly dependent. But there is no reason that the solutions to a linear differential equation should be real analytic. How can we prove that the condition $W[y_1, y_2](t) = 0$ implies the linear dependence of $y_1(t)$ and $y_2(t)$? More generally, how can we prove that the condition $W[y_1, \ldots, y_n](t) = 0$ implies the linear dependence of $y_1(t), \ldots, y_n(t)$?","['ordinary-differential-equations', 'wronskian']"
143695,Necessary and sufficient condition for the matrix $A = I - 2 x x^t$ to be orthogonal,Let $x$ be a non zeo (column) vector in $\mathbb{R}^n$ . What is the necessary and sufficient condition for the matrix $A = I-2xx^t$ to be orthogonal?,"['orthogonal-matrices', 'matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
143696,Must $g$ be the identity if $f = g \circ f$?,"I am finding it hard to solve the following problem. Let $A$ is a set and $f : A \rightarrow A$ and $g : A \rightarrow A$. If $f = g \circ f$, must $g$ be an identity function always? Will there be any counterexamples to show that $g$ must not be a identity function?","['examples-counterexamples', 'elementary-set-theory', 'functions']"
143698,Topology of the tangent bundle of a smooth manifold,"I am having trouble understanding what topology is given to the tangent bundle of a smooth manifold that allows it to be a smooth manifold itself. In my understanding, among other things, the topology must be second countable and Hausdorff. The definition of the tangent bundle $TM$ of a smooth manifold $M$ I am using is $TM = \bigsqcup_{p\in M} T_pM$ , that is, the disjoint union of all $T_pM$ where $T_pM$ is tangent space at $p$ consisting of all derivations at $p$ . Since there is no further specification on what topology this space is given, I assume we take the natural disjoint union topology. However, in that case it seems that $TM$ is not second countable because then every set $(O,p)$ where $O$ is an open subset of $T_pM$ would be open and disjoint from any $(O,q)$ for $q \neq p$ . So unless $M$ is countable there would be an uncountable number of disjoint open sets which contradicts second countability. The only alternative I can think of is using the natural smooth structure of $TM$ as the topology. That is for every open subset $O$ of $M$ the open sets of $TM$ are defined as $\pi^{-1}(O)$ where $\pi$ is the natural projection $TM \rightarrow M$ .
But then $TM$ can not be Hausdorff, since any two elements of the same fiber of $\pi$ could not be separated by open sets. In conclusion, in both cases $TM$ could not be a manifold, so I must be missing something very obvious. Thus, I would really appreciate it if someone could point out my misconception.","['general-topology', 'differential-geometry']"
143711,Show the norm map is surjective,"Let $K/F$ be an extension of finite field. Show that the norm map $N_{K/F}$ is surjective. Here is what I have so far: Since $F$ is a finite field and $K/F$ is a finite extension of degree $n$, so $\operatorname{Gal}(K/F)=\langle\sigma\rangle$, where $\sigma(a)= a^{q}$ with $q=p^{m}=|F|$. In addition, by primitive element theorem, $K=F(\alpha)$ for some $\alpha \in K$. We want to show $N_{K/F}(\alpha)$ generates $F$. By the definition of norm, we have $$N_{K/F}(\alpha)=\alpha^{1+q+\cdots+q^{n-1}}$$ and since $(1+q+\cdots+q^{n-1})(q-1)=q^n-1$, we have the order of $N_{K/F}(\alpha)$ is divided by $q-1$. But I want to show $o(N_{K/F}(\alpha))=q-1$ in order to conclude surjectivity. So any hint for how to proceed? Any other methods are also perferred.","['galois-theory', 'finite-fields', 'abstract-algebra']"
143724,least squares regression in 3space,"robjohn is giving me a hand with this, but in case anybody else knows... I need to do a least-squares regression for linearity on a set of coordinates in 3space. If the dataset is linear, I need to see if it is close to vertical or horizontal. How could I do this? Many thanks in advance Joe Stavitsky","['statistics', 'regression']"
143730,When is the sum of first $n$ numbers equal to the sum of the next $k$ numbers?,"When is the sum $1+2+\cdots + n = (n+1) + (n+2) + \cdots +(n+k)$? The easiest solution $(n,k)$ is $(2,1)$. For example, $1+2 = 3$. Do any others exist? Roots of $(n+k)^2 + (n+k) = 2n^2 +2n$ give solutions (Is this solvable via a Pell equation?) If a complete graph has red($n$) and blue($k$) nodes, and I pick an edge at random; the probability of picking one that connects two red vertices is $1/2$, find the number of blue and red nodes. This may be trivial but I really have no clue so I ask the pros. A (sort of) variation on the first problem of ""Fifty Challenging problems in probability with solutions"" Frederick Mosteller.","['number-theory', 'graphing-functions', 'integer-partitions', 'diophantine-equations', 'probability']"
143735,Cover time and intersection time for lazy random walks on graphs,"Consider a simple lazy random walk on an $n$-vertex undirected, connected graph: this is the Markov chain which transitions from $i$ to $j$ with probability $p_{ij}=1/(2d(i))$ where $d(i)$ is the degree of node $i$. Note that $p_{ii}=1/2$ for all $i$.  Define $C(i)$ be the expected time until a walk starting from node $i$ visits every vertex and let $C = \max_i C(i)$. Let $I(k,l)$ be the expected time until two random walks, starting at vertices $k$ and $l$, intersect (i.e., until they visit the same vertex at the same time). Let $I = \max_{k,l} I(k,l)$. My question is: how big can $I/C$ get as a function of the number of vertices $n$? Is it true that $I/C$ is upper bounded by a constant which is independent of $n$ or of the graph? If not, is it true that $$ \frac{I}{C} \leq k \log^l n$$ for some constants $k,l$ independent of $n$ and of the graph?","['probability-theory', 'graph-theory', 'stochastic-processes']"
143737,Matrix exponential and rank,"Let $A$ be a square matrix. Invertibility of $\exp(A)$ follows easily from properties of the matrix exponential. Is $\int_0^t \exp(A u)du$ also invertible? I believe it should be, and that the inverse should be $I - At/2 + A^2t^2/12 + ...$ This comes from expanding the real-valued function $x/(e^x - 1)$ in a power series about $x=0$. How should I approach a proof of this (or could I find it in a book somewhere?) What about the more general case when $\Phi(t)$ is defined by $\frac{dX}{dt} = A(t)X,\ \  X_0 = x_0 $ and $X(t) = \Phi(t)x_0$? How might one show that $\int_0^t \Phi(u)du$ is invertible?","['matrices', 'linear-algebra', 'calculus']"
143744,Bi-invariant form on compact connected Lie group,"Let $G$ be a Lie group and $\omega$ be a left invariant $k$-form, how to prove that  $r^*_a \omega$ is left invariant?
What I do: $(l^*_g (r^*_a \omega))_x (v_1, \ldots,v_k)=(r^*_a \omega))_{gx} ({l_g}_*v_1, \ldots,{l_g}_*v_k)= \omega_{gxa} ({r_a}_* {l_g}_*v_1, \ldots,{r_a}_* {l_g}_*v_k)=$ and stuck here... The right side of the equality should be $(r^*_a \omega)_x(v_1, \ldots,v_k)$.","['lie-groups', 'differential-geometry']"
143745,Open Balls in Metric Space.,"I'm working with the metric space $(\mathbb{N}, \rho)$ where $\mathbb{N}$ is the set of natural numbers and $\rho(x,y) = |\frac{1}{x} - \frac{1}{y}|$. I'm considering the open balls on this metric. Are there any that are finite? Infinite? All of $\mathbb{N}$? My hunch is that there are open balls that are finite and infinite. For example, the open ball $B(1, \frac{1}{2})$ seems to be just {$1$}. But if we make the radius larger than $1$ doesn't the open ball becoming infinite? Am I correct? Are there any open balls that are finite? Infinite? All of $\mathbb{N}$?
Any other general statements we can make about the open balls?","['metric-spaces', 'real-analysis']"
143746,Stacks in arithmetic geometry [closed],"Closed. This question is off-topic . It is not currently accepting answers. Want to improve this question? Update the question so it's on-topic for Mathematics Stack Exchange. Closed 12 years ago . Improve this question Stacks, of varying kinds, appear in algebraic geometry whenever we have moduli problems, most famously the stacks of (marked) curves. But these seem to be to be very geometric in motivation, so I was wondering if there are natural examples of stacks that arise in arithmetic geometry or number theory. EDIT: the question is now at MO .","['algebraic-geometry', 'soft-question', 'number-theory']"
143751,Combinatorics : Which side is heavier?,"n coins are given, among which exactly 3 are bad and heavier than the good ones. A balance is used to identify the bad coins. Assume k coins are picked in both sides of the balance at a time. What is the probability of left side being heavier right side being heavier both sides being equal in weight Thanks.","['probability', 'combinatorics']"
143759,Galois Group of $(x^3-5)(x^2-3)$,"I am having some trouble calculating the Galois group (over $\mathbb{Q}$) of $(x^3-5)(x^2-3)$. I can see the splitting field is $F:=\mathbb{Q}(\sqrt[3]{5},\omega,\sqrt{3})=\mathbb{Q}(\sqrt[3]{5},i,\sqrt{3})$, where $\omega$ is a primitive 3rd root of unity, and it has degree 12 over $\mathbb{Q}$. Since the extension is Galois (it is a splitting field extension for a separable polynomial), this makes $|\operatorname{Gal}(F/\mathbb{Q})|=12$ so it is either $S_3 \times S_2$ or $A_4$ as these are the only subgroups of of $S_5$ of order 12. It has a transposition (complex conjugation) so I conclude that it is $S_3 \times S_2$. I found all the subgroups of $S_3 \times S_2$, (all 16 of them), but I can't see to match them up with the intermediate fields. For example, there are three subgroups of order 6 in $S_3 \times S_2$ which should correspond to three field extensions of degree 2. One of them should be $\mathbb{Q}(i)$ since this is an intermediate field with a degree 2 minimal polynomial, but none of the subgroups of order 6 I found have every element fix $i$, so this is a problem. (A subgroup of order 6 has an element of order 3, which should be permuting the roots of $x^3-5$, no?) Have I gone wrong somewhere? I would appreciate the help.","['galois-theory', 'abstract-algebra', 'field-theory']"
143775,Combinatorics question in the style of Van der Waerden's theorem,"I would really appreciate some help with the following problem. It resembles Van der Waerden a lot but I don't know how to proceed. I was told an averaging argument might do the trick but I can't see it. Let $N, r$ be positive integers. Then, there exists a subset $X$ of $\left\{1,2,\ldots,N\right\}$ which contains arithmetic progression of length $r$ with at least $k \geq N/r \cdot \text{some constant}$ ratios (more precisely, it contains arithmetic progressions of length $r$ of the form $a,\ a+b,\ \ldots,\ a+rb$, where $b$ varies over $k$ values), and which is of size at most $N^{1-\epsilon(r)}$. Thanks in advance!","['additive-combinatorics', 'combinatorics']"
143780,Are undefined terms allowed in a sequence?,"Is this a valid sequence: $$\left\{\frac1{(n-3)}\right\}?$$ That is, can a sequence have individual terms that are undefined? If so, does this mean that the above sequence is unbounded (since the third term is not smaller than any real number)? [What I do know is that if the above sequence were valid, it would be convergent (by definition), which would in turn mean that it is bounded (by the Boundedness Theorem).] Subsequent to the main discussion here, I have just come across an old exam question that asks us to show that $\langle\tan(\frac{\sqrt n\pi}4):n∈N\rangle$ is divergent. But this sequence is undefined at say, $n=4$ , so like the one above, it isn't even a valid sequence, is it? (I've used a different notation for sequences on Brian M Scott's advice.)","['convergence-divergence', 'sequences-and-series', 'real-analysis', 'limits']"
