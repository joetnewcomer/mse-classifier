question_id,title,body,tags
4747075,Maximum number of n-tuples of equal sum,"I'm interested in a general formula to calculate the maximum number of unique subsets from a set of distinct integers that sum to the same value. For 2-tuples, given s > 1 distinct integers, there are maximally Floor(s / 2) subsets that can be formed whose sum are equal. For 3-tuples, I observe the following: \begin{array}{|c|c|c|}
\hline
\text{s}& \text{No. of subsets} & \text{Tuple representation} \\ \hline
3 & 1 & (x_1, x_2, x_3)\\ \hline
4 & 1 & (x_1, x_2, x_3)\\ \hline
5 & 2 & (x_1, x_2, x_3), (x_3, x_4, x_5)\\ \hline
6 & 3 & (x_1, x_2, x_4), (x_2, x_3, x_5), (x_3, x_4, x_6)\\ \hline
7 & 5 & (x_1, x_2, x_3), (x_4, x_5, x_6), (x_1, x_4, x_7), (x_2, x_5, x_7), (x_3, x_6, x_7)\\ \hline
8 & 6? & (x_1, x_2, x_3), (x_4, x_5, x_6), (x_1, x_4, x_7), (x_2, x_5, x_7), (x_3, x_6, x_7), (x_1, x_6, x_8), ??\\ \hline
\end{array} Then I'm already finding it difficult to calculate for s=8 or beyond. I could try to validate with some algebra, but I'm more interested in generalizing for the case of any s and any n. Appreciate any help.","['number-theory', 'combinatorics', 'problem-solving']"
4747087,Proving that a quadratic form has a single eigenvalue in an open interval,"I need help with this exercise: Show that the matrix
B = \begin{pmatrix}
3 & 2 & 1 \\
2 & 4 & 1 \\
1 & 1 & 1
\end{pmatrix} has exactly one eigenvalue in the open interval (1,2) by studying the signatures of the quadratic forms with matrices $B-I$ and $B-2I$ . The only thing I have managed to do is compute the signatures of $B-I$ and $B-2I$ . They're (2,1) and (1,2) respectively. I don't know how the open interval comes into play.","['multivariable-calculus', 'linear-algebra', 'quadratic-forms']"
4747105,The most explicit way of partitioning the reals into two dense subsets with positive measure,"In @UmbertoP's response to the question, "" Partition of real numbers into dense subsets of positive measure ,"" the answer is understandable to a advanced undergraduate; however, I have inadequate knowledge of measure and set theory. I tried to create a more explicit example here but it's too complicated. Question: Is there a more explicit version of Umberto P's answer that's understandable to an average undergraduate?","['real-numbers', 'proof-explanation', 'measure-theory', 'real-analysis']"
4747113,Examples and characterizations of totally geodesic maps,"A totally geodesic map between two Riemannian manifolds is a map that carries geodesics of the domain manifold to geodesics of the target/co-domain manifold, i.e. following the definition from the introduction section of this paper $F:M\to N$ is totally geodesic if for every geodesic $\gamma\subset M, F(\gamma)\equiv F \circ \gamma \subset N$ is a geodesic in $N.$ This is fairly intuitive and one can see that every local self isometry of $M$ is totally geodesic. This is also a generalization of linear maps between two vector spaces if I'm correct. However, I'd like to get some simple examples and if possible, characterizations of these maps. To start with, is there any characterizations or way to construct examples of totally geodesic maps from $M$ to $\mathbb{R}?$ Clearly, constant maps are there but I want some non trivial examples. We can start with simple questions like: What are all the totally geodesic maps from: $\mathbb{R}^d \to \mathbb{R}?$ The answer to the question when these maps fix the origin is : all linear projections. $\mathbb{S}^d \to \mathbb{R}?$ One could be deceived into thinking it's the stereographic projection followed by a linear projection, but I think it's not, since the stereographic projection has domain $\mathbb{S}^d$ minus a point, and this point maps to infinity. It seems to me that there's no such maps because if it's continuous, it's image must be a compact subset of the real line, and that's not a geodesic. Generalizing 2), it seems to me that the all totally geodesic maps from a compact $M$ to $\mathbb{R}$ must be constant and hence, trivial. Given 2) and 3), one can now ask: what about totally geodesic maps from a noncompact Riemannian manifold to $\mathbb{R}?$ I guess one can start with the simply connected non-positive curvature manifolds, that are diffeomorphic to $\mathbb{R}^d$ by their exponential map (Cartan-Hadamard theorem). Maybe this is a good start, so in this case, is any totally geodesic map the combination of inverse exponential and linear projection? Some relevant literature with lots of examples would be appreciated!","['geodesic', 'geometry', 'riemannian-geometry', 'differential-geometry']"
4747125,Can someone explain me how one can show $\Bbb{P}(\sqrt{s}|N|>\sqrt{1-s}|N'|)=2\arcsin(\sqrt{s})/\pi$?,"Let $s\in [0,1]$ and let $N, N'$ be two independent random variable distributed as $\mathcal{N}(0,1)$ . I have given the following computation which I don't get: $$\begin{align}\Bbb{P}(\sqrt{s}|N|>\sqrt{1-s}|N'|)&=\Bbb{P}\left[ (N,N')\in \{(\pm r\cos(\theta),r\sin(\theta)): |\theta|<\arcsin(\sqrt{s}), r>0\}\right]\\&=4\arcsin(\sqrt{s})/(2\pi)\\&=2\arcsin(\sqrt{s})/\pi\end{align}$$ Nw my problem is that I first of all don't get how to get the first equality, it would be nice if someone could explain me what happens there and how to get there, then also the second one is not really clear.","['proof-explanation', 'probability-distributions', 'normal-distribution', 'probability-theory', 'probability']"
4747135,"Each $A\in SO(3,\mathbb{R})$ satisfies $A^3-\alpha A^2+\alpha A-I_3=0$ with $-1\leq \alpha \leq 3$","Question: Let $G=SO(3,\mathbb{R})=\{A\in GL(3,\mathbb{R}):A^TA=I_3, \det(A)=1\}$ . a) Show that for any element $A$ in $G$ , there exists a real number $\alpha$ with $-1\leq\alpha\leq 3$ such that $A^3-\alpha A^2+\alpha A-I_3=0$ . b) For which real numbers $\alpha$ with $-1\leq \alpha\leq 3$ does there exist an element $A$ in $G$ whose minimal polynomial is $x^3-\alpha x^2+\alpha x-1$ ? Answer: What I know is that since the dimensions of the matrices are three, the characteristic polynomial of each $A$ must be a three dimensional polynomial. Also, since each three dimensional polynomial has at least one real root, every matrix $A$ must have an eigenvector. Moreover, any matrix $A$ in $G$ has the property that sum of the squares of the entries in each row of $A$ must be 1, i.e. $A_{i1}^2+A_{i2}^2+A_{i3}^2=1$ . However, I could not relate this knowledge to solve the questions. Any help/hint would be appreciated. Thanks in advance...","['matrices', 'linear-algebra']"
4747188,Issues about Miranda's definition of the algebraic curve.,"On Page 171 of Rick Miranda's Algebraic Curves and Riemann Surfaces , he gave Theorem 1.9 without a proof. Definition 1.1. Let $S$ be a set of meromorphic functions on a compact Riemann surface $X$ . We say that $S$ separates points of $X$ if for every pair of distinct points $p$ and $q$ in $X$ there is a meromorphic function $f \in S$ such that $f(p) \ne f(q)$ . We say that $S$ separates tangents of $X$ if for every point $p \in X$ there is a meromorphic function $f \in S$ which has multiplicity one at $p$ . A compact Riemann surface $X$ is an algebraic curve if the field $\mathcal{M}(X)$ of global meromorphic functions separates the points and tangents of $X$ . The basic analytic result from which we will proceed is the following. Theorem 1.9. Every compact Riemann surface is an algebraic curve. By reading Otto Forster's text I found the proof of separating points part (Corollary 14.13, Page 116): 14.13. Corollary. Suppose $X$ is a compact Riemann surface and $a_1, \dots, a_n$ are distinct points on $X$ . Then for any given complex numbers $c_1, \dots, c_n \in \mathbb{C}$ , there exists a meromorphic function $f \in \mathscr{M}(X)$ such that $f(a_i) = c_i$ for $i = 1, \dots, n$ . but can anyone tell me how to prove the separating tangents part?","['complex-geometry', 'algebraic-geometry', 'riemann-surfaces']"
4747200,Series expanding from integral?,"I have a specific question with regards to an integral like this one, for example: $\int(x^6)cos(3x)dx$ I noticed that this solves out to: $= (x^6)(\frac{1}{3}\sin(3x) ) - (6x^5)(-\frac{1}{9}\cos(3x) ) $ ....... (it goes on a while) As I'm a student who (admittedly) failed Calculus II the first time and am retaking it and using your notes to practice, I wanted to challenge myself to writing a general series for solving all problems of the type: $$\int (x^a)\cos(b*x)dx$$ where: a is a real integer constant b is a real constant a > 0 so, from the previous problem:
a=6, b=3 I understand that this series would start with sum(n=1, a) but I don't know where to go from there. I also have a few questions about how I would go about doing this: How do I alternate between cos and sin? For example: $\cos(...) + \sin(...) + \cos(...) + \sin(...) ...$ This series seems to start at n=6 and count downwards towards $n=1$ . How do I work around this problem, as (as far as I'm aware) series need to count up? More broadly, beyond this series thing, is there any other way of simplifying the work required in a problem like this, where: $\int(x^{\text{some power}} * \cos(...) dx)$ So that I won't have to go through the process of integrating by parts every little thing into the table, and then putting it all together, since that's super time intensive and I my biggest problem last semester was not doing my math quick enough? I also understand that I might have to write a different counterpart series for any integral like $\int(x^a)\sin(b*x) dx$ Final note:
I'm aware that there's such a thing as ""series expansion of an integral"" but I don't know anything about it, as we haven't covered it in class yet. I understand that if I followed those methods, I could probably do this no problem, but I wanted to challenge myself for the sake of practice. EDIT: So now that I've worked on it for a little bit, I've broken it into parts a little more: alternate sin/cos: $$\sum_{n=0}^a\frac{(-1)^n(x^{2n}+(-\frac{\pi}{2}+x)^{2n})}{(2n)!}$$ which I figured out from extensively playing around with desmos and the equations alternate + or -: $$(-1)^n $$ the first term:
So the first term, going from the end of the integral $(720)(-(1/2187) \sin(3y))$ forward, should therefore be $720$ . $720$ in this case is $6!$ , so I can describe this in my series as $a!$ . However I've gotten stuck here. For all terms after $n=0$ , there is a pattern of $\frac{a!}{n!}$ but at $n=0$ , I can't do that. Is there a way I can keep this pattern, but make it so that when n=0, that denominator becomes $1$ , as it is? So basically this would work if: $\frac{720}{0} = 720$ , $\frac{720}{1} = 720$ , $\frac{720}{2} = 360$ , $\frac{720}{6} = 120...$ so I need that 0 there to act like a $1$ . How would I do this? second term:
for the second term, I can say: $$-\frac{1}{b^{a+1-n}}(...)$$ where (...) is the alternating sin(bx) or cos(bx). I wonder if putting that entire sum from above in here would actually work though, or if I'd have to figure it out more there. So I need more help with the ""first term"" section and possibly with the ""second term"" section. Thank you! EDIT #2: So here's what I have: $$\sum_{n=0}^{a}(-1)^n(\frac{a!}{n!}x^n)((-\frac{1}{b^{a+1-n}})(\frac{(-1)^n((bx)^{2n}+(-\frac{\pi}{2}+(bx))^{2n})}{(2n)!})) $$ Okay so it's clunky and not at all elegant, but that's okay. But most importantly, it doesn't work. The terms do not come out to $(720)(-1/2187\sin(3x)) - (720x)(-1/729\cos(3x)) +.... + (x^6)(1/3\sin(3x))$ as they should. So what did I do wrong here? How can I fix it so that it works?","['integration', 'indefinite-integrals', 'calculus', 'sequences-and-series']"
4747206,When can we flip the entire grid if we contunue flipping the cells of a subgrid？,"Consider a 50-by-50 square grid. Initially all cells have the number $0$ . For each operation, one selects a 1-by-7 or 7-by-1 subgrid and changes the number in the cells of the subgrid from 1 to 0 and from 0 to 1. Is it possible that after a number of operations, all cells have the number $1$ . Context: I remembered that I saw a similar question somewhere (but I cannot remember exactly where). In that question, each operation flips 2-by-2 subgrid. I am interested in this question, but want to do things generally, i.e., to find the general conditions on the size of the square grid and the size of the subgrid, and I thought starting with subgrids of 1-by-m and m-by-1 would be easier.) I am also wondering whether there is any conclusion if we generalize $50$ to $n$ and $7$ to $m$ ?","['chessboard', 'combinatorics', 'discrete-mathematics']"
4747240,Epsilon-Delta proof for Multivariate Limit in Polar Coordinates,"I'm trying to construct an $\varepsilon-\delta$ proof for the following limit, $$\lim_{(x, y) \rightarrow (0, 0)}\frac{4xy}{x^2 + y^2 + 2} = 0,$$ and I would like to see if my proof is sound having converted cartesian coordinates to polar coordinates. My proof is as follows: Let $x = r\cos\theta$ , $y = r\sin\theta$ with $r = \sqrt{x^2 + y^2}$ , then $$\lim_{(x, y) \rightarrow (0, 0)}\frac{4xy}{x^2 + y^2 + 2} = \lim_{r \rightarrow 0}\frac{4r^2\cos\theta\sin\theta}{r^2+2}$$ . We note that $\forall\theta\in\mathbb{R}, 0 \leq|\cos\theta\sin\theta| < 1$ . Now, suppose that $0 < |r| < \delta$ , for $\delta > 0$ . We take $\delta < \frac{\varepsilon}{4}$ , with $\varepsilon > 0$ , then $0 < |r| < \delta < \frac{\varepsilon}{4}$ . $$
\begin{align*}
  \left|\frac{4r^2\cos\theta\sin\theta}{r^2+2} - 0\right| &< \left|\frac{4r^2\cos\theta\sin\theta}{r^2}\right|\\ &= 4|\cos\theta\sin\theta| \\
&<4|r||\cos\theta\sin\theta|\\
&<4|r| \\
&<4 \cdot \frac{\varepsilon}{4} \\
&= \varepsilon
\end{align*}$$ Thus for $0<|r|<\delta$ , we have $\left|\frac{4r^2\cos\theta\sin\theta}{r^2+2}\right| < \varepsilon$ . Hence, we conclude that $$\lim_{(x, y) \rightarrow (0, 0)}\frac{4xy}{x^2 + y^2 + 2} = \lim_{r \rightarrow 0}\frac{4r^2\cos\theta\sin\theta}{r^2+2}$$ exists and is equal to $0$ . Anyways, any feedback of how I could improve the proof (if it is correct) or correct anything would be greatly appreciated.","['limits', 'multivariable-calculus', 'polar-coordinates', 'real-analysis']"
4747300,"If $a,b,c >0 : ab+bc+ca=3,$ find maximal value $\sum\dfrac{a\sqrt{a^2+2}}{a^2+3}$","Question If $a,b,c >0 : ab+bc+ca=3,$ find maximal value $$M=\dfrac{a\sqrt{a^2+2}}{a^2+3}+\dfrac{b\sqrt{b^2+2}}{b^2+3}+\dfrac{c\sqrt{c^2+2}}{c^2+3}$$ By $a=b=c=1,$ I try prove $M\le \dfrac{3\sqrt{3}}{4}$ or $$\sum_{cyc}(ab+ac)\sqrt{a^2+2}\le \dfrac{3\sqrt{3}}{4}(a+b)(b+c)(c+a)$$ I don't know how to find upbound $\sum_{cyc}(ab+ac)\sqrt{a^2+2}$ I need some advices. Thanks.","['multivariable-calculus', 'cauchy-schwarz-inequality', 'symmetric-polynomials', 'inequality', 'convexity-inequality']"
4747312,"N people in a pit are required to press their kill buttons one at a time, what percentage of the initial population is expected to live on?","I posed this problem for myself based on a simpler problem I saw on reddit, here is the more detailed version of my problem: The game master traps N people in a pit and equips them with a sort of kill button. When the button is pressed for the first time it will kill a random person, possibly even killing the button presser. The button cannot be pressed a second time (it simply wont do anything). The game master has each person take a turn pressing their button (if they are still alive). What percentage of the initial group of N people will remain? I am more specifically after $\lim_{n\to \infty} \frac{F(n)}{n}$ Where F(n) is the number of people who remain from an initial population of n people. I made a basic computer simulation of this scenario for an initial population of n people. As I plugged in bigger n values it became obvious that the percentage remaining was approaching 1/e. I've been trying to show on paper that $\lim_{n\to \infty} \frac{F(n)}{n} = \frac{1}{e}$ and can't seem to get it, help would be appreciated. Some useful information: A person can only press their button once. The people press their buttons one at a time. The button will always kill someone. (It will not try to kill an already dead person) The button can kill the button presser. Every living person has an equal chance of being chosen by the button. Information that I have gathered on the problem: The best case scenario happens when the kill buttons always choose to kill someone who has yet to use their button. Resulting in a remaining population of n/2. The worst case scenario is when the kill button kills the user every time, resulting in a remaining population of 0. The functional equation $$f(n,p) = \left(\frac{n}{p}\right)f(n-2,p-1)+\left(1-\frac{n}{p}\right)f(n-1,p-1)$$ With base cases: $$f(0,p)=p$$ $$f(1,p)=p-1$$ Represents the expected number of survivors for a given initial population of p where only the first n are assigned kill buttons. For which $f(n,n)$ is the same as $F(n)$ that I defined earlier. Easier reddit question: n people in a room randomly vote for $1$ person to be killed. When the voting period is over anybody with $1$ vote or more gets killed. What percentage of the initial n people survive? Important info: Multiple people can vote for the same person. Everyone gets $1$ vote. All of the deaths happen at the same time. Answer: The probability that someone votes for you is $\frac 1n$ , so the probability that someone does not vote for you is $1-\frac 1n$ . Then the probability that nobody votes for you is $\left(1-\frac{1}{n}\right)^n$ . The limit as n goes to infinity of that expression is also $\frac 1e$ . Why do these problems seem to have the same answer? My problem seems to be fundamentally different from the reddit one as: In my scenario one person kills one person, multiple people can't be responsible for a single death. In my scenario it is possible for someone to die before or after they kill someone themselves, where that isn't true in the case of the reddit problem.","['calculus', 'combinatorics', 'probability', 'recreational-mathematics']"
4747316,Validity of L'Hospital rule in a specific case [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 10 months ago . Improve this question Question : Evaluate the following limit $$\lim_{x\to 0} \frac{\log_{\sin^2(x)}\cos(x)}{\log_{\sin^2\left(\frac{x^2}{2}\right)}\cos\left(\frac{x}{2}\right)}$$ My Attempt : $$ \frac{\ln \cos x}{\ln \cos\frac{x}{2}}\cdot \frac{\ln \sin^2(\frac{x^2}{2})}{\ln \sin^2 x} $$ so can you (always) apply L'Hospital rule to $\frac{0}{0}\cdot\frac{\infty}{\infty}$ separately on each indeterminate form or is there any contradiction...","['limits', 'calculus', 'algebra-precalculus']"
4747339,A math olympiad polynomial question,"Let $a, b, c$ be roots of  the polynomial $x^3-3x^2-4x+5$ . Compute the remainder when $[\frac{a^4+b^4}{a+b} + \frac{b^4+c^4}{b+c} + \frac{c^4+a^4}{c+a}]$ is divided by $7$ , where $[x]$ denotes floor function. Well, I tried finding out the value of the summation of the fourth powers and then substituting them in the individual terms. I got something like this $[\frac{a^4+b^4}{a+b} + \frac{b^4+c^4}{b+c} + \frac{c^4+a^4}{c+a}]$ = $[\frac{197 - c^4}{3-c} + \frac{197 - a^4}{3-a} + \frac{197 - b^4}{3+b}]$ I am not sure how to proceed after this, as by taking the L.C.M of the denominators and converting it into an expanded form would result in a long fraction. Is there any more efficient way to solve this?","['contest-math', 'algebra-precalculus', 'polynomials']"
4747350,Linearly independent vectors generated from matrices,"I came across this question recently: Do there exist $n$ real $n\times n$ matrices $A_1, A_2, \cdots, A_n$ , such that for any $n$ -dimensional non-zero vector $v$ , $A_1 v, A_2 v, \cdots, A_n v$ are always linearly independent? It seems like a usual linear algebra question, but I can't think of any idea to approach it, and I can't come up with a counterexample either. Is there anything I've missed? Any help would be appreciated. Edit: So for $n=2$ , this is obvious as we can just take two rotation matrices with different angles. The resulting two vectors will always be on different lines. For $n>2$ however, rotation won't work as there'll be an axis of rotation, on which the vectors are eigenvectors, i.e., on the same line after rotation.","['vectors', 'vector-spaces', 'matrices', 'abstract-algebra', 'linear-algebra']"
4747404,Are two triangles congruent if the distances from the incenter of the triangle to the three vertices correspond to equal distances?,"It is known that three given positive numbers determine a unique triangle with the angle bisectors lengths equal to these numbers. Therefore two triangles are congruent on three angle bisectors. Immediately following the above conclusion, here's an interesting question. Are two triangles congruent if the distances from the incenter of the triangle to the three vertices correspond to equal distances? My attempts：Let the  incenter of triangle $ABC$ be point $P$ , and $AB=c,AC=b,BC=a$ , $AP=x,BP=y,CP=z$ , then we can get $$
\begin{cases}
x^2=\dfrac {bc(b+c-a)}{a+b+c}\\
y^2=\dfrac {ac(a+c-b)}{a+b+c}\\
z^2=\dfrac {ab(a+b-c)}{a+b+c}
\end{cases}
\qquad (*)
$$ The problem is equivalent to the question of whether $a,b,c$ are unique given positive numbers $x,y,z$ that satisfy the system of equations $(*)$ . However,It is difficult for me that no substantial progress was made. Is there a better way to solve this problem?","['triangles', 'geometry']"
4747453,Simulate a 10 sided die with a 6 sided die [duplicate],"This question already has answers here : How to generate a random number between 1 and 10 with a six-sided die? (21 answers) Closed 11 months ago . How do you simulate a 10 sided die using a 6 sided die. I came across this question in my Probability textbook My 2 approaches: E[x] of 10 sided die = 5.5,
E[x] of 6 sided die = 3.5 1st Method: Roll the 6 sided twice. Now on the 3rd roll keep rolling until you get a (1,2,3,4,5) and reject/reroll a 6.
Now divide the result of the 3rd roll by 2 and subtract from the sum of the 1st 2 rolls. E[x] = 3.5 + 3.5 - (3/2) = 5.5 2nd Method: Roll the 6 sided die. Now for the 2nd roll keep rolling until you get a (1,2, or 3) i.e. reject/reroll on (4,5, or 6). E[x] = 3.5 + 2 = 5.5 I realize that the 1st method is probably better in terms of fewer rolls required on average I am wondering if there is some other more optimal solution that reduces the average number of rolls required. Thanks!",['probability']
4747474,Does the limit of these sums converge/diverge?,"I am looking at these two sums: $$s_N=\sum_{n=0}^N\frac{\sin n}{n!}x^n, \qquad \text{and} \qquad c_N=\sum_{n=0}^N\frac{\cos n}{n!}x^n, \quad \text{for} \quad x\in \mathbb{R}. $$ I am interested in the the expression $Q(x,N)=\sqrt{s_N^2+c_N^2}$ and the question of whether it converges towards a function or not. In particular, I have the suspicion that: $$\lim_{N \to \infty}Q(x,N) = e^{qx}, \qquad \text{where} \qquad 0.5403 < q < 0.5404.$$ I only suspect this because I plugged $Q$ into a graphics calculator and nudged $q$ around for a bit. But is my assumption true? How can I prove that $Q$ diverges or converges? When trying to calculate $s_N^2+c_N^2$ , I tried the formula $$\left( \sum_{i=0}^N a_i \right)^2= \sum_{i=0}^N a_i^2+2\sum_{i<j}^N a_ia_j$$ which, by using $\sin^2n+\cos^2n =1,$ gives me $$s_N^2+c_N^2 = \sum_{n=0}^N\left( \frac{x^{2n}}{n!^2} \right) + 2\sum_{n<m}^N \frac{\sin(n)\sin(m)+\cos(m)\cos(n)}{n!m!}x^{n+m},$$ but I don't know how to continue from here.","['summation', 'arithmetic', 'sequences-and-series', 'limits', 'convergence-divergence']"
4747519,"A specific ""gaussian"" integral","I wish to compute the following function $$\forall\ \mathbf{b}\in \mathbb{R}^3,\quad V(\mathbf{b}) := \int_{\mathbb{R}^{3}} \frac{ e^{-\alpha \mathbf{k}^2 + \mathbf{b}\cdot \mathbf{k} } }{ \sqrt{\mathbf{k}^2+m^2}}\,  d^{3} \mathbf{k} \quad \text{where}\enspace \mathbf{k}^2:= \lVert \mathbf{k}\rVert^2 $$ I thought of looking for an ODE it would satisfy, but to no avail up to now. A change to spherical coordinates with "" $\mathbf{b}$ "" as the $z$ axis yields $$\begin{aligned}
& \iiint \frac{ e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} + \hspace{.5pt}b\, k\hspace{.5pt} \cos \theta } }{ \sqrt{k^2+m^2} }\, k^2 \hspace{.7pt} \sin \theta\,  d k\, d\theta\, d\varphi = 2\hspace{.7pt}\pi \int_0^{+\infty} \frac{ k^2\, e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} } }{ \sqrt{k^2+m^2} }\, \left[\frac{e^{\hspace{.5pt} b\, k\hspace{.5pt} \cos \theta}}{- \hspace{.5pt} b\, k}\right]_0^{\pi}\, dk \quad \text{where}\enspace k:= \lVert \mathbf{k}\rVert,\ b:= \lVert \mathbf{b}\rVert \\
&= \frac{2\hspace{.7pt} \pi}{b} \int_0^{+\infty} \frac{ k\left( e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} + b\hspace{.3pt} k } - e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} - b\hspace{.3pt} k } \right)}{ \sqrt{k^2+m^2}}\, dk
\end{aligned}$$ Inserting then the following definition of Hermite polynomials : $\quad \displaystyle e^{2\hspace{.3pt}x\hspace{.3pt}t - t^2} = \sum_{n=0}^{+\infty} H_n(x)\, \frac{t^n}{n!} $ with $t:= \sqrt{\alpha}\, k,\ x := \frac{b}{2\sqrt{\alpha}} $ $$ \begin{aligned}
 V(b) &= \frac{2\hspace{.7pt} \pi}{b} \int_0^{+\infty} \frac{k}{ \sqrt{k^2+m^2}}  \sum_{n=0}^{+\infty} \left( H_n\left(\frac{b}{2\sqrt{\alpha}}\right) - H_n\left(\frac{-\, b}{2\sqrt{\alpha}}\right) \right) \frac{\left(\sqrt{\alpha}\, k\right)^n}{n!} \, dk \\
&=  \frac{2\hspace{.7pt} \pi}{b} \sum_{n=0}^{+\infty} H_n\left(\frac{b}{2\sqrt{\alpha}}\right) \big( 1 + (-1)^{n+1}\big) \frac{\left(\sqrt{\alpha}\right)^n}{n!}  \int_0^{+\infty} \frac{k^{n+1}}{\sqrt{k^2+m^2}} \, dk
\end{aligned}$$ The last integral formally looks like a Beta function after the following change of variable $\genfrac{[}{]}{0pt}{0}{l:= \frac{k^2}{m^2}}{dl = \frac{2}{m^2}\, k\, dk}$ $$ \int_0^{+\infty} \frac{k^{n+1}}{m \sqrt{k^2/m^2 + 1}} \, dk = \frac{m}{2}\int_0^{+\infty} \frac{m^n\, l^{\frac{n}{2}}}{\sqrt{l+1}} \, dl  = \frac{m^{n+1}}{2} B\left(\frac{n}{2}+ 1 , -\frac{n+1}{2}\right)$$ Indeed, the integral is bluntly divergent and one should not have permuted sum and integral... Question: can $V$ be expressed in terms of special functions? or as a power series in $b$ ? (in fact I did found one not for $\lVert \mathbf{b}\rVert$ , but as a function of each component $b_1, b_2, b_3$ and it is desperately involved... so I still keep on looking for something more reasonnable...) I also tried integration by parts on $$V(b) = \frac{2\hspace{.7pt} \pi}{b} \int_0^{+\infty} \frac{ k }{ \sqrt{k^2+m^2}} \times e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k)\, dk$$ and complex integration... EDIT: I meant PDE above... in fact I've just found an ODE for $$U(b):= \int_0^{+\infty} \frac{ e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk$$ Indeed $$ \begin{aligned}
U'(b) & = \int_0^{+\infty} \frac{ k\, e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk\\
(\text{Int. by part}) &= \left[ \sqrt{k^2+m^2}\, e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k) \right]_0^{+\infty} - \int_0^{+\infty} \sqrt{k^2+m^2} \left[ (- 2 \alpha k) e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k) +b\, e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) \right]\, dk
\end{aligned}$$ The ""boundary term"" in the integral by part vanishes, and the other can be expressed in terms of the following: $$ U''(b) = \int_0^{+\infty} \frac{k^2 e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk = \int_0^{+\infty} \frac{\big( k^2 +m^2 - m^2\big) e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk $$ Then consider also $U'''(b) +m^2 U'(b)$ . Shake everything, and you  should find a linear ODE... with non constant coeff, so there will still be work...","['gaussian-integral', 'improper-integrals', 'ordinary-differential-equations', 'analytic-functions']"
4747581,Preserving the symplectic 2-form vs phase space volume,"Say I have a Hamiltonian system of $N$ particles in 3D-3V phase space. I'm using some sort of update scheme taking the system from $t^{n-1}$ to $t^{n}$ to $t^{n+1}$ . I want to know if the update scheme is symplectic, volume preserving, or neither, but I want to test this in an experimental way.* If it's symplectic, then the 2-form $\omega = dq \wedge dp = \sum_{i}^{N}{dq_i \wedge dp_i}$ is preserved over time (minor question here, are $dq$ and $dp$ vectors or scalars? I'm assuming vectors representing individual particles in phase space, but I could see an argument made for simply iterating over $3N$ $q,p$ pairs rather than $N$ vectors), which in turn implies phase space volume is preserved. My question is, how do I test this computationally? My current thought: Compute $dq_i^n = q_i^{n}-q_i^{n-1}, dp_i^{n} = p_i^{n}-p_i^{n-1}$ , which are vectors with three components. Taking the 2-form is, in the case of vectors in $\mathbb{R}^3$ , just the cross product (from what I understand it's a bit more complicated than ""just the cross product"" in that the result is a two-vector or something to that effect, but I'm only concerned in having something to compare over the simulation, so as long as I'm on the right track here I'm content). Sum up all $dq_i^n \wedge dp_i^n$ . Do the same for $dq_i^{n+1} \wedge dp_i^{n+1}$ . If the scheme is symplectic, then the difference between these two values should be bound over the simulation (unsure if I'm comparing individual components or the norms of those values). For phase space volume preservation, simply compute $dV_i^n = dq_{1i}^ndq_{2i}^ndq_{3i}^ndp_{2i}^ndp_{1i}^ndp_{3i}^n$ , take the product $\Pi_i^N{dV_i^n}$ up over all $i$ , do the same for $dV_i^{n+1}$ , compare over timesteps, the difference should be bound over the simulation. Frustratingly, I cannot find any resources to confirm or deny this.** So I suppose my question is in two parts, first am I right/where am I wrong, second is what are some good resources for looking more into this? Incidentally, symplecticity $\implies$ phase space volume preservation, but whenever I try to prove that the above follows that rule I very quickly get lost in about a million cross terms, which either means I need to get back to it or I'm on the wrong track. *I know if $\Omega$ is my update matrix, then $\Omega^TJ\Omega = J$ implies symplecticity and $det(\Omega)=1$ implies volume preservation. This is great, but I'd like to have a practical test for my code. I've been using the Hamiltonian as my litmus test, making sure it's bound over time, but from what I understand this doesn't necessarily track phase space volume preservation or symplecticity, just total energy in the system. **Every paper, website, and youtube video I've seen has shown the same picture of a box or a smiley face or whatever in phase space being stretched with the area/volume being preserved over time, which is helpful in getting an intuitive idea, not helpful in actually solving the problem.","['symplectic-geometry', 'classical-mechanics', 'symplectic-linear-algebra', 'differential-forms', 'differential-geometry']"
4747582,Do 'Anti-Fréchet Derivatives' work similar to typical anti-derivatives? Are there two ways different ways to define them?,"Assume a function $f:L_2(R^{+}):R$ is frechet differnetiable in $x\in L_2(R^{+})$ in that there exists a unique function $D(x_i,x)$ (where $x_i\in R^{++}$ is an element of x) such that: $f(x+h)=f(x)+\int_i D(x_i,x)h_idi + o (||h||_2)$ Further assume we can define the second cross partial, $DD(x_i,x_j,x)$ in a similar way. Do we then have that $\int_{-\infty}^{x_j}  DD(x_i,y,x) dy= D(x_i,x)+C?$ It seems like there would be 2 ways to interpret $\int_{-\infty}^{x_j}  DD(x_i,y,x) dy$ . The first being what feels like the ""typical"" sense, where the value of the second item ( $y$ or $x_j$ ) increases along the integral, but also the $x_j$ that lives inside of $x$ increases too, making it seem analogous to the typical partial derivative leading to my feeling the fundamental theorem of calculus holds. The second interpretation is to take the integral while ignoring the fact that $x_j$ is an element of $x$ , which seems to suggest FTC would not necessarily hold. Both interpretations seem like they could arise in certain contexts, but I am not aware of an explicit discussion of the second, though it arises in an application I have due to the way a limit is constructed. Is there a name or way of identifying the different ways of taking the integral, and does anyone know of any resources to help with understanding the second?","['integration', 'frechet-derivative', 'calculus', 'partial-derivative', 'derivatives']"
4747586,Good bound on $ \max_{|z| \le A} \left | \frac{\mathbb{E} [ X^2 \exp( -z X) ]}{ \mathbb{E} [ X \exp( -z X) ]} \right| $ where $z \in \mathbb{C}$,"Consider a random variable $X \in (0,1]$ and consider the following quantity \begin{align}
\max_{|z| \le A} \left |  \frac{\mathbb{E} [  X^2 \exp( -z X) ]}{ \mathbb{E} [  X \exp( -z X) ]} \right|   
\end{align} in the above $z \in \mathbb{C}$ . The question is the following: Can we find a good upper bound on this quantity? Here is what I did: \begin{align}
\max_{|z| \le A} \left |  \frac{\mathbb{E} [  X^2 \exp( -z X) ]}{ \mathbb{E} [  X \exp( -z X) ]} \right|  & \le  \max_{|z| \le A}   \frac{\mathbb{E} [ \left |  X^2 \exp( -z X) \right| ]}{|  \mathbb{E} [  X \exp( -z X) ] | } \\
&\le \exp(A) \max_{|z| \le A}   \frac{\mathbb{E} [  X^2   ]}{ | \mathbb{E} [  X \exp( -z X) ]| }\\
&\le \exp(A)  \frac{\mathbb{E} [   X^2   ]}{ \exp(-A)  \mathbb{E} [  X  ] }   \text{ **Edit:** this step is actually not true see comment below }\\
&\le \exp(2 A)  \frac{\mathbb{E} [  X^2   ]}{   \mathbb{E} [  X  ] }\\
& \le \exp(2 A)
\end{align} My question is, can we do better?  Can some kind of joint optimization be done? Edit: It might be useful to add some examples. If $X$ uniform, then we have that \begin{align}
\frac{\mathbb{E} [  X^2 \exp( -z X) ]}{ \mathbb{E} [  X \exp( -z X) ]} = \frac{z}{z-e^z+1}+\frac{2}{z}
\end{align} Edit2: Note that $ L(z) = \mathbb{E} [  \exp( -z X) ]$ is the Laplace transform of a random variable $X$ . The question can be equivalently rested as \begin{align}
\max_{|z| \le A}  \left|  \frac{L^{(2)}(z)}{L^{(1)}(z)} \right| 
\end{align}","['complex-analysis', 'probability-distributions', 'laplace-transform', 'probability']"
4747611,"$\ \forall x_1,x_2,...,x_n \in \mathbb{R} (x_i\not=x_j)$ in the range of $[-1,1]$ prove:$\sum_{i=1}^{n}\frac{1}{\Pi_{k\not=i}|x_k-x_i|}\ge2^{n-2}$","$\ \forall$ $x_1,x_2,...,x_n$ $\in \mathbb{R}$ $(x_i\not=x_j)$ in the range of $[-1,1]$ prove : $$\sum_{i=1}^{n}\frac{1}{\Pi_{k\not=i}|x_k-x_i|}\ge2^{n-2}$$ my attempt : $$p(x) = \sum_{i=1}^{n}\left(p(x_i)\prod_{k\not=i}\frac{x-x_k}{x_i-x_k}\right)$$ by triangle inequality : $$\left|p(x)\right| \leq \sum_{i=1}^{n}\left|p(x_i)\right|\prod_{k\not=i}\bigg|\frac{x-x_k}{x_i-x_k}\bigg|$$ now if $p(x) = \sum_{i=0}^{n-1}a_ix^i$ then : $$\left|\frac{p(x)}{x^{n-1}}\right|\leq\sum_{k=1}^{n}\frac{\left|p(x_k)\right|}{\prod_{k\not=j}\left|x_k-x_j\right|}\Bigg|\prod_{k\not=j}(1-\frac{x_i}{x})\Bigg|$$ then if we $x\to\infty$ we get : $$\left|a_{n-1}\right|\leq\sum_{k=1}^{n}\frac{\left|p(x_k)\right|}{\prod_{k\not=i}\left|x_k-x_i\right|}$$","['limits', 'lagrange-interpolation', 'polynomials', 'triangle-inequality']"
4747620,Annihilation of some Kähler differential.,"It is mentioned in the proof of
Proposition 2.(2) of this paper that if $K$ is a field of characteristic $p$ and $$ \sum_{\sigma\neq 0}a_1^{\sigma(1)}\cdots a_i^{\sigma(i)}z_{\sigma}^p=x^p $$ where $a_i,x,z_{\sigma}\in K$ (with $x,z_{\sigma}$ not all $0$ ) and $\sigma$ runs through the set of maps $\{1,\ldots,i\}\longrightarrow\{0,\ldots,p-1\}$ that are not identically $0$ , then one has $$ \frac{da_1}{a_1}\wedge\cdots\wedge\frac{da_i}{a_i}=0 $$ in the module of absolue Kähler differentials $\Omega_{K/\mathbb{Z}}^i$ though I can't see why. I want to use the fact that, $K$ being of characteristic $p$ , $d(x^p)=0$ but I don't really know how to link the LHS with the differential above.","['number-theory', 'algebraic-geometry']"
4747632,Does there exist a continuous open map from the closed annulus to the closed disk?,"In this MSE post A function $f:\mathbb{R}^2\to\mathbb{R}^2$ that is open and closed, but not continuous. , user Moishe Kohan provides an example of a non-continuous open and closed (""clopen"") function $\mathbb R^n \to \mathbb R^n$ for $n\geq 3$ by citing and using the following propositions of David Wilson: Propositions 1 and 3 of D. Wilson, Open mappings of the universal curve onto continuous curves. Trans. Amer. Math. Soc. 168 (1972), 497–515. [Moishe Kohan's] Proposition [cooked up from Wilson's propositions]. Let $I^n$ be the closed $n$ -dimensional cube, $n\ge 3$ , and $J^n\subset int(I^n)$ is a closed subcube. Let $Q$ denote the
interior of $J^n$ . Then, there exists an open continuous map $g: I^n \setminus Q\to I^n$ which equals the identity on the boundary of $I^n$ and
sends $\partial Q$ to the interior of $I^n$ . I am interested in the $2$ -dimensional case, for which I propose the following conjecture: Conjecture: there are no continuous open (w.r.t. subspace topologies) maps $f$ from the closed annulus $\mathbb A:= \{x\in \mathbb R^2: 1\leq |x|\leq 2\}$ to the closed disk $\mathbb D := \overline{B(0,2)}$ , which restricts to the identity map on the outer boundary circle $\partial \mathbb D = C(0,2)\subseteq \mathbb A$ , and sends the inner boundary circle $C(0,1)$ to the interior of $\mathbb D$ . I was not able to prove or disprove this conjecture for even the Simpler Case: where $f$ maps the inner boundary circle $C(0,1)$ to the single point $0\in \mathbb D$ . Some attempts I made on the Simpler Case . Attempt 1: Because we want $f$ to be open, we in particular want any open neighborhood in $\mathbb A$ of any boundary point $b\in C(0,1)$ to map to an open set containing $0\in \mathbb D$ , in particular containing some $B(0,\epsilon)$ . My idea was map circles $C(0,1+\eta)$ to circles $C(0,\eta)$ , and to to ""swirl""/""smear"" the circles $C(0,1+\eta)$ more and more extremely as $\eta \searrow 0$ , so that even a very small neighborhood $B(b,\delta)\cap \mathbb A$ of $b\in C(0,1)$ containing just a $\approx \frac{2\delta}{2\pi}$ fraction of the circles $C(0,1+\eta)$ for small enough $\eta>0$ would get ""swirled""/""smeared"" to contain the entire circle $C(0,\eta)$ . So something like $f$ maps $z:=re^{i\theta}\in \mathbb A$ (thinking of $\mathbb R^2$ as the complex plane) to $(r-1)e^{i\theta\cdot \frac{1}{r-1}}$ , mapping an arc of the circle $C(0,1+\eta)$ of arclength $\ell$ to an arc of the circle $C(0,\eta)$ with arclength $\ell \cdot \frac{1}{r-1}$ . Unfortunately, the corresponding formula for $f$ would be $$f(z)=\frac{|z|-1}{|z|^\frac{1}{|z|-1}} \cdot z^{\frac{1}{|z|-1}},$$ which maybe looks fine, until one remembers that raising a complex number to a non-integer power needs (non-continuous) branch cuts to define. :( Attempt 2: One can make a continuous ""swirling only"" (no ""spreading"", i.e. no multiplicative factor in the argument variable) by mapping $r e^{i\theta} \mapsto (r-1) e^{i\theta + i\cdot \frac{1}{r-1}}$ , i.e. $$f(z) = \frac{|z|-1}{|z|} \cdot z \cdot e^{\frac{1}{|z|-1}},$$ which I think is continuous, and does ""swirl"" a very small neighborhood $N_b:= B(b,\delta)\cap \mathbb A$ to something that does sort of ""wrap around"" $0 \in \mathbb D$ , but in doing so has a lot of holes: this $f$ preserves that proportion of the arclength of $N_b \cap C(1+\eta)$ , meaning $\text{arclength}(N_b \cap C(1+\eta)) = \text{arclength}(f(N_b) \cap C(0,\eta))$ , so $f$ can't possibly map $N_b$ to something containing $B(0,\epsilon)$ . Attempt 3: Finally, because of this ""monodromy problem"" of defining this ""swirling/smearing"" map on the entire circle $C(0,1+\eta)$ , I had an idea of cutting up $C(0,1+\eta)$ into $\frac 1\eta$ many pieces (restricting to $\eta \in \{\frac 1n: n=2, 3, 4, \ldots\}$ ), making $f$ ""swirl/smear"" each of those pieces into the entirety of $C(0,\eta)$ , thus guaranteeing that $f(N_b)$ contains complete circles $C(0,\eta)$ arbitrarily close to $0\in \mathbb D$ . More precisely, I would partition $C(0,1+\frac 1n)$ into $2n$ equally sized, equally spaced pieces, and define $\{K_{n, i}\}_{i=1}^n$ to be the closures of the say odd-indexed pieces. I can map $K_{n,i}$ continuously to cover the entirety of $C(0,\frac 1n)$ . Then for any $N_b:= B(b,\delta)\cap \mathbb A$ , it does contain some $K_{n,i}$ for all $n$ sufficiently large, and hence $f(N_b)$ contains $C(0,\frac 1n)$ for all $n$ sufficiently large.
I can extend this $f$ defined on $C(0,1) \cup \bigcup_{n=2}^\infty\bigcup_{i=1}^n K_{n,i}$ via the Tietze extension theorem to a continuous function $\mathbb A \to \mathbb D$ , but again I really doubt it is a an open map. (Cross posted to MO https://mathoverflow.net/questions/452140/does-there-exist-a-continuous-open-map-from-the-closed-annulus-to-the-closed-dis upon suggestion from comments)","['open-map', 'analysis', 'real-analysis', 'continuity', 'general-topology']"
4747654,"Show that $K(t) = \int_0^t \mathrm{e}^{-Cs} \, D \, \mathrm{e}^{-C^\intercal s} \,\mathrm{d} s$ satisfies a given equation involving $K(\infty)$","Assume that for each $t \geq 0$ , we have a matrix $$K(t) = \int_0^t \mathrm{e}^{-Cs} D\, \mathrm{e}^{-C^\intercal s} \mathrm{d} s$$ where $D \in \mathbb{R}^{d \times d}$ is constant, symmetric, and positive semi-definite and that $C \in \mathbb{R}^{d \times d}$ . It is claimed in some papers (without details unfortunately) that we have $$K_\infty = K(t) + \mathrm{e}^{-Ct}\,K_\infty\, \mathrm{e}^{-C^\intercal t} \label{1}\tag{1}$$ for all $t \geq 0$ , where $$K_\infty = \int_0^\infty \mathrm{e}^{-Cs} D\, \mathrm{e}^{-C^\intercal s} \mathrm{d} s.$$ May I know how can we check the validity of equation \eqref{1} from basic matrix calculus? Remark: For a given matrix $A \in \mathbb{R}^{d \times d}$ , we denote its transpose by $A^\intercal$ .","['matrices', 'matrix-equations', 'matrix-calculus']"
4747687,Writing $(-z)^\alpha$ in terms of $z^\alpha$,"I have a very silly confusion in complex analysis. Let $z\in \mathbb{C}$ and $\alpha\in \mathbb{C}$ . Now consider $(-z)^\alpha$ and suppose that we want to relate this to $z^\alpha$ . One thing that I immediately notice is that $(-z)^{\alpha} = (-1)^\alpha z^\alpha$ seems to be ambiguous. In fact, we can write $-1 = e^{i\pi+2\pi k i}$ for $k\in \mathbb{Z}$ , but the choices seem to give different results depending on what $\alpha$ is. In fact we have $$(-z)^\alpha = e^{2\pi k\alpha i} e^{i\alpha \pi} z^\alpha\tag{1}.$$ Now the prefactor $e^{2\pi k\alpha i}$ is in general non-trivial if $\alpha\notin \mathbb{Z}$ . I feel that the right way to write $(-z)^\alpha$ in terms of $z^\alpha$ is by taking into account the branch cut, but I feel a bit confused in how this should be done correctly. So what is going on here? Why writing $(-z)^\alpha$ in terms of $z^\alpha$ seems highly ambiguous? How to identify the correct choice in a given situation?","['complex-analysis', 'branch-cuts', 'branch-points', 'complex-numbers']"
4747697,Doubt regarding Continuous Random Variables,"I do know that the probability of random variables (say $X$ and $Y$ ) taking specific values is considered to be zero (i.e. $P(X=x,Y=y) = 0$ ). My doubt, however, is how do I determine the probability of events wherein ( $a<X<b,Y=y$ )(i.e. $P(a<X<b,Y=y)$ . I am aware that the probability of events such as ( $a<X<b\mid Y=y$ ) can be evaluated by finding the conditional probability density function of $X$ and integrating the function over the interval $(a, b)$ . But what about the case above? Is it also equal to zero? If so, could you please provide a detailed example to clarify the doubt? I have been struggling with continuous random variables for some time now. Also, are the theorems valid for continuous random variables same/analogous to those of discrete random variables? A detailed explanation would be much appreciated. Thanks in advance :)","['statistics', 'probability-distributions', 'random-variables']"
4747785,Where is my mistake in this problem (finding indefinite integral using trig identities),"I am learning calculus.  Currently I am studying integration using trigonometric identities. I encountered this problem and gave it a try.  My answer was incorrect.  I then sought the correct answer, and I see now why it is correct and why the method used to solve it was better than mine.  Here is my issue: I can not for the life of me find where my error was. Why is this answer wrong? I numbered the steps and explained my thinking.  Here is the problem: $$\int \tan^5(x)\cdot \sec^4(x) \,dx$$ I began by looking over my list of trig identities and derivatives, and I found these two which appeared helpful.  My thinking was that I could get the derivative of $\tan(x)$ and then use the identity to express everything that remains in terms of secants to use u-substitution. $\sec^2(x)-1=\tan^2(x)$ and $\frac{d}{dx}\sec(x)=\tan(x)\cdot sec(x)$ First I got $\tan x \cdot \sec x$ by itself: $$\int \left[\tan^5(x)\cdot \sec^4(x) \right] \,dx = \int \left[\tan^4(x)\cdot \sec^3(x)\cdot \tan(x)\cdot \sec(x) \right] dx$$ Then I broke the remaining tangent terms into $\tan^2x$ terms: $$= \int \left[\tan^2(x)\cdot \tan^2(x) \cdot \sec^3(x)\cdot \tan(x)\cdot \sec(x) \right] \,dx$$ Then I replaced each $\tan^2x$ term by its equivalent term in secants: $$=\int \left[ \left(\sec^2(x)-1 \right)\cdot \left(\sec^2(x)-1 \right)\cdot \sec^3(x)\cdot \tan(x)\cdot \sec(x) \right] \,dx$$ Then I distributed all my secant terms: $$=\int \left[(\sec^4(x)-2 \sec^2(x)+1)(\sec^3(x))\cdot \tan(x)\cdot \sec(x) \right] \,dx$$ $$=\int \left[ \left(\sec^7(x)-2 \sec^5(x)+ \sec^3(x) \right)\cdot \tan(x)\cdot \sec(x) \right] dx$$ Then I applied this substitution: $u=\sec(x)$ , and then $\,du=\tan(x)\sec(x) dx$ Applying the substitution: $$=\int \left(u^7-2u^5+u^3 \right) \,du$$ Integrating: $$=\frac{u^8}{8}-\frac{2u^6}{6}+\frac{u^4}{4}+C$$ And then substituting back in: $$=\frac{1}{8} \sec^8(x)-\frac{1}{3} \sec^6(x)+\frac{1}{4} \sec^4(x)+C$$ Of course, this answer is wrong. I would love to know where I went wrong! EDIT: I forgot to say, please forgive errors in the mathjax -- I'm a rank beginner!","['integration', 'indefinite-integrals', 'trigonometric-integrals']"
4747833,"Suppose $\lim_{t \to 0}\frac{g(t)}{t}= 1$, $g(0)= 0$, determine $\lim_{(x,y)\to (0,0)} \frac{ x^2 +[g(y)]^2}{[g(x)]^2+y^2}$ if the limit exists.","The problem is the following: Let $g:\mathbb{R}\to \mathbb{R}$ . Suppose $\lim_{t \to 0}\frac{g(t)}{t}= 1$ , $g(0)= 0$ , determine $\lim_{(x,y)\to (0,0)} \frac{ x^2 +[g(y)]^2}{[g(x)]^2+y^2}$ if the limit exists. A classmate of mine in the calculus class gave this exercise to me, and I've been stuck on this exercise for a few days. Intuitively, the limit $\lim_{t\to 0} g(t)/t =1$ tells us that asymptotically $g $ has exactly the order of $t$ as $t\to 0$ . So we may expect that the limit actually approaches to the same limit as: $$
 \lim_{(x,y)\to (0,0)} \frac{x^2+y^2}{x^2+y^2} = 1.
$$ However, I have no idea how to rigourously approach the problem. I've also noticed that the hypothesis also implies that $ \lim_{t\to 0} g(t)/t  =\lim_{t\to 0} [g(t) - g(0)]/t = g^{'}(0) = 1$ , but this seems to lead me to nowhere. I would like to ask how to solve it. Does the limit exist? Or there's a counterexample to this limit? Thanks.","['multivariable-calculus', 'limits', 'calculus']"
4747840,Does Carathéodory's extension of measure space preserve inclusion?,"A measure space $(X,\mathcal{S},\mu)$ can be extended to a measure space $(X,\hat{\mathcal{S}},\hat{\mu})$ as follows: $\mu$ is extended to an outer measure $\mu^\ast$ (defined as the infimum of the sums of $\mu$ -measures of sets in $\mathcal{S}$ covering a subset of $X$ ), $\hat{\mathcal{S}}\mathrel{\mathop:}=\mathcal{M}(\mu^\ast)$ is the set of $\mu^\ast$ -measurable sets, and $\hat{\mu}\mathrel{\mathop:}=\mu^\ast|_{\mathcal{M}(\mu^\ast)}$ is the restriction of $\mu^\ast$ to $\mathcal{M}(\mu^\ast)$ . (Apparently the standard name for this is Carathéodory's extension.) Now, if there is an inclusion of measure spaces $$(X,\mathcal{S},\mu)\subseteq(X,\mathcal{T},\nu),$$ i.e., $\mathcal{S}\subseteq\mathcal{T}$ and $\nu|_{\mathcal{S}}=\mu$ , then does it follow that $$(X,\hat{\mathcal{S}},\hat{\mu})\subseteq(X,\hat{\mathcal{T}},\hat{\nu}) \ ?$$ I'm asking this because the completion of measure space preserves inclusion, and I wondered if the same holds for Carathéodory's extension. I tried to prove it, but unlike completion, it seems highly nontrivial.","['measure-theory', 'outer-measure']"
4747845,Criterion for deducing connectedness of level set of a smooth map,"Suppose that $U\subset \mathbb R^n$ is open and suppose that $f: U\to \mathbb R$ is smooth. Define $M_c= f^{-1}(c)$ . The question is: is $M_c$ connected? Here is an example where $M_c$ is connected for a particular $c$ but not connected for some other values of $c$ : $n=2, f(x,y)= x^2-y^2$ . In this case, $M_0=$ pair of straight lines passing through the origin and therefore $M_0$ is connected. But $M_1=\{(x,y): x^2-y^2=1\}$ is not connected. It suggests that there should be some restriction on $c$ but I’m not sure how to generalise this. Any suggestions on this are welcome. Thanks for your time.","['general-topology', 'differential-geometry']"
4747850,A problem involving complex polynomials,"This question came up in the partial solution to an olympiad problem, in an attempt to complete the proof Suppose $P(a,b)$ is a complex polynomial satisfying $$P(a,b)= 0 \implies a^2 + b^2 = 0$$ Is it necessarily the case that $$P(a,b) = k(a+ ib)^m(a-ib)^n$$ ? My approach was to define the complex polynomial of one argument, $$f_c(a) = P(a,c)$$ And to note that the roots of $f$ satisfy $$ a = \pm ic $$ So that $f_c$ can be factorised as $$ f_c(a) = k(a+ic)^m(a-ic)^n $$ And so for any $c'$ we also have $$f_{c'}(a) = k'(a+ic')^{m'}(a-ic')^{n'} $$ And then I want to somehow use that $f_c$ and $f_{c}'$ must be connected by a continuous function, and we can ""bring"" $c$ and $c'$ as close to each other as we like, so that eventually the exponents and coefficient are forced to be the same. To try and make this more precise, we can take $\max\{m+n,m'+n'\}-1$ derivatives of both $f_c$ and $f_{c'}.$ W.l.o.g we can assume that the max of these is $m+n$ . Then after taking $m+n-1$ derivatives, assuming $m+n>m'+n'$ we are left with a linear polynomial, $f^{(m+n-1)}_c$ and a constant polynomial $f^{(m+n-1)}_{c'}$ . And it seems reasonable that a continuous complex function $g$ of two variables satisfying $$\forall{\epsilon > 0}\forall_{c}\exists_{c',|c-c'|<\epsilon} \\ [g(z,c) \mbox{ is linear and non constant } \land g(z,c') \mbox{ is constant}] $$ Does not exist, but I am struggling to make this intuition precise, and even if it were made precise, it only allows one to conclude that $m+n= m'+n'$ , and I am not sure how this would even help. Any insight into the problem, or highlighting of errors in my thinking so far would be appreciated. Keep in mind that my background in complex analysis is not fantastic, so an elegant proof using complicated theorems may go over my head","['complex-analysis', 'polynomials', 'contest-math']"
4747864,"Hyper-closed-forms of series involving hyperbolic functions:$\sum_{n=1}^{\infty}\frac1{n^2\cosh(\pi n)^2},\frac{\tanh(\pi n)}{n^3}$ and the like.","Series like $$
\sum_{n=1}^{\infty} \frac{1}{n^2\cosh(\pi n)^2},\sum_{n=1}^{\infty} \frac{\tanh\left ( \pi n \right ) }{n^3}
$$ that used to be recognised as ones with no closed-forms, however, actually could be expressed by (generalized) hypergeometric series(being hyper-closed ). We have \begin{aligned}
&\sum_{n=1}^{\infty} \frac{1}{n^2\cosh(\pi n)^2} 
=S+\frac{3\pi^2}{4}-6\pi\ln(2),\\
&\sum_{n=1}^{\infty} \frac{\tanh(\pi n)}{n^3} 
=\pi S+\frac{5\pi^3}{12}-6\pi^2\ln(2)+4\pi G,\\
&\sum_{n=0}^{\infty}
\frac{1}{(2n+1)^2\sinh\left ( \frac\pi2(2n+1) \right )^2} 
=\frac14S +\frac{\pi^2}{48}-\frac{3\pi}2\ln(2)+2G,\\
&\sum_{n=0}^{\infty} \frac{\coth\left ( \frac\pi2(2n+1) \right ) }{
(2n+1)^3} 
=-\frac{\pi}{8}S
-\frac{\pi^3}{96}+\frac{3\pi^2}4\ln(2)-\frac{\pi G}2,
\end{aligned} where $S=\frac{7}{2}{}_6F_5\left ( \frac34,\frac34,1,1,1,\frac{15}8;
\frac78,\frac54,\frac54,2,2;1 \right ) 
+\frac{\pi}{8}{}_5F_4\left ( 1,1,\frac32,\frac32,\frac32;2,2,2,2;1 \right )$ and $G=\sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)^2}$ denotes Catalan's constant. Mathematica codes for numerical verification: NSum[1/(n^2Cosh[Pi n]^2),{n,1,Infinity}] N[7/2HypergeometricPFQ[{3/4,3/4,1,1,1,15/8},{7/8,5/4,5/4,2,2},1]+Pi/8HypergeometricPFQ[{1,1,3/2,3/2,3/2},{2,2,2,2},1]+3Pi^2/4-6Pi Log[2]] NSum[Tanh[Pi n]/(n^3),{n,1,Infinity}] N[7Pi/2HypergeometricPFQ[{3/4,3/4,1,1,1,15/8},{7/8,5/4,5/4,2,2},1]+Pi^2/8HypergeometricPFQ[{1,1,3/2,3/2,3/2},{2,2,2,2},1]+5Pi^3/12-6Pi^2Log[2]+4Pi Catalan] . Question. Are these results known in literature? Could you provide any other ideas to prove the equalities? The original process is somewhat cumbersome, and I'll capture it in my answer(under this question) very soon.","['hyperbolic-functions', 'closed-form', 'hypergeometric-function', 'sequences-and-series']"
4747866,"Using a compass of fixed opening and straightedge, what is the shortest way to find the centroid of $10$ points?","This question is actually mentioned in the OEIS sequence $A157650$ . After reviewing the initial terms, I believe that for $n=1..9$ , the centroid of $n$ points can be found with the following step counts: $$0, 4, 9, 12, 19, 21, 28, 28, 36$$ Here I need to claim that a compass of fixed opening is a compass whose distance is fixed, and which draws circles of a predetermined and constant, but arbitrary radius. When I attempted to find the centroid of $10$ points, I failed to accomplish it within $39$ steps (Divide all points into $5$ groups of $2$ points each, find the centroid of each group, and then make the centroid of these five midpoints. This requires $5×4+19=39$ steps). But $A157650(10) = 38$ . Does anyone know how to construct the centroid of $10$ points with $38$ steps? See the Figure 1 and Figure 2 for constructing the centroid of $5$ points. Figure 1 : Figure 2 :","['optimization', 'geometric-construction', 'sequences-and-series']"
4747900,Is there a type of product of groups where $C_2 \star C_2 = C_4$?,Hi I am wondering if there is a type of product $\star$ of groups to get $C_4 \cong C_2 \star C_2$ . We know that the composition series of $C_4$ indeed is a 2 copy of $C_2$ so I am wondering if there is a notion of product to recover it back.,"['group-theory', 'abstract-algebra', 'abelian-groups', 'products']"
4747903,Does $\lim_{k\to\infty}\sum_{n\ge1}\left(\frac{n!}{n^n}\right)^k$ converges to 1?,"Context: I was randomly putting infinite sums of this form ( $k\in\mathbb N)$ in online calculators. $$\sum_{n\ge1}\left(\frac{n!}{n^n}\right)^k$$ I noticed that as $k$ increases, the sums becomes more and more closer to $1$ . Question: Can we prove if $$\lim_{k\to\infty}\sum_{n\ge1}\left(\frac{n!}{n^n}\right)^k = 1 $$ Thanks !","['limits', 'summation', 'sequences-and-series', 'real-analysis']"
4747906,Maximize Area of A Rectangle.,"I have a rectangle $ABCD$ and $P$ is a point inside the rectangle and the distance from the point $P$ to all the vertices of the rectangle is given. Now I have to figure out the maximum possible area of the rectangle. For example: If $PA=13$ , $PC=47$ , $PD=43$ , $PB = 23$ , then what can be the maximum area of the rectangle? I know single-variable calculus , but here the main problem is that I cannot get an equation involving only one variable. Two variables are coming into question. I have also tried it using pure geometry by dropping perpendiculars from the point P to the other sides, then applying Pythagoras but doesn't help. Also, angle chasing is not the case here I guess.","['optimization', 'maxima-minima', 'geometry', 'euclidean-geometry']"
4747916,Stuck trying to prove the product property of limits,"I'm trying to come up with a proof of the statement ""if $\lim_{x\to p}f(x)=A$ and $\lim_{x\to p}g(x)=B$ , then $\lim_{x\to p}f(x)g(x)=AB$ "". This is what I've attempted: Choose some $\epsilon$ > $0$ . Since $\lim_{x\to p}f(x)=A$ and $\lim_{x\to p}g(x)=B$ , there must exist a $\delta_{1}$ and $\delta_{2}$ such that: $$0<|x-p|<\delta_{1}\implies|f(x)-A|<\sqrt{\epsilon}$$ $$0<|x-p|<\delta_{2}\implies|g(x)-B|<\sqrt{\epsilon}$$ Because if $\epsilon>0$ , then $\sqrt{\epsilon}>0$ and hence a satisfactory $\delta$ must exist for $\sqrt{\epsilon}$ . Let $\delta_{3}=\min(\delta_{1},\delta_{2})$ .
Then we must have $$0<|x-p|<\delta_{3}\implies|f(x)-A|\cdot|g(x)-B|<\epsilon,$$ because $(\sqrt{\epsilon})^2=\epsilon$ . Now I feel as if I must find a way to show that $$|f(x)-A|\cdot|g(x)-B|=|f(x)g(x)-Ag(x)-Bf(x)+AB|\ge|f(x)g(x)-AB|,$$ but this statement seems to boil down to showing that $-AB\le -Ag(x)-Bf(x)+AB$ , which doesn't look like it holds true in the general case. This makes me feel as if I've made some mistake somewhere along the way. I can't think of any other way to approach this problem, so I've ended up stuck at this point.","['limits', 'calculus', 'real-analysis']"
4747923,Why does $\cos(\sin (x))$ look like a cosine wave,"Motivation: I noted that the graph is closest to the following: It kind of makes sense since $\sin x$ is periodic so throwing it into the cosine means it's still periodic, to some extent. I don't think I'm the first one to think of this but I can't find any explanation anywhere. Maybe I haven't tried enough. My own thought is that the function's butchered Taylor series is $\displaystyle \sum_{n=0}^{\infty} \dfrac{(-1)^n (\sin x)^{2n}}{(2n)!}$ which makes sine waves (also cosine waves, same difference) like so: $f(x) = 1-\dfrac{(\sin x)^2}{2!}+\dfrac{(\sin x)^4}{4!}-\dfrac{(\sin x)^6}{6!} \cdots$ Normally when I do things like this I arrive at very complicated solutions so I'm not even sure this can be solved with at most Calc 3 knowledge. Is there any justification that makes intuitive sense?","['trigonometry', 'taylor-expansion']"
4747927,Type that maximize the number of permutations,"Given a permutation $\sigma \in S_n$ , the type of $\sigma$ is the $n$ -uple $(c_1,...,c_n)$ where $c_k$ is the number of cycles of $\sigma$ of lenght $k$ . It's easy to prove that the number of permutations $\sigma \in S_n$ of type $(c_1,...,c_n)$ is $$\frac{n!}{1^{c_1}c_1!...n^{c_n}c_n!}$$ The exercise 136 (chapter 1) of Stanley's Combinatorics volume 1 asks to find the type that maximizes the number of permutations of that type. The solution in the book just states the answer $(1,0,...,0,1,0)$ (all zero entries except the first one and the second-last one) that was intuitively clear to me, but doesn't give any justification. How do I formally justify this result?","['permutations', 'combinatorics', 'discrete-mathematics', 'permutation-cycles']"
4747930,Showing $\log(f(z))-\log(z^n)$ is single-valued holomorphic.,"For context, I am currently working through ""Dynamics in One Complex Variable"" by John Milnor, specifically, Lemma A.2 in Appendix A. The problem I'm facing is as follows: Let $f:\mathbb{D} \rightarrow \mathbb{C}$ be a holomorphic function that contains no zeros in the annulus region $\{z : r_0 < \vert z \vert < r_1 \}$ . Let $n$ denote the number of zeros in the disk $\mathbb{D}_r$ where $r<1$ .
I want to show that if $r_0 < r < r_1$ , then we have that $\log(f(z)) - \log(z^n)$ is single valued and holomorphic on this annulus. I believe the idea would be to Taylor expand $f(z)$ and combine the logarithms somehow so that the imaginary part cancels out somehow.. but I can't seem to get it to work. Any help would be greatly appreciated.","['complex-analysis', 'functions', 'complex-numbers']"
4747973,prove if $A_n$ converge in distribution to $a\neq 0$ then $\frac{1}{A_n}$ to $\frac{1}{a}$.,"prove if $A_n$ converge in distribution to $a\neq 0$ then $\frac{1}{A_n}$ converges in distribution to $\frac{1}{a}$ . My attemp: $$F_{\frac{1}{Y_{n}}}(t)=P(\frac{1}{Y_{n}}\leq t)=P(\frac{1}{t}\leq Y_{n})=1-P(Y_{n}<\frac{1}{t})\rightarrow^* 1-P(a<\frac{1}{t})=P(a\geq\frac{1}{t})=P(\frac{1}{a}\leq t)=F_{\frac{1}{a}}(t)$$ But * is not correct since $P(A_n \leq t)\rightarrow P(a \leq t)$ but not necessarily $P(A_n < t)\rightarrow P(a < t)$ . Another attemp: I know probability convergence and distribution convergence are equivalent if the limit is a constant, $$P(|Y_{n}-a|<\epsilon)=P(a-\epsilon<Y_{n}<a+\epsilon)=P(\frac{1}{a+\epsilon}<\frac{1}{Y_{n}}<\frac{1}{a-\epsilon})$$ How do you complete the proof? Thank you.",['probability-theory']
4748021,Function as a triplet [duplicate],"This question already has answers here : Give a good reason to define a function from A to B as a triple (F, A, B) rather than a functional set of pairs with domain A and image included in B. (3 answers) Some confusion about what a function ""really is"". (7 answers) Confusion about the definition of function (2 answers) What is the set-theoretic definition of a function? (3 answers) Closed 11 months ago . On this site, many people have defined a function as a triplet $(X,Y,f)$ . What is $f$ ? A transforming process? Can a distinction be made between $f$ and the function? When defining it, people call $f$ a subset of $X\times Y$ , but would you not call the triplet a subset of the sets $X$ and $Y$ ? This question came about because of the idea of applying a function. How is it possible to apply a function if it is defined as a triplet?","['notation', 'algebra-precalculus', 'functions', 'definition']"
4748100,"Isogeny $SL_2(\mathbb{Q}_p)\times SL_2(\mathbb{Q}_p) \rightarrow SO_{2,2}(\mathbb{Q}_p)$","For a field $k$ of characteristic $\neq2$ , consider the special orthogonal group $$SO_{r,s}(k):=\{g\in SL_{r+s}(k): g^TQg=Q\},\quad\text{ where }Q:=\begin{pmatrix}I_r & \\ & -I_s\end{pmatrix}.$$ As shown in Paul Garrett's notes , there exists a 2-to-1 homomorphism $$SL_2(\mathbb{R})\times SL_2(\mathbb{R})\xrightarrow{\Phi} SO_{2,2}(\mathbb{R}),$$ defined as follows. Let $(g,h)\in SL_2(\mathbb{R})\times SL_2(\mathbb{R})$ act on $x\in V:=M_{2,2}(\mathbb{R})$ as $(g,h)\cdot x:=gxh^{-1}$ . The bilinear form $$\langle\cdot,\cdot\rangle:V\times V\rightarrow \mathbb{R}:(x,y)\mapsto\text{tr}(xwy^Tw^{-1}),\quad\text{ where }w:=\begin{pmatrix}& -1 \\ 1 &\end{pmatrix},$$ is symmetric and invariant under the group action of $SL_2(\mathbb{R})\times SL_2(\mathbb{R})$ on $V$ . Since $$\langle\begin{pmatrix} a & b \\ c & d\end{pmatrix},\begin{pmatrix}a' & b' \\ c' & d' \end{pmatrix}\rangle=ad'-bc'-cb'+da',$$ an orthogonal basis for $V$ is given by $$\begin{pmatrix}1 & \\ & 1\end{pmatrix},\quad\begin{pmatrix}1 & \\ & -1\end{pmatrix},\quad\begin{pmatrix} & 1 \\ -1 &\end{pmatrix},\quad\begin{pmatrix}& 1 \\ 1 &\end{pmatrix},$$ which have values $2,-2,2,-2$ under $\langle\cdot,\cdot\rangle$ , thus giving the desired signature. My question is the following: does the same statement also hold for $\mathbb{Q}_p$ , meaning, do we get an isogeny $SL_2(\mathbb{Q}_p)\times SL_2(\mathbb{Q}_p) \rightarrow SO_{2,2}(\mathbb{Q}_p)$ in the same way? I am able to show that the map is 2-to-1 (see next paragraph), but don't know how to show surjectivity. For $(g,h)\in \ker(\Phi)=\{(g,h):gxh^{-1}=x \text{ for all }x\in V\}$ , we have $gh^{-1}=(g,h)\cdot I_2=I_2$ , so $g=h$ . Since $(g,h)$ also acts trivially on $$\begin{pmatrix}1 & \\ & -1\end{pmatrix},\quad\begin{pmatrix} & 1 \\ -1 &\end{pmatrix},\quad\begin{pmatrix}& 1 \\ 1 &\end{pmatrix},$$ it must hold that $g=\lambda I_2\in SL_2(\mathbb{Q}_p)$ for some $\lambda^2=1$ , so $\lambda=\pm1$ .","['p-adic-number-theory', 'orthogonal-matrices', 'group-theory', 'isogeny', 'lie-groups']"
4748129,Asymptotics of sequence of rational numbers,"There is a simple sequence of rational numbers. It starts from $a_1=1$ , and then $$
 a_{n}=\begin{cases}
  a_{n-1}
  &\text{for even }n
 \\
  a_{n-1}-\frac1n a_{\frac{n-1}2}
  &\text{for odd }n
 \end{cases}.
$$ It seems that it converges to $0$ as $$
 a_n\sim \frac{A}n,\ \ \ A=3.258891....
$$ Has anyone seen a sequence like this before? Any fast algorithm for the computation of $A$ ? Any expression for $A$ through known constants? Thanks for any links. There are a few first elements $$
 a_1=1,\ a_2=1,\ a_3=\frac23,\ a_4=\frac23,\ a_5=\frac7{15},\ a_6=\frac7{15}, ....
$$ Update 27/08/2023. Let me say a few words about the source of the problem. Suppose that at each time step, a particle generates one new particle with probability $p$ or stays alone with probability $1-p$ , where $p\in\mathbb{U}(0,1)$ is the uniform random variable on the unit interval. Let $X_t$ be the number of particles at time $t\in\mathbb{N}$ . At the beginning $X_1=1$ - one particle. Denote $$
 \varphi_n:=\lim_{t\to+\infty}\frac{\mathbb{P}(X_t=n)}{\mathbb{P}(X_t=1)},
$$ $$
 \Phi(z)=\varphi_1z+\varphi_2z^2+\varphi_3z^3+...,
$$ $$
 F(z)=\int_0^z\Phi(\zeta)d\zeta.
$$ It can be shown that $F$ satisfies the functional equation with the initial conditions $$
 \frac{F(z)-F(z^2)}{z-z^2}=\frac12F'(z),\ \ F(0)=F'(0)=0,\ \ F''(0)=1,
$$ which leads to the generating equation for $\varphi_n$ similar to that for $a_n$ . It seems that the main part of the asymptotics for $\varphi_n$ is the following, see the Figure At the moment, I still can not prove that the constant before $n$ is $1/2(1-\ln2)$ . This problem is equivalent to the problem from the question above. However, if it is true then other terms in the asymptotics can be expressed through this first constant. If I have time then I will try to write all the details and put them in, e.g., arxiv - because it can be lengthy to put it here. I will add other problems related to branching processes in a random environment. Of course, the link to math.stackexchange.com will be added as well - everybody can see who (Tian Vlašić) suggested $1/(1-\ln2)$ . Update 26/09/2023 The power series coefficients $\varphi_n$ of $\Phi(z)$ , defined above, satisfy $$
 \varphi_{n}=\begin{cases}
  \frac{n+1}{n-1}\varphi_{n-1}
  &\text{for even }n
 \\
  \frac{n+1}{n-1}\varphi_{n-1}-\frac{4}{n-1} \varphi_{\frac{n-1}2}
  &\text{for odd }n
 \end{cases},
$$ which is similar to the functional equation for $a_n$ . It seems that the first few asymptotic terms for $\varphi_n$ contain the linear growth and power law decaying coefficients multiplied by short-phase periodic factors of period $2$ , $4$ , $8$ , etc., see the next Fig. $8$ "" /> However, the next asymptotic term contains a long-phase oscillation of exponential period, see the Fig. Here, $$
 \rho_{8n}=\frac{11\ln2-9}{4\ln2-2},\ \ \ \rho_{8n+1}=\frac{19-31\ln2}{4\ln2-2},\ \ \ \rho_{8n+2}=\frac{27-45\ln2}{4\ln2-2},\ \ \ \rho_{8n+3}=\frac{\ln2-5}{4\ln2-2},
$$ $$
\rho_{8n+4}=\frac{11\ln2-9}{4\ln2-2},\ \ \ \rho_{8n+5}=\frac{33\ln2-13}{4\ln2-2},\ \ \ \rho_{8n+6}=\frac{19\ln2-5}{4\ln2-2},\ \ \ \rho_{8n+7}=\frac{\ln2-5}{4\ln2-2},
$$ and $$
 \alpha=2.545364930374021...\pm10.75397517526887...\mathbf{i}
$$ is the root of $$
 1-2^{\alpha}=-\frac12\alpha.
$$ As it is mentioned above, I still cannot prove that the primary constant is $1/(2-\ln4)$ . As a bonus, there is, perhaps, an equivalent problem: is it true that $$
\lim_{n\to+\infty}\int_0^1\int_{z_n^2}^1...\int_{z_3^2}^1\int_{z_2^2}^1\frac{2z_1}{1+z_1}\prod_{k=2}^n\frac{2z_k}{(1+z_k)(1-z_k^2)}dz_1dz_2...dz_n=1-\ln2?
$$ Some details are available in https://arxiv.org/abs/2309.13765 The link to this question and the explicit mention of the help made by Tian Vlašić with using of WolframAlpha is added to the arxiv version. There is also a maybe relevant paper, where $1/1-\ln2$ appears in some context https://www.jstor.org/stable/3689501 but I did not read it carefully yet.","['limits', 'dynamical-systems', 'constants', 'sequences-and-series']"
4748208,Deriving a differential equation for momentum from the integral form,"The Lugiato-Lefever equation can be written in the form: $$ \frac{\partial \psi}{\partial \tau}  = -(1 + i\alpha)\psi - i\frac{\beta}{2}\frac{\partial ^2 \psi}{\partial \theta^2} + i|\psi|^2 \psi + F_0 \exp[i\delta_m \sin\theta]$$ The solutions to this partial differential equation can have soliton solutions. In a paper that I am reading, I found an equation for the ""momentum"" of these solitons: $$ P = -\frac{i}{2}\int_{-\pi}^{\pi} d\theta \left ( \psi^*\frac{\partial \psi}{\partial \theta} - \psi \frac{\partial \psi^*}{\partial \theta}   \right )$$ The authors then proceed to find the time derivative of the above equation and state without showing any calculations that it is given by: $$ \frac{dP}{d \tau} = -2P - i\int_{-\pi}^{\pi} d\theta \left ( \psi^*\frac{\partial F}{\partial \theta} - \psi \frac{\partial F^*}{\partial \theta}   \right )  $$ where $F(\theta,\tau) = F_0 \exp[i\delta_m \sin\theta]$ . I am struggling to derive the third equation from the second. I attempted to take the derivative of the second equation with respect to $\tau$ , but then I got stuck dealing with too many parameters and I feel like there is an easy way to show this.","['derivatives', 'ordinary-differential-equations', 'partial-differential-equations']"
4748211,Proving that a non-trivial GCD of strings $a$ and $b$ exists iff $a+b=b+a$,"Firstly, assume there are two strings $a$ and $b$ . Let the GCD of the two strings is the longest string which divides both. String $t$ divides $s$ iff $s = t + t + \dots + t$ . How can I show that a non-trivial GCD exists iff $a + b = b + a$ ? One proof I have seen is that: $n \cdot len(gcd) + m \cdot len(gcd) = m \cdot len(gcd) + n \cdot len(gcd)$ , but this seems very incomplete.","['elementary-number-theory', 'gcd-and-lcm', 'discrete-mathematics']"
4748214,Why does $C^\infty(\Omega)$ have the Heine-Borel property?,"$
\let\uto\rightrightarrows
\let\ii\infty
\let\W\Omega
\let\a\alpha
\let\b\beta
\let\e\varepsilon
\let\d\delta
\let\sbe\subseteq
$ I'm struggling to understand the examples at the end of the first chapter of Rudin's Functional Analysis . I will focus the post on $C^\ii(\W)$ as hopefully getting a grasp of $C^\ii(\W)$ will clarify the spaces $C(\W)$ and $\mathcal{D}_K$ . An original post with all my questions on $C^\ii(\W)$ was closed due to lack of focus, so on the current post I will focus on proving $C^\ii(\W)$ has the Heine-Borel property. Below I try to to fill in the gaps in the proof given. I then write my doubts on the proofs and include pictures of the relevant book excerpts. Theorem: $C^\ii(\W)$ has the Heine-Borel property. Proof: let $E\sbe C^\ii(\W)$ be closed and bounded. We wish to show $E$ is compact. a) There are numbers $M_N < \ii$ such that $$p_N(f) \le M_N \iff
 \sup_{\substack{x\in K_N\\ |\a|\le N}} \{|D^\a f(x)|\} \le M_N.$$ b) Fix $\b$ and $N$ with $|\b|\le N-1$ . The family $\mathcal{F}_{\b,N}$ of functions $$\{ D^\b f : f\in E\}$$ is equicontinuous on $K_{N-1}$ i.e. for any $\e>0$ there is a $\d>0$ such that $$d(x,y)< \d \implies |D^\b f(x) - D^\b f(y)| < \e$$ for any $x,y\in K_{N-1}$ and $D^\b f\in\mathcal{F}_{\b,N}$ . c) Since $\mathcal{F}_{\b,N}$ is pointwise bounded and equicontinuous on $K_{N-1}$ , Ascoli's Theorem (and its corollary) implies that $\mathcal{F}_{\b,N}$ is compact and that every sequence in $\mathcal{F}_{\b,N}$ contains a uniformly convergent subsequence. Therefore $E$ is compact. My questions: Firstly: are my outlines of Rudin's proof correct? If at any point I'm deviating from the argument in the book, please let me know. b) Why is $\mathcal{F}_{\b,N}$ equicontinuous on $K_{N-1}$ ? c) While I understand the rest of the paragraph, I do not see why the last sentence ""Therefore $E$ is compact"" follows. The main passage in question (regarding $C^\ii(\W)$ : The main results used:","['proof-explanation', 'topological-vector-spaces', 'analysis', 'solution-verification', 'functional-analysis']"
4748304,Maximum value of $p+q$ when $pq=r^2$,"Question : If $pq=r^2$ , then the maximum value of $(p+q)$ is : $1.)$ I calculated the minimum value of $(p+q)$ which is $2r$ by AM-GM inequality. But, how do I calculate the maximum value using the same inequality? $2.$ I did follow an alternative method of using the first and second derivatives to obtain critical values and then calculate the maximum value of $p+q$ . Following is the calculation executed: Since, $pq = r^2$ , this implies $q = \frac{r^2}{p}$ Substituting the value of $q$ in the function $f(p, q) = p + q$ , we get, $F(p) = p + \frac{r^2}{p}$ To find the critical values, we need to calculate the derivative of $F(p)$ with respect to $p$ , i.e. $F '(p)$ : $F '(p) = 1 - \frac{r^2}{p^ 2}$ Now, setting $F '(p) = 0$ provides the critical values: $ 1 - \frac{r^2}{p^2} = 0 $ Solving for $p$ , we get $p = \pm r$ Now, check the sign of $F''(p)$ for $p = r$ and $p = -r $ . $F''(p)$ = $\frac{r^2}{p^3}$ For $p = r$ , $F''(p)$ = $\frac{\large 2}{\large r}$ $>0$ (assuming $r > 0$ ) and thus $p= r$ has a minimum For $p = -r$ , $F''(p)$ = $\frac{-2}{r}\space\space <0$ (assuming $r > 0$ ) and thus $p= -r$ has a maximum Now, let's find the corresponding values of $q$ by substituting the values of $p$ in $q = \frac{r^2}{p}$ : For $p = r$ , $q = r$ and $p+q = 2r$ (Minimum Value) For $p = -r$ , $q = -r$ and $p+q = -2r$ (Maximum Value) So, is the maximum value $ -2r$ ? I am skeptical about using the assumption ( $r>0$ ) as it is not mentioned in the question and thus think that $-2r$ cannot be said as the maximum value of $p+q$ . Please help me out in finding the correct maximum value.","['calculus', 'algebra-precalculus']"
4748327,Euler Substitution,"Got a math problem that i have to solve using Euler substitution but at one point I'm getting stuck. Tried different ways of solving, can't figure out.
The problem sounds like that: $$
\int_{0}^{1} \frac{1}{x + \sqrt{x^2 + x + 1}} \, dx
$$ Tried to substitute the square root with $x + t$ , tried with $t - x$ , but no success. Can someone give me a hint where I'm doing something wrong?","['integration', 'calculus', 'definite-integrals']"
4748358,"$ab+bc+ca=3,$ find the minimal value $k:$ $\left(\frac{a+b}{c+ab}\right)^k+\left(\frac{c+b}{a+cb}\right)^k+\left(\frac{a+c}{b+ac}\right)^k\ge 3.$","Let $a,b,c\ge 0: ab+bc+ca=3.$ Find the minimal value of constant $k$ satisfying $$\left(\frac{a+b}{c+ab}\right)^k+\left(\frac{c+b}{a+cb}\right)^k+\left(\frac{a+c}{b+ac}\right)^k\ge 3.$$ I've proved when $k=\dfrac{1}{2}.$ Actually, by Holder $$\left(\sqrt{\frac{c+b}{a+cb}}\right)^2.\sum_{cyc}(b+c)^2(a+bc)(2a+b+c)^3\ge [\sum_{cyc}(b+c)(2a+b+c)]^3,$$ the following inequality is ugly but we can use $uvw.$ By checking $k=\dfrac{1}{3},$ the problem seems true but I've had no proof yet. So, what is the range of $k$ ? Thank you very much. Update I see that when $k\ge 1$ we can use AM-GM $$\left(\frac{a+b}{c+ab}\right)^k+k-1\ge k.\frac{a+b}{c+ab}$$ Similarly, we just need to prove $$\frac{a+b}{c+ab}+\frac{c+b}{a+cb}+\frac{a+c}{b+ca}\ge 3$$ But by C-S we will prove $$4(a+b+c)^2\ge 3.(6+3(a+b+c)-3abc) \iff 4(a+b+c)^2-9(a+b+c)-18+9abc\ge 0$$ Is that true approach ? Id est, we need to consider in case $k\le 1.$","['multivariable-calculus', 'inequality']"
4748381,"Prove $\ell(\theta^{(t+1)}) \geq \ell(\theta^{(t)})$, answer proof not clear, need some expainations","Question The problem is to proof the inequation, but the answer is too elliptical, can anyone give some comment or explaination to help me build up the logic chain? Answer The confusion part: why (1) is not $=$ . (2) why they directly come up $\theta^{(t+1)} > \theta^{(t+1)}$ . is there missing something between (0) and (1)?","['self-learning', 'statistics', 'real-analysis', 'probability-theory', 'probability']"
4748399,Question about the group algebra,"Let $G$ be a group and let $k[G]$ be the group algebra of $G$ . If we let $H$ be a subgroup of $G$ and $T=\{t_1,...,t_n\}$ a right transversal for $H$ , then we can write $$
G=\bigcup_{i=1}^nHt_i.
$$ My question is, does this imply that we can write the group algebra as $$
k[G]=\bigoplus_{i=1}^nk[Ht_i],
$$ where $k[Ht_i]$ is the group algebra of the subgroup $Ht_i$ ?","['representation-theory', 'direct-sum', 'modules', 'abstract-algebra', 'group-theory']"
4748438,Number of 5 letter words subject to certain conditions,"How many 5 letter words can we make subject to the following conditions: We can use the letters a, b, c, d, e. Any letter can be used more than once or not at all. No letter can be adjacent to itself anywhere within the sequence. (The first and last letters aren't considered to be adjacent.) No letter can be used more than twice. Showing my work. Let's go through the cases: 1 of each letter. This is just 5! = 120 possibilities. 2 of one letter, 1 of each of the remaining 3 letters. The possible configurations are: xoxoo, xooxo, xooox, oxoxo, oxoox, ooxox. There are 6 of them, hence 6(5 * 4 * 3 * 2) = 720 possibilities. 2 of two letters, 1 of the remaining letter. The possible configurations are: xyxyo, xyxoy, xyoxy, xoyxy, xyoyx, oxyxy. There are 6 of them, hence 6(5 * 4 * 3) = 320 possibilities. So there should be a total of 120 + 720 + 360 = 1200 such 5 letter words. Is this correct? Update in response to comment: Are these four separate problems or do we wish to count the number of five-letter words that satisfy all four of the stated conditions? I want to count the number of five-letter words that satisfy all four of the stated conditions.",['combinatorics']
4748449,"Given behaviours at small and large values of a parameter, how can one deduce there is a singular (non-analytic) point?","A reoccurring statement in condensed matter physics is vaguely the following: Let $f:\mathbb{R}^n\times\mathbb{R}\rightarrow\mathbb{C}$ be some function. $\mathbb{R}^n$ are the spatial coordinates and $\mathbb{R}$ represents temperature. This function is shown to behave very differently at $T\rightarrow 0$ and $T\rightarrow \infty$ . For example, it may hava a finite value for low $T$ but is identically $0$ at large enough $T$ . Another example is that $f \sim \mid{r}\mid ^k$ for low $T$ and $f \sim e^{-m\mid r\mid}$ for large enough $T$ . By $\sim$ I mean that the function converges to these functions at these limits. Now, it is claimed that these types of functions can not be analytic with respect to $T$ for all $T$ : There is a point in which the behavior is singular. My questions are the following: How does one prove this for the cases above? What type of arguments and from which fields are used for these types of problems? Particular theorems will be very welcomed A proof for similar cases will also be a good answer, as my goal is understanding the formal arguments","['asymptotics', 'analysis', 'complex-analysis', 'real-analysis', 'mathematical-physics']"
4748453,Spectral theorem for diagonal matrix in different inner product spaces,"I learned a special case of the spectral theorem for finite dimensional inner product space. As I understand it states that a real matrix is orthogonally diagonalizable with real eigenvalues iff it equal its hermitian adjoint. However when I try to apply it to diagonal matrices, I think I have a problem. If I work with the regular inner product ( $\langle u,v\rangle \to u^tv)$ then I see how this works out ( $A^* = A^t = A$ ), but if I work in a more general inner product space: $\langle u,v\rangle = (Bu)^t(Bv)$ where $B$ is an invertiable matrix, then the hermitian adjoint of a matrix $A$ becomes $A^* = (B^tB)^{-1}A^t(B^tB)$ , and this may not commute with $A$ , if $A$ is some general diagonal matrix, for example let's define $A = \begin{pmatrix}1 & 0\\0 & -1\end{pmatrix}, B = \begin{pmatrix}1 & 1\\0 & 1\end{pmatrix}$ . However we still have that for the identity matrix: $I^* = I$ so a diagonal matrix, is still orthogonally diagonalizable. How this doesn't contradict the spectral theorem?","['orthogonality', 'spectral-theory', 'linear-algebra']"
4748463,How to find the maximum of this trigonometry expression?,"Given acute angles $\alpha_1$ , $\alpha_2$$\dots$ , $\alpha_n$ such that
\[\sin^2\alpha_1+\sin^2\alpha_2+\dots+\sin^2\alpha_n=1,\]
find the maximum of
\[\frac{\sin\alpha_1+\sin\alpha_2+\dots+\sin\alpha_n}{\cos\alpha_1+\cos\alpha_2+\dots+\cos\alpha_n}.\] I tried the equal variable theorem, where we needed $\frac{\mathrm d}{\mathrm dx}\sqrt{1-x^2}$ to be strictly convex, but it is actually strictly concave. So we can not apply the theorem. However, to find the answer, I still assumed $\alpha_1=\alpha_2=\cdots=\alpha_{n-1}$ , and got the answer $\frac1{\sqrt{n-1}}$ .","['optimization', 'algebra-precalculus', 'trigonometry']"
4748517,Geometric meaning of the rising and the lowering of indices,"I'm an undergraduate student and currently I'm approaching tensorial calculus. I was wondering: is there some geometric meaning to the operation of rising/lowering indices (and then if there was any geometric difference between vectors and covectors), or are they only mere formal operations? In case, why don't we simply use only vectors in the definition of tensors?","['vectors', 'tensors', 'linear-algebra', 'manifolds', 'differential-forms']"
4748530,Proof of Simpson's Paradox,"I am studying Implicit Function Theorem and its application in Simpson's Paradox. I got the following problem. I tried it myself, but not sure if my answer is correct. I would really appreciate it if someone could help me check! Problem: A company tests a new medicine in city $C$ and $C'$ . In each city, the tests are conducted in two labs, $U$ and $U'$ . In each lab, there is a test group ( $T$ ) receiving the new medicine and a control group ( $T'$ ) receiving old medicine. Some people became health ( $H$ ), the other did not ( $H'$ ). The new medicine is judged to be better if a higher percentage of people who took the new medicine becomes health than those who took the old one. There exist samples in which the new medicine is better than the old at each of the four labs and in the aggregate in each city, but worse when aggregated over the whole test population. In other samples, the conclusions oscillate with the level: the new medicine is worse than the old at each of the four facilities, is better in each city, but is worse when aggregated over the whole population, and so forth. Present an analytical proof (not using counterexamples) that each of the above scenario is possible using the Implicit Function Theorem. My attempt: Define the following mutually exclusive groups: \begin{equation}
S_1 = TCU,\space\space\space\space S_2 = TCU',\space\space\space\space S_3 = TC'U,\space\space\space\space S_4 = TC'U' \\
S_5 = T'CU, \space\space S_6 = T'CU',\space\space S_7 = T'C'U,\space\space S_8 = T'C'U'.
\end{equation} Let $x_i = Pr\{H|S_i\}$ and $d_i = Pr\{S_i\}$ for $I = 1, \dots, 8$ . Let \begin{equation}
y_1 = Pr\{H|TC\},\space y_2 = Pr\{H|TC'\},\space y_3 = Pr\{H|T'C\},\space y_4 = Pr\{H|T'C'\},
\end{equation} aggregating over the type of test lab. Let \begin{equation}
z_1 = Pr\{H|T\}\space\space\space\space and\space\space\space\space z_2 = Pr\{H|T'\},
\end{equation} the overall aggregate variables. We first show that \begin{equation}
y_i = \frac{x_{2j - 1}d_{2j - 1} + x_{2j}d_{2j}}{d_{2j-1} + d_{2j}}.
\end{equation} Consider the case when $j = 1$ , then we want to show \begin{equation}
Pr\{H|TC\} = \frac{Pr\{H|TCU\}Pr\{TCU\} + Pr\{H|TCU'\}Pr\{TCU'\}}{Pr\{TCU\} + Pr\{TCU'\}}.
\end{equation} By Kolmogorov definition and axiom of conditional probability, we have \begin{equation}
Pr\{H|TC\} = \frac{Pr\{H \cap TC\}}{Pr\{TC\}} = \frac{Pr\{H \cap TCU\} + Pr\{H \cap TCU'\}}{Pr\{TC\}} = \frac{Pr\{H|TCU\}Pr\{TCU\} + Pr\{H|TCU'\}Pr\{TCU'\}}{Pr\{TC\}}.
\end{equation} But $Pr\{TC\} = Pr\{TCU\} + Pr\{TCU'\} - Pr\{TCU \cap TCU'\} = Pr\{TCU\} + Pr\{TCU'\}$ because $TCU \cap TCU' = \emptyset$ . We proved that $y_1 = \frac{x_{1}d_{1} + x_{2}d_{2}}{d_{1} + d_{2}}$ . Similarly, we are able to prove that $y_i = \frac{x_{2j - 1}d_{2j - 1} + x_{2j}d_{2j}}{d_{2j-1} + d_{2j}}$ for all $j = 1, 2, 3, 4$ . By an analogous argument, we are able to prove that \begin{equation}
z_1 = \frac{\sum_{j = 1}^{4}x_jd_j}{\sum_{j = 1}^{4}d_j}\space\space\space\space and\space\space\space\space z_2 = \frac{\sum_{j = 5}^{8}x_jd_j}{\sum_{j = 5}^{8}d_j}.
\end{equation} Now, consider the map $F:[0, 1]^8 \times \Sigma(8) \to \mathbb{R}^7$ defined by \begin{equation}
F(x_1, \dots, x_8, d_1, \dots, d_8) = (x_1 - x_5, x_2 - x_6, x_3 - x_7, x_4 - x_8, y_1 - y_3, y_2 - y_4, z_1 - z_2),
\end{equation} where $\Sigma(8) = \{(d_1, \dots, d_8) | d_i \geq 0, \sum_{i}d_i = 1, i = 1, \dots, 8\}$ . Write $d_8 = 1 - \sum_{i = 1}^{7}d_i$ , then we have \begin{equation}
F(x_1, \dots, x_8, d_1, \dots, d_7) = (x_1 - x_5, x_2 - x_6, x_3 - x_7, x_4 - x_8, \frac{x_1d_1 + x_2d_2}{d_1 + d_2} - \frac{x_5d_5 + x_6d_6}{d_5 + d_6}, \frac{x_3d_3 + x_4d_4}{d_3 + d_4} - \frac{x_7d_7 + x_8(1 - \sum_{i = i}^{7}d_i)}{d_7 + (1 - \sum_{i = 1}^{7}d_i)}, \frac{\sum_{i = 1}^{4}x_id_i}{\sum_{i = 1}^{4}d_i} - \frac{\sum_{i = 5}^{7}x_id_i + x_8(1 - \sum_{i = 1}^{7}d_i)}{1 - \sum_{1 = 1}^{4}d_i}).
\end{equation} Calculate the Jacobian DF: \begin{pmatrix}
1 & 0 & 0 & 0 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0 & 0 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
\frac{d_1}{d_1 + d_2} & \frac{d_2}{d_1 + d_2} & 0 & 0 & -\frac{d_5}{d_5 + d_6} & -\frac{d_6}{d_5 + d_6} & 0 & 0 & \frac{(x_1 - x_2)d_2}{(d_1 + d_2)^2} & \frac{(x_2 - x_1)d_1}{(d_1 + d_2)^2} & 0 & 0 & \frac{(-x_5 + x_6)d_6}{(d_5 + d_6)^2} & \frac{(x_5 - x_6)d_5}{(d_5 + d_6)^2} & 0\\
0 & 0 & \frac{d_3}{d_3 + d_4} & \frac{d_4}{d_3 + d_4} & 0 & 0 & -\frac{d_7}{1 - \sum_{i = 1}^{6}d_i} & -\frac{1 - \sum_{i = 1}^{7}d_i}{1 - \sum_{i = 1}^{6}d_i} & 0 & 0 & \frac{(x_3 - x_4)d_4}{(d_3 + d_4)^2} - \frac{(x_7 - x_8)d_7}{(1 - \sum_{i = 1}^{6}di)^2} & \frac{(x_4 - x_3)d_3}{(d_3 + d_4)^2} - \frac{(x_7 - x_8)d_7}{(1 - \sum_{i = 1}^{6}di)^2} & -\frac{(x_7 - x_8)d_7}{(1 - \sum_{i = 1}^{6}di)^2} & -\frac{(x_7 - x_8)d_7}{(1 - \sum_{i = 1}^{6}di)^2} & -\frac{x_7 - x_8}{1 - \sum_{i = 1}^{6}d_i}\\
\frac{d_1}{\sum_{i = 1}^{4}d_i} & \frac{d_2}{\sum_{i = 1}^{4}d_i} & \frac{d_3}{\sum_{i = 1}^{4}d_i} & \frac{d_4}{\sum_{i = 1}^{4}d_i} & -\frac{d_5}{1 - \sum_{i = 1}^{4}d_i} & -\frac{d_6}{1 - \sum_{i = 1}^{4}d_i} & -\frac{d_7}{1 - \sum_{i = 1}^{4}d_i} & \frac{1 - \sum_{i = 1}^{7}d_i}{1 - \sum_{i = 1}^{4}d_i} & \frac{x_1\sum_{i = 1}^{4}d_i - \sum_{i = 1}^{4}x_id_i}{(\sum_{i = 1}^{4}d_i)^2} - \frac{-x_8(1 - \sum_{i = 1}^{4}d_i) + (\sum_{i = 5}^{7}x_id_i + x_8 - x_8\sum_{i = 1}^{7}d_i)}{(1 - \sum_{i = 1}^{4}d_i)^2} & \frac{x_2\sum_{i = 1}^{4}d_i - \sum_{i = 1}^{4}x_id_i}{(\sum_{i = 1}^{4}d_i)^2} - \frac{-x_8(1 - \sum_{i = 1}^{4}d_i) + (\sum_{i = 5}^{7}x_id_i + x_8 - x_8\sum_{i = 1}^{7}d_i)}{(1 - \sum_{i = 1}^{4}d_i)^2} & \frac{x_3\sum_{i = 1}^{4}d_i - \sum_{i = 1}^{4}x_id_i}{(\sum_{i = 1}^{4}d_i)^2} - \frac{-x_8(1 - \sum_{i = 1}^{4}d_i) + (\sum_{i = 5}^{7}x_id_i + x_8 - x_8\sum_{i = 1}^{7}d_i)}{(1 - \sum_{i = 1}^{4}d_i)^2} & \frac{x_4\sum_{i = 1}^{4}d_i - \sum_{i = 1}^{4}x_id_i}{(\sum_{i = 1}^{4}d_i)^2} - \frac{-x_8(1 - \sum_{i = 1}^{4}d_i) + (\sum_{i = 5}^{7}x_id_i + x_8 - x_8\sum_{i = 1}^{7}d_i)}{(1 - \sum_{i = 1}^{4}d_i)^2} & -\frac{x_5 - x_8}{1 - \sum_{i = 1}^{4}d_i} & -\frac{x_6 - x_8}{1 - \sum_{i = 1}^{4}d_i} & -\frac{x_7 - x_8}{1 - \sum_{i = 1}^{4}d_i}
\end{pmatrix} If we take $x_1 = x_5 \neq x_2 = x_6 \neq x_3 = x_7 \neq x_4 = x_8$ , DF has rank 7. Let $(\mathbf{x^*}, \mathbf{d^*})$ be a point with the properties \begin{equation}
x_1 = x_5 \neq x_2 = x_6 \neq x_3 = x_7 \neq x_4 = x_8 \\
d_1 = \dots = d_8 = \frac{1}{8}.
\end{equation} Then, $F(\mathbf{x^*}, \mathbf{d^*}) = 0$ and $DF(\mathbf{x^*}, \mathbf{d^*})$ has maximal rank. By Implicit Function Theorem, $F$ is locally onto a neighborhood of $\mathbf{0}$ . In other words, if we choose any sign pattern $(\varepsilon_1, \dots, \varepsilon_7)$ , where each $\varepsilon_i = \pm 1$ , in the target space $\mathbb{R}^7$ and a point $\mathbf{z} = (z_1, \dots, z_7)$ near $\mathbf{0}$ that realizes this sign pattern, then there exists a point $\mathbf{x'}, \mathbf{d'}$ in $[0, 1]^8 \times \Sigma(8)$ , such that $F(\mathbf{x'}, \mathbf{d'}) = \mathbf{z}$ . The point $(\mathbf{x'}, \mathbf{d'})$ corresponds to a partitioning of the test population into $S_1, \dots, S_8$ so that the 7-tuple \begin{equation}
Pr\{H|TCU\} - Pr\{H|T'CU\}, Pr\{H|TCU'\} - Pr\{H|T'CU""\}, \\
Pr\{H|TC'U\} - Pr\{H|T'C'U\}, Pr\{H|TC'U'\} - Pr\{H|T'C'U'\}, \\
Pr\{H|TC\} - Pr\{H|T'C\}, Pr\{H|TC'\} - Pr\{H|T'C'\}, \\
Pr\{H|T\} - Pr\{H|T'\},\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space
\end{equation} has the preassigned sign pattern $(\varepsilon_1, \dots, \varepsilon_7)$ . I am not completely sure about my answer, especially the step where I calculated $y_i$ and $z_i$ as well as the step where I picked an $(\mathbf{x^*}, \mathbf{d^*})$ . Could someone please help me check? Thanks a lot in advance!","['statistics', 'conditional-probability', 'solution-verification', 'linear-algebra', 'implicit-function-theorem']"
4748560,Understanding the Relationship Between Measure Theory and Probability Theory?,"I was at a restaurant with some of my friends who study pure math. While at the restaurant, they were having a discussion about the relationship between Measure Theory and Probability Theory . I tried to follow along with the discussion but I unfortunately could not. Prior to the conversation, here is what I already knew: I already loosely know what Probability Theory is - from the undergraduate level courses I took in university, I learned about different concepts from Probability Theory in a very applied way. This included learning about different mathematical properties of Probability Distribution Functions and Random Variables. I am less familiar with Measure Theory. Based on some readings I have done, it seems like Measure Theory is involved with assigning ""quantities"" to ""subsets of a set"". For example, if you flip 2 coins - the sample space is HH, HT, TH, TT. In this case, a ""Measure"" refers to a probability - and by using a special function called a ""Probability Measure"", we assign probabilities to each element in the sample space. The mathematician Andrey Kolmogorov created a set of Mathematical Axioms within Probability Theory. For any given ""experiment"" (i.e. Measure Space) - there must be a sample space, an event space (i.e. an ""event"" corresponds to a given subset of the sample space) and a probability measure which assigns probabilities to each event within the sample space. Kolmogorov's Axioms tell us that negative probabilities are not possible, the probability of at least one event occurring is 1 and the sum of the probabilities for all (disjoint) events is 1. These Axioms  then allow us to derive important rules that can be used to further analyze and interpret probabilities, e.g. P(A U B) =  P(A) + P(B) - P(A & B) Now, here is what I did not understand in the conversation: Supposedly Kolmogorov's Axioms were so important that they revolutionized the field of Probability Theory. Kolmogorov was the first to explicitly describe the relationship between Probability Theory and Measure Theory. But why exactly were these Axioms so important? How exactly did the relationship between Measure Theory and Probability Theory revolutionize Probability Theory? If I understand things correctly, it seems like the field of Probability Theory made significant progress before Kolmogorov was even born. For example, the Normal Distribution was defined by Gauss  - far before the birth of Kolmogorov. On the other hand, important results in Probability Theory such as Chebyshev's Inequality and Markov's Inequality were also defined before Kolmogorov. Thus, if the relationship between Measure Theory and Probability Theory is so important - how were these results possible when this relationship was not defined? In other words: What ""things"" could not have been done prior to defining this relationship between Probability Theory and Measure Theory? And what ""things"" could now be done after defining this relationship between Probability Theory and Measure Theory? To summarize - why is the relationship between Measure Theory and Probability Theory important? Can someone please help me understand these points? Thanks!","['measure-theory', 'math-history', 'soft-question', 'probability-theory', 'probability']"
4748622,Finding singular solution to a Lagrange equation,"I want to find a singular solution (not general) to the following differential equation: $$y=3xy'-7y'^3$$ One method for finding a singular solution is called p-discriminant and it consists in solving this system: $$\cases{F(x,y,y')=0 \\ \frac{\partial F}{\partial y'}=0}$$ Where F is the the differential equation. So the system looks like: $$\cases{y=3xy'-7y'^3 \\y'=\pm \sqrt{\frac{x}{7}}}$$ So if we solve the system we get $$y=\pm \frac{2x^{\frac{3}{2}}}{\sqrt{7}}$$ On first sight there is nothing wrong but if we differentiate we get that $$y'=\pm3\sqrt{\frac{x}{7}}=3y'$$ Which doesn't make any sense. Moreover the text book says that the only singular solution is $y=0$ which I have no idea how to conclude using the only the upper method(without guesswork). Where is my mistake?",['ordinary-differential-equations']
4748641,Finding a set of representatives that sum to zero.,"Let $G$ be an abelian group of order $2n$ , where $n$ is an odd integer (For simplicity, we may assume $G\simeq \mathbb{Z}_{2n}$ (the cyclic group of order $2n$ ). Let $A_1,A_2,\ldots,A_n$ be a partition of the elements of $G$ to two-element subsets. I have the following claim: There always exists set of representants $X=\{a_i \mid a_i\in A_i\}$ such that $\sum_{x\in X}x=0$ where $0$ is the identity element in $G$ . $X$ should contain exactly $1$ element from each pair $A_i$ . My question is: How to prove the statement above and also is there an efficient way of finding the representants? (Say in polynomial time with respect to $n$ )? If the proof would involve too much advanced group theory, we may indeed assume that $G$ is the cyclic group of order $2n$ (but it still has to be interesting). EDIT: $n$ has to be odd!","['graph-theory', 'finite-groups', 'group-theory', 'algorithms', 'abelian-groups']"
4748651,Dugundji: There are continuous nowhere differentiable functions,"In Topology by James Dugunji, it is trying to be proven that there are functions with no derivatives at any point and that the set of such functions form a second category set in $C(I)$ (where $I=[0,1]$ ). The proof starts by trying to prove that $$N_n=\left\{f\in C(I)\left|\; \exists x\in\left[0,1-\frac{1}{n}\right],\forall h\in\left(0,\frac{1}{n}\right],\;\left|\frac{f(x+h)-f(x)}{h}\right|\leq n\right\}\right.$$ is closed for all $n$ . This is done by using the continuity of $\omega\colon C(I)\times I\to \mathbb{R}$ , the evaluation map, to conclude that $$\left\{(f,x)\in C(I)\times [0,1-1/n]\left|\; \left|\frac{f(x+h_0)-f(x)}{h_0}\right|\leq n\right\}\right.$$ is closed for every $h_0\in(0,1/n]$ and then using the projection parallel to the compact $[0,1-1/n]$ to conclude that $$N(h_0)=\left\{f\in C(I)\left|\; \exists x\in\left[0,1-\frac{1}{n}\right],\;\left|\frac{f(x+h_0)-f(x)}{h_0}\right|\leq n\right\}\right.$$ is closed for every $h_0\in(0,1/n]$ . So far so good. My problem is in what follows: Dugundji claims that $N_n=\bigcap\{N(h_0)\mid h_0\in(0,1/n]\}$ ; I don't agree with this statement. If $f$ belong to the RHS, then $\forall h\in (0,1/n],\,f\in N(h_0)$ , that is $\forall h\in(0,1/n],\exists x\in [0,1-1/n],\; \left|\frac{f(x+h)-f(x)}{h}\right|\leq n$ which is a weaker property than that held by the elements on $N_n$ . Is there something that I'm missing? Is the equality actually correct? Source: Topology James Dugundji, Chap. XIV, proposition 4.2.","['proof-explanation', 'analysis', 'real-analysis', 'general-topology', 'derivatives']"
4748657,$\beta_k=\sum_{n=k^2}^{k^2+2k}\alpha_n$ where $|\alpha_n|\leq\frac{1}{n}.$ If $\sum\beta_k$ converges then $\sum\alpha_n$ converges.,"Let $(\alpha_n)_{n=1}^{\infty}$ be an infinite sequence of reals which satisfies: $\forall n \in \mathbb{N} \space \space :|\alpha_n| \leq \frac{1}{n}$ . Denote $\beta_k = \sum\limits_{n=k^2}^{k^2+2k} \alpha_n$ , for each $k \in \mathbb{N}$ . If $\sum\beta_k$ converges, then $\sum\alpha_n$ converges. I observed that the sequence of partial sums of the series $\sum\beta_k$ is a subsequence of those of $\sum\alpha_n$ . Additionally, I see the relation between the assumption regarding $(\alpha_n)_{n=1}^{\infty}$ to the fact that $b_k$ is a sum of exactly $2k+1$ elements of $(\alpha_n)_{n=1}^{\infty}$ .
However, I struggle to see how this is actually true (intuitively speaking) since, generally, I could've only deduced that (without the additional assumption about $(\alpha_n)$ ) if $2k+1$ was bounded, which of course is not. I'm also curious to understand in what other cases similar assumptions about $(\alpha_n)$ would follow the same conclusion, and hence I hope to get some ideas about the proof of this particular case. Thanks","['calculus', 'sequences-and-series', 'real-analysis']"
4748659,"If a closed subset $C \subseteq \mathbb R^2$ is ""fragilely""/“minimally” simple connected, must it be all of $\mathbb R^2$?","It is a corollary of the Riemann mapping theorem that every path-connected simply-connected open subset of $\mathbb R^2$ is homeomorphic to $\mathbb R^2$ (there are more elementary proofs as well https://mathoverflow.net/questions/66048/riemann-mapping-theorem-for-homeomorphisms ). This is of course not true for closed subsets $C \subseteq\mathbb R^2$ ; in fact the invariance of domain theorem tells us that no proper closed subset $C \subsetneq \mathbb R^2$ can be homeomorphic to $\mathbb R^2$ . I am interested however in topological invariants that can tell spaces apart, like path-connectedness, simply-connectedness, compactness, etc.; using the invariants I just mentioned, we see that if a closed subset $C \subseteq \mathbb R^2$ is homeomorphic to $\mathbb R^2$ , then it must be path-connected, simply-connected, and not compact (i.e. $\iff$ unbounded, by Heine-Borel in $\mathbb R^2$ ). The ""next hardest example"" not covered by the above is e.g. $C:=$ a closed half plane. We can rule that out by removing any point $b$ on the boundary of the closed half plane $C$ ; then $C \setminus \{b\}$ is still simply-connected, while $\mathbb R^2$ removed any point is not simply-connected. So, the ""next hardest example"" would be a proper closed set $C \subsetneq \mathbb R^2$ that is path-connected, simply-connected, but removing ANY point of $C$ results in a still path-connected, but no longer simply-connected space. I will call such a set ""fragilely""/“minimally” simply-connected (because it is simply-connected, until you remove ANY point) and ""robustly"" path-connected (since it is path-connected, and remains so after removing ANY point). Question: does a ""fragilely"" simply-connected, ""robustly"" path-connected closed proper subset $C \subsetneq \mathbb R^2$ exist?","['connectedness', 'path-connected', 'fundamental-groups', 'homotopy-theory', 'general-topology']"
4748700,Proper linearization of ODEs of the form $\dot{x}(t) + f(x(t)) + \sigma(t) = 0$?,"For a scalar ODE of the form $$\dot{x}(t) + f\left(x(t)\right) = 0 \label{1}\tag{1}$$ where $f \colon \mathbb R \to \mathbb R$ is some smooth function admitting a unique root $x^*$ such that $f(x^*) = 0$ . The linearization of \eqref{1} is straightforward as we can insert $$ x(t) = x^* + \varepsilon g(t) \label{2}\tag{2}$$ into \eqref{1}, where $0 < |\varepsilon| \ll 1$ , to see that the equation for $g$ (neglecting $\mathcal{O}(\varepsilon^2)$ terms) will be $$ \dot{g}(t) + f'(x^*) g(t) = 0 \label{3} \tag{3}.$$ However, suppose that we want to perturb the ODE \eqref{1} by some external signal modelled by another time-dependent function $\sigma(t)$ (whose precise expression is not available) such that $\sigma(t) \to 0$ as $ t \to \infty$ (we can also impose the condition that $|\sigma(t)|$ is bounded by some exponentially decaying function $\mathrm{e}^{-\lambda t}$ ), i.e., we consider the perturbed ODE $$\dot{x}(t) + f\left(x(t)\right) + \sigma(t) = 0 \label{4}\tag{4}$$ such that $x^*$ remains to be a long-time equilibrium state. May I know how can we can ""linearize"" the equation \eqref{4}? Apparently, employing the ansatz \eqref{2} will not give us a equation for $g$ at the order of $\varepsilon$ ...","['nonlinear-system', 'stability-in-odes', 'linearization', 'ordinary-differential-equations']"
4748746,Is simple connectedness of subsets of $\mathbb{R}^2$ detected by winding numbers?,"If $X\subset\mathbb{R}^2$ is a connected open subset which is not simply connected, then there exists some point $p\in\mathbb{R}^2\setminus X$ such that the inclusion map $X\to\mathbb{R}^2\setminus \{p\}$ induces a nontrivial map on $\pi_1$ .  In other words, if a connected open subset of the plane is not simply connected, this is always because it contains a loop which winds around some point in its complement.  More strongly, in fact, there is some point $p\in\mathbb{R}^2\setminus X$ such that $X$ contains a simple closed curve that has $p$ in its interior.  (See here for one way to prove this.) My question is whether this is still true for arbitrary (not necessarily open) subsets of $\mathbb{R}^2$ .  More precisely: Suppose $X\subset\mathbb{R}^2$ is path-connected but not simply connected.  Must there exist some point $p\in\mathbb{R}^2\setminus X$ such that the inclusion map $X\to\mathbb{R}^2\setminus \{p\}$ induces a nontrivial map on $\pi_1$ ?  More strongly, must there exist such $p$ such that $X$ contains a simple closed curve that has $p$ in its interior, or at least a curve with winding number $1$ around $p$ (so the map on $\pi_1$ is surjective)? I suspect the answer is yes, since I've heard of various results saying that arbitrary subsets of $\mathbb{R}^2$ are homotopically ""tame"" in various ways (for instance, unlike in higher dimensions, they cannot have nontrivial $H_k$ for $k\geq 2$ ).  The closest result to my question that I know of is the result from this answer that if $X\subseteq \mathbb{R}^2$ then any nontrivial element in $\pi_1(X)$ remains nontrivial in $\pi_1(\mathbb{R}^2\setminus F)$ for some finite set $F\subseteq \mathbb{R}^2\setminus X$ .  This is very close to my question: it is weaker since it involves a finite set $F$ rather than a single point, but stronger since it applies to an arbitrary nontrivial element of $\pi_1(X)$ rather than just saying there exists an element of $\pi_1(X)$ that remains nontrivial.  I would guess you can prove an affirmative answer to my question either from that result or using ideas similar to its proof, but I don't see an obvious way to do so.  (Note that the finite set $F$ in that result cannot be reduced to a single point, since your element of $\pi_1(X)$ could be a commutator of loops around different points.) (Alternatively, by the argument I linked above in the case of open subsets, it would suffice to show that there exists a nontrivial loop in $X$ with only finitely many self-intersections, since then you can use that to reduce to a nontrivial simple loop and then invoke the Jordan curve theorem.)","['general-topology', 'fundamental-groups', 'algebraic-topology']"
4748762,Prove boundedness from below and convexity,"For any $a>0$ , define $$f(x):=\cosh(a\sin x)-\cos(a\cos x).$$ It appears from plots to be true that $f(x)\ge 1-\cos a$ and $f(x)$ is convex over $\big[0,\frac\pi4\big]$ . How would one prove this? Better yet, is there a lower bound simple function, such as an exponential function, which is also bounded from below by some positive number such as $1-\cos a$ and is convex on $\big[0,\frac\pi4\big]$ ? We can of course examine the second derivative of $f$ which is \begin{align}&\frac{d^2 f(x)}{dx^2} \\
=& (a\cos x)^2\cosh(a\cos x)-a\sin x\sinh(a\sin x)+(a\sin x)^2\cos(a\cos x) \\
&-a\cos x\sin(a\cos x)
\end{align} But how do we show the above is positive for $x\in \big[0,\frac\pi4\big]$ ?","['inequality', 'convex-analysis', 'trigonometry', 'real-analysis']"
4748772,How to prove $\limsup(A_n\setminus A_{n+1})=\limsup(A_{n+1}\setminus A_n)?$,"For a set sequence $\{A_n\}$ , I want to prove $$\limsup(A_n\setminus A_{n+1})=\limsup(A_{n+1}\setminus A_n).$$ I  wonder how to use the def $\limsup A_n=\bigcap_{i=1}^\infty\bigcup_{k=i}^\infty A_n$ to prove it. Here's my proof from another aspect: define a map $$A:X\to\{0,1\}^{\Bbb N},$$ $$x\mapsto(a_1,a_2,\cdots,a_n,\cdots),$$ the mapping rule is if $x\in A_i,a_i=1$ , for others $a_i=0.$ Then: $x\in\limsup(A_n\setminus A_{n+1})=\limsup(A_n\cap A_{n+1}^c)$ iff the ordered pair $(1,0)$ appears infinitely often in the sequence $A(x)$ $x\in\limsup(A_{n+1}\setminus A_n)=\limsup(A_{n+1}\cap A_n^c)$ iff the ordered pair $(0,1)$ appears infinitely often in the sequence $A(x)$ , and those two situations are the same.",['elementary-set-theory']
4748779,Why is $f_L$ continuous?,"When I read the paper “Hillel Gauchman, Pinching theorems for totally real minimal sub manifolds of $\mathbb{C}P^n(c)$ ”, I don't understand why the following function $f_L$ is continuous ? Let $M$ be a Riemannian manifold and $L$ be a $(0,k)$ -tensor field on $M$ . At any $x\in M$ , $L$ can be considered as a multilinear map $L:T_xM\times \cdots \times T_xM\to \mathbb R$ . Let $UM$ be the unit tangent bundle of $M$ and $UM_x$ its fiber at $x\in M$ . We say that $u \in UM_x$ is a maximal direction at $x$ with respect to $L$ , if it satisfies $L(u,\cdots ,u)=\max_{v\in UM_x}L(v,\cdots ,v)$ . For any $x\in M$ , set $f_L(x)=L(u,\cdots ,u)$ , where $u$ is a maximal direction at $x$ with respect to $L$ . Perhaps this is similar to another question, namely, is the maximum value of the section curvature at each point on the Riemannian manifold a continuous function?","['riemannian-geometry', 'differential-geometry']"
4748816,I'm looking for a function that is continuous and monotone increasing between a and b and maps to the entire real line.,"After some trial-and-error I've got this: $$f(x;a,b)=\textrm{arctanh}\left( 2\frac{x-a}{b-a}-1 \right)$$ Illustrated here on WolframAlpha: arctanh((2*((x-a)/(b-a))-1)) where a=2 and b=12 It kind of does what I wanted, but I wonder if there are any functions like this that doesn't use trigonometric functions. Maybe something like an inverse general sigmoid function. I haven't been able to come up with anything like that.","['continuity', 'functions', 'monotone-functions']"
4748825,Is $\limsup x_n = a \iff \limsup |x_n - a |= 0$ true?,"We know that $\lim x_n = a$ is equivalent to $\lim |x_n-a|=0$ . I am trying to show that $$\limsup x_n = a \iff \limsup |x_n - a |= 0$$ Define $s_n := \sup_{m \geq n} x_m$ . By definition, $\limsup x_n = \lim s_n$ . So $$\limsup x_n = a \iff \lim s_n = a \iff \lim |s_n - a|=0$$ Note that: $s_n - a = \sup_{m \geq n} x_m - a = \sup_{m \geq n} \{x_m - a\}=: \bar s_n$ . So: $$\limsup x_n = a \iff \lim |\bar s_n|=0 $$ Moreover, defining $\tilde s_n := \sup_{m \geq n} |x_m - a|$ $$\limsup |x_n - a |= 0 \iff \lim \tilde s_n  = 0$$ It remains to show that $$\lim |\bar s_n|=0 \iff \lim \tilde s_n  = 0 $$ But I think that it is not true. Do you have a counterexample?  or... is this true?","['limits', 'supremum-and-infimum', 'real-analysis']"
4748828,Evaluate the integral $\int\limits_{\frac{1}{3}}^{\frac{1}{2}} {\frac{{\ln \left( {1 - x} \right)\ln \left( {1 - 2x} \right)}}{x}dx} $,"I am trying to evaluate this integral $$I = \int\limits_{\frac{1}{3}}^{\frac{1}{2}} {\frac{{\ln \left( {1 - x} \right)\ln \left( {1 - 2x} \right)}}{x}dx} $$ Here is my try: $${\text{Let}}:x \to 1 - 2x \Rightarrow I = \int\limits_0^{\frac{1}{3}} {\frac{{\ln \left( x \right)\ln \left( {1 + x} \right)}}{{1 - x}}dx}  - \ln \left( 2 \right)\int\limits_0^{\frac{1}{3}} {\frac{{\ln \left( x \right)}}{{1 - x}}dx} $$ Since the antiderivative of $${\frac{{\ln \left( x \right)}}{{1 - x}}}$$ is $${{\text{L}}{{\text{i}}_2}\left( { 1-x} \right)}$$ then $$I = \int\limits_0^{\frac{1}{3}} {\frac{{\ln \left( x \right)\ln \left( {1 + x} \right)}}{{1 - x}}dx}  - \ln \left( 2 \right)\left( {{\text{L}}{{\text{i}}_2}\left( {\frac{2}{3}} \right) - \frac{{{\pi ^2}}}{6}} \right)$$ I am stucking at the remaining integral, I tried to use series expansion of: $\displaystyle \frac{1}{{1 - x}} = \sum\limits_{n = 0}^\infty  {{x^n}}$ but after changing summation and integration, the integral became more complicated. May I ask for help? Thank you so much. By the way, the closed form is $$\frac{{3{\text{L}}{{\text{i}}_3}\left( {\frac{1}{4}} \right)}}{4} - {\text{L}}{{\text{i}}_2}\left( {\frac{2}{3}} \right)\ln (2) + {\text{L}}{{\text{i}}_2}\left( {\frac{1}{4}} \right)\ln (2) + \frac{{\zeta (3)}}{8} + {\ln ^3}(2) + \frac{{{{\ln }^3}(3)}}{3} - {\ln ^2}(3)\ln (2) + \frac{1}{{12}}{\pi ^2}\ln (2)$$","['integration', 'calculus']"
4748830,"Example of everywhere defined, discontinuous identity map of banach spaces?","The lecture notes ""The open mapping theorem and related theorems"" by Anton R Schep begins with a lemma that asks that the identity map $I : (X, ||\cdot||_1) \to (X, ||\cdot||_2)$ is continuous. I want an example where the identity map is (a) everywhere defined, and (b) discontinuous . All the examples that I can think of involve imposing different norms on the same space, (say , imposing $L_p$ versus $L_q$ on $C[0, 1]$ ). But these kinds of impositions mean that not all elements $f \in C[0, 1]$ with finite $L_p$ norm has finite $L_q$ norm for $q > p$ , and thus the identity mapping fails to be well defined everywhere in the domain. A promising line of attack appears to be to consider a Sobolev space of functions in $C[0, 1]$ with bounded first derivative. I was then hoping to show that the differentiation operator is discontinuous. But I do not actually know a concrete example where this is the case. I would prefer a discrete example in one of the sequence spaces, if at all possible.","['operator-theory', 'functional-analysis']"
4748843,About the closest linear function to an arbitrary function in L1 norm,"Let $\mu$ be a probability distribution over $\mathbb{R}^n$ . All functions discussed henceforth are from $\mathbb{R}^n$ to $\mathbb{R}$ . Let $l^\ast$ be a linear function and $f$ be a function such $f=l^\ast$ on a set that contains strictly larger than 50% fraction of the probability mass. Show that $l^\ast$ is a minimizer of the $L_1(\mu)$ error $\|f-l\|_{L_1(\mu)}$ over all linear functions $l$ . I tried to prove this for specific $f$ 's to get intuition. But, even for specific $f$ 's, there is a lot of cases to handle. It didn't look nice at all, and I decided to give up. I looked at things surrounding the area ""robust linear regression"" on the internet, but couldn't find an explanation for this problem. I tried to find inspiration from the various proofs that median minimizes $\ell_1$ error of some finite set of reals. In this direction, I tried to find the point at which gradient of the objective function is equal to zero. Here too, I got to a point where there was a large number of cases to handle, and decided to give up.","['linear-regression', 'robust-statistics', 'statistics', 'functional-analysis']"
4748900,Harmonic series multiplied by $-2$ every third term converges to $\ln 3$,"Prove that the series $$\sum_{n=1}^\infty a_n = 1+\frac 1 2 -2\cdot \frac 1 3+\frac 1 4 + \frac 1 5 - 2\cdot \frac 1 6 + \cdots $$ converges to $\ln (3)$ . Inserting parentheses every three terms (inserting parentheses of bounded lengths does not damage convergence), we have $$ \begin {align} \sum_{n=1}^\infty a_n  &= \sum_{k=1}^\infty  \Big( \frac 1 {3k-2} + \frac 1 {3k-1} - \frac 2 {3k} \Big) \\
&=\sum_{k=1}^\infty  \frac {9k-4} {27k^3-27k^2+6k}, \end{align}
$$ so the series converges by comparing with $\sum 1/k^2$ . However, I don't see a direct approach to showing that it converges to $\ln (3)$ . Is it possible to do so with Riemann sums? Or using power series of $\ln (2\cdot x/2) = \ln (2) + \ln(x/2)$ ? EDIT: What I refer to by saying ""inserting parentheses of bounded length"" is the following theorem: Let $(a_n)_{n=1}^\infty$ a sequence tending to $0$ . Let $(n_k)_{k=0}^{\infty}$ a strictly ascending sequence of nonnegative integers such that $n_0=0$ , and define the regrouping $b_k=\sum_{n=n_{k-1}+1}^{n_k}a_n$ for all $k\ge 1$ . Then, if $(\Delta n_k)_{k=1}^\infty = (n_k - n_{k-1})_{k=1}^\infty$ is a bounded sequence, the series $\sum_{n=1}^\infty a_n$ converges if and only if $\sum_{k=1}^\infty b_k$ converges, and in this case, both tend to the same limit. (And moreover, this is true also in the more general case in which $\sum_{n=n_{k-1}+1}^{n_k} |a_n| $ tends to $0$ as $k\to \infty$ ).","['real-analysis', 'harmonic-numbers', 'sequences-and-series', 'limits', 'convergence-divergence']"
4748912,Closed embedding is not faithful on derived category,"I was surprised to hear that if $j: Y \to X$ is a closed embedding, then $j_*: D^b(Y) \to D^b(X)$ is in general not faithful (note you shouldn't expect fullness, since $\operatorname{Ext}^1(O_p, O_p) \cong CT_p$ ) ! Although one can show that for sheaves $F,G$ , we have $\operatorname{Hom}^1(F,G) \hookrightarrow \operatorname{Hom}^1(j_*F,j_*G)$ . Could someone give an example of nonfaithfulness (for a regular closed embedding)? Here is a ""proof"" that might hint how to find such an example. Suppose we have $F,G \in D^b(Y)$ and we want to show $\operatorname{Hom}(F,G) \hookrightarrow \operatorname{Hom}(j_*F,j_*G)$ . It's enough to prove the theorem locally (via the base change formula). Locally suppose we have a bundle $V_X$ with $s \in H^0(V)$ cutting out $Y$ , so that we have the koszul resolution $$\wedge^r V^* \to ... \to V^* \to O_X \cong j_*O_Y.$$ We are working locally I remind you. Take $\operatorname{Hom}_Y(F,G)$ , first resolve $F$ using only copies of $O_Y$ (which is projective locally so the hom describes the actual hom in the derived category if we use the resolution) via the resolution of $F$ we denote $$I := ... \to I_1 \to I_0 \cong F.$$ We then consider $\operatorname{Hom}_X(j_*I,j_*G)$ , now $j_*I$ is not projective anymore (since $j_* O_Y$ isn't) but we can resolve each $j_* O_Y$ using the Koszul resolution described above. Now I want to say (and this is the suspicious part) that we can resolve $I$ by pointwise resolving using the Koszul resolution (i.e if we have a map $f:O_Y \to O_Y$ we can lift $f$ to $\tilde{f}: O_X \to O_X$ and so on for the complex koszul complex). If the extension claim is true, then when we use $\operatorname{Hom}(*,F)$ all the koszul maps vanish becoming a direct sum the first summand of which is $O_X \otimes O_Y = O_Y$ which would imply faithfulness. The extension claim is probably not true; while $O_X \to O_Y$ is surjective we won't be able to consistently lift stuff into a complex. For instance the horseshoe lemma forces one of the resolutions and doesn't let you control all of them.","['derived-categories', 'algebraic-geometry', 'examples-counterexamples']"
4748917,Challenging integral evaluation,"I computed the following integral and got that $$\int_S [x_1y_3+x_2(y_2+y_3)+x_3(y_1+y_2)]^n =O\Big(\frac{\log(n)}{n^3}\Big).$$ The integral is over the set $S=\{x_1+x_2+x_3=1, y_1+y_2+y_3=1 | x_i,y_i\geq 0\}$ . I now want evaluate a somewhat different integral, of the form: $$\int\limits_S
 \sum\limits_k
     \binom{n}{k}\frac1k
      [x_1y_3+x_2(y_2+y_3)+x_3(y_1+y_2)]^{n-k}(x_3y_3)^k$$ over the same domain. So far I have found an upper bound of the form $O\Big(\frac{\log^2(n)}{n^2}\Big)$ , which is not good for me. I believe that this upper bound can be improved to $o\Big(\frac1{n^2}\Big)$ but cannot prove this currently. Is there any good way to evaluate this integral? Any way to prove\disprove that it's $o\Big(\frac1{n^2}\Big)$ ?","['integration', 'combinatorics', 'beta-function', 'probability', 'hypergeometric-function']"
4748928,Help understanding what is the projection in problem II.6.3 (a) Hartshorne,"In algebraic geometry by Robin Hartshorne, exercise II.6.3.a is written as follows Cones. In this exercise, we compare the class group of a projective variety $V$ to the class group of its cone (I, Ex. 2.10). So let $V$ be a projective variety in $\mathbf{P}^n$ , which is of dimension $\geqslant 1$ and nonsingular in codimension 1. Let $X=C(V)$ be the affine cone over $V$ in $\mathbf{A}^{n+1}$ , and let $\bar{X}$ be its projective closure in $\mathbf{P}^{n+1}$ . Let $P \in X$ be the vertex of the cone. (a) Let $\pi: \bar{X}-P \rightarrow V$ be the projection map. Show that $V$ can be covered by open subsets $U_i$ such that $\pi^{-1}\left(U_i\right) \cong U_i \times \mathbf{A}^1$ for each $i$ , and then show as in (6.6) that $\pi^*: \mathrm{Cl} V \rightarrow \mathrm{Cl}(\bar{X}-P)$ is an isomorphism. Since $\mathrm{Cl} \bar{X} \cong$ $\mathrm{Cl}(\bar{X}-P)$ , we have also $\mathrm{Cl} V \cong \mathrm{Cl} \bar{X}$ . I don't understand the definition of the projection map. I do understand the definition of the projection from $C(V) -\{0\}\to V$ but not as defined in the exercise","['algebraic-geometry', 'projective-geometry', 'projective-schemes', 'schemes']"
4748994,Help for rewriting induced representation,"Let $G$ be a group and $H$ a subgroup. My lecture book (Algebra by Cohn) defines the (right) $G$ -module induced by a right $H$ -module $U$ by $$\tag{1}
\text{ind}_H^GU=U\otimes_H kG,
$$ where $kG$ is the group algebra. He writes that if we take a coset representation of $G$ $$\tag{2}
G=Ht_1\cup\dots \cup Ht_r,
$$ then we can write Equation $(1)$ as $$\tag{3}
\text{ind}_H^GU=U\otimes Ht_1\oplus \dots\oplus U\otimes Ht_r.
$$ However, I am not sure how he gets from Equation $(1)$ to Equation $(3)$ .
I think that one thing that makes it more confusing is that he uses sloppy notation. The group algebra is defined $$\tag{4}
kG:=\Big\{\sum_{g\in G}a_g\boldsymbol{g}\mid a_g\in k\Big\}.
$$ Define $$\tag{5}
(kH)t_i:=\Big\{\sum_{h\in H}a_h\boldsymbol{ht}_i\mid a_h\in k\Big\}.
$$ Since the collection of $t_i$ define a transversal, each $g\in G$ can be written uniquely in the form $ht_i$ , so an arbitrary $\sum_{g\in G}a_g\boldsymbol{g}\in kG$ can be written uniquely in the form $$\tag{6}
\sum_{g\in G}a_g\boldsymbol{g}=\sum_{h,i}a_{h}^{(t_i)}\boldsymbol{ht_i}=\sum_{i}\Big(\sum_{h}a_{h}^{(t_i)}\boldsymbol{ht_i}\Big).
$$ By definition of the direct sum of vector spaces, this implies that $$\tag{7}
kG=\bigoplus_{i}(kH)t_i,
$$ Since the tensor product is distributive with respect to a direct sum, it then follows from Equation $(7)$ that we can rewrite Equation $(1)$ as $$\tag{8}
\text{ind}_H^GU=U\otimes_H kG=\bigoplus_iU\otimes_H (kH)t_i.
$$ It therefore appears that when he writes $Ht_i$ in Equation $(3)$ , he really means $(kH)t_i$ . Is this correct, or am I misinterpreting something? Edit : For vector spaces, we have the relation $$
U\otimes (V\oplus W)\cong U\otimes V\oplus U\otimes W,
$$ so it appears that the induced $G$ -module should really be written $$
\text{ind}_H^G\cong \bigoplus_iU\otimes_H (kH)t_i.
$$ So when Cohn writes an equality does he really mean an isomorphism?","['representation-theory', 'vector-spaces', 'modules', 'abstract-algebra', 'group-theory']"
4749002,Hausdorff measure of $f(E)$ is greater than zero where $E$ compact and $f$ is (...)?,"I'm studying example 4.1.3 of ""Variational Analysis in Sobolev Spaces and BV Spaces"" by Attouch, Buttazzo and Michaille. At one point it states that Given $f: R^n \to R^m \> (n\leq m)$ one to one of class $C^1(R^n)$ (i.e. with all partial derivatives continous) and given a compact $E$ of $R^n$ Then $n$ -dimensional Hausdorff measure of f(E) is greater than zero. My approach: Being $E$ compact and $f\in C^1(R^n)$ follows $f$ is Lipschtiz on $E$ , so from proposition 4.1.6 of the same book one has $H^n(f(E))=\int_E \Big(  \sum_{i=1}^C |J_i|^2  \Big)^{\frac{1}{2}} d\>m_n$ where $J_i$ are the minors $n\times n$ of Jacobian matrix $J(f)$ By this way is enough to prove that not all minors $n\times n$ are equals to zero that is the rank of Jacobian is max that is the differential of $f$ is injective.
If f has been $R^n \to R^n$ it would be simple from composition of $J(f)$ $J(f^{-1})$ but in this case I don't know the right route.","['jacobian', 'measure-theory', 'hausdorff-measure', 'lipschitz-functions']"
4749009,Proving that an alternative sum of factorials is zero,"I have to prove that for every $k$ the following differential equation is fulfilled by monomials of even degree less than $2k$ : $$
\sum_{j=1}^ka_j^{(k)}x^{j-1}f^{(j)}(x)=0,
$$ with $a_j^{(k)}:=\frac{(2k-j-1)!}{(j-1)!\,(k-j)!\,(-2)^{k-j}}$ .
By linearity I can assume $f(x)=x^{2h}$ for some $h=0,...,k-1$ . If I put it into the equation I obtain something equivalent to $$
\sum_{j=1}^{k}(-2)^{j}\frac{(2k-j-1)!}{(j-1)!(k-j)!}(2h)_j=\sum_{j=1}^{min(k,2h)}(-2)^{j}\frac{(2k-j-1)!(2h)!}{(j-1)!(k-j)!(2h-j)!},
$$ where $(x)_n$ means the falling factorial or the Pochhammer symbol, then computing initial cases I get $0$ . How to prove that it is always $0$ ? My attempt is by induction: the case $h=0$ is obvious. Now, suppose that for some $h=0,...,k-2$ it holds $\sum_{j=1}^{k}(-2)^{j}\frac{(2k-j-1)!}{(j-1)!(k-j)!}(2h)_j=0$ , then for $h+1$ we have $$
\begin{split}
&\sum_{j=1}^{k}(-2)^{j}\frac{(2k-j-1)!}{(j-1)!(k-j)!}(2h+2)_j,
\end{split}
$$ but I cannot use my induction hypothesis. Moreover, something has to tell me that for $h=k-1$ the induction should stop working, since for $h=k$ the sum is not zero. Another way to solve the problem is trying to prove that the sum becomes polynomial in $h$ , with $\{0,1,...,k-1\}$ as its roots: $$
\sum_{j=1}^{k}(-2)^{j}\frac{(2k-j-1)!}{(j-1)!(k-j)!}(2h)_j=(-4)^k\prod_{j=0}^{k-1}(h-j)=(-4)^k(h)_{k+1}.
$$ For this purpose, I've tried to compute the coefficients of those polynomials: it is easy to show that $a_0=0$ for the coefficient $a_l$ it is enough to derive the formal expression $l$ -times and then evaluate in $h=0$ . For example $a_1=(-4)^k(k-1)!$ for both, but the $l$ -derivative is hard to compute. However, this second way is preferable, since it proves that the sum is zero if and only if $0<h<k-1$ . If I try to compute the sum with Wolfram alpha it gives me something related to the hypergeometric function ( https://en.wikipedia.org/wiki/Hypergeometric_function ), but I don't know how to use that information.","['calculus', 'combinatorics', 'summation', 'real-analysis']"
4749040,Measurability of the set of maximal elements of a poset,"Let $(E, <)$ be a poset. We can endow $E$ with the order topology, and thus view it as a measurable space $(E,\mathcal{E})$ with the Borel $\sigma$ -algebra. Is the set $M$ of maximal elements of $E$ , $M:= \{x : y \leq x \text{ for all } y \}$ measurable with respect to $(E,\mathcal{E})$ ? It is quite straightforward to write $M = \bigcap_{y \in E} (-\infty,y)^c$ , where $(-\infty,y) := \{ x \in E: x < y\}$ . Hence, one sufficient condition is that $E$ is countable. However, I am wondering if there are any more necessary or sufficient conditions we can impose. Countability is not necessary. For example, $M$ is measurable for $[0,\infty]$ with the usual order, since $M = \{\infty\} = \bigcap_{n \in \mathbb{N}} [0,n]^c$ . More generally, $M$ will be countable if we can determine whether an element is maximal by only 'comparing' it against countably many other elements, but I am not sure if this is a notion that has been formalised.","['order-theory', 'general-topology', 'measure-theory']"
4749122,How can we use a cartesian product in the definition for bipartite graph when its elements are tuples and edges can be sets?,"I have learnt that in a bipartite graph, we can partition vertices into 2 sets $V_1$ and $V_2$ , and $E \subseteq V_1 \times V_2$ . However, the cartesian product is a set of tuples, and edges (in an undirected graph) are sets. So how can $E$ be a subset of this Cartesian product when a set is a different mathematical object than a tuple, so none of its elements are equal to any of the elements in the Cartesian product?","['elementary-set-theory', 'graph-theory', 'bipartite-graphs', 'discrete-mathematics']"
4749147,Struggling to figure out why $\int\limits_{0}^{1}\frac{\ln \left(x^{2}\right)}{\left(1+x^{2}\right)^{2}}dx = -G-\frac{\pi}{4}$.,"Struggling to figure out why $$\int\limits_{0}^{1}\frac{\ln \left(x^{2}\right)}{\left(1+x^{2}\right)^{2}}dx = -G-\frac{\pi}{4}$$ I've taken note of the exponent in the logarithm, tried substituting $x=\tan\theta$ , and even differentiating under the integral sign with the function $J(a)=\int\limits_{0}^{1}\frac{\ln\left(ax\right)}{\left(1+x^{2}\right)^{2}}$ , but all to no avail. The trigonometric substitution leads me to $$\int\limits_{0}^{\frac{\pi}{4}}\ln\left(\tan^{2}\theta\right)\cos^{2}\theta d\theta$$ which is verifiably equivalent to the original expression, but it also looks ugly and I don't know where to go from here (Surely not integration by parts, since then it either gets very ugly or births a divergent integral). I haven't tried complex analysis but since the bounds are 0 to 1 I doubt that would work. I know that $-G=\int\limits_{0}^{1}\frac{\ln x}{1+x^{2}}dx$ but I can't tell how to transform my expressions into this one. There must be some sneaky but potent substitution that I'm missing. Judging by the Catalan's constant in the solution, I assume there's also some series, presumably for the logarithm, that could be used. Could anyone share some insights, please? This is my first time using this forum, apologies in advance if I did anything wrong.","['integration', 'calculus', 'catalans-constant']"
4749181,Tangents to a projective plane curve.,"I'm studying William Fulton's ""Algebraic Curves,"" and I'm currently studying projective plane curves. One of the problems in the section asks us to find the tangents to $xy^4+yz^4+xz^4$ at its multiple points. However, nowhere in the section does he define what the tangent is. The only thing he comments on is, ""We can define a line L to be tangent to a curve F at P if $I(P, F \bigcap L)>m_P(F).$ "" Where $m_P(F)$ denotes the multiplicity of $F$ at $P$ , and $I(P,F\bigcap L)$ is the intersection number of $F$ and $L$ and $P$ . How am I supposed to use this to find the tangent lines to a given curve? I know how to do this with an affine plane-curve, and my intuition was to find a tangent line in the affine case at the level curve $Z=1$ , and then find the plane which contains that line, and define it as the tangent line to $xy^4+yz^4+xz^4$ , but based off of examples I've been trying to piece together online, it seems like this is the incorrect definition.","['algebraic-curves', 'algebraic-geometry', 'projective-geometry', 'projective-space']"
4749202,Comparison of terms in local formulation of Christoffel-symbols in relation to isometries.,"Let $(M,g),(N,h)$ be semi-riemannian manifolds, and $\varphi \in C^{\infty}(M,N)$ an isometry (hence a diffeomorphism). Let $(U,\psi = (x^1,\ldots,x^n))$ be a coordinate chart of a point $p \in U \subset M$ , and $(\varphi(U),\psi \circ \varphi^{-1} = (y^1,\ldots,y^n))$ be a chart of $\varphi(p) \in \varphi(U) \subset N$ . By a (I believe) standard-theorem, we get $$T_p\varphi\Big(\frac{\partial}{\partial x^i}\Big|_p\Big)$$ $$ = \sum_{j = 1}^{n} \frac{\partial ((\psi \circ \varphi^{-1})^j \circ \varphi \circ \psi^{-1})}{\partial x^i}(\psi(p)) \cdot \frac{\partial}{\partial y^j}\Big|_{\varphi(p)}$$ $$ = \sum_{j = 1}^{n} \frac{\partial x^j}{\partial x^i} \frac{\partial}{\partial y^j}\Big|_{\varphi(p)} = \frac{\partial}{\partial y^i}\Big|_{\varphi(p)}.$$ Now, by the definition of isometries, and the calculation above, we get $$g_{ij}(p) := g\Big(\frac{\partial}{\partial x^i}\Big|_p,\frac{\partial}{\partial x^j}\Big|_p\Big) = h\Big(T_p\varphi\Big(\frac{\partial}{\partial x^i}\Big|_p\Big),T_p\varphi\Big(\frac{\partial}{\partial x^j}\Big|_p\Big)\Big) = h\Big(\frac{\partial}{\partial y^i}\Big|_{\varphi(p)},\frac{\partial}{\partial y^j}\Big|_{\varphi(p}\Big) = h_{ij}(\varphi(p)).$$ Now, to my question: Why does this imply $$\frac{\partial}{\partial y^k}\Big|_{\varphi(p)} h_{ij} = \frac{\partial}{\partial x^k}\Big|_p g_{ij}?$$ I feel like I am missing something obvious. The reason we want to show this is to see that $${}^g\Gamma^k_{ij}(p) = {}^h\Gamma^k_{ij}(\varphi(p))$$ for the Christoffel-symbols on $U,\varphi(U)$ respectively.","['isometry', 'semi-riemannian-geometry', 'differential-geometry']"
4749219,Convex combination of Dirichlet random variables,"For positive integer $k$ , let $(X_1,\ldots,X_k)\sim\mathrm{Dir}(\alpha_1,\ldots,\alpha_k)$ be a probability distribution over $k$ items drawn from a $k$ -component Dirichlet distribution and $p=(p_1,\ldots,p_k)$ be another fixed distribution. What is the pdf of the random variable $Q=\sum_{i=1}^k p_i X_i$ ? If each $X_i$ were independent gamma random variables with parameter $\alpha_i$ , i.e., $X_i\sim\mathrm{Gamma}(\alpha_i,\beta)$ , then this would be easy: by linearity, $Q\sim\mathrm{Gamma}(\sum_{i=1}^k p_i\alpha_i, \beta)$ , following the notations in this lecture note . The Dirichlet random variables can be obtained by normalizing the gamma random variables, and the marginal of each component is a beta random variable. A way to show this is the case is done by observing that if $Y_i\sim\mathrm{Gamma}(\alpha_i,1)$ for $i\in\{1,2\}$ , then $\frac{Y_1}{Y_1+Y_2}\sim\mathrm{Beta}(\alpha_1,\alpha_2)$ . This requires that $Y_1$ and $Y_2$ are independent from this post . I suspect that $Q$ is a beta random variable, since it looks like a convex combination of gamma random variables up to normalization. An obstacle that prevents me from showing this is that for $X_i\sim\mathrm{Gamma}(\alpha_i,\beta)$ , $\sum_{i=1}^k p_i X_i$ is no longer independent of $X_1+\ldots+X_k$ (unless $p$ has some special form). Was convex combination of Dirichlet components studied before? Any comments will be appreciated.","['gamma-distribution', 'probability-distributions', 'probability-theory', 'probability']"
4749221,Show $\sum\limits_{i=1}^n \sum\limits_{j=1}^n \cos(x_i - x_j) \geq 0$ for all real sequences $(x_i)_{1\leq i\leq n}$ [duplicate],"This question already has an answer here : Why is $g = \sum_{k=1}^n $ $\sum_{l=1}^n cos(g_k-g_l)$ always non-negative? (1 answer) Closed 11 months ago . This inequality must be well known, and possibly easy to prove but I could not find it in the literature or here. Does anyone have a proof of $$
\forall n\in \mathbb{N}^*, \forall x_k \in \mathbb{R}, 
\sum\limits_{i=1}^n \sum\limits_{j=1}^n \cos(x_i - x_j) \geq 0.
$$ P.S.: The cases $n=2$ and $x_1=0$ , $x_2 = \pi$ , or the case $n=4$ and $x_k= \frac{k-1}{2}\pi$ , etc... show that the inequality is sharp.","['double-sequence', 'inequality', 'trigonometry']"
4749262,Can tautologies have free variables?,"In propositional logic, a tautology is defined as a statement that is true no matter the truth values assigned to its propositional variables. So, in propositional logic, All apples are red or there exists an apple that is not red is a tautology. Now, Wikipedia says: A tautology in first-order logic is a sentence that can be obtained by taking a tautology of propositional logic and uniformly replacing each propositional variable by a first-order formula (one formula per propositional variable). So, I understand that in first-order logic, (∀x∈N  x is even) ∨ ¬(∀x∈N  x is even) is a tautology as it is obtained by replacing the propositional variable A in the tautology A ∨ ¬A with (∀x∈N  x is even) . However, I haven’t been able to find a text that states whether open sentences can be tautologies. For example, the open sentence (x is even) ∨ ¬(x is even) is obtained by replacing the propositional variable B in the tautology B ∨ ¬B with x is even . B would be an open sentence, which doesn’t have a truth value, so how could B ∨ ¬B be true? Is (x is even) ∨ ¬(x is even) a tautology?","['propositional-calculus', 'predicate-logic', 'logic', 'discrete-mathematics']"
4749287,Finding the range of a function defined as an integral,"The task is to determine the range of the function $$ f(x) = \int_x^{2x} \frac{e^{-xt^2}}{t}dt, \;\; x>0 $$ and I have tried to do this by studying its derivative in hopes of finding critical points.
I differentiate it by introducing an auxiliary function $$ g(x_1,x_2,x_3) = \int_{x_3}^{x_2} \frac{e^{-x_1t^2}}{t}dt $$ so that $f(x) = g(x,2x,x)$ . I compute the total derivative of f by using the Fundamental Theorem of Calculus and by differentiating under the integral sign: $$\frac{d}{dx} g(x_1,x_2,x_3) = \frac{\partial g}{\partial x_1} + \frac{\partial g}{\partial x_2}\frac{\partial x_2}{\partial x} + \frac{\partial g}{\partial x_3}\frac{\partial x_3}{\partial x} $$ $$= \int_{x_3}^{x_2} -te^{x_1 t^2}dt +2\frac{e^{-x_2^2x_1}}{x_2} -\frac{e^{-x_3^2x_1}}{x_3} $$ so $$f'(x)=-\int_{x}^{2x} te^{-t^2x}dt + \frac{e^{-4x^3}}{x}-\frac{e^{-x^3}}{x}  $$ Next, I compute the integral in the expression for $f'(x)$ : $$ -\int_{x}^{2x}te^{-t^2x}dt = \frac{1}{2x}\int_{x}^{2x}(-2xt)e^{-t^2x}dt = \frac{1}{2x}\left[ e^{-t^2x} \right]_x^{2x} \\
= \frac{1}{2x}(e^{-4x^3}-e^{-x^3} ) $$ so it turns out $f'(x) = \frac{3}{2x}(e^{-4x^3}-e^{-x^3}) $ , but this does not have a root that is not also a singular point (let me know if I am using the wrong terms). I do not have any other ideas for how to solve this problem.","['multivariable-calculus', 'derivatives', 'analysis']"
4749323,Combinatorial proof $\sum^n_{k=0} {{2n}\choose{k}}{{n}\choose{k}}{{2n-k}\choose{n}} = {{2n}\choose{n}}^2$,"I need to prove that: $$\sum^n_{k=0} {{2n}\choose{k}}{{n}\choose{k}}{{2n-k}\choose{n}} = {{2n}\choose{n}}^2$$ So I figured that we can think of a situation in which we have $2$ groups of pairs of people. In both of them there are $n$ men and $n$ women. In each group, each man creates a pair with some woman (same can be said from the perspective of each woman). I presented my caoncept with a simple paint drwaing below: In the equation, on the RHS we choose $n$ people from group $1$ and then $n$ from group $2$ . On the LHS we choose $k \leq n$ people from group $1$ and then $k$ men from group of men of group $2$ . Then we take men of group $2$ that were not chosen and at that point we have $n - k + k = n$ people chosen. At the end we choose $n$ people from $2n-k$ that are left in group $1$ . I know that it's wrong but I have no idea how to make it right.","['combinations', 'combinatorics', 'combinatorial-proofs']"
4749359,Find $x$ for which $\int_1^2 \frac{e^{-t}}{1+xt} dt$ is continuous and differentiable.,"Find $x \in \mathbb{R}$ for which $$f(x)=\int_1^2 \frac{e^{-t}}{1+xt} dt$$ is continuous and differentiable. And if possible, find this function and the derivative of this function in the closed form. For $x \ge 0$ , we have $\frac{e^{-t}}{1+xt} \le e^{-t}$ and $\int_1^2 e^{-t} dt$ converges. So $f(x)$ is continuous on $[0, \infty)$ . In the case of the derivative, $$\frac{\partial}{\partial{x}}\frac{e^{-t}}{1+xt}=-\frac{te^{-t}}{(1+xt)^2}$$ and again, for $x \ge 0$ , $$\Biggr|-\frac{te^{-t}}{(1+xt)^2}\Biggl| \le te^{-t}$$ $$\int_1^2 te^{-t} dt = \frac{2}{e} - \frac{3}{e^2}$$ So $$f'(x)= \int_1^2 -\frac{t e^{-t}}{(1+xt)^2} dt$$ is differentiable on $x \in [0, \infty)$ . Also, if $x \in [-1, -\frac{1}{2}]$ , then $$\lim_{t \to -\frac{1}{x}} \frac{e^{-t}}{1+xt} = \text{NOT defined}.$$ So $f(x)$ cannot be defined for $x \in [-1, -\frac{1}{2}]$ . But I'm not sure whether this interval is only exception or not. Here are my questions regarding the above function. I'm not sure how to deal with $x <0$ , is this function continous and differentiable for all $x \in \mathbb{R} \setminus [-1, -\frac{1}{2}]$ ? Also, I'd like to calculate the function or the derivative of the function in closed forms, if possible. Thank you, in advance.","['continuity', 'derivatives', 'real-analysis']"
4749362,Positivity of solutions of a system of ODEs,"Problem: Consider the following system: $$\frac{dx}{dt} = ay-bx$$ $$\frac{dy}{dt} = cx-dy,$$ where $a,b,c,d>0.$ Given positive initial conditions, I want to show that $x$ and $y$ remain positive on $[0,\infty).$ This is what I have done so far: Let $t_1$ be the first point where $x(t_1) = 0$ and $\frac{dx}{dt}(t_1) \leq 0.$ Also let $t_2$ be the first point where $y(t_2) = 0$ and $\frac{dy}{dt}(t_2) \leq 0.$ Assume that $t_1<t_2.$ By plugging in $t_1$ in the first equation we get $0\geq \frac{dx}{dt}(t_1) = ay(t_1),$ which leads to the contradiction $y(t_1)\leq 0.$ Therefore, $t_1 =t_2.$ Help please :) After this point I'm not sure what to do. If I can somehow show that, say, $x(t) = 0$ on $[t_1,\infty),$ then I can use the Identity Theorem to show that $x(t)=0$ for all positive reals and hence obtain a contradiction. But I'm not sure how I can show this. Any help is highly appreciated. Thank you in advance!","['systems-of-equations', 'ordinary-differential-equations', 'analysis', 'real-analysis', 'dynamical-systems']"
4749366,Which $n$ satisfy $4xyz=n(x+y+z)$?,"For which positive integers $n$ are there positive integers $x, y, z$ such that $4xyz=n(x+y+z)$ ? An equivalence which will turn out useful later: A positive integer $n$ is the hypotenuse of a primitive Pythagorean triangle iff every factor of $n$ is congruent to 1 modulo 4. Here's what I have so far. There is at least one solution $(x, y, z)$ for $n$ provided $n\ne 1\mod 12$ : $$\begin{align*}
n &= 2k: (k+1, k, 1) \\
n &= 3k-1: (n, k, 1) \\
n &= 4k-1: ((k+1)n, k, 1) \\
n &= 12k-3: (4k^2+11k-3, k, 3)
\end{align*}$$ Moreover, if $(x, y, z)$ solves $n=n_0$ , then $(ax, ay, az)$ solves $n=a^2 n_0$ . So we need only consider $n=p^2$ for prime $p$ , and square-free $n$ . $n$ has a solution with $x=n$ if $n$ has the form $(4z-1)y-z$ , so $4n+1=(4y-1)(4z-1)$ . So if $4n+1$ is the product of two integers congruent to 3 modulo 4, then this gives such a solution. This fails only if $4n+1$ is the product of primes all of which are congruent to 1 modulo 4, i.e. $4n+1$ is the hypotenuse of a primitive Pythagorean triangle. $n$ has a solution with $z=1$ if $n$ has a factor congruent to 3 modulo 4: Suppose $n=ab$ , and $a=4d-1$ . Then we have a solution with $x=a(y+1)$ , $y=bd$ , $z=1$ : $$\begin{align*}
4d &= a+1\\
\implies 4(bd+1)d &= a(bd+1)+bd+1\\
\implies 4a(bd+1)bd &= ab[a(bd+1)+bd+1]\\
\implies 4xyz &= n(x+y+z)
\end{align*}$$ Thus if $n$ is solutionless, every factor is congruent to 1 modulo 4, i.e. $n$ is the hypotenuse of a primitive Pythagorean triangle. The lowest solutionless $n$ are 1, 13, $25=5^2$ , 37, 97, $169=13^2$ , 181, 193, $289=17^2$ , 541, $625=5^4$ , 673, 757, $841=29^2$ , $949=13\cdot73$ . Neither this list nor the list of primes in it is in OEIS. However, the list of solutionless squares is identifiable. In his answer , Denis Shatrov proved that $n=h^2$ is solutionless iff every factor of $h$ is congruent to 1 modulo 4.","['number-theory', 'diophantine-equations']"
4749394,"Show that for nonzero integers $x, y$, the inequality $|x^2 - (1+\sqrt{2})^2y^2| \geq 1$ holds.","I am trying to show that for nonzero integers $x, y$ , the inequality $|x^2 - (1+\sqrt{2})^2y^2| \geq 1$ holds. What I have done: I think it is enough to consider positive integers $x, y$ , and let $x$ be the integer that is close enough to $(1+\sqrt{2})y$ , but I don’t know how to control $|x-(1+\sqrt{2})y|$ in this situation. Can someone help me?","['analytic-number-theory', 'number-theory', 'dynamical-systems']"
4749400,Euler's formula doesn't work for null graph?,"Given the null graph with no edges or vertices, we have a connected planar graph as no edges cross when this graph is drawn in the plane, and the fact that any two distinct vertices have a path between them is vacuously true. However, Euler's formula doesn't work: plugging into $v+f= e+2$ , we have $1=2$ . Why is this the case? Can we not apply Euler's formula here?","['graph-theory', 'graph-connectivity', 'discrete-mathematics', 'planar-graphs']"
4749426,"If $a^2+b^2+c^2+abc=4$, Find minimum $P=\sqrt{\frac{2a+bc}{3}}+\sqrt{\frac{2b+ca}{3}}+\sqrt{\frac{2c+ab}{3}}-\frac{3(a+b+c+abc)}{2}$","Let $a,b,c\ge 0: a^2+b^2+c^2+abc=4$ . Find minimum $$P=\sqrt{\frac{2a+bc}{3}}+\sqrt{\frac{2b+ca}{3}}+\sqrt{\frac{2c+ab}{3}}-\frac{3(a+b+c+abc)}{2}$$ When $a=b=c=1,$ we get that $P\ge -3$ So we need to prove $$\sqrt{\frac{2a+bc}{3}}+\sqrt{\frac{2b+ca}{3}}+\sqrt{\frac{2c+ab}{3}}\ge \frac{3(a+b+c+abc)}{2}$$ I tried to use substitution $a=\dfrac{2x}{\sqrt{(x+y)(x+z)}}$ but it is complicated
Also, by AM-GM $$\sqrt{\frac{2a+bc}{3}}+\sqrt{\frac{2b+ca}{3}}+\sqrt{\frac{2c+ab}{3}}\ge 3\sqrt[3]{\sqrt{\frac{2a+bc}{3}}+\sqrt{\frac{2b+ca}{3}}\sqrt{\frac{2c+ab}{3}}}\ge\frac{3(a+b+c+abc-2)}{2}$$ But I don't know how to prove that last one. Hope some user here can help. Thank you","['substitution', 'algebra-precalculus', 'cauchy-schwarz-inequality', 'inequality']"
4749441,Are complex numbers the only field that leads to the equivalence of analyticity and differentiability?,"One of the most beloved properties of complex numbers is that doing analysis on complex functions leads to the fact that every continuously differentiable complex function is analytic (and if it is $C^1$ it is automatically $C^{\infty}$ ) which is naturally something very comfortable to have. I was wondering what made the complex numbers so special; certainly, the fact that they constitute a field sets them apart from, for example, $\mathbb{R}^2$ , as a setting for doing analysis on it. But are they unique in that regard? A central fact about analysis on complex functions seems to be that Green's Theorem leads to Cauchy's Theorem, and then the topology of complex numbers leads to Cauchy's integral formula, from whence the equivalence of continuous differentiability and analiticity can be obtained. So maybe they are unique, because for other fields where a similar concept of differentiation this line of reasoning would not hold; but there might be other routes. So my question is as follows: let $\mathbb{F}$ be a field, and, for a function from $\mathbb{F}$ to $\mathbb{F}$ , $g$ , let a similar notion for differentiation be established (ie $g$ is differentiable on $a \in \mathbb{F} $ if $g(a+h)-g(a)=f_a\cdot h+o(h)$ , for $h \in \mathbb{F}, $ and $f_a$ is the derivative). Suppose that every continuously differentiable function in an open set (in some topology one might bestow onto $\mathbb{F}$ ) is analytic in that open set. Does that force $\mathbb{F}$ to be the complex numbers, with the Euclidean topology? Or might there be other fields, with differentent topologies, which have this property? EDIT: I would like to better frame the question following the sugestions in the comments. Firstly, let $\mathbb{F}$ be a topological field such that the operations are continuous given some arbitrary topology. Further, let $g: \mathbb{F} \rightarrow \mathbb{F}$ be a differentiable function in an open set $U$ in the following sense: $$\forall a \in U, \exists g_a \in \mathbb{F} \, \text{s.t} \hspace{2mm}g(a+h)-g(a)=g_a \cdot h+o(h)$$ for some $h \in \mathbb{F}$ and where $o(h)$ is the standard little oh definition. Suppose we have that $g$ differentiable in some open set $U$ $\iff$ $g$ is analytic in $U$ . Does this restrict the topology and the algebra of $\mathbb{F}$ enough to conclude that it should be $\mathbb{C}$ with the euclidean topology? If not, up to where could we constrain them, in view of maybe arriving at some necessary or even sufficient conditions for a topological field to have that property?","['complex-analysis', 'abstract-algebra']"
