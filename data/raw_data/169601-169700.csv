question_id,title,body,tags
2984610,Proof that the Michael Line is Hausdorff ($T_{2}$),"Can I have feedback on my proof, please? Prove the Michael line topology, $T_\mathbb{M}=\{U \cup F: U$ is open in $\mathbb{R}$ and $F\subset \mathbb{R}\setminus \mathbb{Q}\}$ is $T_{2}$ (Hausdorff). Let $a,b \in \mathbb{R}$ . WLOG, let $a<b$ . Let $x \in (a,b)$ . Then $a<x<b$ .
Clearly, $(-\infty , x), (x, \infty)$ are open disjoint subsets of $T_\mathbb{M}$ . Furthermore, $a \in (-\infty , x)$ and $b \in (x, \infty)$ , thus, $T_\mathbb{M}$ is $T_{2}$ or Hausdorff.",['general-topology']
2984629,Are fractional linear transformations continuous?,"I was reading, some answers about fractional linear transformations and find this old question that was never answer and I think is a nice question. How do you prove it? We define a Möbius transformation through: $$z\rightarrow \frac{az+b}{cz+d}, ad-bc\neq0, a,b,c,d\in \mathbb{C}$$ and extend to the Riemann sphere as follows: if $c=0$ , $T(\infty)=\infty$ , and if $c\neq0$ , $T(\infty)=a/c$ and $T(-d/c)=\infty$ .
Show that the Möbius transformations are continuous in that area with the chordal metric.
We define the chordal metric thus: $$d_{c}(z_{1},z_{2})=
\begin{cases}
\frac{2|z_{1}-z_{2}|}{\sqrt{1+|z_{1}|^2}\sqrt{1+|z_{2}|^2}}& \text{if $z_{1},\,z_{2}\neq \infty$}\\
\frac{2}{\sqrt{1+|z_{1}|^2}} & \text{if $z_{2}= \infty$.}
\end{cases}$$","['complex-analysis', 'mobius-transformation']"
2984636,10-gon as connected sum of tori or projective plane,"We're supposed to use cutting and gluing to find out, whether a given surface is a connected sum of tori or a connected sum of $\mathbb{R}P^2$ . Now, there's one surface I'm stuck with. It is a 10-gon with edges (a,b,c,d,e) and presentation: $abcdea^{-1}b^{-1}c^{-1}d^{-1}e^{-1}$ . I connect the crossed opposing pairs and end up with $fgf^{-1}g^{-1}e^{-1}hjh^{-1}j^{-1}e$ . But according to our book, this should not happen if all points are identified. Does anybody see what's going wrong here?","['general-topology', 'surfaces', 'geometry']"
2984642,Find the farthest rotation matrix in $\mathrm{SO}(3)$ from a given matrix.,"Consider the norm $\| A \| = \sqrt{\mathrm{tr}(AA^t)}$ . It's easy to see that $\mathrm{SO}(3)$ is a compact subspace of $3 \times 3$ matrices in the topology induced by this norm because $\mathrm{O}(3)$ is compact and $SO(3)$ being the inverse image of $\{1\}$ under the map $\mathrm{det}$ is a closed subset of $\mathrm{O}(n)$ . So, it makes sense to talk about the nearest rotation and the farthest rotation matrix from a given matrix. The former one, the nearest one, has been discussed online and I could find a lot of information about it by Googling. However, the farthest rotation matrix was not discussed. Out of curiosity, is it possible to find the farthest rotation matrix to a given matrix? I tried to solve the problem using Lagrange multipliers but I didn't know how to proceed because I'm not good at matrix calculus.","['matrices', 'matrix-calculus', 'linear-algebra', 'numerical-linear-algebra', 'optimization']"
2984651,Is this set in $\mathbb{R}^d$ closed?,"Let $X$ be a convex set and for each $ i \in \{1,2,\dots,d \} $ , let $f_{i}: X \rightarrow \mathbb{R} $ be convex functions where $ C_{i} := \{ f_{i}(x) : x \in X \} = [a_{i}, b_{i}]$ - closed interval in $\mathbb{R}$ . Does that mean the set $ C:= \{ \left(f_{1}(x), f_{2}(x),\dots, f_{d}(x)\right) : x \in X   \} \subseteq \mathbb{R}^{d} $ is closed?","['functions', 'convex-analysis', 'metric-spaces']"
2984656,For what $k\in \mathbb{C}$ do we have $\lim_{x\to\infty}\frac{1}{x^{k+1}}\sum_{n=1}^x \sigma_k(n)=\frac{\zeta(k+1)}{k+1}$?,"Can the following claim be extended for some complex $k$ . Perhaps: all $k$ with real part greater than or equal to $1$ ? Do the arguments below fall apart for complex $k$ for some reason? Claim. For any real $k\ge1$ $$\lim_{x\to\infty}\frac{1}{x^{k+1}}\sum_{n=1}^x \sigma_k(n)=\frac{\zeta(k+1)}{k+1}$$ Where $\sigma_k(n)=\sum_{d|n}d^k$ and $\zeta(s)=\sum_{n=1}^\infty{\frac{1}{n^s}}$ . The case for $k=1$ is shown here and the two lemmata below will extend the claim for $k>1$ . I really just copied the arguments from the linked post and added in this symbol $k$ . Lemma 1. For $k>1$ we have $$\sum_{n=1}^x \frac{\sigma_k(n)}{n^k}\in\zeta(k+1)x+O(1)$$ Aside I should comment that I have some feelings about big O notation. I think we should not obfuscate the 'is' of identity and the 'is' of predication. This has caused problems in the past... We should really say $f\in O(x^3)$ and not $f=O(x^3)$ . This opinion is not unique to me. And I don't want to start a discussion of notation here. But I am explaining why we see $\in$ and not the more commonly seen $=$ in my presentation of the lemma. It's because of my notation feelings. Proof $$\begin{align}
&\sum_{n=1}^x\frac{\sigma_k(n)}{n^k} \\
&=\sum_{n=1}^x \frac{1}{n^k}\sum_{d|n} (\frac{n}{d})^k \\
&=\sum_{n=1}^x \sum_{d|n} (\frac{1}{d})^k \\
&=\sum_{n=1}^x \sum_{A\le \frac{x}{d}} (\frac{1}{d})^k \\
&=\sum_{d=1}^x (\frac{1}{d})^k \sum_{A\le \frac{x}{d}}1  \\
&=\sum_{d=1}^x (\frac{1}{d})^k \bigg \lfloor \frac{x}{d} \bigg \rfloor \\
& \text{And since} \bigg \lfloor \frac{x}{d} \bigg \rfloor \in \frac{x}{d}+O(1) \text{we can substitute into the expression above to arrive at} \\
&\sum_{n=1}^x \frac{\sigma_k(n)}{n^k} \in \sum_{n=1}^x{\frac{1}{d^k}}{\bigg(\frac{x}{d}+O(1) \bigg)} \\
&\subseteq x\sum_{d=1}^x\frac{1}{d^{k+1}}+O\bigg( \sum_{d=1}^x\frac{1}{d^k} \bigg) \\
&\subseteq x\bigg(\zeta(k+1)+O\big(\frac{1}{x}\big) \bigg)+O(x^{1-k})\\
&\subseteq x\zeta(k+1)+O(1) 
\end{align}
$$ This is justified because for $k \geq 1$ we have $O(x^{1-k}) \subseteq O(1)$ . This concludes the first lemma. $\square$ Lemma 2. For $k>1$ , $$\sum_{n=1}^x\sigma_{k}(n) \in \frac{\zeta(k+1)}{k+1}x^{k+1}+O(x^k)$$ Proof We will use Abel's Summation: $$\sum_{n=1}^x a_nf(n)= A(x)f(x) +\int_1^x A(t)f'(t) dt $$ where $A(x)=\sum_{n=1}^x a_n $ . We will take $a_n=\frac{\sigma_k(n)}{n^k}$ and $f(x)=x^k \implies f'(x)=kx^{k-1}$ . Substituting we have $$\sum_{n=1}^x \sigma_k(n)= \sum_{n=1}^x \bigg(\frac{\sigma_k(n)}{n^k} \bigg){n^k}=x^k \sum_{n=1}^x \frac{\sigma_k(n)}{n^k}-k\int_1^x{t^{k-1}\sum_{n=1}^t{\frac{\sigma_k(n)}{n^k}}dt}$$ But in view of Lemma 1 we can see that this means $$\begin{align}
&\sum_{n=1}^x \sigma_k(n) \\
&\in x^k[\zeta(k+1)x+O(1)]-k\int_1^xt^{k-1}[\zeta(k+1)t+O(1)]dt\\
&\subseteq\zeta(k+1)x^{k+1}+O(1)-k\zeta(k+1) \int_1^x{t^k}dt+O\bigg(\int_1^x{t^{k-1}dt}\bigg) \\
&\subseteq \zeta(k+1)x^{k+1}+O(1)-k\zeta(k+1)\bigg[\frac{x^{k+1}-1}{k+1}\bigg]+O\bigg(\frac{x^k-1}{k}\bigg) \\
& \subseteq \zeta(k+1) \bigg[ \frac{(k+1)x^{k+1}-kx^{k+1}}{k+1} \bigg]+O(x^k) \\
& \subseteq \frac{\zeta(k+1}{k+1}x^{k+1}+O(x^k)
\end{align}
$$ This concludes the second lemma. $\square$ The claim above follows from these lemmata.","['asymptotics', 'analytic-number-theory', 'divisor-sum', 'sequences-and-series', 'riemann-zeta']"
2984658,How can we formalize Jorge Luis Borges' Aleph?,"Background. Jorge Luis Borges was a post-modern short-story writer of the 20th century, whose stories often invoke a healthy dose of surrealism. One of his works is called The Aleph . In this book, there exists a point in space, aptly called the Aleph, that contains all other points. Anyone who gazes into it can see everything in the universe from every angle simultaneously, without distortion, overlapping, or confusion. I quote: All language is a set of symbols whose use among its speakers assumes a shared past. How, then, can I translate into words the limitless Aleph, which my floundering mind can scarcely encompass? Mystics, faced with the same problem, fall back on symbols: to signify the godhead, one Persian speaks of a bird that somehow is all birds; Alanus de Insulis, of a sphere whose center is everywhere and circumference is nowhere; Ezekiel, of a four-faced angel who at one and the same time moves east and west, north and south. (Not in vain do I recall these inconceivable analogies; they bear some relation to the Aleph.) Perhaps the gods might grant me a similar metaphor, but then this account would become contaminated by literature, by fiction. Really, what I want to do is impossible, for any listing of an endless series is doomed to be infinitesimal. In that single gigantic instant I saw millions of acts both delightful and awful; not one of them occupied the same point in space, without overlapping or 
  transparency. What my eyes beheld was simultaneous, but what I shall now write down will be successive, because language is successive. Nonetheless, I’ll try to recollect what I can. On the back part of the step, toward the right, I saw a small iridescent sphere of almost unbearable brilliance. At first I thought it was revolving; then I realised that this movement was an illusion created by the dizzying world it bounded. The Aleph’s diameter was probably little more than an inch, but all space was there, actual and undiminished. Each thing (a mirror’s face, let us say) was infinite things, since I distinctly saw it from every angle of the universe. I saw the teeming sea; I saw daybreak and nightfall; I saw the multitudes of America; I saw
  a silvery cobweb in the center of a black pyramid; (...) I saw the coupling of love and the modification of death; I saw the Aleph from
  every point and angle, and in the Aleph I saw the earth and in the earth the Aleph and in the Aleph the earth; I saw my own face and my own bowels; I saw your face; and I felt dizzy and wept, for my eyes had seen that secret and conjectured object whose name is common to all men but which no man has looked upon --- the unimaginable universe. I felt infinite wonder, infinite pity. Question. How can we put the concept of The Aleph on a solid mathematical footing? What I tried. To simplify discussion, let's forget about time, and start with a Riemannian $3$ -manifold (whether it's compact I'll leave to the cosmologist) $M$ , which serves as a model for our universe. Let us model The Aleph as an additional point $\aleph$ . A priori it is not connected to the universe; however, we may endow $M \cup \{\aleph\}$ with a topology extending the one on $M$ that has, as its only neighbourhood of $\aleph$ , the entire space. Topologically speaking, this implies that $\aleph$ is 'near every other point'. Upon doing this, however, our space is no longer Riemannian, and as such, much of the relevant notions, such as geodesics, start breaking down. So I'm not sure if this could get us any further. This also does not take into account relativity in any serious way. I am vaguely aware that general relativity views the universe as a Lorentzian $4$ -manifold such that light curves travel along suitable null-geodesics, but I am otherwise unfamiliar with the details; thus to prevent me from saying anything stupid I'll leave it at this.","['geometry', 'recreational-mathematics', 'differential-topology', 'soft-question', 'differential-geometry']"
2984673,Convergence rate of $a_{n+1}=\ln(a_n+1)$,"Let $a_0=1$ and $a_{n+1}=\ln(a_n+1)$ . The goal is to find $$
(\star)\quad \lim_{n\to \infty}\frac{n(na_n-2)}{\ln n}
$$ It is easy to see that $a_n\to 0$ . 
In addition, we can prove that $$
\lim_{n\to \infty}na_n=2.
$$ Indeed, one can obtain by  Stolz's theorem that \begin{align} \lim_{n\to\infty}\frac{n}{\frac{1}{a_n}} &= \lim_{n\to\infty}\frac{(n+1)-n}{\frac{1}{a_{n+1}}-\frac{1}{a_n}} \\ &=\lim_{n\to\infty} \frac{1}{\frac{1}{\ln(1+a_n)}-\frac{1}{a_n}} \\ &= \lim_{n\to\infty}\frac{a_n\ln(1+a_n)}{a_n-\ln(1+a_n)}  \\ &=2 , \end{align} How to estimate the further limit $(\star)$ ?","['limits', 'calculus', 'real-analysis']"
2984677,Separation of closed sets with distance $>0$ by function $f \in C_b^k(\mathbb{R}^d)$,"I'm interested in the following problem on the separation of closed sets: Let $A,B \subseteq \mathbb{R}^d$ be closed sets such that $$d(A,B) := \inf\{|x-y|; x \in A, y \in B\}>0.$$ Question: Does there exist a function $f \in C_b^k(\mathbb{R}^d)$ such that $$f^{-1}(\{0\}) = A \quad \text{and} \quad f^{-1}(\{1\})=B \tag{1}$$ ...? Here, $C_b^k(\mathbb{R}^d)$ denotes the space of functions $f: \mathbb{R}^d \to \mathbb{R}$ with bounded derivatives up to order $k$ , and $k \in \mathbb{N}$ is some fixed number. It is a classical result that there exists a continuous function $f$ satisfying $(1)$ . With a bit more effort it can be shown that there exists a smooth function $f \in C^{\infty}(\mathbb{R}^d)$ which satisfies $(1)$ ; this is e.g. discussed in this question . I strongly believe that the answer to my question is ""yes"" but I couldn't find any results concerning the boundedness of the derivates of Urysohn function $f$ . The condition $d(A,B)>0$ means intuitively that the function $f$ does not need to be arbitrarily steep, and hence it would be naturally that $f$ can be chosen in such a way that its derivates are bounded.  However, it is not obvious for me how to prove this rigorously; the construction discussed in the question, which I linked above, does not seem to be helpful. I would be happy about references and/or your thoughts on the question.","['general-topology', 'real-analysis']"
2984760,Is a set in between two sets of equal measure measurable?,"The Lebesgue sigma algebra is complete with respect to Lebesgue measure, which means that if $A$ is a Lebesgue measurable set with Lebesgue measure $0$ and $B$ is a subset of $A$ , then $B$ is Lebesgue measurable as well.  But I'd like to know if something stronger is true. Suppose that $A$ is a subset of $B$ which is a subset of $C$ , where $A$ and $C$ are Lebesgue measurable sets and the Lebesgue measure of $A$ is equal to the Lebesgue measure of $C$ .  Then my question is, does $B$ have to be Lebesgue measurable as well?","['measure-theory', 'lebesgue-measure']"
2984818,What are the conditions on $f$ for $\sup f$ and $f(\sup)$ be interchangeable?,"Let $f:\mathbb{R}^n \to \mathbb{R}$ . Say that $f$ is magic if for every nonempty set $X$ and every functions $g_1,\dots,g_n : X \to \mathbb{R}$ the following holds: $$\sup_{(x_1,\dots,x_n) \in X^n} f(g_1(x_1),\dots,g_n(x_n)) = f \left(\sup_{x \in X} g_1(x),\dots,\sup_{x \in X} g_n(x) \right)$$ I am looking for a characterization of magicness . In other words, I am looking for necessary and sufficient conditions for magicness , a theorem that looks like: $f$ is magic if and only if [some conditions here] Intuitively, it looks like being a linear operator with non-negative coefficients* is sufficient, but (1) I could be wrong and (2) it seems too strong to be a necessary condition anyway. * By linear operator with non-negative coefficients I mean that $f$ has the form $f(x_1, \dots, x_n) = a_1 x_1 + \dots + a_n x_n$ where every $a_i$ is non-negative.","['supremum-and-infimum', 'real-analysis']"
2984918,On existence of positive integer solution of $\binom{x+y}{2}=ax+by$,"How can I prove this? Prove that for any two positive integers $a,b$ there are two positive integers $x,y$ satisfying the following equation: $$\binom{x+y}{2}=ax+by$$ My idea was that $\binom{x+y}{2}=\dfrac{x+2y-1}{2}+\dfrac{y(y-1)}{2}$ and choose $x,y$ , such that $2a=x+2y-1, 2b=y(y-1)$ , but using this idea, $x,y$ won’t be always positive.","['contest-math', 'number-theory', 'elementary-number-theory', 'binomial-coefficients', 'combinatorics']"
2984952,"$f:[0,1] \to \mathbb R$ be a differentiable function with bounded derivative, then is $\int_0^1 f'(x)=f(1)-f(0)$?","Let $f:[0,1] \to \mathbb R$ be a differentiable function such that $\sup_{x\in [0,1]} |f'(x)|$ is finite. Then since $f'(x)=\lim_{n\to \infty} \dfrac{f(x+1/n)-f(x)}{1/n}$ , so $f'(x)$ is measurable and also the Lebesgue integral $\int_0^1|f'(x)|dx$ is finite, thus $f' \in L^1([0,1])$ . My question is, is it true that $\int_0^1 f'(x)=f(1)-f(0)$ ? Note that fundamental theorem of calculus does not apply here since $f'(x)$ is not continuous (not even known to be Riemann integrable)","['lebesgue-measure', 'lebesgue-integral', 'real-analysis', 'measurable-functions', 'derivatives']"
2984967,$\sum a_{n}$ converges but $\sum a_{n}^2 $ diverges?,I have to give an example of a convergent series $\sum a_{n}$ for which $\sum a_{n}^2 $ diverges. I think that such a series cannot exist because if $\sum a_{n}$ converges absolutely then $\sum a_{n}^2 $ will always converge right?,"['limits', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
2984981,Uncountable family of measurable functions and limits,"Let $(f_t)_{t \in \mathbb{R}}$ be a family of measurable functions on a measurable set $E$ . Suppose that $\lim_{t \to 0} f_t(x) $ exists for all $x \in E$ . Define $f(x) = \lim_{t \to 0} f_t(x)$ . Is f a measurable function? My immediate response is to say “yes”. However, I have only seen results on taking limits $n \to \infty$ of countable sequences of measurable functions. It is not clear to me if the same results should stand here (e.g. that the limit of a sequence of measurable functions is also measurable), since I know uncountability has a bad habit of breaking things! If the result is true, how can I extend the normal results about sequences of measurable functions to this case? If not, how do I find a counter example?","['measure-theory', 'lebesgue-measure', 'real-analysis', 'measurable-functions', 'general-topology']"
2984982,Expectation of sample precision matrix,"Let $x_{i} \sim N(0, \Sigma_{d \times d})$ be $n$ i.i.d. Gaussian vectors, and let $X$ be an $n \times d$ matrix, whose $i$ -th row is equal to $x_{i}$ .
It is then known that $$
\mathbb{E}[(X^{\mathsf{T}}X)^{-1}] = \frac{1}{n - d - 1}\Sigma^{-1}
$$ since $(X^{T}X)^{-1}$ follows an Inverse-Wishart distribution. Now, if we replace the Gaussian distribution, with any other distribution and the same covariance matrix, we would still get $$
\mathbb{E}[X^{T}X] = n\Sigma
$$ and so $$
\mathbb{E}[(X^{T}X)^{-1}] \geq \frac{1}{n} \Sigma^{-1}.
$$ My question is, in what cases can $\mathbb{E}[(X^{\mathsf{T}}X)^{-1}]$ be computed and does it have to be proportional to $\Sigma^{-1}$ ?
If it cannot be computed, what are known ways to upper bound it?
Any references even tangentially related would be much appreciated.","['random-matrices', 'statistics', 'reference-request']"
2984984,Find the probability of forming a triangle by choosing random points from a line $AB$,"Q: Two points are chosen at random on a line $AB$ , each point being chosen according to the uniform distribution on $AB$ , and the choices being made independently of each other. The line $AB$ may now be regarded as divided into three parts. What is the probability that they may be made into a triangle. A: Let $x,y\in(A,B)$ denote such two points, as shown below, then to construct a triangle we must have that $$\begin{cases}
\overline{Ax}+\overline{yx}\geq \overline{By}\\
\overline{Ax}+\overline{By}\geq \overline{xy}\\
\overline{By}+\overline{yx}\geq \overline{Ax}
\end{cases}\implies y\geq\frac{A+B}{2}\quad\wedge\quad x<y.$$ Let $\,T\,$ denote the event of forming a triangle, $\,L1\,$ the event that $\,y\geq\frac{A+B}{2}\,$ and $\,L2\,$ the event that $x<y\implies x<\frac{A+B}{2}$ , then $$P(T)=P(L1)P(L2).$$ But $$P(L1)=\int_{\frac{A+B}{2}}^B \frac{1}{B-A} \, dx=\frac{1}{2}$$ and $$P(L2)=1-P(L1)=1-\frac{1}{2}=\frac{1}{2}$$ so $P(T)=1/4.$ Would this result and reasoning be correct?",['probability-theory']
2985008,Is the Norm of the $L^{2}$ Function Equal to the Given Limit?,"Let $g$ be an $L^{2}$ function on $[0,2]$ , with respect to the Lebesgue measure $m$ . Is it true that $$||g||_{1}=\lim_{p\to 1}\left(\int |g|^{p}~dm\right)^{1/p}?$$ I'm really not sure how to tackle this problem. I'm. assuming that the integral is taken over $[0,2]$ , but it wasn't explicitly given in the statement. I know that $\left(\int_{0}^{2}|g(x)|^{2}~dx\right)^{1/2}<\infty$ since $g\in L^{1}([0,2])$ , so my intuition is that the questioned equality does hold. However, I'm not sure if this is correct, and if it is, I'm not sure how to prove it. Any help is appreciated.","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis', 'lp-spaces']"
2985065,When does an algebraic differential equation have algebraic solutions?,"Suppose $f(x,y)$ is a rational function. Typically the solution curves of the differential equation $$
\frac{dy}{dx}=f(x,y)
$$ are not algebraic (for example, if $f(x,y)=y$ , the solutions are the non-algebraic curves $y=ce^x$ ). There are certain $f$ , however, for which all solutions are algebraic (for example, if $f$ is a rational function in just $x$ , having $0$ residue at every pole). Given $f$ , is there a way to check whether all the solutions to $\frac{dy}{dx}=f(x,y)$ are algebraic?","['algebraic-geometry', 'ordinary-differential-equations']"
2985115,Joint PDF of min and max of iid uniform distributions,"$U_1$ and $U_2$ are identically independently distributed ~uni[0,1].  I'm trying to find the joint PDF between the $P_1$ =min( $U_1$ , $U_2$ ) and $P_2$ =max( $U_1$ , $U_2$ ).  I have found the marginal pdfs: $f_{p1}(y)=2y^2-4y+2$ $f_{p2}(x)=2x$ Since the min and max are not independent, I think the best course of action would be to find the conditional probability by conditioning on $U_1$ and then multiplying by the pdf of $U_1$ . $$\int_0^1 P(min(U_1, U_2)<p_{1} ,P(max(U_1, U_2)<p_{2}\ \ |\ \  U_1=u)\ f_u(u)  \;\mathrm{dx}$$ However, I am stuck now.","['statistics', 'probability-distributions', 'probability']"
2985178,How do calculators calculate the value of trigonometric functions? [duplicate],"This question already has answers here : How does a calculator calculate the sine, cosine, tangent using just a number? (3 answers) Closed 5 years ago . I have an obsession about trig functions. I find them to be so mysterious, because I really don’t understand calculus (other then trying to make sense of it in a few YouTube videos, which doesn’t work for me). Anyhow I was wondering if someone can explain how calculators find the ratios for sine and cosine by just typing in the degrees. This seems impossible to me from a logical standpoint. I don’t see any pattern other then as the angle increase the ratio for cosine gets exponentially smaller. If anyone can tell me how you get an exact number for something that has no pattern, please explain it to me in the simplest possible way. Pretend you're explaining it to your 10-year-old child.  I am very visual so anything with pictures would work. If you start getting into Taylor series with calculus, I won’t understand. I’m basically looking for a simple explanation of what the calculator is doing without getting deep into the math.  Thanks in advance. Cheers","['calculus', 'trigonometry']"
2985212,"If $A$ is a square matrix that satisfies $A^2-A+2I=0$, show that $A+I$ is invertible","If $A$ is a square matrix that satisfies $A^2-A+2I=0$ , show that $A+I$ is invertible. I understand how to find if $A$ is invertible but I don't know how to solve for the $A+I$ version.",['linear-algebra']
2985217,Show that there are exactly $\aleph_0$-many countable $L$-structure if $L$ consists of one unary relation symbol.,"$\textbf{First question:}$ Let $L=\{R\}$ be a language consisting of one unary relation symbol. Show that there are exactly $\aleph_0$ -many countable $L$ -structures up to isomorphism. My (attempted) solution is as follows: Let $\mathcal{M}=(M,R^{\mathcal{M}})$ be an $L$ -structure. Since $R$ is a unary relation symbol, the basic relation $R^{\mathcal{M}}$ is also unary. But a unary relation is just a subset of $M$ . Any countable $L$ -structure is isomorphic to $\omega$ or some $n \in \omega$ . So the set of all countable $L$ -structures up to isomorphism is $$\{(n,E):n \in \omega,E \subseteq n\} \cup \{(\omega,F):F \subseteq \omega\}$$ We first look at the set one the left hand side. It is of cardinality $\aleph_0$ . If I can show the set on the right hand side is also of cardinality $\aleph_0$ then we are done. But how? $|\mathcal{P}(\omega)|=\aleph_1$ hence the cardinality of the set on the right is $\aleph_1$ . $\textbf{What did I do wrong?}$ $\textbf{Second question:}$ Let $L=\{R\}$ be a language consisting of one unary relation symbol. How many $L$ -structure of size $\aleph_1$ are there? I was trying to use the similar argument as in my first question, but in the first question, we consider countable structures and we know every countable set is isomorphic to $\omega$ or some $n \in \omega$ . But if we consider structures of cardinality $\aleph_1$ , I don't know the particular set they are isomorphic to, so I can't use the argument as before. What should I do?","['elementary-set-theory', 'model-theory', 'logic']"
2985228,Prove the pullback bundle is a vector bundle,"I'm stuck at proving that the pullback bundle is a vector bundle.
The question is basically we have a smooth function $f$ from the smooth manifold $N$ to the smooth manifold $M$ . And $(E, \pi, M)$ is a vector bundle. Now we want to prove that $f^{*}E = \{(e,n) \in E \times N \mid \pi(e) = f(n)\}$ is the total space over $N$ .
This is what I tried: Define $$\rho: f^{*}E \to N: (e,n) \mapsto n$$ . First notice that $\rho^{-1}(\{n\}) = (f^{*}E)_{f(n)}$ consists of all pairs $(e,n)$ such that $\pi(e) =f(n)$ , which is  the fiber of $f(n)$ attached to point $n$ , which is again simply identified with $E_{f(n)}$ , which is a $k$ -dimensional vector space (so of course $\rho$ is surjective), and the fibers of $N$ under $\rho$ are the $k$ -dimensional vector spaces that are 'attached' to $M$ .   Now take $p \in N$ . There exist an open neighbourhood $U$ around $f(p) \in M$ such that there exists an diffeomorphism $\Phi: U \times R^k \to pi^{-1}(U)$ , such that $\pi \circ \Phi = \pi_1$ and that $\Phi: \{f(p)\} \times R^k \to E_{f(p)}$ is a vector space isomorphism. Now let's take a look at $V = f^{-1}(U)$ , which is an open neighbourhood of $p$ , since $f$ is smooth. Note that $\rho^{-1}(V) = \{ (e,n) \in f^{*}E \mid \rho((e,n)) = n \in V \}$ . Now I don't actually know what to do to prove the local trivialization. I think I'm overseeing a simple step.","['vector-bundles', 'smooth-manifolds', 'differential-geometry']"
2985231,If for each $x \in \mathbb R$ there exists $n$ such that $f^{(m)}(x)=0$ for all $m \ge n$ then prove that $f$ must be a polynomial,"Let $f:\mathbb R \to \mathbb R$ be an infinitely differentiable function such that for every $x \in \mathbb R$ there exists $n$ such that $f^{(m)}(x)=0$ for all $m \ge n$ . I need to prove that $f$ is a polynomial. It is trivial to show using Baire's theorem that $f$ must be a polynomial in some open ball, but since $f$ is not necessarily analytic I cannot apply the identity principle to obtain the desired result. I would appreciate a hint.","['calculus', 'derivatives', 'real-analysis']"
2985247,Limit of the sequence $1+\frac{\cos n}{n!}$,"$$\lim_{n\to\infty}1+\frac{\cos n}{n!}$$ How would I find the limit of this using the squeeze theorem? I have that: \begin{align*}-1\le{ }&\cos {n} \le 1\\
0\le { }&1+\cos {n}\le 2\\
\frac{0}{n!}\le { }& \frac{1+\cos {n}}{n!} \le \frac{2}{n!}
\end{align*} but $\frac{1+\cos {n}}{n!} \neq 1+ \frac{\cos {n}}{n!}$ How do I go around this?","['limits', 'calculus', 'real-analysis']"
2985359,Gamma Distribution Moments,"Problem. Show that for $X \sim \text{Gamma}(\alpha, \beta)$ , for positive constant $\nu$ , $$ E[X^\nu] = \frac{\beta^\nu \Gamma(\nu + \alpha)}{\Gamma(\alpha)} . $$ I have the following solution: However, I don't understand how we get that $$ \frac{1}{\Gamma(\alpha)\beta^\alpha} \int_{0}^{\infty}x^{(\nu+\alpha)-1}e^{-x/\beta} \, \mathrm{d}x
= \frac{\Gamma(\nu+\alpha)\beta^{\nu+\alpha}}{\Gamma(\alpha)\beta^{\alpha}} $$ Would appreciate any help on how this step was completed. Basically, I don't understand how: $$ \int_{0}^{\infty} x^{(\nu+\alpha)-1}e^{-x/\beta} \, \mathrm{d}x = \Gamma(\nu+\alpha)\beta^{\nu+\alpha}. $$ I see that the left side is close to the definition of the Gamma function, but can't see how exactly to turn it into the right side.","['statistical-inference', 'statistics', 'probability-distributions', 'gamma-distribution']"
2985371,Why the function $f: \emptyset \rightarrow \emptyset$ exist? $0^0 = 1$ [duplicate],"This question already has answers here : Why is an empty function considered a function? (3 answers) Closed 2 years ago . If $f: \emptyset \rightarrow \emptyset$ , that is: $\forall a \in \emptyset \Rightarrow \exists b\in \emptyset, (a,b)\in \emptyset \times \emptyset$ . But how we can say, that for non-existing element exist another non-existing element? $\forall (a_1,b_1),(a_2,b_2) \in {\emptyset}^2$ , either $a_1 \neq a_2$ or $b_1 \neq b_2$ . But if element non-existing, we can say, that $\neg(a_1 \neq a_2$ or $b_1 \neq b_2)$ is true too, no? And this is saying us, that this is $f$ is a function and not a function at one moment, no? The subset of $\emptyset \subset \emptyset \times \emptyset $ exist and the $G_f = \emptyset$ So if this function exist $\Rightarrow$ that this function is only one, becouse $G_f = \emptyset$ and $\emptyset$ is only one in the Set Theory, yes? And because in power set of $\emptyset$ exist only one element -- $\{\emptyset\} = 2^0$ . So, after this we can say, that $0^0 = 1$ , yes? Like that for all numbers in $\mathbb{R}$ will be true, that $r^0 = 1$ , because for all elements of the family $\{X_{\alpha}\}$ of the sets with $r$ -power : $\forall X\in  \{X_{\alpha}\} \Rightarrow |X| = r$ exist only one function $f_r: \emptyset \rightarrow X$ . But why this function only one? How can exist the element in $\emptyset \times X$ ? And why the function $f_{\emptyset} : X\rightarrow \emptyset$ non-existing? The subset of $G_{f_r}\subset \emptyset\times X$ exist but the subset of $G_{f_{\emptyset}}\subset X\times \emptyset$ non-exist? I know, that $0^r$ always $= 0$ but cannot understand it in this situation",['elementary-set-theory']
2985373,Find $y^{(n)}$ if $y=x^2e^x \cos(x)$,"Q :Find $y_n$ if $$y=x^2e^x \cos(x)$$ My Approach : By product rule, $$y'=2xe^x \cos(x)+x^2e^x \cos(x)+x^2e^x \cos(\frac{\pi}{2}+x)$$ $$y''=2e^x \cos(x)+4xe^x \cos(\frac{\pi}{2}+x)+4xe^x \cos(x)+2x^2e^x \cos(\frac{3\pi}{2}-x)$$ If i go in this process than i think i couldn't reach any conclusion because i didn't find any pattern.Any hint or solution will be appreciated. Thanks in advance.","['calculus', 'derivatives']"
2985393,prove that $\int_{a}^b f \ + \int_{f(a)}^{f(b)} \ f^{-1}=bf(b)-af(a)$ [duplicate],"This question already has answers here : Show rigorously that the sum of integrals of $f$ and of its inverse is $bf(b)-af(a)$ (6 answers) Closed 5 years ago . Let $0 <a <b\ $ let $f >0 $ be continuous and strictly increasing  function on $ [a,b]$ . Prove that $$\int_{a}^b f \  + \int_{f(a)}^{f(b)} \ f^{-1}=bf(b)-af(a)$$ How to approach this problem . Any Hint? I am suppose to do it without using antiderivatives.","['integration', 'calculus', 'analysis', 'real-analysis']"
2985411,Is $\cot(\cot(\cot(\cdots\cot1)\cdots))$ always defined?,"Consider sequence $a_1=1$ , $a_{n+1}=\cot a_n$ . Is $a_n$ always defined? Numerical evaluation suggests this conjecture is true. I have proved a weak version of this question: for a fixed $n$ and $a_1=x$ , the measure of $x$ such that $a_n$ is not defined is 0. Proof There is a bijection between $\{x|a_n\text{ is not defined}\}$ and $\mathbb{N}^{n-1}$ , the set is countable, hence the measure is 0. Does anyone have some idea on the original question?",['sequences-and-series']
2985431,A Cauchy sequence $\{x_n\}$ with infinitely many $n$ such that $x_n = c$.,"Is the following argument correct? Proposition . If $\{x_n\}$ is Cauchy sequence such that $x_n = c$ for infinitely many $n$ , then $\lim_{n\to\infty}x_n = c$ . Proof. Let $\epsilon>0$ . Since $\{x_n\}$ is a Cauchy sequence, there exists an $M\in\mathbb{N}$ such that $\forall \, j,k\ge M$ , we have $|x_j-x_k|<\epsilon$ . Now, since $x_n = c$ for infinitely many $c$ , then surely $x_r = c$ for some $r \ge M,$ implying that $|x_j-c|<\epsilon \,\,\forall j\ge M$ , completing the argument. $\blacksquare$","['cauchy-sequences', 'proof-verification', 'real-analysis', 'sequences-and-series', 'limits']"
2985471,Simply connected surfaces and torsion in Grothendieck group,Let $X$ be a projective complex surface (complex manifold of dimension 2). In a paper I met the following claim: if $X$ is simply connected $\pi_1(X) \cong \{1\}$ then the torsion of the Grothendieck group vanishes $K_0(X)_{tors} \cong 0$ . How one can prove this claim? If this is well-known what is a reference?,"['complex-geometry', 'algebraic-geometry', 'reference-request']"
2985482,A martingale bounded from below is $L^1$ bounded,"Let $(X_n,\mathcal F_n)$ be a martingale bounded from below i.e. $X_n\geq M$ for some $M\in\mathbb R$ . Then show that $\sup_n E|X_n|<\infty$ . It is easy to observe that $X_n$ converges almost surely to some $X\in L^1$ as $X_n-M$ is a non-negative martingale. I can conclude $X_n$ is $L^1-$ bounded if I can show $X_n$ is uniformly integrable. But I don't know how to prove it.","['martingales', 'measure-theory', 'probability-theory']"
2985507,Solving $\sin x = \sin y$ by using the prosthaphaeresis formulas,"I'd like to solve the equation $\sin x = \sin y$ by using the prosthaphaeresis formulas: $$ \sin(x)-\sin(y)=0 $$ $$ 2 \cos \left( \frac{x+y}{2} \right) \, \sin \left( \frac{x-y}{2} \right) = 0 $$ There are two possibilities $\cos \left( \frac{x+y}{2} \right)=0$ or $\sin \left( \frac{x-y}{2} \right)=0$ . In the first case: $$ \frac{x+y}{2} = \frac{\pi}{2} + k \, \pi$$ In the second case: $$ \frac{x-y}{2} = k \, \pi $$ The above two equations give the result: $$ x = \frac{\pi}{2} + 2 \, k \, \pi $$ $$ y = \frac{\pi}{2} $$ It seems that the solution is wrong; can you show me the right solution and the way to get it (by using the prosthaphaeresis formulas if possible) please? Thank you for your willingness.","['trigonometry', 'education']"
2985525,Variance of the sum of elements of a Wishart distributed matrix,"Looking for the variance of $S=\sigma _{1,3}-\sigma _{1,4}-\sigma _{2,3}+\sigma _{2,4}$ , where $\sigma_{i,j}$ are Wishart-distributed elements of the random matrix $$\Sigma =\left(
\begin{array}{cccc}
 \sigma _1^2 & \sigma _{1,2} & \sigma _{1,3} & \sigma _{1,4} \\
 \sigma _{1,2} & \sigma _2^2 & \sigma _{2,3} & \sigma _{2,4} \\
 \sigma _{1,3} & \sigma _{2,3} & \sigma _3^2 & \sigma _{3,4} \\
 \sigma _{1,4} & \sigma _{2,4} & \sigma _{3,4} & \sigma _4^2 \\
\end{array}
\right)$$ the $m$ -sample estimation of the covariance matrix of 4 multivariate Gaussian distributed random variables with $n$ observations each. Answer: $$\left(\left(\sigma _{1,3}-\sigma _{1,4}-\sigma _{2,3}+\sigma _{2,4}\right)^2+\left(-2 \sigma _{1,2}+\sigma _1^2+\sigma _2^2\right) \left(-2 \sigma _{3,4}+\sigma _3^2+\sigma _4^2\right)\right)$$","['statistics', 'probability-distributions']"
2985526,$P$ is a point on the angular bisector of $\angle A$. Show that $\frac{1}{AB}+\frac{1}{AC}$ doesn't depend on the line through $P$,"The point $P$ is on the angular bisector of a given angle $\angle A$ . A line $L$ is drawn through $P$ which intersects with the legs of the angle in $B$ and $C$ . Show that $$\dfrac{1}{AB} + \dfrac{1}{AC}$$ is not dependent on the choice of the line $L$ . I started by drawing it up and came to the conclusion that $AC$ + $AB$ should always be the same because of the angles where $\triangle$ $APB$ and $\triangle$ $APC$ are. So I wrote $\angle APB$ = $\beta$ and $\angle APC$ = $\theta$ Then I wrote $L*\sin\beta$ = $AB$ and $L*\sin\theta$ = $AC$ . Then I added them so: $L(\sin(\alpha + \theta)) = AB + AC$ . But $\sin (\alpha + \theta)$ should be $0$ since $\alpha + \theta$ is $180 ^{\circ}$ , and $AB + AC$ is not $0$","['euclidean-geometry', 'triangles', 'angle', 'geometry']"
2985528,"Let $X,Y $ be two independent random variables with exponential distribution and parameter $\lambda > 0$.","Let $X,Y$ be two independent random variables with exponential distribution with parameter $\lambda > 0$ . Let $S = X + Y$ and $T = \frac{X}{S}$ . I want to find the joint density function of $(S,T)$ . I want then to calculate the marginals and say whether or not $S$ and $T$ are independent. 
I start  by finding the density function of $S$ using the convolution: $$ f_S(s) = \int_{-\infty}^{+\infty}f_X(s-t)f_Y(t)dt $$ $$ = \int_{0}^{s} \lambda^2 e^{-\lambda s} dt  = \lambda^2 e^{-\lambda s}$$ Then I tried to calculate the density function of $T $ but I am stuck here: $$ F_T(t) = \mathbb{P}(T \leq t) = \mathbb{P}\left(\frac{X}{X+Y} \leq t\right).$$ Is this the right method of solving this? Should I find the joint density first? (The problem is that I do not know how to to that)","['probability-distributions', 'density-function', 'probability', 'random-variables']"
2985550,Use property to show that if $f_n \rightarrow f$ in measure on finite measure set $E$ then $f_n^2 \rightarrow f^2$ in measure on $E$.,"Consider the following problem: Let $E$ have finite measure, $\{f_n\} \rightarrow f$ in measure on $E$ and $g$ be a measurable function on $E$ that is finite almost everywhere. Use the fact that $\{f_n \cdot g\} \rightarrow f_n \cdot g$ to prove that $\{f_n^2 \} \rightarrow f^2$ in measure on $E$ . Infer from this that if $\{g_n\} \rightarrow g$ in measure on $E$ then $\{f_n \cdot g_n \} \rightarrow f \cdot g$ in measure on $E$ . I'm struggling understand how I can apply the fact that $\{f_n \cdot g\} \rightarrow f_n \cdot g$ in measure to prove that $\{f_n^2 \} \rightarrow f^2$ in measure. I've been able to come up with a proof of the fact that if $\{g_n\} \rightarrow g$ in measure then $\{f_n \cdot g_n \} \rightarrow f \cdot g$ in measure, and this clearly implies the claim that $\{f_n^2 \} \rightarrow f^2$ in measure. Unfortunately my proof doesn't rely on the fact that $\{f_n \cdot g\} \rightarrow f_n \cdot g$ so it doesn't answer the problem since it explicitly asks us to prove it a certain way. So far my best attempt to utilize the fact is as follows. Let $\eta > 0$ then by triangle inequality and monotonicity of measure \begin{align*}
    \lim_{n\rightarrow \infty}\mu\left\{ \left|f_n^2 - f^2\right| > \eta \right\} &\leq \lim_{n\rightarrow \infty}\mu \left\{\left|f_n - f\right|\cdot \left|f_n\right| + \left|f_n - f\right| \cdot \left|f\right| > \eta \right\} \\
    &\leq \lim_{n\rightarrow \infty}\mu\left\{\left|f_n - f\right|\cdot \left|f_n\right| > \eta/2 \right\} + \lim_{n\rightarrow \infty}\mu\left\{\left|f_n - f\right| \cdot \left|f\right| > \eta/2 \right\}
\end{align*} By the fact that $\{f_n \cdot f\} \rightarrow f \cdot f$ in measure \begin{equation*}
\lim_{n\rightarrow \infty}\mu\left\{\left|f_n - f\right| \cdot \left|f\right| > \eta/2 \right\} = \mu\left\{\left|f_n \cdot f - f\cdot f\right| > \eta/2 \right\} = 0.
\end{equation*} I don't see how to deal with the other limit without first proving the most general statement $\{f_n \cdot g_n \} \rightarrow f \cdot g$ in measure. Any help or advice is greatly appreciated.","['measure-theory', 'convergence-divergence']"
2985553,If $a=b+c$ prove that $S=a^4+b^4+c^4$ is twice the square of a positive integer,"If $a=b+c$ , and $a$ , $b$ , $c\in \Bbb N$ , prove that $S=a^4+b^4+c^4$ is twice the square of a positive integer. Source: a list of problems used in the preparation to math contests. My attempt: By making the substitution $a=b+c$ in $S$ and developing $(b+c)^4$ , it is easy to show that $$S=2(b^4+c^4+bc(2b^2+3bc+2c^2))$$ an expression with the form $S=2K$ . The problem now is how to prove that K is a square of a positive integer. I also tried to use Newton Identities but with no luck ( Note : later, after a hint, I found a way to solve using this approach, see below). Hints and answers are welcomed. Sorry if this is a dup.","['contest-math', 'algebra-precalculus']"
2985658,Beta reduction for expression,"I'm given the following where: TRUE = λxy.x
FALSE = λxy.y

IF = λbtf. b t f
OR = λxy. IF x TRUE y and I'm trying to evaluate: OR FALSE TRUE using Beta reduction. Since beta reduction is left associative, i started from the left but I'm having difficulties with the OR where there contains IF and TRUE inside it's expression. So i tried: OR FALSE TRUE
(λxy. IF x TRUE y) (λxy.y) (λxy.x)
(λxy. (λbtf. b t f) x (λxy.x) y) (λxy.y) (λxy.x) #i substituted btf with x here but I'm not fully sure if it's valid
(λxy. x (λxy.x) y) (λxy.y) (λxy.x)
(λxy. x x) (λxy.y) (λxy.x)
(λxy.y) (λxy.y) (λxy.x)
(y) (λxy.x) But I'm pretty sure I messed up somewhere along way as evaluating the OR was confusing enough. I'm not certain if I beta reduced correctly so I would appreciate some help on this. I gave it another go so here's what I did: OR FALSE TRUE
(λxy. IF x TRUE y) FALSE TRUE
IF FALSE TRUE FALSE TRUE
(λbtf. btf) FALSE TRUE FALSE TRUE
FALSE TRUE FALSE TRUE
(λxy.y) TRUE FALSE TRUE
(λx.TRUE) FALSE TRUE
TRUE TRUE
(λxy.x) TRUE
λy.TRUE Again I'm still uncertain if it's correct this way too. Some help needed.","['lambda-calculus', 'functions', 'computer-science']"
2985709,What kind of flow minimizes resistance?,"Consider a domain $D$ , where $\sigma(x)$ is the spatially dependent ""conductivity"". On the boundary we have $2$ ""electrodes"" $E_1$ and $E_2$ where matter flows in and out. The rest of the boundary is insulating material $J\cdot\vec n=0$ (Neumann BC). To each divergence-free vector field $J(x)$ on $D$ which flow strictly from $E_1$ to $E_2$ we can assign a ""resistance"" $R_J$ as follows, because we can insert non-conducting walls along the streamlines, we can consider the streamlines from $E_1$ to $E_2$ as resistors of thickness dt, then $R_J$ is defined as the resistance of them all in parallel (so that would be $1/(1/R_1+1/R_2....))$ , in the limit as dt goes to 0. Each of the streamline resistors is defined as a series connection of resistors (So thats $R_a+R_b...$ ) with length dt2, and we take the limit as dt2 goes to zero aswell. The resistance of each resistor of thickness $dt*dt2$ is computed from $\sigma(x)$ Let $K(x)$ be the $J(x)$ with smallest $R_J$ . What differential equation governs $K(x)$ ? (besides $\nabla \cdot K=0) $ Edit:
This is probably incorrect because du is not constant, streamlines can come closer along the flow for instance, $R_J=1/(\int_{E_1} 1/(\int_{C_u} (1/\sigma(x))ds))du$ Where the u integral is along the length of the electrode, and $C_u$ is the streamline starting at u.","['physics', 'mathematical-physics', 'ordinary-differential-equations', 'fluid-dynamics']"
2985723,"A counting problem from an exam and my solution for it, can I submit an appeal?","The problem is: How many subsets of { ${1,2,...,n}$ } have an even sum of their elements? Please take a look at my solution since this is my answer from the exam and I think to appeal on this question, I'd like to hear your thoughts if I can appeal this. I decided to solve this problem by recursive formula, my solution: Let's define: $a_{n}$ - The number of subsets of { ${1,2,...,n}$ } that have an even sum of their elements. Let's split into two different cases: $*$ 1. $n$ is odd, therefore in this case we got: $2^{n-1}-a_{n-1}$ 2. $n$ is even, therefore in this case we got: $a_{n-1}$ $*$ EXPLANATION: $2^{n-1}$ is the number of subsets of { ${1,2,...,n}$ }. $a_{n-1}$ is the number of subsets of { ${1,2,...,n}$ } that have an even sum of their elements like I defined.  Therefore $2^{n-1}-a_{n-1}$ is the number of subsets of {1,2,...,n} that have an odd sum of their elements. From summarize the cases we got: $a_{n} = 2^{n-1}-a_{n-1}+a_{n-1}$ , Therefore $a_{n} = 2^{n-1}$ The answer to the main question is $a_{11} = 2^{11-1} = 2^{10}$ . The comment I got from my teacher about this answer is about the split into the cases, he wrote that $n$ don't have to be a part of the group and he is completely right. In fact, my final answer was right,  but my split into the two cases is completely wrong and I received 0% points for this answer.
I would like to appeal this question in claiming that I meant other cases separation and it was only a formulation mistake. I'll be glad if someone can tell me if it makes sense, and if it is, how to formulate it right to gain some points.","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
2985731,Do zero divisor pairs in reduced rings always come from two distinct minimal prime ideals?,"Let $S$ be a reduced, noetherian $k$ -algebra ( $k$ a field) with minimal prime ideals $P_1,\ldots,P_r$ . If $a \in P_1$ , $a \neq 0$ . Is there $b \in P_i$ , $b \neq 0$ with $i \neq 1$ such that $ab = 0$ . When I try to come up with examples in geometry, I always produce coordinate rings of plane affine curves $k[T]/f(T)$ where $T := T_1,\ldots,T_n$ and $f(T) \in k[T]$ is reducible into distinct factors. Then the above should hold. But I have no clue if this generalizes. If not, I would like to see an example. Thanks in advance!","['algebraic-geometry', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
2985733,Kantorovich inequality and Cauchy-Schwarz inequality,"On the wikipedia site for the Kantorovich inequality, it is claimed ... the Kantorovich inequality is a particular case of the Cauchy–Schwarz inequality... Here, ""Kantorovich inequality"" refers to $$
(x^\top A \, x) \, (x^\top A^{-1} \, x) \le \frac{(m+M)^2}{4 \, m \, M} \, \|x\|^4
$$ for a symmetric, positive definite matrix $A$ and $m$ , $M$ denote the smallest and largest eigenvalue of $A$ . I was wondering what is meant by the above claim of wikipedia. Is it really true that the Kantorovich inequality is a particular case of CSI (in the sense of: ""can be easily derived from"")? The closest assertion I was able to find is from this paper : If we plug in $x = \sqrt{A} \, y$ , we arrive at $$
\| A \, y\|^2 \, \|y\|^2 \le \frac{(m+M)^2}{4 \, m \, M} \, (y^\top A \, y)^2
$$ which is a reverse of the special case $$
y^\top A \, y \le \|A \, y\| \, \|y\|
$$ of CSI.","['inequality', 'cauchy-schwarz-inequality', 'linear-algebra', 'eigenvalues-eigenvectors']"
2985802,Why é​t​a​l​e​?,"Background: The notion of an étale morphism has proved itself to be ubiquitous within the realm of algebraic geometry. Apart from carrying a rich intuitive idea, it is the first ingredient in notions and theories such as étale cohomology, a Galois theory of schemes, and algebraic spaces. It is safe to say that, of the many classes of morphisms that we care for in algebraic geometry, étale morphisms are among the most important ones. Question: How is it a priori clear that étale morphisms are important, and in particular, how can we see that étale morphisms are
  often 'the right' thing to look at? Example: One could define variants of étale cohomology by redefining the étale site using slightly different classes of morphisms. In some ways this has been done, as things like flat cohomology and Nisnevich cohomology are a thing; however, étale cohomology is arguably more important than these variants. What is it that distinguishes étale cohomology here? Example: We could reconsider Galois theory by replacing 'étale' with something else, and see what happens. Probably this does not lead anywhere (as otherwise someone would've written about it).  Perhaps, we could argue here that in the differential-geometric case, one loses all hopes of a classification if one replaces 'covering space' by something slightly weaker. But I'm not entirely convinced by this. What I know: I know that people love to tell the story of how the Zariski topology does not have enough opens and so we want more. However, this does not tell us why we specifically choose étale morphisms rather than something similar. I also know that, to emphasize the strength of the étale site, people say that the étale site gives us an algebro-geometric analogue on an Implicit Function Theory, however I've never seen this explicitly in action, nor do I think it can explain everything.","['etale-cohomology', 'arithmetic-geometry', 'algebraic-geometry', 'schemes']"
2985815,Simplifing a Double Integral,"I would like to know how this result from a textbook was obtained: $$
\int_0^1du\int_0^u\frac{u}{\sqrt{1-u}}\ln\frac{u}{v}\, dv = \int_0^1\frac{u^2}{\sqrt{1-u}}\, du=\frac{16}{15}
$$ Is this some special result which I need to know? Otherwise if I simplify using the standard approach : $$
\int_0^1 \frac{u^2}{\sqrt{1-u}}\ln u\, du - \int_0^1\frac{u}{\sqrt{1-u}}\int_0^u (1)(\ln v)\,dv
$$ which would require the value of $\ln(0)$ . I have verified that the result $\frac{16}{15}$ is indeed correct using WolframAlpha, but I do not know how the simplification from the text was obtained.","['integration', 'multivariable-calculus', 'definite-integrals']"
2985854,Elastic Net as LASSO,"Good evening everybody, I need help with an excercise on Regularised Regression. What I need to do is turn an Elastic-net problem: \begin{equation}
argmin_\omega \Vert y-\Phi(x)^T \omega \Vert_2^2 + \lambda(\alpha\Vert\omega\Vert_2^2 + (1-\alpha)\Vert w\Vert_1)
\end{equation} Into a LASSO: \begin{equation}
 argmin_\omega \Vert \bar y-\bar\Phi(x)^T \omega \Vert_2^2 + \bar\lambda\Vert w\Vert_1
\end{equation} for suitable new $\bar\lambda$ , $\bar y$ and $\bar \Phi(x)$ . What I have thought is that, if $y-\Phi(x)^T\omega$ and $\omega$ are orthogonal (this is the case if $\omega$ is the solution of the Least Squares problem without regularization), we have that: \begin{equation}
\Vert y-\Phi(x)^T \omega \Vert_2^2 + \lambda\alpha\Vert\omega\Vert_2^2 = \Vert y-(\Phi(x)^T-\lambda^2\alpha^2 I)\omega\Vert_2^2
\end{equation} And so, I have solved the problem. But this is not the case. Any idea? Thank you in advance","['regression', 'statistics', 'machine-learning']"
2985902,Symmetric polynomial and degree,"Sorry if the title is not very explicit, but I didn't find a good title for the problem. In fact, this is the problem : Let $P \in \mathbb{R}[X]$ a polynomial which verify the following
  condition : for all $a,b,c$ such that $ab+ac+bc=0$ , we have : $P(a-b)+
 P(b-c) + P(c-a) = 2P(a+b+c)$ .
  Considering $ax$ , $bx$ , $cx$ for all $x \in \mathbb{R}$ , show that P is the sum of a monomial of degree $2$ and a monomial of degree $4$ . I didn't find how to find the solution. With the indication, I really don't know what to do actually, so I've tried several others method, including the one which consists of writing : $p_0 + p_1((a-b)+(b-c)+(c-a))+...+ p_n((a-b)^n + (b-c)^n + (c-a)^n) = p_0 + p_1(a+b+c)+...+p_n(a+b+c)^n$ and try to reasoning on it to find a condition of the $p_i$ (the coefficients of $P$ ), but it's seems very complicated. I thought about using symmetric polynomial, but I don't see how. Maybe we should prove that for all $p_i$ , except $i=2,4$ , $\sigma_2(a,b,c) / p_i$ for a good choice of $a,b,c$ which verify $ab+ac+bc=0$ but I don't succeed to do it... Someone could help me, please (using the idea suggered by the exercise if it's possible) ? :)","['abstract-algebra', 'symmetric-polynomials', 'polynomials']"
2985950,"Find the probability that A,B,C are connected","I was given the following problem as a homework assignment: Denote with $S$ the ball with center $O$ . Three points $A, B$ and $C$ are chosen at random on its surface, their positions being independent and each
  uniformly distributed on the surface. Points A and B can be connected
  together if the angle $AOB<\pi/2$ . What is the probability that they can be connected (with, for example, $A$ connecting with $B$ via $C$ if necessary)? I was given the answer by my professor as a hint: $(\pi+2)/(4\pi)$ . I thought about it in the following way. Let $r$ denote the line that passes through the center $O$ , which will intersect the ball at a point, call it $A$ . Now, take a plane perpendicular to $r$ and make it pass through $O$ . The plane divides the sphere into $2$ hemispheres, one that contains the point $A$ and one that does not. If $B$ is placed in the hemisphere that contains $A$ , then $AOB<\pi/2$ and so they can connect. Otherwise, they cannot. So the probability of $A$ connecting with $B$ is equivalent to the prbability that $B$ falls in one of the two hemispheres that contains $A$ which is $1/2$ . (The idea of this reasonsing is to keep $A$ fixed and set it at the ""center"" of the hemisphere's surface). Now I am left with calculating the probability that $B$ is placed on the other hemisphere so that $A$ must connect with $B$ through $C$ . Which is again $1/2$ ? What is wrong with this reasoning? And how would one solve this problem?","['geometric-probability', 'probability-theory', 'probability']"
2985972,Integral inequality with a strange condition,"Let $f$ be a continuously differentiable real valued function on $[0,1]$ . It is given that $\displaystyle \int_{\frac{1}{3}}^{\frac{2}{3}}f(x) dx=0$ Find the minimum value of $\dfrac{\int_{0}^{1} (f'(x))^2 dx}{\left( \int_{0}^{1} f(x) dx \right)^2}$ I tried to use Cauchy-Schwarz to show that $$\frac{\int_{0}^{1} (f'(x))^2 dx}{\left( \int_0^1 f(x) dx \right)^2} \ge \frac{\left( \int_0^1 \bigl| f(x)f'(x) \bigr| dx \right)^2}{ \left( \int_0^1 f(x) dx \right)^2} \ge \frac{ f(1)^2 - f(0)^2}{2 \left( \int_0^1 f^2 (x) dx \right)^2}$$ But I can't proceed from here. Also, I don't know how to use the condition $\int_{1/3}^{2/3}f(x) dx=0$","['integration', 'definite-integrals', 'real-analysis', 'inequality', 'derivatives']"
2985976,An inequality with $e^{ix}$,"I am to prove the following statement $$\bigg|e^{ix} - \sum_{k=0}^m \frac{(ix)^k}{k!} \bigg| \le \frac{|x|^{m+1}}{(m+1)!},$$ where $x \in \mathbb{R}$ . I used Taylor's expansion of $e^x$ . That led me to this $$\bigg|\sum_{k=m+1}^{\infty}\frac{(ix)^k}{k!} \bigg| \le \frac{|x|^{m+1}}{(m+1)!}$$ Let's focus on the LHS $$\bigg|\sum_{k=m+1}^{\infty}\frac{(ix)^k}{k!} \bigg|\le \sum_{k=m+1}^{\infty}\big|\frac{(ix)^k}{k!} \big| = \sum_{k=m+1}^{\infty}\frac{|ix|^k}{k!}$$ Because $|i| = 1$ we get $$\sum_{k=m+1}^{\infty}\frac{|x|^k}{k!}$$ Let's look at the RHS. It's the first component of the sum on LHS. All the elements of the sum are positive thus the first one cannot be bigger then the sum. What have I done wrong?","['complex-analysis', 'exponential-function']"
2986010,Does the geometric version of Nakayama's lemma hold for smooth manifolds?,"Consider the following geometric formulation of Nakayama's lemma. Proposition. Let $F$ be a quasi-coherent sheaf locally of finite type on a scheme $X$ . Consider the quotient map $\pi:F_x\to F_x\otimes\Bbbk (x)$ . Given $s_1,\dots ,s_n\in F_x$ , suppose their image generates $F_x\otimes \Bbbk (x)$ . Then the $s_i$ extend to a neighborhood $U\subset X$ of $x$ on which they define a surjective arrow $$(\mathcal O _X|_U)^n\overset{(s_1,\dots ,s_n)}{\longrightarrow}F|_U\to \bf 0$$ on $U$ . When this holds, we say $s_1,\dots ,s_n$ generate $F$ over $U$ . Let $(M,\mathcal T_M)$ be a manifold with the sheaf of sections of its tangent bundle. The $x$ -fiber of $\mathcal T_M$ is the vector space of tangents at $x$ . The $x$ -stalk is the module of germs at $x$ of vector fields. Does ""Nakayama"" hold for $(M,\mathcal T_M)$ ?","['smooth-manifolds', 'algebraic-geometry', 'sheaf-theory', 'commutative-algebra', 'differential-geometry']"
2986015,Standard Normal Moments and Combinatorics,"At around 16-17 mins in this video , the professor calculates the even moments of the standard normal. If $Z \thicksim N(0,1)$ then $$\mathbb{E}[Z^{2n}] = \frac{(2n)!}{2^n \cdot n!}.$$ The right hand side is the number of ways to form $n$ partnerships out of $2n$ people and hints that the moments are linked to this combinatorics problem. My question is, how are these linked? Not entirely sure how to approach this. A possible approach I've seen in another post is expressing $\mathbb{E}[Z^{2n}]$ recursively. Although, the connection is not apparent (at least for me).","['moment-generating-functions', 'combinatorics', 'probability-theory']"
2986070,The relationship between the differential and the directional derivative of a function,"I am currently studying differential manifolds (from John M. Lee 's book), and have a question concerning the difference between what is defined as the $\textbf{differential of a function F}$ , and the $\textbf{directional derivative of a function F}$ . Let $M \subset \mathbb{R}^{m}$ , let $N\subset \mathbb{R}$ , and suppose $F:M \rightarrow N$ is a smooth map. Then, The differential of $F$ at $p \in M$ is a map $dF_{p}:T_{p}M \rightarrow T_{F(p)}N$ defined as, for some $v \in T_{p}M$ , $dF_{p}(v)$ is a derivation in $T_{F(p)}N$ defined as, for all $f \in C^{\infty}(N)$ , $dF_{p}(v)(f)=v(f \circ F)$ . Now, because $M$ and $N$ are Euclidean themselves, if $v=(v_{1},...,v_{N})$ , this can be expressed as $dF_{p}(v)(f)=v_{1} \cdot \frac{\partial f}{\partial F}\ \frac{\partial F}{\partial x_{1}}+...+v_{m} \cdot \frac{\partial f}{\partial F}\ \frac{\partial F}{\partial x_{m}}$ . The directional derivative of $F$ at $p$ in direction $v \in \mathbb{R}^{m}$ is given by $D_{v}F(p)=v_{1} \cdot \frac{\partial F(p)}{\partial x_{1}}+...+v_{m}\cdot \frac{\partial F(p)}{\partial x_{m}}$ . Now, it seems that if $Id:\mathbb{R}\rightarrow \mathbb{R}$ is the identity function on $\mathbb{R}$ , we have that for $v \in T_{p}M$ , $v(F)=dF_{p}(v)(Id)=D_{v}F(p)$ . Am I reading this correctly? I am trying to weed through the abstraction of differentials between manifolds and ground it into something more familiar, the directional derivative. Are directional derivatives in the theory of manifolds expressed as the the differential evaluated at the identity function, which are equivalent to simply evaluating $v(F)$ itself?","['analysis', 'real-analysis', 'general-topology', 'differential-topology', 'differential-geometry']"
2986116,How much longer does it take to paint a similar box with $5$ times more volume?,"This is a question from a precalculus class that I'm a TA for. You build a box that has a volume of 100 cubic feet. It takes two people ten minutes to spray paint this box. How many minutes does it take three people to spray paint a similar box that has a volume of 500 cubic feet? Assume that the area of surface to be covered is proportionate to both the number of people working and the time spent spray painting. A student just asked me how to answer it, so I figured I'd write up the correct calculations and post it online to help anyone else who may wander across it.","['algebra-precalculus', 'unit-of-measure']"
2986118,Is there a combinatorial proof that $e$ is finite?,"I'm looking for an integer $N$ and a combinatorial proof either that $(n+1)^n<Nn^n$ or that $\sum_{k=0}^n \frac{n!}{k!}<N\cdot n!$ . By ""combinatorial proof of $a<b$ "" I mean exhibiting explicit finite sets $A$ and $B$ with cardinalities $a$ and $b$ , respectively, and either an injection $A\to B$ or a surjection $B\to A$ .","['combinatorics', 'combinatorial-proofs']"
2986127,"If the objects of a category form a proper class, do the arrows necessarily form a proper class too?","In some categories, like $\text{Set}$ or $\text{Group}$ , the objects are ""constructed"" out of sets (or are sets, possibly with additional structure). In order to avoid paradoxes, the collection of objects is therefore a proper class✱. If the class of objects is proper, is it possible for the collection of arrows to still be a set? A not-convincing argument against it is that each object gets an identity arrow therefore there are ""too many"" arrows to be a set. I don't trust intuitions about size with things as big as $\text{Set}$ though. A not-convincing argument that it's plausible is that arrows are opaque and I, the category-maker, get to freely pick the labels for the arrows, the definition of the composition relation, and the source and destination for each of the arrow-labels. How do I show that I can't come up with a set of labels big enough to label all of my arrows? ✱ I don't know whether it makes sense to start with a ""possibly-proper class of all sets satisfying some predicate"" and then inspect the class in some way to see if it's a proper class or not.","['elementary-set-theory', 'category-theory']"
2986143,Application of squeeze theorem,I don't really know how the Squeeze theorem works and I tried applying it to solve this limit: $$\lim_{n\to\infty}\text{ } \frac{1}{n^2}+\frac{1}{(n+1)^2}+\dots+\frac{1}{(n+n)^2}$$ So $$\frac{n}{(n+n)^2}\leq \frac{1}{n^2}+\frac{1}{(n+1)^2}+\dots+\frac{1}{(n+n)^2} \leq \frac{n}{n^2} $$ Then: $$\lim_{n\to\infty}\text{ } \frac{n}{n^2}=0$$ $$\lim_{n\to\infty}\text{ } \frac{n}{(n+n)^2}=0$$ Therefore: $$\lim_{n\to\infty}\text{ } \frac{1}{n^2}+\frac{1}{(n+1)^2}+\dots+\frac{1}{(n+n)^2}=0$$ Is this the correct way?,"['limits', 'analysis']"
2986144,What is a form?,"I have read about differential forms, bilinear forms, quadratic forms and some other r-linear forms but I still have this shred of doubt in my mind on what exactly is a form. I have an assumption that it is to a ring what a vector is to a field. I apologise if it is a repost or something though.","['differential-geometry', 'linear-algebra', 'modular-forms', 'differential-forms', 'quadratic-forms']"
2986146,"What is the biggest possible sum $|X_1-X_2|+|X_2-X_3|+\cdots+|X_{n-1}-X_n|$ where $X_1,X_2,\cdots,X_n$ are first $n$ positive integers?","What is the biggest possible sum $|X_{1}-X_{2}|+|X_{2}-X_{3}|+\cdots+|X_{n-1}-X_{n}|$ where $X_{1},X_{2},\cdots,X_{n}$ are first $n$ positive integers?","['permutations', 'extremal-combinatorics', 'combinatorics', 'discrete-optimization', 'inequality']"
2986150,Existence of a diffemorphism that maps one curve to another,"Consider $1 < n\in \mathbb {N} $ , let $\gamma_1,\gamma_2 : [0,1] \to \mathbb{R}^{n}$ be smooth paths such that $$\gamma_1 (0) = \gamma_2(0) \neq \gamma_1(1) = \gamma_2(1) $$ and $\gamma_1, \gamma_2$ are injetive functions. Question: Does there exist a diffeomorphism $\varphi: \mathbb{R}^n \to \mathbb{R}^n$ such that $\varphi (\gamma_1 ([0,1])) = \gamma_2 ([0,1]).$ Does anyone know if this result is true? This seems true but I do not know how to prove it, can anyone help me?","['differential-topology', 'analysis', 'differential-geometry']"
2986173,Inequality in difference of square roots,"I think this inequality is true and I'm trying to prove it: For real, non-negative $a$ and $b$ $$\lvert \sqrt{a} - \sqrt{b}\rvert \le \lvert \sqrt{a - b} \rvert$$ Closest thing I've found is equation $$\sqrt{a} + \sqrt{b} = \sqrt{a + b + \sqrt{4ab}}$$ But this breaks if I set $b$ to $-b$ because then you're taking the root of a negative. Does anyone have a proof of this, or a more general result that implies this?","['real-numbers', 'algebra-precalculus']"
2986371,Kurtosis of uniform distribution,"I am a beginner in statistics, and am self-studying. I want to determine the kurtosis for uniform distribution. Could someone please help me with this problem?","['uniform-distribution', 'probability-theory']"
2986436,How do I determine the function from its graphic?,How am I supposed to know the function from it's graphic?,['functions']
2986454,"$E[\frac{1}{X}]$ for $X\sim\Gamma(n,\theta)$","Consider iid random varibales $(X_i)_{1\le i\le n}$ with $X_i\sim Exp(\theta)$ for $1\le i\le n$ and $\theta\in(0,\infty)$ . Then we have $$\sum_{i=1}^nX_i\sim \Gamma(n,\theta)$$ with density function $$f(x)=\frac{\theta^n}{(n-1)!}x^{n-1}e^{-\theta x}\mathbb{1}_{[0,\infty)}(x).$$ Now I want to calculate \begin{align}E\Bigg[\frac{1}{\frac{1}{n}\sum_{i=1}^nX_i}\Bigg]=n\cdot E\Bigg[\frac{1}{\sum_{i=1}^nX_i}\Bigg]&=n\cdot\int_{0}^\infty \frac{1}{x}\frac{\theta^n}{(n-1)!}x^{n-1}e^{-\theta x} dx\\
&=\frac{n\theta^n}{(n-1)!}\cdot\int_{0}^\infty x^{n-2}e^{-\theta x} dx,\end{align} but I do not know where I have to go from here. I know the antiderivate of $x^{n-2}e^{-x}$ , but I do not know how to deal with $x^{n-2}e^{-\theta x}$ . Can someone give me a hint? Thanks in advance!","['statistics', 'descriptive-statistics', 'stochastic-calculus', 'probability-theory', 'probability']"
2986503,Does there exist a continuous path between two sets of oriented basis for a vector space out of a collection of subspaces?,"Let $V_1, V_2, \dots, V_n$ be a collection of vector subspaces in $\mathbb R^n$ . For each $j=1, \dots, n$ , $\dim(V_j) = m$ with $2 \le m < n$ . We also have the condition: for any collection of $\lceil{\frac n m}\rceil$ vector spaces from $\{V_1, \dots, V_n\}$ , then $V_{k_1} + \dots + V_{k_{\lceil \frac n m \rceil}} = \mathbb R^n$ .  Suppose we construct a basis $U = \{u_1, \dots, u_n\}$ of $\mathbb R^n$ in the manner: $u_j \in V_j$ for each $j$ . Now suppose we construct another basis $W = \{w_1, \dots, w_n\}$ in the same manner, i.e., $w_j \in V_j$ for each $j$ . I am wondering whether $U$ is connected with $W$ in the sense: there is a path $\gamma = \gamma_1 \times \gamma_2 \times \dots \times \gamma_n$ , where each $\gamma_j: [0,1] \to V_j$ is a continuous path connecting $v_j$ and $w_j$ in $V_j$ and for each $t$ : $\gamma(t)$ forms a basis for $\mathbb R^n$ . We assume the basis $\{v_j\}$ and $\{w_j\}$ have the same orientation. The basis can be identified by $GL_n(\mathbb R)_+$ or $GL_n(\mathbb R)_-$ and we know they are connected. But is there a way to guarantee on the path, each column vector only varies in the corresponding subspace? I asked a similar question here Constructing a continuous path between two sets of oriented basis for a vector space out of a collection of subspaces . The conditions are stronger here. An example of $V_1, \dots, V_n$ : suppose $n=5$ , $m=2$ . The construction I have in mind is: $V_i = \text{span} ( (1, a_i, 0, 0, 0), (0, 0, 1, a_i, a_i^2))$ . As long as $a_1 \neq \dots \neq a_5 \neq 0$ , any three subspace would span $\mathbb R^5$ . For other cases, we can use similar idea.","['general-topology', 'path-connected', 'linear-algebra']"
2986546,Subspace of all twice differentiable functions,"I'm a bit new to linear algebra. I have a question about why the solution to the differential equation $y''+2y'+y+x=0$ is not a subspace of the vector space of all twice differentiable functions.
When I solve the equation using the method of unknown coefficients, I get the general solution $y = C_1e^x+C_2xe^x-x+2$ By my line of thinking, this function is twice differentiable for all values $C_1$ and $C_2$ , so all solutions are members of the given set. However, the sum of any two solutions, for example $(C_1e^x+C_2xe^x-x+2) + (B_1e^x+B_2xe^x-x+2)
= e^x(C_1+B_1)+xe^x(C_2+B_2)-2x+4$ Where the expression $-2x+4$ does not satisfy the particular solution. Am I on the right track, or is there another reason that it is not a subspace?","['linear-algebra', 'vector-spaces', 'ordinary-differential-equations']"
2986560,"If a sequence of independent random variables converges almost surely to a random variable, then that limit is almost surely a constant","Let $\{X_n\}$ be a sequence of independent random variables converging almost surely to a random variable $X$ . Then how to show that $X$ is almost surely a constant ? I think I somehow have to apply the Borel-Cantelli lemma for independent events, but I don't know how. Please help.","['measure-theory', 'independence', 'borel-cantelli-lemmas', 'probability-theory', 'random-variables']"
2986563,Counting Measure not $\sum$-finite,"Exercise 3.13e in Cinlar's Probability and Stochastics: Consider the measurable space $(E,B(E))$ , where $E=[0,1]$ and $B(E)$ is the set of all Borel subsets of $E$ . Show that the counting measure $\mu$ on it is not $\sigma$ -finite and also not $\sum$ -finite, where $\sum$ -finite means that there is a sequence of finite measures $\mu_1, \mu_2, ...$ such that $\mu=\sum{\mu_i}$ . I proved the not $\sigma$ -finite part by showing that that implies the countability of $[0,1]$ , a contradiction, but I'm not sure how to prove non- $\sum$ -finiteness.",['measure-theory']
2986589,Let $T: V \to V$ be a linear map such that $T^2-3T+2I=0$.,"Let $T: V \to V$ be a linear map such that $T^2-3T+2I=0$ , where $I$ is the identity map. question: a) Prove that $V=\ker(T-2I) \oplus\ker(T-I)$ b) let $A$ be an $n \times n$ matrix such that $A^2-3A+2I_n=0$ Where $I_n$ is the $n\times n$ identity matrix. True or false: $A$ is diagonalizable I attempted $\ker(T-2I)=(T-2I)V=0$ $\ker(T-I)=(T-I)V=0$ I know the direct sum should be the join of $\ker(T-2T)$ and $\ker(T-I)$ is $0$ , and I am not sure how to prove it I am a first year student from McGill U. I am doing linear mapping on linear algebra. The textbook I am using is Linear Algebre edition sixth by SEYMOUR LIPAXHUTZ","['direct-sum', 'linear-algebra', 'vector-spaces']"
2986658,The inverse image of a measurable set under a measurable function is measurable?,"I have a confusion with measurable functions.
I just saw that the statement ""The inverse image of a measurable set under a measurable function is measurable"" is false with counter-example the function on Cantor set but I know the definition of f measurable is: Let $f:(X,O_X)\to (Y,O_Y)$ with $O_X$ $\sigma$ -algebra of $X$ and $O_Y$ $\sigma$ -algebra of $Y$ . $f$ said $(O_X-O_Y)$ -measurable function if for all $B\in O_Y\ f^{-1}(B)\in O_X$ . But, What is the difference with 
""The inverse image of a measurable set under a measurable function is measurable? ""","['measure-theory', 'cantor-set', 'measurable-functions']"
2986715,"Strong convergence in $L^1$, $\mathbb{R}^d$ with $d>1$","A sequence of positive function $f_n(x)\rightarrow f(x)$ pointwise on the unit ball $B:=\{x:|x|\leq 1\}\subset \mathbb{R}^d$ as $n\rightarrow \infty$ . Suppose the sets $\Lambda_t^n=\{x:f_n(x)\geq t\}$ satisfy the following two properties. The Lebesgue measure of their boundary is zero $\mu(\partial \Lambda_t^n)=0$ for all $n\in \mathbb{N}$ and $t\in \mathbb{Q}$ , and for all $n,m\in \mathbb{N}$ we have $\Lambda_m^n\subset \{x:|x-x^n_m|\leq 1/m\}$ for some $x^n_m\in B_1(0)$ . Prove that $f_n\rightarrow f$ strongly in $L^1(B_1)$ if the dimension $d>1$ . I managed to show that $f_n$ and $f$ are measurable, but I have no idea how to proceed. I don't know what is special about $\mathbb{R}$ that makes it different from $\mathbb{R}^d$ with $d>1$ . Any help would be appreciated.","['measure-theory', 'lebesgue-measure', 'product-measure', 'real-analysis']"
2986720,Proving that f is continous,"In the previous real analysis sessions in my university , a question came as follow:
A function f defined for x>0 is increasing such that g(x) =f(x) /x is decreasing. Prove that f is continous. I tried to work using the ε-δ definition of continuity and to make use of the variations of f by trying to find the limit just before and after any strictly positive number a, but I am not finding a way to bound the difference between f(x) and f(a). I hope any could give me a hint or a way to start with.","['continuity', 'functions', 'real-analysis']"
2986732,"$\langle f(t),g(t)\rangle' = \langle f'(t),g(t)\rangle + \langle f(t),g'(t)\rangle$ for differentiable $f,g : \mathcal{R} \to \mathcal{R}^n$?","Suppose that $f,g$ are differentiable functions from $\mathcal{R}$ to $\mathcal{R}^n$ . Show that $\langle f(t),g(t)\rangle' = \langle f'(t),g(t)\rangle + \langle f(t),g'(t)\rangle$ I've been banging my head against a brick wall on this for a while, I can see that the RHS is something like the product rule for differentiation. We're not given an inner product but even just trying to by parts integrate the standard inner wasn't working. Thanks in advance for any help.","['inner-products', 'derivatives', 'linear-algebra']"
2986748,"How do we formally define ""j-th smallest element""?","Let $A$ be a nonempty finite subset of $\mathbb{R}$ . Firstly, let me write down how to define the term ""the smallest element of $A$ "" formally. Suppose 'for every $x\in A$ , there exists $y \in A$ such that $x>y$ '. Since $A$ is nonempty, we can pick an element $x_0\in A$ . Then by assumption, there exists $x_1$ such that $x_0>x_1$ . Continuing in this manner, we get a finite sequence $x_0>x_1>...>x_{|A|}$ . This means that $\{x_0,...,x_{|A|
}\}$ is a subset of $A$ , but the former set has cardinality greater than the latter set. This is a contradiction. Thus, there exists $x\in A$ such that $x\leq y$ for all $y\in A$ . And uniqueness of such $x$ follows easily. Hence, we just defined the term ""the smallest element of the set $A$ "" formally. Note that this is the minimum among distict elements . If we continue in this manner, we can define the $j$ -th minimum among distinct elements formally. (For example, $2$ -nd minimum can be defined as $min ( A\setminus\{min A\})$ ) Now my question is the below: Say, we have defined the term "" $j$ -the smallest element"" (possibly not disticint) although we have not defined it formally yet because all we know what it means informally. Denote this function by $F_j:\mathbb{R}^n \rightarrow \mathbb{R}$ . This is slightly different situation from previously described one, because the case $x_1=...=x_n$ may happen. However for the case $j=1$ , there is actually no difference from the above distinct-case. However, for $j>1$ , I am not sure how to define it formally. How do we define it? One way is the following: Let $x\in \mathbb{R}^n$ Then, there exists permutation $\sigma\in S_n$ such that $x_{\sigma_1}\leq\cdots\leq x_{\sigma_n}$ . Let $\tau \in S_n$ be another permutation with the same property. If one can show that $x_{\sigma_i}=x_{\tau_i}$ for all $i$ , then $F_j$ is well-defined and we are done. How do I prove this equality? And is there another clever ""neat"" way to define $F_j$ formally? Thank you in advance! [EDIT] My description seems very confusing. To make it clear, take, for example, $a_1=1, a_2=1, a_3=2$ . I want to have $1$ as both the first and second smallest element and $2$ as the third smallest element, so that $F_1(1,1,2)=1, F_2(1,1,2)=1, F_3(1,1,2)=2$ .","['permutations', 'formal-proofs', 'order-theory', 'elementary-set-theory', 'terminology']"
2986764,"Double summation $\sum_{m,n=1\, m\neq n}^\infty{\frac{m^2+n^2}{mn(m^2-n^2)^2}}$","As a follow to this answer I came across the double sum $$\sum_{m,n=1\, m\neq n}^\infty{\frac{m^2+n^2}{mn(m^2-n^2)^2}}.$$ But unfortunately I do not have skills in techniques to handle double summation . Help appreciated. I've made some research in MSE and found several questions which could be helpful: 1) $\sum_{m=1}^{\infty}\sum_{n=0}^{m-1}\frac{(-1)^{m-n}}{(m^2-n^2)^2}=-\frac{17\pi^4}{1440}$ 2) $\sum_{m=1}^{\infty}\sum_{n=1}^{m-1}\frac{ 1}{m n\left(m^2-n^2\right)^2}=\frac{\pi^6}{12960}$ 3) $\sum_{k=1}^{\infty} \sum_{n=1}^{\infty} \frac{1}{n^2k^2(n+k)^2}=  \frac{1}{3}\zeta(6)$","['summation', 'sequences-and-series']"
2986813,Complete elliptic integral $K(k) $ for $k>1$,"I have always tried to work with elliptic integrals with modulus $k\in(0,1)$ to avoid the issues related to complex variables. In what follows I have tried to link the integral of modulus greater than $1$ with those of modulus less than $1$ . Let $k>1$ and consider $$K(k) =\int_{0}^{1}\frac{dx}{\sqrt {(1-x^2)(1-k^2x^2)}} $$ Splitting the range of integration into $[0,1/k]$ and $[1/k,1]$ we get $$K(k) =\frac{1}{k}K\left(\frac{1}{k}\right)-i\int_{1/k}^{1}\frac{dx}{\sqrt{(1-x^2)(k^2x^2-1)}}\tag{1}$$ and let $x=1/\sqrt{1-k'^2y^2}$ then we have $$dx=\frac{k'^2y\,dy}{(1-k'^2y^2)^{3/2}}$$ and $$1-x^2=-\frac{k'^2y^2}{1-k'^2y^2}$$ and $$k^2x^2-1=-\frac {k'^2(1-y^2)}{1-k'^2y^2}$$ and hence we arrive at the relation $$K(k) =\frac{1}{k}\left\{K\left(\frac{1}{k}\right)-ikK\left(k' \right) \right\} $$ Here $k'=i\sqrt{k^2-1}$ is purely imaginary and with some manipulation one can show that $$K(k') =\frac{1}{k}K\left(\frac {\sqrt{k^2-1}}{k}\right)$$ and thus we arrive at $$K(k) =\frac{1}{k}\left\{K\left(\frac {1}{k}\right)-iK\left(\frac{\sqrt{k^2-1}}{k}\right)\right\}$$ where $k>1$ . Replacing $k$ with $1/k$ we get the typical representation of the above formula as $$K(1/k)=k(K(k)-iK(k'))\tag{2}$$ where $0<k<1$ . My question is about the choice of $i$ in equation $(1)$ . We could equally well have used $-i$ instead of $i$ . How does one choose the correct sign of $i$ ? The DLMF reference (equation 19.7.2) gives the rule that the sign of $i$ is opposite to that of imaginary part of $k^2$ . But that does not help here as $k^2$ is real. The derivation above was more to confirm that my calculations are correct and one does not need to get bogged down into them.","['complex-analysis', 'elliptic-integrals']"
2986837,Intersection of closed and open set,"In arbitrary topological space, let $A$ be open and $B$ be closed. If $\text{int}(B) \neq \emptyset$ and $A \cap B \neq \emptyset$ , is it guaranteed that $A \cap \text{int}(B) \neq \emptyset$ ? If not, is there any reasonably weak additional condition that can assure it?",['general-topology']
2986842,Grothendieck group of a smooth complex projective curve,"I'm reading the section of Le Potier's 'Lectures on Vector Bundles' where he proves that the Grothendieck group $K(X)$ of a smooth complex projective curve $X$ (which is the free abelian group on coherent sheaves on $X$ modulo the relations $F = F' + F''$ for exact sequences $0 \to F' \to F \to F'' \to 0$ ) is in fact equal to $A(X) \oplus \mathbb{Z}$ where $A(X)$ is the subgroup generated by the structure sheaves of divisors (things like $\mathcal{O}_{X,P}/\mathfrak{m}_P^n$ ) and $\mathbb{Z}$ corresponds to the trivial bundles of all ranks. The main task is to show that every vector bundle $F$ can be written as an element of $A(X) \oplus \mathbb{Z}$ . If $F$ were generated by its sections, then we can consider an appropriate trivial bundle $\mathcal{O}^r$ such that $\mathcal{O}^r \to F$ is an isomorphism on an open set and so, we'll have - $$0 \to \mathcal{O}^r \to F \to \Theta \to 0$$ where $\Theta$ is going to be the structure sheaf of a divisor. Thus $F \in A(X) \oplus \mathbb{Z}$ . Now, for a general $F$ , some high twist $F(n)$ is going to be generated by its sections and so, $F(n) \in A(X) \oplus \mathbb{Z}$ . Then, he says ""It is clear that every element of $A(X)$ has square zero in $K(X)$ ( $K(X)$ has a ring structure given by tensor product). Therefore, $\mathcal{O}(-n)$ is contained in this subgroup and this subgroup is in fact a subring. It follows that $K(X)=A(X) \oplus \mathbb{Z}$ "". To prove that $O(-n)$ is in this subgroup, I considered the short exact sequence $$0 \to \mathcal{O}(-n) \to \mathcal{O} \to \mathcal{O}_{X,P}/\mathfrak{m}_{P}^n \to 0$$ which implies that $\mathcal{O}(-n) \in A(X) \oplus \mathbb{Z}$ . Now, for a general $F$ , if I were to take the short exact sequence  for some suitable $n$ - $$ 0 \to \mathcal{O}^r \to F(n) \to \Theta \to 0$$ and then twist this by $\mathcal{O}(-n)$ to get - $$0 \to \mathcal{O}(-n)^r \to F \to \Theta' \to 0$$ $F$ would be in $A(X) \oplus \mathbb{Z}$ as $\mathcal{O}(-n)$ and $\Theta'$ both are. I am not able to see the significance of the quoted paragraph and why it is relevant to the proof, I would be extremely grateful if somebody could point that out. I am also not entirely sure whether I am missing something in my proof. Thanks in advance!","['algebraic-vector-bundles', 'algebraic-geometry']"
2986843,How can I get the formula for this function?,"suppose $f:\mathbb N \to \mathbb N$ , And For any number given to it ,acts as follows. Multiplies the number in itself and stores it . then removes the first and last quarters of the resulting number . for example, $f(47)=20 $ because $47^2 =2209 $ when we removes the first and last quarters of $2209 $ we have $20$ . I only work with two digit numbers.my domain. I mean $\{x: 10 \leq x \leq 99\}$ . What is the formula for this function? thanks","['calculus', 'functions']"
2986853,"Are $f(x,y) :=$ min{x,y} and $g:\mathbb{R^n} \to \mathbb{R}$ with $g(x) := \left\lVert x \right\rVert_2$ partially differentiable?","I have to find out if the function $f: \mathbb{R^2} \to \mathbb{R}$ with $f(x,y) :=$ min{x,y} and the function $g:\mathbb{R^n} \to \mathbb{R}$ with $g(x) :=  \left\lVert x \right\rVert_2$ are partially differentiable. Furthermore, I have to find out the gradients $\nabla f(a)$ and $\nabla g(a)$ for all points $a$ , where they exist. I know that $\min \text {{x,y}} = \frac{1}{2} (x+y-|x-y|)$ . $\left\lVert x \right\rVert_2$ = $\sqrt {\sum_{k=1}^n x_i^2 }$ So the partial derivate $\frac{\partial g}{\partial x_i}$ for a generic $x_i$ would be $$\frac{\partial f}{\partial x_i} = \frac{1}{2\sqrt{\sum_{k=1}^n x_i^2}}\cdot2x_i = \frac{x_i}{\left\lVert x \right\rVert}$$ and the gradient of g is $\nabla g= \frac{1}{\left\lVert x \right\rVert}\cdot x$ Though I think that a proof requires more...","['normed-spaces', 'maxima-minima', 'functions', 'partial-derivative', 'derivatives']"
2986900,How many $n$-pointed stars are there?,"Say we have $n$ distinct points spaced evenly in a circle. Define a star as a connected graph with these points as vertices and with $n$ edges, no two having the same endpoints. We think of two stars as being equivalent if they differ only by a rotation (if they differ by a reflection I consider them as different objects). I'd like to know how many different stars there are on $n$ points. For example there are only two 4-pointed stars: a square and a bowtie. There are four 5-pointed stars: And I think there are twelve 6-pointed stars: (I'm not completely sure that I have exhausted the possibilities for 6 points). A quick search of the online encyclopedia of integer sequences hasn't revealed any obvious candidates. Another way to phrase the question:
Let $\rho = (1 2 \ldots n)\in S_n$ . Say that two permutations $\sigma, \tau\in S_n$ are equivalent if $\sigma = \rho^{-k}\tau\rho^k$ for some $k$ . How many equivalence classes contain order $n$ permutations?","['combinatorial-geometry', 'combinatorics', 'algebraic-combinatorics']"
2986901,How to analyze $(-1)^{\left \lfloor n\theta \right \rfloor}$ (in which $\theta$ is an irrational number)?,"Let $\theta = \frac{\sqrt{5}-1}{2}$ . Define $a_{n}=(-1)^{\left \lfloor n\theta \right \rfloor}$ . Please judge whether $S_{n}=\sum_{k=1}^{n} a_{k}$ is unbounded. I tried to relate $\left\lfloor n\theta \right\rfloor$ to $\left\lfloor\theta^n\right\rfloor$ , because $\theta^n$ can be written in the form "" $x_{n}\theta + y_{n}$ "" in which $x_{n}$ and $y_{n}$ are related to Fibonacci sequence and $\left \lfloor \theta^n \right \rfloor$ is $0$ . But in this why I can only analyze some of the $a_{n}$ . Any ideas for help?","['number-theory', 'irrational-numbers', 'sequences-and-series', 'real-analysis']"
2986911,Determining if a differential equation has unique solution,"Our teacher asked a seemingly simple question: What is the value of $f(2)~$ if $~f( f(x) ) = 16x-15~$ ? I found a solution assuming $f(x)$ in the form of $ax+b$ , but how do I show that $f(x)$ must be in that form? I thought of taking derivative of both sides and reached $$f'(f(x)) = \frac{16}{ f'(x) }$$ How can I show that this function has a unique solution or it doesn't have a unique solution?","['functional-equations', 'ordinary-differential-equations']"
2986912,Asymptotic estimation of $A_n$,"Let $A_n$ represent the number of integers that can be written as the product of two element of $[[1,n]]$ . I am looking for an asymptotic estimation of $A_n$ . First, I think it’s a good start to look at the exponent $\alpha$ such that : $$A_n = o(n^\alpha)$$ I think we have : $2 \leq \alpha $ . To prove this lower bound we use the fact that the number of primer numbers $\leq n$ is about $ \frac{n}{\log n}$ .
Hence we have the trivial lower bound (assuming $n$ is big enough) : $$ \binom{ E(\frac{n}{\log n})}{2} = o(n^2)$$ Now is it possible to get a good asymptotic for $A_n$ and not just this lower bound ? 
Is what I’ve done so far correct ? Thank you !","['number-theory', 'asymptotics', 'real-analysis', 'calculus', 'combinatorics']"
2986923,First Derivative Test for inflection points,"Mathworld, ""First Derivative Test"" states: Suppose $f(x)$ is continuous at a stationary point $x_0$ . ... If $f'(x)$ has the same sign on an open interval extending left from $x_0$ and on an open interval extending right from $x_0$ , then $f(x)$ has an inflection point at $x_0$ . I am however having trouble proving the above claim and I suspect that without more conditions, it is false. My questions: If the above claim is true, how do I prove it? If false, how do I strengthen the assumptions so that it becomes true?","['derivatives', 'real-analysis']"
2986934,Can this vague feeling about entropy be made precise?,"Let $I$ denote the unit interval and $\mu$ be the Lebesgue measure.
Let $S:I\to I$ be the map defined as $S(x)=2x \pmod{1}$ . Then it is known that for any measurable subset $A$ of $I$ we have $$
\lim_{n\to \infty} \mu(S^{-n}A\cap B) = \mu(A)\mu(B)
$$ So if we fix a subset $A$ , we see that $A$ gets ""mixed out"" in the interval by backwards iterates of $S$ . I would like to say that $A$ had a certain entropy in the beginning, but as $n$ increases the entropy of $S^{-n}(A)$ increases. It's like what we read in not-so-rigorous thermodynamics: Start with a box with a partition. On one side of the partition is a gas, and the other side is vacuum. Once the partition is removed the gas takes up all the space.  The entropy of the gas (whatever that means) increases in the process. So my question is : Is there a rigorous formulation of the the notion of entropy I was trying to hint at in my example? Also, can the notions of ergodicity and mixing be recast in the language of entropy? Thank you.","['measure-theory', 'ergodic-theory', 'entropy']"
2986941,How to solve a differential equation after integration.,"The question is: Find the general solution of the differential equation $$\frac{dy}{dx}=2y+y^{3}$$ I have integrated by partial fractions to get to the following result: $$\frac{1}{2}ln(y)-\frac{1}{4}ln(y^{2}+2)=x+C_{1}$$ I now need to solve it for y. I have put this question into an online calculator so I know the answer is: $$y=-\frac{{{i\sqrt{2}}
e^{{\frac{1}{2}}(C_{1}+4x)}}}{\sqrt{((e^{C_1+4x})-1)}}$$ and $$y=\frac{{{i\sqrt{2}}
e^{{\frac{1}{2}}(C_{1}+4x)}}}{\sqrt{((e^{C_1+4x})-1)}}$$ I don't understand how to solve $$\frac{1}{2}ln(y)-\frac{1}{4}ln(y^{2}+2)=x+C_{1}$$ to get those answers... I tried using the quadratic formula and could only get it partially so I am either doing something accidentally wrong, or the quadratic formula is the wrong approach. If anyone could show me the steps of how to solve this it would be wonderful, this is driving me crazy.","['integration', 'ordinary-differential-equations']"
2986963,Intuition non-measurability of Vitali-Set,"While Vitali's proof that Vitali sets are not Lebesgue measurable is easy to understand, it feels quite magical for me. Hence, this post. When I heard of it in a lecture yesterday, I was superbly fascinated - it is such a beautiful proof! However, as mentioned, it feels like magic. Why should - intuitively - the Vitali set not be measurable? What makes it that ""bad""? I understand that translation-invariance is the main problem. I also understand that Axiom of Choice can be difficult to talk about. I too understand that the Vitali set is quite pathological. So this is a somewhat vague question. But I still hope there are chances for deeper intuition, as I rarely believe in ""magic"" in mathematics but instead always in clearer intuition at a deeper level.","['measure-theory', 'lebesgue-measure', 'intuition']"
2986972,Showing lim xn = c,"Can anybody help with with (3) My Solution for (1) and (2) Put $g(x):=f(x)-x$ , which is still continuous on $[a,b]$ and differentiable on $(a,b)$ . Observe that $x\in [a,b]$ is a fixed point for $f$ iff $g(x)=0$ . Now if $u<v$ are two distinct fixed points we can apply Rolle's theorem (since $g$ is differentiable on $(u,v)\subseteq (a,b)$ ) and find some $\xi\in (u,v)$ such that $g'(\xi)=0$ , i.e. $f'(\xi)=1$ , contradiction. So there is at most one fixed point. The fact that at least one exists is very well-known: if $g(x)\neq 0$ for any $x\in [a,b]$ then $g(x)$ (by continuity) has always the same sign, but $f(a)\ge a$ and $f(b)\le b$ , so $g(a)\ge 0$ and $g(b)\le 0$ , contradiction. Thus for some $x$ we have $g(x)=0$ and $x$ is the desired fixed point.","['limits', 'fixed-points', 'real-analysis']"
2986975,Simple autonomous ODE perturbed by a constant coefficient,"Consider the ODEs \begin{equation}
y'=f(y), y(0)=y_0
\end{equation} and \begin{equation}
y'=a f(y), y(0)=y_0
\end{equation} where $f$ is Lipschitz. Let $y_1(t)$ ( $y_2(t)$ ) be the unique solution of the first (second) ODE.
Can I conclude that $y_1(at)=y_2(t)$ ? My attempt . I should verify that \begin{align}
\frac{d y_1(at)}{dt} = a f(y_1(at))
\end{align} that is true if and only if \begin{align}
\frac{d y_1(at)}{d(at)} = f(y_1(at))
\end{align} that is true if and only if \begin{align}
\frac{d y_1(x)}{d(x)} = f(y_1(x))
\end{align} which is true because $y_1$ is a solution of the first ODE. Is this correct?","['ordinary-differential-equations', 'real-analysis']"
2986978,Showing that the difference of Chi squared random variables follows a chi squared distribution,"how can I show that given $ \textbf{y} =(y_1,y_2)^T \sim  N (\textbf {0} ,\Sigma ) $ $$(\textbf{y}^t\Sigma^{-1}\textbf{y} - \frac{y_1^2}{\sigma_{11}}) \sim \chi^2_1$$ where $ \Sigma =(\sigma_{i,j})$ I have tried considering moment generating functions.
Is it true that $\textbf{y}^t\Sigma^{-1}\textbf{y}$ is chi squared with 2 degrees of freedom and that $ \frac{y_1^2}{\sigma_{11}}$ is chi squared with 1 df? Thanks for your help","['chi-squared', 'statistics']"
2987034,Discussion on problem 4.17.b in Apostol's analytic number theory,"Given an integer $n>1$ with two factorization $n=\prod\limits_{i=1}^rp_i$ and $n=\prod\limits_{i=1}^tq_i$ where the $p_i$ 's are primes(not necessarily distinct) and the $q_i$ 's are arbitrary integers $>1$ . If $\alpha\ge1$ , then we can prove that $$\sum_{i=1}^rp_i^\alpha\le\sum_{i=1}^tq_i^\alpha$$ The problem 4.17.b in Apostol's analytic number theory says: Obtain a corresponding inequality relating these sums if $0\le\alpha<1$ . Let $f(\alpha)=\sum\limits_{i=1}^rp_i^\alpha-\sum\limits_{i=1}^tq_i^\alpha$ . Clearly $r\ge t$ . If $r=t$ , then $f(\alpha)=0$ . Also $r>t$ gives $f(0)>0$ and $f(1)<0$ . This means there is a unique $\alpha_0$ such that $f(\alpha)>0$ for $\alpha<\alpha_0$ and $f(\alpha)<0$ if $\alpha>\alpha_0$ and $f(\alpha_0)=0$ . Now my question is if this is the answer that is required by the author or yet we should improve the solution to for example find a formula for $\alpha_0$ and other, as the author says ""Obtain a corresponding inequality""? Thanks.","['analytic-number-theory', 'number-theory']"
2987035,Which function (s) is(are) the k-th derivative of itself?,"As I attempted to solve for a function that is the $k$ -th derivative of itself where $k$ is some natural number, and generalise the result, I realised it gets very hard to prove anything. For $k= 1$ , apart from trivial corner cases, it's the exponential function with any constant multiplied. For $k=2$ , along with the exponential function and it's reciprocal, it works for their sum along with any mixture by constants multiplied... For any $k$ , I guessed it should be of the form $e^{x w}$ where w is the $k$ -th root of unity. And we can add all these $k$ terms for each root with constants multiplied. While it is easy to verify it's satisfying the condition, it is beyond my scope to prove this is the only family of functions that satisfy it. I am hoping for some interesting math at work here, and want to know if this is an already established fact. Also, how do I prove anything here?","['calculus', 'derivatives', 'ordinary-differential-equations']"
2987223,"Prove that [0,1] is connected.","I wrote a proof, and I just wanted to verify if it is correct. Proof : Suppose not. Then, $[0,1] = U\cup V$ for $U$ and $V$ open in $[0,1]$ such that $U\cap V = \emptyset$ . Let $0\in U$ . Consider $s= sup\{t \mid [0,t] \subset U\}$ . Clearly, $s\in[0,1]$ . If $s\in U$ , then since $U$ is open, $\exists \epsilon >0$ such that $B(s;\epsilon) \subset U$ . But then $s+\epsilon \in U$ , and $s+\epsilon > s$ , contradicting the definition of $s$ . So, $s \notin U$ . Then, $s\in V$ . But again, since $V$ is open, $\exists \epsilon >0$ such that $B(s;\epsilon)\subset V$ . But, since $s$ is the least upper bound of $U$ , for this $\epsilon$ , $\exists s_{1} \in U$ such that $s_{1} \in B(s;\epsilon)$ . Then, $s_{1} \in U\cap V$ , which contradicts the fact that $U$ and $V$ are disjoint. So, our assumption is wrong and $[0,1]$ is connected.","['general-topology', 'proof-verification']"
2987230,Most powerful hypothesis test for given discrete distribution,"I have two discrete distributions. The second one is a simple distribution where all values from 1 to 6 (think rolling dice) have equal probability (so, that must be $\frac{1}{6}$ ). The first one is defined like this: $f(6) = 0.18$ , $f(5) = 0.14$ , $\ f(4) = f(3) = f(2) = f(1) = 0.17$ . There are two observations. I need to find the most powerful test for a given significance level (0.0196, so this clearly hints at something like getting two 5s in a row, since $f(5)^2 = 0.0196$ ). I only know how to apply the Neyman-Pearson Lemma. First I need to get the likelihood ratio. I can't even do this because I don't understand how to construct the joint probability mass function for the first distribution since it's not a closed-form expression. Maybe it's supposed to be a sort of a multinomial distribution? But the number of observations (2) is less than the number of possible values (6), so how would this work... Then, I need to construct a test based on this likelihood ratio: $$
\phi(x_1, x_2) = 
\begin{cases}
1, \lambda(x_1, x_2) > c \\ 0, \lambda(x_1, x_2) < c \\ \alpha, \lambda(x_1, x_2) = c
\end{cases}
$$ And I do not have a good grasp on how to derive $\alpha$ and $c$ . For $c$ I need to construct an inequality based on the likelihood ratio and use the significance level somehow, but I don't really understand how it's supposed to be done.","['statistics', 'probability-distributions', 'hypothesis-testing']"
2987251,Deducing the nature of critical points from implicit solution,"I have the system of ODE $$x' = x(2y-1) \\ y' = y - x^2 -y^2$$ I am interested in the behavior around the critical point $(-1/2,1/2)$ . The linearization has pure imaginary eigenvalues so I would like to know if $(-1/2,1/2)$ is a spiral point or a center. First I noticed that $$\frac{dy}{dx} = \frac{y - x^2 - y^2}{-x+2xy}$$ or rewriting it, $$x^2 + y^2 - y + (-x+2xy)\frac{dy}{dx}=0$$ is an exact ODE and has solutions given implicitly by $$-xy + xy^2 + \frac{x^3}{3} = c$$ I would think that since I have now solved the ODE I should be able to determine the precise nature of the critical point $(-1/2,1/2)$ but I am at a loss of what to do. Any hints would be much appreciated. I know that I could try to use a Lyapunov function but this seems like it would be overkill since I already solved the system.",['ordinary-differential-equations']
2987374,If $f:M \to N$ is a local homeomorphism between manifolds and closed then the map $f$ is proper?,"A friend of mine asked me the following question: How do I prove that a local homeomorphism $f:M \to N$ between manifolds is closed iff it is proper? It is well known and easy to prove that if $f$ is proper, then f is closed, but I'm not able to prove the converse and, being honest, I am not sure it is really true, but I also wasn't able to provide a counterexample. Question: How do I prove that if $f$ is closed, then it is proper?","['manifolds', 'general-topology']"
2987400,On the size of a semigroup generated by 5x5 matrices,"I was working on a problem with a friend where we were given two 5x5 matrices $A$ and $B$ with entries in $\{0, 1, -1\}$ , which must generate a semigroup (under matrix multiplication) of order $>800$ . On the theory side of things, the principle is that we want $$
f(i_1, i_2, i_3, \cdots) = A^{i_1}B^{i_2}A^{i_3}\cdots = \prod_{i=1}^\infty A^{i_{2n-1}}A^{i_{2n}}
$$ to have finite range, mapping $\mathbb{N}^\mathbb{N}\to \mathbb{M}_{5\times 5}(\mathbb Z)$ (there is no restriction on the entries of matrices in the semigroup, only on the entries of the generators $A$ and $B$ ). We tried toying arround with the ideas of nilpotent matrices, idempotent matrices, relationships between $A$ and $B$ , but couldn't come up with any explicit theory or any guiding principles for how we should choose $A$ and $B$ . There are some minor facts we know. Clearly, $\det(A),\det(B)\in\{0,1,-1\}$ otherwise there will be an infinite number of distinct powers $A^n$ and $B^n$ . Also, there clearly need to be a finite number of powers of $A$ and $B$ , so they're either nilpotent, eventually idempotent, or something else, but then we're not sure what to do to check combinations of $A$ and $B$ . EDIT: As it turns out, the only reason our code below terminated for those $A$ and $B$ was because of an integer overflow error. As far as we can tell, it seems like those $A$ and $B$ actually generate an infinitely large semigroup, though I haven't rigorously proven it. In this section, the semigroup generated by $A$ and $B$ is NOT finite. Then we plugged in the following $A$ and $B$ into a python program that calculated the size of the semigroup generated by them: $$
A = \left[\begin{matrix}
1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & -1 & 0 & -1 & 0 \\
0 & 0 & 1 & -1 & 1 \\
0 & 0 & 0 & 0 & 0
\end{matrix}\right]\quad B = \left[\begin{matrix}
0 & -1 & 0 & -1 & 0 \\
-1 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & -1 \\
1 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0
\end{matrix}\right]
$$ I'm not sure how he came up with these bad boys, he didn't really have a theory for why he chose them, but apparently they generate a semigroup with 12427 elements. Now, we can observe a few things about these matrices: $A^4 = A$ $B^3 = B^4$ (so $B^3 = B^4 = B^5 = \cdots$ ) $(AB)^3 = 0$ $(BA)^4 = 0$ You can keep going generating interesting properties of these two matrices, but bottom line we're not sure why in general there are only a finite number of products of $A$ and $B$ . It seems like you'd have to check a bunch of different edge cases. For example, we may know the nilpotency/idempotency of $A^i B^j$ for $i < 4, j < 3$ , but how would you determine that $$
f(i_1, j_1, i_2, j_2, i_3, j_3, ...)
$$ where $(i_k, j_k)_{k=1}^\infty \in \{1, 2, 3\}\times\{1, 2\}$ is an arbitrary sequence, has a finite range? I suppose you could check something how, if you restrict the sequences $(i_k, j_k)$ such that a specific pair $(i_{k'}, j_{k'})$ doesn't repeat more than the nilpotency/idempotency of $A^{i_{k'}}B^{j_{k'}}$ , then there's only a finite number of sequences you can choose, but this seems like a really difficult/roundabout way of proving this. Also, this doesn't give any insight into how you choose $A$ and $B$ . Can anyone provide any insight into this? Why do the matrices $A$ and $B$ work, and how would you choose other $A$ and $B$ in general? EDIT 2: I've determined the following identity involving $A$ and $B$ . Let $Z = B^2A^2BA$ , and $C = B^3A^2$ . Then $ZC^k$ is zero except the middle row (third row) is $$
\text{Row}_3(ZC^k) = \left[\begin{matrix}-(-2)^{k+1} & (-2)^k & -(-2)^{k+1} & -(-2)^k & -(-2)^{k+1}\end{matrix}\right]
$$ which is unbounded. Of course, there are probably other simpler relationships, this is just the first one we found via computer. EDIT 3: Another friend showed us a different approach. We can fairly easily brute force search for $A, B\in \mathbb{M}_{2\times 2}(\{0, 1, -1\})$ (there are only 6561 = (3^4)^2 choices) such that the semigroup generated by $A$ and $B$ is finite, then extend these to $5\times 5$ matrices in block format: $$
A_{5\times 5} = \left[\begin{matrix}A & O \\ O & I\end{matrix}\right],\qquad B_{5\times 5} = \left[\begin{matrix}B & O \\ O & I\end{matrix}\right]
$$ Though this works to generate such a semigroup, my problem with it is that it doesn't give us any insight into the choice of $A$ and $B$ either; it still requires us to brute force search. If, for example, we were forced to find $A$ and $B$ , $5\times 5$ , that generate a semigroup with order greater than any finite semigroup of $2\times 2$ or $3\times 3$ matrices, we would still be lost. Hence, I'm still curious about the theory behind choosing $A$ and $B$ .","['group-theory', 'semigroups', 'linear-algebra']"
