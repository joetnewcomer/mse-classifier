question_id,title,body,tags
3214877,How to prove that two conditional expectations are equivalent?,"I have the following question: what is claimed when one say that two conditional expectations, with respect to different sigma algebras are equivalent? That is to verify $E[1_F|\mathcal {F } ]=E[1_F|\mathcal {G}]$ do I need to show that $E[1_F|\mathcal {F }]$ is a version of $E[1_F|\mathcal {G} ]$ and then that $E[1_F|\mathcal {G } ]$ is a version of $E[1_F|\mathcal {F }]$ ? Or is it sufficient to show just that one of them is a version of the other? The specific contex is that in a proof it is claimed that $E[1_F|\sigma(X_n)]=E[1_F|\sigma(X_1,X_2,...X_n)]$ and to verify it the author sets about to show that $E[1_F|\sigma(X_n)]$ is a version of $E[1_F|\sigma(X_1,X_2,...X_n)]$ but not the converse. Why is this sufficient? Would it be different for the cases $E[X|\mathcal {F}]=E[Y|\mathcal {F}]$ and $E[X|\mathcal {F}]=E[Y|\mathcal {G}]$ (for $X \neq Y $ )? Thanks in advance!","['conditional-expectation', 'probability-theory']"
3214933,prove problem 12.1 in a probabilistic theory of pattern recognition,"I try to prove Problem 12.1 in A Probabilistic Theory of Pattern Recognition by Luc Devroye. I follow the hint provided in the book. If $Z$ is non-negative r.v. satisfying $P\{Z > t \} \leq c e^{-2 nt^2}$ , then $E\{Z^2\}\leq \frac{ \log(ce)}{2n}$ Hint: $E[Z^2] = \int_0^{\infty} P\{Z^2>t\}dt$ $\int_0^\infty=\int_0^u+\int_u^\infty$ bound the first integral by $u$ , and bound the second integral by exponential inequality Fine u minimizing the upper bound When I follow the second step, I try to break $P\{Z^2>t\}$ into $P\{Z>\sqrt{t}\}+P\{Z<-\sqrt{t}\}$ . Then, $P\{Z<-\sqrt{t}\}$ should be zero.Thus, $P\{Z \leq \sqrt{t} \} \leq ce^{-2nt}$ . Finally, I get $E[Z^2] \leq \frac{c}{2n}$ . It doesn't match the right result. Where should I correct in order to achieve the right way?","['statistics', 'probability', 'inequality']"
3214962,Multiplication in Deligne cohomology: explicit formula for p = q= 1,"In the very beginning of [1] the geometric meaning of Deligne cohomology $H^q(X, \mathbb{Z}(p))_D$ and multiplicative structure on it is being discussed. In particular, it is not hard to see that $H^q(X, \mathbb{Z}(1))$ can be canonically identified with $H^{q-1}(X, \mathcal{O}^{\times}_X)$ . The group $H^2(X, \mathbb{Z}(2))_D$ is identified with the group of holomorphic rank $1$ bundles with holomorphic connection (group structure is given by tensor product) The $\cup$ -multiplication gives us a map $$
H^1(X, \mathbb{Z}(1))_D \otimes H^1(X, \mathbb{Z}(1))_D = H^0(X, \mathcal{O}^{\times}_X) \otimes H^0(X, \mathcal{O}^{\times}_X) \to H^2(X, \mathbb{Z}(2))_D
$$ In other words, given two nowhere vanishing holomorphic functions $f$ and $g$ on $X$ we obtain a holomorphic line bundle with holomorphic connection on $X$ . Though in [1] the explicit formula for this in terms of Čhech cocycles is given, I am looking for another description of the same operation. First of all, observe that each pair of functions $f, g \in H^0(X, \mathcal{O}^{\times}_X)$ define a holomorphic map $F_{f,g} \colon X \to (\mathbb{C}^{\times})^2$ . Following Esnault and Viehweg,  denote the resulting line bundle with holomorphic connection $f \cup g$ by $r(f, g)$ . Then it seems clear from functoriality reasons that $$r(f, g) = F_{f,g}^*r(z,w),$$ where $z$ and $w$ are coordinate functions on $\mathbb{C}^{\times}\times \mathbb{C}^{\times}$ . Thus, I'd be happy to understand, what $r(z, w)$ is. Since $(\mathbb{C}^{\times})^2$ is a product of two Stein manifolds, there are no non-trivial holomorphic line bundles. Therefore, the only ''interesting'' part of $r(z,w)$ is the holomorphic connection. Any holomorphic connection on trivial bundle is given by $\nabla = d + \eta$ , where $\eta$ is a holomorphic $1$ -form. So my questions are: What is this $1$ -form $\eta$ on $\mathbb{C}^{\times} \times \mathbb{C}^{\times}$ ? It seems to me, that $\frac{dz}{z} - \frac{dw}{w}$ would be nice (at least, if this is the case, it satisfies the properties of $r(f, g)$ given in [1]), however I'm not able do deduce this explicitly form Esnault-Viehweg formulas. From my speculations it follows that the underlying line bundle for any $r(f,g)$ is trivial. Is this at least true? If not, than where is my mistake? Thank you for any comments! [1] -- H. Esnault, E. Viehweg. Deligne-Beilinson cohomology. in: Beilinson's Conjectures on Special Values of L-Functions ( Ed.: Rapoport, Schappacher, Schneider ). Perspectives in Math. 4, Academic Press (1988) 43 - 91 
( http://page.mi.fu-berlin.de/esnault/preprints/ec/deligne_beilinson.pdf )","['complex-analysis', 'complex-geometry', 'holomorphic-bundles', 'algebraic-geometry']"
3215003,Tangent bundle of complex torus is trivial,I've seen this post and I've been wondering if it's also true in the complex case. So is the holomorphic tangent bundle of the complex torus also trivial? And can one adapt the proof of the real case? Thanks in advance!,"['complex-geometry', 'tangent-bundle', 'geometry']"
3215023,Are the following statements equivalent to the parallel postulate?,"In one of my Elementary Geometry previous exams, one of the questions was the following: Study if the following statements are equivalent to the paralellism axiom: $(i)$ Any three straight lines have a common transversal $(ii)$ Any four points are in the interior of a triangle I am absolutely clueless about this. Definitely, assuming Playfair's axiom (or some equivalent form, like the right angle axiom) we should be able to prove $(ii)$ (after some work). Is the converse true? It seems pretty odd and unrelated to parallel lines. How should we proceed to prove or disprove this claim? I would really appreciate a detailed answer, for I am learning just from scratch (I've had absolutely no previous experience with this kind of topics). EDIT : I am supposed to work in a Hilbert plane without assuming or denying the Archimedes' axiom, nor the circle-circle intersection property, nor any other additional assumptions, like the existence of limiting parallel lines for any line and any point outside of it.","['euclidean-geometry', 'noneuclidean-geometry', 'axioms', 'geometry', 'axiomatic-geometry']"
3215029,Why to choose to work with a functional instead of a function?,"Why to choose to work with a functional instead of a function? Notice in a function you evaluate points and in functional you evaluate functions. and why a linear functional is important in general ? A functional $\phi(f)$ is linear if the domain of its existence together with the functions $f(x)$ and $\psi(x)$ contain the function $af(x)+b\psi(x)$ and if the equality $\phi(af+b\psi)=a\phi(x)+b\phi(\psi)$ , and $a,b\in\mathbb R$ holds.","['vector-spaces', 'analysis', 'functions', 'functional-analysis', 'linear-transformations']"
3215039,"Is the embedding of $W^{2,p}$ onto $C^1(\overline{I})$ compact?","We know that when $I$ is a bounded interval and $1<p\leq \infty$ that the injection $W^{1,p}\subset C(\overline{I})$ is compact. The proof of this fact uses the Arzela-Ascoli theorem on the unit ball $\mathcal{H}$ of $W^{1,p}(I)$ . Is it true that the embedding of $W^{2,p}$ onto $C^1(\overline{I})$ is also compact? Please Comment on This Part Suppose that $u\in W^{2,p}$ then $u'\in W^{1,p}$ and thus $u'\in C(\overline{I})$ and therefore $u\in C^1(\overline{I})$ . Now let $\mathcal{H}$ be the unit
ball in $W^{2,p}$ . Then $\mathcal{H}$ is equicontinuous since for all $u\in \mathcal{H}$ , $$|u(x)-u(y)|=\left|\int_{y}^xu'(t)dt\right|\leq \|u'\|_p|x-y|^{1/p'}\leq 
|x-y|^{1/p'}.$$ Hence by Ascoli-Arzela, $\mathcal{H}$ has compact closure in $C(\overline{I})$ .
Since $\mathcal{H}\subset C^1(\overline{I})\subset C(\overline{I})$ then $\mathcal{H}$ has compact closure in $C^1(I).$ Is there anything wrong with the argument?","['sobolev-spaces', 'functional-analysis']"
3215049,Show $\lim_{h \to \ 0} \frac{f(x + 2h) - 2f(x+h) + f(x)}{h^{2}} = f''(x)$ Proof verification,"Show $$\lim_{h \to \ 0} \frac{f(x + 2h) - 2f(x+h) + f(x)}{h^{2}} = f''(x)$$ Proof: By definition: $$f'(x) = \lim_{h \to \ 0} \frac{f(x + h) - f(x)}{h}$$ Using this idea it would imply: $$1)\ \ f''(x) = \lim_{h \to \ 0} \frac{f'(x + h) - f'(x)}{h}$$ As such it is required that I find an expression for $f'(x+h)$ . This is where I'm not sure if the step I took is legitimate. An expression for $f'(x + h)$ is: $$f'(x+h) =  \lim_{h \to \ 0} \frac{f(x + 2h) - f(x + h)}{h}$$ Combining this with the definition of $f'(x)$ and inserting it into 1) you arrive at: $$\lim_{h \to \ 0} \frac{f(x + 2h) - 2f(x+h) + f(x)}{h^{2}} = f''(x)$$ As required. Concern: I feel a discomfort with this solution. Even though ""mechanically"" it worked out, if I am taking the limit as $h \rightarrow 0$ that would mean $x + 2h$ and $x + h$ both go to $x$ . But I am attempting to use the idea that $x +2h$ goes to $x + h$ . Perhaps it is a notation idea that I need to communicate better, but I feel it is larger than just that.","['limits', 'proof-verification', 'real-analysis']"
3215052,Is there a name for when we extend the codomain of a function? (sort of like the opposite of the restriction),"If I have a function $f: X \to Y$ and a subset $A \subseteq X$ then I can define the restriction $f|_{A}: A \to X$ by $f|_{A}(a) = f(a)$ for all $a \in A$ . This can be interpreted as composing with the inclusion map $\iota: A \to X$ given by $\iota(a) = a$ for all $a \in A$ since we have $f|_{A} = f \circ \iota$ , and interpreting $f|_{A}$ in this way can be quite useful (for example because the inclusion map is continuous in topology and a homomorphism in algebra, and so if $f$ is continuous/a homomorphism, so is $f|_{A}$ since it is a composition of continuous functions/homomorphisms). However what if instead I have a superset $B$ containing $Y$ ? Let's say I call my altered function $f_B^{\ast}: X \to B$ so that $f_B^{\ast}(x) = f(x)$ for all $x \in X$ . I can similarly define the inclusion map $\iota: Y \to B$ and then I have $f_B^{\ast} = \iota \circ f$ . All I am doing is increasing the codomain to include values which don't get mapped to. For example any function into the natural numbers can equally be considered a function into the integers, or the rationals, or reals, complex numbers, quaternions, and so on. I feel like this is something mathematicians do without reflecting it in their notation. However to me it seems very much dual to the restriction of a function, and I was wondering if it has a name. Thanks for taking the time to read.","['notation', 'functions', 'function-and-relation-composition', 'terminology']"
3215083,Checking if functions are differentiable,"We have two functions : $1) \;f(x,y)=\begin{cases}
\frac{xy}{x^2-y^2}&\text{when }\;|x|\neq|y|
\\0&\text{when }\;|x|=|y|
\end{cases}$ $2) \;g(x,y)=\begin{cases}
x^2\sin\frac1x+y^2&\text{when }\;(x,y)\in \{\mathbb{R} \setminus{0}\} \times  \mathbb{R}
\\y^2&\text{when }\;(x,y)\in\{0\}\times\mathbb{R}
\end{cases}$ I want to check if they are differentiable in $(0,0)$ My work so far : $1)$ Let's take sequences : $x_n=\frac1n,\;y_n=\frac2n$ . Then $ f(x_n,y_n)=\dfrac{\frac1n\cdot \frac2n}{\frac{1}{n^2}-\frac{4}{n^2}}=-\dfrac{2}{3}\nrightarrow0.$ So it's not continous at $(0,0)$ so it cannot be differentiable in that point. $2)$ Let's take sequences : $x_n=\frac{1}{\frac{\pi}{2}-\pi n},\;y_n=0$ . Then $$g(x_n,y_n)=(\frac{1}{\frac{\pi}{2}-\pi n})^2 \cdot \sin(\frac{\pi}{2}-\pi n)=(\frac{1}{\frac{\pi}{2}-\pi n})^2 \cdot \cos(n\pi)=(\frac{1}{\frac{\pi}{2}-\pi n})^2 \cdot (-1)^n.$$ And the above sequence doesn't has limit to this function is not continuous at (0,0), so it cannot be continuous. Am I thinking correctly ?","['continuity', 'calculus', 'derivatives', 'real-analysis']"
3215084,A Double Partial Derivative and Integral,"Consider the following differential equation $$\frac{\partial z}{\partial \phi} . \frac{\partial z }{\partial \mu} = 1$$ My attempt at finding the solution for the complete integral of this is $$\int \int dz\,dz = \int \int d \mu \,d\phi$$ $$\int z\,dz = \int (\mu +A) \, d\phi$$ $$\frac{z^2}{2} = \mu \phi  + A \phi + B$$ $$z = \sqrt{2\phi\mu +2A\phi +2B}$$ where $A$ and $B$ are arbitrary constants. However, referring to my professor's solution, the family of solutions should be given by $$z=A\phi +\frac{1}{A}\mu + B$$ which is obviously not in agreement with my own findings. Can anyone see where I am going wrong?","['integration', 'ordinary-differential-equations', 'multivariable-calculus', 'calculus', 'partial-differential-equations']"
3215086,How can a velocity vector be determined literally as a derivative in the calculus sense?,"I am following Tu's book on manifolds. On pages 178-179 he proves that the Lie algebra of $\mathrm{SL}(n,\mathbb{R})$ is the set of traceless real $n\times n$ matrices. Specifically he writes: Suppose $X \in T_I \mathrm{SL}(2,\mathbb{R})$ . There is a curve $\gamma$ and $\varepsilon > 0$ such that $\gamma : (-\varepsilon,\varepsilon) \to \mathrm{SL}(2,\mathbb{R})$ , $\gamma(0) = I$ and $\gamma'(0) = X$ . Being in $\mathrm{SL}(2, \mathbb{R})$ , this curve satisfies $\mathrm{det} (\gamma(t)) = 1$ . Then comes the part I cannot understand, he then says: Now differentiate both sides with respect to $t$ and evaluate at $t=0$ . On the right hand side we have $0 = \frac{d}{dt}\big|_{t=0} 1$ , and on the left hand side we have $$ \frac{d}{dt}\big|_{t=0} \mathrm{det} (\gamma(t)) = (\mathrm{det \circ \gamma})_* \left( \frac{d}{dt}\big|_{0} \right) \qquad (\dagger) $$ [...] As far as I can tell, in $(\dagger)$ he makes the literal interpretation that the tangent vector $(\mathrm{det} \circ \gamma)_{*0} \frac{d}{dt}\big|_{0}$ is the calculus derivative: $$D_{\gamma(t)} \mathrm{det} \cdot D_t \gamma $$ I cannot understand this because a calculus derivative is a real number, while the tangent vector is a derivation. Could you please explain in a pedagogical way what is going on?","['multivariable-calculus', 'lie-algebras', 'smooth-manifolds']"
3215103,Prove : Every even number can be written as the sum of an odd number and a perfect square,"Prove : Every even number can be written as the sum of an odd number and a perfect square I'm defining an even number as $2n$ , $n$ being any integer
For odd numbers, $2m + 1$ , $m$ being any integer
For perfect squares, $k^2$ , $k$ being any integer Then, $2n = 2m + 1 + k^2$ not quite sure where to go from here...
some things to note are that these numbers can be positive or negative and 
I've tried to disprove it with some simple numbers to no avail even number = perfect square + odd number $0 = 1 + (-1)$ $2 = 9 + (-7)$ $4 = 9 + (-5)$ $6 = 9 + (-3)$ $8 = 9 + (-1)$ $10 = 9 + 1$ $12 = 9 + 3$ and so on so it seems like it holds up","['proof-writing', 'discrete-mathematics']"
3215107,"Density of a class of function in $L^2(\mathbb{R}, e^x\,dx)$","Consider the class of function defined by $$\mathcal{G}=\operatorname{Span}\left\{e^{-\frac{(x+a)^2}{2}}-e^{-x}e^{-\frac{(x+a)^2}{2}}\mid a\in\mathbb{R}\right\}.$$ Is $\mathcal{G}$ dense in $L^2(\mathbb{R}, e^x\,dx)$ ? Note: If in the expression of $\mathcal{G}$ , we had $e^{-(x+a)}$ instead of $e^{-x}$ , the result would be true (by Wiener's Tauberian theorem). Edit: Here is a motivation why I need a result of this sort. I am working with a differential operator $A$ on $L^2(\mathbb{R}, e^x\,dx)$ such that $$Au(x)=e^{-x}u''(x)$$ for all $u\in C^2_b(\mathbb{R})\cap L^2(\mathbb{R},e^x\,dx)$ . It can be shown that $A$ is the generator of a Markov diffusion semigroup. I need to show that for any $\alpha>0$ , $R^{-1}_\alpha(\mathcal{G})$ is dense in $L^2(\mathbb{R}, e^x\,dx)$ , where $R_\alpha=(\alpha-A)^{-1}$ is the resolvent operator.","['fourier-transform', 'fourier-analysis', 'functional-analysis']"
3215109,"Identifying the group $G = \langle x,y \space | \space xy=yx, x^4=y^2 \rangle$ from the given presentation [duplicate]","This question already has an answer here : Describing groups with given presentation? $\langle x,y\ |\ xy=yx,x^5=y^3\rangle$ and $\langle x,y\ |\ xy=yx,x^4=y^2\rangle$. (1 answer) Closed 5 years ago . I'm trying to solve a problem in my textbook which asks me to identify the groups $G_1 = \langle x,y \space | \space x^3y=y^2x^2=x^2y\rangle$ and $G_2 = \langle x,y \space | \space xy=yx, x^4=y^2 \rangle$ from the given presentations. For $G_1$ , I'm pretty sure I can say that $x^3y=x^2y \implies x^3y(x^2y)^{-1} = e \implies x = e$ (where $e$ is the identity), and then $x^3y=y^2x^2 \implies y = y^2$ as $x=e$ to get $G_1 = \langle x,y \space | \space x=y=e \rangle \cong \{e\}$ (though I'd appreciate it if you could tell me if I have this wrong). What I'm struggling with is trying to do the same sort of thing for $G_2$ - I can't see any way of getting this into a form where I can see the represented group. I'd appreciate any help you could offer.","['combinatorial-group-theory', 'group-presentation', 'group-theory']"
3215174,Convexity of the logarithmic barrier function of SOCP,"The logarithmic barrier function for second-order cone programming (SOCP) is usually $$F(x) = \log \left( x_n^2 - x_1^2-\cdots - x_{n-1}^2 \right)$$ How to prove its convexity? The Hessian is too complicated to work with, and I couldn't find any convexity-preserving rules that can be applied here either.","['second-order-cone-programming', 'convex-optimization', 'multivariable-calculus', 'hessian-matrix', 'convex-analysis']"
3215190,A 2-connected graph contains a path passing through all the odd degree vertices,"I am trying to prove the above as an exercise in the topic of connectivity. I have tried to do so using ear decompositions, as odd degree vertices may be characterized as end points of ears, but to no avail. Any recommendations are appreciated.
Thanks","['graph-theory', 'graph-connectivity', 'discrete-mathematics']"
3215196,"Suppose that $W(x_0) = 0$, and $y_1(x_0) = 0$. Show that $y_1(x)$=0 or $y_2(x) = \frac{y'_2(x_0)}{y'_1(x_0)} y_1(x)$","I know that $W(x_0)=y_1(x_0)y'_2(x_0) -y'_1(x_0)y_2(x_0) = 0$ that implies $y'_1(x_0)y_2(x_0) = 0$ , If I supposed that $y_1$ and $y_2$ are solutions of $$y'' + p(x)y' + qy=0$$ I know that $y_1$ and $y_2$ are linearly depedent, then I know that $$y_1(x)y'_2(x) -y'_1(x)y_2(x)=0$$ that implies $$y_1(x)y'_2(x) =y'_1(x)y_2(x)$$ someone could give a hint?",['ordinary-differential-equations']
3215197,Areas of tetrahedron faces in proportion to opposite solid angles?,"Is there a relationship analogous to the law of sines for triangles, but for tetrahedra? A natural generalization would be $$
a : b : c : d \;=\; \sin A : \sin B : \sin C : \sin D
$$ where $a,b,c,d$ are face areas, and $A,B,C,D$ are opposite
solid angles.
If the above does not hold, does some other similar relationship hold connecting $\{a,b,c,d\}$ to $\{A,B,C,D\}$ ?","['solid-geometry', 'solid-angle', 'geometry']"
3215202,Interesting Difference between Lebesgue and Riemann Integral,"The Riemann integral makes it so that if we have $|f| \leq |g|$ on $[0,1]$ , then integrability of $g$ does not necessarily imply the integrability of $f$ . For example, let $f = \chi_\mathbb{Q}$ , $g = 1$ . Then $g$ is integrable but $f$ is not. The idea here seems to be that (at least on spaces with finite measure), the Lebesgue integral does a better job at dealing with a lack of regularity that does not occur due to ""explosions"". Almost always (pun not intended), the Lebesgue integral fails to converge due to a blow-up (i.e $1/x$ ) or a function having tails that are too large ( $1/\sqrt{x}$ , away from the origin). What exactly is it about the Lebesgue integral that prevents singular behavior on finite measure spaces? Holder's inequality tells us that on finite measure spaces, essential boundedness is enough to guarantee us the existence of an integral, but this is in no way true for the Riemann integral. It seems that the catastrophic failure of the Riemann/Darboux integral is this idea that both upper and lower sums need to converge as the partition mesh goes to $0$ . In the case of the rationals, for any finite partition, the upper and lower sums are always $0,1$ respectively, meaning convergence does not happen. Does the Lebesgue integral avoid this by only considering a supremum (say over an increasing simple function approximation?) Edit: The more I think about this, the more I realize that the issue rests with measurability. The Riemann integral for the characteristic of the rationals does not converges because there is no coherent way to assign a Jordan content (strictly speaking it is not a measure) to this set. Specifically, the above monotonicty for converge (RHS converging implies LHS converging) occurs only when both $f,g$ are Lebesgue measurable functions . Thus, I feel that the monotonicity would be valid for the Riemann integral if we were told that both $f,g$ are ""Jordan measurable"" (whatever that might mean).","['riemann-integration', 'lebesgue-integral', 'real-analysis']"
3215228,Equivalent operator norm on dense subset,"Let $X$ and $Y$ be normed vector spaces and let $X_{0}\subset X$ be a dense subspace. Further, let $T:X\longrightarrow Y$ be a bounded, linear operator. Prove that $||T||_{L(X,Y)}:=\underset{x\in X,\\||x||=1}{sup}||Tx||= \underset{x\in X_{0},\\||x||=1}{sup}||Tx||$ The strategy is to prove „ $\leq$ “ in both directions, whereas $||T||\geq \underset{x\in X_{0},\\||x||=1}{sup}||Tx| $ is clear because $X_{0}\subset X$ . However, I do not manage to show the reversed inequality, namely $||T||\leq \underset{x\in X_{0},\\||x||=1}{sup}||Tx||$ , because I don‘t see how to use the density property: for all $\epsilon>0$ and for all $x\in X$ there exists $y\in X_{0}$ such that $||x-y||<\epsilon$ . Can anyone help me out?","['normed-spaces', 'functional-analysis']"
3215251,Find solution to second order linear differential equation in 3 parts.,"Say I have this equation: $$y'' - 4y' + 4y = x - \sin{x}$$ My process is to:
- find complementary solution
- find the particular solution in two parts
- add them together to find general solution. Is this process and answer correct? complementary solution find auxiliary: $$r^2 - 4r + 4 = 0$$ $$(r-2)(r-2) = 0$$ so complementary is: $y_c = c_1e^{2x} + c_2xe^{2x}$ part 1 of particular: $$y_p1 = y'' - 4y' + 4y = x$$ the particular is in the form: $Ax + B$ $$y'p = A$$ $$y''p = 0$$ so via substitution: $$- 4A + 4Ax + 4B = x$$ setting coefficients equal: $-4A = 1$ and $A = \frac{1}{4}$ and $B = \frac{1}{4}$ so this part of this particular solution is $$y_p1 = \frac{1}{4}x + \frac{1}{4}$$ the other part of the particular is this: $$y_p2 = A\sin{x} + B\cos{x}$$ $$y'_p2 = A\cos{x} - B\sin{x}$$ $$y''_p2 = -A\sin{x} - B\cos{x}$$ so substituting into $y'' - 4y' + 4y = x$ : $$-A\sin{x} - B\cos{x} - 4A\cos{x} + 4B\sin{x} + 4A\sin{x} + 4B\cos{x} = -\sin{x}$$ so for the sines: $$3A + 4B = -1$$ for the cosines: $$-3B - 4A = 0$$ solving: $$-3B = 4A$$ $$\frac{-3}{4}B = A$$ $$\frac{-9}{4}B + 4B = -1$$ $$\frac{7}{4}B = -1$$ $$B = \frac{-4}{7}$$ $$A = \frac{12}{28}$$ so summing all together: general solution $$y = c_1e^{2x} + c_2xe^{2x} + \frac{1}{4}x + \frac{1}{4} + \frac{12}{28}\sin x  -\frac{4}{7}\cos x$$ Is this right?","['proof-verification', 'ordinary-differential-equations']"
3215253,Why is the most general solution of a non-homogenous linear ODE the sum of the complementary and particular solutions?,"Take a linear, non-homogenous ODE: $$ Ay^{\prime\prime}(x) + By^\prime(x) + Cy(x) = f(x) $$ It is known that one can find the general solution to this equation by taking a solution to the following: $$ Ay_h^{\prime\prime}(x) + By_h^\prime(x) + Cy_h(x) = 0 $$ And a particular solution of the original ODE: $$ Ay_p^{\prime\prime}(x) + By_p^\prime(x) + Cy_p(x) = f(x) $$ By adding the two together, you get the following: $$ A\Big(y_p^{\prime\prime}(x) + y_c^{\prime\prime}(x)\Big) + B\Big(y_p^\prime(x) + y_c^\prime(x)\Big) + C\Big(y_p(x) + y_c(x)\Big) = f(x) $$ Thus, it is proven that a solution to the ODE is given by $y_p(x) + y_c(x)$ . Why, though, is this the most general solution? I'm aware the solution is made more general by the introduction of the general homogenous solution, but how do we know that there isn't a more comprehensive solution?",['ordinary-differential-equations']
3215295,Dual frame in Riemannian metrics.,"Suppose that we have a Riemannian metric $ds^2=Edu^2+2Fdudv+Gdv^2$ on a local coordinate neighborhood $(U;(u,v))$ prove that for the following vector fields: $$e_{1}=\frac{1}{\sqrt{E}}\frac{\partial}{\partial u},\quad e_{2}=\frac{-1}{\sqrt{EG-F^2}}\left(\frac{F}{\sqrt{E}}\frac{\partial}{\partial u}-\sqrt{E}\frac{\partial}{\partial v}\right)$$ The $1-$ forms: $$\omega_1=\sqrt{E}\left(du+\frac{F}{E}dv\right),\quad \omega_2=\sqrt{\frac{EG-F^2}{E}}dv$$ satisfies: $$\omega_i(e_k)=\delta_{ik}$$ My work: Let $p(u,v)$ a differentiable function, I think that I must show fisrtly that $\omega_1(e_1(p))=p$ , i.e. $\omega_1(e_1)=1$ the identity function. $$\omega_1\left(\tfrac{1}{\sqrt{E}}\tfrac{\partial}{\partial u}(p)\right)=\tfrac{1}{\sqrt{E}}\tfrac{\partial}{\partial u}(\omega_1(p))$$ I do this beacuse an $1-form$ $\alpha$ is such that $\alpha(fX)=f\alpha(X)$ . Then It is correct that $\omega_1(p)=\sqrt{E}\left(pdu+\frac{F}{E}pdv\right)?$ and then apply the partial derivative, my problem is that I don't know ho operate the $1$ -form. Anyone can guide me in how can I reach the result or a explicit form to operate with $\omega_1$ and $\omega_2$ ? I'm using the book ""Umehara, differential geometry of surfaces"".","['manifolds', 'differential-forms', 'differential-geometry']"
3215305,Isomorphism of Ideal tensored with affine open and restriction of ideals,"Let $f:X \rightarrow Y = \operatorname{Spec}A$ be a morphism and $Y = \bigcup U_\alpha$ where $U_\alpha = \operatorname{Spec}A_\alpha$ .
  Given the ideal $I = \{a\in A: f^*(a) = 0\}$ , show that $I \otimes _A A_\alpha \cong I_\alpha$ where $I_\alpha := I|_{U_\alpha}$ = { $a \in A_\alpha : f^*(a) = 0$ } I was proving a different problem and got stuck on this. This seems fine if the covering is a covering of principal opens, but I don't see why it should hold for an arbitrary affine open cover.","['affine-schemes', 'algebraic-geometry', 'tensor-products', 'ideals', 'commutative-algebra']"
3215328,Hanson-Wright Inequality for Symmetric Matrices,"The Hanson-Wright inequality for arbitrary $n \times n$ matrix A, and $X$ a random vector with subgaussian coordinates (of norm 1) is $$
Pr(|X^TAX-\mathbb{E}X^TAX| \geq t) \leq 2\exp \left(-c \min\left(\frac{t^2}{\|A\|_F}, \frac{t}{\|A\|} \right)\right)
$$ A proof of this follows first by decoupling and showing that equivalently we may consider $$
Pr(|X^TAX' - \mathbb{E}X^TAX'| \geq t)
$$ for i.i.d. $X,X'$ . We then establish in the case where $X,X'$ are gaussian the bound $$
\mathbb{E}\exp(\lambda X^TAX') \leq \exp(C\lambda^2\|A\|_F^2)
$$ Finally, one shows that we can replace arbitrary $X,X'$ with normally distributed counterparts while only paying a constant cost (see page 140 of Vershynin High Dimensional Probability). In particular, for $X,X'$ with subgaussian entries of norm 1, and with $g,g' \sim \mathcal{N}(0,I_n)$ , we have the bound \begin{equation}
\mathbb{E}\exp(\lambda X^TAX') \leq\mathbb{E}\exp(C\lambda g^TAg') 
\end{equation} for some constant $C$ . I'm interested in the case where $A$ is a symmetric matrix. When $X$ are gaussian, we can already assume that $A$ in $X^TAX$ is diagonal. Crucially (or so it seems to me), we don't need to decouple here. This lets one bound the moment generating function of $X^TAX$ like \begin{align*}
\mathbb{E}\exp(\lambda X^TAX) &= \prod_{i=1}^n \mathbb{E} \exp(\lambda a_iX_i^2)\\
&= \prod_{i=1}^n (1-2\lambda a_i)^{-1/2} \\
&= \prod_{i=1}^n \exp(2\lambda a_i)
\end{align*} if $\lambda < \frac{1}{4}$ , using the identity $(1-t)^{-1/2} < e^{t}$ for $t < 1/2$ . So one can replace $\|A\|_F$ with $tr(A)$ , which should give a better bound. If we want to get this result in general though, we need some result along the lines of \begin{equation}
\mathbb{E}\exp(\lambda X^TAX) \leq\mathbb{E}\exp(C\lambda g^TAg) 
\end{equation} Does such a bound hold? (I am sourcing this material from chapter 6 of Vershynin's High Dimensional Probability)","['statistics', 'probability-theory', 'probability']"
3215344,Is any connected dimension $0$ scheme affine?,Let $X$ be a connected Scheme of dimension $0$ . Is $X$ necessarily affine ? I know this is true if $X$ is Noetherian (even without assuming $X$ is connected). But what happens if $X$ is not Noetherian ?,"['algebraic-geometry', 'schemes', 'category-theory', 'commutative-algebra']"
3215365,Why is $\frac{\sin (-x)}{\cos (-x)}$ not equal to $\frac{-\sin (x)}{-\cos (x)}$,"This is the first scenario I have seen of math semi-contradicting its rules. By the negative trig identity definition, shouldn't $-\sin(x) = \sin(-x)$ ? Therefore, why is the title statement wrong?","['algebra-precalculus', 'trigonometry']"
3215370,Bounding the absolute value of a function with an integral,"I am having trouble with the following problem in analysis: Suppose that $f, f^\prime \in C([0, 1])$ . Prove that for all $x \in [0, 1]$ $$
|f(x)| \leq \int_0^1 (|f(t)| + |f^\prime (t)|) dt.
$$ Any pointers? I have tried writing this as a Riemann Sum (given arbitrary tagged partition) but am still not sure how to proceed.",['real-analysis']
3215378,"If $f(x)=f(2x)$, then how do we get the solution $f(x)=\sin ( \log _a (x))$ through computation?","In this question Does $f(x) = f(2x)$ for all real $x$, imply that $f(x)$ is a constant function? , one of the answers mentioned a counterexample that $f(x) = \sin (\log _a(x))$ is a solution, (where $a=2^{\frac{1}{2\pi}}$ ) which is albeit not continuous at $x=0 $ , but satisfies the given criteria. I am trying to derive this through differential equations, but the answer is eluding me... (Specifically, I was using $2f'(2x)=f'(x)$ , with $f'(1)=\frac{1}{ \ln(a)}$ , where $a=2^{\frac{1}{2\pi}}$ ) Can someone guide me as to how to derive this solution without guessing at it?","['calculus', 'ordinary-differential-equations']"
3215379,"Let $a_n \to 0$, then $\lim_{n \to \infty} (a_{n+1}-a_n)n $ equals to ??","Let $a_n \to 0.$ Then $$\lim_{n \to \infty} (a_{n+1}-a_n)n $$ equals to ?? I have taken a few examples and got that the limit equals to zero. It seems that the limit is zero, but how to prove it in general ? or my guess is wrong... Please provide some hint. Thank you.","['calculus', 'sequences-and-series', 'analysis', 'real-analysis']"
3215440,Reference books for advanced complex analysis.,"This semester I  got introduced to complex analysis. While starting my studies I came  across this questiom. What is a good complex analysis textbook, barring Ahlfors's? . And I exactly followed top answer. I started with Churchill and for geometric intuition I always look in needham. I enjoy the process truly. Now I want to take things ahead. I want to go to some advance level. Mainly I am looking for a book in which there is good explanation about univalent functions as I have to present seminar next semester on this topic. Suggestions will be really helpful.","['complex-analysis', 'book-recommendation', 'reference-request']"
3215525,What is $\cos\left(\frac{\arctan(x)}{3}\right)$ and $\sin\left(\frac{\arctan(x)}{3}\right)$?,I know that $$\cos(\dfrac{\pi}{3} - \arctan(x))= \dfrac{1}{2\sqrt{(1+x^2)}} + \dfrac{\sqrt{3}x}{2\sqrt{(1+x^2)}}$$ $\cos\left(\dfrac{\pi}{3} - \dfrac{\arctan(x)}{3}\right)$ = ? $\cos\left(\dfrac{\pi}{3} - \dfrac{\arctan(x)}{3}\right) = \cos\left(\dfrac{\pi}{3}\right)\cos\left(\dfrac{\arctan(x)}{3}\right) + \sin\left(\dfrac{\pi}{3}\right)\sin\left(\dfrac{\arctan(x)}{3}\right) = \dfrac{1}{2}\cos\left(\dfrac{\arctan(x)}{3}\right) + \dfrac{\sqrt{3}}{2}\sin\left(\dfrac{\arctan(x)}{3}\right)$ but I can't go further since I don't know how to solve $\sin\left(\dfrac{\arctan(x)}{3}\right)$ and $\cos\left(\dfrac{\arctan(x)}{3}\right)$ . Any suggestion?,['trigonometry']
3215548,"The meaning of ""metric preserving"" connection.","Let $g$ be a metric on a smooth manifold $M$ . In local coordinates the metric takes the form $$g=g_{ij}(dx^i\otimes dx^j+dx^j\otimes dx^i).$$ From here , a connection $\nabla$ on $M$ is said to be metric preserving if $\nabla g=0$ . But what is the quantity $\nabla g$ ?","['connections', 'smooth-manifolds', 'differential-geometry']"
3215590,let $M$ be a Hermitian matrix of order $n\times n$ with rank $k (\neq n)$,"let $M$ be a $n \times n$ matrix of rank $k (\neq n)$ if $\lambda \neq 0$ is an eigenvalue of $M$ with corresponding unit column vector $u$ . with $Mu=\lambda u$ ,then which of the following is\are true?. 1). $rank(M-\lambda uu^{*})=k-1$ 2). $rank(M-\lambda uu^{*})=k$ 3). $rank(M-\lambda uu^{*})=k+1$ 4). $(M-\lambda uu^{*})^{n}$$=M^{n}-\lambda^{n}uu^{*}$ solution I tried in the given question rank $(M-\lambda uu^{*})$$=$ rank $(M-Muu^{*})$ taking $M$ common we get rank $[M(I-uu^{*})]$ further that I  don't know how to proceed please help! Thank you",['linear-algebra']
3215591,Not fully understanding the cosine angle addition identity in a definite integral problem,"For homework in my calculus class, I'm trying to show via u substitution that the following definite integral is equal to zero: $$
\int_{0}^{2} (1-t) \cos(\pi t) \ dt
$$ Here are the u substitution parameters I used: u = 1 - t du = -dt t = 1 - u When u is 0, t = 1 When u is 2, t = -1 This is where I got stuck: $$
-\int_{1}^{-1} u \ cos(\pi(1-u)) \ du
$$ I checked the solution in the back of the book, and they had nearly the same intermediate equation (theirs lacks the leading negative sign). However, the next step confused me. I don't understand how they got from their intermediate equation to the following integral: $$
\int_{1}^{-1} u [\cos(\pi)\cos(u) - \sin(\pi)\sin(u)] \ du
$$ I expected this: $$
\int_{1}^{-1} u [\cos(\pi)\cos(\pi u) + \sin(\pi)\sin(\pi u)] \ du
$$ What step(s) am I missing in applying the cosine angle addition formula that allowed the textbook authors to arrive at their version of the integral? Textbook: OpenStax Calculus Volume 1 Section: 5.5 Exercise: 311, Page 594","['calculus', 'definite-integrals', 'trigonometry']"
3215593,Second derivative of multivariable implicit function,"Find $d^2z$ of the following function $$\frac{x}{z}=
 \ln{\frac{z}{y}}+1$$ Finding the total derivative of the left and right side we arrive at the expression: $$\frac{zdx-xdz}{z^2}=\frac{y}{z} \cdot \frac{ydz-zdy}{y^2} \iff yzdx-xydz-yzdz+z^2dy=0$$ Differentiating again this expression: $$y(x+z) d^2z=zdzdy+(zdy-xdy)dz - y^2dz^2$$ Also $dz=\frac{z(ydx+zdy)}{y(x+z)}$ so by substituting in the last equation we get: $$d^2z= - \frac{z^2(ydx-xdy)^2}{y^2(x+z)^3}$$ I have couple of questions regarding this solution. I've managed to replicate taking the first derivative from both sides of the equation and arrived at the same result. What I noticed, is that during this we do not consider $z$ as a function of $x$ and $y$ . Next when taking the second derivative I didn't get the extra term $y(x+z)d^2z = yx d^2z +z d^2z$ . This term seems to appear by differentiating all the $dz$ parts in $yzdx-xydz-yzdz+z^2dy=0$ . Perhaps it is from somewhere else that we get the $d^2z$ term but I don't know what I'm missing.","['multivariable-calculus', 'implicit-differentiation']"
3215599,Definition of surface measure and integration on submanifolds?,"I'm reading some lecture notes of Terence Tao and at one point he says Of course we give $S^{n-1}$ the usual surface measure $d\sigma$ . though I couldn't find a definition of ""the surface measure"", just a Wikipedia article about surface area . However, I remember that in our Calc III course we introduced integration on submanifolds of $\mathbb R^n$ using the Gramian matrix and I wonder whether I can obtain the surface measure from that framework? In particular, I'm wondering whether the surface measure for some $k$ -dimensional submanifold $M$ of $\mathbb R^n$ with parameterization $\varphi\colon U\subseteq\mathbb R^k\to M$ is given by $$\sigma\colon\mathcal B^n\cap M\to[0,\infty],A\mapsto\int_AdS:=\int_{\varphi^{-1}(A)}\sqrt{\det D\varphi(x)^TD\varphi(x)}d\lambda^k(x),$$ where $\lambda^k$ is the Lebesgue-Borel measure on $\mathbb R^k$ . References regarding this topic are also welcome. Comment to Boris Bilich's answer: (I'm appending this to my question instead of commenting under Boris Bilich's answer, because this comment is rather heavy on formulas). Even though your answer was actually helpful for me in that it helped me find a decent derivation of the surface measure, I think your statement about the integral being dependent on the choice of the parameterization is wrong, because I remember proving the contrary in Calc III. Consider the map $\psi(x):=\varphi(\lambda x),\lambda\neq0$ from your answer, where $\varphi$ is the parameterization from my question above. For $A=\varphi(B)=\psi(\frac1\lambda B)$ we have $\varphi^{-1}(A)=B=\lambda\psi^{-1}(A)$ and $$
D\psi(x)
=D(\varphi\circ\lambda)(x)
=D\varphi(\lambda x)\circ D\lambda(x)
=\lambda D\varphi(\lambda x).
$$ I think you already see, where this is going. Using the transformation theorem the $\lambda$ in the argument of the derivative will cancel the $\lambda$ outside the derivative: $$
\begin{align*}
&\int_{\psi^{-1}(A)}\sqrt{\det D\psi(x)^TD\psi(x)}d\lambda^k(x) \\
&=\int_{\psi^{-1}(A)}\sqrt{\det\lambda^2D\varphi(\lambda x)^TD\varphi(\lambda x)}d\lambda^k(x) \\
&=\int_{\psi^{-1}(A)}\sqrt{\det D\varphi(\lambda x)^TD\varphi(\lambda x)}\cdot\underbrace{|\det D\lambda(x)|}_{=|\lambda|^k}d\lambda^k(x) \\
&=\int_{\lambda\psi^{-1}(A)}\sqrt{\det D\varphi(x)^TD\varphi(x)}d\lambda^k(x) \\
&=\int_{\varphi^{-1}(A)}\sqrt{\det D\varphi(x)^TD\varphi(x)}d\lambda^k(x)
\end{align*}
$$","['measure-theory', 'surfaces']"
3215652,Sphere with given tangent space,"Let $S^n$ be the unit $n$ -sphere equipped with its standard metric inherited from $\mathbb{R}^{n+1}$ , let $p \in S^n$ , and let $V \subset T_pS^n$ be an $m$ -dimensional subspace of the tangent space at $p$ , where $m < n$ . Is it possible to isometrically embed $S^m$ into $S^n$ so that $T_pS^m = V$ ? This seems like it should be true, especially if one considers the case $n=2$ and $m=1$ , since given a tangent of $S^2$ at $p$ , it is easy to find a great circle through $p$ with the same tangent. For the general case I was thinking of taking an orthonormal basis of $V$ , and somehow parametrizing an $m$ -sphere using it, but this seems like an overkill. Could someone give me a hint on how to see this?","['spheres', 'riemannian-geometry', 'differential-geometry']"
3215729,"In the figure, a quarter circle, a semicircle and a circle are mutually tangent inside a square of side length $2$. Find the radius of the circle.","In the figure, a quarter circle, a semicircle and a circle are mutually tangent inside a square of side length $2$ . Find the radius of the circle. I first assumed that when a vertical line is drawn from the radius of the semicircle, that line would be tangent to the smallest circle and it would mean that the radius is $\frac{1}{4}$ , but the correct answer was $\frac{2}{9}$ . I also tried using coordinate geometry, but I got stuck because I did not know how to get the equation of the smallest circle.",['geometry']
3215732,A question about applying Birkhoff's ergodic Theorem.,"Suppose that I have the transformation $$T(x,y)=(x+a,y+b), a,b \in \mathbb{R}$$ from the 2-dimensional torus $\mathbb{R}^2/\mathbb{Z}^2$ to itself. We know that this transformation in ergodic with respect to the Lebesgue 2-d measure on the torus if and only if $a,b $ are rationally independent which means $na+mb \in \mathbb{Z}$ if and only if $n=m=0$ . Now according to Birkhoff's ergodic theorem for any function $f\in C(\mathbb{R}^2/\mathbb{Z}^2,\mathbb{R})$ we have that $$\lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}f(T^j(x,y))=\int f dm$$ for almost all $(x,y) \in \mathbb{R}^2/\mathbb{Z}^2$ , where $T^j$ are the iterates of $T$ and $m$ is the 2-d Lebesgue measure and the integral is over $[0,1]\times[0,1]$ . Let's say that we choose the function $f(x,y)=\cos (2\pi y)$ which is continuous in $[0,1]\times[0,1]$ and we choose $a\not \in \mathbb{Q}$ and $b=0$ . Then $T$ is ergodic and $T^j(x,y)=(x+ja,y)$ , so we have that $$\lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}\cos(2\pi y)=\int_{0}^{1}\int_{0}^{1} \cos (2\pi y) dydx=0$$ But this cannot be true almost everywhere since the sum on the left is equal with $\cos (2\pi y)$ . Now my Question is what am I doing wrong?","['ergodic-theory', 'analysis', 'real-analysis']"
3215746,How to prove directly that the empty set is well ordered?,"I found a post concerning this question, but I cannot understand the proof given there of the fact I'm talking about. The link is : Verification of proof that the empty set is well ordered I think I can prove indirectly that the empty set is well ordered in the following way : (1) suppose the empty set is not well ordered (2) that is, suppose it is false that every non empty subset of the empty set has a first element (3) it means there exists at least some set S such that S is a non empty subset of the empty set and S has no first element (4) which requires the first conjunct  "" S is a non empty subset of the empty set"" to be true (5) but this is impossible, for the empty set has only one subset, which is empty. However , I cannot manage to give a direct proof of the same fact. I cannot go further than this : 
(1) Let S be an arbitrary set 
(2) Assume it is true that :  S is a non empty subset of the empty set 
(3) Derive from this that : S has a first element. ... but how?","['elementary-set-theory', 'logic']"
3215774,Example of a cluster variety,"This question is basically just me asking for something to be either verified or rebutted. So I'm trying to work with cluster varieties, and no matter how much I look around, I simply am not fortunate enough to come across an example that is simple enough for me to feel I have understood things properly. Thence cometh my question. A cluster variety is defined as the spectrum of maximal ideals of the corresponding cluster algebra. The simplest cluster algebra is simply the Laurent polynomial ring in one variable, $\mathbb{F}[x,x^{-1}]$ , and since this is isomorphic to $\mathbb{F}[x,y]/(xy-1)$ , the cluster variety should simply be $$\{(x,y) \in \mathbb{F}^2 | xy = 1\} \cong \mathbb{F}^{*} .$$ Similarly, in the case of the cluster algebra associated with the quiver of two nodes joined by a single arrow, since we have cluster variables $$x_1 , x_2 , \frac{1+x_1}{x_2} , \frac{1+x_2}{x_1} , \frac{1+x_1 + x_2}{x_1 x_2},$$ the cluster algebra is $\mathbb{F} [x_1 , x_2 , \frac{1+x_1}{x_2} , \frac{1+x_2}{x_1} , \frac{1+x_1 + x_2}{x_1 x_2}] \subset \mathbb{F}[x_1^{\pm 1}, x_2^{\pm 1}]$ which is isomorphic to $\mathbb{F} [x,y,z,v,w] / (xz-y-1,yv-x-1,xyw-x-y-1)$ , and so the cluster variety is $$\{ (x,y,z,v,w) \in \mathbb{F}^5 | xz-y= 1, yv-x=1, xyw-x-y=1 \} .$$ Are these two examples accurate, or is there some silly mistake in there somewhere? Look forward to your comments.","['cluster-algebra', 'algebraic-geometry']"
3215805,Coloring a $3\times n$ board using $3$ colors,I have been reading about Combinatorial Material for a while now. I also solved a few examples. However am stuck at this one. Find the total number of ways a $3 \times n$ board can be painted using $3$ colors while making sure no cells of the same row or the same column have entirely the same color.  The answer must be computed modulo $10^9 + 7$ . I saw the solutions to $3 \times n$ using  colors but all these had the constraint that no adjacent cells were to have the same color. There's no such constraint here. So am just not sure how to do this given the center row can have a bunch of options as well. Thanks for any help.,"['graph-theory', 'combinatorics', 'coloring']"
3215821,"Integrate $\int \frac {\sin (2x)}{(\sin x+\cos x)^2}\,dx$","Integrate $$\int \frac {\sin (2x)}{(\sin x+\cos x)^2} \,dx$$ My Attempt: $$=\int \frac {\sin (2x)}{(\sin x + \cos x)^2} \,dx$$ $$=\int \frac {2\sin x \cos x}{(\sin x+ \cos x)^2} \,dx$$ Dividing the numerator and denominator by $\cos^2 x$ $$=\int \frac {2\tan x}{(1+\tan x)^2} \,dx$$","['integration', 'indefinite-integrals', 'calculus', 'trigonometric-integrals']"
3215920,Product of skew symmetric matrices,"Let $A,B$ be skew symmetric 3-dimensional real non-zero matrices. Because dimension is odd they have non-trivial one-dimensional kernels. Is it true that $AB$ is nilpotent  iff $\text{ker}(A)$ $\perp$ $\text{ker}(B)$ ? How to prove it? The example illustrating one direction of the implication: $\begin{bmatrix} 0 & 1 & 2 \\ -1 & 0 & 4 \\ -2 & -4 & 0 \end{bmatrix}\begin{bmatrix} 0 & -2 & 3 \\ 2 & 0 & -1 \\ -3 & 1 & 0 \end{bmatrix}=\begin{bmatrix} -4 & 2 & -1 \\ -12 & 6 & -3 \\ -8 & 4 & -2 \end{bmatrix}=C $ we have here $C^2=0$ .","['matrices', 'linear-algebra']"
3215945,Approximation of subset of Hilbert space by finite-dimensional functions,"I cannot come up with an answer to the following problem, which I came across: Let $H$ be a separable, real Hilbert space with ONB $\{e_n\}_{n \in \mathbb{N}}$ and let $U \subseteq H$ be open (if it helps, we may instead consider $U$ closed). Then it is commonly known that there exist continuous, non-negative functions $f_n: H \to \mathbb{R}$ , $n \in \mathbb{N},$ such that $f_n$ converges monotonically increasing pointwise to $\mathbf{1}_U$ . My question is: Can I choose these functions such that $f_n \in C_b(\mathbb{R}^d)$ for each $n$ , where $d=d_n$ is of course allowed to depend on $n$ ? In saying so, I identify functions $f \ \in C_b(\mathbb{R}^d)$ with the function $f \circ P_d:H \to \mathbb{R},$ where $P_d$ denotes the projection on the linear span of $e_1,...e_d$ . I have tried a number of fruitless attempts. Roughly speaking, most of them failed due to the fact that $\underset{n \in \mathbb{N}}{\cup}span(e_1,...,e_n) \neq H$ . I would appreciate any reasonable input on this! Thank you in advance!","['hilbert-spaces', 'functional-analysis', 'analysis']"
3215973,Stability of equilibrium of a nonlinear system of ODE's,"Suppose we have the nonlinear system of ODE's $$\begin{cases}
\dot{x_1} = -\beta x_1 x_2 \\
\dot{x_2} = \beta x_1 x_2 - \gamma x_2
\end{cases}
$$ Where we take $\beta, \gamma > 0$ arbitrary for now. In particular I am interested in the equilibrium point $(x_1, x_2) = (1, 0)$ . I first linearized the system around the point $(1, 0)$ by using the Jacobian $$J(x_1, x_2) = \begin{pmatrix} -\beta x_2 & -\beta x_1 \\ \beta x_2 & \beta x_1 - \gamma \end{pmatrix}.$$ So the linearized system around $(1, 0)$ is given by $$\begin{pmatrix} \dot{x_1} \\ \dot{x_2} \end{pmatrix} = \begin{pmatrix} 0 & -\beta \\ 0 & \beta - \gamma \end{pmatrix}\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}.$$ Hence, it follows we have eigenvalues $\lambda_1 = 0$ and $\lambda_2 = \beta - \gamma$ . Now if $\beta > \gamma$ we know that the nonlinear system is unstable. However, if we let $\beta \leq \gamma$ we can not determine the stability of the nonlinear system by linearization. The system seems relatively simple and I would expect the equilibrium to be stable or even asymptotically stable in the case $\beta \leq \gamma$ , but how would one prove this when linearization fails to provide a conclusive answer? Or did I make some error in my reasoning?","['systems-of-equations', 'ordinary-differential-equations', 'stability-in-odes', 'stability-theory', 'nonlinear-system']"
3215981,"Motivate $P\{X_0\in B_0,...,X_n\in B_n,X_0+...+X_{n+1}\in B\}=\int_{B_0\times...\times B_n}P_{n+1}(\{y:y+(x_0+...+x_n)\in B\})P_0(dx_0)... P_n(dx_n)$","Suppose we have independent random variables $X_0,X_1,\ldots,X_{n+1 }$ with distributions $P_0,\ldots,P_{n+1 } $ and let $S_k $ denote the sum $X_0+\cdots+X_k $ . I would like to motivate the following calculation: $$P\{X_0 \in B_0,\ldots, X_n \in B_n,S_{n+1}\in B \} =\int _ {B_0\times\cdots\times B_n}P_{n+1 }(\{y:y+(x_0+\cdots+x_n)\in B \})P_0(dx_0)\cdot P_n(d x_n)=\int_ {\{X_o \in B_0 ,\ldots,X_n \in B_n\}}P_{n+1}(\{y:y+S_n(\omega) \in B \} )P(d \omega)  $$ Thus the first thing would be to  verify that the map $(x_o,\ldots,x_n) \mapsto P_{n+1 }(\{y:y+(x_0+\cdots+x_n)\in B \})$ is $\mathcal B(\mathbb R^n)-B(\mathbb R) $ measurable. I'm not sure how to do this. Then i believe the integral caluclation is motivated from som identification that $\omega \in \{X_0 \in B_0,\ldots, X_n \in B_n,S_{n+1}\in B \} $ if and only if $\omega \in \{X_0 \in B_0,\ldots, X_n \in B_n \}\cap \{\omega:X_0(\omega)+\cdots+X_{n+1 } (\omega) \in B \}  $ but then I get stuck relating the probability $P $ of this set to the distribution $P_{n+1 }$ on $\mathbb R $ . Many thanks in advance!",['probability-theory']
3215996,"If $f∈C^1$ and $\{∇f=0\}$ has Lebesgue measure $0$, then $\{f∈B\}$ has Lebesgue measure $0$ for all Borel measurable $B⊆ℝ$ with Lebesgue measure $0$","Let $d\in\mathbb N$ and $f\in C^1(\mathbb R^d)$ . Assume $\left\{\nabla f=0\right\}$ has Lebesgue measure $0$ . How can we conclude that $\left\{f\in B\right\}$ has Lebesgue measure $0$ for all Borel measurable $B\subseteq\mathbb R$ with Lebesgue measure $0$ ? The claim can be found in an answer on mathoverflow . The author writes that the claim ""is true locally, in a neighborhood of each point where $\nabla f\ne0$ , due to the implicit function theorem"". Honestly, I don't even understand what exactly he's meaning. Let $a\in\mathbb R^d$ with $\nabla f(a)\ne0$ . Then surely, by continuity of $\nabla f$ at $a$ , there is an open neighborhood $N$ of $a$ with $$\nabla f(x)\ne0\;\;\;\text{for all }x\in N\tag1.$$ But how do we need to apply the implicit function theorem and what's the resulting ""local"" conclusion? Maybe that $N\cap\left\{f\in B\right\}$ has Lebesgue measure $0$ ?","['measure-theory', 'lebesgue-measure', 'real-analysis', 'implicit-function-theorem', 'differential-geometry']"
3216052,Finding an Integral Surface,"Consider finding the integral surface of $$x^2 p + xy q = xyz-2y^2$$ which passes through the line $x=y e^y$ in the $z=0$ plane. Attempt In Lagrange's subsidiary form $$\frac{dx}{x^2}=\frac{dy}{xy}=\frac{dz}{(xyz-2y^2)}$$ Firstly consider $$\frac{dx}{x^2}=\frac{dy}{xy}$$ One can trivially show that $$a = \frac{x}{y}$$ where $a$ is an arbitrary constant. Now, consider $$\frac{dx}{x^2}=\frac{dz}{(xyz-2y^2)}$$ which may be written as $$\frac{dz}{dx}=\frac{(xyz-2y^2)}{x^2} \equiv \frac{z}{a}-\frac{2}{a^2}$$ having used $a=x/y$ from before. (After this point I am unsure of my working...) $$\frac{dz}{dx}=\frac{az-2}{a^2} \implies\frac{dz}{(az-2)}=\frac{dx}{a^2}$$ As $a$ is a function of $a(x,y)$ , albeit an arbitrary constant, is my solution above sensical or have a made a mistake? I understand that to find the integral surface the general solution is of the form $F(a,b)$ where has so far been determined to be $a=x/y$ . How can I find this over arbitrary constant $b$ ?","['multivariable-calculus', 'characteristics', 'ordinary-differential-equations', 'partial-differential-equations']"
3216081,What is the significance of the largest eigenvalue of a matrix?,"The Tracy-Widom distribution gives the limiting distribution of the largest eigenvalue of a random matrix (in the $\beta$ -Hermite ensemble, where $\beta$ is 1,2 or 4). The second smallest eigenvalue of the Laplacian helps you divide the graph into communities, known as the algebraic connectivity... But what is so important about the largest eigenvalue of a matrix? Is it related to geometry? Or dynamics, where such a matrix may have a direct meaning?","['matrices', 'linear-algebra', 'probability', 'eigenvalues-eigenvectors']"
3216082,Spectral Theorem: Realization of a direct sum of $L^2$ spaces as a single $L^2$ space,"The following is motivated by an attempt to understand the Spectral Theorem for Bounded operators on a none separable Hilbert space. One version of the theorem states that for a bounded (say normal) operator on a Hilbert space $A \in \mathcal{L(H)}$ , there exists an index set $I$ such that there is an isomorphism $$Q:\mathcal{H} \to \bigoplus_{i \in I} L^2 (S,\mu_i)$$ Where $S\in \mathbb{C}$ is the spectrum of $A$ , a compact set, with a probability measure $\mu_i$ , such that for $(f_i(z))_{i \in I}$ a sequence of functions in this direct sum, we have $$QAQ^{-1}(f_i(z))_{i \in I}=(zf_i(z))_{i \in I}$$ This is a remarkable result in it's own right. But one may want a single $L^2$ space to realize this direct sum. In the separable case, one can look certain direct sum of disjoint copies of $S$ with the measures $\mu_i$ , $(X, \mu) = \bigoplus_{i=1}^{\infty}(S,\mu_i)$ , where $\mu = \sum_{i=1}^\infty \frac{1}{2^i} \mu_i$ . Then $L^2(X, \mu)$ realizes $A$ as a multiplication operator with some $F \in L^\infty(X,\mu)$ . How can one do the same thing with a none separable $\mathcal{H}$ ? I.e. how can we realize it as a multiplication operator with a bounded function given this direct sum decomposition? . any reference or answer is appreciated.","['measure-theory', 'operator-theory', 'hilbert-spaces', 'functional-analysis', 'spectral-theory']"
3216103,Solving differential equation using variation in parameters method.,"I'm a bit stuck on this problem and could use some help. I'm trying to solve this differential equation using two methods: variation of parameters and undetermined coefficients. They should be equal. I'll start with the undetermined coefficients first : $$y'' - 2y' + y = e^{2x}$$ auxiliary equation: $$r^2 - 2r + 1 = 0$$ $$(r-1)(r-1)$$ so the root is 1 so the complimentary equation is: $$y_c = c_1e^x + c_2xe^x$$ So, a guess $y_p = Ae^{2x}$ so $y_p' = 2Ae^{2x}$ and so $y_p'' = 4Ae^{2x}$ and so plugging: $$4Ae^{2x} - 4Ae^{2x} + Ae^{2x} = e^{2x}$$ and so A = 1 so $y_p = e^{2x}$ so teh general solution via undetermined coefficients is: $$y = c_1e^x + c_2xe^x + e^{2x}$$ Now for variation in parameters which should be the same: so we have the complimentary equation: $$y_c = c_1e^x + c_2xe^x$$ and so we replace the constants with functions and look for a particular in this form: $$y_p = u_1(x)e^x + u_2(x)xe^x$$ differentiating: $$y_p' = u_1e^x + u_1'e^x + u_2(xe^x + e^x) + u_2'xe^x$$ then set $u_1'e^x + u_2'xe^x = 0$ $$y_p' = u_1e^x + u_2xe^x + u_2e^x$$ so $$y_p'' = u_1e^x + u_1'e^x + u_2(xe^x + e^x) + u_2e^x + u_2'e^x$$ so subbing: $$u_1e^x + u_1'e^x + u_2xe^x + u_2e^x) + u_2e^x + u_2'e^x - 2u_1e^x - 2u_2xe^x + 2u_2e^x + u_1e^x + u_2xe^x = e^{2x}$$ I am having trouble solving: so my two equations are: $$u_1'e^x + u_2'e^x = e^{2x}$$ $$u_1'e^x + u_2'xe^x = 0 \rightarrow u_1'e^x = -u_2'xe^x \rightarrow u_1' = -u_2'x$$ so can I then sub in $u_1'$ like this: $$-u_2'xe^x + u_2'e^x = e^{2x}$$ Is this right so far? how do I go from here?",['ordinary-differential-equations']
3216137,"For a unitary matrix $U$, what's the minimal value of the real part of $\det(U^*)\prod_i U_{ii}$?","For an $n$ -by- $n$ unitary matrix $U$ , what's the minimal value of the real part of $\Delta(U)=\det(U^*)\prod_i U_{ii}$ ? Let $V$ be the orthogonal matrix with diagonal entries equal to $1-2/n$ and all other entries equal to $-2/n$ . This achieves $\Delta(V)=-(1-2/n)^n$ , which computer experiments suggest is optimal. Interestingly this would mean that the large $n$ limit is $-e^{-2}$ . For $n=2$ the minimum is $0$ , which can be proven by writing $U$ in the form $$\begin{pmatrix}\alpha & \beta \\ -e^{-i\theta}\bar\beta & e^{-i\theta}\bar\alpha\end{pmatrix}.$$ The average value of $\Delta(U)$ across the unitary group is $1/n!$ . Indeed, for any permutation $\sigma$ with permutation matrix $P_\sigma$ , $\Delta_\sigma(U)=(-1)^\sigma\det(U^*)\prod_i U_{i,\sigma(i)}$ equals $\Delta(UP_\sigma)$ . The sum $\sum_\sigma \Delta_\sigma(U)$ equals $\det(U^*)\det(U)=1$ , and each $\int_{U(n)}\Delta_\sigma(U)dU$ is equal because multiplication by $P_\sigma$ preserves the Haar measure.","['matrices', 'inequality', 'linear-algebra']"
3216143,How to solve this D.E $y''(\frac{x}{2})+y'(\frac{x}{2})+y(x)=x$,"I know how to slove $y''(x)+y'(x)+y(x)=x$ But I couldn't solve this $$y''(\frac{x}{2})+y'(\frac{x}{2})+y(x)=x$$ any hint to help me?
Thanls","['functional-equations', 'ordinary-differential-equations']"
3216166,What's the name of a function with a variable in the base and exponent?,"For example, $k^x$ , where $k$ is a constant, is an exponential function. It is differentiated quite easily. $x^x$ is also an exponential function. It takes much more effort to differentiate it. Is there a specific name for the latter type of function?","['functions', 'terminology']"
3216181,Different Coloring of regular n-gon,"By using Burnside's lemma, I want to find the number of different coloring of vertices of a regular n-gon, with X colors. By ""different"" I mean : up to rigid motions. I've seen some partial results, but I'm not sure about the genralized version.","['group-theory', 'combinatorics']"
3216215,Do invariant functions form a Banach (sub)manifold in function spaces?,"Let $G$ be a topological group, and $X$ some function space; preferably a Sobolev space $X=W^{1,p}(\Omega)$ , where $\Omega \subset \mathbb{R}^n$ is some invariant subset ( $g\Omega \subset \Omega$ ) or the whole space. Let $G$ be represented as linear, continuous operators $\pi(g)$ on $X$ in the form $$
\pi(g)f(x)=f(gx)
$$ Does the set of invariant functions that satisfy $$
f(gx)=f(x)
$$ form a Banach (sub)manifold of $X$ ? Do we need additional assumptions on the group (e.g.compact groups)? 
As an example consider $G=O(n)$ the orthogonal group. Does the set of radial symmetric functions form a Banach (sub)manifold of $X$ ? Feel free to modify and add assumptions if necessary as I am not too familiar with this field. I am thankful for any reference, hint or remark!","['group-theory', 'sobolev-spaces', 'functional-analysis', 'reference-request']"
3216224,How many nonhomeomorphic embedding of circle into Klein's bottle exists?,"I want to know are my conclusions right. The question is ""How many nonhomeomorphic embeddings of the circle into the Kleins bottle exists"". The idea: We know that the Kleins bottle consists of two Möbius strips, so let us look over two variants: the circle is embedded into one of the Möbius strips the circle is embedded into both Möbius strips Then we need to prove that our two variants are nonhomeomorphic.
My attempt is to cut a circle from the second variant, then we'll get something connected and by cutting the circle from the first one, we'll not get something connected. After that I need two show that there aren't any other variants, I think the fact that all circles that lay on the one Möbius strip are homeomorphic and all circles that lay on both of them are homeomorphic and there aren't any other variants of placing the circle implies that there aren't any other nonhomeomorphic embeddings, but I'm not sure.","['general-topology', 'geometry', 'differential-geometry']"
3216229,Finding the infinite sum involving $\coth$ function using contour integration,"I am looking to show: $\sum_{n=1}^∞ \frac{\coth(nπ)}{n^3} = \frac{7π^3}{180}$ There is a hint earlier that you are supposed to be using the function $f(z)=\frac{\cot z\coth z}{z^3}$ . I have calculated the residue at the pole of order 5 at $z=0$ as $-\frac{7}{45}$ , but I am unsure how to calculate the other residues, so I can use the residue theorem. I think there is a simple pole whenever $z=\frac{(2n+1)π}{2}$ , as this is when $\cot z=0$ but I just don't know how to find the residue here. I'm presuming my residues will lead to the sum I'm wanting to find coming out in some form when I apply the residue theorem, but I'm just not sure how to get there. Thanks so much for any help in advance.","['complex-analysis', 'contour-integration', 'sequences-and-series', 'residue-calculus', 'trigonometry']"
3216256,Evaluating $\int_{0}^{2 \pi} \frac{\sin \phi}{(\sin\theta-\sin\phi)^2+2a(1-\cos(\theta -\phi))} d\phi$,"How Can I evaluate this integral ? $$\int_{0}^{2 \pi} \frac{\sin\phi}{(\sin\theta-\sin\phi)^2+2a(1-\cos(\theta -\phi))} d\phi$$ I have tried to write the denominator as $$\sin( (\theta-\phi)/2 )[2\cos((\theta+\phi)/2)+4a\sin((\theta-\phi)/2)]$$ And writing the numerator as $$\sin(\phi)=2\sin((\phi-\theta)/2)\cos((\phi - \theta)/2)\cos(\theta)+\sin(\theta)(1-2\sin^2((\phi-\theta)/2)$$ Where $$\theta , \phi \in[0,2 \pi]$$","['integration', 'complex-analysis', 'trigonometric-integrals', 'real-analysis']"
3216298,Lower bound for diagonal Ramsey numbers,"In the book ""The Probabilistic Method"" by Alon and Spencer there is a quite clear derivation (provided by Lovasz local lemma) of the fact, that if $ e\binom{k}{2}\binom{n-2}{k-2}\cdot 2^{1-\binom{k}{2}}<1 $ , then $ R(k,k)>n $ . Then authors write that ""a short computation shows that this gives"" $ R(k,k)>(\sqrt{2}/e)(1+o(1))k2^{k/2} $ . To be honest, I completely don't understand how to prove it, because it seems that use of asymptotic equivalence will spoil initial inequality. I've seen the post ( Asymptotic lower bound for R(k,k) ), but I'm not sure that it answers my question.","['ramsey-theory', 'combinatorics', 'asymptotics', 'upper-lower-bounds']"
3216338,How many dimensions will a derivative of a 3-D tensor by a 4-D tensor have? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question As the title above, I find it hard to imagine or illustrate. It is a question from Coursera.","['matrices', 'tensors', 'derivatives', 'tensor-rank']"
3216409,Which is the correct way to compute this surface integral?,"I am trying to find a surface integral $$\iint_Syz\ dS$$ of a cylinder segment  where $S$ is the portion of $x^2 + y^2 = 1$ with $x ≥ 0$ and $z$ between $z = 2$ and $z = 5 − y$ . I thought that there should be two ways to do this. The first is to use rectangular coordinates, and do this $$x=f(y,z)$$ $$||n||=\sqrt{(f_y)^2+(f_z)^2+1}$$ $$={1\over \sqrt{1-y^2}}$$ $$\int_{-1}^1\int_2^{5-y}\ {yz\over \sqrt{1-y^2}} \, dz \, dy =-\frac{5\pi}{2} \approx -7.854$$ I also tried parameterizing the cylinder with $$r_z(z,\theta)=\left<0,0,1\right>$$ $$r_{\theta}(z,\theta)=\left<-\sin\theta,\cos\theta,0\right>$$ $$||n||=\sqrt{(-\sin\theta)^2+(\cos\theta)^2+0^2}=1$$ $$\int_{0}^\pi\int_2^{5-\sin\theta}\ z\sin\theta \, dz \, d\theta \approx 13.813$$ Why am I getting different solutions? I though it might have to do with the fact that the first approach is taking into account the negative parts of $y$ , 
and indeed when I circumvent this I get the same solution as the parametrization: $$2\int_{0}^1\int_2^{5-y}\ {yz\over \sqrt{1-y^2}} \, dz \, dy \approx 13.813$$ But it is still unclear to me which is the correct approach, and why.","['integration', 'multivariable-calculus', 'surface-integrals']"
3216414,Sum of reciprocal sine function $\sum\limits_{k=1}^{n-1} \frac{1}{\sin(\frac{k\pi}{n})}=?$,"The question comes to me when I find there are answers on summation of some forms of trigonometric functions, i.e. $$
\sum\limits_{k=1}^{n-1} \frac{1}{\sin^2(\frac{k\pi}{n})}\\
\sum\limits_{k=0}^{n-1} \tan(\frac{k\pi}{n})\\
$$ Sum of the reciprocal of sine squared Sum of tangent functions where arguments are in specific arithmetic series To show the identity of $\sum\limits_{k=0}^{n-1} \frac{1}{\tan^2(\frac{k\pi}{n})}$ should be trivial as the summand can be rewritten as $\frac{1}{\sin^2(\frac{k\pi}{n})}-1$ . I am wondering what is the following summation: $$
\sum\limits_{k=1}^{n-1} \frac{1}{\sin(\frac{k\pi}{n})}?
$$","['trigonometric-series', 'summation-method', 'trigonometry', 'summation']"
3216437,Non-trivial examples of non-discriminatory functions,"In a 1989 paper by Cybenko , he defines a discriminatory function as follows: For a fixed $n \in \mathbb{Z}^+$ , let $I_n = [0,1]^n$ . A function $\sigma : \mathbb{R} \to \mathbb{R}$ is said to be discriminatory if the only signed regular Borel measure $\mu$ that satisfies $\int_{I_n} \sigma (y^T x + \theta ) d \mu (x)= 0$ for all $y^T \in \mathbb{R}^n$ and $\theta \in \mathbb{R}$ is $\mu = 0$ . I'm interested in finding examples of non-discriminatory functions. Naturally, a function that is almost everywhere zero with respect to the $d$ -dimensional Lebesgue measure is non-discriminatory, but this example is fairly trivial. Are there non-trivial examples of non-discriminatory functions?","['measure-theory', 'lebesgue-measure', 'neural-networks']"
3216438,Equivalent conditions for submartingales (Problem 3.19 in Karatzas and Shreve),"I have a hard time understanding the proof of the following result in Karatzas and Shreve (Problem 3.19, page 18). Proposition The following three conditions are equivalent for a non-negative right-continuous submartingale $\{X_t,0\leq t < \infty\}$ : It is uniformly integrable. It converges in $L^1$ as $t\rightarrow \infty$ . It converges $\mathbb{P}$ -a.s. as $t\rightarrow \infty$ to an integrable random variable $X_{\infty}$ such that $\{X_t,0\leq t \leq \infty\}$ is a submartingale. The book provides a solution to this problem, but I have hard time understanding the argument used to establish that $(ii)\implies(iii)$ and $(iii)\implies(i)$ . To establish $(ii)\implies(iii)$ the authors's argument is following: Let $X_{\infty}$ be the $L^1$ limit of $X_t$ . For any $A_s\in\mathcal{F}_s$ we have $$ \int_A X_s d\mathbb{P}\leq \int_A X_td\mathbb{P}$$ Letting $t\rightarrow \infty$ we obtain that $$\int_A X_s d\mathbb{P}\leq \int_A X_{\infty} d\mathbb{P}$$ for all $0\leq s < \infty$ . Question : How do we arrive at the last inequality. Why can we pass the limit through the integral? To establish $(iii)\implies(i)$ the authors argue the following: For $0\leq t<\infty$ and $\lambda>0$ we have $$\int_{\{|X_t|\geq\lambda\}}X_t d\mathbb{P} \leq \int_{\{|X_t|\geq\lambda\}}X_{\infty}d\mathbb{P}$$ which converges uniformly in $t$ to $0$ since $\mathbb{P}[|X_t|\geq\lambda]\leq(1/\lambda)\mathbb{E}[X_t]\leq(1/\lambda)\mathbb{E}[X_{\infty}]$ Question : How does uniform limit of $\mathbb{P}[|X_t|\geq\lambda]$ as $\lambda\rightarrow \infty$ establishes uniform integrability?","['martingales', 'measure-theory', 'convergence-divergence', 'probability-theory']"
3216469,Closed maps with non-compact fibers,"A geometric example of a non-closed map with compact fibers is obtained by taking a finite covering map and removing a point upstairs. This fragmentation of the fiber destroys closedness but retains compactness of fibers. What are some geometric examples of a closed map with non-compact fibers ? I'm not interested in things like the terminal map of an uncountable discrete space. Hoping for some pictures, the closer to manifolds the better. My motivation is to justify the intuition for proper (universally closed separated) maps as ""complete"" families of compact spaces. I want to understand how a ""mere"" closed map might fail to warrant this description when the fibers aren't compact.","['closed-map', 'general-topology', 'compactness']"
3216484,can't understand derviatives from searching all over the internet,I have tried to follow a lot of tutorials out there for explaining derviatives and show (understandable)examples but i couldn't understand any. Can anyone link me a useful and easy to follow tutorial or explain derviatives  for me?,"['calculus', 'derivatives', 'reference-request']"
3216516,Graph theory and combinations,"Graph $G$ has 9 vertices and 21 edges. 3 vertices have degree $x$ , 3 have degree $y$ , and 3 have degree $z$ . Where $x,y,z$ are allowed to be the same number. a) How many graph G could satisfy these criteria? My work: Total deg (G) = 3x+3y+3z = 2 * 21 3x + 3y + 3z = 42 x + y + z = 14 All the different possibles values that x, y, and z can have to satisfy x + y + z = 14 are 16 choose 14. So there are 120 G graphs that could satisfy these criteria. b) How many graphs G could satisfy these criteria if G has no isolated vertices? My work: If G has no isolated vertices then, x + y + z = 14, where x, y, and x are equal or greater than 1. Hence, I thought that since I already know there are 120 possible G graphs (where x,y,z can be equal to 0). Then I could just count the possible ways x + y + z = 14 when x = 0 + when y = 0 + when z = 0; and then subtract that value from 120 to get the total possible ways x + y + z = 14 if x,y,z are greater than or equal to 1. My answer was 75... Am I doing something wrong?",['combinatorics']
3216525,Is Schilling's Corollary 8.9 really a corollary?,"I am reading René Schilling's Measures, Integrals and Martingales and am confused as to why he considers Corollary 8.9 a corollary of Theorem 8.8, rather than a completely separate theorem (which it appears to be). Theorem 8.8: Let $X$ be a measurable space. Every $\mathcal{A}/\bar{\mathcal{B}}$ -measurable numerical function $u:  X \to\bar{\mathbb{R}}$ is the pointwise limit of simple functions: $u(x) = \lim_{j\to\infty} f_j(x), f_j\in\mathcal{E}(\mathcal{A})$ and $|f_j|\leqslant|u|$ . If $u\geqslant 0$ , all $f_j$ can be chosen to be positive and increasing towards $u$ so that $u = \sup_{j\in\mathbb{N}} f_j$ . Corollary 8.9: Let $X$ be a measurable space. If $u_j: X \to \bar{\mathbb{R}}, j\in\mathbb{N},$ are measurable functions, then so are $$\sup_{j\in\mathbb{N}} u_j,\qquad \inf_{j\in\mathbb{N}} u_j,\qquad \limsup_{j\to\infty} u_j,\qquad \liminf_{j\to\infty} u_j,\qquad $$ and, whenever it exists, $\lim_{j\to\infty} u_j$ . From what I can tell, it seems that 8.9 doesn't follow from 8.8 at all. Schilling offers a proof of 8.9, which I'll insert below, but it doesn't reference anything relating to 8.8. Am I missing a key point here, or is calling this a ""corollary"" just a mistake? Also for completeness, here are Eqs. 8.10–8.12 referenced in the proof: $$\inf_{j\in\mathbb{N}} u_j(x) = -\sup_{j\in\mathbb{N}} u_j(-x), \tag{8.10}$$ $$\liminf_{j\to\infty} u_j(x) := \sup_{k\in\mathbb{N}} \Big( \inf_{j\geqslant k}u_j(x) \Big) = \lim_{k\to\infty} \Big( \inf_{j\geqslant k}u_j(x) \Big), \tag{8.11}$$ $$\limsup_{j\to\infty} u_j(x) := \inf_{k\in\mathbb{N}} \Big( \sup_{j\geqslant k}u_j(x) \Big) = \lim_{k\to\infty} \Big( \sup_{j\geqslant k}u_j(x) \Big), \tag{8.12}$$","['measure-theory', 'proof-verification']"
3216533,"Construct a Borel set on R such that it intersect every open interval with non-zero non-""full"" measure","This is from problem $8$, Chapter II of Rudin's Real and Complex Analysis. The problem asks for a Borel set $M$ on $R$, such that for any interval $I$, $M \cap I$ has measure greater than $0$ and less than $m(I)$. I was thinking of taking the Cantor approach: taking $R$ to be the union of $[a,b]$ with $a$ and $b$ rationals, and for each $[a,b]$ we construct Cantor sets inside it. During theconstruction of each Cantor set, in order to have positive measure on it, we need to take off smaller and smaller intervals from it, namely the proportion goes to $0$. As a result, these Cantor sets are extremely ""dense"" on their ends. If for an interval $I$ it intersects with the Cantor set on $[a,b]$ while $b-a>>m(I)$, we shall expect the measure of intersection to be rather close to $m(I)$ and then we lose control on these cases. Is there any way to fix this or shall I consider other approaches? Thank you","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3216536,Converting an equation based on square roots,"So I was wondering how to convert an equation of the form $\sqrt{x_1}+\sqrt{x_2}+\sqrt{x_3}+...\sqrt{x_n}+k=0$ into a polynomial equation based on each $x_i$ . For example if the equation was $$\sqrt{x_1}+\sqrt{x_2}+k=0$$ , then subtracting $\sqrt{x_2}$ from each side and squaring yields: $$x_1+k^2+2k\sqrt{x_1}=x_2.$$ This can then be rearranged to: $$2k\sqrt{x_1}=-x_1-k^2+x_2.$$ Squaring both sides yields: $$4k^2x_1=x_1^2+k^4+x_2^2+2k^2x_1-2x_1x_2-2k^2x_2.$$ Rearranging/simplifying yields: $$x_1^2+x_2^2+k^4-2k^2x_1-2x_1x_2-2k^2x_2 = 0.$$ How can I find an equation of this form given that $n$ is greater than $4$ ? I am most interested in when $n = 6$ .","['radicals', 'algebra-precalculus', 'roots', 'polynomials']"
3216541,Proving there is an eigenvalue $\lambda$ for which $|\lambda - b_{jj}| < \epsilon \sqrt{n}$,"Let $A$ be an $n\times n$ real symmetric matrix. By applying Jacobi's
  method, suppose we have generated an orthogonal matrix $R$ and a
  symmetric matrix $B$ such that the equality $$B = R^{T}AR $$ holds. Moreover, suppose the inequality $|b_{ij}| < \epsilon$ holds
  for all $i \neq j$ . Show that for each $j = 1, 2, \ldots, n$ , there is at least one
  eigenvalue $\lambda$ of $A$ such that $|\lambda - b_{jj}| < \epsilon \sqrt{n}$ holds. This is an exercise that I am doing to study for my final exam. So, I've just recently learned Jacobi's method, and I know that the eigenvalues and eigenvectors are related to the matrices $B$ and $R$ ; however, I have no idea how to use those results to prove an inequality. I also have no idea how to get the $\sqrt{n}$ term in there. I would greatly appreciate any help in this exercise. Thanks UPDATE: These are some theorems in my book that might help. Theorem (Gerschgorin’s Theorem): Let $n \geq 2$ and $A \in \mathbb{C}^{n\times n}$ . All eigenvalues of $A$ lie in the region $D = \bigcup_{i=1}^{n} D_{i}$ , where $D_{i}$ are the Gerschgorin discs of $A$ . Definition: (Gerschgorin Disc): Suppose $n \geq 2$ and $A \in \mathbb{C}^{n\times n}$ . The Gerschgorin discs $D_{i}$ of the matrix $A$ are defined by the closed circular regions $$D_{i} = \{z \in \mathbb{C} : |z - a_{ii}| \leq R_{i}\}, $$ where $$ R_{i} = \sum_{j = 1, \\ i \neq j}^{n}  |a_{ij}|$$ is the radius of $D_{i}$ . Theorem (Bauer-Fike): Suppose $A$ and $E$ are real symmetric $n\times n$ matrices and $B = A - E$ . Assume, further, that the eigenvalues of $A$ are denoted by $\lambda_{j}, j = 1, 2, 3, \ldots, n$ and $\mu$ is an eigenvalue of $B$ . Then at least one eigenvalue of $\lambda_{j}$ of $A$ satisfies $|\lambda_{j} - \mu| \leq ||E||_{2}$ , where $|| \cdot ||_{2}$ denotes the $2$ -norm of a matrix. Book link: http://newdoc.nccu.edu.tw/teasyllabus/111648701013/Numerical_Analysis.pdf The problem is from chapter 5. I would appreciate it if the answer does not use too many outside results from the book. I suppose a few are okay though, as long as they aren't really strong results that are hard to understand.","['matrices', 'numerical-methods', 'linear-algebra', 'eigenvalues-eigenvectors']"
3216554,Probability of failing 2 out of the last 3 consecutive trials,"In a test with a series of trials, you'd fail the test if you fail 2 out of the last 3 trials (i.e. window of 3). For example, for a series of trials (starting from index 0, 1, ..., 7), we have results: 0, 1, 0, 0, 0, 1, 0, 1 (Fail is 1, Not failing is 0). Thus you fail the test at trial number 7. Now assume the probability of failure in each trial is 1/3 (each trial is independent of each other), and call the trial where you fail the test Y (so in the example above Y = 7). Find P(Y = y) for y = 0, 1, 2, ..., 20. In other words, the probability you fail the test in each of those years. By default then we know P(Y=0) = P(Y=1) = 0, and P(Y=2) = (1/3)^2. However, I have no idea how to proceed from here to higher values of Y. I've tried multiple values but they just don't work. I'm also given as a hint that E(Y) over that range is around 8.5. You can use this as a sanity check for your answer....",['probability']
3216558,binary search tree with key values $ a_1 < \dots < a_k$. How do I choose $j$ to still get a tree with minimal height?,"So at the beginning I have an empty binary search tree. Moreover I have key values $a_1 < \dots < a_k$ . How can we choose $j$ so that after the first insert of an element with key value $a_j$ a tree of minimal height is still possible by further insertions. My thoughts: My first idea was to choose $j=1$ ( later $j=k$ ), which is obviously false. I know that we somehow have to use the fact that we have $a_1 < \dots < a_k$ . Maybe we could choose $j = \frac{k}{2}$ or something like that. But this is just an idea. I don't know how to argue for that.","['trees', 'binary', 'discrete-mathematics']"
3216566,"If $\det(A-\lambda I)=\det(A^{-1}-\lambda I)$, then characteristic polynomial is coefficient symmetry","In one class, professor mentioned the following without proof, Suppose $A\in \mathbb{R}^{2N\times 2N}$ . $\det(A-\lambda I)=\det(A^{-1}-\lambda I)=p(\lambda)$ then if $$p(\lambda) = a_{2N}\lambda^{2N}+a_{2N-1}\lambda^{2N-1}+ \cdots +a_1\lambda^1 +a_0,$$ we have $a_{2N}=a_0, a_{2N-1}=a_1, \cdots.$ I refer to the following discussion Show that $\det(A-\lambda I)=\det(B-\lambda I)$ However, I still have no idea how to see it. Is there any good method without expanding the "" $\det$ "" to show it? Thanks in advanced.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3216581,Prove $\sum_ {k=0} ^n {n \choose k}^2 = {2n \choose n} $? [duplicate],"This question already has answers here : Sum of square binomial coefficients [duplicate] (2 answers) How to prove Vandermonde's Identity: $\sum_{k=0}^{n}\binom{R}{k}\binom{M}{n-k}=\binom{R+M}{n}$? (7 answers) Closed 5 years ago . Give a proof of the following identity using a double-counting argument: $\sum_ {k=0} ^r {m \choose k} {n \choose r - k} = {{m+n} \choose r} $ Then using this result, derive the following special case from it. $\sum_ {k=0} ^n {n \choose k}^2 = {2n \choose n} $ ? For the first one, the method that I have is starting with the right hand side: Assume that k is a small number, which is less than r and m. We choose r from m + n can be divided into 2 parts: Choose k from m. Choose r - k from n. My thinking is we can choose k from m and them choose r - k (because we only need to choose r) from n. I'm stuck with the next part and not sure if my proof for the first part is right or not.","['permutations', 'combinations', 'combinatorics', 'discrete-mathematics']"
3216604,Convexity of $|A^TA|$,"Assume $A\in\mathbb{R}^{m\times n}$ is a $m$ by $n$ matrix. Let $|\cdot|$ denote the Frobenius norm of matrix. Define function $f:\mathbb{R}^{m\times n}\to\mathbb{R}$ as $f(A):=|A^TA|$ . Is $f$ a convex function? Intuitively, I think this function is a composition of a norm and something with quadratic structure, and thus should be convex. To prove this, $f(tA+(1-t)B)=|t^2A^TA+t(1-t)(A^TB+B^TA)+(1-t)^2B^TB|\le t^2|A^TA|+(1-t)^2|B^TB|+t(1-t)|A^TB+B^TA|$ . If $|A^TB+B^TA|\le|A^TA|+|B^TB|$ , then we get the convexity. Is this inequality true?","['matrices', 'convex-analysis', 'matrix-norms']"
3216615,Notation in Vakil's 18.4.O,"In Vakil's November 2017 set of notes, section 18.4 deals the universal plane conic, defined as follows. Fix a base field $k$ , and everything takes place over it. Consider $\mathbb{P}^2$ with homogeneous coordinates $x_i$ , $0\leq i\leq 2$ , and $\mathbb{P}^5$ with homogeneous coordinates $a_{ij}$ , $0\leq i\leq j\leq 2$ . Then let $\mathcal{C}$ denote the closed subscheme of $\mathbb{P}^2 \times \mathbb{P}^5$ defined by the equation $$\sum_{0\leq i\leq j\leq 2} a_{ij} x_i x_j = 0.$$ This comes equipped with a natural projection map $\pi : \mathcal{C} \to \mathbb{P}^5$ . We will think of a point in $\mathcal{C}$ as a pair $(p,C)$ where $C$ is a plane conic and $p$ is a point on it. In $\mathbb{P}^2$ , we fix a point $q$ and a line $\ell$ . Then the set of points $(p,C) \in \mathcal{C}$ with $q \in C$ forms a Weil divisor $\mathrm{D}_q$ on $\mathcal{C}$ . Similarly, the set of points $(p,C) \in \mathcal{C}$ with $p\in \ell$ forms a Weil divisor $\mathrm{D}_\ell$ . Finally, let $K$ denote a fibre of the map $\pi$ over some point. In problem 18.4.O, Vakil uses the notation $\mathrm{D}_q \cdot K$ and $\mathrm{D}_\ell \cdot K$ . I'm not sure what this notation means. My first guess is that it refers to intersection number, but intersection theory hasn't been introduced yet (we'll have to wait until Chapter 20 to see intersection theory), and there's no indication that one should look ahead to do this exercise. (edit: upon rereading, it seems the following is incorrect.) What's worse, it doesn't seem like the intersection theory that is introduced later is set up in a way that we could make use of here. (Chapter 20 only discusses the special case of an intersection of a sheaf with a collection of invertible sheaves.) Am I mistaken in assuming this is related to intersection theory?","['notation', 'algebraic-geometry']"
3216618,$n\#$ (the $n$th primorial) seems very close to $e[(n-1)^n - n^{n-1}]$,"Let $$n\# := P(n) P(n-1) \cdots P(2) P(1) ,$$ where $P(1)$ is the smallest prime, $2$ , $P(2) = 3$ , and so on. Then, as an example of what is claimed in the above title, $$P(11) = 200\,560\,490\,130, \qquad \textrm{and} \qquad 10^{11}-11^{10}=74\,062\,575\,399,$$ which is very close to $1/e$ times the first number. Does this continue to hold? If so, I bet it is already known, but I’d like to know where to find it. There may be a connection with the Chebyshev function on Wikipedia, but that seems more akin to $e^{P(n)}$ .","['number-theory', 'elementary-number-theory', 'prime-numbers']"
3216622,How is the many-to-one function $f(x) = \frac{1}{x-1} + \frac{2}{x-2} + \frac{3}{x-3}$ decreasing?,"The function $f(x) = \frac{1}{x-1} + \frac{2}{x-2} + \frac{3}{x-3}$ is many-to-one, despite it having a strictly negative derivative (the domain being $\mathbb{R} - \{1,2,3\}$ ).  Why is this so?  Is there any way of knowing this without actually graphing $f(x)$ , which seems rather difficult?",['functions']
3216639,Show that the random variables $X$ and $Y$ are uncorrelated but not independent,"Show that the random variables $X$ and $Y$ are uncorrelated but not
  independent The given joint density is $f(x,y)=1\;\; \text{for } \; -y<x<y \; \text{and } 0<y<1$ , otherwise $0$ My main concern here is how should we calculate $f_1(x)$ $f_1(x)=\int_y dy = \int_{-x}^{1}dy + \int_{x}^{1}dy = 1+x +1=2\; \; \forall -1 <x<1$ OR Should we do this ? $f_1(x)$ = $$ \begin{cases} 
      \int_{-x}^{1}dy = 1+x &&  -1<x<0  \\
      \int_{x}^{1}dy = 1-x & & 0\leq x <1 \\
   \end{cases}
$$ In the second case, how do I show they are not independent. I can directly say that the joint distribution does not have a product space but I want to show that $f(x,y)\neq f_1(x)f_2(y)$ Also, for anyone requiring further calculations, $f_2(y) = \int dx = \int_{-y}^{y}dx = 2y$ $\mu_2= \int y f_2(y)dy = \int_{0}^{1}2y^2 = \frac23$ $\sigma_2 ^2 = \int y^2f_2(y)dy - (\frac23) ^2 = \frac12 - \frac49 = \frac1{18}$ $E(XY)= \int_{y=0}^{y=1}\int_{x=-y}^{x=y} xy f(x,y)dxdy =\int_{y=0}^{y=1}\int_{x=-y}^{x=y} xy dxdy$ which seems to be $0$ ? I am not sure about this also.","['independence', 'probability-distributions', 'probability-theory', 'probability']"
3216675,Convergence of $\sum_{n=1}^\infty \frac{z^n}{2^n(1-z^n)}$,"I'm solving the following problem: Find the maximal open set, $\Omega,$ where the following series converges: $$\sum_{n=1}^\infty \dfrac{z^n}{2^n(1-z^n)}.$$ Extra: Prove that the series define a holomorphic function in $\Omega.$ I conclude that a possible election is $\Omega = \mathbb{C} \setminus \partial \mathbb{D}$ doing the following: Let $z \in \bar{B}(0,r)$ with $r < 1.$ We know that $|z|\leq r.$ Furthermore we have that \begin{align*}
|f_n(z)|=\bigg| \frac{z^n}{2^n(1-z^n)} \bigg| &= \frac{|z^n|}{2^n|1-z^n|} \leq \frac{|z^n|}{2^n|1-|z|^n|}\\ &= \frac{|z|^n}{2^n(1-|z|^n)} \leq \frac{r^n}{2^n(1-r^n)} \leq \frac{1}{2^n(1-r^n)} = a_n.
\end{align*} The series $\sum_{n=1}^\infty a_n < +\infty$ since $$\lim_{n \rightarrow \infty} 
 \frac{a_n}{1/2^n} = \lim_{n \rightarrow \infty} \frac{1}{1-r^n} = 1.$$ We conclude that the original series is absolutely convergent. Let $z \in \mathbb{C} \setminus {B}(0,r)$ with $r > 1.$ Hence $r \leq |z|$ and we have that \begin{align*}
|f_n(z)|=\bigg| \frac{z^n}{2^n(1-z^n)} \bigg| &= \frac{|z^n|}{2^n|1-z^n|} = \frac{1}{2^n|1/|z^n|-z^n/|z^n||}\\ &\leq \frac{1}{2^n|1-1/|z^n||} = \frac{1}{2^n(1-1/|z^n|)} \leq \frac{1}{2^n(1-1/r^n)} = b_n.
\end{align*} As above, comparing with $\sum_{n = 1}^{\infty} 1/2^n$ we conclude the absolute convergence of the original series. The Weierstrass M-criterion gives us the uniform convergence of the series and we conclude that the series define a holomorphic function in $\Omega = \mathbb{C} \setminus \partial\mathbb{D}.$ I can't decide if the series converges for some $z \in \partial \mathbb{D}$ and I hope someone could help me. Thanks everyone!","['complex-analysis', 'convergence-divergence', 'uniform-convergence', 'sequences-and-series']"
3216768,Finite groups of cyclicality index 3,"Suppose $G$ is a group. Let’s define the cyclicality index of $G$ using the following recurrent relation: $$CI(G) = \begin{cases} 1 & \quad G \text{ is cyclic} \\ \max_{H < G} CI(H) + 1 & \quad G \text{ is non-cyclic} \end{cases}$$ The finite groups $G$ , such that $CI(G) = 1$ are exactly the cyclic groups. The finite groups $G$ , such that $Cl(G) = 2$ are of the following three classes: 1) $C_p × C_p$ , where $p$ is a prime 2) $Q_8$ 3) $\langle a,b \mid a^p = b^{q^m} = 1, b^{−1}ab = a^{r}\rangle$ , where $p$ and $q$ are distinct primes and $r ≡ 1 \pmod p$ , $r^q ≡1 \pmod p$ . But what can be told about finite groups $G$ , such that $CI(G) = 3$ ? By the classification of finite abelian groups, one can see, that if $G$ is abelian, it should be either $C_p \times C_p \times C_p$ or $C_p \times C_{pq}$ or $C_p \times C_{p^2}$ , where $p$ and $q$ are distinct primes. Non-abelian case remains obscure to me, however I know the examples of such groups: $D_{pq}$ and $D_{p^2}$ , where $p$ and $q$ are distinct primes. However, they are definitely not alone... So, my question is:
Is there a way to fully classify all finite groups $G$ , such that $CI(G) = 3$ ? (In a way similar to the classification of groups $G$ , such that $CI(G) = 2$ )","['group-theory', 'abstract-algebra', 'finite-groups', 'cyclic-groups']"
3216779,"Is the closed, bounded and convex subset version of Shauder-Tychonoff Fixed Point Theorem really in the literature?","In 1930, J. Schauder extended Brouwer's work to arbitrary Banach spaces by stating the theorem; Schauder Fixed Point Theorem: Let $K\subset E$ be a compact convex set where $ E $ is Banach and $ T :\,K\longrightarrow K $ a continuous map. Then, $ T $ has a fixed point. However, there is another fixed point theorem called Shauder-Tychonov Fixed Point Theorem, it states like this; Shauder-Tychonoff Fixed Point Theorem: Let $E$ be a Banach space and $K\subset E$ be a non-empty closed, bounded and convex set. Suppose that $T:K\longrightarrow K$ is completely continuous, then there exists $x^*\in K\;\text{such that} \;Tx^*=x^*.$ I have tried all my best to get this paper. I don't know which of the authors actually wrote the theorem. Several papers I found are misleading. Question: Can anyone please, direct me to a link on the exact paper where Shauder-Tychonoff Fixed Point Theorem was derived? I need it for my literature review. Thanks.","['functional-analysis', 'reference-request']"
3216837,Alternative version of the Final Value theorem for Laplace Transform,"Let $f:[0,\infty) \to \mathbb{C}$ be a continuous and bounded function such that the limit $$\lim_{T\to\infty} \frac{1}{T} \int_{0}^{T}f(t)dt = d \quad\text{exists}.$$ Let $\hat{f}$ be the Laplace transform of f, i.e., $$\hat{f} = \int_{0}^{\infty}e^{-st}f(t)dt.$$ Prove that $$\lim_{s\to0,\:s>0} s\hat{f} = d.$$ I have tried different ways, but for now I still did not get the whole proof. I started with the following idea (non rigorous). Define $T=\frac{1}{s}$ , then $$\lim_{s\to0,\:s>0} s\hat{f} = \lim_{T\to\infty} \frac{1}{T}\hat{f}=\lim_{T\to\infty} \frac{1}{T} \int_{0}^{T}e^{-t/T}f(t)dt$$ where the integrand converges to $f(t)$ and therefore the dominant convergence theorem I guess could be used. However, there are two problems: (a) $T \in \mathbb{C}$ and therefore I am not sure I can do that, (b) I don't want to use the dominant convergence theorem since it was not introduced in class. A different approach is to use the fundamental theorem of calculus since f continuous and define: $$F(T) = F(0) + \int_{0}^{T}f(t)dt \quad \forall \; T \in [0,\infty).$$ Then from here use the formula for the Laplace transform of the derivative, but I did not manage to move on.","['integration', 'functional-analysis', 'laplace-transform']"
3216867,Find the composite solution to this problem,"I want to find a composite solution to the boundary value problem: $$
\epsilon y'' - y' + y^2 = 1, \text{ for }0<x<1 ,\text{ where }y(0) = 1/3,\,y(1) = 1
$$ where $\epsilon\ll 1$ . My approach: I know that I can find a composite solution in four steps: Find an outer solution. Find a boundary-layer solution. Apply matching so that the outer solution and the boundary-layer solution both approximate the same function correctly. Find the composite solution, by adding the outer solution and the boundary-layer solution and subtracting the part where they are equal. Step 1: Outer solution: I assume that the solution has an expansion in powers of $\epsilon$ . So that $$
y\sim y_0 + \epsilon^\alpha y_1 + \epsilon^\beta y_2 + \ldots
$$ with $0 < \alpha < \beta <\ldots$ If we substitute this into the equation we get $$
\epsilon(y_0 + \epsilon^\alpha y_1 + \ldots )'' - (y_0 + \epsilon^\alpha y_1 + \ldots)' + (y_0 + \epsilon^\alpha y_1 + \ldots )^2 = 1
$$ If we only look at the $\mathcal{O}(1)$ terms we get $$
-y_0' + y_0^2 = 1
$$ so that the outer solution becomes $$
y_0 = \dfrac{1 - e^{2c_1 + 2x}}{e^{2c_1 + 2x} + 1}
$$ Step 2: Boundary layer solution: Let's assume that there is a boundary layer at $x = 0$ . I introduce the boundary layer coordinate $$
\bar{x} = \dfrac{x}{\epsilon^\alpha} \Leftrightarrow \dfrac{d}{dx} = \dfrac{1}{\epsilon^\alpha}\dfrac{d}{d\bar{x}}, \dfrac{d^2}{dx^2} = \dfrac{1}{\epsilon^{2\alpha}}\dfrac{d^2}{d\bar{x}^2}
$$ If we let $Y(\bar{x})$ denote the solution of the problem when using this boundary layer coordinate, the original equation becomes $$
\epsilon^{1 - 2\alpha}\dfrac{d^2}{d\bar{x}^2}(Y_0 + \epsilon^\gamma Y_1 + \ldots) - \epsilon^{-\alpha}\dfrac{d}{d\bar{x}}(Y_0 + \epsilon^\gamma Y_1 + \ldots) + (Y_0 + \epsilon^\gamma Y_1 + \ldots ) = 1
$$ To balance this equation we need to look at the different terms. We already used the second and third term in part 1 for the outer solution. We can try balancing the first and the second term so that the third term becomes higher order. We need $1 - 2\alpha = -\alpha\Leftrightarrow \alpha = 1$ . With $\alpha = 1$ the equation with the boundary layer coordinate becomes
$$ $$\dfrac{1}{\epsilon}\dfrac{d^2}{d\bar{x}^2}(Y_0 + \epsilon^\gamma Y_1 + \ldots ) - \dfrac{1}{\epsilon}\dfrac{d}{d\bar{x}}(Y_0 + \epsilon^\gamma Y_1 + \ldots) + (Y_0 + \epsilon^\gamma Y_1 + \ldots)^2 = 1
$$ If we now look at the order $\mathcal{O}\big(\dfrac{1}{\epsilon}\big)$ terms we get: $$
Y_0''(\bar{x}) - Y_0'(\bar{x}) = 1, \, Y_0(0) = 1/3
$$ so that $Y_0(\bar{x}) = c_1e^{\bar{x}} + c_2 - \bar{x}$ . We need $Y_0(0) = 1/3$ so we have $c_1 + c_2 = 1/3$ . If there is a boundary layer at $x = 0$ then we need the outer solution to satisfy the boundary condition at $x = 1$ so that we have $\dfrac{1 - e^{2c_1 + 2}}{e^{2c_1 + 2} + 1} = 1$ . This last expression can be rewritten to \begin{align}
1 - e^{2c_1+2} = e^{2c_1 + 2} + 1\\
\Leftrightarrow -2e^{2c_1 + 2} = 0
\end{align} Since there is no finite $c_1$ for which this expression holds I should probably look if the outer solution should instead satisfy the boundary condition at $x = 0$ . In that case we need to find $c_1$ such that $$
-2e^{2c_1 + 2} = 1/3
$$ but this results in a complex value for $c_1$ and I don't think that I should end up with such a solution. Question: What am I doing wrong? How can I find a correct outer solution and a correct boundary layer solution so that I can start with the matching process?","['boundary-value-problem', 'asymptotics', 'approximation', 'ordinary-differential-equations']"
3216884,"Nonnegative derivative bounded by function, show that $f$ is zero","I'm struggling to prove the following statement, which makes intuitive sense: Let $f: \mathbb{R} \to \mathbb{R}$ be differentiable. Suppose $\forall x \in \mathbb{R} \quad 0 \le f'(x) \le f(x)\,$ . If $f$ vanishes at some point, show that $f$ is identically zero. This is exercise 6 in section 6.7, in Elementary Classical Analysis by Mardsen & Hofmann. I've managed to do the following: Let $\omega \in \mathbb{R}$ be the point such that $f(\omega) = 0$ (which exists by hypothesis). Let $x < \omega$ . By Cauchy's Mean Value Theorem, $$
\exists \xi \in (x, \omega)\colon\quad 0 \le f(x) =f(x) - f(\omega) = f'(\xi)(x - \omega)\,.
$$ By hypothesis, we have that $f'(\xi) \ge 0$ . On the other hand, $x - \omega < 0$ . Thus, $f'(\xi)(x - \omega) \le 0$ . By the above proposition, though, $ f'(\xi)(x - \omega) \ge 0\,$ . Therefore, $f'(\xi) = 0$ which implies $f(x) = 0$ . With this we have proven that $\forall x \in \mathbb{R}\quad x \le \omega \rightarrow f(x) = 0\,$ . But for $x > \omega$ , I haven't been able to prove much. I've got so far: Let $x > \omega$ . By Cauchy's Mean Value Theorem, $$
\exists \xi_1 \in (\omega, x)\colon\quad 0 \le f(x) = f(x) - f(\omega) = f'(\xi_1)(x - \omega) \le f(\xi_1)(x - \omega)\,.
$$ If we apply this reasoning recursively on $f(\xi_1)$ , we get a succession $(\xi_i)_{i\in\mathbb{N}} \subseteq (\omega, x)$ such that $\forall i \in \mathbb{N}\quad \xi_{i+1} \in (\omega, \xi_i)$ and $$
\forall n \in \mathbb{N}\quad f(x) \le f(\xi_n) \prod_{i = 0}^{n - 1} (\xi_i - \omega)
$$ where $\xi_0 = x$ . Is there a hint?","['calculus', 'derivatives', 'real-analysis']"
3216894,"Why in collocation method if $h$ more smaller, the result is bad?","I want to solve BVP numerically \begin{equation}
y''+4y'+13y=e^{-2x}, \text{ } 0\leq x\leq 2
\end{equation} with \begin{equation}\label{konba1}
y(0)=\dfrac{10}{9}
\end{equation} and \begin{equation}\label{konba2}
y(2)=e^{-4}\left(\sin(6)+\cos(6)+\dfrac{1}{9}\right).
\end{equation} using collocation method. Assuming the solution is \begin{equation}\label{meong}
y=\sum\limits_{i=0}^n c_i x^i,
\end{equation} we get system of linear equation \begin{eqnarray*}
	\left[
	\begin{matrix}
		1&0&0\\
		13&4+13x_1&2+8x_1+13{x_1}^2\\
		\vdots&\vdots&\vdots\\
		13&4+13x_{n-1}&2+8x_{n-1}+13{x_{n-1}}^2\\
		1&2&4
	\end{matrix}
	\right.
	\left.
	\begin{matrix}
		\ldots&0\\
		\ldots&n(n-1) {x_1}^{n-2}+4n {x_1}^{n-1}+13{x_1}^n\\
		\ddots&\vdots\\
		\ldots&n(n-1) {x_{n-1}}^{n-2}+4 n {x_{n-1}}^{n-1}+13 {x_{n-1}}^n\\
		\ldots&2^n\\
	\end{matrix}
	\right]\\
	\begin{bmatrix}
		c_0\\c_1\\\vdots\\c_{n-1}\\c_n
	\end{bmatrix}
	=
	\begin{bmatrix}
		\dfrac{10}{9}\\e^{-2{x_1}}\\\vdots\\e^{-2x_{n-1}}\\e^{-4}\left(\sin(6)+\cos(6)+\dfrac{1}{9}\right)
	\end{bmatrix}.
\end{eqnarray*} The matrix is derived like in this reference https://www.slideshare.net/SuddhasheelGhosh/point-collocation-method-used-in-the-solving-of-differential-equations-particularly-in-finite-element-methods I solve it using matlab with this code clear all;
clc;
h=0.1;
x=0:h:2;
n=length(x);
fprintf('METODE KOLOKASI\n===============\n');
for i=1:n
    for j=1:n
        if i==1&&j==1
            A(i,j)=1;
        elseif i==1&&j~=1
            A(i,j)=0;
        elseif i==n
            A(i,j)=2^(j-1);
        else
            A(i,j)=(j-1)*(j-2)*x(i)^(j-3)+4*(j-1)*x(i)^(j-2)+13*x(i)^(j-1);
        end
    end
end
for i=1:n
    if i==1
        B(i)=10/9;
    elseif i==n
        B(i)=exp(-4)*(sin(6)+cos(6)+1/9);
    else
        B(i)=exp(-2*x(i));
    end
end
B=B';
koef=inv(A)*B;
fprintf('  i        ti   y num_i    yeks_i     error\n');
for i=1:n
    ynum(i)=0;
    for in=1:n
        ynum(i)=ynum(i)+koef(in)*x(i)^(in-1);
    end
    yeks(i)=exp(-2*x(i))*(cos(3*x(i))+sin(3*x(i))+1/9);
    error(i)=abs(ynum(i)-yeks(i));
    fprintf('%3d%10.4f%10.5f%10.5f%10.5f\n',i,x(i),ynum(i),yeks(i),error(i));
end
figure(1);
plot(x,ynum,'p','markersize',10,'color','b','markerfacecolor','g');
grid on;
axis equal;
hold on;
plot(x,yeks,'-','color','k','linewidth',1.5);
title(sprintf('Solusi Numerik dan Solusi Eksak Metode Kolokasi untuk h=%.3f',h));
xlabel('x');
ylabel('y');
legend('Solusi Numerik','Solusi Eksak');
figure(2);
plot(x,error,'r-');
grid on;
title(sprintf('Error for h=%.3f',h)); For $h=0.1$ , I get And for $h=0.01$ , I get The matlab output is showing Warning: Matrix is close to singular or badly
scaled. Results may be inaccurate. RCOND =
8.316723e-75. Why if we make $h$ smaller, then the solution of $y(2)$ is not satisfy the boundary condition? And how to make the numerical solution is satisfy the boundary condition?","['numerical-methods', 'ordinary-differential-equations']"
3216900,Differential equation $y'' - y + 2\sin(x)=0$,I need help and explanation with this differential equation. Actually I really don't know how to solve just this type of equations. So the problem: $$y'' - y + 2\sin(x)=0$$ In my opinion first of all we solve homogeneous equation $y''-y=0$ and the solution of this is $y=c_1e^x+c_2e^{-x}$ . And after that to solve it with $2\sin(x)$ . From this point I need help.,"['ordinary-differential-equations', 'fundamental-solution']"
3216935,How to find MLE of this piecewise pdf?,"Suppose $X_1,\ldots, X_n$ are i.i.d. random variables having pdf $$
f_{\theta}(x)=\left\{\begin{array}{ll}{\theta,} & {0 \leqslant x \leqslant 1} \\ {1-\theta,} & {1<x \leqslant 2}\end{array}\right.
$$ Give the maximum likelihood estimate of $\theta$ . I know the likelihood function of $(X_1,\ldots, X_n)$ is $$\ell(\theta;x)=\prod_{i=1}^{n}\left[\theta I(0<x_i<1)+(1-\theta)I(1<x_i<2)\right]$$ But I don't know how to compute $\frac{\partial \log \ell(\theta)}{\partial \theta}=0$ to get $\hat{\theta}$ . Is there anyone can tell me？","['statistical-inference', 'statistics', 'parameter-estimation', 'maximum-likelihood']"
3216954,When a seminormed space is complete?,"Let $(F, \langle \cdot\;,\;\cdot\rangle)$ be a complex Hilbert space.  Let $M$ a positive semidefine operator on $F$ . Consider the following positive semidefinite sesquilinear form: \begin{eqnarray*}
              \langle\cdot\;,\;\cdot\rangle_{M}
              :&F\times F&\longrightarrow \mathbb{C}\\
              &(x,y)&\longmapsto\langle x\;,\;y\rangle_{M} =\langle Mx\;,\;y\rangle.
              \end{eqnarray*} The seminorm induced by $\langle\cdot\;,\;\cdot\rangle_{M}$ is given by $$\|x\|_M=\langle Mx\;,\;x\rangle^{1/2}.$$ Clearly $(F,\|\cdot\|_M)$ is a normed space iff $M$ is injective. If $M$ is not injective, can we say that $(F,\|\cdot\|_M)$ is complete iff the range of $M$ is closed? or we must assume that $M$ is injective because I think that in the definition of a complete space we need that it is separated in order to obtain uniqueness of the limit. The idea is to take $(x_n)\subset F$ such that $\|x_n-x_m\|_{M}\to0$ as $n,m\to\infty$ and to show $(x_n)$ converges with respect $\|\cdot\|_M$ . My problem is that when $M$ is not injective then $\|\cdot\|_M$ is just a seminorm and we don't have the uniqueness of the limit.","['hilbert-spaces', 'general-topology', 'functional-analysis']"
3217041,What is the logic behind the combinations with repetitions formula,"This month, I'm taking combinatorics classes in my school, yesterday we learned about combinations with repetitions formula. Our teacher wrote it on the board, but she didn't really explained what is the logic behind this and why the reduction to combinations without repetition is working. I was curious to know how would you explain this in a more natural way other than just learning the formula as it is. For those being unsure for which formula I mean here it is: $$\bar{C}^{k}_{n} = C^{k}_{n+k-1}$$",['combinatorics']
3217070,Value of the structure sheaf on a general open in Spec(R),"Let $R$ be a commutative ring. Let $U\subseteq X:=\operatorname{Spec}(R)$ be an arbitrary open subset. Let $S$ be the multiplicative set of elements of $R$ which don't vanish anywhere on $U$ . Explicitly, $U$ is open so by definition there exists some ideal $I\subseteq R$ such that $U$ is the prime ideals of $R$ which don't contain $I$ , and then $S$ is the complement of the union of these prime ideals. If $A:=\mathcal{O}_X(U)$ is the value of the structure sheaf at $U$ , then restriction of functions gives us a map $R\to A$ and the image of every $s\in S$ is invertible in $A$ (because locally invertible) so we get a map $R[1/S]\to A$ . What is an example where this map is not an isomorphism of rings? I have no reason to believe that it is an isomorphism in general, but (a) if $U=D(f)$ (that is, $I=(f)$ ) then it is an isomorphism, and (b) if $X$ is affine 2-space over a field $k$ and $U$ is the complement of the origin -- the canonical ""non-affine open in an affine"" -- then $S$ is the non-zero constants and $R[1/S]=k[x,y]=A$ . I need to expand my mental store of examples/counterexamples to answer this one, I think. Can someone help?","['affine-schemes', 'algebraic-geometry', 'schemes']"
3217185,"$X_i \sim \mathscr{U}[0,2]$ independent R.V., find pdf of $Z=|X_1-X_2|$ trouble with setting convolution limits.","$X_i \sim \mathscr{U}[0,2]$ find pdf of $Z=|X_1-X_2|$ trouble with setting convolution limits. Notice $$Z=\begin{cases}X_1-X_2 \ \text{for} \ X_1\geq X_2
\\ X_2-X_1 \ \text{for} \ X_2> X_1\end{cases}$$ First of all I see it is symmetric but I did not know how to use this fact so I proceeded as follows Set $Y=-X_2$ then $Y \sim \mathscr{U}[-2,0]$ we know pdf functions $f_{X_1}(t)=\frac{1}{2}\mathbb{1}_{[0,2]}(t)$ and $f_{Y}(t)=\frac{1}{2}\mathbb{1}_{[-2,0]}(t)$ So convolution $f_{X_1}\ast f_{Y}(t)=\int_\mathbb{R}(\frac{1}{2}\mathbb{1}_{[0,2]}(t-x))(\frac{1}{2}\mathbb{1}_{[-2,0]}(x))\mathop{dx} $ I will solve it for the first case only second one is analogical, so $X_1\geq X_2$ $$\begin{align} \\f_{X_1}\ast f_{Y}(t)&=\int_\mathbb{R}(\frac{1}{2}\mathbb{1}_{[0,2]}(t-x))(\frac{1}{2}\mathbb{1}_{[-2,0]}(x))\mathop{dx}\\ &=\int_\mathbb{R}(\frac{1}{2}\mathbb{1}_{[t-2,t]}(x))(\frac{1}{2}\mathbb{1}_{[-2,0]}(x))\mathop{dx} 
\\&=\int_\mathbb{R}(\frac{1}{4}\mathbb{1}_{[t-2,t]\cap[-2,0]}(x))\mathop{dx} 
\\&= \begin{cases} \int_{-2}^{t}\frac{1}{4}\mathop{dx} \ \ \text{for}  \ t< 0\\\int_{t-2}^{0}\frac{1}{4}\mathop{dx} \  \text{for} \ t\geq0\end{cases}\end{align}$$ And now confusion begins, because I am interested in situation where $t$ is positive since values of $f_{X_i}(t)$ are different than zero only in $[0,2]$ and so is $f_Z(t)$ so should I ignore the $t<0$ part? And second question how to respect the fact that $X_1\geq X_2$ what does it mean for the result I obtained.","['probability-distributions', 'probability-theory', 'probability']"
3217202,Is reversing the order of integration possible here. If so where do I make a mistake?,"I've been looking to solve this problem for a few hours now. But I don't seem to progress to the give solution at all. Maybe you could be of some help to me. The question is as follows: ""Reverse the order of integration and evaluate the resulting iterated integral"" $$\int_{-2}^{4}\int_{\frac{x_2^2}{2}-3}^{x_2+1}(x_1)dx_1dx_2$$ So what I did is this: $$x_{1}=x_{2}+1 \Rightarrow x_2=x_1-1$$ $$x_1=\frac{(x^2)}{2}-3 \Rightarrow x_2= -\sqrt{2x_1+6}$$ Given that where $$x_2=4 \Rightarrow x_1 = 4+1 = 5 $$ Then for, $$x_2=-2 \Rightarrow x_1=\frac{-2^2}{2}-3=-1$$ Which if I'm right gives me the new intervals for the reverse iterated interval. Namely, $$\int_{-1}^{5}\left(\int_{-\sqrt{2x_1+6}}^{x_1-1}(x_1)dx_2\right)dx_1$$ Then integrated to: $$\int_{-1}^{5}\left[x_1*(x_1-1)-(x_1*-\sqrt{2x_1+6})\right] dx_1 $$ According to my teachers solution manual the solution should be should be (18/5). Which is the solution to the integral when not reversing the order of integration.","['multivariable-calculus', 'real-analysis']"
3217226,Munkres Exercise 39.6. Showing that a collection is countably locally finite but not locally finite.,"This is Munkres Exercise 39.6. Consider $\mathbb{R}^\omega$ in the uniform topology. Given $n$ , let $\mathscr{B}_n$ be the collection of all subsets of $\mathbb{R}^\omega$ of the form $\Pi A_i$ , where $A_i = \mathbb{R}$ for $i \le n$ and $A_i$ equals either $\{0\}$ or $\{1\}$ otherwise. Show that the collection $\mathscr{B}=\cup \mathscr{B}_n$ is countable locally finite, but neither countable nor locally finite. The collection is clearly not countable. I have trouble with showing that it is countably locally finite but not locally finite. The idea that I have is, we may focus on points that have, for all but finitely many coordinates, values between $0$ and $2$ . This is because if they have infinitely many coordinates with values $\ge 2$ than a radius $1$ ball will not intersect any of the elements from any collection. Thus, take a point $x$ that takes values $(0,2)$ from some $n$ . Then, if all $x_i\ge 1$ for $i \ge n$ , then take the radius $1$ ball and it intersects only the set $\mathbb{R}^n \times 1 \times 1 \cdots$ from $\mathscr{B}_n$ . 
Hence we may consider $x$ that takes values between $0$ and $1$ for some coordinate $\ge n$ . In this case take the radius $1/2$ ball. Then for the coordinates in $(0,1/2)$ it will intersect only the point $0$ and for the coordinates in $(1/2,1)$ it will intersect only $1$ , while if it is $1/2$ then it will not intersect any. Hence in any case, it will intersect only finitely many elements $A_i$ . To show that it is not locally finite, we simply note that if $x$ is a point with a neighborhood that intersects finite( $>0$ ) number of elements from $\mathscr{B}_n$ , then it intersects with some elements from $\mathscr{B}_m$ , $m\ge n$ . Hence we have intersection with countably infinite number of elements. Is this idea sound? I would appreciate any help.","['general-topology', 'metric-spaces', 'analysis']"
3217234,How to prove the limit exist?,"Let: $\displaystyle f=\int_V \dfrac{x-x'}{|\mathbf{r}-\mathbf{r'}|^3}\ dV'$ where $V'$ is a finite volume in space $\mathbf{r}=(x,y,z)$ are coordinates of all space $\mathbf{r'}=(x',y',z')$ are coordinates of $V'$ $|\mathbf{r}-\mathbf{r'}|=[(x-x')^2+(y-y')^2+(z-z')^2]^{1/2}$ How to prove that: $\lim\limits_{\Delta x \to 0} \dfrac{f(x+\Delta x,y,z)-f(x,y,z)}{\Delta x}$ exist $\text{ }$ MY TRY: I am not sure whether this method would work. If it doesn't please suggest another method to reach my goal. \begin{align}
&\lim\limits_{\Delta x \to 0} \dfrac{f(x+\Delta x,y,z)-f(x,y,z)}{\Delta x}\\ 
=&\lim\limits_{\Delta x \to 0}\dfrac{\displaystyle\int_{V'} \dfrac{(x+\Delta x)-x'}{|\mathbf{r}(x+\Delta x,y,z)-\mathbf{r'}|^3}\ dV' - \int_{V'} \dfrac{x-x'}{|\mathbf{r}(x,y,z)-\mathbf{r'}|^3}\ dV'}{\Delta x}\\ 
=&\lim\limits_{\Delta x \to 0}\displaystyle\int_{V'}                    
\dfrac{\left(  \dfrac{(x+\Delta x)-x'}{|\mathbf{r}(x+\Delta x,y,z)-\mathbf{r'}|^3} 
-\dfrac{x-x'}{|\mathbf{r}(x,y,z)-\mathbf{r'}|^3}   \right)}{\Delta x}dV'
\end{align} Now if only I could take the limit inside the integral (with respect to $V′$ ),I can proceed to show the limit exists. If we can't do that and this method doesn't work, please suggest another method to show that the limit exists.","['definite-integrals', 'functions', 'partial-derivative', 'limits', 'derivatives']"
